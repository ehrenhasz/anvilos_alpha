{
  "module_name": "bfq-iosched.c",
  "hash_id": "43f9d21fc6ff46af16ee895f8a824f109625fb09e545aa57ca007fa883c1bc12",
  "original_prompt": "Ingested from linux-6.6.14/block/bfq-iosched.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/cgroup.h>\n#include <linux/ktime.h>\n#include <linux/rbtree.h>\n#include <linux/ioprio.h>\n#include <linux/sbitmap.h>\n#include <linux/delay.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n#include \"bfq-iosched.h\"\n#include \"blk-wbt.h\"\n\n#define BFQ_BFQQ_FNS(name)\t\t\t\t\t\t\\\nvoid bfq_mark_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__set_bit(BFQQF_##name, &(bfqq)->flags);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nvoid bfq_clear_bfqq_##name(struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t__clear_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nint bfq_bfqq_##name(const struct bfq_queue *bfqq)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BFQQF_##name, &(bfqq)->flags);\t\t\\\n}\n\nBFQ_BFQQ_FNS(just_created);\nBFQ_BFQQ_FNS(busy);\nBFQ_BFQQ_FNS(wait_request);\nBFQ_BFQQ_FNS(non_blocking_wait_rq);\nBFQ_BFQQ_FNS(fifo_expire);\nBFQ_BFQQ_FNS(has_short_ttime);\nBFQ_BFQQ_FNS(sync);\nBFQ_BFQQ_FNS(IO_bound);\nBFQ_BFQQ_FNS(in_large_burst);\nBFQ_BFQQ_FNS(coop);\nBFQ_BFQQ_FNS(split_coop);\nBFQ_BFQQ_FNS(softrt_update);\n#undef BFQ_BFQQ_FNS\t\t\t\t\t\t\\\n\n \nstatic const u64 bfq_fifo_expire[2] = { NSEC_PER_SEC / 4, NSEC_PER_SEC / 8 };\n\n \nstatic const int bfq_back_max = 16 * 1024;\n\n \nstatic const int bfq_back_penalty = 2;\n\n \nstatic u64 bfq_slice_idle = NSEC_PER_SEC / 125;\n\n \nstatic const int bfq_stats_min_budgets = 194;\n\n \nstatic const int bfq_default_max_budget = 16 * 1024;\n\n \nstatic const int bfq_async_charge_factor = 3;\n\n \nconst int bfq_timeout = HZ / 8;\n\n \nstatic const unsigned long bfq_merge_time_limit = HZ/10;\n\nstatic struct kmem_cache *bfq_pool;\n\n \n#define BFQ_MIN_TT\t\t(2 * NSEC_PER_MSEC)\n\n \n#define BFQ_HW_QUEUE_THRESHOLD\t3\n#define BFQ_HW_QUEUE_SAMPLES\t32\n\n#define BFQQ_SEEK_THR\t\t(sector_t)(8 * 100)\n#define BFQQ_SECT_THR_NONROT\t(sector_t)(2 * 32)\n#define BFQ_RQ_SEEKY(bfqd, last_pos, rq) \\\n\t(get_sdist(last_pos, rq) >\t\t\t\\\n\t BFQQ_SEEK_THR &&\t\t\t\t\\\n\t (!blk_queue_nonrot(bfqd->queue) ||\t\t\\\n\t  blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT))\n#define BFQQ_CLOSE_THR\t\t(sector_t)(8 * 1024)\n#define BFQQ_SEEKY(bfqq)\t(hweight32(bfqq->seek_history) > 19)\n \n#define BFQQ_TOTALLY_SEEKY(bfqq)\t(bfqq->seek_history == -1)\n\n \n#define BFQ_RATE_MIN_SAMPLES\t32\n \n#define BFQ_RATE_MIN_INTERVAL\t(300*NSEC_PER_MSEC)\n \n#define BFQ_RATE_REF_INTERVAL\tNSEC_PER_SEC\n\n \n#define BFQ_RATE_SHIFT\t\t16\n\n \nstatic int ref_rate[2] = {14000, 33000};\n \nstatic int ref_wr_duration[2];\n\n \nstatic const unsigned long max_service_from_wr = 120000;\n\n \nstatic const unsigned long bfq_activation_stable_merging = 600;\n \nstatic const unsigned long bfq_late_stable_merging = 600;\n\n#define RQ_BIC(rq)\t\t((struct bfq_io_cq *)((rq)->elv.priv[0]))\n#define RQ_BFQQ(rq)\t\t((rq)->elv.priv[1])\n\nstruct bfq_queue *bic_to_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tif (is_sync)\n\t\treturn bic->bfqq[1][actuator_idx];\n\n\treturn bic->bfqq[0][actuator_idx];\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq);\n\nvoid bic_set_bfqq(struct bfq_io_cq *bic,\n\t\t  struct bfq_queue *bfqq,\n\t\t  bool is_sync,\n\t\t  unsigned int actuator_idx)\n{\n\tstruct bfq_queue *old_bfqq = bic->bfqq[is_sync][actuator_idx];\n\n\t \n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[actuator_idx];\n\n\t \n\tif (old_bfqq && old_bfqq->bic == bic)\n\t\told_bfqq->bic = NULL;\n\n\tif (is_sync)\n\t\tbic->bfqq[1][actuator_idx] = bfqq;\n\telse\n\t\tbic->bfqq[0][actuator_idx] = bfqq;\n\n\tif (bfqq && bfqq_data->stable_merge_bfqq == bfqq) {\n\t\t \n\t\tbfq_put_stable_ref(bfqq_data->stable_merge_bfqq);\n\n\t\tbfqq_data->stable_merge_bfqq = NULL;\n\t}\n}\n\nstruct bfq_data *bic_to_bfqd(struct bfq_io_cq *bic)\n{\n\treturn bic->icq.q->elevator->elevator_data;\n}\n\n \nstatic struct bfq_io_cq *icq_to_bic(struct io_cq *icq)\n{\n\t \n\treturn container_of(icq, struct bfq_io_cq, icq);\n}\n\n \nstatic struct bfq_io_cq *bfq_bic_lookup(struct request_queue *q)\n{\n\tstruct bfq_io_cq *icq;\n\tunsigned long flags;\n\n\tif (!current->io_context)\n\t\treturn NULL;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\ticq = icq_to_bic(ioc_lookup_icq(q));\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\treturn icq;\n}\n\n \nvoid bfq_schedule_dispatch(struct bfq_data *bfqd)\n{\n\tlockdep_assert_held(&bfqd->lock);\n\n\tif (bfqd->queued != 0) {\n\t\tbfq_log(bfqd, \"schedule dispatch\");\n\t\tblk_mq_run_hw_queues(bfqd->queue, true);\n\t}\n}\n\n#define bfq_class_idle(bfqq)\t((bfqq)->ioprio_class == IOPRIO_CLASS_IDLE)\n\n#define bfq_sample_valid(samples)\t((samples) > 80)\n\n \nstatic struct request *bfq_choose_req(struct bfq_data *bfqd,\n\t\t\t\t      struct request *rq1,\n\t\t\t\t      struct request *rq2,\n\t\t\t\t      sector_t last)\n{\n\tsector_t s1, s2, d1 = 0, d2 = 0;\n\tunsigned long back_max;\n#define BFQ_RQ1_WRAP\t0x01  \n#define BFQ_RQ2_WRAP\t0x02  \n\tunsigned int wrap = 0;  \n\n\tif (!rq1 || rq1 == rq2)\n\t\treturn rq2;\n\tif (!rq2)\n\t\treturn rq1;\n\n\tif (rq_is_sync(rq1) && !rq_is_sync(rq2))\n\t\treturn rq1;\n\telse if (rq_is_sync(rq2) && !rq_is_sync(rq1))\n\t\treturn rq2;\n\tif ((rq1->cmd_flags & REQ_META) && !(rq2->cmd_flags & REQ_META))\n\t\treturn rq1;\n\telse if ((rq2->cmd_flags & REQ_META) && !(rq1->cmd_flags & REQ_META))\n\t\treturn rq2;\n\n\ts1 = blk_rq_pos(rq1);\n\ts2 = blk_rq_pos(rq2);\n\n\t \n\tback_max = bfqd->bfq_back_max * 2;\n\n\t \n\tif (s1 >= last)\n\t\td1 = s1 - last;\n\telse if (s1 + back_max >= last)\n\t\td1 = (last - s1) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ1_WRAP;\n\n\tif (s2 >= last)\n\t\td2 = s2 - last;\n\telse if (s2 + back_max >= last)\n\t\td2 = (last - s2) * bfqd->bfq_back_penalty;\n\telse\n\t\twrap |= BFQ_RQ2_WRAP;\n\n\t \n\n\t \n\tswitch (wrap) {\n\tcase 0:  \n\t\tif (d1 < d2)\n\t\t\treturn rq1;\n\t\telse if (d2 < d1)\n\t\t\treturn rq2;\n\n\t\tif (s1 >= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\n\tcase BFQ_RQ2_WRAP:\n\t\treturn rq1;\n\tcase BFQ_RQ1_WRAP:\n\t\treturn rq2;\n\tcase BFQ_RQ1_WRAP|BFQ_RQ2_WRAP:  \n\tdefault:\n\t\t \n\t\tif (s1 <= s2)\n\t\t\treturn rq1;\n\t\telse\n\t\t\treturn rq2;\n\t}\n}\n\n#define BFQ_LIMIT_INLINE_DEPTH 16\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\nstatic bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];\n\tstruct bfq_entity **entities = inline_entities;\n\tint depth, level, alloc_depth = BFQ_LIMIT_INLINE_DEPTH;\n\tint class_idx = bfqq->ioprio_class - 1;\n\tstruct bfq_sched_data *sched_data;\n\tunsigned long wsum;\n\tbool ret = false;\n\n\tif (!entity->on_st_or_in_serv)\n\t\treturn false;\n\nretry:\n\tspin_lock_irq(&bfqd->lock);\n\t \n\tdepth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;\n\tif (depth > alloc_depth) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tif (entities != inline_entities)\n\t\t\tkfree(entities);\n\t\tentities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);\n\t\tif (!entities)\n\t\t\treturn false;\n\t\talloc_depth = depth;\n\t\tgoto retry;\n\t}\n\n\tsched_data = entity->sched_data;\n\t \n\tlevel = 0;\n\tfor_each_entity(entity) {\n\t\t \n\t\tif (!entity->on_st_or_in_serv)\n\t\t\tgoto out;\n\t\t \n\t\tif (WARN_ON_ONCE(level >= depth))\n\t\t\tbreak;\n\t\tentities[level++] = entity;\n\t}\n\tWARN_ON_ONCE(level != depth);\n\tfor (level--; level >= 0; level--) {\n\t\tentity = entities[level];\n\t\tif (level > 0) {\n\t\t\twsum = bfq_entity_service_tree(entity)->wsum;\n\t\t} else {\n\t\t\tint i;\n\t\t\t \n\t\t\twsum = 0;\n\t\t\tfor (i = 0; i <= class_idx; i++) {\n\t\t\t\twsum = wsum * IOPRIO_BE_NR +\n\t\t\t\t\tsched_data->service_tree[i].wsum;\n\t\t\t}\n\t\t}\n\t\tif (!wsum)\n\t\t\tcontinue;\n\t\tlimit = DIV_ROUND_CLOSEST(limit * entity->weight, wsum);\n\t\tif (entity->allocated >= limit) {\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t\"too many requests: allocated %d limit %d level %d\",\n\t\t\t\tentity->allocated, limit, level);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tspin_unlock_irq(&bfqd->lock);\n\tif (entities != inline_entities)\n\t\tkfree(entities);\n\treturn ret;\n}\n#else\nstatic bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)\n{\n\treturn false;\n}\n#endif\n\n \nstatic void bfq_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\tstruct bfq_data *bfqd = data->q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic = bfq_bic_lookup(data->q);\n\tint depth;\n\tunsigned limit = data->q->nr_requests;\n\tunsigned int act_idx;\n\n\t \n\tif (op_is_sync(opf) && !op_is_write(opf)) {\n\t\tdepth = 0;\n\t} else {\n\t\tdepth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(opf)];\n\t\tlimit = (limit * depth) >> bfqd->full_depth_shift;\n\t}\n\n\tfor (act_idx = 0; bic && act_idx < bfqd->num_actuators; act_idx++) {\n\t\tstruct bfq_queue *bfqq =\n\t\t\tbic_to_bfqq(bic, op_is_sync(opf), act_idx);\n\n\t\t \n\t\tif (bfqq && bfqq_request_over_limit(bfqq, limit)) {\n\t\t\tdepth = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbfq_log(bfqd, \"[%s] wr_busy %d sync %d depth %u\",\n\t\t__func__, bfqd->wr_busy_queues, op_is_sync(opf), depth);\n\tif (depth)\n\t\tdata->shallow_depth = depth;\n}\n\nstatic struct bfq_queue *\nbfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,\n\t\t     sector_t sector, struct rb_node **ret_parent,\n\t\t     struct rb_node ***rb_link)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tparent = NULL;\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct rb_node **n;\n\n\t\tparent = *p;\n\t\tbfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\n\t\t \n\t\tif (sector > blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_right;\n\t\telse if (sector < blk_rq_pos(bfqq->next_rq))\n\t\t\tn = &(*p)->rb_left;\n\t\telse\n\t\t\tbreak;\n\t\tp = n;\n\t\tbfqq = NULL;\n\t}\n\n\t*ret_parent = parent;\n\tif (rb_link)\n\t\t*rb_link = p;\n\n\tbfq_log(bfqd, \"rq_pos_tree_lookup %llu: returning %d\",\n\t\t(unsigned long long)sector,\n\t\tbfqq ? bfqq->pid : 0);\n\n\treturn bfqq;\n}\n\nstatic bool bfq_too_late_for_merging(struct bfq_queue *bfqq)\n{\n\treturn bfqq->service_from_backlogged > 0 &&\n\t\ttime_is_before_jiffies(bfqq->first_IO_time +\n\t\t\t\t       bfq_merge_time_limit);\n}\n\n \nvoid __cold\nbfq_pos_tree_add_move(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct rb_node **p, *parent;\n\tstruct bfq_queue *__bfqq;\n\n\tif (bfqq->pos_root) {\n\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\tbfqq->pos_root = NULL;\n\t}\n\n\t \n\tif (bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t \n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn;\n\n\tif (bfq_class_idle(bfqq))\n\t\treturn;\n\tif (!bfqq->next_rq)\n\t\treturn;\n\n\tbfqq->pos_root = &bfqq_group(bfqq)->rq_pos_tree;\n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, bfqq->pos_root,\n\t\t\tblk_rq_pos(bfqq->next_rq), &parent, &p);\n\tif (!__bfqq) {\n\t\trb_link_node(&bfqq->pos_node, parent, p);\n\t\trb_insert_color(&bfqq->pos_node, bfqq->pos_root);\n\t} else\n\t\tbfqq->pos_root = NULL;\n}\n\n \nstatic bool bfq_asymmetric_scenario(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tbool smallest_weight = bfqq &&\n\t\tbfqq->weight_counter &&\n\t\tbfqq->weight_counter ==\n\t\tcontainer_of(\n\t\t\trb_first_cached(&bfqd->queue_weights_tree),\n\t\t\tstruct bfq_weight_counter,\n\t\t\tweights_node);\n\n\t \n\tbool varied_queue_weights = !smallest_weight &&\n\t\t!RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&\n\t\t(bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||\n\t\t bfqd->queue_weights_tree.rb_root.rb_node->rb_right);\n\n\tbool multiple_classes_busy =\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[1]) ||\n\t\t(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||\n\t\t(bfqd->busy_queues[1] && bfqd->busy_queues[2]);\n\n\treturn varied_queue_weights || multiple_classes_busy\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\t       || bfqd->num_groups_with_pending_reqs > 1\n#endif\n\t\t;\n}\n\n \nvoid bfq_weights_tree_add(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root = &bfqq->bfqd->queue_weights_tree;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct rb_node **new = &(root->rb_root.rb_node), *parent = NULL;\n\tbool leftmost = true;\n\n\t \n\tif (bfqq->weight_counter)\n\t\treturn;\n\n\twhile (*new) {\n\t\tstruct bfq_weight_counter *__counter = container_of(*new,\n\t\t\t\t\t\tstruct bfq_weight_counter,\n\t\t\t\t\t\tweights_node);\n\t\tparent = *new;\n\n\t\tif (entity->weight == __counter->weight) {\n\t\t\tbfqq->weight_counter = __counter;\n\t\t\tgoto inc_counter;\n\t\t}\n\t\tif (entity->weight < __counter->weight)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse {\n\t\t\tnew = &((*new)->rb_right);\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\tbfqq->weight_counter = kzalloc(sizeof(struct bfq_weight_counter),\n\t\t\t\t       GFP_ATOMIC);\n\n\t \n\tif (unlikely(!bfqq->weight_counter))\n\t\treturn;\n\n\tbfqq->weight_counter->weight = entity->weight;\n\trb_link_node(&bfqq->weight_counter->weights_node, parent, new);\n\trb_insert_color_cached(&bfqq->weight_counter->weights_node, root,\n\t\t\t\tleftmost);\n\ninc_counter:\n\tbfqq->weight_counter->num_active++;\n\tbfqq->ref++;\n}\n\n \nvoid bfq_weights_tree_remove(struct bfq_queue *bfqq)\n{\n\tstruct rb_root_cached *root;\n\n\tif (!bfqq->weight_counter)\n\t\treturn;\n\n\troot = &bfqq->bfqd->queue_weights_tree;\n\tbfqq->weight_counter->num_active--;\n\tif (bfqq->weight_counter->num_active > 0)\n\t\tgoto reset_entity_pointer;\n\n\trb_erase_cached(&bfqq->weight_counter->weights_node, root);\n\tkfree(bfqq->weight_counter);\n\nreset_entity_pointer:\n\tbfqq->weight_counter = NULL;\n\tbfq_put_queue(bfqq);\n}\n\n \nstatic struct request *bfq_check_fifo(struct bfq_queue *bfqq,\n\t\t\t\t      struct request *last)\n{\n\tstruct request *rq;\n\n\tif (bfq_bfqq_fifo_expire(bfqq))\n\t\treturn NULL;\n\n\tbfq_mark_bfqq_fifo_expire(bfqq);\n\n\trq = rq_entry_fifo(bfqq->fifo.next);\n\n\tif (rq == last || ktime_get_ns() < rq->fifo_time)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"check_fifo: returned %p\", rq);\n\treturn rq;\n}\n\nstatic struct request *bfq_find_next_rq(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\tstruct request *last)\n{\n\tstruct rb_node *rbnext = rb_next(&last->rb_node);\n\tstruct rb_node *rbprev = rb_prev(&last->rb_node);\n\tstruct request *next, *prev = NULL;\n\n\t \n\tnext = bfq_check_fifo(bfqq, last);\n\tif (next)\n\t\treturn next;\n\n\tif (rbprev)\n\t\tprev = rb_entry_rq(rbprev);\n\n\tif (rbnext)\n\t\tnext = rb_entry_rq(rbnext);\n\telse {\n\t\trbnext = rb_first(&bfqq->sort_list);\n\t\tif (rbnext && rbnext != &last->rb_node)\n\t\t\tnext = rb_entry_rq(rbnext);\n\t}\n\n\treturn bfq_choose_req(bfqd, next, prev, blk_rq_pos(last));\n}\n\n \nstatic unsigned long bfq_serv_to_charge(struct request *rq,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\tif (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||\n\t    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))\n\t\treturn blk_rq_sectors(rq);\n\n\treturn blk_rq_sectors(rq) * bfq_async_charge_factor;\n}\n\n \nstatic void bfq_updated_next_req(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\tstruct request *next_rq = bfqq->next_rq;\n\tunsigned long new_budget;\n\n\tif (!next_rq)\n\t\treturn;\n\n\tif (bfqq == bfqd->in_service_queue)\n\t\t \n\t\treturn;\n\n\tnew_budget = max_t(unsigned long,\n\t\t\t   max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t bfq_serv_to_charge(next_rq, bfqq)),\n\t\t\t   entity->service);\n\tif (entity->budget != new_budget) {\n\t\tentity->budget = new_budget;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"updated next rq: new budget %lu\",\n\t\t\t\t\t new_budget);\n\t\tbfq_requeue_bfqq(bfqd, bfqq, false);\n\t}\n}\n\nstatic unsigned int bfq_wr_duration(struct bfq_data *bfqd)\n{\n\tu64 dur;\n\n\tdur = bfqd->rate_dur_prod;\n\tdo_div(dur, bfqd->peak_rate);\n\n\t \n\treturn clamp_val(dur, msecs_to_jiffies(3000), msecs_to_jiffies(25000));\n}\n\n \nstatic void switch_back_to_interactive_wr(struct bfq_queue *bfqq,\n\t\t\t\t\t  struct bfq_data *bfqd)\n{\n\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\tbfqq->last_wr_start_finish = bfqq->wr_start_at_switch_to_srt;\n}\n\nstatic void\nbfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,\n\t\t      struct bfq_io_cq *bic, bool bfq_already_existing)\n{\n\tunsigned int old_wr_coeff = 1;\n\tbool busy = bfq_already_existing && bfq_bfqq_busy(bfqq);\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\tif (bfqq_data->saved_has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\tif (bfqq_data->saved_IO_bound)\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\tbfqq->last_serv_time_ns = bfqq_data->saved_last_serv_time_ns;\n\tbfqq->inject_limit = bfqq_data->saved_inject_limit;\n\tbfqq->decrease_time_jif = bfqq_data->saved_decrease_time_jif;\n\n\tbfqq->entity.new_weight = bfqq_data->saved_weight;\n\tbfqq->ttime = bfqq_data->saved_ttime;\n\tbfqq->io_start_time = bfqq_data->saved_io_start_time;\n\tbfqq->tot_idle_time = bfqq_data->saved_tot_idle_time;\n\t \n\tif (bfqd->low_latency) {\n\t\told_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq->wr_coeff = bfqq_data->saved_wr_coeff;\n\t}\n\tbfqq->service_from_wr = bfqq_data->saved_service_from_wr;\n\tbfqq->wr_start_at_switch_to_srt =\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt;\n\tbfqq->last_wr_start_finish = bfqq_data->saved_last_wr_start_finish;\n\tbfqq->wr_cur_max_time = bfqq_data->saved_wr_cur_max_time;\n\n\tif (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t   bfqq->wr_cur_max_time))) {\n\t\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t     bfq_wr_duration(bfqd))) {\n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t} else {\n\t\t\tbfqq->wr_coeff = 1;\n\t\t\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\t\t     \"resume state: switching off wr\");\n\t\t}\n\t}\n\n\t \n\tbfqq->entity.prio_changed = 1;\n\n\tif (likely(!busy))\n\t\treturn;\n\n\tif (old_wr_coeff == 1 && bfqq->wr_coeff > 1)\n\t\tbfqd->wr_busy_queues++;\n\telse if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)\n\t\tbfqd->wr_busy_queues--;\n}\n\nstatic int bfqq_process_refs(struct bfq_queue *bfqq)\n{\n\treturn bfqq->ref - bfqq->entity.allocated -\n\t\tbfqq->entity.on_st_or_in_serv -\n\t\t(bfqq->weight_counter != NULL) - bfqq->stable_ref;\n}\n\n \nstatic void bfq_reset_burst_list(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(item, n, &bfqd->burst_list, burst_list_node)\n\t\thlist_del_init(&item->burst_list_node);\n\n\t \n\tif (bfq_tot_busy_queues(bfqd) == 0) {\n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n\t\tbfqd->burst_size = 1;\n\t} else\n\t\tbfqd->burst_size = 0;\n\n\tbfqd->burst_parent_entity = bfqq->entity.parent;\n}\n\n \nstatic void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t \n\tbfqd->burst_size++;\n\n\tif (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {\n\t\tstruct bfq_queue *pos, *bfqq_item;\n\t\tstruct hlist_node *n;\n\n\t\t \n\t\tbfqd->large_burst = true;\n\n\t\t \n\t\thlist_for_each_entry(bfqq_item, &bfqd->burst_list,\n\t\t\t\t     burst_list_node)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq_item);\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\n\t\t \n\t\thlist_for_each_entry_safe(pos, n, &bfqd->burst_list,\n\t\t\t\t\t  burst_list_node)\n\t\t\thlist_del_init(&pos->burst_list_node);\n\t} else  \n\t\thlist_add_head(&bfqq->burst_list_node, &bfqd->burst_list);\n}\n\n \nstatic void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t \n\tif (!hlist_unhashed(&bfqq->burst_list_node) ||\n\t    bfq_bfqq_in_large_burst(bfqq) ||\n\t    time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     msecs_to_jiffies(10)))\n\t\treturn;\n\n\t \n\tif (time_is_before_jiffies(bfqd->last_ins_in_burst +\n\t    bfqd->bfq_burst_interval) ||\n\t    bfqq->entity.parent != bfqd->burst_parent_entity) {\n\t\tbfqd->large_burst = false;\n\t\tbfq_reset_burst_list(bfqd, bfqq);\n\t\tgoto end;\n\t}\n\n\t \n\tif (bfqd->large_burst) {\n\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\tgoto end;\n\t}\n\n\t \n\tbfq_add_to_burst(bfqd, bfqq);\nend:\n\t \n\tbfqd->last_ins_in_burst = jiffies;\n}\n\nstatic int bfq_bfqq_budget_left(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\treturn entity->budget - entity->service;\n}\n\n \nstatic int bfq_max_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget;\n\telse\n\t\treturn bfqd->bfq_max_budget;\n}\n\n \nstatic int bfq_min_budget(struct bfq_data *bfqd)\n{\n\tif (bfqd->budgets_assigned < bfq_stats_min_budgets)\n\t\treturn bfq_default_max_budget / 32;\n\telse\n\t\treturn bfqd->bfq_max_budget / 32;\n}\n\n \nstatic bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq,\n\t\t\t\t\t\tbool arrived_in_time)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t \n\tif (bfq_bfqq_non_blocking_wait_rq(bfqq) && arrived_in_time &&\n\t    bfq_bfqq_budget_left(bfqq) > 0) {\n\t\t \n\n\t\t \n\t\tentity->budget = min_t(unsigned long,\n\t\t\t\t       bfq_bfqq_budget_left(bfqq),\n\t\t\t\t       bfqq->max_budget);\n\n\t\t \n\t\tentity->service = 0;\n\n\t\treturn true;\n\t}\n\n\t \n\tentity->service = 0;\n\tentity->budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t       bfq_serv_to_charge(bfqq->next_rq, bfqq));\n\tbfq_clear_bfqq_non_blocking_wait_rq(bfqq);\n\treturn false;\n}\n\n \nstatic unsigned long bfq_smallest_from_now(void)\n{\n\treturn jiffies - MAX_JIFFY_OFFSET;\n}\n\nstatic void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     unsigned int old_wr_coeff,\n\t\t\t\t\t     bool wr_or_deserves_wr,\n\t\t\t\t\t     bool interactive,\n\t\t\t\t\t     bool in_burst,\n\t\t\t\t\t     bool soft_rt)\n{\n\tif (old_wr_coeff == 1 && wr_or_deserves_wr) {\n\t\t \n\t\tif (interactive) {\n\t\t\tbfqq->service_from_wr = 0;\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else {\n\t\t\t \n\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\tbfq_smallest_from_now();\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t}\n\n\t\t \n\t\tbfqq->entity.budget = min_t(unsigned long,\n\t\t\t\t\t    bfqq->entity.budget,\n\t\t\t\t\t    2 * bfq_min_budget(bfqd));\n\t} else if (old_wr_coeff > 1) {\n\t\tif (interactive) {  \n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\t\t} else if (in_burst)\n\t\t\tbfqq->wr_coeff = 1;\n\t\telse if (soft_rt) {\n\t\t\t \n\t\t\tif (bfqq->wr_cur_max_time !=\n\t\t\t\tbfqd->bfq_wr_rt_max_time) {\n\t\t\t\tbfqq->wr_start_at_switch_to_srt =\n\t\t\t\t\tbfqq->last_wr_start_finish;\n\n\t\t\t\tbfqq->wr_cur_max_time =\n\t\t\t\t\tbfqd->bfq_wr_rt_max_time;\n\t\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff *\n\t\t\t\t\tBFQ_SOFTRT_WEIGHT_FACTOR;\n\t\t\t}\n\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\t}\n}\n\nstatic bool bfq_bfqq_idle_for_long_time(struct bfq_data *bfqd,\n\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn bfqq->dispatched == 0 &&\n\t\ttime_is_before_jiffies(\n\t\t\tbfqq->budget_timeout +\n\t\t\tbfqd->bfq_wr_min_idle_time);\n}\n\n\n \nstatic bool bfq_bfqq_higher_class_or_weight(struct bfq_queue *bfqq,\n\t\t\t\t\t    struct bfq_queue *in_serv_bfqq)\n{\n\tint bfqq_weight, in_serv_weight;\n\n\tif (bfqq->ioprio_class < in_serv_bfqq->ioprio_class)\n\t\treturn true;\n\n\tif (in_serv_bfqq->entity.parent == bfqq->entity.parent) {\n\t\tbfqq_weight = bfqq->entity.weight;\n\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t} else {\n\t\tif (bfqq->entity.parent)\n\t\t\tbfqq_weight = bfqq->entity.parent->weight;\n\t\telse\n\t\t\tbfqq_weight = bfqq->entity.weight;\n\t\tif (in_serv_bfqq->entity.parent)\n\t\t\tin_serv_weight = in_serv_bfqq->entity.parent->weight;\n\t\telse\n\t\t\tin_serv_weight = in_serv_bfqq->entity.weight;\n\t}\n\n\treturn bfqq_weight > in_serv_weight;\n}\n\n \nstatic unsigned int bfq_actuator_index(struct bfq_data *bfqd, struct bio *bio)\n{\n\tunsigned int i;\n\tsector_t end;\n\n\t \n\tif (bfqd->num_actuators == 1)\n\t\treturn 0;\n\n\t \n\tend = bio_end_sector(bio) - 1;\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tif (end >= bfqd->sector[i] &&\n\t\t    end < bfqd->sector[i] + bfqd->nr_sectors[i])\n\t\t\treturn i;\n\t}\n\n\tWARN_ONCE(true,\n\t\t  \"bfq_actuator_index: bio sector out of ranges: end=%llu\\n\",\n\t\t  end);\n\treturn 0;\n}\n\nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq);\n\nstatic void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t\t     int old_wr_coeff,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     bool *interactive)\n{\n\tbool soft_rt, in_burst,\twr_or_deserves_wr,\n\t\tbfqq_wants_to_preempt,\n\t\tidle_for_long_time = bfq_bfqq_idle_for_long_time(bfqd, bfqq),\n\t\t \n\t\tarrived_in_time =  ktime_get_ns() <=\n\t\t\tbfqq->ttime.last_end_request +\n\t\t\tbfqd->bfq_slice_idle * 3;\n\tunsigned int act_idx = bfq_actuator_index(bfqd, rq->bio);\n\tbool bfqq_non_merged_or_stably_merged =\n\t\tbfqq->bic || RQ_BIC(rq)->bfqq_data[act_idx].stably_merged;\n\n\t \n\tin_burst = bfq_bfqq_in_large_burst(bfqq);\n\tsoft_rt = bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t\t!BFQQ_TOTALLY_SEEKY(bfqq) &&\n\t\t!in_burst &&\n\t\ttime_is_before_jiffies(bfqq->soft_rt_next_start) &&\n\t\tbfqq->dispatched == 0 &&\n\t\tbfqq->entity.new_weight == 40;\n\t*interactive = !in_burst && idle_for_long_time &&\n\t\tbfqq->entity.new_weight == 40;\n\t \n\twr_or_deserves_wr = bfqd->low_latency &&\n\t\t(bfqq->wr_coeff > 1 ||\n\t\t (bfq_bfqq_sync(bfqq) && bfqq_non_merged_or_stably_merged &&\n\t\t  (*interactive || soft_rt)));\n\n\t \n\tbfqq_wants_to_preempt =\n\t\tbfq_bfqq_update_budg_for_activation(bfqd, bfqq,\n\t\t\t\t\t\t    arrived_in_time);\n\n\t \n\tif (likely(!bfq_bfqq_just_created(bfqq)) &&\n\t    idle_for_long_time &&\n\t    time_is_before_jiffies(\n\t\t    bfqq->budget_timeout +\n\t\t    msecs_to_jiffies(10000))) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t}\n\n\tbfq_clear_bfqq_just_created(bfqq);\n\n\tif (bfqd->low_latency) {\n\t\tif (unlikely(time_is_after_jiffies(bfqq->split_time)))\n\t\t\t \n\t\t\tbfqq->split_time =\n\t\t\t\tjiffies - bfqd->bfq_wr_min_idle_time - 1;\n\n\t\tif (time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t   bfqd->bfq_wr_min_idle_time)) {\n\t\t\tbfq_update_bfqq_wr_on_rq_arrival(bfqd, bfqq,\n\t\t\t\t\t\t\t old_wr_coeff,\n\t\t\t\t\t\t\t wr_or_deserves_wr,\n\t\t\t\t\t\t\t *interactive,\n\t\t\t\t\t\t\t in_burst,\n\t\t\t\t\t\t\t soft_rt);\n\n\t\t\tif (old_wr_coeff != bfqq->wr_coeff)\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n\n\tbfqq->last_idle_bklogged = jiffies;\n\tbfqq->service_from_backlogged = 0;\n\tbfq_clear_bfqq_softrt_update(bfqq);\n\n\tbfq_add_bfqq_busy(bfqq);\n\n\t \n\tif (bfqd->in_service_queue &&\n\t    ((bfqq_wants_to_preempt &&\n\t      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||\n\t     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||\n\t     !bfq_better_to_idle(bfqd->in_service_queue)) &&\n\t    next_queue_may_preempt(bfqd))\n\t\tbfq_bfqq_expire(bfqd, bfqd->in_service_queue,\n\t\t\t\tfalse, BFQQE_PREEMPTED);\n}\n\nstatic void bfq_reset_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\t \n\tbfqq->last_serv_time_ns = 0;\n\n\t \n\tbfqd->waited_rq = NULL;\n\n\t \n\tif (bfq_bfqq_has_short_ttime(bfqq))\n\t\tbfqq->inject_limit = 0;\n\telse\n\t\tbfqq->inject_limit = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)\n{\n\tu64 tot_io_time = now_ns - bfqq->io_start_time;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) && bfqq->dispatched == 0)\n\t\tbfqq->tot_idle_time +=\n\t\t\tnow_ns - bfqq->ttime.last_end_request;\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq)))\n\t\treturn;\n\n\t \n\tif (bfqq->tot_idle_time * 5 > tot_io_time)\n\t\tbfq_clear_bfqq_IO_bound(bfqq);\n\telse\n\t\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\t \n\tif (tot_io_time > 200 * NSEC_PER_MSEC) {\n\t\tbfqq->io_start_time = now_ns - (tot_io_time>>1);\n\t\tbfqq->tot_idle_time >>= 1;\n\t}\n}\n\n \nstatic void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    u64 now_ns)\n{\n\tchar waker_name[MAX_BFQQ_NAME_LENGTH];\n\n\tif (!bfqd->last_completed_rq_bfqq ||\n\t    bfqd->last_completed_rq_bfqq == bfqq ||\n\t    bfq_bfqq_has_short_ttime(bfqq) ||\n\t    now_ns - bfqd->last_completion >= 4 * NSEC_PER_MSEC ||\n\t    bfqd->last_completed_rq_bfqq == &bfqd->oom_bfqq ||\n\t    bfqq == &bfqd->oom_bfqq)\n\t\treturn;\n\n\t \n\tif (bfqd->last_completed_rq_bfqq !=\n\t    bfqq->tentative_waker_bfqq ||\n\t    now_ns > bfqq->waker_detection_started +\n\t\t\t\t\t128 * (u64)bfqd->bfq_slice_idle) {\n\t\t \n\t\tbfqq->tentative_waker_bfqq =\n\t\t\tbfqd->last_completed_rq_bfqq;\n\t\tbfqq->num_waker_detections = 1;\n\t\tbfqq->waker_detection_started = now_ns;\n\t\tbfq_bfqq_name(bfqq->tentative_waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set tentative waker %s\", waker_name);\n\t} else  \n\t\tbfqq->num_waker_detections++;\n\n\tif (bfqq->num_waker_detections == 3) {\n\t\tbfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;\n\t\tbfqq->tentative_waker_bfqq = NULL;\n\t\tbfq_bfqq_name(bfqq->waker_bfqq, waker_name,\n\t\t\t      MAX_BFQQ_NAME_LENGTH);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"set waker %s\", waker_name);\n\n\t\t \n\t\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\t\thlist_del_init(&bfqq->woken_list_node);\n\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t       &bfqd->last_completed_rq_bfqq->woken_list);\n\t}\n}\n\nstatic void bfq_add_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tstruct request *next_rq, *prev;\n\tunsigned int old_wr_coeff = bfqq->wr_coeff;\n\tbool interactive = false;\n\tu64 now_ns = ktime_get_ns();\n\n\tbfq_log_bfqq(bfqd, bfqq, \"add_request %d\", rq_is_sync(rq));\n\tbfqq->queued[rq_is_sync(rq)]++;\n\t \n\tWRITE_ONCE(bfqd->queued, bfqd->queued + 1);\n\n\tif (bfq_bfqq_sync(bfqq) && RQ_BIC(rq)->requests <= 1) {\n\t\tbfq_check_waker(bfqd, bfqq, now_ns);\n\n\t\t \n\t\tif (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t     msecs_to_jiffies(1000)))\n\t\t\tbfq_reset_inject_limit(bfqd, bfqq);\n\n\t\t \n\t\tif (bfqq == bfqd->in_service_queue &&\n\t\t    (bfqd->tot_rq_in_driver == 0 ||\n\t\t     (bfqq->last_serv_time_ns > 0 &&\n\t\t      bfqd->rqs_injected && bfqd->tot_rq_in_driver > 0)) &&\n\t\t    time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t\t      msecs_to_jiffies(10))) {\n\t\t\tbfqd->last_empty_occupied_ns = ktime_get_ns();\n\t\t\t \n\t\t\tbfqd->wait_dispatch = true;\n\t\t\t \n\t\t\tif (bfqd->tot_rq_in_driver == 0)\n\t\t\t\tbfqd->rqs_injected = false;\n\t\t}\n\t}\n\n\tif (bfq_bfqq_sync(bfqq))\n\t\tbfq_update_io_intensity(bfqq, now_ns);\n\n\telv_rb_add(&bfqq->sort_list, rq);\n\n\t \n\tprev = bfqq->next_rq;\n\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);\n\tbfqq->next_rq = next_rq;\n\n\t \n\tif (unlikely(!bfqd->nonrot_with_queueing && prev != bfqq->next_rq))\n\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\n\tif (!bfq_bfqq_busy(bfqq))  \n\t\tbfq_bfqq_handle_idle_busy_switch(bfqd, bfqq, old_wr_coeff,\n\t\t\t\t\t\t rq, &interactive);\n\telse {\n\t\tif (bfqd->low_latency && old_wr_coeff == 1 && !rq_is_sync(rq) &&\n\t\t    time_is_before_jiffies(\n\t\t\t\tbfqq->last_wr_start_finish +\n\t\t\t\tbfqd->bfq_wr_min_inter_arr_async)) {\n\t\t\tbfqq->wr_coeff = bfqd->bfq_wr_coeff;\n\t\t\tbfqq->wr_cur_max_time = bfq_wr_duration(bfqd);\n\n\t\t\tbfqd->wr_busy_queues++;\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t\tif (prev != bfqq->next_rq)\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\t \n\tif (bfqd->low_latency &&\n\t\t(old_wr_coeff == 1 || bfqq->wr_coeff == 1 || interactive))\n\t\tbfqq->last_wr_start_finish = jiffies;\n}\n\nstatic struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,\n\t\t\t\t\t  struct bio *bio,\n\t\t\t\t\t  struct request_queue *q)\n{\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq;\n\n\n\tif (bfqq)\n\t\treturn elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));\n\n\treturn NULL;\n}\n\nstatic sector_t get_sdist(sector_t last_pos, struct request *rq)\n{\n\tif (last_pos)\n\t\treturn abs(blk_rq_pos(rq) - last_pos);\n\n\treturn 0;\n}\n\nstatic void bfq_remove_request(struct request_queue *q,\n\t\t\t       struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tconst int sync = rq_is_sync(rq);\n\n\tif (bfqq->next_rq == rq) {\n\t\tbfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);\n\t\tbfq_updated_next_req(bfqd, bfqq);\n\t}\n\n\tif (rq->queuelist.prev != &rq->queuelist)\n\t\tlist_del_init(&rq->queuelist);\n\tbfqq->queued[sync]--;\n\t \n\tWRITE_ONCE(bfqd->queued, bfqd->queued - 1);\n\telv_rb_del(&bfqq->sort_list, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n\n\tif (RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\tbfqq->next_rq = NULL;\n\n\t\tif (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {\n\t\t\tbfq_del_bfqq_busy(bfqq, false);\n\t\t\t \n\t\t\tbfqq->entity.budget = bfqq->entity.service = 0;\n\t\t}\n\n\t\t \n\t\tif (bfqq->pos_root) {\n\t\t\trb_erase(&bfqq->pos_node, bfqq->pos_root);\n\t\t\tbfqq->pos_root = NULL;\n\t\t}\n\t} else {\n\t\t \n\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending--;\n\n}\n\nstatic bool bfq_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\t \n\tstruct bfq_io_cq *bic = bfq_bic_lookup(q);\n\tbool ret;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tif (bic) {\n\t\t \n\t\tbfq_bic_update_cgroup(bic, bio);\n\n\t\tbfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf),\n\t\t\t\t\t     bfq_actuator_index(bfqd, bio));\n\t} else {\n\t\tbfqd->bio_bfqq = NULL;\n\t}\n\tbfqd->bio_bic = bic;\n\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\n\tspin_unlock_irq(&bfqd->lock);\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\nstatic int bfq_request_merge(struct request_queue *q, struct request **req,\n\t\t\t     struct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct request *__rq;\n\n\t__rq = bfq_find_rq_fmerge(bfqd, bio, q);\n\tif (__rq && elv_bio_merge_ok(__rq, bio)) {\n\t\t*req = __rq;\n\n\t\tif (blk_discard_mergable(__rq))\n\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\treturn ELEVATOR_FRONT_MERGE;\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void bfq_request_merged(struct request_queue *q, struct request *req,\n\t\t\t       enum elv_merge type)\n{\n\tif (type == ELEVATOR_FRONT_MERGE &&\n\t    rb_prev(&req->rb_node) &&\n\t    blk_rq_pos(req) <\n\t    blk_rq_pos(container_of(rb_prev(&req->rb_node),\n\t\t\t\t    struct request, rb_node))) {\n\t\tstruct bfq_queue *bfqq = RQ_BFQQ(req);\n\t\tstruct bfq_data *bfqd;\n\t\tstruct request *prev, *next_rq;\n\n\t\tif (!bfqq)\n\t\t\treturn;\n\n\t\tbfqd = bfqq->bfqd;\n\n\t\t \n\t\telv_rb_del(&bfqq->sort_list, req);\n\t\telv_rb_add(&bfqq->sort_list, req);\n\n\t\t \n\t\tprev = bfqq->next_rq;\n\t\tnext_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,\n\t\t\t\t\t bfqd->last_position);\n\t\tbfqq->next_rq = next_rq;\n\t\t \n\t\tif (prev != bfqq->next_rq) {\n\t\t\tbfq_updated_next_req(bfqd, bfqq);\n\t\t\t \n\t\t\tif (unlikely(!bfqd->nonrot_with_queueing))\n\t\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t\t}\n\t}\n}\n\n \nstatic void bfq_requests_merged(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*next_bfqq = RQ_BFQQ(next);\n\n\tif (!bfqq)\n\t\tgoto remove;\n\n\t \n\tif (bfqq == next_bfqq &&\n\t    !list_empty(&rq->queuelist) && !list_empty(&next->queuelist) &&\n\t    next->fifo_time < rq->fifo_time) {\n\t\tlist_del_init(&rq->queuelist);\n\t\tlist_replace_init(&next->queuelist, &rq->queuelist);\n\t\trq->fifo_time = next->fifo_time;\n\t}\n\n\tif (bfqq->next_rq == next)\n\t\tbfqq->next_rq = rq;\n\n\tbfqg_stats_update_io_merged(bfqq_group(bfqq), next->cmd_flags);\nremove:\n\t \n\tif (!RB_EMPTY_NODE(&next->rb_node)) {\n\t\tbfq_remove_request(next->q, next);\n\t\tif (next_bfqq)\n\t\t\tbfqg_stats_update_io_remove(bfqq_group(next_bfqq),\n\t\t\t\t\t\t    next->cmd_flags);\n\t}\n}\n\n \nstatic void bfq_bfqq_end_wr(struct bfq_queue *bfqq)\n{\n\t \n\n\tif (bfqq->wr_cur_max_time !=\n\t    bfqq->bfqd->bfq_wr_rt_max_time)\n\t\tbfqq->soft_rt_next_start = jiffies;\n\n\tif (bfq_bfqq_busy(bfqq))\n\t\tbfqq->bfqd->wr_busy_queues--;\n\tbfqq->wr_coeff = 1;\n\tbfqq->wr_cur_max_time = 0;\n\tbfqq->last_wr_start_finish = jiffies;\n\t \n\tbfqq->entity.prio_changed = 1;\n}\n\nvoid bfq_end_wr_async_queues(struct bfq_data *bfqd,\n\t\t\t     struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\tif (bfqg->async_bfqq[i][j][k])\n\t\t\t\t\tbfq_bfqq_end_wr(bfqg->async_bfqq[i][j][k]);\n\t\tif (bfqg->async_idle_bfqq[k])\n\t\t\tbfq_bfqq_end_wr(bfqg->async_idle_bfqq[k]);\n\t}\n}\n\nstatic void bfq_end_wr(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq;\n\tint i;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t}\n\tlist_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list)\n\t\tbfq_bfqq_end_wr(bfqq);\n\tbfq_end_wr_async(bfqd);\n\n\tspin_unlock_irq(&bfqd->lock);\n}\n\nstatic sector_t bfq_io_struct_pos(void *io_struct, bool request)\n{\n\tif (request)\n\t\treturn blk_rq_pos(io_struct);\n\telse\n\t\treturn ((struct bio *)io_struct)->bi_iter.bi_sector;\n}\n\nstatic int bfq_rq_close_to_sector(void *io_struct, bool request,\n\t\t\t\t  sector_t sector)\n{\n\treturn abs(bfq_io_struct_pos(io_struct, request) - sector) <=\n\t       BFQQ_CLOSE_THR;\n}\n\nstatic struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,\n\t\t\t\t\t struct bfq_queue *bfqq,\n\t\t\t\t\t sector_t sector)\n{\n\tstruct rb_root *root = &bfqq_group(bfqq)->rq_pos_tree;\n\tstruct rb_node *parent, *node;\n\tstruct bfq_queue *__bfqq;\n\n\tif (RB_EMPTY_ROOT(root))\n\t\treturn NULL;\n\n\t \n\t__bfqq = bfq_rq_pos_tree_lookup(bfqd, root, sector, &parent, NULL);\n\tif (__bfqq)\n\t\treturn __bfqq;\n\n\t \n\t__bfqq = rb_entry(parent, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\tif (blk_rq_pos(__bfqq->next_rq) < sector)\n\t\tnode = rb_next(&__bfqq->pos_node);\n\telse\n\t\tnode = rb_prev(&__bfqq->pos_node);\n\tif (!node)\n\t\treturn NULL;\n\n\t__bfqq = rb_entry(node, struct bfq_queue, pos_node);\n\tif (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))\n\t\treturn __bfqq;\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_find_close_cooperator(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_queue *cur_bfqq,\n\t\t\t\t\t\t   sector_t sector)\n{\n\tstruct bfq_queue *bfqq;\n\n\t \n\tbfqq = bfqq_find_close(bfqd, cur_bfqq, sector);\n\tif (!bfqq || bfqq == cur_bfqq)\n\t\treturn NULL;\n\n\treturn bfqq;\n}\n\nstatic struct bfq_queue *\nbfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tint process_refs, new_process_refs;\n\tstruct bfq_queue *__bfqq;\n\n\t \n\tif (!bfqq_process_refs(new_bfqq))\n\t\treturn NULL;\n\n\t \n\twhile ((__bfqq = new_bfqq->new_bfqq)) {\n\t\tif (__bfqq == bfqq)\n\t\t\treturn NULL;\n\t\tnew_bfqq = __bfqq;\n\t}\n\n\tprocess_refs = bfqq_process_refs(bfqq);\n\tnew_process_refs = bfqq_process_refs(new_bfqq);\n\t \n\tif (process_refs == 0 || new_process_refs == 0)\n\t\treturn NULL;\n\n\t \n\tif (new_bfqq->entity.parent != bfqq->entity.parent)\n\t\treturn NULL;\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"scheduling merge with queue %d\",\n\t\tnew_bfqq->pid);\n\n\t \n\tbfqq->new_bfqq = new_bfqq;\n\t \n\tnew_bfqq->ref += process_refs;\n\treturn new_bfqq;\n}\n\nstatic bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,\n\t\t\t\t\tstruct bfq_queue *new_bfqq)\n{\n\tif (bfq_too_late_for_merging(new_bfqq))\n\t\treturn false;\n\n\tif (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||\n\t    (bfqq->ioprio_class != new_bfqq->ioprio_class))\n\t\treturn false;\n\n\t \n\tif (BFQQ_SEEKY(bfqq) || BFQQ_SEEKY(new_bfqq))\n\t\treturn false;\n\n\t \n\tif (!bfq_bfqq_sync(bfqq) || !bfq_bfqq_sync(new_bfqq))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq);\n\nstatic struct bfq_queue *\nbfq_setup_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct bfq_queue *stable_merge_bfqq,\n\t\t       struct bfq_iocq_bfqq_data *bfqq_data)\n{\n\tint proc_ref = min(bfqq_process_refs(bfqq),\n\t\t\t   bfqq_process_refs(stable_merge_bfqq));\n\tstruct bfq_queue *new_bfqq = NULL;\n\n\tbfqq_data->stable_merge_bfqq = NULL;\n\tif (idling_boosts_thr_without_issues(bfqd, bfqq) || proc_ref == 0)\n\t\tgoto out;\n\n\t \n\tnew_bfqq = bfq_setup_merge(bfqq, stable_merge_bfqq);\n\n\tif (new_bfqq) {\n\t\tbfqq_data->stably_merged = true;\n\t\tif (new_bfqq->bic) {\n\t\t\tunsigned int new_a_idx = new_bfqq->actuator_idx;\n\t\t\tstruct bfq_iocq_bfqq_data *new_bfqq_data =\n\t\t\t\t&new_bfqq->bic->bfqq_data[new_a_idx];\n\n\t\t\tnew_bfqq_data->stably_merged = true;\n\t\t}\n\t}\n\nout:\n\t \n\tbfq_put_stable_ref(stable_merge_bfqq);\n\n\treturn new_bfqq;\n}\n\n \nstatic struct bfq_queue *\nbfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t     void *io_struct, bool request, struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue *in_service_bfqq, *new_bfqq;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t \n\tif (bfqq->new_bfqq)\n\t\treturn bfqq->new_bfqq;\n\n\t \n\tif (unlikely(!bfqd->nonrot_with_queueing)) {\n\t\t \n\t\tif (bfq_bfqq_sync(bfqq) && bfqq_data->stable_merge_bfqq &&\n\t\t    !bfq_bfqq_just_created(bfqq) &&\n\t\t    time_is_before_jiffies(bfqq->split_time +\n\t\t\t\t\t  msecs_to_jiffies(bfq_late_stable_merging)) &&\n\t\t    time_is_before_jiffies(bfqq->creation_time +\n\t\t\t\t\t   msecs_to_jiffies(bfq_late_stable_merging))) {\n\t\t\tstruct bfq_queue *stable_merge_bfqq =\n\t\t\t\tbfqq_data->stable_merge_bfqq;\n\n\t\t\treturn bfq_setup_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t      stable_merge_bfqq,\n\t\t\t\t\t\t      bfqq_data);\n\t\t}\n\t}\n\n\t \n\tif (likely(bfqd->nonrot_with_queueing))\n\t\treturn NULL;\n\n\t \n\tif (bfq_too_late_for_merging(bfqq))\n\t\treturn NULL;\n\n\tif (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))\n\t\treturn NULL;\n\n\t \n\tif (bfq_tot_busy_queues(bfqd) == 1)\n\t\treturn NULL;\n\n\tin_service_bfqq = bfqd->in_service_queue;\n\n\tif (in_service_bfqq && in_service_bfqq != bfqq &&\n\t    likely(in_service_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_rq_close_to_sector(io_struct, request,\n\t\t\t\t   bfqd->in_serv_last_pos) &&\n\t    bfqq->entity.parent == in_service_bfqq->entity.parent &&\n\t    bfq_may_be_close_cooperator(bfqq, in_service_bfqq)) {\n\t\tnew_bfqq = bfq_setup_merge(bfqq, in_service_bfqq);\n\t\tif (new_bfqq)\n\t\t\treturn new_bfqq;\n\t}\n\t \n\tnew_bfqq = bfq_find_close_cooperator(bfqd, bfqq,\n\t\t\tbfq_io_struct_pos(io_struct, request));\n\n\tif (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&\n\t    bfq_may_be_close_cooperator(bfqq, new_bfqq))\n\t\treturn bfq_setup_merge(bfqq, new_bfqq);\n\n\treturn NULL;\n}\n\nstatic void bfq_bfqq_save_state(struct bfq_queue *bfqq)\n{\n\tstruct bfq_io_cq *bic = bfqq->bic;\n\tunsigned int a_idx = bfqq->actuator_idx;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[a_idx];\n\n\t \n\tif (!bic)\n\t\treturn;\n\n\tbfqq_data->saved_last_serv_time_ns = bfqq->last_serv_time_ns;\n\tbfqq_data->saved_inject_limit =\tbfqq->inject_limit;\n\tbfqq_data->saved_decrease_time_jif = bfqq->decrease_time_jif;\n\n\tbfqq_data->saved_weight = bfqq->entity.orig_weight;\n\tbfqq_data->saved_ttime = bfqq->ttime;\n\tbfqq_data->saved_has_short_ttime =\n\t\tbfq_bfqq_has_short_ttime(bfqq);\n\tbfqq_data->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);\n\tbfqq_data->saved_io_start_time = bfqq->io_start_time;\n\tbfqq_data->saved_tot_idle_time = bfqq->tot_idle_time;\n\tbfqq_data->saved_in_large_burst = bfq_bfqq_in_large_burst(bfqq);\n\tbfqq_data->was_in_burst_list =\n\t\t!hlist_unhashed(&bfqq->burst_list_node);\n\n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     !bfq_bfqq_in_large_burst(bfqq) &&\n\t\t     bfqq->bfqd->low_latency)) {\n\t\t \n\t\tbfqq_data->saved_wr_coeff = bfqq->bfqd->bfq_wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfq_smallest_from_now();\n\t\tbfqq_data->saved_wr_cur_max_time =\n\t\t\tbfq_wr_duration(bfqq->bfqd);\n\t\tbfqq_data->saved_last_wr_start_finish = jiffies;\n\t} else {\n\t\tbfqq_data->saved_wr_coeff = bfqq->wr_coeff;\n\t\tbfqq_data->saved_wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tbfqq_data->saved_service_from_wr =\n\t\t\tbfqq->service_from_wr;\n\t\tbfqq_data->saved_last_wr_start_finish =\n\t\t\tbfqq->last_wr_start_finish;\n\t\tbfqq_data->saved_wr_cur_max_time = bfqq->wr_cur_max_time;\n\t}\n}\n\n\nstatic void\nbfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq, struct bfq_queue *new_bfqq)\n{\n\tif (cur_bfqq->entity.parent &&\n\t    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->entity.parent->last_bfqq_created = new_bfqq;\n\telse if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)\n\t\tcur_bfqq->bfqd->last_bfqq_created = new_bfqq;\n}\n\nvoid bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\t \n\tif (bfq_bfqq_busy(bfqq) && RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq != bfqd->in_service_queue)\n\t\tbfq_del_bfqq_busy(bfqq, false);\n\n\tbfq_reassign_last_bfqq(bfqq, NULL);\n\n\tbfq_put_queue(bfqq);\n}\n\nstatic void\nbfq_merge_bfqqs(struct bfq_data *bfqd, struct bfq_io_cq *bic,\n\t\tstruct bfq_queue *bfqq, struct bfq_queue *new_bfqq)\n{\n\tbfq_log_bfqq(bfqd, bfqq, \"merging with queue %lu\",\n\t\t(unsigned long)new_bfqq->pid);\n\t \n\tbfq_bfqq_save_state(bfqq);\n\tbfq_bfqq_save_state(new_bfqq);\n\tif (bfq_bfqq_IO_bound(bfqq))\n\t\tbfq_mark_bfqq_IO_bound(new_bfqq);\n\tbfq_clear_bfqq_IO_bound(bfqq);\n\n\t \n\tif (bfqq->waker_bfqq && !new_bfqq->waker_bfqq &&\n\t    bfqq->waker_bfqq != new_bfqq) {\n\t\tnew_bfqq->waker_bfqq = bfqq->waker_bfqq;\n\t\tnew_bfqq->tentative_waker_bfqq = NULL;\n\n\t\t \n\t\thlist_add_head(&new_bfqq->woken_list_node,\n\t\t\t       &new_bfqq->waker_bfqq->woken_list);\n\n\t}\n\n\t \n\tif (new_bfqq->wr_coeff == 1 && bfqq->wr_coeff > 1) {\n\t\tnew_bfqq->wr_coeff = bfqq->wr_coeff;\n\t\tnew_bfqq->wr_cur_max_time = bfqq->wr_cur_max_time;\n\t\tnew_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;\n\t\tnew_bfqq->wr_start_at_switch_to_srt =\n\t\t\tbfqq->wr_start_at_switch_to_srt;\n\t\tif (bfq_bfqq_busy(new_bfqq))\n\t\t\tbfqd->wr_busy_queues++;\n\t\tnew_bfqq->entity.prio_changed = 1;\n\t}\n\n\tif (bfqq->wr_coeff > 1) {  \n\t\tbfqq->wr_coeff = 1;\n\t\tbfqq->entity.prio_changed = 1;\n\t\tif (bfq_bfqq_busy(bfqq))\n\t\t\tbfqd->wr_busy_queues--;\n\t}\n\n\tbfq_log_bfqq(bfqd, new_bfqq, \"merge_bfqqs: wr_busy %d\",\n\t\t     bfqd->wr_busy_queues);\n\n\t \n\tbic_set_bfqq(bic, new_bfqq, true, bfqq->actuator_idx);\n\tbfq_mark_bfqq_coop(new_bfqq);\n\t \n\tnew_bfqq->bic = NULL;\n\t \n\tnew_bfqq->pid = -1;\n\tbfqq->bic = NULL;\n\n\tbfq_reassign_last_bfqq(bfqq, new_bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct bio *bio)\n{\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tbool is_sync = op_is_sync(bio->bi_opf);\n\tstruct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;\n\n\t \n\tif (is_sync && !rq_is_sync(rq))\n\t\treturn false;\n\n\t \n\tif (!bfqq)\n\t\treturn false;\n\n\t \n\tnew_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);\n\tif (new_bfqq) {\n\t\t \n\t\tbfq_merge_bfqqs(bfqd, bfqd->bio_bic, bfqq,\n\t\t\t\tnew_bfqq);\n\t\t \n\t\tbfqq = new_bfqq;\n\n\t\t \n\t\tbfqd->bio_bfqq = bfqq;\n\t}\n\n\treturn bfqq == RQ_BFQQ(rq);\n}\n\n \nstatic void bfq_set_budget_timeout(struct bfq_data *bfqd,\n\t\t\t\t   struct bfq_queue *bfqq)\n{\n\tunsigned int timeout_coeff;\n\n\tif (bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time)\n\t\ttimeout_coeff = 1;\n\telse\n\t\ttimeout_coeff = bfqq->entity.weight / bfqq->entity.orig_weight;\n\n\tbfqd->last_budget_start = ktime_get();\n\n\tbfqq->budget_timeout = jiffies +\n\t\tbfqd->bfq_timeout * timeout_coeff;\n}\n\nstatic void __bfq_set_in_service_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq)\n{\n\tif (bfqq) {\n\t\tbfq_clear_bfqq_fifo_expire(bfqq);\n\n\t\tbfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;\n\n\t\tif (time_is_before_jiffies(bfqq->last_wr_start_finish) &&\n\t\t    bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t\t    time_is_before_jiffies(bfqq->budget_timeout)) {\n\t\t\t \n\t\t\tif (time_after(bfqq->budget_timeout,\n\t\t\t\t       bfqq->last_wr_start_finish))\n\t\t\t\tbfqq->last_wr_start_finish +=\n\t\t\t\t\tjiffies - bfqq->budget_timeout;\n\t\t\telse\n\t\t\t\tbfqq->last_wr_start_finish = jiffies;\n\t\t}\n\n\t\tbfq_set_budget_timeout(bfqd, bfqq);\n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t     \"set_in_service_queue, cur-budget = %d\",\n\t\t\t     bfqq->entity.budget);\n\t}\n\n\tbfqd->in_service_queue = bfqq;\n\tbfqd->in_serv_last_pos = 0;\n}\n\n \nstatic struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfq_get_next_queue(bfqd);\n\n\t__bfq_set_in_service_queue(bfqd, bfqq);\n\treturn bfqq;\n}\n\nstatic void bfq_arm_slice_timer(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\tu32 sl;\n\n\tbfq_mark_bfqq_wait_request(bfqq);\n\n\t \n\tsl = bfqd->bfq_slice_idle;\n\t \n\tif (BFQQ_SEEKY(bfqq) && bfqq->wr_coeff == 1 &&\n\t    !bfq_asymmetric_scenario(bfqd, bfqq))\n\t\tsl = min_t(u64, sl, BFQ_MIN_TT);\n\telse if (bfqq->wr_coeff > 1)\n\t\tsl = max_t(u32, sl, 20ULL * NSEC_PER_MSEC);\n\n\tbfqd->last_idling_start = ktime_get();\n\tbfqd->last_idling_start_jiffies = jiffies;\n\n\thrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),\n\t\t      HRTIMER_MODE_REL);\n\tbfqg_stats_set_start_idle_time(bfqq_group(bfqq));\n}\n\n \nstatic unsigned long bfq_calc_max_budget(struct bfq_data *bfqd)\n{\n\treturn (u64)bfqd->peak_rate * USEC_PER_MSEC *\n\t\tjiffies_to_msecs(bfqd->bfq_timeout)>>BFQ_RATE_SHIFT;\n}\n\n \nstatic void update_thr_responsiveness_params(struct bfq_data *bfqd)\n{\n\tif (bfqd->bfq_user_max_budget == 0) {\n\t\tbfqd->bfq_max_budget =\n\t\t\tbfq_calc_max_budget(bfqd);\n\t\tbfq_log(bfqd, \"new max_budget = %d\", bfqd->bfq_max_budget);\n\t}\n}\n\nstatic void bfq_reset_rate_computation(struct bfq_data *bfqd,\n\t\t\t\t       struct request *rq)\n{\n\tif (rq != NULL) {  \n\t\tbfqd->last_dispatch = bfqd->first_dispatch = ktime_get_ns();\n\t\tbfqd->peak_rate_samples = 1;\n\t\tbfqd->sequential_samples = 0;\n\t\tbfqd->tot_sectors_dispatched = bfqd->last_rq_max_size =\n\t\t\tblk_rq_sectors(rq);\n\t} else  \n\t\tbfqd->peak_rate_samples = 0;  \n\n\tbfq_log(bfqd,\n\t\t\"reset_rate_computation at end, sample %u/%u tot_sects %llu\",\n\t\tbfqd->peak_rate_samples, bfqd->sequential_samples,\n\t\tbfqd->tot_sectors_dispatched);\n}\n\nstatic void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)\n{\n\tu32 rate, weight, divisor;\n\n\t \n\tif (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||\n\t    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)\n\t\tgoto reset_computation;\n\n\t \n\tbfqd->delta_from_first =\n\t\tmax_t(u64, bfqd->delta_from_first,\n\t\t      bfqd->last_completion - bfqd->first_dispatch);\n\n\t \n\trate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,\n\t\t\tdiv_u64(bfqd->delta_from_first, NSEC_PER_USEC));\n\n\t \n\tif ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&\n\t     rate <= bfqd->peak_rate) ||\n\t\trate > 20<<BFQ_RATE_SHIFT)\n\t\tgoto reset_computation;\n\n\t \n\tweight = (9 * bfqd->sequential_samples) / bfqd->peak_rate_samples;\n\n\t \n\tweight = min_t(u32, 8,\n\t\t       div_u64(weight * bfqd->delta_from_first,\n\t\t\t       BFQ_RATE_REF_INTERVAL));\n\n\t \n\tdivisor = 10 - weight;\n\n\t \n\tbfqd->peak_rate *= divisor-1;\n\tbfqd->peak_rate /= divisor;\n\trate /= divisor;  \n\n\tbfqd->peak_rate += rate;\n\n\t \n\tbfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);\n\n\tupdate_thr_responsiveness_params(bfqd);\n\nreset_computation:\n\tbfq_reset_rate_computation(bfqd, rq);\n}\n\n \nstatic void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tif (bfqd->peak_rate_samples == 0) {  \n\t\tbfq_log(bfqd, \"update_peak_rate: goto reset, samples %d\",\n\t\t\tbfqd->peak_rate_samples);\n\t\tbfq_reset_rate_computation(bfqd, rq);\n\t\tgoto update_last_values;  \n\t}\n\n\t \n\tif (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&\n\t    bfqd->tot_rq_in_driver == 0)\n\t\tgoto update_rate_and_reset;\n\n\t \n\tbfqd->peak_rate_samples++;\n\n\tif ((bfqd->tot_rq_in_driver > 0 ||\n\t\tnow_ns - bfqd->last_completion < BFQ_MIN_TT)\n\t    && !BFQ_RQ_SEEKY(bfqd, bfqd->last_position, rq))\n\t\tbfqd->sequential_samples++;\n\n\tbfqd->tot_sectors_dispatched += blk_rq_sectors(rq);\n\n\t \n\tif (likely(bfqd->peak_rate_samples % 32))\n\t\tbfqd->last_rq_max_size =\n\t\t\tmax_t(u32, blk_rq_sectors(rq), bfqd->last_rq_max_size);\n\telse\n\t\tbfqd->last_rq_max_size = blk_rq_sectors(rq);\n\n\tbfqd->delta_from_first = now_ns - bfqd->first_dispatch;\n\n\t \n\tif (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)\n\t\tgoto update_last_values;\n\nupdate_rate_and_reset:\n\tbfq_update_rate_reset(bfqd, rq);\nupdate_last_values:\n\tbfqd->last_position = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\tif (RQ_BFQQ(rq) == bfqd->in_service_queue)\n\t\tbfqd->in_serv_last_pos = bfqd->last_position;\n\tbfqd->last_dispatch = now_ns;\n}\n\n \nstatic void bfq_dispatch_remove(struct request_queue *q, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\n\t \n\tbfqq->dispatched++;\n\tbfq_update_peak_rate(q->elevator->elevator_data, rq);\n\n\tbfq_remove_request(q, rq);\n}\n\n \nstatic bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tint tot_busy_queues = bfq_tot_busy_queues(bfqd);\n\n\t \n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\treturn (bfqq->wr_coeff > 1 &&\n\t\t(bfqd->wr_busy_queues < tot_busy_queues ||\n\t\t bfqd->tot_rq_in_driver >= bfqq->dispatched + 4)) ||\n\t\tbfq_asymmetric_scenario(bfqd, bfqq) ||\n\t\ttot_busy_queues == 1;\n}\n\nstatic bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t      enum bfqq_expiration reason)\n{\n\t \n\tif (bfq_bfqq_coop(bfqq) && BFQQ_SEEKY(bfqq))\n\t\tbfq_mark_bfqq_split_coop(bfqq);\n\n\t \n\tif (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    !(reason == BFQQE_PREEMPTED &&\n\t      idling_needed_for_service_guarantees(bfqd, bfqq))) {\n\t\tif (bfqq->dispatched == 0)\n\t\t\t \n\t\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_busy(bfqq, true);\n\t} else {\n\t\tbfq_requeue_bfqq(bfqd, bfqq, true);\n\t\t \n\t\tif (unlikely(!bfqd->nonrot_with_queueing &&\n\t\t\t     !RB_EMPTY_ROOT(&bfqq->sort_list)))\n\t\t\tbfq_pos_tree_add_move(bfqd, bfqq);\n\t}\n\n\t \n\treturn __bfq_bfqd_reset_in_service(bfqd);\n}\n\n \nstatic void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,\n\t\t\t\t     struct bfq_queue *bfqq,\n\t\t\t\t     enum bfqq_expiration reason)\n{\n\tstruct request *next_rq;\n\tint budget, min_budget;\n\n\tmin_budget = bfq_min_budget(bfqd);\n\n\tif (bfqq->wr_coeff == 1)\n\t\tbudget = bfqq->max_budget;\n\telse  \n\t\tbudget = 2 * min_budget;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last budg %d, budg left %d\",\n\t\tbfqq->entity.budget, bfq_bfqq_budget_left(bfqq));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: last max_budg %d, min budg %d\",\n\t\tbudget, bfq_min_budget(bfqd));\n\tbfq_log_bfqq(bfqd, bfqq, \"recalc_budg: sync %d, seeky %d\",\n\t\tbfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));\n\n\tif (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {\n\t\tswitch (reason) {\n\t\t \n\t\tcase BFQQE_TOO_IDLE:\n\t\t\t \n\t\t\tif (bfqq->dispatched > 0)  \n\t\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\telse {\n\t\t\t\tif (budget > 5 * min_budget)\n\t\t\t\t\tbudget -= 4 * min_budget;\n\t\t\t\telse\n\t\t\t\t\tbudget = min_budget;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_TIMEOUT:\n\t\t\t \n\t\t\tbudget = min(budget * 2, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_BUDGET_EXHAUSTED:\n\t\t\t \n\t\t\tbudget = min(budget * 4, bfqd->bfq_max_budget);\n\t\t\tbreak;\n\t\tcase BFQQE_NO_MORE_REQUESTS:\n\t\t\t \n\t\t\tbudget = max_t(int, bfqq->entity.service, min_budget);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t} else if (!bfq_bfqq_sync(bfqq)) {\n\t\t \n\t\tbudget = bfqd->bfq_max_budget;\n\t}\n\n\tbfqq->max_budget = budget;\n\n\tif (bfqd->budgets_assigned >= bfq_stats_min_budgets &&\n\t    !bfqd->bfq_user_max_budget)\n\t\tbfqq->max_budget = min(bfqq->max_budget, bfqd->bfq_max_budget);\n\n\t \n\tnext_rq = bfqq->next_rq;\n\tif (next_rq)\n\t\tbfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,\n\t\t\t\t\t    bfq_serv_to_charge(next_rq, bfqq));\n\n\tbfq_log_bfqq(bfqd, bfqq, \"head sect: %u, new budget %d\",\n\t\t\tnext_rq ? blk_rq_sectors(next_rq) : 0,\n\t\t\tbfqq->entity.budget);\n}\n\n \nstatic bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t\t bool compensate, unsigned long *delta_ms)\n{\n\tktime_t delta_ktime;\n\tu32 delta_usecs;\n\tbool slow = BFQQ_SEEKY(bfqq);  \n\n\tif (!bfq_bfqq_sync(bfqq))\n\t\treturn false;\n\n\tif (compensate)\n\t\tdelta_ktime = bfqd->last_idling_start;\n\telse\n\t\tdelta_ktime = ktime_get();\n\tdelta_ktime = ktime_sub(delta_ktime, bfqd->last_budget_start);\n\tdelta_usecs = ktime_to_us(delta_ktime);\n\n\t \n\tif (delta_usecs < 1000) {\n\t\tif (blk_queue_nonrot(bfqd->queue))\n\t\t\t  \n\t\t\t*delta_ms = BFQ_MIN_TT / NSEC_PER_MSEC;\n\t\telse  \n\t\t\t*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;\n\n\t\treturn slow;\n\t}\n\n\t*delta_ms = delta_usecs / USEC_PER_MSEC;\n\n\t \n\tif (delta_usecs > 20000) {\n\t\t \n\t\tslow = bfqq->entity.service < bfqd->bfq_max_budget / 2;\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"bfq_bfqq_is_slow: slow %d\", slow);\n\n\treturn slow;\n}\n\n \nstatic unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,\n\t\t\t\t\t\tstruct bfq_queue *bfqq)\n{\n\treturn max3(bfqq->soft_rt_next_start,\n\t\t    bfqq->last_idle_bklogged +\n\t\t    HZ * bfqq->service_from_backlogged /\n\t\t    bfqd->bfq_wr_max_softrt_rate,\n\t\t    jiffies + nsecs_to_jiffies(bfqq->bfqd->bfq_slice_idle) + 4);\n}\n\n \nvoid bfq_bfqq_expire(struct bfq_data *bfqd,\n\t\t     struct bfq_queue *bfqq,\n\t\t     bool compensate,\n\t\t     enum bfqq_expiration reason)\n{\n\tbool slow;\n\tunsigned long delta = 0;\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\t \n\tslow = bfq_bfqq_is_slow(bfqd, bfqq, compensate, &delta);\n\n\t \n\tif (bfqq->wr_coeff == 1 &&\n\t    (slow ||\n\t     (reason == BFQQE_BUDGET_TIMEOUT &&\n\t      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))\n\t\tbfq_bfqq_charge_time(bfqd, bfqq, delta);\n\n\tif (bfqd->low_latency && bfqq->wr_coeff == 1)\n\t\tbfqq->last_wr_start_finish = jiffies;\n\n\tif (bfqd->low_latency && bfqd->bfq_wr_max_softrt_rate > 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list)) {\n\t\t \n\t\tif (bfqq->dispatched == 0)\n\t\t\tbfqq->soft_rt_next_start =\n\t\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\t\telse if (bfqq->dispatched > 0) {\n\t\t\t \n\t\t\tbfq_mark_bfqq_softrt_update(bfqq);\n\t\t}\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\"expire (%d, slow %d, num_disp %d, short_ttime %d)\", reason,\n\t\tslow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));\n\n\t \n\tbfqd->rqs_injected = bfqd->wait_dispatch = false;\n\tbfqd->waited_rq = NULL;\n\n\t \n\t__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);\n\tif (__bfq_bfqq_expire(bfqd, bfqq, reason))\n\t\t \n\t\treturn;\n\n\t \n\tif (!bfq_bfqq_busy(bfqq) &&\n\t    reason != BFQQE_BUDGET_TIMEOUT &&\n\t    reason != BFQQE_BUDGET_EXHAUSTED) {\n\t\tbfq_mark_bfqq_non_blocking_wait_rq(bfqq);\n\t\t \n\t} else\n\t\tentity->service = 0;\n\n\t \n\tentity = entity->parent;\n\tfor_each_entity(entity)\n\t\tentity->service = 0;\n}\n\n \nstatic bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)\n{\n\treturn time_is_before_eq_jiffies(bfqq->budget_timeout);\n}\n\n \nstatic bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq,\n\t\t\"may_budget_timeout: wait_request %d left %d timeout %d\",\n\t\tbfq_bfqq_wait_request(bfqq),\n\t\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,\n\t\tbfq_bfqq_budget_timeout(bfqq));\n\n\treturn (!bfq_bfqq_wait_request(bfqq) ||\n\t\tbfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3)\n\t\t&&\n\t\tbfq_bfqq_budget_timeout(bfqq);\n}\n\nstatic bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,\n\t\t\t\t\t     struct bfq_queue *bfqq)\n{\n\tbool rot_without_queueing =\n\t\t!blk_queue_nonrot(bfqd->queue) && !bfqd->hw_tag,\n\t\tbfqq_sequential_and_IO_bound,\n\t\tidling_boosts_thr;\n\n\t \n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tbfqq_sequential_and_IO_bound = !BFQQ_SEEKY(bfqq) &&\n\t\tbfq_bfqq_IO_bound(bfqq) && bfq_bfqq_has_short_ttime(bfqq);\n\n\t \n\tidling_boosts_thr = rot_without_queueing ||\n\t\t((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&\n\t\t bfqq_sequential_and_IO_bound);\n\n\t \n\treturn idling_boosts_thr &&\n\t\tbfqd->wr_busy_queues == 0;\n}\n\n \nstatic bool bfq_better_to_idle(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tbool idling_boosts_thr_with_no_issue, idling_needed_for_service_guar;\n\n\t \n\tif (unlikely(!bfqq_process_refs(bfqq)))\n\t\treturn false;\n\n\tif (unlikely(bfqd->strict_guarantees))\n\t\treturn true;\n\n\t \n\tif (bfqd->bfq_slice_idle == 0 || !bfq_bfqq_sync(bfqq) ||\n\t   bfq_class_idle(bfqq))\n\t\treturn false;\n\n\tidling_boosts_thr_with_no_issue =\n\t\tidling_boosts_thr_without_issues(bfqd, bfqq);\n\n\tidling_needed_for_service_guar =\n\t\tidling_needed_for_service_guarantees(bfqd, bfqq);\n\n\t \n\treturn idling_boosts_thr_with_no_issue ||\n\t\tidling_needed_for_service_guar;\n}\n\n \nstatic bool bfq_bfqq_must_idle(struct bfq_queue *bfqq)\n{\n\treturn RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_better_to_idle(bfqq);\n}\n\n \nstatic struct bfq_queue *\nbfq_choose_bfqq_for_injection(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *in_serv_bfqq = bfqd->in_service_queue;\n\tunsigned int limit = in_serv_bfqq->inject_limit;\n\tint i;\n\n\t \n\tbool in_serv_always_inject = in_serv_bfqq->wr_coeff == 1 ||\n\t\t!bfq_bfqq_has_short_ttime(in_serv_bfqq);\n\n\t \n\tif (limit == 0 && in_serv_bfqq->last_serv_time_ns == 0 &&\n\t    bfq_bfqq_wait_request(in_serv_bfqq) &&\n\t    time_is_before_eq_jiffies(bfqd->last_idling_start_jiffies +\n\t\t\t\t      bfqd->bfq_slice_idle)\n\t\t)\n\t\tlimit = 1;\n\n\tif (bfqd->tot_rq_in_driver >= limit)\n\t\treturn NULL;\n\n\t \n\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\tlist_for_each_entry(bfqq, &bfqd->active_list[i], bfqq_list)\n\t\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t\t(in_serv_always_inject || bfqq->wr_coeff > 1) &&\n\t\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t \n\t\t\tif (blk_queue_nonrot(bfqd->queue) &&\n\t\t\t    blk_rq_sectors(bfqq->next_rq) >=\n\t\t\t    BFQQ_SECT_THR_NONROT &&\n\t\t\t    bfqd->tot_rq_in_driver >= 1)\n\t\t\t\tcontinue;\n\t\t\telse {\n\t\t\t\tbfqd->rqs_injected = true;\n\t\t\t\treturn bfqq;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct bfq_queue *\nbfq_find_active_bfqq_for_actuator(struct bfq_data *bfqd, int idx)\n{\n\tstruct bfq_queue *bfqq;\n\n\tif (bfqd->in_service_queue &&\n\t    bfqd->in_service_queue->actuator_idx == idx)\n\t\treturn bfqd->in_service_queue;\n\n\tlist_for_each_entry(bfqq, &bfqd->active_list[idx], bfqq_list) {\n\t\tif (!RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\tbfq_serv_to_charge(bfqq->next_rq, bfqq) <=\n\t\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct bfq_queue *\nbfq_find_bfqq_for_underused_actuator(struct bfq_data *bfqd)\n{\n\tint i;\n\n\tfor (i = 0 ; i < bfqd->num_actuators; i++) {\n\t\tif (bfqd->rq_in_driver[i] < bfqd->actuator_load_threshold &&\n\t\t    (i == bfqd->num_actuators - 1 ||\n\t\t     bfqd->rq_in_driver[i] < bfqd->rq_in_driver[i+1])) {\n\t\t\tstruct bfq_queue *bfqq =\n\t\t\t\tbfq_find_active_bfqq_for_actuator(bfqd, i);\n\n\t\t\tif (bfqq)\n\t\t\t\treturn bfqq;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n\n \nstatic struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq, *inject_bfqq;\n\tstruct request *next_rq;\n\tenum bfqq_expiration reason = BFQQE_BUDGET_TIMEOUT;\n\n\tbfqq = bfqd->in_service_queue;\n\tif (!bfqq)\n\t\tgoto new_queue;\n\n\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: already in-service queue\");\n\n\t \n\tif (bfq_may_expire_for_budg_timeout(bfqq) &&\n\t    !bfq_bfqq_must_idle(bfqq))\n\t\tgoto expire;\n\ncheck_queue:\n\t \n\tinject_bfqq = bfq_find_bfqq_for_underused_actuator(bfqd);\n\tif (inject_bfqq && inject_bfqq != bfqq)\n\t\treturn inject_bfqq;\n\n\t \n\tnext_rq = bfqq->next_rq;\n\t \n\tif (next_rq) {\n\t\tif (bfq_serv_to_charge(next_rq, bfqq) >\n\t\t\tbfq_bfqq_budget_left(bfqq)) {\n\t\t\t \n\t\t\treason = BFQQE_BUDGET_EXHAUSTED;\n\t\t\tgoto expire;\n\t\t} else {\n\t\t\t \n\t\t\tif (bfq_bfqq_wait_request(bfqq)) {\n\t\t\t\t \n\t\t\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\t\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\t\t\t}\n\t\t\tgoto keep_queue;\n\t\t}\n\t}\n\n\t \n\tif (bfq_bfqq_wait_request(bfqq) ||\n\t    (bfqq->dispatched != 0 && bfq_better_to_idle(bfqq))) {\n\t\tunsigned int act_idx = bfqq->actuator_idx;\n\t\tstruct bfq_queue *async_bfqq = NULL;\n\t\tstruct bfq_queue *blocked_bfqq =\n\t\t\t!hlist_empty(&bfqq->woken_list) ?\n\t\t\tcontainer_of(bfqq->woken_list.first,\n\t\t\t\t     struct bfq_queue,\n\t\t\t\t     woken_list_node)\n\t\t\t: NULL;\n\n\t\tif (bfqq->bic && bfqq->bic->bfqq[0][act_idx] &&\n\t\t    bfq_bfqq_busy(bfqq->bic->bfqq[0][act_idx]) &&\n\t\t    bfqq->bic->bfqq[0][act_idx]->next_rq)\n\t\t\tasync_bfqq = bfqq->bic->bfqq[0][act_idx];\n\t\t \n\t\tif (async_bfqq &&\n\t\t    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&\n\t\t    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=\n\t\t    bfq_bfqq_budget_left(async_bfqq))\n\t\t\tbfqq = async_bfqq;\n\t\telse if (bfqq->waker_bfqq &&\n\t\t\t   bfq_bfqq_busy(bfqq->waker_bfqq) &&\n\t\t\t   bfqq->waker_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,\n\t\t\t\t\t      bfqq->waker_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(bfqq->waker_bfqq)\n\t\t\t)\n\t\t\tbfqq = bfqq->waker_bfqq;\n\t\telse if (blocked_bfqq &&\n\t\t\t   bfq_bfqq_busy(blocked_bfqq) &&\n\t\t\t   blocked_bfqq->next_rq &&\n\t\t\t   bfq_serv_to_charge(blocked_bfqq->next_rq,\n\t\t\t\t\t      blocked_bfqq) <=\n\t\t\t   bfq_bfqq_budget_left(blocked_bfqq)\n\t\t\t)\n\t\t\tbfqq = blocked_bfqq;\n\t\telse if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t\t (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||\n\t\t\t  !bfq_bfqq_has_short_ttime(bfqq)))\n\t\t\tbfqq = bfq_choose_bfqq_for_injection(bfqd);\n\t\telse\n\t\t\tbfqq = NULL;\n\n\t\tgoto keep_queue;\n\t}\n\n\treason = BFQQE_NO_MORE_REQUESTS;\nexpire:\n\tbfq_bfqq_expire(bfqd, bfqq, false, reason);\nnew_queue:\n\tbfqq = bfq_set_in_service_queue(bfqd);\n\tif (bfqq) {\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: checking new queue\");\n\t\tgoto check_queue;\n\t}\nkeep_queue:\n\tif (bfqq)\n\t\tbfq_log_bfqq(bfqd, bfqq, \"select_queue: returned this queue\");\n\telse\n\t\tbfq_log(bfqd, \"select_queue: no queue returned\");\n\n\treturn bfqq;\n}\n\nstatic void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tif (bfqq->wr_coeff > 1) {  \n\t\tbfq_log_bfqq(bfqd, bfqq,\n\t\t\t\"raising period dur %u/%u msec, old coeff %u, w %d(%d)\",\n\t\t\tjiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),\n\t\t\tjiffies_to_msecs(bfqq->wr_cur_max_time),\n\t\t\tbfqq->wr_coeff,\n\t\t\tbfqq->entity.weight, bfqq->entity.orig_weight);\n\n\t\tif (entity->prio_changed)\n\t\t\tbfq_log_bfqq(bfqd, bfqq, \"WARN: pending prio change\");\n\n\t\t \n\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\telse if (time_is_before_jiffies(bfqq->last_wr_start_finish +\n\t\t\t\t\t\tbfqq->wr_cur_max_time)) {\n\t\t\tif (bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time ||\n\t\t\ttime_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t       bfq_wr_duration(bfqd))) {\n\t\t\t\t \n\t\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t\t} else {  \n\t\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\t\tbfqq->entity.prio_changed = 1;\n\t\t\t}\n\t\t}\n\t\tif (bfqq->wr_coeff > 1 &&\n\t\t    bfqq->wr_cur_max_time != bfqd->bfq_wr_rt_max_time &&\n\t\t    bfqq->service_from_wr > max_service_from_wr) {\n\t\t\t \n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t}\n\t}\n\t \n\tif ((entity->weight > entity->orig_weight) != (bfqq->wr_coeff > 1))\n\t\t__bfq_entity_update_weight_prio(bfq_entity_service_tree(entity),\n\t\t\t\t\t\tentity, false);\n}\n\n \nstatic struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,\n\t\t\t\t\t\t struct bfq_queue *bfqq)\n{\n\tstruct request *rq = bfqq->next_rq;\n\tunsigned long service_to_charge;\n\n\tservice_to_charge = bfq_serv_to_charge(rq, bfqq);\n\n\tbfq_bfqq_served(bfqq, service_to_charge);\n\n\tif (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {\n\t\tbfqd->wait_dispatch = false;\n\t\tbfqd->waited_rq = rq;\n\t}\n\n\tbfq_dispatch_remove(bfqd->queue, rq);\n\n\tif (bfqq != bfqd->in_service_queue)\n\t\treturn rq;\n\n\t \n\tbfq_update_wr_data(bfqd, bfqq);\n\n\t \n\tif (bfq_tot_busy_queues(bfqd) > 1 && bfq_class_idle(bfqq))\n\t\tbfq_bfqq_expire(bfqd, bfqq, false, BFQQE_BUDGET_EXHAUSTED);\n\n\treturn rq;\n}\n\nstatic bool bfq_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\n\t \n\treturn !list_empty_careful(&bfqd->dispatch) ||\n\t\tREAD_ONCE(bfqd->queued);\n}\n\nstatic struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq = NULL;\n\tstruct bfq_queue *bfqq = NULL;\n\n\tif (!list_empty(&bfqd->dispatch)) {\n\t\trq = list_first_entry(&bfqd->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (bfqq) {\n\t\t\t \n\t\t\tbfqq->dispatched++;\n\n\t\t\tgoto inc_in_driver_start_rq;\n\t\t}\n\n\t\t \n\t\tgoto start_rq;\n\t}\n\n\tbfq_log(bfqd, \"dispatch requests: %d busy queues\",\n\t\tbfq_tot_busy_queues(bfqd));\n\n\tif (bfq_tot_busy_queues(bfqd) == 0)\n\t\tgoto exit;\n\n\t \n\tif (bfqd->strict_guarantees && bfqd->tot_rq_in_driver > 0)\n\t\tgoto exit;\n\n\tbfqq = bfq_select_queue(bfqd);\n\tif (!bfqq)\n\t\tgoto exit;\n\n\trq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);\n\n\tif (rq) {\ninc_in_driver_start_rq:\n\t\tbfqd->rq_in_driver[bfqq->actuator_idx]++;\n\t\tbfqd->tot_rq_in_driver++;\nstart_rq:\n\t\trq->rq_flags |= RQF_STARTED;\n\t}\nexit:\n\treturn rq;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t      struct request *rq,\n\t\t\t\t      struct bfq_queue *in_serv_queue,\n\t\t\t\t      bool idle_timer_disabled)\n{\n\tstruct bfq_queue *bfqq = rq ? RQ_BFQQ(rq) : NULL;\n\n\tif (!idle_timer_disabled && !bfqq)\n\t\treturn;\n\n\t \n\tspin_lock_irq(&q->queue_lock);\n\tif (idle_timer_disabled)\n\t\t \n\t\tbfqg_stats_update_idle_time(bfqq_group(in_serv_queue));\n\tif (bfqq) {\n\t\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\t\tbfqg_stats_update_avg_queue_size(bfqg);\n\t\tbfqg_stats_set_start_empty_time(bfqg);\n\t\tbfqg_stats_update_io_remove(bfqg, rq->cmd_flags);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_dispatch_stats(struct request_queue *q,\n\t\t\t\t\t     struct request *rq,\n\t\t\t\t\t     struct bfq_queue *in_serv_queue,\n\t\t\t\t\t     bool idle_timer_disabled) {}\n#endif  \n\nstatic struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct request *rq;\n\tstruct bfq_queue *in_serv_queue;\n\tbool waiting_rq, idle_timer_disabled = false;\n\n\tspin_lock_irq(&bfqd->lock);\n\n\tin_serv_queue = bfqd->in_service_queue;\n\twaiting_rq = in_serv_queue && bfq_bfqq_wait_request(in_serv_queue);\n\n\trq = __bfq_dispatch_request(hctx);\n\tif (in_serv_queue == bfqd->in_service_queue) {\n\t\tidle_timer_disabled =\n\t\t\twaiting_rq && !bfq_bfqq_wait_request(in_serv_queue);\n\t}\n\n\tspin_unlock_irq(&bfqd->lock);\n\tbfq_update_dispatch_stats(hctx->queue, rq,\n\t\t\tidle_timer_disabled ? in_serv_queue : NULL,\n\t\t\t\tidle_timer_disabled);\n\n\treturn rq;\n}\n\n \nvoid bfq_put_queue(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *item;\n\tstruct hlist_node *n;\n\tstruct bfq_group *bfqg = bfqq_group(bfqq);\n\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"put_queue: %p %d\", bfqq, bfqq->ref);\n\n\tbfqq->ref--;\n\tif (bfqq->ref)\n\t\treturn;\n\n\tif (!hlist_unhashed(&bfqq->burst_list_node)) {\n\t\thlist_del_init(&bfqq->burst_list_node);\n\t\t \n\t\tif (bfqq->bic && bfqq->bfqd->burst_size > 0)\n\t\t\tbfqq->bfqd->burst_size--;\n\t}\n\n\t \n\t \n\tif (!hlist_unhashed(&bfqq->woken_list_node))\n\t\thlist_del_init(&bfqq->woken_list_node);\n\n\t \n\thlist_for_each_entry_safe(item, n, &bfqq->woken_list,\n\t\t\t\t  woken_list_node) {\n\t\titem->waker_bfqq = NULL;\n\t\thlist_del_init(&item->woken_list_node);\n\t}\n\n\tif (bfqq->bfqd->last_completed_rq_bfqq == bfqq)\n\t\tbfqq->bfqd->last_completed_rq_bfqq = NULL;\n\n\tWARN_ON_ONCE(!list_empty(&bfqq->fifo));\n\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&bfqq->sort_list));\n\tWARN_ON_ONCE(bfqq->dispatched);\n\n\tkmem_cache_free(bfq_pool, bfqq);\n\tbfqg_and_blkg_put(bfqg);\n}\n\nstatic void bfq_put_stable_ref(struct bfq_queue *bfqq)\n{\n\tbfqq->stable_ref--;\n\tbfq_put_queue(bfqq);\n}\n\nvoid bfq_put_cooperator(struct bfq_queue *bfqq)\n{\n\tstruct bfq_queue *__bfqq, *next;\n\n\t \n\t__bfqq = bfqq->new_bfqq;\n\twhile (__bfqq) {\n\t\tnext = __bfqq->new_bfqq;\n\t\tbfq_put_queue(__bfqq);\n\t\t__bfqq = next;\n\t}\n}\n\nstatic void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tif (bfqq == bfqd->in_service_queue) {\n\t\t__bfq_bfqq_expire(bfqd, bfqq, BFQQE_BUDGET_TIMEOUT);\n\t\tbfq_schedule_dispatch(bfqd);\n\t}\n\n\tbfq_log_bfqq(bfqd, bfqq, \"exit_bfqq: %p, %d\", bfqq, bfqq->ref);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqd, bfqq);\n}\n\nstatic void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync,\n\t\t\t      unsigned int actuator_idx)\n{\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, actuator_idx);\n\tstruct bfq_data *bfqd;\n\n\tif (bfqq)\n\t\tbfqd = bfqq->bfqd;  \n\n\tif (bfqq && bfqd) {\n\t\tbic_set_bfqq(bic, NULL, is_sync, actuator_idx);\n\t\tbfq_exit_bfqq(bfqd, bfqq);\n\t}\n}\n\nstatic void bfq_exit_icq(struct io_cq *icq)\n{\n\tstruct bfq_io_cq *bic = icq_to_bic(icq);\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tunsigned long flags;\n\tunsigned int act_idx;\n\t \n\tunsigned int num_actuators = BFQ_MAX_ACTUATORS;\n\tstruct bfq_iocq_bfqq_data *bfqq_data = bic->bfqq_data;\n\n\t \n\tif (bfqd) {\n\t\tspin_lock_irqsave(&bfqd->lock, flags);\n\t\tnum_actuators = bfqd->num_actuators;\n\t}\n\n\tfor (act_idx = 0; act_idx < num_actuators; act_idx++) {\n\t\tif (bfqq_data[act_idx].stable_merge_bfqq)\n\t\t\tbfq_put_stable_ref(bfqq_data[act_idx].stable_merge_bfqq);\n\n\t\tbfq_exit_icq_bfqq(bic, true, act_idx);\n\t\tbfq_exit_icq_bfqq(bic, false, act_idx);\n\t}\n\n\tif (bfqd)\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n \nstatic void\nbfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)\n{\n\tstruct task_struct *tsk = current;\n\tint ioprio_class;\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\n\tif (!bfqd)\n\t\treturn;\n\n\tioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tswitch (ioprio_class) {\n\tdefault:\n\t\tpr_err(\"bdi %s: bfq: bad prio class %d\\n\",\n\t\t\tbdi_dev_name(bfqq->bfqd->queue->disk->bdi),\n\t\t\tioprio_class);\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_NONE:\n\t\t \n\t\tbfqq->new_ioprio = task_nice_ioprio(tsk);\n\t\tbfqq->new_ioprio_class = task_nice_ioclass(tsk);\n\t\tbreak;\n\tcase IOPRIO_CLASS_RT:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_RT;\n\t\tbreak;\n\tcase IOPRIO_CLASS_BE:\n\t\tbfqq->new_ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_BE;\n\t\tbreak;\n\tcase IOPRIO_CLASS_IDLE:\n\t\tbfqq->new_ioprio_class = IOPRIO_CLASS_IDLE;\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t\tbreak;\n\t}\n\n\tif (bfqq->new_ioprio >= IOPRIO_NR_LEVELS) {\n\t\tpr_crit(\"bfq_set_next_ioprio_data: new_ioprio %d\\n\",\n\t\t\tbfqq->new_ioprio);\n\t\tbfqq->new_ioprio = IOPRIO_NR_LEVELS - 1;\n\t}\n\n\tbfqq->entity.new_weight = bfq_ioprio_to_weight(bfqq->new_ioprio);\n\tbfq_log_bfqq(bfqd, bfqq, \"new_ioprio %d new_weight %d\",\n\t\t     bfqq->new_ioprio, bfqq->entity.new_weight);\n\tbfqq->entity.prio_changed = 1;\n}\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn);\n\nstatic void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)\n{\n\tstruct bfq_data *bfqd = bic_to_bfqd(bic);\n\tstruct bfq_queue *bfqq;\n\tint ioprio = bic->icq.ioc->ioprio;\n\n\t \n\tif (unlikely(!bfqd) || likely(bic->ioprio == ioprio))\n\t\treturn;\n\n\tbic->ioprio = ioprio;\n\n\tbfqq = bic_to_bfqq(bic, false, bfq_actuator_index(bfqd, bio));\n\tif (bfqq) {\n\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\tbfqq = bfq_get_queue(bfqd, bio, false, bic, true);\n\t\tbic_set_bfqq(bic, bfqq, false, bfq_actuator_index(bfqd, bio));\n\t\tbfq_release_process_ref(bfqd, old_bfqq);\n\t}\n\n\tbfqq = bic_to_bfqq(bic, true, bfq_actuator_index(bfqd, bio));\n\tif (bfqq)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n}\n\nstatic void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic, pid_t pid, int is_sync,\n\t\t\t  unsigned int act_idx)\n{\n\tu64 now_ns = ktime_get_ns();\n\n\tbfqq->actuator_idx = act_idx;\n\tRB_CLEAR_NODE(&bfqq->entity.rb_node);\n\tINIT_LIST_HEAD(&bfqq->fifo);\n\tINIT_HLIST_NODE(&bfqq->burst_list_node);\n\tINIT_HLIST_NODE(&bfqq->woken_list_node);\n\tINIT_HLIST_HEAD(&bfqq->woken_list);\n\n\tbfqq->ref = 0;\n\tbfqq->bfqd = bfqd;\n\n\tif (bic)\n\t\tbfq_set_next_ioprio_data(bfqq, bic);\n\n\tif (is_sync) {\n\t\t \n\t\tif (!bfq_class_idle(bfqq))\n\t\t\t \n\t\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\t\tbfq_mark_bfqq_sync(bfqq);\n\t\tbfq_mark_bfqq_just_created(bfqq);\n\t} else\n\t\tbfq_clear_bfqq_sync(bfqq);\n\n\t \n\tbfqq->ttime.last_end_request = now_ns + 1;\n\n\tbfqq->creation_time = jiffies;\n\n\tbfqq->io_start_time = now_ns;\n\n\tbfq_mark_bfqq_IO_bound(bfqq);\n\n\tbfqq->pid = pid;\n\n\t \n\tbfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;\n\tbfqq->budget_timeout = bfq_smallest_from_now();\n\n\tbfqq->wr_coeff = 1;\n\tbfqq->last_wr_start_finish = jiffies;\n\tbfqq->wr_start_at_switch_to_srt = bfq_smallest_from_now();\n\tbfqq->split_time = bfq_smallest_from_now();\n\n\t \n\tbfqq->soft_rt_next_start = jiffies;\n\n\t \n\tbfqq->seek_history = 1;\n\n\tbfqq->decrease_time_jif = jiffies;\n}\n\nstatic struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,\n\t\t\t\t\t       struct bfq_group *bfqg,\n\t\t\t\t\t       int ioprio_class, int ioprio, int act_idx)\n{\n\tswitch (ioprio_class) {\n\tcase IOPRIO_CLASS_RT:\n\t\treturn &bfqg->async_bfqq[0][ioprio][act_idx];\n\tcase IOPRIO_CLASS_NONE:\n\t\tioprio = IOPRIO_BE_NORM;\n\t\tfallthrough;\n\tcase IOPRIO_CLASS_BE:\n\t\treturn &bfqg->async_bfqq[1][ioprio][act_idx];\n\tcase IOPRIO_CLASS_IDLE:\n\t\treturn &bfqg->async_idle_bfqq[act_idx];\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic struct bfq_queue *\nbfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t  struct bfq_io_cq *bic,\n\t\t\t  struct bfq_queue *last_bfqq_created)\n{\n\tunsigned int a_idx = last_bfqq_created->actuator_idx;\n\tstruct bfq_queue *new_bfqq =\n\t\tbfq_setup_merge(bfqq, last_bfqq_created);\n\n\tif (!new_bfqq)\n\t\treturn bfqq;\n\n\tif (new_bfqq->bic)\n\t\tnew_bfqq->bic->bfqq_data[a_idx].stably_merged = true;\n\tbic->bfqq_data[a_idx].stably_merged = true;\n\n\t \n\tbfqq->bic = bic;\n\tbfq_merge_bfqqs(bfqd, bic, bfqq, new_bfqq);\n\n\treturn new_bfqq;\n}\n\n \nstatic struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,\n\t\t\t\t\t\t      struct bfq_queue *bfqq,\n\t\t\t\t\t\t      struct bfq_io_cq *bic)\n{\n\tstruct bfq_queue **source_bfqq = bfqq->entity.parent ?\n\t\t&bfqq->entity.parent->last_bfqq_created :\n\t\t&bfqd->last_bfqq_created;\n\n\tstruct bfq_queue *last_bfqq_created = *source_bfqq;\n\n\t \n\tif (!last_bfqq_created ||\n\t    time_before(last_bfqq_created->creation_time +\n\t\t\tmsecs_to_jiffies(bfq_activation_stable_merging),\n\t\t\tbfqq->creation_time) ||\n\t\tbfqq->entity.parent != last_bfqq_created->entity.parent ||\n\t\tbfqq->ioprio != last_bfqq_created->ioprio ||\n\t\tbfqq->ioprio_class != last_bfqq_created->ioprio_class ||\n\t\tbfqq->actuator_idx != last_bfqq_created->actuator_idx)\n\t\t*source_bfqq = bfqq;\n\telse if (time_after_eq(last_bfqq_created->creation_time +\n\t\t\t\t bfqd->bfq_burst_interval,\n\t\t\t\t bfqq->creation_time)) {\n\t\tif (likely(bfqd->nonrot_with_queueing))\n\t\t\t \n\t\t\tbfqq = bfq_do_early_stable_merge(bfqd, bfqq,\n\t\t\t\t\t\t\t bic,\n\t\t\t\t\t\t\t last_bfqq_created);\n\t\telse {  \n\t\t\t \n\t\t\tlast_bfqq_created->ref++;\n\t\t\t \n\t\t\tlast_bfqq_created->stable_ref++;\n\t\t\t \n\t\t\tbic->bfqq_data[last_bfqq_created->actuator_idx].stable_merge_bfqq =\n\t\t\t\tlast_bfqq_created;\n\t\t}\n\t}\n\n\treturn bfqq;\n}\n\n\nstatic struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,\n\t\t\t\t       struct bio *bio, bool is_sync,\n\t\t\t\t       struct bfq_io_cq *bic,\n\t\t\t\t       bool respawn)\n{\n\tconst int ioprio = IOPRIO_PRIO_LEVEL(bic->ioprio);\n\tconst int ioprio_class = IOPRIO_PRIO_CLASS(bic->ioprio);\n\tstruct bfq_queue **async_bfqq = NULL;\n\tstruct bfq_queue *bfqq;\n\tstruct bfq_group *bfqg;\n\n\tbfqg = bfq_bio_bfqg(bfqd, bio);\n\tif (!is_sync) {\n\t\tasync_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,\n\t\t\t\t\t\t  ioprio,\n\t\t\t\t\t\t  bfq_actuator_index(bfqd, bio));\n\t\tbfqq = *async_bfqq;\n\t\tif (bfqq)\n\t\t\tgoto out;\n\t}\n\n\tbfqq = kmem_cache_alloc_node(bfq_pool,\n\t\t\t\t     GFP_NOWAIT | __GFP_ZERO | __GFP_NOWARN,\n\t\t\t\t     bfqd->queue->node);\n\n\tif (bfqq) {\n\t\tbfq_init_bfqq(bfqd, bfqq, bic, current->pid,\n\t\t\t      is_sync, bfq_actuator_index(bfqd, bio));\n\t\tbfq_init_entity(&bfqq->entity, bfqg);\n\t\tbfq_log_bfqq(bfqd, bfqq, \"allocated\");\n\t} else {\n\t\tbfqq = &bfqd->oom_bfqq;\n\t\tbfq_log_bfqq(bfqd, bfqq, \"using oom bfqq\");\n\t\tgoto out;\n\t}\n\n\t \n\tif (async_bfqq) {\n\t\tbfqq->ref++;  \n\t\tbfq_log_bfqq(bfqd, bfqq, \"get_queue, bfqq not in async: %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\t*async_bfqq = bfqq;\n\t}\n\nout:\n\tbfqq->ref++;  \n\n\tif (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)\n\t\tbfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);\n\treturn bfqq;\n}\n\nstatic void bfq_update_io_thinktime(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tstruct bfq_ttime *ttime = &bfqq->ttime;\n\tu64 elapsed;\n\n\t \n\tif (bfqq->dispatched || bfq_bfqq_busy(bfqq))\n\t\treturn;\n\telapsed = ktime_get_ns() - bfqq->ttime.last_end_request;\n\telapsed = min_t(u64, elapsed, 2ULL * bfqd->bfq_slice_idle);\n\n\tttime->ttime_samples = (7*ttime->ttime_samples + 256) / 8;\n\tttime->ttime_total = div_u64(7*ttime->ttime_total + 256*elapsed,  8);\n\tttime->ttime_mean = div64_ul(ttime->ttime_total + 128,\n\t\t\t\t     ttime->ttime_samples);\n}\n\nstatic void\nbfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t       struct request *rq)\n{\n\tbfqq->seek_history <<= 1;\n\tbfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);\n\n\tif (bfqq->wr_coeff > 1 &&\n\t    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&\n\t    BFQQ_TOTALLY_SEEKY(bfqq)) {\n\t\tif (time_is_before_jiffies(bfqq->wr_start_at_switch_to_srt +\n\t\t\t\t\t   bfq_wr_duration(bfqd))) {\n\t\t\t \n\t\t\tbfq_bfqq_end_wr(bfqq);\n\t\t} else {  \n\t\t\tswitch_back_to_interactive_wr(bfqq, bfqd);\n\t\t\tbfqq->entity.prio_changed = 1;\n\t\t}\n\t}\n}\n\nstatic void bfq_update_has_short_ttime(struct bfq_data *bfqd,\n\t\t\t\t       struct bfq_queue *bfqq,\n\t\t\t\t       struct bfq_io_cq *bic)\n{\n\tbool has_short_ttime = true, state_changed;\n\n\t \n\tif (!bfq_bfqq_sync(bfqq) || bfq_class_idle(bfqq) ||\n\t    bfqd->bfq_slice_idle == 0)\n\t\treturn;\n\n\t \n\tif (time_is_after_eq_jiffies(bfqq->split_time +\n\t\t\t\t     bfqd->bfq_wr_min_idle_time))\n\t\treturn;\n\n\t \n\tif (atomic_read(&bic->icq.ioc->active_ref) == 0 ||\n\t    (bfq_sample_valid(bfqq->ttime.ttime_samples) &&\n\t     bfqq->ttime.ttime_mean > bfqd->bfq_slice_idle>>1))\n\t\thas_short_ttime = false;\n\n\tstate_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);\n\n\tif (has_short_ttime)\n\t\tbfq_mark_bfqq_has_short_ttime(bfqq);\n\telse\n\t\tbfq_clear_bfqq_has_short_ttime(bfqq);\n\n\t \n\tif (state_changed && bfqq->last_serv_time_ns == 0 &&\n\t    (time_is_before_eq_jiffies(bfqq->decrease_time_jif +\n\t\t\t\t      msecs_to_jiffies(100)) ||\n\t     !has_short_ttime))\n\t\tbfq_reset_inject_limit(bfqd, bfqq);\n}\n\n \nstatic void bfq_rq_enqueued(struct bfq_data *bfqd, struct bfq_queue *bfqq,\n\t\t\t    struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_META)\n\t\tbfqq->meta_pending++;\n\n\tbfqq->last_request_pos = blk_rq_pos(rq) + blk_rq_sectors(rq);\n\n\tif (bfqq == bfqd->in_service_queue && bfq_bfqq_wait_request(bfqq)) {\n\t\tbool small_req = bfqq->queued[rq_is_sync(rq)] == 1 &&\n\t\t\t\t blk_rq_sectors(rq) < 32;\n\t\tbool budget_timeout = bfq_bfqq_budget_timeout(bfqq);\n\n\t\t \n\t\tif (small_req && idling_boosts_thr_without_issues(bfqd, bfqq) &&\n\t\t    !budget_timeout)\n\t\t\treturn;\n\n\t\t \n\t\tbfq_clear_bfqq_wait_request(bfqq);\n\t\thrtimer_try_to_cancel(&bfqd->idle_slice_timer);\n\n\t\t \n\t\tif (budget_timeout)\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t}\n}\n\nstatic void bfqq_request_allocated(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated++;\n}\n\nstatic void bfqq_request_freed(struct bfq_queue *bfqq)\n{\n\tstruct bfq_entity *entity = &bfqq->entity;\n\n\tfor_each_entity(entity)\n\t\tentity->allocated--;\n}\n\n \nstatic bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq),\n\t\t*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,\n\t\t\t\t\t\t RQ_BIC(rq));\n\tbool waiting, idle_timer_disabled = false;\n\n\tif (new_bfqq) {\n\t\t \n\t\tbfqq_request_allocated(new_bfqq);\n\t\tbfqq_request_freed(bfqq);\n\t\tnew_bfqq->ref++;\n\t\t \n\t\tif (bic_to_bfqq(RQ_BIC(rq), true,\n\t\t\t\tbfq_actuator_index(bfqd, rq->bio)) == bfqq)\n\t\t\tbfq_merge_bfqqs(bfqd, RQ_BIC(rq),\n\t\t\t\t\tbfqq, new_bfqq);\n\n\t\tbfq_clear_bfqq_just_created(bfqq);\n\t\t \n\t\tbfq_put_queue(bfqq);\n\t\trq->elv.priv[1] = new_bfqq;\n\t\tbfqq = new_bfqq;\n\t}\n\n\tbfq_update_io_thinktime(bfqd, bfqq);\n\tbfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));\n\tbfq_update_io_seektime(bfqd, bfqq, rq);\n\n\twaiting = bfqq && bfq_bfqq_wait_request(bfqq);\n\tbfq_add_request(rq);\n\tidle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);\n\n\trq->fifo_time = ktime_get_ns() + bfqd->bfq_fifo_expire[rq_is_sync(rq)];\n\tlist_add_tail(&rq->queuelist, &bfqq->fifo);\n\n\tbfq_rq_enqueued(bfqd, bfqq, rq);\n\n\treturn idle_timer_disabled;\n}\n\n#ifdef CONFIG_BFQ_CGROUP_DEBUG\nstatic void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t    struct bfq_queue *bfqq,\n\t\t\t\t    bool idle_timer_disabled,\n\t\t\t\t    blk_opf_t cmd_flags)\n{\n\tif (!bfqq)\n\t\treturn;\n\n\t \n\tspin_lock_irq(&q->queue_lock);\n\tbfqg_stats_update_io_add(bfqq_group(bfqq), bfqq, cmd_flags);\n\tif (idle_timer_disabled)\n\t\tbfqg_stats_update_idle_time(bfqq_group(bfqq));\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else\nstatic inline void bfq_update_insert_stats(struct request_queue *q,\n\t\t\t\t\t   struct bfq_queue *bfqq,\n\t\t\t\t\t   bool idle_timer_disabled,\n\t\t\t\t\t   blk_opf_t cmd_flags) {}\n#endif  \n\nstatic struct bfq_queue *bfq_init_rq(struct request *rq);\n\nstatic void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t       blk_insert_t flags)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_queue *bfqq;\n\tbool idle_timer_disabled = false;\n\tblk_opf_t cmd_flags;\n\tLIST_HEAD(free);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)\n\t\tbfqg_stats_update_legacy_io(q, rq);\n#endif\n\tspin_lock_irq(&bfqd->lock);\n\tbfqq = bfq_init_rq(rq);\n\tif (blk_mq_sched_try_insert_merge(q, rq, &free)) {\n\t\tspin_unlock_irq(&bfqd->lock);\n\t\tblk_mq_free_requests(&free);\n\t\treturn;\n\t}\n\n\ttrace_block_rq_insert(rq);\n\n\tif (flags & BLK_MQ_INSERT_AT_HEAD) {\n\t\tlist_add(&rq->queuelist, &bfqd->dispatch);\n\t} else if (!bfqq) {\n\t\tlist_add_tail(&rq->queuelist, &bfqd->dispatch);\n\t} else {\n\t\tidle_timer_disabled = __bfq_insert_request(bfqd, rq);\n\t\t \n\t\tbfqq = RQ_BFQQ(rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\t}\n\n\t \n\tcmd_flags = rq->cmd_flags;\n\tspin_unlock_irq(&bfqd->lock);\n\n\tbfq_update_insert_stats(q, bfqq, idle_timer_disabled,\n\t\t\t\tcmd_flags);\n}\n\nstatic void bfq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\tstruct list_head *list,\n\t\t\t\tblk_insert_t flags)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tbfq_insert_request(hctx, rq, flags);\n\t}\n}\n\nstatic void bfq_update_hw_tag(struct bfq_data *bfqd)\n{\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\tbfqd->max_rq_in_driver = max_t(int, bfqd->max_rq_in_driver,\n\t\t\t\t       bfqd->tot_rq_in_driver);\n\n\tif (bfqd->hw_tag == 1)\n\t\treturn;\n\n\t \n\tif (bfqd->tot_rq_in_driver + bfqd->queued <= BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\t \n\tif (bfqq && bfq_bfqq_has_short_ttime(bfqq) &&\n\t    bfqq->dispatched + bfqq->queued[0] + bfqq->queued[1] <\n\t    BFQ_HW_QUEUE_THRESHOLD &&\n\t    bfqd->tot_rq_in_driver < BFQ_HW_QUEUE_THRESHOLD)\n\t\treturn;\n\n\tif (bfqd->hw_tag_samples++ < BFQ_HW_QUEUE_SAMPLES)\n\t\treturn;\n\n\tbfqd->hw_tag = bfqd->max_rq_in_driver > BFQ_HW_QUEUE_THRESHOLD;\n\tbfqd->max_rq_in_driver = 0;\n\tbfqd->hw_tag_samples = 0;\n\n\tbfqd->nonrot_with_queueing =\n\t\tblk_queue_nonrot(bfqd->queue) && bfqd->hw_tag;\n}\n\nstatic void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)\n{\n\tu64 now_ns;\n\tu32 delta_us;\n\n\tbfq_update_hw_tag(bfqd);\n\n\tbfqd->rq_in_driver[bfqq->actuator_idx]--;\n\tbfqd->tot_rq_in_driver--;\n\tbfqq->dispatched--;\n\n\tif (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {\n\t\t \n\t\tbfqq->budget_timeout = jiffies;\n\n\t\tbfq_del_bfqq_in_groups_with_pending_reqs(bfqq);\n\t\tbfq_weights_tree_remove(bfqq);\n\t}\n\n\tnow_ns = ktime_get_ns();\n\n\tbfqq->ttime.last_end_request = now_ns;\n\n\t \n\tdelta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);\n\n\t \n\tif (delta_us > BFQ_MIN_TT/NSEC_PER_USEC &&\n\t   (bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us <\n\t\t\t1UL<<(BFQ_RATE_SHIFT - 10))\n\t\tbfq_update_rate_reset(bfqd, NULL);\n\tbfqd->last_completion = now_ns;\n\t \n\tif (!bfq_bfqq_coop(bfqq))\n\t\tbfqd->last_completed_rq_bfqq = bfqq;\n\telse\n\t\tbfqd->last_completed_rq_bfqq = NULL;\n\n\t \n\tif (bfq_bfqq_softrt_update(bfqq) && bfqq->dispatched == 0 &&\n\t    RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t    bfqq->wr_coeff != bfqd->bfq_wr_coeff)\n\t\tbfqq->soft_rt_next_start =\n\t\t\tbfq_bfqq_softrt_next_start(bfqd, bfqq);\n\n\t \n\tif (bfqd->in_service_queue == bfqq) {\n\t\tif (bfq_bfqq_must_idle(bfqq)) {\n\t\t\tif (bfqq->dispatched == 0)\n\t\t\t\tbfq_arm_slice_timer(bfqd);\n\t\t\t \n\t\t\treturn;\n\t\t} else if (bfq_may_expire_for_budg_timeout(bfqq))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_BUDGET_TIMEOUT);\n\t\telse if (RB_EMPTY_ROOT(&bfqq->sort_list) &&\n\t\t\t (bfqq->dispatched == 0 ||\n\t\t\t  !bfq_better_to_idle(bfqq)))\n\t\t\tbfq_bfqq_expire(bfqd, bfqq, false,\n\t\t\t\t\tBFQQE_NO_MORE_REQUESTS);\n\t}\n\n\tif (!bfqd->tot_rq_in_driver)\n\t\tbfq_schedule_dispatch(bfqd);\n}\n\n \nstatic void bfq_update_inject_limit(struct bfq_data *bfqd,\n\t\t\t\t    struct bfq_queue *bfqq)\n{\n\tu64 tot_time_ns = ktime_get_ns() - bfqd->last_empty_occupied_ns;\n\tunsigned int old_limit = bfqq->inject_limit;\n\n\tif (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {\n\t\tu64 threshold = (bfqq->last_serv_time_ns * 3)>>1;\n\n\t\tif (tot_time_ns >= threshold && old_limit > 0) {\n\t\t\tbfqq->inject_limit--;\n\t\t\tbfqq->decrease_time_jif = jiffies;\n\t\t} else if (tot_time_ns < threshold &&\n\t\t\t   old_limit <= bfqd->max_rq_in_driver)\n\t\t\tbfqq->inject_limit++;\n\t}\n\n\t \n\tif ((bfqq->last_serv_time_ns == 0 && bfqd->tot_rq_in_driver == 1) ||\n\t    tot_time_ns < bfqq->last_serv_time_ns) {\n\t\tif (bfqq->last_serv_time_ns == 0) {\n\t\t\t \n\t\t\tbfqq->inject_limit = max_t(unsigned int, 1, old_limit);\n\t\t}\n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\t} else if (!bfqd->rqs_injected && bfqd->tot_rq_in_driver == 1)\n\t\t \n\t\tbfqq->last_serv_time_ns = tot_time_ns;\n\n\n\t \n\tbfqd->waited_rq = NULL;\n\tbfqd->rqs_injected = false;\n}\n\n \nstatic void bfq_finish_requeue_request(struct request *rq)\n{\n\tstruct bfq_queue *bfqq = RQ_BFQQ(rq);\n\tstruct bfq_data *bfqd;\n\tunsigned long flags;\n\n\t \n\tif (!rq->elv.icq || !bfqq)\n\t\treturn;\n\n\tbfqd = bfqq->bfqd;\n\n\tif (rq->rq_flags & RQF_STARTED)\n\t\tbfqg_stats_update_completion(bfqq_group(bfqq),\n\t\t\t\t\t     rq->start_time_ns,\n\t\t\t\t\t     rq->io_start_time_ns,\n\t\t\t\t\t     rq->cmd_flags);\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tif (likely(rq->rq_flags & RQF_STARTED)) {\n\t\tif (rq == bfqd->waited_rq)\n\t\t\tbfq_update_inject_limit(bfqd, bfqq);\n\n\t\tbfq_completed_request(bfqq, bfqd);\n\t}\n\tbfqq_request_freed(bfqq);\n\tbfq_put_queue(bfqq);\n\tRQ_BIC(rq)->requests--;\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\n\t \n\trq->elv.priv[0] = NULL;\n\trq->elv.priv[1] = NULL;\n}\n\nstatic void bfq_finish_request(struct request *rq)\n{\n\tbfq_finish_requeue_request(rq);\n\n\tif (rq->elv.icq) {\n\t\tput_io_context(rq->elv.icq->ioc);\n\t\trq->elv.icq = NULL;\n\t}\n}\n\n \nstatic struct bfq_queue *\nbfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)\n{\n\tbfq_log_bfqq(bfqq->bfqd, bfqq, \"splitting queue\");\n\n\tif (bfqq_process_refs(bfqq) == 1) {\n\t\tbfqq->pid = current->pid;\n\t\tbfq_clear_bfqq_coop(bfqq);\n\t\tbfq_clear_bfqq_split_coop(bfqq);\n\t\treturn bfqq;\n\t}\n\n\tbic_set_bfqq(bic, NULL, true, bfqq->actuator_idx);\n\n\tbfq_put_cooperator(bfqq);\n\n\tbfq_release_process_ref(bfqq->bfqd, bfqq);\n\treturn NULL;\n}\n\nstatic struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,\n\t\t\t\t\t\t   struct bfq_io_cq *bic,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   bool split, bool is_sync,\n\t\t\t\t\t\t   bool *new_queue)\n{\n\tunsigned int act_idx = bfq_actuator_index(bfqd, bio);\n\tstruct bfq_queue *bfqq = bic_to_bfqq(bic, is_sync, act_idx);\n\tstruct bfq_iocq_bfqq_data *bfqq_data = &bic->bfqq_data[act_idx];\n\n\tif (likely(bfqq && bfqq != &bfqd->oom_bfqq))\n\t\treturn bfqq;\n\n\tif (new_queue)\n\t\t*new_queue = true;\n\n\tif (bfqq)\n\t\tbfq_put_queue(bfqq);\n\tbfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);\n\n\tbic_set_bfqq(bic, bfqq, is_sync, act_idx);\n\tif (split && is_sync) {\n\t\tif ((bfqq_data->was_in_burst_list && bfqd->large_burst) ||\n\t\t    bfqq_data->saved_in_large_burst)\n\t\t\tbfq_mark_bfqq_in_large_burst(bfqq);\n\t\telse {\n\t\t\tbfq_clear_bfqq_in_large_burst(bfqq);\n\t\t\tif (bfqq_data->was_in_burst_list)\n\t\t\t\t \n\t\t\t\thlist_add_head(&bfqq->burst_list_node,\n\t\t\t\t\t       &bfqd->burst_list);\n\t\t}\n\t\tbfqq->split_time = jiffies;\n\t}\n\n\treturn bfqq;\n}\n\n \nstatic void bfq_prepare_request(struct request *rq)\n{\n\trq->elv.icq = ioc_find_get_icq(rq->q);\n\n\t \n\trq->elv.priv[0] = rq->elv.priv[1] = NULL;\n}\n\n \nstatic struct bfq_queue *bfq_init_rq(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct bio *bio = rq->bio;\n\tstruct bfq_data *bfqd = q->elevator->elevator_data;\n\tstruct bfq_io_cq *bic;\n\tconst int is_sync = rq_is_sync(rq);\n\tstruct bfq_queue *bfqq;\n\tbool new_queue = false;\n\tbool bfqq_already_existing = false, split = false;\n\tunsigned int a_idx = bfq_actuator_index(bfqd, bio);\n\n\tif (unlikely(!rq->elv.icq))\n\t\treturn NULL;\n\n\t \n\tif (RQ_BFQQ(rq))\n\t\treturn RQ_BFQQ(rq);\n\n\tbic = icq_to_bic(rq->elv.icq);\n\n\tbfq_check_ioprio_change(bic, bio);\n\n\tbfq_bic_update_cgroup(bic, bio);\n\n\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,\n\t\t\t\t\t &new_queue);\n\n\tif (likely(!new_queue)) {\n\t\t \n\t\tif (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) &&\n\t\t\t!bic->bfqq_data[a_idx].stably_merged) {\n\t\t\tstruct bfq_queue *old_bfqq = bfqq;\n\n\t\t\t \n\t\t\tif (bfq_bfqq_in_large_burst(bfqq))\n\t\t\t\tbic->bfqq_data[a_idx].saved_in_large_burst =\n\t\t\t\t\ttrue;\n\n\t\t\tbfqq = bfq_split_bfqq(bic, bfqq);\n\t\t\tsplit = true;\n\n\t\t\tif (!bfqq) {\n\t\t\t\tbfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio,\n\t\t\t\t\t\t\t\t true, is_sync,\n\t\t\t\t\t\t\t\t NULL);\n\t\t\t\tif (unlikely(bfqq == &bfqd->oom_bfqq))\n\t\t\t\t\tbfqq_already_existing = true;\n\t\t\t} else\n\t\t\t\tbfqq_already_existing = true;\n\n\t\t\tif (!bfqq_already_existing) {\n\t\t\t\tbfqq->waker_bfqq = old_bfqq->waker_bfqq;\n\t\t\t\tbfqq->tentative_waker_bfqq = NULL;\n\n\t\t\t\t \n\t\t\t\tif (bfqq->waker_bfqq)\n\t\t\t\t\thlist_add_head(&bfqq->woken_list_node,\n\t\t\t\t\t\t       &bfqq->waker_bfqq->woken_list);\n\t\t\t}\n\t\t}\n\t}\n\n\tbfqq_request_allocated(bfqq);\n\tbfqq->ref++;\n\tbic->requests++;\n\tbfq_log_bfqq(bfqd, bfqq, \"get_request %p: bfqq %p, %d\",\n\t\t     rq, bfqq, bfqq->ref);\n\n\trq->elv.priv[0] = bic;\n\trq->elv.priv[1] = bfqq;\n\n\t \n\tif (likely(bfqq != &bfqd->oom_bfqq) && bfqq_process_refs(bfqq) == 1) {\n\t\tbfqq->bic = bic;\n\t\tif (split) {\n\t\t\t \n\t\t\tbfq_bfqq_resume_state(bfqq, bfqd, bic,\n\t\t\t\t\t      bfqq_already_existing);\n\t\t}\n\t}\n\n\t \n\tif (unlikely(bfq_bfqq_just_created(bfqq) &&\n\t\t     (bfqd->burst_size > 0 ||\n\t\t      bfq_tot_busy_queues(bfqd) == 0)))\n\t\tbfq_handle_burst(bfqd, bfqq);\n\n\treturn bfqq;\n}\n\nstatic void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t \n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t \n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t \n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tbfq_schedule_dispatch(bfqd);\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n}\n\n \nstatic enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)\n{\n\tstruct bfq_data *bfqd = container_of(timer, struct bfq_data,\n\t\t\t\t\t     idle_slice_timer);\n\tstruct bfq_queue *bfqq = bfqd->in_service_queue;\n\n\t \n\tif (bfqq)\n\t\tbfq_idle_slice_timer_body(bfqd, bfqq);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __bfq_put_async_bfqq(struct bfq_data *bfqd,\n\t\t\t\t struct bfq_queue **bfqq_ptr)\n{\n\tstruct bfq_queue *bfqq = *bfqq_ptr;\n\n\tbfq_log(bfqd, \"put_async_bfqq: %p\", bfqq);\n\tif (bfqq) {\n\t\tbfq_bfqq_move(bfqd, bfqq, bfqd->root_group);\n\n\t\tbfq_log_bfqq(bfqd, bfqq, \"put_async_bfqq: putting %p, %d\",\n\t\t\t     bfqq, bfqq->ref);\n\t\tbfq_put_queue(bfqq);\n\t\t*bfqq_ptr = NULL;\n\t}\n}\n\n \nvoid bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)\n{\n\tint i, j, k;\n\n\tfor (k = 0; k < bfqd->num_actuators; k++) {\n\t\tfor (i = 0; i < 2; i++)\n\t\t\tfor (j = 0; j < IOPRIO_NR_LEVELS; j++)\n\t\t\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_bfqq[i][j][k]);\n\n\t\t__bfq_put_async_bfqq(bfqd, &bfqg->async_idle_bfqq[k]);\n\t}\n}\n\n \nstatic void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)\n{\n\tunsigned int depth = 1U << bt->sb.shift;\n\n\tbfqd->full_depth_shift = bt->sb.shift;\n\t \n\t \n\tbfqd->word_depths[0][0] = max(depth >> 1, 1U);\n\t \n\tbfqd->word_depths[0][1] = max((depth * 3) >> 2, 1U);\n\n\t \n\t \n\tbfqd->word_depths[1][0] = max((depth * 3) >> 4, 1U);\n\t \n\tbfqd->word_depths[1][1] = max((depth * 6) >> 4, 1U);\n}\n\nstatic void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct bfq_data *bfqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\n\tbfq_update_depths(bfqd, &tags->bitmap_tags);\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, 1);\n}\n\nstatic int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)\n{\n\tbfq_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void bfq_exit_queue(struct elevator_queue *e)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tstruct bfq_queue *bfqq, *n;\n\tunsigned int actuator;\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\tspin_lock_irq(&bfqd->lock);\n\tlist_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)\n\t\tbfq_deactivate_bfqq(bfqd, bfqq, false, false);\n\tspin_unlock_irq(&bfqd->lock);\n\n\tfor (actuator = 0; actuator < bfqd->num_actuators; actuator++)\n\t\tWARN_ON_ONCE(bfqd->rq_in_driver[actuator]);\n\tWARN_ON_ONCE(bfqd->tot_rq_in_driver);\n\n\thrtimer_cancel(&bfqd->idle_slice_timer);\n\n\t \n\tbfqg_and_blkg_put(bfqd->root_group);\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_deactivate_policy(bfqd->queue->disk, &blkcg_policy_bfq);\n#else\n\tspin_lock_irq(&bfqd->lock);\n\tbfq_put_async_queues(bfqd, bfqd->root_group);\n\tkfree(bfqd->root_group);\n\tspin_unlock_irq(&bfqd->lock);\n#endif\n\n\tblk_stat_disable_accounting(bfqd->queue);\n\tclear_bit(ELEVATOR_FLAG_DISABLE_WBT, &e->flags);\n\twbt_enable_default(bfqd->queue->disk);\n\n\tkfree(bfqd);\n}\n\nstatic void bfq_init_root_group(struct bfq_group *root_group,\n\t\t\t\tstruct bfq_data *bfqd)\n{\n\tint i;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\troot_group->entity.parent = NULL;\n\troot_group->my_entity = NULL;\n\troot_group->bfqd = bfqd;\n#endif\n\troot_group->rq_pos_tree = RB_ROOT;\n\tfor (i = 0; i < BFQ_IOPRIO_CLASSES; i++)\n\t\troot_group->sched_data.service_tree[i] = BFQ_SERVICE_TREE_INIT;\n\troot_group->sched_data.bfq_class_idle_last_service = jiffies;\n}\n\nstatic int bfq_init_queue(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct bfq_data *bfqd;\n\tstruct elevator_queue *eq;\n\tunsigned int i;\n\tstruct blk_independent_access_ranges *ia_ranges = q->disk->ia_ranges;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tbfqd = kzalloc_node(sizeof(*bfqd), GFP_KERNEL, q->node);\n\tif (!bfqd) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn -ENOMEM;\n\t}\n\teq->elevator_data = bfqd;\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->elevator = eq;\n\tspin_unlock_irq(&q->queue_lock);\n\n\t \n\tbfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0, 0);\n\tbfqd->oom_bfqq.ref++;\n\tbfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;\n\tbfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;\n\tbfqd->oom_bfqq.entity.new_weight =\n\t\tbfq_ioprio_to_weight(bfqd->oom_bfqq.new_ioprio);\n\n\t \n\tbfq_clear_bfqq_just_created(&bfqd->oom_bfqq);\n\n\t \n\tbfqd->oom_bfqq.entity.prio_changed = 1;\n\n\tbfqd->queue = q;\n\n\tbfqd->num_actuators = 1;\n\t \n\tspin_lock_irq(&q->queue_lock);\n\tif (ia_ranges) {\n\t\t \n\t\tif (ia_ranges->nr_ia_ranges > BFQ_MAX_ACTUATORS) {\n\t\t\tpr_crit(\"nr_ia_ranges higher than act limit: iars=%d, max=%d.\\n\",\n\t\t\t\tia_ranges->nr_ia_ranges, BFQ_MAX_ACTUATORS);\n\t\t\tpr_crit(\"Falling back to single actuator mode.\\n\");\n\t\t} else {\n\t\t\tbfqd->num_actuators = ia_ranges->nr_ia_ranges;\n\n\t\t\tfor (i = 0; i < bfqd->num_actuators; i++) {\n\t\t\t\tbfqd->sector[i] = ia_ranges->ia_range[i].sector;\n\t\t\t\tbfqd->nr_sectors[i] =\n\t\t\t\t\tia_ranges->ia_range[i].nr_sectors;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (bfqd->num_actuators == 1) {\n\t\tbfqd->sector[0] = 0;\n\t\tbfqd->nr_sectors[0] = get_capacity(q->disk);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n\n\tINIT_LIST_HEAD(&bfqd->dispatch);\n\n\thrtimer_init(&bfqd->idle_slice_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_REL);\n\tbfqd->idle_slice_timer.function = bfq_idle_slice_timer;\n\n\tbfqd->queue_weights_tree = RB_ROOT_CACHED;\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tbfqd->num_groups_with_pending_reqs = 0;\n#endif\n\n\tINIT_LIST_HEAD(&bfqd->active_list[0]);\n\tINIT_LIST_HEAD(&bfqd->active_list[1]);\n\tINIT_LIST_HEAD(&bfqd->idle_list);\n\tINIT_HLIST_HEAD(&bfqd->burst_list);\n\n\tbfqd->hw_tag = -1;\n\tbfqd->nonrot_with_queueing = blk_queue_nonrot(bfqd->queue);\n\n\tbfqd->bfq_max_budget = bfq_default_max_budget;\n\n\tbfqd->bfq_fifo_expire[0] = bfq_fifo_expire[0];\n\tbfqd->bfq_fifo_expire[1] = bfq_fifo_expire[1];\n\tbfqd->bfq_back_max = bfq_back_max;\n\tbfqd->bfq_back_penalty = bfq_back_penalty;\n\tbfqd->bfq_slice_idle = bfq_slice_idle;\n\tbfqd->bfq_timeout = bfq_timeout;\n\n\tbfqd->bfq_large_burst_thresh = 8;\n\tbfqd->bfq_burst_interval = msecs_to_jiffies(180);\n\n\tbfqd->low_latency = true;\n\n\t \n\tbfqd->bfq_wr_coeff = 30;\n\tbfqd->bfq_wr_rt_max_time = msecs_to_jiffies(300);\n\tbfqd->bfq_wr_min_idle_time = msecs_to_jiffies(2000);\n\tbfqd->bfq_wr_min_inter_arr_async = msecs_to_jiffies(500);\n\tbfqd->bfq_wr_max_softrt_rate = 7000;  \n\tbfqd->wr_busy_queues = 0;\n\n\t \n\tbfqd->rate_dur_prod = ref_rate[blk_queue_nonrot(bfqd->queue)] *\n\t\tref_wr_duration[blk_queue_nonrot(bfqd->queue)];\n\tbfqd->peak_rate = ref_rate[blk_queue_nonrot(bfqd->queue)] * 2 / 3;\n\n\t \n\tbfqd->actuator_load_threshold = 4;\n\n\tspin_lock_init(&bfqd->lock);\n\n\t \n\tbfqd->root_group = bfq_create_group_hierarchy(bfqd, q->node);\n\tif (!bfqd->root_group)\n\t\tgoto out_free;\n\tbfq_init_root_group(bfqd->root_group, bfqd);\n\tbfq_init_entity(&bfqd->oom_bfqq.entity, bfqd->root_group);\n\n\t \n\tblk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);\n\n\tset_bit(ELEVATOR_FLAG_DISABLE_WBT, &eq->flags);\n\twbt_disable_default(q->disk);\n\tblk_stat_enable_accounting(q);\n\n\treturn 0;\n\nout_free:\n\tkfree(bfqd);\n\tkobject_put(&eq->kobj);\n\treturn -ENOMEM;\n}\n\nstatic void bfq_slab_kill(void)\n{\n\tkmem_cache_destroy(bfq_pool);\n}\n\nstatic int __init bfq_slab_setup(void)\n{\n\tbfq_pool = KMEM_CACHE(bfq_queue, 0);\n\tif (!bfq_pool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic ssize_t bfq_var_show(unsigned int var, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", var);\n}\n\nstatic int bfq_var_store(unsigned long *var, const char *page)\n{\n\tunsigned long new_val;\n\tint ret = kstrtoul(page, 10, &new_val);\n\n\tif (ret)\n\t\treturn ret;\n\t*var = new_val;\n\treturn 0;\n}\n\n#define SHOW_FUNCTION(__FUNC, __VAR, __CONV)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t__data = jiffies_to_msecs(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t__data = div_u64(__data, NSEC_PER_MSEC);\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nSHOW_FUNCTION(bfq_fifo_expire_sync_show, bfqd->bfq_fifo_expire[1], 2);\nSHOW_FUNCTION(bfq_fifo_expire_async_show, bfqd->bfq_fifo_expire[0], 2);\nSHOW_FUNCTION(bfq_back_seek_max_show, bfqd->bfq_back_max, 0);\nSHOW_FUNCTION(bfq_back_seek_penalty_show, bfqd->bfq_back_penalty, 0);\nSHOW_FUNCTION(bfq_slice_idle_show, bfqd->bfq_slice_idle, 2);\nSHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);\nSHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);\nSHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);\nSHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);\n#undef SHOW_FUNCTION\n\n#define USEC_SHOW_FUNCTION(__FUNC, __VAR)\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tu64 __data = __VAR;\t\t\t\t\t\t\\\n\t__data = div_u64(__data, NSEC_PER_USEC);\t\t\t\\\n\treturn bfq_var_show(__data, (page));\t\t\t\t\\\n}\nUSEC_SHOW_FUNCTION(bfq_slice_idle_us_show, bfqd->bfq_slice_idle);\n#undef USEC_SHOW_FUNCTION\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\tif (__CONV == 1)\t\t\t\t\t\t\\\n\t\t*(__PTR) = msecs_to_jiffies(__data);\t\t\t\\\n\telse if (__CONV == 2)\t\t\t\t\t\t\\\n\t\t*(__PTR) = (u64)__data * NSEC_PER_MSEC;\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t*(__PTR) = __data;\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nSTORE_FUNCTION(bfq_fifo_expire_sync_store, &bfqd->bfq_fifo_expire[1], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_fifo_expire_async_store, &bfqd->bfq_fifo_expire[0], 1,\n\t\tINT_MAX, 2);\nSTORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);\nSTORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,\n\t\tINT_MAX, 0);\nSTORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);\n#undef STORE_FUNCTION\n\n#define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct bfq_data *bfqd = e->elevator_data;\t\t\t\\\n\tunsigned long __data, __min = (MIN), __max = (MAX);\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = bfq_var_store(&__data, (page));\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\tif (__data < __min)\t\t\t\t\t\t\\\n\t\t__data = __min;\t\t\t\t\t\t\\\n\telse if (__data > __max)\t\t\t\t\t\\\n\t\t__data = __max;\t\t\t\t\t\t\\\n\t*(__PTR) = (u64)__data * NSEC_PER_USEC;\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nUSEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,\n\t\t    UINT_MAX);\n#undef USEC_STORE_FUNCTION\n\nstatic ssize_t bfq_max_budget_store(struct elevator_queue *e,\n\t\t\t\t    const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\telse {\n\t\tif (__data > INT_MAX)\n\t\t\t__data = INT_MAX;\n\t\tbfqd->bfq_max_budget = __data;\n\t}\n\n\tbfqd->bfq_user_max_budget = __data;\n\n\treturn count;\n}\n\n \nstatic ssize_t bfq_timeout_sync_store(struct elevator_queue *e,\n\t\t\t\t      const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data < 1)\n\t\t__data = 1;\n\telse if (__data > INT_MAX)\n\t\t__data = INT_MAX;\n\n\tbfqd->bfq_timeout = msecs_to_jiffies(__data);\n\tif (bfqd->bfq_user_max_budget == 0)\n\t\tbfqd->bfq_max_budget = bfq_calc_max_budget(bfqd);\n\n\treturn count;\n}\n\nstatic ssize_t bfq_strict_guarantees_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (!bfqd->strict_guarantees && __data == 1\n\t    && bfqd->bfq_slice_idle < 8 * NSEC_PER_MSEC)\n\t\tbfqd->bfq_slice_idle = 8 * NSEC_PER_MSEC;\n\n\tbfqd->strict_guarantees = __data;\n\n\treturn count;\n}\n\nstatic ssize_t bfq_low_latency_store(struct elevator_queue *e,\n\t\t\t\t     const char *page, size_t count)\n{\n\tstruct bfq_data *bfqd = e->elevator_data;\n\tunsigned long __data;\n\tint ret;\n\n\tret = bfq_var_store(&__data, (page));\n\tif (ret)\n\t\treturn ret;\n\n\tif (__data > 1)\n\t\t__data = 1;\n\tif (__data == 0 && bfqd->low_latency != 0)\n\t\tbfq_end_wr(bfqd);\n\tbfqd->low_latency = __data;\n\n\treturn count;\n}\n\n#define BFQ_ATTR(name) \\\n\t__ATTR(name, 0644, bfq_##name##_show, bfq_##name##_store)\n\nstatic struct elv_fs_entry bfq_attrs[] = {\n\tBFQ_ATTR(fifo_expire_sync),\n\tBFQ_ATTR(fifo_expire_async),\n\tBFQ_ATTR(back_seek_max),\n\tBFQ_ATTR(back_seek_penalty),\n\tBFQ_ATTR(slice_idle),\n\tBFQ_ATTR(slice_idle_us),\n\tBFQ_ATTR(max_budget),\n\tBFQ_ATTR(timeout_sync),\n\tBFQ_ATTR(strict_guarantees),\n\tBFQ_ATTR(low_latency),\n\t__ATTR_NULL\n};\n\nstatic struct elevator_type iosched_bfq_mq = {\n\t.ops = {\n\t\t.limit_depth\t\t= bfq_limit_depth,\n\t\t.prepare_request\t= bfq_prepare_request,\n\t\t.requeue_request        = bfq_finish_requeue_request,\n\t\t.finish_request\t\t= bfq_finish_request,\n\t\t.exit_icq\t\t= bfq_exit_icq,\n\t\t.insert_requests\t= bfq_insert_requests,\n\t\t.dispatch_request\t= bfq_dispatch_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.allow_merge\t\t= bfq_allow_bio_merge,\n\t\t.bio_merge\t\t= bfq_bio_merge,\n\t\t.request_merge\t\t= bfq_request_merge,\n\t\t.requests_merged\t= bfq_requests_merged,\n\t\t.request_merged\t\t= bfq_request_merged,\n\t\t.has_work\t\t= bfq_has_work,\n\t\t.depth_updated\t\t= bfq_depth_updated,\n\t\t.init_hctx\t\t= bfq_init_hctx,\n\t\t.init_sched\t\t= bfq_init_queue,\n\t\t.exit_sched\t\t= bfq_exit_queue,\n\t},\n\n\t.icq_size =\t\tsizeof(struct bfq_io_cq),\n\t.icq_align =\t\t__alignof__(struct bfq_io_cq),\n\t.elevator_attrs =\tbfq_attrs,\n\t.elevator_name =\t\"bfq\",\n\t.elevator_owner =\tTHIS_MODULE,\n};\nMODULE_ALIAS(\"bfq-iosched\");\n\nstatic int __init bfq_init(void)\n{\n\tint ret;\n\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tret = blkcg_policy_register(&blkcg_policy_bfq);\n\tif (ret)\n\t\treturn ret;\n#endif\n\n\tret = -ENOMEM;\n\tif (bfq_slab_setup())\n\t\tgoto err_pol_unreg;\n\n\t \n\tref_wr_duration[0] = msecs_to_jiffies(7000);  \n\tref_wr_duration[1] = msecs_to_jiffies(2500);  \n\n\tret = elv_register(&iosched_bfq_mq);\n\tif (ret)\n\t\tgoto slab_kill;\n\n\treturn 0;\n\nslab_kill:\n\tbfq_slab_kill();\nerr_pol_unreg:\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\treturn ret;\n}\n\nstatic void __exit bfq_exit(void)\n{\n\telv_unregister(&iosched_bfq_mq);\n#ifdef CONFIG_BFQ_GROUP_IOSCHED\n\tblkcg_policy_unregister(&blkcg_policy_bfq);\n#endif\n\tbfq_slab_kill();\n}\n\nmodule_init(bfq_init);\nmodule_exit(bfq_exit);\n\nMODULE_AUTHOR(\"Paolo Valente\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ Budget Fair Queueing I/O Scheduler\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}