{
  "module_name": "blk-ioc.c",
  "hash_id": "8b738de5181ac049ca4621ccb04a89ca69a354d9c70162a30b06db63271475d3",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-ioc.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/slab.h>\n#include <linux/security.h>\n#include <linux/sched/task.h>\n\n#include \"blk.h\"\n#include \"blk-mq-sched.h\"\n\n \nstatic struct kmem_cache *iocontext_cachep;\n\n#ifdef CONFIG_BLK_ICQ\n \nstatic void get_io_context(struct io_context *ioc)\n{\n\tBUG_ON(atomic_long_read(&ioc->refcount) <= 0);\n\tatomic_long_inc(&ioc->refcount);\n}\n\nstatic void icq_free_icq_rcu(struct rcu_head *head)\n{\n\tstruct io_cq *icq = container_of(head, struct io_cq, __rcu_head);\n\n\tkmem_cache_free(icq->__rcu_icq_cache, icq);\n}\n\n \nstatic void ioc_exit_icq(struct io_cq *icq)\n{\n\tstruct elevator_type *et = icq->q->elevator->type;\n\n\tif (icq->flags & ICQ_EXITED)\n\t\treturn;\n\n\tif (et->ops.exit_icq)\n\t\tet->ops.exit_icq(icq);\n\n\ticq->flags |= ICQ_EXITED;\n}\n\nstatic void ioc_exit_icqs(struct io_context *ioc)\n{\n\tstruct io_cq *icq;\n\n\tspin_lock_irq(&ioc->lock);\n\thlist_for_each_entry(icq, &ioc->icq_list, ioc_node)\n\t\tioc_exit_icq(icq);\n\tspin_unlock_irq(&ioc->lock);\n}\n\n \nstatic void ioc_destroy_icq(struct io_cq *icq)\n{\n\tstruct io_context *ioc = icq->ioc;\n\tstruct request_queue *q = icq->q;\n\tstruct elevator_type *et = q->elevator->type;\n\n\tlockdep_assert_held(&ioc->lock);\n\tlockdep_assert_held(&q->queue_lock);\n\n\tif (icq->flags & ICQ_DESTROYED)\n\t\treturn;\n\n\tradix_tree_delete(&ioc->icq_tree, icq->q->id);\n\thlist_del_init(&icq->ioc_node);\n\tlist_del_init(&icq->q_node);\n\n\t \n\tif (rcu_access_pointer(ioc->icq_hint) == icq)\n\t\trcu_assign_pointer(ioc->icq_hint, NULL);\n\n\tioc_exit_icq(icq);\n\n\t \n\ticq->__rcu_icq_cache = et->icq_cache;\n\ticq->flags |= ICQ_DESTROYED;\n\tcall_rcu(&icq->__rcu_head, icq_free_icq_rcu);\n}\n\n \nstatic void ioc_release_fn(struct work_struct *work)\n{\n\tstruct io_context *ioc = container_of(work, struct io_context,\n\t\t\t\t\t      release_work);\n\tspin_lock_irq(&ioc->lock);\n\n\twhile (!hlist_empty(&ioc->icq_list)) {\n\t\tstruct io_cq *icq = hlist_entry(ioc->icq_list.first,\n\t\t\t\t\t\tstruct io_cq, ioc_node);\n\t\tstruct request_queue *q = icq->q;\n\n\t\tif (spin_trylock(&q->queue_lock)) {\n\t\t\tioc_destroy_icq(icq);\n\t\t\tspin_unlock(&q->queue_lock);\n\t\t} else {\n\t\t\t \n\t\t\trcu_read_lock();\n\n\t\t\t \n\t\t\tspin_unlock(&ioc->lock);\n\t\t\tspin_lock(&q->queue_lock);\n\t\t\tspin_lock(&ioc->lock);\n\n\t\t\tioc_destroy_icq(icq);\n\n\t\t\tspin_unlock(&q->queue_lock);\n\t\t\trcu_read_unlock();\n\t\t}\n\t}\n\n\tspin_unlock_irq(&ioc->lock);\n\n\tkmem_cache_free(iocontext_cachep, ioc);\n}\n\n \nstatic bool ioc_delay_free(struct io_context *ioc)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ioc->lock, flags);\n\tif (!hlist_empty(&ioc->icq_list)) {\n\t\tqueue_work(system_power_efficient_wq, &ioc->release_work);\n\t\tspin_unlock_irqrestore(&ioc->lock, flags);\n\t\treturn true;\n\t}\n\tspin_unlock_irqrestore(&ioc->lock, flags);\n\treturn false;\n}\n\n \nvoid ioc_clear_queue(struct request_queue *q)\n{\n\tspin_lock_irq(&q->queue_lock);\n\twhile (!list_empty(&q->icq_list)) {\n\t\tstruct io_cq *icq =\n\t\t\tlist_first_entry(&q->icq_list, struct io_cq, q_node);\n\n\t\t \n\t\tspin_lock(&icq->ioc->lock);\n\t\tioc_destroy_icq(icq);\n\t\tspin_unlock(&icq->ioc->lock);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n}\n#else  \nstatic inline void ioc_exit_icqs(struct io_context *ioc)\n{\n}\nstatic inline bool ioc_delay_free(struct io_context *ioc)\n{\n\treturn false;\n}\n#endif  \n\n \nvoid put_io_context(struct io_context *ioc)\n{\n\tBUG_ON(atomic_long_read(&ioc->refcount) <= 0);\n\tif (atomic_long_dec_and_test(&ioc->refcount) && !ioc_delay_free(ioc))\n\t\tkmem_cache_free(iocontext_cachep, ioc);\n}\nEXPORT_SYMBOL_GPL(put_io_context);\n\n \nvoid exit_io_context(struct task_struct *task)\n{\n\tstruct io_context *ioc;\n\n\ttask_lock(task);\n\tioc = task->io_context;\n\ttask->io_context = NULL;\n\ttask_unlock(task);\n\n\tif (atomic_dec_and_test(&ioc->active_ref)) {\n\t\tioc_exit_icqs(ioc);\n\t\tput_io_context(ioc);\n\t}\n}\n\nstatic struct io_context *alloc_io_context(gfp_t gfp_flags, int node)\n{\n\tstruct io_context *ioc;\n\n\tioc = kmem_cache_alloc_node(iocontext_cachep, gfp_flags | __GFP_ZERO,\n\t\t\t\t    node);\n\tif (unlikely(!ioc))\n\t\treturn NULL;\n\n\tatomic_long_set(&ioc->refcount, 1);\n\tatomic_set(&ioc->active_ref, 1);\n#ifdef CONFIG_BLK_ICQ\n\tspin_lock_init(&ioc->lock);\n\tINIT_RADIX_TREE(&ioc->icq_tree, GFP_ATOMIC);\n\tINIT_HLIST_HEAD(&ioc->icq_list);\n\tINIT_WORK(&ioc->release_work, ioc_release_fn);\n#endif\n\tioc->ioprio = IOPRIO_DEFAULT;\n\n\treturn ioc;\n}\n\nint set_task_ioprio(struct task_struct *task, int ioprio)\n{\n\tint err;\n\tconst struct cred *cred = current_cred(), *tcred;\n\n\trcu_read_lock();\n\ttcred = __task_cred(task);\n\tif (!uid_eq(tcred->uid, cred->euid) &&\n\t    !uid_eq(tcred->uid, cred->uid) && !capable(CAP_SYS_NICE)) {\n\t\trcu_read_unlock();\n\t\treturn -EPERM;\n\t}\n\trcu_read_unlock();\n\n\terr = security_task_setioprio(task, ioprio);\n\tif (err)\n\t\treturn err;\n\n\ttask_lock(task);\n\tif (unlikely(!task->io_context)) {\n\t\tstruct io_context *ioc;\n\n\t\ttask_unlock(task);\n\n\t\tioc = alloc_io_context(GFP_ATOMIC, NUMA_NO_NODE);\n\t\tif (!ioc)\n\t\t\treturn -ENOMEM;\n\n\t\ttask_lock(task);\n\t\tif (task->flags & PF_EXITING) {\n\t\t\tkmem_cache_free(iocontext_cachep, ioc);\n\t\t\tgoto out;\n\t\t}\n\t\tif (task->io_context)\n\t\t\tkmem_cache_free(iocontext_cachep, ioc);\n\t\telse\n\t\t\ttask->io_context = ioc;\n\t}\n\ttask->io_context->ioprio = ioprio;\nout:\n\ttask_unlock(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(set_task_ioprio);\n\nint __copy_io(unsigned long clone_flags, struct task_struct *tsk)\n{\n\tstruct io_context *ioc = current->io_context;\n\n\t \n\tif (clone_flags & CLONE_IO) {\n\t\tatomic_inc(&ioc->active_ref);\n\t\ttsk->io_context = ioc;\n\t} else if (ioprio_valid(ioc->ioprio)) {\n\t\ttsk->io_context = alloc_io_context(GFP_KERNEL, NUMA_NO_NODE);\n\t\tif (!tsk->io_context)\n\t\t\treturn -ENOMEM;\n\t\ttsk->io_context->ioprio = ioc->ioprio;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_ICQ\n \nstruct io_cq *ioc_lookup_icq(struct request_queue *q)\n{\n\tstruct io_context *ioc = current->io_context;\n\tstruct io_cq *icq;\n\n\tlockdep_assert_held(&q->queue_lock);\n\n\t \n\trcu_read_lock();\n\ticq = rcu_dereference(ioc->icq_hint);\n\tif (icq && icq->q == q)\n\t\tgoto out;\n\n\ticq = radix_tree_lookup(&ioc->icq_tree, q->id);\n\tif (icq && icq->q == q)\n\t\trcu_assign_pointer(ioc->icq_hint, icq);\t \n\telse\n\t\ticq = NULL;\nout:\n\trcu_read_unlock();\n\treturn icq;\n}\nEXPORT_SYMBOL(ioc_lookup_icq);\n\n \nstatic struct io_cq *ioc_create_icq(struct request_queue *q)\n{\n\tstruct io_context *ioc = current->io_context;\n\tstruct elevator_type *et = q->elevator->type;\n\tstruct io_cq *icq;\n\n\t \n\ticq = kmem_cache_alloc_node(et->icq_cache, GFP_ATOMIC | __GFP_ZERO,\n\t\t\t\t    q->node);\n\tif (!icq)\n\t\treturn NULL;\n\n\tif (radix_tree_maybe_preload(GFP_ATOMIC) < 0) {\n\t\tkmem_cache_free(et->icq_cache, icq);\n\t\treturn NULL;\n\t}\n\n\ticq->ioc = ioc;\n\ticq->q = q;\n\tINIT_LIST_HEAD(&icq->q_node);\n\tINIT_HLIST_NODE(&icq->ioc_node);\n\n\t \n\tspin_lock_irq(&q->queue_lock);\n\tspin_lock(&ioc->lock);\n\n\tif (likely(!radix_tree_insert(&ioc->icq_tree, q->id, icq))) {\n\t\thlist_add_head(&icq->ioc_node, &ioc->icq_list);\n\t\tlist_add(&icq->q_node, &q->icq_list);\n\t\tif (et->ops.init_icq)\n\t\t\tet->ops.init_icq(icq);\n\t} else {\n\t\tkmem_cache_free(et->icq_cache, icq);\n\t\ticq = ioc_lookup_icq(q);\n\t\tif (!icq)\n\t\t\tprintk(KERN_ERR \"cfq: icq link failed!\\n\");\n\t}\n\n\tspin_unlock(&ioc->lock);\n\tspin_unlock_irq(&q->queue_lock);\n\tradix_tree_preload_end();\n\treturn icq;\n}\n\nstruct io_cq *ioc_find_get_icq(struct request_queue *q)\n{\n\tstruct io_context *ioc = current->io_context;\n\tstruct io_cq *icq = NULL;\n\n\tif (unlikely(!ioc)) {\n\t\tioc = alloc_io_context(GFP_ATOMIC, q->node);\n\t\tif (!ioc)\n\t\t\treturn NULL;\n\n\t\ttask_lock(current);\n\t\tif (current->io_context) {\n\t\t\tkmem_cache_free(iocontext_cachep, ioc);\n\t\t\tioc = current->io_context;\n\t\t} else {\n\t\t\tcurrent->io_context = ioc;\n\t\t}\n\n\t\tget_io_context(ioc);\n\t\ttask_unlock(current);\n\t} else {\n\t\tget_io_context(ioc);\n\n\t\tspin_lock_irq(&q->queue_lock);\n\t\ticq = ioc_lookup_icq(q);\n\t\tspin_unlock_irq(&q->queue_lock);\n\t}\n\n\tif (!icq) {\n\t\ticq = ioc_create_icq(q);\n\t\tif (!icq) {\n\t\t\tput_io_context(ioc);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\treturn icq;\n}\nEXPORT_SYMBOL_GPL(ioc_find_get_icq);\n#endif  \n\nstatic int __init blk_ioc_init(void)\n{\n\tiocontext_cachep = kmem_cache_create(\"blkdev_ioc\",\n\t\t\tsizeof(struct io_context), 0, SLAB_PANIC, NULL);\n\treturn 0;\n}\nsubsys_initcall(blk_ioc_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}