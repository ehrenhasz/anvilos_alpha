{
  "module_name": "kyber-iosched.c",
  "hash_id": "4e12fa84a18c15345623822951c8155ab4a88b095c8eb9850a077144e6235bc5",
  "original_prompt": "Ingested from linux-6.6.14/block/kyber-iosched.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/blkdev.h>\n#include <linux/module.h>\n#include <linux/sbitmap.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-mq-sched.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/kyber.h>\n\n \nenum {\n\tKYBER_READ,\n\tKYBER_WRITE,\n\tKYBER_DISCARD,\n\tKYBER_OTHER,\n\tKYBER_NUM_DOMAINS,\n};\n\nstatic const char *kyber_domain_names[] = {\n\t[KYBER_READ] = \"READ\",\n\t[KYBER_WRITE] = \"WRITE\",\n\t[KYBER_DISCARD] = \"DISCARD\",\n\t[KYBER_OTHER] = \"OTHER\",\n};\n\nenum {\n\t \n\tKYBER_ASYNC_PERCENT = 75,\n};\n\n \nstatic const unsigned int kyber_depth[] = {\n\t[KYBER_READ] = 256,\n\t[KYBER_WRITE] = 128,\n\t[KYBER_DISCARD] = 64,\n\t[KYBER_OTHER] = 16,\n};\n\n \nstatic const u64 kyber_latency_targets[] = {\n\t[KYBER_READ] = 2ULL * NSEC_PER_MSEC,\n\t[KYBER_WRITE] = 10ULL * NSEC_PER_MSEC,\n\t[KYBER_DISCARD] = 5ULL * NSEC_PER_SEC,\n};\n\n \nstatic const unsigned int kyber_batch_size[] = {\n\t[KYBER_READ] = 16,\n\t[KYBER_WRITE] = 8,\n\t[KYBER_DISCARD] = 1,\n\t[KYBER_OTHER] = 1,\n};\n\n \nenum {\n\t \n\tKYBER_LATENCY_SHIFT = 2,\n\t \n\tKYBER_GOOD_BUCKETS = 1 << KYBER_LATENCY_SHIFT,\n\t \n\tKYBER_LATENCY_BUCKETS = 2 << KYBER_LATENCY_SHIFT,\n};\n\n \nenum {\n\tKYBER_TOTAL_LATENCY,\n\tKYBER_IO_LATENCY,\n};\n\nstatic const char *kyber_latency_type_names[] = {\n\t[KYBER_TOTAL_LATENCY] = \"total\",\n\t[KYBER_IO_LATENCY] = \"I/O\",\n};\n\n \nstruct kyber_cpu_latency {\n\tatomic_t buckets[KYBER_OTHER][2][KYBER_LATENCY_BUCKETS];\n};\n\n \nstruct kyber_ctx_queue {\n\t \n\tspinlock_t lock;\n\tstruct list_head rq_list[KYBER_NUM_DOMAINS];\n} ____cacheline_aligned_in_smp;\n\nstruct kyber_queue_data {\n\tstruct request_queue *q;\n\tdev_t dev;\n\n\t \n\tstruct sbitmap_queue domain_tokens[KYBER_NUM_DOMAINS];\n\n\t \n\tunsigned int async_depth;\n\n\tstruct kyber_cpu_latency __percpu *cpu_latency;\n\n\t \n\tstruct timer_list timer;\n\n\tunsigned int latency_buckets[KYBER_OTHER][2][KYBER_LATENCY_BUCKETS];\n\n\tunsigned long latency_timeout[KYBER_OTHER];\n\n\tint domain_p99[KYBER_OTHER];\n\n\t \n\tu64 latency_targets[KYBER_OTHER];\n};\n\nstruct kyber_hctx_data {\n\tspinlock_t lock;\n\tstruct list_head rqs[KYBER_NUM_DOMAINS];\n\tunsigned int cur_domain;\n\tunsigned int batching;\n\tstruct kyber_ctx_queue *kcqs;\n\tstruct sbitmap kcq_map[KYBER_NUM_DOMAINS];\n\tstruct sbq_wait domain_wait[KYBER_NUM_DOMAINS];\n\tstruct sbq_wait_state *domain_ws[KYBER_NUM_DOMAINS];\n\tatomic_t wait_index[KYBER_NUM_DOMAINS];\n};\n\nstatic int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,\n\t\t\t     void *key);\n\nstatic unsigned int kyber_sched_domain(blk_opf_t opf)\n{\n\tswitch (opf & REQ_OP_MASK) {\n\tcase REQ_OP_READ:\n\t\treturn KYBER_READ;\n\tcase REQ_OP_WRITE:\n\t\treturn KYBER_WRITE;\n\tcase REQ_OP_DISCARD:\n\t\treturn KYBER_DISCARD;\n\tdefault:\n\t\treturn KYBER_OTHER;\n\t}\n}\n\nstatic void flush_latency_buckets(struct kyber_queue_data *kqd,\n\t\t\t\t  struct kyber_cpu_latency *cpu_latency,\n\t\t\t\t  unsigned int sched_domain, unsigned int type)\n{\n\tunsigned int *buckets = kqd->latency_buckets[sched_domain][type];\n\tatomic_t *cpu_buckets = cpu_latency->buckets[sched_domain][type];\n\tunsigned int bucket;\n\n\tfor (bucket = 0; bucket < KYBER_LATENCY_BUCKETS; bucket++)\n\t\tbuckets[bucket] += atomic_xchg(&cpu_buckets[bucket], 0);\n}\n\n \nstatic int calculate_percentile(struct kyber_queue_data *kqd,\n\t\t\t\tunsigned int sched_domain, unsigned int type,\n\t\t\t\tunsigned int percentile)\n{\n\tunsigned int *buckets = kqd->latency_buckets[sched_domain][type];\n\tunsigned int bucket, samples = 0, percentile_samples;\n\n\tfor (bucket = 0; bucket < KYBER_LATENCY_BUCKETS; bucket++)\n\t\tsamples += buckets[bucket];\n\n\tif (!samples)\n\t\treturn -1;\n\n\t \n\tif (!kqd->latency_timeout[sched_domain])\n\t\tkqd->latency_timeout[sched_domain] = max(jiffies + HZ, 1UL);\n\tif (samples < 500 &&\n\t    time_is_after_jiffies(kqd->latency_timeout[sched_domain])) {\n\t\treturn -1;\n\t}\n\tkqd->latency_timeout[sched_domain] = 0;\n\n\tpercentile_samples = DIV_ROUND_UP(samples * percentile, 100);\n\tfor (bucket = 0; bucket < KYBER_LATENCY_BUCKETS - 1; bucket++) {\n\t\tif (buckets[bucket] >= percentile_samples)\n\t\t\tbreak;\n\t\tpercentile_samples -= buckets[bucket];\n\t}\n\tmemset(buckets, 0, sizeof(kqd->latency_buckets[sched_domain][type]));\n\n\ttrace_kyber_latency(kqd->dev, kyber_domain_names[sched_domain],\n\t\t\t    kyber_latency_type_names[type], percentile,\n\t\t\t    bucket + 1, 1 << KYBER_LATENCY_SHIFT, samples);\n\n\treturn bucket;\n}\n\nstatic void kyber_resize_domain(struct kyber_queue_data *kqd,\n\t\t\t\tunsigned int sched_domain, unsigned int depth)\n{\n\tdepth = clamp(depth, 1U, kyber_depth[sched_domain]);\n\tif (depth != kqd->domain_tokens[sched_domain].sb.depth) {\n\t\tsbitmap_queue_resize(&kqd->domain_tokens[sched_domain], depth);\n\t\ttrace_kyber_adjust(kqd->dev, kyber_domain_names[sched_domain],\n\t\t\t\t   depth);\n\t}\n}\n\nstatic void kyber_timer_fn(struct timer_list *t)\n{\n\tstruct kyber_queue_data *kqd = from_timer(kqd, t, timer);\n\tunsigned int sched_domain;\n\tint cpu;\n\tbool bad = false;\n\n\t \n\tfor_each_online_cpu(cpu) {\n\t\tstruct kyber_cpu_latency *cpu_latency;\n\n\t\tcpu_latency = per_cpu_ptr(kqd->cpu_latency, cpu);\n\t\tfor (sched_domain = 0; sched_domain < KYBER_OTHER; sched_domain++) {\n\t\t\tflush_latency_buckets(kqd, cpu_latency, sched_domain,\n\t\t\t\t\t      KYBER_TOTAL_LATENCY);\n\t\t\tflush_latency_buckets(kqd, cpu_latency, sched_domain,\n\t\t\t\t\t      KYBER_IO_LATENCY);\n\t\t}\n\t}\n\n\t \n\tfor (sched_domain = 0; sched_domain < KYBER_OTHER; sched_domain++) {\n\t\tint p90;\n\n\t\tp90 = calculate_percentile(kqd, sched_domain, KYBER_IO_LATENCY,\n\t\t\t\t\t   90);\n\t\tif (p90 >= KYBER_GOOD_BUCKETS)\n\t\t\tbad = true;\n\t}\n\n\t \n\tfor (sched_domain = 0; sched_domain < KYBER_OTHER; sched_domain++) {\n\t\tunsigned int orig_depth, depth;\n\t\tint p99;\n\n\t\tp99 = calculate_percentile(kqd, sched_domain,\n\t\t\t\t\t   KYBER_TOTAL_LATENCY, 99);\n\t\t \n\t\tif (bad) {\n\t\t\tif (p99 < 0)\n\t\t\t\tp99 = kqd->domain_p99[sched_domain];\n\t\t\tkqd->domain_p99[sched_domain] = -1;\n\t\t} else if (p99 >= 0) {\n\t\t\tkqd->domain_p99[sched_domain] = p99;\n\t\t}\n\t\tif (p99 < 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (bad || p99 >= KYBER_GOOD_BUCKETS) {\n\t\t\torig_depth = kqd->domain_tokens[sched_domain].sb.depth;\n\t\t\tdepth = (orig_depth * (p99 + 1)) >> KYBER_LATENCY_SHIFT;\n\t\t\tkyber_resize_domain(kqd, sched_domain, depth);\n\t\t}\n\t}\n}\n\nstatic struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)\n{\n\tstruct kyber_queue_data *kqd;\n\tint ret = -ENOMEM;\n\tint i;\n\n\tkqd = kzalloc_node(sizeof(*kqd), GFP_KERNEL, q->node);\n\tif (!kqd)\n\t\tgoto err;\n\n\tkqd->q = q;\n\tkqd->dev = disk_devt(q->disk);\n\n\tkqd->cpu_latency = alloc_percpu_gfp(struct kyber_cpu_latency,\n\t\t\t\t\t    GFP_KERNEL | __GFP_ZERO);\n\tif (!kqd->cpu_latency)\n\t\tgoto err_kqd;\n\n\ttimer_setup(&kqd->timer, kyber_timer_fn, 0);\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\n\t\tWARN_ON(!kyber_depth[i]);\n\t\tWARN_ON(!kyber_batch_size[i]);\n\t\tret = sbitmap_queue_init_node(&kqd->domain_tokens[i],\n\t\t\t\t\t      kyber_depth[i], -1, false,\n\t\t\t\t\t      GFP_KERNEL, q->node);\n\t\tif (ret) {\n\t\t\twhile (--i >= 0)\n\t\t\t\tsbitmap_queue_free(&kqd->domain_tokens[i]);\n\t\t\tgoto err_buckets;\n\t\t}\n\t}\n\n\tfor (i = 0; i < KYBER_OTHER; i++) {\n\t\tkqd->domain_p99[i] = -1;\n\t\tkqd->latency_targets[i] = kyber_latency_targets[i];\n\t}\n\n\treturn kqd;\n\nerr_buckets:\n\tfree_percpu(kqd->cpu_latency);\nerr_kqd:\n\tkfree(kqd);\nerr:\n\treturn ERR_PTR(ret);\n}\n\nstatic int kyber_init_sched(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct kyber_queue_data *kqd;\n\tstruct elevator_queue *eq;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn -ENOMEM;\n\n\tkqd = kyber_queue_data_alloc(q);\n\tif (IS_ERR(kqd)) {\n\t\tkobject_put(&eq->kobj);\n\t\treturn PTR_ERR(kqd);\n\t}\n\n\tblk_stat_enable_accounting(q);\n\n\tblk_queue_flag_clear(QUEUE_FLAG_SQ_SCHED, q);\n\n\teq->elevator_data = kqd;\n\tq->elevator = eq;\n\n\treturn 0;\n}\n\nstatic void kyber_exit_sched(struct elevator_queue *e)\n{\n\tstruct kyber_queue_data *kqd = e->elevator_data;\n\tint i;\n\n\ttimer_shutdown_sync(&kqd->timer);\n\tblk_stat_disable_accounting(kqd->q);\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++)\n\t\tsbitmap_queue_free(&kqd->domain_tokens[i]);\n\tfree_percpu(kqd->cpu_latency);\n\tkfree(kqd);\n}\n\nstatic void kyber_ctx_queue_init(struct kyber_ctx_queue *kcq)\n{\n\tunsigned int i;\n\n\tspin_lock_init(&kcq->lock);\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++)\n\t\tINIT_LIST_HEAD(&kcq->rq_list[i]);\n}\n\nstatic void kyber_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct kyber_queue_data *kqd = hctx->queue->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\tunsigned int shift = tags->bitmap_tags.sb.shift;\n\n\tkqd->async_depth = (1U << shift) * KYBER_ASYNC_PERCENT / 100U;\n\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, kqd->async_depth);\n}\n\nstatic int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tstruct kyber_hctx_data *khd;\n\tint i;\n\n\tkhd = kmalloc_node(sizeof(*khd), GFP_KERNEL, hctx->numa_node);\n\tif (!khd)\n\t\treturn -ENOMEM;\n\n\tkhd->kcqs = kmalloc_array_node(hctx->nr_ctx,\n\t\t\t\t       sizeof(struct kyber_ctx_queue),\n\t\t\t\t       GFP_KERNEL, hctx->numa_node);\n\tif (!khd->kcqs)\n\t\tgoto err_khd;\n\n\tfor (i = 0; i < hctx->nr_ctx; i++)\n\t\tkyber_ctx_queue_init(&khd->kcqs[i]);\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\n\t\tif (sbitmap_init_node(&khd->kcq_map[i], hctx->nr_ctx,\n\t\t\t\t      ilog2(8), GFP_KERNEL, hctx->numa_node,\n\t\t\t\t      false, false)) {\n\t\t\twhile (--i >= 0)\n\t\t\t\tsbitmap_free(&khd->kcq_map[i]);\n\t\t\tgoto err_kcqs;\n\t\t}\n\t}\n\n\tspin_lock_init(&khd->lock);\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\n\t\tINIT_LIST_HEAD(&khd->rqs[i]);\n\t\tkhd->domain_wait[i].sbq = NULL;\n\t\tinit_waitqueue_func_entry(&khd->domain_wait[i].wait,\n\t\t\t\t\t  kyber_domain_wake);\n\t\tkhd->domain_wait[i].wait.private = hctx;\n\t\tINIT_LIST_HEAD(&khd->domain_wait[i].wait.entry);\n\t\tatomic_set(&khd->wait_index[i], 0);\n\t}\n\n\tkhd->cur_domain = 0;\n\tkhd->batching = 0;\n\n\thctx->sched_data = khd;\n\tkyber_depth_updated(hctx);\n\n\treturn 0;\n\nerr_kcqs:\n\tkfree(khd->kcqs);\nerr_khd:\n\tkfree(khd);\n\treturn -ENOMEM;\n}\n\nstatic void kyber_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\tint i;\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++)\n\t\tsbitmap_free(&khd->kcq_map[i]);\n\tkfree(khd->kcqs);\n\tkfree(hctx->sched_data);\n}\n\nstatic int rq_get_domain_token(struct request *rq)\n{\n\treturn (long)rq->elv.priv[0];\n}\n\nstatic void rq_set_domain_token(struct request *rq, int token)\n{\n\trq->elv.priv[0] = (void *)(long)token;\n}\n\nstatic void rq_clear_domain_token(struct kyber_queue_data *kqd,\n\t\t\t\t  struct request *rq)\n{\n\tunsigned int sched_domain;\n\tint nr;\n\n\tnr = rq_get_domain_token(rq);\n\tif (nr != -1) {\n\t\tsched_domain = kyber_sched_domain(rq->cmd_flags);\n\t\tsbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,\n\t\t\t\t    rq->mq_ctx->cpu);\n\t}\n}\n\nstatic void kyber_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\t \n\tif (!op_is_sync(opf)) {\n\t\tstruct kyber_queue_data *kqd = data->q->elevator->elevator_data;\n\n\t\tdata->shallow_depth = kqd->async_depth;\n\t}\n}\n\nstatic bool kyber_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct blk_mq_ctx *ctx = blk_mq_get_ctx(q);\n\tstruct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, bio->bi_opf, ctx);\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\tstruct kyber_ctx_queue *kcq = &khd->kcqs[ctx->index_hw[hctx->type]];\n\tunsigned int sched_domain = kyber_sched_domain(bio->bi_opf);\n\tstruct list_head *rq_list = &kcq->rq_list[sched_domain];\n\tbool merged;\n\n\tspin_lock(&kcq->lock);\n\tmerged = blk_bio_list_merge(hctx->queue, rq_list, bio, nr_segs);\n\tspin_unlock(&kcq->lock);\n\n\treturn merged;\n}\n\nstatic void kyber_prepare_request(struct request *rq)\n{\n\trq_set_domain_token(rq, -1);\n}\n\nstatic void kyber_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct list_head *rq_list,\n\t\t\t\t  blk_insert_t flags)\n{\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\tstruct request *rq, *next;\n\n\tlist_for_each_entry_safe(rq, next, rq_list, queuelist) {\n\t\tunsigned int sched_domain = kyber_sched_domain(rq->cmd_flags);\n\t\tstruct kyber_ctx_queue *kcq = &khd->kcqs[rq->mq_ctx->index_hw[hctx->type]];\n\t\tstruct list_head *head = &kcq->rq_list[sched_domain];\n\n\t\tspin_lock(&kcq->lock);\n\t\ttrace_block_rq_insert(rq);\n\t\tif (flags & BLK_MQ_INSERT_AT_HEAD)\n\t\t\tlist_move(&rq->queuelist, head);\n\t\telse\n\t\t\tlist_move_tail(&rq->queuelist, head);\n\t\tsbitmap_set_bit(&khd->kcq_map[sched_domain],\n\t\t\t\trq->mq_ctx->index_hw[hctx->type]);\n\t\tspin_unlock(&kcq->lock);\n\t}\n}\n\nstatic void kyber_finish_request(struct request *rq)\n{\n\tstruct kyber_queue_data *kqd = rq->q->elevator->elevator_data;\n\n\trq_clear_domain_token(kqd, rq);\n}\n\nstatic void add_latency_sample(struct kyber_cpu_latency *cpu_latency,\n\t\t\t       unsigned int sched_domain, unsigned int type,\n\t\t\t       u64 target, u64 latency)\n{\n\tunsigned int bucket;\n\tu64 divisor;\n\n\tif (latency > 0) {\n\t\tdivisor = max_t(u64, target >> KYBER_LATENCY_SHIFT, 1);\n\t\tbucket = min_t(unsigned int, div64_u64(latency - 1, divisor),\n\t\t\t       KYBER_LATENCY_BUCKETS - 1);\n\t} else {\n\t\tbucket = 0;\n\t}\n\n\tatomic_inc(&cpu_latency->buckets[sched_domain][type][bucket]);\n}\n\nstatic void kyber_completed_request(struct request *rq, u64 now)\n{\n\tstruct kyber_queue_data *kqd = rq->q->elevator->elevator_data;\n\tstruct kyber_cpu_latency *cpu_latency;\n\tunsigned int sched_domain;\n\tu64 target;\n\n\tsched_domain = kyber_sched_domain(rq->cmd_flags);\n\tif (sched_domain == KYBER_OTHER)\n\t\treturn;\n\n\tcpu_latency = get_cpu_ptr(kqd->cpu_latency);\n\ttarget = kqd->latency_targets[sched_domain];\n\tadd_latency_sample(cpu_latency, sched_domain, KYBER_TOTAL_LATENCY,\n\t\t\t   target, now - rq->start_time_ns);\n\tadd_latency_sample(cpu_latency, sched_domain, KYBER_IO_LATENCY, target,\n\t\t\t   now - rq->io_start_time_ns);\n\tput_cpu_ptr(kqd->cpu_latency);\n\n\ttimer_reduce(&kqd->timer, jiffies + HZ / 10);\n}\n\nstruct flush_kcq_data {\n\tstruct kyber_hctx_data *khd;\n\tunsigned int sched_domain;\n\tstruct list_head *list;\n};\n\nstatic bool flush_busy_kcq(struct sbitmap *sb, unsigned int bitnr, void *data)\n{\n\tstruct flush_kcq_data *flush_data = data;\n\tstruct kyber_ctx_queue *kcq = &flush_data->khd->kcqs[bitnr];\n\n\tspin_lock(&kcq->lock);\n\tlist_splice_tail_init(&kcq->rq_list[flush_data->sched_domain],\n\t\t\t      flush_data->list);\n\tsbitmap_clear_bit(sb, bitnr);\n\tspin_unlock(&kcq->lock);\n\n\treturn true;\n}\n\nstatic void kyber_flush_busy_kcqs(struct kyber_hctx_data *khd,\n\t\t\t\t  unsigned int sched_domain,\n\t\t\t\t  struct list_head *list)\n{\n\tstruct flush_kcq_data data = {\n\t\t.khd = khd,\n\t\t.sched_domain = sched_domain,\n\t\t.list = list,\n\t};\n\n\tsbitmap_for_each_set(&khd->kcq_map[sched_domain],\n\t\t\t     flush_busy_kcq, &data);\n}\n\nstatic int kyber_domain_wake(wait_queue_entry_t *wqe, unsigned mode, int flags,\n\t\t\t     void *key)\n{\n\tstruct blk_mq_hw_ctx *hctx = READ_ONCE(wqe->private);\n\tstruct sbq_wait *wait = container_of(wqe, struct sbq_wait, wait);\n\n\tsbitmap_del_wait_queue(wait);\n\tblk_mq_run_hw_queue(hctx, true);\n\treturn 1;\n}\n\nstatic int kyber_get_domain_token(struct kyber_queue_data *kqd,\n\t\t\t\t  struct kyber_hctx_data *khd,\n\t\t\t\t  struct blk_mq_hw_ctx *hctx)\n{\n\tunsigned int sched_domain = khd->cur_domain;\n\tstruct sbitmap_queue *domain_tokens = &kqd->domain_tokens[sched_domain];\n\tstruct sbq_wait *wait = &khd->domain_wait[sched_domain];\n\tstruct sbq_wait_state *ws;\n\tint nr;\n\n\tnr = __sbitmap_queue_get(domain_tokens);\n\n\t \n\tif (nr < 0 && list_empty_careful(&wait->wait.entry)) {\n\t\tws = sbq_wait_ptr(domain_tokens,\n\t\t\t\t  &khd->wait_index[sched_domain]);\n\t\tkhd->domain_ws[sched_domain] = ws;\n\t\tsbitmap_add_wait_queue(domain_tokens, ws, wait);\n\n\t\t \n\t\tnr = __sbitmap_queue_get(domain_tokens);\n\t}\n\n\t \n\tif (nr >= 0 && !list_empty_careful(&wait->wait.entry)) {\n\t\tws = khd->domain_ws[sched_domain];\n\t\tspin_lock_irq(&ws->wait.lock);\n\t\tsbitmap_del_wait_queue(wait);\n\t\tspin_unlock_irq(&ws->wait.lock);\n\t}\n\n\treturn nr;\n}\n\nstatic struct request *\nkyber_dispatch_cur_domain(struct kyber_queue_data *kqd,\n\t\t\t  struct kyber_hctx_data *khd,\n\t\t\t  struct blk_mq_hw_ctx *hctx)\n{\n\tstruct list_head *rqs;\n\tstruct request *rq;\n\tint nr;\n\n\trqs = &khd->rqs[khd->cur_domain];\n\n\t \n\trq = list_first_entry_or_null(rqs, struct request, queuelist);\n\tif (rq) {\n\t\tnr = kyber_get_domain_token(kqd, khd, hctx);\n\t\tif (nr >= 0) {\n\t\t\tkhd->batching++;\n\t\t\trq_set_domain_token(rq, nr);\n\t\t\tlist_del_init(&rq->queuelist);\n\t\t\treturn rq;\n\t\t} else {\n\t\t\ttrace_kyber_throttled(kqd->dev,\n\t\t\t\t\t      kyber_domain_names[khd->cur_domain]);\n\t\t}\n\t} else if (sbitmap_any_bit_set(&khd->kcq_map[khd->cur_domain])) {\n\t\tnr = kyber_get_domain_token(kqd, khd, hctx);\n\t\tif (nr >= 0) {\n\t\t\tkyber_flush_busy_kcqs(khd, khd->cur_domain, rqs);\n\t\t\trq = list_first_entry(rqs, struct request, queuelist);\n\t\t\tkhd->batching++;\n\t\t\trq_set_domain_token(rq, nr);\n\t\t\tlist_del_init(&rq->queuelist);\n\t\t\treturn rq;\n\t\t} else {\n\t\t\ttrace_kyber_throttled(kqd->dev,\n\t\t\t\t\t      kyber_domain_names[khd->cur_domain]);\n\t\t}\n\t}\n\n\t \n\treturn NULL;\n}\n\nstatic struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct kyber_queue_data *kqd = hctx->queue->elevator->elevator_data;\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\tstruct request *rq;\n\tint i;\n\n\tspin_lock(&khd->lock);\n\n\t \n\tif (khd->batching < kyber_batch_size[khd->cur_domain]) {\n\t\trq = kyber_dispatch_cur_domain(kqd, khd, hctx);\n\t\tif (rq)\n\t\t\tgoto out;\n\t}\n\n\t \n\tkhd->batching = 0;\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\n\t\tif (khd->cur_domain == KYBER_NUM_DOMAINS - 1)\n\t\t\tkhd->cur_domain = 0;\n\t\telse\n\t\t\tkhd->cur_domain++;\n\n\t\trq = kyber_dispatch_cur_domain(kqd, khd, hctx);\n\t\tif (rq)\n\t\t\tgoto out;\n\t}\n\n\trq = NULL;\nout:\n\tspin_unlock(&khd->lock);\n\treturn rq;\n}\n\nstatic bool kyber_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\tint i;\n\n\tfor (i = 0; i < KYBER_NUM_DOMAINS; i++) {\n\t\tif (!list_empty_careful(&khd->rqs[i]) ||\n\t\t    sbitmap_any_bit_set(&khd->kcq_map[i]))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#define KYBER_LAT_SHOW_STORE(domain, name)\t\t\t\t\\\nstatic ssize_t kyber_##name##_lat_show(struct elevator_queue *e,\t\\\n\t\t\t\t       char *page)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct kyber_queue_data *kqd = e->elevator_data;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn sprintf(page, \"%llu\\n\", kqd->latency_targets[domain]);\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic ssize_t kyber_##name##_lat_store(struct elevator_queue *e,\t\\\n\t\t\t\t\tconst char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct kyber_queue_data *kqd = e->elevator_data;\t\t\\\n\tunsigned long long nsec;\t\t\t\t\t\\\n\tint ret;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tret = kstrtoull(page, 10, &nsec);\t\t\t\t\\\n\tif (ret)\t\t\t\t\t\t\t\\\n\t\treturn ret;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tkqd->latency_targets[domain] = nsec;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\nKYBER_LAT_SHOW_STORE(KYBER_READ, read);\nKYBER_LAT_SHOW_STORE(KYBER_WRITE, write);\n#undef KYBER_LAT_SHOW_STORE\n\n#define KYBER_LAT_ATTR(op) __ATTR(op##_lat_nsec, 0644, kyber_##op##_lat_show, kyber_##op##_lat_store)\nstatic struct elv_fs_entry kyber_sched_attrs[] = {\n\tKYBER_LAT_ATTR(read),\n\tKYBER_LAT_ATTR(write),\n\t__ATTR_NULL\n};\n#undef KYBER_LAT_ATTR\n\n#ifdef CONFIG_BLK_DEBUG_FS\n#define KYBER_DEBUGFS_DOMAIN_ATTRS(domain, name)\t\t\t\\\nstatic int kyber_##name##_tokens_show(void *data, struct seq_file *m)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = data;\t\t\t\t\t\\\n\tstruct kyber_queue_data *kqd = q->elevator->elevator_data;\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tsbitmap_queue_show(&kqd->domain_tokens[domain], m);\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void *kyber_##name##_rqs_start(struct seq_file *m, loff_t *pos)\t\\\n\t__acquires(&khd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct blk_mq_hw_ctx *hctx = m->private;\t\t\t\\\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_lock(&khd->lock);\t\t\t\t\t\t\\\n\treturn seq_list_start(&khd->rqs[domain], *pos);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void *kyber_##name##_rqs_next(struct seq_file *m, void *v,\t\\\n\t\t\t\t     loff_t *pos)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct blk_mq_hw_ctx *hctx = m->private;\t\t\t\\\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn seq_list_next(v, &khd->rqs[domain], pos);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void kyber_##name##_rqs_stop(struct seq_file *m, void *v)\t\\\n\t__releases(&khd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct blk_mq_hw_ctx *hctx = m->private;\t\t\t\\\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_unlock(&khd->lock);\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic const struct seq_operations kyber_##name##_rqs_seq_ops = {\t\\\n\t.start\t= kyber_##name##_rqs_start,\t\t\t\t\\\n\t.next\t= kyber_##name##_rqs_next,\t\t\t\t\\\n\t.stop\t= kyber_##name##_rqs_stop,\t\t\t\t\\\n\t.show\t= blk_mq_debugfs_rq_show,\t\t\t\t\\\n};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic int kyber_##name##_waiting_show(void *data, struct seq_file *m)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct blk_mq_hw_ctx *hctx = data;\t\t\t\t\\\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\t\t\t\\\n\twait_queue_entry_t *wait = &khd->domain_wait[domain].wait;\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tseq_printf(m, \"%d\\n\", !list_empty_careful(&wait->entry));\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\nKYBER_DEBUGFS_DOMAIN_ATTRS(KYBER_READ, read)\nKYBER_DEBUGFS_DOMAIN_ATTRS(KYBER_WRITE, write)\nKYBER_DEBUGFS_DOMAIN_ATTRS(KYBER_DISCARD, discard)\nKYBER_DEBUGFS_DOMAIN_ATTRS(KYBER_OTHER, other)\n#undef KYBER_DEBUGFS_DOMAIN_ATTRS\n\nstatic int kyber_async_depth_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct kyber_queue_data *kqd = q->elevator->elevator_data;\n\n\tseq_printf(m, \"%u\\n\", kqd->async_depth);\n\treturn 0;\n}\n\nstatic int kyber_cur_domain_show(void *data, struct seq_file *m)\n{\n\tstruct blk_mq_hw_ctx *hctx = data;\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\n\tseq_printf(m, \"%s\\n\", kyber_domain_names[khd->cur_domain]);\n\treturn 0;\n}\n\nstatic int kyber_batching_show(void *data, struct seq_file *m)\n{\n\tstruct blk_mq_hw_ctx *hctx = data;\n\tstruct kyber_hctx_data *khd = hctx->sched_data;\n\n\tseq_printf(m, \"%u\\n\", khd->batching);\n\treturn 0;\n}\n\n#define KYBER_QUEUE_DOMAIN_ATTRS(name)\t\\\n\t{#name \"_tokens\", 0400, kyber_##name##_tokens_show}\nstatic const struct blk_mq_debugfs_attr kyber_queue_debugfs_attrs[] = {\n\tKYBER_QUEUE_DOMAIN_ATTRS(read),\n\tKYBER_QUEUE_DOMAIN_ATTRS(write),\n\tKYBER_QUEUE_DOMAIN_ATTRS(discard),\n\tKYBER_QUEUE_DOMAIN_ATTRS(other),\n\t{\"async_depth\", 0400, kyber_async_depth_show},\n\t{},\n};\n#undef KYBER_QUEUE_DOMAIN_ATTRS\n\n#define KYBER_HCTX_DOMAIN_ATTRS(name)\t\t\t\t\t\\\n\t{#name \"_rqs\", 0400, .seq_ops = &kyber_##name##_rqs_seq_ops},\t\\\n\t{#name \"_waiting\", 0400, kyber_##name##_waiting_show}\nstatic const struct blk_mq_debugfs_attr kyber_hctx_debugfs_attrs[] = {\n\tKYBER_HCTX_DOMAIN_ATTRS(read),\n\tKYBER_HCTX_DOMAIN_ATTRS(write),\n\tKYBER_HCTX_DOMAIN_ATTRS(discard),\n\tKYBER_HCTX_DOMAIN_ATTRS(other),\n\t{\"cur_domain\", 0400, kyber_cur_domain_show},\n\t{\"batching\", 0400, kyber_batching_show},\n\t{},\n};\n#undef KYBER_HCTX_DOMAIN_ATTRS\n#endif\n\nstatic struct elevator_type kyber_sched = {\n\t.ops = {\n\t\t.init_sched = kyber_init_sched,\n\t\t.exit_sched = kyber_exit_sched,\n\t\t.init_hctx = kyber_init_hctx,\n\t\t.exit_hctx = kyber_exit_hctx,\n\t\t.limit_depth = kyber_limit_depth,\n\t\t.bio_merge = kyber_bio_merge,\n\t\t.prepare_request = kyber_prepare_request,\n\t\t.insert_requests = kyber_insert_requests,\n\t\t.finish_request = kyber_finish_request,\n\t\t.requeue_request = kyber_finish_request,\n\t\t.completed_request = kyber_completed_request,\n\t\t.dispatch_request = kyber_dispatch_request,\n\t\t.has_work = kyber_has_work,\n\t\t.depth_updated = kyber_depth_updated,\n\t},\n#ifdef CONFIG_BLK_DEBUG_FS\n\t.queue_debugfs_attrs = kyber_queue_debugfs_attrs,\n\t.hctx_debugfs_attrs = kyber_hctx_debugfs_attrs,\n#endif\n\t.elevator_attrs = kyber_sched_attrs,\n\t.elevator_name = \"kyber\",\n\t.elevator_owner = THIS_MODULE,\n};\n\nstatic int __init kyber_init(void)\n{\n\treturn elv_register(&kyber_sched);\n}\n\nstatic void __exit kyber_exit(void)\n{\n\telv_unregister(&kyber_sched);\n}\n\nmodule_init(kyber_init);\nmodule_exit(kyber_exit);\n\nMODULE_AUTHOR(\"Omar Sandoval\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Kyber I/O scheduler\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}