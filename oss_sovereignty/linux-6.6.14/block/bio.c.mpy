{
  "module_name": "bio.c",
  "hash_id": "fc3f13986622eed642868438a5beb618a8ed285736b411ae6e808410ad1ef5c6",
  "original_prompt": "Ingested from linux-6.6.14/block/bio.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/uio.h>\n#include <linux/iocontext.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/mempool.h>\n#include <linux/workqueue.h>\n#include <linux/cgroup.h>\n#include <linux/highmem.h>\n#include <linux/sched/sysctl.h>\n#include <linux/blk-crypto.h>\n#include <linux/xarray.h>\n\n#include <trace/events/block.h>\n#include \"blk.h\"\n#include \"blk-rq-qos.h\"\n#include \"blk-cgroup.h\"\n\n#define ALLOC_CACHE_THRESHOLD\t16\n#define ALLOC_CACHE_MAX\t\t256\n\nstruct bio_alloc_cache {\n\tstruct bio\t\t*free_list;\n\tstruct bio\t\t*free_list_irq;\n\tunsigned int\t\tnr;\n\tunsigned int\t\tnr_irq;\n};\n\nstatic struct biovec_slab {\n\tint nr_vecs;\n\tchar *name;\n\tstruct kmem_cache *slab;\n} bvec_slabs[] __read_mostly = {\n\t{ .nr_vecs = 16, .name = \"biovec-16\" },\n\t{ .nr_vecs = 64, .name = \"biovec-64\" },\n\t{ .nr_vecs = 128, .name = \"biovec-128\" },\n\t{ .nr_vecs = BIO_MAX_VECS, .name = \"biovec-max\" },\n};\n\nstatic struct biovec_slab *biovec_slab(unsigned short nr_vecs)\n{\n\tswitch (nr_vecs) {\n\t \n\tcase 5 ... 16:\n\t\treturn &bvec_slabs[0];\n\tcase 17 ... 64:\n\t\treturn &bvec_slabs[1];\n\tcase 65 ... 128:\n\t\treturn &bvec_slabs[2];\n\tcase 129 ... BIO_MAX_VECS:\n\t\treturn &bvec_slabs[3];\n\tdefault:\n\t\tBUG();\n\t\treturn NULL;\n\t}\n}\n\n \nstruct bio_set fs_bio_set;\nEXPORT_SYMBOL(fs_bio_set);\n\n \nstruct bio_slab {\n\tstruct kmem_cache *slab;\n\tunsigned int slab_ref;\n\tunsigned int slab_size;\n\tchar name[8];\n};\nstatic DEFINE_MUTEX(bio_slab_lock);\nstatic DEFINE_XARRAY(bio_slabs);\n\nstatic struct bio_slab *create_bio_slab(unsigned int size)\n{\n\tstruct bio_slab *bslab = kzalloc(sizeof(*bslab), GFP_KERNEL);\n\n\tif (!bslab)\n\t\treturn NULL;\n\n\tsnprintf(bslab->name, sizeof(bslab->name), \"bio-%d\", size);\n\tbslab->slab = kmem_cache_create(bslab->name, size,\n\t\t\tARCH_KMALLOC_MINALIGN,\n\t\t\tSLAB_HWCACHE_ALIGN | SLAB_TYPESAFE_BY_RCU, NULL);\n\tif (!bslab->slab)\n\t\tgoto fail_alloc_slab;\n\n\tbslab->slab_ref = 1;\n\tbslab->slab_size = size;\n\n\tif (!xa_err(xa_store(&bio_slabs, size, bslab, GFP_KERNEL)))\n\t\treturn bslab;\n\n\tkmem_cache_destroy(bslab->slab);\n\nfail_alloc_slab:\n\tkfree(bslab);\n\treturn NULL;\n}\n\nstatic inline unsigned int bs_bio_slab_size(struct bio_set *bs)\n{\n\treturn bs->front_pad + sizeof(struct bio) + bs->back_pad;\n}\n\nstatic struct kmem_cache *bio_find_or_create_slab(struct bio_set *bs)\n{\n\tunsigned int size = bs_bio_slab_size(bs);\n\tstruct bio_slab *bslab;\n\n\tmutex_lock(&bio_slab_lock);\n\tbslab = xa_load(&bio_slabs, size);\n\tif (bslab)\n\t\tbslab->slab_ref++;\n\telse\n\t\tbslab = create_bio_slab(size);\n\tmutex_unlock(&bio_slab_lock);\n\n\tif (bslab)\n\t\treturn bslab->slab;\n\treturn NULL;\n}\n\nstatic void bio_put_slab(struct bio_set *bs)\n{\n\tstruct bio_slab *bslab = NULL;\n\tunsigned int slab_size = bs_bio_slab_size(bs);\n\n\tmutex_lock(&bio_slab_lock);\n\n\tbslab = xa_load(&bio_slabs, slab_size);\n\tif (WARN(!bslab, KERN_ERR \"bio: unable to find slab!\\n\"))\n\t\tgoto out;\n\n\tWARN_ON_ONCE(bslab->slab != bs->bio_slab);\n\n\tWARN_ON(!bslab->slab_ref);\n\n\tif (--bslab->slab_ref)\n\t\tgoto out;\n\n\txa_erase(&bio_slabs, slab_size);\n\n\tkmem_cache_destroy(bslab->slab);\n\tkfree(bslab);\n\nout:\n\tmutex_unlock(&bio_slab_lock);\n}\n\nvoid bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned short nr_vecs)\n{\n\tBUG_ON(nr_vecs > BIO_MAX_VECS);\n\n\tif (nr_vecs == BIO_MAX_VECS)\n\t\tmempool_free(bv, pool);\n\telse if (nr_vecs > BIO_INLINE_VECS)\n\t\tkmem_cache_free(biovec_slab(nr_vecs)->slab, bv);\n}\n\n \nstatic inline gfp_t bvec_alloc_gfp(gfp_t gfp)\n{\n\treturn (gfp & ~(__GFP_DIRECT_RECLAIM | __GFP_IO)) |\n\t\t__GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;\n}\n\nstruct bio_vec *bvec_alloc(mempool_t *pool, unsigned short *nr_vecs,\n\t\tgfp_t gfp_mask)\n{\n\tstruct biovec_slab *bvs = biovec_slab(*nr_vecs);\n\n\tif (WARN_ON_ONCE(!bvs))\n\t\treturn NULL;\n\n\t \n\t*nr_vecs = bvs->nr_vecs;\n\n\t \n\tif (*nr_vecs < BIO_MAX_VECS) {\n\t\tstruct bio_vec *bvl;\n\n\t\tbvl = kmem_cache_alloc(bvs->slab, bvec_alloc_gfp(gfp_mask));\n\t\tif (likely(bvl) || !(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\t\treturn bvl;\n\t\t*nr_vecs = BIO_MAX_VECS;\n\t}\n\n\treturn mempool_alloc(pool, gfp_mask);\n}\n\nvoid bio_uninit(struct bio *bio)\n{\n#ifdef CONFIG_BLK_CGROUP\n\tif (bio->bi_blkg) {\n\t\tblkg_put(bio->bi_blkg);\n\t\tbio->bi_blkg = NULL;\n\t}\n#endif\n\tif (bio_integrity(bio))\n\t\tbio_integrity_free(bio);\n\n\tbio_crypt_free_ctx(bio);\n}\nEXPORT_SYMBOL(bio_uninit);\n\nstatic void bio_free(struct bio *bio)\n{\n\tstruct bio_set *bs = bio->bi_pool;\n\tvoid *p = bio;\n\n\tWARN_ON_ONCE(!bs);\n\n\tbio_uninit(bio);\n\tbvec_free(&bs->bvec_pool, bio->bi_io_vec, bio->bi_max_vecs);\n\tmempool_free(p - bs->front_pad, &bs->bio_pool);\n}\n\n \nvoid bio_init(struct bio *bio, struct block_device *bdev, struct bio_vec *table,\n\t      unsigned short max_vecs, blk_opf_t opf)\n{\n\tbio->bi_next = NULL;\n\tbio->bi_bdev = bdev;\n\tbio->bi_opf = opf;\n\tbio->bi_flags = 0;\n\tbio->bi_ioprio = 0;\n\tbio->bi_status = 0;\n\tbio->bi_iter.bi_sector = 0;\n\tbio->bi_iter.bi_size = 0;\n\tbio->bi_iter.bi_idx = 0;\n\tbio->bi_iter.bi_bvec_done = 0;\n\tbio->bi_end_io = NULL;\n\tbio->bi_private = NULL;\n#ifdef CONFIG_BLK_CGROUP\n\tbio->bi_blkg = NULL;\n\tbio->bi_issue.value = 0;\n\tif (bdev)\n\t\tbio_associate_blkg(bio);\n#ifdef CONFIG_BLK_CGROUP_IOCOST\n\tbio->bi_iocost_cost = 0;\n#endif\n#endif\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\tbio->bi_crypt_context = NULL;\n#endif\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tbio->bi_integrity = NULL;\n#endif\n\tbio->bi_vcnt = 0;\n\n\tatomic_set(&bio->__bi_remaining, 1);\n\tatomic_set(&bio->__bi_cnt, 1);\n\tbio->bi_cookie = BLK_QC_T_NONE;\n\n\tbio->bi_max_vecs = max_vecs;\n\tbio->bi_io_vec = table;\n\tbio->bi_pool = NULL;\n}\nEXPORT_SYMBOL(bio_init);\n\n \nvoid bio_reset(struct bio *bio, struct block_device *bdev, blk_opf_t opf)\n{\n\tbio_uninit(bio);\n\tmemset(bio, 0, BIO_RESET_BYTES);\n\tatomic_set(&bio->__bi_remaining, 1);\n\tbio->bi_bdev = bdev;\n\tif (bio->bi_bdev)\n\t\tbio_associate_blkg(bio);\n\tbio->bi_opf = opf;\n}\nEXPORT_SYMBOL(bio_reset);\n\nstatic struct bio *__bio_chain_endio(struct bio *bio)\n{\n\tstruct bio *parent = bio->bi_private;\n\n\tif (bio->bi_status && !parent->bi_status)\n\t\tparent->bi_status = bio->bi_status;\n\tbio_put(bio);\n\treturn parent;\n}\n\nstatic void bio_chain_endio(struct bio *bio)\n{\n\tbio_endio(__bio_chain_endio(bio));\n}\n\n \nvoid bio_chain(struct bio *bio, struct bio *parent)\n{\n\tBUG_ON(bio->bi_private || bio->bi_end_io);\n\n\tbio->bi_private = parent;\n\tbio->bi_end_io\t= bio_chain_endio;\n\tbio_inc_remaining(parent);\n}\nEXPORT_SYMBOL(bio_chain);\n\nstruct bio *blk_next_bio(struct bio *bio, struct block_device *bdev,\n\t\tunsigned int nr_pages, blk_opf_t opf, gfp_t gfp)\n{\n\tstruct bio *new = bio_alloc(bdev, nr_pages, opf, gfp);\n\n\tif (bio) {\n\t\tbio_chain(bio, new);\n\t\tsubmit_bio(bio);\n\t}\n\n\treturn new;\n}\nEXPORT_SYMBOL_GPL(blk_next_bio);\n\nstatic void bio_alloc_rescue(struct work_struct *work)\n{\n\tstruct bio_set *bs = container_of(work, struct bio_set, rescue_work);\n\tstruct bio *bio;\n\n\twhile (1) {\n\t\tspin_lock(&bs->rescue_lock);\n\t\tbio = bio_list_pop(&bs->rescue_list);\n\t\tspin_unlock(&bs->rescue_lock);\n\n\t\tif (!bio)\n\t\t\tbreak;\n\n\t\tsubmit_bio_noacct(bio);\n\t}\n}\n\nstatic void punt_bios_to_rescuer(struct bio_set *bs)\n{\n\tstruct bio_list punt, nopunt;\n\tstruct bio *bio;\n\n\tif (WARN_ON_ONCE(!bs->rescue_workqueue))\n\t\treturn;\n\t \n\n\tbio_list_init(&punt);\n\tbio_list_init(&nopunt);\n\n\twhile ((bio = bio_list_pop(&current->bio_list[0])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[0] = nopunt;\n\n\tbio_list_init(&nopunt);\n\twhile ((bio = bio_list_pop(&current->bio_list[1])))\n\t\tbio_list_add(bio->bi_pool == bs ? &punt : &nopunt, bio);\n\tcurrent->bio_list[1] = nopunt;\n\n\tspin_lock(&bs->rescue_lock);\n\tbio_list_merge(&bs->rescue_list, &punt);\n\tspin_unlock(&bs->rescue_lock);\n\n\tqueue_work(bs->rescue_workqueue, &bs->rescue_work);\n}\n\nstatic void bio_alloc_irq_cache_splice(struct bio_alloc_cache *cache)\n{\n\tunsigned long flags;\n\n\t \n\tif (WARN_ON_ONCE(cache->free_list))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tcache->free_list = cache->free_list_irq;\n\tcache->free_list_irq = NULL;\n\tcache->nr += cache->nr_irq;\n\tcache->nr_irq = 0;\n\tlocal_irq_restore(flags);\n}\n\nstatic struct bio *bio_alloc_percpu_cache(struct block_device *bdev,\n\t\tunsigned short nr_vecs, blk_opf_t opf, gfp_t gfp,\n\t\tstruct bio_set *bs)\n{\n\tstruct bio_alloc_cache *cache;\n\tstruct bio *bio;\n\n\tcache = per_cpu_ptr(bs->cache, get_cpu());\n\tif (!cache->free_list) {\n\t\tif (READ_ONCE(cache->nr_irq) >= ALLOC_CACHE_THRESHOLD)\n\t\t\tbio_alloc_irq_cache_splice(cache);\n\t\tif (!cache->free_list) {\n\t\t\tput_cpu();\n\t\t\treturn NULL;\n\t\t}\n\t}\n\tbio = cache->free_list;\n\tcache->free_list = bio->bi_next;\n\tcache->nr--;\n\tput_cpu();\n\n\tbio_init(bio, bdev, nr_vecs ? bio->bi_inline_vecs : NULL, nr_vecs, opf);\n\tbio->bi_pool = bs;\n\treturn bio;\n}\n\n \nstruct bio *bio_alloc_bioset(struct block_device *bdev, unsigned short nr_vecs,\n\t\t\t     blk_opf_t opf, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs)\n{\n\tgfp_t saved_gfp = gfp_mask;\n\tstruct bio *bio;\n\tvoid *p;\n\n\t \n\tif (WARN_ON_ONCE(!mempool_initialized(&bs->bvec_pool) && nr_vecs > 0))\n\t\treturn NULL;\n\n\tif (opf & REQ_ALLOC_CACHE) {\n\t\tif (bs->cache && nr_vecs <= BIO_INLINE_VECS) {\n\t\t\tbio = bio_alloc_percpu_cache(bdev, nr_vecs, opf,\n\t\t\t\t\t\t     gfp_mask, bs);\n\t\t\tif (bio)\n\t\t\t\treturn bio;\n\t\t\t \n\t\t} else {\n\t\t\topf &= ~REQ_ALLOC_CACHE;\n\t\t}\n\t}\n\n\t \n\tif (current->bio_list &&\n\t    (!bio_list_empty(&current->bio_list[0]) ||\n\t     !bio_list_empty(&current->bio_list[1])) &&\n\t    bs->rescue_workqueue)\n\t\tgfp_mask &= ~__GFP_DIRECT_RECLAIM;\n\n\tp = mempool_alloc(&bs->bio_pool, gfp_mask);\n\tif (!p && gfp_mask != saved_gfp) {\n\t\tpunt_bios_to_rescuer(bs);\n\t\tgfp_mask = saved_gfp;\n\t\tp = mempool_alloc(&bs->bio_pool, gfp_mask);\n\t}\n\tif (unlikely(!p))\n\t\treturn NULL;\n\tif (!mempool_is_saturated(&bs->bio_pool))\n\t\topf &= ~REQ_ALLOC_CACHE;\n\n\tbio = p + bs->front_pad;\n\tif (nr_vecs > BIO_INLINE_VECS) {\n\t\tstruct bio_vec *bvl = NULL;\n\n\t\tbvl = bvec_alloc(&bs->bvec_pool, &nr_vecs, gfp_mask);\n\t\tif (!bvl && gfp_mask != saved_gfp) {\n\t\t\tpunt_bios_to_rescuer(bs);\n\t\t\tgfp_mask = saved_gfp;\n\t\t\tbvl = bvec_alloc(&bs->bvec_pool, &nr_vecs, gfp_mask);\n\t\t}\n\t\tif (unlikely(!bvl))\n\t\t\tgoto err_free;\n\n\t\tbio_init(bio, bdev, bvl, nr_vecs, opf);\n\t} else if (nr_vecs) {\n\t\tbio_init(bio, bdev, bio->bi_inline_vecs, BIO_INLINE_VECS, opf);\n\t} else {\n\t\tbio_init(bio, bdev, NULL, 0, opf);\n\t}\n\n\tbio->bi_pool = bs;\n\treturn bio;\n\nerr_free:\n\tmempool_free(p, &bs->bio_pool);\n\treturn NULL;\n}\nEXPORT_SYMBOL(bio_alloc_bioset);\n\n \nstruct bio *bio_kmalloc(unsigned short nr_vecs, gfp_t gfp_mask)\n{\n\tstruct bio *bio;\n\n\tif (nr_vecs > UIO_MAXIOV)\n\t\treturn NULL;\n\treturn kmalloc(struct_size(bio, bi_inline_vecs, nr_vecs), gfp_mask);\n}\nEXPORT_SYMBOL(bio_kmalloc);\n\nvoid zero_fill_bio_iter(struct bio *bio, struct bvec_iter start)\n{\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\n\t__bio_for_each_segment(bv, bio, iter, start)\n\t\tmemzero_bvec(&bv);\n}\nEXPORT_SYMBOL(zero_fill_bio_iter);\n\n \nstatic void bio_truncate(struct bio *bio, unsigned new_size)\n{\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\tunsigned int done = 0;\n\tbool truncated = false;\n\n\tif (new_size >= bio->bi_iter.bi_size)\n\t\treturn;\n\n\tif (bio_op(bio) != REQ_OP_READ)\n\t\tgoto exit;\n\n\tbio_for_each_segment(bv, bio, iter) {\n\t\tif (done + bv.bv_len > new_size) {\n\t\t\tunsigned offset;\n\n\t\t\tif (!truncated)\n\t\t\t\toffset = new_size - done;\n\t\t\telse\n\t\t\t\toffset = 0;\n\t\t\tzero_user(bv.bv_page, bv.bv_offset + offset,\n\t\t\t\t  bv.bv_len - offset);\n\t\t\ttruncated = true;\n\t\t}\n\t\tdone += bv.bv_len;\n\t}\n\n exit:\n\t \n\tbio->bi_iter.bi_size = new_size;\n}\n\n \nvoid guard_bio_eod(struct bio *bio)\n{\n\tsector_t maxsector = bdev_nr_sectors(bio->bi_bdev);\n\n\tif (!maxsector)\n\t\treturn;\n\n\t \n\tif (unlikely(bio->bi_iter.bi_sector >= maxsector))\n\t\treturn;\n\n\tmaxsector -= bio->bi_iter.bi_sector;\n\tif (likely((bio->bi_iter.bi_size >> 9) <= maxsector))\n\t\treturn;\n\n\tbio_truncate(bio, maxsector << 9);\n}\n\nstatic int __bio_alloc_cache_prune(struct bio_alloc_cache *cache,\n\t\t\t\t   unsigned int nr)\n{\n\tunsigned int i = 0;\n\tstruct bio *bio;\n\n\twhile ((bio = cache->free_list) != NULL) {\n\t\tcache->free_list = bio->bi_next;\n\t\tcache->nr--;\n\t\tbio_free(bio);\n\t\tif (++i == nr)\n\t\t\tbreak;\n\t}\n\treturn i;\n}\n\nstatic void bio_alloc_cache_prune(struct bio_alloc_cache *cache,\n\t\t\t\t  unsigned int nr)\n{\n\tnr -= __bio_alloc_cache_prune(cache, nr);\n\tif (!READ_ONCE(cache->free_list)) {\n\t\tbio_alloc_irq_cache_splice(cache);\n\t\t__bio_alloc_cache_prune(cache, nr);\n\t}\n}\n\nstatic int bio_cpu_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct bio_set *bs;\n\n\tbs = hlist_entry_safe(node, struct bio_set, cpuhp_dead);\n\tif (bs->cache) {\n\t\tstruct bio_alloc_cache *cache = per_cpu_ptr(bs->cache, cpu);\n\n\t\tbio_alloc_cache_prune(cache, -1U);\n\t}\n\treturn 0;\n}\n\nstatic void bio_alloc_cache_destroy(struct bio_set *bs)\n{\n\tint cpu;\n\n\tif (!bs->cache)\n\t\treturn;\n\n\tcpuhp_state_remove_instance_nocalls(CPUHP_BIO_DEAD, &bs->cpuhp_dead);\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct bio_alloc_cache *cache;\n\n\t\tcache = per_cpu_ptr(bs->cache, cpu);\n\t\tbio_alloc_cache_prune(cache, -1U);\n\t}\n\tfree_percpu(bs->cache);\n\tbs->cache = NULL;\n}\n\nstatic inline void bio_put_percpu_cache(struct bio *bio)\n{\n\tstruct bio_alloc_cache *cache;\n\n\tcache = per_cpu_ptr(bio->bi_pool->cache, get_cpu());\n\tif (READ_ONCE(cache->nr_irq) + cache->nr > ALLOC_CACHE_MAX) {\n\t\tput_cpu();\n\t\tbio_free(bio);\n\t\treturn;\n\t}\n\n\tbio_uninit(bio);\n\n\tif ((bio->bi_opf & REQ_POLLED) && !WARN_ON_ONCE(in_interrupt())) {\n\t\tbio->bi_next = cache->free_list;\n\t\tbio->bi_bdev = NULL;\n\t\tcache->free_list = bio;\n\t\tcache->nr++;\n\t} else {\n\t\tunsigned long flags;\n\n\t\tlocal_irq_save(flags);\n\t\tbio->bi_next = cache->free_list_irq;\n\t\tcache->free_list_irq = bio;\n\t\tcache->nr_irq++;\n\t\tlocal_irq_restore(flags);\n\t}\n\tput_cpu();\n}\n\n \nvoid bio_put(struct bio *bio)\n{\n\tif (unlikely(bio_flagged(bio, BIO_REFFED))) {\n\t\tBUG_ON(!atomic_read(&bio->__bi_cnt));\n\t\tif (!atomic_dec_and_test(&bio->__bi_cnt))\n\t\t\treturn;\n\t}\n\tif (bio->bi_opf & REQ_ALLOC_CACHE)\n\t\tbio_put_percpu_cache(bio);\n\telse\n\t\tbio_free(bio);\n}\nEXPORT_SYMBOL(bio_put);\n\nstatic int __bio_clone(struct bio *bio, struct bio *bio_src, gfp_t gfp)\n{\n\tbio_set_flag(bio, BIO_CLONED);\n\tbio->bi_ioprio = bio_src->bi_ioprio;\n\tbio->bi_iter = bio_src->bi_iter;\n\n\tif (bio->bi_bdev) {\n\t\tif (bio->bi_bdev == bio_src->bi_bdev &&\n\t\t    bio_flagged(bio_src, BIO_REMAPPED))\n\t\t\tbio_set_flag(bio, BIO_REMAPPED);\n\t\tbio_clone_blkg_association(bio, bio_src);\n\t}\n\n\tif (bio_crypt_clone(bio, bio_src, gfp) < 0)\n\t\treturn -ENOMEM;\n\tif (bio_integrity(bio_src) &&\n\t    bio_integrity_clone(bio, bio_src, gfp) < 0)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n \nstruct bio *bio_alloc_clone(struct block_device *bdev, struct bio *bio_src,\n\t\tgfp_t gfp, struct bio_set *bs)\n{\n\tstruct bio *bio;\n\n\tbio = bio_alloc_bioset(bdev, 0, bio_src->bi_opf, gfp, bs);\n\tif (!bio)\n\t\treturn NULL;\n\n\tif (__bio_clone(bio, bio_src, gfp) < 0) {\n\t\tbio_put(bio);\n\t\treturn NULL;\n\t}\n\tbio->bi_io_vec = bio_src->bi_io_vec;\n\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_alloc_clone);\n\n \nint bio_init_clone(struct block_device *bdev, struct bio *bio,\n\t\tstruct bio *bio_src, gfp_t gfp)\n{\n\tint ret;\n\n\tbio_init(bio, bdev, bio_src->bi_io_vec, 0, bio_src->bi_opf);\n\tret = __bio_clone(bio, bio_src, gfp);\n\tif (ret)\n\t\tbio_uninit(bio);\n\treturn ret;\n}\nEXPORT_SYMBOL(bio_init_clone);\n\n \nstatic inline bool bio_full(struct bio *bio, unsigned len)\n{\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn true;\n\tif (bio->bi_iter.bi_size > UINT_MAX - len)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool bvec_try_merge_page(struct bio_vec *bv, struct page *page,\n\t\tunsigned int len, unsigned int off, bool *same_page)\n{\n\tsize_t bv_end = bv->bv_offset + bv->bv_len;\n\tphys_addr_t vec_end_addr = page_to_phys(bv->bv_page) + bv_end - 1;\n\tphys_addr_t page_addr = page_to_phys(page);\n\n\tif (vec_end_addr + 1 != page_addr + off)\n\t\treturn false;\n\tif (xen_domain() && !xen_biovec_phys_mergeable(bv, page))\n\t\treturn false;\n\tif (!zone_device_pages_have_same_pgmap(bv->bv_page, page))\n\t\treturn false;\n\n\t*same_page = ((vec_end_addr & PAGE_MASK) == page_addr);\n\tif (!*same_page) {\n\t\tif (IS_ENABLED(CONFIG_KMSAN))\n\t\t\treturn false;\n\t\tif (bv->bv_page + bv_end / PAGE_SIZE != page + off / PAGE_SIZE)\n\t\t\treturn false;\n\t}\n\n\tbv->bv_len += len;\n\treturn true;\n}\n\n \nbool bvec_try_merge_hw_page(struct request_queue *q, struct bio_vec *bv,\n\t\tstruct page *page, unsigned len, unsigned offset,\n\t\tbool *same_page)\n{\n\tunsigned long mask = queue_segment_boundary(q);\n\tphys_addr_t addr1 = page_to_phys(bv->bv_page) + bv->bv_offset;\n\tphys_addr_t addr2 = page_to_phys(page) + offset + len - 1;\n\n\tif ((addr1 | mask) != (addr2 | mask))\n\t\treturn false;\n\tif (bv->bv_len + len > queue_max_segment_size(q))\n\t\treturn false;\n\treturn bvec_try_merge_page(bv, page, len, offset, same_page);\n}\n\n \nint bio_add_hw_page(struct request_queue *q, struct bio *bio,\n\t\tstruct page *page, unsigned int len, unsigned int offset,\n\t\tunsigned int max_sectors, bool *same_page)\n{\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\n\tif (((bio->bi_iter.bi_size + len) >> SECTOR_SHIFT) > max_sectors)\n\t\treturn 0;\n\n\tif (bio->bi_vcnt > 0) {\n\t\tstruct bio_vec *bv = &bio->bi_io_vec[bio->bi_vcnt - 1];\n\n\t\tif (bvec_try_merge_hw_page(q, bv, page, len, offset,\n\t\t\t\tsame_page)) {\n\t\t\tbio->bi_iter.bi_size += len;\n\t\t\treturn len;\n\t\t}\n\n\t\tif (bio->bi_vcnt >=\n\t\t    min(bio->bi_max_vecs, queue_max_segments(q)))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (bvec_gap_to_prev(&q->limits, bv, offset))\n\t\t\treturn 0;\n\t}\n\n\tbvec_set_page(&bio->bi_io_vec[bio->bi_vcnt], page, len, offset);\n\tbio->bi_vcnt++;\n\tbio->bi_iter.bi_size += len;\n\treturn len;\n}\n\n \nint bio_add_pc_page(struct request_queue *q, struct bio *bio,\n\t\tstruct page *page, unsigned int len, unsigned int offset)\n{\n\tbool same_page = false;\n\treturn bio_add_hw_page(q, bio, page, len, offset,\n\t\t\tqueue_max_hw_sectors(q), &same_page);\n}\nEXPORT_SYMBOL(bio_add_pc_page);\n\n \nint bio_add_zone_append_page(struct bio *bio, struct page *page,\n\t\t\t     unsigned int len, unsigned int offset)\n{\n\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\tbool same_page = false;\n\n\tif (WARN_ON_ONCE(bio_op(bio) != REQ_OP_ZONE_APPEND))\n\t\treturn 0;\n\n\tif (WARN_ON_ONCE(!bdev_is_zoned(bio->bi_bdev)))\n\t\treturn 0;\n\n\treturn bio_add_hw_page(q, bio, page, len, offset,\n\t\t\t       queue_max_zone_append_sectors(q), &same_page);\n}\nEXPORT_SYMBOL_GPL(bio_add_zone_append_page);\n\n \nvoid __bio_add_page(struct bio *bio, struct page *page,\n\t\tunsigned int len, unsigned int off)\n{\n\tWARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));\n\tWARN_ON_ONCE(bio_full(bio, len));\n\n\tbvec_set_page(&bio->bi_io_vec[bio->bi_vcnt], page, len, off);\n\tbio->bi_iter.bi_size += len;\n\tbio->bi_vcnt++;\n}\nEXPORT_SYMBOL_GPL(__bio_add_page);\n\n \nint bio_add_page(struct bio *bio, struct page *page,\n\t\t unsigned int len, unsigned int offset)\n{\n\tbool same_page = false;\n\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn 0;\n\tif (bio->bi_iter.bi_size > UINT_MAX - len)\n\t\treturn 0;\n\n\tif (bio->bi_vcnt > 0 &&\n\t    bvec_try_merge_page(&bio->bi_io_vec[bio->bi_vcnt - 1],\n\t\t\t\tpage, len, offset, &same_page)) {\n\t\tbio->bi_iter.bi_size += len;\n\t\treturn len;\n\t}\n\n\tif (bio->bi_vcnt >= bio->bi_max_vecs)\n\t\treturn 0;\n\t__bio_add_page(bio, page, len, offset);\n\treturn len;\n}\nEXPORT_SYMBOL(bio_add_page);\n\nvoid bio_add_folio_nofail(struct bio *bio, struct folio *folio, size_t len,\n\t\t\t  size_t off)\n{\n\tWARN_ON_ONCE(len > UINT_MAX);\n\tWARN_ON_ONCE(off > UINT_MAX);\n\t__bio_add_page(bio, &folio->page, len, off);\n}\n\n \nbool bio_add_folio(struct bio *bio, struct folio *folio, size_t len,\n\t\t   size_t off)\n{\n\tif (len > UINT_MAX || off > UINT_MAX)\n\t\treturn false;\n\treturn bio_add_page(bio, &folio->page, len, off) > 0;\n}\nEXPORT_SYMBOL(bio_add_folio);\n\nvoid __bio_release_pages(struct bio *bio, bool mark_dirty)\n{\n\tstruct folio_iter fi;\n\n\tbio_for_each_folio_all(fi, bio) {\n\t\tstruct page *page;\n\t\tsize_t done = 0;\n\n\t\tif (mark_dirty) {\n\t\t\tfolio_lock(fi.folio);\n\t\t\tfolio_mark_dirty(fi.folio);\n\t\t\tfolio_unlock(fi.folio);\n\t\t}\n\t\tpage = folio_page(fi.folio, fi.offset / PAGE_SIZE);\n\t\tdo {\n\t\t\tbio_release_page(bio, page++);\n\t\t\tdone += PAGE_SIZE;\n\t\t} while (done < fi.length);\n\t}\n}\nEXPORT_SYMBOL_GPL(__bio_release_pages);\n\nvoid bio_iov_bvec_set(struct bio *bio, struct iov_iter *iter)\n{\n\tsize_t size = iov_iter_count(iter);\n\n\tWARN_ON_ONCE(bio->bi_max_vecs);\n\n\tif (bio_op(bio) == REQ_OP_ZONE_APPEND) {\n\t\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\t\tsize_t max_sectors = queue_max_zone_append_sectors(q);\n\n\t\tsize = min(size, max_sectors << SECTOR_SHIFT);\n\t}\n\n\tbio->bi_vcnt = iter->nr_segs;\n\tbio->bi_io_vec = (struct bio_vec *)iter->bvec;\n\tbio->bi_iter.bi_bvec_done = iter->iov_offset;\n\tbio->bi_iter.bi_size = size;\n\tbio_set_flag(bio, BIO_CLONED);\n}\n\nstatic int bio_iov_add_page(struct bio *bio, struct page *page,\n\t\tunsigned int len, unsigned int offset)\n{\n\tbool same_page = false;\n\n\tif (WARN_ON_ONCE(bio->bi_iter.bi_size > UINT_MAX - len))\n\t\treturn -EIO;\n\n\tif (bio->bi_vcnt > 0 &&\n\t    bvec_try_merge_page(&bio->bi_io_vec[bio->bi_vcnt - 1],\n\t\t\t\tpage, len, offset, &same_page)) {\n\t\tbio->bi_iter.bi_size += len;\n\t\tif (same_page)\n\t\t\tbio_release_page(bio, page);\n\t\treturn 0;\n\t}\n\t__bio_add_page(bio, page, len, offset);\n\treturn 0;\n}\n\nstatic int bio_iov_add_zone_append_page(struct bio *bio, struct page *page,\n\t\tunsigned int len, unsigned int offset)\n{\n\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\tbool same_page = false;\n\n\tif (bio_add_hw_page(q, bio, page, len, offset,\n\t\t\tqueue_max_zone_append_sectors(q), &same_page) != len)\n\t\treturn -EINVAL;\n\tif (same_page)\n\t\tbio_release_page(bio, page);\n\treturn 0;\n}\n\n#define PAGE_PTRS_PER_BVEC     (sizeof(struct bio_vec) / sizeof(struct page *))\n\n \nstatic int __bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)\n{\n\tiov_iter_extraction_t extraction_flags = 0;\n\tunsigned short nr_pages = bio->bi_max_vecs - bio->bi_vcnt;\n\tunsigned short entries_left = bio->bi_max_vecs - bio->bi_vcnt;\n\tstruct bio_vec *bv = bio->bi_io_vec + bio->bi_vcnt;\n\tstruct page **pages = (struct page **)bv;\n\tssize_t size, left;\n\tunsigned len, i = 0;\n\tsize_t offset;\n\tint ret = 0;\n\n\t \n\tBUILD_BUG_ON(PAGE_PTRS_PER_BVEC < 2);\n\tpages += entries_left * (PAGE_PTRS_PER_BVEC - 1);\n\n\tif (bio->bi_bdev && blk_queue_pci_p2pdma(bio->bi_bdev->bd_disk->queue))\n\t\textraction_flags |= ITER_ALLOW_P2PDMA;\n\n\t \n\tsize = iov_iter_extract_pages(iter, &pages,\n\t\t\t\t      UINT_MAX - bio->bi_iter.bi_size,\n\t\t\t\t      nr_pages, extraction_flags, &offset);\n\tif (unlikely(size <= 0))\n\t\treturn size ? size : -EFAULT;\n\n\tnr_pages = DIV_ROUND_UP(offset + size, PAGE_SIZE);\n\n\tif (bio->bi_bdev) {\n\t\tsize_t trim = size & (bdev_logical_block_size(bio->bi_bdev) - 1);\n\t\tiov_iter_revert(iter, trim);\n\t\tsize -= trim;\n\t}\n\n\tif (unlikely(!size)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tfor (left = size, i = 0; left > 0; left -= len, i++) {\n\t\tstruct page *page = pages[i];\n\n\t\tlen = min_t(size_t, PAGE_SIZE - offset, left);\n\t\tif (bio_op(bio) == REQ_OP_ZONE_APPEND) {\n\t\t\tret = bio_iov_add_zone_append_page(bio, page, len,\n\t\t\t\t\toffset);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t} else\n\t\t\tbio_iov_add_page(bio, page, len, offset);\n\n\t\toffset = 0;\n\t}\n\n\tiov_iter_revert(iter, left);\nout:\n\twhile (i < nr_pages)\n\t\tbio_release_page(bio, pages[i++]);\n\n\treturn ret;\n}\n\n \nint bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)\n{\n\tint ret = 0;\n\n\tif (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))\n\t\treturn -EIO;\n\n\tif (iov_iter_is_bvec(iter)) {\n\t\tbio_iov_bvec_set(bio, iter);\n\t\tiov_iter_advance(iter, bio->bi_iter.bi_size);\n\t\treturn 0;\n\t}\n\n\tif (iov_iter_extract_will_pin(iter))\n\t\tbio_set_flag(bio, BIO_PAGE_PINNED);\n\tdo {\n\t\tret = __bio_iov_iter_get_pages(bio, iter);\n\t} while (!ret && iov_iter_count(iter) && !bio_full(bio, 0));\n\n\treturn bio->bi_vcnt ? 0 : ret;\n}\nEXPORT_SYMBOL_GPL(bio_iov_iter_get_pages);\n\nstatic void submit_bio_wait_endio(struct bio *bio)\n{\n\tcomplete(bio->bi_private);\n}\n\n \nint submit_bio_wait(struct bio *bio)\n{\n\tDECLARE_COMPLETION_ONSTACK_MAP(done,\n\t\t\tbio->bi_bdev->bd_disk->lockdep_map);\n\tunsigned long hang_check;\n\n\tbio->bi_private = &done;\n\tbio->bi_end_io = submit_bio_wait_endio;\n\tbio->bi_opf |= REQ_SYNC;\n\tsubmit_bio(bio);\n\n\t \n\thang_check = sysctl_hung_task_timeout_secs;\n\tif (hang_check)\n\t\twhile (!wait_for_completion_io_timeout(&done,\n\t\t\t\t\thang_check * (HZ/2)))\n\t\t\t;\n\telse\n\t\twait_for_completion_io(&done);\n\n\treturn blk_status_to_errno(bio->bi_status);\n}\nEXPORT_SYMBOL(submit_bio_wait);\n\nvoid __bio_advance(struct bio *bio, unsigned bytes)\n{\n\tif (bio_integrity(bio))\n\t\tbio_integrity_advance(bio, bytes);\n\n\tbio_crypt_advance(bio, bytes);\n\tbio_advance_iter(bio, &bio->bi_iter, bytes);\n}\nEXPORT_SYMBOL(__bio_advance);\n\nvoid bio_copy_data_iter(struct bio *dst, struct bvec_iter *dst_iter,\n\t\t\tstruct bio *src, struct bvec_iter *src_iter)\n{\n\twhile (src_iter->bi_size && dst_iter->bi_size) {\n\t\tstruct bio_vec src_bv = bio_iter_iovec(src, *src_iter);\n\t\tstruct bio_vec dst_bv = bio_iter_iovec(dst, *dst_iter);\n\t\tunsigned int bytes = min(src_bv.bv_len, dst_bv.bv_len);\n\t\tvoid *src_buf = bvec_kmap_local(&src_bv);\n\t\tvoid *dst_buf = bvec_kmap_local(&dst_bv);\n\n\t\tmemcpy(dst_buf, src_buf, bytes);\n\n\t\tkunmap_local(dst_buf);\n\t\tkunmap_local(src_buf);\n\n\t\tbio_advance_iter_single(src, src_iter, bytes);\n\t\tbio_advance_iter_single(dst, dst_iter, bytes);\n\t}\n}\nEXPORT_SYMBOL(bio_copy_data_iter);\n\n \nvoid bio_copy_data(struct bio *dst, struct bio *src)\n{\n\tstruct bvec_iter src_iter = src->bi_iter;\n\tstruct bvec_iter dst_iter = dst->bi_iter;\n\n\tbio_copy_data_iter(dst, &dst_iter, src, &src_iter);\n}\nEXPORT_SYMBOL(bio_copy_data);\n\nvoid bio_free_pages(struct bio *bio)\n{\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all)\n\t\t__free_page(bvec->bv_page);\n}\nEXPORT_SYMBOL(bio_free_pages);\n\n \n\n \nvoid bio_set_pages_dirty(struct bio *bio)\n{\n\tstruct folio_iter fi;\n\n\tbio_for_each_folio_all(fi, bio) {\n\t\tfolio_lock(fi.folio);\n\t\tfolio_mark_dirty(fi.folio);\n\t\tfolio_unlock(fi.folio);\n\t}\n}\nEXPORT_SYMBOL_GPL(bio_set_pages_dirty);\n\n \n\nstatic void bio_dirty_fn(struct work_struct *work);\n\nstatic DECLARE_WORK(bio_dirty_work, bio_dirty_fn);\nstatic DEFINE_SPINLOCK(bio_dirty_lock);\nstatic struct bio *bio_dirty_list;\n\n \nstatic void bio_dirty_fn(struct work_struct *work)\n{\n\tstruct bio *bio, *next;\n\n\tspin_lock_irq(&bio_dirty_lock);\n\tnext = bio_dirty_list;\n\tbio_dirty_list = NULL;\n\tspin_unlock_irq(&bio_dirty_lock);\n\n\twhile ((bio = next) != NULL) {\n\t\tnext = bio->bi_private;\n\n\t\tbio_release_pages(bio, true);\n\t\tbio_put(bio);\n\t}\n}\n\nvoid bio_check_pages_dirty(struct bio *bio)\n{\n\tstruct folio_iter fi;\n\tunsigned long flags;\n\n\tbio_for_each_folio_all(fi, bio) {\n\t\tif (!folio_test_dirty(fi.folio))\n\t\t\tgoto defer;\n\t}\n\n\tbio_release_pages(bio, false);\n\tbio_put(bio);\n\treturn;\ndefer:\n\tspin_lock_irqsave(&bio_dirty_lock, flags);\n\tbio->bi_private = bio_dirty_list;\n\tbio_dirty_list = bio;\n\tspin_unlock_irqrestore(&bio_dirty_lock, flags);\n\tschedule_work(&bio_dirty_work);\n}\nEXPORT_SYMBOL_GPL(bio_check_pages_dirty);\n\nstatic inline bool bio_remaining_done(struct bio *bio)\n{\n\t \n\tif (!bio_flagged(bio, BIO_CHAIN))\n\t\treturn true;\n\n\tBUG_ON(atomic_read(&bio->__bi_remaining) <= 0);\n\n\tif (atomic_dec_and_test(&bio->__bi_remaining)) {\n\t\tbio_clear_flag(bio, BIO_CHAIN);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nvoid bio_endio(struct bio *bio)\n{\nagain:\n\tif (!bio_remaining_done(bio))\n\t\treturn;\n\tif (!bio_integrity_endio(bio))\n\t\treturn;\n\n\trq_qos_done_bio(bio);\n\n\tif (bio->bi_bdev && bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_complete(bdev_get_queue(bio->bi_bdev), bio);\n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\n\t \n\tif (bio->bi_end_io == bio_chain_endio) {\n\t\tbio = __bio_chain_endio(bio);\n\t\tgoto again;\n\t}\n\n\tblk_throtl_bio_endio(bio);\n\t \n\tbio_uninit(bio);\n\tif (bio->bi_end_io)\n\t\tbio->bi_end_io(bio);\n}\nEXPORT_SYMBOL(bio_endio);\n\n \nstruct bio *bio_split(struct bio *bio, int sectors,\n\t\t      gfp_t gfp, struct bio_set *bs)\n{\n\tstruct bio *split;\n\n\tBUG_ON(sectors <= 0);\n\tBUG_ON(sectors >= bio_sectors(bio));\n\n\t \n\tif (WARN_ON_ONCE(bio_op(bio) == REQ_OP_ZONE_APPEND))\n\t\treturn NULL;\n\n\tsplit = bio_alloc_clone(bio->bi_bdev, bio, gfp, bs);\n\tif (!split)\n\t\treturn NULL;\n\n\tsplit->bi_iter.bi_size = sectors << 9;\n\n\tif (bio_integrity(split))\n\t\tbio_integrity_trim(split);\n\n\tbio_advance(bio, split->bi_iter.bi_size);\n\n\tif (bio_flagged(bio, BIO_TRACE_COMPLETION))\n\t\tbio_set_flag(split, BIO_TRACE_COMPLETION);\n\n\treturn split;\n}\nEXPORT_SYMBOL(bio_split);\n\n \nvoid bio_trim(struct bio *bio, sector_t offset, sector_t size)\n{\n\tif (WARN_ON_ONCE(offset > BIO_MAX_SECTORS || size > BIO_MAX_SECTORS ||\n\t\t\t offset + size > bio_sectors(bio)))\n\t\treturn;\n\n\tsize <<= 9;\n\tif (offset == 0 && size == bio->bi_iter.bi_size)\n\t\treturn;\n\n\tbio_advance(bio, offset << 9);\n\tbio->bi_iter.bi_size = size;\n\n\tif (bio_integrity(bio))\n\t\tbio_integrity_trim(bio);\n}\nEXPORT_SYMBOL_GPL(bio_trim);\n\n \nint biovec_init_pool(mempool_t *pool, int pool_entries)\n{\n\tstruct biovec_slab *bp = bvec_slabs + ARRAY_SIZE(bvec_slabs) - 1;\n\n\treturn mempool_init_slab_pool(pool, pool_entries, bp->slab);\n}\n\n \nvoid bioset_exit(struct bio_set *bs)\n{\n\tbio_alloc_cache_destroy(bs);\n\tif (bs->rescue_workqueue)\n\t\tdestroy_workqueue(bs->rescue_workqueue);\n\tbs->rescue_workqueue = NULL;\n\n\tmempool_exit(&bs->bio_pool);\n\tmempool_exit(&bs->bvec_pool);\n\n\tbioset_integrity_free(bs);\n\tif (bs->bio_slab)\n\t\tbio_put_slab(bs);\n\tbs->bio_slab = NULL;\n}\nEXPORT_SYMBOL(bioset_exit);\n\n \nint bioset_init(struct bio_set *bs,\n\t\tunsigned int pool_size,\n\t\tunsigned int front_pad,\n\t\tint flags)\n{\n\tbs->front_pad = front_pad;\n\tif (flags & BIOSET_NEED_BVECS)\n\t\tbs->back_pad = BIO_INLINE_VECS * sizeof(struct bio_vec);\n\telse\n\t\tbs->back_pad = 0;\n\n\tspin_lock_init(&bs->rescue_lock);\n\tbio_list_init(&bs->rescue_list);\n\tINIT_WORK(&bs->rescue_work, bio_alloc_rescue);\n\n\tbs->bio_slab = bio_find_or_create_slab(bs);\n\tif (!bs->bio_slab)\n\t\treturn -ENOMEM;\n\n\tif (mempool_init_slab_pool(&bs->bio_pool, pool_size, bs->bio_slab))\n\t\tgoto bad;\n\n\tif ((flags & BIOSET_NEED_BVECS) &&\n\t    biovec_init_pool(&bs->bvec_pool, pool_size))\n\t\tgoto bad;\n\n\tif (flags & BIOSET_NEED_RESCUER) {\n\t\tbs->rescue_workqueue = alloc_workqueue(\"bioset\",\n\t\t\t\t\t\t\tWQ_MEM_RECLAIM, 0);\n\t\tif (!bs->rescue_workqueue)\n\t\t\tgoto bad;\n\t}\n\tif (flags & BIOSET_PERCPU_CACHE) {\n\t\tbs->cache = alloc_percpu(struct bio_alloc_cache);\n\t\tif (!bs->cache)\n\t\t\tgoto bad;\n\t\tcpuhp_state_add_instance_nocalls(CPUHP_BIO_DEAD, &bs->cpuhp_dead);\n\t}\n\n\treturn 0;\nbad:\n\tbioset_exit(bs);\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(bioset_init);\n\nstatic int __init init_bio(void)\n{\n\tint i;\n\n\tBUILD_BUG_ON(BIO_FLAG_LAST > 8 * sizeof_field(struct bio, bi_flags));\n\n\tbio_integrity_init();\n\n\tfor (i = 0; i < ARRAY_SIZE(bvec_slabs); i++) {\n\t\tstruct biovec_slab *bvs = bvec_slabs + i;\n\n\t\tbvs->slab = kmem_cache_create(bvs->name,\n\t\t\t\tbvs->nr_vecs * sizeof(struct bio_vec), 0,\n\t\t\t\tSLAB_HWCACHE_ALIGN | SLAB_PANIC, NULL);\n\t}\n\n\tcpuhp_setup_state_multi(CPUHP_BIO_DEAD, \"block/bio:dead\", NULL,\n\t\t\t\t\tbio_cpu_dead);\n\n\tif (bioset_init(&fs_bio_set, BIO_POOL_SIZE, 0,\n\t\t\tBIOSET_NEED_BVECS | BIOSET_PERCPU_CACHE))\n\t\tpanic(\"bio: can't allocate bios\\n\");\n\n\tif (bioset_integrity_create(&fs_bio_set, BIO_POOL_SIZE))\n\t\tpanic(\"bio: can't create integrity pool\\n\");\n\n\treturn 0;\n}\nsubsys_initcall(init_bio);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}