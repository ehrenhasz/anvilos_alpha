{
  "module_name": "blk-stat.c",
  "hash_id": "b8c90832fd629c268fe1e7303ab38a325151d0dbcdbfb3c81d5f91a94c1ffd37",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-stat.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/rculist.h>\n\n#include \"blk-stat.h\"\n#include \"blk-mq.h\"\n#include \"blk.h\"\n\nstruct blk_queue_stats {\n\tstruct list_head callbacks;\n\tspinlock_t lock;\n\tint accounting;\n};\n\nvoid blk_rq_stat_init(struct blk_rq_stat *stat)\n{\n\tstat->min = -1ULL;\n\tstat->max = stat->nr_samples = stat->mean = 0;\n\tstat->batch = 0;\n}\n\n \nvoid blk_rq_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)\n{\n\tif (!src->nr_samples)\n\t\treturn;\n\n\tdst->min = min(dst->min, src->min);\n\tdst->max = max(dst->max, src->max);\n\n\tdst->mean = div_u64(src->batch + dst->mean * dst->nr_samples,\n\t\t\t\tdst->nr_samples + src->nr_samples);\n\n\tdst->nr_samples += src->nr_samples;\n}\n\nvoid blk_rq_stat_add(struct blk_rq_stat *stat, u64 value)\n{\n\tstat->min = min(stat->min, value);\n\tstat->max = max(stat->max, value);\n\tstat->batch += value;\n\tstat->nr_samples++;\n}\n\nvoid blk_stat_add(struct request *rq, u64 now)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_stat_callback *cb;\n\tstruct blk_rq_stat *stat;\n\tint bucket, cpu;\n\tu64 value;\n\n\tvalue = (now >= rq->io_start_time_ns) ? now - rq->io_start_time_ns : 0;\n\n\tif (req_op(rq) == REQ_OP_READ || req_op(rq) == REQ_OP_WRITE)\n\t\tblk_throtl_stat_add(rq, value);\n\n\trcu_read_lock();\n\tcpu = get_cpu();\n\tlist_for_each_entry_rcu(cb, &q->stats->callbacks, list) {\n\t\tif (!blk_stat_is_active(cb))\n\t\t\tcontinue;\n\n\t\tbucket = cb->bucket_fn(rq);\n\t\tif (bucket < 0)\n\t\t\tcontinue;\n\n\t\tstat = &per_cpu_ptr(cb->cpu_stat, cpu)[bucket];\n\t\tblk_rq_stat_add(stat, value);\n\t}\n\tput_cpu();\n\trcu_read_unlock();\n}\n\nstatic void blk_stat_timer_fn(struct timer_list *t)\n{\n\tstruct blk_stat_callback *cb = from_timer(cb, t, timer);\n\tunsigned int bucket;\n\tint cpu;\n\n\tfor (bucket = 0; bucket < cb->buckets; bucket++)\n\t\tblk_rq_stat_init(&cb->stat[bucket]);\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct blk_rq_stat *cpu_stat;\n\n\t\tcpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);\n\t\tfor (bucket = 0; bucket < cb->buckets; bucket++) {\n\t\t\tblk_rq_stat_sum(&cb->stat[bucket], &cpu_stat[bucket]);\n\t\t\tblk_rq_stat_init(&cpu_stat[bucket]);\n\t\t}\n\t}\n\n\tcb->timer_fn(cb);\n}\n\nstruct blk_stat_callback *\nblk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),\n\t\t\tint (*bucket_fn)(const struct request *),\n\t\t\tunsigned int buckets, void *data)\n{\n\tstruct blk_stat_callback *cb;\n\n\tcb = kmalloc(sizeof(*cb), GFP_KERNEL);\n\tif (!cb)\n\t\treturn NULL;\n\n\tcb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),\n\t\t\t\t GFP_KERNEL);\n\tif (!cb->stat) {\n\t\tkfree(cb);\n\t\treturn NULL;\n\t}\n\tcb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),\n\t\t\t\t      __alignof__(struct blk_rq_stat));\n\tif (!cb->cpu_stat) {\n\t\tkfree(cb->stat);\n\t\tkfree(cb);\n\t\treturn NULL;\n\t}\n\n\tcb->timer_fn = timer_fn;\n\tcb->bucket_fn = bucket_fn;\n\tcb->data = data;\n\tcb->buckets = buckets;\n\ttimer_setup(&cb->timer, blk_stat_timer_fn, 0);\n\n\treturn cb;\n}\n\nvoid blk_stat_add_callback(struct request_queue *q,\n\t\t\t   struct blk_stat_callback *cb)\n{\n\tunsigned int bucket;\n\tunsigned long flags;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct blk_rq_stat *cpu_stat;\n\n\t\tcpu_stat = per_cpu_ptr(cb->cpu_stat, cpu);\n\t\tfor (bucket = 0; bucket < cb->buckets; bucket++)\n\t\t\tblk_rq_stat_init(&cpu_stat[bucket]);\n\t}\n\n\tspin_lock_irqsave(&q->stats->lock, flags);\n\tlist_add_tail_rcu(&cb->list, &q->stats->callbacks);\n\tblk_queue_flag_set(QUEUE_FLAG_STATS, q);\n\tspin_unlock_irqrestore(&q->stats->lock, flags);\n}\n\nvoid blk_stat_remove_callback(struct request_queue *q,\n\t\t\t      struct blk_stat_callback *cb)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->stats->lock, flags);\n\tlist_del_rcu(&cb->list);\n\tif (list_empty(&q->stats->callbacks) && !q->stats->accounting)\n\t\tblk_queue_flag_clear(QUEUE_FLAG_STATS, q);\n\tspin_unlock_irqrestore(&q->stats->lock, flags);\n\n\tdel_timer_sync(&cb->timer);\n}\n\nstatic void blk_stat_free_callback_rcu(struct rcu_head *head)\n{\n\tstruct blk_stat_callback *cb;\n\n\tcb = container_of(head, struct blk_stat_callback, rcu);\n\tfree_percpu(cb->cpu_stat);\n\tkfree(cb->stat);\n\tkfree(cb);\n}\n\nvoid blk_stat_free_callback(struct blk_stat_callback *cb)\n{\n\tif (cb)\n\t\tcall_rcu(&cb->rcu, blk_stat_free_callback_rcu);\n}\n\nvoid blk_stat_disable_accounting(struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->stats->lock, flags);\n\tif (!--q->stats->accounting && list_empty(&q->stats->callbacks))\n\t\tblk_queue_flag_clear(QUEUE_FLAG_STATS, q);\n\tspin_unlock_irqrestore(&q->stats->lock, flags);\n}\nEXPORT_SYMBOL_GPL(blk_stat_disable_accounting);\n\nvoid blk_stat_enable_accounting(struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->stats->lock, flags);\n\tif (!q->stats->accounting++ && list_empty(&q->stats->callbacks))\n\t\tblk_queue_flag_set(QUEUE_FLAG_STATS, q);\n\tspin_unlock_irqrestore(&q->stats->lock, flags);\n}\nEXPORT_SYMBOL_GPL(blk_stat_enable_accounting);\n\nstruct blk_queue_stats *blk_alloc_queue_stats(void)\n{\n\tstruct blk_queue_stats *stats;\n\n\tstats = kmalloc(sizeof(*stats), GFP_KERNEL);\n\tif (!stats)\n\t\treturn NULL;\n\n\tINIT_LIST_HEAD(&stats->callbacks);\n\tspin_lock_init(&stats->lock);\n\tstats->accounting = 0;\n\n\treturn stats;\n}\n\nvoid blk_free_queue_stats(struct blk_queue_stats *stats)\n{\n\tif (!stats)\n\t\treturn;\n\n\tWARN_ON(!list_empty(&stats->callbacks));\n\n\tkfree(stats);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}