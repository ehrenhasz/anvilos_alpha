{
  "module_name": "mq-deadline.c",
  "hash_id": "9bcc163562495becffe6a57effdff406b1e5b91625e6fb6a63c17d143dc272ac",
  "original_prompt": "Ingested from linux-6.6.14/block/mq-deadline.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/fs.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/compiler.h>\n#include <linux/rbtree.h>\n#include <linux/sbitmap.h>\n\n#include <trace/events/block.h>\n\n#include \"elevator.h\"\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-mq-sched.h\"\n\n \nstatic const int read_expire = HZ / 2;   \nstatic const int write_expire = 5 * HZ;  \n \nstatic const int prio_aging_expire = 10 * HZ;\nstatic const int writes_starved = 2;     \nstatic const int fifo_batch = 16;        \n\nenum dd_data_dir {\n\tDD_READ\t\t= READ,\n\tDD_WRITE\t= WRITE,\n};\n\nenum { DD_DIR_COUNT = 2 };\n\nenum dd_prio {\n\tDD_RT_PRIO\t= 0,\n\tDD_BE_PRIO\t= 1,\n\tDD_IDLE_PRIO\t= 2,\n\tDD_PRIO_MAX\t= 2,\n};\n\nenum { DD_PRIO_COUNT = 3 };\n\n \nstruct io_stats_per_prio {\n\tuint32_t inserted;\n\tuint32_t merged;\n\tuint32_t dispatched;\n\tatomic_t completed;\n};\n\n \nstruct dd_per_prio {\n\tstruct list_head dispatch;\n\tstruct rb_root sort_list[DD_DIR_COUNT];\n\tstruct list_head fifo_list[DD_DIR_COUNT];\n\t \n\tsector_t latest_pos[DD_DIR_COUNT];\n\tstruct io_stats_per_prio stats;\n};\n\nstruct deadline_data {\n\t \n\n\tstruct dd_per_prio per_prio[DD_PRIO_COUNT];\n\n\t \n\tenum dd_data_dir last_dir;\n\tunsigned int batching;\t\t \n\tunsigned int starved;\t\t \n\n\t \n\tint fifo_expire[DD_DIR_COUNT];\n\tint fifo_batch;\n\tint writes_starved;\n\tint front_merges;\n\tu32 async_depth;\n\tint prio_aging_expire;\n\n\tspinlock_t lock;\n\tspinlock_t zone_lock;\n};\n\n \nstatic const enum dd_prio ioprio_class_to_prio[] = {\n\t[IOPRIO_CLASS_NONE]\t= DD_BE_PRIO,\n\t[IOPRIO_CLASS_RT]\t= DD_RT_PRIO,\n\t[IOPRIO_CLASS_BE]\t= DD_BE_PRIO,\n\t[IOPRIO_CLASS_IDLE]\t= DD_IDLE_PRIO,\n};\n\nstatic inline struct rb_root *\ndeadline_rb_root(struct dd_per_prio *per_prio, struct request *rq)\n{\n\treturn &per_prio->sort_list[rq_data_dir(rq)];\n}\n\n \nstatic u8 dd_rq_ioclass(struct request *rq)\n{\n\treturn IOPRIO_PRIO_CLASS(req_get_ioprio(rq));\n}\n\n \nstatic inline struct request *\ndeadline_earlier_request(struct request *rq)\n{\n\tstruct rb_node *node = rb_prev(&rq->rb_node);\n\n\tif (node)\n\t\treturn rb_entry_rq(node);\n\n\treturn NULL;\n}\n\n \nstatic inline struct request *\ndeadline_latter_request(struct request *rq)\n{\n\tstruct rb_node *node = rb_next(&rq->rb_node);\n\n\tif (node)\n\t\treturn rb_entry_rq(node);\n\n\treturn NULL;\n}\n\n \nstatic inline struct request *deadline_from_pos(struct dd_per_prio *per_prio,\n\t\t\t\tenum dd_data_dir data_dir, sector_t pos)\n{\n\tstruct rb_node *node = per_prio->sort_list[data_dir].rb_node;\n\tstruct request *rq, *res = NULL;\n\n\tif (!node)\n\t\treturn NULL;\n\n\trq = rb_entry_rq(node);\n\t \n\tif (blk_rq_is_seq_zoned_write(rq))\n\t\tpos = round_down(pos, rq->q->limits.chunk_sectors);\n\n\twhile (node) {\n\t\trq = rb_entry_rq(node);\n\t\tif (blk_rq_pos(rq) >= pos) {\n\t\t\tres = rq;\n\t\t\tnode = node->rb_left;\n\t\t} else {\n\t\t\tnode = node->rb_right;\n\t\t}\n\t}\n\treturn res;\n}\n\nstatic void\ndeadline_add_rq_rb(struct dd_per_prio *per_prio, struct request *rq)\n{\n\tstruct rb_root *root = deadline_rb_root(per_prio, rq);\n\n\telv_rb_add(root, rq);\n}\n\nstatic inline void\ndeadline_del_rq_rb(struct dd_per_prio *per_prio, struct request *rq)\n{\n\telv_rb_del(deadline_rb_root(per_prio, rq), rq);\n}\n\n \nstatic void deadline_remove_request(struct request_queue *q,\n\t\t\t\t    struct dd_per_prio *per_prio,\n\t\t\t\t    struct request *rq)\n{\n\tlist_del_init(&rq->queuelist);\n\n\t \n\tif (!RB_EMPTY_NODE(&rq->rb_node))\n\t\tdeadline_del_rq_rb(per_prio, rq);\n\n\telv_rqhash_del(q, rq);\n\tif (q->last_merge == rq)\n\t\tq->last_merge = NULL;\n}\n\nstatic void dd_request_merged(struct request_queue *q, struct request *req,\n\t\t\t      enum elv_merge type)\n{\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tconst u8 ioprio_class = dd_rq_ioclass(req);\n\tconst enum dd_prio prio = ioprio_class_to_prio[ioprio_class];\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\n\n\t \n\tif (type == ELEVATOR_FRONT_MERGE) {\n\t\telv_rb_del(deadline_rb_root(per_prio, req), req);\n\t\tdeadline_add_rq_rb(per_prio, req);\n\t}\n}\n\n \nstatic void dd_merged_requests(struct request_queue *q, struct request *req,\n\t\t\t       struct request *next)\n{\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tconst u8 ioprio_class = dd_rq_ioclass(next);\n\tconst enum dd_prio prio = ioprio_class_to_prio[ioprio_class];\n\n\tlockdep_assert_held(&dd->lock);\n\n\tdd->per_prio[prio].stats.merged++;\n\n\t \n\tif (!list_empty(&req->queuelist) && !list_empty(&next->queuelist)) {\n\t\tif (time_before((unsigned long)next->fifo_time,\n\t\t\t\t(unsigned long)req->fifo_time)) {\n\t\t\tlist_move(&req->queuelist, &next->queuelist);\n\t\t\treq->fifo_time = next->fifo_time;\n\t\t}\n\t}\n\n\t \n\tdeadline_remove_request(q, &dd->per_prio[prio], next);\n}\n\n \nstatic void\ndeadline_move_request(struct deadline_data *dd, struct dd_per_prio *per_prio,\n\t\t      struct request *rq)\n{\n\t \n\tdeadline_remove_request(rq->q, per_prio, rq);\n}\n\n \nstatic u32 dd_queued(struct deadline_data *dd, enum dd_prio prio)\n{\n\tconst struct io_stats_per_prio *stats = &dd->per_prio[prio].stats;\n\n\tlockdep_assert_held(&dd->lock);\n\n\treturn stats->inserted - atomic_read(&stats->completed);\n}\n\n \nstatic inline bool deadline_check_fifo(struct dd_per_prio *per_prio,\n\t\t\t\t       enum dd_data_dir data_dir)\n{\n\tstruct request *rq = rq_entry_fifo(per_prio->fifo_list[data_dir].next);\n\n\treturn time_is_before_eq_jiffies((unsigned long)rq->fifo_time);\n}\n\n \nstatic bool deadline_is_seq_write(struct deadline_data *dd, struct request *rq)\n{\n\tstruct request *prev = deadline_earlier_request(rq);\n\n\tif (!prev)\n\t\treturn false;\n\n\treturn blk_rq_pos(prev) + blk_rq_sectors(prev) == blk_rq_pos(rq);\n}\n\n \nstatic struct request *deadline_skip_seq_writes(struct deadline_data *dd,\n\t\t\t\t\t\tstruct request *rq)\n{\n\tsector_t pos = blk_rq_pos(rq);\n\n\tdo {\n\t\tpos += blk_rq_sectors(rq);\n\t\trq = deadline_latter_request(rq);\n\t} while (rq && blk_rq_pos(rq) == pos);\n\n\treturn rq;\n}\n\n \nstatic struct request *\ndeadline_fifo_request(struct deadline_data *dd, struct dd_per_prio *per_prio,\n\t\t      enum dd_data_dir data_dir)\n{\n\tstruct request *rq, *rb_rq, *next;\n\tunsigned long flags;\n\n\tif (list_empty(&per_prio->fifo_list[data_dir]))\n\t\treturn NULL;\n\n\trq = rq_entry_fifo(per_prio->fifo_list[data_dir].next);\n\tif (data_dir == DD_READ || !blk_queue_is_zoned(rq->q))\n\t\treturn rq;\n\n\t \n\tspin_lock_irqsave(&dd->zone_lock, flags);\n\tlist_for_each_entry_safe(rq, next, &per_prio->fifo_list[DD_WRITE],\n\t\t\t\t queuelist) {\n\t\t \n\t\trb_rq = deadline_from_pos(per_prio, data_dir, blk_rq_pos(rq));\n\t\tif (rb_rq && blk_rq_pos(rb_rq) < blk_rq_pos(rq))\n\t\t\trq = rb_rq;\n\t\tif (blk_req_can_dispatch_to_zone(rq) &&\n\t\t    (blk_queue_nonrot(rq->q) ||\n\t\t     !deadline_is_seq_write(dd, rq)))\n\t\t\tgoto out;\n\t}\n\trq = NULL;\nout:\n\tspin_unlock_irqrestore(&dd->zone_lock, flags);\n\n\treturn rq;\n}\n\n \nstatic struct request *\ndeadline_next_request(struct deadline_data *dd, struct dd_per_prio *per_prio,\n\t\t      enum dd_data_dir data_dir)\n{\n\tstruct request *rq;\n\tunsigned long flags;\n\n\trq = deadline_from_pos(per_prio, data_dir,\n\t\t\t       per_prio->latest_pos[data_dir]);\n\tif (!rq)\n\t\treturn NULL;\n\n\tif (data_dir == DD_READ || !blk_queue_is_zoned(rq->q))\n\t\treturn rq;\n\n\t \n\tspin_lock_irqsave(&dd->zone_lock, flags);\n\twhile (rq) {\n\t\tif (blk_req_can_dispatch_to_zone(rq))\n\t\t\tbreak;\n\t\tif (blk_queue_nonrot(rq->q))\n\t\t\trq = deadline_latter_request(rq);\n\t\telse\n\t\t\trq = deadline_skip_seq_writes(dd, rq);\n\t}\n\tspin_unlock_irqrestore(&dd->zone_lock, flags);\n\n\treturn rq;\n}\n\n \nstatic bool started_after(struct deadline_data *dd, struct request *rq,\n\t\t\t  unsigned long latest_start)\n{\n\tunsigned long start_time = (unsigned long)rq->fifo_time;\n\n\tstart_time -= dd->fifo_expire[rq_data_dir(rq)];\n\n\treturn time_after(start_time, latest_start);\n}\n\n \nstatic struct request *__dd_dispatch_request(struct deadline_data *dd,\n\t\t\t\t\t     struct dd_per_prio *per_prio,\n\t\t\t\t\t     unsigned long latest_start)\n{\n\tstruct request *rq, *next_rq;\n\tenum dd_data_dir data_dir;\n\tenum dd_prio prio;\n\tu8 ioprio_class;\n\n\tlockdep_assert_held(&dd->lock);\n\n\tif (!list_empty(&per_prio->dispatch)) {\n\t\trq = list_first_entry(&per_prio->dispatch, struct request,\n\t\t\t\t      queuelist);\n\t\tif (started_after(dd, rq, latest_start))\n\t\t\treturn NULL;\n\t\tlist_del_init(&rq->queuelist);\n\t\tdata_dir = rq_data_dir(rq);\n\t\tgoto done;\n\t}\n\n\t \n\trq = deadline_next_request(dd, per_prio, dd->last_dir);\n\tif (rq && dd->batching < dd->fifo_batch) {\n\t\t \n\t\tdata_dir = rq_data_dir(rq);\n\t\tgoto dispatch_request;\n\t}\n\n\t \n\n\tif (!list_empty(&per_prio->fifo_list[DD_READ])) {\n\t\tBUG_ON(RB_EMPTY_ROOT(&per_prio->sort_list[DD_READ]));\n\n\t\tif (deadline_fifo_request(dd, per_prio, DD_WRITE) &&\n\t\t    (dd->starved++ >= dd->writes_starved))\n\t\t\tgoto dispatch_writes;\n\n\t\tdata_dir = DD_READ;\n\n\t\tgoto dispatch_find_request;\n\t}\n\n\t \n\n\tif (!list_empty(&per_prio->fifo_list[DD_WRITE])) {\ndispatch_writes:\n\t\tBUG_ON(RB_EMPTY_ROOT(&per_prio->sort_list[DD_WRITE]));\n\n\t\tdd->starved = 0;\n\n\t\tdata_dir = DD_WRITE;\n\n\t\tgoto dispatch_find_request;\n\t}\n\n\treturn NULL;\n\ndispatch_find_request:\n\t \n\tnext_rq = deadline_next_request(dd, per_prio, data_dir);\n\tif (deadline_check_fifo(per_prio, data_dir) || !next_rq) {\n\t\t \n\t\trq = deadline_fifo_request(dd, per_prio, data_dir);\n\t} else {\n\t\t \n\t\trq = next_rq;\n\t}\n\n\t \n\tif (!rq)\n\t\treturn NULL;\n\n\tdd->last_dir = data_dir;\n\tdd->batching = 0;\n\ndispatch_request:\n\tif (started_after(dd, rq, latest_start))\n\t\treturn NULL;\n\n\t \n\tdd->batching++;\n\tdeadline_move_request(dd, per_prio, rq);\ndone:\n\tioprio_class = dd_rq_ioclass(rq);\n\tprio = ioprio_class_to_prio[ioprio_class];\n\tdd->per_prio[prio].latest_pos[data_dir] = blk_rq_pos(rq);\n\tdd->per_prio[prio].stats.dispatched++;\n\t \n\tblk_req_zone_write_lock(rq);\n\trq->rq_flags |= RQF_STARTED;\n\treturn rq;\n}\n\n \nstatic struct request *dd_dispatch_prio_aged_requests(struct deadline_data *dd,\n\t\t\t\t\t\t      unsigned long now)\n{\n\tstruct request *rq;\n\tenum dd_prio prio;\n\tint prio_cnt;\n\n\tlockdep_assert_held(&dd->lock);\n\n\tprio_cnt = !!dd_queued(dd, DD_RT_PRIO) + !!dd_queued(dd, DD_BE_PRIO) +\n\t\t   !!dd_queued(dd, DD_IDLE_PRIO);\n\tif (prio_cnt < 2)\n\t\treturn NULL;\n\n\tfor (prio = DD_BE_PRIO; prio <= DD_PRIO_MAX; prio++) {\n\t\trq = __dd_dispatch_request(dd, &dd->per_prio[prio],\n\t\t\t\t\t   now - dd->prio_aging_expire);\n\t\tif (rq)\n\t\t\treturn rq;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct request *dd_dispatch_request(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct deadline_data *dd = hctx->queue->elevator->elevator_data;\n\tconst unsigned long now = jiffies;\n\tstruct request *rq;\n\tenum dd_prio prio;\n\n\tspin_lock(&dd->lock);\n\trq = dd_dispatch_prio_aged_requests(dd, now);\n\tif (rq)\n\t\tgoto unlock;\n\n\t \n\tfor (prio = 0; prio <= DD_PRIO_MAX; prio++) {\n\t\trq = __dd_dispatch_request(dd, &dd->per_prio[prio], now);\n\t\tif (rq || dd_queued(dd, prio))\n\t\t\tbreak;\n\t}\n\nunlock:\n\tspin_unlock(&dd->lock);\n\n\treturn rq;\n}\n\n \nstatic void dd_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data)\n{\n\tstruct deadline_data *dd = data->q->elevator->elevator_data;\n\n\t \n\tif (op_is_sync(opf) && !op_is_write(opf))\n\t\treturn;\n\n\t \n\tdata->shallow_depth = dd->async_depth;\n}\n\n \nstatic void dd_depth_updated(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tstruct blk_mq_tags *tags = hctx->sched_tags;\n\tunsigned int shift = tags->bitmap_tags.sb.shift;\n\n\tdd->async_depth = max(1U, 3 * (1U << shift)  / 4);\n\n\tsbitmap_queue_min_shallow_depth(&tags->bitmap_tags, dd->async_depth);\n}\n\n \nstatic int dd_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tdd_depth_updated(hctx);\n\treturn 0;\n}\n\nstatic void dd_exit_sched(struct elevator_queue *e)\n{\n\tstruct deadline_data *dd = e->elevator_data;\n\tenum dd_prio prio;\n\n\tfor (prio = 0; prio <= DD_PRIO_MAX; prio++) {\n\t\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\n\t\tconst struct io_stats_per_prio *stats = &per_prio->stats;\n\t\tuint32_t queued;\n\n\t\tWARN_ON_ONCE(!list_empty(&per_prio->fifo_list[DD_READ]));\n\t\tWARN_ON_ONCE(!list_empty(&per_prio->fifo_list[DD_WRITE]));\n\n\t\tspin_lock(&dd->lock);\n\t\tqueued = dd_queued(dd, prio);\n\t\tspin_unlock(&dd->lock);\n\n\t\tWARN_ONCE(queued != 0,\n\t\t\t  \"statistics for priority %d: i %u m %u d %u c %u\\n\",\n\t\t\t  prio, stats->inserted, stats->merged,\n\t\t\t  stats->dispatched, atomic_read(&stats->completed));\n\t}\n\n\tkfree(dd);\n}\n\n \nstatic int dd_init_sched(struct request_queue *q, struct elevator_type *e)\n{\n\tstruct deadline_data *dd;\n\tstruct elevator_queue *eq;\n\tenum dd_prio prio;\n\tint ret = -ENOMEM;\n\n\teq = elevator_alloc(q, e);\n\tif (!eq)\n\t\treturn ret;\n\n\tdd = kzalloc_node(sizeof(*dd), GFP_KERNEL, q->node);\n\tif (!dd)\n\t\tgoto put_eq;\n\n\teq->elevator_data = dd;\n\n\tfor (prio = 0; prio <= DD_PRIO_MAX; prio++) {\n\t\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\n\n\t\tINIT_LIST_HEAD(&per_prio->dispatch);\n\t\tINIT_LIST_HEAD(&per_prio->fifo_list[DD_READ]);\n\t\tINIT_LIST_HEAD(&per_prio->fifo_list[DD_WRITE]);\n\t\tper_prio->sort_list[DD_READ] = RB_ROOT;\n\t\tper_prio->sort_list[DD_WRITE] = RB_ROOT;\n\t}\n\tdd->fifo_expire[DD_READ] = read_expire;\n\tdd->fifo_expire[DD_WRITE] = write_expire;\n\tdd->writes_starved = writes_starved;\n\tdd->front_merges = 1;\n\tdd->last_dir = DD_WRITE;\n\tdd->fifo_batch = fifo_batch;\n\tdd->prio_aging_expire = prio_aging_expire;\n\tspin_lock_init(&dd->lock);\n\tspin_lock_init(&dd->zone_lock);\n\n\t \n\tblk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);\n\n\tq->elevator = eq;\n\treturn 0;\n\nput_eq:\n\tkobject_put(&eq->kobj);\n\treturn ret;\n}\n\n \nstatic int dd_request_merge(struct request_queue *q, struct request **rq,\n\t\t\t    struct bio *bio)\n{\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tconst u8 ioprio_class = IOPRIO_PRIO_CLASS(bio->bi_ioprio);\n\tconst enum dd_prio prio = ioprio_class_to_prio[ioprio_class];\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\n\tsector_t sector = bio_end_sector(bio);\n\tstruct request *__rq;\n\n\tif (!dd->front_merges)\n\t\treturn ELEVATOR_NO_MERGE;\n\n\t__rq = elv_rb_find(&per_prio->sort_list[bio_data_dir(bio)], sector);\n\tif (__rq) {\n\t\tBUG_ON(sector != blk_rq_pos(__rq));\n\n\t\tif (elv_bio_merge_ok(__rq, bio)) {\n\t\t\t*rq = __rq;\n\t\t\tif (blk_discard_mergable(__rq))\n\t\t\t\treturn ELEVATOR_DISCARD_MERGE;\n\t\t\treturn ELEVATOR_FRONT_MERGE;\n\t\t}\n\t}\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\n \nstatic bool dd_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tstruct request *free = NULL;\n\tbool ret;\n\n\tspin_lock(&dd->lock);\n\tret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);\n\tspin_unlock(&dd->lock);\n\n\tif (free)\n\t\tblk_mq_free_request(free);\n\n\treturn ret;\n}\n\n \nstatic void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,\n\t\t\t      blk_insert_t flags, struct list_head *free)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tconst enum dd_data_dir data_dir = rq_data_dir(rq);\n\tu16 ioprio = req_get_ioprio(rq);\n\tu8 ioprio_class = IOPRIO_PRIO_CLASS(ioprio);\n\tstruct dd_per_prio *per_prio;\n\tenum dd_prio prio;\n\n\tlockdep_assert_held(&dd->lock);\n\n\t \n\tblk_req_zone_write_unlock(rq);\n\n\tprio = ioprio_class_to_prio[ioprio_class];\n\tper_prio = &dd->per_prio[prio];\n\tif (!rq->elv.priv[0]) {\n\t\tper_prio->stats.inserted++;\n\t\trq->elv.priv[0] = (void *)(uintptr_t)1;\n\t}\n\n\tif (blk_mq_sched_try_insert_merge(q, rq, free))\n\t\treturn;\n\n\ttrace_block_rq_insert(rq);\n\n\tif (flags & BLK_MQ_INSERT_AT_HEAD) {\n\t\tlist_add(&rq->queuelist, &per_prio->dispatch);\n\t\trq->fifo_time = jiffies;\n\t} else {\n\t\tstruct list_head *insert_before;\n\n\t\tdeadline_add_rq_rb(per_prio, rq);\n\n\t\tif (rq_mergeable(rq)) {\n\t\t\telv_rqhash_add(q, rq);\n\t\t\tif (!q->last_merge)\n\t\t\t\tq->last_merge = rq;\n\t\t}\n\n\t\t \n\t\trq->fifo_time = jiffies + dd->fifo_expire[data_dir];\n\t\tinsert_before = &per_prio->fifo_list[data_dir];\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\t \n\t\tif (blk_rq_is_seq_zoned_write(rq)) {\n\t\t\tstruct request *rq2 = deadline_latter_request(rq);\n\n\t\t\tif (rq2 && blk_rq_zone_no(rq2) == blk_rq_zone_no(rq))\n\t\t\t\tinsert_before = &rq2->queuelist;\n\t\t}\n#endif\n\t\tlist_add_tail(&rq->queuelist, insert_before);\n\t}\n}\n\n \nstatic void dd_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\t\t       struct list_head *list,\n\t\t\t       blk_insert_t flags)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tLIST_HEAD(free);\n\n\tspin_lock(&dd->lock);\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tdd_insert_request(hctx, rq, flags, &free);\n\t}\n\tspin_unlock(&dd->lock);\n\n\tblk_mq_free_requests(&free);\n}\n\n \nstatic void dd_prepare_request(struct request *rq)\n{\n\trq->elv.priv[0] = NULL;\n}\n\nstatic bool dd_has_write_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct deadline_data *dd = hctx->queue->elevator->elevator_data;\n\tenum dd_prio p;\n\n\tfor (p = 0; p <= DD_PRIO_MAX; p++)\n\t\tif (!list_empty_careful(&dd->per_prio[p].fifo_list[DD_WRITE]))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void dd_finish_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tconst u8 ioprio_class = dd_rq_ioclass(rq);\n\tconst enum dd_prio prio = ioprio_class_to_prio[ioprio_class];\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\n\n\t \n\tif (!rq->elv.priv[0])\n\t\treturn;\n\n\tatomic_inc(&per_prio->stats.completed);\n\n\tif (blk_queue_is_zoned(q)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&dd->zone_lock, flags);\n\t\tblk_req_zone_write_unlock(rq);\n\t\tspin_unlock_irqrestore(&dd->zone_lock, flags);\n\n\t\tif (dd_has_write_work(rq->mq_hctx))\n\t\t\tblk_mq_sched_mark_restart_hctx(rq->mq_hctx);\n\t}\n}\n\nstatic bool dd_has_work_for_prio(struct dd_per_prio *per_prio)\n{\n\treturn !list_empty_careful(&per_prio->dispatch) ||\n\t\t!list_empty_careful(&per_prio->fifo_list[DD_READ]) ||\n\t\t!list_empty_careful(&per_prio->fifo_list[DD_WRITE]);\n}\n\nstatic bool dd_has_work(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct deadline_data *dd = hctx->queue->elevator->elevator_data;\n\tenum dd_prio prio;\n\n\tfor (prio = 0; prio <= DD_PRIO_MAX; prio++)\n\t\tif (dd_has_work_for_prio(&dd->per_prio[prio]))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \n#define SHOW_INT(__FUNC, __VAR)\t\t\t\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct deadline_data *dd = e->elevator_data;\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn sysfs_emit(page, \"%d\\n\", __VAR);\t\t\t\t\\\n}\n#define SHOW_JIFFIES(__FUNC, __VAR) SHOW_INT(__FUNC, jiffies_to_msecs(__VAR))\nSHOW_JIFFIES(deadline_read_expire_show, dd->fifo_expire[DD_READ]);\nSHOW_JIFFIES(deadline_write_expire_show, dd->fifo_expire[DD_WRITE]);\nSHOW_JIFFIES(deadline_prio_aging_expire_show, dd->prio_aging_expire);\nSHOW_INT(deadline_writes_starved_show, dd->writes_starved);\nSHOW_INT(deadline_front_merges_show, dd->front_merges);\nSHOW_INT(deadline_async_depth_show, dd->async_depth);\nSHOW_INT(deadline_fifo_batch_show, dd->fifo_batch);\n#undef SHOW_INT\n#undef SHOW_JIFFIES\n\n#define STORE_FUNCTION(__FUNC, __PTR, MIN, MAX, __CONV)\t\t\t\\\nstatic ssize_t __FUNC(struct elevator_queue *e, const char *page, size_t count)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct deadline_data *dd = e->elevator_data;\t\t\t\\\n\tint __data, __ret;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__ret = kstrtoint(page, 0, &__data);\t\t\t\t\\\n\tif (__ret < 0)\t\t\t\t\t\t\t\\\n\t\treturn __ret;\t\t\t\t\t\t\\\n\tif (__data < (MIN))\t\t\t\t\t\t\\\n\t\t__data = (MIN);\t\t\t\t\t\t\\\n\telse if (__data > (MAX))\t\t\t\t\t\\\n\t\t__data = (MAX);\t\t\t\t\t\t\\\n\t*(__PTR) = __CONV(__data);\t\t\t\t\t\\\n\treturn count;\t\t\t\t\t\t\t\\\n}\n#define STORE_INT(__FUNC, __PTR, MIN, MAX)\t\t\t\t\\\n\tSTORE_FUNCTION(__FUNC, __PTR, MIN, MAX, )\n#define STORE_JIFFIES(__FUNC, __PTR, MIN, MAX)\t\t\t\t\\\n\tSTORE_FUNCTION(__FUNC, __PTR, MIN, MAX, msecs_to_jiffies)\nSTORE_JIFFIES(deadline_read_expire_store, &dd->fifo_expire[DD_READ], 0, INT_MAX);\nSTORE_JIFFIES(deadline_write_expire_store, &dd->fifo_expire[DD_WRITE], 0, INT_MAX);\nSTORE_JIFFIES(deadline_prio_aging_expire_store, &dd->prio_aging_expire, 0, INT_MAX);\nSTORE_INT(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX);\nSTORE_INT(deadline_front_merges_store, &dd->front_merges, 0, 1);\nSTORE_INT(deadline_async_depth_store, &dd->async_depth, 1, INT_MAX);\nSTORE_INT(deadline_fifo_batch_store, &dd->fifo_batch, 0, INT_MAX);\n#undef STORE_FUNCTION\n#undef STORE_INT\n#undef STORE_JIFFIES\n\n#define DD_ATTR(name) \\\n\t__ATTR(name, 0644, deadline_##name##_show, deadline_##name##_store)\n\nstatic struct elv_fs_entry deadline_attrs[] = {\n\tDD_ATTR(read_expire),\n\tDD_ATTR(write_expire),\n\tDD_ATTR(writes_starved),\n\tDD_ATTR(front_merges),\n\tDD_ATTR(async_depth),\n\tDD_ATTR(fifo_batch),\n\tDD_ATTR(prio_aging_expire),\n\t__ATTR_NULL\n};\n\n#ifdef CONFIG_BLK_DEBUG_FS\n#define DEADLINE_DEBUGFS_DDIR_ATTRS(prio, data_dir, name)\t\t\\\nstatic void *deadline_##name##_fifo_start(struct seq_file *m,\t\t\\\n\t\t\t\t\t  loff_t *pos)\t\t\t\\\n\t__acquires(&dd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_lock(&dd->lock);\t\t\t\t\t\t\\\n\treturn seq_list_start(&per_prio->fifo_list[data_dir], *pos);\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void *deadline_##name##_fifo_next(struct seq_file *m, void *v,\t\\\n\t\t\t\t\t loff_t *pos)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn seq_list_next(v, &per_prio->fifo_list[data_dir], pos);\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void deadline_##name##_fifo_stop(struct seq_file *m, void *v)\t\\\n\t__releases(&dd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_unlock(&dd->lock);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic const struct seq_operations deadline_##name##_fifo_seq_ops = {\t\\\n\t.start\t= deadline_##name##_fifo_start,\t\t\t\t\\\n\t.next\t= deadline_##name##_fifo_next,\t\t\t\t\\\n\t.stop\t= deadline_##name##_fifo_stop,\t\t\t\t\\\n\t.show\t= blk_mq_debugfs_rq_show,\t\t\t\t\\\n};\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic int deadline_##name##_next_rq_show(void *data,\t\t\t\\\n\t\t\t\t\t  struct seq_file *m)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = data;\t\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\t\t\\\n\tstruct request *rq;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\trq = deadline_from_pos(per_prio, data_dir,\t\t\t\\\n\t\t\t       per_prio->latest_pos[data_dir]);\t\t\\\n\tif (rq)\t\t\t\t\t\t\t\t\\\n\t\t__blk_mq_debugfs_rq_show(m, rq);\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_RT_PRIO, DD_READ, read0);\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_RT_PRIO, DD_WRITE, write0);\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_BE_PRIO, DD_READ, read1);\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_BE_PRIO, DD_WRITE, write1);\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_IDLE_PRIO, DD_READ, read2);\nDEADLINE_DEBUGFS_DDIR_ATTRS(DD_IDLE_PRIO, DD_WRITE, write2);\n#undef DEADLINE_DEBUGFS_DDIR_ATTRS\n\nstatic int deadline_batching_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\n\tseq_printf(m, \"%u\\n\", dd->batching);\n\treturn 0;\n}\n\nstatic int deadline_starved_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\n\tseq_printf(m, \"%u\\n\", dd->starved);\n\treturn 0;\n}\n\nstatic int dd_async_depth_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\n\tseq_printf(m, \"%u\\n\", dd->async_depth);\n\treturn 0;\n}\n\nstatic int dd_queued_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tu32 rt, be, idle;\n\n\tspin_lock(&dd->lock);\n\trt = dd_queued(dd, DD_RT_PRIO);\n\tbe = dd_queued(dd, DD_BE_PRIO);\n\tidle = dd_queued(dd, DD_IDLE_PRIO);\n\tspin_unlock(&dd->lock);\n\n\tseq_printf(m, \"%u %u %u\\n\", rt, be, idle);\n\n\treturn 0;\n}\n\n \nstatic u32 dd_owned_by_driver(struct deadline_data *dd, enum dd_prio prio)\n{\n\tconst struct io_stats_per_prio *stats = &dd->per_prio[prio].stats;\n\n\tlockdep_assert_held(&dd->lock);\n\n\treturn stats->dispatched + stats->merged -\n\t\tatomic_read(&stats->completed);\n}\n\nstatic int dd_owned_by_driver_show(void *data, struct seq_file *m)\n{\n\tstruct request_queue *q = data;\n\tstruct deadline_data *dd = q->elevator->elevator_data;\n\tu32 rt, be, idle;\n\n\tspin_lock(&dd->lock);\n\trt = dd_owned_by_driver(dd, DD_RT_PRIO);\n\tbe = dd_owned_by_driver(dd, DD_BE_PRIO);\n\tidle = dd_owned_by_driver(dd, DD_IDLE_PRIO);\n\tspin_unlock(&dd->lock);\n\n\tseq_printf(m, \"%u %u %u\\n\", rt, be, idle);\n\n\treturn 0;\n}\n\n#define DEADLINE_DISPATCH_ATTR(prio)\t\t\t\t\t\\\nstatic void *deadline_dispatch##prio##_start(struct seq_file *m,\t\\\n\t\t\t\t\t     loff_t *pos)\t\t\\\n\t__acquires(&dd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_lock(&dd->lock);\t\t\t\t\t\t\\\n\treturn seq_list_start(&per_prio->dispatch, *pos);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void *deadline_dispatch##prio##_next(struct seq_file *m,\t\t\\\n\t\t\t\t\t    void *v, loff_t *pos)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\tstruct dd_per_prio *per_prio = &dd->per_prio[prio];\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn seq_list_next(v, &per_prio->dispatch, pos);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic void deadline_dispatch##prio##_stop(struct seq_file *m, void *v)\t\\\n\t__releases(&dd->lock)\t\t\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct request_queue *q = m->private;\t\t\t\t\\\n\tstruct deadline_data *dd = q->elevator->elevator_data;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tspin_unlock(&dd->lock);\t\t\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic const struct seq_operations deadline_dispatch##prio##_seq_ops = { \\\n\t.start\t= deadline_dispatch##prio##_start,\t\t\t\\\n\t.next\t= deadline_dispatch##prio##_next,\t\t\t\\\n\t.stop\t= deadline_dispatch##prio##_stop,\t\t\t\\\n\t.show\t= blk_mq_debugfs_rq_show,\t\t\t\t\\\n}\n\nDEADLINE_DISPATCH_ATTR(0);\nDEADLINE_DISPATCH_ATTR(1);\nDEADLINE_DISPATCH_ATTR(2);\n#undef DEADLINE_DISPATCH_ATTR\n\n#define DEADLINE_QUEUE_DDIR_ATTRS(name)\t\t\t\t\t\\\n\t{#name \"_fifo_list\", 0400,\t\t\t\t\t\\\n\t\t\t.seq_ops = &deadline_##name##_fifo_seq_ops}\n#define DEADLINE_NEXT_RQ_ATTR(name)\t\t\t\t\t\\\n\t{#name \"_next_rq\", 0400, deadline_##name##_next_rq_show}\nstatic const struct blk_mq_debugfs_attr deadline_queue_debugfs_attrs[] = {\n\tDEADLINE_QUEUE_DDIR_ATTRS(read0),\n\tDEADLINE_QUEUE_DDIR_ATTRS(write0),\n\tDEADLINE_QUEUE_DDIR_ATTRS(read1),\n\tDEADLINE_QUEUE_DDIR_ATTRS(write1),\n\tDEADLINE_QUEUE_DDIR_ATTRS(read2),\n\tDEADLINE_QUEUE_DDIR_ATTRS(write2),\n\tDEADLINE_NEXT_RQ_ATTR(read0),\n\tDEADLINE_NEXT_RQ_ATTR(write0),\n\tDEADLINE_NEXT_RQ_ATTR(read1),\n\tDEADLINE_NEXT_RQ_ATTR(write1),\n\tDEADLINE_NEXT_RQ_ATTR(read2),\n\tDEADLINE_NEXT_RQ_ATTR(write2),\n\t{\"batching\", 0400, deadline_batching_show},\n\t{\"starved\", 0400, deadline_starved_show},\n\t{\"async_depth\", 0400, dd_async_depth_show},\n\t{\"dispatch0\", 0400, .seq_ops = &deadline_dispatch0_seq_ops},\n\t{\"dispatch1\", 0400, .seq_ops = &deadline_dispatch1_seq_ops},\n\t{\"dispatch2\", 0400, .seq_ops = &deadline_dispatch2_seq_ops},\n\t{\"owned_by_driver\", 0400, dd_owned_by_driver_show},\n\t{\"queued\", 0400, dd_queued_show},\n\t{},\n};\n#undef DEADLINE_QUEUE_DDIR_ATTRS\n#endif\n\nstatic struct elevator_type mq_deadline = {\n\t.ops = {\n\t\t.depth_updated\t\t= dd_depth_updated,\n\t\t.limit_depth\t\t= dd_limit_depth,\n\t\t.insert_requests\t= dd_insert_requests,\n\t\t.dispatch_request\t= dd_dispatch_request,\n\t\t.prepare_request\t= dd_prepare_request,\n\t\t.finish_request\t\t= dd_finish_request,\n\t\t.next_request\t\t= elv_rb_latter_request,\n\t\t.former_request\t\t= elv_rb_former_request,\n\t\t.bio_merge\t\t= dd_bio_merge,\n\t\t.request_merge\t\t= dd_request_merge,\n\t\t.requests_merged\t= dd_merged_requests,\n\t\t.request_merged\t\t= dd_request_merged,\n\t\t.has_work\t\t= dd_has_work,\n\t\t.init_sched\t\t= dd_init_sched,\n\t\t.exit_sched\t\t= dd_exit_sched,\n\t\t.init_hctx\t\t= dd_init_hctx,\n\t},\n\n#ifdef CONFIG_BLK_DEBUG_FS\n\t.queue_debugfs_attrs = deadline_queue_debugfs_attrs,\n#endif\n\t.elevator_attrs = deadline_attrs,\n\t.elevator_name = \"mq-deadline\",\n\t.elevator_alias = \"deadline\",\n\t.elevator_features = ELEVATOR_F_ZBD_SEQ_WRITE,\n\t.elevator_owner = THIS_MODULE,\n};\nMODULE_ALIAS(\"mq-deadline-iosched\");\n\nstatic int __init deadline_init(void)\n{\n\treturn elv_register(&mq_deadline);\n}\n\nstatic void __exit deadline_exit(void)\n{\n\telv_unregister(&mq_deadline);\n}\n\nmodule_init(deadline_init);\nmodule_exit(deadline_exit);\n\nMODULE_AUTHOR(\"Jens Axboe, Damien Le Moal and Bart Van Assche\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MQ deadline IO scheduler\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}