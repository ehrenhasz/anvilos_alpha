{
  "module_name": "blk-settings.c",
  "hash_id": "96b1bc0e933436ad1e430aefb87a029e62101911b4ef9ba683b9b3043b576155",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-settings.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/pagemap.h>\n#include <linux/backing-dev-defs.h>\n#include <linux/gcd.h>\n#include <linux/lcm.h>\n#include <linux/jiffies.h>\n#include <linux/gfp.h>\n#include <linux/dma-mapping.h>\n\n#include \"blk.h\"\n#include \"blk-rq-qos.h\"\n#include \"blk-wbt.h\"\n\nvoid blk_queue_rq_timeout(struct request_queue *q, unsigned int timeout)\n{\n\tq->rq_timeout = timeout;\n}\nEXPORT_SYMBOL_GPL(blk_queue_rq_timeout);\n\n \nvoid blk_set_default_limits(struct queue_limits *lim)\n{\n\tlim->max_segments = BLK_MAX_SEGMENTS;\n\tlim->max_discard_segments = 1;\n\tlim->max_integrity_segments = 0;\n\tlim->seg_boundary_mask = BLK_SEG_BOUNDARY_MASK;\n\tlim->virt_boundary_mask = 0;\n\tlim->max_segment_size = BLK_MAX_SEGMENT_SIZE;\n\tlim->max_sectors = lim->max_hw_sectors = BLK_SAFE_MAX_SECTORS;\n\tlim->max_user_sectors = lim->max_dev_sectors = 0;\n\tlim->chunk_sectors = 0;\n\tlim->max_write_zeroes_sectors = 0;\n\tlim->max_zone_append_sectors = 0;\n\tlim->max_discard_sectors = 0;\n\tlim->max_hw_discard_sectors = 0;\n\tlim->max_secure_erase_sectors = 0;\n\tlim->discard_granularity = 0;\n\tlim->discard_alignment = 0;\n\tlim->discard_misaligned = 0;\n\tlim->logical_block_size = lim->physical_block_size = lim->io_min = 512;\n\tlim->bounce = BLK_BOUNCE_NONE;\n\tlim->alignment_offset = 0;\n\tlim->io_opt = 0;\n\tlim->misaligned = 0;\n\tlim->zoned = BLK_ZONED_NONE;\n\tlim->zone_write_granularity = 0;\n\tlim->dma_alignment = 511;\n}\n\n \nvoid blk_set_stacking_limits(struct queue_limits *lim)\n{\n\tblk_set_default_limits(lim);\n\n\t \n\tlim->max_segments = USHRT_MAX;\n\tlim->max_discard_segments = USHRT_MAX;\n\tlim->max_hw_sectors = UINT_MAX;\n\tlim->max_segment_size = UINT_MAX;\n\tlim->max_sectors = UINT_MAX;\n\tlim->max_dev_sectors = UINT_MAX;\n\tlim->max_write_zeroes_sectors = UINT_MAX;\n\tlim->max_zone_append_sectors = UINT_MAX;\n}\nEXPORT_SYMBOL(blk_set_stacking_limits);\n\n \nvoid blk_queue_bounce_limit(struct request_queue *q, enum blk_bounce bounce)\n{\n\tq->limits.bounce = bounce;\n}\nEXPORT_SYMBOL(blk_queue_bounce_limit);\n\n \nvoid blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)\n{\n\tstruct queue_limits *limits = &q->limits;\n\tunsigned int max_sectors;\n\n\tif ((max_hw_sectors << 9) < PAGE_SIZE) {\n\t\tmax_hw_sectors = 1 << (PAGE_SHIFT - 9);\n\t\tprintk(KERN_INFO \"%s: set to minimum %d\\n\",\n\t\t       __func__, max_hw_sectors);\n\t}\n\n\tmax_hw_sectors = round_down(max_hw_sectors,\n\t\t\t\t    limits->logical_block_size >> SECTOR_SHIFT);\n\tlimits->max_hw_sectors = max_hw_sectors;\n\n\tmax_sectors = min_not_zero(max_hw_sectors, limits->max_dev_sectors);\n\n\tif (limits->max_user_sectors)\n\t\tmax_sectors = min(max_sectors, limits->max_user_sectors);\n\telse\n\t\tmax_sectors = min(max_sectors, BLK_DEF_MAX_SECTORS);\n\n\tmax_sectors = round_down(max_sectors,\n\t\t\t\t limits->logical_block_size >> SECTOR_SHIFT);\n\tlimits->max_sectors = max_sectors;\n\n\tif (!q->disk)\n\t\treturn;\n\tq->disk->bdi->io_pages = max_sectors >> (PAGE_SHIFT - 9);\n}\nEXPORT_SYMBOL(blk_queue_max_hw_sectors);\n\n \nvoid blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors)\n{\n\tq->limits.chunk_sectors = chunk_sectors;\n}\nEXPORT_SYMBOL(blk_queue_chunk_sectors);\n\n \nvoid blk_queue_max_discard_sectors(struct request_queue *q,\n\t\tunsigned int max_discard_sectors)\n{\n\tq->limits.max_hw_discard_sectors = max_discard_sectors;\n\tq->limits.max_discard_sectors = max_discard_sectors;\n}\nEXPORT_SYMBOL(blk_queue_max_discard_sectors);\n\n \nvoid blk_queue_max_secure_erase_sectors(struct request_queue *q,\n\t\tunsigned int max_sectors)\n{\n\tq->limits.max_secure_erase_sectors = max_sectors;\n}\nEXPORT_SYMBOL(blk_queue_max_secure_erase_sectors);\n\n \nvoid blk_queue_max_write_zeroes_sectors(struct request_queue *q,\n\t\tunsigned int max_write_zeroes_sectors)\n{\n\tq->limits.max_write_zeroes_sectors = max_write_zeroes_sectors;\n}\nEXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);\n\n \nvoid blk_queue_max_zone_append_sectors(struct request_queue *q,\n\t\tunsigned int max_zone_append_sectors)\n{\n\tunsigned int max_sectors;\n\n\tif (WARN_ON(!blk_queue_is_zoned(q)))\n\t\treturn;\n\n\tmax_sectors = min(q->limits.max_hw_sectors, max_zone_append_sectors);\n\tmax_sectors = min(q->limits.chunk_sectors, max_sectors);\n\n\t \n\tWARN_ON(!max_sectors);\n\n\tq->limits.max_zone_append_sectors = max_sectors;\n}\nEXPORT_SYMBOL_GPL(blk_queue_max_zone_append_sectors);\n\n \nvoid blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)\n{\n\tif (!max_segments) {\n\t\tmax_segments = 1;\n\t\tprintk(KERN_INFO \"%s: set to minimum %d\\n\",\n\t\t       __func__, max_segments);\n\t}\n\n\tq->limits.max_segments = max_segments;\n}\nEXPORT_SYMBOL(blk_queue_max_segments);\n\n \nvoid blk_queue_max_discard_segments(struct request_queue *q,\n\t\tunsigned short max_segments)\n{\n\tq->limits.max_discard_segments = max_segments;\n}\nEXPORT_SYMBOL_GPL(blk_queue_max_discard_segments);\n\n \nvoid blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)\n{\n\tif (max_size < PAGE_SIZE) {\n\t\tmax_size = PAGE_SIZE;\n\t\tprintk(KERN_INFO \"%s: set to minimum %d\\n\",\n\t\t       __func__, max_size);\n\t}\n\n\t \n\tWARN_ON_ONCE(q->limits.virt_boundary_mask);\n\n\tq->limits.max_segment_size = max_size;\n}\nEXPORT_SYMBOL(blk_queue_max_segment_size);\n\n \nvoid blk_queue_logical_block_size(struct request_queue *q, unsigned int size)\n{\n\tstruct queue_limits *limits = &q->limits;\n\n\tlimits->logical_block_size = size;\n\n\tif (limits->physical_block_size < size)\n\t\tlimits->physical_block_size = size;\n\n\tif (limits->io_min < limits->physical_block_size)\n\t\tlimits->io_min = limits->physical_block_size;\n\n\tlimits->max_hw_sectors =\n\t\tround_down(limits->max_hw_sectors, size >> SECTOR_SHIFT);\n\tlimits->max_sectors =\n\t\tround_down(limits->max_sectors, size >> SECTOR_SHIFT);\n}\nEXPORT_SYMBOL(blk_queue_logical_block_size);\n\n \nvoid blk_queue_physical_block_size(struct request_queue *q, unsigned int size)\n{\n\tq->limits.physical_block_size = size;\n\n\tif (q->limits.physical_block_size < q->limits.logical_block_size)\n\t\tq->limits.physical_block_size = q->limits.logical_block_size;\n\n\tif (q->limits.io_min < q->limits.physical_block_size)\n\t\tq->limits.io_min = q->limits.physical_block_size;\n}\nEXPORT_SYMBOL(blk_queue_physical_block_size);\n\n \nvoid blk_queue_zone_write_granularity(struct request_queue *q,\n\t\t\t\t      unsigned int size)\n{\n\tif (WARN_ON_ONCE(!blk_queue_is_zoned(q)))\n\t\treturn;\n\n\tq->limits.zone_write_granularity = size;\n\n\tif (q->limits.zone_write_granularity < q->limits.logical_block_size)\n\t\tq->limits.zone_write_granularity = q->limits.logical_block_size;\n}\nEXPORT_SYMBOL_GPL(blk_queue_zone_write_granularity);\n\n \nvoid blk_queue_alignment_offset(struct request_queue *q, unsigned int offset)\n{\n\tq->limits.alignment_offset =\n\t\toffset & (q->limits.physical_block_size - 1);\n\tq->limits.misaligned = 0;\n}\nEXPORT_SYMBOL(blk_queue_alignment_offset);\n\nvoid disk_update_readahead(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\n\t \n\tdisk->bdi->ra_pages =\n\t\tmax(queue_io_opt(q) * 2 / PAGE_SIZE, VM_READAHEAD_PAGES);\n\tdisk->bdi->io_pages = queue_max_sectors(q) >> (PAGE_SHIFT - 9);\n}\nEXPORT_SYMBOL_GPL(disk_update_readahead);\n\n \nvoid blk_limits_io_min(struct queue_limits *limits, unsigned int min)\n{\n\tlimits->io_min = min;\n\n\tif (limits->io_min < limits->logical_block_size)\n\t\tlimits->io_min = limits->logical_block_size;\n\n\tif (limits->io_min < limits->physical_block_size)\n\t\tlimits->io_min = limits->physical_block_size;\n}\nEXPORT_SYMBOL(blk_limits_io_min);\n\n \nvoid blk_queue_io_min(struct request_queue *q, unsigned int min)\n{\n\tblk_limits_io_min(&q->limits, min);\n}\nEXPORT_SYMBOL(blk_queue_io_min);\n\n \nvoid blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)\n{\n\tlimits->io_opt = opt;\n}\nEXPORT_SYMBOL(blk_limits_io_opt);\n\n \nvoid blk_queue_io_opt(struct request_queue *q, unsigned int opt)\n{\n\tblk_limits_io_opt(&q->limits, opt);\n\tif (!q->disk)\n\t\treturn;\n\tq->disk->bdi->ra_pages =\n\t\tmax(queue_io_opt(q) * 2 / PAGE_SIZE, VM_READAHEAD_PAGES);\n}\nEXPORT_SYMBOL(blk_queue_io_opt);\n\nstatic int queue_limit_alignment_offset(const struct queue_limits *lim,\n\t\tsector_t sector)\n{\n\tunsigned int granularity = max(lim->physical_block_size, lim->io_min);\n\tunsigned int alignment = sector_div(sector, granularity >> SECTOR_SHIFT)\n\t\t<< SECTOR_SHIFT;\n\n\treturn (granularity + lim->alignment_offset - alignment) % granularity;\n}\n\nstatic unsigned int queue_limit_discard_alignment(\n\t\tconst struct queue_limits *lim, sector_t sector)\n{\n\tunsigned int alignment, granularity, offset;\n\n\tif (!lim->max_discard_sectors)\n\t\treturn 0;\n\n\t \n\talignment = lim->discard_alignment >> SECTOR_SHIFT;\n\tgranularity = lim->discard_granularity >> SECTOR_SHIFT;\n\tif (!granularity)\n\t\treturn 0;\n\n\t \n\toffset = sector_div(sector, granularity);\n\n\t \n\toffset = (granularity + alignment - offset) % granularity;\n\n\t \n\treturn offset << SECTOR_SHIFT;\n}\n\nstatic unsigned int blk_round_down_sectors(unsigned int sectors, unsigned int lbs)\n{\n\tsectors = round_down(sectors, lbs >> SECTOR_SHIFT);\n\tif (sectors < PAGE_SIZE >> SECTOR_SHIFT)\n\t\tsectors = PAGE_SIZE >> SECTOR_SHIFT;\n\treturn sectors;\n}\n\n \nint blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\n\t\t     sector_t start)\n{\n\tunsigned int top, bottom, alignment, ret = 0;\n\n\tt->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);\n\tt->max_hw_sectors = min_not_zero(t->max_hw_sectors, b->max_hw_sectors);\n\tt->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);\n\tt->max_write_zeroes_sectors = min(t->max_write_zeroes_sectors,\n\t\t\t\t\tb->max_write_zeroes_sectors);\n\tt->max_zone_append_sectors = min(t->max_zone_append_sectors,\n\t\t\t\t\tb->max_zone_append_sectors);\n\tt->bounce = max(t->bounce, b->bounce);\n\n\tt->seg_boundary_mask = min_not_zero(t->seg_boundary_mask,\n\t\t\t\t\t    b->seg_boundary_mask);\n\tt->virt_boundary_mask = min_not_zero(t->virt_boundary_mask,\n\t\t\t\t\t    b->virt_boundary_mask);\n\n\tt->max_segments = min_not_zero(t->max_segments, b->max_segments);\n\tt->max_discard_segments = min_not_zero(t->max_discard_segments,\n\t\t\t\t\t       b->max_discard_segments);\n\tt->max_integrity_segments = min_not_zero(t->max_integrity_segments,\n\t\t\t\t\t\t b->max_integrity_segments);\n\n\tt->max_segment_size = min_not_zero(t->max_segment_size,\n\t\t\t\t\t   b->max_segment_size);\n\n\tt->misaligned |= b->misaligned;\n\n\talignment = queue_limit_alignment_offset(b, start);\n\n\t \n\tif (t->alignment_offset != alignment) {\n\n\t\ttop = max(t->physical_block_size, t->io_min)\n\t\t\t+ t->alignment_offset;\n\t\tbottom = max(b->physical_block_size, b->io_min) + alignment;\n\n\t\t \n\t\tif (max(top, bottom) % min(top, bottom)) {\n\t\t\tt->misaligned = 1;\n\t\t\tret = -1;\n\t\t}\n\t}\n\n\tt->logical_block_size = max(t->logical_block_size,\n\t\t\t\t    b->logical_block_size);\n\n\tt->physical_block_size = max(t->physical_block_size,\n\t\t\t\t     b->physical_block_size);\n\n\tt->io_min = max(t->io_min, b->io_min);\n\tt->io_opt = lcm_not_zero(t->io_opt, b->io_opt);\n\tt->dma_alignment = max(t->dma_alignment, b->dma_alignment);\n\n\t \n\tif (b->chunk_sectors)\n\t\tt->chunk_sectors = gcd(t->chunk_sectors, b->chunk_sectors);\n\n\t \n\tif (t->physical_block_size & (t->logical_block_size - 1)) {\n\t\tt->physical_block_size = t->logical_block_size;\n\t\tt->misaligned = 1;\n\t\tret = -1;\n\t}\n\n\t \n\tif (t->io_min & (t->physical_block_size - 1)) {\n\t\tt->io_min = t->physical_block_size;\n\t\tt->misaligned = 1;\n\t\tret = -1;\n\t}\n\n\t \n\tif (t->io_opt & (t->physical_block_size - 1)) {\n\t\tt->io_opt = 0;\n\t\tt->misaligned = 1;\n\t\tret = -1;\n\t}\n\n\t \n\tif ((t->chunk_sectors << 9) & (t->physical_block_size - 1)) {\n\t\tt->chunk_sectors = 0;\n\t\tt->misaligned = 1;\n\t\tret = -1;\n\t}\n\n\tt->raid_partial_stripes_expensive =\n\t\tmax(t->raid_partial_stripes_expensive,\n\t\t    b->raid_partial_stripes_expensive);\n\n\t \n\tt->alignment_offset = lcm_not_zero(t->alignment_offset, alignment)\n\t\t% max(t->physical_block_size, t->io_min);\n\n\t \n\tif (t->alignment_offset & (t->logical_block_size - 1)) {\n\t\tt->misaligned = 1;\n\t\tret = -1;\n\t}\n\n\tt->max_sectors = blk_round_down_sectors(t->max_sectors, t->logical_block_size);\n\tt->max_hw_sectors = blk_round_down_sectors(t->max_hw_sectors, t->logical_block_size);\n\tt->max_dev_sectors = blk_round_down_sectors(t->max_dev_sectors, t->logical_block_size);\n\n\t \n\tif (b->discard_granularity) {\n\t\talignment = queue_limit_discard_alignment(b, start);\n\n\t\tif (t->discard_granularity != 0 &&\n\t\t    t->discard_alignment != alignment) {\n\t\t\ttop = t->discard_granularity + t->discard_alignment;\n\t\t\tbottom = b->discard_granularity + alignment;\n\n\t\t\t \n\t\t\tif ((max(top, bottom) % min(top, bottom)) != 0)\n\t\t\t\tt->discard_misaligned = 1;\n\t\t}\n\n\t\tt->max_discard_sectors = min_not_zero(t->max_discard_sectors,\n\t\t\t\t\t\t      b->max_discard_sectors);\n\t\tt->max_hw_discard_sectors = min_not_zero(t->max_hw_discard_sectors,\n\t\t\t\t\t\t\t b->max_hw_discard_sectors);\n\t\tt->discard_granularity = max(t->discard_granularity,\n\t\t\t\t\t     b->discard_granularity);\n\t\tt->discard_alignment = lcm_not_zero(t->discard_alignment, alignment) %\n\t\t\tt->discard_granularity;\n\t}\n\tt->max_secure_erase_sectors = min_not_zero(t->max_secure_erase_sectors,\n\t\t\t\t\t\t   b->max_secure_erase_sectors);\n\tt->zone_write_granularity = max(t->zone_write_granularity,\n\t\t\t\t\tb->zone_write_granularity);\n\tt->zoned = max(t->zoned, b->zoned);\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_stack_limits);\n\n \nvoid disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\n\t\t       sector_t offset)\n{\n\tstruct request_queue *t = disk->queue;\n\n\tif (blk_stack_limits(&t->limits, &bdev_get_queue(bdev)->limits,\n\t\t\tget_start_sect(bdev) + (offset >> 9)) < 0)\n\t\tpr_notice(\"%s: Warning: Device %pg is misaligned\\n\",\n\t\t\tdisk->disk_name, bdev);\n\n\tdisk_update_readahead(disk);\n}\nEXPORT_SYMBOL(disk_stack_limits);\n\n \nvoid blk_queue_update_dma_pad(struct request_queue *q, unsigned int mask)\n{\n\tif (mask > q->dma_pad_mask)\n\t\tq->dma_pad_mask = mask;\n}\nEXPORT_SYMBOL(blk_queue_update_dma_pad);\n\n \nvoid blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)\n{\n\tif (mask < PAGE_SIZE - 1) {\n\t\tmask = PAGE_SIZE - 1;\n\t\tprintk(KERN_INFO \"%s: set to minimum %lx\\n\",\n\t\t       __func__, mask);\n\t}\n\n\tq->limits.seg_boundary_mask = mask;\n}\nEXPORT_SYMBOL(blk_queue_segment_boundary);\n\n \nvoid blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)\n{\n\tq->limits.virt_boundary_mask = mask;\n\n\t \n\tif (mask)\n\t\tq->limits.max_segment_size = UINT_MAX;\n}\nEXPORT_SYMBOL(blk_queue_virt_boundary);\n\n \nvoid blk_queue_dma_alignment(struct request_queue *q, int mask)\n{\n\tq->limits.dma_alignment = mask;\n}\nEXPORT_SYMBOL(blk_queue_dma_alignment);\n\n \nvoid blk_queue_update_dma_alignment(struct request_queue *q, int mask)\n{\n\tBUG_ON(mask > PAGE_SIZE);\n\n\tif (mask > q->limits.dma_alignment)\n\t\tq->limits.dma_alignment = mask;\n}\nEXPORT_SYMBOL(blk_queue_update_dma_alignment);\n\n \nvoid blk_set_queue_depth(struct request_queue *q, unsigned int depth)\n{\n\tq->queue_depth = depth;\n\trq_qos_queue_depth_changed(q);\n}\nEXPORT_SYMBOL(blk_set_queue_depth);\n\n \nvoid blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)\n{\n\tif (wc) {\n\t\tblk_queue_flag_set(QUEUE_FLAG_HW_WC, q);\n\t\tblk_queue_flag_set(QUEUE_FLAG_WC, q);\n\t} else {\n\t\tblk_queue_flag_clear(QUEUE_FLAG_HW_WC, q);\n\t\tblk_queue_flag_clear(QUEUE_FLAG_WC, q);\n\t}\n\tif (fua)\n\t\tblk_queue_flag_set(QUEUE_FLAG_FUA, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_FUA, q);\n\n\twbt_set_write_cache(q, test_bit(QUEUE_FLAG_WC, &q->queue_flags));\n}\nEXPORT_SYMBOL_GPL(blk_queue_write_cache);\n\n \nvoid blk_queue_required_elevator_features(struct request_queue *q,\n\t\t\t\t\t  unsigned int features)\n{\n\tq->required_elevator_features = features;\n}\nEXPORT_SYMBOL_GPL(blk_queue_required_elevator_features);\n\n \nbool blk_queue_can_use_dma_map_merging(struct request_queue *q,\n\t\t\t\t       struct device *dev)\n{\n\tunsigned long boundary = dma_get_merge_boundary(dev);\n\n\tif (!boundary)\n\t\treturn false;\n\n\t \n\tblk_queue_virt_boundary(q, boundary);\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_queue_can_use_dma_map_merging);\n\nstatic bool disk_has_partitions(struct gendisk *disk)\n{\n\tunsigned long idx;\n\tstruct block_device *part;\n\tbool ret = false;\n\n\trcu_read_lock();\n\txa_for_each(&disk->part_tbl, idx, part) {\n\t\tif (bdev_is_partition(part)) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nvoid disk_set_zoned(struct gendisk *disk, enum blk_zoned_model model)\n{\n\tstruct request_queue *q = disk->queue;\n\tunsigned int old_model = q->limits.zoned;\n\n\tswitch (model) {\n\tcase BLK_ZONED_HM:\n\t\t \n\t\tWARN_ON_ONCE(!IS_ENABLED(CONFIG_BLK_DEV_ZONED));\n\t\tbreak;\n\tcase BLK_ZONED_HA:\n\t\t \n\t\tif (!IS_ENABLED(CONFIG_BLK_DEV_ZONED) ||\n\t\t    disk_has_partitions(disk))\n\t\t\tmodel = BLK_ZONED_NONE;\n\t\tbreak;\n\tcase BLK_ZONED_NONE:\n\tdefault:\n\t\tif (WARN_ON_ONCE(model != BLK_ZONED_NONE))\n\t\t\tmodel = BLK_ZONED_NONE;\n\t\tbreak;\n\t}\n\n\tq->limits.zoned = model;\n\tif (model != BLK_ZONED_NONE) {\n\t\t \n\t\tblk_queue_zone_write_granularity(q,\n\t\t\t\t\t\tqueue_logical_block_size(q));\n\t} else if (old_model != BLK_ZONED_NONE) {\n\t\tdisk_clear_zone_settings(disk);\n\t}\n}\nEXPORT_SYMBOL_GPL(disk_set_zoned);\n\nint bdev_alignment_offset(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q->limits.misaligned)\n\t\treturn -1;\n\tif (bdev_is_partition(bdev))\n\t\treturn queue_limit_alignment_offset(&q->limits,\n\t\t\t\tbdev->bd_start_sect);\n\treturn q->limits.alignment_offset;\n}\nEXPORT_SYMBOL_GPL(bdev_alignment_offset);\n\nunsigned int bdev_discard_alignment(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (bdev_is_partition(bdev))\n\t\treturn queue_limit_discard_alignment(&q->limits,\n\t\t\t\tbdev->bd_start_sect);\n\treturn q->limits.discard_alignment;\n}\nEXPORT_SYMBOL_GPL(bdev_discard_alignment);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}