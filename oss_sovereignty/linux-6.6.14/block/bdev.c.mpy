{
  "module_name": "bdev.c",
  "hash_id": "07040949999d663d4ebbeec4b721ee36fd60c9cdd292124fc2d7838ba97b8dca",
  "original_prompt": "Ingested from linux-6.6.14/block/bdev.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/kmod.h>\n#include <linux/major.h>\n#include <linux/device_cgroup.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/backing-dev.h>\n#include <linux/module.h>\n#include <linux/blkpg.h>\n#include <linux/magic.h>\n#include <linux/buffer_head.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/mount.h>\n#include <linux/pseudo_fs.h>\n#include <linux/uio.h>\n#include <linux/namei.h>\n#include <linux/part_stat.h>\n#include <linux/uaccess.h>\n#include <linux/stat.h>\n#include \"../fs/internal.h\"\n#include \"blk.h\"\n\nstruct bdev_inode {\n\tstruct block_device bdev;\n\tstruct inode vfs_inode;\n};\n\nstatic inline struct bdev_inode *BDEV_I(struct inode *inode)\n{\n\treturn container_of(inode, struct bdev_inode, vfs_inode);\n}\n\nstruct block_device *I_BDEV(struct inode *inode)\n{\n\treturn &BDEV_I(inode)->bdev;\n}\nEXPORT_SYMBOL(I_BDEV);\n\nstatic void bdev_write_inode(struct block_device *bdev)\n{\n\tstruct inode *inode = bdev->bd_inode;\n\tint ret;\n\n\tspin_lock(&inode->i_lock);\n\twhile (inode->i_state & I_DIRTY) {\n\t\tspin_unlock(&inode->i_lock);\n\t\tret = write_inode_now(inode, true);\n\t\tif (ret)\n\t\t\tpr_warn_ratelimited(\n\t\"VFS: Dirty inode writeback failed for block device %pg (err=%d).\\n\",\n\t\t\t\tbdev, ret);\n\t\tspin_lock(&inode->i_lock);\n\t}\n\tspin_unlock(&inode->i_lock);\n}\n\n \nstatic void kill_bdev(struct block_device *bdev)\n{\n\tstruct address_space *mapping = bdev->bd_inode->i_mapping;\n\n\tif (mapping_empty(mapping))\n\t\treturn;\n\n\tinvalidate_bh_lrus();\n\ttruncate_inode_pages(mapping, 0);\n}\n\n \nvoid invalidate_bdev(struct block_device *bdev)\n{\n\tstruct address_space *mapping = bdev->bd_inode->i_mapping;\n\n\tif (mapping->nrpages) {\n\t\tinvalidate_bh_lrus();\n\t\tlru_add_drain_all();\t \n\t\tinvalidate_mapping_pages(mapping, 0, -1);\n\t}\n}\nEXPORT_SYMBOL(invalidate_bdev);\n\n \nint truncate_bdev_range(struct block_device *bdev, blk_mode_t mode,\n\t\t\tloff_t lstart, loff_t lend)\n{\n\t \n\tif (!(mode & BLK_OPEN_EXCL)) {\n\t\tint err = bd_prepare_to_claim(bdev, truncate_bdev_range, NULL);\n\t\tif (err)\n\t\t\tgoto invalidate;\n\t}\n\n\ttruncate_inode_pages_range(bdev->bd_inode->i_mapping, lstart, lend);\n\tif (!(mode & BLK_OPEN_EXCL))\n\t\tbd_abort_claiming(bdev, truncate_bdev_range);\n\treturn 0;\n\ninvalidate:\n\t \n\treturn invalidate_inode_pages2_range(bdev->bd_inode->i_mapping,\n\t\t\t\t\t     lstart >> PAGE_SHIFT,\n\t\t\t\t\t     lend >> PAGE_SHIFT);\n}\n\nstatic void set_init_blocksize(struct block_device *bdev)\n{\n\tunsigned int bsize = bdev_logical_block_size(bdev);\n\tloff_t size = i_size_read(bdev->bd_inode);\n\n\twhile (bsize < PAGE_SIZE) {\n\t\tif (size & bsize)\n\t\t\tbreak;\n\t\tbsize <<= 1;\n\t}\n\tbdev->bd_inode->i_blkbits = blksize_bits(bsize);\n}\n\nint set_blocksize(struct block_device *bdev, int size)\n{\n\t \n\tif (size > PAGE_SIZE || size < 512 || !is_power_of_2(size))\n\t\treturn -EINVAL;\n\n\t \n\tif (size < bdev_logical_block_size(bdev))\n\t\treturn -EINVAL;\n\n\t \n\tif (bdev->bd_inode->i_blkbits != blksize_bits(size)) {\n\t\tsync_blockdev(bdev);\n\t\tbdev->bd_inode->i_blkbits = blksize_bits(size);\n\t\tkill_bdev(bdev);\n\t}\n\treturn 0;\n}\n\nEXPORT_SYMBOL(set_blocksize);\n\nint sb_set_blocksize(struct super_block *sb, int size)\n{\n\tif (set_blocksize(sb->s_bdev, size))\n\t\treturn 0;\n\t \n\tsb->s_blocksize = size;\n\tsb->s_blocksize_bits = blksize_bits(size);\n\treturn sb->s_blocksize;\n}\n\nEXPORT_SYMBOL(sb_set_blocksize);\n\nint sb_min_blocksize(struct super_block *sb, int size)\n{\n\tint minsize = bdev_logical_block_size(sb->s_bdev);\n\tif (size < minsize)\n\t\tsize = minsize;\n\treturn sb_set_blocksize(sb, size);\n}\n\nEXPORT_SYMBOL(sb_min_blocksize);\n\nint sync_blockdev_nowait(struct block_device *bdev)\n{\n\tif (!bdev)\n\t\treturn 0;\n\treturn filemap_flush(bdev->bd_inode->i_mapping);\n}\nEXPORT_SYMBOL_GPL(sync_blockdev_nowait);\n\n \nint sync_blockdev(struct block_device *bdev)\n{\n\tif (!bdev)\n\t\treturn 0;\n\treturn filemap_write_and_wait(bdev->bd_inode->i_mapping);\n}\nEXPORT_SYMBOL(sync_blockdev);\n\nint sync_blockdev_range(struct block_device *bdev, loff_t lstart, loff_t lend)\n{\n\treturn filemap_write_and_wait_range(bdev->bd_inode->i_mapping,\n\t\t\tlstart, lend);\n}\nEXPORT_SYMBOL(sync_blockdev_range);\n\n \nint freeze_bdev(struct block_device *bdev)\n{\n\tstruct super_block *sb;\n\tint error = 0;\n\n\tmutex_lock(&bdev->bd_fsfreeze_mutex);\n\tif (++bdev->bd_fsfreeze_count > 1)\n\t\tgoto done;\n\n\tsb = get_active_super(bdev);\n\tif (!sb)\n\t\tgoto sync;\n\tif (sb->s_op->freeze_super)\n\t\terror = sb->s_op->freeze_super(sb, FREEZE_HOLDER_USERSPACE);\n\telse\n\t\terror = freeze_super(sb, FREEZE_HOLDER_USERSPACE);\n\tdeactivate_super(sb);\n\n\tif (error) {\n\t\tbdev->bd_fsfreeze_count--;\n\t\tgoto done;\n\t}\n\tbdev->bd_fsfreeze_sb = sb;\n\nsync:\n\tsync_blockdev(bdev);\ndone:\n\tmutex_unlock(&bdev->bd_fsfreeze_mutex);\n\treturn error;\n}\nEXPORT_SYMBOL(freeze_bdev);\n\n \nint thaw_bdev(struct block_device *bdev)\n{\n\tstruct super_block *sb;\n\tint error = -EINVAL;\n\n\tmutex_lock(&bdev->bd_fsfreeze_mutex);\n\tif (!bdev->bd_fsfreeze_count)\n\t\tgoto out;\n\n\terror = 0;\n\tif (--bdev->bd_fsfreeze_count > 0)\n\t\tgoto out;\n\n\tsb = bdev->bd_fsfreeze_sb;\n\tif (!sb)\n\t\tgoto out;\n\n\tif (sb->s_op->thaw_super)\n\t\terror = sb->s_op->thaw_super(sb, FREEZE_HOLDER_USERSPACE);\n\telse\n\t\terror = thaw_super(sb, FREEZE_HOLDER_USERSPACE);\n\tif (error)\n\t\tbdev->bd_fsfreeze_count++;\n\telse\n\t\tbdev->bd_fsfreeze_sb = NULL;\nout:\n\tmutex_unlock(&bdev->bd_fsfreeze_mutex);\n\treturn error;\n}\nEXPORT_SYMBOL(thaw_bdev);\n\n \n\nstatic  __cacheline_aligned_in_smp DEFINE_MUTEX(bdev_lock);\nstatic struct kmem_cache * bdev_cachep __read_mostly;\n\nstatic struct inode *bdev_alloc_inode(struct super_block *sb)\n{\n\tstruct bdev_inode *ei = alloc_inode_sb(sb, bdev_cachep, GFP_KERNEL);\n\n\tif (!ei)\n\t\treturn NULL;\n\tmemset(&ei->bdev, 0, sizeof(ei->bdev));\n\treturn &ei->vfs_inode;\n}\n\nstatic void bdev_free_inode(struct inode *inode)\n{\n\tstruct block_device *bdev = I_BDEV(inode);\n\n\tfree_percpu(bdev->bd_stats);\n\tkfree(bdev->bd_meta_info);\n\n\tif (!bdev_is_partition(bdev)) {\n\t\tif (bdev->bd_disk && bdev->bd_disk->bdi)\n\t\t\tbdi_put(bdev->bd_disk->bdi);\n\t\tkfree(bdev->bd_disk);\n\t}\n\n\tif (MAJOR(bdev->bd_dev) == BLOCK_EXT_MAJOR)\n\t\tblk_free_ext_minor(MINOR(bdev->bd_dev));\n\n\tkmem_cache_free(bdev_cachep, BDEV_I(inode));\n}\n\nstatic void init_once(void *data)\n{\n\tstruct bdev_inode *ei = data;\n\n\tinode_init_once(&ei->vfs_inode);\n}\n\nstatic void bdev_evict_inode(struct inode *inode)\n{\n\ttruncate_inode_pages_final(&inode->i_data);\n\tinvalidate_inode_buffers(inode);  \n\tclear_inode(inode);\n}\n\nstatic const struct super_operations bdev_sops = {\n\t.statfs = simple_statfs,\n\t.alloc_inode = bdev_alloc_inode,\n\t.free_inode = bdev_free_inode,\n\t.drop_inode = generic_delete_inode,\n\t.evict_inode = bdev_evict_inode,\n};\n\nstatic int bd_init_fs_context(struct fs_context *fc)\n{\n\tstruct pseudo_fs_context *ctx = init_pseudo(fc, BDEVFS_MAGIC);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tfc->s_iflags |= SB_I_CGROUPWB;\n\tctx->ops = &bdev_sops;\n\treturn 0;\n}\n\nstatic struct file_system_type bd_type = {\n\t.name\t\t= \"bdev\",\n\t.init_fs_context = bd_init_fs_context,\n\t.kill_sb\t= kill_anon_super,\n};\n\nstruct super_block *blockdev_superblock __read_mostly;\nEXPORT_SYMBOL_GPL(blockdev_superblock);\n\nvoid __init bdev_cache_init(void)\n{\n\tint err;\n\tstatic struct vfsmount *bd_mnt;\n\n\tbdev_cachep = kmem_cache_create(\"bdev_cache\", sizeof(struct bdev_inode),\n\t\t\t0, (SLAB_HWCACHE_ALIGN|SLAB_RECLAIM_ACCOUNT|\n\t\t\t\tSLAB_MEM_SPREAD|SLAB_ACCOUNT|SLAB_PANIC),\n\t\t\tinit_once);\n\terr = register_filesystem(&bd_type);\n\tif (err)\n\t\tpanic(\"Cannot register bdev pseudo-fs\");\n\tbd_mnt = kern_mount(&bd_type);\n\tif (IS_ERR(bd_mnt))\n\t\tpanic(\"Cannot create bdev pseudo-fs\");\n\tblockdev_superblock = bd_mnt->mnt_sb;    \n}\n\nstruct block_device *bdev_alloc(struct gendisk *disk, u8 partno)\n{\n\tstruct block_device *bdev;\n\tstruct inode *inode;\n\n\tinode = new_inode(blockdev_superblock);\n\tif (!inode)\n\t\treturn NULL;\n\tinode->i_mode = S_IFBLK;\n\tinode->i_rdev = 0;\n\tinode->i_data.a_ops = &def_blk_aops;\n\tmapping_set_gfp_mask(&inode->i_data, GFP_USER);\n\n\tbdev = I_BDEV(inode);\n\tmutex_init(&bdev->bd_fsfreeze_mutex);\n\tspin_lock_init(&bdev->bd_size_lock);\n\tmutex_init(&bdev->bd_holder_lock);\n\tbdev->bd_partno = partno;\n\tbdev->bd_inode = inode;\n\tbdev->bd_queue = disk->queue;\n\tif (partno)\n\t\tbdev->bd_has_submit_bio = disk->part0->bd_has_submit_bio;\n\telse\n\t\tbdev->bd_has_submit_bio = false;\n\tbdev->bd_stats = alloc_percpu(struct disk_stats);\n\tif (!bdev->bd_stats) {\n\t\tiput(inode);\n\t\treturn NULL;\n\t}\n\tbdev->bd_disk = disk;\n\treturn bdev;\n}\n\nvoid bdev_set_nr_sectors(struct block_device *bdev, sector_t sectors)\n{\n\tspin_lock(&bdev->bd_size_lock);\n\ti_size_write(bdev->bd_inode, (loff_t)sectors << SECTOR_SHIFT);\n\tbdev->bd_nr_sectors = sectors;\n\tspin_unlock(&bdev->bd_size_lock);\n}\n\nvoid bdev_add(struct block_device *bdev, dev_t dev)\n{\n\tif (bdev_stable_writes(bdev))\n\t\tmapping_set_stable_writes(bdev->bd_inode->i_mapping);\n\tbdev->bd_dev = dev;\n\tbdev->bd_inode->i_rdev = dev;\n\tbdev->bd_inode->i_ino = dev;\n\tinsert_inode_hash(bdev->bd_inode);\n}\n\nlong nr_blockdev_pages(void)\n{\n\tstruct inode *inode;\n\tlong ret = 0;\n\n\tspin_lock(&blockdev_superblock->s_inode_list_lock);\n\tlist_for_each_entry(inode, &blockdev_superblock->s_inodes, i_sb_list)\n\t\tret += inode->i_mapping->nrpages;\n\tspin_unlock(&blockdev_superblock->s_inode_list_lock);\n\n\treturn ret;\n}\n\n \nstatic bool bd_may_claim(struct block_device *bdev, void *holder,\n\t\tconst struct blk_holder_ops *hops)\n{\n\tstruct block_device *whole = bdev_whole(bdev);\n\n\tlockdep_assert_held(&bdev_lock);\n\n\tif (bdev->bd_holder) {\n\t\t \n\t\tif (bdev->bd_holder == holder) {\n\t\t\tif (WARN_ON_ONCE(bdev->bd_holder_ops != hops))\n\t\t\t\treturn false;\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n\n\t \n\tif (whole != bdev &&\n\t    whole->bd_holder && whole->bd_holder != bd_may_claim)\n\t\treturn false;\n\treturn true;\n}\n\n \nint bd_prepare_to_claim(struct block_device *bdev, void *holder,\n\t\tconst struct blk_holder_ops *hops)\n{\n\tstruct block_device *whole = bdev_whole(bdev);\n\n\tif (WARN_ON_ONCE(!holder))\n\t\treturn -EINVAL;\nretry:\n\tmutex_lock(&bdev_lock);\n\t \n\tif (!bd_may_claim(bdev, holder, hops)) {\n\t\tmutex_unlock(&bdev_lock);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tif (whole->bd_claiming) {\n\t\twait_queue_head_t *wq = bit_waitqueue(&whole->bd_claiming, 0);\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(wq, &wait, TASK_UNINTERRUPTIBLE);\n\t\tmutex_unlock(&bdev_lock);\n\t\tschedule();\n\t\tfinish_wait(wq, &wait);\n\t\tgoto retry;\n\t}\n\n\t \n\twhole->bd_claiming = holder;\n\tmutex_unlock(&bdev_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(bd_prepare_to_claim);  \n\nstatic void bd_clear_claiming(struct block_device *whole, void *holder)\n{\n\tlockdep_assert_held(&bdev_lock);\n\t \n\tBUG_ON(whole->bd_claiming != holder);\n\twhole->bd_claiming = NULL;\n\twake_up_bit(&whole->bd_claiming, 0);\n}\n\n \nstatic void bd_finish_claiming(struct block_device *bdev, void *holder,\n\t\tconst struct blk_holder_ops *hops)\n{\n\tstruct block_device *whole = bdev_whole(bdev);\n\n\tmutex_lock(&bdev_lock);\n\tBUG_ON(!bd_may_claim(bdev, holder, hops));\n\t \n\twhole->bd_holders++;\n\twhole->bd_holder = bd_may_claim;\n\tbdev->bd_holders++;\n\tmutex_lock(&bdev->bd_holder_lock);\n\tbdev->bd_holder = holder;\n\tbdev->bd_holder_ops = hops;\n\tmutex_unlock(&bdev->bd_holder_lock);\n\tbd_clear_claiming(whole, holder);\n\tmutex_unlock(&bdev_lock);\n}\n\n \nvoid bd_abort_claiming(struct block_device *bdev, void *holder)\n{\n\tmutex_lock(&bdev_lock);\n\tbd_clear_claiming(bdev_whole(bdev), holder);\n\tmutex_unlock(&bdev_lock);\n}\nEXPORT_SYMBOL(bd_abort_claiming);\n\nstatic void bd_end_claim(struct block_device *bdev, void *holder)\n{\n\tstruct block_device *whole = bdev_whole(bdev);\n\tbool unblock = false;\n\n\t \n\tmutex_lock(&bdev_lock);\n\tWARN_ON_ONCE(bdev->bd_holder != holder);\n\tWARN_ON_ONCE(--bdev->bd_holders < 0);\n\tWARN_ON_ONCE(--whole->bd_holders < 0);\n\tif (!bdev->bd_holders) {\n\t\tmutex_lock(&bdev->bd_holder_lock);\n\t\tbdev->bd_holder = NULL;\n\t\tbdev->bd_holder_ops = NULL;\n\t\tmutex_unlock(&bdev->bd_holder_lock);\n\t\tif (bdev->bd_write_holder)\n\t\t\tunblock = true;\n\t}\n\tif (!whole->bd_holders)\n\t\twhole->bd_holder = NULL;\n\tmutex_unlock(&bdev_lock);\n\n\t \n\tif (unblock) {\n\t\tdisk_unblock_events(bdev->bd_disk);\n\t\tbdev->bd_write_holder = false;\n\t}\n}\n\nstatic void blkdev_flush_mapping(struct block_device *bdev)\n{\n\tWARN_ON_ONCE(bdev->bd_holders);\n\tsync_blockdev(bdev);\n\tkill_bdev(bdev);\n\tbdev_write_inode(bdev);\n}\n\nstatic int blkdev_get_whole(struct block_device *bdev, blk_mode_t mode)\n{\n\tstruct gendisk *disk = bdev->bd_disk;\n\tint ret;\n\n\tif (disk->fops->open) {\n\t\tret = disk->fops->open(disk, mode);\n\t\tif (ret) {\n\t\t\t \n\t\t\tif (ret == -ENOMEDIUM &&\n\t\t\t     test_bit(GD_NEED_PART_SCAN, &disk->state))\n\t\t\t\tbdev_disk_changed(disk, true);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (!atomic_read(&bdev->bd_openers))\n\t\tset_init_blocksize(bdev);\n\tif (test_bit(GD_NEED_PART_SCAN, &disk->state))\n\t\tbdev_disk_changed(disk, false);\n\tatomic_inc(&bdev->bd_openers);\n\treturn 0;\n}\n\nstatic void blkdev_put_whole(struct block_device *bdev)\n{\n\tif (atomic_dec_and_test(&bdev->bd_openers))\n\t\tblkdev_flush_mapping(bdev);\n\tif (bdev->bd_disk->fops->release)\n\t\tbdev->bd_disk->fops->release(bdev->bd_disk);\n}\n\nstatic int blkdev_get_part(struct block_device *part, blk_mode_t mode)\n{\n\tstruct gendisk *disk = part->bd_disk;\n\tint ret;\n\n\tret = blkdev_get_whole(bdev_whole(part), mode);\n\tif (ret)\n\t\treturn ret;\n\n\tret = -ENXIO;\n\tif (!bdev_nr_sectors(part))\n\t\tgoto out_blkdev_put;\n\n\tif (!atomic_read(&part->bd_openers)) {\n\t\tdisk->open_partitions++;\n\t\tset_init_blocksize(part);\n\t}\n\tatomic_inc(&part->bd_openers);\n\treturn 0;\n\nout_blkdev_put:\n\tblkdev_put_whole(bdev_whole(part));\n\treturn ret;\n}\n\nstatic void blkdev_put_part(struct block_device *part)\n{\n\tstruct block_device *whole = bdev_whole(part);\n\n\tif (atomic_dec_and_test(&part->bd_openers)) {\n\t\tblkdev_flush_mapping(part);\n\t\twhole->bd_disk->open_partitions--;\n\t}\n\tblkdev_put_whole(whole);\n}\n\nstruct block_device *blkdev_get_no_open(dev_t dev)\n{\n\tstruct block_device *bdev;\n\tstruct inode *inode;\n\n\tinode = ilookup(blockdev_superblock, dev);\n\tif (!inode && IS_ENABLED(CONFIG_BLOCK_LEGACY_AUTOLOAD)) {\n\t\tblk_request_module(dev);\n\t\tinode = ilookup(blockdev_superblock, dev);\n\t\tif (inode)\n\t\t\tpr_warn_ratelimited(\n\"block device autoloading is deprecated and will be removed.\\n\");\n\t}\n\tif (!inode)\n\t\treturn NULL;\n\n\t \n\tbdev = &BDEV_I(inode)->bdev;\n\tif (!kobject_get_unless_zero(&bdev->bd_device.kobj))\n\t\tbdev = NULL;\n\tiput(inode);\n\treturn bdev;\n}\n\nvoid blkdev_put_no_open(struct block_device *bdev)\n{\n\tput_device(&bdev->bd_device);\n}\n\t\n \nstruct block_device *blkdev_get_by_dev(dev_t dev, blk_mode_t mode, void *holder,\n\t\tconst struct blk_holder_ops *hops)\n{\n\tbool unblock_events = true;\n\tstruct block_device *bdev;\n\tstruct gendisk *disk;\n\tint ret;\n\n\tret = devcgroup_check_permission(DEVCG_DEV_BLOCK,\n\t\t\tMAJOR(dev), MINOR(dev),\n\t\t\t((mode & BLK_OPEN_READ) ? DEVCG_ACC_READ : 0) |\n\t\t\t((mode & BLK_OPEN_WRITE) ? DEVCG_ACC_WRITE : 0));\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tbdev = blkdev_get_no_open(dev);\n\tif (!bdev)\n\t\treturn ERR_PTR(-ENXIO);\n\tdisk = bdev->bd_disk;\n\n\tif (holder) {\n\t\tmode |= BLK_OPEN_EXCL;\n\t\tret = bd_prepare_to_claim(bdev, holder, hops);\n\t\tif (ret)\n\t\t\tgoto put_blkdev;\n\t} else {\n\t\tif (WARN_ON_ONCE(mode & BLK_OPEN_EXCL)) {\n\t\t\tret = -EIO;\n\t\t\tgoto put_blkdev;\n\t\t}\n\t}\n\n\tdisk_block_events(disk);\n\n\tmutex_lock(&disk->open_mutex);\n\tret = -ENXIO;\n\tif (!disk_live(disk))\n\t\tgoto abort_claiming;\n\tif (!try_module_get(disk->fops->owner))\n\t\tgoto abort_claiming;\n\tif (bdev_is_partition(bdev))\n\t\tret = blkdev_get_part(bdev, mode);\n\telse\n\t\tret = blkdev_get_whole(bdev, mode);\n\tif (ret)\n\t\tgoto put_module;\n\tif (holder) {\n\t\tbd_finish_claiming(bdev, holder, hops);\n\n\t\t \n\t\tif ((mode & BLK_OPEN_WRITE) && !bdev->bd_write_holder &&\n\t\t    (disk->event_flags & DISK_EVENT_FLAG_BLOCK_ON_EXCL_WRITE)) {\n\t\t\tbdev->bd_write_holder = true;\n\t\t\tunblock_events = false;\n\t\t}\n\t}\n\tmutex_unlock(&disk->open_mutex);\n\n\tif (unblock_events)\n\t\tdisk_unblock_events(disk);\n\treturn bdev;\nput_module:\n\tmodule_put(disk->fops->owner);\nabort_claiming:\n\tif (holder)\n\t\tbd_abort_claiming(bdev, holder);\n\tmutex_unlock(&disk->open_mutex);\n\tdisk_unblock_events(disk);\nput_blkdev:\n\tblkdev_put_no_open(bdev);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(blkdev_get_by_dev);\n\n \nstruct block_device *blkdev_get_by_path(const char *path, blk_mode_t mode,\n\t\tvoid *holder, const struct blk_holder_ops *hops)\n{\n\tstruct block_device *bdev;\n\tdev_t dev;\n\tint error;\n\n\terror = lookup_bdev(path, &dev);\n\tif (error)\n\t\treturn ERR_PTR(error);\n\n\tbdev = blkdev_get_by_dev(dev, mode, holder, hops);\n\tif (!IS_ERR(bdev) && (mode & BLK_OPEN_WRITE) && bdev_read_only(bdev)) {\n\t\tblkdev_put(bdev, holder);\n\t\treturn ERR_PTR(-EACCES);\n\t}\n\n\treturn bdev;\n}\nEXPORT_SYMBOL(blkdev_get_by_path);\n\nvoid blkdev_put(struct block_device *bdev, void *holder)\n{\n\tstruct gendisk *disk = bdev->bd_disk;\n\n\t \n\tif (atomic_read(&bdev->bd_openers) == 1)\n\t\tsync_blockdev(bdev);\n\n\tmutex_lock(&disk->open_mutex);\n\tif (holder)\n\t\tbd_end_claim(bdev, holder);\n\n\t \n\tdisk_flush_events(disk, DISK_EVENT_MEDIA_CHANGE);\n\n\tif (bdev_is_partition(bdev))\n\t\tblkdev_put_part(bdev);\n\telse\n\t\tblkdev_put_whole(bdev);\n\tmutex_unlock(&disk->open_mutex);\n\n\tmodule_put(disk->fops->owner);\n\tblkdev_put_no_open(bdev);\n}\nEXPORT_SYMBOL(blkdev_put);\n\n \nint lookup_bdev(const char *pathname, dev_t *dev)\n{\n\tstruct inode *inode;\n\tstruct path path;\n\tint error;\n\n\tif (!pathname || !*pathname)\n\t\treturn -EINVAL;\n\n\terror = kern_path(pathname, LOOKUP_FOLLOW, &path);\n\tif (error)\n\t\treturn error;\n\n\tinode = d_backing_inode(path.dentry);\n\terror = -ENOTBLK;\n\tif (!S_ISBLK(inode->i_mode))\n\t\tgoto out_path_put;\n\terror = -EACCES;\n\tif (!may_open_dev(&path))\n\t\tgoto out_path_put;\n\n\t*dev = inode->i_rdev;\n\terror = 0;\nout_path_put:\n\tpath_put(&path);\n\treturn error;\n}\nEXPORT_SYMBOL(lookup_bdev);\n\n \nvoid bdev_mark_dead(struct block_device *bdev, bool surprise)\n{\n\tmutex_lock(&bdev->bd_holder_lock);\n\tif (bdev->bd_holder_ops && bdev->bd_holder_ops->mark_dead)\n\t\tbdev->bd_holder_ops->mark_dead(bdev, surprise);\n\telse\n\t\tsync_blockdev(bdev);\n\tmutex_unlock(&bdev->bd_holder_lock);\n\n\tinvalidate_bdev(bdev);\n}\n#ifdef CONFIG_DASD_MODULE\n \nEXPORT_SYMBOL_GPL(bdev_mark_dead);\n#endif\n\nvoid sync_bdevs(bool wait)\n{\n\tstruct inode *inode, *old_inode = NULL;\n\n\tspin_lock(&blockdev_superblock->s_inode_list_lock);\n\tlist_for_each_entry(inode, &blockdev_superblock->s_inodes, i_sb_list) {\n\t\tstruct address_space *mapping = inode->i_mapping;\n\t\tstruct block_device *bdev;\n\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW) ||\n\t\t    mapping->nrpages == 0) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\tspin_unlock(&blockdev_superblock->s_inode_list_lock);\n\t\t \n\t\tiput(old_inode);\n\t\told_inode = inode;\n\t\tbdev = I_BDEV(inode);\n\n\t\tmutex_lock(&bdev->bd_disk->open_mutex);\n\t\tif (!atomic_read(&bdev->bd_openers)) {\n\t\t\t;  \n\t\t} else if (wait) {\n\t\t\t \n\t\t\tfilemap_fdatawait_keep_errors(inode->i_mapping);\n\t\t} else {\n\t\t\tfilemap_fdatawrite(inode->i_mapping);\n\t\t}\n\t\tmutex_unlock(&bdev->bd_disk->open_mutex);\n\n\t\tspin_lock(&blockdev_superblock->s_inode_list_lock);\n\t}\n\tspin_unlock(&blockdev_superblock->s_inode_list_lock);\n\tiput(old_inode);\n}\n\n \nvoid bdev_statx_dioalign(struct inode *inode, struct kstat *stat)\n{\n\tstruct block_device *bdev;\n\n\tbdev = blkdev_get_no_open(inode->i_rdev);\n\tif (!bdev)\n\t\treturn;\n\n\tstat->dio_mem_align = bdev_dma_alignment(bdev) + 1;\n\tstat->dio_offset_align = bdev_logical_block_size(bdev);\n\tstat->result_mask |= STATX_DIOALIGN;\n\n\tblkdev_put_no_open(bdev);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}