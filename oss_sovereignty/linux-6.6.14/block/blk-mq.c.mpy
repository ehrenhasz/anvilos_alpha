{
  "module_name": "blk-mq.c",
  "hash_id": "755240f479fb4d2ed0f4975c496005cfe13915534696cb03d4f472a566f7f2f9",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-mq.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/backing-dev.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/kmemleak.h>\n#include <linux/mm.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/smp.h>\n#include <linux/interrupt.h>\n#include <linux/llist.h>\n#include <linux/cpu.h>\n#include <linux/cache.h>\n#include <linux/sched/sysctl.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/signal.h>\n#include <linux/delay.h>\n#include <linux/crash_dump.h>\n#include <linux/prefetch.h>\n#include <linux/blk-crypto.h>\n#include <linux/part_stat.h>\n\n#include <trace/events/block.h>\n\n#include <linux/t10-pi.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-pm.h\"\n#include \"blk-stat.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n#include \"blk-ioprio.h\"\n\nstatic DEFINE_PER_CPU(struct llist_head, blk_cpu_done);\nstatic DEFINE_PER_CPU(call_single_data_t, blk_cpu_csd);\n\nstatic void blk_mq_insert_request(struct request *rq, blk_insert_t flags);\nstatic void blk_mq_request_bypass_insert(struct request *rq,\n\t\tblk_insert_t flags);\nstatic void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,\n\t\tstruct list_head *list);\nstatic int blk_hctx_poll(struct request_queue *q, struct blk_mq_hw_ctx *hctx,\n\t\t\t struct io_comp_batch *iob, unsigned int flags);\n\n \nstatic bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)\n{\n\treturn !list_empty_careful(&hctx->dispatch) ||\n\t\tsbitmap_any_bit_set(&hctx->ctx_map) ||\n\t\t\tblk_mq_sched_has_work(hctx);\n}\n\n \nstatic void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t     struct blk_mq_ctx *ctx)\n{\n\tconst int bit = ctx->index_hw[hctx->type];\n\n\tif (!sbitmap_test_bit(&hctx->ctx_map, bit))\n\t\tsbitmap_set_bit(&hctx->ctx_map, bit);\n}\n\nstatic void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t      struct blk_mq_ctx *ctx)\n{\n\tconst int bit = ctx->index_hw[hctx->type];\n\n\tsbitmap_clear_bit(&hctx->ctx_map, bit);\n}\n\nstruct mq_inflight {\n\tstruct block_device *part;\n\tunsigned int inflight[2];\n};\n\nstatic bool blk_mq_check_inflight(struct request *rq, void *priv)\n{\n\tstruct mq_inflight *mi = priv;\n\n\tif (rq->part && blk_do_io_stat(rq) &&\n\t    (!mi->part->bd_partno || rq->part == mi->part) &&\n\t    blk_mq_rq_state(rq) == MQ_RQ_IN_FLIGHT)\n\t\tmi->inflight[rq_data_dir(rq)]++;\n\n\treturn true;\n}\n\nunsigned int blk_mq_in_flight(struct request_queue *q,\n\t\tstruct block_device *part)\n{\n\tstruct mq_inflight mi = { .part = part };\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);\n\n\treturn mi.inflight[0] + mi.inflight[1];\n}\n\nvoid blk_mq_in_flight_rw(struct request_queue *q, struct block_device *part,\n\t\tunsigned int inflight[2])\n{\n\tstruct mq_inflight mi = { .part = part };\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_inflight, &mi);\n\tinflight[0] = mi.inflight[0];\n\tinflight[1] = mi.inflight[1];\n}\n\nvoid blk_freeze_queue_start(struct request_queue *q)\n{\n\tmutex_lock(&q->mq_freeze_lock);\n\tif (++q->mq_freeze_depth == 1) {\n\t\tpercpu_ref_kill(&q->q_usage_counter);\n\t\tmutex_unlock(&q->mq_freeze_lock);\n\t\tif (queue_is_mq(q))\n\t\t\tblk_mq_run_hw_queues(q, false);\n\t} else {\n\t\tmutex_unlock(&q->mq_freeze_lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_freeze_queue_start);\n\nvoid blk_mq_freeze_queue_wait(struct request_queue *q)\n{\n\twait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);\n\nint blk_mq_freeze_queue_wait_timeout(struct request_queue *q,\n\t\t\t\t     unsigned long timeout)\n{\n\treturn wait_event_timeout(q->mq_freeze_wq,\n\t\t\t\t\tpercpu_ref_is_zero(&q->q_usage_counter),\n\t\t\t\t\ttimeout);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait_timeout);\n\n \nvoid blk_freeze_queue(struct request_queue *q)\n{\n\t \n\tblk_freeze_queue_start(q);\n\tblk_mq_freeze_queue_wait(q);\n}\n\nvoid blk_mq_freeze_queue(struct request_queue *q)\n{\n\t \n\tblk_freeze_queue(q);\n}\nEXPORT_SYMBOL_GPL(blk_mq_freeze_queue);\n\nvoid __blk_mq_unfreeze_queue(struct request_queue *q, bool force_atomic)\n{\n\tmutex_lock(&q->mq_freeze_lock);\n\tif (force_atomic)\n\t\tq->q_usage_counter.data->force_atomic = true;\n\tq->mq_freeze_depth--;\n\tWARN_ON_ONCE(q->mq_freeze_depth < 0);\n\tif (!q->mq_freeze_depth) {\n\t\tpercpu_ref_resurrect(&q->q_usage_counter);\n\t\twake_up_all(&q->mq_freeze_wq);\n\t}\n\tmutex_unlock(&q->mq_freeze_lock);\n}\n\nvoid blk_mq_unfreeze_queue(struct request_queue *q)\n{\n\t__blk_mq_unfreeze_queue(q, false);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unfreeze_queue);\n\n \nvoid blk_mq_quiesce_queue_nowait(struct request_queue *q)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\tif (!q->quiesce_depth++)\n\t\tblk_queue_flag_set(QUEUE_FLAG_QUIESCED, q);\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n}\nEXPORT_SYMBOL_GPL(blk_mq_quiesce_queue_nowait);\n\n \nvoid blk_mq_wait_quiesce_done(struct blk_mq_tag_set *set)\n{\n\tif (set->flags & BLK_MQ_F_BLOCKING)\n\t\tsynchronize_srcu(set->srcu);\n\telse\n\t\tsynchronize_rcu();\n}\nEXPORT_SYMBOL_GPL(blk_mq_wait_quiesce_done);\n\n \nvoid blk_mq_quiesce_queue(struct request_queue *q)\n{\n\tblk_mq_quiesce_queue_nowait(q);\n\t \n\tif (queue_is_mq(q))\n\t\tblk_mq_wait_quiesce_done(q->tag_set);\n}\nEXPORT_SYMBOL_GPL(blk_mq_quiesce_queue);\n\n \nvoid blk_mq_unquiesce_queue(struct request_queue *q)\n{\n\tunsigned long flags;\n\tbool run_queue = false;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\tif (WARN_ON_ONCE(q->quiesce_depth <= 0)) {\n\t\t;\n\t} else if (!--q->quiesce_depth) {\n\t\tblk_queue_flag_clear(QUEUE_FLAG_QUIESCED, q);\n\t\trun_queue = true;\n\t}\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\n\t \n\tif (run_queue)\n\t\tblk_mq_run_hw_queues(q, true);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unquiesce_queue);\n\nvoid blk_mq_quiesce_tagset(struct blk_mq_tag_set *set)\n{\n\tstruct request_queue *q;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tif (!blk_queue_skip_tagset_quiesce(q))\n\t\t\tblk_mq_quiesce_queue_nowait(q);\n\t}\n\tblk_mq_wait_quiesce_done(set);\n\tmutex_unlock(&set->tag_list_lock);\n}\nEXPORT_SYMBOL_GPL(blk_mq_quiesce_tagset);\n\nvoid blk_mq_unquiesce_tagset(struct blk_mq_tag_set *set)\n{\n\tstruct request_queue *q;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tif (!blk_queue_skip_tagset_quiesce(q))\n\t\t\tblk_mq_unquiesce_queue(q);\n\t}\n\tmutex_unlock(&set->tag_list_lock);\n}\nEXPORT_SYMBOL_GPL(blk_mq_unquiesce_tagset);\n\nvoid blk_mq_wake_waiters(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\tblk_mq_tag_wakeup_all(hctx->tags, true);\n}\n\nvoid blk_rq_init(struct request_queue *q, struct request *rq)\n{\n\tmemset(rq, 0, sizeof(*rq));\n\n\tINIT_LIST_HEAD(&rq->queuelist);\n\trq->q = q;\n\trq->__sector = (sector_t) -1;\n\tINIT_HLIST_NODE(&rq->hash);\n\tRB_CLEAR_NODE(&rq->rb_node);\n\trq->tag = BLK_MQ_NO_TAG;\n\trq->internal_tag = BLK_MQ_NO_TAG;\n\trq->start_time_ns = ktime_get_ns();\n\trq->part = NULL;\n\tblk_crypto_rq_set_defaults(rq);\n}\nEXPORT_SYMBOL(blk_rq_init);\n\n \nstatic inline void blk_mq_rq_time_init(struct request *rq, u64 alloc_time_ns)\n{\n\tif (blk_mq_need_time_stamp(rq))\n\t\trq->start_time_ns = ktime_get_ns();\n\telse\n\t\trq->start_time_ns = 0;\n\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n\tif (blk_queue_rq_alloc_time(rq->q))\n\t\trq->alloc_time_ns = alloc_time_ns ?: rq->start_time_ns;\n\telse\n\t\trq->alloc_time_ns = 0;\n#endif\n}\n\nstatic struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,\n\t\tstruct blk_mq_tags *tags, unsigned int tag)\n{\n\tstruct blk_mq_ctx *ctx = data->ctx;\n\tstruct blk_mq_hw_ctx *hctx = data->hctx;\n\tstruct request_queue *q = data->q;\n\tstruct request *rq = tags->static_rqs[tag];\n\n\trq->q = q;\n\trq->mq_ctx = ctx;\n\trq->mq_hctx = hctx;\n\trq->cmd_flags = data->cmd_flags;\n\n\tif (data->flags & BLK_MQ_REQ_PM)\n\t\tdata->rq_flags |= RQF_PM;\n\tif (blk_queue_io_stat(q))\n\t\tdata->rq_flags |= RQF_IO_STAT;\n\trq->rq_flags = data->rq_flags;\n\n\tif (data->rq_flags & RQF_SCHED_TAGS) {\n\t\trq->tag = BLK_MQ_NO_TAG;\n\t\trq->internal_tag = tag;\n\t} else {\n\t\trq->tag = tag;\n\t\trq->internal_tag = BLK_MQ_NO_TAG;\n\t}\n\trq->timeout = 0;\n\n\trq->part = NULL;\n\trq->io_start_time_ns = 0;\n\trq->stats_sectors = 0;\n\trq->nr_phys_segments = 0;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\trq->nr_integrity_segments = 0;\n#endif\n\trq->end_io = NULL;\n\trq->end_io_data = NULL;\n\n\tblk_crypto_rq_set_defaults(rq);\n\tINIT_LIST_HEAD(&rq->queuelist);\n\t \n\tWRITE_ONCE(rq->deadline, 0);\n\treq_ref_set(rq, 1);\n\n\tif (rq->rq_flags & RQF_USE_SCHED) {\n\t\tstruct elevator_queue *e = data->q->elevator;\n\n\t\tINIT_HLIST_NODE(&rq->hash);\n\t\tRB_CLEAR_NODE(&rq->rb_node);\n\n\t\tif (e->type->ops.prepare_request)\n\t\t\te->type->ops.prepare_request(rq);\n\t}\n\n\treturn rq;\n}\n\nstatic inline struct request *\n__blk_mq_alloc_requests_batch(struct blk_mq_alloc_data *data)\n{\n\tunsigned int tag, tag_offset;\n\tstruct blk_mq_tags *tags;\n\tstruct request *rq;\n\tunsigned long tag_mask;\n\tint i, nr = 0;\n\n\ttag_mask = blk_mq_get_tags(data, data->nr_tags, &tag_offset);\n\tif (unlikely(!tag_mask))\n\t\treturn NULL;\n\n\ttags = blk_mq_tags_from_data(data);\n\tfor (i = 0; tag_mask; i++) {\n\t\tif (!(tag_mask & (1UL << i)))\n\t\t\tcontinue;\n\t\ttag = tag_offset + i;\n\t\tprefetch(tags->static_rqs[tag]);\n\t\ttag_mask &= ~(1UL << i);\n\t\trq = blk_mq_rq_ctx_init(data, tags, tag);\n\t\trq_list_add(data->cached_rq, rq);\n\t\tnr++;\n\t}\n\t \n\tpercpu_ref_get_many(&data->q->q_usage_counter, nr - 1);\n\tdata->nr_tags -= nr;\n\n\treturn rq_list_pop(data->cached_rq);\n}\n\nstatic struct request *__blk_mq_alloc_requests(struct blk_mq_alloc_data *data)\n{\n\tstruct request_queue *q = data->q;\n\tu64 alloc_time_ns = 0;\n\tstruct request *rq;\n\tunsigned int tag;\n\n\t \n\tif (blk_queue_rq_alloc_time(q))\n\t\talloc_time_ns = ktime_get_ns();\n\n\tif (data->cmd_flags & REQ_NOWAIT)\n\t\tdata->flags |= BLK_MQ_REQ_NOWAIT;\n\n\tif (q->elevator) {\n\t\t \n\t\tdata->rq_flags |= RQF_SCHED_TAGS;\n\n\t\t \n\t\tif ((data->cmd_flags & REQ_OP_MASK) != REQ_OP_FLUSH &&\n\t\t    !blk_op_is_passthrough(data->cmd_flags)) {\n\t\t\tstruct elevator_mq_ops *ops = &q->elevator->type->ops;\n\n\t\t\tWARN_ON_ONCE(data->flags & BLK_MQ_REQ_RESERVED);\n\n\t\t\tdata->rq_flags |= RQF_USE_SCHED;\n\t\t\tif (ops->limit_depth)\n\t\t\t\tops->limit_depth(data->cmd_flags, data);\n\t\t}\n\t}\n\nretry:\n\tdata->ctx = blk_mq_get_ctx(q);\n\tdata->hctx = blk_mq_map_queue(q, data->cmd_flags, data->ctx);\n\tif (!(data->rq_flags & RQF_SCHED_TAGS))\n\t\tblk_mq_tag_busy(data->hctx);\n\n\tif (data->flags & BLK_MQ_REQ_RESERVED)\n\t\tdata->rq_flags |= RQF_RESV;\n\n\t \n\tif (data->nr_tags > 1) {\n\t\trq = __blk_mq_alloc_requests_batch(data);\n\t\tif (rq) {\n\t\t\tblk_mq_rq_time_init(rq, alloc_time_ns);\n\t\t\treturn rq;\n\t\t}\n\t\tdata->nr_tags = 1;\n\t}\n\n\t \n\ttag = blk_mq_get_tag(data);\n\tif (tag == BLK_MQ_NO_TAG) {\n\t\tif (data->flags & BLK_MQ_REQ_NOWAIT)\n\t\t\treturn NULL;\n\t\t \n\t\tmsleep(3);\n\t\tgoto retry;\n\t}\n\n\trq = blk_mq_rq_ctx_init(data, blk_mq_tags_from_data(data), tag);\n\tblk_mq_rq_time_init(rq, alloc_time_ns);\n\treturn rq;\n}\n\nstatic struct request *blk_mq_rq_cache_fill(struct request_queue *q,\n\t\t\t\t\t    struct blk_plug *plug,\n\t\t\t\t\t    blk_opf_t opf,\n\t\t\t\t\t    blk_mq_req_flags_t flags)\n{\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t\t.flags\t\t= flags,\n\t\t.cmd_flags\t= opf,\n\t\t.nr_tags\t= plug->nr_ios,\n\t\t.cached_rq\t= &plug->cached_rq,\n\t};\n\tstruct request *rq;\n\n\tif (blk_queue_enter(q, flags))\n\t\treturn NULL;\n\n\tplug->nr_ios = 1;\n\n\trq = __blk_mq_alloc_requests(&data);\n\tif (unlikely(!rq))\n\t\tblk_queue_exit(q);\n\treturn rq;\n}\n\nstatic struct request *blk_mq_alloc_cached_request(struct request_queue *q,\n\t\t\t\t\t\t   blk_opf_t opf,\n\t\t\t\t\t\t   blk_mq_req_flags_t flags)\n{\n\tstruct blk_plug *plug = current->plug;\n\tstruct request *rq;\n\n\tif (!plug)\n\t\treturn NULL;\n\n\tif (rq_list_empty(plug->cached_rq)) {\n\t\tif (plug->nr_ios == 1)\n\t\t\treturn NULL;\n\t\trq = blk_mq_rq_cache_fill(q, plug, opf, flags);\n\t\tif (!rq)\n\t\t\treturn NULL;\n\t} else {\n\t\trq = rq_list_peek(&plug->cached_rq);\n\t\tif (!rq || rq->q != q)\n\t\t\treturn NULL;\n\n\t\tif (blk_mq_get_hctx_type(opf) != rq->mq_hctx->type)\n\t\t\treturn NULL;\n\t\tif (op_is_flush(rq->cmd_flags) != op_is_flush(opf))\n\t\t\treturn NULL;\n\n\t\tplug->cached_rq = rq_list_next(rq);\n\t\tblk_mq_rq_time_init(rq, 0);\n\t}\n\n\trq->cmd_flags = opf;\n\tINIT_LIST_HEAD(&rq->queuelist);\n\treturn rq;\n}\n\nstruct request *blk_mq_alloc_request(struct request_queue *q, blk_opf_t opf,\n\t\tblk_mq_req_flags_t flags)\n{\n\tstruct request *rq;\n\n\trq = blk_mq_alloc_cached_request(q, opf, flags);\n\tif (!rq) {\n\t\tstruct blk_mq_alloc_data data = {\n\t\t\t.q\t\t= q,\n\t\t\t.flags\t\t= flags,\n\t\t\t.cmd_flags\t= opf,\n\t\t\t.nr_tags\t= 1,\n\t\t};\n\t\tint ret;\n\n\t\tret = blk_queue_enter(q, flags);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\n\t\trq = __blk_mq_alloc_requests(&data);\n\t\tif (!rq)\n\t\t\tgoto out_queue_exit;\n\t}\n\trq->__data_len = 0;\n\trq->__sector = (sector_t) -1;\n\trq->bio = rq->biotail = NULL;\n\treturn rq;\nout_queue_exit:\n\tblk_queue_exit(q);\n\treturn ERR_PTR(-EWOULDBLOCK);\n}\nEXPORT_SYMBOL(blk_mq_alloc_request);\n\nstruct request *blk_mq_alloc_request_hctx(struct request_queue *q,\n\tblk_opf_t opf, blk_mq_req_flags_t flags, unsigned int hctx_idx)\n{\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t\t.flags\t\t= flags,\n\t\t.cmd_flags\t= opf,\n\t\t.nr_tags\t= 1,\n\t};\n\tu64 alloc_time_ns = 0;\n\tstruct request *rq;\n\tunsigned int cpu;\n\tunsigned int tag;\n\tint ret;\n\n\t \n\tif (blk_queue_rq_alloc_time(q))\n\t\talloc_time_ns = ktime_get_ns();\n\n\t \n\tif (WARN_ON_ONCE(!(flags & BLK_MQ_REQ_NOWAIT)) ||\n\t    WARN_ON_ONCE(!(flags & BLK_MQ_REQ_RESERVED)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (hctx_idx >= q->nr_hw_queues)\n\t\treturn ERR_PTR(-EIO);\n\n\tret = blk_queue_enter(q, flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t \n\tret = -EXDEV;\n\tdata.hctx = xa_load(&q->hctx_table, hctx_idx);\n\tif (!blk_mq_hw_queue_mapped(data.hctx))\n\t\tgoto out_queue_exit;\n\tcpu = cpumask_first_and(data.hctx->cpumask, cpu_online_mask);\n\tif (cpu >= nr_cpu_ids)\n\t\tgoto out_queue_exit;\n\tdata.ctx = __blk_mq_get_ctx(q, cpu);\n\n\tif (q->elevator)\n\t\tdata.rq_flags |= RQF_SCHED_TAGS;\n\telse\n\t\tblk_mq_tag_busy(data.hctx);\n\n\tif (flags & BLK_MQ_REQ_RESERVED)\n\t\tdata.rq_flags |= RQF_RESV;\n\n\tret = -EWOULDBLOCK;\n\ttag = blk_mq_get_tag(&data);\n\tif (tag == BLK_MQ_NO_TAG)\n\t\tgoto out_queue_exit;\n\trq = blk_mq_rq_ctx_init(&data, blk_mq_tags_from_data(&data), tag);\n\tblk_mq_rq_time_init(rq, alloc_time_ns);\n\trq->__data_len = 0;\n\trq->__sector = (sector_t) -1;\n\trq->bio = rq->biotail = NULL;\n\treturn rq;\n\nout_queue_exit:\n\tblk_queue_exit(q);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(blk_mq_alloc_request_hctx);\n\nstatic void blk_mq_finish_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tif (rq->rq_flags & RQF_USE_SCHED) {\n\t\tq->elevator->type->ops.finish_request(rq);\n\t\t \n\t\trq->rq_flags &= ~RQF_USE_SCHED;\n\t}\n}\n\nstatic void __blk_mq_free_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\tconst int sched_tag = rq->internal_tag;\n\n\tblk_crypto_free_request(rq);\n\tblk_pm_mark_last_busy(rq);\n\trq->mq_hctx = NULL;\n\n\tif (rq->rq_flags & RQF_MQ_INFLIGHT)\n\t\t__blk_mq_dec_active_requests(hctx);\n\n\tif (rq->tag != BLK_MQ_NO_TAG)\n\t\tblk_mq_put_tag(hctx->tags, ctx, rq->tag);\n\tif (sched_tag != BLK_MQ_NO_TAG)\n\t\tblk_mq_put_tag(hctx->sched_tags, ctx, sched_tag);\n\tblk_mq_sched_restart(hctx);\n\tblk_queue_exit(q);\n}\n\nvoid blk_mq_free_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tblk_mq_finish_request(rq);\n\n\tif (unlikely(laptop_mode && !blk_rq_is_passthrough(rq)))\n\t\tlaptop_io_completion(q->disk->bdi);\n\n\trq_qos_done(q, rq);\n\n\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\tif (req_ref_put_and_test(rq))\n\t\t__blk_mq_free_request(rq);\n}\nEXPORT_SYMBOL_GPL(blk_mq_free_request);\n\nvoid blk_mq_free_plug_rqs(struct blk_plug *plug)\n{\n\tstruct request *rq;\n\n\twhile ((rq = rq_list_pop(&plug->cached_rq)) != NULL)\n\t\tblk_mq_free_request(rq);\n}\n\nvoid blk_dump_rq_flags(struct request *rq, char *msg)\n{\n\tprintk(KERN_INFO \"%s: dev %s: flags=%llx\\n\", msg,\n\t\trq->q->disk ? rq->q->disk->disk_name : \"?\",\n\t\t(__force unsigned long long) rq->cmd_flags);\n\n\tprintk(KERN_INFO \"  sector %llu, nr/cnr %u/%u\\n\",\n\t       (unsigned long long)blk_rq_pos(rq),\n\t       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));\n\tprintk(KERN_INFO \"  bio %p, biotail %p, len %u\\n\",\n\t       rq->bio, rq->biotail, blk_rq_bytes(rq));\n}\nEXPORT_SYMBOL(blk_dump_rq_flags);\n\nstatic void req_bio_endio(struct request *rq, struct bio *bio,\n\t\t\t  unsigned int nbytes, blk_status_t error)\n{\n\tif (unlikely(error)) {\n\t\tbio->bi_status = error;\n\t} else if (req_op(rq) == REQ_OP_ZONE_APPEND) {\n\t\t \n\t\tif (bio->bi_iter.bi_size != nbytes)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\telse\n\t\t\tbio->bi_iter.bi_sector = rq->__sector;\n\t}\n\n\tbio_advance(bio, nbytes);\n\n\tif (unlikely(rq->rq_flags & RQF_QUIET))\n\t\tbio_set_flag(bio, BIO_QUIET);\n\t \n\tif (bio->bi_iter.bi_size == 0 && !(rq->rq_flags & RQF_FLUSH_SEQ))\n\t\tbio_endio(bio);\n}\n\nstatic void blk_account_io_completion(struct request *req, unsigned int bytes)\n{\n\tif (req->part && blk_do_io_stat(req)) {\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\n\t\tpart_stat_lock();\n\t\tpart_stat_add(req->part, sectors[sgrp], bytes >> 9);\n\t\tpart_stat_unlock();\n\t}\n}\n\nstatic void blk_print_req_error(struct request *req, blk_status_t status)\n{\n\tprintk_ratelimited(KERN_ERR\n\t\t\"%s error, dev %s, sector %llu op 0x%x:(%s) flags 0x%x \"\n\t\t\"phys_seg %u prio class %u\\n\",\n\t\tblk_status_to_str(status),\n\t\treq->q->disk ? req->q->disk->disk_name : \"?\",\n\t\tblk_rq_pos(req), (__force u32)req_op(req),\n\t\tblk_op_str(req_op(req)),\n\t\t(__force u32)(req->cmd_flags & ~REQ_OP_MASK),\n\t\treq->nr_phys_segments,\n\t\tIOPRIO_PRIO_CLASS(req->ioprio));\n}\n\n \nstatic void blk_complete_request(struct request *req)\n{\n\tconst bool is_flush = (req->rq_flags & RQF_FLUSH_SEQ) != 0;\n\tint total_bytes = blk_rq_bytes(req);\n\tstruct bio *bio = req->bio;\n\n\ttrace_block_rq_complete(req, BLK_STS_OK, total_bytes);\n\n\tif (!bio)\n\t\treturn;\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tif (blk_integrity_rq(req) && req_op(req) == REQ_OP_READ)\n\t\treq->q->integrity.profile->complete_fn(req, total_bytes);\n#endif\n\n\t \n\tblk_crypto_rq_put_keyslot(req);\n\n\tblk_account_io_completion(req, total_bytes);\n\n\tdo {\n\t\tstruct bio *next = bio->bi_next;\n\n\t\t \n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\n\t\tif (req_op(req) == REQ_OP_ZONE_APPEND)\n\t\t\tbio->bi_iter.bi_sector = req->__sector;\n\n\t\tif (!is_flush)\n\t\t\tbio_endio(bio);\n\t\tbio = next;\n\t} while (bio);\n\n\t \n\tif (!req->end_io) {\n\t\treq->bio = NULL;\n\t\treq->__data_len = 0;\n\t}\n}\n\n \nbool blk_update_request(struct request *req, blk_status_t error,\n\t\tunsigned int nr_bytes)\n{\n\tint total_bytes;\n\n\ttrace_block_rq_complete(req, error, nr_bytes);\n\n\tif (!req->bio)\n\t\treturn false;\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tif (blk_integrity_rq(req) && req_op(req) == REQ_OP_READ &&\n\t    error == BLK_STS_OK)\n\t\treq->q->integrity.profile->complete_fn(req, nr_bytes);\n#endif\n\n\t \n\tif (blk_crypto_rq_has_keyslot(req) && nr_bytes >= blk_rq_bytes(req))\n\t\t__blk_crypto_rq_put_keyslot(req);\n\n\tif (unlikely(error && !blk_rq_is_passthrough(req) &&\n\t\t     !(req->rq_flags & RQF_QUIET)) &&\n\t\t     !test_bit(GD_DEAD, &req->q->disk->state)) {\n\t\tblk_print_req_error(req, error);\n\t\ttrace_block_rq_error(req, error, nr_bytes);\n\t}\n\n\tblk_account_io_completion(req, nr_bytes);\n\n\ttotal_bytes = 0;\n\twhile (req->bio) {\n\t\tstruct bio *bio = req->bio;\n\t\tunsigned bio_bytes = min(bio->bi_iter.bi_size, nr_bytes);\n\n\t\tif (bio_bytes == bio->bi_iter.bi_size)\n\t\t\treq->bio = bio->bi_next;\n\n\t\t \n\t\tbio_clear_flag(bio, BIO_TRACE_COMPLETION);\n\t\treq_bio_endio(req, bio, bio_bytes, error);\n\n\t\ttotal_bytes += bio_bytes;\n\t\tnr_bytes -= bio_bytes;\n\n\t\tif (!nr_bytes)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (!req->bio) {\n\t\t \n\t\treq->__data_len = 0;\n\t\treturn false;\n\t}\n\n\treq->__data_len -= total_bytes;\n\n\t \n\tif (!blk_rq_is_passthrough(req))\n\t\treq->__sector += total_bytes >> 9;\n\n\t \n\tif (req->rq_flags & RQF_MIXED_MERGE) {\n\t\treq->cmd_flags &= ~REQ_FAILFAST_MASK;\n\t\treq->cmd_flags |= req->bio->bi_opf & REQ_FAILFAST_MASK;\n\t}\n\n\tif (!(req->rq_flags & RQF_SPECIAL_PAYLOAD)) {\n\t\t \n\t\tif (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {\n\t\t\tblk_dump_rq_flags(req, \"request botched\");\n\t\t\treq->__data_len = blk_rq_cur_bytes(req);\n\t\t}\n\n\t\t \n\t\treq->nr_phys_segments = blk_recalc_rq_segments(req);\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_update_request);\n\nstatic inline void blk_account_io_done(struct request *req, u64 now)\n{\n\ttrace_block_io_done(req);\n\n\t \n\tif (blk_do_io_stat(req) && req->part &&\n\t    !(req->rq_flags & RQF_FLUSH_SEQ)) {\n\t\tconst int sgrp = op_stat_group(req_op(req));\n\n\t\tpart_stat_lock();\n\t\tupdate_io_ticks(req->part, jiffies, true);\n\t\tpart_stat_inc(req->part, ios[sgrp]);\n\t\tpart_stat_add(req->part, nsecs[sgrp], now - req->start_time_ns);\n\t\tpart_stat_unlock();\n\t}\n}\n\nstatic inline void blk_account_io_start(struct request *req)\n{\n\ttrace_block_io_start(req);\n\n\tif (blk_do_io_stat(req)) {\n\t\t \n\t\tif (req->bio)\n\t\t\treq->part = req->bio->bi_bdev;\n\t\telse\n\t\t\treq->part = req->q->disk->part0;\n\n\t\tpart_stat_lock();\n\t\tupdate_io_ticks(req->part, jiffies, false);\n\t\tpart_stat_unlock();\n\t}\n}\n\nstatic inline void __blk_mq_end_request_acct(struct request *rq, u64 now)\n{\n\tif (rq->rq_flags & RQF_STATS)\n\t\tblk_stat_add(rq, now);\n\n\tblk_mq_sched_completed_request(rq, now);\n\tblk_account_io_done(rq, now);\n}\n\ninline void __blk_mq_end_request(struct request *rq, blk_status_t error)\n{\n\tif (blk_mq_need_time_stamp(rq))\n\t\t__blk_mq_end_request_acct(rq, ktime_get_ns());\n\n\tblk_mq_finish_request(rq);\n\n\tif (rq->end_io) {\n\t\trq_qos_done(rq->q, rq);\n\t\tif (rq->end_io(rq, error) == RQ_END_IO_FREE)\n\t\t\tblk_mq_free_request(rq);\n\t} else {\n\t\tblk_mq_free_request(rq);\n\t}\n}\nEXPORT_SYMBOL(__blk_mq_end_request);\n\nvoid blk_mq_end_request(struct request *rq, blk_status_t error)\n{\n\tif (blk_update_request(rq, error, blk_rq_bytes(rq)))\n\t\tBUG();\n\t__blk_mq_end_request(rq, error);\n}\nEXPORT_SYMBOL(blk_mq_end_request);\n\n#define TAG_COMP_BATCH\t\t32\n\nstatic inline void blk_mq_flush_tag_batch(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t  int *tag_array, int nr_tags)\n{\n\tstruct request_queue *q = hctx->queue;\n\n\t \n\tif (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\t__blk_mq_sub_active_requests(hctx, nr_tags);\n\n\tblk_mq_put_tags(hctx->tags, tag_array, nr_tags);\n\tpercpu_ref_put_many(&q->q_usage_counter, nr_tags);\n}\n\nvoid blk_mq_end_request_batch(struct io_comp_batch *iob)\n{\n\tint tags[TAG_COMP_BATCH], nr_tags = 0;\n\tstruct blk_mq_hw_ctx *cur_hctx = NULL;\n\tstruct request *rq;\n\tu64 now = 0;\n\n\tif (iob->need_ts)\n\t\tnow = ktime_get_ns();\n\n\twhile ((rq = rq_list_pop(&iob->req_list)) != NULL) {\n\t\tprefetch(rq->bio);\n\t\tprefetch(rq->rq_next);\n\n\t\tblk_complete_request(rq);\n\t\tif (iob->need_ts)\n\t\t\t__blk_mq_end_request_acct(rq, now);\n\n\t\tblk_mq_finish_request(rq);\n\n\t\trq_qos_done(rq->q, rq);\n\n\t\t \n\t\tif (rq->end_io && rq->end_io(rq, 0) == RQ_END_IO_NONE)\n\t\t\tcontinue;\n\n\t\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\t\tif (!req_ref_put_and_test(rq))\n\t\t\tcontinue;\n\n\t\tblk_crypto_free_request(rq);\n\t\tblk_pm_mark_last_busy(rq);\n\n\t\tif (nr_tags == TAG_COMP_BATCH || cur_hctx != rq->mq_hctx) {\n\t\t\tif (cur_hctx)\n\t\t\t\tblk_mq_flush_tag_batch(cur_hctx, tags, nr_tags);\n\t\t\tnr_tags = 0;\n\t\t\tcur_hctx = rq->mq_hctx;\n\t\t}\n\t\ttags[nr_tags++] = rq->tag;\n\t}\n\n\tif (nr_tags)\n\t\tblk_mq_flush_tag_batch(cur_hctx, tags, nr_tags);\n}\nEXPORT_SYMBOL_GPL(blk_mq_end_request_batch);\n\nstatic void blk_complete_reqs(struct llist_head *list)\n{\n\tstruct llist_node *entry = llist_reverse_order(llist_del_all(list));\n\tstruct request *rq, *next;\n\n\tllist_for_each_entry_safe(rq, next, entry, ipi_list)\n\t\trq->q->mq_ops->complete(rq);\n}\n\nstatic __latent_entropy void blk_done_softirq(struct softirq_action *h)\n{\n\tblk_complete_reqs(this_cpu_ptr(&blk_cpu_done));\n}\n\nstatic int blk_softirq_cpu_dead(unsigned int cpu)\n{\n\tblk_complete_reqs(&per_cpu(blk_cpu_done, cpu));\n\treturn 0;\n}\n\nstatic void __blk_mq_complete_request_remote(void *data)\n{\n\t__raise_softirq_irqoff(BLOCK_SOFTIRQ);\n}\n\nstatic inline bool blk_mq_complete_need_ipi(struct request *rq)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (!IS_ENABLED(CONFIG_SMP) ||\n\t    !test_bit(QUEUE_FLAG_SAME_COMP, &rq->q->queue_flags))\n\t\treturn false;\n\t \n\tif (force_irqthreads())\n\t\treturn false;\n\n\t \n\tif (cpu == rq->mq_ctx->cpu ||\n\t    (!test_bit(QUEUE_FLAG_SAME_FORCE, &rq->q->queue_flags) &&\n\t     cpus_share_cache(cpu, rq->mq_ctx->cpu)))\n\t\treturn false;\n\n\t \n\treturn cpu_online(rq->mq_ctx->cpu);\n}\n\nstatic void blk_mq_complete_send_ipi(struct request *rq)\n{\n\tunsigned int cpu;\n\n\tcpu = rq->mq_ctx->cpu;\n\tif (llist_add(&rq->ipi_list, &per_cpu(blk_cpu_done, cpu)))\n\t\tsmp_call_function_single_async(cpu, &per_cpu(blk_cpu_csd, cpu));\n}\n\nstatic void blk_mq_raise_softirq(struct request *rq)\n{\n\tstruct llist_head *list;\n\n\tpreempt_disable();\n\tlist = this_cpu_ptr(&blk_cpu_done);\n\tif (llist_add(&rq->ipi_list, list))\n\t\traise_softirq(BLOCK_SOFTIRQ);\n\tpreempt_enable();\n}\n\nbool blk_mq_complete_request_remote(struct request *rq)\n{\n\tWRITE_ONCE(rq->state, MQ_RQ_COMPLETE);\n\n\t \n\tif ((rq->mq_hctx->nr_ctx == 1 &&\n\t     rq->mq_ctx->cpu == raw_smp_processor_id()) ||\n\t     rq->cmd_flags & REQ_POLLED)\n\t\treturn false;\n\n\tif (blk_mq_complete_need_ipi(rq)) {\n\t\tblk_mq_complete_send_ipi(rq);\n\t\treturn true;\n\t}\n\n\tif (rq->q->nr_hw_queues == 1) {\n\t\tblk_mq_raise_softirq(rq);\n\t\treturn true;\n\t}\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(blk_mq_complete_request_remote);\n\n \nvoid blk_mq_complete_request(struct request *rq)\n{\n\tif (!blk_mq_complete_request_remote(rq))\n\t\trq->q->mq_ops->complete(rq);\n}\nEXPORT_SYMBOL(blk_mq_complete_request);\n\n \nvoid blk_mq_start_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\ttrace_block_rq_issue(rq);\n\n\tif (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {\n\t\trq->io_start_time_ns = ktime_get_ns();\n\t\trq->stats_sectors = blk_rq_sectors(rq);\n\t\trq->rq_flags |= RQF_STATS;\n\t\trq_qos_issue(q, rq);\n\t}\n\n\tWARN_ON_ONCE(blk_mq_rq_state(rq) != MQ_RQ_IDLE);\n\n\tblk_add_timer(rq);\n\tWRITE_ONCE(rq->state, MQ_RQ_IN_FLIGHT);\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tif (blk_integrity_rq(rq) && req_op(rq) == REQ_OP_WRITE)\n\t\tq->integrity.profile->prepare_fn(rq);\n#endif\n\tif (rq->bio && rq->bio->bi_opf & REQ_POLLED)\n\t        WRITE_ONCE(rq->bio->bi_cookie, rq->mq_hctx->queue_num);\n}\nEXPORT_SYMBOL(blk_mq_start_request);\n\n \nstatic inline unsigned short blk_plug_max_rq_count(struct blk_plug *plug)\n{\n\tif (plug->multiple_queues)\n\t\treturn BLK_MAX_REQUEST_COUNT * 2;\n\treturn BLK_MAX_REQUEST_COUNT;\n}\n\nstatic void blk_add_rq_to_plug(struct blk_plug *plug, struct request *rq)\n{\n\tstruct request *last = rq_list_peek(&plug->mq_list);\n\n\tif (!plug->rq_count) {\n\t\ttrace_block_plug(rq->q);\n\t} else if (plug->rq_count >= blk_plug_max_rq_count(plug) ||\n\t\t   (!blk_queue_nomerges(rq->q) &&\n\t\t    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE)) {\n\t\tblk_mq_flush_plug_list(plug, false);\n\t\tlast = NULL;\n\t\ttrace_block_plug(rq->q);\n\t}\n\n\tif (!plug->multiple_queues && last && last->q != rq->q)\n\t\tplug->multiple_queues = true;\n\t \n\tif (!plug->has_elevator && (rq->rq_flags & RQF_SCHED_TAGS))\n\t\tplug->has_elevator = true;\n\trq->rq_next = NULL;\n\trq_list_add(&plug->mq_list, rq);\n\tplug->rq_count++;\n}\n\n \nvoid blk_execute_rq_nowait(struct request *rq, bool at_head)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tWARN_ON(irqs_disabled());\n\tWARN_ON(!blk_rq_is_passthrough(rq));\n\n\tblk_account_io_start(rq);\n\n\t \n\tif (current->plug && !at_head) {\n\t\tblk_add_rq_to_plug(current->plug, rq);\n\t\treturn;\n\t}\n\n\tblk_mq_insert_request(rq, at_head ? BLK_MQ_INSERT_AT_HEAD : 0);\n\tblk_mq_run_hw_queue(hctx, hctx->flags & BLK_MQ_F_BLOCKING);\n}\nEXPORT_SYMBOL_GPL(blk_execute_rq_nowait);\n\nstruct blk_rq_wait {\n\tstruct completion done;\n\tblk_status_t ret;\n};\n\nstatic enum rq_end_io_ret blk_end_sync_rq(struct request *rq, blk_status_t ret)\n{\n\tstruct blk_rq_wait *wait = rq->end_io_data;\n\n\twait->ret = ret;\n\tcomplete(&wait->done);\n\treturn RQ_END_IO_NONE;\n}\n\nbool blk_rq_is_poll(struct request *rq)\n{\n\tif (!rq->mq_hctx)\n\t\treturn false;\n\tif (rq->mq_hctx->type != HCTX_TYPE_POLL)\n\t\treturn false;\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(blk_rq_is_poll);\n\nstatic void blk_rq_poll_completion(struct request *rq, struct completion *wait)\n{\n\tdo {\n\t\tblk_hctx_poll(rq->q, rq->mq_hctx, NULL, 0);\n\t\tcond_resched();\n\t} while (!completion_done(wait));\n}\n\n \nblk_status_t blk_execute_rq(struct request *rq, bool at_head)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\tstruct blk_rq_wait wait = {\n\t\t.done = COMPLETION_INITIALIZER_ONSTACK(wait.done),\n\t};\n\n\tWARN_ON(irqs_disabled());\n\tWARN_ON(!blk_rq_is_passthrough(rq));\n\n\trq->end_io_data = &wait;\n\trq->end_io = blk_end_sync_rq;\n\n\tblk_account_io_start(rq);\n\tblk_mq_insert_request(rq, at_head ? BLK_MQ_INSERT_AT_HEAD : 0);\n\tblk_mq_run_hw_queue(hctx, false);\n\n\tif (blk_rq_is_poll(rq)) {\n\t\tblk_rq_poll_completion(rq, &wait.done);\n\t} else {\n\t\t \n\t\tunsigned long hang_check = sysctl_hung_task_timeout_secs;\n\n\t\tif (hang_check)\n\t\t\twhile (!wait_for_completion_io_timeout(&wait.done,\n\t\t\t\t\thang_check * (HZ/2)))\n\t\t\t\t;\n\t\telse\n\t\t\twait_for_completion_io(&wait.done);\n\t}\n\n\treturn wait.ret;\n}\nEXPORT_SYMBOL(blk_execute_rq);\n\nstatic void __blk_mq_requeue_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\n\tblk_mq_put_driver_tag(rq);\n\n\ttrace_block_rq_requeue(rq);\n\trq_qos_requeue(q, rq);\n\n\tif (blk_mq_request_started(rq)) {\n\t\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\t\trq->rq_flags &= ~RQF_TIMED_OUT;\n\t}\n}\n\nvoid blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long flags;\n\n\t__blk_mq_requeue_request(rq);\n\n\t \n\tblk_mq_sched_requeue_request(rq);\n\n\tspin_lock_irqsave(&q->requeue_lock, flags);\n\tlist_add_tail(&rq->queuelist, &q->requeue_list);\n\tspin_unlock_irqrestore(&q->requeue_lock, flags);\n\n\tif (kick_requeue_list)\n\t\tblk_mq_kick_requeue_list(q);\n}\nEXPORT_SYMBOL(blk_mq_requeue_request);\n\nstatic void blk_mq_requeue_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, requeue_work.work);\n\tLIST_HEAD(rq_list);\n\tLIST_HEAD(flush_list);\n\tstruct request *rq;\n\n\tspin_lock_irq(&q->requeue_lock);\n\tlist_splice_init(&q->requeue_list, &rq_list);\n\tlist_splice_init(&q->flush_list, &flush_list);\n\tspin_unlock_irq(&q->requeue_lock);\n\n\twhile (!list_empty(&rq_list)) {\n\t\trq = list_entry(rq_list.next, struct request, queuelist);\n\t\t \n\t\tif (rq->rq_flags & RQF_DONTPREP) {\n\t\t\tlist_del_init(&rq->queuelist);\n\t\t\tblk_mq_request_bypass_insert(rq, 0);\n\t\t} else {\n\t\t\tlist_del_init(&rq->queuelist);\n\t\t\tblk_mq_insert_request(rq, BLK_MQ_INSERT_AT_HEAD);\n\t\t}\n\t}\n\n\twhile (!list_empty(&flush_list)) {\n\t\trq = list_entry(flush_list.next, struct request, queuelist);\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_insert_request(rq, 0);\n\t}\n\n\tblk_mq_run_hw_queues(q, false);\n}\n\nvoid blk_mq_kick_requeue_list(struct request_queue *q)\n{\n\tkblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work, 0);\n}\nEXPORT_SYMBOL(blk_mq_kick_requeue_list);\n\nvoid blk_mq_delay_kick_requeue_list(struct request_queue *q,\n\t\t\t\t    unsigned long msecs)\n{\n\tkblockd_mod_delayed_work_on(WORK_CPU_UNBOUND, &q->requeue_work,\n\t\t\t\t    msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);\n\nstatic bool blk_is_flush_data_rq(struct request *rq)\n{\n\treturn (rq->rq_flags & RQF_FLUSH_SEQ) && !is_flush_rq(rq);\n}\n\nstatic bool blk_mq_rq_inflight(struct request *rq, void *priv)\n{\n\t \n\tif (blk_mq_request_started(rq) && !(blk_queue_quiesced(rq->q) &&\n\t\t\t\tblk_is_flush_data_rq(rq) &&\n\t\t\t\tblk_mq_request_completed(rq))) {\n\t\tbool *busy = priv;\n\n\t\t*busy = true;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool blk_mq_queue_inflight(struct request_queue *q)\n{\n\tbool busy = false;\n\n\tblk_mq_queue_tag_busy_iter(q, blk_mq_rq_inflight, &busy);\n\treturn busy;\n}\nEXPORT_SYMBOL_GPL(blk_mq_queue_inflight);\n\nstatic void blk_mq_rq_timed_out(struct request *req)\n{\n\treq->rq_flags |= RQF_TIMED_OUT;\n\tif (req->q->mq_ops->timeout) {\n\t\tenum blk_eh_timer_return ret;\n\n\t\tret = req->q->mq_ops->timeout(req);\n\t\tif (ret == BLK_EH_DONE)\n\t\t\treturn;\n\t\tWARN_ON_ONCE(ret != BLK_EH_RESET_TIMER);\n\t}\n\n\tblk_add_timer(req);\n}\n\nstruct blk_expired_data {\n\tbool has_timedout_rq;\n\tunsigned long next;\n\tunsigned long timeout_start;\n};\n\nstatic bool blk_mq_req_expired(struct request *rq, struct blk_expired_data *expired)\n{\n\tunsigned long deadline;\n\n\tif (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)\n\t\treturn false;\n\tif (rq->rq_flags & RQF_TIMED_OUT)\n\t\treturn false;\n\n\tdeadline = READ_ONCE(rq->deadline);\n\tif (time_after_eq(expired->timeout_start, deadline))\n\t\treturn true;\n\n\tif (expired->next == 0)\n\t\texpired->next = deadline;\n\telse if (time_after(expired->next, deadline))\n\t\texpired->next = deadline;\n\treturn false;\n}\n\nvoid blk_mq_put_rq_ref(struct request *rq)\n{\n\tif (is_flush_rq(rq)) {\n\t\tif (rq->end_io(rq, 0) == RQ_END_IO_FREE)\n\t\t\tblk_mq_free_request(rq);\n\t} else if (req_ref_put_and_test(rq)) {\n\t\t__blk_mq_free_request(rq);\n\t}\n}\n\nstatic bool blk_mq_check_expired(struct request *rq, void *priv)\n{\n\tstruct blk_expired_data *expired = priv;\n\n\t \n\tif (blk_mq_req_expired(rq, expired)) {\n\t\texpired->has_timedout_rq = true;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool blk_mq_handle_expired(struct request *rq, void *priv)\n{\n\tstruct blk_expired_data *expired = priv;\n\n\tif (blk_mq_req_expired(rq, expired))\n\t\tblk_mq_rq_timed_out(rq);\n\treturn true;\n}\n\nstatic void blk_mq_timeout_work(struct work_struct *work)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(work, struct request_queue, timeout_work);\n\tstruct blk_expired_data expired = {\n\t\t.timeout_start = jiffies,\n\t};\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\t \n\tif (!percpu_ref_tryget(&q->q_usage_counter))\n\t\treturn;\n\n\t \n\tblk_mq_queue_tag_busy_iter(q, blk_mq_check_expired, &expired);\n\tif (expired.has_timedout_rq) {\n\t\t \n\t\tblk_mq_wait_quiesce_done(q->tag_set);\n\n\t\texpired.next = 0;\n\t\tblk_mq_queue_tag_busy_iter(q, blk_mq_handle_expired, &expired);\n\t}\n\n\tif (expired.next != 0) {\n\t\tmod_timer(&q->timeout, expired.next);\n\t} else {\n\t\t \n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\t \n\t\t\tif (blk_mq_hw_queue_mapped(hctx))\n\t\t\t\tblk_mq_tag_idle(hctx);\n\t\t}\n\t}\n\tblk_queue_exit(q);\n}\n\nstruct flush_busy_ctx_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct list_head *list;\n};\n\nstatic bool flush_busy_ctx(struct sbitmap *sb, unsigned int bitnr, void *data)\n{\n\tstruct flush_busy_ctx_data *flush_data = data;\n\tstruct blk_mq_hw_ctx *hctx = flush_data->hctx;\n\tstruct blk_mq_ctx *ctx = hctx->ctxs[bitnr];\n\tenum hctx_type type = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tlist_splice_tail_init(&ctx->rq_lists[type], flush_data->list);\n\tsbitmap_clear_bit(sb, bitnr);\n\tspin_unlock(&ctx->lock);\n\treturn true;\n}\n\n \nvoid blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list)\n{\n\tstruct flush_busy_ctx_data data = {\n\t\t.hctx = hctx,\n\t\t.list = list,\n\t};\n\n\tsbitmap_for_each_set(&hctx->ctx_map, flush_busy_ctx, &data);\n}\nEXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);\n\nstruct dispatch_rq_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request *rq;\n};\n\nstatic bool dispatch_rq_from_ctx(struct sbitmap *sb, unsigned int bitnr,\n\t\tvoid *data)\n{\n\tstruct dispatch_rq_data *dispatch_data = data;\n\tstruct blk_mq_hw_ctx *hctx = dispatch_data->hctx;\n\tstruct blk_mq_ctx *ctx = hctx->ctxs[bitnr];\n\tenum hctx_type type = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_lists[type])) {\n\t\tdispatch_data->rq = list_entry_rq(ctx->rq_lists[type].next);\n\t\tlist_del_init(&dispatch_data->rq->queuelist);\n\t\tif (list_empty(&ctx->rq_lists[type]))\n\t\t\tsbitmap_clear_bit(sb, bitnr);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\treturn !dispatch_data->rq;\n}\n\nstruct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\tstruct blk_mq_ctx *start)\n{\n\tunsigned off = start ? start->index_hw[hctx->type] : 0;\n\tstruct dispatch_rq_data data = {\n\t\t.hctx = hctx,\n\t\t.rq   = NULL,\n\t};\n\n\t__sbitmap_for_each_set(&hctx->ctx_map, off,\n\t\t\t       dispatch_rq_from_ctx, &data);\n\n\treturn data.rq;\n}\n\nstatic bool __blk_mq_alloc_driver_tag(struct request *rq)\n{\n\tstruct sbitmap_queue *bt = &rq->mq_hctx->tags->bitmap_tags;\n\tunsigned int tag_offset = rq->mq_hctx->tags->nr_reserved_tags;\n\tint tag;\n\n\tblk_mq_tag_busy(rq->mq_hctx);\n\n\tif (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag)) {\n\t\tbt = &rq->mq_hctx->tags->breserved_tags;\n\t\ttag_offset = 0;\n\t} else {\n\t\tif (!hctx_may_queue(rq->mq_hctx, bt))\n\t\t\treturn false;\n\t}\n\n\ttag = __sbitmap_queue_get(bt);\n\tif (tag == BLK_MQ_NO_TAG)\n\t\treturn false;\n\n\trq->tag = tag + tag_offset;\n\treturn true;\n}\n\nbool __blk_mq_get_driver_tag(struct blk_mq_hw_ctx *hctx, struct request *rq)\n{\n\tif (rq->tag == BLK_MQ_NO_TAG && !__blk_mq_alloc_driver_tag(rq))\n\t\treturn false;\n\n\tif ((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&\n\t\t\t!(rq->rq_flags & RQF_MQ_INFLIGHT)) {\n\t\trq->rq_flags |= RQF_MQ_INFLIGHT;\n\t\t__blk_mq_inc_active_requests(hctx);\n\t}\n\thctx->tags->rqs[rq->tag] = rq;\n\treturn true;\n}\n\nstatic int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode,\n\t\t\t\tint flags, void *key)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\n\thctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);\n\n\tspin_lock(&hctx->dispatch_wait_lock);\n\tif (!list_empty(&wait->entry)) {\n\t\tstruct sbitmap_queue *sbq;\n\n\t\tlist_del_init(&wait->entry);\n\t\tsbq = &hctx->tags->bitmap_tags;\n\t\tatomic_dec(&sbq->ws_active);\n\t}\n\tspin_unlock(&hctx->dispatch_wait_lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\treturn 1;\n}\n\n \nstatic bool blk_mq_mark_tag_wait(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t struct request *rq)\n{\n\tstruct sbitmap_queue *sbq;\n\tstruct wait_queue_head *wq;\n\twait_queue_entry_t *wait;\n\tbool ret;\n\n\tif (!(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) &&\n\t    !(blk_mq_is_shared_tags(hctx->flags))) {\n\t\tblk_mq_sched_mark_restart_hctx(hctx);\n\n\t\t \n\t\treturn blk_mq_get_driver_tag(rq);\n\t}\n\n\twait = &hctx->dispatch_wait;\n\tif (!list_empty_careful(&wait->entry))\n\t\treturn false;\n\n\tif (blk_mq_tag_is_reserved(rq->mq_hctx->sched_tags, rq->internal_tag))\n\t\tsbq = &hctx->tags->breserved_tags;\n\telse\n\t\tsbq = &hctx->tags->bitmap_tags;\n\twq = &bt_wait_ptr(sbq, hctx)->wait;\n\n\tspin_lock_irq(&wq->lock);\n\tspin_lock(&hctx->dispatch_wait_lock);\n\tif (!list_empty(&wait->entry)) {\n\t\tspin_unlock(&hctx->dispatch_wait_lock);\n\t\tspin_unlock_irq(&wq->lock);\n\t\treturn false;\n\t}\n\n\tatomic_inc(&sbq->ws_active);\n\twait->flags &= ~WQ_FLAG_EXCLUSIVE;\n\t__add_wait_queue(wq, wait);\n\n\t \n\tret = blk_mq_get_driver_tag(rq);\n\tif (!ret) {\n\t\tspin_unlock(&hctx->dispatch_wait_lock);\n\t\tspin_unlock_irq(&wq->lock);\n\t\treturn false;\n\t}\n\n\t \n\tlist_del_init(&wait->entry);\n\tatomic_dec(&sbq->ws_active);\n\tspin_unlock(&hctx->dispatch_wait_lock);\n\tspin_unlock_irq(&wq->lock);\n\n\treturn true;\n}\n\n#define BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT  8\n#define BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR  4\n \nstatic void blk_mq_update_dispatch_busy(struct blk_mq_hw_ctx *hctx, bool busy)\n{\n\tunsigned int ewma;\n\n\tewma = hctx->dispatch_busy;\n\n\tif (!ewma && !busy)\n\t\treturn;\n\n\tewma *= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT - 1;\n\tif (busy)\n\t\tewma += 1 << BLK_MQ_DISPATCH_BUSY_EWMA_FACTOR;\n\tewma /= BLK_MQ_DISPATCH_BUSY_EWMA_WEIGHT;\n\n\thctx->dispatch_busy = ewma;\n}\n\n#define BLK_MQ_RESOURCE_DELAY\t3\t\t \n\nstatic void blk_mq_handle_dev_resource(struct request *rq,\n\t\t\t\t       struct list_head *list)\n{\n\tlist_add(&rq->queuelist, list);\n\t__blk_mq_requeue_request(rq);\n}\n\nstatic void blk_mq_handle_zone_resource(struct request *rq,\n\t\t\t\t\tstruct list_head *zone_list)\n{\n\t \n\tlist_add(&rq->queuelist, zone_list);\n\t__blk_mq_requeue_request(rq);\n}\n\nenum prep_dispatch {\n\tPREP_DISPATCH_OK,\n\tPREP_DISPATCH_NO_TAG,\n\tPREP_DISPATCH_NO_BUDGET,\n};\n\nstatic enum prep_dispatch blk_mq_prep_dispatch_rq(struct request *rq,\n\t\t\t\t\t\t  bool need_budget)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\tint budget_token = -1;\n\n\tif (need_budget) {\n\t\tbudget_token = blk_mq_get_dispatch_budget(rq->q);\n\t\tif (budget_token < 0) {\n\t\t\tblk_mq_put_driver_tag(rq);\n\t\t\treturn PREP_DISPATCH_NO_BUDGET;\n\t\t}\n\t\tblk_mq_set_rq_budget_token(rq, budget_token);\n\t}\n\n\tif (!blk_mq_get_driver_tag(rq)) {\n\t\t \n\t\tif (!blk_mq_mark_tag_wait(hctx, rq)) {\n\t\t\t \n\t\t\tif (need_budget)\n\t\t\t\tblk_mq_put_dispatch_budget(rq->q, budget_token);\n\t\t\treturn PREP_DISPATCH_NO_TAG;\n\t\t}\n\t}\n\n\treturn PREP_DISPATCH_OK;\n}\n\n \nstatic void blk_mq_release_budgets(struct request_queue *q,\n\t\tstruct list_head *list)\n{\n\tstruct request *rq;\n\n\tlist_for_each_entry(rq, list, queuelist) {\n\t\tint budget_token = blk_mq_get_rq_budget_token(rq);\n\n\t\tif (budget_token >= 0)\n\t\t\tblk_mq_put_dispatch_budget(q, budget_token);\n\t}\n}\n\n \nstatic void blk_mq_commit_rqs(struct blk_mq_hw_ctx *hctx, int queued,\n\t\t\t      bool from_schedule)\n{\n\tif (hctx->queue->mq_ops->commit_rqs && queued) {\n\t\ttrace_block_unplug(hctx->queue, queued, !from_schedule);\n\t\thctx->queue->mq_ops->commit_rqs(hctx);\n\t}\n}\n\n \nbool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *list,\n\t\t\t     unsigned int nr_budgets)\n{\n\tenum prep_dispatch prep;\n\tstruct request_queue *q = hctx->queue;\n\tstruct request *rq;\n\tint queued;\n\tblk_status_t ret = BLK_STS_OK;\n\tLIST_HEAD(zone_list);\n\tbool needs_resource = false;\n\n\tif (list_empty(list))\n\t\treturn false;\n\n\t \n\tqueued = 0;\n\tdo {\n\t\tstruct blk_mq_queue_data bd;\n\n\t\trq = list_first_entry(list, struct request, queuelist);\n\n\t\tWARN_ON_ONCE(hctx != rq->mq_hctx);\n\t\tprep = blk_mq_prep_dispatch_rq(rq, !nr_budgets);\n\t\tif (prep != PREP_DISPATCH_OK)\n\t\t\tbreak;\n\n\t\tlist_del_init(&rq->queuelist);\n\n\t\tbd.rq = rq;\n\t\tbd.last = list_empty(list);\n\n\t\t \n\t\tif (nr_budgets)\n\t\t\tnr_budgets--;\n\t\tret = q->mq_ops->queue_rq(hctx, &bd);\n\t\tswitch (ret) {\n\t\tcase BLK_STS_OK:\n\t\t\tqueued++;\n\t\t\tbreak;\n\t\tcase BLK_STS_RESOURCE:\n\t\t\tneeds_resource = true;\n\t\t\tfallthrough;\n\t\tcase BLK_STS_DEV_RESOURCE:\n\t\t\tblk_mq_handle_dev_resource(rq, list);\n\t\t\tgoto out;\n\t\tcase BLK_STS_ZONE_RESOURCE:\n\t\t\t \n\t\t\tblk_mq_handle_zone_resource(rq, &zone_list);\n\t\t\tneeds_resource = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tblk_mq_end_request(rq, ret);\n\t\t}\n\t} while (!list_empty(list));\nout:\n\tif (!list_empty(&zone_list))\n\t\tlist_splice_tail_init(&zone_list, list);\n\n\t \n\tif (!list_empty(list) || ret != BLK_STS_OK)\n\t\tblk_mq_commit_rqs(hctx, queued, false);\n\n\t \n\tif (!list_empty(list)) {\n\t\tbool needs_restart;\n\t\t \n\t\tbool no_tag = prep == PREP_DISPATCH_NO_TAG &&\n\t\t\t((hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED) ||\n\t\t\tblk_mq_is_shared_tags(hctx->flags));\n\n\t\tif (nr_budgets)\n\t\t\tblk_mq_release_budgets(q, list);\n\n\t\tspin_lock(&hctx->lock);\n\t\tlist_splice_tail_init(list, &hctx->dispatch);\n\t\tspin_unlock(&hctx->lock);\n\n\t\t \n\t\tsmp_mb();\n\n\t\t \n\t\tneeds_restart = blk_mq_sched_needs_restart(hctx);\n\t\tif (prep == PREP_DISPATCH_NO_BUDGET)\n\t\t\tneeds_resource = true;\n\t\tif (!needs_restart ||\n\t\t    (no_tag && list_empty_careful(&hctx->dispatch_wait.entry)))\n\t\t\tblk_mq_run_hw_queue(hctx, true);\n\t\telse if (needs_resource)\n\t\t\tblk_mq_delay_run_hw_queue(hctx, BLK_MQ_RESOURCE_DELAY);\n\n\t\tblk_mq_update_dispatch_busy(hctx, true);\n\t\treturn false;\n\t}\n\n\tblk_mq_update_dispatch_busy(hctx, false);\n\treturn true;\n}\n\nstatic inline int blk_mq_first_mapped_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tint cpu = cpumask_first_and(hctx->cpumask, cpu_online_mask);\n\n\tif (cpu >= nr_cpu_ids)\n\t\tcpu = cpumask_first(hctx->cpumask);\n\treturn cpu;\n}\n\n \nstatic int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)\n{\n\tbool tried = false;\n\tint next_cpu = hctx->next_cpu;\n\n\tif (hctx->queue->nr_hw_queues == 1)\n\t\treturn WORK_CPU_UNBOUND;\n\n\tif (--hctx->next_cpu_batch <= 0) {\nselect_cpu:\n\t\tnext_cpu = cpumask_next_and(next_cpu, hctx->cpumask,\n\t\t\t\tcpu_online_mask);\n\t\tif (next_cpu >= nr_cpu_ids)\n\t\t\tnext_cpu = blk_mq_first_mapped_cpu(hctx);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n\n\t \n\tif (!cpu_online(next_cpu)) {\n\t\tif (!tried) {\n\t\t\ttried = true;\n\t\t\tgoto select_cpu;\n\t\t}\n\n\t\t \n\t\thctx->next_cpu = next_cpu;\n\t\thctx->next_cpu_batch = 1;\n\t\treturn WORK_CPU_UNBOUND;\n\t}\n\n\thctx->next_cpu = next_cpu;\n\treturn next_cpu;\n}\n\n \nvoid blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)\n{\n\tif (unlikely(blk_mq_hctx_stopped(hctx)))\n\t\treturn;\n\tkblockd_mod_delayed_work_on(blk_mq_hctx_next_cpu(hctx), &hctx->run_work,\n\t\t\t\t    msecs_to_jiffies(msecs));\n}\nEXPORT_SYMBOL(blk_mq_delay_run_hw_queue);\n\n \nvoid blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tbool need_run;\n\n\t \n\tWARN_ON_ONCE(!async && in_interrupt());\n\n\tmight_sleep_if(!async && hctx->flags & BLK_MQ_F_BLOCKING);\n\n\t \n\t__blk_mq_run_dispatch_ops(hctx->queue, false,\n\t\tneed_run = !blk_queue_quiesced(hctx->queue) &&\n\t\tblk_mq_hctx_has_pending(hctx));\n\n\tif (!need_run)\n\t\treturn;\n\n\tif (async || !cpumask_test_cpu(raw_smp_processor_id(), hctx->cpumask)) {\n\t\tblk_mq_delay_run_hw_queue(hctx, 0);\n\t\treturn;\n\t}\n\n\tblk_mq_run_dispatch_ops(hctx->queue,\n\t\t\t\tblk_mq_sched_dispatch_requests(hctx));\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queue);\n\n \nstatic struct blk_mq_hw_ctx *blk_mq_get_sq_hctx(struct request_queue *q)\n{\n\tstruct blk_mq_ctx *ctx = blk_mq_get_ctx(q);\n\t \n\tstruct blk_mq_hw_ctx *hctx = ctx->hctxs[HCTX_TYPE_DEFAULT];\n\n\tif (!blk_mq_hctx_stopped(hctx))\n\t\treturn hctx;\n\treturn NULL;\n}\n\n \nvoid blk_mq_run_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx, *sq_hctx;\n\tunsigned long i;\n\n\tsq_hctx = NULL;\n\tif (blk_queue_sq_sched(q))\n\t\tsq_hctx = blk_mq_get_sq_hctx(q);\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_hctx_stopped(hctx))\n\t\t\tcontinue;\n\t\t \n\t\tif (!sq_hctx || sq_hctx == hctx ||\n\t\t    !list_empty_careful(&hctx->dispatch))\n\t\t\tblk_mq_run_hw_queue(hctx, async);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_run_hw_queues);\n\n \nvoid blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs)\n{\n\tstruct blk_mq_hw_ctx *hctx, *sq_hctx;\n\tunsigned long i;\n\n\tsq_hctx = NULL;\n\tif (blk_queue_sq_sched(q))\n\t\tsq_hctx = blk_mq_get_sq_hctx(q);\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (blk_mq_hctx_stopped(hctx))\n\t\t\tcontinue;\n\t\t \n\t\tif (delayed_work_pending(&hctx->run_work))\n\t\t\tcontinue;\n\t\t \n\t\tif (!sq_hctx || sq_hctx == hctx ||\n\t\t    !list_empty_careful(&hctx->dispatch))\n\t\t\tblk_mq_delay_run_hw_queue(hctx, msecs);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_delay_run_hw_queues);\n\n \nvoid blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tcancel_delayed_work(&hctx->run_work);\n\n\tset_bit(BLK_MQ_S_STOPPED, &hctx->state);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queue);\n\n \nvoid blk_mq_stop_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_stop_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_stop_hw_queues);\n\nvoid blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)\n{\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\n\tblk_mq_run_hw_queue(hctx, hctx->flags & BLK_MQ_F_BLOCKING);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queue);\n\nvoid blk_mq_start_hw_queues(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_hw_queue(hctx);\n}\nEXPORT_SYMBOL(blk_mq_start_hw_queues);\n\nvoid blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)\n{\n\tif (!blk_mq_hctx_stopped(hctx))\n\t\treturn;\n\n\tclear_bit(BLK_MQ_S_STOPPED, &hctx->state);\n\tblk_mq_run_hw_queue(hctx, async);\n}\nEXPORT_SYMBOL_GPL(blk_mq_start_stopped_hw_queue);\n\nvoid blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tblk_mq_start_stopped_hw_queue(hctx, async ||\n\t\t\t\t\t(hctx->flags & BLK_MQ_F_BLOCKING));\n}\nEXPORT_SYMBOL(blk_mq_start_stopped_hw_queues);\n\nstatic void blk_mq_run_work_fn(struct work_struct *work)\n{\n\tstruct blk_mq_hw_ctx *hctx =\n\t\tcontainer_of(work, struct blk_mq_hw_ctx, run_work.work);\n\n\tblk_mq_run_dispatch_ops(hctx->queue,\n\t\t\t\tblk_mq_sched_dispatch_requests(hctx));\n}\n\n \nstatic void blk_mq_request_bypass_insert(struct request *rq, blk_insert_t flags)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tspin_lock(&hctx->lock);\n\tif (flags & BLK_MQ_INSERT_AT_HEAD)\n\t\tlist_add(&rq->queuelist, &hctx->dispatch);\n\telse\n\t\tlist_add_tail(&rq->queuelist, &hctx->dispatch);\n\tspin_unlock(&hctx->lock);\n}\n\nstatic void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_ctx *ctx, struct list_head *list,\n\t\tbool run_queue_async)\n{\n\tstruct request *rq;\n\tenum hctx_type type = hctx->type;\n\n\t \n\tif (!hctx->dispatch_busy && !run_queue_async) {\n\t\tblk_mq_run_dispatch_ops(hctx->queue,\n\t\t\tblk_mq_try_issue_list_directly(hctx, list));\n\t\tif (list_empty(list))\n\t\t\tgoto out;\n\t}\n\n\t \n\tlist_for_each_entry(rq, list, queuelist) {\n\t\tBUG_ON(rq->mq_ctx != ctx);\n\t\ttrace_block_rq_insert(rq);\n\t\tif (rq->cmd_flags & REQ_NOWAIT)\n\t\t\trun_queue_async = true;\n\t}\n\n\tspin_lock(&ctx->lock);\n\tlist_splice_tail_init(list, &ctx->rq_lists[type]);\n\tblk_mq_hctx_mark_pending(hctx, ctx);\n\tspin_unlock(&ctx->lock);\nout:\n\tblk_mq_run_hw_queue(hctx, run_queue_async);\n}\n\nstatic void blk_mq_insert_request(struct request *rq, blk_insert_t flags)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (blk_rq_is_passthrough(rq)) {\n\t\t \n\t\tblk_mq_request_bypass_insert(rq, flags);\n\t} else if (req_op(rq) == REQ_OP_FLUSH) {\n\t\t \n\t\tblk_mq_request_bypass_insert(rq, BLK_MQ_INSERT_AT_HEAD);\n\t} else if (q->elevator) {\n\t\tLIST_HEAD(list);\n\n\t\tWARN_ON_ONCE(rq->tag != BLK_MQ_NO_TAG);\n\n\t\tlist_add(&rq->queuelist, &list);\n\t\tq->elevator->type->ops.insert_requests(hctx, &list, flags);\n\t} else {\n\t\ttrace_block_rq_insert(rq);\n\n\t\tspin_lock(&ctx->lock);\n\t\tif (flags & BLK_MQ_INSERT_AT_HEAD)\n\t\t\tlist_add(&rq->queuelist, &ctx->rq_lists[hctx->type]);\n\t\telse\n\t\t\tlist_add_tail(&rq->queuelist,\n\t\t\t\t      &ctx->rq_lists[hctx->type]);\n\t\tblk_mq_hctx_mark_pending(hctx, ctx);\n\t\tspin_unlock(&ctx->lock);\n\t}\n}\n\nstatic void blk_mq_bio_to_request(struct request *rq, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tint err;\n\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\trq->cmd_flags |= REQ_FAILFAST_MASK;\n\n\trq->__sector = bio->bi_iter.bi_sector;\n\tblk_rq_bio_prep(rq, bio, nr_segs);\n\n\t \n\terr = blk_crypto_rq_bio_prep(rq, bio, GFP_NOIO);\n\tWARN_ON_ONCE(err);\n\n\tblk_account_io_start(rq);\n}\n\nstatic blk_status_t __blk_mq_issue_directly(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t    struct request *rq, bool last)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_queue_data bd = {\n\t\t.rq = rq,\n\t\t.last = last,\n\t};\n\tblk_status_t ret;\n\n\t \n\tret = q->mq_ops->queue_rq(hctx, &bd);\n\tswitch (ret) {\n\tcase BLK_STS_OK:\n\t\tblk_mq_update_dispatch_busy(hctx, false);\n\t\tbreak;\n\tcase BLK_STS_RESOURCE:\n\tcase BLK_STS_DEV_RESOURCE:\n\t\tblk_mq_update_dispatch_busy(hctx, true);\n\t\t__blk_mq_requeue_request(rq);\n\t\tbreak;\n\tdefault:\n\t\tblk_mq_update_dispatch_busy(hctx, false);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic bool blk_mq_get_budget_and_tag(struct request *rq)\n{\n\tint budget_token;\n\n\tbudget_token = blk_mq_get_dispatch_budget(rq->q);\n\tif (budget_token < 0)\n\t\treturn false;\n\tblk_mq_set_rq_budget_token(rq, budget_token);\n\tif (!blk_mq_get_driver_tag(rq)) {\n\t\tblk_mq_put_dispatch_budget(rq->q, budget_token);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,\n\t\tstruct request *rq)\n{\n\tblk_status_t ret;\n\n\tif (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(rq->q)) {\n\t\tblk_mq_insert_request(rq, 0);\n\t\treturn;\n\t}\n\n\tif ((rq->rq_flags & RQF_USE_SCHED) || !blk_mq_get_budget_and_tag(rq)) {\n\t\tblk_mq_insert_request(rq, 0);\n\t\tblk_mq_run_hw_queue(hctx, rq->cmd_flags & REQ_NOWAIT);\n\t\treturn;\n\t}\n\n\tret = __blk_mq_issue_directly(hctx, rq, true);\n\tswitch (ret) {\n\tcase BLK_STS_OK:\n\t\tbreak;\n\tcase BLK_STS_RESOURCE:\n\tcase BLK_STS_DEV_RESOURCE:\n\t\tblk_mq_request_bypass_insert(rq, 0);\n\t\tblk_mq_run_hw_queue(hctx, false);\n\t\tbreak;\n\tdefault:\n\t\tblk_mq_end_request(rq, ret);\n\t\tbreak;\n\t}\n}\n\nstatic blk_status_t blk_mq_request_issue_directly(struct request *rq, bool last)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(rq->q)) {\n\t\tblk_mq_insert_request(rq, 0);\n\t\treturn BLK_STS_OK;\n\t}\n\n\tif (!blk_mq_get_budget_and_tag(rq))\n\t\treturn BLK_STS_RESOURCE;\n\treturn __blk_mq_issue_directly(hctx, rq, last);\n}\n\nstatic void blk_mq_plug_issue_direct(struct blk_plug *plug)\n{\n\tstruct blk_mq_hw_ctx *hctx = NULL;\n\tstruct request *rq;\n\tint queued = 0;\n\tblk_status_t ret = BLK_STS_OK;\n\n\twhile ((rq = rq_list_pop(&plug->mq_list))) {\n\t\tbool last = rq_list_empty(plug->mq_list);\n\n\t\tif (hctx != rq->mq_hctx) {\n\t\t\tif (hctx) {\n\t\t\t\tblk_mq_commit_rqs(hctx, queued, false);\n\t\t\t\tqueued = 0;\n\t\t\t}\n\t\t\thctx = rq->mq_hctx;\n\t\t}\n\n\t\tret = blk_mq_request_issue_directly(rq, last);\n\t\tswitch (ret) {\n\t\tcase BLK_STS_OK:\n\t\t\tqueued++;\n\t\t\tbreak;\n\t\tcase BLK_STS_RESOURCE:\n\t\tcase BLK_STS_DEV_RESOURCE:\n\t\t\tblk_mq_request_bypass_insert(rq, 0);\n\t\t\tblk_mq_run_hw_queue(hctx, false);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\tblk_mq_end_request(rq, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (ret != BLK_STS_OK)\n\t\tblk_mq_commit_rqs(hctx, queued, false);\n}\n\nstatic void __blk_mq_flush_plug_list(struct request_queue *q,\n\t\t\t\t     struct blk_plug *plug)\n{\n\tif (blk_queue_quiesced(q))\n\t\treturn;\n\tq->mq_ops->queue_rqs(&plug->mq_list);\n}\n\nstatic void blk_mq_dispatch_plug_list(struct blk_plug *plug, bool from_sched)\n{\n\tstruct blk_mq_hw_ctx *this_hctx = NULL;\n\tstruct blk_mq_ctx *this_ctx = NULL;\n\tstruct request *requeue_list = NULL;\n\tstruct request **requeue_lastp = &requeue_list;\n\tunsigned int depth = 0;\n\tbool is_passthrough = false;\n\tLIST_HEAD(list);\n\n\tdo {\n\t\tstruct request *rq = rq_list_pop(&plug->mq_list);\n\n\t\tif (!this_hctx) {\n\t\t\tthis_hctx = rq->mq_hctx;\n\t\t\tthis_ctx = rq->mq_ctx;\n\t\t\tis_passthrough = blk_rq_is_passthrough(rq);\n\t\t} else if (this_hctx != rq->mq_hctx || this_ctx != rq->mq_ctx ||\n\t\t\t   is_passthrough != blk_rq_is_passthrough(rq)) {\n\t\t\trq_list_add_tail(&requeue_lastp, rq);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_add(&rq->queuelist, &list);\n\t\tdepth++;\n\t} while (!rq_list_empty(plug->mq_list));\n\n\tplug->mq_list = requeue_list;\n\ttrace_block_unplug(this_hctx->queue, depth, !from_sched);\n\n\tpercpu_ref_get(&this_hctx->queue->q_usage_counter);\n\t \n\tif (is_passthrough) {\n\t\tspin_lock(&this_hctx->lock);\n\t\tlist_splice_tail_init(&list, &this_hctx->dispatch);\n\t\tspin_unlock(&this_hctx->lock);\n\t\tblk_mq_run_hw_queue(this_hctx, from_sched);\n\t} else if (this_hctx->queue->elevator) {\n\t\tthis_hctx->queue->elevator->type->ops.insert_requests(this_hctx,\n\t\t\t\t&list, 0);\n\t\tblk_mq_run_hw_queue(this_hctx, from_sched);\n\t} else {\n\t\tblk_mq_insert_requests(this_hctx, this_ctx, &list, from_sched);\n\t}\n\tpercpu_ref_put(&this_hctx->queue->q_usage_counter);\n}\n\nvoid blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)\n{\n\tstruct request *rq;\n\n\t \n\tif (plug->rq_count == 0)\n\t\treturn;\n\tplug->rq_count = 0;\n\n\tif (!plug->multiple_queues && !plug->has_elevator && !from_schedule) {\n\t\tstruct request_queue *q;\n\n\t\trq = rq_list_peek(&plug->mq_list);\n\t\tq = rq->q;\n\n\t\t \n\t\tif (q->mq_ops->queue_rqs &&\n\t\t    !(rq->mq_hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {\n\t\t\tblk_mq_run_dispatch_ops(q,\n\t\t\t\t__blk_mq_flush_plug_list(q, plug));\n\t\t\tif (rq_list_empty(plug->mq_list))\n\t\t\t\treturn;\n\t\t}\n\n\t\tblk_mq_run_dispatch_ops(q,\n\t\t\t\tblk_mq_plug_issue_direct(plug));\n\t\tif (rq_list_empty(plug->mq_list))\n\t\t\treturn;\n\t}\n\n\tdo {\n\t\tblk_mq_dispatch_plug_list(plug, from_schedule);\n\t} while (!rq_list_empty(plug->mq_list));\n}\n\nstatic void blk_mq_try_issue_list_directly(struct blk_mq_hw_ctx *hctx,\n\t\tstruct list_head *list)\n{\n\tint queued = 0;\n\tblk_status_t ret = BLK_STS_OK;\n\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq = list_first_entry(list, struct request,\n\t\t\t\tqueuelist);\n\n\t\tlist_del_init(&rq->queuelist);\n\t\tret = blk_mq_request_issue_directly(rq, list_empty(list));\n\t\tswitch (ret) {\n\t\tcase BLK_STS_OK:\n\t\t\tqueued++;\n\t\t\tbreak;\n\t\tcase BLK_STS_RESOURCE:\n\t\tcase BLK_STS_DEV_RESOURCE:\n\t\t\tblk_mq_request_bypass_insert(rq, 0);\n\t\t\tif (list_empty(list))\n\t\t\t\tblk_mq_run_hw_queue(hctx, false);\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\tblk_mq_end_request(rq, ret);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (ret != BLK_STS_OK)\n\t\tblk_mq_commit_rqs(hctx, queued, false);\n}\n\nstatic bool blk_mq_attempt_bio_merge(struct request_queue *q,\n\t\t\t\t     struct bio *bio, unsigned int nr_segs)\n{\n\tif (!blk_queue_nomerges(q) && bio_mergeable(bio)) {\n\t\tif (blk_attempt_plug_merge(q, bio, nr_segs))\n\t\t\treturn true;\n\t\tif (blk_mq_sched_bio_merge(q, bio, nr_segs))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic struct request *blk_mq_get_new_requests(struct request_queue *q,\n\t\t\t\t\t       struct blk_plug *plug,\n\t\t\t\t\t       struct bio *bio,\n\t\t\t\t\t       unsigned int nsegs)\n{\n\tstruct blk_mq_alloc_data data = {\n\t\t.q\t\t= q,\n\t\t.nr_tags\t= 1,\n\t\t.cmd_flags\t= bio->bi_opf,\n\t};\n\tstruct request *rq;\n\n\tif (blk_mq_attempt_bio_merge(q, bio, nsegs))\n\t\treturn NULL;\n\n\trq_qos_throttle(q, bio);\n\n\tif (plug) {\n\t\tdata.nr_tags = plug->nr_ios;\n\t\tplug->nr_ios = 1;\n\t\tdata.cached_rq = &plug->cached_rq;\n\t}\n\n\trq = __blk_mq_alloc_requests(&data);\n\tif (rq)\n\t\treturn rq;\n\trq_qos_cleanup(q, bio);\n\tif (bio->bi_opf & REQ_NOWAIT)\n\t\tbio_wouldblock_error(bio);\n\treturn NULL;\n}\n\n \nstatic bool blk_mq_can_use_cached_rq(struct request *rq, struct blk_plug *plug,\n\t\tstruct bio *bio)\n{\n\tenum hctx_type type = blk_mq_get_hctx_type(bio->bi_opf);\n\tenum hctx_type hctx_type = rq->mq_hctx->type;\n\n\tWARN_ON_ONCE(rq_list_peek(&plug->cached_rq) != rq);\n\n\tif (type != hctx_type &&\n\t    !(type == HCTX_TYPE_READ && hctx_type == HCTX_TYPE_DEFAULT))\n\t\treturn false;\n\tif (op_is_flush(rq->cmd_flags) != op_is_flush(bio->bi_opf))\n\t\treturn false;\n\n\t \n\tplug->cached_rq = rq_list_next(rq);\n\trq_qos_throttle(rq->q, bio);\n\n\tblk_mq_rq_time_init(rq, 0);\n\trq->cmd_flags = bio->bi_opf;\n\tINIT_LIST_HEAD(&rq->queuelist);\n\treturn true;\n}\n\nstatic void bio_set_ioprio(struct bio *bio)\n{\n\t \n\tif (IOPRIO_PRIO_CLASS(bio->bi_ioprio) == IOPRIO_CLASS_NONE)\n\t\tbio->bi_ioprio = get_current_ioprio();\n\tblkcg_set_ioprio(bio);\n}\n\n \nvoid blk_mq_submit_bio(struct bio *bio)\n{\n\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\tstruct blk_plug *plug = blk_mq_plug(bio);\n\tconst int is_sync = op_is_sync(bio->bi_opf);\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request *rq = NULL;\n\tunsigned int nr_segs = 1;\n\tblk_status_t ret;\n\n\tbio = blk_queue_bounce(bio, q);\n\tbio_set_ioprio(bio);\n\n\tif (plug) {\n\t\trq = rq_list_peek(&plug->cached_rq);\n\t\tif (rq && rq->q != q)\n\t\t\trq = NULL;\n\t}\n\tif (rq) {\n\t\tif (unlikely(bio_may_exceed_limits(bio, &q->limits))) {\n\t\t\tbio = __bio_split_to_limits(bio, &q->limits, &nr_segs);\n\t\t\tif (!bio)\n\t\t\t\treturn;\n\t\t}\n\t\tif (!bio_integrity_prep(bio))\n\t\t\treturn;\n\t\tif (blk_mq_attempt_bio_merge(q, bio, nr_segs))\n\t\t\treturn;\n\t\tif (blk_mq_can_use_cached_rq(rq, plug, bio))\n\t\t\tgoto done;\n\t\tpercpu_ref_get(&q->q_usage_counter);\n\t} else {\n\t\tif (unlikely(bio_queue_enter(bio)))\n\t\t\treturn;\n\t\tif (unlikely(bio_may_exceed_limits(bio, &q->limits))) {\n\t\t\tbio = __bio_split_to_limits(bio, &q->limits, &nr_segs);\n\t\t\tif (!bio)\n\t\t\t\tgoto fail;\n\t\t}\n\t\tif (!bio_integrity_prep(bio))\n\t\t\tgoto fail;\n\t}\n\n\trq = blk_mq_get_new_requests(q, plug, bio, nr_segs);\n\tif (unlikely(!rq)) {\nfail:\n\t\tblk_queue_exit(q);\n\t\treturn;\n\t}\n\ndone:\n\ttrace_block_getrq(bio);\n\n\trq_qos_track(q, rq, bio);\n\n\tblk_mq_bio_to_request(rq, bio, nr_segs);\n\n\tret = blk_crypto_rq_get_keyslot(rq);\n\tif (ret != BLK_STS_OK) {\n\t\tbio->bi_status = ret;\n\t\tbio_endio(bio);\n\t\tblk_mq_free_request(rq);\n\t\treturn;\n\t}\n\n\tif (op_is_flush(bio->bi_opf) && blk_insert_flush(rq))\n\t\treturn;\n\n\tif (plug) {\n\t\tblk_add_rq_to_plug(plug, rq);\n\t\treturn;\n\t}\n\n\thctx = rq->mq_hctx;\n\tif ((rq->rq_flags & RQF_USE_SCHED) ||\n\t    (hctx->dispatch_busy && (q->nr_hw_queues == 1 || !is_sync))) {\n\t\tblk_mq_insert_request(rq, 0);\n\t\tblk_mq_run_hw_queue(hctx, true);\n\t} else {\n\t\tblk_mq_run_dispatch_ops(q, blk_mq_try_issue_directly(hctx, rq));\n\t}\n}\n\n#ifdef CONFIG_BLK_MQ_STACKING\n \nblk_status_t blk_insert_cloned_request(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned int max_sectors = blk_queue_get_max_sectors(q, req_op(rq));\n\tunsigned int max_segments = blk_rq_get_max_segments(rq);\n\tblk_status_t ret;\n\n\tif (blk_rq_sectors(rq) > max_sectors) {\n\t\t \n\t\tif (max_sectors == 0)\n\t\t\treturn BLK_STS_NOTSUPP;\n\n\t\tprintk(KERN_ERR \"%s: over max size limit. (%u > %u)\\n\",\n\t\t\t__func__, blk_rq_sectors(rq), max_sectors);\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\t \n\trq->nr_phys_segments = blk_recalc_rq_segments(rq);\n\tif (rq->nr_phys_segments > max_segments) {\n\t\tprintk(KERN_ERR \"%s: over max segments limit. (%u > %u)\\n\",\n\t\t\t__func__, rq->nr_phys_segments, max_segments);\n\t\treturn BLK_STS_IOERR;\n\t}\n\n\tif (q->disk && should_fail_request(q->disk->part0, blk_rq_bytes(rq)))\n\t\treturn BLK_STS_IOERR;\n\n\tret = blk_crypto_rq_get_keyslot(rq);\n\tif (ret != BLK_STS_OK)\n\t\treturn ret;\n\n\tblk_account_io_start(rq);\n\n\t \n\tblk_mq_run_dispatch_ops(q,\n\t\t\tret = blk_mq_request_issue_directly(rq, true));\n\tif (ret)\n\t\tblk_account_io_done(rq, ktime_get_ns());\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blk_insert_cloned_request);\n\n \nvoid blk_rq_unprep_clone(struct request *rq)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = rq->bio) != NULL) {\n\t\trq->bio = bio->bi_next;\n\n\t\tbio_put(bio);\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_rq_unprep_clone);\n\n \nint blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\t      struct bio_set *bs, gfp_t gfp_mask,\n\t\t      int (*bio_ctr)(struct bio *, struct bio *, void *),\n\t\t      void *data)\n{\n\tstruct bio *bio, *bio_src;\n\n\tif (!bs)\n\t\tbs = &fs_bio_set;\n\n\t__rq_for_each_bio(bio_src, rq_src) {\n\t\tbio = bio_alloc_clone(rq->q->disk->part0, bio_src, gfp_mask,\n\t\t\t\t      bs);\n\t\tif (!bio)\n\t\t\tgoto free_and_out;\n\n\t\tif (bio_ctr && bio_ctr(bio, bio_src, data))\n\t\t\tgoto free_and_out;\n\n\t\tif (rq->bio) {\n\t\t\trq->biotail->bi_next = bio;\n\t\t\trq->biotail = bio;\n\t\t} else {\n\t\t\trq->bio = rq->biotail = bio;\n\t\t}\n\t\tbio = NULL;\n\t}\n\n\t \n\trq->__sector = blk_rq_pos(rq_src);\n\trq->__data_len = blk_rq_bytes(rq_src);\n\tif (rq_src->rq_flags & RQF_SPECIAL_PAYLOAD) {\n\t\trq->rq_flags |= RQF_SPECIAL_PAYLOAD;\n\t\trq->special_vec = rq_src->special_vec;\n\t}\n\trq->nr_phys_segments = rq_src->nr_phys_segments;\n\trq->ioprio = rq_src->ioprio;\n\n\tif (rq->bio && blk_crypto_rq_bio_prep(rq, rq->bio, gfp_mask) < 0)\n\t\tgoto free_and_out;\n\n\treturn 0;\n\nfree_and_out:\n\tif (bio)\n\t\tbio_put(bio);\n\tblk_rq_unprep_clone(rq);\n\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(blk_rq_prep_clone);\n#endif  \n\n \nvoid blk_steal_bios(struct bio_list *list, struct request *rq)\n{\n\tif (rq->bio) {\n\t\tif (list->tail)\n\t\t\tlist->tail->bi_next = rq->bio;\n\t\telse\n\t\t\tlist->head = rq->bio;\n\t\tlist->tail = rq->biotail;\n\n\t\trq->bio = NULL;\n\t\trq->biotail = NULL;\n\t}\n\n\trq->__data_len = 0;\n}\nEXPORT_SYMBOL_GPL(blk_steal_bios);\n\nstatic size_t order_to_size(unsigned int order)\n{\n\treturn (size_t)PAGE_SIZE << order;\n}\n\n \nstatic void blk_mq_clear_rq_mapping(struct blk_mq_tags *drv_tags,\n\t\t\t\t    struct blk_mq_tags *tags)\n{\n\tstruct page *page;\n\tunsigned long flags;\n\n\t \n\tif (!drv_tags || drv_tags == tags)\n\t\treturn;\n\n\tlist_for_each_entry(page, &tags->page_list, lru) {\n\t\tunsigned long start = (unsigned long)page_address(page);\n\t\tunsigned long end = start + order_to_size(page->private);\n\t\tint i;\n\n\t\tfor (i = 0; i < drv_tags->nr_tags; i++) {\n\t\t\tstruct request *rq = drv_tags->rqs[i];\n\t\t\tunsigned long rq_addr = (unsigned long)rq;\n\n\t\t\tif (rq_addr >= start && rq_addr < end) {\n\t\t\t\tWARN_ON_ONCE(req_ref_read(rq) != 0);\n\t\t\t\tcmpxchg(&drv_tags->rqs[i], rq, NULL);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tspin_lock_irqsave(&drv_tags->lock, flags);\n\tspin_unlock_irqrestore(&drv_tags->lock, flags);\n}\n\nvoid blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,\n\t\t     unsigned int hctx_idx)\n{\n\tstruct blk_mq_tags *drv_tags;\n\tstruct page *page;\n\n\tif (list_empty(&tags->page_list))\n\t\treturn;\n\n\tif (blk_mq_is_shared_tags(set->flags))\n\t\tdrv_tags = set->shared_tags;\n\telse\n\t\tdrv_tags = set->tags[hctx_idx];\n\n\tif (tags->static_rqs && set->ops->exit_request) {\n\t\tint i;\n\n\t\tfor (i = 0; i < tags->nr_tags; i++) {\n\t\t\tstruct request *rq = tags->static_rqs[i];\n\n\t\t\tif (!rq)\n\t\t\t\tcontinue;\n\t\t\tset->ops->exit_request(set, rq, hctx_idx);\n\t\t\ttags->static_rqs[i] = NULL;\n\t\t}\n\t}\n\n\tblk_mq_clear_rq_mapping(drv_tags, tags);\n\n\twhile (!list_empty(&tags->page_list)) {\n\t\tpage = list_first_entry(&tags->page_list, struct page, lru);\n\t\tlist_del_init(&page->lru);\n\t\t \n\t\tkmemleak_free(page_address(page));\n\t\t__free_pages(page, page->private);\n\t}\n}\n\nvoid blk_mq_free_rq_map(struct blk_mq_tags *tags)\n{\n\tkfree(tags->rqs);\n\ttags->rqs = NULL;\n\tkfree(tags->static_rqs);\n\ttags->static_rqs = NULL;\n\n\tblk_mq_free_tags(tags);\n}\n\nstatic enum hctx_type hctx_idx_to_type(struct blk_mq_tag_set *set,\n\t\tunsigned int hctx_idx)\n{\n\tint i;\n\n\tfor (i = 0; i < set->nr_maps; i++) {\n\t\tunsigned int start = set->map[i].queue_offset;\n\t\tunsigned int end = start + set->map[i].nr_queues;\n\n\t\tif (hctx_idx >= start && hctx_idx < end)\n\t\t\tbreak;\n\t}\n\n\tif (i >= set->nr_maps)\n\t\ti = HCTX_TYPE_DEFAULT;\n\n\treturn i;\n}\n\nstatic int blk_mq_get_hctx_node(struct blk_mq_tag_set *set,\n\t\tunsigned int hctx_idx)\n{\n\tenum hctx_type type = hctx_idx_to_type(set, hctx_idx);\n\n\treturn blk_mq_hw_queue_to_node(&set->map[type], hctx_idx);\n}\n\nstatic struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,\n\t\t\t\t\t       unsigned int hctx_idx,\n\t\t\t\t\t       unsigned int nr_tags,\n\t\t\t\t\t       unsigned int reserved_tags)\n{\n\tint node = blk_mq_get_hctx_node(set, hctx_idx);\n\tstruct blk_mq_tags *tags;\n\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\n\ttags = blk_mq_init_tags(nr_tags, reserved_tags, node,\n\t\t\t\tBLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));\n\tif (!tags)\n\t\treturn NULL;\n\n\ttags->rqs = kcalloc_node(nr_tags, sizeof(struct request *),\n\t\t\t\t GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t node);\n\tif (!tags->rqs)\n\t\tgoto err_free_tags;\n\n\ttags->static_rqs = kcalloc_node(nr_tags, sizeof(struct request *),\n\t\t\t\t\tGFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,\n\t\t\t\t\tnode);\n\tif (!tags->static_rqs)\n\t\tgoto err_free_rqs;\n\n\treturn tags;\n\nerr_free_rqs:\n\tkfree(tags->rqs);\nerr_free_tags:\n\tblk_mq_free_tags(tags);\n\treturn NULL;\n}\n\nstatic int blk_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t       unsigned int hctx_idx, int node)\n{\n\tint ret;\n\n\tif (set->ops->init_request) {\n\t\tret = set->ops->init_request(set, rq, hctx_idx, node);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tWRITE_ONCE(rq->state, MQ_RQ_IDLE);\n\treturn 0;\n}\n\nstatic int blk_mq_alloc_rqs(struct blk_mq_tag_set *set,\n\t\t\t    struct blk_mq_tags *tags,\n\t\t\t    unsigned int hctx_idx, unsigned int depth)\n{\n\tunsigned int i, j, entries_per_page, max_order = 4;\n\tint node = blk_mq_get_hctx_node(set, hctx_idx);\n\tsize_t rq_size, left;\n\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\n\tINIT_LIST_HEAD(&tags->page_list);\n\n\t \n\trq_size = round_up(sizeof(struct request) + set->cmd_size,\n\t\t\t\tcache_line_size());\n\tleft = rq_size * depth;\n\n\tfor (i = 0; i < depth; ) {\n\t\tint this_order = max_order;\n\t\tstruct page *page;\n\t\tint to_do;\n\t\tvoid *p;\n\n\t\twhile (this_order && left < order_to_size(this_order - 1))\n\t\t\tthis_order--;\n\n\t\tdo {\n\t\t\tpage = alloc_pages_node(node,\n\t\t\t\tGFP_NOIO | __GFP_NOWARN | __GFP_NORETRY | __GFP_ZERO,\n\t\t\t\tthis_order);\n\t\t\tif (page)\n\t\t\t\tbreak;\n\t\t\tif (!this_order--)\n\t\t\t\tbreak;\n\t\t\tif (order_to_size(this_order) < rq_size)\n\t\t\t\tbreak;\n\t\t} while (1);\n\n\t\tif (!page)\n\t\t\tgoto fail;\n\n\t\tpage->private = this_order;\n\t\tlist_add_tail(&page->lru, &tags->page_list);\n\n\t\tp = page_address(page);\n\t\t \n\t\tkmemleak_alloc(p, order_to_size(this_order), 1, GFP_NOIO);\n\t\tentries_per_page = order_to_size(this_order) / rq_size;\n\t\tto_do = min(entries_per_page, depth - i);\n\t\tleft -= to_do * rq_size;\n\t\tfor (j = 0; j < to_do; j++) {\n\t\t\tstruct request *rq = p;\n\n\t\t\ttags->static_rqs[i] = rq;\n\t\t\tif (blk_mq_init_request(set, rq, hctx_idx, node)) {\n\t\t\t\ttags->static_rqs[i] = NULL;\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tp += rq_size;\n\t\t\ti++;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tblk_mq_free_rqs(set, tags, hctx_idx);\n\treturn -ENOMEM;\n}\n\nstruct rq_iter_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tbool has_rq;\n};\n\nstatic bool blk_mq_has_request(struct request *rq, void *data)\n{\n\tstruct rq_iter_data *iter_data = data;\n\n\tif (rq->mq_hctx != iter_data->hctx)\n\t\treturn true;\n\titer_data->has_rq = true;\n\treturn false;\n}\n\nstatic bool blk_mq_hctx_has_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct blk_mq_tags *tags = hctx->sched_tags ?\n\t\t\thctx->sched_tags : hctx->tags;\n\tstruct rq_iter_data data = {\n\t\t.hctx\t= hctx,\n\t};\n\n\tblk_mq_all_tag_iter(tags, blk_mq_has_request, &data);\n\treturn data.has_rq;\n}\n\nstatic inline bool blk_mq_last_cpu_in_hctx(unsigned int cpu,\n\t\tstruct blk_mq_hw_ctx *hctx)\n{\n\tif (cpumask_first_and(hctx->cpumask, cpu_online_mask) != cpu)\n\t\treturn false;\n\tif (cpumask_next_and(cpu, hctx->cpumask, cpu_online_mask) < nr_cpu_ids)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int blk_mq_hctx_notify_offline(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,\n\t\t\tstruct blk_mq_hw_ctx, cpuhp_online);\n\n\tif (!cpumask_test_cpu(cpu, hctx->cpumask) ||\n\t    !blk_mq_last_cpu_in_hctx(cpu, hctx))\n\t\treturn 0;\n\n\t \n\tset_bit(BLK_MQ_S_INACTIVE, &hctx->state);\n\tsmp_mb__after_atomic();\n\n\t \n\tif (percpu_ref_tryget(&hctx->queue->q_usage_counter)) {\n\t\twhile (blk_mq_hctx_has_requests(hctx))\n\t\t\tmsleep(5);\n\t\tpercpu_ref_put(&hctx->queue->q_usage_counter);\n\t}\n\n\treturn 0;\n}\n\nstatic int blk_mq_hctx_notify_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx = hlist_entry_safe(node,\n\t\t\tstruct blk_mq_hw_ctx, cpuhp_online);\n\n\tif (cpumask_test_cpu(cpu, hctx->cpumask))\n\t\tclear_bit(BLK_MQ_S_INACTIVE, &hctx->state);\n\treturn 0;\n}\n\n \nstatic int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tLIST_HEAD(tmp);\n\tenum hctx_type type;\n\n\thctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);\n\tif (!cpumask_test_cpu(cpu, hctx->cpumask))\n\t\treturn 0;\n\n\tctx = __blk_mq_get_ctx(hctx->queue, cpu);\n\ttype = hctx->type;\n\n\tspin_lock(&ctx->lock);\n\tif (!list_empty(&ctx->rq_lists[type])) {\n\t\tlist_splice_init(&ctx->rq_lists[type], &tmp);\n\t\tblk_mq_hctx_clear_pending(hctx, ctx);\n\t}\n\tspin_unlock(&ctx->lock);\n\n\tif (list_empty(&tmp))\n\t\treturn 0;\n\n\tspin_lock(&hctx->lock);\n\tlist_splice_tail_init(&tmp, &hctx->dispatch);\n\tspin_unlock(&hctx->lock);\n\n\tblk_mq_run_hw_queue(hctx, true);\n\treturn 0;\n}\n\nstatic void blk_mq_remove_cpuhp(struct blk_mq_hw_ctx *hctx)\n{\n\tif (!(hctx->flags & BLK_MQ_F_STACKING))\n\t\tcpuhp_state_remove_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,\n\t\t\t\t\t\t    &hctx->cpuhp_online);\n\tcpuhp_state_remove_instance_nocalls(CPUHP_BLK_MQ_DEAD,\n\t\t\t\t\t    &hctx->cpuhp_dead);\n}\n\n \nstatic void blk_mq_clear_flush_rq_mapping(struct blk_mq_tags *tags,\n\t\tunsigned int queue_depth, struct request *flush_rq)\n{\n\tint i;\n\tunsigned long flags;\n\n\t \n\tif (!tags)\n\t\treturn;\n\n\tWARN_ON_ONCE(req_ref_read(flush_rq) != 0);\n\n\tfor (i = 0; i < queue_depth; i++)\n\t\tcmpxchg(&tags->rqs[i], flush_rq, NULL);\n\n\t \n\tspin_lock_irqsave(&tags->lock, flags);\n\tspin_unlock_irqrestore(&tags->lock, flags);\n}\n\n \nstatic void blk_mq_exit_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)\n{\n\tstruct request *flush_rq = hctx->fq->flush_rq;\n\n\tif (blk_mq_hw_queue_mapped(hctx))\n\t\tblk_mq_tag_idle(hctx);\n\n\tif (blk_queue_init_done(q))\n\t\tblk_mq_clear_flush_rq_mapping(set->tags[hctx_idx],\n\t\t\t\tset->queue_depth, flush_rq);\n\tif (set->ops->exit_request)\n\t\tset->ops->exit_request(set, flush_rq, hctx_idx);\n\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n\n\tblk_mq_remove_cpuhp(hctx);\n\n\txa_erase(&q->hctx_table, hctx_idx);\n\n\tspin_lock(&q->unused_hctx_lock);\n\tlist_add(&hctx->hctx_list, &q->unused_hctx_list);\n\tspin_unlock(&q->unused_hctx_lock);\n}\n\nstatic void blk_mq_exit_hw_queues(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set, int nr_queue)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (i == nr_queue)\n\t\t\tbreak;\n\t\tblk_mq_exit_hctx(q, set, hctx, i);\n\t}\n}\n\nstatic int blk_mq_init_hctx(struct request_queue *q,\n\t\tstruct blk_mq_tag_set *set,\n\t\tstruct blk_mq_hw_ctx *hctx, unsigned hctx_idx)\n{\n\thctx->queue_num = hctx_idx;\n\n\tif (!(hctx->flags & BLK_MQ_F_STACKING))\n\t\tcpuhp_state_add_instance_nocalls(CPUHP_AP_BLK_MQ_ONLINE,\n\t\t\t\t&hctx->cpuhp_online);\n\tcpuhp_state_add_instance_nocalls(CPUHP_BLK_MQ_DEAD, &hctx->cpuhp_dead);\n\n\thctx->tags = set->tags[hctx_idx];\n\n\tif (set->ops->init_hctx &&\n\t    set->ops->init_hctx(hctx, set->driver_data, hctx_idx))\n\t\tgoto unregister_cpu_notifier;\n\n\tif (blk_mq_init_request(set, hctx->fq->flush_rq, hctx_idx,\n\t\t\t\thctx->numa_node))\n\t\tgoto exit_hctx;\n\n\tif (xa_insert(&q->hctx_table, hctx_idx, hctx, GFP_KERNEL))\n\t\tgoto exit_flush_rq;\n\n\treturn 0;\n\n exit_flush_rq:\n\tif (set->ops->exit_request)\n\t\tset->ops->exit_request(set, hctx->fq->flush_rq, hctx_idx);\n exit_hctx:\n\tif (set->ops->exit_hctx)\n\t\tset->ops->exit_hctx(hctx, hctx_idx);\n unregister_cpu_notifier:\n\tblk_mq_remove_cpuhp(hctx);\n\treturn -1;\n}\n\nstatic struct blk_mq_hw_ctx *\nblk_mq_alloc_hctx(struct request_queue *q, struct blk_mq_tag_set *set,\n\t\tint node)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tgfp_t gfp = GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY;\n\n\thctx = kzalloc_node(sizeof(struct blk_mq_hw_ctx), gfp, node);\n\tif (!hctx)\n\t\tgoto fail_alloc_hctx;\n\n\tif (!zalloc_cpumask_var_node(&hctx->cpumask, gfp, node))\n\t\tgoto free_hctx;\n\n\tatomic_set(&hctx->nr_active, 0);\n\tif (node == NUMA_NO_NODE)\n\t\tnode = set->numa_node;\n\thctx->numa_node = node;\n\n\tINIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);\n\tspin_lock_init(&hctx->lock);\n\tINIT_LIST_HEAD(&hctx->dispatch);\n\thctx->queue = q;\n\thctx->flags = set->flags & ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\n\tINIT_LIST_HEAD(&hctx->hctx_list);\n\n\t \n\thctx->ctxs = kmalloc_array_node(nr_cpu_ids, sizeof(void *),\n\t\t\tgfp, node);\n\tif (!hctx->ctxs)\n\t\tgoto free_cpumask;\n\n\tif (sbitmap_init_node(&hctx->ctx_map, nr_cpu_ids, ilog2(8),\n\t\t\t\tgfp, node, false, false))\n\t\tgoto free_ctxs;\n\thctx->nr_ctx = 0;\n\n\tspin_lock_init(&hctx->dispatch_wait_lock);\n\tinit_waitqueue_func_entry(&hctx->dispatch_wait, blk_mq_dispatch_wake);\n\tINIT_LIST_HEAD(&hctx->dispatch_wait.entry);\n\n\thctx->fq = blk_alloc_flush_queue(hctx->numa_node, set->cmd_size, gfp);\n\tif (!hctx->fq)\n\t\tgoto free_bitmap;\n\n\tblk_mq_hctx_kobj_init(hctx);\n\n\treturn hctx;\n\n free_bitmap:\n\tsbitmap_free(&hctx->ctx_map);\n free_ctxs:\n\tkfree(hctx->ctxs);\n free_cpumask:\n\tfree_cpumask_var(hctx->cpumask);\n free_hctx:\n\tkfree(hctx);\n fail_alloc_hctx:\n\treturn NULL;\n}\n\nstatic void blk_mq_init_cpu_queues(struct request_queue *q,\n\t\t\t\t   unsigned int nr_hw_queues)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tunsigned int i, j;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct blk_mq_ctx *__ctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tstruct blk_mq_hw_ctx *hctx;\n\t\tint k;\n\n\t\t__ctx->cpu = i;\n\t\tspin_lock_init(&__ctx->lock);\n\t\tfor (k = HCTX_TYPE_DEFAULT; k < HCTX_MAX_TYPES; k++)\n\t\t\tINIT_LIST_HEAD(&__ctx->rq_lists[k]);\n\n\t\t__ctx->queue = q;\n\n\t\t \n\t\tfor (j = 0; j < set->nr_maps; j++) {\n\t\t\thctx = blk_mq_map_queue_type(q, j, i);\n\t\t\tif (nr_hw_queues > 1 && hctx->numa_node == NUMA_NO_NODE)\n\t\t\t\thctx->numa_node = cpu_to_node(i);\n\t\t}\n\t}\n}\n\nstruct blk_mq_tags *blk_mq_alloc_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t\t\t     unsigned int hctx_idx,\n\t\t\t\t\t     unsigned int depth)\n{\n\tstruct blk_mq_tags *tags;\n\tint ret;\n\n\ttags = blk_mq_alloc_rq_map(set, hctx_idx, depth, set->reserved_tags);\n\tif (!tags)\n\t\treturn NULL;\n\n\tret = blk_mq_alloc_rqs(set, tags, hctx_idx, depth);\n\tif (ret) {\n\t\tblk_mq_free_rq_map(tags);\n\t\treturn NULL;\n\t}\n\n\treturn tags;\n}\n\nstatic bool __blk_mq_alloc_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t\t       int hctx_idx)\n{\n\tif (blk_mq_is_shared_tags(set->flags)) {\n\t\tset->tags[hctx_idx] = set->shared_tags;\n\n\t\treturn true;\n\t}\n\n\tset->tags[hctx_idx] = blk_mq_alloc_map_and_rqs(set, hctx_idx,\n\t\t\t\t\t\t       set->queue_depth);\n\n\treturn set->tags[hctx_idx];\n}\n\nvoid blk_mq_free_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t     struct blk_mq_tags *tags,\n\t\t\t     unsigned int hctx_idx)\n{\n\tif (tags) {\n\t\tblk_mq_free_rqs(set, tags, hctx_idx);\n\t\tblk_mq_free_rq_map(tags);\n\t}\n}\n\nstatic void __blk_mq_free_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t\t      unsigned int hctx_idx)\n{\n\tif (!blk_mq_is_shared_tags(set->flags))\n\t\tblk_mq_free_map_and_rqs(set, set->tags[hctx_idx], hctx_idx);\n\n\tset->tags[hctx_idx] = NULL;\n}\n\nstatic void blk_mq_map_swqueue(struct request_queue *q)\n{\n\tunsigned int j, hctx_idx;\n\tunsigned long i;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tcpumask_clear(hctx->cpumask);\n\t\thctx->nr_ctx = 0;\n\t\thctx->dispatch_from = NULL;\n\t}\n\n\t \n\tfor_each_possible_cpu(i) {\n\n\t\tctx = per_cpu_ptr(q->queue_ctx, i);\n\t\tfor (j = 0; j < set->nr_maps; j++) {\n\t\t\tif (!set->map[j].nr_queues) {\n\t\t\t\tctx->hctxs[j] = blk_mq_map_queue_type(q,\n\t\t\t\t\t\tHCTX_TYPE_DEFAULT, i);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\thctx_idx = set->map[j].mq_map[i];\n\t\t\t \n\t\t\tif (!set->tags[hctx_idx] &&\n\t\t\t    !__blk_mq_alloc_map_and_rqs(set, hctx_idx)) {\n\t\t\t\t \n\t\t\t\tset->map[j].mq_map[i] = 0;\n\t\t\t}\n\n\t\t\thctx = blk_mq_map_queue_type(q, j, i);\n\t\t\tctx->hctxs[j] = hctx;\n\t\t\t \n\t\t\tif (cpumask_test_cpu(i, hctx->cpumask))\n\t\t\t\tcontinue;\n\n\t\t\tcpumask_set_cpu(i, hctx->cpumask);\n\t\t\thctx->type = j;\n\t\t\tctx->index_hw[hctx->type] = hctx->nr_ctx;\n\t\t\thctx->ctxs[hctx->nr_ctx++] = ctx;\n\n\t\t\t \n\t\t\tBUG_ON(!hctx->nr_ctx);\n\t\t}\n\n\t\tfor (; j < HCTX_MAX_TYPES; j++)\n\t\t\tctx->hctxs[j] = blk_mq_map_queue_type(q,\n\t\t\t\t\tHCTX_TYPE_DEFAULT, i);\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t \n\t\tif (!hctx->nr_ctx) {\n\t\t\t \n\t\t\tif (i)\n\t\t\t\t__blk_mq_free_map_and_rqs(set, i);\n\n\t\t\thctx->tags = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\thctx->tags = set->tags[i];\n\t\tWARN_ON(!hctx->tags);\n\n\t\t \n\t\tsbitmap_resize(&hctx->ctx_map, hctx->nr_ctx);\n\n\t\t \n\t\thctx->next_cpu = blk_mq_first_mapped_cpu(hctx);\n\t\thctx->next_cpu_batch = BLK_MQ_CPU_WORK_BATCH;\n\t}\n}\n\n \nstatic void queue_set_hctx_shared(struct request_queue *q, bool shared)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (shared) {\n\t\t\thctx->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t} else {\n\t\t\tblk_mq_tag_idle(hctx);\n\t\t\thctx->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t}\n\t}\n}\n\nstatic void blk_mq_update_tag_set_shared(struct blk_mq_tag_set *set,\n\t\t\t\t\t bool shared)\n{\n\tstruct request_queue *q;\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_freeze_queue(q);\n\t\tqueue_set_hctx_shared(q, shared);\n\t\tblk_mq_unfreeze_queue(q);\n\t}\n}\n\nstatic void blk_mq_del_queue_tag_set(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tmutex_lock(&set->tag_list_lock);\n\tlist_del(&q->tag_set_list);\n\tif (list_is_singular(&set->tag_list)) {\n\t\t \n\t\tset->flags &= ~BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t \n\t\tblk_mq_update_tag_set_shared(set, false);\n\t}\n\tmutex_unlock(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&q->tag_set_list);\n}\n\nstatic void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,\n\t\t\t\t     struct request_queue *q)\n{\n\tmutex_lock(&set->tag_list_lock);\n\n\t \n\tif (!list_empty(&set->tag_list) &&\n\t    !(set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {\n\t\tset->flags |= BLK_MQ_F_TAG_QUEUE_SHARED;\n\t\t \n\t\tblk_mq_update_tag_set_shared(set, true);\n\t}\n\tif (set->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\tqueue_set_hctx_shared(q, true);\n\tlist_add_tail(&q->tag_set_list, &set->tag_list);\n\n\tmutex_unlock(&set->tag_list_lock);\n}\n\n \nstatic int blk_mq_alloc_ctxs(struct request_queue *q)\n{\n\tstruct blk_mq_ctxs *ctxs;\n\tint cpu;\n\n\tctxs = kzalloc(sizeof(*ctxs), GFP_KERNEL);\n\tif (!ctxs)\n\t\treturn -ENOMEM;\n\n\tctxs->queue_ctx = alloc_percpu(struct blk_mq_ctx);\n\tif (!ctxs->queue_ctx)\n\t\tgoto fail;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct blk_mq_ctx *ctx = per_cpu_ptr(ctxs->queue_ctx, cpu);\n\t\tctx->ctxs = ctxs;\n\t}\n\n\tq->mq_kobj = &ctxs->kobj;\n\tq->queue_ctx = ctxs->queue_ctx;\n\n\treturn 0;\n fail:\n\tkfree(ctxs);\n\treturn -ENOMEM;\n}\n\n \nvoid blk_mq_release(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx, *next;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tWARN_ON_ONCE(hctx && list_empty(&hctx->hctx_list));\n\n\t \n\tlist_for_each_entry_safe(hctx, next, &q->unused_hctx_list, hctx_list) {\n\t\tlist_del_init(&hctx->hctx_list);\n\t\tkobject_put(&hctx->kobj);\n\t}\n\n\txa_destroy(&q->hctx_table);\n\n\t \n\tblk_mq_sysfs_deinit(q);\n}\n\nstatic struct request_queue *blk_mq_init_queue_data(struct blk_mq_tag_set *set,\n\t\tvoid *queuedata)\n{\n\tstruct request_queue *q;\n\tint ret;\n\n\tq = blk_alloc_queue(set->numa_node);\n\tif (!q)\n\t\treturn ERR_PTR(-ENOMEM);\n\tq->queuedata = queuedata;\n\tret = blk_mq_init_allocated_queue(set, q);\n\tif (ret) {\n\t\tblk_put_queue(q);\n\t\treturn ERR_PTR(ret);\n\t}\n\treturn q;\n}\n\nstruct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)\n{\n\treturn blk_mq_init_queue_data(set, NULL);\n}\nEXPORT_SYMBOL(blk_mq_init_queue);\n\n \nvoid blk_mq_destroy_queue(struct request_queue *q)\n{\n\tWARN_ON_ONCE(!queue_is_mq(q));\n\tWARN_ON_ONCE(blk_queue_registered(q));\n\n\tmight_sleep();\n\n\tblk_queue_flag_set(QUEUE_FLAG_DYING, q);\n\tblk_queue_start_drain(q);\n\tblk_mq_freeze_queue_wait(q);\n\n\tblk_sync_queue(q);\n\tblk_mq_cancel_work_sync(q);\n\tblk_mq_exit_queue(q);\n}\nEXPORT_SYMBOL(blk_mq_destroy_queue);\n\nstruct gendisk *__blk_mq_alloc_disk(struct blk_mq_tag_set *set, void *queuedata,\n\t\tstruct lock_class_key *lkclass)\n{\n\tstruct request_queue *q;\n\tstruct gendisk *disk;\n\n\tq = blk_mq_init_queue_data(set, queuedata);\n\tif (IS_ERR(q))\n\t\treturn ERR_CAST(q);\n\n\tdisk = __alloc_disk_node(q, set->numa_node, lkclass);\n\tif (!disk) {\n\t\tblk_mq_destroy_queue(q);\n\t\tblk_put_queue(q);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tset_bit(GD_OWNS_QUEUE, &disk->state);\n\treturn disk;\n}\nEXPORT_SYMBOL(__blk_mq_alloc_disk);\n\nstruct gendisk *blk_mq_alloc_disk_for_queue(struct request_queue *q,\n\t\tstruct lock_class_key *lkclass)\n{\n\tstruct gendisk *disk;\n\n\tif (!blk_get_queue(q))\n\t\treturn NULL;\n\tdisk = __alloc_disk_node(q, NUMA_NO_NODE, lkclass);\n\tif (!disk)\n\t\tblk_put_queue(q);\n\treturn disk;\n}\nEXPORT_SYMBOL(blk_mq_alloc_disk_for_queue);\n\nstatic struct blk_mq_hw_ctx *blk_mq_alloc_and_init_hctx(\n\t\tstruct blk_mq_tag_set *set, struct request_queue *q,\n\t\tint hctx_idx, int node)\n{\n\tstruct blk_mq_hw_ctx *hctx = NULL, *tmp;\n\n\t \n\tspin_lock(&q->unused_hctx_lock);\n\tlist_for_each_entry(tmp, &q->unused_hctx_list, hctx_list) {\n\t\tif (tmp->numa_node == node) {\n\t\t\thctx = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (hctx)\n\t\tlist_del_init(&hctx->hctx_list);\n\tspin_unlock(&q->unused_hctx_lock);\n\n\tif (!hctx)\n\t\thctx = blk_mq_alloc_hctx(q, set, node);\n\tif (!hctx)\n\t\tgoto fail;\n\n\tif (blk_mq_init_hctx(q, set, hctx, hctx_idx))\n\t\tgoto free_hctx;\n\n\treturn hctx;\n\n free_hctx:\n\tkobject_put(&hctx->kobj);\n fail:\n\treturn NULL;\n}\n\nstatic void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,\n\t\t\t\t\t\tstruct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i, j;\n\n\t \n\tmutex_lock(&q->sysfs_lock);\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tint old_node;\n\t\tint node = blk_mq_get_hctx_node(set, i);\n\t\tstruct blk_mq_hw_ctx *old_hctx = xa_load(&q->hctx_table, i);\n\n\t\tif (old_hctx) {\n\t\t\told_node = old_hctx->numa_node;\n\t\t\tblk_mq_exit_hctx(q, set, old_hctx, i);\n\t\t}\n\n\t\tif (!blk_mq_alloc_and_init_hctx(set, q, i, node)) {\n\t\t\tif (!old_hctx)\n\t\t\t\tbreak;\n\t\t\tpr_warn(\"Allocate new hctx on node %d fails, fallback to previous one on node %d\\n\",\n\t\t\t\t\tnode, old_node);\n\t\t\thctx = blk_mq_alloc_and_init_hctx(set, q, i, old_node);\n\t\t\tWARN_ON_ONCE(!hctx);\n\t\t}\n\t}\n\t \n\tif (i != set->nr_hw_queues) {\n\t\tj = q->nr_hw_queues;\n\t} else {\n\t\tj = i;\n\t\tq->nr_hw_queues = set->nr_hw_queues;\n\t}\n\n\txa_for_each_start(&q->hctx_table, j, hctx, j)\n\t\tblk_mq_exit_hctx(q, set, hctx, j);\n\tmutex_unlock(&q->sysfs_lock);\n}\n\nstatic void blk_mq_update_poll_flag(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\tif (set->nr_maps > HCTX_TYPE_POLL &&\n\t    set->map[HCTX_TYPE_POLL].nr_queues)\n\t\tblk_queue_flag_set(QUEUE_FLAG_POLL, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_POLL, q);\n}\n\nint blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,\n\t\tstruct request_queue *q)\n{\n\t \n\tq->mq_ops = set->ops;\n\n\tif (blk_mq_alloc_ctxs(q))\n\t\tgoto err_exit;\n\n\t \n\tblk_mq_sysfs_init(q);\n\n\tINIT_LIST_HEAD(&q->unused_hctx_list);\n\tspin_lock_init(&q->unused_hctx_lock);\n\n\txa_init(&q->hctx_table);\n\n\tblk_mq_realloc_hw_ctxs(set, q);\n\tif (!q->nr_hw_queues)\n\t\tgoto err_hctxs;\n\n\tINIT_WORK(&q->timeout_work, blk_mq_timeout_work);\n\tblk_queue_rq_timeout(q, set->timeout ? set->timeout : 30 * HZ);\n\n\tq->tag_set = set;\n\n\tq->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;\n\tblk_mq_update_poll_flag(q);\n\n\tINIT_DELAYED_WORK(&q->requeue_work, blk_mq_requeue_work);\n\tINIT_LIST_HEAD(&q->flush_list);\n\tINIT_LIST_HEAD(&q->requeue_list);\n\tspin_lock_init(&q->requeue_lock);\n\n\tq->nr_requests = set->queue_depth;\n\n\tblk_mq_init_cpu_queues(q, set->nr_hw_queues);\n\tblk_mq_add_queue_tag_set(set, q);\n\tblk_mq_map_swqueue(q);\n\treturn 0;\n\nerr_hctxs:\n\tblk_mq_release(q);\nerr_exit:\n\tq->mq_ops = NULL;\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL(blk_mq_init_allocated_queue);\n\n \nvoid blk_mq_exit_queue(struct request_queue *q)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\n\t \n\tblk_mq_exit_hw_queues(q, set, set->nr_hw_queues);\n\t \n\tblk_mq_del_queue_tag_set(q);\n}\n\nstatic int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)\n{\n\tint i;\n\n\tif (blk_mq_is_shared_tags(set->flags)) {\n\t\tset->shared_tags = blk_mq_alloc_map_and_rqs(set,\n\t\t\t\t\t\tBLK_MQ_NO_HCTX_IDX,\n\t\t\t\t\t\tset->queue_depth);\n\t\tif (!set->shared_tags)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < set->nr_hw_queues; i++) {\n\t\tif (!__blk_mq_alloc_map_and_rqs(set, i))\n\t\t\tgoto out_unwind;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n\nout_unwind:\n\twhile (--i >= 0)\n\t\t__blk_mq_free_map_and_rqs(set, i);\n\n\tif (blk_mq_is_shared_tags(set->flags)) {\n\t\tblk_mq_free_map_and_rqs(set, set->shared_tags,\n\t\t\t\t\tBLK_MQ_NO_HCTX_IDX);\n\t}\n\n\treturn -ENOMEM;\n}\n\n \nstatic int blk_mq_alloc_set_map_and_rqs(struct blk_mq_tag_set *set)\n{\n\tunsigned int depth;\n\tint err;\n\n\tdepth = set->queue_depth;\n\tdo {\n\t\terr = __blk_mq_alloc_rq_maps(set);\n\t\tif (!err)\n\t\t\tbreak;\n\n\t\tset->queue_depth >>= 1;\n\t\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t} while (set->queue_depth);\n\n\tif (!set->queue_depth || err) {\n\t\tpr_err(\"blk-mq: failed to allocate request map\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (depth != set->queue_depth)\n\t\tpr_info(\"blk-mq: reduced tag depth (%u -> %u)\\n\",\n\t\t\t\t\t\tdepth, set->queue_depth);\n\n\treturn 0;\n}\n\nstatic void blk_mq_update_queue_map(struct blk_mq_tag_set *set)\n{\n\t \n\tif (set->nr_maps == 1)\n\t\tset->map[HCTX_TYPE_DEFAULT].nr_queues = set->nr_hw_queues;\n\n\tif (set->ops->map_queues && !is_kdump_kernel()) {\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0; i < set->nr_maps; i++)\n\t\t\tblk_mq_clear_mq_map(&set->map[i]);\n\n\t\tset->ops->map_queues(set);\n\t} else {\n\t\tBUG_ON(set->nr_maps > 1);\n\t\tblk_mq_map_queues(&set->map[HCTX_TYPE_DEFAULT]);\n\t}\n}\n\nstatic int blk_mq_realloc_tag_set_tags(struct blk_mq_tag_set *set,\n\t\t\t\t       int new_nr_hw_queues)\n{\n\tstruct blk_mq_tags **new_tags;\n\tint i;\n\n\tif (set->nr_hw_queues >= new_nr_hw_queues)\n\t\tgoto done;\n\n\tnew_tags = kcalloc_node(new_nr_hw_queues, sizeof(struct blk_mq_tags *),\n\t\t\t\tGFP_KERNEL, set->numa_node);\n\tif (!new_tags)\n\t\treturn -ENOMEM;\n\n\tif (set->tags)\n\t\tmemcpy(new_tags, set->tags, set->nr_hw_queues *\n\t\t       sizeof(*set->tags));\n\tkfree(set->tags);\n\tset->tags = new_tags;\n\n\tfor (i = set->nr_hw_queues; i < new_nr_hw_queues; i++) {\n\t\tif (!__blk_mq_alloc_map_and_rqs(set, i)) {\n\t\t\twhile (--i >= set->nr_hw_queues)\n\t\t\t\t__blk_mq_free_map_and_rqs(set, i);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tcond_resched();\n\t}\n\ndone:\n\tset->nr_hw_queues = new_nr_hw_queues;\n\treturn 0;\n}\n\n \nint blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i, ret;\n\n\tBUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);\n\n\tif (!set->nr_hw_queues)\n\t\treturn -EINVAL;\n\tif (!set->queue_depth)\n\t\treturn -EINVAL;\n\tif (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->queue_rq)\n\t\treturn -EINVAL;\n\n\tif (!set->ops->get_budget ^ !set->ops->put_budget)\n\t\treturn -EINVAL;\n\n\tif (set->queue_depth > BLK_MQ_MAX_DEPTH) {\n\t\tpr_info(\"blk-mq: reduced tag depth to %u\\n\",\n\t\t\tBLK_MQ_MAX_DEPTH);\n\t\tset->queue_depth = BLK_MQ_MAX_DEPTH;\n\t}\n\n\tif (!set->nr_maps)\n\t\tset->nr_maps = 1;\n\telse if (set->nr_maps > HCTX_MAX_TYPES)\n\t\treturn -EINVAL;\n\n\t \n\tif (is_kdump_kernel()) {\n\t\tset->nr_hw_queues = 1;\n\t\tset->nr_maps = 1;\n\t\tset->queue_depth = min(64U, set->queue_depth);\n\t}\n\t \n\tif (set->nr_maps == 1 && set->nr_hw_queues > nr_cpu_ids)\n\t\tset->nr_hw_queues = nr_cpu_ids;\n\n\tif (set->flags & BLK_MQ_F_BLOCKING) {\n\t\tset->srcu = kmalloc(sizeof(*set->srcu), GFP_KERNEL);\n\t\tif (!set->srcu)\n\t\t\treturn -ENOMEM;\n\t\tret = init_srcu_struct(set->srcu);\n\t\tif (ret)\n\t\t\tgoto out_free_srcu;\n\t}\n\n\tret = -ENOMEM;\n\tset->tags = kcalloc_node(set->nr_hw_queues,\n\t\t\t\t sizeof(struct blk_mq_tags *), GFP_KERNEL,\n\t\t\t\t set->numa_node);\n\tif (!set->tags)\n\t\tgoto out_cleanup_srcu;\n\n\tfor (i = 0; i < set->nr_maps; i++) {\n\t\tset->map[i].mq_map = kcalloc_node(nr_cpu_ids,\n\t\t\t\t\t\t  sizeof(set->map[i].mq_map[0]),\n\t\t\t\t\t\t  GFP_KERNEL, set->numa_node);\n\t\tif (!set->map[i].mq_map)\n\t\t\tgoto out_free_mq_map;\n\t\tset->map[i].nr_queues = is_kdump_kernel() ? 1 : set->nr_hw_queues;\n\t}\n\n\tblk_mq_update_queue_map(set);\n\n\tret = blk_mq_alloc_set_map_and_rqs(set);\n\tif (ret)\n\t\tgoto out_free_mq_map;\n\n\tmutex_init(&set->tag_list_lock);\n\tINIT_LIST_HEAD(&set->tag_list);\n\n\treturn 0;\n\nout_free_mq_map:\n\tfor (i = 0; i < set->nr_maps; i++) {\n\t\tkfree(set->map[i].mq_map);\n\t\tset->map[i].mq_map = NULL;\n\t}\n\tkfree(set->tags);\n\tset->tags = NULL;\nout_cleanup_srcu:\n\tif (set->flags & BLK_MQ_F_BLOCKING)\n\t\tcleanup_srcu_struct(set->srcu);\nout_free_srcu:\n\tif (set->flags & BLK_MQ_F_BLOCKING)\n\t\tkfree(set->srcu);\n\treturn ret;\n}\nEXPORT_SYMBOL(blk_mq_alloc_tag_set);\n\n \nint blk_mq_alloc_sq_tag_set(struct blk_mq_tag_set *set,\n\t\tconst struct blk_mq_ops *ops, unsigned int queue_depth,\n\t\tunsigned int set_flags)\n{\n\tmemset(set, 0, sizeof(*set));\n\tset->ops = ops;\n\tset->nr_hw_queues = 1;\n\tset->nr_maps = 1;\n\tset->queue_depth = queue_depth;\n\tset->numa_node = NUMA_NO_NODE;\n\tset->flags = set_flags;\n\treturn blk_mq_alloc_tag_set(set);\n}\nEXPORT_SYMBOL_GPL(blk_mq_alloc_sq_tag_set);\n\nvoid blk_mq_free_tag_set(struct blk_mq_tag_set *set)\n{\n\tint i, j;\n\n\tfor (i = 0; i < set->nr_hw_queues; i++)\n\t\t__blk_mq_free_map_and_rqs(set, i);\n\n\tif (blk_mq_is_shared_tags(set->flags)) {\n\t\tblk_mq_free_map_and_rqs(set, set->shared_tags,\n\t\t\t\t\tBLK_MQ_NO_HCTX_IDX);\n\t}\n\n\tfor (j = 0; j < set->nr_maps; j++) {\n\t\tkfree(set->map[j].mq_map);\n\t\tset->map[j].mq_map = NULL;\n\t}\n\n\tkfree(set->tags);\n\tset->tags = NULL;\n\tif (set->flags & BLK_MQ_F_BLOCKING) {\n\t\tcleanup_srcu_struct(set->srcu);\n\t\tkfree(set->srcu);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_free_tag_set);\n\nint blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)\n{\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_hw_ctx *hctx;\n\tint ret;\n\tunsigned long i;\n\n\tif (!set)\n\t\treturn -EINVAL;\n\n\tif (q->nr_requests == nr)\n\t\treturn 0;\n\n\tblk_mq_freeze_queue(q);\n\tblk_mq_quiesce_queue(q);\n\n\tret = 0;\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (!hctx->tags)\n\t\t\tcontinue;\n\t\t \n\t\tif (hctx->sched_tags) {\n\t\t\tret = blk_mq_tag_update_depth(hctx, &hctx->sched_tags,\n\t\t\t\t\t\t      nr, true);\n\t\t} else {\n\t\t\tret = blk_mq_tag_update_depth(hctx, &hctx->tags, nr,\n\t\t\t\t\t\t      false);\n\t\t}\n\t\tif (ret)\n\t\t\tbreak;\n\t\tif (q->elevator && q->elevator->type->ops.depth_updated)\n\t\t\tq->elevator->type->ops.depth_updated(hctx);\n\t}\n\tif (!ret) {\n\t\tq->nr_requests = nr;\n\t\tif (blk_mq_is_shared_tags(set->flags)) {\n\t\t\tif (q->elevator)\n\t\t\t\tblk_mq_tag_update_sched_shared_tags(q);\n\t\t\telse\n\t\t\t\tblk_mq_tag_resize_shared_tags(set, nr);\n\t\t}\n\t}\n\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_unfreeze_queue(q);\n\n\treturn ret;\n}\n\n \nstruct blk_mq_qe_pair {\n\tstruct list_head node;\n\tstruct request_queue *q;\n\tstruct elevator_type *type;\n};\n\n \nstatic bool blk_mq_elv_switch_none(struct list_head *head,\n\t\tstruct request_queue *q)\n{\n\tstruct blk_mq_qe_pair *qe;\n\n\tqe = kmalloc(sizeof(*qe), GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY);\n\tif (!qe)\n\t\treturn false;\n\n\t \n\tmutex_lock(&q->sysfs_lock);\n\n\t \n\tif (!q->elevator) {\n\t\tkfree(qe);\n\t\tgoto unlock;\n\t}\n\n\tINIT_LIST_HEAD(&qe->node);\n\tqe->q = q;\n\tqe->type = q->elevator->type;\n\t \n\t__elevator_get(qe->type);\n\tlist_add(&qe->node, head);\n\televator_disable(q);\nunlock:\n\tmutex_unlock(&q->sysfs_lock);\n\n\treturn true;\n}\n\nstatic struct blk_mq_qe_pair *blk_lookup_qe_pair(struct list_head *head,\n\t\t\t\t\t\tstruct request_queue *q)\n{\n\tstruct blk_mq_qe_pair *qe;\n\n\tlist_for_each_entry(qe, head, node)\n\t\tif (qe->q == q)\n\t\t\treturn qe;\n\n\treturn NULL;\n}\n\nstatic void blk_mq_elv_switch_back(struct list_head *head,\n\t\t\t\t  struct request_queue *q)\n{\n\tstruct blk_mq_qe_pair *qe;\n\tstruct elevator_type *t;\n\n\tqe = blk_lookup_qe_pair(head, q);\n\tif (!qe)\n\t\treturn;\n\tt = qe->type;\n\tlist_del(&qe->node);\n\tkfree(qe);\n\n\tmutex_lock(&q->sysfs_lock);\n\televator_switch(q, t);\n\t \n\televator_put(t);\n\tmutex_unlock(&q->sysfs_lock);\n}\n\nstatic void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,\n\t\t\t\t\t\t\tint nr_hw_queues)\n{\n\tstruct request_queue *q;\n\tLIST_HEAD(head);\n\tint prev_nr_hw_queues = set->nr_hw_queues;\n\tint i;\n\n\tlockdep_assert_held(&set->tag_list_lock);\n\n\tif (set->nr_maps == 1 && nr_hw_queues > nr_cpu_ids)\n\t\tnr_hw_queues = nr_cpu_ids;\n\tif (nr_hw_queues < 1)\n\t\treturn;\n\tif (set->nr_maps == 1 && nr_hw_queues == set->nr_hw_queues)\n\t\treturn;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_freeze_queue(q);\n\t \n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tif (!blk_mq_elv_switch_none(&head, q))\n\t\t\tgoto switch_back;\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_debugfs_unregister_hctxs(q);\n\t\tblk_mq_sysfs_unregister_hctxs(q);\n\t}\n\n\tif (blk_mq_realloc_tag_set_tags(set, nr_hw_queues) < 0)\n\t\tgoto reregister;\n\nfallback:\n\tblk_mq_update_queue_map(set);\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_realloc_hw_ctxs(set, q);\n\t\tblk_mq_update_poll_flag(q);\n\t\tif (q->nr_hw_queues != set->nr_hw_queues) {\n\t\t\tint i = prev_nr_hw_queues;\n\n\t\t\tpr_warn(\"Increasing nr_hw_queues to %d fails, fallback to %d\\n\",\n\t\t\t\t\tnr_hw_queues, prev_nr_hw_queues);\n\t\t\tfor (; i < set->nr_hw_queues; i++)\n\t\t\t\t__blk_mq_free_map_and_rqs(set, i);\n\n\t\t\tset->nr_hw_queues = prev_nr_hw_queues;\n\t\t\tgoto fallback;\n\t\t}\n\t\tblk_mq_map_swqueue(q);\n\t}\n\nreregister:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list) {\n\t\tblk_mq_sysfs_register_hctxs(q);\n\t\tblk_mq_debugfs_register_hctxs(q);\n\t}\n\nswitch_back:\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_elv_switch_back(&head, q);\n\n\tlist_for_each_entry(q, &set->tag_list, tag_set_list)\n\t\tblk_mq_unfreeze_queue(q);\n\n\t \n\tfor (i = set->nr_hw_queues; i < prev_nr_hw_queues; i++)\n\t\t__blk_mq_free_map_and_rqs(set, i);\n}\n\nvoid blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)\n{\n\tmutex_lock(&set->tag_list_lock);\n\t__blk_mq_update_nr_hw_queues(set, nr_hw_queues);\n\tmutex_unlock(&set->tag_list_lock);\n}\nEXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);\n\nstatic int blk_hctx_poll(struct request_queue *q, struct blk_mq_hw_ctx *hctx,\n\t\t\t struct io_comp_batch *iob, unsigned int flags)\n{\n\tlong state = get_current_state();\n\tint ret;\n\n\tdo {\n\t\tret = q->mq_ops->poll(hctx, iob);\n\t\tif (ret > 0) {\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (signal_pending_state(state, current))\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\tif (task_is_running(current))\n\t\t\treturn 1;\n\n\t\tif (ret < 0 || (flags & BLK_POLL_ONESHOT))\n\t\t\tbreak;\n\t\tcpu_relax();\n\t} while (!need_resched());\n\n\t__set_current_state(TASK_RUNNING);\n\treturn 0;\n}\n\nint blk_mq_poll(struct request_queue *q, blk_qc_t cookie,\n\t\tstruct io_comp_batch *iob, unsigned int flags)\n{\n\tstruct blk_mq_hw_ctx *hctx = xa_load(&q->hctx_table, cookie);\n\n\treturn blk_hctx_poll(q, hctx, iob, flags);\n}\n\nint blk_rq_poll(struct request *rq, struct io_comp_batch *iob,\n\t\tunsigned int poll_flags)\n{\n\tstruct request_queue *q = rq->q;\n\tint ret;\n\n\tif (!blk_rq_is_poll(rq))\n\t\treturn 0;\n\tif (!percpu_ref_tryget(&q->q_usage_counter))\n\t\treturn 0;\n\n\tret = blk_hctx_poll(q, rq->mq_hctx, iob, poll_flags);\n\tblk_queue_exit(q);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blk_rq_poll);\n\nunsigned int blk_mq_rq_cpu(struct request *rq)\n{\n\treturn rq->mq_ctx->cpu;\n}\nEXPORT_SYMBOL(blk_mq_rq_cpu);\n\nvoid blk_mq_cancel_work_sync(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tcancel_delayed_work_sync(&q->requeue_work);\n\n\tqueue_for_each_hw_ctx(q, hctx, i)\n\t\tcancel_delayed_work_sync(&hctx->run_work);\n}\n\nstatic int __init blk_mq_init(void)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i)\n\t\tinit_llist_head(&per_cpu(blk_cpu_done, i));\n\tfor_each_possible_cpu(i)\n\t\tINIT_CSD(&per_cpu(blk_cpu_csd, i),\n\t\t\t __blk_mq_complete_request_remote, NULL);\n\topen_softirq(BLOCK_SOFTIRQ, blk_done_softirq);\n\n\tcpuhp_setup_state_nocalls(CPUHP_BLOCK_SOFTIRQ_DEAD,\n\t\t\t\t  \"block/softirq:dead\", NULL,\n\t\t\t\t  blk_softirq_cpu_dead);\n\tcpuhp_setup_state_multi(CPUHP_BLK_MQ_DEAD, \"block/mq:dead\", NULL,\n\t\t\t\tblk_mq_hctx_notify_dead);\n\tcpuhp_setup_state_multi(CPUHP_AP_BLK_MQ_ONLINE, \"block/mq:online\",\n\t\t\t\tblk_mq_hctx_notify_online,\n\t\t\t\tblk_mq_hctx_notify_offline);\n\treturn 0;\n}\nsubsys_initcall(blk_mq_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}