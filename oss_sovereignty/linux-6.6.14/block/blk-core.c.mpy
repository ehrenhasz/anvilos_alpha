{
  "module_name": "blk-core.c",
  "hash_id": "c876a98a622565e01d64ba505b958e044d6ddcdc7cfa57cd4b192d7655611eb6",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-core.c",
  "human_readable_source": "\n \n\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-pm.h>\n#include <linux/blk-integrity.h>\n#include <linux/highmem.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/kernel_stat.h>\n#include <linux/string.h>\n#include <linux/init.h>\n#include <linux/completion.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/writeback.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/fault-inject.h>\n#include <linux/list_sort.h>\n#include <linux/delay.h>\n#include <linux/ratelimit.h>\n#include <linux/pm_runtime.h>\n#include <linux/t10-pi.h>\n#include <linux/debugfs.h>\n#include <linux/bpf.h>\n#include <linux/part_stat.h>\n#include <linux/sched/sysctl.h>\n#include <linux/blk-crypto.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-pm.h\"\n#include \"blk-cgroup.h\"\n#include \"blk-throttle.h\"\n\nstruct dentry *blk_debugfs_root;\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_rq_remap);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_bio_complete);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_split);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_unplug);\nEXPORT_TRACEPOINT_SYMBOL_GPL(block_rq_insert);\n\nstatic DEFINE_IDA(blk_queue_ida);\n\n \nstatic struct kmem_cache *blk_requestq_cachep;\n\n \nstatic struct workqueue_struct *kblockd_workqueue;\n\n \nvoid blk_queue_flag_set(unsigned int flag, struct request_queue *q)\n{\n\tset_bit(flag, &q->queue_flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_set);\n\n \nvoid blk_queue_flag_clear(unsigned int flag, struct request_queue *q)\n{\n\tclear_bit(flag, &q->queue_flags);\n}\nEXPORT_SYMBOL(blk_queue_flag_clear);\n\n \nbool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)\n{\n\treturn test_and_set_bit(flag, &q->queue_flags);\n}\nEXPORT_SYMBOL_GPL(blk_queue_flag_test_and_set);\n\n#define REQ_OP_NAME(name) [REQ_OP_##name] = #name\nstatic const char *const blk_op_name[] = {\n\tREQ_OP_NAME(READ),\n\tREQ_OP_NAME(WRITE),\n\tREQ_OP_NAME(FLUSH),\n\tREQ_OP_NAME(DISCARD),\n\tREQ_OP_NAME(SECURE_ERASE),\n\tREQ_OP_NAME(ZONE_RESET),\n\tREQ_OP_NAME(ZONE_RESET_ALL),\n\tREQ_OP_NAME(ZONE_OPEN),\n\tREQ_OP_NAME(ZONE_CLOSE),\n\tREQ_OP_NAME(ZONE_FINISH),\n\tREQ_OP_NAME(ZONE_APPEND),\n\tREQ_OP_NAME(WRITE_ZEROES),\n\tREQ_OP_NAME(DRV_IN),\n\tREQ_OP_NAME(DRV_OUT),\n};\n#undef REQ_OP_NAME\n\n \ninline const char *blk_op_str(enum req_op op)\n{\n\tconst char *op_str = \"UNKNOWN\";\n\n\tif (op < ARRAY_SIZE(blk_op_name) && blk_op_name[op])\n\t\top_str = blk_op_name[op];\n\n\treturn op_str;\n}\nEXPORT_SYMBOL_GPL(blk_op_str);\n\nstatic const struct {\n\tint\t\terrno;\n\tconst char\t*name;\n} blk_errors[] = {\n\t[BLK_STS_OK]\t\t= { 0,\t\t\"\" },\n\t[BLK_STS_NOTSUPP]\t= { -EOPNOTSUPP, \"operation not supported\" },\n\t[BLK_STS_TIMEOUT]\t= { -ETIMEDOUT,\t\"timeout\" },\n\t[BLK_STS_NOSPC]\t\t= { -ENOSPC,\t\"critical space allocation\" },\n\t[BLK_STS_TRANSPORT]\t= { -ENOLINK,\t\"recoverable transport\" },\n\t[BLK_STS_TARGET]\t= { -EREMOTEIO,\t\"critical target\" },\n\t[BLK_STS_RESV_CONFLICT]\t= { -EBADE,\t\"reservation conflict\" },\n\t[BLK_STS_MEDIUM]\t= { -ENODATA,\t\"critical medium\" },\n\t[BLK_STS_PROTECTION]\t= { -EILSEQ,\t\"protection\" },\n\t[BLK_STS_RESOURCE]\t= { -ENOMEM,\t\"kernel resource\" },\n\t[BLK_STS_DEV_RESOURCE]\t= { -EBUSY,\t\"device resource\" },\n\t[BLK_STS_AGAIN]\t\t= { -EAGAIN,\t\"nonblocking retry\" },\n\t[BLK_STS_OFFLINE]\t= { -ENODEV,\t\"device offline\" },\n\n\t \n\t[BLK_STS_DM_REQUEUE]\t= { -EREMCHG, \"dm internal retry\" },\n\n\t \n\t[BLK_STS_ZONE_OPEN_RESOURCE]\t= { -ETOOMANYREFS, \"open zones exceeded\" },\n\t[BLK_STS_ZONE_ACTIVE_RESOURCE]\t= { -EOVERFLOW, \"active zones exceeded\" },\n\n\t \n\t[BLK_STS_DURATION_LIMIT]\t= { -ETIME, \"duration limit exceeded\" },\n\n\t \n\t[BLK_STS_IOERR]\t\t= { -EIO,\t\"I/O\" },\n};\n\nblk_status_t errno_to_blk_status(int errno)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(blk_errors); i++) {\n\t\tif (blk_errors[i].errno == errno)\n\t\t\treturn (__force blk_status_t)i;\n\t}\n\n\treturn BLK_STS_IOERR;\n}\nEXPORT_SYMBOL_GPL(errno_to_blk_status);\n\nint blk_status_to_errno(blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn -EIO;\n\treturn blk_errors[idx].errno;\n}\nEXPORT_SYMBOL_GPL(blk_status_to_errno);\n\nconst char *blk_status_to_str(blk_status_t status)\n{\n\tint idx = (__force int)status;\n\n\tif (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))\n\t\treturn \"<null>\";\n\treturn blk_errors[idx].name;\n}\nEXPORT_SYMBOL_GPL(blk_status_to_str);\n\n \nvoid blk_sync_queue(struct request_queue *q)\n{\n\tdel_timer_sync(&q->timeout);\n\tcancel_work_sync(&q->timeout_work);\n}\nEXPORT_SYMBOL(blk_sync_queue);\n\n \nvoid blk_set_pm_only(struct request_queue *q)\n{\n\tatomic_inc(&q->pm_only);\n}\nEXPORT_SYMBOL_GPL(blk_set_pm_only);\n\nvoid blk_clear_pm_only(struct request_queue *q)\n{\n\tint pm_only;\n\n\tpm_only = atomic_dec_return(&q->pm_only);\n\tWARN_ON_ONCE(pm_only < 0);\n\tif (pm_only == 0)\n\t\twake_up_all(&q->mq_freeze_wq);\n}\nEXPORT_SYMBOL_GPL(blk_clear_pm_only);\n\nstatic void blk_free_queue_rcu(struct rcu_head *rcu_head)\n{\n\tstruct request_queue *q = container_of(rcu_head,\n\t\t\tstruct request_queue, rcu_head);\n\n\tpercpu_ref_exit(&q->q_usage_counter);\n\tkmem_cache_free(blk_requestq_cachep, q);\n}\n\nstatic void blk_free_queue(struct request_queue *q)\n{\n\tblk_free_queue_stats(q->stats);\n\tif (queue_is_mq(q))\n\t\tblk_mq_release(q);\n\n\tida_free(&blk_queue_ida, q->id);\n\tcall_rcu(&q->rcu_head, blk_free_queue_rcu);\n}\n\n \nvoid blk_put_queue(struct request_queue *q)\n{\n\tif (refcount_dec_and_test(&q->refs))\n\t\tblk_free_queue(q);\n}\nEXPORT_SYMBOL(blk_put_queue);\n\nvoid blk_queue_start_drain(struct request_queue *q)\n{\n\t \n\tblk_freeze_queue_start(q);\n\tif (queue_is_mq(q))\n\t\tblk_mq_wake_waiters(q);\n\t \n\twake_up_all(&q->mq_freeze_wq);\n}\n\n \nint blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)\n{\n\tconst bool pm = flags & BLK_MQ_REQ_PM;\n\n\twhile (!blk_try_enter_queue(q, pm)) {\n\t\tif (flags & BLK_MQ_REQ_NOWAIT)\n\t\t\treturn -EAGAIN;\n\n\t\t \n\t\tsmp_rmb();\n\t\twait_event(q->mq_freeze_wq,\n\t\t\t   (!q->mq_freeze_depth &&\n\t\t\t    blk_pm_resume_queue(pm, q)) ||\n\t\t\t   blk_queue_dying(q));\n\t\tif (blk_queue_dying(q))\n\t\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\nint __bio_queue_enter(struct request_queue *q, struct bio *bio)\n{\n\twhile (!blk_try_enter_queue(q, false)) {\n\t\tstruct gendisk *disk = bio->bi_bdev->bd_disk;\n\n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tif (test_bit(GD_DEAD, &disk->state))\n\t\t\t\tgoto dead;\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn -EAGAIN;\n\t\t}\n\n\t\t \n\t\tsmp_rmb();\n\t\twait_event(q->mq_freeze_wq,\n\t\t\t   (!q->mq_freeze_depth &&\n\t\t\t    blk_pm_resume_queue(false, q)) ||\n\t\t\t   test_bit(GD_DEAD, &disk->state));\n\t\tif (test_bit(GD_DEAD, &disk->state))\n\t\t\tgoto dead;\n\t}\n\n\treturn 0;\ndead:\n\tbio_io_error(bio);\n\treturn -ENODEV;\n}\n\nvoid blk_queue_exit(struct request_queue *q)\n{\n\tpercpu_ref_put(&q->q_usage_counter);\n}\n\nstatic void blk_queue_usage_counter_release(struct percpu_ref *ref)\n{\n\tstruct request_queue *q =\n\t\tcontainer_of(ref, struct request_queue, q_usage_counter);\n\n\twake_up_all(&q->mq_freeze_wq);\n}\n\nstatic void blk_rq_timed_out_timer(struct timer_list *t)\n{\n\tstruct request_queue *q = from_timer(q, t, timeout);\n\n\tkblockd_schedule_work(&q->timeout_work);\n}\n\nstatic void blk_timeout_work(struct work_struct *work)\n{\n}\n\nstruct request_queue *blk_alloc_queue(int node_id)\n{\n\tstruct request_queue *q;\n\n\tq = kmem_cache_alloc_node(blk_requestq_cachep, GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t  node_id);\n\tif (!q)\n\t\treturn NULL;\n\n\tq->last_merge = NULL;\n\n\tq->id = ida_alloc(&blk_queue_ida, GFP_KERNEL);\n\tif (q->id < 0)\n\t\tgoto fail_q;\n\n\tq->stats = blk_alloc_queue_stats();\n\tif (!q->stats)\n\t\tgoto fail_id;\n\n\tq->node = node_id;\n\n\tatomic_set(&q->nr_active_requests_shared_tags, 0);\n\n\ttimer_setup(&q->timeout, blk_rq_timed_out_timer, 0);\n\tINIT_WORK(&q->timeout_work, blk_timeout_work);\n\tINIT_LIST_HEAD(&q->icq_list);\n\n\trefcount_set(&q->refs, 1);\n\tmutex_init(&q->debugfs_mutex);\n\tmutex_init(&q->sysfs_lock);\n\tmutex_init(&q->sysfs_dir_lock);\n\tmutex_init(&q->rq_qos_mutex);\n\tspin_lock_init(&q->queue_lock);\n\n\tinit_waitqueue_head(&q->mq_freeze_wq);\n\tmutex_init(&q->mq_freeze_lock);\n\n\t \n\tif (percpu_ref_init(&q->q_usage_counter,\n\t\t\t\tblk_queue_usage_counter_release,\n\t\t\t\tPERCPU_REF_INIT_ATOMIC, GFP_KERNEL))\n\t\tgoto fail_stats;\n\n\tblk_set_default_limits(&q->limits);\n\tq->nr_requests = BLKDEV_DEFAULT_RQ;\n\n\treturn q;\n\nfail_stats:\n\tblk_free_queue_stats(q->stats);\nfail_id:\n\tida_free(&blk_queue_ida, q->id);\nfail_q:\n\tkmem_cache_free(blk_requestq_cachep, q);\n\treturn NULL;\n}\n\n \nbool blk_get_queue(struct request_queue *q)\n{\n\tif (unlikely(blk_queue_dying(q)))\n\t\treturn false;\n\trefcount_inc(&q->refs);\n\treturn true;\n}\nEXPORT_SYMBOL(blk_get_queue);\n\n#ifdef CONFIG_FAIL_MAKE_REQUEST\n\nstatic DECLARE_FAULT_ATTR(fail_make_request);\n\nstatic int __init setup_fail_make_request(char *str)\n{\n\treturn setup_fault_attr(&fail_make_request, str);\n}\n__setup(\"fail_make_request=\", setup_fail_make_request);\n\nbool should_fail_request(struct block_device *part, unsigned int bytes)\n{\n\treturn part->bd_make_it_fail && should_fail(&fail_make_request, bytes);\n}\n\nstatic int __init fail_make_request_debugfs(void)\n{\n\tstruct dentry *dir = fault_create_debugfs_attr(\"fail_make_request\",\n\t\t\t\t\t\tNULL, &fail_make_request);\n\n\treturn PTR_ERR_OR_ZERO(dir);\n}\n\nlate_initcall(fail_make_request_debugfs);\n#endif  \n\nstatic inline void bio_check_ro(struct bio *bio)\n{\n\tif (op_is_write(bio_op(bio)) && bdev_read_only(bio->bi_bdev)) {\n\t\tif (op_is_flush(bio->bi_opf) && !bio_sectors(bio))\n\t\t\treturn;\n\n\t\tif (bio->bi_bdev->bd_ro_warned)\n\t\t\treturn;\n\n\t\tbio->bi_bdev->bd_ro_warned = true;\n\t\t \n\t\tpr_warn(\"Trying to write to read-only block-device %pg\\n\",\n\t\t\tbio->bi_bdev);\n\t}\n}\n\nstatic noinline int should_fail_bio(struct bio *bio)\n{\n\tif (should_fail_request(bdev_whole(bio->bi_bdev), bio->bi_iter.bi_size))\n\t\treturn -EIO;\n\treturn 0;\n}\nALLOW_ERROR_INJECTION(should_fail_bio, ERRNO);\n\n \nstatic inline int bio_check_eod(struct bio *bio)\n{\n\tsector_t maxsector = bdev_nr_sectors(bio->bi_bdev);\n\tunsigned int nr_sectors = bio_sectors(bio);\n\n\tif (nr_sectors &&\n\t    (nr_sectors > maxsector ||\n\t     bio->bi_iter.bi_sector > maxsector - nr_sectors)) {\n\t\tpr_info_ratelimited(\"%s: attempt to access beyond end of device\\n\"\n\t\t\t\t    \"%pg: rw=%d, sector=%llu, nr_sectors = %u limit=%llu\\n\",\n\t\t\t\t    current->comm, bio->bi_bdev, bio->bi_opf,\n\t\t\t\t    bio->bi_iter.bi_sector, nr_sectors, maxsector);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\n \nstatic int blk_partition_remap(struct bio *bio)\n{\n\tstruct block_device *p = bio->bi_bdev;\n\n\tif (unlikely(should_fail_request(p, bio->bi_iter.bi_size)))\n\t\treturn -EIO;\n\tif (bio_sectors(bio)) {\n\t\tbio->bi_iter.bi_sector += p->bd_start_sect;\n\t\ttrace_block_bio_remap(bio, p->bd_dev,\n\t\t\t\t      bio->bi_iter.bi_sector -\n\t\t\t\t      p->bd_start_sect);\n\t}\n\tbio_set_flag(bio, BIO_REMAPPED);\n\treturn 0;\n}\n\n \nstatic inline blk_status_t blk_check_zone_append(struct request_queue *q,\n\t\t\t\t\t\t struct bio *bio)\n{\n\tint nr_sectors = bio_sectors(bio);\n\n\t \n\tif (!bdev_is_zoned(bio->bi_bdev))\n\t\treturn BLK_STS_NOTSUPP;\n\n\t \n\tif (!bdev_is_zone_start(bio->bi_bdev, bio->bi_iter.bi_sector) ||\n\t    !bio_zone_is_seq(bio))\n\t\treturn BLK_STS_IOERR;\n\n\t \n\tif (nr_sectors > q->limits.chunk_sectors)\n\t\treturn BLK_STS_IOERR;\n\n\t \n\tif (nr_sectors > q->limits.max_zone_append_sectors)\n\t\treturn BLK_STS_IOERR;\n\n\tbio->bi_opf |= REQ_NOMERGE;\n\n\treturn BLK_STS_OK;\n}\n\nstatic void __submit_bio(struct bio *bio)\n{\n\tif (unlikely(!blk_crypto_bio_prep(&bio)))\n\t\treturn;\n\n\tif (!bio->bi_bdev->bd_has_submit_bio) {\n\t\tblk_mq_submit_bio(bio);\n\t} else if (likely(bio_queue_enter(bio) == 0)) {\n\t\tstruct gendisk *disk = bio->bi_bdev->bd_disk;\n\n\t\tdisk->fops->submit_bio(bio);\n\t\tblk_queue_exit(disk->queue);\n\t}\n}\n\n \nstatic void __submit_bio_noacct(struct bio *bio)\n{\n\tstruct bio_list bio_list_on_stack[2];\n\n\tBUG_ON(bio->bi_next);\n\n\tbio_list_init(&bio_list_on_stack[0]);\n\tcurrent->bio_list = bio_list_on_stack;\n\n\tdo {\n\t\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\t\tstruct bio_list lower, same;\n\n\t\t \n\t\tbio_list_on_stack[1] = bio_list_on_stack[0];\n\t\tbio_list_init(&bio_list_on_stack[0]);\n\n\t\t__submit_bio(bio);\n\n\t\t \n\t\tbio_list_init(&lower);\n\t\tbio_list_init(&same);\n\t\twhile ((bio = bio_list_pop(&bio_list_on_stack[0])) != NULL)\n\t\t\tif (q == bdev_get_queue(bio->bi_bdev))\n\t\t\t\tbio_list_add(&same, bio);\n\t\t\telse\n\t\t\t\tbio_list_add(&lower, bio);\n\n\t\t \n\t\tbio_list_merge(&bio_list_on_stack[0], &lower);\n\t\tbio_list_merge(&bio_list_on_stack[0], &same);\n\t\tbio_list_merge(&bio_list_on_stack[0], &bio_list_on_stack[1]);\n\t} while ((bio = bio_list_pop(&bio_list_on_stack[0])));\n\n\tcurrent->bio_list = NULL;\n}\n\nstatic void __submit_bio_noacct_mq(struct bio *bio)\n{\n\tstruct bio_list bio_list[2] = { };\n\n\tcurrent->bio_list = bio_list;\n\n\tdo {\n\t\t__submit_bio(bio);\n\t} while ((bio = bio_list_pop(&bio_list[0])));\n\n\tcurrent->bio_list = NULL;\n}\n\nvoid submit_bio_noacct_nocheck(struct bio *bio)\n{\n\tblk_cgroup_bio_start(bio);\n\tblkcg_bio_issue_init(bio);\n\n\tif (!bio_flagged(bio, BIO_TRACE_COMPLETION)) {\n\t\ttrace_block_bio_queue(bio);\n\t\t \n\t\tbio_set_flag(bio, BIO_TRACE_COMPLETION);\n\t}\n\n\t \n\tif (current->bio_list)\n\t\tbio_list_add(&current->bio_list[0], bio);\n\telse if (!bio->bi_bdev->bd_has_submit_bio)\n\t\t__submit_bio_noacct_mq(bio);\n\telse\n\t\t__submit_bio_noacct(bio);\n}\n\n \nvoid submit_bio_noacct(struct bio *bio)\n{\n\tstruct block_device *bdev = bio->bi_bdev;\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tblk_status_t status = BLK_STS_IOERR;\n\n\tmight_sleep();\n\n\t \n\tif ((bio->bi_opf & REQ_NOWAIT) && !bdev_nowait(bdev))\n\t\tgoto not_supported;\n\n\tif (should_fail_bio(bio))\n\t\tgoto end_io;\n\tbio_check_ro(bio);\n\tif (!bio_flagged(bio, BIO_REMAPPED)) {\n\t\tif (unlikely(bio_check_eod(bio)))\n\t\t\tgoto end_io;\n\t\tif (bdev->bd_partno && unlikely(blk_partition_remap(bio)))\n\t\t\tgoto end_io;\n\t}\n\n\t \n\tif (op_is_flush(bio->bi_opf)) {\n\t\tif (WARN_ON_ONCE(bio_op(bio) != REQ_OP_WRITE &&\n\t\t\t\t bio_op(bio) != REQ_OP_ZONE_APPEND))\n\t\t\tgoto end_io;\n\t\tif (!test_bit(QUEUE_FLAG_WC, &q->queue_flags)) {\n\t\t\tbio->bi_opf &= ~(REQ_PREFLUSH | REQ_FUA);\n\t\t\tif (!bio_sectors(bio)) {\n\t\t\t\tstatus = BLK_STS_OK;\n\t\t\t\tgoto end_io;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))\n\t\tbio_clear_polled(bio);\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\t\tif (!bdev_max_discard_sectors(bdev))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\tif (!bdev_max_secure_erase_sectors(bdev))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_ZONE_APPEND:\n\t\tstatus = blk_check_zone_append(q, bio);\n\t\tif (status != BLK_STS_OK)\n\t\t\tgoto end_io;\n\t\tbreak;\n\tcase REQ_OP_ZONE_RESET:\n\tcase REQ_OP_ZONE_OPEN:\n\tcase REQ_OP_ZONE_CLOSE:\n\tcase REQ_OP_ZONE_FINISH:\n\t\tif (!bdev_is_zoned(bio->bi_bdev))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_ZONE_RESET_ALL:\n\t\tif (!bdev_is_zoned(bio->bi_bdev) || !blk_queue_zone_resetall(q))\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tif (!q->limits.max_write_zeroes_sectors)\n\t\t\tgoto not_supported;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (blk_throtl_bio(bio))\n\t\treturn;\n\tsubmit_bio_noacct_nocheck(bio);\n\treturn;\n\nnot_supported:\n\tstatus = BLK_STS_NOTSUPP;\nend_io:\n\tbio->bi_status = status;\n\tbio_endio(bio);\n}\nEXPORT_SYMBOL(submit_bio_noacct);\n\n \nvoid submit_bio(struct bio *bio)\n{\n\tif (bio_op(bio) == REQ_OP_READ) {\n\t\ttask_io_account_read(bio->bi_iter.bi_size);\n\t\tcount_vm_events(PGPGIN, bio_sectors(bio));\n\t} else if (bio_op(bio) == REQ_OP_WRITE) {\n\t\tcount_vm_events(PGPGOUT, bio_sectors(bio));\n\t}\n\n\tsubmit_bio_noacct(bio);\n}\nEXPORT_SYMBOL(submit_bio);\n\n \nint bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags)\n{\n\tblk_qc_t cookie = READ_ONCE(bio->bi_cookie);\n\tstruct block_device *bdev;\n\tstruct request_queue *q;\n\tint ret = 0;\n\n\tbdev = READ_ONCE(bio->bi_bdev);\n\tif (!bdev)\n\t\treturn 0;\n\n\tq = bdev_get_queue(bdev);\n\tif (cookie == BLK_QC_T_NONE ||\n\t    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))\n\t\treturn 0;\n\n\t \n\tblk_flush_plug(current->plug, false);\n\n\t \n\tif (!percpu_ref_tryget(&q->q_usage_counter))\n\t\treturn 0;\n\tif (queue_is_mq(q)) {\n\t\tret = blk_mq_poll(q, cookie, iob, flags);\n\t} else {\n\t\tstruct gendisk *disk = q->disk;\n\n\t\tif (disk && disk->fops->poll_bio)\n\t\t\tret = disk->fops->poll_bio(bio, iob, flags);\n\t}\n\tblk_queue_exit(q);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(bio_poll);\n\n \nint iocb_bio_iopoll(struct kiocb *kiocb, struct io_comp_batch *iob,\n\t\t    unsigned int flags)\n{\n\tstruct bio *bio;\n\tint ret = 0;\n\n\t \n\trcu_read_lock();\n\tbio = READ_ONCE(kiocb->private);\n\tif (bio)\n\t\tret = bio_poll(bio, iob, flags);\n\trcu_read_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iocb_bio_iopoll);\n\nvoid update_io_ticks(struct block_device *part, unsigned long now, bool end)\n{\n\tunsigned long stamp;\nagain:\n\tstamp = READ_ONCE(part->bd_stamp);\n\tif (unlikely(time_after(now, stamp))) {\n\t\tif (likely(try_cmpxchg(&part->bd_stamp, &stamp, now)))\n\t\t\t__part_stat_add(part, io_ticks, end ? now - stamp : 1);\n\t}\n\tif (part->bd_partno) {\n\t\tpart = bdev_whole(part);\n\t\tgoto again;\n\t}\n}\n\nunsigned long bdev_start_io_acct(struct block_device *bdev, enum req_op op,\n\t\t\t\t unsigned long start_time)\n{\n\tpart_stat_lock();\n\tupdate_io_ticks(bdev, start_time, false);\n\tpart_stat_local_inc(bdev, in_flight[op_is_write(op)]);\n\tpart_stat_unlock();\n\n\treturn start_time;\n}\nEXPORT_SYMBOL(bdev_start_io_acct);\n\n \nunsigned long bio_start_io_acct(struct bio *bio)\n{\n\treturn bdev_start_io_acct(bio->bi_bdev, bio_op(bio), jiffies);\n}\nEXPORT_SYMBOL_GPL(bio_start_io_acct);\n\nvoid bdev_end_io_acct(struct block_device *bdev, enum req_op op,\n\t\t      unsigned int sectors, unsigned long start_time)\n{\n\tconst int sgrp = op_stat_group(op);\n\tunsigned long now = READ_ONCE(jiffies);\n\tunsigned long duration = now - start_time;\n\n\tpart_stat_lock();\n\tupdate_io_ticks(bdev, now, true);\n\tpart_stat_inc(bdev, ios[sgrp]);\n\tpart_stat_add(bdev, sectors[sgrp], sectors);\n\tpart_stat_add(bdev, nsecs[sgrp], jiffies_to_nsecs(duration));\n\tpart_stat_local_dec(bdev, in_flight[op_is_write(op)]);\n\tpart_stat_unlock();\n}\nEXPORT_SYMBOL(bdev_end_io_acct);\n\nvoid bio_end_io_acct_remapped(struct bio *bio, unsigned long start_time,\n\t\t\t      struct block_device *orig_bdev)\n{\n\tbdev_end_io_acct(orig_bdev, bio_op(bio), bio_sectors(bio), start_time);\n}\nEXPORT_SYMBOL_GPL(bio_end_io_acct_remapped);\n\n \nint blk_lld_busy(struct request_queue *q)\n{\n\tif (queue_is_mq(q) && q->mq_ops->busy)\n\t\treturn q->mq_ops->busy(q);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(blk_lld_busy);\n\nint kblockd_schedule_work(struct work_struct *work)\n{\n\treturn queue_work(kblockd_workqueue, work);\n}\nEXPORT_SYMBOL(kblockd_schedule_work);\n\nint kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork,\n\t\t\t\tunsigned long delay)\n{\n\treturn mod_delayed_work_on(cpu, kblockd_workqueue, dwork, delay);\n}\nEXPORT_SYMBOL(kblockd_mod_delayed_work_on);\n\nvoid blk_start_plug_nr_ios(struct blk_plug *plug, unsigned short nr_ios)\n{\n\tstruct task_struct *tsk = current;\n\n\t \n\tif (tsk->plug)\n\t\treturn;\n\n\tplug->mq_list = NULL;\n\tplug->cached_rq = NULL;\n\tplug->nr_ios = min_t(unsigned short, nr_ios, BLK_MAX_REQUEST_COUNT);\n\tplug->rq_count = 0;\n\tplug->multiple_queues = false;\n\tplug->has_elevator = false;\n\tINIT_LIST_HEAD(&plug->cb_list);\n\n\t \n\ttsk->plug = plug;\n}\n\n \nvoid blk_start_plug(struct blk_plug *plug)\n{\n\tblk_start_plug_nr_ios(plug, 1);\n}\nEXPORT_SYMBOL(blk_start_plug);\n\nstatic void flush_plug_callbacks(struct blk_plug *plug, bool from_schedule)\n{\n\tLIST_HEAD(callbacks);\n\n\twhile (!list_empty(&plug->cb_list)) {\n\t\tlist_splice_init(&plug->cb_list, &callbacks);\n\n\t\twhile (!list_empty(&callbacks)) {\n\t\t\tstruct blk_plug_cb *cb = list_first_entry(&callbacks,\n\t\t\t\t\t\t\t  struct blk_plug_cb,\n\t\t\t\t\t\t\t  list);\n\t\t\tlist_del(&cb->list);\n\t\t\tcb->callback(cb, from_schedule);\n\t\t}\n\t}\n}\n\nstruct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug, void *data,\n\t\t\t\t      int size)\n{\n\tstruct blk_plug *plug = current->plug;\n\tstruct blk_plug_cb *cb;\n\n\tif (!plug)\n\t\treturn NULL;\n\n\tlist_for_each_entry(cb, &plug->cb_list, list)\n\t\tif (cb->callback == unplug && cb->data == data)\n\t\t\treturn cb;\n\n\t \n\tBUG_ON(size < sizeof(*cb));\n\tcb = kzalloc(size, GFP_ATOMIC);\n\tif (cb) {\n\t\tcb->data = data;\n\t\tcb->callback = unplug;\n\t\tlist_add(&cb->list, &plug->cb_list);\n\t}\n\treturn cb;\n}\nEXPORT_SYMBOL(blk_check_plugged);\n\nvoid __blk_flush_plug(struct blk_plug *plug, bool from_schedule)\n{\n\tif (!list_empty(&plug->cb_list))\n\t\tflush_plug_callbacks(plug, from_schedule);\n\tblk_mq_flush_plug_list(plug, from_schedule);\n\t \n\tif (unlikely(!rq_list_empty(plug->cached_rq)))\n\t\tblk_mq_free_plug_rqs(plug);\n}\n\n \nvoid blk_finish_plug(struct blk_plug *plug)\n{\n\tif (plug == current->plug) {\n\t\t__blk_flush_plug(plug, false);\n\t\tcurrent->plug = NULL;\n\t}\n}\nEXPORT_SYMBOL(blk_finish_plug);\n\nvoid blk_io_schedule(void)\n{\n\t \n\tunsigned long timeout = sysctl_hung_task_timeout_secs * HZ / 2;\n\n\tif (timeout)\n\t\tio_schedule_timeout(timeout);\n\telse\n\t\tio_schedule();\n}\nEXPORT_SYMBOL_GPL(blk_io_schedule);\n\nint __init blk_dev_init(void)\n{\n\tBUILD_BUG_ON((__force u32)REQ_OP_LAST >= (1 << REQ_OP_BITS));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tsizeof_field(struct request, cmd_flags));\n\tBUILD_BUG_ON(REQ_OP_BITS + REQ_FLAG_BITS > 8 *\n\t\t\tsizeof_field(struct bio, bi_opf));\n\n\t \n\tkblockd_workqueue = alloc_workqueue(\"kblockd\",\n\t\t\t\t\t    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!kblockd_workqueue)\n\t\tpanic(\"Failed to create kblockd\\n\");\n\n\tblk_requestq_cachep = kmem_cache_create(\"request_queue\",\n\t\t\tsizeof(struct request_queue), 0, SLAB_PANIC, NULL);\n\n\tblk_debugfs_root = debugfs_create_dir(\"block\", NULL);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}