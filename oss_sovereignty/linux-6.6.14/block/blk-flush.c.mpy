{
  "module_name": "blk-flush.c",
  "hash_id": "b68f0442e83a5e0b5be8a0278fb2e69969681aaec54ffd11f8145b7b84070168",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-flush.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/gfp.h>\n#include <linux/part_stat.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n\n \nenum {\n\tREQ_FSEQ_PREFLUSH\t= (1 << 0),  \n\tREQ_FSEQ_DATA\t\t= (1 << 1),  \n\tREQ_FSEQ_POSTFLUSH\t= (1 << 2),  \n\tREQ_FSEQ_DONE\t\t= (1 << 3),\n\n\tREQ_FSEQ_ACTIONS\t= REQ_FSEQ_PREFLUSH | REQ_FSEQ_DATA |\n\t\t\t\t  REQ_FSEQ_POSTFLUSH,\n\n\t \n\tFLUSH_PENDING_TIMEOUT\t= 5 * HZ,\n};\n\nstatic void blk_kick_flush(struct request_queue *q,\n\t\t\t   struct blk_flush_queue *fq, blk_opf_t flags);\n\nstatic inline struct blk_flush_queue *\nblk_get_flush_queue(struct request_queue *q, struct blk_mq_ctx *ctx)\n{\n\treturn blk_mq_map_queue(q, REQ_OP_FLUSH, ctx)->fq;\n}\n\nstatic unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)\n{\n\tunsigned int policy = 0;\n\n\tif (blk_rq_sectors(rq))\n\t\tpolicy |= REQ_FSEQ_DATA;\n\n\tif (fflags & (1UL << QUEUE_FLAG_WC)) {\n\t\tif (rq->cmd_flags & REQ_PREFLUSH)\n\t\t\tpolicy |= REQ_FSEQ_PREFLUSH;\n\t\tif (!(fflags & (1UL << QUEUE_FLAG_FUA)) &&\n\t\t    (rq->cmd_flags & REQ_FUA))\n\t\t\tpolicy |= REQ_FSEQ_POSTFLUSH;\n\t}\n\treturn policy;\n}\n\nstatic unsigned int blk_flush_cur_seq(struct request *rq)\n{\n\treturn 1 << ffz(rq->flush.seq);\n}\n\nstatic void blk_flush_restore_request(struct request *rq)\n{\n\t \n\trq->bio = rq->biotail;\n\n\t \n\trq->rq_flags &= ~RQF_FLUSH_SEQ;\n\trq->end_io = rq->flush.saved_end_io;\n}\n\nstatic void blk_account_io_flush(struct request *rq)\n{\n\tstruct block_device *part = rq->q->disk->part0;\n\n\tpart_stat_lock();\n\tpart_stat_inc(part, ios[STAT_FLUSH]);\n\tpart_stat_add(part, nsecs[STAT_FLUSH],\n\t\t      ktime_get_ns() - rq->start_time_ns);\n\tpart_stat_unlock();\n}\n\n \nstatic void blk_flush_complete_seq(struct request *rq,\n\t\t\t\t   struct blk_flush_queue *fq,\n\t\t\t\t   unsigned int seq, blk_status_t error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tblk_opf_t cmd_flags;\n\n\tBUG_ON(rq->flush.seq & seq);\n\trq->flush.seq |= seq;\n\tcmd_flags = rq->cmd_flags;\n\n\tif (likely(!error))\n\t\tseq = blk_flush_cur_seq(rq);\n\telse\n\t\tseq = REQ_FSEQ_DONE;\n\n\tswitch (seq) {\n\tcase REQ_FSEQ_PREFLUSH:\n\tcase REQ_FSEQ_POSTFLUSH:\n\t\t \n\t\tif (list_empty(pending))\n\t\t\tfq->flush_pending_since = jiffies;\n\t\tlist_move_tail(&rq->queuelist, pending);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DATA:\n\t\tfq->flush_data_in_flight++;\n\t\tspin_lock(&q->requeue_lock);\n\t\tlist_move(&rq->queuelist, &q->requeue_list);\n\t\tspin_unlock(&q->requeue_lock);\n\t\tblk_mq_kick_requeue_list(q);\n\t\tbreak;\n\n\tcase REQ_FSEQ_DONE:\n\t\t \n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_flush_restore_request(rq);\n\t\tblk_mq_end_request(rq, error);\n\t\tbreak;\n\n\tdefault:\n\t\tBUG();\n\t}\n\n\tblk_kick_flush(q, fq, cmd_flags);\n}\n\nstatic enum rq_end_io_ret flush_end_io(struct request *flush_rq,\n\t\t\t\t       blk_status_t error)\n{\n\tstruct request_queue *q = flush_rq->q;\n\tstruct list_head *running;\n\tstruct request *rq, *n;\n\tunsigned long flags = 0;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, flush_rq->mq_ctx);\n\n\t \n\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\n\tif (!req_ref_put_and_test(flush_rq)) {\n\t\tfq->rq_status = error;\n\t\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n\t\treturn RQ_END_IO_NONE;\n\t}\n\n\tblk_account_io_flush(flush_rq);\n\t \n\tWRITE_ONCE(flush_rq->state, MQ_RQ_IDLE);\n\tif (fq->rq_status != BLK_STS_OK) {\n\t\terror = fq->rq_status;\n\t\tfq->rq_status = BLK_STS_OK;\n\t}\n\n\tif (!q->elevator) {\n\t\tflush_rq->tag = BLK_MQ_NO_TAG;\n\t} else {\n\t\tblk_mq_put_driver_tag(flush_rq);\n\t\tflush_rq->internal_tag = BLK_MQ_NO_TAG;\n\t}\n\n\trunning = &fq->flush_queue[fq->flush_running_idx];\n\tBUG_ON(fq->flush_pending_idx == fq->flush_running_idx);\n\n\t \n\tfq->flush_running_idx ^= 1;\n\n\t \n\tlist_for_each_entry_safe(rq, n, running, queuelist) {\n\t\tunsigned int seq = blk_flush_cur_seq(rq);\n\n\t\tBUG_ON(seq != REQ_FSEQ_PREFLUSH && seq != REQ_FSEQ_POSTFLUSH);\n\t\tblk_flush_complete_seq(rq, fq, seq, error);\n\t}\n\n\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n\treturn RQ_END_IO_NONE;\n}\n\nbool is_flush_rq(struct request *rq)\n{\n\treturn rq->end_io == flush_end_io;\n}\n\n \nstatic void blk_kick_flush(struct request_queue *q, struct blk_flush_queue *fq,\n\t\t\t   blk_opf_t flags)\n{\n\tstruct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];\n\tstruct request *first_rq =\n\t\tlist_first_entry(pending, struct request, queuelist);\n\tstruct request *flush_rq = fq->flush_rq;\n\n\t \n\tif (fq->flush_pending_idx != fq->flush_running_idx || list_empty(pending))\n\t\treturn;\n\n\t \n\tif (fq->flush_data_in_flight &&\n\t    time_before(jiffies,\n\t\t\tfq->flush_pending_since + FLUSH_PENDING_TIMEOUT))\n\t\treturn;\n\n\t \n\tfq->flush_pending_idx ^= 1;\n\n\tblk_rq_init(q, flush_rq);\n\n\t \n\tflush_rq->mq_ctx = first_rq->mq_ctx;\n\tflush_rq->mq_hctx = first_rq->mq_hctx;\n\n\tif (!q->elevator) {\n\t\tflush_rq->tag = first_rq->tag;\n\n\t\t \n\t\tflush_rq->rq_flags |= RQF_MQ_INFLIGHT;\n\t} else\n\t\tflush_rq->internal_tag = first_rq->internal_tag;\n\n\tflush_rq->cmd_flags = REQ_OP_FLUSH | REQ_PREFLUSH;\n\tflush_rq->cmd_flags |= (flags & REQ_DRV) | (flags & REQ_FAILFAST_MASK);\n\tflush_rq->rq_flags |= RQF_FLUSH_SEQ;\n\tflush_rq->end_io = flush_end_io;\n\t \n\tsmp_wmb();\n\treq_ref_set(flush_rq, 1);\n\n\tspin_lock(&q->requeue_lock);\n\tlist_add_tail(&flush_rq->queuelist, &q->flush_list);\n\tspin_unlock(&q->requeue_lock);\n\n\tblk_mq_kick_requeue_list(q);\n}\n\nstatic enum rq_end_io_ret mq_flush_data_end_io(struct request *rq,\n\t\t\t\t\t       blk_status_t error)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\tstruct blk_mq_ctx *ctx = rq->mq_ctx;\n\tunsigned long flags;\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, ctx);\n\n\tif (q->elevator) {\n\t\tWARN_ON(rq->tag < 0);\n\t\tblk_mq_put_driver_tag(rq);\n\t}\n\n\t \n\tspin_lock_irqsave(&fq->mq_flush_lock, flags);\n\tfq->flush_data_in_flight--;\n\t \n\tINIT_LIST_HEAD(&rq->queuelist);\n\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_DATA, error);\n\tspin_unlock_irqrestore(&fq->mq_flush_lock, flags);\n\n\tblk_mq_sched_restart(hctx);\n\treturn RQ_END_IO_NONE;\n}\n\nstatic void blk_rq_init_flush(struct request *rq)\n{\n\trq->flush.seq = 0;\n\trq->rq_flags |= RQF_FLUSH_SEQ;\n\trq->flush.saved_end_io = rq->end_io;  \n\trq->end_io = mq_flush_data_end_io;\n}\n\n \nbool blk_insert_flush(struct request *rq)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned long fflags = q->queue_flags;\t \n\tunsigned int policy = blk_flush_policy(fflags, rq);\n\tstruct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);\n\n\t \n\tWARN_ON_ONCE(rq->bio != rq->biotail);\n\n\t \n\trq->cmd_flags &= ~REQ_PREFLUSH;\n\tif (!(fflags & (1UL << QUEUE_FLAG_FUA)))\n\t\trq->cmd_flags &= ~REQ_FUA;\n\n\t \n\trq->cmd_flags |= REQ_SYNC;\n\n\tswitch (policy) {\n\tcase 0:\n\t\t \n\t\tblk_mq_end_request(rq, 0);\n\t\treturn true;\n\tcase REQ_FSEQ_DATA:\n\t\t \n\t\treturn false;\n\tcase REQ_FSEQ_DATA | REQ_FSEQ_POSTFLUSH:\n\t\t \n\t\tblk_rq_init_flush(rq);\n\t\trq->flush.seq |= REQ_FSEQ_PREFLUSH;\n\t\tspin_lock_irq(&fq->mq_flush_lock);\n\t\tfq->flush_data_in_flight++;\n\t\tspin_unlock_irq(&fq->mq_flush_lock);\n\t\treturn false;\n\tdefault:\n\t\t \n\t\tblk_rq_init_flush(rq);\n\t\tspin_lock_irq(&fq->mq_flush_lock);\n\t\tblk_flush_complete_seq(rq, fq, REQ_FSEQ_ACTIONS & ~policy, 0);\n\t\tspin_unlock_irq(&fq->mq_flush_lock);\n\t\treturn true;\n\t}\n}\n\n \nint blkdev_issue_flush(struct block_device *bdev)\n{\n\tstruct bio bio;\n\n\tbio_init(&bio, bdev, NULL, 0, REQ_OP_WRITE | REQ_PREFLUSH);\n\treturn submit_bio_wait(&bio);\n}\nEXPORT_SYMBOL(blkdev_issue_flush);\n\nstruct blk_flush_queue *blk_alloc_flush_queue(int node, int cmd_size,\n\t\t\t\t\t      gfp_t flags)\n{\n\tstruct blk_flush_queue *fq;\n\tint rq_sz = sizeof(struct request);\n\n\tfq = kzalloc_node(sizeof(*fq), flags, node);\n\tif (!fq)\n\t\tgoto fail;\n\n\tspin_lock_init(&fq->mq_flush_lock);\n\n\trq_sz = round_up(rq_sz + cmd_size, cache_line_size());\n\tfq->flush_rq = kzalloc_node(rq_sz, flags, node);\n\tif (!fq->flush_rq)\n\t\tgoto fail_rq;\n\n\tINIT_LIST_HEAD(&fq->flush_queue[0]);\n\tINIT_LIST_HEAD(&fq->flush_queue[1]);\n\n\treturn fq;\n\n fail_rq:\n\tkfree(fq);\n fail:\n\treturn NULL;\n}\n\nvoid blk_free_flush_queue(struct blk_flush_queue *fq)\n{\n\t \n\tif (!fq)\n\t\treturn;\n\n\tkfree(fq->flush_rq);\n\tkfree(fq);\n}\n\n \nvoid blk_mq_hctx_set_fq_lock_class(struct blk_mq_hw_ctx *hctx,\n\t\tstruct lock_class_key *key)\n{\n\tlockdep_set_class(&hctx->fq->mq_flush_lock, key);\n}\nEXPORT_SYMBOL_GPL(blk_mq_hctx_set_fq_lock_class);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}