{
  "module_name": "blk-sysfs.c",
  "hash_id": "0b9681894e1c399dd1418a19f2bf192213382dd86e06c224252be4b8d3e1c558",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-sysfs.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/blktrace_api.h>\n#include <linux/debugfs.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n#include \"blk-wbt.h\"\n#include \"blk-cgroup.h\"\n#include \"blk-throttle.h\"\n\nstruct queue_sysfs_entry {\n\tstruct attribute attr;\n\tssize_t (*show)(struct request_queue *, char *);\n\tssize_t (*store)(struct request_queue *, const char *, size_t);\n};\n\nstatic ssize_t\nqueue_var_show(unsigned long var, char *page)\n{\n\treturn sprintf(page, \"%lu\\n\", var);\n}\n\nstatic ssize_t\nqueue_var_store(unsigned long *var, const char *page, size_t count)\n{\n\tint err;\n\tunsigned long v;\n\n\terr = kstrtoul(page, 10, &v);\n\tif (err || v > UINT_MAX)\n\t\treturn -EINVAL;\n\n\t*var = v;\n\n\treturn count;\n}\n\nstatic ssize_t queue_requests_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(q->nr_requests, page);\n}\n\nstatic ssize_t\nqueue_requests_store(struct request_queue *q, const char *page, size_t count)\n{\n\tunsigned long nr;\n\tint ret, err;\n\n\tif (!queue_is_mq(q))\n\t\treturn -EINVAL;\n\n\tret = queue_var_store(&nr, page, count);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (nr < BLKDEV_MIN_RQ)\n\t\tnr = BLKDEV_MIN_RQ;\n\n\terr = blk_mq_update_nr_requests(q, nr);\n\tif (err)\n\t\treturn err;\n\n\treturn ret;\n}\n\nstatic ssize_t queue_ra_show(struct request_queue *q, char *page)\n{\n\tunsigned long ra_kb;\n\n\tif (!q->disk)\n\t\treturn -EINVAL;\n\tra_kb = q->disk->bdi->ra_pages << (PAGE_SHIFT - 10);\n\treturn queue_var_show(ra_kb, page);\n}\n\nstatic ssize_t\nqueue_ra_store(struct request_queue *q, const char *page, size_t count)\n{\n\tunsigned long ra_kb;\n\tssize_t ret;\n\n\tif (!q->disk)\n\t\treturn -EINVAL;\n\tret = queue_var_store(&ra_kb, page, count);\n\tif (ret < 0)\n\t\treturn ret;\n\tq->disk->bdi->ra_pages = ra_kb >> (PAGE_SHIFT - 10);\n\treturn ret;\n}\n\nstatic ssize_t queue_max_sectors_show(struct request_queue *q, char *page)\n{\n\tint max_sectors_kb = queue_max_sectors(q) >> 1;\n\n\treturn queue_var_show(max_sectors_kb, page);\n}\n\nstatic ssize_t queue_max_segments_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_max_segments(q), page);\n}\n\nstatic ssize_t queue_max_discard_segments_show(struct request_queue *q,\n\t\tchar *page)\n{\n\treturn queue_var_show(queue_max_discard_segments(q), page);\n}\n\nstatic ssize_t queue_max_integrity_segments_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(q->limits.max_integrity_segments, page);\n}\n\nstatic ssize_t queue_max_segment_size_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_max_segment_size(q), page);\n}\n\nstatic ssize_t queue_logical_block_size_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_logical_block_size(q), page);\n}\n\nstatic ssize_t queue_physical_block_size_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_physical_block_size(q), page);\n}\n\nstatic ssize_t queue_chunk_sectors_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(q->limits.chunk_sectors, page);\n}\n\nstatic ssize_t queue_io_min_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_io_min(q), page);\n}\n\nstatic ssize_t queue_io_opt_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_io_opt(q), page);\n}\n\nstatic ssize_t queue_discard_granularity_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(q->limits.discard_granularity, page);\n}\n\nstatic ssize_t queue_discard_max_hw_show(struct request_queue *q, char *page)\n{\n\n\treturn sprintf(page, \"%llu\\n\",\n\t\t(unsigned long long)q->limits.max_hw_discard_sectors << 9);\n}\n\nstatic ssize_t queue_discard_max_show(struct request_queue *q, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t       (unsigned long long)q->limits.max_discard_sectors << 9);\n}\n\nstatic ssize_t queue_discard_max_store(struct request_queue *q,\n\t\t\t\t       const char *page, size_t count)\n{\n\tunsigned long max_discard;\n\tssize_t ret = queue_var_store(&max_discard, page, count);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (max_discard & (q->limits.discard_granularity - 1))\n\t\treturn -EINVAL;\n\n\tmax_discard >>= 9;\n\tif (max_discard > UINT_MAX)\n\t\treturn -EINVAL;\n\n\tif (max_discard > q->limits.max_hw_discard_sectors)\n\t\tmax_discard = q->limits.max_hw_discard_sectors;\n\n\tq->limits.max_discard_sectors = max_discard;\n\treturn ret;\n}\n\nstatic ssize_t queue_discard_zeroes_data_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(0, page);\n}\n\nstatic ssize_t queue_write_same_max_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(0, page);\n}\n\nstatic ssize_t queue_write_zeroes_max_show(struct request_queue *q, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t(unsigned long long)q->limits.max_write_zeroes_sectors << 9);\n}\n\nstatic ssize_t queue_zone_write_granularity_show(struct request_queue *q,\n\t\t\t\t\t\t char *page)\n{\n\treturn queue_var_show(queue_zone_write_granularity(q), page);\n}\n\nstatic ssize_t queue_zone_append_max_show(struct request_queue *q, char *page)\n{\n\tunsigned long long max_sectors = q->limits.max_zone_append_sectors;\n\n\treturn sprintf(page, \"%llu\\n\", max_sectors << SECTOR_SHIFT);\n}\n\nstatic ssize_t\nqueue_max_sectors_store(struct request_queue *q, const char *page, size_t count)\n{\n\tunsigned long var;\n\tunsigned int max_sectors_kb,\n\t\tmax_hw_sectors_kb = queue_max_hw_sectors(q) >> 1,\n\t\t\tpage_kb = 1 << (PAGE_SHIFT - 10);\n\tssize_t ret = queue_var_store(&var, page, count);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tmax_sectors_kb = (unsigned int)var;\n\tmax_hw_sectors_kb = min_not_zero(max_hw_sectors_kb,\n\t\t\t\t\t q->limits.max_dev_sectors >> 1);\n\tif (max_sectors_kb == 0) {\n\t\tq->limits.max_user_sectors = 0;\n\t\tmax_sectors_kb = min(max_hw_sectors_kb,\n\t\t\t\t     BLK_DEF_MAX_SECTORS >> 1);\n\t} else {\n\t\tif (max_sectors_kb > max_hw_sectors_kb ||\n\t\t    max_sectors_kb < page_kb)\n\t\t\treturn -EINVAL;\n\t\tq->limits.max_user_sectors = max_sectors_kb << 1;\n\t}\n\n\tspin_lock_irq(&q->queue_lock);\n\tq->limits.max_sectors = max_sectors_kb << 1;\n\tif (q->disk)\n\t\tq->disk->bdi->io_pages = max_sectors_kb >> (PAGE_SHIFT - 10);\n\tspin_unlock_irq(&q->queue_lock);\n\n\treturn ret;\n}\n\nstatic ssize_t queue_max_hw_sectors_show(struct request_queue *q, char *page)\n{\n\tint max_hw_sectors_kb = queue_max_hw_sectors(q) >> 1;\n\n\treturn queue_var_show(max_hw_sectors_kb, page);\n}\n\nstatic ssize_t queue_virt_boundary_mask_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(q->limits.virt_boundary_mask, page);\n}\n\nstatic ssize_t queue_dma_alignment_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(queue_dma_alignment(q), page);\n}\n\n#define QUEUE_SYSFS_BIT_FNS(name, flag, neg)\t\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\nqueue_##name##_show(struct request_queue *q, char *page)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tint bit;\t\t\t\t\t\t\t\\\n\tbit = test_bit(QUEUE_FLAG_##flag, &q->queue_flags);\t\t\\\n\treturn queue_var_show(neg ? !bit : bit, page);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\nqueue_##name##_store(struct request_queue *q, const char *page, size_t count) \\\n{\t\t\t\t\t\t\t\t\t\\\n\tunsigned long val;\t\t\t\t\t\t\\\n\tssize_t ret;\t\t\t\t\t\t\t\\\n\tret = queue_var_store(&val, page, count);\t\t\t\\\n\tif (ret < 0)\t\t\t\t\t\t\t\\\n\t\t return ret;\t\t\t\t\t\t\\\n\tif (neg)\t\t\t\t\t\t\t\\\n\t\tval = !val;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (val)\t\t\t\t\t\t\t\\\n\t\tblk_queue_flag_set(QUEUE_FLAG_##flag, q);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tblk_queue_flag_clear(QUEUE_FLAG_##flag, q);\t\t\\\n\treturn ret;\t\t\t\t\t\t\t\\\n}\n\nQUEUE_SYSFS_BIT_FNS(nonrot, NONROT, 1);\nQUEUE_SYSFS_BIT_FNS(random, ADD_RANDOM, 0);\nQUEUE_SYSFS_BIT_FNS(iostats, IO_STAT, 0);\nQUEUE_SYSFS_BIT_FNS(stable_writes, STABLE_WRITES, 0);\n#undef QUEUE_SYSFS_BIT_FNS\n\nstatic ssize_t queue_zoned_show(struct request_queue *q, char *page)\n{\n\tswitch (blk_queue_zoned_model(q)) {\n\tcase BLK_ZONED_HA:\n\t\treturn sprintf(page, \"host-aware\\n\");\n\tcase BLK_ZONED_HM:\n\t\treturn sprintf(page, \"host-managed\\n\");\n\tdefault:\n\t\treturn sprintf(page, \"none\\n\");\n\t}\n}\n\nstatic ssize_t queue_nr_zones_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(disk_nr_zones(q->disk), page);\n}\n\nstatic ssize_t queue_max_open_zones_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(bdev_max_open_zones(q->disk->part0), page);\n}\n\nstatic ssize_t queue_max_active_zones_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(bdev_max_active_zones(q->disk->part0), page);\n}\n\nstatic ssize_t queue_nomerges_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show((blk_queue_nomerges(q) << 1) |\n\t\t\t       blk_queue_noxmerges(q), page);\n}\n\nstatic ssize_t queue_nomerges_store(struct request_queue *q, const char *page,\n\t\t\t\t    size_t count)\n{\n\tunsigned long nm;\n\tssize_t ret = queue_var_store(&nm, page, count);\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\tblk_queue_flag_clear(QUEUE_FLAG_NOMERGES, q);\n\tblk_queue_flag_clear(QUEUE_FLAG_NOXMERGES, q);\n\tif (nm == 2)\n\t\tblk_queue_flag_set(QUEUE_FLAG_NOMERGES, q);\n\telse if (nm)\n\t\tblk_queue_flag_set(QUEUE_FLAG_NOXMERGES, q);\n\n\treturn ret;\n}\n\nstatic ssize_t queue_rq_affinity_show(struct request_queue *q, char *page)\n{\n\tbool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);\n\tbool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);\n\n\treturn queue_var_show(set << force, page);\n}\n\nstatic ssize_t\nqueue_rq_affinity_store(struct request_queue *q, const char *page, size_t count)\n{\n\tssize_t ret = -EINVAL;\n#ifdef CONFIG_SMP\n\tunsigned long val;\n\n\tret = queue_var_store(&val, page, count);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (val == 2) {\n\t\tblk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);\n\t\tblk_queue_flag_set(QUEUE_FLAG_SAME_FORCE, q);\n\t} else if (val == 1) {\n\t\tblk_queue_flag_set(QUEUE_FLAG_SAME_COMP, q);\n\t\tblk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);\n\t} else if (val == 0) {\n\t\tblk_queue_flag_clear(QUEUE_FLAG_SAME_COMP, q);\n\t\tblk_queue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);\n\t}\n#endif\n\treturn ret;\n}\n\nstatic ssize_t queue_poll_delay_show(struct request_queue *q, char *page)\n{\n\treturn sprintf(page, \"%d\\n\", -1);\n}\n\nstatic ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,\n\t\t\t\tsize_t count)\n{\n\treturn count;\n}\n\nstatic ssize_t queue_poll_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);\n}\n\nstatic ssize_t queue_poll_store(struct request_queue *q, const char *page,\n\t\t\t\tsize_t count)\n{\n\tif (!test_bit(QUEUE_FLAG_POLL, &q->queue_flags))\n\t\treturn -EINVAL;\n\tpr_info_ratelimited(\"writes to the poll attribute are ignored.\\n\");\n\tpr_info_ratelimited(\"please use driver specific parameters instead.\\n\");\n\treturn count;\n}\n\nstatic ssize_t queue_io_timeout_show(struct request_queue *q, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", jiffies_to_msecs(q->rq_timeout));\n}\n\nstatic ssize_t queue_io_timeout_store(struct request_queue *q, const char *page,\n\t\t\t\t  size_t count)\n{\n\tunsigned int val;\n\tint err;\n\n\terr = kstrtou32(page, 10, &val);\n\tif (err || val == 0)\n\t\treturn -EINVAL;\n\n\tblk_queue_rq_timeout(q, msecs_to_jiffies(val));\n\n\treturn count;\n}\n\nstatic ssize_t queue_wc_show(struct request_queue *q, char *page)\n{\n\tif (test_bit(QUEUE_FLAG_WC, &q->queue_flags))\n\t\treturn sprintf(page, \"write back\\n\");\n\n\treturn sprintf(page, \"write through\\n\");\n}\n\nstatic ssize_t queue_wc_store(struct request_queue *q, const char *page,\n\t\t\t      size_t count)\n{\n\tif (!strncmp(page, \"write back\", 10)) {\n\t\tif (!test_bit(QUEUE_FLAG_HW_WC, &q->queue_flags))\n\t\t\treturn -EINVAL;\n\t\tblk_queue_flag_set(QUEUE_FLAG_WC, q);\n\t} else if (!strncmp(page, \"write through\", 13) ||\n\t\t !strncmp(page, \"none\", 4)) {\n\t\tblk_queue_flag_clear(QUEUE_FLAG_WC, q);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\treturn count;\n}\n\nstatic ssize_t queue_fua_show(struct request_queue *q, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", test_bit(QUEUE_FLAG_FUA, &q->queue_flags));\n}\n\nstatic ssize_t queue_dax_show(struct request_queue *q, char *page)\n{\n\treturn queue_var_show(blk_queue_dax(q), page);\n}\n\n#define QUEUE_RO_ENTRY(_prefix, _name)\t\t\t\\\nstatic struct queue_sysfs_entry _prefix##_entry = {\t\\\n\t.attr\t= { .name = _name, .mode = 0444 },\t\\\n\t.show\t= _prefix##_show,\t\t\t\\\n};\n\n#define QUEUE_RW_ENTRY(_prefix, _name)\t\t\t\\\nstatic struct queue_sysfs_entry _prefix##_entry = {\t\\\n\t.attr\t= { .name = _name, .mode = 0644 },\t\\\n\t.show\t= _prefix##_show,\t\t\t\\\n\t.store\t= _prefix##_store,\t\t\t\\\n};\n\nQUEUE_RW_ENTRY(queue_requests, \"nr_requests\");\nQUEUE_RW_ENTRY(queue_ra, \"read_ahead_kb\");\nQUEUE_RW_ENTRY(queue_max_sectors, \"max_sectors_kb\");\nQUEUE_RO_ENTRY(queue_max_hw_sectors, \"max_hw_sectors_kb\");\nQUEUE_RO_ENTRY(queue_max_segments, \"max_segments\");\nQUEUE_RO_ENTRY(queue_max_integrity_segments, \"max_integrity_segments\");\nQUEUE_RO_ENTRY(queue_max_segment_size, \"max_segment_size\");\nQUEUE_RW_ENTRY(elv_iosched, \"scheduler\");\n\nQUEUE_RO_ENTRY(queue_logical_block_size, \"logical_block_size\");\nQUEUE_RO_ENTRY(queue_physical_block_size, \"physical_block_size\");\nQUEUE_RO_ENTRY(queue_chunk_sectors, \"chunk_sectors\");\nQUEUE_RO_ENTRY(queue_io_min, \"minimum_io_size\");\nQUEUE_RO_ENTRY(queue_io_opt, \"optimal_io_size\");\n\nQUEUE_RO_ENTRY(queue_max_discard_segments, \"max_discard_segments\");\nQUEUE_RO_ENTRY(queue_discard_granularity, \"discard_granularity\");\nQUEUE_RO_ENTRY(queue_discard_max_hw, \"discard_max_hw_bytes\");\nQUEUE_RW_ENTRY(queue_discard_max, \"discard_max_bytes\");\nQUEUE_RO_ENTRY(queue_discard_zeroes_data, \"discard_zeroes_data\");\n\nQUEUE_RO_ENTRY(queue_write_same_max, \"write_same_max_bytes\");\nQUEUE_RO_ENTRY(queue_write_zeroes_max, \"write_zeroes_max_bytes\");\nQUEUE_RO_ENTRY(queue_zone_append_max, \"zone_append_max_bytes\");\nQUEUE_RO_ENTRY(queue_zone_write_granularity, \"zone_write_granularity\");\n\nQUEUE_RO_ENTRY(queue_zoned, \"zoned\");\nQUEUE_RO_ENTRY(queue_nr_zones, \"nr_zones\");\nQUEUE_RO_ENTRY(queue_max_open_zones, \"max_open_zones\");\nQUEUE_RO_ENTRY(queue_max_active_zones, \"max_active_zones\");\n\nQUEUE_RW_ENTRY(queue_nomerges, \"nomerges\");\nQUEUE_RW_ENTRY(queue_rq_affinity, \"rq_affinity\");\nQUEUE_RW_ENTRY(queue_poll, \"io_poll\");\nQUEUE_RW_ENTRY(queue_poll_delay, \"io_poll_delay\");\nQUEUE_RW_ENTRY(queue_wc, \"write_cache\");\nQUEUE_RO_ENTRY(queue_fua, \"fua\");\nQUEUE_RO_ENTRY(queue_dax, \"dax\");\nQUEUE_RW_ENTRY(queue_io_timeout, \"io_timeout\");\nQUEUE_RO_ENTRY(queue_virt_boundary_mask, \"virt_boundary_mask\");\nQUEUE_RO_ENTRY(queue_dma_alignment, \"dma_alignment\");\n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nQUEUE_RW_ENTRY(blk_throtl_sample_time, \"throttle_sample_time\");\n#endif\n\n \nstatic struct queue_sysfs_entry queue_hw_sector_size_entry = {\n\t.attr = {.name = \"hw_sector_size\", .mode = 0444 },\n\t.show = queue_logical_block_size_show,\n};\n\nQUEUE_RW_ENTRY(queue_nonrot, \"rotational\");\nQUEUE_RW_ENTRY(queue_iostats, \"iostats\");\nQUEUE_RW_ENTRY(queue_random, \"add_random\");\nQUEUE_RW_ENTRY(queue_stable_writes, \"stable_writes\");\n\n#ifdef CONFIG_BLK_WBT\nstatic ssize_t queue_var_store64(s64 *var, const char *page)\n{\n\tint err;\n\ts64 v;\n\n\terr = kstrtos64(page, 10, &v);\n\tif (err < 0)\n\t\treturn err;\n\n\t*var = v;\n\treturn 0;\n}\n\nstatic ssize_t queue_wb_lat_show(struct request_queue *q, char *page)\n{\n\tif (!wbt_rq_qos(q))\n\t\treturn -EINVAL;\n\n\tif (wbt_disabled(q))\n\t\treturn sprintf(page, \"0\\n\");\n\n\treturn sprintf(page, \"%llu\\n\", div_u64(wbt_get_min_lat(q), 1000));\n}\n\nstatic ssize_t queue_wb_lat_store(struct request_queue *q, const char *page,\n\t\t\t\t  size_t count)\n{\n\tstruct rq_qos *rqos;\n\tssize_t ret;\n\ts64 val;\n\n\tret = queue_var_store64(&val, page);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (val < -1)\n\t\treturn -EINVAL;\n\n\trqos = wbt_rq_qos(q);\n\tif (!rqos) {\n\t\tret = wbt_init(q->disk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (val == -1)\n\t\tval = wbt_default_latency_nsec(q);\n\telse if (val >= 0)\n\t\tval *= 1000ULL;\n\n\tif (wbt_get_min_lat(q) == val)\n\t\treturn count;\n\n\t \n\tblk_mq_freeze_queue(q);\n\tblk_mq_quiesce_queue(q);\n\n\twbt_set_min_lat(q, val);\n\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_unfreeze_queue(q);\n\n\treturn count;\n}\n\nQUEUE_RW_ENTRY(queue_wb_lat, \"wbt_lat_usec\");\n#endif\n\nstatic struct attribute *queue_attrs[] = {\n\t&queue_ra_entry.attr,\n\t&queue_max_hw_sectors_entry.attr,\n\t&queue_max_sectors_entry.attr,\n\t&queue_max_segments_entry.attr,\n\t&queue_max_discard_segments_entry.attr,\n\t&queue_max_integrity_segments_entry.attr,\n\t&queue_max_segment_size_entry.attr,\n\t&queue_hw_sector_size_entry.attr,\n\t&queue_logical_block_size_entry.attr,\n\t&queue_physical_block_size_entry.attr,\n\t&queue_chunk_sectors_entry.attr,\n\t&queue_io_min_entry.attr,\n\t&queue_io_opt_entry.attr,\n\t&queue_discard_granularity_entry.attr,\n\t&queue_discard_max_entry.attr,\n\t&queue_discard_max_hw_entry.attr,\n\t&queue_discard_zeroes_data_entry.attr,\n\t&queue_write_same_max_entry.attr,\n\t&queue_write_zeroes_max_entry.attr,\n\t&queue_zone_append_max_entry.attr,\n\t&queue_zone_write_granularity_entry.attr,\n\t&queue_nonrot_entry.attr,\n\t&queue_zoned_entry.attr,\n\t&queue_nr_zones_entry.attr,\n\t&queue_max_open_zones_entry.attr,\n\t&queue_max_active_zones_entry.attr,\n\t&queue_nomerges_entry.attr,\n\t&queue_iostats_entry.attr,\n\t&queue_stable_writes_entry.attr,\n\t&queue_random_entry.attr,\n\t&queue_poll_entry.attr,\n\t&queue_wc_entry.attr,\n\t&queue_fua_entry.attr,\n\t&queue_dax_entry.attr,\n\t&queue_poll_delay_entry.attr,\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\n\t&blk_throtl_sample_time_entry.attr,\n#endif\n\t&queue_virt_boundary_mask_entry.attr,\n\t&queue_dma_alignment_entry.attr,\n\tNULL,\n};\n\nstatic struct attribute *blk_mq_queue_attrs[] = {\n\t&queue_requests_entry.attr,\n\t&elv_iosched_entry.attr,\n\t&queue_rq_affinity_entry.attr,\n\t&queue_io_timeout_entry.attr,\n#ifdef CONFIG_BLK_WBT\n\t&queue_wb_lat_entry.attr,\n#endif\n\tNULL,\n};\n\nstatic umode_t queue_attr_visible(struct kobject *kobj, struct attribute *attr,\n\t\t\t\tint n)\n{\n\tstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\n\tstruct request_queue *q = disk->queue;\n\n\tif ((attr == &queue_max_open_zones_entry.attr ||\n\t     attr == &queue_max_active_zones_entry.attr) &&\n\t    !blk_queue_is_zoned(q))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic umode_t blk_mq_queue_attr_visible(struct kobject *kobj,\n\t\t\t\t\t struct attribute *attr, int n)\n{\n\tstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\n\tstruct request_queue *q = disk->queue;\n\n\tif (!queue_is_mq(q))\n\t\treturn 0;\n\n\tif (attr == &queue_io_timeout_entry.attr && !q->mq_ops->timeout)\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic struct attribute_group queue_attr_group = {\n\t.attrs = queue_attrs,\n\t.is_visible = queue_attr_visible,\n};\n\nstatic struct attribute_group blk_mq_queue_attr_group = {\n\t.attrs = blk_mq_queue_attrs,\n\t.is_visible = blk_mq_queue_attr_visible,\n};\n\n#define to_queue(atr) container_of((atr), struct queue_sysfs_entry, attr)\n\nstatic ssize_t\nqueue_attr_show(struct kobject *kobj, struct attribute *attr, char *page)\n{\n\tstruct queue_sysfs_entry *entry = to_queue(attr);\n\tstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\n\tstruct request_queue *q = disk->queue;\n\tssize_t res;\n\n\tif (!entry->show)\n\t\treturn -EIO;\n\tmutex_lock(&q->sysfs_lock);\n\tres = entry->show(q, page);\n\tmutex_unlock(&q->sysfs_lock);\n\treturn res;\n}\n\nstatic ssize_t\nqueue_attr_store(struct kobject *kobj, struct attribute *attr,\n\t\t    const char *page, size_t length)\n{\n\tstruct queue_sysfs_entry *entry = to_queue(attr);\n\tstruct gendisk *disk = container_of(kobj, struct gendisk, queue_kobj);\n\tstruct request_queue *q = disk->queue;\n\tssize_t res;\n\n\tif (!entry->store)\n\t\treturn -EIO;\n\n\tmutex_lock(&q->sysfs_lock);\n\tres = entry->store(q, page, length);\n\tmutex_unlock(&q->sysfs_lock);\n\treturn res;\n}\n\nstatic const struct sysfs_ops queue_sysfs_ops = {\n\t.show\t= queue_attr_show,\n\t.store\t= queue_attr_store,\n};\n\nstatic const struct attribute_group *blk_queue_attr_groups[] = {\n\t&queue_attr_group,\n\t&blk_mq_queue_attr_group,\n\tNULL\n};\n\nstatic void blk_queue_release(struct kobject *kobj)\n{\n\t \n}\n\nstatic const struct kobj_type blk_queue_ktype = {\n\t.default_groups = blk_queue_attr_groups,\n\t.sysfs_ops\t= &queue_sysfs_ops,\n\t.release\t= blk_queue_release,\n};\n\nstatic void blk_debugfs_remove(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\n\tmutex_lock(&q->debugfs_mutex);\n\tblk_trace_shutdown(q);\n\tdebugfs_remove_recursive(q->debugfs_dir);\n\tq->debugfs_dir = NULL;\n\tq->sched_debugfs_dir = NULL;\n\tq->rqos_debugfs_dir = NULL;\n\tmutex_unlock(&q->debugfs_mutex);\n}\n\n \nint blk_register_queue(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tint ret;\n\n\tmutex_lock(&q->sysfs_dir_lock);\n\tkobject_init(&disk->queue_kobj, &blk_queue_ktype);\n\tret = kobject_add(&disk->queue_kobj, &disk_to_dev(disk)->kobj, \"queue\");\n\tif (ret < 0)\n\t\tgoto out_put_queue_kobj;\n\n\tif (queue_is_mq(q)) {\n\t\tret = blk_mq_sysfs_register(disk);\n\t\tif (ret)\n\t\t\tgoto out_put_queue_kobj;\n\t}\n\tmutex_lock(&q->sysfs_lock);\n\n\tmutex_lock(&q->debugfs_mutex);\n\tq->debugfs_dir = debugfs_create_dir(disk->disk_name, blk_debugfs_root);\n\tif (queue_is_mq(q))\n\t\tblk_mq_debugfs_register(q);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\tret = disk_register_independent_access_ranges(disk);\n\tif (ret)\n\t\tgoto out_debugfs_remove;\n\n\tif (q->elevator) {\n\t\tret = elv_register_queue(q, false);\n\t\tif (ret)\n\t\t\tgoto out_unregister_ia_ranges;\n\t}\n\n\tret = blk_crypto_sysfs_register(disk);\n\tif (ret)\n\t\tgoto out_elv_unregister;\n\n\tblk_queue_flag_set(QUEUE_FLAG_REGISTERED, q);\n\twbt_enable_default(disk);\n\tblk_throtl_register(disk);\n\n\t \n\tkobject_uevent(&disk->queue_kobj, KOBJ_ADD);\n\tif (q->elevator)\n\t\tkobject_uevent(&q->elevator->kobj, KOBJ_ADD);\n\tmutex_unlock(&q->sysfs_lock);\n\tmutex_unlock(&q->sysfs_dir_lock);\n\n\t \n\tif (!blk_queue_init_done(q)) {\n\t\tblk_queue_flag_set(QUEUE_FLAG_INIT_DONE, q);\n\t\tpercpu_ref_switch_to_percpu(&q->q_usage_counter);\n\t}\n\n\treturn ret;\n\nout_elv_unregister:\n\telv_unregister_queue(q);\nout_unregister_ia_ranges:\n\tdisk_unregister_independent_access_ranges(disk);\nout_debugfs_remove:\n\tblk_debugfs_remove(disk);\n\tmutex_unlock(&q->sysfs_lock);\nout_put_queue_kobj:\n\tkobject_put(&disk->queue_kobj);\n\tmutex_unlock(&q->sysfs_dir_lock);\n\treturn ret;\n}\n\n \nvoid blk_unregister_queue(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\n\tif (WARN_ON(!q))\n\t\treturn;\n\n\t \n\tif (!blk_queue_registered(q))\n\t\treturn;\n\n\t \n\tmutex_lock(&q->sysfs_lock);\n\tblk_queue_flag_clear(QUEUE_FLAG_REGISTERED, q);\n\tmutex_unlock(&q->sysfs_lock);\n\n\tmutex_lock(&q->sysfs_dir_lock);\n\t \n\tif (queue_is_mq(q))\n\t\tblk_mq_sysfs_unregister(disk);\n\tblk_crypto_sysfs_unregister(disk);\n\n\tmutex_lock(&q->sysfs_lock);\n\telv_unregister_queue(q);\n\tdisk_unregister_independent_access_ranges(disk);\n\tmutex_unlock(&q->sysfs_lock);\n\n\t \n\tkobject_uevent(&disk->queue_kobj, KOBJ_REMOVE);\n\tkobject_del(&disk->queue_kobj);\n\tmutex_unlock(&q->sysfs_dir_lock);\n\n\tblk_debugfs_remove(disk);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}