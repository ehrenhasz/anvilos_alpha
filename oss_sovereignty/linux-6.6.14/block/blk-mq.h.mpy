{
  "module_name": "blk-mq.h",
  "hash_id": "e3807596c09f1982e3b50a04fb9e5687b1b98f408010b092a7a70a74bb6d917c",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-mq.h",
  "human_readable_source": " \n#ifndef INT_BLK_MQ_H\n#define INT_BLK_MQ_H\n\n#include <linux/blk-mq.h>\n#include \"blk-stat.h\"\n\nstruct blk_mq_tag_set;\n\nstruct blk_mq_ctxs {\n\tstruct kobject kobj;\n\tstruct blk_mq_ctx __percpu\t*queue_ctx;\n};\n\n \nstruct blk_mq_ctx {\n\tstruct {\n\t\tspinlock_t\t\tlock;\n\t\tstruct list_head\trq_lists[HCTX_MAX_TYPES];\n\t} ____cacheline_aligned_in_smp;\n\n\tunsigned int\t\tcpu;\n\tunsigned short\t\tindex_hw[HCTX_MAX_TYPES];\n\tstruct blk_mq_hw_ctx \t*hctxs[HCTX_MAX_TYPES];\n\n\tstruct request_queue\t*queue;\n\tstruct blk_mq_ctxs      *ctxs;\n\tstruct kobject\t\tkobj;\n} ____cacheline_aligned_in_smp;\n\nenum {\n\tBLK_MQ_NO_TAG\t\t= -1U,\n\tBLK_MQ_TAG_MIN\t\t= 1,\n\tBLK_MQ_TAG_MAX\t\t= BLK_MQ_NO_TAG - 1,\n};\n\ntypedef unsigned int __bitwise blk_insert_t;\n#define BLK_MQ_INSERT_AT_HEAD\t\t((__force blk_insert_t)0x01)\n\nvoid blk_mq_submit_bio(struct bio *bio);\nint blk_mq_poll(struct request_queue *q, blk_qc_t cookie, struct io_comp_batch *iob,\n\t\tunsigned int flags);\nvoid blk_mq_exit_queue(struct request_queue *q);\nint blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr);\nvoid blk_mq_wake_waiters(struct request_queue *q);\nbool blk_mq_dispatch_rq_list(struct blk_mq_hw_ctx *hctx, struct list_head *,\n\t\t\t     unsigned int);\nvoid blk_mq_flush_busy_ctxs(struct blk_mq_hw_ctx *hctx, struct list_head *list);\nstruct request *blk_mq_dequeue_from_ctx(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\tstruct blk_mq_ctx *start);\nvoid blk_mq_put_rq_ref(struct request *rq);\n\n \nvoid blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,\n\t\t     unsigned int hctx_idx);\nvoid blk_mq_free_rq_map(struct blk_mq_tags *tags);\nstruct blk_mq_tags *blk_mq_alloc_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t\tunsigned int hctx_idx, unsigned int depth);\nvoid blk_mq_free_map_and_rqs(struct blk_mq_tag_set *set,\n\t\t\t     struct blk_mq_tags *tags,\n\t\t\t     unsigned int hctx_idx);\n\n \nextern int blk_mq_hw_queue_to_node(struct blk_mq_queue_map *qmap, unsigned int);\n\n \nstatic inline struct blk_mq_hw_ctx *blk_mq_map_queue_type(struct request_queue *q,\n\t\t\t\t\t\t\t  enum hctx_type type,\n\t\t\t\t\t\t\t  unsigned int cpu)\n{\n\treturn xa_load(&q->hctx_table, q->tag_set->map[type].mq_map[cpu]);\n}\n\nstatic inline enum hctx_type blk_mq_get_hctx_type(blk_opf_t opf)\n{\n\tenum hctx_type type = HCTX_TYPE_DEFAULT;\n\n\t \n\tif (opf & REQ_POLLED)\n\t\ttype = HCTX_TYPE_POLL;\n\telse if ((opf & REQ_OP_MASK) == REQ_OP_READ)\n\t\ttype = HCTX_TYPE_READ;\n\treturn type;\n}\n\n \nstatic inline struct blk_mq_hw_ctx *blk_mq_map_queue(struct request_queue *q,\n\t\t\t\t\t\t     blk_opf_t opf,\n\t\t\t\t\t\t     struct blk_mq_ctx *ctx)\n{\n\treturn ctx->hctxs[blk_mq_get_hctx_type(opf)];\n}\n\n \nextern void blk_mq_sysfs_init(struct request_queue *q);\nextern void blk_mq_sysfs_deinit(struct request_queue *q);\nint blk_mq_sysfs_register(struct gendisk *disk);\nvoid blk_mq_sysfs_unregister(struct gendisk *disk);\nint blk_mq_sysfs_register_hctxs(struct request_queue *q);\nvoid blk_mq_sysfs_unregister_hctxs(struct request_queue *q);\nextern void blk_mq_hctx_kobj_init(struct blk_mq_hw_ctx *hctx);\nvoid blk_mq_free_plug_rqs(struct blk_plug *plug);\nvoid blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule);\n\nvoid blk_mq_cancel_work_sync(struct request_queue *q);\n\nvoid blk_mq_release(struct request_queue *q);\n\nstatic inline struct blk_mq_ctx *__blk_mq_get_ctx(struct request_queue *q,\n\t\t\t\t\t   unsigned int cpu)\n{\n\treturn per_cpu_ptr(q->queue_ctx, cpu);\n}\n\n \nstatic inline struct blk_mq_ctx *blk_mq_get_ctx(struct request_queue *q)\n{\n\treturn __blk_mq_get_ctx(q, raw_smp_processor_id());\n}\n\nstruct blk_mq_alloc_data {\n\t \n\tstruct request_queue *q;\n\tblk_mq_req_flags_t flags;\n\tunsigned int shallow_depth;\n\tblk_opf_t cmd_flags;\n\treq_flags_t rq_flags;\n\n\t \n\tunsigned int nr_tags;\n\tstruct request **cached_rq;\n\n\t \n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_hw_ctx *hctx;\n};\n\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int nr_tags,\n\t\tunsigned int reserved_tags, int node, int alloc_policy);\nvoid blk_mq_free_tags(struct blk_mq_tags *tags);\nint blk_mq_init_bitmaps(struct sbitmap_queue *bitmap_tags,\n\t\tstruct sbitmap_queue *breserved_tags, unsigned int queue_depth,\n\t\tunsigned int reserved, int node, int alloc_policy);\n\nunsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data);\nunsigned long blk_mq_get_tags(struct blk_mq_alloc_data *data, int nr_tags,\n\t\tunsigned int *offset);\nvoid blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,\n\t\tunsigned int tag);\nvoid blk_mq_put_tags(struct blk_mq_tags *tags, int *tag_array, int nr_tags);\nint blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,\n\t\tstruct blk_mq_tags **tags, unsigned int depth, bool can_grow);\nvoid blk_mq_tag_resize_shared_tags(struct blk_mq_tag_set *set,\n\t\tunsigned int size);\nvoid blk_mq_tag_update_sched_shared_tags(struct request_queue *q);\n\nvoid blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool);\nvoid blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_tag_iter_fn *fn,\n\t\tvoid *priv);\nvoid blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,\n\t\tvoid *priv);\n\nstatic inline struct sbq_wait_state *bt_wait_ptr(struct sbitmap_queue *bt,\n\t\t\t\t\t\t struct blk_mq_hw_ctx *hctx)\n{\n\tif (!hctx)\n\t\treturn &bt->ws[0];\n\treturn sbq_wait_ptr(bt, &hctx->wait_index);\n}\n\nvoid __blk_mq_tag_busy(struct blk_mq_hw_ctx *);\nvoid __blk_mq_tag_idle(struct blk_mq_hw_ctx *);\n\nstatic inline void blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tif (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\t__blk_mq_tag_busy(hctx);\n}\n\nstatic inline void blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tif (hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\t__blk_mq_tag_idle(hctx);\n}\n\nstatic inline bool blk_mq_tag_is_reserved(struct blk_mq_tags *tags,\n\t\t\t\t\t  unsigned int tag)\n{\n\treturn tag < tags->nr_reserved_tags;\n}\n\nstatic inline bool blk_mq_is_shared_tags(unsigned int flags)\n{\n\treturn flags & BLK_MQ_F_TAG_HCTX_SHARED;\n}\n\nstatic inline struct blk_mq_tags *blk_mq_tags_from_data(struct blk_mq_alloc_data *data)\n{\n\tif (data->rq_flags & RQF_SCHED_TAGS)\n\t\treturn data->hctx->sched_tags;\n\treturn data->hctx->tags;\n}\n\nstatic inline bool blk_mq_hctx_stopped(struct blk_mq_hw_ctx *hctx)\n{\n\treturn test_bit(BLK_MQ_S_STOPPED, &hctx->state);\n}\n\nstatic inline bool blk_mq_hw_queue_mapped(struct blk_mq_hw_ctx *hctx)\n{\n\treturn hctx->nr_ctx && hctx->tags;\n}\n\nunsigned int blk_mq_in_flight(struct request_queue *q,\n\t\tstruct block_device *part);\nvoid blk_mq_in_flight_rw(struct request_queue *q, struct block_device *part,\n\t\tunsigned int inflight[2]);\n\nstatic inline void blk_mq_put_dispatch_budget(struct request_queue *q,\n\t\t\t\t\t      int budget_token)\n{\n\tif (q->mq_ops->put_budget)\n\t\tq->mq_ops->put_budget(q, budget_token);\n}\n\nstatic inline int blk_mq_get_dispatch_budget(struct request_queue *q)\n{\n\tif (q->mq_ops->get_budget)\n\t\treturn q->mq_ops->get_budget(q);\n\treturn 0;\n}\n\nstatic inline void blk_mq_set_rq_budget_token(struct request *rq, int token)\n{\n\tif (token < 0)\n\t\treturn;\n\n\tif (rq->q->mq_ops->set_rq_budget_token)\n\t\trq->q->mq_ops->set_rq_budget_token(rq, token);\n}\n\nstatic inline int blk_mq_get_rq_budget_token(struct request *rq)\n{\n\tif (rq->q->mq_ops->get_rq_budget_token)\n\t\treturn rq->q->mq_ops->get_rq_budget_token(rq);\n\treturn -1;\n}\n\nstatic inline void __blk_mq_inc_active_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tif (blk_mq_is_shared_tags(hctx->flags))\n\t\tatomic_inc(&hctx->queue->nr_active_requests_shared_tags);\n\telse\n\t\tatomic_inc(&hctx->nr_active);\n}\n\nstatic inline void __blk_mq_sub_active_requests(struct blk_mq_hw_ctx *hctx,\n\t\tint val)\n{\n\tif (blk_mq_is_shared_tags(hctx->flags))\n\t\tatomic_sub(val, &hctx->queue->nr_active_requests_shared_tags);\n\telse\n\t\tatomic_sub(val, &hctx->nr_active);\n}\n\nstatic inline void __blk_mq_dec_active_requests(struct blk_mq_hw_ctx *hctx)\n{\n\t__blk_mq_sub_active_requests(hctx, 1);\n}\n\nstatic inline int __blk_mq_active_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tif (blk_mq_is_shared_tags(hctx->flags))\n\t\treturn atomic_read(&hctx->queue->nr_active_requests_shared_tags);\n\treturn atomic_read(&hctx->nr_active);\n}\nstatic inline void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t   struct request *rq)\n{\n\tblk_mq_put_tag(hctx->tags, rq->mq_ctx, rq->tag);\n\trq->tag = BLK_MQ_NO_TAG;\n\n\tif (rq->rq_flags & RQF_MQ_INFLIGHT) {\n\t\trq->rq_flags &= ~RQF_MQ_INFLIGHT;\n\t\t__blk_mq_dec_active_requests(hctx);\n\t}\n}\n\nstatic inline void blk_mq_put_driver_tag(struct request *rq)\n{\n\tif (rq->tag == BLK_MQ_NO_TAG || rq->internal_tag == BLK_MQ_NO_TAG)\n\t\treturn;\n\n\t__blk_mq_put_driver_tag(rq->mq_hctx, rq);\n}\n\nbool __blk_mq_get_driver_tag(struct blk_mq_hw_ctx *hctx, struct request *rq);\n\nstatic inline bool blk_mq_get_driver_tag(struct request *rq)\n{\n\tstruct blk_mq_hw_ctx *hctx = rq->mq_hctx;\n\n\tif (rq->tag != BLK_MQ_NO_TAG &&\n\t    !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)) {\n\t\thctx->tags->rqs[rq->tag] = rq;\n\t\treturn true;\n\t}\n\n\treturn __blk_mq_get_driver_tag(hctx, rq);\n}\n\nstatic inline void blk_mq_clear_mq_map(struct blk_mq_queue_map *qmap)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu)\n\t\tqmap->mq_map[cpu] = 0;\n}\n\n \nstatic inline struct blk_plug *blk_mq_plug( struct bio *bio)\n{\n\t \n\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED) &&\n\t    bdev_op_is_zoned_write(bio->bi_bdev, bio_op(bio)))\n\t\treturn NULL;\n\n\t \n\treturn current->plug;\n}\n\n \nstatic inline void blk_mq_free_requests(struct list_head *list)\n{\n\twhile (!list_empty(list)) {\n\t\tstruct request *rq = list_entry_rq(list->next);\n\n\t\tlist_del_init(&rq->queuelist);\n\t\tblk_mq_free_request(rq);\n\t}\n}\n\n \nstatic inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t  struct sbitmap_queue *bt)\n{\n\tunsigned int depth, users;\n\n\tif (!hctx || !(hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED))\n\t\treturn true;\n\n\t \n\tif (bt->sb.depth == 1)\n\t\treturn true;\n\n\tif (blk_mq_is_shared_tags(hctx->flags)) {\n\t\tstruct request_queue *q = hctx->queue;\n\n\t\tif (!test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))\n\t\t\treturn true;\n\t} else {\n\t\tif (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\t\treturn true;\n\t}\n\n\tusers = READ_ONCE(hctx->tags->active_queues);\n\tif (!users)\n\t\treturn true;\n\n\t \n\tdepth = max((bt->sb.depth + users - 1) / users, 4U);\n\treturn __blk_mq_active_requests(hctx) < depth;\n}\n\n \n#define __blk_mq_run_dispatch_ops(q, check_sleep, dispatch_ops)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif ((q)->tag_set->flags & BLK_MQ_F_BLOCKING) {\t\t\\\n\t\tstruct blk_mq_tag_set *__tag_set = (q)->tag_set; \\\n\t\tint srcu_idx;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t\tmight_sleep_if(check_sleep);\t\t\t\\\n\t\tsrcu_idx = srcu_read_lock(__tag_set->srcu);\t\\\n\t\t(dispatch_ops);\t\t\t\t\t\\\n\t\tsrcu_read_unlock(__tag_set->srcu, srcu_idx);\t\\\n\t} else {\t\t\t\t\t\t\\\n\t\trcu_read_lock();\t\t\t\t\\\n\t\t(dispatch_ops);\t\t\t\t\t\\\n\t\trcu_read_unlock();\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n} while (0)\n\n#define blk_mq_run_dispatch_ops(q, dispatch_ops)\t\t\\\n\t__blk_mq_run_dispatch_ops(q, true, dispatch_ops)\t\\\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}