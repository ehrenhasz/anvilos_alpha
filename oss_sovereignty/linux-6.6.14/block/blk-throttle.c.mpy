{
  "module_name": "blk-throttle.c",
  "hash_id": "b5b8a24ed609c7f7f0b4abcbf115c3a2f5af6b35aa9b0fb131b1c091145e3381",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-throttle.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/blktrace_api.h>\n#include \"blk.h\"\n#include \"blk-cgroup-rwstat.h\"\n#include \"blk-stat.h\"\n#include \"blk-throttle.h\"\n\n \n#define THROTL_GRP_QUANTUM 8\n\n \n#define THROTL_QUANTUM 32\n\n \n#define DFL_THROTL_SLICE_HD (HZ / 10)\n#define DFL_THROTL_SLICE_SSD (HZ / 50)\n#define MAX_THROTL_SLICE (HZ)\n#define MAX_IDLE_TIME (5L * 1000 * 1000)  \n#define MIN_THROTL_BPS (320 * 1024)\n#define MIN_THROTL_IOPS (10)\n#define DFL_LATENCY_TARGET (-1L)\n#define DFL_IDLE_THRESHOLD (0)\n#define DFL_HD_BASELINE_LATENCY (4000L)  \n#define LATENCY_FILTERED_SSD (0)\n \n#define LATENCY_FILTERED_HD (1000L)  \n\n \nstatic struct workqueue_struct *kthrotld_workqueue;\n\n#define rb_entry_tg(node)\trb_entry((node), struct throtl_grp, rb_node)\n\n \n#define LATENCY_BUCKET_SIZE 9\n\nstruct latency_bucket {\n\tunsigned long total_latency;  \n\tint samples;\n};\n\nstruct avg_latency_bucket {\n\tunsigned long latency;  \n\tbool valid;\n};\n\nstruct throtl_data\n{\n\t \n\tstruct throtl_service_queue service_queue;\n\n\tstruct request_queue *queue;\n\n\t \n\tunsigned int nr_queued[2];\n\n\tunsigned int throtl_slice;\n\n\t \n\tstruct work_struct dispatch_work;\n\tunsigned int limit_index;\n\tbool limit_valid[LIMIT_CNT];\n\n\tunsigned long low_upgrade_time;\n\tunsigned long low_downgrade_time;\n\n\tunsigned int scale;\n\n\tstruct latency_bucket tmp_buckets[2][LATENCY_BUCKET_SIZE];\n\tstruct avg_latency_bucket avg_buckets[2][LATENCY_BUCKET_SIZE];\n\tstruct latency_bucket __percpu *latency_buckets[2];\n\tunsigned long last_calculate_time;\n\tunsigned long filtered_latency;\n\n\tbool track_bio_latency;\n};\n\nstatic void throtl_pending_timer_fn(struct timer_list *t);\n\nstatic inline struct blkcg_gq *tg_to_blkg(struct throtl_grp *tg)\n{\n\treturn pd_to_blkg(&tg->pd);\n}\n\n \nstatic struct throtl_grp *sq_to_tg(struct throtl_service_queue *sq)\n{\n\tif (sq && sq->parent_sq)\n\t\treturn container_of(sq, struct throtl_grp, service_queue);\n\telse\n\t\treturn NULL;\n}\n\n \nstatic struct throtl_data *sq_to_td(struct throtl_service_queue *sq)\n{\n\tstruct throtl_grp *tg = sq_to_tg(sq);\n\n\tif (tg)\n\t\treturn tg->td;\n\telse\n\t\treturn container_of(sq, struct throtl_data, service_queue);\n}\n\n \nstatic uint64_t throtl_adjusted_limit(uint64_t low, struct throtl_data *td)\n{\n\t \n\tif (td->scale < 4096 && time_after_eq(jiffies,\n\t    td->low_upgrade_time + td->scale * td->throtl_slice))\n\t\ttd->scale = (jiffies - td->low_upgrade_time) / td->throtl_slice;\n\n\treturn low + (low >> 1) * td->scale;\n}\n\nstatic uint64_t tg_bps_limit(struct throtl_grp *tg, int rw)\n{\n\tstruct blkcg_gq *blkg = tg_to_blkg(tg);\n\tstruct throtl_data *td;\n\tuint64_t ret;\n\n\tif (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)\n\t\treturn U64_MAX;\n\n\ttd = tg->td;\n\tret = tg->bps[rw][td->limit_index];\n\tif (ret == 0 && td->limit_index == LIMIT_LOW) {\n\t\t \n\t\tif (!list_empty(&blkg->blkcg->css.children) ||\n\t\t    tg->iops[rw][td->limit_index])\n\t\t\treturn U64_MAX;\n\t\telse\n\t\t\treturn MIN_THROTL_BPS;\n\t}\n\n\tif (td->limit_index == LIMIT_MAX && tg->bps[rw][LIMIT_LOW] &&\n\t    tg->bps[rw][LIMIT_LOW] != tg->bps[rw][LIMIT_MAX]) {\n\t\tuint64_t adjusted;\n\n\t\tadjusted = throtl_adjusted_limit(tg->bps[rw][LIMIT_LOW], td);\n\t\tret = min(tg->bps[rw][LIMIT_MAX], adjusted);\n\t}\n\treturn ret;\n}\n\nstatic unsigned int tg_iops_limit(struct throtl_grp *tg, int rw)\n{\n\tstruct blkcg_gq *blkg = tg_to_blkg(tg);\n\tstruct throtl_data *td;\n\tunsigned int ret;\n\n\tif (cgroup_subsys_on_dfl(io_cgrp_subsys) && !blkg->parent)\n\t\treturn UINT_MAX;\n\n\ttd = tg->td;\n\tret = tg->iops[rw][td->limit_index];\n\tif (ret == 0 && tg->td->limit_index == LIMIT_LOW) {\n\t\t \n\t\tif (!list_empty(&blkg->blkcg->css.children) ||\n\t\t    tg->bps[rw][td->limit_index])\n\t\t\treturn UINT_MAX;\n\t\telse\n\t\t\treturn MIN_THROTL_IOPS;\n\t}\n\n\tif (td->limit_index == LIMIT_MAX && tg->iops[rw][LIMIT_LOW] &&\n\t    tg->iops[rw][LIMIT_LOW] != tg->iops[rw][LIMIT_MAX]) {\n\t\tuint64_t adjusted;\n\n\t\tadjusted = throtl_adjusted_limit(tg->iops[rw][LIMIT_LOW], td);\n\t\tif (adjusted > UINT_MAX)\n\t\t\tadjusted = UINT_MAX;\n\t\tret = min_t(unsigned int, tg->iops[rw][LIMIT_MAX], adjusted);\n\t}\n\treturn ret;\n}\n\n#define request_bucket_index(sectors) \\\n\tclamp_t(int, order_base_2(sectors) - 3, 0, LATENCY_BUCKET_SIZE - 1)\n\n \n#define throtl_log(sq, fmt, args...)\tdo {\t\t\t\t\\\n\tstruct throtl_grp *__tg = sq_to_tg((sq));\t\t\t\\\n\tstruct throtl_data *__td = sq_to_td((sq));\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t(void)__td;\t\t\t\t\t\t\t\\\n\tif (likely(!blk_trace_note_message_enabled(__td->queue)))\t\\\n\t\tbreak;\t\t\t\t\t\t\t\\\n\tif ((__tg)) {\t\t\t\t\t\t\t\\\n\t\tblk_add_cgroup_trace_msg(__td->queue,\t\t\t\\\n\t\t\t&tg_to_blkg(__tg)->blkcg->css, \"throtl \" fmt, ##args);\\\n\t} else {\t\t\t\t\t\t\t\\\n\t\tblk_add_trace_msg(__td->queue, \"throtl \" fmt, ##args);\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\nstatic inline unsigned int throtl_bio_data_size(struct bio *bio)\n{\n\t \n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD))\n\t\treturn 512;\n\treturn bio->bi_iter.bi_size;\n}\n\nstatic void throtl_qnode_init(struct throtl_qnode *qn, struct throtl_grp *tg)\n{\n\tINIT_LIST_HEAD(&qn->node);\n\tbio_list_init(&qn->bios);\n\tqn->tg = tg;\n}\n\n \nstatic void throtl_qnode_add_bio(struct bio *bio, struct throtl_qnode *qn,\n\t\t\t\t struct list_head *queued)\n{\n\tbio_list_add(&qn->bios, bio);\n\tif (list_empty(&qn->node)) {\n\t\tlist_add_tail(&qn->node, queued);\n\t\tblkg_get(tg_to_blkg(qn->tg));\n\t}\n}\n\n \nstatic struct bio *throtl_peek_queued(struct list_head *queued)\n{\n\tstruct throtl_qnode *qn;\n\tstruct bio *bio;\n\n\tif (list_empty(queued))\n\t\treturn NULL;\n\n\tqn = list_first_entry(queued, struct throtl_qnode, node);\n\tbio = bio_list_peek(&qn->bios);\n\tWARN_ON_ONCE(!bio);\n\treturn bio;\n}\n\n \nstatic struct bio *throtl_pop_queued(struct list_head *queued,\n\t\t\t\t     struct throtl_grp **tg_to_put)\n{\n\tstruct throtl_qnode *qn;\n\tstruct bio *bio;\n\n\tif (list_empty(queued))\n\t\treturn NULL;\n\n\tqn = list_first_entry(queued, struct throtl_qnode, node);\n\tbio = bio_list_pop(&qn->bios);\n\tWARN_ON_ONCE(!bio);\n\n\tif (bio_list_empty(&qn->bios)) {\n\t\tlist_del_init(&qn->node);\n\t\tif (tg_to_put)\n\t\t\t*tg_to_put = qn->tg;\n\t\telse\n\t\t\tblkg_put(tg_to_blkg(qn->tg));\n\t} else {\n\t\tlist_move_tail(&qn->node, queued);\n\t}\n\n\treturn bio;\n}\n\n \nstatic void throtl_service_queue_init(struct throtl_service_queue *sq)\n{\n\tINIT_LIST_HEAD(&sq->queued[READ]);\n\tINIT_LIST_HEAD(&sq->queued[WRITE]);\n\tsq->pending_tree = RB_ROOT_CACHED;\n\ttimer_setup(&sq->pending_timer, throtl_pending_timer_fn, 0);\n}\n\nstatic struct blkg_policy_data *throtl_pd_alloc(struct gendisk *disk,\n\t\tstruct blkcg *blkcg, gfp_t gfp)\n{\n\tstruct throtl_grp *tg;\n\tint rw;\n\n\ttg = kzalloc_node(sizeof(*tg), gfp, disk->node_id);\n\tif (!tg)\n\t\treturn NULL;\n\n\tif (blkg_rwstat_init(&tg->stat_bytes, gfp))\n\t\tgoto err_free_tg;\n\n\tif (blkg_rwstat_init(&tg->stat_ios, gfp))\n\t\tgoto err_exit_stat_bytes;\n\n\tthrotl_service_queue_init(&tg->service_queue);\n\n\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\tthrotl_qnode_init(&tg->qnode_on_self[rw], tg);\n\t\tthrotl_qnode_init(&tg->qnode_on_parent[rw], tg);\n\t}\n\n\tRB_CLEAR_NODE(&tg->rb_node);\n\ttg->bps[READ][LIMIT_MAX] = U64_MAX;\n\ttg->bps[WRITE][LIMIT_MAX] = U64_MAX;\n\ttg->iops[READ][LIMIT_MAX] = UINT_MAX;\n\ttg->iops[WRITE][LIMIT_MAX] = UINT_MAX;\n\ttg->bps_conf[READ][LIMIT_MAX] = U64_MAX;\n\ttg->bps_conf[WRITE][LIMIT_MAX] = U64_MAX;\n\ttg->iops_conf[READ][LIMIT_MAX] = UINT_MAX;\n\ttg->iops_conf[WRITE][LIMIT_MAX] = UINT_MAX;\n\t \n\n\ttg->latency_target = DFL_LATENCY_TARGET;\n\ttg->latency_target_conf = DFL_LATENCY_TARGET;\n\ttg->idletime_threshold = DFL_IDLE_THRESHOLD;\n\ttg->idletime_threshold_conf = DFL_IDLE_THRESHOLD;\n\n\treturn &tg->pd;\n\nerr_exit_stat_bytes:\n\tblkg_rwstat_exit(&tg->stat_bytes);\nerr_free_tg:\n\tkfree(tg);\n\treturn NULL;\n}\n\nstatic void throtl_pd_init(struct blkg_policy_data *pd)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\tstruct blkcg_gq *blkg = tg_to_blkg(tg);\n\tstruct throtl_data *td = blkg->q->td;\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\n\t \n\tsq->parent_sq = &td->service_queue;\n\tif (cgroup_subsys_on_dfl(io_cgrp_subsys) && blkg->parent)\n\t\tsq->parent_sq = &blkg_to_tg(blkg->parent)->service_queue;\n\ttg->td = td;\n}\n\n \nstatic void tg_update_has_rules(struct throtl_grp *tg)\n{\n\tstruct throtl_grp *parent_tg = sq_to_tg(tg->service_queue.parent_sq);\n\tstruct throtl_data *td = tg->td;\n\tint rw;\n\n\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\ttg->has_rules_iops[rw] =\n\t\t\t(parent_tg && parent_tg->has_rules_iops[rw]) ||\n\t\t\t(td->limit_valid[td->limit_index] &&\n\t\t\t  tg_iops_limit(tg, rw) != UINT_MAX);\n\t\ttg->has_rules_bps[rw] =\n\t\t\t(parent_tg && parent_tg->has_rules_bps[rw]) ||\n\t\t\t(td->limit_valid[td->limit_index] &&\n\t\t\t (tg_bps_limit(tg, rw) != U64_MAX));\n\t}\n}\n\nstatic void throtl_pd_online(struct blkg_policy_data *pd)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\t \n\ttg_update_has_rules(tg);\n}\n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nstatic void blk_throtl_update_limit_valid(struct throtl_data *td)\n{\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkcg_gq *blkg;\n\tbool low_valid = false;\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_post(blkg, pos_css, td->queue->root_blkg) {\n\t\tstruct throtl_grp *tg = blkg_to_tg(blkg);\n\n\t\tif (tg->bps[READ][LIMIT_LOW] || tg->bps[WRITE][LIMIT_LOW] ||\n\t\t    tg->iops[READ][LIMIT_LOW] || tg->iops[WRITE][LIMIT_LOW]) {\n\t\t\tlow_valid = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\ttd->limit_valid[LIMIT_LOW] = low_valid;\n}\n#else\nstatic inline void blk_throtl_update_limit_valid(struct throtl_data *td)\n{\n}\n#endif\n\nstatic void throtl_upgrade_state(struct throtl_data *td);\nstatic void throtl_pd_offline(struct blkg_policy_data *pd)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\n\ttg->bps[READ][LIMIT_LOW] = 0;\n\ttg->bps[WRITE][LIMIT_LOW] = 0;\n\ttg->iops[READ][LIMIT_LOW] = 0;\n\ttg->iops[WRITE][LIMIT_LOW] = 0;\n\n\tblk_throtl_update_limit_valid(tg->td);\n\n\tif (!tg->td->limit_valid[tg->td->limit_index])\n\t\tthrotl_upgrade_state(tg->td);\n}\n\nstatic void throtl_pd_free(struct blkg_policy_data *pd)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\n\tdel_timer_sync(&tg->service_queue.pending_timer);\n\tblkg_rwstat_exit(&tg->stat_bytes);\n\tblkg_rwstat_exit(&tg->stat_ios);\n\tkfree(tg);\n}\n\nstatic struct throtl_grp *\nthrotl_rb_first(struct throtl_service_queue *parent_sq)\n{\n\tstruct rb_node *n;\n\n\tn = rb_first_cached(&parent_sq->pending_tree);\n\tWARN_ON_ONCE(!n);\n\tif (!n)\n\t\treturn NULL;\n\treturn rb_entry_tg(n);\n}\n\nstatic void throtl_rb_erase(struct rb_node *n,\n\t\t\t    struct throtl_service_queue *parent_sq)\n{\n\trb_erase_cached(n, &parent_sq->pending_tree);\n\tRB_CLEAR_NODE(n);\n}\n\nstatic void update_min_dispatch_time(struct throtl_service_queue *parent_sq)\n{\n\tstruct throtl_grp *tg;\n\n\ttg = throtl_rb_first(parent_sq);\n\tif (!tg)\n\t\treturn;\n\n\tparent_sq->first_pending_disptime = tg->disptime;\n}\n\nstatic void tg_service_queue_add(struct throtl_grp *tg)\n{\n\tstruct throtl_service_queue *parent_sq = tg->service_queue.parent_sq;\n\tstruct rb_node **node = &parent_sq->pending_tree.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct throtl_grp *__tg;\n\tunsigned long key = tg->disptime;\n\tbool leftmost = true;\n\n\twhile (*node != NULL) {\n\t\tparent = *node;\n\t\t__tg = rb_entry_tg(parent);\n\n\t\tif (time_before(key, __tg->disptime))\n\t\t\tnode = &parent->rb_left;\n\t\telse {\n\t\t\tnode = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\n\trb_link_node(&tg->rb_node, parent, node);\n\trb_insert_color_cached(&tg->rb_node, &parent_sq->pending_tree,\n\t\t\t       leftmost);\n}\n\nstatic void throtl_enqueue_tg(struct throtl_grp *tg)\n{\n\tif (!(tg->flags & THROTL_TG_PENDING)) {\n\t\ttg_service_queue_add(tg);\n\t\ttg->flags |= THROTL_TG_PENDING;\n\t\ttg->service_queue.parent_sq->nr_pending++;\n\t}\n}\n\nstatic void throtl_dequeue_tg(struct throtl_grp *tg)\n{\n\tif (tg->flags & THROTL_TG_PENDING) {\n\t\tstruct throtl_service_queue *parent_sq =\n\t\t\ttg->service_queue.parent_sq;\n\n\t\tthrotl_rb_erase(&tg->rb_node, parent_sq);\n\t\t--parent_sq->nr_pending;\n\t\ttg->flags &= ~THROTL_TG_PENDING;\n\t}\n}\n\n \nstatic void throtl_schedule_pending_timer(struct throtl_service_queue *sq,\n\t\t\t\t\t  unsigned long expires)\n{\n\tunsigned long max_expire = jiffies + 8 * sq_to_td(sq)->throtl_slice;\n\n\t \n\tif (time_after(expires, max_expire))\n\t\texpires = max_expire;\n\tmod_timer(&sq->pending_timer, expires);\n\tthrotl_log(sq, \"schedule timer. delay=%lu jiffies=%lu\",\n\t\t   expires - jiffies, jiffies);\n}\n\n \nstatic bool throtl_schedule_next_dispatch(struct throtl_service_queue *sq,\n\t\t\t\t\t  bool force)\n{\n\t \n\tif (!sq->nr_pending)\n\t\treturn true;\n\n\tupdate_min_dispatch_time(sq);\n\n\t \n\tif (force || time_after(sq->first_pending_disptime, jiffies)) {\n\t\tthrotl_schedule_pending_timer(sq, sq->first_pending_disptime);\n\t\treturn true;\n\t}\n\n\t \n\treturn false;\n}\n\nstatic inline void throtl_start_new_slice_with_credit(struct throtl_grp *tg,\n\t\tbool rw, unsigned long start)\n{\n\ttg->bytes_disp[rw] = 0;\n\ttg->io_disp[rw] = 0;\n\ttg->carryover_bytes[rw] = 0;\n\ttg->carryover_ios[rw] = 0;\n\n\t \n\tif (time_after(start, tg->slice_start[rw]))\n\t\ttg->slice_start[rw] = start;\n\n\ttg->slice_end[rw] = jiffies + tg->td->throtl_slice;\n\tthrotl_log(&tg->service_queue,\n\t\t   \"[%c] new slice with credit start=%lu end=%lu jiffies=%lu\",\n\t\t   rw == READ ? 'R' : 'W', tg->slice_start[rw],\n\t\t   tg->slice_end[rw], jiffies);\n}\n\nstatic inline void throtl_start_new_slice(struct throtl_grp *tg, bool rw,\n\t\t\t\t\t  bool clear_carryover)\n{\n\ttg->bytes_disp[rw] = 0;\n\ttg->io_disp[rw] = 0;\n\ttg->slice_start[rw] = jiffies;\n\ttg->slice_end[rw] = jiffies + tg->td->throtl_slice;\n\tif (clear_carryover) {\n\t\ttg->carryover_bytes[rw] = 0;\n\t\ttg->carryover_ios[rw] = 0;\n\t}\n\n\tthrotl_log(&tg->service_queue,\n\t\t   \"[%c] new slice start=%lu end=%lu jiffies=%lu\",\n\t\t   rw == READ ? 'R' : 'W', tg->slice_start[rw],\n\t\t   tg->slice_end[rw], jiffies);\n}\n\nstatic inline void throtl_set_slice_end(struct throtl_grp *tg, bool rw,\n\t\t\t\t\tunsigned long jiffy_end)\n{\n\ttg->slice_end[rw] = roundup(jiffy_end, tg->td->throtl_slice);\n}\n\nstatic inline void throtl_extend_slice(struct throtl_grp *tg, bool rw,\n\t\t\t\t       unsigned long jiffy_end)\n{\n\tthrotl_set_slice_end(tg, rw, jiffy_end);\n\tthrotl_log(&tg->service_queue,\n\t\t   \"[%c] extend slice start=%lu end=%lu jiffies=%lu\",\n\t\t   rw == READ ? 'R' : 'W', tg->slice_start[rw],\n\t\t   tg->slice_end[rw], jiffies);\n}\n\n \nstatic bool throtl_slice_used(struct throtl_grp *tg, bool rw)\n{\n\tif (time_in_range(jiffies, tg->slice_start[rw], tg->slice_end[rw]))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic unsigned int calculate_io_allowed(u32 iops_limit,\n\t\t\t\t\t unsigned long jiffy_elapsed)\n{\n\tunsigned int io_allowed;\n\tu64 tmp;\n\n\t \n\n\ttmp = (u64)iops_limit * jiffy_elapsed;\n\tdo_div(tmp, HZ);\n\n\tif (tmp > UINT_MAX)\n\t\tio_allowed = UINT_MAX;\n\telse\n\t\tio_allowed = tmp;\n\n\treturn io_allowed;\n}\n\nstatic u64 calculate_bytes_allowed(u64 bps_limit, unsigned long jiffy_elapsed)\n{\n\t \n\tif (ilog2(bps_limit) + ilog2(jiffy_elapsed) - ilog2(HZ) > 62)\n\t\treturn U64_MAX;\n\treturn mul_u64_u64_div_u64(bps_limit, (u64)jiffy_elapsed, (u64)HZ);\n}\n\n \nstatic inline void throtl_trim_slice(struct throtl_grp *tg, bool rw)\n{\n\tunsigned long time_elapsed;\n\tlong long bytes_trim;\n\tint io_trim;\n\n\tBUG_ON(time_before(tg->slice_end[rw], tg->slice_start[rw]));\n\n\t \n\tif (throtl_slice_used(tg, rw))\n\t\treturn;\n\n\t \n\n\tthrotl_set_slice_end(tg, rw, jiffies + tg->td->throtl_slice);\n\n\ttime_elapsed = rounddown(jiffies - tg->slice_start[rw],\n\t\t\t\t tg->td->throtl_slice);\n\tif (!time_elapsed)\n\t\treturn;\n\n\tbytes_trim = calculate_bytes_allowed(tg_bps_limit(tg, rw),\n\t\t\t\t\t     time_elapsed) +\n\t\t     tg->carryover_bytes[rw];\n\tio_trim = calculate_io_allowed(tg_iops_limit(tg, rw), time_elapsed) +\n\t\t  tg->carryover_ios[rw];\n\tif (bytes_trim <= 0 && io_trim <= 0)\n\t\treturn;\n\n\ttg->carryover_bytes[rw] = 0;\n\tif ((long long)tg->bytes_disp[rw] >= bytes_trim)\n\t\ttg->bytes_disp[rw] -= bytes_trim;\n\telse\n\t\ttg->bytes_disp[rw] = 0;\n\n\ttg->carryover_ios[rw] = 0;\n\tif ((int)tg->io_disp[rw] >= io_trim)\n\t\ttg->io_disp[rw] -= io_trim;\n\telse\n\t\ttg->io_disp[rw] = 0;\n\n\ttg->slice_start[rw] += time_elapsed;\n\n\tthrotl_log(&tg->service_queue,\n\t\t   \"[%c] trim slice nr=%lu bytes=%lld io=%d start=%lu end=%lu jiffies=%lu\",\n\t\t   rw == READ ? 'R' : 'W', time_elapsed / tg->td->throtl_slice,\n\t\t   bytes_trim, io_trim, tg->slice_start[rw], tg->slice_end[rw],\n\t\t   jiffies);\n}\n\nstatic void __tg_update_carryover(struct throtl_grp *tg, bool rw)\n{\n\tunsigned long jiffy_elapsed = jiffies - tg->slice_start[rw];\n\tu64 bps_limit = tg_bps_limit(tg, rw);\n\tu32 iops_limit = tg_iops_limit(tg, rw);\n\n\t \n\tif (bps_limit != U64_MAX)\n\t\ttg->carryover_bytes[rw] +=\n\t\t\tcalculate_bytes_allowed(bps_limit, jiffy_elapsed) -\n\t\t\ttg->bytes_disp[rw];\n\tif (iops_limit != UINT_MAX)\n\t\ttg->carryover_ios[rw] +=\n\t\t\tcalculate_io_allowed(iops_limit, jiffy_elapsed) -\n\t\t\ttg->io_disp[rw];\n}\n\nstatic void tg_update_carryover(struct throtl_grp *tg)\n{\n\tif (tg->service_queue.nr_queued[READ])\n\t\t__tg_update_carryover(tg, READ);\n\tif (tg->service_queue.nr_queued[WRITE])\n\t\t__tg_update_carryover(tg, WRITE);\n\n\t \n\tthrotl_log(&tg->service_queue, \"%s: %lld %lld %d %d\\n\", __func__,\n\t\t   tg->carryover_bytes[READ], tg->carryover_bytes[WRITE],\n\t\t   tg->carryover_ios[READ], tg->carryover_ios[WRITE]);\n}\n\nstatic unsigned long tg_within_iops_limit(struct throtl_grp *tg, struct bio *bio,\n\t\t\t\t u32 iops_limit)\n{\n\tbool rw = bio_data_dir(bio);\n\tint io_allowed;\n\tunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\n\n\tif (iops_limit == UINT_MAX) {\n\t\treturn 0;\n\t}\n\n\tjiffy_elapsed = jiffies - tg->slice_start[rw];\n\n\t \n\tjiffy_elapsed_rnd = roundup(jiffy_elapsed + 1, tg->td->throtl_slice);\n\tio_allowed = calculate_io_allowed(iops_limit, jiffy_elapsed_rnd) +\n\t\t     tg->carryover_ios[rw];\n\tif (io_allowed > 0 && tg->io_disp[rw] + 1 <= io_allowed)\n\t\treturn 0;\n\n\t \n\tjiffy_wait = jiffy_elapsed_rnd - jiffy_elapsed;\n\treturn jiffy_wait;\n}\n\nstatic unsigned long tg_within_bps_limit(struct throtl_grp *tg, struct bio *bio,\n\t\t\t\tu64 bps_limit)\n{\n\tbool rw = bio_data_dir(bio);\n\tlong long bytes_allowed;\n\tu64 extra_bytes;\n\tunsigned long jiffy_elapsed, jiffy_wait, jiffy_elapsed_rnd;\n\tunsigned int bio_size = throtl_bio_data_size(bio);\n\n\t \n\tif (bps_limit == U64_MAX || bio_flagged(bio, BIO_BPS_THROTTLED)) {\n\t\treturn 0;\n\t}\n\n\tjiffy_elapsed = jiffy_elapsed_rnd = jiffies - tg->slice_start[rw];\n\n\t \n\tif (!jiffy_elapsed)\n\t\tjiffy_elapsed_rnd = tg->td->throtl_slice;\n\n\tjiffy_elapsed_rnd = roundup(jiffy_elapsed_rnd, tg->td->throtl_slice);\n\tbytes_allowed = calculate_bytes_allowed(bps_limit, jiffy_elapsed_rnd) +\n\t\t\ttg->carryover_bytes[rw];\n\tif (bytes_allowed > 0 && tg->bytes_disp[rw] + bio_size <= bytes_allowed)\n\t\treturn 0;\n\n\t \n\textra_bytes = tg->bytes_disp[rw] + bio_size - bytes_allowed;\n\tjiffy_wait = div64_u64(extra_bytes * HZ, bps_limit);\n\n\tif (!jiffy_wait)\n\t\tjiffy_wait = 1;\n\n\t \n\tjiffy_wait = jiffy_wait + (jiffy_elapsed_rnd - jiffy_elapsed);\n\treturn jiffy_wait;\n}\n\n \nstatic bool tg_may_dispatch(struct throtl_grp *tg, struct bio *bio,\n\t\t\t    unsigned long *wait)\n{\n\tbool rw = bio_data_dir(bio);\n\tunsigned long bps_wait = 0, iops_wait = 0, max_wait = 0;\n\tu64 bps_limit = tg_bps_limit(tg, rw);\n\tu32 iops_limit = tg_iops_limit(tg, rw);\n\n\t \n\tBUG_ON(tg->service_queue.nr_queued[rw] &&\n\t       bio != throtl_peek_queued(&tg->service_queue.queued[rw]));\n\n\t \n\tif ((bps_limit == U64_MAX && iops_limit == UINT_MAX) ||\n\t    tg->flags & THROTL_TG_CANCELING) {\n\t\tif (wait)\n\t\t\t*wait = 0;\n\t\treturn true;\n\t}\n\n\t \n\tif (throtl_slice_used(tg, rw) && !(tg->service_queue.nr_queued[rw]))\n\t\tthrotl_start_new_slice(tg, rw, true);\n\telse {\n\t\tif (time_before(tg->slice_end[rw],\n\t\t    jiffies + tg->td->throtl_slice))\n\t\t\tthrotl_extend_slice(tg, rw,\n\t\t\t\tjiffies + tg->td->throtl_slice);\n\t}\n\n\tbps_wait = tg_within_bps_limit(tg, bio, bps_limit);\n\tiops_wait = tg_within_iops_limit(tg, bio, iops_limit);\n\tif (bps_wait + iops_wait == 0) {\n\t\tif (wait)\n\t\t\t*wait = 0;\n\t\treturn true;\n\t}\n\n\tmax_wait = max(bps_wait, iops_wait);\n\n\tif (wait)\n\t\t*wait = max_wait;\n\n\tif (time_before(tg->slice_end[rw], jiffies + max_wait))\n\t\tthrotl_extend_slice(tg, rw, jiffies + max_wait);\n\n\treturn false;\n}\n\nstatic void throtl_charge_bio(struct throtl_grp *tg, struct bio *bio)\n{\n\tbool rw = bio_data_dir(bio);\n\tunsigned int bio_size = throtl_bio_data_size(bio);\n\n\t \n\tif (!bio_flagged(bio, BIO_BPS_THROTTLED)) {\n\t\ttg->bytes_disp[rw] += bio_size;\n\t\ttg->last_bytes_disp[rw] += bio_size;\n\t}\n\n\ttg->io_disp[rw]++;\n\ttg->last_io_disp[rw]++;\n}\n\n \nstatic void throtl_add_bio_tg(struct bio *bio, struct throtl_qnode *qn,\n\t\t\t      struct throtl_grp *tg)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tbool rw = bio_data_dir(bio);\n\n\tif (!qn)\n\t\tqn = &tg->qnode_on_self[rw];\n\n\t \n\tif (!sq->nr_queued[rw])\n\t\ttg->flags |= THROTL_TG_WAS_EMPTY;\n\n\tthrotl_qnode_add_bio(bio, qn, &sq->queued[rw]);\n\n\tsq->nr_queued[rw]++;\n\tthrotl_enqueue_tg(tg);\n}\n\nstatic void tg_update_disptime(struct throtl_grp *tg)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tunsigned long read_wait = -1, write_wait = -1, min_wait = -1, disptime;\n\tstruct bio *bio;\n\n\tbio = throtl_peek_queued(&sq->queued[READ]);\n\tif (bio)\n\t\ttg_may_dispatch(tg, bio, &read_wait);\n\n\tbio = throtl_peek_queued(&sq->queued[WRITE]);\n\tif (bio)\n\t\ttg_may_dispatch(tg, bio, &write_wait);\n\n\tmin_wait = min(read_wait, write_wait);\n\tdisptime = jiffies + min_wait;\n\n\t \n\tthrotl_rb_erase(&tg->rb_node, tg->service_queue.parent_sq);\n\ttg->disptime = disptime;\n\ttg_service_queue_add(tg);\n\n\t \n\ttg->flags &= ~THROTL_TG_WAS_EMPTY;\n}\n\nstatic void start_parent_slice_with_credit(struct throtl_grp *child_tg,\n\t\t\t\t\tstruct throtl_grp *parent_tg, bool rw)\n{\n\tif (throtl_slice_used(parent_tg, rw)) {\n\t\tthrotl_start_new_slice_with_credit(parent_tg, rw,\n\t\t\t\tchild_tg->slice_start[rw]);\n\t}\n\n}\n\nstatic void tg_dispatch_one_bio(struct throtl_grp *tg, bool rw)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tstruct throtl_service_queue *parent_sq = sq->parent_sq;\n\tstruct throtl_grp *parent_tg = sq_to_tg(parent_sq);\n\tstruct throtl_grp *tg_to_put = NULL;\n\tstruct bio *bio;\n\n\t \n\tbio = throtl_pop_queued(&sq->queued[rw], &tg_to_put);\n\tsq->nr_queued[rw]--;\n\n\tthrotl_charge_bio(tg, bio);\n\n\t \n\tif (parent_tg) {\n\t\tthrotl_add_bio_tg(bio, &tg->qnode_on_parent[rw], parent_tg);\n\t\tstart_parent_slice_with_credit(tg, parent_tg, rw);\n\t} else {\n\t\tbio_set_flag(bio, BIO_BPS_THROTTLED);\n\t\tthrotl_qnode_add_bio(bio, &tg->qnode_on_parent[rw],\n\t\t\t\t     &parent_sq->queued[rw]);\n\t\tBUG_ON(tg->td->nr_queued[rw] <= 0);\n\t\ttg->td->nr_queued[rw]--;\n\t}\n\n\tthrotl_trim_slice(tg, rw);\n\n\tif (tg_to_put)\n\t\tblkg_put(tg_to_blkg(tg_to_put));\n}\n\nstatic int throtl_dispatch_tg(struct throtl_grp *tg)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tunsigned int nr_reads = 0, nr_writes = 0;\n\tunsigned int max_nr_reads = THROTL_GRP_QUANTUM * 3 / 4;\n\tunsigned int max_nr_writes = THROTL_GRP_QUANTUM - max_nr_reads;\n\tstruct bio *bio;\n\n\t \n\n\twhile ((bio = throtl_peek_queued(&sq->queued[READ])) &&\n\t       tg_may_dispatch(tg, bio, NULL)) {\n\n\t\ttg_dispatch_one_bio(tg, bio_data_dir(bio));\n\t\tnr_reads++;\n\n\t\tif (nr_reads >= max_nr_reads)\n\t\t\tbreak;\n\t}\n\n\twhile ((bio = throtl_peek_queued(&sq->queued[WRITE])) &&\n\t       tg_may_dispatch(tg, bio, NULL)) {\n\n\t\ttg_dispatch_one_bio(tg, bio_data_dir(bio));\n\t\tnr_writes++;\n\n\t\tif (nr_writes >= max_nr_writes)\n\t\t\tbreak;\n\t}\n\n\treturn nr_reads + nr_writes;\n}\n\nstatic int throtl_select_dispatch(struct throtl_service_queue *parent_sq)\n{\n\tunsigned int nr_disp = 0;\n\n\twhile (1) {\n\t\tstruct throtl_grp *tg;\n\t\tstruct throtl_service_queue *sq;\n\n\t\tif (!parent_sq->nr_pending)\n\t\t\tbreak;\n\n\t\ttg = throtl_rb_first(parent_sq);\n\t\tif (!tg)\n\t\t\tbreak;\n\n\t\tif (time_before(jiffies, tg->disptime))\n\t\t\tbreak;\n\n\t\tnr_disp += throtl_dispatch_tg(tg);\n\n\t\tsq = &tg->service_queue;\n\t\tif (sq->nr_queued[READ] || sq->nr_queued[WRITE])\n\t\t\ttg_update_disptime(tg);\n\t\telse\n\t\t\tthrotl_dequeue_tg(tg);\n\n\t\tif (nr_disp >= THROTL_QUANTUM)\n\t\t\tbreak;\n\t}\n\n\treturn nr_disp;\n}\n\nstatic bool throtl_can_upgrade(struct throtl_data *td,\n\tstruct throtl_grp *this_tg);\n \nstatic void throtl_pending_timer_fn(struct timer_list *t)\n{\n\tstruct throtl_service_queue *sq = from_timer(sq, t, pending_timer);\n\tstruct throtl_grp *tg = sq_to_tg(sq);\n\tstruct throtl_data *td = sq_to_td(sq);\n\tstruct throtl_service_queue *parent_sq;\n\tstruct request_queue *q;\n\tbool dispatched;\n\tint ret;\n\n\t \n\tif (tg)\n\t\tq = tg->pd.blkg->q;\n\telse\n\t\tq = td->queue;\n\n\tspin_lock_irq(&q->queue_lock);\n\n\tif (!q->root_blkg)\n\t\tgoto out_unlock;\n\n\tif (throtl_can_upgrade(td, NULL))\n\t\tthrotl_upgrade_state(td);\n\nagain:\n\tparent_sq = sq->parent_sq;\n\tdispatched = false;\n\n\twhile (true) {\n\t\tthrotl_log(sq, \"dispatch nr_queued=%u read=%u write=%u\",\n\t\t\t   sq->nr_queued[READ] + sq->nr_queued[WRITE],\n\t\t\t   sq->nr_queued[READ], sq->nr_queued[WRITE]);\n\n\t\tret = throtl_select_dispatch(sq);\n\t\tif (ret) {\n\t\t\tthrotl_log(sq, \"bios disp=%u\", ret);\n\t\t\tdispatched = true;\n\t\t}\n\n\t\tif (throtl_schedule_next_dispatch(sq, false))\n\t\t\tbreak;\n\n\t\t \n\t\tspin_unlock_irq(&q->queue_lock);\n\t\tcpu_relax();\n\t\tspin_lock_irq(&q->queue_lock);\n\t}\n\n\tif (!dispatched)\n\t\tgoto out_unlock;\n\n\tif (parent_sq) {\n\t\t \n\t\tif (tg->flags & THROTL_TG_WAS_EMPTY) {\n\t\t\ttg_update_disptime(tg);\n\t\t\tif (!throtl_schedule_next_dispatch(parent_sq, false)) {\n\t\t\t\t \n\t\t\t\tsq = parent_sq;\n\t\t\t\ttg = sq_to_tg(sq);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tqueue_work(kthrotld_workqueue, &td->dispatch_work);\n\t}\nout_unlock:\n\tspin_unlock_irq(&q->queue_lock);\n}\n\n \nstatic void blk_throtl_dispatch_work_fn(struct work_struct *work)\n{\n\tstruct throtl_data *td = container_of(work, struct throtl_data,\n\t\t\t\t\t      dispatch_work);\n\tstruct throtl_service_queue *td_sq = &td->service_queue;\n\tstruct request_queue *q = td->queue;\n\tstruct bio_list bio_list_on_stack;\n\tstruct bio *bio;\n\tstruct blk_plug plug;\n\tint rw;\n\n\tbio_list_init(&bio_list_on_stack);\n\n\tspin_lock_irq(&q->queue_lock);\n\tfor (rw = READ; rw <= WRITE; rw++)\n\t\twhile ((bio = throtl_pop_queued(&td_sq->queued[rw], NULL)))\n\t\t\tbio_list_add(&bio_list_on_stack, bio);\n\tspin_unlock_irq(&q->queue_lock);\n\n\tif (!bio_list_empty(&bio_list_on_stack)) {\n\t\tblk_start_plug(&plug);\n\t\twhile ((bio = bio_list_pop(&bio_list_on_stack)))\n\t\t\tsubmit_bio_noacct_nocheck(bio);\n\t\tblk_finish_plug(&plug);\n\t}\n}\n\nstatic u64 tg_prfill_conf_u64(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t      int off)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\tu64 v = *(u64 *)((void *)tg + off);\n\n\tif (v == U64_MAX)\n\t\treturn 0;\n\treturn __blkg_prfill_u64(sf, pd, v);\n}\n\nstatic u64 tg_prfill_conf_uint(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t       int off)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\tunsigned int v = *(unsigned int *)((void *)tg + off);\n\n\tif (v == UINT_MAX)\n\t\treturn 0;\n\treturn __blkg_prfill_u64(sf, pd, v);\n}\n\nstatic int tg_print_conf_u64(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_conf_u64,\n\t\t\t  &blkcg_policy_throtl, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic int tg_print_conf_uint(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_conf_uint,\n\t\t\t  &blkcg_policy_throtl, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic void tg_conf_updated(struct throtl_grp *tg, bool global)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkcg_gq *blkg;\n\n\tthrotl_log(&tg->service_queue,\n\t\t   \"limit change rbps=%llu wbps=%llu riops=%u wiops=%u\",\n\t\t   tg_bps_limit(tg, READ), tg_bps_limit(tg, WRITE),\n\t\t   tg_iops_limit(tg, READ), tg_iops_limit(tg, WRITE));\n\n\trcu_read_lock();\n\t \n\tblkg_for_each_descendant_pre(blkg, pos_css,\n\t\t\tglobal ? tg->td->queue->root_blkg : tg_to_blkg(tg)) {\n\t\tstruct throtl_grp *this_tg = blkg_to_tg(blkg);\n\t\tstruct throtl_grp *parent_tg;\n\n\t\ttg_update_has_rules(this_tg);\n\t\t \n\t\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys) || !blkg->parent ||\n\t\t    !blkg->parent->parent)\n\t\t\tcontinue;\n\t\tparent_tg = blkg_to_tg(blkg->parent);\n\t\t \n\t\tthis_tg->idletime_threshold = min(this_tg->idletime_threshold,\n\t\t\t\tparent_tg->idletime_threshold);\n\t\tthis_tg->latency_target = max(this_tg->latency_target,\n\t\t\t\tparent_tg->latency_target);\n\t}\n\trcu_read_unlock();\n\n\t \n\tthrotl_start_new_slice(tg, READ, false);\n\tthrotl_start_new_slice(tg, WRITE, false);\n\n\tif (tg->flags & THROTL_TG_PENDING) {\n\t\ttg_update_disptime(tg);\n\t\tthrotl_schedule_next_dispatch(sq->parent_sq, true);\n\t}\n}\n\nstatic ssize_t tg_set_conf(struct kernfs_open_file *of,\n\t\t\t   char *buf, size_t nbytes, loff_t off, bool is_u64)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(of_css(of));\n\tstruct blkg_conf_ctx ctx;\n\tstruct throtl_grp *tg;\n\tint ret;\n\tu64 v;\n\n\tblkg_conf_init(&ctx, buf);\n\n\tret = blkg_conf_prep(blkcg, &blkcg_policy_throtl, &ctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\tret = -EINVAL;\n\tif (sscanf(ctx.body, \"%llu\", &v) != 1)\n\t\tgoto out_finish;\n\tif (!v)\n\t\tv = U64_MAX;\n\n\ttg = blkg_to_tg(ctx.blkg);\n\ttg_update_carryover(tg);\n\n\tif (is_u64)\n\t\t*(u64 *)((void *)tg + of_cft(of)->private) = v;\n\telse\n\t\t*(unsigned int *)((void *)tg + of_cft(of)->private) = v;\n\n\ttg_conf_updated(tg, false);\n\tret = 0;\nout_finish:\n\tblkg_conf_exit(&ctx);\n\treturn ret ?: nbytes;\n}\n\nstatic ssize_t tg_set_conf_u64(struct kernfs_open_file *of,\n\t\t\t       char *buf, size_t nbytes, loff_t off)\n{\n\treturn tg_set_conf(of, buf, nbytes, off, true);\n}\n\nstatic ssize_t tg_set_conf_uint(struct kernfs_open_file *of,\n\t\t\t\tchar *buf, size_t nbytes, loff_t off)\n{\n\treturn tg_set_conf(of, buf, nbytes, off, false);\n}\n\nstatic int tg_print_rwstat(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  blkg_prfill_rwstat, &blkcg_policy_throtl,\n\t\t\t  seq_cft(sf)->private, true);\n\treturn 0;\n}\n\nstatic u64 tg_prfill_rwstat_recursive(struct seq_file *sf,\n\t\t\t\t      struct blkg_policy_data *pd, int off)\n{\n\tstruct blkg_rwstat_sample sum;\n\n\tblkg_rwstat_recursive_sum(pd_to_blkg(pd), &blkcg_policy_throtl, off,\n\t\t\t\t  &sum);\n\treturn __blkg_prfill_rwstat(sf, pd, &sum);\n}\n\nstatic int tg_print_rwstat_recursive(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)),\n\t\t\t  tg_prfill_rwstat_recursive, &blkcg_policy_throtl,\n\t\t\t  seq_cft(sf)->private, true);\n\treturn 0;\n}\n\nstatic struct cftype throtl_legacy_files[] = {\n\t{\n\t\t.name = \"throttle.read_bps_device\",\n\t\t.private = offsetof(struct throtl_grp, bps[READ][LIMIT_MAX]),\n\t\t.seq_show = tg_print_conf_u64,\n\t\t.write = tg_set_conf_u64,\n\t},\n\t{\n\t\t.name = \"throttle.write_bps_device\",\n\t\t.private = offsetof(struct throtl_grp, bps[WRITE][LIMIT_MAX]),\n\t\t.seq_show = tg_print_conf_u64,\n\t\t.write = tg_set_conf_u64,\n\t},\n\t{\n\t\t.name = \"throttle.read_iops_device\",\n\t\t.private = offsetof(struct throtl_grp, iops[READ][LIMIT_MAX]),\n\t\t.seq_show = tg_print_conf_uint,\n\t\t.write = tg_set_conf_uint,\n\t},\n\t{\n\t\t.name = \"throttle.write_iops_device\",\n\t\t.private = offsetof(struct throtl_grp, iops[WRITE][LIMIT_MAX]),\n\t\t.seq_show = tg_print_conf_uint,\n\t\t.write = tg_set_conf_uint,\n\t},\n\t{\n\t\t.name = \"throttle.io_service_bytes\",\n\t\t.private = offsetof(struct throtl_grp, stat_bytes),\n\t\t.seq_show = tg_print_rwstat,\n\t},\n\t{\n\t\t.name = \"throttle.io_service_bytes_recursive\",\n\t\t.private = offsetof(struct throtl_grp, stat_bytes),\n\t\t.seq_show = tg_print_rwstat_recursive,\n\t},\n\t{\n\t\t.name = \"throttle.io_serviced\",\n\t\t.private = offsetof(struct throtl_grp, stat_ios),\n\t\t.seq_show = tg_print_rwstat,\n\t},\n\t{\n\t\t.name = \"throttle.io_serviced_recursive\",\n\t\t.private = offsetof(struct throtl_grp, stat_ios),\n\t\t.seq_show = tg_print_rwstat_recursive,\n\t},\n\t{ }\t \n};\n\nstatic u64 tg_prfill_limit(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t int off)\n{\n\tstruct throtl_grp *tg = pd_to_tg(pd);\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tchar bufs[4][21] = { \"max\", \"max\", \"max\", \"max\" };\n\tu64 bps_dft;\n\tunsigned int iops_dft;\n\tchar idle_time[26] = \"\";\n\tchar latency_time[26] = \"\";\n\n\tif (!dname)\n\t\treturn 0;\n\n\tif (off == LIMIT_LOW) {\n\t\tbps_dft = 0;\n\t\tiops_dft = 0;\n\t} else {\n\t\tbps_dft = U64_MAX;\n\t\tiops_dft = UINT_MAX;\n\t}\n\n\tif (tg->bps_conf[READ][off] == bps_dft &&\n\t    tg->bps_conf[WRITE][off] == bps_dft &&\n\t    tg->iops_conf[READ][off] == iops_dft &&\n\t    tg->iops_conf[WRITE][off] == iops_dft &&\n\t    (off != LIMIT_LOW ||\n\t     (tg->idletime_threshold_conf == DFL_IDLE_THRESHOLD &&\n\t      tg->latency_target_conf == DFL_LATENCY_TARGET)))\n\t\treturn 0;\n\n\tif (tg->bps_conf[READ][off] != U64_MAX)\n\t\tsnprintf(bufs[0], sizeof(bufs[0]), \"%llu\",\n\t\t\ttg->bps_conf[READ][off]);\n\tif (tg->bps_conf[WRITE][off] != U64_MAX)\n\t\tsnprintf(bufs[1], sizeof(bufs[1]), \"%llu\",\n\t\t\ttg->bps_conf[WRITE][off]);\n\tif (tg->iops_conf[READ][off] != UINT_MAX)\n\t\tsnprintf(bufs[2], sizeof(bufs[2]), \"%u\",\n\t\t\ttg->iops_conf[READ][off]);\n\tif (tg->iops_conf[WRITE][off] != UINT_MAX)\n\t\tsnprintf(bufs[3], sizeof(bufs[3]), \"%u\",\n\t\t\ttg->iops_conf[WRITE][off]);\n\tif (off == LIMIT_LOW) {\n\t\tif (tg->idletime_threshold_conf == ULONG_MAX)\n\t\t\tstrcpy(idle_time, \" idle=max\");\n\t\telse\n\t\t\tsnprintf(idle_time, sizeof(idle_time), \" idle=%lu\",\n\t\t\t\ttg->idletime_threshold_conf);\n\n\t\tif (tg->latency_target_conf == ULONG_MAX)\n\t\t\tstrcpy(latency_time, \" latency=max\");\n\t\telse\n\t\t\tsnprintf(latency_time, sizeof(latency_time),\n\t\t\t\t\" latency=%lu\", tg->latency_target_conf);\n\t}\n\n\tseq_printf(sf, \"%s rbps=%s wbps=%s riops=%s wiops=%s%s%s\\n\",\n\t\t   dname, bufs[0], bufs[1], bufs[2], bufs[3], idle_time,\n\t\t   latency_time);\n\treturn 0;\n}\n\nstatic int tg_print_limit(struct seq_file *sf, void *v)\n{\n\tblkcg_print_blkgs(sf, css_to_blkcg(seq_css(sf)), tg_prfill_limit,\n\t\t\t  &blkcg_policy_throtl, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic ssize_t tg_set_limit(struct kernfs_open_file *of,\n\t\t\t  char *buf, size_t nbytes, loff_t off)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(of_css(of));\n\tstruct blkg_conf_ctx ctx;\n\tstruct throtl_grp *tg;\n\tu64 v[4];\n\tunsigned long idle_time;\n\tunsigned long latency_time;\n\tint ret;\n\tint index = of_cft(of)->private;\n\n\tblkg_conf_init(&ctx, buf);\n\n\tret = blkg_conf_prep(blkcg, &blkcg_policy_throtl, &ctx);\n\tif (ret)\n\t\tgoto out_finish;\n\n\ttg = blkg_to_tg(ctx.blkg);\n\ttg_update_carryover(tg);\n\n\tv[0] = tg->bps_conf[READ][index];\n\tv[1] = tg->bps_conf[WRITE][index];\n\tv[2] = tg->iops_conf[READ][index];\n\tv[3] = tg->iops_conf[WRITE][index];\n\n\tidle_time = tg->idletime_threshold_conf;\n\tlatency_time = tg->latency_target_conf;\n\twhile (true) {\n\t\tchar tok[27];\t \n\t\tchar *p;\n\t\tu64 val = U64_MAX;\n\t\tint len;\n\n\t\tif (sscanf(ctx.body, \"%26s%n\", tok, &len) != 1)\n\t\t\tbreak;\n\t\tif (tok[0] == '\\0')\n\t\t\tbreak;\n\t\tctx.body += len;\n\n\t\tret = -EINVAL;\n\t\tp = tok;\n\t\tstrsep(&p, \"=\");\n\t\tif (!p || (sscanf(p, \"%llu\", &val) != 1 && strcmp(p, \"max\")))\n\t\t\tgoto out_finish;\n\n\t\tret = -ERANGE;\n\t\tif (!val)\n\t\t\tgoto out_finish;\n\n\t\tret = -EINVAL;\n\t\tif (!strcmp(tok, \"rbps\") && val > 1)\n\t\t\tv[0] = val;\n\t\telse if (!strcmp(tok, \"wbps\") && val > 1)\n\t\t\tv[1] = val;\n\t\telse if (!strcmp(tok, \"riops\") && val > 1)\n\t\t\tv[2] = min_t(u64, val, UINT_MAX);\n\t\telse if (!strcmp(tok, \"wiops\") && val > 1)\n\t\t\tv[3] = min_t(u64, val, UINT_MAX);\n\t\telse if (off == LIMIT_LOW && !strcmp(tok, \"idle\"))\n\t\t\tidle_time = val;\n\t\telse if (off == LIMIT_LOW && !strcmp(tok, \"latency\"))\n\t\t\tlatency_time = val;\n\t\telse\n\t\t\tgoto out_finish;\n\t}\n\n\ttg->bps_conf[READ][index] = v[0];\n\ttg->bps_conf[WRITE][index] = v[1];\n\ttg->iops_conf[READ][index] = v[2];\n\ttg->iops_conf[WRITE][index] = v[3];\n\n\tif (index == LIMIT_MAX) {\n\t\ttg->bps[READ][index] = v[0];\n\t\ttg->bps[WRITE][index] = v[1];\n\t\ttg->iops[READ][index] = v[2];\n\t\ttg->iops[WRITE][index] = v[3];\n\t}\n\ttg->bps[READ][LIMIT_LOW] = min(tg->bps_conf[READ][LIMIT_LOW],\n\t\ttg->bps_conf[READ][LIMIT_MAX]);\n\ttg->bps[WRITE][LIMIT_LOW] = min(tg->bps_conf[WRITE][LIMIT_LOW],\n\t\ttg->bps_conf[WRITE][LIMIT_MAX]);\n\ttg->iops[READ][LIMIT_LOW] = min(tg->iops_conf[READ][LIMIT_LOW],\n\t\ttg->iops_conf[READ][LIMIT_MAX]);\n\ttg->iops[WRITE][LIMIT_LOW] = min(tg->iops_conf[WRITE][LIMIT_LOW],\n\t\ttg->iops_conf[WRITE][LIMIT_MAX]);\n\ttg->idletime_threshold_conf = idle_time;\n\ttg->latency_target_conf = latency_time;\n\n\t \n\tif (!(tg->bps[READ][LIMIT_LOW] || tg->iops[READ][LIMIT_LOW] ||\n\t      tg->bps[WRITE][LIMIT_LOW] || tg->iops[WRITE][LIMIT_LOW]) ||\n\t    tg->idletime_threshold_conf == DFL_IDLE_THRESHOLD ||\n\t    tg->latency_target_conf == DFL_LATENCY_TARGET) {\n\t\ttg->bps[READ][LIMIT_LOW] = 0;\n\t\ttg->bps[WRITE][LIMIT_LOW] = 0;\n\t\ttg->iops[READ][LIMIT_LOW] = 0;\n\t\ttg->iops[WRITE][LIMIT_LOW] = 0;\n\t\ttg->idletime_threshold = DFL_IDLE_THRESHOLD;\n\t\ttg->latency_target = DFL_LATENCY_TARGET;\n\t} else if (index == LIMIT_LOW) {\n\t\ttg->idletime_threshold = tg->idletime_threshold_conf;\n\t\ttg->latency_target = tg->latency_target_conf;\n\t}\n\n\tblk_throtl_update_limit_valid(tg->td);\n\tif (tg->td->limit_valid[LIMIT_LOW]) {\n\t\tif (index == LIMIT_LOW)\n\t\t\ttg->td->limit_index = LIMIT_LOW;\n\t} else\n\t\ttg->td->limit_index = LIMIT_MAX;\n\ttg_conf_updated(tg, index == LIMIT_LOW &&\n\t\ttg->td->limit_valid[LIMIT_LOW]);\n\tret = 0;\nout_finish:\n\tblkg_conf_exit(&ctx);\n\treturn ret ?: nbytes;\n}\n\nstatic struct cftype throtl_files[] = {\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\n\t{\n\t\t.name = \"low\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = tg_print_limit,\n\t\t.write = tg_set_limit,\n\t\t.private = LIMIT_LOW,\n\t},\n#endif\n\t{\n\t\t.name = \"max\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = tg_print_limit,\n\t\t.write = tg_set_limit,\n\t\t.private = LIMIT_MAX,\n\t},\n\t{ }\t \n};\n\nstatic void throtl_shutdown_wq(struct request_queue *q)\n{\n\tstruct throtl_data *td = q->td;\n\n\tcancel_work_sync(&td->dispatch_work);\n}\n\nstruct blkcg_policy blkcg_policy_throtl = {\n\t.dfl_cftypes\t\t= throtl_files,\n\t.legacy_cftypes\t\t= throtl_legacy_files,\n\n\t.pd_alloc_fn\t\t= throtl_pd_alloc,\n\t.pd_init_fn\t\t= throtl_pd_init,\n\t.pd_online_fn\t\t= throtl_pd_online,\n\t.pd_offline_fn\t\t= throtl_pd_offline,\n\t.pd_free_fn\t\t= throtl_pd_free,\n};\n\nvoid blk_throtl_cancel_bios(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkcg_gq *blkg;\n\n\tspin_lock_irq(&q->queue_lock);\n\t \n\trcu_read_lock();\n\tblkg_for_each_descendant_post(blkg, pos_css, q->root_blkg) {\n\t\tstruct throtl_grp *tg = blkg_to_tg(blkg);\n\t\tstruct throtl_service_queue *sq = &tg->service_queue;\n\n\t\t \n\t\ttg->flags |= THROTL_TG_CANCELING;\n\n\t\t \n\t\tif (!(tg->flags & THROTL_TG_PENDING))\n\t\t\tcontinue;\n\n\t\t \n\t\ttg_update_disptime(tg);\n\n\t\tthrotl_schedule_pending_timer(sq, jiffies + 1);\n\t}\n\trcu_read_unlock();\n\tspin_unlock_irq(&q->queue_lock);\n}\n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nstatic unsigned long __tg_last_low_overflow_time(struct throtl_grp *tg)\n{\n\tunsigned long rtime = jiffies, wtime = jiffies;\n\n\tif (tg->bps[READ][LIMIT_LOW] || tg->iops[READ][LIMIT_LOW])\n\t\trtime = tg->last_low_overflow_time[READ];\n\tif (tg->bps[WRITE][LIMIT_LOW] || tg->iops[WRITE][LIMIT_LOW])\n\t\twtime = tg->last_low_overflow_time[WRITE];\n\treturn min(rtime, wtime);\n}\n\nstatic unsigned long tg_last_low_overflow_time(struct throtl_grp *tg)\n{\n\tstruct throtl_service_queue *parent_sq;\n\tstruct throtl_grp *parent = tg;\n\tunsigned long ret = __tg_last_low_overflow_time(tg);\n\n\twhile (true) {\n\t\tparent_sq = parent->service_queue.parent_sq;\n\t\tparent = sq_to_tg(parent_sq);\n\t\tif (!parent)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!parent->bps[READ][LIMIT_LOW] &&\n\t\t    !parent->iops[READ][LIMIT_LOW] &&\n\t\t    !parent->bps[WRITE][LIMIT_LOW] &&\n\t\t    !parent->iops[WRITE][LIMIT_LOW])\n\t\t\tcontinue;\n\t\tif (time_after(__tg_last_low_overflow_time(parent), ret))\n\t\t\tret = __tg_last_low_overflow_time(parent);\n\t}\n\treturn ret;\n}\n\nstatic bool throtl_tg_is_idle(struct throtl_grp *tg)\n{\n\t \n\tunsigned long time;\n\tbool ret;\n\n\ttime = min_t(unsigned long, MAX_IDLE_TIME, 4 * tg->idletime_threshold);\n\tret = tg->latency_target == DFL_LATENCY_TARGET ||\n\t      tg->idletime_threshold == DFL_IDLE_THRESHOLD ||\n\t      (ktime_get_ns() >> 10) - tg->last_finish_time > time ||\n\t      tg->avg_idletime > tg->idletime_threshold ||\n\t      (tg->latency_target && tg->bio_cnt &&\n\t\ttg->bad_bio_cnt * 5 < tg->bio_cnt);\n\tthrotl_log(&tg->service_queue,\n\t\t\"avg_idle=%ld, idle_threshold=%ld, bad_bio=%d, total_bio=%d, is_idle=%d, scale=%d\",\n\t\ttg->avg_idletime, tg->idletime_threshold, tg->bad_bio_cnt,\n\t\ttg->bio_cnt, ret, tg->td->scale);\n\treturn ret;\n}\n\nstatic bool throtl_low_limit_reached(struct throtl_grp *tg, int rw)\n{\n\tstruct throtl_service_queue *sq = &tg->service_queue;\n\tbool limit = tg->bps[rw][LIMIT_LOW] || tg->iops[rw][LIMIT_LOW];\n\n\t \n\treturn !limit || sq->nr_queued[rw];\n}\n\nstatic bool throtl_tg_can_upgrade(struct throtl_grp *tg)\n{\n\t \n\tif (throtl_low_limit_reached(tg, READ) &&\n\t    throtl_low_limit_reached(tg, WRITE))\n\t\treturn true;\n\n\tif (time_after_eq(jiffies,\n\t\ttg_last_low_overflow_time(tg) + tg->td->throtl_slice) &&\n\t    throtl_tg_is_idle(tg))\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool throtl_hierarchy_can_upgrade(struct throtl_grp *tg)\n{\n\twhile (true) {\n\t\tif (throtl_tg_can_upgrade(tg))\n\t\t\treturn true;\n\t\ttg = sq_to_tg(tg->service_queue.parent_sq);\n\t\tif (!tg || !tg_to_blkg(tg)->parent)\n\t\t\treturn false;\n\t}\n\treturn false;\n}\n\nstatic bool throtl_can_upgrade(struct throtl_data *td,\n\tstruct throtl_grp *this_tg)\n{\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkcg_gq *blkg;\n\n\tif (td->limit_index != LIMIT_LOW)\n\t\treturn false;\n\n\tif (time_before(jiffies, td->low_downgrade_time + td->throtl_slice))\n\t\treturn false;\n\n\trcu_read_lock();\n\tblkg_for_each_descendant_post(blkg, pos_css, td->queue->root_blkg) {\n\t\tstruct throtl_grp *tg = blkg_to_tg(blkg);\n\n\t\tif (tg == this_tg)\n\t\t\tcontinue;\n\t\tif (!list_empty(&tg_to_blkg(tg)->blkcg->css.children))\n\t\t\tcontinue;\n\t\tif (!throtl_hierarchy_can_upgrade(tg)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn true;\n}\n\nstatic void throtl_upgrade_check(struct throtl_grp *tg)\n{\n\tunsigned long now = jiffies;\n\n\tif (tg->td->limit_index != LIMIT_LOW)\n\t\treturn;\n\n\tif (time_after(tg->last_check_time + tg->td->throtl_slice, now))\n\t\treturn;\n\n\ttg->last_check_time = now;\n\n\tif (!time_after_eq(now,\n\t     __tg_last_low_overflow_time(tg) + tg->td->throtl_slice))\n\t\treturn;\n\n\tif (throtl_can_upgrade(tg->td, NULL))\n\t\tthrotl_upgrade_state(tg->td);\n}\n\nstatic void throtl_upgrade_state(struct throtl_data *td)\n{\n\tstruct cgroup_subsys_state *pos_css;\n\tstruct blkcg_gq *blkg;\n\n\tthrotl_log(&td->service_queue, \"upgrade to max\");\n\ttd->limit_index = LIMIT_MAX;\n\ttd->low_upgrade_time = jiffies;\n\ttd->scale = 0;\n\trcu_read_lock();\n\tblkg_for_each_descendant_post(blkg, pos_css, td->queue->root_blkg) {\n\t\tstruct throtl_grp *tg = blkg_to_tg(blkg);\n\t\tstruct throtl_service_queue *sq = &tg->service_queue;\n\n\t\ttg->disptime = jiffies - 1;\n\t\tthrotl_select_dispatch(sq);\n\t\tthrotl_schedule_next_dispatch(sq, true);\n\t}\n\trcu_read_unlock();\n\tthrotl_select_dispatch(&td->service_queue);\n\tthrotl_schedule_next_dispatch(&td->service_queue, true);\n\tqueue_work(kthrotld_workqueue, &td->dispatch_work);\n}\n\nstatic void throtl_downgrade_state(struct throtl_data *td)\n{\n\ttd->scale /= 2;\n\n\tthrotl_log(&td->service_queue, \"downgrade, scale %d\", td->scale);\n\tif (td->scale) {\n\t\ttd->low_upgrade_time = jiffies - td->scale * td->throtl_slice;\n\t\treturn;\n\t}\n\n\ttd->limit_index = LIMIT_LOW;\n\ttd->low_downgrade_time = jiffies;\n}\n\nstatic bool throtl_tg_can_downgrade(struct throtl_grp *tg)\n{\n\tstruct throtl_data *td = tg->td;\n\tunsigned long now = jiffies;\n\n\t \n\tif (time_after_eq(now, tg_last_low_overflow_time(tg) +\n\t\t\t\t\ttd->throtl_slice) &&\n\t    (!throtl_tg_is_idle(tg) ||\n\t     !list_empty(&tg_to_blkg(tg)->blkcg->css.children)))\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool throtl_hierarchy_can_downgrade(struct throtl_grp *tg)\n{\n\tstruct throtl_data *td = tg->td;\n\n\tif (time_before(jiffies, td->low_upgrade_time + td->throtl_slice))\n\t\treturn false;\n\n\twhile (true) {\n\t\tif (!throtl_tg_can_downgrade(tg))\n\t\t\treturn false;\n\t\ttg = sq_to_tg(tg->service_queue.parent_sq);\n\t\tif (!tg || !tg_to_blkg(tg)->parent)\n\t\t\tbreak;\n\t}\n\treturn true;\n}\n\nstatic void throtl_downgrade_check(struct throtl_grp *tg)\n{\n\tuint64_t bps;\n\tunsigned int iops;\n\tunsigned long elapsed_time;\n\tunsigned long now = jiffies;\n\n\tif (tg->td->limit_index != LIMIT_MAX ||\n\t    !tg->td->limit_valid[LIMIT_LOW])\n\t\treturn;\n\tif (!list_empty(&tg_to_blkg(tg)->blkcg->css.children))\n\t\treturn;\n\tif (time_after(tg->last_check_time + tg->td->throtl_slice, now))\n\t\treturn;\n\n\telapsed_time = now - tg->last_check_time;\n\ttg->last_check_time = now;\n\n\tif (time_before(now, tg_last_low_overflow_time(tg) +\n\t\t\ttg->td->throtl_slice))\n\t\treturn;\n\n\tif (tg->bps[READ][LIMIT_LOW]) {\n\t\tbps = tg->last_bytes_disp[READ] * HZ;\n\t\tdo_div(bps, elapsed_time);\n\t\tif (bps >= tg->bps[READ][LIMIT_LOW])\n\t\t\ttg->last_low_overflow_time[READ] = now;\n\t}\n\n\tif (tg->bps[WRITE][LIMIT_LOW]) {\n\t\tbps = tg->last_bytes_disp[WRITE] * HZ;\n\t\tdo_div(bps, elapsed_time);\n\t\tif (bps >= tg->bps[WRITE][LIMIT_LOW])\n\t\t\ttg->last_low_overflow_time[WRITE] = now;\n\t}\n\n\tif (tg->iops[READ][LIMIT_LOW]) {\n\t\tiops = tg->last_io_disp[READ] * HZ / elapsed_time;\n\t\tif (iops >= tg->iops[READ][LIMIT_LOW])\n\t\t\ttg->last_low_overflow_time[READ] = now;\n\t}\n\n\tif (tg->iops[WRITE][LIMIT_LOW]) {\n\t\tiops = tg->last_io_disp[WRITE] * HZ / elapsed_time;\n\t\tif (iops >= tg->iops[WRITE][LIMIT_LOW])\n\t\t\ttg->last_low_overflow_time[WRITE] = now;\n\t}\n\n\t \n\tif (throtl_hierarchy_can_downgrade(tg))\n\t\tthrotl_downgrade_state(tg->td);\n\n\ttg->last_bytes_disp[READ] = 0;\n\ttg->last_bytes_disp[WRITE] = 0;\n\ttg->last_io_disp[READ] = 0;\n\ttg->last_io_disp[WRITE] = 0;\n}\n\nstatic void blk_throtl_update_idletime(struct throtl_grp *tg)\n{\n\tunsigned long now;\n\tunsigned long last_finish_time = tg->last_finish_time;\n\n\tif (last_finish_time == 0)\n\t\treturn;\n\n\tnow = ktime_get_ns() >> 10;\n\tif (now <= last_finish_time ||\n\t    last_finish_time == tg->checked_last_finish_time)\n\t\treturn;\n\n\ttg->avg_idletime = (tg->avg_idletime * 7 + now - last_finish_time) >> 3;\n\ttg->checked_last_finish_time = last_finish_time;\n}\n\nstatic void throtl_update_latency_buckets(struct throtl_data *td)\n{\n\tstruct avg_latency_bucket avg_latency[2][LATENCY_BUCKET_SIZE];\n\tint i, cpu, rw;\n\tunsigned long last_latency[2] = { 0 };\n\tunsigned long latency[2];\n\n\tif (!blk_queue_nonrot(td->queue) || !td->limit_valid[LIMIT_LOW])\n\t\treturn;\n\tif (time_before(jiffies, td->last_calculate_time + HZ))\n\t\treturn;\n\ttd->last_calculate_time = jiffies;\n\n\tmemset(avg_latency, 0, sizeof(avg_latency));\n\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\tfor (i = 0; i < LATENCY_BUCKET_SIZE; i++) {\n\t\t\tstruct latency_bucket *tmp = &td->tmp_buckets[rw][i];\n\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tstruct latency_bucket *bucket;\n\n\t\t\t\t \n\t\t\t\tbucket = per_cpu_ptr(td->latency_buckets[rw],\n\t\t\t\t\tcpu);\n\t\t\t\ttmp->total_latency += bucket[i].total_latency;\n\t\t\t\ttmp->samples += bucket[i].samples;\n\t\t\t\tbucket[i].total_latency = 0;\n\t\t\t\tbucket[i].samples = 0;\n\t\t\t}\n\n\t\t\tif (tmp->samples >= 32) {\n\t\t\t\tint samples = tmp->samples;\n\n\t\t\t\tlatency[rw] = tmp->total_latency;\n\n\t\t\t\ttmp->total_latency = 0;\n\t\t\t\ttmp->samples = 0;\n\t\t\t\tlatency[rw] /= samples;\n\t\t\t\tif (latency[rw] == 0)\n\t\t\t\t\tcontinue;\n\t\t\t\tavg_latency[rw][i].latency = latency[rw];\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\tfor (i = 0; i < LATENCY_BUCKET_SIZE; i++) {\n\t\t\tif (!avg_latency[rw][i].latency) {\n\t\t\t\tif (td->avg_buckets[rw][i].latency < last_latency[rw])\n\t\t\t\t\ttd->avg_buckets[rw][i].latency =\n\t\t\t\t\t\tlast_latency[rw];\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!td->avg_buckets[rw][i].valid)\n\t\t\t\tlatency[rw] = avg_latency[rw][i].latency;\n\t\t\telse\n\t\t\t\tlatency[rw] = (td->avg_buckets[rw][i].latency * 7 +\n\t\t\t\t\tavg_latency[rw][i].latency) >> 3;\n\n\t\t\ttd->avg_buckets[rw][i].latency = max(latency[rw],\n\t\t\t\tlast_latency[rw]);\n\t\t\ttd->avg_buckets[rw][i].valid = true;\n\t\t\tlast_latency[rw] = td->avg_buckets[rw][i].latency;\n\t\t}\n\t}\n\n\tfor (i = 0; i < LATENCY_BUCKET_SIZE; i++)\n\t\tthrotl_log(&td->service_queue,\n\t\t\t\"Latency bucket %d: read latency=%ld, read valid=%d, \"\n\t\t\t\"write latency=%ld, write valid=%d\", i,\n\t\t\ttd->avg_buckets[READ][i].latency,\n\t\t\ttd->avg_buckets[READ][i].valid,\n\t\t\ttd->avg_buckets[WRITE][i].latency,\n\t\t\ttd->avg_buckets[WRITE][i].valid);\n}\n#else\nstatic inline void throtl_update_latency_buckets(struct throtl_data *td)\n{\n}\n\nstatic void blk_throtl_update_idletime(struct throtl_grp *tg)\n{\n}\n\nstatic void throtl_downgrade_check(struct throtl_grp *tg)\n{\n}\n\nstatic void throtl_upgrade_check(struct throtl_grp *tg)\n{\n}\n\nstatic bool throtl_can_upgrade(struct throtl_data *td,\n\tstruct throtl_grp *this_tg)\n{\n\treturn false;\n}\n\nstatic void throtl_upgrade_state(struct throtl_data *td)\n{\n}\n#endif\n\nbool __blk_throtl_bio(struct bio *bio)\n{\n\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\tstruct blkcg_gq *blkg = bio->bi_blkg;\n\tstruct throtl_qnode *qn = NULL;\n\tstruct throtl_grp *tg = blkg_to_tg(blkg);\n\tstruct throtl_service_queue *sq;\n\tbool rw = bio_data_dir(bio);\n\tbool throttled = false;\n\tstruct throtl_data *td = tg->td;\n\n\trcu_read_lock();\n\n\tspin_lock_irq(&q->queue_lock);\n\n\tthrotl_update_latency_buckets(td);\n\n\tblk_throtl_update_idletime(tg);\n\n\tsq = &tg->service_queue;\n\nagain:\n\twhile (true) {\n\t\tif (tg->last_low_overflow_time[rw] == 0)\n\t\t\ttg->last_low_overflow_time[rw] = jiffies;\n\t\tthrotl_downgrade_check(tg);\n\t\tthrotl_upgrade_check(tg);\n\t\t \n\t\tif (sq->nr_queued[rw])\n\t\t\tbreak;\n\n\t\t \n\t\tif (!tg_may_dispatch(tg, bio, NULL)) {\n\t\t\ttg->last_low_overflow_time[rw] = jiffies;\n\t\t\tif (throtl_can_upgrade(td, tg)) {\n\t\t\t\tthrotl_upgrade_state(td);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tthrotl_charge_bio(tg, bio);\n\n\t\t \n\t\tthrotl_trim_slice(tg, rw);\n\n\t\t \n\t\tqn = &tg->qnode_on_parent[rw];\n\t\tsq = sq->parent_sq;\n\t\ttg = sq_to_tg(sq);\n\t\tif (!tg) {\n\t\t\tbio_set_flag(bio, BIO_BPS_THROTTLED);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t \n\tthrotl_log(sq, \"[%c] bio. bdisp=%llu sz=%u bps=%llu iodisp=%u iops=%u queued=%d/%d\",\n\t\t   rw == READ ? 'R' : 'W',\n\t\t   tg->bytes_disp[rw], bio->bi_iter.bi_size,\n\t\t   tg_bps_limit(tg, rw),\n\t\t   tg->io_disp[rw], tg_iops_limit(tg, rw),\n\t\t   sq->nr_queued[READ], sq->nr_queued[WRITE]);\n\n\ttg->last_low_overflow_time[rw] = jiffies;\n\n\ttd->nr_queued[rw]++;\n\tthrotl_add_bio_tg(bio, qn, tg);\n\tthrottled = true;\n\n\t \n\tif (tg->flags & THROTL_TG_WAS_EMPTY) {\n\t\ttg_update_disptime(tg);\n\t\tthrotl_schedule_next_dispatch(tg->service_queue.parent_sq, true);\n\t}\n\nout_unlock:\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\n\tif (throttled || !td->track_bio_latency)\n\t\tbio->bi_issue.value |= BIO_ISSUE_THROTL_SKIP_LATENCY;\n#endif\n\tspin_unlock_irq(&q->queue_lock);\n\n\trcu_read_unlock();\n\treturn throttled;\n}\n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nstatic void throtl_track_latency(struct throtl_data *td, sector_t size,\n\t\t\t\t enum req_op op, unsigned long time)\n{\n\tconst bool rw = op_is_write(op);\n\tstruct latency_bucket *latency;\n\tint index;\n\n\tif (!td || td->limit_index != LIMIT_LOW ||\n\t    !(op == REQ_OP_READ || op == REQ_OP_WRITE) ||\n\t    !blk_queue_nonrot(td->queue))\n\t\treturn;\n\n\tindex = request_bucket_index(size);\n\n\tlatency = get_cpu_ptr(td->latency_buckets[rw]);\n\tlatency[index].total_latency += time;\n\tlatency[index].samples++;\n\tput_cpu_ptr(td->latency_buckets[rw]);\n}\n\nvoid blk_throtl_stat_add(struct request *rq, u64 time_ns)\n{\n\tstruct request_queue *q = rq->q;\n\tstruct throtl_data *td = q->td;\n\n\tthrotl_track_latency(td, blk_rq_stats_sectors(rq), req_op(rq),\n\t\t\t     time_ns >> 10);\n}\n\nvoid blk_throtl_bio_endio(struct bio *bio)\n{\n\tstruct blkcg_gq *blkg;\n\tstruct throtl_grp *tg;\n\tu64 finish_time_ns;\n\tunsigned long finish_time;\n\tunsigned long start_time;\n\tunsigned long lat;\n\tint rw = bio_data_dir(bio);\n\n\tblkg = bio->bi_blkg;\n\tif (!blkg)\n\t\treturn;\n\ttg = blkg_to_tg(blkg);\n\tif (!tg->td->limit_valid[LIMIT_LOW])\n\t\treturn;\n\n\tfinish_time_ns = ktime_get_ns();\n\ttg->last_finish_time = finish_time_ns >> 10;\n\n\tstart_time = bio_issue_time(&bio->bi_issue) >> 10;\n\tfinish_time = __bio_issue_time(finish_time_ns) >> 10;\n\tif (!start_time || finish_time <= start_time)\n\t\treturn;\n\n\tlat = finish_time - start_time;\n\t \n\tif (!(bio->bi_issue.value & BIO_ISSUE_THROTL_SKIP_LATENCY))\n\t\tthrotl_track_latency(tg->td, bio_issue_size(&bio->bi_issue),\n\t\t\t\t     bio_op(bio), lat);\n\n\tif (tg->latency_target && lat >= tg->td->filtered_latency) {\n\t\tint bucket;\n\t\tunsigned int threshold;\n\n\t\tbucket = request_bucket_index(bio_issue_size(&bio->bi_issue));\n\t\tthreshold = tg->td->avg_buckets[rw][bucket].latency +\n\t\t\ttg->latency_target;\n\t\tif (lat > threshold)\n\t\t\ttg->bad_bio_cnt++;\n\t\t \n\t\ttg->bio_cnt++;\n\t}\n\n\tif (time_after(jiffies, tg->bio_cnt_reset_time) || tg->bio_cnt > 1024) {\n\t\ttg->bio_cnt_reset_time = tg->td->throtl_slice + jiffies;\n\t\ttg->bio_cnt /= 2;\n\t\ttg->bad_bio_cnt /= 2;\n\t}\n}\n#endif\n\nint blk_throtl_init(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct throtl_data *td;\n\tint ret;\n\n\ttd = kzalloc_node(sizeof(*td), GFP_KERNEL, q->node);\n\tif (!td)\n\t\treturn -ENOMEM;\n\ttd->latency_buckets[READ] = __alloc_percpu(sizeof(struct latency_bucket) *\n\t\tLATENCY_BUCKET_SIZE, __alignof__(u64));\n\tif (!td->latency_buckets[READ]) {\n\t\tkfree(td);\n\t\treturn -ENOMEM;\n\t}\n\ttd->latency_buckets[WRITE] = __alloc_percpu(sizeof(struct latency_bucket) *\n\t\tLATENCY_BUCKET_SIZE, __alignof__(u64));\n\tif (!td->latency_buckets[WRITE]) {\n\t\tfree_percpu(td->latency_buckets[READ]);\n\t\tkfree(td);\n\t\treturn -ENOMEM;\n\t}\n\n\tINIT_WORK(&td->dispatch_work, blk_throtl_dispatch_work_fn);\n\tthrotl_service_queue_init(&td->service_queue);\n\n\tq->td = td;\n\ttd->queue = q;\n\n\ttd->limit_valid[LIMIT_MAX] = true;\n\ttd->limit_index = LIMIT_MAX;\n\ttd->low_upgrade_time = jiffies;\n\ttd->low_downgrade_time = jiffies;\n\n\t \n\tret = blkcg_activate_policy(disk, &blkcg_policy_throtl);\n\tif (ret) {\n\t\tfree_percpu(td->latency_buckets[READ]);\n\t\tfree_percpu(td->latency_buckets[WRITE]);\n\t\tkfree(td);\n\t}\n\treturn ret;\n}\n\nvoid blk_throtl_exit(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\n\tBUG_ON(!q->td);\n\tdel_timer_sync(&q->td->service_queue.pending_timer);\n\tthrotl_shutdown_wq(q);\n\tblkcg_deactivate_policy(disk, &blkcg_policy_throtl);\n\tfree_percpu(q->td->latency_buckets[READ]);\n\tfree_percpu(q->td->latency_buckets[WRITE]);\n\tkfree(q->td);\n}\n\nvoid blk_throtl_register(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct throtl_data *td;\n\tint i;\n\n\ttd = q->td;\n\tBUG_ON(!td);\n\n\tif (blk_queue_nonrot(q)) {\n\t\ttd->throtl_slice = DFL_THROTL_SLICE_SSD;\n\t\ttd->filtered_latency = LATENCY_FILTERED_SSD;\n\t} else {\n\t\ttd->throtl_slice = DFL_THROTL_SLICE_HD;\n\t\ttd->filtered_latency = LATENCY_FILTERED_HD;\n\t\tfor (i = 0; i < LATENCY_BUCKET_SIZE; i++) {\n\t\t\ttd->avg_buckets[READ][i].latency = DFL_HD_BASELINE_LATENCY;\n\t\t\ttd->avg_buckets[WRITE][i].latency = DFL_HD_BASELINE_LATENCY;\n\t\t}\n\t}\n#ifndef CONFIG_BLK_DEV_THROTTLING_LOW\n\t \n\ttd->throtl_slice = DFL_THROTL_SLICE_HD;\n\n#else\n\ttd->track_bio_latency = !queue_is_mq(q);\n\tif (!td->track_bio_latency)\n\t\tblk_stat_enable_accounting(q);\n#endif\n}\n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nssize_t blk_throtl_sample_time_show(struct request_queue *q, char *page)\n{\n\tif (!q->td)\n\t\treturn -EINVAL;\n\treturn sprintf(page, \"%u\\n\", jiffies_to_msecs(q->td->throtl_slice));\n}\n\nssize_t blk_throtl_sample_time_store(struct request_queue *q,\n\tconst char *page, size_t count)\n{\n\tunsigned long v;\n\tunsigned long t;\n\n\tif (!q->td)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &v))\n\t\treturn -EINVAL;\n\tt = msecs_to_jiffies(v);\n\tif (t == 0 || t > MAX_THROTL_SLICE)\n\t\treturn -EINVAL;\n\tq->td->throtl_slice = t;\n\treturn count;\n}\n#endif\n\nstatic int __init throtl_init(void)\n{\n\tkthrotld_workqueue = alloc_workqueue(\"kthrotld\", WQ_MEM_RECLAIM, 0);\n\tif (!kthrotld_workqueue)\n\t\tpanic(\"Failed to create kthrotld\\n\");\n\n\treturn blkcg_policy_register(&blkcg_policy_throtl);\n}\n\nmodule_init(throtl_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}