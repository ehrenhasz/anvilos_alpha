{
  "module_name": "blk-iocost.c",
  "hash_id": "236f6047c9f1cbd23765a64674761a38655651ecfebf2033770c5927f8864498",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-iocost.c",
  "human_readable_source": " \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/timer.h>\n#include <linux/time64.h>\n#include <linux/parser.h>\n#include <linux/sched/signal.h>\n#include <asm/local.h>\n#include <asm/local64.h>\n#include \"blk-rq-qos.h\"\n#include \"blk-stat.h\"\n#include \"blk-wbt.h\"\n#include \"blk-cgroup.h\"\n\n#ifdef CONFIG_TRACEPOINTS\n\n \n#define TRACE_IOCG_PATH_LEN 1024\nstatic DEFINE_SPINLOCK(trace_iocg_path_lock);\nstatic char trace_iocg_path[TRACE_IOCG_PATH_LEN];\n\n#define TRACE_IOCG_PATH(type, iocg, ...)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags;\t\t\t\t\t\t\\\n\t\tif (trace_iocost_##type##_enabled()) {\t\t\t\t\\\n\t\t\tspin_lock_irqsave(&trace_iocg_path_lock, flags);\t\\\n\t\t\tcgroup_path(iocg_to_blkg(iocg)->blkcg->css.cgroup,\t\\\n\t\t\t\t    trace_iocg_path, TRACE_IOCG_PATH_LEN);\t\\\n\t\t\ttrace_iocost_##type(iocg, trace_iocg_path,\t\t\\\n\t\t\t\t\t      ##__VA_ARGS__);\t\t\t\\\n\t\t\tspin_unlock_irqrestore(&trace_iocg_path_lock, flags);\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#else\t \n#define TRACE_IOCG_PATH(type, iocg, ...)\tdo { } while (0)\n#endif\t \n\nenum {\n\tMILLION\t\t\t= 1000000,\n\n\t \n\tMIN_PERIOD\t\t= USEC_PER_MSEC,\n\tMAX_PERIOD\t\t= USEC_PER_SEC,\n\n\t \n\tMARGIN_MIN_PCT\t\t= 10,\n\tMARGIN_LOW_PCT\t\t= 20,\n\tMARGIN_TARGET_PCT\t= 50,\n\n\tINUSE_ADJ_STEP_PCT\t= 25,\n\n\t \n\tTIMER_SLACK_PCT\t\t= 1,\n\n\t \n\tWEIGHT_ONE\t\t= 1 << 16,\n};\n\nenum {\n\t \n\tVTIME_PER_SEC_SHIFT\t= 37,\n\tVTIME_PER_SEC\t\t= 1LLU << VTIME_PER_SEC_SHIFT,\n\tVTIME_PER_USEC\t\t= VTIME_PER_SEC / USEC_PER_SEC,\n\tVTIME_PER_NSEC\t\t= VTIME_PER_SEC / NSEC_PER_SEC,\n\n\t \n\tVRATE_MIN_PPM\t\t= 10000,\t \n\tVRATE_MAX_PPM\t\t= 100000000,\t \n\n\tVRATE_MIN\t\t= VTIME_PER_USEC * VRATE_MIN_PPM / MILLION,\n\tVRATE_CLAMP_ADJ_PCT\t= 4,\n\n\t \n\tAUTOP_CYCLE_NSEC\t= 10LLU * NSEC_PER_SEC,\n};\n\nenum {\n\t \n\tRQ_WAIT_BUSY_PCT\t= 5,\n\n\t \n\tUNBUSY_THR_PCT\t\t= 75,\n\n\t \n\tMIN_DELAY_THR_PCT\t= 500,\n\tMAX_DELAY_THR_PCT\t= 25000,\n\tMIN_DELAY\t\t= 250,\n\tMAX_DELAY\t\t= 250 * USEC_PER_MSEC,\n\n\t \n\tDFGV_USAGE_PCT\t\t= 50,\n\tDFGV_PERIOD\t\t= 100 * USEC_PER_MSEC,\n\n\t \n\tMAX_LAGGING_PERIODS\t= 10,\n\n\t \n\tIOC_PAGE_SHIFT\t\t= 12,\n\tIOC_PAGE_SIZE\t\t= 1 << IOC_PAGE_SHIFT,\n\tIOC_SECT_TO_PAGE_SHIFT\t= IOC_PAGE_SHIFT - SECTOR_SHIFT,\n\n\t \n\tLCOEF_RANDIO_PAGES\t= 4096,\n};\n\nenum ioc_running {\n\tIOC_IDLE,\n\tIOC_RUNNING,\n\tIOC_STOP,\n};\n\n \nenum {\n\tQOS_ENABLE,\n\tQOS_CTRL,\n\tNR_QOS_CTRL_PARAMS,\n};\n\n \nenum {\n\tQOS_RPPM,\n\tQOS_RLAT,\n\tQOS_WPPM,\n\tQOS_WLAT,\n\tQOS_MIN,\n\tQOS_MAX,\n\tNR_QOS_PARAMS,\n};\n\n \nenum {\n\tCOST_CTRL,\n\tCOST_MODEL,\n\tNR_COST_CTRL_PARAMS,\n};\n\n \nenum {\n\tI_LCOEF_RBPS,\n\tI_LCOEF_RSEQIOPS,\n\tI_LCOEF_RRANDIOPS,\n\tI_LCOEF_WBPS,\n\tI_LCOEF_WSEQIOPS,\n\tI_LCOEF_WRANDIOPS,\n\tNR_I_LCOEFS,\n};\n\nenum {\n\tLCOEF_RPAGE,\n\tLCOEF_RSEQIO,\n\tLCOEF_RRANDIO,\n\tLCOEF_WPAGE,\n\tLCOEF_WSEQIO,\n\tLCOEF_WRANDIO,\n\tNR_LCOEFS,\n};\n\nenum {\n\tAUTOP_INVALID,\n\tAUTOP_HDD,\n\tAUTOP_SSD_QD1,\n\tAUTOP_SSD_DFL,\n\tAUTOP_SSD_FAST,\n};\n\nstruct ioc_params {\n\tu32\t\t\t\tqos[NR_QOS_PARAMS];\n\tu64\t\t\t\ti_lcoefs[NR_I_LCOEFS];\n\tu64\t\t\t\tlcoefs[NR_LCOEFS];\n\tu32\t\t\t\ttoo_fast_vrate_pct;\n\tu32\t\t\t\ttoo_slow_vrate_pct;\n};\n\nstruct ioc_margins {\n\ts64\t\t\t\tmin;\n\ts64\t\t\t\tlow;\n\ts64\t\t\t\ttarget;\n};\n\nstruct ioc_missed {\n\tlocal_t\t\t\t\tnr_met;\n\tlocal_t\t\t\t\tnr_missed;\n\tu32\t\t\t\tlast_met;\n\tu32\t\t\t\tlast_missed;\n};\n\nstruct ioc_pcpu_stat {\n\tstruct ioc_missed\t\tmissed[2];\n\n\tlocal64_t\t\t\trq_wait_ns;\n\tu64\t\t\t\tlast_rq_wait_ns;\n};\n\n \nstruct ioc {\n\tstruct rq_qos\t\t\trqos;\n\n\tbool\t\t\t\tenabled;\n\n\tstruct ioc_params\t\tparams;\n\tstruct ioc_margins\t\tmargins;\n\tu32\t\t\t\tperiod_us;\n\tu32\t\t\t\ttimer_slack_ns;\n\tu64\t\t\t\tvrate_min;\n\tu64\t\t\t\tvrate_max;\n\n\tspinlock_t\t\t\tlock;\n\tstruct timer_list\t\ttimer;\n\tstruct list_head\t\tactive_iocgs;\t \n\tstruct ioc_pcpu_stat __percpu\t*pcpu_stat;\n\n\tenum ioc_running\t\trunning;\n\tatomic64_t\t\t\tvtime_rate;\n\tu64\t\t\t\tvtime_base_rate;\n\ts64\t\t\t\tvtime_err;\n\n\tseqcount_spinlock_t\t\tperiod_seqcount;\n\tu64\t\t\t\tperiod_at;\t \n\tu64\t\t\t\tperiod_at_vtime;  \n\n\tatomic64_t\t\t\tcur_period;\t \n\tint\t\t\t\tbusy_level;\t \n\n\tbool\t\t\t\tweights_updated;\n\tatomic_t\t\t\thweight_gen;\t \n\n\t \n\tu64\t\t\t\tdfgv_period_at;\n\tu64\t\t\t\tdfgv_period_rem;\n\tu64\t\t\t\tdfgv_usage_us_sum;\n\n\tu64\t\t\t\tautop_too_fast_at;\n\tu64\t\t\t\tautop_too_slow_at;\n\tint\t\t\t\tautop_idx;\n\tbool\t\t\t\tuser_qos_params:1;\n\tbool\t\t\t\tuser_cost_model:1;\n};\n\nstruct iocg_pcpu_stat {\n\tlocal64_t\t\t\tabs_vusage;\n};\n\nstruct iocg_stat {\n\tu64\t\t\t\tusage_us;\n\tu64\t\t\t\twait_us;\n\tu64\t\t\t\tindebt_us;\n\tu64\t\t\t\tindelay_us;\n};\n\n \nstruct ioc_gq {\n\tstruct blkg_policy_data\t\tpd;\n\tstruct ioc\t\t\t*ioc;\n\n\t \n\tu32\t\t\t\tcfg_weight;\n\tu32\t\t\t\tweight;\n\tu32\t\t\t\tactive;\n\tu32\t\t\t\tinuse;\n\n\tu32\t\t\t\tlast_inuse;\n\ts64\t\t\t\tsaved_margin;\n\n\tsector_t\t\t\tcursor;\t\t \n\n\t \n\tatomic64_t\t\t\tvtime;\n\tatomic64_t\t\t\tdone_vtime;\n\tu64\t\t\t\tabs_vdebt;\n\n\t \n\tu64\t\t\t\tdelay;\n\tu64\t\t\t\tdelay_at;\n\n\t \n\tatomic64_t\t\t\tactive_period;\n\tstruct list_head\t\tactive_list;\n\n\t \n\tu64\t\t\t\tchild_active_sum;\n\tu64\t\t\t\tchild_inuse_sum;\n\tu64\t\t\t\tchild_adjusted_sum;\n\tint\t\t\t\thweight_gen;\n\tu32\t\t\t\thweight_active;\n\tu32\t\t\t\thweight_inuse;\n\tu32\t\t\t\thweight_donating;\n\tu32\t\t\t\thweight_after_donation;\n\n\tstruct list_head\t\twalk_list;\n\tstruct list_head\t\tsurplus_list;\n\n\tstruct wait_queue_head\t\twaitq;\n\tstruct hrtimer\t\t\twaitq_timer;\n\n\t \n\tu64\t\t\t\tactivated_at;\n\n\t \n\tstruct iocg_pcpu_stat __percpu\t*pcpu_stat;\n\tstruct iocg_stat\t\tstat;\n\tstruct iocg_stat\t\tlast_stat;\n\tu64\t\t\t\tlast_stat_abs_vusage;\n\tu64\t\t\t\tusage_delta_us;\n\tu64\t\t\t\twait_since;\n\tu64\t\t\t\tindebt_since;\n\tu64\t\t\t\tindelay_since;\n\n\t \n\tint\t\t\t\tlevel;\n\tstruct ioc_gq\t\t\t*ancestors[];\n};\n\n \nstruct ioc_cgrp {\n\tstruct blkcg_policy_data\tcpd;\n\tunsigned int\t\t\tdfl_weight;\n};\n\nstruct ioc_now {\n\tu64\t\t\t\tnow_ns;\n\tu64\t\t\t\tnow;\n\tu64\t\t\t\tvnow;\n};\n\nstruct iocg_wait {\n\tstruct wait_queue_entry\t\twait;\n\tstruct bio\t\t\t*bio;\n\tu64\t\t\t\tabs_cost;\n\tbool\t\t\t\tcommitted;\n};\n\nstruct iocg_wake_ctx {\n\tstruct ioc_gq\t\t\t*iocg;\n\tu32\t\t\t\thw_inuse;\n\ts64\t\t\t\tvbudget;\n};\n\nstatic const struct ioc_params autop[] = {\n\t[AUTOP_HDD] = {\n\t\t.qos\t\t\t\t= {\n\t\t\t[QOS_RLAT]\t\t=        250000,  \n\t\t\t[QOS_WLAT]\t\t=        250000,\n\t\t\t[QOS_MIN]\t\t= VRATE_MIN_PPM,\n\t\t\t[QOS_MAX]\t\t= VRATE_MAX_PPM,\n\t\t},\n\t\t.i_lcoefs\t\t\t= {\n\t\t\t[I_LCOEF_RBPS]\t\t=     174019176,\n\t\t\t[I_LCOEF_RSEQIOPS]\t=         41708,\n\t\t\t[I_LCOEF_RRANDIOPS]\t=           370,\n\t\t\t[I_LCOEF_WBPS]\t\t=     178075866,\n\t\t\t[I_LCOEF_WSEQIOPS]\t=         42705,\n\t\t\t[I_LCOEF_WRANDIOPS]\t=           378,\n\t\t},\n\t},\n\t[AUTOP_SSD_QD1] = {\n\t\t.qos\t\t\t\t= {\n\t\t\t[QOS_RLAT]\t\t=         25000,  \n\t\t\t[QOS_WLAT]\t\t=         25000,\n\t\t\t[QOS_MIN]\t\t= VRATE_MIN_PPM,\n\t\t\t[QOS_MAX]\t\t= VRATE_MAX_PPM,\n\t\t},\n\t\t.i_lcoefs\t\t\t= {\n\t\t\t[I_LCOEF_RBPS]\t\t=     245855193,\n\t\t\t[I_LCOEF_RSEQIOPS]\t=         61575,\n\t\t\t[I_LCOEF_RRANDIOPS]\t=          6946,\n\t\t\t[I_LCOEF_WBPS]\t\t=     141365009,\n\t\t\t[I_LCOEF_WSEQIOPS]\t=         33716,\n\t\t\t[I_LCOEF_WRANDIOPS]\t=         26796,\n\t\t},\n\t},\n\t[AUTOP_SSD_DFL] = {\n\t\t.qos\t\t\t\t= {\n\t\t\t[QOS_RLAT]\t\t=         25000,  \n\t\t\t[QOS_WLAT]\t\t=         25000,\n\t\t\t[QOS_MIN]\t\t= VRATE_MIN_PPM,\n\t\t\t[QOS_MAX]\t\t= VRATE_MAX_PPM,\n\t\t},\n\t\t.i_lcoefs\t\t\t= {\n\t\t\t[I_LCOEF_RBPS]\t\t=     488636629,\n\t\t\t[I_LCOEF_RSEQIOPS]\t=          8932,\n\t\t\t[I_LCOEF_RRANDIOPS]\t=          8518,\n\t\t\t[I_LCOEF_WBPS]\t\t=     427891549,\n\t\t\t[I_LCOEF_WSEQIOPS]\t=         28755,\n\t\t\t[I_LCOEF_WRANDIOPS]\t=         21940,\n\t\t},\n\t\t.too_fast_vrate_pct\t\t=           500,\n\t},\n\t[AUTOP_SSD_FAST] = {\n\t\t.qos\t\t\t\t= {\n\t\t\t[QOS_RLAT]\t\t=          5000,  \n\t\t\t[QOS_WLAT]\t\t=          5000,\n\t\t\t[QOS_MIN]\t\t= VRATE_MIN_PPM,\n\t\t\t[QOS_MAX]\t\t= VRATE_MAX_PPM,\n\t\t},\n\t\t.i_lcoefs\t\t\t= {\n\t\t\t[I_LCOEF_RBPS]\t\t=    3102524156LLU,\n\t\t\t[I_LCOEF_RSEQIOPS]\t=        724816,\n\t\t\t[I_LCOEF_RRANDIOPS]\t=        778122,\n\t\t\t[I_LCOEF_WBPS]\t\t=    1742780862LLU,\n\t\t\t[I_LCOEF_WSEQIOPS]\t=        425702,\n\t\t\t[I_LCOEF_WRANDIOPS]\t=\t 443193,\n\t\t},\n\t\t.too_slow_vrate_pct\t\t=            10,\n\t},\n};\n\n \nstatic u32 vrate_adj_pct[] =\n\t{ 0, 0, 0, 0,\n\t  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n\t  2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n\t  4, 4, 4, 4, 4, 4, 4, 4, 8, 8, 8, 8, 8, 8, 8, 8, 16 };\n\nstatic struct blkcg_policy blkcg_policy_iocost;\n\n \nstatic struct ioc *rqos_to_ioc(struct rq_qos *rqos)\n{\n\treturn container_of(rqos, struct ioc, rqos);\n}\n\nstatic struct ioc *q_to_ioc(struct request_queue *q)\n{\n\treturn rqos_to_ioc(rq_qos_id(q, RQ_QOS_COST));\n}\n\nstatic const char __maybe_unused *ioc_name(struct ioc *ioc)\n{\n\tstruct gendisk *disk = ioc->rqos.disk;\n\n\tif (!disk)\n\t\treturn \"<unknown>\";\n\treturn disk->disk_name;\n}\n\nstatic struct ioc_gq *pd_to_iocg(struct blkg_policy_data *pd)\n{\n\treturn pd ? container_of(pd, struct ioc_gq, pd) : NULL;\n}\n\nstatic struct ioc_gq *blkg_to_iocg(struct blkcg_gq *blkg)\n{\n\treturn pd_to_iocg(blkg_to_pd(blkg, &blkcg_policy_iocost));\n}\n\nstatic struct blkcg_gq *iocg_to_blkg(struct ioc_gq *iocg)\n{\n\treturn pd_to_blkg(&iocg->pd);\n}\n\nstatic struct ioc_cgrp *blkcg_to_iocc(struct blkcg *blkcg)\n{\n\treturn container_of(blkcg_to_cpd(blkcg, &blkcg_policy_iocost),\n\t\t\t    struct ioc_cgrp, cpd);\n}\n\n \nstatic u64 abs_cost_to_cost(u64 abs_cost, u32 hw_inuse)\n{\n\treturn DIV64_U64_ROUND_UP(abs_cost * WEIGHT_ONE, hw_inuse);\n}\n\n \nstatic u64 cost_to_abs_cost(u64 cost, u32 hw_inuse)\n{\n\treturn DIV64_U64_ROUND_UP(cost * hw_inuse, WEIGHT_ONE);\n}\n\nstatic void iocg_commit_bio(struct ioc_gq *iocg, struct bio *bio,\n\t\t\t    u64 abs_cost, u64 cost)\n{\n\tstruct iocg_pcpu_stat *gcs;\n\n\tbio->bi_iocost_cost = cost;\n\tatomic64_add(cost, &iocg->vtime);\n\n\tgcs = get_cpu_ptr(iocg->pcpu_stat);\n\tlocal64_add(abs_cost, &gcs->abs_vusage);\n\tput_cpu_ptr(gcs);\n}\n\nstatic void iocg_lock(struct ioc_gq *iocg, bool lock_ioc, unsigned long *flags)\n{\n\tif (lock_ioc) {\n\t\tspin_lock_irqsave(&iocg->ioc->lock, *flags);\n\t\tspin_lock(&iocg->waitq.lock);\n\t} else {\n\t\tspin_lock_irqsave(&iocg->waitq.lock, *flags);\n\t}\n}\n\nstatic void iocg_unlock(struct ioc_gq *iocg, bool unlock_ioc, unsigned long *flags)\n{\n\tif (unlock_ioc) {\n\t\tspin_unlock(&iocg->waitq.lock);\n\t\tspin_unlock_irqrestore(&iocg->ioc->lock, *flags);\n\t} else {\n\t\tspin_unlock_irqrestore(&iocg->waitq.lock, *flags);\n\t}\n}\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/iocost.h>\n\nstatic void ioc_refresh_margins(struct ioc *ioc)\n{\n\tstruct ioc_margins *margins = &ioc->margins;\n\tu32 period_us = ioc->period_us;\n\tu64 vrate = ioc->vtime_base_rate;\n\n\tmargins->min = (period_us * MARGIN_MIN_PCT / 100) * vrate;\n\tmargins->low = (period_us * MARGIN_LOW_PCT / 100) * vrate;\n\tmargins->target = (period_us * MARGIN_TARGET_PCT / 100) * vrate;\n}\n\n \nstatic void ioc_refresh_period_us(struct ioc *ioc)\n{\n\tu32 ppm, lat, multi, period_us;\n\n\tlockdep_assert_held(&ioc->lock);\n\n\t \n\tif (ioc->params.qos[QOS_RLAT] >= ioc->params.qos[QOS_WLAT]) {\n\t\tppm = ioc->params.qos[QOS_RPPM];\n\t\tlat = ioc->params.qos[QOS_RLAT];\n\t} else {\n\t\tppm = ioc->params.qos[QOS_WPPM];\n\t\tlat = ioc->params.qos[QOS_WLAT];\n\t}\n\n\t \n\tif (ppm)\n\t\tmulti = max_t(u32, (MILLION - ppm) / 50000, 2);\n\telse\n\t\tmulti = 2;\n\tperiod_us = multi * lat;\n\tperiod_us = clamp_t(u32, period_us, MIN_PERIOD, MAX_PERIOD);\n\n\t \n\tioc->period_us = period_us;\n\tioc->timer_slack_ns = div64_u64(\n\t\t(u64)period_us * NSEC_PER_USEC * TIMER_SLACK_PCT,\n\t\t100);\n\tioc_refresh_margins(ioc);\n}\n\n \nstatic int ioc_autop_idx(struct ioc *ioc, struct gendisk *disk)\n{\n\tint idx = ioc->autop_idx;\n\tconst struct ioc_params *p = &autop[idx];\n\tu32 vrate_pct;\n\tu64 now_ns;\n\n\t \n\tif (!blk_queue_nonrot(disk->queue))\n\t\treturn AUTOP_HDD;\n\n\t \n\tif (blk_queue_depth(disk->queue) == 1)\n\t\treturn AUTOP_SSD_QD1;\n\n\t \n\tif (idx < AUTOP_SSD_DFL)\n\t\treturn AUTOP_SSD_DFL;\n\n\t \n\tif (ioc->user_qos_params || ioc->user_cost_model)\n\t\treturn idx;\n\n\t \n\tvrate_pct = div64_u64(ioc->vtime_base_rate * 100, VTIME_PER_USEC);\n\tnow_ns = ktime_get_ns();\n\n\tif (p->too_fast_vrate_pct && p->too_fast_vrate_pct <= vrate_pct) {\n\t\tif (!ioc->autop_too_fast_at)\n\t\t\tioc->autop_too_fast_at = now_ns;\n\t\tif (now_ns - ioc->autop_too_fast_at >= AUTOP_CYCLE_NSEC)\n\t\t\treturn idx + 1;\n\t} else {\n\t\tioc->autop_too_fast_at = 0;\n\t}\n\n\tif (p->too_slow_vrate_pct && p->too_slow_vrate_pct >= vrate_pct) {\n\t\tif (!ioc->autop_too_slow_at)\n\t\t\tioc->autop_too_slow_at = now_ns;\n\t\tif (now_ns - ioc->autop_too_slow_at >= AUTOP_CYCLE_NSEC)\n\t\t\treturn idx - 1;\n\t} else {\n\t\tioc->autop_too_slow_at = 0;\n\t}\n\n\treturn idx;\n}\n\n \nstatic void calc_lcoefs(u64 bps, u64 seqiops, u64 randiops,\n\t\t\tu64 *page, u64 *seqio, u64 *randio)\n{\n\tu64 v;\n\n\t*page = *seqio = *randio = 0;\n\n\tif (bps) {\n\t\tu64 bps_pages = DIV_ROUND_UP_ULL(bps, IOC_PAGE_SIZE);\n\n\t\tif (bps_pages)\n\t\t\t*page = DIV64_U64_ROUND_UP(VTIME_PER_SEC, bps_pages);\n\t\telse\n\t\t\t*page = 1;\n\t}\n\n\tif (seqiops) {\n\t\tv = DIV64_U64_ROUND_UP(VTIME_PER_SEC, seqiops);\n\t\tif (v > *page)\n\t\t\t*seqio = v - *page;\n\t}\n\n\tif (randiops) {\n\t\tv = DIV64_U64_ROUND_UP(VTIME_PER_SEC, randiops);\n\t\tif (v > *page)\n\t\t\t*randio = v - *page;\n\t}\n}\n\nstatic void ioc_refresh_lcoefs(struct ioc *ioc)\n{\n\tu64 *u = ioc->params.i_lcoefs;\n\tu64 *c = ioc->params.lcoefs;\n\n\tcalc_lcoefs(u[I_LCOEF_RBPS], u[I_LCOEF_RSEQIOPS], u[I_LCOEF_RRANDIOPS],\n\t\t    &c[LCOEF_RPAGE], &c[LCOEF_RSEQIO], &c[LCOEF_RRANDIO]);\n\tcalc_lcoefs(u[I_LCOEF_WBPS], u[I_LCOEF_WSEQIOPS], u[I_LCOEF_WRANDIOPS],\n\t\t    &c[LCOEF_WPAGE], &c[LCOEF_WSEQIO], &c[LCOEF_WRANDIO]);\n}\n\n \nstatic bool ioc_refresh_params_disk(struct ioc *ioc, bool force,\n\t\t\t\t    struct gendisk *disk)\n{\n\tconst struct ioc_params *p;\n\tint idx;\n\n\tlockdep_assert_held(&ioc->lock);\n\n\tidx = ioc_autop_idx(ioc, disk);\n\tp = &autop[idx];\n\n\tif (idx == ioc->autop_idx && !force)\n\t\treturn false;\n\n\tif (idx != ioc->autop_idx) {\n\t\tatomic64_set(&ioc->vtime_rate, VTIME_PER_USEC);\n\t\tioc->vtime_base_rate = VTIME_PER_USEC;\n\t}\n\n\tioc->autop_idx = idx;\n\tioc->autop_too_fast_at = 0;\n\tioc->autop_too_slow_at = 0;\n\n\tif (!ioc->user_qos_params)\n\t\tmemcpy(ioc->params.qos, p->qos, sizeof(p->qos));\n\tif (!ioc->user_cost_model)\n\t\tmemcpy(ioc->params.i_lcoefs, p->i_lcoefs, sizeof(p->i_lcoefs));\n\n\tioc_refresh_period_us(ioc);\n\tioc_refresh_lcoefs(ioc);\n\n\tioc->vrate_min = DIV64_U64_ROUND_UP((u64)ioc->params.qos[QOS_MIN] *\n\t\t\t\t\t    VTIME_PER_USEC, MILLION);\n\tioc->vrate_max = DIV64_U64_ROUND_UP((u64)ioc->params.qos[QOS_MAX] *\n\t\t\t\t\t    VTIME_PER_USEC, MILLION);\n\n\treturn true;\n}\n\nstatic bool ioc_refresh_params(struct ioc *ioc, bool force)\n{\n\treturn ioc_refresh_params_disk(ioc, force, ioc->rqos.disk);\n}\n\n \nstatic void ioc_refresh_vrate(struct ioc *ioc, struct ioc_now *now)\n{\n\ts64 pleft = ioc->period_at + ioc->period_us - now->now;\n\ts64 vperiod = ioc->period_us * ioc->vtime_base_rate;\n\ts64 vcomp, vcomp_min, vcomp_max;\n\n\tlockdep_assert_held(&ioc->lock);\n\n\t \n\tif (pleft <= 0)\n\t\tgoto done;\n\n\t \n\tvcomp = -div64_s64(ioc->vtime_err, pleft);\n\tvcomp_min = -(ioc->vtime_base_rate >> 1);\n\tvcomp_max = ioc->vtime_base_rate;\n\tvcomp = clamp(vcomp, vcomp_min, vcomp_max);\n\n\tioc->vtime_err += vcomp * pleft;\n\n\tatomic64_set(&ioc->vtime_rate, ioc->vtime_base_rate + vcomp);\ndone:\n\t \n\tioc->vtime_err = clamp(ioc->vtime_err, -vperiod, vperiod);\n}\n\nstatic void ioc_adjust_base_vrate(struct ioc *ioc, u32 rq_wait_pct,\n\t\t\t\t  int nr_lagging, int nr_shortages,\n\t\t\t\t  int prev_busy_level, u32 *missed_ppm)\n{\n\tu64 vrate = ioc->vtime_base_rate;\n\tu64 vrate_min = ioc->vrate_min, vrate_max = ioc->vrate_max;\n\n\tif (!ioc->busy_level || (ioc->busy_level < 0 && nr_lagging)) {\n\t\tif (ioc->busy_level != prev_busy_level || nr_lagging)\n\t\t\ttrace_iocost_ioc_vrate_adj(ioc, vrate,\n\t\t\t\t\t\t   missed_ppm, rq_wait_pct,\n\t\t\t\t\t\t   nr_lagging, nr_shortages);\n\n\t\treturn;\n\t}\n\n\t \n\tif (vrate < vrate_min) {\n\t\tvrate = div64_u64(vrate * (100 + VRATE_CLAMP_ADJ_PCT), 100);\n\t\tvrate = min(vrate, vrate_min);\n\t} else if (vrate > vrate_max) {\n\t\tvrate = div64_u64(vrate * (100 - VRATE_CLAMP_ADJ_PCT), 100);\n\t\tvrate = max(vrate, vrate_max);\n\t} else {\n\t\tint idx = min_t(int, abs(ioc->busy_level),\n\t\t\t\tARRAY_SIZE(vrate_adj_pct) - 1);\n\t\tu32 adj_pct = vrate_adj_pct[idx];\n\n\t\tif (ioc->busy_level > 0)\n\t\t\tadj_pct = 100 - adj_pct;\n\t\telse\n\t\t\tadj_pct = 100 + adj_pct;\n\n\t\tvrate = clamp(DIV64_U64_ROUND_UP(vrate * adj_pct, 100),\n\t\t\t      vrate_min, vrate_max);\n\t}\n\n\ttrace_iocost_ioc_vrate_adj(ioc, vrate, missed_ppm, rq_wait_pct,\n\t\t\t\t   nr_lagging, nr_shortages);\n\n\tioc->vtime_base_rate = vrate;\n\tioc_refresh_margins(ioc);\n}\n\n \nstatic void ioc_now(struct ioc *ioc, struct ioc_now *now)\n{\n\tunsigned seq;\n\tu64 vrate;\n\n\tnow->now_ns = ktime_get();\n\tnow->now = ktime_to_us(now->now_ns);\n\tvrate = atomic64_read(&ioc->vtime_rate);\n\n\t \n\tdo {\n\t\tseq = read_seqcount_begin(&ioc->period_seqcount);\n\t\tnow->vnow = ioc->period_at_vtime +\n\t\t\t(now->now - ioc->period_at) * vrate;\n\t} while (read_seqcount_retry(&ioc->period_seqcount, seq));\n}\n\nstatic void ioc_start_period(struct ioc *ioc, struct ioc_now *now)\n{\n\tWARN_ON_ONCE(ioc->running != IOC_RUNNING);\n\n\twrite_seqcount_begin(&ioc->period_seqcount);\n\tioc->period_at = now->now;\n\tioc->period_at_vtime = now->vnow;\n\twrite_seqcount_end(&ioc->period_seqcount);\n\n\tioc->timer.expires = jiffies + usecs_to_jiffies(ioc->period_us);\n\tadd_timer(&ioc->timer);\n}\n\n \nstatic void __propagate_weights(struct ioc_gq *iocg, u32 active, u32 inuse,\n\t\t\t\tbool save, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tint lvl;\n\n\tlockdep_assert_held(&ioc->lock);\n\n\t \n\tif (list_empty(&iocg->active_list) && iocg->child_active_sum) {\n\t\tinuse = DIV64_U64_ROUND_UP(active * iocg->child_inuse_sum,\n\t\t\t\t\t   iocg->child_active_sum);\n\t} else {\n\t\tinuse = clamp_t(u32, inuse, 1, active);\n\t}\n\n\tiocg->last_inuse = iocg->inuse;\n\tif (save)\n\t\tiocg->saved_margin = now->vnow - atomic64_read(&iocg->vtime);\n\n\tif (active == iocg->active && inuse == iocg->inuse)\n\t\treturn;\n\n\tfor (lvl = iocg->level - 1; lvl >= 0; lvl--) {\n\t\tstruct ioc_gq *parent = iocg->ancestors[lvl];\n\t\tstruct ioc_gq *child = iocg->ancestors[lvl + 1];\n\t\tu32 parent_active = 0, parent_inuse = 0;\n\n\t\t \n\t\tparent->child_active_sum += (s32)(active - child->active);\n\t\tparent->child_inuse_sum += (s32)(inuse - child->inuse);\n\t\t \n\t\tchild->active = active;\n\t\tchild->inuse = inuse;\n\n\t\t \n\t\tif (parent->child_active_sum) {\n\t\t\tparent_active = parent->weight;\n\t\t\tparent_inuse = DIV64_U64_ROUND_UP(\n\t\t\t\tparent_active * parent->child_inuse_sum,\n\t\t\t\tparent->child_active_sum);\n\t\t}\n\n\t\t \n\t\tif (parent_active == parent->active &&\n\t\t    parent_inuse == parent->inuse)\n\t\t\tbreak;\n\n\t\tactive = parent_active;\n\t\tinuse = parent_inuse;\n\t}\n\n\tioc->weights_updated = true;\n}\n\nstatic void commit_weights(struct ioc *ioc)\n{\n\tlockdep_assert_held(&ioc->lock);\n\n\tif (ioc->weights_updated) {\n\t\t \n\t\tsmp_wmb();\n\t\tatomic_inc(&ioc->hweight_gen);\n\t\tioc->weights_updated = false;\n\t}\n}\n\nstatic void propagate_weights(struct ioc_gq *iocg, u32 active, u32 inuse,\n\t\t\t      bool save, struct ioc_now *now)\n{\n\t__propagate_weights(iocg, active, inuse, save, now);\n\tcommit_weights(iocg->ioc);\n}\n\nstatic void current_hweight(struct ioc_gq *iocg, u32 *hw_activep, u32 *hw_inusep)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tint lvl;\n\tu32 hwa, hwi;\n\tint ioc_gen;\n\n\t \n\tioc_gen = atomic_read(&ioc->hweight_gen);\n\tif (ioc_gen == iocg->hweight_gen)\n\t\tgoto out;\n\n\t \n\tsmp_rmb();\n\n\thwa = hwi = WEIGHT_ONE;\n\tfor (lvl = 0; lvl <= iocg->level - 1; lvl++) {\n\t\tstruct ioc_gq *parent = iocg->ancestors[lvl];\n\t\tstruct ioc_gq *child = iocg->ancestors[lvl + 1];\n\t\tu64 active_sum = READ_ONCE(parent->child_active_sum);\n\t\tu64 inuse_sum = READ_ONCE(parent->child_inuse_sum);\n\t\tu32 active = READ_ONCE(child->active);\n\t\tu32 inuse = READ_ONCE(child->inuse);\n\n\t\t \n\t\tif (!active_sum || !inuse_sum)\n\t\t\tcontinue;\n\n\t\tactive_sum = max_t(u64, active, active_sum);\n\t\thwa = div64_u64((u64)hwa * active, active_sum);\n\n\t\tinuse_sum = max_t(u64, inuse, inuse_sum);\n\t\thwi = div64_u64((u64)hwi * inuse, inuse_sum);\n\t}\n\n\tiocg->hweight_active = max_t(u32, hwa, 1);\n\tiocg->hweight_inuse = max_t(u32, hwi, 1);\n\tiocg->hweight_gen = ioc_gen;\nout:\n\tif (hw_activep)\n\t\t*hw_activep = iocg->hweight_active;\n\tif (hw_inusep)\n\t\t*hw_inusep = iocg->hweight_inuse;\n}\n\n \nstatic u32 current_hweight_max(struct ioc_gq *iocg)\n{\n\tu32 hwm = WEIGHT_ONE;\n\tu32 inuse = iocg->active;\n\tu64 child_inuse_sum;\n\tint lvl;\n\n\tlockdep_assert_held(&iocg->ioc->lock);\n\n\tfor (lvl = iocg->level - 1; lvl >= 0; lvl--) {\n\t\tstruct ioc_gq *parent = iocg->ancestors[lvl];\n\t\tstruct ioc_gq *child = iocg->ancestors[lvl + 1];\n\n\t\tchild_inuse_sum = parent->child_inuse_sum + inuse - child->inuse;\n\t\thwm = div64_u64((u64)hwm * inuse, child_inuse_sum);\n\t\tinuse = DIV64_U64_ROUND_UP(parent->active * child_inuse_sum,\n\t\t\t\t\t   parent->child_active_sum);\n\t}\n\n\treturn max_t(u32, hwm, 1);\n}\n\nstatic void weight_updated(struct ioc_gq *iocg, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tstruct blkcg_gq *blkg = iocg_to_blkg(iocg);\n\tstruct ioc_cgrp *iocc = blkcg_to_iocc(blkg->blkcg);\n\tu32 weight;\n\n\tlockdep_assert_held(&ioc->lock);\n\n\tweight = iocg->cfg_weight ?: iocc->dfl_weight;\n\tif (weight != iocg->weight && iocg->active)\n\t\tpropagate_weights(iocg, weight, iocg->inuse, true, now);\n\tiocg->weight = weight;\n}\n\nstatic bool iocg_activate(struct ioc_gq *iocg, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tu64 last_period, cur_period;\n\tu64 vtime, vtarget;\n\tint i;\n\n\t \n\tif (!list_empty(&iocg->active_list)) {\n\t\tioc_now(ioc, now);\n\t\tcur_period = atomic64_read(&ioc->cur_period);\n\t\tif (atomic64_read(&iocg->active_period) != cur_period)\n\t\t\tatomic64_set(&iocg->active_period, cur_period);\n\t\treturn true;\n\t}\n\n\t \n\tif (iocg->child_active_sum)\n\t\treturn false;\n\n\tspin_lock_irq(&ioc->lock);\n\n\tioc_now(ioc, now);\n\n\t \n\tcur_period = atomic64_read(&ioc->cur_period);\n\tlast_period = atomic64_read(&iocg->active_period);\n\tatomic64_set(&iocg->active_period, cur_period);\n\n\t \n\tif (!list_empty(&iocg->active_list))\n\t\tgoto succeed_unlock;\n\tfor (i = iocg->level - 1; i > 0; i--)\n\t\tif (!list_empty(&iocg->ancestors[i]->active_list))\n\t\t\tgoto fail_unlock;\n\n\tif (iocg->child_active_sum)\n\t\tgoto fail_unlock;\n\n\t \n\tvtarget = now->vnow - ioc->margins.target;\n\tvtime = atomic64_read(&iocg->vtime);\n\n\tatomic64_add(vtarget - vtime, &iocg->vtime);\n\tatomic64_add(vtarget - vtime, &iocg->done_vtime);\n\tvtime = vtarget;\n\n\t \n\tiocg->hweight_gen = atomic_read(&ioc->hweight_gen) - 1;\n\tlist_add(&iocg->active_list, &ioc->active_iocgs);\n\n\tpropagate_weights(iocg, iocg->weight,\n\t\t\t  iocg->last_inuse ?: iocg->weight, true, now);\n\n\tTRACE_IOCG_PATH(iocg_activate, iocg, now,\n\t\t\tlast_period, cur_period, vtime);\n\n\tiocg->activated_at = now->now;\n\n\tif (ioc->running == IOC_IDLE) {\n\t\tioc->running = IOC_RUNNING;\n\t\tioc->dfgv_period_at = now->now;\n\t\tioc->dfgv_period_rem = 0;\n\t\tioc_start_period(ioc, now);\n\t}\n\nsucceed_unlock:\n\tspin_unlock_irq(&ioc->lock);\n\treturn true;\n\nfail_unlock:\n\tspin_unlock_irq(&ioc->lock);\n\treturn false;\n}\n\nstatic bool iocg_kick_delay(struct ioc_gq *iocg, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tstruct blkcg_gq *blkg = iocg_to_blkg(iocg);\n\tu64 tdelta, delay, new_delay;\n\ts64 vover, vover_pct;\n\tu32 hwa;\n\n\tlockdep_assert_held(&iocg->waitq.lock);\n\n\t \n\ttdelta = now->now - iocg->delay_at;\n\tif (iocg->delay)\n\t\tdelay = iocg->delay >> div64_u64(tdelta, USEC_PER_SEC);\n\telse\n\t\tdelay = 0;\n\n\t \n\tcurrent_hweight(iocg, &hwa, NULL);\n\tvover = atomic64_read(&iocg->vtime) +\n\t\tabs_cost_to_cost(iocg->abs_vdebt, hwa) - now->vnow;\n\tvover_pct = div64_s64(100 * vover,\n\t\t\t      ioc->period_us * ioc->vtime_base_rate);\n\n\tif (vover_pct <= MIN_DELAY_THR_PCT)\n\t\tnew_delay = 0;\n\telse if (vover_pct >= MAX_DELAY_THR_PCT)\n\t\tnew_delay = MAX_DELAY;\n\telse\n\t\tnew_delay = MIN_DELAY +\n\t\t\tdiv_u64((MAX_DELAY - MIN_DELAY) *\n\t\t\t\t(vover_pct - MIN_DELAY_THR_PCT),\n\t\t\t\tMAX_DELAY_THR_PCT - MIN_DELAY_THR_PCT);\n\n\t \n\tif (new_delay > delay) {\n\t\tiocg->delay = new_delay;\n\t\tiocg->delay_at = now->now;\n\t\tdelay = new_delay;\n\t}\n\n\tif (delay >= MIN_DELAY) {\n\t\tif (!iocg->indelay_since)\n\t\t\tiocg->indelay_since = now->now;\n\t\tblkcg_set_delay(blkg, delay * NSEC_PER_USEC);\n\t\treturn true;\n\t} else {\n\t\tif (iocg->indelay_since) {\n\t\t\tiocg->stat.indelay_us += now->now - iocg->indelay_since;\n\t\t\tiocg->indelay_since = 0;\n\t\t}\n\t\tiocg->delay = 0;\n\t\tblkcg_clear_delay(blkg);\n\t\treturn false;\n\t}\n}\n\nstatic void iocg_incur_debt(struct ioc_gq *iocg, u64 abs_cost,\n\t\t\t    struct ioc_now *now)\n{\n\tstruct iocg_pcpu_stat *gcs;\n\n\tlockdep_assert_held(&iocg->ioc->lock);\n\tlockdep_assert_held(&iocg->waitq.lock);\n\tWARN_ON_ONCE(list_empty(&iocg->active_list));\n\n\t \n\tif (!iocg->abs_vdebt && abs_cost) {\n\t\tiocg->indebt_since = now->now;\n\t\tpropagate_weights(iocg, iocg->active, 0, false, now);\n\t}\n\n\tiocg->abs_vdebt += abs_cost;\n\n\tgcs = get_cpu_ptr(iocg->pcpu_stat);\n\tlocal64_add(abs_cost, &gcs->abs_vusage);\n\tput_cpu_ptr(gcs);\n}\n\nstatic void iocg_pay_debt(struct ioc_gq *iocg, u64 abs_vpay,\n\t\t\t  struct ioc_now *now)\n{\n\tlockdep_assert_held(&iocg->ioc->lock);\n\tlockdep_assert_held(&iocg->waitq.lock);\n\n\t \n\tWARN_ON_ONCE(list_empty(&iocg->active_list));\n\tWARN_ON_ONCE(iocg->inuse > 1);\n\n\tiocg->abs_vdebt -= min(abs_vpay, iocg->abs_vdebt);\n\n\t \n\tif (!iocg->abs_vdebt) {\n\t\tiocg->stat.indebt_us += now->now - iocg->indebt_since;\n\t\tiocg->indebt_since = 0;\n\n\t\tpropagate_weights(iocg, iocg->active, iocg->last_inuse,\n\t\t\t\t  false, now);\n\t}\n}\n\nstatic int iocg_wake_fn(struct wait_queue_entry *wq_entry, unsigned mode,\n\t\t\tint flags, void *key)\n{\n\tstruct iocg_wait *wait = container_of(wq_entry, struct iocg_wait, wait);\n\tstruct iocg_wake_ctx *ctx = key;\n\tu64 cost = abs_cost_to_cost(wait->abs_cost, ctx->hw_inuse);\n\n\tctx->vbudget -= cost;\n\n\tif (ctx->vbudget < 0)\n\t\treturn -1;\n\n\tiocg_commit_bio(ctx->iocg, wait->bio, wait->abs_cost, cost);\n\twait->committed = true;\n\n\t \n\tdefault_wake_function(wq_entry, mode, flags, key);\n\tlist_del_init_careful(&wq_entry->entry);\n\treturn 0;\n}\n\n \nstatic void iocg_kick_waitq(struct ioc_gq *iocg, bool pay_debt,\n\t\t\t    struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tstruct iocg_wake_ctx ctx = { .iocg = iocg };\n\tu64 vshortage, expires, oexpires;\n\ts64 vbudget;\n\tu32 hwa;\n\n\tlockdep_assert_held(&iocg->waitq.lock);\n\n\tcurrent_hweight(iocg, &hwa, NULL);\n\tvbudget = now->vnow - atomic64_read(&iocg->vtime);\n\n\t \n\tif (pay_debt && iocg->abs_vdebt && vbudget > 0) {\n\t\tu64 abs_vbudget = cost_to_abs_cost(vbudget, hwa);\n\t\tu64 abs_vpay = min_t(u64, abs_vbudget, iocg->abs_vdebt);\n\t\tu64 vpay = abs_cost_to_cost(abs_vpay, hwa);\n\n\t\tlockdep_assert_held(&ioc->lock);\n\n\t\tatomic64_add(vpay, &iocg->vtime);\n\t\tatomic64_add(vpay, &iocg->done_vtime);\n\t\tiocg_pay_debt(iocg, abs_vpay, now);\n\t\tvbudget -= vpay;\n\t}\n\n\tif (iocg->abs_vdebt || iocg->delay)\n\t\tiocg_kick_delay(iocg, now);\n\n\t \n\tif (iocg->abs_vdebt) {\n\t\ts64 vdebt = abs_cost_to_cost(iocg->abs_vdebt, hwa);\n\t\tvbudget = min_t(s64, 0, vbudget - vdebt);\n\t}\n\n\t \n\tctx.vbudget = vbudget;\n\tcurrent_hweight(iocg, NULL, &ctx.hw_inuse);\n\n\t__wake_up_locked_key(&iocg->waitq, TASK_NORMAL, &ctx);\n\n\tif (!waitqueue_active(&iocg->waitq)) {\n\t\tif (iocg->wait_since) {\n\t\t\tiocg->stat.wait_us += now->now - iocg->wait_since;\n\t\t\tiocg->wait_since = 0;\n\t\t}\n\t\treturn;\n\t}\n\n\tif (!iocg->wait_since)\n\t\tiocg->wait_since = now->now;\n\n\tif (WARN_ON_ONCE(ctx.vbudget >= 0))\n\t\treturn;\n\n\t \n\tvshortage = -ctx.vbudget;\n\texpires = now->now_ns +\n\t\tDIV64_U64_ROUND_UP(vshortage, ioc->vtime_base_rate) *\n\t\tNSEC_PER_USEC;\n\texpires += ioc->timer_slack_ns;\n\n\t \n\toexpires = ktime_to_ns(hrtimer_get_softexpires(&iocg->waitq_timer));\n\tif (hrtimer_is_queued(&iocg->waitq_timer) &&\n\t    abs(oexpires - expires) <= ioc->timer_slack_ns)\n\t\treturn;\n\n\thrtimer_start_range_ns(&iocg->waitq_timer, ns_to_ktime(expires),\n\t\t\t       ioc->timer_slack_ns, HRTIMER_MODE_ABS);\n}\n\nstatic enum hrtimer_restart iocg_waitq_timer_fn(struct hrtimer *timer)\n{\n\tstruct ioc_gq *iocg = container_of(timer, struct ioc_gq, waitq_timer);\n\tbool pay_debt = READ_ONCE(iocg->abs_vdebt);\n\tstruct ioc_now now;\n\tunsigned long flags;\n\n\tioc_now(iocg->ioc, &now);\n\n\tiocg_lock(iocg, pay_debt, &flags);\n\tiocg_kick_waitq(iocg, pay_debt, &now);\n\tiocg_unlock(iocg, pay_debt, &flags);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void ioc_lat_stat(struct ioc *ioc, u32 *missed_ppm_ar, u32 *rq_wait_pct_p)\n{\n\tu32 nr_met[2] = { };\n\tu32 nr_missed[2] = { };\n\tu64 rq_wait_ns = 0;\n\tint cpu, rw;\n\n\tfor_each_online_cpu(cpu) {\n\t\tstruct ioc_pcpu_stat *stat = per_cpu_ptr(ioc->pcpu_stat, cpu);\n\t\tu64 this_rq_wait_ns;\n\n\t\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\t\tu32 this_met = local_read(&stat->missed[rw].nr_met);\n\t\t\tu32 this_missed = local_read(&stat->missed[rw].nr_missed);\n\n\t\t\tnr_met[rw] += this_met - stat->missed[rw].last_met;\n\t\t\tnr_missed[rw] += this_missed - stat->missed[rw].last_missed;\n\t\t\tstat->missed[rw].last_met = this_met;\n\t\t\tstat->missed[rw].last_missed = this_missed;\n\t\t}\n\n\t\tthis_rq_wait_ns = local64_read(&stat->rq_wait_ns);\n\t\trq_wait_ns += this_rq_wait_ns - stat->last_rq_wait_ns;\n\t\tstat->last_rq_wait_ns = this_rq_wait_ns;\n\t}\n\n\tfor (rw = READ; rw <= WRITE; rw++) {\n\t\tif (nr_met[rw] + nr_missed[rw])\n\t\t\tmissed_ppm_ar[rw] =\n\t\t\t\tDIV64_U64_ROUND_UP((u64)nr_missed[rw] * MILLION,\n\t\t\t\t\t\t   nr_met[rw] + nr_missed[rw]);\n\t\telse\n\t\t\tmissed_ppm_ar[rw] = 0;\n\t}\n\n\t*rq_wait_pct_p = div64_u64(rq_wait_ns * 100,\n\t\t\t\t   ioc->period_us * NSEC_PER_USEC);\n}\n\n \nstatic bool iocg_is_idle(struct ioc_gq *iocg)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\n\t \n\tif (atomic64_read(&iocg->active_period) ==\n\t    atomic64_read(&ioc->cur_period))\n\t\treturn false;\n\n\t \n\tif (atomic64_read(&iocg->done_vtime) != atomic64_read(&iocg->vtime))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic void iocg_build_inner_walk(struct ioc_gq *iocg,\n\t\t\t\t  struct list_head *inner_walk)\n{\n\tint lvl;\n\n\tWARN_ON_ONCE(!list_empty(&iocg->walk_list));\n\n\t \n\tfor (lvl = iocg->level - 1; lvl >= 0; lvl--) {\n\t\tif (!list_empty(&iocg->ancestors[lvl]->walk_list))\n\t\t\tbreak;\n\t}\n\n\t \n\twhile (++lvl <= iocg->level - 1) {\n\t\tstruct ioc_gq *inner = iocg->ancestors[lvl];\n\n\t\t \n\t\tlist_add_tail(&inner->walk_list, inner_walk);\n\t}\n}\n\n \nstatic void iocg_flush_stat_upward(struct ioc_gq *iocg)\n{\n\tif (iocg->level > 0) {\n\t\tstruct iocg_stat *parent_stat =\n\t\t\t&iocg->ancestors[iocg->level - 1]->stat;\n\n\t\tparent_stat->usage_us +=\n\t\t\tiocg->stat.usage_us - iocg->last_stat.usage_us;\n\t\tparent_stat->wait_us +=\n\t\t\tiocg->stat.wait_us - iocg->last_stat.wait_us;\n\t\tparent_stat->indebt_us +=\n\t\t\tiocg->stat.indebt_us - iocg->last_stat.indebt_us;\n\t\tparent_stat->indelay_us +=\n\t\t\tiocg->stat.indelay_us - iocg->last_stat.indelay_us;\n\t}\n\n\tiocg->last_stat = iocg->stat;\n}\n\n \nstatic void iocg_flush_stat_leaf(struct ioc_gq *iocg, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tu64 abs_vusage = 0;\n\tu64 vusage_delta;\n\tint cpu;\n\n\tlockdep_assert_held(&iocg->ioc->lock);\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tabs_vusage += local64_read(\n\t\t\t\tper_cpu_ptr(&iocg->pcpu_stat->abs_vusage, cpu));\n\t}\n\tvusage_delta = abs_vusage - iocg->last_stat_abs_vusage;\n\tiocg->last_stat_abs_vusage = abs_vusage;\n\n\tiocg->usage_delta_us = div64_u64(vusage_delta, ioc->vtime_base_rate);\n\tiocg->stat.usage_us += iocg->usage_delta_us;\n\n\tiocg_flush_stat_upward(iocg);\n}\n\n \nstatic void iocg_flush_stat(struct list_head *target_iocgs, struct ioc_now *now)\n{\n\tLIST_HEAD(inner_walk);\n\tstruct ioc_gq *iocg, *tiocg;\n\n\t \n\tlist_for_each_entry(iocg, target_iocgs, active_list) {\n\t\tiocg_flush_stat_leaf(iocg, now);\n\t\tiocg_build_inner_walk(iocg, &inner_walk);\n\t}\n\n\t \n\tlist_for_each_entry_safe_reverse(iocg, tiocg, &inner_walk, walk_list) {\n\t\tiocg_flush_stat_upward(iocg);\n\t\tlist_del_init(&iocg->walk_list);\n\t}\n}\n\n \nstatic u32 hweight_after_donation(struct ioc_gq *iocg, u32 old_hwi, u32 hwm,\n\t\t\t\t  u32 usage, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tu64 vtime = atomic64_read(&iocg->vtime);\n\ts64 excess, delta, target, new_hwi;\n\n\t \n\tif (iocg->abs_vdebt)\n\t\treturn 1;\n\n\t \n\tif (waitqueue_active(&iocg->waitq) ||\n\t    time_after64(vtime, now->vnow - ioc->margins.min))\n\t\treturn hwm;\n\n\t \n\texcess = now->vnow - vtime - ioc->margins.target;\n\tif (excess > 0) {\n\t\tatomic64_add(excess, &iocg->vtime);\n\t\tatomic64_add(excess, &iocg->done_vtime);\n\t\tvtime += excess;\n\t\tioc->vtime_err -= div64_u64(excess * old_hwi, WEIGHT_ONE);\n\t}\n\n\t \n\tdelta = div64_s64(WEIGHT_ONE * (now->vnow - vtime),\n\t\t\t  now->vnow - ioc->period_at_vtime);\n\ttarget = WEIGHT_ONE * MARGIN_TARGET_PCT / 100;\n\tnew_hwi = div64_s64(WEIGHT_ONE * usage, WEIGHT_ONE - target + delta);\n\n\treturn clamp_t(s64, new_hwi, 1, hwm);\n}\n\n \nstatic void transfer_surpluses(struct list_head *surpluses, struct ioc_now *now)\n{\n\tLIST_HEAD(over_hwa);\n\tLIST_HEAD(inner_walk);\n\tstruct ioc_gq *iocg, *tiocg, *root_iocg;\n\tu32 after_sum, over_sum, over_target, gamma;\n\n\t \n\tafter_sum = 0;\n\tover_sum = 0;\n\tlist_for_each_entry(iocg, surpluses, surplus_list) {\n\t\tu32 hwa;\n\n\t\tcurrent_hweight(iocg, &hwa, NULL);\n\t\tafter_sum += iocg->hweight_after_donation;\n\n\t\tif (iocg->hweight_after_donation > hwa) {\n\t\t\tover_sum += iocg->hweight_after_donation;\n\t\t\tlist_add(&iocg->walk_list, &over_hwa);\n\t\t}\n\t}\n\n\tif (after_sum >= WEIGHT_ONE) {\n\t\t \n\t\tu32 over_delta = after_sum - (WEIGHT_ONE - 1);\n\t\tWARN_ON_ONCE(over_sum <= over_delta);\n\t\tover_target = over_sum - over_delta;\n\t} else {\n\t\tover_target = 0;\n\t}\n\n\tlist_for_each_entry_safe(iocg, tiocg, &over_hwa, walk_list) {\n\t\tif (over_target)\n\t\t\tiocg->hweight_after_donation =\n\t\t\t\tdiv_u64((u64)iocg->hweight_after_donation *\n\t\t\t\t\tover_target, over_sum);\n\t\tlist_del_init(&iocg->walk_list);\n\t}\n\n\t \n\tlist_for_each_entry(iocg, surpluses, surplus_list) {\n\t\tiocg_build_inner_walk(iocg, &inner_walk);\n\t}\n\n\troot_iocg = list_first_entry(&inner_walk, struct ioc_gq, walk_list);\n\tWARN_ON_ONCE(root_iocg->level > 0);\n\n\tlist_for_each_entry(iocg, &inner_walk, walk_list) {\n\t\tiocg->child_adjusted_sum = 0;\n\t\tiocg->hweight_donating = 0;\n\t\tiocg->hweight_after_donation = 0;\n\t}\n\n\t \n\tlist_for_each_entry(iocg, surpluses, surplus_list) {\n\t\tstruct ioc_gq *parent = iocg->ancestors[iocg->level - 1];\n\n\t\tparent->hweight_donating += iocg->hweight_donating;\n\t\tparent->hweight_after_donation += iocg->hweight_after_donation;\n\t}\n\n\tlist_for_each_entry_reverse(iocg, &inner_walk, walk_list) {\n\t\tif (iocg->level > 0) {\n\t\t\tstruct ioc_gq *parent = iocg->ancestors[iocg->level - 1];\n\n\t\t\tparent->hweight_donating += iocg->hweight_donating;\n\t\t\tparent->hweight_after_donation += iocg->hweight_after_donation;\n\t\t}\n\t}\n\n\t \n\tlist_for_each_entry(iocg, &inner_walk, walk_list) {\n\t\tif (iocg->level) {\n\t\t\tstruct ioc_gq *parent = iocg->ancestors[iocg->level - 1];\n\n\t\t\tiocg->hweight_active = DIV64_U64_ROUND_UP(\n\t\t\t\t(u64)parent->hweight_active * iocg->active,\n\t\t\t\tparent->child_active_sum);\n\n\t\t}\n\n\t\tiocg->hweight_donating = min(iocg->hweight_donating,\n\t\t\t\t\t     iocg->hweight_active);\n\t\tiocg->hweight_after_donation = min(iocg->hweight_after_donation,\n\t\t\t\t\t\t   iocg->hweight_donating - 1);\n\t\tif (WARN_ON_ONCE(iocg->hweight_active <= 1 ||\n\t\t\t\t iocg->hweight_donating <= 1 ||\n\t\t\t\t iocg->hweight_after_donation == 0)) {\n\t\t\tpr_warn(\"iocg: invalid donation weights in \");\n\t\t\tpr_cont_cgroup_path(iocg_to_blkg(iocg)->blkcg->css.cgroup);\n\t\t\tpr_cont(\": active=%u donating=%u after=%u\\n\",\n\t\t\t\tiocg->hweight_active, iocg->hweight_donating,\n\t\t\t\tiocg->hweight_after_donation);\n\t\t}\n\t}\n\n\t \n\tgamma = DIV_ROUND_UP(\n\t\t(WEIGHT_ONE - root_iocg->hweight_after_donation) * WEIGHT_ONE,\n\t\tWEIGHT_ONE - min_t(u32, root_iocg->hweight_donating, WEIGHT_ONE - 1));\n\n\t \n\tlist_for_each_entry(iocg, &inner_walk, walk_list) {\n\t\tstruct ioc_gq *parent;\n\t\tu32 inuse, wpt, wptp;\n\t\tu64 st, sf;\n\n\t\tif (iocg->level == 0) {\n\t\t\t \n\t\t\tiocg->child_adjusted_sum = DIV64_U64_ROUND_UP(\n\t\t\t\tiocg->child_active_sum * (WEIGHT_ONE - iocg->hweight_donating),\n\t\t\t\tWEIGHT_ONE - iocg->hweight_after_donation);\n\t\t\tcontinue;\n\t\t}\n\n\t\tparent = iocg->ancestors[iocg->level - 1];\n\n\t\t \n\t\tiocg->hweight_inuse = DIV64_U64_ROUND_UP(\n\t\t\t(u64)gamma * (iocg->hweight_active - iocg->hweight_donating),\n\t\t\tWEIGHT_ONE) + iocg->hweight_after_donation;\n\n\t\t \n\t\tinuse = DIV64_U64_ROUND_UP(\n\t\t\t(u64)parent->child_adjusted_sum * iocg->hweight_inuse,\n\t\t\tparent->hweight_inuse);\n\n\t\t \n\t\tst = DIV64_U64_ROUND_UP(\n\t\t\tiocg->child_active_sum * iocg->hweight_donating,\n\t\t\tiocg->hweight_active);\n\t\tsf = iocg->child_active_sum - st;\n\t\twpt = DIV64_U64_ROUND_UP(\n\t\t\t(u64)iocg->active * iocg->hweight_donating,\n\t\t\tiocg->hweight_active);\n\t\twptp = DIV64_U64_ROUND_UP(\n\t\t\t(u64)inuse * iocg->hweight_after_donation,\n\t\t\tiocg->hweight_inuse);\n\n\t\tiocg->child_adjusted_sum = sf + DIV64_U64_ROUND_UP(st * wptp, wpt);\n\t}\n\n\t \n\tlist_for_each_entry(iocg, surpluses, surplus_list) {\n\t\tstruct ioc_gq *parent = iocg->ancestors[iocg->level - 1];\n\t\tu32 inuse;\n\n\t\t \n\t\tif (iocg->abs_vdebt) {\n\t\t\tWARN_ON_ONCE(iocg->inuse > 1);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tinuse = DIV64_U64_ROUND_UP(\n\t\t\tparent->child_adjusted_sum * iocg->hweight_after_donation,\n\t\t\tparent->hweight_inuse);\n\n\t\tTRACE_IOCG_PATH(inuse_transfer, iocg, now,\n\t\t\t\tiocg->inuse, inuse,\n\t\t\t\tiocg->hweight_inuse,\n\t\t\t\tiocg->hweight_after_donation);\n\n\t\t__propagate_weights(iocg, iocg->active, inuse, true, now);\n\t}\n\n\t \n\tlist_for_each_entry_safe(iocg, tiocg, &inner_walk, walk_list)\n\t\tlist_del_init(&iocg->walk_list);\n}\n\n \nstatic void ioc_forgive_debts(struct ioc *ioc, u64 usage_us_sum, int nr_debtors,\n\t\t\t      struct ioc_now *now)\n{\n\tstruct ioc_gq *iocg;\n\tu64 dur, usage_pct, nr_cycles;\n\n\t \n\tif (!nr_debtors) {\n\t\tioc->dfgv_period_at = now->now;\n\t\tioc->dfgv_period_rem = 0;\n\t\tioc->dfgv_usage_us_sum = 0;\n\t\treturn;\n\t}\n\n\t \n\tif (ioc->busy_level > 0)\n\t\tusage_us_sum = max_t(u64, usage_us_sum, ioc->period_us);\n\n\tioc->dfgv_usage_us_sum += usage_us_sum;\n\tif (time_before64(now->now, ioc->dfgv_period_at + DFGV_PERIOD))\n\t\treturn;\n\n\t \n\tdur = now->now - ioc->dfgv_period_at;\n\tusage_pct = div64_u64(100 * ioc->dfgv_usage_us_sum, dur);\n\n\tioc->dfgv_period_at = now->now;\n\tioc->dfgv_usage_us_sum = 0;\n\n\t \n\tif (usage_pct > DFGV_USAGE_PCT) {\n\t\tioc->dfgv_period_rem = 0;\n\t\treturn;\n\t}\n\n\t \n\tnr_cycles = dur + ioc->dfgv_period_rem;\n\tioc->dfgv_period_rem = do_div(nr_cycles, DFGV_PERIOD);\n\n\tlist_for_each_entry(iocg, &ioc->active_iocgs, active_list) {\n\t\tu64 __maybe_unused old_debt, __maybe_unused old_delay;\n\n\t\tif (!iocg->abs_vdebt && !iocg->delay)\n\t\t\tcontinue;\n\n\t\tspin_lock(&iocg->waitq.lock);\n\n\t\told_debt = iocg->abs_vdebt;\n\t\told_delay = iocg->delay;\n\n\t\tif (iocg->abs_vdebt)\n\t\t\tiocg->abs_vdebt = iocg->abs_vdebt >> nr_cycles ?: 1;\n\t\tif (iocg->delay)\n\t\t\tiocg->delay = iocg->delay >> nr_cycles ?: 1;\n\n\t\tiocg_kick_waitq(iocg, true, now);\n\n\t\tTRACE_IOCG_PATH(iocg_forgive_debt, iocg, now, usage_pct,\n\t\t\t\told_debt, iocg->abs_vdebt,\n\t\t\t\told_delay, iocg->delay);\n\n\t\tspin_unlock(&iocg->waitq.lock);\n\t}\n}\n\n \nstatic int ioc_check_iocgs(struct ioc *ioc, struct ioc_now *now)\n{\n\tint nr_debtors = 0;\n\tstruct ioc_gq *iocg, *tiocg;\n\n\tlist_for_each_entry_safe(iocg, tiocg, &ioc->active_iocgs, active_list) {\n\t\tif (!waitqueue_active(&iocg->waitq) && !iocg->abs_vdebt &&\n\t\t    !iocg->delay && !iocg_is_idle(iocg))\n\t\t\tcontinue;\n\n\t\tspin_lock(&iocg->waitq.lock);\n\n\t\t \n\t\tif (iocg->wait_since) {\n\t\t\tiocg->stat.wait_us += now->now - iocg->wait_since;\n\t\t\tiocg->wait_since = now->now;\n\t\t}\n\t\tif (iocg->indebt_since) {\n\t\t\tiocg->stat.indebt_us +=\n\t\t\t\tnow->now - iocg->indebt_since;\n\t\t\tiocg->indebt_since = now->now;\n\t\t}\n\t\tif (iocg->indelay_since) {\n\t\t\tiocg->stat.indelay_us +=\n\t\t\t\tnow->now - iocg->indelay_since;\n\t\t\tiocg->indelay_since = now->now;\n\t\t}\n\n\t\tif (waitqueue_active(&iocg->waitq) || iocg->abs_vdebt ||\n\t\t    iocg->delay) {\n\t\t\t \n\t\t\tiocg_kick_waitq(iocg, true, now);\n\t\t\tif (iocg->abs_vdebt || iocg->delay)\n\t\t\t\tnr_debtors++;\n\t\t} else if (iocg_is_idle(iocg)) {\n\t\t\t \n\t\t\tu64 vtime = atomic64_read(&iocg->vtime);\n\t\t\ts64 excess;\n\n\t\t\t \n\t\t\texcess = now->vnow - vtime - ioc->margins.target;\n\t\t\tif (excess > 0) {\n\t\t\t\tu32 old_hwi;\n\n\t\t\t\tcurrent_hweight(iocg, NULL, &old_hwi);\n\t\t\t\tioc->vtime_err -= div64_u64(excess * old_hwi,\n\t\t\t\t\t\t\t    WEIGHT_ONE);\n\t\t\t}\n\n\t\t\tTRACE_IOCG_PATH(iocg_idle, iocg, now,\n\t\t\t\t\tatomic64_read(&iocg->active_period),\n\t\t\t\t\tatomic64_read(&ioc->cur_period), vtime);\n\t\t\t__propagate_weights(iocg, 0, 0, false, now);\n\t\t\tlist_del_init(&iocg->active_list);\n\t\t}\n\n\t\tspin_unlock(&iocg->waitq.lock);\n\t}\n\n\tcommit_weights(ioc);\n\treturn nr_debtors;\n}\n\nstatic void ioc_timer_fn(struct timer_list *timer)\n{\n\tstruct ioc *ioc = container_of(timer, struct ioc, timer);\n\tstruct ioc_gq *iocg, *tiocg;\n\tstruct ioc_now now;\n\tLIST_HEAD(surpluses);\n\tint nr_debtors, nr_shortages = 0, nr_lagging = 0;\n\tu64 usage_us_sum = 0;\n\tu32 ppm_rthr;\n\tu32 ppm_wthr;\n\tu32 missed_ppm[2], rq_wait_pct;\n\tu64 period_vtime;\n\tint prev_busy_level;\n\n\t \n\tioc_lat_stat(ioc, missed_ppm, &rq_wait_pct);\n\n\t \n\tspin_lock_irq(&ioc->lock);\n\n\tppm_rthr = MILLION - ioc->params.qos[QOS_RPPM];\n\tppm_wthr = MILLION - ioc->params.qos[QOS_WPPM];\n\tioc_now(ioc, &now);\n\n\tperiod_vtime = now.vnow - ioc->period_at_vtime;\n\tif (WARN_ON_ONCE(!period_vtime)) {\n\t\tspin_unlock_irq(&ioc->lock);\n\t\treturn;\n\t}\n\n\tnr_debtors = ioc_check_iocgs(ioc, &now);\n\n\t \n\tiocg_flush_stat(&ioc->active_iocgs, &now);\n\n\t \n\tlist_for_each_entry(iocg, &ioc->active_iocgs, active_list) {\n\t\tu64 vdone, vtime, usage_us;\n\t\tu32 hw_active, hw_inuse;\n\n\t\t \n\t\tvdone = atomic64_read(&iocg->done_vtime);\n\t\tvtime = atomic64_read(&iocg->vtime);\n\t\tcurrent_hweight(iocg, &hw_active, &hw_inuse);\n\n\t\t \n\t\tif ((ppm_rthr != MILLION || ppm_wthr != MILLION) &&\n\t\t    !atomic_read(&iocg_to_blkg(iocg)->use_delay) &&\n\t\t    time_after64(vtime, vdone) &&\n\t\t    time_after64(vtime, now.vnow -\n\t\t\t\t MAX_LAGGING_PERIODS * period_vtime) &&\n\t\t    time_before64(vdone, now.vnow - period_vtime))\n\t\t\tnr_lagging++;\n\n\t\t \n\t\tusage_us = iocg->usage_delta_us;\n\t\tusage_us_sum += usage_us;\n\n\t\t \n\t\tWARN_ON_ONCE(!list_empty(&iocg->surplus_list));\n\t\tif (hw_inuse < hw_active ||\n\t\t    (!waitqueue_active(&iocg->waitq) &&\n\t\t     time_before64(vtime, now.vnow - ioc->margins.low))) {\n\t\t\tu32 hwa, old_hwi, hwm, new_hwi, usage;\n\t\t\tu64 usage_dur;\n\n\t\t\tif (vdone != vtime) {\n\t\t\t\tu64 inflight_us = DIV64_U64_ROUND_UP(\n\t\t\t\t\tcost_to_abs_cost(vtime - vdone, hw_inuse),\n\t\t\t\t\tioc->vtime_base_rate);\n\n\t\t\t\tusage_us = max(usage_us, inflight_us);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (time_after64(iocg->activated_at, ioc->period_at))\n\t\t\t\tusage_dur = max_t(u64, now.now - iocg->activated_at, 1);\n\t\t\telse\n\t\t\t\tusage_dur = max_t(u64, now.now - ioc->period_at, 1);\n\n\t\t\tusage = clamp_t(u32,\n\t\t\t\tDIV64_U64_ROUND_UP(usage_us * WEIGHT_ONE,\n\t\t\t\t\t\t   usage_dur),\n\t\t\t\t1, WEIGHT_ONE);\n\n\t\t\t \n\t\t\tcurrent_hweight(iocg, &hwa, &old_hwi);\n\t\t\thwm = current_hweight_max(iocg);\n\t\t\tnew_hwi = hweight_after_donation(iocg, old_hwi, hwm,\n\t\t\t\t\t\t\t usage, &now);\n\t\t\t \n\t\t\tif (new_hwi < hwm && hwa >= 2) {\n\t\t\t\tiocg->hweight_donating = hwa;\n\t\t\t\tiocg->hweight_after_donation = new_hwi;\n\t\t\t\tlist_add(&iocg->surplus_list, &surpluses);\n\t\t\t} else if (!iocg->abs_vdebt) {\n\t\t\t\t \n\t\t\t\tTRACE_IOCG_PATH(inuse_shortage, iocg, &now,\n\t\t\t\t\t\tiocg->inuse, iocg->active,\n\t\t\t\t\t\tiocg->hweight_inuse, new_hwi);\n\n\t\t\t\t__propagate_weights(iocg, iocg->active,\n\t\t\t\t\t\t    iocg->active, true, &now);\n\t\t\t\tnr_shortages++;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tnr_shortages++;\n\t\t}\n\t}\n\n\tif (!list_empty(&surpluses) && nr_shortages)\n\t\ttransfer_surpluses(&surpluses, &now);\n\n\tcommit_weights(ioc);\n\n\t \n\tlist_for_each_entry_safe(iocg, tiocg, &surpluses, surplus_list)\n\t\tlist_del_init(&iocg->surplus_list);\n\n\t \n\tprev_busy_level = ioc->busy_level;\n\tif (rq_wait_pct > RQ_WAIT_BUSY_PCT ||\n\t    missed_ppm[READ] > ppm_rthr ||\n\t    missed_ppm[WRITE] > ppm_wthr) {\n\t\t \n\t\tioc->busy_level = max(ioc->busy_level, 0);\n\t\tioc->busy_level++;\n\t} else if (rq_wait_pct <= RQ_WAIT_BUSY_PCT * UNBUSY_THR_PCT / 100 &&\n\t\t   missed_ppm[READ] <= ppm_rthr * UNBUSY_THR_PCT / 100 &&\n\t\t   missed_ppm[WRITE] <= ppm_wthr * UNBUSY_THR_PCT / 100) {\n\t\t \n\t\tif (nr_shortages) {\n\t\t\t \n\t\t\tioc->busy_level = min(ioc->busy_level, 0);\n\n\t\t\t \n\t\t\tif (!nr_lagging)\n\t\t\t\tioc->busy_level--;\n\t\t} else {\n\t\t\t \n\t\t\tioc->busy_level = 0;\n\t\t}\n\t} else {\n\t\t \n\t\tioc->busy_level = 0;\n\t}\n\n\tioc->busy_level = clamp(ioc->busy_level, -1000, 1000);\n\n\tioc_adjust_base_vrate(ioc, rq_wait_pct, nr_lagging, nr_shortages,\n\t\t\t      prev_busy_level, missed_ppm);\n\n\tioc_refresh_params(ioc, false);\n\n\tioc_forgive_debts(ioc, usage_us_sum, nr_debtors, &now);\n\n\t \n\tatomic64_inc(&ioc->cur_period);\n\n\tif (ioc->running != IOC_STOP) {\n\t\tif (!list_empty(&ioc->active_iocgs)) {\n\t\t\tioc_start_period(ioc, &now);\n\t\t} else {\n\t\t\tioc->busy_level = 0;\n\t\t\tioc->vtime_err = 0;\n\t\t\tioc->running = IOC_IDLE;\n\t\t}\n\n\t\tioc_refresh_vrate(ioc, &now);\n\t}\n\n\tspin_unlock_irq(&ioc->lock);\n}\n\nstatic u64 adjust_inuse_and_calc_cost(struct ioc_gq *iocg, u64 vtime,\n\t\t\t\t      u64 abs_cost, struct ioc_now *now)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tstruct ioc_margins *margins = &ioc->margins;\n\tu32 __maybe_unused old_inuse = iocg->inuse, __maybe_unused old_hwi;\n\tu32 hwi, adj_step;\n\ts64 margin;\n\tu64 cost, new_inuse;\n\tunsigned long flags;\n\n\tcurrent_hweight(iocg, NULL, &hwi);\n\told_hwi = hwi;\n\tcost = abs_cost_to_cost(abs_cost, hwi);\n\tmargin = now->vnow - vtime - cost;\n\n\t \n\tif (iocg->abs_vdebt)\n\t\treturn cost;\n\n\t \n\tif (margin >= iocg->saved_margin || margin >= margins->low ||\n\t    iocg->inuse == iocg->active)\n\t\treturn cost;\n\n\tspin_lock_irqsave(&ioc->lock, flags);\n\n\t \n\tif (iocg->abs_vdebt || list_empty(&iocg->active_list)) {\n\t\tspin_unlock_irqrestore(&ioc->lock, flags);\n\t\treturn cost;\n\t}\n\n\t \n\tnew_inuse = iocg->inuse;\n\tadj_step = DIV_ROUND_UP(iocg->active * INUSE_ADJ_STEP_PCT, 100);\n\tdo {\n\t\tnew_inuse = new_inuse + adj_step;\n\t\tpropagate_weights(iocg, iocg->active, new_inuse, true, now);\n\t\tcurrent_hweight(iocg, NULL, &hwi);\n\t\tcost = abs_cost_to_cost(abs_cost, hwi);\n\t} while (time_after64(vtime + cost, now->vnow) &&\n\t\t iocg->inuse != iocg->active);\n\n\tspin_unlock_irqrestore(&ioc->lock, flags);\n\n\tTRACE_IOCG_PATH(inuse_adjust, iocg, now,\n\t\t\told_inuse, iocg->inuse, old_hwi, hwi);\n\n\treturn cost;\n}\n\nstatic void calc_vtime_cost_builtin(struct bio *bio, struct ioc_gq *iocg,\n\t\t\t\t    bool is_merge, u64 *costp)\n{\n\tstruct ioc *ioc = iocg->ioc;\n\tu64 coef_seqio, coef_randio, coef_page;\n\tu64 pages = max_t(u64, bio_sectors(bio) >> IOC_SECT_TO_PAGE_SHIFT, 1);\n\tu64 seek_pages = 0;\n\tu64 cost = 0;\n\n\t \n\tif (!bio->bi_iter.bi_size)\n\t\tgoto out;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_READ:\n\t\tcoef_seqio\t= ioc->params.lcoefs[LCOEF_RSEQIO];\n\t\tcoef_randio\t= ioc->params.lcoefs[LCOEF_RRANDIO];\n\t\tcoef_page\t= ioc->params.lcoefs[LCOEF_RPAGE];\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\tcoef_seqio\t= ioc->params.lcoefs[LCOEF_WSEQIO];\n\t\tcoef_randio\t= ioc->params.lcoefs[LCOEF_WRANDIO];\n\t\tcoef_page\t= ioc->params.lcoefs[LCOEF_WPAGE];\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tif (iocg->cursor) {\n\t\tseek_pages = abs(bio->bi_iter.bi_sector - iocg->cursor);\n\t\tseek_pages >>= IOC_SECT_TO_PAGE_SHIFT;\n\t}\n\n\tif (!is_merge) {\n\t\tif (seek_pages > LCOEF_RANDIO_PAGES) {\n\t\t\tcost += coef_randio;\n\t\t} else {\n\t\t\tcost += coef_seqio;\n\t\t}\n\t}\n\tcost += pages * coef_page;\nout:\n\t*costp = cost;\n}\n\nstatic u64 calc_vtime_cost(struct bio *bio, struct ioc_gq *iocg, bool is_merge)\n{\n\tu64 cost;\n\n\tcalc_vtime_cost_builtin(bio, iocg, is_merge, &cost);\n\treturn cost;\n}\n\nstatic void calc_size_vtime_cost_builtin(struct request *rq, struct ioc *ioc,\n\t\t\t\t\t u64 *costp)\n{\n\tunsigned int pages = blk_rq_stats_sectors(rq) >> IOC_SECT_TO_PAGE_SHIFT;\n\n\tswitch (req_op(rq)) {\n\tcase REQ_OP_READ:\n\t\t*costp = pages * ioc->params.lcoefs[LCOEF_RPAGE];\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\t*costp = pages * ioc->params.lcoefs[LCOEF_WPAGE];\n\t\tbreak;\n\tdefault:\n\t\t*costp = 0;\n\t}\n}\n\nstatic u64 calc_size_vtime_cost(struct request *rq, struct ioc *ioc)\n{\n\tu64 cost;\n\n\tcalc_size_vtime_cost_builtin(rq, ioc, &cost);\n\treturn cost;\n}\n\nstatic void ioc_rqos_throttle(struct rq_qos *rqos, struct bio *bio)\n{\n\tstruct blkcg_gq *blkg = bio->bi_blkg;\n\tstruct ioc *ioc = rqos_to_ioc(rqos);\n\tstruct ioc_gq *iocg = blkg_to_iocg(blkg);\n\tstruct ioc_now now;\n\tstruct iocg_wait wait;\n\tu64 abs_cost, cost, vtime;\n\tbool use_debt, ioc_locked;\n\tunsigned long flags;\n\n\t \n\tif (!ioc->enabled || !iocg || !iocg->level)\n\t\treturn;\n\n\t \n\tabs_cost = calc_vtime_cost(bio, iocg, false);\n\tif (!abs_cost)\n\t\treturn;\n\n\tif (!iocg_activate(iocg, &now))\n\t\treturn;\n\n\tiocg->cursor = bio_end_sector(bio);\n\tvtime = atomic64_read(&iocg->vtime);\n\tcost = adjust_inuse_and_calc_cost(iocg, vtime, abs_cost, &now);\n\n\t \n\tif (!waitqueue_active(&iocg->waitq) && !iocg->abs_vdebt &&\n\t    time_before_eq64(vtime + cost, now.vnow)) {\n\t\tiocg_commit_bio(iocg, bio, abs_cost, cost);\n\t\treturn;\n\t}\n\n\t \n\tuse_debt = bio_issue_as_root_blkg(bio) || fatal_signal_pending(current);\n\tioc_locked = use_debt || READ_ONCE(iocg->abs_vdebt);\nretry_lock:\n\tiocg_lock(iocg, ioc_locked, &flags);\n\n\t \n\tif (unlikely(list_empty(&iocg->active_list))) {\n\t\tiocg_unlock(iocg, ioc_locked, &flags);\n\t\tiocg_commit_bio(iocg, bio, abs_cost, cost);\n\t\treturn;\n\t}\n\n\t \n\tif (use_debt) {\n\t\tiocg_incur_debt(iocg, abs_cost, &now);\n\t\tif (iocg_kick_delay(iocg, &now))\n\t\t\tblkcg_schedule_throttle(rqos->disk,\n\t\t\t\t\t(bio->bi_opf & REQ_SWAP) == REQ_SWAP);\n\t\tiocg_unlock(iocg, ioc_locked, &flags);\n\t\treturn;\n\t}\n\n\t \n\tif (!iocg->abs_vdebt && iocg->inuse != iocg->active) {\n\t\tif (!ioc_locked) {\n\t\t\tiocg_unlock(iocg, false, &flags);\n\t\t\tioc_locked = true;\n\t\t\tgoto retry_lock;\n\t\t}\n\t\tpropagate_weights(iocg, iocg->active, iocg->active, true,\n\t\t\t\t  &now);\n\t}\n\n\t \n\tinit_waitqueue_func_entry(&wait.wait, iocg_wake_fn);\n\twait.wait.private = current;\n\twait.bio = bio;\n\twait.abs_cost = abs_cost;\n\twait.committed = false;\t \n\n\t__add_wait_queue_entry_tail(&iocg->waitq, &wait.wait);\n\tiocg_kick_waitq(iocg, ioc_locked, &now);\n\n\tiocg_unlock(iocg, ioc_locked, &flags);\n\n\twhile (true) {\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tif (wait.committed)\n\t\t\tbreak;\n\t\tio_schedule();\n\t}\n\n\t \n\tfinish_wait(&iocg->waitq, &wait.wait);\n}\n\nstatic void ioc_rqos_merge(struct rq_qos *rqos, struct request *rq,\n\t\t\t   struct bio *bio)\n{\n\tstruct ioc_gq *iocg = blkg_to_iocg(bio->bi_blkg);\n\tstruct ioc *ioc = rqos_to_ioc(rqos);\n\tsector_t bio_end = bio_end_sector(bio);\n\tstruct ioc_now now;\n\tu64 vtime, abs_cost, cost;\n\tunsigned long flags;\n\n\t \n\tif (!ioc->enabled || !iocg || !iocg->level)\n\t\treturn;\n\n\tabs_cost = calc_vtime_cost(bio, iocg, true);\n\tif (!abs_cost)\n\t\treturn;\n\n\tioc_now(ioc, &now);\n\n\tvtime = atomic64_read(&iocg->vtime);\n\tcost = adjust_inuse_and_calc_cost(iocg, vtime, abs_cost, &now);\n\n\t \n\tif (blk_rq_pos(rq) < bio_end &&\n\t    blk_rq_pos(rq) + blk_rq_sectors(rq) == iocg->cursor)\n\t\tiocg->cursor = bio_end;\n\n\t \n\tif (rq->bio && rq->bio->bi_iocost_cost &&\n\t    time_before_eq64(atomic64_read(&iocg->vtime) + cost, now.vnow)) {\n\t\tiocg_commit_bio(iocg, bio, abs_cost, cost);\n\t\treturn;\n\t}\n\n\t \n\tspin_lock_irqsave(&ioc->lock, flags);\n\tspin_lock(&iocg->waitq.lock);\n\n\tif (likely(!list_empty(&iocg->active_list))) {\n\t\tiocg_incur_debt(iocg, abs_cost, &now);\n\t\tif (iocg_kick_delay(iocg, &now))\n\t\t\tblkcg_schedule_throttle(rqos->disk,\n\t\t\t\t\t(bio->bi_opf & REQ_SWAP) == REQ_SWAP);\n\t} else {\n\t\tiocg_commit_bio(iocg, bio, abs_cost, cost);\n\t}\n\n\tspin_unlock(&iocg->waitq.lock);\n\tspin_unlock_irqrestore(&ioc->lock, flags);\n}\n\nstatic void ioc_rqos_done_bio(struct rq_qos *rqos, struct bio *bio)\n{\n\tstruct ioc_gq *iocg = blkg_to_iocg(bio->bi_blkg);\n\n\tif (iocg && bio->bi_iocost_cost)\n\t\tatomic64_add(bio->bi_iocost_cost, &iocg->done_vtime);\n}\n\nstatic void ioc_rqos_done(struct rq_qos *rqos, struct request *rq)\n{\n\tstruct ioc *ioc = rqos_to_ioc(rqos);\n\tstruct ioc_pcpu_stat *ccs;\n\tu64 on_q_ns, rq_wait_ns, size_nsec;\n\tint pidx, rw;\n\n\tif (!ioc->enabled || !rq->alloc_time_ns || !rq->start_time_ns)\n\t\treturn;\n\n\tswitch (req_op(rq)) {\n\tcase REQ_OP_READ:\n\t\tpidx = QOS_RLAT;\n\t\trw = READ;\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\tpidx = QOS_WLAT;\n\t\trw = WRITE;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\ton_q_ns = ktime_get_ns() - rq->alloc_time_ns;\n\trq_wait_ns = rq->start_time_ns - rq->alloc_time_ns;\n\tsize_nsec = div64_u64(calc_size_vtime_cost(rq, ioc), VTIME_PER_NSEC);\n\n\tccs = get_cpu_ptr(ioc->pcpu_stat);\n\n\tif (on_q_ns <= size_nsec ||\n\t    on_q_ns - size_nsec <= ioc->params.qos[pidx] * NSEC_PER_USEC)\n\t\tlocal_inc(&ccs->missed[rw].nr_met);\n\telse\n\t\tlocal_inc(&ccs->missed[rw].nr_missed);\n\n\tlocal64_add(rq_wait_ns, &ccs->rq_wait_ns);\n\n\tput_cpu_ptr(ccs);\n}\n\nstatic void ioc_rqos_queue_depth_changed(struct rq_qos *rqos)\n{\n\tstruct ioc *ioc = rqos_to_ioc(rqos);\n\n\tspin_lock_irq(&ioc->lock);\n\tioc_refresh_params(ioc, false);\n\tspin_unlock_irq(&ioc->lock);\n}\n\nstatic void ioc_rqos_exit(struct rq_qos *rqos)\n{\n\tstruct ioc *ioc = rqos_to_ioc(rqos);\n\n\tblkcg_deactivate_policy(rqos->disk, &blkcg_policy_iocost);\n\n\tspin_lock_irq(&ioc->lock);\n\tioc->running = IOC_STOP;\n\tspin_unlock_irq(&ioc->lock);\n\n\ttimer_shutdown_sync(&ioc->timer);\n\tfree_percpu(ioc->pcpu_stat);\n\tkfree(ioc);\n}\n\nstatic const struct rq_qos_ops ioc_rqos_ops = {\n\t.throttle = ioc_rqos_throttle,\n\t.merge = ioc_rqos_merge,\n\t.done_bio = ioc_rqos_done_bio,\n\t.done = ioc_rqos_done,\n\t.queue_depth_changed = ioc_rqos_queue_depth_changed,\n\t.exit = ioc_rqos_exit,\n};\n\nstatic int blk_iocost_init(struct gendisk *disk)\n{\n\tstruct ioc *ioc;\n\tint i, cpu, ret;\n\n\tioc = kzalloc(sizeof(*ioc), GFP_KERNEL);\n\tif (!ioc)\n\t\treturn -ENOMEM;\n\n\tioc->pcpu_stat = alloc_percpu(struct ioc_pcpu_stat);\n\tif (!ioc->pcpu_stat) {\n\t\tkfree(ioc);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct ioc_pcpu_stat *ccs = per_cpu_ptr(ioc->pcpu_stat, cpu);\n\n\t\tfor (i = 0; i < ARRAY_SIZE(ccs->missed); i++) {\n\t\t\tlocal_set(&ccs->missed[i].nr_met, 0);\n\t\t\tlocal_set(&ccs->missed[i].nr_missed, 0);\n\t\t}\n\t\tlocal64_set(&ccs->rq_wait_ns, 0);\n\t}\n\n\tspin_lock_init(&ioc->lock);\n\ttimer_setup(&ioc->timer, ioc_timer_fn, 0);\n\tINIT_LIST_HEAD(&ioc->active_iocgs);\n\n\tioc->running = IOC_IDLE;\n\tioc->vtime_base_rate = VTIME_PER_USEC;\n\tatomic64_set(&ioc->vtime_rate, VTIME_PER_USEC);\n\tseqcount_spinlock_init(&ioc->period_seqcount, &ioc->lock);\n\tioc->period_at = ktime_to_us(ktime_get());\n\tatomic64_set(&ioc->cur_period, 0);\n\tatomic_set(&ioc->hweight_gen, 0);\n\n\tspin_lock_irq(&ioc->lock);\n\tioc->autop_idx = AUTOP_INVALID;\n\tioc_refresh_params_disk(ioc, true, disk);\n\tspin_unlock_irq(&ioc->lock);\n\n\t \n\tret = rq_qos_add(&ioc->rqos, disk, RQ_QOS_COST, &ioc_rqos_ops);\n\tif (ret)\n\t\tgoto err_free_ioc;\n\n\tret = blkcg_activate_policy(disk, &blkcg_policy_iocost);\n\tif (ret)\n\t\tgoto err_del_qos;\n\treturn 0;\n\nerr_del_qos:\n\trq_qos_del(&ioc->rqos);\nerr_free_ioc:\n\tfree_percpu(ioc->pcpu_stat);\n\tkfree(ioc);\n\treturn ret;\n}\n\nstatic struct blkcg_policy_data *ioc_cpd_alloc(gfp_t gfp)\n{\n\tstruct ioc_cgrp *iocc;\n\n\tiocc = kzalloc(sizeof(struct ioc_cgrp), gfp);\n\tif (!iocc)\n\t\treturn NULL;\n\n\tiocc->dfl_weight = CGROUP_WEIGHT_DFL * WEIGHT_ONE;\n\treturn &iocc->cpd;\n}\n\nstatic void ioc_cpd_free(struct blkcg_policy_data *cpd)\n{\n\tkfree(container_of(cpd, struct ioc_cgrp, cpd));\n}\n\nstatic struct blkg_policy_data *ioc_pd_alloc(struct gendisk *disk,\n\t\tstruct blkcg *blkcg, gfp_t gfp)\n{\n\tint levels = blkcg->css.cgroup->level + 1;\n\tstruct ioc_gq *iocg;\n\n\tiocg = kzalloc_node(struct_size(iocg, ancestors, levels), gfp,\n\t\t\t    disk->node_id);\n\tif (!iocg)\n\t\treturn NULL;\n\n\tiocg->pcpu_stat = alloc_percpu_gfp(struct iocg_pcpu_stat, gfp);\n\tif (!iocg->pcpu_stat) {\n\t\tkfree(iocg);\n\t\treturn NULL;\n\t}\n\n\treturn &iocg->pd;\n}\n\nstatic void ioc_pd_init(struct blkg_policy_data *pd)\n{\n\tstruct ioc_gq *iocg = pd_to_iocg(pd);\n\tstruct blkcg_gq *blkg = pd_to_blkg(&iocg->pd);\n\tstruct ioc *ioc = q_to_ioc(blkg->q);\n\tstruct ioc_now now;\n\tstruct blkcg_gq *tblkg;\n\tunsigned long flags;\n\n\tioc_now(ioc, &now);\n\n\tiocg->ioc = ioc;\n\tatomic64_set(&iocg->vtime, now.vnow);\n\tatomic64_set(&iocg->done_vtime, now.vnow);\n\tatomic64_set(&iocg->active_period, atomic64_read(&ioc->cur_period));\n\tINIT_LIST_HEAD(&iocg->active_list);\n\tINIT_LIST_HEAD(&iocg->walk_list);\n\tINIT_LIST_HEAD(&iocg->surplus_list);\n\tiocg->hweight_active = WEIGHT_ONE;\n\tiocg->hweight_inuse = WEIGHT_ONE;\n\n\tinit_waitqueue_head(&iocg->waitq);\n\thrtimer_init(&iocg->waitq_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);\n\tiocg->waitq_timer.function = iocg_waitq_timer_fn;\n\n\tiocg->level = blkg->blkcg->css.cgroup->level;\n\n\tfor (tblkg = blkg; tblkg; tblkg = tblkg->parent) {\n\t\tstruct ioc_gq *tiocg = blkg_to_iocg(tblkg);\n\t\tiocg->ancestors[tiocg->level] = tiocg;\n\t}\n\n\tspin_lock_irqsave(&ioc->lock, flags);\n\tweight_updated(iocg, &now);\n\tspin_unlock_irqrestore(&ioc->lock, flags);\n}\n\nstatic void ioc_pd_free(struct blkg_policy_data *pd)\n{\n\tstruct ioc_gq *iocg = pd_to_iocg(pd);\n\tstruct ioc *ioc = iocg->ioc;\n\tunsigned long flags;\n\n\tif (ioc) {\n\t\tspin_lock_irqsave(&ioc->lock, flags);\n\n\t\tif (!list_empty(&iocg->active_list)) {\n\t\t\tstruct ioc_now now;\n\n\t\t\tioc_now(ioc, &now);\n\t\t\tpropagate_weights(iocg, 0, 0, false, &now);\n\t\t\tlist_del_init(&iocg->active_list);\n\t\t}\n\n\t\tWARN_ON_ONCE(!list_empty(&iocg->walk_list));\n\t\tWARN_ON_ONCE(!list_empty(&iocg->surplus_list));\n\n\t\tspin_unlock_irqrestore(&ioc->lock, flags);\n\n\t\thrtimer_cancel(&iocg->waitq_timer);\n\t}\n\tfree_percpu(iocg->pcpu_stat);\n\tkfree(iocg);\n}\n\nstatic void ioc_pd_stat(struct blkg_policy_data *pd, struct seq_file *s)\n{\n\tstruct ioc_gq *iocg = pd_to_iocg(pd);\n\tstruct ioc *ioc = iocg->ioc;\n\n\tif (!ioc->enabled)\n\t\treturn;\n\n\tif (iocg->level == 0) {\n\t\tunsigned vp10k = DIV64_U64_ROUND_CLOSEST(\n\t\t\tioc->vtime_base_rate * 10000,\n\t\t\tVTIME_PER_USEC);\n\t\tseq_printf(s, \" cost.vrate=%u.%02u\", vp10k / 100, vp10k % 100);\n\t}\n\n\tseq_printf(s, \" cost.usage=%llu\", iocg->last_stat.usage_us);\n\n\tif (blkcg_debug_stats)\n\t\tseq_printf(s, \" cost.wait=%llu cost.indebt=%llu cost.indelay=%llu\",\n\t\t\tiocg->last_stat.wait_us,\n\t\t\tiocg->last_stat.indebt_us,\n\t\t\tiocg->last_stat.indelay_us);\n}\n\nstatic u64 ioc_weight_prfill(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t     int off)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tstruct ioc_gq *iocg = pd_to_iocg(pd);\n\n\tif (dname && iocg->cfg_weight)\n\t\tseq_printf(sf, \"%s %u\\n\", dname, iocg->cfg_weight / WEIGHT_ONE);\n\treturn 0;\n}\n\n\nstatic int ioc_weight_show(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\tstruct ioc_cgrp *iocc = blkcg_to_iocc(blkcg);\n\n\tseq_printf(sf, \"default %u\\n\", iocc->dfl_weight / WEIGHT_ONE);\n\tblkcg_print_blkgs(sf, blkcg, ioc_weight_prfill,\n\t\t\t  &blkcg_policy_iocost, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic ssize_t ioc_weight_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\tsize_t nbytes, loff_t off)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(of_css(of));\n\tstruct ioc_cgrp *iocc = blkcg_to_iocc(blkcg);\n\tstruct blkg_conf_ctx ctx;\n\tstruct ioc_now now;\n\tstruct ioc_gq *iocg;\n\tu32 v;\n\tint ret;\n\n\tif (!strchr(buf, ':')) {\n\t\tstruct blkcg_gq *blkg;\n\n\t\tif (!sscanf(buf, \"default %u\", &v) && !sscanf(buf, \"%u\", &v))\n\t\t\treturn -EINVAL;\n\n\t\tif (v < CGROUP_WEIGHT_MIN || v > CGROUP_WEIGHT_MAX)\n\t\t\treturn -EINVAL;\n\n\t\tspin_lock_irq(&blkcg->lock);\n\t\tiocc->dfl_weight = v * WEIGHT_ONE;\n\t\thlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\t\tstruct ioc_gq *iocg = blkg_to_iocg(blkg);\n\n\t\t\tif (iocg) {\n\t\t\t\tspin_lock(&iocg->ioc->lock);\n\t\t\t\tioc_now(iocg->ioc, &now);\n\t\t\t\tweight_updated(iocg, &now);\n\t\t\t\tspin_unlock(&iocg->ioc->lock);\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&blkcg->lock);\n\n\t\treturn nbytes;\n\t}\n\n\tblkg_conf_init(&ctx, buf);\n\n\tret = blkg_conf_prep(blkcg, &blkcg_policy_iocost, &ctx);\n\tif (ret)\n\t\tgoto err;\n\n\tiocg = blkg_to_iocg(ctx.blkg);\n\n\tif (!strncmp(ctx.body, \"default\", 7)) {\n\t\tv = 0;\n\t} else {\n\t\tif (!sscanf(ctx.body, \"%u\", &v))\n\t\t\tgoto einval;\n\t\tif (v < CGROUP_WEIGHT_MIN || v > CGROUP_WEIGHT_MAX)\n\t\t\tgoto einval;\n\t}\n\n\tspin_lock(&iocg->ioc->lock);\n\tiocg->cfg_weight = v * WEIGHT_ONE;\n\tioc_now(iocg->ioc, &now);\n\tweight_updated(iocg, &now);\n\tspin_unlock(&iocg->ioc->lock);\n\n\tblkg_conf_exit(&ctx);\n\treturn nbytes;\n\neinval:\n\tret = -EINVAL;\nerr:\n\tblkg_conf_exit(&ctx);\n\treturn ret;\n}\n\nstatic u64 ioc_qos_prfill(struct seq_file *sf, struct blkg_policy_data *pd,\n\t\t\t  int off)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tstruct ioc *ioc = pd_to_iocg(pd)->ioc;\n\n\tif (!dname)\n\t\treturn 0;\n\n\tspin_lock_irq(&ioc->lock);\n\tseq_printf(sf, \"%s enable=%d ctrl=%s rpct=%u.%02u rlat=%u wpct=%u.%02u wlat=%u min=%u.%02u max=%u.%02u\\n\",\n\t\t   dname, ioc->enabled, ioc->user_qos_params ? \"user\" : \"auto\",\n\t\t   ioc->params.qos[QOS_RPPM] / 10000,\n\t\t   ioc->params.qos[QOS_RPPM] % 10000 / 100,\n\t\t   ioc->params.qos[QOS_RLAT],\n\t\t   ioc->params.qos[QOS_WPPM] / 10000,\n\t\t   ioc->params.qos[QOS_WPPM] % 10000 / 100,\n\t\t   ioc->params.qos[QOS_WLAT],\n\t\t   ioc->params.qos[QOS_MIN] / 10000,\n\t\t   ioc->params.qos[QOS_MIN] % 10000 / 100,\n\t\t   ioc->params.qos[QOS_MAX] / 10000,\n\t\t   ioc->params.qos[QOS_MAX] % 10000 / 100);\n\tspin_unlock_irq(&ioc->lock);\n\treturn 0;\n}\n\nstatic int ioc_qos_show(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\n\tblkcg_print_blkgs(sf, blkcg, ioc_qos_prfill,\n\t\t\t  &blkcg_policy_iocost, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic const match_table_t qos_ctrl_tokens = {\n\t{ QOS_ENABLE,\t\t\"enable=%u\"\t},\n\t{ QOS_CTRL,\t\t\"ctrl=%s\"\t},\n\t{ NR_QOS_CTRL_PARAMS,\tNULL\t\t},\n};\n\nstatic const match_table_t qos_tokens = {\n\t{ QOS_RPPM,\t\t\"rpct=%s\"\t},\n\t{ QOS_RLAT,\t\t\"rlat=%u\"\t},\n\t{ QOS_WPPM,\t\t\"wpct=%s\"\t},\n\t{ QOS_WLAT,\t\t\"wlat=%u\"\t},\n\t{ QOS_MIN,\t\t\"min=%s\"\t},\n\t{ QOS_MAX,\t\t\"max=%s\"\t},\n\t{ NR_QOS_PARAMS,\tNULL\t\t},\n};\n\nstatic ssize_t ioc_qos_write(struct kernfs_open_file *of, char *input,\n\t\t\t     size_t nbytes, loff_t off)\n{\n\tstruct blkg_conf_ctx ctx;\n\tstruct gendisk *disk;\n\tstruct ioc *ioc;\n\tu32 qos[NR_QOS_PARAMS];\n\tbool enable, user;\n\tchar *body, *p;\n\tint ret;\n\n\tblkg_conf_init(&ctx, input);\n\n\tret = blkg_conf_open_bdev(&ctx);\n\tif (ret)\n\t\tgoto err;\n\n\tbody = ctx.body;\n\tdisk = ctx.bdev->bd_disk;\n\tif (!queue_is_mq(disk->queue)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto err;\n\t}\n\n\tioc = q_to_ioc(disk->queue);\n\tif (!ioc) {\n\t\tret = blk_iocost_init(disk);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tioc = q_to_ioc(disk->queue);\n\t}\n\n\tblk_mq_freeze_queue(disk->queue);\n\tblk_mq_quiesce_queue(disk->queue);\n\n\tspin_lock_irq(&ioc->lock);\n\tmemcpy(qos, ioc->params.qos, sizeof(qos));\n\tenable = ioc->enabled;\n\tuser = ioc->user_qos_params;\n\n\twhile ((p = strsep(&body, \" \\t\\n\"))) {\n\t\tsubstring_t args[MAX_OPT_ARGS];\n\t\tchar buf[32];\n\t\tint tok;\n\t\ts64 v;\n\n\t\tif (!*p)\n\t\t\tcontinue;\n\n\t\tswitch (match_token(p, qos_ctrl_tokens, args)) {\n\t\tcase QOS_ENABLE:\n\t\t\tif (match_u64(&args[0], &v))\n\t\t\t\tgoto einval;\n\t\t\tenable = v;\n\t\t\tcontinue;\n\t\tcase QOS_CTRL:\n\t\t\tmatch_strlcpy(buf, &args[0], sizeof(buf));\n\t\t\tif (!strcmp(buf, \"auto\"))\n\t\t\t\tuser = false;\n\t\t\telse if (!strcmp(buf, \"user\"))\n\t\t\t\tuser = true;\n\t\t\telse\n\t\t\t\tgoto einval;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttok = match_token(p, qos_tokens, args);\n\t\tswitch (tok) {\n\t\tcase QOS_RPPM:\n\t\tcase QOS_WPPM:\n\t\t\tif (match_strlcpy(buf, &args[0], sizeof(buf)) >=\n\t\t\t    sizeof(buf))\n\t\t\t\tgoto einval;\n\t\t\tif (cgroup_parse_float(buf, 2, &v))\n\t\t\t\tgoto einval;\n\t\t\tif (v < 0 || v > 10000)\n\t\t\t\tgoto einval;\n\t\t\tqos[tok] = v * 100;\n\t\t\tbreak;\n\t\tcase QOS_RLAT:\n\t\tcase QOS_WLAT:\n\t\t\tif (match_u64(&args[0], &v))\n\t\t\t\tgoto einval;\n\t\t\tqos[tok] = v;\n\t\t\tbreak;\n\t\tcase QOS_MIN:\n\t\tcase QOS_MAX:\n\t\t\tif (match_strlcpy(buf, &args[0], sizeof(buf)) >=\n\t\t\t    sizeof(buf))\n\t\t\t\tgoto einval;\n\t\t\tif (cgroup_parse_float(buf, 2, &v))\n\t\t\t\tgoto einval;\n\t\t\tif (v < 0)\n\t\t\t\tgoto einval;\n\t\t\tqos[tok] = clamp_t(s64, v * 100,\n\t\t\t\t\t   VRATE_MIN_PPM, VRATE_MAX_PPM);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto einval;\n\t\t}\n\t\tuser = true;\n\t}\n\n\tif (qos[QOS_MIN] > qos[QOS_MAX])\n\t\tgoto einval;\n\n\tif (enable && !ioc->enabled) {\n\t\tblk_stat_enable_accounting(disk->queue);\n\t\tblk_queue_flag_set(QUEUE_FLAG_RQ_ALLOC_TIME, disk->queue);\n\t\tioc->enabled = true;\n\t} else if (!enable && ioc->enabled) {\n\t\tblk_stat_disable_accounting(disk->queue);\n\t\tblk_queue_flag_clear(QUEUE_FLAG_RQ_ALLOC_TIME, disk->queue);\n\t\tioc->enabled = false;\n\t}\n\n\tif (user) {\n\t\tmemcpy(ioc->params.qos, qos, sizeof(qos));\n\t\tioc->user_qos_params = true;\n\t} else {\n\t\tioc->user_qos_params = false;\n\t}\n\n\tioc_refresh_params(ioc, true);\n\tspin_unlock_irq(&ioc->lock);\n\n\tif (enable)\n\t\twbt_disable_default(disk);\n\telse\n\t\twbt_enable_default(disk);\n\n\tblk_mq_unquiesce_queue(disk->queue);\n\tblk_mq_unfreeze_queue(disk->queue);\n\n\tblkg_conf_exit(&ctx);\n\treturn nbytes;\neinval:\n\tspin_unlock_irq(&ioc->lock);\n\n\tblk_mq_unquiesce_queue(disk->queue);\n\tblk_mq_unfreeze_queue(disk->queue);\n\n\tret = -EINVAL;\nerr:\n\tblkg_conf_exit(&ctx);\n\treturn ret;\n}\n\nstatic u64 ioc_cost_model_prfill(struct seq_file *sf,\n\t\t\t\t struct blkg_policy_data *pd, int off)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\tstruct ioc *ioc = pd_to_iocg(pd)->ioc;\n\tu64 *u = ioc->params.i_lcoefs;\n\n\tif (!dname)\n\t\treturn 0;\n\n\tspin_lock_irq(&ioc->lock);\n\tseq_printf(sf, \"%s ctrl=%s model=linear \"\n\t\t   \"rbps=%llu rseqiops=%llu rrandiops=%llu \"\n\t\t   \"wbps=%llu wseqiops=%llu wrandiops=%llu\\n\",\n\t\t   dname, ioc->user_cost_model ? \"user\" : \"auto\",\n\t\t   u[I_LCOEF_RBPS], u[I_LCOEF_RSEQIOPS], u[I_LCOEF_RRANDIOPS],\n\t\t   u[I_LCOEF_WBPS], u[I_LCOEF_WSEQIOPS], u[I_LCOEF_WRANDIOPS]);\n\tspin_unlock_irq(&ioc->lock);\n\treturn 0;\n}\n\nstatic int ioc_cost_model_show(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\n\tblkcg_print_blkgs(sf, blkcg, ioc_cost_model_prfill,\n\t\t\t  &blkcg_policy_iocost, seq_cft(sf)->private, false);\n\treturn 0;\n}\n\nstatic const match_table_t cost_ctrl_tokens = {\n\t{ COST_CTRL,\t\t\"ctrl=%s\"\t},\n\t{ COST_MODEL,\t\t\"model=%s\"\t},\n\t{ NR_COST_CTRL_PARAMS,\tNULL\t\t},\n};\n\nstatic const match_table_t i_lcoef_tokens = {\n\t{ I_LCOEF_RBPS,\t\t\"rbps=%u\"\t},\n\t{ I_LCOEF_RSEQIOPS,\t\"rseqiops=%u\"\t},\n\t{ I_LCOEF_RRANDIOPS,\t\"rrandiops=%u\"\t},\n\t{ I_LCOEF_WBPS,\t\t\"wbps=%u\"\t},\n\t{ I_LCOEF_WSEQIOPS,\t\"wseqiops=%u\"\t},\n\t{ I_LCOEF_WRANDIOPS,\t\"wrandiops=%u\"\t},\n\t{ NR_I_LCOEFS,\t\tNULL\t\t},\n};\n\nstatic ssize_t ioc_cost_model_write(struct kernfs_open_file *of, char *input,\n\t\t\t\t    size_t nbytes, loff_t off)\n{\n\tstruct blkg_conf_ctx ctx;\n\tstruct request_queue *q;\n\tstruct ioc *ioc;\n\tu64 u[NR_I_LCOEFS];\n\tbool user;\n\tchar *body, *p;\n\tint ret;\n\n\tblkg_conf_init(&ctx, input);\n\n\tret = blkg_conf_open_bdev(&ctx);\n\tif (ret)\n\t\tgoto err;\n\n\tbody = ctx.body;\n\tq = bdev_get_queue(ctx.bdev);\n\tif (!queue_is_mq(q)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto err;\n\t}\n\n\tioc = q_to_ioc(q);\n\tif (!ioc) {\n\t\tret = blk_iocost_init(ctx.bdev->bd_disk);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tioc = q_to_ioc(q);\n\t}\n\n\tblk_mq_freeze_queue(q);\n\tblk_mq_quiesce_queue(q);\n\n\tspin_lock_irq(&ioc->lock);\n\tmemcpy(u, ioc->params.i_lcoefs, sizeof(u));\n\tuser = ioc->user_cost_model;\n\n\twhile ((p = strsep(&body, \" \\t\\n\"))) {\n\t\tsubstring_t args[MAX_OPT_ARGS];\n\t\tchar buf[32];\n\t\tint tok;\n\t\tu64 v;\n\n\t\tif (!*p)\n\t\t\tcontinue;\n\n\t\tswitch (match_token(p, cost_ctrl_tokens, args)) {\n\t\tcase COST_CTRL:\n\t\t\tmatch_strlcpy(buf, &args[0], sizeof(buf));\n\t\t\tif (!strcmp(buf, \"auto\"))\n\t\t\t\tuser = false;\n\t\t\telse if (!strcmp(buf, \"user\"))\n\t\t\t\tuser = true;\n\t\t\telse\n\t\t\t\tgoto einval;\n\t\t\tcontinue;\n\t\tcase COST_MODEL:\n\t\t\tmatch_strlcpy(buf, &args[0], sizeof(buf));\n\t\t\tif (strcmp(buf, \"linear\"))\n\t\t\t\tgoto einval;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttok = match_token(p, i_lcoef_tokens, args);\n\t\tif (tok == NR_I_LCOEFS)\n\t\t\tgoto einval;\n\t\tif (match_u64(&args[0], &v))\n\t\t\tgoto einval;\n\t\tu[tok] = v;\n\t\tuser = true;\n\t}\n\n\tif (user) {\n\t\tmemcpy(ioc->params.i_lcoefs, u, sizeof(u));\n\t\tioc->user_cost_model = true;\n\t} else {\n\t\tioc->user_cost_model = false;\n\t}\n\tioc_refresh_params(ioc, true);\n\tspin_unlock_irq(&ioc->lock);\n\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_unfreeze_queue(q);\n\n\tblkg_conf_exit(&ctx);\n\treturn nbytes;\n\neinval:\n\tspin_unlock_irq(&ioc->lock);\n\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_unfreeze_queue(q);\n\n\tret = -EINVAL;\nerr:\n\tblkg_conf_exit(&ctx);\n\treturn ret;\n}\n\nstatic struct cftype ioc_files[] = {\n\t{\n\t\t.name = \"weight\",\n\t\t.flags = CFTYPE_NOT_ON_ROOT,\n\t\t.seq_show = ioc_weight_show,\n\t\t.write = ioc_weight_write,\n\t},\n\t{\n\t\t.name = \"cost.qos\",\n\t\t.flags = CFTYPE_ONLY_ON_ROOT,\n\t\t.seq_show = ioc_qos_show,\n\t\t.write = ioc_qos_write,\n\t},\n\t{\n\t\t.name = \"cost.model\",\n\t\t.flags = CFTYPE_ONLY_ON_ROOT,\n\t\t.seq_show = ioc_cost_model_show,\n\t\t.write = ioc_cost_model_write,\n\t},\n\t{}\n};\n\nstatic struct blkcg_policy blkcg_policy_iocost = {\n\t.dfl_cftypes\t= ioc_files,\n\t.cpd_alloc_fn\t= ioc_cpd_alloc,\n\t.cpd_free_fn\t= ioc_cpd_free,\n\t.pd_alloc_fn\t= ioc_pd_alloc,\n\t.pd_init_fn\t= ioc_pd_init,\n\t.pd_free_fn\t= ioc_pd_free,\n\t.pd_stat_fn\t= ioc_pd_stat,\n};\n\nstatic int __init ioc_init(void)\n{\n\treturn blkcg_policy_register(&blkcg_policy_iocost);\n}\n\nstatic void __exit ioc_exit(void)\n{\n\tblkcg_policy_unregister(&blkcg_policy_iocost);\n}\n\nmodule_init(ioc_init);\nmodule_exit(ioc_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}