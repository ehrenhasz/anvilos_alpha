{
  "module_name": "blk-cgroup.c",
  "hash_id": "333b8ddd2df7e584b23c30e8eacc50be2351584990f4c5e77fde287156f4871e",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-cgroup.c",
  "human_readable_source": "\n \n#include <linux/ioprio.h>\n#include <linux/kdev_t.h>\n#include <linux/module.h>\n#include <linux/sched/signal.h>\n#include <linux/err.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/ctype.h>\n#include <linux/resume_user_mode.h>\n#include <linux/psi.h>\n#include <linux/part_stat.h>\n#include \"blk.h\"\n#include \"blk-cgroup.h\"\n#include \"blk-ioprio.h\"\n#include \"blk-throttle.h\"\n\nstatic void __blkcg_rstat_flush(struct blkcg *blkcg, int cpu);\n\n \nstatic DEFINE_MUTEX(blkcg_pol_register_mutex);\nstatic DEFINE_MUTEX(blkcg_pol_mutex);\n\nstruct blkcg blkcg_root;\nEXPORT_SYMBOL_GPL(blkcg_root);\n\nstruct cgroup_subsys_state * const blkcg_root_css = &blkcg_root.css;\nEXPORT_SYMBOL_GPL(blkcg_root_css);\n\nstatic struct blkcg_policy *blkcg_policy[BLKCG_MAX_POLS];\n\nstatic LIST_HEAD(all_blkcgs);\t\t \n\nbool blkcg_debug_stats = false;\n\nstatic DEFINE_RAW_SPINLOCK(blkg_stat_lock);\n\n#define BLKG_DESTROY_BATCH_SIZE  64\n\n \nstatic int init_blkcg_llists(struct blkcg *blkcg)\n{\n\tint cpu;\n\n\tblkcg->lhead = alloc_percpu_gfp(struct llist_head, GFP_KERNEL);\n\tif (!blkcg->lhead)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(cpu)\n\t\tinit_llist_head(per_cpu_ptr(blkcg->lhead, cpu));\n\treturn 0;\n}\n\n \nstatic struct cgroup_subsys_state *blkcg_css(void)\n{\n\tstruct cgroup_subsys_state *css;\n\n\tcss = kthread_blkcg();\n\tif (css)\n\t\treturn css;\n\treturn task_css(current, io_cgrp_id);\n}\n\nstatic bool blkcg_policy_enabled(struct request_queue *q,\n\t\t\t\t const struct blkcg_policy *pol)\n{\n\treturn pol && test_bit(pol->plid, q->blkcg_pols);\n}\n\nstatic void blkg_free_workfn(struct work_struct *work)\n{\n\tstruct blkcg_gq *blkg = container_of(work, struct blkcg_gq,\n\t\t\t\t\t     free_work);\n\tstruct request_queue *q = blkg->q;\n\tint i;\n\n\t \n\tmutex_lock(&q->blkcg_mutex);\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkg->pd[i])\n\t\t\tblkcg_policy[i]->pd_free_fn(blkg->pd[i]);\n\tif (blkg->parent)\n\t\tblkg_put(blkg->parent);\n\tspin_lock_irq(&q->queue_lock);\n\tlist_del_init(&blkg->q_node);\n\tspin_unlock_irq(&q->queue_lock);\n\tmutex_unlock(&q->blkcg_mutex);\n\n\tblk_put_queue(q);\n\tfree_percpu(blkg->iostat_cpu);\n\tpercpu_ref_exit(&blkg->refcnt);\n\tkfree(blkg);\n}\n\n \nstatic void blkg_free(struct blkcg_gq *blkg)\n{\n\tif (!blkg)\n\t\treturn;\n\n\t \n\tINIT_WORK(&blkg->free_work, blkg_free_workfn);\n\tschedule_work(&blkg->free_work);\n}\n\nstatic void __blkg_release(struct rcu_head *rcu)\n{\n\tstruct blkcg_gq *blkg = container_of(rcu, struct blkcg_gq, rcu_head);\n\tstruct blkcg *blkcg = blkg->blkcg;\n\tint cpu;\n\n#ifdef CONFIG_BLK_CGROUP_PUNT_BIO\n\tWARN_ON(!bio_list_empty(&blkg->async_bios));\n#endif\n\t \n\tfor_each_possible_cpu(cpu)\n\t\t__blkcg_rstat_flush(blkcg, cpu);\n\n\t \n\tcss_put(&blkg->blkcg->css);\n\tblkg_free(blkg);\n}\n\n \nstatic void blkg_release(struct percpu_ref *ref)\n{\n\tstruct blkcg_gq *blkg = container_of(ref, struct blkcg_gq, refcnt);\n\n\tcall_rcu(&blkg->rcu_head, __blkg_release);\n}\n\n#ifdef CONFIG_BLK_CGROUP_PUNT_BIO\nstatic struct workqueue_struct *blkcg_punt_bio_wq;\n\nstatic void blkg_async_bio_workfn(struct work_struct *work)\n{\n\tstruct blkcg_gq *blkg = container_of(work, struct blkcg_gq,\n\t\t\t\t\t     async_bio_work);\n\tstruct bio_list bios = BIO_EMPTY_LIST;\n\tstruct bio *bio;\n\tstruct blk_plug plug;\n\tbool need_plug = false;\n\n\t \n\tspin_lock(&blkg->async_bio_lock);\n\tbio_list_merge(&bios, &blkg->async_bios);\n\tbio_list_init(&blkg->async_bios);\n\tspin_unlock(&blkg->async_bio_lock);\n\n\t \n\tif (bios.head && bios.head->bi_next) {\n\t\tneed_plug = true;\n\t\tblk_start_plug(&plug);\n\t}\n\twhile ((bio = bio_list_pop(&bios)))\n\t\tsubmit_bio(bio);\n\tif (need_plug)\n\t\tblk_finish_plug(&plug);\n}\n\n \nvoid blkcg_punt_bio_submit(struct bio *bio)\n{\n\tstruct blkcg_gq *blkg = bio->bi_blkg;\n\n\tif (blkg->parent) {\n\t\tspin_lock(&blkg->async_bio_lock);\n\t\tbio_list_add(&blkg->async_bios, bio);\n\t\tspin_unlock(&blkg->async_bio_lock);\n\t\tqueue_work(blkcg_punt_bio_wq, &blkg->async_bio_work);\n\t} else {\n\t\t \n\t\tsubmit_bio(bio);\n\t}\n}\nEXPORT_SYMBOL_GPL(blkcg_punt_bio_submit);\n\nstatic int __init blkcg_punt_bio_init(void)\n{\n\tblkcg_punt_bio_wq = alloc_workqueue(\"blkcg_punt_bio\",\n\t\t\t\t\t    WQ_MEM_RECLAIM | WQ_FREEZABLE |\n\t\t\t\t\t    WQ_UNBOUND | WQ_SYSFS, 0);\n\tif (!blkcg_punt_bio_wq)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nsubsys_initcall(blkcg_punt_bio_init);\n#endif  \n\n \nstruct cgroup_subsys_state *bio_blkcg_css(struct bio *bio)\n{\n\tif (!bio || !bio->bi_blkg)\n\t\treturn NULL;\n\treturn &bio->bi_blkg->blkcg->css;\n}\nEXPORT_SYMBOL_GPL(bio_blkcg_css);\n\n \nstatic inline struct blkcg *blkcg_parent(struct blkcg *blkcg)\n{\n\treturn css_to_blkcg(blkcg->css.parent);\n}\n\n \nstatic struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct gendisk *disk,\n\t\t\t\t   gfp_t gfp_mask)\n{\n\tstruct blkcg_gq *blkg;\n\tint i, cpu;\n\n\t \n\tblkg = kzalloc_node(sizeof(*blkg), gfp_mask, disk->queue->node);\n\tif (!blkg)\n\t\treturn NULL;\n\tif (percpu_ref_init(&blkg->refcnt, blkg_release, 0, gfp_mask))\n\t\tgoto out_free_blkg;\n\tblkg->iostat_cpu = alloc_percpu_gfp(struct blkg_iostat_set, gfp_mask);\n\tif (!blkg->iostat_cpu)\n\t\tgoto out_exit_refcnt;\n\tif (!blk_get_queue(disk->queue))\n\t\tgoto out_free_iostat;\n\n\tblkg->q = disk->queue;\n\tINIT_LIST_HEAD(&blkg->q_node);\n\tblkg->blkcg = blkcg;\n#ifdef CONFIG_BLK_CGROUP_PUNT_BIO\n\tspin_lock_init(&blkg->async_bio_lock);\n\tbio_list_init(&blkg->async_bios);\n\tINIT_WORK(&blkg->async_bio_work, blkg_async_bio_workfn);\n#endif\n\n\tu64_stats_init(&blkg->iostat.sync);\n\tfor_each_possible_cpu(cpu) {\n\t\tu64_stats_init(&per_cpu_ptr(blkg->iostat_cpu, cpu)->sync);\n\t\tper_cpu_ptr(blkg->iostat_cpu, cpu)->blkg = blkg;\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (!blkcg_policy_enabled(disk->queue, pol))\n\t\t\tcontinue;\n\n\t\t \n\t\tpd = pol->pd_alloc_fn(disk, blkcg, gfp_mask);\n\t\tif (!pd)\n\t\t\tgoto out_free_pds;\n\t\tblkg->pd[i] = pd;\n\t\tpd->blkg = blkg;\n\t\tpd->plid = i;\n\t\tpd->online = false;\n\t}\n\n\treturn blkg;\n\nout_free_pds:\n\twhile (--i >= 0)\n\t\tif (blkg->pd[i])\n\t\t\tblkcg_policy[i]->pd_free_fn(blkg->pd[i]);\n\tblk_put_queue(disk->queue);\nout_free_iostat:\n\tfree_percpu(blkg->iostat_cpu);\nout_exit_refcnt:\n\tpercpu_ref_exit(&blkg->refcnt);\nout_free_blkg:\n\tkfree(blkg);\n\treturn NULL;\n}\n\n \nstatic struct blkcg_gq *blkg_create(struct blkcg *blkcg, struct gendisk *disk,\n\t\t\t\t    struct blkcg_gq *new_blkg)\n{\n\tstruct blkcg_gq *blkg;\n\tint i, ret;\n\n\tlockdep_assert_held(&disk->queue->queue_lock);\n\n\t \n\tif (blk_queue_dying(disk->queue)) {\n\t\tret = -ENODEV;\n\t\tgoto err_free_blkg;\n\t}\n\n\t \n\tif (!css_tryget_online(&blkcg->css)) {\n\t\tret = -ENODEV;\n\t\tgoto err_free_blkg;\n\t}\n\n\t \n\tif (!new_blkg) {\n\t\tnew_blkg = blkg_alloc(blkcg, disk, GFP_NOWAIT | __GFP_NOWARN);\n\t\tif (unlikely(!new_blkg)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_put_css;\n\t\t}\n\t}\n\tblkg = new_blkg;\n\n\t \n\tif (blkcg_parent(blkcg)) {\n\t\tblkg->parent = blkg_lookup(blkcg_parent(blkcg), disk->queue);\n\t\tif (WARN_ON_ONCE(!blkg->parent)) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto err_put_css;\n\t\t}\n\t\tblkg_get(blkg->parent);\n\t}\n\n\t \n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(blkg->pd[i]);\n\t}\n\n\t \n\tspin_lock(&blkcg->lock);\n\tret = radix_tree_insert(&blkcg->blkg_tree, disk->queue->id, blkg);\n\tif (likely(!ret)) {\n\t\thlist_add_head_rcu(&blkg->blkcg_node, &blkcg->blkg_list);\n\t\tlist_add(&blkg->q_node, &disk->queue->blkg_list);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i]) {\n\t\t\t\tif (pol->pd_online_fn)\n\t\t\t\t\tpol->pd_online_fn(blkg->pd[i]);\n\t\t\t\tblkg->pd[i]->online = true;\n\t\t\t}\n\t\t}\n\t}\n\tblkg->online = true;\n\tspin_unlock(&blkcg->lock);\n\n\tif (!ret)\n\t\treturn blkg;\n\n\t \n\tblkg_put(blkg);\n\treturn ERR_PTR(ret);\n\nerr_put_css:\n\tcss_put(&blkcg->css);\nerr_free_blkg:\n\tif (new_blkg)\n\t\tblkg_free(new_blkg);\n\treturn ERR_PTR(ret);\n}\n\n \nstatic struct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,\n\t\tstruct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct blkcg_gq *blkg;\n\tunsigned long flags;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tblkg = blkg_lookup(blkcg, q);\n\tif (blkg)\n\t\treturn blkg;\n\n\tspin_lock_irqsave(&q->queue_lock, flags);\n\tblkg = blkg_lookup(blkcg, q);\n\tif (blkg) {\n\t\tif (blkcg != &blkcg_root &&\n\t\t    blkg != rcu_dereference(blkcg->blkg_hint))\n\t\t\trcu_assign_pointer(blkcg->blkg_hint, blkg);\n\t\tgoto found;\n\t}\n\n\t \n\twhile (true) {\n\t\tstruct blkcg *pos = blkcg;\n\t\tstruct blkcg *parent = blkcg_parent(blkcg);\n\t\tstruct blkcg_gq *ret_blkg = q->root_blkg;\n\n\t\twhile (parent) {\n\t\t\tblkg = blkg_lookup(parent, q);\n\t\t\tif (blkg) {\n\t\t\t\t \n\t\t\t\tret_blkg = blkg;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpos = parent;\n\t\t\tparent = blkcg_parent(parent);\n\t\t}\n\n\t\tblkg = blkg_create(pos, disk, NULL);\n\t\tif (IS_ERR(blkg)) {\n\t\t\tblkg = ret_blkg;\n\t\t\tbreak;\n\t\t}\n\t\tif (pos == blkcg)\n\t\t\tbreak;\n\t}\n\nfound:\n\tspin_unlock_irqrestore(&q->queue_lock, flags);\n\treturn blkg;\n}\n\nstatic void blkg_destroy(struct blkcg_gq *blkg)\n{\n\tstruct blkcg *blkcg = blkg->blkcg;\n\tint i;\n\n\tlockdep_assert_held(&blkg->q->queue_lock);\n\tlockdep_assert_held(&blkcg->lock);\n\n\t \n\tif (hlist_unhashed(&blkg->blkcg_node))\n\t\treturn;\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (blkg->pd[i] && blkg->pd[i]->online) {\n\t\t\tblkg->pd[i]->online = false;\n\t\t\tif (pol->pd_offline_fn)\n\t\t\t\tpol->pd_offline_fn(blkg->pd[i]);\n\t\t}\n\t}\n\n\tblkg->online = false;\n\n\tradix_tree_delete(&blkcg->blkg_tree, blkg->q->id);\n\thlist_del_init_rcu(&blkg->blkcg_node);\n\n\t \n\tif (rcu_access_pointer(blkcg->blkg_hint) == blkg)\n\t\trcu_assign_pointer(blkcg->blkg_hint, NULL);\n\n\t \n\tpercpu_ref_kill(&blkg->refcnt);\n}\n\nstatic void blkg_destroy_all(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct blkcg_gq *blkg, *n;\n\tint count = BLKG_DESTROY_BATCH_SIZE;\n\tint i;\n\nrestart:\n\tspin_lock_irq(&q->queue_lock);\n\tlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\n\t\tstruct blkcg *blkcg = blkg->blkcg;\n\n\t\tif (hlist_unhashed(&blkg->blkcg_node))\n\t\t\tcontinue;\n\n\t\tspin_lock(&blkcg->lock);\n\t\tblkg_destroy(blkg);\n\t\tspin_unlock(&blkcg->lock);\n\n\t\t \n\t\tif (!(--count)) {\n\t\t\tcount = BLKG_DESTROY_BATCH_SIZE;\n\t\t\tspin_unlock_irq(&q->queue_lock);\n\t\t\tcond_resched();\n\t\t\tgoto restart;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (pol)\n\t\t\t__clear_bit(pol->plid, q->blkcg_pols);\n\t}\n\n\tq->root_blkg = NULL;\n\tspin_unlock_irq(&q->queue_lock);\n}\n\nstatic int blkcg_reset_stats(struct cgroup_subsys_state *css,\n\t\t\t     struct cftype *cftype, u64 val)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tstruct blkcg_gq *blkg;\n\tint i, cpu;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\tspin_lock_irq(&blkcg->lock);\n\n\t \n\thlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct blkg_iostat_set *bis =\n\t\t\t\tper_cpu_ptr(blkg->iostat_cpu, cpu);\n\t\t\tmemset(bis, 0, sizeof(*bis));\n\n\t\t\t \n\t\t\tu64_stats_init(&bis->sync);\n\t\t\tbis->blkg = blkg;\n\t\t}\n\t\tmemset(&blkg->iostat, 0, sizeof(blkg->iostat));\n\t\tu64_stats_init(&blkg->iostat.sync);\n\n\t\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\t\tif (blkg->pd[i] && pol->pd_reset_stats_fn)\n\t\t\t\tpol->pd_reset_stats_fn(blkg->pd[i]);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn 0;\n}\n\nconst char *blkg_dev_name(struct blkcg_gq *blkg)\n{\n\tif (!blkg->q->disk)\n\t\treturn NULL;\n\treturn bdi_dev_name(blkg->q->disk->bdi);\n}\n\n \nvoid blkcg_print_blkgs(struct seq_file *sf, struct blkcg *blkcg,\n\t\t       u64 (*prfill)(struct seq_file *,\n\t\t\t\t     struct blkg_policy_data *, int),\n\t\t       const struct blkcg_policy *pol, int data,\n\t\t       bool show_total)\n{\n\tstruct blkcg_gq *blkg;\n\tu64 total = 0;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tspin_lock_irq(&blkg->q->queue_lock);\n\t\tif (blkcg_policy_enabled(blkg->q, pol))\n\t\t\ttotal += prfill(sf, blkg->pd[pol->plid], data);\n\t\tspin_unlock_irq(&blkg->q->queue_lock);\n\t}\n\trcu_read_unlock();\n\n\tif (show_total)\n\t\tseq_printf(sf, \"Total %llu\\n\", (unsigned long long)total);\n}\nEXPORT_SYMBOL_GPL(blkcg_print_blkgs);\n\n \nu64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)\n{\n\tconst char *dname = blkg_dev_name(pd->blkg);\n\n\tif (!dname)\n\t\treturn 0;\n\n\tseq_printf(sf, \"%s %llu\\n\", dname, (unsigned long long)v);\n\treturn v;\n}\nEXPORT_SYMBOL_GPL(__blkg_prfill_u64);\n\n \nvoid blkg_conf_init(struct blkg_conf_ctx *ctx, char *input)\n{\n\t*ctx = (struct blkg_conf_ctx){ .input = input };\n}\nEXPORT_SYMBOL_GPL(blkg_conf_init);\n\n \nint blkg_conf_open_bdev(struct blkg_conf_ctx *ctx)\n{\n\tchar *input = ctx->input;\n\tunsigned int major, minor;\n\tstruct block_device *bdev;\n\tint key_len;\n\n\tif (ctx->bdev)\n\t\treturn 0;\n\n\tif (sscanf(input, \"%u:%u%n\", &major, &minor, &key_len) != 2)\n\t\treturn -EINVAL;\n\n\tinput += key_len;\n\tif (!isspace(*input))\n\t\treturn -EINVAL;\n\tinput = skip_spaces(input);\n\n\tbdev = blkdev_get_no_open(MKDEV(major, minor));\n\tif (!bdev)\n\t\treturn -ENODEV;\n\tif (bdev_is_partition(bdev)) {\n\t\tblkdev_put_no_open(bdev);\n\t\treturn -ENODEV;\n\t}\n\n\tmutex_lock(&bdev->bd_queue->rq_qos_mutex);\n\tif (!disk_live(bdev->bd_disk)) {\n\t\tblkdev_put_no_open(bdev);\n\t\tmutex_unlock(&bdev->bd_queue->rq_qos_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tctx->body = input;\n\tctx->bdev = bdev;\n\treturn 0;\n}\n\n \nint blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,\n\t\t   struct blkg_conf_ctx *ctx)\n\t__acquires(&bdev->bd_queue->queue_lock)\n{\n\tstruct gendisk *disk;\n\tstruct request_queue *q;\n\tstruct blkcg_gq *blkg;\n\tint ret;\n\n\tret = blkg_conf_open_bdev(ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tdisk = ctx->bdev->bd_disk;\n\tq = disk->queue;\n\n\t \n\tret = blk_queue_enter(q, 0);\n\tif (ret)\n\t\tgoto fail;\n\n\tspin_lock_irq(&q->queue_lock);\n\n\tif (!blkcg_policy_enabled(q, pol)) {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto fail_unlock;\n\t}\n\n\tblkg = blkg_lookup(blkcg, q);\n\tif (blkg)\n\t\tgoto success;\n\n\t \n\twhile (true) {\n\t\tstruct blkcg *pos = blkcg;\n\t\tstruct blkcg *parent;\n\t\tstruct blkcg_gq *new_blkg;\n\n\t\tparent = blkcg_parent(blkcg);\n\t\twhile (parent && !blkg_lookup(parent, q)) {\n\t\t\tpos = parent;\n\t\t\tparent = blkcg_parent(parent);\n\t\t}\n\n\t\t \n\t\tspin_unlock_irq(&q->queue_lock);\n\n\t\tnew_blkg = blkg_alloc(pos, disk, GFP_KERNEL);\n\t\tif (unlikely(!new_blkg)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail_exit_queue;\n\t\t}\n\n\t\tif (radix_tree_preload(GFP_KERNEL)) {\n\t\t\tblkg_free(new_blkg);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail_exit_queue;\n\t\t}\n\n\t\tspin_lock_irq(&q->queue_lock);\n\n\t\tif (!blkcg_policy_enabled(q, pol)) {\n\t\t\tblkg_free(new_blkg);\n\t\t\tret = -EOPNOTSUPP;\n\t\t\tgoto fail_preloaded;\n\t\t}\n\n\t\tblkg = blkg_lookup(pos, q);\n\t\tif (blkg) {\n\t\t\tblkg_free(new_blkg);\n\t\t} else {\n\t\t\tblkg = blkg_create(pos, disk, new_blkg);\n\t\t\tif (IS_ERR(blkg)) {\n\t\t\t\tret = PTR_ERR(blkg);\n\t\t\t\tgoto fail_preloaded;\n\t\t\t}\n\t\t}\n\n\t\tradix_tree_preload_end();\n\n\t\tif (pos == blkcg)\n\t\t\tgoto success;\n\t}\nsuccess:\n\tblk_queue_exit(q);\n\tctx->blkg = blkg;\n\treturn 0;\n\nfail_preloaded:\n\tradix_tree_preload_end();\nfail_unlock:\n\tspin_unlock_irq(&q->queue_lock);\nfail_exit_queue:\n\tblk_queue_exit(q);\nfail:\n\t \n\tif (ret == -EBUSY) {\n\t\tmsleep(10);\n\t\tret = restart_syscall();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkg_conf_prep);\n\n \nvoid blkg_conf_exit(struct blkg_conf_ctx *ctx)\n\t__releases(&ctx->bdev->bd_queue->queue_lock)\n\t__releases(&ctx->bdev->bd_queue->rq_qos_mutex)\n{\n\tif (ctx->blkg) {\n\t\tspin_unlock_irq(&bdev_get_queue(ctx->bdev)->queue_lock);\n\t\tctx->blkg = NULL;\n\t}\n\n\tif (ctx->bdev) {\n\t\tmutex_unlock(&ctx->bdev->bd_queue->rq_qos_mutex);\n\t\tblkdev_put_no_open(ctx->bdev);\n\t\tctx->body = NULL;\n\t\tctx->bdev = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(blkg_conf_exit);\n\nstatic void blkg_iostat_set(struct blkg_iostat *dst, struct blkg_iostat *src)\n{\n\tint i;\n\n\tfor (i = 0; i < BLKG_IOSTAT_NR; i++) {\n\t\tdst->bytes[i] = src->bytes[i];\n\t\tdst->ios[i] = src->ios[i];\n\t}\n}\n\nstatic void blkg_iostat_add(struct blkg_iostat *dst, struct blkg_iostat *src)\n{\n\tint i;\n\n\tfor (i = 0; i < BLKG_IOSTAT_NR; i++) {\n\t\tdst->bytes[i] += src->bytes[i];\n\t\tdst->ios[i] += src->ios[i];\n\t}\n}\n\nstatic void blkg_iostat_sub(struct blkg_iostat *dst, struct blkg_iostat *src)\n{\n\tint i;\n\n\tfor (i = 0; i < BLKG_IOSTAT_NR; i++) {\n\t\tdst->bytes[i] -= src->bytes[i];\n\t\tdst->ios[i] -= src->ios[i];\n\t}\n}\n\nstatic void blkcg_iostat_update(struct blkcg_gq *blkg, struct blkg_iostat *cur,\n\t\t\t\tstruct blkg_iostat *last)\n{\n\tstruct blkg_iostat delta;\n\tunsigned long flags;\n\n\t \n\tflags = u64_stats_update_begin_irqsave(&blkg->iostat.sync);\n\tblkg_iostat_set(&delta, cur);\n\tblkg_iostat_sub(&delta, last);\n\tblkg_iostat_add(&blkg->iostat.cur, &delta);\n\tblkg_iostat_add(last, &delta);\n\tu64_stats_update_end_irqrestore(&blkg->iostat.sync, flags);\n}\n\nstatic void __blkcg_rstat_flush(struct blkcg *blkcg, int cpu)\n{\n\tstruct llist_head *lhead = per_cpu_ptr(blkcg->lhead, cpu);\n\tstruct llist_node *lnode;\n\tstruct blkg_iostat_set *bisc, *next_bisc;\n\tunsigned long flags;\n\n\trcu_read_lock();\n\n\tlnode = llist_del_all(lhead);\n\tif (!lnode)\n\t\tgoto out;\n\n\t \n\traw_spin_lock_irqsave(&blkg_stat_lock, flags);\n\n\t \n\tllist_for_each_entry_safe(bisc, next_bisc, lnode, lnode) {\n\t\tstruct blkcg_gq *blkg = bisc->blkg;\n\t\tstruct blkcg_gq *parent = blkg->parent;\n\t\tstruct blkg_iostat cur;\n\t\tunsigned int seq;\n\n\t\tWRITE_ONCE(bisc->lqueued, false);\n\n\t\t \n\t\tdo {\n\t\t\tseq = u64_stats_fetch_begin(&bisc->sync);\n\t\t\tblkg_iostat_set(&cur, &bisc->cur);\n\t\t} while (u64_stats_fetch_retry(&bisc->sync, seq));\n\n\t\tblkcg_iostat_update(blkg, &cur, &bisc->last);\n\n\t\t \n\t\tif (parent && parent->parent)\n\t\t\tblkcg_iostat_update(parent, &blkg->iostat.cur,\n\t\t\t\t\t    &blkg->iostat.last);\n\t}\n\traw_spin_unlock_irqrestore(&blkg_stat_lock, flags);\nout:\n\trcu_read_unlock();\n}\n\nstatic void blkcg_rstat_flush(struct cgroup_subsys_state *css, int cpu)\n{\n\t \n\tif (cgroup_parent(css->cgroup))\n\t\t__blkcg_rstat_flush(css_to_blkcg(css), cpu);\n}\n\n \nstatic void blkcg_fill_root_iostats(void)\n{\n\tstruct class_dev_iter iter;\n\tstruct device *dev;\n\n\tclass_dev_iter_init(&iter, &block_class, NULL, &disk_type);\n\twhile ((dev = class_dev_iter_next(&iter))) {\n\t\tstruct block_device *bdev = dev_to_bdev(dev);\n\t\tstruct blkcg_gq *blkg = bdev->bd_disk->queue->root_blkg;\n\t\tstruct blkg_iostat tmp;\n\t\tint cpu;\n\t\tunsigned long flags;\n\n\t\tmemset(&tmp, 0, sizeof(tmp));\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tstruct disk_stats *cpu_dkstats;\n\n\t\t\tcpu_dkstats = per_cpu_ptr(bdev->bd_stats, cpu);\n\t\t\ttmp.ios[BLKG_IOSTAT_READ] +=\n\t\t\t\tcpu_dkstats->ios[STAT_READ];\n\t\t\ttmp.ios[BLKG_IOSTAT_WRITE] +=\n\t\t\t\tcpu_dkstats->ios[STAT_WRITE];\n\t\t\ttmp.ios[BLKG_IOSTAT_DISCARD] +=\n\t\t\t\tcpu_dkstats->ios[STAT_DISCARD];\n\t\t\t\n\t\t\ttmp.bytes[BLKG_IOSTAT_READ] +=\n\t\t\t\tcpu_dkstats->sectors[STAT_READ] << 9;\n\t\t\ttmp.bytes[BLKG_IOSTAT_WRITE] +=\n\t\t\t\tcpu_dkstats->sectors[STAT_WRITE] << 9;\n\t\t\ttmp.bytes[BLKG_IOSTAT_DISCARD] +=\n\t\t\t\tcpu_dkstats->sectors[STAT_DISCARD] << 9;\n\t\t}\n\n\t\tflags = u64_stats_update_begin_irqsave(&blkg->iostat.sync);\n\t\tblkg_iostat_set(&blkg->iostat.cur, &tmp);\n\t\tu64_stats_update_end_irqrestore(&blkg->iostat.sync, flags);\n\t}\n}\n\nstatic void blkcg_print_one_stat(struct blkcg_gq *blkg, struct seq_file *s)\n{\n\tstruct blkg_iostat_set *bis = &blkg->iostat;\n\tu64 rbytes, wbytes, rios, wios, dbytes, dios;\n\tconst char *dname;\n\tunsigned seq;\n\tint i;\n\n\tif (!blkg->online)\n\t\treturn;\n\n\tdname = blkg_dev_name(blkg);\n\tif (!dname)\n\t\treturn;\n\n\tseq_printf(s, \"%s \", dname);\n\n\tdo {\n\t\tseq = u64_stats_fetch_begin(&bis->sync);\n\n\t\trbytes = bis->cur.bytes[BLKG_IOSTAT_READ];\n\t\twbytes = bis->cur.bytes[BLKG_IOSTAT_WRITE];\n\t\tdbytes = bis->cur.bytes[BLKG_IOSTAT_DISCARD];\n\t\trios = bis->cur.ios[BLKG_IOSTAT_READ];\n\t\twios = bis->cur.ios[BLKG_IOSTAT_WRITE];\n\t\tdios = bis->cur.ios[BLKG_IOSTAT_DISCARD];\n\t} while (u64_stats_fetch_retry(&bis->sync, seq));\n\n\tif (rbytes || wbytes || rios || wios) {\n\t\tseq_printf(s, \"rbytes=%llu wbytes=%llu rios=%llu wios=%llu dbytes=%llu dios=%llu\",\n\t\t\trbytes, wbytes, rios, wios,\n\t\t\tdbytes, dios);\n\t}\n\n\tif (blkcg_debug_stats && atomic_read(&blkg->use_delay)) {\n\t\tseq_printf(s, \" use_delay=%d delay_nsec=%llu\",\n\t\t\tatomic_read(&blkg->use_delay),\n\t\t\tatomic64_read(&blkg->delay_nsec));\n\t}\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\n\t\tif (!blkg->pd[i] || !pol->pd_stat_fn)\n\t\t\tcontinue;\n\n\t\tpol->pd_stat_fn(blkg->pd[i], s);\n\t}\n\n\tseq_puts(s, \"\\n\");\n}\n\nstatic int blkcg_print_stat(struct seq_file *sf, void *v)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(seq_css(sf));\n\tstruct blkcg_gq *blkg;\n\n\tif (!seq_css(sf)->parent)\n\t\tblkcg_fill_root_iostats();\n\telse\n\t\tcgroup_rstat_flush(blkcg->css.cgroup);\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\n\t\tspin_lock_irq(&blkg->q->queue_lock);\n\t\tblkcg_print_one_stat(blkg, sf);\n\t\tspin_unlock_irq(&blkg->q->queue_lock);\n\t}\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct cftype blkcg_files[] = {\n\t{\n\t\t.name = \"stat\",\n\t\t.seq_show = blkcg_print_stat,\n\t},\n\t{ }\t \n};\n\nstatic struct cftype blkcg_legacy_files[] = {\n\t{\n\t\t.name = \"reset_stats\",\n\t\t.write_u64 = blkcg_reset_stats,\n\t},\n\t{ }\t \n};\n\n#ifdef CONFIG_CGROUP_WRITEBACK\nstruct list_head *blkcg_get_cgwb_list(struct cgroup_subsys_state *css)\n{\n\treturn &css_to_blkcg(css)->cgwb_list;\n}\n#endif\n\n \n\n \nstatic void blkcg_destroy_blkgs(struct blkcg *blkcg)\n{\n\tmight_sleep();\n\n\tspin_lock_irq(&blkcg->lock);\n\n\twhile (!hlist_empty(&blkcg->blkg_list)) {\n\t\tstruct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,\n\t\t\t\t\t\tstruct blkcg_gq, blkcg_node);\n\t\tstruct request_queue *q = blkg->q;\n\n\t\tif (need_resched() || !spin_trylock(&q->queue_lock)) {\n\t\t\t \n\t\t\tspin_unlock_irq(&blkcg->lock);\n\t\t\tcond_resched();\n\t\t\tspin_lock_irq(&blkcg->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tblkg_destroy(blkg);\n\t\tspin_unlock(&q->queue_lock);\n\t}\n\n\tspin_unlock_irq(&blkcg->lock);\n}\n\n \nvoid blkcg_pin_online(struct cgroup_subsys_state *blkcg_css)\n{\n\trefcount_inc(&css_to_blkcg(blkcg_css)->online_pin);\n}\n\n \nvoid blkcg_unpin_online(struct cgroup_subsys_state *blkcg_css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(blkcg_css);\n\n\tdo {\n\t\tif (!refcount_dec_and_test(&blkcg->online_pin))\n\t\t\tbreak;\n\t\tblkcg_destroy_blkgs(blkcg);\n\t\tblkcg = blkcg_parent(blkcg);\n\t} while (blkcg);\n}\n\n \nstatic void blkcg_css_offline(struct cgroup_subsys_state *css)\n{\n\t \n\twb_blkcg_offline(css);\n\n\t \n\tblkcg_unpin_online(css);\n}\n\nstatic void blkcg_css_free(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *blkcg = css_to_blkcg(css);\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tlist_del(&blkcg->all_blkcgs_node);\n\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\tfree_percpu(blkcg->lhead);\n\tkfree(blkcg);\n}\n\nstatic struct cgroup_subsys_state *\nblkcg_css_alloc(struct cgroup_subsys_state *parent_css)\n{\n\tstruct blkcg *blkcg;\n\tint i;\n\n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (!parent_css) {\n\t\tblkcg = &blkcg_root;\n\t} else {\n\t\tblkcg = kzalloc(sizeof(*blkcg), GFP_KERNEL);\n\t\tif (!blkcg)\n\t\t\tgoto unlock;\n\t}\n\n\tif (init_blkcg_llists(blkcg))\n\t\tgoto free_blkcg;\n\n\tfor (i = 0; i < BLKCG_MAX_POLS ; i++) {\n\t\tstruct blkcg_policy *pol = blkcg_policy[i];\n\t\tstruct blkcg_policy_data *cpd;\n\n\t\t \n\t\tif (!pol || !pol->cpd_alloc_fn)\n\t\t\tcontinue;\n\n\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\tif (!cpd)\n\t\t\tgoto free_pd_blkcg;\n\n\t\tblkcg->cpd[i] = cpd;\n\t\tcpd->blkcg = blkcg;\n\t\tcpd->plid = i;\n\t}\n\n\tspin_lock_init(&blkcg->lock);\n\trefcount_set(&blkcg->online_pin, 1);\n\tINIT_RADIX_TREE(&blkcg->blkg_tree, GFP_NOWAIT | __GFP_NOWARN);\n\tINIT_HLIST_HEAD(&blkcg->blkg_list);\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tINIT_LIST_HEAD(&blkcg->cgwb_list);\n#endif\n\tlist_add_tail(&blkcg->all_blkcgs_node, &all_blkcgs);\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn &blkcg->css;\n\nfree_pd_blkcg:\n\tfor (i--; i >= 0; i--)\n\t\tif (blkcg->cpd[i])\n\t\t\tblkcg_policy[i]->cpd_free_fn(blkcg->cpd[i]);\n\tfree_percpu(blkcg->lhead);\nfree_blkcg:\n\tif (blkcg != &blkcg_root)\n\t\tkfree(blkcg);\nunlock:\n\tmutex_unlock(&blkcg_pol_mutex);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic int blkcg_css_online(struct cgroup_subsys_state *css)\n{\n\tstruct blkcg *parent = blkcg_parent(css_to_blkcg(css));\n\n\t \n\tif (parent)\n\t\tblkcg_pin_online(&parent->css);\n\treturn 0;\n}\n\nint blkcg_init_disk(struct gendisk *disk)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct blkcg_gq *new_blkg, *blkg;\n\tbool preloaded;\n\tint ret;\n\n\tINIT_LIST_HEAD(&q->blkg_list);\n\tmutex_init(&q->blkcg_mutex);\n\n\tnew_blkg = blkg_alloc(&blkcg_root, disk, GFP_KERNEL);\n\tif (!new_blkg)\n\t\treturn -ENOMEM;\n\n\tpreloaded = !radix_tree_preload(GFP_KERNEL);\n\n\t \n\t \n\tspin_lock_irq(&q->queue_lock);\n\tblkg = blkg_create(&blkcg_root, disk, new_blkg);\n\tif (IS_ERR(blkg))\n\t\tgoto err_unlock;\n\tq->root_blkg = blkg;\n\tspin_unlock_irq(&q->queue_lock);\n\n\tif (preloaded)\n\t\tradix_tree_preload_end();\n\n\tret = blk_ioprio_init(disk);\n\tif (ret)\n\t\tgoto err_destroy_all;\n\n\tret = blk_throtl_init(disk);\n\tif (ret)\n\t\tgoto err_ioprio_exit;\n\n\treturn 0;\n\nerr_ioprio_exit:\n\tblk_ioprio_exit(disk);\nerr_destroy_all:\n\tblkg_destroy_all(disk);\n\treturn ret;\nerr_unlock:\n\tspin_unlock_irq(&q->queue_lock);\n\tif (preloaded)\n\t\tradix_tree_preload_end();\n\treturn PTR_ERR(blkg);\n}\n\nvoid blkcg_exit_disk(struct gendisk *disk)\n{\n\tblkg_destroy_all(disk);\n\tblk_throtl_exit(disk);\n}\n\nstatic void blkcg_exit(struct task_struct *tsk)\n{\n\tif (tsk->throttle_disk)\n\t\tput_disk(tsk->throttle_disk);\n\ttsk->throttle_disk = NULL;\n}\n\nstruct cgroup_subsys io_cgrp_subsys = {\n\t.css_alloc = blkcg_css_alloc,\n\t.css_online = blkcg_css_online,\n\t.css_offline = blkcg_css_offline,\n\t.css_free = blkcg_css_free,\n\t.css_rstat_flush = blkcg_rstat_flush,\n\t.dfl_cftypes = blkcg_files,\n\t.legacy_cftypes = blkcg_legacy_files,\n\t.legacy_name = \"blkio\",\n\t.exit = blkcg_exit,\n#ifdef CONFIG_MEMCG\n\t \n\t.depends_on = 1 << memory_cgrp_id,\n#endif\n};\nEXPORT_SYMBOL_GPL(io_cgrp_subsys);\n\n \nint blkcg_activate_policy(struct gendisk *disk, const struct blkcg_policy *pol)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct blkg_policy_data *pd_prealloc = NULL;\n\tstruct blkcg_gq *blkg, *pinned_blkg = NULL;\n\tint ret;\n\n\tif (blkcg_policy_enabled(q, pol))\n\t\treturn 0;\n\n\tif (queue_is_mq(q))\n\t\tblk_mq_freeze_queue(q);\nretry:\n\tspin_lock_irq(&q->queue_lock);\n\n\t \n\tlist_for_each_entry_reverse(blkg, &q->blkg_list, q_node) {\n\t\tstruct blkg_policy_data *pd;\n\n\t\tif (blkg->pd[pol->plid])\n\t\t\tcontinue;\n\n\t\t \n\t\tif (blkg == pinned_blkg) {\n\t\t\tpd = pd_prealloc;\n\t\t\tpd_prealloc = NULL;\n\t\t} else {\n\t\t\tpd = pol->pd_alloc_fn(disk, blkg->blkcg,\n\t\t\t\t\t      GFP_NOWAIT | __GFP_NOWARN);\n\t\t}\n\n\t\tif (!pd) {\n\t\t\t \n\t\t\tif (pinned_blkg)\n\t\t\t\tblkg_put(pinned_blkg);\n\t\t\tblkg_get(blkg);\n\t\t\tpinned_blkg = blkg;\n\n\t\t\tspin_unlock_irq(&q->queue_lock);\n\n\t\t\tif (pd_prealloc)\n\t\t\t\tpol->pd_free_fn(pd_prealloc);\n\t\t\tpd_prealloc = pol->pd_alloc_fn(disk, blkg->blkcg,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\t\tif (pd_prealloc)\n\t\t\t\tgoto retry;\n\t\t\telse\n\t\t\t\tgoto enomem;\n\t\t}\n\n\t\tspin_lock(&blkg->blkcg->lock);\n\n\t\tpd->blkg = blkg;\n\t\tpd->plid = pol->plid;\n\t\tblkg->pd[pol->plid] = pd;\n\n\t\tif (pol->pd_init_fn)\n\t\t\tpol->pd_init_fn(pd);\n\n\t\tif (pol->pd_online_fn)\n\t\t\tpol->pd_online_fn(pd);\n\t\tpd->online = true;\n\n\t\tspin_unlock(&blkg->blkcg->lock);\n\t}\n\n\t__set_bit(pol->plid, q->blkcg_pols);\n\tret = 0;\n\n\tspin_unlock_irq(&q->queue_lock);\nout:\n\tif (queue_is_mq(q))\n\t\tblk_mq_unfreeze_queue(q);\n\tif (pinned_blkg)\n\t\tblkg_put(pinned_blkg);\n\tif (pd_prealloc)\n\t\tpol->pd_free_fn(pd_prealloc);\n\treturn ret;\n\nenomem:\n\t \n\tspin_lock_irq(&q->queue_lock);\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\tstruct blkcg *blkcg = blkg->blkcg;\n\t\tstruct blkg_policy_data *pd;\n\n\t\tspin_lock(&blkcg->lock);\n\t\tpd = blkg->pd[pol->plid];\n\t\tif (pd) {\n\t\t\tif (pd->online && pol->pd_offline_fn)\n\t\t\t\tpol->pd_offline_fn(pd);\n\t\t\tpd->online = false;\n\t\t\tpol->pd_free_fn(pd);\n\t\t\tblkg->pd[pol->plid] = NULL;\n\t\t}\n\t\tspin_unlock(&blkcg->lock);\n\t}\n\tspin_unlock_irq(&q->queue_lock);\n\tret = -ENOMEM;\n\tgoto out;\n}\nEXPORT_SYMBOL_GPL(blkcg_activate_policy);\n\n \nvoid blkcg_deactivate_policy(struct gendisk *disk,\n\t\t\t     const struct blkcg_policy *pol)\n{\n\tstruct request_queue *q = disk->queue;\n\tstruct blkcg_gq *blkg;\n\n\tif (!blkcg_policy_enabled(q, pol))\n\t\treturn;\n\n\tif (queue_is_mq(q))\n\t\tblk_mq_freeze_queue(q);\n\n\tmutex_lock(&q->blkcg_mutex);\n\tspin_lock_irq(&q->queue_lock);\n\n\t__clear_bit(pol->plid, q->blkcg_pols);\n\n\tlist_for_each_entry(blkg, &q->blkg_list, q_node) {\n\t\tstruct blkcg *blkcg = blkg->blkcg;\n\n\t\tspin_lock(&blkcg->lock);\n\t\tif (blkg->pd[pol->plid]) {\n\t\t\tif (blkg->pd[pol->plid]->online && pol->pd_offline_fn)\n\t\t\t\tpol->pd_offline_fn(blkg->pd[pol->plid]);\n\t\t\tpol->pd_free_fn(blkg->pd[pol->plid]);\n\t\t\tblkg->pd[pol->plid] = NULL;\n\t\t}\n\t\tspin_unlock(&blkcg->lock);\n\t}\n\n\tspin_unlock_irq(&q->queue_lock);\n\tmutex_unlock(&q->blkcg_mutex);\n\n\tif (queue_is_mq(q))\n\t\tblk_mq_unfreeze_queue(q);\n}\nEXPORT_SYMBOL_GPL(blkcg_deactivate_policy);\n\nstatic void blkcg_free_all_cpd(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\n\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\tif (blkcg->cpd[pol->plid]) {\n\t\t\tpol->cpd_free_fn(blkcg->cpd[pol->plid]);\n\t\t\tblkcg->cpd[pol->plid] = NULL;\n\t\t}\n\t}\n}\n\n \nint blkcg_policy_register(struct blkcg_policy *pol)\n{\n\tstruct blkcg *blkcg;\n\tint i, ret;\n\n\tmutex_lock(&blkcg_pol_register_mutex);\n\tmutex_lock(&blkcg_pol_mutex);\n\n\t \n\tret = -ENOSPC;\n\tfor (i = 0; i < BLKCG_MAX_POLS; i++)\n\t\tif (!blkcg_policy[i])\n\t\t\tbreak;\n\tif (i >= BLKCG_MAX_POLS) {\n\t\tpr_warn(\"blkcg_policy_register: BLKCG_MAX_POLS too small\\n\");\n\t\tgoto err_unlock;\n\t}\n\n\t \n\tif ((!pol->cpd_alloc_fn ^ !pol->cpd_free_fn) ||\n\t\t(!pol->pd_alloc_fn ^ !pol->pd_free_fn))\n\t\tgoto err_unlock;\n\n\t \n\tpol->plid = i;\n\tblkcg_policy[pol->plid] = pol;\n\n\t \n\tif (pol->cpd_alloc_fn) {\n\t\tlist_for_each_entry(blkcg, &all_blkcgs, all_blkcgs_node) {\n\t\t\tstruct blkcg_policy_data *cpd;\n\n\t\t\tcpd = pol->cpd_alloc_fn(GFP_KERNEL);\n\t\t\tif (!cpd)\n\t\t\t\tgoto err_free_cpds;\n\n\t\t\tblkcg->cpd[pol->plid] = cpd;\n\t\t\tcpd->blkcg = blkcg;\n\t\t\tcpd->plid = pol->plid;\n\t\t}\n\t}\n\n\tmutex_unlock(&blkcg_pol_mutex);\n\n\t \n\tif (pol->dfl_cftypes)\n\t\tWARN_ON(cgroup_add_dfl_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t       pol->dfl_cftypes));\n\tif (pol->legacy_cftypes)\n\t\tWARN_ON(cgroup_add_legacy_cftypes(&io_cgrp_subsys,\n\t\t\t\t\t\t  pol->legacy_cftypes));\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn 0;\n\nerr_free_cpds:\n\tif (pol->cpd_free_fn)\n\t\tblkcg_free_all_cpd(pol);\n\n\tblkcg_policy[pol->plid] = NULL;\nerr_unlock:\n\tmutex_unlock(&blkcg_pol_mutex);\n\tmutex_unlock(&blkcg_pol_register_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_register);\n\n \nvoid blkcg_policy_unregister(struct blkcg_policy *pol)\n{\n\tmutex_lock(&blkcg_pol_register_mutex);\n\n\tif (WARN_ON(blkcg_policy[pol->plid] != pol))\n\t\tgoto out_unlock;\n\n\t \n\tif (pol->dfl_cftypes)\n\t\tcgroup_rm_cftypes(pol->dfl_cftypes);\n\tif (pol->legacy_cftypes)\n\t\tcgroup_rm_cftypes(pol->legacy_cftypes);\n\n\t \n\tmutex_lock(&blkcg_pol_mutex);\n\n\tif (pol->cpd_free_fn)\n\t\tblkcg_free_all_cpd(pol);\n\n\tblkcg_policy[pol->plid] = NULL;\n\n\tmutex_unlock(&blkcg_pol_mutex);\nout_unlock:\n\tmutex_unlock(&blkcg_pol_register_mutex);\n}\nEXPORT_SYMBOL_GPL(blkcg_policy_unregister);\n\n \nstatic void blkcg_scale_delay(struct blkcg_gq *blkg, u64 now)\n{\n\tu64 old = atomic64_read(&blkg->delay_start);\n\n\t \n\tif (atomic_read(&blkg->use_delay) < 0)\n\t\treturn;\n\n\t \n\tif (time_before64(old + NSEC_PER_SEC, now) &&\n\t    atomic64_try_cmpxchg(&blkg->delay_start, &old, now)) {\n\t\tu64 cur = atomic64_read(&blkg->delay_nsec);\n\t\tu64 sub = min_t(u64, blkg->last_delay, now - old);\n\t\tint cur_use = atomic_read(&blkg->use_delay);\n\n\t\t \n\t\tif (cur_use < blkg->last_use)\n\t\t\tsub = max_t(u64, sub, blkg->last_delay >> 1);\n\n\t\t \n\t\tif (unlikely(cur < sub)) {\n\t\t\tatomic64_set(&blkg->delay_nsec, 0);\n\t\t\tblkg->last_delay = 0;\n\t\t} else {\n\t\t\tatomic64_sub(sub, &blkg->delay_nsec);\n\t\t\tblkg->last_delay = cur - sub;\n\t\t}\n\t\tblkg->last_use = cur_use;\n\t}\n}\n\n \nstatic void blkcg_maybe_throttle_blkg(struct blkcg_gq *blkg, bool use_memdelay)\n{\n\tunsigned long pflags;\n\tbool clamp;\n\tu64 now = ktime_to_ns(ktime_get());\n\tu64 exp;\n\tu64 delay_nsec = 0;\n\tint tok;\n\n\twhile (blkg->parent) {\n\t\tint use_delay = atomic_read(&blkg->use_delay);\n\n\t\tif (use_delay) {\n\t\t\tu64 this_delay;\n\n\t\t\tblkcg_scale_delay(blkg, now);\n\t\t\tthis_delay = atomic64_read(&blkg->delay_nsec);\n\t\t\tif (this_delay > delay_nsec) {\n\t\t\t\tdelay_nsec = this_delay;\n\t\t\t\tclamp = use_delay > 0;\n\t\t\t}\n\t\t}\n\t\tblkg = blkg->parent;\n\t}\n\n\tif (!delay_nsec)\n\t\treturn;\n\n\t \n\tif (clamp)\n\t\tdelay_nsec = min_t(u64, delay_nsec, 250 * NSEC_PER_MSEC);\n\n\tif (use_memdelay)\n\t\tpsi_memstall_enter(&pflags);\n\n\texp = ktime_add_ns(now, delay_nsec);\n\ttok = io_schedule_prepare();\n\tdo {\n\t\t__set_current_state(TASK_KILLABLE);\n\t\tif (!schedule_hrtimeout(&exp, HRTIMER_MODE_ABS))\n\t\t\tbreak;\n\t} while (!fatal_signal_pending(current));\n\tio_schedule_finish(tok);\n\n\tif (use_memdelay)\n\t\tpsi_memstall_leave(&pflags);\n}\n\n \nvoid blkcg_maybe_throttle_current(void)\n{\n\tstruct gendisk *disk = current->throttle_disk;\n\tstruct blkcg *blkcg;\n\tstruct blkcg_gq *blkg;\n\tbool use_memdelay = current->use_memdelay;\n\n\tif (!disk)\n\t\treturn;\n\n\tcurrent->throttle_disk = NULL;\n\tcurrent->use_memdelay = false;\n\n\trcu_read_lock();\n\tblkcg = css_to_blkcg(blkcg_css());\n\tif (!blkcg)\n\t\tgoto out;\n\tblkg = blkg_lookup(blkcg, disk->queue);\n\tif (!blkg)\n\t\tgoto out;\n\tif (!blkg_tryget(blkg))\n\t\tgoto out;\n\trcu_read_unlock();\n\n\tblkcg_maybe_throttle_blkg(blkg, use_memdelay);\n\tblkg_put(blkg);\n\tput_disk(disk);\n\treturn;\nout:\n\trcu_read_unlock();\n}\n\n \nvoid blkcg_schedule_throttle(struct gendisk *disk, bool use_memdelay)\n{\n\tif (unlikely(current->flags & PF_KTHREAD))\n\t\treturn;\n\n\tif (current->throttle_disk != disk) {\n\t\tif (test_bit(GD_DEAD, &disk->state))\n\t\t\treturn;\n\t\tget_device(disk_to_dev(disk));\n\n\t\tif (current->throttle_disk)\n\t\t\tput_disk(current->throttle_disk);\n\t\tcurrent->throttle_disk = disk;\n\t}\n\n\tif (use_memdelay)\n\t\tcurrent->use_memdelay = use_memdelay;\n\tset_notify_resume(current);\n}\n\n \nvoid blkcg_add_delay(struct blkcg_gq *blkg, u64 now, u64 delta)\n{\n\tif (WARN_ON_ONCE(atomic_read(&blkg->use_delay) < 0))\n\t\treturn;\n\tblkcg_scale_delay(blkg, now);\n\tatomic64_add(delta, &blkg->delay_nsec);\n}\n\n \nstatic inline struct blkcg_gq *blkg_tryget_closest(struct bio *bio,\n\t\tstruct cgroup_subsys_state *css)\n{\n\tstruct blkcg_gq *blkg, *ret_blkg = NULL;\n\n\trcu_read_lock();\n\tblkg = blkg_lookup_create(css_to_blkcg(css), bio->bi_bdev->bd_disk);\n\twhile (blkg) {\n\t\tif (blkg_tryget(blkg)) {\n\t\t\tret_blkg = blkg;\n\t\t\tbreak;\n\t\t}\n\t\tblkg = blkg->parent;\n\t}\n\trcu_read_unlock();\n\n\treturn ret_blkg;\n}\n\n \nvoid bio_associate_blkg_from_css(struct bio *bio,\n\t\t\t\t struct cgroup_subsys_state *css)\n{\n\tif (bio->bi_blkg)\n\t\tblkg_put(bio->bi_blkg);\n\n\tif (css && css->parent) {\n\t\tbio->bi_blkg = blkg_tryget_closest(bio, css);\n\t} else {\n\t\tblkg_get(bdev_get_queue(bio->bi_bdev)->root_blkg);\n\t\tbio->bi_blkg = bdev_get_queue(bio->bi_bdev)->root_blkg;\n\t}\n}\nEXPORT_SYMBOL_GPL(bio_associate_blkg_from_css);\n\n \nvoid bio_associate_blkg(struct bio *bio)\n{\n\tstruct cgroup_subsys_state *css;\n\n\trcu_read_lock();\n\n\tif (bio->bi_blkg)\n\t\tcss = bio_blkcg_css(bio);\n\telse\n\t\tcss = blkcg_css();\n\n\tbio_associate_blkg_from_css(bio, css);\n\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(bio_associate_blkg);\n\n \nvoid bio_clone_blkg_association(struct bio *dst, struct bio *src)\n{\n\tif (src->bi_blkg)\n\t\tbio_associate_blkg_from_css(dst, bio_blkcg_css(src));\n}\nEXPORT_SYMBOL_GPL(bio_clone_blkg_association);\n\nstatic int blk_cgroup_io_type(struct bio *bio)\n{\n\tif (op_is_discard(bio->bi_opf))\n\t\treturn BLKG_IOSTAT_DISCARD;\n\tif (op_is_write(bio->bi_opf))\n\t\treturn BLKG_IOSTAT_WRITE;\n\treturn BLKG_IOSTAT_READ;\n}\n\nvoid blk_cgroup_bio_start(struct bio *bio)\n{\n\tstruct blkcg *blkcg = bio->bi_blkg->blkcg;\n\tint rwd = blk_cgroup_io_type(bio), cpu;\n\tstruct blkg_iostat_set *bis;\n\tunsigned long flags;\n\n\tif (!cgroup_subsys_on_dfl(io_cgrp_subsys))\n\t\treturn;\n\n\t \n\tif (!cgroup_parent(blkcg->css.cgroup))\n\t\treturn;\n\n\tcpu = get_cpu();\n\tbis = per_cpu_ptr(bio->bi_blkg->iostat_cpu, cpu);\n\tflags = u64_stats_update_begin_irqsave(&bis->sync);\n\n\t \n\tif (!bio_flagged(bio, BIO_CGROUP_ACCT)) {\n\t\tbio_set_flag(bio, BIO_CGROUP_ACCT);\n\t\tbis->cur.bytes[rwd] += bio->bi_iter.bi_size;\n\t}\n\tbis->cur.ios[rwd]++;\n\n\t \n\tif (!READ_ONCE(bis->lqueued)) {\n\t\tstruct llist_head *lhead = this_cpu_ptr(blkcg->lhead);\n\n\t\tllist_add(&bis->lnode, lhead);\n\t\tWRITE_ONCE(bis->lqueued, true);\n\t}\n\n\tu64_stats_update_end_irqrestore(&bis->sync, flags);\n\tcgroup_rstat_updated(blkcg->css.cgroup, cpu);\n\tput_cpu();\n}\n\nbool blk_cgroup_congested(void)\n{\n\tstruct cgroup_subsys_state *css;\n\tbool ret = false;\n\n\trcu_read_lock();\n\tfor (css = blkcg_css(); css; css = css->parent) {\n\t\tif (atomic_read(&css->cgroup->congestion_count)) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn ret;\n}\n\nmodule_param(blkcg_debug_stats, bool, 0644);\nMODULE_PARM_DESC(blkcg_debug_stats, \"True if you want debug stats, false if not\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}