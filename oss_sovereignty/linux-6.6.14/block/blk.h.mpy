{
  "module_name": "blk.h",
  "hash_id": "3296e4b6eb416e21520144a6460020f72114c763a27dc00140743ed886f2b805",
  "original_prompt": "Ingested from linux-6.6.14/block/blk.h",
  "human_readable_source": " \n#ifndef BLK_INTERNAL_H\n#define BLK_INTERNAL_H\n\n#include <linux/blk-crypto.h>\n#include <linux/memblock.h>\t \n#include <xen/xen.h>\n#include \"blk-crypto-internal.h\"\n\nstruct elevator_type;\n\n \n#define BLK_MAX_TIMEOUT\t\t(5 * HZ)\n\nextern struct dentry *blk_debugfs_root;\n\nstruct blk_flush_queue {\n\tspinlock_t\t\tmq_flush_lock;\n\tunsigned int\t\tflush_pending_idx:1;\n\tunsigned int\t\tflush_running_idx:1;\n\tblk_status_t \t\trq_status;\n\tunsigned long\t\tflush_pending_since;\n\tstruct list_head\tflush_queue[2];\n\tunsigned long\t\tflush_data_in_flight;\n\tstruct request\t\t*flush_rq;\n};\n\nbool is_flush_rq(struct request *req);\n\nstruct blk_flush_queue *blk_alloc_flush_queue(int node, int cmd_size,\n\t\t\t\t\t      gfp_t flags);\nvoid blk_free_flush_queue(struct blk_flush_queue *q);\n\nvoid blk_freeze_queue(struct request_queue *q);\nvoid __blk_mq_unfreeze_queue(struct request_queue *q, bool force_atomic);\nvoid blk_queue_start_drain(struct request_queue *q);\nint __bio_queue_enter(struct request_queue *q, struct bio *bio);\nvoid submit_bio_noacct_nocheck(struct bio *bio);\n\nstatic inline bool blk_try_enter_queue(struct request_queue *q, bool pm)\n{\n\trcu_read_lock();\n\tif (!percpu_ref_tryget_live_rcu(&q->q_usage_counter))\n\t\tgoto fail;\n\n\t \n\tif (blk_queue_pm_only(q) &&\n\t    (!pm || queue_rpm_status(q) == RPM_SUSPENDED))\n\t\tgoto fail_put;\n\n\trcu_read_unlock();\n\treturn true;\n\nfail_put:\n\tblk_queue_exit(q);\nfail:\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic inline int bio_queue_enter(struct bio *bio)\n{\n\tstruct request_queue *q = bdev_get_queue(bio->bi_bdev);\n\n\tif (blk_try_enter_queue(q, false))\n\t\treturn 0;\n\treturn __bio_queue_enter(q, bio);\n}\n\n#define BIO_INLINE_VECS 4\nstruct bio_vec *bvec_alloc(mempool_t *pool, unsigned short *nr_vecs,\n\t\tgfp_t gfp_mask);\nvoid bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned short nr_vecs);\n\nbool bvec_try_merge_hw_page(struct request_queue *q, struct bio_vec *bv,\n\t\tstruct page *page, unsigned len, unsigned offset,\n\t\tbool *same_page);\n\nstatic inline bool biovec_phys_mergeable(struct request_queue *q,\n\t\tstruct bio_vec *vec1, struct bio_vec *vec2)\n{\n\tunsigned long mask = queue_segment_boundary(q);\n\tphys_addr_t addr1 = page_to_phys(vec1->bv_page) + vec1->bv_offset;\n\tphys_addr_t addr2 = page_to_phys(vec2->bv_page) + vec2->bv_offset;\n\n\t \n\tif (IS_ENABLED(CONFIG_KMSAN))\n\t\treturn false;\n\n\tif (addr1 + vec1->bv_len != addr2)\n\t\treturn false;\n\tif (xen_domain() && !xen_biovec_phys_mergeable(vec1, vec2->bv_page))\n\t\treturn false;\n\tif ((addr1 | mask) != ((addr2 + vec2->bv_len - 1) | mask))\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool __bvec_gap_to_prev(const struct queue_limits *lim,\n\t\tstruct bio_vec *bprv, unsigned int offset)\n{\n\treturn (offset & lim->virt_boundary_mask) ||\n\t\t((bprv->bv_offset + bprv->bv_len) & lim->virt_boundary_mask);\n}\n\n \nstatic inline bool bvec_gap_to_prev(const struct queue_limits *lim,\n\t\tstruct bio_vec *bprv, unsigned int offset)\n{\n\tif (!lim->virt_boundary_mask)\n\t\treturn false;\n\treturn __bvec_gap_to_prev(lim, bprv, offset);\n}\n\nstatic inline bool rq_mergeable(struct request *rq)\n{\n\tif (blk_rq_is_passthrough(rq))\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_FLUSH)\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_WRITE_ZEROES)\n\t\treturn false;\n\n\tif (req_op(rq) == REQ_OP_ZONE_APPEND)\n\t\treturn false;\n\n\tif (rq->cmd_flags & REQ_NOMERGE_FLAGS)\n\t\treturn false;\n\tif (rq->rq_flags & RQF_NOMERGE_FLAGS)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic inline bool blk_discard_mergable(struct request *req)\n{\n\tif (req_op(req) == REQ_OP_DISCARD &&\n\t    queue_max_discard_segments(req->q) > 1)\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline unsigned int blk_rq_get_max_segments(struct request *rq)\n{\n\tif (req_op(rq) == REQ_OP_DISCARD)\n\t\treturn queue_max_discard_segments(rq->q);\n\treturn queue_max_segments(rq->q);\n}\n\nstatic inline unsigned int blk_queue_get_max_sectors(struct request_queue *q,\n\t\t\t\t\t\t     enum req_op op)\n{\n\tif (unlikely(op == REQ_OP_DISCARD || op == REQ_OP_SECURE_ERASE))\n\t\treturn min(q->limits.max_discard_sectors,\n\t\t\t   UINT_MAX >> SECTOR_SHIFT);\n\n\tif (unlikely(op == REQ_OP_WRITE_ZEROES))\n\t\treturn q->limits.max_write_zeroes_sectors;\n\n\treturn q->limits.max_sectors;\n}\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\nvoid blk_flush_integrity(void);\nbool __bio_integrity_endio(struct bio *);\nvoid bio_integrity_free(struct bio *bio);\nstatic inline bool bio_integrity_endio(struct bio *bio)\n{\n\tif (bio_integrity(bio))\n\t\treturn __bio_integrity_endio(bio);\n\treturn true;\n}\n\nbool blk_integrity_merge_rq(struct request_queue *, struct request *,\n\t\tstruct request *);\nbool blk_integrity_merge_bio(struct request_queue *, struct request *,\n\t\tstruct bio *);\n\nstatic inline bool integrity_req_gap_back_merge(struct request *req,\n\t\tstruct bio *next)\n{\n\tstruct bio_integrity_payload *bip = bio_integrity(req->bio);\n\tstruct bio_integrity_payload *bip_next = bio_integrity(next);\n\n\treturn bvec_gap_to_prev(&req->q->limits,\n\t\t\t\t&bip->bip_vec[bip->bip_vcnt - 1],\n\t\t\t\tbip_next->bip_vec[0].bv_offset);\n}\n\nstatic inline bool integrity_req_gap_front_merge(struct request *req,\n\t\tstruct bio *bio)\n{\n\tstruct bio_integrity_payload *bip = bio_integrity(bio);\n\tstruct bio_integrity_payload *bip_next = bio_integrity(req->bio);\n\n\treturn bvec_gap_to_prev(&req->q->limits,\n\t\t\t\t&bip->bip_vec[bip->bip_vcnt - 1],\n\t\t\t\tbip_next->bip_vec[0].bv_offset);\n}\n\nextern const struct attribute_group blk_integrity_attr_group;\n#else  \nstatic inline bool blk_integrity_merge_rq(struct request_queue *rq,\n\t\tstruct request *r1, struct request *r2)\n{\n\treturn true;\n}\nstatic inline bool blk_integrity_merge_bio(struct request_queue *rq,\n\t\tstruct request *r, struct bio *b)\n{\n\treturn true;\n}\nstatic inline bool integrity_req_gap_back_merge(struct request *req,\n\t\tstruct bio *next)\n{\n\treturn false;\n}\nstatic inline bool integrity_req_gap_front_merge(struct request *req,\n\t\tstruct bio *bio)\n{\n\treturn false;\n}\n\nstatic inline void blk_flush_integrity(void)\n{\n}\nstatic inline bool bio_integrity_endio(struct bio *bio)\n{\n\treturn true;\n}\nstatic inline void bio_integrity_free(struct bio *bio)\n{\n}\n#endif  \n\nunsigned long blk_rq_timeout(unsigned long timeout);\nvoid blk_add_timer(struct request *req);\n\nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs);\nbool blk_bio_list_merge(struct request_queue *q, struct list_head *list,\n\t\t\tstruct bio *bio, unsigned int nr_segs);\n\n \n#define BLK_MAX_REQUEST_COUNT\t32\n#define BLK_PLUG_FLUSH_SIZE\t(128 * 1024)\n\n \n#define ELV_ON_HASH(rq) ((rq)->rq_flags & RQF_HASHED)\n\nbool blk_insert_flush(struct request *rq);\n\nint elevator_switch(struct request_queue *q, struct elevator_type *new_e);\nvoid elevator_disable(struct request_queue *q);\nvoid elevator_exit(struct request_queue *q);\nint elv_register_queue(struct request_queue *q, bool uevent);\nvoid elv_unregister_queue(struct request_queue *q);\n\nssize_t part_size_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf);\nssize_t part_stat_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf);\nssize_t part_inflight_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf);\nssize_t part_fail_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf);\nssize_t part_fail_store(struct device *dev, struct device_attribute *attr,\n\t\tconst char *buf, size_t count);\nssize_t part_timeout_show(struct device *, struct device_attribute *, char *);\nssize_t part_timeout_store(struct device *, struct device_attribute *,\n\t\t\t\tconst char *, size_t);\n\nstatic inline bool bio_may_exceed_limits(struct bio *bio,\n\t\t\t\t\t const struct queue_limits *lim)\n{\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn true;  \n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\treturn lim->chunk_sectors || bio->bi_vcnt != 1 ||\n\t\tbio->bi_io_vec->bv_len + bio->bi_io_vec->bv_offset > PAGE_SIZE;\n}\n\nstruct bio *__bio_split_to_limits(struct bio *bio,\n\t\t\t\t  const struct queue_limits *lim,\n\t\t\t\t  unsigned int *nr_segs);\nint ll_back_merge_fn(struct request *req, struct bio *bio,\n\t\tunsigned int nr_segs);\nbool blk_attempt_req_merge(struct request_queue *q, struct request *rq,\n\t\t\t\tstruct request *next);\nunsigned int blk_recalc_rq_segments(struct request *rq);\nvoid blk_rq_set_mixed_merge(struct request *rq);\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio);\nenum elv_merge blk_try_merge(struct request *rq, struct bio *bio);\n\nvoid blk_set_default_limits(struct queue_limits *lim);\nint blk_dev_init(void);\n\n \nstatic inline bool blk_do_io_stat(struct request *rq)\n{\n\treturn (rq->rq_flags & RQF_IO_STAT) && !blk_rq_is_passthrough(rq);\n}\n\nvoid update_io_ticks(struct block_device *part, unsigned long now, bool end);\n\nstatic inline void req_set_nomerge(struct request_queue *q, struct request *req)\n{\n\treq->cmd_flags |= REQ_NOMERGE;\n\tif (req == q->last_merge)\n\t\tq->last_merge = NULL;\n}\n\n \nstruct io_cq *ioc_find_get_icq(struct request_queue *q);\nstruct io_cq *ioc_lookup_icq(struct request_queue *q);\n#ifdef CONFIG_BLK_ICQ\nvoid ioc_clear_queue(struct request_queue *q);\n#else\nstatic inline void ioc_clear_queue(struct request_queue *q)\n{\n}\n#endif  \n\n#ifdef CONFIG_BLK_DEV_THROTTLING_LOW\nextern ssize_t blk_throtl_sample_time_show(struct request_queue *q, char *page);\nextern ssize_t blk_throtl_sample_time_store(struct request_queue *q,\n\tconst char *page, size_t count);\nextern void blk_throtl_bio_endio(struct bio *bio);\nextern void blk_throtl_stat_add(struct request *rq, u64 time);\n#else\nstatic inline void blk_throtl_bio_endio(struct bio *bio) { }\nstatic inline void blk_throtl_stat_add(struct request *rq, u64 time) { }\n#endif\n\nstruct bio *__blk_queue_bounce(struct bio *bio, struct request_queue *q);\n\nstatic inline bool blk_queue_may_bounce(struct request_queue *q)\n{\n\treturn IS_ENABLED(CONFIG_BOUNCE) &&\n\t\tq->limits.bounce == BLK_BOUNCE_HIGH &&\n\t\tmax_low_pfn >= max_pfn;\n}\n\nstatic inline struct bio *blk_queue_bounce(struct bio *bio,\n\t\tstruct request_queue *q)\n{\n\tif (unlikely(blk_queue_may_bounce(q) && bio_has_data(bio)))\n\t\treturn __blk_queue_bounce(bio, q);\n\treturn bio;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nvoid disk_free_zone_bitmaps(struct gendisk *disk);\nvoid disk_clear_zone_settings(struct gendisk *disk);\nint blkdev_report_zones_ioctl(struct block_device *bdev, unsigned int cmd,\n\t\tunsigned long arg);\nint blkdev_zone_mgmt_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\tunsigned int cmd, unsigned long arg);\n#else  \nstatic inline void disk_free_zone_bitmaps(struct gendisk *disk) {}\nstatic inline void disk_clear_zone_settings(struct gendisk *disk) {}\nstatic inline int blkdev_report_zones_ioctl(struct block_device *bdev,\n\t\tunsigned int cmd, unsigned long arg)\n{\n\treturn -ENOTTY;\n}\nstatic inline int blkdev_zone_mgmt_ioctl(struct block_device *bdev,\n\t\tblk_mode_t mode, unsigned int cmd, unsigned long arg)\n{\n\treturn -ENOTTY;\n}\n#endif  \n\nstruct block_device *bdev_alloc(struct gendisk *disk, u8 partno);\nvoid bdev_add(struct block_device *bdev, dev_t dev);\n\nint blk_alloc_ext_minor(void);\nvoid blk_free_ext_minor(unsigned int minor);\n#define ADDPART_FLAG_NONE\t0\n#define ADDPART_FLAG_RAID\t1\n#define ADDPART_FLAG_WHOLEDISK\t2\nint bdev_add_partition(struct gendisk *disk, int partno, sector_t start,\n\t\tsector_t length);\nint bdev_del_partition(struct gendisk *disk, int partno);\nint bdev_resize_partition(struct gendisk *disk, int partno, sector_t start,\n\t\tsector_t length);\nvoid drop_partition(struct block_device *part);\n\nvoid bdev_set_nr_sectors(struct block_device *bdev, sector_t sectors);\n\nstruct gendisk *__alloc_disk_node(struct request_queue *q, int node_id,\n\t\tstruct lock_class_key *lkclass);\n\nint bio_add_hw_page(struct request_queue *q, struct bio *bio,\n\t\tstruct page *page, unsigned int len, unsigned int offset,\n\t\tunsigned int max_sectors, bool *same_page);\n\n \nstatic inline void bio_release_page(struct bio *bio, struct page *page)\n{\n\tif (bio_flagged(bio, BIO_PAGE_PINNED))\n\t\tunpin_user_page(page);\n}\n\nstruct request_queue *blk_alloc_queue(int node_id);\n\nint disk_scan_partitions(struct gendisk *disk, blk_mode_t mode);\n\nint disk_alloc_events(struct gendisk *disk);\nvoid disk_add_events(struct gendisk *disk);\nvoid disk_del_events(struct gendisk *disk);\nvoid disk_release_events(struct gendisk *disk);\nvoid disk_block_events(struct gendisk *disk);\nvoid disk_unblock_events(struct gendisk *disk);\nvoid disk_flush_events(struct gendisk *disk, unsigned int mask);\nextern struct device_attribute dev_attr_events;\nextern struct device_attribute dev_attr_events_async;\nextern struct device_attribute dev_attr_events_poll_msecs;\n\nextern struct attribute_group blk_trace_attr_group;\n\nblk_mode_t file_to_blk_mode(struct file *file);\nint truncate_bdev_range(struct block_device *bdev, blk_mode_t mode,\n\t\tloff_t lstart, loff_t lend);\nlong blkdev_ioctl(struct file *file, unsigned cmd, unsigned long arg);\nlong compat_blkdev_ioctl(struct file *file, unsigned cmd, unsigned long arg);\n\nextern const struct address_space_operations def_blk_aops;\n\nint disk_register_independent_access_ranges(struct gendisk *disk);\nvoid disk_unregister_independent_access_ranges(struct gendisk *disk);\n\n#ifdef CONFIG_FAIL_MAKE_REQUEST\nbool should_fail_request(struct block_device *part, unsigned int bytes);\n#else  \nstatic inline bool should_fail_request(struct block_device *part,\n\t\t\t\t\tunsigned int bytes)\n{\n\treturn false;\n}\n#endif  \n\n \n#define req_ref_zero_or_close_to_overflow(req)\t\\\n\t((unsigned int) atomic_read(&(req->ref)) + 127u <= 127u)\n\nstatic inline bool req_ref_inc_not_zero(struct request *req)\n{\n\treturn atomic_inc_not_zero(&req->ref);\n}\n\nstatic inline bool req_ref_put_and_test(struct request *req)\n{\n\tWARN_ON_ONCE(req_ref_zero_or_close_to_overflow(req));\n\treturn atomic_dec_and_test(&req->ref);\n}\n\nstatic inline void req_ref_set(struct request *req, int value)\n{\n\tatomic_set(&req->ref, value);\n}\n\nstatic inline int req_ref_read(struct request *req)\n{\n\treturn atomic_read(&req->ref);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}