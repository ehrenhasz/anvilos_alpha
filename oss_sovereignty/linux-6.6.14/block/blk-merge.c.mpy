{
  "module_name": "blk-merge.c",
  "hash_id": "adda8dc7a130aa76b423a297af1adc02250adc95f4658373aa0eb8c93fb268b7",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-merge.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/scatterlist.h>\n#include <linux/part_stat.h>\n#include <linux/blk-cgroup.h>\n\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-rq-qos.h\"\n#include \"blk-throttle.h\"\n\nstatic inline void bio_get_first_bvec(struct bio *bio, struct bio_vec *bv)\n{\n\t*bv = mp_bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);\n}\n\nstatic inline void bio_get_last_bvec(struct bio *bio, struct bio_vec *bv)\n{\n\tstruct bvec_iter iter = bio->bi_iter;\n\tint idx;\n\n\tbio_get_first_bvec(bio, bv);\n\tif (bv->bv_len == bio->bi_iter.bi_size)\n\t\treturn;\t\t \n\n\tbio_advance_iter(bio, &iter, iter.bi_size);\n\n\tif (!iter.bi_bvec_done)\n\t\tidx = iter.bi_idx - 1;\n\telse\t \n\t\tidx = iter.bi_idx;\n\n\t*bv = bio->bi_io_vec[idx];\n\n\t \n\tif (iter.bi_bvec_done)\n\t\tbv->bv_len = iter.bi_bvec_done;\n}\n\nstatic inline bool bio_will_gap(struct request_queue *q,\n\t\tstruct request *prev_rq, struct bio *prev, struct bio *next)\n{\n\tstruct bio_vec pb, nb;\n\n\tif (!bio_has_data(prev) || !queue_virt_boundary(q))\n\t\treturn false;\n\n\t \n\tif (prev_rq)\n\t\tbio_get_first_bvec(prev_rq->bio, &pb);\n\telse\n\t\tbio_get_first_bvec(prev, &pb);\n\tif (pb.bv_offset & queue_virt_boundary(q))\n\t\treturn true;\n\n\t \n\tbio_get_last_bvec(prev, &pb);\n\tbio_get_first_bvec(next, &nb);\n\tif (biovec_phys_mergeable(q, &pb, &nb))\n\t\treturn false;\n\treturn __bvec_gap_to_prev(&q->limits, &pb, nb.bv_offset);\n}\n\nstatic inline bool req_gap_back_merge(struct request *req, struct bio *bio)\n{\n\treturn bio_will_gap(req->q, req, req->biotail, bio);\n}\n\nstatic inline bool req_gap_front_merge(struct request *req, struct bio *bio)\n{\n\treturn bio_will_gap(req->q, NULL, bio, req->bio);\n}\n\n \nstatic unsigned int bio_allowed_max_sectors(const struct queue_limits *lim)\n{\n\treturn round_down(UINT_MAX, lim->logical_block_size) >> SECTOR_SHIFT;\n}\n\nstatic struct bio *bio_split_discard(struct bio *bio,\n\t\t\t\t     const struct queue_limits *lim,\n\t\t\t\t     unsigned *nsegs, struct bio_set *bs)\n{\n\tunsigned int max_discard_sectors, granularity;\n\tsector_t tmp;\n\tunsigned split_sectors;\n\n\t*nsegs = 1;\n\n\t \n\tgranularity = max(lim->discard_granularity >> 9, 1U);\n\n\tmax_discard_sectors =\n\t\tmin(lim->max_discard_sectors, bio_allowed_max_sectors(lim));\n\tmax_discard_sectors -= max_discard_sectors % granularity;\n\n\tif (unlikely(!max_discard_sectors)) {\n\t\t \n\t\treturn NULL;\n\t}\n\n\tif (bio_sectors(bio) <= max_discard_sectors)\n\t\treturn NULL;\n\n\tsplit_sectors = max_discard_sectors;\n\n\t \n\ttmp = bio->bi_iter.bi_sector + split_sectors -\n\t\t((lim->discard_alignment >> 9) % granularity);\n\ttmp = sector_div(tmp, granularity);\n\n\tif (split_sectors > tmp)\n\t\tsplit_sectors -= tmp;\n\n\treturn bio_split(bio, split_sectors, GFP_NOIO, bs);\n}\n\nstatic struct bio *bio_split_write_zeroes(struct bio *bio,\n\t\t\t\t\t  const struct queue_limits *lim,\n\t\t\t\t\t  unsigned *nsegs, struct bio_set *bs)\n{\n\t*nsegs = 0;\n\tif (!lim->max_write_zeroes_sectors)\n\t\treturn NULL;\n\tif (bio_sectors(bio) <= lim->max_write_zeroes_sectors)\n\t\treturn NULL;\n\treturn bio_split(bio, lim->max_write_zeroes_sectors, GFP_NOIO, bs);\n}\n\n \nstatic inline unsigned get_max_io_size(struct bio *bio,\n\t\t\t\t       const struct queue_limits *lim)\n{\n\tunsigned pbs = lim->physical_block_size >> SECTOR_SHIFT;\n\tunsigned lbs = lim->logical_block_size >> SECTOR_SHIFT;\n\tunsigned max_sectors = lim->max_sectors, start, end;\n\n\tif (lim->chunk_sectors) {\n\t\tmax_sectors = min(max_sectors,\n\t\t\tblk_chunk_sectors_left(bio->bi_iter.bi_sector,\n\t\t\t\t\t       lim->chunk_sectors));\n\t}\n\n\tstart = bio->bi_iter.bi_sector & (pbs - 1);\n\tend = (start + max_sectors) & ~(pbs - 1);\n\tif (end > start)\n\t\treturn end - start;\n\treturn max_sectors & ~(lbs - 1);\n}\n\n \nstatic inline unsigned get_max_segment_size(const struct queue_limits *lim,\n\t\tstruct page *start_page, unsigned long offset)\n{\n\tunsigned long mask = lim->seg_boundary_mask;\n\n\toffset = mask & (page_to_phys(start_page) + offset);\n\n\t \n\treturn min(mask - offset, (unsigned long)lim->max_segment_size - 1) + 1;\n}\n\n \nstatic bool bvec_split_segs(const struct queue_limits *lim,\n\t\tconst struct bio_vec *bv, unsigned *nsegs, unsigned *bytes,\n\t\tunsigned max_segs, unsigned max_bytes)\n{\n\tunsigned max_len = min(max_bytes, UINT_MAX) - *bytes;\n\tunsigned len = min(bv->bv_len, max_len);\n\tunsigned total_len = 0;\n\tunsigned seg_size = 0;\n\n\twhile (len && *nsegs < max_segs) {\n\t\tseg_size = get_max_segment_size(lim, bv->bv_page,\n\t\t\t\t\t\tbv->bv_offset + total_len);\n\t\tseg_size = min(seg_size, len);\n\n\t\t(*nsegs)++;\n\t\ttotal_len += seg_size;\n\t\tlen -= seg_size;\n\n\t\tif ((bv->bv_offset + total_len) & lim->virt_boundary_mask)\n\t\t\tbreak;\n\t}\n\n\t*bytes += total_len;\n\n\t \n\treturn len > 0 || bv->bv_len > max_len;\n}\n\n \nstruct bio *bio_split_rw(struct bio *bio, const struct queue_limits *lim,\n\t\tunsigned *segs, struct bio_set *bs, unsigned max_bytes)\n{\n\tstruct bio_vec bv, bvprv, *bvprvp = NULL;\n\tstruct bvec_iter iter;\n\tunsigned nsegs = 0, bytes = 0;\n\n\tbio_for_each_bvec(bv, bio, iter) {\n\t\t \n\t\tif (bvprvp && bvec_gap_to_prev(lim, bvprvp, bv.bv_offset))\n\t\t\tgoto split;\n\n\t\tif (nsegs < lim->max_segments &&\n\t\t    bytes + bv.bv_len <= max_bytes &&\n\t\t    bv.bv_offset + bv.bv_len <= PAGE_SIZE) {\n\t\t\tnsegs++;\n\t\t\tbytes += bv.bv_len;\n\t\t} else {\n\t\t\tif (bvec_split_segs(lim, &bv, &nsegs, &bytes,\n\t\t\t\t\tlim->max_segments, max_bytes))\n\t\t\t\tgoto split;\n\t\t}\n\n\t\tbvprv = bv;\n\t\tbvprvp = &bvprv;\n\t}\n\n\t*segs = nsegs;\n\treturn NULL;\nsplit:\n\t \n\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\tbio->bi_status = BLK_STS_AGAIN;\n\t\tbio_endio(bio);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\t*segs = nsegs;\n\n\t \n\tbytes = ALIGN_DOWN(bytes, lim->logical_block_size);\n\n\t \n\tbio_clear_polled(bio);\n\treturn bio_split(bio, bytes >> SECTOR_SHIFT, GFP_NOIO, bs);\n}\nEXPORT_SYMBOL_GPL(bio_split_rw);\n\n \nstruct bio *__bio_split_to_limits(struct bio *bio,\n\t\t\t\t  const struct queue_limits *lim,\n\t\t\t\t  unsigned int *nr_segs)\n{\n\tstruct bio_set *bs = &bio->bi_bdev->bd_disk->bio_split;\n\tstruct bio *split;\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\t\tsplit = bio_split_discard(bio, lim, nr_segs, bs);\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tsplit = bio_split_write_zeroes(bio, lim, nr_segs, bs);\n\t\tbreak;\n\tdefault:\n\t\tsplit = bio_split_rw(bio, lim, nr_segs, bs,\n\t\t\t\tget_max_io_size(bio, lim) << SECTOR_SHIFT);\n\t\tif (IS_ERR(split))\n\t\t\treturn NULL;\n\t\tbreak;\n\t}\n\n\tif (split) {\n\t\t \n\t\tsplit->bi_opf |= REQ_NOMERGE;\n\n\t\tblkcg_bio_issue_init(split);\n\t\tbio_chain(split, bio);\n\t\ttrace_block_split(split, bio->bi_iter.bi_sector);\n\t\tsubmit_bio_noacct(bio);\n\t\treturn split;\n\t}\n\treturn bio;\n}\n\n \nstruct bio *bio_split_to_limits(struct bio *bio)\n{\n\tconst struct queue_limits *lim = &bdev_get_queue(bio->bi_bdev)->limits;\n\tunsigned int nr_segs;\n\n\tif (bio_may_exceed_limits(bio, lim))\n\t\treturn __bio_split_to_limits(bio, lim, &nr_segs);\n\treturn bio;\n}\nEXPORT_SYMBOL(bio_split_to_limits);\n\nunsigned int blk_recalc_rq_segments(struct request *rq)\n{\n\tunsigned int nr_phys_segs = 0;\n\tunsigned int bytes = 0;\n\tstruct req_iterator iter;\n\tstruct bio_vec bv;\n\n\tif (!rq->bio)\n\t\treturn 0;\n\n\tswitch (bio_op(rq->bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\t\tif (queue_max_discard_segments(rq->q) > 1) {\n\t\t\tstruct bio *bio = rq->bio;\n\n\t\t\tfor_each_bio(bio)\n\t\t\t\tnr_phys_segs++;\n\t\t\treturn nr_phys_segs;\n\t\t}\n\t\treturn 1;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn 0;\n\tdefault:\n\t\tbreak;\n\t}\n\n\trq_for_each_bvec(bv, rq, iter)\n\t\tbvec_split_segs(&rq->q->limits, &bv, &nr_phys_segs, &bytes,\n\t\t\t\tUINT_MAX, UINT_MAX);\n\treturn nr_phys_segs;\n}\n\nstatic inline struct scatterlist *blk_next_sg(struct scatterlist **sg,\n\t\tstruct scatterlist *sglist)\n{\n\tif (!*sg)\n\t\treturn sglist;\n\n\t \n\tsg_unmark_end(*sg);\n\treturn sg_next(*sg);\n}\n\nstatic unsigned blk_bvec_map_sg(struct request_queue *q,\n\t\tstruct bio_vec *bvec, struct scatterlist *sglist,\n\t\tstruct scatterlist **sg)\n{\n\tunsigned nbytes = bvec->bv_len;\n\tunsigned nsegs = 0, total = 0;\n\n\twhile (nbytes > 0) {\n\t\tunsigned offset = bvec->bv_offset + total;\n\t\tunsigned len = min(get_max_segment_size(&q->limits,\n\t\t\t\t   bvec->bv_page, offset), nbytes);\n\t\tstruct page *page = bvec->bv_page;\n\n\t\t \n\t\tpage += (offset >> PAGE_SHIFT);\n\t\toffset &= ~PAGE_MASK;\n\n\t\t*sg = blk_next_sg(sg, sglist);\n\t\tsg_set_page(*sg, page, len, offset);\n\n\t\ttotal += len;\n\t\tnbytes -= len;\n\t\tnsegs++;\n\t}\n\n\treturn nsegs;\n}\n\nstatic inline int __blk_bvec_map_sg(struct bio_vec bv,\n\t\tstruct scatterlist *sglist, struct scatterlist **sg)\n{\n\t*sg = blk_next_sg(sg, sglist);\n\tsg_set_page(*sg, bv.bv_page, bv.bv_len, bv.bv_offset);\n\treturn 1;\n}\n\n \nstatic inline bool\n__blk_segment_map_sg_merge(struct request_queue *q, struct bio_vec *bvec,\n\t\t\t   struct bio_vec *bvprv, struct scatterlist **sg)\n{\n\n\tint nbytes = bvec->bv_len;\n\n\tif (!*sg)\n\t\treturn false;\n\n\tif ((*sg)->length + nbytes > queue_max_segment_size(q))\n\t\treturn false;\n\n\tif (!biovec_phys_mergeable(q, bvprv, bvec))\n\t\treturn false;\n\n\t(*sg)->length += nbytes;\n\n\treturn true;\n}\n\nstatic int __blk_bios_map_sg(struct request_queue *q, struct bio *bio,\n\t\t\t     struct scatterlist *sglist,\n\t\t\t     struct scatterlist **sg)\n{\n\tstruct bio_vec bvec, bvprv = { NULL };\n\tstruct bvec_iter iter;\n\tint nsegs = 0;\n\tbool new_bio = false;\n\n\tfor_each_bio(bio) {\n\t\tbio_for_each_bvec(bvec, bio, iter) {\n\t\t\t \n\t\t\tif (new_bio &&\n\t\t\t    __blk_segment_map_sg_merge(q, &bvec, &bvprv, sg))\n\t\t\t\tgoto next_bvec;\n\n\t\t\tif (bvec.bv_offset + bvec.bv_len <= PAGE_SIZE)\n\t\t\t\tnsegs += __blk_bvec_map_sg(bvec, sglist, sg);\n\t\t\telse\n\t\t\t\tnsegs += blk_bvec_map_sg(q, &bvec, sglist, sg);\n next_bvec:\n\t\t\tnew_bio = false;\n\t\t}\n\t\tif (likely(bio->bi_iter.bi_size)) {\n\t\t\tbvprv = bvec;\n\t\t\tnew_bio = true;\n\t\t}\n\t}\n\n\treturn nsegs;\n}\n\n \nint __blk_rq_map_sg(struct request_queue *q, struct request *rq,\n\t\tstruct scatterlist *sglist, struct scatterlist **last_sg)\n{\n\tint nsegs = 0;\n\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\tnsegs = __blk_bvec_map_sg(rq->special_vec, sglist, last_sg);\n\telse if (rq->bio)\n\t\tnsegs = __blk_bios_map_sg(q, rq->bio, sglist, last_sg);\n\n\tif (*last_sg)\n\t\tsg_mark_end(*last_sg);\n\n\t \n\tWARN_ON(nsegs > blk_rq_nr_phys_segments(rq));\n\n\treturn nsegs;\n}\nEXPORT_SYMBOL(__blk_rq_map_sg);\n\nstatic inline unsigned int blk_rq_get_max_sectors(struct request *rq,\n\t\t\t\t\t\t  sector_t offset)\n{\n\tstruct request_queue *q = rq->q;\n\tunsigned int max_sectors;\n\n\tif (blk_rq_is_passthrough(rq))\n\t\treturn q->limits.max_hw_sectors;\n\n\tmax_sectors = blk_queue_get_max_sectors(q, req_op(rq));\n\tif (!q->limits.chunk_sectors ||\n\t    req_op(rq) == REQ_OP_DISCARD ||\n\t    req_op(rq) == REQ_OP_SECURE_ERASE)\n\t\treturn max_sectors;\n\treturn min(max_sectors,\n\t\t   blk_chunk_sectors_left(offset, q->limits.chunk_sectors));\n}\n\nstatic inline int ll_new_hw_segment(struct request *req, struct bio *bio,\n\t\tunsigned int nr_phys_segs)\n{\n\tif (!blk_cgroup_mergeable(req, bio))\n\t\tgoto no_merge;\n\n\tif (blk_integrity_merge_bio(req->q, req, bio) == false)\n\t\tgoto no_merge;\n\n\t \n\tif (req_op(req) == REQ_OP_DISCARD)\n\t\treturn 1;\n\n\tif (req->nr_phys_segments + nr_phys_segs > blk_rq_get_max_segments(req))\n\t\tgoto no_merge;\n\n\t \n\treq->nr_phys_segments += nr_phys_segs;\n\treturn 1;\n\nno_merge:\n\treq_set_nomerge(req->q, req);\n\treturn 0;\n}\n\nint ll_back_merge_fn(struct request *req, struct bio *bio, unsigned int nr_segs)\n{\n\tif (req_gap_back_merge(req, bio))\n\t\treturn 0;\n\tif (blk_integrity_rq(req) &&\n\t    integrity_req_gap_back_merge(req, bio))\n\t\treturn 0;\n\tif (!bio_crypt_ctx_back_mergeable(req, bio))\n\t\treturn 0;\n\tif (blk_rq_sectors(req) + bio_sectors(bio) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req))) {\n\t\treq_set_nomerge(req->q, req);\n\t\treturn 0;\n\t}\n\n\treturn ll_new_hw_segment(req, bio, nr_segs);\n}\n\nstatic int ll_front_merge_fn(struct request *req, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tif (req_gap_front_merge(req, bio))\n\t\treturn 0;\n\tif (blk_integrity_rq(req) &&\n\t    integrity_req_gap_front_merge(req, bio))\n\t\treturn 0;\n\tif (!bio_crypt_ctx_front_mergeable(req, bio))\n\t\treturn 0;\n\tif (blk_rq_sectors(req) + bio_sectors(bio) >\n\t    blk_rq_get_max_sectors(req, bio->bi_iter.bi_sector)) {\n\t\treq_set_nomerge(req->q, req);\n\t\treturn 0;\n\t}\n\n\treturn ll_new_hw_segment(req, bio, nr_segs);\n}\n\nstatic bool req_attempt_discard_merge(struct request_queue *q, struct request *req,\n\t\tstruct request *next)\n{\n\tunsigned short segments = blk_rq_nr_discard_segments(req);\n\n\tif (segments >= queue_max_discard_segments(q))\n\t\tgoto no_merge;\n\tif (blk_rq_sectors(req) + bio_sectors(next->bio) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req)))\n\t\tgoto no_merge;\n\n\treq->nr_phys_segments = segments + blk_rq_nr_discard_segments(next);\n\treturn true;\nno_merge:\n\treq_set_nomerge(q, req);\n\treturn false;\n}\n\nstatic int ll_merge_requests_fn(struct request_queue *q, struct request *req,\n\t\t\t\tstruct request *next)\n{\n\tint total_phys_segments;\n\n\tif (req_gap_back_merge(req, next->bio))\n\t\treturn 0;\n\n\t \n\tif ((blk_rq_sectors(req) + blk_rq_sectors(next)) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req)))\n\t\treturn 0;\n\n\ttotal_phys_segments = req->nr_phys_segments + next->nr_phys_segments;\n\tif (total_phys_segments > blk_rq_get_max_segments(req))\n\t\treturn 0;\n\n\tif (!blk_cgroup_mergeable(req, next->bio))\n\t\treturn 0;\n\n\tif (blk_integrity_merge_rq(q, req, next) == false)\n\t\treturn 0;\n\n\tif (!bio_crypt_ctx_merge_rq(req, next))\n\t\treturn 0;\n\n\t \n\treq->nr_phys_segments = total_phys_segments;\n\treturn 1;\n}\n\n \nvoid blk_rq_set_mixed_merge(struct request *rq)\n{\n\tblk_opf_t ff = rq->cmd_flags & REQ_FAILFAST_MASK;\n\tstruct bio *bio;\n\n\tif (rq->rq_flags & RQF_MIXED_MERGE)\n\t\treturn;\n\n\t \n\tfor (bio = rq->bio; bio; bio = bio->bi_next) {\n\t\tWARN_ON_ONCE((bio->bi_opf & REQ_FAILFAST_MASK) &&\n\t\t\t     (bio->bi_opf & REQ_FAILFAST_MASK) != ff);\n\t\tbio->bi_opf |= ff;\n\t}\n\trq->rq_flags |= RQF_MIXED_MERGE;\n}\n\nstatic inline blk_opf_t bio_failfast(const struct bio *bio)\n{\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\treturn REQ_FAILFAST_MASK;\n\n\treturn bio->bi_opf & REQ_FAILFAST_MASK;\n}\n\n \nstatic inline void blk_update_mixed_merge(struct request *req,\n\t\tstruct bio *bio, bool front_merge)\n{\n\tif (req->rq_flags & RQF_MIXED_MERGE) {\n\t\tif (bio->bi_opf & REQ_RAHEAD)\n\t\t\tbio->bi_opf |= REQ_FAILFAST_MASK;\n\n\t\tif (front_merge) {\n\t\t\treq->cmd_flags &= ~REQ_FAILFAST_MASK;\n\t\t\treq->cmd_flags |= bio->bi_opf & REQ_FAILFAST_MASK;\n\t\t}\n\t}\n}\n\nstatic void blk_account_io_merge_request(struct request *req)\n{\n\tif (blk_do_io_stat(req)) {\n\t\tpart_stat_lock();\n\t\tpart_stat_inc(req->part, merges[op_stat_group(req_op(req))]);\n\t\tpart_stat_unlock();\n\t}\n}\n\nstatic enum elv_merge blk_try_req_merge(struct request *req,\n\t\t\t\t\tstruct request *next)\n{\n\tif (blk_discard_mergable(req))\n\t\treturn ELEVATOR_DISCARD_MERGE;\n\telse if (blk_rq_pos(req) + blk_rq_sectors(req) == blk_rq_pos(next))\n\t\treturn ELEVATOR_BACK_MERGE;\n\n\treturn ELEVATOR_NO_MERGE;\n}\n\n \nstatic struct request *attempt_merge(struct request_queue *q,\n\t\t\t\t     struct request *req, struct request *next)\n{\n\tif (!rq_mergeable(req) || !rq_mergeable(next))\n\t\treturn NULL;\n\n\tif (req_op(req) != req_op(next))\n\t\treturn NULL;\n\n\tif (rq_data_dir(req) != rq_data_dir(next))\n\t\treturn NULL;\n\n\tif (req->ioprio != next->ioprio)\n\t\treturn NULL;\n\n\t \n\n\tswitch (blk_try_req_merge(req, next)) {\n\tcase ELEVATOR_DISCARD_MERGE:\n\t\tif (!req_attempt_discard_merge(q, req, next))\n\t\t\treturn NULL;\n\t\tbreak;\n\tcase ELEVATOR_BACK_MERGE:\n\t\tif (!ll_merge_requests_fn(q, req, next))\n\t\t\treturn NULL;\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\t \n\tif (((req->rq_flags | next->rq_flags) & RQF_MIXED_MERGE) ||\n\t    (req->cmd_flags & REQ_FAILFAST_MASK) !=\n\t    (next->cmd_flags & REQ_FAILFAST_MASK)) {\n\t\tblk_rq_set_mixed_merge(req);\n\t\tblk_rq_set_mixed_merge(next);\n\t}\n\n\t \n\tif (next->start_time_ns < req->start_time_ns)\n\t\treq->start_time_ns = next->start_time_ns;\n\n\treq->biotail->bi_next = next->bio;\n\treq->biotail = next->biotail;\n\n\treq->__data_len += blk_rq_bytes(next);\n\n\tif (!blk_discard_mergable(req))\n\t\telv_merge_requests(q, req, next);\n\n\tblk_crypto_rq_put_keyslot(next);\n\n\t \n\tblk_account_io_merge_request(next);\n\n\ttrace_block_rq_merge(next);\n\n\t \n\tnext->bio = NULL;\n\treturn next;\n}\n\nstatic struct request *attempt_back_merge(struct request_queue *q,\n\t\tstruct request *rq)\n{\n\tstruct request *next = elv_latter_request(q, rq);\n\n\tif (next)\n\t\treturn attempt_merge(q, rq, next);\n\n\treturn NULL;\n}\n\nstatic struct request *attempt_front_merge(struct request_queue *q,\n\t\tstruct request *rq)\n{\n\tstruct request *prev = elv_former_request(q, rq);\n\n\tif (prev)\n\t\treturn attempt_merge(q, prev, rq);\n\n\treturn NULL;\n}\n\n \nbool blk_attempt_req_merge(struct request_queue *q, struct request *rq,\n\t\t\t   struct request *next)\n{\n\treturn attempt_merge(q, rq, next);\n}\n\nbool blk_rq_merge_ok(struct request *rq, struct bio *bio)\n{\n\tif (!rq_mergeable(rq) || !bio_mergeable(bio))\n\t\treturn false;\n\n\tif (req_op(rq) != bio_op(bio))\n\t\treturn false;\n\n\t \n\tif (bio_data_dir(bio) != rq_data_dir(rq))\n\t\treturn false;\n\n\t \n\tif (!blk_cgroup_mergeable(rq, bio))\n\t\treturn false;\n\n\t \n\tif (blk_integrity_merge_bio(rq->q, rq, bio) == false)\n\t\treturn false;\n\n\t \n\tif (!bio_crypt_rq_ctx_compatible(rq, bio))\n\t\treturn false;\n\n\tif (rq->ioprio != bio_prio(bio))\n\t\treturn false;\n\n\treturn true;\n}\n\nenum elv_merge blk_try_merge(struct request *rq, struct bio *bio)\n{\n\tif (blk_discard_mergable(rq))\n\t\treturn ELEVATOR_DISCARD_MERGE;\n\telse if (blk_rq_pos(rq) + blk_rq_sectors(rq) == bio->bi_iter.bi_sector)\n\t\treturn ELEVATOR_BACK_MERGE;\n\telse if (blk_rq_pos(rq) - bio_sectors(bio) == bio->bi_iter.bi_sector)\n\t\treturn ELEVATOR_FRONT_MERGE;\n\treturn ELEVATOR_NO_MERGE;\n}\n\nstatic void blk_account_io_merge_bio(struct request *req)\n{\n\tif (!blk_do_io_stat(req))\n\t\treturn;\n\n\tpart_stat_lock();\n\tpart_stat_inc(req->part, merges[op_stat_group(req_op(req))]);\n\tpart_stat_unlock();\n}\n\nenum bio_merge_status {\n\tBIO_MERGE_OK,\n\tBIO_MERGE_NONE,\n\tBIO_MERGE_FAILED,\n};\n\nstatic enum bio_merge_status bio_attempt_back_merge(struct request *req,\n\t\tstruct bio *bio, unsigned int nr_segs)\n{\n\tconst blk_opf_t ff = bio_failfast(bio);\n\n\tif (!ll_back_merge_fn(req, bio, nr_segs))\n\t\treturn BIO_MERGE_FAILED;\n\n\ttrace_block_bio_backmerge(bio);\n\trq_qos_merge(req->q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\tblk_update_mixed_merge(req, bio, false);\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\n\tbio_crypt_free_ctx(bio);\n\n\tblk_account_io_merge_bio(req);\n\treturn BIO_MERGE_OK;\n}\n\nstatic enum bio_merge_status bio_attempt_front_merge(struct request *req,\n\t\tstruct bio *bio, unsigned int nr_segs)\n{\n\tconst blk_opf_t ff = bio_failfast(bio);\n\n\tif (!ll_front_merge_fn(req, bio, nr_segs))\n\t\treturn BIO_MERGE_FAILED;\n\n\ttrace_block_bio_frontmerge(bio);\n\trq_qos_merge(req->q, req, bio);\n\n\tif ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)\n\t\tblk_rq_set_mixed_merge(req);\n\n\tblk_update_mixed_merge(req, bio, true);\n\n\tbio->bi_next = req->bio;\n\treq->bio = bio;\n\n\treq->__sector = bio->bi_iter.bi_sector;\n\treq->__data_len += bio->bi_iter.bi_size;\n\n\tbio_crypt_do_front_merge(req, bio);\n\n\tblk_account_io_merge_bio(req);\n\treturn BIO_MERGE_OK;\n}\n\nstatic enum bio_merge_status bio_attempt_discard_merge(struct request_queue *q,\n\t\tstruct request *req, struct bio *bio)\n{\n\tunsigned short segments = blk_rq_nr_discard_segments(req);\n\n\tif (segments >= queue_max_discard_segments(q))\n\t\tgoto no_merge;\n\tif (blk_rq_sectors(req) + bio_sectors(bio) >\n\t    blk_rq_get_max_sectors(req, blk_rq_pos(req)))\n\t\tgoto no_merge;\n\n\trq_qos_merge(q, req, bio);\n\n\treq->biotail->bi_next = bio;\n\treq->biotail = bio;\n\treq->__data_len += bio->bi_iter.bi_size;\n\treq->nr_phys_segments = segments + 1;\n\n\tblk_account_io_merge_bio(req);\n\treturn BIO_MERGE_OK;\nno_merge:\n\treq_set_nomerge(q, req);\n\treturn BIO_MERGE_FAILED;\n}\n\nstatic enum bio_merge_status blk_attempt_bio_merge(struct request_queue *q,\n\t\t\t\t\t\t   struct request *rq,\n\t\t\t\t\t\t   struct bio *bio,\n\t\t\t\t\t\t   unsigned int nr_segs,\n\t\t\t\t\t\t   bool sched_allow_merge)\n{\n\tif (!blk_rq_merge_ok(rq, bio))\n\t\treturn BIO_MERGE_NONE;\n\n\tswitch (blk_try_merge(rq, bio)) {\n\tcase ELEVATOR_BACK_MERGE:\n\t\tif (!sched_allow_merge || blk_mq_sched_allow_merge(q, rq, bio))\n\t\t\treturn bio_attempt_back_merge(rq, bio, nr_segs);\n\t\tbreak;\n\tcase ELEVATOR_FRONT_MERGE:\n\t\tif (!sched_allow_merge || blk_mq_sched_allow_merge(q, rq, bio))\n\t\t\treturn bio_attempt_front_merge(rq, bio, nr_segs);\n\t\tbreak;\n\tcase ELEVATOR_DISCARD_MERGE:\n\t\treturn bio_attempt_discard_merge(q, rq, bio);\n\tdefault:\n\t\treturn BIO_MERGE_NONE;\n\t}\n\n\treturn BIO_MERGE_FAILED;\n}\n\n \nbool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct blk_plug *plug;\n\tstruct request *rq;\n\n\tplug = blk_mq_plug(bio);\n\tif (!plug || rq_list_empty(plug->mq_list))\n\t\treturn false;\n\n\trq_list_for_each(&plug->mq_list, rq) {\n\t\tif (rq->q == q) {\n\t\t\tif (blk_attempt_bio_merge(q, rq, bio, nr_segs, false) ==\n\t\t\t    BIO_MERGE_OK)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!plug->multiple_queues)\n\t\t\tbreak;\n\t}\n\treturn false;\n}\n\n \nbool blk_bio_list_merge(struct request_queue *q, struct list_head *list,\n\t\t\tstruct bio *bio, unsigned int nr_segs)\n{\n\tstruct request *rq;\n\tint checked = 8;\n\n\tlist_for_each_entry_reverse(rq, list, queuelist) {\n\t\tif (!checked--)\n\t\t\tbreak;\n\n\t\tswitch (blk_attempt_bio_merge(q, rq, bio, nr_segs, true)) {\n\t\tcase BIO_MERGE_NONE:\n\t\t\tcontinue;\n\t\tcase BIO_MERGE_OK:\n\t\t\treturn true;\n\t\tcase BIO_MERGE_FAILED:\n\t\t\treturn false;\n\t\t}\n\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(blk_bio_list_merge);\n\nbool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs, struct request **merged_request)\n{\n\tstruct request *rq;\n\n\tswitch (elv_merge(q, &rq, bio)) {\n\tcase ELEVATOR_BACK_MERGE:\n\t\tif (!blk_mq_sched_allow_merge(q, rq, bio))\n\t\t\treturn false;\n\t\tif (bio_attempt_back_merge(rq, bio, nr_segs) != BIO_MERGE_OK)\n\t\t\treturn false;\n\t\t*merged_request = attempt_back_merge(q, rq);\n\t\tif (!*merged_request)\n\t\t\telv_merged_request(q, rq, ELEVATOR_BACK_MERGE);\n\t\treturn true;\n\tcase ELEVATOR_FRONT_MERGE:\n\t\tif (!blk_mq_sched_allow_merge(q, rq, bio))\n\t\t\treturn false;\n\t\tif (bio_attempt_front_merge(rq, bio, nr_segs) != BIO_MERGE_OK)\n\t\t\treturn false;\n\t\t*merged_request = attempt_front_merge(q, rq);\n\t\tif (!*merged_request)\n\t\t\telv_merged_request(q, rq, ELEVATOR_FRONT_MERGE);\n\t\treturn true;\n\tcase ELEVATOR_DISCARD_MERGE:\n\t\treturn bio_attempt_discard_merge(q, rq, bio) == BIO_MERGE_OK;\n\tdefault:\n\t\treturn false;\n\t}\n}\nEXPORT_SYMBOL_GPL(blk_mq_sched_try_merge);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}