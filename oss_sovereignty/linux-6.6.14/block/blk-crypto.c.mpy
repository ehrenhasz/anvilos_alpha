{
  "module_name": "blk-crypto.c",
  "hash_id": "7297df999f368eb592e755ebc422c0f11658df8ec723192ad76863a65ea46867",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-crypto.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt) \"blk-crypto: \" fmt\n\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-crypto-profile.h>\n#include <linux/module.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n\n#include \"blk-crypto-internal.h\"\n\nconst struct blk_crypto_mode blk_crypto_modes[] = {\n\t[BLK_ENCRYPTION_MODE_AES_256_XTS] = {\n\t\t.name = \"AES-256-XTS\",\n\t\t.cipher_str = \"xts(aes)\",\n\t\t.keysize = 64,\n\t\t.ivsize = 16,\n\t},\n\t[BLK_ENCRYPTION_MODE_AES_128_CBC_ESSIV] = {\n\t\t.name = \"AES-128-CBC-ESSIV\",\n\t\t.cipher_str = \"essiv(cbc(aes),sha256)\",\n\t\t.keysize = 16,\n\t\t.ivsize = 16,\n\t},\n\t[BLK_ENCRYPTION_MODE_ADIANTUM] = {\n\t\t.name = \"Adiantum\",\n\t\t.cipher_str = \"adiantum(xchacha12,aes)\",\n\t\t.keysize = 32,\n\t\t.ivsize = 32,\n\t},\n\t[BLK_ENCRYPTION_MODE_SM4_XTS] = {\n\t\t.name = \"SM4-XTS\",\n\t\t.cipher_str = \"xts(sm4)\",\n\t\t.keysize = 32,\n\t\t.ivsize = 16,\n\t},\n};\n\n \nstatic int num_prealloc_crypt_ctxs = 128;\n\nmodule_param(num_prealloc_crypt_ctxs, int, 0444);\nMODULE_PARM_DESC(num_prealloc_crypt_ctxs,\n\t\t\"Number of bio crypto contexts to preallocate\");\n\nstatic struct kmem_cache *bio_crypt_ctx_cache;\nstatic mempool_t *bio_crypt_ctx_pool;\n\nstatic int __init bio_crypt_ctx_init(void)\n{\n\tsize_t i;\n\n\tbio_crypt_ctx_cache = KMEM_CACHE(bio_crypt_ctx, 0);\n\tif (!bio_crypt_ctx_cache)\n\t\tgoto out_no_mem;\n\n\tbio_crypt_ctx_pool = mempool_create_slab_pool(num_prealloc_crypt_ctxs,\n\t\t\t\t\t\t      bio_crypt_ctx_cache);\n\tif (!bio_crypt_ctx_pool)\n\t\tgoto out_no_mem;\n\n\t \n\tBUILD_BUG_ON(BLK_ENCRYPTION_MODE_INVALID != 0);\n\n\t \n\tfor (i = 0; i < BLK_ENCRYPTION_MODE_MAX; i++) {\n\t\tBUG_ON(blk_crypto_modes[i].keysize > BLK_CRYPTO_MAX_KEY_SIZE);\n\t\tBUG_ON(blk_crypto_modes[i].ivsize > BLK_CRYPTO_MAX_IV_SIZE);\n\t}\n\n\treturn 0;\nout_no_mem:\n\tpanic(\"Failed to allocate mem for bio crypt ctxs\\n\");\n}\nsubsys_initcall(bio_crypt_ctx_init);\n\nvoid bio_crypt_set_ctx(struct bio *bio, const struct blk_crypto_key *key,\n\t\t       const u64 dun[BLK_CRYPTO_DUN_ARRAY_SIZE], gfp_t gfp_mask)\n{\n\tstruct bio_crypt_ctx *bc;\n\n\t \n\tWARN_ON_ONCE(!(gfp_mask & __GFP_DIRECT_RECLAIM));\n\n\tbc = mempool_alloc(bio_crypt_ctx_pool, gfp_mask);\n\n\tbc->bc_key = key;\n\tmemcpy(bc->bc_dun, dun, sizeof(bc->bc_dun));\n\n\tbio->bi_crypt_context = bc;\n}\n\nvoid __bio_crypt_free_ctx(struct bio *bio)\n{\n\tmempool_free(bio->bi_crypt_context, bio_crypt_ctx_pool);\n\tbio->bi_crypt_context = NULL;\n}\n\nint __bio_crypt_clone(struct bio *dst, struct bio *src, gfp_t gfp_mask)\n{\n\tdst->bi_crypt_context = mempool_alloc(bio_crypt_ctx_pool, gfp_mask);\n\tif (!dst->bi_crypt_context)\n\t\treturn -ENOMEM;\n\t*dst->bi_crypt_context = *src->bi_crypt_context;\n\treturn 0;\n}\n\n \nvoid bio_crypt_dun_increment(u64 dun[BLK_CRYPTO_DUN_ARRAY_SIZE],\n\t\t\t     unsigned int inc)\n{\n\tint i;\n\n\tfor (i = 0; inc && i < BLK_CRYPTO_DUN_ARRAY_SIZE; i++) {\n\t\tdun[i] += inc;\n\t\t \n\t\tif (dun[i] < inc)\n\t\t\tinc = 1;\n\t\telse\n\t\t\tinc = 0;\n\t}\n}\n\nvoid __bio_crypt_advance(struct bio *bio, unsigned int bytes)\n{\n\tstruct bio_crypt_ctx *bc = bio->bi_crypt_context;\n\n\tbio_crypt_dun_increment(bc->bc_dun,\n\t\t\t\tbytes >> bc->bc_key->data_unit_size_bits);\n}\n\n \nbool bio_crypt_dun_is_contiguous(const struct bio_crypt_ctx *bc,\n\t\t\t\t unsigned int bytes,\n\t\t\t\t const u64 next_dun[BLK_CRYPTO_DUN_ARRAY_SIZE])\n{\n\tint i;\n\tunsigned int carry = bytes >> bc->bc_key->data_unit_size_bits;\n\n\tfor (i = 0; i < BLK_CRYPTO_DUN_ARRAY_SIZE; i++) {\n\t\tif (bc->bc_dun[i] + carry != next_dun[i])\n\t\t\treturn false;\n\t\t \n\t\tif ((bc->bc_dun[i] + carry) < carry)\n\t\t\tcarry = 1;\n\t\telse\n\t\t\tcarry = 0;\n\t}\n\n\t \n\treturn carry == 0;\n}\n\n \nstatic bool bio_crypt_ctx_compatible(struct bio_crypt_ctx *bc1,\n\t\t\t\t     struct bio_crypt_ctx *bc2)\n{\n\tif (!bc1)\n\t\treturn !bc2;\n\n\treturn bc2 && bc1->bc_key == bc2->bc_key;\n}\n\nbool bio_crypt_rq_ctx_compatible(struct request *rq, struct bio *bio)\n{\n\treturn bio_crypt_ctx_compatible(rq->crypt_ctx, bio->bi_crypt_context);\n}\n\n \nbool bio_crypt_ctx_mergeable(struct bio_crypt_ctx *bc1, unsigned int bc1_bytes,\n\t\t\t     struct bio_crypt_ctx *bc2)\n{\n\tif (!bio_crypt_ctx_compatible(bc1, bc2))\n\t\treturn false;\n\n\treturn !bc1 || bio_crypt_dun_is_contiguous(bc1, bc1_bytes, bc2->bc_dun);\n}\n\n \nstatic bool bio_crypt_check_alignment(struct bio *bio)\n{\n\tconst unsigned int data_unit_size =\n\t\tbio->bi_crypt_context->bc_key->crypto_cfg.data_unit_size;\n\tstruct bvec_iter iter;\n\tstruct bio_vec bv;\n\n\tbio_for_each_segment(bv, bio, iter) {\n\t\tif (!IS_ALIGNED(bv.bv_len | bv.bv_offset, data_unit_size))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nblk_status_t __blk_crypto_rq_get_keyslot(struct request *rq)\n{\n\treturn blk_crypto_get_keyslot(rq->q->crypto_profile,\n\t\t\t\t      rq->crypt_ctx->bc_key,\n\t\t\t\t      &rq->crypt_keyslot);\n}\n\nvoid __blk_crypto_rq_put_keyslot(struct request *rq)\n{\n\tblk_crypto_put_keyslot(rq->crypt_keyslot);\n\trq->crypt_keyslot = NULL;\n}\n\nvoid __blk_crypto_free_request(struct request *rq)\n{\n\t \n\tif (WARN_ON_ONCE(rq->crypt_keyslot))\n\t\t__blk_crypto_rq_put_keyslot(rq);\n\n\tmempool_free(rq->crypt_ctx, bio_crypt_ctx_pool);\n\trq->crypt_ctx = NULL;\n}\n\n \nbool __blk_crypto_bio_prep(struct bio **bio_ptr)\n{\n\tstruct bio *bio = *bio_ptr;\n\tconst struct blk_crypto_key *bc_key = bio->bi_crypt_context->bc_key;\n\n\t \n\tif (WARN_ON_ONCE(!bio_has_data(bio))) {\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tgoto fail;\n\t}\n\n\tif (!bio_crypt_check_alignment(bio)) {\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tgoto fail;\n\t}\n\n\t \n\tif (blk_crypto_config_supported_natively(bio->bi_bdev,\n\t\t\t\t\t\t &bc_key->crypto_cfg))\n\t\treturn true;\n\tif (blk_crypto_fallback_bio_prep(bio_ptr))\n\t\treturn true;\nfail:\n\tbio_endio(*bio_ptr);\n\treturn false;\n}\n\nint __blk_crypto_rq_bio_prep(struct request *rq, struct bio *bio,\n\t\t\t     gfp_t gfp_mask)\n{\n\tif (!rq->crypt_ctx) {\n\t\trq->crypt_ctx = mempool_alloc(bio_crypt_ctx_pool, gfp_mask);\n\t\tif (!rq->crypt_ctx)\n\t\t\treturn -ENOMEM;\n\t}\n\t*rq->crypt_ctx = *bio->bi_crypt_context;\n\treturn 0;\n}\n\n \nint blk_crypto_init_key(struct blk_crypto_key *blk_key, const u8 *raw_key,\n\t\t\tenum blk_crypto_mode_num crypto_mode,\n\t\t\tunsigned int dun_bytes,\n\t\t\tunsigned int data_unit_size)\n{\n\tconst struct blk_crypto_mode *mode;\n\n\tmemset(blk_key, 0, sizeof(*blk_key));\n\n\tif (crypto_mode >= ARRAY_SIZE(blk_crypto_modes))\n\t\treturn -EINVAL;\n\n\tmode = &blk_crypto_modes[crypto_mode];\n\tif (mode->keysize == 0)\n\t\treturn -EINVAL;\n\n\tif (dun_bytes == 0 || dun_bytes > mode->ivsize)\n\t\treturn -EINVAL;\n\n\tif (!is_power_of_2(data_unit_size))\n\t\treturn -EINVAL;\n\n\tblk_key->crypto_cfg.crypto_mode = crypto_mode;\n\tblk_key->crypto_cfg.dun_bytes = dun_bytes;\n\tblk_key->crypto_cfg.data_unit_size = data_unit_size;\n\tblk_key->data_unit_size_bits = ilog2(data_unit_size);\n\tblk_key->size = mode->keysize;\n\tmemcpy(blk_key->raw, raw_key, mode->keysize);\n\n\treturn 0;\n}\n\nbool blk_crypto_config_supported_natively(struct block_device *bdev,\n\t\t\t\t\t  const struct blk_crypto_config *cfg)\n{\n\treturn __blk_crypto_cfg_supported(bdev_get_queue(bdev)->crypto_profile,\n\t\t\t\t\t  cfg);\n}\n\n \nbool blk_crypto_config_supported(struct block_device *bdev,\n\t\t\t\t const struct blk_crypto_config *cfg)\n{\n\treturn IS_ENABLED(CONFIG_BLK_INLINE_ENCRYPTION_FALLBACK) ||\n\t       blk_crypto_config_supported_natively(bdev, cfg);\n}\n\n \nint blk_crypto_start_using_key(struct block_device *bdev,\n\t\t\t       const struct blk_crypto_key *key)\n{\n\tif (blk_crypto_config_supported_natively(bdev, &key->crypto_cfg))\n\t\treturn 0;\n\treturn blk_crypto_fallback_start_using_mode(key->crypto_cfg.crypto_mode);\n}\n\n \nvoid blk_crypto_evict_key(struct block_device *bdev,\n\t\t\t  const struct blk_crypto_key *key)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tint err;\n\n\tif (blk_crypto_config_supported_natively(bdev, &key->crypto_cfg))\n\t\terr = __blk_crypto_evict_key(q->crypto_profile, key);\n\telse\n\t\terr = blk_crypto_fallback_evict_key(key);\n\t \n\tif (err)\n\t\tpr_warn_ratelimited(\"%pg: error %d evicting key\\n\", bdev, err);\n}\nEXPORT_SYMBOL_GPL(blk_crypto_evict_key);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}