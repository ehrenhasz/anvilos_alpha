{
  "module_name": "blk-mq-tag.c",
  "hash_id": "cf873f7a0e617b0469459cd6eb5668ec3c596ec51bedf622d862e4cb4ed7a4d1",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-mq-tag.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n\n#include <linux/delay.h>\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-sched.h\"\n\n \nstatic void blk_mq_update_wake_batch(struct blk_mq_tags *tags,\n\t\tunsigned int users)\n{\n\tif (!users)\n\t\treturn;\n\n\tsbitmap_queue_recalculate_wake_batch(&tags->bitmap_tags,\n\t\t\tusers);\n\tsbitmap_queue_recalculate_wake_batch(&tags->breserved_tags,\n\t\t\tusers);\n}\n\n \nvoid __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)\n{\n\tunsigned int users;\n\tstruct blk_mq_tags *tags = hctx->tags;\n\n\t \n\tif (blk_mq_is_shared_tags(hctx->flags)) {\n\t\tstruct request_queue *q = hctx->queue;\n\n\t\tif (test_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags) ||\n\t\t    test_and_set_bit(QUEUE_FLAG_HCTX_ACTIVE, &q->queue_flags))\n\t\t\treturn;\n\t} else {\n\t\tif (test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) ||\n\t\t    test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\t\treturn;\n\t}\n\n\tspin_lock_irq(&tags->lock);\n\tusers = tags->active_queues + 1;\n\tWRITE_ONCE(tags->active_queues, users);\n\tblk_mq_update_wake_batch(tags, users);\n\tspin_unlock_irq(&tags->lock);\n}\n\n \nvoid blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)\n{\n\tsbitmap_queue_wake_all(&tags->bitmap_tags);\n\tif (include_reserve)\n\t\tsbitmap_queue_wake_all(&tags->breserved_tags);\n}\n\n \nvoid __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct blk_mq_tags *tags = hctx->tags;\n\tunsigned int users;\n\n\tif (blk_mq_is_shared_tags(hctx->flags)) {\n\t\tstruct request_queue *q = hctx->queue;\n\n\t\tif (!test_and_clear_bit(QUEUE_FLAG_HCTX_ACTIVE,\n\t\t\t\t\t&q->queue_flags))\n\t\t\treturn;\n\t} else {\n\t\tif (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))\n\t\t\treturn;\n\t}\n\n\tspin_lock_irq(&tags->lock);\n\tusers = tags->active_queues - 1;\n\tWRITE_ONCE(tags->active_queues, users);\n\tblk_mq_update_wake_batch(tags, users);\n\tspin_unlock_irq(&tags->lock);\n\n\tblk_mq_tag_wakeup_all(tags, false);\n}\n\nstatic int __blk_mq_get_tag(struct blk_mq_alloc_data *data,\n\t\t\t    struct sbitmap_queue *bt)\n{\n\tif (!data->q->elevator && !(data->flags & BLK_MQ_REQ_RESERVED) &&\n\t\t\t!hctx_may_queue(data->hctx, bt))\n\t\treturn BLK_MQ_NO_TAG;\n\n\tif (data->shallow_depth)\n\t\treturn sbitmap_queue_get_shallow(bt, data->shallow_depth);\n\telse\n\t\treturn __sbitmap_queue_get(bt);\n}\n\nunsigned long blk_mq_get_tags(struct blk_mq_alloc_data *data, int nr_tags,\n\t\t\t      unsigned int *offset)\n{\n\tstruct blk_mq_tags *tags = blk_mq_tags_from_data(data);\n\tstruct sbitmap_queue *bt = &tags->bitmap_tags;\n\tunsigned long ret;\n\n\tif (data->shallow_depth ||data->flags & BLK_MQ_REQ_RESERVED ||\n\t    data->hctx->flags & BLK_MQ_F_TAG_QUEUE_SHARED)\n\t\treturn 0;\n\tret = __sbitmap_queue_get_batch(bt, nr_tags, offset);\n\t*offset += tags->nr_reserved_tags;\n\treturn ret;\n}\n\nunsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)\n{\n\tstruct blk_mq_tags *tags = blk_mq_tags_from_data(data);\n\tstruct sbitmap_queue *bt;\n\tstruct sbq_wait_state *ws;\n\tDEFINE_SBQ_WAIT(wait);\n\tunsigned int tag_offset;\n\tint tag;\n\n\tif (data->flags & BLK_MQ_REQ_RESERVED) {\n\t\tif (unlikely(!tags->nr_reserved_tags)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn BLK_MQ_NO_TAG;\n\t\t}\n\t\tbt = &tags->breserved_tags;\n\t\ttag_offset = 0;\n\t} else {\n\t\tbt = &tags->bitmap_tags;\n\t\ttag_offset = tags->nr_reserved_tags;\n\t}\n\n\ttag = __blk_mq_get_tag(data, bt);\n\tif (tag != BLK_MQ_NO_TAG)\n\t\tgoto found_tag;\n\n\tif (data->flags & BLK_MQ_REQ_NOWAIT)\n\t\treturn BLK_MQ_NO_TAG;\n\n\tws = bt_wait_ptr(bt, data->hctx);\n\tdo {\n\t\tstruct sbitmap_queue *bt_prev;\n\n\t\t \n\t\tblk_mq_run_hw_queue(data->hctx, false);\n\n\t\t \n\t\ttag = __blk_mq_get_tag(data, bt);\n\t\tif (tag != BLK_MQ_NO_TAG)\n\t\t\tbreak;\n\n\t\tsbitmap_prepare_to_wait(bt, ws, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\ttag = __blk_mq_get_tag(data, bt);\n\t\tif (tag != BLK_MQ_NO_TAG)\n\t\t\tbreak;\n\n\t\tbt_prev = bt;\n\t\tio_schedule();\n\n\t\tsbitmap_finish_wait(bt, ws, &wait);\n\n\t\tdata->ctx = blk_mq_get_ctx(data->q);\n\t\tdata->hctx = blk_mq_map_queue(data->q, data->cmd_flags,\n\t\t\t\t\t\tdata->ctx);\n\t\ttags = blk_mq_tags_from_data(data);\n\t\tif (data->flags & BLK_MQ_REQ_RESERVED)\n\t\t\tbt = &tags->breserved_tags;\n\t\telse\n\t\t\tbt = &tags->bitmap_tags;\n\n\t\t \n\t\tif (bt != bt_prev)\n\t\t\tsbitmap_queue_wake_up(bt_prev, 1);\n\n\t\tws = bt_wait_ptr(bt, data->hctx);\n\t} while (1);\n\n\tsbitmap_finish_wait(bt, ws, &wait);\n\nfound_tag:\n\t \n\tif (unlikely(test_bit(BLK_MQ_S_INACTIVE, &data->hctx->state))) {\n\t\tblk_mq_put_tag(tags, data->ctx, tag + tag_offset);\n\t\treturn BLK_MQ_NO_TAG;\n\t}\n\treturn tag + tag_offset;\n}\n\nvoid blk_mq_put_tag(struct blk_mq_tags *tags, struct blk_mq_ctx *ctx,\n\t\t    unsigned int tag)\n{\n\tif (!blk_mq_tag_is_reserved(tags, tag)) {\n\t\tconst int real_tag = tag - tags->nr_reserved_tags;\n\n\t\tBUG_ON(real_tag >= tags->nr_tags);\n\t\tsbitmap_queue_clear(&tags->bitmap_tags, real_tag, ctx->cpu);\n\t} else {\n\t\tsbitmap_queue_clear(&tags->breserved_tags, tag, ctx->cpu);\n\t}\n}\n\nvoid blk_mq_put_tags(struct blk_mq_tags *tags, int *tag_array, int nr_tags)\n{\n\tsbitmap_queue_clear_batch(&tags->bitmap_tags, tags->nr_reserved_tags,\n\t\t\t\t\ttag_array, nr_tags);\n}\n\nstruct bt_iter_data {\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct request_queue *q;\n\tbusy_tag_iter_fn *fn;\n\tvoid *data;\n\tbool reserved;\n};\n\nstatic struct request *blk_mq_find_and_get_req(struct blk_mq_tags *tags,\n\t\tunsigned int bitnr)\n{\n\tstruct request *rq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tags->lock, flags);\n\trq = tags->rqs[bitnr];\n\tif (!rq || rq->tag != bitnr || !req_ref_inc_not_zero(rq))\n\t\trq = NULL;\n\tspin_unlock_irqrestore(&tags->lock, flags);\n\treturn rq;\n}\n\nstatic bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)\n{\n\tstruct bt_iter_data *iter_data = data;\n\tstruct blk_mq_hw_ctx *hctx = iter_data->hctx;\n\tstruct request_queue *q = iter_data->q;\n\tstruct blk_mq_tag_set *set = q->tag_set;\n\tstruct blk_mq_tags *tags;\n\tstruct request *rq;\n\tbool ret = true;\n\n\tif (blk_mq_is_shared_tags(set->flags))\n\t\ttags = set->shared_tags;\n\telse\n\t\ttags = hctx->tags;\n\n\tif (!iter_data->reserved)\n\t\tbitnr += tags->nr_reserved_tags;\n\t \n\trq = blk_mq_find_and_get_req(tags, bitnr);\n\tif (!rq)\n\t\treturn true;\n\n\tif (rq->q == q && (!hctx || rq->mq_hctx == hctx))\n\t\tret = iter_data->fn(rq, iter_data->data);\n\tblk_mq_put_rq_ref(rq);\n\treturn ret;\n}\n\n \nstatic void bt_for_each(struct blk_mq_hw_ctx *hctx, struct request_queue *q,\n\t\t\tstruct sbitmap_queue *bt, busy_tag_iter_fn *fn,\n\t\t\tvoid *data, bool reserved)\n{\n\tstruct bt_iter_data iter_data = {\n\t\t.hctx = hctx,\n\t\t.fn = fn,\n\t\t.data = data,\n\t\t.reserved = reserved,\n\t\t.q = q,\n\t};\n\n\tsbitmap_for_each_set(&bt->sb, bt_iter, &iter_data);\n}\n\nstruct bt_tags_iter_data {\n\tstruct blk_mq_tags *tags;\n\tbusy_tag_iter_fn *fn;\n\tvoid *data;\n\tunsigned int flags;\n};\n\n#define BT_TAG_ITER_RESERVED\t\t(1 << 0)\n#define BT_TAG_ITER_STARTED\t\t(1 << 1)\n#define BT_TAG_ITER_STATIC_RQS\t\t(1 << 2)\n\nstatic bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)\n{\n\tstruct bt_tags_iter_data *iter_data = data;\n\tstruct blk_mq_tags *tags = iter_data->tags;\n\tstruct request *rq;\n\tbool ret = true;\n\tbool iter_static_rqs = !!(iter_data->flags & BT_TAG_ITER_STATIC_RQS);\n\n\tif (!(iter_data->flags & BT_TAG_ITER_RESERVED))\n\t\tbitnr += tags->nr_reserved_tags;\n\n\t \n\tif (iter_static_rqs)\n\t\trq = tags->static_rqs[bitnr];\n\telse\n\t\trq = blk_mq_find_and_get_req(tags, bitnr);\n\tif (!rq)\n\t\treturn true;\n\n\tif (!(iter_data->flags & BT_TAG_ITER_STARTED) ||\n\t    blk_mq_request_started(rq))\n\t\tret = iter_data->fn(rq, iter_data->data);\n\tif (!iter_static_rqs)\n\t\tblk_mq_put_rq_ref(rq);\n\treturn ret;\n}\n\n \nstatic void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,\n\t\t\t     busy_tag_iter_fn *fn, void *data, unsigned int flags)\n{\n\tstruct bt_tags_iter_data iter_data = {\n\t\t.tags = tags,\n\t\t.fn = fn,\n\t\t.data = data,\n\t\t.flags = flags,\n\t};\n\n\tif (tags->rqs)\n\t\tsbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);\n}\n\nstatic void __blk_mq_all_tag_iter(struct blk_mq_tags *tags,\n\t\tbusy_tag_iter_fn *fn, void *priv, unsigned int flags)\n{\n\tWARN_ON_ONCE(flags & BT_TAG_ITER_RESERVED);\n\n\tif (tags->nr_reserved_tags)\n\t\tbt_tags_for_each(tags, &tags->breserved_tags, fn, priv,\n\t\t\t\t flags | BT_TAG_ITER_RESERVED);\n\tbt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, flags);\n}\n\n \nvoid blk_mq_all_tag_iter(struct blk_mq_tags *tags, busy_tag_iter_fn *fn,\n\t\tvoid *priv)\n{\n\t__blk_mq_all_tag_iter(tags, fn, priv, BT_TAG_ITER_STATIC_RQS);\n}\n\n \nvoid blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,\n\t\tbusy_tag_iter_fn *fn, void *priv)\n{\n\tunsigned int flags = tagset->flags;\n\tint i, nr_tags;\n\n\tnr_tags = blk_mq_is_shared_tags(flags) ? 1 : tagset->nr_hw_queues;\n\n\tfor (i = 0; i < nr_tags; i++) {\n\t\tif (tagset->tags && tagset->tags[i])\n\t\t\t__blk_mq_all_tag_iter(tagset->tags[i], fn, priv,\n\t\t\t\t\t      BT_TAG_ITER_STARTED);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_tagset_busy_iter);\n\nstatic bool blk_mq_tagset_count_completed_rqs(struct request *rq, void *data)\n{\n\tunsigned *count = data;\n\n\tif (blk_mq_request_completed(rq))\n\t\t(*count)++;\n\treturn true;\n}\n\n \nvoid blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset)\n{\n\twhile (true) {\n\t\tunsigned count = 0;\n\n\t\tblk_mq_tagset_busy_iter(tagset,\n\t\t\t\tblk_mq_tagset_count_completed_rqs, &count);\n\t\tif (!count)\n\t\t\tbreak;\n\t\tmsleep(5);\n\t}\n}\nEXPORT_SYMBOL(blk_mq_tagset_wait_completed_request);\n\n \nvoid blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_tag_iter_fn *fn,\n\t\tvoid *priv)\n{\n\t \n\tif (!percpu_ref_tryget(&q->q_usage_counter))\n\t\treturn;\n\n\tif (blk_mq_is_shared_tags(q->tag_set->flags)) {\n\t\tstruct blk_mq_tags *tags = q->tag_set->shared_tags;\n\t\tstruct sbitmap_queue *bresv = &tags->breserved_tags;\n\t\tstruct sbitmap_queue *btags = &tags->bitmap_tags;\n\n\t\tif (tags->nr_reserved_tags)\n\t\t\tbt_for_each(NULL, q, bresv, fn, priv, true);\n\t\tbt_for_each(NULL, q, btags, fn, priv, false);\n\t} else {\n\t\tstruct blk_mq_hw_ctx *hctx;\n\t\tunsigned long i;\n\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\tstruct blk_mq_tags *tags = hctx->tags;\n\t\t\tstruct sbitmap_queue *bresv = &tags->breserved_tags;\n\t\t\tstruct sbitmap_queue *btags = &tags->bitmap_tags;\n\n\t\t\t \n\t\t\tif (!blk_mq_hw_queue_mapped(hctx))\n\t\t\t\tcontinue;\n\n\t\t\tif (tags->nr_reserved_tags)\n\t\t\t\tbt_for_each(hctx, q, bresv, fn, priv, true);\n\t\t\tbt_for_each(hctx, q, btags, fn, priv, false);\n\t\t}\n\t}\n\tblk_queue_exit(q);\n}\n\nstatic int bt_alloc(struct sbitmap_queue *bt, unsigned int depth,\n\t\t    bool round_robin, int node)\n{\n\treturn sbitmap_queue_init_node(bt, depth, -1, round_robin, GFP_KERNEL,\n\t\t\t\t       node);\n}\n\nint blk_mq_init_bitmaps(struct sbitmap_queue *bitmap_tags,\n\t\t\tstruct sbitmap_queue *breserved_tags,\n\t\t\tunsigned int queue_depth, unsigned int reserved,\n\t\t\tint node, int alloc_policy)\n{\n\tunsigned int depth = queue_depth - reserved;\n\tbool round_robin = alloc_policy == BLK_TAG_ALLOC_RR;\n\n\tif (bt_alloc(bitmap_tags, depth, round_robin, node))\n\t\treturn -ENOMEM;\n\tif (bt_alloc(breserved_tags, reserved, round_robin, node))\n\t\tgoto free_bitmap_tags;\n\n\treturn 0;\n\nfree_bitmap_tags:\n\tsbitmap_queue_free(bitmap_tags);\n\treturn -ENOMEM;\n}\n\nstruct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,\n\t\t\t\t     unsigned int reserved_tags,\n\t\t\t\t     int node, int alloc_policy)\n{\n\tstruct blk_mq_tags *tags;\n\n\tif (total_tags > BLK_MQ_TAG_MAX) {\n\t\tpr_err(\"blk-mq: tag depth too large\\n\");\n\t\treturn NULL;\n\t}\n\n\ttags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);\n\tif (!tags)\n\t\treturn NULL;\n\n\ttags->nr_tags = total_tags;\n\ttags->nr_reserved_tags = reserved_tags;\n\tspin_lock_init(&tags->lock);\n\n\tif (blk_mq_init_bitmaps(&tags->bitmap_tags, &tags->breserved_tags,\n\t\t\t\ttotal_tags, reserved_tags, node,\n\t\t\t\talloc_policy) < 0) {\n\t\tkfree(tags);\n\t\treturn NULL;\n\t}\n\treturn tags;\n}\n\nvoid blk_mq_free_tags(struct blk_mq_tags *tags)\n{\n\tsbitmap_queue_free(&tags->bitmap_tags);\n\tsbitmap_queue_free(&tags->breserved_tags);\n\tkfree(tags);\n}\n\nint blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,\n\t\t\t    struct blk_mq_tags **tagsptr, unsigned int tdepth,\n\t\t\t    bool can_grow)\n{\n\tstruct blk_mq_tags *tags = *tagsptr;\n\n\tif (tdepth <= tags->nr_reserved_tags)\n\t\treturn -EINVAL;\n\n\t \n\tif (tdepth > tags->nr_tags) {\n\t\tstruct blk_mq_tag_set *set = hctx->queue->tag_set;\n\t\tstruct blk_mq_tags *new;\n\n\t\tif (!can_grow)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (tdepth > MAX_SCHED_RQ)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (blk_mq_is_shared_tags(set->flags))\n\t\t\treturn 0;\n\n\t\tnew = blk_mq_alloc_map_and_rqs(set, hctx->queue_num, tdepth);\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\n\t\tblk_mq_free_map_and_rqs(set, *tagsptr, hctx->queue_num);\n\t\t*tagsptr = new;\n\t} else {\n\t\t \n\t\tsbitmap_queue_resize(&tags->bitmap_tags,\n\t\t\t\ttdepth - tags->nr_reserved_tags);\n\t}\n\n\treturn 0;\n}\n\nvoid blk_mq_tag_resize_shared_tags(struct blk_mq_tag_set *set, unsigned int size)\n{\n\tstruct blk_mq_tags *tags = set->shared_tags;\n\n\tsbitmap_queue_resize(&tags->bitmap_tags, size - set->reserved_tags);\n}\n\nvoid blk_mq_tag_update_sched_shared_tags(struct request_queue *q)\n{\n\tsbitmap_queue_resize(&q->sched_shared_tags->bitmap_tags,\n\t\t\t     q->nr_requests - q->tag_set->reserved_tags);\n}\n\n \nu32 blk_mq_unique_tag(struct request *rq)\n{\n\treturn (rq->mq_hctx->queue_num << BLK_MQ_UNIQUE_TAG_BITS) |\n\t\t(rq->tag & BLK_MQ_UNIQUE_TAG_MASK);\n}\nEXPORT_SYMBOL(blk_mq_unique_tag);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}