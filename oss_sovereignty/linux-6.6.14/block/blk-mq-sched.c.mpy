{
  "module_name": "blk-mq-sched.c",
  "hash_id": "6f01bd0e605c47abb2dbc68de8137068829e2713d6196fe574410cb64fda5dac",
  "original_prompt": "Ingested from linux-6.6.14/block/blk-mq-sched.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/list_sort.h>\n\n#include <trace/events/block.h>\n\n#include \"blk.h\"\n#include \"blk-mq.h\"\n#include \"blk-mq-debugfs.h\"\n#include \"blk-mq-sched.h\"\n#include \"blk-wbt.h\"\n\n \nvoid blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)\n{\n\tif (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))\n\t\treturn;\n\n\tset_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);\n}\nEXPORT_SYMBOL_GPL(blk_mq_sched_mark_restart_hctx);\n\nvoid __blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx)\n{\n\tclear_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state);\n\n\t \n\tsmp_mb();\n\n\tblk_mq_run_hw_queue(hctx, true);\n}\n\nstatic int sched_rq_cmp(void *priv, const struct list_head *a,\n\t\t\tconst struct list_head *b)\n{\n\tstruct request *rqa = container_of(a, struct request, queuelist);\n\tstruct request *rqb = container_of(b, struct request, queuelist);\n\n\treturn rqa->mq_hctx > rqb->mq_hctx;\n}\n\nstatic bool blk_mq_dispatch_hctx_list(struct list_head *rq_list)\n{\n\tstruct blk_mq_hw_ctx *hctx =\n\t\tlist_first_entry(rq_list, struct request, queuelist)->mq_hctx;\n\tstruct request *rq;\n\tLIST_HEAD(hctx_list);\n\tunsigned int count = 0;\n\n\tlist_for_each_entry(rq, rq_list, queuelist) {\n\t\tif (rq->mq_hctx != hctx) {\n\t\t\tlist_cut_before(&hctx_list, rq_list, &rq->queuelist);\n\t\t\tgoto dispatch;\n\t\t}\n\t\tcount++;\n\t}\n\tlist_splice_tail_init(rq_list, &hctx_list);\n\ndispatch:\n\treturn blk_mq_dispatch_rq_list(hctx, &hctx_list, count);\n}\n\n#define BLK_MQ_BUDGET_DELAY\t3\t\t \n\n \nstatic int __blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\tstruct elevator_queue *e = q->elevator;\n\tbool multi_hctxs = false, run_queue = false;\n\tbool dispatched = false, busy = false;\n\tunsigned int max_dispatch;\n\tLIST_HEAD(rq_list);\n\tint count = 0;\n\n\tif (hctx->dispatch_busy)\n\t\tmax_dispatch = 1;\n\telse\n\t\tmax_dispatch = hctx->queue->nr_requests;\n\n\tdo {\n\t\tstruct request *rq;\n\t\tint budget_token;\n\n\t\tif (e->type->ops.has_work && !e->type->ops.has_work(hctx))\n\t\t\tbreak;\n\n\t\tif (!list_empty_careful(&hctx->dispatch)) {\n\t\t\tbusy = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tbudget_token = blk_mq_get_dispatch_budget(q);\n\t\tif (budget_token < 0)\n\t\t\tbreak;\n\n\t\trq = e->type->ops.dispatch_request(hctx);\n\t\tif (!rq) {\n\t\t\tblk_mq_put_dispatch_budget(q, budget_token);\n\t\t\t \n\t\t\trun_queue = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tblk_mq_set_rq_budget_token(rq, budget_token);\n\n\t\t \n\t\tlist_add_tail(&rq->queuelist, &rq_list);\n\t\tcount++;\n\t\tif (rq->mq_hctx != hctx)\n\t\t\tmulti_hctxs = true;\n\n\t\t \n\t\tif (!blk_mq_get_driver_tag(rq))\n\t\t\tbreak;\n\t} while (count < max_dispatch);\n\n\tif (!count) {\n\t\tif (run_queue)\n\t\t\tblk_mq_delay_run_hw_queues(q, BLK_MQ_BUDGET_DELAY);\n\t} else if (multi_hctxs) {\n\t\t \n\t\tlist_sort(NULL, &rq_list, sched_rq_cmp);\n\t\tdo {\n\t\t\tdispatched |= blk_mq_dispatch_hctx_list(&rq_list);\n\t\t} while (!list_empty(&rq_list));\n\t} else {\n\t\tdispatched = blk_mq_dispatch_rq_list(hctx, &rq_list, count);\n\t}\n\n\tif (busy)\n\t\treturn -EAGAIN;\n\treturn !!dispatched;\n}\n\nstatic int blk_mq_do_dispatch_sched(struct blk_mq_hw_ctx *hctx)\n{\n\tunsigned long end = jiffies + HZ;\n\tint ret;\n\n\tdo {\n\t\tret = __blk_mq_do_dispatch_sched(hctx);\n\t\tif (ret != 1)\n\t\t\tbreak;\n\t\tif (need_resched() || time_is_before_jiffies(end)) {\n\t\t\tblk_mq_delay_run_hw_queue(hctx, 0);\n\t\t\tbreak;\n\t\t}\n\t} while (1);\n\n\treturn ret;\n}\n\nstatic struct blk_mq_ctx *blk_mq_next_ctx(struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t  struct blk_mq_ctx *ctx)\n{\n\tunsigned short idx = ctx->index_hw[hctx->type];\n\n\tif (++idx == hctx->nr_ctx)\n\t\tidx = 0;\n\n\treturn hctx->ctxs[idx];\n}\n\n \nstatic int blk_mq_do_dispatch_ctx(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\tLIST_HEAD(rq_list);\n\tstruct blk_mq_ctx *ctx = READ_ONCE(hctx->dispatch_from);\n\tint ret = 0;\n\tstruct request *rq;\n\n\tdo {\n\t\tint budget_token;\n\n\t\tif (!list_empty_careful(&hctx->dispatch)) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!sbitmap_any_bit_set(&hctx->ctx_map))\n\t\t\tbreak;\n\n\t\tbudget_token = blk_mq_get_dispatch_budget(q);\n\t\tif (budget_token < 0)\n\t\t\tbreak;\n\n\t\trq = blk_mq_dequeue_from_ctx(hctx, ctx);\n\t\tif (!rq) {\n\t\t\tblk_mq_put_dispatch_budget(q, budget_token);\n\t\t\t \n\t\t\tblk_mq_delay_run_hw_queues(q, BLK_MQ_BUDGET_DELAY);\n\t\t\tbreak;\n\t\t}\n\n\t\tblk_mq_set_rq_budget_token(rq, budget_token);\n\n\t\t \n\t\tlist_add(&rq->queuelist, &rq_list);\n\n\t\t \n\t\tctx = blk_mq_next_ctx(hctx, rq->mq_ctx);\n\n\t} while (blk_mq_dispatch_rq_list(rq->mq_hctx, &rq_list, 1));\n\n\tWRITE_ONCE(hctx->dispatch_from, ctx);\n\treturn ret;\n}\n\nstatic int __blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tbool need_dispatch = false;\n\tLIST_HEAD(rq_list);\n\n\t \n\tif (!list_empty_careful(&hctx->dispatch)) {\n\t\tspin_lock(&hctx->lock);\n\t\tif (!list_empty(&hctx->dispatch))\n\t\t\tlist_splice_init(&hctx->dispatch, &rq_list);\n\t\tspin_unlock(&hctx->lock);\n\t}\n\n\t \n\tif (!list_empty(&rq_list)) {\n\t\tblk_mq_sched_mark_restart_hctx(hctx);\n\t\tif (!blk_mq_dispatch_rq_list(hctx, &rq_list, 0))\n\t\t\treturn 0;\n\t\tneed_dispatch = true;\n\t} else {\n\t\tneed_dispatch = hctx->dispatch_busy;\n\t}\n\n\tif (hctx->queue->elevator)\n\t\treturn blk_mq_do_dispatch_sched(hctx);\n\n\t \n\tif (need_dispatch)\n\t\treturn blk_mq_do_dispatch_ctx(hctx);\n\tblk_mq_flush_busy_ctxs(hctx, &rq_list);\n\tblk_mq_dispatch_rq_list(hctx, &rq_list, 0);\n\treturn 0;\n}\n\nvoid blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct request_queue *q = hctx->queue;\n\n\t \n\tif (unlikely(blk_mq_hctx_stopped(hctx) || blk_queue_quiesced(q)))\n\t\treturn;\n\n\thctx->run++;\n\n\t \n\tif (__blk_mq_sched_dispatch_requests(hctx) == -EAGAIN) {\n\t\tif (__blk_mq_sched_dispatch_requests(hctx) == -EAGAIN)\n\t\t\tblk_mq_run_hw_queue(hctx, true);\n\t}\n}\n\nbool blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\tstruct elevator_queue *e = q->elevator;\n\tstruct blk_mq_ctx *ctx;\n\tstruct blk_mq_hw_ctx *hctx;\n\tbool ret = false;\n\tenum hctx_type type;\n\n\tif (e && e->type->ops.bio_merge) {\n\t\tret = e->type->ops.bio_merge(q, bio, nr_segs);\n\t\tgoto out_put;\n\t}\n\n\tctx = blk_mq_get_ctx(q);\n\thctx = blk_mq_map_queue(q, bio->bi_opf, ctx);\n\ttype = hctx->type;\n\tif (!(hctx->flags & BLK_MQ_F_SHOULD_MERGE) ||\n\t    list_empty_careful(&ctx->rq_lists[type]))\n\t\tgoto out_put;\n\n\t \n\tspin_lock(&ctx->lock);\n\t \n\tif (blk_bio_list_merge(q, &ctx->rq_lists[type], bio, nr_segs))\n\t\tret = true;\n\n\tspin_unlock(&ctx->lock);\nout_put:\n\treturn ret;\n}\n\nbool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq,\n\t\t\t\t   struct list_head *free)\n{\n\treturn rq_mergeable(rq) && elv_attempt_insert_merge(q, rq, free);\n}\nEXPORT_SYMBOL_GPL(blk_mq_sched_try_insert_merge);\n\nstatic int blk_mq_sched_alloc_map_and_rqs(struct request_queue *q,\n\t\t\t\t\t  struct blk_mq_hw_ctx *hctx,\n\t\t\t\t\t  unsigned int hctx_idx)\n{\n\tif (blk_mq_is_shared_tags(q->tag_set->flags)) {\n\t\thctx->sched_tags = q->sched_shared_tags;\n\t\treturn 0;\n\t}\n\n\thctx->sched_tags = blk_mq_alloc_map_and_rqs(q->tag_set, hctx_idx,\n\t\t\t\t\t\t    q->nr_requests);\n\n\tif (!hctx->sched_tags)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void blk_mq_exit_sched_shared_tags(struct request_queue *queue)\n{\n\tblk_mq_free_rq_map(queue->sched_shared_tags);\n\tqueue->sched_shared_tags = NULL;\n}\n\n \nstatic void blk_mq_sched_tags_teardown(struct request_queue *q, unsigned int flags)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (hctx->sched_tags) {\n\t\t\tif (!blk_mq_is_shared_tags(flags))\n\t\t\t\tblk_mq_free_rq_map(hctx->sched_tags);\n\t\t\thctx->sched_tags = NULL;\n\t\t}\n\t}\n\n\tif (blk_mq_is_shared_tags(flags))\n\t\tblk_mq_exit_sched_shared_tags(q);\n}\n\nstatic int blk_mq_init_sched_shared_tags(struct request_queue *queue)\n{\n\tstruct blk_mq_tag_set *set = queue->tag_set;\n\n\t \n\tqueue->sched_shared_tags = blk_mq_alloc_map_and_rqs(set,\n\t\t\t\t\t\tBLK_MQ_NO_HCTX_IDX,\n\t\t\t\t\t\tMAX_SCHED_RQ);\n\tif (!queue->sched_shared_tags)\n\t\treturn -ENOMEM;\n\n\tblk_mq_tag_update_sched_shared_tags(queue);\n\n\treturn 0;\n}\n\n \nint blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)\n{\n\tunsigned int flags = q->tag_set->flags;\n\tstruct blk_mq_hw_ctx *hctx;\n\tstruct elevator_queue *eq;\n\tunsigned long i;\n\tint ret;\n\n\t \n\tq->nr_requests = 2 * min_t(unsigned int, q->tag_set->queue_depth,\n\t\t\t\t   BLKDEV_DEFAULT_RQ);\n\n\tif (blk_mq_is_shared_tags(flags)) {\n\t\tret = blk_mq_init_sched_shared_tags(q);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tret = blk_mq_sched_alloc_map_and_rqs(q, hctx, i);\n\t\tif (ret)\n\t\t\tgoto err_free_map_and_rqs;\n\t}\n\n\tret = e->ops.init_sched(q, e);\n\tif (ret)\n\t\tgoto err_free_map_and_rqs;\n\n\tmutex_lock(&q->debugfs_mutex);\n\tblk_mq_debugfs_register_sched(q);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tif (e->ops.init_hctx) {\n\t\t\tret = e->ops.init_hctx(hctx, i);\n\t\t\tif (ret) {\n\t\t\t\teq = q->elevator;\n\t\t\t\tblk_mq_sched_free_rqs(q);\n\t\t\t\tblk_mq_exit_sched(q, eq);\n\t\t\t\tkobject_put(&eq->kobj);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tmutex_lock(&q->debugfs_mutex);\n\t\tblk_mq_debugfs_register_sched_hctx(q, hctx);\n\t\tmutex_unlock(&q->debugfs_mutex);\n\t}\n\n\treturn 0;\n\nerr_free_map_and_rqs:\n\tblk_mq_sched_free_rqs(q);\n\tblk_mq_sched_tags_teardown(q, flags);\n\n\tq->elevator = NULL;\n\treturn ret;\n}\n\n \nvoid blk_mq_sched_free_rqs(struct request_queue *q)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\n\tif (blk_mq_is_shared_tags(q->tag_set->flags)) {\n\t\tblk_mq_free_rqs(q->tag_set, q->sched_shared_tags,\n\t\t\t\tBLK_MQ_NO_HCTX_IDX);\n\t} else {\n\t\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\t\tif (hctx->sched_tags)\n\t\t\t\tblk_mq_free_rqs(q->tag_set,\n\t\t\t\t\t\thctx->sched_tags, i);\n\t\t}\n\t}\n}\n\nvoid blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)\n{\n\tstruct blk_mq_hw_ctx *hctx;\n\tunsigned long i;\n\tunsigned int flags = 0;\n\n\tqueue_for_each_hw_ctx(q, hctx, i) {\n\t\tmutex_lock(&q->debugfs_mutex);\n\t\tblk_mq_debugfs_unregister_sched_hctx(hctx);\n\t\tmutex_unlock(&q->debugfs_mutex);\n\n\t\tif (e->type->ops.exit_hctx && hctx->sched_data) {\n\t\t\te->type->ops.exit_hctx(hctx, i);\n\t\t\thctx->sched_data = NULL;\n\t\t}\n\t\tflags = hctx->flags;\n\t}\n\n\tmutex_lock(&q->debugfs_mutex);\n\tblk_mq_debugfs_unregister_sched(q);\n\tmutex_unlock(&q->debugfs_mutex);\n\n\tif (e->type->ops.exit_sched)\n\t\te->type->ops.exit_sched(e);\n\tblk_mq_sched_tags_teardown(q, flags);\n\tq->elevator = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}