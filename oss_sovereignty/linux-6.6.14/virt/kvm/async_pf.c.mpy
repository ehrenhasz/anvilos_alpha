{
  "module_name": "async_pf.c",
  "hash_id": "f3f3ebd28ef3317c5d646cffb8203bdd41cfaf8bafad72de3eefd1dbd72a4dd6",
  "original_prompt": "Ingested from linux-6.6.14/virt/kvm/async_pf.c",
  "human_readable_source": "\n \n\n#include <linux/kvm_host.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/mmu_context.h>\n#include <linux/sched/mm.h>\n\n#include \"async_pf.h\"\n#include <trace/events/kvm.h>\n\nstatic struct kmem_cache *async_pf_cache;\n\nint kvm_async_pf_init(void)\n{\n\tasync_pf_cache = KMEM_CACHE(kvm_async_pf, 0);\n\n\tif (!async_pf_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid kvm_async_pf_deinit(void)\n{\n\tkmem_cache_destroy(async_pf_cache);\n\tasync_pf_cache = NULL;\n}\n\nvoid kvm_async_pf_vcpu_init(struct kvm_vcpu *vcpu)\n{\n\tINIT_LIST_HEAD(&vcpu->async_pf.done);\n\tINIT_LIST_HEAD(&vcpu->async_pf.queue);\n\tspin_lock_init(&vcpu->async_pf.lock);\n}\n\nstatic void async_pf_execute(struct work_struct *work)\n{\n\tstruct kvm_async_pf *apf =\n\t\tcontainer_of(work, struct kvm_async_pf, work);\n\tstruct mm_struct *mm = apf->mm;\n\tstruct kvm_vcpu *vcpu = apf->vcpu;\n\tunsigned long addr = apf->addr;\n\tgpa_t cr2_or_gpa = apf->cr2_or_gpa;\n\tint locked = 1;\n\tbool first;\n\n\tmight_sleep();\n\n\t \n\tmmap_read_lock(mm);\n\tget_user_pages_remote(mm, addr, 1, FOLL_WRITE, NULL, &locked);\n\tif (locked)\n\t\tmmap_read_unlock(mm);\n\n\tif (IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))\n\t\tkvm_arch_async_page_present(vcpu, apf);\n\n\tspin_lock(&vcpu->async_pf.lock);\n\tfirst = list_empty(&vcpu->async_pf.done);\n\tlist_add_tail(&apf->link, &vcpu->async_pf.done);\n\tapf->vcpu = NULL;\n\tspin_unlock(&vcpu->async_pf.lock);\n\n\tif (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)\n\t\tkvm_arch_async_page_present_queued(vcpu);\n\n\t \n\n\ttrace_kvm_async_pf_completed(addr, cr2_or_gpa);\n\n\t__kvm_vcpu_wake_up(vcpu);\n\n\tmmput(mm);\n\tkvm_put_kvm(vcpu->kvm);\n}\n\nvoid kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)\n{\n\tspin_lock(&vcpu->async_pf.lock);\n\n\t \n\twhile (!list_empty(&vcpu->async_pf.queue)) {\n\t\tstruct kvm_async_pf *work =\n\t\t\tlist_first_entry(&vcpu->async_pf.queue,\n\t\t\t\t\t typeof(*work), queue);\n\t\tlist_del(&work->queue);\n\n\t\t \n\t\tif (!work->vcpu)\n\t\t\tcontinue;\n\n\t\tspin_unlock(&vcpu->async_pf.lock);\n#ifdef CONFIG_KVM_ASYNC_PF_SYNC\n\t\tflush_work(&work->work);\n#else\n\t\tif (cancel_work_sync(&work->work)) {\n\t\t\tmmput(work->mm);\n\t\t\tkvm_put_kvm(vcpu->kvm);  \n\t\t\tkmem_cache_free(async_pf_cache, work);\n\t\t}\n#endif\n\t\tspin_lock(&vcpu->async_pf.lock);\n\t}\n\n\twhile (!list_empty(&vcpu->async_pf.done)) {\n\t\tstruct kvm_async_pf *work =\n\t\t\tlist_first_entry(&vcpu->async_pf.done,\n\t\t\t\t\t typeof(*work), link);\n\t\tlist_del(&work->link);\n\t\tkmem_cache_free(async_pf_cache, work);\n\t}\n\tspin_unlock(&vcpu->async_pf.lock);\n\n\tvcpu->async_pf.queued = 0;\n}\n\nvoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_async_pf *work;\n\n\twhile (!list_empty_careful(&vcpu->async_pf.done) &&\n\t      kvm_arch_can_dequeue_async_page_present(vcpu)) {\n\t\tspin_lock(&vcpu->async_pf.lock);\n\t\twork = list_first_entry(&vcpu->async_pf.done, typeof(*work),\n\t\t\t\t\t      link);\n\t\tlist_del(&work->link);\n\t\tspin_unlock(&vcpu->async_pf.lock);\n\n\t\tkvm_arch_async_page_ready(vcpu, work);\n\t\tif (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC))\n\t\t\tkvm_arch_async_page_present(vcpu, work);\n\n\t\tlist_del(&work->queue);\n\t\tvcpu->async_pf.queued--;\n\t\tkmem_cache_free(async_pf_cache, work);\n\t}\n}\n\n \nbool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\tunsigned long hva, struct kvm_arch_async_pf *arch)\n{\n\tstruct kvm_async_pf *work;\n\n\tif (vcpu->async_pf.queued >= ASYNC_PF_PER_VCPU)\n\t\treturn false;\n\n\t \n\tif (unlikely(kvm_is_error_hva(hva)))\n\t\treturn false;\n\n\t \n\twork = kmem_cache_zalloc(async_pf_cache, GFP_NOWAIT | __GFP_NOWARN);\n\tif (!work)\n\t\treturn false;\n\n\twork->wakeup_all = false;\n\twork->vcpu = vcpu;\n\twork->cr2_or_gpa = cr2_or_gpa;\n\twork->addr = hva;\n\twork->arch = *arch;\n\twork->mm = current->mm;\n\tmmget(work->mm);\n\tkvm_get_kvm(work->vcpu->kvm);\n\n\tINIT_WORK(&work->work, async_pf_execute);\n\n\tlist_add_tail(&work->queue, &vcpu->async_pf.queue);\n\tvcpu->async_pf.queued++;\n\twork->notpresent_injected = kvm_arch_async_page_not_present(vcpu, work);\n\n\tschedule_work(&work->work);\n\n\treturn true;\n}\n\nint kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_async_pf *work;\n\tbool first;\n\n\tif (!list_empty_careful(&vcpu->async_pf.done))\n\t\treturn 0;\n\n\twork = kmem_cache_zalloc(async_pf_cache, GFP_ATOMIC);\n\tif (!work)\n\t\treturn -ENOMEM;\n\n\twork->wakeup_all = true;\n\tINIT_LIST_HEAD(&work->queue);  \n\n\tspin_lock(&vcpu->async_pf.lock);\n\tfirst = list_empty(&vcpu->async_pf.done);\n\tlist_add_tail(&work->link, &vcpu->async_pf.done);\n\tspin_unlock(&vcpu->async_pf.lock);\n\n\tif (!IS_ENABLED(CONFIG_KVM_ASYNC_PF_SYNC) && first)\n\t\tkvm_arch_async_page_present_queued(vcpu);\n\n\tvcpu->async_pf.queued++;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}