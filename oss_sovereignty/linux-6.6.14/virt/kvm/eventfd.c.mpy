{
  "module_name": "eventfd.c",
  "hash_id": "d8cf3d2497accff6b814034b16ebbc3e69055b1df6c73b2b092111b60650747b",
  "original_prompt": "Ingested from linux-6.6.14/virt/kvm/eventfd.c",
  "human_readable_source": "\n \n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/kvm_irqfd.h>\n#include <linux/workqueue.h>\n#include <linux/syscalls.h>\n#include <linux/wait.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/list.h>\n#include <linux/eventfd.h>\n#include <linux/kernel.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/seqlock.h>\n#include <linux/irqbypass.h>\n#include <trace/events/kvm.h>\n\n#include <kvm/iodev.h>\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\nstatic struct workqueue_struct *irqfd_cleanup_wq;\n\nbool __attribute__((weak))\nkvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\treturn true;\n}\n\nstatic void\nirqfd_inject(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, inject);\n\tstruct kvm *kvm = irqfd->kvm;\n\n\tif (!irqfd->resampler) {\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 1,\n\t\t\t\tfalse);\n\t\tkvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID, irqfd->gsi, 0,\n\t\t\t\tfalse);\n\t} else\n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    irqfd->gsi, 1, false);\n}\n\nstatic void irqfd_resampler_notify(struct kvm_kernel_irqfd_resampler *resampler)\n{\n\tstruct kvm_kernel_irqfd *irqfd;\n\n\tlist_for_each_entry_srcu(irqfd, &resampler->list, resampler_link,\n\t\t\t\t srcu_read_lock_held(&resampler->kvm->irq_srcu))\n\t\teventfd_signal(irqfd->resamplefd, 1);\n}\n\n \nstatic void\nirqfd_resampler_ack(struct kvm_irq_ack_notifier *kian)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler;\n\tstruct kvm *kvm;\n\tint idx;\n\n\tresampler = container_of(kian,\n\t\t\tstruct kvm_kernel_irqfd_resampler, notifier);\n\tkvm = resampler->kvm;\n\n\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t    resampler->notifier.gsi, 0, false);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirqfd_resampler_notify(resampler);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nstatic void\nirqfd_resampler_shutdown(struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler = irqfd->resampler;\n\tstruct kvm *kvm = resampler->kvm;\n\n\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\tlist_del_rcu(&irqfd->resampler_link);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\n\tif (list_empty(&resampler->list)) {\n\t\tlist_del_rcu(&resampler->link);\n\t\tkvm_unregister_irq_ack_notifier(kvm, &resampler->notifier);\n\t\t \n\t\tkvm_set_irq(kvm, KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID,\n\t\t\t    resampler->notifier.gsi, 0, false);\n\t\tkfree(resampler);\n\t}\n\n\tmutex_unlock(&kvm->irqfds.resampler_lock);\n}\n\n \nstatic void\nirqfd_shutdown(struct work_struct *work)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(work, struct kvm_kernel_irqfd, shutdown);\n\tstruct kvm *kvm = irqfd->kvm;\n\tu64 cnt;\n\n\t \n\tsynchronize_srcu(&kvm->irq_srcu);\n\n\t \n\teventfd_ctx_remove_wait_queue(irqfd->eventfd, &irqfd->wait, &cnt);\n\n\t \n\tflush_work(&irqfd->inject);\n\n\tif (irqfd->resampler) {\n\t\tirqfd_resampler_shutdown(irqfd);\n\t\teventfd_ctx_put(irqfd->resamplefd);\n\t}\n\n\t \n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tirq_bypass_unregister_consumer(&irqfd->consumer);\n#endif\n\teventfd_ctx_put(irqfd->eventfd);\n\tkfree(irqfd);\n}\n\n\n \nstatic bool\nirqfd_is_active(struct kvm_kernel_irqfd *irqfd)\n{\n\treturn list_empty(&irqfd->list) ? false : true;\n}\n\n \nstatic void\nirqfd_deactivate(struct kvm_kernel_irqfd *irqfd)\n{\n\tBUG_ON(!irqfd_is_active(irqfd));\n\n\tlist_del_init(&irqfd->list);\n\n\tqueue_work(irqfd_cleanup_wq, &irqfd->shutdown);\n}\n\nint __attribute__((weak)) kvm_arch_set_irq_inatomic(\n\t\t\t\tstruct kvm_kernel_irq_routing_entry *irq,\n\t\t\t\tstruct kvm *kvm, int irq_source_id,\n\t\t\t\tint level,\n\t\t\t\tbool line_status)\n{\n\treturn -EWOULDBLOCK;\n}\n\n \nstatic int\nirqfd_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(wait, struct kvm_kernel_irqfd, wait);\n\t__poll_t flags = key_to_poll(key);\n\tstruct kvm_kernel_irq_routing_entry irq;\n\tstruct kvm *kvm = irqfd->kvm;\n\tunsigned seq;\n\tint idx;\n\tint ret = 0;\n\n\tif (flags & EPOLLIN) {\n\t\tu64 cnt;\n\t\teventfd_ctx_do_read(irqfd->eventfd, &cnt);\n\n\t\tidx = srcu_read_lock(&kvm->irq_srcu);\n\t\tdo {\n\t\t\tseq = read_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirq = irqfd->irq_entry;\n\t\t} while (read_seqcount_retry(&irqfd->irq_entry_sc, seq));\n\t\t \n\t\tif (kvm_arch_set_irq_inatomic(&irq, kvm,\n\t\t\t\t\t      KVM_USERSPACE_IRQ_SOURCE_ID, 1,\n\t\t\t\t\t      false) == -EWOULDBLOCK)\n\t\t\tschedule_work(&irqfd->inject);\n\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t\tret = 1;\n\t}\n\n\tif (flags & EPOLLHUP) {\n\t\t \n\t\tunsigned long iflags;\n\n\t\tspin_lock_irqsave(&kvm->irqfds.lock, iflags);\n\n\t\t \n\t\tif (irqfd_is_active(irqfd))\n\t\t\tirqfd_deactivate(irqfd);\n\n\t\tspin_unlock_irqrestore(&kvm->irqfds.lock, iflags);\n\t}\n\n\treturn ret;\n}\n\nstatic void\nirqfd_ptable_queue_proc(struct file *file, wait_queue_head_t *wqh,\n\t\t\tpoll_table *pt)\n{\n\tstruct kvm_kernel_irqfd *irqfd =\n\t\tcontainer_of(pt, struct kvm_kernel_irqfd, pt);\n\tadd_wait_queue_priority(wqh, &irqfd->wait);\n}\n\n \nstatic void irqfd_update(struct kvm *kvm, struct kvm_kernel_irqfd *irqfd)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_kernel_irq_routing_entry entries[KVM_NR_IRQCHIPS];\n\tint n_entries;\n\n\tn_entries = kvm_irq_map_gsi(kvm, entries, irqfd->gsi);\n\n\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\n\te = entries;\n\tif (n_entries == 1)\n\t\tirqfd->irq_entry = *e;\n\telse\n\t\tirqfd->irq_entry.type = 0;\n\n\twrite_seqcount_end(&irqfd->irq_entry_sc);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\nvoid __attribute__((weak)) kvm_arch_irq_bypass_stop(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nvoid __attribute__((weak)) kvm_arch_irq_bypass_start(\n\t\t\t\tstruct irq_bypass_consumer *cons)\n{\n}\n\nint  __attribute__((weak)) kvm_arch_update_irqfd_routing(\n\t\t\t\tstruct kvm *kvm, unsigned int host_irq,\n\t\t\t\tuint32_t guest_irq, bool set)\n{\n\treturn 0;\n}\n\nbool __attribute__((weak)) kvm_arch_irqfd_route_changed(\n\t\t\t\tstruct kvm_kernel_irq_routing_entry *old,\n\t\t\t\tstruct kvm_kernel_irq_routing_entry *new)\n{\n\treturn true;\n}\n#endif\n\nstatic int\nkvm_irqfd_assign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct fd f;\n\tstruct eventfd_ctx *eventfd = NULL, *resamplefd = NULL;\n\tint ret;\n\t__poll_t events;\n\tint idx;\n\n\tif (!kvm_arch_intc_initialized(kvm))\n\t\treturn -EAGAIN;\n\n\tif (!kvm_arch_irqfd_allowed(kvm, args))\n\t\treturn -EINVAL;\n\n\tirqfd = kzalloc(sizeof(*irqfd), GFP_KERNEL_ACCOUNT);\n\tif (!irqfd)\n\t\treturn -ENOMEM;\n\n\tirqfd->kvm = kvm;\n\tirqfd->gsi = args->gsi;\n\tINIT_LIST_HEAD(&irqfd->list);\n\tINIT_WORK(&irqfd->inject, irqfd_inject);\n\tINIT_WORK(&irqfd->shutdown, irqfd_shutdown);\n\tseqcount_spinlock_init(&irqfd->irq_entry_sc, &kvm->irqfds.lock);\n\n\tf = fdget(args->fd);\n\tif (!f.file) {\n\t\tret = -EBADF;\n\t\tgoto out;\n\t}\n\n\teventfd = eventfd_ctx_fileget(f.file);\n\tif (IS_ERR(eventfd)) {\n\t\tret = PTR_ERR(eventfd);\n\t\tgoto fail;\n\t}\n\n\tirqfd->eventfd = eventfd;\n\n\tif (args->flags & KVM_IRQFD_FLAG_RESAMPLE) {\n\t\tstruct kvm_kernel_irqfd_resampler *resampler;\n\n\t\tresamplefd = eventfd_ctx_fdget(args->resamplefd);\n\t\tif (IS_ERR(resamplefd)) {\n\t\t\tret = PTR_ERR(resamplefd);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tirqfd->resamplefd = resamplefd;\n\t\tINIT_LIST_HEAD(&irqfd->resampler_link);\n\n\t\tmutex_lock(&kvm->irqfds.resampler_lock);\n\n\t\tlist_for_each_entry(resampler,\n\t\t\t\t    &kvm->irqfds.resampler_list, link) {\n\t\t\tif (resampler->notifier.gsi == irqfd->gsi) {\n\t\t\t\tirqfd->resampler = resampler;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!irqfd->resampler) {\n\t\t\tresampler = kzalloc(sizeof(*resampler),\n\t\t\t\t\t    GFP_KERNEL_ACCOUNT);\n\t\t\tif (!resampler) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tresampler->kvm = kvm;\n\t\t\tINIT_LIST_HEAD(&resampler->list);\n\t\t\tresampler->notifier.gsi = irqfd->gsi;\n\t\t\tresampler->notifier.irq_acked = irqfd_resampler_ack;\n\t\t\tINIT_LIST_HEAD(&resampler->link);\n\n\t\t\tlist_add_rcu(&resampler->link, &kvm->irqfds.resampler_list);\n\t\t\tkvm_register_irq_ack_notifier(kvm,\n\t\t\t\t\t\t      &resampler->notifier);\n\t\t\tirqfd->resampler = resampler;\n\t\t}\n\n\t\tlist_add_rcu(&irqfd->resampler_link, &irqfd->resampler->list);\n\t\tsynchronize_srcu(&kvm->irq_srcu);\n\n\t\tmutex_unlock(&kvm->irqfds.resampler_lock);\n\t}\n\n\t \n\tinit_waitqueue_func_entry(&irqfd->wait, irqfd_wakeup);\n\tinit_poll_funcptr(&irqfd->pt, irqfd_ptable_queue_proc);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tret = 0;\n\tlist_for_each_entry(tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd != tmp->eventfd)\n\t\t\tcontinue;\n\t\t \n\t\tret = -EBUSY;\n\t\tspin_unlock_irq(&kvm->irqfds.lock);\n\t\tgoto fail;\n\t}\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirqfd_update(kvm, irqfd);\n\n\tlist_add_tail(&irqfd->list, &kvm->irqfds.items);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t \n\tevents = vfs_poll(f.file, &irqfd->pt);\n\n\tif (events & EPOLLIN)\n\t\tschedule_work(&irqfd->inject);\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\tif (kvm_arch_has_irq_bypass()) {\n\t\tirqfd->consumer.token = (void *)irqfd->eventfd;\n\t\tirqfd->consumer.add_producer = kvm_arch_irq_bypass_add_producer;\n\t\tirqfd->consumer.del_producer = kvm_arch_irq_bypass_del_producer;\n\t\tirqfd->consumer.stop = kvm_arch_irq_bypass_stop;\n\t\tirqfd->consumer.start = kvm_arch_irq_bypass_start;\n\t\tret = irq_bypass_register_consumer(&irqfd->consumer);\n\t\tif (ret)\n\t\t\tpr_info(\"irq bypass consumer (token %p) registration fails: %d\\n\",\n\t\t\t\tirqfd->consumer.token, ret);\n\t}\n#endif\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\t \n\tfdput(f);\n\treturn 0;\n\nfail:\n\tif (irqfd->resampler)\n\t\tirqfd_resampler_shutdown(irqfd);\n\n\tif (resamplefd && !IS_ERR(resamplefd))\n\t\teventfd_ctx_put(resamplefd);\n\n\tif (eventfd && !IS_ERR(eventfd))\n\t\teventfd_ctx_put(eventfd);\n\n\tfdput(f);\n\nout:\n\tkfree(irqfd);\n\treturn ret;\n}\n\nbool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\tint gsi, idx;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\thlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t\t  link, srcu_read_lock_held(&kvm->irq_srcu))\n\t\t\tif (kian->gsi == gsi) {\n\t\t\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t\t\t\treturn true;\n\t\t\t}\n\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(kvm_irq_has_notifier);\n\nvoid kvm_notify_acked_gsi(struct kvm *kvm, int gsi)\n{\n\tstruct kvm_irq_ack_notifier *kian;\n\n\thlist_for_each_entry_srcu(kian, &kvm->irq_ack_notifier_list,\n\t\t\t\t  link, srcu_read_lock_held(&kvm->irq_srcu))\n\t\tif (kian->gsi == gsi)\n\t\t\tkian->irq_acked(kian);\n}\n\nvoid kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin)\n{\n\tint gsi, idx;\n\n\ttrace_kvm_ack_irq(irqchip, pin);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1)\n\t\tkvm_notify_acked_gsi(kvm, gsi);\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n}\n\nvoid kvm_register_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t   struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_add_head_rcu(&kian->link, &kvm->irq_ack_notifier_list);\n\tmutex_unlock(&kvm->irq_lock);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n\nvoid kvm_unregister_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t    struct kvm_irq_ack_notifier *kian)\n{\n\tmutex_lock(&kvm->irq_lock);\n\thlist_del_init_rcu(&kian->link);\n\tmutex_unlock(&kvm->irq_lock);\n\tsynchronize_srcu(&kvm->irq_srcu);\n\tkvm_arch_post_irq_ack_notifier_list_update(kvm);\n}\n#endif\n\nvoid\nkvm_eventfd_init(struct kvm *kvm)\n{\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\tspin_lock_init(&kvm->irqfds.lock);\n\tINIT_LIST_HEAD(&kvm->irqfds.items);\n\tINIT_LIST_HEAD(&kvm->irqfds.resampler_list);\n\tmutex_init(&kvm->irqfds.resampler_lock);\n#endif\n\tINIT_LIST_HEAD(&kvm->ioeventfds);\n}\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\n \nstatic int\nkvm_irqfd_deassign(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\tstruct eventfd_ctx *eventfd;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list) {\n\t\tif (irqfd->eventfd == eventfd && irqfd->gsi == args->gsi) {\n\t\t\t \n\t\t\twrite_seqcount_begin(&irqfd->irq_entry_sc);\n\t\t\tirqfd->irq_entry.type = 0;\n\t\t\twrite_seqcount_end(&irqfd->irq_entry_sc);\n\t\t\tirqfd_deactivate(irqfd);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\teventfd_ctx_put(eventfd);\n\n\t \n\tflush_workqueue(irqfd_cleanup_wq);\n\n\treturn 0;\n}\n\nint\nkvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\tif (args->flags & ~(KVM_IRQFD_FLAG_DEASSIGN | KVM_IRQFD_FLAG_RESAMPLE))\n\t\treturn -EINVAL;\n\n\tif (args->flags & KVM_IRQFD_FLAG_DEASSIGN)\n\t\treturn kvm_irqfd_deassign(kvm, args);\n\n\treturn kvm_irqfd_assign(kvm, args);\n}\n\n \nvoid\nkvm_irqfd_release(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd, *tmp;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry_safe(irqfd, tmp, &kvm->irqfds.items, list)\n\t\tirqfd_deactivate(irqfd);\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n\n\t \n\tflush_workqueue(irqfd_cleanup_wq);\n\n}\n\n \nvoid kvm_irq_routing_update(struct kvm *kvm)\n{\n\tstruct kvm_kernel_irqfd *irqfd;\n\n\tspin_lock_irq(&kvm->irqfds.lock);\n\n\tlist_for_each_entry(irqfd, &kvm->irqfds.items, list) {\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\t\t \n\t\tstruct kvm_kernel_irq_routing_entry old = irqfd->irq_entry;\n#endif\n\n\t\tirqfd_update(kvm, irqfd);\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\n\t\tif (irqfd->producer &&\n\t\t    kvm_arch_irqfd_route_changed(&old, &irqfd->irq_entry)) {\n\t\t\tint ret = kvm_arch_update_irqfd_routing(\n\t\t\t\t\tirqfd->kvm, irqfd->producer->irq,\n\t\t\t\t\tirqfd->gsi, 1);\n\t\t\tWARN_ON(ret);\n\t\t}\n#endif\n\t}\n\n\tspin_unlock_irq(&kvm->irqfds.lock);\n}\n\nbool kvm_notify_irqfd_resampler(struct kvm *kvm,\n\t\t\t\tunsigned int irqchip,\n\t\t\t\tunsigned int pin)\n{\n\tstruct kvm_kernel_irqfd_resampler *resampler;\n\tint gsi, idx;\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tgsi = kvm_irq_map_chip_pin(kvm, irqchip, pin);\n\tif (gsi != -1) {\n\t\tlist_for_each_entry_srcu(resampler,\n\t\t\t\t\t &kvm->irqfds.resampler_list, link,\n\t\t\t\t\t srcu_read_lock_held(&kvm->irq_srcu)) {\n\t\t\tif (resampler->notifier.gsi == gsi) {\n\t\t\t\tirqfd_resampler_notify(resampler);\n\t\t\t\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\n\treturn false;\n}\n\n \nint kvm_irqfd_init(void)\n{\n\tirqfd_cleanup_wq = alloc_workqueue(\"kvm-irqfd-cleanup\", 0, 0);\n\tif (!irqfd_cleanup_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid kvm_irqfd_exit(void)\n{\n\tdestroy_workqueue(irqfd_cleanup_wq);\n}\n#endif\n\n \n\nstruct _ioeventfd {\n\tstruct list_head     list;\n\tu64                  addr;\n\tint                  length;\n\tstruct eventfd_ctx  *eventfd;\n\tu64                  datamatch;\n\tstruct kvm_io_device dev;\n\tu8                   bus_idx;\n\tbool                 wildcard;\n};\n\nstatic inline struct _ioeventfd *\nto_ioeventfd(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct _ioeventfd, dev);\n}\n\nstatic void\nioeventfd_release(struct _ioeventfd *p)\n{\n\teventfd_ctx_put(p->eventfd);\n\tlist_del(&p->list);\n\tkfree(p);\n}\n\nstatic bool\nioeventfd_in_range(struct _ioeventfd *p, gpa_t addr, int len, const void *val)\n{\n\tu64 _val;\n\n\tif (addr != p->addr)\n\t\t \n\t\treturn false;\n\n\tif (!p->length)\n\t\t \n\t\treturn true;\n\n\tif (len != p->length)\n\t\t \n\t\treturn false;\n\n\tif (p->wildcard)\n\t\t \n\t\treturn true;\n\n\t \n\n\tBUG_ON(!IS_ALIGNED((unsigned long)val, len));\n\n\tswitch (len) {\n\tcase 1:\n\t\t_val = *(u8 *)val;\n\t\tbreak;\n\tcase 2:\n\t\t_val = *(u16 *)val;\n\t\tbreak;\n\tcase 4:\n\t\t_val = *(u32 *)val;\n\t\tbreak;\n\tcase 8:\n\t\t_val = *(u64 *)val;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn _val == p->datamatch;\n}\n\n \nstatic int\nioeventfd_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this, gpa_t addr,\n\t\tint len, const void *val)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tif (!ioeventfd_in_range(p, addr, len, val))\n\t\treturn -EOPNOTSUPP;\n\n\teventfd_signal(p->eventfd, 1);\n\treturn 0;\n}\n\n \nstatic void\nioeventfd_destructor(struct kvm_io_device *this)\n{\n\tstruct _ioeventfd *p = to_ioeventfd(this);\n\n\tioeventfd_release(p);\n}\n\nstatic const struct kvm_io_device_ops ioeventfd_ops = {\n\t.write      = ioeventfd_write,\n\t.destructor = ioeventfd_destructor,\n};\n\n \nstatic bool\nioeventfd_check_collision(struct kvm *kvm, struct _ioeventfd *p)\n{\n\tstruct _ioeventfd *_p;\n\n\tlist_for_each_entry(_p, &kvm->ioeventfds, list)\n\t\tif (_p->bus_idx == p->bus_idx &&\n\t\t    _p->addr == p->addr &&\n\t\t    (!_p->length || !p->length ||\n\t\t     (_p->length == p->length &&\n\t\t      (_p->wildcard || p->wildcard ||\n\t\t       _p->datamatch == p->datamatch))))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic enum kvm_bus ioeventfd_bus_from_flags(__u32 flags)\n{\n\tif (flags & KVM_IOEVENTFD_FLAG_PIO)\n\t\treturn KVM_PIO_BUS;\n\tif (flags & KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY)\n\t\treturn KVM_VIRTIO_CCW_NOTIFY_BUS;\n\treturn KVM_MMIO_BUS;\n}\n\nstatic int kvm_assign_ioeventfd_idx(struct kvm *kvm,\n\t\t\t\tenum kvm_bus bus_idx,\n\t\t\t\tstruct kvm_ioeventfd *args)\n{\n\n\tstruct eventfd_ctx *eventfd;\n\tstruct _ioeventfd *p;\n\tint ret;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\tp = kzalloc(sizeof(*p), GFP_KERNEL_ACCOUNT);\n\tif (!p) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tINIT_LIST_HEAD(&p->list);\n\tp->addr    = args->addr;\n\tp->bus_idx = bus_idx;\n\tp->length  = args->len;\n\tp->eventfd = eventfd;\n\n\t \n\tif (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH)\n\t\tp->datamatch = args->datamatch;\n\telse\n\t\tp->wildcard = true;\n\n\tmutex_lock(&kvm->slots_lock);\n\n\t \n\tif (ioeventfd_check_collision(kvm, p)) {\n\t\tret = -EEXIST;\n\t\tgoto unlock_fail;\n\t}\n\n\tkvm_iodevice_init(&p->dev, &ioeventfd_ops);\n\n\tret = kvm_io_bus_register_dev(kvm, bus_idx, p->addr, p->length,\n\t\t\t\t      &p->dev);\n\tif (ret < 0)\n\t\tgoto unlock_fail;\n\n\tkvm_get_bus(kvm, bus_idx)->ioeventfd_count++;\n\tlist_add_tail(&p->list, &kvm->ioeventfds);\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\treturn 0;\n\nunlock_fail:\n\tmutex_unlock(&kvm->slots_lock);\n\tkfree(p);\n\nfail:\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int\nkvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t   struct kvm_ioeventfd *args)\n{\n\tstruct _ioeventfd        *p;\n\tstruct eventfd_ctx       *eventfd;\n\tstruct kvm_io_bus\t *bus;\n\tint                       ret = -ENOENT;\n\tbool                      wildcard;\n\n\teventfd = eventfd_ctx_fdget(args->fd);\n\tif (IS_ERR(eventfd))\n\t\treturn PTR_ERR(eventfd);\n\n\twildcard = !(args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH);\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tlist_for_each_entry(p, &kvm->ioeventfds, list) {\n\t\tif (p->bus_idx != bus_idx ||\n\t\t    p->eventfd != eventfd  ||\n\t\t    p->addr != args->addr  ||\n\t\t    p->length != args->len ||\n\t\t    p->wildcard != wildcard)\n\t\t\tcontinue;\n\n\t\tif (!p->wildcard && p->datamatch != args->datamatch)\n\t\t\tcontinue;\n\n\t\tkvm_io_bus_unregister_dev(kvm, bus_idx, &p->dev);\n\t\tbus = kvm_get_bus(kvm, bus_idx);\n\t\tif (bus)\n\t\t\tbus->ioeventfd_count--;\n\t\tret = 0;\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\teventfd_ctx_put(eventfd);\n\n\treturn ret;\n}\n\nstatic int kvm_deassign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus bus_idx = ioeventfd_bus_from_flags(args->flags);\n\tint ret = kvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\n\n\tif (!args->len && bus_idx == KVM_MMIO_BUS)\n\t\tkvm_deassign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\n\treturn ret;\n}\n\nstatic int\nkvm_assign_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tenum kvm_bus              bus_idx;\n\tint ret;\n\n\tbus_idx = ioeventfd_bus_from_flags(args->flags);\n\t \n\tswitch (args->len) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\tcase 8:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (args->addr + args->len < args->addr)\n\t\treturn -EINVAL;\n\n\t \n\tif (args->flags & ~KVM_IOEVENTFD_VALID_FLAG_MASK)\n\t\treturn -EINVAL;\n\n\t \n\tif (!args->len && (args->flags & KVM_IOEVENTFD_FLAG_DATAMATCH))\n\t\treturn -EINVAL;\n\n\tret = kvm_assign_ioeventfd_idx(kvm, bus_idx, args);\n\tif (ret)\n\t\tgoto fail;\n\n\t \n\tif (!args->len && bus_idx == KVM_MMIO_BUS) {\n\t\tret = kvm_assign_ioeventfd_idx(kvm, KVM_FAST_MMIO_BUS, args);\n\t\tif (ret < 0)\n\t\t\tgoto fast_fail;\n\t}\n\n\treturn 0;\n\nfast_fail:\n\tkvm_deassign_ioeventfd_idx(kvm, bus_idx, args);\nfail:\n\treturn ret;\n}\n\nint\nkvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\tif (args->flags & KVM_IOEVENTFD_FLAG_DEASSIGN)\n\t\treturn kvm_deassign_ioeventfd(kvm, args);\n\n\treturn kvm_assign_ioeventfd(kvm, args);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}