{
  "module_name": "ib_verbs.h",
  "hash_id": "3289408144ef3b9659ab869078b73d3167b9ffb17059ea6190c1365fe33512be",
  "original_prompt": "Ingested from linux-6.6.14/include/rdma/ib_verbs.h",
  "human_readable_source": " \n \n\n#ifndef IB_VERBS_H\n#define IB_VERBS_H\n\n#include <linux/ethtool.h>\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/rwsem.h>\n#include <linux/workqueue.h>\n#include <linux/irq_poll.h>\n#include <uapi/linux/if_ether.h>\n#include <net/ipv6.h>\n#include <net/ip.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/netdevice.h>\n#include <linux/refcount.h>\n#include <linux/if_link.h>\n#include <linux/atomic.h>\n#include <linux/mmu_notifier.h>\n#include <linux/uaccess.h>\n#include <linux/cgroup_rdma.h>\n#include <linux/irqflags.h>\n#include <linux/preempt.h>\n#include <linux/dim.h>\n#include <uapi/rdma/ib_user_verbs.h>\n#include <rdma/rdma_counter.h>\n#include <rdma/restrack.h>\n#include <rdma/signature.h>\n#include <uapi/rdma/rdma_user_ioctl.h>\n#include <uapi/rdma/ib_user_ioctl_verbs.h>\n\n#define IB_FW_VERSION_NAME_MAX\tETHTOOL_FWVERS_LEN\n\nstruct ib_umem_odp;\nstruct ib_uqp_object;\nstruct ib_usrq_object;\nstruct ib_uwq_object;\nstruct rdma_cm_id;\nstruct ib_port;\nstruct hw_stats_device_data;\n\nextern struct workqueue_struct *ib_wq;\nextern struct workqueue_struct *ib_comp_wq;\nextern struct workqueue_struct *ib_comp_unbound_wq;\n\nstruct ib_ucq_object;\n\n__printf(3, 4) __cold\nvoid ibdev_printk(const char *level, const struct ib_device *ibdev,\n\t\t  const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_emerg(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_alert(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_crit(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_err(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_warn(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_notice(const struct ib_device *ibdev, const char *format, ...);\n__printf(2, 3) __cold\nvoid ibdev_info(const struct ib_device *ibdev, const char *format, ...);\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n#define ibdev_dbg(__dev, format, args...)                       \\\n\tdynamic_ibdev_dbg(__dev, format, ##args)\n#else\n__printf(2, 3) __cold\nstatic inline\nvoid ibdev_dbg(const struct ib_device *ibdev, const char *format, ...) {}\n#endif\n\n#define ibdev_level_ratelimited(ibdev_level, ibdev, fmt, ...)           \\\ndo {                                                                    \\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,                              \\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,       \\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);         \\\n\tif (__ratelimit(&_rs))                                          \\\n\t\tibdev_level(ibdev, fmt, ##__VA_ARGS__);                 \\\n} while (0)\n\n#define ibdev_emerg_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_emerg, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_alert_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_alert, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_crit_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_crit, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_err_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_err, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_warn_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_warn, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_notice_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_notice, ibdev, fmt, ##__VA_ARGS__)\n#define ibdev_info_ratelimited(ibdev, fmt, ...) \\\n\tibdev_level_ratelimited(ibdev_info, ibdev, fmt, ##__VA_ARGS__)\n\n#if defined(CONFIG_DYNAMIC_DEBUG) || \\\n\t(defined(CONFIG_DYNAMIC_DEBUG_CORE) && defined(DYNAMIC_DEBUG_MODULE))\n \n#define ibdev_dbg_ratelimited(ibdev, fmt, ...)                          \\\ndo {                                                                    \\\n\tstatic DEFINE_RATELIMIT_STATE(_rs,                              \\\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,       \\\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);         \\\n\tDEFINE_DYNAMIC_DEBUG_METADATA(descriptor, fmt);                 \\\n\tif (DYNAMIC_DEBUG_BRANCH(descriptor) && __ratelimit(&_rs))      \\\n\t\t__dynamic_ibdev_dbg(&descriptor, ibdev, fmt,            \\\n\t\t\t\t    ##__VA_ARGS__);                     \\\n} while (0)\n#else\n__printf(2, 3) __cold\nstatic inline\nvoid ibdev_dbg_ratelimited(const struct ib_device *ibdev, const char *format, ...) {}\n#endif\n\nunion ib_gid {\n\tu8\traw[16];\n\tstruct {\n\t\t__be64\tsubnet_prefix;\n\t\t__be64\tinterface_id;\n\t} global;\n};\n\nextern union ib_gid zgid;\n\nenum ib_gid_type {\n\tIB_GID_TYPE_IB = IB_UVERBS_GID_TYPE_IB,\n\tIB_GID_TYPE_ROCE = IB_UVERBS_GID_TYPE_ROCE_V1,\n\tIB_GID_TYPE_ROCE_UDP_ENCAP = IB_UVERBS_GID_TYPE_ROCE_V2,\n\tIB_GID_TYPE_SIZE\n};\n\n#define ROCE_V2_UDP_DPORT      4791\nstruct ib_gid_attr {\n\tstruct net_device __rcu\t*ndev;\n\tstruct ib_device\t*device;\n\tunion ib_gid\t\tgid;\n\tenum ib_gid_type\tgid_type;\n\tu16\t\t\tindex;\n\tu32\t\t\tport_num;\n};\n\nenum {\n\t \n\tIB_SA_WELL_KNOWN_GUID\t= BIT_ULL(57) | 2,\n};\n\nenum rdma_transport_type {\n\tRDMA_TRANSPORT_IB,\n\tRDMA_TRANSPORT_IWARP,\n\tRDMA_TRANSPORT_USNIC,\n\tRDMA_TRANSPORT_USNIC_UDP,\n\tRDMA_TRANSPORT_UNSPECIFIED,\n};\n\nenum rdma_protocol_type {\n\tRDMA_PROTOCOL_IB,\n\tRDMA_PROTOCOL_IBOE,\n\tRDMA_PROTOCOL_IWARP,\n\tRDMA_PROTOCOL_USNIC_UDP\n};\n\n__attribute_const__ enum rdma_transport_type\nrdma_node_get_transport(unsigned int node_type);\n\nenum rdma_network_type {\n\tRDMA_NETWORK_IB,\n\tRDMA_NETWORK_ROCE_V1,\n\tRDMA_NETWORK_IPV4,\n\tRDMA_NETWORK_IPV6\n};\n\nstatic inline enum ib_gid_type ib_network_to_gid_type(enum rdma_network_type network_type)\n{\n\tif (network_type == RDMA_NETWORK_IPV4 ||\n\t    network_type == RDMA_NETWORK_IPV6)\n\t\treturn IB_GID_TYPE_ROCE_UDP_ENCAP;\n\telse if (network_type == RDMA_NETWORK_ROCE_V1)\n\t\treturn IB_GID_TYPE_ROCE;\n\telse\n\t\treturn IB_GID_TYPE_IB;\n}\n\nstatic inline enum rdma_network_type\nrdma_gid_attr_network_type(const struct ib_gid_attr *attr)\n{\n\tif (attr->gid_type == IB_GID_TYPE_IB)\n\t\treturn RDMA_NETWORK_IB;\n\n\tif (attr->gid_type == IB_GID_TYPE_ROCE)\n\t\treturn RDMA_NETWORK_ROCE_V1;\n\n\tif (ipv6_addr_v4mapped((struct in6_addr *)&attr->gid))\n\t\treturn RDMA_NETWORK_IPV4;\n\telse\n\t\treturn RDMA_NETWORK_IPV6;\n}\n\nenum rdma_link_layer {\n\tIB_LINK_LAYER_UNSPECIFIED,\n\tIB_LINK_LAYER_INFINIBAND,\n\tIB_LINK_LAYER_ETHERNET,\n};\n\nenum ib_device_cap_flags {\n\tIB_DEVICE_RESIZE_MAX_WR = IB_UVERBS_DEVICE_RESIZE_MAX_WR,\n\tIB_DEVICE_BAD_PKEY_CNTR = IB_UVERBS_DEVICE_BAD_PKEY_CNTR,\n\tIB_DEVICE_BAD_QKEY_CNTR = IB_UVERBS_DEVICE_BAD_QKEY_CNTR,\n\tIB_DEVICE_RAW_MULTI = IB_UVERBS_DEVICE_RAW_MULTI,\n\tIB_DEVICE_AUTO_PATH_MIG = IB_UVERBS_DEVICE_AUTO_PATH_MIG,\n\tIB_DEVICE_CHANGE_PHY_PORT = IB_UVERBS_DEVICE_CHANGE_PHY_PORT,\n\tIB_DEVICE_UD_AV_PORT_ENFORCE = IB_UVERBS_DEVICE_UD_AV_PORT_ENFORCE,\n\tIB_DEVICE_CURR_QP_STATE_MOD = IB_UVERBS_DEVICE_CURR_QP_STATE_MOD,\n\tIB_DEVICE_SHUTDOWN_PORT = IB_UVERBS_DEVICE_SHUTDOWN_PORT,\n\t \n\tIB_DEVICE_PORT_ACTIVE_EVENT = IB_UVERBS_DEVICE_PORT_ACTIVE_EVENT,\n\tIB_DEVICE_SYS_IMAGE_GUID = IB_UVERBS_DEVICE_SYS_IMAGE_GUID,\n\tIB_DEVICE_RC_RNR_NAK_GEN = IB_UVERBS_DEVICE_RC_RNR_NAK_GEN,\n\tIB_DEVICE_SRQ_RESIZE = IB_UVERBS_DEVICE_SRQ_RESIZE,\n\tIB_DEVICE_N_NOTIFY_CQ = IB_UVERBS_DEVICE_N_NOTIFY_CQ,\n\n\t \n\tIB_DEVICE_MEM_WINDOW = IB_UVERBS_DEVICE_MEM_WINDOW,\n\t \n\tIB_DEVICE_UD_IP_CSUM = IB_UVERBS_DEVICE_UD_IP_CSUM,\n\tIB_DEVICE_XRC = IB_UVERBS_DEVICE_XRC,\n\n\t \n\tIB_DEVICE_MEM_MGT_EXTENSIONS = IB_UVERBS_DEVICE_MEM_MGT_EXTENSIONS,\n\tIB_DEVICE_MEM_WINDOW_TYPE_2A = IB_UVERBS_DEVICE_MEM_WINDOW_TYPE_2A,\n\tIB_DEVICE_MEM_WINDOW_TYPE_2B = IB_UVERBS_DEVICE_MEM_WINDOW_TYPE_2B,\n\tIB_DEVICE_RC_IP_CSUM = IB_UVERBS_DEVICE_RC_IP_CSUM,\n\t \n\tIB_DEVICE_RAW_IP_CSUM = IB_UVERBS_DEVICE_RAW_IP_CSUM,\n\tIB_DEVICE_MANAGED_FLOW_STEERING =\n\t\tIB_UVERBS_DEVICE_MANAGED_FLOW_STEERING,\n\t \n\tIB_DEVICE_RAW_SCATTER_FCS = IB_UVERBS_DEVICE_RAW_SCATTER_FCS,\n\t \n\tIB_DEVICE_PCI_WRITE_END_PADDING =\n\t\tIB_UVERBS_DEVICE_PCI_WRITE_END_PADDING,\n\t \n\tIB_DEVICE_FLUSH_GLOBAL = IB_UVERBS_DEVICE_FLUSH_GLOBAL,\n\tIB_DEVICE_FLUSH_PERSISTENT = IB_UVERBS_DEVICE_FLUSH_PERSISTENT,\n\tIB_DEVICE_ATOMIC_WRITE = IB_UVERBS_DEVICE_ATOMIC_WRITE,\n};\n\nenum ib_kernel_cap_flags {\n\t \n\tIBK_LOCAL_DMA_LKEY = 1 << 0,\n\t \n\tIBK_INTEGRITY_HANDOVER = 1 << 1,\n\t \n\tIBK_ON_DEMAND_PAGING = 1 << 2,\n\t \n\tIBK_SG_GAPS_REG = 1 << 3,\n\t \n\tIBK_ALLOW_USER_UNREG = 1 << 4,\n\n\t \n\tIBK_BLOCK_MULTICAST_LOOPBACK = 1 << 5,\n\t \n\tIBK_UD_TSO = 1 << 6,\n\t \n\tIBK_VIRTUAL_FUNCTION = 1 << 7,\n\t \n\tIBK_RDMA_NETDEV_OPA = 1 << 8,\n};\n\nenum ib_atomic_cap {\n\tIB_ATOMIC_NONE,\n\tIB_ATOMIC_HCA,\n\tIB_ATOMIC_GLOB\n};\n\nenum ib_odp_general_cap_bits {\n\tIB_ODP_SUPPORT\t\t= 1 << 0,\n\tIB_ODP_SUPPORT_IMPLICIT = 1 << 1,\n};\n\nenum ib_odp_transport_cap_bits {\n\tIB_ODP_SUPPORT_SEND\t= 1 << 0,\n\tIB_ODP_SUPPORT_RECV\t= 1 << 1,\n\tIB_ODP_SUPPORT_WRITE\t= 1 << 2,\n\tIB_ODP_SUPPORT_READ\t= 1 << 3,\n\tIB_ODP_SUPPORT_ATOMIC\t= 1 << 4,\n\tIB_ODP_SUPPORT_SRQ_RECV\t= 1 << 5,\n};\n\nstruct ib_odp_caps {\n\tuint64_t general_caps;\n\tstruct {\n\t\tuint32_t  rc_odp_caps;\n\t\tuint32_t  uc_odp_caps;\n\t\tuint32_t  ud_odp_caps;\n\t\tuint32_t  xrc_odp_caps;\n\t} per_transport_caps;\n};\n\nstruct ib_rss_caps {\n\t \n\tu32 supported_qpts;\n\tu32 max_rwq_indirection_tables;\n\tu32 max_rwq_indirection_table_size;\n};\n\nenum ib_tm_cap_flags {\n\t \n\tIB_TM_CAP_RNDV_RC = 1 << 0,\n};\n\nstruct ib_tm_caps {\n\t \n\tu32 max_rndv_hdr_size;\n\t \n\tu32 max_num_tags;\n\t \n\tu32 flags;\n\t \n\tu32 max_ops;\n\t \n\tu32 max_sge;\n};\n\nstruct ib_cq_init_attr {\n\tunsigned int\tcqe;\n\tu32\t\tcomp_vector;\n\tu32\t\tflags;\n};\n\nenum ib_cq_attr_mask {\n\tIB_CQ_MODERATE = 1 << 0,\n};\n\nstruct ib_cq_caps {\n\tu16     max_cq_moderation_count;\n\tu16     max_cq_moderation_period;\n};\n\nstruct ib_dm_mr_attr {\n\tu64\t\tlength;\n\tu64\t\toffset;\n\tu32\t\taccess_flags;\n};\n\nstruct ib_dm_alloc_attr {\n\tu64\tlength;\n\tu32\talignment;\n\tu32\tflags;\n};\n\nstruct ib_device_attr {\n\tu64\t\t\tfw_ver;\n\t__be64\t\t\tsys_image_guid;\n\tu64\t\t\tmax_mr_size;\n\tu64\t\t\tpage_size_cap;\n\tu32\t\t\tvendor_id;\n\tu32\t\t\tvendor_part_id;\n\tu32\t\t\thw_ver;\n\tint\t\t\tmax_qp;\n\tint\t\t\tmax_qp_wr;\n\tu64\t\t\tdevice_cap_flags;\n\tu64\t\t\tkernel_cap_flags;\n\tint\t\t\tmax_send_sge;\n\tint\t\t\tmax_recv_sge;\n\tint\t\t\tmax_sge_rd;\n\tint\t\t\tmax_cq;\n\tint\t\t\tmax_cqe;\n\tint\t\t\tmax_mr;\n\tint\t\t\tmax_pd;\n\tint\t\t\tmax_qp_rd_atom;\n\tint\t\t\tmax_ee_rd_atom;\n\tint\t\t\tmax_res_rd_atom;\n\tint\t\t\tmax_qp_init_rd_atom;\n\tint\t\t\tmax_ee_init_rd_atom;\n\tenum ib_atomic_cap\tatomic_cap;\n\tenum ib_atomic_cap\tmasked_atomic_cap;\n\tint\t\t\tmax_ee;\n\tint\t\t\tmax_rdd;\n\tint\t\t\tmax_mw;\n\tint\t\t\tmax_raw_ipv6_qp;\n\tint\t\t\tmax_raw_ethy_qp;\n\tint\t\t\tmax_mcast_grp;\n\tint\t\t\tmax_mcast_qp_attach;\n\tint\t\t\tmax_total_mcast_qp_attach;\n\tint\t\t\tmax_ah;\n\tint\t\t\tmax_srq;\n\tint\t\t\tmax_srq_wr;\n\tint\t\t\tmax_srq_sge;\n\tunsigned int\t\tmax_fast_reg_page_list_len;\n\tunsigned int\t\tmax_pi_fast_reg_page_list_len;\n\tu16\t\t\tmax_pkeys;\n\tu8\t\t\tlocal_ca_ack_delay;\n\tint\t\t\tsig_prot_cap;\n\tint\t\t\tsig_guard_cap;\n\tstruct ib_odp_caps\todp_caps;\n\tuint64_t\t\ttimestamp_mask;\n\tuint64_t\t\thca_core_clock;  \n\tstruct ib_rss_caps\trss_caps;\n\tu32\t\t\tmax_wq_type_rq;\n\tu32\t\t\traw_packet_caps;  \n\tstruct ib_tm_caps\ttm_caps;\n\tstruct ib_cq_caps       cq_caps;\n\tu64\t\t\tmax_dm_size;\n\t \n\tu32\t\t\tmax_sgl_rd;\n};\n\nenum ib_mtu {\n\tIB_MTU_256  = 1,\n\tIB_MTU_512  = 2,\n\tIB_MTU_1024 = 3,\n\tIB_MTU_2048 = 4,\n\tIB_MTU_4096 = 5\n};\n\nenum opa_mtu {\n\tOPA_MTU_8192 = 6,\n\tOPA_MTU_10240 = 7\n};\n\nstatic inline int ib_mtu_enum_to_int(enum ib_mtu mtu)\n{\n\tswitch (mtu) {\n\tcase IB_MTU_256:  return  256;\n\tcase IB_MTU_512:  return  512;\n\tcase IB_MTU_1024: return 1024;\n\tcase IB_MTU_2048: return 2048;\n\tcase IB_MTU_4096: return 4096;\n\tdefault: \t  return -1;\n\t}\n}\n\nstatic inline enum ib_mtu ib_mtu_int_to_enum(int mtu)\n{\n\tif (mtu >= 4096)\n\t\treturn IB_MTU_4096;\n\telse if (mtu >= 2048)\n\t\treturn IB_MTU_2048;\n\telse if (mtu >= 1024)\n\t\treturn IB_MTU_1024;\n\telse if (mtu >= 512)\n\t\treturn IB_MTU_512;\n\telse\n\t\treturn IB_MTU_256;\n}\n\nstatic inline int opa_mtu_enum_to_int(enum opa_mtu mtu)\n{\n\tswitch (mtu) {\n\tcase OPA_MTU_8192:\n\t\treturn 8192;\n\tcase OPA_MTU_10240:\n\t\treturn 10240;\n\tdefault:\n\t\treturn(ib_mtu_enum_to_int((enum ib_mtu)mtu));\n\t}\n}\n\nstatic inline enum opa_mtu opa_mtu_int_to_enum(int mtu)\n{\n\tif (mtu >= 10240)\n\t\treturn OPA_MTU_10240;\n\telse if (mtu >= 8192)\n\t\treturn OPA_MTU_8192;\n\telse\n\t\treturn ((enum opa_mtu)ib_mtu_int_to_enum(mtu));\n}\n\nenum ib_port_state {\n\tIB_PORT_NOP\t\t= 0,\n\tIB_PORT_DOWN\t\t= 1,\n\tIB_PORT_INIT\t\t= 2,\n\tIB_PORT_ARMED\t\t= 3,\n\tIB_PORT_ACTIVE\t\t= 4,\n\tIB_PORT_ACTIVE_DEFER\t= 5\n};\n\nenum ib_port_phys_state {\n\tIB_PORT_PHYS_STATE_SLEEP = 1,\n\tIB_PORT_PHYS_STATE_POLLING = 2,\n\tIB_PORT_PHYS_STATE_DISABLED = 3,\n\tIB_PORT_PHYS_STATE_PORT_CONFIGURATION_TRAINING = 4,\n\tIB_PORT_PHYS_STATE_LINK_UP = 5,\n\tIB_PORT_PHYS_STATE_LINK_ERROR_RECOVERY = 6,\n\tIB_PORT_PHYS_STATE_PHY_TEST = 7,\n};\n\nenum ib_port_width {\n\tIB_WIDTH_1X\t= 1,\n\tIB_WIDTH_2X\t= 16,\n\tIB_WIDTH_4X\t= 2,\n\tIB_WIDTH_8X\t= 4,\n\tIB_WIDTH_12X\t= 8\n};\n\nstatic inline int ib_width_enum_to_int(enum ib_port_width width)\n{\n\tswitch (width) {\n\tcase IB_WIDTH_1X:  return  1;\n\tcase IB_WIDTH_2X:  return  2;\n\tcase IB_WIDTH_4X:  return  4;\n\tcase IB_WIDTH_8X:  return  8;\n\tcase IB_WIDTH_12X: return 12;\n\tdefault: \t  return -1;\n\t}\n}\n\nenum ib_port_speed {\n\tIB_SPEED_SDR\t= 1,\n\tIB_SPEED_DDR\t= 2,\n\tIB_SPEED_QDR\t= 4,\n\tIB_SPEED_FDR10\t= 8,\n\tIB_SPEED_FDR\t= 16,\n\tIB_SPEED_EDR\t= 32,\n\tIB_SPEED_HDR\t= 64,\n\tIB_SPEED_NDR\t= 128,\n};\n\nenum ib_stat_flag {\n\tIB_STAT_FLAG_OPTIONAL = 1 << 0,\n};\n\n \nstruct rdma_stat_desc {\n\tconst char *name;\n\tunsigned int flags;\n\tconst void *priv;\n};\n\n \nstruct rdma_hw_stats {\n\tstruct mutex\tlock;  \n\tunsigned long\ttimestamp;\n\tunsigned long\tlifespan;\n\tconst struct rdma_stat_desc *descs;\n\tunsigned long\t*is_disabled;\n\tint\t\tnum_counters;\n\tu64\t\tvalue[];\n};\n\n#define RDMA_HW_STATS_DEFAULT_LIFESPAN 10\n\nstruct rdma_hw_stats *rdma_alloc_hw_stats_struct(\n\tconst struct rdma_stat_desc *descs, int num_counters,\n\tunsigned long lifespan);\n\nvoid rdma_free_hw_stats_struct(struct rdma_hw_stats *stats);\n\n \n \n#define RDMA_CORE_CAP_IB_MAD            0x00000001\n#define RDMA_CORE_CAP_IB_SMI            0x00000002\n#define RDMA_CORE_CAP_IB_CM             0x00000004\n#define RDMA_CORE_CAP_IW_CM             0x00000008\n#define RDMA_CORE_CAP_IB_SA             0x00000010\n#define RDMA_CORE_CAP_OPA_MAD           0x00000020\n\n \n#define RDMA_CORE_CAP_AF_IB             0x00001000\n#define RDMA_CORE_CAP_ETH_AH            0x00002000\n#define RDMA_CORE_CAP_OPA_AH            0x00004000\n#define RDMA_CORE_CAP_IB_GRH_REQUIRED   0x00008000\n\n \n#define RDMA_CORE_CAP_PROT_IB           0x00100000\n#define RDMA_CORE_CAP_PROT_ROCE         0x00200000\n#define RDMA_CORE_CAP_PROT_IWARP        0x00400000\n#define RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP 0x00800000\n#define RDMA_CORE_CAP_PROT_RAW_PACKET   0x01000000\n#define RDMA_CORE_CAP_PROT_USNIC        0x02000000\n\n#define RDMA_CORE_PORT_IB_GRH_REQUIRED (RDMA_CORE_CAP_IB_GRH_REQUIRED \\\n\t\t\t\t\t| RDMA_CORE_CAP_PROT_ROCE     \\\n\t\t\t\t\t| RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP)\n\n#define RDMA_CORE_PORT_IBA_IB          (RDMA_CORE_CAP_PROT_IB  \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_MAD \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_SMI \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_CM  \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_SA  \\\n\t\t\t\t\t| RDMA_CORE_CAP_AF_IB)\n#define RDMA_CORE_PORT_IBA_ROCE        (RDMA_CORE_CAP_PROT_ROCE \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_MAD  \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_CM   \\\n\t\t\t\t\t| RDMA_CORE_CAP_AF_IB   \\\n\t\t\t\t\t| RDMA_CORE_CAP_ETH_AH)\n#define RDMA_CORE_PORT_IBA_ROCE_UDP_ENCAP\t\t\t\\\n\t\t\t\t\t(RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_MAD  \\\n\t\t\t\t\t| RDMA_CORE_CAP_IB_CM   \\\n\t\t\t\t\t| RDMA_CORE_CAP_AF_IB   \\\n\t\t\t\t\t| RDMA_CORE_CAP_ETH_AH)\n#define RDMA_CORE_PORT_IWARP           (RDMA_CORE_CAP_PROT_IWARP \\\n\t\t\t\t\t| RDMA_CORE_CAP_IW_CM)\n#define RDMA_CORE_PORT_INTEL_OPA       (RDMA_CORE_PORT_IBA_IB  \\\n\t\t\t\t\t| RDMA_CORE_CAP_OPA_MAD)\n\n#define RDMA_CORE_PORT_RAW_PACKET\t(RDMA_CORE_CAP_PROT_RAW_PACKET)\n\n#define RDMA_CORE_PORT_USNIC\t\t(RDMA_CORE_CAP_PROT_USNIC)\n\nstruct ib_port_attr {\n\tu64\t\t\tsubnet_prefix;\n\tenum ib_port_state\tstate;\n\tenum ib_mtu\t\tmax_mtu;\n\tenum ib_mtu\t\tactive_mtu;\n\tu32                     phys_mtu;\n\tint\t\t\tgid_tbl_len;\n\tunsigned int\t\tip_gids:1;\n\t \n\tu32\t\t\tport_cap_flags;\n\tu32\t\t\tmax_msg_sz;\n\tu32\t\t\tbad_pkey_cntr;\n\tu32\t\t\tqkey_viol_cntr;\n\tu16\t\t\tpkey_tbl_len;\n\tu32\t\t\tsm_lid;\n\tu32\t\t\tlid;\n\tu8\t\t\tlmc;\n\tu8\t\t\tmax_vl_num;\n\tu8\t\t\tsm_sl;\n\tu8\t\t\tsubnet_timeout;\n\tu8\t\t\tinit_type_reply;\n\tu8\t\t\tactive_width;\n\tu16\t\t\tactive_speed;\n\tu8                      phys_state;\n\tu16\t\t\tport_cap_flags2;\n};\n\nenum ib_device_modify_flags {\n\tIB_DEVICE_MODIFY_SYS_IMAGE_GUID\t= 1 << 0,\n\tIB_DEVICE_MODIFY_NODE_DESC\t= 1 << 1\n};\n\n#define IB_DEVICE_NODE_DESC_MAX 64\n\nstruct ib_device_modify {\n\tu64\tsys_image_guid;\n\tchar\tnode_desc[IB_DEVICE_NODE_DESC_MAX];\n};\n\nenum ib_port_modify_flags {\n\tIB_PORT_SHUTDOWN\t\t= 1,\n\tIB_PORT_INIT_TYPE\t\t= (1<<2),\n\tIB_PORT_RESET_QKEY_CNTR\t\t= (1<<3),\n\tIB_PORT_OPA_MASK_CHG\t\t= (1<<4)\n};\n\nstruct ib_port_modify {\n\tu32\tset_port_cap_mask;\n\tu32\tclr_port_cap_mask;\n\tu8\tinit_type;\n};\n\nenum ib_event_type {\n\tIB_EVENT_CQ_ERR,\n\tIB_EVENT_QP_FATAL,\n\tIB_EVENT_QP_REQ_ERR,\n\tIB_EVENT_QP_ACCESS_ERR,\n\tIB_EVENT_COMM_EST,\n\tIB_EVENT_SQ_DRAINED,\n\tIB_EVENT_PATH_MIG,\n\tIB_EVENT_PATH_MIG_ERR,\n\tIB_EVENT_DEVICE_FATAL,\n\tIB_EVENT_PORT_ACTIVE,\n\tIB_EVENT_PORT_ERR,\n\tIB_EVENT_LID_CHANGE,\n\tIB_EVENT_PKEY_CHANGE,\n\tIB_EVENT_SM_CHANGE,\n\tIB_EVENT_SRQ_ERR,\n\tIB_EVENT_SRQ_LIMIT_REACHED,\n\tIB_EVENT_QP_LAST_WQE_REACHED,\n\tIB_EVENT_CLIENT_REREGISTER,\n\tIB_EVENT_GID_CHANGE,\n\tIB_EVENT_WQ_FATAL,\n};\n\nconst char *__attribute_const__ ib_event_msg(enum ib_event_type event);\n\nstruct ib_event {\n\tstruct ib_device\t*device;\n\tunion {\n\t\tstruct ib_cq\t*cq;\n\t\tstruct ib_qp\t*qp;\n\t\tstruct ib_srq\t*srq;\n\t\tstruct ib_wq\t*wq;\n\t\tu32\t\tport_num;\n\t} element;\n\tenum ib_event_type\tevent;\n};\n\nstruct ib_event_handler {\n\tstruct ib_device *device;\n\tvoid            (*handler)(struct ib_event_handler *, struct ib_event *);\n\tstruct list_head  list;\n};\n\n#define INIT_IB_EVENT_HANDLER(_ptr, _device, _handler)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\t(_ptr)->device  = _device;\t\t\t\\\n\t\t(_ptr)->handler = _handler;\t\t\t\\\n\t\tINIT_LIST_HEAD(&(_ptr)->list);\t\t\t\\\n\t} while (0)\n\nstruct ib_global_route {\n\tconst struct ib_gid_attr *sgid_attr;\n\tunion ib_gid\tdgid;\n\tu32\t\tflow_label;\n\tu8\t\tsgid_index;\n\tu8\t\thop_limit;\n\tu8\t\ttraffic_class;\n};\n\nstruct ib_grh {\n\t__be32\t\tversion_tclass_flow;\n\t__be16\t\tpaylen;\n\tu8\t\tnext_hdr;\n\tu8\t\thop_limit;\n\tunion ib_gid\tsgid;\n\tunion ib_gid\tdgid;\n};\n\nunion rdma_network_hdr {\n\tstruct ib_grh ibgrh;\n\tstruct {\n\t\t \n\t\tu8\t\treserved[20];\n\t\tstruct iphdr\troce4grh;\n\t};\n};\n\n#define IB_QPN_MASK\t\t0xFFFFFF\n\nenum {\n\tIB_MULTICAST_QPN = 0xffffff\n};\n\n#define IB_LID_PERMISSIVE\tcpu_to_be16(0xFFFF)\n#define IB_MULTICAST_LID_BASE\tcpu_to_be16(0xC000)\n\nenum ib_ah_flags {\n\tIB_AH_GRH\t= 1\n};\n\nenum ib_rate {\n\tIB_RATE_PORT_CURRENT = 0,\n\tIB_RATE_2_5_GBPS = 2,\n\tIB_RATE_5_GBPS   = 5,\n\tIB_RATE_10_GBPS  = 3,\n\tIB_RATE_20_GBPS  = 6,\n\tIB_RATE_30_GBPS  = 4,\n\tIB_RATE_40_GBPS  = 7,\n\tIB_RATE_60_GBPS  = 8,\n\tIB_RATE_80_GBPS  = 9,\n\tIB_RATE_120_GBPS = 10,\n\tIB_RATE_14_GBPS  = 11,\n\tIB_RATE_56_GBPS  = 12,\n\tIB_RATE_112_GBPS = 13,\n\tIB_RATE_168_GBPS = 14,\n\tIB_RATE_25_GBPS  = 15,\n\tIB_RATE_100_GBPS = 16,\n\tIB_RATE_200_GBPS = 17,\n\tIB_RATE_300_GBPS = 18,\n\tIB_RATE_28_GBPS  = 19,\n\tIB_RATE_50_GBPS  = 20,\n\tIB_RATE_400_GBPS = 21,\n\tIB_RATE_600_GBPS = 22,\n};\n\n \n__attribute_const__ int ib_rate_to_mult(enum ib_rate rate);\n\n \n__attribute_const__ int ib_rate_to_mbps(enum ib_rate rate);\n\n\n \nenum ib_mr_type {\n\tIB_MR_TYPE_MEM_REG,\n\tIB_MR_TYPE_SG_GAPS,\n\tIB_MR_TYPE_DM,\n\tIB_MR_TYPE_USER,\n\tIB_MR_TYPE_DMA,\n\tIB_MR_TYPE_INTEGRITY,\n};\n\nenum ib_mr_status_check {\n\tIB_MR_CHECK_SIG_STATUS = 1,\n};\n\n \nstruct ib_mr_status {\n\tu32\t\t    fail_status;\n\tstruct ib_sig_err   sig_err;\n};\n\n \n__attribute_const__ enum ib_rate mult_to_ib_rate(int mult);\n\nstruct rdma_ah_init_attr {\n\tstruct rdma_ah_attr *ah_attr;\n\tu32 flags;\n\tstruct net_device *xmit_slave;\n};\n\nenum rdma_ah_attr_type {\n\tRDMA_AH_ATTR_TYPE_UNDEFINED,\n\tRDMA_AH_ATTR_TYPE_IB,\n\tRDMA_AH_ATTR_TYPE_ROCE,\n\tRDMA_AH_ATTR_TYPE_OPA,\n};\n\nstruct ib_ah_attr {\n\tu16\t\t\tdlid;\n\tu8\t\t\tsrc_path_bits;\n};\n\nstruct roce_ah_attr {\n\tu8\t\t\tdmac[ETH_ALEN];\n};\n\nstruct opa_ah_attr {\n\tu32\t\t\tdlid;\n\tu8\t\t\tsrc_path_bits;\n\tbool\t\t\tmake_grd;\n};\n\nstruct rdma_ah_attr {\n\tstruct ib_global_route\tgrh;\n\tu8\t\t\tsl;\n\tu8\t\t\tstatic_rate;\n\tu32\t\t\tport_num;\n\tu8\t\t\tah_flags;\n\tenum rdma_ah_attr_type type;\n\tunion {\n\t\tstruct ib_ah_attr ib;\n\t\tstruct roce_ah_attr roce;\n\t\tstruct opa_ah_attr opa;\n\t};\n};\n\nenum ib_wc_status {\n\tIB_WC_SUCCESS,\n\tIB_WC_LOC_LEN_ERR,\n\tIB_WC_LOC_QP_OP_ERR,\n\tIB_WC_LOC_EEC_OP_ERR,\n\tIB_WC_LOC_PROT_ERR,\n\tIB_WC_WR_FLUSH_ERR,\n\tIB_WC_MW_BIND_ERR,\n\tIB_WC_BAD_RESP_ERR,\n\tIB_WC_LOC_ACCESS_ERR,\n\tIB_WC_REM_INV_REQ_ERR,\n\tIB_WC_REM_ACCESS_ERR,\n\tIB_WC_REM_OP_ERR,\n\tIB_WC_RETRY_EXC_ERR,\n\tIB_WC_RNR_RETRY_EXC_ERR,\n\tIB_WC_LOC_RDD_VIOL_ERR,\n\tIB_WC_REM_INV_RD_REQ_ERR,\n\tIB_WC_REM_ABORT_ERR,\n\tIB_WC_INV_EECN_ERR,\n\tIB_WC_INV_EEC_STATE_ERR,\n\tIB_WC_FATAL_ERR,\n\tIB_WC_RESP_TIMEOUT_ERR,\n\tIB_WC_GENERAL_ERR\n};\n\nconst char *__attribute_const__ ib_wc_status_msg(enum ib_wc_status status);\n\nenum ib_wc_opcode {\n\tIB_WC_SEND = IB_UVERBS_WC_SEND,\n\tIB_WC_RDMA_WRITE = IB_UVERBS_WC_RDMA_WRITE,\n\tIB_WC_RDMA_READ = IB_UVERBS_WC_RDMA_READ,\n\tIB_WC_COMP_SWAP = IB_UVERBS_WC_COMP_SWAP,\n\tIB_WC_FETCH_ADD = IB_UVERBS_WC_FETCH_ADD,\n\tIB_WC_BIND_MW = IB_UVERBS_WC_BIND_MW,\n\tIB_WC_LOCAL_INV = IB_UVERBS_WC_LOCAL_INV,\n\tIB_WC_LSO = IB_UVERBS_WC_TSO,\n\tIB_WC_ATOMIC_WRITE = IB_UVERBS_WC_ATOMIC_WRITE,\n\tIB_WC_REG_MR,\n\tIB_WC_MASKED_COMP_SWAP,\n\tIB_WC_MASKED_FETCH_ADD,\n\tIB_WC_FLUSH = IB_UVERBS_WC_FLUSH,\n \n\tIB_WC_RECV\t\t\t= 1 << 7,\n\tIB_WC_RECV_RDMA_WITH_IMM\n};\n\nenum ib_wc_flags {\n\tIB_WC_GRH\t\t= 1,\n\tIB_WC_WITH_IMM\t\t= (1<<1),\n\tIB_WC_WITH_INVALIDATE\t= (1<<2),\n\tIB_WC_IP_CSUM_OK\t= (1<<3),\n\tIB_WC_WITH_SMAC\t\t= (1<<4),\n\tIB_WC_WITH_VLAN\t\t= (1<<5),\n\tIB_WC_WITH_NETWORK_HDR_TYPE\t= (1<<6),\n};\n\nstruct ib_wc {\n\tunion {\n\t\tu64\t\twr_id;\n\t\tstruct ib_cqe\t*wr_cqe;\n\t};\n\tenum ib_wc_status\tstatus;\n\tenum ib_wc_opcode\topcode;\n\tu32\t\t\tvendor_err;\n\tu32\t\t\tbyte_len;\n\tstruct ib_qp\t       *qp;\n\tunion {\n\t\t__be32\t\timm_data;\n\t\tu32\t\tinvalidate_rkey;\n\t} ex;\n\tu32\t\t\tsrc_qp;\n\tu32\t\t\tslid;\n\tint\t\t\twc_flags;\n\tu16\t\t\tpkey_index;\n\tu8\t\t\tsl;\n\tu8\t\t\tdlid_path_bits;\n\tu32 port_num;  \n\tu8\t\t\tsmac[ETH_ALEN];\n\tu16\t\t\tvlan_id;\n\tu8\t\t\tnetwork_hdr_type;\n};\n\nenum ib_cq_notify_flags {\n\tIB_CQ_SOLICITED\t\t\t= 1 << 0,\n\tIB_CQ_NEXT_COMP\t\t\t= 1 << 1,\n\tIB_CQ_SOLICITED_MASK\t\t= IB_CQ_SOLICITED | IB_CQ_NEXT_COMP,\n\tIB_CQ_REPORT_MISSED_EVENTS\t= 1 << 2,\n};\n\nenum ib_srq_type {\n\tIB_SRQT_BASIC = IB_UVERBS_SRQT_BASIC,\n\tIB_SRQT_XRC = IB_UVERBS_SRQT_XRC,\n\tIB_SRQT_TM = IB_UVERBS_SRQT_TM,\n};\n\nstatic inline bool ib_srq_has_cq(enum ib_srq_type srq_type)\n{\n\treturn srq_type == IB_SRQT_XRC ||\n\t       srq_type == IB_SRQT_TM;\n}\n\nenum ib_srq_attr_mask {\n\tIB_SRQ_MAX_WR\t= 1 << 0,\n\tIB_SRQ_LIMIT\t= 1 << 1,\n};\n\nstruct ib_srq_attr {\n\tu32\tmax_wr;\n\tu32\tmax_sge;\n\tu32\tsrq_limit;\n};\n\nstruct ib_srq_init_attr {\n\tvoid\t\t      (*event_handler)(struct ib_event *, void *);\n\tvoid\t\t       *srq_context;\n\tstruct ib_srq_attr\tattr;\n\tenum ib_srq_type\tsrq_type;\n\n\tstruct {\n\t\tstruct ib_cq   *cq;\n\t\tunion {\n\t\t\tstruct {\n\t\t\t\tstruct ib_xrcd *xrcd;\n\t\t\t} xrc;\n\n\t\t\tstruct {\n\t\t\t\tu32\t\tmax_num_tags;\n\t\t\t} tag_matching;\n\t\t};\n\t} ext;\n};\n\nstruct ib_qp_cap {\n\tu32\tmax_send_wr;\n\tu32\tmax_recv_wr;\n\tu32\tmax_send_sge;\n\tu32\tmax_recv_sge;\n\tu32\tmax_inline_data;\n\n\t \n\tu32\tmax_rdma_ctxs;\n};\n\nenum ib_sig_type {\n\tIB_SIGNAL_ALL_WR,\n\tIB_SIGNAL_REQ_WR\n};\n\nenum ib_qp_type {\n\t \n\tIB_QPT_SMI,\n\tIB_QPT_GSI,\n\n\tIB_QPT_RC = IB_UVERBS_QPT_RC,\n\tIB_QPT_UC = IB_UVERBS_QPT_UC,\n\tIB_QPT_UD = IB_UVERBS_QPT_UD,\n\tIB_QPT_RAW_IPV6,\n\tIB_QPT_RAW_ETHERTYPE,\n\tIB_QPT_RAW_PACKET = IB_UVERBS_QPT_RAW_PACKET,\n\tIB_QPT_XRC_INI = IB_UVERBS_QPT_XRC_INI,\n\tIB_QPT_XRC_TGT = IB_UVERBS_QPT_XRC_TGT,\n\tIB_QPT_MAX,\n\tIB_QPT_DRIVER = IB_UVERBS_QPT_DRIVER,\n\t \n\tIB_QPT_RESERVED1 = 0x1000,\n\tIB_QPT_RESERVED2,\n\tIB_QPT_RESERVED3,\n\tIB_QPT_RESERVED4,\n\tIB_QPT_RESERVED5,\n\tIB_QPT_RESERVED6,\n\tIB_QPT_RESERVED7,\n\tIB_QPT_RESERVED8,\n\tIB_QPT_RESERVED9,\n\tIB_QPT_RESERVED10,\n};\n\nenum ib_qp_create_flags {\n\tIB_QP_CREATE_IPOIB_UD_LSO\t\t= 1 << 0,\n\tIB_QP_CREATE_BLOCK_MULTICAST_LOOPBACK\t=\n\t\tIB_UVERBS_QP_CREATE_BLOCK_MULTICAST_LOOPBACK,\n\tIB_QP_CREATE_CROSS_CHANNEL              = 1 << 2,\n\tIB_QP_CREATE_MANAGED_SEND               = 1 << 3,\n\tIB_QP_CREATE_MANAGED_RECV               = 1 << 4,\n\tIB_QP_CREATE_NETIF_QP\t\t\t= 1 << 5,\n\tIB_QP_CREATE_INTEGRITY_EN\t\t= 1 << 6,\n\tIB_QP_CREATE_NETDEV_USE\t\t\t= 1 << 7,\n\tIB_QP_CREATE_SCATTER_FCS\t\t=\n\t\tIB_UVERBS_QP_CREATE_SCATTER_FCS,\n\tIB_QP_CREATE_CVLAN_STRIPPING\t\t=\n\t\tIB_UVERBS_QP_CREATE_CVLAN_STRIPPING,\n\tIB_QP_CREATE_SOURCE_QPN\t\t\t= 1 << 10,\n\tIB_QP_CREATE_PCI_WRITE_END_PADDING\t=\n\t\tIB_UVERBS_QP_CREATE_PCI_WRITE_END_PADDING,\n\t \n\tIB_QP_CREATE_RESERVED_START\t\t= 1 << 26,\n\tIB_QP_CREATE_RESERVED_END\t\t= 1 << 31,\n};\n\n \n\nstruct ib_qp_init_attr {\n\t \n\tvoid                  (*event_handler)(struct ib_event *, void *);\n\n\tvoid\t\t       *qp_context;\n\tstruct ib_cq\t       *send_cq;\n\tstruct ib_cq\t       *recv_cq;\n\tstruct ib_srq\t       *srq;\n\tstruct ib_xrcd\t       *xrcd;      \n\tstruct ib_qp_cap\tcap;\n\tenum ib_sig_type\tsq_sig_type;\n\tenum ib_qp_type\t\tqp_type;\n\tu32\t\t\tcreate_flags;\n\n\t \n\tu32\t\t\tport_num;\n\tstruct ib_rwq_ind_table *rwq_ind_tbl;\n\tu32\t\t\tsource_qpn;\n};\n\nstruct ib_qp_open_attr {\n\tvoid                  (*event_handler)(struct ib_event *, void *);\n\tvoid\t\t       *qp_context;\n\tu32\t\t\tqp_num;\n\tenum ib_qp_type\t\tqp_type;\n};\n\nenum ib_rnr_timeout {\n\tIB_RNR_TIMER_655_36 =  0,\n\tIB_RNR_TIMER_000_01 =  1,\n\tIB_RNR_TIMER_000_02 =  2,\n\tIB_RNR_TIMER_000_03 =  3,\n\tIB_RNR_TIMER_000_04 =  4,\n\tIB_RNR_TIMER_000_06 =  5,\n\tIB_RNR_TIMER_000_08 =  6,\n\tIB_RNR_TIMER_000_12 =  7,\n\tIB_RNR_TIMER_000_16 =  8,\n\tIB_RNR_TIMER_000_24 =  9,\n\tIB_RNR_TIMER_000_32 = 10,\n\tIB_RNR_TIMER_000_48 = 11,\n\tIB_RNR_TIMER_000_64 = 12,\n\tIB_RNR_TIMER_000_96 = 13,\n\tIB_RNR_TIMER_001_28 = 14,\n\tIB_RNR_TIMER_001_92 = 15,\n\tIB_RNR_TIMER_002_56 = 16,\n\tIB_RNR_TIMER_003_84 = 17,\n\tIB_RNR_TIMER_005_12 = 18,\n\tIB_RNR_TIMER_007_68 = 19,\n\tIB_RNR_TIMER_010_24 = 20,\n\tIB_RNR_TIMER_015_36 = 21,\n\tIB_RNR_TIMER_020_48 = 22,\n\tIB_RNR_TIMER_030_72 = 23,\n\tIB_RNR_TIMER_040_96 = 24,\n\tIB_RNR_TIMER_061_44 = 25,\n\tIB_RNR_TIMER_081_92 = 26,\n\tIB_RNR_TIMER_122_88 = 27,\n\tIB_RNR_TIMER_163_84 = 28,\n\tIB_RNR_TIMER_245_76 = 29,\n\tIB_RNR_TIMER_327_68 = 30,\n\tIB_RNR_TIMER_491_52 = 31\n};\n\nenum ib_qp_attr_mask {\n\tIB_QP_STATE\t\t\t= 1,\n\tIB_QP_CUR_STATE\t\t\t= (1<<1),\n\tIB_QP_EN_SQD_ASYNC_NOTIFY\t= (1<<2),\n\tIB_QP_ACCESS_FLAGS\t\t= (1<<3),\n\tIB_QP_PKEY_INDEX\t\t= (1<<4),\n\tIB_QP_PORT\t\t\t= (1<<5),\n\tIB_QP_QKEY\t\t\t= (1<<6),\n\tIB_QP_AV\t\t\t= (1<<7),\n\tIB_QP_PATH_MTU\t\t\t= (1<<8),\n\tIB_QP_TIMEOUT\t\t\t= (1<<9),\n\tIB_QP_RETRY_CNT\t\t\t= (1<<10),\n\tIB_QP_RNR_RETRY\t\t\t= (1<<11),\n\tIB_QP_RQ_PSN\t\t\t= (1<<12),\n\tIB_QP_MAX_QP_RD_ATOMIC\t\t= (1<<13),\n\tIB_QP_ALT_PATH\t\t\t= (1<<14),\n\tIB_QP_MIN_RNR_TIMER\t\t= (1<<15),\n\tIB_QP_SQ_PSN\t\t\t= (1<<16),\n\tIB_QP_MAX_DEST_RD_ATOMIC\t= (1<<17),\n\tIB_QP_PATH_MIG_STATE\t\t= (1<<18),\n\tIB_QP_CAP\t\t\t= (1<<19),\n\tIB_QP_DEST_QPN\t\t\t= (1<<20),\n\tIB_QP_RESERVED1\t\t\t= (1<<21),\n\tIB_QP_RESERVED2\t\t\t= (1<<22),\n\tIB_QP_RESERVED3\t\t\t= (1<<23),\n\tIB_QP_RESERVED4\t\t\t= (1<<24),\n\tIB_QP_RATE_LIMIT\t\t= (1<<25),\n\n\tIB_QP_ATTR_STANDARD_BITS = GENMASK(20, 0),\n};\n\nenum ib_qp_state {\n\tIB_QPS_RESET,\n\tIB_QPS_INIT,\n\tIB_QPS_RTR,\n\tIB_QPS_RTS,\n\tIB_QPS_SQD,\n\tIB_QPS_SQE,\n\tIB_QPS_ERR\n};\n\nenum ib_mig_state {\n\tIB_MIG_MIGRATED,\n\tIB_MIG_REARM,\n\tIB_MIG_ARMED\n};\n\nenum ib_mw_type {\n\tIB_MW_TYPE_1 = 1,\n\tIB_MW_TYPE_2 = 2\n};\n\nstruct ib_qp_attr {\n\tenum ib_qp_state\tqp_state;\n\tenum ib_qp_state\tcur_qp_state;\n\tenum ib_mtu\t\tpath_mtu;\n\tenum ib_mig_state\tpath_mig_state;\n\tu32\t\t\tqkey;\n\tu32\t\t\trq_psn;\n\tu32\t\t\tsq_psn;\n\tu32\t\t\tdest_qp_num;\n\tint\t\t\tqp_access_flags;\n\tstruct ib_qp_cap\tcap;\n\tstruct rdma_ah_attr\tah_attr;\n\tstruct rdma_ah_attr\talt_ah_attr;\n\tu16\t\t\tpkey_index;\n\tu16\t\t\talt_pkey_index;\n\tu8\t\t\ten_sqd_async_notify;\n\tu8\t\t\tsq_draining;\n\tu8\t\t\tmax_rd_atomic;\n\tu8\t\t\tmax_dest_rd_atomic;\n\tu8\t\t\tmin_rnr_timer;\n\tu32\t\t\tport_num;\n\tu8\t\t\ttimeout;\n\tu8\t\t\tretry_cnt;\n\tu8\t\t\trnr_retry;\n\tu32\t\t\talt_port_num;\n\tu8\t\t\talt_timeout;\n\tu32\t\t\trate_limit;\n\tstruct net_device\t*xmit_slave;\n};\n\nenum ib_wr_opcode {\n\t \n\tIB_WR_RDMA_WRITE = IB_UVERBS_WR_RDMA_WRITE,\n\tIB_WR_RDMA_WRITE_WITH_IMM = IB_UVERBS_WR_RDMA_WRITE_WITH_IMM,\n\tIB_WR_SEND = IB_UVERBS_WR_SEND,\n\tIB_WR_SEND_WITH_IMM = IB_UVERBS_WR_SEND_WITH_IMM,\n\tIB_WR_RDMA_READ = IB_UVERBS_WR_RDMA_READ,\n\tIB_WR_ATOMIC_CMP_AND_SWP = IB_UVERBS_WR_ATOMIC_CMP_AND_SWP,\n\tIB_WR_ATOMIC_FETCH_AND_ADD = IB_UVERBS_WR_ATOMIC_FETCH_AND_ADD,\n\tIB_WR_BIND_MW = IB_UVERBS_WR_BIND_MW,\n\tIB_WR_LSO = IB_UVERBS_WR_TSO,\n\tIB_WR_SEND_WITH_INV = IB_UVERBS_WR_SEND_WITH_INV,\n\tIB_WR_RDMA_READ_WITH_INV = IB_UVERBS_WR_RDMA_READ_WITH_INV,\n\tIB_WR_LOCAL_INV = IB_UVERBS_WR_LOCAL_INV,\n\tIB_WR_MASKED_ATOMIC_CMP_AND_SWP =\n\t\tIB_UVERBS_WR_MASKED_ATOMIC_CMP_AND_SWP,\n\tIB_WR_MASKED_ATOMIC_FETCH_AND_ADD =\n\t\tIB_UVERBS_WR_MASKED_ATOMIC_FETCH_AND_ADD,\n\tIB_WR_FLUSH = IB_UVERBS_WR_FLUSH,\n\tIB_WR_ATOMIC_WRITE = IB_UVERBS_WR_ATOMIC_WRITE,\n\n\t \n\tIB_WR_REG_MR = 0x20,\n\tIB_WR_REG_MR_INTEGRITY,\n\n\t \n\tIB_WR_RESERVED1 = 0xf0,\n\tIB_WR_RESERVED2,\n\tIB_WR_RESERVED3,\n\tIB_WR_RESERVED4,\n\tIB_WR_RESERVED5,\n\tIB_WR_RESERVED6,\n\tIB_WR_RESERVED7,\n\tIB_WR_RESERVED8,\n\tIB_WR_RESERVED9,\n\tIB_WR_RESERVED10,\n};\n\nenum ib_send_flags {\n\tIB_SEND_FENCE\t\t= 1,\n\tIB_SEND_SIGNALED\t= (1<<1),\n\tIB_SEND_SOLICITED\t= (1<<2),\n\tIB_SEND_INLINE\t\t= (1<<3),\n\tIB_SEND_IP_CSUM\t\t= (1<<4),\n\n\t \n\tIB_SEND_RESERVED_START\t= (1 << 26),\n\tIB_SEND_RESERVED_END\t= (1 << 31),\n};\n\nstruct ib_sge {\n\tu64\taddr;\n\tu32\tlength;\n\tu32\tlkey;\n};\n\nstruct ib_cqe {\n\tvoid (*done)(struct ib_cq *cq, struct ib_wc *wc);\n};\n\nstruct ib_send_wr {\n\tstruct ib_send_wr      *next;\n\tunion {\n\t\tu64\t\twr_id;\n\t\tstruct ib_cqe\t*wr_cqe;\n\t};\n\tstruct ib_sge\t       *sg_list;\n\tint\t\t\tnum_sge;\n\tenum ib_wr_opcode\topcode;\n\tint\t\t\tsend_flags;\n\tunion {\n\t\t__be32\t\timm_data;\n\t\tu32\t\tinvalidate_rkey;\n\t} ex;\n};\n\nstruct ib_rdma_wr {\n\tstruct ib_send_wr\twr;\n\tu64\t\t\tremote_addr;\n\tu32\t\t\trkey;\n};\n\nstatic inline const struct ib_rdma_wr *rdma_wr(const struct ib_send_wr *wr)\n{\n\treturn container_of(wr, struct ib_rdma_wr, wr);\n}\n\nstruct ib_atomic_wr {\n\tstruct ib_send_wr\twr;\n\tu64\t\t\tremote_addr;\n\tu64\t\t\tcompare_add;\n\tu64\t\t\tswap;\n\tu64\t\t\tcompare_add_mask;\n\tu64\t\t\tswap_mask;\n\tu32\t\t\trkey;\n};\n\nstatic inline const struct ib_atomic_wr *atomic_wr(const struct ib_send_wr *wr)\n{\n\treturn container_of(wr, struct ib_atomic_wr, wr);\n}\n\nstruct ib_ud_wr {\n\tstruct ib_send_wr\twr;\n\tstruct ib_ah\t\t*ah;\n\tvoid\t\t\t*header;\n\tint\t\t\thlen;\n\tint\t\t\tmss;\n\tu32\t\t\tremote_qpn;\n\tu32\t\t\tremote_qkey;\n\tu16\t\t\tpkey_index;  \n\tu32\t\t\tport_num;  \n};\n\nstatic inline const struct ib_ud_wr *ud_wr(const struct ib_send_wr *wr)\n{\n\treturn container_of(wr, struct ib_ud_wr, wr);\n}\n\nstruct ib_reg_wr {\n\tstruct ib_send_wr\twr;\n\tstruct ib_mr\t\t*mr;\n\tu32\t\t\tkey;\n\tint\t\t\taccess;\n};\n\nstatic inline const struct ib_reg_wr *reg_wr(const struct ib_send_wr *wr)\n{\n\treturn container_of(wr, struct ib_reg_wr, wr);\n}\n\nstruct ib_recv_wr {\n\tstruct ib_recv_wr      *next;\n\tunion {\n\t\tu64\t\twr_id;\n\t\tstruct ib_cqe\t*wr_cqe;\n\t};\n\tstruct ib_sge\t       *sg_list;\n\tint\t\t\tnum_sge;\n};\n\nenum ib_access_flags {\n\tIB_ACCESS_LOCAL_WRITE = IB_UVERBS_ACCESS_LOCAL_WRITE,\n\tIB_ACCESS_REMOTE_WRITE = IB_UVERBS_ACCESS_REMOTE_WRITE,\n\tIB_ACCESS_REMOTE_READ = IB_UVERBS_ACCESS_REMOTE_READ,\n\tIB_ACCESS_REMOTE_ATOMIC = IB_UVERBS_ACCESS_REMOTE_ATOMIC,\n\tIB_ACCESS_MW_BIND = IB_UVERBS_ACCESS_MW_BIND,\n\tIB_ZERO_BASED = IB_UVERBS_ACCESS_ZERO_BASED,\n\tIB_ACCESS_ON_DEMAND = IB_UVERBS_ACCESS_ON_DEMAND,\n\tIB_ACCESS_HUGETLB = IB_UVERBS_ACCESS_HUGETLB,\n\tIB_ACCESS_RELAXED_ORDERING = IB_UVERBS_ACCESS_RELAXED_ORDERING,\n\tIB_ACCESS_FLUSH_GLOBAL = IB_UVERBS_ACCESS_FLUSH_GLOBAL,\n\tIB_ACCESS_FLUSH_PERSISTENT = IB_UVERBS_ACCESS_FLUSH_PERSISTENT,\n\n\tIB_ACCESS_OPTIONAL = IB_UVERBS_ACCESS_OPTIONAL_RANGE,\n\tIB_ACCESS_SUPPORTED =\n\t\t((IB_ACCESS_FLUSH_PERSISTENT << 1) - 1) | IB_ACCESS_OPTIONAL,\n};\n\n \nenum ib_mr_rereg_flags {\n\tIB_MR_REREG_TRANS\t= 1,\n\tIB_MR_REREG_PD\t\t= (1<<1),\n\tIB_MR_REREG_ACCESS\t= (1<<2),\n\tIB_MR_REREG_SUPPORTED\t= ((IB_MR_REREG_ACCESS << 1) - 1)\n};\n\nstruct ib_umem;\n\nenum rdma_remove_reason {\n\t \n\tRDMA_REMOVE_DESTROY,\n\t \n\tRDMA_REMOVE_CLOSE,\n\t \n\tRDMA_REMOVE_DRIVER_REMOVE,\n\t \n\tRDMA_REMOVE_ABORT,\n\t \n\tRDMA_REMOVE_DRIVER_FAILURE,\n};\n\nstruct ib_rdmacg_object {\n#ifdef CONFIG_CGROUP_RDMA\n\tstruct rdma_cgroup\t*cg;\t\t \n#endif\n};\n\nstruct ib_ucontext {\n\tstruct ib_device       *device;\n\tstruct ib_uverbs_file  *ufile;\n\n\tstruct ib_rdmacg_object\tcg_obj;\n\t \n\tstruct rdma_restrack_entry res;\n\tstruct xarray mmap_xa;\n};\n\nstruct ib_uobject {\n\tu64\t\t\tuser_handle;\t \n\t \n\tstruct ib_uverbs_file  *ufile;\n\t \n\tstruct ib_ucontext     *context;\t \n\tvoid\t\t       *object;\t\t \n\tstruct list_head\tlist;\t\t \n\tstruct ib_rdmacg_object\tcg_obj;\t\t \n\tint\t\t\tid;\t\t \n\tstruct kref\t\tref;\n\tatomic_t\t\tusecnt;\t\t \n\tstruct rcu_head\t\trcu;\t\t \n\n\tconst struct uverbs_api_object *uapi_object;\n};\n\nstruct ib_udata {\n\tconst void __user *inbuf;\n\tvoid __user *outbuf;\n\tsize_t       inlen;\n\tsize_t       outlen;\n};\n\nstruct ib_pd {\n\tu32\t\t\tlocal_dma_lkey;\n\tu32\t\t\tflags;\n\tstruct ib_device       *device;\n\tstruct ib_uobject      *uobject;\n\tatomic_t          \tusecnt;  \n\n\tu32\t\t\tunsafe_global_rkey;\n\n\t \n\tstruct ib_mr\t       *__internal_mr;\n\tstruct rdma_restrack_entry res;\n};\n\nstruct ib_xrcd {\n\tstruct ib_device       *device;\n\tatomic_t\t\tusecnt;  \n\tstruct inode\t       *inode;\n\tstruct rw_semaphore\ttgt_qps_rwsem;\n\tstruct xarray\t\ttgt_qps;\n};\n\nstruct ib_ah {\n\tstruct ib_device\t*device;\n\tstruct ib_pd\t\t*pd;\n\tstruct ib_uobject\t*uobject;\n\tconst struct ib_gid_attr *sgid_attr;\n\tenum rdma_ah_attr_type\ttype;\n};\n\ntypedef void (*ib_comp_handler)(struct ib_cq *cq, void *cq_context);\n\nenum ib_poll_context {\n\tIB_POLL_SOFTIRQ,\t    \n\tIB_POLL_WORKQUEUE,\t    \n\tIB_POLL_UNBOUND_WORKQUEUE,  \n\tIB_POLL_LAST_POOL_TYPE = IB_POLL_UNBOUND_WORKQUEUE,\n\n\tIB_POLL_DIRECT,\t\t    \n};\n\nstruct ib_cq {\n\tstruct ib_device       *device;\n\tstruct ib_ucq_object   *uobject;\n\tib_comp_handler   \tcomp_handler;\n\tvoid                  (*event_handler)(struct ib_event *, void *);\n\tvoid                   *cq_context;\n\tint               \tcqe;\n\tunsigned int\t\tcqe_used;\n\tatomic_t          \tusecnt;  \n\tenum ib_poll_context\tpoll_ctx;\n\tstruct ib_wc\t\t*wc;\n\tstruct list_head        pool_entry;\n\tunion {\n\t\tstruct irq_poll\t\tiop;\n\t\tstruct work_struct\twork;\n\t};\n\tstruct workqueue_struct *comp_wq;\n\tstruct dim *dim;\n\n\t \n\tktime_t timestamp;\n\tu8 interrupt:1;\n\tu8 shared:1;\n\tunsigned int comp_vector;\n\n\t \n\tstruct rdma_restrack_entry res;\n};\n\nstruct ib_srq {\n\tstruct ib_device       *device;\n\tstruct ib_pd\t       *pd;\n\tstruct ib_usrq_object  *uobject;\n\tvoid\t\t      (*event_handler)(struct ib_event *, void *);\n\tvoid\t\t       *srq_context;\n\tenum ib_srq_type\tsrq_type;\n\tatomic_t\t\tusecnt;\n\n\tstruct {\n\t\tstruct ib_cq   *cq;\n\t\tunion {\n\t\t\tstruct {\n\t\t\t\tstruct ib_xrcd *xrcd;\n\t\t\t\tu32\t\tsrq_num;\n\t\t\t} xrc;\n\t\t};\n\t} ext;\n\n\t \n\tstruct rdma_restrack_entry res;\n};\n\nenum ib_raw_packet_caps {\n\t \n\tIB_RAW_PACKET_CAP_CVLAN_STRIPPING =\n\t\tIB_UVERBS_RAW_PACKET_CAP_CVLAN_STRIPPING,\n\t \n\tIB_RAW_PACKET_CAP_SCATTER_FCS = IB_UVERBS_RAW_PACKET_CAP_SCATTER_FCS,\n\t \n\tIB_RAW_PACKET_CAP_IP_CSUM = IB_UVERBS_RAW_PACKET_CAP_IP_CSUM,\n\t \n\tIB_RAW_PACKET_CAP_DELAY_DROP = IB_UVERBS_RAW_PACKET_CAP_DELAY_DROP,\n};\n\nenum ib_wq_type {\n\tIB_WQT_RQ = IB_UVERBS_WQT_RQ,\n};\n\nenum ib_wq_state {\n\tIB_WQS_RESET,\n\tIB_WQS_RDY,\n\tIB_WQS_ERR\n};\n\nstruct ib_wq {\n\tstruct ib_device       *device;\n\tstruct ib_uwq_object   *uobject;\n\tvoid\t\t    *wq_context;\n\tvoid\t\t    (*event_handler)(struct ib_event *, void *);\n\tstruct ib_pd\t       *pd;\n\tstruct ib_cq\t       *cq;\n\tu32\t\twq_num;\n\tenum ib_wq_state       state;\n\tenum ib_wq_type\twq_type;\n\tatomic_t\t\tusecnt;\n};\n\nenum ib_wq_flags {\n\tIB_WQ_FLAGS_CVLAN_STRIPPING\t= IB_UVERBS_WQ_FLAGS_CVLAN_STRIPPING,\n\tIB_WQ_FLAGS_SCATTER_FCS\t\t= IB_UVERBS_WQ_FLAGS_SCATTER_FCS,\n\tIB_WQ_FLAGS_DELAY_DROP\t\t= IB_UVERBS_WQ_FLAGS_DELAY_DROP,\n\tIB_WQ_FLAGS_PCI_WRITE_END_PADDING =\n\t\t\t\tIB_UVERBS_WQ_FLAGS_PCI_WRITE_END_PADDING,\n};\n\nstruct ib_wq_init_attr {\n\tvoid\t\t       *wq_context;\n\tenum ib_wq_type\twq_type;\n\tu32\t\tmax_wr;\n\tu32\t\tmax_sge;\n\tstruct\tib_cq\t       *cq;\n\tvoid\t\t    (*event_handler)(struct ib_event *, void *);\n\tu32\t\tcreate_flags;  \n};\n\nenum ib_wq_attr_mask {\n\tIB_WQ_STATE\t\t= 1 << 0,\n\tIB_WQ_CUR_STATE\t\t= 1 << 1,\n\tIB_WQ_FLAGS\t\t= 1 << 2,\n};\n\nstruct ib_wq_attr {\n\tenum\tib_wq_state\twq_state;\n\tenum\tib_wq_state\tcurr_wq_state;\n\tu32\t\t\tflags;  \n\tu32\t\t\tflags_mask;  \n};\n\nstruct ib_rwq_ind_table {\n\tstruct ib_device\t*device;\n\tstruct ib_uobject      *uobject;\n\tatomic_t\t\tusecnt;\n\tu32\t\tind_tbl_num;\n\tu32\t\tlog_ind_tbl_size;\n\tstruct ib_wq\t**ind_tbl;\n};\n\nstruct ib_rwq_ind_table_init_attr {\n\tu32\t\tlog_ind_tbl_size;\n\t \n\tstruct ib_wq\t**ind_tbl;\n};\n\nenum port_pkey_state {\n\tIB_PORT_PKEY_NOT_VALID = 0,\n\tIB_PORT_PKEY_VALID = 1,\n\tIB_PORT_PKEY_LISTED = 2,\n};\n\nstruct ib_qp_security;\n\nstruct ib_port_pkey {\n\tenum port_pkey_state\tstate;\n\tu16\t\t\tpkey_index;\n\tu32\t\t\tport_num;\n\tstruct list_head\tqp_list;\n\tstruct list_head\tto_error_list;\n\tstruct ib_qp_security  *sec;\n};\n\nstruct ib_ports_pkeys {\n\tstruct ib_port_pkey\tmain;\n\tstruct ib_port_pkey\talt;\n};\n\nstruct ib_qp_security {\n\tstruct ib_qp\t       *qp;\n\tstruct ib_device       *dev;\n\t \n\tstruct mutex\t\tmutex;\n\tstruct ib_ports_pkeys  *ports_pkeys;\n\t \n\tstruct list_head        shared_qp_list;\n\tvoid                   *security;\n\tbool\t\t\tdestroying;\n\tatomic_t\t\terror_list_count;\n\tstruct completion\terror_complete;\n\tint\t\t\terror_comps_pending;\n};\n\n \nstruct ib_qp {\n\tstruct ib_device       *device;\n\tstruct ib_pd\t       *pd;\n\tstruct ib_cq\t       *send_cq;\n\tstruct ib_cq\t       *recv_cq;\n\tspinlock_t\t\tmr_lock;\n\tint\t\t\tmrs_used;\n\tstruct list_head\trdma_mrs;\n\tstruct list_head\tsig_mrs;\n\tstruct ib_srq\t       *srq;\n\tstruct ib_xrcd\t       *xrcd;  \n\tstruct list_head\txrcd_list;\n\n\t \n\tatomic_t\t\tusecnt;\n\tstruct list_head\topen_list;\n\tstruct ib_qp           *real_qp;\n\tstruct ib_uqp_object   *uobject;\n\tvoid                  (*event_handler)(struct ib_event *, void *);\n\tvoid\t\t       *qp_context;\n\t \n\tconst struct ib_gid_attr *av_sgid_attr;\n\tconst struct ib_gid_attr *alt_path_sgid_attr;\n\tu32\t\t\tqp_num;\n\tu32\t\t\tmax_write_sge;\n\tu32\t\t\tmax_read_sge;\n\tenum ib_qp_type\t\tqp_type;\n\tstruct ib_rwq_ind_table *rwq_ind_tbl;\n\tstruct ib_qp_security  *qp_sec;\n\tu32\t\t\tport;\n\n\tbool\t\t\tintegrity_en;\n\t \n\tstruct rdma_restrack_entry     res;\n\n\t \n\tstruct rdma_counter    *counter;\n};\n\nstruct ib_dm {\n\tstruct ib_device  *device;\n\tu32\t\t   length;\n\tu32\t\t   flags;\n\tstruct ib_uobject *uobject;\n\tatomic_t\t   usecnt;\n};\n\nstruct ib_mr {\n\tstruct ib_device  *device;\n\tstruct ib_pd\t  *pd;\n\tu32\t\t   lkey;\n\tu32\t\t   rkey;\n\tu64\t\t   iova;\n\tu64\t\t   length;\n\tunsigned int\t   page_size;\n\tenum ib_mr_type\t   type;\n\tbool\t\t   need_inval;\n\tunion {\n\t\tstruct ib_uobject\t*uobject;\t \n\t\tstruct list_head\tqp_entry;\t \n\t};\n\n\tstruct ib_dm      *dm;\n\tstruct ib_sig_attrs *sig_attrs;  \n\t \n\tstruct rdma_restrack_entry res;\n};\n\nstruct ib_mw {\n\tstruct ib_device\t*device;\n\tstruct ib_pd\t\t*pd;\n\tstruct ib_uobject\t*uobject;\n\tu32\t\t\trkey;\n\tenum ib_mw_type         type;\n};\n\n \nenum ib_flow_attr_type {\n\t \n\tIB_FLOW_ATTR_NORMAL\t\t= 0x0,\n\t \n\tIB_FLOW_ATTR_ALL_DEFAULT\t= 0x1,\n\t \n\tIB_FLOW_ATTR_MC_DEFAULT\t\t= 0x2,\n\t \n\tIB_FLOW_ATTR_SNIFFER\t\t= 0x3\n};\n\n \nenum ib_flow_spec_type {\n\t \n\tIB_FLOW_SPEC_ETH\t\t= 0x20,\n\tIB_FLOW_SPEC_IB\t\t\t= 0x22,\n\t \n\tIB_FLOW_SPEC_IPV4\t\t= 0x30,\n\tIB_FLOW_SPEC_IPV6\t\t= 0x31,\n\tIB_FLOW_SPEC_ESP                = 0x34,\n\t \n\tIB_FLOW_SPEC_TCP\t\t= 0x40,\n\tIB_FLOW_SPEC_UDP\t\t= 0x41,\n\tIB_FLOW_SPEC_VXLAN_TUNNEL\t= 0x50,\n\tIB_FLOW_SPEC_GRE\t\t= 0x51,\n\tIB_FLOW_SPEC_MPLS\t\t= 0x60,\n\tIB_FLOW_SPEC_INNER\t\t= 0x100,\n\t \n\tIB_FLOW_SPEC_ACTION_TAG         = 0x1000,\n\tIB_FLOW_SPEC_ACTION_DROP        = 0x1001,\n\tIB_FLOW_SPEC_ACTION_HANDLE\t= 0x1002,\n\tIB_FLOW_SPEC_ACTION_COUNT       = 0x1003,\n};\n#define IB_FLOW_SPEC_LAYER_MASK\t0xF0\n#define IB_FLOW_SPEC_SUPPORT_LAYERS 10\n\nenum ib_flow_flags {\n\tIB_FLOW_ATTR_FLAGS_DONT_TRAP = 1UL << 1,  \n\tIB_FLOW_ATTR_FLAGS_EGRESS = 1UL << 2,  \n\tIB_FLOW_ATTR_FLAGS_RESERVED  = 1UL << 3   \n};\n\nstruct ib_flow_eth_filter {\n\tu8\tdst_mac[6];\n\tu8\tsrc_mac[6];\n\t__be16\tether_type;\n\t__be16\tvlan_tag;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_eth {\n\tu32\t\t\t  type;\n\tu16\t\t\t  size;\n\tstruct ib_flow_eth_filter val;\n\tstruct ib_flow_eth_filter mask;\n};\n\nstruct ib_flow_ib_filter {\n\t__be16 dlid;\n\t__u8   sl;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_ib {\n\tu32\t\t\t type;\n\tu16\t\t\t size;\n\tstruct ib_flow_ib_filter val;\n\tstruct ib_flow_ib_filter mask;\n};\n\n \nenum ib_ipv4_flags {\n\tIB_IPV4_DONT_FRAG = 0x2,  \n\tIB_IPV4_MORE_FRAG = 0X4   \n};\n\nstruct ib_flow_ipv4_filter {\n\t__be32\tsrc_ip;\n\t__be32\tdst_ip;\n\tu8\tproto;\n\tu8\ttos;\n\tu8\tttl;\n\tu8\tflags;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_ipv4 {\n\tu32\t\t\t   type;\n\tu16\t\t\t   size;\n\tstruct ib_flow_ipv4_filter val;\n\tstruct ib_flow_ipv4_filter mask;\n};\n\nstruct ib_flow_ipv6_filter {\n\tu8\tsrc_ip[16];\n\tu8\tdst_ip[16];\n\t__be32\tflow_label;\n\tu8\tnext_hdr;\n\tu8\ttraffic_class;\n\tu8\thop_limit;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_ipv6 {\n\tu32\t\t\t   type;\n\tu16\t\t\t   size;\n\tstruct ib_flow_ipv6_filter val;\n\tstruct ib_flow_ipv6_filter mask;\n};\n\nstruct ib_flow_tcp_udp_filter {\n\t__be16\tdst_port;\n\t__be16\tsrc_port;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_tcp_udp {\n\tu32\t\t\t      type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_tcp_udp_filter val;\n\tstruct ib_flow_tcp_udp_filter mask;\n};\n\nstruct ib_flow_tunnel_filter {\n\t__be32\ttunnel_id;\n\tu8\treal_sz[];\n};\n\n \nstruct ib_flow_spec_tunnel {\n\tu32\t\t\t      type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_tunnel_filter  val;\n\tstruct ib_flow_tunnel_filter  mask;\n};\n\nstruct ib_flow_esp_filter {\n\t__be32\tspi;\n\t__be32  seq;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_esp {\n\tu32                           type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_esp_filter     val;\n\tstruct ib_flow_esp_filter     mask;\n};\n\nstruct ib_flow_gre_filter {\n\t__be16 c_ks_res0_ver;\n\t__be16 protocol;\n\t__be32 key;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_gre {\n\tu32                           type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_gre_filter     val;\n\tstruct ib_flow_gre_filter     mask;\n};\n\nstruct ib_flow_mpls_filter {\n\t__be32 tag;\n\t \n\tu8\treal_sz[];\n};\n\nstruct ib_flow_spec_mpls {\n\tu32                           type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_mpls_filter     val;\n\tstruct ib_flow_mpls_filter     mask;\n};\n\nstruct ib_flow_spec_action_tag {\n\tenum ib_flow_spec_type\t      type;\n\tu16\t\t\t      size;\n\tu32                           tag_id;\n};\n\nstruct ib_flow_spec_action_drop {\n\tenum ib_flow_spec_type\t      type;\n\tu16\t\t\t      size;\n};\n\nstruct ib_flow_spec_action_handle {\n\tenum ib_flow_spec_type\t      type;\n\tu16\t\t\t      size;\n\tstruct ib_flow_action\t     *act;\n};\n\nenum ib_counters_description {\n\tIB_COUNTER_PACKETS,\n\tIB_COUNTER_BYTES,\n};\n\nstruct ib_flow_spec_action_count {\n\tenum ib_flow_spec_type type;\n\tu16 size;\n\tstruct ib_counters *counters;\n};\n\nunion ib_flow_spec {\n\tstruct {\n\t\tu32\t\t\ttype;\n\t\tu16\t\t\tsize;\n\t};\n\tstruct ib_flow_spec_eth\t\teth;\n\tstruct ib_flow_spec_ib\t\tib;\n\tstruct ib_flow_spec_ipv4        ipv4;\n\tstruct ib_flow_spec_tcp_udp\ttcp_udp;\n\tstruct ib_flow_spec_ipv6        ipv6;\n\tstruct ib_flow_spec_tunnel      tunnel;\n\tstruct ib_flow_spec_esp\t\tesp;\n\tstruct ib_flow_spec_gre\t\tgre;\n\tstruct ib_flow_spec_mpls\tmpls;\n\tstruct ib_flow_spec_action_tag  flow_tag;\n\tstruct ib_flow_spec_action_drop drop;\n\tstruct ib_flow_spec_action_handle action;\n\tstruct ib_flow_spec_action_count flow_count;\n};\n\nstruct ib_flow_attr {\n\tenum ib_flow_attr_type type;\n\tu16\t     size;\n\tu16\t     priority;\n\tu32\t     flags;\n\tu8\t     num_of_specs;\n\tu32\t     port;\n\tunion ib_flow_spec flows[];\n};\n\nstruct ib_flow {\n\tstruct ib_qp\t\t*qp;\n\tstruct ib_device\t*device;\n\tstruct ib_uobject\t*uobject;\n};\n\nenum ib_flow_action_type {\n\tIB_FLOW_ACTION_UNSPECIFIED,\n\tIB_FLOW_ACTION_ESP = 1,\n};\n\nstruct ib_flow_action_attrs_esp_keymats {\n\tenum ib_uverbs_flow_action_esp_keymat\t\t\tprotocol;\n\tunion {\n\t\tstruct ib_uverbs_flow_action_esp_keymat_aes_gcm aes_gcm;\n\t} keymat;\n};\n\nstruct ib_flow_action_attrs_esp_replays {\n\tenum ib_uverbs_flow_action_esp_replay\t\t\tprotocol;\n\tunion {\n\t\tstruct ib_uverbs_flow_action_esp_replay_bmp\tbmp;\n\t} replay;\n};\n\nenum ib_flow_action_attrs_esp_flags {\n\t \n\n\t \n\tIB_FLOW_ACTION_ESP_FLAGS_ESN_TRIGGERED\t= 1ULL << 32,\n\tIB_FLOW_ACTION_ESP_FLAGS_MOD_ESP_ATTRS\t= 1ULL << 33,\n};\n\nstruct ib_flow_spec_list {\n\tstruct ib_flow_spec_list\t*next;\n\tunion ib_flow_spec\t\tspec;\n};\n\nstruct ib_flow_action_attrs_esp {\n\tstruct ib_flow_action_attrs_esp_keymats\t\t*keymat;\n\tstruct ib_flow_action_attrs_esp_replays\t\t*replay;\n\tstruct ib_flow_spec_list\t\t\t*encap;\n\t \n\tu32\t\t\t\t\t\tesn;\n\tu32\t\t\t\t\t\tspi;\n\tu32\t\t\t\t\t\tseq;\n\tu32\t\t\t\t\t\ttfc_pad;\n\t \n\tu64\t\t\t\t\t\tflags;\n\tu64\t\t\t\t\t\thard_limit_pkts;\n};\n\nstruct ib_flow_action {\n\tstruct ib_device\t\t*device;\n\tstruct ib_uobject\t\t*uobject;\n\tenum ib_flow_action_type\ttype;\n\tatomic_t\t\t\tusecnt;\n};\n\nstruct ib_mad;\n\nenum ib_process_mad_flags {\n\tIB_MAD_IGNORE_MKEY\t= 1,\n\tIB_MAD_IGNORE_BKEY\t= 2,\n\tIB_MAD_IGNORE_ALL\t= IB_MAD_IGNORE_MKEY | IB_MAD_IGNORE_BKEY\n};\n\nenum ib_mad_result {\n\tIB_MAD_RESULT_FAILURE  = 0,       \n\tIB_MAD_RESULT_SUCCESS  = 1 << 0,  \n\tIB_MAD_RESULT_REPLY    = 1 << 1,  \n\tIB_MAD_RESULT_CONSUMED = 1 << 2   \n};\n\nstruct ib_port_cache {\n\tu64\t\t      subnet_prefix;\n\tstruct ib_pkey_cache  *pkey;\n\tstruct ib_gid_table   *gid;\n\tu8                     lmc;\n\tenum ib_port_state     port_state;\n};\n\nstruct ib_port_immutable {\n\tint                           pkey_tbl_len;\n\tint                           gid_tbl_len;\n\tu32                           core_cap_flags;\n\tu32                           max_mad_size;\n};\n\nstruct ib_port_data {\n\tstruct ib_device *ib_dev;\n\n\tstruct ib_port_immutable immutable;\n\n\tspinlock_t pkey_list_lock;\n\n\tspinlock_t netdev_lock;\n\n\tstruct list_head pkey_list;\n\n\tstruct ib_port_cache cache;\n\n\tstruct net_device __rcu *netdev;\n\tnetdevice_tracker netdev_tracker;\n\tstruct hlist_node ndev_hash_link;\n\tstruct rdma_port_counter port_counter;\n\tstruct ib_port *sysfs;\n};\n\n \nenum rdma_netdev_t {\n\tRDMA_NETDEV_OPA_VNIC,\n\tRDMA_NETDEV_IPOIB,\n};\n\n \nstruct rdma_netdev {\n\tvoid              *clnt_priv;\n\tstruct ib_device  *hca;\n\tu32\t\t   port_num;\n\tint                mtu;\n\n\t \n\tvoid (*free_rdma_netdev)(struct net_device *netdev);\n\n\t \n\tvoid (*set_id)(struct net_device *netdev, int id);\n\t \n\tint (*send)(struct net_device *dev, struct sk_buff *skb,\n\t\t    struct ib_ah *address, u32 dqpn);\n\t \n\tint (*attach_mcast)(struct net_device *dev, struct ib_device *hca,\n\t\t\t    union ib_gid *gid, u16 mlid,\n\t\t\t    int set_qkey, u32 qkey);\n\tint (*detach_mcast)(struct net_device *dev, struct ib_device *hca,\n\t\t\t    union ib_gid *gid, u16 mlid);\n\t \n\tvoid (*tx_timeout)(struct net_device *dev, unsigned int txqueue);\n};\n\nstruct rdma_netdev_alloc_params {\n\tsize_t sizeof_priv;\n\tunsigned int txqs;\n\tunsigned int rxqs;\n\tvoid *param;\n\n\tint (*initialize_rdma_netdev)(struct ib_device *device, u32 port_num,\n\t\t\t\t      struct net_device *netdev, void *param);\n};\n\nstruct ib_odp_counters {\n\tatomic64_t faults;\n\tatomic64_t invalidations;\n\tatomic64_t prefetch;\n};\n\nstruct ib_counters {\n\tstruct ib_device\t*device;\n\tstruct ib_uobject\t*uobject;\n\t \n\tatomic_t\tusecnt;\n};\n\nstruct ib_counters_read_attr {\n\tu64\t*counters_buff;\n\tu32\tncounters;\n\tu32\tflags;  \n};\n\nstruct uverbs_attr_bundle;\nstruct iw_cm_id;\nstruct iw_cm_conn_param;\n\n#define INIT_RDMA_OBJ_SIZE(ib_struct, drv_struct, member)                      \\\n\t.size_##ib_struct =                                                    \\\n\t\t(sizeof(struct drv_struct) +                                   \\\n\t\t BUILD_BUG_ON_ZERO(offsetof(struct drv_struct, member)) +      \\\n\t\t BUILD_BUG_ON_ZERO(                                            \\\n\t\t\t !__same_type(((struct drv_struct *)NULL)->member,     \\\n\t\t\t\t      struct ib_struct)))\n\n#define rdma_zalloc_drv_obj_gfp(ib_dev, ib_type, gfp)                          \\\n\t((struct ib_type *)rdma_zalloc_obj(ib_dev, ib_dev->ops.size_##ib_type, \\\n\t\t\t\t\t   gfp, false))\n\n#define rdma_zalloc_drv_obj_numa(ib_dev, ib_type)                              \\\n\t((struct ib_type *)rdma_zalloc_obj(ib_dev, ib_dev->ops.size_##ib_type, \\\n\t\t\t\t\t   GFP_KERNEL, true))\n\n#define rdma_zalloc_drv_obj(ib_dev, ib_type)                                   \\\n\trdma_zalloc_drv_obj_gfp(ib_dev, ib_type, GFP_KERNEL)\n\n#define DECLARE_RDMA_OBJ_SIZE(ib_struct) size_t size_##ib_struct\n\nstruct rdma_user_mmap_entry {\n\tstruct kref ref;\n\tstruct ib_ucontext *ucontext;\n\tunsigned long start_pgoff;\n\tsize_t npages;\n\tbool driver_removed;\n};\n\n \nstatic inline u64\nrdma_user_mmap_get_offset(const struct rdma_user_mmap_entry *entry)\n{\n\treturn (u64)entry->start_pgoff << PAGE_SHIFT;\n}\n\n \nstruct ib_device_ops {\n\tstruct module *owner;\n\tenum rdma_driver_id driver_id;\n\tu32 uverbs_abi_ver;\n\tunsigned int uverbs_no_driver_id_binding:1;\n\n\t \n\tconst struct attribute_group *device_group;\n\tconst struct attribute_group **port_groups;\n\n\tint (*post_send)(struct ib_qp *qp, const struct ib_send_wr *send_wr,\n\t\t\t const struct ib_send_wr **bad_send_wr);\n\tint (*post_recv)(struct ib_qp *qp, const struct ib_recv_wr *recv_wr,\n\t\t\t const struct ib_recv_wr **bad_recv_wr);\n\tvoid (*drain_rq)(struct ib_qp *qp);\n\tvoid (*drain_sq)(struct ib_qp *qp);\n\tint (*poll_cq)(struct ib_cq *cq, int num_entries, struct ib_wc *wc);\n\tint (*peek_cq)(struct ib_cq *cq, int wc_cnt);\n\tint (*req_notify_cq)(struct ib_cq *cq, enum ib_cq_notify_flags flags);\n\tint (*post_srq_recv)(struct ib_srq *srq,\n\t\t\t     const struct ib_recv_wr *recv_wr,\n\t\t\t     const struct ib_recv_wr **bad_recv_wr);\n\tint (*process_mad)(struct ib_device *device, int process_mad_flags,\n\t\t\t   u32 port_num, const struct ib_wc *in_wc,\n\t\t\t   const struct ib_grh *in_grh,\n\t\t\t   const struct ib_mad *in_mad, struct ib_mad *out_mad,\n\t\t\t   size_t *out_mad_size, u16 *out_mad_pkey_index);\n\tint (*query_device)(struct ib_device *device,\n\t\t\t    struct ib_device_attr *device_attr,\n\t\t\t    struct ib_udata *udata);\n\tint (*modify_device)(struct ib_device *device, int device_modify_mask,\n\t\t\t     struct ib_device_modify *device_modify);\n\tvoid (*get_dev_fw_str)(struct ib_device *device, char *str);\n\tconst struct cpumask *(*get_vector_affinity)(struct ib_device *ibdev,\n\t\t\t\t\t\t     int comp_vector);\n\tint (*query_port)(struct ib_device *device, u32 port_num,\n\t\t\t  struct ib_port_attr *port_attr);\n\tint (*modify_port)(struct ib_device *device, u32 port_num,\n\t\t\t   int port_modify_mask,\n\t\t\t   struct ib_port_modify *port_modify);\n\t \n\tint (*get_port_immutable)(struct ib_device *device, u32 port_num,\n\t\t\t\t  struct ib_port_immutable *immutable);\n\tenum rdma_link_layer (*get_link_layer)(struct ib_device *device,\n\t\t\t\t\t       u32 port_num);\n\t \n\tstruct net_device *(*get_netdev)(struct ib_device *device,\n\t\t\t\t\t u32 port_num);\n\t \n\tstruct net_device *(*alloc_rdma_netdev)(\n\t\tstruct ib_device *device, u32 port_num, enum rdma_netdev_t type,\n\t\tconst char *name, unsigned char name_assign_type,\n\t\tvoid (*setup)(struct net_device *));\n\n\tint (*rdma_netdev_get_params)(struct ib_device *device, u32 port_num,\n\t\t\t\t      enum rdma_netdev_t type,\n\t\t\t\t      struct rdma_netdev_alloc_params *params);\n\t \n\tint (*query_gid)(struct ib_device *device, u32 port_num, int index,\n\t\t\t union ib_gid *gid);\n\t \n\tint (*add_gid)(const struct ib_gid_attr *attr, void **context);\n\t \n\tint (*del_gid)(const struct ib_gid_attr *attr, void **context);\n\tint (*query_pkey)(struct ib_device *device, u32 port_num, u16 index,\n\t\t\t  u16 *pkey);\n\tint (*alloc_ucontext)(struct ib_ucontext *context,\n\t\t\t      struct ib_udata *udata);\n\tvoid (*dealloc_ucontext)(struct ib_ucontext *context);\n\tint (*mmap)(struct ib_ucontext *context, struct vm_area_struct *vma);\n\t \n\tvoid (*mmap_free)(struct rdma_user_mmap_entry *entry);\n\tvoid (*disassociate_ucontext)(struct ib_ucontext *ibcontext);\n\tint (*alloc_pd)(struct ib_pd *pd, struct ib_udata *udata);\n\tint (*dealloc_pd)(struct ib_pd *pd, struct ib_udata *udata);\n\tint (*create_ah)(struct ib_ah *ah, struct rdma_ah_init_attr *attr,\n\t\t\t struct ib_udata *udata);\n\tint (*create_user_ah)(struct ib_ah *ah, struct rdma_ah_init_attr *attr,\n\t\t\t      struct ib_udata *udata);\n\tint (*modify_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);\n\tint (*query_ah)(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);\n\tint (*destroy_ah)(struct ib_ah *ah, u32 flags);\n\tint (*create_srq)(struct ib_srq *srq,\n\t\t\t  struct ib_srq_init_attr *srq_init_attr,\n\t\t\t  struct ib_udata *udata);\n\tint (*modify_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr,\n\t\t\t  enum ib_srq_attr_mask srq_attr_mask,\n\t\t\t  struct ib_udata *udata);\n\tint (*query_srq)(struct ib_srq *srq, struct ib_srq_attr *srq_attr);\n\tint (*destroy_srq)(struct ib_srq *srq, struct ib_udata *udata);\n\tint (*create_qp)(struct ib_qp *qp, struct ib_qp_init_attr *qp_init_attr,\n\t\t\t struct ib_udata *udata);\n\tint (*modify_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,\n\t\t\t int qp_attr_mask, struct ib_udata *udata);\n\tint (*query_qp)(struct ib_qp *qp, struct ib_qp_attr *qp_attr,\n\t\t\tint qp_attr_mask, struct ib_qp_init_attr *qp_init_attr);\n\tint (*destroy_qp)(struct ib_qp *qp, struct ib_udata *udata);\n\tint (*create_cq)(struct ib_cq *cq, const struct ib_cq_init_attr *attr,\n\t\t\t struct ib_udata *udata);\n\tint (*modify_cq)(struct ib_cq *cq, u16 cq_count, u16 cq_period);\n\tint (*destroy_cq)(struct ib_cq *cq, struct ib_udata *udata);\n\tint (*resize_cq)(struct ib_cq *cq, int cqe, struct ib_udata *udata);\n\tstruct ib_mr *(*get_dma_mr)(struct ib_pd *pd, int mr_access_flags);\n\tstruct ib_mr *(*reg_user_mr)(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t\t     u64 virt_addr, int mr_access_flags,\n\t\t\t\t     struct ib_udata *udata);\n\tstruct ib_mr *(*reg_user_mr_dmabuf)(struct ib_pd *pd, u64 offset,\n\t\t\t\t\t    u64 length, u64 virt_addr, int fd,\n\t\t\t\t\t    int mr_access_flags,\n\t\t\t\t\t    struct ib_udata *udata);\n\tstruct ib_mr *(*rereg_user_mr)(struct ib_mr *mr, int flags, u64 start,\n\t\t\t\t       u64 length, u64 virt_addr,\n\t\t\t\t       int mr_access_flags, struct ib_pd *pd,\n\t\t\t\t       struct ib_udata *udata);\n\tint (*dereg_mr)(struct ib_mr *mr, struct ib_udata *udata);\n\tstruct ib_mr *(*alloc_mr)(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t\t  u32 max_num_sg);\n\tstruct ib_mr *(*alloc_mr_integrity)(struct ib_pd *pd,\n\t\t\t\t\t    u32 max_num_data_sg,\n\t\t\t\t\t    u32 max_num_meta_sg);\n\tint (*advise_mr)(struct ib_pd *pd,\n\t\t\t enum ib_uverbs_advise_mr_advice advice, u32 flags,\n\t\t\t struct ib_sge *sg_list, u32 num_sge,\n\t\t\t struct uverbs_attr_bundle *attrs);\n\n\t \n\tint (*map_mr_sg)(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,\n\t\t\t unsigned int *sg_offset);\n\tint (*check_mr_status)(struct ib_mr *mr, u32 check_mask,\n\t\t\t       struct ib_mr_status *mr_status);\n\tint (*alloc_mw)(struct ib_mw *mw, struct ib_udata *udata);\n\tint (*dealloc_mw)(struct ib_mw *mw);\n\tint (*attach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);\n\tint (*detach_mcast)(struct ib_qp *qp, union ib_gid *gid, u16 lid);\n\tint (*alloc_xrcd)(struct ib_xrcd *xrcd, struct ib_udata *udata);\n\tint (*dealloc_xrcd)(struct ib_xrcd *xrcd, struct ib_udata *udata);\n\tstruct ib_flow *(*create_flow)(struct ib_qp *qp,\n\t\t\t\t       struct ib_flow_attr *flow_attr,\n\t\t\t\t       struct ib_udata *udata);\n\tint (*destroy_flow)(struct ib_flow *flow_id);\n\tint (*destroy_flow_action)(struct ib_flow_action *action);\n\tint (*set_vf_link_state)(struct ib_device *device, int vf, u32 port,\n\t\t\t\t int state);\n\tint (*get_vf_config)(struct ib_device *device, int vf, u32 port,\n\t\t\t     struct ifla_vf_info *ivf);\n\tint (*get_vf_stats)(struct ib_device *device, int vf, u32 port,\n\t\t\t    struct ifla_vf_stats *stats);\n\tint (*get_vf_guid)(struct ib_device *device, int vf, u32 port,\n\t\t\t    struct ifla_vf_guid *node_guid,\n\t\t\t    struct ifla_vf_guid *port_guid);\n\tint (*set_vf_guid)(struct ib_device *device, int vf, u32 port, u64 guid,\n\t\t\t   int type);\n\tstruct ib_wq *(*create_wq)(struct ib_pd *pd,\n\t\t\t\t   struct ib_wq_init_attr *init_attr,\n\t\t\t\t   struct ib_udata *udata);\n\tint (*destroy_wq)(struct ib_wq *wq, struct ib_udata *udata);\n\tint (*modify_wq)(struct ib_wq *wq, struct ib_wq_attr *attr,\n\t\t\t u32 wq_attr_mask, struct ib_udata *udata);\n\tint (*create_rwq_ind_table)(struct ib_rwq_ind_table *ib_rwq_ind_table,\n\t\t\t\t    struct ib_rwq_ind_table_init_attr *init_attr,\n\t\t\t\t    struct ib_udata *udata);\n\tint (*destroy_rwq_ind_table)(struct ib_rwq_ind_table *wq_ind_table);\n\tstruct ib_dm *(*alloc_dm)(struct ib_device *device,\n\t\t\t\t  struct ib_ucontext *context,\n\t\t\t\t  struct ib_dm_alloc_attr *attr,\n\t\t\t\t  struct uverbs_attr_bundle *attrs);\n\tint (*dealloc_dm)(struct ib_dm *dm, struct uverbs_attr_bundle *attrs);\n\tstruct ib_mr *(*reg_dm_mr)(struct ib_pd *pd, struct ib_dm *dm,\n\t\t\t\t   struct ib_dm_mr_attr *attr,\n\t\t\t\t   struct uverbs_attr_bundle *attrs);\n\tint (*create_counters)(struct ib_counters *counters,\n\t\t\t       struct uverbs_attr_bundle *attrs);\n\tint (*destroy_counters)(struct ib_counters *counters);\n\tint (*read_counters)(struct ib_counters *counters,\n\t\t\t     struct ib_counters_read_attr *counters_read_attr,\n\t\t\t     struct uverbs_attr_bundle *attrs);\n\tint (*map_mr_sg_pi)(struct ib_mr *mr, struct scatterlist *data_sg,\n\t\t\t    int data_sg_nents, unsigned int *data_sg_offset,\n\t\t\t    struct scatterlist *meta_sg, int meta_sg_nents,\n\t\t\t    unsigned int *meta_sg_offset);\n\n\t \n\tstruct rdma_hw_stats *(*alloc_hw_device_stats)(struct ib_device *device);\n\tstruct rdma_hw_stats *(*alloc_hw_port_stats)(struct ib_device *device,\n\t\t\t\t\t\t     u32 port_num);\n\t \n\tint (*get_hw_stats)(struct ib_device *device,\n\t\t\t    struct rdma_hw_stats *stats, u32 port, int index);\n\n\t \n\tint (*modify_hw_stat)(struct ib_device *device, u32 port,\n\t\t\t      unsigned int counter_index, bool enable);\n\t \n\tint (*fill_res_mr_entry)(struct sk_buff *msg, struct ib_mr *ibmr);\n\tint (*fill_res_mr_entry_raw)(struct sk_buff *msg, struct ib_mr *ibmr);\n\tint (*fill_res_cq_entry)(struct sk_buff *msg, struct ib_cq *ibcq);\n\tint (*fill_res_cq_entry_raw)(struct sk_buff *msg, struct ib_cq *ibcq);\n\tint (*fill_res_qp_entry)(struct sk_buff *msg, struct ib_qp *ibqp);\n\tint (*fill_res_qp_entry_raw)(struct sk_buff *msg, struct ib_qp *ibqp);\n\tint (*fill_res_cm_id_entry)(struct sk_buff *msg, struct rdma_cm_id *id);\n\n\t \n\t \n\tint (*enable_driver)(struct ib_device *dev);\n\t \n\tvoid (*dealloc_driver)(struct ib_device *dev);\n\n\t \n\tvoid (*iw_add_ref)(struct ib_qp *qp);\n\tvoid (*iw_rem_ref)(struct ib_qp *qp);\n\tstruct ib_qp *(*iw_get_qp)(struct ib_device *device, int qpn);\n\tint (*iw_connect)(struct iw_cm_id *cm_id,\n\t\t\t  struct iw_cm_conn_param *conn_param);\n\tint (*iw_accept)(struct iw_cm_id *cm_id,\n\t\t\t struct iw_cm_conn_param *conn_param);\n\tint (*iw_reject)(struct iw_cm_id *cm_id, const void *pdata,\n\t\t\t u8 pdata_len);\n\tint (*iw_create_listen)(struct iw_cm_id *cm_id, int backlog);\n\tint (*iw_destroy_listen)(struct iw_cm_id *cm_id);\n\t \n\tint (*counter_bind_qp)(struct rdma_counter *counter, struct ib_qp *qp);\n\t \n\tint (*counter_unbind_qp)(struct ib_qp *qp);\n\t \n\tint (*counter_dealloc)(struct rdma_counter *counter);\n\t \n\tstruct rdma_hw_stats *(*counter_alloc_stats)(\n\t\tstruct rdma_counter *counter);\n\t \n\tint (*counter_update_stats)(struct rdma_counter *counter);\n\n\t \n\tint (*fill_stat_mr_entry)(struct sk_buff *msg, struct ib_mr *ibmr);\n\n\t \n\tint (*query_ucontext)(struct ib_ucontext *context,\n\t\t\t      struct uverbs_attr_bundle *attrs);\n\n\t \n\tint (*get_numa_node)(struct ib_device *dev);\n\n\tDECLARE_RDMA_OBJ_SIZE(ib_ah);\n\tDECLARE_RDMA_OBJ_SIZE(ib_counters);\n\tDECLARE_RDMA_OBJ_SIZE(ib_cq);\n\tDECLARE_RDMA_OBJ_SIZE(ib_mw);\n\tDECLARE_RDMA_OBJ_SIZE(ib_pd);\n\tDECLARE_RDMA_OBJ_SIZE(ib_qp);\n\tDECLARE_RDMA_OBJ_SIZE(ib_rwq_ind_table);\n\tDECLARE_RDMA_OBJ_SIZE(ib_srq);\n\tDECLARE_RDMA_OBJ_SIZE(ib_ucontext);\n\tDECLARE_RDMA_OBJ_SIZE(ib_xrcd);\n};\n\nstruct ib_core_device {\n\t \n\tstruct device dev;\n\tpossible_net_t rdma_net;\n\tstruct kobject *ports_kobj;\n\tstruct list_head port_list;\n\tstruct ib_device *owner;  \n};\n\nstruct rdma_restrack_root;\nstruct ib_device {\n\t \n\tstruct device                *dma_device;\n\tstruct ib_device_ops\t     ops;\n\tchar                          name[IB_DEVICE_NAME_MAX];\n\tstruct rcu_head rcu_head;\n\n\tstruct list_head              event_handler_list;\n\t \n\tstruct rw_semaphore event_handler_rwsem;\n\n\t \n\tspinlock_t qp_open_list_lock;\n\n\tstruct rw_semaphore\t      client_data_rwsem;\n\tstruct xarray                 client_data;\n\tstruct mutex                  unregistration_lock;\n\n\t \n\trwlock_t cache_lock;\n\t \n\tstruct ib_port_data *port_data;\n\n\tint\t\t\t      num_comp_vectors;\n\n\tunion {\n\t\tstruct device\t\tdev;\n\t\tstruct ib_core_device\tcoredev;\n\t};\n\n\t \n\tconst struct attribute_group\t*groups[4];\n\n\tu64\t\t\t     uverbs_cmd_mask;\n\n\tchar\t\t\t     node_desc[IB_DEVICE_NODE_DESC_MAX];\n\t__be64\t\t\t     node_guid;\n\tu32\t\t\t     local_dma_lkey;\n\tu16                          is_switch:1;\n\t \n\tu16                          kverbs_provider:1;\n\t \n\tu16                          use_cq_dim:1;\n\tu8                           node_type;\n\tu32\t\t\t     phys_port_cnt;\n\tstruct ib_device_attr        attrs;\n\tstruct hw_stats_device_data *hw_stats_data;\n\n#ifdef CONFIG_CGROUP_RDMA\n\tstruct rdmacg_device         cg_device;\n#endif\n\n\tu32                          index;\n\n\tspinlock_t                   cq_pools_lock;\n\tstruct list_head             cq_pools[IB_POLL_LAST_POOL_TYPE + 1];\n\n\tstruct rdma_restrack_root *res;\n\n\tconst struct uapi_definition   *driver_def;\n\n\t \n\trefcount_t refcount;\n\tstruct completion unreg_completion;\n\tstruct work_struct unregistration_work;\n\n\tconst struct rdma_link_ops *link_ops;\n\n\t \n\tstruct mutex compat_devs_mutex;\n\t \n\tstruct xarray compat_devs;\n\n\t \n\tchar iw_ifname[IFNAMSIZ];\n\tu32 iw_driver_flags;\n\tu32 lag_flags;\n};\n\nstatic inline void *rdma_zalloc_obj(struct ib_device *dev, size_t size,\n\t\t\t\t    gfp_t gfp, bool is_numa_aware)\n{\n\tif (is_numa_aware && dev->ops.get_numa_node)\n\t\treturn kzalloc_node(size, gfp, dev->ops.get_numa_node(dev));\n\n\treturn kzalloc(size, gfp);\n}\n\nstruct ib_client_nl_info;\nstruct ib_client {\n\tconst char *name;\n\tint (*add)(struct ib_device *ibdev);\n\tvoid (*remove)(struct ib_device *, void *client_data);\n\tvoid (*rename)(struct ib_device *dev, void *client_data);\n\tint (*get_nl_info)(struct ib_device *ibdev, void *client_data,\n\t\t\t   struct ib_client_nl_info *res);\n\tint (*get_global_nl_info)(struct ib_client_nl_info *res);\n\n\t \n\tstruct net_device *(*get_net_dev_by_params)(\n\t\t\tstruct ib_device *dev,\n\t\t\tu32 port,\n\t\t\tu16 pkey,\n\t\t\tconst union ib_gid *gid,\n\t\t\tconst struct sockaddr *addr,\n\t\t\tvoid *client_data);\n\n\trefcount_t uses;\n\tstruct completion uses_zero;\n\tu32 client_id;\n\n\t \n\tu8 no_kverbs_req:1;\n};\n\n \nstruct ib_block_iter {\n\t \n\tstruct scatterlist *__sg;\t \n\tdma_addr_t __dma_addr;\t\t \n\tsize_t __sg_numblocks;\t\t \n\tunsigned int __sg_nents;\t \n\tunsigned int __sg_advance;\t \n\tunsigned int __pg_bit;\t\t \n};\n\nstruct ib_device *_ib_alloc_device(size_t size);\n#define ib_alloc_device(drv_struct, member)                                    \\\n\tcontainer_of(_ib_alloc_device(sizeof(struct drv_struct) +              \\\n\t\t\t\t      BUILD_BUG_ON_ZERO(offsetof(              \\\n\t\t\t\t\t      struct drv_struct, member))),    \\\n\t\t     struct drv_struct, member)\n\nvoid ib_dealloc_device(struct ib_device *device);\n\nvoid ib_get_device_fw_str(struct ib_device *device, char *str);\n\nint ib_register_device(struct ib_device *device, const char *name,\n\t\t       struct device *dma_device);\nvoid ib_unregister_device(struct ib_device *device);\nvoid ib_unregister_driver(enum rdma_driver_id driver_id);\nvoid ib_unregister_device_and_put(struct ib_device *device);\nvoid ib_unregister_device_queued(struct ib_device *ib_dev);\n\nint ib_register_client   (struct ib_client *client);\nvoid ib_unregister_client(struct ib_client *client);\n\nvoid __rdma_block_iter_start(struct ib_block_iter *biter,\n\t\t\t     struct scatterlist *sglist,\n\t\t\t     unsigned int nents,\n\t\t\t     unsigned long pgsz);\nbool __rdma_block_iter_next(struct ib_block_iter *biter);\n\n \nstatic inline dma_addr_t\nrdma_block_iter_dma_address(struct ib_block_iter *biter)\n{\n\treturn biter->__dma_addr & ~(BIT_ULL(biter->__pg_bit) - 1);\n}\n\n \n#define rdma_for_each_block(sglist, biter, nents, pgsz)\t\t\\\n\tfor (__rdma_block_iter_start(biter, sglist, nents,\t\\\n\t\t\t\t     pgsz);\t\t\t\\\n\t     __rdma_block_iter_next(biter);)\n\n \nstatic inline void *ib_get_client_data(struct ib_device *device,\n\t\t\t\t       struct ib_client *client)\n{\n\treturn xa_load(&device->client_data, client->client_id);\n}\nvoid  ib_set_client_data(struct ib_device *device, struct ib_client *client,\n\t\t\t void *data);\nvoid ib_set_device_ops(struct ib_device *device,\n\t\t       const struct ib_device_ops *ops);\n\nint rdma_user_mmap_io(struct ib_ucontext *ucontext, struct vm_area_struct *vma,\n\t\t      unsigned long pfn, unsigned long size, pgprot_t prot,\n\t\t      struct rdma_user_mmap_entry *entry);\nint rdma_user_mmap_entry_insert(struct ib_ucontext *ucontext,\n\t\t\t\tstruct rdma_user_mmap_entry *entry,\n\t\t\t\tsize_t length);\nint rdma_user_mmap_entry_insert_range(struct ib_ucontext *ucontext,\n\t\t\t\t      struct rdma_user_mmap_entry *entry,\n\t\t\t\t      size_t length, u32 min_pgoff,\n\t\t\t\t      u32 max_pgoff);\n\nstatic inline int\nrdma_user_mmap_entry_insert_exact(struct ib_ucontext *ucontext,\n\t\t\t\t  struct rdma_user_mmap_entry *entry,\n\t\t\t\t  size_t length, u32 pgoff)\n{\n\treturn rdma_user_mmap_entry_insert_range(ucontext, entry, length, pgoff,\n\t\t\t\t\t\t pgoff);\n}\n\nstruct rdma_user_mmap_entry *\nrdma_user_mmap_entry_get_pgoff(struct ib_ucontext *ucontext,\n\t\t\t       unsigned long pgoff);\nstruct rdma_user_mmap_entry *\nrdma_user_mmap_entry_get(struct ib_ucontext *ucontext,\n\t\t\t struct vm_area_struct *vma);\nvoid rdma_user_mmap_entry_put(struct rdma_user_mmap_entry *entry);\n\nvoid rdma_user_mmap_entry_remove(struct rdma_user_mmap_entry *entry);\n\nstatic inline int ib_copy_from_udata(void *dest, struct ib_udata *udata, size_t len)\n{\n\treturn copy_from_user(dest, udata->inbuf, len) ? -EFAULT : 0;\n}\n\nstatic inline int ib_copy_to_udata(struct ib_udata *udata, void *src, size_t len)\n{\n\treturn copy_to_user(udata->outbuf, src, len) ? -EFAULT : 0;\n}\n\nstatic inline bool ib_is_buffer_cleared(const void __user *p,\n\t\t\t\t\tsize_t len)\n{\n\tbool ret;\n\tu8 *buf;\n\n\tif (len > USHRT_MAX)\n\t\treturn false;\n\n\tbuf = memdup_user(p, len);\n\tif (IS_ERR(buf))\n\t\treturn false;\n\n\tret = !memchr_inv(buf, 0, len);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic inline bool ib_is_udata_cleared(struct ib_udata *udata,\n\t\t\t\t       size_t offset,\n\t\t\t\t       size_t len)\n{\n\treturn ib_is_buffer_cleared(udata->inbuf + offset, len);\n}\n\n \nbool ib_modify_qp_is_ok(enum ib_qp_state cur_state, enum ib_qp_state next_state,\n\t\t\tenum ib_qp_type type, enum ib_qp_attr_mask mask);\n\nvoid ib_register_event_handler(struct ib_event_handler *event_handler);\nvoid ib_unregister_event_handler(struct ib_event_handler *event_handler);\nvoid ib_dispatch_event(const struct ib_event *event);\n\nint ib_query_port(struct ib_device *device,\n\t\t  u32 port_num, struct ib_port_attr *port_attr);\n\nenum rdma_link_layer rdma_port_get_link_layer(struct ib_device *device,\n\t\t\t\t\t       u32 port_num);\n\n \nstatic inline bool rdma_cap_ib_switch(const struct ib_device *device)\n{\n\treturn device->is_switch;\n}\n\n \nstatic inline u32 rdma_start_port(const struct ib_device *device)\n{\n\treturn rdma_cap_ib_switch(device) ? 0 : 1;\n}\n\n \n#define rdma_for_each_port(device, iter)                                       \\\n\tfor (iter = rdma_start_port(device +\t\t\t\t       \\\n\t\t\t\t    BUILD_BUG_ON_ZERO(!__same_type(u32,\t       \\\n\t\t\t\t\t\t\t\t   iter)));    \\\n\t     iter <= rdma_end_port(device); iter++)\n\n \nstatic inline u32 rdma_end_port(const struct ib_device *device)\n{\n\treturn rdma_cap_ib_switch(device) ? 0 : device->phys_port_cnt;\n}\n\nstatic inline int rdma_is_port_valid(const struct ib_device *device,\n\t\t\t\t     unsigned int port)\n{\n\treturn (port >= rdma_start_port(device) &&\n\t\tport <= rdma_end_port(device));\n}\n\nstatic inline bool rdma_is_grh_required(const struct ib_device *device,\n\t\t\t\t\tu32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_PORT_IB_GRH_REQUIRED;\n}\n\nstatic inline bool rdma_protocol_ib(const struct ib_device *device,\n\t\t\t\t    u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_IB;\n}\n\nstatic inline bool rdma_protocol_roce(const struct ib_device *device,\n\t\t\t\t      u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       (RDMA_CORE_CAP_PROT_ROCE | RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP);\n}\n\nstatic inline bool rdma_protocol_roce_udp_encap(const struct ib_device *device,\n\t\t\t\t\t\tu32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_ROCE_UDP_ENCAP;\n}\n\nstatic inline bool rdma_protocol_roce_eth_encap(const struct ib_device *device,\n\t\t\t\t\t\tu32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_ROCE;\n}\n\nstatic inline bool rdma_protocol_iwarp(const struct ib_device *device,\n\t\t\t\t       u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_IWARP;\n}\n\nstatic inline bool rdma_ib_or_roce(const struct ib_device *device,\n\t\t\t\t   u32 port_num)\n{\n\treturn rdma_protocol_ib(device, port_num) ||\n\t\trdma_protocol_roce(device, port_num);\n}\n\nstatic inline bool rdma_protocol_raw_packet(const struct ib_device *device,\n\t\t\t\t\t    u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_RAW_PACKET;\n}\n\nstatic inline bool rdma_protocol_usnic(const struct ib_device *device,\n\t\t\t\t       u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_PROT_USNIC;\n}\n\n \nstatic inline bool rdma_cap_ib_mad(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_IB_MAD;\n}\n\n \nstatic inline bool rdma_cap_opa_mad(struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t\tRDMA_CORE_CAP_OPA_MAD;\n}\n\n \nstatic inline bool rdma_cap_ib_smi(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_IB_SMI;\n}\n\n \nstatic inline bool rdma_cap_ib_cm(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_IB_CM;\n}\n\n \nstatic inline bool rdma_cap_iw_cm(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_IW_CM;\n}\n\n \nstatic inline bool rdma_cap_ib_sa(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_IB_SA;\n}\n\n \nstatic inline bool rdma_cap_ib_mcast(const struct ib_device *device,\n\t\t\t\t     u32 port_num)\n{\n\treturn rdma_cap_ib_sa(device, port_num);\n}\n\n \nstatic inline bool rdma_cap_af_ib(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_AF_IB;\n}\n\n \nstatic inline bool rdma_cap_eth_ah(const struct ib_device *device, u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.core_cap_flags &\n\t       RDMA_CORE_CAP_ETH_AH;\n}\n\n \nstatic inline bool rdma_cap_opa_ah(struct ib_device *device, u32 port_num)\n{\n\treturn (device->port_data[port_num].immutable.core_cap_flags &\n\t\tRDMA_CORE_CAP_OPA_AH) == RDMA_CORE_CAP_OPA_AH;\n}\n\n \nstatic inline size_t rdma_max_mad_size(const struct ib_device *device,\n\t\t\t\t       u32 port_num)\n{\n\treturn device->port_data[port_num].immutable.max_mad_size;\n}\n\n \nstatic inline bool rdma_cap_roce_gid_table(const struct ib_device *device,\n\t\t\t\t\t   u32 port_num)\n{\n\treturn rdma_protocol_roce(device, port_num) &&\n\t\tdevice->ops.add_gid && device->ops.del_gid;\n}\n\n \nstatic inline bool rdma_cap_read_inv(struct ib_device *dev, u32 port_num)\n{\n\t \n\treturn rdma_protocol_iwarp(dev, port_num);\n}\n\n \nstatic inline bool rdma_core_cap_opa_port(struct ib_device *device,\n\t\t\t\t\t  u32 port_num)\n{\n\treturn (device->port_data[port_num].immutable.core_cap_flags &\n\t\tRDMA_CORE_PORT_INTEL_OPA) == RDMA_CORE_PORT_INTEL_OPA;\n}\n\n \nstatic inline int rdma_mtu_enum_to_int(struct ib_device *device, u32 port,\n\t\t\t\t       int mtu)\n{\n\tif (rdma_core_cap_opa_port(device, port))\n\t\treturn opa_mtu_enum_to_int((enum opa_mtu)mtu);\n\telse\n\t\treturn ib_mtu_enum_to_int((enum ib_mtu)mtu);\n}\n\n \nstatic inline int rdma_mtu_from_attr(struct ib_device *device, u32 port,\n\t\t\t\t     struct ib_port_attr *attr)\n{\n\tif (rdma_core_cap_opa_port(device, port))\n\t\treturn attr->phys_mtu;\n\telse\n\t\treturn ib_mtu_enum_to_int(attr->max_mtu);\n}\n\nint ib_set_vf_link_state(struct ib_device *device, int vf, u32 port,\n\t\t\t int state);\nint ib_get_vf_config(struct ib_device *device, int vf, u32 port,\n\t\t     struct ifla_vf_info *info);\nint ib_get_vf_stats(struct ib_device *device, int vf, u32 port,\n\t\t    struct ifla_vf_stats *stats);\nint ib_get_vf_guid(struct ib_device *device, int vf, u32 port,\n\t\t    struct ifla_vf_guid *node_guid,\n\t\t    struct ifla_vf_guid *port_guid);\nint ib_set_vf_guid(struct ib_device *device, int vf, u32 port, u64 guid,\n\t\t   int type);\n\nint ib_query_pkey(struct ib_device *device,\n\t\t  u32 port_num, u16 index, u16 *pkey);\n\nint ib_modify_device(struct ib_device *device,\n\t\t     int device_modify_mask,\n\t\t     struct ib_device_modify *device_modify);\n\nint ib_modify_port(struct ib_device *device,\n\t\t   u32 port_num, int port_modify_mask,\n\t\t   struct ib_port_modify *port_modify);\n\nint ib_find_gid(struct ib_device *device, union ib_gid *gid,\n\t\tu32 *port_num, u16 *index);\n\nint ib_find_pkey(struct ib_device *device,\n\t\t u32 port_num, u16 pkey, u16 *index);\n\nenum ib_pd_flags {\n\t \n\tIB_PD_UNSAFE_GLOBAL_RKEY\t= 0x01,\n};\n\nstruct ib_pd *__ib_alloc_pd(struct ib_device *device, unsigned int flags,\n\t\tconst char *caller);\n\n \n#define ib_alloc_pd(device, flags) \\\n\t__ib_alloc_pd((device), (flags), KBUILD_MODNAME)\n\nint ib_dealloc_pd_user(struct ib_pd *pd, struct ib_udata *udata);\n\n \nstatic inline void ib_dealloc_pd(struct ib_pd *pd)\n{\n\tint ret = ib_dealloc_pd_user(pd, NULL);\n\n\tWARN_ONCE(ret, \"Destroy of kernel PD shouldn't fail\");\n}\n\nenum rdma_create_ah_flags {\n\t \n\tRDMA_CREATE_AH_SLEEPABLE = BIT(0),\n};\n\n \nstruct ib_ah *rdma_create_ah(struct ib_pd *pd, struct rdma_ah_attr *ah_attr,\n\t\t\t     u32 flags);\n\n \nstruct ib_ah *rdma_create_user_ah(struct ib_pd *pd,\n\t\t\t\t  struct rdma_ah_attr *ah_attr,\n\t\t\t\t  struct ib_udata *udata);\n \nint ib_get_gids_from_rdma_hdr(const union rdma_network_hdr *hdr,\n\t\t\t      enum rdma_network_type net_type,\n\t\t\t      union ib_gid *sgid, union ib_gid *dgid);\n\n \nint ib_get_rdma_header_version(const union rdma_network_hdr *hdr);\n\n \nint ib_init_ah_attr_from_wc(struct ib_device *device, u32 port_num,\n\t\t\t    const struct ib_wc *wc, const struct ib_grh *grh,\n\t\t\t    struct rdma_ah_attr *ah_attr);\n\n \nstruct ib_ah *ib_create_ah_from_wc(struct ib_pd *pd, const struct ib_wc *wc,\n\t\t\t\t   const struct ib_grh *grh, u32 port_num);\n\n \nint rdma_modify_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);\n\n \nint rdma_query_ah(struct ib_ah *ah, struct rdma_ah_attr *ah_attr);\n\nenum rdma_destroy_ah_flags {\n\t \n\tRDMA_DESTROY_AH_SLEEPABLE = BIT(0),\n};\n\n \nint rdma_destroy_ah_user(struct ib_ah *ah, u32 flags, struct ib_udata *udata);\n\n \nstatic inline void rdma_destroy_ah(struct ib_ah *ah, u32 flags)\n{\n\tint ret = rdma_destroy_ah_user(ah, flags, NULL);\n\n\tWARN_ONCE(ret, \"Destroy of kernel AH shouldn't fail\");\n}\n\nstruct ib_srq *ib_create_srq_user(struct ib_pd *pd,\n\t\t\t\t  struct ib_srq_init_attr *srq_init_attr,\n\t\t\t\t  struct ib_usrq_object *uobject,\n\t\t\t\t  struct ib_udata *udata);\nstatic inline struct ib_srq *\nib_create_srq(struct ib_pd *pd, struct ib_srq_init_attr *srq_init_attr)\n{\n\tif (!pd->device->ops.create_srq)\n\t\treturn ERR_PTR(-EOPNOTSUPP);\n\n\treturn ib_create_srq_user(pd, srq_init_attr, NULL, NULL);\n}\n\n \nint ib_modify_srq(struct ib_srq *srq,\n\t\t  struct ib_srq_attr *srq_attr,\n\t\t  enum ib_srq_attr_mask srq_attr_mask);\n\n \nint ib_query_srq(struct ib_srq *srq,\n\t\t struct ib_srq_attr *srq_attr);\n\n \nint ib_destroy_srq_user(struct ib_srq *srq, struct ib_udata *udata);\n\n \nstatic inline void ib_destroy_srq(struct ib_srq *srq)\n{\n\tint ret = ib_destroy_srq_user(srq, NULL);\n\n\tWARN_ONCE(ret, \"Destroy of kernel SRQ shouldn't fail\");\n}\n\n \nstatic inline int ib_post_srq_recv(struct ib_srq *srq,\n\t\t\t\t   const struct ib_recv_wr *recv_wr,\n\t\t\t\t   const struct ib_recv_wr **bad_recv_wr)\n{\n\tconst struct ib_recv_wr *dummy;\n\n\treturn srq->device->ops.post_srq_recv(srq, recv_wr,\n\t\t\t\t\t      bad_recv_wr ? : &dummy);\n}\n\nstruct ib_qp *ib_create_qp_kernel(struct ib_pd *pd,\n\t\t\t\t  struct ib_qp_init_attr *qp_init_attr,\n\t\t\t\t  const char *caller);\n \nstatic inline struct ib_qp *ib_create_qp(struct ib_pd *pd,\n\t\t\t\t\t struct ib_qp_init_attr *init_attr)\n{\n\treturn ib_create_qp_kernel(pd, init_attr, KBUILD_MODNAME);\n}\n\n \nint ib_modify_qp_with_udata(struct ib_qp *qp,\n\t\t\t    struct ib_qp_attr *attr,\n\t\t\t    int attr_mask,\n\t\t\t    struct ib_udata *udata);\n\n \nint ib_modify_qp(struct ib_qp *qp,\n\t\t struct ib_qp_attr *qp_attr,\n\t\t int qp_attr_mask);\n\n \nint ib_query_qp(struct ib_qp *qp,\n\t\tstruct ib_qp_attr *qp_attr,\n\t\tint qp_attr_mask,\n\t\tstruct ib_qp_init_attr *qp_init_attr);\n\n \nint ib_destroy_qp_user(struct ib_qp *qp, struct ib_udata *udata);\n\n \nstatic inline int ib_destroy_qp(struct ib_qp *qp)\n{\n\treturn ib_destroy_qp_user(qp, NULL);\n}\n\n \nstruct ib_qp *ib_open_qp(struct ib_xrcd *xrcd,\n\t\t\t struct ib_qp_open_attr *qp_open_attr);\n\n \nint ib_close_qp(struct ib_qp *qp);\n\n \nstatic inline int ib_post_send(struct ib_qp *qp,\n\t\t\t       const struct ib_send_wr *send_wr,\n\t\t\t       const struct ib_send_wr **bad_send_wr)\n{\n\tconst struct ib_send_wr *dummy;\n\n\treturn qp->device->ops.post_send(qp, send_wr, bad_send_wr ? : &dummy);\n}\n\n \nstatic inline int ib_post_recv(struct ib_qp *qp,\n\t\t\t       const struct ib_recv_wr *recv_wr,\n\t\t\t       const struct ib_recv_wr **bad_recv_wr)\n{\n\tconst struct ib_recv_wr *dummy;\n\n\treturn qp->device->ops.post_recv(qp, recv_wr, bad_recv_wr ? : &dummy);\n}\n\nstruct ib_cq *__ib_alloc_cq(struct ib_device *dev, void *private, int nr_cqe,\n\t\t\t    int comp_vector, enum ib_poll_context poll_ctx,\n\t\t\t    const char *caller);\nstatic inline struct ib_cq *ib_alloc_cq(struct ib_device *dev, void *private,\n\t\t\t\t\tint nr_cqe, int comp_vector,\n\t\t\t\t\tenum ib_poll_context poll_ctx)\n{\n\treturn __ib_alloc_cq(dev, private, nr_cqe, comp_vector, poll_ctx,\n\t\t\t     KBUILD_MODNAME);\n}\n\nstruct ib_cq *__ib_alloc_cq_any(struct ib_device *dev, void *private,\n\t\t\t\tint nr_cqe, enum ib_poll_context poll_ctx,\n\t\t\t\tconst char *caller);\n\n \nstatic inline struct ib_cq *ib_alloc_cq_any(struct ib_device *dev,\n\t\t\t\t\t    void *private, int nr_cqe,\n\t\t\t\t\t    enum ib_poll_context poll_ctx)\n{\n\treturn __ib_alloc_cq_any(dev, private, nr_cqe, poll_ctx,\n\t\t\t\t KBUILD_MODNAME);\n}\n\nvoid ib_free_cq(struct ib_cq *cq);\nint ib_process_cq_direct(struct ib_cq *cq, int budget);\n\n \nstruct ib_cq *__ib_create_cq(struct ib_device *device,\n\t\t\t     ib_comp_handler comp_handler,\n\t\t\t     void (*event_handler)(struct ib_event *, void *),\n\t\t\t     void *cq_context,\n\t\t\t     const struct ib_cq_init_attr *cq_attr,\n\t\t\t     const char *caller);\n#define ib_create_cq(device, cmp_hndlr, evt_hndlr, cq_ctxt, cq_attr) \\\n\t__ib_create_cq((device), (cmp_hndlr), (evt_hndlr), (cq_ctxt), (cq_attr), KBUILD_MODNAME)\n\n \nint ib_resize_cq(struct ib_cq *cq, int cqe);\n\n \nint rdma_set_cq_moderation(struct ib_cq *cq, u16 cq_count, u16 cq_period);\n\n \nint ib_destroy_cq_user(struct ib_cq *cq, struct ib_udata *udata);\n\n \nstatic inline void ib_destroy_cq(struct ib_cq *cq)\n{\n\tint ret = ib_destroy_cq_user(cq, NULL);\n\n\tWARN_ONCE(ret, \"Destroy of kernel CQ shouldn't fail\");\n}\n\n \nstatic inline int ib_poll_cq(struct ib_cq *cq, int num_entries,\n\t\t\t     struct ib_wc *wc)\n{\n\treturn cq->device->ops.poll_cq(cq, num_entries, wc);\n}\n\n \nstatic inline int ib_req_notify_cq(struct ib_cq *cq,\n\t\t\t\t   enum ib_cq_notify_flags flags)\n{\n\treturn cq->device->ops.req_notify_cq(cq, flags);\n}\n\nstruct ib_cq *ib_cq_pool_get(struct ib_device *dev, unsigned int nr_cqe,\n\t\t\t     int comp_vector_hint,\n\t\t\t     enum ib_poll_context poll_ctx);\n\nvoid ib_cq_pool_put(struct ib_cq *cq, unsigned int nr_cqe);\n\n \nstatic inline bool ib_uses_virt_dma(struct ib_device *dev)\n{\n\treturn IS_ENABLED(CONFIG_INFINIBAND_VIRT_DMA) && !dev->dma_device;\n}\n\n \nstatic inline bool ib_dma_pci_p2p_dma_supported(struct ib_device *dev)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn false;\n\n\treturn dma_pci_p2pdma_supported(dev->dma_device);\n}\n\n \nstatic inline void *ib_virt_dma_to_ptr(u64 dma_addr)\n{\n\t \n\treturn (void *)(uintptr_t)dma_addr;\n}\n\n \nstatic inline struct page *ib_virt_dma_to_page(u64 dma_addr)\n{\n\treturn virt_to_page(ib_virt_dma_to_ptr(dma_addr));\n}\n\n \nstatic inline int ib_dma_mapping_error(struct ib_device *dev, u64 dma_addr)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn 0;\n\treturn dma_mapping_error(dev->dma_device, dma_addr);\n}\n\n \nstatic inline u64 ib_dma_map_single(struct ib_device *dev,\n\t\t\t\t    void *cpu_addr, size_t size,\n\t\t\t\t    enum dma_data_direction direction)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn (uintptr_t)cpu_addr;\n\treturn dma_map_single(dev->dma_device, cpu_addr, size, direction);\n}\n\n \nstatic inline void ib_dma_unmap_single(struct ib_device *dev,\n\t\t\t\t       u64 addr, size_t size,\n\t\t\t\t       enum dma_data_direction direction)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_unmap_single(dev->dma_device, addr, size, direction);\n}\n\n \nstatic inline u64 ib_dma_map_page(struct ib_device *dev,\n\t\t\t\t  struct page *page,\n\t\t\t\t  unsigned long offset,\n\t\t\t\t  size_t size,\n\t\t\t\t\t enum dma_data_direction direction)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn (uintptr_t)(page_address(page) + offset);\n\treturn dma_map_page(dev->dma_device, page, offset, size, direction);\n}\n\n \nstatic inline void ib_dma_unmap_page(struct ib_device *dev,\n\t\t\t\t     u64 addr, size_t size,\n\t\t\t\t     enum dma_data_direction direction)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_unmap_page(dev->dma_device, addr, size, direction);\n}\n\nint ib_dma_virt_map_sg(struct ib_device *dev, struct scatterlist *sg, int nents);\nstatic inline int ib_dma_map_sg_attrs(struct ib_device *dev,\n\t\t\t\t      struct scatterlist *sg, int nents,\n\t\t\t\t      enum dma_data_direction direction,\n\t\t\t\t      unsigned long dma_attrs)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn ib_dma_virt_map_sg(dev, sg, nents);\n\treturn dma_map_sg_attrs(dev->dma_device, sg, nents, direction,\n\t\t\t\tdma_attrs);\n}\n\nstatic inline void ib_dma_unmap_sg_attrs(struct ib_device *dev,\n\t\t\t\t\t struct scatterlist *sg, int nents,\n\t\t\t\t\t enum dma_data_direction direction,\n\t\t\t\t\t unsigned long dma_attrs)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_unmap_sg_attrs(dev->dma_device, sg, nents, direction,\n\t\t\t\t   dma_attrs);\n}\n\n \nstatic inline int ib_dma_map_sgtable_attrs(struct ib_device *dev,\n\t\t\t\t\t   struct sg_table *sgt,\n\t\t\t\t\t   enum dma_data_direction direction,\n\t\t\t\t\t   unsigned long dma_attrs)\n{\n\tint nents;\n\n\tif (ib_uses_virt_dma(dev)) {\n\t\tnents = ib_dma_virt_map_sg(dev, sgt->sgl, sgt->orig_nents);\n\t\tif (!nents)\n\t\t\treturn -EIO;\n\t\tsgt->nents = nents;\n\t\treturn 0;\n\t}\n\treturn dma_map_sgtable(dev->dma_device, sgt, direction, dma_attrs);\n}\n\nstatic inline void ib_dma_unmap_sgtable_attrs(struct ib_device *dev,\n\t\t\t\t\t      struct sg_table *sgt,\n\t\t\t\t\t      enum dma_data_direction direction,\n\t\t\t\t\t      unsigned long dma_attrs)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_unmap_sgtable(dev->dma_device, sgt, direction, dma_attrs);\n}\n\n \nstatic inline int ib_dma_map_sg(struct ib_device *dev,\n\t\t\t\tstruct scatterlist *sg, int nents,\n\t\t\t\tenum dma_data_direction direction)\n{\n\treturn ib_dma_map_sg_attrs(dev, sg, nents, direction, 0);\n}\n\n \nstatic inline void ib_dma_unmap_sg(struct ib_device *dev,\n\t\t\t\t   struct scatterlist *sg, int nents,\n\t\t\t\t   enum dma_data_direction direction)\n{\n\tib_dma_unmap_sg_attrs(dev, sg, nents, direction, 0);\n}\n\n \nstatic inline unsigned int ib_dma_max_seg_size(struct ib_device *dev)\n{\n\tif (ib_uses_virt_dma(dev))\n\t\treturn UINT_MAX;\n\treturn dma_get_max_seg_size(dev->dma_device);\n}\n\n \nstatic inline void ib_dma_sync_single_for_cpu(struct ib_device *dev,\n\t\t\t\t\t      u64 addr,\n\t\t\t\t\t      size_t size,\n\t\t\t\t\t      enum dma_data_direction dir)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_sync_single_for_cpu(dev->dma_device, addr, size, dir);\n}\n\n \nstatic inline void ib_dma_sync_single_for_device(struct ib_device *dev,\n\t\t\t\t\t\t u64 addr,\n\t\t\t\t\t\t size_t size,\n\t\t\t\t\t\t enum dma_data_direction dir)\n{\n\tif (!ib_uses_virt_dma(dev))\n\t\tdma_sync_single_for_device(dev->dma_device, addr, size, dir);\n}\n\n \nstruct ib_mr *ib_reg_user_mr(struct ib_pd *pd, u64 start, u64 length,\n\t\t\t     u64 virt_addr, int mr_access_flags);\n\n \nint ib_advise_mr(struct ib_pd *pd, enum ib_uverbs_advise_mr_advice advice,\n\t\t u32 flags, struct ib_sge *sg_list, u32 num_sge);\n \nint ib_dereg_mr_user(struct ib_mr *mr, struct ib_udata *udata);\n\n \nstatic inline int ib_dereg_mr(struct ib_mr *mr)\n{\n\treturn ib_dereg_mr_user(mr, NULL);\n}\n\nstruct ib_mr *ib_alloc_mr(struct ib_pd *pd, enum ib_mr_type mr_type,\n\t\t\t  u32 max_num_sg);\n\nstruct ib_mr *ib_alloc_mr_integrity(struct ib_pd *pd,\n\t\t\t\t    u32 max_num_data_sg,\n\t\t\t\t    u32 max_num_meta_sg);\n\n \nstatic inline void ib_update_fast_reg_key(struct ib_mr *mr, u8 newkey)\n{\n\tmr->lkey = (mr->lkey & 0xffffff00) | newkey;\n\tmr->rkey = (mr->rkey & 0xffffff00) | newkey;\n}\n\n \nstatic inline u32 ib_inc_rkey(u32 rkey)\n{\n\tconst u32 mask = 0x000000ff;\n\treturn ((rkey + 1) & mask) | (rkey & ~mask);\n}\n\n \nint ib_attach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);\n\n \nint ib_detach_mcast(struct ib_qp *qp, union ib_gid *gid, u16 lid);\n\nstruct ib_xrcd *ib_alloc_xrcd_user(struct ib_device *device,\n\t\t\t\t   struct inode *inode, struct ib_udata *udata);\nint ib_dealloc_xrcd_user(struct ib_xrcd *xrcd, struct ib_udata *udata);\n\nstatic inline int ib_check_mr_access(struct ib_device *ib_dev,\n\t\t\t\t     unsigned int flags)\n{\n\tu64 device_cap = ib_dev->attrs.device_cap_flags;\n\n\t \n\tif (flags & (IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_REMOTE_WRITE) &&\n\t    !(flags & IB_ACCESS_LOCAL_WRITE))\n\t\treturn -EINVAL;\n\n\tif (flags & ~IB_ACCESS_SUPPORTED)\n\t\treturn -EINVAL;\n\n\tif (flags & IB_ACCESS_ON_DEMAND &&\n\t    !(ib_dev->attrs.kernel_cap_flags & IBK_ON_DEMAND_PAGING))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((flags & IB_ACCESS_FLUSH_GLOBAL &&\n\t    !(device_cap & IB_DEVICE_FLUSH_GLOBAL)) ||\n\t    (flags & IB_ACCESS_FLUSH_PERSISTENT &&\n\t    !(device_cap & IB_DEVICE_FLUSH_PERSISTENT)))\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n}\n\nstatic inline bool ib_access_writable(int access_flags)\n{\n\t \n\treturn access_flags &\n\t\t(IB_ACCESS_LOCAL_WRITE   | IB_ACCESS_REMOTE_WRITE |\n\t\t IB_ACCESS_REMOTE_ATOMIC | IB_ACCESS_MW_BIND);\n}\n\n \nint ib_check_mr_status(struct ib_mr *mr, u32 check_mask,\n\t\t       struct ib_mr_status *mr_status);\n\n \nstatic inline bool ib_device_try_get(struct ib_device *dev)\n{\n\treturn refcount_inc_not_zero(&dev->refcount);\n}\n\nvoid ib_device_put(struct ib_device *device);\nstruct ib_device *ib_device_get_by_netdev(struct net_device *ndev,\n\t\t\t\t\t  enum rdma_driver_id driver_id);\nstruct ib_device *ib_device_get_by_name(const char *name,\n\t\t\t\t\tenum rdma_driver_id driver_id);\nstruct net_device *ib_get_net_dev_by_params(struct ib_device *dev, u32 port,\n\t\t\t\t\t    u16 pkey, const union ib_gid *gid,\n\t\t\t\t\t    const struct sockaddr *addr);\nint ib_device_set_netdev(struct ib_device *ib_dev, struct net_device *ndev,\n\t\t\t unsigned int port);\nstruct ib_wq *ib_create_wq(struct ib_pd *pd,\n\t\t\t   struct ib_wq_init_attr *init_attr);\nint ib_destroy_wq_user(struct ib_wq *wq, struct ib_udata *udata);\n\nint ib_map_mr_sg(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,\n\t\t unsigned int *sg_offset, unsigned int page_size);\nint ib_map_mr_sg_pi(struct ib_mr *mr, struct scatterlist *data_sg,\n\t\t    int data_sg_nents, unsigned int *data_sg_offset,\n\t\t    struct scatterlist *meta_sg, int meta_sg_nents,\n\t\t    unsigned int *meta_sg_offset, unsigned int page_size);\n\nstatic inline int\nib_map_mr_sg_zbva(struct ib_mr *mr, struct scatterlist *sg, int sg_nents,\n\t\t  unsigned int *sg_offset, unsigned int page_size)\n{\n\tint n;\n\n\tn = ib_map_mr_sg(mr, sg, sg_nents, sg_offset, page_size);\n\tmr->iova = 0;\n\n\treturn n;\n}\n\nint ib_sg_to_pages(struct ib_mr *mr, struct scatterlist *sgl, int sg_nents,\n\t\tunsigned int *sg_offset, int (*set_page)(struct ib_mr *, u64));\n\nvoid ib_drain_rq(struct ib_qp *qp);\nvoid ib_drain_sq(struct ib_qp *qp);\nvoid ib_drain_qp(struct ib_qp *qp);\n\nint ib_get_eth_speed(struct ib_device *dev, u32 port_num, u16 *speed,\n\t\t     u8 *width);\n\nstatic inline u8 *rdma_ah_retrieve_dmac(struct rdma_ah_attr *attr)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_ROCE)\n\t\treturn attr->roce.dmac;\n\treturn NULL;\n}\n\nstatic inline void rdma_ah_set_dlid(struct rdma_ah_attr *attr, u32 dlid)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_IB)\n\t\tattr->ib.dlid = (u16)dlid;\n\telse if (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\tattr->opa.dlid = dlid;\n}\n\nstatic inline u32 rdma_ah_get_dlid(const struct rdma_ah_attr *attr)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_IB)\n\t\treturn attr->ib.dlid;\n\telse if (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\treturn attr->opa.dlid;\n\treturn 0;\n}\n\nstatic inline void rdma_ah_set_sl(struct rdma_ah_attr *attr, u8 sl)\n{\n\tattr->sl = sl;\n}\n\nstatic inline u8 rdma_ah_get_sl(const struct rdma_ah_attr *attr)\n{\n\treturn attr->sl;\n}\n\nstatic inline void rdma_ah_set_path_bits(struct rdma_ah_attr *attr,\n\t\t\t\t\t u8 src_path_bits)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_IB)\n\t\tattr->ib.src_path_bits = src_path_bits;\n\telse if (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\tattr->opa.src_path_bits = src_path_bits;\n}\n\nstatic inline u8 rdma_ah_get_path_bits(const struct rdma_ah_attr *attr)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_IB)\n\t\treturn attr->ib.src_path_bits;\n\telse if (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\treturn attr->opa.src_path_bits;\n\treturn 0;\n}\n\nstatic inline void rdma_ah_set_make_grd(struct rdma_ah_attr *attr,\n\t\t\t\t\tbool make_grd)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\tattr->opa.make_grd = make_grd;\n}\n\nstatic inline bool rdma_ah_get_make_grd(const struct rdma_ah_attr *attr)\n{\n\tif (attr->type == RDMA_AH_ATTR_TYPE_OPA)\n\t\treturn attr->opa.make_grd;\n\treturn false;\n}\n\nstatic inline void rdma_ah_set_port_num(struct rdma_ah_attr *attr, u32 port_num)\n{\n\tattr->port_num = port_num;\n}\n\nstatic inline u32 rdma_ah_get_port_num(const struct rdma_ah_attr *attr)\n{\n\treturn attr->port_num;\n}\n\nstatic inline void rdma_ah_set_static_rate(struct rdma_ah_attr *attr,\n\t\t\t\t\t   u8 static_rate)\n{\n\tattr->static_rate = static_rate;\n}\n\nstatic inline u8 rdma_ah_get_static_rate(const struct rdma_ah_attr *attr)\n{\n\treturn attr->static_rate;\n}\n\nstatic inline void rdma_ah_set_ah_flags(struct rdma_ah_attr *attr,\n\t\t\t\t\tenum ib_ah_flags flag)\n{\n\tattr->ah_flags = flag;\n}\n\nstatic inline enum ib_ah_flags\n\t\trdma_ah_get_ah_flags(const struct rdma_ah_attr *attr)\n{\n\treturn attr->ah_flags;\n}\n\nstatic inline const struct ib_global_route\n\t\t*rdma_ah_read_grh(const struct rdma_ah_attr *attr)\n{\n\treturn &attr->grh;\n}\n\n \nstatic inline struct ib_global_route\n\t\t*rdma_ah_retrieve_grh(struct rdma_ah_attr *attr)\n{\n\treturn &attr->grh;\n}\n\nstatic inline void rdma_ah_set_dgid_raw(struct rdma_ah_attr *attr, void *dgid)\n{\n\tstruct ib_global_route *grh = rdma_ah_retrieve_grh(attr);\n\n\tmemcpy(grh->dgid.raw, dgid, sizeof(grh->dgid));\n}\n\nstatic inline void rdma_ah_set_subnet_prefix(struct rdma_ah_attr *attr,\n\t\t\t\t\t     __be64 prefix)\n{\n\tstruct ib_global_route *grh = rdma_ah_retrieve_grh(attr);\n\n\tgrh->dgid.global.subnet_prefix = prefix;\n}\n\nstatic inline void rdma_ah_set_interface_id(struct rdma_ah_attr *attr,\n\t\t\t\t\t    __be64 if_id)\n{\n\tstruct ib_global_route *grh = rdma_ah_retrieve_grh(attr);\n\n\tgrh->dgid.global.interface_id = if_id;\n}\n\nstatic inline void rdma_ah_set_grh(struct rdma_ah_attr *attr,\n\t\t\t\t   union ib_gid *dgid, u32 flow_label,\n\t\t\t\t   u8 sgid_index, u8 hop_limit,\n\t\t\t\t   u8 traffic_class)\n{\n\tstruct ib_global_route *grh = rdma_ah_retrieve_grh(attr);\n\n\tattr->ah_flags = IB_AH_GRH;\n\tif (dgid)\n\t\tgrh->dgid = *dgid;\n\tgrh->flow_label = flow_label;\n\tgrh->sgid_index = sgid_index;\n\tgrh->hop_limit = hop_limit;\n\tgrh->traffic_class = traffic_class;\n\tgrh->sgid_attr = NULL;\n}\n\nvoid rdma_destroy_ah_attr(struct rdma_ah_attr *ah_attr);\nvoid rdma_move_grh_sgid_attr(struct rdma_ah_attr *attr, union ib_gid *dgid,\n\t\t\t     u32 flow_label, u8 hop_limit, u8 traffic_class,\n\t\t\t     const struct ib_gid_attr *sgid_attr);\nvoid rdma_copy_ah_attr(struct rdma_ah_attr *dest,\n\t\t       const struct rdma_ah_attr *src);\nvoid rdma_replace_ah_attr(struct rdma_ah_attr *old,\n\t\t\t  const struct rdma_ah_attr *new);\nvoid rdma_move_ah_attr(struct rdma_ah_attr *dest, struct rdma_ah_attr *src);\n\n \nstatic inline enum rdma_ah_attr_type rdma_ah_find_type(struct ib_device *dev,\n\t\t\t\t\t\t       u32 port_num)\n{\n\tif (rdma_protocol_roce(dev, port_num))\n\t\treturn RDMA_AH_ATTR_TYPE_ROCE;\n\tif (rdma_protocol_ib(dev, port_num)) {\n\t\tif (rdma_cap_opa_ah(dev, port_num))\n\t\t\treturn RDMA_AH_ATTR_TYPE_OPA;\n\t\treturn RDMA_AH_ATTR_TYPE_IB;\n\t}\n\n\treturn RDMA_AH_ATTR_TYPE_UNDEFINED;\n}\n\n \nstatic inline u16 ib_lid_cpu16(u32 lid)\n{\n\tWARN_ON_ONCE(lid & 0xFFFF0000);\n\treturn (u16)lid;\n}\n\n \nstatic inline __be16 ib_lid_be16(u32 lid)\n{\n\tWARN_ON_ONCE(lid & 0xFFFF0000);\n\treturn cpu_to_be16((u16)lid);\n}\n\n \nstatic inline const struct cpumask *\nib_get_vector_affinity(struct ib_device *device, int comp_vector)\n{\n\tif (comp_vector < 0 || comp_vector >= device->num_comp_vectors ||\n\t    !device->ops.get_vector_affinity)\n\t\treturn NULL;\n\n\treturn device->ops.get_vector_affinity(device, comp_vector);\n\n}\n\n \nvoid rdma_roce_rescan_device(struct ib_device *ibdev);\n\nstruct ib_ucontext *ib_uverbs_get_ucontext_file(struct ib_uverbs_file *ufile);\n\nint uverbs_destroy_def_handler(struct uverbs_attr_bundle *attrs);\n\nstruct net_device *rdma_alloc_netdev(struct ib_device *device, u32 port_num,\n\t\t\t\t     enum rdma_netdev_t type, const char *name,\n\t\t\t\t     unsigned char name_assign_type,\n\t\t\t\t     void (*setup)(struct net_device *));\n\nint rdma_init_netdev(struct ib_device *device, u32 port_num,\n\t\t     enum rdma_netdev_t type, const char *name,\n\t\t     unsigned char name_assign_type,\n\t\t     void (*setup)(struct net_device *),\n\t\t     struct net_device *netdev);\n\n \nstatic inline struct ib_device *rdma_device_to_ibdev(struct device *device)\n{\n\tstruct ib_core_device *coredev =\n\t\tcontainer_of(device, struct ib_core_device, dev);\n\n\treturn coredev->owner;\n}\n\n \nstatic inline int ibdev_to_node(struct ib_device *ibdev)\n{\n\tstruct device *parent = ibdev->dev.parent;\n\n\tif (!parent)\n\t\treturn NUMA_NO_NODE;\n\treturn dev_to_node(parent);\n}\n\n \n#define rdma_device_to_drv_device(dev, drv_dev_struct, ibdev_member)           \\\n\tcontainer_of(rdma_device_to_ibdev(dev), drv_dev_struct, ibdev_member)\n\nbool rdma_dev_access_netns(const struct ib_device *device,\n\t\t\t   const struct net *net);\n\n#define IB_ROCE_UDP_ENCAP_VALID_PORT_MIN (0xC000)\n#define IB_ROCE_UDP_ENCAP_VALID_PORT_MAX (0xFFFF)\n#define IB_GRH_FLOWLABEL_MASK (0x000FFFFF)\n\n \nstatic inline u16 rdma_flow_label_to_udp_sport(u32 fl)\n{\n\tu32 fl_low = fl & 0x03fff, fl_high = fl & 0xFC000;\n\n\tfl_low ^= fl_high >> 14;\n\treturn (u16)(fl_low | IB_ROCE_UDP_ENCAP_VALID_PORT_MIN);\n}\n\n \nstatic inline u32 rdma_calc_flow_label(u32 lqpn, u32 rqpn)\n{\n\tu64 v = (u64)lqpn * rqpn;\n\n\tv ^= v >> 20;\n\tv ^= v >> 40;\n\n\treturn (u32)(v & IB_GRH_FLOWLABEL_MASK);\n}\n\n \nstatic inline u16 rdma_get_udp_sport(u32 fl, u32 lqpn, u32 rqpn)\n{\n\tif (!fl)\n\t\tfl = rdma_calc_flow_label(lqpn, rqpn);\n\n\treturn rdma_flow_label_to_udp_sport(fl);\n}\n\nconst struct ib_port_immutable*\nib_port_immutable_read(struct ib_device *dev, unsigned int port);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}