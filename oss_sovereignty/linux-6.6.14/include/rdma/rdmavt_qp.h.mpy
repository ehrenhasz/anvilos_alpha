{
  "module_name": "rdmavt_qp.h",
  "hash_id": "1467993be531fcf004914b3308f11945a20e2b3e35d41f84c8b62ca355a63ee2",
  "original_prompt": "Ingested from linux-6.6.14/include/rdma/rdmavt_qp.h",
  "human_readable_source": " \n \n\n#ifndef DEF_RDMAVT_INCQP_H\n#define DEF_RDMAVT_INCQP_H\n\n#include <rdma/rdma_vt.h>\n#include <rdma/ib_pack.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdmavt_cq.h>\n#include <rdma/rvt-abi.h>\n \n#define RVT_R_WRID_VALID        0\n#define RVT_R_REWIND_SGE        1\n\n \n#define RVT_R_REUSE_SGE 0x01\n#define RVT_R_RDMAR_SEQ 0x02\n#define RVT_R_RSP_NAK   0x04\n#define RVT_R_RSP_SEND  0x08\n#define RVT_R_COMM_EST  0x10\n\n \n#define RVT_KDETH_QP_PREFIX       0x80\n#define RVT_KDETH_QP_SUFFIX       0xffff\n#define RVT_KDETH_QP_PREFIX_MASK  0x00ff0000\n#define RVT_KDETH_QP_PREFIX_SHIFT 16\n#define RVT_KDETH_QP_BASE         (u32)(RVT_KDETH_QP_PREFIX << \\\n\t\t\t\t\tRVT_KDETH_QP_PREFIX_SHIFT)\n#define RVT_KDETH_QP_MAX          (u32)(RVT_KDETH_QP_BASE + RVT_KDETH_QP_SUFFIX)\n\n \n#define RVT_AIP_QP_PREFIX       0x81\n#define RVT_AIP_QP_SUFFIX       0xffff\n#define RVT_AIP_QP_PREFIX_MASK  0x00ff0000\n#define RVT_AIP_QP_PREFIX_SHIFT 16\n#define RVT_AIP_QP_BASE         (u32)(RVT_AIP_QP_PREFIX << \\\n\t\t\t\t      RVT_AIP_QP_PREFIX_SHIFT)\n#define RVT_AIP_QPN_MAX         BIT(RVT_AIP_QP_PREFIX_SHIFT)\n#define RVT_AIP_QP_MAX          (u32)(RVT_AIP_QP_BASE + RVT_AIP_QPN_MAX - 1)\n\n \n#define RVT_S_SIGNAL_REQ_WR\t0x0001\n#define RVT_S_BUSY\t\t0x0002\n#define RVT_S_TIMER\t\t0x0004\n#define RVT_S_RESP_PENDING\t0x0008\n#define RVT_S_ACK_PENDING\t0x0010\n#define RVT_S_WAIT_FENCE\t0x0020\n#define RVT_S_WAIT_RDMAR\t0x0040\n#define RVT_S_WAIT_RNR\t\t0x0080\n#define RVT_S_WAIT_SSN_CREDIT\t0x0100\n#define RVT_S_WAIT_DMA\t\t0x0200\n#define RVT_S_WAIT_PIO\t\t0x0400\n#define RVT_S_WAIT_TX\t\t0x0800\n#define RVT_S_WAIT_DMA_DESC\t0x1000\n#define RVT_S_WAIT_KMEM\t\t0x2000\n#define RVT_S_WAIT_PSN\t\t0x4000\n#define RVT_S_WAIT_ACK\t\t0x8000\n#define RVT_S_SEND_ONE\t\t0x10000\n#define RVT_S_UNLIMITED_CREDIT\t0x20000\n#define RVT_S_ECN\t\t0x40000\n#define RVT_S_MAX_BIT_MASK\t0x800000\n\n \n\n \n#define RVT_S_ANY_WAIT_IO \\\n\t(RVT_S_WAIT_PIO | RVT_S_WAIT_TX | \\\n\t RVT_S_WAIT_DMA_DESC | RVT_S_WAIT_KMEM)\n\n \n#define RVT_S_ANY_WAIT_SEND (RVT_S_WAIT_FENCE | RVT_S_WAIT_RDMAR | \\\n\tRVT_S_WAIT_RNR | RVT_S_WAIT_SSN_CREDIT | RVT_S_WAIT_DMA | \\\n\tRVT_S_WAIT_PSN | RVT_S_WAIT_ACK)\n\n#define RVT_S_ANY_WAIT (RVT_S_ANY_WAIT_IO | RVT_S_ANY_WAIT_SEND)\n\n \n#define RVT_OPCODE_QP_MASK 0xE0\n\n \n#define RVT_POST_SEND_OK                0x01\n#define RVT_POST_RECV_OK                0x02\n#define RVT_PROCESS_RECV_OK             0x04\n#define RVT_PROCESS_SEND_OK             0x08\n#define RVT_PROCESS_NEXT_SEND_OK        0x10\n#define RVT_FLUSH_SEND\t\t\t0x20\n#define RVT_FLUSH_RECV\t\t\t0x40\n#define RVT_PROCESS_OR_FLUSH_SEND \\\n\t(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND)\n#define RVT_SEND_OR_FLUSH_OR_RECV_OK \\\n\t(RVT_PROCESS_SEND_OK | RVT_FLUSH_SEND | RVT_PROCESS_RECV_OK)\n\n \n#define RVT_SEND_RESERVE_USED           IB_SEND_RESERVED_START\n#define RVT_SEND_COMPLETION_ONLY\t(IB_SEND_RESERVED_START << 1)\n\n \nstruct rvt_ud_wr {\n\tstruct ib_ud_wr wr;\n\tstruct rdma_ah_attr *attr;\n};\n\n \nstruct rvt_swqe {\n\tunion {\n\t\tstruct ib_send_wr wr;    \n\t\tstruct rvt_ud_wr ud_wr;\n\t\tstruct ib_reg_wr reg_wr;\n\t\tstruct ib_rdma_wr rdma_wr;\n\t\tstruct ib_atomic_wr atomic_wr;\n\t};\n\tu32 psn;                 \n\tu32 lpsn;                \n\tu32 ssn;                 \n\tu32 length;              \n\tvoid *priv;              \n\tstruct rvt_sge sg_list[];\n};\n\n \nstruct rvt_krwq {\n\tspinlock_t p_lock;\t \n\tu32 head;                \n\n\t \n\tspinlock_t c_lock ____cacheline_aligned_in_smp;\n\tu32 tail;                \n\tu32 count;\t\t \n\tstruct rvt_rwqe *curr_wq;\n\tstruct rvt_rwqe wq[];\n};\n\n \nstatic inline struct rvt_ah *rvt_get_swqe_ah(struct rvt_swqe *swqe)\n{\n\treturn ibah_to_rvtah(swqe->ud_wr.wr.ah);\n}\n\n \nstatic inline struct rdma_ah_attr *rvt_get_swqe_ah_attr(struct rvt_swqe *swqe)\n{\n\treturn swqe->ud_wr.attr;\n}\n\n \nstatic inline u32 rvt_get_swqe_remote_qpn(struct rvt_swqe *swqe)\n{\n\treturn swqe->ud_wr.wr.remote_qpn;\n}\n\n \nstatic inline u32 rvt_get_swqe_remote_qkey(struct rvt_swqe *swqe)\n{\n\treturn swqe->ud_wr.wr.remote_qkey;\n}\n\n \nstatic inline u16 rvt_get_swqe_pkey_index(struct rvt_swqe *swqe)\n{\n\treturn swqe->ud_wr.wr.pkey_index;\n}\n\nstruct rvt_rq {\n\tstruct rvt_rwq *wq;\n\tstruct rvt_krwq *kwq;\n\tu32 size;                \n\tu8 max_sge;\n\t \n\tspinlock_t lock ____cacheline_aligned_in_smp;\n};\n\n \n\nstatic inline u32 rvt_get_rq_count(struct rvt_rq *rq, u32 head, u32 tail)\n{\n\tu32 count = head - tail;\n\n\tif ((s32)count < 0)\n\t\tcount += rq->size;\n\treturn count;\n}\n\n \nstruct rvt_ack_entry {\n\tstruct rvt_sge rdma_sge;\n\tu64 atomic_data;\n\tu32 psn;\n\tu32 lpsn;\n\tu8 opcode;\n\tu8 sent;\n\tvoid *priv;\n};\n\n#define\tRC_QP_SCALING_INTERVAL\t5\n\n#define RVT_OPERATION_PRIV        0x00000001\n#define RVT_OPERATION_ATOMIC      0x00000002\n#define RVT_OPERATION_ATOMIC_SGE  0x00000004\n#define RVT_OPERATION_LOCAL       0x00000008\n#define RVT_OPERATION_USE_RESERVE 0x00000010\n#define RVT_OPERATION_IGN_RNR_CNT 0x00000020\n\n#define RVT_OPERATION_MAX (IB_WR_RESERVED10 + 1)\n\n \n\nstruct rvt_operation_params {\n\tsize_t length;\n\tu32 qpt_support;\n\tu32 flags;\n};\n\n \nstruct rvt_qp {\n\tstruct ib_qp ibqp;\n\tvoid *priv;  \n\t \n\tstruct rdma_ah_attr remote_ah_attr;\n\tstruct rdma_ah_attr alt_ah_attr;\n\tstruct rvt_qp __rcu *next;            \n\tstruct rvt_swqe *s_wq;   \n\tstruct rvt_mmap_info *ip;\n\n\tunsigned long timeout_jiffies;   \n\n\tint srate_mbps;\t\t \n\tpid_t pid;\t\t \n\tu32 remote_qpn;\n\tu32 qkey;                \n\tu32 s_size;              \n\n\tu16 pmtu;\t\t \n\tu8 log_pmtu;\t\t \n\tu8 state;                \n\tu8 allowed_ops;\t\t \n\tu8 qp_access_flags;\n\tu8 alt_timeout;          \n\tu8 timeout;              \n\tu8 s_srate;\n\tu8 s_mig_state;\n\tu8 port_num;\n\tu8 s_pkey_index;         \n\tu8 s_alt_pkey_index;     \n\tu8 r_max_rd_atomic;      \n\tu8 s_max_rd_atomic;      \n\tu8 s_retry_cnt;          \n\tu8 s_rnr_retry_cnt;\n\tu8 r_min_rnr_timer;      \n\tu8 s_max_sge;            \n\tu8 s_draining;\n\n\t \n\tatomic_t refcount ____cacheline_aligned_in_smp;\n\twait_queue_head_t wait;\n\n\tstruct rvt_ack_entry *s_ack_queue;\n\tstruct rvt_sge_state s_rdma_read_sge;\n\n\tspinlock_t r_lock ____cacheline_aligned_in_smp;       \n\tu32 r_psn;               \n\tunsigned long r_aflags;\n\tu64 r_wr_id;             \n\tu32 r_ack_psn;           \n\tu32 r_len;               \n\tu32 r_rcv_len;           \n\tu32 r_msn;               \n\n\tu8 r_state;              \n\tu8 r_flags;\n\tu8 r_head_ack_queue;     \n\tu8 r_adefered;           \n\n\tstruct list_head rspwait;        \n\n\tstruct rvt_sge_state r_sge;      \n\tstruct rvt_rq r_rq;              \n\n\t \n\tspinlock_t s_hlock ____cacheline_aligned_in_smp;\n\tu32 s_head;              \n\tu32 s_next_psn;          \n\tu32 s_avail;             \n\tu32 s_ssn;               \n\tatomic_t s_reserved_used;  \n\n\tspinlock_t s_lock ____cacheline_aligned_in_smp;\n\tu32 s_flags;\n\tstruct rvt_sge_state *s_cur_sge;\n\tstruct rvt_swqe *s_wqe;\n\tstruct rvt_sge_state s_sge;      \n\tstruct rvt_mregion *s_rdma_mr;\n\tu32 s_len;               \n\tu32 s_rdma_read_len;     \n\tu32 s_last_psn;          \n\tu32 s_sending_psn;       \n\tu32 s_sending_hpsn;      \n\tu32 s_psn;               \n\tu32 s_ack_rdma_psn;      \n\tu32 s_ack_psn;           \n\tu32 s_tail;              \n\tu32 s_cur;               \n\tu32 s_acked;             \n\tu32 s_last;              \n\tu32 s_lsn;               \n\tu32 s_ahgpsn;            \n\tu16 s_cur_size;          \n\tu16 s_rdma_ack_cnt;\n\tu8 s_hdrwords;          \n\ts8 s_ahgidx;\n\tu8 s_state;              \n\tu8 s_ack_state;          \n\tu8 s_nak_state;          \n\tu8 r_nak_state;          \n\tu8 s_retry;              \n\tu8 s_rnr_retry;          \n\tu8 s_num_rd_atomic;      \n\tu8 s_tail_ack_queue;     \n\tu8 s_acked_ack_queue;    \n\n\tstruct rvt_sge_state s_ack_rdma_sge;\n\tstruct timer_list s_timer;\n\tstruct hrtimer s_rnr_timer;\n\n\tatomic_t local_ops_pending;  \n\n\t \n\tstruct rvt_sge *r_sg_list  \n\t\t____cacheline_aligned_in_smp;\n};\n\nstruct rvt_srq {\n\tstruct ib_srq ibsrq;\n\tstruct rvt_rq rq;\n\tstruct rvt_mmap_info *ip;\n\t \n\tu32 limit;\n};\n\nstatic inline struct rvt_srq *ibsrq_to_rvtsrq(struct ib_srq *ibsrq)\n{\n\treturn container_of(ibsrq, struct rvt_srq, ibsrq);\n}\n\nstatic inline struct rvt_qp *ibqp_to_rvtqp(struct ib_qp *ibqp)\n{\n\treturn container_of(ibqp, struct rvt_qp, ibqp);\n}\n\n#define RVT_QPN_MAX                 BIT(24)\n#define RVT_QPNMAP_ENTRIES          (RVT_QPN_MAX / PAGE_SIZE / BITS_PER_BYTE)\n#define RVT_BITS_PER_PAGE           (PAGE_SIZE * BITS_PER_BYTE)\n#define RVT_BITS_PER_PAGE_MASK      (RVT_BITS_PER_PAGE - 1)\n#define RVT_QPN_MASK\t\t    IB_QPN_MASK\n\n \nstruct rvt_qpn_map {\n\tvoid *page;\n};\n\nstruct rvt_qpn_table {\n\tspinlock_t lock;  \n\tunsigned flags;          \n\tu32 last;                \n\tu32 nmaps;               \n\tu16 limit;\n\tu8  incr;\n\t \n\tstruct rvt_qpn_map map[RVT_QPNMAP_ENTRIES];\n};\n\nstruct rvt_qp_ibdev {\n\tu32 qp_table_size;\n\tu32 qp_table_bits;\n\tstruct rvt_qp __rcu **qp_table;\n\tspinlock_t qpt_lock;  \n\tstruct rvt_qpn_table qpn_table;\n};\n\n \nstruct rvt_mcast_qp {\n\tstruct list_head list;\n\tstruct rvt_qp *qp;\n};\n\nstruct rvt_mcast_addr {\n\tunion ib_gid mgid;\n\tu16 lid;\n};\n\nstruct rvt_mcast {\n\tstruct rb_node rb_node;\n\tstruct rvt_mcast_addr mcast_addr;\n\tstruct list_head qp_list;\n\twait_queue_head_t wait;\n\tatomic_t refcount;\n\tint n_attached;\n};\n\n \nstatic inline struct rvt_swqe *rvt_get_swqe_ptr(struct rvt_qp *qp,\n\t\t\t\t\t\tunsigned n)\n{\n\treturn (struct rvt_swqe *)((char *)qp->s_wq +\n\t\t\t\t     (sizeof(struct rvt_swqe) +\n\t\t\t\t      qp->s_max_sge *\n\t\t\t\t      sizeof(struct rvt_sge)) * n);\n}\n\n \nstatic inline struct rvt_rwqe *rvt_get_rwqe_ptr(struct rvt_rq *rq, unsigned n)\n{\n\treturn (struct rvt_rwqe *)\n\t\t((char *)rq->kwq->curr_wq +\n\t\t (sizeof(struct rvt_rwqe) +\n\t\t  rq->max_sge * sizeof(struct ib_sge)) * n);\n}\n\n \nstatic inline bool rvt_is_user_qp(struct rvt_qp *qp)\n{\n\treturn !!qp->pid;\n}\n\n \nstatic inline void rvt_get_qp(struct rvt_qp *qp)\n{\n\tatomic_inc(&qp->refcount);\n}\n\n \nstatic inline void rvt_put_qp(struct rvt_qp *qp)\n{\n\tif (qp && atomic_dec_and_test(&qp->refcount))\n\t\twake_up(&qp->wait);\n}\n\n \nstatic inline void rvt_put_swqe(struct rvt_swqe *wqe)\n{\n\tint i;\n\n\tfor (i = 0; i < wqe->wr.num_sge; i++) {\n\t\tstruct rvt_sge *sge = &wqe->sg_list[i];\n\n\t\trvt_put_mr(sge->mr);\n\t}\n}\n\n \nstatic inline void rvt_qp_wqe_reserve(\n\tstruct rvt_qp *qp,\n\tstruct rvt_swqe *wqe)\n{\n\tatomic_inc(&qp->s_reserved_used);\n}\n\n \nstatic inline void rvt_qp_wqe_unreserve(struct rvt_qp *qp, int flags)\n{\n\tif (unlikely(flags & RVT_SEND_RESERVE_USED)) {\n\t\tatomic_dec(&qp->s_reserved_used);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t}\n}\n\nextern const enum ib_wc_opcode ib_rvt_wc_opcode[];\n\n \nstatic inline int rvt_cmp_msn(u32 a, u32 b)\n{\n\treturn (((int)a) - ((int)b)) << 8;\n}\n\n__be32 rvt_compute_aeth(struct rvt_qp *qp);\n\nvoid rvt_get_credit(struct rvt_qp *qp, u32 aeth);\n\nu32 rvt_restart_sge(struct rvt_sge_state *ss, struct rvt_swqe *wqe, u32 len);\n\n \nstatic inline u32 rvt_div_round_up_mtu(struct rvt_qp *qp, u32 len)\n{\n\treturn (len + qp->pmtu - 1) >> qp->log_pmtu;\n}\n\n \nstatic inline u32 rvt_div_mtu(struct rvt_qp *qp, u32 len)\n{\n\treturn len >> qp->log_pmtu;\n}\n\n \nstatic inline unsigned long rvt_timeout_to_jiffies(u8 timeout)\n{\n\tif (timeout > 31)\n\t\ttimeout = 31;\n\n\treturn usecs_to_jiffies(1U << timeout) * 4096UL / 1000UL;\n}\n\n \nstatic inline struct rvt_qp *rvt_lookup_qpn(struct rvt_dev_info *rdi,\n\t\t\t\t\t    struct rvt_ibport *rvp,\n\t\t\t\t\t    u32 qpn) __must_hold(RCU)\n{\n\tstruct rvt_qp *qp = NULL;\n\n\tif (unlikely(qpn <= 1)) {\n\t\tqp = rcu_dereference(rvp->qp[qpn]);\n\t} else {\n\t\tu32 n = hash_32(qpn, rdi->qp_dev->qp_table_bits);\n\n\t\tfor (qp = rcu_dereference(rdi->qp_dev->qp_table[n]); qp;\n\t\t\tqp = rcu_dereference(qp->next))\n\t\t\tif (qp->ibqp.qp_num == qpn)\n\t\t\t\tbreak;\n\t}\n\treturn qp;\n}\n\n \nstatic inline void rvt_mod_retry_timer_ext(struct rvt_qp *qp, u8 shift)\n{\n\tstruct ib_qp *ibqp = &qp->ibqp;\n\tstruct rvt_dev_info *rdi = ib_to_rvt(ibqp->device);\n\n\tlockdep_assert_held(&qp->s_lock);\n\tqp->s_flags |= RVT_S_TIMER;\n\t \n\tmod_timer(&qp->s_timer, jiffies + rdi->busy_jiffies +\n\t\t  (qp->timeout_jiffies << shift));\n}\n\nstatic inline void rvt_mod_retry_timer(struct rvt_qp *qp)\n{\n\treturn rvt_mod_retry_timer_ext(qp, 0);\n}\n\n \nstatic inline void rvt_put_qp_swqe(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\trvt_put_swqe(wqe);\n\tif (qp->allowed_ops == IB_OPCODE_UD)\n\t\trdma_destroy_ah_attr(wqe->ud_wr.attr);\n}\n\n \nstatic inline u32\nrvt_qp_swqe_incr(struct rvt_qp *qp, u32 val)\n{\n\tif (++val >= qp->s_size)\n\t\tval = 0;\n\treturn val;\n}\n\nint rvt_error_qp(struct rvt_qp *qp, enum ib_wc_status err);\n\n \nstatic inline void rvt_recv_cq(struct rvt_qp *qp, struct ib_wc *wc,\n\t\t\t       bool solicited)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(qp->ibqp.recv_cq);\n\n\tif (unlikely(!rvt_cq_enter(cq, wc, solicited)))\n\t\trvt_error_qp(qp, IB_WC_LOC_QP_OP_ERR);\n}\n\n \nstatic inline void rvt_send_cq(struct rvt_qp *qp, struct ib_wc *wc,\n\t\t\t       bool solicited)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(qp->ibqp.send_cq);\n\n\tif (unlikely(!rvt_cq_enter(cq, wc, solicited)))\n\t\trvt_error_qp(qp, IB_WC_LOC_QP_OP_ERR);\n}\n\n \nstatic inline u32\nrvt_qp_complete_swqe(struct rvt_qp *qp,\n\t\t     struct rvt_swqe *wqe,\n\t\t     enum ib_wc_opcode opcode,\n\t\t     enum ib_wc_status status)\n{\n\tbool need_completion;\n\tu64 wr_id;\n\tu32 byte_len, last;\n\tint flags = wqe->wr.send_flags;\n\n\trvt_qp_wqe_unreserve(qp, flags);\n\trvt_put_qp_swqe(qp, wqe);\n\n\tneed_completion =\n\t\t!(flags & RVT_SEND_RESERVE_USED) &&\n\t\t(!(qp->s_flags & RVT_S_SIGNAL_REQ_WR) ||\n\t\t(flags & IB_SEND_SIGNALED) ||\n\t\tstatus != IB_WC_SUCCESS);\n\tif (need_completion) {\n\t\twr_id = wqe->wr.wr_id;\n\t\tbyte_len = wqe->length;\n\t\t \n\t}\n\tlast = rvt_qp_swqe_incr(qp, qp->s_last);\n\t \n\tsmp_store_release(&qp->s_last, last);\n\tif (need_completion) {\n\t\tstruct ib_wc w = {\n\t\t\t.wr_id = wr_id,\n\t\t\t.status = status,\n\t\t\t.opcode = opcode,\n\t\t\t.qp = &qp->ibqp,\n\t\t\t.byte_len = byte_len,\n\t\t};\n\t\trvt_send_cq(qp, &w, status != IB_WC_SUCCESS);\n\t}\n\treturn last;\n}\n\nextern const int  ib_rvt_state_ops[];\n\nstruct rvt_dev_info;\nint rvt_get_rwqe(struct rvt_qp *qp, bool wr_id_only);\nvoid rvt_comm_est(struct rvt_qp *qp);\nvoid rvt_rc_error(struct rvt_qp *qp, enum ib_wc_status err);\nunsigned long rvt_rnr_tbl_to_usec(u32 index);\nenum hrtimer_restart rvt_rc_rnr_retry(struct hrtimer *t);\nvoid rvt_add_rnr_timer(struct rvt_qp *qp, u32 aeth);\nvoid rvt_del_timers_sync(struct rvt_qp *qp);\nvoid rvt_stop_rc_timers(struct rvt_qp *qp);\nvoid rvt_add_retry_timer_ext(struct rvt_qp *qp, u8 shift);\nstatic inline void rvt_add_retry_timer(struct rvt_qp *qp)\n{\n\trvt_add_retry_timer_ext(qp, 0);\n}\n\nvoid rvt_copy_sge(struct rvt_qp *qp, struct rvt_sge_state *ss,\n\t\t  void *data, u32 length,\n\t\t  bool release, bool copy_last);\nvoid rvt_send_complete(struct rvt_qp *qp, struct rvt_swqe *wqe,\n\t\t       enum ib_wc_status status);\nvoid rvt_ruc_loopback(struct rvt_qp *qp);\n\n \nstruct rvt_qp_iter {\n\tstruct rvt_qp *qp;\n\t \n\tstruct rvt_dev_info *rdi;\n\t \n\tvoid (*cb)(struct rvt_qp *qp, u64 v);\n\t \n\tu64 v;\n\t \n\tint specials;\n\t \n\tint n;\n};\n\n \nstatic inline u32 ib_cq_tail(struct ib_cq *send_cq)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(send_cq);\n\n\treturn ibcq_to_rvtcq(send_cq)->ip ?\n\t       RDMA_READ_UAPI_ATOMIC(cq->queue->tail) :\n\t       ibcq_to_rvtcq(send_cq)->kqueue->tail;\n}\n\n \nstatic inline u32 ib_cq_head(struct ib_cq *send_cq)\n{\n\tstruct rvt_cq *cq = ibcq_to_rvtcq(send_cq);\n\n\treturn ibcq_to_rvtcq(send_cq)->ip ?\n\t       RDMA_READ_UAPI_ATOMIC(cq->queue->head) :\n\t       ibcq_to_rvtcq(send_cq)->kqueue->head;\n}\n\n \nstatic inline void rvt_free_rq(struct rvt_rq *rq)\n{\n\tkvfree(rq->kwq);\n\trq->kwq = NULL;\n\tvfree(rq->wq);\n\trq->wq = NULL;\n}\n\n \nstatic inline struct rvt_ibport *rvt_to_iport(struct rvt_qp *qp)\n{\n\tstruct rvt_dev_info *rdi = ib_to_rvt(qp->ibqp.device);\n\n\treturn rdi->ports[qp->port_num - 1];\n}\n\n \nstatic inline bool rvt_rc_credit_avail(struct rvt_qp *qp, struct rvt_swqe *wqe)\n{\n\tlockdep_assert_held(&qp->s_lock);\n\tif (!(qp->s_flags & RVT_S_UNLIMITED_CREDIT) &&\n\t    rvt_cmp_msn(wqe->ssn, qp->s_lsn + 1) > 0) {\n\t\tstruct rvt_ibport *rvp = rvt_to_iport(qp);\n\n\t\tqp->s_flags |= RVT_S_WAIT_SSN_CREDIT;\n\t\trvp->n_rc_crwaits++;\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstruct rvt_qp_iter *rvt_qp_iter_init(struct rvt_dev_info *rdi,\n\t\t\t\t     u64 v,\n\t\t\t\t     void (*cb)(struct rvt_qp *qp, u64 v));\nint rvt_qp_iter_next(struct rvt_qp_iter *iter);\nvoid rvt_qp_iter(struct rvt_dev_info *rdi,\n\t\t u64 v,\n\t\t void (*cb)(struct rvt_qp *qp, u64 v));\nvoid rvt_qp_mr_clean(struct rvt_qp *qp, u32 lkey);\n#endif           \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}