{
  "module_name": "drm_gpuva_mgr.h",
  "hash_id": "0e9fa5b7b3c820ecbfcf2018103fcce41e6d2c5268461cb8b6b1aab8a1654ac6",
  "original_prompt": "Ingested from linux-6.6.14/include/drm/drm_gpuva_mgr.h",
  "human_readable_source": " \n\n#ifndef __DRM_GPUVA_MGR_H__\n#define __DRM_GPUVA_MGR_H__\n\n \n\n#include <linux/list.h>\n#include <linux/rbtree.h>\n#include <linux/types.h>\n\n#include <drm/drm_gem.h>\n\nstruct drm_gpuva_manager;\nstruct drm_gpuva_fn_ops;\n\n \nenum drm_gpuva_flags {\n\t \n\tDRM_GPUVA_INVALIDATED = (1 << 0),\n\n\t \n\tDRM_GPUVA_SPARSE = (1 << 1),\n\n\t \n\tDRM_GPUVA_USERBITS = (1 << 2),\n};\n\n \nstruct drm_gpuva {\n\t \n\tstruct drm_gpuva_manager *mgr;\n\n\t \n\tenum drm_gpuva_flags flags;\n\n\t \n\tstruct {\n\t\t \n\t\tu64 addr;\n\n\t\t \n\t\tu64 range;\n\t} va;\n\n\t \n\tstruct {\n\t\t \n\t\tu64 offset;\n\n\t\t \n\t\tstruct drm_gem_object *obj;\n\n\t\t \n\t\tstruct list_head entry;\n\t} gem;\n\n\t \n\tstruct {\n\t\t \n\t\tstruct rb_node node;\n\n\t\t \n\t\tstruct list_head entry;\n\n\t\t \n\t\tu64 __subtree_last;\n\t} rb;\n};\n\nint drm_gpuva_insert(struct drm_gpuva_manager *mgr, struct drm_gpuva *va);\nvoid drm_gpuva_remove(struct drm_gpuva *va);\n\nvoid drm_gpuva_link(struct drm_gpuva *va);\nvoid drm_gpuva_unlink(struct drm_gpuva *va);\n\nstruct drm_gpuva *drm_gpuva_find(struct drm_gpuva_manager *mgr,\n\t\t\t\t u64 addr, u64 range);\nstruct drm_gpuva *drm_gpuva_find_first(struct drm_gpuva_manager *mgr,\n\t\t\t\t       u64 addr, u64 range);\nstruct drm_gpuva *drm_gpuva_find_prev(struct drm_gpuva_manager *mgr, u64 start);\nstruct drm_gpuva *drm_gpuva_find_next(struct drm_gpuva_manager *mgr, u64 end);\n\nbool drm_gpuva_interval_empty(struct drm_gpuva_manager *mgr, u64 addr, u64 range);\n\nstatic inline void drm_gpuva_init(struct drm_gpuva *va, u64 addr, u64 range,\n\t\t\t\t  struct drm_gem_object *obj, u64 offset)\n{\n\tva->va.addr = addr;\n\tva->va.range = range;\n\tva->gem.obj = obj;\n\tva->gem.offset = offset;\n}\n\n \nstatic inline void drm_gpuva_invalidate(struct drm_gpuva *va, bool invalidate)\n{\n\tif (invalidate)\n\t\tva->flags |= DRM_GPUVA_INVALIDATED;\n\telse\n\t\tva->flags &= ~DRM_GPUVA_INVALIDATED;\n}\n\n \nstatic inline bool drm_gpuva_invalidated(struct drm_gpuva *va)\n{\n\treturn va->flags & DRM_GPUVA_INVALIDATED;\n}\n\n \nstruct drm_gpuva_manager {\n\t \n\tconst char *name;\n\n\t \n\tu64 mm_start;\n\n\t \n\tu64 mm_range;\n\n\t \n\tstruct {\n\t\t \n\t\tstruct rb_root_cached tree;\n\n\t\t \n\t\tstruct list_head list;\n\t} rb;\n\n\t \n\tstruct drm_gpuva kernel_alloc_node;\n\n\t \n\tconst struct drm_gpuva_fn_ops *ops;\n};\n\nvoid drm_gpuva_manager_init(struct drm_gpuva_manager *mgr,\n\t\t\t    const char *name,\n\t\t\t    u64 start_offset, u64 range,\n\t\t\t    u64 reserve_offset, u64 reserve_range,\n\t\t\t    const struct drm_gpuva_fn_ops *ops);\nvoid drm_gpuva_manager_destroy(struct drm_gpuva_manager *mgr);\n\nstatic inline struct drm_gpuva *\n__drm_gpuva_next(struct drm_gpuva *va)\n{\n\tif (va && !list_is_last(&va->rb.entry, &va->mgr->rb.list))\n\t\treturn list_next_entry(va, rb.entry);\n\n\treturn NULL;\n}\n\n \n#define drm_gpuva_for_each_va_range(va__, mgr__, start__, end__) \\\n\tfor (va__ = drm_gpuva_find_first((mgr__), (start__), (end__) - (start__)); \\\n\t     va__ && (va__->va.addr < (end__)); \\\n\t     va__ = __drm_gpuva_next(va__))\n\n \n#define drm_gpuva_for_each_va_range_safe(va__, next__, mgr__, start__, end__) \\\n\tfor (va__ = drm_gpuva_find_first((mgr__), (start__), (end__) - (start__)), \\\n\t     next__ = __drm_gpuva_next(va__); \\\n\t     va__ && (va__->va.addr < (end__)); \\\n\t     va__ = next__, next__ = __drm_gpuva_next(va__))\n\n \n#define drm_gpuva_for_each_va(va__, mgr__) \\\n\tlist_for_each_entry(va__, &(mgr__)->rb.list, rb.entry)\n\n \n#define drm_gpuva_for_each_va_safe(va__, next__, mgr__) \\\n\tlist_for_each_entry_safe(va__, next__, &(mgr__)->rb.list, rb.entry)\n\n \nenum drm_gpuva_op_type {\n\t \n\tDRM_GPUVA_OP_MAP,\n\n\t \n\tDRM_GPUVA_OP_REMAP,\n\n\t \n\tDRM_GPUVA_OP_UNMAP,\n\n\t \n\tDRM_GPUVA_OP_PREFETCH,\n};\n\n \nstruct drm_gpuva_op_map {\n\t \n\tstruct {\n\t\t \n\t\tu64 addr;\n\n\t\t \n\t\tu64 range;\n\t} va;\n\n\t \n\tstruct {\n\t\t \n\t\tu64 offset;\n\n\t\t \n\t\tstruct drm_gem_object *obj;\n\t} gem;\n};\n\n \nstruct drm_gpuva_op_unmap {\n\t \n\tstruct drm_gpuva *va;\n\n\t \n\tbool keep;\n};\n\n \nstruct drm_gpuva_op_remap {\n\t \n\tstruct drm_gpuva_op_map *prev;\n\n\t \n\tstruct drm_gpuva_op_map *next;\n\n\t \n\tstruct drm_gpuva_op_unmap *unmap;\n};\n\n \nstruct drm_gpuva_op_prefetch {\n\t \n\tstruct drm_gpuva *va;\n};\n\n \nstruct drm_gpuva_op {\n\t \n\tstruct list_head entry;\n\n\t \n\tenum drm_gpuva_op_type op;\n\n\tunion {\n\t\t \n\t\tstruct drm_gpuva_op_map map;\n\n\t\t \n\t\tstruct drm_gpuva_op_remap remap;\n\n\t\t \n\t\tstruct drm_gpuva_op_unmap unmap;\n\n\t\t \n\t\tstruct drm_gpuva_op_prefetch prefetch;\n\t};\n};\n\n \nstruct drm_gpuva_ops {\n\t \n\tstruct list_head list;\n};\n\n \n#define drm_gpuva_for_each_op(op, ops) list_for_each_entry(op, &(ops)->list, entry)\n\n \n#define drm_gpuva_for_each_op_safe(op, next, ops) \\\n\tlist_for_each_entry_safe(op, next, &(ops)->list, entry)\n\n \n#define drm_gpuva_for_each_op_from_reverse(op, ops) \\\n\tlist_for_each_entry_from_reverse(op, &(ops)->list, entry)\n\n \n#define drm_gpuva_first_op(ops) \\\n\tlist_first_entry(&(ops)->list, struct drm_gpuva_op, entry)\n\n \n#define drm_gpuva_last_op(ops) \\\n\tlist_last_entry(&(ops)->list, struct drm_gpuva_op, entry)\n\n \n#define drm_gpuva_prev_op(op) list_prev_entry(op, entry)\n\n \n#define drm_gpuva_next_op(op) list_next_entry(op, entry)\n\nstruct drm_gpuva_ops *\ndrm_gpuva_sm_map_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t    u64 addr, u64 range,\n\t\t\t    struct drm_gem_object *obj, u64 offset);\nstruct drm_gpuva_ops *\ndrm_gpuva_sm_unmap_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t      u64 addr, u64 range);\n\nstruct drm_gpuva_ops *\ndrm_gpuva_prefetch_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t\t u64 addr, u64 range);\n\nstruct drm_gpuva_ops *\ndrm_gpuva_gem_unmap_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t       struct drm_gem_object *obj);\n\nvoid drm_gpuva_ops_free(struct drm_gpuva_manager *mgr,\n\t\t\tstruct drm_gpuva_ops *ops);\n\nstatic inline void drm_gpuva_init_from_op(struct drm_gpuva *va,\n\t\t\t\t\t  struct drm_gpuva_op_map *op)\n{\n\tdrm_gpuva_init(va, op->va.addr, op->va.range,\n\t\t       op->gem.obj, op->gem.offset);\n}\n\n \nstruct drm_gpuva_fn_ops {\n\t \n\tstruct drm_gpuva_op *(*op_alloc)(void);\n\n\t \n\tvoid (*op_free)(struct drm_gpuva_op *op);\n\n\t \n\tint (*sm_step_map)(struct drm_gpuva_op *op, void *priv);\n\n\t \n\tint (*sm_step_remap)(struct drm_gpuva_op *op, void *priv);\n\n\t \n\tint (*sm_step_unmap)(struct drm_gpuva_op *op, void *priv);\n};\n\nint drm_gpuva_sm_map(struct drm_gpuva_manager *mgr, void *priv,\n\t\t     u64 addr, u64 range,\n\t\t     struct drm_gem_object *obj, u64 offset);\n\nint drm_gpuva_sm_unmap(struct drm_gpuva_manager *mgr, void *priv,\n\t\t       u64 addr, u64 range);\n\nvoid drm_gpuva_map(struct drm_gpuva_manager *mgr,\n\t\t   struct drm_gpuva *va,\n\t\t   struct drm_gpuva_op_map *op);\n\nvoid drm_gpuva_remap(struct drm_gpuva *prev,\n\t\t     struct drm_gpuva *next,\n\t\t     struct drm_gpuva_op_remap *op);\n\nvoid drm_gpuva_unmap(struct drm_gpuva_op_unmap *op);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}