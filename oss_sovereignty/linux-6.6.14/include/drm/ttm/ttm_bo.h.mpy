{
  "module_name": "ttm_bo.h",
  "hash_id": "38b4f499eff290591bf5627a281bfe48a0dbf173046122783cf4d0b42163ab24",
  "original_prompt": "Ingested from linux-6.6.14/include/drm/ttm/ttm_bo.h",
  "human_readable_source": " \n \n\n#ifndef _TTM_BO_API_H_\n#define _TTM_BO_API_H_\n\n#include <drm/drm_gem.h>\n\n#include <linux/kref.h>\n#include <linux/list.h>\n\n#include \"ttm_device.h\"\n\n \n#define TTM_BO_VM_NUM_PREFAULT 16\n\nstruct iosys_map;\n\nstruct ttm_global;\nstruct ttm_device;\nstruct ttm_placement;\nstruct ttm_place;\nstruct ttm_resource;\nstruct ttm_resource_manager;\nstruct ttm_tt;\n\n \nenum ttm_bo_type {\n\tttm_bo_type_device,\n\tttm_bo_type_kernel,\n\tttm_bo_type_sg\n};\n\n \nstruct ttm_buffer_object {\n\tstruct drm_gem_object base;\n\n\t \n\tstruct ttm_device *bdev;\n\tenum ttm_bo_type type;\n\tuint32_t page_alignment;\n\tvoid (*destroy) (struct ttm_buffer_object *);\n\n\t \n\tstruct kref kref;\n\n\t \n\tstruct ttm_resource *resource;\n\tstruct ttm_tt *ttm;\n\tbool deleted;\n\tstruct ttm_lru_bulk_move *bulk_move;\n\tunsigned priority;\n\tunsigned pin_count;\n\n\t \n\tstruct work_struct delayed_delete;\n\n\t \n\tstruct sg_table *sg;\n};\n\n \n#define TTM_BO_MAP_IOMEM_MASK 0x80\nstruct ttm_bo_kmap_obj {\n\tvoid *virtual;\n\tstruct page *page;\n\tenum {\n\t\tttm_bo_map_iomap        = 1 | TTM_BO_MAP_IOMEM_MASK,\n\t\tttm_bo_map_vmap         = 2,\n\t\tttm_bo_map_kmap         = 3,\n\t\tttm_bo_map_premapped    = 4 | TTM_BO_MAP_IOMEM_MASK,\n\t} bo_kmap_type;\n\tstruct ttm_buffer_object *bo;\n};\n\n \nstruct ttm_operation_ctx {\n\tbool interruptible;\n\tbool no_wait_gpu;\n\tbool gfp_retry_mayfail;\n\tbool allow_res_evict;\n\tbool force_alloc;\n\tstruct dma_resv *resv;\n\tuint64_t bytes_moved;\n};\n\n \nstatic inline void ttm_bo_get(struct ttm_buffer_object *bo)\n{\n\tkref_get(&bo->kref);\n}\n\n \nstatic inline __must_check struct ttm_buffer_object *\nttm_bo_get_unless_zero(struct ttm_buffer_object *bo)\n{\n\tif (!kref_get_unless_zero(&bo->kref))\n\t\treturn NULL;\n\treturn bo;\n}\n\n \nstatic inline int ttm_bo_reserve(struct ttm_buffer_object *bo,\n\t\t\t\t bool interruptible, bool no_wait,\n\t\t\t\t struct ww_acquire_ctx *ticket)\n{\n\tint ret = 0;\n\n\tif (no_wait) {\n\t\tbool success;\n\n\t\tif (WARN_ON(ticket))\n\t\t\treturn -EBUSY;\n\n\t\tsuccess = dma_resv_trylock(bo->base.resv);\n\t\treturn success ? 0 : -EBUSY;\n\t}\n\n\tif (interruptible)\n\t\tret = dma_resv_lock_interruptible(bo->base.resv, ticket);\n\telse\n\t\tret = dma_resv_lock(bo->base.resv, ticket);\n\tif (ret == -EINTR)\n\t\treturn -ERESTARTSYS;\n\treturn ret;\n}\n\n \nstatic inline int ttm_bo_reserve_slowpath(struct ttm_buffer_object *bo,\n\t\t\t\t\t  bool interruptible,\n\t\t\t\t\t  struct ww_acquire_ctx *ticket)\n{\n\tif (interruptible) {\n\t\tint ret = dma_resv_lock_slow_interruptible(bo->base.resv,\n\t\t\t\t\t\t\t   ticket);\n\t\tif (ret == -EINTR)\n\t\t\tret = -ERESTARTSYS;\n\t\treturn ret;\n\t}\n\tdma_resv_lock_slow(bo->base.resv, ticket);\n\treturn 0;\n}\n\nvoid ttm_bo_move_to_lru_tail(struct ttm_buffer_object *bo);\n\nstatic inline void\nttm_bo_move_to_lru_tail_unlocked(struct ttm_buffer_object *bo)\n{\n\tspin_lock(&bo->bdev->lru_lock);\n\tttm_bo_move_to_lru_tail(bo);\n\tspin_unlock(&bo->bdev->lru_lock);\n}\n\nstatic inline void ttm_bo_assign_mem(struct ttm_buffer_object *bo,\n\t\t\t\t     struct ttm_resource *new_mem)\n{\n\tWARN_ON(bo->resource);\n\tbo->resource = new_mem;\n}\n\n \nstatic inline void ttm_bo_move_null(struct ttm_buffer_object *bo,\n\t\t\t\t    struct ttm_resource *new_mem)\n{\n\tttm_resource_free(bo, &bo->resource);\n\tttm_bo_assign_mem(bo, new_mem);\n}\n\n \nstatic inline void ttm_bo_unreserve(struct ttm_buffer_object *bo)\n{\n\tttm_bo_move_to_lru_tail_unlocked(bo);\n\tdma_resv_unlock(bo->base.resv);\n}\n\n \nstatic inline void *ttm_kmap_obj_virtual(struct ttm_bo_kmap_obj *map,\n\t\t\t\t\t bool *is_iomem)\n{\n\t*is_iomem = !!(map->bo_kmap_type & TTM_BO_MAP_IOMEM_MASK);\n\treturn map->virtual;\n}\n\nint ttm_bo_wait_ctx(struct ttm_buffer_object *bo,\n\t\t    struct ttm_operation_ctx *ctx);\nint ttm_bo_validate(struct ttm_buffer_object *bo,\n\t\t    struct ttm_placement *placement,\n\t\t    struct ttm_operation_ctx *ctx);\nvoid ttm_bo_put(struct ttm_buffer_object *bo);\nvoid ttm_bo_set_bulk_move(struct ttm_buffer_object *bo,\n\t\t\t  struct ttm_lru_bulk_move *bulk);\nbool ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,\n\t\t\t      const struct ttm_place *place);\nint ttm_bo_init_reserved(struct ttm_device *bdev, struct ttm_buffer_object *bo,\n\t\t\t enum ttm_bo_type type, struct ttm_placement *placement,\n\t\t\t uint32_t alignment, struct ttm_operation_ctx *ctx,\n\t\t\t struct sg_table *sg, struct dma_resv *resv,\n\t\t\t void (*destroy)(struct ttm_buffer_object *));\nint ttm_bo_init_validate(struct ttm_device *bdev, struct ttm_buffer_object *bo,\n\t\t\t enum ttm_bo_type type, struct ttm_placement *placement,\n\t\t\t uint32_t alignment, bool interruptible,\n\t\t\t struct sg_table *sg, struct dma_resv *resv,\n\t\t\t void (*destroy)(struct ttm_buffer_object *));\nint ttm_bo_kmap(struct ttm_buffer_object *bo, unsigned long start_page,\n\t\tunsigned long num_pages, struct ttm_bo_kmap_obj *map);\nvoid ttm_bo_kunmap(struct ttm_bo_kmap_obj *map);\nint ttm_bo_vmap(struct ttm_buffer_object *bo, struct iosys_map *map);\nvoid ttm_bo_vunmap(struct ttm_buffer_object *bo, struct iosys_map *map);\nint ttm_bo_mmap_obj(struct vm_area_struct *vma, struct ttm_buffer_object *bo);\nint ttm_bo_swapout(struct ttm_buffer_object *bo, struct ttm_operation_ctx *ctx,\n\t\t   gfp_t gfp_flags);\nvoid ttm_bo_pin(struct ttm_buffer_object *bo);\nvoid ttm_bo_unpin(struct ttm_buffer_object *bo);\nint ttm_mem_evict_first(struct ttm_device *bdev,\n\t\t\tstruct ttm_resource_manager *man,\n\t\t\tconst struct ttm_place *place,\n\t\t\tstruct ttm_operation_ctx *ctx,\n\t\t\tstruct ww_acquire_ctx *ticket);\nvm_fault_t ttm_bo_vm_reserve(struct ttm_buffer_object *bo,\n\t\t\t     struct vm_fault *vmf);\nvm_fault_t ttm_bo_vm_fault_reserved(struct vm_fault *vmf,\n\t\t\t\t    pgprot_t prot,\n\t\t\t\t    pgoff_t num_prefault);\nvm_fault_t ttm_bo_vm_fault(struct vm_fault *vmf);\nvoid ttm_bo_vm_open(struct vm_area_struct *vma);\nvoid ttm_bo_vm_close(struct vm_area_struct *vma);\nint ttm_bo_vm_access(struct vm_area_struct *vma, unsigned long addr,\n\t\t     void *buf, int len, int write);\nvm_fault_t ttm_bo_vm_dummy_page(struct vm_fault *vmf, pgprot_t prot);\n\nint ttm_bo_mem_space(struct ttm_buffer_object *bo,\n\t\t     struct ttm_placement *placement,\n\t\t     struct ttm_resource **mem,\n\t\t     struct ttm_operation_ctx *ctx);\n\nvoid ttm_bo_unmap_virtual(struct ttm_buffer_object *bo);\n \nint ttm_mem_io_reserve(struct ttm_device *bdev,\n\t\t       struct ttm_resource *mem);\nvoid ttm_mem_io_free(struct ttm_device *bdev,\n\t\t     struct ttm_resource *mem);\nvoid ttm_move_memcpy(bool clear, u32 num_pages,\n\t\t     struct ttm_kmap_iter *dst_iter,\n\t\t     struct ttm_kmap_iter *src_iter);\nint ttm_bo_move_memcpy(struct ttm_buffer_object *bo,\n\t\t       struct ttm_operation_ctx *ctx,\n\t\t       struct ttm_resource *new_mem);\nint ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,\n\t\t\t      struct dma_fence *fence, bool evict,\n\t\t\t      bool pipeline,\n\t\t\t      struct ttm_resource *new_mem);\nvoid ttm_bo_move_sync_cleanup(struct ttm_buffer_object *bo,\n\t\t\t      struct ttm_resource *new_mem);\nint ttm_bo_pipeline_gutting(struct ttm_buffer_object *bo);\npgprot_t ttm_io_prot(struct ttm_buffer_object *bo, struct ttm_resource *res,\n\t\t     pgprot_t tmp);\nvoid ttm_bo_tt_destroy(struct ttm_buffer_object *bo);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}