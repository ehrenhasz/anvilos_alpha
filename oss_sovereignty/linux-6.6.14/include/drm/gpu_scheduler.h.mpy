{
  "module_name": "gpu_scheduler.h",
  "hash_id": "f8918e7981621e1c1f9534d0c1d325772f3fd2b5c01f0da4ec2ad7bc700c8ebb",
  "original_prompt": "Ingested from linux-6.6.14/include/drm/gpu_scheduler.h",
  "human_readable_source": " \n\n#ifndef _DRM_GPU_SCHEDULER_H_\n#define _DRM_GPU_SCHEDULER_H_\n\n#include <drm/spsc_queue.h>\n#include <linux/dma-fence.h>\n#include <linux/completion.h>\n#include <linux/xarray.h>\n#include <linux/workqueue.h>\n\n#define MAX_WAIT_SCHED_ENTITY_Q_EMPTY msecs_to_jiffies(1000)\n\n \n#define DRM_SCHED_FENCE_DONT_PIPELINE\tDMA_FENCE_FLAG_USER_BITS\n\n \n#define DRM_SCHED_FENCE_FLAG_HAS_DEADLINE_BIT\t(DMA_FENCE_FLAG_USER_BITS + 1)\n\nenum dma_resv_usage;\nstruct dma_resv;\nstruct drm_gem_object;\n\nstruct drm_gpu_scheduler;\nstruct drm_sched_rq;\n\nstruct drm_file;\n\n \nenum drm_sched_priority {\n\tDRM_SCHED_PRIORITY_MIN,\n\tDRM_SCHED_PRIORITY_NORMAL,\n\tDRM_SCHED_PRIORITY_HIGH,\n\tDRM_SCHED_PRIORITY_KERNEL,\n\n\tDRM_SCHED_PRIORITY_COUNT\n};\n\n \nextern int drm_sched_policy;\n\n#define DRM_SCHED_POLICY_RR    0\n#define DRM_SCHED_POLICY_FIFO  1\n\n \nstruct drm_sched_entity {\n\t \n\tstruct list_head\t\tlist;\n\n\t \n\tstruct drm_sched_rq\t\t*rq;\n\n\t \n\tstruct drm_gpu_scheduler        **sched_list;\n\n\t \n\tunsigned int                    num_sched_list;\n\n\t \n\tenum drm_sched_priority         priority;\n\n\t \n\tspinlock_t\t\t\trq_lock;\n\n\t \n\tstruct spsc_queue\t\tjob_queue;\n\n\t \n\tatomic_t\t\t\tfence_seq;\n\n\t \n\tuint64_t\t\t\tfence_context;\n\n\t \n\tstruct dma_fence\t\t*dependency;\n\n\t \n\tstruct dma_fence_cb\t\tcb;\n\n\t \n\tatomic_t\t\t\t*guilty;\n\n\t \n\tstruct dma_fence __rcu\t\t*last_scheduled;\n\n\t \n\tstruct task_struct\t\t*last_user;\n\n\t \n\tbool \t\t\t\tstopped;\n\n\t \n\tstruct completion\t\tentity_idle;\n\n\t \n\tktime_t\t\t\t\toldest_job_waiting;\n\n\t \n\tstruct rb_node\t\t\trb_tree_node;\n\n};\n\n \nstruct drm_sched_rq {\n\tspinlock_t\t\t\tlock;\n\tstruct drm_gpu_scheduler\t*sched;\n\tstruct list_head\t\tentities;\n\tstruct drm_sched_entity\t\t*current_entity;\n\tstruct rb_root_cached\t\trb_tree_root;\n};\n\n \nstruct drm_sched_fence {\n         \n\tstruct dma_fence\t\tscheduled;\n\n         \n\tstruct dma_fence\t\tfinished;\n\n\t \n\tktime_t\t\t\t\tdeadline;\n\n         \n\tstruct dma_fence\t\t*parent;\n         \n\tstruct drm_gpu_scheduler\t*sched;\n         \n\tspinlock_t\t\t\tlock;\n         \n\tvoid\t\t\t\t*owner;\n};\n\nstruct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f);\n\n \nstruct drm_sched_job {\n\tstruct spsc_node\t\tqueue_node;\n\tstruct list_head\t\tlist;\n\tstruct drm_gpu_scheduler\t*sched;\n\tstruct drm_sched_fence\t\t*s_fence;\n\n\t \n\tunion {\n\t\tstruct dma_fence_cb\t\tfinish_cb;\n\t\tstruct work_struct\t\twork;\n\t};\n\n\tuint64_t\t\t\tid;\n\tatomic_t\t\t\tkarma;\n\tenum drm_sched_priority\t\ts_priority;\n\tstruct drm_sched_entity         *entity;\n\tstruct dma_fence_cb\t\tcb;\n\t \n\tstruct xarray\t\t\tdependencies;\n\n\t \n\tunsigned long\t\t\tlast_dependency;\n\n\t \n\tktime_t                         submit_ts;\n};\n\nstatic inline bool drm_sched_invalidate_job(struct drm_sched_job *s_job,\n\t\t\t\t\t    int threshold)\n{\n\treturn s_job && atomic_inc_return(&s_job->karma) > threshold;\n}\n\nenum drm_gpu_sched_stat {\n\tDRM_GPU_SCHED_STAT_NONE,  \n\tDRM_GPU_SCHED_STAT_NOMINAL,\n\tDRM_GPU_SCHED_STAT_ENODEV,\n};\n\n \nstruct drm_sched_backend_ops {\n\t \n\tstruct dma_fence *(*prepare_job)(struct drm_sched_job *sched_job,\n\t\t\t\t\t struct drm_sched_entity *s_entity);\n\n\t \n\tstruct dma_fence *(*run_job)(struct drm_sched_job *sched_job);\n\n\t \n\tenum drm_gpu_sched_stat (*timedout_job)(struct drm_sched_job *sched_job);\n\n\t \n\tvoid (*free_job)(struct drm_sched_job *sched_job);\n};\n\n \nstruct drm_gpu_scheduler {\n\tconst struct drm_sched_backend_ops\t*ops;\n\tuint32_t\t\t\thw_submission_limit;\n\tlong\t\t\t\ttimeout;\n\tconst char\t\t\t*name;\n\tstruct drm_sched_rq\t\tsched_rq[DRM_SCHED_PRIORITY_COUNT];\n\twait_queue_head_t\t\twake_up_worker;\n\twait_queue_head_t\t\tjob_scheduled;\n\tatomic_t\t\t\thw_rq_count;\n\tatomic64_t\t\t\tjob_id_count;\n\tstruct workqueue_struct\t\t*timeout_wq;\n\tstruct delayed_work\t\twork_tdr;\n\tstruct task_struct\t\t*thread;\n\tstruct list_head\t\tpending_list;\n\tspinlock_t\t\t\tjob_list_lock;\n\tint\t\t\t\thang_limit;\n\tatomic_t                        *score;\n\tatomic_t                        _score;\n\tbool\t\t\t\tready;\n\tbool\t\t\t\tfree_guilty;\n\tstruct device\t\t\t*dev;\n};\n\nint drm_sched_init(struct drm_gpu_scheduler *sched,\n\t\t   const struct drm_sched_backend_ops *ops,\n\t\t   uint32_t hw_submission, unsigned hang_limit,\n\t\t   long timeout, struct workqueue_struct *timeout_wq,\n\t\t   atomic_t *score, const char *name, struct device *dev);\n\nvoid drm_sched_fini(struct drm_gpu_scheduler *sched);\nint drm_sched_job_init(struct drm_sched_job *job,\n\t\t       struct drm_sched_entity *entity,\n\t\t       void *owner);\nvoid drm_sched_job_arm(struct drm_sched_job *job);\nint drm_sched_job_add_dependency(struct drm_sched_job *job,\n\t\t\t\t struct dma_fence *fence);\nint drm_sched_job_add_syncobj_dependency(struct drm_sched_job *job,\n\t\t\t\t\t struct drm_file *file,\n\t\t\t\t\t u32 handle,\n\t\t\t\t\t u32 point);\nint drm_sched_job_add_resv_dependencies(struct drm_sched_job *job,\n\t\t\t\t\tstruct dma_resv *resv,\n\t\t\t\t\tenum dma_resv_usage usage);\nint drm_sched_job_add_implicit_dependencies(struct drm_sched_job *job,\n\t\t\t\t\t    struct drm_gem_object *obj,\n\t\t\t\t\t    bool write);\n\n\nvoid drm_sched_entity_modify_sched(struct drm_sched_entity *entity,\n\t\t\t\t    struct drm_gpu_scheduler **sched_list,\n                                   unsigned int num_sched_list);\n\nvoid drm_sched_job_cleanup(struct drm_sched_job *job);\nvoid drm_sched_wakeup_if_can_queue(struct drm_gpu_scheduler *sched);\nvoid drm_sched_stop(struct drm_gpu_scheduler *sched, struct drm_sched_job *bad);\nvoid drm_sched_start(struct drm_gpu_scheduler *sched, bool full_recovery);\nvoid drm_sched_resubmit_jobs(struct drm_gpu_scheduler *sched);\nvoid drm_sched_increase_karma(struct drm_sched_job *bad);\nvoid drm_sched_reset_karma(struct drm_sched_job *bad);\nvoid drm_sched_increase_karma_ext(struct drm_sched_job *bad, int type);\nbool drm_sched_dependency_optimized(struct dma_fence* fence,\n\t\t\t\t    struct drm_sched_entity *entity);\nvoid drm_sched_fault(struct drm_gpu_scheduler *sched);\n\nvoid drm_sched_rq_add_entity(struct drm_sched_rq *rq,\n\t\t\t     struct drm_sched_entity *entity);\nvoid drm_sched_rq_remove_entity(struct drm_sched_rq *rq,\n\t\t\t\tstruct drm_sched_entity *entity);\n\nvoid drm_sched_rq_update_fifo(struct drm_sched_entity *entity, ktime_t ts);\n\nint drm_sched_entity_init(struct drm_sched_entity *entity,\n\t\t\t  enum drm_sched_priority priority,\n\t\t\t  struct drm_gpu_scheduler **sched_list,\n\t\t\t  unsigned int num_sched_list,\n\t\t\t  atomic_t *guilty);\nlong drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout);\nvoid drm_sched_entity_fini(struct drm_sched_entity *entity);\nvoid drm_sched_entity_destroy(struct drm_sched_entity *entity);\nvoid drm_sched_entity_select_rq(struct drm_sched_entity *entity);\nstruct drm_sched_job *drm_sched_entity_pop_job(struct drm_sched_entity *entity);\nvoid drm_sched_entity_push_job(struct drm_sched_job *sched_job);\nvoid drm_sched_entity_set_priority(struct drm_sched_entity *entity,\n\t\t\t\t   enum drm_sched_priority priority);\nbool drm_sched_entity_is_ready(struct drm_sched_entity *entity);\nint drm_sched_entity_error(struct drm_sched_entity *entity);\n\nstruct drm_sched_fence *drm_sched_fence_alloc(\n\tstruct drm_sched_entity *s_entity, void *owner);\nvoid drm_sched_fence_init(struct drm_sched_fence *fence,\n\t\t\t  struct drm_sched_entity *entity);\nvoid drm_sched_fence_free(struct drm_sched_fence *fence);\n\nvoid drm_sched_fence_scheduled(struct drm_sched_fence *fence,\n\t\t\t       struct dma_fence *parent);\nvoid drm_sched_fence_finished(struct drm_sched_fence *fence, int result);\n\nunsigned long drm_sched_suspend_timeout(struct drm_gpu_scheduler *sched);\nvoid drm_sched_resume_timeout(struct drm_gpu_scheduler *sched,\n\t\t                unsigned long remaining);\nstruct drm_gpu_scheduler *\ndrm_sched_pick_best(struct drm_gpu_scheduler **sched_list,\n\t\t     unsigned int num_sched_list);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}