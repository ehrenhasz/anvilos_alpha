{
  "module_name": "pagemap.h",
  "hash_id": "07c642808714e06cd8678c82676c8dcd74b1e1a14ee9bf06025511d00f70f19f",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/pagemap.h",
  "human_readable_source": " \n#ifndef _LINUX_PAGEMAP_H\n#define _LINUX_PAGEMAP_H\n\n \n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/list.h>\n#include <linux/highmem.h>\n#include <linux/compiler.h>\n#include <linux/uaccess.h>\n#include <linux/gfp.h>\n#include <linux/bitops.h>\n#include <linux/hardirq.h>  \n#include <linux/hugetlb_inline.h>\n\nstruct folio_batch;\n\nunsigned long invalidate_mapping_pages(struct address_space *mapping,\n\t\t\t\t\tpgoff_t start, pgoff_t end);\n\nstatic inline void invalidate_remote_inode(struct inode *inode)\n{\n\tif (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||\n\t    S_ISLNK(inode->i_mode))\n\t\tinvalidate_mapping_pages(inode->i_mapping, 0, -1);\n}\nint invalidate_inode_pages2(struct address_space *mapping);\nint invalidate_inode_pages2_range(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t end);\nint kiocb_invalidate_pages(struct kiocb *iocb, size_t count);\nvoid kiocb_invalidate_post_direct_write(struct kiocb *iocb, size_t count);\n\nint write_inode_now(struct inode *, int sync);\nint filemap_fdatawrite(struct address_space *);\nint filemap_flush(struct address_space *);\nint filemap_fdatawait_keep_errors(struct address_space *mapping);\nint filemap_fdatawait_range(struct address_space *, loff_t lstart, loff_t lend);\nint filemap_fdatawait_range_keep_errors(struct address_space *mapping,\n\t\tloff_t start_byte, loff_t end_byte);\n\nstatic inline int filemap_fdatawait(struct address_space *mapping)\n{\n\treturn filemap_fdatawait_range(mapping, 0, LLONG_MAX);\n}\n\nbool filemap_range_has_page(struct address_space *, loff_t lstart, loff_t lend);\nint filemap_write_and_wait_range(struct address_space *mapping,\n\t\tloff_t lstart, loff_t lend);\nint __filemap_fdatawrite_range(struct address_space *mapping,\n\t\tloff_t start, loff_t end, int sync_mode);\nint filemap_fdatawrite_range(struct address_space *mapping,\n\t\tloff_t start, loff_t end);\nint filemap_check_errors(struct address_space *mapping);\nvoid __filemap_set_wb_err(struct address_space *mapping, int err);\nint filemap_fdatawrite_wbc(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc);\nint kiocb_write_and_wait(struct kiocb *iocb, size_t count);\n\nstatic inline int filemap_write_and_wait(struct address_space *mapping)\n{\n\treturn filemap_write_and_wait_range(mapping, 0, LLONG_MAX);\n}\n\n \nstatic inline void filemap_set_wb_err(struct address_space *mapping, int err)\n{\n\t \n\tif (unlikely(err))\n\t\t__filemap_set_wb_err(mapping, err);\n}\n\n \nstatic inline int filemap_check_wb_err(struct address_space *mapping,\n\t\t\t\t\terrseq_t since)\n{\n\treturn errseq_check(&mapping->wb_err, since);\n}\n\n \nstatic inline errseq_t filemap_sample_wb_err(struct address_space *mapping)\n{\n\treturn errseq_sample(&mapping->wb_err);\n}\n\n \nstatic inline errseq_t file_sample_sb_err(struct file *file)\n{\n\treturn errseq_sample(&file->f_path.dentry->d_sb->s_wb_err);\n}\n\n \nstatic inline int inode_drain_writes(struct inode *inode)\n{\n\tinode_dio_wait(inode);\n\treturn filemap_write_and_wait(inode->i_mapping);\n}\n\nstatic inline bool mapping_empty(struct address_space *mapping)\n{\n\treturn xa_empty(&mapping->i_pages);\n}\n\n \nstatic inline bool mapping_shrinkable(struct address_space *mapping)\n{\n\tvoid *head;\n\n\t \n\tif (IS_ENABLED(CONFIG_HIGHMEM))\n\t\treturn true;\n\n\t \n\thead = rcu_access_pointer(mapping->i_pages.xa_head);\n\tif (!head)\n\t\treturn true;\n\n\t \n\tif (!xa_is_node(head) && xa_is_value(head))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nenum mapping_flags {\n\tAS_EIO\t\t= 0,\t \n\tAS_ENOSPC\t= 1,\t \n\tAS_MM_ALL_LOCKS\t= 2,\t \n\tAS_UNEVICTABLE\t= 3,\t \n\tAS_EXITING\t= 4, \t \n\t \n\tAS_NO_WRITEBACK_TAGS = 5,\n\tAS_LARGE_FOLIO_SUPPORT = 6,\n\tAS_RELEASE_ALWAYS,\t \n\tAS_STABLE_WRITES,\t \n};\n\n \nstatic inline void mapping_set_error(struct address_space *mapping, int error)\n{\n\tif (likely(!error))\n\t\treturn;\n\n\t \n\t__filemap_set_wb_err(mapping, error);\n\n\t \n\tif (mapping->host)\n\t\terrseq_set(&mapping->host->i_sb->s_wb_err, error);\n\n\t \n\tif (error == -ENOSPC)\n\t\tset_bit(AS_ENOSPC, &mapping->flags);\n\telse\n\t\tset_bit(AS_EIO, &mapping->flags);\n}\n\nstatic inline void mapping_set_unevictable(struct address_space *mapping)\n{\n\tset_bit(AS_UNEVICTABLE, &mapping->flags);\n}\n\nstatic inline void mapping_clear_unevictable(struct address_space *mapping)\n{\n\tclear_bit(AS_UNEVICTABLE, &mapping->flags);\n}\n\nstatic inline bool mapping_unevictable(struct address_space *mapping)\n{\n\treturn mapping && test_bit(AS_UNEVICTABLE, &mapping->flags);\n}\n\nstatic inline void mapping_set_exiting(struct address_space *mapping)\n{\n\tset_bit(AS_EXITING, &mapping->flags);\n}\n\nstatic inline int mapping_exiting(struct address_space *mapping)\n{\n\treturn test_bit(AS_EXITING, &mapping->flags);\n}\n\nstatic inline void mapping_set_no_writeback_tags(struct address_space *mapping)\n{\n\tset_bit(AS_NO_WRITEBACK_TAGS, &mapping->flags);\n}\n\nstatic inline int mapping_use_writeback_tags(struct address_space *mapping)\n{\n\treturn !test_bit(AS_NO_WRITEBACK_TAGS, &mapping->flags);\n}\n\nstatic inline bool mapping_release_always(const struct address_space *mapping)\n{\n\treturn test_bit(AS_RELEASE_ALWAYS, &mapping->flags);\n}\n\nstatic inline void mapping_set_release_always(struct address_space *mapping)\n{\n\tset_bit(AS_RELEASE_ALWAYS, &mapping->flags);\n}\n\nstatic inline void mapping_clear_release_always(struct address_space *mapping)\n{\n\tclear_bit(AS_RELEASE_ALWAYS, &mapping->flags);\n}\n\nstatic inline bool mapping_stable_writes(const struct address_space *mapping)\n{\n\treturn test_bit(AS_STABLE_WRITES, &mapping->flags);\n}\n\nstatic inline void mapping_set_stable_writes(struct address_space *mapping)\n{\n\tset_bit(AS_STABLE_WRITES, &mapping->flags);\n}\n\nstatic inline void mapping_clear_stable_writes(struct address_space *mapping)\n{\n\tclear_bit(AS_STABLE_WRITES, &mapping->flags);\n}\n\nstatic inline gfp_t mapping_gfp_mask(struct address_space * mapping)\n{\n\treturn mapping->gfp_mask;\n}\n\n \nstatic inline gfp_t mapping_gfp_constraint(struct address_space *mapping,\n\t\tgfp_t gfp_mask)\n{\n\treturn mapping_gfp_mask(mapping) & gfp_mask;\n}\n\n \nstatic inline void mapping_set_gfp_mask(struct address_space *m, gfp_t mask)\n{\n\tm->gfp_mask = mask;\n}\n\n \nstatic inline void mapping_set_large_folios(struct address_space *mapping)\n{\n\t__set_bit(AS_LARGE_FOLIO_SUPPORT, &mapping->flags);\n}\n\n \nstatic inline bool mapping_large_folio_support(struct address_space *mapping)\n{\n\treturn IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) &&\n\t\ttest_bit(AS_LARGE_FOLIO_SUPPORT, &mapping->flags);\n}\n\nstatic inline int filemap_nr_thps(struct address_space *mapping)\n{\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\treturn atomic_read(&mapping->nr_thps);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void filemap_nr_thps_inc(struct address_space *mapping)\n{\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tif (!mapping_large_folio_support(mapping))\n\t\tatomic_inc(&mapping->nr_thps);\n#else\n\tWARN_ON_ONCE(mapping_large_folio_support(mapping) == 0);\n#endif\n}\n\nstatic inline void filemap_nr_thps_dec(struct address_space *mapping)\n{\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tif (!mapping_large_folio_support(mapping))\n\t\tatomic_dec(&mapping->nr_thps);\n#else\n\tWARN_ON_ONCE(mapping_large_folio_support(mapping) == 0);\n#endif\n}\n\nstruct address_space *page_mapping(struct page *);\nstruct address_space *folio_mapping(struct folio *);\nstruct address_space *swapcache_mapping(struct folio *);\n\n \nstatic inline struct address_space *folio_file_mapping(struct folio *folio)\n{\n\tif (unlikely(folio_test_swapcache(folio)))\n\t\treturn swapcache_mapping(folio);\n\n\treturn folio->mapping;\n}\n\n \nstatic inline struct address_space *folio_flush_mapping(struct folio *folio)\n{\n\tif (unlikely(folio_test_swapcache(folio)))\n\t\treturn NULL;\n\n\treturn folio_mapping(folio);\n}\n\nstatic inline struct address_space *page_file_mapping(struct page *page)\n{\n\treturn folio_file_mapping(page_folio(page));\n}\n\n \nstatic inline struct inode *folio_inode(struct folio *folio)\n{\n\treturn folio->mapping->host;\n}\n\n \nstatic inline void folio_attach_private(struct folio *folio, void *data)\n{\n\tfolio_get(folio);\n\tfolio->private = data;\n\tfolio_set_private(folio);\n}\n\n \nstatic inline void *folio_change_private(struct folio *folio, void *data)\n{\n\tvoid *old = folio_get_private(folio);\n\n\tfolio->private = data;\n\treturn old;\n}\n\n \nstatic inline void *folio_detach_private(struct folio *folio)\n{\n\tvoid *data = folio_get_private(folio);\n\n\tif (!folio_test_private(folio))\n\t\treturn NULL;\n\tfolio_clear_private(folio);\n\tfolio->private = NULL;\n\tfolio_put(folio);\n\n\treturn data;\n}\n\nstatic inline void attach_page_private(struct page *page, void *data)\n{\n\tfolio_attach_private(page_folio(page), data);\n}\n\nstatic inline void *detach_page_private(struct page *page)\n{\n\treturn folio_detach_private(page_folio(page));\n}\n\n \n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define MAX_PAGECACHE_ORDER\tHPAGE_PMD_ORDER\n#else\n#define MAX_PAGECACHE_ORDER\t8\n#endif\n\n#ifdef CONFIG_NUMA\nstruct folio *filemap_alloc_folio(gfp_t gfp, unsigned int order);\n#else\nstatic inline struct folio *filemap_alloc_folio(gfp_t gfp, unsigned int order)\n{\n\treturn folio_alloc(gfp, order);\n}\n#endif\n\nstatic inline struct page *__page_cache_alloc(gfp_t gfp)\n{\n\treturn &filemap_alloc_folio(gfp, 0)->page;\n}\n\nstatic inline struct page *page_cache_alloc(struct address_space *x)\n{\n\treturn __page_cache_alloc(mapping_gfp_mask(x));\n}\n\nstatic inline gfp_t readahead_gfp_mask(struct address_space *x)\n{\n\treturn mapping_gfp_mask(x) | __GFP_NORETRY | __GFP_NOWARN;\n}\n\ntypedef int filler_t(struct file *, struct folio *);\n\npgoff_t page_cache_next_miss(struct address_space *mapping,\n\t\t\t     pgoff_t index, unsigned long max_scan);\npgoff_t page_cache_prev_miss(struct address_space *mapping,\n\t\t\t     pgoff_t index, unsigned long max_scan);\n\n \ntypedef unsigned int __bitwise fgf_t;\n\n#define FGP_ACCESSED\t\t((__force fgf_t)0x00000001)\n#define FGP_LOCK\t\t((__force fgf_t)0x00000002)\n#define FGP_CREAT\t\t((__force fgf_t)0x00000004)\n#define FGP_WRITE\t\t((__force fgf_t)0x00000008)\n#define FGP_NOFS\t\t((__force fgf_t)0x00000010)\n#define FGP_NOWAIT\t\t((__force fgf_t)0x00000020)\n#define FGP_FOR_MMAP\t\t((__force fgf_t)0x00000040)\n#define FGP_STABLE\t\t((__force fgf_t)0x00000080)\n#define FGF_GET_ORDER(fgf)\t(((__force unsigned)fgf) >> 26)\t \n\n#define FGP_WRITEBEGIN\t\t(FGP_LOCK | FGP_WRITE | FGP_CREAT | FGP_STABLE)\n\n \nstatic inline fgf_t fgf_set_order(size_t size)\n{\n\tunsigned int shift = ilog2(size);\n\n\tif (shift <= PAGE_SHIFT)\n\t\treturn 0;\n\treturn (__force fgf_t)((shift - PAGE_SHIFT) << 26);\n}\n\nvoid *filemap_get_entry(struct address_space *mapping, pgoff_t index);\nstruct folio *__filemap_get_folio(struct address_space *mapping, pgoff_t index,\n\t\tfgf_t fgp_flags, gfp_t gfp);\nstruct page *pagecache_get_page(struct address_space *mapping, pgoff_t index,\n\t\tfgf_t fgp_flags, gfp_t gfp);\n\n \nstatic inline struct folio *filemap_get_folio(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index)\n{\n\treturn __filemap_get_folio(mapping, index, 0, 0);\n}\n\n \nstatic inline struct folio *filemap_lock_folio(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index)\n{\n\treturn __filemap_get_folio(mapping, index, FGP_LOCK, 0);\n}\n\n \nstatic inline struct folio *filemap_grab_folio(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index)\n{\n\treturn __filemap_get_folio(mapping, index,\n\t\t\tFGP_LOCK | FGP_ACCESSED | FGP_CREAT,\n\t\t\tmapping_gfp_mask(mapping));\n}\n\n \nstatic inline struct page *find_get_page(struct address_space *mapping,\n\t\t\t\t\tpgoff_t offset)\n{\n\treturn pagecache_get_page(mapping, offset, 0, 0);\n}\n\nstatic inline struct page *find_get_page_flags(struct address_space *mapping,\n\t\t\t\t\tpgoff_t offset, fgf_t fgp_flags)\n{\n\treturn pagecache_get_page(mapping, offset, fgp_flags, 0);\n}\n\n \nstatic inline struct page *find_lock_page(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index)\n{\n\treturn pagecache_get_page(mapping, index, FGP_LOCK, 0);\n}\n\n \nstatic inline struct page *find_or_create_page(struct address_space *mapping,\n\t\t\t\t\tpgoff_t index, gfp_t gfp_mask)\n{\n\treturn pagecache_get_page(mapping, index,\n\t\t\t\t\tFGP_LOCK|FGP_ACCESSED|FGP_CREAT,\n\t\t\t\t\tgfp_mask);\n}\n\n \nstatic inline struct page *grab_cache_page_nowait(struct address_space *mapping,\n\t\t\t\tpgoff_t index)\n{\n\treturn pagecache_get_page(mapping, index,\n\t\t\tFGP_LOCK|FGP_CREAT|FGP_NOFS|FGP_NOWAIT,\n\t\t\tmapping_gfp_mask(mapping));\n}\n\n#define swapcache_index(folio)\t__page_file_index(&(folio)->page)\n\n \nstatic inline pgoff_t folio_index(struct folio *folio)\n{\n        if (unlikely(folio_test_swapcache(folio)))\n                return swapcache_index(folio);\n        return folio->index;\n}\n\n \nstatic inline pgoff_t folio_next_index(struct folio *folio)\n{\n\treturn folio->index + folio_nr_pages(folio);\n}\n\n \nstatic inline struct page *folio_file_page(struct folio *folio, pgoff_t index)\n{\n\t \n\tif (folio_test_hugetlb(folio))\n\t\treturn &folio->page;\n\treturn folio_page(folio, index & (folio_nr_pages(folio) - 1));\n}\n\n \nstatic inline bool folio_contains(struct folio *folio, pgoff_t index)\n{\n\t \n\tif (folio_test_hugetlb(folio))\n\t\treturn folio->index == index;\n\treturn index - folio_index(folio) < folio_nr_pages(folio);\n}\n\n \nstatic inline struct page *find_subpage(struct page *head, pgoff_t index)\n{\n\t \n\tif (PageHuge(head))\n\t\treturn head;\n\n\treturn head + (index & (thp_nr_pages(head) - 1));\n}\n\nunsigned filemap_get_folios(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, struct folio_batch *fbatch);\nunsigned filemap_get_folios_contig(struct address_space *mapping,\n\t\tpgoff_t *start, pgoff_t end, struct folio_batch *fbatch);\nunsigned filemap_get_folios_tag(struct address_space *mapping, pgoff_t *start,\n\t\tpgoff_t end, xa_mark_t tag, struct folio_batch *fbatch);\n\nstruct page *grab_cache_page_write_begin(struct address_space *mapping,\n\t\t\tpgoff_t index);\n\n \nstatic inline struct page *grab_cache_page(struct address_space *mapping,\n\t\t\t\t\t\t\t\tpgoff_t index)\n{\n\treturn find_or_create_page(mapping, index, mapping_gfp_mask(mapping));\n}\n\nstruct folio *read_cache_folio(struct address_space *, pgoff_t index,\n\t\tfiller_t *filler, struct file *file);\nstruct folio *mapping_read_folio_gfp(struct address_space *, pgoff_t index,\n\t\tgfp_t flags);\nstruct page *read_cache_page(struct address_space *, pgoff_t index,\n\t\tfiller_t *filler, struct file *file);\nextern struct page * read_cache_page_gfp(struct address_space *mapping,\n\t\t\t\tpgoff_t index, gfp_t gfp_mask);\n\nstatic inline struct page *read_mapping_page(struct address_space *mapping,\n\t\t\t\tpgoff_t index, struct file *file)\n{\n\treturn read_cache_page(mapping, index, NULL, file);\n}\n\nstatic inline struct folio *read_mapping_folio(struct address_space *mapping,\n\t\t\t\tpgoff_t index, struct file *file)\n{\n\treturn read_cache_folio(mapping, index, NULL, file);\n}\n\n \nstatic inline pgoff_t page_to_index(struct page *page)\n{\n\tstruct page *head;\n\n\tif (likely(!PageTransTail(page)))\n\t\treturn page->index;\n\n\thead = compound_head(page);\n\t \n\treturn head->index + page - head;\n}\n\nextern pgoff_t hugetlb_basepage_index(struct page *page);\n\n \nstatic inline pgoff_t page_to_pgoff(struct page *page)\n{\n\tif (unlikely(PageHuge(page)))\n\t\treturn hugetlb_basepage_index(page);\n\treturn page_to_index(page);\n}\n\n \nstatic inline loff_t page_offset(struct page *page)\n{\n\treturn ((loff_t)page->index) << PAGE_SHIFT;\n}\n\nstatic inline loff_t page_file_offset(struct page *page)\n{\n\treturn ((loff_t)page_index(page)) << PAGE_SHIFT;\n}\n\n \nstatic inline loff_t folio_pos(struct folio *folio)\n{\n\treturn page_offset(&folio->page);\n}\n\n \nstatic inline loff_t folio_file_pos(struct folio *folio)\n{\n\treturn page_file_offset(&folio->page);\n}\n\n \nstatic inline pgoff_t folio_pgoff(struct folio *folio)\n{\n\tif (unlikely(folio_test_hugetlb(folio)))\n\t\treturn hugetlb_basepage_index(&folio->page);\n\treturn folio->index;\n}\n\nextern pgoff_t linear_hugepage_index(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long address);\n\nstatic inline pgoff_t linear_page_index(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address)\n{\n\tpgoff_t pgoff;\n\tif (unlikely(is_vm_hugetlb_page(vma)))\n\t\treturn linear_hugepage_index(vma, address);\n\tpgoff = (address - vma->vm_start) >> PAGE_SHIFT;\n\tpgoff += vma->vm_pgoff;\n\treturn pgoff;\n}\n\nstruct wait_page_key {\n\tstruct folio *folio;\n\tint bit_nr;\n\tint page_match;\n};\n\nstruct wait_page_queue {\n\tstruct folio *folio;\n\tint bit_nr;\n\twait_queue_entry_t wait;\n};\n\nstatic inline bool wake_page_match(struct wait_page_queue *wait_page,\n\t\t\t\t  struct wait_page_key *key)\n{\n\tif (wait_page->folio != key->folio)\n\t       return false;\n\tkey->page_match = 1;\n\n\tif (wait_page->bit_nr != key->bit_nr)\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid __folio_lock(struct folio *folio);\nint __folio_lock_killable(struct folio *folio);\nvm_fault_t __folio_lock_or_retry(struct folio *folio, struct vm_fault *vmf);\nvoid unlock_page(struct page *page);\nvoid folio_unlock(struct folio *folio);\n\n \nstatic inline bool folio_trylock(struct folio *folio)\n{\n\treturn likely(!test_and_set_bit_lock(PG_locked, folio_flags(folio, 0)));\n}\n\n \nstatic inline int trylock_page(struct page *page)\n{\n\treturn folio_trylock(page_folio(page));\n}\n\n \nstatic inline void folio_lock(struct folio *folio)\n{\n\tmight_sleep();\n\tif (!folio_trylock(folio))\n\t\t__folio_lock(folio);\n}\n\n \nstatic inline void lock_page(struct page *page)\n{\n\tstruct folio *folio;\n\tmight_sleep();\n\n\tfolio = page_folio(page);\n\tif (!folio_trylock(folio))\n\t\t__folio_lock(folio);\n}\n\n \nstatic inline int folio_lock_killable(struct folio *folio)\n{\n\tmight_sleep();\n\tif (!folio_trylock(folio))\n\t\treturn __folio_lock_killable(folio);\n\treturn 0;\n}\n\n \nstatic inline vm_fault_t folio_lock_or_retry(struct folio *folio,\n\t\t\t\t\t     struct vm_fault *vmf)\n{\n\tmight_sleep();\n\tif (!folio_trylock(folio))\n\t\treturn __folio_lock_or_retry(folio, vmf);\n\treturn 0;\n}\n\n \nvoid folio_wait_bit(struct folio *folio, int bit_nr);\nint folio_wait_bit_killable(struct folio *folio, int bit_nr);\n\n \nstatic inline void folio_wait_locked(struct folio *folio)\n{\n\tif (folio_test_locked(folio))\n\t\tfolio_wait_bit(folio, PG_locked);\n}\n\nstatic inline int folio_wait_locked_killable(struct folio *folio)\n{\n\tif (!folio_test_locked(folio))\n\t\treturn 0;\n\treturn folio_wait_bit_killable(folio, PG_locked);\n}\n\nstatic inline void wait_on_page_locked(struct page *page)\n{\n\tfolio_wait_locked(page_folio(page));\n}\n\nvoid wait_on_page_writeback(struct page *page);\nvoid folio_wait_writeback(struct folio *folio);\nint folio_wait_writeback_killable(struct folio *folio);\nvoid end_page_writeback(struct page *page);\nvoid folio_end_writeback(struct folio *folio);\nvoid wait_for_stable_page(struct page *page);\nvoid folio_wait_stable(struct folio *folio);\nvoid __folio_mark_dirty(struct folio *folio, struct address_space *, int warn);\nstatic inline void __set_page_dirty(struct page *page,\n\t\tstruct address_space *mapping, int warn)\n{\n\t__folio_mark_dirty(page_folio(page), mapping, warn);\n}\nvoid folio_account_cleaned(struct folio *folio, struct bdi_writeback *wb);\nvoid __folio_cancel_dirty(struct folio *folio);\nstatic inline void folio_cancel_dirty(struct folio *folio)\n{\n\t \n\tif (folio_test_dirty(folio))\n\t\t__folio_cancel_dirty(folio);\n}\nbool folio_clear_dirty_for_io(struct folio *folio);\nbool clear_page_dirty_for_io(struct page *page);\nvoid folio_invalidate(struct folio *folio, size_t offset, size_t length);\nint __set_page_dirty_nobuffers(struct page *page);\nbool noop_dirty_folio(struct address_space *mapping, struct folio *folio);\n\n#ifdef CONFIG_MIGRATION\nint filemap_migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode);\n#else\n#define filemap_migrate_folio NULL\n#endif\nvoid folio_end_private_2(struct folio *folio);\nvoid folio_wait_private_2(struct folio *folio);\nint folio_wait_private_2_killable(struct folio *folio);\n\n \nvoid folio_add_wait_queue(struct folio *folio, wait_queue_entry_t *waiter);\n\n \nsize_t fault_in_writeable(char __user *uaddr, size_t size);\nsize_t fault_in_subpage_writeable(char __user *uaddr, size_t size);\nsize_t fault_in_safe_writeable(const char __user *uaddr, size_t size);\nsize_t fault_in_readable(const char __user *uaddr, size_t size);\n\nint add_to_page_cache_lru(struct page *page, struct address_space *mapping,\n\t\tpgoff_t index, gfp_t gfp);\nint filemap_add_folio(struct address_space *mapping, struct folio *folio,\n\t\tpgoff_t index, gfp_t gfp);\nvoid filemap_remove_folio(struct folio *folio);\nvoid __filemap_remove_folio(struct folio *folio, void *shadow);\nvoid replace_page_cache_folio(struct folio *old, struct folio *new);\nvoid delete_from_page_cache_batch(struct address_space *mapping,\n\t\t\t\t  struct folio_batch *fbatch);\nbool filemap_release_folio(struct folio *folio, gfp_t gfp);\nloff_t mapping_seek_hole_data(struct address_space *, loff_t start, loff_t end,\n\t\tint whence);\n\n \nint __filemap_add_folio(struct address_space *mapping, struct folio *folio,\n\t\tpgoff_t index, gfp_t gfp, void **shadowp);\n\nbool filemap_range_has_writeback(struct address_space *mapping,\n\t\t\t\t loff_t start_byte, loff_t end_byte);\n\n \nstatic inline bool filemap_range_needs_writeback(struct address_space *mapping,\n\t\t\t\t\t\t loff_t start_byte,\n\t\t\t\t\t\t loff_t end_byte)\n{\n\tif (!mapping->nrpages)\n\t\treturn false;\n\tif (!mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&\n\t    !mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK))\n\t\treturn false;\n\treturn filemap_range_has_writeback(mapping, start_byte, end_byte);\n}\n\n \nstruct readahead_control {\n\tstruct file *file;\n\tstruct address_space *mapping;\n\tstruct file_ra_state *ra;\n \n\tpgoff_t _index;\n\tunsigned int _nr_pages;\n\tunsigned int _batch_count;\n\tbool _workingset;\n\tunsigned long _pflags;\n};\n\n#define DEFINE_READAHEAD(ractl, f, r, m, i)\t\t\t\t\\\n\tstruct readahead_control ractl = {\t\t\t\t\\\n\t\t.file = f,\t\t\t\t\t\t\\\n\t\t.mapping = m,\t\t\t\t\t\t\\\n\t\t.ra = r,\t\t\t\t\t\t\\\n\t\t._index = i,\t\t\t\t\t\t\\\n\t}\n\n#define VM_READAHEAD_PAGES\t(SZ_128K / PAGE_SIZE)\n\nvoid page_cache_ra_unbounded(struct readahead_control *,\n\t\tunsigned long nr_to_read, unsigned long lookahead_count);\nvoid page_cache_sync_ra(struct readahead_control *, unsigned long req_count);\nvoid page_cache_async_ra(struct readahead_control *, struct folio *,\n\t\tunsigned long req_count);\nvoid readahead_expand(struct readahead_control *ractl,\n\t\t      loff_t new_start, size_t new_len);\n\n \nstatic inline\nvoid page_cache_sync_readahead(struct address_space *mapping,\n\t\tstruct file_ra_state *ra, struct file *file, pgoff_t index,\n\t\tunsigned long req_count)\n{\n\tDEFINE_READAHEAD(ractl, file, ra, mapping, index);\n\tpage_cache_sync_ra(&ractl, req_count);\n}\n\n \nstatic inline\nvoid page_cache_async_readahead(struct address_space *mapping,\n\t\tstruct file_ra_state *ra, struct file *file,\n\t\tstruct folio *folio, pgoff_t index, unsigned long req_count)\n{\n\tDEFINE_READAHEAD(ractl, file, ra, mapping, index);\n\tpage_cache_async_ra(&ractl, folio, req_count);\n}\n\nstatic inline struct folio *__readahead_folio(struct readahead_control *ractl)\n{\n\tstruct folio *folio;\n\n\tBUG_ON(ractl->_batch_count > ractl->_nr_pages);\n\tractl->_nr_pages -= ractl->_batch_count;\n\tractl->_index += ractl->_batch_count;\n\n\tif (!ractl->_nr_pages) {\n\t\tractl->_batch_count = 0;\n\t\treturn NULL;\n\t}\n\n\tfolio = xa_load(&ractl->mapping->i_pages, ractl->_index);\n\tVM_BUG_ON_FOLIO(!folio_test_locked(folio), folio);\n\tractl->_batch_count = folio_nr_pages(folio);\n\n\treturn folio;\n}\n\n \nstatic inline struct page *readahead_page(struct readahead_control *ractl)\n{\n\tstruct folio *folio = __readahead_folio(ractl);\n\n\treturn &folio->page;\n}\n\n \nstatic inline struct folio *readahead_folio(struct readahead_control *ractl)\n{\n\tstruct folio *folio = __readahead_folio(ractl);\n\n\tif (folio)\n\t\tfolio_put(folio);\n\treturn folio;\n}\n\nstatic inline unsigned int __readahead_batch(struct readahead_control *rac,\n\t\tstruct page **array, unsigned int array_sz)\n{\n\tunsigned int i = 0;\n\tXA_STATE(xas, &rac->mapping->i_pages, 0);\n\tstruct page *page;\n\n\tBUG_ON(rac->_batch_count > rac->_nr_pages);\n\trac->_nr_pages -= rac->_batch_count;\n\trac->_index += rac->_batch_count;\n\trac->_batch_count = 0;\n\n\txas_set(&xas, rac->_index);\n\trcu_read_lock();\n\txas_for_each(&xas, page, rac->_index + rac->_nr_pages - 1) {\n\t\tif (xas_retry(&xas, page))\n\t\t\tcontinue;\n\t\tVM_BUG_ON_PAGE(!PageLocked(page), page);\n\t\tVM_BUG_ON_PAGE(PageTail(page), page);\n\t\tarray[i++] = page;\n\t\trac->_batch_count += thp_nr_pages(page);\n\t\tif (i == array_sz)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn i;\n}\n\n \n#define readahead_page_batch(rac, array)\t\t\t\t\\\n\t__readahead_batch(rac, array, ARRAY_SIZE(array))\n\n \nstatic inline loff_t readahead_pos(struct readahead_control *rac)\n{\n\treturn (loff_t)rac->_index * PAGE_SIZE;\n}\n\n \nstatic inline size_t readahead_length(struct readahead_control *rac)\n{\n\treturn rac->_nr_pages * PAGE_SIZE;\n}\n\n \nstatic inline pgoff_t readahead_index(struct readahead_control *rac)\n{\n\treturn rac->_index;\n}\n\n \nstatic inline unsigned int readahead_count(struct readahead_control *rac)\n{\n\treturn rac->_nr_pages;\n}\n\n \nstatic inline size_t readahead_batch_length(struct readahead_control *rac)\n{\n\treturn rac->_batch_count * PAGE_SIZE;\n}\n\nstatic inline unsigned long dir_pages(struct inode *inode)\n{\n\treturn (unsigned long)(inode->i_size + PAGE_SIZE - 1) >>\n\t\t\t       PAGE_SHIFT;\n}\n\n \nstatic inline ssize_t folio_mkwrite_check_truncate(struct folio *folio,\n\t\t\t\t\t      struct inode *inode)\n{\n\tloff_t size = i_size_read(inode);\n\tpgoff_t index = size >> PAGE_SHIFT;\n\tsize_t offset = offset_in_folio(folio, size);\n\n\tif (!folio->mapping)\n\t\treturn -EFAULT;\n\n\t \n\tif (folio_next_index(folio) - 1 < index)\n\t\treturn folio_size(folio);\n\t \n\tif (folio->index > index || !offset)\n\t\treturn -EFAULT;\n\t \n\treturn offset;\n}\n\n \nstatic inline int page_mkwrite_check_truncate(struct page *page,\n\t\t\t\t\t      struct inode *inode)\n{\n\tloff_t size = i_size_read(inode);\n\tpgoff_t index = size >> PAGE_SHIFT;\n\tint offset = offset_in_page(size);\n\n\tif (page->mapping != inode->i_mapping)\n\t\treturn -EFAULT;\n\n\t \n\tif (page->index < index)\n\t\treturn PAGE_SIZE;\n\t \n\tif (page->index > index || !offset)\n\t\treturn -EFAULT;\n\t \n\treturn offset;\n}\n\n \nstatic inline\nunsigned int i_blocks_per_folio(struct inode *inode, struct folio *folio)\n{\n\treturn folio_size(folio) >> inode->i_blkbits;\n}\n\nstatic inline\nunsigned int i_blocks_per_page(struct inode *inode, struct page *page)\n{\n\treturn i_blocks_per_folio(inode, page_folio(page));\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}