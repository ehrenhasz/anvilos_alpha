{
  "module_name": "huge_mm.h",
  "hash_id": "b4d38dde52a7e351a42015f96469b1857e133a46c1de34dbc850f0ea5ed81260",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/huge_mm.h",
  "human_readable_source": " \n#ifndef _LINUX_HUGE_MM_H\n#define _LINUX_HUGE_MM_H\n\n#include <linux/sched/coredump.h>\n#include <linux/mm_types.h>\n\n#include <linux/fs.h>  \n\nvm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf);\nint copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t  pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,\n\t\t  struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma);\nvoid huge_pmd_set_accessed(struct vm_fault *vmf);\nint copy_huge_pud(struct mm_struct *dst_mm, struct mm_struct *src_mm,\n\t\t  pud_t *dst_pud, pud_t *src_pud, unsigned long addr,\n\t\t  struct vm_area_struct *vma);\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nvoid huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud);\n#else\nstatic inline void huge_pud_set_accessed(struct vm_fault *vmf, pud_t orig_pud)\n{\n}\n#endif\n\nvm_fault_t do_huge_pmd_wp_page(struct vm_fault *vmf);\nbool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t\t   pmd_t *pmd, unsigned long addr, unsigned long next);\nint zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma, pmd_t *pmd,\n\t\t unsigned long addr);\nint zap_huge_pud(struct mmu_gather *tlb, struct vm_area_struct *vma, pud_t *pud,\n\t\t unsigned long addr);\nbool move_huge_pmd(struct vm_area_struct *vma, unsigned long old_addr,\n\t\t   unsigned long new_addr, pmd_t *old_pmd, pmd_t *new_pmd);\nint change_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,\n\t\t    pmd_t *pmd, unsigned long addr, pgprot_t newprot,\n\t\t    unsigned long cp_flags);\n\nvm_fault_t vmf_insert_pfn_pmd(struct vm_fault *vmf, pfn_t pfn, bool write);\nvm_fault_t vmf_insert_pfn_pud(struct vm_fault *vmf, pfn_t pfn, bool write);\n\nenum transparent_hugepage_flag {\n\tTRANSPARENT_HUGEPAGE_UNSUPPORTED,\n\tTRANSPARENT_HUGEPAGE_FLAG,\n\tTRANSPARENT_HUGEPAGE_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_DIRECT_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG,\n\tTRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG,\n\tTRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG,\n};\n\nstruct kobject;\nstruct kobj_attribute;\n\nssize_t single_hugepage_flag_store(struct kobject *kobj,\n\t\t\t\t   struct kobj_attribute *attr,\n\t\t\t\t   const char *buf, size_t count,\n\t\t\t\t   enum transparent_hugepage_flag flag);\nssize_t single_hugepage_flag_show(struct kobject *kobj,\n\t\t\t\t  struct kobj_attribute *attr, char *buf,\n\t\t\t\t  enum transparent_hugepage_flag flag);\nextern struct kobj_attribute shmem_enabled_attr;\n\n#define HPAGE_PMD_ORDER (HPAGE_PMD_SHIFT-PAGE_SHIFT)\n#define HPAGE_PMD_NR (1<<HPAGE_PMD_ORDER)\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define HPAGE_PMD_SHIFT PMD_SHIFT\n#define HPAGE_PMD_SIZE\t((1UL) << HPAGE_PMD_SHIFT)\n#define HPAGE_PMD_MASK\t(~(HPAGE_PMD_SIZE - 1))\n\n#define HPAGE_PUD_SHIFT PUD_SHIFT\n#define HPAGE_PUD_SIZE\t((1UL) << HPAGE_PUD_SHIFT)\n#define HPAGE_PUD_MASK\t(~(HPAGE_PUD_SIZE - 1))\n\nextern unsigned long transparent_hugepage_flags;\n\n#define hugepage_flags_enabled()\t\t\t\t\t       \\\n\t(transparent_hugepage_flags &\t\t\t\t       \\\n\t ((1<<TRANSPARENT_HUGEPAGE_FLAG) |\t\t       \\\n\t  (1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)))\n#define hugepage_flags_always()\t\t\t\t\\\n\t(transparent_hugepage_flags &\t\t\t\\\n\t (1<<TRANSPARENT_HUGEPAGE_FLAG))\n\n \nstatic inline bool transhuge_vma_suitable(struct vm_area_struct *vma,\n\t\tunsigned long addr)\n{\n\tunsigned long haddr;\n\n\t \n\tif (!vma_is_anonymous(vma)) {\n\t\tif (!IS_ALIGNED((vma->vm_start >> PAGE_SHIFT) - vma->vm_pgoff,\n\t\t\t\tHPAGE_PMD_NR))\n\t\t\treturn false;\n\t}\n\n\thaddr = addr & HPAGE_PMD_MASK;\n\n\tif (haddr < vma->vm_start || haddr + HPAGE_PMD_SIZE > vma->vm_end)\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool file_thp_enabled(struct vm_area_struct *vma)\n{\n\tstruct inode *inode;\n\n\tif (!vma->vm_file)\n\t\treturn false;\n\n\tinode = vma->vm_file->f_inode;\n\n\treturn (IS_ENABLED(CONFIG_READ_ONLY_THP_FOR_FS)) &&\n\t       (vma->vm_flags & VM_EXEC) &&\n\t       !inode_is_open_for_write(inode) && S_ISREG(inode->i_mode);\n}\n\nbool hugepage_vma_check(struct vm_area_struct *vma, unsigned long vm_flags,\n\t\t\tbool smaps, bool in_pf, bool enforce_sysfs);\n\n#define transparent_hugepage_use_zero_page()\t\t\t\t\\\n\t(transparent_hugepage_flags &\t\t\t\t\t\\\n\t (1<<TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG))\n\nunsigned long thp_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\tunsigned long len, unsigned long pgoff, unsigned long flags);\n\nvoid folio_prep_large_rmappable(struct folio *folio);\nbool can_split_folio(struct folio *folio, int *pextra_pins);\nint split_huge_page_to_list(struct page *page, struct list_head *list);\nstatic inline int split_huge_page(struct page *page)\n{\n\treturn split_huge_page_to_list(page, NULL);\n}\nvoid deferred_split_folio(struct folio *folio);\n\nvoid __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct folio *folio);\n\n#define split_huge_pmd(__vma, __pmd, __address)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpmd_t *____pmd = (__pmd);\t\t\t\t\\\n\t\tif (is_swap_pmd(*____pmd) || pmd_trans_huge(*____pmd)\t\\\n\t\t\t\t\t|| pmd_devmap(*____pmd))\t\\\n\t\t\t__split_huge_pmd(__vma, __pmd, __address,\t\\\n\t\t\t\t\t\tfalse, NULL);\t\t\\\n\t}  while (0)\n\n\nvoid split_huge_pmd_address(struct vm_area_struct *vma, unsigned long address,\n\t\tbool freeze, struct folio *folio);\n\nvoid __split_huge_pud(struct vm_area_struct *vma, pud_t *pud,\n\t\tunsigned long address);\n\n#define split_huge_pud(__vma, __pud, __address)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tpud_t *____pud = (__pud);\t\t\t\t\\\n\t\tif (pud_trans_huge(*____pud)\t\t\t\t\\\n\t\t\t\t\t|| pud_devmap(*____pud))\t\\\n\t\t\t__split_huge_pud(__vma, __pud, __address);\t\\\n\t}  while (0)\n\nint hugepage_madvise(struct vm_area_struct *vma, unsigned long *vm_flags,\n\t\t     int advice);\nint madvise_collapse(struct vm_area_struct *vma,\n\t\t     struct vm_area_struct **prev,\n\t\t     unsigned long start, unsigned long end);\nvoid vma_adjust_trans_huge(struct vm_area_struct *vma, unsigned long start,\n\t\t\t   unsigned long end, long adjust_next);\nspinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma);\nspinlock_t *__pud_trans_huge_lock(pud_t *pud, struct vm_area_struct *vma);\n\nstatic inline int is_swap_pmd(pmd_t pmd)\n{\n\treturn !pmd_none(pmd) && !pmd_present(pmd);\n}\n\n \nstatic inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,\n\t\tstruct vm_area_struct *vma)\n{\n\tif (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd))\n\t\treturn __pmd_trans_huge_lock(pmd, vma);\n\telse\n\t\treturn NULL;\n}\nstatic inline spinlock_t *pud_trans_huge_lock(pud_t *pud,\n\t\tstruct vm_area_struct *vma)\n{\n\tif (pud_trans_huge(*pud) || pud_devmap(*pud))\n\t\treturn __pud_trans_huge_lock(pud, vma);\n\telse\n\t\treturn NULL;\n}\n\n \nstatic inline bool folio_test_pmd_mappable(struct folio *folio)\n{\n\treturn folio_order(folio) >= HPAGE_PMD_ORDER;\n}\n\nstruct page *follow_devmap_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\tpmd_t *pmd, int flags, struct dev_pagemap **pgmap);\nstruct page *follow_devmap_pud(struct vm_area_struct *vma, unsigned long addr,\n\t\tpud_t *pud, int flags, struct dev_pagemap **pgmap);\n\nvm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf);\n\nextern struct page *huge_zero_page;\nextern unsigned long huge_zero_pfn;\n\nstatic inline bool is_huge_zero_page(struct page *page)\n{\n\treturn READ_ONCE(huge_zero_page) == page;\n}\n\nstatic inline bool is_huge_zero_pmd(pmd_t pmd)\n{\n\treturn pmd_present(pmd) && READ_ONCE(huge_zero_pfn) == pmd_pfn(pmd);\n}\n\nstatic inline bool is_huge_zero_pud(pud_t pud)\n{\n\treturn false;\n}\n\nstruct page *mm_get_huge_zero_page(struct mm_struct *mm);\nvoid mm_put_huge_zero_page(struct mm_struct *mm);\n\n#define mk_huge_pmd(page, prot) pmd_mkhuge(mk_pmd(page, prot))\n\nstatic inline bool thp_migration_supported(void)\n{\n\treturn IS_ENABLED(CONFIG_ARCH_ENABLE_THP_MIGRATION);\n}\n\n#else  \n#define HPAGE_PMD_SHIFT ({ BUILD_BUG(); 0; })\n#define HPAGE_PMD_MASK ({ BUILD_BUG(); 0; })\n#define HPAGE_PMD_SIZE ({ BUILD_BUG(); 0; })\n\n#define HPAGE_PUD_SHIFT ({ BUILD_BUG(); 0; })\n#define HPAGE_PUD_MASK ({ BUILD_BUG(); 0; })\n#define HPAGE_PUD_SIZE ({ BUILD_BUG(); 0; })\n\nstatic inline bool folio_test_pmd_mappable(struct folio *folio)\n{\n\treturn false;\n}\n\nstatic inline bool transhuge_vma_suitable(struct vm_area_struct *vma,\n\t\tunsigned long addr)\n{\n\treturn false;\n}\n\nstatic inline bool hugepage_vma_check(struct vm_area_struct *vma,\n\t\t\t\t      unsigned long vm_flags, bool smaps,\n\t\t\t\t      bool in_pf, bool enforce_sysfs)\n{\n\treturn false;\n}\n\nstatic inline void folio_prep_large_rmappable(struct folio *folio) {}\n\n#define transparent_hugepage_flags 0UL\n\n#define thp_get_unmapped_area\tNULL\n\nstatic inline bool\ncan_split_folio(struct folio *folio, int *pextra_pins)\n{\n\treturn false;\n}\nstatic inline int\nsplit_huge_page_to_list(struct page *page, struct list_head *list)\n{\n\treturn 0;\n}\nstatic inline int split_huge_page(struct page *page)\n{\n\treturn 0;\n}\nstatic inline void deferred_split_folio(struct folio *folio) {}\n#define split_huge_pmd(__vma, __pmd, __address)\t\\\n\tdo { } while (0)\n\nstatic inline void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct folio *folio) {}\nstatic inline void split_huge_pmd_address(struct vm_area_struct *vma,\n\t\tunsigned long address, bool freeze, struct folio *folio) {}\n\n#define split_huge_pud(__vma, __pmd, __address)\t\\\n\tdo { } while (0)\n\nstatic inline int hugepage_madvise(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long *vm_flags, int advice)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int madvise_collapse(struct vm_area_struct *vma,\n\t\t\t\t   struct vm_area_struct **prev,\n\t\t\t\t   unsigned long start, unsigned long end)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void vma_adjust_trans_huge(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long start,\n\t\t\t\t\t unsigned long end,\n\t\t\t\t\t long adjust_next)\n{\n}\nstatic inline int is_swap_pmd(pmd_t pmd)\n{\n\treturn 0;\n}\nstatic inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,\n\t\tstruct vm_area_struct *vma)\n{\n\treturn NULL;\n}\nstatic inline spinlock_t *pud_trans_huge_lock(pud_t *pud,\n\t\tstruct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nstatic inline vm_fault_t do_huge_pmd_numa_page(struct vm_fault *vmf)\n{\n\treturn 0;\n}\n\nstatic inline bool is_huge_zero_page(struct page *page)\n{\n\treturn false;\n}\n\nstatic inline bool is_huge_zero_pmd(pmd_t pmd)\n{\n\treturn false;\n}\n\nstatic inline bool is_huge_zero_pud(pud_t pud)\n{\n\treturn false;\n}\n\nstatic inline void mm_put_huge_zero_page(struct mm_struct *mm)\n{\n\treturn;\n}\n\nstatic inline struct page *follow_devmap_pmd(struct vm_area_struct *vma,\n\tunsigned long addr, pmd_t *pmd, int flags, struct dev_pagemap **pgmap)\n{\n\treturn NULL;\n}\n\nstatic inline struct page *follow_devmap_pud(struct vm_area_struct *vma,\n\tunsigned long addr, pud_t *pud, int flags, struct dev_pagemap **pgmap)\n{\n\treturn NULL;\n}\n\nstatic inline bool thp_migration_supported(void)\n{\n\treturn false;\n}\n#endif  \n\nstatic inline int split_folio_to_list(struct folio *folio,\n\t\tstruct list_head *list)\n{\n\treturn split_huge_page_to_list(&folio->page, list);\n}\n\nstatic inline int split_folio(struct folio *folio)\n{\n\treturn split_folio_to_list(folio, NULL);\n}\n\n \n#ifndef arch_thp_swp_supported\nstatic inline bool arch_thp_swp_supported(void)\n{\n\treturn true;\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}