{
  "module_name": "netdevice.h",
  "hash_id": "cc2d92697a6ddfaa79a6807d17eced5b271e39eaa369ab5ddc4d591fdcbc169c",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/netdevice.h",
  "human_readable_source": " \n \n#ifndef _LINUX_NETDEVICE_H\n#define _LINUX_NETDEVICE_H\n\n#include <linux/timer.h>\n#include <linux/bug.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/prefetch.h>\n#include <asm/cache.h>\n#include <asm/byteorder.h>\n#include <asm/local.h>\n\n#include <linux/percpu.h>\n#include <linux/rculist.h>\n#include <linux/workqueue.h>\n#include <linux/dynamic_queue_limits.h>\n\n#include <net/net_namespace.h>\n#ifdef CONFIG_DCB\n#include <net/dcbnl.h>\n#endif\n#include <net/netprio_cgroup.h>\n\n#include <linux/netdev_features.h>\n#include <linux/neighbour.h>\n#include <uapi/linux/netdevice.h>\n#include <uapi/linux/if_bonding.h>\n#include <uapi/linux/pkt_cls.h>\n#include <uapi/linux/netdev.h>\n#include <linux/hashtable.h>\n#include <linux/rbtree.h>\n#include <net/net_trackers.h>\n#include <net/net_debug.h>\n#include <net/dropreason-core.h>\n\nstruct netpoll_info;\nstruct device;\nstruct ethtool_ops;\nstruct kernel_hwtstamp_config;\nstruct phy_device;\nstruct dsa_port;\nstruct ip_tunnel_parm;\nstruct macsec_context;\nstruct macsec_ops;\nstruct netdev_name_node;\nstruct sd_flow_limit;\nstruct sfp_bus;\n \nstruct wireless_dev;\n \nstruct wpan_dev;\nstruct mpls_dev;\n \nstruct udp_tunnel_info;\nstruct udp_tunnel_nic_info;\nstruct udp_tunnel_nic;\nstruct bpf_prog;\nstruct xdp_buff;\nstruct xdp_frame;\nstruct xdp_metadata_ops;\nstruct xdp_md;\n\ntypedef u32 xdp_features_t;\n\nvoid synchronize_net(void);\nvoid netdev_set_default_ethtool_ops(struct net_device *dev,\n\t\t\t\t    const struct ethtool_ops *ops);\nvoid netdev_sw_irq_coalesce_default_on(struct net_device *dev);\n\n \n#define NET_RX_SUCCESS\t\t0\t \n#define NET_RX_DROP\t\t1\t \n\n#define MAX_NEST_DEV 8\n\n \n\n \n#define NET_XMIT_SUCCESS\t0x00\n#define NET_XMIT_DROP\t\t0x01\t \n#define NET_XMIT_CN\t\t0x02\t \n#define NET_XMIT_MASK\t\t0x0f\t \n\n \n#define net_xmit_eval(e)\t((e) == NET_XMIT_CN ? 0 : (e))\n#define net_xmit_errno(e)\t((e) != NET_XMIT_CN ? -ENOBUFS : 0)\n\n \n#define NETDEV_TX_MASK\t\t0xf0\n\nenum netdev_tx {\n\t__NETDEV_TX_MIN\t = INT_MIN,\t \n\tNETDEV_TX_OK\t = 0x00,\t \n\tNETDEV_TX_BUSY\t = 0x10,\t \n};\ntypedef enum netdev_tx netdev_tx_t;\n\n \nstatic inline bool dev_xmit_complete(int rc)\n{\n\t \n\tif (likely(rc < NET_XMIT_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\n \n\n#if defined(CONFIG_HYPERV_NET)\n# define LL_MAX_HEADER 128\n#elif defined(CONFIG_WLAN) || IS_ENABLED(CONFIG_AX25)\n# if defined(CONFIG_MAC80211_MESH)\n#  define LL_MAX_HEADER 128\n# else\n#  define LL_MAX_HEADER 96\n# endif\n#else\n# define LL_MAX_HEADER 32\n#endif\n\n#if !IS_ENABLED(CONFIG_NET_IPIP) && !IS_ENABLED(CONFIG_NET_IPGRE) && \\\n    !IS_ENABLED(CONFIG_IPV6_SIT) && !IS_ENABLED(CONFIG_IPV6_TUNNEL)\n#define MAX_HEADER LL_MAX_HEADER\n#else\n#define MAX_HEADER (LL_MAX_HEADER + 48)\n#endif\n\n \n\n#define NET_DEV_STAT(FIELD)\t\t\t\\\n\tunion {\t\t\t\t\t\\\n\t\tunsigned long FIELD;\t\t\\\n\t\tatomic_long_t __##FIELD;\t\\\n\t}\n\nstruct net_device_stats {\n\tNET_DEV_STAT(rx_packets);\n\tNET_DEV_STAT(tx_packets);\n\tNET_DEV_STAT(rx_bytes);\n\tNET_DEV_STAT(tx_bytes);\n\tNET_DEV_STAT(rx_errors);\n\tNET_DEV_STAT(tx_errors);\n\tNET_DEV_STAT(rx_dropped);\n\tNET_DEV_STAT(tx_dropped);\n\tNET_DEV_STAT(multicast);\n\tNET_DEV_STAT(collisions);\n\tNET_DEV_STAT(rx_length_errors);\n\tNET_DEV_STAT(rx_over_errors);\n\tNET_DEV_STAT(rx_crc_errors);\n\tNET_DEV_STAT(rx_frame_errors);\n\tNET_DEV_STAT(rx_fifo_errors);\n\tNET_DEV_STAT(rx_missed_errors);\n\tNET_DEV_STAT(tx_aborted_errors);\n\tNET_DEV_STAT(tx_carrier_errors);\n\tNET_DEV_STAT(tx_fifo_errors);\n\tNET_DEV_STAT(tx_heartbeat_errors);\n\tNET_DEV_STAT(tx_window_errors);\n\tNET_DEV_STAT(rx_compressed);\n\tNET_DEV_STAT(tx_compressed);\n};\n#undef NET_DEV_STAT\n\n \nstruct net_device_core_stats {\n\tunsigned long\trx_dropped;\n\tunsigned long\ttx_dropped;\n\tunsigned long\trx_nohandler;\n\tunsigned long\trx_otherhost_dropped;\n} __aligned(4 * sizeof(unsigned long));\n\n#include <linux/cache.h>\n#include <linux/skbuff.h>\n\n#ifdef CONFIG_RPS\n#include <linux/static_key.h>\nextern struct static_key_false rps_needed;\nextern struct static_key_false rfs_needed;\n#endif\n\nstruct neighbour;\nstruct neigh_parms;\nstruct sk_buff;\n\nstruct netdev_hw_addr {\n\tstruct list_head\tlist;\n\tstruct rb_node\t\tnode;\n\tunsigned char\t\taddr[MAX_ADDR_LEN];\n\tunsigned char\t\ttype;\n#define NETDEV_HW_ADDR_T_LAN\t\t1\n#define NETDEV_HW_ADDR_T_SAN\t\t2\n#define NETDEV_HW_ADDR_T_UNICAST\t3\n#define NETDEV_HW_ADDR_T_MULTICAST\t4\n\tbool\t\t\tglobal_use;\n\tint\t\t\tsync_cnt;\n\tint\t\t\trefcount;\n\tint\t\t\tsynced;\n\tstruct rcu_head\t\trcu_head;\n};\n\nstruct netdev_hw_addr_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n\n\t \n\tstruct rb_root\t\ttree;\n};\n\n#define netdev_hw_addr_list_count(l) ((l)->count)\n#define netdev_hw_addr_list_empty(l) (netdev_hw_addr_list_count(l) == 0)\n#define netdev_hw_addr_list_for_each(ha, l) \\\n\tlist_for_each_entry(ha, &(l)->list, list)\n\n#define netdev_uc_count(dev) netdev_hw_addr_list_count(&(dev)->uc)\n#define netdev_uc_empty(dev) netdev_hw_addr_list_empty(&(dev)->uc)\n#define netdev_for_each_uc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->uc)\n#define netdev_for_each_synced_uc_addr(_ha, _dev) \\\n\tnetdev_for_each_uc_addr((_ha), (_dev)) \\\n\t\tif ((_ha)->sync_cnt)\n\n#define netdev_mc_count(dev) netdev_hw_addr_list_count(&(dev)->mc)\n#define netdev_mc_empty(dev) netdev_hw_addr_list_empty(&(dev)->mc)\n#define netdev_for_each_mc_addr(ha, dev) \\\n\tnetdev_hw_addr_list_for_each(ha, &(dev)->mc)\n#define netdev_for_each_synced_mc_addr(_ha, _dev) \\\n\tnetdev_for_each_mc_addr((_ha), (_dev)) \\\n\t\tif ((_ha)->sync_cnt)\n\nstruct hh_cache {\n\tunsigned int\thh_len;\n\tseqlock_t\thh_lock;\n\n\t \n#define HH_DATA_MOD\t16\n#define HH_DATA_OFF(__len) \\\n\t(HH_DATA_MOD - (((__len - 1) & (HH_DATA_MOD - 1)) + 1))\n#define HH_DATA_ALIGN(__len) \\\n\t(((__len)+(HH_DATA_MOD-1))&~(HH_DATA_MOD - 1))\n\tunsigned long\thh_data[HH_DATA_ALIGN(LL_MAX_HEADER) / sizeof(long)];\n};\n\n \n#define LL_RESERVED_SPACE(dev) \\\n\t((((dev)->hard_header_len + READ_ONCE((dev)->needed_headroom)) \\\n\t  & ~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n#define LL_RESERVED_SPACE_EXTRA(dev,extra) \\\n\t((((dev)->hard_header_len + READ_ONCE((dev)->needed_headroom) + (extra)) \\\n\t  & ~(HH_DATA_MOD - 1)) + HH_DATA_MOD)\n\nstruct header_ops {\n\tint\t(*create) (struct sk_buff *skb, struct net_device *dev,\n\t\t\t   unsigned short type, const void *daddr,\n\t\t\t   const void *saddr, unsigned int len);\n\tint\t(*parse)(const struct sk_buff *skb, unsigned char *haddr);\n\tint\t(*cache)(const struct neighbour *neigh, struct hh_cache *hh, __be16 type);\n\tvoid\t(*cache_update)(struct hh_cache *hh,\n\t\t\t\tconst struct net_device *dev,\n\t\t\t\tconst unsigned char *haddr);\n\tbool\t(*validate)(const char *ll_header, unsigned int len);\n\t__be16\t(*parse_protocol)(const struct sk_buff *skb);\n};\n\n \n\nenum netdev_state_t {\n\t__LINK_STATE_START,\n\t__LINK_STATE_PRESENT,\n\t__LINK_STATE_NOCARRIER,\n\t__LINK_STATE_LINKWATCH_PENDING,\n\t__LINK_STATE_DORMANT,\n\t__LINK_STATE_TESTING,\n};\n\nstruct gro_list {\n\tstruct list_head\tlist;\n\tint\t\t\tcount;\n};\n\n \n#define GRO_HASH_BUCKETS\t8\n\n \nstruct napi_struct {\n\t \n\tstruct list_head\tpoll_list;\n\n\tunsigned long\t\tstate;\n\tint\t\t\tweight;\n\tint\t\t\tdefer_hard_irqs_count;\n\tunsigned long\t\tgro_bitmask;\n\tint\t\t\t(*poll)(struct napi_struct *, int);\n#ifdef CONFIG_NETPOLL\n\t \n\tint\t\t\tpoll_owner;\n#endif\n\t \n\tint\t\t\tlist_owner;\n\tstruct net_device\t*dev;\n\tstruct gro_list\t\tgro_hash[GRO_HASH_BUCKETS];\n\tstruct sk_buff\t\t*skb;\n\tstruct list_head\trx_list;  \n\tint\t\t\trx_count;  \n\tunsigned int\t\tnapi_id;\n\tstruct hrtimer\t\ttimer;\n\tstruct task_struct\t*thread;\n\t \n\tstruct list_head\tdev_list;\n\tstruct hlist_node\tnapi_hash_node;\n};\n\nenum {\n\tNAPI_STATE_SCHED,\t\t \n\tNAPI_STATE_MISSED,\t\t \n\tNAPI_STATE_DISABLE,\t\t \n\tNAPI_STATE_NPSVC,\t\t \n\tNAPI_STATE_LISTED,\t\t \n\tNAPI_STATE_NO_BUSY_POLL,\t \n\tNAPI_STATE_IN_BUSY_POLL,\t \n\tNAPI_STATE_PREFER_BUSY_POLL,\t \n\tNAPI_STATE_THREADED,\t\t \n\tNAPI_STATE_SCHED_THREADED,\t \n};\n\nenum {\n\tNAPIF_STATE_SCHED\t\t= BIT(NAPI_STATE_SCHED),\n\tNAPIF_STATE_MISSED\t\t= BIT(NAPI_STATE_MISSED),\n\tNAPIF_STATE_DISABLE\t\t= BIT(NAPI_STATE_DISABLE),\n\tNAPIF_STATE_NPSVC\t\t= BIT(NAPI_STATE_NPSVC),\n\tNAPIF_STATE_LISTED\t\t= BIT(NAPI_STATE_LISTED),\n\tNAPIF_STATE_NO_BUSY_POLL\t= BIT(NAPI_STATE_NO_BUSY_POLL),\n\tNAPIF_STATE_IN_BUSY_POLL\t= BIT(NAPI_STATE_IN_BUSY_POLL),\n\tNAPIF_STATE_PREFER_BUSY_POLL\t= BIT(NAPI_STATE_PREFER_BUSY_POLL),\n\tNAPIF_STATE_THREADED\t\t= BIT(NAPI_STATE_THREADED),\n\tNAPIF_STATE_SCHED_THREADED\t= BIT(NAPI_STATE_SCHED_THREADED),\n};\n\nenum gro_result {\n\tGRO_MERGED,\n\tGRO_MERGED_FREE,\n\tGRO_HELD,\n\tGRO_NORMAL,\n\tGRO_CONSUMED,\n};\ntypedef enum gro_result gro_result_t;\n\n \n\nenum rx_handler_result {\n\tRX_HANDLER_CONSUMED,\n\tRX_HANDLER_ANOTHER,\n\tRX_HANDLER_EXACT,\n\tRX_HANDLER_PASS,\n};\ntypedef enum rx_handler_result rx_handler_result_t;\ntypedef rx_handler_result_t rx_handler_func_t(struct sk_buff **pskb);\n\nvoid __napi_schedule(struct napi_struct *n);\nvoid __napi_schedule_irqoff(struct napi_struct *n);\n\nstatic inline bool napi_disable_pending(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_DISABLE, &n->state);\n}\n\nstatic inline bool napi_prefer_busy_poll(struct napi_struct *n)\n{\n\treturn test_bit(NAPI_STATE_PREFER_BUSY_POLL, &n->state);\n}\n\nbool napi_schedule_prep(struct napi_struct *n);\n\n \nstatic inline void napi_schedule(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule(n);\n}\n\n \nstatic inline void napi_schedule_irqoff(struct napi_struct *n)\n{\n\tif (napi_schedule_prep(n))\n\t\t__napi_schedule_irqoff(n);\n}\n\n \nstatic inline bool napi_reschedule(struct napi_struct *napi)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\t__napi_schedule(napi);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool napi_complete_done(struct napi_struct *n, int work_done);\n\nstatic inline bool napi_complete(struct napi_struct *n)\n{\n\treturn napi_complete_done(n, 0);\n}\n\nint dev_set_threaded(struct net_device *dev, bool threaded);\n\n \nvoid napi_disable(struct napi_struct *n);\n\nvoid napi_enable(struct napi_struct *n);\n\n \nstatic inline void napi_synchronize(const struct napi_struct *n)\n{\n\tif (IS_ENABLED(CONFIG_SMP))\n\t\twhile (test_bit(NAPI_STATE_SCHED, &n->state))\n\t\t\tmsleep(1);\n\telse\n\t\tbarrier();\n}\n\n \nstatic inline bool napi_if_scheduled_mark_missed(struct napi_struct *n)\n{\n\tunsigned long val, new;\n\n\tval = READ_ONCE(n->state);\n\tdo {\n\t\tif (val & NAPIF_STATE_DISABLE)\n\t\t\treturn true;\n\n\t\tif (!(val & NAPIF_STATE_SCHED))\n\t\t\treturn false;\n\n\t\tnew = val | NAPIF_STATE_MISSED;\n\t} while (!try_cmpxchg(&n->state, &val, new));\n\n\treturn true;\n}\n\nenum netdev_queue_state_t {\n\t__QUEUE_STATE_DRV_XOFF,\n\t__QUEUE_STATE_STACK_XOFF,\n\t__QUEUE_STATE_FROZEN,\n};\n\n#define QUEUE_STATE_DRV_XOFF\t(1 << __QUEUE_STATE_DRV_XOFF)\n#define QUEUE_STATE_STACK_XOFF\t(1 << __QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_FROZEN\t(1 << __QUEUE_STATE_FROZEN)\n\n#define QUEUE_STATE_ANY_XOFF\t(QUEUE_STATE_DRV_XOFF | QUEUE_STATE_STACK_XOFF)\n#define QUEUE_STATE_ANY_XOFF_OR_FROZEN (QUEUE_STATE_ANY_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n#define QUEUE_STATE_DRV_XOFF_OR_FROZEN (QUEUE_STATE_DRV_XOFF | \\\n\t\t\t\t\tQUEUE_STATE_FROZEN)\n\n \n\nstruct netdev_queue {\n \n\tstruct net_device\t*dev;\n\tnetdevice_tracker\tdev_tracker;\n\n\tstruct Qdisc __rcu\t*qdisc;\n\tstruct Qdisc __rcu\t*qdisc_sleeping;\n#ifdef CONFIG_SYSFS\n\tstruct kobject\t\tkobj;\n#endif\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tint\t\t\tnuma_node;\n#endif\n\tunsigned long\t\ttx_maxrate;\n\t \n\tatomic_long_t\t\ttrans_timeout;\n\n\t \n\tstruct net_device\t*sb_dev;\n#ifdef CONFIG_XDP_SOCKETS\n\tstruct xsk_buff_pool    *pool;\n#endif\n \n\tspinlock_t\t\t_xmit_lock ____cacheline_aligned_in_smp;\n\tint\t\t\txmit_lock_owner;\n\t \n\tunsigned long\t\ttrans_start;\n\n\tunsigned long\t\tstate;\n\n#ifdef CONFIG_BQL\n\tstruct dql\t\tdql;\n#endif\n} ____cacheline_aligned_in_smp;\n\nextern int sysctl_fb_tunnels_only_for_init_net;\nextern int sysctl_devconf_inherit_init_net;\n\n \nstatic inline bool net_has_fallback_tunnels(const struct net *net)\n{\n#if IS_ENABLED(CONFIG_SYSCTL)\n\tint fb_tunnels_only_for_init_net = READ_ONCE(sysctl_fb_tunnels_only_for_init_net);\n\n\treturn !fb_tunnels_only_for_init_net ||\n\t\t(net_eq(net, &init_net) && fb_tunnels_only_for_init_net == 1);\n#else\n\treturn true;\n#endif\n}\n\nstatic inline int net_inherit_devconf(void)\n{\n#if IS_ENABLED(CONFIG_SYSCTL)\n\treturn READ_ONCE(sysctl_devconf_inherit_init_net);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline int netdev_queue_numa_node_read(const struct netdev_queue *q)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\treturn q->numa_node;\n#else\n\treturn NUMA_NO_NODE;\n#endif\n}\n\nstatic inline void netdev_queue_numa_node_write(struct netdev_queue *q, int node)\n{\n#if defined(CONFIG_XPS) && defined(CONFIG_NUMA)\n\tq->numa_node = node;\n#endif\n}\n\n#ifdef CONFIG_RPS\n \nstruct rps_map {\n\tunsigned int len;\n\tstruct rcu_head rcu;\n\tu16 cpus[];\n};\n#define RPS_MAP_SIZE(_num) (sizeof(struct rps_map) + ((_num) * sizeof(u16)))\n\n \nstruct rps_dev_flow {\n\tu16 cpu;\n\tu16 filter;\n\tunsigned int last_qtail;\n};\n#define RPS_NO_FILTER 0xffff\n\n \nstruct rps_dev_flow_table {\n\tunsigned int mask;\n\tstruct rcu_head rcu;\n\tstruct rps_dev_flow flows[];\n};\n#define RPS_DEV_FLOW_TABLE_SIZE(_num) (sizeof(struct rps_dev_flow_table) + \\\n    ((_num) * sizeof(struct rps_dev_flow)))\n\n \nstruct rps_sock_flow_table {\n\tu32\tmask;\n\n\tu32\tents[] ____cacheline_aligned_in_smp;\n};\n#define\tRPS_SOCK_FLOW_TABLE_SIZE(_num) (offsetof(struct rps_sock_flow_table, ents[_num]))\n\n#define RPS_NO_CPU 0xffff\n\nextern u32 rps_cpu_mask;\nextern struct rps_sock_flow_table __rcu *rps_sock_flow_table;\n\nstatic inline void rps_record_sock_flow(struct rps_sock_flow_table *table,\n\t\t\t\t\tu32 hash)\n{\n\tif (table && hash) {\n\t\tunsigned int index = hash & table->mask;\n\t\tu32 val = hash & ~rps_cpu_mask;\n\n\t\t \n\t\tval |= raw_smp_processor_id();\n\n\t\t \n\t\tif (READ_ONCE(table->ents[index]) != val)\n\t\t\tWRITE_ONCE(table->ents[index], val);\n\t}\n}\n\n#ifdef CONFIG_RFS_ACCEL\nbool rps_may_expire_flow(struct net_device *dev, u16 rxq_index, u32 flow_id,\n\t\t\t u16 filter_id);\n#endif\n#endif  \n\n \nenum xps_map_type {\n\tXPS_CPUS = 0,\n\tXPS_RXQS,\n\tXPS_MAPS_MAX,\n};\n\n#ifdef CONFIG_XPS\n \nstruct xps_map {\n\tunsigned int len;\n\tunsigned int alloc_len;\n\tstruct rcu_head rcu;\n\tu16 queues[];\n};\n#define XPS_MAP_SIZE(_num) (sizeof(struct xps_map) + ((_num) * sizeof(u16)))\n#define XPS_MIN_MAP_ALLOC ((L1_CACHE_ALIGN(offsetof(struct xps_map, queues[1])) \\\n       - sizeof(struct xps_map)) / sizeof(u16))\n\n \nstruct xps_dev_maps {\n\tstruct rcu_head rcu;\n\tunsigned int nr_ids;\n\ts16 num_tc;\n\tstruct xps_map __rcu *attr_map[];  \n};\n\n#define XPS_CPU_DEV_MAPS_SIZE(_tcs) (sizeof(struct xps_dev_maps) +\t\\\n\t(nr_cpu_ids * (_tcs) * sizeof(struct xps_map *)))\n\n#define XPS_RXQ_DEV_MAPS_SIZE(_tcs, _rxqs) (sizeof(struct xps_dev_maps) +\\\n\t(_rxqs * (_tcs) * sizeof(struct xps_map *)))\n\n#endif  \n\n#define TC_MAX_QUEUE\t16\n#define TC_BITMASK\t15\n \nstruct netdev_tc_txq {\n\tu16 count;\n\tu16 offset;\n};\n\n#if defined(CONFIG_FCOE) || defined(CONFIG_FCOE_MODULE)\n \nstruct netdev_fcoe_hbainfo {\n\tchar\tmanufacturer[64];\n\tchar\tserial_number[64];\n\tchar\thardware_version[64];\n\tchar\tdriver_version[64];\n\tchar\toptionrom_version[64];\n\tchar\tfirmware_version[64];\n\tchar\tmodel[256];\n\tchar\tmodel_description[256];\n};\n#endif\n\n#define MAX_PHYS_ITEM_ID_LEN 32\n\n \nstruct netdev_phys_item_id {\n\tunsigned char id[MAX_PHYS_ITEM_ID_LEN];\n\tunsigned char id_len;\n};\n\nstatic inline bool netdev_phys_item_id_same(struct netdev_phys_item_id *a,\n\t\t\t\t\t    struct netdev_phys_item_id *b)\n{\n\treturn a->id_len == b->id_len &&\n\t       memcmp(a->id, b->id, a->id_len) == 0;\n}\n\ntypedef u16 (*select_queue_fallback_t)(struct net_device *dev,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct net_device *sb_dev);\n\nenum net_device_path_type {\n\tDEV_PATH_ETHERNET = 0,\n\tDEV_PATH_VLAN,\n\tDEV_PATH_BRIDGE,\n\tDEV_PATH_PPPOE,\n\tDEV_PATH_DSA,\n\tDEV_PATH_MTK_WDMA,\n};\n\nstruct net_device_path {\n\tenum net_device_path_type\ttype;\n\tconst struct net_device\t\t*dev;\n\tunion {\n\t\tstruct {\n\t\t\tu16\t\tid;\n\t\t\t__be16\t\tproto;\n\t\t\tu8\t\th_dest[ETH_ALEN];\n\t\t} encap;\n\t\tstruct {\n\t\t\tenum {\n\t\t\t\tDEV_PATH_BR_VLAN_KEEP,\n\t\t\t\tDEV_PATH_BR_VLAN_TAG,\n\t\t\t\tDEV_PATH_BR_VLAN_UNTAG,\n\t\t\t\tDEV_PATH_BR_VLAN_UNTAG_HW,\n\t\t\t}\t\tvlan_mode;\n\t\t\tu16\t\tvlan_id;\n\t\t\t__be16\t\tvlan_proto;\n\t\t} bridge;\n\t\tstruct {\n\t\t\tint port;\n\t\t\tu16 proto;\n\t\t} dsa;\n\t\tstruct {\n\t\t\tu8 wdma_idx;\n\t\t\tu8 queue;\n\t\t\tu16 wcid;\n\t\t\tu8 bss;\n\t\t} mtk_wdma;\n\t};\n};\n\n#define NET_DEVICE_PATH_STACK_MAX\t5\n#define NET_DEVICE_PATH_VLAN_MAX\t2\n\nstruct net_device_path_stack {\n\tint\t\t\tnum_paths;\n\tstruct net_device_path\tpath[NET_DEVICE_PATH_STACK_MAX];\n};\n\nstruct net_device_path_ctx {\n\tconst struct net_device *dev;\n\tu8\t\t\tdaddr[ETH_ALEN];\n\n\tint\t\t\tnum_vlans;\n\tstruct {\n\t\tu16\t\tid;\n\t\t__be16\t\tproto;\n\t} vlan[NET_DEVICE_PATH_VLAN_MAX];\n};\n\nenum tc_setup_type {\n\tTC_QUERY_CAPS,\n\tTC_SETUP_QDISC_MQPRIO,\n\tTC_SETUP_CLSU32,\n\tTC_SETUP_CLSFLOWER,\n\tTC_SETUP_CLSMATCHALL,\n\tTC_SETUP_CLSBPF,\n\tTC_SETUP_BLOCK,\n\tTC_SETUP_QDISC_CBS,\n\tTC_SETUP_QDISC_RED,\n\tTC_SETUP_QDISC_PRIO,\n\tTC_SETUP_QDISC_MQ,\n\tTC_SETUP_QDISC_ETF,\n\tTC_SETUP_ROOT_QDISC,\n\tTC_SETUP_QDISC_GRED,\n\tTC_SETUP_QDISC_TAPRIO,\n\tTC_SETUP_FT,\n\tTC_SETUP_QDISC_ETS,\n\tTC_SETUP_QDISC_TBF,\n\tTC_SETUP_QDISC_FIFO,\n\tTC_SETUP_QDISC_HTB,\n\tTC_SETUP_ACT,\n};\n\n \nenum bpf_netdev_command {\n\t \n\tXDP_SETUP_PROG,\n\tXDP_SETUP_PROG_HW,\n\t \n\tBPF_OFFLOAD_MAP_ALLOC,\n\tBPF_OFFLOAD_MAP_FREE,\n\tXDP_SETUP_XSK_POOL,\n};\n\nstruct bpf_prog_offload_ops;\nstruct netlink_ext_ack;\nstruct xdp_umem;\nstruct xdp_dev_bulk_queue;\nstruct bpf_xdp_link;\n\nenum bpf_xdp_mode {\n\tXDP_MODE_SKB = 0,\n\tXDP_MODE_DRV = 1,\n\tXDP_MODE_HW = 2,\n\t__MAX_XDP_MODE\n};\n\nstruct bpf_xdp_entity {\n\tstruct bpf_prog *prog;\n\tstruct bpf_xdp_link *link;\n};\n\nstruct netdev_bpf {\n\tenum bpf_netdev_command command;\n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tu32 flags;\n\t\t\tstruct bpf_prog *prog;\n\t\t\tstruct netlink_ext_ack *extack;\n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tstruct bpf_offloaded_map *offmap;\n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tstruct xsk_buff_pool *pool;\n\t\t\tu16 queue_id;\n\t\t} xsk;\n\t};\n};\n\n \n#define XDP_WAKEUP_RX (1 << 0)\n#define XDP_WAKEUP_TX (1 << 1)\n\n#ifdef CONFIG_XFRM_OFFLOAD\nstruct xfrmdev_ops {\n\tint\t(*xdo_dev_state_add) (struct xfrm_state *x, struct netlink_ext_ack *extack);\n\tvoid\t(*xdo_dev_state_delete) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_free) (struct xfrm_state *x);\n\tbool\t(*xdo_dev_offload_ok) (struct sk_buff *skb,\n\t\t\t\t       struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_advance_esn) (struct xfrm_state *x);\n\tvoid\t(*xdo_dev_state_update_curlft) (struct xfrm_state *x);\n\tint\t(*xdo_dev_policy_add) (struct xfrm_policy *x, struct netlink_ext_ack *extack);\n\tvoid\t(*xdo_dev_policy_delete) (struct xfrm_policy *x);\n\tvoid\t(*xdo_dev_policy_free) (struct xfrm_policy *x);\n};\n#endif\n\nstruct dev_ifalias {\n\tstruct rcu_head rcuhead;\n\tchar ifalias[];\n};\n\nstruct devlink;\nstruct tlsdev_ops;\n\nstruct netdev_net_notifier {\n\tstruct list_head list;\n\tstruct notifier_block *nb;\n};\n\n \nstruct net_device_ops {\n\tint\t\t\t(*ndo_init)(struct net_device *dev);\n\tvoid\t\t\t(*ndo_uninit)(struct net_device *dev);\n\tint\t\t\t(*ndo_open)(struct net_device *dev);\n\tint\t\t\t(*ndo_stop)(struct net_device *dev);\n\tnetdev_tx_t\t\t(*ndo_start_xmit)(struct sk_buff *skb,\n\t\t\t\t\t\t  struct net_device *dev);\n\tnetdev_features_t\t(*ndo_features_check)(struct sk_buff *skb,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      netdev_features_t features);\n\tu16\t\t\t(*ndo_select_queue)(struct net_device *dev,\n\t\t\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t\t\t    struct net_device *sb_dev);\n\tvoid\t\t\t(*ndo_change_rx_flags)(struct net_device *dev,\n\t\t\t\t\t\t       int flags);\n\tvoid\t\t\t(*ndo_set_rx_mode)(struct net_device *dev);\n\tint\t\t\t(*ndo_set_mac_address)(struct net_device *dev,\n\t\t\t\t\t\t       void *addr);\n\tint\t\t\t(*ndo_validate_addr)(struct net_device *dev);\n\tint\t\t\t(*ndo_do_ioctl)(struct net_device *dev,\n\t\t\t\t\t        struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_eth_ioctl)(struct net_device *dev,\n\t\t\t\t\t\t struct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_siocbond)(struct net_device *dev,\n\t\t\t\t\t\tstruct ifreq *ifr, int cmd);\n\tint\t\t\t(*ndo_siocwandev)(struct net_device *dev,\n\t\t\t\t\t\t  struct if_settings *ifs);\n\tint\t\t\t(*ndo_siocdevprivate)(struct net_device *dev,\n\t\t\t\t\t\t      struct ifreq *ifr,\n\t\t\t\t\t\t      void __user *data, int cmd);\n\tint\t\t\t(*ndo_set_config)(struct net_device *dev,\n\t\t\t\t\t          struct ifmap *map);\n\tint\t\t\t(*ndo_change_mtu)(struct net_device *dev,\n\t\t\t\t\t\t  int new_mtu);\n\tint\t\t\t(*ndo_neigh_setup)(struct net_device *dev,\n\t\t\t\t\t\t   struct neigh_parms *);\n\tvoid\t\t\t(*ndo_tx_timeout) (struct net_device *dev,\n\t\t\t\t\t\t   unsigned int txqueue);\n\n\tvoid\t\t\t(*ndo_get_stats64)(struct net_device *dev,\n\t\t\t\t\t\t   struct rtnl_link_stats64 *storage);\n\tbool\t\t\t(*ndo_has_offload_stats)(const struct net_device *dev, int attr_id);\n\tint\t\t\t(*ndo_get_offload_stats)(int attr_id,\n\t\t\t\t\t\t\t const struct net_device *dev,\n\t\t\t\t\t\t\t void *attr_data);\n\tstruct net_device_stats* (*ndo_get_stats)(struct net_device *dev);\n\n\tint\t\t\t(*ndo_vlan_rx_add_vid)(struct net_device *dev,\n\t\t\t\t\t\t       __be16 proto, u16 vid);\n\tint\t\t\t(*ndo_vlan_rx_kill_vid)(struct net_device *dev,\n\t\t\t\t\t\t        __be16 proto, u16 vid);\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\tvoid                    (*ndo_poll_controller)(struct net_device *dev);\n\tint\t\t\t(*ndo_netpoll_setup)(struct net_device *dev,\n\t\t\t\t\t\t     struct netpoll_info *info);\n\tvoid\t\t\t(*ndo_netpoll_cleanup)(struct net_device *dev);\n#endif\n\tint\t\t\t(*ndo_set_vf_mac)(struct net_device *dev,\n\t\t\t\t\t\t  int queue, u8 *mac);\n\tint\t\t\t(*ndo_set_vf_vlan)(struct net_device *dev,\n\t\t\t\t\t\t   int queue, u16 vlan,\n\t\t\t\t\t\t   u8 qos, __be16 proto);\n\tint\t\t\t(*ndo_set_vf_rate)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, int min_tx_rate,\n\t\t\t\t\t\t   int max_tx_rate);\n\tint\t\t\t(*ndo_set_vf_spoofchk)(struct net_device *dev,\n\t\t\t\t\t\t       int vf, bool setting);\n\tint\t\t\t(*ndo_set_vf_trust)(struct net_device *dev,\n\t\t\t\t\t\t    int vf, bool setting);\n\tint\t\t\t(*ndo_get_vf_config)(struct net_device *dev,\n\t\t\t\t\t\t     int vf,\n\t\t\t\t\t\t     struct ifla_vf_info *ivf);\n\tint\t\t\t(*ndo_set_vf_link_state)(struct net_device *dev,\n\t\t\t\t\t\t\t int vf, int link_state);\n\tint\t\t\t(*ndo_get_vf_stats)(struct net_device *dev,\n\t\t\t\t\t\t    int vf,\n\t\t\t\t\t\t    struct ifla_vf_stats\n\t\t\t\t\t\t    *vf_stats);\n\tint\t\t\t(*ndo_set_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct nlattr *port[]);\n\tint\t\t\t(*ndo_get_vf_port)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, struct sk_buff *skb);\n\tint\t\t\t(*ndo_get_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf,\n\t\t\t\t\t\t   struct ifla_vf_guid *node_guid,\n\t\t\t\t\t\t   struct ifla_vf_guid *port_guid);\n\tint\t\t\t(*ndo_set_vf_guid)(struct net_device *dev,\n\t\t\t\t\t\t   int vf, u64 guid,\n\t\t\t\t\t\t   int guid_type);\n\tint\t\t\t(*ndo_set_vf_rss_query_en)(\n\t\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t\t   int vf, bool setting);\n\tint\t\t\t(*ndo_setup_tc)(struct net_device *dev,\n\t\t\t\t\t\tenum tc_setup_type type,\n\t\t\t\t\t\tvoid *type_data);\n#if IS_ENABLED(CONFIG_FCOE)\n\tint\t\t\t(*ndo_fcoe_enable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_disable)(struct net_device *dev);\n\tint\t\t\t(*ndo_fcoe_ddp_setup)(struct net_device *dev,\n\t\t\t\t\t\t      u16 xid,\n\t\t\t\t\t\t      struct scatterlist *sgl,\n\t\t\t\t\t\t      unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_ddp_done)(struct net_device *dev,\n\t\t\t\t\t\t     u16 xid);\n\tint\t\t\t(*ndo_fcoe_ddp_target)(struct net_device *dev,\n\t\t\t\t\t\t       u16 xid,\n\t\t\t\t\t\t       struct scatterlist *sgl,\n\t\t\t\t\t\t       unsigned int sgc);\n\tint\t\t\t(*ndo_fcoe_get_hbainfo)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_fcoe_hbainfo *hbainfo);\n#endif\n\n#if IS_ENABLED(CONFIG_LIBFCOE)\n#define NETDEV_FCOE_WWNN 0\n#define NETDEV_FCOE_WWPN 1\n\tint\t\t\t(*ndo_fcoe_get_wwn)(struct net_device *dev,\n\t\t\t\t\t\t    u64 *wwn, int type);\n#endif\n\n#ifdef CONFIG_RFS_ACCEL\n\tint\t\t\t(*ndo_rx_flow_steer)(struct net_device *dev,\n\t\t\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t\t\t     u16 rxq_index,\n\t\t\t\t\t\t     u32 flow_id);\n#endif\n\tint\t\t\t(*ndo_add_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev,\n\t\t\t\t\t\t struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_del_slave)(struct net_device *dev,\n\t\t\t\t\t\t struct net_device *slave_dev);\n\tstruct net_device*\t(*ndo_get_xmit_slave)(struct net_device *dev,\n\t\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t\t      bool all_slaves);\n\tstruct net_device*\t(*ndo_sk_get_lower_dev)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct sock *sk);\n\tnetdev_features_t\t(*ndo_fix_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_set_features)(struct net_device *dev,\n\t\t\t\t\t\t    netdev_features_t features);\n\tint\t\t\t(*ndo_neigh_construct)(struct net_device *dev,\n\t\t\t\t\t\t       struct neighbour *n);\n\tvoid\t\t\t(*ndo_neigh_destroy)(struct net_device *dev,\n\t\t\t\t\t\t     struct neighbour *n);\n\n\tint\t\t\t(*ndo_fdb_add)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid,\n\t\t\t\t\t       u16 flags,\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_fdb_del)(struct ndmsg *ndm,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid, struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_fdb_del_bulk)(struct ndmsg *ndm,\n\t\t\t\t\t\t    struct nlattr *tb[],\n\t\t\t\t\t\t    struct net_device *dev,\n\t\t\t\t\t\t    u16 vid,\n\t\t\t\t\t\t    struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_fdb_dump)(struct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb,\n\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\tstruct net_device *filter_dev,\n\t\t\t\t\t\tint *idx);\n\tint\t\t\t(*ndo_fdb_get)(struct sk_buff *skb,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct net_device *dev,\n\t\t\t\t\t       const unsigned char *addr,\n\t\t\t\t\t       u16 vid, u32 portid, u32 seq,\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_mdb_add)(struct net_device *dev,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       u16 nlmsg_flags,\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_mdb_del)(struct net_device *dev,\n\t\t\t\t\t       struct nlattr *tb[],\n\t\t\t\t\t       struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_mdb_dump)(struct net_device *dev,\n\t\t\t\t\t\tstruct sk_buff *skb,\n\t\t\t\t\t\tstruct netlink_callback *cb);\n\tint\t\t\t(*ndo_bridge_setlink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags,\n\t\t\t\t\t\t      struct netlink_ext_ack *extack);\n\tint\t\t\t(*ndo_bridge_getlink)(struct sk_buff *skb,\n\t\t\t\t\t\t      u32 pid, u32 seq,\n\t\t\t\t\t\t      struct net_device *dev,\n\t\t\t\t\t\t      u32 filter_mask,\n\t\t\t\t\t\t      int nlflags);\n\tint\t\t\t(*ndo_bridge_dellink)(struct net_device *dev,\n\t\t\t\t\t\t      struct nlmsghdr *nlh,\n\t\t\t\t\t\t      u16 flags);\n\tint\t\t\t(*ndo_change_carrier)(struct net_device *dev,\n\t\t\t\t\t\t      bool new_carrier);\n\tint\t\t\t(*ndo_get_phys_port_id)(struct net_device *dev,\n\t\t\t\t\t\t\tstruct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_port_parent_id)(struct net_device *dev,\n\t\t\t\t\t\t\t  struct netdev_phys_item_id *ppid);\n\tint\t\t\t(*ndo_get_phys_port_name)(struct net_device *dev,\n\t\t\t\t\t\t\t  char *name, size_t len);\n\tvoid*\t\t\t(*ndo_dfwd_add_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tstruct net_device *dev);\n\tvoid\t\t\t(*ndo_dfwd_del_station)(struct net_device *pdev,\n\t\t\t\t\t\t\tvoid *priv);\n\n\tint\t\t\t(*ndo_set_tx_maxrate)(struct net_device *dev,\n\t\t\t\t\t\t      int queue_index,\n\t\t\t\t\t\t      u32 maxrate);\n\tint\t\t\t(*ndo_get_iflink)(const struct net_device *dev);\n\tint\t\t\t(*ndo_fill_metadata_dst)(struct net_device *dev,\n\t\t\t\t\t\t       struct sk_buff *skb);\n\tvoid\t\t\t(*ndo_set_rx_headroom)(struct net_device *dev,\n\t\t\t\t\t\t       int needed_headroom);\n\tint\t\t\t(*ndo_bpf)(struct net_device *dev,\n\t\t\t\t\t   struct netdev_bpf *bpf);\n\tint\t\t\t(*ndo_xdp_xmit)(struct net_device *dev, int n,\n\t\t\t\t\t\tstruct xdp_frame **xdp,\n\t\t\t\t\t\tu32 flags);\n\tstruct net_device *\t(*ndo_xdp_get_xmit_slave)(struct net_device *dev,\n\t\t\t\t\t\t\t  struct xdp_buff *xdp);\n\tint\t\t\t(*ndo_xsk_wakeup)(struct net_device *dev,\n\t\t\t\t\t\t  u32 queue_id, u32 flags);\n\tint\t\t\t(*ndo_tunnel_ctl)(struct net_device *dev,\n\t\t\t\t\t\t  struct ip_tunnel_parm *p, int cmd);\n\tstruct net_device *\t(*ndo_get_peer_dev)(struct net_device *dev);\n\tint                     (*ndo_fill_forward_path)(struct net_device_path_ctx *ctx,\n                                                         struct net_device_path *path);\n\tktime_t\t\t\t(*ndo_get_tstamp)(struct net_device *dev,\n\t\t\t\t\t\t  const struct skb_shared_hwtstamps *hwtstamps,\n\t\t\t\t\t\t  bool cycles);\n\tint\t\t\t(*ndo_hwtstamp_get)(struct net_device *dev,\n\t\t\t\t\t\t    struct kernel_hwtstamp_config *kernel_config);\n\tint\t\t\t(*ndo_hwtstamp_set)(struct net_device *dev,\n\t\t\t\t\t\t    struct kernel_hwtstamp_config *kernel_config,\n\t\t\t\t\t\t    struct netlink_ext_ack *extack);\n};\n\n \nenum netdev_priv_flags {\n\tIFF_802_1Q_VLAN\t\t\t= 1<<0,\n\tIFF_EBRIDGE\t\t\t= 1<<1,\n\tIFF_BONDING\t\t\t= 1<<2,\n\tIFF_ISATAP\t\t\t= 1<<3,\n\tIFF_WAN_HDLC\t\t\t= 1<<4,\n\tIFF_XMIT_DST_RELEASE\t\t= 1<<5,\n\tIFF_DONT_BRIDGE\t\t\t= 1<<6,\n\tIFF_DISABLE_NETPOLL\t\t= 1<<7,\n\tIFF_MACVLAN_PORT\t\t= 1<<8,\n\tIFF_BRIDGE_PORT\t\t\t= 1<<9,\n\tIFF_OVS_DATAPATH\t\t= 1<<10,\n\tIFF_TX_SKB_SHARING\t\t= 1<<11,\n\tIFF_UNICAST_FLT\t\t\t= 1<<12,\n\tIFF_TEAM_PORT\t\t\t= 1<<13,\n\tIFF_SUPP_NOFCS\t\t\t= 1<<14,\n\tIFF_LIVE_ADDR_CHANGE\t\t= 1<<15,\n\tIFF_MACVLAN\t\t\t= 1<<16,\n\tIFF_XMIT_DST_RELEASE_PERM\t= 1<<17,\n\tIFF_L3MDEV_MASTER\t\t= 1<<18,\n\tIFF_NO_QUEUE\t\t\t= 1<<19,\n\tIFF_OPENVSWITCH\t\t\t= 1<<20,\n\tIFF_L3MDEV_SLAVE\t\t= 1<<21,\n\tIFF_TEAM\t\t\t= 1<<22,\n\tIFF_RXFH_CONFIGURED\t\t= 1<<23,\n\tIFF_PHONY_HEADROOM\t\t= 1<<24,\n\tIFF_MACSEC\t\t\t= 1<<25,\n\tIFF_NO_RX_HANDLER\t\t= 1<<26,\n\tIFF_FAILOVER\t\t\t= 1<<27,\n\tIFF_FAILOVER_SLAVE\t\t= 1<<28,\n\tIFF_L3MDEV_RX_HANDLER\t\t= 1<<29,\n\tIFF_NO_ADDRCONF\t\t\t= BIT_ULL(30),\n\tIFF_TX_SKB_NO_LINEAR\t\t= BIT_ULL(31),\n\tIFF_CHANGE_PROTO_DOWN\t\t= BIT_ULL(32),\n\tIFF_SEE_ALL_HWTSTAMP_REQUESTS\t= BIT_ULL(33),\n};\n\n#define IFF_802_1Q_VLAN\t\t\tIFF_802_1Q_VLAN\n#define IFF_EBRIDGE\t\t\tIFF_EBRIDGE\n#define IFF_BONDING\t\t\tIFF_BONDING\n#define IFF_ISATAP\t\t\tIFF_ISATAP\n#define IFF_WAN_HDLC\t\t\tIFF_WAN_HDLC\n#define IFF_XMIT_DST_RELEASE\t\tIFF_XMIT_DST_RELEASE\n#define IFF_DONT_BRIDGE\t\t\tIFF_DONT_BRIDGE\n#define IFF_DISABLE_NETPOLL\t\tIFF_DISABLE_NETPOLL\n#define IFF_MACVLAN_PORT\t\tIFF_MACVLAN_PORT\n#define IFF_BRIDGE_PORT\t\t\tIFF_BRIDGE_PORT\n#define IFF_OVS_DATAPATH\t\tIFF_OVS_DATAPATH\n#define IFF_TX_SKB_SHARING\t\tIFF_TX_SKB_SHARING\n#define IFF_UNICAST_FLT\t\t\tIFF_UNICAST_FLT\n#define IFF_TEAM_PORT\t\t\tIFF_TEAM_PORT\n#define IFF_SUPP_NOFCS\t\t\tIFF_SUPP_NOFCS\n#define IFF_LIVE_ADDR_CHANGE\t\tIFF_LIVE_ADDR_CHANGE\n#define IFF_MACVLAN\t\t\tIFF_MACVLAN\n#define IFF_XMIT_DST_RELEASE_PERM\tIFF_XMIT_DST_RELEASE_PERM\n#define IFF_L3MDEV_MASTER\t\tIFF_L3MDEV_MASTER\n#define IFF_NO_QUEUE\t\t\tIFF_NO_QUEUE\n#define IFF_OPENVSWITCH\t\t\tIFF_OPENVSWITCH\n#define IFF_L3MDEV_SLAVE\t\tIFF_L3MDEV_SLAVE\n#define IFF_TEAM\t\t\tIFF_TEAM\n#define IFF_RXFH_CONFIGURED\t\tIFF_RXFH_CONFIGURED\n#define IFF_PHONY_HEADROOM\t\tIFF_PHONY_HEADROOM\n#define IFF_MACSEC\t\t\tIFF_MACSEC\n#define IFF_NO_RX_HANDLER\t\tIFF_NO_RX_HANDLER\n#define IFF_FAILOVER\t\t\tIFF_FAILOVER\n#define IFF_FAILOVER_SLAVE\t\tIFF_FAILOVER_SLAVE\n#define IFF_L3MDEV_RX_HANDLER\t\tIFF_L3MDEV_RX_HANDLER\n#define IFF_TX_SKB_NO_LINEAR\t\tIFF_TX_SKB_NO_LINEAR\n\n \nenum netdev_ml_priv_type {\n\tML_PRIV_NONE,\n\tML_PRIV_CAN,\n};\n\nenum netdev_stat_type {\n\tNETDEV_PCPU_STAT_NONE,\n\tNETDEV_PCPU_STAT_LSTATS,  \n\tNETDEV_PCPU_STAT_TSTATS,  \n\tNETDEV_PCPU_STAT_DSTATS,  \n};\n\n \n\nstruct net_device {\n\tchar\t\t\tname[IFNAMSIZ];\n\tstruct netdev_name_node\t*name_node;\n\tstruct dev_ifalias\t__rcu *ifalias;\n\t \n\tunsigned long\t\tmem_end;\n\tunsigned long\t\tmem_start;\n\tunsigned long\t\tbase_addr;\n\n\t \n\n\tunsigned long\t\tstate;\n\n\tstruct list_head\tdev_list;\n\tstruct list_head\tnapi_list;\n\tstruct list_head\tunreg_list;\n\tstruct list_head\tclose_list;\n\tstruct list_head\tptype_all;\n\tstruct list_head\tptype_specific;\n\n\tstruct {\n\t\tstruct list_head upper;\n\t\tstruct list_head lower;\n\t} adj_list;\n\n\t \n\tunsigned int\t\tflags;\n\txdp_features_t\t\txdp_features;\n\tunsigned long long\tpriv_flags;\n\tconst struct net_device_ops *netdev_ops;\n\tconst struct xdp_metadata_ops *xdp_metadata_ops;\n\tint\t\t\tifindex;\n\tunsigned short\t\tgflags;\n\tunsigned short\t\thard_header_len;\n\n\t \n\tunsigned int\t\tmtu;\n\tunsigned short\t\tneeded_headroom;\n\tunsigned short\t\tneeded_tailroom;\n\n\tnetdev_features_t\tfeatures;\n\tnetdev_features_t\thw_features;\n\tnetdev_features_t\twanted_features;\n\tnetdev_features_t\tvlan_features;\n\tnetdev_features_t\thw_enc_features;\n\tnetdev_features_t\tmpls_features;\n\tnetdev_features_t\tgso_partial_features;\n\n\tunsigned int\t\tmin_mtu;\n\tunsigned int\t\tmax_mtu;\n\tunsigned short\t\ttype;\n\tunsigned char\t\tmin_header_len;\n\tunsigned char\t\tname_assign_type;\n\n\tint\t\t\tgroup;\n\n\tstruct net_device_stats\tstats;  \n\n\tstruct net_device_core_stats __percpu *core_stats;\n\n\t \n\tatomic_t\t\tcarrier_up_count;\n\tatomic_t\t\tcarrier_down_count;\n\n#ifdef CONFIG_WIRELESS_EXT\n\tconst struct iw_handler_def *wireless_handlers;\n\tstruct iw_public_data\t*wireless_data;\n#endif\n\tconst struct ethtool_ops *ethtool_ops;\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tconst struct l3mdev_ops\t*l3mdev_ops;\n#endif\n#if IS_ENABLED(CONFIG_IPV6)\n\tconst struct ndisc_ops *ndisc_ops;\n#endif\n\n#ifdef CONFIG_XFRM_OFFLOAD\n\tconst struct xfrmdev_ops *xfrmdev_ops;\n#endif\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\n\tconst struct tlsdev_ops *tlsdev_ops;\n#endif\n\n\tconst struct header_ops *header_ops;\n\n\tunsigned char\t\toperstate;\n\tunsigned char\t\tlink_mode;\n\n\tunsigned char\t\tif_port;\n\tunsigned char\t\tdma;\n\n\t \n\tunsigned char\t\tperm_addr[MAX_ADDR_LEN];\n\tunsigned char\t\taddr_assign_type;\n\tunsigned char\t\taddr_len;\n\tunsigned char\t\tupper_level;\n\tunsigned char\t\tlower_level;\n\n\tunsigned short\t\tneigh_priv_len;\n\tunsigned short          dev_id;\n\tunsigned short          dev_port;\n\tunsigned short\t\tpadded;\n\n\tspinlock_t\t\taddr_list_lock;\n\tint\t\t\tirq;\n\n\tstruct netdev_hw_addr_list\tuc;\n\tstruct netdev_hw_addr_list\tmc;\n\tstruct netdev_hw_addr_list\tdev_addrs;\n\n#ifdef CONFIG_SYSFS\n\tstruct kset\t\t*queues_kset;\n#endif\n#ifdef CONFIG_LOCKDEP\n\tstruct list_head\tunlink_list;\n#endif\n\tunsigned int\t\tpromiscuity;\n\tunsigned int\t\tallmulti;\n\tbool\t\t\tuc_promisc;\n#ifdef CONFIG_LOCKDEP\n\tunsigned char\t\tnested_level;\n#endif\n\n\n\t \n\n\tstruct in_device __rcu\t*ip_ptr;\n\tstruct inet6_dev __rcu\t*ip6_ptr;\n#if IS_ENABLED(CONFIG_VLAN_8021Q)\n\tstruct vlan_info __rcu\t*vlan_info;\n#endif\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tstruct dsa_port\t\t*dsa_ptr;\n#endif\n#if IS_ENABLED(CONFIG_TIPC)\n\tstruct tipc_bearer __rcu *tipc_ptr;\n#endif\n#if IS_ENABLED(CONFIG_ATALK)\n\tvoid \t\t\t*atalk_ptr;\n#endif\n#if IS_ENABLED(CONFIG_AX25)\n\tvoid\t\t\t*ax25_ptr;\n#endif\n#if IS_ENABLED(CONFIG_CFG80211)\n\tstruct wireless_dev\t*ieee80211_ptr;\n#endif\n#if IS_ENABLED(CONFIG_IEEE802154) || IS_ENABLED(CONFIG_6LOWPAN)\n\tstruct wpan_dev\t\t*ieee802154_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MPLS_ROUTING)\n\tstruct mpls_dev __rcu\t*mpls_ptr;\n#endif\n#if IS_ENABLED(CONFIG_MCTP)\n\tstruct mctp_dev __rcu\t*mctp_ptr;\n#endif\n\n \n\t \n\tconst unsigned char\t*dev_addr;\n\n\tstruct netdev_rx_queue\t*_rx;\n\tunsigned int\t\tnum_rx_queues;\n\tunsigned int\t\treal_num_rx_queues;\n\n\tstruct bpf_prog __rcu\t*xdp_prog;\n\tunsigned long\t\tgro_flush_timeout;\n\tint\t\t\tnapi_defer_hard_irqs;\n#define GRO_LEGACY_MAX_SIZE\t65536u\n \n#define GRO_MAX_SIZE\t\t(8 * 65535u)\n\tunsigned int\t\tgro_max_size;\n\tunsigned int\t\tgro_ipv4_max_size;\n\tunsigned int\t\txdp_zc_max_segs;\n\trx_handler_func_t __rcu\t*rx_handler;\n\tvoid __rcu\t\t*rx_handler_data;\n#ifdef CONFIG_NET_XGRESS\n\tstruct bpf_mprog_entry __rcu *tcx_ingress;\n#endif\n\tstruct netdev_queue __rcu *ingress_queue;\n#ifdef CONFIG_NETFILTER_INGRESS\n\tstruct nf_hook_entries __rcu *nf_hooks_ingress;\n#endif\n\n\tunsigned char\t\tbroadcast[MAX_ADDR_LEN];\n#ifdef CONFIG_RFS_ACCEL\n\tstruct cpu_rmap\t\t*rx_cpu_rmap;\n#endif\n\tstruct hlist_node\tindex_hlist;\n\n \n\tstruct netdev_queue\t*_tx ____cacheline_aligned_in_smp;\n\tunsigned int\t\tnum_tx_queues;\n\tunsigned int\t\treal_num_tx_queues;\n\tstruct Qdisc __rcu\t*qdisc;\n\tunsigned int\t\ttx_queue_len;\n\tspinlock_t\t\ttx_global_lock;\n\n\tstruct xdp_dev_bulk_queue __percpu *xdp_bulkq;\n\n#ifdef CONFIG_XPS\n\tstruct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];\n#endif\n#ifdef CONFIG_NET_XGRESS\n\tstruct bpf_mprog_entry __rcu *tcx_egress;\n#endif\n#ifdef CONFIG_NETFILTER_EGRESS\n\tstruct nf_hook_entries __rcu *nf_hooks_egress;\n#endif\n\n#ifdef CONFIG_NET_SCHED\n\tDECLARE_HASHTABLE\t(qdisc_hash, 4);\n#endif\n\t \n\tstruct timer_list\twatchdog_timer;\n\tint\t\t\twatchdog_timeo;\n\n\tu32                     proto_down_reason;\n\n\tstruct list_head\ttodo_list;\n\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\tint __percpu\t\t*pcpu_refcnt;\n#else\n\trefcount_t\t\tdev_refcnt;\n#endif\n\tstruct ref_tracker_dir\trefcnt_tracker;\n\n\tstruct list_head\tlink_watch_list;\n\n\tenum { NETREG_UNINITIALIZED=0,\n\t       NETREG_REGISTERED,\t \n\t       NETREG_UNREGISTERING,\t \n\t       NETREG_UNREGISTERED,\t \n\t       NETREG_RELEASED,\t\t \n\t       NETREG_DUMMY,\t\t \n\t} reg_state:8;\n\n\tbool dismantle;\n\n\tenum {\n\t\tRTNL_LINK_INITIALIZED,\n\t\tRTNL_LINK_INITIALIZING,\n\t} rtnl_link_state:16;\n\n\tbool needs_free_netdev;\n\tvoid (*priv_destructor)(struct net_device *dev);\n\n#ifdef CONFIG_NETPOLL\n\tstruct netpoll_info __rcu\t*npinfo;\n#endif\n\n\tpossible_net_t\t\t\tnd_net;\n\n\t \n\tvoid\t\t\t\t*ml_priv;\n\tenum netdev_ml_priv_type\tml_priv_type;\n\n\tenum netdev_stat_type\t\tpcpu_stat_type:8;\n\tunion {\n\t\tstruct pcpu_lstats __percpu\t\t*lstats;\n\t\tstruct pcpu_sw_netstats __percpu\t*tstats;\n\t\tstruct pcpu_dstats __percpu\t\t*dstats;\n\t};\n\n#if IS_ENABLED(CONFIG_GARP)\n\tstruct garp_port __rcu\t*garp_port;\n#endif\n#if IS_ENABLED(CONFIG_MRP)\n\tstruct mrp_port __rcu\t*mrp_port;\n#endif\n#if IS_ENABLED(CONFIG_NET_DROP_MONITOR)\n\tstruct dm_hw_stat_delta __rcu *dm_private;\n#endif\n\tstruct device\t\tdev;\n\tconst struct attribute_group *sysfs_groups[4];\n\tconst struct attribute_group *sysfs_rx_queue_group;\n\n\tconst struct rtnl_link_ops *rtnl_link_ops;\n\n\t \n#define GSO_MAX_SEGS\t\t65535u\n#define GSO_LEGACY_MAX_SIZE\t65536u\n \n#define GSO_MAX_SIZE\t\t(8 * GSO_MAX_SEGS)\n\n\tunsigned int\t\tgso_max_size;\n#define TSO_LEGACY_MAX_SIZE\t65536\n#define TSO_MAX_SIZE\t\tUINT_MAX\n\tunsigned int\t\ttso_max_size;\n\tu16\t\t\tgso_max_segs;\n#define TSO_MAX_SEGS\t\tU16_MAX\n\tu16\t\t\ttso_max_segs;\n\tunsigned int\t\tgso_ipv4_max_size;\n\n#ifdef CONFIG_DCB\n\tconst struct dcbnl_rtnl_ops *dcbnl_ops;\n#endif\n\ts16\t\t\tnum_tc;\n\tstruct netdev_tc_txq\ttc_to_txq[TC_MAX_QUEUE];\n\tu8\t\t\tprio_tc_map[TC_BITMASK + 1];\n\n#if IS_ENABLED(CONFIG_FCOE)\n\tunsigned int\t\tfcoe_ddp_xid;\n#endif\n#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)\n\tstruct netprio_map __rcu *priomap;\n#endif\n\tstruct phy_device\t*phydev;\n\tstruct sfp_bus\t\t*sfp_bus;\n\tstruct lock_class_key\t*qdisc_tx_busylock;\n\tbool\t\t\tproto_down;\n\tunsigned\t\twol_enabled:1;\n\tunsigned\t\tthreaded:1;\n\n\tstruct list_head\tnet_notifier_list;\n\n#if IS_ENABLED(CONFIG_MACSEC)\n\t \n\tconst struct macsec_ops *macsec_ops;\n#endif\n\tconst struct udp_tunnel_nic_info\t*udp_tunnel_nic_info;\n\tstruct udp_tunnel_nic\t*udp_tunnel_nic;\n\n\t \n\tstruct bpf_xdp_entity\txdp_state[__MAX_XDP_MODE];\n\n\tu8 dev_addr_shadow[MAX_ADDR_LEN];\n\tnetdevice_tracker\tlinkwatch_dev_tracker;\n\tnetdevice_tracker\twatchdog_dev_tracker;\n\tnetdevice_tracker\tdev_registered_tracker;\n\tstruct rtnl_hw_stats64\t*offload_xstats_l3;\n\n\tstruct devlink_port\t*devlink_port;\n};\n#define to_net_dev(d) container_of(d, struct net_device, dev)\n\n \n#define SET_NETDEV_DEVLINK_PORT(dev, port)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tWARN_ON((dev)->reg_state != NETREG_UNINITIALIZED);\t\\\n\t((dev)->devlink_port = (port));\t\t\t\t\\\n})\n\nstatic inline bool netif_elide_gro(const struct net_device *dev)\n{\n\tif (!(dev->features & NETIF_F_GRO) || dev->xdp_prog)\n\t\treturn true;\n\treturn false;\n}\n\n#define\tNETDEV_ALIGN\t\t32\n\nstatic inline\nint netdev_get_prio_tc_map(const struct net_device *dev, u32 prio)\n{\n\treturn dev->prio_tc_map[prio & TC_BITMASK];\n}\n\nstatic inline\nint netdev_set_prio_tc_map(struct net_device *dev, u8 prio, u8 tc)\n{\n\tif (tc >= dev->num_tc)\n\t\treturn -EINVAL;\n\n\tdev->prio_tc_map[prio & TC_BITMASK] = tc & TC_BITMASK;\n\treturn 0;\n}\n\nint netdev_txq_to_tc(struct net_device *dev, unsigned int txq);\nvoid netdev_reset_tc(struct net_device *dev);\nint netdev_set_tc_queue(struct net_device *dev, u8 tc, u16 count, u16 offset);\nint netdev_set_num_tc(struct net_device *dev, u8 num_tc);\n\nstatic inline\nint netdev_get_num_tc(struct net_device *dev)\n{\n\treturn dev->num_tc;\n}\n\nstatic inline void net_prefetch(void *p)\n{\n\tprefetch(p);\n#if L1_CACHE_BYTES < 128\n\tprefetch((u8 *)p + L1_CACHE_BYTES);\n#endif\n}\n\nstatic inline void net_prefetchw(void *p)\n{\n\tprefetchw(p);\n#if L1_CACHE_BYTES < 128\n\tprefetchw((u8 *)p + L1_CACHE_BYTES);\n#endif\n}\n\nvoid netdev_unbind_sb_channel(struct net_device *dev,\n\t\t\t      struct net_device *sb_dev);\nint netdev_bind_sb_channel_queue(struct net_device *dev,\n\t\t\t\t struct net_device *sb_dev,\n\t\t\t\t u8 tc, u16 count, u16 offset);\nint netdev_set_sb_channel(struct net_device *dev, u16 channel);\nstatic inline int netdev_get_sb_channel(struct net_device *dev)\n{\n\treturn max_t(int, -dev->num_tc, 0);\n}\n\nstatic inline\nstruct netdev_queue *netdev_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t unsigned int index)\n{\n\tDEBUG_NET_WARN_ON_ONCE(index >= dev->num_tx_queues);\n\treturn &dev->_tx[index];\n}\n\nstatic inline struct netdev_queue *skb_get_tx_queue(const struct net_device *dev,\n\t\t\t\t\t\t    const struct sk_buff *skb)\n{\n\treturn netdev_get_tx_queue(dev, skb_get_queue_mapping(skb));\n}\n\nstatic inline void netdev_for_each_tx_queue(struct net_device *dev,\n\t\t\t\t\t    void (*f)(struct net_device *,\n\t\t\t\t\t\t      struct netdev_queue *,\n\t\t\t\t\t\t      void *),\n\t\t\t\t\t    void *arg)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tf(dev, &dev->_tx[i], arg);\n}\n\n#define netdev_lockdep_set_classes(dev)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key qdisc_tx_busylock_key;\t\\\n\tstatic struct lock_class_key qdisc_xmit_lock_key;\t\\\n\tstatic struct lock_class_key dev_addr_list_lock_key;\t\\\n\tunsigned int i;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t(dev)->qdisc_tx_busylock = &qdisc_tx_busylock_key;\t\\\n\tlockdep_set_class(&(dev)->addr_list_lock,\t\t\\\n\t\t\t  &dev_addr_list_lock_key);\t\t\\\n\tfor (i = 0; i < (dev)->num_tx_queues; i++)\t\t\\\n\t\tlockdep_set_class(&(dev)->_tx[i]._xmit_lock,\t\\\n\t\t\t\t  &qdisc_xmit_lock_key);\t\\\n}\n\nu16 netdev_pick_tx(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev);\nstruct netdev_queue *netdev_core_pick_tx(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct net_device *sb_dev);\n\n \nstatic inline unsigned netdev_get_fwd_headroom(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_PHONY_HEADROOM ? 0 : dev->needed_headroom;\n}\n\nstatic inline void netdev_set_rx_headroom(struct net_device *dev, int new_hr)\n{\n\tif (dev->netdev_ops->ndo_set_rx_headroom)\n\t\tdev->netdev_ops->ndo_set_rx_headroom(dev, new_hr);\n}\n\n \nstatic inline void netdev_reset_rx_headroom(struct net_device *dev)\n{\n\tnetdev_set_rx_headroom(dev, -1);\n}\n\nstatic inline void *netdev_get_ml_priv(struct net_device *dev,\n\t\t\t\t       enum netdev_ml_priv_type type)\n{\n\tif (dev->ml_priv_type != type)\n\t\treturn NULL;\n\n\treturn dev->ml_priv;\n}\n\nstatic inline void netdev_set_ml_priv(struct net_device *dev,\n\t\t\t\t      void *ml_priv,\n\t\t\t\t      enum netdev_ml_priv_type type)\n{\n\tWARN(dev->ml_priv_type && dev->ml_priv_type != type,\n\t     \"Overwriting already set ml_priv_type (%u) with different ml_priv_type (%u)!\\n\",\n\t     dev->ml_priv_type, type);\n\tWARN(!dev->ml_priv_type && dev->ml_priv,\n\t     \"Overwriting already set ml_priv and ml_priv_type is ML_PRIV_NONE!\\n\");\n\n\tdev->ml_priv = ml_priv;\n\tdev->ml_priv_type = type;\n}\n\n \nstatic inline\nstruct net *dev_net(const struct net_device *dev)\n{\n\treturn read_pnet(&dev->nd_net);\n}\n\nstatic inline\nvoid dev_net_set(struct net_device *dev, struct net *net)\n{\n\twrite_pnet(&dev->nd_net, net);\n}\n\n \nstatic inline void *netdev_priv(const struct net_device *dev)\n{\n\treturn (char *)dev + ALIGN(sizeof(struct net_device), NETDEV_ALIGN);\n}\n\n \n#define SET_NETDEV_DEV(net, pdev)\t((net)->dev.parent = (pdev))\n\n \n#define SET_NETDEV_DEVTYPE(net, devtype)\t((net)->dev.type = (devtype))\n\n \n#define NAPI_POLL_WEIGHT 64\n\nvoid netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi,\n\t\t\t   int (*poll)(struct napi_struct *, int), int weight);\n\n \nstatic inline void\nnetif_napi_add(struct net_device *dev, struct napi_struct *napi,\n\t       int (*poll)(struct napi_struct *, int))\n{\n\tnetif_napi_add_weight(dev, napi, poll, NAPI_POLL_WEIGHT);\n}\n\nstatic inline void\nnetif_napi_add_tx_weight(struct net_device *dev,\n\t\t\t struct napi_struct *napi,\n\t\t\t int (*poll)(struct napi_struct *, int),\n\t\t\t int weight)\n{\n\tset_bit(NAPI_STATE_NO_BUSY_POLL, &napi->state);\n\tnetif_napi_add_weight(dev, napi, poll, weight);\n}\n\n \nstatic inline void netif_napi_add_tx(struct net_device *dev,\n\t\t\t\t     struct napi_struct *napi,\n\t\t\t\t     int (*poll)(struct napi_struct *, int))\n{\n\tnetif_napi_add_tx_weight(dev, napi, poll, NAPI_POLL_WEIGHT);\n}\n\n \nvoid __netif_napi_del(struct napi_struct *napi);\n\n \nstatic inline void netif_napi_del(struct napi_struct *napi)\n{\n\t__netif_napi_del(napi);\n\tsynchronize_net();\n}\n\nstruct packet_type {\n\t__be16\t\t\ttype;\t \n\tbool\t\t\tignore_outgoing;\n\tstruct net_device\t*dev;\t \n\tnetdevice_tracker\tdev_tracker;\n\tint\t\t\t(*func) (struct sk_buff *,\n\t\t\t\t\t struct net_device *,\n\t\t\t\t\t struct packet_type *,\n\t\t\t\t\t struct net_device *);\n\tvoid\t\t\t(*list_func) (struct list_head *,\n\t\t\t\t\t      struct packet_type *,\n\t\t\t\t\t      struct net_device *);\n\tbool\t\t\t(*id_match)(struct packet_type *ptype,\n\t\t\t\t\t    struct sock *sk);\n\tstruct net\t\t*af_packet_net;\n\tvoid\t\t\t*af_packet_priv;\n\tstruct list_head\tlist;\n};\n\nstruct offload_callbacks {\n\tstruct sk_buff\t\t*(*gso_segment)(struct sk_buff *skb,\n\t\t\t\t\t\tnetdev_features_t features);\n\tstruct sk_buff\t\t*(*gro_receive)(struct list_head *head,\n\t\t\t\t\t\tstruct sk_buff *skb);\n\tint\t\t\t(*gro_complete)(struct sk_buff *skb, int nhoff);\n};\n\nstruct packet_offload {\n\t__be16\t\t\t type;\t \n\tu16\t\t\t priority;\n\tstruct offload_callbacks callbacks;\n\tstruct list_head\t list;\n};\n\n \nstruct pcpu_sw_netstats {\n\tu64_stats_t\t\trx_packets;\n\tu64_stats_t\t\trx_bytes;\n\tu64_stats_t\t\ttx_packets;\n\tu64_stats_t\t\ttx_bytes;\n\tstruct u64_stats_sync   syncp;\n} __aligned(4 * sizeof(u64));\n\nstruct pcpu_dstats {\n\tu64\t\t\trx_packets;\n\tu64\t\t\trx_bytes;\n\tu64\t\t\trx_drops;\n\tu64\t\t\ttx_packets;\n\tu64\t\t\ttx_bytes;\n\tu64\t\t\ttx_drops;\n\tstruct u64_stats_sync\tsyncp;\n} __aligned(8 * sizeof(u64));\n\nstruct pcpu_lstats {\n\tu64_stats_t packets;\n\tu64_stats_t bytes;\n\tstruct u64_stats_sync syncp;\n} __aligned(2 * sizeof(u64));\n\nvoid dev_lstats_read(struct net_device *dev, u64 *packets, u64 *bytes);\n\nstatic inline void dev_sw_netstats_rx_add(struct net_device *dev, unsigned int len)\n{\n\tstruct pcpu_sw_netstats *tstats = this_cpu_ptr(dev->tstats);\n\n\tu64_stats_update_begin(&tstats->syncp);\n\tu64_stats_add(&tstats->rx_bytes, len);\n\tu64_stats_inc(&tstats->rx_packets);\n\tu64_stats_update_end(&tstats->syncp);\n}\n\nstatic inline void dev_sw_netstats_tx_add(struct net_device *dev,\n\t\t\t\t\t  unsigned int packets,\n\t\t\t\t\t  unsigned int len)\n{\n\tstruct pcpu_sw_netstats *tstats = this_cpu_ptr(dev->tstats);\n\n\tu64_stats_update_begin(&tstats->syncp);\n\tu64_stats_add(&tstats->tx_bytes, len);\n\tu64_stats_add(&tstats->tx_packets, packets);\n\tu64_stats_update_end(&tstats->syncp);\n}\n\nstatic inline void dev_lstats_add(struct net_device *dev, unsigned int len)\n{\n\tstruct pcpu_lstats *lstats = this_cpu_ptr(dev->lstats);\n\n\tu64_stats_update_begin(&lstats->syncp);\n\tu64_stats_add(&lstats->bytes, len);\n\tu64_stats_inc(&lstats->packets);\n\tu64_stats_update_end(&lstats->syncp);\n}\n\n#define __netdev_alloc_pcpu_stats(type, gfp)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = alloc_percpu_gfp(type, gfp);\\\n\tif (pcpu_stats)\t{\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\n#define netdev_alloc_pcpu_stats(type)\t\t\t\t\t\\\n\t__netdev_alloc_pcpu_stats(type, GFP_KERNEL)\n\n#define devm_netdev_alloc_pcpu_stats(dev, type)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(type) __percpu *pcpu_stats = devm_alloc_percpu(dev, type);\\\n\tif (pcpu_stats) {\t\t\t\t\t\t\\\n\t\tint __cpu;\t\t\t\t\t\t\\\n\t\tfor_each_possible_cpu(__cpu) {\t\t\t\t\\\n\t\t\ttypeof(type) *stat;\t\t\t\t\\\n\t\t\tstat = per_cpu_ptr(pcpu_stats, __cpu);\t\t\\\n\t\t\tu64_stats_init(&stat->syncp);\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tpcpu_stats;\t\t\t\t\t\t\t\\\n})\n\nenum netdev_lag_tx_type {\n\tNETDEV_LAG_TX_TYPE_UNKNOWN,\n\tNETDEV_LAG_TX_TYPE_RANDOM,\n\tNETDEV_LAG_TX_TYPE_BROADCAST,\n\tNETDEV_LAG_TX_TYPE_ROUNDROBIN,\n\tNETDEV_LAG_TX_TYPE_ACTIVEBACKUP,\n\tNETDEV_LAG_TX_TYPE_HASH,\n};\n\nenum netdev_lag_hash {\n\tNETDEV_LAG_HASH_NONE,\n\tNETDEV_LAG_HASH_L2,\n\tNETDEV_LAG_HASH_L34,\n\tNETDEV_LAG_HASH_L23,\n\tNETDEV_LAG_HASH_E23,\n\tNETDEV_LAG_HASH_E34,\n\tNETDEV_LAG_HASH_VLAN_SRCMAC,\n\tNETDEV_LAG_HASH_UNKNOWN,\n};\n\nstruct netdev_lag_upper_info {\n\tenum netdev_lag_tx_type tx_type;\n\tenum netdev_lag_hash hash_type;\n};\n\nstruct netdev_lag_lower_state_info {\n\tu8 link_up : 1,\n\t   tx_enabled : 1;\n};\n\n#include <linux/notifier.h>\n\n \nenum netdev_cmd {\n\tNETDEV_UP\t= 1,\t \n\tNETDEV_DOWN,\n\tNETDEV_REBOOT,\t\t \n\tNETDEV_CHANGE,\t\t \n\tNETDEV_REGISTER,\n\tNETDEV_UNREGISTER,\n\tNETDEV_CHANGEMTU,\t \n\tNETDEV_CHANGEADDR,\t \n\tNETDEV_PRE_CHANGEADDR,\t \n\tNETDEV_GOING_DOWN,\n\tNETDEV_CHANGENAME,\n\tNETDEV_FEAT_CHANGE,\n\tNETDEV_BONDING_FAILOVER,\n\tNETDEV_PRE_UP,\n\tNETDEV_PRE_TYPE_CHANGE,\n\tNETDEV_POST_TYPE_CHANGE,\n\tNETDEV_POST_INIT,\n\tNETDEV_PRE_UNINIT,\n\tNETDEV_RELEASE,\n\tNETDEV_NOTIFY_PEERS,\n\tNETDEV_JOIN,\n\tNETDEV_CHANGEUPPER,\n\tNETDEV_RESEND_IGMP,\n\tNETDEV_PRECHANGEMTU,\t \n\tNETDEV_CHANGEINFODATA,\n\tNETDEV_BONDING_INFO,\n\tNETDEV_PRECHANGEUPPER,\n\tNETDEV_CHANGELOWERSTATE,\n\tNETDEV_UDP_TUNNEL_PUSH_INFO,\n\tNETDEV_UDP_TUNNEL_DROP_INFO,\n\tNETDEV_CHANGE_TX_QUEUE_LEN,\n\tNETDEV_CVLAN_FILTER_PUSH_INFO,\n\tNETDEV_CVLAN_FILTER_DROP_INFO,\n\tNETDEV_SVLAN_FILTER_PUSH_INFO,\n\tNETDEV_SVLAN_FILTER_DROP_INFO,\n\tNETDEV_OFFLOAD_XSTATS_ENABLE,\n\tNETDEV_OFFLOAD_XSTATS_DISABLE,\n\tNETDEV_OFFLOAD_XSTATS_REPORT_USED,\n\tNETDEV_OFFLOAD_XSTATS_REPORT_DELTA,\n\tNETDEV_XDP_FEAT_CHANGE,\n};\nconst char *netdev_cmd_to_name(enum netdev_cmd cmd);\n\nint register_netdevice_notifier(struct notifier_block *nb);\nint unregister_netdevice_notifier(struct notifier_block *nb);\nint register_netdevice_notifier_net(struct net *net, struct notifier_block *nb);\nint unregister_netdevice_notifier_net(struct net *net,\n\t\t\t\t      struct notifier_block *nb);\nint register_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\tstruct notifier_block *nb,\n\t\t\t\t\tstruct netdev_net_notifier *nn);\nint unregister_netdevice_notifier_dev_net(struct net_device *dev,\n\t\t\t\t\t  struct notifier_block *nb,\n\t\t\t\t\t  struct netdev_net_notifier *nn);\n\nstruct netdev_notifier_info {\n\tstruct net_device\t*dev;\n\tstruct netlink_ext_ack\t*extack;\n};\n\nstruct netdev_notifier_info_ext {\n\tstruct netdev_notifier_info info;  \n\tunion {\n\t\tu32 mtu;\n\t} ext;\n};\n\nstruct netdev_notifier_change_info {\n\tstruct netdev_notifier_info info;  \n\tunsigned int flags_changed;\n};\n\nstruct netdev_notifier_changeupper_info {\n\tstruct netdev_notifier_info info;  \n\tstruct net_device *upper_dev;  \n\tbool master;  \n\tbool linking;  \n\tvoid *upper_info;  \n};\n\nstruct netdev_notifier_changelowerstate_info {\n\tstruct netdev_notifier_info info;  \n\tvoid *lower_state_info;  \n};\n\nstruct netdev_notifier_pre_changeaddr_info {\n\tstruct netdev_notifier_info info;  \n\tconst unsigned char *dev_addr;\n};\n\nenum netdev_offload_xstats_type {\n\tNETDEV_OFFLOAD_XSTATS_TYPE_L3 = 1,\n};\n\nstruct netdev_notifier_offload_xstats_info {\n\tstruct netdev_notifier_info info;  \n\tenum netdev_offload_xstats_type type;\n\n\tunion {\n\t\t \n\t\tstruct netdev_notifier_offload_xstats_rd *report_delta;\n\t\t \n\t\tstruct netdev_notifier_offload_xstats_ru *report_used;\n\t};\n};\n\nint netdev_offload_xstats_enable(struct net_device *dev,\n\t\t\t\t enum netdev_offload_xstats_type type,\n\t\t\t\t struct netlink_ext_ack *extack);\nint netdev_offload_xstats_disable(struct net_device *dev,\n\t\t\t\t  enum netdev_offload_xstats_type type);\nbool netdev_offload_xstats_enabled(const struct net_device *dev,\n\t\t\t\t   enum netdev_offload_xstats_type type);\nint netdev_offload_xstats_get(struct net_device *dev,\n\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t      struct rtnl_hw_stats64 *stats, bool *used,\n\t\t\t      struct netlink_ext_ack *extack);\nvoid\nnetdev_offload_xstats_report_delta(struct netdev_notifier_offload_xstats_rd *rd,\n\t\t\t\t   const struct rtnl_hw_stats64 *stats);\nvoid\nnetdev_offload_xstats_report_used(struct netdev_notifier_offload_xstats_ru *ru);\nvoid netdev_offload_xstats_push_delta(struct net_device *dev,\n\t\t\t\t      enum netdev_offload_xstats_type type,\n\t\t\t\t      const struct rtnl_hw_stats64 *stats);\n\nstatic inline void netdev_notifier_info_init(struct netdev_notifier_info *info,\n\t\t\t\t\t     struct net_device *dev)\n{\n\tinfo->dev = dev;\n\tinfo->extack = NULL;\n}\n\nstatic inline struct net_device *\nnetdev_notifier_info_to_dev(const struct netdev_notifier_info *info)\n{\n\treturn info->dev;\n}\n\nstatic inline struct netlink_ext_ack *\nnetdev_notifier_info_to_extack(const struct netdev_notifier_info *info)\n{\n\treturn info->extack;\n}\n\nint call_netdevice_notifiers(unsigned long val, struct net_device *dev);\nint call_netdevice_notifiers_info(unsigned long val,\n\t\t\t\t  struct netdev_notifier_info *info);\n\nextern rwlock_t\t\t\t\tdev_base_lock;\t\t \n\n#define for_each_netdev(net, d)\t\t\\\n\t\tlist_for_each_entry(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_reverse(net, d)\t\\\n\t\tlist_for_each_entry_reverse(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_rcu(net, d)\t\t\\\n\t\tlist_for_each_entry_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_safe(net, d, n)\t\\\n\t\tlist_for_each_entry_safe(d, n, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue(net, d)\t\t\\\n\t\tlist_for_each_entry_continue(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_continue_reverse(net, d)\t\t\\\n\t\tlist_for_each_entry_continue_reverse(d, &(net)->dev_base_head, \\\n\t\t\t\t\t\t     dev_list)\n#define for_each_netdev_continue_rcu(net, d)\t\t\\\n\tlist_for_each_entry_continue_rcu(d, &(net)->dev_base_head, dev_list)\n#define for_each_netdev_in_bond_rcu(bond, slave)\t\\\n\t\tfor_each_netdev_rcu(&init_net, slave)\t\\\n\t\t\tif (netdev_master_upper_dev_get_rcu(slave) == (bond))\n#define net_device_entry(lh)\tlist_entry(lh, struct net_device, dev_list)\n\n#define for_each_netdev_dump(net, d, ifindex)\t\t\t\t\\\n\txa_for_each_start(&(net)->dev_by_index, (ifindex), (d), (ifindex))\n\nstatic inline struct net_device *next_net_device(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = dev->dev_list.next;\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *next_net_device_rcu(struct net_device *dev)\n{\n\tstruct list_head *lh;\n\tstruct net *net;\n\n\tnet = dev_net(dev);\n\tlh = rcu_dereference(list_next_rcu(&dev->dev_list));\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nstatic inline struct net_device *first_net_device(struct net *net)\n{\n\treturn list_empty(&net->dev_base_head) ? NULL :\n\t\tnet_device_entry(net->dev_base_head.next);\n}\n\nstatic inline struct net_device *first_net_device_rcu(struct net *net)\n{\n\tstruct list_head *lh = rcu_dereference(list_next_rcu(&net->dev_base_head));\n\n\treturn lh == &net->dev_base_head ? NULL : net_device_entry(lh);\n}\n\nint netdev_boot_setup_check(struct net_device *dev);\nstruct net_device *dev_getbyhwaddr_rcu(struct net *net, unsigned short type,\n\t\t\t\t       const char *hwaddr);\nstruct net_device *dev_getfirstbyhwtype(struct net *net, unsigned short type);\nvoid dev_add_pack(struct packet_type *pt);\nvoid dev_remove_pack(struct packet_type *pt);\nvoid __dev_remove_pack(struct packet_type *pt);\nvoid dev_add_offload(struct packet_offload *po);\nvoid dev_remove_offload(struct packet_offload *po);\n\nint dev_get_iflink(const struct net_device *dev);\nint dev_fill_metadata_dst(struct net_device *dev, struct sk_buff *skb);\nint dev_fill_forward_path(const struct net_device *dev, const u8 *daddr,\n\t\t\t  struct net_device_path_stack *stack);\nstruct net_device *__dev_get_by_flags(struct net *net, unsigned short flags,\n\t\t\t\t      unsigned short mask);\nstruct net_device *dev_get_by_name(struct net *net, const char *name);\nstruct net_device *dev_get_by_name_rcu(struct net *net, const char *name);\nstruct net_device *__dev_get_by_name(struct net *net, const char *name);\nbool netdev_name_in_use(struct net *net, const char *name);\nint dev_alloc_name(struct net_device *dev, const char *name);\nint dev_open(struct net_device *dev, struct netlink_ext_ack *extack);\nvoid dev_close(struct net_device *dev);\nvoid dev_close_many(struct list_head *head, bool unlink);\nvoid dev_disable_lro(struct net_device *dev);\nint dev_loopback_xmit(struct net *net, struct sock *sk, struct sk_buff *newskb);\nu16 dev_pick_tx_zero(struct net_device *dev, struct sk_buff *skb,\n\t\t     struct net_device *sb_dev);\nu16 dev_pick_tx_cpu_id(struct net_device *dev, struct sk_buff *skb,\n\t\t       struct net_device *sb_dev);\n\nint __dev_queue_xmit(struct sk_buff *skb, struct net_device *sb_dev);\nint __dev_direct_xmit(struct sk_buff *skb, u16 queue_id);\n\nstatic inline int dev_queue_xmit(struct sk_buff *skb)\n{\n\treturn __dev_queue_xmit(skb, NULL);\n}\n\nstatic inline int dev_queue_xmit_accel(struct sk_buff *skb,\n\t\t\t\t       struct net_device *sb_dev)\n{\n\treturn __dev_queue_xmit(skb, sb_dev);\n}\n\nstatic inline int dev_direct_xmit(struct sk_buff *skb, u16 queue_id)\n{\n\tint ret;\n\n\tret = __dev_direct_xmit(skb, queue_id);\n\tif (!dev_xmit_complete(ret))\n\t\tkfree_skb(skb);\n\treturn ret;\n}\n\nint register_netdevice(struct net_device *dev);\nvoid unregister_netdevice_queue(struct net_device *dev, struct list_head *head);\nvoid unregister_netdevice_many(struct list_head *head);\nstatic inline void unregister_netdevice(struct net_device *dev)\n{\n\tunregister_netdevice_queue(dev, NULL);\n}\n\nint netdev_refcnt_read(const struct net_device *dev);\nvoid free_netdev(struct net_device *dev);\nvoid netdev_freemem(struct net_device *dev);\nint init_dummy_netdev(struct net_device *dev);\n\nstruct net_device *netdev_get_xmit_slave(struct net_device *dev,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t bool all_slaves);\nstruct net_device *netdev_sk_get_lowest_dev(struct net_device *dev,\n\t\t\t\t\t    struct sock *sk);\nstruct net_device *dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *__dev_get_by_index(struct net *net, int ifindex);\nstruct net_device *netdev_get_by_index(struct net *net, int ifindex,\n\t\t\t\t       netdevice_tracker *tracker, gfp_t gfp);\nstruct net_device *netdev_get_by_name(struct net *net, const char *name,\n\t\t\t\t      netdevice_tracker *tracker, gfp_t gfp);\nstruct net_device *dev_get_by_index_rcu(struct net *net, int ifindex);\nstruct net_device *dev_get_by_napi_id(unsigned int napi_id);\n\nstatic inline int dev_hard_header(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t  unsigned short type,\n\t\t\t\t  const void *daddr, const void *saddr,\n\t\t\t\t  unsigned int len)\n{\n\tif (!dev->header_ops || !dev->header_ops->create)\n\t\treturn 0;\n\n\treturn dev->header_ops->create(skb, dev, type, daddr, saddr, len);\n}\n\nstatic inline int dev_parse_header(const struct sk_buff *skb,\n\t\t\t\t   unsigned char *haddr)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse)\n\t\treturn 0;\n\treturn dev->header_ops->parse(skb, haddr);\n}\n\nstatic inline __be16 dev_parse_header_protocol(const struct sk_buff *skb)\n{\n\tconst struct net_device *dev = skb->dev;\n\n\tif (!dev->header_ops || !dev->header_ops->parse_protocol)\n\t\treturn 0;\n\treturn dev->header_ops->parse_protocol(skb);\n}\n\n \nstatic inline bool dev_validate_header(const struct net_device *dev,\n\t\t\t\t       char *ll_header, int len)\n{\n\tif (likely(len >= dev->hard_header_len))\n\t\treturn true;\n\tif (len < dev->min_header_len)\n\t\treturn false;\n\n\tif (capable(CAP_SYS_RAWIO)) {\n\t\tmemset(ll_header + len, 0, dev->hard_header_len - len);\n\t\treturn true;\n\t}\n\n\tif (dev->header_ops && dev->header_ops->validate)\n\t\treturn dev->header_ops->validate(ll_header, len);\n\n\treturn false;\n}\n\nstatic inline bool dev_has_header(const struct net_device *dev)\n{\n\treturn dev->header_ops && dev->header_ops->create;\n}\n\n \nstruct softnet_data {\n\tstruct list_head\tpoll_list;\n\tstruct sk_buff_head\tprocess_queue;\n\n\t \n\tunsigned int\t\tprocessed;\n\tunsigned int\t\ttime_squeeze;\n#ifdef CONFIG_RPS\n\tstruct softnet_data\t*rps_ipi_list;\n#endif\n\n\tbool\t\t\tin_net_rx_action;\n\tbool\t\t\tin_napi_threaded_poll;\n\n#ifdef CONFIG_NET_FLOW_LIMIT\n\tstruct sd_flow_limit __rcu *flow_limit;\n#endif\n\tstruct Qdisc\t\t*output_queue;\n\tstruct Qdisc\t\t**output_queue_tailp;\n\tstruct sk_buff\t\t*completion_queue;\n#ifdef CONFIG_XFRM_OFFLOAD\n\tstruct sk_buff_head\txfrm_backlog;\n#endif\n\t \n\tstruct {\n\t\tu16 recursion;\n\t\tu8  more;\n#ifdef CONFIG_NET_EGRESS\n\t\tu8  skip_txqueue;\n#endif\n\t} xmit;\n#ifdef CONFIG_RPS\n\t \n\tunsigned int\t\tinput_queue_head ____cacheline_aligned_in_smp;\n\n\t \n\tcall_single_data_t\tcsd ____cacheline_aligned_in_smp;\n\tstruct softnet_data\t*rps_ipi_next;\n\tunsigned int\t\tcpu;\n\tunsigned int\t\tinput_queue_tail;\n#endif\n\tunsigned int\t\treceived_rps;\n\tunsigned int\t\tdropped;\n\tstruct sk_buff_head\tinput_pkt_queue;\n\tstruct napi_struct\tbacklog;\n\n\t \n\tspinlock_t\t\tdefer_lock ____cacheline_aligned_in_smp;\n\tint\t\t\tdefer_count;\n\tint\t\t\tdefer_ipi_scheduled;\n\tstruct sk_buff\t\t*defer_list;\n\tcall_single_data_t\tdefer_csd;\n};\n\nstatic inline void input_queue_head_incr(struct softnet_data *sd)\n{\n#ifdef CONFIG_RPS\n\tsd->input_queue_head++;\n#endif\n}\n\nstatic inline void input_queue_tail_incr_save(struct softnet_data *sd,\n\t\t\t\t\t      unsigned int *qtail)\n{\n#ifdef CONFIG_RPS\n\t*qtail = ++sd->input_queue_tail;\n#endif\n}\n\nDECLARE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);\n\nstatic inline int dev_recursion_level(void)\n{\n\treturn this_cpu_read(softnet_data.xmit.recursion);\n}\n\n#define XMIT_RECURSION_LIMIT\t8\nstatic inline bool dev_xmit_recursion(void)\n{\n\treturn unlikely(__this_cpu_read(softnet_data.xmit.recursion) >\n\t\t\tXMIT_RECURSION_LIMIT);\n}\n\nstatic inline void dev_xmit_recursion_inc(void)\n{\n\t__this_cpu_inc(softnet_data.xmit.recursion);\n}\n\nstatic inline void dev_xmit_recursion_dec(void)\n{\n\t__this_cpu_dec(softnet_data.xmit.recursion);\n}\n\nvoid __netif_schedule(struct Qdisc *q);\nvoid netif_schedule_queue(struct netdev_queue *txq);\n\nstatic inline void netif_tx_schedule_all(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++)\n\t\tnetif_schedule_queue(netdev_get_tx_queue(dev, i));\n}\n\nstatic __always_inline void netif_tx_start_queue(struct netdev_queue *dev_queue)\n{\n\tclear_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n \nstatic inline void netif_start_queue(struct net_device *dev)\n{\n\tnetif_tx_start_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_start_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_start_queue(txq);\n\t}\n}\n\nvoid netif_tx_wake_queue(struct netdev_queue *dev_queue);\n\n \nstatic inline void netif_wake_queue(struct net_device *dev)\n{\n\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline void netif_tx_wake_all_queues(struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tnetif_tx_wake_queue(txq);\n\t}\n}\n\nstatic __always_inline void netif_tx_stop_queue(struct netdev_queue *dev_queue)\n{\n\t \n\tset_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n \nstatic inline void netif_stop_queue(struct net_device *dev)\n{\n\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, 0));\n}\n\nvoid netif_tx_stop_all_queues(struct net_device *dev);\n\nstatic inline bool netif_tx_queue_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn test_bit(__QUEUE_STATE_DRV_XOFF, &dev_queue->state);\n}\n\n \nstatic inline bool netif_queue_stopped(const struct net_device *dev)\n{\n\treturn netif_tx_queue_stopped(netdev_get_tx_queue(dev, 0));\n}\n\nstatic inline bool netif_xmit_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_ANY_XOFF_OR_FROZEN;\n}\n\nstatic inline bool\nnetif_xmit_frozen_or_drv_stopped(const struct netdev_queue *dev_queue)\n{\n\treturn dev_queue->state & QUEUE_STATE_DRV_XOFF_OR_FROZEN;\n}\n\n \nstatic inline void netdev_queue_set_dql_min_limit(struct netdev_queue *dev_queue,\n\t\t\t\t\t\t  unsigned int min_limit)\n{\n#ifdef CONFIG_BQL\n\tdev_queue->dql.min_limit = min_limit;\n#endif\n}\n\n \nstatic inline void netdev_txq_bql_enqueue_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.num_queued);\n#endif\n}\n\n \nstatic inline void netdev_txq_bql_complete_prefetchw(struct netdev_queue *dev_queue)\n{\n#ifdef CONFIG_BQL\n\tprefetchw(&dev_queue->dql.limit);\n#endif\n}\n\n \nstatic inline void netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\tunsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tdql_queued(&dev_queue->dql, bytes);\n\n\tif (likely(dql_avail(&dev_queue->dql) >= 0))\n\t\treturn;\n\n\tset_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n\n\t \n\tsmp_mb();\n\n\t \n\tif (unlikely(dql_avail(&dev_queue->dql) >= 0))\n\t\tclear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state);\n#endif\n}\n\n \nstatic inline bool __netdev_tx_sent_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t  unsigned int bytes,\n\t\t\t\t\t  bool xmit_more)\n{\n\tif (xmit_more) {\n#ifdef CONFIG_BQL\n\t\tdql_queued(&dev_queue->dql, bytes);\n#endif\n\t\treturn netif_tx_queue_stopped(dev_queue);\n\t}\n\tnetdev_tx_sent_queue(dev_queue, bytes);\n\treturn true;\n}\n\n \nstatic inline void netdev_sent_queue(struct net_device *dev, unsigned int bytes)\n{\n\tnetdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes);\n}\n\nstatic inline bool __netdev_sent_queue(struct net_device *dev,\n\t\t\t\t       unsigned int bytes,\n\t\t\t\t       bool xmit_more)\n{\n\treturn __netdev_tx_sent_queue(netdev_get_tx_queue(dev, 0), bytes,\n\t\t\t\t      xmit_more);\n}\n\n \nstatic inline void netdev_tx_completed_queue(struct netdev_queue *dev_queue,\n\t\t\t\t\t     unsigned int pkts, unsigned int bytes)\n{\n#ifdef CONFIG_BQL\n\tif (unlikely(!bytes))\n\t\treturn;\n\n\tdql_completed(&dev_queue->dql, bytes);\n\n\t \n\tsmp_mb();  \n\n\tif (unlikely(dql_avail(&dev_queue->dql) < 0))\n\t\treturn;\n\n\tif (test_and_clear_bit(__QUEUE_STATE_STACK_XOFF, &dev_queue->state))\n\t\tnetif_schedule_queue(dev_queue);\n#endif\n}\n\n \nstatic inline void netdev_completed_queue(struct net_device *dev,\n\t\t\t\t\t  unsigned int pkts, unsigned int bytes)\n{\n\tnetdev_tx_completed_queue(netdev_get_tx_queue(dev, 0), pkts, bytes);\n}\n\nstatic inline void netdev_tx_reset_queue(struct netdev_queue *q)\n{\n#ifdef CONFIG_BQL\n\tclear_bit(__QUEUE_STATE_STACK_XOFF, &q->state);\n\tdql_reset(&q->dql);\n#endif\n}\n\n \nstatic inline void netdev_reset_queue(struct net_device *dev_queue)\n{\n\tnetdev_tx_reset_queue(netdev_get_tx_queue(dev_queue, 0));\n}\n\n \nstatic inline u16 netdev_cap_txqueue(struct net_device *dev, u16 queue_index)\n{\n\tif (unlikely(queue_index >= dev->real_num_tx_queues)) {\n\t\tnet_warn_ratelimited(\"%s selects TX queue %d, but real number of TX queues is %d\\n\",\n\t\t\t\t     dev->name, queue_index,\n\t\t\t\t     dev->real_num_tx_queues);\n\t\treturn 0;\n\t}\n\n\treturn queue_index;\n}\n\n \nstatic inline bool netif_running(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_START, &dev->state);\n}\n\n \n\n \nstatic inline void netif_start_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_start_queue(txq);\n}\n\n \nstatic inline void netif_stop_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\tnetif_tx_stop_queue(txq);\n}\n\n \nstatic inline bool __netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t    u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\treturn netif_tx_queue_stopped(txq);\n}\n\n \nstatic inline bool netif_subqueue_stopped(const struct net_device *dev,\n\t\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __netif_subqueue_stopped(dev, skb_get_queue_mapping(skb));\n}\n\n \nstatic inline void netif_wake_subqueue(struct net_device *dev, u16 queue_index)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, queue_index);\n\n\tnetif_tx_wake_queue(txq);\n}\n\n#ifdef CONFIG_XPS\nint netif_set_xps_queue(struct net_device *dev, const struct cpumask *mask,\n\t\t\tu16 index);\nint __netif_set_xps_queue(struct net_device *dev, const unsigned long *mask,\n\t\t\t  u16 index, enum xps_map_type type);\n\n \nstatic inline bool netif_attr_test_mask(unsigned long j,\n\t\t\t\t\tconst unsigned long *mask,\n\t\t\t\t\tunsigned int nr_bits)\n{\n\tcpu_max_bits_warn(j, nr_bits);\n\treturn test_bit(j, mask);\n}\n\n \nstatic inline bool netif_attr_test_online(unsigned long j,\n\t\t\t\t\t  const unsigned long *online_mask,\n\t\t\t\t\t  unsigned int nr_bits)\n{\n\tcpu_max_bits_warn(j, nr_bits);\n\n\tif (online_mask)\n\t\treturn test_bit(j, online_mask);\n\n\treturn (j < nr_bits);\n}\n\n \nstatic inline unsigned int netif_attrmask_next(int n, const unsigned long *srcp,\n\t\t\t\t\t       unsigned int nr_bits)\n{\n\t \n\tif (n != -1)\n\t\tcpu_max_bits_warn(n, nr_bits);\n\n\tif (srcp)\n\t\treturn find_next_bit(srcp, nr_bits, n + 1);\n\n\treturn n + 1;\n}\n\n \nstatic inline int netif_attrmask_next_and(int n, const unsigned long *src1p,\n\t\t\t\t\t  const unsigned long *src2p,\n\t\t\t\t\t  unsigned int nr_bits)\n{\n\t \n\tif (n != -1)\n\t\tcpu_max_bits_warn(n, nr_bits);\n\n\tif (src1p && src2p)\n\t\treturn find_next_and_bit(src1p, src2p, nr_bits, n + 1);\n\telse if (src1p)\n\t\treturn find_next_bit(src1p, nr_bits, n + 1);\n\telse if (src2p)\n\t\treturn find_next_bit(src2p, nr_bits, n + 1);\n\n\treturn n + 1;\n}\n#else\nstatic inline int netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t      const struct cpumask *mask,\n\t\t\t\t      u16 index)\n{\n\treturn 0;\n}\n\nstatic inline int __netif_set_xps_queue(struct net_device *dev,\n\t\t\t\t\tconst unsigned long *mask,\n\t\t\t\t\tu16 index, enum xps_map_type type)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic inline bool netif_is_multiqueue(const struct net_device *dev)\n{\n\treturn dev->num_tx_queues > 1;\n}\n\nint netif_set_real_num_tx_queues(struct net_device *dev, unsigned int txq);\n\n#ifdef CONFIG_SYSFS\nint netif_set_real_num_rx_queues(struct net_device *dev, unsigned int rxq);\n#else\nstatic inline int netif_set_real_num_rx_queues(struct net_device *dev,\n\t\t\t\t\t\tunsigned int rxqs)\n{\n\tdev->real_num_rx_queues = rxqs;\n\treturn 0;\n}\n#endif\nint netif_set_real_num_queues(struct net_device *dev,\n\t\t\t      unsigned int txq, unsigned int rxq);\n\nint netif_get_num_default_rss_queues(void);\n\nvoid dev_kfree_skb_irq_reason(struct sk_buff *skb, enum skb_drop_reason reason);\nvoid dev_kfree_skb_any_reason(struct sk_buff *skb, enum skb_drop_reason reason);\n\n \nstatic inline void dev_kfree_skb_irq(struct sk_buff *skb)\n{\n\tdev_kfree_skb_irq_reason(skb, SKB_DROP_REASON_NOT_SPECIFIED);\n}\n\nstatic inline void dev_consume_skb_irq(struct sk_buff *skb)\n{\n\tdev_kfree_skb_irq_reason(skb, SKB_CONSUMED);\n}\n\nstatic inline void dev_kfree_skb_any(struct sk_buff *skb)\n{\n\tdev_kfree_skb_any_reason(skb, SKB_DROP_REASON_NOT_SPECIFIED);\n}\n\nstatic inline void dev_consume_skb_any(struct sk_buff *skb)\n{\n\tdev_kfree_skb_any_reason(skb, SKB_CONSUMED);\n}\n\nu32 bpf_prog_run_generic_xdp(struct sk_buff *skb, struct xdp_buff *xdp,\n\t\t\t     struct bpf_prog *xdp_prog);\nvoid generic_xdp_tx(struct sk_buff *skb, struct bpf_prog *xdp_prog);\nint do_xdp_generic(struct bpf_prog *xdp_prog, struct sk_buff *skb);\nint netif_rx(struct sk_buff *skb);\nint __netif_rx(struct sk_buff *skb);\n\nint netif_receive_skb(struct sk_buff *skb);\nint netif_receive_skb_core(struct sk_buff *skb);\nvoid netif_receive_skb_list_internal(struct list_head *head);\nvoid netif_receive_skb_list(struct list_head *head);\ngro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb);\nvoid napi_gro_flush(struct napi_struct *napi, bool flush_old);\nstruct sk_buff *napi_get_frags(struct napi_struct *napi);\nvoid napi_get_frags_check(struct napi_struct *napi);\ngro_result_t napi_gro_frags(struct napi_struct *napi);\nstruct packet_offload *gro_find_receive_by_type(__be16 type);\nstruct packet_offload *gro_find_complete_by_type(__be16 type);\n\nstatic inline void napi_free_frags(struct napi_struct *napi)\n{\n\tkfree_skb(napi->skb);\n\tnapi->skb = NULL;\n}\n\nbool netdev_is_rx_handler_busy(struct net_device *dev);\nint netdev_rx_handler_register(struct net_device *dev,\n\t\t\t       rx_handler_func_t *rx_handler,\n\t\t\t       void *rx_handler_data);\nvoid netdev_rx_handler_unregister(struct net_device *dev);\n\nbool dev_valid_name(const char *name);\nstatic inline bool is_socket_ioctl_cmd(unsigned int cmd)\n{\n\treturn _IOC_TYPE(cmd) == SOCK_IOC_TYPE;\n}\nint get_user_ifreq(struct ifreq *ifr, void __user **ifrdata, void __user *arg);\nint put_user_ifreq(struct ifreq *ifr, void __user *arg);\nint dev_ioctl(struct net *net, unsigned int cmd, struct ifreq *ifr,\n\t\tvoid __user *data, bool *need_copyout);\nint dev_ifconf(struct net *net, struct ifconf __user *ifc);\nint generic_hwtstamp_get_lower(struct net_device *dev,\n\t\t\t       struct kernel_hwtstamp_config *kernel_cfg);\nint generic_hwtstamp_set_lower(struct net_device *dev,\n\t\t\t       struct kernel_hwtstamp_config *kernel_cfg,\n\t\t\t       struct netlink_ext_ack *extack);\nint dev_ethtool(struct net *net, struct ifreq *ifr, void __user *userdata);\nunsigned int dev_get_flags(const struct net_device *);\nint __dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t       struct netlink_ext_ack *extack);\nint dev_change_flags(struct net_device *dev, unsigned int flags,\n\t\t     struct netlink_ext_ack *extack);\nint dev_set_alias(struct net_device *, const char *, size_t);\nint dev_get_alias(const struct net_device *, char *, size_t);\nint __dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t       const char *pat, int new_ifindex);\nstatic inline\nint dev_change_net_namespace(struct net_device *dev, struct net *net,\n\t\t\t     const char *pat)\n{\n\treturn __dev_change_net_namespace(dev, net, pat, 0);\n}\nint __dev_set_mtu(struct net_device *, int);\nint dev_set_mtu(struct net_device *, int);\nint dev_pre_changeaddr_notify(struct net_device *dev, const char *addr,\n\t\t\t      struct netlink_ext_ack *extack);\nint dev_set_mac_address(struct net_device *dev, struct sockaddr *sa,\n\t\t\tstruct netlink_ext_ack *extack);\nint dev_set_mac_address_user(struct net_device *dev, struct sockaddr *sa,\n\t\t\t     struct netlink_ext_ack *extack);\nint dev_get_mac_address(struct sockaddr *sa, struct net *net, char *dev_name);\nint dev_get_port_parent_id(struct net_device *dev,\n\t\t\t   struct netdev_phys_item_id *ppid, bool recurse);\nbool netdev_port_same_parent_id(struct net_device *a, struct net_device *b);\nstruct sk_buff *validate_xmit_skb_list(struct sk_buff *skb, struct net_device *dev, bool *again);\nstruct sk_buff *dev_hard_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t    struct netdev_queue *txq, int *ret);\n\nint bpf_xdp_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);\nu8 dev_xdp_prog_count(struct net_device *dev);\nu32 dev_xdp_prog_id(struct net_device *dev, enum bpf_xdp_mode mode);\n\nint __dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb(struct net_device *dev, struct sk_buff *skb);\nint dev_forward_skb_nomtu(struct net_device *dev, struct sk_buff *skb);\nbool is_skb_forwardable(const struct net_device *dev,\n\t\t\tconst struct sk_buff *skb);\n\nstatic __always_inline bool __is_skb_forwardable(const struct net_device *dev,\n\t\t\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t\t\t const bool check_mtu)\n{\n\tconst u32 vlan_hdr_len = 4;  \n\tunsigned int len;\n\n\tif (!(dev->flags & IFF_UP))\n\t\treturn false;\n\n\tif (!check_mtu)\n\t\treturn true;\n\n\tlen = dev->mtu + dev->hard_header_len + vlan_hdr_len;\n\tif (skb->len <= len)\n\t\treturn true;\n\n\t \n\tif (skb_is_gso(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\nstruct net_device_core_stats __percpu *netdev_core_stats_alloc(struct net_device *dev);\n\nstatic inline struct net_device_core_stats __percpu *dev_core_stats(struct net_device *dev)\n{\n\t \n\tstruct net_device_core_stats __percpu *p = READ_ONCE(dev->core_stats);\n\n\tif (likely(p))\n\t\treturn p;\n\n\treturn netdev_core_stats_alloc(dev);\n}\n\n#define DEV_CORE_STATS_INC(FIELD)\t\t\t\t\t\t\\\nstatic inline void dev_core_stats_##FIELD##_inc(struct net_device *dev)\t\t\\\n{\t\t\t\t\t\t\t\t\t\t\\\n\tstruct net_device_core_stats __percpu *p;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tp = dev_core_stats(dev);\t\t\t\t\t\t\\\n\tif (p)\t\t\t\t\t\t\t\t\t\\\n\t\tthis_cpu_inc(p->FIELD);\t\t\t\t\t\t\\\n}\nDEV_CORE_STATS_INC(rx_dropped)\nDEV_CORE_STATS_INC(tx_dropped)\nDEV_CORE_STATS_INC(rx_nohandler)\nDEV_CORE_STATS_INC(rx_otherhost_dropped)\n\nstatic __always_inline int ____dev_forward_skb(struct net_device *dev,\n\t\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t\t       const bool check_mtu)\n{\n\tif (skb_orphan_frags(skb, GFP_ATOMIC) ||\n\t    unlikely(!__is_skb_forwardable(dev, skb, check_mtu))) {\n\t\tdev_core_stats_rx_dropped_inc(dev);\n\t\tkfree_skb(skb);\n\t\treturn NET_RX_DROP;\n\t}\n\n\tskb_scrub_packet(skb, !net_eq(dev_net(dev), dev_net(skb->dev)));\n\tskb->priority = 0;\n\treturn 0;\n}\n\nbool dev_nit_active(struct net_device *dev);\nvoid dev_queue_xmit_nit(struct sk_buff *skb, struct net_device *dev);\n\nstatic inline void __dev_put(struct net_device *dev)\n{\n\tif (dev) {\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\t\tthis_cpu_dec(*dev->pcpu_refcnt);\n#else\n\t\trefcount_dec(&dev->dev_refcnt);\n#endif\n\t}\n}\n\nstatic inline void __dev_hold(struct net_device *dev)\n{\n\tif (dev) {\n#ifdef CONFIG_PCPU_DEV_REFCNT\n\t\tthis_cpu_inc(*dev->pcpu_refcnt);\n#else\n\t\trefcount_inc(&dev->dev_refcnt);\n#endif\n\t}\n}\n\nstatic inline void __netdev_tracker_alloc(struct net_device *dev,\n\t\t\t\t\t  netdevice_tracker *tracker,\n\t\t\t\t\t  gfp_t gfp)\n{\n#ifdef CONFIG_NET_DEV_REFCNT_TRACKER\n\tref_tracker_alloc(&dev->refcnt_tracker, tracker, gfp);\n#endif\n}\n\n \nstatic inline void netdev_tracker_alloc(struct net_device *dev,\n\t\t\t\t\tnetdevice_tracker *tracker, gfp_t gfp)\n{\n#ifdef CONFIG_NET_DEV_REFCNT_TRACKER\n\trefcount_dec(&dev->refcnt_tracker.no_tracker);\n\t__netdev_tracker_alloc(dev, tracker, gfp);\n#endif\n}\n\nstatic inline void netdev_tracker_free(struct net_device *dev,\n\t\t\t\t       netdevice_tracker *tracker)\n{\n#ifdef CONFIG_NET_DEV_REFCNT_TRACKER\n\tref_tracker_free(&dev->refcnt_tracker, tracker);\n#endif\n}\n\nstatic inline void netdev_hold(struct net_device *dev,\n\t\t\t       netdevice_tracker *tracker, gfp_t gfp)\n{\n\tif (dev) {\n\t\t__dev_hold(dev);\n\t\t__netdev_tracker_alloc(dev, tracker, gfp);\n\t}\n}\n\nstatic inline void netdev_put(struct net_device *dev,\n\t\t\t      netdevice_tracker *tracker)\n{\n\tif (dev) {\n\t\tnetdev_tracker_free(dev, tracker);\n\t\t__dev_put(dev);\n\t}\n}\n\n \nstatic inline void dev_hold(struct net_device *dev)\n{\n\tnetdev_hold(dev, NULL, GFP_ATOMIC);\n}\n\n \nstatic inline void dev_put(struct net_device *dev)\n{\n\tnetdev_put(dev, NULL);\n}\n\nstatic inline void netdev_ref_replace(struct net_device *odev,\n\t\t\t\t      struct net_device *ndev,\n\t\t\t\t      netdevice_tracker *tracker,\n\t\t\t\t      gfp_t gfp)\n{\n\tif (odev)\n\t\tnetdev_tracker_free(odev, tracker);\n\n\t__dev_hold(ndev);\n\t__dev_put(odev);\n\n\tif (ndev)\n\t\t__netdev_tracker_alloc(ndev, tracker, gfp);\n}\n\n \nvoid linkwatch_fire_event(struct net_device *dev);\n\n \nstatic inline bool netif_carrier_ok(const struct net_device *dev)\n{\n\treturn !test_bit(__LINK_STATE_NOCARRIER, &dev->state);\n}\n\nunsigned long dev_trans_start(struct net_device *dev);\n\nvoid __netdev_watchdog_up(struct net_device *dev);\n\nvoid netif_carrier_on(struct net_device *dev);\nvoid netif_carrier_off(struct net_device *dev);\nvoid netif_carrier_event(struct net_device *dev);\n\n \nstatic inline void netif_dormant_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n \nstatic inline void netif_dormant_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_DORMANT, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n \nstatic inline bool netif_dormant(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_DORMANT, &dev->state);\n}\n\n\n \nstatic inline void netif_testing_on(struct net_device *dev)\n{\n\tif (!test_and_set_bit(__LINK_STATE_TESTING, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n \nstatic inline void netif_testing_off(struct net_device *dev)\n{\n\tif (test_and_clear_bit(__LINK_STATE_TESTING, &dev->state))\n\t\tlinkwatch_fire_event(dev);\n}\n\n \nstatic inline bool netif_testing(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_TESTING, &dev->state);\n}\n\n\n \nstatic inline bool netif_oper_up(const struct net_device *dev)\n{\n\treturn (dev->operstate == IF_OPER_UP ||\n\t\tdev->operstate == IF_OPER_UNKNOWN  );\n}\n\n \nstatic inline bool netif_device_present(const struct net_device *dev)\n{\n\treturn test_bit(__LINK_STATE_PRESENT, &dev->state);\n}\n\nvoid netif_device_detach(struct net_device *dev);\n\nvoid netif_device_attach(struct net_device *dev);\n\n \n\nenum {\n\tNETIF_MSG_DRV_BIT,\n\tNETIF_MSG_PROBE_BIT,\n\tNETIF_MSG_LINK_BIT,\n\tNETIF_MSG_TIMER_BIT,\n\tNETIF_MSG_IFDOWN_BIT,\n\tNETIF_MSG_IFUP_BIT,\n\tNETIF_MSG_RX_ERR_BIT,\n\tNETIF_MSG_TX_ERR_BIT,\n\tNETIF_MSG_TX_QUEUED_BIT,\n\tNETIF_MSG_INTR_BIT,\n\tNETIF_MSG_TX_DONE_BIT,\n\tNETIF_MSG_RX_STATUS_BIT,\n\tNETIF_MSG_PKTDATA_BIT,\n\tNETIF_MSG_HW_BIT,\n\tNETIF_MSG_WOL_BIT,\n\n\t \n\tNETIF_MSG_CLASS_COUNT,\n};\n \nstatic_assert(NETIF_MSG_CLASS_COUNT <= 32);\n\n#define __NETIF_MSG_BIT(bit)\t((u32)1 << (bit))\n#define __NETIF_MSG(name)\t__NETIF_MSG_BIT(NETIF_MSG_ ## name ## _BIT)\n\n#define NETIF_MSG_DRV\t\t__NETIF_MSG(DRV)\n#define NETIF_MSG_PROBE\t\t__NETIF_MSG(PROBE)\n#define NETIF_MSG_LINK\t\t__NETIF_MSG(LINK)\n#define NETIF_MSG_TIMER\t\t__NETIF_MSG(TIMER)\n#define NETIF_MSG_IFDOWN\t__NETIF_MSG(IFDOWN)\n#define NETIF_MSG_IFUP\t\t__NETIF_MSG(IFUP)\n#define NETIF_MSG_RX_ERR\t__NETIF_MSG(RX_ERR)\n#define NETIF_MSG_TX_ERR\t__NETIF_MSG(TX_ERR)\n#define NETIF_MSG_TX_QUEUED\t__NETIF_MSG(TX_QUEUED)\n#define NETIF_MSG_INTR\t\t__NETIF_MSG(INTR)\n#define NETIF_MSG_TX_DONE\t__NETIF_MSG(TX_DONE)\n#define NETIF_MSG_RX_STATUS\t__NETIF_MSG(RX_STATUS)\n#define NETIF_MSG_PKTDATA\t__NETIF_MSG(PKTDATA)\n#define NETIF_MSG_HW\t\t__NETIF_MSG(HW)\n#define NETIF_MSG_WOL\t\t__NETIF_MSG(WOL)\n\n#define netif_msg_drv(p)\t((p)->msg_enable & NETIF_MSG_DRV)\n#define netif_msg_probe(p)\t((p)->msg_enable & NETIF_MSG_PROBE)\n#define netif_msg_link(p)\t((p)->msg_enable & NETIF_MSG_LINK)\n#define netif_msg_timer(p)\t((p)->msg_enable & NETIF_MSG_TIMER)\n#define netif_msg_ifdown(p)\t((p)->msg_enable & NETIF_MSG_IFDOWN)\n#define netif_msg_ifup(p)\t((p)->msg_enable & NETIF_MSG_IFUP)\n#define netif_msg_rx_err(p)\t((p)->msg_enable & NETIF_MSG_RX_ERR)\n#define netif_msg_tx_err(p)\t((p)->msg_enable & NETIF_MSG_TX_ERR)\n#define netif_msg_tx_queued(p)\t((p)->msg_enable & NETIF_MSG_TX_QUEUED)\n#define netif_msg_intr(p)\t((p)->msg_enable & NETIF_MSG_INTR)\n#define netif_msg_tx_done(p)\t((p)->msg_enable & NETIF_MSG_TX_DONE)\n#define netif_msg_rx_status(p)\t((p)->msg_enable & NETIF_MSG_RX_STATUS)\n#define netif_msg_pktdata(p)\t((p)->msg_enable & NETIF_MSG_PKTDATA)\n#define netif_msg_hw(p)\t\t((p)->msg_enable & NETIF_MSG_HW)\n#define netif_msg_wol(p)\t((p)->msg_enable & NETIF_MSG_WOL)\n\nstatic inline u32 netif_msg_init(int debug_value, int default_msg_enable_bits)\n{\n\t \n\tif (debug_value < 0 || debug_value >= (sizeof(u32) * 8))\n\t\treturn default_msg_enable_bits;\n\tif (debug_value == 0)\t \n\t\treturn 0;\n\t \n\treturn (1U << debug_value) - 1;\n}\n\nstatic inline void __netif_tx_lock(struct netdev_queue *txq, int cpu)\n{\n\tspin_lock(&txq->_xmit_lock);\n\t \n\tWRITE_ONCE(txq->xmit_lock_owner, cpu);\n}\n\nstatic inline bool __netif_tx_acquire(struct netdev_queue *txq)\n{\n\t__acquire(&txq->_xmit_lock);\n\treturn true;\n}\n\nstatic inline void __netif_tx_release(struct netdev_queue *txq)\n{\n\t__release(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_lock_bh(struct netdev_queue *txq)\n{\n\tspin_lock_bh(&txq->_xmit_lock);\n\t \n\tWRITE_ONCE(txq->xmit_lock_owner, smp_processor_id());\n}\n\nstatic inline bool __netif_tx_trylock(struct netdev_queue *txq)\n{\n\tbool ok = spin_trylock(&txq->_xmit_lock);\n\n\tif (likely(ok)) {\n\t\t \n\t\tWRITE_ONCE(txq->xmit_lock_owner, smp_processor_id());\n\t}\n\treturn ok;\n}\n\nstatic inline void __netif_tx_unlock(struct netdev_queue *txq)\n{\n\t \n\tWRITE_ONCE(txq->xmit_lock_owner, -1);\n\tspin_unlock(&txq->_xmit_lock);\n}\n\nstatic inline void __netif_tx_unlock_bh(struct netdev_queue *txq)\n{\n\t \n\tWRITE_ONCE(txq->xmit_lock_owner, -1);\n\tspin_unlock_bh(&txq->_xmit_lock);\n}\n\n \nstatic inline void txq_trans_update(struct netdev_queue *txq)\n{\n\tif (txq->xmit_lock_owner != -1)\n\t\tWRITE_ONCE(txq->trans_start, jiffies);\n}\n\nstatic inline void txq_trans_cond_update(struct netdev_queue *txq)\n{\n\tunsigned long now = jiffies;\n\n\tif (READ_ONCE(txq->trans_start) != now)\n\t\tWRITE_ONCE(txq->trans_start, now);\n}\n\n \nstatic inline void netif_trans_update(struct net_device *dev)\n{\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, 0);\n\n\ttxq_trans_cond_update(txq);\n}\n\n \nvoid netif_tx_lock(struct net_device *dev);\n\nstatic inline void netif_tx_lock_bh(struct net_device *dev)\n{\n\tlocal_bh_disable();\n\tnetif_tx_lock(dev);\n}\n\nvoid netif_tx_unlock(struct net_device *dev);\n\nstatic inline void netif_tx_unlock_bh(struct net_device *dev)\n{\n\tnetif_tx_unlock(dev);\n\tlocal_bh_enable();\n}\n\n#define HARD_TX_LOCK(dev, txq, cpu) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_lock(txq, cpu);\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_acquire(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\n#define HARD_TX_TRYLOCK(dev, txq)\t\t\t\\\n\t(((dev->features & NETIF_F_LLTX) == 0) ?\t\\\n\t\t__netif_tx_trylock(txq) :\t\t\\\n\t\t__netif_tx_acquire(txq))\n\n#define HARD_TX_UNLOCK(dev, txq) {\t\t\t\\\n\tif ((dev->features & NETIF_F_LLTX) == 0) {\t\\\n\t\t__netif_tx_unlock(txq);\t\t\t\\\n\t} else {\t\t\t\t\t\\\n\t\t__netif_tx_release(txq);\t\t\\\n\t}\t\t\t\t\t\t\\\n}\n\nstatic inline void netif_tx_disable(struct net_device *dev)\n{\n\tunsigned int i;\n\tint cpu;\n\n\tlocal_bh_disable();\n\tcpu = smp_processor_id();\n\tspin_lock(&dev->tx_global_lock);\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\t__netif_tx_lock(txq, cpu);\n\t\tnetif_tx_stop_queue(txq);\n\t\t__netif_tx_unlock(txq);\n\t}\n\tspin_unlock(&dev->tx_global_lock);\n\tlocal_bh_enable();\n}\n\nstatic inline void netif_addr_lock(struct net_device *dev)\n{\n\tunsigned char nest_level = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tnest_level = dev->nested_level;\n#endif\n\tspin_lock_nested(&dev->addr_list_lock, nest_level);\n}\n\nstatic inline void netif_addr_lock_bh(struct net_device *dev)\n{\n\tunsigned char nest_level = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tnest_level = dev->nested_level;\n#endif\n\tlocal_bh_disable();\n\tspin_lock_nested(&dev->addr_list_lock, nest_level);\n}\n\nstatic inline void netif_addr_unlock(struct net_device *dev)\n{\n\tspin_unlock(&dev->addr_list_lock);\n}\n\nstatic inline void netif_addr_unlock_bh(struct net_device *dev)\n{\n\tspin_unlock_bh(&dev->addr_list_lock);\n}\n\n \n#define for_each_dev_addr(dev, ha) \\\n\t\tlist_for_each_entry_rcu(ha, &dev->dev_addrs.list, list)\n\n \n\nvoid ether_setup(struct net_device *dev);\n\n \nstruct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,\n\t\t\t\t    unsigned char name_assign_type,\n\t\t\t\t    void (*setup)(struct net_device *),\n\t\t\t\t    unsigned int txqs, unsigned int rxqs);\n#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, 1, 1)\n\n#define alloc_netdev_mq(sizeof_priv, name, name_assign_type, setup, count) \\\n\talloc_netdev_mqs(sizeof_priv, name, name_assign_type, setup, count, \\\n\t\t\t count)\n\nint register_netdev(struct net_device *dev);\nvoid unregister_netdev(struct net_device *dev);\n\nint devm_register_netdev(struct device *dev, struct net_device *ndev);\n\n \nint __hw_addr_sync(struct netdev_hw_addr_list *to_list,\n\t\t   struct netdev_hw_addr_list *from_list, int addr_len);\nvoid __hw_addr_unsync(struct netdev_hw_addr_list *to_list,\n\t\t      struct netdev_hw_addr_list *from_list, int addr_len);\nint __hw_addr_sync_dev(struct netdev_hw_addr_list *list,\n\t\t       struct net_device *dev,\n\t\t       int (*sync)(struct net_device *, const unsigned char *),\n\t\t       int (*unsync)(struct net_device *,\n\t\t\t\t     const unsigned char *));\nint __hw_addr_ref_sync_dev(struct netdev_hw_addr_list *list,\n\t\t\t   struct net_device *dev,\n\t\t\t   int (*sync)(struct net_device *,\n\t\t\t\t       const unsigned char *, int),\n\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t const unsigned char *, int));\nvoid __hw_addr_ref_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t      struct net_device *dev,\n\t\t\t      int (*unsync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *, int));\nvoid __hw_addr_unsync_dev(struct netdev_hw_addr_list *list,\n\t\t\t  struct net_device *dev,\n\t\t\t  int (*unsync)(struct net_device *,\n\t\t\t\t\tconst unsigned char *));\nvoid __hw_addr_init(struct netdev_hw_addr_list *list);\n\n \nvoid dev_addr_mod(struct net_device *dev, unsigned int offset,\n\t\t  const void *addr, size_t len);\n\nstatic inline void\n__dev_addr_set(struct net_device *dev, const void *addr, size_t len)\n{\n\tdev_addr_mod(dev, 0, addr, len);\n}\n\nstatic inline void dev_addr_set(struct net_device *dev, const u8 *addr)\n{\n\t__dev_addr_set(dev, addr, dev->addr_len);\n}\n\nint dev_addr_add(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\nint dev_addr_del(struct net_device *dev, const unsigned char *addr,\n\t\t unsigned char addr_type);\n\n \nint dev_uc_add(struct net_device *dev, const unsigned char *addr);\nint dev_uc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_uc_del(struct net_device *dev, const unsigned char *addr);\nint dev_uc_sync(struct net_device *to, struct net_device *from);\nint dev_uc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_uc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_uc_flush(struct net_device *dev);\nvoid dev_uc_init(struct net_device *dev);\n\n \nstatic inline int __dev_uc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->uc, dev, sync, unsync);\n}\n\n \nstatic inline void __dev_uc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->uc, dev, unsync);\n}\n\n \nint dev_mc_add(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_add_excl(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del(struct net_device *dev, const unsigned char *addr);\nint dev_mc_del_global(struct net_device *dev, const unsigned char *addr);\nint dev_mc_sync(struct net_device *to, struct net_device *from);\nint dev_mc_sync_multiple(struct net_device *to, struct net_device *from);\nvoid dev_mc_unsync(struct net_device *to, struct net_device *from);\nvoid dev_mc_flush(struct net_device *dev);\nvoid dev_mc_init(struct net_device *dev);\n\n \nstatic inline int __dev_mc_sync(struct net_device *dev,\n\t\t\t\tint (*sync)(struct net_device *,\n\t\t\t\t\t    const unsigned char *),\n\t\t\t\tint (*unsync)(struct net_device *,\n\t\t\t\t\t      const unsigned char *))\n{\n\treturn __hw_addr_sync_dev(&dev->mc, dev, sync, unsync);\n}\n\n \nstatic inline void __dev_mc_unsync(struct net_device *dev,\n\t\t\t\t   int (*unsync)(struct net_device *,\n\t\t\t\t\t\t const unsigned char *))\n{\n\t__hw_addr_unsync_dev(&dev->mc, dev, unsync);\n}\n\n \nvoid dev_set_rx_mode(struct net_device *dev);\nint dev_set_promiscuity(struct net_device *dev, int inc);\nint dev_set_allmulti(struct net_device *dev, int inc);\nvoid netdev_state_change(struct net_device *dev);\nvoid __netdev_notify_peers(struct net_device *dev);\nvoid netdev_notify_peers(struct net_device *dev);\nvoid netdev_features_change(struct net_device *dev);\n \nvoid dev_load(struct net *net, const char *name);\nstruct rtnl_link_stats64 *dev_get_stats(struct net_device *dev,\n\t\t\t\t\tstruct rtnl_link_stats64 *storage);\nvoid netdev_stats_to_stats64(struct rtnl_link_stats64 *stats64,\n\t\t\t     const struct net_device_stats *netdev_stats);\nvoid dev_fetch_sw_netstats(struct rtnl_link_stats64 *s,\n\t\t\t   const struct pcpu_sw_netstats __percpu *netstats);\nvoid dev_get_tstats64(struct net_device *dev, struct rtnl_link_stats64 *s);\n\nextern int\t\tnetdev_max_backlog;\nextern int\t\tdev_rx_weight;\nextern int\t\tdev_tx_weight;\nextern int\t\tgro_normal_batch;\n\nenum {\n\tNESTED_SYNC_IMM_BIT,\n\tNESTED_SYNC_TODO_BIT,\n};\n\n#define __NESTED_SYNC_BIT(bit)\t((u32)1 << (bit))\n#define __NESTED_SYNC(name)\t__NESTED_SYNC_BIT(NESTED_SYNC_ ## name ## _BIT)\n\n#define NESTED_SYNC_IMM\t\t__NESTED_SYNC(IMM)\n#define NESTED_SYNC_TODO\t__NESTED_SYNC(TODO)\n\nstruct netdev_nested_priv {\n\tunsigned char flags;\n\tvoid *data;\n};\n\nbool netdev_has_upper_dev(struct net_device *dev, struct net_device *upper_dev);\nstruct net_device *netdev_upper_get_next_dev_rcu(struct net_device *dev,\n\t\t\t\t\t\t     struct list_head **iter);\n\n \n#define netdev_for_each_upper_dev_rcu(dev, updev, iter) \\\n\tfor (iter = &(dev)->adj_list.upper, \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)); \\\n\t     updev; \\\n\t     updev = netdev_upper_get_next_dev_rcu(dev, &(iter)))\n\nint netdev_walk_all_upper_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *upper_dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv);\n\nbool netdev_has_upper_dev_all_rcu(struct net_device *dev,\n\t\t\t\t  struct net_device *upper_dev);\n\nbool netdev_has_any_upper_dev(struct net_device *dev);\n\nvoid *netdev_lower_get_next_private(struct net_device *dev,\n\t\t\t\t    struct list_head **iter);\nvoid *netdev_lower_get_next_private_rcu(struct net_device *dev,\n\t\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_private(dev, priv, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private(dev, &(iter)))\n\n#define netdev_for_each_lower_private_rcu(dev, priv, iter) \\\n\tfor (iter = &(dev)->adj_list.lower, \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)); \\\n\t     priv; \\\n\t     priv = netdev_lower_get_next_private_rcu(dev, &(iter)))\n\nvoid *netdev_lower_get_next(struct net_device *dev,\n\t\t\t\tstruct list_head **iter);\n\n#define netdev_for_each_lower_dev(dev, ldev, iter) \\\n\tfor (iter = (dev)->adj_list.lower.next, \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)); \\\n\t     ldev; \\\n\t     ldev = netdev_lower_get_next(dev, &(iter)))\n\nstruct net_device *netdev_next_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t\t     struct list_head **iter);\nint netdev_walk_all_lower_dev(struct net_device *dev,\n\t\t\t      int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\tstruct netdev_nested_priv *priv),\n\t\t\t      struct netdev_nested_priv *priv);\nint netdev_walk_all_lower_dev_rcu(struct net_device *dev,\n\t\t\t\t  int (*fn)(struct net_device *lower_dev,\n\t\t\t\t\t    struct netdev_nested_priv *priv),\n\t\t\t\t  struct netdev_nested_priv *priv);\n\nvoid *netdev_adjacent_get_private(struct list_head *adj_list);\nvoid *netdev_lower_get_first_private_rcu(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get(struct net_device *dev);\nstruct net_device *netdev_master_upper_dev_get_rcu(struct net_device *dev);\nint netdev_upper_dev_link(struct net_device *dev, struct net_device *upper_dev,\n\t\t\t  struct netlink_ext_ack *extack);\nint netdev_master_upper_dev_link(struct net_device *dev,\n\t\t\t\t struct net_device *upper_dev,\n\t\t\t\t void *upper_priv, void *upper_info,\n\t\t\t\t struct netlink_ext_ack *extack);\nvoid netdev_upper_dev_unlink(struct net_device *dev,\n\t\t\t     struct net_device *upper_dev);\nint netdev_adjacent_change_prepare(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev,\n\t\t\t\t   struct netlink_ext_ack *extack);\nvoid netdev_adjacent_change_commit(struct net_device *old_dev,\n\t\t\t\t   struct net_device *new_dev,\n\t\t\t\t   struct net_device *dev);\nvoid netdev_adjacent_change_abort(struct net_device *old_dev,\n\t\t\t\t  struct net_device *new_dev,\n\t\t\t\t  struct net_device *dev);\nvoid netdev_adjacent_rename_links(struct net_device *dev, char *oldname);\nvoid *netdev_lower_dev_get_private(struct net_device *dev,\n\t\t\t\t   struct net_device *lower_dev);\nvoid netdev_lower_state_changed(struct net_device *lower_dev,\n\t\t\t\tvoid *lower_state_info);\n\n \n#define NETDEV_RSS_KEY_LEN 52\nextern u8 netdev_rss_key[NETDEV_RSS_KEY_LEN] __read_mostly;\nvoid netdev_rss_key_fill(void *buffer, size_t len);\n\nint skb_checksum_help(struct sk_buff *skb);\nint skb_crc32c_csum_help(struct sk_buff *skb);\nint skb_csum_hwoffload_help(struct sk_buff *skb,\n\t\t\t    const netdev_features_t features);\n\nstruct netdev_bonding_info {\n\tifslave\tslave;\n\tifbond\tmaster;\n};\n\nstruct netdev_notifier_bonding_info {\n\tstruct netdev_notifier_info info;  \n\tstruct netdev_bonding_info  bonding_info;\n};\n\nvoid netdev_bonding_info_change(struct net_device *dev,\n\t\t\t\tstruct netdev_bonding_info *bonding_info);\n\n#if IS_ENABLED(CONFIG_ETHTOOL_NETLINK)\nvoid ethtool_notify(struct net_device *dev, unsigned int cmd, const void *data);\n#else\nstatic inline void ethtool_notify(struct net_device *dev, unsigned int cmd,\n\t\t\t\t  const void *data)\n{\n}\n#endif\n\n__be16 skb_network_protocol(struct sk_buff *skb, int *depth);\n\nstatic inline bool can_checksum_protocol(netdev_features_t features,\n\t\t\t\t\t __be16 protocol)\n{\n\tif (protocol == htons(ETH_P_FCOE))\n\t\treturn !!(features & NETIF_F_FCOE_CRC);\n\n\t \n\n\tif (features & NETIF_F_HW_CSUM) {\n\t\t \n\t\treturn true;\n\t}\n\n\tswitch (protocol) {\n\tcase htons(ETH_P_IP):\n\t\treturn !!(features & NETIF_F_IP_CSUM);\n\tcase htons(ETH_P_IPV6):\n\t\treturn !!(features & NETIF_F_IPV6_CSUM);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_BUG\nvoid netdev_rx_csum_fault(struct net_device *dev, struct sk_buff *skb);\n#else\nstatic inline void netdev_rx_csum_fault(struct net_device *dev,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n}\n#endif\n \nvoid net_enable_timestamp(void);\nvoid net_disable_timestamp(void);\n\nstatic inline ktime_t netdev_get_tstamp(struct net_device *dev,\n\t\t\t\t\tconst struct skb_shared_hwtstamps *hwtstamps,\n\t\t\t\t\tbool cycles)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\n\tif (ops->ndo_get_tstamp)\n\t\treturn ops->ndo_get_tstamp(dev, hwtstamps, cycles);\n\n\treturn hwtstamps->hwtstamp;\n}\n\nstatic inline netdev_tx_t __netdev_start_xmit(const struct net_device_ops *ops,\n\t\t\t\t\t      struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t      bool more)\n{\n\t__this_cpu_write(softnet_data.xmit.more, more);\n\treturn ops->ndo_start_xmit(skb, dev);\n}\n\nstatic inline bool netdev_xmit_more(void)\n{\n\treturn __this_cpu_read(softnet_data.xmit.more);\n}\n\nstatic inline netdev_tx_t netdev_start_xmit(struct sk_buff *skb, struct net_device *dev,\n\t\t\t\t\t    struct netdev_queue *txq, bool more)\n{\n\tconst struct net_device_ops *ops = dev->netdev_ops;\n\tnetdev_tx_t rc;\n\n\trc = __netdev_start_xmit(ops, skb, dev, more);\n\tif (rc == NETDEV_TX_OK)\n\t\ttxq_trans_update(txq);\n\n\treturn rc;\n}\n\nint netdev_class_create_file_ns(const struct class_attribute *class_attr,\n\t\t\t\tconst void *ns);\nvoid netdev_class_remove_file_ns(const struct class_attribute *class_attr,\n\t\t\t\t const void *ns);\n\nextern const struct kobj_ns_type_operations net_ns_type_operations;\n\nconst char *netdev_drivername(const struct net_device *dev);\n\nstatic inline netdev_features_t netdev_intersect_features(netdev_features_t f1,\n\t\t\t\t\t\t\t  netdev_features_t f2)\n{\n\tif ((f1 ^ f2) & NETIF_F_HW_CSUM) {\n\t\tif (f1 & NETIF_F_HW_CSUM)\n\t\t\tf1 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t\telse\n\t\t\tf2 |= (NETIF_F_IP_CSUM|NETIF_F_IPV6_CSUM);\n\t}\n\n\treturn f1 & f2;\n}\n\nstatic inline netdev_features_t netdev_get_wanted_features(\n\tstruct net_device *dev)\n{\n\treturn (dev->features & ~dev->hw_features) | dev->wanted_features;\n}\nnetdev_features_t netdev_increment_features(netdev_features_t all,\n\tnetdev_features_t one, netdev_features_t mask);\n\n \nstatic inline netdev_features_t netdev_add_tso_features(netdev_features_t features,\n\t\t\t\t\t\t\tnetdev_features_t mask)\n{\n\treturn netdev_increment_features(features, NETIF_F_ALL_TSO, mask);\n}\n\nint __netdev_update_features(struct net_device *dev);\nvoid netdev_update_features(struct net_device *dev);\nvoid netdev_change_features(struct net_device *dev);\n\nvoid netif_stacked_transfer_operstate(const struct net_device *rootdev,\n\t\t\t\t\tstruct net_device *dev);\n\nnetdev_features_t passthru_features_check(struct sk_buff *skb,\n\t\t\t\t\t  struct net_device *dev,\n\t\t\t\t\t  netdev_features_t features);\nnetdev_features_t netif_skb_features(struct sk_buff *skb);\nvoid skb_warn_bad_offload(const struct sk_buff *skb);\n\nstatic inline bool net_gso_ok(netdev_features_t features, int gso_type)\n{\n\tnetdev_features_t feature = (netdev_features_t)gso_type << NETIF_F_GSO_SHIFT;\n\n\t \n\tBUILD_BUG_ON(SKB_GSO_TCPV4   != (NETIF_F_TSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_DODGY   != (NETIF_F_GSO_ROBUST >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_ECN != (NETIF_F_TSO_ECN >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCP_FIXEDID != (NETIF_F_TSO_MANGLEID >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TCPV6   != (NETIF_F_TSO6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FCOE    != (NETIF_F_FSO >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE     != (NETIF_F_GSO_GRE >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_GRE_CSUM != (NETIF_F_GSO_GRE_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP4  != (NETIF_F_GSO_IPXIP4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_IPXIP6  != (NETIF_F_GSO_IPXIP6 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL != (NETIF_F_GSO_UDP_TUNNEL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_TUNNEL_CSUM != (NETIF_F_GSO_UDP_TUNNEL_CSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_PARTIAL != (NETIF_F_GSO_PARTIAL >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_TUNNEL_REMCSUM != (NETIF_F_GSO_TUNNEL_REMCSUM >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_SCTP    != (NETIF_F_GSO_SCTP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_ESP != (NETIF_F_GSO_ESP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP != (NETIF_F_GSO_UDP >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_UDP_L4 != (NETIF_F_GSO_UDP_L4 >> NETIF_F_GSO_SHIFT));\n\tBUILD_BUG_ON(SKB_GSO_FRAGLIST != (NETIF_F_GSO_FRAGLIST >> NETIF_F_GSO_SHIFT));\n\n\treturn (features & feature) == feature;\n}\n\nstatic inline bool skb_gso_ok(struct sk_buff *skb, netdev_features_t features)\n{\n\treturn net_gso_ok(features, skb_shinfo(skb)->gso_type) &&\n\t       (!skb_has_frag_list(skb) || (features & NETIF_F_FRAGLIST));\n}\n\nstatic inline bool netif_needs_gso(struct sk_buff *skb,\n\t\t\t\t   netdev_features_t features)\n{\n\treturn skb_is_gso(skb) && (!skb_gso_ok(skb, features) ||\n\t\tunlikely((skb->ip_summed != CHECKSUM_PARTIAL) &&\n\t\t\t (skb->ip_summed != CHECKSUM_UNNECESSARY)));\n}\n\nvoid netif_set_tso_max_size(struct net_device *dev, unsigned int size);\nvoid netif_set_tso_max_segs(struct net_device *dev, unsigned int segs);\nvoid netif_inherit_tso_max(struct net_device *to,\n\t\t\t   const struct net_device *from);\n\nstatic inline bool netif_is_macsec(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACSEC;\n}\n\nstatic inline bool netif_is_macvlan(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN;\n}\n\nstatic inline bool netif_is_macvlan_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_MACVLAN_PORT;\n}\n\nstatic inline bool netif_is_bond_master(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_MASTER && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_is_bond_slave(const struct net_device *dev)\n{\n\treturn dev->flags & IFF_SLAVE && dev->priv_flags & IFF_BONDING;\n}\n\nstatic inline bool netif_supports_nofcs(struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_SUPP_NOFCS;\n}\n\nstatic inline bool netif_has_l3_rx_handler(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_RX_HANDLER;\n}\n\nstatic inline bool netif_is_l3_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_MASTER;\n}\n\nstatic inline bool netif_is_l3_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_L3MDEV_SLAVE;\n}\n\nstatic inline int dev_sdif(const struct net_device *dev)\n{\n#ifdef CONFIG_NET_L3_MASTER_DEV\n\tif (netif_is_l3_slave(dev))\n\t\treturn dev->ifindex;\n#endif\n\treturn 0;\n}\n\nstatic inline bool netif_is_bridge_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_EBRIDGE;\n}\n\nstatic inline bool netif_is_bridge_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_BRIDGE_PORT;\n}\n\nstatic inline bool netif_is_ovs_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OPENVSWITCH;\n}\n\nstatic inline bool netif_is_ovs_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_OVS_DATAPATH;\n}\n\nstatic inline bool netif_is_any_bridge_master(const struct net_device *dev)\n{\n\treturn netif_is_bridge_master(dev) || netif_is_ovs_master(dev);\n}\n\nstatic inline bool netif_is_any_bridge_port(const struct net_device *dev)\n{\n\treturn netif_is_bridge_port(dev) || netif_is_ovs_port(dev);\n}\n\nstatic inline bool netif_is_team_master(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM;\n}\n\nstatic inline bool netif_is_team_port(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_TEAM_PORT;\n}\n\nstatic inline bool netif_is_lag_master(const struct net_device *dev)\n{\n\treturn netif_is_bond_master(dev) || netif_is_team_master(dev);\n}\n\nstatic inline bool netif_is_lag_port(const struct net_device *dev)\n{\n\treturn netif_is_bond_slave(dev) || netif_is_team_port(dev);\n}\n\nstatic inline bool netif_is_rxfh_configured(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_RXFH_CONFIGURED;\n}\n\nstatic inline bool netif_is_failover(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_FAILOVER;\n}\n\nstatic inline bool netif_is_failover_slave(const struct net_device *dev)\n{\n\treturn dev->priv_flags & IFF_FAILOVER_SLAVE;\n}\n\n \nstatic inline void netif_keep_dst(struct net_device *dev)\n{\n\tdev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_XMIT_DST_RELEASE_PERM);\n}\n\n \nstatic inline bool netif_reduces_vlan_mtu(struct net_device *dev)\n{\n\t \n\treturn netif_is_macsec(dev);\n}\n\nextern struct pernet_operations __net_initdata loopback_net_ops;\n\n \n\n \n\nstatic inline const char *netdev_name(const struct net_device *dev)\n{\n\tif (!dev->name[0] || strchr(dev->name, '%'))\n\t\treturn \"(unnamed net_device)\";\n\treturn dev->name;\n}\n\nstatic inline const char *netdev_reg_state(const struct net_device *dev)\n{\n\tswitch (dev->reg_state) {\n\tcase NETREG_UNINITIALIZED: return \" (uninitialized)\";\n\tcase NETREG_REGISTERED: return \"\";\n\tcase NETREG_UNREGISTERING: return \" (unregistering)\";\n\tcase NETREG_UNREGISTERED: return \" (unregistered)\";\n\tcase NETREG_RELEASED: return \" (released)\";\n\tcase NETREG_DUMMY: return \" (dummy)\";\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown reg_state %d\\n\", dev->name, dev->reg_state);\n\treturn \" (unknown)\";\n}\n\n#define MODULE_ALIAS_NETDEV(device) \\\n\tMODULE_ALIAS(\"netdev-\" device)\n\n \n#define netdev_WARN(dev, format, args...)\t\t\t\\\n\tWARN(1, \"netdevice: %s%s: \" format, netdev_name(dev),\t\\\n\t     netdev_reg_state(dev), ##args)\n\n#define netdev_WARN_ONCE(dev, format, args...)\t\t\t\t\\\n\tWARN_ONCE(1, \"netdevice: %s%s: \" format, netdev_name(dev),\t\\\n\t\t  netdev_reg_state(dev), ##args)\n\n \n#define PTYPE_HASH_SIZE\t(16)\n#define PTYPE_HASH_MASK\t(PTYPE_HASH_SIZE - 1)\n\nextern struct list_head ptype_all __read_mostly;\nextern struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;\n\nextern struct net_device *blackhole_netdev;\n\n \n#define DEV_STATS_INC(DEV, FIELD) atomic_long_inc(&(DEV)->stats.__##FIELD)\n#define DEV_STATS_ADD(DEV, FIELD, VAL) \t\\\n\t\tatomic_long_add((VAL), &(DEV)->stats.__##FIELD)\n#define DEV_STATS_READ(DEV, FIELD) atomic_long_read(&(DEV)->stats.__##FIELD)\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}