{
  "module_name": "memcontrol.h",
  "hash_id": "4c1c4af69f22de83eb43a8187f6c7362d9198d9978660906e5ca09b87132dace",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/memcontrol.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_MEMCONTROL_H\n#define _LINUX_MEMCONTROL_H\n#include <linux/cgroup.h>\n#include <linux/vm_event_item.h>\n#include <linux/hardirq.h>\n#include <linux/jump_label.h>\n#include <linux/page_counter.h>\n#include <linux/vmpressure.h>\n#include <linux/eventfd.h>\n#include <linux/mm.h>\n#include <linux/vmstat.h>\n#include <linux/writeback.h>\n#include <linux/page-flags.h>\n\nstruct mem_cgroup;\nstruct obj_cgroup;\nstruct page;\nstruct mm_struct;\nstruct kmem_cache;\n\n \nenum memcg_stat_item {\n\tMEMCG_SWAP = NR_VM_NODE_STAT_ITEMS,\n\tMEMCG_SOCK,\n\tMEMCG_PERCPU_B,\n\tMEMCG_VMALLOC,\n\tMEMCG_KMEM,\n\tMEMCG_ZSWAP_B,\n\tMEMCG_ZSWAPPED,\n\tMEMCG_NR_STAT,\n};\n\nenum memcg_memory_event {\n\tMEMCG_LOW,\n\tMEMCG_HIGH,\n\tMEMCG_MAX,\n\tMEMCG_OOM,\n\tMEMCG_OOM_KILL,\n\tMEMCG_OOM_GROUP_KILL,\n\tMEMCG_SWAP_HIGH,\n\tMEMCG_SWAP_MAX,\n\tMEMCG_SWAP_FAIL,\n\tMEMCG_NR_MEMORY_EVENTS,\n};\n\nstruct mem_cgroup_reclaim_cookie {\n\tpg_data_t *pgdat;\n\tunsigned int generation;\n};\n\n#ifdef CONFIG_MEMCG\n\n#define MEM_CGROUP_ID_SHIFT\t16\n\nstruct mem_cgroup_id {\n\tint id;\n\trefcount_t ref;\n};\n\n \nenum mem_cgroup_events_target {\n\tMEM_CGROUP_TARGET_THRESH,\n\tMEM_CGROUP_TARGET_SOFTLIMIT,\n\tMEM_CGROUP_NTARGETS,\n};\n\nstruct memcg_vmstats_percpu;\nstruct memcg_vmstats;\n\nstruct mem_cgroup_reclaim_iter {\n\tstruct mem_cgroup *position;\n\t \n\tunsigned int generation;\n};\n\n \nstruct shrinker_info {\n\tstruct rcu_head rcu;\n\tatomic_long_t *nr_deferred;\n\tunsigned long *map;\n\tint map_nr_max;\n};\n\nstruct lruvec_stats_percpu {\n\t \n\tlong state[NR_VM_NODE_STAT_ITEMS];\n\n\t \n\tlong state_prev[NR_VM_NODE_STAT_ITEMS];\n};\n\nstruct lruvec_stats {\n\t \n\tlong state[NR_VM_NODE_STAT_ITEMS];\n\n\t \n\tlong state_local[NR_VM_NODE_STAT_ITEMS];\n\n\t \n\tlong state_pending[NR_VM_NODE_STAT_ITEMS];\n};\n\n \nstruct mem_cgroup_per_node {\n\tstruct lruvec\t\tlruvec;\n\n\tstruct lruvec_stats_percpu __percpu\t*lruvec_stats_percpu;\n\tstruct lruvec_stats\t\t\tlruvec_stats;\n\n\tunsigned long\t\tlru_zone_size[MAX_NR_ZONES][NR_LRU_LISTS];\n\n\tstruct mem_cgroup_reclaim_iter\titer;\n\n\tstruct shrinker_info __rcu\t*shrinker_info;\n\n\tstruct rb_node\t\ttree_node;\t \n\tunsigned long\t\tusage_in_excess; \n\t\t\t\t\t\t \n\tbool\t\t\ton_tree;\n\tstruct mem_cgroup\t*memcg;\t\t \n\t\t\t\t\t\t \n};\n\nstruct mem_cgroup_threshold {\n\tstruct eventfd_ctx *eventfd;\n\tunsigned long threshold;\n};\n\n \nstruct mem_cgroup_threshold_ary {\n\t \n\tint current_threshold;\n\t \n\tunsigned int size;\n\t \n\tstruct mem_cgroup_threshold entries[];\n};\n\nstruct mem_cgroup_thresholds {\n\t \n\tstruct mem_cgroup_threshold_ary *primary;\n\t \n\tstruct mem_cgroup_threshold_ary *spare;\n};\n\n \n#define MEMCG_CGWB_FRN_CNT\t4\n\nstruct memcg_cgwb_frn {\n\tu64 bdi_id;\t\t\t \n\tint memcg_id;\t\t\t \n\tu64 at;\t\t\t\t \n\tstruct wb_completion done;\t \n};\n\n \nstruct obj_cgroup {\n\tstruct percpu_ref refcnt;\n\tstruct mem_cgroup *memcg;\n\tatomic_t nr_charged_bytes;\n\tunion {\n\t\tstruct list_head list;  \n\t\tstruct rcu_head rcu;\n\t};\n};\n\n \nstruct mem_cgroup {\n\tstruct cgroup_subsys_state css;\n\n\t \n\tstruct mem_cgroup_id id;\n\n\t \n\tstruct page_counter memory;\t\t \n\n\tunion {\n\t\tstruct page_counter swap;\t \n\t\tstruct page_counter memsw;\t \n\t};\n\n\t \n\tstruct page_counter kmem;\t\t \n\tstruct page_counter tcpmem;\t\t \n\n\t \n\tstruct work_struct high_work;\n\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\n\tunsigned long zswap_max;\n#endif\n\n\tunsigned long soft_limit;\n\n\t \n\tstruct vmpressure vmpressure;\n\n\t \n\tbool oom_group;\n\n\t \n\tbool\t\toom_lock;\n\tint\t\tunder_oom;\n\n\tint\tswappiness;\n\t \n\tint\t\toom_kill_disable;\n\n\t \n\tstruct cgroup_file events_file;\n\tstruct cgroup_file events_local_file;\n\n\t \n\tstruct cgroup_file swap_events_file;\n\n\t \n\tstruct mutex thresholds_lock;\n\n\t \n\tstruct mem_cgroup_thresholds thresholds;\n\n\t \n\tstruct mem_cgroup_thresholds memsw_thresholds;\n\n\t \n\tstruct list_head oom_notify;\n\n\t \n\tunsigned long move_charge_at_immigrate;\n\t \n\tspinlock_t\t\tmove_lock;\n\tunsigned long\t\tmove_lock_flags;\n\n\tCACHELINE_PADDING(_pad1_);\n\n\t \n\tstruct memcg_vmstats\t*vmstats;\n\n\t \n\tatomic_long_t\t\tmemory_events[MEMCG_NR_MEMORY_EVENTS];\n\tatomic_long_t\t\tmemory_events_local[MEMCG_NR_MEMORY_EVENTS];\n\n\t \n\tunsigned long\t\tsocket_pressure;\n\n\t \n\tbool\t\t\ttcpmem_active;\n\tint\t\t\ttcpmem_pressure;\n\n#ifdef CONFIG_MEMCG_KMEM\n\tint kmemcg_id;\n\tstruct obj_cgroup __rcu *objcg;\n\t \n\tstruct list_head objcg_list;\n#endif\n\n\tCACHELINE_PADDING(_pad2_);\n\n\t \n\tatomic_t\t\tmoving_account;\n\tstruct task_struct\t*move_lock_task;\n\n\tstruct memcg_vmstats_percpu __percpu *vmstats_percpu;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tstruct list_head cgwb_list;\n\tstruct wb_domain cgwb_domain;\n\tstruct memcg_cgwb_frn cgwb_frn[MEMCG_CGWB_FRN_CNT];\n#endif\n\n\t \n\tstruct list_head event_list;\n\tspinlock_t event_list_lock;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct deferred_split deferred_split_queue;\n#endif\n\n#ifdef CONFIG_LRU_GEN\n\t \n\tstruct lru_gen_mm_list mm_list;\n#endif\n\n\tstruct mem_cgroup_per_node *nodeinfo[];\n};\n\n \n#define MEMCG_CHARGE_BATCH 64U\n\nextern struct mem_cgroup *root_mem_cgroup;\n\nenum page_memcg_data_flags {\n\t \n\tMEMCG_DATA_OBJCGS = (1UL << 0),\n\t \n\tMEMCG_DATA_KMEM = (1UL << 1),\n\t \n\t__NR_MEMCG_DATA_FLAGS  = (1UL << 2),\n};\n\n#define MEMCG_DATA_FLAGS_MASK (__NR_MEMCG_DATA_FLAGS - 1)\n\nstatic inline bool folio_memcg_kmem(struct folio *folio);\n\n \nstatic inline struct mem_cgroup *obj_cgroup_memcg(struct obj_cgroup *objcg)\n{\n\treturn READ_ONCE(objcg->memcg);\n}\n\n \nstatic inline struct mem_cgroup *__folio_memcg(struct folio *folio)\n{\n\tunsigned long memcg_data = folio->memcg_data;\n\n\tVM_BUG_ON_FOLIO(folio_test_slab(folio), folio);\n\tVM_BUG_ON_FOLIO(memcg_data & MEMCG_DATA_OBJCGS, folio);\n\tVM_BUG_ON_FOLIO(memcg_data & MEMCG_DATA_KMEM, folio);\n\n\treturn (struct mem_cgroup *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n}\n\n \nstatic inline struct obj_cgroup *__folio_objcg(struct folio *folio)\n{\n\tunsigned long memcg_data = folio->memcg_data;\n\n\tVM_BUG_ON_FOLIO(folio_test_slab(folio), folio);\n\tVM_BUG_ON_FOLIO(memcg_data & MEMCG_DATA_OBJCGS, folio);\n\tVM_BUG_ON_FOLIO(!(memcg_data & MEMCG_DATA_KMEM), folio);\n\n\treturn (struct obj_cgroup *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n}\n\n \nstatic inline struct mem_cgroup *folio_memcg(struct folio *folio)\n{\n\tif (folio_memcg_kmem(folio))\n\t\treturn obj_cgroup_memcg(__folio_objcg(folio));\n\treturn __folio_memcg(folio);\n}\n\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn folio_memcg(page_folio(page));\n}\n\n \nstatic inline struct mem_cgroup *folio_memcg_rcu(struct folio *folio)\n{\n\tunsigned long memcg_data = READ_ONCE(folio->memcg_data);\n\n\tVM_BUG_ON_FOLIO(folio_test_slab(folio), folio);\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (memcg_data & MEMCG_DATA_KMEM) {\n\t\tstruct obj_cgroup *objcg;\n\n\t\tobjcg = (void *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n\t\treturn obj_cgroup_memcg(objcg);\n\t}\n\n\treturn (struct mem_cgroup *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n}\n\n \nstatic inline struct mem_cgroup *folio_memcg_check(struct folio *folio)\n{\n\t \n\tunsigned long memcg_data = READ_ONCE(folio->memcg_data);\n\n\tif (memcg_data & MEMCG_DATA_OBJCGS)\n\t\treturn NULL;\n\n\tif (memcg_data & MEMCG_DATA_KMEM) {\n\t\tstruct obj_cgroup *objcg;\n\n\t\tobjcg = (void *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n\t\treturn obj_cgroup_memcg(objcg);\n\t}\n\n\treturn (struct mem_cgroup *)(memcg_data & ~MEMCG_DATA_FLAGS_MASK);\n}\n\nstatic inline struct mem_cgroup *page_memcg_check(struct page *page)\n{\n\tif (PageTail(page))\n\t\treturn NULL;\n\treturn folio_memcg_check((struct folio *)page);\n}\n\nstatic inline struct mem_cgroup *get_mem_cgroup_from_objcg(struct obj_cgroup *objcg)\n{\n\tstruct mem_cgroup *memcg;\n\n\trcu_read_lock();\nretry:\n\tmemcg = obj_cgroup_memcg(objcg);\n\tif (unlikely(!css_tryget(&memcg->css)))\n\t\tgoto retry;\n\trcu_read_unlock();\n\n\treturn memcg;\n}\n\n#ifdef CONFIG_MEMCG_KMEM\n \nstatic inline bool folio_memcg_kmem(struct folio *folio)\n{\n\tVM_BUG_ON_PGFLAGS(PageTail(&folio->page), &folio->page);\n\tVM_BUG_ON_FOLIO(folio->memcg_data & MEMCG_DATA_OBJCGS, folio);\n\treturn folio->memcg_data & MEMCG_DATA_KMEM;\n}\n\n\n#else\nstatic inline bool folio_memcg_kmem(struct folio *folio)\n{\n\treturn false;\n}\n\n#endif\n\nstatic inline bool PageMemcgKmem(struct page *page)\n{\n\treturn folio_memcg_kmem(page_folio(page));\n}\n\nstatic inline bool mem_cgroup_is_root(struct mem_cgroup *memcg)\n{\n\treturn (memcg == root_mem_cgroup);\n}\n\nstatic inline bool mem_cgroup_disabled(void)\n{\n\treturn !cgroup_subsys_enabled(memory_cgrp_subsys);\n}\n\nstatic inline void mem_cgroup_protection(struct mem_cgroup *root,\n\t\t\t\t\t struct mem_cgroup *memcg,\n\t\t\t\t\t unsigned long *min,\n\t\t\t\t\t unsigned long *low)\n{\n\t*min = *low = 0;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\t \n\tif (root == memcg)\n\t\treturn;\n\n\t*min = READ_ONCE(memcg->memory.emin);\n\t*low = READ_ONCE(memcg->memory.elow);\n}\n\nvoid mem_cgroup_calculate_protection(struct mem_cgroup *root,\n\t\t\t\t     struct mem_cgroup *memcg);\n\nstatic inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n\t\t\t\t\t  struct mem_cgroup *memcg)\n{\n\t \n\treturn mem_cgroup_disabled() || mem_cgroup_is_root(memcg) ||\n\t\tmemcg == target;\n}\n\nstatic inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n\t\t\t\t\tstruct mem_cgroup *memcg)\n{\n\tif (mem_cgroup_unprotected(target, memcg))\n\t\treturn false;\n\n\treturn READ_ONCE(memcg->memory.elow) >=\n\t\tpage_counter_read(&memcg->memory);\n}\n\nstatic inline bool mem_cgroup_below_min(struct mem_cgroup *target,\n\t\t\t\t\tstruct mem_cgroup *memcg)\n{\n\tif (mem_cgroup_unprotected(target, memcg))\n\t\treturn false;\n\n\treturn READ_ONCE(memcg->memory.emin) >=\n\t\tpage_counter_read(&memcg->memory);\n}\n\nint __mem_cgroup_charge(struct folio *folio, struct mm_struct *mm, gfp_t gfp);\n\n \nstatic inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,\n\t\t\t\t    gfp_t gfp)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn 0;\n\treturn __mem_cgroup_charge(folio, mm, gfp);\n}\n\nint mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,\n\t\t\t\t  gfp_t gfp, swp_entry_t entry);\nvoid mem_cgroup_swapin_uncharge_swap(swp_entry_t entry);\n\nvoid __mem_cgroup_uncharge(struct folio *folio);\n\n \nstatic inline void mem_cgroup_uncharge(struct folio *folio)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\t__mem_cgroup_uncharge(folio);\n}\n\nvoid __mem_cgroup_uncharge_list(struct list_head *page_list);\nstatic inline void mem_cgroup_uncharge_list(struct list_head *page_list)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\t__mem_cgroup_uncharge_list(page_list);\n}\n\nvoid mem_cgroup_migrate(struct folio *old, struct folio *new);\n\n \nstatic inline struct lruvec *mem_cgroup_lruvec(struct mem_cgroup *memcg,\n\t\t\t\t\t       struct pglist_data *pgdat)\n{\n\tstruct mem_cgroup_per_node *mz;\n\tstruct lruvec *lruvec;\n\n\tif (mem_cgroup_disabled()) {\n\t\tlruvec = &pgdat->__lruvec;\n\t\tgoto out;\n\t}\n\n\tif (!memcg)\n\t\tmemcg = root_mem_cgroup;\n\n\tmz = memcg->nodeinfo[pgdat->node_id];\n\tlruvec = &mz->lruvec;\nout:\n\t \n\tif (unlikely(lruvec->pgdat != pgdat))\n\t\tlruvec->pgdat = pgdat;\n\treturn lruvec;\n}\n\n \nstatic inline struct lruvec *folio_lruvec(struct folio *folio)\n{\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\n\tVM_WARN_ON_ONCE_FOLIO(!memcg && !mem_cgroup_disabled(), folio);\n\treturn mem_cgroup_lruvec(memcg, folio_pgdat(folio));\n}\n\nstruct mem_cgroup *mem_cgroup_from_task(struct task_struct *p);\n\nstruct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm);\n\nstruct lruvec *folio_lruvec_lock(struct folio *folio);\nstruct lruvec *folio_lruvec_lock_irq(struct folio *folio);\nstruct lruvec *folio_lruvec_lock_irqsave(struct folio *folio,\n\t\t\t\t\t\tunsigned long *flags);\n\n#ifdef CONFIG_DEBUG_VM\nvoid lruvec_memcg_debug(struct lruvec *lruvec, struct folio *folio);\n#else\nstatic inline\nvoid lruvec_memcg_debug(struct lruvec *lruvec, struct folio *folio)\n{\n}\n#endif\n\nstatic inline\nstruct mem_cgroup *mem_cgroup_from_css(struct cgroup_subsys_state *css){\n\treturn css ? container_of(css, struct mem_cgroup, css) : NULL;\n}\n\nstatic inline bool obj_cgroup_tryget(struct obj_cgroup *objcg)\n{\n\treturn percpu_ref_tryget(&objcg->refcnt);\n}\n\nstatic inline void obj_cgroup_get(struct obj_cgroup *objcg)\n{\n\tpercpu_ref_get(&objcg->refcnt);\n}\n\nstatic inline void obj_cgroup_get_many(struct obj_cgroup *objcg,\n\t\t\t\t       unsigned long nr)\n{\n\tpercpu_ref_get_many(&objcg->refcnt, nr);\n}\n\nstatic inline void obj_cgroup_put(struct obj_cgroup *objcg)\n{\n\tpercpu_ref_put(&objcg->refcnt);\n}\n\nstatic inline bool mem_cgroup_tryget(struct mem_cgroup *memcg)\n{\n\treturn !memcg || css_tryget(&memcg->css);\n}\n\nstatic inline void mem_cgroup_put(struct mem_cgroup *memcg)\n{\n\tif (memcg)\n\t\tcss_put(&memcg->css);\n}\n\n#define mem_cgroup_from_counter(counter, member)\t\\\n\tcontainer_of(counter, struct mem_cgroup, member)\n\nstruct mem_cgroup *mem_cgroup_iter(struct mem_cgroup *,\n\t\t\t\t   struct mem_cgroup *,\n\t\t\t\t   struct mem_cgroup_reclaim_cookie *);\nvoid mem_cgroup_iter_break(struct mem_cgroup *, struct mem_cgroup *);\nvoid mem_cgroup_scan_tasks(struct mem_cgroup *memcg,\n\t\t\t   int (*)(struct task_struct *, void *), void *arg);\n\nstatic inline unsigned short mem_cgroup_id(struct mem_cgroup *memcg)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn 0;\n\n\treturn memcg->id.id;\n}\nstruct mem_cgroup *mem_cgroup_from_id(unsigned short id);\n\n#ifdef CONFIG_SHRINKER_DEBUG\nstatic inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n{\n\treturn memcg ? cgroup_ino(memcg->css.cgroup) : 0;\n}\n\nstruct mem_cgroup *mem_cgroup_get_from_ino(unsigned long ino);\n#endif\n\nstatic inline struct mem_cgroup *mem_cgroup_from_seq(struct seq_file *m)\n{\n\treturn mem_cgroup_from_css(seq_css(m));\n}\n\nstatic inline struct mem_cgroup *lruvec_memcg(struct lruvec *lruvec)\n{\n\tstruct mem_cgroup_per_node *mz;\n\n\tif (mem_cgroup_disabled())\n\t\treturn NULL;\n\n\tmz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\treturn mz->memcg;\n}\n\n \nstatic inline struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg)\n{\n\treturn mem_cgroup_from_css(memcg->css.parent);\n}\n\nstatic inline bool mem_cgroup_is_descendant(struct mem_cgroup *memcg,\n\t\t\t      struct mem_cgroup *root)\n{\n\tif (root == memcg)\n\t\treturn true;\n\treturn cgroup_is_descendant(memcg->css.cgroup, root->css.cgroup);\n}\n\nstatic inline bool mm_match_cgroup(struct mm_struct *mm,\n\t\t\t\t   struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *task_memcg;\n\tbool match = false;\n\n\trcu_read_lock();\n\ttask_memcg = mem_cgroup_from_task(rcu_dereference(mm->owner));\n\tif (task_memcg)\n\t\tmatch = mem_cgroup_is_descendant(task_memcg, memcg);\n\trcu_read_unlock();\n\treturn match;\n}\n\nstruct cgroup_subsys_state *mem_cgroup_css_from_folio(struct folio *folio);\nino_t page_cgroup_ino(struct page *page);\n\nstatic inline bool mem_cgroup_online(struct mem_cgroup *memcg)\n{\n\tif (mem_cgroup_disabled())\n\t\treturn true;\n\treturn !!(memcg->css.flags & CSS_ONLINE);\n}\n\nvoid mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,\n\t\tint zid, int nr_pages);\n\nstatic inline\nunsigned long mem_cgroup_get_zone_lru_size(struct lruvec *lruvec,\n\t\tenum lru_list lru, int zone_idx)\n{\n\tstruct mem_cgroup_per_node *mz;\n\n\tmz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\treturn READ_ONCE(mz->lru_zone_size[zone_idx][lru]);\n}\n\nvoid mem_cgroup_handle_over_high(gfp_t gfp_mask);\n\nunsigned long mem_cgroup_get_max(struct mem_cgroup *memcg);\n\nunsigned long mem_cgroup_size(struct mem_cgroup *memcg);\n\nvoid mem_cgroup_print_oom_context(struct mem_cgroup *memcg,\n\t\t\t\tstruct task_struct *p);\n\nvoid mem_cgroup_print_oom_meminfo(struct mem_cgroup *memcg);\n\nstatic inline void mem_cgroup_enter_user_fault(void)\n{\n\tWARN_ON(current->in_user_fault);\n\tcurrent->in_user_fault = 1;\n}\n\nstatic inline void mem_cgroup_exit_user_fault(void)\n{\n\tWARN_ON(!current->in_user_fault);\n\tcurrent->in_user_fault = 0;\n}\n\nstatic inline bool task_in_memcg_oom(struct task_struct *p)\n{\n\treturn p->memcg_in_oom;\n}\n\nbool mem_cgroup_oom_synchronize(bool wait);\nstruct mem_cgroup *mem_cgroup_get_oom_group(struct task_struct *victim,\n\t\t\t\t\t    struct mem_cgroup *oom_domain);\nvoid mem_cgroup_print_oom_group(struct mem_cgroup *memcg);\n\nvoid folio_memcg_lock(struct folio *folio);\nvoid folio_memcg_unlock(struct folio *folio);\n\nvoid __mod_memcg_state(struct mem_cgroup *memcg, int idx, int val);\n\n \nstatic inline bool mem_cgroup_trylock_pages(struct mem_cgroup *memcg)\n{\n\trcu_read_lock();\n\n\tif (mem_cgroup_disabled() || !atomic_read(&memcg->moving_account))\n\t\treturn true;\n\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic inline void mem_cgroup_unlock_pages(void)\n{\n\trcu_read_unlock();\n}\n\n \nstatic inline void mod_memcg_state(struct mem_cgroup *memcg,\n\t\t\t\t   int idx, int val)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__mod_memcg_state(memcg, idx, val);\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void mod_memcg_page_state(struct page *page,\n\t\t\t\t\tint idx, int val)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = page_memcg(page);\n\tif (memcg)\n\t\tmod_memcg_state(memcg, idx, val);\n\trcu_read_unlock();\n}\n\nunsigned long memcg_page_state(struct mem_cgroup *memcg, int idx);\n\nstatic inline unsigned long lruvec_page_state(struct lruvec *lruvec,\n\t\t\t\t\t      enum node_stat_item idx)\n{\n\tstruct mem_cgroup_per_node *pn;\n\tlong x;\n\n\tif (mem_cgroup_disabled())\n\t\treturn node_page_state(lruvec_pgdat(lruvec), idx);\n\n\tpn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\tx = READ_ONCE(pn->lruvec_stats.state[idx]);\n#ifdef CONFIG_SMP\n\tif (x < 0)\n\t\tx = 0;\n#endif\n\treturn x;\n}\n\nstatic inline unsigned long lruvec_page_state_local(struct lruvec *lruvec,\n\t\t\t\t\t\t    enum node_stat_item idx)\n{\n\tstruct mem_cgroup_per_node *pn;\n\tlong x = 0;\n\n\tif (mem_cgroup_disabled())\n\t\treturn node_page_state(lruvec_pgdat(lruvec), idx);\n\n\tpn = container_of(lruvec, struct mem_cgroup_per_node, lruvec);\n\tx = READ_ONCE(pn->lruvec_stats.state_local[idx]);\n#ifdef CONFIG_SMP\n\tif (x < 0)\n\t\tx = 0;\n#endif\n\treturn x;\n}\n\nvoid mem_cgroup_flush_stats(void);\nvoid mem_cgroup_flush_stats_ratelimited(void);\n\nvoid __mod_memcg_lruvec_state(struct lruvec *lruvec, enum node_stat_item idx,\n\t\t\t      int val);\nvoid __mod_lruvec_kmem_state(void *p, enum node_stat_item idx, int val);\n\nstatic inline void mod_lruvec_kmem_state(void *p, enum node_stat_item idx,\n\t\t\t\t\t int val)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__mod_lruvec_kmem_state(p, idx, val);\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void mod_memcg_lruvec_state(struct lruvec *lruvec,\n\t\t\t\t\t  enum node_stat_item idx, int val)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__mod_memcg_lruvec_state(lruvec, idx, val);\n\tlocal_irq_restore(flags);\n}\n\nvoid __count_memcg_events(struct mem_cgroup *memcg, enum vm_event_item idx,\n\t\t\t  unsigned long count);\n\nstatic inline void count_memcg_events(struct mem_cgroup *memcg,\n\t\t\t\t      enum vm_event_item idx,\n\t\t\t\t      unsigned long count)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\t__count_memcg_events(memcg, idx, count);\n\tlocal_irq_restore(flags);\n}\n\nstatic inline void count_memcg_page_event(struct page *page,\n\t\t\t\t\t  enum vm_event_item idx)\n{\n\tstruct mem_cgroup *memcg = page_memcg(page);\n\n\tif (memcg)\n\t\tcount_memcg_events(memcg, idx, 1);\n}\n\nstatic inline void count_memcg_folio_events(struct folio *folio,\n\t\tenum vm_event_item idx, unsigned long nr)\n{\n\tstruct mem_cgroup *memcg = folio_memcg(folio);\n\n\tif (memcg)\n\t\tcount_memcg_events(memcg, idx, nr);\n}\n\nstatic inline void count_memcg_event_mm(struct mm_struct *mm,\n\t\t\t\t\tenum vm_event_item idx)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_task(rcu_dereference(mm->owner));\n\tif (likely(memcg))\n\t\tcount_memcg_events(memcg, idx, 1);\n\trcu_read_unlock();\n}\n\nstatic inline void memcg_memory_event(struct mem_cgroup *memcg,\n\t\t\t\t      enum memcg_memory_event event)\n{\n\tbool swap_event = event == MEMCG_SWAP_HIGH || event == MEMCG_SWAP_MAX ||\n\t\t\t  event == MEMCG_SWAP_FAIL;\n\n\tatomic_long_inc(&memcg->memory_events_local[event]);\n\tif (!swap_event)\n\t\tcgroup_file_notify(&memcg->events_local_file);\n\n\tdo {\n\t\tatomic_long_inc(&memcg->memory_events[event]);\n\t\tif (swap_event)\n\t\t\tcgroup_file_notify(&memcg->swap_events_file);\n\t\telse\n\t\t\tcgroup_file_notify(&memcg->events_file);\n\n\t\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\t\tbreak;\n\t\tif (cgrp_dfl_root.flags & CGRP_ROOT_MEMORY_LOCAL_EVENTS)\n\t\t\tbreak;\n\t} while ((memcg = parent_mem_cgroup(memcg)) &&\n\t\t !mem_cgroup_is_root(memcg));\n}\n\nstatic inline void memcg_memory_event_mm(struct mm_struct *mm,\n\t\t\t\t\t enum memcg_memory_event event)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = mem_cgroup_from_task(rcu_dereference(mm->owner));\n\tif (likely(memcg))\n\t\tmemcg_memory_event(memcg, event);\n\trcu_read_unlock();\n}\n\nvoid split_page_memcg(struct page *head, unsigned int nr);\n\nunsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,\n\t\t\t\t\t\tgfp_t gfp_mask,\n\t\t\t\t\t\tunsigned long *total_scanned);\n\n#else  \n\n#define MEM_CGROUP_ID_SHIFT\t0\n\nstatic inline struct mem_cgroup *folio_memcg(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *page_memcg(struct page *page)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *folio_memcg_rcu(struct folio *folio)\n{\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *folio_memcg_check(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *page_memcg_check(struct page *page)\n{\n\treturn NULL;\n}\n\nstatic inline bool folio_memcg_kmem(struct folio *folio)\n{\n\treturn false;\n}\n\nstatic inline bool PageMemcgKmem(struct page *page)\n{\n\treturn false;\n}\n\nstatic inline bool mem_cgroup_is_root(struct mem_cgroup *memcg)\n{\n\treturn true;\n}\n\nstatic inline bool mem_cgroup_disabled(void)\n{\n\treturn true;\n}\n\nstatic inline void memcg_memory_event(struct mem_cgroup *memcg,\n\t\t\t\t      enum memcg_memory_event event)\n{\n}\n\nstatic inline void memcg_memory_event_mm(struct mm_struct *mm,\n\t\t\t\t\t enum memcg_memory_event event)\n{\n}\n\nstatic inline void mem_cgroup_protection(struct mem_cgroup *root,\n\t\t\t\t\t struct mem_cgroup *memcg,\n\t\t\t\t\t unsigned long *min,\n\t\t\t\t\t unsigned long *low)\n{\n\t*min = *low = 0;\n}\n\nstatic inline void mem_cgroup_calculate_protection(struct mem_cgroup *root,\n\t\t\t\t\t\t   struct mem_cgroup *memcg)\n{\n}\n\nstatic inline bool mem_cgroup_unprotected(struct mem_cgroup *target,\n\t\t\t\t\t  struct mem_cgroup *memcg)\n{\n\treturn true;\n}\nstatic inline bool mem_cgroup_below_low(struct mem_cgroup *target,\n\t\t\t\t\tstruct mem_cgroup *memcg)\n{\n\treturn false;\n}\n\nstatic inline bool mem_cgroup_below_min(struct mem_cgroup *target,\n\t\t\t\t\tstruct mem_cgroup *memcg)\n{\n\treturn false;\n}\n\nstatic inline int mem_cgroup_charge(struct folio *folio,\n\t\tstruct mm_struct *mm, gfp_t gfp)\n{\n\treturn 0;\n}\n\nstatic inline int mem_cgroup_swapin_charge_folio(struct folio *folio,\n\t\t\tstruct mm_struct *mm, gfp_t gfp, swp_entry_t entry)\n{\n\treturn 0;\n}\n\nstatic inline void mem_cgroup_swapin_uncharge_swap(swp_entry_t entry)\n{\n}\n\nstatic inline void mem_cgroup_uncharge(struct folio *folio)\n{\n}\n\nstatic inline void mem_cgroup_uncharge_list(struct list_head *page_list)\n{\n}\n\nstatic inline void mem_cgroup_migrate(struct folio *old, struct folio *new)\n{\n}\n\nstatic inline struct lruvec *mem_cgroup_lruvec(struct mem_cgroup *memcg,\n\t\t\t\t\t       struct pglist_data *pgdat)\n{\n\treturn &pgdat->__lruvec;\n}\n\nstatic inline struct lruvec *folio_lruvec(struct folio *folio)\n{\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\treturn &pgdat->__lruvec;\n}\n\nstatic inline\nvoid lruvec_memcg_debug(struct lruvec *lruvec, struct folio *folio)\n{\n}\n\nstatic inline struct mem_cgroup *parent_mem_cgroup(struct mem_cgroup *memcg)\n{\n\treturn NULL;\n}\n\nstatic inline bool mm_match_cgroup(struct mm_struct *mm,\n\t\tstruct mem_cgroup *memcg)\n{\n\treturn true;\n}\n\nstatic inline struct mem_cgroup *get_mem_cgroup_from_mm(struct mm_struct *mm)\n{\n\treturn NULL;\n}\n\nstatic inline\nstruct mem_cgroup *mem_cgroup_from_css(struct cgroup_subsys_state *css)\n{\n\treturn NULL;\n}\n\nstatic inline void obj_cgroup_put(struct obj_cgroup *objcg)\n{\n}\n\nstatic inline bool mem_cgroup_tryget(struct mem_cgroup *memcg)\n{\n\treturn true;\n}\n\nstatic inline void mem_cgroup_put(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline struct lruvec *folio_lruvec_lock(struct folio *folio)\n{\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\n\tspin_lock(&pgdat->__lruvec.lru_lock);\n\treturn &pgdat->__lruvec;\n}\n\nstatic inline struct lruvec *folio_lruvec_lock_irq(struct folio *folio)\n{\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\n\tspin_lock_irq(&pgdat->__lruvec.lru_lock);\n\treturn &pgdat->__lruvec;\n}\n\nstatic inline struct lruvec *folio_lruvec_lock_irqsave(struct folio *folio,\n\t\tunsigned long *flagsp)\n{\n\tstruct pglist_data *pgdat = folio_pgdat(folio);\n\n\tspin_lock_irqsave(&pgdat->__lruvec.lru_lock, *flagsp);\n\treturn &pgdat->__lruvec;\n}\n\nstatic inline struct mem_cgroup *\nmem_cgroup_iter(struct mem_cgroup *root,\n\t\tstruct mem_cgroup *prev,\n\t\tstruct mem_cgroup_reclaim_cookie *reclaim)\n{\n\treturn NULL;\n}\n\nstatic inline void mem_cgroup_iter_break(struct mem_cgroup *root,\n\t\t\t\t\t struct mem_cgroup *prev)\n{\n}\n\nstatic inline void mem_cgroup_scan_tasks(struct mem_cgroup *memcg,\n\t\tint (*fn)(struct task_struct *, void *), void *arg)\n{\n}\n\nstatic inline unsigned short mem_cgroup_id(struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic inline struct mem_cgroup *mem_cgroup_from_id(unsigned short id)\n{\n\tWARN_ON_ONCE(id);\n\t \n\treturn NULL;\n}\n\n#ifdef CONFIG_SHRINKER_DEBUG\nstatic inline unsigned long mem_cgroup_ino(struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic inline struct mem_cgroup *mem_cgroup_get_from_ino(unsigned long ino)\n{\n\treturn NULL;\n}\n#endif\n\nstatic inline struct mem_cgroup *mem_cgroup_from_seq(struct seq_file *m)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *lruvec_memcg(struct lruvec *lruvec)\n{\n\treturn NULL;\n}\n\nstatic inline bool mem_cgroup_online(struct mem_cgroup *memcg)\n{\n\treturn true;\n}\n\nstatic inline\nunsigned long mem_cgroup_get_zone_lru_size(struct lruvec *lruvec,\n\t\tenum lru_list lru, int zone_idx)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long mem_cgroup_get_max(struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long mem_cgroup_size(struct mem_cgroup *memcg)\n{\n\treturn 0;\n}\n\nstatic inline void\nmem_cgroup_print_oom_context(struct mem_cgroup *memcg, struct task_struct *p)\n{\n}\n\nstatic inline void\nmem_cgroup_print_oom_meminfo(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void folio_memcg_lock(struct folio *folio)\n{\n}\n\nstatic inline void folio_memcg_unlock(struct folio *folio)\n{\n}\n\nstatic inline bool mem_cgroup_trylock_pages(struct mem_cgroup *memcg)\n{\n\t \n\trcu_read_lock();\n\treturn true;\n}\n\nstatic inline void mem_cgroup_unlock_pages(void)\n{\n\trcu_read_unlock();\n}\n\nstatic inline void mem_cgroup_handle_over_high(gfp_t gfp_mask)\n{\n}\n\nstatic inline void mem_cgroup_enter_user_fault(void)\n{\n}\n\nstatic inline void mem_cgroup_exit_user_fault(void)\n{\n}\n\nstatic inline bool task_in_memcg_oom(struct task_struct *p)\n{\n\treturn false;\n}\n\nstatic inline bool mem_cgroup_oom_synchronize(bool wait)\n{\n\treturn false;\n}\n\nstatic inline struct mem_cgroup *mem_cgroup_get_oom_group(\n\tstruct task_struct *victim, struct mem_cgroup *oom_domain)\n{\n\treturn NULL;\n}\n\nstatic inline void mem_cgroup_print_oom_group(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void __mod_memcg_state(struct mem_cgroup *memcg,\n\t\t\t\t     int idx,\n\t\t\t\t     int nr)\n{\n}\n\nstatic inline void mod_memcg_state(struct mem_cgroup *memcg,\n\t\t\t\t   int idx,\n\t\t\t\t   int nr)\n{\n}\n\nstatic inline void mod_memcg_page_state(struct page *page,\n\t\t\t\t\tint idx, int val)\n{\n}\n\nstatic inline unsigned long memcg_page_state(struct mem_cgroup *memcg, int idx)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long lruvec_page_state(struct lruvec *lruvec,\n\t\t\t\t\t      enum node_stat_item idx)\n{\n\treturn node_page_state(lruvec_pgdat(lruvec), idx);\n}\n\nstatic inline unsigned long lruvec_page_state_local(struct lruvec *lruvec,\n\t\t\t\t\t\t    enum node_stat_item idx)\n{\n\treturn node_page_state(lruvec_pgdat(lruvec), idx);\n}\n\nstatic inline void mem_cgroup_flush_stats(void)\n{\n}\n\nstatic inline void mem_cgroup_flush_stats_ratelimited(void)\n{\n}\n\nstatic inline void __mod_memcg_lruvec_state(struct lruvec *lruvec,\n\t\t\t\t\t    enum node_stat_item idx, int val)\n{\n}\n\nstatic inline void __mod_lruvec_kmem_state(void *p, enum node_stat_item idx,\n\t\t\t\t\t   int val)\n{\n\tstruct page *page = virt_to_head_page(p);\n\n\t__mod_node_page_state(page_pgdat(page), idx, val);\n}\n\nstatic inline void mod_lruvec_kmem_state(void *p, enum node_stat_item idx,\n\t\t\t\t\t int val)\n{\n\tstruct page *page = virt_to_head_page(p);\n\n\tmod_node_page_state(page_pgdat(page), idx, val);\n}\n\nstatic inline void count_memcg_events(struct mem_cgroup *memcg,\n\t\t\t\t      enum vm_event_item idx,\n\t\t\t\t      unsigned long count)\n{\n}\n\nstatic inline void __count_memcg_events(struct mem_cgroup *memcg,\n\t\t\t\t\tenum vm_event_item idx,\n\t\t\t\t\tunsigned long count)\n{\n}\n\nstatic inline void count_memcg_page_event(struct page *page,\n\t\t\t\t\t  int idx)\n{\n}\n\nstatic inline void count_memcg_folio_events(struct folio *folio,\n\t\tenum vm_event_item idx, unsigned long nr)\n{\n}\n\nstatic inline\nvoid count_memcg_event_mm(struct mm_struct *mm, enum vm_event_item idx)\n{\n}\n\nstatic inline void split_page_memcg(struct page *head, unsigned int nr)\n{\n}\n\nstatic inline\nunsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,\n\t\t\t\t\t    gfp_t gfp_mask,\n\t\t\t\t\t    unsigned long *total_scanned)\n{\n\treturn 0;\n}\n#endif  \n\nstatic inline void __inc_lruvec_kmem_state(void *p, enum node_stat_item idx)\n{\n\t__mod_lruvec_kmem_state(p, idx, 1);\n}\n\nstatic inline void __dec_lruvec_kmem_state(void *p, enum node_stat_item idx)\n{\n\t__mod_lruvec_kmem_state(p, idx, -1);\n}\n\nstatic inline struct lruvec *parent_lruvec(struct lruvec *lruvec)\n{\n\tstruct mem_cgroup *memcg;\n\n\tmemcg = lruvec_memcg(lruvec);\n\tif (!memcg)\n\t\treturn NULL;\n\tmemcg = parent_mem_cgroup(memcg);\n\tif (!memcg)\n\t\treturn NULL;\n\treturn mem_cgroup_lruvec(memcg, lruvec_pgdat(lruvec));\n}\n\nstatic inline void unlock_page_lruvec(struct lruvec *lruvec)\n{\n\tspin_unlock(&lruvec->lru_lock);\n}\n\nstatic inline void unlock_page_lruvec_irq(struct lruvec *lruvec)\n{\n\tspin_unlock_irq(&lruvec->lru_lock);\n}\n\nstatic inline void unlock_page_lruvec_irqrestore(struct lruvec *lruvec,\n\t\tunsigned long flags)\n{\n\tspin_unlock_irqrestore(&lruvec->lru_lock, flags);\n}\n\n \nstatic inline bool folio_matches_lruvec(struct folio *folio,\n\t\tstruct lruvec *lruvec)\n{\n\treturn lruvec_pgdat(lruvec) == folio_pgdat(folio) &&\n\t       lruvec_memcg(lruvec) == folio_memcg(folio);\n}\n\n \nstatic inline struct lruvec *folio_lruvec_relock_irq(struct folio *folio,\n\t\tstruct lruvec *locked_lruvec)\n{\n\tif (locked_lruvec) {\n\t\tif (folio_matches_lruvec(folio, locked_lruvec))\n\t\t\treturn locked_lruvec;\n\n\t\tunlock_page_lruvec_irq(locked_lruvec);\n\t}\n\n\treturn folio_lruvec_lock_irq(folio);\n}\n\n \nstatic inline struct lruvec *folio_lruvec_relock_irqsave(struct folio *folio,\n\t\tstruct lruvec *locked_lruvec, unsigned long *flags)\n{\n\tif (locked_lruvec) {\n\t\tif (folio_matches_lruvec(folio, locked_lruvec))\n\t\t\treturn locked_lruvec;\n\n\t\tunlock_page_lruvec_irqrestore(locked_lruvec, *flags);\n\t}\n\n\treturn folio_lruvec_lock_irqsave(folio, flags);\n}\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\nstruct wb_domain *mem_cgroup_wb_domain(struct bdi_writeback *wb);\nvoid mem_cgroup_wb_stats(struct bdi_writeback *wb, unsigned long *pfilepages,\n\t\t\t unsigned long *pheadroom, unsigned long *pdirty,\n\t\t\t unsigned long *pwriteback);\n\nvoid mem_cgroup_track_foreign_dirty_slowpath(struct folio *folio,\n\t\t\t\t\t     struct bdi_writeback *wb);\n\nstatic inline void mem_cgroup_track_foreign_dirty(struct folio *folio,\n\t\t\t\t\t\t  struct bdi_writeback *wb)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (mem_cgroup_disabled())\n\t\treturn;\n\n\tmemcg = folio_memcg(folio);\n\tif (unlikely(memcg && &memcg->css != wb->memcg_css))\n\t\tmem_cgroup_track_foreign_dirty_slowpath(folio, wb);\n}\n\nvoid mem_cgroup_flush_foreign(struct bdi_writeback *wb);\n\n#else\t \n\nstatic inline struct wb_domain *mem_cgroup_wb_domain(struct bdi_writeback *wb)\n{\n\treturn NULL;\n}\n\nstatic inline void mem_cgroup_wb_stats(struct bdi_writeback *wb,\n\t\t\t\t       unsigned long *pfilepages,\n\t\t\t\t       unsigned long *pheadroom,\n\t\t\t\t       unsigned long *pdirty,\n\t\t\t\t       unsigned long *pwriteback)\n{\n}\n\nstatic inline void mem_cgroup_track_foreign_dirty(struct folio *folio,\n\t\t\t\t\t\t  struct bdi_writeback *wb)\n{\n}\n\nstatic inline void mem_cgroup_flush_foreign(struct bdi_writeback *wb)\n{\n}\n\n#endif\t \n\nstruct sock;\nbool mem_cgroup_charge_skmem(struct mem_cgroup *memcg, unsigned int nr_pages,\n\t\t\t     gfp_t gfp_mask);\nvoid mem_cgroup_uncharge_skmem(struct mem_cgroup *memcg, unsigned int nr_pages);\n#ifdef CONFIG_MEMCG\nextern struct static_key_false memcg_sockets_enabled_key;\n#define mem_cgroup_sockets_enabled static_branch_unlikely(&memcg_sockets_enabled_key)\nvoid mem_cgroup_sk_alloc(struct sock *sk);\nvoid mem_cgroup_sk_free(struct sock *sk);\nstatic inline bool mem_cgroup_under_socket_pressure(struct mem_cgroup *memcg)\n{\n\tif (!cgroup_subsys_on_dfl(memory_cgrp_subsys))\n\t\treturn !!memcg->tcpmem_pressure;\n\tdo {\n\t\tif (time_before(jiffies, READ_ONCE(memcg->socket_pressure)))\n\t\t\treturn true;\n\t} while ((memcg = parent_mem_cgroup(memcg)));\n\treturn false;\n}\n\nint alloc_shrinker_info(struct mem_cgroup *memcg);\nvoid free_shrinker_info(struct mem_cgroup *memcg);\nvoid set_shrinker_bit(struct mem_cgroup *memcg, int nid, int shrinker_id);\nvoid reparent_shrinker_deferred(struct mem_cgroup *memcg);\n#else\n#define mem_cgroup_sockets_enabled 0\nstatic inline void mem_cgroup_sk_alloc(struct sock *sk) { };\nstatic inline void mem_cgroup_sk_free(struct sock *sk) { };\nstatic inline bool mem_cgroup_under_socket_pressure(struct mem_cgroup *memcg)\n{\n\treturn false;\n}\n\nstatic inline void set_shrinker_bit(struct mem_cgroup *memcg,\n\t\t\t\t    int nid, int shrinker_id)\n{\n}\n#endif\n\n#ifdef CONFIG_MEMCG_KMEM\nbool mem_cgroup_kmem_disabled(void);\nint __memcg_kmem_charge_page(struct page *page, gfp_t gfp, int order);\nvoid __memcg_kmem_uncharge_page(struct page *page, int order);\n\nstruct obj_cgroup *get_obj_cgroup_from_current(void);\nstruct obj_cgroup *get_obj_cgroup_from_folio(struct folio *folio);\n\nint obj_cgroup_charge(struct obj_cgroup *objcg, gfp_t gfp, size_t size);\nvoid obj_cgroup_uncharge(struct obj_cgroup *objcg, size_t size);\n\nextern struct static_key_false memcg_bpf_enabled_key;\nstatic inline bool memcg_bpf_enabled(void)\n{\n\treturn static_branch_likely(&memcg_bpf_enabled_key);\n}\n\nextern struct static_key_false memcg_kmem_online_key;\n\nstatic inline bool memcg_kmem_online(void)\n{\n\treturn static_branch_likely(&memcg_kmem_online_key);\n}\n\nstatic inline int memcg_kmem_charge_page(struct page *page, gfp_t gfp,\n\t\t\t\t\t int order)\n{\n\tif (memcg_kmem_online())\n\t\treturn __memcg_kmem_charge_page(page, gfp, order);\n\treturn 0;\n}\n\nstatic inline void memcg_kmem_uncharge_page(struct page *page, int order)\n{\n\tif (memcg_kmem_online())\n\t\t__memcg_kmem_uncharge_page(page, order);\n}\n\n \nstatic inline int memcg_kmem_id(struct mem_cgroup *memcg)\n{\n\treturn memcg ? memcg->kmemcg_id : -1;\n}\n\nstruct mem_cgroup *mem_cgroup_from_obj(void *p);\nstruct mem_cgroup *mem_cgroup_from_slab_obj(void *p);\n\nstatic inline void count_objcg_event(struct obj_cgroup *objcg,\n\t\t\t\t     enum vm_event_item idx)\n{\n\tstruct mem_cgroup *memcg;\n\n\tif (!memcg_kmem_online())\n\t\treturn;\n\n\trcu_read_lock();\n\tmemcg = obj_cgroup_memcg(objcg);\n\tcount_memcg_events(memcg, idx, 1);\n\trcu_read_unlock();\n}\n\n#else\nstatic inline bool mem_cgroup_kmem_disabled(void)\n{\n\treturn true;\n}\n\nstatic inline int memcg_kmem_charge_page(struct page *page, gfp_t gfp,\n\t\t\t\t\t int order)\n{\n\treturn 0;\n}\n\nstatic inline void memcg_kmem_uncharge_page(struct page *page, int order)\n{\n}\n\nstatic inline int __memcg_kmem_charge_page(struct page *page, gfp_t gfp,\n\t\t\t\t\t   int order)\n{\n\treturn 0;\n}\n\nstatic inline void __memcg_kmem_uncharge_page(struct page *page, int order)\n{\n}\n\nstatic inline struct obj_cgroup *get_obj_cgroup_from_folio(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic inline bool memcg_bpf_enabled(void)\n{\n\treturn false;\n}\n\nstatic inline bool memcg_kmem_online(void)\n{\n\treturn false;\n}\n\nstatic inline int memcg_kmem_id(struct mem_cgroup *memcg)\n{\n\treturn -1;\n}\n\nstatic inline struct mem_cgroup *mem_cgroup_from_obj(void *p)\n{\n\treturn NULL;\n}\n\nstatic inline struct mem_cgroup *mem_cgroup_from_slab_obj(void *p)\n{\n\treturn NULL;\n}\n\nstatic inline void count_objcg_event(struct obj_cgroup *objcg,\n\t\t\t\t     enum vm_event_item idx)\n{\n}\n\n#endif  \n\n#if defined(CONFIG_MEMCG_KMEM) && defined(CONFIG_ZSWAP)\nbool obj_cgroup_may_zswap(struct obj_cgroup *objcg);\nvoid obj_cgroup_charge_zswap(struct obj_cgroup *objcg, size_t size);\nvoid obj_cgroup_uncharge_zswap(struct obj_cgroup *objcg, size_t size);\n#else\nstatic inline bool obj_cgroup_may_zswap(struct obj_cgroup *objcg)\n{\n\treturn true;\n}\nstatic inline void obj_cgroup_charge_zswap(struct obj_cgroup *objcg,\n\t\t\t\t\t   size_t size)\n{\n}\nstatic inline void obj_cgroup_uncharge_zswap(struct obj_cgroup *objcg,\n\t\t\t\t\t     size_t size)\n{\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}