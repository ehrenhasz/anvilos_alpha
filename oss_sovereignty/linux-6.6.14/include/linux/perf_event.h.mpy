{
  "module_name": "perf_event.h",
  "hash_id": "b504306a93162d98a36d2e747b96de5b495272558065cce6e862d22ce8d7145d",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/perf_event.h",
  "human_readable_source": " \n#ifndef _LINUX_PERF_EVENT_H\n#define _LINUX_PERF_EVENT_H\n\n#include <uapi/linux/perf_event.h>\n#include <uapi/linux/bpf_perf_event.h>\n\n \n\n#ifdef CONFIG_PERF_EVENTS\n# include <asm/perf_event.h>\n# include <asm/local64.h>\n#endif\n\n#define PERF_GUEST_ACTIVE\t0x01\n#define PERF_GUEST_USER\t0x02\n\nstruct perf_guest_info_callbacks {\n\tunsigned int\t\t\t(*state)(void);\n\tunsigned long\t\t\t(*get_ip)(void);\n\tunsigned int\t\t\t(*handle_intel_pt_intr)(void);\n};\n\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n#include <linux/rhashtable-types.h>\n#include <asm/hw_breakpoint.h>\n#endif\n\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/rculist.h>\n#include <linux/rcupdate.h>\n#include <linux/spinlock.h>\n#include <linux/hrtimer.h>\n#include <linux/fs.h>\n#include <linux/pid_namespace.h>\n#include <linux/workqueue.h>\n#include <linux/ftrace.h>\n#include <linux/cpu.h>\n#include <linux/irq_work.h>\n#include <linux/static_key.h>\n#include <linux/jump_label_ratelimit.h>\n#include <linux/atomic.h>\n#include <linux/sysfs.h>\n#include <linux/perf_regs.h>\n#include <linux/cgroup.h>\n#include <linux/refcount.h>\n#include <linux/security.h>\n#include <linux/static_call.h>\n#include <linux/lockdep.h>\n#include <asm/local.h>\n\nstruct perf_callchain_entry {\n\t__u64\t\t\t\tnr;\n\t__u64\t\t\t\tip[];  \n};\n\nstruct perf_callchain_entry_ctx {\n\tstruct perf_callchain_entry *entry;\n\tu32\t\t\t    max_stack;\n\tu32\t\t\t    nr;\n\tshort\t\t\t    contexts;\n\tbool\t\t\t    contexts_maxed;\n};\n\ntypedef unsigned long (*perf_copy_f)(void *dst, const void *src,\n\t\t\t\t     unsigned long off, unsigned long len);\n\nstruct perf_raw_frag {\n\tunion {\n\t\tstruct perf_raw_frag\t*next;\n\t\tunsigned long\t\tpad;\n\t};\n\tperf_copy_f\t\t\tcopy;\n\tvoid\t\t\t\t*data;\n\tu32\t\t\t\tsize;\n} __packed;\n\nstruct perf_raw_record {\n\tstruct perf_raw_frag\t\tfrag;\n\tu32\t\t\t\tsize;\n};\n\nstatic __always_inline bool perf_raw_frag_last(const struct perf_raw_frag *frag)\n{\n\treturn frag->pad < sizeof(u64);\n}\n\n \nstruct perf_branch_stack {\n\t__u64\t\t\t\tnr;\n\t__u64\t\t\t\thw_idx;\n\tstruct perf_branch_entry\tentries[];\n};\n\nstruct task_struct;\n\n \nstruct hw_perf_event_extra {\n\tu64\t\tconfig;\t \n\tunsigned int\treg;\t \n\tint\t\talloc;\t \n\tint\t\tidx;\t \n};\n\n \n#define PERF_EVENT_FLAG_ARCH\t\t\t0x000fffff\n#define PERF_EVENT_FLAG_USER_READ_CNT\t\t0x80000000\n\nstatic_assert((PERF_EVENT_FLAG_USER_READ_CNT & PERF_EVENT_FLAG_ARCH) == 0);\n\n \nstruct hw_perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\tunion {\n\t\tstruct {  \n\t\t\tu64\t\tconfig;\n\t\t\tu64\t\tlast_tag;\n\t\t\tunsigned long\tconfig_base;\n\t\t\tunsigned long\tevent_base;\n\t\t\tint\t\tevent_base_rdpmc;\n\t\t\tint\t\tidx;\n\t\t\tint\t\tlast_cpu;\n\t\t\tint\t\tflags;\n\n\t\t\tstruct hw_perf_event_extra extra_reg;\n\t\t\tstruct hw_perf_event_extra branch_reg;\n\t\t};\n\t\tstruct {  \n\t\t\tstruct hrtimer\thrtimer;\n\t\t};\n\t\tstruct {  \n\t\t\t \n\t\t\tstruct list_head\ttp_list;\n\t\t};\n\t\tstruct {  \n\t\t\tu64\tpwr_acc;\n\t\t\tu64\tptsc;\n\t\t};\n#ifdef CONFIG_HAVE_HW_BREAKPOINT\n\t\tstruct {  \n\t\t\t \n\t\t\tstruct arch_hw_breakpoint\tinfo;\n\t\t\tstruct rhlist_head\t\tbp_list;\n\t\t};\n#endif\n\t\tstruct {  \n\t\t\tu8\tiommu_bank;\n\t\t\tu8\tiommu_cntr;\n\t\t\tu16\tpadding;\n\t\t\tu64\tconf;\n\t\t\tu64\tconf1;\n\t\t};\n\t};\n\t \n\tstruct task_struct\t\t*target;\n\n\t \n\tvoid\t\t\t\t*addr_filters;\n\n\t \n\tunsigned long\t\t\taddr_filters_gen;\n\n \n#define PERF_HES_STOPPED\t0x01  \n#define PERF_HES_UPTODATE\t0x02  \n#define PERF_HES_ARCH\t\t0x04\n\n\tint\t\t\t\tstate;\n\n\t \n\tlocal64_t\t\t\tprev_count;\n\n\t \n\tu64\t\t\t\tsample_period;\n\n\tunion {\n\t\tstruct {  \n\t\t\t \n\t\t\tu64\t\t\t\tlast_period;\n\n\t\t\t \n\t\t\tlocal64_t\t\t\tperiod_left;\n\t\t};\n\t\tstruct {  \n\t\t\tu64\t\t\t\tsaved_metric;\n\t\t\tu64\t\t\t\tsaved_slots;\n\t\t};\n\t};\n\n\t \n\tu64                             interrupts_seq;\n\tu64\t\t\t\tinterrupts;\n\n\t \n\tu64\t\t\t\tfreq_time_stamp;\n\tu64\t\t\t\tfreq_count_stamp;\n#endif\n};\n\nstruct perf_event;\nstruct perf_event_pmu_context;\n\n \n#define PERF_PMU_TXN_ADD  0x1\t\t \n#define PERF_PMU_TXN_READ 0x2\t\t \n\n \n#define PERF_PMU_CAP_NO_INTERRUPT\t\t0x0001\n#define PERF_PMU_CAP_NO_NMI\t\t\t0x0002\n#define PERF_PMU_CAP_AUX_NO_SG\t\t\t0x0004\n#define PERF_PMU_CAP_EXTENDED_REGS\t\t0x0008\n#define PERF_PMU_CAP_EXCLUSIVE\t\t\t0x0010\n#define PERF_PMU_CAP_ITRACE\t\t\t0x0020\n#define PERF_PMU_CAP_NO_EXCLUDE\t\t\t0x0040\n#define PERF_PMU_CAP_AUX_OUTPUT\t\t\t0x0080\n#define PERF_PMU_CAP_EXTENDED_HW_TYPE\t\t0x0100\n\nstruct perf_output_handle;\n\n#define PMU_NULL_DEV\t((void *)(~0UL))\n\n \nstruct pmu {\n\tstruct list_head\t\tentry;\n\n\tstruct module\t\t\t*module;\n\tstruct device\t\t\t*dev;\n\tstruct device\t\t\t*parent;\n\tconst struct attribute_group\t**attr_groups;\n\tconst struct attribute_group\t**attr_update;\n\tconst char\t\t\t*name;\n\tint\t\t\t\ttype;\n\n\t \n\tint\t\t\t\tcapabilities;\n\n\tint __percpu\t\t\t*pmu_disable_count;\n\tstruct perf_cpu_pmu_context __percpu *cpu_pmu_context;\n\tatomic_t\t\t\texclusive_cnt;  \n\tint\t\t\t\ttask_ctx_nr;\n\tint\t\t\t\thrtimer_interval_ms;\n\n\t \n\tunsigned int\t\t\tnr_addr_filters;\n\n\t \n\tvoid (*pmu_enable)\t\t(struct pmu *pmu);  \n\tvoid (*pmu_disable)\t\t(struct pmu *pmu);  \n\n\t \n\tint (*event_init)\t\t(struct perf_event *event);\n\n\t \n\tvoid (*event_mapped)\t\t(struct perf_event *event, struct mm_struct *mm);  \n\tvoid (*event_unmapped)\t\t(struct perf_event *event, struct mm_struct *mm);  \n\n\t \n#define PERF_EF_START\t0x01\t\t \n#define PERF_EF_RELOAD\t0x02\t\t \n#define PERF_EF_UPDATE\t0x04\t\t \n\n\t \n\tint  (*add)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*del)\t\t\t(struct perf_event *event, int flags);\n\n\t \n\tvoid (*start)\t\t\t(struct perf_event *event, int flags);\n\tvoid (*stop)\t\t\t(struct perf_event *event, int flags);\n\n\t \n\tvoid (*read)\t\t\t(struct perf_event *event);\n\n\t \n\tvoid (*start_txn)\t\t(struct pmu *pmu, unsigned int txn_flags);\n\t \n\tint  (*commit_txn)\t\t(struct pmu *pmu);\n\t \n\tvoid (*cancel_txn)\t\t(struct pmu *pmu);\n\n\t \n\tint (*event_idx)\t\t(struct perf_event *event);  \n\n\t \n\tvoid (*sched_task)\t\t(struct perf_event_pmu_context *pmu_ctx,\n\t\t\t\t\tbool sched_in);\n\n\t \n\tstruct kmem_cache\t\t*task_ctx_cache;\n\n\t \n\tvoid (*swap_task_ctx)\t\t(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t\t struct perf_event_pmu_context *next_epc);\n\t\t\t\t\t \n\n\t \n\tvoid *(*setup_aux)\t\t(struct perf_event *event, void **pages,\n\t\t\t\t\t int nr_pages, bool overwrite);\n\t\t\t\t\t \n\n\t \n\tvoid (*free_aux)\t\t(void *aux);  \n\n\t \n\tlong (*snapshot_aux)\t\t(struct perf_event *event,\n\t\t\t\t\t struct perf_output_handle *handle,\n\t\t\t\t\t unsigned long size);\n\n\t \n\tint (*addr_filters_validate)\t(struct list_head *filters);\n\t\t\t\t\t \n\n\t \n\tvoid (*addr_filters_sync)\t(struct perf_event *event);\n\t\t\t\t\t \n\n\t \n\tint (*aux_output_match)\t\t(struct perf_event *event);\n\t\t\t\t\t \n\n\t \n\tbool (*filter)\t\t\t(struct pmu *pmu, int cpu);  \n\n\t \n\tint (*check_period)\t\t(struct perf_event *event, u64 value);  \n};\n\nenum perf_addr_filter_action_t {\n\tPERF_ADDR_FILTER_ACTION_STOP = 0,\n\tPERF_ADDR_FILTER_ACTION_START,\n\tPERF_ADDR_FILTER_ACTION_FILTER,\n};\n\n \nstruct perf_addr_filter {\n\tstruct list_head\tentry;\n\tstruct path\t\tpath;\n\tunsigned long\t\toffset;\n\tunsigned long\t\tsize;\n\tenum perf_addr_filter_action_t\taction;\n};\n\n \nstruct perf_addr_filters_head {\n\tstruct list_head\tlist;\n\traw_spinlock_t\t\tlock;\n\tunsigned int\t\tnr_file_filters;\n};\n\nstruct perf_addr_filter_range {\n\tunsigned long\t\tstart;\n\tunsigned long\t\tsize;\n};\n\n \nenum perf_event_state {\n\tPERF_EVENT_STATE_DEAD\t\t= -4,\n\tPERF_EVENT_STATE_EXIT\t\t= -3,\n\tPERF_EVENT_STATE_ERROR\t\t= -2,\n\tPERF_EVENT_STATE_OFF\t\t= -1,\n\tPERF_EVENT_STATE_INACTIVE\t=  0,\n\tPERF_EVENT_STATE_ACTIVE\t\t=  1,\n};\n\nstruct file;\nstruct perf_sample_data;\n\ntypedef void (*perf_overflow_handler_t)(struct perf_event *,\n\t\t\t\t\tstruct perf_sample_data *,\n\t\t\t\t\tstruct pt_regs *regs);\n\n \n#define PERF_EV_CAP_SOFTWARE\t\tBIT(0)\n#define PERF_EV_CAP_READ_ACTIVE_PKG\tBIT(1)\n#define PERF_EV_CAP_SIBLING\t\tBIT(2)\n\n#define SWEVENT_HLIST_BITS\t\t8\n#define SWEVENT_HLIST_SIZE\t\t(1 << SWEVENT_HLIST_BITS)\n\nstruct swevent_hlist {\n\tstruct hlist_head\t\theads[SWEVENT_HLIST_SIZE];\n\tstruct rcu_head\t\t\trcu_head;\n};\n\n#define PERF_ATTACH_CONTEXT\t0x01\n#define PERF_ATTACH_GROUP\t0x02\n#define PERF_ATTACH_TASK\t0x04\n#define PERF_ATTACH_TASK_DATA\t0x08\n#define PERF_ATTACH_ITRACE\t0x10\n#define PERF_ATTACH_SCHED_CB\t0x20\n#define PERF_ATTACH_CHILD\t0x40\n\nstruct bpf_prog;\nstruct perf_cgroup;\nstruct perf_buffer;\n\nstruct pmu_event_list {\n\traw_spinlock_t\t\tlock;\n\tstruct list_head\tlist;\n};\n\n \n#ifdef CONFIG_PROVE_LOCKING\n#define lockdep_assert_event_ctx(event)\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled &&\t\t\t\\\n\t\t     (this_cpu_read(hardirqs_enabled) &&\t\\\n\t\t      lockdep_is_held(&(event)->ctx->mutex) != LOCK_STATE_HELD))\n#else\n#define lockdep_assert_event_ctx(event)\n#endif\n\n#define for_each_sibling_event(sibling, event)\t\t\t\\\n\tlockdep_assert_event_ctx(event);\t\t\t\\\n\tif ((event)->group_leader == (event))\t\t\t\\\n\t\tlist_for_each_entry((sibling), &(event)->sibling_list, sibling_list)\n\n \nstruct perf_event {\n#ifdef CONFIG_PERF_EVENTS\n\t \n\tstruct list_head\t\tevent_entry;\n\n\t \n\tstruct list_head\t\tsibling_list;\n\tstruct list_head\t\tactive_list;\n\t \n\tstruct rb_node\t\t\tgroup_node;\n\tu64\t\t\t\tgroup_index;\n\t \n\tstruct list_head\t\tmigrate_entry;\n\n\tstruct hlist_node\t\thlist_entry;\n\tstruct list_head\t\tactive_entry;\n\tint\t\t\t\tnr_siblings;\n\n\t \n\tint\t\t\t\tevent_caps;\n\t \n\tint\t\t\t\tgroup_caps;\n\n\tunsigned int\t\t\tgroup_generation;\n\tstruct perf_event\t\t*group_leader;\n\t \n\tstruct pmu\t\t\t*pmu;\n\tvoid\t\t\t\t*pmu_private;\n\n\tenum perf_event_state\t\tstate;\n\tunsigned int\t\t\tattach_state;\n\tlocal64_t\t\t\tcount;\n\tatomic64_t\t\t\tchild_count;\n\n\t \n\tu64\t\t\t\ttotal_time_enabled;\n\tu64\t\t\t\ttotal_time_running;\n\tu64\t\t\t\ttstamp;\n\n\tstruct perf_event_attr\t\tattr;\n\tu16\t\t\t\theader_size;\n\tu16\t\t\t\tid_header_size;\n\tu16\t\t\t\tread_size;\n\tstruct hw_perf_event\t\thw;\n\n\tstruct perf_event_context\t*ctx;\n\t \n\tstruct perf_event_pmu_context\t*pmu_ctx;\n\tatomic_long_t\t\t\trefcount;\n\n\t \n\tatomic64_t\t\t\tchild_total_time_enabled;\n\tatomic64_t\t\t\tchild_total_time_running;\n\n\t \n\tstruct mutex\t\t\tchild_mutex;\n\tstruct list_head\t\tchild_list;\n\tstruct perf_event\t\t*parent;\n\n\tint\t\t\t\toncpu;\n\tint\t\t\t\tcpu;\n\n\tstruct list_head\t\towner_entry;\n\tstruct task_struct\t\t*owner;\n\n\t \n\tstruct mutex\t\t\tmmap_mutex;\n\tatomic_t\t\t\tmmap_count;\n\n\tstruct perf_buffer\t\t*rb;\n\tstruct list_head\t\trb_entry;\n\tunsigned long\t\t\trcu_batches;\n\tint\t\t\t\trcu_pending;\n\n\t \n\twait_queue_head_t\t\twaitq;\n\tstruct fasync_struct\t\t*fasync;\n\n\t \n\tunsigned int\t\t\tpending_wakeup;\n\tunsigned int\t\t\tpending_kill;\n\tunsigned int\t\t\tpending_disable;\n\tunsigned int\t\t\tpending_sigtrap;\n\tunsigned long\t\t\tpending_addr;\t \n\tstruct irq_work\t\t\tpending_irq;\n\tstruct callback_head\t\tpending_task;\n\tunsigned int\t\t\tpending_work;\n\n\tatomic_t\t\t\tevent_limit;\n\n\t \n\tstruct perf_addr_filters_head\taddr_filters;\n\t \n\tstruct perf_addr_filter_range\t*addr_filter_ranges;\n\tunsigned long\t\t\taddr_filters_gen;\n\n\t \n\tstruct perf_event\t\t*aux_event;\n\n\tvoid (*destroy)(struct perf_event *);\n\tstruct rcu_head\t\t\trcu_head;\n\n\tstruct pid_namespace\t\t*ns;\n\tu64\t\t\t\tid;\n\n\tatomic64_t\t\t\tlost_samples;\n\n\tu64\t\t\t\t(*clock)(void);\n\tperf_overflow_handler_t\t\toverflow_handler;\n\tvoid\t\t\t\t*overflow_handler_context;\n#ifdef CONFIG_BPF_SYSCALL\n\tperf_overflow_handler_t\t\torig_overflow_handler;\n\tstruct bpf_prog\t\t\t*prog;\n\tu64\t\t\t\tbpf_cookie;\n#endif\n\n#ifdef CONFIG_EVENT_TRACING\n\tstruct trace_event_call\t\t*tp_event;\n\tstruct event_filter\t\t*filter;\n#ifdef CONFIG_FUNCTION_TRACER\n\tstruct ftrace_ops               ftrace_ops;\n#endif\n#endif\n\n#ifdef CONFIG_CGROUP_PERF\n\tstruct perf_cgroup\t\t*cgrp;  \n#endif\n\n#ifdef CONFIG_SECURITY\n\tvoid *security;\n#endif\n\tstruct list_head\t\tsb_list;\n\n\t \n\t__u32\t\t\t\torig_type;\n#endif  \n};\n\n \nstruct perf_event_pmu_context {\n\tstruct pmu\t\t\t*pmu;\n\tstruct perf_event_context       *ctx;\n\n\tstruct list_head\t\tpmu_ctx_entry;\n\n\tstruct list_head\t\tpinned_active;\n\tstruct list_head\t\tflexible_active;\n\n\t \n\tunsigned int\t\t\tembedded : 1;\n\n\tunsigned int\t\t\tnr_events;\n\tunsigned int\t\t\tnr_cgroups;\n\n\tatomic_t\t\t\trefcount;  \n\tstruct rcu_head\t\t\trcu_head;\n\n\tvoid\t\t\t\t*task_ctx_data;  \n\t \n\tint\t\t\t\trotate_necessary;\n};\n\nstruct perf_event_groups {\n\tstruct rb_root\ttree;\n\tu64\t\tindex;\n};\n\n\n \nstruct perf_event_context {\n\t \n\traw_spinlock_t\t\t\tlock;\n\t \n\tstruct mutex\t\t\tmutex;\n\n\tstruct list_head\t\tpmu_ctx_list;\n\tstruct perf_event_groups\tpinned_groups;\n\tstruct perf_event_groups\tflexible_groups;\n\tstruct list_head\t\tevent_list;\n\n\tint\t\t\t\tnr_events;\n\tint\t\t\t\tnr_user;\n\tint\t\t\t\tis_active;\n\n\tint\t\t\t\tnr_task_data;\n\tint\t\t\t\tnr_stat;\n\tint\t\t\t\tnr_freq;\n\tint\t\t\t\trotate_disable;\n\n\trefcount_t\t\t\trefcount;  \n\tstruct task_struct\t\t*task;\n\n\t \n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n\tu64\t\t\t\ttimeoffset;\n\n\t \n\tstruct perf_event_context\t*parent_ctx;\n\tu64\t\t\t\tparent_gen;\n\tu64\t\t\t\tgeneration;\n\tint\t\t\t\tpin_count;\n#ifdef CONFIG_CGROUP_PERF\n\tint\t\t\t\tnr_cgroups;\t  \n#endif\n\tstruct rcu_head\t\t\trcu_head;\n\n\t \n\tlocal_t\t\t\t\tnr_pending;\n};\n\n \n#define PERF_NR_CONTEXTS\t4\n\nstruct perf_cpu_pmu_context {\n\tstruct perf_event_pmu_context\tepc;\n\tstruct perf_event_pmu_context\t*task_epc;\n\n\tstruct list_head\t\tsched_cb_entry;\n\tint\t\t\t\tsched_cb_usage;\n\n\tint\t\t\t\tactive_oncpu;\n\tint\t\t\t\texclusive;\n\n\traw_spinlock_t\t\t\thrtimer_lock;\n\tstruct hrtimer\t\t\thrtimer;\n\tktime_t\t\t\t\thrtimer_interval;\n\tunsigned int\t\t\thrtimer_active;\n};\n\n \nstruct perf_cpu_context {\n\tstruct perf_event_context\tctx;\n\tstruct perf_event_context\t*task_ctx;\n\tint\t\t\t\tonline;\n\n#ifdef CONFIG_CGROUP_PERF\n\tstruct perf_cgroup\t\t*cgrp;\n#endif\n\n\t \n\tint\t\t\t\theap_size;\n\tstruct perf_event\t\t**heap;\n\tstruct perf_event\t\t*heap_default[2];\n};\n\nstruct perf_output_handle {\n\tstruct perf_event\t\t*event;\n\tstruct perf_buffer\t\t*rb;\n\tunsigned long\t\t\twakeup;\n\tunsigned long\t\t\tsize;\n\tu64\t\t\t\taux_flags;\n\tunion {\n\t\tvoid\t\t\t*addr;\n\t\tunsigned long\t\thead;\n\t};\n\tint\t\t\t\tpage;\n};\n\nstruct bpf_perf_event_data_kern {\n\tbpf_user_pt_regs_t *regs;\n\tstruct perf_sample_data *data;\n\tstruct perf_event *event;\n};\n\n#ifdef CONFIG_CGROUP_PERF\n\n \nstruct perf_cgroup_info {\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\ttimestamp;\n\tu64\t\t\t\ttimeoffset;\n\tint\t\t\t\tactive;\n};\n\nstruct perf_cgroup {\n\tstruct cgroup_subsys_state\tcss;\n\tstruct perf_cgroup_info\t__percpu *info;\n};\n\n \nstatic inline struct perf_cgroup *\nperf_cgroup_from_task(struct task_struct *task, struct perf_event_context *ctx)\n{\n\treturn container_of(task_css_check(task, perf_event_cgrp_id,\n\t\t\t\t\t   ctx ? lockdep_is_held(&ctx->lock)\n\t\t\t\t\t       : true),\n\t\t\t    struct perf_cgroup, css);\n}\n#endif  \n\n#ifdef CONFIG_PERF_EVENTS\n\nextern struct perf_event_context *perf_cpu_task_ctx(void);\n\nextern void *perf_aux_output_begin(struct perf_output_handle *handle,\n\t\t\t\t   struct perf_event *event);\nextern void perf_aux_output_end(struct perf_output_handle *handle,\n\t\t\t\tunsigned long size);\nextern int perf_aux_output_skip(struct perf_output_handle *handle,\n\t\t\t\tunsigned long size);\nextern void *perf_get_aux(struct perf_output_handle *handle);\nextern void perf_aux_output_flag(struct perf_output_handle *handle, u64 flags);\nextern void perf_event_itrace_started(struct perf_event *event);\n\nextern int perf_pmu_register(struct pmu *pmu, const char *name, int type);\nextern void perf_pmu_unregister(struct pmu *pmu);\n\nextern void __perf_event_task_sched_in(struct task_struct *prev,\n\t\t\t\t       struct task_struct *task);\nextern void __perf_event_task_sched_out(struct task_struct *prev,\n\t\t\t\t\tstruct task_struct *next);\nextern int perf_event_init_task(struct task_struct *child, u64 clone_flags);\nextern void perf_event_exit_task(struct task_struct *child);\nextern void perf_event_free_task(struct task_struct *task);\nextern void perf_event_delayed_put(struct task_struct *task);\nextern struct file *perf_event_get(unsigned int fd);\nextern const struct perf_event *perf_get_event(struct file *file);\nextern const struct perf_event_attr *perf_event_attrs(struct perf_event *event);\nextern void perf_event_print_debug(void);\nextern void perf_pmu_disable(struct pmu *pmu);\nextern void perf_pmu_enable(struct pmu *pmu);\nextern void perf_sched_cb_dec(struct pmu *pmu);\nextern void perf_sched_cb_inc(struct pmu *pmu);\nextern int perf_event_task_disable(void);\nextern int perf_event_task_enable(void);\n\nextern void perf_pmu_resched(struct pmu *pmu);\n\nextern int perf_event_refresh(struct perf_event *event, int refresh);\nextern void perf_event_update_userpage(struct perf_event *event);\nextern int perf_event_release_kernel(struct perf_event *event);\nextern struct perf_event *\nperf_event_create_kernel_counter(struct perf_event_attr *attr,\n\t\t\t\tint cpu,\n\t\t\t\tstruct task_struct *task,\n\t\t\t\tperf_overflow_handler_t callback,\n\t\t\t\tvoid *context);\nextern void perf_pmu_migrate_context(struct pmu *pmu,\n\t\t\t\tint src_cpu, int dst_cpu);\nint perf_event_read_local(struct perf_event *event, u64 *value,\n\t\t\t  u64 *enabled, u64 *running);\nextern u64 perf_event_read_value(struct perf_event *event,\n\t\t\t\t u64 *enabled, u64 *running);\n\nextern struct perf_callchain_entry *perf_callchain(struct perf_event *event, struct pt_regs *regs);\n\nstatic inline bool branch_sample_no_flags(const struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_NO_FLAGS;\n}\n\nstatic inline bool branch_sample_no_cycles(const struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_NO_CYCLES;\n}\n\nstatic inline bool branch_sample_type(const struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_TYPE_SAVE;\n}\n\nstatic inline bool branch_sample_hw_index(const struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_HW_INDEX;\n}\n\nstatic inline bool branch_sample_priv(const struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_PRIV_SAVE;\n}\n\n\nstruct perf_sample_data {\n\t \n\tu64\t\t\t\tsample_flags;\n\tu64\t\t\t\tperiod;\n\tu64\t\t\t\tdyn_size;\n\n\t \n\tu64\t\t\t\ttype;\n\tstruct {\n\t\tu32\tpid;\n\t\tu32\ttid;\n\t}\t\t\t\ttid_entry;\n\tu64\t\t\t\ttime;\n\tu64\t\t\t\tid;\n\tstruct {\n\t\tu32\tcpu;\n\t\tu32\treserved;\n\t}\t\t\t\tcpu_entry;\n\n\t \n\tu64\t\t\t\tip;\n\tstruct perf_callchain_entry\t*callchain;\n\tstruct perf_raw_record\t\t*raw;\n\tstruct perf_branch_stack\t*br_stack;\n\tunion perf_sample_weight\tweight;\n\tunion  perf_mem_data_src\tdata_src;\n\tu64\t\t\t\ttxn;\n\n\tstruct perf_regs\t\tregs_user;\n\tstruct perf_regs\t\tregs_intr;\n\tu64\t\t\t\tstack_user_size;\n\n\tu64\t\t\t\tstream_id;\n\tu64\t\t\t\tcgroup;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tphys_addr;\n\tu64\t\t\t\tdata_page_size;\n\tu64\t\t\t\tcode_page_size;\n\tu64\t\t\t\taux_size;\n} ____cacheline_aligned;\n\n \n#define PERF_MEM_NA (PERF_MEM_S(OP, NA)   |\\\n\t\t    PERF_MEM_S(LVL, NA)   |\\\n\t\t    PERF_MEM_S(SNOOP, NA) |\\\n\t\t    PERF_MEM_S(LOCK, NA)  |\\\n\t\t    PERF_MEM_S(TLB, NA)   |\\\n\t\t    PERF_MEM_S(LVLNUM, NA))\n\nstatic inline void perf_sample_data_init(struct perf_sample_data *data,\n\t\t\t\t\t u64 addr, u64 period)\n{\n\t \n\tdata->sample_flags = PERF_SAMPLE_PERIOD;\n\tdata->period = period;\n\tdata->dyn_size = 0;\n\n\tif (addr) {\n\t\tdata->addr = addr;\n\t\tdata->sample_flags |= PERF_SAMPLE_ADDR;\n\t}\n}\n\nstatic inline void perf_sample_save_callchain(struct perf_sample_data *data,\n\t\t\t\t\t      struct perf_event *event,\n\t\t\t\t\t      struct pt_regs *regs)\n{\n\tint size = 1;\n\n\tdata->callchain = perf_callchain(event, regs);\n\tsize += data->callchain->nr;\n\n\tdata->dyn_size += size * sizeof(u64);\n\tdata->sample_flags |= PERF_SAMPLE_CALLCHAIN;\n}\n\nstatic inline void perf_sample_save_raw_data(struct perf_sample_data *data,\n\t\t\t\t\t     struct perf_raw_record *raw)\n{\n\tstruct perf_raw_frag *frag = &raw->frag;\n\tu32 sum = 0;\n\tint size;\n\n\tdo {\n\t\tsum += frag->size;\n\t\tif (perf_raw_frag_last(frag))\n\t\t\tbreak;\n\t\tfrag = frag->next;\n\t} while (1);\n\n\tsize = round_up(sum + sizeof(u32), sizeof(u64));\n\traw->size = size - sizeof(u32);\n\tfrag->pad = raw->size - sum;\n\n\tdata->raw = raw;\n\tdata->dyn_size += size;\n\tdata->sample_flags |= PERF_SAMPLE_RAW;\n}\n\nstatic inline void perf_sample_save_brstack(struct perf_sample_data *data,\n\t\t\t\t\t    struct perf_event *event,\n\t\t\t\t\t    struct perf_branch_stack *brs)\n{\n\tint size = sizeof(u64);  \n\n\tif (branch_sample_hw_index(event))\n\t\tsize += sizeof(u64);\n\tsize += brs->nr * sizeof(struct perf_branch_entry);\n\n\tdata->br_stack = brs;\n\tdata->dyn_size += size;\n\tdata->sample_flags |= PERF_SAMPLE_BRANCH_STACK;\n}\n\nstatic inline u32 perf_sample_data_size(struct perf_sample_data *data,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tu32 size = sizeof(struct perf_event_header);\n\n\tsize += event->header_size + event->id_header_size;\n\tsize += data->dyn_size;\n\n\treturn size;\n}\n\n \nstatic inline void perf_clear_branch_entry_bitfields(struct perf_branch_entry *br)\n{\n\tbr->mispred = 0;\n\tbr->predicted = 0;\n\tbr->in_tx = 0;\n\tbr->abort = 0;\n\tbr->cycles = 0;\n\tbr->type = 0;\n\tbr->spec = PERF_BR_SPEC_NA;\n\tbr->reserved = 0;\n}\n\nextern void perf_output_sample(struct perf_output_handle *handle,\n\t\t\t       struct perf_event_header *header,\n\t\t\t       struct perf_sample_data *data,\n\t\t\t       struct perf_event *event);\nextern void perf_prepare_sample(struct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event,\n\t\t\t\tstruct pt_regs *regs);\nextern void perf_prepare_header(struct perf_event_header *header,\n\t\t\t\tstruct perf_sample_data *data,\n\t\t\t\tstruct perf_event *event,\n\t\t\t\tstruct pt_regs *regs);\n\nextern int perf_event_overflow(struct perf_event *event,\n\t\t\t\t struct perf_sample_data *data,\n\t\t\t\t struct pt_regs *regs);\n\nextern void perf_event_output_forward(struct perf_event *event,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct pt_regs *regs);\nextern void perf_event_output_backward(struct perf_event *event,\n\t\t\t\t       struct perf_sample_data *data,\n\t\t\t\t       struct pt_regs *regs);\nextern int perf_event_output(struct perf_event *event,\n\t\t\t     struct perf_sample_data *data,\n\t\t\t     struct pt_regs *regs);\n\nstatic inline bool\n__is_default_overflow_handler(perf_overflow_handler_t overflow_handler)\n{\n\tif (likely(overflow_handler == perf_event_output_forward))\n\t\treturn true;\n\tif (unlikely(overflow_handler == perf_event_output_backward))\n\t\treturn true;\n\treturn false;\n}\n\n#define is_default_overflow_handler(event) \\\n\t__is_default_overflow_handler((event)->overflow_handler)\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic inline bool uses_default_overflow_handler(struct perf_event *event)\n{\n\tif (likely(is_default_overflow_handler(event)))\n\t\treturn true;\n\n\treturn __is_default_overflow_handler(event->orig_overflow_handler);\n}\n#else\n#define uses_default_overflow_handler(event) \\\n\tis_default_overflow_handler(event)\n#endif\n\nextern void\nperf_event_header__init_id(struct perf_event_header *header,\n\t\t\t   struct perf_sample_data *data,\n\t\t\t   struct perf_event *event);\nextern void\nperf_event__output_id_sample(struct perf_event *event,\n\t\t\t     struct perf_output_handle *handle,\n\t\t\t     struct perf_sample_data *sample);\n\nextern void\nperf_log_lost_samples(struct perf_event *event, u64 lost);\n\nstatic inline bool event_has_any_exclude_flag(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\n\treturn attr->exclude_idle || attr->exclude_user ||\n\t       attr->exclude_kernel || attr->exclude_hv ||\n\t       attr->exclude_guest || attr->exclude_host;\n}\n\nstatic inline bool is_sampling_event(struct perf_event *event)\n{\n\treturn event->attr.sample_period != 0;\n}\n\n \nstatic inline int is_software_event(struct perf_event *event)\n{\n\treturn event->event_caps & PERF_EV_CAP_SOFTWARE;\n}\n\n \nstatic inline int in_software_context(struct perf_event *event)\n{\n\treturn event->pmu_ctx->pmu->task_ctx_nr == perf_sw_context;\n}\n\nstatic inline int is_exclusive_pmu(struct pmu *pmu)\n{\n\treturn pmu->capabilities & PERF_PMU_CAP_EXCLUSIVE;\n}\n\nextern struct static_key perf_swevent_enabled[PERF_COUNT_SW_MAX];\n\nextern void ___perf_sw_event(u32, u64, struct pt_regs *, u64);\nextern void __perf_sw_event(u32, u64, struct pt_regs *, u64);\n\n#ifndef perf_arch_fetch_caller_regs\nstatic inline void perf_arch_fetch_caller_regs(struct pt_regs *regs, unsigned long ip) { }\n#endif\n\n \nstatic inline void perf_fetch_caller_regs(struct pt_regs *regs)\n{\n\tperf_arch_fetch_caller_regs(regs, CALLER_ADDR0);\n}\n\nstatic __always_inline void\nperf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\n{\n\tif (static_key_false(&perf_swevent_enabled[event_id]))\n\t\t__perf_sw_event(event_id, nr, regs, addr);\n}\n\nDECLARE_PER_CPU(struct pt_regs, __perf_regs[4]);\n\n \nstatic __always_inline void __perf_sw_event_sched(u32 event_id, u64 nr, u64 addr)\n{\n\tstruct pt_regs *regs = this_cpu_ptr(&__perf_regs[0]);\n\n\tperf_fetch_caller_regs(regs);\n\t___perf_sw_event(event_id, nr, regs, addr);\n}\n\nextern struct static_key_false perf_sched_events;\n\nstatic __always_inline bool __perf_sw_enabled(int swevt)\n{\n\treturn static_key_false(&perf_swevent_enabled[swevt]);\n}\n\nstatic inline void perf_event_task_migrate(struct task_struct *task)\n{\n\tif (__perf_sw_enabled(PERF_COUNT_SW_CPU_MIGRATIONS))\n\t\ttask->sched_migrated = 1;\n}\n\nstatic inline void perf_event_task_sched_in(struct task_struct *prev,\n\t\t\t\t\t    struct task_struct *task)\n{\n\tif (static_branch_unlikely(&perf_sched_events))\n\t\t__perf_event_task_sched_in(prev, task);\n\n\tif (__perf_sw_enabled(PERF_COUNT_SW_CPU_MIGRATIONS) &&\n\t    task->sched_migrated) {\n\t\t__perf_sw_event_sched(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 0);\n\t\ttask->sched_migrated = 0;\n\t}\n}\n\nstatic inline void perf_event_task_sched_out(struct task_struct *prev,\n\t\t\t\t\t     struct task_struct *next)\n{\n\tif (__perf_sw_enabled(PERF_COUNT_SW_CONTEXT_SWITCHES))\n\t\t__perf_sw_event_sched(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 0);\n\n#ifdef CONFIG_CGROUP_PERF\n\tif (__perf_sw_enabled(PERF_COUNT_SW_CGROUP_SWITCHES) &&\n\t    perf_cgroup_from_task(prev, NULL) !=\n\t    perf_cgroup_from_task(next, NULL))\n\t\t__perf_sw_event_sched(PERF_COUNT_SW_CGROUP_SWITCHES, 1, 0);\n#endif\n\n\tif (static_branch_unlikely(&perf_sched_events))\n\t\t__perf_event_task_sched_out(prev, next);\n}\n\nextern void perf_event_mmap(struct vm_area_struct *vma);\n\nextern void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,\n\t\t\t       bool unregister, const char *sym);\nextern void perf_event_bpf_event(struct bpf_prog *prog,\n\t\t\t\t enum perf_bpf_event_type type,\n\t\t\t\t u16 flags);\n\n#ifdef CONFIG_GUEST_PERF_EVENTS\nextern struct perf_guest_info_callbacks __rcu *perf_guest_cbs;\n\nDECLARE_STATIC_CALL(__perf_guest_state, *perf_guest_cbs->state);\nDECLARE_STATIC_CALL(__perf_guest_get_ip, *perf_guest_cbs->get_ip);\nDECLARE_STATIC_CALL(__perf_guest_handle_intel_pt_intr, *perf_guest_cbs->handle_intel_pt_intr);\n\nstatic inline unsigned int perf_guest_state(void)\n{\n\treturn static_call(__perf_guest_state)();\n}\nstatic inline unsigned long perf_guest_get_ip(void)\n{\n\treturn static_call(__perf_guest_get_ip)();\n}\nstatic inline unsigned int perf_guest_handle_intel_pt_intr(void)\n{\n\treturn static_call(__perf_guest_handle_intel_pt_intr)();\n}\nextern void perf_register_guest_info_callbacks(struct perf_guest_info_callbacks *cbs);\nextern void perf_unregister_guest_info_callbacks(struct perf_guest_info_callbacks *cbs);\n#else\nstatic inline unsigned int perf_guest_state(void)\t\t { return 0; }\nstatic inline unsigned long perf_guest_get_ip(void)\t\t { return 0; }\nstatic inline unsigned int perf_guest_handle_intel_pt_intr(void) { return 0; }\n#endif  \n\nextern void perf_event_exec(void);\nextern void perf_event_comm(struct task_struct *tsk, bool exec);\nextern void perf_event_namespaces(struct task_struct *tsk);\nextern void perf_event_fork(struct task_struct *tsk);\nextern void perf_event_text_poke(const void *addr,\n\t\t\t\t const void *old_bytes, size_t old_len,\n\t\t\t\t const void *new_bytes, size_t new_len);\n\n \nDECLARE_PER_CPU(struct perf_callchain_entry, perf_callchain_entry);\n\nextern void perf_callchain_user(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs);\nextern void perf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs);\nextern struct perf_callchain_entry *\nget_perf_callchain(struct pt_regs *regs, u32 init_nr, bool kernel, bool user,\n\t\t   u32 max_stack, bool crosstask, bool add_mark);\nextern int get_callchain_buffers(int max_stack);\nextern void put_callchain_buffers(void);\nextern struct perf_callchain_entry *get_callchain_entry(int *rctx);\nextern void put_callchain_entry(int rctx);\n\nextern int sysctl_perf_event_max_stack;\nextern int sysctl_perf_event_max_contexts_per_stack;\n\nstatic inline int perf_callchain_store_context(struct perf_callchain_entry_ctx *ctx, u64 ip)\n{\n\tif (ctx->contexts < sysctl_perf_event_max_contexts_per_stack) {\n\t\tstruct perf_callchain_entry *entry = ctx->entry;\n\t\tentry->ip[entry->nr++] = ip;\n\t\t++ctx->contexts;\n\t\treturn 0;\n\t} else {\n\t\tctx->contexts_maxed = true;\n\t\treturn -1;  \n\t}\n}\n\nstatic inline int perf_callchain_store(struct perf_callchain_entry_ctx *ctx, u64 ip)\n{\n\tif (ctx->nr < ctx->max_stack && !ctx->contexts_maxed) {\n\t\tstruct perf_callchain_entry *entry = ctx->entry;\n\t\tentry->ip[entry->nr++] = ip;\n\t\t++ctx->nr;\n\t\treturn 0;\n\t} else {\n\t\treturn -1;  \n\t}\n}\n\nextern int sysctl_perf_event_paranoid;\nextern int sysctl_perf_event_mlock;\nextern int sysctl_perf_event_sample_rate;\nextern int sysctl_perf_cpu_time_max_percent;\n\nextern void perf_sample_event_took(u64 sample_len_ns);\n\nint perf_proc_update_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nint perf_cpu_time_max_percent_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\nint perf_event_max_stack_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos);\n\n \n#define PERF_SECURITY_OPEN\t\t0\n\n \n#define PERF_SECURITY_CPU\t\t1\n#define PERF_SECURITY_KERNEL\t\t2\n#define PERF_SECURITY_TRACEPOINT\t3\n\nstatic inline int perf_is_paranoid(void)\n{\n\treturn sysctl_perf_event_paranoid > -1;\n}\n\nstatic inline int perf_allow_kernel(struct perf_event_attr *attr)\n{\n\tif (sysctl_perf_event_paranoid > 1 && !perfmon_capable())\n\t\treturn -EACCES;\n\n\treturn security_perf_event_open(attr, PERF_SECURITY_KERNEL);\n}\n\nstatic inline int perf_allow_cpu(struct perf_event_attr *attr)\n{\n\tif (sysctl_perf_event_paranoid > 0 && !perfmon_capable())\n\t\treturn -EACCES;\n\n\treturn security_perf_event_open(attr, PERF_SECURITY_CPU);\n}\n\nstatic inline int perf_allow_tracepoint(struct perf_event_attr *attr)\n{\n\tif (sysctl_perf_event_paranoid > -1 && !perfmon_capable())\n\t\treturn -EPERM;\n\n\treturn security_perf_event_open(attr, PERF_SECURITY_TRACEPOINT);\n}\n\nextern void perf_event_init(void);\nextern void perf_tp_event(u16 event_type, u64 count, void *record,\n\t\t\t  int entry_size, struct pt_regs *regs,\n\t\t\t  struct hlist_head *head, int rctx,\n\t\t\t  struct task_struct *task);\nextern void perf_bp_event(struct perf_event *event, void *data);\n\n#ifndef perf_misc_flags\n# define perf_misc_flags(regs) \\\n\t\t(user_mode(regs) ? PERF_RECORD_MISC_USER : PERF_RECORD_MISC_KERNEL)\n# define perf_instruction_pointer(regs)\tinstruction_pointer(regs)\n#endif\n#ifndef perf_arch_bpf_user_pt_regs\n# define perf_arch_bpf_user_pt_regs(regs) regs\n#endif\n\nstatic inline bool has_branch_stack(struct perf_event *event)\n{\n\treturn event->attr.sample_type & PERF_SAMPLE_BRANCH_STACK;\n}\n\nstatic inline bool needs_branch_stack(struct perf_event *event)\n{\n\treturn event->attr.branch_sample_type != 0;\n}\n\nstatic inline bool has_aux(struct perf_event *event)\n{\n\treturn event->pmu->setup_aux;\n}\n\nstatic inline bool is_write_backward(struct perf_event *event)\n{\n\treturn !!event->attr.write_backward;\n}\n\nstatic inline bool has_addr_filter(struct perf_event *event)\n{\n\treturn event->pmu->nr_addr_filters;\n}\n\n \nstatic inline struct perf_addr_filters_head *\nperf_event_addr_filters(struct perf_event *event)\n{\n\tstruct perf_addr_filters_head *ifh = &event->addr_filters;\n\n\tif (event->parent)\n\t\tifh = &event->parent->addr_filters;\n\n\treturn ifh;\n}\n\nextern void perf_event_addr_filters_sync(struct perf_event *event);\nextern void perf_report_aux_output_id(struct perf_event *event, u64 hw_id);\n\nextern int perf_output_begin(struct perf_output_handle *handle,\n\t\t\t     struct perf_sample_data *data,\n\t\t\t     struct perf_event *event, unsigned int size);\nextern int perf_output_begin_forward(struct perf_output_handle *handle,\n\t\t\t\t     struct perf_sample_data *data,\n\t\t\t\t     struct perf_event *event,\n\t\t\t\t     unsigned int size);\nextern int perf_output_begin_backward(struct perf_output_handle *handle,\n\t\t\t\t      struct perf_sample_data *data,\n\t\t\t\t      struct perf_event *event,\n\t\t\t\t      unsigned int size);\n\nextern void perf_output_end(struct perf_output_handle *handle);\nextern unsigned int perf_output_copy(struct perf_output_handle *handle,\n\t\t\t     const void *buf, unsigned int len);\nextern unsigned int perf_output_skip(struct perf_output_handle *handle,\n\t\t\t\t     unsigned int len);\nextern long perf_output_copy_aux(struct perf_output_handle *aux_handle,\n\t\t\t\t struct perf_output_handle *handle,\n\t\t\t\t unsigned long from, unsigned long to);\nextern int perf_swevent_get_recursion_context(void);\nextern void perf_swevent_put_recursion_context(int rctx);\nextern u64 perf_swevent_set_period(struct perf_event *event);\nextern void perf_event_enable(struct perf_event *event);\nextern void perf_event_disable(struct perf_event *event);\nextern void perf_event_disable_local(struct perf_event *event);\nextern void perf_event_disable_inatomic(struct perf_event *event);\nextern void perf_event_task_tick(void);\nextern int perf_event_account_interrupt(struct perf_event *event);\nextern int perf_event_period(struct perf_event *event, u64 value);\nextern u64 perf_event_pause(struct perf_event *event, bool reset);\n#else  \nstatic inline void *\nperf_aux_output_begin(struct perf_output_handle *handle,\n\t\t      struct perf_event *event)\t\t\t\t{ return NULL; }\nstatic inline void\nperf_aux_output_end(struct perf_output_handle *handle, unsigned long size)\n\t\t\t\t\t\t\t\t\t{ }\nstatic inline int\nperf_aux_output_skip(struct perf_output_handle *handle,\n\t\t     unsigned long size)\t\t\t\t{ return -EINVAL; }\nstatic inline void *\nperf_get_aux(struct perf_output_handle *handle)\t\t\t\t{ return NULL; }\nstatic inline void\nperf_event_task_migrate(struct task_struct *task)\t\t\t{ }\nstatic inline void\nperf_event_task_sched_in(struct task_struct *prev,\n\t\t\t struct task_struct *task)\t\t\t{ }\nstatic inline void\nperf_event_task_sched_out(struct task_struct *prev,\n\t\t\t  struct task_struct *next)\t\t\t{ }\nstatic inline int perf_event_init_task(struct task_struct *child,\n\t\t\t\t       u64 clone_flags)\t\t\t{ return 0; }\nstatic inline void perf_event_exit_task(struct task_struct *child)\t{ }\nstatic inline void perf_event_free_task(struct task_struct *task)\t{ }\nstatic inline void perf_event_delayed_put(struct task_struct *task)\t{ }\nstatic inline struct file *perf_event_get(unsigned int fd)\t{ return ERR_PTR(-EINVAL); }\nstatic inline const struct perf_event *perf_get_event(struct file *file)\n{\n\treturn ERR_PTR(-EINVAL);\n}\nstatic inline const struct perf_event_attr *perf_event_attrs(struct perf_event *event)\n{\n\treturn ERR_PTR(-EINVAL);\n}\nstatic inline int perf_event_read_local(struct perf_event *event, u64 *value,\n\t\t\t\t\tu64 *enabled, u64 *running)\n{\n\treturn -EINVAL;\n}\nstatic inline void perf_event_print_debug(void)\t\t\t\t{ }\nstatic inline int perf_event_task_disable(void)\t\t\t\t{ return -EINVAL; }\nstatic inline int perf_event_task_enable(void)\t\t\t\t{ return -EINVAL; }\nstatic inline int perf_event_refresh(struct perf_event *event, int refresh)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void\nperf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)\t{ }\nstatic inline void\nperf_bp_event(struct perf_event *event, void *data)\t\t\t{ }\n\nstatic inline void perf_event_mmap(struct vm_area_struct *vma)\t\t{ }\n\ntypedef int (perf_ksymbol_get_name_f)(char *name, int name_len, void *data);\nstatic inline void perf_event_ksymbol(u16 ksym_type, u64 addr, u32 len,\n\t\t\t\t      bool unregister, const char *sym)\t{ }\nstatic inline void perf_event_bpf_event(struct bpf_prog *prog,\n\t\t\t\t\tenum perf_bpf_event_type type,\n\t\t\t\t\tu16 flags)\t\t\t{ }\nstatic inline void perf_event_exec(void)\t\t\t\t{ }\nstatic inline void perf_event_comm(struct task_struct *tsk, bool exec)\t{ }\nstatic inline void perf_event_namespaces(struct task_struct *tsk)\t{ }\nstatic inline void perf_event_fork(struct task_struct *tsk)\t\t{ }\nstatic inline void perf_event_text_poke(const void *addr,\n\t\t\t\t\tconst void *old_bytes,\n\t\t\t\t\tsize_t old_len,\n\t\t\t\t\tconst void *new_bytes,\n\t\t\t\t\tsize_t new_len)\t\t\t{ }\nstatic inline void perf_event_init(void)\t\t\t\t{ }\nstatic inline int  perf_swevent_get_recursion_context(void)\t\t{ return -1; }\nstatic inline void perf_swevent_put_recursion_context(int rctx)\t\t{ }\nstatic inline u64 perf_swevent_set_period(struct perf_event *event)\t{ return 0; }\nstatic inline void perf_event_enable(struct perf_event *event)\t\t{ }\nstatic inline void perf_event_disable(struct perf_event *event)\t\t{ }\nstatic inline int __perf_event_disable(void *info)\t\t\t{ return -1; }\nstatic inline void perf_event_task_tick(void)\t\t\t\t{ }\nstatic inline int perf_event_release_kernel(struct perf_event *event)\t{ return 0; }\nstatic inline int perf_event_period(struct perf_event *event, u64 value)\n{\n\treturn -EINVAL;\n}\nstatic inline u64 perf_event_pause(struct perf_event *event, bool reset)\n{\n\treturn 0;\n}\n#endif\n\n#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)\nextern void perf_restore_debug_store(void);\n#else\nstatic inline void perf_restore_debug_store(void)\t\t\t{ }\n#endif\n\n#define perf_output_put(handle, x) perf_output_copy((handle), &(x), sizeof(x))\n\nstruct perf_pmu_events_attr {\n\tstruct device_attribute attr;\n\tu64 id;\n\tconst char *event_str;\n};\n\nstruct perf_pmu_events_ht_attr {\n\tstruct device_attribute\t\t\tattr;\n\tu64\t\t\t\t\tid;\n\tconst char\t\t\t\t*event_str_ht;\n\tconst char\t\t\t\t*event_str_noht;\n};\n\nstruct perf_pmu_events_hybrid_attr {\n\tstruct device_attribute\t\t\tattr;\n\tu64\t\t\t\t\tid;\n\tconst char\t\t\t\t*event_str;\n\tu64\t\t\t\t\tpmu_type;\n};\n\nstruct perf_pmu_format_hybrid_attr {\n\tstruct device_attribute\t\t\tattr;\n\tu64\t\t\t\t\tpmu_type;\n};\n\nssize_t perf_event_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *page);\n\n#define PMU_EVENT_ATTR(_name, _var, _id, _show)\t\t\t\t\\\nstatic struct perf_pmu_events_attr _var = {\t\t\t\t\\\n\t.attr = __ATTR(_name, 0444, _show, NULL),\t\t\t\\\n\t.id   =  _id,\t\t\t\t\t\t\t\\\n};\n\n#define PMU_EVENT_ATTR_STRING(_name, _var, _str)\t\t\t    \\\nstatic struct perf_pmu_events_attr _var = {\t\t\t\t    \\\n\t.attr\t\t= __ATTR(_name, 0444, perf_event_sysfs_show, NULL), \\\n\t.id\t\t= 0,\t\t\t\t\t\t    \\\n\t.event_str\t= _str,\t\t\t\t\t\t    \\\n};\n\n#define PMU_EVENT_ATTR_ID(_name, _show, _id)\t\t\t\t\\\n\t(&((struct perf_pmu_events_attr[]) {\t\t\t\t\\\n\t\t{ .attr = __ATTR(_name, 0444, _show, NULL),\t\t\\\n\t\t  .id = _id, }\t\t\t\t\t\t\\\n\t})[0].attr.attr)\n\n#define PMU_FORMAT_ATTR_SHOW(_name, _format)\t\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n_name##_show(struct device *dev,\t\t\t\t\t\\\n\t\t\t       struct device_attribute *attr,\t\t\\\n\t\t\t       char *page)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);\t\t\t\\\n\treturn sprintf(page, _format \"\\n\");\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\n#define PMU_FORMAT_ATTR(_name, _format)\t\t\t\t\t\\\n\tPMU_FORMAT_ATTR_SHOW(_name, _format)\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute format_attr_##_name = __ATTR_RO(_name)\n\n \n#ifdef CONFIG_PERF_EVENTS\nint perf_event_init_cpu(unsigned int cpu);\nint perf_event_exit_cpu(unsigned int cpu);\n#else\n#define perf_event_init_cpu\tNULL\n#define perf_event_exit_cpu\tNULL\n#endif\n\nextern void arch_perf_update_userpage(struct perf_event *event,\n\t\t\t\t      struct perf_event_mmap_page *userpg,\n\t\t\t\t      u64 now);\n\n \n\n \ntypedef int (perf_snapshot_branch_stack_t)(struct perf_branch_entry *entries,\n\t\t\t\t\t   unsigned int cnt);\nDECLARE_STATIC_CALL(perf_snapshot_branch_stack, perf_snapshot_branch_stack_t);\n\n#ifndef PERF_NEEDS_LOPWR_CB\nstatic inline void perf_lopwr_cb(bool mode)\n{\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}