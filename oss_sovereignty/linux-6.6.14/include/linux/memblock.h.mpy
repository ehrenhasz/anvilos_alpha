{
  "module_name": "memblock.h",
  "hash_id": "8772744e1cd73da887eafd82112bafccb126b3754fe3965a590a271f2132d8b5",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/memblock.h",
  "human_readable_source": " \n#ifndef _LINUX_MEMBLOCK_H\n#define _LINUX_MEMBLOCK_H\n\n \n\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <asm/dma.h>\n\nextern unsigned long max_low_pfn;\nextern unsigned long min_low_pfn;\n\n \nextern unsigned long max_pfn;\n \nextern unsigned long long max_possible_pfn;\n\n \nenum memblock_flags {\n\tMEMBLOCK_NONE\t\t= 0x0,\t \n\tMEMBLOCK_HOTPLUG\t= 0x1,\t \n\tMEMBLOCK_MIRROR\t\t= 0x2,\t \n\tMEMBLOCK_NOMAP\t\t= 0x4,\t \n\tMEMBLOCK_DRIVER_MANAGED = 0x8,\t \n};\n\n \nstruct memblock_region {\n\tphys_addr_t base;\n\tphys_addr_t size;\n\tenum memblock_flags flags;\n#ifdef CONFIG_NUMA\n\tint nid;\n#endif\n};\n\n \nstruct memblock_type {\n\tunsigned long cnt;\n\tunsigned long max;\n\tphys_addr_t total_size;\n\tstruct memblock_region *regions;\n\tchar *name;\n};\n\n \nstruct memblock {\n\tbool bottom_up;   \n\tphys_addr_t current_limit;\n\tstruct memblock_type memory;\n\tstruct memblock_type reserved;\n};\n\nextern struct memblock memblock;\n\n#ifndef CONFIG_ARCH_KEEP_MEMBLOCK\n#define __init_memblock __meminit\n#define __initdata_memblock __meminitdata\nvoid memblock_discard(void);\n#else\n#define __init_memblock\n#define __initdata_memblock\nstatic inline void memblock_discard(void) {}\n#endif\n\nvoid memblock_allow_resize(void);\nint memblock_add_node(phys_addr_t base, phys_addr_t size, int nid,\n\t\t      enum memblock_flags flags);\nint memblock_add(phys_addr_t base, phys_addr_t size);\nint memblock_remove(phys_addr_t base, phys_addr_t size);\nint memblock_phys_free(phys_addr_t base, phys_addr_t size);\nint memblock_reserve(phys_addr_t base, phys_addr_t size);\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\nint memblock_physmem_add(phys_addr_t base, phys_addr_t size);\n#endif\nvoid memblock_trim_memory(phys_addr_t align);\nbool memblock_overlaps_region(struct memblock_type *type,\n\t\t\t      phys_addr_t base, phys_addr_t size);\nint memblock_mark_hotplug(phys_addr_t base, phys_addr_t size);\nint memblock_clear_hotplug(phys_addr_t base, phys_addr_t size);\nint memblock_mark_mirror(phys_addr_t base, phys_addr_t size);\nint memblock_mark_nomap(phys_addr_t base, phys_addr_t size);\nint memblock_clear_nomap(phys_addr_t base, phys_addr_t size);\n\nvoid memblock_free_all(void);\nvoid memblock_free(void *ptr, size_t size);\nvoid reset_all_zones_managed_pages(void);\n\n \nvoid __next_mem_range(u64 *idx, int nid, enum memblock_flags flags,\n\t\t      struct memblock_type *type_a,\n\t\t      struct memblock_type *type_b, phys_addr_t *out_start,\n\t\t      phys_addr_t *out_end, int *out_nid);\n\nvoid __next_mem_range_rev(u64 *idx, int nid, enum memblock_flags flags,\n\t\t\t  struct memblock_type *type_a,\n\t\t\t  struct memblock_type *type_b, phys_addr_t *out_start,\n\t\t\t  phys_addr_t *out_end, int *out_nid);\n\nvoid memblock_free_late(phys_addr_t base, phys_addr_t size);\n\n#ifdef CONFIG_HAVE_MEMBLOCK_PHYS_MAP\nstatic inline void __next_physmem_range(u64 *idx, struct memblock_type *type,\n\t\t\t\t\tphys_addr_t *out_start,\n\t\t\t\t\tphys_addr_t *out_end)\n{\n\textern struct memblock_type physmem;\n\n\t__next_mem_range(idx, NUMA_NO_NODE, MEMBLOCK_NONE, &physmem, type,\n\t\t\t out_start, out_end, NULL);\n}\n\n \n#define for_each_physmem_range(i, type, p_start, p_end)\t\t\t\\\n\tfor (i = 0, __next_physmem_range(&i, type, p_start, p_end);\t\\\n\t     i != (u64)ULLONG_MAX;\t\t\t\t\t\\\n\t     __next_physmem_range(&i, type, p_start, p_end))\n#endif  \n\n \n#define __for_each_mem_range(i, type_a, type_b, nid, flags,\t\t\\\n\t\t\t   p_start, p_end, p_nid)\t\t\t\\\n\tfor (i = 0, __next_mem_range(&i, nid, flags, type_a, type_b,\t\\\n\t\t\t\t     p_start, p_end, p_nid);\t\t\\\n\t     i != (u64)ULLONG_MAX;\t\t\t\t\t\\\n\t     __next_mem_range(&i, nid, flags, type_a, type_b,\t\t\\\n\t\t\t      p_start, p_end, p_nid))\n\n \n#define __for_each_mem_range_rev(i, type_a, type_b, nid, flags,\t\t\\\n\t\t\t\t p_start, p_end, p_nid)\t\t\t\\\n\tfor (i = (u64)ULLONG_MAX,\t\t\t\t\t\\\n\t\t     __next_mem_range_rev(&i, nid, flags, type_a, type_b, \\\n\t\t\t\t\t  p_start, p_end, p_nid);\t\\\n\t     i != (u64)ULLONG_MAX;\t\t\t\t\t\\\n\t     __next_mem_range_rev(&i, nid, flags, type_a, type_b,\t\\\n\t\t\t\t  p_start, p_end, p_nid))\n\n \n#define for_each_mem_range(i, p_start, p_end) \\\n\t__for_each_mem_range(i, &memblock.memory, NULL, NUMA_NO_NODE,\t\\\n\t\t\t     MEMBLOCK_HOTPLUG | MEMBLOCK_DRIVER_MANAGED, \\\n\t\t\t     p_start, p_end, NULL)\n\n \n#define for_each_mem_range_rev(i, p_start, p_end)\t\t\t\\\n\t__for_each_mem_range_rev(i, &memblock.memory, NULL, NUMA_NO_NODE, \\\n\t\t\t\t MEMBLOCK_HOTPLUG | MEMBLOCK_DRIVER_MANAGED,\\\n\t\t\t\t p_start, p_end, NULL)\n\n \n#define for_each_reserved_mem_range(i, p_start, p_end)\t\t\t\\\n\t__for_each_mem_range(i, &memblock.reserved, NULL, NUMA_NO_NODE,\t\\\n\t\t\t     MEMBLOCK_NONE, p_start, p_end, NULL)\n\nstatic inline bool memblock_is_hotpluggable(struct memblock_region *m)\n{\n\treturn m->flags & MEMBLOCK_HOTPLUG;\n}\n\nstatic inline bool memblock_is_mirror(struct memblock_region *m)\n{\n\treturn m->flags & MEMBLOCK_MIRROR;\n}\n\nstatic inline bool memblock_is_nomap(struct memblock_region *m)\n{\n\treturn m->flags & MEMBLOCK_NOMAP;\n}\n\nstatic inline bool memblock_is_driver_managed(struct memblock_region *m)\n{\n\treturn m->flags & MEMBLOCK_DRIVER_MANAGED;\n}\n\nint memblock_search_pfn_nid(unsigned long pfn, unsigned long *start_pfn,\n\t\t\t    unsigned long  *end_pfn);\nvoid __next_mem_pfn_range(int *idx, int nid, unsigned long *out_start_pfn,\n\t\t\t  unsigned long *out_end_pfn, int *out_nid);\n\n \n#define for_each_mem_pfn_range(i, nid, p_start, p_end, p_nid)\t\t\\\n\tfor (i = -1, __next_mem_pfn_range(&i, nid, p_start, p_end, p_nid); \\\n\t     i >= 0; __next_mem_pfn_range(&i, nid, p_start, p_end, p_nid))\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nvoid __next_mem_pfn_range_in_zone(u64 *idx, struct zone *zone,\n\t\t\t\t  unsigned long *out_spfn,\n\t\t\t\t  unsigned long *out_epfn);\n \n#define for_each_free_mem_pfn_range_in_zone(i, zone, p_start, p_end)\t\\\n\tfor (i = 0,\t\t\t\t\t\t\t\\\n\t     __next_mem_pfn_range_in_zone(&i, zone, p_start, p_end);\t\\\n\t     i != U64_MAX;\t\t\t\t\t\\\n\t     __next_mem_pfn_range_in_zone(&i, zone, p_start, p_end))\n\n \n#define for_each_free_mem_pfn_range_in_zone_from(i, zone, p_start, p_end) \\\n\tfor (; i != U64_MAX;\t\t\t\t\t  \\\n\t     __next_mem_pfn_range_in_zone(&i, zone, p_start, p_end))\n\nint __init deferred_page_init_max_threads(const struct cpumask *node_cpumask);\n\n#endif  \n\n \n#define for_each_free_mem_range(i, nid, flags, p_start, p_end, p_nid)\t\\\n\t__for_each_mem_range(i, &memblock.memory, &memblock.reserved,\t\\\n\t\t\t     nid, flags, p_start, p_end, p_nid)\n\n \n#define for_each_free_mem_range_reverse(i, nid, flags, p_start, p_end,\t\\\n\t\t\t\t\tp_nid)\t\t\t\t\\\n\t__for_each_mem_range_rev(i, &memblock.memory, &memblock.reserved, \\\n\t\t\t\t nid, flags, p_start, p_end, p_nid)\n\nint memblock_set_node(phys_addr_t base, phys_addr_t size,\n\t\t      struct memblock_type *type, int nid);\n\n#ifdef CONFIG_NUMA\nstatic inline void memblock_set_region_node(struct memblock_region *r, int nid)\n{\n\tr->nid = nid;\n}\n\nstatic inline int memblock_get_region_node(const struct memblock_region *r)\n{\n\treturn r->nid;\n}\n#else\nstatic inline void memblock_set_region_node(struct memblock_region *r, int nid)\n{\n}\n\nstatic inline int memblock_get_region_node(const struct memblock_region *r)\n{\n\treturn 0;\n}\n#endif  \n\n \n#define MEMBLOCK_ALLOC_ANYWHERE\t(~(phys_addr_t)0)\n#define MEMBLOCK_ALLOC_ACCESSIBLE\t0\n#define MEMBLOCK_ALLOC_NOLEAKTRACE\t1\n\n \n#define MEMBLOCK_LOW_LIMIT 0\n\n#ifndef ARCH_LOW_ADDRESS_LIMIT\n#define ARCH_LOW_ADDRESS_LIMIT  0xffffffffUL\n#endif\n\nphys_addr_t memblock_phys_alloc_range(phys_addr_t size, phys_addr_t align,\n\t\t\t\t      phys_addr_t start, phys_addr_t end);\nphys_addr_t memblock_alloc_range_nid(phys_addr_t size,\n\t\t\t\t      phys_addr_t align, phys_addr_t start,\n\t\t\t\t      phys_addr_t end, int nid, bool exact_nid);\nphys_addr_t memblock_phys_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid);\n\nstatic __always_inline phys_addr_t memblock_phys_alloc(phys_addr_t size,\n\t\t\t\t\t\t       phys_addr_t align)\n{\n\treturn memblock_phys_alloc_range(size, align, 0,\n\t\t\t\t\t MEMBLOCK_ALLOC_ACCESSIBLE);\n}\n\nvoid *memblock_alloc_exact_nid_raw(phys_addr_t size, phys_addr_t align,\n\t\t\t\t phys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\t\t int nid);\nvoid *memblock_alloc_try_nid_raw(phys_addr_t size, phys_addr_t align,\n\t\t\t\t phys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\t\t int nid);\nvoid *memblock_alloc_try_nid(phys_addr_t size, phys_addr_t align,\n\t\t\t     phys_addr_t min_addr, phys_addr_t max_addr,\n\t\t\t     int nid);\n\nstatic __always_inline void *memblock_alloc(phys_addr_t size, phys_addr_t align)\n{\n\treturn memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,\n\t\t\t\t      MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE);\n}\n\nstatic inline void *memblock_alloc_raw(phys_addr_t size,\n\t\t\t\t\t       phys_addr_t align)\n{\n\treturn memblock_alloc_try_nid_raw(size, align, MEMBLOCK_LOW_LIMIT,\n\t\t\t\t\t  MEMBLOCK_ALLOC_ACCESSIBLE,\n\t\t\t\t\t  NUMA_NO_NODE);\n}\n\nstatic inline void *memblock_alloc_from(phys_addr_t size,\n\t\t\t\t\t\tphys_addr_t align,\n\t\t\t\t\t\tphys_addr_t min_addr)\n{\n\treturn memblock_alloc_try_nid(size, align, min_addr,\n\t\t\t\t      MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE);\n}\n\nstatic inline void *memblock_alloc_low(phys_addr_t size,\n\t\t\t\t\t       phys_addr_t align)\n{\n\treturn memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,\n\t\t\t\t      ARCH_LOW_ADDRESS_LIMIT, NUMA_NO_NODE);\n}\n\nstatic inline void *memblock_alloc_node(phys_addr_t size,\n\t\t\t\t\t\tphys_addr_t align, int nid)\n{\n\treturn memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,\n\t\t\t\t      MEMBLOCK_ALLOC_ACCESSIBLE, nid);\n}\n\n \nstatic inline __init_memblock void memblock_set_bottom_up(bool enable)\n{\n\tmemblock.bottom_up = enable;\n}\n\n \nstatic inline __init_memblock bool memblock_bottom_up(void)\n{\n\treturn memblock.bottom_up;\n}\n\nphys_addr_t memblock_phys_mem_size(void);\nphys_addr_t memblock_reserved_size(void);\nphys_addr_t memblock_start_of_DRAM(void);\nphys_addr_t memblock_end_of_DRAM(void);\nvoid memblock_enforce_memory_limit(phys_addr_t memory_limit);\nvoid memblock_cap_memory_range(phys_addr_t base, phys_addr_t size);\nvoid memblock_mem_limit_remove_map(phys_addr_t limit);\nbool memblock_is_memory(phys_addr_t addr);\nbool memblock_is_map_memory(phys_addr_t addr);\nbool memblock_is_region_memory(phys_addr_t base, phys_addr_t size);\nbool memblock_is_reserved(phys_addr_t addr);\nbool memblock_is_region_reserved(phys_addr_t base, phys_addr_t size);\n\nvoid memblock_dump_all(void);\n\n \nvoid memblock_set_current_limit(phys_addr_t limit);\n\n\nphys_addr_t memblock_get_current_limit(void);\n\n \n\n \nstatic inline unsigned long memblock_region_memory_base_pfn(const struct memblock_region *reg)\n{\n\treturn PFN_UP(reg->base);\n}\n\n \nstatic inline unsigned long memblock_region_memory_end_pfn(const struct memblock_region *reg)\n{\n\treturn PFN_DOWN(reg->base + reg->size);\n}\n\n \nstatic inline unsigned long memblock_region_reserved_base_pfn(const struct memblock_region *reg)\n{\n\treturn PFN_DOWN(reg->base);\n}\n\n \nstatic inline unsigned long memblock_region_reserved_end_pfn(const struct memblock_region *reg)\n{\n\treturn PFN_UP(reg->base + reg->size);\n}\n\n \n#define for_each_mem_region(region)\t\t\t\t\t\\\n\tfor (region = memblock.memory.regions;\t\t\t\t\\\n\t     region < (memblock.memory.regions + memblock.memory.cnt);\t\\\n\t     region++)\n\n \n#define for_each_reserved_mem_region(region)\t\t\t\t\\\n\tfor (region = memblock.reserved.regions;\t\t\t\\\n\t     region < (memblock.reserved.regions + memblock.reserved.cnt); \\\n\t     region++)\n\nextern void *alloc_large_system_hash(const char *tablename,\n\t\t\t\t     unsigned long bucketsize,\n\t\t\t\t     unsigned long numentries,\n\t\t\t\t     int scale,\n\t\t\t\t     int flags,\n\t\t\t\t     unsigned int *_hash_shift,\n\t\t\t\t     unsigned int *_hash_mask,\n\t\t\t\t     unsigned long low_limit,\n\t\t\t\t     unsigned long high_limit);\n\n#define HASH_EARLY\t0x00000001\t \n#define HASH_ZERO\t0x00000002\t \n\n \n#ifdef CONFIG_NUMA\n#define HASHDIST_DEFAULT IS_ENABLED(CONFIG_64BIT)\nextern int hashdist;\t\t \n#else\n#define hashdist (0)\n#endif\n\n#ifdef CONFIG_MEMTEST\nvoid early_memtest(phys_addr_t start, phys_addr_t end);\nvoid memtest_report_meminfo(struct seq_file *m);\n#else\nstatic inline void early_memtest(phys_addr_t start, phys_addr_t end) { }\nstatic inline void memtest_report_meminfo(struct seq_file *m) { }\n#endif\n\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}