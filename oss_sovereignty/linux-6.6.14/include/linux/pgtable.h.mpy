{
  "module_name": "pgtable.h",
  "hash_id": "eb2a33612fbc5a39bcec5c9089067526dad2788f8cc31c177f6ce526caf76ce7",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/pgtable.h",
  "human_readable_source": " \n#ifndef _LINUX_PGTABLE_H\n#define _LINUX_PGTABLE_H\n\n#include <linux/pfn.h>\n#include <asm/pgtable.h>\n\n#define PMD_ORDER\t(PMD_SHIFT - PAGE_SHIFT)\n#define PUD_ORDER\t(PUD_SHIFT - PAGE_SHIFT)\n\n#ifndef __ASSEMBLY__\n#ifdef CONFIG_MMU\n\n#include <linux/mm_types.h>\n#include <linux/bug.h>\n#include <linux/errno.h>\n#include <asm-generic/pgtable_uffd.h>\n#include <linux/page_table_check.h>\n\n#if 5 - defined(__PAGETABLE_P4D_FOLDED) - defined(__PAGETABLE_PUD_FOLDED) - \\\n\tdefined(__PAGETABLE_PMD_FOLDED) != CONFIG_PGTABLE_LEVELS\n#error CONFIG_PGTABLE_LEVELS is not consistent with __PAGETABLE_{P4D,PUD,PMD}_FOLDED\n#endif\n\n \n#ifndef USER_PGTABLES_CEILING\n#define USER_PGTABLES_CEILING\t0UL\n#endif\n\n \n#ifndef FIRST_USER_ADDRESS\n#define FIRST_USER_ADDRESS\t0UL\n#endif\n\n \n#ifndef pmd_pgtable\n#define pmd_pgtable(pmd) pmd_page(pmd)\n#endif\n\n \n\nstatic inline unsigned long pte_index(unsigned long address)\n{\n\treturn (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);\n}\n\n#ifndef pmd_index\nstatic inline unsigned long pmd_index(unsigned long address)\n{\n\treturn (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);\n}\n#define pmd_index pmd_index\n#endif\n\n#ifndef pud_index\nstatic inline unsigned long pud_index(unsigned long address)\n{\n\treturn (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);\n}\n#define pud_index pud_index\n#endif\n\n#ifndef pgd_index\n \n#define pgd_index(a)  (((a) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))\n#endif\n\n#ifndef pte_offset_kernel\nstatic inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)\n{\n\treturn (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);\n}\n#define pte_offset_kernel pte_offset_kernel\n#endif\n\n#ifdef CONFIG_HIGHPTE\n#define __pte_map(pmd, address) \\\n\t((pte_t *)kmap_local_page(pmd_page(*(pmd))) + pte_index((address)))\n#define pte_unmap(pte)\tdo {\t\\\n\tkunmap_local((pte));\t\\\n\trcu_read_unlock();\t\\\n} while (0)\n#else\nstatic inline pte_t *__pte_map(pmd_t *pmd, unsigned long address)\n{\n\treturn pte_offset_kernel(pmd, address);\n}\nstatic inline void pte_unmap(pte_t *pte)\n{\n\trcu_read_unlock();\n}\n#endif\n\nvoid pte_free_defer(struct mm_struct *mm, pgtable_t pgtable);\n\n \n#ifndef pmd_offset\nstatic inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)\n{\n\treturn pud_pgtable(*pud) + pmd_index(address);\n}\n#define pmd_offset pmd_offset\n#endif\n\n#ifndef pud_offset\nstatic inline pud_t *pud_offset(p4d_t *p4d, unsigned long address)\n{\n\treturn p4d_pgtable(*p4d) + pud_index(address);\n}\n#define pud_offset pud_offset\n#endif\n\nstatic inline pgd_t *pgd_offset_pgd(pgd_t *pgd, unsigned long address)\n{\n\treturn (pgd + pgd_index(address));\n};\n\n \n#ifndef pgd_offset\n#define pgd_offset(mm, address)\t\tpgd_offset_pgd((mm)->pgd, (address))\n#endif\n\n \n#ifndef pgd_offset_k\n#define pgd_offset_k(address)\t\tpgd_offset(&init_mm, (address))\n#endif\n\n \nstatic inline pmd_t *pmd_off(struct mm_struct *mm, unsigned long va)\n{\n\treturn pmd_offset(pud_offset(p4d_offset(pgd_offset(mm, va), va), va), va);\n}\n\nstatic inline pmd_t *pmd_off_k(unsigned long va)\n{\n\treturn pmd_offset(pud_offset(p4d_offset(pgd_offset_k(va), va), va), va);\n}\n\nstatic inline pte_t *virt_to_kpte(unsigned long vaddr)\n{\n\tpmd_t *pmd = pmd_off_k(vaddr);\n\n\treturn pmd_none(*pmd) ? NULL : pte_offset_kernel(pmd, vaddr);\n}\n\n#ifndef pmd_young\nstatic inline int pmd_young(pmd_t pmd)\n{\n\treturn 0;\n}\n#endif\n\n \n#ifndef __HAVE_ARCH_ENTER_LAZY_MMU_MODE\n#define arch_enter_lazy_mmu_mode()\tdo {} while (0)\n#define arch_leave_lazy_mmu_mode()\tdo {} while (0)\n#define arch_flush_lazy_mmu_mode()\tdo {} while (0)\n#endif\n\n#ifndef set_ptes\n\n#ifndef pte_next_pfn\nstatic inline pte_t pte_next_pfn(pte_t pte)\n{\n\treturn __pte(pte_val(pte) + (1UL << PFN_PTE_SHIFT));\n}\n#endif\n\n \nstatic inline void set_ptes(struct mm_struct *mm, unsigned long addr,\n\t\tpte_t *ptep, pte_t pte, unsigned int nr)\n{\n\tpage_table_check_ptes_set(mm, ptep, pte, nr);\n\n\tarch_enter_lazy_mmu_mode();\n\tfor (;;) {\n\t\tset_pte(ptep, pte);\n\t\tif (--nr == 0)\n\t\t\tbreak;\n\t\tptep++;\n\t\tpte = pte_next_pfn(pte);\n\t}\n\tarch_leave_lazy_mmu_mode();\n}\n#endif\n#define set_pte_at(mm, addr, ptep, pte) set_ptes(mm, addr, ptep, pte, 1)\n\n#ifndef __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS\nextern int ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pte_t *ptep,\n\t\t\t\t pte_t entry, int dirty);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp,\n\t\t\t\t pmd_t entry, int dirty);\nextern int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pud_t *pudp,\n\t\t\t\t pud_t entry, int dirty);\n#else\nstatic inline int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address, pmd_t *pmdp,\n\t\t\t\t\tpmd_t entry, int dirty)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\nstatic inline int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address, pud_t *pudp,\n\t\t\t\t\tpud_t entry, int dirty)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif  \n#endif\n\n#ifndef ptep_get\nstatic inline pte_t ptep_get(pte_t *ptep)\n{\n\treturn READ_ONCE(*ptep);\n}\n#endif\n\n#ifndef pmdp_get\nstatic inline pmd_t pmdp_get(pmd_t *pmdp)\n{\n\treturn READ_ONCE(*pmdp);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG\nstatic inline int ptep_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pte_t *ptep)\n{\n\tpte_t pte = ptep_get(ptep);\n\tint r = 1;\n\tif (!pte_young(pte))\n\t\tr = 0;\n\telse\n\t\tset_pte_at(vma->vm_mm, address, ptep, pte_mkold(pte));\n\treturn r;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG)\nstatic inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tpmd_t pmd = *pmdp;\n\tint r = 1;\n\tif (!pmd_young(pmd))\n\t\tr = 0;\n\telse\n\t\tset_pmd_at(vma->vm_mm, address, pmdp, pmd_mkold(pmd));\n\treturn r;\n}\n#else\nstatic inline int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif  \n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH\nint ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pte_t *ptep);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pmd_t *pmdp);\n#else\n \nstatic inline int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address, pmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn 0;\n}\n#endif  \n#endif\n\n#ifndef arch_has_hw_nonleaf_pmd_young\n \nstatic inline bool arch_has_hw_nonleaf_pmd_young(void)\n{\n\treturn IS_ENABLED(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG);\n}\n#endif\n\n#ifndef arch_has_hw_pte_young\n \nstatic inline bool arch_has_hw_pte_young(void)\n{\n\treturn false;\n}\n#endif\n\n#ifndef arch_check_zapped_pte\nstatic inline void arch_check_zapped_pte(struct vm_area_struct *vma,\n\t\t\t\t\t pte_t pte)\n{\n}\n#endif\n\n#ifndef arch_check_zapped_pmd\nstatic inline void arch_check_zapped_pmd(struct vm_area_struct *vma,\n\t\t\t\t\t pmd_t pmd)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR\nstatic inline pte_t ptep_get_and_clear(struct mm_struct *mm,\n\t\t\t\t       unsigned long address,\n\t\t\t\t       pte_t *ptep)\n{\n\tpte_t pte = ptep_get(ptep);\n\tpte_clear(mm, address, ptep);\n\tpage_table_check_pte_clear(mm, pte);\n\treturn pte;\n}\n#endif\n\nstatic inline void ptep_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pte_t *ptep)\n{\n\tptep_get_and_clear(mm, addr, ptep);\n}\n\n#ifdef CONFIG_GUP_GET_PXX_LOW_HIGH\n \nstatic inline pte_t ptep_get_lockless(pte_t *ptep)\n{\n\tpte_t pte;\n\n\tdo {\n\t\tpte.pte_low = ptep->pte_low;\n\t\tsmp_rmb();\n\t\tpte.pte_high = ptep->pte_high;\n\t\tsmp_rmb();\n\t} while (unlikely(pte.pte_low != ptep->pte_low));\n\n\treturn pte;\n}\n#define ptep_get_lockless ptep_get_lockless\n\n#if CONFIG_PGTABLE_LEVELS > 2\nstatic inline pmd_t pmdp_get_lockless(pmd_t *pmdp)\n{\n\tpmd_t pmd;\n\n\tdo {\n\t\tpmd.pmd_low = pmdp->pmd_low;\n\t\tsmp_rmb();\n\t\tpmd.pmd_high = pmdp->pmd_high;\n\t\tsmp_rmb();\n\t} while (unlikely(pmd.pmd_low != pmdp->pmd_low));\n\n\treturn pmd;\n}\n#define pmdp_get_lockless pmdp_get_lockless\n#define pmdp_get_lockless_sync() tlb_remove_table_sync_one()\n#endif  \n#endif  \n\n \n#ifndef ptep_get_lockless\nstatic inline pte_t ptep_get_lockless(pte_t *ptep)\n{\n\treturn ptep_get(ptep);\n}\n#endif\n\n#ifndef pmdp_get_lockless\nstatic inline pmd_t pmdp_get_lockless(pmd_t *pmdp)\n{\n\treturn pmdp_get(pmdp);\n}\nstatic inline void pmdp_get_lockless_sync(void)\n{\n}\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR\nstatic inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pmd_t *pmdp)\n{\n\tpmd_t pmd = *pmdp;\n\n\tpmd_clear(pmdp);\n\tpage_table_check_pmd_clear(mm, pmd);\n\n\treturn pmd;\n}\n#endif  \n#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR\nstatic inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address,\n\t\t\t\t\t    pud_t *pudp)\n{\n\tpud_t pud = *pudp;\n\n\tpud_clear(pudp);\n\tpage_table_check_pud_clear(mm, pud);\n\n\treturn pud;\n}\n#endif  \n#endif  \n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#ifndef __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR_FULL\nstatic inline pmd_t pmdp_huge_get_and_clear_full(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address, pmd_t *pmdp,\n\t\t\t\t\t    int full)\n{\n\treturn pmdp_huge_get_and_clear(vma->vm_mm, address, pmdp);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR_FULL\nstatic inline pud_t pudp_huge_get_and_clear_full(struct vm_area_struct *vma,\n\t\t\t\t\t    unsigned long address, pud_t *pudp,\n\t\t\t\t\t    int full)\n{\n\treturn pudp_huge_get_and_clear(vma->vm_mm, address, pudp);\n}\n#endif\n#endif  \n\n#ifndef __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL\nstatic inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long address, pte_t *ptep,\n\t\t\t\t\t    int full)\n{\n\treturn ptep_get_and_clear(mm, address, ptep);\n}\n#endif\n\n\n \n#ifndef __HAVE_ARCH_UPDATE_MMU_TLB\nstatic inline void update_mmu_tlb(struct vm_area_struct *vma,\n\t\t\t\tunsigned long address, pte_t *ptep)\n{\n}\n#define __HAVE_ARCH_UPDATE_MMU_TLB\n#endif\n\n \n#ifndef __HAVE_ARCH_PTE_CLEAR_NOT_PRESENT_FULL\nstatic inline void pte_clear_not_present_full(struct mm_struct *mm,\n\t\t\t\t\t      unsigned long address,\n\t\t\t\t\t      pte_t *ptep,\n\t\t\t\t\t      int full)\n{\n\tpte_clear(mm, address, ptep);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_CLEAR_FLUSH\nextern pte_t ptep_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pte_t *ptep);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_HUGE_CLEAR_FLUSH\nextern pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pmd_t *pmdp);\nextern pud_t pudp_huge_clear_flush(struct vm_area_struct *vma,\n\t\t\t      unsigned long address,\n\t\t\t      pud_t *pudp);\n#endif\n\n#ifndef pte_mkwrite\nstatic inline pte_t pte_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\treturn pte_mkwrite_novma(pte);\n}\n#endif\n\n#if defined(CONFIG_ARCH_WANT_PMD_MKWRITE) && !defined(pmd_mkwrite)\nstatic inline pmd_t pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)\n{\n\treturn pmd_mkwrite_novma(pmd);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTEP_SET_WRPROTECT\nstruct mm_struct;\nstatic inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long address, pte_t *ptep)\n{\n\tpte_t old_pte = ptep_get(ptep);\n\tset_pte_at(mm, address, ptep, pte_wrprotect(old_pte));\n}\n#endif\n\n \n#ifndef pte_sw_mkyoung\nstatic inline pte_t pte_sw_mkyoung(pte_t pte)\n{\n\treturn pte;\n}\n#define pte_sw_mkyoung\tpte_sw_mkyoung\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_SET_WRPROTECT\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pmd_t *pmdp)\n{\n\tpmd_t old_pmd = *pmdp;\n\tset_pmd_at(mm, address, pmdp, pmd_wrprotect(old_pmd));\n}\n#else\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pmd_t *pmdp)\n{\n\tBUILD_BUG();\n}\n#endif  \n#endif\n#ifndef __HAVE_ARCH_PUDP_SET_WRPROTECT\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline void pudp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pud_t *pudp)\n{\n\tpud_t old_pud = *pudp;\n\n\tset_pud_at(mm, address, pudp, pud_wrprotect(old_pud));\n}\n#else\nstatic inline void pudp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long address, pud_t *pudp)\n{\n\tBUILD_BUG();\n}\n#endif  \n#endif  \n#endif\n\n#ifndef pmdp_collapse_flush\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nextern pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp);\n#else\nstatic inline pmd_t pmdp_collapse_flush(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long address,\n\t\t\t\t\tpmd_t *pmdp)\n{\n\tBUILD_BUG();\n\treturn *pmdp;\n}\n#define pmdp_collapse_flush pmdp_collapse_flush\n#endif  \n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_DEPOSIT\nextern void pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,\n\t\t\t\t       pgtable_t pgtable);\n#endif\n\n#ifndef __HAVE_ARCH_PGTABLE_WITHDRAW\nextern pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);\n#endif\n\n#ifndef arch_needs_pgtable_deposit\n#define arch_needs_pgtable_deposit() (false)\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \nstatic inline pmd_t generic_pmdp_establish(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmdp, pmd_t pmd)\n{\n\tpmd_t old_pmd = *pmdp;\n\tset_pmd_at(vma->vm_mm, address, pmdp, pmd);\n\treturn old_pmd;\n}\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_INVALIDATE\nextern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,\n\t\t\t    pmd_t *pmdp);\n#endif\n\n#ifndef __HAVE_ARCH_PMDP_INVALIDATE_AD\n\n \nextern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,\n\t\t\t\tunsigned long address, pmd_t *pmdp);\n#endif\n\n#ifndef __HAVE_ARCH_PTE_SAME\nstatic inline int pte_same(pte_t pte_a, pte_t pte_b)\n{\n\treturn pte_val(pte_a) == pte_val(pte_b);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PTE_UNUSED\n \nstatic inline int pte_unused(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef pte_access_permitted\n#define pte_access_permitted(pte, write) \\\n\t(pte_present(pte) && (!(write) || pte_write(pte)))\n#endif\n\n#ifndef pmd_access_permitted\n#define pmd_access_permitted(pmd, write) \\\n\t(pmd_present(pmd) && (!(write) || pmd_write(pmd)))\n#endif\n\n#ifndef pud_access_permitted\n#define pud_access_permitted(pud, write) \\\n\t(pud_present(pud) && (!(write) || pud_write(pud)))\n#endif\n\n#ifndef p4d_access_permitted\n#define p4d_access_permitted(p4d, write) \\\n\t(p4d_present(p4d) && (!(write) || p4d_write(p4d)))\n#endif\n\n#ifndef pgd_access_permitted\n#define pgd_access_permitted(pgd, write) \\\n\t(pgd_present(pgd) && (!(write) || pgd_write(pgd)))\n#endif\n\n#ifndef __HAVE_ARCH_PMD_SAME\nstatic inline int pmd_same(pmd_t pmd_a, pmd_t pmd_b)\n{\n\treturn pmd_val(pmd_a) == pmd_val(pmd_b);\n}\n#endif\n\n#ifndef pud_same\nstatic inline int pud_same(pud_t pud_a, pud_t pud_b)\n{\n\treturn pud_val(pud_a) == pud_val(pud_b);\n}\n#define pud_same pud_same\n#endif\n\n#ifndef __HAVE_ARCH_P4D_SAME\nstatic inline int p4d_same(p4d_t p4d_a, p4d_t p4d_b)\n{\n\treturn p4d_val(p4d_a) == p4d_val(p4d_b);\n}\n#endif\n\n#ifndef __HAVE_ARCH_PGD_SAME\nstatic inline int pgd_same(pgd_t pgd_a, pgd_t pgd_b)\n{\n\treturn pgd_val(pgd_a) == pgd_val(pgd_b);\n}\n#endif\n\n \n#define set_pte_safe(ptep, pte) \\\n({ \\\n\tWARN_ON_ONCE(pte_present(*ptep) && !pte_same(*ptep, pte)); \\\n\tset_pte(ptep, pte); \\\n})\n\n#define set_pmd_safe(pmdp, pmd) \\\n({ \\\n\tWARN_ON_ONCE(pmd_present(*pmdp) && !pmd_same(*pmdp, pmd)); \\\n\tset_pmd(pmdp, pmd); \\\n})\n\n#define set_pud_safe(pudp, pud) \\\n({ \\\n\tWARN_ON_ONCE(pud_present(*pudp) && !pud_same(*pudp, pud)); \\\n\tset_pud(pudp, pud); \\\n})\n\n#define set_p4d_safe(p4dp, p4d) \\\n({ \\\n\tWARN_ON_ONCE(p4d_present(*p4dp) && !p4d_same(*p4dp, p4d)); \\\n\tset_p4d(p4dp, p4d); \\\n})\n\n#define set_pgd_safe(pgdp, pgd) \\\n({ \\\n\tWARN_ON_ONCE(pgd_present(*pgdp) && !pgd_same(*pgdp, pgd)); \\\n\tset_pgd(pgdp, pgd); \\\n})\n\n#ifndef __HAVE_ARCH_DO_SWAP_PAGE\n \nstatic inline void arch_do_swap_page(struct mm_struct *mm,\n\t\t\t\t     struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr,\n\t\t\t\t     pte_t pte, pte_t oldpte)\n{\n\n}\n#endif\n\n#ifndef __HAVE_ARCH_UNMAP_ONE\n \nstatic inline int arch_unmap_one(struct mm_struct *mm,\n\t\t\t\t  struct vm_area_struct *vma,\n\t\t\t\t  unsigned long addr,\n\t\t\t\t  pte_t orig_pte)\n{\n\treturn 0;\n}\n#endif\n\n \n#ifndef __HAVE_ARCH_PREPARE_TO_SWAP\nstatic inline int arch_prepare_to_swap(struct page *page)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef __HAVE_ARCH_SWAP_INVALIDATE\nstatic inline void arch_swap_invalidate_page(int type, pgoff_t offset)\n{\n}\n\nstatic inline void arch_swap_invalidate_area(int type)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_SWAP_RESTORE\nstatic inline void arch_swap_restore(swp_entry_t entry, struct folio *folio)\n{\n}\n#endif\n\n#ifndef __HAVE_ARCH_PGD_OFFSET_GATE\n#define pgd_offset_gate(mm, addr)\tpgd_offset(mm, addr)\n#endif\n\n#ifndef __HAVE_ARCH_MOVE_PTE\n#define move_pte(pte, prot, old_addr, new_addr)\t(pte)\n#endif\n\n#ifndef pte_accessible\n# define pte_accessible(mm, pte)\t((void)(pte), 1)\n#endif\n\n#ifndef flush_tlb_fix_spurious_fault\n#define flush_tlb_fix_spurious_fault(vma, address, ptep) flush_tlb_page(vma, address)\n#endif\n\n \n\n#define pgd_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PGDIR_SIZE) & PGDIR_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n\n#ifndef p4d_addr_end\n#define p4d_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + P4D_SIZE) & P4D_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n#ifndef pud_addr_end\n#define pud_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PUD_SIZE) & PUD_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n#ifndef pmd_addr_end\n#define pmd_addr_end(addr, end)\t\t\t\t\t\t\\\n({\tunsigned long __boundary = ((addr) + PMD_SIZE) & PMD_MASK;\t\\\n\t(__boundary - 1 < (end) - 1)? __boundary: (end);\t\t\\\n})\n#endif\n\n \nvoid pgd_clear_bad(pgd_t *);\n\n#ifndef __PAGETABLE_P4D_FOLDED\nvoid p4d_clear_bad(p4d_t *);\n#else\n#define p4d_clear_bad(p4d)        do { } while (0)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\nvoid pud_clear_bad(pud_t *);\n#else\n#define pud_clear_bad(p4d)        do { } while (0)\n#endif\n\nvoid pmd_clear_bad(pmd_t *);\n\nstatic inline int pgd_none_or_clear_bad(pgd_t *pgd)\n{\n\tif (pgd_none(*pgd))\n\t\treturn 1;\n\tif (unlikely(pgd_bad(*pgd))) {\n\t\tpgd_clear_bad(pgd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int p4d_none_or_clear_bad(p4d_t *p4d)\n{\n\tif (p4d_none(*p4d))\n\t\treturn 1;\n\tif (unlikely(p4d_bad(*p4d))) {\n\t\tp4d_clear_bad(p4d);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int pud_none_or_clear_bad(pud_t *pud)\n{\n\tif (pud_none(*pud))\n\t\treturn 1;\n\tif (unlikely(pud_bad(*pud))) {\n\t\tpud_clear_bad(pud);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline int pmd_none_or_clear_bad(pmd_t *pmd)\n{\n\tif (pmd_none(*pmd))\n\t\treturn 1;\n\tif (unlikely(pmd_bad(*pmd))) {\n\t\tpmd_clear_bad(pmd);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic inline pte_t __ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     pte_t *ptep)\n{\n\t \n\treturn ptep_get_and_clear(vma->vm_mm, addr, ptep);\n}\n\nstatic inline void __ptep_modify_prot_commit(struct vm_area_struct *vma,\n\t\t\t\t\t     unsigned long addr,\n\t\t\t\t\t     pte_t *ptep, pte_t pte)\n{\n\t \n\tset_pte_at(vma->vm_mm, addr, ptep, pte);\n}\n\n#ifndef __HAVE_ARCH_PTEP_MODIFY_PROT_TRANSACTION\n \nstatic inline pte_t ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   pte_t *ptep)\n{\n\treturn __ptep_modify_prot_start(vma, addr, ptep);\n}\n\n \nstatic inline void ptep_modify_prot_commit(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   pte_t *ptep, pte_t old_pte, pte_t pte)\n{\n\t__ptep_modify_prot_commit(vma, addr, ptep, pte);\n}\n#endif  \n#endif  \n\n \n\n#ifndef pgprot_nx\n#define pgprot_nx(prot)\t(prot)\n#endif\n\n#ifndef pgprot_noncached\n#define pgprot_noncached(prot)\t(prot)\n#endif\n\n#ifndef pgprot_writecombine\n#define pgprot_writecombine pgprot_noncached\n#endif\n\n#ifndef pgprot_writethrough\n#define pgprot_writethrough pgprot_noncached\n#endif\n\n#ifndef pgprot_device\n#define pgprot_device pgprot_noncached\n#endif\n\n#ifndef pgprot_mhp\n#define pgprot_mhp(prot)\t(prot)\n#endif\n\n#ifdef CONFIG_MMU\n#ifndef pgprot_modify\n#define pgprot_modify pgprot_modify\nstatic inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)\n{\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_noncached(oldprot)))\n\t\tnewprot = pgprot_noncached(newprot);\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_writecombine(oldprot)))\n\t\tnewprot = pgprot_writecombine(newprot);\n\tif (pgprot_val(oldprot) == pgprot_val(pgprot_device(oldprot)))\n\t\tnewprot = pgprot_device(newprot);\n\treturn newprot;\n}\n#endif\n#endif  \n\n#ifndef pgprot_encrypted\n#define pgprot_encrypted(prot)\t(prot)\n#endif\n\n#ifndef pgprot_decrypted\n#define pgprot_decrypted(prot)\t(prot)\n#endif\n\n \n#ifndef __HAVE_ARCH_START_CONTEXT_SWITCH\n#define arch_start_context_switch(prev)\tdo {} while (0)\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\n#ifndef CONFIG_ARCH_ENABLE_THP_MIGRATION\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n#endif\n#else  \nstatic inline int pte_soft_dirty(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline int pmd_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_mksoft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline pte_t pte_clear_soft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline pte_t pte_swp_mksoft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline int pte_swp_soft_dirty(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_swp_clear_soft_dirty(pte_t pte)\n{\n\treturn pte;\n}\n\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd;\n}\n#endif\n\n#ifndef __HAVE_PFNMAP_TRACKING\n \n\n \nstatic inline int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t\t  unsigned long pfn, unsigned long addr,\n\t\t\t\t  unsigned long size)\n{\n\treturn 0;\n}\n\n \nstatic inline void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t\t    pfn_t pfn)\n{\n}\n\n \nstatic inline int track_pfn_copy(struct vm_area_struct *vma)\n{\n\treturn 0;\n}\n\n \nstatic inline void untrack_pfn(struct vm_area_struct *vma,\n\t\t\t       unsigned long pfn, unsigned long size,\n\t\t\t       bool mm_wr_locked)\n{\n}\n\n \nstatic inline void untrack_pfn_clear(struct vm_area_struct *vma)\n{\n}\n#else\nextern int track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t   unsigned long pfn, unsigned long addr,\n\t\t\t   unsigned long size);\nextern void track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t\t     pfn_t pfn);\nextern int track_pfn_copy(struct vm_area_struct *vma);\nextern void untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,\n\t\t\tunsigned long size, bool mm_wr_locked);\nextern void untrack_pfn_clear(struct vm_area_struct *vma);\n#endif\n\n#ifdef CONFIG_MMU\n#ifdef __HAVE_COLOR_ZERO_PAGE\nstatic inline int is_zero_pfn(unsigned long pfn)\n{\n\textern unsigned long zero_pfn;\n\tunsigned long offset_from_zero_pfn = pfn - zero_pfn;\n\treturn offset_from_zero_pfn <= (zero_page_mask >> PAGE_SHIFT);\n}\n\n#define my_zero_pfn(addr)\tpage_to_pfn(ZERO_PAGE(addr))\n\n#else\nstatic inline int is_zero_pfn(unsigned long pfn)\n{\n\textern unsigned long zero_pfn;\n\treturn pfn == zero_pfn;\n}\n\nstatic inline unsigned long my_zero_pfn(unsigned long addr)\n{\n\textern unsigned long zero_pfn;\n\treturn zero_pfn;\n}\n#endif\n#else\nstatic inline int is_zero_pfn(unsigned long pfn)\n{\n\treturn 0;\n}\n\nstatic inline unsigned long my_zero_pfn(unsigned long addr)\n{\n\treturn 0;\n}\n#endif  \n\n#ifdef CONFIG_MMU\n\n#ifndef CONFIG_TRANSPARENT_HUGEPAGE\nstatic inline int pmd_trans_huge(pmd_t pmd)\n{\n\treturn 0;\n}\n#ifndef pmd_write\nstatic inline int pmd_write(pmd_t pmd)\n{\n\tBUG();\n\treturn 0;\n}\n#endif  \n#endif  \n\n#ifndef pud_write\nstatic inline int pud_write(pud_t pud)\n{\n\tBUG();\n\treturn 0;\n}\n#endif  \n\n#if !defined(CONFIG_ARCH_HAS_PTE_DEVMAP) || !defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn 0;\n}\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn 0;\n}\nstatic inline int pgd_devmap(pgd_t pgd)\n{\n\treturn 0;\n}\n#endif\n\n#if !defined(CONFIG_TRANSPARENT_HUGEPAGE) || \\\n\t!defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\nstatic inline int pud_trans_huge(pud_t pud)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int pud_trans_unstable(pud_t *pud)\n{\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && \\\n\tdefined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n\tpud_t pudval = READ_ONCE(*pud);\n\n\tif (pud_none(pudval) || pud_trans_huge(pudval) || pud_devmap(pudval))\n\t\treturn 1;\n\tif (unlikely(pud_bad(pudval))) {\n\t\tpud_clear_bad(pud);\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\n#ifndef CONFIG_NUMA_BALANCING\n \nstatic inline int pte_protnone(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline int pmd_protnone(pmd_t pmd)\n{\n\treturn 0;\n}\n#endif  \n\n#endif  \n\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP\n\n#ifndef __PAGETABLE_P4D_FOLDED\nint p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot);\nvoid p4d_clear_huge(p4d_t *p4d);\n#else\nstatic inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline void p4d_clear_huge(p4d_t *p4d) { }\n#endif  \n\nint pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot);\nint pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot);\nint pud_clear_huge(pud_t *pud);\nint pmd_clear_huge(pmd_t *pmd);\nint p4d_free_pud_page(p4d_t *p4d, unsigned long addr);\nint pud_free_pmd_page(pud_t *pud, unsigned long addr);\nint pmd_free_pte_page(pmd_t *pmd, unsigned long addr);\n#else\t \nstatic inline int p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline int pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\nstatic inline void p4d_clear_huge(p4d_t *p4d) { }\nstatic inline int pud_clear_huge(pud_t *pud)\n{\n\treturn 0;\n}\nstatic inline int pmd_clear_huge(pmd_t *pmd)\n{\n\treturn 0;\n}\nstatic inline int p4d_free_pud_page(p4d_t *p4d, unsigned long addr)\n{\n\treturn 0;\n}\nstatic inline int pud_free_pmd_page(pud_t *pud, unsigned long addr)\n{\n\treturn 0;\n}\nstatic inline int pmd_free_pte_page(pmd_t *pmd, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t \n\n#ifndef __HAVE_ARCH_FLUSH_PMD_TLB_RANGE\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \n#define flush_pmd_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#define flush_pud_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#else\n#define flush_pmd_tlb_range(vma, addr, end)\tBUILD_BUG()\n#define flush_pud_tlb_range(vma, addr, end)\tBUILD_BUG()\n#endif\n#endif\n\nstruct file;\nint phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,\n\t\t\tunsigned long size, pgprot_t *vma_prot);\n\n#ifndef CONFIG_X86_ESPFIX64\nstatic inline void init_espfix_bsp(void) { }\n#endif\n\nextern void __init pgtable_cache_init(void);\n\n#ifndef __HAVE_ARCH_PFN_MODIFY_ALLOWED\nstatic inline bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot)\n{\n\treturn true;\n}\n\nstatic inline bool arch_has_pfn_modify_check(void)\n{\n\treturn false;\n}\n#endif  \n\n \n\n#ifndef PAGE_KERNEL_RO\n# define PAGE_KERNEL_RO PAGE_KERNEL\n#endif\n\n#ifndef PAGE_KERNEL_EXEC\n# define PAGE_KERNEL_EXEC PAGE_KERNEL\n#endif\n\n \n#define\t\t__PGTBL_PGD_MODIFIED\t0\n#define\t\t__PGTBL_P4D_MODIFIED\t1\n#define\t\t__PGTBL_PUD_MODIFIED\t2\n#define\t\t__PGTBL_PMD_MODIFIED\t3\n#define\t\t__PGTBL_PTE_MODIFIED\t4\n\n#define\t\tPGTBL_PGD_MODIFIED\tBIT(__PGTBL_PGD_MODIFIED)\n#define\t\tPGTBL_P4D_MODIFIED\tBIT(__PGTBL_P4D_MODIFIED)\n#define\t\tPGTBL_PUD_MODIFIED\tBIT(__PGTBL_PUD_MODIFIED)\n#define\t\tPGTBL_PMD_MODIFIED\tBIT(__PGTBL_PMD_MODIFIED)\n#define\t\tPGTBL_PTE_MODIFIED\tBIT(__PGTBL_PTE_MODIFIED)\n\n \ntypedef unsigned int pgtbl_mod_mask;\n\n#endif  \n\n#if !defined(MAX_POSSIBLE_PHYSMEM_BITS) && !defined(CONFIG_64BIT)\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n \n#error Missing MAX_POSSIBLE_PHYSMEM_BITS definition\n#else\n#define MAX_POSSIBLE_PHYSMEM_BITS 32\n#endif\n#endif\n\n#ifndef has_transparent_hugepage\n#define has_transparent_hugepage() IS_BUILTIN(CONFIG_TRANSPARENT_HUGEPAGE)\n#endif\n\n#ifndef has_transparent_pud_hugepage\n#define has_transparent_pud_hugepage() IS_BUILTIN(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)\n#endif\n \n#ifndef mm_p4d_folded\n#define mm_p4d_folded(mm)\t__is_defined(__PAGETABLE_P4D_FOLDED)\n#endif\n\n#ifndef mm_pud_folded\n#define mm_pud_folded(mm)\t__is_defined(__PAGETABLE_PUD_FOLDED)\n#endif\n\n#ifndef mm_pmd_folded\n#define mm_pmd_folded(mm)\t__is_defined(__PAGETABLE_PMD_FOLDED)\n#endif\n\n#ifndef p4d_offset_lockless\n#define p4d_offset_lockless(pgdp, pgd, address) p4d_offset(&(pgd), address)\n#endif\n#ifndef pud_offset_lockless\n#define pud_offset_lockless(p4dp, p4d, address) pud_offset(&(p4d), address)\n#endif\n#ifndef pmd_offset_lockless\n#define pmd_offset_lockless(pudp, pud, address) pmd_offset(&(pud), address)\n#endif\n\n \n#ifndef pgd_leaf\n#define pgd_leaf(x)\t0\n#endif\n#ifndef p4d_leaf\n#define p4d_leaf(x)\t0\n#endif\n#ifndef pud_leaf\n#define pud_leaf(x)\t0\n#endif\n#ifndef pmd_leaf\n#define pmd_leaf(x)\t0\n#endif\n\n#ifndef pgd_leaf_size\n#define pgd_leaf_size(x) (1ULL << PGDIR_SHIFT)\n#endif\n#ifndef p4d_leaf_size\n#define p4d_leaf_size(x) P4D_SIZE\n#endif\n#ifndef pud_leaf_size\n#define pud_leaf_size(x) PUD_SIZE\n#endif\n#ifndef pmd_leaf_size\n#define pmd_leaf_size(x) PMD_SIZE\n#endif\n#ifndef pte_leaf_size\n#define pte_leaf_size(x) PAGE_SIZE\n#endif\n\n \n\n#ifndef MAX_PTRS_PER_PTE\n#define MAX_PTRS_PER_PTE PTRS_PER_PTE\n#endif\n\n#ifndef MAX_PTRS_PER_PMD\n#define MAX_PTRS_PER_PMD PTRS_PER_PMD\n#endif\n\n#ifndef MAX_PTRS_PER_PUD\n#define MAX_PTRS_PER_PUD PTRS_PER_PUD\n#endif\n\n#ifndef MAX_PTRS_PER_P4D\n#define MAX_PTRS_PER_P4D PTRS_PER_P4D\n#endif\n\n \n#define DECLARE_VM_GET_PAGE_PROT\t\t\t\t\t\\\npgprot_t vm_get_page_prot(unsigned long vm_flags)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\t\treturn protection_map[vm_flags &\t\t\t\\\n\t\t\t(VM_READ | VM_WRITE | VM_EXEC | VM_SHARED)];\t\\\n}\t\t\t\t\t\t\t\t\t\\\nEXPORT_SYMBOL(vm_get_page_prot);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}