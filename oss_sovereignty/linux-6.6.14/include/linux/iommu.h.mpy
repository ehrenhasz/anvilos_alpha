{
  "module_name": "iommu.h",
  "hash_id": "94be25b2e1eeb9cf76fa11a73c6e699a956ec843b70c353b3b7d030a505a310e",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/iommu.h",
  "human_readable_source": " \n \n\n#ifndef __LINUX_IOMMU_H\n#define __LINUX_IOMMU_H\n\n#include <linux/scatterlist.h>\n#include <linux/device.h>\n#include <linux/types.h>\n#include <linux/errno.h>\n#include <linux/err.h>\n#include <linux/of.h>\n#include <uapi/linux/iommu.h>\n\n#define IOMMU_READ\t(1 << 0)\n#define IOMMU_WRITE\t(1 << 1)\n#define IOMMU_CACHE\t(1 << 2)  \n#define IOMMU_NOEXEC\t(1 << 3)\n#define IOMMU_MMIO\t(1 << 4)  \n \n#define IOMMU_PRIV\t(1 << 5)\n\nstruct iommu_ops;\nstruct iommu_group;\nstruct bus_type;\nstruct device;\nstruct iommu_domain;\nstruct iommu_domain_ops;\nstruct notifier_block;\nstruct iommu_sva;\nstruct iommu_fault_event;\nstruct iommu_dma_cookie;\n\n \n#define IOMMU_FAULT_READ\t0x0\n#define IOMMU_FAULT_WRITE\t0x1\n\ntypedef int (*iommu_fault_handler_t)(struct iommu_domain *,\n\t\t\tstruct device *, unsigned long, int, void *);\ntypedef int (*iommu_dev_fault_handler_t)(struct iommu_fault *, void *);\n\nstruct iommu_domain_geometry {\n\tdma_addr_t aperture_start;  \n\tdma_addr_t aperture_end;    \n\tbool force_aperture;        \n};\n\n \n#define __IOMMU_DOMAIN_PAGING\t(1U << 0)   \n#define __IOMMU_DOMAIN_DMA_API\t(1U << 1)   \n#define __IOMMU_DOMAIN_PT\t(1U << 2)   \n#define __IOMMU_DOMAIN_DMA_FQ\t(1U << 3)   \n\n#define __IOMMU_DOMAIN_SVA\t(1U << 4)   \n\n#define IOMMU_DOMAIN_ALLOC_FLAGS ~__IOMMU_DOMAIN_DMA_FQ\n \n#define IOMMU_DOMAIN_BLOCKED\t(0U)\n#define IOMMU_DOMAIN_IDENTITY\t(__IOMMU_DOMAIN_PT)\n#define IOMMU_DOMAIN_UNMANAGED\t(__IOMMU_DOMAIN_PAGING)\n#define IOMMU_DOMAIN_DMA\t(__IOMMU_DOMAIN_PAGING |\t\\\n\t\t\t\t __IOMMU_DOMAIN_DMA_API)\n#define IOMMU_DOMAIN_DMA_FQ\t(__IOMMU_DOMAIN_PAGING |\t\\\n\t\t\t\t __IOMMU_DOMAIN_DMA_API |\t\\\n\t\t\t\t __IOMMU_DOMAIN_DMA_FQ)\n#define IOMMU_DOMAIN_SVA\t(__IOMMU_DOMAIN_SVA)\n\nstruct iommu_domain {\n\tunsigned type;\n\tconst struct iommu_domain_ops *ops;\n\tunsigned long pgsize_bitmap;\t \n\tstruct iommu_domain_geometry geometry;\n\tstruct iommu_dma_cookie *iova_cookie;\n\tenum iommu_page_response_code (*iopf_handler)(struct iommu_fault *fault,\n\t\t\t\t\t\t      void *data);\n\tvoid *fault_data;\n\tunion {\n\t\tstruct {\n\t\t\tiommu_fault_handler_t handler;\n\t\t\tvoid *handler_token;\n\t\t};\n\t\tstruct {\t \n\t\t\tstruct mm_struct *mm;\n\t\t\tint users;\n\t\t};\n\t};\n};\n\nstatic inline bool iommu_is_dma_domain(struct iommu_domain *domain)\n{\n\treturn domain->type & __IOMMU_DOMAIN_DMA_API;\n}\n\nenum iommu_cap {\n\tIOMMU_CAP_CACHE_COHERENCY,\t \n\tIOMMU_CAP_NOEXEC,\t\t \n\tIOMMU_CAP_PRE_BOOT_PROTECTION,\t \n\t \n\tIOMMU_CAP_ENFORCE_CACHE_COHERENCY,\n\t \n\tIOMMU_CAP_DEFERRED_FLUSH,\n};\n\n \nenum iommu_resv_type {\n\t \n\tIOMMU_RESV_DIRECT,\n\t \n\tIOMMU_RESV_DIRECT_RELAXABLE,\n\t \n\tIOMMU_RESV_RESERVED,\n\t \n\tIOMMU_RESV_MSI,\n\t \n\tIOMMU_RESV_SW_MSI,\n};\n\n \nstruct iommu_resv_region {\n\tstruct list_head\tlist;\n\tphys_addr_t\t\tstart;\n\tsize_t\t\t\tlength;\n\tint\t\t\tprot;\n\tenum iommu_resv_type\ttype;\n\tvoid (*free)(struct device *dev, struct iommu_resv_region *region);\n};\n\nstruct iommu_iort_rmr_data {\n\tstruct iommu_resv_region rr;\n\n\t \n\tconst u32 *sids;\n\tu32 num_sids;\n};\n\n \nenum iommu_dev_features {\n\tIOMMU_DEV_FEAT_SVA,\n\tIOMMU_DEV_FEAT_IOPF,\n};\n\n#define IOMMU_NO_PASID\t(0U)  \n#define IOMMU_FIRST_GLOBAL_PASID\t(1U)  \n#define IOMMU_PASID_INVALID\t(-1U)\ntypedef unsigned int ioasid_t;\n\n#ifdef CONFIG_IOMMU_API\n\n \nstruct iommu_iotlb_gather {\n\tunsigned long\t\tstart;\n\tunsigned long\t\tend;\n\tsize_t\t\t\tpgsize;\n\tstruct list_head\tfreelist;\n\tbool\t\t\tqueued;\n};\n\n \nstruct iommu_ops {\n\tbool (*capable)(struct device *dev, enum iommu_cap);\n\tvoid *(*hw_info)(struct device *dev, u32 *length, u32 *type);\n\n\t \n\tstruct iommu_domain *(*domain_alloc)(unsigned iommu_domain_type);\n\n\tstruct iommu_device *(*probe_device)(struct device *dev);\n\tvoid (*release_device)(struct device *dev);\n\tvoid (*probe_finalize)(struct device *dev);\n\tvoid (*set_platform_dma_ops)(struct device *dev);\n\tstruct iommu_group *(*device_group)(struct device *dev);\n\n\t \n\tvoid (*get_resv_regions)(struct device *dev, struct list_head *list);\n\n\tint (*of_xlate)(struct device *dev, struct of_phandle_args *args);\n\tbool (*is_attach_deferred)(struct device *dev);\n\n\t \n\tint (*dev_enable_feat)(struct device *dev, enum iommu_dev_features f);\n\tint (*dev_disable_feat)(struct device *dev, enum iommu_dev_features f);\n\n\tint (*page_response)(struct device *dev,\n\t\t\t     struct iommu_fault_event *evt,\n\t\t\t     struct iommu_page_response *msg);\n\n\tint (*def_domain_type)(struct device *dev);\n\tvoid (*remove_dev_pasid)(struct device *dev, ioasid_t pasid);\n\n\tconst struct iommu_domain_ops *default_domain_ops;\n\tunsigned long pgsize_bitmap;\n\tstruct module *owner;\n};\n\n \nstruct iommu_domain_ops {\n\tint (*attach_dev)(struct iommu_domain *domain, struct device *dev);\n\tint (*set_dev_pasid)(struct iommu_domain *domain, struct device *dev,\n\t\t\t     ioasid_t pasid);\n\n\tint (*map)(struct iommu_domain *domain, unsigned long iova,\n\t\t   phys_addr_t paddr, size_t size, int prot, gfp_t gfp);\n\tint (*map_pages)(struct iommu_domain *domain, unsigned long iova,\n\t\t\t phys_addr_t paddr, size_t pgsize, size_t pgcount,\n\t\t\t int prot, gfp_t gfp, size_t *mapped);\n\tsize_t (*unmap)(struct iommu_domain *domain, unsigned long iova,\n\t\t\tsize_t size, struct iommu_iotlb_gather *iotlb_gather);\n\tsize_t (*unmap_pages)(struct iommu_domain *domain, unsigned long iova,\n\t\t\t      size_t pgsize, size_t pgcount,\n\t\t\t      struct iommu_iotlb_gather *iotlb_gather);\n\n\tvoid (*flush_iotlb_all)(struct iommu_domain *domain);\n\tvoid (*iotlb_sync_map)(struct iommu_domain *domain, unsigned long iova,\n\t\t\t       size_t size);\n\tvoid (*iotlb_sync)(struct iommu_domain *domain,\n\t\t\t   struct iommu_iotlb_gather *iotlb_gather);\n\n\tphys_addr_t (*iova_to_phys)(struct iommu_domain *domain,\n\t\t\t\t    dma_addr_t iova);\n\n\tbool (*enforce_cache_coherency)(struct iommu_domain *domain);\n\tint (*enable_nesting)(struct iommu_domain *domain);\n\tint (*set_pgtable_quirks)(struct iommu_domain *domain,\n\t\t\t\t  unsigned long quirks);\n\n\tvoid (*free)(struct iommu_domain *domain);\n};\n\n \nstruct iommu_device {\n\tstruct list_head list;\n\tconst struct iommu_ops *ops;\n\tstruct fwnode_handle *fwnode;\n\tstruct device *dev;\n\tu32 max_pasids;\n};\n\n \nstruct iommu_fault_event {\n\tstruct iommu_fault fault;\n\tstruct list_head list;\n};\n\n \nstruct iommu_fault_param {\n\tiommu_dev_fault_handler_t handler;\n\tvoid *data;\n\tstruct list_head faults;\n\tstruct mutex lock;\n};\n\n \nstruct dev_iommu {\n\tstruct mutex lock;\n\tstruct iommu_fault_param\t*fault_param;\n\tstruct iopf_device_param\t*iopf_param;\n\tstruct iommu_fwspec\t\t*fwspec;\n\tstruct iommu_device\t\t*iommu_dev;\n\tvoid\t\t\t\t*priv;\n\tu32\t\t\t\tmax_pasids;\n\tu32\t\t\t\tattach_deferred:1;\n\tu32\t\t\t\tpci_32bit_workaround:1;\n\tu32\t\t\t\trequire_direct:1;\n};\n\nint iommu_device_register(struct iommu_device *iommu,\n\t\t\t  const struct iommu_ops *ops,\n\t\t\t  struct device *hwdev);\nvoid iommu_device_unregister(struct iommu_device *iommu);\nint  iommu_device_sysfs_add(struct iommu_device *iommu,\n\t\t\t    struct device *parent,\n\t\t\t    const struct attribute_group **groups,\n\t\t\t    const char *fmt, ...) __printf(4, 5);\nvoid iommu_device_sysfs_remove(struct iommu_device *iommu);\nint  iommu_device_link(struct iommu_device   *iommu, struct device *link);\nvoid iommu_device_unlink(struct iommu_device *iommu, struct device *link);\nint iommu_deferred_attach(struct device *dev, struct iommu_domain *domain);\n\nstatic inline struct iommu_device *dev_to_iommu_device(struct device *dev)\n{\n\treturn (struct iommu_device *)dev_get_drvdata(dev);\n}\n\nstatic inline void iommu_iotlb_gather_init(struct iommu_iotlb_gather *gather)\n{\n\t*gather = (struct iommu_iotlb_gather) {\n\t\t.start\t= ULONG_MAX,\n\t\t.freelist = LIST_HEAD_INIT(gather->freelist),\n\t};\n}\n\nextern int bus_iommu_probe(const struct bus_type *bus);\nextern bool iommu_present(const struct bus_type *bus);\nextern bool device_iommu_capable(struct device *dev, enum iommu_cap cap);\nextern bool iommu_group_has_isolated_msi(struct iommu_group *group);\nextern struct iommu_domain *iommu_domain_alloc(const struct bus_type *bus);\nextern void iommu_domain_free(struct iommu_domain *domain);\nextern int iommu_attach_device(struct iommu_domain *domain,\n\t\t\t       struct device *dev);\nextern void iommu_detach_device(struct iommu_domain *domain,\n\t\t\t\tstruct device *dev);\nextern int iommu_sva_unbind_gpasid(struct iommu_domain *domain,\n\t\t\t\t   struct device *dev, ioasid_t pasid);\nextern struct iommu_domain *iommu_get_domain_for_dev(struct device *dev);\nextern struct iommu_domain *iommu_get_dma_domain(struct device *dev);\nextern int iommu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t     phys_addr_t paddr, size_t size, int prot, gfp_t gfp);\nextern size_t iommu_unmap(struct iommu_domain *domain, unsigned long iova,\n\t\t\t  size_t size);\nextern size_t iommu_unmap_fast(struct iommu_domain *domain,\n\t\t\t       unsigned long iova, size_t size,\n\t\t\t       struct iommu_iotlb_gather *iotlb_gather);\nextern ssize_t iommu_map_sg(struct iommu_domain *domain, unsigned long iova,\n\t\t\t    struct scatterlist *sg, unsigned int nents,\n\t\t\t    int prot, gfp_t gfp);\nextern phys_addr_t iommu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova);\nextern void iommu_set_fault_handler(struct iommu_domain *domain,\n\t\t\tiommu_fault_handler_t handler, void *token);\n\nextern void iommu_get_resv_regions(struct device *dev, struct list_head *list);\nextern void iommu_put_resv_regions(struct device *dev, struct list_head *list);\nextern void iommu_set_default_passthrough(bool cmd_line);\nextern void iommu_set_default_translated(bool cmd_line);\nextern bool iommu_default_passthrough(void);\nextern struct iommu_resv_region *\niommu_alloc_resv_region(phys_addr_t start, size_t length, int prot,\n\t\t\tenum iommu_resv_type type, gfp_t gfp);\nextern int iommu_get_group_resv_regions(struct iommu_group *group,\n\t\t\t\t\tstruct list_head *head);\n\nextern int iommu_attach_group(struct iommu_domain *domain,\n\t\t\t      struct iommu_group *group);\nextern void iommu_detach_group(struct iommu_domain *domain,\n\t\t\t       struct iommu_group *group);\nextern struct iommu_group *iommu_group_alloc(void);\nextern void *iommu_group_get_iommudata(struct iommu_group *group);\nextern void iommu_group_set_iommudata(struct iommu_group *group,\n\t\t\t\t      void *iommu_data,\n\t\t\t\t      void (*release)(void *iommu_data));\nextern int iommu_group_set_name(struct iommu_group *group, const char *name);\nextern int iommu_group_add_device(struct iommu_group *group,\n\t\t\t\t  struct device *dev);\nextern void iommu_group_remove_device(struct device *dev);\nextern int iommu_group_for_each_dev(struct iommu_group *group, void *data,\n\t\t\t\t    int (*fn)(struct device *, void *));\nextern struct iommu_group *iommu_group_get(struct device *dev);\nextern struct iommu_group *iommu_group_ref_get(struct iommu_group *group);\nextern void iommu_group_put(struct iommu_group *group);\nextern int iommu_register_device_fault_handler(struct device *dev,\n\t\t\t\t\tiommu_dev_fault_handler_t handler,\n\t\t\t\t\tvoid *data);\n\nextern int iommu_unregister_device_fault_handler(struct device *dev);\n\nextern int iommu_report_device_fault(struct device *dev,\n\t\t\t\t     struct iommu_fault_event *evt);\nextern int iommu_page_response(struct device *dev,\n\t\t\t       struct iommu_page_response *msg);\n\nextern int iommu_group_id(struct iommu_group *group);\nextern struct iommu_domain *iommu_group_default_domain(struct iommu_group *);\n\nint iommu_enable_nesting(struct iommu_domain *domain);\nint iommu_set_pgtable_quirks(struct iommu_domain *domain,\n\t\tunsigned long quirks);\n\nvoid iommu_set_dma_strict(void);\n\nextern int report_iommu_fault(struct iommu_domain *domain, struct device *dev,\n\t\t\t      unsigned long iova, int flags);\n\nstatic inline void iommu_flush_iotlb_all(struct iommu_domain *domain)\n{\n\tif (domain->ops->flush_iotlb_all)\n\t\tdomain->ops->flush_iotlb_all(domain);\n}\n\nstatic inline void iommu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\t  struct iommu_iotlb_gather *iotlb_gather)\n{\n\tif (domain->ops->iotlb_sync)\n\t\tdomain->ops->iotlb_sync(domain, iotlb_gather);\n\n\tiommu_iotlb_gather_init(iotlb_gather);\n}\n\n \nstatic inline\nbool iommu_iotlb_gather_is_disjoint(struct iommu_iotlb_gather *gather,\n\t\t\t\t    unsigned long iova, size_t size)\n{\n\tunsigned long start = iova, end = start + size - 1;\n\n\treturn gather->end != 0 &&\n\t\t(end + 1 < gather->start || start > gather->end + 1);\n}\n\n\n \nstatic inline void iommu_iotlb_gather_add_range(struct iommu_iotlb_gather *gather,\n\t\t\t\t\t\tunsigned long iova, size_t size)\n{\n\tunsigned long end = iova + size - 1;\n\n\tif (gather->start > iova)\n\t\tgather->start = iova;\n\tif (gather->end < end)\n\t\tgather->end = end;\n}\n\n \nstatic inline void iommu_iotlb_gather_add_page(struct iommu_domain *domain,\n\t\t\t\t\t       struct iommu_iotlb_gather *gather,\n\t\t\t\t\t       unsigned long iova, size_t size)\n{\n\t \n\tif ((gather->pgsize && gather->pgsize != size) ||\n\t    iommu_iotlb_gather_is_disjoint(gather, iova, size))\n\t\tiommu_iotlb_sync(domain, gather);\n\n\tgather->pgsize = size;\n\tiommu_iotlb_gather_add_range(gather, iova, size);\n}\n\nstatic inline bool iommu_iotlb_gather_queued(struct iommu_iotlb_gather *gather)\n{\n\treturn gather && gather->queued;\n}\n\n \nextern struct iommu_group *pci_device_group(struct device *dev);\n \nextern struct iommu_group *generic_device_group(struct device *dev);\n \nstruct iommu_group *fsl_mc_device_group(struct device *dev);\n\n \nstruct iommu_fwspec {\n\tconst struct iommu_ops\t*ops;\n\tstruct fwnode_handle\t*iommu_fwnode;\n\tu32\t\t\tflags;\n\tunsigned int\t\tnum_ids;\n\tu32\t\t\tids[];\n};\n\n \n#define IOMMU_FWSPEC_PCI_RC_ATS\t\t\t(1 << 0)\n\n \nstruct iommu_sva {\n\tstruct device\t\t\t*dev;\n\tstruct iommu_domain\t\t*domain;\n};\n\nint iommu_fwspec_init(struct device *dev, struct fwnode_handle *iommu_fwnode,\n\t\t      const struct iommu_ops *ops);\nvoid iommu_fwspec_free(struct device *dev);\nint iommu_fwspec_add_ids(struct device *dev, u32 *ids, int num_ids);\nconst struct iommu_ops *iommu_ops_from_fwnode(struct fwnode_handle *fwnode);\n\nstatic inline struct iommu_fwspec *dev_iommu_fwspec_get(struct device *dev)\n{\n\tif (dev->iommu)\n\t\treturn dev->iommu->fwspec;\n\telse\n\t\treturn NULL;\n}\n\nstatic inline void dev_iommu_fwspec_set(struct device *dev,\n\t\t\t\t\tstruct iommu_fwspec *fwspec)\n{\n\tdev->iommu->fwspec = fwspec;\n}\n\nstatic inline void *dev_iommu_priv_get(struct device *dev)\n{\n\tif (dev->iommu)\n\t\treturn dev->iommu->priv;\n\telse\n\t\treturn NULL;\n}\n\nstatic inline void dev_iommu_priv_set(struct device *dev, void *priv)\n{\n\tdev->iommu->priv = priv;\n}\n\nextern struct mutex iommu_probe_device_lock;\nint iommu_probe_device(struct device *dev);\n\nint iommu_dev_enable_feature(struct device *dev, enum iommu_dev_features f);\nint iommu_dev_disable_feature(struct device *dev, enum iommu_dev_features f);\n\nint iommu_device_use_default_domain(struct device *dev);\nvoid iommu_device_unuse_default_domain(struct device *dev);\n\nint iommu_group_claim_dma_owner(struct iommu_group *group, void *owner);\nvoid iommu_group_release_dma_owner(struct iommu_group *group);\nbool iommu_group_dma_owner_claimed(struct iommu_group *group);\n\nint iommu_device_claim_dma_owner(struct device *dev, void *owner);\nvoid iommu_device_release_dma_owner(struct device *dev);\n\nstruct iommu_domain *iommu_sva_domain_alloc(struct device *dev,\n\t\t\t\t\t    struct mm_struct *mm);\nint iommu_attach_device_pasid(struct iommu_domain *domain,\n\t\t\t      struct device *dev, ioasid_t pasid);\nvoid iommu_detach_device_pasid(struct iommu_domain *domain,\n\t\t\t       struct device *dev, ioasid_t pasid);\nstruct iommu_domain *\niommu_get_domain_for_dev_pasid(struct device *dev, ioasid_t pasid,\n\t\t\t       unsigned int type);\nioasid_t iommu_alloc_global_pasid(struct device *dev);\nvoid iommu_free_global_pasid(ioasid_t pasid);\n#else  \n\nstruct iommu_ops {};\nstruct iommu_group {};\nstruct iommu_fwspec {};\nstruct iommu_device {};\nstruct iommu_fault_param {};\nstruct iommu_iotlb_gather {};\n\nstatic inline bool iommu_present(const struct bus_type *bus)\n{\n\treturn false;\n}\n\nstatic inline bool device_iommu_capable(struct device *dev, enum iommu_cap cap)\n{\n\treturn false;\n}\n\nstatic inline struct iommu_domain *iommu_domain_alloc(const struct bus_type *bus)\n{\n\treturn NULL;\n}\n\nstatic inline void iommu_domain_free(struct iommu_domain *domain)\n{\n}\n\nstatic inline int iommu_attach_device(struct iommu_domain *domain,\n\t\t\t\t      struct device *dev)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_detach_device(struct iommu_domain *domain,\n\t\t\t\t       struct device *dev)\n{\n}\n\nstatic inline struct iommu_domain *iommu_get_domain_for_dev(struct device *dev)\n{\n\treturn NULL;\n}\n\nstatic inline int iommu_map(struct iommu_domain *domain, unsigned long iova,\n\t\t\t    phys_addr_t paddr, size_t size, int prot, gfp_t gfp)\n{\n\treturn -ENODEV;\n}\n\nstatic inline size_t iommu_unmap(struct iommu_domain *domain,\n\t\t\t\t unsigned long iova, size_t size)\n{\n\treturn 0;\n}\n\nstatic inline size_t iommu_unmap_fast(struct iommu_domain *domain,\n\t\t\t\t      unsigned long iova, int gfp_order,\n\t\t\t\t      struct iommu_iotlb_gather *iotlb_gather)\n{\n\treturn 0;\n}\n\nstatic inline ssize_t iommu_map_sg(struct iommu_domain *domain,\n\t\t\t\t   unsigned long iova, struct scatterlist *sg,\n\t\t\t\t   unsigned int nents, int prot, gfp_t gfp)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_flush_iotlb_all(struct iommu_domain *domain)\n{\n}\n\nstatic inline void iommu_iotlb_sync(struct iommu_domain *domain,\n\t\t\t\t  struct iommu_iotlb_gather *iotlb_gather)\n{\n}\n\nstatic inline phys_addr_t iommu_iova_to_phys(struct iommu_domain *domain, dma_addr_t iova)\n{\n\treturn 0;\n}\n\nstatic inline void iommu_set_fault_handler(struct iommu_domain *domain,\n\t\t\t\tiommu_fault_handler_t handler, void *token)\n{\n}\n\nstatic inline void iommu_get_resv_regions(struct device *dev,\n\t\t\t\t\tstruct list_head *list)\n{\n}\n\nstatic inline void iommu_put_resv_regions(struct device *dev,\n\t\t\t\t\tstruct list_head *list)\n{\n}\n\nstatic inline int iommu_get_group_resv_regions(struct iommu_group *group,\n\t\t\t\t\t       struct list_head *head)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_set_default_passthrough(bool cmd_line)\n{\n}\n\nstatic inline void iommu_set_default_translated(bool cmd_line)\n{\n}\n\nstatic inline bool iommu_default_passthrough(void)\n{\n\treturn true;\n}\n\nstatic inline int iommu_attach_group(struct iommu_domain *domain,\n\t\t\t\t     struct iommu_group *group)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_detach_group(struct iommu_domain *domain,\n\t\t\t\t      struct iommu_group *group)\n{\n}\n\nstatic inline struct iommu_group *iommu_group_alloc(void)\n{\n\treturn ERR_PTR(-ENODEV);\n}\n\nstatic inline void *iommu_group_get_iommudata(struct iommu_group *group)\n{\n\treturn NULL;\n}\n\nstatic inline void iommu_group_set_iommudata(struct iommu_group *group,\n\t\t\t\t\t     void *iommu_data,\n\t\t\t\t\t     void (*release)(void *iommu_data))\n{\n}\n\nstatic inline int iommu_group_set_name(struct iommu_group *group,\n\t\t\t\t       const char *name)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_group_add_device(struct iommu_group *group,\n\t\t\t\t\t struct device *dev)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_group_remove_device(struct device *dev)\n{\n}\n\nstatic inline int iommu_group_for_each_dev(struct iommu_group *group,\n\t\t\t\t\t   void *data,\n\t\t\t\t\t   int (*fn)(struct device *, void *))\n{\n\treturn -ENODEV;\n}\n\nstatic inline struct iommu_group *iommu_group_get(struct device *dev)\n{\n\treturn NULL;\n}\n\nstatic inline void iommu_group_put(struct iommu_group *group)\n{\n}\n\nstatic inline\nint iommu_register_device_fault_handler(struct device *dev,\n\t\t\t\t\tiommu_dev_fault_handler_t handler,\n\t\t\t\t\tvoid *data)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_unregister_device_fault_handler(struct device *dev)\n{\n\treturn 0;\n}\n\nstatic inline\nint iommu_report_device_fault(struct device *dev, struct iommu_fault_event *evt)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_page_response(struct device *dev,\n\t\t\t\t      struct iommu_page_response *msg)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_group_id(struct iommu_group *group)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_set_pgtable_quirks(struct iommu_domain *domain,\n\t\tunsigned long quirks)\n{\n\treturn 0;\n}\n\nstatic inline int iommu_device_register(struct iommu_device *iommu,\n\t\t\t\t\tconst struct iommu_ops *ops,\n\t\t\t\t\tstruct device *hwdev)\n{\n\treturn -ENODEV;\n}\n\nstatic inline struct iommu_device *dev_to_iommu_device(struct device *dev)\n{\n\treturn NULL;\n}\n\nstatic inline void iommu_iotlb_gather_init(struct iommu_iotlb_gather *gather)\n{\n}\n\nstatic inline void iommu_iotlb_gather_add_page(struct iommu_domain *domain,\n\t\t\t\t\t       struct iommu_iotlb_gather *gather,\n\t\t\t\t\t       unsigned long iova, size_t size)\n{\n}\n\nstatic inline bool iommu_iotlb_gather_queued(struct iommu_iotlb_gather *gather)\n{\n\treturn false;\n}\n\nstatic inline void iommu_device_unregister(struct iommu_device *iommu)\n{\n}\n\nstatic inline int  iommu_device_sysfs_add(struct iommu_device *iommu,\n\t\t\t\t\t  struct device *parent,\n\t\t\t\t\t  const struct attribute_group **groups,\n\t\t\t\t\t  const char *fmt, ...)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_device_sysfs_remove(struct iommu_device *iommu)\n{\n}\n\nstatic inline int iommu_device_link(struct device *dev, struct device *link)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void iommu_device_unlink(struct device *dev, struct device *link)\n{\n}\n\nstatic inline int iommu_fwspec_init(struct device *dev,\n\t\t\t\t    struct fwnode_handle *iommu_fwnode,\n\t\t\t\t    const struct iommu_ops *ops)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_fwspec_free(struct device *dev)\n{\n}\n\nstatic inline int iommu_fwspec_add_ids(struct device *dev, u32 *ids,\n\t\t\t\t       int num_ids)\n{\n\treturn -ENODEV;\n}\n\nstatic inline\nconst struct iommu_ops *iommu_ops_from_fwnode(struct fwnode_handle *fwnode)\n{\n\treturn NULL;\n}\n\nstatic inline int\niommu_dev_enable_feature(struct device *dev, enum iommu_dev_features feat)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int\niommu_dev_disable_feature(struct device *dev, enum iommu_dev_features feat)\n{\n\treturn -ENODEV;\n}\n\nstatic inline struct iommu_fwspec *dev_iommu_fwspec_get(struct device *dev)\n{\n\treturn NULL;\n}\n\nstatic inline int iommu_device_use_default_domain(struct device *dev)\n{\n\treturn 0;\n}\n\nstatic inline void iommu_device_unuse_default_domain(struct device *dev)\n{\n}\n\nstatic inline int\niommu_group_claim_dma_owner(struct iommu_group *group, void *owner)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_group_release_dma_owner(struct iommu_group *group)\n{\n}\n\nstatic inline bool iommu_group_dma_owner_claimed(struct iommu_group *group)\n{\n\treturn false;\n}\n\nstatic inline void iommu_device_release_dma_owner(struct device *dev)\n{\n}\n\nstatic inline int iommu_device_claim_dma_owner(struct device *dev, void *owner)\n{\n\treturn -ENODEV;\n}\n\nstatic inline struct iommu_domain *\niommu_sva_domain_alloc(struct device *dev, struct mm_struct *mm)\n{\n\treturn NULL;\n}\n\nstatic inline int iommu_attach_device_pasid(struct iommu_domain *domain,\n\t\t\t\t\t    struct device *dev, ioasid_t pasid)\n{\n\treturn -ENODEV;\n}\n\nstatic inline void iommu_detach_device_pasid(struct iommu_domain *domain,\n\t\t\t\t\t     struct device *dev, ioasid_t pasid)\n{\n}\n\nstatic inline struct iommu_domain *\niommu_get_domain_for_dev_pasid(struct device *dev, ioasid_t pasid,\n\t\t\t       unsigned int type)\n{\n\treturn NULL;\n}\n\nstatic inline ioasid_t iommu_alloc_global_pasid(struct device *dev)\n{\n\treturn IOMMU_PASID_INVALID;\n}\n\nstatic inline void iommu_free_global_pasid(ioasid_t pasid) {}\n#endif  \n\n \nstatic inline size_t iommu_map_sgtable(struct iommu_domain *domain,\n\t\t\tunsigned long iova, struct sg_table *sgt, int prot)\n{\n\treturn iommu_map_sg(domain, iova, sgt->sgl, sgt->orig_nents, prot,\n\t\t\t    GFP_KERNEL);\n}\n\n#ifdef CONFIG_IOMMU_DEBUGFS\nextern\tstruct dentry *iommu_debugfs_dir;\nvoid iommu_debugfs_setup(void);\n#else\nstatic inline void iommu_debugfs_setup(void) {}\n#endif\n\n#ifdef CONFIG_IOMMU_DMA\n#include <linux/msi.h>\n\n \nvoid iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 dma_limit);\n\nint iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base);\n\nint iommu_dma_prepare_msi(struct msi_desc *desc, phys_addr_t msi_addr);\nvoid iommu_dma_compose_msi_msg(struct msi_desc *desc, struct msi_msg *msg);\n\n#else  \n\nstruct msi_desc;\nstruct msi_msg;\n\nstatic inline void iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 dma_limit)\n{\n}\n\nstatic inline int iommu_get_msi_cookie(struct iommu_domain *domain, dma_addr_t base)\n{\n\treturn -ENODEV;\n}\n\nstatic inline int iommu_dma_prepare_msi(struct msi_desc *desc, phys_addr_t msi_addr)\n{\n\treturn 0;\n}\n\nstatic inline void iommu_dma_compose_msi_msg(struct msi_desc *desc, struct msi_msg *msg)\n{\n}\n\n#endif\t \n\n \n#define TEGRA_STREAM_ID_BYPASS 0x7f\n\nstatic inline bool tegra_dev_iommu_get_stream_id(struct device *dev, u32 *stream_id)\n{\n#ifdef CONFIG_IOMMU_API\n\tstruct iommu_fwspec *fwspec = dev_iommu_fwspec_get(dev);\n\n\tif (fwspec && fwspec->num_ids == 1) {\n\t\t*stream_id = fwspec->ids[0] & 0xffff;\n\t\treturn true;\n\t}\n#endif\n\n\treturn false;\n}\n\n#ifdef CONFIG_IOMMU_SVA\nstatic inline void mm_pasid_init(struct mm_struct *mm)\n{\n\tmm->pasid = IOMMU_PASID_INVALID;\n}\nstatic inline bool mm_valid_pasid(struct mm_struct *mm)\n{\n\treturn mm->pasid != IOMMU_PASID_INVALID;\n}\nvoid mm_pasid_drop(struct mm_struct *mm);\nstruct iommu_sva *iommu_sva_bind_device(struct device *dev,\n\t\t\t\t\tstruct mm_struct *mm);\nvoid iommu_sva_unbind_device(struct iommu_sva *handle);\nu32 iommu_sva_get_pasid(struct iommu_sva *handle);\n#else\nstatic inline struct iommu_sva *\niommu_sva_bind_device(struct device *dev, struct mm_struct *mm)\n{\n\treturn NULL;\n}\n\nstatic inline void iommu_sva_unbind_device(struct iommu_sva *handle)\n{\n}\n\nstatic inline u32 iommu_sva_get_pasid(struct iommu_sva *handle)\n{\n\treturn IOMMU_PASID_INVALID;\n}\nstatic inline void mm_pasid_init(struct mm_struct *mm) {}\nstatic inline bool mm_valid_pasid(struct mm_struct *mm) { return false; }\nstatic inline void mm_pasid_drop(struct mm_struct *mm) {}\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}