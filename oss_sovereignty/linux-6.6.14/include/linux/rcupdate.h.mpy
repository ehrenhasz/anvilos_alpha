{
  "module_name": "rcupdate.h",
  "hash_id": "67ee2f2c67b250d3332ae3ebd273aa12a7e65ca6d1d8bc7af177357ba8599d67",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/rcupdate.h",
  "human_readable_source": " \n \n\n#ifndef __LINUX_RCUPDATE_H\n#define __LINUX_RCUPDATE_H\n\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/atomic.h>\n#include <linux/irqflags.h>\n#include <linux/preempt.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <linux/cleanup.h>\n#include <asm/processor.h>\n#include <linux/cpumask.h>\n#include <linux/context_tracking_irq.h>\n\n#define ULONG_CMP_GE(a, b)\t(ULONG_MAX / 2 >= (a) - (b))\n#define ULONG_CMP_LT(a, b)\t(ULONG_MAX / 2 < (a) - (b))\n#define ulong2long(a)\t\t(*(long *)(&(a)))\n#define USHORT_CMP_GE(a, b)\t(USHRT_MAX / 2 >= (unsigned short)((a) - (b)))\n#define USHORT_CMP_LT(a, b)\t(USHRT_MAX / 2 < (unsigned short)((a) - (b)))\n\n \nvoid call_rcu(struct rcu_head *head, rcu_callback_t func);\nvoid rcu_barrier_tasks(void);\nvoid rcu_barrier_tasks_rude(void);\nvoid synchronize_rcu(void);\n\nstruct rcu_gp_oldstate;\nunsigned long get_completed_synchronize_rcu(void);\nvoid get_completed_synchronize_rcu_full(struct rcu_gp_oldstate *rgosp);\n\n\n\n#define NUM_ACTIVE_RCU_POLL_OLDSTATE 2\n\n \nstatic inline bool same_state_synchronize_rcu(unsigned long oldstate1, unsigned long oldstate2)\n{\n\treturn oldstate1 == oldstate2;\n}\n\n#ifdef CONFIG_PREEMPT_RCU\n\nvoid __rcu_read_lock(void);\nvoid __rcu_read_unlock(void);\n\n \n#define rcu_preempt_depth() READ_ONCE(current->rcu_read_lock_nesting)\n\n#else  \n\n#ifdef CONFIG_TINY_RCU\n#define rcu_read_unlock_strict() do { } while (0)\n#else\nvoid rcu_read_unlock_strict(void);\n#endif\n\nstatic inline void __rcu_read_lock(void)\n{\n\tpreempt_disable();\n}\n\nstatic inline void __rcu_read_unlock(void)\n{\n\tpreempt_enable();\n\tif (IS_ENABLED(CONFIG_RCU_STRICT_GRACE_PERIOD))\n\t\trcu_read_unlock_strict();\n}\n\nstatic inline int rcu_preempt_depth(void)\n{\n\treturn 0;\n}\n\n#endif  \n\n#ifdef CONFIG_RCU_LAZY\nvoid call_rcu_hurry(struct rcu_head *head, rcu_callback_t func);\n#else\nstatic inline void call_rcu_hurry(struct rcu_head *head, rcu_callback_t func)\n{\n\tcall_rcu(head, func);\n}\n#endif\n\n \nvoid rcu_init(void);\nextern int rcu_scheduler_active;\nvoid rcu_sched_clock_irq(int user);\nvoid rcu_report_dead(unsigned int cpu);\nvoid rcutree_migrate_callbacks(int cpu);\n\n#ifdef CONFIG_TASKS_RCU_GENERIC\nvoid rcu_init_tasks_generic(void);\n#else\nstatic inline void rcu_init_tasks_generic(void) { }\n#endif\n\n#ifdef CONFIG_RCU_STALL_COMMON\nvoid rcu_sysrq_start(void);\nvoid rcu_sysrq_end(void);\n#else  \nstatic inline void rcu_sysrq_start(void) { }\nstatic inline void rcu_sysrq_end(void) { }\n#endif  \n\n#if defined(CONFIG_NO_HZ_FULL) && (!defined(CONFIG_GENERIC_ENTRY) || !defined(CONFIG_KVM_XFER_TO_GUEST_WORK))\nvoid rcu_irq_work_resched(void);\n#else\nstatic inline void rcu_irq_work_resched(void) { }\n#endif\n\n#ifdef CONFIG_RCU_NOCB_CPU\nvoid rcu_init_nohz(void);\nint rcu_nocb_cpu_offload(int cpu);\nint rcu_nocb_cpu_deoffload(int cpu);\nvoid rcu_nocb_flush_deferred_wakeup(void);\n#else  \nstatic inline void rcu_init_nohz(void) { }\nstatic inline int rcu_nocb_cpu_offload(int cpu) { return -EINVAL; }\nstatic inline int rcu_nocb_cpu_deoffload(int cpu) { return 0; }\nstatic inline void rcu_nocb_flush_deferred_wakeup(void) { }\n#endif  \n\n \n#ifdef CONFIG_TASKS_RCU_GENERIC\n\n# ifdef CONFIG_TASKS_RCU\n# define rcu_tasks_classic_qs(t, preempt)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (!(preempt) && READ_ONCE((t)->rcu_tasks_holdout))\t\\\n\t\t\tWRITE_ONCE((t)->rcu_tasks_holdout, false);\t\\\n\t} while (0)\nvoid call_rcu_tasks(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks(void);\n# else\n# define rcu_tasks_classic_qs(t, preempt) do { } while (0)\n# define call_rcu_tasks call_rcu\n# define synchronize_rcu_tasks synchronize_rcu\n# endif\n\n# ifdef CONFIG_TASKS_TRACE_RCU\n\n#define TRC_NEED_QS\t\t0x1  \n#define TRC_NEED_QS_CHECKED\t0x2  \n\nu8 rcu_trc_cmpxchg_need_qs(struct task_struct *t, u8 old, u8 new);\nvoid rcu_tasks_trace_qs_blkd(struct task_struct *t);\n\n# define rcu_tasks_trace_qs(t)\t\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\t\\\n\t\tint ___rttq_nesting = READ_ONCE((t)->trc_reader_nesting);\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\t\tif (likely(!READ_ONCE((t)->trc_reader_special.b.need_qs)) &&\t\\\n\t\t    likely(!___rttq_nesting)) {\t\t\t\t\t\\\n\t\t\trcu_trc_cmpxchg_need_qs((t), 0,\tTRC_NEED_QS_CHECKED);\t\\\n\t\t} else if (___rttq_nesting && ___rttq_nesting != INT_MIN &&\t\\\n\t\t\t   !READ_ONCE((t)->trc_reader_special.b.blocked)) {\t\\\n\t\t\trcu_tasks_trace_qs_blkd(t);\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\t} while (0)\n# else\n# define rcu_tasks_trace_qs(t) do { } while (0)\n# endif\n\n#define rcu_tasks_qs(t, preempt)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\trcu_tasks_classic_qs((t), (preempt));\t\t\t\t\\\n\trcu_tasks_trace_qs(t);\t\t\t\t\t\t\\\n} while (0)\n\n# ifdef CONFIG_TASKS_RUDE_RCU\nvoid call_rcu_tasks_rude(struct rcu_head *head, rcu_callback_t func);\nvoid synchronize_rcu_tasks_rude(void);\n# endif\n\n#define rcu_note_voluntary_context_switch(t) rcu_tasks_qs(t, false)\nvoid exit_tasks_rcu_start(void);\nvoid exit_tasks_rcu_stop(void);\nvoid exit_tasks_rcu_finish(void);\n#else  \n#define rcu_tasks_classic_qs(t, preempt) do { } while (0)\n#define rcu_tasks_qs(t, preempt) do { } while (0)\n#define rcu_note_voluntary_context_switch(t) do { } while (0)\n#define call_rcu_tasks call_rcu\n#define synchronize_rcu_tasks synchronize_rcu\nstatic inline void exit_tasks_rcu_start(void) { }\nstatic inline void exit_tasks_rcu_stop(void) { }\nstatic inline void exit_tasks_rcu_finish(void) { }\n#endif  \n\n \nstatic inline bool rcu_trace_implies_rcu_gp(void) { return true; }\n\n \n#define cond_resched_tasks_rcu_qs() \\\ndo { \\\n\trcu_tasks_qs(current, false); \\\n\tcond_resched(); \\\n} while (0)\n\n \n\n#if defined(CONFIG_TREE_RCU)\n#include <linux/rcutree.h>\n#elif defined(CONFIG_TINY_RCU)\n#include <linux/rcutiny.h>\n#else\n#error \"Unknown RCU implementation specified to kernel configuration\"\n#endif\n\n \n#ifdef CONFIG_DEBUG_OBJECTS_RCU_HEAD\nvoid init_rcu_head(struct rcu_head *head);\nvoid destroy_rcu_head(struct rcu_head *head);\nvoid init_rcu_head_on_stack(struct rcu_head *head);\nvoid destroy_rcu_head_on_stack(struct rcu_head *head);\n#else  \nstatic inline void init_rcu_head(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head(struct rcu_head *head) { }\nstatic inline void init_rcu_head_on_stack(struct rcu_head *head) { }\nstatic inline void destroy_rcu_head_on_stack(struct rcu_head *head) { }\n#endif\t \n\n#if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_PROVE_RCU)\nbool rcu_lockdep_current_cpu_online(void);\n#else  \nstatic inline bool rcu_lockdep_current_cpu_online(void) { return true; }\n#endif  \n\nextern struct lockdep_map rcu_lock_map;\nextern struct lockdep_map rcu_bh_lock_map;\nextern struct lockdep_map rcu_sched_lock_map;\nextern struct lockdep_map rcu_callback_map;\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nstatic inline void rcu_lock_acquire(struct lockdep_map *map)\n{\n\tlock_acquire(map, 0, 0, 2, 0, NULL, _THIS_IP_);\n}\n\nstatic inline void rcu_try_lock_acquire(struct lockdep_map *map)\n{\n\tlock_acquire(map, 0, 1, 2, 0, NULL, _THIS_IP_);\n}\n\nstatic inline void rcu_lock_release(struct lockdep_map *map)\n{\n\tlock_release(map, _THIS_IP_);\n}\n\nint debug_lockdep_rcu_enabled(void);\nint rcu_read_lock_held(void);\nint rcu_read_lock_bh_held(void);\nint rcu_read_lock_sched_held(void);\nint rcu_read_lock_any_held(void);\n\n#else  \n\n# define rcu_lock_acquire(a)\t\tdo { } while (0)\n# define rcu_try_lock_acquire(a)\tdo { } while (0)\n# define rcu_lock_release(a)\t\tdo { } while (0)\n\nstatic inline int rcu_read_lock_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_bh_held(void)\n{\n\treturn 1;\n}\n\nstatic inline int rcu_read_lock_sched_held(void)\n{\n\treturn !preemptible();\n}\n\nstatic inline int rcu_read_lock_any_held(void)\n{\n\treturn !preemptible();\n}\n\nstatic inline int debug_lockdep_rcu_enabled(void)\n{\n\treturn 0;\n}\n\n#endif  \n\n#ifdef CONFIG_PROVE_RCU\n\n \n#define RCU_LOCKDEP_WARN(c, s)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstatic bool __section(\".data.unlikely\") __warned;\t\\\n\t\tif (debug_lockdep_rcu_enabled() && (c) &&\t\t\\\n\t\t    debug_lockdep_rcu_enabled() && !__warned) {\t\t\\\n\t\t\t__warned = true;\t\t\t\t\\\n\t\t\tlockdep_rcu_suspicious(__FILE__, __LINE__, s);\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t} while (0)\n\n#if defined(CONFIG_PROVE_RCU) && !defined(CONFIG_PREEMPT_RCU)\nstatic inline void rcu_preempt_sleep_check(void)\n{\n\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_lock_map),\n\t\t\t \"Illegal context switch in RCU read-side critical section\");\n}\n#else  \nstatic inline void rcu_preempt_sleep_check(void) { }\n#endif  \n\n#define rcu_sleep_check()\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\trcu_preempt_sleep_check();\t\t\t\t\\\n\t\tif (!IS_ENABLED(CONFIG_PREEMPT_RT))\t\t\t\\\n\t\t    RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-bh read-side critical section\"); \\\n\t\tRCU_LOCKDEP_WARN(lock_is_held(&rcu_sched_lock_map),\t\\\n\t\t\t\t \"Illegal context switch in RCU-sched read-side critical section\"); \\\n\t} while (0)\n\n#else  \n\n#define RCU_LOCKDEP_WARN(c, s) do { } while (0 && (c))\n#define rcu_sleep_check() do { } while (0)\n\n#endif  \n\n \n\n#ifdef __CHECKER__\n#define rcu_check_sparse(p, space) \\\n\t((void)(((typeof(*p) space *)p) == p))\n#else  \n#define rcu_check_sparse(p, space)\n#endif  \n\n#define __unrcu_pointer(p, local)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(*p) *local = (typeof(*p) *__force)(p);\t\t\t\\\n\trcu_check_sparse(p, __rcu);\t\t\t\t\t\\\n\t((typeof(*p) __force __kernel *)(local)); \t\t\t\\\n})\n \n#define unrcu_pointer(p) __unrcu_pointer(p, __UNIQUE_ID(rcu))\n\n#define __rcu_access_pointer(p, local, space) \\\n({ \\\n\ttypeof(*p) *local = (typeof(*p) *__force)READ_ONCE(p); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(local)); \\\n})\n#define __rcu_dereference_check(p, local, c, space) \\\n({ \\\n\t  \\\n\ttypeof(*p) *local = (typeof(*p) *__force)READ_ONCE(p); \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_check() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(local)); \\\n})\n#define __rcu_dereference_protected(p, local, c, space) \\\n({ \\\n\tRCU_LOCKDEP_WARN(!(c), \"suspicious rcu_dereference_protected() usage\"); \\\n\trcu_check_sparse(p, space); \\\n\t((typeof(*p) __force __kernel *)(p)); \\\n})\n#define __rcu_dereference_raw(p, local) \\\n({ \\\n\t  \\\n\ttypeof(p) local = READ_ONCE(p); \\\n\t((typeof(*p) __force __kernel *)(local)); \\\n})\n#define rcu_dereference_raw(p) __rcu_dereference_raw(p, __UNIQUE_ID(rcu))\n\n \n#define RCU_INITIALIZER(v) (typeof(*(v)) __force __rcu *)(v)\n\n \n#define rcu_assign_pointer(p, v)\t\t\t\t\t      \\\ndo {\t\t\t\t\t\t\t\t\t      \\\n\tuintptr_t _r_a_p__v = (uintptr_t)(v);\t\t\t\t      \\\n\trcu_check_sparse(p, __rcu);\t\t\t\t\t      \\\n\t\t\t\t\t\t\t\t\t      \\\n\tif (__builtin_constant_p(v) && (_r_a_p__v) == (uintptr_t)NULL)\t      \\\n\t\tWRITE_ONCE((p), (typeof(p))(_r_a_p__v));\t\t      \\\n\telse\t\t\t\t\t\t\t\t      \\\n\t\tsmp_store_release(&p, RCU_INITIALIZER((typeof(p))_r_a_p__v)); \\\n} while (0)\n\n \n#define rcu_replace_pointer(rcu_ptr, ptr, c)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) __tmp = rcu_dereference_protected((rcu_ptr), (c));\t\\\n\trcu_assign_pointer((rcu_ptr), (ptr));\t\t\t\t\\\n\t__tmp;\t\t\t\t\t\t\t\t\\\n})\n\n \n#define rcu_access_pointer(p) __rcu_access_pointer((p), __UNIQUE_ID(rcu), __rcu)\n\n \n#define rcu_dereference_check(p, c) \\\n\t__rcu_dereference_check((p), __UNIQUE_ID(rcu), \\\n\t\t\t\t(c) || rcu_read_lock_held(), __rcu)\n\n \n#define rcu_dereference_bh_check(p, c) \\\n\t__rcu_dereference_check((p), __UNIQUE_ID(rcu), \\\n\t\t\t\t(c) || rcu_read_lock_bh_held(), __rcu)\n\n \n#define rcu_dereference_sched_check(p, c) \\\n\t__rcu_dereference_check((p), __UNIQUE_ID(rcu), \\\n\t\t\t\t(c) || rcu_read_lock_sched_held(), \\\n\t\t\t\t__rcu)\n\n \n#define rcu_dereference_raw_check(p) \\\n\t__rcu_dereference_check((p), __UNIQUE_ID(rcu), 1, __rcu)\n\n \n#define rcu_dereference_protected(p, c) \\\n\t__rcu_dereference_protected((p), __UNIQUE_ID(rcu), (c), __rcu)\n\n\n \n#define rcu_dereference(p) rcu_dereference_check(p, 0)\n\n \n#define rcu_dereference_bh(p) rcu_dereference_bh_check(p, 0)\n\n \n#define rcu_dereference_sched(p) rcu_dereference_sched_check(p, 0)\n\n \n#define rcu_pointer_handoff(p) (p)\n\n \nstatic __always_inline void rcu_read_lock(void)\n{\n\t__rcu_read_lock();\n\t__acquire(RCU);\n\trcu_lock_acquire(&rcu_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock() used illegally while idle\");\n}\n\n \n\n \nstatic inline void rcu_read_unlock(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock() used illegally while idle\");\n\t__release(RCU);\n\t__rcu_read_unlock();\n\trcu_lock_release(&rcu_lock_map);  \n}\n\n \nstatic inline void rcu_read_lock_bh(void)\n{\n\tlocal_bh_disable();\n\t__acquire(RCU_BH);\n\trcu_lock_acquire(&rcu_bh_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_bh() used illegally while idle\");\n}\n\n \nstatic inline void rcu_read_unlock_bh(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_bh() used illegally while idle\");\n\trcu_lock_release(&rcu_bh_lock_map);\n\t__release(RCU_BH);\n\tlocal_bh_enable();\n}\n\n \nstatic inline void rcu_read_lock_sched(void)\n{\n\tpreempt_disable();\n\t__acquire(RCU_SCHED);\n\trcu_lock_acquire(&rcu_sched_lock_map);\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_lock_sched() used illegally while idle\");\n}\n\n \nstatic inline notrace void rcu_read_lock_sched_notrace(void)\n{\n\tpreempt_disable_notrace();\n\t__acquire(RCU_SCHED);\n}\n\n \nstatic inline void rcu_read_unlock_sched(void)\n{\n\tRCU_LOCKDEP_WARN(!rcu_is_watching(),\n\t\t\t \"rcu_read_unlock_sched() used illegally while idle\");\n\trcu_lock_release(&rcu_sched_lock_map);\n\t__release(RCU_SCHED);\n\tpreempt_enable();\n}\n\n \nstatic inline notrace void rcu_read_unlock_sched_notrace(void)\n{\n\t__release(RCU_SCHED);\n\tpreempt_enable_notrace();\n}\n\n \n#define RCU_INIT_POINTER(p, v) \\\n\tdo { \\\n\t\trcu_check_sparse(p, __rcu); \\\n\t\tWRITE_ONCE(p, RCU_INITIALIZER(v)); \\\n\t} while (0)\n\n \n#define RCU_POINTER_INITIALIZER(p, v) \\\n\t\t.p = RCU_INITIALIZER(v)\n\n \n#define __is_kvfree_rcu_offset(offset) ((offset) < 4096)\n\n \n#define kfree_rcu(ptr, rhf) kvfree_rcu_arg_2(ptr, rhf)\n#define kvfree_rcu(ptr, rhf) kvfree_rcu_arg_2(ptr, rhf)\n\n \n#define kfree_rcu_mightsleep(ptr) kvfree_rcu_arg_1(ptr)\n#define kvfree_rcu_mightsleep(ptr) kvfree_rcu_arg_1(ptr)\n\n#define kvfree_rcu_arg_2(ptr, rhf)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypeof (ptr) ___p = (ptr);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (___p) {\t\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(!__is_kvfree_rcu_offset(offsetof(typeof(*(ptr)), rhf)));\t\\\n\t\tkvfree_call_rcu(&((___p)->rhf), (void *) (___p));\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define kvfree_rcu_arg_1(ptr)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttypeof(ptr) ___p = (ptr);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (___p)\t\t\t\t\t\t\\\n\t\tkvfree_call_rcu(NULL, (void *) (___p));\t\t\\\n} while (0)\n\n \n#ifdef CONFIG_ARCH_WEAK_RELEASE_ACQUIRE\n#define smp_mb__after_unlock_lock()\tsmp_mb()   \n#else  \n#define smp_mb__after_unlock_lock()\tdo { } while (0)\n#endif  \n\n\n \n\n \nstatic inline void rcu_head_init(struct rcu_head *rhp)\n{\n\trhp->func = (rcu_callback_t)~0L;\n}\n\n \nstatic inline bool\nrcu_head_after_call_rcu(struct rcu_head *rhp, rcu_callback_t f)\n{\n\trcu_callback_t func = READ_ONCE(rhp->func);\n\n\tif (func == f)\n\t\treturn true;\n\tWARN_ON_ONCE(func != (rcu_callback_t)~0L);\n\treturn false;\n}\n\n \nextern int rcu_expedited;\nextern int rcu_normal;\n\nDEFINE_LOCK_GUARD_0(rcu, rcu_read_lock(), rcu_read_unlock())\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}