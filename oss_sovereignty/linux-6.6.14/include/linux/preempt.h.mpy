{
  "module_name": "preempt.h",
  "hash_id": "756788715f508e357b5a7ec65c2222d3b4674b882c6098d38f56e8cefed9a79f",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/preempt.h",
  "human_readable_source": " \n#ifndef __LINUX_PREEMPT_H\n#define __LINUX_PREEMPT_H\n\n \n\n#include <linux/linkage.h>\n#include <linux/cleanup.h>\n#include <linux/list.h>\n\n \n#define PREEMPT_BITS\t8\n#define SOFTIRQ_BITS\t8\n#define HARDIRQ_BITS\t4\n#define NMI_BITS\t4\n\n#define PREEMPT_SHIFT\t0\n#define SOFTIRQ_SHIFT\t(PREEMPT_SHIFT + PREEMPT_BITS)\n#define HARDIRQ_SHIFT\t(SOFTIRQ_SHIFT + SOFTIRQ_BITS)\n#define NMI_SHIFT\t(HARDIRQ_SHIFT + HARDIRQ_BITS)\n\n#define __IRQ_MASK(x)\t((1UL << (x))-1)\n\n#define PREEMPT_MASK\t(__IRQ_MASK(PREEMPT_BITS) << PREEMPT_SHIFT)\n#define SOFTIRQ_MASK\t(__IRQ_MASK(SOFTIRQ_BITS) << SOFTIRQ_SHIFT)\n#define HARDIRQ_MASK\t(__IRQ_MASK(HARDIRQ_BITS) << HARDIRQ_SHIFT)\n#define NMI_MASK\t(__IRQ_MASK(NMI_BITS)     << NMI_SHIFT)\n\n#define PREEMPT_OFFSET\t(1UL << PREEMPT_SHIFT)\n#define SOFTIRQ_OFFSET\t(1UL << SOFTIRQ_SHIFT)\n#define HARDIRQ_OFFSET\t(1UL << HARDIRQ_SHIFT)\n#define NMI_OFFSET\t(1UL << NMI_SHIFT)\n\n#define SOFTIRQ_DISABLE_OFFSET\t(2 * SOFTIRQ_OFFSET)\n\n#define PREEMPT_DISABLED\t(PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)\n\n \n#define INIT_PREEMPT_COUNT\tPREEMPT_OFFSET\n\n \n#define FORK_PREEMPT_COUNT\t(2*PREEMPT_DISABLE_OFFSET + PREEMPT_ENABLED)\n\n \n#include <asm/preempt.h>\n\n \nstatic __always_inline unsigned char interrupt_context_level(void)\n{\n\tunsigned long pc = preempt_count();\n\tunsigned char level = 0;\n\n\tlevel += !!(pc & (NMI_MASK));\n\tlevel += !!(pc & (NMI_MASK | HARDIRQ_MASK));\n\tlevel += !!(pc & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET));\n\n\treturn level;\n}\n\n \n\n#define nmi_count()\t(preempt_count() & NMI_MASK)\n#define hardirq_count()\t(preempt_count() & HARDIRQ_MASK)\n#ifdef CONFIG_PREEMPT_RT\n# define softirq_count()\t(current->softirq_disable_cnt & SOFTIRQ_MASK)\n# define irq_count()\t\t((preempt_count() & (NMI_MASK | HARDIRQ_MASK)) | softirq_count())\n#else\n# define softirq_count()\t(preempt_count() & SOFTIRQ_MASK)\n# define irq_count()\t\t(preempt_count() & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_MASK))\n#endif\n\n \n#define in_nmi()\t\t(nmi_count())\n#define in_hardirq()\t\t(hardirq_count())\n#define in_serving_softirq()\t(softirq_count() & SOFTIRQ_OFFSET)\n#ifdef CONFIG_PREEMPT_RT\n# define in_task()\t\t(!((preempt_count() & (NMI_MASK | HARDIRQ_MASK)) | in_serving_softirq()))\n#else\n# define in_task()\t\t(!(preempt_count() & (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET)))\n#endif\n\n \n#define in_irq()\t\t(hardirq_count())\n#define in_softirq()\t\t(softirq_count())\n#define in_interrupt()\t\t(irq_count())\n\n \n#if defined(CONFIG_PREEMPT_COUNT)\n# define PREEMPT_DISABLE_OFFSET\tPREEMPT_OFFSET\n#else\n# define PREEMPT_DISABLE_OFFSET\t0\n#endif\n\n \n#if !defined(CONFIG_PREEMPT_RT)\n#define PREEMPT_LOCK_OFFSET\t\tPREEMPT_DISABLE_OFFSET\n#else\n \n#define PREEMPT_LOCK_OFFSET\t\t0\n#endif\n\n \n#define SOFTIRQ_LOCK_OFFSET (SOFTIRQ_DISABLE_OFFSET + PREEMPT_LOCK_OFFSET)\n\n \n#define in_atomic()\t(preempt_count() != 0)\n\n \n#define in_atomic_preempt_off() (preempt_count() != PREEMPT_DISABLE_OFFSET)\n\n#if defined(CONFIG_DEBUG_PREEMPT) || defined(CONFIG_TRACE_PREEMPT_TOGGLE)\nextern void preempt_count_add(int val);\nextern void preempt_count_sub(int val);\n#define preempt_count_dec_and_test() \\\n\t({ preempt_count_sub(1); should_resched(0); })\n#else\n#define preempt_count_add(val)\t__preempt_count_add(val)\n#define preempt_count_sub(val)\t__preempt_count_sub(val)\n#define preempt_count_dec_and_test() __preempt_count_dec_and_test()\n#endif\n\n#define __preempt_count_inc() __preempt_count_add(1)\n#define __preempt_count_dec() __preempt_count_sub(1)\n\n#define preempt_count_inc() preempt_count_add(1)\n#define preempt_count_dec() preempt_count_sub(1)\n\n#ifdef CONFIG_PREEMPT_COUNT\n\n#define preempt_disable() \\\ndo { \\\n\tpreempt_count_inc(); \\\n\tbarrier(); \\\n} while (0)\n\n#define sched_preempt_enable_no_resched() \\\ndo { \\\n\tbarrier(); \\\n\tpreempt_count_dec(); \\\n} while (0)\n\n#define preempt_enable_no_resched() sched_preempt_enable_no_resched()\n\n#define preemptible()\t(preempt_count() == 0 && !irqs_disabled())\n\n#ifdef CONFIG_PREEMPTION\n#define preempt_enable() \\\ndo { \\\n\tbarrier(); \\\n\tif (unlikely(preempt_count_dec_and_test())) \\\n\t\t__preempt_schedule(); \\\n} while (0)\n\n#define preempt_enable_notrace() \\\ndo { \\\n\tbarrier(); \\\n\tif (unlikely(__preempt_count_dec_and_test())) \\\n\t\t__preempt_schedule_notrace(); \\\n} while (0)\n\n#define preempt_check_resched() \\\ndo { \\\n\tif (should_resched(0)) \\\n\t\t__preempt_schedule(); \\\n} while (0)\n\n#else  \n#define preempt_enable() \\\ndo { \\\n\tbarrier(); \\\n\tpreempt_count_dec(); \\\n} while (0)\n\n#define preempt_enable_notrace() \\\ndo { \\\n\tbarrier(); \\\n\t__preempt_count_dec(); \\\n} while (0)\n\n#define preempt_check_resched() do { } while (0)\n#endif  \n\n#define preempt_disable_notrace() \\\ndo { \\\n\t__preempt_count_inc(); \\\n\tbarrier(); \\\n} while (0)\n\n#define preempt_enable_no_resched_notrace() \\\ndo { \\\n\tbarrier(); \\\n\t__preempt_count_dec(); \\\n} while (0)\n\n#else  \n\n \n#define preempt_disable()\t\t\tbarrier()\n#define sched_preempt_enable_no_resched()\tbarrier()\n#define preempt_enable_no_resched()\t\tbarrier()\n#define preempt_enable()\t\t\tbarrier()\n#define preempt_check_resched()\t\t\tdo { } while (0)\n\n#define preempt_disable_notrace()\t\tbarrier()\n#define preempt_enable_no_resched_notrace()\tbarrier()\n#define preempt_enable_notrace()\t\tbarrier()\n#define preemptible()\t\t\t\t0\n\n#endif  \n\n#ifdef MODULE\n \n#undef sched_preempt_enable_no_resched\n#undef preempt_enable_no_resched\n#undef preempt_enable_no_resched_notrace\n#undef preempt_check_resched\n#endif\n\n#define preempt_set_need_resched() \\\ndo { \\\n\tset_preempt_need_resched(); \\\n} while (0)\n#define preempt_fold_need_resched() \\\ndo { \\\n\tif (tif_need_resched()) \\\n\t\tset_preempt_need_resched(); \\\n} while (0)\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\nstruct preempt_notifier;\n\n \nstruct preempt_ops {\n\tvoid (*sched_in)(struct preempt_notifier *notifier, int cpu);\n\tvoid (*sched_out)(struct preempt_notifier *notifier,\n\t\t\t  struct task_struct *next);\n};\n\n \nstruct preempt_notifier {\n\tstruct hlist_node link;\n\tstruct preempt_ops *ops;\n};\n\nvoid preempt_notifier_inc(void);\nvoid preempt_notifier_dec(void);\nvoid preempt_notifier_register(struct preempt_notifier *notifier);\nvoid preempt_notifier_unregister(struct preempt_notifier *notifier);\n\nstatic inline void preempt_notifier_init(struct preempt_notifier *notifier,\n\t\t\t\t     struct preempt_ops *ops)\n{\n\tINIT_HLIST_NODE(&notifier->link);\n\tnotifier->ops = ops;\n}\n\n#endif\n\n#ifdef CONFIG_SMP\n\n \nextern void migrate_disable(void);\nextern void migrate_enable(void);\n\n#else\n\nstatic inline void migrate_disable(void) { }\nstatic inline void migrate_enable(void) { }\n\n#endif  \n\n \n \n#define preempt_disable_nested()\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\t\t\t\\\n\t\tpreempt_disable();\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\\\n\t\tlockdep_assert_preemption_disabled();\t\t\\\n} while (0)\n\n \nstatic __always_inline void preempt_enable_nested(void)\n{\n\tif (IS_ENABLED(CONFIG_PREEMPT_RT))\n\t\tpreempt_enable();\n}\n\nDEFINE_LOCK_GUARD_0(preempt, preempt_disable(), preempt_enable())\nDEFINE_LOCK_GUARD_0(preempt_notrace, preempt_disable_notrace(), preempt_enable_notrace())\nDEFINE_LOCK_GUARD_0(migrate, migrate_disable(), migrate_enable())\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}