{
  "module_name": "blkdev.h",
  "hash_id": "ee6272141ffd561fa964702e2c704f7d606fc594bc24f072e47efc56f166da58",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/blkdev.h",
  "human_readable_source": " \n \n#ifndef _LINUX_BLKDEV_H\n#define _LINUX_BLKDEV_H\n\n#include <linux/types.h>\n#include <linux/blk_types.h>\n#include <linux/device.h>\n#include <linux/list.h>\n#include <linux/llist.h>\n#include <linux/minmax.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/wait.h>\n#include <linux/bio.h>\n#include <linux/gfp.h>\n#include <linux/kdev_t.h>\n#include <linux/rcupdate.h>\n#include <linux/percpu-refcount.h>\n#include <linux/blkzoned.h>\n#include <linux/sched.h>\n#include <linux/sbitmap.h>\n#include <linux/uuid.h>\n#include <linux/xarray.h>\n\nstruct module;\nstruct request_queue;\nstruct elevator_queue;\nstruct blk_trace;\nstruct request;\nstruct sg_io_hdr;\nstruct blkcg_gq;\nstruct blk_flush_queue;\nstruct kiocb;\nstruct pr_ops;\nstruct rq_qos;\nstruct blk_queue_stats;\nstruct blk_stat_callback;\nstruct blk_crypto_profile;\n\nextern const struct device_type disk_type;\nextern const struct device_type part_type;\nextern struct class block_class;\n\n \n#define BLKCG_MAX_POLS\t\t6\n\n#define DISK_MAX_PARTS\t\t\t256\n#define DISK_NAME_LEN\t\t\t32\n\n#define PARTITION_META_INFO_VOLNAMELTH\t64\n \n#define PARTITION_META_INFO_UUIDLTH\t(UUID_STRING_LEN + 1)\n\nstruct partition_meta_info {\n\tchar uuid[PARTITION_META_INFO_UUIDLTH];\n\tu8 volname[PARTITION_META_INFO_VOLNAMELTH];\n};\n\n \nenum {\n\tGENHD_FL_REMOVABLE\t\t\t= 1 << 0,\n\tGENHD_FL_HIDDEN\t\t\t\t= 1 << 1,\n\tGENHD_FL_NO_PART\t\t\t= 1 << 2,\n};\n\nenum {\n\tDISK_EVENT_MEDIA_CHANGE\t\t\t= 1 << 0,  \n\tDISK_EVENT_EJECT_REQUEST\t\t= 1 << 1,  \n};\n\nenum {\n\t \n\tDISK_EVENT_FLAG_POLL\t\t\t= 1 << 0,\n\t \n\tDISK_EVENT_FLAG_UEVENT\t\t\t= 1 << 1,\n\t \n\tDISK_EVENT_FLAG_BLOCK_ON_EXCL_WRITE\t= 1 << 2,\n};\n\nstruct disk_events;\nstruct badblocks;\n\nstruct blk_integrity {\n\tconst struct blk_integrity_profile\t*profile;\n\tunsigned char\t\t\t\tflags;\n\tunsigned char\t\t\t\ttuple_size;\n\tunsigned char\t\t\t\tinterval_exp;\n\tunsigned char\t\t\t\ttag_size;\n};\n\ntypedef unsigned int __bitwise blk_mode_t;\n\n \n#define BLK_OPEN_READ\t\t((__force blk_mode_t)(1 << 0))\n \n#define BLK_OPEN_WRITE\t\t((__force blk_mode_t)(1 << 1))\n \n#define BLK_OPEN_EXCL\t\t((__force blk_mode_t)(1 << 2))\n \n#define BLK_OPEN_NDELAY\t\t((__force blk_mode_t)(1 << 3))\n \n#define BLK_OPEN_WRITE_IOCTL\t((__force blk_mode_t)(1 << 4))\n\nstruct gendisk {\n\t \n\tint major;\n\tint first_minor;\n\tint minors;\n\n\tchar disk_name[DISK_NAME_LEN];\t \n\n\tunsigned short events;\t\t \n\tunsigned short event_flags;\t \n\n\tstruct xarray part_tbl;\n\tstruct block_device *part0;\n\n\tconst struct block_device_operations *fops;\n\tstruct request_queue *queue;\n\tvoid *private_data;\n\n\tstruct bio_set bio_split;\n\n\tint flags;\n\tunsigned long state;\n#define GD_NEED_PART_SCAN\t\t0\n#define GD_READ_ONLY\t\t\t1\n#define GD_DEAD\t\t\t\t2\n#define GD_NATIVE_CAPACITY\t\t3\n#define GD_ADDED\t\t\t4\n#define GD_SUPPRESS_PART_SCAN\t\t5\n#define GD_OWNS_QUEUE\t\t\t6\n\n\tstruct mutex open_mutex;\t \n\tunsigned open_partitions;\t \n\n\tstruct backing_dev_info\t*bdi;\n\tstruct kobject queue_kobj;\t \n\tstruct kobject *slave_dir;\n#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED\n\tstruct list_head slave_bdevs;\n#endif\n\tstruct timer_rand_state *random;\n\tatomic_t sync_io;\t\t \n\tstruct disk_events *ev;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\t \n\tunsigned int\t\tnr_zones;\n\tunsigned int\t\tmax_open_zones;\n\tunsigned int\t\tmax_active_zones;\n\tunsigned long\t\t*conv_zones_bitmap;\n\tunsigned long\t\t*seq_zones_wlock;\n#endif  \n\n#if IS_ENABLED(CONFIG_CDROM)\n\tstruct cdrom_device_info *cdi;\n#endif\n\tint node_id;\n\tstruct badblocks *bb;\n\tstruct lockdep_map lockdep_map;\n\tu64 diskseq;\n\tblk_mode_t open_mode;\n\n\t \n\tstruct blk_independent_access_ranges *ia_ranges;\n};\n\nstatic inline bool disk_live(struct gendisk *disk)\n{\n\treturn !inode_unhashed(disk->part0->bd_inode);\n}\n\n \nstatic inline unsigned int disk_openers(struct gendisk *disk)\n{\n\treturn atomic_read(&disk->part0->bd_openers);\n}\n\n \n#define dev_to_disk(device) \\\n\t(dev_to_bdev(device)->bd_disk)\n#define disk_to_dev(disk) \\\n\t(&((disk)->part0->bd_device))\n\n#if IS_REACHABLE(CONFIG_CDROM)\n#define disk_to_cdi(disk)\t((disk)->cdi)\n#else\n#define disk_to_cdi(disk)\tNULL\n#endif\n\nstatic inline dev_t disk_devt(struct gendisk *disk)\n{\n\treturn MKDEV(disk->major, disk->first_minor);\n}\n\nstatic inline int blk_validate_block_size(unsigned long bsize)\n{\n\tif (bsize < 512 || bsize > PAGE_SIZE || !is_power_of_2(bsize))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic inline bool blk_op_is_passthrough(blk_opf_t op)\n{\n\top &= REQ_OP_MASK;\n\treturn op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;\n}\n\n \nenum blk_zoned_model {\n\tBLK_ZONED_NONE = 0,\t \n\tBLK_ZONED_HA,\t\t \n\tBLK_ZONED_HM,\t\t \n};\n\n \nenum blk_bounce {\n\tBLK_BOUNCE_NONE,\n\tBLK_BOUNCE_HIGH,\n};\n\nstruct queue_limits {\n\tenum blk_bounce\t\tbounce;\n\tunsigned long\t\tseg_boundary_mask;\n\tunsigned long\t\tvirt_boundary_mask;\n\n\tunsigned int\t\tmax_hw_sectors;\n\tunsigned int\t\tmax_dev_sectors;\n\tunsigned int\t\tchunk_sectors;\n\tunsigned int\t\tmax_sectors;\n\tunsigned int\t\tmax_user_sectors;\n\tunsigned int\t\tmax_segment_size;\n\tunsigned int\t\tphysical_block_size;\n\tunsigned int\t\tlogical_block_size;\n\tunsigned int\t\talignment_offset;\n\tunsigned int\t\tio_min;\n\tunsigned int\t\tio_opt;\n\tunsigned int\t\tmax_discard_sectors;\n\tunsigned int\t\tmax_hw_discard_sectors;\n\tunsigned int\t\tmax_secure_erase_sectors;\n\tunsigned int\t\tmax_write_zeroes_sectors;\n\tunsigned int\t\tmax_zone_append_sectors;\n\tunsigned int\t\tdiscard_granularity;\n\tunsigned int\t\tdiscard_alignment;\n\tunsigned int\t\tzone_write_granularity;\n\n\tunsigned short\t\tmax_segments;\n\tunsigned short\t\tmax_integrity_segments;\n\tunsigned short\t\tmax_discard_segments;\n\n\tunsigned char\t\tmisaligned;\n\tunsigned char\t\tdiscard_misaligned;\n\tunsigned char\t\traid_partial_stripes_expensive;\n\tenum blk_zoned_model\tzoned;\n\n\t \n\tunsigned int\t\tdma_alignment;\n};\n\ntypedef int (*report_zones_cb)(struct blk_zone *zone, unsigned int idx,\n\t\t\t       void *data);\n\nvoid disk_set_zoned(struct gendisk *disk, enum blk_zoned_model model);\n\n#ifdef CONFIG_BLK_DEV_ZONED\n#define BLK_ALL_ZONES  ((unsigned int)-1)\nint blkdev_report_zones(struct block_device *bdev, sector_t sector,\n\t\t\tunsigned int nr_zones, report_zones_cb cb, void *data);\nunsigned int bdev_nr_zones(struct block_device *bdev);\nextern int blkdev_zone_mgmt(struct block_device *bdev, enum req_op op,\n\t\t\t    sector_t sectors, sector_t nr_sectors,\n\t\t\t    gfp_t gfp_mask);\nint blk_revalidate_disk_zones(struct gendisk *disk,\n\t\t\t      void (*update_driver_data)(struct gendisk *disk));\n#else  \nstatic inline unsigned int bdev_nr_zones(struct block_device *bdev)\n{\n\treturn 0;\n}\n#endif  \n\n \nstruct blk_independent_access_range {\n\tstruct kobject\t\tkobj;\n\tsector_t\t\tsector;\n\tsector_t\t\tnr_sectors;\n};\n\nstruct blk_independent_access_ranges {\n\tstruct kobject\t\t\t\tkobj;\n\tbool\t\t\t\t\tsysfs_registered;\n\tunsigned int\t\t\t\tnr_ia_ranges;\n\tstruct blk_independent_access_range\tia_range[];\n};\n\nstruct request_queue {\n\tstruct request\t\t*last_merge;\n\tstruct elevator_queue\t*elevator;\n\n\tstruct percpu_ref\tq_usage_counter;\n\n\tstruct blk_queue_stats\t*stats;\n\tstruct rq_qos\t\t*rq_qos;\n\tstruct mutex\t\trq_qos_mutex;\n\n\tconst struct blk_mq_ops\t*mq_ops;\n\n\t \n\tstruct blk_mq_ctx __percpu\t*queue_ctx;\n\n\tunsigned int\t\tqueue_depth;\n\n\t \n\tstruct xarray\t\thctx_table;\n\tunsigned int\t\tnr_hw_queues;\n\n\t \n\tvoid\t\t\t*queuedata;\n\n\t \n\tunsigned long\t\tqueue_flags;\n\t \n\tatomic_t\t\tpm_only;\n\n\t \n\tint\t\t\tid;\n\n\tspinlock_t\t\tqueue_lock;\n\n\tstruct gendisk\t\t*disk;\n\n\trefcount_t\t\trefs;\n\n\t \n\tstruct kobject *mq_kobj;\n\n#ifdef  CONFIG_BLK_DEV_INTEGRITY\n\tstruct blk_integrity integrity;\n#endif\t \n\n#ifdef CONFIG_PM\n\tstruct device\t\t*dev;\n\tenum rpm_status\t\trpm_status;\n#endif\n\n\t \n\tunsigned long\t\tnr_requests;\t \n\n\tunsigned int\t\tdma_pad_mask;\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\tstruct blk_crypto_profile *crypto_profile;\n\tstruct kobject *crypto_kobject;\n#endif\n\n\tunsigned int\t\trq_timeout;\n\n\tstruct timer_list\ttimeout;\n\tstruct work_struct\ttimeout_work;\n\n\tatomic_t\t\tnr_active_requests_shared_tags;\n\n\tstruct blk_mq_tags\t*sched_shared_tags;\n\n\tstruct list_head\ticq_list;\n#ifdef CONFIG_BLK_CGROUP\n\tDECLARE_BITMAP\t\t(blkcg_pols, BLKCG_MAX_POLS);\n\tstruct blkcg_gq\t\t*root_blkg;\n\tstruct list_head\tblkg_list;\n\tstruct mutex\t\tblkcg_mutex;\n#endif\n\n\tstruct queue_limits\tlimits;\n\n\tunsigned int\t\trequired_elevator_features;\n\n\tint\t\t\tnode;\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tstruct blk_trace __rcu\t*blk_trace;\n#endif\n\t \n\tstruct blk_flush_queue\t*fq;\n\tstruct list_head\tflush_list;\n\n\tstruct list_head\trequeue_list;\n\tspinlock_t\t\trequeue_lock;\n\tstruct delayed_work\trequeue_work;\n\n\tstruct mutex\t\tsysfs_lock;\n\tstruct mutex\t\tsysfs_dir_lock;\n\n\t \n\tstruct list_head\tunused_hctx_list;\n\tspinlock_t\t\tunused_hctx_lock;\n\n\tint\t\t\tmq_freeze_depth;\n\n#ifdef CONFIG_BLK_DEV_THROTTLING\n\t \n\tstruct throtl_data *td;\n#endif\n\tstruct rcu_head\t\trcu_head;\n\twait_queue_head_t\tmq_freeze_wq;\n\t \n\tstruct mutex\t\tmq_freeze_lock;\n\n\tint\t\t\tquiesce_depth;\n\n\tstruct blk_mq_tag_set\t*tag_set;\n\tstruct list_head\ttag_set_list;\n\n\tstruct dentry\t\t*debugfs_dir;\n\tstruct dentry\t\t*sched_debugfs_dir;\n\tstruct dentry\t\t*rqos_debugfs_dir;\n\t \n\tstruct mutex\t\tdebugfs_mutex;\n\n\tbool\t\t\tmq_sysfs_init_done;\n};\n\n \n#define QUEUE_FLAG_STOPPED\t0\t \n#define QUEUE_FLAG_DYING\t1\t \n#define QUEUE_FLAG_NOMERGES     3\t \n#define QUEUE_FLAG_SAME_COMP\t4\t \n#define QUEUE_FLAG_FAIL_IO\t5\t \n#define QUEUE_FLAG_NONROT\t6\t \n#define QUEUE_FLAG_VIRT\t\tQUEUE_FLAG_NONROT  \n#define QUEUE_FLAG_IO_STAT\t7\t \n#define QUEUE_FLAG_NOXMERGES\t9\t \n#define QUEUE_FLAG_ADD_RANDOM\t10\t \n#define QUEUE_FLAG_SYNCHRONOUS\t11\t \n#define QUEUE_FLAG_SAME_FORCE\t12\t \n#define QUEUE_FLAG_HW_WC\t13\t \n#define QUEUE_FLAG_INIT_DONE\t14\t \n#define QUEUE_FLAG_STABLE_WRITES 15\t \n#define QUEUE_FLAG_POLL\t\t16\t \n#define QUEUE_FLAG_WC\t\t17\t \n#define QUEUE_FLAG_FUA\t\t18\t \n#define QUEUE_FLAG_DAX\t\t19\t \n#define QUEUE_FLAG_STATS\t20\t \n#define QUEUE_FLAG_REGISTERED\t22\t \n#define QUEUE_FLAG_QUIESCED\t24\t \n#define QUEUE_FLAG_PCI_P2PDMA\t25\t \n#define QUEUE_FLAG_ZONE_RESETALL 26\t \n#define QUEUE_FLAG_RQ_ALLOC_TIME 27\t \n#define QUEUE_FLAG_HCTX_ACTIVE\t28\t \n#define QUEUE_FLAG_NOWAIT       29\t \n#define QUEUE_FLAG_SQ_SCHED     30\t \n#define QUEUE_FLAG_SKIP_TAGSET_QUIESCE\t31  \n\n#define QUEUE_FLAG_MQ_DEFAULT\t((1UL << QUEUE_FLAG_IO_STAT) |\t\t\\\n\t\t\t\t (1UL << QUEUE_FLAG_SAME_COMP) |\t\\\n\t\t\t\t (1UL << QUEUE_FLAG_NOWAIT))\n\nvoid blk_queue_flag_set(unsigned int flag, struct request_queue *q);\nvoid blk_queue_flag_clear(unsigned int flag, struct request_queue *q);\nbool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q);\n\n#define blk_queue_stopped(q)\ttest_bit(QUEUE_FLAG_STOPPED, &(q)->queue_flags)\n#define blk_queue_dying(q)\ttest_bit(QUEUE_FLAG_DYING, &(q)->queue_flags)\n#define blk_queue_init_done(q)\ttest_bit(QUEUE_FLAG_INIT_DONE, &(q)->queue_flags)\n#define blk_queue_nomerges(q)\ttest_bit(QUEUE_FLAG_NOMERGES, &(q)->queue_flags)\n#define blk_queue_noxmerges(q)\t\\\n\ttest_bit(QUEUE_FLAG_NOXMERGES, &(q)->queue_flags)\n#define blk_queue_nonrot(q)\ttest_bit(QUEUE_FLAG_NONROT, &(q)->queue_flags)\n#define blk_queue_stable_writes(q) \\\n\ttest_bit(QUEUE_FLAG_STABLE_WRITES, &(q)->queue_flags)\n#define blk_queue_io_stat(q)\ttest_bit(QUEUE_FLAG_IO_STAT, &(q)->queue_flags)\n#define blk_queue_add_random(q)\ttest_bit(QUEUE_FLAG_ADD_RANDOM, &(q)->queue_flags)\n#define blk_queue_zone_resetall(q)\t\\\n\ttest_bit(QUEUE_FLAG_ZONE_RESETALL, &(q)->queue_flags)\n#define blk_queue_dax(q)\ttest_bit(QUEUE_FLAG_DAX, &(q)->queue_flags)\n#define blk_queue_pci_p2pdma(q)\t\\\n\ttest_bit(QUEUE_FLAG_PCI_P2PDMA, &(q)->queue_flags)\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n#define blk_queue_rq_alloc_time(q)\t\\\n\ttest_bit(QUEUE_FLAG_RQ_ALLOC_TIME, &(q)->queue_flags)\n#else\n#define blk_queue_rq_alloc_time(q)\tfalse\n#endif\n\n#define blk_noretry_request(rq) \\\n\t((rq)->cmd_flags & (REQ_FAILFAST_DEV|REQ_FAILFAST_TRANSPORT| \\\n\t\t\t     REQ_FAILFAST_DRIVER))\n#define blk_queue_quiesced(q)\ttest_bit(QUEUE_FLAG_QUIESCED, &(q)->queue_flags)\n#define blk_queue_pm_only(q)\tatomic_read(&(q)->pm_only)\n#define blk_queue_registered(q)\ttest_bit(QUEUE_FLAG_REGISTERED, &(q)->queue_flags)\n#define blk_queue_sq_sched(q)\ttest_bit(QUEUE_FLAG_SQ_SCHED, &(q)->queue_flags)\n#define blk_queue_skip_tagset_quiesce(q) \\\n\ttest_bit(QUEUE_FLAG_SKIP_TAGSET_QUIESCE, &(q)->queue_flags)\n\nextern void blk_set_pm_only(struct request_queue *q);\nextern void blk_clear_pm_only(struct request_queue *q);\n\n#define list_entry_rq(ptr)\tlist_entry((ptr), struct request, queuelist)\n\n#define dma_map_bvec(dev, bv, dir, attrs) \\\n\tdma_map_page_attrs(dev, (bv)->bv_page, (bv)->bv_offset, (bv)->bv_len, \\\n\t(dir), (attrs))\n\nstatic inline bool queue_is_mq(struct request_queue *q)\n{\n\treturn q->mq_ops;\n}\n\n#ifdef CONFIG_PM\nstatic inline enum rpm_status queue_rpm_status(struct request_queue *q)\n{\n\treturn q->rpm_status;\n}\n#else\nstatic inline enum rpm_status queue_rpm_status(struct request_queue *q)\n{\n\treturn RPM_ACTIVE;\n}\n#endif\n\nstatic inline enum blk_zoned_model\nblk_queue_zoned_model(struct request_queue *q)\n{\n\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED))\n\t\treturn q->limits.zoned;\n\treturn BLK_ZONED_NONE;\n}\n\nstatic inline bool blk_queue_is_zoned(struct request_queue *q)\n{\n\tswitch (blk_queue_zoned_model(q)) {\n\tcase BLK_ZONED_HA:\n\tcase BLK_ZONED_HM:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline unsigned int disk_nr_zones(struct gendisk *disk)\n{\n\treturn blk_queue_is_zoned(disk->queue) ? disk->nr_zones : 0;\n}\n\nstatic inline unsigned int disk_zone_no(struct gendisk *disk, sector_t sector)\n{\n\tif (!blk_queue_is_zoned(disk->queue))\n\t\treturn 0;\n\treturn sector >> ilog2(disk->queue->limits.chunk_sectors);\n}\n\nstatic inline bool disk_zone_is_seq(struct gendisk *disk, sector_t sector)\n{\n\tif (!blk_queue_is_zoned(disk->queue))\n\t\treturn false;\n\tif (!disk->conv_zones_bitmap)\n\t\treturn true;\n\treturn !test_bit(disk_zone_no(disk, sector), disk->conv_zones_bitmap);\n}\n\nstatic inline void disk_set_max_open_zones(struct gendisk *disk,\n\t\tunsigned int max_open_zones)\n{\n\tdisk->max_open_zones = max_open_zones;\n}\n\nstatic inline void disk_set_max_active_zones(struct gendisk *disk,\n\t\tunsigned int max_active_zones)\n{\n\tdisk->max_active_zones = max_active_zones;\n}\n\nstatic inline unsigned int bdev_max_open_zones(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->max_open_zones;\n}\n\nstatic inline unsigned int bdev_max_active_zones(struct block_device *bdev)\n{\n\treturn bdev->bd_disk->max_active_zones;\n}\n\n#else  \nstatic inline unsigned int disk_nr_zones(struct gendisk *disk)\n{\n\treturn 0;\n}\nstatic inline bool disk_zone_is_seq(struct gendisk *disk, sector_t sector)\n{\n\treturn false;\n}\nstatic inline unsigned int disk_zone_no(struct gendisk *disk, sector_t sector)\n{\n\treturn 0;\n}\nstatic inline unsigned int bdev_max_open_zones(struct block_device *bdev)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int bdev_max_active_zones(struct block_device *bdev)\n{\n\treturn 0;\n}\n#endif  \n\nstatic inline unsigned int blk_queue_depth(struct request_queue *q)\n{\n\tif (q->queue_depth)\n\t\treturn q->queue_depth;\n\n\treturn q->nr_requests;\n}\n\n \n#define BLK_DEFAULT_SG_TIMEOUT\t(60 * HZ)\n#define BLK_MIN_SG_TIMEOUT\t(7 * HZ)\n\n \n#define for_each_bio(_bio)\t\t\\\n\tfor (; _bio; _bio = _bio->bi_next)\n\nint __must_check device_add_disk(struct device *parent, struct gendisk *disk,\n\t\t\t\t const struct attribute_group **groups);\nstatic inline int __must_check add_disk(struct gendisk *disk)\n{\n\treturn device_add_disk(NULL, disk, NULL);\n}\nvoid del_gendisk(struct gendisk *gp);\nvoid invalidate_disk(struct gendisk *disk);\nvoid set_disk_ro(struct gendisk *disk, bool read_only);\nvoid disk_uevent(struct gendisk *disk, enum kobject_action action);\n\nstatic inline int get_disk_ro(struct gendisk *disk)\n{\n\treturn disk->part0->bd_read_only ||\n\t\ttest_bit(GD_READ_ONLY, &disk->state);\n}\n\nstatic inline int bdev_read_only(struct block_device *bdev)\n{\n\treturn bdev->bd_read_only || get_disk_ro(bdev->bd_disk);\n}\n\nbool set_capacity_and_notify(struct gendisk *disk, sector_t size);\nvoid disk_force_media_change(struct gendisk *disk);\nvoid bdev_mark_dead(struct block_device *bdev, bool surprise);\n\nvoid add_disk_randomness(struct gendisk *disk) __latent_entropy;\nvoid rand_initialize_disk(struct gendisk *disk);\n\nstatic inline sector_t get_start_sect(struct block_device *bdev)\n{\n\treturn bdev->bd_start_sect;\n}\n\nstatic inline sector_t bdev_nr_sectors(struct block_device *bdev)\n{\n\treturn bdev->bd_nr_sectors;\n}\n\nstatic inline loff_t bdev_nr_bytes(struct block_device *bdev)\n{\n\treturn (loff_t)bdev_nr_sectors(bdev) << SECTOR_SHIFT;\n}\n\nstatic inline sector_t get_capacity(struct gendisk *disk)\n{\n\treturn bdev_nr_sectors(disk->part0);\n}\n\nstatic inline u64 sb_bdev_nr_blocks(struct super_block *sb)\n{\n\treturn bdev_nr_sectors(sb->s_bdev) >>\n\t\t(sb->s_blocksize_bits - SECTOR_SHIFT);\n}\n\nint bdev_disk_changed(struct gendisk *disk, bool invalidate);\n\nvoid put_disk(struct gendisk *disk);\nstruct gendisk *__blk_alloc_disk(int node, struct lock_class_key *lkclass);\n\n \n#define blk_alloc_disk(node_id)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__blk_alloc_disk(node_id, &__key);\t\t\t\t\\\n})\n\nint __register_blkdev(unsigned int major, const char *name,\n\t\tvoid (*probe)(dev_t devt));\n#define register_blkdev(major, name) \\\n\t__register_blkdev(major, name, NULL)\nvoid unregister_blkdev(unsigned int major, const char *name);\n\nbool disk_check_media_change(struct gendisk *disk);\nvoid set_capacity(struct gendisk *disk, sector_t size);\n\n#ifdef CONFIG_BLOCK_HOLDER_DEPRECATED\nint bd_link_disk_holder(struct block_device *bdev, struct gendisk *disk);\nvoid bd_unlink_disk_holder(struct block_device *bdev, struct gendisk *disk);\n#else\nstatic inline int bd_link_disk_holder(struct block_device *bdev,\n\t\t\t\t      struct gendisk *disk)\n{\n\treturn 0;\n}\nstatic inline void bd_unlink_disk_holder(struct block_device *bdev,\n\t\t\t\t\t struct gendisk *disk)\n{\n}\n#endif  \n\ndev_t part_devt(struct gendisk *disk, u8 partno);\nvoid inc_diskseq(struct gendisk *disk);\nvoid blk_request_module(dev_t devt);\n\nextern int blk_register_queue(struct gendisk *disk);\nextern void blk_unregister_queue(struct gendisk *disk);\nvoid submit_bio_noacct(struct bio *bio);\nstruct bio *bio_split_to_limits(struct bio *bio);\n\nextern int blk_lld_busy(struct request_queue *q);\nextern int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags);\nextern void blk_queue_exit(struct request_queue *q);\nextern void blk_sync_queue(struct request_queue *q);\n\n \nextern const char *blk_op_str(enum req_op op);\n\nint blk_status_to_errno(blk_status_t status);\nblk_status_t errno_to_blk_status(int errno);\nconst char *blk_status_to_str(blk_status_t status);\n\n \n#define BLK_POLL_ONESHOT\t\t(1 << 0)\nint bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags);\nint iocb_bio_iopoll(struct kiocb *kiocb, struct io_comp_batch *iob,\n\t\t\tunsigned int flags);\n\nstatic inline struct request_queue *bdev_get_queue(struct block_device *bdev)\n{\n\treturn bdev->bd_queue;\t \n}\n\n \nconst char *blk_zone_cond_str(enum blk_zone_cond zone_cond);\n\nstatic inline unsigned int bio_zone_no(struct bio *bio)\n{\n\treturn disk_zone_no(bio->bi_bdev->bd_disk, bio->bi_iter.bi_sector);\n}\n\nstatic inline unsigned int bio_zone_is_seq(struct bio *bio)\n{\n\treturn disk_zone_is_seq(bio->bi_bdev->bd_disk, bio->bi_iter.bi_sector);\n}\n\n \nstatic inline unsigned int blk_chunk_sectors_left(sector_t offset,\n\t\tunsigned int chunk_sectors)\n{\n\tif (unlikely(!is_power_of_2(chunk_sectors)))\n\t\treturn chunk_sectors - sector_div(offset, chunk_sectors);\n\treturn chunk_sectors - (offset & (chunk_sectors - 1));\n}\n\n \nvoid blk_queue_bounce_limit(struct request_queue *q, enum blk_bounce limit);\nextern void blk_queue_max_hw_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_chunk_sectors(struct request_queue *, unsigned int);\nextern void blk_queue_max_segments(struct request_queue *, unsigned short);\nextern void blk_queue_max_discard_segments(struct request_queue *,\n\t\tunsigned short);\nvoid blk_queue_max_secure_erase_sectors(struct request_queue *q,\n\t\tunsigned int max_sectors);\nextern void blk_queue_max_segment_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_discard_sectors(struct request_queue *q,\n\t\tunsigned int max_discard_sectors);\nextern void blk_queue_max_write_zeroes_sectors(struct request_queue *q,\n\t\tunsigned int max_write_same_sectors);\nextern void blk_queue_logical_block_size(struct request_queue *, unsigned int);\nextern void blk_queue_max_zone_append_sectors(struct request_queue *q,\n\t\tunsigned int max_zone_append_sectors);\nextern void blk_queue_physical_block_size(struct request_queue *, unsigned int);\nvoid blk_queue_zone_write_granularity(struct request_queue *q,\n\t\t\t\t      unsigned int size);\nextern void blk_queue_alignment_offset(struct request_queue *q,\n\t\t\t\t       unsigned int alignment);\nvoid disk_update_readahead(struct gendisk *disk);\nextern void blk_limits_io_min(struct queue_limits *limits, unsigned int min);\nextern void blk_queue_io_min(struct request_queue *q, unsigned int min);\nextern void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt);\nextern void blk_queue_io_opt(struct request_queue *q, unsigned int opt);\nextern void blk_set_queue_depth(struct request_queue *q, unsigned int depth);\nextern void blk_set_stacking_limits(struct queue_limits *lim);\nextern int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,\n\t\t\t    sector_t offset);\nextern void disk_stack_limits(struct gendisk *disk, struct block_device *bdev,\n\t\t\t      sector_t offset);\nextern void blk_queue_update_dma_pad(struct request_queue *, unsigned int);\nextern void blk_queue_segment_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_virt_boundary(struct request_queue *, unsigned long);\nextern void blk_queue_dma_alignment(struct request_queue *, int);\nextern void blk_queue_update_dma_alignment(struct request_queue *, int);\nextern void blk_queue_rq_timeout(struct request_queue *, unsigned int);\nextern void blk_queue_write_cache(struct request_queue *q, bool enabled, bool fua);\n\nstruct blk_independent_access_ranges *\ndisk_alloc_independent_access_ranges(struct gendisk *disk, int nr_ia_ranges);\nvoid disk_set_independent_access_ranges(struct gendisk *disk,\n\t\t\t\tstruct blk_independent_access_ranges *iars);\n\n \n \n#define ELEVATOR_F_ZBD_SEQ_WRITE\t(1U << 0)\n\nextern void blk_queue_required_elevator_features(struct request_queue *q,\n\t\t\t\t\t\t unsigned int features);\nextern bool blk_queue_can_use_dma_map_merging(struct request_queue *q,\n\t\t\t\t\t      struct device *dev);\n\nbool __must_check blk_get_queue(struct request_queue *);\nextern void blk_put_queue(struct request_queue *);\n\nvoid blk_mark_disk_dead(struct gendisk *disk);\n\n#ifdef CONFIG_BLOCK\n \nstruct blk_plug {\n\tstruct request *mq_list;  \n\n\t \n\tstruct request *cached_rq;\n\tunsigned short nr_ios;\n\n\tunsigned short rq_count;\n\n\tbool multiple_queues;\n\tbool has_elevator;\n\n\tstruct list_head cb_list;  \n};\n\nstruct blk_plug_cb;\ntypedef void (*blk_plug_cb_fn)(struct blk_plug_cb *, bool);\nstruct blk_plug_cb {\n\tstruct list_head list;\n\tblk_plug_cb_fn callback;\n\tvoid *data;\n};\nextern struct blk_plug_cb *blk_check_plugged(blk_plug_cb_fn unplug,\n\t\t\t\t\t     void *data, int size);\nextern void blk_start_plug(struct blk_plug *);\nextern void blk_start_plug_nr_ios(struct blk_plug *, unsigned short);\nextern void blk_finish_plug(struct blk_plug *);\n\nvoid __blk_flush_plug(struct blk_plug *plug, bool from_schedule);\nstatic inline void blk_flush_plug(struct blk_plug *plug, bool async)\n{\n\tif (plug)\n\t\t__blk_flush_plug(plug, async);\n}\n\nint blkdev_issue_flush(struct block_device *bdev);\nlong nr_blockdev_pages(void);\n#else  \nstruct blk_plug {\n};\n\nstatic inline void blk_start_plug_nr_ios(struct blk_plug *plug,\n\t\t\t\t\t unsigned short nr_ios)\n{\n}\n\nstatic inline void blk_start_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_finish_plug(struct blk_plug *plug)\n{\n}\n\nstatic inline void blk_flush_plug(struct blk_plug *plug, bool async)\n{\n}\n\nstatic inline int blkdev_issue_flush(struct block_device *bdev)\n{\n\treturn 0;\n}\n\nstatic inline long nr_blockdev_pages(void)\n{\n\treturn 0;\n}\n#endif  \n\nextern void blk_io_schedule(void);\n\nint blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask);\nint __blkdev_issue_discard(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, struct bio **biop);\nint blkdev_issue_secure_erase(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp);\n\n#define BLKDEV_ZERO_NOUNMAP\t(1 << 0)   \n#define BLKDEV_ZERO_NOFALLBACK\t(1 << 1)   \n\nextern int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, struct bio **biop,\n\t\tunsigned flags);\nextern int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector,\n\t\tsector_t nr_sects, gfp_t gfp_mask, unsigned flags);\n\nstatic inline int sb_issue_discard(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask, unsigned long flags)\n{\n\treturn blkdev_issue_discard(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits -\n\t\t\t\t\t      SECTOR_SHIFT),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits -\n\t\t\t\t\t\t  SECTOR_SHIFT),\n\t\t\t\t    gfp_mask);\n}\nstatic inline int sb_issue_zeroout(struct super_block *sb, sector_t block,\n\t\tsector_t nr_blocks, gfp_t gfp_mask)\n{\n\treturn blkdev_issue_zeroout(sb->s_bdev,\n\t\t\t\t    block << (sb->s_blocksize_bits -\n\t\t\t\t\t      SECTOR_SHIFT),\n\t\t\t\t    nr_blocks << (sb->s_blocksize_bits -\n\t\t\t\t\t\t  SECTOR_SHIFT),\n\t\t\t\t    gfp_mask, 0);\n}\n\nstatic inline bool bdev_is_partition(struct block_device *bdev)\n{\n\treturn bdev->bd_partno;\n}\n\nenum blk_default_limits {\n\tBLK_MAX_SEGMENTS\t= 128,\n\tBLK_SAFE_MAX_SECTORS\t= 255,\n\tBLK_MAX_SEGMENT_SIZE\t= 65536,\n\tBLK_SEG_BOUNDARY_MASK\t= 0xFFFFFFFFUL,\n};\n\n#define BLK_DEF_MAX_SECTORS 2560u\n\nstatic inline unsigned long queue_segment_boundary(const struct request_queue *q)\n{\n\treturn q->limits.seg_boundary_mask;\n}\n\nstatic inline unsigned long queue_virt_boundary(const struct request_queue *q)\n{\n\treturn q->limits.virt_boundary_mask;\n}\n\nstatic inline unsigned int queue_max_sectors(const struct request_queue *q)\n{\n\treturn q->limits.max_sectors;\n}\n\nstatic inline unsigned int queue_max_bytes(struct request_queue *q)\n{\n\treturn min_t(unsigned int, queue_max_sectors(q), INT_MAX >> 9) << 9;\n}\n\nstatic inline unsigned int queue_max_hw_sectors(const struct request_queue *q)\n{\n\treturn q->limits.max_hw_sectors;\n}\n\nstatic inline unsigned short queue_max_segments(const struct request_queue *q)\n{\n\treturn q->limits.max_segments;\n}\n\nstatic inline unsigned short queue_max_discard_segments(const struct request_queue *q)\n{\n\treturn q->limits.max_discard_segments;\n}\n\nstatic inline unsigned int queue_max_segment_size(const struct request_queue *q)\n{\n\treturn q->limits.max_segment_size;\n}\n\nstatic inline unsigned int queue_max_zone_append_sectors(const struct request_queue *q)\n{\n\n\tconst struct queue_limits *l = &q->limits;\n\n\treturn min(l->max_zone_append_sectors, l->max_sectors);\n}\n\nstatic inline unsigned int\nbdev_max_zone_append_sectors(struct block_device *bdev)\n{\n\treturn queue_max_zone_append_sectors(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int bdev_max_segments(struct block_device *bdev)\n{\n\treturn queue_max_segments(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned queue_logical_block_size(const struct request_queue *q)\n{\n\tint retval = 512;\n\n\tif (q && q->limits.logical_block_size)\n\t\tretval = q->limits.logical_block_size;\n\n\treturn retval;\n}\n\nstatic inline unsigned int bdev_logical_block_size(struct block_device *bdev)\n{\n\treturn queue_logical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_physical_block_size(const struct request_queue *q)\n{\n\treturn q->limits.physical_block_size;\n}\n\nstatic inline unsigned int bdev_physical_block_size(struct block_device *bdev)\n{\n\treturn queue_physical_block_size(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_min(const struct request_queue *q)\n{\n\treturn q->limits.io_min;\n}\n\nstatic inline int bdev_io_min(struct block_device *bdev)\n{\n\treturn queue_io_min(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int queue_io_opt(const struct request_queue *q)\n{\n\treturn q->limits.io_opt;\n}\n\nstatic inline int bdev_io_opt(struct block_device *bdev)\n{\n\treturn queue_io_opt(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int\nqueue_zone_write_granularity(const struct request_queue *q)\n{\n\treturn q->limits.zone_write_granularity;\n}\n\nstatic inline unsigned int\nbdev_zone_write_granularity(struct block_device *bdev)\n{\n\treturn queue_zone_write_granularity(bdev_get_queue(bdev));\n}\n\nint bdev_alignment_offset(struct block_device *bdev);\nunsigned int bdev_discard_alignment(struct block_device *bdev);\n\nstatic inline unsigned int bdev_max_discard_sectors(struct block_device *bdev)\n{\n\treturn bdev_get_queue(bdev)->limits.max_discard_sectors;\n}\n\nstatic inline unsigned int bdev_discard_granularity(struct block_device *bdev)\n{\n\treturn bdev_get_queue(bdev)->limits.discard_granularity;\n}\n\nstatic inline unsigned int\nbdev_max_secure_erase_sectors(struct block_device *bdev)\n{\n\treturn bdev_get_queue(bdev)->limits.max_secure_erase_sectors;\n}\n\nstatic inline unsigned int bdev_write_zeroes_sectors(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (q)\n\t\treturn q->limits.max_write_zeroes_sectors;\n\n\treturn 0;\n}\n\nstatic inline bool bdev_nonrot(struct block_device *bdev)\n{\n\treturn blk_queue_nonrot(bdev_get_queue(bdev));\n}\n\nstatic inline bool bdev_synchronous(struct block_device *bdev)\n{\n\treturn test_bit(QUEUE_FLAG_SYNCHRONOUS,\n\t\t\t&bdev_get_queue(bdev)->queue_flags);\n}\n\nstatic inline bool bdev_stable_writes(struct block_device *bdev)\n{\n\treturn test_bit(QUEUE_FLAG_STABLE_WRITES,\n\t\t\t&bdev_get_queue(bdev)->queue_flags);\n}\n\nstatic inline bool bdev_write_cache(struct block_device *bdev)\n{\n\treturn test_bit(QUEUE_FLAG_WC, &bdev_get_queue(bdev)->queue_flags);\n}\n\nstatic inline bool bdev_fua(struct block_device *bdev)\n{\n\treturn test_bit(QUEUE_FLAG_FUA, &bdev_get_queue(bdev)->queue_flags);\n}\n\nstatic inline bool bdev_nowait(struct block_device *bdev)\n{\n\treturn test_bit(QUEUE_FLAG_NOWAIT, &bdev_get_queue(bdev)->queue_flags);\n}\n\nstatic inline enum blk_zoned_model bdev_zoned_model(struct block_device *bdev)\n{\n\treturn blk_queue_zoned_model(bdev_get_queue(bdev));\n}\n\nstatic inline bool bdev_is_zoned(struct block_device *bdev)\n{\n\treturn blk_queue_is_zoned(bdev_get_queue(bdev));\n}\n\nstatic inline unsigned int bdev_zone_no(struct block_device *bdev, sector_t sec)\n{\n\treturn disk_zone_no(bdev->bd_disk, sec);\n}\n\n \nstatic inline bool op_needs_zoned_write_locking(enum req_op op)\n{\n\treturn op == REQ_OP_WRITE || op == REQ_OP_WRITE_ZEROES;\n}\n\nstatic inline bool bdev_op_is_zoned_write(struct block_device *bdev,\n\t\t\t\t\t  enum req_op op)\n{\n\treturn bdev_is_zoned(bdev) && op_needs_zoned_write_locking(op);\n}\n\nstatic inline sector_t bdev_zone_sectors(struct block_device *bdev)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (!blk_queue_is_zoned(q))\n\t\treturn 0;\n\treturn q->limits.chunk_sectors;\n}\n\nstatic inline sector_t bdev_offset_from_zone_start(struct block_device *bdev,\n\t\t\t\t\t\t   sector_t sector)\n{\n\treturn sector & (bdev_zone_sectors(bdev) - 1);\n}\n\nstatic inline bool bdev_is_zone_start(struct block_device *bdev,\n\t\t\t\t      sector_t sector)\n{\n\treturn bdev_offset_from_zone_start(bdev, sector) == 0;\n}\n\nstatic inline int queue_dma_alignment(const struct request_queue *q)\n{\n\treturn q ? q->limits.dma_alignment : 511;\n}\n\nstatic inline unsigned int bdev_dma_alignment(struct block_device *bdev)\n{\n\treturn queue_dma_alignment(bdev_get_queue(bdev));\n}\n\nstatic inline bool bdev_iter_is_aligned(struct block_device *bdev,\n\t\t\t\t\tstruct iov_iter *iter)\n{\n\treturn iov_iter_is_aligned(iter, bdev_dma_alignment(bdev),\n\t\t\t\t   bdev_logical_block_size(bdev) - 1);\n}\n\nstatic inline int blk_rq_aligned(struct request_queue *q, unsigned long addr,\n\t\t\t\t unsigned int len)\n{\n\tunsigned int alignment = queue_dma_alignment(q) | q->dma_pad_mask;\n\treturn !(addr & alignment) && !(len & alignment);\n}\n\n \nstatic inline unsigned int blksize_bits(unsigned int size)\n{\n\treturn order_base_2(size >> SECTOR_SHIFT) + SECTOR_SHIFT;\n}\n\nstatic inline unsigned int block_size(struct block_device *bdev)\n{\n\treturn 1 << bdev->bd_inode->i_blkbits;\n}\n\nint kblockd_schedule_work(struct work_struct *work);\nint kblockd_mod_delayed_work_on(int cpu, struct delayed_work *dwork, unsigned long delay);\n\n#define MODULE_ALIAS_BLOCKDEV(major,minor) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-\" __stringify(minor))\n#define MODULE_ALIAS_BLOCKDEV_MAJOR(major) \\\n\tMODULE_ALIAS(\"block-major-\" __stringify(major) \"-*\")\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\nbool blk_crypto_register(struct blk_crypto_profile *profile,\n\t\t\t struct request_queue *q);\n\n#else  \n\nstatic inline bool blk_crypto_register(struct blk_crypto_profile *profile,\n\t\t\t\t       struct request_queue *q)\n{\n\treturn true;\n}\n\n#endif  \n\nenum blk_unique_id {\n\t \n\tBLK_UID_T10\t= 1,\n\tBLK_UID_EUI64\t= 2,\n\tBLK_UID_NAA\t= 3,\n};\n\nstruct block_device_operations {\n\tvoid (*submit_bio)(struct bio *bio);\n\tint (*poll_bio)(struct bio *bio, struct io_comp_batch *iob,\n\t\t\tunsigned int flags);\n\tint (*open)(struct gendisk *disk, blk_mode_t mode);\n\tvoid (*release)(struct gendisk *disk);\n\tint (*ioctl)(struct block_device *bdev, blk_mode_t mode,\n\t\t\tunsigned cmd, unsigned long arg);\n\tint (*compat_ioctl)(struct block_device *bdev, blk_mode_t mode,\n\t\t\tunsigned cmd, unsigned long arg);\n\tunsigned int (*check_events) (struct gendisk *disk,\n\t\t\t\t      unsigned int clearing);\n\tvoid (*unlock_native_capacity) (struct gendisk *);\n\tint (*getgeo)(struct block_device *, struct hd_geometry *);\n\tint (*set_read_only)(struct block_device *bdev, bool ro);\n\tvoid (*free_disk)(struct gendisk *disk);\n\t \n\tvoid (*swap_slot_free_notify) (struct block_device *, unsigned long);\n\tint (*report_zones)(struct gendisk *, sector_t sector,\n\t\t\tunsigned int nr_zones, report_zones_cb cb, void *data);\n\tchar *(*devnode)(struct gendisk *disk, umode_t *mode);\n\t \n\tint (*get_unique_id)(struct gendisk *disk, u8 id[16],\n\t\t\tenum blk_unique_id id_type);\n\tstruct module *owner;\n\tconst struct pr_ops *pr_ops;\n\n\t \n\tint (*alternative_gpt_sector)(struct gendisk *disk, sector_t *sector);\n};\n\n#ifdef CONFIG_COMPAT\nextern int blkdev_compat_ptr_ioctl(struct block_device *, blk_mode_t,\n\t\t\t\t      unsigned int, unsigned long);\n#else\n#define blkdev_compat_ptr_ioctl NULL\n#endif\n\nstatic inline void blk_wake_io_task(struct task_struct *waiter)\n{\n\t \n\tif (waiter == current)\n\t\t__set_current_state(TASK_RUNNING);\n\telse\n\t\twake_up_process(waiter);\n}\n\nunsigned long bdev_start_io_acct(struct block_device *bdev, enum req_op op,\n\t\t\t\t unsigned long start_time);\nvoid bdev_end_io_acct(struct block_device *bdev, enum req_op op,\n\t\t      unsigned int sectors, unsigned long start_time);\n\nunsigned long bio_start_io_acct(struct bio *bio);\nvoid bio_end_io_acct_remapped(struct bio *bio, unsigned long start_time,\n\t\tstruct block_device *orig_bdev);\n\n \nstatic inline void bio_end_io_acct(struct bio *bio, unsigned long start_time)\n{\n\treturn bio_end_io_acct_remapped(bio, start_time, bio->bi_bdev);\n}\n\nint bdev_read_only(struct block_device *bdev);\nint set_blocksize(struct block_device *bdev, int size);\n\nint lookup_bdev(const char *pathname, dev_t *dev);\n\nvoid blkdev_show(struct seq_file *seqf, off_t offset);\n\n#define BDEVNAME_SIZE\t32\t \n#define BDEVT_SIZE\t10\t \n#ifdef CONFIG_BLOCK\n#define BLKDEV_MAJOR_MAX\t512\n#else\n#define BLKDEV_MAJOR_MAX\t0\n#endif\n\nstruct blk_holder_ops {\n\tvoid (*mark_dead)(struct block_device *bdev, bool surprise);\n\n\t \n\tvoid (*sync)(struct block_device *bdev);\n};\n\nextern const struct blk_holder_ops fs_holder_ops;\n\n \n#define sb_open_mode(flags) \\\n\t(BLK_OPEN_READ | (((flags) & SB_RDONLY) ? 0 : BLK_OPEN_WRITE))\n\nstruct block_device *blkdev_get_by_dev(dev_t dev, blk_mode_t mode, void *holder,\n\t\tconst struct blk_holder_ops *hops);\nstruct block_device *blkdev_get_by_path(const char *path, blk_mode_t mode,\n\t\tvoid *holder, const struct blk_holder_ops *hops);\nint bd_prepare_to_claim(struct block_device *bdev, void *holder,\n\t\tconst struct blk_holder_ops *hops);\nvoid bd_abort_claiming(struct block_device *bdev, void *holder);\nvoid blkdev_put(struct block_device *bdev, void *holder);\n\n \nstruct block_device *blkdev_get_no_open(dev_t dev);\nvoid blkdev_put_no_open(struct block_device *bdev);\n\nstruct block_device *I_BDEV(struct inode *inode);\n\n#ifdef CONFIG_BLOCK\nvoid invalidate_bdev(struct block_device *bdev);\nint sync_blockdev(struct block_device *bdev);\nint sync_blockdev_range(struct block_device *bdev, loff_t lstart, loff_t lend);\nint sync_blockdev_nowait(struct block_device *bdev);\nvoid sync_bdevs(bool wait);\nvoid bdev_statx_dioalign(struct inode *inode, struct kstat *stat);\nvoid printk_all_partitions(void);\nint __init early_lookup_bdev(const char *pathname, dev_t *dev);\n#else\nstatic inline void invalidate_bdev(struct block_device *bdev)\n{\n}\nstatic inline int sync_blockdev(struct block_device *bdev)\n{\n\treturn 0;\n}\nstatic inline int sync_blockdev_nowait(struct block_device *bdev)\n{\n\treturn 0;\n}\nstatic inline void sync_bdevs(bool wait)\n{\n}\nstatic inline void bdev_statx_dioalign(struct inode *inode, struct kstat *stat)\n{\n}\nstatic inline void printk_all_partitions(void)\n{\n}\nstatic inline int early_lookup_bdev(const char *pathname, dev_t *dev)\n{\n\treturn -EINVAL;\n}\n#endif  \n\nint freeze_bdev(struct block_device *bdev);\nint thaw_bdev(struct block_device *bdev);\n\nstruct io_comp_batch {\n\tstruct request *req_list;\n\tbool need_ts;\n\tvoid (*complete)(struct io_comp_batch *);\n};\n\n#define DEFINE_IO_COMP_BATCH(name)\tstruct io_comp_batch name = { }\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}