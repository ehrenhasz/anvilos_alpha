{
  "module_name": "vmalloc.h",
  "hash_id": "97ec36d27f64fce9ab292bc9cc7ebe36de4ac0d6c451ff7897c385de2faada60",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/vmalloc.h",
  "human_readable_source": " \n#ifndef _LINUX_VMALLOC_H\n#define _LINUX_VMALLOC_H\n\n#include <linux/spinlock.h>\n#include <linux/init.h>\n#include <linux/list.h>\n#include <linux/llist.h>\n#include <asm/page.h>\t\t \n#include <linux/rbtree.h>\n#include <linux/overflow.h>\n\n#include <asm/vmalloc.h>\n\nstruct vm_area_struct;\t\t \nstruct notifier_block;\t\t \nstruct iov_iter;\t\t \n\n \n#define VM_IOREMAP\t\t0x00000001\t \n#define VM_ALLOC\t\t0x00000002\t \n#define VM_MAP\t\t\t0x00000004\t \n#define VM_USERMAP\t\t0x00000008\t \n#define VM_DMA_COHERENT\t\t0x00000010\t \n#define VM_UNINITIALIZED\t0x00000020\t \n#define VM_NO_GUARD\t\t0x00000040       \n#define VM_KASAN\t\t0x00000080       \n#define VM_FLUSH_RESET_PERMS\t0x00000100\t \n#define VM_MAP_PUT_PAGES\t0x00000200\t \n#define VM_ALLOW_HUGE_VMAP\t0x00000400       \n\n#if (defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)) && \\\n\t!defined(CONFIG_KASAN_VMALLOC)\n#define VM_DEFER_KMEMLEAK\t0x00000800\t \n#else\n#define VM_DEFER_KMEMLEAK\t0\n#endif\n\n \n\n \n#ifndef IOREMAP_MAX_ORDER\n#define IOREMAP_MAX_ORDER\t(7 + PAGE_SHIFT)\t \n#endif\n\nstruct vm_struct {\n\tstruct vm_struct\t*next;\n\tvoid\t\t\t*addr;\n\tunsigned long\t\tsize;\n\tunsigned long\t\tflags;\n\tstruct page\t\t**pages;\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC\n\tunsigned int\t\tpage_order;\n#endif\n\tunsigned int\t\tnr_pages;\n\tphys_addr_t\t\tphys_addr;\n\tconst void\t\t*caller;\n};\n\nstruct vmap_area {\n\tunsigned long va_start;\n\tunsigned long va_end;\n\n\tstruct rb_node rb_node;          \n\tstruct list_head list;           \n\n\t \n\tunion {\n\t\tunsigned long subtree_max_size;  \n\t\tstruct vm_struct *vm;            \n\t};\n\tunsigned long flags;  \n};\n\n \n#ifndef arch_vmap_p4d_supported\nstatic inline bool arch_vmap_p4d_supported(pgprot_t prot)\n{\n\treturn false;\n}\n#endif\n\n#ifndef arch_vmap_pud_supported\nstatic inline bool arch_vmap_pud_supported(pgprot_t prot)\n{\n\treturn false;\n}\n#endif\n\n#ifndef arch_vmap_pmd_supported\nstatic inline bool arch_vmap_pmd_supported(pgprot_t prot)\n{\n\treturn false;\n}\n#endif\n\n#ifndef arch_vmap_pte_range_map_size\nstatic inline unsigned long arch_vmap_pte_range_map_size(unsigned long addr, unsigned long end,\n\t\t\t\t\t\t\t u64 pfn, unsigned int max_page_shift)\n{\n\treturn PAGE_SIZE;\n}\n#endif\n\n#ifndef arch_vmap_pte_supported_shift\nstatic inline int arch_vmap_pte_supported_shift(unsigned long size)\n{\n\treturn PAGE_SHIFT;\n}\n#endif\n\n#ifndef arch_vmap_pgprot_tagged\nstatic inline pgprot_t arch_vmap_pgprot_tagged(pgprot_t prot)\n{\n\treturn prot;\n}\n#endif\n\n \nextern void vm_unmap_ram(const void *mem, unsigned int count);\nextern void *vm_map_ram(struct page **pages, unsigned int count, int node);\nextern void vm_unmap_aliases(void);\n\n#ifdef CONFIG_MMU\nextern unsigned long vmalloc_nr_pages(void);\n#else\nstatic inline unsigned long vmalloc_nr_pages(void) { return 0; }\n#endif\n\nextern void *vmalloc(unsigned long size) __alloc_size(1);\nextern void *vzalloc(unsigned long size) __alloc_size(1);\nextern void *vmalloc_user(unsigned long size) __alloc_size(1);\nextern void *vmalloc_node(unsigned long size, int node) __alloc_size(1);\nextern void *vzalloc_node(unsigned long size, int node) __alloc_size(1);\nextern void *vmalloc_32(unsigned long size) __alloc_size(1);\nextern void *vmalloc_32_user(unsigned long size) __alloc_size(1);\nextern void *__vmalloc(unsigned long size, gfp_t gfp_mask) __alloc_size(1);\nextern void *__vmalloc_node_range(unsigned long size, unsigned long align,\n\t\t\tunsigned long start, unsigned long end, gfp_t gfp_mask,\n\t\t\tpgprot_t prot, unsigned long vm_flags, int node,\n\t\t\tconst void *caller) __alloc_size(1);\nvoid *__vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask,\n\t\tint node, const void *caller) __alloc_size(1);\nvoid *vmalloc_huge(unsigned long size, gfp_t gfp_mask) __alloc_size(1);\n\nextern void *__vmalloc_array(size_t n, size_t size, gfp_t flags) __alloc_size(1, 2);\nextern void *vmalloc_array(size_t n, size_t size) __alloc_size(1, 2);\nextern void *__vcalloc(size_t n, size_t size, gfp_t flags) __alloc_size(1, 2);\nextern void *vcalloc(size_t n, size_t size) __alloc_size(1, 2);\n\nextern void vfree(const void *addr);\nextern void vfree_atomic(const void *addr);\n\nextern void *vmap(struct page **pages, unsigned int count,\n\t\t\tunsigned long flags, pgprot_t prot);\nvoid *vmap_pfn(unsigned long *pfns, unsigned int count, pgprot_t prot);\nextern void vunmap(const void *addr);\n\nextern int remap_vmalloc_range_partial(struct vm_area_struct *vma,\n\t\t\t\t       unsigned long uaddr, void *kaddr,\n\t\t\t\t       unsigned long pgoff, unsigned long size);\n\nextern int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,\n\t\t\t\t\t\t\tunsigned long pgoff);\n\n \n#ifndef ARCH_PAGE_TABLE_SYNC_MASK\n#define ARCH_PAGE_TABLE_SYNC_MASK 0\n#endif\n\n \nvoid arch_sync_kernel_mappings(unsigned long start, unsigned long end);\n\n \n\nstatic inline size_t get_vm_area_size(const struct vm_struct *area)\n{\n\tif (!(area->flags & VM_NO_GUARD))\n\t\t \n\t\treturn area->size - PAGE_SIZE;\n\telse\n\t\treturn area->size;\n\n}\n\nextern struct vm_struct *get_vm_area(unsigned long size, unsigned long flags);\nextern struct vm_struct *get_vm_area_caller(unsigned long size,\n\t\t\t\t\tunsigned long flags, const void *caller);\nextern struct vm_struct *__get_vm_area_caller(unsigned long size,\n\t\t\t\t\tunsigned long flags,\n\t\t\t\t\tunsigned long start, unsigned long end,\n\t\t\t\t\tconst void *caller);\nvoid free_vm_area(struct vm_struct *area);\nextern struct vm_struct *remove_vm_area(const void *addr);\nextern struct vm_struct *find_vm_area(const void *addr);\nstruct vmap_area *find_vmap_area(unsigned long addr);\n\nstatic inline bool is_vm_area_hugepages(const void *addr)\n{\n\t \n#ifdef CONFIG_HAVE_ARCH_HUGE_VMALLOC\n\treturn find_vm_area(addr)->page_order > 0;\n#else\n\treturn false;\n#endif\n}\n\n#ifdef CONFIG_MMU\nvoid vunmap_range(unsigned long addr, unsigned long end);\nstatic inline void set_vm_flush_reset_perms(void *addr)\n{\n\tstruct vm_struct *vm = find_vm_area(addr);\n\n\tif (vm)\n\t\tvm->flags |= VM_FLUSH_RESET_PERMS;\n}\n\n#else\nstatic inline void set_vm_flush_reset_perms(void *addr)\n{\n}\n#endif\n\n \nextern long vread_iter(struct iov_iter *iter, const char *addr, size_t count);\n\n \nextern struct list_head vmap_area_list;\nextern __init void vm_area_add_early(struct vm_struct *vm);\nextern __init void vm_area_register_early(struct vm_struct *vm, size_t align);\n\n#ifdef CONFIG_SMP\n# ifdef CONFIG_MMU\nstruct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,\n\t\t\t\t     const size_t *sizes, int nr_vms,\n\t\t\t\t     size_t align);\n\nvoid pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms);\n# else\nstatic inline struct vm_struct **\npcpu_get_vm_areas(const unsigned long *offsets,\n\t\tconst size_t *sizes, int nr_vms,\n\t\tsize_t align)\n{\n\treturn NULL;\n}\n\nstatic inline void\npcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)\n{\n}\n# endif\n#endif\n\n#ifdef CONFIG_MMU\n#define VMALLOC_TOTAL (VMALLOC_END - VMALLOC_START)\n#else\n#define VMALLOC_TOTAL 0UL\n#endif\n\nint register_vmap_purge_notifier(struct notifier_block *nb);\nint unregister_vmap_purge_notifier(struct notifier_block *nb);\n\n#if defined(CONFIG_MMU) && defined(CONFIG_PRINTK)\nbool vmalloc_dump_obj(void *object);\n#else\nstatic inline bool vmalloc_dump_obj(void *object) { return false; }\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}