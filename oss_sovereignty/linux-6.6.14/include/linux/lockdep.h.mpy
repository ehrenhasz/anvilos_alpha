{
  "module_name": "lockdep.h",
  "hash_id": "401a32c115257cdd981a25f992afffdcf913770161b6fadda86fb386d8c86b0e",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/lockdep.h",
  "human_readable_source": " \n \n#ifndef __LINUX_LOCKDEP_H\n#define __LINUX_LOCKDEP_H\n\n#include <linux/lockdep_types.h>\n#include <linux/smp.h>\n#include <asm/percpu.h>\n\nstruct task_struct;\n\n#ifdef CONFIG_LOCKDEP\n\n#include <linux/linkage.h>\n#include <linux/list.h>\n#include <linux/debug_locks.h>\n#include <linux/stacktrace.h>\n\nstatic inline void lockdep_copy_map(struct lockdep_map *to,\n\t\t\t\t    struct lockdep_map *from)\n{\n\tint i;\n\n\t*to = *from;\n\t \n\tfor (i = 0; i < NR_LOCKDEP_CACHING_CLASSES; i++)\n\t\tto->class_cache[i] = NULL;\n}\n\n \nstruct lock_list {\n\tstruct list_head\t\tentry;\n\tstruct lock_class\t\t*class;\n\tstruct lock_class\t\t*links_to;\n\tconst struct lock_trace\t\t*trace;\n\tu16\t\t\t\tdistance;\n\t \n\tu8\t\t\t\tdep;\n\t \n\tu8\t\t\t\tonly_xr;\n\n\t \n\tstruct lock_list\t\t*parent;\n};\n\n \nstruct lock_chain {\n\t \n\tunsigned int\t\t\tirq_context :  2,\n\t\t\t\t\tdepth       :  6,\n\t\t\t\t\tbase\t    : 24;\n\t \n\tstruct hlist_node\t\tentry;\n\tu64\t\t\t\tchain_key;\n};\n\n#define MAX_LOCKDEP_KEYS_BITS\t\t13\n#define MAX_LOCKDEP_KEYS\t\t(1UL << MAX_LOCKDEP_KEYS_BITS)\n#define INITIAL_CHAIN_KEY\t\t-1\n\nstruct held_lock {\n\t \n\tu64\t\t\t\tprev_chain_key;\n\tunsigned long\t\t\tacquire_ip;\n\tstruct lockdep_map\t\t*instance;\n\tstruct lockdep_map\t\t*nest_lock;\n#ifdef CONFIG_LOCK_STAT\n\tu64 \t\t\t\twaittime_stamp;\n\tu64\t\t\t\tholdtime_stamp;\n#endif\n\t \n\tunsigned int\t\t\tclass_idx:MAX_LOCKDEP_KEYS_BITS;\n\t \n\tunsigned int irq_context:2;  \n\tunsigned int trylock:1;\t\t\t\t\t\t \n\n\tunsigned int read:2;         \n\tunsigned int check:1;        \n\tunsigned int hardirqs_off:1;\n\tunsigned int sync:1;\n\tunsigned int references:11;\t\t\t\t\t \n\tunsigned int pin_count;\n};\n\n \nextern void lockdep_init(void);\nextern void lockdep_reset(void);\nextern void lockdep_reset_lock(struct lockdep_map *lock);\nextern void lockdep_free_key_range(void *start, unsigned long size);\nextern asmlinkage void lockdep_sys_exit(void);\nextern void lockdep_set_selftest_task(struct task_struct *task);\n\nextern void lockdep_init_task(struct task_struct *task);\n\n \n#define LOCKDEP_RECURSION_BITS\t16\n#define LOCKDEP_OFF\t\t(1U << LOCKDEP_RECURSION_BITS)\n#define LOCKDEP_RECURSION_MASK\t(LOCKDEP_OFF - 1)\n\n \n\n#define lockdep_off()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tcurrent->lockdep_recursion += LOCKDEP_OFF;\t\\\n} while (0)\n\n#define lockdep_on()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tcurrent->lockdep_recursion -= LOCKDEP_OFF;\t\\\n} while (0)\n\nextern void lockdep_register_key(struct lock_class_key *key);\nextern void lockdep_unregister_key(struct lock_class_key *key);\n\n \n\nextern void lockdep_init_map_type(struct lockdep_map *lock, const char *name,\n\tstruct lock_class_key *key, int subclass, u8 inner, u8 outer, u8 lock_type);\n\nstatic inline void\nlockdep_init_map_waits(struct lockdep_map *lock, const char *name,\n\t\t       struct lock_class_key *key, int subclass, u8 inner, u8 outer)\n{\n\tlockdep_init_map_type(lock, name, key, subclass, inner, outer, LD_LOCK_NORMAL);\n}\n\nstatic inline void\nlockdep_init_map_wait(struct lockdep_map *lock, const char *name,\n\t\t      struct lock_class_key *key, int subclass, u8 inner)\n{\n\tlockdep_init_map_waits(lock, name, key, subclass, inner, LD_WAIT_INV);\n}\n\nstatic inline void lockdep_init_map(struct lockdep_map *lock, const char *name,\n\t\t\t     struct lock_class_key *key, int subclass)\n{\n\tlockdep_init_map_wait(lock, name, key, subclass, LD_WAIT_INV);\n}\n\n \n#define lockdep_set_class(lock, key)\t\t\t\t\\\n\tlockdep_init_map_type(&(lock)->dep_map, #key, key, 0,\t\\\n\t\t\t      (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t      (lock)->dep_map.wait_type_outer,\t\\\n\t\t\t      (lock)->dep_map.lock_type)\n\n#define lockdep_set_class_and_name(lock, key, name)\t\t\\\n\tlockdep_init_map_type(&(lock)->dep_map, name, key, 0,\t\\\n\t\t\t      (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t      (lock)->dep_map.wait_type_outer,\t\\\n\t\t\t      (lock)->dep_map.lock_type)\n\n#define lockdep_set_class_and_subclass(lock, key, sub)\t\t\\\n\tlockdep_init_map_type(&(lock)->dep_map, #key, key, sub,\t\\\n\t\t\t      (lock)->dep_map.wait_type_inner,\t\\\n\t\t\t      (lock)->dep_map.wait_type_outer,\t\\\n\t\t\t      (lock)->dep_map.lock_type)\n\n#define lockdep_set_subclass(lock, sub)\t\t\t\t\t\\\n\tlockdep_init_map_type(&(lock)->dep_map, #lock, (lock)->dep_map.key, sub,\\\n\t\t\t      (lock)->dep_map.wait_type_inner,\t\t\\\n\t\t\t      (lock)->dep_map.wait_type_outer,\t\t\\\n\t\t\t      (lock)->dep_map.lock_type)\n\n#define lockdep_set_novalidate_class(lock) \\\n\tlockdep_set_class_and_name(lock, &__lockdep_no_validate__, #lock)\n\n \n#define lockdep_match_class(lock, key) lockdep_match_key(&(lock)->dep_map, key)\n\nstatic inline int lockdep_match_key(struct lockdep_map *lock,\n\t\t\t\t    struct lock_class_key *key)\n{\n\treturn lock->key == key;\n}\n\n \nextern void lock_acquire(struct lockdep_map *lock, unsigned int subclass,\n\t\t\t int trylock, int read, int check,\n\t\t\t struct lockdep_map *nest_lock, unsigned long ip);\n\nextern void lock_release(struct lockdep_map *lock, unsigned long ip);\n\nextern void lock_sync(struct lockdep_map *lock, unsigned int subclass,\n\t\t      int read, int check, struct lockdep_map *nest_lock,\n\t\t      unsigned long ip);\n\n \n#define LOCK_STATE_UNKNOWN\t-1\n#define LOCK_STATE_NOT_HELD\t0\n#define LOCK_STATE_HELD\t\t1\n\n \nextern int lock_is_held_type(const struct lockdep_map *lock, int read);\n\nstatic inline int lock_is_held(const struct lockdep_map *lock)\n{\n\treturn lock_is_held_type(lock, -1);\n}\n\n#define lockdep_is_held(lock)\t\tlock_is_held(&(lock)->dep_map)\n#define lockdep_is_held_type(lock, r)\tlock_is_held_type(&(lock)->dep_map, (r))\n\nextern void lock_set_class(struct lockdep_map *lock, const char *name,\n\t\t\t   struct lock_class_key *key, unsigned int subclass,\n\t\t\t   unsigned long ip);\n\n#define lock_set_novalidate_class(l, n, i) \\\n\tlock_set_class(l, n, &__lockdep_no_validate__, 0, i)\n\nstatic inline void lock_set_subclass(struct lockdep_map *lock,\n\t\tunsigned int subclass, unsigned long ip)\n{\n\tlock_set_class(lock, lock->name, lock->key, subclass, ip);\n}\n\nextern void lock_downgrade(struct lockdep_map *lock, unsigned long ip);\n\n#define NIL_COOKIE (struct pin_cookie){ .val = 0U, }\n\nextern struct pin_cookie lock_pin_lock(struct lockdep_map *lock);\nextern void lock_repin_lock(struct lockdep_map *lock, struct pin_cookie);\nextern void lock_unpin_lock(struct lockdep_map *lock, struct pin_cookie);\n\n#define lockdep_depth(tsk)\t(debug_locks ? (tsk)->lockdep_depth : 0)\n\n#define lockdep_assert(cond)\t\t\\\n\tdo { WARN_ON(debug_locks && !(cond)); } while (0)\n\n#define lockdep_assert_once(cond)\t\\\n\tdo { WARN_ON_ONCE(debug_locks && !(cond)); } while (0)\n\n#define lockdep_assert_held(l)\t\t\\\n\tlockdep_assert(lockdep_is_held(l) != LOCK_STATE_NOT_HELD)\n\n#define lockdep_assert_not_held(l)\t\\\n\tlockdep_assert(lockdep_is_held(l) != LOCK_STATE_HELD)\n\n#define lockdep_assert_held_write(l)\t\\\n\tlockdep_assert(lockdep_is_held_type(l, 0))\n\n#define lockdep_assert_held_read(l)\t\\\n\tlockdep_assert(lockdep_is_held_type(l, 1))\n\n#define lockdep_assert_held_once(l)\t\t\\\n\tlockdep_assert_once(lockdep_is_held(l) != LOCK_STATE_NOT_HELD)\n\n#define lockdep_assert_none_held_once()\t\t\\\n\tlockdep_assert_once(!current->lockdep_depth)\n\n#define lockdep_recursing(tsk)\t((tsk)->lockdep_recursion)\n\n#define lockdep_pin_lock(l)\tlock_pin_lock(&(l)->dep_map)\n#define lockdep_repin_lock(l,c)\tlock_repin_lock(&(l)->dep_map, (c))\n#define lockdep_unpin_lock(l,c)\tlock_unpin_lock(&(l)->dep_map, (c))\n\n \n#define DEFINE_WAIT_OVERRIDE_MAP(_name, _wait_type)\t\\\n\tstruct lockdep_map _name = {\t\t\t\\\n\t\t.name = #_name \"-wait-type-override\",\t\\\n\t\t.wait_type_inner = _wait_type,\t\t\\\n\t\t.lock_type = LD_LOCK_WAIT_OVERRIDE, }\n\n#else  \n\nstatic inline void lockdep_init_task(struct task_struct *task)\n{\n}\n\nstatic inline void lockdep_off(void)\n{\n}\n\nstatic inline void lockdep_on(void)\n{\n}\n\nstatic inline void lockdep_set_selftest_task(struct task_struct *task)\n{\n}\n\n# define lock_acquire(l, s, t, r, c, n, i)\tdo { } while (0)\n# define lock_release(l, i)\t\t\tdo { } while (0)\n# define lock_downgrade(l, i)\t\t\tdo { } while (0)\n# define lock_set_class(l, n, key, s, i)\tdo { (void)(key); } while (0)\n# define lock_set_novalidate_class(l, n, i)\tdo { } while (0)\n# define lock_set_subclass(l, s, i)\t\tdo { } while (0)\n# define lockdep_init()\t\t\t\tdo { } while (0)\n# define lockdep_init_map_type(lock, name, key, sub, inner, outer, type) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_init_map_waits(lock, name, key, sub, inner, outer) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_init_map_wait(lock, name, key, sub, inner) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_init_map(lock, name, key, sub) \\\n\t\tdo { (void)(name); (void)(key); } while (0)\n# define lockdep_set_class(lock, key)\t\tdo { (void)(key); } while (0)\n# define lockdep_set_class_and_name(lock, key, name) \\\n\t\tdo { (void)(key); (void)(name); } while (0)\n#define lockdep_set_class_and_subclass(lock, key, sub) \\\n\t\tdo { (void)(key); } while (0)\n#define lockdep_set_subclass(lock, sub)\t\tdo { } while (0)\n\n#define lockdep_set_novalidate_class(lock) do { } while (0)\n\n \n\n# define lockdep_reset()\t\tdo { debug_locks = 1; } while (0)\n# define lockdep_free_key_range(start, size)\tdo { } while (0)\n# define lockdep_sys_exit() \t\t\tdo { } while (0)\n\nstatic inline void lockdep_register_key(struct lock_class_key *key)\n{\n}\n\nstatic inline void lockdep_unregister_key(struct lock_class_key *key)\n{\n}\n\n#define lockdep_depth(tsk)\t(0)\n\n \nextern int lock_is_held(const void *);\nextern int lockdep_is_held(const void *);\n#define lockdep_is_held_type(l, r)\t\t(1)\n\n#define lockdep_assert(c)\t\t\tdo { } while (0)\n#define lockdep_assert_once(c)\t\t\tdo { } while (0)\n\n#define lockdep_assert_held(l)\t\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_not_held(l)\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_write(l)\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_read(l)\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_held_once(l)\t\tdo { (void)(l); } while (0)\n#define lockdep_assert_none_held_once()\tdo { } while (0)\n\n#define lockdep_recursing(tsk)\t\t\t(0)\n\n#define NIL_COOKIE (struct pin_cookie){ }\n\n#define lockdep_pin_lock(l)\t\t\t({ struct pin_cookie cookie = { }; cookie; })\n#define lockdep_repin_lock(l, c)\t\tdo { (void)(l); (void)(c); } while (0)\n#define lockdep_unpin_lock(l, c)\t\tdo { (void)(l); (void)(c); } while (0)\n\n#define DEFINE_WAIT_OVERRIDE_MAP(_name, _wait_type)\t\\\n\tstruct lockdep_map __maybe_unused _name = {}\n\n#endif  \n\n#ifdef CONFIG_PROVE_LOCKING\nvoid lockdep_set_lock_cmp_fn(struct lockdep_map *, lock_cmp_fn, lock_print_fn);\n\n#define lock_set_cmp_fn(lock, ...)\tlockdep_set_lock_cmp_fn(&(lock)->dep_map, __VA_ARGS__)\n#else\n#define lock_set_cmp_fn(lock, ...)\tdo { } while (0)\n#endif\n\nenum xhlock_context_t {\n\tXHLOCK_HARD,\n\tXHLOCK_SOFT,\n\tXHLOCK_CTX_NR,\n};\n\n \n#define STATIC_LOCKDEP_MAP_INIT(_name, _key) \\\n\t{ .name = (_name), .key = (void *)(_key), }\n\nstatic inline void lockdep_invariant_state(bool force) {}\nstatic inline void lockdep_free_task(struct task_struct *task) {}\n\n#ifdef CONFIG_LOCK_STAT\n\nextern void lock_contended(struct lockdep_map *lock, unsigned long ip);\nextern void lock_acquired(struct lockdep_map *lock, unsigned long ip);\n\n#define LOCK_CONTENDED(_lock, try, lock)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (!try(_lock)) {\t\t\t\t\t\\\n\t\tlock_contended(&(_lock)->dep_map, _RET_IP_);\t\\\n\t\tlock(_lock);\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tlock_acquired(&(_lock)->dep_map, _RET_IP_);\t\t\t\\\n} while (0)\n\n#define LOCK_CONTENDED_RETURN(_lock, try, lock)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint ____err = 0;\t\t\t\t\t\\\n\tif (!try(_lock)) {\t\t\t\t\t\\\n\t\tlock_contended(&(_lock)->dep_map, _RET_IP_);\t\\\n\t\t____err = lock(_lock);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\n\tif (!____err)\t\t\t\t\t\t\\\n\t\tlock_acquired(&(_lock)->dep_map, _RET_IP_);\t\\\n\t____err;\t\t\t\t\t\t\\\n})\n\n#else  \n\n#define lock_contended(lockdep_map, ip) do {} while (0)\n#define lock_acquired(lockdep_map, ip) do {} while (0)\n\n#define LOCK_CONTENDED(_lock, try, lock) \\\n\tlock(_lock)\n\n#define LOCK_CONTENDED_RETURN(_lock, try, lock) \\\n\tlock(_lock)\n\n#endif  \n\n#ifdef CONFIG_PROVE_LOCKING\nextern void print_irqtrace_events(struct task_struct *curr);\n#else\nstatic inline void print_irqtrace_events(struct task_struct *curr)\n{\n}\n#endif\n\n \n#ifdef CONFIG_DEBUG_LOCKING_API_SELFTESTS\nextern unsigned int force_read_lock_recursive;\n#else  \n#define force_read_lock_recursive 0\n#endif  \n\n#ifdef CONFIG_LOCKDEP\nextern bool read_lock_is_recursive(void);\n#else  \n \n#define read_lock_is_recursive() 0\n#endif\n\n \n#define SINGLE_DEPTH_NESTING\t\t\t1\n\n \n\n#define lock_acquire_exclusive(l, s, t, n, i)\t\tlock_acquire(l, s, t, 0, 1, n, i)\n#define lock_acquire_shared(l, s, t, n, i)\t\tlock_acquire(l, s, t, 1, 1, n, i)\n#define lock_acquire_shared_recursive(l, s, t, n, i)\tlock_acquire(l, s, t, 2, 1, n, i)\n\n#define spin_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define spin_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define spin_release(l, i)\t\t\tlock_release(l, i)\n\n#define rwlock_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define rwlock_acquire_read(l, s, t, i)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (read_lock_is_recursive())\t\t\t\t\t\\\n\t\tlock_acquire_shared_recursive(l, s, t, NULL, i);\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tlock_acquire_shared(l, s, t, NULL, i);\t\t\t\\\n} while (0)\n\n#define rwlock_release(l, i)\t\t\tlock_release(l, i)\n\n#define seqcount_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define seqcount_acquire_read(l, s, t, i)\tlock_acquire_shared_recursive(l, s, t, NULL, i)\n#define seqcount_release(l, i)\t\t\tlock_release(l, i)\n\n#define mutex_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define mutex_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define mutex_release(l, i)\t\t\tlock_release(l, i)\n\n#define rwsem_acquire(l, s, t, i)\t\tlock_acquire_exclusive(l, s, t, NULL, i)\n#define rwsem_acquire_nest(l, s, t, n, i)\tlock_acquire_exclusive(l, s, t, n, i)\n#define rwsem_acquire_read(l, s, t, i)\t\tlock_acquire_shared(l, s, t, NULL, i)\n#define rwsem_release(l, i)\t\t\tlock_release(l, i)\n\n#define lock_map_acquire(l)\t\t\tlock_acquire_exclusive(l, 0, 0, NULL, _THIS_IP_)\n#define lock_map_acquire_try(l)\t\t\tlock_acquire_exclusive(l, 0, 1, NULL, _THIS_IP_)\n#define lock_map_acquire_read(l)\t\tlock_acquire_shared_recursive(l, 0, 0, NULL, _THIS_IP_)\n#define lock_map_acquire_tryread(l)\t\tlock_acquire_shared_recursive(l, 0, 1, NULL, _THIS_IP_)\n#define lock_map_release(l)\t\t\tlock_release(l, _THIS_IP_)\n#define lock_map_sync(l)\t\t\tlock_sync(l, 0, 0, 1, NULL, _THIS_IP_)\n\n#ifdef CONFIG_PROVE_LOCKING\n# define might_lock(lock)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, 0, 0, 0, 1, NULL, _THIS_IP_);\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n# define might_lock_read(lock)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, 0, 0, 1, 1, NULL, _THIS_IP_);\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n# define might_lock_nested(lock, subclass)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\ttypecheck(struct lockdep_map *, &(lock)->dep_map);\t\t\\\n\tlock_acquire(&(lock)->dep_map, subclass, 0, 1, 1, NULL,\t\t\\\n\t\t     _THIS_IP_);\t\t\t\t\t\\\n\tlock_release(&(lock)->dep_map, _THIS_IP_);\t\t\t\\\n} while (0)\n\nDECLARE_PER_CPU(int, hardirqs_enabled);\nDECLARE_PER_CPU(int, hardirq_context);\nDECLARE_PER_CPU(unsigned int, lockdep_recursion);\n\n#define __lockdep_enabled\t(debug_locks && !this_cpu_read(lockdep_recursion))\n\n#define lockdep_assert_irqs_enabled()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && !this_cpu_read(hardirqs_enabled)); \\\n} while (0)\n\n#define lockdep_assert_irqs_disabled()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && this_cpu_read(hardirqs_enabled)); \\\n} while (0)\n\n#define lockdep_assert_in_irq()\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && !this_cpu_read(hardirq_context)); \\\n} while (0)\n\n#define lockdep_assert_no_hardirq()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled && (this_cpu_read(hardirq_context) || \\\n\t\t\t\t\t   !this_cpu_read(hardirqs_enabled))); \\\n} while (0)\n\n#define lockdep_assert_preemption_enabled()\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(IS_ENABLED(CONFIG_PREEMPT_COUNT)\t&&\t\t\\\n\t\t     __lockdep_enabled\t\t\t&&\t\t\\\n\t\t     (preempt_count() != 0\t\t||\t\t\\\n\t\t      !this_cpu_read(hardirqs_enabled)));\t\t\\\n} while (0)\n\n#define lockdep_assert_preemption_disabled()\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(IS_ENABLED(CONFIG_PREEMPT_COUNT)\t&&\t\t\\\n\t\t     __lockdep_enabled\t\t\t&&\t\t\\\n\t\t     (preempt_count() == 0\t\t&&\t\t\\\n\t\t      this_cpu_read(hardirqs_enabled)));\t\t\\\n} while (0)\n\n \n#define lockdep_assert_in_softirq()\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(__lockdep_enabled\t\t\t&&\t\t\\\n\t\t     (!in_softirq() || in_irq() || in_nmi()));\t\t\\\n} while (0)\n\n#else\n# define might_lock(lock) do { } while (0)\n# define might_lock_read(lock) do { } while (0)\n# define might_lock_nested(lock, subclass) do { } while (0)\n\n# define lockdep_assert_irqs_enabled() do { } while (0)\n# define lockdep_assert_irqs_disabled() do { } while (0)\n# define lockdep_assert_in_irq() do { } while (0)\n# define lockdep_assert_no_hardirq() do { } while (0)\n\n# define lockdep_assert_preemption_enabled() do { } while (0)\n# define lockdep_assert_preemption_disabled() do { } while (0)\n# define lockdep_assert_in_softirq() do { } while (0)\n#endif\n\n#ifdef CONFIG_PROVE_RAW_LOCK_NESTING\n\n# define lockdep_assert_RT_in_threaded_ctx() do {\t\t\t\\\n\t\tWARN_ONCE(debug_locks && !current->lockdep_recursion &&\t\\\n\t\t\t  lockdep_hardirq_context() &&\t\t\t\\\n\t\t\t  !(current->hardirq_threaded || current->irq_config),\t\\\n\t\t\t  \"Not in threaded context on PREEMPT_RT as expected\\n\");\t\\\n} while (0)\n\n#else\n\n# define lockdep_assert_RT_in_threaded_ctx() do { } while (0)\n\n#endif\n\n#ifdef CONFIG_LOCKDEP\nvoid lockdep_rcu_suspicious(const char *file, const int line, const char *s);\n#else\nstatic inline void\nlockdep_rcu_suspicious(const char *file, const int line, const char *s)\n{\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}