{
  "module_name": "blk-mq.h",
  "hash_id": "ccdc0c8484564a5cbf0a50dbddbf881d08fe6b086a4d7f7ba3a61ad19cbc21e2",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/blk-mq.h",
  "human_readable_source": " \n#ifndef BLK_MQ_H\n#define BLK_MQ_H\n\n#include <linux/blkdev.h>\n#include <linux/sbitmap.h>\n#include <linux/lockdep.h>\n#include <linux/scatterlist.h>\n#include <linux/prefetch.h>\n#include <linux/srcu.h>\n\nstruct blk_mq_tags;\nstruct blk_flush_queue;\n\n#define BLKDEV_MIN_RQ\t4\n#define BLKDEV_DEFAULT_RQ\t128\n\nenum rq_end_io_ret {\n\tRQ_END_IO_NONE,\n\tRQ_END_IO_FREE,\n};\n\ntypedef enum rq_end_io_ret (rq_end_io_fn)(struct request *, blk_status_t);\n\n \ntypedef __u32 __bitwise req_flags_t;\n\n \n#define RQF_STARTED\t\t((__force req_flags_t)(1 << 1))\n \n#define RQF_FLUSH_SEQ\t\t((__force req_flags_t)(1 << 4))\n \n#define RQF_MIXED_MERGE\t\t((__force req_flags_t)(1 << 5))\n \n#define RQF_MQ_INFLIGHT\t\t((__force req_flags_t)(1 << 6))\n \n#define RQF_DONTPREP\t\t((__force req_flags_t)(1 << 7))\n \n#define RQF_SCHED_TAGS\t\t((__force req_flags_t)(1 << 8))\n \n#define RQF_USE_SCHED\t\t((__force req_flags_t)(1 << 9))\n \n#define RQF_FAILED\t\t((__force req_flags_t)(1 << 10))\n \n#define RQF_QUIET\t\t((__force req_flags_t)(1 << 11))\n \n#define RQF_IO_STAT\t\t((__force req_flags_t)(1 << 13))\n \n#define RQF_PM\t\t\t((__force req_flags_t)(1 << 15))\n \n#define RQF_HASHED\t\t((__force req_flags_t)(1 << 16))\n \n#define RQF_STATS\t\t((__force req_flags_t)(1 << 17))\n \n#define RQF_SPECIAL_PAYLOAD\t((__force req_flags_t)(1 << 18))\n \n#define RQF_ZONE_WRITE_LOCKED\t((__force req_flags_t)(1 << 19))\n \n#define RQF_TIMED_OUT\t\t((__force req_flags_t)(1 << 21))\n#define RQF_RESV\t\t((__force req_flags_t)(1 << 23))\n\n \n#define RQF_NOMERGE_FLAGS \\\n\t(RQF_STARTED | RQF_FLUSH_SEQ | RQF_SPECIAL_PAYLOAD)\n\nenum mq_rq_state {\n\tMQ_RQ_IDLE\t\t= 0,\n\tMQ_RQ_IN_FLIGHT\t\t= 1,\n\tMQ_RQ_COMPLETE\t\t= 2,\n};\n\n \nstruct request {\n\tstruct request_queue *q;\n\tstruct blk_mq_ctx *mq_ctx;\n\tstruct blk_mq_hw_ctx *mq_hctx;\n\n\tblk_opf_t cmd_flags;\t\t \n\treq_flags_t rq_flags;\n\n\tint tag;\n\tint internal_tag;\n\n\tunsigned int timeout;\n\n\t \n\tunsigned int __data_len;\t \n\tsector_t __sector;\t\t \n\n\tstruct bio *bio;\n\tstruct bio *biotail;\n\n\tunion {\n\t\tstruct list_head queuelist;\n\t\tstruct request *rq_next;\n\t};\n\n\tstruct block_device *part;\n#ifdef CONFIG_BLK_RQ_ALLOC_TIME\n\t \n\tu64 alloc_time_ns;\n#endif\n\t \n\tu64 start_time_ns;\n\t \n\tu64 io_start_time_ns;\n\n#ifdef CONFIG_BLK_WBT\n\tunsigned short wbt_flags;\n#endif\n\t \n\tunsigned short stats_sectors;\n\n\t \n\tunsigned short nr_phys_segments;\n\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tunsigned short nr_integrity_segments;\n#endif\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\tstruct bio_crypt_ctx *crypt_ctx;\n\tstruct blk_crypto_keyslot *crypt_keyslot;\n#endif\n\n\tunsigned short ioprio;\n\n\tenum mq_rq_state state;\n\tatomic_t ref;\n\n\tunsigned long deadline;\n\n\t \n\tunion {\n\t\tstruct hlist_node hash;\t \n\t\tstruct llist_node ipi_list;\n\t};\n\n\t \n\tunion {\n\t\tstruct rb_node rb_node;\t \n\t\tstruct bio_vec special_vec;\n\t};\n\n\t \n\tstruct {\n\t\tstruct io_cq\t\t*icq;\n\t\tvoid\t\t\t*priv[2];\n\t} elv;\n\n\tstruct {\n\t\tunsigned int\t\tseq;\n\t\trq_end_io_fn\t\t*saved_end_io;\n\t} flush;\n\n\tu64 fifo_time;\n\n\t \n\trq_end_io_fn *end_io;\n\tvoid *end_io_data;\n};\n\nstatic inline enum req_op req_op(const struct request *req)\n{\n\treturn req->cmd_flags & REQ_OP_MASK;\n}\n\nstatic inline bool blk_rq_is_passthrough(struct request *rq)\n{\n\treturn blk_op_is_passthrough(rq->cmd_flags);\n}\n\nstatic inline unsigned short req_get_ioprio(struct request *req)\n{\n\treturn req->ioprio;\n}\n\n#define rq_data_dir(rq)\t\t(op_is_write(req_op(rq)) ? WRITE : READ)\n\n#define rq_dma_dir(rq) \\\n\t(op_is_write(req_op(rq)) ? DMA_TO_DEVICE : DMA_FROM_DEVICE)\n\n#define rq_list_add(listptr, rq)\tdo {\t\t\\\n\t(rq)->rq_next = *(listptr);\t\t\t\\\n\t*(listptr) = rq;\t\t\t\t\\\n} while (0)\n\n#define rq_list_add_tail(lastpptr, rq)\tdo {\t\t\\\n\t(rq)->rq_next = NULL;\t\t\t\t\\\n\t**(lastpptr) = rq;\t\t\t\t\\\n\t*(lastpptr) = &rq->rq_next;\t\t\t\\\n} while (0)\n\n#define rq_list_pop(listptr)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tstruct request *__req = NULL;\t\t\t\\\n\tif ((listptr) && *(listptr))\t{\t\t\\\n\t\t__req = *(listptr);\t\t\t\\\n\t\t*(listptr) = __req->rq_next;\t\t\\\n\t}\t\t\t\t\t\t\\\n\t__req;\t\t\t\t\t\t\\\n})\n\n#define rq_list_peek(listptr)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tstruct request *__req = NULL;\t\t\t\\\n\tif ((listptr) && *(listptr))\t\t\t\\\n\t\t__req = *(listptr);\t\t\t\\\n\t__req;\t\t\t\t\t\t\\\n})\n\n#define rq_list_for_each(listptr, pos)\t\t\t\\\n\tfor (pos = rq_list_peek((listptr)); pos; pos = rq_list_next(pos))\n\n#define rq_list_for_each_safe(listptr, pos, nxt)\t\t\t\\\n\tfor (pos = rq_list_peek((listptr)), nxt = rq_list_next(pos);\t\\\n\t\tpos; pos = nxt, nxt = pos ? rq_list_next(pos) : NULL)\n\n#define rq_list_next(rq)\t(rq)->rq_next\n#define rq_list_empty(list)\t((list) == (struct request *) NULL)\n\n \nstatic inline void rq_list_move(struct request **src, struct request **dst,\n\t\t\t\tstruct request *rq, struct request *prev)\n{\n\tif (prev)\n\t\tprev->rq_next = rq->rq_next;\n\telse\n\t\t*src = rq->rq_next;\n\trq_list_add(dst, rq);\n}\n\n \nenum blk_eh_timer_return {\n\tBLK_EH_DONE,\n\tBLK_EH_RESET_TIMER,\n};\n\n#define BLK_TAG_ALLOC_FIFO 0  \n#define BLK_TAG_ALLOC_RR 1  \n\n \nstruct blk_mq_hw_ctx {\n\tstruct {\n\t\t \n\t\tspinlock_t\t\tlock;\n\t\t \n\t\tstruct list_head\tdispatch;\n\t\t  \n\t\tunsigned long\t\tstate;\n\t} ____cacheline_aligned_in_smp;\n\n\t \n\tstruct delayed_work\trun_work;\n\t \n\tcpumask_var_t\t\tcpumask;\n\t \n\tint\t\t\tnext_cpu;\n\t \n\tint\t\t\tnext_cpu_batch;\n\n\t \n\tunsigned long\t\tflags;\n\n\t \n\tvoid\t\t\t*sched_data;\n\t \n\tstruct request_queue\t*queue;\n\t \n\tstruct blk_flush_queue\t*fq;\n\n\t \n\tvoid\t\t\t*driver_data;\n\n\t \n\tstruct sbitmap\t\tctx_map;\n\n\t \n\tstruct blk_mq_ctx\t*dispatch_from;\n\t \n\tunsigned int\t\tdispatch_busy;\n\n\t \n\tunsigned short\t\ttype;\n\t \n\tunsigned short\t\tnr_ctx;\n\t \n\tstruct blk_mq_ctx\t**ctxs;\n\n\t \n\tspinlock_t\t\tdispatch_wait_lock;\n\t \n\twait_queue_entry_t\tdispatch_wait;\n\n\t \n\tatomic_t\t\twait_index;\n\n\t \n\tstruct blk_mq_tags\t*tags;\n\t \n\tstruct blk_mq_tags\t*sched_tags;\n\n\t \n\tunsigned long\t\trun;\n\n\t \n\tunsigned int\t\tnuma_node;\n\t \n\tunsigned int\t\tqueue_num;\n\n\t \n\tatomic_t\t\tnr_active;\n\n\t \n\tstruct hlist_node\tcpuhp_online;\n\t \n\tstruct hlist_node\tcpuhp_dead;\n\t \n\tstruct kobject\t\tkobj;\n\n#ifdef CONFIG_BLK_DEBUG_FS\n\t \n\tstruct dentry\t\t*debugfs_dir;\n\t \n\tstruct dentry\t\t*sched_debugfs_dir;\n#endif\n\n\t \n\tstruct list_head\thctx_list;\n};\n\n \nstruct blk_mq_queue_map {\n\tunsigned int *mq_map;\n\tunsigned int nr_queues;\n\tunsigned int queue_offset;\n};\n\n \nenum hctx_type {\n\tHCTX_TYPE_DEFAULT,\n\tHCTX_TYPE_READ,\n\tHCTX_TYPE_POLL,\n\n\tHCTX_MAX_TYPES,\n};\n\n \nstruct blk_mq_tag_set {\n\tconst struct blk_mq_ops\t*ops;\n\tstruct blk_mq_queue_map\tmap[HCTX_MAX_TYPES];\n\tunsigned int\t\tnr_maps;\n\tunsigned int\t\tnr_hw_queues;\n\tunsigned int\t\tqueue_depth;\n\tunsigned int\t\treserved_tags;\n\tunsigned int\t\tcmd_size;\n\tint\t\t\tnuma_node;\n\tunsigned int\t\ttimeout;\n\tunsigned int\t\tflags;\n\tvoid\t\t\t*driver_data;\n\n\tstruct blk_mq_tags\t**tags;\n\n\tstruct blk_mq_tags\t*shared_tags;\n\n\tstruct mutex\t\ttag_list_lock;\n\tstruct list_head\ttag_list;\n\tstruct srcu_struct\t*srcu;\n};\n\n \nstruct blk_mq_queue_data {\n\tstruct request *rq;\n\tbool last;\n};\n\ntypedef bool (busy_tag_iter_fn)(struct request *, void *);\n\n \nstruct blk_mq_ops {\n\t \n\tblk_status_t (*queue_rq)(struct blk_mq_hw_ctx *,\n\t\t\t\t const struct blk_mq_queue_data *);\n\n\t \n\tvoid (*commit_rqs)(struct blk_mq_hw_ctx *);\n\n\t \n\tvoid (*queue_rqs)(struct request **rqlist);\n\n\t \n\tint (*get_budget)(struct request_queue *);\n\n\t \n\tvoid (*put_budget)(struct request_queue *, int);\n\n\t \n\tvoid (*set_rq_budget_token)(struct request *, int);\n\t \n\tint (*get_rq_budget_token)(struct request *);\n\n\t \n\tenum blk_eh_timer_return (*timeout)(struct request *);\n\n\t \n\tint (*poll)(struct blk_mq_hw_ctx *, struct io_comp_batch *);\n\n\t \n\tvoid (*complete)(struct request *);\n\n\t \n\tint (*init_hctx)(struct blk_mq_hw_ctx *, void *, unsigned int);\n\t \n\tvoid (*exit_hctx)(struct blk_mq_hw_ctx *, unsigned int);\n\n\t \n\tint (*init_request)(struct blk_mq_tag_set *set, struct request *,\n\t\t\t    unsigned int, unsigned int);\n\t \n\tvoid (*exit_request)(struct blk_mq_tag_set *set, struct request *,\n\t\t\t     unsigned int);\n\n\t \n\tvoid (*cleanup_rq)(struct request *);\n\n\t \n\tbool (*busy)(struct request_queue *);\n\n\t \n\tvoid (*map_queues)(struct blk_mq_tag_set *set);\n\n#ifdef CONFIG_BLK_DEBUG_FS\n\t \n\tvoid (*show_rq)(struct seq_file *m, struct request *rq);\n#endif\n};\n\nenum {\n\tBLK_MQ_F_SHOULD_MERGE\t= 1 << 0,\n\tBLK_MQ_F_TAG_QUEUE_SHARED = 1 << 1,\n\t \n\tBLK_MQ_F_STACKING\t= 1 << 2,\n\tBLK_MQ_F_TAG_HCTX_SHARED = 1 << 3,\n\tBLK_MQ_F_BLOCKING\t= 1 << 5,\n\t \n\tBLK_MQ_F_NO_SCHED\t= 1 << 6,\n\t \n\tBLK_MQ_F_NO_SCHED_BY_DEFAULT\t= 1 << 7,\n\tBLK_MQ_F_ALLOC_POLICY_START_BIT = 8,\n\tBLK_MQ_F_ALLOC_POLICY_BITS = 1,\n\n\tBLK_MQ_S_STOPPED\t= 0,\n\tBLK_MQ_S_TAG_ACTIVE\t= 1,\n\tBLK_MQ_S_SCHED_RESTART\t= 2,\n\n\t \n\tBLK_MQ_S_INACTIVE\t= 3,\n\n\tBLK_MQ_MAX_DEPTH\t= 10240,\n\n\tBLK_MQ_CPU_WORK_BATCH\t= 8,\n};\n#define BLK_MQ_FLAG_TO_ALLOC_POLICY(flags) \\\n\t((flags >> BLK_MQ_F_ALLOC_POLICY_START_BIT) & \\\n\t\t((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1))\n#define BLK_ALLOC_POLICY_TO_MQ_FLAG(policy) \\\n\t((policy & ((1 << BLK_MQ_F_ALLOC_POLICY_BITS) - 1)) \\\n\t\t<< BLK_MQ_F_ALLOC_POLICY_START_BIT)\n\n#define BLK_MQ_NO_HCTX_IDX\t(-1U)\n\nstruct gendisk *__blk_mq_alloc_disk(struct blk_mq_tag_set *set, void *queuedata,\n\t\tstruct lock_class_key *lkclass);\n#define blk_mq_alloc_disk(set, queuedata)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__blk_mq_alloc_disk(set, queuedata, &__key);\t\t\t\\\n})\nstruct gendisk *blk_mq_alloc_disk_for_queue(struct request_queue *q,\n\t\tstruct lock_class_key *lkclass);\nstruct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *);\nint blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,\n\t\tstruct request_queue *q);\nvoid blk_mq_destroy_queue(struct request_queue *);\n\nint blk_mq_alloc_tag_set(struct blk_mq_tag_set *set);\nint blk_mq_alloc_sq_tag_set(struct blk_mq_tag_set *set,\n\t\tconst struct blk_mq_ops *ops, unsigned int queue_depth,\n\t\tunsigned int set_flags);\nvoid blk_mq_free_tag_set(struct blk_mq_tag_set *set);\n\nvoid blk_mq_free_request(struct request *rq);\nint blk_rq_poll(struct request *rq, struct io_comp_batch *iob,\n\t\tunsigned int poll_flags);\n\nbool blk_mq_queue_inflight(struct request_queue *q);\n\nenum {\n\t \n\tBLK_MQ_REQ_NOWAIT\t= (__force blk_mq_req_flags_t)(1 << 0),\n\t \n\tBLK_MQ_REQ_RESERVED\t= (__force blk_mq_req_flags_t)(1 << 1),\n\t \n\tBLK_MQ_REQ_PM\t\t= (__force blk_mq_req_flags_t)(1 << 2),\n};\n\nstruct request *blk_mq_alloc_request(struct request_queue *q, blk_opf_t opf,\n\t\tblk_mq_req_flags_t flags);\nstruct request *blk_mq_alloc_request_hctx(struct request_queue *q,\n\t\tblk_opf_t opf, blk_mq_req_flags_t flags,\n\t\tunsigned int hctx_idx);\n\n \nstruct blk_mq_tags {\n\tunsigned int nr_tags;\n\tunsigned int nr_reserved_tags;\n\tunsigned int active_queues;\n\n\tstruct sbitmap_queue bitmap_tags;\n\tstruct sbitmap_queue breserved_tags;\n\n\tstruct request **rqs;\n\tstruct request **static_rqs;\n\tstruct list_head page_list;\n\n\t \n\tspinlock_t lock;\n};\n\nstatic inline struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags,\n\t\t\t\t\t       unsigned int tag)\n{\n\tif (tag < tags->nr_tags) {\n\t\tprefetch(tags->rqs[tag]);\n\t\treturn tags->rqs[tag];\n\t}\n\n\treturn NULL;\n}\n\nenum {\n\tBLK_MQ_UNIQUE_TAG_BITS = 16,\n\tBLK_MQ_UNIQUE_TAG_MASK = (1 << BLK_MQ_UNIQUE_TAG_BITS) - 1,\n};\n\nu32 blk_mq_unique_tag(struct request *rq);\n\nstatic inline u16 blk_mq_unique_tag_to_hwq(u32 unique_tag)\n{\n\treturn unique_tag >> BLK_MQ_UNIQUE_TAG_BITS;\n}\n\nstatic inline u16 blk_mq_unique_tag_to_tag(u32 unique_tag)\n{\n\treturn unique_tag & BLK_MQ_UNIQUE_TAG_MASK;\n}\n\n \nstatic inline enum mq_rq_state blk_mq_rq_state(struct request *rq)\n{\n\treturn READ_ONCE(rq->state);\n}\n\nstatic inline int blk_mq_request_started(struct request *rq)\n{\n\treturn blk_mq_rq_state(rq) != MQ_RQ_IDLE;\n}\n\nstatic inline int blk_mq_request_completed(struct request *rq)\n{\n\treturn blk_mq_rq_state(rq) == MQ_RQ_COMPLETE;\n}\n\n \nstatic inline void blk_mq_set_request_complete(struct request *rq)\n{\n\tWRITE_ONCE(rq->state, MQ_RQ_COMPLETE);\n}\n\n \nstatic inline void blk_mq_complete_request_direct(struct request *rq,\n\t\t   void (*complete)(struct request *rq))\n{\n\tWRITE_ONCE(rq->state, MQ_RQ_COMPLETE);\n\tcomplete(rq);\n}\n\nvoid blk_mq_start_request(struct request *rq);\nvoid blk_mq_end_request(struct request *rq, blk_status_t error);\nvoid __blk_mq_end_request(struct request *rq, blk_status_t error);\nvoid blk_mq_end_request_batch(struct io_comp_batch *ib);\n\n \nstatic inline bool blk_mq_need_time_stamp(struct request *rq)\n{\n\treturn (rq->rq_flags & (RQF_IO_STAT | RQF_STATS | RQF_USE_SCHED));\n}\n\nstatic inline bool blk_mq_is_reserved_rq(struct request *rq)\n{\n\treturn rq->rq_flags & RQF_RESV;\n}\n\n \nstatic inline bool blk_mq_add_to_batch(struct request *req,\n\t\t\t\t       struct io_comp_batch *iob, int ioerror,\n\t\t\t\t       void (*complete)(struct io_comp_batch *))\n{\n\t \n\tif (!iob || (req->rq_flags & RQF_SCHED_TAGS) || ioerror ||\n\t\t\t(req->end_io && !blk_rq_is_passthrough(req)))\n\t\treturn false;\n\n\tif (!iob->complete)\n\t\tiob->complete = complete;\n\telse if (iob->complete != complete)\n\t\treturn false;\n\tiob->need_ts |= blk_mq_need_time_stamp(req);\n\trq_list_add(&iob->req_list, req);\n\treturn true;\n}\n\nvoid blk_mq_requeue_request(struct request *rq, bool kick_requeue_list);\nvoid blk_mq_kick_requeue_list(struct request_queue *q);\nvoid blk_mq_delay_kick_requeue_list(struct request_queue *q, unsigned long msecs);\nvoid blk_mq_complete_request(struct request *rq);\nbool blk_mq_complete_request_remote(struct request *rq);\nvoid blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx);\nvoid blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx);\nvoid blk_mq_stop_hw_queues(struct request_queue *q);\nvoid blk_mq_start_hw_queues(struct request_queue *q);\nvoid blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);\nvoid blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async);\nvoid blk_mq_quiesce_queue(struct request_queue *q);\nvoid blk_mq_wait_quiesce_done(struct blk_mq_tag_set *set);\nvoid blk_mq_quiesce_tagset(struct blk_mq_tag_set *set);\nvoid blk_mq_unquiesce_tagset(struct blk_mq_tag_set *set);\nvoid blk_mq_unquiesce_queue(struct request_queue *q);\nvoid blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs);\nvoid blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async);\nvoid blk_mq_run_hw_queues(struct request_queue *q, bool async);\nvoid blk_mq_delay_run_hw_queues(struct request_queue *q, unsigned long msecs);\nvoid blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,\n\t\tbusy_tag_iter_fn *fn, void *priv);\nvoid blk_mq_tagset_wait_completed_request(struct blk_mq_tag_set *tagset);\nvoid blk_mq_freeze_queue(struct request_queue *q);\nvoid blk_mq_unfreeze_queue(struct request_queue *q);\nvoid blk_freeze_queue_start(struct request_queue *q);\nvoid blk_mq_freeze_queue_wait(struct request_queue *q);\nint blk_mq_freeze_queue_wait_timeout(struct request_queue *q,\n\t\t\t\t     unsigned long timeout);\n\nvoid blk_mq_map_queues(struct blk_mq_queue_map *qmap);\nvoid blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues);\n\nvoid blk_mq_quiesce_queue_nowait(struct request_queue *q);\n\nunsigned int blk_mq_rq_cpu(struct request *rq);\n\nbool __blk_should_fake_timeout(struct request_queue *q);\nstatic inline bool blk_should_fake_timeout(struct request_queue *q)\n{\n\tif (IS_ENABLED(CONFIG_FAIL_IO_TIMEOUT) &&\n\t    test_bit(QUEUE_FLAG_FAIL_IO, &q->queue_flags))\n\t\treturn __blk_should_fake_timeout(q);\n\treturn false;\n}\n\n \nstatic inline struct request *blk_mq_rq_from_pdu(void *pdu)\n{\n\treturn pdu - sizeof(struct request);\n}\n\n \nstatic inline void *blk_mq_rq_to_pdu(struct request *rq)\n{\n\treturn rq + 1;\n}\n\n#define queue_for_each_hw_ctx(q, hctx, i)\t\t\t\t\\\n\txa_for_each(&(q)->hctx_table, (i), (hctx))\n\n#define hctx_for_each_ctx(hctx, ctx, i)\t\t\t\t\t\\\n\tfor ((i) = 0; (i) < (hctx)->nr_ctx &&\t\t\t\t\\\n\t     ({ ctx = (hctx)->ctxs[(i)]; 1; }); (i)++)\n\nstatic inline void blk_mq_cleanup_rq(struct request *rq)\n{\n\tif (rq->q->mq_ops->cleanup_rq)\n\t\trq->q->mq_ops->cleanup_rq(rq);\n}\n\nstatic inline void blk_rq_bio_prep(struct request *rq, struct bio *bio,\n\t\tunsigned int nr_segs)\n{\n\trq->nr_phys_segments = nr_segs;\n\trq->__data_len = bio->bi_iter.bi_size;\n\trq->bio = rq->biotail = bio;\n\trq->ioprio = bio_prio(bio);\n}\n\nvoid blk_mq_hctx_set_fq_lock_class(struct blk_mq_hw_ctx *hctx,\n\t\tstruct lock_class_key *key);\n\nstatic inline bool rq_is_sync(struct request *rq)\n{\n\treturn op_is_sync(rq->cmd_flags);\n}\n\nvoid blk_rq_init(struct request_queue *q, struct request *rq);\nint blk_rq_prep_clone(struct request *rq, struct request *rq_src,\n\t\tstruct bio_set *bs, gfp_t gfp_mask,\n\t\tint (*bio_ctr)(struct bio *, struct bio *, void *), void *data);\nvoid blk_rq_unprep_clone(struct request *rq);\nblk_status_t blk_insert_cloned_request(struct request *rq);\n\nstruct rq_map_data {\n\tstruct page **pages;\n\tunsigned long offset;\n\tunsigned short page_order;\n\tunsigned short nr_entries;\n\tbool null_mapped;\n\tbool from_user;\n};\n\nint blk_rq_map_user(struct request_queue *, struct request *,\n\t\tstruct rq_map_data *, void __user *, unsigned long, gfp_t);\nint blk_rq_map_user_io(struct request *, struct rq_map_data *,\n\t\tvoid __user *, unsigned long, gfp_t, bool, int, bool, int);\nint blk_rq_map_user_iov(struct request_queue *, struct request *,\n\t\tstruct rq_map_data *, const struct iov_iter *, gfp_t);\nint blk_rq_unmap_user(struct bio *);\nint blk_rq_map_kern(struct request_queue *, struct request *, void *,\n\t\tunsigned int, gfp_t);\nint blk_rq_append_bio(struct request *rq, struct bio *bio);\nvoid blk_execute_rq_nowait(struct request *rq, bool at_head);\nblk_status_t blk_execute_rq(struct request *rq, bool at_head);\nbool blk_rq_is_poll(struct request *rq);\n\nstruct req_iterator {\n\tstruct bvec_iter iter;\n\tstruct bio *bio;\n};\n\n#define __rq_for_each_bio(_bio, rq)\t\\\n\tif ((rq->bio))\t\t\t\\\n\t\tfor (_bio = (rq)->bio; _bio; _bio = _bio->bi_next)\n\n#define rq_for_each_segment(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_segment(bvl, _iter.bio, _iter.iter)\n\n#define rq_for_each_bvec(bvl, _rq, _iter)\t\t\t\\\n\t__rq_for_each_bio(_iter.bio, _rq)\t\t\t\\\n\t\tbio_for_each_bvec(bvl, _iter.bio, _iter.iter)\n\n#define rq_iter_last(bvec, _iter)\t\t\t\t\\\n\t\t(_iter.bio->bi_next == NULL &&\t\t\t\\\n\t\t bio_iter_last(bvec, _iter.iter))\n\n \nstatic inline sector_t blk_rq_pos(const struct request *rq)\n{\n\treturn rq->__sector;\n}\n\nstatic inline unsigned int blk_rq_bytes(const struct request *rq)\n{\n\treturn rq->__data_len;\n}\n\nstatic inline int blk_rq_cur_bytes(const struct request *rq)\n{\n\tif (!rq->bio)\n\t\treturn 0;\n\tif (!bio_has_data(rq->bio))\t \n\t\treturn rq->bio->bi_iter.bi_size;\n\treturn bio_iovec(rq->bio).bv_len;\n}\n\nstatic inline unsigned int blk_rq_sectors(const struct request *rq)\n{\n\treturn blk_rq_bytes(rq) >> SECTOR_SHIFT;\n}\n\nstatic inline unsigned int blk_rq_cur_sectors(const struct request *rq)\n{\n\treturn blk_rq_cur_bytes(rq) >> SECTOR_SHIFT;\n}\n\nstatic inline unsigned int blk_rq_stats_sectors(const struct request *rq)\n{\n\treturn rq->stats_sectors;\n}\n\n \nstatic inline unsigned int blk_rq_payload_bytes(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn rq->special_vec.bv_len;\n\treturn blk_rq_bytes(rq);\n}\n\n \nstatic inline struct bio_vec req_bvec(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn rq->special_vec;\n\treturn mp_bvec_iter_bvec(rq->bio->bi_io_vec, rq->bio->bi_iter);\n}\n\nstatic inline unsigned int blk_rq_count_bios(struct request *rq)\n{\n\tunsigned int nr_bios = 0;\n\tstruct bio *bio;\n\n\t__rq_for_each_bio(bio, rq)\n\t\tnr_bios++;\n\n\treturn nr_bios;\n}\n\nvoid blk_steal_bios(struct bio_list *list, struct request *rq);\n\n \nbool blk_update_request(struct request *rq, blk_status_t error,\n\t\t\t       unsigned int nr_bytes);\nvoid blk_abort_request(struct request *);\n\n \nstatic inline unsigned short blk_rq_nr_phys_segments(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD)\n\t\treturn 1;\n\treturn rq->nr_phys_segments;\n}\n\n \nstatic inline unsigned short blk_rq_nr_discard_segments(struct request *rq)\n{\n\treturn max_t(unsigned short, rq->nr_phys_segments, 1);\n}\n\nint __blk_rq_map_sg(struct request_queue *q, struct request *rq,\n\t\tstruct scatterlist *sglist, struct scatterlist **last_sg);\nstatic inline int blk_rq_map_sg(struct request_queue *q, struct request *rq,\n\t\tstruct scatterlist *sglist)\n{\n\tstruct scatterlist *last_sg = NULL;\n\n\treturn __blk_rq_map_sg(q, rq, sglist, &last_sg);\n}\nvoid blk_dump_rq_flags(struct request *, char *);\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic inline unsigned int blk_rq_zone_no(struct request *rq)\n{\n\treturn disk_zone_no(rq->q->disk, blk_rq_pos(rq));\n}\n\nstatic inline unsigned int blk_rq_zone_is_seq(struct request *rq)\n{\n\treturn disk_zone_is_seq(rq->q->disk, blk_rq_pos(rq));\n}\n\n \nstatic inline bool blk_rq_is_seq_zoned_write(struct request *rq)\n{\n\treturn op_needs_zoned_write_locking(req_op(rq)) &&\n\t\tblk_rq_zone_is_seq(rq);\n}\n\nbool blk_req_needs_zone_write_lock(struct request *rq);\nbool blk_req_zone_write_trylock(struct request *rq);\nvoid __blk_req_zone_write_lock(struct request *rq);\nvoid __blk_req_zone_write_unlock(struct request *rq);\n\nstatic inline void blk_req_zone_write_lock(struct request *rq)\n{\n\tif (blk_req_needs_zone_write_lock(rq))\n\t\t__blk_req_zone_write_lock(rq);\n}\n\nstatic inline void blk_req_zone_write_unlock(struct request *rq)\n{\n\tif (rq->rq_flags & RQF_ZONE_WRITE_LOCKED)\n\t\t__blk_req_zone_write_unlock(rq);\n}\n\nstatic inline bool blk_req_zone_is_write_locked(struct request *rq)\n{\n\treturn rq->q->disk->seq_zones_wlock &&\n\t\ttest_bit(blk_rq_zone_no(rq), rq->q->disk->seq_zones_wlock);\n}\n\nstatic inline bool blk_req_can_dispatch_to_zone(struct request *rq)\n{\n\tif (!blk_req_needs_zone_write_lock(rq))\n\t\treturn true;\n\treturn !blk_req_zone_is_write_locked(rq);\n}\n#else  \nstatic inline bool blk_rq_is_seq_zoned_write(struct request *rq)\n{\n\treturn false;\n}\n\nstatic inline bool blk_req_needs_zone_write_lock(struct request *rq)\n{\n\treturn false;\n}\n\nstatic inline void blk_req_zone_write_lock(struct request *rq)\n{\n}\n\nstatic inline void blk_req_zone_write_unlock(struct request *rq)\n{\n}\nstatic inline bool blk_req_zone_is_write_locked(struct request *rq)\n{\n\treturn false;\n}\n\nstatic inline bool blk_req_can_dispatch_to_zone(struct request *rq)\n{\n\treturn true;\n}\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}