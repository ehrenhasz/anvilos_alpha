{
  "module_name": "skmsg.h",
  "hash_id": "703c7936a00067a17d007081c0a113e4af5a73d33710fe496ab256064c78e5c8",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/skmsg.h",
  "human_readable_source": " \n \n\tstruct scatterlist\t\tdata[MAX_MSG_FRAGS + 2];\n};\n\n \nstruct sk_msg {\n\tstruct sk_msg_sg\t\tsg;\n\tvoid\t\t\t\t*data;\n\tvoid\t\t\t\t*data_end;\n\tu32\t\t\t\tapply_bytes;\n\tu32\t\t\t\tcork_bytes;\n\tu32\t\t\t\tflags;\n\tstruct sk_buff\t\t\t*skb;\n\tstruct sock\t\t\t*sk_redir;\n\tstruct sock\t\t\t*sk;\n\tstruct list_head\t\tlist;\n};\n\nstruct sk_psock_progs {\n\tstruct bpf_prog\t\t\t*msg_parser;\n\tstruct bpf_prog\t\t\t*stream_parser;\n\tstruct bpf_prog\t\t\t*stream_verdict;\n\tstruct bpf_prog\t\t\t*skb_verdict;\n};\n\nenum sk_psock_state_bits {\n\tSK_PSOCK_TX_ENABLED,\n\tSK_PSOCK_RX_STRP_ENABLED,\n};\n\nstruct sk_psock_link {\n\tstruct list_head\t\tlist;\n\tstruct bpf_map\t\t\t*map;\n\tvoid\t\t\t\t*link_raw;\n};\n\nstruct sk_psock_work_state {\n\tu32\t\t\t\tlen;\n\tu32\t\t\t\toff;\n};\n\nstruct sk_psock {\n\tstruct sock\t\t\t*sk;\n\tstruct sock\t\t\t*sk_redir;\n\tu32\t\t\t\tapply_bytes;\n\tu32\t\t\t\tcork_bytes;\n\tu32\t\t\t\teval;\n\tbool\t\t\t\tredir_ingress;  \n\tstruct sk_msg\t\t\t*cork;\n\tstruct sk_psock_progs\t\tprogs;\n#if IS_ENABLED(CONFIG_BPF_STREAM_PARSER)\n\tstruct strparser\t\tstrp;\n#endif\n\tstruct sk_buff_head\t\tingress_skb;\n\tstruct list_head\t\tingress_msg;\n\tspinlock_t\t\t\tingress_lock;\n\tunsigned long\t\t\tstate;\n\tstruct list_head\t\tlink;\n\tspinlock_t\t\t\tlink_lock;\n\trefcount_t\t\t\trefcnt;\n\tvoid (*saved_unhash)(struct sock *sk);\n\tvoid (*saved_destroy)(struct sock *sk);\n\tvoid (*saved_close)(struct sock *sk, long timeout);\n\tvoid (*saved_write_space)(struct sock *sk);\n\tvoid (*saved_data_ready)(struct sock *sk);\n\tint  (*psock_update_sk_prot)(struct sock *sk, struct sk_psock *psock,\n\t\t\t\t     bool restore);\n\tstruct proto\t\t\t*sk_proto;\n\tstruct mutex\t\t\twork_mutex;\n\tstruct sk_psock_work_state\twork_state;\n\tstruct delayed_work\t\twork;\n\tstruct sock\t\t\t*sk_pair;\n\tstruct rcu_work\t\t\trwork;\n};\n\nint sk_msg_alloc(struct sock *sk, struct sk_msg *msg, int len,\n\t\t int elem_first_coalesce);\nint sk_msg_clone(struct sock *sk, struct sk_msg *dst, struct sk_msg *src,\n\t\t u32 off, u32 len);\nvoid sk_msg_trim(struct sock *sk, struct sk_msg *msg, int len);\nint sk_msg_free(struct sock *sk, struct sk_msg *msg);\nint sk_msg_free_nocharge(struct sock *sk, struct sk_msg *msg);\nvoid sk_msg_free_partial(struct sock *sk, struct sk_msg *msg, u32 bytes);\nvoid sk_msg_free_partial_nocharge(struct sock *sk, struct sk_msg *msg,\n\t\t\t\t  u32 bytes);\n\nvoid sk_msg_return(struct sock *sk, struct sk_msg *msg, int bytes);\nvoid sk_msg_return_zero(struct sock *sk, struct sk_msg *msg, int bytes);\n\nint sk_msg_zerocopy_from_iter(struct sock *sk, struct iov_iter *from,\n\t\t\t      struct sk_msg *msg, u32 bytes);\nint sk_msg_memcopy_from_iter(struct sock *sk, struct iov_iter *from,\n\t\t\t     struct sk_msg *msg, u32 bytes);\nint sk_msg_recvmsg(struct sock *sk, struct sk_psock *psock, struct msghdr *msg,\n\t\t   int len, int flags);\nbool sk_msg_is_readable(struct sock *sk);\n\nstatic inline void sk_msg_check_to_free(struct sk_msg *msg, u32 i, u32 bytes)\n{\n\tWARN_ON(i == msg->sg.end && bytes);\n}\n\nstatic inline void sk_msg_apply_bytes(struct sk_psock *psock, u32 bytes)\n{\n\tif (psock->apply_bytes) {\n\t\tif (psock->apply_bytes < bytes)\n\t\t\tpsock->apply_bytes = 0;\n\t\telse\n\t\t\tpsock->apply_bytes -= bytes;\n\t}\n}\n\nstatic inline u32 sk_msg_iter_dist(u32 start, u32 end)\n{\n\treturn end >= start ? end - start : end + (NR_MSG_FRAG_IDS - start);\n}\n\n#define sk_msg_iter_var_prev(var)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (var == 0)\t\t\t\t\\\n\t\t\tvar = NR_MSG_FRAG_IDS - 1;\t\\\n\t\telse\t\t\t\t\t\\\n\t\t\tvar--;\t\t\t\t\\\n\t} while (0)\n\n#define sk_msg_iter_var_next(var)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tvar++;\t\t\t\t\t\\\n\t\tif (var == NR_MSG_FRAG_IDS)\t\t\\\n\t\t\tvar = 0;\t\t\t\\\n\t} while (0)\n\n#define sk_msg_iter_prev(msg, which)\t\t\t\\\n\tsk_msg_iter_var_prev(msg->sg.which)\n\n#define sk_msg_iter_next(msg, which)\t\t\t\\\n\tsk_msg_iter_var_next(msg->sg.which)\n\nstatic inline void sk_msg_init(struct sk_msg *msg)\n{\n\tBUILD_BUG_ON(ARRAY_SIZE(msg->sg.data) - 1 != NR_MSG_FRAG_IDS);\n\tmemset(msg, 0, sizeof(*msg));\n\tsg_init_marker(msg->sg.data, NR_MSG_FRAG_IDS);\n}\n\nstatic inline void sk_msg_xfer(struct sk_msg *dst, struct sk_msg *src,\n\t\t\t       int which, u32 size)\n{\n\tdst->sg.data[which] = src->sg.data[which];\n\tdst->sg.data[which].length  = size;\n\tdst->sg.size\t\t   += size;\n\tsrc->sg.size\t\t   -= size;\n\tsrc->sg.data[which].length -= size;\n\tsrc->sg.data[which].offset += size;\n}\n\nstatic inline void sk_msg_xfer_full(struct sk_msg *dst, struct sk_msg *src)\n{\n\tmemcpy(dst, src, sizeof(*src));\n\tsk_msg_init(src);\n}\n\nstatic inline bool sk_msg_full(const struct sk_msg *msg)\n{\n\treturn sk_msg_iter_dist(msg->sg.start, msg->sg.end) == MAX_MSG_FRAGS;\n}\n\nstatic inline u32 sk_msg_elem_used(const struct sk_msg *msg)\n{\n\treturn sk_msg_iter_dist(msg->sg.start, msg->sg.end);\n}\n\nstatic inline struct scatterlist *sk_msg_elem(struct sk_msg *msg, int which)\n{\n\treturn &msg->sg.data[which];\n}\n\nstatic inline struct scatterlist sk_msg_elem_cpy(struct sk_msg *msg, int which)\n{\n\treturn msg->sg.data[which];\n}\n\nstatic inline struct page *sk_msg_page(struct sk_msg *msg, int which)\n{\n\treturn sg_page(sk_msg_elem(msg, which));\n}\n\nstatic inline bool sk_msg_to_ingress(const struct sk_msg *msg)\n{\n\treturn msg->flags & BPF_F_INGRESS;\n}\n\nstatic inline void sk_msg_compute_data_pointers(struct sk_msg *msg)\n{\n\tstruct scatterlist *sge = sk_msg_elem(msg, msg->sg.start);\n\n\tif (test_bit(msg->sg.start, msg->sg.copy)) {\n\t\tmsg->data = NULL;\n\t\tmsg->data_end = NULL;\n\t} else {\n\t\tmsg->data = sg_virt(sge);\n\t\tmsg->data_end = msg->data + sge->length;\n\t}\n}\n\nstatic inline void sk_msg_page_add(struct sk_msg *msg, struct page *page,\n\t\t\t\t   u32 len, u32 offset)\n{\n\tstruct scatterlist *sge;\n\n\tget_page(page);\n\tsge = sk_msg_elem(msg, msg->sg.end);\n\tsg_set_page(sge, page, len, offset);\n\tsg_unmark_end(sge);\n\n\t__set_bit(msg->sg.end, msg->sg.copy);\n\tmsg->sg.size += len;\n\tsk_msg_iter_next(msg, end);\n}\n\nstatic inline void sk_msg_sg_copy(struct sk_msg *msg, u32 i, bool copy_state)\n{\n\tdo {\n\t\tif (copy_state)\n\t\t\t__set_bit(i, msg->sg.copy);\n\t\telse\n\t\t\t__clear_bit(i, msg->sg.copy);\n\t\tsk_msg_iter_var_next(i);\n\t\tif (i == msg->sg.end)\n\t\t\tbreak;\n\t} while (1);\n}\n\nstatic inline void sk_msg_sg_copy_set(struct sk_msg *msg, u32 start)\n{\n\tsk_msg_sg_copy(msg, start, true);\n}\n\nstatic inline void sk_msg_sg_copy_clear(struct sk_msg *msg, u32 start)\n{\n\tsk_msg_sg_copy(msg, start, false);\n}\n\nstatic inline struct sk_psock *sk_psock(const struct sock *sk)\n{\n\treturn __rcu_dereference_sk_user_data_with_flags(sk,\n\t\t\t\t\t\t\t SK_USER_DATA_PSOCK);\n}\n\nstatic inline void sk_psock_set_state(struct sk_psock *psock,\n\t\t\t\t      enum sk_psock_state_bits bit)\n{\n\tset_bit(bit, &psock->state);\n}\n\nstatic inline void sk_psock_clear_state(struct sk_psock *psock,\n\t\t\t\t\tenum sk_psock_state_bits bit)\n{\n\tclear_bit(bit, &psock->state);\n}\n\nstatic inline bool sk_psock_test_state(const struct sk_psock *psock,\n\t\t\t\t       enum sk_psock_state_bits bit)\n{\n\treturn test_bit(bit, &psock->state);\n}\n\nstatic inline void sock_drop(struct sock *sk, struct sk_buff *skb)\n{\n\tsk_drops_add(sk, skb);\n\tkfree_skb(skb);\n}\n\nstatic inline void sk_psock_queue_msg(struct sk_psock *psock,\n\t\t\t\t      struct sk_msg *msg)\n{\n\tspin_lock_bh(&psock->ingress_lock);\n\tif (sk_psock_test_state(psock, SK_PSOCK_TX_ENABLED))\n\t\tlist_add_tail(&msg->list, &psock->ingress_msg);\n\telse {\n\t\tsk_msg_free(psock->sk, msg);\n\t\tkfree(msg);\n\t}\n\tspin_unlock_bh(&psock->ingress_lock);\n}\n\nstatic inline struct sk_msg *sk_psock_dequeue_msg(struct sk_psock *psock)\n{\n\tstruct sk_msg *msg;\n\n\tspin_lock_bh(&psock->ingress_lock);\n\tmsg = list_first_entry_or_null(&psock->ingress_msg, struct sk_msg, list);\n\tif (msg)\n\t\tlist_del(&msg->list);\n\tspin_unlock_bh(&psock->ingress_lock);\n\treturn msg;\n}\n\nstatic inline struct sk_msg *sk_psock_peek_msg(struct sk_psock *psock)\n{\n\tstruct sk_msg *msg;\n\n\tspin_lock_bh(&psock->ingress_lock);\n\tmsg = list_first_entry_or_null(&psock->ingress_msg, struct sk_msg, list);\n\tspin_unlock_bh(&psock->ingress_lock);\n\treturn msg;\n}\n\nstatic inline struct sk_msg *sk_psock_next_msg(struct sk_psock *psock,\n\t\t\t\t\t       struct sk_msg *msg)\n{\n\tstruct sk_msg *ret;\n\n\tspin_lock_bh(&psock->ingress_lock);\n\tif (list_is_last(&msg->list, &psock->ingress_msg))\n\t\tret = NULL;\n\telse\n\t\tret = list_next_entry(msg, list);\n\tspin_unlock_bh(&psock->ingress_lock);\n\treturn ret;\n}\n\nstatic inline bool sk_psock_queue_empty(const struct sk_psock *psock)\n{\n\treturn psock ? list_empty(&psock->ingress_msg) : true;\n}\n\nstatic inline void kfree_sk_msg(struct sk_msg *msg)\n{\n\tif (msg->skb)\n\t\tconsume_skb(msg->skb);\n\tkfree(msg);\n}\n\nstatic inline void sk_psock_report_error(struct sk_psock *psock, int err)\n{\n\tstruct sock *sk = psock->sk;\n\n\tsk->sk_err = err;\n\tsk_error_report(sk);\n}\n\nstruct sk_psock *sk_psock_init(struct sock *sk, int node);\nvoid sk_psock_stop(struct sk_psock *psock);\n\n#if IS_ENABLED(CONFIG_BPF_STREAM_PARSER)\nint sk_psock_init_strp(struct sock *sk, struct sk_psock *psock);\nvoid sk_psock_start_strp(struct sock *sk, struct sk_psock *psock);\nvoid sk_psock_stop_strp(struct sock *sk, struct sk_psock *psock);\n#else\nstatic inline int sk_psock_init_strp(struct sock *sk, struct sk_psock *psock)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void sk_psock_start_strp(struct sock *sk, struct sk_psock *psock)\n{\n}\n\nstatic inline void sk_psock_stop_strp(struct sock *sk, struct sk_psock *psock)\n{\n}\n#endif\n\nvoid sk_psock_start_verdict(struct sock *sk, struct sk_psock *psock);\nvoid sk_psock_stop_verdict(struct sock *sk, struct sk_psock *psock);\n\nint sk_psock_msg_verdict(struct sock *sk, struct sk_psock *psock,\n\t\t\t struct sk_msg *msg);\n\nstatic inline struct sk_psock_link *sk_psock_init_link(void)\n{\n\treturn kzalloc(sizeof(struct sk_psock_link),\n\t\t       GFP_ATOMIC | __GFP_NOWARN);\n}\n\nstatic inline void sk_psock_free_link(struct sk_psock_link *link)\n{\n\tkfree(link);\n}\n\nstruct sk_psock_link *sk_psock_link_pop(struct sk_psock *psock);\n\nstatic inline void sk_psock_cork_free(struct sk_psock *psock)\n{\n\tif (psock->cork) {\n\t\tsk_msg_free(psock->sk, psock->cork);\n\t\tkfree(psock->cork);\n\t\tpsock->cork = NULL;\n\t}\n}\n\nstatic inline void sk_psock_restore_proto(struct sock *sk,\n\t\t\t\t\t  struct sk_psock *psock)\n{\n\tif (psock->psock_update_sk_prot)\n\t\tpsock->psock_update_sk_prot(sk, psock, true);\n}\n\nstatic inline struct sk_psock *sk_psock_get(struct sock *sk)\n{\n\tstruct sk_psock *psock;\n\n\trcu_read_lock();\n\tpsock = sk_psock(sk);\n\tif (psock && !refcount_inc_not_zero(&psock->refcnt))\n\t\tpsock = NULL;\n\trcu_read_unlock();\n\treturn psock;\n}\n\nvoid sk_psock_drop(struct sock *sk, struct sk_psock *psock);\n\nstatic inline void sk_psock_put(struct sock *sk, struct sk_psock *psock)\n{\n\tif (refcount_dec_and_test(&psock->refcnt))\n\t\tsk_psock_drop(sk, psock);\n}\n\nstatic inline void sk_psock_data_ready(struct sock *sk, struct sk_psock *psock)\n{\n\tif (psock->saved_data_ready)\n\t\tpsock->saved_data_ready(sk);\n\telse\n\t\tsk->sk_data_ready(sk);\n}\n\nstatic inline void psock_set_prog(struct bpf_prog **pprog,\n\t\t\t\t  struct bpf_prog *prog)\n{\n\tprog = xchg(pprog, prog);\n\tif (prog)\n\t\tbpf_prog_put(prog);\n}\n\nstatic inline int psock_replace_prog(struct bpf_prog **pprog,\n\t\t\t\t     struct bpf_prog *prog,\n\t\t\t\t     struct bpf_prog *old)\n{\n\tif (cmpxchg(pprog, old, prog) != old)\n\t\treturn -ENOENT;\n\n\tif (old)\n\t\tbpf_prog_put(old);\n\n\treturn 0;\n}\n\nstatic inline void psock_progs_drop(struct sk_psock_progs *progs)\n{\n\tpsock_set_prog(&progs->msg_parser, NULL);\n\tpsock_set_prog(&progs->stream_parser, NULL);\n\tpsock_set_prog(&progs->stream_verdict, NULL);\n\tpsock_set_prog(&progs->skb_verdict, NULL);\n}\n\nint sk_psock_tls_strp_read(struct sk_psock *psock, struct sk_buff *skb);\n\nstatic inline bool sk_psock_strp_enabled(struct sk_psock *psock)\n{\n\tif (!psock)\n\t\treturn false;\n\treturn !!psock->saved_data_ready;\n}\n\nstatic inline bool sk_is_udp(const struct sock *sk)\n{\n\treturn sk->sk_type == SOCK_DGRAM &&\n\t       sk->sk_protocol == IPPROTO_UDP;\n}\n\n#if IS_ENABLED(CONFIG_NET_SOCK_MSG)\n\n#define BPF_F_STRPARSER\t(1UL << 1)\n\n \n#define BPF_F_PTR_MASK ~(BPF_F_INGRESS | BPF_F_STRPARSER)\n\nstatic inline bool skb_bpf_strparser(const struct sk_buff *skb)\n{\n\tunsigned long sk_redir = skb->_sk_redir;\n\n\treturn sk_redir & BPF_F_STRPARSER;\n}\n\nstatic inline void skb_bpf_set_strparser(struct sk_buff *skb)\n{\n\tskb->_sk_redir |= BPF_F_STRPARSER;\n}\n\nstatic inline bool skb_bpf_ingress(const struct sk_buff *skb)\n{\n\tunsigned long sk_redir = skb->_sk_redir;\n\n\treturn sk_redir & BPF_F_INGRESS;\n}\n\nstatic inline void skb_bpf_set_ingress(struct sk_buff *skb)\n{\n\tskb->_sk_redir |= BPF_F_INGRESS;\n}\n\nstatic inline void skb_bpf_set_redir(struct sk_buff *skb, struct sock *sk_redir,\n\t\t\t\t     bool ingress)\n{\n\tskb->_sk_redir = (unsigned long)sk_redir;\n\tif (ingress)\n\t\tskb->_sk_redir |= BPF_F_INGRESS;\n}\n\nstatic inline struct sock *skb_bpf_redirect_fetch(const struct sk_buff *skb)\n{\n\tunsigned long sk_redir = skb->_sk_redir;\n\n\treturn (struct sock *)(sk_redir & BPF_F_PTR_MASK);\n}\n\nstatic inline void skb_bpf_redirect_clear(struct sk_buff *skb)\n{\n\tskb->_sk_redir = 0;\n}\n#endif  \n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}