{
  "module_name": "mm.h",
  "hash_id": "e5457a391320c0a2454e11b78cabdcd90602fd1c27e23ca798141e6e2cc999df",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/mm.h",
  "human_readable_source": " \n#ifndef _LINUX_MM_H\n#define _LINUX_MM_H\n\n#include <linux/errno.h>\n#include <linux/mmdebug.h>\n#include <linux/gfp.h>\n#include <linux/bug.h>\n#include <linux/list.h>\n#include <linux/mmzone.h>\n#include <linux/rbtree.h>\n#include <linux/atomic.h>\n#include <linux/debug_locks.h>\n#include <linux/mm_types.h>\n#include <linux/mmap_lock.h>\n#include <linux/range.h>\n#include <linux/pfn.h>\n#include <linux/percpu-refcount.h>\n#include <linux/bit_spinlock.h>\n#include <linux/shrinker.h>\n#include <linux/resource.h>\n#include <linux/page_ext.h>\n#include <linux/err.h>\n#include <linux/page-flags.h>\n#include <linux/page_ref.h>\n#include <linux/overflow.h>\n#include <linux/sizes.h>\n#include <linux/sched.h>\n#include <linux/pgtable.h>\n#include <linux/kasan.h>\n#include <linux/memremap.h>\n#include <linux/slab.h>\n\nstruct mempolicy;\nstruct anon_vma;\nstruct anon_vma_chain;\nstruct user_struct;\nstruct pt_regs;\n\nextern int sysctl_page_lock_unfairness;\n\nvoid mm_core_init(void);\nvoid init_mm_internals(void);\n\n#ifndef CONFIG_NUMA\t\t \nextern unsigned long max_mapnr;\n\nstatic inline void set_max_mapnr(unsigned long limit)\n{\n\tmax_mapnr = limit;\n}\n#else\nstatic inline void set_max_mapnr(unsigned long limit) { }\n#endif\n\nextern atomic_long_t _totalram_pages;\nstatic inline unsigned long totalram_pages(void)\n{\n\treturn (unsigned long)atomic_long_read(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_inc(void)\n{\n\tatomic_long_inc(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_dec(void)\n{\n\tatomic_long_dec(&_totalram_pages);\n}\n\nstatic inline void totalram_pages_add(long count)\n{\n\tatomic_long_add(count, &_totalram_pages);\n}\n\nextern void * high_memory;\nextern int page_cluster;\nextern const int page_cluster_max;\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_legacy_va_layout;\n#else\n#define sysctl_legacy_va_layout 0\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_BITS\nextern const int mmap_rnd_bits_min;\nextern const int mmap_rnd_bits_max;\nextern int mmap_rnd_bits __read_mostly;\n#endif\n#ifdef CONFIG_HAVE_ARCH_MMAP_RND_COMPAT_BITS\nextern const int mmap_rnd_compat_bits_min;\nextern const int mmap_rnd_compat_bits_max;\nextern int mmap_rnd_compat_bits __read_mostly;\n#endif\n\n#include <asm/page.h>\n#include <asm/processor.h>\n\n#ifndef __pa_symbol\n#define __pa_symbol(x)  __pa(RELOC_HIDE((unsigned long)(x), 0))\n#endif\n\n#ifndef page_to_virt\n#define page_to_virt(x)\t__va(PFN_PHYS(page_to_pfn(x)))\n#endif\n\n#ifndef lm_alias\n#define lm_alias(x)\t__va(__pa_symbol(x))\n#endif\n\n \n#ifndef mm_forbids_zeropage\n#define mm_forbids_zeropage(X)\t(0)\n#endif\n\n \n#if BITS_PER_LONG == 64\n \n#define\tmm_zero_struct_page(pp) __mm_zero_struct_page(pp)\nstatic inline void __mm_zero_struct_page(struct page *page)\n{\n\tunsigned long *_pp = (void *)page;\n\n\t  \n\tBUILD_BUG_ON(sizeof(struct page) & 7);\n\tBUILD_BUG_ON(sizeof(struct page) < 56);\n\tBUILD_BUG_ON(sizeof(struct page) > 96);\n\n\tswitch (sizeof(struct page)) {\n\tcase 96:\n\t\t_pp[11] = 0;\n\t\tfallthrough;\n\tcase 88:\n\t\t_pp[10] = 0;\n\t\tfallthrough;\n\tcase 80:\n\t\t_pp[9] = 0;\n\t\tfallthrough;\n\tcase 72:\n\t\t_pp[8] = 0;\n\t\tfallthrough;\n\tcase 64:\n\t\t_pp[7] = 0;\n\t\tfallthrough;\n\tcase 56:\n\t\t_pp[6] = 0;\n\t\t_pp[5] = 0;\n\t\t_pp[4] = 0;\n\t\t_pp[3] = 0;\n\t\t_pp[2] = 0;\n\t\t_pp[1] = 0;\n\t\t_pp[0] = 0;\n\t}\n}\n#else\n#define mm_zero_struct_page(pp)  ((void)memset((pp), 0, sizeof(struct page)))\n#endif\n\n \n#define MAPCOUNT_ELF_CORE_MARGIN\t(5)\n#define DEFAULT_MAX_MAP_COUNT\t(USHRT_MAX - MAPCOUNT_ELF_CORE_MARGIN)\n\nextern int sysctl_max_map_count;\n\nextern unsigned long sysctl_user_reserve_kbytes;\nextern unsigned long sysctl_admin_reserve_kbytes;\n\nextern int sysctl_overcommit_memory;\nextern int sysctl_overcommit_ratio;\nextern unsigned long sysctl_overcommit_kbytes;\n\nint overcommit_ratio_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\nint overcommit_kbytes_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\nint overcommit_policy_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define nth_page(page,n) pfn_to_page(page_to_pfn((page)) + (n))\n#define folio_page_idx(folio, p)\t(page_to_pfn(p) - folio_pfn(folio))\n#else\n#define nth_page(page,n) ((page) + (n))\n#define folio_page_idx(folio, p)\t((p) - &(folio)->page)\n#endif\n\n \n#define PAGE_ALIGN(addr) ALIGN(addr, PAGE_SIZE)\n\n \n#define PAGE_ALIGN_DOWN(addr) ALIGN_DOWN(addr, PAGE_SIZE)\n\n \n#define PAGE_ALIGNED(addr)\tIS_ALIGNED((unsigned long)(addr), PAGE_SIZE)\n\n#define lru_to_page(head) (list_entry((head)->prev, struct page, lru))\nstatic inline struct folio *lru_to_folio(struct list_head *head)\n{\n\treturn list_entry((head)->prev, struct folio, lru);\n}\n\nvoid setup_initial_init_mm(void *start_code, void *end_code,\n\t\t\t   void *end_data, void *brk);\n\n \n\nstruct vm_area_struct *vm_area_alloc(struct mm_struct *);\nstruct vm_area_struct *vm_area_dup(struct vm_area_struct *);\nvoid vm_area_free(struct vm_area_struct *);\n \nvoid __vm_area_free(struct vm_area_struct *vma);\n\n#ifndef CONFIG_MMU\nextern struct rb_root nommu_region_tree;\nextern struct rw_semaphore nommu_region_sem;\n\nextern unsigned int kobjsize(const void *objp);\n#endif\n\n \n#define VM_NONE\t\t0x00000000\n\n#define VM_READ\t\t0x00000001\t \n#define VM_WRITE\t0x00000002\n#define VM_EXEC\t\t0x00000004\n#define VM_SHARED\t0x00000008\n\n \n#define VM_MAYREAD\t0x00000010\t \n#define VM_MAYWRITE\t0x00000020\n#define VM_MAYEXEC\t0x00000040\n#define VM_MAYSHARE\t0x00000080\n\n#define VM_GROWSDOWN\t0x00000100\t \n#ifdef CONFIG_MMU\n#define VM_UFFD_MISSING\t0x00000200\t \n#else  \n#define VM_MAYOVERLAY\t0x00000200\t \n#define VM_UFFD_MISSING\t0\n#endif  \n#define VM_PFNMAP\t0x00000400\t \n#define VM_UFFD_WP\t0x00001000\t \n\n#define VM_LOCKED\t0x00002000\n#define VM_IO           0x00004000\t \n\n\t\t\t\t\t \n#define VM_SEQ_READ\t0x00008000\t \n#define VM_RAND_READ\t0x00010000\t \n\n#define VM_DONTCOPY\t0x00020000       \n#define VM_DONTEXPAND\t0x00040000\t \n#define VM_LOCKONFAULT\t0x00080000\t \n#define VM_ACCOUNT\t0x00100000\t \n#define VM_NORESERVE\t0x00200000\t \n#define VM_HUGETLB\t0x00400000\t \n#define VM_SYNC\t\t0x00800000\t \n#define VM_ARCH_1\t0x01000000\t \n#define VM_WIPEONFORK\t0x02000000\t \n#define VM_DONTDUMP\t0x04000000\t \n\n#ifdef CONFIG_MEM_SOFT_DIRTY\n# define VM_SOFTDIRTY\t0x08000000\t \n#else\n# define VM_SOFTDIRTY\t0\n#endif\n\n#define VM_MIXEDMAP\t0x10000000\t \n#define VM_HUGEPAGE\t0x20000000\t \n#define VM_NOHUGEPAGE\t0x40000000\t \n#define VM_MERGEABLE\t0x80000000\t \n\n#ifdef CONFIG_ARCH_USES_HIGH_VMA_FLAGS\n#define VM_HIGH_ARCH_BIT_0\t32\t \n#define VM_HIGH_ARCH_BIT_1\t33\t \n#define VM_HIGH_ARCH_BIT_2\t34\t \n#define VM_HIGH_ARCH_BIT_3\t35\t \n#define VM_HIGH_ARCH_BIT_4\t36\t \n#define VM_HIGH_ARCH_BIT_5\t37\t \n#define VM_HIGH_ARCH_0\tBIT(VM_HIGH_ARCH_BIT_0)\n#define VM_HIGH_ARCH_1\tBIT(VM_HIGH_ARCH_BIT_1)\n#define VM_HIGH_ARCH_2\tBIT(VM_HIGH_ARCH_BIT_2)\n#define VM_HIGH_ARCH_3\tBIT(VM_HIGH_ARCH_BIT_3)\n#define VM_HIGH_ARCH_4\tBIT(VM_HIGH_ARCH_BIT_4)\n#define VM_HIGH_ARCH_5\tBIT(VM_HIGH_ARCH_BIT_5)\n#endif  \n\n#ifdef CONFIG_ARCH_HAS_PKEYS\n# define VM_PKEY_SHIFT\tVM_HIGH_ARCH_BIT_0\n# define VM_PKEY_BIT0\tVM_HIGH_ARCH_0\t \n# define VM_PKEY_BIT1\tVM_HIGH_ARCH_1\t \n# define VM_PKEY_BIT2\tVM_HIGH_ARCH_2\n# define VM_PKEY_BIT3\tVM_HIGH_ARCH_3\n#ifdef CONFIG_PPC\n# define VM_PKEY_BIT4  VM_HIGH_ARCH_4\n#else\n# define VM_PKEY_BIT4  0\n#endif\n#endif  \n\n#ifdef CONFIG_X86_USER_SHADOW_STACK\n \n# define VM_SHADOW_STACK\tVM_HIGH_ARCH_5\n#else\n# define VM_SHADOW_STACK\tVM_NONE\n#endif\n\n#if defined(CONFIG_X86)\n# define VM_PAT\t\tVM_ARCH_1\t \n#elif defined(CONFIG_PPC)\n# define VM_SAO\t\tVM_ARCH_1\t \n#elif defined(CONFIG_PARISC)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_IA64)\n# define VM_GROWSUP\tVM_ARCH_1\n#elif defined(CONFIG_SPARC64)\n# define VM_SPARC_ADI\tVM_ARCH_1\t \n# define VM_ARCH_CLEAR\tVM_SPARC_ADI\n#elif defined(CONFIG_ARM64)\n# define VM_ARM64_BTI\tVM_ARCH_1\t \n# define VM_ARCH_CLEAR\tVM_ARM64_BTI\n#elif !defined(CONFIG_MMU)\n# define VM_MAPPED_COPY\tVM_ARCH_1\t \n#endif\n\n#if defined(CONFIG_ARM64_MTE)\n# define VM_MTE\t\tVM_HIGH_ARCH_0\t \n# define VM_MTE_ALLOWED\tVM_HIGH_ARCH_1\t \n#else\n# define VM_MTE\t\tVM_NONE\n# define VM_MTE_ALLOWED\tVM_NONE\n#endif\n\n#ifndef VM_GROWSUP\n# define VM_GROWSUP\tVM_NONE\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR\n# define VM_UFFD_MINOR_BIT\t38\n# define VM_UFFD_MINOR\t\tBIT(VM_UFFD_MINOR_BIT)\t \n#else  \n# define VM_UFFD_MINOR\t\tVM_NONE\n#endif  \n\n \n#define VM_STACK_INCOMPLETE_SETUP (VM_RAND_READ | VM_SEQ_READ | VM_STACK_EARLY)\n\n#define TASK_EXEC ((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0)\n\n \n#define VM_DATA_FLAGS_TSK_EXEC\t(VM_READ | VM_WRITE | TASK_EXEC | \\\n\t\t\t\t VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)\n#define VM_DATA_FLAGS_NON_EXEC\t(VM_READ | VM_WRITE | VM_MAYREAD | \\\n\t\t\t\t VM_MAYWRITE | VM_MAYEXEC)\n#define VM_DATA_FLAGS_EXEC\t(VM_READ | VM_WRITE | VM_EXEC | \\\n\t\t\t\t VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)\n\n#ifndef VM_DATA_DEFAULT_FLAGS\t\t \n#define VM_DATA_DEFAULT_FLAGS  VM_DATA_FLAGS_EXEC\n#endif\n\n#ifndef VM_STACK_DEFAULT_FLAGS\t\t \n#define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS\n#endif\n\n#define VM_STARTGAP_FLAGS (VM_GROWSDOWN | VM_SHADOW_STACK)\n\n#ifdef CONFIG_STACK_GROWSUP\n#define VM_STACK\tVM_GROWSUP\n#define VM_STACK_EARLY\tVM_GROWSDOWN\n#else\n#define VM_STACK\tVM_GROWSDOWN\n#define VM_STACK_EARLY\t0\n#endif\n\n#define VM_STACK_FLAGS\t(VM_STACK | VM_STACK_DEFAULT_FLAGS | VM_ACCOUNT)\n\n \n#define VM_ACCESS_FLAGS (VM_READ | VM_WRITE | VM_EXEC)\n\n\n \n#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_PFNMAP | VM_MIXEDMAP)\n\n \n#define VM_NO_KHUGEPAGED (VM_SPECIAL | VM_HUGETLB)\n\n \n#define VM_INIT_DEF_MASK\tVM_NOHUGEPAGE\n\n \n#define VM_LOCKED_MASK\t(VM_LOCKED | VM_LOCKONFAULT)\n\n \n#ifndef VM_ARCH_CLEAR\n# define VM_ARCH_CLEAR\tVM_NONE\n#endif\n#define VM_FLAGS_CLEAR\t(ARCH_VM_PKEY_FLAGS | VM_ARCH_CLEAR)\n\n \n\n \n#define FAULT_FLAG_DEFAULT  (FAULT_FLAG_ALLOW_RETRY | \\\n\t\t\t     FAULT_FLAG_KILLABLE | \\\n\t\t\t     FAULT_FLAG_INTERRUPTIBLE)\n\n \nstatic inline bool fault_flag_allow_retry_first(enum fault_flag flags)\n{\n\treturn (flags & FAULT_FLAG_ALLOW_RETRY) &&\n\t    (!(flags & FAULT_FLAG_TRIED));\n}\n\n#define FAULT_FLAG_TRACE \\\n\t{ FAULT_FLAG_WRITE,\t\t\"WRITE\" }, \\\n\t{ FAULT_FLAG_MKWRITE,\t\t\"MKWRITE\" }, \\\n\t{ FAULT_FLAG_ALLOW_RETRY,\t\"ALLOW_RETRY\" }, \\\n\t{ FAULT_FLAG_RETRY_NOWAIT,\t\"RETRY_NOWAIT\" }, \\\n\t{ FAULT_FLAG_KILLABLE,\t\t\"KILLABLE\" }, \\\n\t{ FAULT_FLAG_TRIED,\t\t\"TRIED\" }, \\\n\t{ FAULT_FLAG_USER,\t\t\"USER\" }, \\\n\t{ FAULT_FLAG_REMOTE,\t\t\"REMOTE\" }, \\\n\t{ FAULT_FLAG_INSTRUCTION,\t\"INSTRUCTION\" }, \\\n\t{ FAULT_FLAG_INTERRUPTIBLE,\t\"INTERRUPTIBLE\" }, \\\n\t{ FAULT_FLAG_VMA_LOCK,\t\t\"VMA_LOCK\" }\n\n \nstruct vm_fault {\n\tconst struct {\n\t\tstruct vm_area_struct *vma;\t \n\t\tgfp_t gfp_mask;\t\t\t \n\t\tpgoff_t pgoff;\t\t\t \n\t\tunsigned long address;\t\t \n\t\tunsigned long real_address;\t \n\t};\n\tenum fault_flag flags;\t\t \n\tpmd_t *pmd;\t\t\t \n\tpud_t *pud;\t\t\t \n\tunion {\n\t\tpte_t orig_pte;\t\t \n\t\tpmd_t orig_pmd;\t\t \n\t};\n\n\tstruct page *cow_page;\t\t \n\tstruct page *page;\t\t \n\t \n\tpte_t *pte;\t\t\t \n\tspinlock_t *ptl;\t\t \n\tpgtable_t prealloc_pte;\t\t \n};\n\n \nstruct vm_operations_struct {\n\tvoid (*open)(struct vm_area_struct * area);\n\t \n\tvoid (*close)(struct vm_area_struct * area);\n\t \n\tint (*may_split)(struct vm_area_struct *area, unsigned long addr);\n\tint (*mremap)(struct vm_area_struct *area);\n\t \n\tint (*mprotect)(struct vm_area_struct *vma, unsigned long start,\n\t\t\tunsigned long end, unsigned long newflags);\n\tvm_fault_t (*fault)(struct vm_fault *vmf);\n\tvm_fault_t (*huge_fault)(struct vm_fault *vmf, unsigned int order);\n\tvm_fault_t (*map_pages)(struct vm_fault *vmf,\n\t\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\n\tunsigned long (*pagesize)(struct vm_area_struct * area);\n\n\t \n\tvm_fault_t (*page_mkwrite)(struct vm_fault *vmf);\n\n\t \n\tvm_fault_t (*pfn_mkwrite)(struct vm_fault *vmf);\n\n\t \n\tint (*access)(struct vm_area_struct *vma, unsigned long addr,\n\t\t      void *buf, int len, int write);\n\n\t \n\tconst char *(*name)(struct vm_area_struct *vma);\n\n#ifdef CONFIG_NUMA\n\t \n\tint (*set_policy)(struct vm_area_struct *vma, struct mempolicy *new);\n\n\t \n\tstruct mempolicy *(*get_policy)(struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr);\n#endif\n\t \n\tstruct page *(*find_special_page)(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr);\n};\n\n#ifdef CONFIG_NUMA_BALANCING\nstatic inline void vma_numab_state_init(struct vm_area_struct *vma)\n{\n\tvma->numab_state = NULL;\n}\nstatic inline void vma_numab_state_free(struct vm_area_struct *vma)\n{\n\tkfree(vma->numab_state);\n}\n#else\nstatic inline void vma_numab_state_init(struct vm_area_struct *vma) {}\nstatic inline void vma_numab_state_free(struct vm_area_struct *vma) {}\n#endif  \n\n#ifdef CONFIG_PER_VMA_LOCK\n \nstatic inline bool vma_start_read(struct vm_area_struct *vma)\n{\n\t \n\tif (READ_ONCE(vma->vm_lock_seq) == READ_ONCE(vma->vm_mm->mm_lock_seq))\n\t\treturn false;\n\n\tif (unlikely(down_read_trylock(&vma->vm_lock->lock) == 0))\n\t\treturn false;\n\n\t \n\tif (unlikely(vma->vm_lock_seq == smp_load_acquire(&vma->vm_mm->mm_lock_seq))) {\n\t\tup_read(&vma->vm_lock->lock);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic inline void vma_end_read(struct vm_area_struct *vma)\n{\n\trcu_read_lock();  \n\tup_read(&vma->vm_lock->lock);\n\trcu_read_unlock();\n}\n\n \nstatic bool __is_vma_write_locked(struct vm_area_struct *vma, int *mm_lock_seq)\n{\n\tmmap_assert_write_locked(vma->vm_mm);\n\n\t \n\t*mm_lock_seq = vma->vm_mm->mm_lock_seq;\n\treturn (vma->vm_lock_seq == *mm_lock_seq);\n}\n\n \nstatic inline void vma_start_write(struct vm_area_struct *vma)\n{\n\tint mm_lock_seq;\n\n\tif (__is_vma_write_locked(vma, &mm_lock_seq))\n\t\treturn;\n\n\tdown_write(&vma->vm_lock->lock);\n\t \n\tWRITE_ONCE(vma->vm_lock_seq, mm_lock_seq);\n\tup_write(&vma->vm_lock->lock);\n}\n\nstatic inline void vma_assert_write_locked(struct vm_area_struct *vma)\n{\n\tint mm_lock_seq;\n\n\tVM_BUG_ON_VMA(!__is_vma_write_locked(vma, &mm_lock_seq), vma);\n}\n\nstatic inline void vma_assert_locked(struct vm_area_struct *vma)\n{\n\tif (!rwsem_is_locked(&vma->vm_lock->lock))\n\t\tvma_assert_write_locked(vma);\n}\n\nstatic inline void vma_mark_detached(struct vm_area_struct *vma, bool detached)\n{\n\t \n\tif (detached)\n\t\tvma_assert_write_locked(vma);\n\tvma->detached = detached;\n}\n\nstatic inline void release_fault_lock(struct vm_fault *vmf)\n{\n\tif (vmf->flags & FAULT_FLAG_VMA_LOCK)\n\t\tvma_end_read(vmf->vma);\n\telse\n\t\tmmap_read_unlock(vmf->vma->vm_mm);\n}\n\nstatic inline void assert_fault_locked(struct vm_fault *vmf)\n{\n\tif (vmf->flags & FAULT_FLAG_VMA_LOCK)\n\t\tvma_assert_locked(vmf->vma);\n\telse\n\t\tmmap_assert_locked(vmf->vma->vm_mm);\n}\n\nstruct vm_area_struct *lock_vma_under_rcu(struct mm_struct *mm,\n\t\t\t\t\t  unsigned long address);\n\n#else  \n\nstatic inline bool vma_start_read(struct vm_area_struct *vma)\n\t\t{ return false; }\nstatic inline void vma_end_read(struct vm_area_struct *vma) {}\nstatic inline void vma_start_write(struct vm_area_struct *vma) {}\nstatic inline void vma_assert_write_locked(struct vm_area_struct *vma)\n\t\t{ mmap_assert_write_locked(vma->vm_mm); }\nstatic inline void vma_mark_detached(struct vm_area_struct *vma,\n\t\t\t\t     bool detached) {}\n\nstatic inline struct vm_area_struct *lock_vma_under_rcu(struct mm_struct *mm,\n\t\tunsigned long address)\n{\n\treturn NULL;\n}\n\nstatic inline void release_fault_lock(struct vm_fault *vmf)\n{\n\tmmap_read_unlock(vmf->vma->vm_mm);\n}\n\nstatic inline void assert_fault_locked(struct vm_fault *vmf)\n{\n\tmmap_assert_locked(vmf->vma->vm_mm);\n}\n\n#endif  \n\nextern const struct vm_operations_struct vma_dummy_vm_ops;\n\n \nstatic inline void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)\n{\n\tmemset(vma, 0, sizeof(*vma));\n\tvma->vm_mm = mm;\n\tvma->vm_ops = &vma_dummy_vm_ops;\n\tINIT_LIST_HEAD(&vma->anon_vma_chain);\n\tvma_mark_detached(vma, false);\n\tvma_numab_state_init(vma);\n}\n\n \nstatic inline void vm_flags_init(struct vm_area_struct *vma,\n\t\t\t\t vm_flags_t flags)\n{\n\tACCESS_PRIVATE(vma, __vm_flags) = flags;\n}\n\n \nstatic inline void vm_flags_reset(struct vm_area_struct *vma,\n\t\t\t\t  vm_flags_t flags)\n{\n\tvma_assert_write_locked(vma);\n\tvm_flags_init(vma, flags);\n}\n\nstatic inline void vm_flags_reset_once(struct vm_area_struct *vma,\n\t\t\t\t       vm_flags_t flags)\n{\n\tvma_assert_write_locked(vma);\n\tWRITE_ONCE(ACCESS_PRIVATE(vma, __vm_flags), flags);\n}\n\nstatic inline void vm_flags_set(struct vm_area_struct *vma,\n\t\t\t\tvm_flags_t flags)\n{\n\tvma_start_write(vma);\n\tACCESS_PRIVATE(vma, __vm_flags) |= flags;\n}\n\nstatic inline void vm_flags_clear(struct vm_area_struct *vma,\n\t\t\t\t  vm_flags_t flags)\n{\n\tvma_start_write(vma);\n\tACCESS_PRIVATE(vma, __vm_flags) &= ~flags;\n}\n\n \nstatic inline void __vm_flags_mod(struct vm_area_struct *vma,\n\t\t\t\t  vm_flags_t set, vm_flags_t clear)\n{\n\tvm_flags_init(vma, (vma->vm_flags | set) & ~clear);\n}\n\n \nstatic inline void vm_flags_mod(struct vm_area_struct *vma,\n\t\t\t\tvm_flags_t set, vm_flags_t clear)\n{\n\tvma_start_write(vma);\n\t__vm_flags_mod(vma, set, clear);\n}\n\nstatic inline void vma_set_anonymous(struct vm_area_struct *vma)\n{\n\tvma->vm_ops = NULL;\n}\n\nstatic inline bool vma_is_anonymous(struct vm_area_struct *vma)\n{\n\treturn !vma->vm_ops;\n}\n\n \nstatic inline bool vma_is_initial_heap(const struct vm_area_struct *vma)\n{\n       return vma->vm_start <= vma->vm_mm->brk &&\n\t\tvma->vm_end >= vma->vm_mm->start_brk;\n}\n\n \nstatic inline bool vma_is_initial_stack(const struct vm_area_struct *vma)\n{\n\t \n       return vma->vm_start <= vma->vm_mm->start_stack &&\n\t       vma->vm_end >= vma->vm_mm->start_stack;\n}\n\nstatic inline bool vma_is_temporary_stack(struct vm_area_struct *vma)\n{\n\tint maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);\n\n\tif (!maybe_stack)\n\t\treturn false;\n\n\tif ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==\n\t\t\t\t\t\tVM_STACK_INCOMPLETE_SETUP)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool vma_is_foreign(struct vm_area_struct *vma)\n{\n\tif (!current->mm)\n\t\treturn true;\n\n\tif (current->mm != vma->vm_mm)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool vma_is_accessible(struct vm_area_struct *vma)\n{\n\treturn vma->vm_flags & VM_ACCESS_FLAGS;\n}\n\nstatic inline\nstruct vm_area_struct *vma_find(struct vma_iterator *vmi, unsigned long max)\n{\n\treturn mas_find(&vmi->mas, max - 1);\n}\n\nstatic inline struct vm_area_struct *vma_next(struct vma_iterator *vmi)\n{\n\t \n\treturn mas_find(&vmi->mas, ULONG_MAX);\n}\n\nstatic inline\nstruct vm_area_struct *vma_iter_next_range(struct vma_iterator *vmi)\n{\n\treturn mas_next_range(&vmi->mas, ULONG_MAX);\n}\n\n\nstatic inline struct vm_area_struct *vma_prev(struct vma_iterator *vmi)\n{\n\treturn mas_prev(&vmi->mas, 0);\n}\n\nstatic inline\nstruct vm_area_struct *vma_iter_prev_range(struct vma_iterator *vmi)\n{\n\treturn mas_prev_range(&vmi->mas, 0);\n}\n\nstatic inline unsigned long vma_iter_addr(struct vma_iterator *vmi)\n{\n\treturn vmi->mas.index;\n}\n\nstatic inline unsigned long vma_iter_end(struct vma_iterator *vmi)\n{\n\treturn vmi->mas.last + 1;\n}\nstatic inline int vma_iter_bulk_alloc(struct vma_iterator *vmi,\n\t\t\t\t      unsigned long count)\n{\n\treturn mas_expected_entries(&vmi->mas, count);\n}\n\n \nstatic inline void vma_iter_free(struct vma_iterator *vmi)\n{\n\tmas_destroy(&vmi->mas);\n}\n\nstatic inline int vma_iter_bulk_store(struct vma_iterator *vmi,\n\t\t\t\t      struct vm_area_struct *vma)\n{\n\tvmi->mas.index = vma->vm_start;\n\tvmi->mas.last = vma->vm_end - 1;\n\tmas_store(&vmi->mas, vma);\n\tif (unlikely(mas_is_err(&vmi->mas)))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline void vma_iter_invalidate(struct vma_iterator *vmi)\n{\n\tmas_pause(&vmi->mas);\n}\n\nstatic inline void vma_iter_set(struct vma_iterator *vmi, unsigned long addr)\n{\n\tmas_set(&vmi->mas, addr);\n}\n\n#define for_each_vma(__vmi, __vma)\t\t\t\t\t\\\n\twhile (((__vma) = vma_next(&(__vmi))) != NULL)\n\n \n#define for_each_vma_range(__vmi, __vma, __end)\t\t\t\t\\\n\twhile (((__vma) = vma_find(&(__vmi), (__end))) != NULL)\n\n#ifdef CONFIG_SHMEM\n \nbool vma_is_shmem(struct vm_area_struct *vma);\nbool vma_is_anon_shmem(struct vm_area_struct *vma);\n#else\nstatic inline bool vma_is_shmem(struct vm_area_struct *vma) { return false; }\nstatic inline bool vma_is_anon_shmem(struct vm_area_struct *vma) { return false; }\n#endif\n\nint vma_is_stack_for_current(struct vm_area_struct *vma);\n\n \n#define TLB_FLUSH_VMA(mm,flags) { .vm_mm = (mm), .vm_flags = (flags) }\n\nstruct mmu_gather;\nstruct inode;\n\n \nstatic inline unsigned int compound_order(struct page *page)\n{\n\tstruct folio *folio = (struct folio *)page;\n\n\tif (!test_bit(PG_head, &folio->flags))\n\t\treturn 0;\n\treturn folio->_flags_1 & 0xff;\n}\n\n \nstatic inline unsigned int folio_order(struct folio *folio)\n{\n\tif (!folio_test_large(folio))\n\t\treturn 0;\n\treturn folio->_flags_1 & 0xff;\n}\n\n#include <linux/huge_mm.h>\n\n \n\n \nstatic inline int put_page_testzero(struct page *page)\n{\n\tVM_BUG_ON_PAGE(page_ref_count(page) == 0, page);\n\treturn page_ref_dec_and_test(page);\n}\n\nstatic inline int folio_put_testzero(struct folio *folio)\n{\n\treturn put_page_testzero(&folio->page);\n}\n\n \nstatic inline bool get_page_unless_zero(struct page *page)\n{\n\treturn page_ref_add_unless(page, 1, 0);\n}\n\nstatic inline struct folio *folio_get_nontail_page(struct page *page)\n{\n\tif (unlikely(!get_page_unless_zero(page)))\n\t\treturn NULL;\n\treturn (struct folio *)page;\n}\n\nextern int page_is_ram(unsigned long pfn);\n\nenum {\n\tREGION_INTERSECTS,\n\tREGION_DISJOINT,\n\tREGION_MIXED,\n};\n\nint region_intersects(resource_size_t offset, size_t size, unsigned long flags,\n\t\t      unsigned long desc);\n\n \nstruct page *vmalloc_to_page(const void *addr);\nunsigned long vmalloc_to_pfn(const void *addr);\n\n \n#ifdef CONFIG_MMU\nextern bool is_vmalloc_addr(const void *x);\nextern int is_vmalloc_or_module_addr(const void *x);\n#else\nstatic inline bool is_vmalloc_addr(const void *x)\n{\n\treturn false;\n}\nstatic inline int is_vmalloc_or_module_addr(const void *x)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic inline int folio_entire_mapcount(struct folio *folio)\n{\n\tVM_BUG_ON_FOLIO(!folio_test_large(folio), folio);\n\treturn atomic_read(&folio->_entire_mapcount) + 1;\n}\n\n \nstatic inline void page_mapcount_reset(struct page *page)\n{\n\tatomic_set(&(page)->_mapcount, -1);\n}\n\n \nstatic inline int page_mapcount(struct page *page)\n{\n\tint mapcount = atomic_read(&page->_mapcount) + 1;\n\n\tif (unlikely(PageCompound(page)))\n\t\tmapcount += folio_entire_mapcount(page_folio(page));\n\n\treturn mapcount;\n}\n\nint folio_total_mapcount(struct folio *folio);\n\n \nstatic inline int folio_mapcount(struct folio *folio)\n{\n\tif (likely(!folio_test_large(folio)))\n\t\treturn atomic_read(&folio->_mapcount) + 1;\n\treturn folio_total_mapcount(folio);\n}\n\nstatic inline int total_mapcount(struct page *page)\n{\n\tif (likely(!PageCompound(page)))\n\t\treturn atomic_read(&page->_mapcount) + 1;\n\treturn folio_total_mapcount(page_folio(page));\n}\n\nstatic inline bool folio_large_is_mapped(struct folio *folio)\n{\n\t \n\treturn atomic_read(&folio->_nr_pages_mapped) > 0 ||\n\t\tatomic_read(&folio->_entire_mapcount) >= 0;\n}\n\n \nstatic inline bool folio_mapped(struct folio *folio)\n{\n\tif (likely(!folio_test_large(folio)))\n\t\treturn atomic_read(&folio->_mapcount) >= 0;\n\treturn folio_large_is_mapped(folio);\n}\n\n \nstatic inline bool page_mapped(struct page *page)\n{\n\tif (likely(!PageCompound(page)))\n\t\treturn atomic_read(&page->_mapcount) >= 0;\n\treturn folio_large_is_mapped(page_folio(page));\n}\n\nstatic inline struct page *virt_to_head_page(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\n\treturn compound_head(page);\n}\n\nstatic inline struct folio *virt_to_folio(const void *x)\n{\n\tstruct page *page = virt_to_page(x);\n\n\treturn page_folio(page);\n}\n\nvoid __folio_put(struct folio *folio);\n\nvoid put_pages_list(struct list_head *pages);\n\nvoid split_page(struct page *page, unsigned int order);\nvoid folio_copy(struct folio *dst, struct folio *src);\n\nunsigned long nr_free_buffer_pages(void);\n\nvoid destroy_large_folio(struct folio *folio);\n\n \nstatic inline unsigned long page_size(struct page *page)\n{\n\treturn PAGE_SIZE << compound_order(page);\n}\n\n \nstatic inline unsigned int page_shift(struct page *page)\n{\n\treturn PAGE_SHIFT + compound_order(page);\n}\n\n \nstatic inline unsigned int thp_order(struct page *page)\n{\n\tVM_BUG_ON_PGFLAGS(PageTail(page), page);\n\treturn compound_order(page);\n}\n\n \nstatic inline unsigned long thp_size(struct page *page)\n{\n\treturn PAGE_SIZE << thp_order(page);\n}\n\n#ifdef CONFIG_MMU\n \nstatic inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (likely(vma->vm_flags & VM_WRITE))\n\t\tpte = pte_mkwrite(pte, vma);\n\treturn pte;\n}\n\nvm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page);\nvoid set_pte_range(struct vm_fault *vmf, struct folio *folio,\n\t\tstruct page *page, unsigned int nr, unsigned long addr);\n\nvm_fault_t finish_fault(struct vm_fault *vmf);\nvm_fault_t finish_mkwrite_fault(struct vm_fault *vmf);\n#endif\n\n \n\n#if defined(CONFIG_ZONE_DEVICE) && defined(CONFIG_FS_DAX)\nDECLARE_STATIC_KEY_FALSE(devmap_managed_key);\n\nbool __put_devmap_managed_page_refs(struct page *page, int refs);\nstatic inline bool put_devmap_managed_page_refs(struct page *page, int refs)\n{\n\tif (!static_branch_unlikely(&devmap_managed_key))\n\t\treturn false;\n\tif (!is_zone_device_page(page))\n\t\treturn false;\n\treturn __put_devmap_managed_page_refs(page, refs);\n}\n#else  \nstatic inline bool put_devmap_managed_page_refs(struct page *page, int refs)\n{\n\treturn false;\n}\n#endif  \n\nstatic inline bool put_devmap_managed_page(struct page *page)\n{\n\treturn put_devmap_managed_page_refs(page, 1);\n}\n\n \n#define folio_ref_zero_or_close_to_overflow(folio) \\\n\t((unsigned int) folio_ref_count(folio) + 127u <= 127u)\n\n \nstatic inline void folio_get(struct folio *folio)\n{\n\tVM_BUG_ON_FOLIO(folio_ref_zero_or_close_to_overflow(folio), folio);\n\tfolio_ref_inc(folio);\n}\n\nstatic inline void get_page(struct page *page)\n{\n\tfolio_get(page_folio(page));\n}\n\nstatic inline __must_check bool try_get_page(struct page *page)\n{\n\tpage = compound_head(page);\n\tif (WARN_ON_ONCE(page_ref_count(page) <= 0))\n\t\treturn false;\n\tpage_ref_inc(page);\n\treturn true;\n}\n\n \nstatic inline void folio_put(struct folio *folio)\n{\n\tif (folio_put_testzero(folio))\n\t\t__folio_put(folio);\n}\n\n \nstatic inline void folio_put_refs(struct folio *folio, int refs)\n{\n\tif (folio_ref_sub_and_test(folio, refs))\n\t\t__folio_put(folio);\n}\n\n \ntypedef union {\n\tstruct page **pages;\n\tstruct folio **folios;\n\tstruct encoded_page **encoded_pages;\n} release_pages_arg __attribute__ ((__transparent_union__));\n\nvoid release_pages(release_pages_arg, int nr);\n\n \nstatic inline void folios_put(struct folio **folios, unsigned int nr)\n{\n\trelease_pages(folios, nr);\n}\n\nstatic inline void put_page(struct page *page)\n{\n\tstruct folio *folio = page_folio(page);\n\n\t \n\tif (put_devmap_managed_page(&folio->page))\n\t\treturn;\n\tfolio_put(folio);\n}\n\n \n#define GUP_PIN_COUNTING_BIAS (1U << 10)\n\nvoid unpin_user_page(struct page *page);\nvoid unpin_user_pages_dirty_lock(struct page **pages, unsigned long npages,\n\t\t\t\t bool make_dirty);\nvoid unpin_user_page_range_dirty_lock(struct page *page, unsigned long npages,\n\t\t\t\t      bool make_dirty);\nvoid unpin_user_pages(struct page **pages, unsigned long npages);\n\nstatic inline bool is_cow_mapping(vm_flags_t flags)\n{\n\treturn (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;\n}\n\n#ifndef CONFIG_MMU\nstatic inline bool is_nommu_shared_mapping(vm_flags_t flags)\n{\n\t \n\treturn flags & (VM_MAYSHARE | VM_MAYOVERLAY);\n}\n#endif\n\n#if defined(CONFIG_SPARSEMEM) && !defined(CONFIG_SPARSEMEM_VMEMMAP)\n#define SECTION_IN_PAGE_FLAGS\n#endif\n\n \nstatic inline int page_zone_id(struct page *page)\n{\n\treturn (page->flags >> ZONEID_PGSHIFT) & ZONEID_MASK;\n}\n\n#ifdef NODE_NOT_IN_PAGE_FLAGS\nextern int page_to_nid(const struct page *page);\n#else\nstatic inline int page_to_nid(const struct page *page)\n{\n\tstruct page *p = (struct page *)page;\n\n\treturn (PF_POISONED_CHECK(p)->flags >> NODES_PGSHIFT) & NODES_MASK;\n}\n#endif\n\nstatic inline int folio_nid(const struct folio *folio)\n{\n\treturn page_to_nid(&folio->page);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n \n#define PAGE_ACCESS_TIME_MIN_BITS\t12\n#if LAST_CPUPID_SHIFT < PAGE_ACCESS_TIME_MIN_BITS\n#define PAGE_ACCESS_TIME_BUCKETS\t\t\t\t\\\n\t(PAGE_ACCESS_TIME_MIN_BITS - LAST_CPUPID_SHIFT)\n#else\n#define PAGE_ACCESS_TIME_BUCKETS\t0\n#endif\n\n#define PAGE_ACCESS_TIME_MASK\t\t\t\t\\\n\t(LAST_CPUPID_MASK << PAGE_ACCESS_TIME_BUCKETS)\n\nstatic inline int cpu_pid_to_cpupid(int cpu, int pid)\n{\n\treturn ((cpu & LAST__CPU_MASK) << LAST__PID_SHIFT) | (pid & LAST__PID_MASK);\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn cpupid & LAST__PID_MASK;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn (cpupid >> LAST__PID_SHIFT) & LAST__CPU_MASK;\n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn cpu_to_node(cpupid_to_cpu(cpupid));\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn cpupid_to_pid(cpupid) == (-1 & LAST__PID_MASK);\n}\n\nstatic inline bool cpupid_cpu_unset(int cpupid)\n{\n\treturn cpupid_to_cpu(cpupid) == (-1 & LAST__CPU_MASK);\n}\n\nstatic inline bool __cpupid_match_pid(pid_t task_pid, int cpupid)\n{\n\treturn (task_pid & LAST__PID_MASK) == cpupid_to_pid(cpupid);\n}\n\n#define cpupid_match_pid(task, cpupid) __cpupid_match_pid(task->pid, cpupid)\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn xchg(&page->_last_cpupid, cpupid & LAST_CPUPID_MASK);\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page->_last_cpupid;\n}\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->_last_cpupid = -1 & LAST_CPUPID_MASK;\n}\n#else\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;\n}\n\nextern int page_cpupid_xchg_last(struct page *page, int cpupid);\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n\tpage->flags |= LAST_CPUPID_MASK << LAST_CPUPID_PGSHIFT;\n}\n#endif  \n\nstatic inline int xchg_page_access_time(struct page *page, int time)\n{\n\tint last_time;\n\n\tlast_time = page_cpupid_xchg_last(page, time >> PAGE_ACCESS_TIME_BUCKETS);\n\treturn last_time << PAGE_ACCESS_TIME_BUCKETS;\n}\n\nstatic inline void vma_set_access_pid_bit(struct vm_area_struct *vma)\n{\n\tunsigned int pid_bit;\n\n\tpid_bit = hash_32(current->pid, ilog2(BITS_PER_LONG));\n\tif (vma->numab_state && !test_bit(pid_bit, &vma->numab_state->access_pids[1])) {\n\t\t__set_bit(pid_bit, &vma->numab_state->access_pids[1]);\n\t}\n}\n#else  \nstatic inline int page_cpupid_xchg_last(struct page *page, int cpupid)\n{\n\treturn page_to_nid(page);  \n}\n\nstatic inline int xchg_page_access_time(struct page *page, int time)\n{\n\treturn 0;\n}\n\nstatic inline int page_cpupid_last(struct page *page)\n{\n\treturn page_to_nid(page);  \n}\n\nstatic inline int cpupid_to_nid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_pid(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpupid_to_cpu(int cpupid)\n{\n\treturn -1;\n}\n\nstatic inline int cpu_pid_to_cpupid(int nid, int pid)\n{\n\treturn -1;\n}\n\nstatic inline bool cpupid_pid_unset(int cpupid)\n{\n\treturn true;\n}\n\nstatic inline void page_cpupid_reset_last(struct page *page)\n{\n}\n\nstatic inline bool cpupid_match_pid(struct task_struct *task, int cpupid)\n{\n\treturn false;\n}\n\nstatic inline void vma_set_access_pid_bit(struct vm_area_struct *vma)\n{\n}\n#endif  \n\n#if defined(CONFIG_KASAN_SW_TAGS) || defined(CONFIG_KASAN_HW_TAGS)\n\n \n\nstatic inline u8 page_kasan_tag(const struct page *page)\n{\n\tu8 tag = 0xff;\n\n\tif (kasan_enabled()) {\n\t\ttag = (page->flags >> KASAN_TAG_PGSHIFT) & KASAN_TAG_MASK;\n\t\ttag ^= 0xff;\n\t}\n\n\treturn tag;\n}\n\nstatic inline void page_kasan_tag_set(struct page *page, u8 tag)\n{\n\tunsigned long old_flags, flags;\n\n\tif (!kasan_enabled())\n\t\treturn;\n\n\ttag ^= 0xff;\n\told_flags = READ_ONCE(page->flags);\n\tdo {\n\t\tflags = old_flags;\n\t\tflags &= ~(KASAN_TAG_MASK << KASAN_TAG_PGSHIFT);\n\t\tflags |= (tag & KASAN_TAG_MASK) << KASAN_TAG_PGSHIFT;\n\t} while (unlikely(!try_cmpxchg(&page->flags, &old_flags, flags)));\n}\n\nstatic inline void page_kasan_tag_reset(struct page *page)\n{\n\tif (kasan_enabled())\n\t\tpage_kasan_tag_set(page, 0xff);\n}\n\n#else  \n\nstatic inline u8 page_kasan_tag(const struct page *page)\n{\n\treturn 0xff;\n}\n\nstatic inline void page_kasan_tag_set(struct page *page, u8 tag) { }\nstatic inline void page_kasan_tag_reset(struct page *page) { }\n\n#endif  \n\nstatic inline struct zone *page_zone(const struct page *page)\n{\n\treturn &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];\n}\n\nstatic inline pg_data_t *page_pgdat(const struct page *page)\n{\n\treturn NODE_DATA(page_to_nid(page));\n}\n\nstatic inline struct zone *folio_zone(const struct folio *folio)\n{\n\treturn page_zone(&folio->page);\n}\n\nstatic inline pg_data_t *folio_pgdat(const struct folio *folio)\n{\n\treturn page_pgdat(&folio->page);\n}\n\n#ifdef SECTION_IN_PAGE_FLAGS\nstatic inline void set_page_section(struct page *page, unsigned long section)\n{\n\tpage->flags &= ~(SECTIONS_MASK << SECTIONS_PGSHIFT);\n\tpage->flags |= (section & SECTIONS_MASK) << SECTIONS_PGSHIFT;\n}\n\nstatic inline unsigned long page_to_section(const struct page *page)\n{\n\treturn (page->flags >> SECTIONS_PGSHIFT) & SECTIONS_MASK;\n}\n#endif\n\n \nstatic inline unsigned long folio_pfn(struct folio *folio)\n{\n\treturn page_to_pfn(&folio->page);\n}\n\nstatic inline struct folio *pfn_folio(unsigned long pfn)\n{\n\treturn page_folio(pfn_to_page(pfn));\n}\n\n \nstatic inline bool folio_maybe_dma_pinned(struct folio *folio)\n{\n\tif (folio_test_large(folio))\n\t\treturn atomic_read(&folio->_pincount) > 0;\n\n\t \n\treturn ((unsigned int)folio_ref_count(folio)) >=\n\t\tGUP_PIN_COUNTING_BIAS;\n}\n\nstatic inline bool page_maybe_dma_pinned(struct page *page)\n{\n\treturn folio_maybe_dma_pinned(page_folio(page));\n}\n\n \nstatic inline bool page_needs_cow_for_dma(struct vm_area_struct *vma,\n\t\t\t\t\t  struct page *page)\n{\n\tVM_BUG_ON(!(raw_read_seqcount(&vma->vm_mm->write_protect_seq) & 1));\n\n\tif (!test_bit(MMF_HAS_PINNED, &vma->vm_mm->flags))\n\t\treturn false;\n\n\treturn page_maybe_dma_pinned(page);\n}\n\n \nstatic inline bool is_zero_page(const struct page *page)\n{\n\treturn is_zero_pfn(page_to_pfn(page));\n}\n\n \nstatic inline bool is_zero_folio(const struct folio *folio)\n{\n\treturn is_zero_page(&folio->page);\n}\n\n \n#ifdef CONFIG_MIGRATION\nstatic inline bool folio_is_longterm_pinnable(struct folio *folio)\n{\n#ifdef CONFIG_CMA\n\tint mt = folio_migratetype(folio);\n\n\tif (mt == MIGRATE_CMA || mt == MIGRATE_ISOLATE)\n\t\treturn false;\n#endif\n\t \n\tif (is_zero_folio(folio))\n\t\treturn true;\n\n\t \n\tif (folio_is_device_coherent(folio))\n\t\treturn false;\n\n\t \n\treturn !folio_is_zone_movable(folio);\n\n}\n#else\nstatic inline bool folio_is_longterm_pinnable(struct folio *folio)\n{\n\treturn true;\n}\n#endif\n\nstatic inline void set_page_zone(struct page *page, enum zone_type zone)\n{\n\tpage->flags &= ~(ZONES_MASK << ZONES_PGSHIFT);\n\tpage->flags |= (zone & ZONES_MASK) << ZONES_PGSHIFT;\n}\n\nstatic inline void set_page_node(struct page *page, unsigned long node)\n{\n\tpage->flags &= ~(NODES_MASK << NODES_PGSHIFT);\n\tpage->flags |= (node & NODES_MASK) << NODES_PGSHIFT;\n}\n\nstatic inline void set_page_links(struct page *page, enum zone_type zone,\n\tunsigned long node, unsigned long pfn)\n{\n\tset_page_zone(page, zone);\n\tset_page_node(page, node);\n#ifdef SECTION_IN_PAGE_FLAGS\n\tset_page_section(page, pfn_to_section_nr(pfn));\n#endif\n}\n\n \nstatic inline long folio_nr_pages(struct folio *folio)\n{\n\tif (!folio_test_large(folio))\n\t\treturn 1;\n#ifdef CONFIG_64BIT\n\treturn folio->_folio_nr_pages;\n#else\n\treturn 1L << (folio->_flags_1 & 0xff);\n#endif\n}\n\n \nstatic inline unsigned long compound_nr(struct page *page)\n{\n\tstruct folio *folio = (struct folio *)page;\n\n\tif (!test_bit(PG_head, &folio->flags))\n\t\treturn 1;\n#ifdef CONFIG_64BIT\n\treturn folio->_folio_nr_pages;\n#else\n\treturn 1L << (folio->_flags_1 & 0xff);\n#endif\n}\n\n \nstatic inline int thp_nr_pages(struct page *page)\n{\n\treturn folio_nr_pages((struct folio *)page);\n}\n\n \nstatic inline struct folio *folio_next(struct folio *folio)\n{\n\treturn (struct folio *)folio_page(folio, folio_nr_pages(folio));\n}\n\n \nstatic inline unsigned int folio_shift(struct folio *folio)\n{\n\treturn PAGE_SHIFT + folio_order(folio);\n}\n\n \nstatic inline size_t folio_size(struct folio *folio)\n{\n\treturn PAGE_SIZE << folio_order(folio);\n}\n\n \nstatic inline int folio_estimated_sharers(struct folio *folio)\n{\n\treturn page_mapcount(folio_page(folio, 0));\n}\n\n#ifndef HAVE_ARCH_MAKE_PAGE_ACCESSIBLE\nstatic inline int arch_make_page_accessible(struct page *page)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef HAVE_ARCH_MAKE_FOLIO_ACCESSIBLE\nstatic inline int arch_make_folio_accessible(struct folio *folio)\n{\n\tint ret;\n\tlong i, nr = folio_nr_pages(folio);\n\n\tfor (i = 0; i < nr; i++) {\n\t\tret = arch_make_page_accessible(folio_page(folio, i));\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n#endif\n\n \n#include <linux/vmstat.h>\n\nstatic __always_inline void *lowmem_page_address(const struct page *page)\n{\n\treturn page_to_virt(page);\n}\n\n#if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)\n#define HASHED_PAGE_VIRTUAL\n#endif\n\n#if defined(WANT_PAGE_VIRTUAL)\nstatic inline void *page_address(const struct page *page)\n{\n\treturn page->virtual;\n}\nstatic inline void set_page_address(struct page *page, void *address)\n{\n\tpage->virtual = address;\n}\n#define page_address_init()  do { } while(0)\n#endif\n\n#if defined(HASHED_PAGE_VIRTUAL)\nvoid *page_address(const struct page *page);\nvoid set_page_address(struct page *page, void *virtual);\nvoid page_address_init(void);\n#endif\n\n#if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)\n#define page_address(page) lowmem_page_address(page)\n#define set_page_address(page, address)  do { } while(0)\n#define page_address_init()  do { } while(0)\n#endif\n\nstatic inline void *folio_address(const struct folio *folio)\n{\n\treturn page_address(&folio->page);\n}\n\nextern pgoff_t __page_file_index(struct page *page);\n\n \nstatic inline pgoff_t page_index(struct page *page)\n{\n\tif (unlikely(PageSwapCache(page)))\n\t\treturn __page_file_index(page);\n\treturn page->index;\n}\n\n \nstatic inline bool page_is_pfmemalloc(const struct page *page)\n{\n\t \n\treturn (uintptr_t)page->lru.next & BIT(1);\n}\n\n \nstatic inline bool folio_is_pfmemalloc(const struct folio *folio)\n{\n\t \n\treturn (uintptr_t)folio->lru.next & BIT(1);\n}\n\n \nstatic inline void set_page_pfmemalloc(struct page *page)\n{\n\tpage->lru.next = (void *)BIT(1);\n}\n\nstatic inline void clear_page_pfmemalloc(struct page *page)\n{\n\tpage->lru.next = NULL;\n}\n\n \nextern void pagefault_out_of_memory(void);\n\n#define offset_in_page(p)\t((unsigned long)(p) & ~PAGE_MASK)\n#define offset_in_thp(page, p)\t((unsigned long)(p) & (thp_size(page) - 1))\n#define offset_in_folio(folio, p) ((unsigned long)(p) & (folio_size(folio) - 1))\n\n \nstruct zap_details {\n\tstruct folio *single_folio;\t \n\tbool even_cows;\t\t\t \n\tzap_flags_t zap_flags;\t\t \n};\n\n \n#define  ZAP_FLAG_DROP_MARKER        ((__force zap_flags_t) BIT(0))\n \n#define  ZAP_FLAG_UNMAP              ((__force zap_flags_t) BIT(1))\n\n#ifdef CONFIG_SCHED_MM_CID\nvoid sched_mm_cid_before_execve(struct task_struct *t);\nvoid sched_mm_cid_after_execve(struct task_struct *t);\nvoid sched_mm_cid_fork(struct task_struct *t);\nvoid sched_mm_cid_exit_signals(struct task_struct *t);\nstatic inline int task_mm_cid(struct task_struct *t)\n{\n\treturn t->mm_cid;\n}\n#else\nstatic inline void sched_mm_cid_before_execve(struct task_struct *t) { }\nstatic inline void sched_mm_cid_after_execve(struct task_struct *t) { }\nstatic inline void sched_mm_cid_fork(struct task_struct *t) { }\nstatic inline void sched_mm_cid_exit_signals(struct task_struct *t) { }\nstatic inline int task_mm_cid(struct task_struct *t)\n{\n\t \n\treturn raw_smp_processor_id();\n}\n#endif\n\n#ifdef CONFIG_MMU\nextern bool can_do_mlock(void);\n#else\nstatic inline bool can_do_mlock(void) { return false; }\n#endif\nextern int user_shm_lock(size_t, struct ucounts *);\nextern void user_shm_unlock(size_t, struct ucounts *);\n\nstruct folio *vm_normal_folio(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t     pte_t pte);\nstruct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t     pte_t pte);\nstruct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\tpmd_t pmd);\n\nvoid zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,\n\t\t  unsigned long size);\nvoid zap_page_range_single(struct vm_area_struct *vma, unsigned long address,\n\t\t\t   unsigned long size, struct zap_details *details);\nstatic inline void zap_vma_pages(struct vm_area_struct *vma)\n{\n\tzap_page_range_single(vma, vma->vm_start,\n\t\t\t      vma->vm_end - vma->vm_start, NULL);\n}\nvoid unmap_vmas(struct mmu_gather *tlb, struct ma_state *mas,\n\t\tstruct vm_area_struct *start_vma, unsigned long start,\n\t\tunsigned long end, unsigned long tree_end, bool mm_wr_locked);\n\nstruct mmu_notifier_range;\n\nvoid free_pgd_range(struct mmu_gather *tlb, unsigned long addr,\n\t\tunsigned long end, unsigned long floor, unsigned long ceiling);\nint\ncopy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma);\nint follow_pte(struct mm_struct *mm, unsigned long address,\n\t       pte_t **ptepp, spinlock_t **ptlp);\nint follow_pfn(struct vm_area_struct *vma, unsigned long address,\n\tunsigned long *pfn);\nint follow_phys(struct vm_area_struct *vma, unsigned long address,\n\t\tunsigned int flags, unsigned long *prot, resource_size_t *phys);\nint generic_access_phys(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tvoid *buf, int len, int write);\n\nextern void truncate_pagecache(struct inode *inode, loff_t new);\nextern void truncate_setsize(struct inode *inode, loff_t newsize);\nvoid pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to);\nvoid truncate_pagecache_range(struct inode *inode, loff_t offset, loff_t end);\nint generic_error_remove_page(struct address_space *mapping, struct page *page);\n\nstruct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,\n\t\tunsigned long address, struct pt_regs *regs);\n\n#ifdef CONFIG_MMU\nextern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, unsigned int flags,\n\t\t\t\t  struct pt_regs *regs);\nextern int fixup_user_fault(struct mm_struct *mm,\n\t\t\t    unsigned long address, unsigned int fault_flags,\n\t\t\t    bool *unlocked);\nvoid unmap_mapping_pages(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t nr, bool even_cows);\nvoid unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows);\n#else\nstatic inline vm_fault_t handle_mm_fault(struct vm_area_struct *vma,\n\t\t\t\t\t unsigned long address, unsigned int flags,\n\t\t\t\t\t struct pt_regs *regs)\n{\n\t \n\tBUG();\n\treturn VM_FAULT_SIGBUS;\n}\nstatic inline int fixup_user_fault(struct mm_struct *mm, unsigned long address,\n\t\tunsigned int fault_flags, bool *unlocked)\n{\n\t \n\tBUG();\n\treturn -EFAULT;\n}\nstatic inline void unmap_mapping_pages(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t nr, bool even_cows) { }\nstatic inline void unmap_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen, int even_cows) { }\n#endif\n\nstatic inline void unmap_shared_mapping_range(struct address_space *mapping,\n\t\tloff_t const holebegin, loff_t const holelen)\n{\n\tunmap_mapping_range(mapping, holebegin, holelen, 0);\n}\n\nstatic inline struct vm_area_struct *vma_lookup(struct mm_struct *mm,\n\t\t\t\t\t\tunsigned long addr);\n\nextern int access_process_vm(struct task_struct *tsk, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags);\nextern int access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\tvoid *buf, int len, unsigned int gup_flags);\nextern int __access_remote_vm(struct mm_struct *mm, unsigned long addr,\n\t\t\t      void *buf, int len, unsigned int gup_flags);\n\nlong get_user_pages_remote(struct mm_struct *mm,\n\t\t\t   unsigned long start, unsigned long nr_pages,\n\t\t\t   unsigned int gup_flags, struct page **pages,\n\t\t\t   int *locked);\nlong pin_user_pages_remote(struct mm_struct *mm,\n\t\t\t   unsigned long start, unsigned long nr_pages,\n\t\t\t   unsigned int gup_flags, struct page **pages,\n\t\t\t   int *locked);\n\nstatic inline struct page *get_user_page_vma_remote(struct mm_struct *mm,\n\t\t\t\t\t\t    unsigned long addr,\n\t\t\t\t\t\t    int gup_flags,\n\t\t\t\t\t\t    struct vm_area_struct **vmap)\n{\n\tstruct page *page;\n\tstruct vm_area_struct *vma;\n\tint got = get_user_pages_remote(mm, addr, 1, gup_flags, &page, NULL);\n\n\tif (got < 0)\n\t\treturn ERR_PTR(got);\n\tif (got == 0)\n\t\treturn NULL;\n\n\tvma = vma_lookup(mm, addr);\n\tif (WARN_ON_ONCE(!vma)) {\n\t\tput_page(page);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t*vmap = vma;\n\treturn page;\n}\n\nlong get_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages);\nlong pin_user_pages(unsigned long start, unsigned long nr_pages,\n\t\t    unsigned int gup_flags, struct page **pages);\nlong get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    struct page **pages, unsigned int gup_flags);\nlong pin_user_pages_unlocked(unsigned long start, unsigned long nr_pages,\n\t\t    struct page **pages, unsigned int gup_flags);\n\nint get_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages);\nint pin_user_pages_fast(unsigned long start, int nr_pages,\n\t\t\tunsigned int gup_flags, struct page **pages);\nvoid folio_add_pin(struct folio *folio);\n\nint account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc);\nint __account_locked_vm(struct mm_struct *mm, unsigned long pages, bool inc,\n\t\t\tstruct task_struct *task, bool bypass_rlim);\n\nstruct kvec;\nstruct page *get_dump_page(unsigned long addr);\n\nbool folio_mark_dirty(struct folio *folio);\nbool set_page_dirty(struct page *page);\nint set_page_dirty_lock(struct page *page);\n\nint get_cmdline(struct task_struct *task, char *buffer, int buflen);\n\nextern unsigned long move_page_tables(struct vm_area_struct *vma,\n\t\tunsigned long old_addr, struct vm_area_struct *new_vma,\n\t\tunsigned long new_addr, unsigned long len,\n\t\tbool need_rmap_locks);\n\n \n \n#define  MM_CP_TRY_CHANGE_WRITABLE\t   (1UL << 0)\n \n#define  MM_CP_PROT_NUMA                   (1UL << 1)\n \n#define  MM_CP_UFFD_WP                     (1UL << 2)  \n#define  MM_CP_UFFD_WP_RESOLVE             (1UL << 3)  \n#define  MM_CP_UFFD_WP_ALL                 (MM_CP_UFFD_WP | \\\n\t\t\t\t\t    MM_CP_UFFD_WP_RESOLVE)\n\nbool vma_needs_dirty_tracking(struct vm_area_struct *vma);\nint vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot);\nstatic inline bool vma_wants_manual_pte_write_upgrade(struct vm_area_struct *vma)\n{\n\t \n\tif (vma->vm_flags & VM_SHARED)\n\t\treturn vma_wants_writenotify(vma, vma->vm_page_prot);\n\treturn !!(vma->vm_flags & VM_WRITE);\n\n}\nbool can_change_pte_writable(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t     pte_t pte);\nextern long change_protection(struct mmu_gather *tlb,\n\t\t\t      struct vm_area_struct *vma, unsigned long start,\n\t\t\t      unsigned long end, unsigned long cp_flags);\nextern int mprotect_fixup(struct vma_iterator *vmi, struct mmu_gather *tlb,\n\t  struct vm_area_struct *vma, struct vm_area_struct **pprev,\n\t  unsigned long start, unsigned long end, unsigned long newflags);\n\n \nint get_user_pages_fast_only(unsigned long start, int nr_pages,\n\t\t\t     unsigned int gup_flags, struct page **pages);\n\nstatic inline bool get_user_page_fast_only(unsigned long addr,\n\t\t\tunsigned int gup_flags, struct page **pagep)\n{\n\treturn get_user_pages_fast_only(addr, 1, gup_flags, pagep) == 1;\n}\n \nstatic inline unsigned long get_mm_counter(struct mm_struct *mm, int member)\n{\n\treturn percpu_counter_read_positive(&mm->rss_stat[member]);\n}\n\nvoid mm_trace_rss_stat(struct mm_struct *mm, int member);\n\nstatic inline void add_mm_counter(struct mm_struct *mm, int member, long value)\n{\n\tpercpu_counter_add(&mm->rss_stat[member], value);\n\n\tmm_trace_rss_stat(mm, member);\n}\n\nstatic inline void inc_mm_counter(struct mm_struct *mm, int member)\n{\n\tpercpu_counter_inc(&mm->rss_stat[member]);\n\n\tmm_trace_rss_stat(mm, member);\n}\n\nstatic inline void dec_mm_counter(struct mm_struct *mm, int member)\n{\n\tpercpu_counter_dec(&mm->rss_stat[member]);\n\n\tmm_trace_rss_stat(mm, member);\n}\n\n \nstatic inline int mm_counter_file(struct page *page)\n{\n\tif (PageSwapBacked(page))\n\t\treturn MM_SHMEMPAGES;\n\treturn MM_FILEPAGES;\n}\n\nstatic inline int mm_counter(struct page *page)\n{\n\tif (PageAnon(page))\n\t\treturn MM_ANONPAGES;\n\treturn mm_counter_file(page);\n}\n\nstatic inline unsigned long get_mm_rss(struct mm_struct *mm)\n{\n\treturn get_mm_counter(mm, MM_FILEPAGES) +\n\t\tget_mm_counter(mm, MM_ANONPAGES) +\n\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n}\n\nstatic inline unsigned long get_mm_hiwater_rss(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_rss, get_mm_rss(mm));\n}\n\nstatic inline unsigned long get_mm_hiwater_vm(struct mm_struct *mm)\n{\n\treturn max(mm->hiwater_vm, mm->total_vm);\n}\n\nstatic inline void update_hiwater_rss(struct mm_struct *mm)\n{\n\tunsigned long _rss = get_mm_rss(mm);\n\n\tif ((mm)->hiwater_rss < _rss)\n\t\t(mm)->hiwater_rss = _rss;\n}\n\nstatic inline void update_hiwater_vm(struct mm_struct *mm)\n{\n\tif (mm->hiwater_vm < mm->total_vm)\n\t\tmm->hiwater_vm = mm->total_vm;\n}\n\nstatic inline void reset_mm_hiwater_rss(struct mm_struct *mm)\n{\n\tmm->hiwater_rss = get_mm_rss(mm);\n}\n\nstatic inline void setmax_mm_hiwater_rss(unsigned long *maxrss,\n\t\t\t\t\t struct mm_struct *mm)\n{\n\tunsigned long hiwater_rss = get_mm_hiwater_rss(mm);\n\n\tif (*maxrss < hiwater_rss)\n\t\t*maxrss = hiwater_rss;\n}\n\n#if defined(SPLIT_RSS_COUNTING)\nvoid sync_mm_rss(struct mm_struct *mm);\n#else\nstatic inline void sync_mm_rss(struct mm_struct *mm)\n{\n}\n#endif\n\n#ifndef CONFIG_ARCH_HAS_PTE_SPECIAL\nstatic inline int pte_special(pte_t pte)\n{\n\treturn 0;\n}\n\nstatic inline pte_t pte_mkspecial(pte_t pte)\n{\n\treturn pte;\n}\n#endif\n\n#ifndef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t pte)\n{\n\treturn 0;\n}\n#endif\n\nextern pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t       spinlock_t **ptl);\nstatic inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t    spinlock_t **ptl)\n{\n\tpte_t *ptep;\n\t__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));\n\treturn ptep;\n}\n\n#ifdef __PAGETABLE_P4D_FOLDED\nstatic inline int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n#else\nint __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address);\n#endif\n\n#if defined(__PAGETABLE_PUD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pud_alloc(struct mm_struct *mm, p4d_t *p4d,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\nstatic inline void mm_inc_nr_puds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_puds(struct mm_struct *mm) {}\n\n#else\nint __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address);\n\nstatic inline void mm_inc_nr_puds(struct mm_struct *mm)\n{\n\tif (mm_pud_folded(mm))\n\t\treturn;\n\tatomic_long_add(PTRS_PER_PUD * sizeof(pud_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_puds(struct mm_struct *mm)\n{\n\tif (mm_pud_folded(mm))\n\t\treturn;\n\tatomic_long_sub(PTRS_PER_PUD * sizeof(pud_t), &mm->pgtables_bytes);\n}\n#endif\n\n#if defined(__PAGETABLE_PMD_FOLDED) || !defined(CONFIG_MMU)\nstatic inline int __pmd_alloc(struct mm_struct *mm, pud_t *pud,\n\t\t\t\t\t\tunsigned long address)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm) {}\n\n#else\nint __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address);\n\nstatic inline void mm_inc_nr_pmds(struct mm_struct *mm)\n{\n\tif (mm_pmd_folded(mm))\n\t\treturn;\n\tatomic_long_add(PTRS_PER_PMD * sizeof(pmd_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_pmds(struct mm_struct *mm)\n{\n\tif (mm_pmd_folded(mm))\n\t\treturn;\n\tatomic_long_sub(PTRS_PER_PMD * sizeof(pmd_t), &mm->pgtables_bytes);\n}\n#endif\n\n#ifdef CONFIG_MMU\nstatic inline void mm_pgtables_bytes_init(struct mm_struct *mm)\n{\n\tatomic_long_set(&mm->pgtables_bytes, 0);\n}\n\nstatic inline unsigned long mm_pgtables_bytes(const struct mm_struct *mm)\n{\n\treturn atomic_long_read(&mm->pgtables_bytes);\n}\n\nstatic inline void mm_inc_nr_ptes(struct mm_struct *mm)\n{\n\tatomic_long_add(PTRS_PER_PTE * sizeof(pte_t), &mm->pgtables_bytes);\n}\n\nstatic inline void mm_dec_nr_ptes(struct mm_struct *mm)\n{\n\tatomic_long_sub(PTRS_PER_PTE * sizeof(pte_t), &mm->pgtables_bytes);\n}\n#else\n\nstatic inline void mm_pgtables_bytes_init(struct mm_struct *mm) {}\nstatic inline unsigned long mm_pgtables_bytes(const struct mm_struct *mm)\n{\n\treturn 0;\n}\n\nstatic inline void mm_inc_nr_ptes(struct mm_struct *mm) {}\nstatic inline void mm_dec_nr_ptes(struct mm_struct *mm) {}\n#endif\n\nint __pte_alloc(struct mm_struct *mm, pmd_t *pmd);\nint __pte_alloc_kernel(pmd_t *pmd);\n\n#if defined(CONFIG_MMU)\n\nstatic inline p4d_t *p4d_alloc(struct mm_struct *mm, pgd_t *pgd,\n\t\tunsigned long address)\n{\n\treturn (unlikely(pgd_none(*pgd)) && __p4d_alloc(mm, pgd, address)) ?\n\t\tNULL : p4d_offset(pgd, address);\n}\n\nstatic inline pud_t *pud_alloc(struct mm_struct *mm, p4d_t *p4d,\n\t\tunsigned long address)\n{\n\treturn (unlikely(p4d_none(*p4d)) && __pud_alloc(mm, p4d, address)) ?\n\t\tNULL : pud_offset(p4d, address);\n}\n\nstatic inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)\n{\n\treturn (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?\n\t\tNULL: pmd_offset(pud, address);\n}\n#endif  \n\nstatic inline struct ptdesc *virt_to_ptdesc(const void *x)\n{\n\treturn page_ptdesc(virt_to_page(x));\n}\n\nstatic inline void *ptdesc_to_virt(const struct ptdesc *pt)\n{\n\treturn page_to_virt(ptdesc_page(pt));\n}\n\nstatic inline void *ptdesc_address(const struct ptdesc *pt)\n{\n\treturn folio_address(ptdesc_folio(pt));\n}\n\nstatic inline bool pagetable_is_reserved(struct ptdesc *pt)\n{\n\treturn folio_test_reserved(ptdesc_folio(pt));\n}\n\n \nstatic inline struct ptdesc *pagetable_alloc(gfp_t gfp, unsigned int order)\n{\n\tstruct page *page = alloc_pages(gfp | __GFP_COMP, order);\n\n\treturn page_ptdesc(page);\n}\n\n \nstatic inline void pagetable_free(struct ptdesc *pt)\n{\n\tstruct page *page = ptdesc_page(pt);\n\n\t__free_pages(page, compound_order(page));\n}\n\n#if USE_SPLIT_PTE_PTLOCKS\n#if ALLOC_SPLIT_PTLOCKS\nvoid __init ptlock_cache_init(void);\nbool ptlock_alloc(struct ptdesc *ptdesc);\nvoid ptlock_free(struct ptdesc *ptdesc);\n\nstatic inline spinlock_t *ptlock_ptr(struct ptdesc *ptdesc)\n{\n\treturn ptdesc->ptl;\n}\n#else  \nstatic inline void ptlock_cache_init(void)\n{\n}\n\nstatic inline bool ptlock_alloc(struct ptdesc *ptdesc)\n{\n\treturn true;\n}\n\nstatic inline void ptlock_free(struct ptdesc *ptdesc)\n{\n}\n\nstatic inline spinlock_t *ptlock_ptr(struct ptdesc *ptdesc)\n{\n\treturn &ptdesc->ptl;\n}\n#endif  \n\nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(page_ptdesc(pmd_page(*pmd)));\n}\n\nstatic inline bool ptlock_init(struct ptdesc *ptdesc)\n{\n\t \n\tVM_BUG_ON_PAGE(*(unsigned long *)&ptdesc->ptl, ptdesc_page(ptdesc));\n\tif (!ptlock_alloc(ptdesc))\n\t\treturn false;\n\tspin_lock_init(ptlock_ptr(ptdesc));\n\treturn true;\n}\n\n#else\t \n \nstatic inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\nstatic inline void ptlock_cache_init(void) {}\nstatic inline bool ptlock_init(struct ptdesc *ptdesc) { return true; }\nstatic inline void ptlock_free(struct ptdesc *ptdesc) {}\n#endif  \n\nstatic inline bool pagetable_pte_ctor(struct ptdesc *ptdesc)\n{\n\tstruct folio *folio = ptdesc_folio(ptdesc);\n\n\tif (!ptlock_init(ptdesc))\n\t\treturn false;\n\t__folio_set_pgtable(folio);\n\tlruvec_stat_add_folio(folio, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pagetable_pte_dtor(struct ptdesc *ptdesc)\n{\n\tstruct folio *folio = ptdesc_folio(ptdesc);\n\n\tptlock_free(ptdesc);\n\t__folio_clear_pgtable(folio);\n\tlruvec_stat_sub_folio(folio, NR_PAGETABLE);\n}\n\npte_t *__pte_offset_map(pmd_t *pmd, unsigned long addr, pmd_t *pmdvalp);\nstatic inline pte_t *pte_offset_map(pmd_t *pmd, unsigned long addr)\n{\n\treturn __pte_offset_map(pmd, addr, NULL);\n}\n\npte_t *__pte_offset_map_lock(struct mm_struct *mm, pmd_t *pmd,\n\t\t\tunsigned long addr, spinlock_t **ptlp);\nstatic inline pte_t *pte_offset_map_lock(struct mm_struct *mm, pmd_t *pmd,\n\t\t\tunsigned long addr, spinlock_t **ptlp)\n{\n\tpte_t *pte;\n\n\t__cond_lock(*ptlp, pte = __pte_offset_map_lock(mm, pmd, addr, ptlp));\n\treturn pte;\n}\n\npte_t *pte_offset_map_nolock(struct mm_struct *mm, pmd_t *pmd,\n\t\t\tunsigned long addr, spinlock_t **ptlp);\n\n#define pte_unmap_unlock(pte, ptl)\tdo {\t\t\\\n\tspin_unlock(ptl);\t\t\t\t\\\n\tpte_unmap(pte);\t\t\t\t\t\\\n} while (0)\n\n#define pte_alloc(mm, pmd) (unlikely(pmd_none(*(pmd))) && __pte_alloc(mm, pmd))\n\n#define pte_alloc_map(mm, pmd, address)\t\t\t\\\n\t(pte_alloc(mm, pmd) ? NULL : pte_offset_map(pmd, address))\n\n#define pte_alloc_map_lock(mm, pmd, address, ptlp)\t\\\n\t(pte_alloc(mm, pmd) ?\t\t\t\\\n\t\t NULL : pte_offset_map_lock(mm, pmd, address, ptlp))\n\n#define pte_alloc_kernel(pmd, address)\t\t\t\\\n\t((unlikely(pmd_none(*(pmd))) && __pte_alloc_kernel(pmd))? \\\n\t\tNULL: pte_offset_kernel(pmd, address))\n\n#if USE_SPLIT_PMD_PTLOCKS\n\nstatic inline struct page *pmd_pgtable_page(pmd_t *pmd)\n{\n\tunsigned long mask = ~(PTRS_PER_PMD * sizeof(pmd_t) - 1);\n\treturn virt_to_page((void *)((unsigned long) pmd & mask));\n}\n\nstatic inline struct ptdesc *pmd_ptdesc(pmd_t *pmd)\n{\n\treturn page_ptdesc(pmd_pgtable_page(pmd));\n}\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn ptlock_ptr(pmd_ptdesc(pmd));\n}\n\nstatic inline bool pmd_ptlock_init(struct ptdesc *ptdesc)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptdesc->pmd_huge_pte = NULL;\n#endif\n\treturn ptlock_init(ptdesc);\n}\n\nstatic inline void pmd_ptlock_free(struct ptdesc *ptdesc)\n{\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tVM_BUG_ON_PAGE(ptdesc->pmd_huge_pte, ptdesc_page(ptdesc));\n#endif\n\tptlock_free(ptdesc);\n}\n\n#define pmd_huge_pte(mm, pmd) (pmd_ptdesc(pmd)->pmd_huge_pte)\n\n#else\n\nstatic inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline bool pmd_ptlock_init(struct ptdesc *ptdesc) { return true; }\nstatic inline void pmd_ptlock_free(struct ptdesc *ptdesc) {}\n\n#define pmd_huge_pte(mm, pmd) ((mm)->pmd_huge_pte)\n\n#endif\n\nstatic inline spinlock_t *pmd_lock(struct mm_struct *mm, pmd_t *pmd)\n{\n\tspinlock_t *ptl = pmd_lockptr(mm, pmd);\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nstatic inline bool pagetable_pmd_ctor(struct ptdesc *ptdesc)\n{\n\tstruct folio *folio = ptdesc_folio(ptdesc);\n\n\tif (!pmd_ptlock_init(ptdesc))\n\t\treturn false;\n\t__folio_set_pgtable(folio);\n\tlruvec_stat_add_folio(folio, NR_PAGETABLE);\n\treturn true;\n}\n\nstatic inline void pagetable_pmd_dtor(struct ptdesc *ptdesc)\n{\n\tstruct folio *folio = ptdesc_folio(ptdesc);\n\n\tpmd_ptlock_free(ptdesc);\n\t__folio_clear_pgtable(folio);\n\tlruvec_stat_sub_folio(folio, NR_PAGETABLE);\n}\n\n \nstatic inline spinlock_t *pud_lockptr(struct mm_struct *mm, pud_t *pud)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline spinlock_t *pud_lock(struct mm_struct *mm, pud_t *pud)\n{\n\tspinlock_t *ptl = pud_lockptr(mm, pud);\n\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\nextern void __init pagecache_init(void);\nextern void free_initmem(void);\n\n \nextern unsigned long free_reserved_area(void *start, void *end,\n\t\t\t\t\tint poison, const char *s);\n\nextern void adjust_managed_page_count(struct page *page, long count);\n\nextern void reserve_bootmem_region(phys_addr_t start,\n\t\t\t\t   phys_addr_t end, int nid);\n\n \nstatic inline void free_reserved_page(struct page *page)\n{\n\tClearPageReserved(page);\n\tinit_page_count(page);\n\t__free_page(page);\n\tadjust_managed_page_count(page, 1);\n}\n#define free_highmem_page(page) free_reserved_page(page)\n\nstatic inline void mark_page_reserved(struct page *page)\n{\n\tSetPageReserved(page);\n\tadjust_managed_page_count(page, -1);\n}\n\nstatic inline void free_reserved_ptdesc(struct ptdesc *pt)\n{\n\tfree_reserved_page(ptdesc_page(pt));\n}\n\n \nstatic inline unsigned long free_initmem_default(int poison)\n{\n\textern char __init_begin[], __init_end[];\n\n\treturn free_reserved_area(&__init_begin, &__init_end,\n\t\t\t\t  poison, \"unused kernel image (initmem)\");\n}\n\nstatic inline unsigned long get_num_physpages(void)\n{\n\tint nid;\n\tunsigned long phys_pages = 0;\n\n\tfor_each_online_node(nid)\n\t\tphys_pages += node_present_pages(nid);\n\n\treturn phys_pages;\n}\n\n \nvoid free_area_init(unsigned long *max_zone_pfn);\nunsigned long node_map_pfn_alignment(void);\nunsigned long __absent_pages_in_range(int nid, unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern unsigned long absent_pages_in_range(unsigned long start_pfn,\n\t\t\t\t\t\tunsigned long end_pfn);\nextern void get_pfn_range_for_nid(unsigned int nid,\n\t\t\tunsigned long *start_pfn, unsigned long *end_pfn);\n\n#ifndef CONFIG_NUMA\nstatic inline int early_pfn_to_nid(unsigned long pfn)\n{\n\treturn 0;\n}\n#else\n \nextern int __meminit early_pfn_to_nid(unsigned long pfn);\n#endif\n\nextern void set_dma_reserve(unsigned long new_dma_reserve);\nextern void mem_init(void);\nextern void __init mmap_init(void);\n\nextern void __show_mem(unsigned int flags, nodemask_t *nodemask, int max_zone_idx);\nstatic inline void show_mem(void)\n{\n\t__show_mem(0, NULL, MAX_NR_ZONES - 1);\n}\nextern long si_mem_available(void);\nextern void si_meminfo(struct sysinfo * val);\nextern void si_meminfo_node(struct sysinfo *val, int nid);\n#ifdef __HAVE_ARCH_RESERVED_KERNEL_PAGES\nextern unsigned long arch_reserved_kernel_pages(void);\n#endif\n\nextern __printf(3, 4)\nvoid warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...);\n\nextern void setup_per_cpu_pageset(void);\n\n \nextern atomic_long_t mmap_pages_allocated;\nextern int nommu_shrink_inode_mappings(struct inode *, size_t, size_t);\n\n \nvoid vma_interval_tree_insert(struct vm_area_struct *node,\n\t\t\t      struct rb_root_cached *root);\nvoid vma_interval_tree_insert_after(struct vm_area_struct *node,\n\t\t\t\t    struct vm_area_struct *prev,\n\t\t\t\t    struct rb_root_cached *root);\nvoid vma_interval_tree_remove(struct vm_area_struct *node,\n\t\t\t      struct rb_root_cached *root);\nstruct vm_area_struct *vma_interval_tree_iter_first(struct rb_root_cached *root,\n\t\t\t\tunsigned long start, unsigned long last);\nstruct vm_area_struct *vma_interval_tree_iter_next(struct vm_area_struct *node,\n\t\t\t\tunsigned long start, unsigned long last);\n\n#define vma_interval_tree_foreach(vma, root, start, last)\t\t\\\n\tfor (vma = vma_interval_tree_iter_first(root, start, last);\t\\\n\t     vma; vma = vma_interval_tree_iter_next(vma, start, last))\n\nvoid anon_vma_interval_tree_insert(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root);\nvoid anon_vma_interval_tree_remove(struct anon_vma_chain *node,\n\t\t\t\t   struct rb_root_cached *root);\nstruct anon_vma_chain *\nanon_vma_interval_tree_iter_first(struct rb_root_cached *root,\n\t\t\t\t  unsigned long start, unsigned long last);\nstruct anon_vma_chain *anon_vma_interval_tree_iter_next(\n\tstruct anon_vma_chain *node, unsigned long start, unsigned long last);\n#ifdef CONFIG_DEBUG_VM_RB\nvoid anon_vma_interval_tree_verify(struct anon_vma_chain *node);\n#endif\n\n#define anon_vma_interval_tree_foreach(avc, root, start, last)\t\t \\\n\tfor (avc = anon_vma_interval_tree_iter_first(root, start, last); \\\n\t     avc; avc = anon_vma_interval_tree_iter_next(avc, start, last))\n\n \nextern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);\nextern int vma_expand(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\t      unsigned long start, unsigned long end, pgoff_t pgoff,\n\t\t      struct vm_area_struct *next);\nextern int vma_shrink(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\t       unsigned long start, unsigned long end, pgoff_t pgoff);\nextern struct vm_area_struct *vma_merge(struct vma_iterator *vmi,\n\tstruct mm_struct *, struct vm_area_struct *prev, unsigned long addr,\n\tunsigned long end, unsigned long vm_flags, struct anon_vma *,\n\tstruct file *, pgoff_t, struct mempolicy *, struct vm_userfaultfd_ctx,\n\tstruct anon_vma_name *);\nextern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);\nextern int __split_vma(struct vma_iterator *vmi, struct vm_area_struct *,\n\t\t       unsigned long addr, int new_below);\nextern int split_vma(struct vma_iterator *vmi, struct vm_area_struct *,\n\t\t\t unsigned long addr, int new_below);\nextern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);\nextern void unlink_file_vma(struct vm_area_struct *);\nextern struct vm_area_struct *copy_vma(struct vm_area_struct **,\n\tunsigned long addr, unsigned long len, pgoff_t pgoff,\n\tbool *need_rmap_locks);\nextern void exit_mmap(struct mm_struct *);\n\nstatic inline int check_data_rlimit(unsigned long rlim,\n\t\t\t\t    unsigned long new,\n\t\t\t\t    unsigned long start,\n\t\t\t\t    unsigned long end_data,\n\t\t\t\t    unsigned long start_data)\n{\n\tif (rlim < RLIM_INFINITY) {\n\t\tif (((new - start) + (end_data - start_data)) > rlim)\n\t\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nextern int mm_take_all_locks(struct mm_struct *mm);\nextern void mm_drop_all_locks(struct mm_struct *mm);\n\nextern int set_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\nextern int replace_mm_exe_file(struct mm_struct *mm, struct file *new_exe_file);\nextern struct file *get_mm_exe_file(struct mm_struct *mm);\nextern struct file *get_task_exe_file(struct task_struct *task);\n\nextern bool may_expand_vm(struct mm_struct *, vm_flags_t, unsigned long npages);\nextern void vm_stat_account(struct mm_struct *, vm_flags_t, long npages);\n\nextern bool vma_is_special_mapping(const struct vm_area_struct *vma,\n\t\t\t\t   const struct vm_special_mapping *sm);\nextern struct vm_area_struct *_install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags,\n\t\t\t\t   const struct vm_special_mapping *spec);\n \nextern int install_special_mapping(struct mm_struct *mm,\n\t\t\t\t   unsigned long addr, unsigned long len,\n\t\t\t\t   unsigned long flags, struct page **pages);\n\nunsigned long randomize_stack_top(unsigned long stack_top);\nunsigned long randomize_page(unsigned long start, unsigned long range);\n\nextern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned long, unsigned long, unsigned long);\n\nextern unsigned long mmap_region(struct file *file, unsigned long addr,\n\tunsigned long len, vm_flags_t vm_flags, unsigned long pgoff,\n\tstruct list_head *uf);\nextern unsigned long do_mmap(struct file *file, unsigned long addr,\n\tunsigned long len, unsigned long prot, unsigned long flags,\n\tvm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,\n\tstruct list_head *uf);\nextern int do_vmi_munmap(struct vma_iterator *vmi, struct mm_struct *mm,\n\t\t\t unsigned long start, size_t len, struct list_head *uf,\n\t\t\t bool unlock);\nextern int do_munmap(struct mm_struct *, unsigned long, size_t,\n\t\t     struct list_head *uf);\nextern int do_madvise(struct mm_struct *mm, unsigned long start, size_t len_in, int behavior);\n\n#ifdef CONFIG_MMU\nextern int do_vma_munmap(struct vma_iterator *vmi, struct vm_area_struct *vma,\n\t\t\t unsigned long start, unsigned long end,\n\t\t\t struct list_head *uf, bool unlock);\nextern int __mm_populate(unsigned long addr, unsigned long len,\n\t\t\t int ignore_errors);\nstatic inline void mm_populate(unsigned long addr, unsigned long len)\n{\n\t \n\t(void) __mm_populate(addr, len, 1);\n}\n#else\nstatic inline void mm_populate(unsigned long addr, unsigned long len) {}\n#endif\n\n \nextern int __must_check vm_brk(unsigned long, unsigned long);\nextern int __must_check vm_brk_flags(unsigned long, unsigned long, unsigned long);\nextern int vm_munmap(unsigned long, size_t);\nextern unsigned long __must_check vm_mmap(struct file *, unsigned long,\n        unsigned long, unsigned long,\n        unsigned long, unsigned long);\n\nstruct vm_unmapped_area_info {\n#define VM_UNMAPPED_AREA_TOPDOWN 1\n\tunsigned long flags;\n\tunsigned long length;\n\tunsigned long low_limit;\n\tunsigned long high_limit;\n\tunsigned long align_mask;\n\tunsigned long align_offset;\n};\n\nextern unsigned long vm_unmapped_area(struct vm_unmapped_area_info *info);\n\n \nextern void truncate_inode_pages(struct address_space *, loff_t);\nextern void truncate_inode_pages_range(struct address_space *,\n\t\t\t\t       loff_t lstart, loff_t lend);\nextern void truncate_inode_pages_final(struct address_space *);\n\n \nextern vm_fault_t filemap_fault(struct vm_fault *vmf);\nextern vm_fault_t filemap_map_pages(struct vm_fault *vmf,\n\t\tpgoff_t start_pgoff, pgoff_t end_pgoff);\nextern vm_fault_t filemap_page_mkwrite(struct vm_fault *vmf);\n\nextern unsigned long stack_guard_gap;\n \nint expand_stack_locked(struct vm_area_struct *vma, unsigned long address);\nstruct vm_area_struct *expand_stack(struct mm_struct * mm, unsigned long addr);\n\n \nint expand_downwards(struct vm_area_struct *vma, unsigned long address);\n\n \nextern struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr);\nextern struct vm_area_struct * find_vma_prev(struct mm_struct * mm, unsigned long addr,\n\t\t\t\t\t     struct vm_area_struct **pprev);\n\n \nstruct vm_area_struct *find_vma_intersection(struct mm_struct *mm,\n\t\t\tunsigned long start_addr, unsigned long end_addr);\n\n \nstatic inline\nstruct vm_area_struct *vma_lookup(struct mm_struct *mm, unsigned long addr)\n{\n\treturn mtree_load(&mm->mm_mt, addr);\n}\n\nstatic inline unsigned long stack_guard_start_gap(struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_GROWSDOWN)\n\t\treturn stack_guard_gap;\n\n\t \n\tif (vma->vm_flags & VM_SHADOW_STACK)\n\t\treturn PAGE_SIZE;\n\n\treturn 0;\n}\n\nstatic inline unsigned long vm_start_gap(struct vm_area_struct *vma)\n{\n\tunsigned long gap = stack_guard_start_gap(vma);\n\tunsigned long vm_start = vma->vm_start;\n\n\tvm_start -= gap;\n\tif (vm_start > vma->vm_start)\n\t\tvm_start = 0;\n\treturn vm_start;\n}\n\nstatic inline unsigned long vm_end_gap(struct vm_area_struct *vma)\n{\n\tunsigned long vm_end = vma->vm_end;\n\n\tif (vma->vm_flags & VM_GROWSUP) {\n\t\tvm_end += stack_guard_gap;\n\t\tif (vm_end < vma->vm_end)\n\t\t\tvm_end = -PAGE_SIZE;\n\t}\n\treturn vm_end;\n}\n\nstatic inline unsigned long vma_pages(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;\n}\n\n \nstatic inline struct vm_area_struct *find_exact_vma(struct mm_struct *mm,\n\t\t\t\tunsigned long vm_start, unsigned long vm_end)\n{\n\tstruct vm_area_struct *vma = vma_lookup(mm, vm_start);\n\n\tif (vma && (vma->vm_start != vm_start || vma->vm_end != vm_end))\n\t\tvma = NULL;\n\n\treturn vma;\n}\n\nstatic inline bool range_in_vma(struct vm_area_struct *vma,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\treturn (vma && vma->vm_start <= start && end <= vma->vm_end);\n}\n\n#ifdef CONFIG_MMU\npgprot_t vm_get_page_prot(unsigned long vm_flags);\nvoid vma_set_page_prot(struct vm_area_struct *vma);\n#else\nstatic inline pgprot_t vm_get_page_prot(unsigned long vm_flags)\n{\n\treturn __pgprot(0);\n}\nstatic inline void vma_set_page_prot(struct vm_area_struct *vma)\n{\n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n}\n#endif\n\nvoid vma_set_file(struct vm_area_struct *vma, struct file *file);\n\n#ifdef CONFIG_NUMA_BALANCING\nunsigned long change_prot_numa(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end);\n#endif\n\nstruct vm_area_struct *find_extend_vma_locked(struct mm_struct *,\n\t\tunsigned long addr);\nint remap_pfn_range(struct vm_area_struct *, unsigned long addr,\n\t\t\tunsigned long pfn, unsigned long size, pgprot_t);\nint remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,\n\t\tunsigned long pfn, unsigned long size, pgprot_t prot);\nint vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);\nint vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tstruct page **pages, unsigned long *num);\nint vm_map_pages(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num);\nint vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,\n\t\t\t\tunsigned long num);\nvm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn);\nvm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tunsigned long pfn, pgprot_t pgprot);\nvm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,\n\t\t\tpfn_t pfn);\nvm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,\n\t\tunsigned long addr, pfn_t pfn);\nint vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len);\n\nstatic inline vm_fault_t vmf_insert_page(struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, struct page *page)\n{\n\tint err = vm_insert_page(vma, addr, page);\n\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\tif (err < 0 && err != -EBUSY)\n\t\treturn VM_FAULT_SIGBUS;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n#ifndef io_remap_pfn_range\nstatic inline int io_remap_pfn_range(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, unsigned long pfn,\n\t\t\t\t     unsigned long size, pgprot_t prot)\n{\n\treturn remap_pfn_range(vma, addr, pfn, size, pgprot_decrypted(prot));\n}\n#endif\n\nstatic inline vm_fault_t vmf_error(int err)\n{\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\telse if (err == -EHWPOISON)\n\t\treturn VM_FAULT_HWPOISON;\n\treturn VM_FAULT_SIGBUS;\n}\n\n \nstatic inline vm_fault_t vmf_fs_error(int err)\n{\n\tif (err == 0)\n\t\treturn VM_FAULT_LOCKED;\n\tif (err == -EFAULT || err == -EAGAIN)\n\t\treturn VM_FAULT_NOPAGE;\n\tif (err == -ENOMEM)\n\t\treturn VM_FAULT_OOM;\n\t \n\treturn VM_FAULT_SIGBUS;\n}\n\nstruct page *follow_page(struct vm_area_struct *vma, unsigned long address,\n\t\t\t unsigned int foll_flags);\n\nstatic inline int vm_fault_to_errno(vm_fault_t vm_fault, int foll_flags)\n{\n\tif (vm_fault & VM_FAULT_OOM)\n\t\treturn -ENOMEM;\n\tif (vm_fault & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))\n\t\treturn (foll_flags & FOLL_HWPOISON) ? -EHWPOISON : -EFAULT;\n\tif (vm_fault & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n \nstatic inline bool gup_can_follow_protnone(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned int flags)\n{\n\t \n\tif (!(flags & FOLL_HONOR_NUMA_FAULT))\n\t\treturn true;\n\n\t \n\treturn !vma_is_accessible(vma);\n}\n\ntypedef int (*pte_fn_t)(pte_t *pte, unsigned long addr, void *data);\nextern int apply_to_page_range(struct mm_struct *mm, unsigned long address,\n\t\t\t       unsigned long size, pte_fn_t fn, void *data);\nextern int apply_to_existing_page_range(struct mm_struct *mm,\n\t\t\t\t   unsigned long address, unsigned long size,\n\t\t\t\t   pte_fn_t fn, void *data);\n\n#ifdef CONFIG_PAGE_POISONING\nextern void __kernel_poison_pages(struct page *page, int numpages);\nextern void __kernel_unpoison_pages(struct page *page, int numpages);\nextern bool _page_poisoning_enabled_early;\nDECLARE_STATIC_KEY_FALSE(_page_poisoning_enabled);\nstatic inline bool page_poisoning_enabled(void)\n{\n\treturn _page_poisoning_enabled_early;\n}\n \nstatic inline bool page_poisoning_enabled_static(void)\n{\n\treturn static_branch_unlikely(&_page_poisoning_enabled);\n}\nstatic inline void kernel_poison_pages(struct page *page, int numpages)\n{\n\tif (page_poisoning_enabled_static())\n\t\t__kernel_poison_pages(page, numpages);\n}\nstatic inline void kernel_unpoison_pages(struct page *page, int numpages)\n{\n\tif (page_poisoning_enabled_static())\n\t\t__kernel_unpoison_pages(page, numpages);\n}\n#else\nstatic inline bool page_poisoning_enabled(void) { return false; }\nstatic inline bool page_poisoning_enabled_static(void) { return false; }\nstatic inline void __kernel_poison_pages(struct page *page, int nunmpages) { }\nstatic inline void kernel_poison_pages(struct page *page, int numpages) { }\nstatic inline void kernel_unpoison_pages(struct page *page, int numpages) { }\n#endif\n\nDECLARE_STATIC_KEY_MAYBE(CONFIG_INIT_ON_ALLOC_DEFAULT_ON, init_on_alloc);\nstatic inline bool want_init_on_alloc(gfp_t flags)\n{\n\tif (static_branch_maybe(CONFIG_INIT_ON_ALLOC_DEFAULT_ON,\n\t\t\t\t&init_on_alloc))\n\t\treturn true;\n\treturn flags & __GFP_ZERO;\n}\n\nDECLARE_STATIC_KEY_MAYBE(CONFIG_INIT_ON_FREE_DEFAULT_ON, init_on_free);\nstatic inline bool want_init_on_free(void)\n{\n\treturn static_branch_maybe(CONFIG_INIT_ON_FREE_DEFAULT_ON,\n\t\t\t\t   &init_on_free);\n}\n\nextern bool _debug_pagealloc_enabled_early;\nDECLARE_STATIC_KEY_FALSE(_debug_pagealloc_enabled);\n\nstatic inline bool debug_pagealloc_enabled(void)\n{\n\treturn IS_ENABLED(CONFIG_DEBUG_PAGEALLOC) &&\n\t\t_debug_pagealloc_enabled_early;\n}\n\n \nstatic inline bool debug_pagealloc_enabled_static(void)\n{\n\tif (!IS_ENABLED(CONFIG_DEBUG_PAGEALLOC))\n\t\treturn false;\n\n\treturn static_branch_unlikely(&_debug_pagealloc_enabled);\n}\n\n \nextern void __kernel_map_pages(struct page *page, int numpages, int enable);\n#ifdef CONFIG_DEBUG_PAGEALLOC\nstatic inline void debug_pagealloc_map_pages(struct page *page, int numpages)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\t__kernel_map_pages(page, numpages, 1);\n}\n\nstatic inline void debug_pagealloc_unmap_pages(struct page *page, int numpages)\n{\n\tif (debug_pagealloc_enabled_static())\n\t\t__kernel_map_pages(page, numpages, 0);\n}\n\nextern unsigned int _debug_guardpage_minorder;\nDECLARE_STATIC_KEY_FALSE(_debug_guardpage_enabled);\n\nstatic inline unsigned int debug_guardpage_minorder(void)\n{\n\treturn _debug_guardpage_minorder;\n}\n\nstatic inline bool debug_guardpage_enabled(void)\n{\n\treturn static_branch_unlikely(&_debug_guardpage_enabled);\n}\n\nstatic inline bool page_is_guard(struct page *page)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\n\treturn PageGuard(page);\n}\n\nbool __set_page_guard(struct zone *zone, struct page *page, unsigned int order,\n\t\t      int migratetype);\nstatic inline bool set_page_guard(struct zone *zone, struct page *page,\n\t\t\t\t  unsigned int order, int migratetype)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn false;\n\treturn __set_page_guard(zone, page, order, migratetype);\n}\n\nvoid __clear_page_guard(struct zone *zone, struct page *page, unsigned int order,\n\t\t\tint migratetype);\nstatic inline void clear_page_guard(struct zone *zone, struct page *page,\n\t\t\t\t    unsigned int order, int migratetype)\n{\n\tif (!debug_guardpage_enabled())\n\t\treturn;\n\t__clear_page_guard(zone, page, order, migratetype);\n}\n\n#else\t \nstatic inline void debug_pagealloc_map_pages(struct page *page, int numpages) {}\nstatic inline void debug_pagealloc_unmap_pages(struct page *page, int numpages) {}\nstatic inline unsigned int debug_guardpage_minorder(void) { return 0; }\nstatic inline bool debug_guardpage_enabled(void) { return false; }\nstatic inline bool page_is_guard(struct page *page) { return false; }\nstatic inline bool set_page_guard(struct zone *zone, struct page *page,\n\t\t\tunsigned int order, int migratetype) { return false; }\nstatic inline void clear_page_guard(struct zone *zone, struct page *page,\n\t\t\t\tunsigned int order, int migratetype) {}\n#endif\t \n\n#ifdef __HAVE_ARCH_GATE_AREA\nextern struct vm_area_struct *get_gate_vma(struct mm_struct *mm);\nextern int in_gate_area_no_mm(unsigned long addr);\nextern int in_gate_area(struct mm_struct *mm, unsigned long addr);\n#else\nstatic inline struct vm_area_struct *get_gate_vma(struct mm_struct *mm)\n{\n\treturn NULL;\n}\nstatic inline int in_gate_area_no_mm(unsigned long addr) { return 0; }\nstatic inline int in_gate_area(struct mm_struct *mm, unsigned long addr)\n{\n\treturn 0;\n}\n#endif\t \n\nextern bool process_shares_mm(struct task_struct *p, struct mm_struct *mm);\n\n#ifdef CONFIG_SYSCTL\nextern int sysctl_drop_caches;\nint drop_caches_sysctl_handler(struct ctl_table *, int, void *, size_t *,\n\t\tloff_t *);\n#endif\n\nvoid drop_slab(void);\n\n#ifndef CONFIG_MMU\n#define randomize_va_space 0\n#else\nextern int randomize_va_space;\n#endif\n\nconst char * arch_vma_name(struct vm_area_struct *vma);\n#ifdef CONFIG_MMU\nvoid print_vma_addr(char *prefix, unsigned long rip);\n#else\nstatic inline void print_vma_addr(char *prefix, unsigned long rip)\n{\n}\n#endif\n\nvoid *sparse_buffer_alloc(unsigned long size);\nstruct page * __populate_section_memmap(unsigned long pfn,\n\t\tunsigned long nr_pages, int nid, struct vmem_altmap *altmap,\n\t\tstruct dev_pagemap *pgmap);\nvoid pmd_init(void *addr);\nvoid pud_init(void *addr);\npgd_t *vmemmap_pgd_populate(unsigned long addr, int node);\np4d_t *vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node);\npud_t *vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node);\npmd_t *vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node);\npte_t *vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node,\n\t\t\t    struct vmem_altmap *altmap, struct page *reuse);\nvoid *vmemmap_alloc_block(unsigned long size, int node);\nstruct vmem_altmap;\nvoid *vmemmap_alloc_block_buf(unsigned long size, int node,\n\t\t\t      struct vmem_altmap *altmap);\nvoid vmemmap_verify(pte_t *, int, unsigned long, unsigned long);\nvoid vmemmap_set_pmd(pmd_t *pmd, void *p, int node,\n\t\t     unsigned long addr, unsigned long next);\nint vmemmap_check_pmd(pmd_t *pmd, int node,\n\t\t      unsigned long addr, unsigned long next);\nint vmemmap_populate_basepages(unsigned long start, unsigned long end,\n\t\t\t       int node, struct vmem_altmap *altmap);\nint vmemmap_populate_hugepages(unsigned long start, unsigned long end,\n\t\t\t       int node, struct vmem_altmap *altmap);\nint vmemmap_populate(unsigned long start, unsigned long end, int node,\n\t\tstruct vmem_altmap *altmap);\nvoid vmemmap_populate_print_last(void);\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid vmemmap_free(unsigned long start, unsigned long end,\n\t\tstruct vmem_altmap *altmap);\n#endif\n\n#define VMEMMAP_RESERVE_NR\t2\n#ifdef CONFIG_ARCH_WANT_OPTIMIZE_DAX_VMEMMAP\nstatic inline bool __vmemmap_can_optimize(struct vmem_altmap *altmap,\n\t\t\t\t\t  struct dev_pagemap *pgmap)\n{\n\tunsigned long nr_pages;\n\tunsigned long nr_vmemmap_pages;\n\n\tif (!pgmap || !is_power_of_2(sizeof(struct page)))\n\t\treturn false;\n\n\tnr_pages = pgmap_vmemmap_nr(pgmap);\n\tnr_vmemmap_pages = ((nr_pages * sizeof(struct page)) >> PAGE_SHIFT);\n\t \n\treturn !altmap && (nr_vmemmap_pages > VMEMMAP_RESERVE_NR);\n}\n \n#ifndef vmemmap_can_optimize\n#define vmemmap_can_optimize __vmemmap_can_optimize\n#endif\n\n#else\nstatic inline bool vmemmap_can_optimize(struct vmem_altmap *altmap,\n\t\t\t\t\t   struct dev_pagemap *pgmap)\n{\n\treturn false;\n}\n#endif\n\nvoid register_page_bootmem_memmap(unsigned long section_nr, struct page *map,\n\t\t\t\t  unsigned long nr_pages);\n\nenum mf_flags {\n\tMF_COUNT_INCREASED = 1 << 0,\n\tMF_ACTION_REQUIRED = 1 << 1,\n\tMF_MUST_KILL = 1 << 2,\n\tMF_SOFT_OFFLINE = 1 << 3,\n\tMF_UNPOISON = 1 << 4,\n\tMF_SW_SIMULATED = 1 << 5,\n\tMF_NO_RETRY = 1 << 6,\n};\nint mf_dax_kill_procs(struct address_space *mapping, pgoff_t index,\n\t\t      unsigned long count, int mf_flags);\nextern int memory_failure(unsigned long pfn, int flags);\nextern void memory_failure_queue_kick(int cpu);\nextern int unpoison_memory(unsigned long pfn);\nextern void shake_page(struct page *p);\nextern atomic_long_t num_poisoned_pages __read_mostly;\nextern int soft_offline_page(unsigned long pfn, int flags);\n#ifdef CONFIG_MEMORY_FAILURE\n \nextern const struct attribute_group memory_failure_attr_group;\nextern void memory_failure_queue(unsigned long pfn, int flags);\nextern int __get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\t\tbool *migratable_cleared);\nvoid num_poisoned_pages_inc(unsigned long pfn);\nvoid num_poisoned_pages_sub(unsigned long pfn, long i);\nstruct task_struct *task_early_kill(struct task_struct *tsk, int force_early);\n#else\nstatic inline void memory_failure_queue(unsigned long pfn, int flags)\n{\n}\n\nstatic inline int __get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\t\tbool *migratable_cleared)\n{\n\treturn 0;\n}\n\nstatic inline void num_poisoned_pages_inc(unsigned long pfn)\n{\n}\n\nstatic inline void num_poisoned_pages_sub(unsigned long pfn, long i)\n{\n}\n#endif\n\n#if defined(CONFIG_MEMORY_FAILURE) && defined(CONFIG_KSM)\nvoid add_to_kill_ksm(struct task_struct *tsk, struct page *p,\n\t\t     struct vm_area_struct *vma, struct list_head *to_kill,\n\t\t     unsigned long ksm_addr);\n#endif\n\n#if defined(CONFIG_MEMORY_FAILURE) && defined(CONFIG_MEMORY_HOTPLUG)\nextern void memblk_nr_poison_inc(unsigned long pfn);\nextern void memblk_nr_poison_sub(unsigned long pfn, long i);\n#else\nstatic inline void memblk_nr_poison_inc(unsigned long pfn)\n{\n}\n\nstatic inline void memblk_nr_poison_sub(unsigned long pfn, long i)\n{\n}\n#endif\n\n#ifndef arch_memory_failure\nstatic inline int arch_memory_failure(unsigned long pfn, int flags)\n{\n\treturn -ENXIO;\n}\n#endif\n\n#ifndef arch_is_platform_page\nstatic inline bool arch_is_platform_page(u64 paddr)\n{\n\treturn false;\n}\n#endif\n\n \nenum mf_result {\n\tMF_IGNORED,\t \n\tMF_FAILED,\t \n\tMF_DELAYED,\t \n\tMF_RECOVERED,\t \n};\n\nenum mf_action_page_type {\n\tMF_MSG_KERNEL,\n\tMF_MSG_KERNEL_HIGH_ORDER,\n\tMF_MSG_SLAB,\n\tMF_MSG_DIFFERENT_COMPOUND,\n\tMF_MSG_HUGE,\n\tMF_MSG_FREE_HUGE,\n\tMF_MSG_UNMAP_FAILED,\n\tMF_MSG_DIRTY_SWAPCACHE,\n\tMF_MSG_CLEAN_SWAPCACHE,\n\tMF_MSG_DIRTY_MLOCKED_LRU,\n\tMF_MSG_CLEAN_MLOCKED_LRU,\n\tMF_MSG_DIRTY_UNEVICTABLE_LRU,\n\tMF_MSG_CLEAN_UNEVICTABLE_LRU,\n\tMF_MSG_DIRTY_LRU,\n\tMF_MSG_CLEAN_LRU,\n\tMF_MSG_TRUNCATED_LRU,\n\tMF_MSG_BUDDY,\n\tMF_MSG_DAX,\n\tMF_MSG_UNSPLIT_THP,\n\tMF_MSG_UNKNOWN,\n};\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_HUGETLBFS)\nextern void clear_huge_page(struct page *page,\n\t\t\t    unsigned long addr_hint,\n\t\t\t    unsigned int pages_per_huge_page);\nint copy_user_large_folio(struct folio *dst, struct folio *src,\n\t\t\t  unsigned long addr_hint,\n\t\t\t  struct vm_area_struct *vma);\nlong copy_folio_from_user(struct folio *dst_folio,\n\t\t\t   const void __user *usr_src,\n\t\t\t   bool allow_pagefault);\n\n \nstatic inline bool vma_is_special_huge(const struct vm_area_struct *vma)\n{\n\treturn vma_is_dax(vma) || (vma->vm_file &&\n\t\t\t\t   (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)));\n}\n\n#endif  \n\n#if MAX_NUMNODES > 1\nvoid __init setup_nr_node_ids(void);\n#else\nstatic inline void setup_nr_node_ids(void) {}\n#endif\n\nextern int memcmp_pages(struct page *page1, struct page *page2);\n\nstatic inline int pages_identical(struct page *page1, struct page *page2)\n{\n\treturn !memcmp_pages(page1, page2);\n}\n\n#ifdef CONFIG_MAPPING_DIRTY_HELPERS\nunsigned long clean_record_shared_mapping_range(struct address_space *mapping,\n\t\t\t\t\t\tpgoff_t first_index, pgoff_t nr,\n\t\t\t\t\t\tpgoff_t bitmap_pgoff,\n\t\t\t\t\t\tunsigned long *bitmap,\n\t\t\t\t\t\tpgoff_t *start,\n\t\t\t\t\t\tpgoff_t *end);\n\nunsigned long wp_shared_mapping_range(struct address_space *mapping,\n\t\t\t\t      pgoff_t first_index, pgoff_t nr);\n#endif\n\nextern int sysctl_nr_trim_pages;\n\n#ifdef CONFIG_PRINTK\nvoid mem_dump_obj(void *object);\n#else\nstatic inline void mem_dump_obj(void *object) {}\n#endif\n\n \nstatic inline int seal_check_future_write(int seals, struct vm_area_struct *vma)\n{\n\tif (seals & F_SEAL_FUTURE_WRITE) {\n\t\t \n\t\tif ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_WRITE))\n\t\t\treturn -EPERM;\n\n\t\t \n\t\tif (vma->vm_flags & VM_SHARED)\n\t\t\tvm_flags_clear(vma, VM_MAYWRITE);\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_ANON_VMA_NAME\nint madvise_set_anon_name(struct mm_struct *mm, unsigned long start,\n\t\t\t  unsigned long len_in,\n\t\t\t  struct anon_vma_name *anon_name);\n#else\nstatic inline int\nmadvise_set_anon_name(struct mm_struct *mm, unsigned long start,\n\t\t      unsigned long len_in, struct anon_vma_name *anon_name) {\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\nbool range_contains_unaccepted_memory(phys_addr_t start, phys_addr_t end);\nvoid accept_memory(phys_addr_t start, phys_addr_t end);\n\n#else\n\nstatic inline bool range_contains_unaccepted_memory(phys_addr_t start,\n\t\t\t\t\t\t    phys_addr_t end)\n{\n\treturn false;\n}\n\nstatic inline void accept_memory(phys_addr_t start, phys_addr_t end)\n{\n}\n\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}