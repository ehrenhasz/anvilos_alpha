{
  "module_name": "mm_types.h",
  "hash_id": "247254949b3f655b844e6b1a5eadf6c54936ef9aba134b8e13b3019f3d66d6b4",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/mm_types.h",
  "human_readable_source": " \n#ifndef _LINUX_MM_TYPES_H\n#define _LINUX_MM_TYPES_H\n\n#include <linux/mm_types_task.h>\n\n#include <linux/auxvec.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/rbtree.h>\n#include <linux/maple_tree.h>\n#include <linux/rwsem.h>\n#include <linux/completion.h>\n#include <linux/cpumask.h>\n#include <linux/uprobes.h>\n#include <linux/rcupdate.h>\n#include <linux/page-flags-layout.h>\n#include <linux/workqueue.h>\n#include <linux/seqlock.h>\n#include <linux/percpu_counter.h>\n\n#include <asm/mmu.h>\n\n#ifndef AT_VECTOR_SIZE_ARCH\n#define AT_VECTOR_SIZE_ARCH 0\n#endif\n#define AT_VECTOR_SIZE (2*(AT_VECTOR_SIZE_ARCH + AT_VECTOR_SIZE_BASE + 1))\n\n#define INIT_PASID\t0\n\nstruct address_space;\nstruct mem_cgroup;\n\n \n#ifdef CONFIG_HAVE_ALIGNED_STRUCT_PAGE\n#define _struct_page_alignment\t__aligned(2 * sizeof(unsigned long))\n#else\n#define _struct_page_alignment\t__aligned(sizeof(unsigned long))\n#endif\n\nstruct page {\n\tunsigned long flags;\t\t \n\t \n\tunion {\n\t\tstruct {\t \n\t\t\t \n\t\t\tunion {\n\t\t\t\tstruct list_head lru;\n\n\t\t\t\t \n\t\t\t\tstruct {\n\t\t\t\t\t \n\t\t\t\t\tvoid *__filler;\n\t\t\t\t\t \n\t\t\t\t\tunsigned int mlock_count;\n\t\t\t\t};\n\n\t\t\t\t \n\t\t\t\tstruct list_head buddy_list;\n\t\t\t\tstruct list_head pcp_list;\n\t\t\t};\n\t\t\t \n\t\t\tstruct address_space *mapping;\n\t\t\tunion {\n\t\t\t\tpgoff_t index;\t\t \n\t\t\t\tunsigned long share;\t \n\t\t\t};\n\t\t\t \n\t\t\tunsigned long private;\n\t\t};\n\t\tstruct {\t \n\t\t\t \n\t\t\tunsigned long pp_magic;\n\t\t\tstruct page_pool *pp;\n\t\t\tunsigned long _pp_mapping_pad;\n\t\t\tunsigned long dma_addr;\n\t\t\tunion {\n\t\t\t\t \n\t\t\t\tunsigned long dma_addr_upper;\n\t\t\t\t \n\t\t\t\tatomic_long_t pp_frag_count;\n\t\t\t};\n\t\t};\n\t\tstruct {\t \n\t\t\tunsigned long compound_head;\t \n\t\t};\n\t\tstruct {\t \n\t\t\t \n\t\t\tstruct dev_pagemap *pgmap;\n\t\t\tvoid *zone_device_data;\n\t\t\t \n\t\t};\n\n\t\t \n\t\tstruct rcu_head rcu_head;\n\t};\n\n\tunion {\t\t \n\t\t \n\t\tatomic_t _mapcount;\n\n\t\t \n\t\tunsigned int page_type;\n\t};\n\n\t \n\tatomic_t _refcount;\n\n#ifdef CONFIG_MEMCG\n\tunsigned long memcg_data;\n#endif\n\n\t \n#if defined(WANT_PAGE_VIRTUAL)\n\tvoid *virtual;\t\t\t \n#endif  \n\n#ifdef CONFIG_KMSAN\n\t \n\tstruct page *kmsan_shadow;\n\tstruct page *kmsan_origin;\n#endif\n\n#ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS\n\tint _last_cpupid;\n#endif\n} _struct_page_alignment;\n\n \nstruct encoded_page;\n#define ENCODE_PAGE_BITS 3ul\nstatic __always_inline struct encoded_page *encode_page(struct page *page, unsigned long flags)\n{\n\tBUILD_BUG_ON(flags > ENCODE_PAGE_BITS);\n\treturn (struct encoded_page *)(flags | (unsigned long)page);\n}\n\nstatic inline unsigned long encoded_page_flags(struct encoded_page *page)\n{\n\treturn ENCODE_PAGE_BITS & (unsigned long)page;\n}\n\nstatic inline struct page *encoded_page_ptr(struct encoded_page *page)\n{\n\treturn (struct page *)(~ENCODE_PAGE_BITS & (unsigned long)page);\n}\n\n \ntypedef struct {\n\tunsigned long val;\n} swp_entry_t;\n\n \nstruct folio {\n\t \n\tunion {\n\t\tstruct {\n\t \n\t\t\tunsigned long flags;\n\t\t\tunion {\n\t\t\t\tstruct list_head lru;\n\t \n\t\t\t\tstruct {\n\t\t\t\t\tvoid *__filler;\n\t \n\t\t\t\t\tunsigned int mlock_count;\n\t \n\t\t\t\t};\n\t \n\t\t\t};\n\t\t\tstruct address_space *mapping;\n\t\t\tpgoff_t index;\n\t\t\tunion {\n\t\t\t\tvoid *private;\n\t\t\t\tswp_entry_t swap;\n\t\t\t};\n\t\t\tatomic_t _mapcount;\n\t\t\tatomic_t _refcount;\n#ifdef CONFIG_MEMCG\n\t\t\tunsigned long memcg_data;\n#endif\n\t \n\t\t};\n\t\tstruct page page;\n\t};\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long _flags_1;\n\t\t\tunsigned long _head_1;\n\t\t\tunsigned long _folio_avail;\n\t \n\t\t\tatomic_t _entire_mapcount;\n\t\t\tatomic_t _nr_pages_mapped;\n\t\t\tatomic_t _pincount;\n#ifdef CONFIG_64BIT\n\t\t\tunsigned int _folio_nr_pages;\n#endif\n\t \n\t\t};\n\t\tstruct page __page_1;\n\t};\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long _flags_2;\n\t\t\tunsigned long _head_2;\n\t \n\t\t\tvoid *_hugetlb_subpool;\n\t\t\tvoid *_hugetlb_cgroup;\n\t\t\tvoid *_hugetlb_cgroup_rsvd;\n\t\t\tvoid *_hugetlb_hwpoison;\n\t \n\t\t};\n\t\tstruct {\n\t\t\tunsigned long _flags_2a;\n\t\t\tunsigned long _head_2a;\n\t \n\t\t\tstruct list_head _deferred_list;\n\t \n\t\t};\n\t\tstruct page __page_2;\n\t};\n};\n\n#define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n\tstatic_assert(offsetof(struct page, pg) == offsetof(struct folio, fl))\nFOLIO_MATCH(flags, flags);\nFOLIO_MATCH(lru, lru);\nFOLIO_MATCH(mapping, mapping);\nFOLIO_MATCH(compound_head, lru);\nFOLIO_MATCH(index, index);\nFOLIO_MATCH(private, private);\nFOLIO_MATCH(_mapcount, _mapcount);\nFOLIO_MATCH(_refcount, _refcount);\n#ifdef CONFIG_MEMCG\nFOLIO_MATCH(memcg_data, memcg_data);\n#endif\n#undef FOLIO_MATCH\n#define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n\tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n\t\t\toffsetof(struct page, pg) + sizeof(struct page))\nFOLIO_MATCH(flags, _flags_1);\nFOLIO_MATCH(compound_head, _head_1);\n#undef FOLIO_MATCH\n#define FOLIO_MATCH(pg, fl)\t\t\t\t\t\t\\\n\tstatic_assert(offsetof(struct folio, fl) ==\t\t\t\\\n\t\t\toffsetof(struct page, pg) + 2 * sizeof(struct page))\nFOLIO_MATCH(flags, _flags_2);\nFOLIO_MATCH(compound_head, _head_2);\nFOLIO_MATCH(flags, _flags_2a);\nFOLIO_MATCH(compound_head, _head_2a);\n#undef FOLIO_MATCH\n\n \nstruct ptdesc {\n\tunsigned long __page_flags;\n\n\tunion {\n\t\tstruct rcu_head pt_rcu_head;\n\t\tstruct list_head pt_list;\n\t\tstruct {\n\t\t\tunsigned long _pt_pad_1;\n\t\t\tpgtable_t pmd_huge_pte;\n\t\t};\n\t};\n\tunsigned long __page_mapping;\n\n\tunion {\n\t\tstruct mm_struct *pt_mm;\n\t\tatomic_t pt_frag_refcount;\n\t};\n\n\tunion {\n\t\tunsigned long _pt_pad_2;\n#if ALLOC_SPLIT_PTLOCKS\n\t\tspinlock_t *ptl;\n#else\n\t\tspinlock_t ptl;\n#endif\n\t};\n\tunsigned int __page_type;\n\tatomic_t _refcount;\n#ifdef CONFIG_MEMCG\n\tunsigned long pt_memcg_data;\n#endif\n};\n\n#define TABLE_MATCH(pg, pt)\t\t\t\t\t\t\\\n\tstatic_assert(offsetof(struct page, pg) == offsetof(struct ptdesc, pt))\nTABLE_MATCH(flags, __page_flags);\nTABLE_MATCH(compound_head, pt_list);\nTABLE_MATCH(compound_head, _pt_pad_1);\nTABLE_MATCH(mapping, __page_mapping);\nTABLE_MATCH(rcu_head, pt_rcu_head);\nTABLE_MATCH(page_type, __page_type);\nTABLE_MATCH(_refcount, _refcount);\n#ifdef CONFIG_MEMCG\nTABLE_MATCH(memcg_data, pt_memcg_data);\n#endif\n#undef TABLE_MATCH\nstatic_assert(sizeof(struct ptdesc) <= sizeof(struct page));\n\n#define ptdesc_page(pt)\t\t\t(_Generic((pt),\t\t\t\\\n\tconst struct ptdesc *:\t\t(const struct page *)(pt),\t\\\n\tstruct ptdesc *:\t\t(struct page *)(pt)))\n\n#define ptdesc_folio(pt)\t\t(_Generic((pt),\t\t\t\\\n\tconst struct ptdesc *:\t\t(const struct folio *)(pt),\t\\\n\tstruct ptdesc *:\t\t(struct folio *)(pt)))\n\n#define page_ptdesc(p)\t\t\t(_Generic((p),\t\t\t\\\n\tconst struct page *:\t\t(const struct ptdesc *)(p),\t\\\n\tstruct page *:\t\t\t(struct ptdesc *)(p)))\n\n \n#define STRUCT_PAGE_MAX_SHIFT\t(order_base_2(sizeof(struct page)))\n\n#define PAGE_FRAG_CACHE_MAX_SIZE\t__ALIGN_MASK(32768, ~PAGE_MASK)\n#define PAGE_FRAG_CACHE_MAX_ORDER\tget_order(PAGE_FRAG_CACHE_MAX_SIZE)\n\n \n#define page_private(page)\t\t((page)->private)\n\nstatic inline void set_page_private(struct page *page, unsigned long private)\n{\n\tpage->private = private;\n}\n\nstatic inline void *folio_get_private(struct folio *folio)\n{\n\treturn folio->private;\n}\n\nstruct page_frag_cache {\n\tvoid * va;\n#if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)\n\t__u16 offset;\n\t__u16 size;\n#else\n\t__u32 offset;\n#endif\n\t \n\tunsigned int\t\tpagecnt_bias;\n\tbool pfmemalloc;\n};\n\ntypedef unsigned long vm_flags_t;\n\n \nstruct vm_region {\n\tstruct rb_node\tvm_rb;\t\t \n\tvm_flags_t\tvm_flags;\t \n\tunsigned long\tvm_start;\t \n\tunsigned long\tvm_end;\t\t \n\tunsigned long\tvm_top;\t\t \n\tunsigned long\tvm_pgoff;\t \n\tstruct file\t*vm_file;\t \n\n\tint\t\tvm_usage;\t \n\tbool\t\tvm_icache_flushed : 1;  \n};\n\n#ifdef CONFIG_USERFAULTFD\n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) { NULL, })\nstruct vm_userfaultfd_ctx {\n\tstruct userfaultfd_ctx *ctx;\n};\n#else  \n#define NULL_VM_UFFD_CTX ((struct vm_userfaultfd_ctx) {})\nstruct vm_userfaultfd_ctx {};\n#endif  \n\nstruct anon_vma_name {\n\tstruct kref kref;\n\t \n\tchar name[];\n};\n\nstruct vma_lock {\n\tstruct rw_semaphore lock;\n};\n\nstruct vma_numab_state {\n\tunsigned long next_scan;\n\tunsigned long next_pid_reset;\n\tunsigned long access_pids[2];\n};\n\n \nstruct vm_area_struct {\n\t \n\n\tunion {\n\t\tstruct {\n\t\t\t \n\t\t\tunsigned long vm_start;\n\t\t\tunsigned long vm_end;\n\t\t};\n#ifdef CONFIG_PER_VMA_LOCK\n\t\tstruct rcu_head vm_rcu;\t \n#endif\n\t};\n\n\tstruct mm_struct *vm_mm;\t \n\tpgprot_t vm_page_prot;           \n\n\t \n\tunion {\n\t\tconst vm_flags_t vm_flags;\n\t\tvm_flags_t __private __vm_flags;\n\t};\n\n#ifdef CONFIG_PER_VMA_LOCK\n\t \n\tint vm_lock_seq;\n\tstruct vma_lock *vm_lock;\n\n\t \n\tbool detached;\n#endif\n\n\t \n\tstruct {\n\t\tstruct rb_node rb;\n\t\tunsigned long rb_subtree_last;\n\t} shared;\n\n\t \n\tstruct list_head anon_vma_chain;  \n\tstruct anon_vma *anon_vma;\t \n\n\t \n\tconst struct vm_operations_struct *vm_ops;\n\n\t \n\tunsigned long vm_pgoff;\t\t \n\tstruct file * vm_file;\t\t \n\tvoid * vm_private_data;\t\t \n\n#ifdef CONFIG_ANON_VMA_NAME\n\t \n\tstruct anon_vma_name *anon_name;\n#endif\n#ifdef CONFIG_SWAP\n\tatomic_long_t swap_readahead_info;\n#endif\n#ifndef CONFIG_MMU\n\tstruct vm_region *vm_region;\t \n#endif\n#ifdef CONFIG_NUMA\n\tstruct mempolicy *vm_policy;\t \n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tstruct vma_numab_state *numab_state;\t \n#endif\n\tstruct vm_userfaultfd_ctx vm_userfaultfd_ctx;\n} __randomize_layout;\n\n#ifdef CONFIG_SCHED_MM_CID\nstruct mm_cid {\n\tu64 time;\n\tint cid;\n};\n#endif\n\nstruct kioctx_table;\nstruct mm_struct {\n\tstruct {\n\t\t \n\t\tstruct {\n\t\t\t \n\t\t\tatomic_t mm_count;\n\t\t} ____cacheline_aligned_in_smp;\n\n\t\tstruct maple_tree mm_mt;\n#ifdef CONFIG_MMU\n\t\tunsigned long (*get_unmapped_area) (struct file *filp,\n\t\t\t\tunsigned long addr, unsigned long len,\n\t\t\t\tunsigned long pgoff, unsigned long flags);\n#endif\n\t\tunsigned long mmap_base;\t \n\t\tunsigned long mmap_legacy_base;\t \n#ifdef CONFIG_HAVE_ARCH_COMPAT_MMAP_BASES\n\t\t \n\t\tunsigned long mmap_compat_base;\n\t\tunsigned long mmap_compat_legacy_base;\n#endif\n\t\tunsigned long task_size;\t \n\t\tpgd_t * pgd;\n\n#ifdef CONFIG_MEMBARRIER\n\t\t \n\t\tatomic_t membarrier_state;\n#endif\n\n\t\t \n\t\tatomic_t mm_users;\n\n#ifdef CONFIG_SCHED_MM_CID\n\t\t \n\t\tstruct mm_cid __percpu *pcpu_cid;\n\t\t \n\t\tunsigned long mm_cid_next_scan;\n#endif\n#ifdef CONFIG_MMU\n\t\tatomic_long_t pgtables_bytes;\t \n#endif\n\t\tint map_count;\t\t\t \n\n\t\tspinlock_t page_table_lock;  \n\t\t \n\t\tstruct rw_semaphore mmap_lock;\n\n\t\tstruct list_head mmlist;  \n#ifdef CONFIG_PER_VMA_LOCK\n\t\t \n\t\tint mm_lock_seq;\n#endif\n\n\n\t\tunsigned long hiwater_rss;  \n\t\tunsigned long hiwater_vm;   \n\n\t\tunsigned long total_vm;\t    \n\t\tunsigned long locked_vm;    \n\t\tatomic64_t    pinned_vm;    \n\t\tunsigned long data_vm;\t    \n\t\tunsigned long exec_vm;\t    \n\t\tunsigned long stack_vm;\t    \n\t\tunsigned long def_flags;\n\n\t\t \n\t\tseqcount_t write_protect_seq;\n\n\t\tspinlock_t arg_lock;  \n\n\t\tunsigned long start_code, end_code, start_data, end_data;\n\t\tunsigned long start_brk, brk, start_stack;\n\t\tunsigned long arg_start, arg_end, env_start, env_end;\n\n\t\tunsigned long saved_auxv[AT_VECTOR_SIZE];  \n\n\t\tstruct percpu_counter rss_stat[NR_MM_COUNTERS];\n\n\t\tstruct linux_binfmt *binfmt;\n\n\t\t \n\t\tmm_context_t context;\n\n\t\tunsigned long flags;  \n\n#ifdef CONFIG_AIO\n\t\tspinlock_t\t\t\tioctx_lock;\n\t\tstruct kioctx_table __rcu\t*ioctx_table;\n#endif\n#ifdef CONFIG_MEMCG\n\t\t \n\t\tstruct task_struct __rcu *owner;\n#endif\n\t\tstruct user_namespace *user_ns;\n\n\t\t \n\t\tstruct file __rcu *exe_file;\n#ifdef CONFIG_MMU_NOTIFIER\n\t\tstruct mmu_notifier_subscriptions *notifier_subscriptions;\n#endif\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS\n\t\tpgtable_t pmd_huge_pte;  \n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\t\t \n\t\tunsigned long numa_next_scan;\n\n\t\t \n\t\tunsigned long numa_scan_offset;\n\n\t\t \n\t\tint numa_scan_seq;\n#endif\n\t\t \n\t\tatomic_t tlb_flush_pending;\n#ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH\n\t\t \n\t\tatomic_t tlb_flush_batched;\n#endif\n\t\tstruct uprobes_state uprobes_state;\n#ifdef CONFIG_PREEMPT_RT\n\t\tstruct rcu_head delayed_drop;\n#endif\n#ifdef CONFIG_HUGETLB_PAGE\n\t\tatomic_long_t hugetlb_usage;\n#endif\n\t\tstruct work_struct async_put_work;\n\n#ifdef CONFIG_IOMMU_SVA\n\t\tu32 pasid;\n#endif\n#ifdef CONFIG_KSM\n\t\t \n\t\tunsigned long ksm_merging_pages;\n\t\t \n\t\tunsigned long ksm_rmap_items;\n\t\t \n\t\tunsigned long ksm_zero_pages;\n#endif  \n#ifdef CONFIG_LRU_GEN\n\t\tstruct {\n\t\t\t \n\t\t\tstruct list_head list;\n\t\t\t \n\t\t\tunsigned long bitmap;\n#ifdef CONFIG_MEMCG\n\t\t\t \n\t\t\tstruct mem_cgroup *memcg;\n#endif\n\t\t} lru_gen;\n#endif  \n\t} __randomize_layout;\n\n\t \n\tunsigned long cpu_bitmap[];\n};\n\n#define MM_MT_FLAGS\t(MT_FLAGS_ALLOC_RANGE | MT_FLAGS_LOCK_EXTERN | \\\n\t\t\t MT_FLAGS_USE_RCU)\nextern struct mm_struct init_mm;\n\n \nstatic inline void mm_init_cpumask(struct mm_struct *mm)\n{\n\tunsigned long cpu_bitmap = (unsigned long)mm;\n\n\tcpu_bitmap += offsetof(struct mm_struct, cpu_bitmap);\n\tcpumask_clear((struct cpumask *)cpu_bitmap);\n}\n\n \nstatic inline cpumask_t *mm_cpumask(struct mm_struct *mm)\n{\n\treturn (struct cpumask *)&mm->cpu_bitmap;\n}\n\n#ifdef CONFIG_LRU_GEN\n\nstruct lru_gen_mm_list {\n\t \n\tstruct list_head fifo;\n\t \n\tspinlock_t lock;\n};\n\nvoid lru_gen_add_mm(struct mm_struct *mm);\nvoid lru_gen_del_mm(struct mm_struct *mm);\n#ifdef CONFIG_MEMCG\nvoid lru_gen_migrate_mm(struct mm_struct *mm);\n#endif\n\nstatic inline void lru_gen_init_mm(struct mm_struct *mm)\n{\n\tINIT_LIST_HEAD(&mm->lru_gen.list);\n\tmm->lru_gen.bitmap = 0;\n#ifdef CONFIG_MEMCG\n\tmm->lru_gen.memcg = NULL;\n#endif\n}\n\nstatic inline void lru_gen_use_mm(struct mm_struct *mm)\n{\n\t \n\tWRITE_ONCE(mm->lru_gen.bitmap, -1);\n}\n\n#else  \n\nstatic inline void lru_gen_add_mm(struct mm_struct *mm)\n{\n}\n\nstatic inline void lru_gen_del_mm(struct mm_struct *mm)\n{\n}\n\n#ifdef CONFIG_MEMCG\nstatic inline void lru_gen_migrate_mm(struct mm_struct *mm)\n{\n}\n#endif\n\nstatic inline void lru_gen_init_mm(struct mm_struct *mm)\n{\n}\n\nstatic inline void lru_gen_use_mm(struct mm_struct *mm)\n{\n}\n\n#endif  \n\nstruct vma_iterator {\n\tstruct ma_state mas;\n};\n\n#define VMA_ITERATOR(name, __mm, __addr)\t\t\t\t\\\n\tstruct vma_iterator name = {\t\t\t\t\t\\\n\t\t.mas = {\t\t\t\t\t\t\\\n\t\t\t.tree = &(__mm)->mm_mt,\t\t\t\t\\\n\t\t\t.index = __addr,\t\t\t\t\\\n\t\t\t.node = MAS_START,\t\t\t\t\\\n\t\t},\t\t\t\t\t\t\t\\\n\t}\n\nstatic inline void vma_iter_init(struct vma_iterator *vmi,\n\t\tstruct mm_struct *mm, unsigned long addr)\n{\n\tmas_init(&vmi->mas, &mm->mm_mt, addr);\n}\n\n#ifdef CONFIG_SCHED_MM_CID\n\nenum mm_cid_state {\n\tMM_CID_UNSET = -1U,\t\t \n\tMM_CID_LAZY_PUT = (1U << 31),\n};\n\nstatic inline bool mm_cid_is_unset(int cid)\n{\n\treturn cid == MM_CID_UNSET;\n}\n\nstatic inline bool mm_cid_is_lazy_put(int cid)\n{\n\treturn !mm_cid_is_unset(cid) && (cid & MM_CID_LAZY_PUT);\n}\n\nstatic inline bool mm_cid_is_valid(int cid)\n{\n\treturn !(cid & MM_CID_LAZY_PUT);\n}\n\nstatic inline int mm_cid_set_lazy_put(int cid)\n{\n\treturn cid | MM_CID_LAZY_PUT;\n}\n\nstatic inline int mm_cid_clear_lazy_put(int cid)\n{\n\treturn cid & ~MM_CID_LAZY_PUT;\n}\n\n \nstatic inline cpumask_t *mm_cidmask(struct mm_struct *mm)\n{\n\tunsigned long cid_bitmap = (unsigned long)mm;\n\n\tcid_bitmap += offsetof(struct mm_struct, cpu_bitmap);\n\t \n\tcid_bitmap += cpumask_size();\n\treturn (struct cpumask *)cid_bitmap;\n}\n\nstatic inline void mm_init_cid(struct mm_struct *mm)\n{\n\tint i;\n\n\tfor_each_possible_cpu(i) {\n\t\tstruct mm_cid *pcpu_cid = per_cpu_ptr(mm->pcpu_cid, i);\n\n\t\tpcpu_cid->cid = MM_CID_UNSET;\n\t\tpcpu_cid->time = 0;\n\t}\n\tcpumask_clear(mm_cidmask(mm));\n}\n\nstatic inline int mm_alloc_cid(struct mm_struct *mm)\n{\n\tmm->pcpu_cid = alloc_percpu(struct mm_cid);\n\tif (!mm->pcpu_cid)\n\t\treturn -ENOMEM;\n\tmm_init_cid(mm);\n\treturn 0;\n}\n\nstatic inline void mm_destroy_cid(struct mm_struct *mm)\n{\n\tfree_percpu(mm->pcpu_cid);\n\tmm->pcpu_cid = NULL;\n}\n\nstatic inline unsigned int mm_cid_size(void)\n{\n\treturn cpumask_size();\n}\n#else  \nstatic inline void mm_init_cid(struct mm_struct *mm) { }\nstatic inline int mm_alloc_cid(struct mm_struct *mm) { return 0; }\nstatic inline void mm_destroy_cid(struct mm_struct *mm) { }\nstatic inline unsigned int mm_cid_size(void)\n{\n\treturn 0;\n}\n#endif  \n\nstruct mmu_gather;\nextern void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm);\nextern void tlb_gather_mmu_fullmm(struct mmu_gather *tlb, struct mm_struct *mm);\nextern void tlb_finish_mmu(struct mmu_gather *tlb);\n\nstruct vm_fault;\n\n \ntypedef __bitwise unsigned int vm_fault_t;\n\n \nenum vm_fault_reason {\n\tVM_FAULT_OOM            = (__force vm_fault_t)0x000001,\n\tVM_FAULT_SIGBUS         = (__force vm_fault_t)0x000002,\n\tVM_FAULT_MAJOR          = (__force vm_fault_t)0x000004,\n\tVM_FAULT_HWPOISON       = (__force vm_fault_t)0x000010,\n\tVM_FAULT_HWPOISON_LARGE = (__force vm_fault_t)0x000020,\n\tVM_FAULT_SIGSEGV        = (__force vm_fault_t)0x000040,\n\tVM_FAULT_NOPAGE         = (__force vm_fault_t)0x000100,\n\tVM_FAULT_LOCKED         = (__force vm_fault_t)0x000200,\n\tVM_FAULT_RETRY          = (__force vm_fault_t)0x000400,\n\tVM_FAULT_FALLBACK       = (__force vm_fault_t)0x000800,\n\tVM_FAULT_DONE_COW       = (__force vm_fault_t)0x001000,\n\tVM_FAULT_NEEDDSYNC      = (__force vm_fault_t)0x002000,\n\tVM_FAULT_COMPLETED      = (__force vm_fault_t)0x004000,\n\tVM_FAULT_HINDEX_MASK    = (__force vm_fault_t)0x0f0000,\n};\n\n \n#define VM_FAULT_SET_HINDEX(x) ((__force vm_fault_t)((x) << 16))\n#define VM_FAULT_GET_HINDEX(x) (((__force unsigned int)(x) >> 16) & 0xf)\n\n#define VM_FAULT_ERROR (VM_FAULT_OOM | VM_FAULT_SIGBUS |\t\\\n\t\t\tVM_FAULT_SIGSEGV | VM_FAULT_HWPOISON |\t\\\n\t\t\tVM_FAULT_HWPOISON_LARGE | VM_FAULT_FALLBACK)\n\n#define VM_FAULT_RESULT_TRACE \\\n\t{ VM_FAULT_OOM,                 \"OOM\" },\t\\\n\t{ VM_FAULT_SIGBUS,              \"SIGBUS\" },\t\\\n\t{ VM_FAULT_MAJOR,               \"MAJOR\" },\t\\\n\t{ VM_FAULT_HWPOISON,            \"HWPOISON\" },\t\\\n\t{ VM_FAULT_HWPOISON_LARGE,      \"HWPOISON_LARGE\" },\t\\\n\t{ VM_FAULT_SIGSEGV,             \"SIGSEGV\" },\t\\\n\t{ VM_FAULT_NOPAGE,              \"NOPAGE\" },\t\\\n\t{ VM_FAULT_LOCKED,              \"LOCKED\" },\t\\\n\t{ VM_FAULT_RETRY,               \"RETRY\" },\t\\\n\t{ VM_FAULT_FALLBACK,            \"FALLBACK\" },\t\\\n\t{ VM_FAULT_DONE_COW,            \"DONE_COW\" },\t\\\n\t{ VM_FAULT_NEEDDSYNC,           \"NEEDDSYNC\" },\t\\\n\t{ VM_FAULT_COMPLETED,           \"COMPLETED\" }\n\nstruct vm_special_mapping {\n\tconst char *name;\t \n\n\t \n\tstruct page **pages;\n\n\t \n\tvm_fault_t (*fault)(const struct vm_special_mapping *sm,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tstruct vm_fault *vmf);\n\n\tint (*mremap)(const struct vm_special_mapping *sm,\n\t\t     struct vm_area_struct *new_vma);\n};\n\nenum tlb_flush_reason {\n\tTLB_FLUSH_ON_TASK_SWITCH,\n\tTLB_REMOTE_SHOOTDOWN,\n\tTLB_LOCAL_SHOOTDOWN,\n\tTLB_LOCAL_MM_SHOOTDOWN,\n\tTLB_REMOTE_SEND_IPI,\n\tNR_TLB_FLUSH_REASONS,\n};\n\n \nenum fault_flag {\n\tFAULT_FLAG_WRITE =\t\t1 << 0,\n\tFAULT_FLAG_MKWRITE =\t\t1 << 1,\n\tFAULT_FLAG_ALLOW_RETRY =\t1 << 2,\n\tFAULT_FLAG_RETRY_NOWAIT = \t1 << 3,\n\tFAULT_FLAG_KILLABLE =\t\t1 << 4,\n\tFAULT_FLAG_TRIED = \t\t1 << 5,\n\tFAULT_FLAG_USER =\t\t1 << 6,\n\tFAULT_FLAG_REMOTE =\t\t1 << 7,\n\tFAULT_FLAG_INSTRUCTION =\t1 << 8,\n\tFAULT_FLAG_INTERRUPTIBLE =\t1 << 9,\n\tFAULT_FLAG_UNSHARE =\t\t1 << 10,\n\tFAULT_FLAG_ORIG_PTE_VALID =\t1 << 11,\n\tFAULT_FLAG_VMA_LOCK =\t\t1 << 12,\n};\n\ntypedef unsigned int __bitwise zap_flags_t;\n\n \n\nenum {\n\t \n\tFOLL_WRITE = 1 << 0,\n\t \n\tFOLL_GET = 1 << 1,\n\t \n\tFOLL_DUMP = 1 << 2,\n\t \n\tFOLL_FORCE = 1 << 3,\n\t \n\tFOLL_NOWAIT = 1 << 4,\n\t \n\tFOLL_NOFAULT = 1 << 5,\n\t \n\tFOLL_HWPOISON = 1 << 6,\n\t \n\tFOLL_ANON = 1 << 7,\n\t \n\tFOLL_LONGTERM = 1 << 8,\n\t \n\tFOLL_SPLIT_PMD = 1 << 9,\n\t \n\tFOLL_PCI_P2PDMA = 1 << 10,\n\t \n\tFOLL_INTERRUPTIBLE = 1 << 11,\n\t \n\tFOLL_HONOR_NUMA_FAULT = 1 << 12,\n\n\t \n};\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}