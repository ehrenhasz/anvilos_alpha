{
  "module_name": "spinlock.h",
  "hash_id": "22cbf238ce63f62b2977c5665f81bc4afdcaafffd9598e56684d2e3c1021ca73",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/spinlock.h",
  "human_readable_source": " \n#ifndef __LINUX_SPINLOCK_H\n#define __LINUX_SPINLOCK_H\n#define __LINUX_INSIDE_SPINLOCK_H\n\n \n\n#include <linux/typecheck.h>\n#include <linux/preempt.h>\n#include <linux/linkage.h>\n#include <linux/compiler.h>\n#include <linux/irqflags.h>\n#include <linux/thread_info.h>\n#include <linux/stringify.h>\n#include <linux/bottom_half.h>\n#include <linux/lockdep.h>\n#include <linux/cleanup.h>\n#include <asm/barrier.h>\n#include <asm/mmiowb.h>\n\n\n \n#define LOCK_SECTION_NAME \".text..lock.\"KBUILD_BASENAME\n\n#define LOCK_SECTION_START(extra)               \\\n        \".subsection 1\\n\\t\"                     \\\n        extra                                   \\\n        \".ifndef \" LOCK_SECTION_NAME \"\\n\\t\"     \\\n        LOCK_SECTION_NAME \":\\n\\t\"               \\\n        \".endif\\n\"\n\n#define LOCK_SECTION_END                        \\\n        \".previous\\n\\t\"\n\n#define __lockfunc __section(\".spinlock.text\")\n\n \n#include <linux/spinlock_types.h>\n\n \n#ifdef CONFIG_SMP\n# include <asm/spinlock.h>\n#else\n# include <linux/spinlock_up.h>\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n  extern void __raw_spin_lock_init(raw_spinlock_t *lock, const char *name,\n\t\t\t\t   struct lock_class_key *key, short inner);\n\n# define raw_spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init((lock), #lock, &__key, LD_WAIT_SPIN);\t\\\n} while (0)\n\n#else\n# define raw_spin_lock_init(lock)\t\t\t\t\\\n\tdo { *(lock) = __RAW_SPIN_LOCK_UNLOCKED(lock); } while (0)\n#endif\n\n#define raw_spin_is_locked(lock)\tarch_spin_is_locked(&(lock)->raw_lock)\n\n#ifdef arch_spin_is_contended\n#define raw_spin_is_contended(lock)\tarch_spin_is_contended(&(lock)->raw_lock)\n#else\n#define raw_spin_is_contended(lock)\t(((void)(lock), 0))\n#endif  \n\n \n#ifndef smp_mb__after_spinlock\n#define smp_mb__after_spinlock()\tkcsan_mb()\n#endif\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n extern void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock);\n extern int do_raw_spin_trylock(raw_spinlock_t *lock);\n extern void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock);\n#else\nstatic inline void do_raw_spin_lock(raw_spinlock_t *lock) __acquires(lock)\n{\n\t__acquire(lock);\n\tarch_spin_lock(&lock->raw_lock);\n\tmmiowb_spin_lock();\n}\n\nstatic inline int do_raw_spin_trylock(raw_spinlock_t *lock)\n{\n\tint ret = arch_spin_trylock(&(lock)->raw_lock);\n\n\tif (ret)\n\t\tmmiowb_spin_lock();\n\n\treturn ret;\n}\n\nstatic inline void do_raw_spin_unlock(raw_spinlock_t *lock) __releases(lock)\n{\n\tmmiowb_spin_unlock();\n\tarch_spin_unlock(&lock->raw_lock);\n\t__release(lock);\n}\n#endif\n\n \n#define raw_spin_trylock(lock)\t__cond_lock(lock, _raw_spin_trylock(lock))\n\n#define raw_spin_lock(lock)\t_raw_spin_lock(lock)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n# define raw_spin_lock_nested(lock, subclass) \\\n\t_raw_spin_lock_nested(lock, subclass)\n\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t\t\t\\\n\t do {\t\t\t\t\t\t\t\t\\\n\t\t typecheck(struct lockdep_map *, &(nest_lock)->dep_map);\\\n\t\t _raw_spin_lock_nest_lock(lock, &(nest_lock)->dep_map);\t\\\n\t } while (0)\n#else\n \n# define raw_spin_lock_nested(lock, subclass)\t\t\\\n\t_raw_spin_lock(((void)(subclass), (lock)))\n# define raw_spin_lock_nest_lock(lock, nest_lock)\t_raw_spin_lock(lock)\n#endif\n\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\\\n\t} while (0)\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave_nested(lock, subclass);\t\\\n\t} while (0)\n#else\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\t\\\n\t\tflags = _raw_spin_lock_irqsave(lock);\t\t\t\\\n\t} while (0)\n#endif\n\n#else\n\n#define raw_spin_lock_irqsave(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\\\n\t\t_raw_spin_lock_irqsave(lock, flags);\t\\\n\t} while (0)\n\n#define raw_spin_lock_irqsave_nested(lock, flags, subclass)\t\\\n\traw_spin_lock_irqsave(lock, flags)\n\n#endif\n\n#define raw_spin_lock_irq(lock)\t\t_raw_spin_lock_irq(lock)\n#define raw_spin_lock_bh(lock)\t\t_raw_spin_lock_bh(lock)\n#define raw_spin_unlock(lock)\t\t_raw_spin_unlock(lock)\n#define raw_spin_unlock_irq(lock)\t_raw_spin_unlock_irq(lock)\n\n#define raw_spin_unlock_irqrestore(lock, flags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\ttypecheck(unsigned long, flags);\t\t\\\n\t\t_raw_spin_unlock_irqrestore(lock, flags);\t\\\n\t} while (0)\n#define raw_spin_unlock_bh(lock)\t_raw_spin_unlock_bh(lock)\n\n#define raw_spin_trylock_bh(lock) \\\n\t__cond_lock(lock, _raw_spin_trylock_bh(lock))\n\n#define raw_spin_trylock_irq(lock) \\\n({ \\\n\tlocal_irq_disable(); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_enable(); 0;  }); \\\n})\n\n#define raw_spin_trylock_irqsave(lock, flags) \\\n({ \\\n\tlocal_irq_save(flags); \\\n\traw_spin_trylock(lock) ? \\\n\t1 : ({ local_irq_restore(flags); 0; }); \\\n})\n\n#ifndef CONFIG_PREEMPT_RT\n \n#include <linux/rwlock.h>\n#endif\n\n \n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n\n \n#ifndef CONFIG_PREEMPT_RT\n\n \n\nstatic __always_inline raw_spinlock_t *spinlock_check(spinlock_t *lock)\n{\n\treturn &lock->rlock;\n}\n\n#ifdef CONFIG_DEBUG_SPINLOCK\n\n# define spin_lock_init(lock)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t__raw_spin_lock_init(spinlock_check(lock),\t\t\\\n\t\t\t     #lock, &__key, LD_WAIT_CONFIG);\t\\\n} while (0)\n\n#else\n\n# define spin_lock_init(_lock)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tspinlock_check(_lock);\t\t\t\\\n\t*(_lock) = __SPIN_LOCK_UNLOCKED(_lock);\t\\\n} while (0)\n\n#endif\n\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n\traw_spin_lock(&lock->rlock);\n}\n\nstatic __always_inline void spin_lock_bh(spinlock_t *lock)\n{\n\traw_spin_lock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock(spinlock_t *lock)\n{\n\treturn raw_spin_trylock(&lock->rlock);\n}\n\n#define spin_lock_nested(lock, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nested(spinlock_check(lock), subclass);\t\\\n} while (0)\n\n#define spin_lock_nest_lock(lock, nest_lock)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_nest_lock(spinlock_check(lock), nest_lock);\t\\\n} while (0)\n\nstatic __always_inline void spin_lock_irq(spinlock_t *lock)\n{\n\traw_spin_lock_irq(&lock->rlock);\n}\n\n#define spin_lock_irqsave(lock, flags)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave(spinlock_check(lock), flags);\t\\\n} while (0)\n\n#define spin_lock_irqsave_nested(lock, flags, subclass)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\traw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \\\n} while (0)\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n\traw_spin_unlock(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_bh(spinlock_t *lock)\n{\n\traw_spin_unlock_bh(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irq(spinlock_t *lock)\n{\n\traw_spin_unlock_irq(&lock->rlock);\n}\n\nstatic __always_inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)\n{\n\traw_spin_unlock_irqrestore(&lock->rlock, flags);\n}\n\nstatic __always_inline int spin_trylock_bh(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_bh(&lock->rlock);\n}\n\nstatic __always_inline int spin_trylock_irq(spinlock_t *lock)\n{\n\treturn raw_spin_trylock_irq(&lock->rlock);\n}\n\n#define spin_trylock_irqsave(lock, flags)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\traw_spin_trylock_irqsave(spinlock_check(lock), flags); \\\n})\n\n \nstatic __always_inline int spin_is_locked(spinlock_t *lock)\n{\n\treturn raw_spin_is_locked(&lock->rlock);\n}\n\nstatic __always_inline int spin_is_contended(spinlock_t *lock)\n{\n\treturn raw_spin_is_contended(&lock->rlock);\n}\n\n#define assert_spin_locked(lock)\tassert_raw_spin_locked(&(lock)->rlock)\n\n#else   \n# include <linux/spinlock_rt.h>\n#endif  \n\n \n#include <linux/atomic.h>\n \nextern int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock);\n#define atomic_dec_and_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock(atomic, lock))\n\nextern int _atomic_dec_and_lock_irqsave(atomic_t *atomic, spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_lock_irqsave(atomic, lock, &(flags)))\n\nextern int _atomic_dec_and_raw_lock(atomic_t *atomic, raw_spinlock_t *lock);\n#define atomic_dec_and_raw_lock(atomic, lock) \\\n\t\t__cond_lock(lock, _atomic_dec_and_raw_lock(atomic, lock))\n\nextern int _atomic_dec_and_raw_lock_irqsave(atomic_t *atomic, raw_spinlock_t *lock,\n\t\t\t\t\tunsigned long *flags);\n#define atomic_dec_and_raw_lock_irqsave(atomic, lock, flags) \\\n\t\t__cond_lock(lock, _atomic_dec_and_raw_lock_irqsave(atomic, lock, &(flags)))\n\nint __alloc_bucket_spinlocks(spinlock_t **locks, unsigned int *lock_mask,\n\t\t\t     size_t max_size, unsigned int cpu_mult,\n\t\t\t     gfp_t gfp, const char *name,\n\t\t\t     struct lock_class_key *key);\n\n#define alloc_bucket_spinlocks(locks, lock_mask, max_size, cpu_mult, gfp)    \\\n\t({\t\t\t\t\t\t\t\t     \\\n\t\tstatic struct lock_class_key key;\t\t\t     \\\n\t\tint ret;\t\t\t\t\t\t     \\\n\t\t\t\t\t\t\t\t\t     \\\n\t\tret = __alloc_bucket_spinlocks(locks, lock_mask, max_size,   \\\n\t\t\t\t\t       cpu_mult, gfp, #locks, &key); \\\n\t\tret;\t\t\t\t\t\t\t     \\\n\t})\n\nvoid free_bucket_spinlocks(spinlock_t *locks);\n\nDEFINE_LOCK_GUARD_1(raw_spinlock, raw_spinlock_t,\n\t\t    raw_spin_lock(_T->lock),\n\t\t    raw_spin_unlock(_T->lock))\n\nDEFINE_LOCK_GUARD_1(raw_spinlock_nested, raw_spinlock_t,\n\t\t    raw_spin_lock_nested(_T->lock, SINGLE_DEPTH_NESTING),\n\t\t    raw_spin_unlock(_T->lock))\n\nDEFINE_LOCK_GUARD_1(raw_spinlock_irq, raw_spinlock_t,\n\t\t    raw_spin_lock_irq(_T->lock),\n\t\t    raw_spin_unlock_irq(_T->lock))\n\nDEFINE_LOCK_GUARD_1(raw_spinlock_irqsave, raw_spinlock_t,\n\t\t    raw_spin_lock_irqsave(_T->lock, _T->flags),\n\t\t    raw_spin_unlock_irqrestore(_T->lock, _T->flags),\n\t\t    unsigned long flags)\n\nDEFINE_LOCK_GUARD_1(spinlock, spinlock_t,\n\t\t    spin_lock(_T->lock),\n\t\t    spin_unlock(_T->lock))\n\nDEFINE_LOCK_GUARD_1(spinlock_irq, spinlock_t,\n\t\t    spin_lock_irq(_T->lock),\n\t\t    spin_unlock_irq(_T->lock))\n\nDEFINE_LOCK_GUARD_1(spinlock_irqsave, spinlock_t,\n\t\t    spin_lock_irqsave(_T->lock, _T->flags),\n\t\t    spin_unlock_irqrestore(_T->lock, _T->flags),\n\t\t    unsigned long flags)\n\n#undef __LINUX_INSIDE_SPINLOCK_H\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}