{
  "module_name": "cpuset.h",
  "hash_id": "1224c033f409cc9939408d30879c30b171f2647721526e5f5b152b543e359da5",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/cpuset.h",
  "human_readable_source": " \n#ifndef _LINUX_CPUSET_H\n#define _LINUX_CPUSET_H\n \n\n#include <linux/sched.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/task.h>\n#include <linux/cpumask.h>\n#include <linux/nodemask.h>\n#include <linux/mm.h>\n#include <linux/mmu_context.h>\n#include <linux/jump_label.h>\n\n#ifdef CONFIG_CPUSETS\n\n \nextern struct static_key_false cpusets_pre_enable_key;\nextern struct static_key_false cpusets_enabled_key;\nextern struct static_key_false cpusets_insane_config_key;\n\nstatic inline bool cpusets_enabled(void)\n{\n\treturn static_branch_unlikely(&cpusets_enabled_key);\n}\n\nstatic inline void cpuset_inc(void)\n{\n\tstatic_branch_inc_cpuslocked(&cpusets_pre_enable_key);\n\tstatic_branch_inc_cpuslocked(&cpusets_enabled_key);\n}\n\nstatic inline void cpuset_dec(void)\n{\n\tstatic_branch_dec_cpuslocked(&cpusets_enabled_key);\n\tstatic_branch_dec_cpuslocked(&cpusets_pre_enable_key);\n}\n\n \nstatic inline bool cpusets_insane_config(void)\n{\n\treturn static_branch_unlikely(&cpusets_insane_config_key);\n}\n\nextern int cpuset_init(void);\nextern void cpuset_init_smp(void);\nextern void cpuset_force_rebuild(void);\nextern void cpuset_update_active_cpus(void);\nextern void cpuset_wait_for_hotplug(void);\nextern void inc_dl_tasks_cs(struct task_struct *task);\nextern void dec_dl_tasks_cs(struct task_struct *task);\nextern void cpuset_lock(void);\nextern void cpuset_unlock(void);\nextern void cpuset_cpus_allowed(struct task_struct *p, struct cpumask *mask);\nextern bool cpuset_cpus_allowed_fallback(struct task_struct *p);\nextern nodemask_t cpuset_mems_allowed(struct task_struct *p);\n#define cpuset_current_mems_allowed (current->mems_allowed)\nvoid cpuset_init_current_mems_allowed(void);\nint cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask);\n\nextern bool cpuset_node_allowed(int node, gfp_t gfp_mask);\n\nstatic inline bool __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n{\n\treturn cpuset_node_allowed(zone_to_nid(z), gfp_mask);\n}\n\nstatic inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n{\n\tif (cpusets_enabled())\n\t\treturn __cpuset_zone_allowed(z, gfp_mask);\n\treturn true;\n}\n\nextern int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n\t\t\t\t\t  const struct task_struct *tsk2);\n\n#define cpuset_memory_pressure_bump() \t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (cpuset_memory_pressure_enabled)\t\t\\\n\t\t\t__cpuset_memory_pressure_bump();\t\\\n\t} while (0)\nextern int cpuset_memory_pressure_enabled;\nextern void __cpuset_memory_pressure_bump(void);\n\nextern void cpuset_task_status_allowed(struct seq_file *m,\n\t\t\t\t\tstruct task_struct *task);\nextern int proc_cpuset_show(struct seq_file *m, struct pid_namespace *ns,\n\t\t\t    struct pid *pid, struct task_struct *tsk);\n\nextern int cpuset_mem_spread_node(void);\nextern int cpuset_slab_spread_node(void);\n\nstatic inline int cpuset_do_page_mem_spread(void)\n{\n\treturn task_spread_page(current);\n}\n\nstatic inline int cpuset_do_slab_mem_spread(void)\n{\n\treturn task_spread_slab(current);\n}\n\nextern bool current_cpuset_is_being_rebound(void);\n\nextern void rebuild_sched_domains(void);\n\nextern void cpuset_print_current_mems_allowed(void);\n\n \nstatic inline unsigned int read_mems_allowed_begin(void)\n{\n\tif (!static_branch_unlikely(&cpusets_pre_enable_key))\n\t\treturn 0;\n\n\treturn read_seqcount_begin(&current->mems_allowed_seq);\n}\n\n \nstatic inline bool read_mems_allowed_retry(unsigned int seq)\n{\n\tif (!static_branch_unlikely(&cpusets_enabled_key))\n\t\treturn false;\n\n\treturn read_seqcount_retry(&current->mems_allowed_seq, seq);\n}\n\nstatic inline void set_mems_allowed(nodemask_t nodemask)\n{\n\tunsigned long flags;\n\n\ttask_lock(current);\n\tlocal_irq_save(flags);\n\twrite_seqcount_begin(&current->mems_allowed_seq);\n\tcurrent->mems_allowed = nodemask;\n\twrite_seqcount_end(&current->mems_allowed_seq);\n\tlocal_irq_restore(flags);\n\ttask_unlock(current);\n}\n\n#else  \n\nstatic inline bool cpusets_enabled(void) { return false; }\n\nstatic inline bool cpusets_insane_config(void) { return false; }\n\nstatic inline int cpuset_init(void) { return 0; }\nstatic inline void cpuset_init_smp(void) {}\n\nstatic inline void cpuset_force_rebuild(void) { }\n\nstatic inline void cpuset_update_active_cpus(void)\n{\n\tpartition_sched_domains(1, NULL, NULL);\n}\n\nstatic inline void cpuset_wait_for_hotplug(void) { }\n\nstatic inline void inc_dl_tasks_cs(struct task_struct *task) { }\nstatic inline void dec_dl_tasks_cs(struct task_struct *task) { }\nstatic inline void cpuset_lock(void) { }\nstatic inline void cpuset_unlock(void) { }\n\nstatic inline void cpuset_cpus_allowed(struct task_struct *p,\n\t\t\t\t       struct cpumask *mask)\n{\n\tcpumask_copy(mask, task_cpu_possible_mask(p));\n}\n\nstatic inline bool cpuset_cpus_allowed_fallback(struct task_struct *p)\n{\n\treturn false;\n}\n\nstatic inline nodemask_t cpuset_mems_allowed(struct task_struct *p)\n{\n\treturn node_possible_map;\n}\n\n#define cpuset_current_mems_allowed (node_states[N_MEMORY])\nstatic inline void cpuset_init_current_mems_allowed(void) {}\n\nstatic inline int cpuset_nodemask_valid_mems_allowed(nodemask_t *nodemask)\n{\n\treturn 1;\n}\n\nstatic inline bool __cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n{\n\treturn true;\n}\n\nstatic inline bool cpuset_zone_allowed(struct zone *z, gfp_t gfp_mask)\n{\n\treturn true;\n}\n\nstatic inline int cpuset_mems_allowed_intersects(const struct task_struct *tsk1,\n\t\t\t\t\t\t const struct task_struct *tsk2)\n{\n\treturn 1;\n}\n\nstatic inline void cpuset_memory_pressure_bump(void) {}\n\nstatic inline void cpuset_task_status_allowed(struct seq_file *m,\n\t\t\t\t\t\tstruct task_struct *task)\n{\n}\n\nstatic inline int cpuset_mem_spread_node(void)\n{\n\treturn 0;\n}\n\nstatic inline int cpuset_slab_spread_node(void)\n{\n\treturn 0;\n}\n\nstatic inline int cpuset_do_page_mem_spread(void)\n{\n\treturn 0;\n}\n\nstatic inline int cpuset_do_slab_mem_spread(void)\n{\n\treturn 0;\n}\n\nstatic inline bool current_cpuset_is_being_rebound(void)\n{\n\treturn false;\n}\n\nstatic inline void rebuild_sched_domains(void)\n{\n\tpartition_sched_domains(1, NULL, NULL);\n}\n\nstatic inline void cpuset_print_current_mems_allowed(void)\n{\n}\n\nstatic inline void set_mems_allowed(nodemask_t nodemask)\n{\n}\n\nstatic inline unsigned int read_mems_allowed_begin(void)\n{\n\treturn 0;\n}\n\nstatic inline bool read_mems_allowed_retry(unsigned int seq)\n{\n\treturn false;\n}\n\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}