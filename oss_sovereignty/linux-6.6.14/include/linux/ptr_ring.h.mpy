{
  "module_name": "ptr_ring.h",
  "hash_id": "95ecdfa0361a37112730d46e9495d0bc4e8e5ad4681fb187f9518035a29c624d",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/ptr_ring.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_PTR_RING_H\n#define _LINUX_PTR_RING_H 1\n\n#ifdef __KERNEL__\n#include <linux/spinlock.h>\n#include <linux/cache.h>\n#include <linux/types.h>\n#include <linux/compiler.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <asm/errno.h>\n#endif\n\nstruct ptr_ring {\n\tint producer ____cacheline_aligned_in_smp;\n\tspinlock_t producer_lock;\n\tint consumer_head ____cacheline_aligned_in_smp;  \n\tint consumer_tail;  \n\tspinlock_t consumer_lock;\n\t \n\t \n\tint size ____cacheline_aligned_in_smp;  \n\tint batch;  \n\tvoid **queue;\n};\n\n \nstatic inline bool __ptr_ring_full(struct ptr_ring *r)\n{\n\treturn r->queue[r->producer];\n}\n\nstatic inline bool ptr_ring_full(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock(&r->producer_lock);\n\tret = __ptr_ring_full(r);\n\tspin_unlock(&r->producer_lock);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_full_irq(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock_irq(&r->producer_lock);\n\tret = __ptr_ring_full(r);\n\tspin_unlock_irq(&r->producer_lock);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_full_any(struct ptr_ring *r)\n{\n\tunsigned long flags;\n\tbool ret;\n\n\tspin_lock_irqsave(&r->producer_lock, flags);\n\tret = __ptr_ring_full(r);\n\tspin_unlock_irqrestore(&r->producer_lock, flags);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_full_bh(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock_bh(&r->producer_lock);\n\tret = __ptr_ring_full(r);\n\tspin_unlock_bh(&r->producer_lock);\n\n\treturn ret;\n}\n\n \nstatic inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)\n{\n\tif (unlikely(!r->size) || r->queue[r->producer])\n\t\treturn -ENOSPC;\n\n\t \n\t \n\tsmp_wmb();\n\n\tWRITE_ONCE(r->queue[r->producer++], ptr);\n\tif (unlikely(r->producer >= r->size))\n\t\tr->producer = 0;\n\treturn 0;\n}\n\n \nstatic inline int ptr_ring_produce(struct ptr_ring *r, void *ptr)\n{\n\tint ret;\n\n\tspin_lock(&r->producer_lock);\n\tret = __ptr_ring_produce(r, ptr);\n\tspin_unlock(&r->producer_lock);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_produce_irq(struct ptr_ring *r, void *ptr)\n{\n\tint ret;\n\n\tspin_lock_irq(&r->producer_lock);\n\tret = __ptr_ring_produce(r, ptr);\n\tspin_unlock_irq(&r->producer_lock);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_produce_any(struct ptr_ring *r, void *ptr)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&r->producer_lock, flags);\n\tret = __ptr_ring_produce(r, ptr);\n\tspin_unlock_irqrestore(&r->producer_lock, flags);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_produce_bh(struct ptr_ring *r, void *ptr)\n{\n\tint ret;\n\n\tspin_lock_bh(&r->producer_lock);\n\tret = __ptr_ring_produce(r, ptr);\n\tspin_unlock_bh(&r->producer_lock);\n\n\treturn ret;\n}\n\nstatic inline void *__ptr_ring_peek(struct ptr_ring *r)\n{\n\tif (likely(r->size))\n\t\treturn READ_ONCE(r->queue[r->consumer_head]);\n\treturn NULL;\n}\n\n \nstatic inline bool __ptr_ring_empty(struct ptr_ring *r)\n{\n\tif (likely(r->size))\n\t\treturn !r->queue[READ_ONCE(r->consumer_head)];\n\treturn true;\n}\n\nstatic inline bool ptr_ring_empty(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock(&r->consumer_lock);\n\tret = __ptr_ring_empty(r);\n\tspin_unlock(&r->consumer_lock);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_empty_irq(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock_irq(&r->consumer_lock);\n\tret = __ptr_ring_empty(r);\n\tspin_unlock_irq(&r->consumer_lock);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_empty_any(struct ptr_ring *r)\n{\n\tunsigned long flags;\n\tbool ret;\n\n\tspin_lock_irqsave(&r->consumer_lock, flags);\n\tret = __ptr_ring_empty(r);\n\tspin_unlock_irqrestore(&r->consumer_lock, flags);\n\n\treturn ret;\n}\n\nstatic inline bool ptr_ring_empty_bh(struct ptr_ring *r)\n{\n\tbool ret;\n\n\tspin_lock_bh(&r->consumer_lock);\n\tret = __ptr_ring_empty(r);\n\tspin_unlock_bh(&r->consumer_lock);\n\n\treturn ret;\n}\n\n \nstatic inline void __ptr_ring_discard_one(struct ptr_ring *r)\n{\n\t \n\t \n\tint consumer_head = r->consumer_head;\n\tint head = consumer_head++;\n\n\t \n\tif (unlikely(consumer_head - r->consumer_tail >= r->batch ||\n\t\t     consumer_head >= r->size)) {\n\t\t \n\t\twhile (likely(head >= r->consumer_tail))\n\t\t\tr->queue[head--] = NULL;\n\t\tr->consumer_tail = consumer_head;\n\t}\n\tif (unlikely(consumer_head >= r->size)) {\n\t\tconsumer_head = 0;\n\t\tr->consumer_tail = 0;\n\t}\n\t \n\tWRITE_ONCE(r->consumer_head, consumer_head);\n}\n\nstatic inline void *__ptr_ring_consume(struct ptr_ring *r)\n{\n\tvoid *ptr;\n\n\t \n\tptr = __ptr_ring_peek(r);\n\tif (ptr)\n\t\t__ptr_ring_discard_one(r);\n\n\treturn ptr;\n}\n\nstatic inline int __ptr_ring_consume_batched(struct ptr_ring *r,\n\t\t\t\t\t     void **array, int n)\n{\n\tvoid *ptr;\n\tint i;\n\n\tfor (i = 0; i < n; i++) {\n\t\tptr = __ptr_ring_consume(r);\n\t\tif (!ptr)\n\t\t\tbreak;\n\t\tarray[i] = ptr;\n\t}\n\n\treturn i;\n}\n\n \nstatic inline void *ptr_ring_consume(struct ptr_ring *r)\n{\n\tvoid *ptr;\n\n\tspin_lock(&r->consumer_lock);\n\tptr = __ptr_ring_consume(r);\n\tspin_unlock(&r->consumer_lock);\n\n\treturn ptr;\n}\n\nstatic inline void *ptr_ring_consume_irq(struct ptr_ring *r)\n{\n\tvoid *ptr;\n\n\tspin_lock_irq(&r->consumer_lock);\n\tptr = __ptr_ring_consume(r);\n\tspin_unlock_irq(&r->consumer_lock);\n\n\treturn ptr;\n}\n\nstatic inline void *ptr_ring_consume_any(struct ptr_ring *r)\n{\n\tunsigned long flags;\n\tvoid *ptr;\n\n\tspin_lock_irqsave(&r->consumer_lock, flags);\n\tptr = __ptr_ring_consume(r);\n\tspin_unlock_irqrestore(&r->consumer_lock, flags);\n\n\treturn ptr;\n}\n\nstatic inline void *ptr_ring_consume_bh(struct ptr_ring *r)\n{\n\tvoid *ptr;\n\n\tspin_lock_bh(&r->consumer_lock);\n\tptr = __ptr_ring_consume(r);\n\tspin_unlock_bh(&r->consumer_lock);\n\n\treturn ptr;\n}\n\nstatic inline int ptr_ring_consume_batched(struct ptr_ring *r,\n\t\t\t\t\t   void **array, int n)\n{\n\tint ret;\n\n\tspin_lock(&r->consumer_lock);\n\tret = __ptr_ring_consume_batched(r, array, n);\n\tspin_unlock(&r->consumer_lock);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_consume_batched_irq(struct ptr_ring *r,\n\t\t\t\t\t       void **array, int n)\n{\n\tint ret;\n\n\tspin_lock_irq(&r->consumer_lock);\n\tret = __ptr_ring_consume_batched(r, array, n);\n\tspin_unlock_irq(&r->consumer_lock);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_consume_batched_any(struct ptr_ring *r,\n\t\t\t\t\t       void **array, int n)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&r->consumer_lock, flags);\n\tret = __ptr_ring_consume_batched(r, array, n);\n\tspin_unlock_irqrestore(&r->consumer_lock, flags);\n\n\treturn ret;\n}\n\nstatic inline int ptr_ring_consume_batched_bh(struct ptr_ring *r,\n\t\t\t\t\t      void **array, int n)\n{\n\tint ret;\n\n\tspin_lock_bh(&r->consumer_lock);\n\tret = __ptr_ring_consume_batched(r, array, n);\n\tspin_unlock_bh(&r->consumer_lock);\n\n\treturn ret;\n}\n\n \n#define __PTR_RING_PEEK_CALL(r, f) ((f)(__ptr_ring_peek(r)))\n\n#define PTR_RING_PEEK_CALL(r, f) ({ \\\n\ttypeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \\\n\t\\\n\tspin_lock(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \\\n\tspin_unlock(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v; \\\n})\n\n#define PTR_RING_PEEK_CALL_IRQ(r, f) ({ \\\n\ttypeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \\\n\t\\\n\tspin_lock_irq(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \\\n\tspin_unlock_irq(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v; \\\n})\n\n#define PTR_RING_PEEK_CALL_BH(r, f) ({ \\\n\ttypeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \\\n\t\\\n\tspin_lock_bh(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \\\n\tspin_unlock_bh(&(r)->consumer_lock); \\\n\t__PTR_RING_PEEK_CALL_v; \\\n})\n\n#define PTR_RING_PEEK_CALL_ANY(r, f) ({ \\\n\ttypeof((f)(NULL)) __PTR_RING_PEEK_CALL_v; \\\n\tunsigned long __PTR_RING_PEEK_CALL_f;\\\n\t\\\n\tspin_lock_irqsave(&(r)->consumer_lock, __PTR_RING_PEEK_CALL_f); \\\n\t__PTR_RING_PEEK_CALL_v = __PTR_RING_PEEK_CALL(r, f); \\\n\tspin_unlock_irqrestore(&(r)->consumer_lock, __PTR_RING_PEEK_CALL_f); \\\n\t__PTR_RING_PEEK_CALL_v; \\\n})\n\n \nstatic inline void **__ptr_ring_init_queue_alloc(unsigned int size, gfp_t gfp)\n{\n\tif (size > KMALLOC_MAX_SIZE / sizeof(void *))\n\t\treturn NULL;\n\treturn kvmalloc_array(size, sizeof(void *), gfp | __GFP_ZERO);\n}\n\nstatic inline void __ptr_ring_set_size(struct ptr_ring *r, int size)\n{\n\tr->size = size;\n\tr->batch = SMP_CACHE_BYTES * 2 / sizeof(*(r->queue));\n\t \n\tif (r->batch > r->size / 2 || !r->batch)\n\t\tr->batch = 1;\n}\n\nstatic inline int ptr_ring_init(struct ptr_ring *r, int size, gfp_t gfp)\n{\n\tr->queue = __ptr_ring_init_queue_alloc(size, gfp);\n\tif (!r->queue)\n\t\treturn -ENOMEM;\n\n\t__ptr_ring_set_size(r, size);\n\tr->producer = r->consumer_head = r->consumer_tail = 0;\n\tspin_lock_init(&r->producer_lock);\n\tspin_lock_init(&r->consumer_lock);\n\n\treturn 0;\n}\n\n \nstatic inline void ptr_ring_unconsume(struct ptr_ring *r, void **batch, int n,\n\t\t\t\t      void (*destroy)(void *))\n{\n\tunsigned long flags;\n\tint head;\n\n\tspin_lock_irqsave(&r->consumer_lock, flags);\n\tspin_lock(&r->producer_lock);\n\n\tif (!r->size)\n\t\tgoto done;\n\n\t \n\thead = r->consumer_head - 1;\n\twhile (likely(head >= r->consumer_tail))\n\t\tr->queue[head--] = NULL;\n\tr->consumer_tail = r->consumer_head;\n\n\t \n\twhile (n) {\n\t\thead = r->consumer_head - 1;\n\t\tif (head < 0)\n\t\t\thead = r->size - 1;\n\t\tif (r->queue[head]) {\n\t\t\t \n\t\t\tgoto done;\n\t\t}\n\t\tr->queue[head] = batch[--n];\n\t\tr->consumer_tail = head;\n\t\t \n\t\tWRITE_ONCE(r->consumer_head, head);\n\t}\n\ndone:\n\t \n\twhile (n)\n\t\tdestroy(batch[--n]);\n\tspin_unlock(&r->producer_lock);\n\tspin_unlock_irqrestore(&r->consumer_lock, flags);\n}\n\nstatic inline void **__ptr_ring_swap_queue(struct ptr_ring *r, void **queue,\n\t\t\t\t\t   int size, gfp_t gfp,\n\t\t\t\t\t   void (*destroy)(void *))\n{\n\tint producer = 0;\n\tvoid **old;\n\tvoid *ptr;\n\n\twhile ((ptr = __ptr_ring_consume(r)))\n\t\tif (producer < size)\n\t\t\tqueue[producer++] = ptr;\n\t\telse if (destroy)\n\t\t\tdestroy(ptr);\n\n\tif (producer >= size)\n\t\tproducer = 0;\n\t__ptr_ring_set_size(r, size);\n\tr->producer = producer;\n\tr->consumer_head = 0;\n\tr->consumer_tail = 0;\n\told = r->queue;\n\tr->queue = queue;\n\n\treturn old;\n}\n\n \nstatic inline int ptr_ring_resize(struct ptr_ring *r, int size, gfp_t gfp,\n\t\t\t\t  void (*destroy)(void *))\n{\n\tunsigned long flags;\n\tvoid **queue = __ptr_ring_init_queue_alloc(size, gfp);\n\tvoid **old;\n\n\tif (!queue)\n\t\treturn -ENOMEM;\n\n\tspin_lock_irqsave(&(r)->consumer_lock, flags);\n\tspin_lock(&(r)->producer_lock);\n\n\told = __ptr_ring_swap_queue(r, queue, size, gfp, destroy);\n\n\tspin_unlock(&(r)->producer_lock);\n\tspin_unlock_irqrestore(&(r)->consumer_lock, flags);\n\n\tkvfree(old);\n\n\treturn 0;\n}\n\n \nstatic inline int ptr_ring_resize_multiple(struct ptr_ring **rings,\n\t\t\t\t\t   unsigned int nrings,\n\t\t\t\t\t   int size,\n\t\t\t\t\t   gfp_t gfp, void (*destroy)(void *))\n{\n\tunsigned long flags;\n\tvoid ***queues;\n\tint i;\n\n\tqueues = kmalloc_array(nrings, sizeof(*queues), gfp);\n\tif (!queues)\n\t\tgoto noqueues;\n\n\tfor (i = 0; i < nrings; ++i) {\n\t\tqueues[i] = __ptr_ring_init_queue_alloc(size, gfp);\n\t\tif (!queues[i])\n\t\t\tgoto nomem;\n\t}\n\n\tfor (i = 0; i < nrings; ++i) {\n\t\tspin_lock_irqsave(&(rings[i])->consumer_lock, flags);\n\t\tspin_lock(&(rings[i])->producer_lock);\n\t\tqueues[i] = __ptr_ring_swap_queue(rings[i], queues[i],\n\t\t\t\t\t\t  size, gfp, destroy);\n\t\tspin_unlock(&(rings[i])->producer_lock);\n\t\tspin_unlock_irqrestore(&(rings[i])->consumer_lock, flags);\n\t}\n\n\tfor (i = 0; i < nrings; ++i)\n\t\tkvfree(queues[i]);\n\n\tkfree(queues);\n\n\treturn 0;\n\nnomem:\n\twhile (--i >= 0)\n\t\tkvfree(queues[i]);\n\n\tkfree(queues);\n\nnoqueues:\n\treturn -ENOMEM;\n}\n\nstatic inline void ptr_ring_cleanup(struct ptr_ring *r, void (*destroy)(void *))\n{\n\tvoid *ptr;\n\n\tif (destroy)\n\t\twhile ((ptr = ptr_ring_consume(r)))\n\t\t\tdestroy(ptr);\n\tkvfree(r->queue);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}