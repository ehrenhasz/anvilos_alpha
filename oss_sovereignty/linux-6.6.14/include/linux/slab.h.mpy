{
  "module_name": "slab.h",
  "hash_id": "343f6bf632fb193d0b475c2664d9090365095151ac03fb7870ece5c89df132d8",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/slab.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_SLAB_H\n#define\t_LINUX_SLAB_H\n\n#include <linux/cache.h>\n#include <linux/gfp.h>\n#include <linux/overflow.h>\n#include <linux/types.h>\n#include <linux/workqueue.h>\n#include <linux/percpu-refcount.h>\n#include <linux/cleanup.h>\n#include <linux/hash.h>\n\n\n \n \n#define SLAB_CONSISTENCY_CHECKS\t((slab_flags_t __force)0x00000100U)\n \n#define SLAB_RED_ZONE\t\t((slab_flags_t __force)0x00000400U)\n \n#define SLAB_POISON\t\t((slab_flags_t __force)0x00000800U)\n \n#define SLAB_KMALLOC\t\t((slab_flags_t __force)0x00001000U)\n \n#define SLAB_HWCACHE_ALIGN\t((slab_flags_t __force)0x00002000U)\n \n#define SLAB_CACHE_DMA\t\t((slab_flags_t __force)0x00004000U)\n \n#define SLAB_CACHE_DMA32\t((slab_flags_t __force)0x00008000U)\n \n#define SLAB_STORE_USER\t\t((slab_flags_t __force)0x00010000U)\n \n#define SLAB_PANIC\t\t((slab_flags_t __force)0x00040000U)\n \n \n#define SLAB_TYPESAFE_BY_RCU\t((slab_flags_t __force)0x00080000U)\n \n#define SLAB_MEM_SPREAD\t\t((slab_flags_t __force)0x00100000U)\n \n#define SLAB_TRACE\t\t((slab_flags_t __force)0x00200000U)\n\n \n#ifdef CONFIG_DEBUG_OBJECTS\n# define SLAB_DEBUG_OBJECTS\t((slab_flags_t __force)0x00400000U)\n#else\n# define SLAB_DEBUG_OBJECTS\t0\n#endif\n\n \n#define SLAB_NOLEAKTRACE\t((slab_flags_t __force)0x00800000U)\n\n \n#define SLAB_NO_MERGE\t\t((slab_flags_t __force)0x01000000U)\n\n \n#ifdef CONFIG_FAILSLAB\n# define SLAB_FAILSLAB\t\t((slab_flags_t __force)0x02000000U)\n#else\n# define SLAB_FAILSLAB\t\t0\n#endif\n \n#ifdef CONFIG_MEMCG_KMEM\n# define SLAB_ACCOUNT\t\t((slab_flags_t __force)0x04000000U)\n#else\n# define SLAB_ACCOUNT\t\t0\n#endif\n\n#ifdef CONFIG_KASAN_GENERIC\n#define SLAB_KASAN\t\t((slab_flags_t __force)0x08000000U)\n#else\n#define SLAB_KASAN\t\t0\n#endif\n\n \n#define SLAB_NO_USER_FLAGS\t((slab_flags_t __force)0x10000000U)\n\n#ifdef CONFIG_KFENCE\n#define SLAB_SKIP_KFENCE\t((slab_flags_t __force)0x20000000U)\n#else\n#define SLAB_SKIP_KFENCE\t0\n#endif\n\n \n \n#ifndef CONFIG_SLUB_TINY\n#define SLAB_RECLAIM_ACCOUNT\t((slab_flags_t __force)0x00020000U)\n#else\n#define SLAB_RECLAIM_ACCOUNT\t((slab_flags_t __force)0)\n#endif\n#define SLAB_TEMPORARY\t\tSLAB_RECLAIM_ACCOUNT\t \n\n \n#define ZERO_SIZE_PTR ((void *)16)\n\n#define ZERO_OR_NULL_PTR(x) ((unsigned long)(x) <= \\\n\t\t\t\t(unsigned long)ZERO_SIZE_PTR)\n\n#include <linux/kasan.h>\n\nstruct list_lru;\nstruct mem_cgroup;\n \nbool slab_is_available(void);\n\nstruct kmem_cache *kmem_cache_create(const char *name, unsigned int size,\n\t\t\tunsigned int align, slab_flags_t flags,\n\t\t\tvoid (*ctor)(void *));\nstruct kmem_cache *kmem_cache_create_usercopy(const char *name,\n\t\t\tunsigned int size, unsigned int align,\n\t\t\tslab_flags_t flags,\n\t\t\tunsigned int useroffset, unsigned int usersize,\n\t\t\tvoid (*ctor)(void *));\nvoid kmem_cache_destroy(struct kmem_cache *s);\nint kmem_cache_shrink(struct kmem_cache *s);\n\n \n#define KMEM_CACHE(__struct, __flags)\t\t\t\t\t\\\n\t\tkmem_cache_create(#__struct, sizeof(struct __struct),\t\\\n\t\t\t__alignof__(struct __struct), (__flags), NULL)\n\n \n#define KMEM_CACHE_USERCOPY(__struct, __flags, __field)\t\t\t\\\n\t\tkmem_cache_create_usercopy(#__struct,\t\t\t\\\n\t\t\tsizeof(struct __struct),\t\t\t\\\n\t\t\t__alignof__(struct __struct), (__flags),\t\\\n\t\t\toffsetof(struct __struct, __field),\t\t\\\n\t\t\tsizeof_field(struct __struct, __field), NULL)\n\n \nvoid * __must_check krealloc(const void *objp, size_t new_size, gfp_t flags) __realloc_size(2);\nvoid kfree(const void *objp);\nvoid kfree_sensitive(const void *objp);\nsize_t __ksize(const void *objp);\n\nDEFINE_FREE(kfree, void *, if (_T) kfree(_T))\n\n \nsize_t ksize(const void *objp);\n\n#ifdef CONFIG_PRINTK\nbool kmem_valid_obj(void *object);\nvoid kmem_dump_obj(void *object);\n#endif\n\n \n#ifdef ARCH_HAS_DMA_MINALIGN\n#if ARCH_DMA_MINALIGN > 8 && !defined(ARCH_KMALLOC_MINALIGN)\n#define ARCH_KMALLOC_MINALIGN ARCH_DMA_MINALIGN\n#endif\n#endif\n\n#ifndef ARCH_KMALLOC_MINALIGN\n#define ARCH_KMALLOC_MINALIGN __alignof__(unsigned long long)\n#elif ARCH_KMALLOC_MINALIGN > 8\n#define KMALLOC_MIN_SIZE ARCH_KMALLOC_MINALIGN\n#define KMALLOC_SHIFT_LOW ilog2(KMALLOC_MIN_SIZE)\n#endif\n\n \n#ifndef ARCH_SLAB_MINALIGN\n#define ARCH_SLAB_MINALIGN __alignof__(unsigned long long)\n#endif\n\n \n#ifndef arch_slab_minalign\nstatic inline unsigned int arch_slab_minalign(void)\n{\n\treturn ARCH_SLAB_MINALIGN;\n}\n#endif\n\n \n#define __assume_kmalloc_alignment __assume_aligned(ARCH_KMALLOC_MINALIGN)\n#define __assume_slab_alignment __assume_aligned(ARCH_SLAB_MINALIGN)\n#define __assume_page_alignment __assume_aligned(PAGE_SIZE)\n\n \n\n#ifdef CONFIG_SLAB\n \n#define KMALLOC_SHIFT_HIGH\t(PAGE_SHIFT + 1)\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t5\n#endif\n#endif\n\n#ifdef CONFIG_SLUB\n#define KMALLOC_SHIFT_HIGH\t(PAGE_SHIFT + 1)\n#define KMALLOC_SHIFT_MAX\t(MAX_ORDER + PAGE_SHIFT)\n#ifndef KMALLOC_SHIFT_LOW\n#define KMALLOC_SHIFT_LOW\t3\n#endif\n#endif\n\n \n#define KMALLOC_MAX_SIZE\t(1UL << KMALLOC_SHIFT_MAX)\n \n#define KMALLOC_MAX_CACHE_SIZE\t(1UL << KMALLOC_SHIFT_HIGH)\n \n#define KMALLOC_MAX_ORDER\t(KMALLOC_SHIFT_MAX - PAGE_SHIFT)\n\n \n#ifndef KMALLOC_MIN_SIZE\n#define KMALLOC_MIN_SIZE (1 << KMALLOC_SHIFT_LOW)\n#endif\n\n \n#define SLAB_OBJ_MIN_SIZE      (KMALLOC_MIN_SIZE < 16 ? \\\n                               (KMALLOC_MIN_SIZE) : 16)\n\n#ifdef CONFIG_RANDOM_KMALLOC_CACHES\n#define RANDOM_KMALLOC_CACHES_NR\t15 \n#else\n#define RANDOM_KMALLOC_CACHES_NR\t0\n#endif\n\n \nenum kmalloc_cache_type {\n\tKMALLOC_NORMAL = 0,\n#ifndef CONFIG_ZONE_DMA\n\tKMALLOC_DMA = KMALLOC_NORMAL,\n#endif\n#ifndef CONFIG_MEMCG_KMEM\n\tKMALLOC_CGROUP = KMALLOC_NORMAL,\n#endif\n\tKMALLOC_RANDOM_START = KMALLOC_NORMAL,\n\tKMALLOC_RANDOM_END = KMALLOC_RANDOM_START + RANDOM_KMALLOC_CACHES_NR,\n#ifdef CONFIG_SLUB_TINY\n\tKMALLOC_RECLAIM = KMALLOC_NORMAL,\n#else\n\tKMALLOC_RECLAIM,\n#endif\n#ifdef CONFIG_ZONE_DMA\n\tKMALLOC_DMA,\n#endif\n#ifdef CONFIG_MEMCG_KMEM\n\tKMALLOC_CGROUP,\n#endif\n\tNR_KMALLOC_TYPES\n};\n\nextern struct kmem_cache *\nkmalloc_caches[NR_KMALLOC_TYPES][KMALLOC_SHIFT_HIGH + 1];\n\n \n#define KMALLOC_NOT_NORMAL_BITS\t\t\t\t\t\\\n\t(__GFP_RECLAIMABLE |\t\t\t\t\t\\\n\t(IS_ENABLED(CONFIG_ZONE_DMA)   ? __GFP_DMA : 0) |\t\\\n\t(IS_ENABLED(CONFIG_MEMCG_KMEM) ? __GFP_ACCOUNT : 0))\n\nextern unsigned long random_kmalloc_seed;\n\nstatic __always_inline enum kmalloc_cache_type kmalloc_type(gfp_t flags, unsigned long caller)\n{\n\t \n\tif (likely((flags & KMALLOC_NOT_NORMAL_BITS) == 0))\n#ifdef CONFIG_RANDOM_KMALLOC_CACHES\n\t\t \n\t\treturn KMALLOC_RANDOM_START + hash_64(caller ^ random_kmalloc_seed,\n\t\t\t\t\t\t      ilog2(RANDOM_KMALLOC_CACHES_NR + 1));\n#else\n\t\treturn KMALLOC_NORMAL;\n#endif\n\n\t \n\tif (IS_ENABLED(CONFIG_ZONE_DMA) && (flags & __GFP_DMA))\n\t\treturn KMALLOC_DMA;\n\tif (!IS_ENABLED(CONFIG_MEMCG_KMEM) || (flags & __GFP_RECLAIMABLE))\n\t\treturn KMALLOC_RECLAIM;\n\telse\n\t\treturn KMALLOC_CGROUP;\n}\n\n \nstatic __always_inline unsigned int __kmalloc_index(size_t size,\n\t\t\t\t\t\t    bool size_is_constant)\n{\n\tif (!size)\n\t\treturn 0;\n\n\tif (size <= KMALLOC_MIN_SIZE)\n\t\treturn KMALLOC_SHIFT_LOW;\n\n\tif (KMALLOC_MIN_SIZE <= 32 && size > 64 && size <= 96)\n\t\treturn 1;\n\tif (KMALLOC_MIN_SIZE <= 64 && size > 128 && size <= 192)\n\t\treturn 2;\n\tif (size <=          8) return 3;\n\tif (size <=         16) return 4;\n\tif (size <=         32) return 5;\n\tif (size <=         64) return 6;\n\tif (size <=        128) return 7;\n\tif (size <=        256) return 8;\n\tif (size <=        512) return 9;\n\tif (size <=       1024) return 10;\n\tif (size <=   2 * 1024) return 11;\n\tif (size <=   4 * 1024) return 12;\n\tif (size <=   8 * 1024) return 13;\n\tif (size <=  16 * 1024) return 14;\n\tif (size <=  32 * 1024) return 15;\n\tif (size <=  64 * 1024) return 16;\n\tif (size <= 128 * 1024) return 17;\n\tif (size <= 256 * 1024) return 18;\n\tif (size <= 512 * 1024) return 19;\n\tif (size <= 1024 * 1024) return 20;\n\tif (size <=  2 * 1024 * 1024) return 21;\n\n\tif (!IS_ENABLED(CONFIG_PROFILE_ALL_BRANCHES) && size_is_constant)\n\t\tBUILD_BUG_ON_MSG(1, \"unexpected size in kmalloc_index()\");\n\telse\n\t\tBUG();\n\n\t \n\treturn -1;\n}\nstatic_assert(PAGE_SHIFT <= 20);\n#define kmalloc_index(s) __kmalloc_index(s, true)\n\nvoid *__kmalloc(size_t size, gfp_t flags) __assume_kmalloc_alignment __alloc_size(1);\n\n \nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags) __assume_slab_alignment __malloc;\nvoid *kmem_cache_alloc_lru(struct kmem_cache *s, struct list_lru *lru,\n\t\t\t   gfp_t gfpflags) __assume_slab_alignment __malloc;\nvoid kmem_cache_free(struct kmem_cache *s, void *objp);\n\n \nvoid kmem_cache_free_bulk(struct kmem_cache *s, size_t size, void **p);\nint kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size, void **p);\n\nstatic __always_inline void kfree_bulk(size_t size, void **p)\n{\n\tkmem_cache_free_bulk(NULL, size, p);\n}\n\nvoid *__kmalloc_node(size_t size, gfp_t flags, int node) __assume_kmalloc_alignment\n\t\t\t\t\t\t\t __alloc_size(1);\nvoid *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t flags, int node) __assume_slab_alignment\n\t\t\t\t\t\t\t\t\t __malloc;\n\nvoid *kmalloc_trace(struct kmem_cache *s, gfp_t flags, size_t size)\n\t\t    __assume_kmalloc_alignment __alloc_size(3);\n\nvoid *kmalloc_node_trace(struct kmem_cache *s, gfp_t gfpflags,\n\t\t\t int node, size_t size) __assume_kmalloc_alignment\n\t\t\t\t\t\t__alloc_size(4);\nvoid *kmalloc_large(size_t size, gfp_t flags) __assume_page_alignment\n\t\t\t\t\t      __alloc_size(1);\n\nvoid *kmalloc_large_node(size_t size, gfp_t flags, int node) __assume_page_alignment\n\t\t\t\t\t\t\t     __alloc_size(1);\n\n \nstatic __always_inline __alloc_size(1) void *kmalloc(size_t size, gfp_t flags)\n{\n\tif (__builtin_constant_p(size) && size) {\n\t\tunsigned int index;\n\n\t\tif (size > KMALLOC_MAX_CACHE_SIZE)\n\t\t\treturn kmalloc_large(size, flags);\n\n\t\tindex = kmalloc_index(size);\n\t\treturn kmalloc_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags, _RET_IP_)][index],\n\t\t\t\tflags, size);\n\t}\n\treturn __kmalloc(size, flags);\n}\n\nstatic __always_inline __alloc_size(1) void *kmalloc_node(size_t size, gfp_t flags, int node)\n{\n\tif (__builtin_constant_p(size) && size) {\n\t\tunsigned int index;\n\n\t\tif (size > KMALLOC_MAX_CACHE_SIZE)\n\t\t\treturn kmalloc_large_node(size, flags, node);\n\n\t\tindex = kmalloc_index(size);\n\t\treturn kmalloc_node_trace(\n\t\t\t\tkmalloc_caches[kmalloc_type(flags, _RET_IP_)][index],\n\t\t\t\tflags, node, size);\n\t}\n\treturn __kmalloc_node(size, flags, node);\n}\n\n \nstatic inline __alloc_size(1, 2) void *kmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc(bytes, flags);\n\treturn __kmalloc(bytes, flags);\n}\n\n \nstatic inline __realloc_size(2, 3) void * __must_check krealloc_array(void *p,\n\t\t\t\t\t\t\t\t      size_t new_n,\n\t\t\t\t\t\t\t\t      size_t new_size,\n\t\t\t\t\t\t\t\t      gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(new_n, new_size, &bytes)))\n\t\treturn NULL;\n\n\treturn krealloc(p, bytes, flags);\n}\n\n \nstatic inline __alloc_size(1, 2) void *kcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\nvoid *__kmalloc_node_track_caller(size_t size, gfp_t flags, int node,\n\t\t\t\t  unsigned long caller) __alloc_size(1);\n#define kmalloc_node_track_caller(size, flags, node) \\\n\t__kmalloc_node_track_caller(size, flags, node, \\\n\t\t\t\t    _RET_IP_)\n\n \n#define kmalloc_track_caller(size, flags) \\\n\t__kmalloc_node_track_caller(size, flags, \\\n\t\t\t\t    NUMA_NO_NODE, _RET_IP_)\n\nstatic inline __alloc_size(1, 2) void *kmalloc_array_node(size_t n, size_t size, gfp_t flags,\n\t\t\t\t\t\t\t  int node)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\tif (__builtin_constant_p(n) && __builtin_constant_p(size))\n\t\treturn kmalloc_node(bytes, flags, node);\n\treturn __kmalloc_node(bytes, flags, node);\n}\n\nstatic inline __alloc_size(1, 2) void *kcalloc_node(size_t n, size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_array_node(n, size, flags | __GFP_ZERO, node);\n}\n\n \nstatic inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)\n{\n\treturn kmem_cache_alloc(k, flags | __GFP_ZERO);\n}\n\n \nstatic inline __alloc_size(1) void *kzalloc(size_t size, gfp_t flags)\n{\n\treturn kmalloc(size, flags | __GFP_ZERO);\n}\n\n \nstatic inline __alloc_size(1) void *kzalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn kmalloc_node(size, flags | __GFP_ZERO, node);\n}\n\nextern void *kvmalloc_node(size_t size, gfp_t flags, int node) __alloc_size(1);\nstatic inline __alloc_size(1) void *kvmalloc(size_t size, gfp_t flags)\n{\n\treturn kvmalloc_node(size, flags, NUMA_NO_NODE);\n}\nstatic inline __alloc_size(1) void *kvzalloc_node(size_t size, gfp_t flags, int node)\n{\n\treturn kvmalloc_node(size, flags | __GFP_ZERO, node);\n}\nstatic inline __alloc_size(1) void *kvzalloc(size_t size, gfp_t flags)\n{\n\treturn kvmalloc(size, flags | __GFP_ZERO);\n}\n\nstatic inline __alloc_size(1, 2) void *kvmalloc_array(size_t n, size_t size, gfp_t flags)\n{\n\tsize_t bytes;\n\n\tif (unlikely(check_mul_overflow(n, size, &bytes)))\n\t\treturn NULL;\n\n\treturn kvmalloc(bytes, flags);\n}\n\nstatic inline __alloc_size(1, 2) void *kvcalloc(size_t n, size_t size, gfp_t flags)\n{\n\treturn kvmalloc_array(n, size, flags | __GFP_ZERO);\n}\n\nextern void *kvrealloc(const void *p, size_t oldsize, size_t newsize, gfp_t flags)\n\t\t      __realloc_size(3);\nextern void kvfree(const void *addr);\nextern void kvfree_sensitive(const void *addr, size_t len);\n\nunsigned int kmem_cache_size(struct kmem_cache *s);\n\n \nsize_t kmalloc_size_roundup(size_t size);\n\nvoid __init kmem_cache_init_late(void);\n\n#if defined(CONFIG_SMP) && defined(CONFIG_SLAB)\nint slab_prepare_cpu(unsigned int cpu);\nint slab_dead_cpu(unsigned int cpu);\n#else\n#define slab_prepare_cpu\tNULL\n#define slab_dead_cpu\t\tNULL\n#endif\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}