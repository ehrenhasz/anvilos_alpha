{
  "module_name": "topology.h",
  "hash_id": "e47410d71d01a91d8987abc24ef97bd0b234f48f155d572791f918caac62ae76",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/sched/topology.h",
  "human_readable_source": " \n#ifndef _LINUX_SCHED_TOPOLOGY_H\n#define _LINUX_SCHED_TOPOLOGY_H\n\n#include <linux/topology.h>\n\n#include <linux/sched/idle.h>\n\n \n#ifdef CONFIG_SMP\n\n \n#define SD_FLAG(name, mflags) __##name,\nenum {\n\t#include <linux/sched/sd_flags.h>\n\t__SD_FLAG_CNT,\n};\n#undef SD_FLAG\n \n#define SD_FLAG(name, mflags) name = 1 << __##name,\nenum {\n\t#include <linux/sched/sd_flags.h>\n};\n#undef SD_FLAG\n\n#ifdef CONFIG_SCHED_DEBUG\n\nstruct sd_flag_debug {\n\tunsigned int meta_flags;\n\tchar *name;\n};\nextern const struct sd_flag_debug sd_flag_debug[];\n\n#endif\n\n#ifdef CONFIG_SCHED_SMT\nstatic inline int cpu_smt_flags(void)\n{\n\treturn SD_SHARE_CPUCAPACITY | SD_SHARE_PKG_RESOURCES;\n}\n#endif\n\n#ifdef CONFIG_SCHED_CLUSTER\nstatic inline int cpu_cluster_flags(void)\n{\n\treturn SD_SHARE_PKG_RESOURCES;\n}\n#endif\n\n#ifdef CONFIG_SCHED_MC\nstatic inline int cpu_core_flags(void)\n{\n\treturn SD_SHARE_PKG_RESOURCES;\n}\n#endif\n\n#ifdef CONFIG_NUMA\nstatic inline int cpu_numa_flags(void)\n{\n\treturn SD_NUMA;\n}\n#endif\n\nextern int arch_asym_cpu_priority(int cpu);\n\nstruct sched_domain_attr {\n\tint relax_domain_level;\n};\n\n#define SD_ATTR_INIT\t(struct sched_domain_attr) {\t\\\n\t.relax_domain_level = -1,\t\t\t\\\n}\n\nextern int sched_domain_level_max;\n\nstruct sched_group;\n\nstruct sched_domain_shared {\n\tatomic_t\tref;\n\tatomic_t\tnr_busy_cpus;\n\tint\t\thas_idle_cores;\n\tint\t\tnr_idle_scan;\n};\n\nstruct sched_domain {\n\t \n\tstruct sched_domain __rcu *parent;\t \n\tstruct sched_domain __rcu *child;\t \n\tstruct sched_group *groups;\t \n\tunsigned long min_interval;\t \n\tunsigned long max_interval;\t \n\tunsigned int busy_factor;\t \n\tunsigned int imbalance_pct;\t \n\tunsigned int cache_nice_tries;\t \n\tunsigned int imb_numa_nr;\t \n\n\tint nohz_idle;\t\t\t \n\tint flags;\t\t\t \n\tint level;\n\n\t \n\tunsigned long last_balance;\t \n\tunsigned int balance_interval;\t \n\tunsigned int nr_balance_failed;  \n\n\t \n\tu64 max_newidle_lb_cost;\n\tunsigned long last_decay_max_lb_cost;\n\n\tu64 avg_scan_cost;\t\t \n\n#ifdef CONFIG_SCHEDSTATS\n\t \n\tunsigned int lb_count[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_failed[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_balanced[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_imbalance[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_hot_gained[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyg[CPU_MAX_IDLE_TYPES];\n\tunsigned int lb_nobusyq[CPU_MAX_IDLE_TYPES];\n\n\t \n\tunsigned int alb_count;\n\tunsigned int alb_failed;\n\tunsigned int alb_pushed;\n\n\t \n\tunsigned int sbe_count;\n\tunsigned int sbe_balanced;\n\tunsigned int sbe_pushed;\n\n\t \n\tunsigned int sbf_count;\n\tunsigned int sbf_balanced;\n\tunsigned int sbf_pushed;\n\n\t \n\tunsigned int ttwu_wake_remote;\n\tunsigned int ttwu_move_affine;\n\tunsigned int ttwu_move_balance;\n#endif\n#ifdef CONFIG_SCHED_DEBUG\n\tchar *name;\n#endif\n\tunion {\n\t\tvoid *private;\t\t \n\t\tstruct rcu_head rcu;\t \n\t};\n\tstruct sched_domain_shared *shared;\n\n\tunsigned int span_weight;\n\t \n\tunsigned long span[];\n};\n\nstatic inline struct cpumask *sched_domain_span(struct sched_domain *sd)\n{\n\treturn to_cpumask(sd->span);\n}\n\nextern void partition_sched_domains_locked(int ndoms_new,\n\t\t\t\t\t   cpumask_var_t doms_new[],\n\t\t\t\t\t   struct sched_domain_attr *dattr_new);\n\nextern void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t\t    struct sched_domain_attr *dattr_new);\n\n \ncpumask_var_t *alloc_sched_domains(unsigned int ndoms);\nvoid free_sched_domains(cpumask_var_t doms[], unsigned int ndoms);\n\nbool cpus_share_cache(int this_cpu, int that_cpu);\n\ntypedef const struct cpumask *(*sched_domain_mask_f)(int cpu);\ntypedef int (*sched_domain_flags_f)(void);\n\n#define SDTL_OVERLAP\t0x01\n\nstruct sd_data {\n\tstruct sched_domain *__percpu *sd;\n\tstruct sched_domain_shared *__percpu *sds;\n\tstruct sched_group *__percpu *sg;\n\tstruct sched_group_capacity *__percpu *sgc;\n};\n\nstruct sched_domain_topology_level {\n\tsched_domain_mask_f mask;\n\tsched_domain_flags_f sd_flags;\n\tint\t\t    flags;\n\tint\t\t    numa_level;\n\tstruct sd_data      data;\n#ifdef CONFIG_SCHED_DEBUG\n\tchar                *name;\n#endif\n};\n\nextern void __init set_sched_topology(struct sched_domain_topology_level *tl);\n\n#ifdef CONFIG_SCHED_DEBUG\n# define SD_INIT_NAME(type)\t\t.name = #type\n#else\n# define SD_INIT_NAME(type)\n#endif\n\n#else  \n\nstruct sched_domain_attr;\n\nstatic inline void\npartition_sched_domains_locked(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\t       struct sched_domain_attr *dattr_new)\n{\n}\n\nstatic inline void\npartition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],\n\t\t\tstruct sched_domain_attr *dattr_new)\n{\n}\n\nstatic inline bool cpus_share_cache(int this_cpu, int that_cpu)\n{\n\treturn true;\n}\n\n#endif\t \n\n#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)\nextern void rebuild_sched_domains_energy(void);\n#else\nstatic inline void rebuild_sched_domains_energy(void)\n{\n}\n#endif\n\n#ifndef arch_scale_cpu_capacity\n \nstatic __always_inline\nunsigned long arch_scale_cpu_capacity(int cpu)\n{\n\treturn SCHED_CAPACITY_SCALE;\n}\n#endif\n\n#ifndef arch_scale_thermal_pressure\nstatic __always_inline\nunsigned long arch_scale_thermal_pressure(int cpu)\n{\n\treturn 0;\n}\n#endif\n\n#ifndef arch_update_thermal_pressure\nstatic __always_inline\nvoid arch_update_thermal_pressure(const struct cpumask *cpus,\n\t\t\t\t  unsigned long capped_frequency)\n{ }\n#endif\n\nstatic inline int task_node(const struct task_struct *p)\n{\n\treturn cpu_to_node(task_cpu(p));\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}