{
  "module_name": "mm.h",
  "hash_id": "3f5c414ee30b06e6771553242ae8970e3d85cd4bbe46c35a3903ecd90490bbde",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/sched/mm.h",
  "human_readable_source": " \n#ifndef _LINUX_SCHED_MM_H\n#define _LINUX_SCHED_MM_H\n\n#include <linux/kernel.h>\n#include <linux/atomic.h>\n#include <linux/sched.h>\n#include <linux/mm_types.h>\n#include <linux/gfp.h>\n#include <linux/sync_core.h>\n\n \nextern struct mm_struct *mm_alloc(void);\n\n \nstatic inline void mmgrab(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_count);\n}\n\nstatic inline void smp_mb__after_mmgrab(void)\n{\n\tsmp_mb__after_atomic();\n}\n\nextern void __mmdrop(struct mm_struct *mm);\n\nstatic inline void mmdrop(struct mm_struct *mm)\n{\n\t \n\tif (unlikely(atomic_dec_and_test(&mm->mm_count)))\n\t\t__mmdrop(mm);\n}\n\n#ifdef CONFIG_PREEMPT_RT\n \nstatic inline void __mmdrop_delayed(struct rcu_head *rhp)\n{\n\tstruct mm_struct *mm = container_of(rhp, struct mm_struct, delayed_drop);\n\n\t__mmdrop(mm);\n}\n\n \nstatic inline void mmdrop_sched(struct mm_struct *mm)\n{\n\t \n\tif (atomic_dec_and_test(&mm->mm_count))\n\t\tcall_rcu(&mm->delayed_drop, __mmdrop_delayed);\n}\n#else\nstatic inline void mmdrop_sched(struct mm_struct *mm)\n{\n\tmmdrop(mm);\n}\n#endif\n\n \nstatic inline void mmgrab_lazy_tlb(struct mm_struct *mm)\n{\n\tif (IS_ENABLED(CONFIG_MMU_LAZY_TLB_REFCOUNT))\n\t\tmmgrab(mm);\n}\n\nstatic inline void mmdrop_lazy_tlb(struct mm_struct *mm)\n{\n\tif (IS_ENABLED(CONFIG_MMU_LAZY_TLB_REFCOUNT)) {\n\t\tmmdrop(mm);\n\t} else {\n\t\t \n\t\tsmp_mb();\n\t}\n}\n\nstatic inline void mmdrop_lazy_tlb_sched(struct mm_struct *mm)\n{\n\tif (IS_ENABLED(CONFIG_MMU_LAZY_TLB_REFCOUNT))\n\t\tmmdrop_sched(mm);\n\telse\n\t\tsmp_mb();  \n}\n\n \nstatic inline void mmget(struct mm_struct *mm)\n{\n\tatomic_inc(&mm->mm_users);\n}\n\nstatic inline bool mmget_not_zero(struct mm_struct *mm)\n{\n\treturn atomic_inc_not_zero(&mm->mm_users);\n}\n\n \nextern void mmput(struct mm_struct *);\n#ifdef CONFIG_MMU\n \nvoid mmput_async(struct mm_struct *);\n#endif\n\n \nextern struct mm_struct *get_task_mm(struct task_struct *task);\n \nextern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);\n \nextern void exit_mm_release(struct task_struct *, struct mm_struct *);\n \nextern void exec_mm_release(struct task_struct *, struct mm_struct *);\n\n#ifdef CONFIG_MEMCG\nextern void mm_update_next_owner(struct mm_struct *mm);\n#else\nstatic inline void mm_update_next_owner(struct mm_struct *mm)\n{\n}\n#endif  \n\n#ifdef CONFIG_MMU\n#ifndef arch_get_mmap_end\n#define arch_get_mmap_end(addr, len, flags)\t(TASK_SIZE)\n#endif\n\n#ifndef arch_get_mmap_base\n#define arch_get_mmap_base(addr, base) (base)\n#endif\n\nextern void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t  struct rlimit *rlim_stack);\nextern unsigned long\narch_get_unmapped_area(struct file *, unsigned long, unsigned long,\n\t\t       unsigned long, unsigned long);\nextern unsigned long\narch_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\n\nunsigned long\ngeneric_get_unmapped_area(struct file *filp, unsigned long addr,\n\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t  unsigned long flags);\nunsigned long\ngeneric_get_unmapped_area_topdown(struct file *filp, unsigned long addr,\n\t\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t\t  unsigned long flags);\n#else\nstatic inline void arch_pick_mmap_layout(struct mm_struct *mm,\n\t\t\t\t\t struct rlimit *rlim_stack) {}\n#endif\n\nstatic inline bool in_vfork(struct task_struct *tsk)\n{\n\tbool ret;\n\n\t \n\trcu_read_lock();\n\tret = tsk->vfork_done &&\n\t\t\trcu_dereference(tsk->real_parent)->mm == tsk->mm;\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nstatic inline gfp_t current_gfp_context(gfp_t flags)\n{\n\tunsigned int pflags = READ_ONCE(current->flags);\n\n\tif (unlikely(pflags & (PF_MEMALLOC_NOIO | PF_MEMALLOC_NOFS | PF_MEMALLOC_PIN))) {\n\t\t \n\t\tif (pflags & PF_MEMALLOC_NOIO)\n\t\t\tflags &= ~(__GFP_IO | __GFP_FS);\n\t\telse if (pflags & PF_MEMALLOC_NOFS)\n\t\t\tflags &= ~__GFP_FS;\n\n\t\tif (pflags & PF_MEMALLOC_PIN)\n\t\t\tflags &= ~__GFP_MOVABLE;\n\t}\n\treturn flags;\n}\n\n#ifdef CONFIG_LOCKDEP\nextern void __fs_reclaim_acquire(unsigned long ip);\nextern void __fs_reclaim_release(unsigned long ip);\nextern void fs_reclaim_acquire(gfp_t gfp_mask);\nextern void fs_reclaim_release(gfp_t gfp_mask);\n#else\nstatic inline void __fs_reclaim_acquire(unsigned long ip) { }\nstatic inline void __fs_reclaim_release(unsigned long ip) { }\nstatic inline void fs_reclaim_acquire(gfp_t gfp_mask) { }\nstatic inline void fs_reclaim_release(gfp_t gfp_mask) { }\n#endif\n\n \nstatic inline void memalloc_retry_wait(gfp_t gfp_flags)\n{\n\t \n\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\tgfp_flags = current_gfp_context(gfp_flags);\n\tif (gfpflags_allow_blocking(gfp_flags) &&\n\t    !(gfp_flags & __GFP_NORETRY))\n\t\t \n\t\tio_schedule_timeout(1);\n\telse\n\t\t \n\t\tio_schedule_timeout(HZ/50);\n}\n\n \nstatic inline void might_alloc(gfp_t gfp_mask)\n{\n\tfs_reclaim_acquire(gfp_mask);\n\tfs_reclaim_release(gfp_mask);\n\n\tmight_sleep_if(gfpflags_allow_blocking(gfp_mask));\n}\n\n \nstatic inline unsigned int memalloc_noio_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOIO;\n\tcurrent->flags |= PF_MEMALLOC_NOIO;\n\treturn flags;\n}\n\n \nstatic inline void memalloc_noio_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOIO) | flags;\n}\n\n \nstatic inline unsigned int memalloc_nofs_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_NOFS;\n\tcurrent->flags |= PF_MEMALLOC_NOFS;\n\treturn flags;\n}\n\n \nstatic inline void memalloc_nofs_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_NOFS) | flags;\n}\n\nstatic inline unsigned int memalloc_noreclaim_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC;\n\tcurrent->flags |= PF_MEMALLOC;\n\treturn flags;\n}\n\nstatic inline void memalloc_noreclaim_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC) | flags;\n}\n\nstatic inline unsigned int memalloc_pin_save(void)\n{\n\tunsigned int flags = current->flags & PF_MEMALLOC_PIN;\n\n\tcurrent->flags |= PF_MEMALLOC_PIN;\n\treturn flags;\n}\n\nstatic inline void memalloc_pin_restore(unsigned int flags)\n{\n\tcurrent->flags = (current->flags & ~PF_MEMALLOC_PIN) | flags;\n}\n\n#ifdef CONFIG_MEMCG\nDECLARE_PER_CPU(struct mem_cgroup *, int_active_memcg);\n \nstatic inline struct mem_cgroup *\nset_active_memcg(struct mem_cgroup *memcg)\n{\n\tstruct mem_cgroup *old;\n\n\tif (!in_task()) {\n\t\told = this_cpu_read(int_active_memcg);\n\t\tthis_cpu_write(int_active_memcg, memcg);\n\t} else {\n\t\told = current->active_memcg;\n\t\tcurrent->active_memcg = memcg;\n\t}\n\n\treturn old;\n}\n#else\nstatic inline struct mem_cgroup *\nset_active_memcg(struct mem_cgroup *memcg)\n{\n\treturn NULL;\n}\n#endif\n\n#ifdef CONFIG_MEMBARRIER\nenum {\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_READY\t\t= (1U << 0),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED\t\t\t= (1U << 1),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED_READY\t\t\t= (1U << 2),\n\tMEMBARRIER_STATE_GLOBAL_EXPEDITED\t\t\t= (1U << 3),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE_READY\t= (1U << 4),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE\t\t= (1U << 5),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ_READY\t\t= (1U << 6),\n\tMEMBARRIER_STATE_PRIVATE_EXPEDITED_RSEQ\t\t\t= (1U << 7),\n};\n\nenum {\n\tMEMBARRIER_FLAG_SYNC_CORE\t= (1U << 0),\n\tMEMBARRIER_FLAG_RSEQ\t\t= (1U << 1),\n};\n\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\n#include <asm/membarrier.h>\n#endif\n\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n\tif (current->mm != mm)\n\t\treturn;\n\tif (likely(!(atomic_read(&mm->membarrier_state) &\n\t\t     MEMBARRIER_STATE_PRIVATE_EXPEDITED_SYNC_CORE)))\n\t\treturn;\n\tsync_core_before_usermode();\n}\n\nextern void membarrier_exec_mmap(struct mm_struct *mm);\n\nextern void membarrier_update_current_mm(struct mm_struct *next_mm);\n\n#else\n#ifdef CONFIG_ARCH_HAS_MEMBARRIER_CALLBACKS\nstatic inline void membarrier_arch_switch_mm(struct mm_struct *prev,\n\t\t\t\t\t     struct mm_struct *next,\n\t\t\t\t\t     struct task_struct *tsk)\n{\n}\n#endif\nstatic inline void membarrier_exec_mmap(struct mm_struct *mm)\n{\n}\nstatic inline void membarrier_mm_sync_core_before_usermode(struct mm_struct *mm)\n{\n}\nstatic inline void membarrier_update_current_mm(struct mm_struct *next_mm)\n{\n}\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}