{
  "module_name": "signal.h",
  "hash_id": "f10573d7452067ccd34d49fca36584e1fbffbedc9984e0e21403167e21234561",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/sched/signal.h",
  "human_readable_source": " \n#ifndef _LINUX_SCHED_SIGNAL_H\n#define _LINUX_SCHED_SIGNAL_H\n\n#include <linux/rculist.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/sched/jobctl.h>\n#include <linux/sched/task.h>\n#include <linux/cred.h>\n#include <linux/refcount.h>\n#include <linux/posix-timers.h>\n#include <linux/mm_types.h>\n#include <asm/ptrace.h>\n\n \n\nstruct sighand_struct {\n\tspinlock_t\t\tsiglock;\n\trefcount_t\t\tcount;\n\twait_queue_head_t\tsignalfd_wqh;\n\tstruct k_sigaction\taction[_NSIG];\n};\n\n \nstruct pacct_struct {\n\tint\t\t\tac_flag;\n\tlong\t\t\tac_exitcode;\n\tunsigned long\t\tac_mem;\n\tu64\t\t\tac_utime, ac_stime;\n\tunsigned long\t\tac_minflt, ac_majflt;\n};\n\nstruct cpu_itimer {\n\tu64 expires;\n\tu64 incr;\n};\n\n \nstruct task_cputime_atomic {\n\tatomic64_t utime;\n\tatomic64_t stime;\n\tatomic64_t sum_exec_runtime;\n};\n\n#define INIT_CPUTIME_ATOMIC \\\n\t(struct task_cputime_atomic) {\t\t\t\t\\\n\t\t.utime = ATOMIC64_INIT(0),\t\t\t\\\n\t\t.stime = ATOMIC64_INIT(0),\t\t\t\\\n\t\t.sum_exec_runtime = ATOMIC64_INIT(0),\t\t\\\n\t}\n \nstruct thread_group_cputimer {\n\tstruct task_cputime_atomic cputime_atomic;\n};\n\nstruct multiprocess_signals {\n\tsigset_t signal;\n\tstruct hlist_node node;\n};\n\nstruct core_thread {\n\tstruct task_struct *task;\n\tstruct core_thread *next;\n};\n\nstruct core_state {\n\tatomic_t nr_threads;\n\tstruct core_thread dumper;\n\tstruct completion startup;\n};\n\n \nstruct signal_struct {\n\trefcount_t\t\tsigcnt;\n\tatomic_t\t\tlive;\n\tint\t\t\tnr_threads;\n\tint\t\t\tquick_threads;\n\tstruct list_head\tthread_head;\n\n\twait_queue_head_t\twait_chldexit;\t \n\n\t \n\tstruct task_struct\t*curr_target;\n\n\t \n\tstruct sigpending\tshared_pending;\n\n\t \n\tstruct hlist_head\tmultiprocess;\n\n\t \n\tint\t\t\tgroup_exit_code;\n\t \n\tint\t\t\tnotify_count;\n\tstruct task_struct\t*group_exec_task;\n\n\t \n\tint\t\t\tgroup_stop_count;\n\tunsigned int\t\tflags;  \n\n\tstruct core_state *core_state;  \n\n\t \n\tunsigned int\t\tis_child_subreaper:1;\n\tunsigned int\t\thas_child_subreaper:1;\n\n#ifdef CONFIG_POSIX_TIMERS\n\n\t \n\tunsigned int\t\tnext_posix_timer_id;\n\tstruct list_head\tposix_timers;\n\n\t \n\tstruct hrtimer real_timer;\n\tktime_t it_real_incr;\n\n\t \n\tstruct cpu_itimer it[2];\n\n\t \n\tstruct thread_group_cputimer cputimer;\n\n#endif\n\t \n\tstruct posix_cputimers posix_cputimers;\n\n\t \n\tstruct pid *pids[PIDTYPE_MAX];\n\n#ifdef CONFIG_NO_HZ_FULL\n\tatomic_t tick_dep_mask;\n#endif\n\n\tstruct pid *tty_old_pgrp;\n\n\t \n\tint leader;\n\n\tstruct tty_struct *tty;  \n\n#ifdef CONFIG_SCHED_AUTOGROUP\n\tstruct autogroup *autogroup;\n#endif\n\t \n\tseqlock_t stats_lock;\n\tu64 utime, stime, cutime, cstime;\n\tu64 gtime;\n\tu64 cgtime;\n\tstruct prev_cputime prev_cputime;\n\tunsigned long nvcsw, nivcsw, cnvcsw, cnivcsw;\n\tunsigned long min_flt, maj_flt, cmin_flt, cmaj_flt;\n\tunsigned long inblock, oublock, cinblock, coublock;\n\tunsigned long maxrss, cmaxrss;\n\tstruct task_io_accounting ioac;\n\n\t \n\tunsigned long long sum_sched_runtime;\n\n\t \n\tstruct rlimit rlim[RLIM_NLIMITS];\n\n#ifdef CONFIG_BSD_PROCESS_ACCT\n\tstruct pacct_struct pacct;\t \n#endif\n#ifdef CONFIG_TASKSTATS\n\tstruct taskstats *stats;\n#endif\n#ifdef CONFIG_AUDIT\n\tunsigned audit_tty;\n\tstruct tty_audit_buf *tty_audit_buf;\n#endif\n\n\t \n\tbool oom_flag_origin;\n\tshort oom_score_adj;\t\t \n\tshort oom_score_adj_min;\t \n\tstruct mm_struct *oom_mm;\t \n\n\tstruct mutex cred_guard_mutex;\t \n\tstruct rw_semaphore exec_update_lock;\t \n} __randomize_layout;\n\n \n#define SIGNAL_STOP_STOPPED\t0x00000001  \n#define SIGNAL_STOP_CONTINUED\t0x00000002  \n#define SIGNAL_GROUP_EXIT\t0x00000004  \n \n#define SIGNAL_CLD_STOPPED\t0x00000010\n#define SIGNAL_CLD_CONTINUED\t0x00000020\n#define SIGNAL_CLD_MASK\t\t(SIGNAL_CLD_STOPPED|SIGNAL_CLD_CONTINUED)\n\n#define SIGNAL_UNKILLABLE\t0x00000040  \n\n#define SIGNAL_STOP_MASK (SIGNAL_CLD_MASK | SIGNAL_STOP_STOPPED | \\\n\t\t\t  SIGNAL_STOP_CONTINUED)\n\nstatic inline void signal_set_stop_flags(struct signal_struct *sig,\n\t\t\t\t\t unsigned int flags)\n{\n\tWARN_ON(sig->flags & SIGNAL_GROUP_EXIT);\n\tsig->flags = (sig->flags & ~SIGNAL_STOP_MASK) | flags;\n}\n\nextern void flush_signals(struct task_struct *);\nextern void ignore_signals(struct task_struct *);\nextern void flush_signal_handlers(struct task_struct *, int force_default);\nextern int dequeue_signal(struct task_struct *task, sigset_t *mask,\n\t\t\t  kernel_siginfo_t *info, enum pid_type *type);\n\nstatic inline int kernel_dequeue_signal(void)\n{\n\tstruct task_struct *task = current;\n\tkernel_siginfo_t __info;\n\tenum pid_type __type;\n\tint ret;\n\n\tspin_lock_irq(&task->sighand->siglock);\n\tret = dequeue_signal(task, &task->blocked, &__info, &__type);\n\tspin_unlock_irq(&task->sighand->siglock);\n\n\treturn ret;\n}\n\nstatic inline void kernel_signal_stop(void)\n{\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (current->jobctl & JOBCTL_STOP_DEQUEUED) {\n\t\tcurrent->jobctl |= JOBCTL_STOPPED;\n\t\tset_special_state(TASK_STOPPED);\n\t}\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tschedule();\n}\n#ifdef __ia64__\n# define ___ARCH_SI_IA64(_a1, _a2, _a3) , _a1, _a2, _a3\n#else\n# define ___ARCH_SI_IA64(_a1, _a2, _a3)\n#endif\n\nint force_sig_fault_to_task(int sig, int code, void __user *addr\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t);\nint force_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr));\nint send_sig_fault(int sig, int code, void __user *addr\n\t___ARCH_SI_IA64(int imm, unsigned int flags, unsigned long isr)\n\t, struct task_struct *t);\n\nint force_sig_mceerr(int code, void __user *, short);\nint send_sig_mceerr(int code, void __user *, short, struct task_struct *);\n\nint force_sig_bnderr(void __user *addr, void __user *lower, void __user *upper);\nint force_sig_pkuerr(void __user *addr, u32 pkey);\nint send_sig_perf(void __user *addr, u32 type, u64 sig_data);\n\nint force_sig_ptrace_errno_trap(int errno, void __user *addr);\nint force_sig_fault_trapno(int sig, int code, void __user *addr, int trapno);\nint send_sig_fault_trapno(int sig, int code, void __user *addr, int trapno,\n\t\t\tstruct task_struct *t);\nint force_sig_seccomp(int syscall, int reason, bool force_coredump);\n\nextern int send_sig_info(int, struct kernel_siginfo *, struct task_struct *);\nextern void force_sigsegv(int sig);\nextern int force_sig_info(struct kernel_siginfo *);\nextern int __kill_pgrp_info(int sig, struct kernel_siginfo *info, struct pid *pgrp);\nextern int kill_pid_info(int sig, struct kernel_siginfo *info, struct pid *pid);\nextern int kill_pid_usb_asyncio(int sig, int errno, sigval_t addr, struct pid *,\n\t\t\t\tconst struct cred *);\nextern int kill_pgrp(struct pid *pid, int sig, int priv);\nextern int kill_pid(struct pid *pid, int sig, int priv);\nextern __must_check bool do_notify_parent(struct task_struct *, int);\nextern void __wake_up_parent(struct task_struct *p, struct task_struct *parent);\nextern void force_sig(int);\nextern void force_fatal_sig(int);\nextern void force_exit_sig(int);\nextern int send_sig(int, struct task_struct *, int);\nextern int zap_other_threads(struct task_struct *p);\nextern struct sigqueue *sigqueue_alloc(void);\nextern void sigqueue_free(struct sigqueue *);\nextern int send_sigqueue(struct sigqueue *, struct pid *, enum pid_type);\nextern int do_sigaction(int, struct k_sigaction *, struct k_sigaction *);\n\nstatic inline void clear_notify_signal(void)\n{\n\tclear_thread_flag(TIF_NOTIFY_SIGNAL);\n\tsmp_mb__after_atomic();\n}\n\n \nstatic inline bool __set_notify_signal(struct task_struct *task)\n{\n\treturn !test_and_set_tsk_thread_flag(task, TIF_NOTIFY_SIGNAL) &&\n\t       !wake_up_state(task, TASK_INTERRUPTIBLE);\n}\n\n \nstatic inline void set_notify_signal(struct task_struct *task)\n{\n\tif (__set_notify_signal(task))\n\t\tkick_process(task);\n}\n\nstatic inline int restart_syscall(void)\n{\n\tset_tsk_thread_flag(current, TIF_SIGPENDING);\n\treturn -ERESTARTNOINTR;\n}\n\nstatic inline int task_sigpending(struct task_struct *p)\n{\n\treturn unlikely(test_tsk_thread_flag(p,TIF_SIGPENDING));\n}\n\nstatic inline int signal_pending(struct task_struct *p)\n{\n\t \n\tif (unlikely(test_tsk_thread_flag(p, TIF_NOTIFY_SIGNAL)))\n\t\treturn 1;\n\treturn task_sigpending(p);\n}\n\nstatic inline int __fatal_signal_pending(struct task_struct *p)\n{\n\treturn unlikely(sigismember(&p->pending.signal, SIGKILL));\n}\n\nstatic inline int fatal_signal_pending(struct task_struct *p)\n{\n\treturn task_sigpending(p) && __fatal_signal_pending(p);\n}\n\nstatic inline int signal_pending_state(unsigned int state, struct task_struct *p)\n{\n\tif (!(state & (TASK_INTERRUPTIBLE | TASK_WAKEKILL)))\n\t\treturn 0;\n\tif (!signal_pending(p))\n\t\treturn 0;\n\n\treturn (state & TASK_INTERRUPTIBLE) || __fatal_signal_pending(p);\n}\n\n \nstatic inline bool fault_signal_pending(vm_fault_t fault_flags,\n\t\t\t\t\tstruct pt_regs *regs)\n{\n\treturn unlikely((fault_flags & VM_FAULT_RETRY) &&\n\t\t\t(fatal_signal_pending(current) ||\n\t\t\t (user_mode(regs) && signal_pending(current))));\n}\n\n \nextern void recalc_sigpending_and_wake(struct task_struct *t);\nextern void recalc_sigpending(void);\nextern void calculate_sigpending(void);\n\nextern void signal_wake_up_state(struct task_struct *t, unsigned int state);\n\nstatic inline void signal_wake_up(struct task_struct *t, bool fatal)\n{\n\tunsigned int state = 0;\n\tif (fatal && !(t->jobctl & JOBCTL_PTRACE_FROZEN)) {\n\t\tt->jobctl &= ~(JOBCTL_STOPPED | JOBCTL_TRACED);\n\t\tstate = TASK_WAKEKILL | __TASK_TRACED;\n\t}\n\tsignal_wake_up_state(t, state);\n}\nstatic inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)\n{\n\tunsigned int state = 0;\n\tif (resume) {\n\t\tt->jobctl &= ~JOBCTL_TRACED;\n\t\tstate = __TASK_TRACED;\n\t}\n\tsignal_wake_up_state(t, state);\n}\n\nvoid task_join_group_stop(struct task_struct *task);\n\n#ifdef TIF_RESTORE_SIGMASK\n \n\n \nstatic inline void set_restore_sigmask(void)\n{\n\tset_thread_flag(TIF_RESTORE_SIGMASK);\n}\n\nstatic inline void clear_tsk_restore_sigmask(struct task_struct *task)\n{\n\tclear_tsk_thread_flag(task, TIF_RESTORE_SIGMASK);\n}\n\nstatic inline void clear_restore_sigmask(void)\n{\n\tclear_thread_flag(TIF_RESTORE_SIGMASK);\n}\nstatic inline bool test_tsk_restore_sigmask(struct task_struct *task)\n{\n\treturn test_tsk_thread_flag(task, TIF_RESTORE_SIGMASK);\n}\nstatic inline bool test_restore_sigmask(void)\n{\n\treturn test_thread_flag(TIF_RESTORE_SIGMASK);\n}\nstatic inline bool test_and_clear_restore_sigmask(void)\n{\n\treturn test_and_clear_thread_flag(TIF_RESTORE_SIGMASK);\n}\n\n#else\t \n\n \nstatic inline void set_restore_sigmask(void)\n{\n\tcurrent->restore_sigmask = true;\n}\nstatic inline void clear_tsk_restore_sigmask(struct task_struct *task)\n{\n\ttask->restore_sigmask = false;\n}\nstatic inline void clear_restore_sigmask(void)\n{\n\tcurrent->restore_sigmask = false;\n}\nstatic inline bool test_restore_sigmask(void)\n{\n\treturn current->restore_sigmask;\n}\nstatic inline bool test_tsk_restore_sigmask(struct task_struct *task)\n{\n\treturn task->restore_sigmask;\n}\nstatic inline bool test_and_clear_restore_sigmask(void)\n{\n\tif (!current->restore_sigmask)\n\t\treturn false;\n\tcurrent->restore_sigmask = false;\n\treturn true;\n}\n#endif\n\nstatic inline void restore_saved_sigmask(void)\n{\n\tif (test_and_clear_restore_sigmask())\n\t\t__set_current_blocked(&current->saved_sigmask);\n}\n\nextern int set_user_sigmask(const sigset_t __user *umask, size_t sigsetsize);\n\nstatic inline void restore_saved_sigmask_unless(bool interrupted)\n{\n\tif (interrupted)\n\t\tWARN_ON(!signal_pending(current));\n\telse\n\t\trestore_saved_sigmask();\n}\n\nstatic inline sigset_t *sigmask_to_save(void)\n{\n\tsigset_t *res = &current->blocked;\n\tif (unlikely(test_restore_sigmask()))\n\t\tres = &current->saved_sigmask;\n\treturn res;\n}\n\nstatic inline int kill_cad_pid(int sig, int priv)\n{\n\treturn kill_pid(cad_pid, sig, priv);\n}\n\n \n#define SEND_SIG_NOINFO ((struct kernel_siginfo *) 0)\n#define SEND_SIG_PRIV\t((struct kernel_siginfo *) 1)\n\nstatic inline int __on_sig_stack(unsigned long sp)\n{\n#ifdef CONFIG_STACK_GROWSUP\n\treturn sp >= current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp < current->sas_ss_size;\n#else\n\treturn sp > current->sas_ss_sp &&\n\t\tsp - current->sas_ss_sp <= current->sas_ss_size;\n#endif\n}\n\n \nstatic inline int on_sig_stack(unsigned long sp)\n{\n\t \n\tif (current->sas_ss_flags & SS_AUTODISARM)\n\t\treturn 0;\n\n\treturn __on_sig_stack(sp);\n}\n\nstatic inline int sas_ss_flags(unsigned long sp)\n{\n\tif (!current->sas_ss_size)\n\t\treturn SS_DISABLE;\n\n\treturn on_sig_stack(sp) ? SS_ONSTACK : 0;\n}\n\nstatic inline void sas_ss_reset(struct task_struct *p)\n{\n\tp->sas_ss_sp = 0;\n\tp->sas_ss_size = 0;\n\tp->sas_ss_flags = SS_DISABLE;\n}\n\nstatic inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)\n{\n\tif (unlikely((ksig->ka.sa.sa_flags & SA_ONSTACK)) && ! sas_ss_flags(sp))\n#ifdef CONFIG_STACK_GROWSUP\n\t\treturn current->sas_ss_sp;\n#else\n\t\treturn current->sas_ss_sp + current->sas_ss_size;\n#endif\n\treturn sp;\n}\n\nextern void __cleanup_sighand(struct sighand_struct *);\nextern void flush_itimer_signals(void);\n\n#define tasklist_empty() \\\n\tlist_empty(&init_task.tasks)\n\n#define next_task(p) \\\n\tlist_entry_rcu((p)->tasks.next, struct task_struct, tasks)\n\n#define for_each_process(p) \\\n\tfor (p = &init_task ; (p = next_task(p)) != &init_task ; )\n\nextern bool current_is_single_threaded(void);\n\n \n#define while_each_thread(g, t) \\\n\twhile ((t = next_thread(t)) != g)\n\n#define __for_each_thread(signal, t)\t\\\n\tlist_for_each_entry_rcu(t, &(signal)->thread_head, thread_node)\n\n#define for_each_thread(p, t)\t\t\\\n\t__for_each_thread((p)->signal, t)\n\n \n#define for_each_process_thread(p, t)\t\\\n\tfor_each_process(p) for_each_thread(p, t)\n\ntypedef int (*proc_visitor)(struct task_struct *p, void *data);\nvoid walk_process_tree(struct task_struct *top, proc_visitor, void *);\n\nstatic inline\nstruct pid *task_pid_type(struct task_struct *task, enum pid_type type)\n{\n\tstruct pid *pid;\n\tif (type == PIDTYPE_PID)\n\t\tpid = task_pid(task);\n\telse\n\t\tpid = task->signal->pids[type];\n\treturn pid;\n}\n\nstatic inline struct pid *task_tgid(struct task_struct *task)\n{\n\treturn task->signal->pids[PIDTYPE_TGID];\n}\n\n \nstatic inline struct pid *task_pgrp(struct task_struct *task)\n{\n\treturn task->signal->pids[PIDTYPE_PGID];\n}\n\nstatic inline struct pid *task_session(struct task_struct *task)\n{\n\treturn task->signal->pids[PIDTYPE_SID];\n}\n\nstatic inline int get_nr_threads(struct task_struct *task)\n{\n\treturn task->signal->nr_threads;\n}\n\nstatic inline bool thread_group_leader(struct task_struct *p)\n{\n\treturn p->exit_signal >= 0;\n}\n\nstatic inline\nbool same_thread_group(struct task_struct *p1, struct task_struct *p2)\n{\n\treturn p1->signal == p2->signal;\n}\n\nstatic inline struct task_struct *next_thread(const struct task_struct *p)\n{\n\treturn list_entry_rcu(p->thread_group.next,\n\t\t\t      struct task_struct, thread_group);\n}\n\nstatic inline int thread_group_empty(struct task_struct *p)\n{\n\treturn list_empty(&p->thread_group);\n}\n\n#define delay_group_leader(p) \\\n\t\t(thread_group_leader(p) && !thread_group_empty(p))\n\nextern bool thread_group_exited(struct pid *pid);\n\nextern struct sighand_struct *__lock_task_sighand(struct task_struct *task,\n\t\t\t\t\t\t\tunsigned long *flags);\n\nstatic inline struct sighand_struct *lock_task_sighand(struct task_struct *task,\n\t\t\t\t\t\t       unsigned long *flags)\n{\n\tstruct sighand_struct *ret;\n\n\tret = __lock_task_sighand(task, flags);\n\t(void)__cond_lock(&task->sighand->siglock, ret);\n\treturn ret;\n}\n\nstatic inline void unlock_task_sighand(struct task_struct *task,\n\t\t\t\t\t\tunsigned long *flags)\n{\n\tspin_unlock_irqrestore(&task->sighand->siglock, *flags);\n}\n\n#ifdef CONFIG_LOCKDEP\nextern void lockdep_assert_task_sighand_held(struct task_struct *task);\n#else\nstatic inline void lockdep_assert_task_sighand_held(struct task_struct *task) { }\n#endif\n\nstatic inline unsigned long task_rlimit(const struct task_struct *task,\n\t\tunsigned int limit)\n{\n\treturn READ_ONCE(task->signal->rlim[limit].rlim_cur);\n}\n\nstatic inline unsigned long task_rlimit_max(const struct task_struct *task,\n\t\tunsigned int limit)\n{\n\treturn READ_ONCE(task->signal->rlim[limit].rlim_max);\n}\n\nstatic inline unsigned long rlimit(unsigned int limit)\n{\n\treturn task_rlimit(current, limit);\n}\n\nstatic inline unsigned long rlimit_max(unsigned int limit)\n{\n\treturn task_rlimit_max(current, limit);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}