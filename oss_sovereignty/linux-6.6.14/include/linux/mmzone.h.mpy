{
  "module_name": "mmzone.h",
  "hash_id": "1dd8e1872ce763f84a90c546b21c65dde883286985c778d4033b7d09f10eabd4",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/mmzone.h",
  "human_readable_source": " \n#ifndef _LINUX_MMZONE_H\n#define _LINUX_MMZONE_H\n\n#ifndef __ASSEMBLY__\n#ifndef __GENERATING_BOUNDS_H\n\n#include <linux/spinlock.h>\n#include <linux/list.h>\n#include <linux/list_nulls.h>\n#include <linux/wait.h>\n#include <linux/bitops.h>\n#include <linux/cache.h>\n#include <linux/threads.h>\n#include <linux/numa.h>\n#include <linux/init.h>\n#include <linux/seqlock.h>\n#include <linux/nodemask.h>\n#include <linux/pageblock-flags.h>\n#include <linux/page-flags-layout.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n#include <linux/page-flags.h>\n#include <linux/local_lock.h>\n#include <asm/page.h>\n\n \n#ifndef CONFIG_ARCH_FORCE_MAX_ORDER\n#define MAX_ORDER 10\n#else\n#define MAX_ORDER CONFIG_ARCH_FORCE_MAX_ORDER\n#endif\n#define MAX_ORDER_NR_PAGES (1 << MAX_ORDER)\n\n#define IS_MAX_ORDER_ALIGNED(pfn) IS_ALIGNED(pfn, MAX_ORDER_NR_PAGES)\n\n \n#define PAGE_ALLOC_COSTLY_ORDER 3\n\nenum migratetype {\n\tMIGRATE_UNMOVABLE,\n\tMIGRATE_MOVABLE,\n\tMIGRATE_RECLAIMABLE,\n\tMIGRATE_PCPTYPES,\t \n\tMIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,\n#ifdef CONFIG_CMA\n\t \n\tMIGRATE_CMA,\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n\tMIGRATE_ISOLATE,\t \n#endif\n\tMIGRATE_TYPES\n};\n\n \nextern const char * const migratetype_names[MIGRATE_TYPES];\n\n#ifdef CONFIG_CMA\n#  define is_migrate_cma(migratetype) unlikely((migratetype) == MIGRATE_CMA)\n#  define is_migrate_cma_page(_page) (get_pageblock_migratetype(_page) == MIGRATE_CMA)\n#else\n#  define is_migrate_cma(migratetype) false\n#  define is_migrate_cma_page(_page) false\n#endif\n\nstatic inline bool is_migrate_movable(int mt)\n{\n\treturn is_migrate_cma(mt) || mt == MIGRATE_MOVABLE;\n}\n\n \nstatic inline bool migratetype_is_mergeable(int mt)\n{\n\treturn mt < MIGRATE_PCPTYPES;\n}\n\n#define for_each_migratetype_order(order, type) \\\n\tfor (order = 0; order <= MAX_ORDER; order++) \\\n\t\tfor (type = 0; type < MIGRATE_TYPES; type++)\n\nextern int page_group_by_mobility_disabled;\n\n#define MIGRATETYPE_MASK ((1UL << PB_migratetype_bits) - 1)\n\n#define get_pageblock_migratetype(page)\t\t\t\t\t\\\n\tget_pfnblock_flags_mask(page, page_to_pfn(page), MIGRATETYPE_MASK)\n\n#define folio_migratetype(folio)\t\t\t\t\\\n\tget_pfnblock_flags_mask(&folio->page, folio_pfn(folio),\t\t\\\n\t\t\tMIGRATETYPE_MASK)\nstruct free_area {\n\tstruct list_head\tfree_list[MIGRATE_TYPES];\n\tunsigned long\t\tnr_free;\n};\n\nstruct pglist_data;\n\n#ifdef CONFIG_NUMA\nenum numa_stat_item {\n\tNUMA_HIT,\t\t \n\tNUMA_MISS,\t\t \n\tNUMA_FOREIGN,\t\t \n\tNUMA_INTERLEAVE_HIT,\t \n\tNUMA_LOCAL,\t\t \n\tNUMA_OTHER,\t\t \n\tNR_VM_NUMA_EVENT_ITEMS\n};\n#else\n#define NR_VM_NUMA_EVENT_ITEMS 0\n#endif\n\nenum zone_stat_item {\n\t \n\tNR_FREE_PAGES,\n\tNR_ZONE_LRU_BASE,  \n\tNR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE,\n\tNR_ZONE_ACTIVE_ANON,\n\tNR_ZONE_INACTIVE_FILE,\n\tNR_ZONE_ACTIVE_FILE,\n\tNR_ZONE_UNEVICTABLE,\n\tNR_ZONE_WRITE_PENDING,\t \n\tNR_MLOCK,\t\t \n\t \n\tNR_BOUNCE,\n#if IS_ENABLED(CONFIG_ZSMALLOC)\n\tNR_ZSPAGES,\t\t \n#endif\n\tNR_FREE_CMA_PAGES,\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\tNR_UNACCEPTED,\n#endif\n\tNR_VM_ZONE_STAT_ITEMS };\n\nenum node_stat_item {\n\tNR_LRU_BASE,\n\tNR_INACTIVE_ANON = NR_LRU_BASE,  \n\tNR_ACTIVE_ANON,\t\t \n\tNR_INACTIVE_FILE,\t \n\tNR_ACTIVE_FILE,\t\t \n\tNR_UNEVICTABLE,\t\t \n\tNR_SLAB_RECLAIMABLE_B,\n\tNR_SLAB_UNRECLAIMABLE_B,\n\tNR_ISOLATED_ANON,\t \n\tNR_ISOLATED_FILE,\t \n\tWORKINGSET_NODES,\n\tWORKINGSET_REFAULT_BASE,\n\tWORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE,\n\tWORKINGSET_REFAULT_FILE,\n\tWORKINGSET_ACTIVATE_BASE,\n\tWORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE,\n\tWORKINGSET_ACTIVATE_FILE,\n\tWORKINGSET_RESTORE_BASE,\n\tWORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE,\n\tWORKINGSET_RESTORE_FILE,\n\tWORKINGSET_NODERECLAIM,\n\tNR_ANON_MAPPED,\t \n\tNR_FILE_MAPPED,\t \n\tNR_FILE_PAGES,\n\tNR_FILE_DIRTY,\n\tNR_WRITEBACK,\n\tNR_WRITEBACK_TEMP,\t \n\tNR_SHMEM,\t\t \n\tNR_SHMEM_THPS,\n\tNR_SHMEM_PMDMAPPED,\n\tNR_FILE_THPS,\n\tNR_FILE_PMDMAPPED,\n\tNR_ANON_THPS,\n\tNR_VMSCAN_WRITE,\n\tNR_VMSCAN_IMMEDIATE,\t \n\tNR_DIRTIED,\t\t \n\tNR_WRITTEN,\t\t \n\tNR_THROTTLED_WRITTEN,\t \n\tNR_KERNEL_MISC_RECLAIMABLE,\t \n\tNR_FOLL_PIN_ACQUIRED,\t \n\tNR_FOLL_PIN_RELEASED,\t \n\tNR_KERNEL_STACK_KB,\t \n#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK)\n\tNR_KERNEL_SCS_KB,\t \n#endif\n\tNR_PAGETABLE,\t\t \n\tNR_SECONDARY_PAGETABLE,  \n#ifdef CONFIG_SWAP\n\tNR_SWAPCACHE,\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tPGPROMOTE_SUCCESS,\t \n\tPGPROMOTE_CANDIDATE,\t \n#endif\n\tNR_VM_NODE_STAT_ITEMS\n};\n\n \nstatic __always_inline bool vmstat_item_print_in_thp(enum node_stat_item item)\n{\n\tif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))\n\t\treturn false;\n\n\treturn item == NR_ANON_THPS ||\n\t       item == NR_FILE_THPS ||\n\t       item == NR_SHMEM_THPS ||\n\t       item == NR_SHMEM_PMDMAPPED ||\n\t       item == NR_FILE_PMDMAPPED;\n}\n\n \nstatic __always_inline bool vmstat_item_in_bytes(int idx)\n{\n\t \n\treturn (idx == NR_SLAB_RECLAIMABLE_B ||\n\t\tidx == NR_SLAB_UNRECLAIMABLE_B);\n}\n\n \n#define LRU_BASE 0\n#define LRU_ACTIVE 1\n#define LRU_FILE 2\n\nenum lru_list {\n\tLRU_INACTIVE_ANON = LRU_BASE,\n\tLRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,\n\tLRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,\n\tLRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,\n\tLRU_UNEVICTABLE,\n\tNR_LRU_LISTS\n};\n\nenum vmscan_throttle_state {\n\tVMSCAN_THROTTLE_WRITEBACK,\n\tVMSCAN_THROTTLE_ISOLATED,\n\tVMSCAN_THROTTLE_NOPROGRESS,\n\tVMSCAN_THROTTLE_CONGESTED,\n\tNR_VMSCAN_THROTTLE,\n};\n\n#define for_each_lru(lru) for (lru = 0; lru < NR_LRU_LISTS; lru++)\n\n#define for_each_evictable_lru(lru) for (lru = 0; lru <= LRU_ACTIVE_FILE; lru++)\n\nstatic inline bool is_file_lru(enum lru_list lru)\n{\n\treturn (lru == LRU_INACTIVE_FILE || lru == LRU_ACTIVE_FILE);\n}\n\nstatic inline bool is_active_lru(enum lru_list lru)\n{\n\treturn (lru == LRU_ACTIVE_ANON || lru == LRU_ACTIVE_FILE);\n}\n\n#define WORKINGSET_ANON 0\n#define WORKINGSET_FILE 1\n#define ANON_AND_FILE 2\n\nenum lruvec_flags {\n\t \n\tLRUVEC_CGROUP_CONGESTED,\n\tLRUVEC_NODE_CONGESTED,\n};\n\n#endif  \n\n \n#define MIN_NR_GENS\t\t2U\n#define MAX_NR_GENS\t\t4U\n\n \n#define MAX_NR_TIERS\t\t4U\n\n#ifndef __GENERATING_BOUNDS_H\n\nstruct lruvec;\nstruct page_vma_mapped_walk;\n\n#define LRU_GEN_MASK\t\t((BIT(LRU_GEN_WIDTH) - 1) << LRU_GEN_PGOFF)\n#define LRU_REFS_MASK\t\t((BIT(LRU_REFS_WIDTH) - 1) << LRU_REFS_PGOFF)\n\n#ifdef CONFIG_LRU_GEN\n\nenum {\n\tLRU_GEN_ANON,\n\tLRU_GEN_FILE,\n};\n\nenum {\n\tLRU_GEN_CORE,\n\tLRU_GEN_MM_WALK,\n\tLRU_GEN_NONLEAF_YOUNG,\n\tNR_LRU_GEN_CAPS\n};\n\n#define MIN_LRU_BATCH\t\tBITS_PER_LONG\n#define MAX_LRU_BATCH\t\t(MIN_LRU_BATCH * 64)\n\n \n#ifdef CONFIG_LRU_GEN_STATS\n#define NR_HIST_GENS\t\tMAX_NR_GENS\n#else\n#define NR_HIST_GENS\t\t1U\n#endif\n\n \nstruct lru_gen_folio {\n\t \n\tunsigned long max_seq;\n\t \n\tunsigned long min_seq[ANON_AND_FILE];\n\t \n\tunsigned long timestamps[MAX_NR_GENS];\n\t \n\tstruct list_head folios[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n\t \n\tlong nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n\t \n\tunsigned long avg_refaulted[ANON_AND_FILE][MAX_NR_TIERS];\n\t \n\tunsigned long avg_total[ANON_AND_FILE][MAX_NR_TIERS];\n\t \n\tunsigned long protected[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS - 1];\n\t \n\tatomic_long_t evicted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];\n\tatomic_long_t refaulted[NR_HIST_GENS][ANON_AND_FILE][MAX_NR_TIERS];\n\t \n\tbool enabled;\n#ifdef CONFIG_MEMCG\n\t \n\tu8 gen;\n\t \n\tu8 seg;\n\t \n\tstruct hlist_nulls_node list;\n#endif\n};\n\nenum {\n\tMM_LEAF_TOTAL,\t\t \n\tMM_LEAF_OLD,\t\t \n\tMM_LEAF_YOUNG,\t\t \n\tMM_NONLEAF_TOTAL,\t \n\tMM_NONLEAF_FOUND,\t \n\tMM_NONLEAF_ADDED,\t \n\tNR_MM_STATS\n};\n\n \n#define NR_BLOOM_FILTERS\t2\n\nstruct lru_gen_mm_state {\n\t \n\tunsigned long seq;\n\t \n\tstruct list_head *head;\n\t \n\tstruct list_head *tail;\n\t \n\tunsigned long *filters[NR_BLOOM_FILTERS];\n\t \n\tunsigned long stats[NR_HIST_GENS][NR_MM_STATS];\n};\n\nstruct lru_gen_mm_walk {\n\t \n\tstruct lruvec *lruvec;\n\t \n\tunsigned long max_seq;\n\t \n\tunsigned long next_addr;\n\t \n\tint nr_pages[MAX_NR_GENS][ANON_AND_FILE][MAX_NR_ZONES];\n\t \n\tint mm_stats[NR_MM_STATS];\n\t \n\tint batched;\n\tbool can_swap;\n\tbool force_scan;\n};\n\nvoid lru_gen_init_lruvec(struct lruvec *lruvec);\nvoid lru_gen_look_around(struct page_vma_mapped_walk *pvmw);\n\n#ifdef CONFIG_MEMCG\n\n \n#define MEMCG_NR_GENS\t3\n#define MEMCG_NR_BINS\t8\n\nstruct lru_gen_memcg {\n\t \n\tunsigned long seq;\n\t \n\tunsigned long nr_memcgs[MEMCG_NR_GENS];\n\t \n\tstruct hlist_nulls_head\tfifo[MEMCG_NR_GENS][MEMCG_NR_BINS];\n\t \n\tspinlock_t lock;\n};\n\nvoid lru_gen_init_pgdat(struct pglist_data *pgdat);\n\nvoid lru_gen_init_memcg(struct mem_cgroup *memcg);\nvoid lru_gen_exit_memcg(struct mem_cgroup *memcg);\nvoid lru_gen_online_memcg(struct mem_cgroup *memcg);\nvoid lru_gen_offline_memcg(struct mem_cgroup *memcg);\nvoid lru_gen_release_memcg(struct mem_cgroup *memcg);\nvoid lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid);\n\n#else  \n\n#define MEMCG_NR_GENS\t1\n\nstruct lru_gen_memcg {\n};\n\nstatic inline void lru_gen_init_pgdat(struct pglist_data *pgdat)\n{\n}\n\n#endif  \n\n#else  \n\nstatic inline void lru_gen_init_pgdat(struct pglist_data *pgdat)\n{\n}\n\nstatic inline void lru_gen_init_lruvec(struct lruvec *lruvec)\n{\n}\n\nstatic inline void lru_gen_look_around(struct page_vma_mapped_walk *pvmw)\n{\n}\n\n#ifdef CONFIG_MEMCG\n\nstatic inline void lru_gen_init_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void lru_gen_exit_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void lru_gen_online_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void lru_gen_offline_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void lru_gen_release_memcg(struct mem_cgroup *memcg)\n{\n}\n\nstatic inline void lru_gen_soft_reclaim(struct mem_cgroup *memcg, int nid)\n{\n}\n\n#endif  \n\n#endif  \n\nstruct lruvec {\n\tstruct list_head\t\tlists[NR_LRU_LISTS];\n\t \n\tspinlock_t\t\t\tlru_lock;\n\t \n\tunsigned long\t\t\tanon_cost;\n\tunsigned long\t\t\tfile_cost;\n\t \n\tatomic_long_t\t\t\tnonresident_age;\n\t \n\tunsigned long\t\t\trefaults[ANON_AND_FILE];\n\t \n\tunsigned long\t\t\tflags;\n#ifdef CONFIG_LRU_GEN\n\t \n\tstruct lru_gen_folio\t\tlrugen;\n\t \n\tstruct lru_gen_mm_state\t\tmm_state;\n#endif\n#ifdef CONFIG_MEMCG\n\tstruct pglist_data *pgdat;\n#endif\n};\n\n \n#define ISOLATE_UNMAPPED\t((__force isolate_mode_t)0x2)\n \n#define ISOLATE_ASYNC_MIGRATE\t((__force isolate_mode_t)0x4)\n \n#define ISOLATE_UNEVICTABLE\t((__force isolate_mode_t)0x8)\n\n \ntypedef unsigned __bitwise isolate_mode_t;\n\nenum zone_watermarks {\n\tWMARK_MIN,\n\tWMARK_LOW,\n\tWMARK_HIGH,\n\tWMARK_PROMO,\n\tNR_WMARK\n};\n\n \n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n#define NR_PCP_THP 1\n#else\n#define NR_PCP_THP 0\n#endif\n#define NR_LOWORDER_PCP_LISTS (MIGRATE_PCPTYPES * (PAGE_ALLOC_COSTLY_ORDER + 1))\n#define NR_PCP_LISTS (NR_LOWORDER_PCP_LISTS + NR_PCP_THP)\n\n#define min_wmark_pages(z) (z->_watermark[WMARK_MIN] + z->watermark_boost)\n#define low_wmark_pages(z) (z->_watermark[WMARK_LOW] + z->watermark_boost)\n#define high_wmark_pages(z) (z->_watermark[WMARK_HIGH] + z->watermark_boost)\n#define wmark_pages(z, i) (z->_watermark[i] + z->watermark_boost)\n\nstruct per_cpu_pages {\n\tspinlock_t lock;\t \n\tint count;\t\t \n\tint high;\t\t \n\tint batch;\t\t \n\tshort free_factor;\t \n#ifdef CONFIG_NUMA\n\tshort expire;\t\t \n#endif\n\n\t \n\tstruct list_head lists[NR_PCP_LISTS];\n} ____cacheline_aligned_in_smp;\n\nstruct per_cpu_zonestat {\n#ifdef CONFIG_SMP\n\ts8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];\n\ts8 stat_threshold;\n#endif\n#ifdef CONFIG_NUMA\n\t \n\tunsigned long vm_numa_event[NR_VM_NUMA_EVENT_ITEMS];\n#endif\n};\n\nstruct per_cpu_nodestat {\n\ts8 stat_threshold;\n\ts8 vm_node_stat_diff[NR_VM_NODE_STAT_ITEMS];\n};\n\n#endif  \n\nenum zone_type {\n\t \n#ifdef CONFIG_ZONE_DMA\n\tZONE_DMA,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\tZONE_DMA32,\n#endif\n\t \n\tZONE_NORMAL,\n#ifdef CONFIG_HIGHMEM\n\t \n\tZONE_HIGHMEM,\n#endif\n\t \n\tZONE_MOVABLE,\n#ifdef CONFIG_ZONE_DEVICE\n\tZONE_DEVICE,\n#endif\n\t__MAX_NR_ZONES\n\n};\n\n#ifndef __GENERATING_BOUNDS_H\n\n#define ASYNC_AND_SYNC 2\n\nstruct zone {\n\t \n\n\t \n\tunsigned long _watermark[NR_WMARK];\n\tunsigned long watermark_boost;\n\n\tunsigned long nr_reserved_highatomic;\n\n\t \n\tlong lowmem_reserve[MAX_NR_ZONES];\n\n#ifdef CONFIG_NUMA\n\tint node;\n#endif\n\tstruct pglist_data\t*zone_pgdat;\n\tstruct per_cpu_pages\t__percpu *per_cpu_pageset;\n\tstruct per_cpu_zonestat\t__percpu *per_cpu_zonestats;\n\t \n\tint pageset_high;\n\tint pageset_batch;\n\n#ifndef CONFIG_SPARSEMEM\n\t \n\tunsigned long\t\t*pageblock_flags;\n#endif  \n\n\t \n\tunsigned long\t\tzone_start_pfn;\n\n\t \n\tatomic_long_t\t\tmanaged_pages;\n\tunsigned long\t\tspanned_pages;\n\tunsigned long\t\tpresent_pages;\n#if defined(CONFIG_MEMORY_HOTPLUG)\n\tunsigned long\t\tpresent_early_pages;\n#endif\n#ifdef CONFIG_CMA\n\tunsigned long\t\tcma_pages;\n#endif\n\n\tconst char\t\t*name;\n\n#ifdef CONFIG_MEMORY_ISOLATION\n\t \n\tunsigned long\t\tnr_isolate_pageblock;\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\t \n\tseqlock_t\t\tspan_seqlock;\n#endif\n\n\tint initialized;\n\n\t \n\tCACHELINE_PADDING(_pad1_);\n\n\t \n\tstruct free_area\tfree_area[MAX_ORDER + 1];\n\n#ifdef CONFIG_UNACCEPTED_MEMORY\n\t \n\tstruct list_head\tunaccepted_pages;\n#endif\n\n\t \n\tunsigned long\t\tflags;\n\n\t \n\tspinlock_t\t\tlock;\n\n\t \n\tCACHELINE_PADDING(_pad2_);\n\n\t \n\tunsigned long percpu_drift_mark;\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\t \n\tunsigned long\t\tcompact_cached_free_pfn;\n\t \n\tunsigned long\t\tcompact_cached_migrate_pfn[ASYNC_AND_SYNC];\n\tunsigned long\t\tcompact_init_migrate_pfn;\n\tunsigned long\t\tcompact_init_free_pfn;\n#endif\n\n#ifdef CONFIG_COMPACTION\n\t \n\tunsigned int\t\tcompact_considered;\n\tunsigned int\t\tcompact_defer_shift;\n\tint\t\t\tcompact_order_failed;\n#endif\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n\t \n\tbool\t\t\tcompact_blockskip_flush;\n#endif\n\n\tbool\t\t\tcontiguous;\n\n\tCACHELINE_PADDING(_pad3_);\n\t \n\tatomic_long_t\t\tvm_stat[NR_VM_ZONE_STAT_ITEMS];\n\tatomic_long_t\t\tvm_numa_event[NR_VM_NUMA_EVENT_ITEMS];\n} ____cacheline_internodealigned_in_smp;\n\nenum pgdat_flags {\n\tPGDAT_DIRTY,\t\t\t \n\tPGDAT_WRITEBACK,\t\t \n\tPGDAT_RECLAIM_LOCKED,\t\t \n};\n\nenum zone_flags {\n\tZONE_BOOSTED_WATERMARK,\t\t \n\tZONE_RECLAIM_ACTIVE,\t\t \n};\n\nstatic inline unsigned long zone_managed_pages(struct zone *zone)\n{\n\treturn (unsigned long)atomic_long_read(&zone->managed_pages);\n}\n\nstatic inline unsigned long zone_cma_pages(struct zone *zone)\n{\n#ifdef CONFIG_CMA\n\treturn zone->cma_pages;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline unsigned long zone_end_pfn(const struct zone *zone)\n{\n\treturn zone->zone_start_pfn + zone->spanned_pages;\n}\n\nstatic inline bool zone_spans_pfn(const struct zone *zone, unsigned long pfn)\n{\n\treturn zone->zone_start_pfn <= pfn && pfn < zone_end_pfn(zone);\n}\n\nstatic inline bool zone_is_initialized(struct zone *zone)\n{\n\treturn zone->initialized;\n}\n\nstatic inline bool zone_is_empty(struct zone *zone)\n{\n\treturn zone->spanned_pages == 0;\n}\n\n#ifndef BUILD_VDSO32_64\n \n\n \n#define SECTIONS_PGOFF\t\t((sizeof(unsigned long)*8) - SECTIONS_WIDTH)\n#define NODES_PGOFF\t\t(SECTIONS_PGOFF - NODES_WIDTH)\n#define ZONES_PGOFF\t\t(NODES_PGOFF - ZONES_WIDTH)\n#define LAST_CPUPID_PGOFF\t(ZONES_PGOFF - LAST_CPUPID_WIDTH)\n#define KASAN_TAG_PGOFF\t\t(LAST_CPUPID_PGOFF - KASAN_TAG_WIDTH)\n#define LRU_GEN_PGOFF\t\t(KASAN_TAG_PGOFF - LRU_GEN_WIDTH)\n#define LRU_REFS_PGOFF\t\t(LRU_GEN_PGOFF - LRU_REFS_WIDTH)\n\n \n#define SECTIONS_PGSHIFT\t(SECTIONS_PGOFF * (SECTIONS_WIDTH != 0))\n#define NODES_PGSHIFT\t\t(NODES_PGOFF * (NODES_WIDTH != 0))\n#define ZONES_PGSHIFT\t\t(ZONES_PGOFF * (ZONES_WIDTH != 0))\n#define LAST_CPUPID_PGSHIFT\t(LAST_CPUPID_PGOFF * (LAST_CPUPID_WIDTH != 0))\n#define KASAN_TAG_PGSHIFT\t(KASAN_TAG_PGOFF * (KASAN_TAG_WIDTH != 0))\n\n \n#ifdef NODE_NOT_IN_PAGE_FLAGS\n#define ZONEID_SHIFT\t\t(SECTIONS_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((SECTIONS_PGOFF < ZONES_PGOFF) ? \\\n\t\t\t\t\t\tSECTIONS_PGOFF : ZONES_PGOFF)\n#else\n#define ZONEID_SHIFT\t\t(NODES_SHIFT + ZONES_SHIFT)\n#define ZONEID_PGOFF\t\t((NODES_PGOFF < ZONES_PGOFF) ? \\\n\t\t\t\t\t\tNODES_PGOFF : ZONES_PGOFF)\n#endif\n\n#define ZONEID_PGSHIFT\t\t(ZONEID_PGOFF * (ZONEID_SHIFT != 0))\n\n#define ZONES_MASK\t\t((1UL << ZONES_WIDTH) - 1)\n#define NODES_MASK\t\t((1UL << NODES_WIDTH) - 1)\n#define SECTIONS_MASK\t\t((1UL << SECTIONS_WIDTH) - 1)\n#define LAST_CPUPID_MASK\t((1UL << LAST_CPUPID_SHIFT) - 1)\n#define KASAN_TAG_MASK\t\t((1UL << KASAN_TAG_WIDTH) - 1)\n#define ZONEID_MASK\t\t((1UL << ZONEID_SHIFT) - 1)\n\nstatic inline enum zone_type page_zonenum(const struct page *page)\n{\n\tASSERT_EXCLUSIVE_BITS(page->flags, ZONES_MASK << ZONES_PGSHIFT);\n\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;\n}\n\nstatic inline enum zone_type folio_zonenum(const struct folio *folio)\n{\n\treturn page_zonenum(&folio->page);\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn page_zonenum(page) == ZONE_DEVICE;\n}\n\n \nstatic inline bool zone_device_pages_have_same_pgmap(const struct page *a,\n\t\t\t\t\t\t     const struct page *b)\n{\n\tif (is_zone_device_page(a) != is_zone_device_page(b))\n\t\treturn false;\n\tif (!is_zone_device_page(a))\n\t\treturn true;\n\treturn a->pgmap == b->pgmap;\n}\n\nextern void memmap_init_zone_device(struct zone *, unsigned long,\n\t\t\t\t    unsigned long, struct dev_pagemap *);\n#else\nstatic inline bool is_zone_device_page(const struct page *page)\n{\n\treturn false;\n}\nstatic inline bool zone_device_pages_have_same_pgmap(const struct page *a,\n\t\t\t\t\t\t     const struct page *b)\n{\n\treturn true;\n}\n#endif\n\nstatic inline bool folio_is_zone_device(const struct folio *folio)\n{\n\treturn is_zone_device_page(&folio->page);\n}\n\nstatic inline bool is_zone_movable_page(const struct page *page)\n{\n\treturn page_zonenum(page) == ZONE_MOVABLE;\n}\n\nstatic inline bool folio_is_zone_movable(const struct folio *folio)\n{\n\treturn folio_zonenum(folio) == ZONE_MOVABLE;\n}\n#endif\n\n \nstatic inline bool zone_intersects(struct zone *zone,\n\t\tunsigned long start_pfn, unsigned long nr_pages)\n{\n\tif (zone_is_empty(zone))\n\t\treturn false;\n\tif (start_pfn >= zone_end_pfn(zone) ||\n\t    start_pfn + nr_pages <= zone->zone_start_pfn)\n\t\treturn false;\n\n\treturn true;\n}\n\n \n#define DEF_PRIORITY 12\n\n \n#define MAX_ZONES_PER_ZONELIST (MAX_NUMNODES * MAX_NR_ZONES)\n\nenum {\n\tZONELIST_FALLBACK,\t \n#ifdef CONFIG_NUMA\n\t \n\tZONELIST_NOFALLBACK,\t \n#endif\n\tMAX_ZONELISTS\n};\n\n \nstruct zoneref {\n\tstruct zone *zone;\t \n\tint zone_idx;\t\t \n};\n\n \nstruct zonelist {\n\tstruct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];\n};\n\n \nextern struct page *mem_map;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstruct deferred_split {\n\tspinlock_t split_queue_lock;\n\tstruct list_head split_queue;\n\tunsigned long split_queue_len;\n};\n#endif\n\n#ifdef CONFIG_MEMORY_FAILURE\n \nstruct memory_failure_stats {\n\t \n\tunsigned long total;\n\t \n\tunsigned long ignored;\n\tunsigned long failed;\n\tunsigned long delayed;\n\tunsigned long recovered;\n};\n#endif\n\n \ntypedef struct pglist_data {\n\t \n\tstruct zone node_zones[MAX_NR_ZONES];\n\n\t \n\tstruct zonelist node_zonelists[MAX_ZONELISTS];\n\n\tint nr_zones;  \n#ifdef CONFIG_FLATMEM\t \n\tstruct page *node_mem_map;\n#ifdef CONFIG_PAGE_EXTENSION\n\tstruct page_ext *node_page_ext;\n#endif\n#endif\n#if defined(CONFIG_MEMORY_HOTPLUG) || defined(CONFIG_DEFERRED_STRUCT_PAGE_INIT)\n\t \n\tspinlock_t node_size_lock;\n#endif\n\tunsigned long node_start_pfn;\n\tunsigned long node_present_pages;  \n\tunsigned long node_spanned_pages;  \n\tint node_id;\n\twait_queue_head_t kswapd_wait;\n\twait_queue_head_t pfmemalloc_wait;\n\n\t \n\twait_queue_head_t reclaim_wait[NR_VMSCAN_THROTTLE];\n\n\tatomic_t nr_writeback_throttled; \n\tunsigned long nr_reclaim_start;\t \n#ifdef CONFIG_MEMORY_HOTPLUG\n\tstruct mutex kswapd_lock;\n#endif\n\tstruct task_struct *kswapd;\t \n\tint kswapd_order;\n\tenum zone_type kswapd_highest_zoneidx;\n\n\tint kswapd_failures;\t\t \n\n#ifdef CONFIG_COMPACTION\n\tint kcompactd_max_order;\n\tenum zone_type kcompactd_highest_zoneidx;\n\twait_queue_head_t kcompactd_wait;\n\tstruct task_struct *kcompactd;\n\tbool proactive_compact_trigger;\n#endif\n\t \n\tunsigned long\t\ttotalreserve_pages;\n\n#ifdef CONFIG_NUMA\n\t \n\tunsigned long\t\tmin_unmapped_pages;\n\tunsigned long\t\tmin_slab_pages;\n#endif  \n\n\t \n\tCACHELINE_PADDING(_pad1_);\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n\t \n\tunsigned long first_deferred_pfn;\n#endif  \n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tstruct deferred_split deferred_split_queue;\n#endif\n\n#ifdef CONFIG_NUMA_BALANCING\n\t \n\tunsigned int nbp_rl_start;\n\t \n\tunsigned long nbp_rl_nr_cand;\n\t \n\tunsigned int nbp_threshold;\n\t \n\tunsigned int nbp_th_start;\n\t \n\tunsigned long nbp_th_nr_cand;\n#endif\n\t \n\n\t \n\tstruct lruvec\t\t__lruvec;\n\n\tunsigned long\t\tflags;\n\n#ifdef CONFIG_LRU_GEN\n\t \n\tstruct lru_gen_mm_walk mm_walk;\n\t \n\tstruct lru_gen_memcg memcg_lru;\n#endif\n\n\tCACHELINE_PADDING(_pad2_);\n\n\t \n\tstruct per_cpu_nodestat __percpu *per_cpu_nodestats;\n\tatomic_long_t\t\tvm_stat[NR_VM_NODE_STAT_ITEMS];\n#ifdef CONFIG_NUMA\n\tstruct memory_tier __rcu *memtier;\n#endif\n#ifdef CONFIG_MEMORY_FAILURE\n\tstruct memory_failure_stats mf_stats;\n#endif\n} pg_data_t;\n\n#define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n#define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n\n#define node_start_pfn(nid)\t(NODE_DATA(nid)->node_start_pfn)\n#define node_end_pfn(nid) pgdat_end_pfn(NODE_DATA(nid))\n\nstatic inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)\n{\n\treturn pgdat->node_start_pfn + pgdat->node_spanned_pages;\n}\n\n#include <linux/memory_hotplug.h>\n\nvoid build_all_zonelists(pg_data_t *pgdat);\nvoid wakeup_kswapd(struct zone *zone, gfp_t gfp_mask, int order,\n\t\t   enum zone_type highest_zoneidx);\nbool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,\n\t\t\t int highest_zoneidx, unsigned int alloc_flags,\n\t\t\t long free_pages);\nbool zone_watermark_ok(struct zone *z, unsigned int order,\n\t\tunsigned long mark, int highest_zoneidx,\n\t\tunsigned int alloc_flags);\nbool zone_watermark_ok_safe(struct zone *z, unsigned int order,\n\t\tunsigned long mark, int highest_zoneidx);\n \nenum meminit_context {\n\tMEMINIT_EARLY,\n\tMEMINIT_HOTPLUG,\n};\n\nextern void init_currently_empty_zone(struct zone *zone, unsigned long start_pfn,\n\t\t\t\t     unsigned long size);\n\nextern void lruvec_init(struct lruvec *lruvec);\n\nstatic inline struct pglist_data *lruvec_pgdat(struct lruvec *lruvec)\n{\n#ifdef CONFIG_MEMCG\n\treturn lruvec->pgdat;\n#else\n\treturn container_of(lruvec, struct pglist_data, __lruvec);\n#endif\n}\n\n#ifdef CONFIG_HAVE_MEMORYLESS_NODES\nint local_memory_node(int node_id);\n#else\nstatic inline int local_memory_node(int node_id) { return node_id; };\n#endif\n\n \n#define zone_idx(zone)\t\t((zone) - (zone)->zone_pgdat->node_zones)\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic inline bool zone_is_zone_device(struct zone *zone)\n{\n\treturn zone_idx(zone) == ZONE_DEVICE;\n}\n#else\nstatic inline bool zone_is_zone_device(struct zone *zone)\n{\n\treturn false;\n}\n#endif\n\n \nstatic inline bool managed_zone(struct zone *zone)\n{\n\treturn zone_managed_pages(zone);\n}\n\n \nstatic inline bool populated_zone(struct zone *zone)\n{\n\treturn zone->present_pages;\n}\n\n#ifdef CONFIG_NUMA\nstatic inline int zone_to_nid(struct zone *zone)\n{\n\treturn zone->node;\n}\n\nstatic inline void zone_set_nid(struct zone *zone, int nid)\n{\n\tzone->node = nid;\n}\n#else\nstatic inline int zone_to_nid(struct zone *zone)\n{\n\treturn 0;\n}\n\nstatic inline void zone_set_nid(struct zone *zone, int nid) {}\n#endif\n\nextern int movable_zone;\n\nstatic inline int is_highmem_idx(enum zone_type idx)\n{\n#ifdef CONFIG_HIGHMEM\n\treturn (idx == ZONE_HIGHMEM ||\n\t\t(idx == ZONE_MOVABLE && movable_zone == ZONE_HIGHMEM));\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic inline int is_highmem(struct zone *zone)\n{\n\treturn is_highmem_idx(zone_idx(zone));\n}\n\n#ifdef CONFIG_ZONE_DMA\nbool has_managed_dma(void);\n#else\nstatic inline bool has_managed_dma(void)\n{\n\treturn false;\n}\n#endif\n\n\n#ifndef CONFIG_NUMA\n\nextern struct pglist_data contig_page_data;\nstatic inline struct pglist_data *NODE_DATA(int nid)\n{\n\treturn &contig_page_data;\n}\n\n#else  \n\n#include <asm/mmzone.h>\n\n#endif  \n\nextern struct pglist_data *first_online_pgdat(void);\nextern struct pglist_data *next_online_pgdat(struct pglist_data *pgdat);\nextern struct zone *next_zone(struct zone *zone);\n\n \n#define for_each_online_pgdat(pgdat)\t\t\t\\\n\tfor (pgdat = first_online_pgdat();\t\t\\\n\t     pgdat;\t\t\t\t\t\\\n\t     pgdat = next_online_pgdat(pgdat))\n \n#define for_each_zone(zone)\t\t\t        \\\n\tfor (zone = (first_online_pgdat())->node_zones; \\\n\t     zone;\t\t\t\t\t\\\n\t     zone = next_zone(zone))\n\n#define for_each_populated_zone(zone)\t\t        \\\n\tfor (zone = (first_online_pgdat())->node_zones; \\\n\t     zone;\t\t\t\t\t\\\n\t     zone = next_zone(zone))\t\t\t\\\n\t\tif (!populated_zone(zone))\t\t\\\n\t\t\t;  \t\t\\\n\t\telse\n\nstatic inline struct zone *zonelist_zone(struct zoneref *zoneref)\n{\n\treturn zoneref->zone;\n}\n\nstatic inline int zonelist_zone_idx(struct zoneref *zoneref)\n{\n\treturn zoneref->zone_idx;\n}\n\nstatic inline int zonelist_node_idx(struct zoneref *zoneref)\n{\n\treturn zone_to_nid(zoneref->zone);\n}\n\nstruct zoneref *__next_zones_zonelist(struct zoneref *z,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes);\n\n \nstatic __always_inline struct zoneref *next_zones_zonelist(struct zoneref *z,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes)\n{\n\tif (likely(!nodes && zonelist_zone_idx(z) <= highest_zoneidx))\n\t\treturn z;\n\treturn __next_zones_zonelist(z, highest_zoneidx, nodes);\n}\n\n \nstatic inline struct zoneref *first_zones_zonelist(struct zonelist *zonelist,\n\t\t\t\t\tenum zone_type highest_zoneidx,\n\t\t\t\t\tnodemask_t *nodes)\n{\n\treturn next_zones_zonelist(zonelist->_zonerefs,\n\t\t\t\t\t\t\thighest_zoneidx, nodes);\n}\n\n \n#define for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask) \\\n\tfor (z = first_zones_zonelist(zlist, highidx, nodemask), zone = zonelist_zone(z);\t\\\n\t\tzone;\t\t\t\t\t\t\t\\\n\t\tz = next_zones_zonelist(++z, highidx, nodemask),\t\\\n\t\t\tzone = zonelist_zone(z))\n\n#define for_next_zone_zonelist_nodemask(zone, z, highidx, nodemask) \\\n\tfor (zone = z->zone;\t\\\n\t\tzone;\t\t\t\t\t\t\t\\\n\t\tz = next_zones_zonelist(++z, highidx, nodemask),\t\\\n\t\t\tzone = zonelist_zone(z))\n\n\n \n#define for_each_zone_zonelist(zone, z, zlist, highidx) \\\n\tfor_each_zone_zonelist_nodemask(zone, z, zlist, highidx, NULL)\n\n \nstatic inline bool movable_only_nodes(nodemask_t *nodes)\n{\n\tstruct zonelist *zonelist;\n\tstruct zoneref *z;\n\tint nid;\n\n\tif (nodes_empty(*nodes))\n\t\treturn false;\n\n\t \n\tnid = first_node(*nodes);\n\tzonelist = &NODE_DATA(nid)->node_zonelists[ZONELIST_FALLBACK];\n\tz = first_zones_zonelist(zonelist, ZONE_NORMAL,\tnodes);\n\treturn (!z->zone) ? true : false;\n}\n\n\n#ifdef CONFIG_SPARSEMEM\n#include <asm/sparsemem.h>\n#endif\n\n#ifdef CONFIG_FLATMEM\n#define pfn_to_nid(pfn)\t\t(0)\n#endif\n\n#ifdef CONFIG_SPARSEMEM\n\n \n#define PA_SECTION_SHIFT\t(SECTION_SIZE_BITS)\n#define PFN_SECTION_SHIFT\t(SECTION_SIZE_BITS - PAGE_SHIFT)\n\n#define NR_MEM_SECTIONS\t\t(1UL << SECTIONS_SHIFT)\n\n#define PAGES_PER_SECTION       (1UL << PFN_SECTION_SHIFT)\n#define PAGE_SECTION_MASK\t(~(PAGES_PER_SECTION-1))\n\n#define SECTION_BLOCKFLAGS_BITS \\\n\t((1UL << (PFN_SECTION_SHIFT - pageblock_order)) * NR_PAGEBLOCK_BITS)\n\n#if (MAX_ORDER + PAGE_SHIFT) > SECTION_SIZE_BITS\n#error Allocator MAX_ORDER exceeds SECTION_SIZE\n#endif\n\nstatic inline unsigned long pfn_to_section_nr(unsigned long pfn)\n{\n\treturn pfn >> PFN_SECTION_SHIFT;\n}\nstatic inline unsigned long section_nr_to_pfn(unsigned long sec)\n{\n\treturn sec << PFN_SECTION_SHIFT;\n}\n\n#define SECTION_ALIGN_UP(pfn)\t(((pfn) + PAGES_PER_SECTION - 1) & PAGE_SECTION_MASK)\n#define SECTION_ALIGN_DOWN(pfn)\t((pfn) & PAGE_SECTION_MASK)\n\n#define SUBSECTION_SHIFT 21\n#define SUBSECTION_SIZE (1UL << SUBSECTION_SHIFT)\n\n#define PFN_SUBSECTION_SHIFT (SUBSECTION_SHIFT - PAGE_SHIFT)\n#define PAGES_PER_SUBSECTION (1UL << PFN_SUBSECTION_SHIFT)\n#define PAGE_SUBSECTION_MASK (~(PAGES_PER_SUBSECTION-1))\n\n#if SUBSECTION_SHIFT > SECTION_SIZE_BITS\n#error Subsection size exceeds section size\n#else\n#define SUBSECTIONS_PER_SECTION (1UL << (SECTION_SIZE_BITS - SUBSECTION_SHIFT))\n#endif\n\n#define SUBSECTION_ALIGN_UP(pfn) ALIGN((pfn), PAGES_PER_SUBSECTION)\n#define SUBSECTION_ALIGN_DOWN(pfn) ((pfn) & PAGE_SUBSECTION_MASK)\n\nstruct mem_section_usage {\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n\tDECLARE_BITMAP(subsection_map, SUBSECTIONS_PER_SECTION);\n#endif\n\t \n\tunsigned long pageblock_flags[0];\n};\n\nvoid subsection_map_init(unsigned long pfn, unsigned long nr_pages);\n\nstruct page;\nstruct page_ext;\nstruct mem_section {\n\t \n\tunsigned long section_mem_map;\n\n\tstruct mem_section_usage *usage;\n#ifdef CONFIG_PAGE_EXTENSION\n\t \n\tstruct page_ext *page_ext;\n\tunsigned long pad;\n#endif\n\t \n};\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\n#define SECTIONS_PER_ROOT       (PAGE_SIZE / sizeof (struct mem_section))\n#else\n#define SECTIONS_PER_ROOT\t1\n#endif\n\n#define SECTION_NR_TO_ROOT(sec)\t((sec) / SECTIONS_PER_ROOT)\n#define NR_SECTION_ROOTS\tDIV_ROUND_UP(NR_MEM_SECTIONS, SECTIONS_PER_ROOT)\n#define SECTION_ROOT_MASK\t(SECTIONS_PER_ROOT - 1)\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\nextern struct mem_section **mem_section;\n#else\nextern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];\n#endif\n\nstatic inline unsigned long *section_to_usemap(struct mem_section *ms)\n{\n\treturn ms->usage->pageblock_flags;\n}\n\nstatic inline struct mem_section *__nr_to_section(unsigned long nr)\n{\n\tunsigned long root = SECTION_NR_TO_ROOT(nr);\n\n\tif (unlikely(root >= NR_SECTION_ROOTS))\n\t\treturn NULL;\n\n#ifdef CONFIG_SPARSEMEM_EXTREME\n\tif (!mem_section || !mem_section[root])\n\t\treturn NULL;\n#endif\n\treturn &mem_section[root][nr & SECTION_ROOT_MASK];\n}\nextern size_t mem_section_usage_size(void);\n\n \nenum {\n\tSECTION_MARKED_PRESENT_BIT,\n\tSECTION_HAS_MEM_MAP_BIT,\n\tSECTION_IS_ONLINE_BIT,\n\tSECTION_IS_EARLY_BIT,\n#ifdef CONFIG_ZONE_DEVICE\n\tSECTION_TAINT_ZONE_DEVICE_BIT,\n#endif\n\tSECTION_MAP_LAST_BIT,\n};\n\n#define SECTION_MARKED_PRESENT\t\tBIT(SECTION_MARKED_PRESENT_BIT)\n#define SECTION_HAS_MEM_MAP\t\tBIT(SECTION_HAS_MEM_MAP_BIT)\n#define SECTION_IS_ONLINE\t\tBIT(SECTION_IS_ONLINE_BIT)\n#define SECTION_IS_EARLY\t\tBIT(SECTION_IS_EARLY_BIT)\n#ifdef CONFIG_ZONE_DEVICE\n#define SECTION_TAINT_ZONE_DEVICE\tBIT(SECTION_TAINT_ZONE_DEVICE_BIT)\n#endif\n#define SECTION_MAP_MASK\t\t(~(BIT(SECTION_MAP_LAST_BIT) - 1))\n#define SECTION_NID_SHIFT\t\tSECTION_MAP_LAST_BIT\n\nstatic inline struct page *__section_mem_map_addr(struct mem_section *section)\n{\n\tunsigned long map = section->section_mem_map;\n\tmap &= SECTION_MAP_MASK;\n\treturn (struct page *)map;\n}\n\nstatic inline int present_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_MARKED_PRESENT));\n}\n\nstatic inline int present_section_nr(unsigned long nr)\n{\n\treturn present_section(__nr_to_section(nr));\n}\n\nstatic inline int valid_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));\n}\n\nstatic inline int early_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_IS_EARLY));\n}\n\nstatic inline int valid_section_nr(unsigned long nr)\n{\n\treturn valid_section(__nr_to_section(nr));\n}\n\nstatic inline int online_section(struct mem_section *section)\n{\n\treturn (section && (section->section_mem_map & SECTION_IS_ONLINE));\n}\n\n#ifdef CONFIG_ZONE_DEVICE\nstatic inline int online_device_section(struct mem_section *section)\n{\n\tunsigned long flags = SECTION_IS_ONLINE | SECTION_TAINT_ZONE_DEVICE;\n\n\treturn section && ((section->section_mem_map & flags) == flags);\n}\n#else\nstatic inline int online_device_section(struct mem_section *section)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int online_section_nr(unsigned long nr)\n{\n\treturn online_section(__nr_to_section(nr));\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\nvoid online_mem_sections(unsigned long start_pfn, unsigned long end_pfn);\nvoid offline_mem_sections(unsigned long start_pfn, unsigned long end_pfn);\n#endif\n\nstatic inline struct mem_section *__pfn_to_section(unsigned long pfn)\n{\n\treturn __nr_to_section(pfn_to_section_nr(pfn));\n}\n\nextern unsigned long __highest_present_section_nr;\n\nstatic inline int subsection_map_index(unsigned long pfn)\n{\n\treturn (pfn & ~(PAGE_SECTION_MASK)) / PAGES_PER_SUBSECTION;\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\nstatic inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)\n{\n\tint idx = subsection_map_index(pfn);\n\n\treturn test_bit(idx, ms->usage->subsection_map);\n}\n#else\nstatic inline int pfn_section_valid(struct mem_section *ms, unsigned long pfn)\n{\n\treturn 1;\n}\n#endif\n\n#ifndef CONFIG_HAVE_ARCH_PFN_VALID\n \nstatic inline int pfn_valid(unsigned long pfn)\n{\n\tstruct mem_section *ms;\n\n\t \n\tif (PHYS_PFN(PFN_PHYS(pfn)) != pfn)\n\t\treturn 0;\n\n\tif (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)\n\t\treturn 0;\n\tms = __pfn_to_section(pfn);\n\tif (!valid_section(ms))\n\t\treturn 0;\n\t \n\treturn early_section(ms) || pfn_section_valid(ms, pfn);\n}\n#endif\n\nstatic inline int pfn_in_present_section(unsigned long pfn)\n{\n\tif (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)\n\t\treturn 0;\n\treturn present_section(__pfn_to_section(pfn));\n}\n\nstatic inline unsigned long next_present_section_nr(unsigned long section_nr)\n{\n\twhile (++section_nr <= __highest_present_section_nr) {\n\t\tif (present_section_nr(section_nr))\n\t\t\treturn section_nr;\n\t}\n\n\treturn -1;\n}\n\n \n#ifdef CONFIG_NUMA\n#define pfn_to_nid(pfn)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned long __pfn_to_nid_pfn = (pfn);\t\t\t\t\\\n\tpage_to_nid(pfn_to_page(__pfn_to_nid_pfn));\t\t\t\\\n})\n#else\n#define pfn_to_nid(pfn)\t\t(0)\n#endif\n\nvoid sparse_init(void);\n#else\n#define sparse_init()\tdo {} while (0)\n#define sparse_index_init(_sec, _nid)  do {} while (0)\n#define pfn_in_present_section pfn_valid\n#define subsection_map_init(_pfn, _nr_pages) do {} while (0)\n#endif  \n\n#endif  \n#endif  \n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}