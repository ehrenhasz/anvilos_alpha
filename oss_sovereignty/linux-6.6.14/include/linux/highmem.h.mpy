{
  "module_name": "highmem.h",
  "hash_id": "15538de5bc9c66848625e996d9c2b45f0bf8ee792497d059ef1aad74f5d06d1d",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/highmem.h",
  "human_readable_source": " \n#ifndef _LINUX_HIGHMEM_H\n#define _LINUX_HIGHMEM_H\n\n#include <linux/fs.h>\n#include <linux/kernel.h>\n#include <linux/bug.h>\n#include <linux/cacheflush.h>\n#include <linux/kmsan.h>\n#include <linux/mm.h>\n#include <linux/uaccess.h>\n#include <linux/hardirq.h>\n\n#include \"highmem-internal.h\"\n\n \nstatic inline void *kmap(struct page *page);\n\n \nstatic inline void kunmap(struct page *page);\n\n \nstatic inline struct page *kmap_to_page(void *addr);\n\n \nstatic inline void kmap_flush_unused(void);\n\n \nstatic inline void *kmap_local_page(struct page *page);\n\n \nstatic inline void *kmap_local_folio(struct folio *folio, size_t offset);\n\n \nstatic inline void *kmap_atomic(struct page *page);\n\n \nstatic inline unsigned int nr_free_highpages(void);\nstatic inline unsigned long totalhigh_pages(void);\n\n#ifndef ARCH_HAS_FLUSH_ANON_PAGE\nstatic inline void flush_anon_page(struct vm_area_struct *vma, struct page *page, unsigned long vmaddr)\n{\n}\n#endif\n\n#ifndef ARCH_IMPLEMENTS_FLUSH_KERNEL_VMAP_RANGE\nstatic inline void flush_kernel_vmap_range(void *vaddr, int size)\n{\n}\nstatic inline void invalidate_kernel_vmap_range(void *vaddr, int size)\n{\n}\n#endif\n\n \n#ifndef clear_user_highpage\nstatic inline void clear_user_highpage(struct page *page, unsigned long vaddr)\n{\n\tvoid *addr = kmap_local_page(page);\n\tclear_user_page(addr, vaddr, page);\n\tkunmap_local(addr);\n}\n#endif\n\n#ifndef vma_alloc_zeroed_movable_folio\n \nstatic inline\nstruct folio *vma_alloc_zeroed_movable_folio(struct vm_area_struct *vma,\n\t\t\t\t   unsigned long vaddr)\n{\n\tstruct folio *folio;\n\n\tfolio = vma_alloc_folio(GFP_HIGHUSER_MOVABLE, 0, vma, vaddr, false);\n\tif (folio)\n\t\tclear_user_highpage(&folio->page, vaddr);\n\n\treturn folio;\n}\n#endif\n\nstatic inline void clear_highpage(struct page *page)\n{\n\tvoid *kaddr = kmap_local_page(page);\n\tclear_page(kaddr);\n\tkunmap_local(kaddr);\n}\n\nstatic inline void clear_highpage_kasan_tagged(struct page *page)\n{\n\tvoid *kaddr = kmap_local_page(page);\n\n\tclear_page(kasan_reset_tag(kaddr));\n\tkunmap_local(kaddr);\n}\n\n#ifndef __HAVE_ARCH_TAG_CLEAR_HIGHPAGE\n\nstatic inline void tag_clear_highpage(struct page *page)\n{\n}\n\n#endif\n\n \n#ifdef CONFIG_HIGHMEM\nvoid zero_user_segments(struct page *page, unsigned start1, unsigned end1,\n\t\tunsigned start2, unsigned end2);\n#else\nstatic inline void zero_user_segments(struct page *page,\n\t\tunsigned start1, unsigned end1,\n\t\tunsigned start2, unsigned end2)\n{\n\tvoid *kaddr = kmap_local_page(page);\n\tunsigned int i;\n\n\tBUG_ON(end1 > page_size(page) || end2 > page_size(page));\n\n\tif (end1 > start1)\n\t\tmemset(kaddr + start1, 0, end1 - start1);\n\n\tif (end2 > start2)\n\t\tmemset(kaddr + start2, 0, end2 - start2);\n\n\tkunmap_local(kaddr);\n\tfor (i = 0; i < compound_nr(page); i++)\n\t\tflush_dcache_page(page + i);\n}\n#endif\n\nstatic inline void zero_user_segment(struct page *page,\n\tunsigned start, unsigned end)\n{\n\tzero_user_segments(page, start, end, 0, 0);\n}\n\nstatic inline void zero_user(struct page *page,\n\tunsigned start, unsigned size)\n{\n\tzero_user_segments(page, start, start + size, 0, 0);\n}\n\n#ifndef __HAVE_ARCH_COPY_USER_HIGHPAGE\n\nstatic inline void copy_user_highpage(struct page *to, struct page *from,\n\tunsigned long vaddr, struct vm_area_struct *vma)\n{\n\tchar *vfrom, *vto;\n\n\tvfrom = kmap_local_page(from);\n\tvto = kmap_local_page(to);\n\tcopy_user_page(vto, vfrom, vaddr, to);\n\tkmsan_unpoison_memory(page_address(to), PAGE_SIZE);\n\tkunmap_local(vto);\n\tkunmap_local(vfrom);\n}\n\n#endif\n\n#ifndef __HAVE_ARCH_COPY_HIGHPAGE\n\nstatic inline void copy_highpage(struct page *to, struct page *from)\n{\n\tchar *vfrom, *vto;\n\n\tvfrom = kmap_local_page(from);\n\tvto = kmap_local_page(to);\n\tcopy_page(vto, vfrom);\n\tkmsan_copy_page_meta(to, from);\n\tkunmap_local(vto);\n\tkunmap_local(vfrom);\n}\n\n#endif\n\n#ifdef copy_mc_to_kernel\n \nstatic inline int copy_mc_user_highpage(struct page *to, struct page *from,\n\t\t\t\t\tunsigned long vaddr, struct vm_area_struct *vma)\n{\n\tunsigned long ret;\n\tchar *vfrom, *vto;\n\n\tvfrom = kmap_local_page(from);\n\tvto = kmap_local_page(to);\n\tret = copy_mc_to_kernel(vto, vfrom, PAGE_SIZE);\n\tif (!ret)\n\t\tkmsan_unpoison_memory(page_address(to), PAGE_SIZE);\n\tkunmap_local(vto);\n\tkunmap_local(vfrom);\n\n\treturn ret;\n}\n\nstatic inline int copy_mc_highpage(struct page *to, struct page *from)\n{\n\tunsigned long ret;\n\tchar *vfrom, *vto;\n\n\tvfrom = kmap_local_page(from);\n\tvto = kmap_local_page(to);\n\tret = copy_mc_to_kernel(vto, vfrom, PAGE_SIZE);\n\tif (!ret)\n\t\tkmsan_copy_page_meta(to, from);\n\tkunmap_local(vto);\n\tkunmap_local(vfrom);\n\n\treturn ret;\n}\n#else\nstatic inline int copy_mc_user_highpage(struct page *to, struct page *from,\n\t\t\t\t\tunsigned long vaddr, struct vm_area_struct *vma)\n{\n\tcopy_user_highpage(to, from, vaddr, vma);\n\treturn 0;\n}\n\nstatic inline int copy_mc_highpage(struct page *to, struct page *from)\n{\n\tcopy_highpage(to, from);\n\treturn 0;\n}\n#endif\n\nstatic inline void memcpy_page(struct page *dst_page, size_t dst_off,\n\t\t\t       struct page *src_page, size_t src_off,\n\t\t\t       size_t len)\n{\n\tchar *dst = kmap_local_page(dst_page);\n\tchar *src = kmap_local_page(src_page);\n\n\tVM_BUG_ON(dst_off + len > PAGE_SIZE || src_off + len > PAGE_SIZE);\n\tmemcpy(dst + dst_off, src + src_off, len);\n\tkunmap_local(src);\n\tkunmap_local(dst);\n}\n\nstatic inline void memset_page(struct page *page, size_t offset, int val,\n\t\t\t       size_t len)\n{\n\tchar *addr = kmap_local_page(page);\n\n\tVM_BUG_ON(offset + len > PAGE_SIZE);\n\tmemset(addr + offset, val, len);\n\tkunmap_local(addr);\n}\n\nstatic inline void memcpy_from_page(char *to, struct page *page,\n\t\t\t\t    size_t offset, size_t len)\n{\n\tchar *from = kmap_local_page(page);\n\n\tVM_BUG_ON(offset + len > PAGE_SIZE);\n\tmemcpy(to, from + offset, len);\n\tkunmap_local(from);\n}\n\nstatic inline void memcpy_to_page(struct page *page, size_t offset,\n\t\t\t\t  const char *from, size_t len)\n{\n\tchar *to = kmap_local_page(page);\n\n\tVM_BUG_ON(offset + len > PAGE_SIZE);\n\tmemcpy(to + offset, from, len);\n\tflush_dcache_page(page);\n\tkunmap_local(to);\n}\n\nstatic inline void memzero_page(struct page *page, size_t offset, size_t len)\n{\n\tchar *addr = kmap_local_page(page);\n\n\tVM_BUG_ON(offset + len > PAGE_SIZE);\n\tmemset(addr + offset, 0, len);\n\tflush_dcache_page(page);\n\tkunmap_local(addr);\n}\n\nstatic inline void memcpy_from_folio(char *to, struct folio *folio,\n\t\tsize_t offset, size_t len)\n{\n\tVM_BUG_ON(offset + len > folio_size(folio));\n\n\tdo {\n\t\tconst char *from = kmap_local_folio(folio, offset);\n\t\tsize_t chunk = len;\n\n\t\tif (folio_test_highmem(folio) &&\n\t\t    chunk > PAGE_SIZE - offset_in_page(offset))\n\t\t\tchunk = PAGE_SIZE - offset_in_page(offset);\n\t\tmemcpy(to, from, chunk);\n\t\tkunmap_local(from);\n\n\t\tto += chunk;\n\t\toffset += chunk;\n\t\tlen -= chunk;\n\t} while (len > 0);\n}\n\nstatic inline void memcpy_to_folio(struct folio *folio, size_t offset,\n\t\tconst char *from, size_t len)\n{\n\tVM_BUG_ON(offset + len > folio_size(folio));\n\n\tdo {\n\t\tchar *to = kmap_local_folio(folio, offset);\n\t\tsize_t chunk = len;\n\n\t\tif (folio_test_highmem(folio) &&\n\t\t    chunk > PAGE_SIZE - offset_in_page(offset))\n\t\t\tchunk = PAGE_SIZE - offset_in_page(offset);\n\t\tmemcpy(to, from, chunk);\n\t\tkunmap_local(to);\n\n\t\tfrom += chunk;\n\t\toffset += chunk;\n\t\tlen -= chunk;\n\t} while (len > 0);\n\n\tflush_dcache_folio(folio);\n}\n\n \nstatic inline size_t memcpy_from_file_folio(char *to, struct folio *folio,\n\t\tloff_t pos, size_t len)\n{\n\tsize_t offset = offset_in_folio(folio, pos);\n\tchar *from = kmap_local_folio(folio, offset);\n\n\tif (folio_test_highmem(folio)) {\n\t\toffset = offset_in_page(offset);\n\t\tlen = min_t(size_t, len, PAGE_SIZE - offset);\n\t} else\n\t\tlen = min(len, folio_size(folio) - offset);\n\n\tmemcpy(to, from, len);\n\tkunmap_local(from);\n\n\treturn len;\n}\n\n \nstatic inline void folio_zero_segments(struct folio *folio,\n\t\tsize_t start1, size_t xend1, size_t start2, size_t xend2)\n{\n\tzero_user_segments(&folio->page, start1, xend1, start2, xend2);\n}\n\n \nstatic inline void folio_zero_segment(struct folio *folio,\n\t\tsize_t start, size_t xend)\n{\n\tzero_user_segments(&folio->page, start, xend, 0, 0);\n}\n\n \nstatic inline void folio_zero_range(struct folio *folio,\n\t\tsize_t start, size_t length)\n{\n\tzero_user_segments(&folio->page, start, start + length, 0, 0);\n}\n\nstatic inline void unmap_and_put_page(struct page *page, void *addr)\n{\n\tkunmap_local(addr);\n\tput_page(page);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}