{
  "module_name": "vfio.h",
  "hash_id": "d8d86b1c45685facf6d0f614300a7d91d83fd6b60d3160f76fc469a4aad522de",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/vfio.h",
  "human_readable_source": " \n \n#ifndef VFIO_H\n#define VFIO_H\n\n\n#include <linux/iommu.h>\n#include <linux/mm.h>\n#include <linux/workqueue.h>\n#include <linux/poll.h>\n#include <linux/cdev.h>\n#include <uapi/linux/vfio.h>\n#include <linux/iova_bitmap.h>\n\nstruct kvm;\nstruct iommufd_ctx;\nstruct iommufd_device;\nstruct iommufd_access;\n\n \nstruct vfio_device_set {\n\tvoid *set_id;\n\tstruct mutex lock;\n\tstruct list_head device_list;\n\tunsigned int device_count;\n};\n\nstruct vfio_device {\n\tstruct device *dev;\n\tconst struct vfio_device_ops *ops;\n\t \n\tconst struct vfio_migration_ops *mig_ops;\n\tconst struct vfio_log_ops *log_ops;\n#if IS_ENABLED(CONFIG_VFIO_GROUP)\n\tstruct vfio_group *group;\n\tstruct list_head group_next;\n\tstruct list_head iommu_entry;\n#endif\n\tstruct vfio_device_set *dev_set;\n\tstruct list_head dev_set_list;\n\tunsigned int migration_flags;\n\tstruct kvm *kvm;\n\n\t \n\tunsigned int index;\n\tstruct device device;\t \n#if IS_ENABLED(CONFIG_VFIO_DEVICE_CDEV)\n\tstruct cdev cdev;\n#endif\n\trefcount_t refcount;\t \n\tunsigned int open_count;\n\tstruct completion comp;\n\tstruct iommufd_access *iommufd_access;\n\tvoid (*put_kvm)(struct kvm *kvm);\n#if IS_ENABLED(CONFIG_IOMMUFD)\n\tstruct iommufd_device *iommufd_device;\n\tu8 iommufd_attached:1;\n#endif\n\tu8 cdev_opened:1;\n};\n\n \nstruct vfio_device_ops {\n\tchar\t*name;\n\tint\t(*init)(struct vfio_device *vdev);\n\tvoid\t(*release)(struct vfio_device *vdev);\n\tint\t(*bind_iommufd)(struct vfio_device *vdev,\n\t\t\t\tstruct iommufd_ctx *ictx, u32 *out_device_id);\n\tvoid\t(*unbind_iommufd)(struct vfio_device *vdev);\n\tint\t(*attach_ioas)(struct vfio_device *vdev, u32 *pt_id);\n\tvoid\t(*detach_ioas)(struct vfio_device *vdev);\n\tint\t(*open_device)(struct vfio_device *vdev);\n\tvoid\t(*close_device)(struct vfio_device *vdev);\n\tssize_t\t(*read)(struct vfio_device *vdev, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos);\n\tssize_t\t(*write)(struct vfio_device *vdev, const char __user *buf,\n\t\t\t size_t count, loff_t *size);\n\tlong\t(*ioctl)(struct vfio_device *vdev, unsigned int cmd,\n\t\t\t unsigned long arg);\n\tint\t(*mmap)(struct vfio_device *vdev, struct vm_area_struct *vma);\n\tvoid\t(*request)(struct vfio_device *vdev, unsigned int count);\n\tint\t(*match)(struct vfio_device *vdev, char *buf);\n\tvoid\t(*dma_unmap)(struct vfio_device *vdev, u64 iova, u64 length);\n\tint\t(*device_feature)(struct vfio_device *device, u32 flags,\n\t\t\t\t  void __user *arg, size_t argsz);\n};\n\n#if IS_ENABLED(CONFIG_IOMMUFD)\nstruct iommufd_ctx *vfio_iommufd_device_ictx(struct vfio_device *vdev);\nint vfio_iommufd_get_dev_id(struct vfio_device *vdev, struct iommufd_ctx *ictx);\nint vfio_iommufd_physical_bind(struct vfio_device *vdev,\n\t\t\t       struct iommufd_ctx *ictx, u32 *out_device_id);\nvoid vfio_iommufd_physical_unbind(struct vfio_device *vdev);\nint vfio_iommufd_physical_attach_ioas(struct vfio_device *vdev, u32 *pt_id);\nvoid vfio_iommufd_physical_detach_ioas(struct vfio_device *vdev);\nint vfio_iommufd_emulated_bind(struct vfio_device *vdev,\n\t\t\t       struct iommufd_ctx *ictx, u32 *out_device_id);\nvoid vfio_iommufd_emulated_unbind(struct vfio_device *vdev);\nint vfio_iommufd_emulated_attach_ioas(struct vfio_device *vdev, u32 *pt_id);\nvoid vfio_iommufd_emulated_detach_ioas(struct vfio_device *vdev);\n#else\nstatic inline struct iommufd_ctx *\nvfio_iommufd_device_ictx(struct vfio_device *vdev)\n{\n\treturn NULL;\n}\n\nstatic inline int\nvfio_iommufd_get_dev_id(struct vfio_device *vdev, struct iommufd_ctx *ictx)\n{\n\treturn VFIO_PCI_DEVID_NOT_OWNED;\n}\n\n#define vfio_iommufd_physical_bind                                      \\\n\t((int (*)(struct vfio_device *vdev, struct iommufd_ctx *ictx,   \\\n\t\t  u32 *out_device_id)) NULL)\n#define vfio_iommufd_physical_unbind \\\n\t((void (*)(struct vfio_device *vdev)) NULL)\n#define vfio_iommufd_physical_attach_ioas \\\n\t((int (*)(struct vfio_device *vdev, u32 *pt_id)) NULL)\n#define vfio_iommufd_physical_detach_ioas \\\n\t((void (*)(struct vfio_device *vdev)) NULL)\n#define vfio_iommufd_emulated_bind                                      \\\n\t((int (*)(struct vfio_device *vdev, struct iommufd_ctx *ictx,   \\\n\t\t  u32 *out_device_id)) NULL)\n#define vfio_iommufd_emulated_unbind \\\n\t((void (*)(struct vfio_device *vdev)) NULL)\n#define vfio_iommufd_emulated_attach_ioas \\\n\t((int (*)(struct vfio_device *vdev, u32 *pt_id)) NULL)\n#define vfio_iommufd_emulated_detach_ioas \\\n\t((void (*)(struct vfio_device *vdev)) NULL)\n#endif\n\nstatic inline bool vfio_device_cdev_opened(struct vfio_device *device)\n{\n\treturn device->cdev_opened;\n}\n\n \nstruct vfio_migration_ops {\n\tstruct file *(*migration_set_state)(\n\t\tstruct vfio_device *device,\n\t\tenum vfio_device_mig_state new_state);\n\tint (*migration_get_state)(struct vfio_device *device,\n\t\t\t\t   enum vfio_device_mig_state *curr_state);\n\tint (*migration_get_data_size)(struct vfio_device *device,\n\t\t\t\t       unsigned long *stop_copy_length);\n};\n\n \nstruct vfio_log_ops {\n\tint (*log_start)(struct vfio_device *device,\n\t\tstruct rb_root_cached *ranges, u32 nnodes, u64 *page_size);\n\tint (*log_stop)(struct vfio_device *device);\n\tint (*log_read_and_clear)(struct vfio_device *device,\n\t\tunsigned long iova, unsigned long length,\n\t\tstruct iova_bitmap *dirty);\n};\n\n \nstatic inline int vfio_check_feature(u32 flags, size_t argsz, u32 supported_ops,\n\t\t\t\t    size_t minsz)\n{\n\tif ((flags & (VFIO_DEVICE_FEATURE_GET | VFIO_DEVICE_FEATURE_SET)) &\n\t    ~supported_ops)\n\t\treturn -EINVAL;\n\tif (flags & VFIO_DEVICE_FEATURE_PROBE)\n\t\treturn 0;\n\t \n\tif (!(flags & (VFIO_DEVICE_FEATURE_GET | VFIO_DEVICE_FEATURE_SET)))\n\t\treturn -EINVAL;\n\tif (argsz < minsz)\n\t\treturn -EINVAL;\n\treturn 1;\n}\n\nstruct vfio_device *_vfio_alloc_device(size_t size, struct device *dev,\n\t\t\t\t       const struct vfio_device_ops *ops);\n#define vfio_alloc_device(dev_struct, member, dev, ops)\t\t\t\t\\\n\tcontainer_of(_vfio_alloc_device(sizeof(struct dev_struct) +\t\t\\\n\t\t\t\t\tBUILD_BUG_ON_ZERO(offsetof(\t\t\\\n\t\t\t\t\t\tstruct dev_struct, member)),\t\\\n\t\t\t\t\tdev, ops),\t\t\t\t\\\n\t\t     struct dev_struct, member)\n\nstatic inline void vfio_put_device(struct vfio_device *device)\n{\n\tput_device(&device->device);\n}\n\nint vfio_register_group_dev(struct vfio_device *device);\nint vfio_register_emulated_iommu_dev(struct vfio_device *device);\nvoid vfio_unregister_group_dev(struct vfio_device *device);\n\nint vfio_assign_device_set(struct vfio_device *device, void *set_id);\nunsigned int vfio_device_set_open_count(struct vfio_device_set *dev_set);\nstruct vfio_device *\nvfio_find_device_in_devset(struct vfio_device_set *dev_set,\n\t\t\t   struct device *dev);\n\nint vfio_mig_get_next_state(struct vfio_device *device,\n\t\t\t    enum vfio_device_mig_state cur_fsm,\n\t\t\t    enum vfio_device_mig_state new_fsm,\n\t\t\t    enum vfio_device_mig_state *next_fsm);\n\nvoid vfio_combine_iova_ranges(struct rb_root_cached *root, u32 cur_nodes,\n\t\t\t      u32 req_nodes);\n\n \nstruct iommu_group *vfio_file_iommu_group(struct file *file);\n\n#if IS_ENABLED(CONFIG_VFIO_GROUP)\nbool vfio_file_is_group(struct file *file);\nbool vfio_file_has_dev(struct file *file, struct vfio_device *device);\n#else\nstatic inline bool vfio_file_is_group(struct file *file)\n{\n\treturn false;\n}\n\nstatic inline bool vfio_file_has_dev(struct file *file, struct vfio_device *device)\n{\n\treturn false;\n}\n#endif\nbool vfio_file_is_valid(struct file *file);\nbool vfio_file_enforced_coherent(struct file *file);\nvoid vfio_file_set_kvm(struct file *file, struct kvm *kvm);\n\n#define VFIO_PIN_PAGES_MAX_ENTRIES\t(PAGE_SIZE/sizeof(unsigned long))\n\nint vfio_pin_pages(struct vfio_device *device, dma_addr_t iova,\n\t\t   int npage, int prot, struct page **pages);\nvoid vfio_unpin_pages(struct vfio_device *device, dma_addr_t iova, int npage);\nint vfio_dma_rw(struct vfio_device *device, dma_addr_t iova,\n\t\tvoid *data, size_t len, bool write);\n\n \nstruct vfio_info_cap {\n\tstruct vfio_info_cap_header *buf;\n\tsize_t size;\n};\nstruct vfio_info_cap_header *vfio_info_cap_add(struct vfio_info_cap *caps,\n\t\t\t\t\t       size_t size, u16 id,\n\t\t\t\t\t       u16 version);\nvoid vfio_info_cap_shift(struct vfio_info_cap *caps, size_t offset);\n\nint vfio_info_add_capability(struct vfio_info_cap *caps,\n\t\t\t     struct vfio_info_cap_header *cap, size_t size);\n\nint vfio_set_irqs_validate_and_prepare(struct vfio_irq_set *hdr,\n\t\t\t\t       int num_irqs, int max_irq_type,\n\t\t\t\t       size_t *data_size);\n\n \nstruct virqfd {\n\tvoid\t\t\t*opaque;\n\tstruct eventfd_ctx\t*eventfd;\n\tint\t\t\t(*handler)(void *, void *);\n\tvoid\t\t\t(*thread)(void *, void *);\n\tvoid\t\t\t*data;\n\tstruct work_struct\tinject;\n\twait_queue_entry_t\t\twait;\n\tpoll_table\t\tpt;\n\tstruct work_struct\tshutdown;\n\tstruct virqfd\t\t**pvirqfd;\n};\n\nint vfio_virqfd_enable(void *opaque, int (*handler)(void *, void *),\n\t\t       void (*thread)(void *, void *), void *data,\n\t\t       struct virqfd **pvirqfd, int fd);\nvoid vfio_virqfd_disable(struct virqfd **pvirqfd);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}