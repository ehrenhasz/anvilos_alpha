{
  "module_name": "bpf.h",
  "hash_id": "d4cfa5b10dc79f7e479cdf3021d06e022c0f1ac1f354ccb2ad0a8ad9f934049e",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/bpf.h",
  "human_readable_source": " \n \n#ifndef _LINUX_BPF_H\n#define _LINUX_BPF_H 1\n\n#include <uapi/linux/bpf.h>\n#include <uapi/linux/filter.h>\n\n#include <linux/workqueue.h>\n#include <linux/file.h>\n#include <linux/percpu.h>\n#include <linux/err.h>\n#include <linux/rbtree_latch.h>\n#include <linux/numa.h>\n#include <linux/mm_types.h>\n#include <linux/wait.h>\n#include <linux/refcount.h>\n#include <linux/mutex.h>\n#include <linux/module.h>\n#include <linux/kallsyms.h>\n#include <linux/capability.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n#include <linux/percpu-refcount.h>\n#include <linux/stddef.h>\n#include <linux/bpfptr.h>\n#include <linux/btf.h>\n#include <linux/rcupdate_trace.h>\n#include <linux/static_call.h>\n#include <linux/memcontrol.h>\n\nstruct bpf_verifier_env;\nstruct bpf_verifier_log;\nstruct perf_event;\nstruct bpf_prog;\nstruct bpf_prog_aux;\nstruct bpf_map;\nstruct sock;\nstruct seq_file;\nstruct btf;\nstruct btf_type;\nstruct exception_table_entry;\nstruct seq_operations;\nstruct bpf_iter_aux_info;\nstruct bpf_local_storage;\nstruct bpf_local_storage_map;\nstruct kobject;\nstruct mem_cgroup;\nstruct module;\nstruct bpf_func_state;\nstruct ftrace_ops;\nstruct cgroup;\n\nextern struct idr btf_idr;\nextern spinlock_t btf_idr_lock;\nextern struct kobject *btf_kobj;\nextern struct bpf_mem_alloc bpf_global_ma;\nextern bool bpf_global_ma_set;\n\ntypedef u64 (*bpf_callback_t)(u64, u64, u64, u64, u64);\ntypedef int (*bpf_iter_init_seq_priv_t)(void *private_data,\n\t\t\t\t\tstruct bpf_iter_aux_info *aux);\ntypedef void (*bpf_iter_fini_seq_priv_t)(void *private_data);\ntypedef unsigned int (*bpf_func_t)(const void *,\n\t\t\t\t   const struct bpf_insn *);\nstruct bpf_iter_seq_info {\n\tconst struct seq_operations *seq_ops;\n\tbpf_iter_init_seq_priv_t init_seq_private;\n\tbpf_iter_fini_seq_priv_t fini_seq_private;\n\tu32 seq_priv_size;\n};\n\n \nstruct bpf_map_ops {\n\t \n\tint (*map_alloc_check)(union bpf_attr *attr);\n\tstruct bpf_map *(*map_alloc)(union bpf_attr *attr);\n\tvoid (*map_release)(struct bpf_map *map, struct file *map_file);\n\tvoid (*map_free)(struct bpf_map *map);\n\tint (*map_get_next_key)(struct bpf_map *map, void *key, void *next_key);\n\tvoid (*map_release_uref)(struct bpf_map *map);\n\tvoid *(*map_lookup_elem_sys_only)(struct bpf_map *map, void *key);\n\tint (*map_lookup_batch)(struct bpf_map *map, const union bpf_attr *attr,\n\t\t\t\tunion bpf_attr __user *uattr);\n\tint (*map_lookup_and_delete_elem)(struct bpf_map *map, void *key,\n\t\t\t\t\t  void *value, u64 flags);\n\tint (*map_lookup_and_delete_batch)(struct bpf_map *map,\n\t\t\t\t\t   const union bpf_attr *attr,\n\t\t\t\t\t   union bpf_attr __user *uattr);\n\tint (*map_update_batch)(struct bpf_map *map, struct file *map_file,\n\t\t\t\tconst union bpf_attr *attr,\n\t\t\t\tunion bpf_attr __user *uattr);\n\tint (*map_delete_batch)(struct bpf_map *map, const union bpf_attr *attr,\n\t\t\t\tunion bpf_attr __user *uattr);\n\n\t \n\tvoid *(*map_lookup_elem)(struct bpf_map *map, void *key);\n\tlong (*map_update_elem)(struct bpf_map *map, void *key, void *value, u64 flags);\n\tlong (*map_delete_elem)(struct bpf_map *map, void *key);\n\tlong (*map_push_elem)(struct bpf_map *map, void *value, u64 flags);\n\tlong (*map_pop_elem)(struct bpf_map *map, void *value);\n\tlong (*map_peek_elem)(struct bpf_map *map, void *value);\n\tvoid *(*map_lookup_percpu_elem)(struct bpf_map *map, void *key, u32 cpu);\n\n\t \n\tvoid *(*map_fd_get_ptr)(struct bpf_map *map, struct file *map_file,\n\t\t\t\tint fd);\n\t \n\tvoid (*map_fd_put_ptr)(struct bpf_map *map, void *ptr, bool need_defer);\n\tint (*map_gen_lookup)(struct bpf_map *map, struct bpf_insn *insn_buf);\n\tu32 (*map_fd_sys_lookup_elem)(void *ptr);\n\tvoid (*map_seq_show_elem)(struct bpf_map *map, void *key,\n\t\t\t\t  struct seq_file *m);\n\tint (*map_check_btf)(const struct bpf_map *map,\n\t\t\t     const struct btf *btf,\n\t\t\t     const struct btf_type *key_type,\n\t\t\t     const struct btf_type *value_type);\n\n\t \n\tint (*map_poke_track)(struct bpf_map *map, struct bpf_prog_aux *aux);\n\tvoid (*map_poke_untrack)(struct bpf_map *map, struct bpf_prog_aux *aux);\n\tvoid (*map_poke_run)(struct bpf_map *map, u32 key, struct bpf_prog *old,\n\t\t\t     struct bpf_prog *new);\n\n\t \n\tint (*map_direct_value_addr)(const struct bpf_map *map,\n\t\t\t\t     u64 *imm, u32 off);\n\tint (*map_direct_value_meta)(const struct bpf_map *map,\n\t\t\t\t     u64 imm, u32 *off);\n\tint (*map_mmap)(struct bpf_map *map, struct vm_area_struct *vma);\n\t__poll_t (*map_poll)(struct bpf_map *map, struct file *filp,\n\t\t\t     struct poll_table_struct *pts);\n\n\t \n\tint (*map_local_storage_charge)(struct bpf_local_storage_map *smap,\n\t\t\t\t\tvoid *owner, u32 size);\n\tvoid (*map_local_storage_uncharge)(struct bpf_local_storage_map *smap,\n\t\t\t\t\t   void *owner, u32 size);\n\tstruct bpf_local_storage __rcu ** (*map_owner_storage_ptr)(void *owner);\n\n\t \n\tlong (*map_redirect)(struct bpf_map *map, u64 key, u64 flags);\n\n\t \n\tbool (*map_meta_equal)(const struct bpf_map *meta0,\n\t\t\t       const struct bpf_map *meta1);\n\n\n\tint (*map_set_for_each_callback_args)(struct bpf_verifier_env *env,\n\t\t\t\t\t      struct bpf_func_state *caller,\n\t\t\t\t\t      struct bpf_func_state *callee);\n\tlong (*map_for_each_callback)(struct bpf_map *map,\n\t\t\t\t     bpf_callback_t callback_fn,\n\t\t\t\t     void *callback_ctx, u64 flags);\n\n\tu64 (*map_mem_usage)(const struct bpf_map *map);\n\n\t \n\tint *map_btf_id;\n\n\t \n\tconst struct bpf_iter_seq_info *iter_seq_info;\n};\n\nenum {\n\t \n\tBTF_FIELDS_MAX\t   = 10,\n};\n\nenum btf_field_type {\n\tBPF_SPIN_LOCK  = (1 << 0),\n\tBPF_TIMER      = (1 << 1),\n\tBPF_KPTR_UNREF = (1 << 2),\n\tBPF_KPTR_REF   = (1 << 3),\n\tBPF_KPTR       = BPF_KPTR_UNREF | BPF_KPTR_REF,\n\tBPF_LIST_HEAD  = (1 << 4),\n\tBPF_LIST_NODE  = (1 << 5),\n\tBPF_RB_ROOT    = (1 << 6),\n\tBPF_RB_NODE    = (1 << 7),\n\tBPF_GRAPH_NODE_OR_ROOT = BPF_LIST_NODE | BPF_LIST_HEAD |\n\t\t\t\t BPF_RB_NODE | BPF_RB_ROOT,\n\tBPF_REFCOUNT   = (1 << 8),\n};\n\ntypedef void (*btf_dtor_kfunc_t)(void *);\n\nstruct btf_field_kptr {\n\tstruct btf *btf;\n\tstruct module *module;\n\t \n\tbtf_dtor_kfunc_t dtor;\n\tu32 btf_id;\n};\n\nstruct btf_field_graph_root {\n\tstruct btf *btf;\n\tu32 value_btf_id;\n\tu32 node_offset;\n\tstruct btf_record *value_rec;\n};\n\nstruct btf_field {\n\tu32 offset;\n\tu32 size;\n\tenum btf_field_type type;\n\tunion {\n\t\tstruct btf_field_kptr kptr;\n\t\tstruct btf_field_graph_root graph_root;\n\t};\n};\n\nstruct btf_record {\n\tu32 cnt;\n\tu32 field_mask;\n\tint spin_lock_off;\n\tint timer_off;\n\tint refcount_off;\n\tstruct btf_field fields[];\n};\n\n \nstruct bpf_rb_node_kern {\n\tstruct rb_node rb_node;\n\tvoid *owner;\n} __attribute__((aligned(8)));\n\n \nstruct bpf_list_node_kern {\n\tstruct list_head list_head;\n\tvoid *owner;\n} __attribute__((aligned(8)));\n\nstruct bpf_map {\n\t \n\tconst struct bpf_map_ops *ops ____cacheline_aligned;\n\tstruct bpf_map *inner_map_meta;\n#ifdef CONFIG_SECURITY\n\tvoid *security;\n#endif\n\tenum bpf_map_type map_type;\n\tu32 key_size;\n\tu32 value_size;\n\tu32 max_entries;\n\tu64 map_extra;  \n\tu32 map_flags;\n\tu32 id;\n\tstruct btf_record *record;\n\tint numa_node;\n\tu32 btf_key_type_id;\n\tu32 btf_value_type_id;\n\tu32 btf_vmlinux_value_type_id;\n\tstruct btf *btf;\n#ifdef CONFIG_MEMCG_KMEM\n\tstruct obj_cgroup *objcg;\n#endif\n\tchar name[BPF_OBJ_NAME_LEN];\n\t \n\tatomic64_t refcnt ____cacheline_aligned;\n\tatomic64_t usercnt;\n\t \n\tunion {\n\t\tstruct work_struct work;\n\t\tstruct rcu_head rcu;\n\t};\n\tstruct mutex freeze_mutex;\n\tatomic64_t writecnt;\n\t \n\tstruct {\n\t\tspinlock_t lock;\n\t\tenum bpf_prog_type type;\n\t\tbool jited;\n\t\tbool xdp_has_frags;\n\t} owner;\n\tbool bypass_spec_v1;\n\tbool frozen;  \n\tbool free_after_mult_rcu_gp;\n\ts64 __percpu *elem_count;\n};\n\nstatic inline const char *btf_field_type_name(enum btf_field_type type)\n{\n\tswitch (type) {\n\tcase BPF_SPIN_LOCK:\n\t\treturn \"bpf_spin_lock\";\n\tcase BPF_TIMER:\n\t\treturn \"bpf_timer\";\n\tcase BPF_KPTR_UNREF:\n\tcase BPF_KPTR_REF:\n\t\treturn \"kptr\";\n\tcase BPF_LIST_HEAD:\n\t\treturn \"bpf_list_head\";\n\tcase BPF_LIST_NODE:\n\t\treturn \"bpf_list_node\";\n\tcase BPF_RB_ROOT:\n\t\treturn \"bpf_rb_root\";\n\tcase BPF_RB_NODE:\n\t\treturn \"bpf_rb_node\";\n\tcase BPF_REFCOUNT:\n\t\treturn \"bpf_refcount\";\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn \"unknown\";\n\t}\n}\n\nstatic inline u32 btf_field_type_size(enum btf_field_type type)\n{\n\tswitch (type) {\n\tcase BPF_SPIN_LOCK:\n\t\treturn sizeof(struct bpf_spin_lock);\n\tcase BPF_TIMER:\n\t\treturn sizeof(struct bpf_timer);\n\tcase BPF_KPTR_UNREF:\n\tcase BPF_KPTR_REF:\n\t\treturn sizeof(u64);\n\tcase BPF_LIST_HEAD:\n\t\treturn sizeof(struct bpf_list_head);\n\tcase BPF_LIST_NODE:\n\t\treturn sizeof(struct bpf_list_node);\n\tcase BPF_RB_ROOT:\n\t\treturn sizeof(struct bpf_rb_root);\n\tcase BPF_RB_NODE:\n\t\treturn sizeof(struct bpf_rb_node);\n\tcase BPF_REFCOUNT:\n\t\treturn sizeof(struct bpf_refcount);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn 0;\n\t}\n}\n\nstatic inline u32 btf_field_type_align(enum btf_field_type type)\n{\n\tswitch (type) {\n\tcase BPF_SPIN_LOCK:\n\t\treturn __alignof__(struct bpf_spin_lock);\n\tcase BPF_TIMER:\n\t\treturn __alignof__(struct bpf_timer);\n\tcase BPF_KPTR_UNREF:\n\tcase BPF_KPTR_REF:\n\t\treturn __alignof__(u64);\n\tcase BPF_LIST_HEAD:\n\t\treturn __alignof__(struct bpf_list_head);\n\tcase BPF_LIST_NODE:\n\t\treturn __alignof__(struct bpf_list_node);\n\tcase BPF_RB_ROOT:\n\t\treturn __alignof__(struct bpf_rb_root);\n\tcase BPF_RB_NODE:\n\t\treturn __alignof__(struct bpf_rb_node);\n\tcase BPF_REFCOUNT:\n\t\treturn __alignof__(struct bpf_refcount);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn 0;\n\t}\n}\n\nstatic inline void bpf_obj_init_field(const struct btf_field *field, void *addr)\n{\n\tmemset(addr, 0, field->size);\n\n\tswitch (field->type) {\n\tcase BPF_REFCOUNT:\n\t\trefcount_set((refcount_t *)addr, 1);\n\t\tbreak;\n\tcase BPF_RB_NODE:\n\t\tRB_CLEAR_NODE((struct rb_node *)addr);\n\t\tbreak;\n\tcase BPF_LIST_HEAD:\n\tcase BPF_LIST_NODE:\n\t\tINIT_LIST_HEAD((struct list_head *)addr);\n\t\tbreak;\n\tcase BPF_RB_ROOT:\n\t\t \n\tcase BPF_SPIN_LOCK:\n\tcase BPF_TIMER:\n\tcase BPF_KPTR_UNREF:\n\tcase BPF_KPTR_REF:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n}\n\nstatic inline bool btf_record_has_field(const struct btf_record *rec, enum btf_field_type type)\n{\n\tif (IS_ERR_OR_NULL(rec))\n\t\treturn false;\n\treturn rec->field_mask & type;\n}\n\nstatic inline void bpf_obj_init(const struct btf_record *rec, void *obj)\n{\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rec))\n\t\treturn;\n\tfor (i = 0; i < rec->cnt; i++)\n\t\tbpf_obj_init_field(&rec->fields[i], obj + rec->fields[i].offset);\n}\n\n \nstatic inline void check_and_init_map_value(struct bpf_map *map, void *dst)\n{\n\tbpf_obj_init(map->record, dst);\n}\n\n \nstatic inline void bpf_long_memcpy(void *dst, const void *src, u32 size)\n{\n\tconst long *lsrc = src;\n\tlong *ldst = dst;\n\n\tsize /= sizeof(long);\n\twhile (size--)\n\t\tdata_race(*ldst++ = *lsrc++);\n}\n\n \nstatic inline void bpf_obj_memcpy(struct btf_record *rec,\n\t\t\t\t  void *dst, void *src, u32 size,\n\t\t\t\t  bool long_memcpy)\n{\n\tu32 curr_off = 0;\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rec)) {\n\t\tif (long_memcpy)\n\t\t\tbpf_long_memcpy(dst, src, round_up(size, 8));\n\t\telse\n\t\t\tmemcpy(dst, src, size);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tu32 next_off = rec->fields[i].offset;\n\t\tu32 sz = next_off - curr_off;\n\n\t\tmemcpy(dst + curr_off, src + curr_off, sz);\n\t\tcurr_off += rec->fields[i].size + sz;\n\t}\n\tmemcpy(dst + curr_off, src + curr_off, size - curr_off);\n}\n\nstatic inline void copy_map_value(struct bpf_map *map, void *dst, void *src)\n{\n\tbpf_obj_memcpy(map->record, dst, src, map->value_size, false);\n}\n\nstatic inline void copy_map_value_long(struct bpf_map *map, void *dst, void *src)\n{\n\tbpf_obj_memcpy(map->record, dst, src, map->value_size, true);\n}\n\nstatic inline void bpf_obj_memzero(struct btf_record *rec, void *dst, u32 size)\n{\n\tu32 curr_off = 0;\n\tint i;\n\n\tif (IS_ERR_OR_NULL(rec)) {\n\t\tmemset(dst, 0, size);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < rec->cnt; i++) {\n\t\tu32 next_off = rec->fields[i].offset;\n\t\tu32 sz = next_off - curr_off;\n\n\t\tmemset(dst + curr_off, 0, sz);\n\t\tcurr_off += rec->fields[i].size + sz;\n\t}\n\tmemset(dst + curr_off, 0, size - curr_off);\n}\n\nstatic inline void zero_map_value(struct bpf_map *map, void *dst)\n{\n\tbpf_obj_memzero(map->record, dst, map->value_size);\n}\n\nvoid copy_map_value_locked(struct bpf_map *map, void *dst, void *src,\n\t\t\t   bool lock_src);\nvoid bpf_timer_cancel_and_free(void *timer);\nvoid bpf_list_head_free(const struct btf_field *field, void *list_head,\n\t\t\tstruct bpf_spin_lock *spin_lock);\nvoid bpf_rb_root_free(const struct btf_field *field, void *rb_root,\n\t\t      struct bpf_spin_lock *spin_lock);\n\n\nint bpf_obj_name_cpy(char *dst, const char *src, unsigned int size);\n\nstruct bpf_offload_dev;\nstruct bpf_offloaded_map;\n\nstruct bpf_map_dev_ops {\n\tint (*map_get_next_key)(struct bpf_offloaded_map *map,\n\t\t\t\tvoid *key, void *next_key);\n\tint (*map_lookup_elem)(struct bpf_offloaded_map *map,\n\t\t\t       void *key, void *value);\n\tint (*map_update_elem)(struct bpf_offloaded_map *map,\n\t\t\t       void *key, void *value, u64 flags);\n\tint (*map_delete_elem)(struct bpf_offloaded_map *map, void *key);\n};\n\nstruct bpf_offloaded_map {\n\tstruct bpf_map map;\n\tstruct net_device *netdev;\n\tconst struct bpf_map_dev_ops *dev_ops;\n\tvoid *dev_priv;\n\tstruct list_head offloads;\n};\n\nstatic inline struct bpf_offloaded_map *map_to_offmap(struct bpf_map *map)\n{\n\treturn container_of(map, struct bpf_offloaded_map, map);\n}\n\nstatic inline bool bpf_map_offload_neutral(const struct bpf_map *map)\n{\n\treturn map->map_type == BPF_MAP_TYPE_PERF_EVENT_ARRAY;\n}\n\nstatic inline bool bpf_map_support_seq_show(const struct bpf_map *map)\n{\n\treturn (map->btf_value_type_id || map->btf_vmlinux_value_type_id) &&\n\t\tmap->ops->map_seq_show_elem;\n}\n\nint map_check_no_btf(const struct bpf_map *map,\n\t\t     const struct btf *btf,\n\t\t     const struct btf_type *key_type,\n\t\t     const struct btf_type *value_type);\n\nbool bpf_map_meta_equal(const struct bpf_map *meta0,\n\t\t\tconst struct bpf_map *meta1);\n\nextern const struct bpf_map_ops bpf_map_offload_ops;\n\n \n#define BPF_BASE_TYPE_BITS\t8\n\nenum bpf_type_flag {\n\t \n\tPTR_MAYBE_NULL\t\t= BIT(0 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_RDONLY\t\t= BIT(1 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_RINGBUF\t\t= BIT(2 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_USER\t\t= BIT(3 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_PERCPU\t\t= BIT(4 + BPF_BASE_TYPE_BITS),\n\n\t \n\tOBJ_RELEASE\t\t= BIT(5 + BPF_BASE_TYPE_BITS),\n\n\t \n\tPTR_UNTRUSTED\t\t= BIT(6 + BPF_BASE_TYPE_BITS),\n\n\tMEM_UNINIT\t\t= BIT(7 + BPF_BASE_TYPE_BITS),\n\n\t \n\tDYNPTR_TYPE_LOCAL\t= BIT(8 + BPF_BASE_TYPE_BITS),\n\n\t \n\tDYNPTR_TYPE_RINGBUF\t= BIT(9 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_FIXED_SIZE\t\t= BIT(10 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_ALLOC\t\t= BIT(11 + BPF_BASE_TYPE_BITS),\n\n\t \n\tPTR_TRUSTED\t\t= BIT(12 + BPF_BASE_TYPE_BITS),\n\n\t \n\tMEM_RCU\t\t\t= BIT(13 + BPF_BASE_TYPE_BITS),\n\n\t \n\tNON_OWN_REF\t\t= BIT(14 + BPF_BASE_TYPE_BITS),\n\n\t \n\tDYNPTR_TYPE_SKB\t\t= BIT(15 + BPF_BASE_TYPE_BITS),\n\n\t \n\tDYNPTR_TYPE_XDP\t\t= BIT(16 + BPF_BASE_TYPE_BITS),\n\n\t__BPF_TYPE_FLAG_MAX,\n\t__BPF_TYPE_LAST_FLAG\t= __BPF_TYPE_FLAG_MAX - 1,\n};\n\n#define DYNPTR_TYPE_FLAG_MASK\t(DYNPTR_TYPE_LOCAL | DYNPTR_TYPE_RINGBUF | DYNPTR_TYPE_SKB \\\n\t\t\t\t | DYNPTR_TYPE_XDP)\n\n \n#define BPF_BASE_TYPE_LIMIT\t(1UL << BPF_BASE_TYPE_BITS)\n\n \n#define BPF_TYPE_LIMIT\t\t(__BPF_TYPE_LAST_FLAG | (__BPF_TYPE_LAST_FLAG - 1))\n\n \nenum bpf_arg_type {\n\tARG_DONTCARE = 0,\t \n\n\t \n\tARG_CONST_MAP_PTR,\t \n\tARG_PTR_TO_MAP_KEY,\t \n\tARG_PTR_TO_MAP_VALUE,\t \n\n\t \n\tARG_PTR_TO_MEM,\t\t \n\n\tARG_CONST_SIZE,\t\t \n\tARG_CONST_SIZE_OR_ZERO,\t \n\n\tARG_PTR_TO_CTX,\t\t \n\tARG_ANYTHING,\t\t \n\tARG_PTR_TO_SPIN_LOCK,\t \n\tARG_PTR_TO_SOCK_COMMON,\t \n\tARG_PTR_TO_INT,\t\t \n\tARG_PTR_TO_LONG,\t \n\tARG_PTR_TO_SOCKET,\t \n\tARG_PTR_TO_BTF_ID,\t \n\tARG_PTR_TO_RINGBUF_MEM,\t \n\tARG_CONST_ALLOC_SIZE_OR_ZERO,\t \n\tARG_PTR_TO_BTF_ID_SOCK_COMMON,\t \n\tARG_PTR_TO_PERCPU_BTF_ID,\t \n\tARG_PTR_TO_FUNC,\t \n\tARG_PTR_TO_STACK,\t \n\tARG_PTR_TO_CONST_STR,\t \n\tARG_PTR_TO_TIMER,\t \n\tARG_PTR_TO_KPTR,\t \n\tARG_PTR_TO_DYNPTR,       \n\t__BPF_ARG_TYPE_MAX,\n\n\t \n\tARG_PTR_TO_MAP_VALUE_OR_NULL\t= PTR_MAYBE_NULL | ARG_PTR_TO_MAP_VALUE,\n\tARG_PTR_TO_MEM_OR_NULL\t\t= PTR_MAYBE_NULL | ARG_PTR_TO_MEM,\n\tARG_PTR_TO_CTX_OR_NULL\t\t= PTR_MAYBE_NULL | ARG_PTR_TO_CTX,\n\tARG_PTR_TO_SOCKET_OR_NULL\t= PTR_MAYBE_NULL | ARG_PTR_TO_SOCKET,\n\tARG_PTR_TO_STACK_OR_NULL\t= PTR_MAYBE_NULL | ARG_PTR_TO_STACK,\n\tARG_PTR_TO_BTF_ID_OR_NULL\t= PTR_MAYBE_NULL | ARG_PTR_TO_BTF_ID,\n\t \n\tARG_PTR_TO_UNINIT_MEM\t\t= MEM_UNINIT | ARG_PTR_TO_MEM,\n\t \n\tARG_PTR_TO_FIXED_SIZE_MEM\t= MEM_FIXED_SIZE | ARG_PTR_TO_MEM,\n\n\t \n\t__BPF_ARG_TYPE_LIMIT\t= BPF_TYPE_LIMIT,\n};\nstatic_assert(__BPF_ARG_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);\n\n \nenum bpf_return_type {\n\tRET_INTEGER,\t\t\t \n\tRET_VOID,\t\t\t \n\tRET_PTR_TO_MAP_VALUE,\t\t \n\tRET_PTR_TO_SOCKET,\t\t \n\tRET_PTR_TO_TCP_SOCK,\t\t \n\tRET_PTR_TO_SOCK_COMMON,\t\t \n\tRET_PTR_TO_MEM,\t\t\t \n\tRET_PTR_TO_MEM_OR_BTF_ID,\t \n\tRET_PTR_TO_BTF_ID,\t\t \n\t__BPF_RET_TYPE_MAX,\n\n\t \n\tRET_PTR_TO_MAP_VALUE_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_MAP_VALUE,\n\tRET_PTR_TO_SOCKET_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_SOCKET,\n\tRET_PTR_TO_TCP_SOCK_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_TCP_SOCK,\n\tRET_PTR_TO_SOCK_COMMON_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_SOCK_COMMON,\n\tRET_PTR_TO_RINGBUF_MEM_OR_NULL\t= PTR_MAYBE_NULL | MEM_RINGBUF | RET_PTR_TO_MEM,\n\tRET_PTR_TO_DYNPTR_MEM_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_MEM,\n\tRET_PTR_TO_BTF_ID_OR_NULL\t= PTR_MAYBE_NULL | RET_PTR_TO_BTF_ID,\n\tRET_PTR_TO_BTF_ID_TRUSTED\t= PTR_TRUSTED\t | RET_PTR_TO_BTF_ID,\n\n\t \n\t__BPF_RET_TYPE_LIMIT\t= BPF_TYPE_LIMIT,\n};\nstatic_assert(__BPF_RET_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);\n\n \nstruct bpf_func_proto {\n\tu64 (*func)(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\n\tbool gpl_only;\n\tbool pkt_access;\n\tbool might_sleep;\n\tenum bpf_return_type ret_type;\n\tunion {\n\t\tstruct {\n\t\t\tenum bpf_arg_type arg1_type;\n\t\t\tenum bpf_arg_type arg2_type;\n\t\t\tenum bpf_arg_type arg3_type;\n\t\t\tenum bpf_arg_type arg4_type;\n\t\t\tenum bpf_arg_type arg5_type;\n\t\t};\n\t\tenum bpf_arg_type arg_type[5];\n\t};\n\tunion {\n\t\tstruct {\n\t\t\tu32 *arg1_btf_id;\n\t\t\tu32 *arg2_btf_id;\n\t\t\tu32 *arg3_btf_id;\n\t\t\tu32 *arg4_btf_id;\n\t\t\tu32 *arg5_btf_id;\n\t\t};\n\t\tu32 *arg_btf_id[5];\n\t\tstruct {\n\t\t\tsize_t arg1_size;\n\t\t\tsize_t arg2_size;\n\t\t\tsize_t arg3_size;\n\t\t\tsize_t arg4_size;\n\t\t\tsize_t arg5_size;\n\t\t};\n\t\tsize_t arg_size[5];\n\t};\n\tint *ret_btf_id;  \n\tbool (*allowed)(const struct bpf_prog *prog);\n};\n\n \nstruct bpf_context;\n\nenum bpf_access_type {\n\tBPF_READ = 1,\n\tBPF_WRITE = 2\n};\n\n \n \nenum bpf_reg_type {\n\tNOT_INIT = 0,\t\t  \n\tSCALAR_VALUE,\t\t  \n\tPTR_TO_CTX,\t\t  \n\tCONST_PTR_TO_MAP,\t  \n\tPTR_TO_MAP_VALUE,\t  \n\tPTR_TO_MAP_KEY,\t\t  \n\tPTR_TO_STACK,\t\t  \n\tPTR_TO_PACKET_META,\t  \n\tPTR_TO_PACKET,\t\t  \n\tPTR_TO_PACKET_END,\t  \n\tPTR_TO_FLOW_KEYS,\t  \n\tPTR_TO_SOCKET,\t\t  \n\tPTR_TO_SOCK_COMMON,\t  \n\tPTR_TO_TCP_SOCK,\t  \n\tPTR_TO_TP_BUFFER,\t  \n\tPTR_TO_XDP_SOCK,\t  \n\t \n\tPTR_TO_BTF_ID,\n\t \n\tPTR_TO_MEM,\t\t  \n\tPTR_TO_BUF,\t\t  \n\tPTR_TO_FUNC,\t\t  \n\tCONST_PTR_TO_DYNPTR,\t  \n\t__BPF_REG_TYPE_MAX,\n\n\t \n\tPTR_TO_MAP_VALUE_OR_NULL\t= PTR_MAYBE_NULL | PTR_TO_MAP_VALUE,\n\tPTR_TO_SOCKET_OR_NULL\t\t= PTR_MAYBE_NULL | PTR_TO_SOCKET,\n\tPTR_TO_SOCK_COMMON_OR_NULL\t= PTR_MAYBE_NULL | PTR_TO_SOCK_COMMON,\n\tPTR_TO_TCP_SOCK_OR_NULL\t\t= PTR_MAYBE_NULL | PTR_TO_TCP_SOCK,\n\tPTR_TO_BTF_ID_OR_NULL\t\t= PTR_MAYBE_NULL | PTR_TO_BTF_ID,\n\n\t \n\t__BPF_REG_TYPE_LIMIT\t= BPF_TYPE_LIMIT,\n};\nstatic_assert(__BPF_REG_TYPE_MAX <= BPF_BASE_TYPE_LIMIT);\n\n \nstruct bpf_insn_access_aux {\n\tenum bpf_reg_type reg_type;\n\tunion {\n\t\tint ctx_field_size;\n\t\tstruct {\n\t\t\tstruct btf *btf;\n\t\t\tu32 btf_id;\n\t\t};\n\t};\n\tstruct bpf_verifier_log *log;  \n};\n\nstatic inline void\nbpf_ctx_record_field_size(struct bpf_insn_access_aux *aux, u32 size)\n{\n\taux->ctx_field_size = size;\n}\n\nstatic bool bpf_is_ldimm64(const struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_LD | BPF_IMM | BPF_DW);\n}\n\nstatic inline bool bpf_pseudo_func(const struct bpf_insn *insn)\n{\n\treturn bpf_is_ldimm64(insn) && insn->src_reg == BPF_PSEUDO_FUNC;\n}\n\nstruct bpf_prog_ops {\n\tint (*test_run)(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\tunion bpf_attr __user *uattr);\n};\n\nstruct bpf_reg_state;\nstruct bpf_verifier_ops {\n\t \n\tconst struct bpf_func_proto *\n\t(*get_func_proto)(enum bpf_func_id func_id,\n\t\t\t  const struct bpf_prog *prog);\n\n\t \n\tbool (*is_valid_access)(int off, int size, enum bpf_access_type type,\n\t\t\t\tconst struct bpf_prog *prog,\n\t\t\t\tstruct bpf_insn_access_aux *info);\n\tint (*gen_prologue)(struct bpf_insn *insn, bool direct_write,\n\t\t\t    const struct bpf_prog *prog);\n\tint (*gen_ld_abs)(const struct bpf_insn *orig,\n\t\t\t  struct bpf_insn *insn_buf);\n\tu32 (*convert_ctx_access)(enum bpf_access_type type,\n\t\t\t\t  const struct bpf_insn *src,\n\t\t\t\t  struct bpf_insn *dst,\n\t\t\t\t  struct bpf_prog *prog, u32 *target_size);\n\tint (*btf_struct_access)(struct bpf_verifier_log *log,\n\t\t\t\t const struct bpf_reg_state *reg,\n\t\t\t\t int off, int size);\n};\n\nstruct bpf_prog_offload_ops {\n\t \n\tint (*insn_hook)(struct bpf_verifier_env *env,\n\t\t\t int insn_idx, int prev_insn_idx);\n\tint (*finalize)(struct bpf_verifier_env *env);\n\t \n\tint (*replace_insn)(struct bpf_verifier_env *env, u32 off,\n\t\t\t    struct bpf_insn *insn);\n\tint (*remove_insns)(struct bpf_verifier_env *env, u32 off, u32 cnt);\n\t \n\tint (*prepare)(struct bpf_prog *prog);\n\tint (*translate)(struct bpf_prog *prog);\n\tvoid (*destroy)(struct bpf_prog *prog);\n};\n\nstruct bpf_prog_offload {\n\tstruct bpf_prog\t\t*prog;\n\tstruct net_device\t*netdev;\n\tstruct bpf_offload_dev\t*offdev;\n\tvoid\t\t\t*dev_priv;\n\tstruct list_head\toffloads;\n\tbool\t\t\tdev_state;\n\tbool\t\t\topt_failed;\n\tvoid\t\t\t*jited_image;\n\tu32\t\t\tjited_len;\n};\n\nenum bpf_cgroup_storage_type {\n\tBPF_CGROUP_STORAGE_SHARED,\n\tBPF_CGROUP_STORAGE_PERCPU,\n\t__BPF_CGROUP_STORAGE_MAX\n};\n\n#define MAX_BPF_CGROUP_STORAGE_TYPE __BPF_CGROUP_STORAGE_MAX\n\n \n#define MAX_BPF_FUNC_ARGS 12\n\n \n#define MAX_BPF_FUNC_REG_ARGS 5\n\n \n#define BTF_FMODEL_STRUCT_ARG\t\tBIT(0)\n\n \n#define BTF_FMODEL_SIGNED_ARG\t\tBIT(1)\n\nstruct btf_func_model {\n\tu8 ret_size;\n\tu8 ret_flags;\n\tu8 nr_args;\n\tu8 arg_size[MAX_BPF_FUNC_ARGS];\n\tu8 arg_flags[MAX_BPF_FUNC_ARGS];\n};\n\n \n#define BPF_TRAMP_F_RESTORE_REGS\tBIT(0)\n \n#define BPF_TRAMP_F_CALL_ORIG\t\tBIT(1)\n \n#define BPF_TRAMP_F_SKIP_FRAME\t\tBIT(2)\n \n#define BPF_TRAMP_F_IP_ARG\t\tBIT(3)\n \n#define BPF_TRAMP_F_RET_FENTRY_RET\tBIT(4)\n\n \n#define BPF_TRAMP_F_ORIG_STACK\t\tBIT(5)\n\n \n#define BPF_TRAMP_F_SHARE_IPMODIFY\tBIT(6)\n\n \n#define BPF_TRAMP_F_TAIL_CALL_CTX\tBIT(7)\n\n \nenum {\n#if defined(__s390x__)\n\tBPF_MAX_TRAMP_LINKS = 27,\n#else\n\tBPF_MAX_TRAMP_LINKS = 38,\n#endif\n};\n\nstruct bpf_tramp_links {\n\tstruct bpf_tramp_link *links[BPF_MAX_TRAMP_LINKS];\n\tint nr_links;\n};\n\nstruct bpf_tramp_run_ctx;\n\n \nstruct bpf_tramp_image;\nint arch_prepare_bpf_trampoline(struct bpf_tramp_image *tr, void *image, void *image_end,\n\t\t\t\tconst struct btf_func_model *m, u32 flags,\n\t\t\t\tstruct bpf_tramp_links *tlinks,\n\t\t\t\tvoid *orig_call);\nu64 notrace __bpf_prog_enter_sleepable_recur(struct bpf_prog *prog,\n\t\t\t\t\t     struct bpf_tramp_run_ctx *run_ctx);\nvoid notrace __bpf_prog_exit_sleepable_recur(struct bpf_prog *prog, u64 start,\n\t\t\t\t\t     struct bpf_tramp_run_ctx *run_ctx);\nvoid notrace __bpf_tramp_enter(struct bpf_tramp_image *tr);\nvoid notrace __bpf_tramp_exit(struct bpf_tramp_image *tr);\ntypedef u64 (*bpf_trampoline_enter_t)(struct bpf_prog *prog,\n\t\t\t\t      struct bpf_tramp_run_ctx *run_ctx);\ntypedef void (*bpf_trampoline_exit_t)(struct bpf_prog *prog, u64 start,\n\t\t\t\t      struct bpf_tramp_run_ctx *run_ctx);\nbpf_trampoline_enter_t bpf_trampoline_enter(const struct bpf_prog *prog);\nbpf_trampoline_exit_t bpf_trampoline_exit(const struct bpf_prog *prog);\n\nstruct bpf_ksym {\n\tunsigned long\t\t start;\n\tunsigned long\t\t end;\n\tchar\t\t\t name[KSYM_NAME_LEN];\n\tstruct list_head\t lnode;\n\tstruct latch_tree_node\t tnode;\n\tbool\t\t\t prog;\n};\n\nenum bpf_tramp_prog_type {\n\tBPF_TRAMP_FENTRY,\n\tBPF_TRAMP_FEXIT,\n\tBPF_TRAMP_MODIFY_RETURN,\n\tBPF_TRAMP_MAX,\n\tBPF_TRAMP_REPLACE,  \n};\n\nstruct bpf_tramp_image {\n\tvoid *image;\n\tstruct bpf_ksym ksym;\n\tstruct percpu_ref pcref;\n\tvoid *ip_after_call;\n\tvoid *ip_epilogue;\n\tunion {\n\t\tstruct rcu_head rcu;\n\t\tstruct work_struct work;\n\t};\n};\n\nstruct bpf_trampoline {\n\t \n\tstruct hlist_node hlist;\n\tstruct ftrace_ops *fops;\n\t \n\tstruct mutex mutex;\n\trefcount_t refcnt;\n\tu32 flags;\n\tu64 key;\n\tstruct {\n\t\tstruct btf_func_model model;\n\t\tvoid *addr;\n\t\tbool ftrace_managed;\n\t} func;\n\t \n\tstruct bpf_prog *extension_prog;\n\t \n\tstruct hlist_head progs_hlist[BPF_TRAMP_MAX];\n\t \n\tint progs_cnt[BPF_TRAMP_MAX];\n\t \n\tstruct bpf_tramp_image *cur_image;\n\tstruct module *mod;\n};\n\nstruct bpf_attach_target_info {\n\tstruct btf_func_model fmodel;\n\tlong tgt_addr;\n\tstruct module *tgt_mod;\n\tconst char *tgt_name;\n\tconst struct btf_type *tgt_type;\n};\n\n#define BPF_DISPATCHER_MAX 48  \n\nstruct bpf_dispatcher_prog {\n\tstruct bpf_prog *prog;\n\trefcount_t users;\n};\n\nstruct bpf_dispatcher {\n\t \n\tstruct mutex mutex;\n\tvoid *func;\n\tstruct bpf_dispatcher_prog progs[BPF_DISPATCHER_MAX];\n\tint num_progs;\n\tvoid *image;\n\tvoid *rw_image;\n\tu32 image_off;\n\tstruct bpf_ksym ksym;\n#ifdef CONFIG_HAVE_STATIC_CALL\n\tstruct static_call_key *sc_key;\n\tvoid *sc_tramp;\n#endif\n};\n\nstatic __always_inline __nocfi unsigned int bpf_dispatcher_nop_func(\n\tconst void *ctx,\n\tconst struct bpf_insn *insnsi,\n\tbpf_func_t bpf_func)\n{\n\treturn bpf_func(ctx, insnsi);\n}\n\n \nstruct bpf_dynptr_kern {\n\tvoid *data;\n\t \n\tu32 size;\n\tu32 offset;\n} __aligned(8);\n\nenum bpf_dynptr_type {\n\tBPF_DYNPTR_TYPE_INVALID,\n\t \n\tBPF_DYNPTR_TYPE_LOCAL,\n\t \n\tBPF_DYNPTR_TYPE_RINGBUF,\n\t \n\tBPF_DYNPTR_TYPE_SKB,\n\t \n\tBPF_DYNPTR_TYPE_XDP,\n};\n\nint bpf_dynptr_check_size(u32 size);\nu32 __bpf_dynptr_size(const struct bpf_dynptr_kern *ptr);\n\n#ifdef CONFIG_BPF_JIT\nint bpf_trampoline_link_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr);\nint bpf_trampoline_unlink_prog(struct bpf_tramp_link *link, struct bpf_trampoline *tr);\nstruct bpf_trampoline *bpf_trampoline_get(u64 key,\n\t\t\t\t\t  struct bpf_attach_target_info *tgt_info);\nvoid bpf_trampoline_put(struct bpf_trampoline *tr);\nint arch_prepare_bpf_dispatcher(void *image, void *buf, s64 *funcs, int num_funcs);\n\n \n#ifdef CONFIG_HAVE_STATIC_CALL\n\n#define __BPF_DISPATCHER_SC_INIT(_name)\t\t\t\t\\\n\t.sc_key = &STATIC_CALL_KEY(_name),\t\t\t\\\n\t.sc_tramp = STATIC_CALL_TRAMP_ADDR(_name),\n\n#define __BPF_DISPATCHER_SC(name)\t\t\t\t\\\n\tDEFINE_STATIC_CALL(bpf_dispatcher_##name##_call, bpf_dispatcher_nop_func)\n\n#define __BPF_DISPATCHER_CALL(name)\t\t\t\t\\\n\tstatic_call(bpf_dispatcher_##name##_call)(ctx, insnsi, bpf_func)\n\n#define __BPF_DISPATCHER_UPDATE(_d, _new)\t\t\t\\\n\t__static_call_update((_d)->sc_key, (_d)->sc_tramp, (_new))\n\n#else\n#define __BPF_DISPATCHER_SC_INIT(name)\n#define __BPF_DISPATCHER_SC(name)\n#define __BPF_DISPATCHER_CALL(name)\t\tbpf_func(ctx, insnsi)\n#define __BPF_DISPATCHER_UPDATE(_d, _new)\n#endif\n\n#define BPF_DISPATCHER_INIT(_name) {\t\t\t\t\\\n\t.mutex = __MUTEX_INITIALIZER(_name.mutex),\t\t\\\n\t.func = &_name##_func,\t\t\t\t\t\\\n\t.progs = {},\t\t\t\t\t\t\\\n\t.num_progs = 0,\t\t\t\t\t\t\\\n\t.image = NULL,\t\t\t\t\t\t\\\n\t.image_off = 0,\t\t\t\t\t\t\\\n\t.ksym = {\t\t\t\t\t\t\\\n\t\t.name  = #_name,\t\t\t\t\\\n\t\t.lnode = LIST_HEAD_INIT(_name.ksym.lnode),\t\\\n\t},\t\t\t\t\t\t\t\\\n\t__BPF_DISPATCHER_SC_INIT(_name##_call)\t\t\t\\\n}\n\n#define DEFINE_BPF_DISPATCHER(name)\t\t\t\t\t\\\n\t__BPF_DISPATCHER_SC(name);\t\t\t\t\t\\\n\tnoinline __nocfi unsigned int bpf_dispatcher_##name##_func(\t\\\n\t\tconst void *ctx,\t\t\t\t\t\\\n\t\tconst struct bpf_insn *insnsi,\t\t\t\t\\\n\t\tbpf_func_t bpf_func)\t\t\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn __BPF_DISPATCHER_CALL(name);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tEXPORT_SYMBOL(bpf_dispatcher_##name##_func);\t\t\t\\\n\tstruct bpf_dispatcher bpf_dispatcher_##name =\t\t\t\\\n\t\tBPF_DISPATCHER_INIT(bpf_dispatcher_##name);\n\n#define DECLARE_BPF_DISPATCHER(name)\t\t\t\t\t\\\n\tunsigned int bpf_dispatcher_##name##_func(\t\t\t\\\n\t\tconst void *ctx,\t\t\t\t\t\\\n\t\tconst struct bpf_insn *insnsi,\t\t\t\t\\\n\t\tbpf_func_t bpf_func);\t\t\t\t\t\\\n\textern struct bpf_dispatcher bpf_dispatcher_##name;\n\n#define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_##name##_func\n#define BPF_DISPATCHER_PTR(name) (&bpf_dispatcher_##name)\nvoid bpf_dispatcher_change_prog(struct bpf_dispatcher *d, struct bpf_prog *from,\n\t\t\t\tstruct bpf_prog *to);\n \nvoid bpf_image_ksym_add(void *data, struct bpf_ksym *ksym);\nvoid bpf_image_ksym_del(struct bpf_ksym *ksym);\nvoid bpf_ksym_add(struct bpf_ksym *ksym);\nvoid bpf_ksym_del(struct bpf_ksym *ksym);\nint bpf_jit_charge_modmem(u32 size);\nvoid bpf_jit_uncharge_modmem(u32 size);\nbool bpf_prog_has_trampoline(const struct bpf_prog *prog);\n#else\nstatic inline int bpf_trampoline_link_prog(struct bpf_tramp_link *link,\n\t\t\t\t\t   struct bpf_trampoline *tr)\n{\n\treturn -ENOTSUPP;\n}\nstatic inline int bpf_trampoline_unlink_prog(struct bpf_tramp_link *link,\n\t\t\t\t\t     struct bpf_trampoline *tr)\n{\n\treturn -ENOTSUPP;\n}\nstatic inline struct bpf_trampoline *bpf_trampoline_get(u64 key,\n\t\t\t\t\t\t\tstruct bpf_attach_target_info *tgt_info)\n{\n\treturn NULL;\n}\nstatic inline void bpf_trampoline_put(struct bpf_trampoline *tr) {}\n#define DEFINE_BPF_DISPATCHER(name)\n#define DECLARE_BPF_DISPATCHER(name)\n#define BPF_DISPATCHER_FUNC(name) bpf_dispatcher_nop_func\n#define BPF_DISPATCHER_PTR(name) NULL\nstatic inline void bpf_dispatcher_change_prog(struct bpf_dispatcher *d,\n\t\t\t\t\t      struct bpf_prog *from,\n\t\t\t\t\t      struct bpf_prog *to) {}\nstatic inline bool is_bpf_image_address(unsigned long address)\n{\n\treturn false;\n}\nstatic inline bool bpf_prog_has_trampoline(const struct bpf_prog *prog)\n{\n\treturn false;\n}\n#endif\n\nstruct bpf_func_info_aux {\n\tu16 linkage;\n\tbool unreliable;\n};\n\nenum bpf_jit_poke_reason {\n\tBPF_POKE_REASON_TAIL_CALL,\n};\n\n \nstruct bpf_jit_poke_descriptor {\n\tvoid *tailcall_target;\n\tvoid *tailcall_bypass;\n\tvoid *bypass_addr;\n\tvoid *aux;\n\tunion {\n\t\tstruct {\n\t\t\tstruct bpf_map *map;\n\t\t\tu32 key;\n\t\t} tail_call;\n\t};\n\tbool tailcall_target_stable;\n\tu8 adj_off;\n\tu16 reason;\n\tu32 insn_idx;\n};\n\n \nstruct bpf_ctx_arg_aux {\n\tu32 offset;\n\tenum bpf_reg_type reg_type;\n\tu32 btf_id;\n};\n\nstruct btf_mod_pair {\n\tstruct btf *btf;\n\tstruct module *module;\n};\n\nstruct bpf_kfunc_desc_tab;\n\nstruct bpf_prog_aux {\n\tatomic64_t refcnt;\n\tu32 used_map_cnt;\n\tu32 used_btf_cnt;\n\tu32 max_ctx_offset;\n\tu32 max_pkt_offset;\n\tu32 max_tp_access;\n\tu32 stack_depth;\n\tu32 id;\n\tu32 func_cnt;  \n\tu32 func_idx;  \n\tu32 attach_btf_id;  \n\tu32 ctx_arg_info_size;\n\tu32 max_rdonly_access;\n\tu32 max_rdwr_access;\n\tstruct btf *attach_btf;\n\tconst struct bpf_ctx_arg_aux *ctx_arg_info;\n\tstruct mutex dst_mutex;  \n\tstruct bpf_prog *dst_prog;\n\tstruct bpf_trampoline *dst_trampoline;\n\tenum bpf_prog_type saved_dst_prog_type;\n\tenum bpf_attach_type saved_dst_attach_type;\n\tbool verifier_zext;  \n\tbool dev_bound;  \n\tbool offload_requested;  \n\tbool attach_btf_trace;  \n\tbool func_proto_unreliable;\n\tbool sleepable;\n\tbool tail_call_reachable;\n\tbool xdp_has_frags;\n\t \n\tconst struct btf_type *attach_func_proto;\n\t \n\tconst char *attach_func_name;\n\tstruct bpf_prog **func;\n\tvoid *jit_data;  \n\tstruct bpf_jit_poke_descriptor *poke_tab;\n\tstruct bpf_kfunc_desc_tab *kfunc_tab;\n\tstruct bpf_kfunc_btf_tab *kfunc_btf_tab;\n\tu32 size_poke_tab;\n\tstruct bpf_ksym ksym;\n\tconst struct bpf_prog_ops *ops;\n\tstruct bpf_map **used_maps;\n\tstruct mutex used_maps_mutex;  \n\tstruct btf_mod_pair *used_btfs;\n\tstruct bpf_prog *prog;\n\tstruct user_struct *user;\n\tu64 load_time;  \n\tu32 verified_insns;\n\tint cgroup_atype;  \n\tstruct bpf_map *cgroup_storage[MAX_BPF_CGROUP_STORAGE_TYPE];\n\tchar name[BPF_OBJ_NAME_LEN];\n#ifdef CONFIG_SECURITY\n\tvoid *security;\n#endif\n\tstruct bpf_prog_offload *offload;\n\tstruct btf *btf;\n\tstruct bpf_func_info *func_info;\n\tstruct bpf_func_info_aux *func_info_aux;\n\t \n\tstruct bpf_line_info *linfo;\n\t \n\tvoid **jited_linfo;\n\tu32 func_info_cnt;\n\tu32 nr_linfo;\n\t \n\tu32 linfo_idx;\n\tstruct module *mod;\n\tu32 num_exentries;\n\tstruct exception_table_entry *extable;\n\tunion {\n\t\tstruct work_struct work;\n\t\tstruct rcu_head\trcu;\n\t};\n};\n\nstruct bpf_prog {\n\tu16\t\t\tpages;\t\t \n\tu16\t\t\tjited:1,\t \n\t\t\t\tjit_requested:1, \n\t\t\t\tgpl_compatible:1,  \n\t\t\t\tcb_access:1,\t \n\t\t\t\tdst_needed:1,\t \n\t\t\t\tblinding_requested:1,  \n\t\t\t\tblinded:1,\t \n\t\t\t\tis_func:1,\t \n\t\t\t\tkprobe_override:1,  \n\t\t\t\thas_callchain_buf:1,  \n\t\t\t\tenforce_expected_attach_type:1,  \n\t\t\t\tcall_get_stack:1,  \n\t\t\t\tcall_get_func_ip:1,  \n\t\t\t\ttstamp_type_access:1;  \n\tenum bpf_prog_type\ttype;\t\t \n\tenum bpf_attach_type\texpected_attach_type;  \n\tu32\t\t\tlen;\t\t \n\tu32\t\t\tjited_len;\t \n\tu8\t\t\ttag[BPF_TAG_SIZE];\n\tstruct bpf_prog_stats __percpu *stats;\n\tint __percpu\t\t*active;\n\tunsigned int\t\t(*bpf_func)(const void *ctx,\n\t\t\t\t\t    const struct bpf_insn *insn);\n\tstruct bpf_prog_aux\t*aux;\t\t \n\tstruct sock_fprog_kern\t*orig_prog;\t \n\t \n\tunion {\n\t\tDECLARE_FLEX_ARRAY(struct sock_filter, insns);\n\t\tDECLARE_FLEX_ARRAY(struct bpf_insn, insnsi);\n\t};\n};\n\nstruct bpf_array_aux {\n\t \n\tstruct list_head poke_progs;\n\tstruct bpf_map *map;\n\tstruct mutex poke_mutex;\n\tstruct work_struct work;\n};\n\nstruct bpf_link {\n\tatomic64_t refcnt;\n\tu32 id;\n\tenum bpf_link_type type;\n\tconst struct bpf_link_ops *ops;\n\tstruct bpf_prog *prog;\n\tstruct work_struct work;\n};\n\nstruct bpf_link_ops {\n\tvoid (*release)(struct bpf_link *link);\n\tvoid (*dealloc)(struct bpf_link *link);\n\tint (*detach)(struct bpf_link *link);\n\tint (*update_prog)(struct bpf_link *link, struct bpf_prog *new_prog,\n\t\t\t   struct bpf_prog *old_prog);\n\tvoid (*show_fdinfo)(const struct bpf_link *link, struct seq_file *seq);\n\tint (*fill_link_info)(const struct bpf_link *link,\n\t\t\t      struct bpf_link_info *info);\n\tint (*update_map)(struct bpf_link *link, struct bpf_map *new_map,\n\t\t\t  struct bpf_map *old_map);\n};\n\nstruct bpf_tramp_link {\n\tstruct bpf_link link;\n\tstruct hlist_node tramp_hlist;\n\tu64 cookie;\n};\n\nstruct bpf_shim_tramp_link {\n\tstruct bpf_tramp_link link;\n\tstruct bpf_trampoline *trampoline;\n};\n\nstruct bpf_tracing_link {\n\tstruct bpf_tramp_link link;\n\tenum bpf_attach_type attach_type;\n\tstruct bpf_trampoline *trampoline;\n\tstruct bpf_prog *tgt_prog;\n};\n\nstruct bpf_link_primer {\n\tstruct bpf_link *link;\n\tstruct file *file;\n\tint fd;\n\tu32 id;\n};\n\nstruct bpf_struct_ops_value;\nstruct btf_member;\n\n#define BPF_STRUCT_OPS_MAX_NR_MEMBERS 64\n \nstruct bpf_struct_ops {\n\tconst struct bpf_verifier_ops *verifier_ops;\n\tint (*init)(struct btf *btf);\n\tint (*check_member)(const struct btf_type *t,\n\t\t\t    const struct btf_member *member,\n\t\t\t    const struct bpf_prog *prog);\n\tint (*init_member)(const struct btf_type *t,\n\t\t\t   const struct btf_member *member,\n\t\t\t   void *kdata, const void *udata);\n\tint (*reg)(void *kdata);\n\tvoid (*unreg)(void *kdata);\n\tint (*update)(void *kdata, void *old_kdata);\n\tint (*validate)(void *kdata);\n\tconst struct btf_type *type;\n\tconst struct btf_type *value_type;\n\tconst char *name;\n\tstruct btf_func_model func_models[BPF_STRUCT_OPS_MAX_NR_MEMBERS];\n\tu32 type_id;\n\tu32 value_id;\n};\n\n#if defined(CONFIG_BPF_JIT) && defined(CONFIG_BPF_SYSCALL)\n#define BPF_MODULE_OWNER ((void *)((0xeB9FUL << 2) + POISON_POINTER_DELTA))\nconst struct bpf_struct_ops *bpf_struct_ops_find(u32 type_id);\nvoid bpf_struct_ops_init(struct btf *btf, struct bpf_verifier_log *log);\nbool bpf_struct_ops_get(const void *kdata);\nvoid bpf_struct_ops_put(const void *kdata);\nint bpf_struct_ops_map_sys_lookup_elem(struct bpf_map *map, void *key,\n\t\t\t\t       void *value);\nint bpf_struct_ops_prepare_trampoline(struct bpf_tramp_links *tlinks,\n\t\t\t\t      struct bpf_tramp_link *link,\n\t\t\t\t      const struct btf_func_model *model,\n\t\t\t\t      void *image, void *image_end);\nstatic inline bool bpf_try_module_get(const void *data, struct module *owner)\n{\n\tif (owner == BPF_MODULE_OWNER)\n\t\treturn bpf_struct_ops_get(data);\n\telse\n\t\treturn try_module_get(owner);\n}\nstatic inline void bpf_module_put(const void *data, struct module *owner)\n{\n\tif (owner == BPF_MODULE_OWNER)\n\t\tbpf_struct_ops_put(data);\n\telse\n\t\tmodule_put(owner);\n}\nint bpf_struct_ops_link_create(union bpf_attr *attr);\n\n#ifdef CONFIG_NET\n \nstruct bpf_dummy_ops_state {\n\tint val;\n};\n\nstruct bpf_dummy_ops {\n\tint (*test_1)(struct bpf_dummy_ops_state *cb);\n\tint (*test_2)(struct bpf_dummy_ops_state *cb, int a1, unsigned short a2,\n\t\t      char a3, unsigned long a4);\n\tint (*test_sleepable)(struct bpf_dummy_ops_state *cb);\n};\n\nint bpf_struct_ops_test_run(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t    union bpf_attr __user *uattr);\n#endif\n#else\nstatic inline const struct bpf_struct_ops *bpf_struct_ops_find(u32 type_id)\n{\n\treturn NULL;\n}\nstatic inline void bpf_struct_ops_init(struct btf *btf,\n\t\t\t\t       struct bpf_verifier_log *log)\n{\n}\nstatic inline bool bpf_try_module_get(const void *data, struct module *owner)\n{\n\treturn try_module_get(owner);\n}\nstatic inline void bpf_module_put(const void *data, struct module *owner)\n{\n\tmodule_put(owner);\n}\nstatic inline int bpf_struct_ops_map_sys_lookup_elem(struct bpf_map *map,\n\t\t\t\t\t\t     void *key,\n\t\t\t\t\t\t     void *value)\n{\n\treturn -EINVAL;\n}\nstatic inline int bpf_struct_ops_link_create(union bpf_attr *attr)\n{\n\treturn -EOPNOTSUPP;\n}\n\n#endif\n\n#if defined(CONFIG_CGROUP_BPF) && defined(CONFIG_BPF_LSM)\nint bpf_trampoline_link_cgroup_shim(struct bpf_prog *prog,\n\t\t\t\t    int cgroup_atype);\nvoid bpf_trampoline_unlink_cgroup_shim(struct bpf_prog *prog);\n#else\nstatic inline int bpf_trampoline_link_cgroup_shim(struct bpf_prog *prog,\n\t\t\t\t\t\t  int cgroup_atype)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline void bpf_trampoline_unlink_cgroup_shim(struct bpf_prog *prog)\n{\n}\n#endif\n\nstruct bpf_array {\n\tstruct bpf_map map;\n\tu32 elem_size;\n\tu32 index_mask;\n\tstruct bpf_array_aux *aux;\n\tunion {\n\t\tDECLARE_FLEX_ARRAY(char, value) __aligned(8);\n\t\tDECLARE_FLEX_ARRAY(void *, ptrs) __aligned(8);\n\t\tDECLARE_FLEX_ARRAY(void __percpu *, pptrs) __aligned(8);\n\t};\n};\n\n#define BPF_COMPLEXITY_LIMIT_INSNS      1000000  \n#define MAX_TAIL_CALL_CNT 33\n\n \nenum {\n\tBPF_MAX_LOOPS = 8 * 1024 * 1024,\n};\n\n#define BPF_F_ACCESS_MASK\t(BPF_F_RDONLY |\t\t\\\n\t\t\t\t BPF_F_RDONLY_PROG |\t\\\n\t\t\t\t BPF_F_WRONLY |\t\t\\\n\t\t\t\t BPF_F_WRONLY_PROG)\n\n#define BPF_MAP_CAN_READ\tBIT(0)\n#define BPF_MAP_CAN_WRITE\tBIT(1)\n\n \n#define BPF_MAX_USER_RINGBUF_SAMPLES (128 * 1024)\n\nstatic inline u32 bpf_map_flags_to_cap(struct bpf_map *map)\n{\n\tu32 access_flags = map->map_flags & (BPF_F_RDONLY_PROG | BPF_F_WRONLY_PROG);\n\n\t \n\tif (access_flags & BPF_F_RDONLY_PROG)\n\t\treturn BPF_MAP_CAN_READ;\n\telse if (access_flags & BPF_F_WRONLY_PROG)\n\t\treturn BPF_MAP_CAN_WRITE;\n\telse\n\t\treturn BPF_MAP_CAN_READ | BPF_MAP_CAN_WRITE;\n}\n\nstatic inline bool bpf_map_flags_access_ok(u32 access_flags)\n{\n\treturn (access_flags & (BPF_F_RDONLY_PROG | BPF_F_WRONLY_PROG)) !=\n\t       (BPF_F_RDONLY_PROG | BPF_F_WRONLY_PROG);\n}\n\nstruct bpf_event_entry {\n\tstruct perf_event *event;\n\tstruct file *perf_file;\n\tstruct file *map_file;\n\tstruct rcu_head rcu;\n};\n\nstatic inline bool map_type_contains_progs(struct bpf_map *map)\n{\n\treturn map->map_type == BPF_MAP_TYPE_PROG_ARRAY ||\n\t       map->map_type == BPF_MAP_TYPE_DEVMAP ||\n\t       map->map_type == BPF_MAP_TYPE_CPUMAP;\n}\n\nbool bpf_prog_map_compatible(struct bpf_map *map, const struct bpf_prog *fp);\nint bpf_prog_calc_tag(struct bpf_prog *fp);\n\nconst struct bpf_func_proto *bpf_get_trace_printk_proto(void);\nconst struct bpf_func_proto *bpf_get_trace_vprintk_proto(void);\n\ntypedef unsigned long (*bpf_ctx_copy_t)(void *dst, const void *src,\n\t\t\t\t\tunsigned long off, unsigned long len);\ntypedef u32 (*bpf_convert_ctx_access_t)(enum bpf_access_type type,\n\t\t\t\t\tconst struct bpf_insn *src,\n\t\t\t\t\tstruct bpf_insn *dst,\n\t\t\t\t\tstruct bpf_prog *prog,\n\t\t\t\t\tu32 *target_size);\n\nu64 bpf_event_output(struct bpf_map *map, u64 flags, void *meta, u64 meta_size,\n\t\t     void *ctx, u64 ctx_size, bpf_ctx_copy_t ctx_copy);\n\n \nstruct bpf_prog_array_item {\n\tstruct bpf_prog *prog;\n\tunion {\n\t\tstruct bpf_cgroup_storage *cgroup_storage[MAX_BPF_CGROUP_STORAGE_TYPE];\n\t\tu64 bpf_cookie;\n\t};\n};\n\nstruct bpf_prog_array {\n\tstruct rcu_head rcu;\n\tstruct bpf_prog_array_item items[];\n};\n\nstruct bpf_empty_prog_array {\n\tstruct bpf_prog_array hdr;\n\tstruct bpf_prog *null_prog;\n};\n\n \nextern struct bpf_empty_prog_array bpf_empty_prog_array;\n\nstruct bpf_prog_array *bpf_prog_array_alloc(u32 prog_cnt, gfp_t flags);\nvoid bpf_prog_array_free(struct bpf_prog_array *progs);\n \nvoid bpf_prog_array_free_sleepable(struct bpf_prog_array *progs);\nint bpf_prog_array_length(struct bpf_prog_array *progs);\nbool bpf_prog_array_is_empty(struct bpf_prog_array *array);\nint bpf_prog_array_copy_to_user(struct bpf_prog_array *progs,\n\t\t\t\t__u32 __user *prog_ids, u32 cnt);\n\nvoid bpf_prog_array_delete_safe(struct bpf_prog_array *progs,\n\t\t\t\tstruct bpf_prog *old_prog);\nint bpf_prog_array_delete_safe_at(struct bpf_prog_array *array, int index);\nint bpf_prog_array_update_at(struct bpf_prog_array *array, int index,\n\t\t\t     struct bpf_prog *prog);\nint bpf_prog_array_copy_info(struct bpf_prog_array *array,\n\t\t\t     u32 *prog_ids, u32 request_cnt,\n\t\t\t     u32 *prog_cnt);\nint bpf_prog_array_copy(struct bpf_prog_array *old_array,\n\t\t\tstruct bpf_prog *exclude_prog,\n\t\t\tstruct bpf_prog *include_prog,\n\t\t\tu64 bpf_cookie,\n\t\t\tstruct bpf_prog_array **new_array);\n\nstruct bpf_run_ctx {};\n\nstruct bpf_cg_run_ctx {\n\tstruct bpf_run_ctx run_ctx;\n\tconst struct bpf_prog_array_item *prog_item;\n\tint retval;\n};\n\nstruct bpf_trace_run_ctx {\n\tstruct bpf_run_ctx run_ctx;\n\tu64 bpf_cookie;\n\tbool is_uprobe;\n};\n\nstruct bpf_tramp_run_ctx {\n\tstruct bpf_run_ctx run_ctx;\n\tu64 bpf_cookie;\n\tstruct bpf_run_ctx *saved_run_ctx;\n};\n\nstatic inline struct bpf_run_ctx *bpf_set_run_ctx(struct bpf_run_ctx *new_ctx)\n{\n\tstruct bpf_run_ctx *old_ctx = NULL;\n\n#ifdef CONFIG_BPF_SYSCALL\n\told_ctx = current->bpf_ctx;\n\tcurrent->bpf_ctx = new_ctx;\n#endif\n\treturn old_ctx;\n}\n\nstatic inline void bpf_reset_run_ctx(struct bpf_run_ctx *old_ctx)\n{\n#ifdef CONFIG_BPF_SYSCALL\n\tcurrent->bpf_ctx = old_ctx;\n#endif\n}\n\n \n#define BPF_RET_BIND_NO_CAP_NET_BIND_SERVICE\t\t\t(1 << 0)\n \n#define BPF_RET_SET_CN\t\t\t\t\t\t(1 << 0)\n\ntypedef u32 (*bpf_prog_run_fn)(const struct bpf_prog *prog, const void *ctx);\n\nstatic __always_inline u32\nbpf_prog_run_array(const struct bpf_prog_array *array,\n\t\t   const void *ctx, bpf_prog_run_fn run_prog)\n{\n\tconst struct bpf_prog_array_item *item;\n\tconst struct bpf_prog *prog;\n\tstruct bpf_run_ctx *old_run_ctx;\n\tstruct bpf_trace_run_ctx run_ctx;\n\tu32 ret = 1;\n\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held(), \"no rcu lock held\");\n\n\tif (unlikely(!array))\n\t\treturn ret;\n\n\trun_ctx.is_uprobe = false;\n\n\tmigrate_disable();\n\told_run_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\titem = &array->items[0];\n\twhile ((prog = READ_ONCE(item->prog))) {\n\t\trun_ctx.bpf_cookie = item->bpf_cookie;\n\t\tret &= run_prog(prog, ctx);\n\t\titem++;\n\t}\n\tbpf_reset_run_ctx(old_run_ctx);\n\tmigrate_enable();\n\treturn ret;\n}\n\n \nstatic __always_inline u32\nbpf_prog_run_array_uprobe(const struct bpf_prog_array __rcu *array_rcu,\n\t\t\t  const void *ctx, bpf_prog_run_fn run_prog)\n{\n\tconst struct bpf_prog_array_item *item;\n\tconst struct bpf_prog *prog;\n\tconst struct bpf_prog_array *array;\n\tstruct bpf_run_ctx *old_run_ctx;\n\tstruct bpf_trace_run_ctx run_ctx;\n\tu32 ret = 1;\n\n\tmight_fault();\n\n\trcu_read_lock_trace();\n\tmigrate_disable();\n\n\trun_ctx.is_uprobe = true;\n\n\tarray = rcu_dereference_check(array_rcu, rcu_read_lock_trace_held());\n\tif (unlikely(!array))\n\t\tgoto out;\n\told_run_ctx = bpf_set_run_ctx(&run_ctx.run_ctx);\n\titem = &array->items[0];\n\twhile ((prog = READ_ONCE(item->prog))) {\n\t\tif (!prog->aux->sleepable)\n\t\t\trcu_read_lock();\n\n\t\trun_ctx.bpf_cookie = item->bpf_cookie;\n\t\tret &= run_prog(prog, ctx);\n\t\titem++;\n\n\t\tif (!prog->aux->sleepable)\n\t\t\trcu_read_unlock();\n\t}\n\tbpf_reset_run_ctx(old_run_ctx);\nout:\n\tmigrate_enable();\n\trcu_read_unlock_trace();\n\treturn ret;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nDECLARE_PER_CPU(int, bpf_prog_active);\nextern struct mutex bpf_stats_enabled_mutex;\n\n \nstatic inline void bpf_disable_instrumentation(void)\n{\n\tmigrate_disable();\n\tthis_cpu_inc(bpf_prog_active);\n}\n\nstatic inline void bpf_enable_instrumentation(void)\n{\n\tthis_cpu_dec(bpf_prog_active);\n\tmigrate_enable();\n}\n\nextern const struct file_operations bpf_map_fops;\nextern const struct file_operations bpf_prog_fops;\nextern const struct file_operations bpf_iter_fops;\n\n#define BPF_PROG_TYPE(_id, _name, prog_ctx_type, kern_ctx_type) \\\n\textern const struct bpf_prog_ops _name ## _prog_ops; \\\n\textern const struct bpf_verifier_ops _name ## _verifier_ops;\n#define BPF_MAP_TYPE(_id, _ops) \\\n\textern const struct bpf_map_ops _ops;\n#define BPF_LINK_TYPE(_id, _name)\n#include <linux/bpf_types.h>\n#undef BPF_PROG_TYPE\n#undef BPF_MAP_TYPE\n#undef BPF_LINK_TYPE\n\nextern const struct bpf_prog_ops bpf_offload_prog_ops;\nextern const struct bpf_verifier_ops tc_cls_act_analyzer_ops;\nextern const struct bpf_verifier_ops xdp_analyzer_ops;\n\nstruct bpf_prog *bpf_prog_get(u32 ufd);\nstruct bpf_prog *bpf_prog_get_type_dev(u32 ufd, enum bpf_prog_type type,\n\t\t\t\t       bool attach_drv);\nvoid bpf_prog_add(struct bpf_prog *prog, int i);\nvoid bpf_prog_sub(struct bpf_prog *prog, int i);\nvoid bpf_prog_inc(struct bpf_prog *prog);\nstruct bpf_prog * __must_check bpf_prog_inc_not_zero(struct bpf_prog *prog);\nvoid bpf_prog_put(struct bpf_prog *prog);\n\nvoid bpf_prog_free_id(struct bpf_prog *prog);\nvoid bpf_map_free_id(struct bpf_map *map);\n\nstruct btf_field *btf_record_find(const struct btf_record *rec,\n\t\t\t\t  u32 offset, u32 field_mask);\nvoid btf_record_free(struct btf_record *rec);\nvoid bpf_map_free_record(struct bpf_map *map);\nstruct btf_record *btf_record_dup(const struct btf_record *rec);\nbool btf_record_equal(const struct btf_record *rec_a, const struct btf_record *rec_b);\nvoid bpf_obj_free_timer(const struct btf_record *rec, void *obj);\nvoid bpf_obj_free_fields(const struct btf_record *rec, void *obj);\n\nstruct bpf_map *bpf_map_get(u32 ufd);\nstruct bpf_map *bpf_map_get_with_uref(u32 ufd);\nstruct bpf_map *__bpf_map_get(struct fd f);\nvoid bpf_map_inc(struct bpf_map *map);\nvoid bpf_map_inc_with_uref(struct bpf_map *map);\nstruct bpf_map *__bpf_map_inc_not_zero(struct bpf_map *map, bool uref);\nstruct bpf_map * __must_check bpf_map_inc_not_zero(struct bpf_map *map);\nvoid bpf_map_put_with_uref(struct bpf_map *map);\nvoid bpf_map_put(struct bpf_map *map);\nvoid *bpf_map_area_alloc(u64 size, int numa_node);\nvoid *bpf_map_area_mmapable_alloc(u64 size, int numa_node);\nvoid bpf_map_area_free(void *base);\nbool bpf_map_write_active(const struct bpf_map *map);\nvoid bpf_map_init_from_attr(struct bpf_map *map, union bpf_attr *attr);\nint  generic_map_lookup_batch(struct bpf_map *map,\n\t\t\t      const union bpf_attr *attr,\n\t\t\t      union bpf_attr __user *uattr);\nint  generic_map_update_batch(struct bpf_map *map, struct file *map_file,\n\t\t\t      const union bpf_attr *attr,\n\t\t\t      union bpf_attr __user *uattr);\nint  generic_map_delete_batch(struct bpf_map *map,\n\t\t\t      const union bpf_attr *attr,\n\t\t\t      union bpf_attr __user *uattr);\nstruct bpf_map *bpf_map_get_curr_or_next(u32 *id);\nstruct bpf_prog *bpf_prog_get_curr_or_next(u32 *id);\n\n#ifdef CONFIG_MEMCG_KMEM\nvoid *bpf_map_kmalloc_node(const struct bpf_map *map, size_t size, gfp_t flags,\n\t\t\t   int node);\nvoid *bpf_map_kzalloc(const struct bpf_map *map, size_t size, gfp_t flags);\nvoid *bpf_map_kvcalloc(struct bpf_map *map, size_t n, size_t size,\n\t\t       gfp_t flags);\nvoid __percpu *bpf_map_alloc_percpu(const struct bpf_map *map, size_t size,\n\t\t\t\t    size_t align, gfp_t flags);\n#else\nstatic inline void *\nbpf_map_kmalloc_node(const struct bpf_map *map, size_t size, gfp_t flags,\n\t\t     int node)\n{\n\treturn kmalloc_node(size, flags, node);\n}\n\nstatic inline void *\nbpf_map_kzalloc(const struct bpf_map *map, size_t size, gfp_t flags)\n{\n\treturn kzalloc(size, flags);\n}\n\nstatic inline void *\nbpf_map_kvcalloc(struct bpf_map *map, size_t n, size_t size, gfp_t flags)\n{\n\treturn kvcalloc(n, size, flags);\n}\n\nstatic inline void __percpu *\nbpf_map_alloc_percpu(const struct bpf_map *map, size_t size, size_t align,\n\t\t     gfp_t flags)\n{\n\treturn __alloc_percpu_gfp(size, align, flags);\n}\n#endif\n\nstatic inline int\nbpf_map_init_elem_count(struct bpf_map *map)\n{\n\tsize_t size = sizeof(*map->elem_count), align = size;\n\tgfp_t flags = GFP_USER | __GFP_NOWARN;\n\n\tmap->elem_count = bpf_map_alloc_percpu(map, size, align, flags);\n\tif (!map->elem_count)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic inline void\nbpf_map_free_elem_count(struct bpf_map *map)\n{\n\tfree_percpu(map->elem_count);\n}\n\nstatic inline void bpf_map_inc_elem_count(struct bpf_map *map)\n{\n\tthis_cpu_inc(*map->elem_count);\n}\n\nstatic inline void bpf_map_dec_elem_count(struct bpf_map *map)\n{\n\tthis_cpu_dec(*map->elem_count);\n}\n\nextern int sysctl_unprivileged_bpf_disabled;\n\nstatic inline bool bpf_allow_ptr_leaks(void)\n{\n\treturn perfmon_capable();\n}\n\nstatic inline bool bpf_allow_uninit_stack(void)\n{\n\treturn perfmon_capable();\n}\n\nstatic inline bool bpf_bypass_spec_v1(void)\n{\n\treturn perfmon_capable();\n}\n\nstatic inline bool bpf_bypass_spec_v4(void)\n{\n\treturn perfmon_capable();\n}\n\nint bpf_map_new_fd(struct bpf_map *map, int flags);\nint bpf_prog_new_fd(struct bpf_prog *prog);\n\nvoid bpf_link_init(struct bpf_link *link, enum bpf_link_type type,\n\t\t   const struct bpf_link_ops *ops, struct bpf_prog *prog);\nint bpf_link_prime(struct bpf_link *link, struct bpf_link_primer *primer);\nint bpf_link_settle(struct bpf_link_primer *primer);\nvoid bpf_link_cleanup(struct bpf_link_primer *primer);\nvoid bpf_link_inc(struct bpf_link *link);\nvoid bpf_link_put(struct bpf_link *link);\nint bpf_link_new_fd(struct bpf_link *link);\nstruct bpf_link *bpf_link_get_from_fd(u32 ufd);\nstruct bpf_link *bpf_link_get_curr_or_next(u32 *id);\n\nint bpf_obj_pin_user(u32 ufd, int path_fd, const char __user *pathname);\nint bpf_obj_get_user(int path_fd, const char __user *pathname, int flags);\n\n#define BPF_ITER_FUNC_PREFIX \"bpf_iter_\"\n#define DEFINE_BPF_ITER_FUNC(target, args...)\t\t\t\\\n\textern int bpf_iter_ ## target(args);\t\t\t\\\n\tint __init bpf_iter_ ## target(args) { return 0; }\n\n \nenum bpf_iter_task_type {\n\tBPF_TASK_ITER_ALL = 0,\n\tBPF_TASK_ITER_TID,\n\tBPF_TASK_ITER_TGID,\n};\n\nstruct bpf_iter_aux_info {\n\t \n\tstruct bpf_map *map;\n\n\t \n\tstruct {\n\t\tstruct cgroup *start;  \n\t\tenum bpf_cgroup_iter_order order;\n\t} cgroup;\n\tstruct {\n\t\tenum bpf_iter_task_type\ttype;\n\t\tu32 pid;\n\t} task;\n};\n\ntypedef int (*bpf_iter_attach_target_t)(struct bpf_prog *prog,\n\t\t\t\t\tunion bpf_iter_link_info *linfo,\n\t\t\t\t\tstruct bpf_iter_aux_info *aux);\ntypedef void (*bpf_iter_detach_target_t)(struct bpf_iter_aux_info *aux);\ntypedef void (*bpf_iter_show_fdinfo_t) (const struct bpf_iter_aux_info *aux,\n\t\t\t\t\tstruct seq_file *seq);\ntypedef int (*bpf_iter_fill_link_info_t)(const struct bpf_iter_aux_info *aux,\n\t\t\t\t\t struct bpf_link_info *info);\ntypedef const struct bpf_func_proto *\n(*bpf_iter_get_func_proto_t)(enum bpf_func_id func_id,\n\t\t\t     const struct bpf_prog *prog);\n\nenum bpf_iter_feature {\n\tBPF_ITER_RESCHED\t= BIT(0),\n};\n\n#define BPF_ITER_CTX_ARG_MAX 2\nstruct bpf_iter_reg {\n\tconst char *target;\n\tbpf_iter_attach_target_t attach_target;\n\tbpf_iter_detach_target_t detach_target;\n\tbpf_iter_show_fdinfo_t show_fdinfo;\n\tbpf_iter_fill_link_info_t fill_link_info;\n\tbpf_iter_get_func_proto_t get_func_proto;\n\tu32 ctx_arg_info_size;\n\tu32 feature;\n\tstruct bpf_ctx_arg_aux ctx_arg_info[BPF_ITER_CTX_ARG_MAX];\n\tconst struct bpf_iter_seq_info *seq_info;\n};\n\nstruct bpf_iter_meta {\n\t__bpf_md_ptr(struct seq_file *, seq);\n\tu64 session_id;\n\tu64 seq_num;\n};\n\nstruct bpf_iter__bpf_map_elem {\n\t__bpf_md_ptr(struct bpf_iter_meta *, meta);\n\t__bpf_md_ptr(struct bpf_map *, map);\n\t__bpf_md_ptr(void *, key);\n\t__bpf_md_ptr(void *, value);\n};\n\nint bpf_iter_reg_target(const struct bpf_iter_reg *reg_info);\nvoid bpf_iter_unreg_target(const struct bpf_iter_reg *reg_info);\nbool bpf_iter_prog_supported(struct bpf_prog *prog);\nconst struct bpf_func_proto *\nbpf_iter_get_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog);\nint bpf_iter_link_attach(const union bpf_attr *attr, bpfptr_t uattr, struct bpf_prog *prog);\nint bpf_iter_new_fd(struct bpf_link *link);\nbool bpf_link_is_iter(struct bpf_link *link);\nstruct bpf_prog *bpf_iter_get_info(struct bpf_iter_meta *meta, bool in_stop);\nint bpf_iter_run_prog(struct bpf_prog *prog, void *ctx);\nvoid bpf_iter_map_show_fdinfo(const struct bpf_iter_aux_info *aux,\n\t\t\t      struct seq_file *seq);\nint bpf_iter_map_fill_link_info(const struct bpf_iter_aux_info *aux,\n\t\t\t\tstruct bpf_link_info *info);\n\nint map_set_for_each_callback_args(struct bpf_verifier_env *env,\n\t\t\t\t   struct bpf_func_state *caller,\n\t\t\t\t   struct bpf_func_state *callee);\n\nint bpf_percpu_hash_copy(struct bpf_map *map, void *key, void *value);\nint bpf_percpu_array_copy(struct bpf_map *map, void *key, void *value);\nint bpf_percpu_hash_update(struct bpf_map *map, void *key, void *value,\n\t\t\t   u64 flags);\nint bpf_percpu_array_update(struct bpf_map *map, void *key, void *value,\n\t\t\t    u64 flags);\n\nint bpf_stackmap_copy(struct bpf_map *map, void *key, void *value);\n\nint bpf_fd_array_map_update_elem(struct bpf_map *map, struct file *map_file,\n\t\t\t\t void *key, void *value, u64 map_flags);\nint bpf_fd_array_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);\nint bpf_fd_htab_map_update_elem(struct bpf_map *map, struct file *map_file,\n\t\t\t\tvoid *key, void *value, u64 map_flags);\nint bpf_fd_htab_map_lookup_elem(struct bpf_map *map, void *key, u32 *value);\n\nint bpf_get_file_flag(int flags);\nint bpf_check_uarg_tail_zero(bpfptr_t uaddr, size_t expected_size,\n\t\t\t     size_t actual_size);\n\n \nint bpf_check(struct bpf_prog **fp, union bpf_attr *attr, bpfptr_t uattr, u32 uattr_size);\n\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\nvoid bpf_patch_call_args(struct bpf_insn *insn, u32 stack_depth);\n#endif\n\nstruct btf *bpf_get_btf_vmlinux(void);\n\n \nstruct xdp_frame;\nstruct sk_buff;\nstruct bpf_dtab_netdev;\nstruct bpf_cpu_map_entry;\n\nvoid __dev_flush(void);\nint dev_xdp_enqueue(struct net_device *dev, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx);\nint dev_map_enqueue(struct bpf_dtab_netdev *dst, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx);\nint dev_map_enqueue_multi(struct xdp_frame *xdpf, struct net_device *dev_rx,\n\t\t\t  struct bpf_map *map, bool exclude_ingress);\nint dev_map_generic_redirect(struct bpf_dtab_netdev *dst, struct sk_buff *skb,\n\t\t\t     struct bpf_prog *xdp_prog);\nint dev_map_redirect_multi(struct net_device *dev, struct sk_buff *skb,\n\t\t\t   struct bpf_prog *xdp_prog, struct bpf_map *map,\n\t\t\t   bool exclude_ingress);\n\nvoid __cpu_map_flush(void);\nint cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx);\nint cpu_map_generic_redirect(struct bpf_cpu_map_entry *rcpu,\n\t\t\t     struct sk_buff *skb);\n\n \nstatic inline int bpf_map_attr_numa_node(const union bpf_attr *attr)\n{\n\treturn (attr->map_flags & BPF_F_NUMA_NODE) ?\n\t\tattr->numa_node : NUMA_NO_NODE;\n}\n\nstruct bpf_prog *bpf_prog_get_type_path(const char *name, enum bpf_prog_type type);\nint array_map_alloc_check(union bpf_attr *attr);\n\nint bpf_prog_test_run_xdp(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t  union bpf_attr __user *uattr);\nint bpf_prog_test_run_skb(struct bpf_prog *prog, const union bpf_attr *kattr,\n\t\t\t  union bpf_attr __user *uattr);\nint bpf_prog_test_run_tracing(struct bpf_prog *prog,\n\t\t\t      const union bpf_attr *kattr,\n\t\t\t      union bpf_attr __user *uattr);\nint bpf_prog_test_run_flow_dissector(struct bpf_prog *prog,\n\t\t\t\t     const union bpf_attr *kattr,\n\t\t\t\t     union bpf_attr __user *uattr);\nint bpf_prog_test_run_raw_tp(struct bpf_prog *prog,\n\t\t\t     const union bpf_attr *kattr,\n\t\t\t     union bpf_attr __user *uattr);\nint bpf_prog_test_run_sk_lookup(struct bpf_prog *prog,\n\t\t\t\tconst union bpf_attr *kattr,\n\t\t\t\tunion bpf_attr __user *uattr);\nint bpf_prog_test_run_nf(struct bpf_prog *prog,\n\t\t\t const union bpf_attr *kattr,\n\t\t\t union bpf_attr __user *uattr);\nbool btf_ctx_access(int off, int size, enum bpf_access_type type,\n\t\t    const struct bpf_prog *prog,\n\t\t    struct bpf_insn_access_aux *info);\n\nstatic inline bool bpf_tracing_ctx_access(int off, int size,\n\t\t\t\t\t  enum bpf_access_type type)\n{\n\tif (off < 0 || off >= sizeof(__u64) * MAX_BPF_FUNC_ARGS)\n\t\treturn false;\n\tif (type != BPF_READ)\n\t\treturn false;\n\tif (off % size != 0)\n\t\treturn false;\n\treturn true;\n}\n\nstatic inline bool bpf_tracing_btf_ctx_access(int off, int size,\n\t\t\t\t\t      enum bpf_access_type type,\n\t\t\t\t\t      const struct bpf_prog *prog,\n\t\t\t\t\t      struct bpf_insn_access_aux *info)\n{\n\tif (!bpf_tracing_ctx_access(off, size, type))\n\t\treturn false;\n\treturn btf_ctx_access(off, size, type, prog, info);\n}\n\nint btf_struct_access(struct bpf_verifier_log *log,\n\t\t      const struct bpf_reg_state *reg,\n\t\t      int off, int size, enum bpf_access_type atype,\n\t\t      u32 *next_btf_id, enum bpf_type_flag *flag, const char **field_name);\nbool btf_struct_ids_match(struct bpf_verifier_log *log,\n\t\t\t  const struct btf *btf, u32 id, int off,\n\t\t\t  const struct btf *need_btf, u32 need_type_id,\n\t\t\t  bool strict);\n\nint btf_distill_func_proto(struct bpf_verifier_log *log,\n\t\t\t   struct btf *btf,\n\t\t\t   const struct btf_type *func_proto,\n\t\t\t   const char *func_name,\n\t\t\t   struct btf_func_model *m);\n\nstruct bpf_reg_state;\nint btf_check_subprog_arg_match(struct bpf_verifier_env *env, int subprog,\n\t\t\t\tstruct bpf_reg_state *regs);\nint btf_check_subprog_call(struct bpf_verifier_env *env, int subprog,\n\t\t\t   struct bpf_reg_state *regs);\nint btf_prepare_func_args(struct bpf_verifier_env *env, int subprog,\n\t\t\t  struct bpf_reg_state *reg);\nint btf_check_type_match(struct bpf_verifier_log *log, const struct bpf_prog *prog,\n\t\t\t struct btf *btf, const struct btf_type *t);\n\nstruct bpf_prog *bpf_prog_by_id(u32 id);\nstruct bpf_link *bpf_link_by_id(u32 id);\n\nconst struct bpf_func_proto *bpf_base_func_proto(enum bpf_func_id func_id);\nvoid bpf_task_storage_free(struct task_struct *task);\nvoid bpf_cgrp_storage_free(struct cgroup *cgroup);\nbool bpf_prog_has_kfunc_call(const struct bpf_prog *prog);\nconst struct btf_func_model *\nbpf_jit_find_kfunc_model(const struct bpf_prog *prog,\n\t\t\t const struct bpf_insn *insn);\nint bpf_get_kfunc_addr(const struct bpf_prog *prog, u32 func_id,\n\t\t       u16 btf_fd_idx, u8 **func_addr);\n\nstruct bpf_core_ctx {\n\tstruct bpf_verifier_log *log;\n\tconst struct btf *btf;\n};\n\nbool btf_nested_type_is_trusted(struct bpf_verifier_log *log,\n\t\t\t\tconst struct bpf_reg_state *reg,\n\t\t\t\tconst char *field_name, u32 btf_id, const char *suffix);\n\nbool btf_type_ids_nocast_alias(struct bpf_verifier_log *log,\n\t\t\t       const struct btf *reg_btf, u32 reg_id,\n\t\t\t       const struct btf *arg_btf, u32 arg_id);\n\nint bpf_core_apply(struct bpf_core_ctx *ctx, const struct bpf_core_relo *relo,\n\t\t   int relo_idx, void *insn);\n\nstatic inline bool unprivileged_ebpf_enabled(void)\n{\n\treturn !sysctl_unprivileged_bpf_disabled;\n}\n\n \nstatic inline bool has_current_bpf_ctx(void)\n{\n\treturn !!current->bpf_ctx;\n}\n\nvoid notrace bpf_prog_inc_misses_counter(struct bpf_prog *prog);\n\nvoid bpf_dynptr_init(struct bpf_dynptr_kern *ptr, void *data,\n\t\t     enum bpf_dynptr_type type, u32 offset, u32 size);\nvoid bpf_dynptr_set_null(struct bpf_dynptr_kern *ptr);\nvoid bpf_dynptr_set_rdonly(struct bpf_dynptr_kern *ptr);\n#else  \nstatic inline struct bpf_prog *bpf_prog_get(u32 ufd)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline struct bpf_prog *bpf_prog_get_type_dev(u32 ufd,\n\t\t\t\t\t\t     enum bpf_prog_type type,\n\t\t\t\t\t\t     bool attach_drv)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline void bpf_prog_add(struct bpf_prog *prog, int i)\n{\n}\n\nstatic inline void bpf_prog_sub(struct bpf_prog *prog, int i)\n{\n}\n\nstatic inline void bpf_prog_put(struct bpf_prog *prog)\n{\n}\n\nstatic inline void bpf_prog_inc(struct bpf_prog *prog)\n{\n}\n\nstatic inline struct bpf_prog *__must_check\nbpf_prog_inc_not_zero(struct bpf_prog *prog)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline void bpf_link_init(struct bpf_link *link, enum bpf_link_type type,\n\t\t\t\t const struct bpf_link_ops *ops,\n\t\t\t\t struct bpf_prog *prog)\n{\n}\n\nstatic inline int bpf_link_prime(struct bpf_link *link,\n\t\t\t\t struct bpf_link_primer *primer)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int bpf_link_settle(struct bpf_link_primer *primer)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void bpf_link_cleanup(struct bpf_link_primer *primer)\n{\n}\n\nstatic inline void bpf_link_inc(struct bpf_link *link)\n{\n}\n\nstatic inline void bpf_link_put(struct bpf_link *link)\n{\n}\n\nstatic inline int bpf_obj_get_user(const char __user *pathname, int flags)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void __dev_flush(void)\n{\n}\n\nstruct xdp_frame;\nstruct bpf_dtab_netdev;\nstruct bpf_cpu_map_entry;\n\nstatic inline\nint dev_xdp_enqueue(struct net_device *dev, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx)\n{\n\treturn 0;\n}\n\nstatic inline\nint dev_map_enqueue(struct bpf_dtab_netdev *dst, struct xdp_frame *xdpf,\n\t\t    struct net_device *dev_rx)\n{\n\treturn 0;\n}\n\nstatic inline\nint dev_map_enqueue_multi(struct xdp_frame *xdpf, struct net_device *dev_rx,\n\t\t\t  struct bpf_map *map, bool exclude_ingress)\n{\n\treturn 0;\n}\n\nstruct sk_buff;\n\nstatic inline int dev_map_generic_redirect(struct bpf_dtab_netdev *dst,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct bpf_prog *xdp_prog)\n{\n\treturn 0;\n}\n\nstatic inline\nint dev_map_redirect_multi(struct net_device *dev, struct sk_buff *skb,\n\t\t\t   struct bpf_prog *xdp_prog, struct bpf_map *map,\n\t\t\t   bool exclude_ingress)\n{\n\treturn 0;\n}\n\nstatic inline void __cpu_map_flush(void)\n{\n}\n\nstatic inline int cpu_map_enqueue(struct bpf_cpu_map_entry *rcpu,\n\t\t\t\t  struct xdp_frame *xdpf,\n\t\t\t\t  struct net_device *dev_rx)\n{\n\treturn 0;\n}\n\nstatic inline int cpu_map_generic_redirect(struct bpf_cpu_map_entry *rcpu,\n\t\t\t\t\t   struct sk_buff *skb)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline struct bpf_prog *bpf_prog_get_type_path(const char *name,\n\t\t\t\tenum bpf_prog_type type)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline int bpf_prog_test_run_xdp(struct bpf_prog *prog,\n\t\t\t\t\tconst union bpf_attr *kattr,\n\t\t\t\t\tunion bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline int bpf_prog_test_run_skb(struct bpf_prog *prog,\n\t\t\t\t\tconst union bpf_attr *kattr,\n\t\t\t\t\tunion bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline int bpf_prog_test_run_tracing(struct bpf_prog *prog,\n\t\t\t\t\t    const union bpf_attr *kattr,\n\t\t\t\t\t    union bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline int bpf_prog_test_run_flow_dissector(struct bpf_prog *prog,\n\t\t\t\t\t\t   const union bpf_attr *kattr,\n\t\t\t\t\t\t   union bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline int bpf_prog_test_run_sk_lookup(struct bpf_prog *prog,\n\t\t\t\t\t      const union bpf_attr *kattr,\n\t\t\t\t\t      union bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline void bpf_map_put(struct bpf_map *map)\n{\n}\n\nstatic inline struct bpf_prog *bpf_prog_by_id(u32 id)\n{\n\treturn ERR_PTR(-ENOTSUPP);\n}\n\nstatic inline int btf_struct_access(struct bpf_verifier_log *log,\n\t\t\t\t    const struct bpf_reg_state *reg,\n\t\t\t\t    int off, int size, enum bpf_access_type atype,\n\t\t\t\t    u32 *next_btf_id, enum bpf_type_flag *flag,\n\t\t\t\t    const char **field_name)\n{\n\treturn -EACCES;\n}\n\nstatic inline const struct bpf_func_proto *\nbpf_base_func_proto(enum bpf_func_id func_id)\n{\n\treturn NULL;\n}\n\nstatic inline void bpf_task_storage_free(struct task_struct *task)\n{\n}\n\nstatic inline bool bpf_prog_has_kfunc_call(const struct bpf_prog *prog)\n{\n\treturn false;\n}\n\nstatic inline const struct btf_func_model *\nbpf_jit_find_kfunc_model(const struct bpf_prog *prog,\n\t\t\t const struct bpf_insn *insn)\n{\n\treturn NULL;\n}\n\nstatic inline int\nbpf_get_kfunc_addr(const struct bpf_prog *prog, u32 func_id,\n\t\t   u16 btf_fd_idx, u8 **func_addr)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline bool unprivileged_ebpf_enabled(void)\n{\n\treturn false;\n}\n\nstatic inline bool has_current_bpf_ctx(void)\n{\n\treturn false;\n}\n\nstatic inline void bpf_prog_inc_misses_counter(struct bpf_prog *prog)\n{\n}\n\nstatic inline void bpf_cgrp_storage_free(struct cgroup *cgroup)\n{\n}\n\nstatic inline void bpf_dynptr_init(struct bpf_dynptr_kern *ptr, void *data,\n\t\t\t\t   enum bpf_dynptr_type type, u32 offset, u32 size)\n{\n}\n\nstatic inline void bpf_dynptr_set_null(struct bpf_dynptr_kern *ptr)\n{\n}\n\nstatic inline void bpf_dynptr_set_rdonly(struct bpf_dynptr_kern *ptr)\n{\n}\n#endif  \n\nstatic __always_inline int\nbpf_probe_read_kernel_common(void *dst, u32 size, const void *unsafe_ptr)\n{\n\tint ret = -EFAULT;\n\n\tif (IS_ENABLED(CONFIG_BPF_EVENTS))\n\t\tret = copy_from_kernel_nofault(dst, unsafe_ptr, size);\n\tif (unlikely(ret < 0))\n\t\tmemset(dst, 0, size);\n\treturn ret;\n}\n\nvoid __bpf_free_used_btfs(struct bpf_prog_aux *aux,\n\t\t\t  struct btf_mod_pair *used_btfs, u32 len);\n\nstatic inline struct bpf_prog *bpf_prog_get_type(u32 ufd,\n\t\t\t\t\t\t enum bpf_prog_type type)\n{\n\treturn bpf_prog_get_type_dev(ufd, type, false);\n}\n\nvoid __bpf_free_used_maps(struct bpf_prog_aux *aux,\n\t\t\t  struct bpf_map **used_maps, u32 len);\n\nbool bpf_prog_get_ok(struct bpf_prog *, enum bpf_prog_type *, bool);\n\nint bpf_prog_offload_compile(struct bpf_prog *prog);\nvoid bpf_prog_dev_bound_destroy(struct bpf_prog *prog);\nint bpf_prog_offload_info_fill(struct bpf_prog_info *info,\n\t\t\t       struct bpf_prog *prog);\n\nint bpf_map_offload_info_fill(struct bpf_map_info *info, struct bpf_map *map);\n\nint bpf_map_offload_lookup_elem(struct bpf_map *map, void *key, void *value);\nint bpf_map_offload_update_elem(struct bpf_map *map,\n\t\t\t\tvoid *key, void *value, u64 flags);\nint bpf_map_offload_delete_elem(struct bpf_map *map, void *key);\nint bpf_map_offload_get_next_key(struct bpf_map *map,\n\t\t\t\t void *key, void *next_key);\n\nbool bpf_offload_prog_map_match(struct bpf_prog *prog, struct bpf_map *map);\n\nstruct bpf_offload_dev *\nbpf_offload_dev_create(const struct bpf_prog_offload_ops *ops, void *priv);\nvoid bpf_offload_dev_destroy(struct bpf_offload_dev *offdev);\nvoid *bpf_offload_dev_priv(struct bpf_offload_dev *offdev);\nint bpf_offload_dev_netdev_register(struct bpf_offload_dev *offdev,\n\t\t\t\t    struct net_device *netdev);\nvoid bpf_offload_dev_netdev_unregister(struct bpf_offload_dev *offdev,\n\t\t\t\t       struct net_device *netdev);\nbool bpf_offload_dev_match(struct bpf_prog *prog, struct net_device *netdev);\n\nvoid unpriv_ebpf_notify(int new_state);\n\n#if defined(CONFIG_NET) && defined(CONFIG_BPF_SYSCALL)\nint bpf_dev_bound_kfunc_check(struct bpf_verifier_log *log,\n\t\t\t      struct bpf_prog_aux *prog_aux);\nvoid *bpf_dev_bound_resolve_kfunc(struct bpf_prog *prog, u32 func_id);\nint bpf_prog_dev_bound_init(struct bpf_prog *prog, union bpf_attr *attr);\nint bpf_prog_dev_bound_inherit(struct bpf_prog *new_prog, struct bpf_prog *old_prog);\nvoid bpf_dev_bound_netdev_unregister(struct net_device *dev);\n\nstatic inline bool bpf_prog_is_dev_bound(const struct bpf_prog_aux *aux)\n{\n\treturn aux->dev_bound;\n}\n\nstatic inline bool bpf_prog_is_offloaded(const struct bpf_prog_aux *aux)\n{\n\treturn aux->offload_requested;\n}\n\nbool bpf_prog_dev_bound_match(const struct bpf_prog *lhs, const struct bpf_prog *rhs);\n\nstatic inline bool bpf_map_is_offloaded(struct bpf_map *map)\n{\n\treturn unlikely(map->ops == &bpf_map_offload_ops);\n}\n\nstruct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr);\nvoid bpf_map_offload_map_free(struct bpf_map *map);\nu64 bpf_map_offload_map_mem_usage(const struct bpf_map *map);\nint bpf_prog_test_run_syscall(struct bpf_prog *prog,\n\t\t\t      const union bpf_attr *kattr,\n\t\t\t      union bpf_attr __user *uattr);\n\nint sock_map_get_from_fd(const union bpf_attr *attr, struct bpf_prog *prog);\nint sock_map_prog_detach(const union bpf_attr *attr, enum bpf_prog_type ptype);\nint sock_map_update_elem_sys(struct bpf_map *map, void *key, void *value, u64 flags);\nint sock_map_bpf_prog_query(const union bpf_attr *attr,\n\t\t\t    union bpf_attr __user *uattr);\n\nvoid sock_map_unhash(struct sock *sk);\nvoid sock_map_destroy(struct sock *sk);\nvoid sock_map_close(struct sock *sk, long timeout);\n#else\nstatic inline int bpf_dev_bound_kfunc_check(struct bpf_verifier_log *log,\n\t\t\t\t\t    struct bpf_prog_aux *prog_aux)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void *bpf_dev_bound_resolve_kfunc(struct bpf_prog *prog,\n\t\t\t\t\t\tu32 func_id)\n{\n\treturn NULL;\n}\n\nstatic inline int bpf_prog_dev_bound_init(struct bpf_prog *prog,\n\t\t\t\t\t  union bpf_attr *attr)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int bpf_prog_dev_bound_inherit(struct bpf_prog *new_prog,\n\t\t\t\t\t     struct bpf_prog *old_prog)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void bpf_dev_bound_netdev_unregister(struct net_device *dev)\n{\n}\n\nstatic inline bool bpf_prog_is_dev_bound(const struct bpf_prog_aux *aux)\n{\n\treturn false;\n}\n\nstatic inline bool bpf_prog_is_offloaded(struct bpf_prog_aux *aux)\n{\n\treturn false;\n}\n\nstatic inline bool bpf_prog_dev_bound_match(const struct bpf_prog *lhs, const struct bpf_prog *rhs)\n{\n\treturn false;\n}\n\nstatic inline bool bpf_map_is_offloaded(struct bpf_map *map)\n{\n\treturn false;\n}\n\nstatic inline struct bpf_map *bpf_map_offload_map_alloc(union bpf_attr *attr)\n{\n\treturn ERR_PTR(-EOPNOTSUPP);\n}\n\nstatic inline void bpf_map_offload_map_free(struct bpf_map *map)\n{\n}\n\nstatic inline u64 bpf_map_offload_map_mem_usage(const struct bpf_map *map)\n{\n\treturn 0;\n}\n\nstatic inline int bpf_prog_test_run_syscall(struct bpf_prog *prog,\n\t\t\t\t\t    const union bpf_attr *kattr,\n\t\t\t\t\t    union bpf_attr __user *uattr)\n{\n\treturn -ENOTSUPP;\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic inline int sock_map_get_from_fd(const union bpf_attr *attr,\n\t\t\t\t       struct bpf_prog *prog)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int sock_map_prog_detach(const union bpf_attr *attr,\n\t\t\t\t       enum bpf_prog_type ptype)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int sock_map_update_elem_sys(struct bpf_map *map, void *key, void *value,\n\t\t\t\t\t   u64 flags)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int sock_map_bpf_prog_query(const union bpf_attr *attr,\n\t\t\t\t\t  union bpf_attr __user *uattr)\n{\n\treturn -EINVAL;\n}\n#endif  \n#endif  \n\n#if defined(CONFIG_INET) && defined(CONFIG_BPF_SYSCALL)\nvoid bpf_sk_reuseport_detach(struct sock *sk);\nint bpf_fd_reuseport_array_lookup_elem(struct bpf_map *map, void *key,\n\t\t\t\t       void *value);\nint bpf_fd_reuseport_array_update_elem(struct bpf_map *map, void *key,\n\t\t\t\t       void *value, u64 map_flags);\n#else\nstatic inline void bpf_sk_reuseport_detach(struct sock *sk)\n{\n}\n\n#ifdef CONFIG_BPF_SYSCALL\nstatic inline int bpf_fd_reuseport_array_lookup_elem(struct bpf_map *map,\n\t\t\t\t\t\t     void *key, void *value)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int bpf_fd_reuseport_array_update_elem(struct bpf_map *map,\n\t\t\t\t\t\t     void *key, void *value,\n\t\t\t\t\t\t     u64 map_flags)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif  \n#endif  \n\n \nextern const struct bpf_func_proto bpf_map_lookup_elem_proto;\nextern const struct bpf_func_proto bpf_map_update_elem_proto;\nextern const struct bpf_func_proto bpf_map_delete_elem_proto;\nextern const struct bpf_func_proto bpf_map_push_elem_proto;\nextern const struct bpf_func_proto bpf_map_pop_elem_proto;\nextern const struct bpf_func_proto bpf_map_peek_elem_proto;\nextern const struct bpf_func_proto bpf_map_lookup_percpu_elem_proto;\n\nextern const struct bpf_func_proto bpf_get_prandom_u32_proto;\nextern const struct bpf_func_proto bpf_get_smp_processor_id_proto;\nextern const struct bpf_func_proto bpf_get_numa_node_id_proto;\nextern const struct bpf_func_proto bpf_tail_call_proto;\nextern const struct bpf_func_proto bpf_ktime_get_ns_proto;\nextern const struct bpf_func_proto bpf_ktime_get_boot_ns_proto;\nextern const struct bpf_func_proto bpf_ktime_get_tai_ns_proto;\nextern const struct bpf_func_proto bpf_get_current_pid_tgid_proto;\nextern const struct bpf_func_proto bpf_get_current_uid_gid_proto;\nextern const struct bpf_func_proto bpf_get_current_comm_proto;\nextern const struct bpf_func_proto bpf_get_stackid_proto;\nextern const struct bpf_func_proto bpf_get_stack_proto;\nextern const struct bpf_func_proto bpf_get_task_stack_proto;\nextern const struct bpf_func_proto bpf_get_stackid_proto_pe;\nextern const struct bpf_func_proto bpf_get_stack_proto_pe;\nextern const struct bpf_func_proto bpf_sock_map_update_proto;\nextern const struct bpf_func_proto bpf_sock_hash_update_proto;\nextern const struct bpf_func_proto bpf_get_current_cgroup_id_proto;\nextern const struct bpf_func_proto bpf_get_current_ancestor_cgroup_id_proto;\nextern const struct bpf_func_proto bpf_get_cgroup_classid_curr_proto;\nextern const struct bpf_func_proto bpf_msg_redirect_hash_proto;\nextern const struct bpf_func_proto bpf_msg_redirect_map_proto;\nextern const struct bpf_func_proto bpf_sk_redirect_hash_proto;\nextern const struct bpf_func_proto bpf_sk_redirect_map_proto;\nextern const struct bpf_func_proto bpf_spin_lock_proto;\nextern const struct bpf_func_proto bpf_spin_unlock_proto;\nextern const struct bpf_func_proto bpf_get_local_storage_proto;\nextern const struct bpf_func_proto bpf_strtol_proto;\nextern const struct bpf_func_proto bpf_strtoul_proto;\nextern const struct bpf_func_proto bpf_tcp_sock_proto;\nextern const struct bpf_func_proto bpf_jiffies64_proto;\nextern const struct bpf_func_proto bpf_get_ns_current_pid_tgid_proto;\nextern const struct bpf_func_proto bpf_event_output_data_proto;\nextern const struct bpf_func_proto bpf_ringbuf_output_proto;\nextern const struct bpf_func_proto bpf_ringbuf_reserve_proto;\nextern const struct bpf_func_proto bpf_ringbuf_submit_proto;\nextern const struct bpf_func_proto bpf_ringbuf_discard_proto;\nextern const struct bpf_func_proto bpf_ringbuf_query_proto;\nextern const struct bpf_func_proto bpf_ringbuf_reserve_dynptr_proto;\nextern const struct bpf_func_proto bpf_ringbuf_submit_dynptr_proto;\nextern const struct bpf_func_proto bpf_ringbuf_discard_dynptr_proto;\nextern const struct bpf_func_proto bpf_skc_to_tcp6_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_tcp_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_tcp_timewait_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_tcp_request_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_udp6_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_unix_sock_proto;\nextern const struct bpf_func_proto bpf_skc_to_mptcp_sock_proto;\nextern const struct bpf_func_proto bpf_copy_from_user_proto;\nextern const struct bpf_func_proto bpf_snprintf_btf_proto;\nextern const struct bpf_func_proto bpf_snprintf_proto;\nextern const struct bpf_func_proto bpf_per_cpu_ptr_proto;\nextern const struct bpf_func_proto bpf_this_cpu_ptr_proto;\nextern const struct bpf_func_proto bpf_ktime_get_coarse_ns_proto;\nextern const struct bpf_func_proto bpf_sock_from_file_proto;\nextern const struct bpf_func_proto bpf_get_socket_ptr_cookie_proto;\nextern const struct bpf_func_proto bpf_task_storage_get_recur_proto;\nextern const struct bpf_func_proto bpf_task_storage_get_proto;\nextern const struct bpf_func_proto bpf_task_storage_delete_recur_proto;\nextern const struct bpf_func_proto bpf_task_storage_delete_proto;\nextern const struct bpf_func_proto bpf_for_each_map_elem_proto;\nextern const struct bpf_func_proto bpf_btf_find_by_name_kind_proto;\nextern const struct bpf_func_proto bpf_sk_setsockopt_proto;\nextern const struct bpf_func_proto bpf_sk_getsockopt_proto;\nextern const struct bpf_func_proto bpf_unlocked_sk_setsockopt_proto;\nextern const struct bpf_func_proto bpf_unlocked_sk_getsockopt_proto;\nextern const struct bpf_func_proto bpf_find_vma_proto;\nextern const struct bpf_func_proto bpf_loop_proto;\nextern const struct bpf_func_proto bpf_copy_from_user_task_proto;\nextern const struct bpf_func_proto bpf_set_retval_proto;\nextern const struct bpf_func_proto bpf_get_retval_proto;\nextern const struct bpf_func_proto bpf_user_ringbuf_drain_proto;\nextern const struct bpf_func_proto bpf_cgrp_storage_get_proto;\nextern const struct bpf_func_proto bpf_cgrp_storage_delete_proto;\n\nconst struct bpf_func_proto *tracing_prog_func_proto(\n  enum bpf_func_id func_id, const struct bpf_prog *prog);\n\n \nvoid bpf_user_rnd_init_once(void);\nu64 bpf_user_rnd_u32(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\nu64 bpf_get_raw_cpu_id(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\n\n#if defined(CONFIG_NET)\nbool bpf_sock_common_is_valid_access(int off, int size,\n\t\t\t\t     enum bpf_access_type type,\n\t\t\t\t     struct bpf_insn_access_aux *info);\nbool bpf_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t      struct bpf_insn_access_aux *info);\nu32 bpf_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\tconst struct bpf_insn *si,\n\t\t\t\tstruct bpf_insn *insn_buf,\n\t\t\t\tstruct bpf_prog *prog,\n\t\t\t\tu32 *target_size);\nint bpf_dynptr_from_skb_rdonly(struct sk_buff *skb, u64 flags,\n\t\t\t       struct bpf_dynptr_kern *ptr);\n#else\nstatic inline bool bpf_sock_common_is_valid_access(int off, int size,\n\t\t\t\t\t\t   enum bpf_access_type type,\n\t\t\t\t\t\t   struct bpf_insn_access_aux *info)\n{\n\treturn false;\n}\nstatic inline bool bpf_sock_is_valid_access(int off, int size,\n\t\t\t\t\t    enum bpf_access_type type,\n\t\t\t\t\t    struct bpf_insn_access_aux *info)\n{\n\treturn false;\n}\nstatic inline u32 bpf_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t      const struct bpf_insn *si,\n\t\t\t\t\t      struct bpf_insn *insn_buf,\n\t\t\t\t\t      struct bpf_prog *prog,\n\t\t\t\t\t      u32 *target_size)\n{\n\treturn 0;\n}\nstatic inline int bpf_dynptr_from_skb_rdonly(struct sk_buff *skb, u64 flags,\n\t\t\t\t\t     struct bpf_dynptr_kern *ptr)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif\n\n#ifdef CONFIG_INET\nstruct sk_reuseport_kern {\n\tstruct sk_buff *skb;\n\tstruct sock *sk;\n\tstruct sock *selected_sk;\n\tstruct sock *migrating_sk;\n\tvoid *data_end;\n\tu32 hash;\n\tu32 reuseport_id;\n\tbool bind_inany;\n};\nbool bpf_tcp_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t  struct bpf_insn_access_aux *info);\n\nu32 bpf_tcp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t    const struct bpf_insn *si,\n\t\t\t\t    struct bpf_insn *insn_buf,\n\t\t\t\t    struct bpf_prog *prog,\n\t\t\t\t    u32 *target_size);\n\nbool bpf_xdp_sock_is_valid_access(int off, int size, enum bpf_access_type type,\n\t\t\t\t  struct bpf_insn_access_aux *info);\n\nu32 bpf_xdp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t    const struct bpf_insn *si,\n\t\t\t\t    struct bpf_insn *insn_buf,\n\t\t\t\t    struct bpf_prog *prog,\n\t\t\t\t    u32 *target_size);\n#else\nstatic inline bool bpf_tcp_sock_is_valid_access(int off, int size,\n\t\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\treturn false;\n}\n\nstatic inline u32 bpf_tcp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t\t\t  struct bpf_prog *prog,\n\t\t\t\t\t\t  u32 *target_size)\n{\n\treturn 0;\n}\nstatic inline bool bpf_xdp_sock_is_valid_access(int off, int size,\n\t\t\t\t\t\tenum bpf_access_type type,\n\t\t\t\t\t\tstruct bpf_insn_access_aux *info)\n{\n\treturn false;\n}\n\nstatic inline u32 bpf_xdp_sock_convert_ctx_access(enum bpf_access_type type,\n\t\t\t\t\t\t  const struct bpf_insn *si,\n\t\t\t\t\t\t  struct bpf_insn *insn_buf,\n\t\t\t\t\t\t  struct bpf_prog *prog,\n\t\t\t\t\t\t  u32 *target_size)\n{\n\treturn 0;\n}\n#endif  \n\nenum bpf_text_poke_type {\n\tBPF_MOD_CALL,\n\tBPF_MOD_JUMP,\n};\n\nint bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,\n\t\t       void *addr1, void *addr2);\n\nvoid bpf_arch_poke_desc_update(struct bpf_jit_poke_descriptor *poke,\n\t\t\t       struct bpf_prog *new, struct bpf_prog *old);\n\nvoid *bpf_arch_text_copy(void *dst, void *src, size_t len);\nint bpf_arch_text_invalidate(void *dst, size_t len);\n\nstruct btf_id_set;\nbool btf_id_set_contains(const struct btf_id_set *set, u32 id);\n\n#define MAX_BPRINTF_VARARGS\t\t12\n#define MAX_BPRINTF_BUF\t\t\t1024\n\nstruct bpf_bprintf_data {\n\tu32 *bin_args;\n\tchar *buf;\n\tbool get_bin_args;\n\tbool get_buf;\n};\n\nint bpf_bprintf_prepare(char *fmt, u32 fmt_size, const u64 *raw_args,\n\t\t\tu32 num_args, struct bpf_bprintf_data *data);\nvoid bpf_bprintf_cleanup(struct bpf_bprintf_data *data);\n\n#ifdef CONFIG_BPF_LSM\nvoid bpf_cgroup_atype_get(u32 attach_btf_id, int cgroup_atype);\nvoid bpf_cgroup_atype_put(int cgroup_atype);\n#else\nstatic inline void bpf_cgroup_atype_get(u32 attach_btf_id, int cgroup_atype) {}\nstatic inline void bpf_cgroup_atype_put(int cgroup_atype) {}\n#endif  \n\nstruct key;\n\n#ifdef CONFIG_KEYS\nstruct bpf_key {\n\tstruct key *key;\n\tbool has_ref;\n};\n#endif  \n\nstatic inline bool type_is_alloc(u32 type)\n{\n\treturn type & MEM_ALLOC;\n}\n\nstatic inline gfp_t bpf_memcg_flags(gfp_t flags)\n{\n\tif (memcg_bpf_enabled())\n\t\treturn flags | __GFP_ACCOUNT;\n\treturn flags;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}