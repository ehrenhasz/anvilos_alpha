{
  "module_name": "uio.h",
  "hash_id": "6d28f3fb6541b89b7c2d7240844187b69fa1f8410327be895e8a5b85a5247da2",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/uio.h",
  "human_readable_source": " \n \n#ifndef __LINUX_UIO_H\n#define __LINUX_UIO_H\n\n#include <linux/kernel.h>\n#include <linux/thread_info.h>\n#include <linux/mm_types.h>\n#include <uapi/linux/uio.h>\n\nstruct page;\n\ntypedef unsigned int __bitwise iov_iter_extraction_t;\n\nstruct kvec {\n\tvoid *iov_base;  \n\tsize_t iov_len;\n};\n\nenum iter_type {\n\t \n\tITER_IOVEC,\n\tITER_KVEC,\n\tITER_BVEC,\n\tITER_XARRAY,\n\tITER_DISCARD,\n\tITER_UBUF,\n};\n\n#define ITER_SOURCE\t1\t\n#define ITER_DEST\t0\t\n\nstruct iov_iter_state {\n\tsize_t iov_offset;\n\tsize_t count;\n\tunsigned long nr_segs;\n};\n\nstruct iov_iter {\n\tu8 iter_type;\n\tbool copy_mc;\n\tbool nofault;\n\tbool data_source;\n\tbool user_backed;\n\tunion {\n\t\tsize_t iov_offset;\n\t\tint last_offset;\n\t};\n\t \n\tunion {\n\t\t \n\t\tstruct iovec __ubuf_iovec;\n\t\tstruct {\n\t\t\tunion {\n\t\t\t\t \n\t\t\t\tconst struct iovec *__iov;\n\t\t\t\tconst struct kvec *kvec;\n\t\t\t\tconst struct bio_vec *bvec;\n\t\t\t\tstruct xarray *xarray;\n\t\t\t\tvoid __user *ubuf;\n\t\t\t};\n\t\t\tsize_t count;\n\t\t};\n\t};\n\tunion {\n\t\tunsigned long nr_segs;\n\t\tloff_t xarray_start;\n\t};\n};\n\nstatic inline const struct iovec *iter_iov(const struct iov_iter *iter)\n{\n\tif (iter->iter_type == ITER_UBUF)\n\t\treturn (const struct iovec *) &iter->__ubuf_iovec;\n\treturn iter->__iov;\n}\n\n#define iter_iov_addr(iter)\t(iter_iov(iter)->iov_base + (iter)->iov_offset)\n#define iter_iov_len(iter)\t(iter_iov(iter)->iov_len - (iter)->iov_offset)\n\nstatic inline enum iter_type iov_iter_type(const struct iov_iter *i)\n{\n\treturn i->iter_type;\n}\n\nstatic inline void iov_iter_save_state(struct iov_iter *iter,\n\t\t\t\t       struct iov_iter_state *state)\n{\n\tstate->iov_offset = iter->iov_offset;\n\tstate->count = iter->count;\n\tstate->nr_segs = iter->nr_segs;\n}\n\nstatic inline bool iter_is_ubuf(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_UBUF;\n}\n\nstatic inline bool iter_is_iovec(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_IOVEC;\n}\n\nstatic inline bool iov_iter_is_kvec(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_KVEC;\n}\n\nstatic inline bool iov_iter_is_bvec(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_BVEC;\n}\n\nstatic inline bool iov_iter_is_discard(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_DISCARD;\n}\n\nstatic inline bool iov_iter_is_xarray(const struct iov_iter *i)\n{\n\treturn iov_iter_type(i) == ITER_XARRAY;\n}\n\nstatic inline unsigned char iov_iter_rw(const struct iov_iter *i)\n{\n\treturn i->data_source ? WRITE : READ;\n}\n\nstatic inline bool user_backed_iter(const struct iov_iter *i)\n{\n\treturn i->user_backed;\n}\n\n \nstatic inline size_t iov_length(const struct iovec *iov, unsigned long nr_segs)\n{\n\tunsigned long seg;\n\tsize_t ret = 0;\n\n\tfor (seg = 0; seg < nr_segs; seg++)\n\t\tret += iov[seg].iov_len;\n\treturn ret;\n}\n\nsize_t copy_page_from_iter_atomic(struct page *page, size_t offset,\n\t\t\t\t  size_t bytes, struct iov_iter *i);\nvoid iov_iter_advance(struct iov_iter *i, size_t bytes);\nvoid iov_iter_revert(struct iov_iter *i, size_t bytes);\nsize_t fault_in_iov_iter_readable(const struct iov_iter *i, size_t bytes);\nsize_t fault_in_iov_iter_writeable(const struct iov_iter *i, size_t bytes);\nsize_t iov_iter_single_seg_count(const struct iov_iter *i);\nsize_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i);\nsize_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,\n\t\t\t struct iov_iter *i);\n\nsize_t _copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i);\nsize_t _copy_from_iter(void *addr, size_t bytes, struct iov_iter *i);\nsize_t _copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i);\n\nstatic inline size_t copy_folio_to_iter(struct folio *folio, size_t offset,\n\t\tsize_t bytes, struct iov_iter *i)\n{\n\treturn copy_page_to_iter(&folio->page, offset, bytes, i);\n}\n\nstatic inline size_t copy_folio_from_iter_atomic(struct folio *folio,\n\t\tsize_t offset, size_t bytes, struct iov_iter *i)\n{\n\treturn copy_page_from_iter_atomic(&folio->page, offset, bytes, i);\n}\n\nsize_t copy_page_to_iter_nofault(struct page *page, unsigned offset,\n\t\t\t\t size_t bytes, struct iov_iter *i);\n\nstatic __always_inline __must_check\nsize_t copy_to_iter(const void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (check_copy_size(addr, bytes, true))\n\t\treturn _copy_to_iter(addr, bytes, i);\n\treturn 0;\n}\n\nstatic __always_inline __must_check\nsize_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (check_copy_size(addr, bytes, false))\n\t\treturn _copy_from_iter(addr, bytes, i);\n\treturn 0;\n}\n\nstatic __always_inline __must_check\nbool copy_from_iter_full(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tsize_t copied = copy_from_iter(addr, bytes, i);\n\tif (likely(copied == bytes))\n\t\treturn true;\n\tiov_iter_revert(i, copied);\n\treturn false;\n}\n\nstatic __always_inline __must_check\nsize_t copy_from_iter_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tif (check_copy_size(addr, bytes, false))\n\t\treturn _copy_from_iter_nocache(addr, bytes, i);\n\treturn 0;\n}\n\nstatic __always_inline __must_check\nbool copy_from_iter_full_nocache(void *addr, size_t bytes, struct iov_iter *i)\n{\n\tsize_t copied = copy_from_iter_nocache(addr, bytes, i);\n\tif (likely(copied == bytes))\n\t\treturn true;\n\tiov_iter_revert(i, copied);\n\treturn false;\n}\n\n#ifdef CONFIG_ARCH_HAS_UACCESS_FLUSHCACHE\n \nsize_t _copy_from_iter_flushcache(void *addr, size_t bytes, struct iov_iter *i);\n#else\n#define _copy_from_iter_flushcache _copy_from_iter_nocache\n#endif\n\n#ifdef CONFIG_ARCH_HAS_COPY_MC\nsize_t _copy_mc_to_iter(const void *addr, size_t bytes, struct iov_iter *i);\nstatic inline void iov_iter_set_copy_mc(struct iov_iter *i)\n{\n\ti->copy_mc = true;\n}\n\nstatic inline bool iov_iter_is_copy_mc(const struct iov_iter *i)\n{\n\treturn i->copy_mc;\n}\n#else\n#define _copy_mc_to_iter _copy_to_iter\nstatic inline void iov_iter_set_copy_mc(struct iov_iter *i) { }\nstatic inline bool iov_iter_is_copy_mc(const struct iov_iter *i)\n{\n\treturn false;\n}\n#endif\n\nsize_t iov_iter_zero(size_t bytes, struct iov_iter *);\nbool iov_iter_is_aligned(const struct iov_iter *i, unsigned addr_mask,\n\t\t\tunsigned len_mask);\nunsigned long iov_iter_alignment(const struct iov_iter *i);\nunsigned long iov_iter_gap_alignment(const struct iov_iter *i);\nvoid iov_iter_init(struct iov_iter *i, unsigned int direction, const struct iovec *iov,\n\t\t\tunsigned long nr_segs, size_t count);\nvoid iov_iter_kvec(struct iov_iter *i, unsigned int direction, const struct kvec *kvec,\n\t\t\tunsigned long nr_segs, size_t count);\nvoid iov_iter_bvec(struct iov_iter *i, unsigned int direction, const struct bio_vec *bvec,\n\t\t\tunsigned long nr_segs, size_t count);\nvoid iov_iter_discard(struct iov_iter *i, unsigned int direction, size_t count);\nvoid iov_iter_xarray(struct iov_iter *i, unsigned int direction, struct xarray *xarray,\n\t\t     loff_t start, size_t count);\nssize_t iov_iter_get_pages2(struct iov_iter *i, struct page **pages,\n\t\t\tsize_t maxsize, unsigned maxpages, size_t *start);\nssize_t iov_iter_get_pages_alloc2(struct iov_iter *i, struct page ***pages,\n\t\t\tsize_t maxsize, size_t *start);\nint iov_iter_npages(const struct iov_iter *i, int maxpages);\nvoid iov_iter_restore(struct iov_iter *i, struct iov_iter_state *state);\n\nconst void *dup_iter(struct iov_iter *new, struct iov_iter *old, gfp_t flags);\n\nstatic inline size_t iov_iter_count(const struct iov_iter *i)\n{\n\treturn i->count;\n}\n\n \nstatic inline void iov_iter_truncate(struct iov_iter *i, u64 count)\n{\n\t \n\tif (i->count > count)\n\t\ti->count = count;\n}\n\n \nstatic inline void iov_iter_reexpand(struct iov_iter *i, size_t count)\n{\n\ti->count = count;\n}\n\nstatic inline int\niov_iter_npages_cap(struct iov_iter *i, int maxpages, size_t max_bytes)\n{\n\tsize_t shorted = 0;\n\tint npages;\n\n\tif (iov_iter_count(i) > max_bytes) {\n\t\tshorted = iov_iter_count(i) - max_bytes;\n\t\tiov_iter_truncate(i, max_bytes);\n\t}\n\tnpages = iov_iter_npages(i, maxpages);\n\tif (shorted)\n\t\tiov_iter_reexpand(i, iov_iter_count(i) + shorted);\n\n\treturn npages;\n}\n\nstruct csum_state {\n\t__wsum csum;\n\tsize_t off;\n};\n\nsize_t csum_and_copy_to_iter(const void *addr, size_t bytes, void *csstate, struct iov_iter *i);\nsize_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum, struct iov_iter *i);\n\nstatic __always_inline __must_check\nbool csum_and_copy_from_iter_full(void *addr, size_t bytes,\n\t\t\t\t  __wsum *csum, struct iov_iter *i)\n{\n\tsize_t copied = csum_and_copy_from_iter(addr, bytes, csum, i);\n\tif (likely(copied == bytes))\n\t\treturn true;\n\tiov_iter_revert(i, copied);\n\treturn false;\n}\nsize_t hash_and_copy_to_iter(const void *addr, size_t bytes, void *hashp,\n\t\tstruct iov_iter *i);\n\nstruct iovec *iovec_from_user(const struct iovec __user *uvector,\n\t\tunsigned long nr_segs, unsigned long fast_segs,\n\t\tstruct iovec *fast_iov, bool compat);\nssize_t import_iovec(int type, const struct iovec __user *uvec,\n\t\t unsigned nr_segs, unsigned fast_segs, struct iovec **iovp,\n\t\t struct iov_iter *i);\nssize_t __import_iovec(int type, const struct iovec __user *uvec,\n\t\t unsigned nr_segs, unsigned fast_segs, struct iovec **iovp,\n\t\t struct iov_iter *i, bool compat);\nint import_single_range(int type, void __user *buf, size_t len,\n\t\t struct iovec *iov, struct iov_iter *i);\nint import_ubuf(int type, void __user *buf, size_t len, struct iov_iter *i);\n\nstatic inline void iov_iter_ubuf(struct iov_iter *i, unsigned int direction,\n\t\t\tvoid __user *buf, size_t count)\n{\n\tWARN_ON(direction & ~(READ | WRITE));\n\t*i = (struct iov_iter) {\n\t\t.iter_type = ITER_UBUF,\n\t\t.copy_mc = false,\n\t\t.user_backed = true,\n\t\t.data_source = direction,\n\t\t.ubuf = buf,\n\t\t.count = count,\n\t\t.nr_segs = 1\n\t};\n}\n \n \n#define ITER_ALLOW_P2PDMA\t((__force iov_iter_extraction_t)0x01)\n\nssize_t iov_iter_extract_pages(struct iov_iter *i, struct page ***pages,\n\t\t\t       size_t maxsize, unsigned int maxpages,\n\t\t\t       iov_iter_extraction_t extraction_flags,\n\t\t\t       size_t *offset0);\n\n \nstatic inline bool iov_iter_extract_will_pin(const struct iov_iter *iter)\n{\n\treturn user_backed_iter(iter);\n}\n\nstruct sg_table;\nssize_t extract_iter_to_sg(struct iov_iter *iter, size_t len,\n\t\t\t   struct sg_table *sgtable, unsigned int sg_max,\n\t\t\t   iov_iter_extraction_t extraction_flags);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}