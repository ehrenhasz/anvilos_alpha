{
  "module_name": "trace_events.h",
  "hash_id": "81bef5c0aac41b4393ce3d233e8a7614bccd7f56ff093aac4e02ff317855073f",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/trace_events.h",
  "human_readable_source": " \n\n#ifndef _LINUX_TRACE_EVENT_H\n#define _LINUX_TRACE_EVENT_H\n\n#include <linux/ring_buffer.h>\n#include <linux/trace_seq.h>\n#include <linux/percpu.h>\n#include <linux/hardirq.h>\n#include <linux/perf_event.h>\n#include <linux/tracepoint.h>\n\nstruct trace_array;\nstruct array_buffer;\nstruct tracer;\nstruct dentry;\nstruct bpf_prog;\nunion bpf_attr;\n\nconst char *trace_print_flags_seq(struct trace_seq *p, const char *delim,\n\t\t\t\t  unsigned long flags,\n\t\t\t\t  const struct trace_print_flags *flag_array);\n\nconst char *trace_print_symbols_seq(struct trace_seq *p, unsigned long val,\n\t\t\t\t    const struct trace_print_flags *symbol_array);\n\n#if BITS_PER_LONG == 32\nconst char *trace_print_flags_seq_u64(struct trace_seq *p, const char *delim,\n\t\t      unsigned long long flags,\n\t\t      const struct trace_print_flags_u64 *flag_array);\n\nconst char *trace_print_symbols_seq_u64(struct trace_seq *p,\n\t\t\t\t\tunsigned long long val,\n\t\t\t\t\tconst struct trace_print_flags_u64\n\t\t\t\t\t\t\t\t *symbol_array);\n#endif\n\nconst char *trace_print_bitmask_seq(struct trace_seq *p, void *bitmask_ptr,\n\t\t\t\t    unsigned int bitmask_size);\n\nconst char *trace_print_hex_seq(struct trace_seq *p,\n\t\t\t\tconst unsigned char *buf, int len,\n\t\t\t\tbool concatenate);\n\nconst char *trace_print_array_seq(struct trace_seq *p,\n\t\t\t\t   const void *buf, int count,\n\t\t\t\t   size_t el_size);\n\nconst char *\ntrace_print_hex_dump_seq(struct trace_seq *p, const char *prefix_str,\n\t\t\t int prefix_type, int rowsize, int groupsize,\n\t\t\t const void *buf, size_t len, bool ascii);\n\nstruct trace_iterator;\nstruct trace_event;\n\nint trace_raw_output_prep(struct trace_iterator *iter,\n\t\t\t  struct trace_event *event);\nextern __printf(2, 3)\nvoid trace_event_printf(struct trace_iterator *iter, const char *fmt, ...);\n\n \nstruct trace_dynamic_info {\n#ifdef CONFIG_CPU_BIG_ENDIAN\n\tu16\tlen;\n\tu16\toffset;\n#else\n\tu16\toffset;\n\tu16\tlen;\n#endif\n} __packed;\n\n \nstruct trace_entry {\n\tunsigned short\t\ttype;\n\tunsigned char\t\tflags;\n\tunsigned char\t\tpreempt_count;\n\tint\t\t\tpid;\n};\n\n#define TRACE_EVENT_TYPE_MAX\t\t\t\t\t\t\\\n\t((1 << (sizeof(((struct trace_entry *)0)->type) * 8)) - 1)\n\n \nstruct trace_iterator {\n\tstruct trace_array\t*tr;\n\tstruct tracer\t\t*trace;\n\tstruct array_buffer\t*array_buffer;\n\tvoid\t\t\t*private;\n\tint\t\t\tcpu_file;\n\tstruct mutex\t\tmutex;\n\tstruct ring_buffer_iter\t**buffer_iter;\n\tunsigned long\t\titer_flags;\n\tvoid\t\t\t*temp;\t \n\tunsigned int\t\ttemp_size;\n\tchar\t\t\t*fmt;\t \n\tunsigned int\t\tfmt_size;\n\tlong\t\t\twait_index;\n\n\t \n\tstruct trace_seq\ttmp_seq;\n\n\tcpumask_var_t\t\tstarted;\n\n\t \n\tbool\t\t\tsnapshot;\n\n\t \n\tstruct trace_seq\tseq;\n\tstruct trace_entry\t*ent;\n\tunsigned long\t\tlost_events;\n\tint\t\t\tleftover;\n\tint\t\t\tent_size;\n\tint\t\t\tcpu;\n\tu64\t\t\tts;\n\n\tloff_t\t\t\tpos;\n\tlong\t\t\tidx;\n\n\t \n};\n\nenum trace_iter_flags {\n\tTRACE_FILE_LAT_FMT\t= 1,\n\tTRACE_FILE_ANNOTATE\t= 2,\n\tTRACE_FILE_TIME_IN_NS\t= 4,\n};\n\n\ntypedef enum print_line_t (*trace_print_func)(struct trace_iterator *iter,\n\t\t\t\t      int flags, struct trace_event *event);\n\nstruct trace_event_functions {\n\ttrace_print_func\ttrace;\n\ttrace_print_func\traw;\n\ttrace_print_func\thex;\n\ttrace_print_func\tbinary;\n};\n\nstruct trace_event {\n\tstruct hlist_node\t\tnode;\n\tint\t\t\t\ttype;\n\tstruct trace_event_functions\t*funcs;\n};\n\nextern int register_trace_event(struct trace_event *event);\nextern int unregister_trace_event(struct trace_event *event);\n\n \nenum print_line_t {\n\tTRACE_TYPE_PARTIAL_LINE\t= 0,\t \n\tTRACE_TYPE_HANDLED\t= 1,\n\tTRACE_TYPE_UNHANDLED\t= 2,\t \n\tTRACE_TYPE_NO_CONSUME\t= 3\t \n};\n\nenum print_line_t trace_handle_return(struct trace_seq *s);\n\nstatic inline void tracing_generic_entry_update(struct trace_entry *entry,\n\t\t\t\t\t\tunsigned short type,\n\t\t\t\t\t\tunsigned int trace_ctx)\n{\n\tentry->preempt_count\t\t= trace_ctx & 0xff;\n\tentry->pid\t\t\t= current->pid;\n\tentry->type\t\t\t= type;\n\tentry->flags =\t\t\ttrace_ctx >> 16;\n}\n\nunsigned int tracing_gen_ctx_irq_test(unsigned int irqs_status);\n\nenum trace_flag_type {\n\tTRACE_FLAG_IRQS_OFF\t\t= 0x01,\n\tTRACE_FLAG_IRQS_NOSUPPORT\t= 0x02,\n\tTRACE_FLAG_NEED_RESCHED\t\t= 0x04,\n\tTRACE_FLAG_HARDIRQ\t\t= 0x08,\n\tTRACE_FLAG_SOFTIRQ\t\t= 0x10,\n\tTRACE_FLAG_PREEMPT_RESCHED\t= 0x20,\n\tTRACE_FLAG_NMI\t\t\t= 0x40,\n\tTRACE_FLAG_BH_OFF\t\t= 0x80,\n};\n\n#ifdef CONFIG_TRACE_IRQFLAGS_SUPPORT\nstatic inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)\n{\n\tunsigned int irq_status = irqs_disabled_flags(irqflags) ?\n\t\tTRACE_FLAG_IRQS_OFF : 0;\n\treturn tracing_gen_ctx_irq_test(irq_status);\n}\nstatic inline unsigned int tracing_gen_ctx(void)\n{\n\tunsigned long irqflags;\n\n\tlocal_save_flags(irqflags);\n\treturn tracing_gen_ctx_flags(irqflags);\n}\n#else\n\nstatic inline unsigned int tracing_gen_ctx_flags(unsigned long irqflags)\n{\n\treturn tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);\n}\nstatic inline unsigned int tracing_gen_ctx(void)\n{\n\treturn tracing_gen_ctx_irq_test(TRACE_FLAG_IRQS_NOSUPPORT);\n}\n#endif\n\nstatic inline unsigned int tracing_gen_ctx_dec(void)\n{\n\tunsigned int trace_ctx;\n\n\ttrace_ctx = tracing_gen_ctx();\n\t \n\tif (IS_ENABLED(CONFIG_PREEMPTION))\n\t\ttrace_ctx--;\n\treturn trace_ctx;\n}\n\nstruct trace_event_file;\n\nstruct ring_buffer_event *\ntrace_event_buffer_lock_reserve(struct trace_buffer **current_buffer,\n\t\t\t\tstruct trace_event_file *trace_file,\n\t\t\t\tint type, unsigned long len,\n\t\t\t\tunsigned int trace_ctx);\n\n#define TRACE_RECORD_CMDLINE\tBIT(0)\n#define TRACE_RECORD_TGID\tBIT(1)\n\nvoid tracing_record_taskinfo(struct task_struct *task, int flags);\nvoid tracing_record_taskinfo_sched_switch(struct task_struct *prev,\n\t\t\t\t\t  struct task_struct *next, int flags);\n\nvoid tracing_record_cmdline(struct task_struct *task);\nvoid tracing_record_tgid(struct task_struct *task);\n\nint trace_output_call(struct trace_iterator *iter, char *name, char *fmt, ...)\n\t __printf(3, 4);\n\nstruct event_filter;\n\nenum trace_reg {\n\tTRACE_REG_REGISTER,\n\tTRACE_REG_UNREGISTER,\n#ifdef CONFIG_PERF_EVENTS\n\tTRACE_REG_PERF_REGISTER,\n\tTRACE_REG_PERF_UNREGISTER,\n\tTRACE_REG_PERF_OPEN,\n\tTRACE_REG_PERF_CLOSE,\n\t \n\tTRACE_REG_PERF_ADD,\n\tTRACE_REG_PERF_DEL,\n#endif\n};\n\nstruct trace_event_call;\n\n#define TRACE_FUNCTION_TYPE ((const char *)~0UL)\n\nstruct trace_event_fields {\n\tconst char *type;\n\tunion {\n\t\tstruct {\n\t\t\tconst char *name;\n\t\t\tconst int  size;\n\t\t\tconst int  align;\n\t\t\tconst int  is_signed;\n\t\t\tconst int  filter_type;\n\t\t\tconst int  len;\n\t\t};\n\t\tint (*define_fields)(struct trace_event_call *);\n\t};\n};\n\nstruct trace_event_class {\n\tconst char\t\t*system;\n\tvoid\t\t\t*probe;\n#ifdef CONFIG_PERF_EVENTS\n\tvoid\t\t\t*perf_probe;\n#endif\n\tint\t\t\t(*reg)(struct trace_event_call *event,\n\t\t\t\t       enum trace_reg type, void *data);\n\tstruct trace_event_fields *fields_array;\n\tstruct list_head\t*(*get_fields)(struct trace_event_call *);\n\tstruct list_head\tfields;\n\tint\t\t\t(*raw_init)(struct trace_event_call *);\n};\n\nextern int trace_event_reg(struct trace_event_call *event,\n\t\t\t    enum trace_reg type, void *data);\n\nstruct trace_event_buffer {\n\tstruct trace_buffer\t\t*buffer;\n\tstruct ring_buffer_event\t*event;\n\tstruct trace_event_file\t\t*trace_file;\n\tvoid\t\t\t\t*entry;\n\tunsigned int\t\t\ttrace_ctx;\n\tstruct pt_regs\t\t\t*regs;\n};\n\nvoid *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,\n\t\t\t\t  struct trace_event_file *trace_file,\n\t\t\t\t  unsigned long len);\n\nvoid trace_event_buffer_commit(struct trace_event_buffer *fbuffer);\n\nenum {\n\tTRACE_EVENT_FL_FILTERED_BIT,\n\tTRACE_EVENT_FL_CAP_ANY_BIT,\n\tTRACE_EVENT_FL_NO_SET_FILTER_BIT,\n\tTRACE_EVENT_FL_IGNORE_ENABLE_BIT,\n\tTRACE_EVENT_FL_TRACEPOINT_BIT,\n\tTRACE_EVENT_FL_DYNAMIC_BIT,\n\tTRACE_EVENT_FL_KPROBE_BIT,\n\tTRACE_EVENT_FL_UPROBE_BIT,\n\tTRACE_EVENT_FL_EPROBE_BIT,\n\tTRACE_EVENT_FL_FPROBE_BIT,\n\tTRACE_EVENT_FL_CUSTOM_BIT,\n};\n\n \nenum {\n\tTRACE_EVENT_FL_FILTERED\t\t= (1 << TRACE_EVENT_FL_FILTERED_BIT),\n\tTRACE_EVENT_FL_CAP_ANY\t\t= (1 << TRACE_EVENT_FL_CAP_ANY_BIT),\n\tTRACE_EVENT_FL_NO_SET_FILTER\t= (1 << TRACE_EVENT_FL_NO_SET_FILTER_BIT),\n\tTRACE_EVENT_FL_IGNORE_ENABLE\t= (1 << TRACE_EVENT_FL_IGNORE_ENABLE_BIT),\n\tTRACE_EVENT_FL_TRACEPOINT\t= (1 << TRACE_EVENT_FL_TRACEPOINT_BIT),\n\tTRACE_EVENT_FL_DYNAMIC\t\t= (1 << TRACE_EVENT_FL_DYNAMIC_BIT),\n\tTRACE_EVENT_FL_KPROBE\t\t= (1 << TRACE_EVENT_FL_KPROBE_BIT),\n\tTRACE_EVENT_FL_UPROBE\t\t= (1 << TRACE_EVENT_FL_UPROBE_BIT),\n\tTRACE_EVENT_FL_EPROBE\t\t= (1 << TRACE_EVENT_FL_EPROBE_BIT),\n\tTRACE_EVENT_FL_FPROBE\t\t= (1 << TRACE_EVENT_FL_FPROBE_BIT),\n\tTRACE_EVENT_FL_CUSTOM\t\t= (1 << TRACE_EVENT_FL_CUSTOM_BIT),\n};\n\n#define TRACE_EVENT_FL_UKPROBE (TRACE_EVENT_FL_KPROBE | TRACE_EVENT_FL_UPROBE)\n\nstruct trace_event_call {\n\tstruct list_head\tlist;\n\tstruct trace_event_class *class;\n\tunion {\n\t\tchar\t\t\t*name;\n\t\t \n\t\tstruct tracepoint\t*tp;\n\t};\n\tstruct trace_event\tevent;\n\tchar\t\t\t*print_fmt;\n\tstruct event_filter\t*filter;\n\t \n\tunion {\n\t\tvoid\t\t\t\t*module;\n\t\tatomic_t\t\t\trefcnt;\n\t};\n\tvoid\t\t\t*data;\n\n\t \n\tint\t\t\tflags;  \n\n#ifdef CONFIG_PERF_EVENTS\n\tint\t\t\t\tperf_refcount;\n\tstruct hlist_head __percpu\t*perf_events;\n\tstruct bpf_prog_array __rcu\t*prog_array;\n\n\tint\t(*perf_perm)(struct trace_event_call *,\n\t\t\t     struct perf_event *);\n#endif\n};\n\n#ifdef CONFIG_DYNAMIC_EVENTS\nbool trace_event_dyn_try_get_ref(struct trace_event_call *call);\nvoid trace_event_dyn_put_ref(struct trace_event_call *call);\nbool trace_event_dyn_busy(struct trace_event_call *call);\n#else\nstatic inline bool trace_event_dyn_try_get_ref(struct trace_event_call *call)\n{\n\t \n\treturn false;\n}\nstatic inline void trace_event_dyn_put_ref(struct trace_event_call *call)\n{\n}\nstatic inline bool trace_event_dyn_busy(struct trace_event_call *call)\n{\n\t \n\treturn true;\n}\n#endif\n\nstatic inline bool trace_event_try_get_ref(struct trace_event_call *call)\n{\n\tif (call->flags & TRACE_EVENT_FL_DYNAMIC)\n\t\treturn trace_event_dyn_try_get_ref(call);\n\telse\n\t\treturn try_module_get(call->module);\n}\n\nstatic inline void trace_event_put_ref(struct trace_event_call *call)\n{\n\tif (call->flags & TRACE_EVENT_FL_DYNAMIC)\n\t\ttrace_event_dyn_put_ref(call);\n\telse\n\t\tmodule_put(call->module);\n}\n\n#ifdef CONFIG_PERF_EVENTS\nstatic inline bool bpf_prog_array_valid(struct trace_event_call *call)\n{\n\t \n\treturn !!READ_ONCE(call->prog_array);\n}\n#endif\n\nstatic inline const char *\ntrace_event_name(struct trace_event_call *call)\n{\n\tif (call->flags & TRACE_EVENT_FL_CUSTOM)\n\t\treturn call->name;\n\telse if (call->flags & TRACE_EVENT_FL_TRACEPOINT)\n\t\treturn call->tp ? call->tp->name : NULL;\n\telse\n\t\treturn call->name;\n}\n\nstatic inline struct list_head *\ntrace_get_fields(struct trace_event_call *event_call)\n{\n\tif (!event_call->class->get_fields)\n\t\treturn &event_call->class->fields;\n\treturn event_call->class->get_fields(event_call);\n}\n\nstruct trace_subsystem_dir;\n\nenum {\n\tEVENT_FILE_FL_ENABLED_BIT,\n\tEVENT_FILE_FL_RECORDED_CMD_BIT,\n\tEVENT_FILE_FL_RECORDED_TGID_BIT,\n\tEVENT_FILE_FL_FILTERED_BIT,\n\tEVENT_FILE_FL_NO_SET_FILTER_BIT,\n\tEVENT_FILE_FL_SOFT_MODE_BIT,\n\tEVENT_FILE_FL_SOFT_DISABLED_BIT,\n\tEVENT_FILE_FL_TRIGGER_MODE_BIT,\n\tEVENT_FILE_FL_TRIGGER_COND_BIT,\n\tEVENT_FILE_FL_PID_FILTER_BIT,\n\tEVENT_FILE_FL_WAS_ENABLED_BIT,\n\tEVENT_FILE_FL_FREED_BIT,\n};\n\nextern struct trace_event_file *trace_get_event_file(const char *instance,\n\t\t\t\t\t\t     const char *system,\n\t\t\t\t\t\t     const char *event);\nextern void trace_put_event_file(struct trace_event_file *file);\n\n#define MAX_DYNEVENT_CMD_LEN\t(2048)\n\nenum dynevent_type {\n\tDYNEVENT_TYPE_SYNTH = 1,\n\tDYNEVENT_TYPE_KPROBE,\n\tDYNEVENT_TYPE_NONE,\n};\n\nstruct dynevent_cmd;\n\ntypedef int (*dynevent_create_fn_t)(struct dynevent_cmd *cmd);\n\nstruct dynevent_cmd {\n\tstruct seq_buf\t\tseq;\n\tconst char\t\t*event_name;\n\tunsigned int\t\tn_fields;\n\tenum dynevent_type\ttype;\n\tdynevent_create_fn_t\trun_command;\n\tvoid\t\t\t*private_data;\n};\n\nextern int dynevent_create(struct dynevent_cmd *cmd);\n\nextern int synth_event_delete(const char *name);\n\nextern void synth_event_cmd_init(struct dynevent_cmd *cmd,\n\t\t\t\t char *buf, int maxlen);\n\nextern int __synth_event_gen_cmd_start(struct dynevent_cmd *cmd,\n\t\t\t\t       const char *name,\n\t\t\t\t       struct module *mod, ...);\n\n#define synth_event_gen_cmd_start(cmd, name, mod, ...)\t\\\n\t__synth_event_gen_cmd_start(cmd, name, mod, ## __VA_ARGS__, NULL)\n\nstruct synth_field_desc {\n\tconst char *type;\n\tconst char *name;\n};\n\nextern int synth_event_gen_cmd_array_start(struct dynevent_cmd *cmd,\n\t\t\t\t\t   const char *name,\n\t\t\t\t\t   struct module *mod,\n\t\t\t\t\t   struct synth_field_desc *fields,\n\t\t\t\t\t   unsigned int n_fields);\nextern int synth_event_create(const char *name,\n\t\t\t      struct synth_field_desc *fields,\n\t\t\t      unsigned int n_fields, struct module *mod);\n\nextern int synth_event_add_field(struct dynevent_cmd *cmd,\n\t\t\t\t const char *type,\n\t\t\t\t const char *name);\nextern int synth_event_add_field_str(struct dynevent_cmd *cmd,\n\t\t\t\t     const char *type_name);\nextern int synth_event_add_fields(struct dynevent_cmd *cmd,\n\t\t\t\t  struct synth_field_desc *fields,\n\t\t\t\t  unsigned int n_fields);\n\n#define synth_event_gen_cmd_end(cmd)\t\\\n\tdynevent_create(cmd)\n\nstruct synth_event;\n\nstruct synth_event_trace_state {\n\tstruct trace_event_buffer fbuffer;\n\tstruct synth_trace_event *entry;\n\tstruct trace_buffer *buffer;\n\tstruct synth_event *event;\n\tunsigned int cur_field;\n\tunsigned int n_u64;\n\tbool disabled;\n\tbool add_next;\n\tbool add_name;\n};\n\nextern int synth_event_trace(struct trace_event_file *file,\n\t\t\t     unsigned int n_vals, ...);\nextern int synth_event_trace_array(struct trace_event_file *file, u64 *vals,\n\t\t\t\t   unsigned int n_vals);\nextern int synth_event_trace_start(struct trace_event_file *file,\n\t\t\t\t   struct synth_event_trace_state *trace_state);\nextern int synth_event_add_next_val(u64 val,\n\t\t\t\t    struct synth_event_trace_state *trace_state);\nextern int synth_event_add_val(const char *field_name, u64 val,\n\t\t\t       struct synth_event_trace_state *trace_state);\nextern int synth_event_trace_end(struct synth_event_trace_state *trace_state);\n\nextern int kprobe_event_delete(const char *name);\n\nextern void kprobe_event_cmd_init(struct dynevent_cmd *cmd,\n\t\t\t\t  char *buf, int maxlen);\n\n#define kprobe_event_gen_cmd_start(cmd, name, loc, ...)\t\t\t\\\n\t__kprobe_event_gen_cmd_start(cmd, false, name, loc, ## __VA_ARGS__, NULL)\n\n#define kretprobe_event_gen_cmd_start(cmd, name, loc, ...)\t\t\\\n\t__kprobe_event_gen_cmd_start(cmd, true, name, loc, ## __VA_ARGS__, NULL)\n\nextern int __kprobe_event_gen_cmd_start(struct dynevent_cmd *cmd,\n\t\t\t\t\tbool kretprobe,\n\t\t\t\t\tconst char *name,\n\t\t\t\t\tconst char *loc, ...);\n\n#define kprobe_event_add_fields(cmd, ...)\t\\\n\t__kprobe_event_add_fields(cmd, ## __VA_ARGS__, NULL)\n\n#define kprobe_event_add_field(cmd, field)\t\\\n\t__kprobe_event_add_fields(cmd, field, NULL)\n\nextern int __kprobe_event_add_fields(struct dynevent_cmd *cmd, ...);\n\n#define kprobe_event_gen_cmd_end(cmd)\t\t\\\n\tdynevent_create(cmd)\n\n#define kretprobe_event_gen_cmd_end(cmd)\t\\\n\tdynevent_create(cmd)\n\n \nenum {\n\tEVENT_FILE_FL_ENABLED\t\t= (1 << EVENT_FILE_FL_ENABLED_BIT),\n\tEVENT_FILE_FL_RECORDED_CMD\t= (1 << EVENT_FILE_FL_RECORDED_CMD_BIT),\n\tEVENT_FILE_FL_RECORDED_TGID\t= (1 << EVENT_FILE_FL_RECORDED_TGID_BIT),\n\tEVENT_FILE_FL_FILTERED\t\t= (1 << EVENT_FILE_FL_FILTERED_BIT),\n\tEVENT_FILE_FL_NO_SET_FILTER\t= (1 << EVENT_FILE_FL_NO_SET_FILTER_BIT),\n\tEVENT_FILE_FL_SOFT_MODE\t\t= (1 << EVENT_FILE_FL_SOFT_MODE_BIT),\n\tEVENT_FILE_FL_SOFT_DISABLED\t= (1 << EVENT_FILE_FL_SOFT_DISABLED_BIT),\n\tEVENT_FILE_FL_TRIGGER_MODE\t= (1 << EVENT_FILE_FL_TRIGGER_MODE_BIT),\n\tEVENT_FILE_FL_TRIGGER_COND\t= (1 << EVENT_FILE_FL_TRIGGER_COND_BIT),\n\tEVENT_FILE_FL_PID_FILTER\t= (1 << EVENT_FILE_FL_PID_FILTER_BIT),\n\tEVENT_FILE_FL_WAS_ENABLED\t= (1 << EVENT_FILE_FL_WAS_ENABLED_BIT),\n\tEVENT_FILE_FL_FREED\t\t= (1 << EVENT_FILE_FL_FREED_BIT),\n};\n\nstruct trace_event_file {\n\tstruct list_head\t\tlist;\n\tstruct trace_event_call\t\t*event_call;\n\tstruct event_filter __rcu\t*filter;\n\tstruct eventfs_file             *ef;\n\tstruct trace_array\t\t*tr;\n\tstruct trace_subsystem_dir\t*system;\n\tstruct list_head\t\ttriggers;\n\n\t \n\tunsigned long\t\tflags;\n\tatomic_t\t\tref;\t \n\tatomic_t\t\tsm_ref;\t \n\tatomic_t\t\ttm_ref;\t \n};\n\n#define __TRACE_EVENT_FLAGS(name, value)\t\t\t\t\\\n\tstatic int __init trace_init_flags_##name(void)\t\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tevent_##name.flags |= value;\t\t\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tearly_initcall(trace_init_flags_##name);\n\n#define __TRACE_EVENT_PERF_PERM(name, expr...)\t\t\t\t\\\n\tstatic int perf_perm_##name(struct trace_event_call *tp_event, \\\n\t\t\t\t    struct perf_event *p_event)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\treturn ({ expr; });\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tstatic int __init trace_init_perf_perm_##name(void)\t\t\\\n\t{\t\t\t\t\t\t\t\t\\\n\t\tevent_##name.perf_perm = &perf_perm_##name;\t\t\\\n\t\treturn 0;\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tearly_initcall(trace_init_perf_perm_##name);\n\n#define PERF_MAX_TRACE_SIZE\t8192\n\n#define MAX_FILTER_STR_VAL\t256U\t \n\nenum event_trigger_type {\n\tETT_NONE\t\t= (0),\n\tETT_TRACE_ONOFF\t\t= (1 << 0),\n\tETT_SNAPSHOT\t\t= (1 << 1),\n\tETT_STACKTRACE\t\t= (1 << 2),\n\tETT_EVENT_ENABLE\t= (1 << 3),\n\tETT_EVENT_HIST\t\t= (1 << 4),\n\tETT_HIST_ENABLE\t\t= (1 << 5),\n\tETT_EVENT_EPROBE\t= (1 << 6),\n};\n\nextern int filter_match_preds(struct event_filter *filter, void *rec);\n\nextern enum event_trigger_type\nevent_triggers_call(struct trace_event_file *file,\n\t\t    struct trace_buffer *buffer, void *rec,\n\t\t    struct ring_buffer_event *event);\nextern void\nevent_triggers_post_call(struct trace_event_file *file,\n\t\t\t enum event_trigger_type tt);\n\nbool trace_event_ignore_this_pid(struct trace_event_file *trace_file);\n\nbool __trace_trigger_soft_disabled(struct trace_event_file *file);\n\n \nstatic __always_inline bool\ntrace_trigger_soft_disabled(struct trace_event_file *file)\n{\n\tunsigned long eflags = file->flags;\n\n\tif (likely(!(eflags & (EVENT_FILE_FL_TRIGGER_MODE |\n\t\t\t       EVENT_FILE_FL_SOFT_DISABLED |\n\t\t\t       EVENT_FILE_FL_PID_FILTER))))\n\t\treturn false;\n\n\tif (likely(eflags & EVENT_FILE_FL_TRIGGER_COND))\n\t\treturn false;\n\n\treturn __trace_trigger_soft_disabled(file);\n}\n\n#ifdef CONFIG_BPF_EVENTS\nunsigned int trace_call_bpf(struct trace_event_call *call, void *ctx);\nint perf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog, u64 bpf_cookie);\nvoid perf_event_detach_bpf_prog(struct perf_event *event);\nint perf_event_query_prog_array(struct perf_event *event, void __user *info);\nint bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *prog);\nint bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *prog);\nstruct bpf_raw_event_map *bpf_get_raw_tracepoint(const char *name);\nvoid bpf_put_raw_tracepoint(struct bpf_raw_event_map *btp);\nint bpf_get_perf_event_info(const struct perf_event *event, u32 *prog_id,\n\t\t\t    u32 *fd_type, const char **buf,\n\t\t\t    u64 *probe_offset, u64 *probe_addr);\nint bpf_kprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);\nint bpf_uprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog);\n#else\nstatic inline unsigned int trace_call_bpf(struct trace_event_call *call, void *ctx)\n{\n\treturn 1;\n}\n\nstatic inline int\nperf_event_attach_bpf_prog(struct perf_event *event, struct bpf_prog *prog, u64 bpf_cookie)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void perf_event_detach_bpf_prog(struct perf_event *event) { }\n\nstatic inline int\nperf_event_query_prog_array(struct perf_event *event, void __user *info)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline int bpf_probe_register(struct bpf_raw_event_map *btp, struct bpf_prog *p)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline int bpf_probe_unregister(struct bpf_raw_event_map *btp, struct bpf_prog *p)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline struct bpf_raw_event_map *bpf_get_raw_tracepoint(const char *name)\n{\n\treturn NULL;\n}\nstatic inline void bpf_put_raw_tracepoint(struct bpf_raw_event_map *btp)\n{\n}\nstatic inline int bpf_get_perf_event_info(const struct perf_event *event,\n\t\t\t\t\t  u32 *prog_id, u32 *fd_type,\n\t\t\t\t\t  const char **buf, u64 *probe_offset,\n\t\t\t\t\t  u64 *probe_addr)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline int\nbpf_kprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\treturn -EOPNOTSUPP;\n}\nstatic inline int\nbpf_uprobe_multi_link_attach(const union bpf_attr *attr, struct bpf_prog *prog)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nenum {\n\tFILTER_OTHER = 0,\n\tFILTER_STATIC_STRING,\n\tFILTER_DYN_STRING,\n\tFILTER_RDYN_STRING,\n\tFILTER_PTR_STRING,\n\tFILTER_TRACE_FN,\n\tFILTER_CPUMASK,\n\tFILTER_COMM,\n\tFILTER_CPU,\n\tFILTER_STACKTRACE,\n};\n\nextern int trace_event_raw_init(struct trace_event_call *call);\nextern int trace_define_field(struct trace_event_call *call, const char *type,\n\t\t\t      const char *name, int offset, int size,\n\t\t\t      int is_signed, int filter_type);\nextern int trace_add_event_call(struct trace_event_call *call);\nextern int trace_remove_event_call(struct trace_event_call *call);\nextern int trace_event_get_offsets(struct trace_event_call *call);\n\nint ftrace_set_clr_event(struct trace_array *tr, char *buf, int set);\nint trace_set_clr_event(const char *system, const char *event, int set);\nint trace_array_set_clr_event(struct trace_array *tr, const char *system,\n\t\tconst char *event, bool enable);\n \n#define event_trace_printk(ip, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__trace_printk_check_format(fmt, ##args);\t\t\t\\\n\ttracing_record_cmdline(current);\t\t\t\t\\\n\tif (__builtin_constant_p(fmt)) {\t\t\t\t\\\n\t\tstatic const char *trace_printk_fmt\t\t\t\\\n\t\t  __section(\"__trace_printk_fmt\") =\t\t\t\\\n\t\t\t__builtin_constant_p(fmt) ? fmt : NULL;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__trace_bprintk(ip, trace_printk_fmt, ##args);\t\t\\\n\t} else\t\t\t\t\t\t\t\t\\\n\t\t__trace_printk(ip, fmt, ##args);\t\t\t\\\n} while (0)\n\n#ifdef CONFIG_PERF_EVENTS\nstruct perf_event;\n\nDECLARE_PER_CPU(struct pt_regs, perf_trace_regs);\nDECLARE_PER_CPU(int, bpf_kprobe_override);\n\nextern int  perf_trace_init(struct perf_event *event);\nextern void perf_trace_destroy(struct perf_event *event);\nextern int  perf_trace_add(struct perf_event *event, int flags);\nextern void perf_trace_del(struct perf_event *event, int flags);\n#ifdef CONFIG_KPROBE_EVENTS\nextern int  perf_kprobe_init(struct perf_event *event, bool is_retprobe);\nextern void perf_kprobe_destroy(struct perf_event *event);\nextern int bpf_get_kprobe_info(const struct perf_event *event,\n\t\t\t       u32 *fd_type, const char **symbol,\n\t\t\t       u64 *probe_offset, u64 *probe_addr,\n\t\t\t       bool perf_type_tracepoint);\n#endif\n#ifdef CONFIG_UPROBE_EVENTS\nextern int  perf_uprobe_init(struct perf_event *event,\n\t\t\t     unsigned long ref_ctr_offset, bool is_retprobe);\nextern void perf_uprobe_destroy(struct perf_event *event);\nextern int bpf_get_uprobe_info(const struct perf_event *event,\n\t\t\t       u32 *fd_type, const char **filename,\n\t\t\t       u64 *probe_offset, u64 *probe_addr,\n\t\t\t       bool perf_type_tracepoint);\n#endif\nextern int  ftrace_profile_set_filter(struct perf_event *event, int event_id,\n\t\t\t\t     char *filter_str);\nextern void ftrace_profile_free_filter(struct perf_event *event);\nvoid perf_trace_buf_update(void *record, u16 type);\nvoid *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp);\n\nint perf_event_set_bpf_prog(struct perf_event *event, struct bpf_prog *prog, u64 bpf_cookie);\nvoid perf_event_free_bpf_prog(struct perf_event *event);\n\nvoid bpf_trace_run1(struct bpf_prog *prog, u64 arg1);\nvoid bpf_trace_run2(struct bpf_prog *prog, u64 arg1, u64 arg2);\nvoid bpf_trace_run3(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3);\nvoid bpf_trace_run4(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4);\nvoid bpf_trace_run5(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4, u64 arg5);\nvoid bpf_trace_run6(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4, u64 arg5, u64 arg6);\nvoid bpf_trace_run7(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7);\nvoid bpf_trace_run8(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,\n\t\t    u64 arg8);\nvoid bpf_trace_run9(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t    u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,\n\t\t    u64 arg8, u64 arg9);\nvoid bpf_trace_run10(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,\n\t\t     u64 arg8, u64 arg9, u64 arg10);\nvoid bpf_trace_run11(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,\n\t\t     u64 arg8, u64 arg9, u64 arg10, u64 arg11);\nvoid bpf_trace_run12(struct bpf_prog *prog, u64 arg1, u64 arg2,\n\t\t     u64 arg3, u64 arg4, u64 arg5, u64 arg6, u64 arg7,\n\t\t     u64 arg8, u64 arg9, u64 arg10, u64 arg11, u64 arg12);\nvoid perf_trace_run_bpf_submit(void *raw_data, int size, int rctx,\n\t\t\t       struct trace_event_call *call, u64 count,\n\t\t\t       struct pt_regs *regs, struct hlist_head *head,\n\t\t\t       struct task_struct *task);\n\nstatic inline void\nperf_trace_buf_submit(void *raw_data, int size, int rctx, u16 type,\n\t\t       u64 count, struct pt_regs *regs, void *head,\n\t\t       struct task_struct *task)\n{\n\tperf_tp_event(type, count, raw_data, size, regs, head, rctx, task);\n}\n\n#endif\n\n#define TRACE_EVENT_STR_MAX\t512\n\n \n#define __trace_event_vstr_len(fmt, va)\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tva_list __ap;\t\t\t\t\t\\\n\tint __ret;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tva_copy(__ap, *(va));\t\t\t\t\\\n\t__ret = vsnprintf(NULL, 0, fmt, __ap) + 1;\t\\\n\tva_end(__ap);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tmin(__ret, TRACE_EVENT_STR_MAX);\t\t\\\n})\n\n#endif  \n\n \n\n#ifndef TRACE_CUSTOM_EVENT\n\n#define DECLARE_CUSTOM_EVENT_CLASS(name, proto, args, tstruct, assign, print)\n#define DEFINE_CUSTOM_EVENT(template, name, proto, args)\n#define TRACE_CUSTOM_EVENT(name, proto, args, struct, assign, print)\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}