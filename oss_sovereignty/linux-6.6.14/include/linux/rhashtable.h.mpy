{
  "module_name": "rhashtable.h",
  "hash_id": "35ffabdee0cc05418ed314af72c283514981dab87b59d470c4e94c05c74259a9",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/rhashtable.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_RHASHTABLE_H\n#define _LINUX_RHASHTABLE_H\n\n#include <linux/err.h>\n#include <linux/errno.h>\n#include <linux/jhash.h>\n#include <linux/list_nulls.h>\n#include <linux/workqueue.h>\n#include <linux/rculist.h>\n#include <linux/bit_spinlock.h>\n\n#include <linux/rhashtable-types.h>\n \nstruct rhash_lock_head {};\n\n \n#define RHT_ELASTICITY\t16u\n\n \nstruct bucket_table {\n\tunsigned int\t\tsize;\n\tunsigned int\t\tnest;\n\tu32\t\t\thash_rnd;\n\tstruct list_head\twalkers;\n\tstruct rcu_head\t\trcu;\n\n\tstruct bucket_table __rcu *future_tbl;\n\n\tstruct lockdep_map\tdep_map;\n\n\tstruct rhash_lock_head __rcu *buckets[] ____cacheline_aligned_in_smp;\n};\n\n \n#define\tRHT_NULLS_MARKER(ptr)\t\\\n\t((void *)NULLS_MARKER(((unsigned long) (ptr)) >> 1))\n#define INIT_RHT_NULLS_HEAD(ptr)\t\\\n\t((ptr) = NULL)\n\nstatic inline bool rht_is_a_nulls(const struct rhash_head *ptr)\n{\n\treturn ((unsigned long) ptr & 1);\n}\n\nstatic inline void *rht_obj(const struct rhashtable *ht,\n\t\t\t    const struct rhash_head *he)\n{\n\treturn (char *)he - ht->p.head_offset;\n}\n\nstatic inline unsigned int rht_bucket_index(const struct bucket_table *tbl,\n\t\t\t\t\t    unsigned int hash)\n{\n\treturn hash & (tbl->size - 1);\n}\n\nstatic inline unsigned int rht_key_get_hash(struct rhashtable *ht,\n\tconst void *key, const struct rhashtable_params params,\n\tunsigned int hash_rnd)\n{\n\tunsigned int hash;\n\n\t \n\tif (!__builtin_constant_p(params.key_len))\n\t\thash = ht->p.hashfn(key, ht->key_len, hash_rnd);\n\telse if (params.key_len) {\n\t\tunsigned int key_len = params.key_len;\n\n\t\tif (params.hashfn)\n\t\t\thash = params.hashfn(key, key_len, hash_rnd);\n\t\telse if (key_len & (sizeof(u32) - 1))\n\t\t\thash = jhash(key, key_len, hash_rnd);\n\t\telse\n\t\t\thash = jhash2(key, key_len / sizeof(u32), hash_rnd);\n\t} else {\n\t\tunsigned int key_len = ht->p.key_len;\n\n\t\tif (params.hashfn)\n\t\t\thash = params.hashfn(key, key_len, hash_rnd);\n\t\telse\n\t\t\thash = jhash(key, key_len, hash_rnd);\n\t}\n\n\treturn hash;\n}\n\nstatic inline unsigned int rht_key_hashfn(\n\tstruct rhashtable *ht, const struct bucket_table *tbl,\n\tconst void *key, const struct rhashtable_params params)\n{\n\tunsigned int hash = rht_key_get_hash(ht, key, params, tbl->hash_rnd);\n\n\treturn rht_bucket_index(tbl, hash);\n}\n\nstatic inline unsigned int rht_head_hashfn(\n\tstruct rhashtable *ht, const struct bucket_table *tbl,\n\tconst struct rhash_head *he, const struct rhashtable_params params)\n{\n\tconst char *ptr = rht_obj(ht, he);\n\n\treturn likely(params.obj_hashfn) ?\n\t       rht_bucket_index(tbl, params.obj_hashfn(ptr, params.key_len ?:\n\t\t\t\t\t\t\t    ht->p.key_len,\n\t\t\t\t\t\t       tbl->hash_rnd)) :\n\t       rht_key_hashfn(ht, tbl, ptr + params.key_offset, params);\n}\n\n \nstatic inline bool rht_grow_above_75(const struct rhashtable *ht,\n\t\t\t\t     const struct bucket_table *tbl)\n{\n\t \n\treturn atomic_read(&ht->nelems) > (tbl->size / 4 * 3) &&\n\t       (!ht->p.max_size || tbl->size < ht->p.max_size);\n}\n\n \nstatic inline bool rht_shrink_below_30(const struct rhashtable *ht,\n\t\t\t\t       const struct bucket_table *tbl)\n{\n\t \n\treturn atomic_read(&ht->nelems) < (tbl->size * 3 / 10) &&\n\t       tbl->size > ht->p.min_size;\n}\n\n \nstatic inline bool rht_grow_above_100(const struct rhashtable *ht,\n\t\t\t\t      const struct bucket_table *tbl)\n{\n\treturn atomic_read(&ht->nelems) > tbl->size &&\n\t\t(!ht->p.max_size || tbl->size < ht->p.max_size);\n}\n\n \nstatic inline bool rht_grow_above_max(const struct rhashtable *ht,\n\t\t\t\t      const struct bucket_table *tbl)\n{\n\treturn atomic_read(&ht->nelems) >= ht->max_elems;\n}\n\n#ifdef CONFIG_PROVE_LOCKING\nint lockdep_rht_mutex_is_held(struct rhashtable *ht);\nint lockdep_rht_bucket_is_held(const struct bucket_table *tbl, u32 hash);\n#else\nstatic inline int lockdep_rht_mutex_is_held(struct rhashtable *ht)\n{\n\treturn 1;\n}\n\nstatic inline int lockdep_rht_bucket_is_held(const struct bucket_table *tbl,\n\t\t\t\t\t     u32 hash)\n{\n\treturn 1;\n}\n#endif  \n\nvoid *rhashtable_insert_slow(struct rhashtable *ht, const void *key,\n\t\t\t     struct rhash_head *obj);\n\nvoid rhashtable_walk_enter(struct rhashtable *ht,\n\t\t\t   struct rhashtable_iter *iter);\nvoid rhashtable_walk_exit(struct rhashtable_iter *iter);\nint rhashtable_walk_start_check(struct rhashtable_iter *iter) __acquires(RCU);\n\nstatic inline void rhashtable_walk_start(struct rhashtable_iter *iter)\n{\n\t(void)rhashtable_walk_start_check(iter);\n}\n\nvoid *rhashtable_walk_next(struct rhashtable_iter *iter);\nvoid *rhashtable_walk_peek(struct rhashtable_iter *iter);\nvoid rhashtable_walk_stop(struct rhashtable_iter *iter) __releases(RCU);\n\nvoid rhashtable_free_and_destroy(struct rhashtable *ht,\n\t\t\t\t void (*free_fn)(void *ptr, void *arg),\n\t\t\t\t void *arg);\nvoid rhashtable_destroy(struct rhashtable *ht);\n\nstruct rhash_lock_head __rcu **rht_bucket_nested(\n\tconst struct bucket_table *tbl, unsigned int hash);\nstruct rhash_lock_head __rcu **__rht_bucket_nested(\n\tconst struct bucket_table *tbl, unsigned int hash);\nstruct rhash_lock_head __rcu **rht_bucket_nested_insert(\n\tstruct rhashtable *ht, struct bucket_table *tbl, unsigned int hash);\n\n#define rht_dereference(p, ht) \\\n\trcu_dereference_protected(p, lockdep_rht_mutex_is_held(ht))\n\n#define rht_dereference_rcu(p, ht) \\\n\trcu_dereference_check(p, lockdep_rht_mutex_is_held(ht))\n\n#define rht_dereference_bucket(p, tbl, hash) \\\n\trcu_dereference_protected(p, lockdep_rht_bucket_is_held(tbl, hash))\n\n#define rht_dereference_bucket_rcu(p, tbl, hash) \\\n\trcu_dereference_check(p, lockdep_rht_bucket_is_held(tbl, hash))\n\n#define rht_entry(tpos, pos, member) \\\n\t({ tpos = container_of(pos, typeof(*tpos), member); 1; })\n\nstatic inline struct rhash_lock_head __rcu *const *rht_bucket(\n\tconst struct bucket_table *tbl, unsigned int hash)\n{\n\treturn unlikely(tbl->nest) ? rht_bucket_nested(tbl, hash) :\n\t\t\t\t     &tbl->buckets[hash];\n}\n\nstatic inline struct rhash_lock_head __rcu **rht_bucket_var(\n\tstruct bucket_table *tbl, unsigned int hash)\n{\n\treturn unlikely(tbl->nest) ? __rht_bucket_nested(tbl, hash) :\n\t\t\t\t     &tbl->buckets[hash];\n}\n\nstatic inline struct rhash_lock_head __rcu **rht_bucket_insert(\n\tstruct rhashtable *ht, struct bucket_table *tbl, unsigned int hash)\n{\n\treturn unlikely(tbl->nest) ? rht_bucket_nested_insert(ht, tbl, hash) :\n\t\t\t\t     &tbl->buckets[hash];\n}\n\n \n\nstatic inline unsigned long rht_lock(struct bucket_table *tbl,\n\t\t\t\t     struct rhash_lock_head __rcu **bkt)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tbit_spin_lock(0, (unsigned long *)bkt);\n\tlock_map_acquire(&tbl->dep_map);\n\treturn flags;\n}\n\nstatic inline unsigned long rht_lock_nested(struct bucket_table *tbl,\n\t\t\t\t\tstruct rhash_lock_head __rcu **bucket,\n\t\t\t\t\tunsigned int subclass)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tbit_spin_lock(0, (unsigned long *)bucket);\n\tlock_acquire_exclusive(&tbl->dep_map, subclass, 0, NULL, _THIS_IP_);\n\treturn flags;\n}\n\nstatic inline void rht_unlock(struct bucket_table *tbl,\n\t\t\t      struct rhash_lock_head __rcu **bkt,\n\t\t\t      unsigned long flags)\n{\n\tlock_map_release(&tbl->dep_map);\n\tbit_spin_unlock(0, (unsigned long *)bkt);\n\tlocal_irq_restore(flags);\n}\n\nstatic inline struct rhash_head *__rht_ptr(\n\tstruct rhash_lock_head *p, struct rhash_lock_head __rcu *const *bkt)\n{\n\treturn (struct rhash_head *)\n\t\t((unsigned long)p & ~BIT(0) ?:\n\t\t (unsigned long)RHT_NULLS_MARKER(bkt));\n}\n\n \nstatic inline struct rhash_head *rht_ptr_rcu(\n\tstruct rhash_lock_head __rcu *const *bkt)\n{\n\treturn __rht_ptr(rcu_dereference(*bkt), bkt);\n}\n\nstatic inline struct rhash_head *rht_ptr(\n\tstruct rhash_lock_head __rcu *const *bkt,\n\tstruct bucket_table *tbl,\n\tunsigned int hash)\n{\n\treturn __rht_ptr(rht_dereference_bucket(*bkt, tbl, hash), bkt);\n}\n\nstatic inline struct rhash_head *rht_ptr_exclusive(\n\tstruct rhash_lock_head __rcu *const *bkt)\n{\n\treturn __rht_ptr(rcu_dereference_protected(*bkt, 1), bkt);\n}\n\nstatic inline void rht_assign_locked(struct rhash_lock_head __rcu **bkt,\n\t\t\t\t     struct rhash_head *obj)\n{\n\tif (rht_is_a_nulls(obj))\n\t\tobj = NULL;\n\trcu_assign_pointer(*bkt, (void *)((unsigned long)obj | BIT(0)));\n}\n\nstatic inline void rht_assign_unlock(struct bucket_table *tbl,\n\t\t\t\t     struct rhash_lock_head __rcu **bkt,\n\t\t\t\t     struct rhash_head *obj,\n\t\t\t\t     unsigned long flags)\n{\n\tif (rht_is_a_nulls(obj))\n\t\tobj = NULL;\n\tlock_map_release(&tbl->dep_map);\n\trcu_assign_pointer(*bkt, (void *)obj);\n\tpreempt_enable();\n\t__release(bitlock);\n\tlocal_irq_restore(flags);\n}\n\n \n#define rht_for_each_from(pos, head, tbl, hash) \\\n\tfor (pos = head;\t\t\t\\\n\t     !rht_is_a_nulls(pos);\t\t\\\n\t     pos = rht_dereference_bucket((pos)->next, tbl, hash))\n\n \n#define rht_for_each(pos, tbl, hash) \\\n\trht_for_each_from(pos, rht_ptr(rht_bucket(tbl, hash), tbl, hash),  \\\n\t\t\t  tbl, hash)\n\n \n#define rht_for_each_entry_from(tpos, pos, head, tbl, hash, member)\t\\\n\tfor (pos = head;\t\t\t\t\t\t\\\n\t     (!rht_is_a_nulls(pos)) && rht_entry(tpos, pos, member);\t\\\n\t     pos = rht_dereference_bucket((pos)->next, tbl, hash))\n\n \n#define rht_for_each_entry(tpos, pos, tbl, hash, member)\t\t\\\n\trht_for_each_entry_from(tpos, pos,\t\t\t\t\\\n\t\t\t\trht_ptr(rht_bucket(tbl, hash), tbl, hash), \\\n\t\t\t\ttbl, hash, member)\n\n \n#define rht_for_each_entry_safe(tpos, pos, next, tbl, hash, member)\t      \\\n\tfor (pos = rht_ptr(rht_bucket(tbl, hash), tbl, hash),\t\t      \\\n\t     next = !rht_is_a_nulls(pos) ?\t\t\t\t      \\\n\t\t       rht_dereference_bucket(pos->next, tbl, hash) : NULL;   \\\n\t     (!rht_is_a_nulls(pos)) && rht_entry(tpos, pos, member);\t      \\\n\t     pos = next,\t\t\t\t\t\t      \\\n\t     next = !rht_is_a_nulls(pos) ?\t\t\t\t      \\\n\t\t       rht_dereference_bucket(pos->next, tbl, hash) : NULL)\n\n \n#define rht_for_each_rcu_from(pos, head, tbl, hash)\t\t\t\\\n\tfor (({barrier(); }),\t\t\t\t\t\t\\\n\t     pos = head;\t\t\t\t\t\t\\\n\t     !rht_is_a_nulls(pos);\t\t\t\t\t\\\n\t     pos = rcu_dereference_raw(pos->next))\n\n \n#define rht_for_each_rcu(pos, tbl, hash)\t\t\t\\\n\tfor (({barrier(); }),\t\t\t\t\t\\\n\t     pos = rht_ptr_rcu(rht_bucket(tbl, hash));\t\t\\\n\t     !rht_is_a_nulls(pos);\t\t\t\t\\\n\t     pos = rcu_dereference_raw(pos->next))\n\n \n#define rht_for_each_entry_rcu_from(tpos, pos, head, tbl, hash, member) \\\n\tfor (({barrier(); }),\t\t\t\t\t\t    \\\n\t     pos = head;\t\t\t\t\t\t    \\\n\t     (!rht_is_a_nulls(pos)) && rht_entry(tpos, pos, member);\t    \\\n\t     pos = rht_dereference_bucket_rcu(pos->next, tbl, hash))\n\n \n#define rht_for_each_entry_rcu(tpos, pos, tbl, hash, member)\t\t   \\\n\trht_for_each_entry_rcu_from(tpos, pos,\t\t\t\t   \\\n\t\t\t\t    rht_ptr_rcu(rht_bucket(tbl, hash)),\t   \\\n\t\t\t\t    tbl, hash, member)\n\n \n#define rhl_for_each_rcu(pos, list)\t\t\t\t\t\\\n\tfor (pos = list; pos; pos = rcu_dereference_raw(pos->next))\n\n \n#define rhl_for_each_entry_rcu(tpos, pos, list, member)\t\t\t\\\n\tfor (pos = list; pos && rht_entry(tpos, pos, member);\t\t\\\n\t     pos = rcu_dereference_raw(pos->next))\n\nstatic inline int rhashtable_compare(struct rhashtable_compare_arg *arg,\n\t\t\t\t     const void *obj)\n{\n\tstruct rhashtable *ht = arg->ht;\n\tconst char *ptr = obj;\n\n\treturn memcmp(ptr + ht->p.key_offset, arg->key, ht->p.key_len);\n}\n\n \nstatic inline struct rhash_head *__rhashtable_lookup(\n\tstruct rhashtable *ht, const void *key,\n\tconst struct rhashtable_params params)\n{\n\tstruct rhashtable_compare_arg arg = {\n\t\t.ht = ht,\n\t\t.key = key,\n\t};\n\tstruct rhash_lock_head __rcu *const *bkt;\n\tstruct bucket_table *tbl;\n\tstruct rhash_head *he;\n\tunsigned int hash;\n\n\ttbl = rht_dereference_rcu(ht->tbl, ht);\nrestart:\n\thash = rht_key_hashfn(ht, tbl, key, params);\n\tbkt = rht_bucket(tbl, hash);\n\tdo {\n\t\trht_for_each_rcu_from(he, rht_ptr_rcu(bkt), tbl, hash) {\n\t\t\tif (params.obj_cmpfn ?\n\t\t\t    params.obj_cmpfn(&arg, rht_obj(ht, he)) :\n\t\t\t    rhashtable_compare(&arg, rht_obj(ht, he)))\n\t\t\t\tcontinue;\n\t\t\treturn he;\n\t\t}\n\t\t \n\t} while (he != RHT_NULLS_MARKER(bkt));\n\n\t \n\tsmp_rmb();\n\n\ttbl = rht_dereference_rcu(tbl->future_tbl, ht);\n\tif (unlikely(tbl))\n\t\tgoto restart;\n\n\treturn NULL;\n}\n\n \nstatic inline void *rhashtable_lookup(\n\tstruct rhashtable *ht, const void *key,\n\tconst struct rhashtable_params params)\n{\n\tstruct rhash_head *he = __rhashtable_lookup(ht, key, params);\n\n\treturn he ? rht_obj(ht, he) : NULL;\n}\n\n \nstatic inline void *rhashtable_lookup_fast(\n\tstruct rhashtable *ht, const void *key,\n\tconst struct rhashtable_params params)\n{\n\tvoid *obj;\n\n\trcu_read_lock();\n\tobj = rhashtable_lookup(ht, key, params);\n\trcu_read_unlock();\n\n\treturn obj;\n}\n\n \nstatic inline struct rhlist_head *rhltable_lookup(\n\tstruct rhltable *hlt, const void *key,\n\tconst struct rhashtable_params params)\n{\n\tstruct rhash_head *he = __rhashtable_lookup(&hlt->ht, key, params);\n\n\treturn he ? container_of(he, struct rhlist_head, rhead) : NULL;\n}\n\n \nstatic inline void *__rhashtable_insert_fast(\n\tstruct rhashtable *ht, const void *key, struct rhash_head *obj,\n\tconst struct rhashtable_params params, bool rhlist)\n{\n\tstruct rhashtable_compare_arg arg = {\n\t\t.ht = ht,\n\t\t.key = key,\n\t};\n\tstruct rhash_lock_head __rcu **bkt;\n\tstruct rhash_head __rcu **pprev;\n\tstruct bucket_table *tbl;\n\tstruct rhash_head *head;\n\tunsigned long flags;\n\tunsigned int hash;\n\tint elasticity;\n\tvoid *data;\n\n\trcu_read_lock();\n\n\ttbl = rht_dereference_rcu(ht->tbl, ht);\n\thash = rht_head_hashfn(ht, tbl, obj, params);\n\telasticity = RHT_ELASTICITY;\n\tbkt = rht_bucket_insert(ht, tbl, hash);\n\tdata = ERR_PTR(-ENOMEM);\n\tif (!bkt)\n\t\tgoto out;\n\tpprev = NULL;\n\tflags = rht_lock(tbl, bkt);\n\n\tif (unlikely(rcu_access_pointer(tbl->future_tbl))) {\nslow_path:\n\t\trht_unlock(tbl, bkt, flags);\n\t\trcu_read_unlock();\n\t\treturn rhashtable_insert_slow(ht, key, obj);\n\t}\n\n\trht_for_each_from(head, rht_ptr(bkt, tbl, hash), tbl, hash) {\n\t\tstruct rhlist_head *plist;\n\t\tstruct rhlist_head *list;\n\n\t\telasticity--;\n\t\tif (!key ||\n\t\t    (params.obj_cmpfn ?\n\t\t     params.obj_cmpfn(&arg, rht_obj(ht, head)) :\n\t\t     rhashtable_compare(&arg, rht_obj(ht, head)))) {\n\t\t\tpprev = &head->next;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdata = rht_obj(ht, head);\n\n\t\tif (!rhlist)\n\t\t\tgoto out_unlock;\n\n\n\t\tlist = container_of(obj, struct rhlist_head, rhead);\n\t\tplist = container_of(head, struct rhlist_head, rhead);\n\n\t\tRCU_INIT_POINTER(list->next, plist);\n\t\thead = rht_dereference_bucket(head->next, tbl, hash);\n\t\tRCU_INIT_POINTER(list->rhead.next, head);\n\t\tif (pprev) {\n\t\t\trcu_assign_pointer(*pprev, obj);\n\t\t\trht_unlock(tbl, bkt, flags);\n\t\t} else\n\t\t\trht_assign_unlock(tbl, bkt, obj, flags);\n\t\tdata = NULL;\n\t\tgoto out;\n\t}\n\n\tif (elasticity <= 0)\n\t\tgoto slow_path;\n\n\tdata = ERR_PTR(-E2BIG);\n\tif (unlikely(rht_grow_above_max(ht, tbl)))\n\t\tgoto out_unlock;\n\n\tif (unlikely(rht_grow_above_100(ht, tbl)))\n\t\tgoto slow_path;\n\n\t \n\thead = rht_ptr(bkt, tbl, hash);\n\n\tRCU_INIT_POINTER(obj->next, head);\n\tif (rhlist) {\n\t\tstruct rhlist_head *list;\n\n\t\tlist = container_of(obj, struct rhlist_head, rhead);\n\t\tRCU_INIT_POINTER(list->next, NULL);\n\t}\n\n\tatomic_inc(&ht->nelems);\n\trht_assign_unlock(tbl, bkt, obj, flags);\n\n\tif (rht_grow_above_75(ht, tbl))\n\t\tschedule_work(&ht->run_work);\n\n\tdata = NULL;\nout:\n\trcu_read_unlock();\n\n\treturn data;\n\nout_unlock:\n\trht_unlock(tbl, bkt, flags);\n\tgoto out;\n}\n\n \nstatic inline int rhashtable_insert_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\tvoid *ret;\n\n\tret = __rhashtable_insert_fast(ht, NULL, obj, params, false);\n\tif (IS_ERR(ret))\n\t\treturn PTR_ERR(ret);\n\n\treturn ret == NULL ? 0 : -EEXIST;\n}\n\n \nstatic inline int rhltable_insert_key(\n\tstruct rhltable *hlt, const void *key, struct rhlist_head *list,\n\tconst struct rhashtable_params params)\n{\n\treturn PTR_ERR(__rhashtable_insert_fast(&hlt->ht, key, &list->rhead,\n\t\t\t\t\t\tparams, true));\n}\n\n \nstatic inline int rhltable_insert(\n\tstruct rhltable *hlt, struct rhlist_head *list,\n\tconst struct rhashtable_params params)\n{\n\tconst char *key = rht_obj(&hlt->ht, &list->rhead);\n\n\tkey += params.key_offset;\n\n\treturn rhltable_insert_key(hlt, key, list, params);\n}\n\n \nstatic inline int rhashtable_lookup_insert_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\tconst char *key = rht_obj(ht, obj);\n\tvoid *ret;\n\n\tBUG_ON(ht->p.obj_hashfn);\n\n\tret = __rhashtable_insert_fast(ht, key + ht->p.key_offset, obj, params,\n\t\t\t\t       false);\n\tif (IS_ERR(ret))\n\t\treturn PTR_ERR(ret);\n\n\treturn ret == NULL ? 0 : -EEXIST;\n}\n\n \nstatic inline void *rhashtable_lookup_get_insert_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\tconst char *key = rht_obj(ht, obj);\n\n\tBUG_ON(ht->p.obj_hashfn);\n\n\treturn __rhashtable_insert_fast(ht, key + ht->p.key_offset, obj, params,\n\t\t\t\t\tfalse);\n}\n\n \nstatic inline int rhashtable_lookup_insert_key(\n\tstruct rhashtable *ht, const void *key, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\tvoid *ret;\n\n\tBUG_ON(!ht->p.obj_hashfn || !key);\n\n\tret = __rhashtable_insert_fast(ht, key, obj, params, false);\n\tif (IS_ERR(ret))\n\t\treturn PTR_ERR(ret);\n\n\treturn ret == NULL ? 0 : -EEXIST;\n}\n\n \nstatic inline void *rhashtable_lookup_get_insert_key(\n\tstruct rhashtable *ht, const void *key, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\tBUG_ON(!ht->p.obj_hashfn || !key);\n\n\treturn __rhashtable_insert_fast(ht, key, obj, params, false);\n}\n\n \nstatic inline int __rhashtable_remove_fast_one(\n\tstruct rhashtable *ht, struct bucket_table *tbl,\n\tstruct rhash_head *obj, const struct rhashtable_params params,\n\tbool rhlist)\n{\n\tstruct rhash_lock_head __rcu **bkt;\n\tstruct rhash_head __rcu **pprev;\n\tstruct rhash_head *he;\n\tunsigned long flags;\n\tunsigned int hash;\n\tint err = -ENOENT;\n\n\thash = rht_head_hashfn(ht, tbl, obj, params);\n\tbkt = rht_bucket_var(tbl, hash);\n\tif (!bkt)\n\t\treturn -ENOENT;\n\tpprev = NULL;\n\tflags = rht_lock(tbl, bkt);\n\n\trht_for_each_from(he, rht_ptr(bkt, tbl, hash), tbl, hash) {\n\t\tstruct rhlist_head *list;\n\n\t\tlist = container_of(he, struct rhlist_head, rhead);\n\n\t\tif (he != obj) {\n\t\t\tstruct rhlist_head __rcu **lpprev;\n\n\t\t\tpprev = &he->next;\n\n\t\t\tif (!rhlist)\n\t\t\t\tcontinue;\n\n\t\t\tdo {\n\t\t\t\tlpprev = &list->next;\n\t\t\t\tlist = rht_dereference_bucket(list->next,\n\t\t\t\t\t\t\t      tbl, hash);\n\t\t\t} while (list && obj != &list->rhead);\n\n\t\t\tif (!list)\n\t\t\t\tcontinue;\n\n\t\t\tlist = rht_dereference_bucket(list->next, tbl, hash);\n\t\t\tRCU_INIT_POINTER(*lpprev, list);\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tobj = rht_dereference_bucket(obj->next, tbl, hash);\n\t\terr = 1;\n\n\t\tif (rhlist) {\n\t\t\tlist = rht_dereference_bucket(list->next, tbl, hash);\n\t\t\tif (list) {\n\t\t\t\tRCU_INIT_POINTER(list->rhead.next, obj);\n\t\t\t\tobj = &list->rhead;\n\t\t\t\terr = 0;\n\t\t\t}\n\t\t}\n\n\t\tif (pprev) {\n\t\t\trcu_assign_pointer(*pprev, obj);\n\t\t\trht_unlock(tbl, bkt, flags);\n\t\t} else {\n\t\t\trht_assign_unlock(tbl, bkt, obj, flags);\n\t\t}\n\t\tgoto unlocked;\n\t}\n\n\trht_unlock(tbl, bkt, flags);\nunlocked:\n\tif (err > 0) {\n\t\tatomic_dec(&ht->nelems);\n\t\tif (unlikely(ht->p.automatic_shrinking &&\n\t\t\t     rht_shrink_below_30(ht, tbl)))\n\t\t\tschedule_work(&ht->run_work);\n\t\terr = 0;\n\t}\n\n\treturn err;\n}\n\n \nstatic inline int __rhashtable_remove_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj,\n\tconst struct rhashtable_params params, bool rhlist)\n{\n\tstruct bucket_table *tbl;\n\tint err;\n\n\trcu_read_lock();\n\n\ttbl = rht_dereference_rcu(ht->tbl, ht);\n\n\t \n\twhile ((err = __rhashtable_remove_fast_one(ht, tbl, obj, params,\n\t\t\t\t\t\t   rhlist)) &&\n\t       (tbl = rht_dereference_rcu(tbl->future_tbl, ht)))\n\t\t;\n\n\trcu_read_unlock();\n\n\treturn err;\n}\n\n \nstatic inline int rhashtable_remove_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj,\n\tconst struct rhashtable_params params)\n{\n\treturn __rhashtable_remove_fast(ht, obj, params, false);\n}\n\n \nstatic inline int rhltable_remove(\n\tstruct rhltable *hlt, struct rhlist_head *list,\n\tconst struct rhashtable_params params)\n{\n\treturn __rhashtable_remove_fast(&hlt->ht, &list->rhead, params, true);\n}\n\n \nstatic inline int __rhashtable_replace_fast(\n\tstruct rhashtable *ht, struct bucket_table *tbl,\n\tstruct rhash_head *obj_old, struct rhash_head *obj_new,\n\tconst struct rhashtable_params params)\n{\n\tstruct rhash_lock_head __rcu **bkt;\n\tstruct rhash_head __rcu **pprev;\n\tstruct rhash_head *he;\n\tunsigned long flags;\n\tunsigned int hash;\n\tint err = -ENOENT;\n\n\t \n\thash = rht_head_hashfn(ht, tbl, obj_old, params);\n\tif (hash != rht_head_hashfn(ht, tbl, obj_new, params))\n\t\treturn -EINVAL;\n\n\tbkt = rht_bucket_var(tbl, hash);\n\tif (!bkt)\n\t\treturn -ENOENT;\n\n\tpprev = NULL;\n\tflags = rht_lock(tbl, bkt);\n\n\trht_for_each_from(he, rht_ptr(bkt, tbl, hash), tbl, hash) {\n\t\tif (he != obj_old) {\n\t\t\tpprev = &he->next;\n\t\t\tcontinue;\n\t\t}\n\n\t\trcu_assign_pointer(obj_new->next, obj_old->next);\n\t\tif (pprev) {\n\t\t\trcu_assign_pointer(*pprev, obj_new);\n\t\t\trht_unlock(tbl, bkt, flags);\n\t\t} else {\n\t\t\trht_assign_unlock(tbl, bkt, obj_new, flags);\n\t\t}\n\t\terr = 0;\n\t\tgoto unlocked;\n\t}\n\n\trht_unlock(tbl, bkt, flags);\n\nunlocked:\n\treturn err;\n}\n\n \nstatic inline int rhashtable_replace_fast(\n\tstruct rhashtable *ht, struct rhash_head *obj_old,\n\tstruct rhash_head *obj_new,\n\tconst struct rhashtable_params params)\n{\n\tstruct bucket_table *tbl;\n\tint err;\n\n\trcu_read_lock();\n\n\ttbl = rht_dereference_rcu(ht->tbl, ht);\n\n\t \n\twhile ((err = __rhashtable_replace_fast(ht, tbl, obj_old,\n\t\t\t\t\t\tobj_new, params)) &&\n\t       (tbl = rht_dereference_rcu(tbl->future_tbl, ht)))\n\t\t;\n\n\trcu_read_unlock();\n\n\treturn err;\n}\n\n \nstatic inline void rhltable_walk_enter(struct rhltable *hlt,\n\t\t\t\t       struct rhashtable_iter *iter)\n{\n\treturn rhashtable_walk_enter(&hlt->ht, iter);\n}\n\n \nstatic inline void rhltable_free_and_destroy(struct rhltable *hlt,\n\t\t\t\t\t     void (*free_fn)(void *ptr,\n\t\t\t\t\t\t\t     void *arg),\n\t\t\t\t\t     void *arg)\n{\n\treturn rhashtable_free_and_destroy(&hlt->ht, free_fn, arg);\n}\n\nstatic inline void rhltable_destroy(struct rhltable *hlt)\n{\n\treturn rhltable_free_and_destroy(hlt, NULL, NULL);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}