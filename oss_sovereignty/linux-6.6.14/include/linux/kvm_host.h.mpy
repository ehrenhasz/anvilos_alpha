{
  "module_name": "kvm_host.h",
  "hash_id": "b64f152e927647f62cc1c469ffc59b8d43cdfc2ca74f1be0aa84068905816a21",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/kvm_host.h",
  "human_readable_source": " \n#ifndef __KVM_HOST_H\n#define __KVM_HOST_H\n\n\n#include <linux/types.h>\n#include <linux/hardirq.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/spinlock.h>\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/sched/stat.h>\n#include <linux/bug.h>\n#include <linux/minmax.h>\n#include <linux/mm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/preempt.h>\n#include <linux/msi.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/rcupdate.h>\n#include <linux/ratelimit.h>\n#include <linux/err.h>\n#include <linux/irqflags.h>\n#include <linux/context_tracking.h>\n#include <linux/irqbypass.h>\n#include <linux/rcuwait.h>\n#include <linux/refcount.h>\n#include <linux/nospec.h>\n#include <linux/notifier.h>\n#include <linux/ftrace.h>\n#include <linux/hashtable.h>\n#include <linux/instrumentation.h>\n#include <linux/interval_tree.h>\n#include <linux/rbtree.h>\n#include <linux/xarray.h>\n#include <asm/signal.h>\n\n#include <linux/kvm.h>\n#include <linux/kvm_para.h>\n\n#include <linux/kvm_types.h>\n\n#include <asm/kvm_host.h>\n#include <linux/kvm_dirty_ring.h>\n\n#ifndef KVM_MAX_VCPU_IDS\n#define KVM_MAX_VCPU_IDS KVM_MAX_VCPUS\n#endif\n\n \n#define KVM_MEMSLOT_INVALID\t(1UL << 16)\n\n \n#define KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS\tBIT_ULL(63)\n\n \n#define KVM_MAX_MMIO_FRAGMENTS\t2\n\n#ifndef KVM_ADDRESS_SPACE_NUM\n#define KVM_ADDRESS_SPACE_NUM\t1\n#endif\n\n \n#define KVM_PFN_ERR_MASK\t(0x7ffULL << 52)\n#define KVM_PFN_ERR_NOSLOT_MASK\t(0xfffULL << 52)\n#define KVM_PFN_NOSLOT\t\t(0x1ULL << 63)\n\n#define KVM_PFN_ERR_FAULT\t(KVM_PFN_ERR_MASK)\n#define KVM_PFN_ERR_HWPOISON\t(KVM_PFN_ERR_MASK + 1)\n#define KVM_PFN_ERR_RO_FAULT\t(KVM_PFN_ERR_MASK + 2)\n#define KVM_PFN_ERR_SIGPENDING\t(KVM_PFN_ERR_MASK + 3)\n\n \nstatic inline bool is_error_pfn(kvm_pfn_t pfn)\n{\n\treturn !!(pfn & KVM_PFN_ERR_MASK);\n}\n\n \nstatic inline bool is_sigpending_pfn(kvm_pfn_t pfn)\n{\n\treturn pfn == KVM_PFN_ERR_SIGPENDING;\n}\n\n \nstatic inline bool is_error_noslot_pfn(kvm_pfn_t pfn)\n{\n\treturn !!(pfn & KVM_PFN_ERR_NOSLOT_MASK);\n}\n\n \nstatic inline bool is_noslot_pfn(kvm_pfn_t pfn)\n{\n\treturn pfn == KVM_PFN_NOSLOT;\n}\n\n \n#ifndef KVM_HVA_ERR_BAD\n\n#define KVM_HVA_ERR_BAD\t\t(PAGE_OFFSET)\n#define KVM_HVA_ERR_RO_BAD\t(PAGE_OFFSET + PAGE_SIZE)\n\nstatic inline bool kvm_is_error_hva(unsigned long addr)\n{\n\treturn addr >= PAGE_OFFSET;\n}\n\n#endif\n\n#define KVM_ERR_PTR_BAD_PAGE\t(ERR_PTR(-ENOENT))\n\nstatic inline bool is_error_page(struct page *page)\n{\n\treturn IS_ERR(page);\n}\n\n#define KVM_REQUEST_MASK           GENMASK(7,0)\n#define KVM_REQUEST_NO_WAKEUP      BIT(8)\n#define KVM_REQUEST_WAIT           BIT(9)\n#define KVM_REQUEST_NO_ACTION      BIT(10)\n \n#define KVM_REQ_TLB_FLUSH\t\t(0 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_VM_DEAD\t\t\t(1 | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_UNBLOCK\t\t\t2\n#define KVM_REQ_DIRTY_RING_SOFT_FULL\t3\n#define KVM_REQUEST_ARCH_BASE\t\t8\n\n \n#define KVM_REQ_OUTSIDE_GUEST_MODE\t(KVM_REQUEST_NO_ACTION | KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n\n#define KVM_ARCH_REQ_FLAGS(nr, flags) ({ \\\n\tBUILD_BUG_ON((unsigned)(nr) >= (sizeof_field(struct kvm_vcpu, requests) * 8) - KVM_REQUEST_ARCH_BASE); \\\n\t(unsigned)(((nr) + KVM_REQUEST_ARCH_BASE) | (flags)); \\\n})\n#define KVM_ARCH_REQ(nr)           KVM_ARCH_REQ_FLAGS(nr, 0)\n\nbool kvm_make_vcpus_request_mask(struct kvm *kvm, unsigned int req,\n\t\t\t\t unsigned long *vcpu_bitmap);\nbool kvm_make_all_cpus_request(struct kvm *kvm, unsigned int req);\nbool kvm_make_all_cpus_request_except(struct kvm *kvm, unsigned int req,\n\t\t\t\t      struct kvm_vcpu *except);\n\n#define KVM_USERSPACE_IRQ_SOURCE_ID\t\t0\n#define KVM_IRQFD_RESAMPLE_IRQ_SOURCE_ID\t1\n\nextern struct mutex kvm_lock;\nextern struct list_head vm_list;\n\nstruct kvm_io_range {\n\tgpa_t addr;\n\tint len;\n\tstruct kvm_io_device *dev;\n};\n\n#define NR_IOBUS_DEVS 1000\n\nstruct kvm_io_bus {\n\tint dev_count;\n\tint ioeventfd_count;\n\tstruct kvm_io_range range[];\n};\n\nenum kvm_bus {\n\tKVM_MMIO_BUS,\n\tKVM_PIO_BUS,\n\tKVM_VIRTIO_CCW_NOTIFY_BUS,\n\tKVM_FAST_MMIO_BUS,\n\tKVM_NR_BUSES\n};\n\nint kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,\n\t\t     int len, const void *val);\nint kvm_io_bus_write_cookie(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx,\n\t\t\t    gpa_t addr, int len, const void *val, long cookie);\nint kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,\n\t\t    int len, void *val);\nint kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,\n\t\t\t    int len, struct kvm_io_device *dev);\nint kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t      struct kvm_io_device *dev);\nstruct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,\n\t\t\t\t\t gpa_t addr);\n\n#ifdef CONFIG_KVM_ASYNC_PF\nstruct kvm_async_pf {\n\tstruct work_struct work;\n\tstruct list_head link;\n\tstruct list_head queue;\n\tstruct kvm_vcpu *vcpu;\n\tstruct mm_struct *mm;\n\tgpa_t cr2_or_gpa;\n\tunsigned long addr;\n\tstruct kvm_arch_async_pf arch;\n\tbool   wakeup_all;\n\tbool notpresent_injected;\n};\n\nvoid kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu);\nvoid kvm_check_async_pf_completion(struct kvm_vcpu *vcpu);\nbool kvm_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\tunsigned long hva, struct kvm_arch_async_pf *arch);\nint kvm_async_pf_wakeup_all(struct kvm_vcpu *vcpu);\n#endif\n\n#ifdef KVM_ARCH_WANT_MMU_NOTIFIER\nunion kvm_mmu_notifier_arg {\n\tpte_t pte;\n};\n\nstruct kvm_gfn_range {\n\tstruct kvm_memory_slot *slot;\n\tgfn_t start;\n\tgfn_t end;\n\tunion kvm_mmu_notifier_arg arg;\n\tbool may_block;\n};\nbool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range);\nbool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range);\nbool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range);\nbool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range);\n#endif\n\nenum {\n\tOUTSIDE_GUEST_MODE,\n\tIN_GUEST_MODE,\n\tEXITING_GUEST_MODE,\n\tREADING_SHADOW_PAGE_TABLES,\n};\n\n#define KVM_UNMAPPED_PAGE\t((void *) 0x500 + POISON_POINTER_DELTA)\n\nstruct kvm_host_map {\n\t \n\tstruct page *page;\n\tvoid *hva;\n\tkvm_pfn_t pfn;\n\tkvm_pfn_t gfn;\n};\n\n \nstatic inline bool kvm_vcpu_mapped(struct kvm_host_map *map)\n{\n\treturn !!map->hva;\n}\n\nstatic inline bool kvm_vcpu_can_poll(ktime_t cur, ktime_t stop)\n{\n\treturn single_task_running() && !need_resched() && ktime_before(cur, stop);\n}\n\n \nstruct kvm_mmio_fragment {\n\tgpa_t gpa;\n\tvoid *data;\n\tunsigned len;\n};\n\nstruct kvm_vcpu {\n\tstruct kvm *kvm;\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\tstruct preempt_notifier preempt_notifier;\n#endif\n\tint cpu;\n\tint vcpu_id;  \n\tint vcpu_idx;  \n\tint ____srcu_idx;  \n#ifdef CONFIG_PROVE_RCU\n\tint srcu_depth;\n#endif\n\tint mode;\n\tu64 requests;\n\tunsigned long guest_debug;\n\n\tstruct mutex mutex;\n\tstruct kvm_run *run;\n\n#ifndef __KVM_HAVE_ARCH_WQP\n\tstruct rcuwait wait;\n#endif\n\tstruct pid __rcu *pid;\n\tint sigset_active;\n\tsigset_t sigset;\n\tunsigned int halt_poll_ns;\n\tbool valid_wakeup;\n\n#ifdef CONFIG_HAS_IOMEM\n\tint mmio_needed;\n\tint mmio_read_completed;\n\tint mmio_is_write;\n\tint mmio_cur_fragment;\n\tint mmio_nr_fragments;\n\tstruct kvm_mmio_fragment mmio_fragments[KVM_MAX_MMIO_FRAGMENTS];\n#endif\n\n#ifdef CONFIG_KVM_ASYNC_PF\n\tstruct {\n\t\tu32 queued;\n\t\tstruct list_head queue;\n\t\tstruct list_head done;\n\t\tspinlock_t lock;\n\t} async_pf;\n#endif\n\n#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT\n\t \n\tstruct {\n\t\tbool in_spin_loop;\n\t\tbool dy_eligible;\n\t} spin_loop;\n#endif\n\tbool preempted;\n\tbool ready;\n\tstruct kvm_vcpu_arch arch;\n\tstruct kvm_vcpu_stat stat;\n\tchar stats_id[KVM_STATS_NAME_SIZE];\n\tstruct kvm_dirty_ring dirty_ring;\n\n\t \n\tstruct kvm_memory_slot *last_used_slot;\n\tu64 last_used_slot_gen;\n};\n\n \nstatic __always_inline void guest_timing_enter_irqoff(void)\n{\n\t \n\tinstrumentation_begin();\n\tvtime_account_guest_enter();\n\tinstrumentation_end();\n}\n\n \nstatic __always_inline void guest_context_enter_irqoff(void)\n{\n\t \n\tif (!context_tracking_guest_enter()) {\n\t\tinstrumentation_begin();\n\t\trcu_virt_note_context_switch();\n\t\tinstrumentation_end();\n\t}\n}\n\n \nstatic __always_inline void guest_enter_irqoff(void)\n{\n\tguest_timing_enter_irqoff();\n\tguest_context_enter_irqoff();\n}\n\n \nstatic __always_inline void guest_state_enter_irqoff(void)\n{\n\tinstrumentation_begin();\n\ttrace_hardirqs_on_prepare();\n\tlockdep_hardirqs_on_prepare();\n\tinstrumentation_end();\n\n\tguest_context_enter_irqoff();\n\tlockdep_hardirqs_on(CALLER_ADDR0);\n}\n\n \nstatic __always_inline void guest_context_exit_irqoff(void)\n{\n\tcontext_tracking_guest_exit();\n}\n\n \nstatic __always_inline void guest_timing_exit_irqoff(void)\n{\n\tinstrumentation_begin();\n\t \n\tvtime_account_guest_exit();\n\tinstrumentation_end();\n}\n\n \nstatic __always_inline void guest_exit_irqoff(void)\n{\n\tguest_context_exit_irqoff();\n\tguest_timing_exit_irqoff();\n}\n\nstatic inline void guest_exit(void)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tguest_exit_irqoff();\n\tlocal_irq_restore(flags);\n}\n\n \nstatic __always_inline void guest_state_exit_irqoff(void)\n{\n\tlockdep_hardirqs_off(CALLER_ADDR0);\n\tguest_context_exit_irqoff();\n\n\tinstrumentation_begin();\n\ttrace_hardirqs_off_finish();\n\tinstrumentation_end();\n}\n\nstatic inline int kvm_vcpu_exiting_guest_mode(struct kvm_vcpu *vcpu)\n{\n\t \n\tsmp_mb__before_atomic();\n\treturn cmpxchg(&vcpu->mode, IN_GUEST_MODE, EXITING_GUEST_MODE);\n}\n\n \n#define KVM_MEM_MAX_NR_PAGES ((1UL << 31) - 1)\n\n \nstruct kvm_memory_slot {\n\tstruct hlist_node id_node[2];\n\tstruct interval_tree_node hva_node[2];\n\tstruct rb_node gfn_node[2];\n\tgfn_t base_gfn;\n\tunsigned long npages;\n\tunsigned long *dirty_bitmap;\n\tstruct kvm_arch_memory_slot arch;\n\tunsigned long userspace_addr;\n\tu32 flags;\n\tshort id;\n\tu16 as_id;\n};\n\nstatic inline bool kvm_slot_dirty_track_enabled(const struct kvm_memory_slot *slot)\n{\n\treturn slot->flags & KVM_MEM_LOG_DIRTY_PAGES;\n}\n\nstatic inline unsigned long kvm_dirty_bitmap_bytes(struct kvm_memory_slot *memslot)\n{\n\treturn ALIGN(memslot->npages, BITS_PER_LONG) / 8;\n}\n\nstatic inline unsigned long *kvm_second_dirty_bitmap(struct kvm_memory_slot *memslot)\n{\n\tunsigned long len = kvm_dirty_bitmap_bytes(memslot);\n\n\treturn memslot->dirty_bitmap + len / sizeof(*memslot->dirty_bitmap);\n}\n\n#ifndef KVM_DIRTY_LOG_MANUAL_CAPS\n#define KVM_DIRTY_LOG_MANUAL_CAPS KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE\n#endif\n\nstruct kvm_s390_adapter_int {\n\tu64 ind_addr;\n\tu64 summary_addr;\n\tu64 ind_offset;\n\tu32 summary_offset;\n\tu32 adapter_id;\n};\n\nstruct kvm_hv_sint {\n\tu32 vcpu;\n\tu32 sint;\n};\n\nstruct kvm_xen_evtchn {\n\tu32 port;\n\tu32 vcpu_id;\n\tint vcpu_idx;\n\tu32 priority;\n};\n\nstruct kvm_kernel_irq_routing_entry {\n\tu32 gsi;\n\tu32 type;\n\tint (*set)(struct kvm_kernel_irq_routing_entry *e,\n\t\t   struct kvm *kvm, int irq_source_id, int level,\n\t\t   bool line_status);\n\tunion {\n\t\tstruct {\n\t\t\tunsigned irqchip;\n\t\t\tunsigned pin;\n\t\t} irqchip;\n\t\tstruct {\n\t\t\tu32 address_lo;\n\t\t\tu32 address_hi;\n\t\t\tu32 data;\n\t\t\tu32 flags;\n\t\t\tu32 devid;\n\t\t} msi;\n\t\tstruct kvm_s390_adapter_int adapter;\n\t\tstruct kvm_hv_sint hv_sint;\n\t\tstruct kvm_xen_evtchn xen_evtchn;\n\t};\n\tstruct hlist_node link;\n};\n\n#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING\nstruct kvm_irq_routing_table {\n\tint chip[KVM_NR_IRQCHIPS][KVM_IRQCHIP_NUM_PINS];\n\tu32 nr_rt_entries;\n\t \n\tstruct hlist_head map[];\n};\n#endif\n\nbool kvm_arch_irqchip_in_kernel(struct kvm *kvm);\n\n#ifndef KVM_INTERNAL_MEM_SLOTS\n#define KVM_INTERNAL_MEM_SLOTS 0\n#endif\n\n#define KVM_MEM_SLOTS_NUM SHRT_MAX\n#define KVM_USER_MEM_SLOTS (KVM_MEM_SLOTS_NUM - KVM_INTERNAL_MEM_SLOTS)\n\n#ifndef __KVM_VCPU_MULTIPLE_ADDRESS_SPACE\nstatic inline int kvm_arch_vcpu_memslots_id(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n#endif\n\nstruct kvm_memslots {\n\tu64 generation;\n\tatomic_long_t last_used_slot;\n\tstruct rb_root_cached hva_tree;\n\tstruct rb_root gfn_tree;\n\t \n\tDECLARE_HASHTABLE(id_hash, 7);\n\tint node_idx;\n};\n\nstruct kvm {\n#ifdef KVM_HAVE_MMU_RWLOCK\n\trwlock_t mmu_lock;\n#else\n\tspinlock_t mmu_lock;\n#endif  \n\n\tstruct mutex slots_lock;\n\n\t \n\tstruct mutex slots_arch_lock;\n\tstruct mm_struct *mm;  \n\tunsigned long nr_memslot_pages;\n\t \n\tstruct kvm_memslots __memslots[KVM_ADDRESS_SPACE_NUM][2];\n\t \n\tstruct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];\n\tstruct xarray vcpu_array;\n\t \n\tatomic_t nr_memslots_dirty_logging;\n\n\t \n\tspinlock_t mn_invalidate_lock;\n\tunsigned long mn_active_invalidate_count;\n\tstruct rcuwait mn_memslots_update_rcuwait;\n\n\t \n\tspinlock_t gpc_lock;\n\tstruct list_head gpc_list;\n\n\t \n\tatomic_t online_vcpus;\n\tint max_vcpus;\n\tint created_vcpus;\n\tint last_boosted_vcpu;\n\tstruct list_head vm_list;\n\tstruct mutex lock;\n\tstruct kvm_io_bus __rcu *buses[KVM_NR_BUSES];\n#ifdef CONFIG_HAVE_KVM_EVENTFD\n\tstruct {\n\t\tspinlock_t        lock;\n\t\tstruct list_head  items;\n\t\t \n\t\tstruct list_head  resampler_list;\n\t\tstruct mutex      resampler_lock;\n\t} irqfds;\n\tstruct list_head ioeventfds;\n#endif\n\tstruct kvm_vm_stat stat;\n\tstruct kvm_arch arch;\n\trefcount_t users_count;\n#ifdef CONFIG_KVM_MMIO\n\tstruct kvm_coalesced_mmio_ring *coalesced_mmio_ring;\n\tspinlock_t ring_lock;\n\tstruct list_head coalesced_zones;\n#endif\n\n\tstruct mutex irq_lock;\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\n\t \n\tstruct kvm_irq_routing_table __rcu *irq_routing;\n#endif\n#ifdef CONFIG_HAVE_KVM_IRQFD\n\tstruct hlist_head irq_ack_notifier_list;\n#endif\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\n\tstruct mmu_notifier mmu_notifier;\n\tunsigned long mmu_invalidate_seq;\n\tlong mmu_invalidate_in_progress;\n\tunsigned long mmu_invalidate_range_start;\n\tunsigned long mmu_invalidate_range_end;\n#endif\n\tstruct list_head devices;\n\tu64 manual_dirty_log_protect;\n\tstruct dentry *debugfs_dentry;\n\tstruct kvm_stat_data **debugfs_stat_data;\n\tstruct srcu_struct srcu;\n\tstruct srcu_struct irq_srcu;\n\tpid_t userspace_pid;\n\tbool override_halt_poll_ns;\n\tunsigned int max_halt_poll_ns;\n\tu32 dirty_ring_size;\n\tbool dirty_ring_with_bitmap;\n\tbool vm_bugged;\n\tbool vm_dead;\n\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\n\tstruct notifier_block pm_notifier;\n#endif\n\tchar stats_id[KVM_STATS_NAME_SIZE];\n};\n\n#define kvm_err(fmt, ...) \\\n\tpr_err(\"kvm [%i]: \" fmt, task_pid_nr(current), ## __VA_ARGS__)\n#define kvm_info(fmt, ...) \\\n\tpr_info(\"kvm [%i]: \" fmt, task_pid_nr(current), ## __VA_ARGS__)\n#define kvm_debug(fmt, ...) \\\n\tpr_debug(\"kvm [%i]: \" fmt, task_pid_nr(current), ## __VA_ARGS__)\n#define kvm_debug_ratelimited(fmt, ...) \\\n\tpr_debug_ratelimited(\"kvm [%i]: \" fmt, task_pid_nr(current), \\\n\t\t\t     ## __VA_ARGS__)\n#define kvm_pr_unimpl(fmt, ...) \\\n\tpr_err_ratelimited(\"kvm [%i]: \" fmt, \\\n\t\t\t   task_tgid_nr(current), ## __VA_ARGS__)\n\n \n#define vcpu_unimpl(vcpu, fmt, ...)\t\t\t\t\t\\\n\tkvm_pr_unimpl(\"vcpu%i, guest rIP: 0x%lx \" fmt,\t\t\t\\\n\t\t\t(vcpu)->vcpu_id, kvm_rip_read(vcpu), ## __VA_ARGS__)\n\n#define vcpu_debug(vcpu, fmt, ...)\t\t\t\t\t\\\n\tkvm_debug(\"vcpu%i \" fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)\n#define vcpu_debug_ratelimited(vcpu, fmt, ...)\t\t\t\t\\\n\tkvm_debug_ratelimited(\"vcpu%i \" fmt, (vcpu)->vcpu_id,           \\\n\t\t\t      ## __VA_ARGS__)\n#define vcpu_err(vcpu, fmt, ...)\t\t\t\t\t\\\n\tkvm_err(\"vcpu%i \" fmt, (vcpu)->vcpu_id, ## __VA_ARGS__)\n\nstatic inline void kvm_vm_dead(struct kvm *kvm)\n{\n\tkvm->vm_dead = true;\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_VM_DEAD);\n}\n\nstatic inline void kvm_vm_bugged(struct kvm *kvm)\n{\n\tkvm->vm_bugged = true;\n\tkvm_vm_dead(kvm);\n}\n\n\n#define KVM_BUG(cond, kvm, fmt...)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tbool __ret = !!(cond);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (WARN_ONCE(__ret && !(kvm)->vm_bugged, fmt))\t\t\\\n\t\tkvm_vm_bugged(kvm);\t\t\t\t\\\n\tunlikely(__ret);\t\t\t\t\t\\\n})\n\n#define KVM_BUG_ON(cond, kvm)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tbool __ret = !!(cond);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (WARN_ON_ONCE(__ret && !(kvm)->vm_bugged))\t\t\\\n\t\tkvm_vm_bugged(kvm);\t\t\t\t\\\n\tunlikely(__ret);\t\t\t\t\t\\\n})\n\n \n#define KVM_BUG_ON_DATA_CORRUPTION(cond, kvm)\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tbool __ret = !!(cond);\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (IS_ENABLED(CONFIG_BUG_ON_DATA_CORRUPTION))\t\t\\\n\t\tBUG_ON(__ret);\t\t\t\t\t\\\n\telse if (WARN_ON_ONCE(__ret && !(kvm)->vm_bugged))\t\\\n\t\tkvm_vm_bugged(kvm);\t\t\t\t\\\n\tunlikely(__ret);\t\t\t\t\t\\\n})\n\nstatic inline void kvm_vcpu_srcu_read_lock(struct kvm_vcpu *vcpu)\n{\n#ifdef CONFIG_PROVE_RCU\n\tWARN_ONCE(vcpu->srcu_depth++,\n\t\t  \"KVM: Illegal vCPU srcu_idx LOCK, depth=%d\", vcpu->srcu_depth - 1);\n#endif\n\tvcpu->____srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n}\n\nstatic inline void kvm_vcpu_srcu_read_unlock(struct kvm_vcpu *vcpu)\n{\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->____srcu_idx);\n\n#ifdef CONFIG_PROVE_RCU\n\tWARN_ONCE(--vcpu->srcu_depth,\n\t\t  \"KVM: Illegal vCPU srcu_idx UNLOCK, depth=%d\", vcpu->srcu_depth);\n#endif\n}\n\nstatic inline bool kvm_dirty_log_manual_protect_and_init_set(struct kvm *kvm)\n{\n\treturn !!(kvm->manual_dirty_log_protect & KVM_DIRTY_LOG_INITIALLY_SET);\n}\n\nstatic inline struct kvm_io_bus *kvm_get_bus(struct kvm *kvm, enum kvm_bus idx)\n{\n\treturn srcu_dereference_check(kvm->buses[idx], &kvm->srcu,\n\t\t\t\t      lockdep_is_held(&kvm->slots_lock) ||\n\t\t\t\t      !refcount_read(&kvm->users_count));\n}\n\nstatic inline struct kvm_vcpu *kvm_get_vcpu(struct kvm *kvm, int i)\n{\n\tint num_vcpus = atomic_read(&kvm->online_vcpus);\n\ti = array_index_nospec(i, num_vcpus);\n\n\t \n\tsmp_rmb();\n\treturn xa_load(&kvm->vcpu_array, i);\n}\n\n#define kvm_for_each_vcpu(idx, vcpup, kvm)\t\t   \\\n\txa_for_each_range(&kvm->vcpu_array, idx, vcpup, 0, \\\n\t\t\t  (atomic_read(&kvm->online_vcpus) - 1))\n\nstatic inline struct kvm_vcpu *kvm_get_vcpu_by_id(struct kvm *kvm, int id)\n{\n\tstruct kvm_vcpu *vcpu = NULL;\n\tunsigned long i;\n\n\tif (id < 0)\n\t\treturn NULL;\n\tif (id < KVM_MAX_VCPUS)\n\t\tvcpu = kvm_get_vcpu(kvm, id);\n\tif (vcpu && vcpu->vcpu_id == id)\n\t\treturn vcpu;\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tif (vcpu->vcpu_id == id)\n\t\t\treturn vcpu;\n\treturn NULL;\n}\n\nvoid kvm_destroy_vcpus(struct kvm *kvm);\n\nvoid vcpu_load(struct kvm_vcpu *vcpu);\nvoid vcpu_put(struct kvm_vcpu *vcpu);\n\n#ifdef __KVM_HAVE_IOAPIC\nvoid kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm);\nvoid kvm_arch_post_irq_routing_update(struct kvm *kvm);\n#else\nstatic inline void kvm_arch_post_irq_ack_notifier_list_update(struct kvm *kvm)\n{\n}\nstatic inline void kvm_arch_post_irq_routing_update(struct kvm *kvm)\n{\n}\n#endif\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\nint kvm_irqfd_init(void);\nvoid kvm_irqfd_exit(void);\n#else\nstatic inline int kvm_irqfd_init(void)\n{\n\treturn 0;\n}\n\nstatic inline void kvm_irqfd_exit(void)\n{\n}\n#endif\nint kvm_init(unsigned vcpu_size, unsigned vcpu_align, struct module *module);\nvoid kvm_exit(void);\n\nvoid kvm_get_kvm(struct kvm *kvm);\nbool kvm_get_kvm_safe(struct kvm *kvm);\nvoid kvm_put_kvm(struct kvm *kvm);\nbool file_is_kvm(struct file *file);\nvoid kvm_put_kvm_no_destroy(struct kvm *kvm);\n\nstatic inline struct kvm_memslots *__kvm_memslots(struct kvm *kvm, int as_id)\n{\n\tas_id = array_index_nospec(as_id, KVM_ADDRESS_SPACE_NUM);\n\treturn srcu_dereference_check(kvm->memslots[as_id], &kvm->srcu,\n\t\t\tlockdep_is_held(&kvm->slots_lock) ||\n\t\t\t!refcount_read(&kvm->users_count));\n}\n\nstatic inline struct kvm_memslots *kvm_memslots(struct kvm *kvm)\n{\n\treturn __kvm_memslots(kvm, 0);\n}\n\nstatic inline struct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu)\n{\n\tint as_id = kvm_arch_vcpu_memslots_id(vcpu);\n\n\treturn __kvm_memslots(vcpu->kvm, as_id);\n}\n\nstatic inline bool kvm_memslots_empty(struct kvm_memslots *slots)\n{\n\treturn RB_EMPTY_ROOT(&slots->gfn_tree);\n}\n\nbool kvm_are_all_memslots_empty(struct kvm *kvm);\n\n#define kvm_for_each_memslot(memslot, bkt, slots)\t\t\t      \\\n\thash_for_each(slots->id_hash, bkt, memslot, id_node[slots->node_idx]) \\\n\t\tif (WARN_ON_ONCE(!memslot->npages)) {\t\t\t      \\\n\t\t} else\n\nstatic inline\nstruct kvm_memory_slot *id_to_memslot(struct kvm_memslots *slots, int id)\n{\n\tstruct kvm_memory_slot *slot;\n\tint idx = slots->node_idx;\n\n\thash_for_each_possible(slots->id_hash, slot, id_node[idx], id) {\n\t\tif (slot->id == id)\n\t\t\treturn slot;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct kvm_memslot_iter {\n\tstruct kvm_memslots *slots;\n\tstruct rb_node *node;\n\tstruct kvm_memory_slot *slot;\n};\n\nstatic inline void kvm_memslot_iter_next(struct kvm_memslot_iter *iter)\n{\n\titer->node = rb_next(iter->node);\n\tif (!iter->node)\n\t\treturn;\n\n\titer->slot = container_of(iter->node, struct kvm_memory_slot, gfn_node[iter->slots->node_idx]);\n}\n\nstatic inline void kvm_memslot_iter_start(struct kvm_memslot_iter *iter,\n\t\t\t\t\t  struct kvm_memslots *slots,\n\t\t\t\t\t  gfn_t start)\n{\n\tint idx = slots->node_idx;\n\tstruct rb_node *tmp;\n\tstruct kvm_memory_slot *slot;\n\n\titer->slots = slots;\n\n\t \n\titer->node = NULL;\n\tfor (tmp = slots->gfn_tree.rb_node; tmp; ) {\n\t\tslot = container_of(tmp, struct kvm_memory_slot, gfn_node[idx]);\n\t\tif (start < slot->base_gfn) {\n\t\t\titer->node = tmp;\n\t\t\ttmp = tmp->rb_left;\n\t\t} else {\n\t\t\ttmp = tmp->rb_right;\n\t\t}\n\t}\n\n\t \n\tif (iter->node) {\n\t\t \n\t\ttmp = rb_prev(iter->node);\n\t\tif (tmp)\n\t\t\titer->node = tmp;\n\t} else {\n\t\t \n\t\titer->node = rb_last(&slots->gfn_tree);\n\t}\n\n\tif (iter->node) {\n\t\titer->slot = container_of(iter->node, struct kvm_memory_slot, gfn_node[idx]);\n\n\t\t \n\t\tif (iter->slot->base_gfn + iter->slot->npages <= start)\n\t\t\tkvm_memslot_iter_next(iter);\n\t}\n}\n\nstatic inline bool kvm_memslot_iter_is_valid(struct kvm_memslot_iter *iter, gfn_t end)\n{\n\tif (!iter->node)\n\t\treturn false;\n\n\t \n\treturn iter->slot->base_gfn < end;\n}\n\n \n#define kvm_for_each_memslot_in_gfn_range(iter, slots, start, end)\t\\\n\tfor (kvm_memslot_iter_start(iter, slots, start);\t\t\\\n\t     kvm_memslot_iter_is_valid(iter, end);\t\t\t\\\n\t     kvm_memslot_iter_next(iter))\n\n \nenum kvm_mr_change {\n\tKVM_MR_CREATE,\n\tKVM_MR_DELETE,\n\tKVM_MR_MOVE,\n\tKVM_MR_FLAGS_ONLY,\n};\n\nint kvm_set_memory_region(struct kvm *kvm,\n\t\t\t  const struct kvm_userspace_memory_region *mem);\nint __kvm_set_memory_region(struct kvm *kvm,\n\t\t\t    const struct kvm_userspace_memory_region *mem);\nvoid kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot);\nvoid kvm_arch_memslots_updated(struct kvm *kvm, u64 gen);\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\n\t\t\t\tconst struct kvm_memory_slot *old,\n\t\t\t\tstruct kvm_memory_slot *new,\n\t\t\t\tenum kvm_mr_change change);\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_memory_slot *old,\n\t\t\t\tconst struct kvm_memory_slot *new,\n\t\t\t\tenum kvm_mr_change change);\n \nvoid kvm_arch_flush_shadow_all(struct kvm *kvm);\n \nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot);\n\nint gfn_to_page_many_atomic(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t    struct page **pages, int nr_pages);\n\nstruct page *gfn_to_page(struct kvm *kvm, gfn_t gfn);\nunsigned long gfn_to_hva(struct kvm *kvm, gfn_t gfn);\nunsigned long gfn_to_hva_prot(struct kvm *kvm, gfn_t gfn, bool *writable);\nunsigned long gfn_to_hva_memslot(struct kvm_memory_slot *slot, gfn_t gfn);\nunsigned long gfn_to_hva_memslot_prot(struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t      bool *writable);\nvoid kvm_release_page_clean(struct page *page);\nvoid kvm_release_page_dirty(struct page *page);\n\nkvm_pfn_t gfn_to_pfn(struct kvm *kvm, gfn_t gfn);\nkvm_pfn_t gfn_to_pfn_prot(struct kvm *kvm, gfn_t gfn, bool write_fault,\n\t\t      bool *writable);\nkvm_pfn_t gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn);\nkvm_pfn_t gfn_to_pfn_memslot_atomic(const struct kvm_memory_slot *slot, gfn_t gfn);\nkvm_pfn_t __gfn_to_pfn_memslot(const struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t       bool atomic, bool interruptible, bool *async,\n\t\t\t       bool write_fault, bool *writable, hva_t *hva);\n\nvoid kvm_release_pfn_clean(kvm_pfn_t pfn);\nvoid kvm_release_pfn_dirty(kvm_pfn_t pfn);\nvoid kvm_set_pfn_dirty(kvm_pfn_t pfn);\nvoid kvm_set_pfn_accessed(kvm_pfn_t pfn);\n\nvoid kvm_release_pfn(kvm_pfn_t pfn, bool dirty);\nint kvm_read_guest_page(struct kvm *kvm, gfn_t gfn, void *data, int offset,\n\t\t\tint len);\nint kvm_read_guest(struct kvm *kvm, gpa_t gpa, void *data, unsigned long len);\nint kvm_read_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len);\nint kvm_read_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t\t void *data, unsigned int offset,\n\t\t\t\t unsigned long len);\nint kvm_write_guest_page(struct kvm *kvm, gfn_t gfn, const void *data,\n\t\t\t int offset, int len);\nint kvm_write_guest(struct kvm *kvm, gpa_t gpa, const void *data,\n\t\t    unsigned long len);\nint kvm_write_guest_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t   void *data, unsigned long len);\nint kvm_write_guest_offset_cached(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t\t  void *data, unsigned int offset,\n\t\t\t\t  unsigned long len);\nint kvm_gfn_to_hva_cache_init(struct kvm *kvm, struct gfn_to_hva_cache *ghc,\n\t\t\t      gpa_t gpa, unsigned long len);\n\n#define __kvm_get_guest(kvm, gfn, offset, v)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned long __addr = gfn_to_hva(kvm, gfn);\t\t\t\\\n\ttypeof(v) __user *__uaddr = (typeof(__uaddr))(__addr + offset);\t\\\n\tint __ret = -EFAULT;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!kvm_is_error_hva(__addr))\t\t\t\t\t\\\n\t\t__ret = get_user(v, __uaddr);\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define kvm_get_guest(kvm, gpa, v)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tgpa_t __gpa = gpa;\t\t\t\t\t\t\\\n\tstruct kvm *__kvm = kvm;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__kvm_get_guest(__kvm, __gpa >> PAGE_SHIFT,\t\t\t\\\n\t\t\toffset_in_page(__gpa), v);\t\t\t\\\n})\n\n#define __kvm_put_guest(kvm, gfn, offset, v)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned long __addr = gfn_to_hva(kvm, gfn);\t\t\t\\\n\ttypeof(v) __user *__uaddr = (typeof(__uaddr))(__addr + offset);\t\\\n\tint __ret = -EFAULT;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif (!kvm_is_error_hva(__addr))\t\t\t\t\t\\\n\t\t__ret = put_user(v, __uaddr);\t\t\t\t\\\n\tif (!__ret)\t\t\t\t\t\t\t\\\n\t\tmark_page_dirty(kvm, gfn);\t\t\t\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define kvm_put_guest(kvm, gpa, v)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tgpa_t __gpa = gpa;\t\t\t\t\t\t\\\n\tstruct kvm *__kvm = kvm;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t__kvm_put_guest(__kvm, __gpa >> PAGE_SHIFT,\t\t\t\\\n\t\t\toffset_in_page(__gpa), v);\t\t\t\\\n})\n\nint kvm_clear_guest(struct kvm *kvm, gpa_t gpa, unsigned long len);\nstruct kvm_memory_slot *gfn_to_memslot(struct kvm *kvm, gfn_t gfn);\nbool kvm_is_visible_gfn(struct kvm *kvm, gfn_t gfn);\nbool kvm_vcpu_is_visible_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);\nunsigned long kvm_host_page_size(struct kvm_vcpu *vcpu, gfn_t gfn);\nvoid mark_page_dirty_in_slot(struct kvm *kvm, const struct kvm_memory_slot *memslot, gfn_t gfn);\nvoid mark_page_dirty(struct kvm *kvm, gfn_t gfn);\n\nstruct kvm_memslots *kvm_vcpu_memslots(struct kvm_vcpu *vcpu);\nstruct kvm_memory_slot *kvm_vcpu_gfn_to_memslot(struct kvm_vcpu *vcpu, gfn_t gfn);\nkvm_pfn_t kvm_vcpu_gfn_to_pfn_atomic(struct kvm_vcpu *vcpu, gfn_t gfn);\nkvm_pfn_t kvm_vcpu_gfn_to_pfn(struct kvm_vcpu *vcpu, gfn_t gfn);\nint kvm_vcpu_map(struct kvm_vcpu *vcpu, gpa_t gpa, struct kvm_host_map *map);\nvoid kvm_vcpu_unmap(struct kvm_vcpu *vcpu, struct kvm_host_map *map, bool dirty);\nunsigned long kvm_vcpu_gfn_to_hva(struct kvm_vcpu *vcpu, gfn_t gfn);\nunsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *writable);\nint kvm_vcpu_read_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, void *data, int offset,\n\t\t\t     int len);\nint kvm_vcpu_read_guest_atomic(struct kvm_vcpu *vcpu, gpa_t gpa, void *data,\n\t\t\t       unsigned long len);\nint kvm_vcpu_read_guest(struct kvm_vcpu *vcpu, gpa_t gpa, void *data,\n\t\t\tunsigned long len);\nint kvm_vcpu_write_guest_page(struct kvm_vcpu *vcpu, gfn_t gfn, const void *data,\n\t\t\t      int offset, int len);\nint kvm_vcpu_write_guest(struct kvm_vcpu *vcpu, gpa_t gpa, const void *data,\n\t\t\t unsigned long len);\nvoid kvm_vcpu_mark_page_dirty(struct kvm_vcpu *vcpu, gfn_t gfn);\n\n \nvoid kvm_gpc_init(struct gfn_to_pfn_cache *gpc, struct kvm *kvm,\n\t\t  struct kvm_vcpu *vcpu, enum pfn_cache_usage usage);\n\n \nint kvm_gpc_activate(struct gfn_to_pfn_cache *gpc, gpa_t gpa, unsigned long len);\n\n \nbool kvm_gpc_check(struct gfn_to_pfn_cache *gpc, unsigned long len);\n\n \nint kvm_gpc_refresh(struct gfn_to_pfn_cache *gpc, unsigned long len);\n\n \nvoid kvm_gpc_deactivate(struct gfn_to_pfn_cache *gpc);\n\nvoid kvm_sigset_activate(struct kvm_vcpu *vcpu);\nvoid kvm_sigset_deactivate(struct kvm_vcpu *vcpu);\n\nvoid kvm_vcpu_halt(struct kvm_vcpu *vcpu);\nbool kvm_vcpu_block(struct kvm_vcpu *vcpu);\nvoid kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);\nvoid kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);\nbool kvm_vcpu_wake_up(struct kvm_vcpu *vcpu);\nvoid kvm_vcpu_kick(struct kvm_vcpu *vcpu);\nint kvm_vcpu_yield_to(struct kvm_vcpu *target);\nvoid kvm_vcpu_on_spin(struct kvm_vcpu *vcpu, bool yield_to_kernel_mode);\n\nvoid kvm_flush_remote_tlbs(struct kvm *kvm);\nvoid kvm_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages);\nvoid kvm_flush_remote_tlbs_memslot(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *memslot);\n\n#ifdef KVM_ARCH_NR_OBJS_PER_MEMORY_CACHE\nint kvm_mmu_topup_memory_cache(struct kvm_mmu_memory_cache *mc, int min);\nint __kvm_mmu_topup_memory_cache(struct kvm_mmu_memory_cache *mc, int capacity, int min);\nint kvm_mmu_memory_cache_nr_free_objects(struct kvm_mmu_memory_cache *mc);\nvoid kvm_mmu_free_memory_cache(struct kvm_mmu_memory_cache *mc);\nvoid *kvm_mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc);\n#endif\n\nvoid kvm_mmu_invalidate_begin(struct kvm *kvm, unsigned long start,\n\t\t\t      unsigned long end);\nvoid kvm_mmu_invalidate_end(struct kvm *kvm, unsigned long start,\n\t\t\t    unsigned long end);\n\nlong kvm_arch_dev_ioctl(struct file *filp,\n\t\t\tunsigned int ioctl, unsigned long arg);\nlong kvm_arch_vcpu_ioctl(struct file *filp,\n\t\t\t unsigned int ioctl, unsigned long arg);\nvm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf);\n\nint kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext);\n\nvoid kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t\tstruct kvm_memory_slot *slot,\n\t\t\t\t\tgfn_t gfn_offset,\n\t\t\t\t\tunsigned long mask);\nvoid kvm_arch_sync_dirty_log(struct kvm *kvm, struct kvm_memory_slot *memslot);\n\n#ifndef CONFIG_KVM_GENERIC_DIRTYLOG_READ_PROTECT\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log);\nint kvm_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log,\n\t\t      int *is_dirty, struct kvm_memory_slot **memslot);\n#endif\n\nint kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_level,\n\t\t\tbool line_status);\nint kvm_vm_ioctl_enable_cap(struct kvm *kvm,\n\t\t\t    struct kvm_enable_cap *cap);\nint kvm_arch_vm_ioctl(struct file *filp, unsigned int ioctl, unsigned long arg);\nlong kvm_arch_vm_compat_ioctl(struct file *filp, unsigned int ioctl,\n\t\t\t      unsigned long arg);\n\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu);\n\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_translation *tr);\n\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs);\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs);\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs);\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_sregs *sregs);\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state);\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mp_state *mp_state);\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_guest_debug *dbg);\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu);\n\nvoid kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu);\n\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu);\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu);\nint kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id);\nint kvm_arch_vcpu_create(struct kvm_vcpu *vcpu);\nvoid kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu);\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu);\n\n#ifdef CONFIG_HAVE_KVM_PM_NOTIFIER\nint kvm_arch_pm_notifier(struct kvm *kvm, unsigned long state);\n#endif\n\n#ifdef __KVM_HAVE_ARCH_VCPU_DEBUGFS\nvoid kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu, struct dentry *debugfs_dentry);\n#else\nstatic inline void kvm_create_vcpu_debugfs(struct kvm_vcpu *vcpu) {}\n#endif\n\n#ifdef CONFIG_KVM_GENERIC_HARDWARE_ENABLING\nint kvm_arch_hardware_enable(void);\nvoid kvm_arch_hardware_disable(void);\n#endif\nint kvm_arch_vcpu_runnable(struct kvm_vcpu *vcpu);\nbool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu);\nint kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu);\nbool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu);\nbool kvm_arch_dy_has_pending_interrupt(struct kvm_vcpu *vcpu);\nint kvm_arch_post_init_vm(struct kvm *kvm);\nvoid kvm_arch_pre_destroy_vm(struct kvm *kvm);\nint kvm_arch_create_vm_debugfs(struct kvm *kvm);\n\n#ifndef __KVM_HAVE_ARCH_VM_ALLOC\n \nstatic inline struct kvm *kvm_arch_alloc_vm(void)\n{\n\treturn kzalloc(sizeof(struct kvm), GFP_KERNEL_ACCOUNT);\n}\n#endif\n\nstatic inline void __kvm_arch_free_vm(struct kvm *kvm)\n{\n\tkvfree(kvm);\n}\n\n#ifndef __KVM_HAVE_ARCH_VM_FREE\nstatic inline void kvm_arch_free_vm(struct kvm *kvm)\n{\n\t__kvm_arch_free_vm(kvm);\n}\n#endif\n\n#ifndef __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS\nstatic inline int kvm_arch_flush_remote_tlbs(struct kvm *kvm)\n{\n\treturn -ENOTSUPP;\n}\n#else\nint kvm_arch_flush_remote_tlbs(struct kvm *kvm);\n#endif\n\n#ifndef __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS_RANGE\nstatic inline int kvm_arch_flush_remote_tlbs_range(struct kvm *kvm,\n\t\t\t\t\t\t    gfn_t gfn, u64 nr_pages)\n{\n\treturn -EOPNOTSUPP;\n}\n#else\nint kvm_arch_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages);\n#endif\n\n#ifdef __KVM_HAVE_ARCH_NONCOHERENT_DMA\nvoid kvm_arch_register_noncoherent_dma(struct kvm *kvm);\nvoid kvm_arch_unregister_noncoherent_dma(struct kvm *kvm);\nbool kvm_arch_has_noncoherent_dma(struct kvm *kvm);\n#else\nstatic inline void kvm_arch_register_noncoherent_dma(struct kvm *kvm)\n{\n}\n\nstatic inline void kvm_arch_unregister_noncoherent_dma(struct kvm *kvm)\n{\n}\n\nstatic inline bool kvm_arch_has_noncoherent_dma(struct kvm *kvm)\n{\n\treturn false;\n}\n#endif\n#ifdef __KVM_HAVE_ARCH_ASSIGNED_DEVICE\nvoid kvm_arch_start_assignment(struct kvm *kvm);\nvoid kvm_arch_end_assignment(struct kvm *kvm);\nbool kvm_arch_has_assigned_device(struct kvm *kvm);\n#else\nstatic inline void kvm_arch_start_assignment(struct kvm *kvm)\n{\n}\n\nstatic inline void kvm_arch_end_assignment(struct kvm *kvm)\n{\n}\n\nstatic __always_inline bool kvm_arch_has_assigned_device(struct kvm *kvm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline struct rcuwait *kvm_arch_vcpu_get_wait(struct kvm_vcpu *vcpu)\n{\n#ifdef __KVM_HAVE_ARCH_WQP\n\treturn vcpu->arch.waitp;\n#else\n\treturn &vcpu->wait;\n#endif\n}\n\n \nstatic inline bool __kvm_vcpu_wake_up(struct kvm_vcpu *vcpu)\n{\n\treturn !!rcuwait_wake_up(kvm_arch_vcpu_get_wait(vcpu));\n}\n\nstatic inline bool kvm_vcpu_is_blocking(struct kvm_vcpu *vcpu)\n{\n\treturn rcuwait_active(kvm_arch_vcpu_get_wait(vcpu));\n}\n\n#ifdef __KVM_HAVE_ARCH_INTC_INITIALIZED\n \nbool kvm_arch_intc_initialized(struct kvm *kvm);\n#else\nstatic inline bool kvm_arch_intc_initialized(struct kvm *kvm)\n{\n\treturn true;\n}\n#endif\n\n#ifdef CONFIG_GUEST_PERF_EVENTS\nunsigned long kvm_arch_vcpu_get_ip(struct kvm_vcpu *vcpu);\n\nvoid kvm_register_perf_callbacks(unsigned int (*pt_intr_handler)(void));\nvoid kvm_unregister_perf_callbacks(void);\n#else\nstatic inline void kvm_register_perf_callbacks(void *ign) {}\nstatic inline void kvm_unregister_perf_callbacks(void) {}\n#endif  \n\nint kvm_arch_init_vm(struct kvm *kvm, unsigned long type);\nvoid kvm_arch_destroy_vm(struct kvm *kvm);\nvoid kvm_arch_sync_events(struct kvm *kvm);\n\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu);\n\nstruct page *kvm_pfn_to_refcounted_page(kvm_pfn_t pfn);\nbool kvm_is_zone_device_page(struct page *page);\n\nstruct kvm_irq_ack_notifier {\n\tstruct hlist_node link;\n\tunsigned gsi;\n\tvoid (*irq_acked)(struct kvm_irq_ack_notifier *kian);\n};\n\nint kvm_irq_map_gsi(struct kvm *kvm,\n\t\t    struct kvm_kernel_irq_routing_entry *entries, int gsi);\nint kvm_irq_map_chip_pin(struct kvm *kvm, unsigned irqchip, unsigned pin);\n\nint kvm_set_irq(struct kvm *kvm, int irq_source_id, u32 irq, int level,\n\t\tbool line_status);\nint kvm_set_msi(struct kvm_kernel_irq_routing_entry *irq_entry, struct kvm *kvm,\n\t\tint irq_source_id, int level, bool line_status);\nint kvm_arch_set_irq_inatomic(struct kvm_kernel_irq_routing_entry *e,\n\t\t\t       struct kvm *kvm, int irq_source_id,\n\t\t\t       int level, bool line_status);\nbool kvm_irq_has_notifier(struct kvm *kvm, unsigned irqchip, unsigned pin);\nvoid kvm_notify_acked_gsi(struct kvm *kvm, int gsi);\nvoid kvm_notify_acked_irq(struct kvm *kvm, unsigned irqchip, unsigned pin);\nvoid kvm_register_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t   struct kvm_irq_ack_notifier *kian);\nvoid kvm_unregister_irq_ack_notifier(struct kvm *kvm,\n\t\t\t\t   struct kvm_irq_ack_notifier *kian);\nint kvm_request_irq_source_id(struct kvm *kvm);\nvoid kvm_free_irq_source_id(struct kvm *kvm, int irq_source_id);\nbool kvm_arch_irqfd_allowed(struct kvm *kvm, struct kvm_irqfd *args);\n\n \nstatic inline struct kvm_memory_slot *\ntry_get_memslot(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tif (!slot)\n\t\treturn NULL;\n\n\tif (gfn >= slot->base_gfn && gfn < slot->base_gfn + slot->npages)\n\t\treturn slot;\n\telse\n\t\treturn NULL;\n}\n\n \nstatic inline struct kvm_memory_slot *\nsearch_memslots(struct kvm_memslots *slots, gfn_t gfn, bool approx)\n{\n\tstruct kvm_memory_slot *slot;\n\tstruct rb_node *node;\n\tint idx = slots->node_idx;\n\n\tslot = NULL;\n\tfor (node = slots->gfn_tree.rb_node; node; ) {\n\t\tslot = container_of(node, struct kvm_memory_slot, gfn_node[idx]);\n\t\tif (gfn >= slot->base_gfn) {\n\t\t\tif (gfn < slot->base_gfn + slot->npages)\n\t\t\t\treturn slot;\n\t\t\tnode = node->rb_right;\n\t\t} else\n\t\t\tnode = node->rb_left;\n\t}\n\n\treturn approx ? slot : NULL;\n}\n\nstatic inline struct kvm_memory_slot *\n____gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn, bool approx)\n{\n\tstruct kvm_memory_slot *slot;\n\n\tslot = (struct kvm_memory_slot *)atomic_long_read(&slots->last_used_slot);\n\tslot = try_get_memslot(slot, gfn);\n\tif (slot)\n\t\treturn slot;\n\n\tslot = search_memslots(slots, gfn, approx);\n\tif (slot) {\n\t\tatomic_long_set(&slots->last_used_slot, (unsigned long)slot);\n\t\treturn slot;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic inline struct kvm_memory_slot *\n__gfn_to_memslot(struct kvm_memslots *slots, gfn_t gfn)\n{\n\treturn ____gfn_to_memslot(slots, gfn, false);\n}\n\nstatic inline unsigned long\n__gfn_to_hva_memslot(const struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\t \n\tunsigned long offset = gfn - slot->base_gfn;\n\toffset = array_index_nospec(offset, slot->npages);\n\treturn slot->userspace_addr + offset * PAGE_SIZE;\n}\n\nstatic inline int memslot_id(struct kvm *kvm, gfn_t gfn)\n{\n\treturn gfn_to_memslot(kvm, gfn)->id;\n}\n\nstatic inline gfn_t\nhva_to_gfn_memslot(unsigned long hva, struct kvm_memory_slot *slot)\n{\n\tgfn_t gfn_offset = (hva - slot->userspace_addr) >> PAGE_SHIFT;\n\n\treturn slot->base_gfn + gfn_offset;\n}\n\nstatic inline gpa_t gfn_to_gpa(gfn_t gfn)\n{\n\treturn (gpa_t)gfn << PAGE_SHIFT;\n}\n\nstatic inline gfn_t gpa_to_gfn(gpa_t gpa)\n{\n\treturn (gfn_t)(gpa >> PAGE_SHIFT);\n}\n\nstatic inline hpa_t pfn_to_hpa(kvm_pfn_t pfn)\n{\n\treturn (hpa_t)pfn << PAGE_SHIFT;\n}\n\nstatic inline bool kvm_is_error_gpa(struct kvm *kvm, gpa_t gpa)\n{\n\tunsigned long hva = gfn_to_hva(kvm, gpa_to_gfn(gpa));\n\n\treturn kvm_is_error_hva(hva);\n}\n\nenum kvm_stat_kind {\n\tKVM_STAT_VM,\n\tKVM_STAT_VCPU,\n};\n\nstruct kvm_stat_data {\n\tstruct kvm *kvm;\n\tconst struct _kvm_stats_desc *desc;\n\tenum kvm_stat_kind kind;\n};\n\nstruct _kvm_stats_desc {\n\tstruct kvm_stats_desc desc;\n\tchar name[KVM_STATS_NAME_SIZE];\n};\n\n#define STATS_DESC_COMMON(type, unit, base, exp, sz, bsz)\t\t       \\\n\t.flags = type | unit | base |\t\t\t\t\t       \\\n\t\t BUILD_BUG_ON_ZERO(type & ~KVM_STATS_TYPE_MASK) |\t       \\\n\t\t BUILD_BUG_ON_ZERO(unit & ~KVM_STATS_UNIT_MASK) |\t       \\\n\t\t BUILD_BUG_ON_ZERO(base & ~KVM_STATS_BASE_MASK),\t       \\\n\t.exponent = exp,\t\t\t\t\t\t       \\\n\t.size = sz,\t\t\t\t\t\t\t       \\\n\t.bucket_size = bsz\n\n#define VM_GENERIC_STATS_DESC(stat, type, unit, base, exp, sz, bsz)\t       \\\n\t{\t\t\t\t\t\t\t\t       \\\n\t\t{\t\t\t\t\t\t\t       \\\n\t\t\tSTATS_DESC_COMMON(type, unit, base, exp, sz, bsz),     \\\n\t\t\t.offset = offsetof(struct kvm_vm_stat, generic.stat)   \\\n\t\t},\t\t\t\t\t\t\t       \\\n\t\t.name = #stat,\t\t\t\t\t\t       \\\n\t}\n#define VCPU_GENERIC_STATS_DESC(stat, type, unit, base, exp, sz, bsz)\t       \\\n\t{\t\t\t\t\t\t\t\t       \\\n\t\t{\t\t\t\t\t\t\t       \\\n\t\t\tSTATS_DESC_COMMON(type, unit, base, exp, sz, bsz),     \\\n\t\t\t.offset = offsetof(struct kvm_vcpu_stat, generic.stat) \\\n\t\t},\t\t\t\t\t\t\t       \\\n\t\t.name = #stat,\t\t\t\t\t\t       \\\n\t}\n#define VM_STATS_DESC(stat, type, unit, base, exp, sz, bsz)\t\t       \\\n\t{\t\t\t\t\t\t\t\t       \\\n\t\t{\t\t\t\t\t\t\t       \\\n\t\t\tSTATS_DESC_COMMON(type, unit, base, exp, sz, bsz),     \\\n\t\t\t.offset = offsetof(struct kvm_vm_stat, stat)\t       \\\n\t\t},\t\t\t\t\t\t\t       \\\n\t\t.name = #stat,\t\t\t\t\t\t       \\\n\t}\n#define VCPU_STATS_DESC(stat, type, unit, base, exp, sz, bsz)\t\t       \\\n\t{\t\t\t\t\t\t\t\t       \\\n\t\t{\t\t\t\t\t\t\t       \\\n\t\t\tSTATS_DESC_COMMON(type, unit, base, exp, sz, bsz),     \\\n\t\t\t.offset = offsetof(struct kvm_vcpu_stat, stat)\t       \\\n\t\t},\t\t\t\t\t\t\t       \\\n\t\t.name = #stat,\t\t\t\t\t\t       \\\n\t}\n \n#define STATS_DESC(SCOPE, stat, type, unit, base, exp, sz, bsz)\t\t       \\\n\tSCOPE##_STATS_DESC(stat, type, unit, base, exp, sz, bsz)\n\n#define STATS_DESC_CUMULATIVE(SCOPE, name, unit, base, exponent)\t       \\\n\tSTATS_DESC(SCOPE, name, KVM_STATS_TYPE_CUMULATIVE,\t\t       \\\n\t\tunit, base, exponent, 1, 0)\n#define STATS_DESC_INSTANT(SCOPE, name, unit, base, exponent)\t\t       \\\n\tSTATS_DESC(SCOPE, name, KVM_STATS_TYPE_INSTANT,\t\t\t       \\\n\t\tunit, base, exponent, 1, 0)\n#define STATS_DESC_PEAK(SCOPE, name, unit, base, exponent)\t\t       \\\n\tSTATS_DESC(SCOPE, name, KVM_STATS_TYPE_PEAK,\t\t\t       \\\n\t\tunit, base, exponent, 1, 0)\n#define STATS_DESC_LINEAR_HIST(SCOPE, name, unit, base, exponent, sz, bsz)     \\\n\tSTATS_DESC(SCOPE, name, KVM_STATS_TYPE_LINEAR_HIST,\t\t       \\\n\t\tunit, base, exponent, sz, bsz)\n#define STATS_DESC_LOG_HIST(SCOPE, name, unit, base, exponent, sz)\t       \\\n\tSTATS_DESC(SCOPE, name, KVM_STATS_TYPE_LOG_HIST,\t\t       \\\n\t\tunit, base, exponent, sz, 0)\n\n \n#define STATS_DESC_COUNTER(SCOPE, name)\t\t\t\t\t       \\\n\tSTATS_DESC_CUMULATIVE(SCOPE, name, KVM_STATS_UNIT_NONE,\t\t       \\\n\t\tKVM_STATS_BASE_POW10, 0)\n \n#define STATS_DESC_ICOUNTER(SCOPE, name)\t\t\t\t       \\\n\tSTATS_DESC_INSTANT(SCOPE, name, KVM_STATS_UNIT_NONE,\t\t       \\\n\t\tKVM_STATS_BASE_POW10, 0)\n \n#define STATS_DESC_PCOUNTER(SCOPE, name)\t\t\t\t       \\\n\tSTATS_DESC_PEAK(SCOPE, name, KVM_STATS_UNIT_NONE,\t\t       \\\n\t\tKVM_STATS_BASE_POW10, 0)\n\n \n#define STATS_DESC_IBOOLEAN(SCOPE, name)\t\t\t\t       \\\n\tSTATS_DESC_INSTANT(SCOPE, name, KVM_STATS_UNIT_BOOLEAN,\t\t       \\\n\t\tKVM_STATS_BASE_POW10, 0)\n \n#define STATS_DESC_PBOOLEAN(SCOPE, name)\t\t\t\t       \\\n\tSTATS_DESC_PEAK(SCOPE, name, KVM_STATS_UNIT_BOOLEAN,\t\t       \\\n\t\tKVM_STATS_BASE_POW10, 0)\n\n \n#define STATS_DESC_TIME_NSEC(SCOPE, name)\t\t\t\t       \\\n\tSTATS_DESC_CUMULATIVE(SCOPE, name, KVM_STATS_UNIT_SECONDS,\t       \\\n\t\tKVM_STATS_BASE_POW10, -9)\n \n#define STATS_DESC_LINHIST_TIME_NSEC(SCOPE, name, sz, bsz)\t\t       \\\n\tSTATS_DESC_LINEAR_HIST(SCOPE, name, KVM_STATS_UNIT_SECONDS,\t       \\\n\t\tKVM_STATS_BASE_POW10, -9, sz, bsz)\n \n#define STATS_DESC_LOGHIST_TIME_NSEC(SCOPE, name, sz)\t\t\t       \\\n\tSTATS_DESC_LOG_HIST(SCOPE, name, KVM_STATS_UNIT_SECONDS,\t       \\\n\t\tKVM_STATS_BASE_POW10, -9, sz)\n\n#define KVM_GENERIC_VM_STATS()\t\t\t\t\t\t       \\\n\tSTATS_DESC_COUNTER(VM_GENERIC, remote_tlb_flush),\t\t       \\\n\tSTATS_DESC_COUNTER(VM_GENERIC, remote_tlb_flush_requests)\n\n#define KVM_GENERIC_VCPU_STATS()\t\t\t\t\t       \\\n\tSTATS_DESC_COUNTER(VCPU_GENERIC, halt_successful_poll),\t\t       \\\n\tSTATS_DESC_COUNTER(VCPU_GENERIC, halt_attempted_poll),\t\t       \\\n\tSTATS_DESC_COUNTER(VCPU_GENERIC, halt_poll_invalid),\t\t       \\\n\tSTATS_DESC_COUNTER(VCPU_GENERIC, halt_wakeup),\t\t\t       \\\n\tSTATS_DESC_TIME_NSEC(VCPU_GENERIC, halt_poll_success_ns),\t       \\\n\tSTATS_DESC_TIME_NSEC(VCPU_GENERIC, halt_poll_fail_ns),\t\t       \\\n\tSTATS_DESC_TIME_NSEC(VCPU_GENERIC, halt_wait_ns),\t\t       \\\n\tSTATS_DESC_LOGHIST_TIME_NSEC(VCPU_GENERIC, halt_poll_success_hist,     \\\n\t\t\tHALT_POLL_HIST_COUNT),\t\t\t\t       \\\n\tSTATS_DESC_LOGHIST_TIME_NSEC(VCPU_GENERIC, halt_poll_fail_hist,\t       \\\n\t\t\tHALT_POLL_HIST_COUNT),\t\t\t\t       \\\n\tSTATS_DESC_LOGHIST_TIME_NSEC(VCPU_GENERIC, halt_wait_hist,\t       \\\n\t\t\tHALT_POLL_HIST_COUNT),\t\t\t\t       \\\n\tSTATS_DESC_IBOOLEAN(VCPU_GENERIC, blocking)\n\nextern struct dentry *kvm_debugfs_dir;\n\nssize_t kvm_stats_read(char *id, const struct kvm_stats_header *header,\n\t\t       const struct _kvm_stats_desc *desc,\n\t\t       void *stats, size_t size_stats,\n\t\t       char __user *user_buffer, size_t size, loff_t *offset);\n\n \nstatic inline void kvm_stats_linear_hist_update(u64 *data, size_t size,\n\t\t\t\t\t\tu64 value, size_t bucket_size)\n{\n\tsize_t index = div64_u64(value, bucket_size);\n\n\tindex = min(index, size - 1);\n\t++data[index];\n}\n\n \nstatic inline void kvm_stats_log_hist_update(u64 *data, size_t size, u64 value)\n{\n\tsize_t index = fls64(value);\n\n\tindex = min(index, size - 1);\n\t++data[index];\n}\n\n#define KVM_STATS_LINEAR_HIST_UPDATE(array, value, bsize)\t\t       \\\n\tkvm_stats_linear_hist_update(array, ARRAY_SIZE(array), value, bsize)\n#define KVM_STATS_LOG_HIST_UPDATE(array, value)\t\t\t\t       \\\n\tkvm_stats_log_hist_update(array, ARRAY_SIZE(array), value)\n\n\nextern const struct kvm_stats_header kvm_vm_stats_header;\nextern const struct _kvm_stats_desc kvm_vm_stats_desc[];\nextern const struct kvm_stats_header kvm_vcpu_stats_header;\nextern const struct _kvm_stats_desc kvm_vcpu_stats_desc[];\n\n#if defined(CONFIG_MMU_NOTIFIER) && defined(KVM_ARCH_WANT_MMU_NOTIFIER)\nstatic inline int mmu_invalidate_retry(struct kvm *kvm, unsigned long mmu_seq)\n{\n\tif (unlikely(kvm->mmu_invalidate_in_progress))\n\t\treturn 1;\n\t \n\tsmp_rmb();\n\tif (kvm->mmu_invalidate_seq != mmu_seq)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int mmu_invalidate_retry_hva(struct kvm *kvm,\n\t\t\t\t\t   unsigned long mmu_seq,\n\t\t\t\t\t   unsigned long hva)\n{\n\tlockdep_assert_held(&kvm->mmu_lock);\n\t \n\tif (unlikely(kvm->mmu_invalidate_in_progress) &&\n\t    hva >= kvm->mmu_invalidate_range_start &&\n\t    hva < kvm->mmu_invalidate_range_end)\n\t\treturn 1;\n\tif (kvm->mmu_invalidate_seq != mmu_seq)\n\t\treturn 1;\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_HAVE_KVM_IRQ_ROUTING\n\n#define KVM_MAX_IRQ_ROUTES 4096  \n\nbool kvm_arch_can_set_irq_routing(struct kvm *kvm);\nint kvm_set_irq_routing(struct kvm *kvm,\n\t\t\tconst struct kvm_irq_routing_entry *entries,\n\t\t\tunsigned nr,\n\t\t\tunsigned flags);\nint kvm_set_routing_entry(struct kvm *kvm,\n\t\t\t  struct kvm_kernel_irq_routing_entry *e,\n\t\t\t  const struct kvm_irq_routing_entry *ue);\nvoid kvm_free_irq_routing(struct kvm *kvm);\n\n#else\n\nstatic inline void kvm_free_irq_routing(struct kvm *kvm) {}\n\n#endif\n\nint kvm_send_userspace_msi(struct kvm *kvm, struct kvm_msi *msi);\n\n#ifdef CONFIG_HAVE_KVM_EVENTFD\n\nvoid kvm_eventfd_init(struct kvm *kvm);\nint kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args);\n\n#ifdef CONFIG_HAVE_KVM_IRQFD\nint kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args);\nvoid kvm_irqfd_release(struct kvm *kvm);\nbool kvm_notify_irqfd_resampler(struct kvm *kvm,\n\t\t\t\tunsigned int irqchip,\n\t\t\t\tunsigned int pin);\nvoid kvm_irq_routing_update(struct kvm *);\n#else\nstatic inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void kvm_irqfd_release(struct kvm *kvm) {}\n\nstatic inline bool kvm_notify_irqfd_resampler(struct kvm *kvm,\n\t\t\t\t\t      unsigned int irqchip,\n\t\t\t\t\t      unsigned int pin)\n{\n\treturn false;\n}\n#endif\n\n#else\n\nstatic inline void kvm_eventfd_init(struct kvm *kvm) {}\n\nstatic inline int kvm_irqfd(struct kvm *kvm, struct kvm_irqfd *args)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void kvm_irqfd_release(struct kvm *kvm) {}\n\n#ifdef CONFIG_HAVE_KVM_IRQCHIP\nstatic inline void kvm_irq_routing_update(struct kvm *kvm)\n{\n}\n#endif\n\nstatic inline int kvm_ioeventfd(struct kvm *kvm, struct kvm_ioeventfd *args)\n{\n\treturn -ENOSYS;\n}\n\n#endif  \n\nvoid kvm_arch_irq_routing_update(struct kvm *kvm);\n\nstatic inline void __kvm_make_request(int req, struct kvm_vcpu *vcpu)\n{\n\t \n\tsmp_wmb();\n\tset_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);\n}\n\nstatic __always_inline void kvm_make_request(int req, struct kvm_vcpu *vcpu)\n{\n\t \n\tBUILD_BUG_ON(!__builtin_constant_p(req) ||\n\t\t     (req & KVM_REQUEST_NO_ACTION));\n\n\t__kvm_make_request(req, vcpu);\n}\n\nstatic inline bool kvm_request_pending(struct kvm_vcpu *vcpu)\n{\n\treturn READ_ONCE(vcpu->requests);\n}\n\nstatic inline bool kvm_test_request(int req, struct kvm_vcpu *vcpu)\n{\n\treturn test_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);\n}\n\nstatic inline void kvm_clear_request(int req, struct kvm_vcpu *vcpu)\n{\n\tclear_bit(req & KVM_REQUEST_MASK, (void *)&vcpu->requests);\n}\n\nstatic inline bool kvm_check_request(int req, struct kvm_vcpu *vcpu)\n{\n\tif (kvm_test_request(req, vcpu)) {\n\t\tkvm_clear_request(req, vcpu);\n\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\treturn true;\n\t} else {\n\t\treturn false;\n\t}\n}\n\n#ifdef CONFIG_KVM_GENERIC_HARDWARE_ENABLING\nextern bool kvm_rebooting;\n#endif\n\nextern unsigned int halt_poll_ns;\nextern unsigned int halt_poll_ns_grow;\nextern unsigned int halt_poll_ns_grow_start;\nextern unsigned int halt_poll_ns_shrink;\n\nstruct kvm_device {\n\tconst struct kvm_device_ops *ops;\n\tstruct kvm *kvm;\n\tvoid *private;\n\tstruct list_head vm_node;\n};\n\n \nstruct kvm_device_ops {\n\tconst char *name;\n\n\t \n\tint (*create)(struct kvm_device *dev, u32 type);\n\n\t \n\tvoid (*init)(struct kvm_device *dev);\n\n\t \n\tvoid (*destroy)(struct kvm_device *dev);\n\n\t \n\tvoid (*release)(struct kvm_device *dev);\n\n\tint (*set_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);\n\tint (*get_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);\n\tint (*has_attr)(struct kvm_device *dev, struct kvm_device_attr *attr);\n\tlong (*ioctl)(struct kvm_device *dev, unsigned int ioctl,\n\t\t      unsigned long arg);\n\tint (*mmap)(struct kvm_device *dev, struct vm_area_struct *vma);\n};\n\nstruct kvm_device *kvm_device_from_filp(struct file *filp);\nint kvm_register_device_ops(const struct kvm_device_ops *ops, u32 type);\nvoid kvm_unregister_device_ops(u32 type);\n\nextern struct kvm_device_ops kvm_mpic_ops;\nextern struct kvm_device_ops kvm_arm_vgic_v2_ops;\nextern struct kvm_device_ops kvm_arm_vgic_v3_ops;\n\n#ifdef CONFIG_HAVE_KVM_CPU_RELAX_INTERCEPT\n\nstatic inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)\n{\n\tvcpu->spin_loop.in_spin_loop = val;\n}\nstatic inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)\n{\n\tvcpu->spin_loop.dy_eligible = val;\n}\n\n#else  \n\nstatic inline void kvm_vcpu_set_in_spin_loop(struct kvm_vcpu *vcpu, bool val)\n{\n}\n\nstatic inline void kvm_vcpu_set_dy_eligible(struct kvm_vcpu *vcpu, bool val)\n{\n}\n#endif  \n\nstatic inline bool kvm_is_visible_memslot(struct kvm_memory_slot *memslot)\n{\n\treturn (memslot && memslot->id < KVM_USER_MEM_SLOTS &&\n\t\t!(memslot->flags & KVM_MEMSLOT_INVALID));\n}\n\nstruct kvm_vcpu *kvm_get_running_vcpu(void);\nstruct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);\n\n#ifdef CONFIG_HAVE_KVM_IRQ_BYPASS\nbool kvm_arch_has_irq_bypass(void);\nint kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *,\n\t\t\t   struct irq_bypass_producer *);\nvoid kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *,\n\t\t\t   struct irq_bypass_producer *);\nvoid kvm_arch_irq_bypass_stop(struct irq_bypass_consumer *);\nvoid kvm_arch_irq_bypass_start(struct irq_bypass_consumer *);\nint kvm_arch_update_irqfd_routing(struct kvm *kvm, unsigned int host_irq,\n\t\t\t\t  uint32_t guest_irq, bool set);\nbool kvm_arch_irqfd_route_changed(struct kvm_kernel_irq_routing_entry *,\n\t\t\t\t  struct kvm_kernel_irq_routing_entry *);\n#endif  \n\n#ifdef CONFIG_HAVE_KVM_INVALID_WAKEUPS\n \nstatic inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->valid_wakeup;\n}\n\n#else\nstatic inline bool vcpu_valid_wakeup(struct kvm_vcpu *vcpu)\n{\n\treturn true;\n}\n#endif  \n\n#ifdef CONFIG_HAVE_KVM_NO_POLL\n \nbool kvm_arch_no_poll(struct kvm_vcpu *vcpu);\n#else\nstatic inline bool kvm_arch_no_poll(struct kvm_vcpu *vcpu)\n{\n\treturn false;\n}\n#endif  \n\n#ifdef CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL\nlong kvm_arch_vcpu_async_ioctl(struct file *filp,\n\t\t\t       unsigned int ioctl, unsigned long arg);\n#else\nstatic inline long kvm_arch_vcpu_async_ioctl(struct file *filp,\n\t\t\t\t\t     unsigned int ioctl,\n\t\t\t\t\t     unsigned long arg)\n{\n\treturn -ENOIOCTLCMD;\n}\n#endif  \n\nvoid kvm_arch_guest_memory_reclaimed(struct kvm *kvm);\n\n#ifdef CONFIG_HAVE_KVM_VCPU_RUN_PID_CHANGE\nint kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu);\n#else\nstatic inline int kvm_arch_vcpu_run_pid_change(struct kvm_vcpu *vcpu)\n{\n\treturn 0;\n}\n#endif  \n\ntypedef int (*kvm_vm_thread_fn_t)(struct kvm *kvm, uintptr_t data);\n\nint kvm_vm_create_worker_thread(struct kvm *kvm, kvm_vm_thread_fn_t thread_fn,\n\t\t\t\tuintptr_t data, const char *name,\n\t\t\t\tstruct task_struct **thread_ptr);\n\n#ifdef CONFIG_KVM_XFER_TO_GUEST_WORK\nstatic inline void kvm_handle_signal_exit(struct kvm_vcpu *vcpu)\n{\n\tvcpu->run->exit_reason = KVM_EXIT_INTR;\n\tvcpu->stat.signal_exits++;\n}\n#endif  \n\n \nstatic inline void kvm_account_pgtable_pages(void *virt, int nr)\n{\n\tmod_lruvec_page_state(virt_to_page(virt), NR_SECONDARY_PAGETABLE, nr);\n}\n\n \n#define  KVM_DIRTY_RING_RSVD_ENTRIES  64\n\n \n#define  KVM_DIRTY_RING_MAX_ENTRIES  65536\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}