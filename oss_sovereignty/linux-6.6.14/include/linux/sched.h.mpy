{
  "module_name": "sched.h",
  "hash_id": "6cd00a1a5715ac471f2b8de850b233733bccc3f16a6cbc329e6ea4278f7f2c78",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/sched.h",
  "human_readable_source": " \n#ifndef _LINUX_SCHED_H\n#define _LINUX_SCHED_H\n\n \n\n#include <uapi/linux/sched.h>\n\n#include <asm/current.h>\n\n#include <linux/pid.h>\n#include <linux/sem.h>\n#include <linux/shm.h>\n#include <linux/kmsan_types.h>\n#include <linux/mutex.h>\n#include <linux/plist.h>\n#include <linux/hrtimer.h>\n#include <linux/irqflags.h>\n#include <linux/seccomp.h>\n#include <linux/nodemask.h>\n#include <linux/rcupdate.h>\n#include <linux/refcount.h>\n#include <linux/resource.h>\n#include <linux/latencytop.h>\n#include <linux/sched/prio.h>\n#include <linux/sched/types.h>\n#include <linux/signal_types.h>\n#include <linux/syscall_user_dispatch.h>\n#include <linux/mm_types_task.h>\n#include <linux/task_io_accounting.h>\n#include <linux/posix-timers.h>\n#include <linux/rseq.h>\n#include <linux/seqlock.h>\n#include <linux/kcsan.h>\n#include <linux/rv.h>\n#include <linux/livepatch_sched.h>\n#include <asm/kmap_size.h>\n\n \nstruct audit_context;\nstruct bio_list;\nstruct blk_plug;\nstruct bpf_local_storage;\nstruct bpf_run_ctx;\nstruct capture_control;\nstruct cfs_rq;\nstruct fs_struct;\nstruct futex_pi_state;\nstruct io_context;\nstruct io_uring_task;\nstruct mempolicy;\nstruct nameidata;\nstruct nsproxy;\nstruct perf_event_context;\nstruct pid_namespace;\nstruct pipe_inode_info;\nstruct rcu_node;\nstruct reclaim_state;\nstruct robust_list_head;\nstruct root_domain;\nstruct rq;\nstruct sched_attr;\nstruct sched_param;\nstruct seq_file;\nstruct sighand_struct;\nstruct signal_struct;\nstruct task_delay_info;\nstruct task_group;\nstruct user_event_mm;\n\n \n\n \n#define TASK_RUNNING\t\t\t0x00000000\n#define TASK_INTERRUPTIBLE\t\t0x00000001\n#define TASK_UNINTERRUPTIBLE\t\t0x00000002\n#define __TASK_STOPPED\t\t\t0x00000004\n#define __TASK_TRACED\t\t\t0x00000008\n \n#define EXIT_DEAD\t\t\t0x00000010\n#define EXIT_ZOMBIE\t\t\t0x00000020\n#define EXIT_TRACE\t\t\t(EXIT_ZOMBIE | EXIT_DEAD)\n \n#define TASK_PARKED\t\t\t0x00000040\n#define TASK_DEAD\t\t\t0x00000080\n#define TASK_WAKEKILL\t\t\t0x00000100\n#define TASK_WAKING\t\t\t0x00000200\n#define TASK_NOLOAD\t\t\t0x00000400\n#define TASK_NEW\t\t\t0x00000800\n#define TASK_RTLOCK_WAIT\t\t0x00001000\n#define TASK_FREEZABLE\t\t\t0x00002000\n#define __TASK_FREEZABLE_UNSAFE\t       (0x00004000 * IS_ENABLED(CONFIG_LOCKDEP))\n#define TASK_FROZEN\t\t\t0x00008000\n#define TASK_STATE_MAX\t\t\t0x00010000\n\n#define TASK_ANY\t\t\t(TASK_STATE_MAX-1)\n\n \n#define TASK_FREEZABLE_UNSAFE\t\t(TASK_FREEZABLE | __TASK_FREEZABLE_UNSAFE)\n\n \n#define TASK_KILLABLE\t\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)\n#define TASK_STOPPED\t\t\t(TASK_WAKEKILL | __TASK_STOPPED)\n#define TASK_TRACED\t\t\t__TASK_TRACED\n\n#define TASK_IDLE\t\t\t(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)\n\n \n#define TASK_NORMAL\t\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)\n\n \n#define TASK_REPORT\t\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\n\t\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\n\t\t\t\t\t __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\\n\t\t\t\t\t TASK_PARKED)\n\n#define task_is_running(task)\t\t(READ_ONCE((task)->__state) == TASK_RUNNING)\n\n#define task_is_traced(task)\t\t((READ_ONCE(task->jobctl) & JOBCTL_TRACED) != 0)\n#define task_is_stopped(task)\t\t((READ_ONCE(task->jobctl) & JOBCTL_STOPPED) != 0)\n#define task_is_stopped_or_traced(task)\t((READ_ONCE(task->jobctl) & (JOBCTL_STOPPED | JOBCTL_TRACED)) != 0)\n\n \n#define is_special_task_state(state)\t\t\t\t\\\n\t((state) & (__TASK_STOPPED | __TASK_TRACED | TASK_PARKED | TASK_DEAD))\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n# define debug_normal_state_change(state_value)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(is_special_task_state(state_value));\t\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\t\\\n\t} while (0)\n\n# define debug_special_state_change(state_value)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tWARN_ON_ONCE(!is_special_task_state(state_value));\t\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\t\\\n\t} while (0)\n\n# define debug_rtlock_wait_set_state()\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t \\\n\t\tcurrent->saved_state_change = current->task_state_change;\\\n\t\tcurrent->task_state_change = _THIS_IP_;\t\t\t \\\n\t} while (0)\n\n# define debug_rtlock_wait_restore_state()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t \\\n\t\tcurrent->task_state_change = current->saved_state_change;\\\n\t} while (0)\n\n#else\n# define debug_normal_state_change(cond)\tdo { } while (0)\n# define debug_special_state_change(cond)\tdo { } while (0)\n# define debug_rtlock_wait_set_state()\t\tdo { } while (0)\n# define debug_rtlock_wait_restore_state()\tdo { } while (0)\n#endif\n\n \n#define __set_current_state(state_value)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tdebug_normal_state_change((state_value));\t\t\\\n\t\tWRITE_ONCE(current->__state, (state_value));\t\t\\\n\t} while (0)\n\n#define set_current_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tdebug_normal_state_change((state_value));\t\t\\\n\t\tsmp_store_mb(current->__state, (state_value));\t\t\\\n\t} while (0)\n\n \n#define set_special_state(state_value)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tunsigned long flags;  \t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\traw_spin_lock_irqsave(&current->pi_lock, flags);\t\\\n\t\tdebug_special_state_change((state_value));\t\t\\\n\t\tWRITE_ONCE(current->__state, (state_value));\t\t\\\n\t\traw_spin_unlock_irqrestore(&current->pi_lock, flags);\t\\\n\t} while (0)\n\n \n#define current_save_and_set_rtlock_wait_state()\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tlockdep_assert_irqs_disabled();\t\t\t\t\\\n\t\traw_spin_lock(&current->pi_lock);\t\t\t\\\n\t\tcurrent->saved_state = current->__state;\t\t\\\n\t\tdebug_rtlock_wait_set_state();\t\t\t\t\\\n\t\tWRITE_ONCE(current->__state, TASK_RTLOCK_WAIT);\t\t\\\n\t\traw_spin_unlock(&current->pi_lock);\t\t\t\\\n\t} while (0);\n\n#define current_restore_rtlock_saved_state()\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tlockdep_assert_irqs_disabled();\t\t\t\t\\\n\t\traw_spin_lock(&current->pi_lock);\t\t\t\\\n\t\tdebug_rtlock_wait_restore_state();\t\t\t\\\n\t\tWRITE_ONCE(current->__state, current->saved_state);\t\\\n\t\tcurrent->saved_state = TASK_RUNNING;\t\t\t\\\n\t\traw_spin_unlock(&current->pi_lock);\t\t\t\\\n\t} while (0);\n\n#define get_current_state()\tREAD_ONCE(current->__state)\n\n \nenum {\n\tTASK_COMM_LEN = 16,\n};\n\nextern void scheduler_tick(void);\n\n#define\tMAX_SCHEDULE_TIMEOUT\t\tLONG_MAX\n\nextern long schedule_timeout(long timeout);\nextern long schedule_timeout_interruptible(long timeout);\nextern long schedule_timeout_killable(long timeout);\nextern long schedule_timeout_uninterruptible(long timeout);\nextern long schedule_timeout_idle(long timeout);\nasmlinkage void schedule(void);\nextern void schedule_preempt_disabled(void);\nasmlinkage void preempt_schedule_irq(void);\n#ifdef CONFIG_PREEMPT_RT\n extern void schedule_rtlock(void);\n#endif\n\nextern int __must_check io_schedule_prepare(void);\nextern void io_schedule_finish(int token);\nextern long io_schedule_timeout(long timeout);\nextern void io_schedule(void);\n\n \nstruct prev_cputime {\n#ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n\traw_spinlock_t\t\t\tlock;\n#endif\n};\n\nenum vtime_state {\n\t \n\tVTIME_INACTIVE = 0,\n\t \n\tVTIME_IDLE,\n\t \n\tVTIME_SYS,\n\t \n\tVTIME_USER,\n\t \n\tVTIME_GUEST,\n};\n\nstruct vtime {\n\tseqcount_t\t\tseqcount;\n\tunsigned long long\tstarttime;\n\tenum vtime_state\tstate;\n\tunsigned int\t\tcpu;\n\tu64\t\t\tutime;\n\tu64\t\t\tstime;\n\tu64\t\t\tgtime;\n};\n\n \nenum uclamp_id {\n\tUCLAMP_MIN = 0,\n\tUCLAMP_MAX,\n\tUCLAMP_CNT\n};\n\n#ifdef CONFIG_SMP\nextern struct root_domain def_root_domain;\nextern struct mutex sched_domains_mutex;\n#endif\n\nstruct sched_info {\n#ifdef CONFIG_SCHED_INFO\n\t \n\n\t \n\tunsigned long\t\t\tpcount;\n\n\t \n\tunsigned long long\t\trun_delay;\n\n\t \n\n\t \n\tunsigned long long\t\tlast_arrival;\n\n\t \n\tunsigned long long\t\tlast_queued;\n\n#endif  \n};\n\n \n# define SCHED_FIXEDPOINT_SHIFT\t\t10\n# define SCHED_FIXEDPOINT_SCALE\t\t(1L << SCHED_FIXEDPOINT_SHIFT)\n\n \n# define SCHED_CAPACITY_SHIFT\t\tSCHED_FIXEDPOINT_SHIFT\n# define SCHED_CAPACITY_SCALE\t\t(1L << SCHED_CAPACITY_SHIFT)\n\nstruct load_weight {\n\tunsigned long\t\t\tweight;\n\tu32\t\t\t\tinv_weight;\n};\n\n \nstruct util_est {\n\tunsigned int\t\t\tenqueued;\n\tunsigned int\t\t\tewma;\n#define UTIL_EST_WEIGHT_SHIFT\t\t2\n#define UTIL_AVG_UNCHANGED\t\t0x80000000\n} __attribute__((__aligned__(sizeof(u64))));\n\n \nstruct sched_avg {\n\tu64\t\t\t\tlast_update_time;\n\tu64\t\t\t\tload_sum;\n\tu64\t\t\t\trunnable_sum;\n\tu32\t\t\t\tutil_sum;\n\tu32\t\t\t\tperiod_contrib;\n\tunsigned long\t\t\tload_avg;\n\tunsigned long\t\t\trunnable_avg;\n\tunsigned long\t\t\tutil_avg;\n\tstruct util_est\t\t\tutil_est;\n} ____cacheline_aligned;\n\nstruct sched_statistics {\n#ifdef CONFIG_SCHEDSTATS\n\tu64\t\t\t\twait_start;\n\tu64\t\t\t\twait_max;\n\tu64\t\t\t\twait_count;\n\tu64\t\t\t\twait_sum;\n\tu64\t\t\t\tiowait_count;\n\tu64\t\t\t\tiowait_sum;\n\n\tu64\t\t\t\tsleep_start;\n\tu64\t\t\t\tsleep_max;\n\ts64\t\t\t\tsum_sleep_runtime;\n\n\tu64\t\t\t\tblock_start;\n\tu64\t\t\t\tblock_max;\n\ts64\t\t\t\tsum_block_runtime;\n\n\tu64\t\t\t\texec_max;\n\tu64\t\t\t\tslice_max;\n\n\tu64\t\t\t\tnr_migrations_cold;\n\tu64\t\t\t\tnr_failed_migrations_affine;\n\tu64\t\t\t\tnr_failed_migrations_running;\n\tu64\t\t\t\tnr_failed_migrations_hot;\n\tu64\t\t\t\tnr_forced_migrations;\n\n\tu64\t\t\t\tnr_wakeups;\n\tu64\t\t\t\tnr_wakeups_sync;\n\tu64\t\t\t\tnr_wakeups_migrate;\n\tu64\t\t\t\tnr_wakeups_local;\n\tu64\t\t\t\tnr_wakeups_remote;\n\tu64\t\t\t\tnr_wakeups_affine;\n\tu64\t\t\t\tnr_wakeups_affine_attempts;\n\tu64\t\t\t\tnr_wakeups_passive;\n\tu64\t\t\t\tnr_wakeups_idle;\n\n#ifdef CONFIG_SCHED_CORE\n\tu64\t\t\t\tcore_forceidle_sum;\n#endif\n#endif  \n} ____cacheline_aligned;\n\nstruct sched_entity {\n\t \n\tstruct load_weight\t\tload;\n\tstruct rb_node\t\t\trun_node;\n\tu64\t\t\t\tdeadline;\n\tu64\t\t\t\tmin_deadline;\n\n\tstruct list_head\t\tgroup_node;\n\tunsigned int\t\t\ton_rq;\n\n\tu64\t\t\t\texec_start;\n\tu64\t\t\t\tsum_exec_runtime;\n\tu64\t\t\t\tprev_sum_exec_runtime;\n\tu64\t\t\t\tvruntime;\n\ts64\t\t\t\tvlag;\n\tu64\t\t\t\tslice;\n\n\tu64\t\t\t\tnr_migrations;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n\tint\t\t\t\tdepth;\n\tstruct sched_entity\t\t*parent;\n\t \n\tstruct cfs_rq\t\t\t*cfs_rq;\n\t \n\tstruct cfs_rq\t\t\t*my_q;\n\t \n\tunsigned long\t\t\trunnable_weight;\n#endif\n\n#ifdef CONFIG_SMP\n\t \n\tstruct sched_avg\t\tavg;\n#endif\n};\n\nstruct sched_rt_entity {\n\tstruct list_head\t\trun_list;\n\tunsigned long\t\t\ttimeout;\n\tunsigned long\t\t\twatchdog_stamp;\n\tunsigned int\t\t\ttime_slice;\n\tunsigned short\t\t\ton_rq;\n\tunsigned short\t\t\ton_list;\n\n\tstruct sched_rt_entity\t\t*back;\n#ifdef CONFIG_RT_GROUP_SCHED\n\tstruct sched_rt_entity\t\t*parent;\n\t \n\tstruct rt_rq\t\t\t*rt_rq;\n\t \n\tstruct rt_rq\t\t\t*my_q;\n#endif\n} __randomize_layout;\n\nstruct sched_dl_entity {\n\tstruct rb_node\t\t\trb_node;\n\n\t \n\tu64\t\t\t\tdl_runtime;\t \n\tu64\t\t\t\tdl_deadline;\t \n\tu64\t\t\t\tdl_period;\t \n\tu64\t\t\t\tdl_bw;\t\t \n\tu64\t\t\t\tdl_density;\t \n\n\t \n\ts64\t\t\t\truntime;\t \n\tu64\t\t\t\tdeadline;\t \n\tunsigned int\t\t\tflags;\t\t \n\n\t \n\tunsigned int\t\t\tdl_throttled      : 1;\n\tunsigned int\t\t\tdl_yielded        : 1;\n\tunsigned int\t\t\tdl_non_contending : 1;\n\tunsigned int\t\t\tdl_overrun\t  : 1;\n\n\t \n\tstruct hrtimer\t\t\tdl_timer;\n\n\t \n\tstruct hrtimer inactive_timer;\n\n#ifdef CONFIG_RT_MUTEXES\n\t \n\tstruct sched_dl_entity *pi_se;\n#endif\n};\n\n#ifdef CONFIG_UCLAMP_TASK\n \n#define UCLAMP_BUCKETS CONFIG_UCLAMP_BUCKETS_COUNT\n\n \nstruct uclamp_se {\n\tunsigned int value\t\t: bits_per(SCHED_CAPACITY_SCALE);\n\tunsigned int bucket_id\t\t: bits_per(UCLAMP_BUCKETS);\n\tunsigned int active\t\t: 1;\n\tunsigned int user_defined\t: 1;\n};\n#endif  \n\nunion rcu_special {\n\tstruct {\n\t\tu8\t\t\tblocked;\n\t\tu8\t\t\tneed_qs;\n\t\tu8\t\t\texp_hint;  \n\t\tu8\t\t\tneed_mb;  \n\t} b;  \n\tu32 s;  \n};\n\nenum perf_event_task_context {\n\tperf_invalid_context = -1,\n\tperf_hw_context = 0,\n\tperf_sw_context,\n\tperf_nr_task_contexts,\n};\n\nstruct wake_q_node {\n\tstruct wake_q_node *next;\n};\n\nstruct kmap_ctrl {\n#ifdef CONFIG_KMAP_LOCAL\n\tint\t\t\t\tidx;\n\tpte_t\t\t\t\tpteval[KM_MAX_IDX];\n#endif\n};\n\nstruct task_struct {\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t \n\tstruct thread_info\t\tthread_info;\n#endif\n\tunsigned int\t\t\t__state;\n\n#ifdef CONFIG_PREEMPT_RT\n\t \n\tunsigned int\t\t\tsaved_state;\n#endif\n\n\t \n\trandomized_struct_fields_start\n\n\tvoid\t\t\t\t*stack;\n\trefcount_t\t\t\tusage;\n\t \n\tunsigned int\t\t\tflags;\n\tunsigned int\t\t\tptrace;\n\n#ifdef CONFIG_SMP\n\tint\t\t\t\ton_cpu;\n\tstruct __call_single_node\twake_entry;\n\tunsigned int\t\t\twakee_flips;\n\tunsigned long\t\t\twakee_flip_decay_ts;\n\tstruct task_struct\t\t*last_wakee;\n\n\t \n\tint\t\t\t\trecent_used_cpu;\n\tint\t\t\t\twake_cpu;\n#endif\n\tint\t\t\t\ton_rq;\n\n\tint\t\t\t\tprio;\n\tint\t\t\t\tstatic_prio;\n\tint\t\t\t\tnormal_prio;\n\tunsigned int\t\t\trt_priority;\n\n\tstruct sched_entity\t\tse;\n\tstruct sched_rt_entity\t\trt;\n\tstruct sched_dl_entity\t\tdl;\n\tconst struct sched_class\t*sched_class;\n\n#ifdef CONFIG_SCHED_CORE\n\tstruct rb_node\t\t\tcore_node;\n\tunsigned long\t\t\tcore_cookie;\n\tunsigned int\t\t\tcore_occupation;\n#endif\n\n#ifdef CONFIG_CGROUP_SCHED\n\tstruct task_group\t\t*sched_task_group;\n#endif\n\n#ifdef CONFIG_UCLAMP_TASK\n\t \n\tstruct uclamp_se\t\tuclamp_req[UCLAMP_CNT];\n\t \n\tstruct uclamp_se\t\tuclamp[UCLAMP_CNT];\n#endif\n\n\tstruct sched_statistics         stats;\n\n#ifdef CONFIG_PREEMPT_NOTIFIERS\n\t \n\tstruct hlist_head\t\tpreempt_notifiers;\n#endif\n\n#ifdef CONFIG_BLK_DEV_IO_TRACE\n\tunsigned int\t\t\tbtrace_seq;\n#endif\n\n\tunsigned int\t\t\tpolicy;\n\tint\t\t\t\tnr_cpus_allowed;\n\tconst cpumask_t\t\t\t*cpus_ptr;\n\tcpumask_t\t\t\t*user_cpus_ptr;\n\tcpumask_t\t\t\tcpus_mask;\n\tvoid\t\t\t\t*migration_pending;\n#ifdef CONFIG_SMP\n\tunsigned short\t\t\tmigration_disabled;\n#endif\n\tunsigned short\t\t\tmigration_flags;\n\n#ifdef CONFIG_PREEMPT_RCU\n\tint\t\t\t\trcu_read_lock_nesting;\n\tunion rcu_special\t\trcu_read_unlock_special;\n\tstruct list_head\t\trcu_node_entry;\n\tstruct rcu_node\t\t\t*rcu_blocked_node;\n#endif  \n\n#ifdef CONFIG_TASKS_RCU\n\tunsigned long\t\t\trcu_tasks_nvcsw;\n\tu8\t\t\t\trcu_tasks_holdout;\n\tu8\t\t\t\trcu_tasks_idx;\n\tint\t\t\t\trcu_tasks_idle_cpu;\n\tstruct list_head\t\trcu_tasks_holdout_list;\n#endif  \n\n#ifdef CONFIG_TASKS_TRACE_RCU\n\tint\t\t\t\ttrc_reader_nesting;\n\tint\t\t\t\ttrc_ipi_to_cpu;\n\tunion rcu_special\t\ttrc_reader_special;\n\tstruct list_head\t\ttrc_holdout_list;\n\tstruct list_head\t\ttrc_blkd_node;\n\tint\t\t\t\ttrc_blkd_cpu;\n#endif  \n\n\tstruct sched_info\t\tsched_info;\n\n\tstruct list_head\t\ttasks;\n#ifdef CONFIG_SMP\n\tstruct plist_node\t\tpushable_tasks;\n\tstruct rb_node\t\t\tpushable_dl_tasks;\n#endif\n\n\tstruct mm_struct\t\t*mm;\n\tstruct mm_struct\t\t*active_mm;\n\n\tint\t\t\t\texit_state;\n\tint\t\t\t\texit_code;\n\tint\t\t\t\texit_signal;\n\t \n\tint\t\t\t\tpdeath_signal;\n\t \n\tunsigned long\t\t\tjobctl;\n\n\t \n\tunsigned int\t\t\tpersonality;\n\n\t \n\tunsigned\t\t\tsched_reset_on_fork:1;\n\tunsigned\t\t\tsched_contributes_to_load:1;\n\tunsigned\t\t\tsched_migrated:1;\n\n\t \n\tunsigned\t\t\t:0;\n\n\t \n\n\t \n\tunsigned\t\t\tsched_remote_wakeup:1;\n\n\t \n\tunsigned\t\t\tin_execve:1;\n\tunsigned\t\t\tin_iowait:1;\n#ifndef TIF_RESTORE_SIGMASK\n\tunsigned\t\t\trestore_sigmask:1;\n#endif\n#ifdef CONFIG_MEMCG\n\tunsigned\t\t\tin_user_fault:1;\n#endif\n#ifdef CONFIG_LRU_GEN\n\t \n\tunsigned\t\t\tin_lru_fault:1;\n#endif\n#ifdef CONFIG_COMPAT_BRK\n\tunsigned\t\t\tbrk_randomized:1;\n#endif\n#ifdef CONFIG_CGROUPS\n\t \n\tunsigned\t\t\tno_cgroup_migration:1;\n\t \n\tunsigned\t\t\tfrozen:1;\n#endif\n#ifdef CONFIG_BLK_CGROUP\n\tunsigned\t\t\tuse_memdelay:1;\n#endif\n#ifdef CONFIG_PSI\n\t \n\tunsigned\t\t\tin_memstall:1;\n#endif\n#ifdef CONFIG_PAGE_OWNER\n\t \n\tunsigned\t\t\tin_page_owner:1;\n#endif\n#ifdef CONFIG_EVENTFD\n\t \n\tunsigned\t\t\tin_eventfd:1;\n#endif\n#ifdef CONFIG_IOMMU_SVA\n\tunsigned\t\t\tpasid_activated:1;\n#endif\n#ifdef\tCONFIG_CPU_SUP_INTEL\n\tunsigned\t\t\treported_split_lock:1;\n#endif\n#ifdef CONFIG_TASK_DELAY_ACCT\n\t \n\tunsigned                        in_thrashing:1;\n#endif\n\n\tunsigned long\t\t\tatomic_flags;  \n\n\tstruct restart_block\t\trestart_block;\n\n\tpid_t\t\t\t\tpid;\n\tpid_t\t\t\t\ttgid;\n\n#ifdef CONFIG_STACKPROTECTOR\n\t \n\tunsigned long\t\t\tstack_canary;\n#endif\n\t \n\n\t \n\tstruct task_struct __rcu\t*real_parent;\n\n\t \n\tstruct task_struct __rcu\t*parent;\n\n\t \n\tstruct list_head\t\tchildren;\n\tstruct list_head\t\tsibling;\n\tstruct task_struct\t\t*group_leader;\n\n\t \n\tstruct list_head\t\tptraced;\n\tstruct list_head\t\tptrace_entry;\n\n\t \n\tstruct pid\t\t\t*thread_pid;\n\tstruct hlist_node\t\tpid_links[PIDTYPE_MAX];\n\tstruct list_head\t\tthread_group;\n\tstruct list_head\t\tthread_node;\n\n\tstruct completion\t\t*vfork_done;\n\n\t \n\tint __user\t\t\t*set_child_tid;\n\n\t \n\tint __user\t\t\t*clear_child_tid;\n\n\t \n\tvoid\t\t\t\t*worker_private;\n\n\tu64\t\t\t\tutime;\n\tu64\t\t\t\tstime;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tu64\t\t\t\tutimescaled;\n\tu64\t\t\t\tstimescaled;\n#endif\n\tu64\t\t\t\tgtime;\n\tstruct prev_cputime\t\tprev_cputime;\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tstruct vtime\t\t\tvtime;\n#endif\n\n#ifdef CONFIG_NO_HZ_FULL\n\tatomic_t\t\t\ttick_dep_mask;\n#endif\n\t \n\tunsigned long\t\t\tnvcsw;\n\tunsigned long\t\t\tnivcsw;\n\n\t \n\tu64\t\t\t\tstart_time;\n\n\t \n\tu64\t\t\t\tstart_boottime;\n\n\t \n\tunsigned long\t\t\tmin_flt;\n\tunsigned long\t\t\tmaj_flt;\n\n\t \n\tstruct posix_cputimers\t\tposix_cputimers;\n\n#ifdef CONFIG_POSIX_CPU_TIMERS_TASK_WORK\n\tstruct posix_cputimers_work\tposix_cputimers_work;\n#endif\n\n\t \n\n\t \n\tconst struct cred __rcu\t\t*ptracer_cred;\n\n\t \n\tconst struct cred __rcu\t\t*real_cred;\n\n\t \n\tconst struct cred __rcu\t\t*cred;\n\n#ifdef CONFIG_KEYS\n\t \n\tstruct key\t\t\t*cached_requested_key;\n#endif\n\n\t \n\tchar\t\t\t\tcomm[TASK_COMM_LEN];\n\n\tstruct nameidata\t\t*nameidata;\n\n#ifdef CONFIG_SYSVIPC\n\tstruct sysv_sem\t\t\tsysvsem;\n\tstruct sysv_shm\t\t\tsysvshm;\n#endif\n#ifdef CONFIG_DETECT_HUNG_TASK\n\tunsigned long\t\t\tlast_switch_count;\n\tunsigned long\t\t\tlast_switch_time;\n#endif\n\t \n\tstruct fs_struct\t\t*fs;\n\n\t \n\tstruct files_struct\t\t*files;\n\n#ifdef CONFIG_IO_URING\n\tstruct io_uring_task\t\t*io_uring;\n#endif\n\n\t \n\tstruct nsproxy\t\t\t*nsproxy;\n\n\t \n\tstruct signal_struct\t\t*signal;\n\tstruct sighand_struct __rcu\t\t*sighand;\n\tsigset_t\t\t\tblocked;\n\tsigset_t\t\t\treal_blocked;\n\t \n\tsigset_t\t\t\tsaved_sigmask;\n\tstruct sigpending\t\tpending;\n\tunsigned long\t\t\tsas_ss_sp;\n\tsize_t\t\t\t\tsas_ss_size;\n\tunsigned int\t\t\tsas_ss_flags;\n\n\tstruct callback_head\t\t*task_works;\n\n#ifdef CONFIG_AUDIT\n#ifdef CONFIG_AUDITSYSCALL\n\tstruct audit_context\t\t*audit_context;\n#endif\n\tkuid_t\t\t\t\tloginuid;\n\tunsigned int\t\t\tsessionid;\n#endif\n\tstruct seccomp\t\t\tseccomp;\n\tstruct syscall_user_dispatch\tsyscall_dispatch;\n\n\t \n\tu64\t\t\t\tparent_exec_id;\n\tu64\t\t\t\tself_exec_id;\n\n\t \n\tspinlock_t\t\t\talloc_lock;\n\n\t \n\traw_spinlock_t\t\t\tpi_lock;\n\n\tstruct wake_q_node\t\twake_q;\n\n#ifdef CONFIG_RT_MUTEXES\n\t \n\tstruct rb_root_cached\t\tpi_waiters;\n\t \n\tstruct task_struct\t\t*pi_top_task;\n\t \n\tstruct rt_mutex_waiter\t\t*pi_blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\t \n\tstruct mutex_waiter\t\t*blocked_on;\n#endif\n\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tint\t\t\t\tnon_block_count;\n#endif\n\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tstruct irqtrace_events\t\tirqtrace;\n\tunsigned int\t\t\thardirq_threaded;\n\tu64\t\t\t\thardirq_chain_key;\n\tint\t\t\t\tsoftirqs_enabled;\n\tint\t\t\t\tsoftirq_context;\n\tint\t\t\t\tirq_config;\n#endif\n#ifdef CONFIG_PREEMPT_RT\n\tint\t\t\t\tsoftirq_disable_cnt;\n#endif\n\n#ifdef CONFIG_LOCKDEP\n# define MAX_LOCK_DEPTH\t\t\t48UL\n\tu64\t\t\t\tcurr_chain_key;\n\tint\t\t\t\tlockdep_depth;\n\tunsigned int\t\t\tlockdep_recursion;\n\tstruct held_lock\t\theld_locks[MAX_LOCK_DEPTH];\n#endif\n\n#if defined(CONFIG_UBSAN) && !defined(CONFIG_UBSAN_TRAP)\n\tunsigned int\t\t\tin_ubsan;\n#endif\n\n\t \n\tvoid\t\t\t\t*journal_info;\n\n\t \n\tstruct bio_list\t\t\t*bio_list;\n\n\t \n\tstruct blk_plug\t\t\t*plug;\n\n\t \n\tstruct reclaim_state\t\t*reclaim_state;\n\n\tstruct io_context\t\t*io_context;\n\n#ifdef CONFIG_COMPACTION\n\tstruct capture_control\t\t*capture_control;\n#endif\n\t \n\tunsigned long\t\t\tptrace_message;\n\tkernel_siginfo_t\t\t*last_siginfo;\n\n\tstruct task_io_accounting\tioac;\n#ifdef CONFIG_PSI\n\t \n\tunsigned int\t\t\tpsi_flags;\n#endif\n#ifdef CONFIG_TASK_XACCT\n\t \n\tu64\t\t\t\tacct_rss_mem1;\n\t \n\tu64\t\t\t\tacct_vm_mem1;\n\t \n\tu64\t\t\t\tacct_timexpd;\n#endif\n#ifdef CONFIG_CPUSETS\n\t \n\tnodemask_t\t\t\tmems_allowed;\n\t \n\tseqcount_spinlock_t\t\tmems_allowed_seq;\n\tint\t\t\t\tcpuset_mem_spread_rotor;\n\tint\t\t\t\tcpuset_slab_spread_rotor;\n#endif\n#ifdef CONFIG_CGROUPS\n\t \n\tstruct css_set __rcu\t\t*cgroups;\n\t \n\tstruct list_head\t\tcg_list;\n#endif\n#ifdef CONFIG_X86_CPU_RESCTRL\n\tu32\t\t\t\tclosid;\n\tu32\t\t\t\trmid;\n#endif\n#ifdef CONFIG_FUTEX\n\tstruct robust_list_head __user\t*robust_list;\n#ifdef CONFIG_COMPAT\n\tstruct compat_robust_list_head __user *compat_robust_list;\n#endif\n\tstruct list_head\t\tpi_state_list;\n\tstruct futex_pi_state\t\t*pi_state_cache;\n\tstruct mutex\t\t\tfutex_exit_mutex;\n\tunsigned int\t\t\tfutex_state;\n#endif\n#ifdef CONFIG_PERF_EVENTS\n\tstruct perf_event_context\t*perf_event_ctxp;\n\tstruct mutex\t\t\tperf_event_mutex;\n\tstruct list_head\t\tperf_event_list;\n#endif\n#ifdef CONFIG_DEBUG_PREEMPT\n\tunsigned long\t\t\tpreempt_disable_ip;\n#endif\n#ifdef CONFIG_NUMA\n\t \n\tstruct mempolicy\t\t*mempolicy;\n\tshort\t\t\t\til_prev;\n\tshort\t\t\t\tpref_node_fork;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n\tint\t\t\t\tnuma_scan_seq;\n\tunsigned int\t\t\tnuma_scan_period;\n\tunsigned int\t\t\tnuma_scan_period_max;\n\tint\t\t\t\tnuma_preferred_nid;\n\tunsigned long\t\t\tnuma_migrate_retry;\n\t \n\tu64\t\t\t\tnode_stamp;\n\tu64\t\t\t\tlast_task_numa_placement;\n\tu64\t\t\t\tlast_sum_exec_runtime;\n\tstruct callback_head\t\tnuma_work;\n\n\t \n\tstruct numa_group __rcu\t\t*numa_group;\n\n\t \n\tunsigned long\t\t\t*numa_faults;\n\tunsigned long\t\t\ttotal_numa_faults;\n\n\t \n\tunsigned long\t\t\tnuma_faults_locality[3];\n\n\tunsigned long\t\t\tnuma_pages_migrated;\n#endif  \n\n#ifdef CONFIG_RSEQ\n\tstruct rseq __user *rseq;\n\tu32 rseq_len;\n\tu32 rseq_sig;\n\t \n\tunsigned long rseq_event_mask;\n#endif\n\n#ifdef CONFIG_SCHED_MM_CID\n\tint\t\t\t\tmm_cid;\t\t \n\tint\t\t\t\tlast_mm_cid;\t \n\tint\t\t\t\tmigrate_from_cpu;\n\tint\t\t\t\tmm_cid_active;\t \n\tstruct callback_head\t\tcid_work;\n#endif\n\n\tstruct tlbflush_unmap_batch\ttlb_ubc;\n\n\t \n\tstruct pipe_inode_info\t\t*splice_pipe;\n\n\tstruct page_frag\t\ttask_frag;\n\n#ifdef CONFIG_TASK_DELAY_ACCT\n\tstruct task_delay_info\t\t*delays;\n#endif\n\n#ifdef CONFIG_FAULT_INJECTION\n\tint\t\t\t\tmake_it_fail;\n\tunsigned int\t\t\tfail_nth;\n#endif\n\t \n\tint\t\t\t\tnr_dirtied;\n\tint\t\t\t\tnr_dirtied_pause;\n\t \n\tunsigned long\t\t\tdirty_paused_when;\n\n#ifdef CONFIG_LATENCYTOP\n\tint\t\t\t\tlatency_record_count;\n\tstruct latency_record\t\tlatency_record[LT_SAVECOUNT];\n#endif\n\t \n\tu64\t\t\t\ttimer_slack_ns;\n\tu64\t\t\t\tdefault_timer_slack_ns;\n\n#if defined(CONFIG_KASAN_GENERIC) || defined(CONFIG_KASAN_SW_TAGS)\n\tunsigned int\t\t\tkasan_depth;\n#endif\n\n#ifdef CONFIG_KCSAN\n\tstruct kcsan_ctx\t\tkcsan_ctx;\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tstruct irqtrace_events\t\tkcsan_save_irqtrace;\n#endif\n#ifdef CONFIG_KCSAN_WEAK_MEMORY\n\tint\t\t\t\tkcsan_stack_depth;\n#endif\n#endif\n\n#ifdef CONFIG_KMSAN\n\tstruct kmsan_ctx\t\tkmsan_ctx;\n#endif\n\n#if IS_ENABLED(CONFIG_KUNIT)\n\tstruct kunit\t\t\t*kunit_test;\n#endif\n\n#ifdef CONFIG_FUNCTION_GRAPH_TRACER\n\t \n\tint\t\t\t\tcurr_ret_stack;\n\tint\t\t\t\tcurr_ret_depth;\n\n\t \n\tstruct ftrace_ret_stack\t\t*ret_stack;\n\n\t \n\tunsigned long long\t\tftrace_timestamp;\n\n\t \n\tatomic_t\t\t\ttrace_overrun;\n\n\t \n\tatomic_t\t\t\ttracing_graph_pause;\n#endif\n\n#ifdef CONFIG_TRACING\n\t \n\tunsigned long\t\t\ttrace_recursion;\n#endif  \n\n#ifdef CONFIG_KCOV\n\t \n\n\t \n\tunsigned int\t\t\tkcov_mode;\n\n\t \n\tunsigned int\t\t\tkcov_size;\n\n\t \n\tvoid\t\t\t\t*kcov_area;\n\n\t \n\tstruct kcov\t\t\t*kcov;\n\n\t \n\tu64\t\t\t\tkcov_handle;\n\n\t \n\tint\t\t\t\tkcov_sequence;\n\n\t \n\tunsigned int\t\t\tkcov_softirq;\n#endif\n\n#ifdef CONFIG_MEMCG\n\tstruct mem_cgroup\t\t*memcg_in_oom;\n\tgfp_t\t\t\t\tmemcg_oom_gfp_mask;\n\tint\t\t\t\tmemcg_oom_order;\n\n\t \n\tunsigned int\t\t\tmemcg_nr_pages_over_high;\n\n\t \n\tstruct mem_cgroup\t\t*active_memcg;\n#endif\n\n#ifdef CONFIG_BLK_CGROUP\n\tstruct gendisk\t\t\t*throttle_disk;\n#endif\n\n#ifdef CONFIG_UPROBES\n\tstruct uprobe_task\t\t*utask;\n#endif\n#if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)\n\tunsigned int\t\t\tsequential_io;\n\tunsigned int\t\t\tsequential_io_avg;\n#endif\n\tstruct kmap_ctrl\t\tkmap_ctrl;\n#ifdef CONFIG_DEBUG_ATOMIC_SLEEP\n\tunsigned long\t\t\ttask_state_change;\n# ifdef CONFIG_PREEMPT_RT\n\tunsigned long\t\t\tsaved_state_change;\n# endif\n#endif\n\tstruct rcu_head\t\t\trcu;\n\trefcount_t\t\t\trcu_users;\n\tint\t\t\t\tpagefault_disabled;\n#ifdef CONFIG_MMU\n\tstruct task_struct\t\t*oom_reaper_list;\n\tstruct timer_list\t\toom_reaper_timer;\n#endif\n#ifdef CONFIG_VMAP_STACK\n\tstruct vm_struct\t\t*stack_vm_area;\n#endif\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n\t \n\trefcount_t\t\t\tstack_refcount;\n#endif\n#ifdef CONFIG_LIVEPATCH\n\tint patch_state;\n#endif\n#ifdef CONFIG_SECURITY\n\t \n\tvoid\t\t\t\t*security;\n#endif\n#ifdef CONFIG_BPF_SYSCALL\n\t \n\tstruct bpf_local_storage __rcu\t*bpf_storage;\n\t \n\tstruct bpf_run_ctx\t\t*bpf_ctx;\n#endif\n\n#ifdef CONFIG_GCC_PLUGIN_STACKLEAK\n\tunsigned long\t\t\tlowest_stack;\n\tunsigned long\t\t\tprev_lowest_stack;\n#endif\n\n#ifdef CONFIG_X86_MCE\n\tvoid __user\t\t\t*mce_vaddr;\n\t__u64\t\t\t\tmce_kflags;\n\tu64\t\t\t\tmce_addr;\n\t__u64\t\t\t\tmce_ripv : 1,\n\t\t\t\t\tmce_whole_page : 1,\n\t\t\t\t\t__mce_reserved : 62;\n\tstruct callback_head\t\tmce_kill_me;\n\tint\t\t\t\tmce_count;\n#endif\n\n#ifdef CONFIG_KRETPROBES\n\tstruct llist_head               kretprobe_instances;\n#endif\n#ifdef CONFIG_RETHOOK\n\tstruct llist_head               rethooks;\n#endif\n\n#ifdef CONFIG_ARCH_HAS_PARANOID_L1D_FLUSH\n\t \n\tstruct callback_head\t\tl1d_flush_kill;\n#endif\n\n#ifdef CONFIG_RV\n\t \n\tunion rv_task_monitor\t\trv[RV_PER_TASK_MONITORS];\n#endif\n\n#ifdef CONFIG_USER_EVENTS\n\tstruct user_event_mm\t\t*user_event_mm;\n#endif\n\n\t \n\trandomized_struct_fields_end\n\n\t \n\tstruct thread_struct\t\tthread;\n\n\t \n};\n\nstatic inline struct pid *task_pid(struct task_struct *task)\n{\n\treturn task->thread_pid;\n}\n\n \npid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);\n\nstatic inline pid_t task_pid_nr(struct task_struct *tsk)\n{\n\treturn tsk->pid;\n}\n\nstatic inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);\n}\n\nstatic inline pid_t task_pid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);\n}\n\n\nstatic inline pid_t task_tgid_nr(struct task_struct *tsk)\n{\n\treturn tsk->tgid;\n}\n\n \nstatic inline int pid_alive(const struct task_struct *p)\n{\n\treturn p->thread_pid != NULL;\n}\n\nstatic inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);\n}\n\nstatic inline pid_t task_pgrp_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);\n}\n\n\nstatic inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);\n}\n\nstatic inline pid_t task_session_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);\n}\n\nstatic inline pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, ns);\n}\n\nstatic inline pid_t task_tgid_vnr(struct task_struct *tsk)\n{\n\treturn __task_pid_nr_ns(tsk, PIDTYPE_TGID, NULL);\n}\n\nstatic inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)\n{\n\tpid_t pid = 0;\n\n\trcu_read_lock();\n\tif (pid_alive(tsk))\n\t\tpid = task_tgid_nr_ns(rcu_dereference(tsk->real_parent), ns);\n\trcu_read_unlock();\n\n\treturn pid;\n}\n\nstatic inline pid_t task_ppid_nr(const struct task_struct *tsk)\n{\n\treturn task_ppid_nr_ns(tsk, &init_pid_ns);\n}\n\n \nstatic inline pid_t task_pgrp_nr(struct task_struct *tsk)\n{\n\treturn task_pgrp_nr_ns(tsk, &init_pid_ns);\n}\n\n#define TASK_REPORT_IDLE\t(TASK_REPORT + 1)\n#define TASK_REPORT_MAX\t\t(TASK_REPORT_IDLE << 1)\n\nstatic inline unsigned int __task_state_index(unsigned int tsk_state,\n\t\t\t\t\t      unsigned int tsk_exit_state)\n{\n\tunsigned int state = (tsk_state | tsk_exit_state) & TASK_REPORT;\n\n\tBUILD_BUG_ON_NOT_POWER_OF_2(TASK_REPORT_MAX);\n\n\tif ((tsk_state & TASK_IDLE) == TASK_IDLE)\n\t\tstate = TASK_REPORT_IDLE;\n\n\t \n\tif (tsk_state & TASK_RTLOCK_WAIT)\n\t\tstate = TASK_UNINTERRUPTIBLE;\n\n\treturn fls(state);\n}\n\nstatic inline unsigned int task_state_index(struct task_struct *tsk)\n{\n\treturn __task_state_index(READ_ONCE(tsk->__state), tsk->exit_state);\n}\n\nstatic inline char task_index_to_char(unsigned int state)\n{\n\tstatic const char state_char[] = \"RSDTtXZPI\";\n\n\tBUILD_BUG_ON(1 + ilog2(TASK_REPORT_MAX) != sizeof(state_char) - 1);\n\n\treturn state_char[state];\n}\n\nstatic inline char task_state_to_char(struct task_struct *tsk)\n{\n\treturn task_index_to_char(task_state_index(tsk));\n}\n\n \nstatic inline int is_global_init(struct task_struct *tsk)\n{\n\treturn task_tgid_nr(tsk) == 1;\n}\n\nextern struct pid *cad_pid;\n\n \n#define PF_VCPU\t\t\t0x00000001\t \n#define PF_IDLE\t\t\t0x00000002\t \n#define PF_EXITING\t\t0x00000004\t \n#define PF_POSTCOREDUMP\t\t0x00000008\t \n#define PF_IO_WORKER\t\t0x00000010\t \n#define PF_WQ_WORKER\t\t0x00000020\t \n#define PF_FORKNOEXEC\t\t0x00000040\t \n#define PF_MCE_PROCESS\t\t0x00000080       \n#define PF_SUPERPRIV\t\t0x00000100\t \n#define PF_DUMPCORE\t\t0x00000200\t \n#define PF_SIGNALED\t\t0x00000400\t \n#define PF_MEMALLOC\t\t0x00000800\t \n#define PF_NPROC_EXCEEDED\t0x00001000\t \n#define PF_USED_MATH\t\t0x00002000\t \n#define PF_USER_WORKER\t\t0x00004000\t \n#define PF_NOFREEZE\t\t0x00008000\t \n#define PF__HOLE__00010000\t0x00010000\n#define PF_KSWAPD\t\t0x00020000\t \n#define PF_MEMALLOC_NOFS\t0x00040000\t \n#define PF_MEMALLOC_NOIO\t0x00080000\t \n#define PF_LOCAL_THROTTLE\t0x00100000\t \n#define PF_KTHREAD\t\t0x00200000\t \n#define PF_RANDOMIZE\t\t0x00400000\t \n#define PF__HOLE__00800000\t0x00800000\n#define PF__HOLE__01000000\t0x01000000\n#define PF__HOLE__02000000\t0x02000000\n#define PF_NO_SETAFFINITY\t0x04000000\t \n#define PF_MCE_EARLY\t\t0x08000000       \n#define PF_MEMALLOC_PIN\t\t0x10000000\t \n#define PF__HOLE__20000000\t0x20000000\n#define PF__HOLE__40000000\t0x40000000\n#define PF_SUSPEND_TASK\t\t0x80000000       \n\n \n#define clear_stopped_child_used_math(child)\tdo { (child)->flags &= ~PF_USED_MATH; } while (0)\n#define set_stopped_child_used_math(child)\tdo { (child)->flags |= PF_USED_MATH; } while (0)\n#define clear_used_math()\t\t\tclear_stopped_child_used_math(current)\n#define set_used_math()\t\t\t\tset_stopped_child_used_math(current)\n\n#define conditional_stopped_child_used_math(condition, child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= (condition) ? PF_USED_MATH : 0; } while (0)\n\n#define conditional_used_math(condition)\tconditional_stopped_child_used_math(condition, current)\n\n#define copy_to_stopped_child_used_math(child) \\\n\tdo { (child)->flags &= ~PF_USED_MATH, (child)->flags |= current->flags & PF_USED_MATH; } while (0)\n\n \n#define tsk_used_math(p)\t\t\t((p)->flags & PF_USED_MATH)\n#define used_math()\t\t\t\ttsk_used_math(current)\n\nstatic __always_inline bool is_percpu_thread(void)\n{\n#ifdef CONFIG_SMP\n\treturn (current->flags & PF_NO_SETAFFINITY) &&\n\t\t(current->nr_cpus_allowed  == 1);\n#else\n\treturn true;\n#endif\n}\n\n \n#define PFA_NO_NEW_PRIVS\t\t0\t \n#define PFA_SPREAD_PAGE\t\t\t1\t \n#define PFA_SPREAD_SLAB\t\t\t2\t \n#define PFA_SPEC_SSB_DISABLE\t\t3\t \n#define PFA_SPEC_SSB_FORCE_DISABLE\t4\t \n#define PFA_SPEC_IB_DISABLE\t\t5\t \n#define PFA_SPEC_IB_FORCE_DISABLE\t6\t \n#define PFA_SPEC_SSB_NOEXEC\t\t7\t \n\n#define TASK_PFA_TEST(name, func)\t\t\t\t\t\\\n\tstatic inline bool task_##func(struct task_struct *p)\t\t\\\n\t{ return test_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_SET(name, func)\t\t\t\t\t\\\n\tstatic inline void task_set_##func(struct task_struct *p)\t\\\n\t{ set_bit(PFA_##name, &p->atomic_flags); }\n\n#define TASK_PFA_CLEAR(name, func)\t\t\t\t\t\\\n\tstatic inline void task_clear_##func(struct task_struct *p)\t\\\n\t{ clear_bit(PFA_##name, &p->atomic_flags); }\n\nTASK_PFA_TEST(NO_NEW_PRIVS, no_new_privs)\nTASK_PFA_SET(NO_NEW_PRIVS, no_new_privs)\n\nTASK_PFA_TEST(SPREAD_PAGE, spread_page)\nTASK_PFA_SET(SPREAD_PAGE, spread_page)\nTASK_PFA_CLEAR(SPREAD_PAGE, spread_page)\n\nTASK_PFA_TEST(SPREAD_SLAB, spread_slab)\nTASK_PFA_SET(SPREAD_SLAB, spread_slab)\nTASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)\n\nTASK_PFA_TEST(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_SET(SPEC_SSB_DISABLE, spec_ssb_disable)\nTASK_PFA_CLEAR(SPEC_SSB_DISABLE, spec_ssb_disable)\n\nTASK_PFA_TEST(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_SET(SPEC_SSB_NOEXEC, spec_ssb_noexec)\nTASK_PFA_CLEAR(SPEC_SSB_NOEXEC, spec_ssb_noexec)\n\nTASK_PFA_TEST(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\nTASK_PFA_SET(SPEC_SSB_FORCE_DISABLE, spec_ssb_force_disable)\n\nTASK_PFA_TEST(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_SET(SPEC_IB_DISABLE, spec_ib_disable)\nTASK_PFA_CLEAR(SPEC_IB_DISABLE, spec_ib_disable)\n\nTASK_PFA_TEST(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\nTASK_PFA_SET(SPEC_IB_FORCE_DISABLE, spec_ib_force_disable)\n\nstatic inline void\ncurrent_restore_flags(unsigned long orig_flags, unsigned long flags)\n{\n\tcurrent->flags &= ~flags;\n\tcurrent->flags |= orig_flags & flags;\n}\n\nextern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);\nextern int task_can_attach(struct task_struct *p);\nextern int dl_bw_alloc(int cpu, u64 dl_bw);\nextern void dl_bw_free(int cpu, u64 dl_bw);\n#ifdef CONFIG_SMP\n\n \nextern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);\n\n \nextern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);\nextern int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src, int node);\nextern void release_user_cpus_ptr(struct task_struct *p);\nextern int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask);\nextern void force_compatible_cpus_allowed_ptr(struct task_struct *p);\nextern void relax_compatible_cpus_allowed_ptr(struct task_struct *p);\n#else\nstatic inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)\n{\n}\nstatic inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)\n{\n\tif (!cpumask_test_cpu(0, new_mask))\n\t\treturn -EINVAL;\n\treturn 0;\n}\nstatic inline int dup_user_cpus_ptr(struct task_struct *dst, struct task_struct *src, int node)\n{\n\tif (src->user_cpus_ptr)\n\t\treturn -EINVAL;\n\treturn 0;\n}\nstatic inline void release_user_cpus_ptr(struct task_struct *p)\n{\n\tWARN_ON(p->user_cpus_ptr);\n}\n\nstatic inline int dl_task_check_affinity(struct task_struct *p, const struct cpumask *mask)\n{\n\treturn 0;\n}\n#endif\n\nextern int yield_to(struct task_struct *p, bool preempt);\nextern void set_user_nice(struct task_struct *p, long nice);\nextern int task_prio(const struct task_struct *p);\n\n \nstatic inline int task_nice(const struct task_struct *p)\n{\n\treturn PRIO_TO_NICE((p)->static_prio);\n}\n\nextern int can_nice(const struct task_struct *p, const int nice);\nextern int task_curr(const struct task_struct *p);\nextern int idle_cpu(int cpu);\nextern int available_idle_cpu(int cpu);\nextern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);\nextern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);\nextern void sched_set_fifo(struct task_struct *p);\nextern void sched_set_fifo_low(struct task_struct *p);\nextern void sched_set_normal(struct task_struct *p, int nice);\nextern int sched_setattr(struct task_struct *, const struct sched_attr *);\nextern int sched_setattr_nocheck(struct task_struct *, const struct sched_attr *);\nextern struct task_struct *idle_task(int cpu);\n\n \nstatic __always_inline bool is_idle_task(const struct task_struct *p)\n{\n\treturn !!(p->flags & PF_IDLE);\n}\n\nextern struct task_struct *curr_task(int cpu);\nextern void ia64_set_curr_task(int cpu, struct task_struct *p);\n\nvoid yield(void);\n\nunion thread_union {\n#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK\n\tstruct task_struct task;\n#endif\n#ifndef CONFIG_THREAD_INFO_IN_TASK\n\tstruct thread_info thread_info;\n#endif\n\tunsigned long stack[THREAD_SIZE/sizeof(long)];\n};\n\n#ifndef CONFIG_THREAD_INFO_IN_TASK\nextern struct thread_info init_thread_info;\n#endif\n\nextern unsigned long init_stack[THREAD_SIZE / sizeof(unsigned long)];\n\n#ifdef CONFIG_THREAD_INFO_IN_TASK\n# define task_thread_info(task)\t(&(task)->thread_info)\n#elif !defined(__HAVE_THREAD_FUNCTIONS)\n# define task_thread_info(task)\t((struct thread_info *)(task)->stack)\n#endif\n\n \n\nextern struct task_struct *find_task_by_vpid(pid_t nr);\nextern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);\n\n \nextern struct task_struct *find_get_task_by_vpid(pid_t nr);\n\nextern int wake_up_state(struct task_struct *tsk, unsigned int state);\nextern int wake_up_process(struct task_struct *tsk);\nextern void wake_up_new_task(struct task_struct *tsk);\n\n#ifdef CONFIG_SMP\nextern void kick_process(struct task_struct *tsk);\n#else\nstatic inline void kick_process(struct task_struct *tsk) { }\n#endif\n\nextern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);\n\nstatic inline void set_task_comm(struct task_struct *tsk, const char *from)\n{\n\t__set_task_comm(tsk, from, false);\n}\n\nextern char *__get_task_comm(char *to, size_t len, struct task_struct *tsk);\n#define get_task_comm(buf, tsk) ({\t\t\t\\\n\tBUILD_BUG_ON(sizeof(buf) != TASK_COMM_LEN);\t\\\n\t__get_task_comm(buf, sizeof(buf), tsk);\t\t\\\n})\n\n#ifdef CONFIG_SMP\nstatic __always_inline void scheduler_ipi(void)\n{\n\t \n\tpreempt_fold_need_resched();\n}\n#else\nstatic inline void scheduler_ipi(void) { }\n#endif\n\nextern unsigned long wait_task_inactive(struct task_struct *, unsigned int match_state);\n\n \nstatic inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tset_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\tclear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void update_tsk_thread_flag(struct task_struct *tsk, int flag,\n\t\t\t\t\t  bool value)\n{\n\tupdate_ti_thread_flag(task_thread_info(tsk), flag, value);\n}\n\nstatic inline int test_and_set_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_set_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_and_clear_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)\n{\n\treturn test_ti_thread_flag(task_thread_info(tsk), flag);\n}\n\nstatic inline void set_tsk_need_resched(struct task_struct *tsk)\n{\n\tset_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline void clear_tsk_need_resched(struct task_struct *tsk)\n{\n\tclear_tsk_thread_flag(tsk,TIF_NEED_RESCHED);\n}\n\nstatic inline int test_tsk_need_resched(struct task_struct *tsk)\n{\n\treturn unlikely(test_tsk_thread_flag(tsk,TIF_NEED_RESCHED));\n}\n\n \n#if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)\nextern int __cond_resched(void);\n\n#if defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_CALL)\n\nvoid sched_dynamic_klp_enable(void);\nvoid sched_dynamic_klp_disable(void);\n\nDECLARE_STATIC_CALL(cond_resched, __cond_resched);\n\nstatic __always_inline int _cond_resched(void)\n{\n\treturn static_call_mod(cond_resched)();\n}\n\n#elif defined(CONFIG_PREEMPT_DYNAMIC) && defined(CONFIG_HAVE_PREEMPT_DYNAMIC_KEY)\n\nextern int dynamic_cond_resched(void);\n\nstatic __always_inline int _cond_resched(void)\n{\n\treturn dynamic_cond_resched();\n}\n\n#else  \n\nstatic inline int _cond_resched(void)\n{\n\tklp_sched_try_switch();\n\treturn __cond_resched();\n}\n\n#endif  \n\n#else  \n\nstatic inline int _cond_resched(void)\n{\n\tklp_sched_try_switch();\n\treturn 0;\n}\n\n#endif  \n\n#define cond_resched() ({\t\t\t\\\n\t__might_resched(__FILE__, __LINE__, 0);\t\\\n\t_cond_resched();\t\t\t\\\n})\n\nextern int __cond_resched_lock(spinlock_t *lock);\nextern int __cond_resched_rwlock_read(rwlock_t *lock);\nextern int __cond_resched_rwlock_write(rwlock_t *lock);\n\n#define MIGHT_RESCHED_RCU_SHIFT\t\t8\n#define MIGHT_RESCHED_PREEMPT_MASK\t((1U << MIGHT_RESCHED_RCU_SHIFT) - 1)\n\n#ifndef CONFIG_PREEMPT_RT\n \n# define PREEMPT_LOCK_RESCHED_OFFSETS\tPREEMPT_LOCK_OFFSET\n#else\n \n# define PREEMPT_LOCK_RESCHED_OFFSETS\t\\\n\t(PREEMPT_LOCK_OFFSET + (1U << MIGHT_RESCHED_RCU_SHIFT))\n#endif\n\n#define cond_resched_lock(lock) ({\t\t\t\t\t\t\\\n\t__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);\t\\\n\t__cond_resched_lock(lock);\t\t\t\t\t\t\\\n})\n\n#define cond_resched_rwlock_read(lock) ({\t\t\t\t\t\\\n\t__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);\t\\\n\t__cond_resched_rwlock_read(lock);\t\t\t\t\t\\\n})\n\n#define cond_resched_rwlock_write(lock) ({\t\t\t\t\t\\\n\t__might_resched(__FILE__, __LINE__, PREEMPT_LOCK_RESCHED_OFFSETS);\t\\\n\t__cond_resched_rwlock_write(lock);\t\t\t\t\t\\\n})\n\nstatic inline void cond_resched_rcu(void)\n{\n#if defined(CONFIG_DEBUG_ATOMIC_SLEEP) || !defined(CONFIG_PREEMPT_RCU)\n\trcu_read_unlock();\n\tcond_resched();\n\trcu_read_lock();\n#endif\n}\n\n#ifdef CONFIG_PREEMPT_DYNAMIC\n\nextern bool preempt_model_none(void);\nextern bool preempt_model_voluntary(void);\nextern bool preempt_model_full(void);\n\n#else\n\nstatic inline bool preempt_model_none(void)\n{\n\treturn IS_ENABLED(CONFIG_PREEMPT_NONE);\n}\nstatic inline bool preempt_model_voluntary(void)\n{\n\treturn IS_ENABLED(CONFIG_PREEMPT_VOLUNTARY);\n}\nstatic inline bool preempt_model_full(void)\n{\n\treturn IS_ENABLED(CONFIG_PREEMPT);\n}\n\n#endif\n\nstatic inline bool preempt_model_rt(void)\n{\n\treturn IS_ENABLED(CONFIG_PREEMPT_RT);\n}\n\n \nstatic inline bool preempt_model_preemptible(void)\n{\n\treturn preempt_model_full() || preempt_model_rt();\n}\n\n \nstatic inline int spin_needbreak(spinlock_t *lock)\n{\n#ifdef CONFIG_PREEMPTION\n\treturn spin_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic inline int rwlock_needbreak(rwlock_t *lock)\n{\n#ifdef CONFIG_PREEMPTION\n\treturn rwlock_is_contended(lock);\n#else\n\treturn 0;\n#endif\n}\n\nstatic __always_inline bool need_resched(void)\n{\n\treturn unlikely(tif_need_resched());\n}\n\n \n#ifdef CONFIG_SMP\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn READ_ONCE(task_thread_info(p)->cpu);\n}\n\nextern void set_task_cpu(struct task_struct *p, unsigned int cpu);\n\n#else\n\nstatic inline unsigned int task_cpu(const struct task_struct *p)\n{\n\treturn 0;\n}\n\nstatic inline void set_task_cpu(struct task_struct *p, unsigned int cpu)\n{\n}\n\n#endif  \n\nextern bool sched_task_on_rq(struct task_struct *p);\nextern unsigned long get_wchan(struct task_struct *p);\nextern struct task_struct *cpu_curr_snapshot(int cpu);\n\n \n#ifndef vcpu_is_preempted\nstatic inline bool vcpu_is_preempted(int cpu)\n{\n\treturn false;\n}\n#endif\n\nextern long sched_setaffinity(pid_t pid, const struct cpumask *new_mask);\nextern long sched_getaffinity(pid_t pid, struct cpumask *mask);\n\n#ifndef TASK_SIZE_OF\n#define TASK_SIZE_OF(tsk)\tTASK_SIZE\n#endif\n\n#ifdef CONFIG_SMP\nstatic inline bool owner_on_cpu(struct task_struct *owner)\n{\n\t \n\treturn READ_ONCE(owner->on_cpu) && !vcpu_is_preempted(task_cpu(owner));\n}\n\n \nunsigned long sched_cpu_util(int cpu);\n#endif  \n\n#ifdef CONFIG_RSEQ\n\n \nenum rseq_event_mask_bits {\n\tRSEQ_EVENT_PREEMPT_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_PREEMPT_BIT,\n\tRSEQ_EVENT_SIGNAL_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_SIGNAL_BIT,\n\tRSEQ_EVENT_MIGRATE_BIT\t= RSEQ_CS_FLAG_NO_RESTART_ON_MIGRATE_BIT,\n};\n\nenum rseq_event_mask {\n\tRSEQ_EVENT_PREEMPT\t= (1U << RSEQ_EVENT_PREEMPT_BIT),\n\tRSEQ_EVENT_SIGNAL\t= (1U << RSEQ_EVENT_SIGNAL_BIT),\n\tRSEQ_EVENT_MIGRATE\t= (1U << RSEQ_EVENT_MIGRATE_BIT),\n};\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n\tif (t->rseq)\n\t\tset_tsk_thread_flag(t, TIF_NOTIFY_RESUME);\n}\n\nvoid __rseq_handle_notify_resume(struct ksignal *sig, struct pt_regs *regs);\n\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n\tif (current->rseq)\n\t\t__rseq_handle_notify_resume(ksig, regs);\n}\n\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n\tpreempt_disable();\n\t__set_bit(RSEQ_EVENT_SIGNAL_BIT, &current->rseq_event_mask);\n\tpreempt_enable();\n\trseq_handle_notify_resume(ksig, regs);\n}\n\n \nstatic inline void rseq_preempt(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_PREEMPT_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n \nstatic inline void rseq_migrate(struct task_struct *t)\n{\n\t__set_bit(RSEQ_EVENT_MIGRATE_BIT, &t->rseq_event_mask);\n\trseq_set_notify_resume(t);\n}\n\n \nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n\tif (clone_flags & CLONE_VM) {\n\t\tt->rseq = NULL;\n\t\tt->rseq_len = 0;\n\t\tt->rseq_sig = 0;\n\t\tt->rseq_event_mask = 0;\n\t} else {\n\t\tt->rseq = current->rseq;\n\t\tt->rseq_len = current->rseq_len;\n\t\tt->rseq_sig = current->rseq_sig;\n\t\tt->rseq_event_mask = current->rseq_event_mask;\n\t}\n}\n\nstatic inline void rseq_execve(struct task_struct *t)\n{\n\tt->rseq = NULL;\n\tt->rseq_len = 0;\n\tt->rseq_sig = 0;\n\tt->rseq_event_mask = 0;\n}\n\n#else\n\nstatic inline void rseq_set_notify_resume(struct task_struct *t)\n{\n}\nstatic inline void rseq_handle_notify_resume(struct ksignal *ksig,\n\t\t\t\t\t     struct pt_regs *regs)\n{\n}\nstatic inline void rseq_signal_deliver(struct ksignal *ksig,\n\t\t\t\t       struct pt_regs *regs)\n{\n}\nstatic inline void rseq_preempt(struct task_struct *t)\n{\n}\nstatic inline void rseq_migrate(struct task_struct *t)\n{\n}\nstatic inline void rseq_fork(struct task_struct *t, unsigned long clone_flags)\n{\n}\nstatic inline void rseq_execve(struct task_struct *t)\n{\n}\n\n#endif\n\n#ifdef CONFIG_DEBUG_RSEQ\n\nvoid rseq_syscall(struct pt_regs *regs);\n\n#else\n\nstatic inline void rseq_syscall(struct pt_regs *regs)\n{\n}\n\n#endif\n\n#ifdef CONFIG_SCHED_CORE\nextern void sched_core_free(struct task_struct *tsk);\nextern void sched_core_fork(struct task_struct *p);\nextern int sched_core_share_pid(unsigned int cmd, pid_t pid, enum pid_type type,\n\t\t\t\tunsigned long uaddr);\nextern int sched_core_idle_cpu(int cpu);\n#else\nstatic inline void sched_core_free(struct task_struct *tsk) { }\nstatic inline void sched_core_fork(struct task_struct *p) { }\nstatic inline int sched_core_idle_cpu(int cpu) { return idle_cpu(cpu); }\n#endif\n\nextern void sched_set_stop_task(int cpu, struct task_struct *stop);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}