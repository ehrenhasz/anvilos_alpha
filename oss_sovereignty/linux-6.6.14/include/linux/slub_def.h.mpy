{
  "module_name": "slub_def.h",
  "hash_id": "668e854a8c4cd403fce7c4c760dc2e59a27c6b1b142fe8ccdf2d05b4485d9c10",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/slub_def.h",
  "human_readable_source": " \n#ifndef _LINUX_SLUB_DEF_H\n#define _LINUX_SLUB_DEF_H\n\n \n#include <linux/kfence.h>\n#include <linux/kobject.h>\n#include <linux/reciprocal_div.h>\n#include <linux/local_lock.h>\n\nenum stat_item {\n\tALLOC_FASTPATH,\t\t \n\tALLOC_SLOWPATH,\t\t \n\tFREE_FASTPATH,\t\t \n\tFREE_SLOWPATH,\t\t \n\tFREE_FROZEN,\t\t \n\tFREE_ADD_PARTIAL,\t \n\tFREE_REMOVE_PARTIAL,\t \n\tALLOC_FROM_PARTIAL,\t \n\tALLOC_SLAB,\t\t \n\tALLOC_REFILL,\t\t \n\tALLOC_NODE_MISMATCH,\t \n\tFREE_SLAB,\t\t \n\tCPUSLAB_FLUSH,\t\t \n\tDEACTIVATE_FULL,\t \n\tDEACTIVATE_EMPTY,\t \n\tDEACTIVATE_TO_HEAD,\t \n\tDEACTIVATE_TO_TAIL,\t \n\tDEACTIVATE_REMOTE_FREES, \n\tDEACTIVATE_BYPASS,\t \n\tORDER_FALLBACK,\t\t \n\tCMPXCHG_DOUBLE_CPU_FAIL, \n\tCMPXCHG_DOUBLE_FAIL,\t \n\tCPU_PARTIAL_ALLOC,\t \n\tCPU_PARTIAL_FREE,\t \n\tCPU_PARTIAL_NODE,\t \n\tCPU_PARTIAL_DRAIN,\t \n\tNR_SLUB_STAT_ITEMS\n};\n\n#ifndef CONFIG_SLUB_TINY\n \nstruct kmem_cache_cpu {\n\tunion {\n\t\tstruct {\n\t\t\tvoid **freelist;\t \n\t\t\tunsigned long tid;\t \n\t\t};\n\t\tfreelist_aba_t freelist_tid;\n\t};\n\tstruct slab *slab;\t \n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\tstruct slab *partial;\t \n#endif\n\tlocal_lock_t lock;\t \n#ifdef CONFIG_SLUB_STATS\n\tunsigned stat[NR_SLUB_STAT_ITEMS];\n#endif\n};\n#endif  \n\n#ifdef CONFIG_SLUB_CPU_PARTIAL\n#define slub_percpu_partial(c)\t\t((c)->partial)\n\n#define slub_set_percpu_partial(c, p)\t\t\\\n({\t\t\t\t\t\t\\\n\tslub_percpu_partial(c) = (p)->next;\t\\\n})\n\n#define slub_percpu_partial_read_once(c)     READ_ONCE(slub_percpu_partial(c))\n#else\n#define slub_percpu_partial(c)\t\t\tNULL\n\n#define slub_set_percpu_partial(c, p)\n\n#define slub_percpu_partial_read_once(c)\tNULL\n#endif \n\n \nstruct kmem_cache_order_objects {\n\tunsigned int x;\n};\n\n \nstruct kmem_cache {\n#ifndef CONFIG_SLUB_TINY\n\tstruct kmem_cache_cpu __percpu *cpu_slab;\n#endif\n\t \n\tslab_flags_t flags;\n\tunsigned long min_partial;\n\tunsigned int size;\t \n\tunsigned int object_size; \n\tstruct reciprocal_value reciprocal_size;\n\tunsigned int offset;\t \n#ifdef CONFIG_SLUB_CPU_PARTIAL\n\t \n\tunsigned int cpu_partial;\n\t \n\tunsigned int cpu_partial_slabs;\n#endif\n\tstruct kmem_cache_order_objects oo;\n\n\t \n\tstruct kmem_cache_order_objects min;\n\tgfp_t allocflags;\t \n\tint refcount;\t\t \n\tvoid (*ctor)(void *);\n\tunsigned int inuse;\t\t \n\tunsigned int align;\t\t \n\tunsigned int red_left_pad;\t \n\tconst char *name;\t \n\tstruct list_head list;\t \n#ifdef CONFIG_SYSFS\n\tstruct kobject kobj;\t \n#endif\n#ifdef CONFIG_SLAB_FREELIST_HARDENED\n\tunsigned long random;\n#endif\n\n#ifdef CONFIG_NUMA\n\t \n\tunsigned int remote_node_defrag_ratio;\n#endif\n\n#ifdef CONFIG_SLAB_FREELIST_RANDOM\n\tunsigned int *random_seq;\n#endif\n\n#ifdef CONFIG_KASAN_GENERIC\n\tstruct kasan_cache kasan_info;\n#endif\n\n#ifdef CONFIG_HARDENED_USERCOPY\n\tunsigned int useroffset;\t \n\tunsigned int usersize;\t\t \n#endif\n\n\tstruct kmem_cache_node *node[MAX_NUMNODES];\n};\n\n#if defined(CONFIG_SYSFS) && !defined(CONFIG_SLUB_TINY)\n#define SLAB_SUPPORTS_SYSFS\nvoid sysfs_slab_unlink(struct kmem_cache *);\nvoid sysfs_slab_release(struct kmem_cache *);\n#else\nstatic inline void sysfs_slab_unlink(struct kmem_cache *s)\n{\n}\nstatic inline void sysfs_slab_release(struct kmem_cache *s)\n{\n}\n#endif\n\nvoid *fixup_red_left(struct kmem_cache *s, void *p);\n\nstatic inline void *nearest_obj(struct kmem_cache *cache, const struct slab *slab,\n\t\t\t\tvoid *x) {\n\tvoid *object = x - (x - slab_address(slab)) % cache->size;\n\tvoid *last_object = slab_address(slab) +\n\t\t(slab->objects - 1) * cache->size;\n\tvoid *result = (unlikely(object > last_object)) ? last_object : object;\n\n\tresult = fixup_red_left(cache, result);\n\treturn result;\n}\n\n \nstatic inline unsigned int __obj_to_index(const struct kmem_cache *cache,\n\t\t\t\t\t  void *addr, void *obj)\n{\n\treturn reciprocal_divide(kasan_reset_tag(obj) - addr,\n\t\t\t\t cache->reciprocal_size);\n}\n\nstatic inline unsigned int obj_to_index(const struct kmem_cache *cache,\n\t\t\t\t\tconst struct slab *slab, void *obj)\n{\n\tif (is_kfence_address(obj))\n\t\treturn 0;\n\treturn __obj_to_index(cache, slab_address(slab), obj);\n}\n\nstatic inline int objs_per_slab(const struct kmem_cache *cache,\n\t\t\t\t     const struct slab *slab)\n{\n\treturn slab->objects;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}