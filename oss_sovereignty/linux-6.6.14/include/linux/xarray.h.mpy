{
  "module_name": "xarray.h",
  "hash_id": "2e8089b2713939b9bc7f449564a271cddb1ca8b7c48ae8b99adef56b16623e74",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/xarray.h",
  "human_readable_source": " \n#ifndef _LINUX_XARRAY_H\n#define _LINUX_XARRAY_H\n \n\n#include <linux/bitmap.h>\n#include <linux/bug.h>\n#include <linux/compiler.h>\n#include <linux/gfp.h>\n#include <linux/kconfig.h>\n#include <linux/kernel.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/mm.h>\n#include <linux/spinlock.h>\n#include <linux/types.h>\n\n \n\n#define BITS_PER_XA_VALUE\t(BITS_PER_LONG - 1)\n\n \nstatic inline void *xa_mk_value(unsigned long v)\n{\n\tWARN_ON((long)v < 0);\n\treturn (void *)((v << 1) | 1);\n}\n\n \nstatic inline unsigned long xa_to_value(const void *entry)\n{\n\treturn (unsigned long)entry >> 1;\n}\n\n \nstatic inline bool xa_is_value(const void *entry)\n{\n\treturn (unsigned long)entry & 1;\n}\n\n \nstatic inline void *xa_tag_pointer(void *p, unsigned long tag)\n{\n\treturn (void *)((unsigned long)p | tag);\n}\n\n \nstatic inline void *xa_untag_pointer(void *entry)\n{\n\treturn (void *)((unsigned long)entry & ~3UL);\n}\n\n \nstatic inline unsigned int xa_pointer_tag(void *entry)\n{\n\treturn (unsigned long)entry & 3UL;\n}\n\n \nstatic inline void *xa_mk_internal(unsigned long v)\n{\n\treturn (void *)((v << 2) | 2);\n}\n\n \nstatic inline unsigned long xa_to_internal(const void *entry)\n{\n\treturn (unsigned long)entry >> 2;\n}\n\n \nstatic inline bool xa_is_internal(const void *entry)\n{\n\treturn ((unsigned long)entry & 3) == 2;\n}\n\n#define XA_ZERO_ENTRY\t\txa_mk_internal(257)\n\n \nstatic inline bool xa_is_zero(const void *entry)\n{\n\treturn unlikely(entry == XA_ZERO_ENTRY);\n}\n\n \nstatic inline bool xa_is_err(const void *entry)\n{\n\treturn unlikely(xa_is_internal(entry) &&\n\t\t\tentry >= xa_mk_internal(-MAX_ERRNO));\n}\n\n \nstatic inline int xa_err(void *entry)\n{\n\t \n\tif (xa_is_err(entry))\n\t\treturn (long)entry >> 2;\n\treturn 0;\n}\n\n \nstruct xa_limit {\n\tu32 max;\n\tu32 min;\n};\n\n#define XA_LIMIT(_min, _max) (struct xa_limit) { .min = _min, .max = _max }\n\n#define xa_limit_32b\tXA_LIMIT(0, UINT_MAX)\n#define xa_limit_31b\tXA_LIMIT(0, INT_MAX)\n#define xa_limit_16b\tXA_LIMIT(0, USHRT_MAX)\n\ntypedef unsigned __bitwise xa_mark_t;\n#define XA_MARK_0\t\t((__force xa_mark_t)0U)\n#define XA_MARK_1\t\t((__force xa_mark_t)1U)\n#define XA_MARK_2\t\t((__force xa_mark_t)2U)\n#define XA_PRESENT\t\t((__force xa_mark_t)8U)\n#define XA_MARK_MAX\t\tXA_MARK_2\n#define XA_FREE_MARK\t\tXA_MARK_0\n\nenum xa_lock_type {\n\tXA_LOCK_IRQ = 1,\n\tXA_LOCK_BH = 2,\n};\n\n \n#define XA_FLAGS_LOCK_IRQ\t((__force gfp_t)XA_LOCK_IRQ)\n#define XA_FLAGS_LOCK_BH\t((__force gfp_t)XA_LOCK_BH)\n#define XA_FLAGS_TRACK_FREE\t((__force gfp_t)4U)\n#define XA_FLAGS_ZERO_BUSY\t((__force gfp_t)8U)\n#define XA_FLAGS_ALLOC_WRAPPED\t((__force gfp_t)16U)\n#define XA_FLAGS_ACCOUNT\t((__force gfp_t)32U)\n#define XA_FLAGS_MARK(mark)\t((__force gfp_t)((1U << __GFP_BITS_SHIFT) << \\\n\t\t\t\t\t\t(__force unsigned)(mark)))\n\n \n#define XA_FLAGS_ALLOC\t(XA_FLAGS_TRACK_FREE | XA_FLAGS_MARK(XA_FREE_MARK))\n#define XA_FLAGS_ALLOC1\t(XA_FLAGS_TRACK_FREE | XA_FLAGS_ZERO_BUSY)\n\n \n \nstruct xarray {\n\tspinlock_t\txa_lock;\n \n\tgfp_t\t\txa_flags;\n\tvoid __rcu *\txa_head;\n};\n\n#define XARRAY_INIT(name, flags) {\t\t\t\t\\\n\t.xa_lock = __SPIN_LOCK_UNLOCKED(name.xa_lock),\t\t\\\n\t.xa_flags = flags,\t\t\t\t\t\\\n\t.xa_head = NULL,\t\t\t\t\t\\\n}\n\n \n#define DEFINE_XARRAY_FLAGS(name, flags)\t\t\t\t\\\n\tstruct xarray name = XARRAY_INIT(name, flags)\n\n \n#define DEFINE_XARRAY(name) DEFINE_XARRAY_FLAGS(name, 0)\n\n \n#define DEFINE_XARRAY_ALLOC(name) DEFINE_XARRAY_FLAGS(name, XA_FLAGS_ALLOC)\n\n \n#define DEFINE_XARRAY_ALLOC1(name) DEFINE_XARRAY_FLAGS(name, XA_FLAGS_ALLOC1)\n\nvoid *xa_load(struct xarray *, unsigned long index);\nvoid *xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);\nvoid *xa_erase(struct xarray *, unsigned long index);\nvoid *xa_store_range(struct xarray *, unsigned long first, unsigned long last,\n\t\t\tvoid *entry, gfp_t);\nbool xa_get_mark(struct xarray *, unsigned long index, xa_mark_t);\nvoid xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);\nvoid xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);\nvoid *xa_find(struct xarray *xa, unsigned long *index,\n\t\tunsigned long max, xa_mark_t) __attribute__((nonnull(2)));\nvoid *xa_find_after(struct xarray *xa, unsigned long *index,\n\t\tunsigned long max, xa_mark_t) __attribute__((nonnull(2)));\nunsigned int xa_extract(struct xarray *, void **dst, unsigned long start,\n\t\tunsigned long max, unsigned int n, xa_mark_t);\nvoid xa_destroy(struct xarray *);\n\n \nstatic inline void xa_init_flags(struct xarray *xa, gfp_t flags)\n{\n\tspin_lock_init(&xa->xa_lock);\n\txa->xa_flags = flags;\n\txa->xa_head = NULL;\n}\n\n \nstatic inline void xa_init(struct xarray *xa)\n{\n\txa_init_flags(xa, 0);\n}\n\n \nstatic inline bool xa_empty(const struct xarray *xa)\n{\n\treturn xa->xa_head == NULL;\n}\n\n \nstatic inline bool xa_marked(const struct xarray *xa, xa_mark_t mark)\n{\n\treturn xa->xa_flags & XA_FLAGS_MARK(mark);\n}\n\n \n#define xa_for_each_range(xa, index, entry, start, last)\t\t\\\n\tfor (index = start,\t\t\t\t\t\t\\\n\t     entry = xa_find(xa, &index, last, XA_PRESENT);\t\t\\\n\t     entry;\t\t\t\t\t\t\t\\\n\t     entry = xa_find_after(xa, &index, last, XA_PRESENT))\n\n \n#define xa_for_each_start(xa, index, entry, start) \\\n\txa_for_each_range(xa, index, entry, start, ULONG_MAX)\n\n \n#define xa_for_each(xa, index, entry) \\\n\txa_for_each_start(xa, index, entry, 0)\n\n \n#define xa_for_each_marked(xa, index, entry, filter) \\\n\tfor (index = 0, entry = xa_find(xa, &index, ULONG_MAX, filter); \\\n\t     entry; entry = xa_find_after(xa, &index, ULONG_MAX, filter))\n\n#define xa_trylock(xa)\t\tspin_trylock(&(xa)->xa_lock)\n#define xa_lock(xa)\t\tspin_lock(&(xa)->xa_lock)\n#define xa_unlock(xa)\t\tspin_unlock(&(xa)->xa_lock)\n#define xa_lock_bh(xa)\t\tspin_lock_bh(&(xa)->xa_lock)\n#define xa_unlock_bh(xa)\tspin_unlock_bh(&(xa)->xa_lock)\n#define xa_lock_irq(xa)\t\tspin_lock_irq(&(xa)->xa_lock)\n#define xa_unlock_irq(xa)\tspin_unlock_irq(&(xa)->xa_lock)\n#define xa_lock_irqsave(xa, flags) \\\n\t\t\t\tspin_lock_irqsave(&(xa)->xa_lock, flags)\n#define xa_unlock_irqrestore(xa, flags) \\\n\t\t\t\tspin_unlock_irqrestore(&(xa)->xa_lock, flags)\n#define xa_lock_nested(xa, subclass) \\\n\t\t\t\tspin_lock_nested(&(xa)->xa_lock, subclass)\n#define xa_lock_bh_nested(xa, subclass) \\\n\t\t\t\tspin_lock_bh_nested(&(xa)->xa_lock, subclass)\n#define xa_lock_irq_nested(xa, subclass) \\\n\t\t\t\tspin_lock_irq_nested(&(xa)->xa_lock, subclass)\n#define xa_lock_irqsave_nested(xa, flags, subclass) \\\n\t\tspin_lock_irqsave_nested(&(xa)->xa_lock, flags, subclass)\n\n \nvoid *__xa_erase(struct xarray *, unsigned long index);\nvoid *__xa_store(struct xarray *, unsigned long index, void *entry, gfp_t);\nvoid *__xa_cmpxchg(struct xarray *, unsigned long index, void *old,\n\t\tvoid *entry, gfp_t);\nint __must_check __xa_insert(struct xarray *, unsigned long index,\n\t\tvoid *entry, gfp_t);\nint __must_check __xa_alloc(struct xarray *, u32 *id, void *entry,\n\t\tstruct xa_limit, gfp_t);\nint __must_check __xa_alloc_cyclic(struct xarray *, u32 *id, void *entry,\n\t\tstruct xa_limit, u32 *next, gfp_t);\nvoid __xa_set_mark(struct xarray *, unsigned long index, xa_mark_t);\nvoid __xa_clear_mark(struct xarray *, unsigned long index, xa_mark_t);\n\n \nstatic inline void *xa_store_bh(struct xarray *xa, unsigned long index,\n\t\tvoid *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\tmight_alloc(gfp);\n\txa_lock_bh(xa);\n\tcurr = __xa_store(xa, index, entry, gfp);\n\txa_unlock_bh(xa);\n\n\treturn curr;\n}\n\n \nstatic inline void *xa_store_irq(struct xarray *xa, unsigned long index,\n\t\tvoid *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\tmight_alloc(gfp);\n\txa_lock_irq(xa);\n\tcurr = __xa_store(xa, index, entry, gfp);\n\txa_unlock_irq(xa);\n\n\treturn curr;\n}\n\n \nstatic inline void *xa_erase_bh(struct xarray *xa, unsigned long index)\n{\n\tvoid *entry;\n\n\txa_lock_bh(xa);\n\tentry = __xa_erase(xa, index);\n\txa_unlock_bh(xa);\n\n\treturn entry;\n}\n\n \nstatic inline void *xa_erase_irq(struct xarray *xa, unsigned long index)\n{\n\tvoid *entry;\n\n\txa_lock_irq(xa);\n\tentry = __xa_erase(xa, index);\n\txa_unlock_irq(xa);\n\n\treturn entry;\n}\n\n \nstatic inline void *xa_cmpxchg(struct xarray *xa, unsigned long index,\n\t\t\tvoid *old, void *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\tmight_alloc(gfp);\n\txa_lock(xa);\n\tcurr = __xa_cmpxchg(xa, index, old, entry, gfp);\n\txa_unlock(xa);\n\n\treturn curr;\n}\n\n \nstatic inline void *xa_cmpxchg_bh(struct xarray *xa, unsigned long index,\n\t\t\tvoid *old, void *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\tmight_alloc(gfp);\n\txa_lock_bh(xa);\n\tcurr = __xa_cmpxchg(xa, index, old, entry, gfp);\n\txa_unlock_bh(xa);\n\n\treturn curr;\n}\n\n \nstatic inline void *xa_cmpxchg_irq(struct xarray *xa, unsigned long index,\n\t\t\tvoid *old, void *entry, gfp_t gfp)\n{\n\tvoid *curr;\n\n\tmight_alloc(gfp);\n\txa_lock_irq(xa);\n\tcurr = __xa_cmpxchg(xa, index, old, entry, gfp);\n\txa_unlock_irq(xa);\n\n\treturn curr;\n}\n\n \nstatic inline int __must_check xa_insert(struct xarray *xa,\n\t\tunsigned long index, void *entry, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock(xa);\n\terr = __xa_insert(xa, index, entry, gfp);\n\txa_unlock(xa);\n\n\treturn err;\n}\n\n \nstatic inline int __must_check xa_insert_bh(struct xarray *xa,\n\t\tunsigned long index, void *entry, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_bh(xa);\n\terr = __xa_insert(xa, index, entry, gfp);\n\txa_unlock_bh(xa);\n\n\treturn err;\n}\n\n \nstatic inline int __must_check xa_insert_irq(struct xarray *xa,\n\t\tunsigned long index, void *entry, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_irq(xa);\n\terr = __xa_insert(xa, index, entry, gfp);\n\txa_unlock_irq(xa);\n\n\treturn err;\n}\n\n \nstatic inline __must_check int xa_alloc(struct xarray *xa, u32 *id,\n\t\tvoid *entry, struct xa_limit limit, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock(xa);\n\terr = __xa_alloc(xa, id, entry, limit, gfp);\n\txa_unlock(xa);\n\n\treturn err;\n}\n\n \nstatic inline int __must_check xa_alloc_bh(struct xarray *xa, u32 *id,\n\t\tvoid *entry, struct xa_limit limit, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_bh(xa);\n\terr = __xa_alloc(xa, id, entry, limit, gfp);\n\txa_unlock_bh(xa);\n\n\treturn err;\n}\n\n \nstatic inline int __must_check xa_alloc_irq(struct xarray *xa, u32 *id,\n\t\tvoid *entry, struct xa_limit limit, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_irq(xa);\n\terr = __xa_alloc(xa, id, entry, limit, gfp);\n\txa_unlock_irq(xa);\n\n\treturn err;\n}\n\n \nstatic inline int xa_alloc_cyclic(struct xarray *xa, u32 *id, void *entry,\n\t\tstruct xa_limit limit, u32 *next, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock(xa);\n\terr = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);\n\txa_unlock(xa);\n\n\treturn err;\n}\n\n \nstatic inline int xa_alloc_cyclic_bh(struct xarray *xa, u32 *id, void *entry,\n\t\tstruct xa_limit limit, u32 *next, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_bh(xa);\n\terr = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);\n\txa_unlock_bh(xa);\n\n\treturn err;\n}\n\n \nstatic inline int xa_alloc_cyclic_irq(struct xarray *xa, u32 *id, void *entry,\n\t\tstruct xa_limit limit, u32 *next, gfp_t gfp)\n{\n\tint err;\n\n\tmight_alloc(gfp);\n\txa_lock_irq(xa);\n\terr = __xa_alloc_cyclic(xa, id, entry, limit, next, gfp);\n\txa_unlock_irq(xa);\n\n\treturn err;\n}\n\n \nstatic inline __must_check\nint xa_reserve(struct xarray *xa, unsigned long index, gfp_t gfp)\n{\n\treturn xa_err(xa_cmpxchg(xa, index, NULL, XA_ZERO_ENTRY, gfp));\n}\n\n \nstatic inline __must_check\nint xa_reserve_bh(struct xarray *xa, unsigned long index, gfp_t gfp)\n{\n\treturn xa_err(xa_cmpxchg_bh(xa, index, NULL, XA_ZERO_ENTRY, gfp));\n}\n\n \nstatic inline __must_check\nint xa_reserve_irq(struct xarray *xa, unsigned long index, gfp_t gfp)\n{\n\treturn xa_err(xa_cmpxchg_irq(xa, index, NULL, XA_ZERO_ENTRY, gfp));\n}\n\n \nstatic inline void xa_release(struct xarray *xa, unsigned long index)\n{\n\txa_cmpxchg(xa, index, XA_ZERO_ENTRY, NULL, 0);\n}\n\n \n\n \n#ifndef XA_CHUNK_SHIFT\n#define XA_CHUNK_SHIFT\t\t(CONFIG_BASE_SMALL ? 4 : 6)\n#endif\n#define XA_CHUNK_SIZE\t\t(1UL << XA_CHUNK_SHIFT)\n#define XA_CHUNK_MASK\t\t(XA_CHUNK_SIZE - 1)\n#define XA_MAX_MARKS\t\t3\n#define XA_MARK_LONGS\t\tDIV_ROUND_UP(XA_CHUNK_SIZE, BITS_PER_LONG)\n\n \nstruct xa_node {\n\tunsigned char\tshift;\t\t \n\tunsigned char\toffset;\t\t \n\tunsigned char\tcount;\t\t \n\tunsigned char\tnr_values;\t \n\tstruct xa_node __rcu *parent;\t \n\tstruct xarray\t*array;\t\t \n\tunion {\n\t\tstruct list_head private_list;\t \n\t\tstruct rcu_head\trcu_head;\t \n\t};\n\tvoid __rcu\t*slots[XA_CHUNK_SIZE];\n\tunion {\n\t\tunsigned long\ttags[XA_MAX_MARKS][XA_MARK_LONGS];\n\t\tunsigned long\tmarks[XA_MAX_MARKS][XA_MARK_LONGS];\n\t};\n};\n\nvoid xa_dump(const struct xarray *);\nvoid xa_dump_node(const struct xa_node *);\n\n#ifdef XA_DEBUG\n#define XA_BUG_ON(xa, x) do {\t\t\t\t\t\\\n\t\tif (x) {\t\t\t\t\t\\\n\t\t\txa_dump(xa);\t\t\t\t\\\n\t\t\tBUG();\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t} while (0)\n#define XA_NODE_BUG_ON(node, x) do {\t\t\t\t\\\n\t\tif (x) {\t\t\t\t\t\\\n\t\t\tif (node) xa_dump_node(node);\t\t\\\n\t\t\tBUG();\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t} while (0)\n#else\n#define XA_BUG_ON(xa, x)\tdo { } while (0)\n#define XA_NODE_BUG_ON(node, x)\tdo { } while (0)\n#endif\n\n \nstatic inline void *xa_head(const struct xarray *xa)\n{\n\treturn rcu_dereference_check(xa->xa_head,\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline void *xa_head_locked(const struct xarray *xa)\n{\n\treturn rcu_dereference_protected(xa->xa_head,\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline void *xa_entry(const struct xarray *xa,\n\t\t\t\tconst struct xa_node *node, unsigned int offset)\n{\n\tXA_NODE_BUG_ON(node, offset >= XA_CHUNK_SIZE);\n\treturn rcu_dereference_check(node->slots[offset],\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline void *xa_entry_locked(const struct xarray *xa,\n\t\t\t\tconst struct xa_node *node, unsigned int offset)\n{\n\tXA_NODE_BUG_ON(node, offset >= XA_CHUNK_SIZE);\n\treturn rcu_dereference_protected(node->slots[offset],\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline struct xa_node *xa_parent(const struct xarray *xa,\n\t\t\t\t\tconst struct xa_node *node)\n{\n\treturn rcu_dereference_check(node->parent,\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline struct xa_node *xa_parent_locked(const struct xarray *xa,\n\t\t\t\t\tconst struct xa_node *node)\n{\n\treturn rcu_dereference_protected(node->parent,\n\t\t\t\t\t\tlockdep_is_held(&xa->xa_lock));\n}\n\n \nstatic inline void *xa_mk_node(const struct xa_node *node)\n{\n\treturn (void *)((unsigned long)node | 2);\n}\n\n \nstatic inline struct xa_node *xa_to_node(const void *entry)\n{\n\treturn (struct xa_node *)((unsigned long)entry - 2);\n}\n\n \nstatic inline bool xa_is_node(const void *entry)\n{\n\treturn xa_is_internal(entry) && (unsigned long)entry > 4096;\n}\n\n \nstatic inline void *xa_mk_sibling(unsigned int offset)\n{\n\treturn xa_mk_internal(offset);\n}\n\n \nstatic inline unsigned long xa_to_sibling(const void *entry)\n{\n\treturn xa_to_internal(entry);\n}\n\n \nstatic inline bool xa_is_sibling(const void *entry)\n{\n\treturn IS_ENABLED(CONFIG_XARRAY_MULTI) && xa_is_internal(entry) &&\n\t\t(entry < xa_mk_sibling(XA_CHUNK_SIZE - 1));\n}\n\n#define XA_RETRY_ENTRY\t\txa_mk_internal(256)\n\n \nstatic inline bool xa_is_retry(const void *entry)\n{\n\treturn unlikely(entry == XA_RETRY_ENTRY);\n}\n\n \nstatic inline bool xa_is_advanced(const void *entry)\n{\n\treturn xa_is_internal(entry) && (entry <= XA_RETRY_ENTRY);\n}\n\n \ntypedef void (*xa_update_node_t)(struct xa_node *node);\n\nvoid xa_delete_node(struct xa_node *, xa_update_node_t);\n\n \nstruct xa_state {\n\tstruct xarray *xa;\n\tunsigned long xa_index;\n\tunsigned char xa_shift;\n\tunsigned char xa_sibs;\n\tunsigned char xa_offset;\n\tunsigned char xa_pad;\t\t \n\tstruct xa_node *xa_node;\n\tstruct xa_node *xa_alloc;\n\txa_update_node_t xa_update;\n\tstruct list_lru *xa_lru;\n};\n\n \n#define XA_ERROR(errno) ((struct xa_node *)(((unsigned long)errno << 2) | 2UL))\n#define XAS_BOUNDS\t((struct xa_node *)1UL)\n#define XAS_RESTART\t((struct xa_node *)3UL)\n\n#define __XA_STATE(array, index, shift, sibs)  {\t\\\n\t.xa = array,\t\t\t\t\t\\\n\t.xa_index = index,\t\t\t\t\\\n\t.xa_shift = shift,\t\t\t\t\\\n\t.xa_sibs = sibs,\t\t\t\t\\\n\t.xa_offset = 0,\t\t\t\t\t\\\n\t.xa_pad = 0,\t\t\t\t\t\\\n\t.xa_node = XAS_RESTART,\t\t\t\t\\\n\t.xa_alloc = NULL,\t\t\t\t\\\n\t.xa_update = NULL,\t\t\t\t\\\n\t.xa_lru = NULL,\t\t\t\t\t\\\n}\n\n \n#define XA_STATE(name, array, index)\t\t\t\t\\\n\tstruct xa_state name = __XA_STATE(array, index, 0, 0)\n\n \n#define XA_STATE_ORDER(name, array, index, order)\t\t\\\n\tstruct xa_state name = __XA_STATE(array,\t\t\\\n\t\t\t(index >> order) << order,\t\t\\\n\t\t\torder - (order % XA_CHUNK_SHIFT),\t\\\n\t\t\t(1U << (order % XA_CHUNK_SHIFT)) - 1)\n\n#define xas_marked(xas, mark)\txa_marked((xas)->xa, (mark))\n#define xas_trylock(xas)\txa_trylock((xas)->xa)\n#define xas_lock(xas)\t\txa_lock((xas)->xa)\n#define xas_unlock(xas)\t\txa_unlock((xas)->xa)\n#define xas_lock_bh(xas)\txa_lock_bh((xas)->xa)\n#define xas_unlock_bh(xas)\txa_unlock_bh((xas)->xa)\n#define xas_lock_irq(xas)\txa_lock_irq((xas)->xa)\n#define xas_unlock_irq(xas)\txa_unlock_irq((xas)->xa)\n#define xas_lock_irqsave(xas, flags) \\\n\t\t\t\txa_lock_irqsave((xas)->xa, flags)\n#define xas_unlock_irqrestore(xas, flags) \\\n\t\t\t\txa_unlock_irqrestore((xas)->xa, flags)\n\n \nstatic inline int xas_error(const struct xa_state *xas)\n{\n\treturn xa_err(xas->xa_node);\n}\n\n \nstatic inline void xas_set_err(struct xa_state *xas, long err)\n{\n\txas->xa_node = XA_ERROR(err);\n}\n\n \nstatic inline bool xas_invalid(const struct xa_state *xas)\n{\n\treturn (unsigned long)xas->xa_node & 3;\n}\n\n \nstatic inline bool xas_valid(const struct xa_state *xas)\n{\n\treturn !xas_invalid(xas);\n}\n\n \nstatic inline bool xas_is_node(const struct xa_state *xas)\n{\n\treturn xas_valid(xas) && xas->xa_node;\n}\n\n \nstatic inline bool xas_not_node(struct xa_node *node)\n{\n\treturn ((unsigned long)node & 3) || !node;\n}\n\n \nstatic inline bool xas_frozen(struct xa_node *node)\n{\n\treturn (unsigned long)node & 2;\n}\n\n \nstatic inline bool xas_top(struct xa_node *node)\n{\n\treturn node <= XAS_RESTART;\n}\n\n \nstatic inline void xas_reset(struct xa_state *xas)\n{\n\txas->xa_node = XAS_RESTART;\n}\n\n \nstatic inline bool xas_retry(struct xa_state *xas, const void *entry)\n{\n\tif (xa_is_zero(entry))\n\t\treturn true;\n\tif (!xa_is_retry(entry))\n\t\treturn false;\n\txas_reset(xas);\n\treturn true;\n}\n\nvoid *xas_load(struct xa_state *);\nvoid *xas_store(struct xa_state *, void *entry);\nvoid *xas_find(struct xa_state *, unsigned long max);\nvoid *xas_find_conflict(struct xa_state *);\n\nbool xas_get_mark(const struct xa_state *, xa_mark_t);\nvoid xas_set_mark(const struct xa_state *, xa_mark_t);\nvoid xas_clear_mark(const struct xa_state *, xa_mark_t);\nvoid *xas_find_marked(struct xa_state *, unsigned long max, xa_mark_t);\nvoid xas_init_marks(const struct xa_state *);\n\nbool xas_nomem(struct xa_state *, gfp_t);\nvoid xas_destroy(struct xa_state *);\nvoid xas_pause(struct xa_state *);\n\nvoid xas_create_range(struct xa_state *);\n\n#ifdef CONFIG_XARRAY_MULTI\nint xa_get_order(struct xarray *, unsigned long index);\nvoid xas_split(struct xa_state *, void *entry, unsigned int order);\nvoid xas_split_alloc(struct xa_state *, void *entry, unsigned int order, gfp_t);\n#else\nstatic inline int xa_get_order(struct xarray *xa, unsigned long index)\n{\n\treturn 0;\n}\n\nstatic inline void xas_split(struct xa_state *xas, void *entry,\n\t\tunsigned int order)\n{\n\txas_store(xas, entry);\n}\n\nstatic inline void xas_split_alloc(struct xa_state *xas, void *entry,\n\t\tunsigned int order, gfp_t gfp)\n{\n}\n#endif\n\n \nstatic inline void *xas_reload(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tvoid *entry;\n\tchar offset;\n\n\tif (!node)\n\t\treturn xa_head(xas->xa);\n\tif (IS_ENABLED(CONFIG_XARRAY_MULTI)) {\n\t\toffset = (xas->xa_index >> node->shift) & XA_CHUNK_MASK;\n\t\tentry = xa_entry(xas->xa, node, offset);\n\t\tif (!xa_is_sibling(entry))\n\t\t\treturn entry;\n\t\toffset = xa_to_sibling(entry);\n\t} else {\n\t\toffset = xas->xa_offset;\n\t}\n\treturn xa_entry(xas->xa, node, offset);\n}\n\n \nstatic inline void xas_set(struct xa_state *xas, unsigned long index)\n{\n\txas->xa_index = index;\n\txas->xa_node = XAS_RESTART;\n}\n\n \nstatic inline void xas_advance(struct xa_state *xas, unsigned long index)\n{\n\tunsigned char shift = xas_is_node(xas) ? xas->xa_node->shift : 0;\n\n\txas->xa_index = index;\n\txas->xa_offset = (index >> shift) & XA_CHUNK_MASK;\n}\n\n \nstatic inline void xas_set_order(struct xa_state *xas, unsigned long index,\n\t\t\t\t\tunsigned int order)\n{\n#ifdef CONFIG_XARRAY_MULTI\n\txas->xa_index = order < BITS_PER_LONG ? (index >> order) << order : 0;\n\txas->xa_shift = order - (order % XA_CHUNK_SHIFT);\n\txas->xa_sibs = (1 << (order % XA_CHUNK_SHIFT)) - 1;\n\txas->xa_node = XAS_RESTART;\n#else\n\tBUG_ON(order > 0);\n\txas_set(xas, index);\n#endif\n}\n\n \nstatic inline void xas_set_update(struct xa_state *xas, xa_update_node_t update)\n{\n\txas->xa_update = update;\n}\n\nstatic inline void xas_set_lru(struct xa_state *xas, struct list_lru *lru)\n{\n\txas->xa_lru = lru;\n}\n\n \nstatic inline void *xas_next_entry(struct xa_state *xas, unsigned long max)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tvoid *entry;\n\n\tif (unlikely(xas_not_node(node) || node->shift ||\n\t\t\txas->xa_offset != (xas->xa_index & XA_CHUNK_MASK)))\n\t\treturn xas_find(xas, max);\n\n\tdo {\n\t\tif (unlikely(xas->xa_index >= max))\n\t\t\treturn xas_find(xas, max);\n\t\tif (unlikely(xas->xa_offset == XA_CHUNK_MASK))\n\t\t\treturn xas_find(xas, max);\n\t\tentry = xa_entry(xas->xa, node, xas->xa_offset + 1);\n\t\tif (unlikely(xa_is_internal(entry)))\n\t\t\treturn xas_find(xas, max);\n\t\txas->xa_offset++;\n\t\txas->xa_index++;\n\t} while (!entry);\n\n\treturn entry;\n}\n\n \nstatic inline unsigned int xas_find_chunk(struct xa_state *xas, bool advance,\n\t\txa_mark_t mark)\n{\n\tunsigned long *addr = xas->xa_node->marks[(__force unsigned)mark];\n\tunsigned int offset = xas->xa_offset;\n\n\tif (advance)\n\t\toffset++;\n\tif (XA_CHUNK_SIZE == BITS_PER_LONG) {\n\t\tif (offset < XA_CHUNK_SIZE) {\n\t\t\tunsigned long data = *addr & (~0UL << offset);\n\t\t\tif (data)\n\t\t\t\treturn __ffs(data);\n\t\t}\n\t\treturn XA_CHUNK_SIZE;\n\t}\n\n\treturn find_next_bit(addr, XA_CHUNK_SIZE, offset);\n}\n\n \nstatic inline void *xas_next_marked(struct xa_state *xas, unsigned long max,\n\t\t\t\t\t\t\t\txa_mark_t mark)\n{\n\tstruct xa_node *node = xas->xa_node;\n\tvoid *entry;\n\tunsigned int offset;\n\n\tif (unlikely(xas_not_node(node) || node->shift))\n\t\treturn xas_find_marked(xas, max, mark);\n\toffset = xas_find_chunk(xas, true, mark);\n\txas->xa_offset = offset;\n\txas->xa_index = (xas->xa_index & ~XA_CHUNK_MASK) + offset;\n\tif (xas->xa_index > max)\n\t\treturn NULL;\n\tif (offset == XA_CHUNK_SIZE)\n\t\treturn xas_find_marked(xas, max, mark);\n\tentry = xa_entry(xas->xa, node, offset);\n\tif (!entry)\n\t\treturn xas_find_marked(xas, max, mark);\n\treturn entry;\n}\n\n \nenum {\n\tXA_CHECK_SCHED = 4096,\n};\n\n \n#define xas_for_each(xas, entry, max) \\\n\tfor (entry = xas_find(xas, max); entry; \\\n\t     entry = xas_next_entry(xas, max))\n\n \n#define xas_for_each_marked(xas, entry, max, mark) \\\n\tfor (entry = xas_find_marked(xas, max, mark); entry; \\\n\t     entry = xas_next_marked(xas, max, mark))\n\n \n#define xas_for_each_conflict(xas, entry) \\\n\twhile ((entry = xas_find_conflict(xas)))\n\nvoid *__xas_next(struct xa_state *);\nvoid *__xas_prev(struct xa_state *);\n\n \nstatic inline void *xas_prev(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\n\tif (unlikely(xas_not_node(node) || node->shift ||\n\t\t\t\txas->xa_offset == 0))\n\t\treturn __xas_prev(xas);\n\n\txas->xa_index--;\n\txas->xa_offset--;\n\treturn xa_entry(xas->xa, node, xas->xa_offset);\n}\n\n \nstatic inline void *xas_next(struct xa_state *xas)\n{\n\tstruct xa_node *node = xas->xa_node;\n\n\tif (unlikely(xas_not_node(node) || node->shift ||\n\t\t\t\txas->xa_offset == XA_CHUNK_MASK))\n\t\treturn __xas_next(xas);\n\n\txas->xa_index++;\n\txas->xa_offset++;\n\treturn xa_entry(xas->xa, node, xas->xa_offset);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}