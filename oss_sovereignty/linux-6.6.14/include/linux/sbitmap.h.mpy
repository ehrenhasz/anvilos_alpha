{
  "module_name": "sbitmap.h",
  "hash_id": "ac38a136d97838cf487fed5fcf36461eaba5194facc4c1dba8b4ad688d46138d",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/sbitmap.h",
  "human_readable_source": " \n \n\n#ifndef __LINUX_SCALE_BITMAP_H\n#define __LINUX_SCALE_BITMAP_H\n\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include <linux/cache.h>\n#include <linux/list.h>\n#include <linux/log2.h>\n#include <linux/minmax.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/smp.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n\nstruct seq_file;\n\n \nstruct sbitmap_word {\n\t \n\tunsigned long word;\n\n\t \n\tunsigned long cleared ____cacheline_aligned_in_smp;\n} ____cacheline_aligned_in_smp;\n\n \nstruct sbitmap {\n\t \n\tunsigned int depth;\n\n\t \n\tunsigned int shift;\n\n\t \n\tunsigned int map_nr;\n\n\t \n\tbool round_robin;\n\n\t \n\tstruct sbitmap_word *map;\n\n\t \n\tunsigned int __percpu *alloc_hint;\n};\n\n#define SBQ_WAIT_QUEUES 8\n#define SBQ_WAKE_BATCH 8\n\n \nstruct sbq_wait_state {\n\t \n\twait_queue_head_t wait;\n} ____cacheline_aligned_in_smp;\n\n \nstruct sbitmap_queue {\n\t \n\tstruct sbitmap sb;\n\n\t \n\tunsigned int wake_batch;\n\n\t \n\tatomic_t wake_index;\n\n\t \n\tstruct sbq_wait_state *ws;\n\n\t \n\tatomic_t ws_active;\n\n\t \n\tunsigned int min_shallow_depth;\n\n\t \n\tatomic_t completion_cnt;\n\n\t \n\tatomic_t wakeup_cnt;\n};\n\n \nint sbitmap_init_node(struct sbitmap *sb, unsigned int depth, int shift,\n\t\t      gfp_t flags, int node, bool round_robin, bool alloc_hint);\n\n \nstatic inline unsigned int __map_depth(const struct sbitmap *sb, int index)\n{\n\tif (index == sb->map_nr - 1)\n\t\treturn sb->depth - (index << sb->shift);\n\treturn 1U << sb->shift;\n}\n\n \nstatic inline void sbitmap_free(struct sbitmap *sb)\n{\n\tfree_percpu(sb->alloc_hint);\n\tkvfree(sb->map);\n\tsb->map = NULL;\n}\n\n \nvoid sbitmap_resize(struct sbitmap *sb, unsigned int depth);\n\n \nint sbitmap_get(struct sbitmap *sb);\n\n \nint sbitmap_get_shallow(struct sbitmap *sb, unsigned long shallow_depth);\n\n \nbool sbitmap_any_bit_set(const struct sbitmap *sb);\n\n#define SB_NR_TO_INDEX(sb, bitnr) ((bitnr) >> (sb)->shift)\n#define SB_NR_TO_BIT(sb, bitnr) ((bitnr) & ((1U << (sb)->shift) - 1U))\n\ntypedef bool (*sb_for_each_fn)(struct sbitmap *, unsigned int, void *);\n\n \nstatic inline void __sbitmap_for_each_set(struct sbitmap *sb,\n\t\t\t\t\t  unsigned int start,\n\t\t\t\t\t  sb_for_each_fn fn, void *data)\n{\n\tunsigned int index;\n\tunsigned int nr;\n\tunsigned int scanned = 0;\n\n\tif (start >= sb->depth)\n\t\tstart = 0;\n\tindex = SB_NR_TO_INDEX(sb, start);\n\tnr = SB_NR_TO_BIT(sb, start);\n\n\twhile (scanned < sb->depth) {\n\t\tunsigned long word;\n\t\tunsigned int depth = min_t(unsigned int,\n\t\t\t\t\t   __map_depth(sb, index) - nr,\n\t\t\t\t\t   sb->depth - scanned);\n\n\t\tscanned += depth;\n\t\tword = sb->map[index].word & ~sb->map[index].cleared;\n\t\tif (!word)\n\t\t\tgoto next;\n\n\t\t \n\t\tdepth += nr;\n\t\twhile (1) {\n\t\t\tnr = find_next_bit(&word, depth, nr);\n\t\t\tif (nr >= depth)\n\t\t\t\tbreak;\n\t\t\tif (!fn(sb, (index << sb->shift) + nr, data))\n\t\t\t\treturn;\n\n\t\t\tnr++;\n\t\t}\nnext:\n\t\tnr = 0;\n\t\tif (++index >= sb->map_nr)\n\t\t\tindex = 0;\n\t}\n}\n\n \nstatic inline void sbitmap_for_each_set(struct sbitmap *sb, sb_for_each_fn fn,\n\t\t\t\t\tvoid *data)\n{\n\t__sbitmap_for_each_set(sb, 0, fn, data);\n}\n\nstatic inline unsigned long *__sbitmap_word(struct sbitmap *sb,\n\t\t\t\t\t    unsigned int bitnr)\n{\n\treturn &sb->map[SB_NR_TO_INDEX(sb, bitnr)].word;\n}\n\n \n\nstatic inline void sbitmap_set_bit(struct sbitmap *sb, unsigned int bitnr)\n{\n\tset_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));\n}\n\nstatic inline void sbitmap_clear_bit(struct sbitmap *sb, unsigned int bitnr)\n{\n\tclear_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));\n}\n\n \nstatic inline void sbitmap_deferred_clear_bit(struct sbitmap *sb, unsigned int bitnr)\n{\n\tunsigned long *addr = &sb->map[SB_NR_TO_INDEX(sb, bitnr)].cleared;\n\n\tset_bit(SB_NR_TO_BIT(sb, bitnr), addr);\n}\n\n \nstatic inline void sbitmap_put(struct sbitmap *sb, unsigned int bitnr)\n{\n\tsbitmap_deferred_clear_bit(sb, bitnr);\n\n\tif (likely(sb->alloc_hint && !sb->round_robin && bitnr < sb->depth))\n\t\t*raw_cpu_ptr(sb->alloc_hint) = bitnr;\n}\n\nstatic inline int sbitmap_test_bit(struct sbitmap *sb, unsigned int bitnr)\n{\n\treturn test_bit(SB_NR_TO_BIT(sb, bitnr), __sbitmap_word(sb, bitnr));\n}\n\nstatic inline int sbitmap_calculate_shift(unsigned int depth)\n{\n\tint\tshift = ilog2(BITS_PER_LONG);\n\n\t \n\tif (depth >= 4) {\n\t\twhile ((4U << shift) > depth)\n\t\t\tshift--;\n\t}\n\n\treturn shift;\n}\n\n \nvoid sbitmap_show(struct sbitmap *sb, struct seq_file *m);\n\n\n \nunsigned int sbitmap_weight(const struct sbitmap *sb);\n\n \nvoid sbitmap_bitmap_show(struct sbitmap *sb, struct seq_file *m);\n\n \nint sbitmap_queue_init_node(struct sbitmap_queue *sbq, unsigned int depth,\n\t\t\t    int shift, bool round_robin, gfp_t flags, int node);\n\n \nstatic inline void sbitmap_queue_free(struct sbitmap_queue *sbq)\n{\n\tkfree(sbq->ws);\n\tsbitmap_free(&sbq->sb);\n}\n\n \nvoid sbitmap_queue_recalculate_wake_batch(struct sbitmap_queue *sbq,\n\t\t\t\t\t    unsigned int users);\n\n \nvoid sbitmap_queue_resize(struct sbitmap_queue *sbq, unsigned int depth);\n\n \nint __sbitmap_queue_get(struct sbitmap_queue *sbq);\n\n \nunsigned long __sbitmap_queue_get_batch(struct sbitmap_queue *sbq, int nr_tags,\n\t\t\t\t\tunsigned int *offset);\n\n \nint sbitmap_queue_get_shallow(struct sbitmap_queue *sbq,\n\t\t\t      unsigned int shallow_depth);\n\n \nstatic inline int sbitmap_queue_get(struct sbitmap_queue *sbq,\n\t\t\t\t    unsigned int *cpu)\n{\n\tint nr;\n\n\t*cpu = get_cpu();\n\tnr = __sbitmap_queue_get(sbq);\n\tput_cpu();\n\treturn nr;\n}\n\n \nvoid sbitmap_queue_min_shallow_depth(struct sbitmap_queue *sbq,\n\t\t\t\t     unsigned int min_shallow_depth);\n\n \nvoid sbitmap_queue_clear(struct sbitmap_queue *sbq, unsigned int nr,\n\t\t\t unsigned int cpu);\n\n \nvoid sbitmap_queue_clear_batch(struct sbitmap_queue *sbq, int offset,\n\t\t\t\tint *tags, int nr_tags);\n\nstatic inline int sbq_index_inc(int index)\n{\n\treturn (index + 1) & (SBQ_WAIT_QUEUES - 1);\n}\n\nstatic inline void sbq_index_atomic_inc(atomic_t *index)\n{\n\tint old = atomic_read(index);\n\tint new = sbq_index_inc(old);\n\tatomic_cmpxchg(index, old, new);\n}\n\n \nstatic inline struct sbq_wait_state *sbq_wait_ptr(struct sbitmap_queue *sbq,\n\t\t\t\t\t\t  atomic_t *wait_index)\n{\n\tstruct sbq_wait_state *ws;\n\n\tws = &sbq->ws[atomic_read(wait_index)];\n\tsbq_index_atomic_inc(wait_index);\n\treturn ws;\n}\n\n \nvoid sbitmap_queue_wake_all(struct sbitmap_queue *sbq);\n\n \nvoid sbitmap_queue_wake_up(struct sbitmap_queue *sbq, int nr);\n\n \nvoid sbitmap_queue_show(struct sbitmap_queue *sbq, struct seq_file *m);\n\nstruct sbq_wait {\n\tstruct sbitmap_queue *sbq;\t \n\tstruct wait_queue_entry wait;\n};\n\n#define DEFINE_SBQ_WAIT(name)\t\t\t\t\t\t\t\\\n\tstruct sbq_wait name = {\t\t\t\t\t\t\\\n\t\t.sbq = NULL,\t\t\t\t\t\t\t\\\n\t\t.wait = {\t\t\t\t\t\t\t\\\n\t\t\t.private\t= current,\t\t\t\t\\\n\t\t\t.func\t\t= autoremove_wake_function,\t\t\\\n\t\t\t.entry\t\t= LIST_HEAD_INIT((name).wait.entry),\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\t}\n\n \nvoid sbitmap_prepare_to_wait(struct sbitmap_queue *sbq,\n\t\t\t\tstruct sbq_wait_state *ws,\n\t\t\t\tstruct sbq_wait *sbq_wait, int state);\n\n \nvoid sbitmap_finish_wait(struct sbitmap_queue *sbq, struct sbq_wait_state *ws,\n\t\t\t\tstruct sbq_wait *sbq_wait);\n\n \nvoid sbitmap_add_wait_queue(struct sbitmap_queue *sbq,\n\t\t\t    struct sbq_wait_state *ws,\n\t\t\t    struct sbq_wait *sbq_wait);\n\n \nvoid sbitmap_del_wait_queue(struct sbq_wait *sbq_wait);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}