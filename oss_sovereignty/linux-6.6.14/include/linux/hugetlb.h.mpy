{
  "module_name": "hugetlb.h",
  "hash_id": "91f6fa7ea797f8b90eb58d6092e48f3d8db4e9951e2dba12f9b7757743ce0c57",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/hugetlb.h",
  "human_readable_source": " \n#ifndef _LINUX_HUGETLB_H\n#define _LINUX_HUGETLB_H\n\n#include <linux/mm.h>\n#include <linux/mm_types.h>\n#include <linux/mmdebug.h>\n#include <linux/fs.h>\n#include <linux/hugetlb_inline.h>\n#include <linux/cgroup.h>\n#include <linux/page_ref.h>\n#include <linux/list.h>\n#include <linux/kref.h>\n#include <linux/pgtable.h>\n#include <linux/gfp.h>\n#include <linux/userfaultfd_k.h>\n\nstruct ctl_table;\nstruct user_struct;\nstruct mmu_gather;\nstruct node;\n\n#ifndef CONFIG_ARCH_HAS_HUGEPD\ntypedef struct { unsigned long pd; } hugepd_t;\n#define is_hugepd(hugepd) (0)\n#define __hugepd(x) ((hugepd_t) { (x) })\n#endif\n\nvoid free_huge_folio(struct folio *folio);\n\n#ifdef CONFIG_HUGETLB_PAGE\n\n#include <linux/mempolicy.h>\n#include <linux/shm.h>\n#include <asm/tlbflush.h>\n\n \n#define __NR_USED_SUBPAGE 3\n\nstruct hugepage_subpool {\n\tspinlock_t lock;\n\tlong count;\n\tlong max_hpages;\t \n\tlong used_hpages;\t \n\t\t\t\t \n\tstruct hstate *hstate;\n\tlong min_hpages;\t \n\tlong rsv_hpages;\t \n\t\t\t\t \n};\n\nstruct resv_map {\n\tstruct kref refs;\n\tspinlock_t lock;\n\tstruct list_head regions;\n\tlong adds_in_progress;\n\tstruct list_head region_cache;\n\tlong region_cache_count;\n\tstruct rw_semaphore rw_sema;\n#ifdef CONFIG_CGROUP_HUGETLB\n\t \n\tstruct page_counter *reservation_counter;\n\tunsigned long pages_per_hpage;\n\tstruct cgroup_subsys_state *css;\n#endif\n};\n\n \nstruct file_region {\n\tstruct list_head link;\n\tlong from;\n\tlong to;\n#ifdef CONFIG_CGROUP_HUGETLB\n\t \n\tstruct page_counter *reservation_counter;\n\tstruct cgroup_subsys_state *css;\n#endif\n};\n\nstruct hugetlb_vma_lock {\n\tstruct kref refs;\n\tstruct rw_semaphore rw_sema;\n\tstruct vm_area_struct *vma;\n};\n\nextern struct resv_map *resv_map_alloc(void);\nvoid resv_map_release(struct kref *ref);\n\nextern spinlock_t hugetlb_lock;\nextern int hugetlb_max_hstate __read_mostly;\n#define for_each_hstate(h) \\\n\tfor ((h) = hstates; (h) < &hstates[hugetlb_max_hstate]; (h)++)\n\nstruct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,\n\t\t\t\t\t\tlong min_hpages);\nvoid hugepage_put_subpool(struct hugepage_subpool *spool);\n\nvoid hugetlb_dup_vma_private(struct vm_area_struct *vma);\nvoid clear_vma_resv_huge_pages(struct vm_area_struct *vma);\nint move_hugetlb_page_tables(struct vm_area_struct *vma,\n\t\t\t     struct vm_area_struct *new_vma,\n\t\t\t     unsigned long old_addr, unsigned long new_addr,\n\t\t\t     unsigned long len);\nint copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *,\n\t\t\t    struct vm_area_struct *, struct vm_area_struct *);\nstruct page *hugetlb_follow_page_mask(struct vm_area_struct *vma,\n\t\t\t\t      unsigned long address, unsigned int flags,\n\t\t\t\t      unsigned int *page_mask);\nvoid unmap_hugepage_range(struct vm_area_struct *,\n\t\t\t  unsigned long, unsigned long, struct page *,\n\t\t\t  zap_flags_t);\nvoid __unmap_hugepage_range(struct mmu_gather *tlb,\n\t\t\t  struct vm_area_struct *vma,\n\t\t\t  unsigned long start, unsigned long end,\n\t\t\t  struct page *ref_page, zap_flags_t zap_flags);\nvoid hugetlb_report_meminfo(struct seq_file *);\nint hugetlb_report_node_meminfo(char *buf, int len, int nid);\nvoid hugetlb_show_meminfo_node(int nid);\nunsigned long hugetlb_total_pages(void);\nvm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long address, unsigned int flags);\n#ifdef CONFIG_USERFAULTFD\nint hugetlb_mfill_atomic_pte(pte_t *dst_pte,\n\t\t\t     struct vm_area_struct *dst_vma,\n\t\t\t     unsigned long dst_addr,\n\t\t\t     unsigned long src_addr,\n\t\t\t     uffd_flags_t flags,\n\t\t\t     struct folio **foliop);\n#endif  \nbool hugetlb_reserve_pages(struct inode *inode, long from, long to,\n\t\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\t\tvm_flags_t vm_flags);\nlong hugetlb_unreserve_pages(struct inode *inode, long start, long end,\n\t\t\t\t\t\tlong freed);\nbool isolate_hugetlb(struct folio *folio, struct list_head *list);\nint get_hwpoison_hugetlb_folio(struct folio *folio, bool *hugetlb, bool unpoison);\nint get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\tbool *migratable_cleared);\nvoid folio_putback_active_hugetlb(struct folio *folio);\nvoid move_hugetlb_state(struct folio *old_folio, struct folio *new_folio, int reason);\nvoid hugetlb_fix_reserve_counts(struct inode *inode);\nextern struct mutex *hugetlb_fault_mutex_table;\nu32 hugetlb_fault_mutex_hash(struct address_space *mapping, pgoff_t idx);\n\npte_t *huge_pmd_share(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t      unsigned long addr, pud_t *pud);\n\nstruct address_space *hugetlb_page_mapping_lock_write(struct page *hpage);\n\nextern int sysctl_hugetlb_shm_group;\nextern struct list_head huge_boot_pages;\n\n \n\n#ifndef CONFIG_HIGHPTE\n \nstatic inline pte_t *pte_offset_huge(pmd_t *pmd, unsigned long address)\n{\n\treturn pte_offset_kernel(pmd, address);\n}\nstatic inline pte_t *pte_alloc_huge(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t\t    unsigned long address)\n{\n\treturn pte_alloc(mm, pmd) ? NULL : pte_offset_huge(pmd, address);\n}\n#endif\n\npte_t *huge_pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\tunsigned long addr, unsigned long sz);\n \npte_t *huge_pte_offset(struct mm_struct *mm,\n\t\t       unsigned long addr, unsigned long sz);\nunsigned long hugetlb_mask_last_page(struct hstate *h);\nint huge_pmd_unshare(struct mm_struct *mm, struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, pte_t *ptep);\nvoid adjust_range_if_pmd_sharing_possible(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end);\n\nextern void __hugetlb_zap_begin(struct vm_area_struct *vma,\n\t\t\t\tunsigned long *begin, unsigned long *end);\nextern void __hugetlb_zap_end(struct vm_area_struct *vma,\n\t\t\t      struct zap_details *details);\n\nstatic inline void hugetlb_zap_begin(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long *start, unsigned long *end)\n{\n\tif (is_vm_hugetlb_page(vma))\n\t\t__hugetlb_zap_begin(vma, start, end);\n}\n\nstatic inline void hugetlb_zap_end(struct vm_area_struct *vma,\n\t\t\t\t   struct zap_details *details)\n{\n\tif (is_vm_hugetlb_page(vma))\n\t\t__hugetlb_zap_end(vma, details);\n}\n\nvoid hugetlb_vma_lock_read(struct vm_area_struct *vma);\nvoid hugetlb_vma_unlock_read(struct vm_area_struct *vma);\nvoid hugetlb_vma_lock_write(struct vm_area_struct *vma);\nvoid hugetlb_vma_unlock_write(struct vm_area_struct *vma);\nint hugetlb_vma_trylock_write(struct vm_area_struct *vma);\nvoid hugetlb_vma_assert_locked(struct vm_area_struct *vma);\nvoid hugetlb_vma_lock_release(struct kref *kref);\n\nint pmd_huge(pmd_t pmd);\nint pud_huge(pud_t pud);\nlong hugetlb_change_protection(struct vm_area_struct *vma,\n\t\tunsigned long address, unsigned long end, pgprot_t newprot,\n\t\tunsigned long cp_flags);\n\nbool is_hugetlb_entry_migration(pte_t pte);\nvoid hugetlb_unshare_all_pmds(struct vm_area_struct *vma);\n\n#else  \n\nstatic inline void hugetlb_dup_vma_private(struct vm_area_struct *vma)\n{\n}\n\nstatic inline void clear_vma_resv_huge_pages(struct vm_area_struct *vma)\n{\n}\n\nstatic inline unsigned long hugetlb_total_pages(void)\n{\n\treturn 0;\n}\n\nstatic inline struct address_space *hugetlb_page_mapping_lock_write(\n\t\t\t\t\t\t\tstruct page *hpage)\n{\n\treturn NULL;\n}\n\nstatic inline int huge_pmd_unshare(struct mm_struct *mm,\n\t\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr, pte_t *ptep)\n{\n\treturn 0;\n}\n\nstatic inline void adjust_range_if_pmd_sharing_possible(\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n}\n\nstatic inline void hugetlb_zap_begin(\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tunsigned long *start, unsigned long *end)\n{\n}\n\nstatic inline void hugetlb_zap_end(\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tstruct zap_details *details)\n{\n}\n\nstatic inline struct page *hugetlb_follow_page_mask(\n    struct vm_area_struct *vma, unsigned long address, unsigned int flags,\n    unsigned int *page_mask)\n{\n\tBUILD_BUG();  \n}\n\nstatic inline int copy_hugetlb_page_range(struct mm_struct *dst,\n\t\t\t\t\t  struct mm_struct *src,\n\t\t\t\t\t  struct vm_area_struct *dst_vma,\n\t\t\t\t\t  struct vm_area_struct *src_vma)\n{\n\tBUG();\n\treturn 0;\n}\n\nstatic inline int move_hugetlb_page_tables(struct vm_area_struct *vma,\n\t\t\t\t\t   struct vm_area_struct *new_vma,\n\t\t\t\t\t   unsigned long old_addr,\n\t\t\t\t\t   unsigned long new_addr,\n\t\t\t\t\t   unsigned long len)\n{\n\tBUG();\n\treturn 0;\n}\n\nstatic inline void hugetlb_report_meminfo(struct seq_file *m)\n{\n}\n\nstatic inline int hugetlb_report_node_meminfo(char *buf, int len, int nid)\n{\n\treturn 0;\n}\n\nstatic inline void hugetlb_show_meminfo_node(int nid)\n{\n}\n\nstatic inline int prepare_hugepage_range(struct file *file,\n\t\t\t\tunsigned long addr, unsigned long len)\n{\n\treturn -EINVAL;\n}\n\nstatic inline void hugetlb_vma_lock_read(struct vm_area_struct *vma)\n{\n}\n\nstatic inline void hugetlb_vma_unlock_read(struct vm_area_struct *vma)\n{\n}\n\nstatic inline void hugetlb_vma_lock_write(struct vm_area_struct *vma)\n{\n}\n\nstatic inline void hugetlb_vma_unlock_write(struct vm_area_struct *vma)\n{\n}\n\nstatic inline int hugetlb_vma_trylock_write(struct vm_area_struct *vma)\n{\n\treturn 1;\n}\n\nstatic inline void hugetlb_vma_assert_locked(struct vm_area_struct *vma)\n{\n}\n\nstatic inline int pmd_huge(pmd_t pmd)\n{\n\treturn 0;\n}\n\nstatic inline int pud_huge(pud_t pud)\n{\n\treturn 0;\n}\n\nstatic inline int is_hugepage_only_range(struct mm_struct *mm,\n\t\t\t\t\tunsigned long addr, unsigned long len)\n{\n\treturn 0;\n}\n\nstatic inline void hugetlb_free_pgd_range(struct mmu_gather *tlb,\n\t\t\t\tunsigned long addr, unsigned long end,\n\t\t\t\tunsigned long floor, unsigned long ceiling)\n{\n\tBUG();\n}\n\n#ifdef CONFIG_USERFAULTFD\nstatic inline int hugetlb_mfill_atomic_pte(pte_t *dst_pte,\n\t\t\t\t\t   struct vm_area_struct *dst_vma,\n\t\t\t\t\t   unsigned long dst_addr,\n\t\t\t\t\t   unsigned long src_addr,\n\t\t\t\t\t   uffd_flags_t flags,\n\t\t\t\t\t   struct folio **foliop)\n{\n\tBUG();\n\treturn 0;\n}\n#endif  \n\nstatic inline pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t\tunsigned long sz)\n{\n\treturn NULL;\n}\n\nstatic inline bool isolate_hugetlb(struct folio *folio, struct list_head *list)\n{\n\treturn false;\n}\n\nstatic inline int get_hwpoison_hugetlb_folio(struct folio *folio, bool *hugetlb, bool unpoison)\n{\n\treturn 0;\n}\n\nstatic inline int get_huge_page_for_hwpoison(unsigned long pfn, int flags,\n\t\t\t\t\tbool *migratable_cleared)\n{\n\treturn 0;\n}\n\nstatic inline void folio_putback_active_hugetlb(struct folio *folio)\n{\n}\n\nstatic inline void move_hugetlb_state(struct folio *old_folio,\n\t\t\t\t\tstruct folio *new_folio, int reason)\n{\n}\n\nstatic inline long hugetlb_change_protection(\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tunsigned long end, pgprot_t newprot,\n\t\t\tunsigned long cp_flags)\n{\n\treturn 0;\n}\n\nstatic inline void __unmap_hugepage_range(struct mmu_gather *tlb,\n\t\t\tstruct vm_area_struct *vma, unsigned long start,\n\t\t\tunsigned long end, struct page *ref_page,\n\t\t\tzap_flags_t zap_flags)\n{\n\tBUG();\n}\n\nstatic inline vm_fault_t hugetlb_fault(struct mm_struct *mm,\n\t\t\tstruct vm_area_struct *vma, unsigned long address,\n\t\t\tunsigned int flags)\n{\n\tBUG();\n\treturn 0;\n}\n\nstatic inline void hugetlb_unshare_all_pmds(struct vm_area_struct *vma) { }\n\n#endif  \n \n#ifndef pgd_huge\n#define pgd_huge(x)\t0\n#endif\n#ifndef p4d_huge\n#define p4d_huge(x)\t0\n#endif\n\n#ifndef pgd_write\nstatic inline int pgd_write(pgd_t pgd)\n{\n\tBUG();\n\treturn 0;\n}\n#endif\n\n#define HUGETLB_ANON_FILE \"anon_hugepage\"\n\nenum {\n\t \n\tHUGETLB_SHMFS_INODE     = 1,\n\t \n\tHUGETLB_ANONHUGE_INODE  = 2,\n};\n\n#ifdef CONFIG_HUGETLBFS\nstruct hugetlbfs_sb_info {\n\tlong\tmax_inodes;    \n\tlong\tfree_inodes;   \n\tspinlock_t\tstat_lock;\n\tstruct hstate *hstate;\n\tstruct hugepage_subpool *spool;\n\tkuid_t\tuid;\n\tkgid_t\tgid;\n\tumode_t mode;\n};\n\nstatic inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)\n{\n\treturn sb->s_fs_info;\n}\n\nstruct hugetlbfs_inode_info {\n\tstruct shared_policy policy;\n\tstruct inode vfs_inode;\n\tunsigned int seals;\n};\n\nstatic inline struct hugetlbfs_inode_info *HUGETLBFS_I(struct inode *inode)\n{\n\treturn container_of(inode, struct hugetlbfs_inode_info, vfs_inode);\n}\n\nextern const struct file_operations hugetlbfs_file_operations;\nextern const struct vm_operations_struct hugetlb_vm_ops;\nstruct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,\n\t\t\t\tint creat_flags, int page_size_log);\n\nstatic inline bool is_file_hugepages(struct file *file)\n{\n\tif (file->f_op == &hugetlbfs_file_operations)\n\t\treturn true;\n\n\treturn is_file_shm_hugepages(file);\n}\n\nstatic inline struct hstate *hstate_inode(struct inode *i)\n{\n\treturn HUGETLBFS_SB(i->i_sb)->hstate;\n}\n#else  \n\n#define is_file_hugepages(file)\t\t\tfalse\nstatic inline struct file *\nhugetlb_file_setup(const char *name, size_t size, vm_flags_t acctflag,\n\t\tint creat_flags, int page_size_log)\n{\n\treturn ERR_PTR(-ENOSYS);\n}\n\nstatic inline struct hstate *hstate_inode(struct inode *i)\n{\n\treturn NULL;\n}\n#endif  \n\n#ifdef HAVE_ARCH_HUGETLB_UNMAPPED_AREA\nunsigned long hugetlb_get_unmapped_area(struct file *file, unsigned long addr,\n\t\t\t\t\tunsigned long len, unsigned long pgoff,\n\t\t\t\t\tunsigned long flags);\n#endif  \n\nunsigned long\ngeneric_hugetlb_get_unmapped_area(struct file *file, unsigned long addr,\n\t\t\t\t  unsigned long len, unsigned long pgoff,\n\t\t\t\t  unsigned long flags);\n\n \nenum hugetlb_page_flags {\n\tHPG_restore_reserve = 0,\n\tHPG_migratable,\n\tHPG_temporary,\n\tHPG_freed,\n\tHPG_vmemmap_optimized,\n\tHPG_raw_hwp_unreliable,\n\t__NR_HPAGEFLAGS,\n};\n\n \n#ifdef CONFIG_HUGETLB_PAGE\n#define TESTHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic __always_inline\t\t\t\t\t\t\\\nbool folio_test_hugetlb_##flname(struct folio *folio)\t\t\\\n\t{\tvoid *private = &folio->private;\t\t\\\n\t\treturn test_bit(HPG_##flname, private);\t\t\\\n\t}\t\t\t\t\t\t\t\\\nstatic inline int HPage##uname(struct page *page)\t\t\\\n\t{ return test_bit(HPG_##flname, &(page->private)); }\n\n#define SETHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic __always_inline\t\t\t\t\t\t\\\nvoid folio_set_hugetlb_##flname(struct folio *folio)\t\t\\\n\t{\tvoid *private = &folio->private;\t\t\\\n\t\tset_bit(HPG_##flname, private);\t\t\t\\\n\t}\t\t\t\t\t\t\t\\\nstatic inline void SetHPage##uname(struct page *page)\t\t\\\n\t{ set_bit(HPG_##flname, &(page->private)); }\n\n#define CLEARHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic __always_inline\t\t\t\t\t\t\\\nvoid folio_clear_hugetlb_##flname(struct folio *folio)\t\t\\\n\t{\tvoid *private = &folio->private;\t\t\\\n\t\tclear_bit(HPG_##flname, private);\t\t\\\n\t}\t\t\t\t\t\t\t\\\nstatic inline void ClearHPage##uname(struct page *page)\t\t\\\n\t{ clear_bit(HPG_##flname, &(page->private)); }\n#else\n#define TESTHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic inline bool\t\t\t\t\t\t\\\nfolio_test_hugetlb_##flname(struct folio *folio)\t\t\\\n\t{ return 0; }\t\t\t\t\t\t\\\nstatic inline int HPage##uname(struct page *page)\t\t\\\n\t{ return 0; }\n\n#define SETHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\\\nfolio_set_hugetlb_##flname(struct folio *folio) \t\t\\\n\t{ }\t\t\t\t\t\t\t\\\nstatic inline void SetHPage##uname(struct page *page)\t\t\\\n\t{ }\n\n#define CLEARHPAGEFLAG(uname, flname)\t\t\t\t\\\nstatic inline void\t\t\t\t\t\t\\\nfolio_clear_hugetlb_##flname(struct folio *folio)\t\t\\\n\t{ }\t\t\t\t\t\t\t\\\nstatic inline void ClearHPage##uname(struct page *page)\t\t\\\n\t{ }\n#endif\n\n#define HPAGEFLAG(uname, flname)\t\t\t\t\\\n\tTESTHPAGEFLAG(uname, flname)\t\t\t\t\\\n\tSETHPAGEFLAG(uname, flname)\t\t\t\t\\\n\tCLEARHPAGEFLAG(uname, flname)\t\t\t\t\\\n\n \nHPAGEFLAG(RestoreReserve, restore_reserve)\nHPAGEFLAG(Migratable, migratable)\nHPAGEFLAG(Temporary, temporary)\nHPAGEFLAG(Freed, freed)\nHPAGEFLAG(VmemmapOptimized, vmemmap_optimized)\nHPAGEFLAG(RawHwpUnreliable, raw_hwp_unreliable)\n\n#ifdef CONFIG_HUGETLB_PAGE\n\n#define HSTATE_NAME_LEN 32\n \nstruct hstate {\n\tstruct mutex resize_lock;\n\tint next_nid_to_alloc;\n\tint next_nid_to_free;\n\tunsigned int order;\n\tunsigned int demote_order;\n\tunsigned long mask;\n\tunsigned long max_huge_pages;\n\tunsigned long nr_huge_pages;\n\tunsigned long free_huge_pages;\n\tunsigned long resv_huge_pages;\n\tunsigned long surplus_huge_pages;\n\tunsigned long nr_overcommit_huge_pages;\n\tstruct list_head hugepage_activelist;\n\tstruct list_head hugepage_freelists[MAX_NUMNODES];\n\tunsigned int max_huge_pages_node[MAX_NUMNODES];\n\tunsigned int nr_huge_pages_node[MAX_NUMNODES];\n\tunsigned int free_huge_pages_node[MAX_NUMNODES];\n\tunsigned int surplus_huge_pages_node[MAX_NUMNODES];\n#ifdef CONFIG_CGROUP_HUGETLB\n\t \n\tstruct cftype cgroup_files_dfl[8];\n\tstruct cftype cgroup_files_legacy[10];\n#endif\n\tchar name[HSTATE_NAME_LEN];\n};\n\nstruct huge_bootmem_page {\n\tstruct list_head list;\n\tstruct hstate *hstate;\n};\n\nint isolate_or_dissolve_huge_page(struct page *page, struct list_head *list);\nstruct folio *alloc_hugetlb_folio(struct vm_area_struct *vma,\n\t\t\t\tunsigned long addr, int avoid_reserve);\nstruct folio *alloc_hugetlb_folio_nodemask(struct hstate *h, int preferred_nid,\n\t\t\t\tnodemask_t *nmask, gfp_t gfp_mask);\nstruct folio *alloc_hugetlb_folio_vma(struct hstate *h, struct vm_area_struct *vma,\n\t\t\t\tunsigned long address);\nint hugetlb_add_to_page_cache(struct folio *folio, struct address_space *mapping,\n\t\t\tpgoff_t idx);\nvoid restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,\n\t\t\t\tunsigned long address, struct folio *folio);\n\n \nint __init __alloc_bootmem_huge_page(struct hstate *h, int nid);\nint __init alloc_bootmem_huge_page(struct hstate *h, int nid);\nbool __init hugetlb_node_alloc_supported(void);\n\nvoid __init hugetlb_add_hstate(unsigned order);\nbool __init arch_hugetlb_valid_size(unsigned long size);\nstruct hstate *size_to_hstate(unsigned long size);\n\n#ifndef HUGE_MAX_HSTATE\n#define HUGE_MAX_HSTATE 1\n#endif\n\nextern struct hstate hstates[HUGE_MAX_HSTATE];\nextern unsigned int default_hstate_idx;\n\n#define default_hstate (hstates[default_hstate_idx])\n\nstatic inline struct hugepage_subpool *hugetlb_folio_subpool(struct folio *folio)\n{\n\treturn folio->_hugetlb_subpool;\n}\n\nstatic inline void hugetlb_set_folio_subpool(struct folio *folio,\n\t\t\t\t\tstruct hugepage_subpool *subpool)\n{\n\tfolio->_hugetlb_subpool = subpool;\n}\n\nstatic inline struct hstate *hstate_file(struct file *f)\n{\n\treturn hstate_inode(file_inode(f));\n}\n\nstatic inline struct hstate *hstate_sizelog(int page_size_log)\n{\n\tif (!page_size_log)\n\t\treturn &default_hstate;\n\n\tif (page_size_log < BITS_PER_LONG)\n\t\treturn size_to_hstate(1UL << page_size_log);\n\n\treturn NULL;\n}\n\nstatic inline struct hstate *hstate_vma(struct vm_area_struct *vma)\n{\n\treturn hstate_file(vma->vm_file);\n}\n\nstatic inline unsigned long huge_page_size(const struct hstate *h)\n{\n\treturn (unsigned long)PAGE_SIZE << h->order;\n}\n\nextern unsigned long vma_kernel_pagesize(struct vm_area_struct *vma);\n\nextern unsigned long vma_mmu_pagesize(struct vm_area_struct *vma);\n\nstatic inline unsigned long huge_page_mask(struct hstate *h)\n{\n\treturn h->mask;\n}\n\nstatic inline unsigned int huge_page_order(struct hstate *h)\n{\n\treturn h->order;\n}\n\nstatic inline unsigned huge_page_shift(struct hstate *h)\n{\n\treturn h->order + PAGE_SHIFT;\n}\n\nstatic inline bool hstate_is_gigantic(struct hstate *h)\n{\n\treturn huge_page_order(h) > MAX_ORDER;\n}\n\nstatic inline unsigned int pages_per_huge_page(const struct hstate *h)\n{\n\treturn 1 << h->order;\n}\n\nstatic inline unsigned int blocks_per_huge_page(struct hstate *h)\n{\n\treturn huge_page_size(h) / 512;\n}\n\n#include <asm/hugetlb.h>\n\n#ifndef is_hugepage_only_range\nstatic inline int is_hugepage_only_range(struct mm_struct *mm,\n\t\t\t\t\tunsigned long addr, unsigned long len)\n{\n\treturn 0;\n}\n#define is_hugepage_only_range is_hugepage_only_range\n#endif\n\n#ifndef arch_clear_hugepage_flags\nstatic inline void arch_clear_hugepage_flags(struct page *page) { }\n#define arch_clear_hugepage_flags arch_clear_hugepage_flags\n#endif\n\n#ifndef arch_make_huge_pte\nstatic inline pte_t arch_make_huge_pte(pte_t entry, unsigned int shift,\n\t\t\t\t       vm_flags_t flags)\n{\n\treturn pte_mkhuge(entry);\n}\n#endif\n\nstatic inline struct hstate *folio_hstate(struct folio *folio)\n{\n\tVM_BUG_ON_FOLIO(!folio_test_hugetlb(folio), folio);\n\treturn size_to_hstate(folio_size(folio));\n}\n\nstatic inline unsigned hstate_index_to_shift(unsigned index)\n{\n\treturn hstates[index].order + PAGE_SHIFT;\n}\n\nstatic inline int hstate_index(struct hstate *h)\n{\n\treturn h - hstates;\n}\n\nextern int dissolve_free_huge_page(struct page *page);\nextern int dissolve_free_huge_pages(unsigned long start_pfn,\n\t\t\t\t    unsigned long end_pfn);\n\n#ifdef CONFIG_MEMORY_FAILURE\nextern void folio_clear_hugetlb_hwpoison(struct folio *folio);\n#else\nstatic inline void folio_clear_hugetlb_hwpoison(struct folio *folio)\n{\n}\n#endif\n\n#ifdef CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION\n#ifndef arch_hugetlb_migration_supported\nstatic inline bool arch_hugetlb_migration_supported(struct hstate *h)\n{\n\tif ((huge_page_shift(h) == PMD_SHIFT) ||\n\t\t(huge_page_shift(h) == PUD_SHIFT) ||\n\t\t\t(huge_page_shift(h) == PGDIR_SHIFT))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n#endif\n#else\nstatic inline bool arch_hugetlb_migration_supported(struct hstate *h)\n{\n\treturn false;\n}\n#endif\n\nstatic inline bool hugepage_migration_supported(struct hstate *h)\n{\n\treturn arch_hugetlb_migration_supported(h);\n}\n\n \nstatic inline bool hugepage_movable_supported(struct hstate *h)\n{\n\tif (!hugepage_migration_supported(h))\n\t\treturn false;\n\n\tif (hstate_is_gigantic(h))\n\t\treturn false;\n\treturn true;\n}\n\n \nstatic inline gfp_t htlb_alloc_mask(struct hstate *h)\n{\n\tif (hugepage_movable_supported(h))\n\t\treturn GFP_HIGHUSER_MOVABLE;\n\telse\n\t\treturn GFP_HIGHUSER;\n}\n\nstatic inline gfp_t htlb_modify_alloc_mask(struct hstate *h, gfp_t gfp_mask)\n{\n\tgfp_t modified_mask = htlb_alloc_mask(h);\n\n\t \n\tmodified_mask |= (gfp_mask & __GFP_THISNODE);\n\n\tmodified_mask |= (gfp_mask & __GFP_NOWARN);\n\n\treturn modified_mask;\n}\n\nstatic inline spinlock_t *huge_pte_lockptr(struct hstate *h,\n\t\t\t\t\t   struct mm_struct *mm, pte_t *pte)\n{\n\tif (huge_page_size(h) == PMD_SIZE)\n\t\treturn pmd_lockptr(mm, (pmd_t *) pte);\n\tVM_BUG_ON(huge_page_size(h) == PAGE_SIZE);\n\treturn &mm->page_table_lock;\n}\n\n#ifndef hugepages_supported\n \n#define hugepages_supported() (HPAGE_SHIFT != 0)\n#endif\n\nvoid hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm);\n\nstatic inline void hugetlb_count_init(struct mm_struct *mm)\n{\n\tatomic_long_set(&mm->hugetlb_usage, 0);\n}\n\nstatic inline void hugetlb_count_add(long l, struct mm_struct *mm)\n{\n\tatomic_long_add(l, &mm->hugetlb_usage);\n}\n\nstatic inline void hugetlb_count_sub(long l, struct mm_struct *mm)\n{\n\tatomic_long_sub(l, &mm->hugetlb_usage);\n}\n\n#ifndef huge_ptep_modify_prot_start\n#define huge_ptep_modify_prot_start huge_ptep_modify_prot_start\nstatic inline pte_t huge_ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long addr, pte_t *ptep)\n{\n\treturn huge_ptep_get_and_clear(vma->vm_mm, addr, ptep);\n}\n#endif\n\n#ifndef huge_ptep_modify_prot_commit\n#define huge_ptep_modify_prot_commit huge_ptep_modify_prot_commit\nstatic inline void huge_ptep_modify_prot_commit(struct vm_area_struct *vma,\n\t\t\t\t\t\tunsigned long addr, pte_t *ptep,\n\t\t\t\t\t\tpte_t old_pte, pte_t pte)\n{\n\tunsigned long psize = huge_page_size(hstate_vma(vma));\n\n\tset_huge_pte_at(vma->vm_mm, addr, ptep, pte, psize);\n}\n#endif\n\n#ifdef CONFIG_NUMA\nvoid hugetlb_register_node(struct node *node);\nvoid hugetlb_unregister_node(struct node *node);\n#endif\n\n \nbool is_raw_hwpoison_page_in_hugepage(struct page *page);\n\n#else\t \nstruct hstate {};\n\nstatic inline struct hugepage_subpool *hugetlb_folio_subpool(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic inline int isolate_or_dissolve_huge_page(struct page *page,\n\t\t\t\t\t\tstruct list_head *list)\n{\n\treturn -ENOMEM;\n}\n\nstatic inline struct folio *alloc_hugetlb_folio(struct vm_area_struct *vma,\n\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t   int avoid_reserve)\n{\n\treturn NULL;\n}\n\nstatic inline struct folio *\nalloc_hugetlb_folio_nodemask(struct hstate *h, int preferred_nid,\n\t\t\tnodemask_t *nmask, gfp_t gfp_mask)\n{\n\treturn NULL;\n}\n\nstatic inline struct folio *alloc_hugetlb_folio_vma(struct hstate *h,\n\t\t\t\t\t       struct vm_area_struct *vma,\n\t\t\t\t\t       unsigned long address)\n{\n\treturn NULL;\n}\n\nstatic inline int __alloc_bootmem_huge_page(struct hstate *h)\n{\n\treturn 0;\n}\n\nstatic inline struct hstate *hstate_file(struct file *f)\n{\n\treturn NULL;\n}\n\nstatic inline struct hstate *hstate_sizelog(int page_size_log)\n{\n\treturn NULL;\n}\n\nstatic inline struct hstate *hstate_vma(struct vm_area_struct *vma)\n{\n\treturn NULL;\n}\n\nstatic inline struct hstate *folio_hstate(struct folio *folio)\n{\n\treturn NULL;\n}\n\nstatic inline struct hstate *size_to_hstate(unsigned long size)\n{\n\treturn NULL;\n}\n\nstatic inline unsigned long huge_page_size(struct hstate *h)\n{\n\treturn PAGE_SIZE;\n}\n\nstatic inline unsigned long huge_page_mask(struct hstate *h)\n{\n\treturn PAGE_MASK;\n}\n\nstatic inline unsigned long vma_kernel_pagesize(struct vm_area_struct *vma)\n{\n\treturn PAGE_SIZE;\n}\n\nstatic inline unsigned long vma_mmu_pagesize(struct vm_area_struct *vma)\n{\n\treturn PAGE_SIZE;\n}\n\nstatic inline unsigned int huge_page_order(struct hstate *h)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int huge_page_shift(struct hstate *h)\n{\n\treturn PAGE_SHIFT;\n}\n\nstatic inline bool hstate_is_gigantic(struct hstate *h)\n{\n\treturn false;\n}\n\nstatic inline unsigned int pages_per_huge_page(struct hstate *h)\n{\n\treturn 1;\n}\n\nstatic inline unsigned hstate_index_to_shift(unsigned index)\n{\n\treturn 0;\n}\n\nstatic inline int hstate_index(struct hstate *h)\n{\n\treturn 0;\n}\n\nstatic inline int dissolve_free_huge_page(struct page *page)\n{\n\treturn 0;\n}\n\nstatic inline int dissolve_free_huge_pages(unsigned long start_pfn,\n\t\t\t\t\t   unsigned long end_pfn)\n{\n\treturn 0;\n}\n\nstatic inline bool hugepage_migration_supported(struct hstate *h)\n{\n\treturn false;\n}\n\nstatic inline bool hugepage_movable_supported(struct hstate *h)\n{\n\treturn false;\n}\n\nstatic inline gfp_t htlb_alloc_mask(struct hstate *h)\n{\n\treturn 0;\n}\n\nstatic inline gfp_t htlb_modify_alloc_mask(struct hstate *h, gfp_t gfp_mask)\n{\n\treturn 0;\n}\n\nstatic inline spinlock_t *huge_pte_lockptr(struct hstate *h,\n\t\t\t\t\t   struct mm_struct *mm, pte_t *pte)\n{\n\treturn &mm->page_table_lock;\n}\n\nstatic inline void hugetlb_count_init(struct mm_struct *mm)\n{\n}\n\nstatic inline void hugetlb_report_usage(struct seq_file *f, struct mm_struct *m)\n{\n}\n\nstatic inline void hugetlb_count_sub(long l, struct mm_struct *mm)\n{\n}\n\nstatic inline pte_t huge_ptep_clear_flush(struct vm_area_struct *vma,\n\t\t\t\t\t  unsigned long addr, pte_t *ptep)\n{\n#ifdef CONFIG_MMU\n\treturn ptep_get(ptep);\n#else\n\treturn *ptep;\n#endif\n}\n\nstatic inline void set_huge_pte_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t   pte_t *ptep, pte_t pte, unsigned long sz)\n{\n}\n\nstatic inline void hugetlb_register_node(struct node *node)\n{\n}\n\nstatic inline void hugetlb_unregister_node(struct node *node)\n{\n}\n#endif\t \n\nstatic inline spinlock_t *huge_pte_lock(struct hstate *h,\n\t\t\t\t\tstruct mm_struct *mm, pte_t *pte)\n{\n\tspinlock_t *ptl;\n\n\tptl = huge_pte_lockptr(h, mm, pte);\n\tspin_lock(ptl);\n\treturn ptl;\n}\n\n#if defined(CONFIG_HUGETLB_PAGE) && defined(CONFIG_CMA)\nextern void __init hugetlb_cma_reserve(int order);\n#else\nstatic inline __init void hugetlb_cma_reserve(int order)\n{\n}\n#endif\n\n#ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE\nstatic inline bool hugetlb_pmd_shared(pte_t *pte)\n{\n\treturn page_count(virt_to_page(pte)) > 1;\n}\n#else\nstatic inline bool hugetlb_pmd_shared(pte_t *pte)\n{\n\treturn false;\n}\n#endif\n\nbool want_pmd_share(struct vm_area_struct *vma, unsigned long addr);\n\n#ifndef __HAVE_ARCH_FLUSH_HUGETLB_TLB_RANGE\n \n#define flush_hugetlb_tlb_range(vma, addr, end)\tflush_tlb_range(vma, addr, end)\n#endif\n\nstatic inline bool __vma_shareable_lock(struct vm_area_struct *vma)\n{\n\treturn (vma->vm_flags & VM_MAYSHARE) && vma->vm_private_data;\n}\n\nbool __vma_private_lock(struct vm_area_struct *vma);\n\n \nstatic inline pte_t *\nhugetlb_walk(struct vm_area_struct *vma, unsigned long addr, unsigned long sz)\n{\n#if defined(CONFIG_HUGETLB_PAGE) && \\\n\tdefined(CONFIG_ARCH_WANT_HUGE_PMD_SHARE) && defined(CONFIG_LOCKDEP)\n\tstruct hugetlb_vma_lock *vma_lock = vma->vm_private_data;\n\n\t \n\tif (__vma_shareable_lock(vma))\n\t\tWARN_ON_ONCE(!lockdep_is_held(&vma_lock->rw_sema) &&\n\t\t\t     !lockdep_is_held(\n\t\t\t\t &vma->vm_file->f_mapping->i_mmap_rwsem));\n#endif\n\treturn huge_pte_offset(vma->vm_mm, addr, sz);\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}