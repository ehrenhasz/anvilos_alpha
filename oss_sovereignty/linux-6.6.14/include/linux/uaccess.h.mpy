{
  "module_name": "uaccess.h",
  "hash_id": "d9975a508430295f01a8000e9bbcfe94980c557386b4218f8dc9c65e52287a72",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/uaccess.h",
  "human_readable_source": " \n#ifndef __LINUX_UACCESS_H__\n#define __LINUX_UACCESS_H__\n\n#include <linux/fault-inject-usercopy.h>\n#include <linux/instrumented.h>\n#include <linux/minmax.h>\n#include <linux/sched.h>\n#include <linux/thread_info.h>\n\n#include <asm/uaccess.h>\n\n \n#ifndef untagged_addr\n#define untagged_addr(addr) (addr)\n#endif\n\n#ifndef untagged_addr_remote\n#define untagged_addr_remote(mm, addr)\t({\t\t\\\n\tmmap_assert_locked(mm);\t\t\t\t\\\n\tuntagged_addr(addr);\t\t\t\t\\\n})\n#endif\n\n \n\nstatic __always_inline __must_check unsigned long\n__copy_from_user_inatomic(void *to, const void __user *from, unsigned long n)\n{\n\tunsigned long res;\n\n\tinstrument_copy_from_user_before(to, from, n);\n\tcheck_object_size(to, n, false);\n\tres = raw_copy_from_user(to, from, n);\n\tinstrument_copy_from_user_after(to, from, n, res);\n\treturn res;\n}\n\nstatic __always_inline __must_check unsigned long\n__copy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tunsigned long res;\n\n\tmight_fault();\n\tinstrument_copy_from_user_before(to, from, n);\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tcheck_object_size(to, n, false);\n\tres = raw_copy_from_user(to, from, n);\n\tinstrument_copy_from_user_after(to, from, n, res);\n\treturn res;\n}\n\n \nstatic __always_inline __must_check unsigned long\n__copy_to_user_inatomic(void __user *to, const void *from, unsigned long n)\n{\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tinstrument_copy_to_user(to, from, n);\n\tcheck_object_size(from, n, true);\n\treturn raw_copy_to_user(to, from, n);\n}\n\nstatic __always_inline __must_check unsigned long\n__copy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\tmight_fault();\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tinstrument_copy_to_user(to, from, n);\n\tcheck_object_size(from, n, true);\n\treturn raw_copy_to_user(to, from, n);\n}\n\n#ifdef INLINE_COPY_FROM_USER\nstatic inline __must_check unsigned long\n_copy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tunsigned long res = n;\n\tmight_fault();\n\tif (!should_fail_usercopy() && likely(access_ok(from, n))) {\n\t\tinstrument_copy_from_user_before(to, from, n);\n\t\tres = raw_copy_from_user(to, from, n);\n\t\tinstrument_copy_from_user_after(to, from, n, res);\n\t}\n\tif (unlikely(res))\n\t\tmemset(to + (n - res), 0, res);\n\treturn res;\n}\n#else\nextern __must_check unsigned long\n_copy_from_user(void *, const void __user *, unsigned long);\n#endif\n\n#ifdef INLINE_COPY_TO_USER\nstatic inline __must_check unsigned long\n_copy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\tmight_fault();\n\tif (should_fail_usercopy())\n\t\treturn n;\n\tif (access_ok(to, n)) {\n\t\tinstrument_copy_to_user(to, from, n);\n\t\tn = raw_copy_to_user(to, from, n);\n\t}\n\treturn n;\n}\n#else\nextern __must_check unsigned long\n_copy_to_user(void __user *, const void *, unsigned long);\n#endif\n\nstatic __always_inline unsigned long __must_check\ncopy_from_user(void *to, const void __user *from, unsigned long n)\n{\n\tif (check_copy_size(to, n, false))\n\t\tn = _copy_from_user(to, from, n);\n\treturn n;\n}\n\nstatic __always_inline unsigned long __must_check\ncopy_to_user(void __user *to, const void *from, unsigned long n)\n{\n\tif (check_copy_size(from, n, true))\n\t\tn = _copy_to_user(to, from, n);\n\treturn n;\n}\n\n#ifndef copy_mc_to_kernel\n \nstatic inline unsigned long __must_check\ncopy_mc_to_kernel(void *dst, const void *src, size_t cnt)\n{\n\tmemcpy(dst, src, cnt);\n\treturn 0;\n}\n#endif\n\nstatic __always_inline void pagefault_disabled_inc(void)\n{\n\tcurrent->pagefault_disabled++;\n}\n\nstatic __always_inline void pagefault_disabled_dec(void)\n{\n\tcurrent->pagefault_disabled--;\n}\n\n \nstatic inline void pagefault_disable(void)\n{\n\tpagefault_disabled_inc();\n\t \n\tbarrier();\n}\n\nstatic inline void pagefault_enable(void)\n{\n\t \n\tbarrier();\n\tpagefault_disabled_dec();\n}\n\n \nstatic inline bool pagefault_disabled(void)\n{\n\treturn current->pagefault_disabled != 0;\n}\n\n \n#define faulthandler_disabled() (pagefault_disabled() || in_atomic())\n\n#ifndef CONFIG_ARCH_HAS_SUBPAGE_FAULTS\n\n \nstatic inline size_t probe_subpage_writeable(char __user *uaddr, size_t size)\n{\n\treturn 0;\n}\n\n#endif  \n\n#ifndef ARCH_HAS_NOCACHE_UACCESS\n\nstatic inline __must_check unsigned long\n__copy_from_user_inatomic_nocache(void *to, const void __user *from,\n\t\t\t\t  unsigned long n)\n{\n\treturn __copy_from_user_inatomic(to, from, n);\n}\n\n#endif\t\t \n\nextern __must_check int check_zeroed_user(const void __user *from, size_t size);\n\n \nstatic __always_inline __must_check int\ncopy_struct_from_user(void *dst, size_t ksize, const void __user *src,\n\t\t      size_t usize)\n{\n\tsize_t size = min(ksize, usize);\n\tsize_t rest = max(ksize, usize) - size;\n\n\t \n\tif (WARN_ON_ONCE(ksize > __builtin_object_size(dst, 1)))\n\t\treturn -E2BIG;\n\n\t \n\tif (usize < ksize) {\n\t\tmemset(dst + size, 0, rest);\n\t} else if (usize > ksize) {\n\t\tint ret = check_zeroed_user(src + size, rest);\n\t\tif (ret <= 0)\n\t\t\treturn ret ?: -E2BIG;\n\t}\n\t \n\tif (copy_from_user(dst, src, size))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nbool copy_from_kernel_nofault_allowed(const void *unsafe_src, size_t size);\n\nlong copy_from_kernel_nofault(void *dst, const void *src, size_t size);\nlong notrace copy_to_kernel_nofault(void *dst, const void *src, size_t size);\n\nlong copy_from_user_nofault(void *dst, const void __user *src, size_t size);\nlong notrace copy_to_user_nofault(void __user *dst, const void *src,\n\t\tsize_t size);\n\nlong strncpy_from_kernel_nofault(char *dst, const void *unsafe_addr,\n\t\tlong count);\n\nlong strncpy_from_user_nofault(char *dst, const void __user *unsafe_addr,\n\t\tlong count);\nlong strnlen_user_nofault(const void __user *unsafe_addr, long count);\n\n#ifndef __get_kernel_nofault\n#define __get_kernel_nofault(dst, src, type, label)\t\\\ndo {\t\t\t\t\t\t\t\\\n\ttype __user *p = (type __force __user *)(src);\t\\\n\ttype data;\t\t\t\t\t\\\n\tif (__get_user(data, p))\t\t\t\\\n\t\tgoto label;\t\t\t\t\\\n\t*(type *)dst = data;\t\t\t\t\\\n} while (0)\n\n#define __put_kernel_nofault(dst, src, type, label)\t\\\ndo {\t\t\t\t\t\t\t\\\n\ttype __user *p = (type __force __user *)(dst);\t\\\n\ttype data = *(type *)src;\t\t\t\\\n\tif (__put_user(data, p))\t\t\t\\\n\t\tgoto label;\t\t\t\t\\\n} while (0)\n#endif\n\n \n#define get_kernel_nofault(val, ptr) ({\t\t\t\t\\\n\tconst typeof(val) *__gk_ptr = (ptr);\t\t\t\\\n\tcopy_from_kernel_nofault(&(val), __gk_ptr, sizeof(val));\\\n})\n\n#ifndef user_access_begin\n#define user_access_begin(ptr,len) access_ok(ptr, len)\n#define user_access_end() do { } while (0)\n#define unsafe_op_wrap(op, err) do { if (unlikely(op)) goto err; } while (0)\n#define unsafe_get_user(x,p,e) unsafe_op_wrap(__get_user(x,p),e)\n#define unsafe_put_user(x,p,e) unsafe_op_wrap(__put_user(x,p),e)\n#define unsafe_copy_to_user(d,s,l,e) unsafe_op_wrap(__copy_to_user(d,s,l),e)\n#define unsafe_copy_from_user(d,s,l,e) unsafe_op_wrap(__copy_from_user(d,s,l),e)\nstatic inline unsigned long user_access_save(void) { return 0UL; }\nstatic inline void user_access_restore(unsigned long flags) { }\n#endif\n#ifndef user_write_access_begin\n#define user_write_access_begin user_access_begin\n#define user_write_access_end user_access_end\n#endif\n#ifndef user_read_access_begin\n#define user_read_access_begin user_access_begin\n#define user_read_access_end user_access_end\n#endif\n\n#ifdef CONFIG_HARDENED_USERCOPY\nvoid __noreturn usercopy_abort(const char *name, const char *detail,\n\t\t\t       bool to_user, unsigned long offset,\n\t\t\t       unsigned long len);\n#endif\n\n#endif\t\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}