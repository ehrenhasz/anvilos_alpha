{
  "module_name": "workqueue.h",
  "hash_id": "c83e49df1617d2e40d2be71ae6e81eae52f3c58e713dcb78aafc0bc877100f2a",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/workqueue.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_WORKQUEUE_H\n#define _LINUX_WORKQUEUE_H\n\n#include <linux/timer.h>\n#include <linux/linkage.h>\n#include <linux/bitops.h>\n#include <linux/lockdep.h>\n#include <linux/threads.h>\n#include <linux/atomic.h>\n#include <linux/cpumask.h>\n#include <linux/rcupdate.h>\n\nstruct workqueue_struct;\n\nstruct work_struct;\ntypedef void (*work_func_t)(struct work_struct *work);\nvoid delayed_work_timer_fn(struct timer_list *t);\n\n \n#define work_data_bits(work) ((unsigned long *)(&(work)->data))\n\nenum {\n\tWORK_STRUCT_PENDING_BIT\t= 0,\t \n\tWORK_STRUCT_INACTIVE_BIT= 1,\t \n\tWORK_STRUCT_PWQ_BIT\t= 2,\t \n\tWORK_STRUCT_LINKED_BIT\t= 3,\t \n#ifdef CONFIG_DEBUG_OBJECTS_WORK\n\tWORK_STRUCT_STATIC_BIT\t= 4,\t \n\tWORK_STRUCT_COLOR_SHIFT\t= 5,\t \n#else\n\tWORK_STRUCT_COLOR_SHIFT\t= 4,\t \n#endif\n\n\tWORK_STRUCT_COLOR_BITS\t= 4,\n\n\tWORK_STRUCT_PENDING\t= 1 << WORK_STRUCT_PENDING_BIT,\n\tWORK_STRUCT_INACTIVE\t= 1 << WORK_STRUCT_INACTIVE_BIT,\n\tWORK_STRUCT_PWQ\t\t= 1 << WORK_STRUCT_PWQ_BIT,\n\tWORK_STRUCT_LINKED\t= 1 << WORK_STRUCT_LINKED_BIT,\n#ifdef CONFIG_DEBUG_OBJECTS_WORK\n\tWORK_STRUCT_STATIC\t= 1 << WORK_STRUCT_STATIC_BIT,\n#else\n\tWORK_STRUCT_STATIC\t= 0,\n#endif\n\n\tWORK_NR_COLORS\t\t= (1 << WORK_STRUCT_COLOR_BITS),\n\n\t \n\tWORK_CPU_UNBOUND\t= NR_CPUS,\n\n\t \n\tWORK_STRUCT_FLAG_BITS\t= WORK_STRUCT_COLOR_SHIFT +\n\t\t\t\t  WORK_STRUCT_COLOR_BITS,\n\n\t \n\tWORK_OFFQ_FLAG_BASE\t= WORK_STRUCT_COLOR_SHIFT,\n\n\t__WORK_OFFQ_CANCELING\t= WORK_OFFQ_FLAG_BASE,\n\n\t \n\tWORK_OFFQ_FLAG_BITS\t= 1,\n\tWORK_OFFQ_POOL_SHIFT\t= WORK_OFFQ_FLAG_BASE + WORK_OFFQ_FLAG_BITS,\n\tWORK_OFFQ_LEFT\t\t= BITS_PER_LONG - WORK_OFFQ_POOL_SHIFT,\n\tWORK_OFFQ_POOL_BITS\t= WORK_OFFQ_LEFT <= 31 ? WORK_OFFQ_LEFT : 31,\n\n\t \n\tWORK_BUSY_PENDING\t= 1 << 0,\n\tWORK_BUSY_RUNNING\t= 1 << 1,\n\n\t \n\tWORKER_DESC_LEN\t\t= 24,\n};\n\n \n#define WORK_OFFQ_CANCELING\t(1ul << __WORK_OFFQ_CANCELING)\n#define WORK_OFFQ_POOL_NONE\t((1ul << WORK_OFFQ_POOL_BITS) - 1)\n#define WORK_STRUCT_NO_POOL\t(WORK_OFFQ_POOL_NONE << WORK_OFFQ_POOL_SHIFT)\n\n#define WORK_STRUCT_FLAG_MASK    ((1ul << WORK_STRUCT_FLAG_BITS) - 1)\n#define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)\n\nstruct work_struct {\n\tatomic_long_t data;\n\tstruct list_head entry;\n\twork_func_t func;\n#ifdef CONFIG_LOCKDEP\n\tstruct lockdep_map lockdep_map;\n#endif\n};\n\n#define WORK_DATA_INIT()\tATOMIC_LONG_INIT((unsigned long)WORK_STRUCT_NO_POOL)\n#define WORK_DATA_STATIC_INIT()\t\\\n\tATOMIC_LONG_INIT((unsigned long)(WORK_STRUCT_NO_POOL | WORK_STRUCT_STATIC))\n\nstruct delayed_work {\n\tstruct work_struct work;\n\tstruct timer_list timer;\n\n\t \n\tstruct workqueue_struct *wq;\n\tint cpu;\n};\n\nstruct rcu_work {\n\tstruct work_struct work;\n\tstruct rcu_head rcu;\n\n\t \n\tstruct workqueue_struct *wq;\n};\n\nenum wq_affn_scope {\n\tWQ_AFFN_DFL,\t\t\t \n\tWQ_AFFN_CPU,\t\t\t \n\tWQ_AFFN_SMT,\t\t\t \n\tWQ_AFFN_CACHE,\t\t\t \n\tWQ_AFFN_NUMA,\t\t\t \n\tWQ_AFFN_SYSTEM,\t\t\t \n\n\tWQ_AFFN_NR_TYPES,\n};\n\n \nstruct workqueue_attrs {\n\t \n\tint nice;\n\n\t \n\tcpumask_var_t cpumask;\n\n\t \n\tcpumask_var_t __pod_cpumask;\n\n\t \n\tbool affn_strict;\n\n\t \n\n\t \n\tenum wq_affn_scope affn_scope;\n\n\t \n\tbool ordered;\n};\n\nstatic inline struct delayed_work *to_delayed_work(struct work_struct *work)\n{\n\treturn container_of(work, struct delayed_work, work);\n}\n\nstatic inline struct rcu_work *to_rcu_work(struct work_struct *work)\n{\n\treturn container_of(work, struct rcu_work, work);\n}\n\nstruct execute_work {\n\tstruct work_struct work;\n};\n\n#ifdef CONFIG_LOCKDEP\n \n#define __WORK_INIT_LOCKDEP_MAP(n, k) \\\n\t.lockdep_map = STATIC_LOCKDEP_MAP_INIT(n, k),\n#else\n#define __WORK_INIT_LOCKDEP_MAP(n, k)\n#endif\n\n#define __WORK_INITIALIZER(n, f) {\t\t\t\t\t\\\n\t.data = WORK_DATA_STATIC_INIT(),\t\t\t\t\\\n\t.entry\t= { &(n).entry, &(n).entry },\t\t\t\t\\\n\t.func = (f),\t\t\t\t\t\t\t\\\n\t__WORK_INIT_LOCKDEP_MAP(#n, &(n))\t\t\t\t\\\n\t}\n\n#define __DELAYED_WORK_INITIALIZER(n, f, tflags) {\t\t\t\\\n\t.work = __WORK_INITIALIZER((n).work, (f)),\t\t\t\\\n\t.timer = __TIMER_INITIALIZER(delayed_work_timer_fn,\\\n\t\t\t\t     (tflags) | TIMER_IRQSAFE),\t\t\\\n\t}\n\n#define DECLARE_WORK(n, f)\t\t\t\t\t\t\\\n\tstruct work_struct n = __WORK_INITIALIZER(n, f)\n\n#define DECLARE_DELAYED_WORK(n, f)\t\t\t\t\t\\\n\tstruct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, 0)\n\n#define DECLARE_DEFERRABLE_WORK(n, f)\t\t\t\t\t\\\n\tstruct delayed_work n = __DELAYED_WORK_INITIALIZER(n, f, TIMER_DEFERRABLE)\n\n#ifdef CONFIG_DEBUG_OBJECTS_WORK\nextern void __init_work(struct work_struct *work, int onstack);\nextern void destroy_work_on_stack(struct work_struct *work);\nextern void destroy_delayed_work_on_stack(struct delayed_work *work);\nstatic inline unsigned int work_static(struct work_struct *work)\n{\n\treturn *work_data_bits(work) & WORK_STRUCT_STATIC;\n}\n#else\nstatic inline void __init_work(struct work_struct *work, int onstack) { }\nstatic inline void destroy_work_on_stack(struct work_struct *work) { }\nstatic inline void destroy_delayed_work_on_stack(struct delayed_work *work) { }\nstatic inline unsigned int work_static(struct work_struct *work) { return 0; }\n#endif\n\n \n#ifdef CONFIG_LOCKDEP\n#define __INIT_WORK_KEY(_work, _func, _onstack, _key)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t__init_work((_work), _onstack);\t\t\t\t\\\n\t\t(_work)->data = (atomic_long_t) WORK_DATA_INIT();\t\\\n\t\tlockdep_init_map(&(_work)->lockdep_map, \"(work_completion)\"#_work, (_key), 0); \\\n\t\tINIT_LIST_HEAD(&(_work)->entry);\t\t\t\\\n\t\t(_work)->func = (_func);\t\t\t\t\\\n\t} while (0)\n#else\n#define __INIT_WORK_KEY(_work, _func, _onstack, _key)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\t__init_work((_work), _onstack);\t\t\t\t\\\n\t\t(_work)->data = (atomic_long_t) WORK_DATA_INIT();\t\\\n\t\tINIT_LIST_HEAD(&(_work)->entry);\t\t\t\\\n\t\t(_work)->func = (_func);\t\t\t\t\\\n\t} while (0)\n#endif\n\n#define __INIT_WORK(_work, _func, _onstack)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tstatic __maybe_unused struct lock_class_key __key;\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t__INIT_WORK_KEY(_work, _func, _onstack, &__key);\t\\\n\t} while (0)\n\n#define INIT_WORK(_work, _func)\t\t\t\t\t\t\\\n\t__INIT_WORK((_work), (_func), 0)\n\n#define INIT_WORK_ONSTACK(_work, _func)\t\t\t\t\t\\\n\t__INIT_WORK((_work), (_func), 1)\n\n#define INIT_WORK_ONSTACK_KEY(_work, _func, _key)\t\t\t\\\n\t__INIT_WORK_KEY((_work), (_func), 1, _key)\n\n#define __INIT_DELAYED_WORK(_work, _func, _tflags)\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tINIT_WORK(&(_work)->work, (_func));\t\t\t\\\n\t\t__init_timer(&(_work)->timer,\t\t\t\t\\\n\t\t\t     delayed_work_timer_fn,\t\t\t\\\n\t\t\t     (_tflags) | TIMER_IRQSAFE);\t\t\\\n\t} while (0)\n\n#define __INIT_DELAYED_WORK_ONSTACK(_work, _func, _tflags)\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tINIT_WORK_ONSTACK(&(_work)->work, (_func));\t\t\\\n\t\t__init_timer_on_stack(&(_work)->timer,\t\t\t\\\n\t\t\t\t      delayed_work_timer_fn,\t\t\\\n\t\t\t\t      (_tflags) | TIMER_IRQSAFE);\t\\\n\t} while (0)\n\n#define INIT_DELAYED_WORK(_work, _func)\t\t\t\t\t\\\n\t__INIT_DELAYED_WORK(_work, _func, 0)\n\n#define INIT_DELAYED_WORK_ONSTACK(_work, _func)\t\t\t\t\\\n\t__INIT_DELAYED_WORK_ONSTACK(_work, _func, 0)\n\n#define INIT_DEFERRABLE_WORK(_work, _func)\t\t\t\t\\\n\t__INIT_DELAYED_WORK(_work, _func, TIMER_DEFERRABLE)\n\n#define INIT_DEFERRABLE_WORK_ONSTACK(_work, _func)\t\t\t\\\n\t__INIT_DELAYED_WORK_ONSTACK(_work, _func, TIMER_DEFERRABLE)\n\n#define INIT_RCU_WORK(_work, _func)\t\t\t\t\t\\\n\tINIT_WORK(&(_work)->work, (_func))\n\n#define INIT_RCU_WORK_ONSTACK(_work, _func)\t\t\t\t\\\n\tINIT_WORK_ONSTACK(&(_work)->work, (_func))\n\n \n#define work_pending(work) \\\n\ttest_bit(WORK_STRUCT_PENDING_BIT, work_data_bits(work))\n\n \n#define delayed_work_pending(w) \\\n\twork_pending(&(w)->work)\n\n \nenum {\n\tWQ_UNBOUND\t\t= 1 << 1,  \n\tWQ_FREEZABLE\t\t= 1 << 2,  \n\tWQ_MEM_RECLAIM\t\t= 1 << 3,  \n\tWQ_HIGHPRI\t\t= 1 << 4,  \n\tWQ_CPU_INTENSIVE\t= 1 << 5,  \n\tWQ_SYSFS\t\t= 1 << 6,  \n\n\t \n\tWQ_POWER_EFFICIENT\t= 1 << 7,\n\n\t__WQ_DESTROYING\t\t= 1 << 15,  \n\t__WQ_DRAINING\t\t= 1 << 16,  \n\t__WQ_ORDERED\t\t= 1 << 17,  \n\t__WQ_LEGACY\t\t= 1 << 18,  \n\t__WQ_ORDERED_EXPLICIT\t= 1 << 19,  \n\n\tWQ_MAX_ACTIVE\t\t= 512,\t   \n\tWQ_UNBOUND_MAX_ACTIVE\t= WQ_MAX_ACTIVE,\n\tWQ_DFL_ACTIVE\t\t= WQ_MAX_ACTIVE / 2,\n};\n\n \nextern struct workqueue_struct *system_wq;\nextern struct workqueue_struct *system_highpri_wq;\nextern struct workqueue_struct *system_long_wq;\nextern struct workqueue_struct *system_unbound_wq;\nextern struct workqueue_struct *system_freezable_wq;\nextern struct workqueue_struct *system_power_efficient_wq;\nextern struct workqueue_struct *system_freezable_power_efficient_wq;\n\n \n__printf(1, 4) struct workqueue_struct *\nalloc_workqueue(const char *fmt, unsigned int flags, int max_active, ...);\n\n \n#define alloc_ordered_workqueue(fmt, flags, args...)\t\t\t\\\n\talloc_workqueue(fmt, WQ_UNBOUND | __WQ_ORDERED |\t\t\\\n\t\t\t__WQ_ORDERED_EXPLICIT | (flags), 1, ##args)\n\n#define create_workqueue(name)\t\t\t\t\t\t\\\n\talloc_workqueue(\"%s\", __WQ_LEGACY | WQ_MEM_RECLAIM, 1, (name))\n#define create_freezable_workqueue(name)\t\t\t\t\\\n\talloc_workqueue(\"%s\", __WQ_LEGACY | WQ_FREEZABLE | WQ_UNBOUND |\t\\\n\t\t\tWQ_MEM_RECLAIM, 1, (name))\n#define create_singlethread_workqueue(name)\t\t\t\t\\\n\talloc_ordered_workqueue(\"%s\", __WQ_LEGACY | WQ_MEM_RECLAIM, name)\n\nextern void destroy_workqueue(struct workqueue_struct *wq);\n\nstruct workqueue_attrs *alloc_workqueue_attrs(void);\nvoid free_workqueue_attrs(struct workqueue_attrs *attrs);\nint apply_workqueue_attrs(struct workqueue_struct *wq,\n\t\t\t  const struct workqueue_attrs *attrs);\nint workqueue_set_unbound_cpumask(cpumask_var_t cpumask);\n\nextern bool queue_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\tstruct work_struct *work);\nextern bool queue_work_node(int node, struct workqueue_struct *wq,\n\t\t\t    struct work_struct *work);\nextern bool queue_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\tstruct delayed_work *work, unsigned long delay);\nextern bool mod_delayed_work_on(int cpu, struct workqueue_struct *wq,\n\t\t\tstruct delayed_work *dwork, unsigned long delay);\nextern bool queue_rcu_work(struct workqueue_struct *wq, struct rcu_work *rwork);\n\nextern void __flush_workqueue(struct workqueue_struct *wq);\nextern void drain_workqueue(struct workqueue_struct *wq);\n\nextern int schedule_on_each_cpu(work_func_t func);\n\nint execute_in_process_context(work_func_t fn, struct execute_work *);\n\nextern bool flush_work(struct work_struct *work);\nextern bool cancel_work(struct work_struct *work);\nextern bool cancel_work_sync(struct work_struct *work);\n\nextern bool flush_delayed_work(struct delayed_work *dwork);\nextern bool cancel_delayed_work(struct delayed_work *dwork);\nextern bool cancel_delayed_work_sync(struct delayed_work *dwork);\n\nextern bool flush_rcu_work(struct rcu_work *rwork);\n\nextern void workqueue_set_max_active(struct workqueue_struct *wq,\n\t\t\t\t     int max_active);\nextern struct work_struct *current_work(void);\nextern bool current_is_workqueue_rescuer(void);\nextern bool workqueue_congested(int cpu, struct workqueue_struct *wq);\nextern unsigned int work_busy(struct work_struct *work);\nextern __printf(1, 2) void set_worker_desc(const char *fmt, ...);\nextern void print_worker_info(const char *log_lvl, struct task_struct *task);\nextern void show_all_workqueues(void);\nextern void show_freezable_workqueues(void);\nextern void show_one_workqueue(struct workqueue_struct *wq);\nextern void wq_worker_comm(char *buf, size_t size, struct task_struct *task);\n\n \nstatic inline bool queue_work(struct workqueue_struct *wq,\n\t\t\t      struct work_struct *work)\n{\n\treturn queue_work_on(WORK_CPU_UNBOUND, wq, work);\n}\n\n \nstatic inline bool queue_delayed_work(struct workqueue_struct *wq,\n\t\t\t\t      struct delayed_work *dwork,\n\t\t\t\t      unsigned long delay)\n{\n\treturn queue_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);\n}\n\n \nstatic inline bool mod_delayed_work(struct workqueue_struct *wq,\n\t\t\t\t    struct delayed_work *dwork,\n\t\t\t\t    unsigned long delay)\n{\n\treturn mod_delayed_work_on(WORK_CPU_UNBOUND, wq, dwork, delay);\n}\n\n \nstatic inline bool schedule_work_on(int cpu, struct work_struct *work)\n{\n\treturn queue_work_on(cpu, system_wq, work);\n}\n\n \nstatic inline bool schedule_work(struct work_struct *work)\n{\n\treturn queue_work(system_wq, work);\n}\n\n \nextern void __warn_flushing_systemwide_wq(void)\n\t__compiletime_warning(\"Please avoid flushing system-wide workqueues.\");\n\n \n#define flush_scheduled_work()\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__warn_flushing_systemwide_wq();\t\t\t\t\\\n\t__flush_workqueue(system_wq);\t\t\t\t\t\\\n})\n\n#define flush_workqueue(wq)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tstruct workqueue_struct *_wq = (wq);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tif ((__builtin_constant_p(_wq == system_wq) &&\t\t\t\\\n\t     _wq == system_wq) ||\t\t\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_highpri_wq) &&\t\t\\\n\t     _wq == system_highpri_wq) ||\t\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_long_wq) &&\t\t\\\n\t     _wq == system_long_wq) ||\t\t\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_unbound_wq) &&\t\t\\\n\t     _wq == system_unbound_wq) ||\t\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_freezable_wq) &&\t\\\n\t     _wq == system_freezable_wq) ||\t\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_power_efficient_wq) &&\t\\\n\t     _wq == system_power_efficient_wq) ||\t\t\t\\\n\t    (__builtin_constant_p(_wq == system_freezable_power_efficient_wq) && \\\n\t     _wq == system_freezable_power_efficient_wq))\t\t\\\n\t\t__warn_flushing_systemwide_wq();\t\t\t\\\n\t__flush_workqueue(_wq);\t\t\t\t\t\t\\\n})\n\n \nstatic inline bool schedule_delayed_work_on(int cpu, struct delayed_work *dwork,\n\t\t\t\t\t    unsigned long delay)\n{\n\treturn queue_delayed_work_on(cpu, system_wq, dwork, delay);\n}\n\n \nstatic inline bool schedule_delayed_work(struct delayed_work *dwork,\n\t\t\t\t\t unsigned long delay)\n{\n\treturn queue_delayed_work(system_wq, dwork, delay);\n}\n\n#ifndef CONFIG_SMP\nstatic inline long work_on_cpu(int cpu, long (*fn)(void *), void *arg)\n{\n\treturn fn(arg);\n}\nstatic inline long work_on_cpu_safe(int cpu, long (*fn)(void *), void *arg)\n{\n\treturn fn(arg);\n}\n#else\nlong work_on_cpu_key(int cpu, long (*fn)(void *),\n\t\t     void *arg, struct lock_class_key *key);\n \n#define work_on_cpu(_cpu, _fn, _arg)\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\\\n\t\t\t\t\t\t\t\\\n\twork_on_cpu_key(_cpu, _fn, _arg, &__key);\t\\\n})\n\nlong work_on_cpu_safe_key(int cpu, long (*fn)(void *),\n\t\t\t  void *arg, struct lock_class_key *key);\n\n \n#define work_on_cpu_safe(_cpu, _fn, _arg)\t\t\\\n({\t\t\t\t\t\t\t\\\n\tstatic struct lock_class_key __key;\t\t\\\n\t\t\t\t\t\t\t\\\n\twork_on_cpu_safe_key(_cpu, _fn, _arg, &__key);\t\\\n})\n#endif  \n\n#ifdef CONFIG_FREEZER\nextern void freeze_workqueues_begin(void);\nextern bool freeze_workqueues_busy(void);\nextern void thaw_workqueues(void);\n#endif  \n\n#ifdef CONFIG_SYSFS\nint workqueue_sysfs_register(struct workqueue_struct *wq);\n#else\t \nstatic inline int workqueue_sysfs_register(struct workqueue_struct *wq)\n{ return 0; }\n#endif\t \n\n#ifdef CONFIG_WQ_WATCHDOG\nvoid wq_watchdog_touch(int cpu);\n#else\t \nstatic inline void wq_watchdog_touch(int cpu) { }\n#endif\t \n\n#ifdef CONFIG_SMP\nint workqueue_prepare_cpu(unsigned int cpu);\nint workqueue_online_cpu(unsigned int cpu);\nint workqueue_offline_cpu(unsigned int cpu);\n#endif\n\nvoid __init workqueue_init_early(void);\nvoid __init workqueue_init(void);\nvoid __init workqueue_init_topology(void);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}