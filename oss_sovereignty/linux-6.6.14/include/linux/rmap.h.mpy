{
  "module_name": "rmap.h",
  "hash_id": "783312414d4bbcf4ce44d0e8e33a95d0998b4cbfbf56db5e3eb87fab37e30629",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/rmap.h",
  "human_readable_source": " \n#ifndef _LINUX_RMAP_H\n#define _LINUX_RMAP_H\n \n\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/rwsem.h>\n#include <linux/memcontrol.h>\n#include <linux/highmem.h>\n#include <linux/pagemap.h>\n#include <linux/memremap.h>\n\n \nstruct anon_vma {\n\tstruct anon_vma *root;\t\t \n\tstruct rw_semaphore rwsem;\t \n\t \n\tatomic_t refcount;\n\n\t \n\tunsigned long num_children;\n\t \n\tunsigned long num_active_vmas;\n\n\tstruct anon_vma *parent;\t \n\n\t \n\n\t \n\tstruct rb_root_cached rb_root;\n};\n\n \nstruct anon_vma_chain {\n\tstruct vm_area_struct *vma;\n\tstruct anon_vma *anon_vma;\n\tstruct list_head same_vma;    \n\tstruct rb_node rb;\t\t\t \n\tunsigned long rb_subtree_last;\n#ifdef CONFIG_DEBUG_VM_RB\n\tunsigned long cached_vma_start, cached_vma_last;\n#endif\n};\n\nenum ttu_flags {\n\tTTU_SPLIT_HUGE_PMD\t= 0x4,\t \n\tTTU_IGNORE_MLOCK\t= 0x8,\t \n\tTTU_SYNC\t\t= 0x10,\t \n\tTTU_HWPOISON\t\t= 0x20,\t \n\tTTU_BATCH_FLUSH\t\t= 0x40,\t \n\tTTU_RMAP_LOCKED\t\t= 0x80,\t \n};\n\n#ifdef CONFIG_MMU\nstatic inline void get_anon_vma(struct anon_vma *anon_vma)\n{\n\tatomic_inc(&anon_vma->refcount);\n}\n\nvoid __put_anon_vma(struct anon_vma *anon_vma);\n\nstatic inline void put_anon_vma(struct anon_vma *anon_vma)\n{\n\tif (atomic_dec_and_test(&anon_vma->refcount))\n\t\t__put_anon_vma(anon_vma);\n}\n\nstatic inline void anon_vma_lock_write(struct anon_vma *anon_vma)\n{\n\tdown_write(&anon_vma->root->rwsem);\n}\n\nstatic inline void anon_vma_unlock_write(struct anon_vma *anon_vma)\n{\n\tup_write(&anon_vma->root->rwsem);\n}\n\nstatic inline void anon_vma_lock_read(struct anon_vma *anon_vma)\n{\n\tdown_read(&anon_vma->root->rwsem);\n}\n\nstatic inline int anon_vma_trylock_read(struct anon_vma *anon_vma)\n{\n\treturn down_read_trylock(&anon_vma->root->rwsem);\n}\n\nstatic inline void anon_vma_unlock_read(struct anon_vma *anon_vma)\n{\n\tup_read(&anon_vma->root->rwsem);\n}\n\n\n \nvoid anon_vma_init(void);\t \nint  __anon_vma_prepare(struct vm_area_struct *);\nvoid unlink_anon_vmas(struct vm_area_struct *);\nint anon_vma_clone(struct vm_area_struct *, struct vm_area_struct *);\nint anon_vma_fork(struct vm_area_struct *, struct vm_area_struct *);\n\nstatic inline int anon_vma_prepare(struct vm_area_struct *vma)\n{\n\tif (likely(vma->anon_vma))\n\t\treturn 0;\n\n\treturn __anon_vma_prepare(vma);\n}\n\nstatic inline void anon_vma_merge(struct vm_area_struct *vma,\n\t\t\t\t  struct vm_area_struct *next)\n{\n\tVM_BUG_ON_VMA(vma->anon_vma != next->anon_vma, vma);\n\tunlink_anon_vmas(next);\n}\n\nstruct anon_vma *folio_get_anon_vma(struct folio *folio);\n\n \ntypedef int __bitwise rmap_t;\n\n \n#define RMAP_NONE\t\t((__force rmap_t)0)\n\n \n#define RMAP_EXCLUSIVE\t\t((__force rmap_t)BIT(0))\n\n \n#define RMAP_COMPOUND\t\t((__force rmap_t)BIT(1))\n\n \nvoid page_move_anon_rmap(struct page *, struct vm_area_struct *);\nvoid page_add_anon_rmap(struct page *, struct vm_area_struct *,\n\t\tunsigned long address, rmap_t flags);\nvoid page_add_new_anon_rmap(struct page *, struct vm_area_struct *,\n\t\tunsigned long address);\nvoid folio_add_new_anon_rmap(struct folio *, struct vm_area_struct *,\n\t\tunsigned long address);\nvoid page_add_file_rmap(struct page *, struct vm_area_struct *,\n\t\tbool compound);\nvoid folio_add_file_rmap_range(struct folio *, struct page *, unsigned int nr,\n\t\tstruct vm_area_struct *, bool compound);\nvoid page_remove_rmap(struct page *, struct vm_area_struct *,\n\t\tbool compound);\n\nvoid hugepage_add_anon_rmap(struct page *, struct vm_area_struct *,\n\t\tunsigned long address, rmap_t flags);\nvoid hugepage_add_new_anon_rmap(struct folio *, struct vm_area_struct *,\n\t\tunsigned long address);\n\nstatic inline void __page_dup_rmap(struct page *page, bool compound)\n{\n\tif (compound) {\n\t\tstruct folio *folio = (struct folio *)page;\n\n\t\tVM_BUG_ON_PAGE(compound && !PageHead(page), page);\n\t\tatomic_inc(&folio->_entire_mapcount);\n\t} else {\n\t\tatomic_inc(&page->_mapcount);\n\t}\n}\n\nstatic inline void page_dup_file_rmap(struct page *page, bool compound)\n{\n\t__page_dup_rmap(page, compound);\n}\n\n \nstatic inline int page_try_dup_anon_rmap(struct page *page, bool compound,\n\t\t\t\t\t struct vm_area_struct *vma)\n{\n\tVM_BUG_ON_PAGE(!PageAnon(page), page);\n\n\t \n\tif (!PageAnonExclusive(page))\n\t\tgoto dup;\n\n\t \n\tif (likely(!is_device_private_page(page) &&\n\t    unlikely(page_needs_cow_for_dma(vma, page))))\n\t\treturn -EBUSY;\n\n\tClearPageAnonExclusive(page);\n\t \ndup:\n\t__page_dup_rmap(page, compound);\n\treturn 0;\n}\n\n \nstatic inline int page_try_share_anon_rmap(struct page *page)\n{\n\tVM_BUG_ON_PAGE(!PageAnon(page) || !PageAnonExclusive(page), page);\n\n\t \n\tif (unlikely(is_device_private_page(page))) {\n\t\tClearPageAnonExclusive(page);\n\t\treturn 0;\n\t}\n\n\t \n\n\t \n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP))\n\t\tsmp_mb();\n\n\tif (unlikely(page_maybe_dma_pinned(page)))\n\t\treturn -EBUSY;\n\tClearPageAnonExclusive(page);\n\n\t \n\tif (IS_ENABLED(CONFIG_HAVE_FAST_GUP))\n\t\tsmp_mb__after_atomic();\n\treturn 0;\n}\n\n \nint folio_referenced(struct folio *, int is_locked,\n\t\t\tstruct mem_cgroup *memcg, unsigned long *vm_flags);\n\nvoid try_to_migrate(struct folio *folio, enum ttu_flags flags);\nvoid try_to_unmap(struct folio *, enum ttu_flags flags);\n\nint make_device_exclusive_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, struct page **pages,\n\t\t\t\tvoid *arg);\n\n \n#define PVMW_SYNC\t\t(1 << 0)\n \n#define PVMW_MIGRATION\t\t(1 << 1)\n\nstruct page_vma_mapped_walk {\n\tunsigned long pfn;\n\tunsigned long nr_pages;\n\tpgoff_t pgoff;\n\tstruct vm_area_struct *vma;\n\tunsigned long address;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\tunsigned int flags;\n};\n\n#define DEFINE_PAGE_VMA_WALK(name, _page, _vma, _address, _flags)\t\\\n\tstruct page_vma_mapped_walk name = {\t\t\t\t\\\n\t\t.pfn = page_to_pfn(_page),\t\t\t\t\\\n\t\t.nr_pages = compound_nr(_page),\t\t\t\t\\\n\t\t.pgoff = page_to_pgoff(_page),\t\t\t\t\\\n\t\t.vma = _vma,\t\t\t\t\t\t\\\n\t\t.address = _address,\t\t\t\t\t\\\n\t\t.flags = _flags,\t\t\t\t\t\\\n\t}\n\n#define DEFINE_FOLIO_VMA_WALK(name, _folio, _vma, _address, _flags)\t\\\n\tstruct page_vma_mapped_walk name = {\t\t\t\t\\\n\t\t.pfn = folio_pfn(_folio),\t\t\t\t\\\n\t\t.nr_pages = folio_nr_pages(_folio),\t\t\t\\\n\t\t.pgoff = folio_pgoff(_folio),\t\t\t\t\\\n\t\t.vma = _vma,\t\t\t\t\t\t\\\n\t\t.address = _address,\t\t\t\t\t\\\n\t\t.flags = _flags,\t\t\t\t\t\\\n\t}\n\nstatic inline void page_vma_mapped_walk_done(struct page_vma_mapped_walk *pvmw)\n{\n\t \n\tif (pvmw->pte && !is_vm_hugetlb_page(pvmw->vma))\n\t\tpte_unmap(pvmw->pte);\n\tif (pvmw->ptl)\n\t\tspin_unlock(pvmw->ptl);\n}\n\nbool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw);\n\n \nunsigned long page_address_in_vma(struct page *, struct vm_area_struct *);\n\n \nint folio_mkclean(struct folio *);\n\nint pfn_mkclean_range(unsigned long pfn, unsigned long nr_pages, pgoff_t pgoff,\n\t\t      struct vm_area_struct *vma);\n\nvoid remove_migration_ptes(struct folio *src, struct folio *dst, bool locked);\n\nint page_mapped_in_vma(struct page *page, struct vm_area_struct *vma);\n\n \nstruct rmap_walk_control {\n\tvoid *arg;\n\tbool try_lock;\n\tbool contended;\n\t \n\tbool (*rmap_one)(struct folio *folio, struct vm_area_struct *vma,\n\t\t\t\t\tunsigned long addr, void *arg);\n\tint (*done)(struct folio *folio);\n\tstruct anon_vma *(*anon_lock)(struct folio *folio,\n\t\t\t\t      struct rmap_walk_control *rwc);\n\tbool (*invalid_vma)(struct vm_area_struct *vma, void *arg);\n};\n\nvoid rmap_walk(struct folio *folio, struct rmap_walk_control *rwc);\nvoid rmap_walk_locked(struct folio *folio, struct rmap_walk_control *rwc);\nstruct anon_vma *folio_lock_anon_vma_read(struct folio *folio,\n\t\t\t\t\t  struct rmap_walk_control *rwc);\n\n#else\t \n\n#define anon_vma_init()\t\tdo {} while (0)\n#define anon_vma_prepare(vma)\t(0)\n\nstatic inline int folio_referenced(struct folio *folio, int is_locked,\n\t\t\t\t  struct mem_cgroup *memcg,\n\t\t\t\t  unsigned long *vm_flags)\n{\n\t*vm_flags = 0;\n\treturn 0;\n}\n\nstatic inline void try_to_unmap(struct folio *folio, enum ttu_flags flags)\n{\n}\n\nstatic inline int folio_mkclean(struct folio *folio)\n{\n\treturn 0;\n}\n#endif\t \n\nstatic inline int page_mkclean(struct page *page)\n{\n\treturn folio_mkclean(page_folio(page));\n}\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}