{
  "module_name": "etherdevice.h",
  "hash_id": "454781c3fabbaafaa57c4423621cfe904433295396fdf1477af860c898d65ae7",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/etherdevice.h",
  "human_readable_source": " \n \n#ifndef _LINUX_ETHERDEVICE_H\n#define _LINUX_ETHERDEVICE_H\n\n#include <linux/if_ether.h>\n#include <linux/netdevice.h>\n#include <linux/random.h>\n#include <linux/crc32.h>\n#include <asm/unaligned.h>\n#include <asm/bitsperlong.h>\n\n#ifdef __KERNEL__\nstruct device;\nstruct fwnode_handle;\n\nint eth_platform_get_mac_address(struct device *dev, u8 *mac_addr);\nint platform_get_ethdev_address(struct device *dev, struct net_device *netdev);\nunsigned char *arch_get_platform_mac_address(void);\nint nvmem_get_mac_address(struct device *dev, void *addrbuf);\nint device_get_mac_address(struct device *dev, char *addr);\nint device_get_ethdev_address(struct device *dev, struct net_device *netdev);\nint fwnode_get_mac_address(struct fwnode_handle *fwnode, char *addr);\n\nu32 eth_get_headlen(const struct net_device *dev, const void *data, u32 len);\n__be16 eth_type_trans(struct sk_buff *skb, struct net_device *dev);\nextern const struct header_ops eth_header_ops;\n\nint eth_header(struct sk_buff *skb, struct net_device *dev, unsigned short type,\n\t       const void *daddr, const void *saddr, unsigned len);\nint eth_header_parse(const struct sk_buff *skb, unsigned char *haddr);\nint eth_header_cache(const struct neighbour *neigh, struct hh_cache *hh,\n\t\t     __be16 type);\nvoid eth_header_cache_update(struct hh_cache *hh, const struct net_device *dev,\n\t\t\t     const unsigned char *haddr);\n__be16 eth_header_parse_protocol(const struct sk_buff *skb);\nint eth_prepare_mac_addr_change(struct net_device *dev, void *p);\nvoid eth_commit_mac_addr_change(struct net_device *dev, void *p);\nint eth_mac_addr(struct net_device *dev, void *p);\nint eth_validate_addr(struct net_device *dev);\n\nstruct net_device *alloc_etherdev_mqs(int sizeof_priv, unsigned int txqs,\n\t\t\t\t\t    unsigned int rxqs);\n#define alloc_etherdev(sizeof_priv) alloc_etherdev_mq(sizeof_priv, 1)\n#define alloc_etherdev_mq(sizeof_priv, count) alloc_etherdev_mqs(sizeof_priv, count, count)\n\nstruct net_device *devm_alloc_etherdev_mqs(struct device *dev, int sizeof_priv,\n\t\t\t\t\t   unsigned int txqs,\n\t\t\t\t\t   unsigned int rxqs);\n#define devm_alloc_etherdev(dev, sizeof_priv) devm_alloc_etherdev_mqs(dev, sizeof_priv, 1, 1)\n\nstruct sk_buff *eth_gro_receive(struct list_head *head, struct sk_buff *skb);\nint eth_gro_complete(struct sk_buff *skb, int nhoff);\n\n \nstatic const u8 eth_reserved_addr_base[ETH_ALEN] __aligned(2) =\n{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x00 };\n#define eth_stp_addr eth_reserved_addr_base\n\n \nstatic inline bool is_link_local_ether_addr(const u8 *addr)\n{\n\t__be16 *a = (__be16 *)addr;\n\tstatic const __be16 *b = (const __be16 *)eth_reserved_addr_base;\n\tstatic const __be16 m = cpu_to_be16(0xfff0);\n\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\treturn (((*(const u32 *)addr) ^ (*(const u32 *)b)) |\n\t\t(__force int)((a[2] ^ b[2]) & m)) == 0;\n#else\n\treturn ((a[0] ^ b[0]) | (a[1] ^ b[1]) | ((a[2] ^ b[2]) & m)) == 0;\n#endif\n}\n\n \nstatic inline bool is_zero_ether_addr(const u8 *addr)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\treturn ((*(const u32 *)addr) | (*(const u16 *)(addr + 4))) == 0;\n#else\n\treturn (*(const u16 *)(addr + 0) |\n\t\t*(const u16 *)(addr + 2) |\n\t\t*(const u16 *)(addr + 4)) == 0;\n#endif\n}\n\n \nstatic inline bool is_multicast_ether_addr(const u8 *addr)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\tu32 a = *(const u32 *)addr;\n#else\n\tu16 a = *(const u16 *)addr;\n#endif\n#ifdef __BIG_ENDIAN\n\treturn 0x01 & (a >> ((sizeof(a) * 8) - 8));\n#else\n\treturn 0x01 & a;\n#endif\n}\n\nstatic inline bool is_multicast_ether_addr_64bits(const u8 *addr)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n#ifdef __BIG_ENDIAN\n\treturn 0x01 & ((*(const u64 *)addr) >> 56);\n#else\n\treturn 0x01 & (*(const u64 *)addr);\n#endif\n#else\n\treturn is_multicast_ether_addr(addr);\n#endif\n}\n\n \nstatic inline bool is_local_ether_addr(const u8 *addr)\n{\n\treturn 0x02 & addr[0];\n}\n\n \nstatic inline bool is_broadcast_ether_addr(const u8 *addr)\n{\n\treturn (*(const u16 *)(addr + 0) &\n\t\t*(const u16 *)(addr + 2) &\n\t\t*(const u16 *)(addr + 4)) == 0xffff;\n}\n\n \nstatic inline bool is_unicast_ether_addr(const u8 *addr)\n{\n\treturn !is_multicast_ether_addr(addr);\n}\n\n \nstatic inline bool is_valid_ether_addr(const u8 *addr)\n{\n\t \n\treturn !is_multicast_ether_addr(addr) && !is_zero_ether_addr(addr);\n}\n\n \nstatic inline bool eth_proto_is_802_3(__be16 proto)\n{\n#ifndef __BIG_ENDIAN\n\t \n\tproto &= htons(0xFF00);\n#endif\n\t \n\treturn (__force u16)proto >= (__force u16)htons(ETH_P_802_3_MIN);\n}\n\n \nstatic inline void eth_random_addr(u8 *addr)\n{\n\tget_random_bytes(addr, ETH_ALEN);\n\taddr[0] &= 0xfe;\t \n\taddr[0] |= 0x02;\t \n}\n\n \nstatic inline void eth_broadcast_addr(u8 *addr)\n{\n\tmemset(addr, 0xff, ETH_ALEN);\n}\n\n \nstatic inline void eth_zero_addr(u8 *addr)\n{\n\tmemset(addr, 0x00, ETH_ALEN);\n}\n\n \nstatic inline void eth_hw_addr_random(struct net_device *dev)\n{\n\tu8 addr[ETH_ALEN];\n\n\teth_random_addr(addr);\n\t__dev_addr_set(dev, addr, ETH_ALEN);\n\tdev->addr_assign_type = NET_ADDR_RANDOM;\n}\n\n \nstatic inline u32 eth_hw_addr_crc(struct netdev_hw_addr *ha)\n{\n\treturn ether_crc(ETH_ALEN, ha->addr);\n}\n\n \nstatic inline void ether_addr_copy(u8 *dst, const u8 *src)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\t*(u32 *)dst = *(const u32 *)src;\n\t*(u16 *)(dst + 4) = *(const u16 *)(src + 4);\n#else\n\tu16 *a = (u16 *)dst;\n\tconst u16 *b = (const u16 *)src;\n\n\ta[0] = b[0];\n\ta[1] = b[1];\n\ta[2] = b[2];\n#endif\n}\n\n \nstatic inline void eth_hw_addr_set(struct net_device *dev, const u8 *addr)\n{\n\t__dev_addr_set(dev, addr, ETH_ALEN);\n}\n\n \nstatic inline void eth_hw_addr_inherit(struct net_device *dst,\n\t\t\t\t       struct net_device *src)\n{\n\tdst->addr_assign_type = src->addr_assign_type;\n\teth_hw_addr_set(dst, src->dev_addr);\n}\n\n \nstatic inline bool ether_addr_equal(const u8 *addr1, const u8 *addr2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\tu32 fold = ((*(const u32 *)addr1) ^ (*(const u32 *)addr2)) |\n\t\t   ((*(const u16 *)(addr1 + 4)) ^ (*(const u16 *)(addr2 + 4)));\n\n\treturn fold == 0;\n#else\n\tconst u16 *a = (const u16 *)addr1;\n\tconst u16 *b = (const u16 *)addr2;\n\n\treturn ((a[0] ^ b[0]) | (a[1] ^ b[1]) | (a[2] ^ b[2])) == 0;\n#endif\n}\n\n \n\nstatic inline bool ether_addr_equal_64bits(const u8 *addr1, const u8 *addr2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 fold = (*(const u64 *)addr1) ^ (*(const u64 *)addr2);\n\n#ifdef __BIG_ENDIAN\n\treturn (fold >> 16) == 0;\n#else\n\treturn (fold << 16) == 0;\n#endif\n#else\n\treturn ether_addr_equal(addr1, addr2);\n#endif\n}\n\n \nstatic inline bool ether_addr_equal_unaligned(const u8 *addr1, const u8 *addr2)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\n\treturn ether_addr_equal(addr1, addr2);\n#else\n\treturn memcmp(addr1, addr2, ETH_ALEN) == 0;\n#endif\n}\n\n \nstatic inline bool ether_addr_equal_masked(const u8 *addr1, const u8 *addr2,\n\t\t\t\t\t   const u8 *mask)\n{\n\tint i;\n\n\tfor (i = 0; i < ETH_ALEN; i++) {\n\t\tif ((addr1[i] ^ addr2[i]) & mask[i])\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic inline bool ether_addr_is_ipv4_mcast(const u8 *addr)\n{\n\tu8 base[ETH_ALEN] = { 0x01, 0x00, 0x5e, 0x00, 0x00, 0x00 };\n\tu8 mask[ETH_ALEN] = { 0xff, 0xff, 0xff, 0x80, 0x00, 0x00 };\n\n\treturn ether_addr_equal_masked(addr, base, mask);\n}\n\nstatic inline bool ether_addr_is_ipv6_mcast(const u8 *addr)\n{\n\tu8 base[ETH_ALEN] = { 0x33, 0x33, 0x00, 0x00, 0x00, 0x00 };\n\tu8 mask[ETH_ALEN] = { 0xff, 0xff, 0x00, 0x00, 0x00, 0x00 };\n\n\treturn ether_addr_equal_masked(addr, base, mask);\n}\n\nstatic inline bool ether_addr_is_ip_mcast(const u8 *addr)\n{\n\treturn ether_addr_is_ipv4_mcast(addr) ||\n\t\tether_addr_is_ipv6_mcast(addr);\n}\n\n \nstatic inline u64 ether_addr_to_u64(const u8 *addr)\n{\n\tu64 u = 0;\n\tint i;\n\n\tfor (i = 0; i < ETH_ALEN; i++)\n\t\tu = u << 8 | addr[i];\n\n\treturn u;\n}\n\n \nstatic inline void u64_to_ether_addr(u64 u, u8 *addr)\n{\n\tint i;\n\n\tfor (i = ETH_ALEN - 1; i >= 0; i--) {\n\t\taddr[i] = u & 0xff;\n\t\tu = u >> 8;\n\t}\n}\n\n \nstatic inline void eth_addr_dec(u8 *addr)\n{\n\tu64 u = ether_addr_to_u64(addr);\n\n\tu--;\n\tu64_to_ether_addr(u, addr);\n}\n\n \nstatic inline void eth_addr_inc(u8 *addr)\n{\n\tu64 u = ether_addr_to_u64(addr);\n\n\tu++;\n\tu64_to_ether_addr(u, addr);\n}\n\n \nstatic inline void eth_addr_add(u8 *addr, long offset)\n{\n\tu64 u = ether_addr_to_u64(addr);\n\n\tu += offset;\n\tu64_to_ether_addr(u, addr);\n}\n\n \nstatic inline bool is_etherdev_addr(const struct net_device *dev,\n\t\t\t\t    const u8 addr[6 + 2])\n{\n\tstruct netdev_hw_addr *ha;\n\tbool res = false;\n\n\trcu_read_lock();\n\tfor_each_dev_addr(dev, ha) {\n\t\tres = ether_addr_equal_64bits(addr, ha->addr);\n\t\tif (res)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn res;\n}\n#endif\t \n\n \n\nstatic inline unsigned long compare_ether_header(const void *a, const void *b)\n{\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tunsigned long fold;\n\n\t \n\tfold = *(unsigned long *)a ^ *(unsigned long *)b;\n\tfold |= *(unsigned long *)(a + 6) ^ *(unsigned long *)(b + 6);\n\treturn fold;\n#else\n\tu32 *a32 = (u32 *)((u8 *)a + 2);\n\tu32 *b32 = (u32 *)((u8 *)b + 2);\n\n\treturn (*(u16 *)a ^ *(u16 *)b) | (a32[0] ^ b32[0]) |\n\t       (a32[1] ^ b32[1]) | (a32[2] ^ b32[2]);\n#endif\n}\n\n \nstatic inline void eth_hw_addr_gen(struct net_device *dev, const u8 *base_addr,\n\t\t\t\t   unsigned int id)\n{\n\tu64 u = ether_addr_to_u64(base_addr);\n\tu8 addr[ETH_ALEN];\n\n\tu += id;\n\tu64_to_ether_addr(u, addr);\n\teth_hw_addr_set(dev, addr);\n}\n\n \nstatic inline int eth_skb_pad(struct sk_buff *skb)\n{\n\treturn skb_put_padto(skb, ETH_ZLEN);\n}\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}