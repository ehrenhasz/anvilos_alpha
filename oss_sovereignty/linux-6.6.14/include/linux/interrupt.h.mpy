{
  "module_name": "interrupt.h",
  "hash_id": "be403ed36a0a802d2a59331337e89ec417ee91213aed6c17e9bf8a8612b4f5a9",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/interrupt.h",
  "human_readable_source": " \n \n#ifndef _LINUX_INTERRUPT_H\n#define _LINUX_INTERRUPT_H\n\n#include <linux/kernel.h>\n#include <linux/bitops.h>\n#include <linux/cpumask.h>\n#include <linux/irqreturn.h>\n#include <linux/irqnr.h>\n#include <linux/hardirq.h>\n#include <linux/irqflags.h>\n#include <linux/hrtimer.h>\n#include <linux/kref.h>\n#include <linux/workqueue.h>\n#include <linux/jump_label.h>\n\n#include <linux/atomic.h>\n#include <asm/ptrace.h>\n#include <asm/irq.h>\n#include <asm/sections.h>\n\n \n#define IRQF_TRIGGER_NONE\t0x00000000\n#define IRQF_TRIGGER_RISING\t0x00000001\n#define IRQF_TRIGGER_FALLING\t0x00000002\n#define IRQF_TRIGGER_HIGH\t0x00000004\n#define IRQF_TRIGGER_LOW\t0x00000008\n#define IRQF_TRIGGER_MASK\t(IRQF_TRIGGER_HIGH | IRQF_TRIGGER_LOW | \\\n\t\t\t\t IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING)\n#define IRQF_TRIGGER_PROBE\t0x00000010\n\n \n#define IRQF_SHARED\t\t0x00000080\n#define IRQF_PROBE_SHARED\t0x00000100\n#define __IRQF_TIMER\t\t0x00000200\n#define IRQF_PERCPU\t\t0x00000400\n#define IRQF_NOBALANCING\t0x00000800\n#define IRQF_IRQPOLL\t\t0x00001000\n#define IRQF_ONESHOT\t\t0x00002000\n#define IRQF_NO_SUSPEND\t\t0x00004000\n#define IRQF_FORCE_RESUME\t0x00008000\n#define IRQF_NO_THREAD\t\t0x00010000\n#define IRQF_EARLY_RESUME\t0x00020000\n#define IRQF_COND_SUSPEND\t0x00040000\n#define IRQF_NO_AUTOEN\t\t0x00080000\n#define IRQF_NO_DEBUG\t\t0x00100000\n\n#define IRQF_TIMER\t\t(__IRQF_TIMER | IRQF_NO_SUSPEND | IRQF_NO_THREAD)\n\n \nenum {\n\tIRQC_IS_HARDIRQ\t= 0,\n\tIRQC_IS_NESTED,\n};\n\ntypedef irqreturn_t (*irq_handler_t)(int, void *);\n\n \nstruct irqaction {\n\tirq_handler_t\t\thandler;\n\tvoid\t\t\t*dev_id;\n\tvoid __percpu\t\t*percpu_dev_id;\n\tstruct irqaction\t*next;\n\tirq_handler_t\t\tthread_fn;\n\tstruct task_struct\t*thread;\n\tstruct irqaction\t*secondary;\n\tunsigned int\t\tirq;\n\tunsigned int\t\tflags;\n\tunsigned long\t\tthread_flags;\n\tunsigned long\t\tthread_mask;\n\tconst char\t\t*name;\n\tstruct proc_dir_entry\t*dir;\n} ____cacheline_internodealigned_in_smp;\n\nextern irqreturn_t no_action(int cpl, void *dev_id);\n\n \n#define IRQ_NOTCONNECTED\t(1U << 31)\n\nextern int __must_check\nrequest_threaded_irq(unsigned int irq, irq_handler_t handler,\n\t\t     irq_handler_t thread_fn,\n\t\t     unsigned long flags, const char *name, void *dev);\n\n \nstatic inline int __must_check\nrequest_irq(unsigned int irq, irq_handler_t handler, unsigned long flags,\n\t    const char *name, void *dev)\n{\n\treturn request_threaded_irq(irq, handler, NULL, flags, name, dev);\n}\n\nextern int __must_check\nrequest_any_context_irq(unsigned int irq, irq_handler_t handler,\n\t\t\tunsigned long flags, const char *name, void *dev_id);\n\nextern int __must_check\n__request_percpu_irq(unsigned int irq, irq_handler_t handler,\n\t\t     unsigned long flags, const char *devname,\n\t\t     void __percpu *percpu_dev_id);\n\nextern int __must_check\nrequest_nmi(unsigned int irq, irq_handler_t handler, unsigned long flags,\n\t    const char *name, void *dev);\n\nstatic inline int __must_check\nrequest_percpu_irq(unsigned int irq, irq_handler_t handler,\n\t\t   const char *devname, void __percpu *percpu_dev_id)\n{\n\treturn __request_percpu_irq(irq, handler, 0,\n\t\t\t\t    devname, percpu_dev_id);\n}\n\nextern int __must_check\nrequest_percpu_nmi(unsigned int irq, irq_handler_t handler,\n\t\t   const char *devname, void __percpu *dev);\n\nextern const void *free_irq(unsigned int, void *);\nextern void free_percpu_irq(unsigned int, void __percpu *);\n\nextern const void *free_nmi(unsigned int irq, void *dev_id);\nextern void free_percpu_nmi(unsigned int irq, void __percpu *percpu_dev_id);\n\nstruct device;\n\nextern int __must_check\ndevm_request_threaded_irq(struct device *dev, unsigned int irq,\n\t\t\t  irq_handler_t handler, irq_handler_t thread_fn,\n\t\t\t  unsigned long irqflags, const char *devname,\n\t\t\t  void *dev_id);\n\nstatic inline int __must_check\ndevm_request_irq(struct device *dev, unsigned int irq, irq_handler_t handler,\n\t\t unsigned long irqflags, const char *devname, void *dev_id)\n{\n\treturn devm_request_threaded_irq(dev, irq, handler, NULL, irqflags,\n\t\t\t\t\t devname, dev_id);\n}\n\nextern int __must_check\ndevm_request_any_context_irq(struct device *dev, unsigned int irq,\n\t\t irq_handler_t handler, unsigned long irqflags,\n\t\t const char *devname, void *dev_id);\n\nextern void devm_free_irq(struct device *dev, unsigned int irq, void *dev_id);\n\nbool irq_has_action(unsigned int irq);\nextern void disable_irq_nosync(unsigned int irq);\nextern bool disable_hardirq(unsigned int irq);\nextern void disable_irq(unsigned int irq);\nextern void disable_percpu_irq(unsigned int irq);\nextern void enable_irq(unsigned int irq);\nextern void enable_percpu_irq(unsigned int irq, unsigned int type);\nextern bool irq_percpu_is_enabled(unsigned int irq);\nextern void irq_wake_thread(unsigned int irq, void *dev_id);\n\nextern void disable_nmi_nosync(unsigned int irq);\nextern void disable_percpu_nmi(unsigned int irq);\nextern void enable_nmi(unsigned int irq);\nextern void enable_percpu_nmi(unsigned int irq, unsigned int type);\nextern int prepare_percpu_nmi(unsigned int irq);\nextern void teardown_percpu_nmi(unsigned int irq);\n\nextern int irq_inject_interrupt(unsigned int irq);\n\n \nextern void suspend_device_irqs(void);\nextern void resume_device_irqs(void);\nextern void rearm_wake_irq(unsigned int irq);\n\n \nstruct irq_affinity_notify {\n\tunsigned int irq;\n\tstruct kref kref;\n\tstruct work_struct work;\n\tvoid (*notify)(struct irq_affinity_notify *, const cpumask_t *mask);\n\tvoid (*release)(struct kref *ref);\n};\n\n#define\tIRQ_AFFINITY_MAX_SETS  4\n\n \nstruct irq_affinity {\n\tunsigned int\tpre_vectors;\n\tunsigned int\tpost_vectors;\n\tunsigned int\tnr_sets;\n\tunsigned int\tset_size[IRQ_AFFINITY_MAX_SETS];\n\tvoid\t\t(*calc_sets)(struct irq_affinity *, unsigned int nvecs);\n\tvoid\t\t*priv;\n};\n\n \nstruct irq_affinity_desc {\n\tstruct cpumask\tmask;\n\tunsigned int\tis_managed : 1;\n};\n\n#if defined(CONFIG_SMP)\n\nextern cpumask_var_t irq_default_affinity;\n\nextern int irq_set_affinity(unsigned int irq, const struct cpumask *cpumask);\nextern int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask);\n\nextern int irq_can_set_affinity(unsigned int irq);\nextern int irq_select_affinity(unsigned int irq);\n\nextern int __irq_apply_affinity_hint(unsigned int irq, const struct cpumask *m,\n\t\t\t\t     bool setaffinity);\n\n \nstatic inline int\nirq_update_affinity_hint(unsigned int irq, const struct cpumask *m)\n{\n\treturn __irq_apply_affinity_hint(irq, m, false);\n}\n\n \nstatic inline int\nirq_set_affinity_and_hint(unsigned int irq, const struct cpumask *m)\n{\n\treturn __irq_apply_affinity_hint(irq, m, true);\n}\n\n \nstatic inline int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)\n{\n\treturn irq_set_affinity_and_hint(irq, m);\n}\n\nextern int irq_update_affinity_desc(unsigned int irq,\n\t\t\t\t    struct irq_affinity_desc *affinity);\n\nextern int\nirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify);\n\nstruct irq_affinity_desc *\nirq_create_affinity_masks(unsigned int nvec, struct irq_affinity *affd);\n\nunsigned int irq_calc_affinity_vectors(unsigned int minvec, unsigned int maxvec,\n\t\t\t\t       const struct irq_affinity *affd);\n\n#else  \n\nstatic inline int irq_set_affinity(unsigned int irq, const struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_force_affinity(unsigned int irq, const struct cpumask *cpumask)\n{\n\treturn 0;\n}\n\nstatic inline int irq_can_set_affinity(unsigned int irq)\n{\n\treturn 0;\n}\n\nstatic inline int irq_select_affinity(unsigned int irq)  { return 0; }\n\nstatic inline int irq_update_affinity_hint(unsigned int irq,\n\t\t\t\t\t   const struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_set_affinity_and_hint(unsigned int irq,\n\t\t\t\t\t    const struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_set_affinity_hint(unsigned int irq,\n\t\t\t\t\tconst struct cpumask *m)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int irq_update_affinity_desc(unsigned int irq,\n\t\t\t\t\t   struct irq_affinity_desc *affinity)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int\nirq_set_affinity_notifier(unsigned int irq, struct irq_affinity_notify *notify)\n{\n\treturn 0;\n}\n\nstatic inline struct irq_affinity_desc *\nirq_create_affinity_masks(unsigned int nvec, struct irq_affinity *affd)\n{\n\treturn NULL;\n}\n\nstatic inline unsigned int\nirq_calc_affinity_vectors(unsigned int minvec, unsigned int maxvec,\n\t\t\t  const struct irq_affinity *affd)\n{\n\treturn maxvec;\n}\n\n#endif  \n\n \nstatic inline void disable_irq_nosync_lockdep(unsigned int irq)\n{\n\tdisable_irq_nosync(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_disable();\n#endif\n}\n\nstatic inline void disable_irq_nosync_lockdep_irqsave(unsigned int irq, unsigned long *flags)\n{\n\tdisable_irq_nosync(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_save(*flags);\n#endif\n}\n\nstatic inline void disable_irq_lockdep(unsigned int irq)\n{\n\tdisable_irq(irq);\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_disable();\n#endif\n}\n\nstatic inline void enable_irq_lockdep(unsigned int irq)\n{\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_enable();\n#endif\n\tenable_irq(irq);\n}\n\nstatic inline void enable_irq_lockdep_irqrestore(unsigned int irq, unsigned long *flags)\n{\n#ifdef CONFIG_LOCKDEP\n\tlocal_irq_restore(*flags);\n#endif\n\tenable_irq(irq);\n}\n\n \nextern int irq_set_irq_wake(unsigned int irq, unsigned int on);\n\nstatic inline int enable_irq_wake(unsigned int irq)\n{\n\treturn irq_set_irq_wake(irq, 1);\n}\n\nstatic inline int disable_irq_wake(unsigned int irq)\n{\n\treturn irq_set_irq_wake(irq, 0);\n}\n\n \nenum irqchip_irq_state {\n\tIRQCHIP_STATE_PENDING,\t\t \n\tIRQCHIP_STATE_ACTIVE,\t\t \n\tIRQCHIP_STATE_MASKED,\t\t \n\tIRQCHIP_STATE_LINE_LEVEL,\t \n};\n\nextern int irq_get_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t\t bool *state);\nextern int irq_set_irqchip_state(unsigned int irq, enum irqchip_irq_state which,\n\t\t\t\t bool state);\n\n#ifdef CONFIG_IRQ_FORCED_THREADING\n# ifdef CONFIG_PREEMPT_RT\n#  define force_irqthreads()\t(true)\n# else\nDECLARE_STATIC_KEY_FALSE(force_irqthreads_key);\n#  define force_irqthreads()\t(static_branch_unlikely(&force_irqthreads_key))\n# endif\n#else\n#define force_irqthreads()\t(false)\n#endif\n\n#ifndef local_softirq_pending\n\n#ifndef local_softirq_pending_ref\n#define local_softirq_pending_ref irq_stat.__softirq_pending\n#endif\n\n#define local_softirq_pending()\t(__this_cpu_read(local_softirq_pending_ref))\n#define set_softirq_pending(x)\t(__this_cpu_write(local_softirq_pending_ref, (x)))\n#define or_softirq_pending(x)\t(__this_cpu_or(local_softirq_pending_ref, (x)))\n\n#endif  \n\n \n#ifndef hard_irq_disable\n#define hard_irq_disable()\tdo { } while(0)\n#endif\n\n \n\nenum\n{\n\tHI_SOFTIRQ=0,\n\tTIMER_SOFTIRQ,\n\tNET_TX_SOFTIRQ,\n\tNET_RX_SOFTIRQ,\n\tBLOCK_SOFTIRQ,\n\tIRQ_POLL_SOFTIRQ,\n\tTASKLET_SOFTIRQ,\n\tSCHED_SOFTIRQ,\n\tHRTIMER_SOFTIRQ,\n\tRCU_SOFTIRQ,     \n\n\tNR_SOFTIRQS\n};\n\n \n#define SOFTIRQ_HOTPLUG_SAFE_MASK (BIT(TIMER_SOFTIRQ) | BIT(IRQ_POLL_SOFTIRQ) |\\\n\t\t\t\t   BIT(HRTIMER_SOFTIRQ) | BIT(RCU_SOFTIRQ))\n\n\n \nextern const char * const softirq_to_name[NR_SOFTIRQS];\n\n \n\nstruct softirq_action\n{\n\tvoid\t(*action)(struct softirq_action *);\n};\n\nasmlinkage void do_softirq(void);\nasmlinkage void __do_softirq(void);\n\n#ifdef CONFIG_PREEMPT_RT\nextern void do_softirq_post_smp_call_flush(unsigned int was_pending);\n#else\nstatic inline void do_softirq_post_smp_call_flush(unsigned int unused)\n{\n\tdo_softirq();\n}\n#endif\n\nextern void open_softirq(int nr, void (*action)(struct softirq_action *));\nextern void softirq_init(void);\nextern void __raise_softirq_irqoff(unsigned int nr);\n\nextern void raise_softirq_irqoff(unsigned int nr);\nextern void raise_softirq(unsigned int nr);\n\nDECLARE_PER_CPU(struct task_struct *, ksoftirqd);\n\nstatic inline struct task_struct *this_cpu_ksoftirqd(void)\n{\n\treturn this_cpu_read(ksoftirqd);\n}\n\n \n\nstruct tasklet_struct\n{\n\tstruct tasklet_struct *next;\n\tunsigned long state;\n\tatomic_t count;\n\tbool use_callback;\n\tunion {\n\t\tvoid (*func)(unsigned long data);\n\t\tvoid (*callback)(struct tasklet_struct *t);\n\t};\n\tunsigned long data;\n};\n\n#define DECLARE_TASKLET(name, _callback)\t\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(0),\t\t\t\\\n\t.callback = _callback,\t\t\t\t\\\n\t.use_callback = true,\t\t\t\t\\\n}\n\n#define DECLARE_TASKLET_DISABLED(name, _callback)\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(1),\t\t\t\\\n\t.callback = _callback,\t\t\t\t\\\n\t.use_callback = true,\t\t\t\t\\\n}\n\n#define from_tasklet(var, callback_tasklet, tasklet_fieldname)\t\\\n\tcontainer_of(callback_tasklet, typeof(*var), tasklet_fieldname)\n\n#define DECLARE_TASKLET_OLD(name, _func)\t\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(0),\t\t\t\\\n\t.func = _func,\t\t\t\t\t\\\n}\n\n#define DECLARE_TASKLET_DISABLED_OLD(name, _func)\t\\\nstruct tasklet_struct name = {\t\t\t\t\\\n\t.count = ATOMIC_INIT(1),\t\t\t\\\n\t.func = _func,\t\t\t\t\t\\\n}\n\nenum\n{\n\tTASKLET_STATE_SCHED,\t \n\tTASKLET_STATE_RUN\t \n};\n\n#if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT_RT)\nstatic inline int tasklet_trylock(struct tasklet_struct *t)\n{\n\treturn !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);\n}\n\nvoid tasklet_unlock(struct tasklet_struct *t);\nvoid tasklet_unlock_wait(struct tasklet_struct *t);\nvoid tasklet_unlock_spin_wait(struct tasklet_struct *t);\n\n#else\nstatic inline int tasklet_trylock(struct tasklet_struct *t) { return 1; }\nstatic inline void tasklet_unlock(struct tasklet_struct *t) { }\nstatic inline void tasklet_unlock_wait(struct tasklet_struct *t) { }\nstatic inline void tasklet_unlock_spin_wait(struct tasklet_struct *t) { }\n#endif\n\nextern void __tasklet_schedule(struct tasklet_struct *t);\n\nstatic inline void tasklet_schedule(struct tasklet_struct *t)\n{\n\tif (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))\n\t\t__tasklet_schedule(t);\n}\n\nextern void __tasklet_hi_schedule(struct tasklet_struct *t);\n\nstatic inline void tasklet_hi_schedule(struct tasklet_struct *t)\n{\n\tif (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))\n\t\t__tasklet_hi_schedule(t);\n}\n\nstatic inline void tasklet_disable_nosync(struct tasklet_struct *t)\n{\n\tatomic_inc(&t->count);\n\tsmp_mb__after_atomic();\n}\n\n \nstatic inline void tasklet_disable_in_atomic(struct tasklet_struct *t)\n{\n\ttasklet_disable_nosync(t);\n\ttasklet_unlock_spin_wait(t);\n\tsmp_mb();\n}\n\nstatic inline void tasklet_disable(struct tasklet_struct *t)\n{\n\ttasklet_disable_nosync(t);\n\ttasklet_unlock_wait(t);\n\tsmp_mb();\n}\n\nstatic inline void tasklet_enable(struct tasklet_struct *t)\n{\n\tsmp_mb__before_atomic();\n\tatomic_dec(&t->count);\n}\n\nextern void tasklet_kill(struct tasklet_struct *t);\nextern void tasklet_init(struct tasklet_struct *t,\n\t\t\t void (*func)(unsigned long), unsigned long data);\nextern void tasklet_setup(struct tasklet_struct *t,\n\t\t\t  void (*callback)(struct tasklet_struct *));\n\n \n\n#if !defined(CONFIG_GENERIC_IRQ_PROBE) \nstatic inline unsigned long probe_irq_on(void)\n{\n\treturn 0;\n}\nstatic inline int probe_irq_off(unsigned long val)\n{\n\treturn 0;\n}\nstatic inline unsigned int probe_irq_mask(unsigned long val)\n{\n\treturn 0;\n}\n#else\nextern unsigned long probe_irq_on(void);\t \nextern int probe_irq_off(unsigned long);\t \nextern unsigned int probe_irq_mask(unsigned long);\t \n#endif\n\n#ifdef CONFIG_PROC_FS\n \nextern void init_irq_proc(void);\n#else\nstatic inline void init_irq_proc(void)\n{\n}\n#endif\n\n#ifdef CONFIG_IRQ_TIMINGS\nvoid irq_timings_enable(void);\nvoid irq_timings_disable(void);\nu64 irq_timings_next_event(u64 now);\n#endif\n\nstruct seq_file;\nint show_interrupts(struct seq_file *p, void *v);\nint arch_show_interrupts(struct seq_file *p, int prec);\n\nextern int early_irq_init(void);\nextern int arch_probe_nr_irqs(void);\nextern int arch_early_irq_init(void);\n\n \n#ifndef __irq_entry\n# define __irq_entry\t __section(\".irqentry.text\")\n#endif\n\n#define __softirq_entry  __section(\".softirqentry.text\")\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}