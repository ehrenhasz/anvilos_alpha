{
  "module_name": "bitops.h",
  "hash_id": "357833175bfc38f4d6b1d2f3060f4d435ccfc67bf0bafd94496167537e37aa58",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/bitops.h",
  "human_readable_source": " \n#ifndef _LINUX_BITOPS_H\n#define _LINUX_BITOPS_H\n\n#include <asm/types.h>\n#include <linux/bits.h>\n#include <linux/typecheck.h>\n\n#include <uapi/linux/kernel.h>\n\n \n#ifdef __LITTLE_ENDIAN\n#  define aligned_byte_mask(n) ((1UL << 8*(n))-1)\n#else\n#  define aligned_byte_mask(n) (~0xffUL << (BITS_PER_LONG - 8 - 8*(n)))\n#endif\n\n#define BITS_PER_TYPE(type)\t(sizeof(type) * BITS_PER_BYTE)\n#define BITS_TO_LONGS(nr)\t__KERNEL_DIV_ROUND_UP(nr, BITS_PER_TYPE(long))\n#define BITS_TO_U64(nr)\t\t__KERNEL_DIV_ROUND_UP(nr, BITS_PER_TYPE(u64))\n#define BITS_TO_U32(nr)\t\t__KERNEL_DIV_ROUND_UP(nr, BITS_PER_TYPE(u32))\n#define BITS_TO_BYTES(nr)\t__KERNEL_DIV_ROUND_UP(nr, BITS_PER_TYPE(char))\n\nextern unsigned int __sw_hweight8(unsigned int w);\nextern unsigned int __sw_hweight16(unsigned int w);\nextern unsigned int __sw_hweight32(unsigned int w);\nextern unsigned long __sw_hweight64(__u64 w);\n\n \n\n#include <asm-generic/bitops/generic-non-atomic.h>\n\n \n#define bitop(op, nr, addr)\t\t\t\t\t\t\\\n\t((__builtin_constant_p(nr) &&\t\t\t\t\t\\\n\t  __builtin_constant_p((uintptr_t)(addr) != (uintptr_t)NULL) &&\t\\\n\t  (uintptr_t)(addr) != (uintptr_t)NULL &&\t\t\t\\\n\t  __builtin_constant_p(*(const unsigned long *)(addr))) ?\t\\\n\t const##op(nr, addr) : op(nr, addr))\n\n#define __set_bit(nr, addr)\t\tbitop(___set_bit, nr, addr)\n#define __clear_bit(nr, addr)\t\tbitop(___clear_bit, nr, addr)\n#define __change_bit(nr, addr)\t\tbitop(___change_bit, nr, addr)\n#define __test_and_set_bit(nr, addr)\tbitop(___test_and_set_bit, nr, addr)\n#define __test_and_clear_bit(nr, addr)\tbitop(___test_and_clear_bit, nr, addr)\n#define __test_and_change_bit(nr, addr)\tbitop(___test_and_change_bit, nr, addr)\n#define test_bit(nr, addr)\t\tbitop(_test_bit, nr, addr)\n#define test_bit_acquire(nr, addr)\tbitop(_test_bit_acquire, nr, addr)\n\n \n#include <asm/bitops.h>\n\n \n#define __check_bitop_pr(name)\t\t\t\t\t\t\\\n\tstatic_assert(__same_type(arch_##name, generic_##name) &&\t\\\n\t\t      __same_type(const_##name, generic_##name) &&\t\\\n\t\t      __same_type(_##name, generic_##name))\n\n__check_bitop_pr(__set_bit);\n__check_bitop_pr(__clear_bit);\n__check_bitop_pr(__change_bit);\n__check_bitop_pr(__test_and_set_bit);\n__check_bitop_pr(__test_and_clear_bit);\n__check_bitop_pr(__test_and_change_bit);\n__check_bitop_pr(test_bit);\n\n#undef __check_bitop_pr\n\nstatic inline int get_bitmask_order(unsigned int count)\n{\n\tint order;\n\n\torder = fls(count);\n\treturn order;\t \n}\n\nstatic __always_inline unsigned long hweight_long(unsigned long w)\n{\n\treturn sizeof(w) == 4 ? hweight32(w) : hweight64((__u64)w);\n}\n\n \nstatic inline __u64 rol64(__u64 word, unsigned int shift)\n{\n\treturn (word << (shift & 63)) | (word >> ((-shift) & 63));\n}\n\n \nstatic inline __u64 ror64(__u64 word, unsigned int shift)\n{\n\treturn (word >> (shift & 63)) | (word << ((-shift) & 63));\n}\n\n \nstatic inline __u32 rol32(__u32 word, unsigned int shift)\n{\n\treturn (word << (shift & 31)) | (word >> ((-shift) & 31));\n}\n\n \nstatic inline __u32 ror32(__u32 word, unsigned int shift)\n{\n\treturn (word >> (shift & 31)) | (word << ((-shift) & 31));\n}\n\n \nstatic inline __u16 rol16(__u16 word, unsigned int shift)\n{\n\treturn (word << (shift & 15)) | (word >> ((-shift) & 15));\n}\n\n \nstatic inline __u16 ror16(__u16 word, unsigned int shift)\n{\n\treturn (word >> (shift & 15)) | (word << ((-shift) & 15));\n}\n\n \nstatic inline __u8 rol8(__u8 word, unsigned int shift)\n{\n\treturn (word << (shift & 7)) | (word >> ((-shift) & 7));\n}\n\n \nstatic inline __u8 ror8(__u8 word, unsigned int shift)\n{\n\treturn (word >> (shift & 7)) | (word << ((-shift) & 7));\n}\n\n \nstatic __always_inline __s32 sign_extend32(__u32 value, int index)\n{\n\t__u8 shift = 31 - index;\n\treturn (__s32)(value << shift) >> shift;\n}\n\n \nstatic __always_inline __s64 sign_extend64(__u64 value, int index)\n{\n\t__u8 shift = 63 - index;\n\treturn (__s64)(value << shift) >> shift;\n}\n\nstatic inline unsigned fls_long(unsigned long l)\n{\n\tif (sizeof(l) == 4)\n\t\treturn fls(l);\n\treturn fls64(l);\n}\n\nstatic inline int get_count_order(unsigned int count)\n{\n\tif (count == 0)\n\t\treturn -1;\n\n\treturn fls(--count);\n}\n\n \nstatic inline int get_count_order_long(unsigned long l)\n{\n\tif (l == 0UL)\n\t\treturn -1;\n\treturn (int)fls_long(--l);\n}\n\n \nstatic inline unsigned long __ffs64(u64 word)\n{\n#if BITS_PER_LONG == 32\n\tif (((u32)word) == 0UL)\n\t\treturn __ffs((u32)(word >> 32)) + 32;\n#elif BITS_PER_LONG != 64\n#error BITS_PER_LONG not 32 or 64\n#endif\n\treturn __ffs((unsigned long)word);\n}\n\n \nstatic inline unsigned long fns(unsigned long word, unsigned int n)\n{\n\tunsigned int bit;\n\n\twhile (word) {\n\t\tbit = __ffs(word);\n\t\tif (n-- == 0)\n\t\t\treturn bit;\n\t\t__clear_bit(bit, &word);\n\t}\n\n\treturn BITS_PER_LONG;\n}\n\n \nstatic __always_inline void assign_bit(long nr, volatile unsigned long *addr,\n\t\t\t\t       bool value)\n{\n\tif (value)\n\t\tset_bit(nr, addr);\n\telse\n\t\tclear_bit(nr, addr);\n}\n\nstatic __always_inline void __assign_bit(long nr, volatile unsigned long *addr,\n\t\t\t\t\t bool value)\n{\n\tif (value)\n\t\t__set_bit(nr, addr);\n\telse\n\t\t__clear_bit(nr, addr);\n}\n\n \n#define __ptr_set_bit(nr, addr)                         \\\n\t({                                              \\\n\t\ttypecheck_pointer(*(addr));             \\\n\t\t__set_bit(nr, (unsigned long *)(addr)); \\\n\t})\n\n \n#define __ptr_clear_bit(nr, addr)                         \\\n\t({                                                \\\n\t\ttypecheck_pointer(*(addr));               \\\n\t\t__clear_bit(nr, (unsigned long *)(addr)); \\\n\t})\n\n \n#define __ptr_test_bit(nr, addr)                       \\\n\t({                                             \\\n\t\ttypecheck_pointer(*(addr));            \\\n\t\ttest_bit(nr, (unsigned long *)(addr)); \\\n\t})\n\n#ifdef __KERNEL__\n\n#ifndef set_mask_bits\n#define set_mask_bits(ptr, mask, bits)\t\\\n({\t\t\t\t\t\t\t\t\\\n\tconst typeof(*(ptr)) mask__ = (mask), bits__ = (bits);\t\\\n\ttypeof(*(ptr)) old__, new__;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\told__ = READ_ONCE(*(ptr));\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tnew__ = (old__ & ~mask__) | bits__;\t\t\\\n\t} while (!try_cmpxchg(ptr, &old__, new__));\t\t\\\n\t\t\t\t\t\t\t\t\\\n\told__;\t\t\t\t\t\t\t\\\n})\n#endif\n\n#ifndef bit_clear_unless\n#define bit_clear_unless(ptr, clear, test)\t\\\n({\t\t\t\t\t\t\t\t\\\n\tconst typeof(*(ptr)) clear__ = (clear), test__ = (test);\\\n\ttypeof(*(ptr)) old__, new__;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\told__ = READ_ONCE(*(ptr));\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (old__ & test__)\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\\\n\t\tnew__ = old__ & ~clear__;\t\t\t\\\n\t} while (!try_cmpxchg(ptr, &old__, new__));\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t!(old__ & test__);\t\t\t\t\t\\\n})\n#endif\n\n#endif  \n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}