{
  "module_name": "bio.h",
  "hash_id": "528f2c9f5c8509c8be9b5a9160901f531614686ba070136b3506c2c16dc1b15a",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/bio.h",
  "human_readable_source": " \n \n#ifndef __LINUX_BIO_H\n#define __LINUX_BIO_H\n\n#include <linux/mempool.h>\n \n#include <linux/blk_types.h>\n#include <linux/uio.h>\n\n#define BIO_MAX_VECS\t\t256U\n\nstruct queue_limits;\n\nstatic inline unsigned int bio_max_segs(unsigned int nr_segs)\n{\n\treturn min(nr_segs, BIO_MAX_VECS);\n}\n\n#define bio_prio(bio)\t\t\t(bio)->bi_ioprio\n#define bio_set_prio(bio, prio)\t\t((bio)->bi_ioprio = prio)\n\n#define bio_iter_iovec(bio, iter)\t\t\t\t\\\n\tbvec_iter_bvec((bio)->bi_io_vec, (iter))\n\n#define bio_iter_page(bio, iter)\t\t\t\t\\\n\tbvec_iter_page((bio)->bi_io_vec, (iter))\n#define bio_iter_len(bio, iter)\t\t\t\t\t\\\n\tbvec_iter_len((bio)->bi_io_vec, (iter))\n#define bio_iter_offset(bio, iter)\t\t\t\t\\\n\tbvec_iter_offset((bio)->bi_io_vec, (iter))\n\n#define bio_page(bio)\t\tbio_iter_page((bio), (bio)->bi_iter)\n#define bio_offset(bio)\t\tbio_iter_offset((bio), (bio)->bi_iter)\n#define bio_iovec(bio)\t\tbio_iter_iovec((bio), (bio)->bi_iter)\n\n#define bvec_iter_sectors(iter)\t((iter).bi_size >> 9)\n#define bvec_iter_end_sector(iter) ((iter).bi_sector + bvec_iter_sectors((iter)))\n\n#define bio_sectors(bio)\tbvec_iter_sectors((bio)->bi_iter)\n#define bio_end_sector(bio)\tbvec_iter_end_sector((bio)->bi_iter)\n\n \n#define bio_data_dir(bio) \\\n\t(op_is_write(bio_op(bio)) ? WRITE : READ)\n\n \nstatic inline bool bio_has_data(struct bio *bio)\n{\n\tif (bio &&\n\t    bio->bi_iter.bi_size &&\n\t    bio_op(bio) != REQ_OP_DISCARD &&\n\t    bio_op(bio) != REQ_OP_SECURE_ERASE &&\n\t    bio_op(bio) != REQ_OP_WRITE_ZEROES)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool bio_no_advance_iter(const struct bio *bio)\n{\n\treturn bio_op(bio) == REQ_OP_DISCARD ||\n\t       bio_op(bio) == REQ_OP_SECURE_ERASE ||\n\t       bio_op(bio) == REQ_OP_WRITE_ZEROES;\n}\n\nstatic inline void *bio_data(struct bio *bio)\n{\n\tif (bio_has_data(bio))\n\t\treturn page_address(bio_page(bio)) + bio_offset(bio);\n\n\treturn NULL;\n}\n\nstatic inline bool bio_next_segment(const struct bio *bio,\n\t\t\t\t    struct bvec_iter_all *iter)\n{\n\tif (iter->idx >= bio->bi_vcnt)\n\t\treturn false;\n\n\tbvec_advance(&bio->bi_io_vec[iter->idx], iter);\n\treturn true;\n}\n\n \n#define bio_for_each_segment_all(bvl, bio, iter) \\\n\tfor (bvl = bvec_init_iter_all(&iter); bio_next_segment((bio), &iter); )\n\nstatic inline void bio_advance_iter(const struct bio *bio,\n\t\t\t\t    struct bvec_iter *iter, unsigned int bytes)\n{\n\titer->bi_sector += bytes >> 9;\n\n\tif (bio_no_advance_iter(bio))\n\t\titer->bi_size -= bytes;\n\telse\n\t\tbvec_iter_advance(bio->bi_io_vec, iter, bytes);\n\t\t \n}\n\n \nstatic inline void bio_advance_iter_single(const struct bio *bio,\n\t\t\t\t\t   struct bvec_iter *iter,\n\t\t\t\t\t   unsigned int bytes)\n{\n\titer->bi_sector += bytes >> 9;\n\n\tif (bio_no_advance_iter(bio))\n\t\titer->bi_size -= bytes;\n\telse\n\t\tbvec_iter_advance_single(bio->bi_io_vec, iter, bytes);\n}\n\nvoid __bio_advance(struct bio *, unsigned bytes);\n\n \nstatic inline void bio_advance(struct bio *bio, unsigned int nbytes)\n{\n\tif (nbytes == bio->bi_iter.bi_size) {\n\t\tbio->bi_iter.bi_size = 0;\n\t\treturn;\n\t}\n\t__bio_advance(bio, nbytes);\n}\n\n#define __bio_for_each_segment(bvl, bio, iter, start)\t\t\t\\\n\tfor (iter = (start);\t\t\t\t\t\t\\\n\t     (iter).bi_size &&\t\t\t\t\t\t\\\n\t\t((bvl = bio_iter_iovec((bio), (iter))), 1);\t\t\\\n\t     bio_advance_iter_single((bio), &(iter), (bvl).bv_len))\n\n#define bio_for_each_segment(bvl, bio, iter)\t\t\t\t\\\n\t__bio_for_each_segment(bvl, bio, iter, (bio)->bi_iter)\n\n#define __bio_for_each_bvec(bvl, bio, iter, start)\t\t\\\n\tfor (iter = (start);\t\t\t\t\t\t\\\n\t     (iter).bi_size &&\t\t\t\t\t\t\\\n\t\t((bvl = mp_bvec_iter_bvec((bio)->bi_io_vec, (iter))), 1); \\\n\t     bio_advance_iter_single((bio), &(iter), (bvl).bv_len))\n\n \n#define bio_for_each_bvec(bvl, bio, iter)\t\t\t\\\n\t__bio_for_each_bvec(bvl, bio, iter, (bio)->bi_iter)\n\n \n#define bio_for_each_bvec_all(bvl, bio, i)\t\t\\\n\tfor (i = 0, bvl = bio_first_bvec_all(bio);\t\\\n\t     i < (bio)->bi_vcnt; i++, bvl++)\n\n#define bio_iter_last(bvec, iter) ((iter).bi_size == (bvec).bv_len)\n\nstatic inline unsigned bio_segments(struct bio *bio)\n{\n\tunsigned segs = 0;\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\n\t \n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_SECURE_ERASE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn 0;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tbio_for_each_segment(bv, bio, iter)\n\t\tsegs++;\n\n\treturn segs;\n}\n\n \nstatic inline void bio_get(struct bio *bio)\n{\n\tbio->bi_flags |= (1 << BIO_REFFED);\n\tsmp_mb__before_atomic();\n\tatomic_inc(&bio->__bi_cnt);\n}\n\nstatic inline void bio_cnt_set(struct bio *bio, unsigned int count)\n{\n\tif (count != 1) {\n\t\tbio->bi_flags |= (1 << BIO_REFFED);\n\t\tsmp_mb();\n\t}\n\tatomic_set(&bio->__bi_cnt, count);\n}\n\nstatic inline bool bio_flagged(struct bio *bio, unsigned int bit)\n{\n\treturn bio->bi_flags & (1U << bit);\n}\n\nstatic inline void bio_set_flag(struct bio *bio, unsigned int bit)\n{\n\tbio->bi_flags |= (1U << bit);\n}\n\nstatic inline void bio_clear_flag(struct bio *bio, unsigned int bit)\n{\n\tbio->bi_flags &= ~(1U << bit);\n}\n\nstatic inline struct bio_vec *bio_first_bvec_all(struct bio *bio)\n{\n\tWARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));\n\treturn bio->bi_io_vec;\n}\n\nstatic inline struct page *bio_first_page_all(struct bio *bio)\n{\n\treturn bio_first_bvec_all(bio)->bv_page;\n}\n\nstatic inline struct folio *bio_first_folio_all(struct bio *bio)\n{\n\treturn page_folio(bio_first_page_all(bio));\n}\n\nstatic inline struct bio_vec *bio_last_bvec_all(struct bio *bio)\n{\n\tWARN_ON_ONCE(bio_flagged(bio, BIO_CLONED));\n\treturn &bio->bi_io_vec[bio->bi_vcnt - 1];\n}\n\n \nstruct folio_iter {\n\tstruct folio *folio;\n\tsize_t offset;\n\tsize_t length;\n\t \n\tstruct folio *_next;\n\tsize_t _seg_count;\n\tint _i;\n};\n\nstatic inline void bio_first_folio(struct folio_iter *fi, struct bio *bio,\n\t\t\t\t   int i)\n{\n\tstruct bio_vec *bvec = bio_first_bvec_all(bio) + i;\n\n\tif (unlikely(i >= bio->bi_vcnt)) {\n\t\tfi->folio = NULL;\n\t\treturn;\n\t}\n\n\tfi->folio = page_folio(bvec->bv_page);\n\tfi->offset = bvec->bv_offset +\n\t\t\tPAGE_SIZE * (bvec->bv_page - &fi->folio->page);\n\tfi->_seg_count = bvec->bv_len;\n\tfi->length = min(folio_size(fi->folio) - fi->offset, fi->_seg_count);\n\tfi->_next = folio_next(fi->folio);\n\tfi->_i = i;\n}\n\nstatic inline void bio_next_folio(struct folio_iter *fi, struct bio *bio)\n{\n\tfi->_seg_count -= fi->length;\n\tif (fi->_seg_count) {\n\t\tfi->folio = fi->_next;\n\t\tfi->offset = 0;\n\t\tfi->length = min(folio_size(fi->folio), fi->_seg_count);\n\t\tfi->_next = folio_next(fi->folio);\n\t} else {\n\t\tbio_first_folio(fi, bio, fi->_i + 1);\n\t}\n}\n\n \n#define bio_for_each_folio_all(fi, bio)\t\t\t\t\\\n\tfor (bio_first_folio(&fi, bio, 0); fi.folio; bio_next_folio(&fi, bio))\n\nenum bip_flags {\n\tBIP_BLOCK_INTEGRITY\t= 1 << 0,  \n\tBIP_MAPPED_INTEGRITY\t= 1 << 1,  \n\tBIP_CTRL_NOCHECK\t= 1 << 2,  \n\tBIP_DISK_NOCHECK\t= 1 << 3,  \n\tBIP_IP_CHECKSUM\t\t= 1 << 4,  \n};\n\n \nstruct bio_integrity_payload {\n\tstruct bio\t\t*bip_bio;\t \n\n\tstruct bvec_iter\tbip_iter;\n\n\tunsigned short\t\tbip_vcnt;\t \n\tunsigned short\t\tbip_max_vcnt;\t \n\tunsigned short\t\tbip_flags;\t \n\n\tstruct bvec_iter\tbio_iter;\t \n\n\tstruct work_struct\tbip_work;\t \n\n\tstruct bio_vec\t\t*bip_vec;\n\tstruct bio_vec\t\tbip_inline_vecs[]; \n};\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\nstatic inline struct bio_integrity_payload *bio_integrity(struct bio *bio)\n{\n\tif (bio->bi_opf & REQ_INTEGRITY)\n\t\treturn bio->bi_integrity;\n\n\treturn NULL;\n}\n\nstatic inline bool bio_integrity_flagged(struct bio *bio, enum bip_flags flag)\n{\n\tstruct bio_integrity_payload *bip = bio_integrity(bio);\n\n\tif (bip)\n\t\treturn bip->bip_flags & flag;\n\n\treturn false;\n}\n\nstatic inline sector_t bip_get_seed(struct bio_integrity_payload *bip)\n{\n\treturn bip->bip_iter.bi_sector;\n}\n\nstatic inline void bip_set_seed(struct bio_integrity_payload *bip,\n\t\t\t\tsector_t seed)\n{\n\tbip->bip_iter.bi_sector = seed;\n}\n\n#endif  \n\nvoid bio_trim(struct bio *bio, sector_t offset, sector_t size);\nextern struct bio *bio_split(struct bio *bio, int sectors,\n\t\t\t     gfp_t gfp, struct bio_set *bs);\nstruct bio *bio_split_rw(struct bio *bio, const struct queue_limits *lim,\n\t\tunsigned *segs, struct bio_set *bs, unsigned max_bytes);\n\n \nstatic inline struct bio *bio_next_split(struct bio *bio, int sectors,\n\t\t\t\t\t gfp_t gfp, struct bio_set *bs)\n{\n\tif (sectors >= bio_sectors(bio))\n\t\treturn bio;\n\n\treturn bio_split(bio, sectors, gfp, bs);\n}\n\nenum {\n\tBIOSET_NEED_BVECS = BIT(0),\n\tBIOSET_NEED_RESCUER = BIT(1),\n\tBIOSET_PERCPU_CACHE = BIT(2),\n};\nextern int bioset_init(struct bio_set *, unsigned int, unsigned int, int flags);\nextern void bioset_exit(struct bio_set *);\nextern int biovec_init_pool(mempool_t *pool, int pool_entries);\n\nstruct bio *bio_alloc_bioset(struct block_device *bdev, unsigned short nr_vecs,\n\t\t\t     blk_opf_t opf, gfp_t gfp_mask,\n\t\t\t     struct bio_set *bs);\nstruct bio *bio_kmalloc(unsigned short nr_vecs, gfp_t gfp_mask);\nextern void bio_put(struct bio *);\n\nstruct bio *bio_alloc_clone(struct block_device *bdev, struct bio *bio_src,\n\t\tgfp_t gfp, struct bio_set *bs);\nint bio_init_clone(struct block_device *bdev, struct bio *bio,\n\t\tstruct bio *bio_src, gfp_t gfp);\n\nextern struct bio_set fs_bio_set;\n\nstatic inline struct bio *bio_alloc(struct block_device *bdev,\n\t\tunsigned short nr_vecs, blk_opf_t opf, gfp_t gfp_mask)\n{\n\treturn bio_alloc_bioset(bdev, nr_vecs, opf, gfp_mask, &fs_bio_set);\n}\n\nvoid submit_bio(struct bio *bio);\n\nextern void bio_endio(struct bio *);\n\nstatic inline void bio_io_error(struct bio *bio)\n{\n\tbio->bi_status = BLK_STS_IOERR;\n\tbio_endio(bio);\n}\n\nstatic inline void bio_wouldblock_error(struct bio *bio)\n{\n\tbio_set_flag(bio, BIO_QUIET);\n\tbio->bi_status = BLK_STS_AGAIN;\n\tbio_endio(bio);\n}\n\n \nstatic inline int bio_iov_vecs_to_alloc(struct iov_iter *iter, int max_segs)\n{\n\tif (iov_iter_is_bvec(iter))\n\t\treturn 0;\n\treturn iov_iter_npages(iter, max_segs);\n}\n\nstruct request_queue;\n\nextern int submit_bio_wait(struct bio *bio);\nvoid bio_init(struct bio *bio, struct block_device *bdev, struct bio_vec *table,\n\t      unsigned short max_vecs, blk_opf_t opf);\nextern void bio_uninit(struct bio *);\nvoid bio_reset(struct bio *bio, struct block_device *bdev, blk_opf_t opf);\nvoid bio_chain(struct bio *, struct bio *);\n\nint __must_check bio_add_page(struct bio *bio, struct page *page, unsigned len,\n\t\t\t      unsigned off);\nbool __must_check bio_add_folio(struct bio *bio, struct folio *folio,\n\t\t\t\tsize_t len, size_t off);\nextern int bio_add_pc_page(struct request_queue *, struct bio *, struct page *,\n\t\t\t   unsigned int, unsigned int);\nint bio_add_zone_append_page(struct bio *bio, struct page *page,\n\t\t\t     unsigned int len, unsigned int offset);\nvoid __bio_add_page(struct bio *bio, struct page *page,\n\t\tunsigned int len, unsigned int off);\nvoid bio_add_folio_nofail(struct bio *bio, struct folio *folio, size_t len,\n\t\t\t  size_t off);\nint bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter);\nvoid bio_iov_bvec_set(struct bio *bio, struct iov_iter *iter);\nvoid __bio_release_pages(struct bio *bio, bool mark_dirty);\nextern void bio_set_pages_dirty(struct bio *bio);\nextern void bio_check_pages_dirty(struct bio *bio);\n\nextern void bio_copy_data_iter(struct bio *dst, struct bvec_iter *dst_iter,\n\t\t\t       struct bio *src, struct bvec_iter *src_iter);\nextern void bio_copy_data(struct bio *dst, struct bio *src);\nextern void bio_free_pages(struct bio *bio);\nvoid guard_bio_eod(struct bio *bio);\nvoid zero_fill_bio_iter(struct bio *bio, struct bvec_iter iter);\n\nstatic inline void zero_fill_bio(struct bio *bio)\n{\n\tzero_fill_bio_iter(bio, bio->bi_iter);\n}\n\nstatic inline void bio_release_pages(struct bio *bio, bool mark_dirty)\n{\n\tif (bio_flagged(bio, BIO_PAGE_PINNED))\n\t\t__bio_release_pages(bio, mark_dirty);\n}\n\n#define bio_dev(bio) \\\n\tdisk_devt((bio)->bi_bdev->bd_disk)\n\n#ifdef CONFIG_BLK_CGROUP\nvoid bio_associate_blkg(struct bio *bio);\nvoid bio_associate_blkg_from_css(struct bio *bio,\n\t\t\t\t struct cgroup_subsys_state *css);\nvoid bio_clone_blkg_association(struct bio *dst, struct bio *src);\nvoid blkcg_punt_bio_submit(struct bio *bio);\n#else\t \nstatic inline void bio_associate_blkg(struct bio *bio) { }\nstatic inline void bio_associate_blkg_from_css(struct bio *bio,\n\t\t\t\t\t       struct cgroup_subsys_state *css)\n{ }\nstatic inline void bio_clone_blkg_association(struct bio *dst,\n\t\t\t\t\t      struct bio *src) { }\nstatic inline void blkcg_punt_bio_submit(struct bio *bio)\n{\n\tsubmit_bio(bio);\n}\n#endif\t \n\nstatic inline void bio_set_dev(struct bio *bio, struct block_device *bdev)\n{\n\tbio_clear_flag(bio, BIO_REMAPPED);\n\tif (bio->bi_bdev != bdev)\n\t\tbio_clear_flag(bio, BIO_BPS_THROTTLED);\n\tbio->bi_bdev = bdev;\n\tbio_associate_blkg(bio);\n}\n\n \nstruct bio_list {\n\tstruct bio *head;\n\tstruct bio *tail;\n};\n\nstatic inline int bio_list_empty(const struct bio_list *bl)\n{\n\treturn bl->head == NULL;\n}\n\nstatic inline void bio_list_init(struct bio_list *bl)\n{\n\tbl->head = bl->tail = NULL;\n}\n\n#define BIO_EMPTY_LIST\t{ NULL, NULL }\n\n#define bio_list_for_each(bio, bl) \\\n\tfor (bio = (bl)->head; bio; bio = bio->bi_next)\n\nstatic inline unsigned bio_list_size(const struct bio_list *bl)\n{\n\tunsigned sz = 0;\n\tstruct bio *bio;\n\n\tbio_list_for_each(bio, bl)\n\t\tsz++;\n\n\treturn sz;\n}\n\nstatic inline void bio_list_add(struct bio_list *bl, struct bio *bio)\n{\n\tbio->bi_next = NULL;\n\n\tif (bl->tail)\n\t\tbl->tail->bi_next = bio;\n\telse\n\t\tbl->head = bio;\n\n\tbl->tail = bio;\n}\n\nstatic inline void bio_list_add_head(struct bio_list *bl, struct bio *bio)\n{\n\tbio->bi_next = bl->head;\n\n\tbl->head = bio;\n\n\tif (!bl->tail)\n\t\tbl->tail = bio;\n}\n\nstatic inline void bio_list_merge(struct bio_list *bl, struct bio_list *bl2)\n{\n\tif (!bl2->head)\n\t\treturn;\n\n\tif (bl->tail)\n\t\tbl->tail->bi_next = bl2->head;\n\telse\n\t\tbl->head = bl2->head;\n\n\tbl->tail = bl2->tail;\n}\n\nstatic inline void bio_list_merge_head(struct bio_list *bl,\n\t\t\t\t       struct bio_list *bl2)\n{\n\tif (!bl2->head)\n\t\treturn;\n\n\tif (bl->head)\n\t\tbl2->tail->bi_next = bl->head;\n\telse\n\t\tbl->tail = bl2->tail;\n\n\tbl->head = bl2->head;\n}\n\nstatic inline struct bio *bio_list_peek(struct bio_list *bl)\n{\n\treturn bl->head;\n}\n\nstatic inline struct bio *bio_list_pop(struct bio_list *bl)\n{\n\tstruct bio *bio = bl->head;\n\n\tif (bio) {\n\t\tbl->head = bl->head->bi_next;\n\t\tif (!bl->head)\n\t\t\tbl->tail = NULL;\n\n\t\tbio->bi_next = NULL;\n\t}\n\n\treturn bio;\n}\n\nstatic inline struct bio *bio_list_get(struct bio_list *bl)\n{\n\tstruct bio *bio = bl->head;\n\n\tbl->head = bl->tail = NULL;\n\n\treturn bio;\n}\n\n \nstatic inline void bio_inc_remaining(struct bio *bio)\n{\n\tbio_set_flag(bio, BIO_CHAIN);\n\tsmp_mb__before_atomic();\n\tatomic_inc(&bio->__bi_remaining);\n}\n\n \n#define BIO_POOL_SIZE 2\n\nstruct bio_set {\n\tstruct kmem_cache *bio_slab;\n\tunsigned int front_pad;\n\n\t \n\tstruct bio_alloc_cache __percpu *cache;\n\n\tmempool_t bio_pool;\n\tmempool_t bvec_pool;\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\tmempool_t bio_integrity_pool;\n\tmempool_t bvec_integrity_pool;\n#endif\n\n\tunsigned int back_pad;\n\t \n\tspinlock_t\t\trescue_lock;\n\tstruct bio_list\t\trescue_list;\n\tstruct work_struct\trescue_work;\n\tstruct workqueue_struct\t*rescue_workqueue;\n\n\t \n\tstruct hlist_node cpuhp_dead;\n};\n\nstatic inline bool bioset_initialized(struct bio_set *bs)\n{\n\treturn bs->bio_slab != NULL;\n}\n\n#if defined(CONFIG_BLK_DEV_INTEGRITY)\n\n#define bip_for_each_vec(bvl, bip, iter)\t\t\t\t\\\n\tfor_each_bvec(bvl, (bip)->bip_vec, iter, (bip)->bip_iter)\n\n#define bio_for_each_integrity_vec(_bvl, _bio, _iter)\t\t\t\\\n\tfor_each_bio(_bio)\t\t\t\t\t\t\\\n\t\tbip_for_each_vec(_bvl, _bio->bi_integrity, _iter)\n\nextern struct bio_integrity_payload *bio_integrity_alloc(struct bio *, gfp_t, unsigned int);\nextern int bio_integrity_add_page(struct bio *, struct page *, unsigned int, unsigned int);\nextern bool bio_integrity_prep(struct bio *);\nextern void bio_integrity_advance(struct bio *, unsigned int);\nextern void bio_integrity_trim(struct bio *);\nextern int bio_integrity_clone(struct bio *, struct bio *, gfp_t);\nextern int bioset_integrity_create(struct bio_set *, int);\nextern void bioset_integrity_free(struct bio_set *);\nextern void bio_integrity_init(void);\n\n#else  \n\nstatic inline void *bio_integrity(struct bio *bio)\n{\n\treturn NULL;\n}\n\nstatic inline int bioset_integrity_create(struct bio_set *bs, int pool_size)\n{\n\treturn 0;\n}\n\nstatic inline void bioset_integrity_free (struct bio_set *bs)\n{\n\treturn;\n}\n\nstatic inline bool bio_integrity_prep(struct bio *bio)\n{\n\treturn true;\n}\n\nstatic inline int bio_integrity_clone(struct bio *bio, struct bio *bio_src,\n\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn 0;\n}\n\nstatic inline void bio_integrity_advance(struct bio *bio,\n\t\t\t\t\t unsigned int bytes_done)\n{\n\treturn;\n}\n\nstatic inline void bio_integrity_trim(struct bio *bio)\n{\n\treturn;\n}\n\nstatic inline void bio_integrity_init(void)\n{\n\treturn;\n}\n\nstatic inline bool bio_integrity_flagged(struct bio *bio, enum bip_flags flag)\n{\n\treturn false;\n}\n\nstatic inline void *bio_integrity_alloc(struct bio * bio, gfp_t gfp,\n\t\t\t\t\t\t\t\tunsigned int nr)\n{\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic inline int bio_integrity_add_page(struct bio *bio, struct page *page,\n\t\t\t\t\tunsigned int len, unsigned int offset)\n{\n\treturn 0;\n}\n\n#endif  \n\n \nstatic inline void bio_set_polled(struct bio *bio, struct kiocb *kiocb)\n{\n\tbio->bi_opf |= REQ_POLLED;\n\tif (kiocb->ki_flags & IOCB_NOWAIT)\n\t\tbio->bi_opf |= REQ_NOWAIT;\n}\n\nstatic inline void bio_clear_polled(struct bio *bio)\n{\n\tbio->bi_opf &= ~REQ_POLLED;\n}\n\nstruct bio *blk_next_bio(struct bio *bio, struct block_device *bdev,\n\t\tunsigned int nr_pages, blk_opf_t opf, gfp_t gfp);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}