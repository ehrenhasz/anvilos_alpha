{
  "module_name": "nand.h",
  "hash_id": "db7a1ad72a173cf21d69e42a718ac27ece22768f9849d8d0f1167bba729f3074",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/mtd/nand.h",
  "human_readable_source": " \n \n\n#ifndef __LINUX_MTD_NAND_H\n#define __LINUX_MTD_NAND_H\n\n#include <linux/mtd/mtd.h>\n\nstruct nand_device;\n\n \nstruct nand_memory_organization {\n\tunsigned int bits_per_cell;\n\tunsigned int pagesize;\n\tunsigned int oobsize;\n\tunsigned int pages_per_eraseblock;\n\tunsigned int eraseblocks_per_lun;\n\tunsigned int max_bad_eraseblocks_per_lun;\n\tunsigned int planes_per_lun;\n\tunsigned int luns_per_target;\n\tunsigned int ntargets;\n};\n\n#define NAND_MEMORG(bpc, ps, os, ppe, epl, mbb, ppl, lpt, nt)\t\\\n\t{\t\t\t\t\t\t\t\\\n\t\t.bits_per_cell = (bpc),\t\t\t\t\\\n\t\t.pagesize = (ps),\t\t\t\t\\\n\t\t.oobsize = (os),\t\t\t\t\\\n\t\t.pages_per_eraseblock = (ppe),\t\t\t\\\n\t\t.eraseblocks_per_lun = (epl),\t\t\t\\\n\t\t.max_bad_eraseblocks_per_lun = (mbb),\t\t\\\n\t\t.planes_per_lun = (ppl),\t\t\t\\\n\t\t.luns_per_target = (lpt),\t\t\t\\\n\t\t.ntargets = (nt),\t\t\t\t\\\n\t}\n\n \nstruct nand_row_converter {\n\tunsigned int lun_addr_shift;\n\tunsigned int eraseblock_addr_shift;\n};\n\n \nstruct nand_pos {\n\tunsigned int target;\n\tunsigned int lun;\n\tunsigned int plane;\n\tunsigned int eraseblock;\n\tunsigned int page;\n};\n\n \nenum nand_page_io_req_type {\n\tNAND_PAGE_READ = 0,\n\tNAND_PAGE_WRITE,\n};\n\n \nstruct nand_page_io_req {\n\tenum nand_page_io_req_type type;\n\tstruct nand_pos pos;\n\tunsigned int dataoffs;\n\tunsigned int datalen;\n\tunion {\n\t\tconst void *out;\n\t\tvoid *in;\n\t} databuf;\n\tunsigned int ooboffs;\n\tunsigned int ooblen;\n\tunion {\n\t\tconst void *out;\n\t\tvoid *in;\n\t} oobbuf;\n\tint mode;\n};\n\nconst struct mtd_ooblayout_ops *nand_get_small_page_ooblayout(void);\nconst struct mtd_ooblayout_ops *nand_get_large_page_ooblayout(void);\nconst struct mtd_ooblayout_ops *nand_get_large_page_hamming_ooblayout(void);\n\n \nenum nand_ecc_engine_type {\n\tNAND_ECC_ENGINE_TYPE_INVALID,\n\tNAND_ECC_ENGINE_TYPE_NONE,\n\tNAND_ECC_ENGINE_TYPE_SOFT,\n\tNAND_ECC_ENGINE_TYPE_ON_HOST,\n\tNAND_ECC_ENGINE_TYPE_ON_DIE,\n};\n\n \nenum nand_ecc_placement {\n\tNAND_ECC_PLACEMENT_UNKNOWN,\n\tNAND_ECC_PLACEMENT_OOB,\n\tNAND_ECC_PLACEMENT_INTERLEAVED,\n};\n\n \nenum nand_ecc_algo {\n\tNAND_ECC_ALGO_UNKNOWN,\n\tNAND_ECC_ALGO_HAMMING,\n\tNAND_ECC_ALGO_BCH,\n\tNAND_ECC_ALGO_RS,\n};\n\n \nstruct nand_ecc_props {\n\tenum nand_ecc_engine_type engine_type;\n\tenum nand_ecc_placement placement;\n\tenum nand_ecc_algo algo;\n\tunsigned int strength;\n\tunsigned int step_size;\n\tunsigned int flags;\n};\n\n#define NAND_ECCREQ(str, stp) { .strength = (str), .step_size = (stp) }\n\n \n#define NAND_ECC_MAXIMIZE_STRENGTH BIT(0)\n\n \nstruct nand_bbt {\n\tunsigned long *cache;\n};\n\n \nstruct nand_ops {\n\tint (*erase)(struct nand_device *nand, const struct nand_pos *pos);\n\tint (*markbad)(struct nand_device *nand, const struct nand_pos *pos);\n\tbool (*isbad)(struct nand_device *nand, const struct nand_pos *pos);\n};\n\n \nstruct nand_ecc_context {\n\tstruct nand_ecc_props conf;\n\tunsigned int nsteps;\n\tunsigned int total;\n\tvoid *priv;\n};\n\n \nstruct nand_ecc_engine_ops {\n\tint (*init_ctx)(struct nand_device *nand);\n\tvoid (*cleanup_ctx)(struct nand_device *nand);\n\tint (*prepare_io_req)(struct nand_device *nand,\n\t\t\t      struct nand_page_io_req *req);\n\tint (*finish_io_req)(struct nand_device *nand,\n\t\t\t     struct nand_page_io_req *req);\n};\n\n \nenum nand_ecc_engine_integration {\n\tNAND_ECC_ENGINE_INTEGRATION_INVALID,\n\tNAND_ECC_ENGINE_INTEGRATION_PIPELINED,\n\tNAND_ECC_ENGINE_INTEGRATION_EXTERNAL,\n};\n\n \nstruct nand_ecc_engine {\n\tstruct device *dev;\n\tstruct list_head node;\n\tstruct nand_ecc_engine_ops *ops;\n\tenum nand_ecc_engine_integration integration;\n\tvoid *priv;\n};\n\nvoid of_get_nand_ecc_user_config(struct nand_device *nand);\nint nand_ecc_init_ctx(struct nand_device *nand);\nvoid nand_ecc_cleanup_ctx(struct nand_device *nand);\nint nand_ecc_prepare_io_req(struct nand_device *nand,\n\t\t\t    struct nand_page_io_req *req);\nint nand_ecc_finish_io_req(struct nand_device *nand,\n\t\t\t   struct nand_page_io_req *req);\nbool nand_ecc_is_strong_enough(struct nand_device *nand);\n\n#if IS_REACHABLE(CONFIG_MTD_NAND_CORE)\nint nand_ecc_register_on_host_hw_engine(struct nand_ecc_engine *engine);\nint nand_ecc_unregister_on_host_hw_engine(struct nand_ecc_engine *engine);\n#else\nstatic inline int\nnand_ecc_register_on_host_hw_engine(struct nand_ecc_engine *engine)\n{\n\treturn -ENOTSUPP;\n}\nstatic inline int\nnand_ecc_unregister_on_host_hw_engine(struct nand_ecc_engine *engine)\n{\n\treturn -ENOTSUPP;\n}\n#endif\n\nstruct nand_ecc_engine *nand_ecc_get_sw_engine(struct nand_device *nand);\nstruct nand_ecc_engine *nand_ecc_get_on_die_hw_engine(struct nand_device *nand);\nstruct nand_ecc_engine *nand_ecc_get_on_host_hw_engine(struct nand_device *nand);\nvoid nand_ecc_put_on_host_hw_engine(struct nand_device *nand);\nstruct device *nand_ecc_get_engine_dev(struct device *host);\n\n#if IS_ENABLED(CONFIG_MTD_NAND_ECC_SW_HAMMING)\nstruct nand_ecc_engine *nand_ecc_sw_hamming_get_engine(void);\n#else\nstatic inline struct nand_ecc_engine *nand_ecc_sw_hamming_get_engine(void)\n{\n\treturn NULL;\n}\n#endif  \n\n#if IS_ENABLED(CONFIG_MTD_NAND_ECC_SW_BCH)\nstruct nand_ecc_engine *nand_ecc_sw_bch_get_engine(void);\n#else\nstatic inline struct nand_ecc_engine *nand_ecc_sw_bch_get_engine(void)\n{\n\treturn NULL;\n}\n#endif  \n\n \nstruct nand_ecc_req_tweak_ctx {\n\tstruct nand_page_io_req orig_req;\n\tstruct nand_device *nand;\n\tunsigned int page_buffer_size;\n\tunsigned int oob_buffer_size;\n\tvoid *spare_databuf;\n\tvoid *spare_oobbuf;\n\tbool bounce_data;\n\tbool bounce_oob;\n};\n\nint nand_ecc_init_req_tweaking(struct nand_ecc_req_tweak_ctx *ctx,\n\t\t\t       struct nand_device *nand);\nvoid nand_ecc_cleanup_req_tweaking(struct nand_ecc_req_tweak_ctx *ctx);\nvoid nand_ecc_tweak_req(struct nand_ecc_req_tweak_ctx *ctx,\n\t\t\tstruct nand_page_io_req *req);\nvoid nand_ecc_restore_req(struct nand_ecc_req_tweak_ctx *ctx,\n\t\t\t  struct nand_page_io_req *req);\n\n \nstruct nand_ecc {\n\tstruct nand_ecc_props defaults;\n\tstruct nand_ecc_props requirements;\n\tstruct nand_ecc_props user_conf;\n\tstruct nand_ecc_context ctx;\n\tstruct nand_ecc_engine *ondie_engine;\n\tstruct nand_ecc_engine *engine;\n};\n\n \nstruct nand_device {\n\tstruct mtd_info mtd;\n\tstruct nand_memory_organization memorg;\n\tstruct nand_ecc ecc;\n\tstruct nand_row_converter rowconv;\n\tstruct nand_bbt bbt;\n\tconst struct nand_ops *ops;\n};\n\n \nstruct nand_io_iter {\n\tstruct nand_page_io_req req;\n\tunsigned int oobbytes_per_page;\n\tunsigned int dataleft;\n\tunsigned int oobleft;\n};\n\n \nstatic inline struct nand_device *mtd_to_nanddev(struct mtd_info *mtd)\n{\n\treturn container_of(mtd, struct nand_device, mtd);\n}\n\n \nstatic inline struct mtd_info *nanddev_to_mtd(struct nand_device *nand)\n{\n\treturn &nand->mtd;\n}\n\n \nstatic inline unsigned int nanddev_bits_per_cell(const struct nand_device *nand)\n{\n\treturn nand->memorg.bits_per_cell;\n}\n\n \nstatic inline size_t nanddev_page_size(const struct nand_device *nand)\n{\n\treturn nand->memorg.pagesize;\n}\n\n \nstatic inline unsigned int\nnanddev_per_page_oobsize(const struct nand_device *nand)\n{\n\treturn nand->memorg.oobsize;\n}\n\n \nstatic inline unsigned int\nnanddev_pages_per_eraseblock(const struct nand_device *nand)\n{\n\treturn nand->memorg.pages_per_eraseblock;\n}\n\n \nstatic inline unsigned int\nnanddev_pages_per_target(const struct nand_device *nand)\n{\n\treturn nand->memorg.pages_per_eraseblock *\n\t       nand->memorg.eraseblocks_per_lun *\n\t       nand->memorg.luns_per_target;\n}\n\n \nstatic inline size_t nanddev_eraseblock_size(const struct nand_device *nand)\n{\n\treturn nand->memorg.pagesize * nand->memorg.pages_per_eraseblock;\n}\n\n \nstatic inline unsigned int\nnanddev_eraseblocks_per_lun(const struct nand_device *nand)\n{\n\treturn nand->memorg.eraseblocks_per_lun;\n}\n\n \nstatic inline unsigned int\nnanddev_eraseblocks_per_target(const struct nand_device *nand)\n{\n\treturn nand->memorg.eraseblocks_per_lun * nand->memorg.luns_per_target;\n}\n\n \nstatic inline u64 nanddev_target_size(const struct nand_device *nand)\n{\n\treturn (u64)nand->memorg.luns_per_target *\n\t       nand->memorg.eraseblocks_per_lun *\n\t       nand->memorg.pages_per_eraseblock *\n\t       nand->memorg.pagesize;\n}\n\n \nstatic inline unsigned int nanddev_ntargets(const struct nand_device *nand)\n{\n\treturn nand->memorg.ntargets;\n}\n\n \nstatic inline unsigned int nanddev_neraseblocks(const struct nand_device *nand)\n{\n\treturn nand->memorg.ntargets * nand->memorg.luns_per_target *\n\t       nand->memorg.eraseblocks_per_lun;\n}\n\n \nstatic inline u64 nanddev_size(const struct nand_device *nand)\n{\n\treturn nanddev_target_size(nand) * nanddev_ntargets(nand);\n}\n\n \nstatic inline struct nand_memory_organization *\nnanddev_get_memorg(struct nand_device *nand)\n{\n\treturn &nand->memorg;\n}\n\n \nstatic inline const struct nand_ecc_props *\nnanddev_get_ecc_conf(struct nand_device *nand)\n{\n\treturn &nand->ecc.ctx.conf;\n}\n\n \nstatic inline unsigned int\nnanddev_get_ecc_nsteps(struct nand_device *nand)\n{\n\treturn nand->ecc.ctx.nsteps;\n}\n\n \nstatic inline unsigned int\nnanddev_get_ecc_bytes_per_step(struct nand_device *nand)\n{\n\treturn nand->ecc.ctx.total / nand->ecc.ctx.nsteps;\n}\n\n \nstatic inline const struct nand_ecc_props *\nnanddev_get_ecc_requirements(struct nand_device *nand)\n{\n\treturn &nand->ecc.requirements;\n}\n\n \nstatic inline void\nnanddev_set_ecc_requirements(struct nand_device *nand,\n\t\t\t     const struct nand_ecc_props *reqs)\n{\n\tnand->ecc.requirements = *reqs;\n}\n\nint nanddev_init(struct nand_device *nand, const struct nand_ops *ops,\n\t\t struct module *owner);\nvoid nanddev_cleanup(struct nand_device *nand);\n\n \nstatic inline int nanddev_register(struct nand_device *nand)\n{\n\treturn mtd_device_register(&nand->mtd, NULL, 0);\n}\n\n \nstatic inline int nanddev_unregister(struct nand_device *nand)\n{\n\treturn mtd_device_unregister(&nand->mtd);\n}\n\n \nstatic inline void nanddev_set_of_node(struct nand_device *nand,\n\t\t\t\t       struct device_node *np)\n{\n\tmtd_set_of_node(&nand->mtd, np);\n}\n\n \nstatic inline struct device_node *nanddev_get_of_node(struct nand_device *nand)\n{\n\treturn mtd_get_of_node(&nand->mtd);\n}\n\n \nstatic inline unsigned int nanddev_offs_to_pos(struct nand_device *nand,\n\t\t\t\t\t       loff_t offs,\n\t\t\t\t\t       struct nand_pos *pos)\n{\n\tunsigned int pageoffs;\n\tu64 tmp = offs;\n\n\tpageoffs = do_div(tmp, nand->memorg.pagesize);\n\tpos->page = do_div(tmp, nand->memorg.pages_per_eraseblock);\n\tpos->eraseblock = do_div(tmp, nand->memorg.eraseblocks_per_lun);\n\tpos->plane = pos->eraseblock % nand->memorg.planes_per_lun;\n\tpos->lun = do_div(tmp, nand->memorg.luns_per_target);\n\tpos->target = tmp;\n\n\treturn pageoffs;\n}\n\n \nstatic inline int nanddev_pos_cmp(const struct nand_pos *a,\n\t\t\t\t  const struct nand_pos *b)\n{\n\tif (a->target != b->target)\n\t\treturn a->target < b->target ? -1 : 1;\n\n\tif (a->lun != b->lun)\n\t\treturn a->lun < b->lun ? -1 : 1;\n\n\tif (a->eraseblock != b->eraseblock)\n\t\treturn a->eraseblock < b->eraseblock ? -1 : 1;\n\n\tif (a->page != b->page)\n\t\treturn a->page < b->page ? -1 : 1;\n\n\treturn 0;\n}\n\n \nstatic inline loff_t nanddev_pos_to_offs(struct nand_device *nand,\n\t\t\t\t\t const struct nand_pos *pos)\n{\n\tunsigned int npages;\n\n\tnpages = pos->page +\n\t\t ((pos->eraseblock +\n\t\t   (pos->lun +\n\t\t    (pos->target * nand->memorg.luns_per_target)) *\n\t\t   nand->memorg.eraseblocks_per_lun) *\n\t\t  nand->memorg.pages_per_eraseblock);\n\n\treturn (loff_t)npages * nand->memorg.pagesize;\n}\n\n \nstatic inline unsigned int nanddev_pos_to_row(struct nand_device *nand,\n\t\t\t\t\t      const struct nand_pos *pos)\n{\n\treturn (pos->lun << nand->rowconv.lun_addr_shift) |\n\t       (pos->eraseblock << nand->rowconv.eraseblock_addr_shift) |\n\t       pos->page;\n}\n\n \nstatic inline void nanddev_pos_next_target(struct nand_device *nand,\n\t\t\t\t\t   struct nand_pos *pos)\n{\n\tpos->page = 0;\n\tpos->plane = 0;\n\tpos->eraseblock = 0;\n\tpos->lun = 0;\n\tpos->target++;\n}\n\n \nstatic inline void nanddev_pos_next_lun(struct nand_device *nand,\n\t\t\t\t\tstruct nand_pos *pos)\n{\n\tif (pos->lun >= nand->memorg.luns_per_target - 1)\n\t\treturn nanddev_pos_next_target(nand, pos);\n\n\tpos->lun++;\n\tpos->page = 0;\n\tpos->plane = 0;\n\tpos->eraseblock = 0;\n}\n\n \nstatic inline void nanddev_pos_next_eraseblock(struct nand_device *nand,\n\t\t\t\t\t       struct nand_pos *pos)\n{\n\tif (pos->eraseblock >= nand->memorg.eraseblocks_per_lun - 1)\n\t\treturn nanddev_pos_next_lun(nand, pos);\n\n\tpos->eraseblock++;\n\tpos->page = 0;\n\tpos->plane = pos->eraseblock % nand->memorg.planes_per_lun;\n}\n\n \nstatic inline void nanddev_pos_next_page(struct nand_device *nand,\n\t\t\t\t\t struct nand_pos *pos)\n{\n\tif (pos->page >= nand->memorg.pages_per_eraseblock - 1)\n\t\treturn nanddev_pos_next_eraseblock(nand, pos);\n\n\tpos->page++;\n}\n\n \nstatic inline void nanddev_io_iter_init(struct nand_device *nand,\n\t\t\t\t\tenum nand_page_io_req_type reqtype,\n\t\t\t\t\tloff_t offs, struct mtd_oob_ops *req,\n\t\t\t\t\tstruct nand_io_iter *iter)\n{\n\tstruct mtd_info *mtd = nanddev_to_mtd(nand);\n\n\titer->req.type = reqtype;\n\titer->req.mode = req->mode;\n\titer->req.dataoffs = nanddev_offs_to_pos(nand, offs, &iter->req.pos);\n\titer->req.ooboffs = req->ooboffs;\n\titer->oobbytes_per_page = mtd_oobavail(mtd, req);\n\titer->dataleft = req->len;\n\titer->oobleft = req->ooblen;\n\titer->req.databuf.in = req->datbuf;\n\titer->req.datalen = min_t(unsigned int,\n\t\t\t\t  nand->memorg.pagesize - iter->req.dataoffs,\n\t\t\t\t  iter->dataleft);\n\titer->req.oobbuf.in = req->oobbuf;\n\titer->req.ooblen = min_t(unsigned int,\n\t\t\t\t iter->oobbytes_per_page - iter->req.ooboffs,\n\t\t\t\t iter->oobleft);\n}\n\n \nstatic inline void nanddev_io_iter_next_page(struct nand_device *nand,\n\t\t\t\t\t     struct nand_io_iter *iter)\n{\n\tnanddev_pos_next_page(nand, &iter->req.pos);\n\titer->dataleft -= iter->req.datalen;\n\titer->req.databuf.in += iter->req.datalen;\n\titer->oobleft -= iter->req.ooblen;\n\titer->req.oobbuf.in += iter->req.ooblen;\n\titer->req.dataoffs = 0;\n\titer->req.ooboffs = 0;\n\titer->req.datalen = min_t(unsigned int, nand->memorg.pagesize,\n\t\t\t\t  iter->dataleft);\n\titer->req.ooblen = min_t(unsigned int, iter->oobbytes_per_page,\n\t\t\t\t iter->oobleft);\n}\n\n \nstatic inline bool nanddev_io_iter_end(struct nand_device *nand,\n\t\t\t\t       const struct nand_io_iter *iter)\n{\n\tif (iter->dataleft || iter->oobleft)\n\t\treturn false;\n\n\treturn true;\n}\n\n \n#define nanddev_io_for_each_page(nand, type, start, req, iter)\t\t\\\n\tfor (nanddev_io_iter_init(nand, type, start, req, iter);\t\\\n\t     !nanddev_io_iter_end(nand, iter);\t\t\t\t\\\n\t     nanddev_io_iter_next_page(nand, iter))\n\nbool nanddev_isbad(struct nand_device *nand, const struct nand_pos *pos);\nbool nanddev_isreserved(struct nand_device *nand, const struct nand_pos *pos);\nint nanddev_markbad(struct nand_device *nand, const struct nand_pos *pos);\n\n \nint nanddev_ecc_engine_init(struct nand_device *nand);\nvoid nanddev_ecc_engine_cleanup(struct nand_device *nand);\n\nstatic inline void *nand_to_ecc_ctx(struct nand_device *nand)\n{\n\treturn nand->ecc.ctx.priv;\n}\n\n \nenum nand_bbt_block_status {\n\tNAND_BBT_BLOCK_STATUS_UNKNOWN,\n\tNAND_BBT_BLOCK_GOOD,\n\tNAND_BBT_BLOCK_WORN,\n\tNAND_BBT_BLOCK_RESERVED,\n\tNAND_BBT_BLOCK_FACTORY_BAD,\n\tNAND_BBT_BLOCK_NUM_STATUS,\n};\n\nint nanddev_bbt_init(struct nand_device *nand);\nvoid nanddev_bbt_cleanup(struct nand_device *nand);\nint nanddev_bbt_update(struct nand_device *nand);\nint nanddev_bbt_get_block_status(const struct nand_device *nand,\n\t\t\t\t unsigned int entry);\nint nanddev_bbt_set_block_status(struct nand_device *nand, unsigned int entry,\n\t\t\t\t enum nand_bbt_block_status status);\nint nanddev_bbt_markbad(struct nand_device *nand, unsigned int block);\n\n \nstatic inline unsigned int nanddev_bbt_pos_to_entry(struct nand_device *nand,\n\t\t\t\t\t\t    const struct nand_pos *pos)\n{\n\treturn pos->eraseblock +\n\t       ((pos->lun + (pos->target * nand->memorg.luns_per_target)) *\n\t\tnand->memorg.eraseblocks_per_lun);\n}\n\n \nstatic inline bool nanddev_bbt_is_initialized(struct nand_device *nand)\n{\n\treturn !!nand->bbt.cache;\n}\n\n \nint nanddev_mtd_erase(struct mtd_info *mtd, struct erase_info *einfo);\nint nanddev_mtd_max_bad_blocks(struct mtd_info *mtd, loff_t offs, size_t len);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}