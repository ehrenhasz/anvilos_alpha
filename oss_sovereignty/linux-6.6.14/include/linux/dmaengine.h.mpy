{
  "module_name": "dmaengine.h",
  "hash_id": "618c9813a6edbd54941f24b712e5e37b6948bea9fdb65c21af323b2642015a80",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/dmaengine.h",
  "human_readable_source": " \n \n#ifndef LINUX_DMAENGINE_H\n#define LINUX_DMAENGINE_H\n\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/uio.h>\n#include <linux/bug.h>\n#include <linux/scatterlist.h>\n#include <linux/bitmap.h>\n#include <linux/types.h>\n#include <asm/page.h>\n\n \ntypedef s32 dma_cookie_t;\n#define DMA_MIN_COOKIE\t1\n\nstatic inline int dma_submit_error(dma_cookie_t cookie)\n{\n\treturn cookie < 0 ? cookie : 0;\n}\n\n \nenum dma_status {\n\tDMA_COMPLETE,\n\tDMA_IN_PROGRESS,\n\tDMA_PAUSED,\n\tDMA_ERROR,\n\tDMA_OUT_OF_ORDER,\n};\n\n \nenum dma_transaction_type {\n\tDMA_MEMCPY,\n\tDMA_XOR,\n\tDMA_PQ,\n\tDMA_XOR_VAL,\n\tDMA_PQ_VAL,\n\tDMA_MEMSET,\n\tDMA_MEMSET_SG,\n\tDMA_INTERRUPT,\n\tDMA_PRIVATE,\n\tDMA_ASYNC_TX,\n\tDMA_SLAVE,\n\tDMA_CYCLIC,\n\tDMA_INTERLEAVE,\n\tDMA_COMPLETION_NO_ORDER,\n\tDMA_REPEAT,\n\tDMA_LOAD_EOT,\n \n\tDMA_TX_TYPE_END,\n};\n\n \nenum dma_transfer_direction {\n\tDMA_MEM_TO_MEM,\n\tDMA_MEM_TO_DEV,\n\tDMA_DEV_TO_MEM,\n\tDMA_DEV_TO_DEV,\n\tDMA_TRANS_NONE,\n};\n\n \n\n \nstruct data_chunk {\n\tsize_t size;\n\tsize_t icg;\n\tsize_t dst_icg;\n\tsize_t src_icg;\n};\n\n \nstruct dma_interleaved_template {\n\tdma_addr_t src_start;\n\tdma_addr_t dst_start;\n\tenum dma_transfer_direction dir;\n\tbool src_inc;\n\tbool dst_inc;\n\tbool src_sgl;\n\tbool dst_sgl;\n\tsize_t numf;\n\tsize_t frame_size;\n\tstruct data_chunk sgl[];\n};\n\n \nenum dma_ctrl_flags {\n\tDMA_PREP_INTERRUPT = (1 << 0),\n\tDMA_CTRL_ACK = (1 << 1),\n\tDMA_PREP_PQ_DISABLE_P = (1 << 2),\n\tDMA_PREP_PQ_DISABLE_Q = (1 << 3),\n\tDMA_PREP_CONTINUE = (1 << 4),\n\tDMA_PREP_FENCE = (1 << 5),\n\tDMA_CTRL_REUSE = (1 << 6),\n\tDMA_PREP_CMD = (1 << 7),\n\tDMA_PREP_REPEAT = (1 << 8),\n\tDMA_PREP_LOAD_EOT = (1 << 9),\n};\n\n \nenum sum_check_bits {\n\tSUM_CHECK_P = 0,\n\tSUM_CHECK_Q = 1,\n};\n\n \nenum sum_check_flags {\n\tSUM_CHECK_P_RESULT = (1 << SUM_CHECK_P),\n\tSUM_CHECK_Q_RESULT = (1 << SUM_CHECK_Q),\n};\n\n\n \ntypedef struct { DECLARE_BITMAP(bits, DMA_TX_TYPE_END); } dma_cap_mask_t;\n\n \nenum dma_desc_metadata_mode {\n\tDESC_METADATA_NONE = 0,\n\tDESC_METADATA_CLIENT = BIT(0),\n\tDESC_METADATA_ENGINE = BIT(1),\n};\n\n \nstruct dma_chan_percpu {\n\t \n\tunsigned long memcpy_count;\n\tunsigned long bytes_transferred;\n};\n\n \nstruct dma_router {\n\tstruct device *dev;\n\tvoid (*route_free)(struct device *dev, void *route_data);\n};\n\n \nstruct dma_chan {\n\tstruct dma_device *device;\n\tstruct device *slave;\n\tdma_cookie_t cookie;\n\tdma_cookie_t completed_cookie;\n\n\t \n\tint chan_id;\n\tstruct dma_chan_dev *dev;\n\tconst char *name;\n#ifdef CONFIG_DEBUG_FS\n\tchar *dbg_client_name;\n#endif\n\n\tstruct list_head device_node;\n\tstruct dma_chan_percpu __percpu *local;\n\tint client_count;\n\tint table_count;\n\n\t \n\tstruct dma_router *router;\n\tvoid *route_data;\n\n\tvoid *private;\n};\n\n \nstruct dma_chan_dev {\n\tstruct dma_chan *chan;\n\tstruct device device;\n\tint dev_id;\n\tbool chan_dma_dev;\n};\n\n \nenum dma_slave_buswidth {\n\tDMA_SLAVE_BUSWIDTH_UNDEFINED = 0,\n\tDMA_SLAVE_BUSWIDTH_1_BYTE = 1,\n\tDMA_SLAVE_BUSWIDTH_2_BYTES = 2,\n\tDMA_SLAVE_BUSWIDTH_3_BYTES = 3,\n\tDMA_SLAVE_BUSWIDTH_4_BYTES = 4,\n\tDMA_SLAVE_BUSWIDTH_8_BYTES = 8,\n\tDMA_SLAVE_BUSWIDTH_16_BYTES = 16,\n\tDMA_SLAVE_BUSWIDTH_32_BYTES = 32,\n\tDMA_SLAVE_BUSWIDTH_64_BYTES = 64,\n\tDMA_SLAVE_BUSWIDTH_128_BYTES = 128,\n};\n\n \nstruct dma_slave_config {\n\tenum dma_transfer_direction direction;\n\tphys_addr_t src_addr;\n\tphys_addr_t dst_addr;\n\tenum dma_slave_buswidth src_addr_width;\n\tenum dma_slave_buswidth dst_addr_width;\n\tu32 src_maxburst;\n\tu32 dst_maxburst;\n\tu32 src_port_window_size;\n\tu32 dst_port_window_size;\n\tbool device_fc;\n\tvoid *peripheral_config;\n\tsize_t peripheral_size;\n};\n\n \nenum dma_residue_granularity {\n\tDMA_RESIDUE_GRANULARITY_DESCRIPTOR = 0,\n\tDMA_RESIDUE_GRANULARITY_SEGMENT = 1,\n\tDMA_RESIDUE_GRANULARITY_BURST = 2,\n};\n\n \nstruct dma_slave_caps {\n\tu32 src_addr_widths;\n\tu32 dst_addr_widths;\n\tu32 directions;\n\tu32 min_burst;\n\tu32 max_burst;\n\tu32 max_sg_burst;\n\tbool cmd_pause;\n\tbool cmd_resume;\n\tbool cmd_terminate;\n\tenum dma_residue_granularity residue_granularity;\n\tbool descriptor_reuse;\n};\n\nstatic inline const char *dma_chan_name(struct dma_chan *chan)\n{\n\treturn dev_name(&chan->dev->device);\n}\n\nvoid dma_chan_cleanup(struct kref *kref);\n\n \ntypedef bool (*dma_filter_fn)(struct dma_chan *chan, void *filter_param);\n\ntypedef void (*dma_async_tx_callback)(void *dma_async_param);\n\nenum dmaengine_tx_result {\n\tDMA_TRANS_NOERROR = 0,\t\t \n\tDMA_TRANS_READ_FAILED,\t\t \n\tDMA_TRANS_WRITE_FAILED,\t\t \n\tDMA_TRANS_ABORTED,\t\t \n};\n\nstruct dmaengine_result {\n\tenum dmaengine_tx_result result;\n\tu32 residue;\n};\n\ntypedef void (*dma_async_tx_callback_result)(void *dma_async_param,\n\t\t\t\tconst struct dmaengine_result *result);\n\nstruct dmaengine_unmap_data {\n#if IS_ENABLED(CONFIG_DMA_ENGINE_RAID)\n\tu16 map_cnt;\n#else\n\tu8 map_cnt;\n#endif\n\tu8 to_cnt;\n\tu8 from_cnt;\n\tu8 bidi_cnt;\n\tstruct device *dev;\n\tstruct kref kref;\n\tsize_t len;\n\tdma_addr_t addr[];\n};\n\nstruct dma_async_tx_descriptor;\n\nstruct dma_descriptor_metadata_ops {\n\tint (*attach)(struct dma_async_tx_descriptor *desc, void *data,\n\t\t      size_t len);\n\n\tvoid *(*get_ptr)(struct dma_async_tx_descriptor *desc,\n\t\t\t size_t *payload_len, size_t *max_len);\n\tint (*set_len)(struct dma_async_tx_descriptor *desc,\n\t\t       size_t payload_len);\n};\n\n \nstruct dma_async_tx_descriptor {\n\tdma_cookie_t cookie;\n\tenum dma_ctrl_flags flags;  \n\tdma_addr_t phys;\n\tstruct dma_chan *chan;\n\tdma_cookie_t (*tx_submit)(struct dma_async_tx_descriptor *tx);\n\tint (*desc_free)(struct dma_async_tx_descriptor *tx);\n\tdma_async_tx_callback callback;\n\tdma_async_tx_callback_result callback_result;\n\tvoid *callback_param;\n\tstruct dmaengine_unmap_data *unmap;\n\tenum dma_desc_metadata_mode desc_metadata_mode;\n\tstruct dma_descriptor_metadata_ops *metadata_ops;\n#ifdef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH\n\tstruct dma_async_tx_descriptor *next;\n\tstruct dma_async_tx_descriptor *parent;\n\tspinlock_t lock;\n#endif\n};\n\n#ifdef CONFIG_DMA_ENGINE\nstatic inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,\n\t\t\t\t struct dmaengine_unmap_data *unmap)\n{\n\tkref_get(&unmap->kref);\n\ttx->unmap = unmap;\n}\n\nstruct dmaengine_unmap_data *\ndmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags);\nvoid dmaengine_unmap_put(struct dmaengine_unmap_data *unmap);\n#else\nstatic inline void dma_set_unmap(struct dma_async_tx_descriptor *tx,\n\t\t\t\t struct dmaengine_unmap_data *unmap)\n{\n}\nstatic inline struct dmaengine_unmap_data *\ndmaengine_get_unmap_data(struct device *dev, int nr, gfp_t flags)\n{\n\treturn NULL;\n}\nstatic inline void dmaengine_unmap_put(struct dmaengine_unmap_data *unmap)\n{\n}\n#endif\n\nstatic inline void dma_descriptor_unmap(struct dma_async_tx_descriptor *tx)\n{\n\tif (!tx->unmap)\n\t\treturn;\n\n\tdmaengine_unmap_put(tx->unmap);\n\ttx->unmap = NULL;\n}\n\n#ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH\nstatic inline void txd_lock(struct dma_async_tx_descriptor *txd)\n{\n}\nstatic inline void txd_unlock(struct dma_async_tx_descriptor *txd)\n{\n}\nstatic inline void txd_chain(struct dma_async_tx_descriptor *txd, struct dma_async_tx_descriptor *next)\n{\n\tBUG();\n}\nstatic inline void txd_clear_parent(struct dma_async_tx_descriptor *txd)\n{\n}\nstatic inline void txd_clear_next(struct dma_async_tx_descriptor *txd)\n{\n}\nstatic inline struct dma_async_tx_descriptor *txd_next(struct dma_async_tx_descriptor *txd)\n{\n\treturn NULL;\n}\nstatic inline struct dma_async_tx_descriptor *txd_parent(struct dma_async_tx_descriptor *txd)\n{\n\treturn NULL;\n}\n\n#else\nstatic inline void txd_lock(struct dma_async_tx_descriptor *txd)\n{\n\tspin_lock_bh(&txd->lock);\n}\nstatic inline void txd_unlock(struct dma_async_tx_descriptor *txd)\n{\n\tspin_unlock_bh(&txd->lock);\n}\nstatic inline void txd_chain(struct dma_async_tx_descriptor *txd, struct dma_async_tx_descriptor *next)\n{\n\ttxd->next = next;\n\tnext->parent = txd;\n}\nstatic inline void txd_clear_parent(struct dma_async_tx_descriptor *txd)\n{\n\ttxd->parent = NULL;\n}\nstatic inline void txd_clear_next(struct dma_async_tx_descriptor *txd)\n{\n\ttxd->next = NULL;\n}\nstatic inline struct dma_async_tx_descriptor *txd_parent(struct dma_async_tx_descriptor *txd)\n{\n\treturn txd->parent;\n}\nstatic inline struct dma_async_tx_descriptor *txd_next(struct dma_async_tx_descriptor *txd)\n{\n\treturn txd->next;\n}\n#endif\n\n \nstruct dma_tx_state {\n\tdma_cookie_t last;\n\tdma_cookie_t used;\n\tu32 residue;\n\tu32 in_flight_bytes;\n};\n\n \nenum dmaengine_alignment {\n\tDMAENGINE_ALIGN_1_BYTE = 0,\n\tDMAENGINE_ALIGN_2_BYTES = 1,\n\tDMAENGINE_ALIGN_4_BYTES = 2,\n\tDMAENGINE_ALIGN_8_BYTES = 3,\n\tDMAENGINE_ALIGN_16_BYTES = 4,\n\tDMAENGINE_ALIGN_32_BYTES = 5,\n\tDMAENGINE_ALIGN_64_BYTES = 6,\n\tDMAENGINE_ALIGN_128_BYTES = 7,\n\tDMAENGINE_ALIGN_256_BYTES = 8,\n};\n\n \nstruct dma_slave_map {\n\tconst char *devname;\n\tconst char *slave;\n\tvoid *param;\n};\n\n \nstruct dma_filter {\n\tdma_filter_fn fn;\n\tint mapcnt;\n\tconst struct dma_slave_map *map;\n};\n\n \nstruct dma_device {\n\tstruct kref ref;\n\tunsigned int chancnt;\n\tunsigned int privatecnt;\n\tstruct list_head channels;\n\tstruct list_head global_node;\n\tstruct dma_filter filter;\n\tdma_cap_mask_t cap_mask;\n\tenum dma_desc_metadata_mode desc_metadata_modes;\n\tunsigned short max_xor;\n\tunsigned short max_pq;\n\tenum dmaengine_alignment copy_align;\n\tenum dmaengine_alignment xor_align;\n\tenum dmaengine_alignment pq_align;\n\tenum dmaengine_alignment fill_align;\n\t#define DMA_HAS_PQ_CONTINUE (1 << 15)\n\n\tint dev_id;\n\tstruct device *dev;\n\tstruct module *owner;\n\tstruct ida chan_ida;\n\n\tu32 src_addr_widths;\n\tu32 dst_addr_widths;\n\tu32 directions;\n\tu32 min_burst;\n\tu32 max_burst;\n\tu32 max_sg_burst;\n\tbool descriptor_reuse;\n\tenum dma_residue_granularity residue_granularity;\n\n\tint (*device_alloc_chan_resources)(struct dma_chan *chan);\n\tint (*device_router_config)(struct dma_chan *chan);\n\tvoid (*device_free_chan_resources)(struct dma_chan *chan);\n\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_memcpy)(\n\t\tstruct dma_chan *chan, dma_addr_t dst, dma_addr_t src,\n\t\tsize_t len, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_xor)(\n\t\tstruct dma_chan *chan, dma_addr_t dst, dma_addr_t *src,\n\t\tunsigned int src_cnt, size_t len, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_xor_val)(\n\t\tstruct dma_chan *chan, dma_addr_t *src,\tunsigned int src_cnt,\n\t\tsize_t len, enum sum_check_flags *result, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_pq)(\n\t\tstruct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,\n\t\tunsigned int src_cnt, const unsigned char *scf,\n\t\tsize_t len, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_pq_val)(\n\t\tstruct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,\n\t\tunsigned int src_cnt, const unsigned char *scf, size_t len,\n\t\tenum sum_check_flags *pqres, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_memset)(\n\t\tstruct dma_chan *chan, dma_addr_t dest, int value, size_t len,\n\t\tunsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_memset_sg)(\n\t\tstruct dma_chan *chan, struct scatterlist *sg,\n\t\tunsigned int nents, int value, unsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_interrupt)(\n\t\tstruct dma_chan *chan, unsigned long flags);\n\n\tstruct dma_async_tx_descriptor *(*device_prep_slave_sg)(\n\t\tstruct dma_chan *chan, struct scatterlist *sgl,\n\t\tunsigned int sg_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags, void *context);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_cyclic)(\n\t\tstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\t\tsize_t period_len, enum dma_transfer_direction direction,\n\t\tunsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_interleaved_dma)(\n\t\tstruct dma_chan *chan, struct dma_interleaved_template *xt,\n\t\tunsigned long flags);\n\tstruct dma_async_tx_descriptor *(*device_prep_dma_imm_data)(\n\t\tstruct dma_chan *chan, dma_addr_t dst, u64 data,\n\t\tunsigned long flags);\n\n\tvoid (*device_caps)(struct dma_chan *chan, struct dma_slave_caps *caps);\n\tint (*device_config)(struct dma_chan *chan, struct dma_slave_config *config);\n\tint (*device_pause)(struct dma_chan *chan);\n\tint (*device_resume)(struct dma_chan *chan);\n\tint (*device_terminate_all)(struct dma_chan *chan);\n\tvoid (*device_synchronize)(struct dma_chan *chan);\n\n\tenum dma_status (*device_tx_status)(struct dma_chan *chan,\n\t\t\t\t\t    dma_cookie_t cookie,\n\t\t\t\t\t    struct dma_tx_state *txstate);\n\tvoid (*device_issue_pending)(struct dma_chan *chan);\n\tvoid (*device_release)(struct dma_device *dev);\n\t \n\tvoid (*dbg_summary_show)(struct seq_file *s, struct dma_device *dev);\n\tstruct dentry *dbg_dev_root;\n};\n\nstatic inline int dmaengine_slave_config(struct dma_chan *chan,\n\t\t\t\t\t  struct dma_slave_config *config)\n{\n\tif (chan->device->device_config)\n\t\treturn chan->device->device_config(chan, config);\n\n\treturn -ENOSYS;\n}\n\nstatic inline bool is_slave_direction(enum dma_transfer_direction direction)\n{\n\treturn (direction == DMA_MEM_TO_DEV) || (direction == DMA_DEV_TO_MEM);\n}\n\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_slave_single(\n\tstruct dma_chan *chan, dma_addr_t buf, size_t len,\n\tenum dma_transfer_direction dir, unsigned long flags)\n{\n\tstruct scatterlist sg;\n\tsg_init_table(&sg, 1);\n\tsg_dma_address(&sg) = buf;\n\tsg_dma_len(&sg) = len;\n\n\tif (!chan || !chan->device || !chan->device->device_prep_slave_sg)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_slave_sg(chan, &sg, 1,\n\t\t\t\t\t\t  dir, flags, NULL);\n}\n\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_slave_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl,\tunsigned int sg_len,\n\tenum dma_transfer_direction dir, unsigned long flags)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_slave_sg)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_slave_sg(chan, sgl, sg_len,\n\t\t\t\t\t\t  dir, flags, NULL);\n}\n\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\nstruct rio_dma_ext;\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_rio_sg(\n\tstruct dma_chan *chan, struct scatterlist *sgl,\tunsigned int sg_len,\n\tenum dma_transfer_direction dir, unsigned long flags,\n\tstruct rio_dma_ext *rio_ext)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_slave_sg)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_slave_sg(chan, sgl, sg_len,\n\t\t\t\t\t\t  dir, flags, rio_ext);\n}\n#endif\n\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_dma_cyclic(\n\t\tstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\n\t\tsize_t period_len, enum dma_transfer_direction dir,\n\t\tunsigned long flags)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_dma_cyclic)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_dma_cyclic(chan, buf_addr, buf_len,\n\t\t\t\t\t\tperiod_len, dir, flags);\n}\n\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_interleaved_dma(\n\t\tstruct dma_chan *chan, struct dma_interleaved_template *xt,\n\t\tunsigned long flags)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_interleaved_dma)\n\t\treturn NULL;\n\tif (flags & DMA_PREP_REPEAT &&\n\t    !test_bit(DMA_REPEAT, chan->device->cap_mask.bits))\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_interleaved_dma(chan, xt, flags);\n}\n\n \nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memset(\n\t\tstruct dma_chan *chan, dma_addr_t dest, int value, size_t len,\n\t\tunsigned long flags)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_dma_memset)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_dma_memset(chan, dest, value,\n\t\t\t\t\t\t    len, flags);\n}\n\nstatic inline struct dma_async_tx_descriptor *dmaengine_prep_dma_memcpy(\n\t\tstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\n\t\tsize_t len, unsigned long flags)\n{\n\tif (!chan || !chan->device || !chan->device->device_prep_dma_memcpy)\n\t\treturn NULL;\n\n\treturn chan->device->device_prep_dma_memcpy(chan, dest, src,\n\t\t\t\t\t\t    len, flags);\n}\n\nstatic inline bool dmaengine_is_metadata_mode_supported(struct dma_chan *chan,\n\t\tenum dma_desc_metadata_mode mode)\n{\n\tif (!chan)\n\t\treturn false;\n\n\treturn !!(chan->device->desc_metadata_modes & mode);\n}\n\n#ifdef CONFIG_DMA_ENGINE\nint dmaengine_desc_attach_metadata(struct dma_async_tx_descriptor *desc,\n\t\t\t\t   void *data, size_t len);\nvoid *dmaengine_desc_get_metadata_ptr(struct dma_async_tx_descriptor *desc,\n\t\t\t\t      size_t *payload_len, size_t *max_len);\nint dmaengine_desc_set_metadata_len(struct dma_async_tx_descriptor *desc,\n\t\t\t\t    size_t payload_len);\n#else  \nstatic inline int dmaengine_desc_attach_metadata(\n\t\tstruct dma_async_tx_descriptor *desc, void *data, size_t len)\n{\n\treturn -EINVAL;\n}\nstatic inline void *dmaengine_desc_get_metadata_ptr(\n\t\tstruct dma_async_tx_descriptor *desc, size_t *payload_len,\n\t\tsize_t *max_len)\n{\n\treturn NULL;\n}\nstatic inline int dmaengine_desc_set_metadata_len(\n\t\tstruct dma_async_tx_descriptor *desc, size_t payload_len)\n{\n\treturn -EINVAL;\n}\n#endif  \n\n \nstatic inline int dmaengine_terminate_all(struct dma_chan *chan)\n{\n\tif (chan->device->device_terminate_all)\n\t\treturn chan->device->device_terminate_all(chan);\n\n\treturn -ENOSYS;\n}\n\n \nstatic inline int dmaengine_terminate_async(struct dma_chan *chan)\n{\n\tif (chan->device->device_terminate_all)\n\t\treturn chan->device->device_terminate_all(chan);\n\n\treturn -EINVAL;\n}\n\n \nstatic inline void dmaengine_synchronize(struct dma_chan *chan)\n{\n\tmight_sleep();\n\n\tif (chan->device->device_synchronize)\n\t\tchan->device->device_synchronize(chan);\n}\n\n \nstatic inline int dmaengine_terminate_sync(struct dma_chan *chan)\n{\n\tint ret;\n\n\tret = dmaengine_terminate_async(chan);\n\tif (ret)\n\t\treturn ret;\n\n\tdmaengine_synchronize(chan);\n\n\treturn 0;\n}\n\nstatic inline int dmaengine_pause(struct dma_chan *chan)\n{\n\tif (chan->device->device_pause)\n\t\treturn chan->device->device_pause(chan);\n\n\treturn -ENOSYS;\n}\n\nstatic inline int dmaengine_resume(struct dma_chan *chan)\n{\n\tif (chan->device->device_resume)\n\t\treturn chan->device->device_resume(chan);\n\n\treturn -ENOSYS;\n}\n\nstatic inline enum dma_status dmaengine_tx_status(struct dma_chan *chan,\n\tdma_cookie_t cookie, struct dma_tx_state *state)\n{\n\treturn chan->device->device_tx_status(chan, cookie, state);\n}\n\nstatic inline dma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc)\n{\n\treturn desc->tx_submit(desc);\n}\n\nstatic inline bool dmaengine_check_align(enum dmaengine_alignment align,\n\t\t\t\t\t size_t off1, size_t off2, size_t len)\n{\n\treturn !(((1 << align) - 1) & (off1 | off2 | len));\n}\n\nstatic inline bool is_dma_copy_aligned(struct dma_device *dev, size_t off1,\n\t\t\t\t       size_t off2, size_t len)\n{\n\treturn dmaengine_check_align(dev->copy_align, off1, off2, len);\n}\n\nstatic inline bool is_dma_xor_aligned(struct dma_device *dev, size_t off1,\n\t\t\t\t      size_t off2, size_t len)\n{\n\treturn dmaengine_check_align(dev->xor_align, off1, off2, len);\n}\n\nstatic inline bool is_dma_pq_aligned(struct dma_device *dev, size_t off1,\n\t\t\t\t     size_t off2, size_t len)\n{\n\treturn dmaengine_check_align(dev->pq_align, off1, off2, len);\n}\n\nstatic inline bool is_dma_fill_aligned(struct dma_device *dev, size_t off1,\n\t\t\t\t       size_t off2, size_t len)\n{\n\treturn dmaengine_check_align(dev->fill_align, off1, off2, len);\n}\n\nstatic inline void\ndma_set_maxpq(struct dma_device *dma, int maxpq, int has_pq_continue)\n{\n\tdma->max_pq = maxpq;\n\tif (has_pq_continue)\n\t\tdma->max_pq |= DMA_HAS_PQ_CONTINUE;\n}\n\nstatic inline bool dmaf_continue(enum dma_ctrl_flags flags)\n{\n\treturn (flags & DMA_PREP_CONTINUE) == DMA_PREP_CONTINUE;\n}\n\nstatic inline bool dmaf_p_disabled_continue(enum dma_ctrl_flags flags)\n{\n\tenum dma_ctrl_flags mask = DMA_PREP_CONTINUE | DMA_PREP_PQ_DISABLE_P;\n\n\treturn (flags & mask) == mask;\n}\n\nstatic inline bool dma_dev_has_pq_continue(struct dma_device *dma)\n{\n\treturn (dma->max_pq & DMA_HAS_PQ_CONTINUE) == DMA_HAS_PQ_CONTINUE;\n}\n\nstatic inline unsigned short dma_dev_to_maxpq(struct dma_device *dma)\n{\n\treturn dma->max_pq & ~DMA_HAS_PQ_CONTINUE;\n}\n\n \nstatic inline int dma_maxpq(struct dma_device *dma, enum dma_ctrl_flags flags)\n{\n\tif (dma_dev_has_pq_continue(dma) || !dmaf_continue(flags))\n\t\treturn dma_dev_to_maxpq(dma);\n\tif (dmaf_p_disabled_continue(flags))\n\t\treturn dma_dev_to_maxpq(dma) - 1;\n\tif (dmaf_continue(flags))\n\t\treturn dma_dev_to_maxpq(dma) - 3;\n\tBUG();\n}\n\nstatic inline size_t dmaengine_get_icg(bool inc, bool sgl, size_t icg,\n\t\t\t\t      size_t dir_icg)\n{\n\tif (inc) {\n\t\tif (dir_icg)\n\t\t\treturn dir_icg;\n\t\tif (sgl)\n\t\t\treturn icg;\n\t}\n\n\treturn 0;\n}\n\nstatic inline size_t dmaengine_get_dst_icg(struct dma_interleaved_template *xt,\n\t\t\t\t\t   struct data_chunk *chunk)\n{\n\treturn dmaengine_get_icg(xt->dst_inc, xt->dst_sgl,\n\t\t\t\t chunk->icg, chunk->dst_icg);\n}\n\nstatic inline size_t dmaengine_get_src_icg(struct dma_interleaved_template *xt,\n\t\t\t\t\t   struct data_chunk *chunk)\n{\n\treturn dmaengine_get_icg(xt->src_inc, xt->src_sgl,\n\t\t\t\t chunk->icg, chunk->src_icg);\n}\n\n \n\n#ifdef CONFIG_DMA_ENGINE\nvoid dmaengine_get(void);\nvoid dmaengine_put(void);\n#else\nstatic inline void dmaengine_get(void)\n{\n}\nstatic inline void dmaengine_put(void)\n{\n}\n#endif\n\n#ifdef CONFIG_ASYNC_TX_DMA\n#define async_dmaengine_get()\tdmaengine_get()\n#define async_dmaengine_put()\tdmaengine_put()\n#ifndef CONFIG_ASYNC_TX_ENABLE_CHANNEL_SWITCH\n#define async_dma_find_channel(type) dma_find_channel(DMA_ASYNC_TX)\n#else\n#define async_dma_find_channel(type) dma_find_channel(type)\n#endif  \n#else\nstatic inline void async_dmaengine_get(void)\n{\n}\nstatic inline void async_dmaengine_put(void)\n{\n}\nstatic inline struct dma_chan *\nasync_dma_find_channel(enum dma_transaction_type type)\n{\n\treturn NULL;\n}\n#endif  \nvoid dma_async_tx_descriptor_init(struct dma_async_tx_descriptor *tx,\n\t\t\t\t  struct dma_chan *chan);\n\nstatic inline void async_tx_ack(struct dma_async_tx_descriptor *tx)\n{\n\ttx->flags |= DMA_CTRL_ACK;\n}\n\nstatic inline void async_tx_clear_ack(struct dma_async_tx_descriptor *tx)\n{\n\ttx->flags &= ~DMA_CTRL_ACK;\n}\n\nstatic inline bool async_tx_test_ack(struct dma_async_tx_descriptor *tx)\n{\n\treturn (tx->flags & DMA_CTRL_ACK) == DMA_CTRL_ACK;\n}\n\n#define dma_cap_set(tx, mask) __dma_cap_set((tx), &(mask))\nstatic inline void\n__dma_cap_set(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)\n{\n\tset_bit(tx_type, dstp->bits);\n}\n\n#define dma_cap_clear(tx, mask) __dma_cap_clear((tx), &(mask))\nstatic inline void\n__dma_cap_clear(enum dma_transaction_type tx_type, dma_cap_mask_t *dstp)\n{\n\tclear_bit(tx_type, dstp->bits);\n}\n\n#define dma_cap_zero(mask) __dma_cap_zero(&(mask))\nstatic inline void __dma_cap_zero(dma_cap_mask_t *dstp)\n{\n\tbitmap_zero(dstp->bits, DMA_TX_TYPE_END);\n}\n\n#define dma_has_cap(tx, mask) __dma_has_cap((tx), &(mask))\nstatic inline int\n__dma_has_cap(enum dma_transaction_type tx_type, dma_cap_mask_t *srcp)\n{\n\treturn test_bit(tx_type, srcp->bits);\n}\n\n#define for_each_dma_cap_mask(cap, mask) \\\n\tfor_each_set_bit(cap, mask.bits, DMA_TX_TYPE_END)\n\n \nstatic inline void dma_async_issue_pending(struct dma_chan *chan)\n{\n\tchan->device->device_issue_pending(chan);\n}\n\n \nstatic inline enum dma_status dma_async_is_tx_complete(struct dma_chan *chan,\n\tdma_cookie_t cookie, dma_cookie_t *last, dma_cookie_t *used)\n{\n\tstruct dma_tx_state state;\n\tenum dma_status status;\n\n\tstatus = chan->device->device_tx_status(chan, cookie, &state);\n\tif (last)\n\t\t*last = state.last;\n\tif (used)\n\t\t*used = state.used;\n\treturn status;\n}\n\n \nstatic inline enum dma_status dma_async_is_complete(dma_cookie_t cookie,\n\t\t\tdma_cookie_t last_complete, dma_cookie_t last_used)\n{\n\tif (last_complete <= last_used) {\n\t\tif ((cookie <= last_complete) || (cookie > last_used))\n\t\t\treturn DMA_COMPLETE;\n\t} else {\n\t\tif ((cookie <= last_complete) && (cookie > last_used))\n\t\t\treturn DMA_COMPLETE;\n\t}\n\treturn DMA_IN_PROGRESS;\n}\n\nstatic inline void\ndma_set_tx_state(struct dma_tx_state *st, dma_cookie_t last, dma_cookie_t used, u32 residue)\n{\n\tif (!st)\n\t\treturn;\n\n\tst->last = last;\n\tst->used = used;\n\tst->residue = residue;\n}\n\n#ifdef CONFIG_DMA_ENGINE\nstruct dma_chan *dma_find_channel(enum dma_transaction_type tx_type);\nenum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie);\nenum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx);\nvoid dma_issue_pending_all(void);\nstruct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,\n\t\t\t\t       dma_filter_fn fn, void *fn_param,\n\t\t\t\t       struct device_node *np);\n\nstruct dma_chan *dma_request_chan(struct device *dev, const char *name);\nstruct dma_chan *dma_request_chan_by_mask(const dma_cap_mask_t *mask);\n\nvoid dma_release_channel(struct dma_chan *chan);\nint dma_get_slave_caps(struct dma_chan *chan, struct dma_slave_caps *caps);\n#else\nstatic inline struct dma_chan *dma_find_channel(enum dma_transaction_type tx_type)\n{\n\treturn NULL;\n}\nstatic inline enum dma_status dma_sync_wait(struct dma_chan *chan, dma_cookie_t cookie)\n{\n\treturn DMA_COMPLETE;\n}\nstatic inline enum dma_status dma_wait_for_async_tx(struct dma_async_tx_descriptor *tx)\n{\n\treturn DMA_COMPLETE;\n}\nstatic inline void dma_issue_pending_all(void)\n{\n}\nstatic inline struct dma_chan *__dma_request_channel(const dma_cap_mask_t *mask,\n\t\t\t\t\t\t     dma_filter_fn fn,\n\t\t\t\t\t\t     void *fn_param,\n\t\t\t\t\t\t     struct device_node *np)\n{\n\treturn NULL;\n}\nstatic inline struct dma_chan *dma_request_chan(struct device *dev,\n\t\t\t\t\t\tconst char *name)\n{\n\treturn ERR_PTR(-ENODEV);\n}\nstatic inline struct dma_chan *dma_request_chan_by_mask(\n\t\t\t\t\t\tconst dma_cap_mask_t *mask)\n{\n\treturn ERR_PTR(-ENODEV);\n}\nstatic inline void dma_release_channel(struct dma_chan *chan)\n{\n}\nstatic inline int dma_get_slave_caps(struct dma_chan *chan,\n\t\t\t\t     struct dma_slave_caps *caps)\n{\n\treturn -ENXIO;\n}\n#endif\n\nstatic inline int dmaengine_desc_set_reuse(struct dma_async_tx_descriptor *tx)\n{\n\tstruct dma_slave_caps caps;\n\tint ret;\n\n\tret = dma_get_slave_caps(tx->chan, &caps);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!caps.descriptor_reuse)\n\t\treturn -EPERM;\n\n\ttx->flags |= DMA_CTRL_REUSE;\n\treturn 0;\n}\n\nstatic inline void dmaengine_desc_clear_reuse(struct dma_async_tx_descriptor *tx)\n{\n\ttx->flags &= ~DMA_CTRL_REUSE;\n}\n\nstatic inline bool dmaengine_desc_test_reuse(struct dma_async_tx_descriptor *tx)\n{\n\treturn (tx->flags & DMA_CTRL_REUSE) == DMA_CTRL_REUSE;\n}\n\nstatic inline int dmaengine_desc_free(struct dma_async_tx_descriptor *desc)\n{\n\t \n\tif (!dmaengine_desc_test_reuse(desc))\n\t\treturn -EPERM;\n\n\treturn desc->desc_free(desc);\n}\n\n \n\nint dma_async_device_register(struct dma_device *device);\nint dmaenginem_async_device_register(struct dma_device *device);\nvoid dma_async_device_unregister(struct dma_device *device);\nint dma_async_device_channel_register(struct dma_device *device,\n\t\t\t\t      struct dma_chan *chan);\nvoid dma_async_device_channel_unregister(struct dma_device *device,\n\t\t\t\t\t struct dma_chan *chan);\nvoid dma_run_dependencies(struct dma_async_tx_descriptor *tx);\n#define dma_request_channel(mask, x, y) \\\n\t__dma_request_channel(&(mask), x, y, NULL)\n\n \nstatic inline struct dma_chan * __deprecated\ndma_request_slave_channel(struct device *dev, const char *name)\n{\n\tstruct dma_chan *ch = dma_request_chan(dev, name);\n\n\treturn IS_ERR(ch) ? NULL : ch;\n}\n\nstatic inline struct dma_chan\n*dma_request_slave_channel_compat(const dma_cap_mask_t mask,\n\t\t\t\t  dma_filter_fn fn, void *fn_param,\n\t\t\t\t  struct device *dev, const char *name)\n{\n\tstruct dma_chan *chan;\n\n\tchan = dma_request_slave_channel(dev, name);\n\tif (chan)\n\t\treturn chan;\n\n\tif (!fn || !fn_param)\n\t\treturn NULL;\n\n\treturn __dma_request_channel(&mask, fn, fn_param, NULL);\n}\n\nstatic inline char *\ndmaengine_get_direction_text(enum dma_transfer_direction dir)\n{\n\tswitch (dir) {\n\tcase DMA_DEV_TO_MEM:\n\t\treturn \"DEV_TO_MEM\";\n\tcase DMA_MEM_TO_DEV:\n\t\treturn \"MEM_TO_DEV\";\n\tcase DMA_MEM_TO_MEM:\n\t\treturn \"MEM_TO_MEM\";\n\tcase DMA_DEV_TO_DEV:\n\t\treturn \"DEV_TO_DEV\";\n\tdefault:\n\t\treturn \"invalid\";\n\t}\n}\n\nstatic inline struct device *dmaengine_get_dma_device(struct dma_chan *chan)\n{\n\tif (chan->dev->chan_dma_dev)\n\t\treturn &chan->dev->device;\n\n\treturn chan->device->dev;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}