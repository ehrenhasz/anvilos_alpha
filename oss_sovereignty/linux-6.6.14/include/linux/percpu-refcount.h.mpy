{
  "module_name": "percpu-refcount.h",
  "hash_id": "1663debef3323e2d99c5a73713d5fa1a257ad10054687f93a30ee9e95c407343",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/percpu-refcount.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_PERCPU_REFCOUNT_H\n#define _LINUX_PERCPU_REFCOUNT_H\n\n#include <linux/atomic.h>\n#include <linux/percpu.h>\n#include <linux/rcupdate.h>\n#include <linux/types.h>\n#include <linux/gfp.h>\n\nstruct percpu_ref;\ntypedef void (percpu_ref_func_t)(struct percpu_ref *);\n\n \nenum {\n\t__PERCPU_REF_ATOMIC\t= 1LU << 0,\t \n\t__PERCPU_REF_DEAD\t= 1LU << 1,\t \n\t__PERCPU_REF_ATOMIC_DEAD = __PERCPU_REF_ATOMIC | __PERCPU_REF_DEAD,\n\n\t__PERCPU_REF_FLAG_BITS\t= 2,\n};\n\n \nenum {\n\t \n\tPERCPU_REF_INIT_ATOMIC\t= 1 << 0,\n\n\t \n\tPERCPU_REF_INIT_DEAD\t= 1 << 1,\n\n\t \n\tPERCPU_REF_ALLOW_REINIT\t= 1 << 2,\n};\n\nstruct percpu_ref_data {\n\tatomic_long_t\t\tcount;\n\tpercpu_ref_func_t\t*release;\n\tpercpu_ref_func_t\t*confirm_switch;\n\tbool\t\t\tforce_atomic:1;\n\tbool\t\t\tallow_reinit:1;\n\tstruct rcu_head\t\trcu;\n\tstruct percpu_ref\t*ref;\n};\n\nstruct percpu_ref {\n\t \n\tunsigned long\t\tpercpu_count_ptr;\n\n\t \n\tstruct percpu_ref_data  *data;\n};\n\nint __must_check percpu_ref_init(struct percpu_ref *ref,\n\t\t\t\t percpu_ref_func_t *release, unsigned int flags,\n\t\t\t\t gfp_t gfp);\nvoid percpu_ref_exit(struct percpu_ref *ref);\nvoid percpu_ref_switch_to_atomic(struct percpu_ref *ref,\n\t\t\t\t percpu_ref_func_t *confirm_switch);\nvoid percpu_ref_switch_to_atomic_sync(struct percpu_ref *ref);\nvoid percpu_ref_switch_to_percpu(struct percpu_ref *ref);\nvoid percpu_ref_kill_and_confirm(struct percpu_ref *ref,\n\t\t\t\t percpu_ref_func_t *confirm_kill);\nvoid percpu_ref_resurrect(struct percpu_ref *ref);\nvoid percpu_ref_reinit(struct percpu_ref *ref);\nbool percpu_ref_is_zero(struct percpu_ref *ref);\n\n \nstatic inline void percpu_ref_kill(struct percpu_ref *ref)\n{\n\tpercpu_ref_kill_and_confirm(ref, NULL);\n}\n\n \nstatic inline bool __ref_is_percpu(struct percpu_ref *ref,\n\t\t\t\t\t  unsigned long __percpu **percpu_countp)\n{\n\tunsigned long percpu_ptr;\n\n\t \n\tpercpu_ptr = READ_ONCE(ref->percpu_count_ptr);\n\n\t \n\tif (unlikely(percpu_ptr & __PERCPU_REF_ATOMIC_DEAD))\n\t\treturn false;\n\n\t*percpu_countp = (unsigned long __percpu *)percpu_ptr;\n\treturn true;\n}\n\n \nstatic inline void percpu_ref_get_many(struct percpu_ref *ref, unsigned long nr)\n{\n\tunsigned long __percpu *percpu_count;\n\n\trcu_read_lock();\n\n\tif (__ref_is_percpu(ref, &percpu_count))\n\t\tthis_cpu_add(*percpu_count, nr);\n\telse\n\t\tatomic_long_add(nr, &ref->data->count);\n\n\trcu_read_unlock();\n}\n\n \nstatic inline void percpu_ref_get(struct percpu_ref *ref)\n{\n\tpercpu_ref_get_many(ref, 1);\n}\n\n \nstatic inline bool percpu_ref_tryget_many(struct percpu_ref *ref,\n\t\t\t\t\t  unsigned long nr)\n{\n\tunsigned long __percpu *percpu_count;\n\tbool ret;\n\n\trcu_read_lock();\n\n\tif (__ref_is_percpu(ref, &percpu_count)) {\n\t\tthis_cpu_add(*percpu_count, nr);\n\t\tret = true;\n\t} else {\n\t\tret = atomic_long_add_unless(&ref->data->count, nr, 0);\n\t}\n\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\n \nstatic inline bool percpu_ref_tryget(struct percpu_ref *ref)\n{\n\treturn percpu_ref_tryget_many(ref, 1);\n}\n\n \nstatic inline bool percpu_ref_tryget_live_rcu(struct percpu_ref *ref)\n{\n\tunsigned long __percpu *percpu_count;\n\tbool ret = false;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\n\tif (likely(__ref_is_percpu(ref, &percpu_count))) {\n\t\tthis_cpu_inc(*percpu_count);\n\t\tret = true;\n\t} else if (!(ref->percpu_count_ptr & __PERCPU_REF_DEAD)) {\n\t\tret = atomic_long_inc_not_zero(&ref->data->count);\n\t}\n\treturn ret;\n}\n\n \nstatic inline bool percpu_ref_tryget_live(struct percpu_ref *ref)\n{\n\tbool ret = false;\n\n\trcu_read_lock();\n\tret = percpu_ref_tryget_live_rcu(ref);\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nstatic inline void percpu_ref_put_many(struct percpu_ref *ref, unsigned long nr)\n{\n\tunsigned long __percpu *percpu_count;\n\n\trcu_read_lock();\n\n\tif (__ref_is_percpu(ref, &percpu_count))\n\t\tthis_cpu_sub(*percpu_count, nr);\n\telse if (unlikely(atomic_long_sub_and_test(nr, &ref->data->count)))\n\t\tref->data->release(ref);\n\n\trcu_read_unlock();\n}\n\n \nstatic inline void percpu_ref_put(struct percpu_ref *ref)\n{\n\tpercpu_ref_put_many(ref, 1);\n}\n\n \nstatic inline bool percpu_ref_is_dying(struct percpu_ref *ref)\n{\n\treturn ref->percpu_count_ptr & __PERCPU_REF_DEAD;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}