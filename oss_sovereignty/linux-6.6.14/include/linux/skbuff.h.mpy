{
  "module_name": "skbuff.h",
  "hash_id": "072bd91005878a208bd60a0097775b33276fe52824f3366180dfea5502833b72",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/skbuff.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_SKBUFF_H\n#define _LINUX_SKBUFF_H\n\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/time.h>\n#include <linux/bug.h>\n#include <linux/bvec.h>\n#include <linux/cache.h>\n#include <linux/rbtree.h>\n#include <linux/socket.h>\n#include <linux/refcount.h>\n\n#include <linux/atomic.h>\n#include <asm/types.h>\n#include <linux/spinlock.h>\n#include <net/checksum.h>\n#include <linux/rcupdate.h>\n#include <linux/dma-mapping.h>\n#include <linux/netdev_features.h>\n#include <net/flow_dissector.h>\n#include <linux/in6.h>\n#include <linux/if_packet.h>\n#include <linux/llist.h>\n#include <net/flow.h>\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n#include <linux/netfilter/nf_conntrack_common.h>\n#endif\n#include <net/net_debug.h>\n#include <net/dropreason-core.h>\n\n \n\n \n#define CHECKSUM_NONE\t\t0\n#define CHECKSUM_UNNECESSARY\t1\n#define CHECKSUM_COMPLETE\t2\n#define CHECKSUM_PARTIAL\t3\n\n \n#define SKB_MAX_CSUM_LEVEL\t3\n\n#define SKB_DATA_ALIGN(X)\tALIGN(X, SMP_CACHE_BYTES)\n#define SKB_WITH_OVERHEAD(X)\t\\\n\t((X) - SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\n \n#define SKB_HEAD_ALIGN(X) (SKB_DATA_ALIGN(X) + \\\n\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\n#define SKB_MAX_ORDER(X, ORDER) \\\n\tSKB_WITH_OVERHEAD((PAGE_SIZE << (ORDER)) - (X))\n#define SKB_MAX_HEAD(X)\t\t(SKB_MAX_ORDER((X), 0))\n#define SKB_MAX_ALLOC\t\t(SKB_MAX_ORDER(0, 2))\n\n \n#define SKB_TRUESIZE(X) ((X) +\t\t\t\t\t\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct sk_buff)) +\t\\\n\t\t\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstruct ahash_request;\nstruct net_device;\nstruct scatterlist;\nstruct pipe_inode_info;\nstruct iov_iter;\nstruct napi_struct;\nstruct bpf_prog;\nunion bpf_attr;\nstruct skb_ext;\nstruct ts_config;\n\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\nstruct nf_bridge_info {\n\tenum {\n\t\tBRNF_PROTO_UNCHANGED,\n\t\tBRNF_PROTO_8021Q,\n\t\tBRNF_PROTO_PPPOE\n\t} orig_proto:8;\n\tu8\t\t\tpkt_otherhost:1;\n\tu8\t\t\tin_prerouting:1;\n\tu8\t\t\tbridged_dnat:1;\n\tu8\t\t\tsabotage_in_done:1;\n\t__u16\t\t\tfrag_max_size;\n\tint\t\t\tphysinif;\n\n\t \n\tstruct net_device\t*physoutdev;\n\tunion {\n\t\t \n\t\t__be32          ipv4_daddr;\n\t\tstruct in6_addr ipv6_daddr;\n\n\t\t \n\t\tchar neigh_header[8];\n\t};\n};\n#endif\n\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n \nstruct tc_skb_ext {\n\tunion {\n\t\tu64 act_miss_cookie;\n\t\t__u32 chain;\n\t};\n\t__u16 mru;\n\t__u16 zone;\n\tu8 post_ct:1;\n\tu8 post_ct_snat:1;\n\tu8 post_ct_dnat:1;\n\tu8 act_miss:1;  \n\tu8 l2_miss:1;  \n};\n#endif\n\nstruct sk_buff_head {\n\t \n\tstruct_group_tagged(sk_buff_list, list,\n\t\tstruct sk_buff\t*next;\n\t\tstruct sk_buff\t*prev;\n\t);\n\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct sk_buff;\n\n#ifndef CONFIG_MAX_SKB_FRAGS\n# define CONFIG_MAX_SKB_FRAGS 17\n#endif\n\n#define MAX_SKB_FRAGS CONFIG_MAX_SKB_FRAGS\n\nextern int sysctl_max_skb_frags;\n\n \n#define GSO_BY_FRAGS\t0xFFFF\n\ntypedef struct bio_vec skb_frag_t;\n\n \nstatic inline unsigned int skb_frag_size(const skb_frag_t *frag)\n{\n\treturn frag->bv_len;\n}\n\n \nstatic inline void skb_frag_size_set(skb_frag_t *frag, unsigned int size)\n{\n\tfrag->bv_len = size;\n}\n\n \nstatic inline void skb_frag_size_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len += delta;\n}\n\n \nstatic inline void skb_frag_size_sub(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_len -= delta;\n}\n\n \nstatic inline bool skb_frag_must_loop(struct page *p)\n{\n#if defined(CONFIG_HIGHMEM)\n\tif (IS_ENABLED(CONFIG_DEBUG_KMAP_LOCAL_FORCE_MAP) || PageHighMem(p))\n\t\treturn true;\n#endif\n\treturn false;\n}\n\n \n#define skb_frag_foreach_page(f, f_off, f_len, p, p_off, p_len, copied)\t\\\n\tfor (p = skb_frag_page(f) + ((f_off) >> PAGE_SHIFT),\t\t\\\n\t     p_off = (f_off) & (PAGE_SIZE - 1),\t\t\t\t\\\n\t     p_len = skb_frag_must_loop(p) ?\t\t\t\t\\\n\t     min_t(u32, f_len, PAGE_SIZE - p_off) : f_len,\t\t\\\n\t     copied = 0;\t\t\t\t\t\t\\\n\t     copied < f_len;\t\t\t\t\t\t\\\n\t     copied += p_len, p++, p_off = 0,\t\t\t\t\\\n\t     p_len = min_t(u32, f_len - copied, PAGE_SIZE))\t\t\\\n\n \nstruct skb_shared_hwtstamps {\n\tunion {\n\t\tktime_t\thwtstamp;\n\t\tvoid *netdev_data;\n\t};\n};\n\n \nenum {\n\t \n\tSKBTX_HW_TSTAMP = 1 << 0,\n\n\t \n\tSKBTX_SW_TSTAMP = 1 << 1,\n\n\t \n\tSKBTX_IN_PROGRESS = 1 << 2,\n\n\t \n\tSKBTX_HW_TSTAMP_USE_CYCLES = 1 << 3,\n\n\t \n\tSKBTX_WIFI_STATUS = 1 << 4,\n\n\t \n\tSKBTX_HW_TSTAMP_NETDEV = 1 << 5,\n\n\t \n\tSKBTX_SCHED_TSTAMP = 1 << 6,\n};\n\n#define SKBTX_ANY_SW_TSTAMP\t(SKBTX_SW_TSTAMP    | \\\n\t\t\t\t SKBTX_SCHED_TSTAMP)\n#define SKBTX_ANY_TSTAMP\t(SKBTX_HW_TSTAMP | \\\n\t\t\t\t SKBTX_HW_TSTAMP_USE_CYCLES | \\\n\t\t\t\t SKBTX_ANY_SW_TSTAMP)\n\n \nenum {\n\t \n\tSKBFL_ZEROCOPY_ENABLE = BIT(0),\n\n\t \n\tSKBFL_SHARED_FRAG = BIT(1),\n\n\t \n\tSKBFL_PURE_ZEROCOPY = BIT(2),\n\n\tSKBFL_DONT_ORPHAN = BIT(3),\n\n\t \n\tSKBFL_MANAGED_FRAG_REFS = BIT(4),\n};\n\n#define SKBFL_ZEROCOPY_FRAG\t(SKBFL_ZEROCOPY_ENABLE | SKBFL_SHARED_FRAG)\n#define SKBFL_ALL_ZEROCOPY\t(SKBFL_ZEROCOPY_FRAG | SKBFL_PURE_ZEROCOPY | \\\n\t\t\t\t SKBFL_DONT_ORPHAN | SKBFL_MANAGED_FRAG_REFS)\n\n \nstruct ubuf_info {\n\tvoid (*callback)(struct sk_buff *, struct ubuf_info *,\n\t\t\t bool zerocopy_success);\n\trefcount_t refcnt;\n\tu8 flags;\n};\n\nstruct ubuf_info_msgzc {\n\tstruct ubuf_info ubuf;\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long desc;\n\t\t\tvoid *ctx;\n\t\t};\n\t\tstruct {\n\t\t\tu32 id;\n\t\t\tu16 len;\n\t\t\tu16 zerocopy:1;\n\t\t\tu32 bytelen;\n\t\t};\n\t};\n\n\tstruct mmpin {\n\t\tstruct user_struct *user;\n\t\tunsigned int num_pg;\n\t} mmp;\n};\n\n#define skb_uarg(SKB)\t((struct ubuf_info *)(skb_shinfo(SKB)->destructor_arg))\n#define uarg_to_msgzc(ubuf_ptr)\tcontainer_of((ubuf_ptr), struct ubuf_info_msgzc, \\\n\t\t\t\t\t     ubuf)\n\nint mm_account_pinned_pages(struct mmpin *mmp, size_t size);\nvoid mm_unaccount_pinned_pages(struct mmpin *mmp);\n\n \nstruct skb_shared_info {\n\t__u8\t\tflags;\n\t__u8\t\tmeta_len;\n\t__u8\t\tnr_frags;\n\t__u8\t\ttx_flags;\n\tunsigned short\tgso_size;\n\t \n\tunsigned short\tgso_segs;\n\tstruct sk_buff\t*frag_list;\n\tstruct skb_shared_hwtstamps hwtstamps;\n\tunsigned int\tgso_type;\n\tu32\t\ttskey;\n\n\t \n\tatomic_t\tdataref;\n\tunsigned int\txdp_frags_size;\n\n\t \n\tvoid *\t\tdestructor_arg;\n\n\t \n\tskb_frag_t\tfrags[MAX_SKB_FRAGS];\n};\n\n \n#define SKB_DATAREF_SHIFT 16\n#define SKB_DATAREF_MASK ((1 << SKB_DATAREF_SHIFT) - 1)\n\n\nenum {\n\tSKB_FCLONE_UNAVAILABLE,\t \n\tSKB_FCLONE_ORIG,\t \n\tSKB_FCLONE_CLONE,\t \n};\n\nenum {\n\tSKB_GSO_TCPV4 = 1 << 0,\n\n\t \n\tSKB_GSO_DODGY = 1 << 1,\n\n\t \n\tSKB_GSO_TCP_ECN = 1 << 2,\n\n\tSKB_GSO_TCP_FIXEDID = 1 << 3,\n\n\tSKB_GSO_TCPV6 = 1 << 4,\n\n\tSKB_GSO_FCOE = 1 << 5,\n\n\tSKB_GSO_GRE = 1 << 6,\n\n\tSKB_GSO_GRE_CSUM = 1 << 7,\n\n\tSKB_GSO_IPXIP4 = 1 << 8,\n\n\tSKB_GSO_IPXIP6 = 1 << 9,\n\n\tSKB_GSO_UDP_TUNNEL = 1 << 10,\n\n\tSKB_GSO_UDP_TUNNEL_CSUM = 1 << 11,\n\n\tSKB_GSO_PARTIAL = 1 << 12,\n\n\tSKB_GSO_TUNNEL_REMCSUM = 1 << 13,\n\n\tSKB_GSO_SCTP = 1 << 14,\n\n\tSKB_GSO_ESP = 1 << 15,\n\n\tSKB_GSO_UDP = 1 << 16,\n\n\tSKB_GSO_UDP_L4 = 1 << 17,\n\n\tSKB_GSO_FRAGLIST = 1 << 18,\n};\n\n#if BITS_PER_LONG > 32\n#define NET_SKBUFF_DATA_USES_OFFSET 1\n#endif\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\ntypedef unsigned int sk_buff_data_t;\n#else\ntypedef unsigned char *sk_buff_data_t;\n#endif\n\n \n\n \n\nstruct sk_buff {\n\tunion {\n\t\tstruct {\n\t\t\t \n\t\t\tstruct sk_buff\t\t*next;\n\t\t\tstruct sk_buff\t\t*prev;\n\n\t\t\tunion {\n\t\t\t\tstruct net_device\t*dev;\n\t\t\t\t \n\t\t\t\tunsigned long\t\tdev_scratch;\n\t\t\t};\n\t\t};\n\t\tstruct rb_node\t\trbnode;  \n\t\tstruct list_head\tlist;\n\t\tstruct llist_node\tll_node;\n\t};\n\n\tunion {\n\t\tstruct sock\t\t*sk;\n\t\tint\t\t\tip_defrag_offset;\n\t};\n\n\tunion {\n\t\tktime_t\t\ttstamp;\n\t\tu64\t\tskb_mstamp_ns;  \n\t};\n\t \n\tchar\t\t\tcb[48] __aligned(8);\n\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\t_skb_refdst;\n\t\t\tvoid\t\t(*destructor)(struct sk_buff *skb);\n\t\t};\n\t\tstruct list_head\ttcp_tsorted_anchor;\n#ifdef CONFIG_NET_SOCK_MSG\n\t\tunsigned long\t\t_sk_redir;\n#endif\n\t};\n\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tunsigned long\t\t _nfct;\n#endif\n\tunsigned int\t\tlen,\n\t\t\t\tdata_len;\n\t__u16\t\t\tmac_len,\n\t\t\t\thdr_len;\n\n\t \n\t__u16\t\t\tqueue_mapping;\n\n \n#ifdef __BIG_ENDIAN_BITFIELD\n#define CLONED_MASK\t(1 << 7)\n#else\n#define CLONED_MASK\t1\n#endif\n#define CLONED_OFFSET\t\toffsetof(struct sk_buff, __cloned_offset)\n\n\t \n\t__u8\t\t\t__cloned_offset[0];\n\t \n\t__u8\t\t\tcloned:1,\n\t\t\t\tnohdr:1,\n\t\t\t\tfclone:2,\n\t\t\t\tpeeked:1,\n\t\t\t\thead_frag:1,\n\t\t\t\tpfmemalloc:1,\n\t\t\t\tpp_recycle:1;  \n#ifdef CONFIG_SKB_EXTENSIONS\n\t__u8\t\t\tactive_extensions;\n#endif\n\n\t \n\tstruct_group(headers,\n\n\t \n\t__u8\t\t\t__pkt_type_offset[0];\n\t \n\t__u8\t\t\tpkt_type:3;  \n\t__u8\t\t\tignore_df:1;\n\t__u8\t\t\tdst_pending_confirm:1;\n\t__u8\t\t\tip_summed:2;\n\t__u8\t\t\tooo_okay:1;\n\n\t \n\t__u8\t\t\t__mono_tc_offset[0];\n\t \n\t__u8\t\t\tmono_delivery_time:1;\t \n#ifdef CONFIG_NET_XGRESS\n\t__u8\t\t\ttc_at_ingress:1;\t \n\t__u8\t\t\ttc_skip_classify:1;\n#endif\n\t__u8\t\t\tremcsum_offload:1;\n\t__u8\t\t\tcsum_complete_sw:1;\n\t__u8\t\t\tcsum_level:2;\n\t__u8\t\t\tinner_protocol_type:1;\n\n\t__u8\t\t\tl4_hash:1;\n\t__u8\t\t\tsw_hash:1;\n#ifdef CONFIG_WIRELESS\n\t__u8\t\t\twifi_acked_valid:1;\n\t__u8\t\t\twifi_acked:1;\n#endif\n\t__u8\t\t\tno_fcs:1;\n\t \n\t__u8\t\t\tencapsulation:1;\n\t__u8\t\t\tencap_hdr_csum:1;\n\t__u8\t\t\tcsum_valid:1;\n#ifdef CONFIG_IPV6_NDISC_NODETYPE\n\t__u8\t\t\tndisc_nodetype:2;\n#endif\n\n#if IS_ENABLED(CONFIG_IP_VS)\n\t__u8\t\t\tipvs_property:1;\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)\n\t__u8\t\t\tnf_trace:1;\n#endif\n#ifdef CONFIG_NET_SWITCHDEV\n\t__u8\t\t\toffload_fwd_mark:1;\n\t__u8\t\t\toffload_l3_fwd_mark:1;\n#endif\n\t__u8\t\t\tredirected:1;\n#ifdef CONFIG_NET_REDIRECT\n\t__u8\t\t\tfrom_ingress:1;\n#endif\n#ifdef CONFIG_NETFILTER_SKIP_EGRESS\n\t__u8\t\t\tnf_skip_egress:1;\n#endif\n#ifdef CONFIG_TLS_DEVICE\n\t__u8\t\t\tdecrypted:1;\n#endif\n\t__u8\t\t\tslow_gro:1;\n#if IS_ENABLED(CONFIG_IP_SCTP)\n\t__u8\t\t\tcsum_not_inet:1;\n#endif\n\n#if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)\n\t__u16\t\t\ttc_index;\t \n#endif\n\n\tu16\t\t\talloc_cpu;\n\n\tunion {\n\t\t__wsum\t\tcsum;\n\t\tstruct {\n\t\t\t__u16\tcsum_start;\n\t\t\t__u16\tcsum_offset;\n\t\t};\n\t};\n\t__u32\t\t\tpriority;\n\tint\t\t\tskb_iif;\n\t__u32\t\t\thash;\n\tunion {\n\t\tu32\t\tvlan_all;\n\t\tstruct {\n\t\t\t__be16\tvlan_proto;\n\t\t\t__u16\tvlan_tci;\n\t\t};\n\t};\n#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)\n\tunion {\n\t\tunsigned int\tnapi_id;\n\t\tunsigned int\tsender_cpu;\n\t};\n#endif\n#ifdef CONFIG_NETWORK_SECMARK\n\t__u32\t\tsecmark;\n#endif\n\n\tunion {\n\t\t__u32\t\tmark;\n\t\t__u32\t\treserved_tailroom;\n\t};\n\n\tunion {\n\t\t__be16\t\tinner_protocol;\n\t\t__u8\t\tinner_ipproto;\n\t};\n\n\t__u16\t\t\tinner_transport_header;\n\t__u16\t\t\tinner_network_header;\n\t__u16\t\t\tinner_mac_header;\n\n\t__be16\t\t\tprotocol;\n\t__u16\t\t\ttransport_header;\n\t__u16\t\t\tnetwork_header;\n\t__u16\t\t\tmac_header;\n\n#ifdef CONFIG_KCOV\n\tu64\t\t\tkcov_handle;\n#endif\n\n\t);  \n\n\t \n\tsk_buff_data_t\t\ttail;\n\tsk_buff_data_t\t\tend;\n\tunsigned char\t\t*head,\n\t\t\t\t*data;\n\tunsigned int\t\ttruesize;\n\trefcount_t\t\tusers;\n\n#ifdef CONFIG_SKB_EXTENSIONS\n\t \n\tstruct skb_ext\t\t*extensions;\n#endif\n};\n\n \n#ifdef __BIG_ENDIAN_BITFIELD\n#define PKT_TYPE_MAX\t(7 << 5)\n#else\n#define PKT_TYPE_MAX\t7\n#endif\n#define PKT_TYPE_OFFSET\t\toffsetof(struct sk_buff, __pkt_type_offset)\n\n \n#ifdef __BIG_ENDIAN_BITFIELD\n#define SKB_MONO_DELIVERY_TIME_MASK\t(1 << 7)\n#define TC_AT_INGRESS_MASK\t\t(1 << 6)\n#else\n#define SKB_MONO_DELIVERY_TIME_MASK\t(1 << 0)\n#define TC_AT_INGRESS_MASK\t\t(1 << 1)\n#endif\n#define SKB_BF_MONO_TC_OFFSET\t\toffsetof(struct sk_buff, __mono_tc_offset)\n\n#ifdef __KERNEL__\n \n\n#define SKB_ALLOC_FCLONE\t0x01\n#define SKB_ALLOC_RX\t\t0x02\n#define SKB_ALLOC_NAPI\t\t0x04\n\n \nstatic inline bool skb_pfmemalloc(const struct sk_buff *skb)\n{\n\treturn unlikely(skb->pfmemalloc);\n}\n\n \n#define SKB_DST_NOREF\t1UL\n#define SKB_DST_PTRMASK\t~(SKB_DST_NOREF)\n\n \nstatic inline struct dst_entry *skb_dst(const struct sk_buff *skb)\n{\n\t \n\tWARN_ON((skb->_skb_refdst & SKB_DST_NOREF) &&\n\t\t!rcu_read_lock_held() &&\n\t\t!rcu_read_lock_bh_held());\n\treturn (struct dst_entry *)(skb->_skb_refdst & SKB_DST_PTRMASK);\n}\n\n \nstatic inline void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tskb->slow_gro |= !!dst;\n\tskb->_skb_refdst = (unsigned long)dst;\n}\n\n \nstatic inline void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)\n{\n\tWARN_ON(!rcu_read_lock_held() && !rcu_read_lock_bh_held());\n\tskb->slow_gro |= !!dst;\n\tskb->_skb_refdst = (unsigned long)dst | SKB_DST_NOREF;\n}\n\n \nstatic inline bool skb_dst_is_noref(const struct sk_buff *skb)\n{\n\treturn (skb->_skb_refdst & SKB_DST_NOREF) && skb_dst(skb);\n}\n\n \nstatic inline struct rtable *skb_rtable(const struct sk_buff *skb)\n{\n\treturn (struct rtable *)skb_dst(skb);\n}\n\n \nstatic inline bool skb_pkt_type_ok(u32 ptype)\n{\n\treturn ptype <= PACKET_OTHERHOST;\n}\n\n \nstatic inline unsigned int skb_napi_id(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn skb->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_wifi_acked_valid(const struct sk_buff *skb)\n{\n#ifdef CONFIG_WIRELESS\n\treturn skb->wifi_acked_valid;\n#else\n\treturn 0;\n#endif\n}\n\n \nstatic inline bool skb_unref(struct sk_buff *skb)\n{\n\tif (unlikely(!skb))\n\t\treturn false;\n\tif (likely(refcount_read(&skb->users) == 1))\n\t\tsmp_rmb();\n\telse if (likely(!refcount_dec_and_test(&skb->users)))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid __fix_address\nkfree_skb_reason(struct sk_buff *skb, enum skb_drop_reason reason);\n\n \nstatic inline void kfree_skb(struct sk_buff *skb)\n{\n\tkfree_skb_reason(skb, SKB_DROP_REASON_NOT_SPECIFIED);\n}\n\nvoid skb_release_head_state(struct sk_buff *skb);\nvoid kfree_skb_list_reason(struct sk_buff *segs,\n\t\t\t   enum skb_drop_reason reason);\nvoid skb_dump(const char *level, const struct sk_buff *skb, bool full_pkt);\nvoid skb_tx_error(struct sk_buff *skb);\n\nstatic inline void kfree_skb_list(struct sk_buff *segs)\n{\n\tkfree_skb_list_reason(segs, SKB_DROP_REASON_NOT_SPECIFIED);\n}\n\n#ifdef CONFIG_TRACEPOINTS\nvoid consume_skb(struct sk_buff *skb);\n#else\nstatic inline void consume_skb(struct sk_buff *skb)\n{\n\treturn kfree_skb(skb);\n}\n#endif\n\nvoid __consume_stateless_skb(struct sk_buff *skb);\nvoid  __kfree_skb(struct sk_buff *skb);\nextern struct kmem_cache *skbuff_cache;\n\nvoid kfree_skb_partial(struct sk_buff *skb, bool head_stolen);\nbool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,\n\t\t      bool *fragstolen, int *delta_truesize);\n\nstruct sk_buff *__alloc_skb(unsigned int size, gfp_t priority, int flags,\n\t\t\t    int node);\nstruct sk_buff *__build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *build_skb_around(struct sk_buff *skb,\n\t\t\t\t void *data, unsigned int frag_size);\nvoid skb_attempt_defer_free(struct sk_buff *skb);\n\nstruct sk_buff *napi_build_skb(void *data, unsigned int frag_size);\nstruct sk_buff *slab_build_skb(void *data);\n\n \nstatic inline struct sk_buff *alloc_skb(unsigned int size,\n\t\t\t\t\tgfp_t priority)\n{\n\treturn __alloc_skb(size, priority, 0, NUMA_NO_NODE);\n}\n\nstruct sk_buff *alloc_skb_with_frags(unsigned long header_len,\n\t\t\t\t     unsigned long data_len,\n\t\t\t\t     int max_page_order,\n\t\t\t\t     int *errcode,\n\t\t\t\t     gfp_t gfp_mask);\nstruct sk_buff *alloc_skb_for_msg(struct sk_buff *first);\n\n \nstruct sk_buff_fclones {\n\tstruct sk_buff\tskb1;\n\n\tstruct sk_buff\tskb2;\n\n\trefcount_t\tfclone_ref;\n};\n\n \nstatic inline bool skb_fclone_busy(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\tconst struct sk_buff_fclones *fclones;\n\n\tfclones = container_of(skb, struct sk_buff_fclones, skb1);\n\n\treturn skb->fclone == SKB_FCLONE_ORIG &&\n\t       refcount_read(&fclones->fclone_ref) > 1 &&\n\t       READ_ONCE(fclones->skb2.sk) == sk;\n}\n\n \nstatic inline struct sk_buff *alloc_skb_fclone(unsigned int size,\n\t\t\t\t\t       gfp_t priority)\n{\n\treturn __alloc_skb(size, priority, SKB_ALLOC_FCLONE, NUMA_NO_NODE);\n}\n\nstruct sk_buff *skb_morph(struct sk_buff *dst, struct sk_buff *src);\nvoid skb_headers_offset_update(struct sk_buff *skb, int off);\nint skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask);\nstruct sk_buff *skb_clone(struct sk_buff *skb, gfp_t priority);\nvoid skb_copy_header(struct sk_buff *new, const struct sk_buff *old);\nstruct sk_buff *skb_copy(const struct sk_buff *skb, gfp_t priority);\nstruct sk_buff *__pskb_copy_fclone(struct sk_buff *skb, int headroom,\n\t\t\t\t   gfp_t gfp_mask, bool fclone);\nstatic inline struct sk_buff *__pskb_copy(struct sk_buff *skb, int headroom,\n\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, headroom, gfp_mask, false);\n}\n\nint pskb_expand_head(struct sk_buff *skb, int nhead, int ntail, gfp_t gfp_mask);\nstruct sk_buff *skb_realloc_headroom(struct sk_buff *skb,\n\t\t\t\t     unsigned int headroom);\nstruct sk_buff *skb_expand_head(struct sk_buff *skb, unsigned int headroom);\nstruct sk_buff *skb_copy_expand(const struct sk_buff *skb, int newheadroom,\n\t\t\t\tint newtailroom, gfp_t priority);\nint __must_check skb_to_sgvec_nomark(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t\t     int offset, int len);\nint __must_check skb_to_sgvec(struct sk_buff *skb, struct scatterlist *sg,\n\t\t\t      int offset, int len);\nint skb_cow_data(struct sk_buff *skb, int tailbits, struct sk_buff **trailer);\nint __skb_pad(struct sk_buff *skb, int pad, bool free_on_error);\n\n \nstatic inline int skb_pad(struct sk_buff *skb, int pad)\n{\n\treturn __skb_pad(skb, pad, true);\n}\n#define dev_kfree_skb(a)\tconsume_skb(a)\n\nint skb_append_pagefrags(struct sk_buff *skb, struct page *page,\n\t\t\t int offset, size_t size, size_t max_frags);\n\nstruct skb_seq_state {\n\t__u32\t\tlower_offset;\n\t__u32\t\tupper_offset;\n\t__u32\t\tfrag_idx;\n\t__u32\t\tstepped_offset;\n\tstruct sk_buff\t*root_skb;\n\tstruct sk_buff\t*cur_skb;\n\t__u8\t\t*frag_data;\n\t__u32\t\tfrag_off;\n};\n\nvoid skb_prepare_seq_read(struct sk_buff *skb, unsigned int from,\n\t\t\t  unsigned int to, struct skb_seq_state *st);\nunsigned int skb_seq_read(unsigned int consumed, const u8 **data,\n\t\t\t  struct skb_seq_state *st);\nvoid skb_abort_seq_read(struct skb_seq_state *st);\n\nunsigned int skb_find_text(struct sk_buff *skb, unsigned int from,\n\t\t\t   unsigned int to, struct ts_config *config);\n\n \nenum pkt_hash_types {\n\tPKT_HASH_TYPE_NONE,\t \n\tPKT_HASH_TYPE_L2,\t \n\tPKT_HASH_TYPE_L3,\t \n\tPKT_HASH_TYPE_L4,\t \n};\n\nstatic inline void skb_clear_hash(struct sk_buff *skb)\n{\n\tskb->hash = 0;\n\tskb->sw_hash = 0;\n\tskb->l4_hash = 0;\n}\n\nstatic inline void skb_clear_hash_if_not_l4(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash)\n\t\tskb_clear_hash(skb);\n}\n\nstatic inline void\n__skb_set_hash(struct sk_buff *skb, __u32 hash, bool is_sw, bool is_l4)\n{\n\tskb->l4_hash = is_l4;\n\tskb->sw_hash = is_sw;\n\tskb->hash = hash;\n}\n\nstatic inline void\nskb_set_hash(struct sk_buff *skb, __u32 hash, enum pkt_hash_types type)\n{\n\t \n\t__skb_set_hash(skb, hash, false, type == PKT_HASH_TYPE_L4);\n}\n\nstatic inline void\n__skb_set_sw_hash(struct sk_buff *skb, __u32 hash, bool is_l4)\n{\n\t__skb_set_hash(skb, hash, true, is_l4);\n}\n\nvoid __skb_get_hash(struct sk_buff *skb);\nu32 __skb_get_hash_symmetric(const struct sk_buff *skb);\nu32 skb_get_poff(const struct sk_buff *skb);\nu32 __skb_get_poff(const struct sk_buff *skb, const void *data,\n\t\t   const struct flow_keys_basic *keys, int hlen);\n__be32 __skb_flow_get_ports(const struct sk_buff *skb, int thoff, u8 ip_proto,\n\t\t\t    const void *data, int hlen_proto);\n\nstatic inline __be32 skb_flow_get_ports(const struct sk_buff *skb,\n\t\t\t\t\tint thoff, u8 ip_proto)\n{\n\treturn __skb_flow_get_ports(skb, thoff, ip_proto, NULL, 0);\n}\n\nvoid skb_flow_dissector_init(struct flow_dissector *flow_dissector,\n\t\t\t     const struct flow_dissector_key *key,\n\t\t\t     unsigned int key_count);\n\nstruct bpf_flow_dissector;\nu32 bpf_flow_dissect(struct bpf_prog *prog, struct bpf_flow_dissector *ctx,\n\t\t     __be16 proto, int nhoff, int hlen, unsigned int flags);\n\nbool __skb_flow_dissect(const struct net *net,\n\t\t\tconst struct sk_buff *skb,\n\t\t\tstruct flow_dissector *flow_dissector,\n\t\t\tvoid *target_container, const void *data,\n\t\t\t__be16 proto, int nhoff, int hlen, unsigned int flags);\n\nstatic inline bool skb_flow_dissect(const struct sk_buff *skb,\n\t\t\t\t    struct flow_dissector *flow_dissector,\n\t\t\t\t    void *target_container, unsigned int flags)\n{\n\treturn __skb_flow_dissect(NULL, skb, flow_dissector,\n\t\t\t\t  target_container, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool skb_flow_dissect_flow_keys(const struct sk_buff *skb,\n\t\t\t\t\t      struct flow_keys *flow,\n\t\t\t\t\t      unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(NULL, skb, &flow_keys_dissector,\n\t\t\t\t  flow, NULL, 0, 0, 0, flags);\n}\n\nstatic inline bool\nskb_flow_dissect_flow_keys_basic(const struct net *net,\n\t\t\t\t const struct sk_buff *skb,\n\t\t\t\t struct flow_keys_basic *flow,\n\t\t\t\t const void *data, __be16 proto,\n\t\t\t\t int nhoff, int hlen, unsigned int flags)\n{\n\tmemset(flow, 0, sizeof(*flow));\n\treturn __skb_flow_dissect(net, skb, &flow_keys_basic_dissector, flow,\n\t\t\t\t  data, proto, nhoff, hlen, flags);\n}\n\nvoid skb_flow_dissect_meta(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\n \nvoid\nskb_flow_dissect_ct(const struct sk_buff *skb,\n\t\t    struct flow_dissector *flow_dissector,\n\t\t    void *target_container,\n\t\t    u16 *ctinfo_map, size_t mapsize,\n\t\t    bool post_ct, u16 zone);\nvoid\nskb_flow_dissect_tunnel_info(const struct sk_buff *skb,\n\t\t\t     struct flow_dissector *flow_dissector,\n\t\t\t     void *target_container);\n\nvoid skb_flow_dissect_hash(const struct sk_buff *skb,\n\t\t\t   struct flow_dissector *flow_dissector,\n\t\t\t   void *target_container);\n\nstatic inline __u32 skb_get_hash(struct sk_buff *skb)\n{\n\tif (!skb->l4_hash && !skb->sw_hash)\n\t\t__skb_get_hash(skb);\n\n\treturn skb->hash;\n}\n\nstatic inline __u32 skb_get_hash_flowi6(struct sk_buff *skb, const struct flowi6 *fl6)\n{\n\tif (!skb->l4_hash && !skb->sw_hash) {\n\t\tstruct flow_keys keys;\n\t\t__u32 hash = __get_hash_from_flowi6(fl6, &keys);\n\n\t\t__skb_set_sw_hash(skb, hash, flow_keys_have_l4(&keys));\n\t}\n\n\treturn skb->hash;\n}\n\n__u32 skb_get_hash_perturb(const struct sk_buff *skb,\n\t\t\t   const siphash_key_t *perturb);\n\nstatic inline __u32 skb_get_hash_raw(const struct sk_buff *skb)\n{\n\treturn skb->hash;\n}\n\nstatic inline void skb_copy_hash(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->hash = from->hash;\n\tto->sw_hash = from->sw_hash;\n\tto->l4_hash = from->l4_hash;\n};\n\nstatic inline int skb_cmp_decrypted(const struct sk_buff *skb1,\n\t\t\t\t    const struct sk_buff *skb2)\n{\n#ifdef CONFIG_TLS_DEVICE\n\treturn skb2->decrypted - skb1->decrypted;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void skb_copy_decrypted(struct sk_buff *to,\n\t\t\t\t      const struct sk_buff *from)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tto->decrypted = from->decrypted;\n#endif\n}\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline void skb_set_end_offset(struct sk_buff *skb, unsigned int offset)\n{\n\tskb->end = offset;\n}\n#else\nstatic inline unsigned char *skb_end_pointer(const struct sk_buff *skb)\n{\n\treturn skb->end;\n}\n\nstatic inline unsigned int skb_end_offset(const struct sk_buff *skb)\n{\n\treturn skb->end - skb->head;\n}\n\nstatic inline void skb_set_end_offset(struct sk_buff *skb, unsigned int offset)\n{\n\tskb->end = skb->head + offset;\n}\n#endif\n\nstruct ubuf_info *msg_zerocopy_realloc(struct sock *sk, size_t size,\n\t\t\t\t       struct ubuf_info *uarg);\n\nvoid msg_zerocopy_put_abort(struct ubuf_info *uarg, bool have_uref);\n\nvoid msg_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t   bool success);\n\nint __zerocopy_sg_from_iter(struct msghdr *msg, struct sock *sk,\n\t\t\t    struct sk_buff *skb, struct iov_iter *from,\n\t\t\t    size_t length);\n\nstatic inline int skb_zerocopy_iter_dgram(struct sk_buff *skb,\n\t\t\t\t\t  struct msghdr *msg, int len)\n{\n\treturn __zerocopy_sg_from_iter(msg, skb->sk, skb, &msg->msg_iter, len);\n}\n\nint skb_zerocopy_iter_stream(struct sock *sk, struct sk_buff *skb,\n\t\t\t     struct msghdr *msg, int len,\n\t\t\t     struct ubuf_info *uarg);\n\n \n#define skb_shinfo(SKB)\t((struct skb_shared_info *)(skb_end_pointer(SKB)))\n\nstatic inline struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb)\n{\n\treturn &skb_shinfo(skb)->hwtstamps;\n}\n\nstatic inline struct ubuf_info *skb_zcopy(struct sk_buff *skb)\n{\n\tbool is_zcopy = skb && skb_shinfo(skb)->flags & SKBFL_ZEROCOPY_ENABLE;\n\n\treturn is_zcopy ? skb_uarg(skb) : NULL;\n}\n\nstatic inline bool skb_zcopy_pure(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->flags & SKBFL_PURE_ZEROCOPY;\n}\n\nstatic inline bool skb_zcopy_managed(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->flags & SKBFL_MANAGED_FRAG_REFS;\n}\n\nstatic inline bool skb_pure_zcopy_same(const struct sk_buff *skb1,\n\t\t\t\t       const struct sk_buff *skb2)\n{\n\treturn skb_zcopy_pure(skb1) == skb_zcopy_pure(skb2);\n}\n\nstatic inline void net_zcopy_get(struct ubuf_info *uarg)\n{\n\trefcount_inc(&uarg->refcnt);\n}\n\nstatic inline void skb_zcopy_init(struct sk_buff *skb, struct ubuf_info *uarg)\n{\n\tskb_shinfo(skb)->destructor_arg = uarg;\n\tskb_shinfo(skb)->flags |= uarg->flags;\n}\n\nstatic inline void skb_zcopy_set(struct sk_buff *skb, struct ubuf_info *uarg,\n\t\t\t\t bool *have_ref)\n{\n\tif (skb && uarg && !skb_zcopy(skb)) {\n\t\tif (unlikely(have_ref && *have_ref))\n\t\t\t*have_ref = false;\n\t\telse\n\t\t\tnet_zcopy_get(uarg);\n\t\tskb_zcopy_init(skb, uarg);\n\t}\n}\n\nstatic inline void skb_zcopy_set_nouarg(struct sk_buff *skb, void *val)\n{\n\tskb_shinfo(skb)->destructor_arg = (void *)((uintptr_t) val | 0x1UL);\n\tskb_shinfo(skb)->flags |= SKBFL_ZEROCOPY_FRAG;\n}\n\nstatic inline bool skb_zcopy_is_nouarg(struct sk_buff *skb)\n{\n\treturn (uintptr_t) skb_shinfo(skb)->destructor_arg & 0x1UL;\n}\n\nstatic inline void *skb_zcopy_get_nouarg(struct sk_buff *skb)\n{\n\treturn (void *)((uintptr_t) skb_shinfo(skb)->destructor_arg & ~0x1UL);\n}\n\nstatic inline void net_zcopy_put(struct ubuf_info *uarg)\n{\n\tif (uarg)\n\t\tuarg->callback(NULL, uarg, true);\n}\n\nstatic inline void net_zcopy_put_abort(struct ubuf_info *uarg, bool have_uref)\n{\n\tif (uarg) {\n\t\tif (uarg->callback == msg_zerocopy_callback)\n\t\t\tmsg_zerocopy_put_abort(uarg, have_uref);\n\t\telse if (have_uref)\n\t\t\tnet_zcopy_put(uarg);\n\t}\n}\n\n \nstatic inline void skb_zcopy_clear(struct sk_buff *skb, bool zerocopy_success)\n{\n\tstruct ubuf_info *uarg = skb_zcopy(skb);\n\n\tif (uarg) {\n\t\tif (!skb_zcopy_is_nouarg(skb))\n\t\t\tuarg->callback(skb, uarg, zerocopy_success);\n\n\t\tskb_shinfo(skb)->flags &= ~SKBFL_ALL_ZEROCOPY;\n\t}\n}\n\nvoid __skb_zcopy_downgrade_managed(struct sk_buff *skb);\n\nstatic inline void skb_zcopy_downgrade_managed(struct sk_buff *skb)\n{\n\tif (unlikely(skb_zcopy_managed(skb)))\n\t\t__skb_zcopy_downgrade_managed(skb);\n}\n\nstatic inline void skb_mark_not_on_list(struct sk_buff *skb)\n{\n\tskb->next = NULL;\n}\n\nstatic inline void skb_poison_list(struct sk_buff *skb)\n{\n#ifdef CONFIG_DEBUG_NET\n\tskb->next = SKB_LIST_POISON_NEXT;\n#endif\n}\n\n \n#define skb_list_walk_safe(first, skb, next_skb)                               \\\n\tfor ((skb) = (first), (next_skb) = (skb) ? (skb)->next : NULL; (skb);  \\\n\t     (skb) = (next_skb), (next_skb) = (skb) ? (skb)->next : NULL)\n\nstatic inline void skb_list_del_init(struct sk_buff *skb)\n{\n\t__list_del_entry(&skb->list);\n\tskb_mark_not_on_list(skb);\n}\n\n \nstatic inline int skb_queue_empty(const struct sk_buff_head *list)\n{\n\treturn list->next == (const struct sk_buff *) list;\n}\n\n \nstatic inline bool skb_queue_empty_lockless(const struct sk_buff_head *list)\n{\n\treturn READ_ONCE(list->next) == (const struct sk_buff *) list;\n}\n\n\n \nstatic inline bool skb_queue_is_last(const struct sk_buff_head *list,\n\t\t\t\t     const struct sk_buff *skb)\n{\n\treturn skb->next == (const struct sk_buff *) list;\n}\n\n \nstatic inline bool skb_queue_is_first(const struct sk_buff_head *list,\n\t\t\t\t      const struct sk_buff *skb)\n{\n\treturn skb->prev == (const struct sk_buff *) list;\n}\n\n \nstatic inline struct sk_buff *skb_queue_next(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t \n\tBUG_ON(skb_queue_is_last(list, skb));\n\treturn skb->next;\n}\n\n \nstatic inline struct sk_buff *skb_queue_prev(const struct sk_buff_head *list,\n\t\t\t\t\t     const struct sk_buff *skb)\n{\n\t \n\tBUG_ON(skb_queue_is_first(list, skb));\n\treturn skb->prev;\n}\n\n \nstatic inline struct sk_buff *skb_get(struct sk_buff *skb)\n{\n\trefcount_inc(&skb->users);\n\treturn skb;\n}\n\n \n\n \nstatic inline int skb_cloned(const struct sk_buff *skb)\n{\n\treturn skb->cloned &&\n\t       (atomic_read(&skb_shinfo(skb)->dataref) & SKB_DATAREF_MASK) != 1;\n}\n\nstatic inline int skb_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n \nint __skb_unclone_keeptruesize(struct sk_buff *skb, gfp_t pri);\nstatic inline int skb_unclone_keeptruesize(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_cloned(skb))\n\t\treturn __skb_unclone_keeptruesize(skb, pri);\n\treturn 0;\n}\n\n \nstatic inline int skb_header_cloned(const struct sk_buff *skb)\n{\n\tint dataref;\n\n\tif (!skb->cloned)\n\t\treturn 0;\n\n\tdataref = atomic_read(&skb_shinfo(skb)->dataref);\n\tdataref = (dataref & SKB_DATAREF_MASK) - (dataref >> SKB_DATAREF_SHIFT);\n\treturn dataref != 1;\n}\n\nstatic inline int skb_header_unclone(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\n\tif (skb_header_cloned(skb))\n\t\treturn pskb_expand_head(skb, 0, 0, pri);\n\n\treturn 0;\n}\n\n \nstatic inline void __skb_header_release(struct sk_buff *skb)\n{\n\tskb->nohdr = 1;\n\tatomic_set(&skb_shinfo(skb)->dataref, 1 + (1 << SKB_DATAREF_SHIFT));\n}\n\n\n \nstatic inline int skb_shared(const struct sk_buff *skb)\n{\n\treturn refcount_read(&skb->users) != 1;\n}\n\n \nstatic inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_shared(skb)) {\n\t\tstruct sk_buff *nskb = skb_clone(skb, pri);\n\n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n \n\n \nstatic inline struct sk_buff *skb_unshare(struct sk_buff *skb,\n\t\t\t\t\t  gfp_t pri)\n{\n\tmight_sleep_if(gfpflags_allow_blocking(pri));\n\tif (skb_cloned(skb)) {\n\t\tstruct sk_buff *nskb = skb_copy(skb, pri);\n\n\t\t \n\t\tif (likely(nskb))\n\t\t\tconsume_skb(skb);\n\t\telse\n\t\t\tkfree_skb(skb);\n\t\tskb = nskb;\n\t}\n\treturn skb;\n}\n\n \nstatic inline struct sk_buff *skb_peek(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = list_->next;\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n}\n\n \nstatic inline struct sk_buff *__skb_peek(const struct sk_buff_head *list_)\n{\n\treturn list_->next;\n}\n\n \nstatic inline struct sk_buff *skb_peek_next(struct sk_buff *skb,\n\t\tconst struct sk_buff_head *list_)\n{\n\tstruct sk_buff *next = skb->next;\n\n\tif (next == (struct sk_buff *)list_)\n\t\tnext = NULL;\n\treturn next;\n}\n\n \nstatic inline struct sk_buff *skb_peek_tail(const struct sk_buff_head *list_)\n{\n\tstruct sk_buff *skb = READ_ONCE(list_->prev);\n\n\tif (skb == (struct sk_buff *)list_)\n\t\tskb = NULL;\n\treturn skb;\n\n}\n\n \nstatic inline __u32 skb_queue_len(const struct sk_buff_head *list_)\n{\n\treturn list_->qlen;\n}\n\n \nstatic inline __u32 skb_queue_len_lockless(const struct sk_buff_head *list_)\n{\n\treturn READ_ONCE(list_->qlen);\n}\n\n \nstatic inline void __skb_queue_head_init(struct sk_buff_head *list)\n{\n\tlist->prev = list->next = (struct sk_buff *)list;\n\tlist->qlen = 0;\n}\n\n \nstatic inline void skb_queue_head_init(struct sk_buff_head *list)\n{\n\tspin_lock_init(&list->lock);\n\t__skb_queue_head_init(list);\n}\n\nstatic inline void skb_queue_head_init_class(struct sk_buff_head *list,\n\t\tstruct lock_class_key *class)\n{\n\tskb_queue_head_init(list);\n\tlockdep_set_class(&list->lock, class);\n}\n\n \nstatic inline void __skb_insert(struct sk_buff *newsk,\n\t\t\t\tstruct sk_buff *prev, struct sk_buff *next,\n\t\t\t\tstruct sk_buff_head *list)\n{\n\t \n\tWRITE_ONCE(newsk->next, next);\n\tWRITE_ONCE(newsk->prev, prev);\n\tWRITE_ONCE(((struct sk_buff_list *)next)->prev, newsk);\n\tWRITE_ONCE(((struct sk_buff_list *)prev)->next, newsk);\n\tWRITE_ONCE(list->qlen, list->qlen + 1);\n}\n\nstatic inline void __skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *prev,\n\t\t\t\t      struct sk_buff *next)\n{\n\tstruct sk_buff *first = list->next;\n\tstruct sk_buff *last = list->prev;\n\n\tWRITE_ONCE(first->prev, prev);\n\tWRITE_ONCE(prev->next, first);\n\n\tWRITE_ONCE(last->next, next);\n\tWRITE_ONCE(next->prev, last);\n}\n\n \nstatic inline void skb_queue_splice(const struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n \nstatic inline void skb_queue_splice_init(struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, (struct sk_buff *) head, head->next);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n \nstatic inline void skb_queue_splice_tail(const struct sk_buff_head *list,\n\t\t\t\t\t struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t}\n}\n\n \nstatic inline void skb_queue_splice_tail_init(struct sk_buff_head *list,\n\t\t\t\t\t      struct sk_buff_head *head)\n{\n\tif (!skb_queue_empty(list)) {\n\t\t__skb_queue_splice(list, head->prev, (struct sk_buff *) head);\n\t\thead->qlen += list->qlen;\n\t\t__skb_queue_head_init(list);\n\t}\n}\n\n \nstatic inline void __skb_queue_after(struct sk_buff_head *list,\n\t\t\t\t     struct sk_buff *prev,\n\t\t\t\t     struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, prev, ((struct sk_buff_list *)prev)->next, list);\n}\n\nvoid skb_append(struct sk_buff *old, struct sk_buff *newsk,\n\t\tstruct sk_buff_head *list);\n\nstatic inline void __skb_queue_before(struct sk_buff_head *list,\n\t\t\t\t      struct sk_buff *next,\n\t\t\t\t      struct sk_buff *newsk)\n{\n\t__skb_insert(newsk, ((struct sk_buff_list *)next)->prev, next, list);\n}\n\n \nstatic inline void __skb_queue_head(struct sk_buff_head *list,\n\t\t\t\t    struct sk_buff *newsk)\n{\n\t__skb_queue_after(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_head(struct sk_buff_head *list, struct sk_buff *newsk);\n\n \nstatic inline void __skb_queue_tail(struct sk_buff_head *list,\n\t\t\t\t   struct sk_buff *newsk)\n{\n\t__skb_queue_before(list, (struct sk_buff *)list, newsk);\n}\nvoid skb_queue_tail(struct sk_buff_head *list, struct sk_buff *newsk);\n\n \nvoid skb_unlink(struct sk_buff *skb, struct sk_buff_head *list);\nstatic inline void __skb_unlink(struct sk_buff *skb, struct sk_buff_head *list)\n{\n\tstruct sk_buff *next, *prev;\n\n\tWRITE_ONCE(list->qlen, list->qlen - 1);\n\tnext\t   = skb->next;\n\tprev\t   = skb->prev;\n\tskb->next  = skb->prev = NULL;\n\tWRITE_ONCE(next->prev, prev);\n\tWRITE_ONCE(prev->next, next);\n}\n\n \nstatic inline struct sk_buff *__skb_dequeue(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue(struct sk_buff_head *list);\n\n \nstatic inline struct sk_buff *__skb_dequeue_tail(struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb = skb_peek_tail(list);\n\tif (skb)\n\t\t__skb_unlink(skb, list);\n\treturn skb;\n}\nstruct sk_buff *skb_dequeue_tail(struct sk_buff_head *list);\n\n\nstatic inline bool skb_is_nonlinear(const struct sk_buff *skb)\n{\n\treturn skb->data_len;\n}\n\nstatic inline unsigned int skb_headlen(const struct sk_buff *skb)\n{\n\treturn skb->len - skb->data_len;\n}\n\nstatic inline unsigned int __skb_pagelen(const struct sk_buff *skb)\n{\n\tunsigned int i, len = 0;\n\n\tfor (i = skb_shinfo(skb)->nr_frags - 1; (int)i >= 0; i--)\n\t\tlen += skb_frag_size(&skb_shinfo(skb)->frags[i]);\n\treturn len;\n}\n\nstatic inline unsigned int skb_pagelen(const struct sk_buff *skb)\n{\n\treturn skb_headlen(skb) + __skb_pagelen(skb);\n}\n\nstatic inline void skb_frag_fill_page_desc(skb_frag_t *frag,\n\t\t\t\t\t   struct page *page,\n\t\t\t\t\t   int off, int size)\n{\n\tfrag->bv_page = page;\n\tfrag->bv_offset = off;\n\tskb_frag_size_set(frag, size);\n}\n\nstatic inline void __skb_fill_page_desc_noacc(struct skb_shared_info *shinfo,\n\t\t\t\t\t      int i, struct page *page,\n\t\t\t\t\t      int off, int size)\n{\n\tskb_frag_t *frag = &shinfo->frags[i];\n\n\tskb_frag_fill_page_desc(frag, page, off, size);\n}\n\n \nstatic inline void skb_len_add(struct sk_buff *skb, int delta)\n{\n\tskb->len += delta;\n\tskb->data_len += delta;\n\tskb->truesize += delta;\n}\n\n \nstatic inline void __skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t\tstruct page *page, int off, int size)\n{\n\t__skb_fill_page_desc_noacc(skb_shinfo(skb), i, page, off, size);\n\n\t \n\tpage = compound_head(page);\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc\t= true;\n}\n\n \nstatic inline void skb_fill_page_desc(struct sk_buff *skb, int i,\n\t\t\t\t      struct page *page, int off, int size)\n{\n\t__skb_fill_page_desc(skb, i, page, off, size);\n\tskb_shinfo(skb)->nr_frags = i + 1;\n}\n\n \nstatic inline void skb_fill_page_desc_noacc(struct sk_buff *skb, int i,\n\t\t\t\t\t    struct page *page, int off,\n\t\t\t\t\t    int size)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\t__skb_fill_page_desc_noacc(shinfo, i, page, off, size);\n\tshinfo->nr_frags = i + 1;\n}\n\nvoid skb_add_rx_frag(struct sk_buff *skb, int i, struct page *page, int off,\n\t\t     int size, unsigned int truesize);\n\nvoid skb_coalesce_rx_frag(struct sk_buff *skb, int i, int size,\n\t\t\t  unsigned int truesize);\n\n#define SKB_LINEAR_ASSERT(skb)  BUG_ON(skb_is_nonlinear(skb))\n\n#ifdef NET_SKBUFF_DATA_USES_OFFSET\nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data - skb->head;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_tail_pointer(skb);\n\tskb->tail += offset;\n}\n\n#else  \nstatic inline unsigned char *skb_tail_pointer(const struct sk_buff *skb)\n{\n\treturn skb->tail;\n}\n\nstatic inline void skb_reset_tail_pointer(struct sk_buff *skb)\n{\n\tskb->tail = skb->data;\n}\n\nstatic inline void skb_set_tail_pointer(struct sk_buff *skb, const int offset)\n{\n\tskb->tail = skb->data + offset;\n}\n\n#endif  \n\nstatic inline void skb_assert_len(struct sk_buff *skb)\n{\n#ifdef CONFIG_DEBUG_NET\n\tif (WARN_ONCE(!skb->len, \"%s\\n\", __func__))\n\t\tDO_ONCE_LITE(skb_dump, KERN_ERR, skb, false);\n#endif  \n}\n\n \nvoid *pskb_put(struct sk_buff *skb, struct sk_buff *tail, int len);\nvoid *skb_put(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_put(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_tail_pointer(skb);\n\tSKB_LINEAR_ASSERT(skb);\n\tskb->tail += len;\n\tskb->len  += len;\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\treturn tmp;\n}\n\nstatic inline void *__skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t   unsigned int len)\n{\n\tvoid *tmp = __skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\treturn tmp;\n}\n\nstatic inline void __skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)__skb_put(skb, 1) = val;\n}\n\nstatic inline void *skb_put_zero(struct sk_buff *skb, unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemset(tmp, 0, len);\n\n\treturn tmp;\n}\n\nstatic inline void *skb_put_data(struct sk_buff *skb, const void *data,\n\t\t\t\t unsigned int len)\n{\n\tvoid *tmp = skb_put(skb, len);\n\n\tmemcpy(tmp, data, len);\n\n\treturn tmp;\n}\n\nstatic inline void skb_put_u8(struct sk_buff *skb, u8 val)\n{\n\t*(u8 *)skb_put(skb, 1) = val;\n}\n\nvoid *skb_push(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_push(struct sk_buff *skb, unsigned int len)\n{\n\tskb->data -= len;\n\tskb->len  += len;\n\treturn skb->data;\n}\n\nvoid *skb_pull(struct sk_buff *skb, unsigned int len);\nstatic inline void *__skb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tskb->len -= len;\n\tif (unlikely(skb->len < skb->data_len)) {\n#if defined(CONFIG_DEBUG_NET)\n\t\tskb->len += len;\n\t\tpr_err(\"__skb_pull(len=%u)\\n\", len);\n\t\tskb_dump(KERN_ERR, skb, false);\n#endif\n\t\tBUG();\n\t}\n\treturn skb->data += len;\n}\n\nstatic inline void *skb_pull_inline(struct sk_buff *skb, unsigned int len)\n{\n\treturn unlikely(len > skb->len) ? NULL : __skb_pull(skb, len);\n}\n\nvoid *skb_pull_data(struct sk_buff *skb, size_t len);\n\nvoid *__pskb_pull_tail(struct sk_buff *skb, int delta);\n\nstatic inline enum skb_drop_reason\npskb_may_pull_reason(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len <= skb_headlen(skb)))\n\t\treturn SKB_NOT_DROPPED_YET;\n\n\tif (unlikely(len > skb->len))\n\t\treturn SKB_DROP_REASON_PKT_TOO_SMALL;\n\n\tif (unlikely(!__pskb_pull_tail(skb, len - skb_headlen(skb))))\n\t\treturn SKB_DROP_REASON_NOMEM;\n\n\treturn SKB_NOT_DROPPED_YET;\n}\n\nstatic inline bool pskb_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull_reason(skb, len) == SKB_NOT_DROPPED_YET;\n}\n\nstatic inline void *pskb_pull(struct sk_buff *skb, unsigned int len)\n{\n\tif (!pskb_may_pull(skb, len))\n\t\treturn NULL;\n\n\tskb->len -= len;\n\treturn skb->data += len;\n}\n\nvoid skb_condense(struct sk_buff *skb);\n\n \nstatic inline unsigned int skb_headroom(const struct sk_buff *skb)\n{\n\treturn skb->data - skb->head;\n}\n\n \nstatic inline int skb_tailroom(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? 0 : skb->end - skb->tail;\n}\n\n \nstatic inline int skb_availroom(const struct sk_buff *skb)\n{\n\tif (skb_is_nonlinear(skb))\n\t\treturn 0;\n\n\treturn skb->end - skb->tail - skb->reserved_tailroom;\n}\n\n \nstatic inline void skb_reserve(struct sk_buff *skb, int len)\n{\n\tskb->data += len;\n\tskb->tail += len;\n}\n\n \nstatic inline void skb_tailroom_reserve(struct sk_buff *skb, unsigned int mtu,\n\t\t\t\t\tunsigned int needed_tailroom)\n{\n\tSKB_LINEAR_ASSERT(skb);\n\tif (mtu < skb_tailroom(skb) - needed_tailroom)\n\t\t \n\t\tskb->reserved_tailroom = skb_tailroom(skb) - mtu;\n\telse\n\t\t \n\t\tskb->reserved_tailroom = needed_tailroom;\n}\n\n#define ENCAP_TYPE_ETHER\t0\n#define ENCAP_TYPE_IPPROTO\t1\n\nstatic inline void skb_set_inner_protocol(struct sk_buff *skb,\n\t\t\t\t\t  __be16 protocol)\n{\n\tskb->inner_protocol = protocol;\n\tskb->inner_protocol_type = ENCAP_TYPE_ETHER;\n}\n\nstatic inline void skb_set_inner_ipproto(struct sk_buff *skb,\n\t\t\t\t\t __u8 ipproto)\n{\n\tskb->inner_ipproto = ipproto;\n\tskb->inner_protocol_type = ENCAP_TYPE_IPPROTO;\n}\n\nstatic inline void skb_reset_inner_headers(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->mac_header;\n\tskb->inner_network_header = skb->network_header;\n\tskb->inner_transport_header = skb->transport_header;\n}\n\nstatic inline void skb_reset_mac_len(struct sk_buff *skb)\n{\n\tskb->mac_len = skb->network_header - skb->mac_header;\n}\n\nstatic inline unsigned char *skb_inner_transport_header(const struct sk_buff\n\t\t\t\t\t\t\t*skb)\n{\n\treturn skb->head + skb->inner_transport_header;\n}\n\nstatic inline int skb_inner_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_transport_header(skb) - skb->data;\n}\n\nstatic inline void skb_reset_inner_transport_header(struct sk_buff *skb)\n{\n\tskb->inner_transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_transport_header(struct sk_buff *skb,\n\t\t\t\t\t\t   const int offset)\n{\n\tskb_reset_inner_transport_header(skb);\n\tskb->inner_transport_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_network_header;\n}\n\nstatic inline void skb_reset_inner_network_header(struct sk_buff *skb)\n{\n\tskb->inner_network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_network_header(struct sk_buff *skb,\n\t\t\t\t\t\tconst int offset)\n{\n\tskb_reset_inner_network_header(skb);\n\tskb->inner_network_header += offset;\n}\n\nstatic inline unsigned char *skb_inner_mac_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->inner_mac_header;\n}\n\nstatic inline void skb_reset_inner_mac_header(struct sk_buff *skb)\n{\n\tskb->inner_mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_inner_mac_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_inner_mac_header(skb);\n\tskb->inner_mac_header += offset;\n}\nstatic inline bool skb_transport_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->transport_header != (typeof(skb->transport_header))~0U;\n}\n\nstatic inline unsigned char *skb_transport_header(const struct sk_buff *skb)\n{\n\tDEBUG_NET_WARN_ON_ONCE(!skb_transport_header_was_set(skb));\n\treturn skb->head + skb->transport_header;\n}\n\nstatic inline void skb_reset_transport_header(struct sk_buff *skb)\n{\n\tskb->transport_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_transport_header(struct sk_buff *skb,\n\t\t\t\t\t    const int offset)\n{\n\tskb_reset_transport_header(skb);\n\tskb->transport_header += offset;\n}\n\nstatic inline unsigned char *skb_network_header(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->network_header;\n}\n\nstatic inline void skb_reset_network_header(struct sk_buff *skb)\n{\n\tskb->network_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_network_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_network_header(skb);\n\tskb->network_header += offset;\n}\n\nstatic inline int skb_mac_header_was_set(const struct sk_buff *skb)\n{\n\treturn skb->mac_header != (typeof(skb->mac_header))~0U;\n}\n\nstatic inline unsigned char *skb_mac_header(const struct sk_buff *skb)\n{\n\tDEBUG_NET_WARN_ON_ONCE(!skb_mac_header_was_set(skb));\n\treturn skb->head + skb->mac_header;\n}\n\nstatic inline int skb_mac_offset(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_mac_header_len(const struct sk_buff *skb)\n{\n\tDEBUG_NET_WARN_ON_ONCE(!skb_mac_header_was_set(skb));\n\treturn skb->network_header - skb->mac_header;\n}\n\nstatic inline void skb_unset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = (typeof(skb->mac_header))~0U;\n}\n\nstatic inline void skb_reset_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->data - skb->head;\n}\n\nstatic inline void skb_set_mac_header(struct sk_buff *skb, const int offset)\n{\n\tskb_reset_mac_header(skb);\n\tskb->mac_header += offset;\n}\n\nstatic inline void skb_pop_mac_header(struct sk_buff *skb)\n{\n\tskb->mac_header = skb->network_header;\n}\n\nstatic inline void skb_probe_transport_header(struct sk_buff *skb)\n{\n\tstruct flow_keys_basic keys;\n\n\tif (skb_transport_header_was_set(skb))\n\t\treturn;\n\n\tif (skb_flow_dissect_flow_keys_basic(NULL, skb, &keys,\n\t\t\t\t\t     NULL, 0, 0, 0, 0))\n\t\tskb_set_transport_header(skb, keys.control.thoff);\n}\n\nstatic inline void skb_mac_header_rebuild(struct sk_buff *skb)\n{\n\tif (skb_mac_header_was_set(skb)) {\n\t\tconst unsigned char *old_mac = skb_mac_header(skb);\n\n\t\tskb_set_mac_header(skb, -skb->mac_len);\n\t\tmemmove(skb_mac_header(skb), old_mac, skb->mac_len);\n\t}\n}\n\nstatic inline int skb_checksum_start_offset(const struct sk_buff *skb)\n{\n\treturn skb->csum_start - skb_headroom(skb);\n}\n\nstatic inline unsigned char *skb_checksum_start(const struct sk_buff *skb)\n{\n\treturn skb->head + skb->csum_start;\n}\n\nstatic inline int skb_transport_offset(const struct sk_buff *skb)\n{\n\treturn skb_transport_header(skb) - skb->data;\n}\n\nstatic inline u32 skb_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->transport_header - skb->network_header;\n}\n\nstatic inline u32 skb_inner_network_header_len(const struct sk_buff *skb)\n{\n\treturn skb->inner_transport_header - skb->inner_network_header;\n}\n\nstatic inline int skb_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_network_header(skb) - skb->data;\n}\n\nstatic inline int skb_inner_network_offset(const struct sk_buff *skb)\n{\n\treturn skb_inner_network_header(skb) - skb->data;\n}\n\nstatic inline int pskb_network_may_pull(struct sk_buff *skb, unsigned int len)\n{\n\treturn pskb_may_pull(skb, skb_network_offset(skb) + len);\n}\n\n \n#ifndef NET_IP_ALIGN\n#define NET_IP_ALIGN\t2\n#endif\n\n \n#ifndef NET_SKB_PAD\n#define NET_SKB_PAD\tmax(32, L1_CACHE_BYTES)\n#endif\n\nint ___pskb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline void __skb_set_length(struct sk_buff *skb, unsigned int len)\n{\n\tif (WARN_ON(skb_is_nonlinear(skb)))\n\t\treturn;\n\tskb->len = len;\n\tskb_set_tail_pointer(skb, len);\n}\n\nstatic inline void __skb_trim(struct sk_buff *skb, unsigned int len)\n{\n\t__skb_set_length(skb, len);\n}\n\nvoid skb_trim(struct sk_buff *skb, unsigned int len);\n\nstatic inline int __pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->data_len)\n\t\treturn ___pskb_trim(skb, len);\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int pskb_trim(struct sk_buff *skb, unsigned int len)\n{\n\treturn (len < skb->len) ? __pskb_trim(skb, len) : 0;\n}\n\n \nstatic inline void pskb_trim_unique(struct sk_buff *skb, unsigned int len)\n{\n\tint err = pskb_trim(skb, len);\n\tBUG_ON(err);\n}\n\nstatic inline int __skb_grow(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int diff = len - skb->len;\n\n\tif (skb_tailroom(skb) < diff) {\n\t\tint ret = pskb_expand_head(skb, 0, diff - skb_tailroom(skb),\n\t\t\t\t\t   GFP_ATOMIC);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\t__skb_set_length(skb, len);\n\treturn 0;\n}\n\n \nstatic inline void skb_orphan(struct sk_buff *skb)\n{\n\tif (skb->destructor) {\n\t\tskb->destructor(skb);\n\t\tskb->destructor = NULL;\n\t\tskb->sk\t\t= NULL;\n\t} else {\n\t\tBUG_ON(skb->sk);\n\t}\n}\n\n \nstatic inline int skb_orphan_frags(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\tif (skb_shinfo(skb)->flags & SKBFL_DONT_ORPHAN)\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n \nstatic inline int skb_orphan_frags_rx(struct sk_buff *skb, gfp_t gfp_mask)\n{\n\tif (likely(!skb_zcopy(skb)))\n\t\treturn 0;\n\treturn skb_copy_ubufs(skb, gfp_mask);\n}\n\n \nstatic inline void __skb_queue_purge_reason(struct sk_buff_head *list,\n\t\t\t\t\t    enum skb_drop_reason reason)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(list)) != NULL)\n\t\tkfree_skb_reason(skb, reason);\n}\n\nstatic inline void __skb_queue_purge(struct sk_buff_head *list)\n{\n\t__skb_queue_purge_reason(list, SKB_DROP_REASON_QUEUE_PURGE);\n}\n\nvoid skb_queue_purge_reason(struct sk_buff_head *list,\n\t\t\t    enum skb_drop_reason reason);\n\nstatic inline void skb_queue_purge(struct sk_buff_head *list)\n{\n\tskb_queue_purge_reason(list, SKB_DROP_REASON_QUEUE_PURGE);\n}\n\nunsigned int skb_rbtree_purge(struct rb_root *root);\nvoid skb_errqueue_purge(struct sk_buff_head *list);\n\nvoid *__netdev_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\n \nstatic inline void *netdev_alloc_frag(unsigned int fragsz)\n{\n\treturn __netdev_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *netdev_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t    unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __netdev_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,\n\t\t\t\t   gfp_t gfp_mask);\n\n \nstatic inline struct sk_buff *netdev_alloc_skb(struct net_device *dev,\n\t\t\t\t\t       unsigned int length)\n{\n\treturn __netdev_alloc_skb(dev, length, GFP_ATOMIC);\n}\n\n \nstatic inline struct sk_buff *__dev_alloc_skb(unsigned int length,\n\t\t\t\t\t      gfp_t gfp_mask)\n{\n\treturn __netdev_alloc_skb(NULL, length, gfp_mask);\n}\n\n \nstatic inline struct sk_buff *dev_alloc_skb(unsigned int length)\n{\n\treturn netdev_alloc_skb(NULL, length);\n}\n\n\nstatic inline struct sk_buff *__netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length, gfp_t gfp)\n{\n\tstruct sk_buff *skb = __netdev_alloc_skb(dev, length + NET_IP_ALIGN, gfp);\n\n\tif (NET_IP_ALIGN && skb)\n\t\tskb_reserve(skb, NET_IP_ALIGN);\n\treturn skb;\n}\n\nstatic inline struct sk_buff *netdev_alloc_skb_ip_align(struct net_device *dev,\n\t\tunsigned int length)\n{\n\treturn __netdev_alloc_skb_ip_align(dev, length, GFP_ATOMIC);\n}\n\nstatic inline void skb_free_frag(void *addr)\n{\n\tpage_frag_free(addr);\n}\n\nvoid *__napi_alloc_frag_align(unsigned int fragsz, unsigned int align_mask);\n\nstatic inline void *napi_alloc_frag(unsigned int fragsz)\n{\n\treturn __napi_alloc_frag_align(fragsz, ~0u);\n}\n\nstatic inline void *napi_alloc_frag_align(unsigned int fragsz,\n\t\t\t\t\t  unsigned int align)\n{\n\tWARN_ON_ONCE(!is_power_of_2(align));\n\treturn __napi_alloc_frag_align(fragsz, -align);\n}\n\nstruct sk_buff *__napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t unsigned int length, gfp_t gfp_mask);\nstatic inline struct sk_buff *napi_alloc_skb(struct napi_struct *napi,\n\t\t\t\t\t     unsigned int length)\n{\n\treturn __napi_alloc_skb(napi, length, GFP_ATOMIC);\n}\nvoid napi_consume_skb(struct sk_buff *skb, int budget);\n\nvoid napi_skb_free_stolen_head(struct sk_buff *skb);\nvoid __napi_kfree_skb(struct sk_buff *skb, enum skb_drop_reason reason);\n\n \nstatic inline struct page *__dev_alloc_pages(gfp_t gfp_mask,\n\t\t\t\t\t     unsigned int order)\n{\n\t \n\tgfp_mask |= __GFP_COMP | __GFP_MEMALLOC;\n\n\treturn alloc_pages_node(NUMA_NO_NODE, gfp_mask, order);\n}\n\nstatic inline struct page *dev_alloc_pages(unsigned int order)\n{\n\treturn __dev_alloc_pages(GFP_ATOMIC | __GFP_NOWARN, order);\n}\n\n \nstatic inline struct page *__dev_alloc_page(gfp_t gfp_mask)\n{\n\treturn __dev_alloc_pages(gfp_mask, 0);\n}\n\nstatic inline struct page *dev_alloc_page(void)\n{\n\treturn dev_alloc_pages(0);\n}\n\n \nstatic inline bool dev_page_is_reusable(const struct page *page)\n{\n\treturn likely(page_to_nid(page) == numa_mem_id() &&\n\t\t      !page_is_pfmemalloc(page));\n}\n\n \nstatic inline void skb_propagate_pfmemalloc(const struct page *page,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\tif (page_is_pfmemalloc(page))\n\t\tskb->pfmemalloc = true;\n}\n\n \nstatic inline unsigned int skb_frag_off(const skb_frag_t *frag)\n{\n\treturn frag->bv_offset;\n}\n\n \nstatic inline void skb_frag_off_add(skb_frag_t *frag, int delta)\n{\n\tfrag->bv_offset += delta;\n}\n\n \nstatic inline void skb_frag_off_set(skb_frag_t *frag, unsigned int offset)\n{\n\tfrag->bv_offset = offset;\n}\n\n \nstatic inline void skb_frag_off_copy(skb_frag_t *fragto,\n\t\t\t\t     const skb_frag_t *fragfrom)\n{\n\tfragto->bv_offset = fragfrom->bv_offset;\n}\n\n \nstatic inline struct page *skb_frag_page(const skb_frag_t *frag)\n{\n\treturn frag->bv_page;\n}\n\n \nstatic inline void __skb_frag_ref(skb_frag_t *frag)\n{\n\tget_page(skb_frag_page(frag));\n}\n\n \nstatic inline void skb_frag_ref(struct sk_buff *skb, int f)\n{\n\t__skb_frag_ref(&skb_shinfo(skb)->frags[f]);\n}\n\nbool napi_pp_put_page(struct page *page, bool napi_safe);\n\nstatic inline void\nnapi_frag_unref(skb_frag_t *frag, bool recycle, bool napi_safe)\n{\n\tstruct page *page = skb_frag_page(frag);\n\n#ifdef CONFIG_PAGE_POOL\n\tif (recycle && napi_pp_put_page(page, napi_safe))\n\t\treturn;\n#endif\n\tput_page(page);\n}\n\n \nstatic inline void __skb_frag_unref(skb_frag_t *frag, bool recycle)\n{\n\tnapi_frag_unref(frag, recycle, false);\n}\n\n \nstatic inline void skb_frag_unref(struct sk_buff *skb, int f)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (!skb_zcopy_managed(skb))\n\t\t__skb_frag_unref(&shinfo->frags[f], skb->pp_recycle);\n}\n\n \nstatic inline void *skb_frag_address(const skb_frag_t *frag)\n{\n\treturn page_address(skb_frag_page(frag)) + skb_frag_off(frag);\n}\n\n \nstatic inline void *skb_frag_address_safe(const skb_frag_t *frag)\n{\n\tvoid *ptr = page_address(skb_frag_page(frag));\n\tif (unlikely(!ptr))\n\t\treturn NULL;\n\n\treturn ptr + skb_frag_off(frag);\n}\n\n \nstatic inline void skb_frag_page_copy(skb_frag_t *fragto,\n\t\t\t\t      const skb_frag_t *fragfrom)\n{\n\tfragto->bv_page = fragfrom->bv_page;\n}\n\nbool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t prio);\n\n \nstatic inline dma_addr_t skb_frag_dma_map(struct device *dev,\n\t\t\t\t\t  const skb_frag_t *frag,\n\t\t\t\t\t  size_t offset, size_t size,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\treturn dma_map_page(dev, skb_frag_page(frag),\n\t\t\t    skb_frag_off(frag) + offset, size, dir);\n}\n\nstatic inline struct sk_buff *pskb_copy(struct sk_buff *skb,\n\t\t\t\t\tgfp_t gfp_mask)\n{\n\treturn __pskb_copy(skb, skb_headroom(skb), gfp_mask);\n}\n\n\nstatic inline struct sk_buff *pskb_copy_for_clone(struct sk_buff *skb,\n\t\t\t\t\t\t  gfp_t gfp_mask)\n{\n\treturn __pskb_copy_fclone(skb, skb_headroom(skb), gfp_mask, true);\n}\n\n\n \nstatic inline int skb_clone_writable(const struct sk_buff *skb, unsigned int len)\n{\n\treturn !skb_header_cloned(skb) &&\n\t       skb_headroom(skb) + len <= skb->hdr_len;\n}\n\nstatic inline int skb_try_make_writable(struct sk_buff *skb,\n\t\t\t\t\tunsigned int write_len)\n{\n\treturn skb_cloned(skb) && !skb_clone_writable(skb, write_len) &&\n\t       pskb_expand_head(skb, 0, 0, GFP_ATOMIC);\n}\n\nstatic inline int __skb_cow(struct sk_buff *skb, unsigned int headroom,\n\t\t\t    int cloned)\n{\n\tint delta = 0;\n\n\tif (headroom > skb_headroom(skb))\n\t\tdelta = headroom - skb_headroom(skb);\n\n\tif (delta || cloned)\n\t\treturn pskb_expand_head(skb, ALIGN(delta, NET_SKB_PAD), 0,\n\t\t\t\t\tGFP_ATOMIC);\n\treturn 0;\n}\n\n \nstatic inline int skb_cow(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_cloned(skb));\n}\n\n \nstatic inline int skb_cow_head(struct sk_buff *skb, unsigned int headroom)\n{\n\treturn __skb_cow(skb, headroom, skb_header_cloned(skb));\n}\n\n \nstatic inline int skb_padto(struct sk_buff *skb, unsigned int len)\n{\n\tunsigned int size = skb->len;\n\tif (likely(size >= len))\n\t\treturn 0;\n\treturn skb_pad(skb, len - size);\n}\n\n \nstatic inline int __must_check __skb_put_padto(struct sk_buff *skb,\n\t\t\t\t\t       unsigned int len,\n\t\t\t\t\t       bool free_on_error)\n{\n\tunsigned int size = skb->len;\n\n\tif (unlikely(size < len)) {\n\t\tlen -= size;\n\t\tif (__skb_pad(skb, len, free_on_error))\n\t\t\treturn -ENOMEM;\n\t\t__skb_put(skb, len);\n\t}\n\treturn 0;\n}\n\n \nstatic inline int __must_check skb_put_padto(struct sk_buff *skb, unsigned int len)\n{\n\treturn __skb_put_padto(skb, len, true);\n}\n\nstatic inline int skb_add_data(struct sk_buff *skb,\n\t\t\t       struct iov_iter *from, int copy)\n{\n\tconst int off = skb->len;\n\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (csum_and_copy_from_iter_full(skb_put(skb, copy), copy,\n\t\t\t\t\t         &csum, from)) {\n\t\t\tskb->csum = csum_block_add(skb->csum, csum, off);\n\t\t\treturn 0;\n\t\t}\n\t} else if (copy_from_iter_full(skb_put(skb, copy), copy, from))\n\t\treturn 0;\n\n\t__skb_trim(skb, off);\n\treturn -EFAULT;\n}\n\nstatic inline bool skb_can_coalesce(struct sk_buff *skb, int i,\n\t\t\t\t    const struct page *page, int off)\n{\n\tif (skb_zcopy(skb))\n\t\treturn false;\n\tif (i) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i - 1];\n\n\t\treturn page == skb_frag_page(frag) &&\n\t\t       off == skb_frag_off(frag) + skb_frag_size(frag);\n\t}\n\treturn false;\n}\n\nstatic inline int __skb_linearize(struct sk_buff *skb)\n{\n\treturn __pskb_pull_tail(skb, skb->data_len) ? 0 : -ENOMEM;\n}\n\n \nstatic inline int skb_linearize(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) ? __skb_linearize(skb) : 0;\n}\n\n \nstatic inline bool skb_has_shared_frag(const struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       skb_shinfo(skb)->flags & SKBFL_SHARED_FRAG;\n}\n\n \nstatic inline int skb_linearize_cow(struct sk_buff *skb)\n{\n\treturn skb_is_nonlinear(skb) || skb_cloned(skb) ?\n\t       __skb_linearize(skb) : 0;\n}\n\nstatic __always_inline void\n__skb_postpull_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_sub(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n \nstatic inline void skb_postpull_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = wsum_negate(csum_partial(start, len,\n\t\t\t\t\t\t     wsum_negate(skb->csum)));\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) < 0)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\nstatic __always_inline void\n__skb_postpush_rcsum(struct sk_buff *skb, const void *start, unsigned int len,\n\t\t     unsigned int off)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->csum = csum_block_add(skb->csum,\n\t\t\t\t\t   csum_partial(start, len, 0), off);\n}\n\n \nstatic inline void skb_postpush_rcsum(struct sk_buff *skb,\n\t\t\t\t      const void *start, unsigned int len)\n{\n\t__skb_postpush_rcsum(skb, start, len, 0);\n}\n\nvoid *skb_pull_rcsum(struct sk_buff *skb, unsigned int len);\n\n \nstatic inline void *skb_push_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tskb_push(skb, len);\n\tskb_postpush_rcsum(skb, skb->data, len);\n\treturn skb->data;\n}\n\nint pskb_trim_rcsum_slow(struct sk_buff *skb, unsigned int len);\n \n\nstatic inline int pskb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (likely(len >= skb->len))\n\t\treturn 0;\n\treturn pskb_trim_rcsum_slow(skb, len);\n}\n\nstatic inline int __skb_trim_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t__skb_trim(skb, len);\n\treturn 0;\n}\n\nstatic inline int __skb_grow_rcsum(struct sk_buff *skb, unsigned int len)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\treturn __skb_grow(skb, len);\n}\n\n#define rb_to_skb(rb) rb_entry_safe(rb, struct sk_buff, rbnode)\n#define skb_rb_first(root) rb_to_skb(rb_first(root))\n#define skb_rb_last(root)  rb_to_skb(rb_last(root))\n#define skb_rb_next(skb)   rb_to_skb(rb_next(&(skb)->rbnode))\n#define skb_rb_prev(skb)   rb_to_skb(rb_prev(&(skb)->rbnode))\n\n#define skb_queue_walk(queue, skb) \\\n\t\tfor (skb = (queue)->next;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_queue_walk_safe(queue, skb, tmp)\t\t\t\t\t\\\n\t\tfor (skb = (queue)->next, tmp = skb->next;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_walk_from(queue, skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != (struct sk_buff *)(queue);\t\t\t\\\n\t\t     skb = skb->next)\n\n#define skb_rbtree_walk(skb, root)\t\t\t\t\t\t\\\n\t\tfor (skb = skb_rb_first(root); skb != NULL;\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from(skb)\t\t\t\t\t\t\\\n\t\tfor (; skb != NULL;\t\t\t\t\t\t\\\n\t\t     skb = skb_rb_next(skb))\n\n#define skb_rbtree_walk_from_safe(skb, tmp)\t\t\t\t\t\\\n\t\tfor (; tmp = skb ? skb_rb_next(skb) : NULL, (skb != NULL);\t\\\n\t\t     skb = tmp)\n\n#define skb_queue_walk_from_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (tmp = skb->next;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->next)\n\n#define skb_queue_reverse_walk(queue, skb) \\\n\t\tfor (skb = (queue)->prev;\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = skb->prev)\n\n#define skb_queue_reverse_walk_safe(queue, skb, tmp)\t\t\t\t\\\n\t\tfor (skb = (queue)->prev, tmp = skb->prev;\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\n#define skb_queue_reverse_walk_from_safe(queue, skb, tmp)\t\t\t\\\n\t\tfor (tmp = skb->prev;\t\t\t\t\t\t\\\n\t\t     skb != (struct sk_buff *)(queue);\t\t\t\t\\\n\t\t     skb = tmp, tmp = skb->prev)\n\nstatic inline bool skb_has_frag_list(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->frag_list != NULL;\n}\n\nstatic inline void skb_frag_list_init(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->frag_list = NULL;\n}\n\n#define skb_walk_frags(skb, iter)\t\\\n\tfor (iter = skb_shinfo(skb)->frag_list; iter; iter = iter->next)\n\n\nint __skb_wait_for_more_packets(struct sock *sk, struct sk_buff_head *queue,\n\t\t\t\tint *err, long *timeo_p,\n\t\t\t\tconst struct sk_buff *skb);\nstruct sk_buff *__skb_try_recv_from_queue(struct sock *sk,\n\t\t\t\t\t  struct sk_buff_head *queue,\n\t\t\t\t\t  unsigned int flags,\n\t\t\t\t\t  int *off, int *err,\n\t\t\t\t\t  struct sk_buff **last);\nstruct sk_buff *__skb_try_recv_datagram(struct sock *sk,\n\t\t\t\t\tstruct sk_buff_head *queue,\n\t\t\t\t\tunsigned int flags, int *off, int *err,\n\t\t\t\t\tstruct sk_buff **last);\nstruct sk_buff *__skb_recv_datagram(struct sock *sk,\n\t\t\t\t    struct sk_buff_head *sk_queue,\n\t\t\t\t    unsigned int flags, int *off, int *err);\nstruct sk_buff *skb_recv_datagram(struct sock *sk, unsigned int flags, int *err);\n__poll_t datagram_poll(struct file *file, struct socket *sock,\n\t\t\t   struct poll_table_struct *wait);\nint skb_copy_datagram_iter(const struct sk_buff *from, int offset,\n\t\t\t   struct iov_iter *to, int size);\nstatic inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,\n\t\t\t\t\tstruct msghdr *msg, int size)\n{\n\treturn skb_copy_datagram_iter(from, offset, &msg->msg_iter, size);\n}\nint skb_copy_and_csum_datagram_msg(struct sk_buff *skb, int hlen,\n\t\t\t\t   struct msghdr *msg);\nint skb_copy_and_hash_datagram_iter(const struct sk_buff *skb, int offset,\n\t\t\t   struct iov_iter *to, int len,\n\t\t\t   struct ahash_request *hash);\nint skb_copy_datagram_from_iter(struct sk_buff *skb, int offset,\n\t\t\t\t struct iov_iter *from, int len);\nint zerocopy_sg_from_iter(struct sk_buff *skb, struct iov_iter *frm);\nvoid skb_free_datagram(struct sock *sk, struct sk_buff *skb);\nvoid __skb_free_datagram_locked(struct sock *sk, struct sk_buff *skb, int len);\nstatic inline void skb_free_datagram_locked(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\t__skb_free_datagram_locked(sk, skb, 0);\n}\nint skb_kill_datagram(struct sock *sk, struct sk_buff *skb, unsigned int flags);\nint skb_copy_bits(const struct sk_buff *skb, int offset, void *to, int len);\nint skb_store_bits(struct sk_buff *skb, int offset, const void *from, int len);\n__wsum skb_copy_and_csum_bits(const struct sk_buff *skb, int offset, u8 *to,\n\t\t\t      int len);\nint skb_splice_bits(struct sk_buff *skb, struct sock *sk, unsigned int offset,\n\t\t    struct pipe_inode_info *pipe, unsigned int len,\n\t\t    unsigned int flags);\nint skb_send_sock_locked(struct sock *sk, struct sk_buff *skb, int offset,\n\t\t\t int len);\nint skb_send_sock(struct sock *sk, struct sk_buff *skb, int offset, int len);\nvoid skb_copy_and_csum_dev(const struct sk_buff *skb, u8 *to);\nunsigned int skb_zerocopy_headlen(const struct sk_buff *from);\nint skb_zerocopy(struct sk_buff *to, struct sk_buff *from,\n\t\t int len, int hlen);\nvoid skb_split(struct sk_buff *skb, struct sk_buff *skb1, const u32 len);\nint skb_shift(struct sk_buff *tgt, struct sk_buff *skb, int shiftlen);\nvoid skb_scrub_packet(struct sk_buff *skb, bool xnet);\nstruct sk_buff *skb_segment(struct sk_buff *skb, netdev_features_t features);\nstruct sk_buff *skb_segment_list(struct sk_buff *skb, netdev_features_t features,\n\t\t\t\t unsigned int offset);\nstruct sk_buff *skb_vlan_untag(struct sk_buff *skb);\nint skb_ensure_writable(struct sk_buff *skb, unsigned int write_len);\nint __skb_vlan_pop(struct sk_buff *skb, u16 *vlan_tci);\nint skb_vlan_pop(struct sk_buff *skb);\nint skb_vlan_push(struct sk_buff *skb, __be16 vlan_proto, u16 vlan_tci);\nint skb_eth_pop(struct sk_buff *skb);\nint skb_eth_push(struct sk_buff *skb, const unsigned char *dst,\n\t\t const unsigned char *src);\nint skb_mpls_push(struct sk_buff *skb, __be32 mpls_lse, __be16 mpls_proto,\n\t\t  int mac_len, bool ethernet);\nint skb_mpls_pop(struct sk_buff *skb, __be16 next_proto, int mac_len,\n\t\t bool ethernet);\nint skb_mpls_update_lse(struct sk_buff *skb, __be32 mpls_lse);\nint skb_mpls_dec_ttl(struct sk_buff *skb);\nstruct sk_buff *pskb_extract(struct sk_buff *skb, int off, int to_copy,\n\t\t\t     gfp_t gfp);\n\nstatic inline int memcpy_from_msg(void *data, struct msghdr *msg, int len)\n{\n\treturn copy_from_iter_full(data, len, &msg->msg_iter) ? 0 : -EFAULT;\n}\n\nstatic inline int memcpy_to_msg(struct msghdr *msg, void *data, int len)\n{\n\treturn copy_to_iter(data, len, &msg->msg_iter) == len ? 0 : -EFAULT;\n}\n\nstruct skb_checksum_ops {\n\t__wsum (*update)(const void *mem, int len, __wsum wsum);\n\t__wsum (*combine)(__wsum csum, __wsum csum2, int offset, int len);\n};\n\nextern const struct skb_checksum_ops *crc32c_csum_stub __read_mostly;\n\n__wsum __skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t      __wsum csum, const struct skb_checksum_ops *ops);\n__wsum skb_checksum(const struct sk_buff *skb, int offset, int len,\n\t\t    __wsum csum);\n\nstatic inline void * __must_check\n__skb_header_pointer(const struct sk_buff *skb, int offset, int len,\n\t\t     const void *data, int hlen, void *buffer)\n{\n\tif (likely(hlen - offset >= len))\n\t\treturn (void *)data + offset;\n\n\tif (!skb || unlikely(skb_copy_bits(skb, offset, buffer, len) < 0))\n\t\treturn NULL;\n\n\treturn buffer;\n}\n\nstatic inline void * __must_check\nskb_header_pointer(const struct sk_buff *skb, int offset, int len, void *buffer)\n{\n\treturn __skb_header_pointer(skb, offset, len, skb->data,\n\t\t\t\t    skb_headlen(skb), buffer);\n}\n\nstatic inline void * __must_check\nskb_pointer_if_linear(const struct sk_buff *skb, int offset, int len)\n{\n\tif (likely(skb_headlen(skb) - offset >= len))\n\t\treturn skb->data + offset;\n\treturn NULL;\n}\n\n \nstatic inline bool skb_needs_linearize(struct sk_buff *skb,\n\t\t\t\t       netdev_features_t features)\n{\n\treturn skb_is_nonlinear(skb) &&\n\t       ((skb_has_frag_list(skb) && !(features & NETIF_F_FRAGLIST)) ||\n\t\t(skb_shinfo(skb)->nr_frags && !(features & NETIF_F_SG)));\n}\n\nstatic inline void skb_copy_from_linear_data(const struct sk_buff *skb,\n\t\t\t\t\t     void *to,\n\t\t\t\t\t     const unsigned int len)\n{\n\tmemcpy(to, skb->data, len);\n}\n\nstatic inline void skb_copy_from_linear_data_offset(const struct sk_buff *skb,\n\t\t\t\t\t\t    const int offset, void *to,\n\t\t\t\t\t\t    const unsigned int len)\n{\n\tmemcpy(to, skb->data + offset, len);\n}\n\nstatic inline void skb_copy_to_linear_data(struct sk_buff *skb,\n\t\t\t\t\t   const void *from,\n\t\t\t\t\t   const unsigned int len)\n{\n\tmemcpy(skb->data, from, len);\n}\n\nstatic inline void skb_copy_to_linear_data_offset(struct sk_buff *skb,\n\t\t\t\t\t\t  const int offset,\n\t\t\t\t\t\t  const void *from,\n\t\t\t\t\t\t  const unsigned int len)\n{\n\tmemcpy(skb->data + offset, from, len);\n}\n\nvoid skb_init(void);\n\nstatic inline ktime_t skb_get_ktime(const struct sk_buff *skb)\n{\n\treturn skb->tstamp;\n}\n\n \nstatic inline void skb_get_timestamp(const struct sk_buff *skb,\n\t\t\t\t     struct __kernel_old_timeval *stamp)\n{\n\t*stamp = ns_to_kernel_old_timeval(skb->tstamp);\n}\n\nstatic inline void skb_get_new_timestamp(const struct sk_buff *skb,\n\t\t\t\t\t struct __kernel_sock_timeval *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_usec = ts.tv_nsec / 1000;\n}\n\nstatic inline void skb_get_timestampns(const struct sk_buff *skb,\n\t\t\t\t       struct __kernel_old_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void skb_get_new_timestampns(const struct sk_buff *skb,\n\t\t\t\t\t   struct __kernel_timespec *stamp)\n{\n\tstruct timespec64 ts = ktime_to_timespec64(skb->tstamp);\n\n\tstamp->tv_sec = ts.tv_sec;\n\tstamp->tv_nsec = ts.tv_nsec;\n}\n\nstatic inline void __net_timestamp(struct sk_buff *skb)\n{\n\tskb->tstamp = ktime_get_real();\n\tskb->mono_delivery_time = 0;\n}\n\nstatic inline ktime_t net_timedelta(ktime_t t)\n{\n\treturn ktime_sub(ktime_get_real(), t);\n}\n\nstatic inline void skb_set_delivery_time(struct sk_buff *skb, ktime_t kt,\n\t\t\t\t\t bool mono)\n{\n\tskb->tstamp = kt;\n\tskb->mono_delivery_time = kt && mono;\n}\n\nDECLARE_STATIC_KEY_FALSE(netstamp_needed_key);\n\n \nstatic inline void skb_clear_delivery_time(struct sk_buff *skb)\n{\n\tif (skb->mono_delivery_time) {\n\t\tskb->mono_delivery_time = 0;\n\t\tif (static_branch_unlikely(&netstamp_needed_key))\n\t\t\tskb->tstamp = ktime_get_real();\n\t\telse\n\t\t\tskb->tstamp = 0;\n\t}\n}\n\nstatic inline void skb_clear_tstamp(struct sk_buff *skb)\n{\n\tif (skb->mono_delivery_time)\n\t\treturn;\n\n\tskb->tstamp = 0;\n}\n\nstatic inline ktime_t skb_tstamp(const struct sk_buff *skb)\n{\n\tif (skb->mono_delivery_time)\n\t\treturn 0;\n\n\treturn skb->tstamp;\n}\n\nstatic inline ktime_t skb_tstamp_cond(const struct sk_buff *skb, bool cond)\n{\n\tif (!skb->mono_delivery_time && skb->tstamp)\n\t\treturn skb->tstamp;\n\n\tif (static_branch_unlikely(&netstamp_needed_key) || cond)\n\t\treturn ktime_get_real();\n\n\treturn 0;\n}\n\nstatic inline u8 skb_metadata_len(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->meta_len;\n}\n\nstatic inline void *skb_metadata_end(const struct sk_buff *skb)\n{\n\treturn skb_mac_header(skb);\n}\n\nstatic inline bool __skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\t  const struct sk_buff *skb_b,\n\t\t\t\t\t  u8 meta_len)\n{\n\tconst void *a = skb_metadata_end(skb_a);\n\tconst void *b = skb_metadata_end(skb_b);\n\t \n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64\n\tu64 diffs = 0;\n\n\tswitch (meta_len) {\n#define __it(x, op) (x -= sizeof(u##op))\n#define __it_diff(a, b, op) (*(u##op *)__it(a, op)) ^ (*(u##op *)__it(b, op))\n\tcase 32: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 24: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 16: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  8: diffs |= __it_diff(a, b, 64);\n\t\tbreak;\n\tcase 28: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 20: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase 12: diffs |= __it_diff(a, b, 64);\n\t\tfallthrough;\n\tcase  4: diffs |= __it_diff(a, b, 32);\n\t\tbreak;\n\t}\n\treturn diffs;\n#else\n\treturn memcmp(a - meta_len, b - meta_len, meta_len);\n#endif\n}\n\nstatic inline bool skb_metadata_differs(const struct sk_buff *skb_a,\n\t\t\t\t\tconst struct sk_buff *skb_b)\n{\n\tu8 len_a = skb_metadata_len(skb_a);\n\tu8 len_b = skb_metadata_len(skb_b);\n\n\tif (!(len_a | len_b))\n\t\treturn false;\n\n\treturn len_a != len_b ?\n\t       true : __skb_metadata_differs(skb_a, skb_b, len_a);\n}\n\nstatic inline void skb_metadata_set(struct sk_buff *skb, u8 meta_len)\n{\n\tskb_shinfo(skb)->meta_len = meta_len;\n}\n\nstatic inline void skb_metadata_clear(struct sk_buff *skb)\n{\n\tskb_metadata_set(skb, 0);\n}\n\nstruct sk_buff *skb_clone_sk(struct sk_buff *skb);\n\n#ifdef CONFIG_NETWORK_PHY_TIMESTAMPING\n\nvoid skb_clone_tx_timestamp(struct sk_buff *skb);\nbool skb_defer_rx_timestamp(struct sk_buff *skb);\n\n#else  \n\nstatic inline void skb_clone_tx_timestamp(struct sk_buff *skb)\n{\n}\n\nstatic inline bool skb_defer_rx_timestamp(struct sk_buff *skb)\n{\n\treturn false;\n}\n\n#endif  \n\n \nvoid skb_complete_tx_timestamp(struct sk_buff *skb,\n\t\t\t       struct skb_shared_hwtstamps *hwtstamps);\n\nvoid __skb_tstamp_tx(struct sk_buff *orig_skb, const struct sk_buff *ack_skb,\n\t\t     struct skb_shared_hwtstamps *hwtstamps,\n\t\t     struct sock *sk, int tstype);\n\n \nvoid skb_tstamp_tx(struct sk_buff *orig_skb,\n\t\t   struct skb_shared_hwtstamps *hwtstamps);\n\n \nstatic inline void skb_tx_timestamp(struct sk_buff *skb)\n{\n\tskb_clone_tx_timestamp(skb);\n\tif (skb_shinfo(skb)->tx_flags & SKBTX_SW_TSTAMP)\n\t\tskb_tstamp_tx(skb, NULL);\n}\n\n \nvoid skb_complete_wifi_ack(struct sk_buff *skb, bool acked);\n\n__sum16 __skb_checksum_complete_head(struct sk_buff *skb, int len);\n__sum16 __skb_checksum_complete(struct sk_buff *skb);\n\nstatic inline int skb_csum_unnecessary(const struct sk_buff *skb)\n{\n\treturn ((skb->ip_summed == CHECKSUM_UNNECESSARY) ||\n\t\tskb->csum_valid ||\n\t\t(skb->ip_summed == CHECKSUM_PARTIAL &&\n\t\t skb_checksum_start_offset(skb) >= 0));\n}\n\n \nstatic inline __sum16 skb_checksum_complete(struct sk_buff *skb)\n{\n\treturn skb_csum_unnecessary(skb) ?\n\t       0 : __skb_checksum_complete(skb);\n}\n\nstatic inline void __skb_decr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level == 0)\n\t\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\telse\n\t\t\tskb->csum_level--;\n\t}\n}\n\nstatic inline void __skb_incr_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tif (skb->csum_level < SKB_MAX_CSUM_LEVEL)\n\t\t\tskb->csum_level++;\n\t} else if (skb->ip_summed == CHECKSUM_NONE) {\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\tskb->csum_level = 0;\n\t}\n}\n\nstatic inline void __skb_reset_checksum_unnecessary(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_UNNECESSARY) {\n\t\tskb->ip_summed = CHECKSUM_NONE;\n\t\tskb->csum_level = 0;\n\t}\n}\n\n \nstatic inline bool __skb_checksum_validate_needed(struct sk_buff *skb,\n\t\t\t\t\t\t  bool zero_okay,\n\t\t\t\t\t\t  __sum16 check)\n{\n\tif (skb_csum_unnecessary(skb) || (zero_okay && !check)) {\n\t\tskb->csum_valid = 1;\n\t\t__skb_decr_checksum_unnecessary(skb);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \n#define CHECKSUM_BREAK 76\n\n \nstatic inline void skb_checksum_complete_unset(struct sk_buff *skb)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n \nstatic inline __sum16 __skb_checksum_validate_complete(struct sk_buff *skb,\n\t\t\t\t\t\t       bool complete,\n\t\t\t\t\t\t       __wsum psum)\n{\n\tif (skb->ip_summed == CHECKSUM_COMPLETE) {\n\t\tif (!csum_fold(csum_add(psum, skb->csum))) {\n\t\t\tskb->csum_valid = 1;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tskb->csum = psum;\n\n\tif (complete || skb->len <= CHECKSUM_BREAK) {\n\t\t__sum16 csum;\n\n\t\tcsum = __skb_checksum_complete(skb);\n\t\tskb->csum_valid = !csum;\n\t\treturn csum;\n\t}\n\n\treturn 0;\n}\n\nstatic inline __wsum null_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn 0;\n}\n\n \n#define __skb_checksum_validate(skb, proto, complete,\t\t\t\\\n\t\t\t\tzero_okay, check, compute_pseudo)\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__sum16 __ret = 0;\t\t\t\t\t\t\\\n\tskb->csum_valid = 0;\t\t\t\t\t\t\\\n\tif (__skb_checksum_validate_needed(skb, zero_okay, check))\t\\\n\t\t__ret = __skb_checksum_validate_complete(skb,\t\t\\\n\t\t\t\tcomplete, compute_pseudo(skb, proto));\t\\\n\t__ret;\t\t\t\t\t\t\t\t\\\n})\n\n#define skb_checksum_init(skb, proto, compute_pseudo)\t\t\t\\\n\t__skb_checksum_validate(skb, proto, false, false, 0, compute_pseudo)\n\n#define skb_checksum_init_zero_check(skb, proto, check, compute_pseudo)\t\\\n\t__skb_checksum_validate(skb, proto, false, true, check, compute_pseudo)\n\n#define skb_checksum_validate(skb, proto, compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, false, 0, compute_pseudo)\n\n#define skb_checksum_validate_zero_check(skb, proto, check,\t\t\\\n\t\t\t\t\t compute_pseudo)\t\t\\\n\t__skb_checksum_validate(skb, proto, true, true, check, compute_pseudo)\n\n#define skb_checksum_simple_validate(skb)\t\t\t\t\\\n\t__skb_checksum_validate(skb, 0, true, false, 0, null_compute_pseudo)\n\nstatic inline bool __skb_checksum_convert_check(struct sk_buff *skb)\n{\n\treturn (skb->ip_summed == CHECKSUM_NONE && skb->csum_valid);\n}\n\nstatic inline void __skb_checksum_convert(struct sk_buff *skb, __wsum pseudo)\n{\n\tskb->csum = ~pseudo;\n\tskb->ip_summed = CHECKSUM_COMPLETE;\n}\n\n#define skb_checksum_try_convert(skb, proto, compute_pseudo)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (__skb_checksum_convert_check(skb))\t\t\t\t\\\n\t\t__skb_checksum_convert(skb, compute_pseudo(skb, proto)); \\\n} while (0)\n\nstatic inline void skb_remcsum_adjust_partial(struct sk_buff *skb, void *ptr,\n\t\t\t\t\t      u16 start, u16 offset)\n{\n\tskb->ip_summed = CHECKSUM_PARTIAL;\n\tskb->csum_start = ((unsigned char *)ptr + start) - skb->head;\n\tskb->csum_offset = offset - start;\n}\n\n \nstatic inline void skb_remcsum_process(struct sk_buff *skb, void *ptr,\n\t\t\t\t       int start, int offset, bool nopartial)\n{\n\t__wsum delta;\n\n\tif (!nopartial) {\n\t\tskb_remcsum_adjust_partial(skb, ptr, start, offset);\n\t\treturn;\n\t}\n\n\tif (unlikely(skb->ip_summed != CHECKSUM_COMPLETE)) {\n\t\t__skb_checksum_complete(skb);\n\t\tskb_postpull_rcsum(skb, skb->data, ptr - (void *)skb->data);\n\t}\n\n\tdelta = remcsum_adjust(ptr, skb->csum, start, offset);\n\n\t \n\tskb->csum = csum_add(skb->csum, delta);\n}\n\nstatic inline struct nf_conntrack *skb_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn (void *)(skb->_nfct & NFCT_PTRMASK);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline unsigned long skb_get_nfct(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\treturn skb->_nfct;\n#else\n\treturn 0UL;\n#endif\n}\n\nstatic inline void skb_set_nfct(struct sk_buff *skb, unsigned long nfct)\n{\n#if IS_ENABLED(CONFIG_NF_CONNTRACK)\n\tskb->slow_gro |= !!nfct;\n\tskb->_nfct = nfct;\n#endif\n}\n\n#ifdef CONFIG_SKB_EXTENSIONS\nenum skb_ext_id {\n#if IS_ENABLED(CONFIG_BRIDGE_NETFILTER)\n\tSKB_EXT_BRIDGE_NF,\n#endif\n#ifdef CONFIG_XFRM\n\tSKB_EXT_SEC_PATH,\n#endif\n#if IS_ENABLED(CONFIG_NET_TC_SKB_EXT)\n\tTC_SKB_EXT,\n#endif\n#if IS_ENABLED(CONFIG_MPTCP)\n\tSKB_EXT_MPTCP,\n#endif\n#if IS_ENABLED(CONFIG_MCTP_FLOWS)\n\tSKB_EXT_MCTP,\n#endif\n\tSKB_EXT_NUM,  \n};\n\n \nstruct skb_ext {\n\trefcount_t refcnt;\n\tu8 offset[SKB_EXT_NUM];  \n\tu8 chunks;\t\t \n\tchar data[] __aligned(8);\n};\n\nstruct skb_ext *__skb_ext_alloc(gfp_t flags);\nvoid *__skb_ext_set(struct sk_buff *skb, enum skb_ext_id id,\n\t\t    struct skb_ext *ext);\nvoid *skb_ext_add(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_del(struct sk_buff *skb, enum skb_ext_id id);\nvoid __skb_ext_put(struct skb_ext *ext);\n\nstatic inline void skb_ext_put(struct sk_buff *skb)\n{\n\tif (skb->active_extensions)\n\t\t__skb_ext_put(skb->extensions);\n}\n\nstatic inline void __skb_ext_copy(struct sk_buff *dst,\n\t\t\t\t  const struct sk_buff *src)\n{\n\tdst->active_extensions = src->active_extensions;\n\n\tif (src->active_extensions) {\n\t\tstruct skb_ext *ext = src->extensions;\n\n\t\trefcount_inc(&ext->refcnt);\n\t\tdst->extensions = ext;\n\t}\n}\n\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n\tskb_ext_put(dst);\n\t__skb_ext_copy(dst, src);\n}\n\nstatic inline bool __skb_ext_exist(const struct skb_ext *ext, enum skb_ext_id i)\n{\n\treturn !!ext->offset[i];\n}\n\nstatic inline bool skb_ext_exist(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\treturn skb->active_extensions & (1 << id);\n}\n\nstatic inline void skb_ext_del(struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id))\n\t\t__skb_ext_del(skb, id);\n}\n\nstatic inline void *skb_ext_find(const struct sk_buff *skb, enum skb_ext_id id)\n{\n\tif (skb_ext_exist(skb, id)) {\n\t\tstruct skb_ext *ext = skb->extensions;\n\n\t\treturn (void *)ext + (ext->offset[id] << 3);\n\t}\n\n\treturn NULL;\n}\n\nstatic inline void skb_ext_reset(struct sk_buff *skb)\n{\n\tif (unlikely(skb->active_extensions)) {\n\t\t__skb_ext_put(skb->extensions);\n\t\tskb->active_extensions = 0;\n\t}\n}\n\nstatic inline bool skb_has_extensions(struct sk_buff *skb)\n{\n\treturn unlikely(skb->active_extensions);\n}\n#else\nstatic inline void skb_ext_put(struct sk_buff *skb) {}\nstatic inline void skb_ext_reset(struct sk_buff *skb) {}\nstatic inline void skb_ext_del(struct sk_buff *skb, int unused) {}\nstatic inline void __skb_ext_copy(struct sk_buff *d, const struct sk_buff *s) {}\nstatic inline void skb_ext_copy(struct sk_buff *dst, const struct sk_buff *s) {}\nstatic inline bool skb_has_extensions(struct sk_buff *skb) { return false; }\n#endif  \n\nstatic inline void nf_reset_ct(struct sk_buff *skb)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(skb));\n\tskb->_nfct = 0;\n#endif\n}\n\nstatic inline void nf_reset_trace(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)\n\tskb->nf_trace = 0;\n#endif\n}\n\nstatic inline void ipvs_reset(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_VS)\n\tskb->ipvs_property = 0;\n#endif\n}\n\n \nstatic inline void __nf_copy(struct sk_buff *dst, const struct sk_buff *src,\n\t\t\t     bool copy)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tdst->_nfct = src->_nfct;\n\tnf_conntrack_get(skb_nfct(src));\n#endif\n#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)\n\tif (copy)\n\t\tdst->nf_trace = src->nf_trace;\n#endif\n}\n\nstatic inline void nf_copy(struct sk_buff *dst, const struct sk_buff *src)\n{\n#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)\n\tnf_conntrack_put(skb_nfct(dst));\n#endif\n\tdst->slow_gro = src->slow_gro;\n\t__nf_copy(dst, src, true);\n}\n\n#ifdef CONFIG_NETWORK_SECMARK\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->secmark = from->secmark;\n}\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{\n\tskb->secmark = 0;\n}\n#else\nstatic inline void skb_copy_secmark(struct sk_buff *to, const struct sk_buff *from)\n{ }\n\nstatic inline void skb_init_secmark(struct sk_buff *skb)\n{ }\n#endif\n\nstatic inline int secpath_exists(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_exist(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool skb_irq_freeable(const struct sk_buff *skb)\n{\n\treturn !skb->destructor &&\n\t\t!secpath_exists(skb) &&\n\t\t!skb_nfct(skb) &&\n\t\t!skb->_skb_refdst &&\n\t\t!skb_has_frag_list(skb);\n}\n\nstatic inline void skb_set_queue_mapping(struct sk_buff *skb, u16 queue_mapping)\n{\n\tskb->queue_mapping = queue_mapping;\n}\n\nstatic inline u16 skb_get_queue_mapping(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping;\n}\n\nstatic inline void skb_copy_queue_mapping(struct sk_buff *to, const struct sk_buff *from)\n{\n\tto->queue_mapping = from->queue_mapping;\n}\n\nstatic inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)\n{\n\tskb->queue_mapping = rx_queue + 1;\n}\n\nstatic inline u16 skb_get_rx_queue(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping - 1;\n}\n\nstatic inline bool skb_rx_queue_recorded(const struct sk_buff *skb)\n{\n\treturn skb->queue_mapping != 0;\n}\n\nstatic inline void skb_set_dst_pending_confirm(struct sk_buff *skb, u32 val)\n{\n\tskb->dst_pending_confirm = val;\n}\n\nstatic inline bool skb_get_dst_pending_confirm(const struct sk_buff *skb)\n{\n\treturn skb->dst_pending_confirm != 0;\n}\n\nstatic inline struct sec_path *skb_sec_path(const struct sk_buff *skb)\n{\n#ifdef CONFIG_XFRM\n\treturn skb_ext_find(skb, SKB_EXT_SEC_PATH);\n#else\n\treturn NULL;\n#endif\n}\n\nstatic inline bool skb_is_gso(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_size;\n}\n\n \nstatic inline bool skb_is_gso_v6(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6;\n}\n\n \nstatic inline bool skb_is_gso_sctp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & SKB_GSO_SCTP;\n}\n\n \nstatic inline bool skb_is_gso_tcp(const struct sk_buff *skb)\n{\n\treturn skb_shinfo(skb)->gso_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6);\n}\n\nstatic inline void skb_gso_reset(struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->gso_size = 0;\n\tskb_shinfo(skb)->gso_segs = 0;\n\tskb_shinfo(skb)->gso_type = 0;\n}\n\nstatic inline void skb_increase_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 increment)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size += increment;\n}\n\nstatic inline void skb_decrease_gso_size(struct skb_shared_info *shinfo,\n\t\t\t\t\t u16 decrement)\n{\n\tif (WARN_ON_ONCE(shinfo->gso_size == GSO_BY_FRAGS))\n\t\treturn;\n\tshinfo->gso_size -= decrement;\n}\n\nvoid __skb_warn_lro_forwarding(const struct sk_buff *skb);\n\nstatic inline bool skb_warn_if_lro(const struct sk_buff *skb)\n{\n\t \n\tconst struct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (skb_is_nonlinear(skb) && shinfo->gso_size != 0 &&\n\t    unlikely(shinfo->gso_type == 0)) {\n\t\t__skb_warn_lro_forwarding(skb);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void skb_forward_csum(struct sk_buff *skb)\n{\n\t \n\tif (skb->ip_summed == CHECKSUM_COMPLETE)\n\t\tskb->ip_summed = CHECKSUM_NONE;\n}\n\n \nstatic inline void skb_checksum_none_assert(const struct sk_buff *skb)\n{\n\tDEBUG_NET_WARN_ON_ONCE(skb->ip_summed != CHECKSUM_NONE);\n}\n\nbool skb_partial_csum_set(struct sk_buff *skb, u16 start, u16 off);\n\nint skb_checksum_setup(struct sk_buff *skb, bool recalculate);\nstruct sk_buff *skb_checksum_trimmed(struct sk_buff *skb,\n\t\t\t\t     unsigned int transport_len,\n\t\t\t\t     __sum16(*skb_chkf)(struct sk_buff *skb));\n\n \nstatic inline bool skb_head_is_locked(const struct sk_buff *skb)\n{\n\treturn !skb->head_frag || skb_cloned(skb);\n}\n\n \nstatic inline __wsum lco_csum(struct sk_buff *skb)\n{\n\tunsigned char *csum_start = skb_checksum_start(skb);\n\tunsigned char *l4_hdr = skb_transport_header(skb);\n\t__wsum partial;\n\n\t \n\tpartial = ~csum_unfold(*(__force __sum16 *)(csum_start +\n\t\t\t\t\t\t    skb->csum_offset));\n\n\t \n\treturn csum_partial(l4_hdr, csum_start - l4_hdr, partial);\n}\n\nstatic inline bool skb_is_redirected(const struct sk_buff *skb)\n{\n\treturn skb->redirected;\n}\n\nstatic inline void skb_set_redirected(struct sk_buff *skb, bool from_ingress)\n{\n\tskb->redirected = 1;\n#ifdef CONFIG_NET_REDIRECT\n\tskb->from_ingress = from_ingress;\n\tif (skb->from_ingress)\n\t\tskb_clear_tstamp(skb);\n#endif\n}\n\nstatic inline void skb_reset_redirect(struct sk_buff *skb)\n{\n\tskb->redirected = 0;\n}\n\nstatic inline void skb_set_redirected_noclear(struct sk_buff *skb,\n\t\t\t\t\t      bool from_ingress)\n{\n\tskb->redirected = 1;\n#ifdef CONFIG_NET_REDIRECT\n\tskb->from_ingress = from_ingress;\n#endif\n}\n\nstatic inline bool skb_csum_is_sctp(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_IP_SCTP)\n\treturn skb->csum_not_inet;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void skb_reset_csum_not_inet(struct sk_buff *skb)\n{\n\tskb->ip_summed = CHECKSUM_NONE;\n#if IS_ENABLED(CONFIG_IP_SCTP)\n\tskb->csum_not_inet = 0;\n#endif\n}\n\nstatic inline void skb_set_kcov_handle(struct sk_buff *skb,\n\t\t\t\t       const u64 kcov_handle)\n{\n#ifdef CONFIG_KCOV\n\tskb->kcov_handle = kcov_handle;\n#endif\n}\n\nstatic inline u64 skb_get_kcov_handle(struct sk_buff *skb)\n{\n#ifdef CONFIG_KCOV\n\treturn skb->kcov_handle;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void skb_mark_for_recycle(struct sk_buff *skb)\n{\n#ifdef CONFIG_PAGE_POOL\n\tskb->pp_recycle = 1;\n#endif\n}\n\nssize_t skb_splice_from_iter(struct sk_buff *skb, struct iov_iter *iter,\n\t\t\t     ssize_t maxsize, gfp_t gfp);\n\n#endif\t \n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}