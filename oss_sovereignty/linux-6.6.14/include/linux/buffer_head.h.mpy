{
  "module_name": "buffer_head.h",
  "hash_id": "8fd4ae13e738b4f29536ccf08854109cd3c88a699d4efcc5eb60b0c43ba6fc46",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/buffer_head.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_BUFFER_HEAD_H\n#define _LINUX_BUFFER_HEAD_H\n\n#include <linux/types.h>\n#include <linux/blk_types.h>\n#include <linux/fs.h>\n#include <linux/linkage.h>\n#include <linux/pagemap.h>\n#include <linux/wait.h>\n#include <linux/atomic.h>\n\nenum bh_state_bits {\n\tBH_Uptodate,\t \n\tBH_Dirty,\t \n\tBH_Lock,\t \n\tBH_Req,\t\t \n\n\tBH_Mapped,\t \n\tBH_New,\t\t \n\tBH_Async_Read,\t \n\tBH_Async_Write,\t \n\tBH_Delay,\t \n\tBH_Boundary,\t \n\tBH_Write_EIO,\t \n\tBH_Unwritten,\t \n\tBH_Quiet,\t \n\tBH_Meta,\t \n\tBH_Prio,\t \n\tBH_Defer_Completion,  \n\n\tBH_PrivateStart, \n};\n\n#define MAX_BUF_PER_PAGE (PAGE_SIZE / 512)\n\nstruct page;\nstruct buffer_head;\nstruct address_space;\ntypedef void (bh_end_io_t)(struct buffer_head *bh, int uptodate);\n\n \nstruct buffer_head {\n\tunsigned long b_state;\t\t \n\tstruct buffer_head *b_this_page; \n\tunion {\n\t\tstruct page *b_page;\t \n\t\tstruct folio *b_folio;\t \n\t};\n\n\tsector_t b_blocknr;\t\t \n\tsize_t b_size;\t\t\t \n\tchar *b_data;\t\t\t \n\n\tstruct block_device *b_bdev;\n\tbh_end_io_t *b_end_io;\t\t \n \tvoid *b_private;\t\t \n\tstruct list_head b_assoc_buffers;  \n\tstruct address_space *b_assoc_map;\t \n\tatomic_t b_count;\t\t \n\tspinlock_t b_uptodate_lock;\t \n};\n\n \n#define BUFFER_FNS(bit, name)\t\t\t\t\t\t\\\nstatic __always_inline void set_buffer_##name(struct buffer_head *bh)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tif (!test_bit(BH_##bit, &(bh)->b_state))\t\t\t\\\n\t\tset_bit(BH_##bit, &(bh)->b_state);\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic __always_inline void clear_buffer_##name(struct buffer_head *bh)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tclear_bit(BH_##bit, &(bh)->b_state);\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic __always_inline int buffer_##name(const struct buffer_head *bh)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_bit(BH_##bit, &(bh)->b_state);\t\t\t\\\n}\n\n \n#define TAS_BUFFER_FNS(bit, name)\t\t\t\t\t\\\nstatic __always_inline int test_set_buffer_##name(struct buffer_head *bh) \\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_and_set_bit(BH_##bit, &(bh)->b_state);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic __always_inline int test_clear_buffer_##name(struct buffer_head *bh) \\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn test_and_clear_bit(BH_##bit, &(bh)->b_state);\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\n \nBUFFER_FNS(Dirty, dirty)\nTAS_BUFFER_FNS(Dirty, dirty)\nBUFFER_FNS(Lock, locked)\nBUFFER_FNS(Req, req)\nTAS_BUFFER_FNS(Req, req)\nBUFFER_FNS(Mapped, mapped)\nBUFFER_FNS(New, new)\nBUFFER_FNS(Async_Read, async_read)\nBUFFER_FNS(Async_Write, async_write)\nBUFFER_FNS(Delay, delay)\nBUFFER_FNS(Boundary, boundary)\nBUFFER_FNS(Write_EIO, write_io_error)\nBUFFER_FNS(Unwritten, unwritten)\nBUFFER_FNS(Meta, meta)\nBUFFER_FNS(Prio, prio)\nBUFFER_FNS(Defer_Completion, defer_completion)\n\nstatic __always_inline void set_buffer_uptodate(struct buffer_head *bh)\n{\n\t \n\tif (test_bit(BH_Uptodate, &bh->b_state))\n\t\treturn;\n\n\t \n\tsmp_mb__before_atomic();\n\tset_bit(BH_Uptodate, &bh->b_state);\n}\n\nstatic __always_inline void clear_buffer_uptodate(struct buffer_head *bh)\n{\n\tclear_bit(BH_Uptodate, &bh->b_state);\n}\n\nstatic __always_inline int buffer_uptodate(const struct buffer_head *bh)\n{\n\t \n\treturn test_bit_acquire(BH_Uptodate, &bh->b_state);\n}\n\nstatic inline unsigned long bh_offset(const struct buffer_head *bh)\n{\n\treturn (unsigned long)(bh)->b_data & (page_size(bh->b_page) - 1);\n}\n\n \n#define page_buffers(page)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\\\n\t\tBUG_ON(!PagePrivate(page));\t\t\t\\\n\t\t((struct buffer_head *)page_private(page));\t\\\n\t})\n#define page_has_buffers(page)\tPagePrivate(page)\n#define folio_buffers(folio)\t\tfolio_get_private(folio)\n\nvoid buffer_check_dirty_writeback(struct folio *folio,\n\t\t\t\t     bool *dirty, bool *writeback);\n\n \n\nvoid mark_buffer_dirty(struct buffer_head *bh);\nvoid mark_buffer_write_io_error(struct buffer_head *bh);\nvoid touch_buffer(struct buffer_head *bh);\nvoid folio_set_bh(struct buffer_head *bh, struct folio *folio,\n\t\t  unsigned long offset);\nstruct buffer_head *folio_alloc_buffers(struct folio *folio, unsigned long size,\n\t\t\t\t\tbool retry);\nstruct buffer_head *alloc_page_buffers(struct page *page, unsigned long size,\n\t\tbool retry);\nvoid create_empty_buffers(struct page *, unsigned long,\n\t\t\tunsigned long b_state);\nvoid folio_create_empty_buffers(struct folio *folio, unsigned long blocksize,\n\t\t\t\tunsigned long b_state);\nvoid end_buffer_read_sync(struct buffer_head *bh, int uptodate);\nvoid end_buffer_write_sync(struct buffer_head *bh, int uptodate);\nvoid end_buffer_async_write(struct buffer_head *bh, int uptodate);\n\n \nvoid mark_buffer_dirty_inode(struct buffer_head *bh, struct inode *inode);\nint generic_buffers_fsync_noflush(struct file *file, loff_t start, loff_t end,\n\t\t\t\t  bool datasync);\nint generic_buffers_fsync(struct file *file, loff_t start, loff_t end,\n\t\t\t  bool datasync);\nvoid clean_bdev_aliases(struct block_device *bdev, sector_t block,\n\t\t\tsector_t len);\nstatic inline void clean_bdev_bh_alias(struct buffer_head *bh)\n{\n\tclean_bdev_aliases(bh->b_bdev, bh->b_blocknr, 1);\n}\n\nvoid mark_buffer_async_write(struct buffer_head *bh);\nvoid __wait_on_buffer(struct buffer_head *);\nwait_queue_head_t *bh_waitq_head(struct buffer_head *bh);\nstruct buffer_head *__find_get_block(struct block_device *bdev, sector_t block,\n\t\t\tunsigned size);\nstruct buffer_head *__getblk_gfp(struct block_device *bdev, sector_t block,\n\t\t\t\t  unsigned size, gfp_t gfp);\nvoid __brelse(struct buffer_head *);\nvoid __bforget(struct buffer_head *);\nvoid __breadahead(struct block_device *, sector_t block, unsigned int size);\nstruct buffer_head *__bread_gfp(struct block_device *,\n\t\t\t\tsector_t block, unsigned size, gfp_t gfp);\nstruct buffer_head *alloc_buffer_head(gfp_t gfp_flags);\nvoid free_buffer_head(struct buffer_head * bh);\nvoid unlock_buffer(struct buffer_head *bh);\nvoid __lock_buffer(struct buffer_head *bh);\nint sync_dirty_buffer(struct buffer_head *bh);\nint __sync_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags);\nvoid write_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags);\nvoid submit_bh(blk_opf_t, struct buffer_head *);\nvoid write_boundary_block(struct block_device *bdev,\n\t\t\tsector_t bblock, unsigned blocksize);\nint bh_uptodate_or_lock(struct buffer_head *bh);\nint __bh_read(struct buffer_head *bh, blk_opf_t op_flags, bool wait);\nvoid __bh_read_batch(int nr, struct buffer_head *bhs[],\n\t\t     blk_opf_t op_flags, bool force_lock);\n\n \nvoid block_invalidate_folio(struct folio *folio, size_t offset, size_t length);\nint block_write_full_page(struct page *page, get_block_t *get_block,\n\t\t\t\tstruct writeback_control *wbc);\nint __block_write_full_folio(struct inode *inode, struct folio *folio,\n\t\t\tget_block_t *get_block, struct writeback_control *wbc,\n\t\t\tbh_end_io_t *handler);\nint block_read_full_folio(struct folio *, get_block_t *);\nbool block_is_partially_uptodate(struct folio *, size_t from, size_t count);\nint block_write_begin(struct address_space *mapping, loff_t pos, unsigned len,\n\t\tstruct page **pagep, get_block_t *get_block);\nint __block_write_begin(struct page *page, loff_t pos, unsigned len,\n\t\tget_block_t *get_block);\nint block_write_end(struct file *, struct address_space *,\n\t\t\t\tloff_t, unsigned, unsigned,\n\t\t\t\tstruct page *, void *);\nint generic_write_end(struct file *, struct address_space *,\n\t\t\t\tloff_t, unsigned, unsigned,\n\t\t\t\tstruct page *, void *);\nvoid folio_zero_new_buffers(struct folio *folio, size_t from, size_t to);\nvoid clean_page_buffers(struct page *page);\nint cont_write_begin(struct file *, struct address_space *, loff_t,\n\t\t\tunsigned, struct page **, void **,\n\t\t\tget_block_t *, loff_t *);\nint generic_cont_expand_simple(struct inode *inode, loff_t size);\nvoid block_commit_write(struct page *page, unsigned int from, unsigned int to);\nint block_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf,\n\t\t\t\tget_block_t get_block);\nsector_t generic_block_bmap(struct address_space *, sector_t, get_block_t *);\nint block_truncate_page(struct address_space *, loff_t, get_block_t *);\n\n#ifdef CONFIG_MIGRATION\nextern int buffer_migrate_folio(struct address_space *,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode);\nextern int buffer_migrate_folio_norefs(struct address_space *,\n\t\tstruct folio *dst, struct folio *src, enum migrate_mode);\n#else\n#define buffer_migrate_folio NULL\n#define buffer_migrate_folio_norefs NULL\n#endif\n\n \n\nstatic inline void get_bh(struct buffer_head *bh)\n{\n        atomic_inc(&bh->b_count);\n}\n\nstatic inline void put_bh(struct buffer_head *bh)\n{\n        smp_mb__before_atomic();\n        atomic_dec(&bh->b_count);\n}\n\nstatic inline void brelse(struct buffer_head *bh)\n{\n\tif (bh)\n\t\t__brelse(bh);\n}\n\nstatic inline void bforget(struct buffer_head *bh)\n{\n\tif (bh)\n\t\t__bforget(bh);\n}\n\nstatic inline struct buffer_head *\nsb_bread(struct super_block *sb, sector_t block)\n{\n\treturn __bread_gfp(sb->s_bdev, block, sb->s_blocksize, __GFP_MOVABLE);\n}\n\nstatic inline struct buffer_head *\nsb_bread_unmovable(struct super_block *sb, sector_t block)\n{\n\treturn __bread_gfp(sb->s_bdev, block, sb->s_blocksize, 0);\n}\n\nstatic inline void\nsb_breadahead(struct super_block *sb, sector_t block)\n{\n\t__breadahead(sb->s_bdev, block, sb->s_blocksize);\n}\n\nstatic inline struct buffer_head *\nsb_getblk(struct super_block *sb, sector_t block)\n{\n\treturn __getblk_gfp(sb->s_bdev, block, sb->s_blocksize, __GFP_MOVABLE);\n}\n\n\nstatic inline struct buffer_head *\nsb_getblk_gfp(struct super_block *sb, sector_t block, gfp_t gfp)\n{\n\treturn __getblk_gfp(sb->s_bdev, block, sb->s_blocksize, gfp);\n}\n\nstatic inline struct buffer_head *\nsb_find_get_block(struct super_block *sb, sector_t block)\n{\n\treturn __find_get_block(sb->s_bdev, block, sb->s_blocksize);\n}\n\nstatic inline void\nmap_bh(struct buffer_head *bh, struct super_block *sb, sector_t block)\n{\n\tset_buffer_mapped(bh);\n\tbh->b_bdev = sb->s_bdev;\n\tbh->b_blocknr = block;\n\tbh->b_size = sb->s_blocksize;\n}\n\nstatic inline void wait_on_buffer(struct buffer_head *bh)\n{\n\tmight_sleep();\n\tif (buffer_locked(bh))\n\t\t__wait_on_buffer(bh);\n}\n\nstatic inline int trylock_buffer(struct buffer_head *bh)\n{\n\treturn likely(!test_and_set_bit_lock(BH_Lock, &bh->b_state));\n}\n\nstatic inline void lock_buffer(struct buffer_head *bh)\n{\n\tmight_sleep();\n\tif (!trylock_buffer(bh))\n\t\t__lock_buffer(bh);\n}\n\nstatic inline struct buffer_head *getblk_unmovable(struct block_device *bdev,\n\t\t\t\t\t\t   sector_t block,\n\t\t\t\t\t\t   unsigned size)\n{\n\treturn __getblk_gfp(bdev, block, size, 0);\n}\n\nstatic inline struct buffer_head *__getblk(struct block_device *bdev,\n\t\t\t\t\t   sector_t block,\n\t\t\t\t\t   unsigned size)\n{\n\treturn __getblk_gfp(bdev, block, size, __GFP_MOVABLE);\n}\n\nstatic inline void bh_readahead(struct buffer_head *bh, blk_opf_t op_flags)\n{\n\tif (!buffer_uptodate(bh) && trylock_buffer(bh)) {\n\t\tif (!buffer_uptodate(bh))\n\t\t\t__bh_read(bh, op_flags, false);\n\t\telse\n\t\t\tunlock_buffer(bh);\n\t}\n}\n\nstatic inline void bh_read_nowait(struct buffer_head *bh, blk_opf_t op_flags)\n{\n\tif (!bh_uptodate_or_lock(bh))\n\t\t__bh_read(bh, op_flags, false);\n}\n\n \nstatic inline int bh_read(struct buffer_head *bh, blk_opf_t op_flags)\n{\n\tif (bh_uptodate_or_lock(bh))\n\t\treturn 1;\n\treturn __bh_read(bh, op_flags, true);\n}\n\nstatic inline void bh_read_batch(int nr, struct buffer_head *bhs[])\n{\n\t__bh_read_batch(nr, bhs, 0, true);\n}\n\nstatic inline void bh_readahead_batch(int nr, struct buffer_head *bhs[],\n\t\t\t\t      blk_opf_t op_flags)\n{\n\t__bh_read_batch(nr, bhs, op_flags, false);\n}\n\n \nstatic inline struct buffer_head *\n__bread(struct block_device *bdev, sector_t block, unsigned size)\n{\n\treturn __bread_gfp(bdev, block, size, __GFP_MOVABLE);\n}\n\nbool block_dirty_folio(struct address_space *mapping, struct folio *folio);\n\n#ifdef CONFIG_BUFFER_HEAD\n\nvoid buffer_init(void);\nbool try_to_free_buffers(struct folio *folio);\nint inode_has_buffers(struct inode *inode);\nvoid invalidate_inode_buffers(struct inode *inode);\nint remove_inode_buffers(struct inode *inode);\nint sync_mapping_buffers(struct address_space *mapping);\nvoid invalidate_bh_lrus(void);\nvoid invalidate_bh_lrus_cpu(void);\nbool has_bh_in_lru(int cpu, void *dummy);\nextern int buffer_heads_over_limit;\n\n#else  \n\nstatic inline void buffer_init(void) {}\nstatic inline bool try_to_free_buffers(struct folio *folio) { return true; }\nstatic inline int inode_has_buffers(struct inode *inode) { return 0; }\nstatic inline void invalidate_inode_buffers(struct inode *inode) {}\nstatic inline int remove_inode_buffers(struct inode *inode) { return 1; }\nstatic inline int sync_mapping_buffers(struct address_space *mapping) { return 0; }\nstatic inline void invalidate_bh_lrus(void) {}\nstatic inline void invalidate_bh_lrus_cpu(void) {}\nstatic inline bool has_bh_in_lru(int cpu, void *dummy) { return false; }\n#define buffer_heads_over_limit 0\n\n#endif  \n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}