{
  "module_name": "filter.h",
  "hash_id": "742d1d138dd6ba14fc0b82b7d2f6d8f531008f6038fe3b8b6222e69b47fdc8e6",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/filter.h",
  "human_readable_source": " \n \n#ifndef __LINUX_FILTER_H__\n#define __LINUX_FILTER_H__\n\n#include <linux/atomic.h>\n#include <linux/bpf.h>\n#include <linux/refcount.h>\n#include <linux/compat.h>\n#include <linux/skbuff.h>\n#include <linux/linkage.h>\n#include <linux/printk.h>\n#include <linux/workqueue.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/capability.h>\n#include <linux/set_memory.h>\n#include <linux/kallsyms.h>\n#include <linux/if_vlan.h>\n#include <linux/vmalloc.h>\n#include <linux/sockptr.h>\n#include <crypto/sha1.h>\n#include <linux/u64_stats_sync.h>\n\n#include <net/sch_generic.h>\n\n#include <asm/byteorder.h>\n#include <uapi/linux/filter.h>\n\nstruct sk_buff;\nstruct sock;\nstruct seccomp_data;\nstruct bpf_prog_aux;\nstruct xdp_rxq_info;\nstruct xdp_buff;\nstruct sock_reuseport;\nstruct ctl_table;\nstruct ctl_table_header;\n\n \n#define BPF_REG_ARG1\tBPF_REG_1\n#define BPF_REG_ARG2\tBPF_REG_2\n#define BPF_REG_ARG3\tBPF_REG_3\n#define BPF_REG_ARG4\tBPF_REG_4\n#define BPF_REG_ARG5\tBPF_REG_5\n#define BPF_REG_CTX\tBPF_REG_6\n#define BPF_REG_FP\tBPF_REG_10\n\n \n#define BPF_REG_A\tBPF_REG_0\n#define BPF_REG_X\tBPF_REG_7\n#define BPF_REG_TMP\tBPF_REG_2\t \n#define BPF_REG_D\tBPF_REG_8\t \n#define BPF_REG_H\tBPF_REG_9\t \n\n \n#define BPF_REG_AX\t\tMAX_BPF_REG\n#define MAX_BPF_EXT_REG\t\t(MAX_BPF_REG + 1)\n#define MAX_BPF_JIT_REG\t\tMAX_BPF_EXT_REG\n\n \n#define BPF_TAIL_CALL\t0xf0\n\n \n#define BPF_PROBE_MEM\t0x20\n\n \n#define BPF_PROBE_MEMSX\t0x40\n\n \n#define BPF_CALL_ARGS\t0xe0\n\n \n#define BPF_NOSPEC\t0xc0\n\n \n#define BPF_SYM_ELF_TYPE\t't'\n\n \n#define MAX_BPF_STACK\t512\n\n \n\n \n\n#define BPF_ALU64_REG_OFF(OP, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n#define BPF_ALU64_REG(OP, DST, SRC)\t\t\t\t\\\n\tBPF_ALU64_REG_OFF(OP, DST, SRC, 0)\n\n#define BPF_ALU32_REG_OFF(OP, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_OP(OP) | BPF_X,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n#define BPF_ALU32_REG(OP, DST, SRC)\t\t\t\t\\\n\tBPF_ALU32_REG_OFF(OP, DST, SRC, 0)\n\n \n\n#define BPF_ALU64_IMM(OP, DST, IMM)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n#define BPF_ALU32_IMM(OP, DST, IMM)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_OP(OP) | BPF_K,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_ENDIAN(TYPE, DST, LEN)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_END | BPF_SRC(TYPE),\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = LEN })\n\n \n\n#define BPF_MOV64_REG(DST, SRC)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU64 | BPF_MOV | BPF_X,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n#define BPF_MOV32_REG(DST, SRC)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_MOV | BPF_X,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_MOV64_IMM(DST, IMM)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU64 | BPF_MOV | BPF_K,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n#define BPF_MOV32_IMM(DST, IMM)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_MOV | BPF_K,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n#define BPF_ZEXT_REG(DST)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_MOV | BPF_X,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = DST,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = 1 })\n\nstatic inline bool insn_is_zext(const struct bpf_insn *insn)\n{\n\treturn insn->code == (BPF_ALU | BPF_MOV | BPF_X) && insn->imm == 1;\n}\n\n \n#define BPF_LD_IMM64(DST, IMM)\t\t\t\t\t\\\n\tBPF_LD_IMM64_RAW(DST, 0, IMM)\n\n#define BPF_LD_IMM64_RAW(DST, SRC, IMM)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_LD | BPF_DW | BPF_IMM,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = (__u32) (IMM) }),\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = 0,  \t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = ((__u64) (IMM)) >> 32 })\n\n \n#define BPF_LD_MAP_FD(DST, MAP_FD)\t\t\t\t\\\n\tBPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)\n\n \n\n#define BPF_MOV64_RAW(TYPE, DST, SRC, IMM)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU64 | BPF_MOV | BPF_SRC(TYPE),\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n#define BPF_MOV32_RAW(TYPE, DST, SRC, IMM)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ALU | BPF_MOV | BPF_SRC(TYPE),\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_LD_ABS(SIZE, IMM)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_ABS,\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_LD_IND(SIZE, SRC, IMM)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_LD | BPF_SIZE(SIZE) | BPF_IND,\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_STX_MEM(SIZE, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n\n \n\n#define BPF_ATOMIC_OP(SIZE, OP, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_ATOMIC,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = OP })\n\n \n#define BPF_STX_XADD(SIZE, DST, SRC, OFF) BPF_ATOMIC_OP(SIZE, BPF_ADD, DST, SRC, OFF)\n\n \n\n#define BPF_ST_MEM(SIZE, DST, OFF, IMM)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ST | BPF_SIZE(SIZE) | BPF_MEM,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_JMP_REG(OP, DST, SRC, OFF)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_OP(OP) | BPF_X,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_JMP_IMM(OP, DST, IMM, OFF)\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_OP(OP) | BPF_K,\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_JMP32_REG(OP, DST, SRC, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP32 | BPF_OP(OP) | BPF_X,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_JMP32_IMM(OP, DST, IMM, OFF)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP32 | BPF_OP(OP) | BPF_K,\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_JMP_A(OFF)\t\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_JA,\t\t\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_CALL_REL(TGT)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_CALL,\t\t\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = BPF_PSEUDO_CALL,\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = TGT })\n\n \n\n#define BPF_CALL_IMM(x)\t((void *)(x) - (void *)__bpf_call_base)\n\n#define BPF_EMIT_CALL(FUNC)\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_CALL,\t\t\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = BPF_CALL_IMM(FUNC) })\n\n \n\n#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = CODE,\t\t\t\t\t\\\n\t\t.dst_reg = DST,\t\t\t\t\t\\\n\t\t.src_reg = SRC,\t\t\t\t\t\\\n\t\t.off   = OFF,\t\t\t\t\t\\\n\t\t.imm   = IMM })\n\n \n\n#define BPF_EXIT_INSN()\t\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_JMP | BPF_EXIT,\t\t\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define BPF_ST_NOSPEC()\t\t\t\t\t\t\\\n\t((struct bpf_insn) {\t\t\t\t\t\\\n\t\t.code  = BPF_ST | BPF_NOSPEC,\t\t\t\\\n\t\t.dst_reg = 0,\t\t\t\t\t\\\n\t\t.src_reg = 0,\t\t\t\t\t\\\n\t\t.off   = 0,\t\t\t\t\t\\\n\t\t.imm   = 0 })\n\n \n\n#define __BPF_STMT(CODE, K)\t\t\t\t\t\\\n\t((struct sock_filter) BPF_STMT(CODE, K))\n\n#define __BPF_JUMP(CODE, K, JT, JF)\t\t\t\t\\\n\t((struct sock_filter) BPF_JUMP(CODE, K, JT, JF))\n\n#define bytes_to_bpf_size(bytes)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint bpf_size = -EINVAL;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (bytes == sizeof(u8))\t\t\t\t\\\n\t\tbpf_size = BPF_B;\t\t\t\t\\\n\telse if (bytes == sizeof(u16))\t\t\t\t\\\n\t\tbpf_size = BPF_H;\t\t\t\t\\\n\telse if (bytes == sizeof(u32))\t\t\t\t\\\n\t\tbpf_size = BPF_W;\t\t\t\t\\\n\telse if (bytes == sizeof(u64))\t\t\t\t\\\n\t\tbpf_size = BPF_DW;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tbpf_size;\t\t\t\t\t\t\\\n})\n\n#define bpf_size_to_bytes(bpf_size)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tint bytes = -EINVAL;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tif (bpf_size == BPF_B)\t\t\t\t\t\\\n\t\tbytes = sizeof(u8);\t\t\t\t\\\n\telse if (bpf_size == BPF_H)\t\t\t\t\\\n\t\tbytes = sizeof(u16);\t\t\t\t\\\n\telse if (bpf_size == BPF_W)\t\t\t\t\\\n\t\tbytes = sizeof(u32);\t\t\t\t\\\n\telse if (bpf_size == BPF_DW)\t\t\t\t\\\n\t\tbytes = sizeof(u64);\t\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\tbytes;\t\t\t\t\t\t\t\\\n})\n\n#define BPF_SIZEOF(type)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\\\n\t\tconst int __size = bytes_to_bpf_size(sizeof(type)); \\\n\t\tBUILD_BUG_ON(__size < 0);\t\t\t\\\n\t\t__size;\t\t\t\t\t\t\\\n\t})\n\n#define BPF_FIELD_SIZEOF(type, field)\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\\\n\t\tconst int __size = bytes_to_bpf_size(sizeof_field(type, field)); \\\n\t\tBUILD_BUG_ON(__size < 0);\t\t\t\\\n\t\t__size;\t\t\t\t\t\t\\\n\t})\n\n#define BPF_LDST_BYTES(insn)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\\\n\t\tconst int __size = bpf_size_to_bytes(BPF_SIZE((insn)->code)); \\\n\t\tWARN_ON(__size < 0);\t\t\t\t\\\n\t\t__size;\t\t\t\t\t\t\\\n\t})\n\n#define __BPF_MAP_0(m, v, ...) v\n#define __BPF_MAP_1(m, v, t, a, ...) m(t, a)\n#define __BPF_MAP_2(m, v, t, a, ...) m(t, a), __BPF_MAP_1(m, v, __VA_ARGS__)\n#define __BPF_MAP_3(m, v, t, a, ...) m(t, a), __BPF_MAP_2(m, v, __VA_ARGS__)\n#define __BPF_MAP_4(m, v, t, a, ...) m(t, a), __BPF_MAP_3(m, v, __VA_ARGS__)\n#define __BPF_MAP_5(m, v, t, a, ...) m(t, a), __BPF_MAP_4(m, v, __VA_ARGS__)\n\n#define __BPF_REG_0(...) __BPF_PAD(5)\n#define __BPF_REG_1(...) __BPF_MAP(1, __VA_ARGS__), __BPF_PAD(4)\n#define __BPF_REG_2(...) __BPF_MAP(2, __VA_ARGS__), __BPF_PAD(3)\n#define __BPF_REG_3(...) __BPF_MAP(3, __VA_ARGS__), __BPF_PAD(2)\n#define __BPF_REG_4(...) __BPF_MAP(4, __VA_ARGS__), __BPF_PAD(1)\n#define __BPF_REG_5(...) __BPF_MAP(5, __VA_ARGS__)\n\n#define __BPF_MAP(n, ...) __BPF_MAP_##n(__VA_ARGS__)\n#define __BPF_REG(n, ...) __BPF_REG_##n(__VA_ARGS__)\n\n#define __BPF_CAST(t, a)\t\t\t\t\t\t       \\\n\t(__force t)\t\t\t\t\t\t\t       \\\n\t(__force\t\t\t\t\t\t\t       \\\n\t typeof(__builtin_choose_expr(sizeof(t) == sizeof(unsigned long),      \\\n\t\t\t\t      (unsigned long)0, (t)0))) a\n#define __BPF_V void\n#define __BPF_N\n\n#define __BPF_DECL_ARGS(t, a) t   a\n#define __BPF_DECL_REGS(t, a) u64 a\n\n#define __BPF_PAD(n)\t\t\t\t\t\t\t       \\\n\t__BPF_MAP(n, __BPF_DECL_ARGS, __BPF_N, u64, __ur_1, u64, __ur_2,       \\\n\t\t  u64, __ur_3, u64, __ur_4, u64, __ur_5)\n\n#define BPF_CALL_x(x, name, ...)\t\t\t\t\t       \\\n\tstatic __always_inline\t\t\t\t\t\t       \\\n\tu64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__));   \\\n\ttypedef u64 (*btf_##name)(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__)); \\\n\tu64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__));\t       \\\n\tu64 name(__BPF_REG(x, __BPF_DECL_REGS, __BPF_N, __VA_ARGS__))\t       \\\n\t{\t\t\t\t\t\t\t\t       \\\n\t\treturn ((btf_##name)____##name)(__BPF_MAP(x,__BPF_CAST,__BPF_N,__VA_ARGS__));\\\n\t}\t\t\t\t\t\t\t\t       \\\n\tstatic __always_inline\t\t\t\t\t\t       \\\n\tu64 ____##name(__BPF_MAP(x, __BPF_DECL_ARGS, __BPF_V, __VA_ARGS__))\n\n#define BPF_CALL_0(name, ...)\tBPF_CALL_x(0, name, __VA_ARGS__)\n#define BPF_CALL_1(name, ...)\tBPF_CALL_x(1, name, __VA_ARGS__)\n#define BPF_CALL_2(name, ...)\tBPF_CALL_x(2, name, __VA_ARGS__)\n#define BPF_CALL_3(name, ...)\tBPF_CALL_x(3, name, __VA_ARGS__)\n#define BPF_CALL_4(name, ...)\tBPF_CALL_x(4, name, __VA_ARGS__)\n#define BPF_CALL_5(name, ...)\tBPF_CALL_x(5, name, __VA_ARGS__)\n\n#define bpf_ctx_range(TYPE, MEMBER)\t\t\t\t\t\t\\\n\toffsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1\n#define bpf_ctx_range_till(TYPE, MEMBER1, MEMBER2)\t\t\t\t\\\n\toffsetof(TYPE, MEMBER1) ... offsetofend(TYPE, MEMBER2) - 1\n#if BITS_PER_LONG == 64\n# define bpf_ctx_range_ptr(TYPE, MEMBER)\t\t\t\t\t\\\n\toffsetof(TYPE, MEMBER) ... offsetofend(TYPE, MEMBER) - 1\n#else\n# define bpf_ctx_range_ptr(TYPE, MEMBER)\t\t\t\t\t\\\n\toffsetof(TYPE, MEMBER) ... offsetof(TYPE, MEMBER) + 8 - 1\n#endif  \n\n#define bpf_target_off(TYPE, MEMBER, SIZE, PTR_SIZE)\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\t\\\n\t\tBUILD_BUG_ON(sizeof_field(TYPE, MEMBER) != (SIZE));\t\t\\\n\t\t*(PTR_SIZE) = (SIZE);\t\t\t\t\t\t\\\n\t\toffsetof(TYPE, MEMBER);\t\t\t\t\t\t\\\n\t})\n\n \nstruct compat_sock_fprog {\n\tu16\t\tlen;\n\tcompat_uptr_t\tfilter;\t \n};\n\nstruct sock_fprog_kern {\n\tu16\t\t\tlen;\n\tstruct sock_filter\t*filter;\n};\n\n \n#define BPF_IMAGE_ALIGNMENT 8\n\nstruct bpf_binary_header {\n\tu32 size;\n\tu8 image[] __aligned(BPF_IMAGE_ALIGNMENT);\n};\n\nstruct bpf_prog_stats {\n\tu64_stats_t cnt;\n\tu64_stats_t nsecs;\n\tu64_stats_t misses;\n\tstruct u64_stats_sync syncp;\n} __aligned(2 * sizeof(u64));\n\nstruct sk_filter {\n\trefcount_t\trefcnt;\n\tstruct rcu_head\trcu;\n\tstruct bpf_prog\t*prog;\n};\n\nDECLARE_STATIC_KEY_FALSE(bpf_stats_enabled_key);\n\nextern struct mutex nf_conn_btf_access_lock;\nextern int (*nfct_btf_struct_access)(struct bpf_verifier_log *log,\n\t\t\t\t     const struct bpf_reg_state *reg,\n\t\t\t\t     int off, int size);\n\ntypedef unsigned int (*bpf_dispatcher_fn)(const void *ctx,\n\t\t\t\t\t  const struct bpf_insn *insnsi,\n\t\t\t\t\t  unsigned int (*bpf_func)(const void *,\n\t\t\t\t\t\t\t\t   const struct bpf_insn *));\n\nstatic __always_inline u32 __bpf_prog_run(const struct bpf_prog *prog,\n\t\t\t\t\t  const void *ctx,\n\t\t\t\t\t  bpf_dispatcher_fn dfunc)\n{\n\tu32 ret;\n\n\tcant_migrate();\n\tif (static_branch_unlikely(&bpf_stats_enabled_key)) {\n\t\tstruct bpf_prog_stats *stats;\n\t\tu64 start = sched_clock();\n\t\tunsigned long flags;\n\n\t\tret = dfunc(ctx, prog->insnsi, prog->bpf_func);\n\t\tstats = this_cpu_ptr(prog->stats);\n\t\tflags = u64_stats_update_begin_irqsave(&stats->syncp);\n\t\tu64_stats_inc(&stats->cnt);\n\t\tu64_stats_add(&stats->nsecs, sched_clock() - start);\n\t\tu64_stats_update_end_irqrestore(&stats->syncp, flags);\n\t} else {\n\t\tret = dfunc(ctx, prog->insnsi, prog->bpf_func);\n\t}\n\treturn ret;\n}\n\nstatic __always_inline u32 bpf_prog_run(const struct bpf_prog *prog, const void *ctx)\n{\n\treturn __bpf_prog_run(prog, ctx, bpf_dispatcher_nop_func);\n}\n\n \nstatic inline u32 bpf_prog_run_pin_on_cpu(const struct bpf_prog *prog,\n\t\t\t\t\t  const void *ctx)\n{\n\tu32 ret;\n\n\tmigrate_disable();\n\tret = bpf_prog_run(prog, ctx);\n\tmigrate_enable();\n\treturn ret;\n}\n\n#define BPF_SKB_CB_LEN QDISC_CB_PRIV_LEN\n\nstruct bpf_skb_data_end {\n\tstruct qdisc_skb_cb qdisc_cb;\n\tvoid *data_meta;\n\tvoid *data_end;\n};\n\nstruct bpf_nh_params {\n\tu32 nh_family;\n\tunion {\n\t\tu32 ipv4_nh;\n\t\tstruct in6_addr ipv6_nh;\n\t};\n};\n\nstruct bpf_redirect_info {\n\tu64 tgt_index;\n\tvoid *tgt_value;\n\tstruct bpf_map *map;\n\tu32 flags;\n\tu32 kern_flags;\n\tu32 map_id;\n\tenum bpf_map_type map_type;\n\tstruct bpf_nh_params nh;\n};\n\nDECLARE_PER_CPU(struct bpf_redirect_info, bpf_redirect_info);\n\n \n#define BPF_RI_F_RF_NO_DIRECT\tBIT(0)\t \n\n \nstatic inline void bpf_compute_data_pointers(struct sk_buff *skb)\n{\n\tstruct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;\n\n\tBUILD_BUG_ON(sizeof(*cb) > sizeof_field(struct sk_buff, cb));\n\tcb->data_meta = skb->data - skb_metadata_len(skb);\n\tcb->data_end  = skb->data + skb_headlen(skb);\n}\n\n \nstatic inline void bpf_compute_and_save_data_end(\n\tstruct sk_buff *skb, void **saved_data_end)\n{\n\tstruct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;\n\n\t*saved_data_end = cb->data_end;\n\tcb->data_end  = skb->data + skb_headlen(skb);\n}\n\n \nstatic inline void bpf_restore_data_end(\n\tstruct sk_buff *skb, void *saved_data_end)\n{\n\tstruct bpf_skb_data_end *cb = (struct bpf_skb_data_end *)skb->cb;\n\n\tcb->data_end = saved_data_end;\n}\n\nstatic inline u8 *bpf_skb_cb(const struct sk_buff *skb)\n{\n\t \n\tBUILD_BUG_ON(sizeof_field(struct __sk_buff, cb) != BPF_SKB_CB_LEN);\n\tBUILD_BUG_ON(sizeof_field(struct __sk_buff, cb) !=\n\t\t     sizeof_field(struct qdisc_skb_cb, data));\n\n\treturn qdisc_skb_cb(skb)->data;\n}\n\n \nstatic inline u32 __bpf_prog_run_save_cb(const struct bpf_prog *prog,\n\t\t\t\t\t const void *ctx)\n{\n\tconst struct sk_buff *skb = ctx;\n\tu8 *cb_data = bpf_skb_cb(skb);\n\tu8 cb_saved[BPF_SKB_CB_LEN];\n\tu32 res;\n\n\tif (unlikely(prog->cb_access)) {\n\t\tmemcpy(cb_saved, cb_data, sizeof(cb_saved));\n\t\tmemset(cb_data, 0, sizeof(cb_saved));\n\t}\n\n\tres = bpf_prog_run(prog, skb);\n\n\tif (unlikely(prog->cb_access))\n\t\tmemcpy(cb_data, cb_saved, sizeof(cb_saved));\n\n\treturn res;\n}\n\nstatic inline u32 bpf_prog_run_save_cb(const struct bpf_prog *prog,\n\t\t\t\t       struct sk_buff *skb)\n{\n\tu32 res;\n\n\tmigrate_disable();\n\tres = __bpf_prog_run_save_cb(prog, skb);\n\tmigrate_enable();\n\treturn res;\n}\n\nstatic inline u32 bpf_prog_run_clear_cb(const struct bpf_prog *prog,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tu8 *cb_data = bpf_skb_cb(skb);\n\tu32 res;\n\n\tif (unlikely(prog->cb_access))\n\t\tmemset(cb_data, 0, BPF_SKB_CB_LEN);\n\n\tres = bpf_prog_run_pin_on_cpu(prog, skb);\n\treturn res;\n}\n\nDECLARE_BPF_DISPATCHER(xdp)\n\nDECLARE_STATIC_KEY_FALSE(bpf_master_redirect_enabled_key);\n\nu32 xdp_master_redirect(struct xdp_buff *xdp);\n\nvoid bpf_prog_change_xdp(struct bpf_prog *prev_prog, struct bpf_prog *prog);\n\nstatic inline u32 bpf_prog_insn_size(const struct bpf_prog *prog)\n{\n\treturn prog->len * sizeof(struct bpf_insn);\n}\n\nstatic inline u32 bpf_prog_tag_scratch_size(const struct bpf_prog *prog)\n{\n\treturn round_up(bpf_prog_insn_size(prog) +\n\t\t\tsizeof(__be64) + 1, SHA1_BLOCK_SIZE);\n}\n\nstatic inline unsigned int bpf_prog_size(unsigned int proglen)\n{\n\treturn max(sizeof(struct bpf_prog),\n\t\t   offsetof(struct bpf_prog, insns[proglen]));\n}\n\nstatic inline bool bpf_prog_was_classic(const struct bpf_prog *prog)\n{\n\t \n\treturn prog->type == BPF_PROG_TYPE_UNSPEC;\n}\n\nstatic inline u32 bpf_ctx_off_adjust_machine(u32 size)\n{\n\tconst u32 size_machine = sizeof(unsigned long);\n\n\tif (size > size_machine && size % size_machine == 0)\n\t\tsize = size_machine;\n\n\treturn size;\n}\n\nstatic inline bool\nbpf_ctx_narrow_access_ok(u32 off, u32 size, u32 size_default)\n{\n\treturn size <= size_default && (size & (size - 1)) == 0;\n}\n\nstatic inline u8\nbpf_ctx_narrow_access_offset(u32 off, u32 size, u32 size_default)\n{\n\tu8 access_off = off & (size_default - 1);\n\n#ifdef __LITTLE_ENDIAN\n\treturn access_off;\n#else\n\treturn size_default - (access_off + size);\n#endif\n}\n\n#define bpf_ctx_wide_access_ok(off, size, type, field)\t\t\t\\\n\t(size == sizeof(__u64) &&\t\t\t\t\t\\\n\toff >= offsetof(type, field) &&\t\t\t\t\t\\\n\toff + sizeof(__u64) <= offsetofend(type, field) &&\t\t\\\n\toff % sizeof(__u64) == 0)\n\n#define bpf_classic_proglen(fprog) (fprog->len * sizeof(fprog->filter[0]))\n\nstatic inline void bpf_prog_lock_ro(struct bpf_prog *fp)\n{\n#ifndef CONFIG_BPF_JIT_ALWAYS_ON\n\tif (!fp->jited) {\n\t\tset_vm_flush_reset_perms(fp);\n\t\tset_memory_ro((unsigned long)fp, fp->pages);\n\t}\n#endif\n}\n\nstatic inline void bpf_jit_binary_lock_ro(struct bpf_binary_header *hdr)\n{\n\tset_vm_flush_reset_perms(hdr);\n\tset_memory_rox((unsigned long)hdr, hdr->size >> PAGE_SHIFT);\n}\n\nint sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int cap);\nstatic inline int sk_filter(struct sock *sk, struct sk_buff *skb)\n{\n\treturn sk_filter_trim_cap(sk, skb, 1);\n}\n\nstruct bpf_prog *bpf_prog_select_runtime(struct bpf_prog *fp, int *err);\nvoid bpf_prog_free(struct bpf_prog *fp);\n\nbool bpf_opcode_in_insntable(u8 code);\n\nvoid bpf_prog_fill_jited_linfo(struct bpf_prog *prog,\n\t\t\t       const u32 *insn_to_jit_off);\nint bpf_prog_alloc_jited_linfo(struct bpf_prog *prog);\nvoid bpf_prog_jit_attempt_done(struct bpf_prog *prog);\n\nstruct bpf_prog *bpf_prog_alloc(unsigned int size, gfp_t gfp_extra_flags);\nstruct bpf_prog *bpf_prog_alloc_no_stats(unsigned int size, gfp_t gfp_extra_flags);\nstruct bpf_prog *bpf_prog_realloc(struct bpf_prog *fp_old, unsigned int size,\n\t\t\t\t  gfp_t gfp_extra_flags);\nvoid __bpf_prog_free(struct bpf_prog *fp);\n\nstatic inline void bpf_prog_unlock_free(struct bpf_prog *fp)\n{\n\t__bpf_prog_free(fp);\n}\n\ntypedef int (*bpf_aux_classic_check_t)(struct sock_filter *filter,\n\t\t\t\t       unsigned int flen);\n\nint bpf_prog_create(struct bpf_prog **pfp, struct sock_fprog_kern *fprog);\nint bpf_prog_create_from_user(struct bpf_prog **pfp, struct sock_fprog *fprog,\n\t\t\t      bpf_aux_classic_check_t trans, bool save_orig);\nvoid bpf_prog_destroy(struct bpf_prog *fp);\n\nint sk_attach_filter(struct sock_fprog *fprog, struct sock *sk);\nint sk_attach_bpf(u32 ufd, struct sock *sk);\nint sk_reuseport_attach_filter(struct sock_fprog *fprog, struct sock *sk);\nint sk_reuseport_attach_bpf(u32 ufd, struct sock *sk);\nvoid sk_reuseport_prog_free(struct bpf_prog *prog);\nint sk_detach_filter(struct sock *sk);\nint sk_get_filter(struct sock *sk, sockptr_t optval, unsigned int len);\n\nbool sk_filter_charge(struct sock *sk, struct sk_filter *fp);\nvoid sk_filter_uncharge(struct sock *sk, struct sk_filter *fp);\n\nu64 __bpf_call_base(u64 r1, u64 r2, u64 r3, u64 r4, u64 r5);\n#define __bpf_call_base_args \\\n\t((u64 (*)(u64, u64, u64, u64, u64, const struct bpf_insn *)) \\\n\t (void *)__bpf_call_base)\n\nstruct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog);\nvoid bpf_jit_compile(struct bpf_prog *prog);\nbool bpf_jit_needs_zext(void);\nbool bpf_jit_supports_subprog_tailcalls(void);\nbool bpf_jit_supports_kfunc_call(void);\nbool bpf_jit_supports_far_kfunc_call(void);\nbool bpf_helper_changes_pkt_data(void *func);\n\nstatic inline bool bpf_dump_raw_ok(const struct cred *cred)\n{\n\t \n\treturn kallsyms_show_value(cred);\n}\n\nstruct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,\n\t\t\t\t       const struct bpf_insn *patch, u32 len);\nint bpf_remove_insns(struct bpf_prog *prog, u32 off, u32 cnt);\n\nvoid bpf_clear_redirect_map(struct bpf_map *map);\n\nstatic inline bool xdp_return_frame_no_direct(void)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\treturn ri->kern_flags & BPF_RI_F_RF_NO_DIRECT;\n}\n\nstatic inline void xdp_set_return_frame_no_direct(void)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tri->kern_flags |= BPF_RI_F_RF_NO_DIRECT;\n}\n\nstatic inline void xdp_clear_return_frame_no_direct(void)\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\n\tri->kern_flags &= ~BPF_RI_F_RF_NO_DIRECT;\n}\n\nstatic inline int xdp_ok_fwd_dev(const struct net_device *fwd,\n\t\t\t\t unsigned int pktlen)\n{\n\tunsigned int len;\n\n\tif (unlikely(!(fwd->flags & IFF_UP)))\n\t\treturn -ENETDOWN;\n\n\tlen = fwd->mtu + fwd->hard_header_len + VLAN_HLEN;\n\tif (pktlen > len)\n\t\treturn -EMSGSIZE;\n\n\treturn 0;\n}\n\n \nint xdp_do_generic_redirect(struct net_device *dev, struct sk_buff *skb,\n\t\t\t    struct xdp_buff *xdp, struct bpf_prog *prog);\nint xdp_do_redirect(struct net_device *dev,\n\t\t    struct xdp_buff *xdp,\n\t\t    struct bpf_prog *prog);\nint xdp_do_redirect_frame(struct net_device *dev,\n\t\t\t  struct xdp_buff *xdp,\n\t\t\t  struct xdp_frame *xdpf,\n\t\t\t  struct bpf_prog *prog);\nvoid xdp_do_flush(void);\n\n \n#define xdp_do_flush_map xdp_do_flush\n\nvoid bpf_warn_invalid_xdp_action(struct net_device *dev, struct bpf_prog *prog, u32 act);\n\n#ifdef CONFIG_INET\nstruct sock *bpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,\n\t\t\t\t  struct bpf_prog *prog, struct sk_buff *skb,\n\t\t\t\t  struct sock *migrating_sk,\n\t\t\t\t  u32 hash);\n#else\nstatic inline struct sock *\nbpf_run_sk_reuseport(struct sock_reuseport *reuse, struct sock *sk,\n\t\t     struct bpf_prog *prog, struct sk_buff *skb,\n\t\t     struct sock *migrating_sk,\n\t\t     u32 hash)\n{\n\treturn NULL;\n}\n#endif\n\n#ifdef CONFIG_BPF_JIT\nextern int bpf_jit_enable;\nextern int bpf_jit_harden;\nextern int bpf_jit_kallsyms;\nextern long bpf_jit_limit;\nextern long bpf_jit_limit_max;\n\ntypedef void (*bpf_jit_fill_hole_t)(void *area, unsigned int size);\n\nvoid bpf_jit_fill_hole_with_zero(void *area, unsigned int size);\n\nstruct bpf_binary_header *\nbpf_jit_binary_alloc(unsigned int proglen, u8 **image_ptr,\n\t\t     unsigned int alignment,\n\t\t     bpf_jit_fill_hole_t bpf_fill_ill_insns);\nvoid bpf_jit_binary_free(struct bpf_binary_header *hdr);\nu64 bpf_jit_alloc_exec_limit(void);\nvoid *bpf_jit_alloc_exec(unsigned long size);\nvoid bpf_jit_free_exec(void *addr);\nvoid bpf_jit_free(struct bpf_prog *fp);\nstruct bpf_binary_header *\nbpf_jit_binary_pack_hdr(const struct bpf_prog *fp);\n\nvoid *bpf_prog_pack_alloc(u32 size, bpf_jit_fill_hole_t bpf_fill_ill_insns);\nvoid bpf_prog_pack_free(struct bpf_binary_header *hdr);\n\nstatic inline bool bpf_prog_kallsyms_verify_off(const struct bpf_prog *fp)\n{\n\treturn list_empty(&fp->aux->ksym.lnode) ||\n\t       fp->aux->ksym.lnode.prev == LIST_POISON2;\n}\n\nstruct bpf_binary_header *\nbpf_jit_binary_pack_alloc(unsigned int proglen, u8 **ro_image,\n\t\t\t  unsigned int alignment,\n\t\t\t  struct bpf_binary_header **rw_hdr,\n\t\t\t  u8 **rw_image,\n\t\t\t  bpf_jit_fill_hole_t bpf_fill_ill_insns);\nint bpf_jit_binary_pack_finalize(struct bpf_prog *prog,\n\t\t\t\t struct bpf_binary_header *ro_header,\n\t\t\t\t struct bpf_binary_header *rw_header);\nvoid bpf_jit_binary_pack_free(struct bpf_binary_header *ro_header,\n\t\t\t      struct bpf_binary_header *rw_header);\n\nint bpf_jit_add_poke_descriptor(struct bpf_prog *prog,\n\t\t\t\tstruct bpf_jit_poke_descriptor *poke);\n\nint bpf_jit_get_func_addr(const struct bpf_prog *prog,\n\t\t\t  const struct bpf_insn *insn, bool extra_pass,\n\t\t\t  u64 *func_addr, bool *func_addr_fixed);\n\nstruct bpf_prog *bpf_jit_blind_constants(struct bpf_prog *fp);\nvoid bpf_jit_prog_release_other(struct bpf_prog *fp, struct bpf_prog *fp_other);\n\nstatic inline void bpf_jit_dump(unsigned int flen, unsigned int proglen,\n\t\t\t\tu32 pass, void *image)\n{\n\tpr_err(\"flen=%u proglen=%u pass=%u image=%pK from=%s pid=%d\\n\", flen,\n\t       proglen, pass, image, current->comm, task_pid_nr(current));\n\n\tif (image)\n\t\tprint_hex_dump(KERN_ERR, \"JIT code: \", DUMP_PREFIX_OFFSET,\n\t\t\t       16, 1, image, proglen, false);\n}\n\nstatic inline bool bpf_jit_is_ebpf(void)\n{\n# ifdef CONFIG_HAVE_EBPF_JIT\n\treturn true;\n# else\n\treturn false;\n# endif\n}\n\nstatic inline bool ebpf_jit_enabled(void)\n{\n\treturn bpf_jit_enable && bpf_jit_is_ebpf();\n}\n\nstatic inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)\n{\n\treturn fp->jited && bpf_jit_is_ebpf();\n}\n\nstatic inline bool bpf_jit_blinding_enabled(struct bpf_prog *prog)\n{\n\t \n\tif (!bpf_jit_is_ebpf())\n\t\treturn false;\n\tif (!prog->jit_requested)\n\t\treturn false;\n\tif (!bpf_jit_harden)\n\t\treturn false;\n\tif (bpf_jit_harden == 1 && bpf_capable())\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic inline bool bpf_jit_kallsyms_enabled(void)\n{\n\t \n\tif (bpf_jit_harden)\n\t\treturn false;\n\tif (!bpf_jit_kallsyms)\n\t\treturn false;\n\tif (bpf_jit_kallsyms == 1)\n\t\treturn true;\n\n\treturn false;\n}\n\nconst char *__bpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t\t\t unsigned long *off, char *sym);\nbool is_bpf_text_address(unsigned long addr);\nint bpf_get_kallsym(unsigned int symnum, unsigned long *value, char *type,\n\t\t    char *sym);\n\nstatic inline const char *\nbpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t   unsigned long *off, char **modname, char *sym)\n{\n\tconst char *ret = __bpf_address_lookup(addr, size, off, sym);\n\n\tif (ret && modname)\n\t\t*modname = NULL;\n\treturn ret;\n}\n\nvoid bpf_prog_kallsyms_add(struct bpf_prog *fp);\nvoid bpf_prog_kallsyms_del(struct bpf_prog *fp);\n\n#else  \n\nstatic inline bool ebpf_jit_enabled(void)\n{\n\treturn false;\n}\n\nstatic inline bool bpf_jit_blinding_enabled(struct bpf_prog *prog)\n{\n\treturn false;\n}\n\nstatic inline bool bpf_prog_ebpf_jited(const struct bpf_prog *fp)\n{\n\treturn false;\n}\n\nstatic inline int\nbpf_jit_add_poke_descriptor(struct bpf_prog *prog,\n\t\t\t    struct bpf_jit_poke_descriptor *poke)\n{\n\treturn -ENOTSUPP;\n}\n\nstatic inline void bpf_jit_free(struct bpf_prog *fp)\n{\n\tbpf_prog_unlock_free(fp);\n}\n\nstatic inline bool bpf_jit_kallsyms_enabled(void)\n{\n\treturn false;\n}\n\nstatic inline const char *\n__bpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t     unsigned long *off, char *sym)\n{\n\treturn NULL;\n}\n\nstatic inline bool is_bpf_text_address(unsigned long addr)\n{\n\treturn false;\n}\n\nstatic inline int bpf_get_kallsym(unsigned int symnum, unsigned long *value,\n\t\t\t\t  char *type, char *sym)\n{\n\treturn -ERANGE;\n}\n\nstatic inline const char *\nbpf_address_lookup(unsigned long addr, unsigned long *size,\n\t\t   unsigned long *off, char **modname, char *sym)\n{\n\treturn NULL;\n}\n\nstatic inline void bpf_prog_kallsyms_add(struct bpf_prog *fp)\n{\n}\n\nstatic inline void bpf_prog_kallsyms_del(struct bpf_prog *fp)\n{\n}\n\n#endif  \n\nvoid bpf_prog_kallsyms_del_all(struct bpf_prog *fp);\n\n#define BPF_ANC\t\tBIT(15)\n\nstatic inline bool bpf_needs_clear_a(const struct sock_filter *first)\n{\n\tswitch (first->code) {\n\tcase BPF_RET | BPF_K:\n\tcase BPF_LD | BPF_W | BPF_LEN:\n\t\treturn false;\n\n\tcase BPF_LD | BPF_W | BPF_ABS:\n\tcase BPF_LD | BPF_H | BPF_ABS:\n\tcase BPF_LD | BPF_B | BPF_ABS:\n\t\tif (first->k == SKF_AD_OFF + SKF_AD_ALU_XOR_X)\n\t\t\treturn true;\n\t\treturn false;\n\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic inline u16 bpf_anc_helper(const struct sock_filter *ftest)\n{\n\tBUG_ON(ftest->code & BPF_ANC);\n\n\tswitch (ftest->code) {\n\tcase BPF_LD | BPF_W | BPF_ABS:\n\tcase BPF_LD | BPF_H | BPF_ABS:\n\tcase BPF_LD | BPF_B | BPF_ABS:\n#define BPF_ANCILLARY(CODE)\tcase SKF_AD_OFF + SKF_AD_##CODE:\t\\\n\t\t\t\treturn BPF_ANC | SKF_AD_##CODE\n\t\tswitch (ftest->k) {\n\t\tBPF_ANCILLARY(PROTOCOL);\n\t\tBPF_ANCILLARY(PKTTYPE);\n\t\tBPF_ANCILLARY(IFINDEX);\n\t\tBPF_ANCILLARY(NLATTR);\n\t\tBPF_ANCILLARY(NLATTR_NEST);\n\t\tBPF_ANCILLARY(MARK);\n\t\tBPF_ANCILLARY(QUEUE);\n\t\tBPF_ANCILLARY(HATYPE);\n\t\tBPF_ANCILLARY(RXHASH);\n\t\tBPF_ANCILLARY(CPU);\n\t\tBPF_ANCILLARY(ALU_XOR_X);\n\t\tBPF_ANCILLARY(VLAN_TAG);\n\t\tBPF_ANCILLARY(VLAN_TAG_PRESENT);\n\t\tBPF_ANCILLARY(PAY_OFFSET);\n\t\tBPF_ANCILLARY(RANDOM);\n\t\tBPF_ANCILLARY(VLAN_TPID);\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\treturn ftest->code;\n\t}\n}\n\nvoid *bpf_internal_load_pointer_neg_helper(const struct sk_buff *skb,\n\t\t\t\t\t   int k, unsigned int size);\n\nstatic inline int bpf_tell_extensions(void)\n{\n\treturn SKF_AD_MAX;\n}\n\nstruct bpf_sock_addr_kern {\n\tstruct sock *sk;\n\tstruct sockaddr *uaddr;\n\t \n\tu64 tmp_reg;\n\tvoid *t_ctx;\t \n};\n\nstruct bpf_sock_ops_kern {\n\tstruct\tsock *sk;\n\tunion {\n\t\tu32 args[4];\n\t\tu32 reply;\n\t\tu32 replylong[4];\n\t};\n\tstruct sk_buff\t*syn_skb;\n\tstruct sk_buff\t*skb;\n\tvoid\t*skb_data_end;\n\tu8\top;\n\tu8\tis_fullsock;\n\tu8\tremaining_opt_len;\n\tu64\ttemp;\t\t\t \n};\n\nstruct bpf_sysctl_kern {\n\tstruct ctl_table_header *head;\n\tstruct ctl_table *table;\n\tvoid *cur_val;\n\tsize_t cur_len;\n\tvoid *new_val;\n\tsize_t new_len;\n\tint new_updated;\n\tint write;\n\tloff_t *ppos;\n\t \n\tu64 tmp_reg;\n};\n\n#define BPF_SOCKOPT_KERN_BUF_SIZE\t32\nstruct bpf_sockopt_buf {\n\tu8\t\tdata[BPF_SOCKOPT_KERN_BUF_SIZE];\n};\n\nstruct bpf_sockopt_kern {\n\tstruct sock\t*sk;\n\tu8\t\t*optval;\n\tu8\t\t*optval_end;\n\ts32\t\tlevel;\n\ts32\t\toptname;\n\ts32\t\toptlen;\n\t \n\tstruct task_struct *current_task;\n\t \n\tu64\t\ttmp_reg;\n};\n\nint copy_bpf_fprog_from_user(struct sock_fprog *dst, sockptr_t src, int len);\n\nstruct bpf_sk_lookup_kern {\n\tu16\t\tfamily;\n\tu16\t\tprotocol;\n\t__be16\t\tsport;\n\tu16\t\tdport;\n\tstruct {\n\t\t__be32 saddr;\n\t\t__be32 daddr;\n\t} v4;\n\tstruct {\n\t\tconst struct in6_addr *saddr;\n\t\tconst struct in6_addr *daddr;\n\t} v6;\n\tstruct sock\t*selected_sk;\n\tu32\t\tingress_ifindex;\n\tbool\t\tno_reuseport;\n};\n\nextern struct static_key_false bpf_sk_lookup_enabled;\n\n \n#define BPF_PROG_SK_LOOKUP_RUN_ARRAY(array, ctx, func)\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tstruct bpf_sk_lookup_kern *_ctx = &(ctx);\t\t\\\n\t\tstruct bpf_prog_array_item *_item;\t\t\t\\\n\t\tstruct sock *_selected_sk = NULL;\t\t\t\\\n\t\tbool _no_reuseport = false;\t\t\t\t\\\n\t\tstruct bpf_prog *_prog;\t\t\t\t\t\\\n\t\tbool _all_pass = true;\t\t\t\t\t\\\n\t\tu32 _ret;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tmigrate_disable();\t\t\t\t\t\\\n\t\t_item = &(array)->items[0];\t\t\t\t\\\n\t\twhile ((_prog = READ_ONCE(_item->prog))) {\t\t\\\n\t\t\t \t\t\\\n\t\t\t_ctx->selected_sk = _selected_sk;\t\t\\\n\t\t\t_ctx->no_reuseport = _no_reuseport;\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\t\t_ret = func(_prog, _ctx);\t\t\t\\\n\t\t\tif (_ret == SK_PASS && _ctx->selected_sk) {\t\\\n\t\t\t\t \t\\\n\t\t\t\t_selected_sk = _ctx->selected_sk;\t\\\n\t\t\t\t_no_reuseport = _ctx->no_reuseport;\t\\\n\t\t\t} else if (_ret == SK_DROP && _all_pass) {\t\\\n\t\t\t\t_all_pass = false;\t\t\t\\\n\t\t\t}\t\t\t\t\t\t\\\n\t\t\t_item++;\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t_ctx->selected_sk = _selected_sk;\t\t\t\\\n\t\t_ctx->no_reuseport = _no_reuseport;\t\t\t\\\n\t\tmigrate_enable();\t\t\t\t\t\\\n\t\t_all_pass || _selected_sk ? SK_PASS : SK_DROP;\t\t\\\n\t })\n\nstatic inline bool bpf_sk_lookup_run_v4(struct net *net, int protocol,\n\t\t\t\t\tconst __be32 saddr, const __be16 sport,\n\t\t\t\t\tconst __be32 daddr, const u16 dport,\n\t\t\t\t\tconst int ifindex, struct sock **psk)\n{\n\tstruct bpf_prog_array *run_array;\n\tstruct sock *selected_sk = NULL;\n\tbool no_reuseport = false;\n\n\trcu_read_lock();\n\trun_array = rcu_dereference(net->bpf.run_array[NETNS_BPF_SK_LOOKUP]);\n\tif (run_array) {\n\t\tstruct bpf_sk_lookup_kern ctx = {\n\t\t\t.family\t\t= AF_INET,\n\t\t\t.protocol\t= protocol,\n\t\t\t.v4.saddr\t= saddr,\n\t\t\t.v4.daddr\t= daddr,\n\t\t\t.sport\t\t= sport,\n\t\t\t.dport\t\t= dport,\n\t\t\t.ingress_ifindex\t= ifindex,\n\t\t};\n\t\tu32 act;\n\n\t\tact = BPF_PROG_SK_LOOKUP_RUN_ARRAY(run_array, ctx, bpf_prog_run);\n\t\tif (act == SK_PASS) {\n\t\t\tselected_sk = ctx.selected_sk;\n\t\t\tno_reuseport = ctx.no_reuseport;\n\t\t} else {\n\t\t\tselected_sk = ERR_PTR(-ECONNREFUSED);\n\t\t}\n\t}\n\trcu_read_unlock();\n\t*psk = selected_sk;\n\treturn no_reuseport;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic inline bool bpf_sk_lookup_run_v6(struct net *net, int protocol,\n\t\t\t\t\tconst struct in6_addr *saddr,\n\t\t\t\t\tconst __be16 sport,\n\t\t\t\t\tconst struct in6_addr *daddr,\n\t\t\t\t\tconst u16 dport,\n\t\t\t\t\tconst int ifindex, struct sock **psk)\n{\n\tstruct bpf_prog_array *run_array;\n\tstruct sock *selected_sk = NULL;\n\tbool no_reuseport = false;\n\n\trcu_read_lock();\n\trun_array = rcu_dereference(net->bpf.run_array[NETNS_BPF_SK_LOOKUP]);\n\tif (run_array) {\n\t\tstruct bpf_sk_lookup_kern ctx = {\n\t\t\t.family\t\t= AF_INET6,\n\t\t\t.protocol\t= protocol,\n\t\t\t.v6.saddr\t= saddr,\n\t\t\t.v6.daddr\t= daddr,\n\t\t\t.sport\t\t= sport,\n\t\t\t.dport\t\t= dport,\n\t\t\t.ingress_ifindex\t= ifindex,\n\t\t};\n\t\tu32 act;\n\n\t\tact = BPF_PROG_SK_LOOKUP_RUN_ARRAY(run_array, ctx, bpf_prog_run);\n\t\tif (act == SK_PASS) {\n\t\t\tselected_sk = ctx.selected_sk;\n\t\t\tno_reuseport = ctx.no_reuseport;\n\t\t} else {\n\t\t\tselected_sk = ERR_PTR(-ECONNREFUSED);\n\t\t}\n\t}\n\trcu_read_unlock();\n\t*psk = selected_sk;\n\treturn no_reuseport;\n}\n#endif  \n\nstatic __always_inline long __bpf_xdp_redirect_map(struct bpf_map *map, u64 index,\n\t\t\t\t\t\t   u64 flags, const u64 flag_mask,\n\t\t\t\t\t\t   void *lookup_elem(struct bpf_map *map, u32 key))\n{\n\tstruct bpf_redirect_info *ri = this_cpu_ptr(&bpf_redirect_info);\n\tconst u64 action_mask = XDP_ABORTED | XDP_DROP | XDP_PASS | XDP_TX;\n\n\t \n\tif (unlikely(flags & ~(action_mask | flag_mask)))\n\t\treturn XDP_ABORTED;\n\n\tri->tgt_value = lookup_elem(map, index);\n\tif (unlikely(!ri->tgt_value) && !(flags & BPF_F_BROADCAST)) {\n\t\t \n\t\tri->map_id = INT_MAX;  \n\t\tri->map_type = BPF_MAP_TYPE_UNSPEC;\n\t\treturn flags & action_mask;\n\t}\n\n\tri->tgt_index = index;\n\tri->map_id = map->id;\n\tri->map_type = map->map_type;\n\n\tif (flags & BPF_F_BROADCAST) {\n\t\tWRITE_ONCE(ri->map, map);\n\t\tri->flags = flags;\n\t} else {\n\t\tWRITE_ONCE(ri->map, NULL);\n\t\tri->flags = 0;\n\t}\n\n\treturn XDP_REDIRECT;\n}\n\n#ifdef CONFIG_NET\nint __bpf_skb_load_bytes(const struct sk_buff *skb, u32 offset, void *to, u32 len);\nint __bpf_skb_store_bytes(struct sk_buff *skb, u32 offset, const void *from,\n\t\t\t  u32 len, u64 flags);\nint __bpf_xdp_load_bytes(struct xdp_buff *xdp, u32 offset, void *buf, u32 len);\nint __bpf_xdp_store_bytes(struct xdp_buff *xdp, u32 offset, void *buf, u32 len);\nvoid *bpf_xdp_pointer(struct xdp_buff *xdp, u32 offset, u32 len);\nvoid bpf_xdp_copy_buf(struct xdp_buff *xdp, unsigned long off,\n\t\t      void *buf, unsigned long len, bool flush);\n#else  \nstatic inline int __bpf_skb_load_bytes(const struct sk_buff *skb, u32 offset,\n\t\t\t\t       void *to, u32 len)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int __bpf_skb_store_bytes(struct sk_buff *skb, u32 offset,\n\t\t\t\t\tconst void *from, u32 len, u64 flags)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int __bpf_xdp_load_bytes(struct xdp_buff *xdp, u32 offset,\n\t\t\t\t       void *buf, u32 len)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline int __bpf_xdp_store_bytes(struct xdp_buff *xdp, u32 offset,\n\t\t\t\t\tvoid *buf, u32 len)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void *bpf_xdp_pointer(struct xdp_buff *xdp, u32 offset, u32 len)\n{\n\treturn NULL;\n}\n\nstatic inline void bpf_xdp_copy_buf(struct xdp_buff *xdp, unsigned long off, void *buf,\n\t\t\t\t    unsigned long len, bool flush)\n{\n}\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}