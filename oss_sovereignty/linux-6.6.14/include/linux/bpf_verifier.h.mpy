{
  "module_name": "bpf_verifier.h",
  "hash_id": "c57e470e7af7451ebe7f4838eba2ac235008f7975b13011938cb4b57d64b3f55",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/bpf_verifier.h",
  "human_readable_source": " \n \n#ifndef _LINUX_BPF_VERIFIER_H\n#define _LINUX_BPF_VERIFIER_H 1\n\n#include <linux/bpf.h>  \n#include <linux/btf.h>  \n#include <linux/filter.h>  \n#include <linux/tnum.h>\n\n \n#define BPF_MAX_VAR_OFF\t(1 << 29)\n \n#define BPF_MAX_VAR_SIZ\t(1 << 29)\n \n#define TMP_STR_BUF_LEN 320\n\n \nenum bpf_reg_liveness {\n\tREG_LIVE_NONE = 0,  \n\tREG_LIVE_READ32 = 0x1,  \n\tREG_LIVE_READ64 = 0x2,  \n\tREG_LIVE_READ = REG_LIVE_READ32 | REG_LIVE_READ64,\n\tREG_LIVE_WRITTEN = 0x4,  \n\tREG_LIVE_DONE = 0x8,  \n};\n\n \nstruct bpf_active_lock {\n\t \n\tvoid *ptr;\n\t \n\tu32 id;\n};\n\n#define ITER_PREFIX \"bpf_iter_\"\n\nenum bpf_iter_state {\n\tBPF_ITER_STATE_INVALID,  \n\tBPF_ITER_STATE_ACTIVE,\n\tBPF_ITER_STATE_DRAINED,\n};\n\nstruct bpf_reg_state {\n\t \n\tenum bpf_reg_type type;\n\t \n\ts32 off;\n\tunion {\n\t\t \n\t\tint range;\n\n\t\t \n\t\tstruct {\n\t\t\tstruct bpf_map *map_ptr;\n\t\t\t \n\t\t\tu32 map_uid;\n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\tstruct btf *btf;\n\t\t\tu32 btf_id;\n\t\t};\n\n\t\tstruct {  \n\t\t\tu32 mem_size;\n\t\t\tu32 dynptr_id;  \n\t\t};\n\n\t\t \n\t\tstruct {\n\t\t\tenum bpf_dynptr_type type;\n\t\t\t \n\t\t\tbool first_slot;\n\t\t} dynptr;\n\n\t\t \n\t\tstruct {\n\t\t\t \n\t\t\tstruct btf *btf;\n\t\t\tu32 btf_id;\n\t\t\t \n\t\t\tenum bpf_iter_state state:2;\n\t\t\tint depth:30;\n\t\t} iter;\n\n\t\t \n\t\tstruct {\n\t\t\tunsigned long raw1;\n\t\t\tunsigned long raw2;\n\t\t} raw;\n\n\t\tu32 subprogno;  \n\t};\n\t \n\tstruct tnum var_off;\n\t \n\ts64 smin_value;  \n\ts64 smax_value;  \n\tu64 umin_value;  \n\tu64 umax_value;  \n\ts32 s32_min_value;  \n\ts32 s32_max_value;  \n\tu32 u32_min_value;  \n\tu32 u32_max_value;  \n\t \n\tu32 id;\n\t \n\tu32 ref_obj_id;\n\t \n\tstruct bpf_reg_state *parent;\n\t \n\tu32 frameno;\n\t \n\ts32 subreg_def;\n\tenum bpf_reg_liveness live;\n\t \n\tbool precise;\n};\n\nenum bpf_stack_slot_type {\n\tSTACK_INVALID,     \n\tSTACK_SPILL,       \n\tSTACK_MISC,\t   \n\tSTACK_ZERO,\t   \n\t \n\tSTACK_DYNPTR,\n\tSTACK_ITER,\n};\n\n#define BPF_REG_SIZE 8\t \n\n#define BPF_REGMASK_ARGS ((1 << BPF_REG_1) | (1 << BPF_REG_2) | \\\n\t\t\t  (1 << BPF_REG_3) | (1 << BPF_REG_4) | \\\n\t\t\t  (1 << BPF_REG_5))\n\n#define BPF_DYNPTR_SIZE\t\tsizeof(struct bpf_dynptr_kern)\n#define BPF_DYNPTR_NR_SLOTS\t\t(BPF_DYNPTR_SIZE / BPF_REG_SIZE)\n\nstruct bpf_stack_state {\n\tstruct bpf_reg_state spilled_ptr;\n\tu8 slot_type[BPF_REG_SIZE];\n};\n\nstruct bpf_reference_state {\n\t \n\tint id;\n\t \n\tint insn_idx;\n\t \n\tint callback_ref;\n};\n\n \nstruct bpf_func_state {\n\tstruct bpf_reg_state regs[MAX_BPF_REG];\n\t \n\tint callsite;\n\t \n\tu32 frameno;\n\t \n\tu32 subprogno;\n\t \n\tu32 async_entry_cnt;\n\tbool in_callback_fn;\n\tstruct tnum callback_ret_range;\n\tbool in_async_callback_fn;\n\n\t \n\tint acquired_refs;\n\tstruct bpf_reference_state *refs;\n\tint allocated_stack;\n\tstruct bpf_stack_state *stack;\n};\n\nstruct bpf_idx_pair {\n\tu32 prev_idx;\n\tu32 idx;\n};\n\n#define MAX_CALL_FRAMES 8\n \n#define BPF_ID_MAP_SIZE ((MAX_BPF_REG + MAX_BPF_STACK / BPF_REG_SIZE) * MAX_CALL_FRAMES)\nstruct bpf_verifier_state {\n\t \n\tstruct bpf_func_state *frame[MAX_CALL_FRAMES];\n\tstruct bpf_verifier_state *parent;\n\t \n\tu32 branches;\n\tu32 insn_idx;\n\tu32 curframe;\n\n\tstruct bpf_active_lock active_lock;\n\tbool speculative;\n\tbool active_rcu_lock;\n\n\t \n\tu32 first_insn_idx;\n\tu32 last_insn_idx;\n\t \n\tstruct bpf_idx_pair *jmp_history;\n\tu32 jmp_history_cnt;\n};\n\n#define bpf_get_spilled_reg(slot, frame)\t\t\t\t\\\n\t(((slot < frame->allocated_stack / BPF_REG_SIZE) &&\t\t\\\n\t  (frame->stack[slot].slot_type[0] == STACK_SPILL))\t\t\\\n\t ? &frame->stack[slot].spilled_ptr : NULL)\n\n \n#define bpf_for_each_spilled_reg(iter, frame, reg)\t\t\t\\\n\tfor (iter = 0, reg = bpf_get_spilled_reg(iter, frame);\t\t\\\n\t     iter < frame->allocated_stack / BPF_REG_SIZE;\t\t\\\n\t     iter++, reg = bpf_get_spilled_reg(iter, frame))\n\n \n#define bpf_for_each_reg_in_vstate(__vst, __state, __reg, __expr)   \\\n\t({                                                               \\\n\t\tstruct bpf_verifier_state *___vstate = __vst;            \\\n\t\tint ___i, ___j;                                          \\\n\t\tfor (___i = 0; ___i <= ___vstate->curframe; ___i++) {    \\\n\t\t\tstruct bpf_reg_state *___regs;                   \\\n\t\t\t__state = ___vstate->frame[___i];                \\\n\t\t\t___regs = __state->regs;                         \\\n\t\t\tfor (___j = 0; ___j < MAX_BPF_REG; ___j++) {     \\\n\t\t\t\t__reg = &___regs[___j];                  \\\n\t\t\t\t(void)(__expr);                          \\\n\t\t\t}                                                \\\n\t\t\tbpf_for_each_spilled_reg(___j, __state, __reg) { \\\n\t\t\t\tif (!__reg)                              \\\n\t\t\t\t\tcontinue;                        \\\n\t\t\t\t(void)(__expr);                          \\\n\t\t\t}                                                \\\n\t\t}                                                        \\\n\t})\n\n \nstruct bpf_verifier_state_list {\n\tstruct bpf_verifier_state state;\n\tstruct bpf_verifier_state_list *next;\n\tint miss_cnt, hit_cnt;\n};\n\nstruct bpf_loop_inline_state {\n\tunsigned int initialized:1;  \n\tunsigned int fit_for_inline:1;  \n\tu32 callback_subprogno;  \n};\n\n \n#define BPF_ALU_SANITIZE_SRC\t\t(1U << 0)\n#define BPF_ALU_SANITIZE_DST\t\t(1U << 1)\n#define BPF_ALU_NEG_VALUE\t\t(1U << 2)\n#define BPF_ALU_NON_POINTER\t\t(1U << 3)\n#define BPF_ALU_IMMEDIATE\t\t(1U << 4)\n#define BPF_ALU_SANITIZE\t\t(BPF_ALU_SANITIZE_SRC | \\\n\t\t\t\t\t BPF_ALU_SANITIZE_DST)\n\nstruct bpf_insn_aux_data {\n\tunion {\n\t\tenum bpf_reg_type ptr_type;\t \n\t\tunsigned long map_ptr_state;\t \n\t\ts32 call_imm;\t\t\t \n\t\tu32 alu_limit;\t\t\t \n\t\tstruct {\n\t\t\tu32 map_index;\t\t \n\t\t\tu32 map_off;\t\t \n\t\t};\n\t\tstruct {\n\t\t\tenum bpf_reg_type reg_type;\t \n\t\t\tunion {\n\t\t\t\tstruct {\n\t\t\t\t\tstruct btf *btf;\n\t\t\t\t\tu32 btf_id;\t \n\t\t\t\t};\n\t\t\t\tu32 mem_size;\t \n\t\t\t};\n\t\t} btf_var;\n\t\t \n\t\tstruct bpf_loop_inline_state loop_inline_state;\n\t};\n\tunion {\n\t\t \n\t\tu64 obj_new_size;\n\t\t \n\t\tu64 insert_off;\n\t};\n\tstruct btf_struct_meta *kptr_struct_meta;\n\tu64 map_key_state;  \n\tint ctx_field_size;  \n\tu32 seen;  \n\tbool sanitize_stack_spill;  \n\tbool zext_dst;  \n\tbool storage_get_func_atomic;  \n\tbool is_iter_next;  \n\tu8 alu_state;  \n\n\t \n\tunsigned int orig_idx;  \n\tbool jmp_point;\n\tbool prune_point;\n\t \n\tbool force_checkpoint;\n};\n\n#define MAX_USED_MAPS 64  \n#define MAX_USED_BTFS 64  \n\n#define BPF_VERIFIER_TMP_LOG_SIZE\t1024\n\nstruct bpf_verifier_log {\n\t \n\tu64 start_pos;\n\tu64 end_pos;\n\tchar __user *ubuf;\n\tu32 level;\n\tu32 len_total;\n\tu32 len_max;\n\tchar kbuf[BPF_VERIFIER_TMP_LOG_SIZE];\n};\n\n#define BPF_LOG_LEVEL1\t1\n#define BPF_LOG_LEVEL2\t2\n#define BPF_LOG_STATS\t4\n#define BPF_LOG_FIXED\t8\n#define BPF_LOG_LEVEL\t(BPF_LOG_LEVEL1 | BPF_LOG_LEVEL2)\n#define BPF_LOG_MASK\t(BPF_LOG_LEVEL | BPF_LOG_STATS | BPF_LOG_FIXED)\n#define BPF_LOG_KERNEL\t(BPF_LOG_MASK + 1)  \n#define BPF_LOG_MIN_ALIGNMENT 8U\n#define BPF_LOG_ALIGNMENT 40U\n\nstatic inline bool bpf_verifier_log_needed(const struct bpf_verifier_log *log)\n{\n\treturn log && log->level;\n}\n\n#define BPF_MAX_SUBPROGS 256\n\nstruct bpf_subprog_info {\n\t \n\tu32 start;  \n\tu32 linfo_idx;  \n\tu16 stack_depth;  \n\tbool has_tail_call;\n\tbool tail_call_reachable;\n\tbool has_ld_abs;\n\tbool is_async_cb;\n};\n\nstruct bpf_verifier_env;\n\nstruct backtrack_state {\n\tstruct bpf_verifier_env *env;\n\tu32 frame;\n\tu32 reg_masks[MAX_CALL_FRAMES];\n\tu64 stack_masks[MAX_CALL_FRAMES];\n};\n\nstruct bpf_id_pair {\n\tu32 old;\n\tu32 cur;\n};\n\nstruct bpf_idmap {\n\tu32 tmp_id_gen;\n\tstruct bpf_id_pair map[BPF_ID_MAP_SIZE];\n};\n\nstruct bpf_idset {\n\tu32 count;\n\tu32 ids[BPF_ID_MAP_SIZE];\n};\n\n \nstruct bpf_verifier_env {\n\tu32 insn_idx;\n\tu32 prev_insn_idx;\n\tstruct bpf_prog *prog;\t\t \n\tconst struct bpf_verifier_ops *ops;\n\tstruct bpf_verifier_stack_elem *head;  \n\tint stack_size;\t\t\t \n\tbool strict_alignment;\t\t \n\tbool test_state_freq;\t\t \n\tstruct bpf_verifier_state *cur_state;  \n\tstruct bpf_verifier_state_list **explored_states;  \n\tstruct bpf_verifier_state_list *free_list;\n\tstruct bpf_map *used_maps[MAX_USED_MAPS];  \n\tstruct btf_mod_pair used_btfs[MAX_USED_BTFS];  \n\tu32 used_map_cnt;\t\t \n\tu32 used_btf_cnt;\t\t \n\tu32 id_gen;\t\t\t \n\tbool explore_alu_limits;\n\tbool allow_ptr_leaks;\n\tbool allow_uninit_stack;\n\tbool bpf_capable;\n\tbool bypass_spec_v1;\n\tbool bypass_spec_v4;\n\tbool seen_direct_write;\n\tstruct bpf_insn_aux_data *insn_aux_data;  \n\tconst struct bpf_line_info *prev_linfo;\n\tstruct bpf_verifier_log log;\n\tstruct bpf_subprog_info subprog_info[BPF_MAX_SUBPROGS + 1];\n\tunion {\n\t\tstruct bpf_idmap idmap_scratch;\n\t\tstruct bpf_idset idset_scratch;\n\t};\n\tstruct {\n\t\tint *insn_state;\n\t\tint *insn_stack;\n\t\tint cur_stack;\n\t} cfg;\n\tstruct backtrack_state bt;\n\tu32 pass_cnt;  \n\tu32 subprog_cnt;\n\t \n\tu32 prev_insn_processed, insn_processed;\n\t \n\tu32 prev_jmps_processed, jmps_processed;\n\t \n\tu64 verification_time;\n\t \n\tu32 max_states_per_insn;\n\t \n\tu32 total_states;\n\t \n\tu32 peak_states;\n\t \n\tu32 longest_mark_read_walk;\n\tbpfptr_t fd_array;\n\n\t \n\tu32 scratched_regs;\n\t \n\tu64 scratched_stack_slots;\n\tu64 prev_log_pos, prev_insn_print_pos;\n\t \n\tchar tmp_str_buf[TMP_STR_BUF_LEN];\n};\n\n__printf(2, 0) void bpf_verifier_vlog(struct bpf_verifier_log *log,\n\t\t\t\t      const char *fmt, va_list args);\n__printf(2, 3) void bpf_verifier_log_write(struct bpf_verifier_env *env,\n\t\t\t\t\t   const char *fmt, ...);\n__printf(2, 3) void bpf_log(struct bpf_verifier_log *log,\n\t\t\t    const char *fmt, ...);\nint bpf_vlog_init(struct bpf_verifier_log *log, u32 log_level,\n\t\t  char __user *log_buf, u32 log_size);\nvoid bpf_vlog_reset(struct bpf_verifier_log *log, u64 new_pos);\nint bpf_vlog_finalize(struct bpf_verifier_log *log, u32 *log_size_actual);\n\nstatic inline struct bpf_func_state *cur_func(struct bpf_verifier_env *env)\n{\n\tstruct bpf_verifier_state *cur = env->cur_state;\n\n\treturn cur->frame[cur->curframe];\n}\n\nstatic inline struct bpf_reg_state *cur_regs(struct bpf_verifier_env *env)\n{\n\treturn cur_func(env)->regs;\n}\n\nint bpf_prog_offload_verifier_prep(struct bpf_prog *prog);\nint bpf_prog_offload_verify_insn(struct bpf_verifier_env *env,\n\t\t\t\t int insn_idx, int prev_insn_idx);\nint bpf_prog_offload_finalize(struct bpf_verifier_env *env);\nvoid\nbpf_prog_offload_replace_insn(struct bpf_verifier_env *env, u32 off,\n\t\t\t      struct bpf_insn *insn);\nvoid\nbpf_prog_offload_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt);\n\nint check_ptr_off_reg(struct bpf_verifier_env *env,\n\t\t      const struct bpf_reg_state *reg, int regno);\nint check_func_arg_reg_off(struct bpf_verifier_env *env,\n\t\t\t   const struct bpf_reg_state *reg, int regno,\n\t\t\t   enum bpf_arg_type arg_type);\nint check_mem_reg(struct bpf_verifier_env *env, struct bpf_reg_state *reg,\n\t\t   u32 regno, u32 mem_size);\n\n \nstatic inline u64 bpf_trampoline_compute_key(const struct bpf_prog *tgt_prog,\n\t\t\t\t\t     struct btf *btf, u32 btf_id)\n{\n\tif (tgt_prog)\n\t\treturn ((u64)tgt_prog->aux->id << 32) | btf_id;\n\telse\n\t\treturn ((u64)btf_obj_id(btf) << 32) | 0x80000000 | btf_id;\n}\n\n \nstatic inline void bpf_trampoline_unpack_key(u64 key, u32 *obj_id, u32 *btf_id)\n{\n\tif (obj_id)\n\t\t*obj_id = key >> 32;\n\tif (btf_id)\n\t\t*btf_id = key & 0x7FFFFFFF;\n}\n\nint bpf_check_attach_target(struct bpf_verifier_log *log,\n\t\t\t    const struct bpf_prog *prog,\n\t\t\t    const struct bpf_prog *tgt_prog,\n\t\t\t    u32 btf_id,\n\t\t\t    struct bpf_attach_target_info *tgt_info);\nvoid bpf_free_kfunc_btf_tab(struct bpf_kfunc_btf_tab *tab);\n\nint mark_chain_precision(struct bpf_verifier_env *env, int regno);\n\n#define BPF_BASE_TYPE_MASK\tGENMASK(BPF_BASE_TYPE_BITS - 1, 0)\n\n \nstatic inline u32 base_type(u32 type)\n{\n\treturn type & BPF_BASE_TYPE_MASK;\n}\n\n \nstatic inline u32 type_flag(u32 type)\n{\n\treturn type & ~BPF_BASE_TYPE_MASK;\n}\n\n \nstatic inline enum bpf_prog_type resolve_prog_type(const struct bpf_prog *prog)\n{\n\treturn prog->type == BPF_PROG_TYPE_EXT ?\n\t\tprog->aux->dst_prog->type : prog->type;\n}\n\nstatic inline bool bpf_prog_check_recur(const struct bpf_prog *prog)\n{\n\tswitch (resolve_prog_type(prog)) {\n\tcase BPF_PROG_TYPE_TRACING:\n\t\treturn prog->expected_attach_type != BPF_TRACE_ITER;\n\tcase BPF_PROG_TYPE_STRUCT_OPS:\n\tcase BPF_PROG_TYPE_LSM:\n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n#define BPF_REG_TRUSTED_MODIFIERS (MEM_ALLOC | PTR_TRUSTED | NON_OWN_REF)\n\nstatic inline bool bpf_type_has_unsafe_modifiers(u32 type)\n{\n\treturn type_flag(type) & ~BPF_REG_TRUSTED_MODIFIERS;\n}\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}