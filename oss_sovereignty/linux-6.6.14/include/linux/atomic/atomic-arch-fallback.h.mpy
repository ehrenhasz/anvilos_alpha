{
  "module_name": "atomic-arch-fallback.h",
  "hash_id": "81b7e3b7ac4a1915d86ccaff0fd89108fedf79bf758b2877c512b75fa47ae6af",
  "original_prompt": "Ingested from linux-6.6.14/include/linux/atomic/atomic-arch-fallback.h",
  "human_readable_source": "\n\n\n\n\n#ifndef _LINUX_ATOMIC_FALLBACK_H\n#define _LINUX_ATOMIC_FALLBACK_H\n\n#include <linux/compiler.h>\n\n#if defined(arch_xchg)\n#define raw_xchg arch_xchg\n#elif defined(arch_xchg_relaxed)\n#define raw_xchg(...) \\\n\t__atomic_op_fence(arch_xchg, __VA_ARGS__)\n#else\nextern void raw_xchg_not_implemented(void);\n#define raw_xchg(...) raw_xchg_not_implemented()\n#endif\n\n#if defined(arch_xchg_acquire)\n#define raw_xchg_acquire arch_xchg_acquire\n#elif defined(arch_xchg_relaxed)\n#define raw_xchg_acquire(...) \\\n\t__atomic_op_acquire(arch_xchg, __VA_ARGS__)\n#elif defined(arch_xchg)\n#define raw_xchg_acquire arch_xchg\n#else\nextern void raw_xchg_acquire_not_implemented(void);\n#define raw_xchg_acquire(...) raw_xchg_acquire_not_implemented()\n#endif\n\n#if defined(arch_xchg_release)\n#define raw_xchg_release arch_xchg_release\n#elif defined(arch_xchg_relaxed)\n#define raw_xchg_release(...) \\\n\t__atomic_op_release(arch_xchg, __VA_ARGS__)\n#elif defined(arch_xchg)\n#define raw_xchg_release arch_xchg\n#else\nextern void raw_xchg_release_not_implemented(void);\n#define raw_xchg_release(...) raw_xchg_release_not_implemented()\n#endif\n\n#if defined(arch_xchg_relaxed)\n#define raw_xchg_relaxed arch_xchg_relaxed\n#elif defined(arch_xchg)\n#define raw_xchg_relaxed arch_xchg\n#else\nextern void raw_xchg_relaxed_not_implemented(void);\n#define raw_xchg_relaxed(...) raw_xchg_relaxed_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg)\n#define raw_cmpxchg arch_cmpxchg\n#elif defined(arch_cmpxchg_relaxed)\n#define raw_cmpxchg(...) \\\n\t__atomic_op_fence(arch_cmpxchg, __VA_ARGS__)\n#else\nextern void raw_cmpxchg_not_implemented(void);\n#define raw_cmpxchg(...) raw_cmpxchg_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg_acquire)\n#define raw_cmpxchg_acquire arch_cmpxchg_acquire\n#elif defined(arch_cmpxchg_relaxed)\n#define raw_cmpxchg_acquire(...) \\\n\t__atomic_op_acquire(arch_cmpxchg, __VA_ARGS__)\n#elif defined(arch_cmpxchg)\n#define raw_cmpxchg_acquire arch_cmpxchg\n#else\nextern void raw_cmpxchg_acquire_not_implemented(void);\n#define raw_cmpxchg_acquire(...) raw_cmpxchg_acquire_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg_release)\n#define raw_cmpxchg_release arch_cmpxchg_release\n#elif defined(arch_cmpxchg_relaxed)\n#define raw_cmpxchg_release(...) \\\n\t__atomic_op_release(arch_cmpxchg, __VA_ARGS__)\n#elif defined(arch_cmpxchg)\n#define raw_cmpxchg_release arch_cmpxchg\n#else\nextern void raw_cmpxchg_release_not_implemented(void);\n#define raw_cmpxchg_release(...) raw_cmpxchg_release_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg_relaxed)\n#define raw_cmpxchg_relaxed arch_cmpxchg_relaxed\n#elif defined(arch_cmpxchg)\n#define raw_cmpxchg_relaxed arch_cmpxchg\n#else\nextern void raw_cmpxchg_relaxed_not_implemented(void);\n#define raw_cmpxchg_relaxed(...) raw_cmpxchg_relaxed_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg64)\n#define raw_cmpxchg64 arch_cmpxchg64\n#elif defined(arch_cmpxchg64_relaxed)\n#define raw_cmpxchg64(...) \\\n\t__atomic_op_fence(arch_cmpxchg64, __VA_ARGS__)\n#else\nextern void raw_cmpxchg64_not_implemented(void);\n#define raw_cmpxchg64(...) raw_cmpxchg64_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg64_acquire)\n#define raw_cmpxchg64_acquire arch_cmpxchg64_acquire\n#elif defined(arch_cmpxchg64_relaxed)\n#define raw_cmpxchg64_acquire(...) \\\n\t__atomic_op_acquire(arch_cmpxchg64, __VA_ARGS__)\n#elif defined(arch_cmpxchg64)\n#define raw_cmpxchg64_acquire arch_cmpxchg64\n#else\nextern void raw_cmpxchg64_acquire_not_implemented(void);\n#define raw_cmpxchg64_acquire(...) raw_cmpxchg64_acquire_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg64_release)\n#define raw_cmpxchg64_release arch_cmpxchg64_release\n#elif defined(arch_cmpxchg64_relaxed)\n#define raw_cmpxchg64_release(...) \\\n\t__atomic_op_release(arch_cmpxchg64, __VA_ARGS__)\n#elif defined(arch_cmpxchg64)\n#define raw_cmpxchg64_release arch_cmpxchg64\n#else\nextern void raw_cmpxchg64_release_not_implemented(void);\n#define raw_cmpxchg64_release(...) raw_cmpxchg64_release_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg64_relaxed)\n#define raw_cmpxchg64_relaxed arch_cmpxchg64_relaxed\n#elif defined(arch_cmpxchg64)\n#define raw_cmpxchg64_relaxed arch_cmpxchg64\n#else\nextern void raw_cmpxchg64_relaxed_not_implemented(void);\n#define raw_cmpxchg64_relaxed(...) raw_cmpxchg64_relaxed_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg128)\n#define raw_cmpxchg128 arch_cmpxchg128\n#elif defined(arch_cmpxchg128_relaxed)\n#define raw_cmpxchg128(...) \\\n\t__atomic_op_fence(arch_cmpxchg128, __VA_ARGS__)\n#else\nextern void raw_cmpxchg128_not_implemented(void);\n#define raw_cmpxchg128(...) raw_cmpxchg128_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg128_acquire)\n#define raw_cmpxchg128_acquire arch_cmpxchg128_acquire\n#elif defined(arch_cmpxchg128_relaxed)\n#define raw_cmpxchg128_acquire(...) \\\n\t__atomic_op_acquire(arch_cmpxchg128, __VA_ARGS__)\n#elif defined(arch_cmpxchg128)\n#define raw_cmpxchg128_acquire arch_cmpxchg128\n#else\nextern void raw_cmpxchg128_acquire_not_implemented(void);\n#define raw_cmpxchg128_acquire(...) raw_cmpxchg128_acquire_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg128_release)\n#define raw_cmpxchg128_release arch_cmpxchg128_release\n#elif defined(arch_cmpxchg128_relaxed)\n#define raw_cmpxchg128_release(...) \\\n\t__atomic_op_release(arch_cmpxchg128, __VA_ARGS__)\n#elif defined(arch_cmpxchg128)\n#define raw_cmpxchg128_release arch_cmpxchg128\n#else\nextern void raw_cmpxchg128_release_not_implemented(void);\n#define raw_cmpxchg128_release(...) raw_cmpxchg128_release_not_implemented()\n#endif\n\n#if defined(arch_cmpxchg128_relaxed)\n#define raw_cmpxchg128_relaxed arch_cmpxchg128_relaxed\n#elif defined(arch_cmpxchg128)\n#define raw_cmpxchg128_relaxed arch_cmpxchg128\n#else\nextern void raw_cmpxchg128_relaxed_not_implemented(void);\n#define raw_cmpxchg128_relaxed(...) raw_cmpxchg128_relaxed_not_implemented()\n#endif\n\n#if defined(arch_try_cmpxchg)\n#define raw_try_cmpxchg arch_try_cmpxchg\n#elif defined(arch_try_cmpxchg_relaxed)\n#define raw_try_cmpxchg(...) \\\n\t__atomic_op_fence(arch_try_cmpxchg, __VA_ARGS__)\n#else\n#define raw_try_cmpxchg(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg_acquire)\n#define raw_try_cmpxchg_acquire arch_try_cmpxchg_acquire\n#elif defined(arch_try_cmpxchg_relaxed)\n#define raw_try_cmpxchg_acquire(...) \\\n\t__atomic_op_acquire(arch_try_cmpxchg, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg)\n#define raw_try_cmpxchg_acquire arch_try_cmpxchg\n#else\n#define raw_try_cmpxchg_acquire(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg_acquire((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg_release)\n#define raw_try_cmpxchg_release arch_try_cmpxchg_release\n#elif defined(arch_try_cmpxchg_relaxed)\n#define raw_try_cmpxchg_release(...) \\\n\t__atomic_op_release(arch_try_cmpxchg, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg)\n#define raw_try_cmpxchg_release arch_try_cmpxchg\n#else\n#define raw_try_cmpxchg_release(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg_release((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg_relaxed)\n#define raw_try_cmpxchg_relaxed arch_try_cmpxchg_relaxed\n#elif defined(arch_try_cmpxchg)\n#define raw_try_cmpxchg_relaxed arch_try_cmpxchg\n#else\n#define raw_try_cmpxchg_relaxed(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg_relaxed((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg64)\n#define raw_try_cmpxchg64 arch_try_cmpxchg64\n#elif defined(arch_try_cmpxchg64_relaxed)\n#define raw_try_cmpxchg64(...) \\\n\t__atomic_op_fence(arch_try_cmpxchg64, __VA_ARGS__)\n#else\n#define raw_try_cmpxchg64(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg64((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg64_acquire)\n#define raw_try_cmpxchg64_acquire arch_try_cmpxchg64_acquire\n#elif defined(arch_try_cmpxchg64_relaxed)\n#define raw_try_cmpxchg64_acquire(...) \\\n\t__atomic_op_acquire(arch_try_cmpxchg64, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg64)\n#define raw_try_cmpxchg64_acquire arch_try_cmpxchg64\n#else\n#define raw_try_cmpxchg64_acquire(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg64_acquire((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg64_release)\n#define raw_try_cmpxchg64_release arch_try_cmpxchg64_release\n#elif defined(arch_try_cmpxchg64_relaxed)\n#define raw_try_cmpxchg64_release(...) \\\n\t__atomic_op_release(arch_try_cmpxchg64, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg64)\n#define raw_try_cmpxchg64_release arch_try_cmpxchg64\n#else\n#define raw_try_cmpxchg64_release(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg64_release((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg64_relaxed)\n#define raw_try_cmpxchg64_relaxed arch_try_cmpxchg64_relaxed\n#elif defined(arch_try_cmpxchg64)\n#define raw_try_cmpxchg64_relaxed arch_try_cmpxchg64\n#else\n#define raw_try_cmpxchg64_relaxed(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg64_relaxed((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg128)\n#define raw_try_cmpxchg128 arch_try_cmpxchg128\n#elif defined(arch_try_cmpxchg128_relaxed)\n#define raw_try_cmpxchg128(...) \\\n\t__atomic_op_fence(arch_try_cmpxchg128, __VA_ARGS__)\n#else\n#define raw_try_cmpxchg128(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg128((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg128_acquire)\n#define raw_try_cmpxchg128_acquire arch_try_cmpxchg128_acquire\n#elif defined(arch_try_cmpxchg128_relaxed)\n#define raw_try_cmpxchg128_acquire(...) \\\n\t__atomic_op_acquire(arch_try_cmpxchg128, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg128)\n#define raw_try_cmpxchg128_acquire arch_try_cmpxchg128\n#else\n#define raw_try_cmpxchg128_acquire(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg128_acquire((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg128_release)\n#define raw_try_cmpxchg128_release arch_try_cmpxchg128_release\n#elif defined(arch_try_cmpxchg128_relaxed)\n#define raw_try_cmpxchg128_release(...) \\\n\t__atomic_op_release(arch_try_cmpxchg128, __VA_ARGS__)\n#elif defined(arch_try_cmpxchg128)\n#define raw_try_cmpxchg128_release arch_try_cmpxchg128\n#else\n#define raw_try_cmpxchg128_release(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg128_release((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#if defined(arch_try_cmpxchg128_relaxed)\n#define raw_try_cmpxchg128_relaxed arch_try_cmpxchg128_relaxed\n#elif defined(arch_try_cmpxchg128)\n#define raw_try_cmpxchg128_relaxed arch_try_cmpxchg128\n#else\n#define raw_try_cmpxchg128_relaxed(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg128_relaxed((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#define raw_cmpxchg_local arch_cmpxchg_local\n\n#ifdef arch_try_cmpxchg_local\n#define raw_try_cmpxchg_local arch_try_cmpxchg_local\n#else\n#define raw_try_cmpxchg_local(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg_local((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#define raw_cmpxchg64_local arch_cmpxchg64_local\n\n#ifdef arch_try_cmpxchg64_local\n#define raw_try_cmpxchg64_local arch_try_cmpxchg64_local\n#else\n#define raw_try_cmpxchg64_local(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg64_local((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#define raw_cmpxchg128_local arch_cmpxchg128_local\n\n#ifdef arch_try_cmpxchg128_local\n#define raw_try_cmpxchg128_local arch_try_cmpxchg128_local\n#else\n#define raw_try_cmpxchg128_local(_ptr, _oldp, _new) \\\n({ \\\n\ttypeof(*(_ptr)) *___op = (_oldp), ___o = *___op, ___r; \\\n\t___r = raw_cmpxchg128_local((_ptr), ___o, (_new)); \\\n\tif (unlikely(___r != ___o)) \\\n\t\t*___op = ___r; \\\n\tlikely(___r == ___o); \\\n})\n#endif\n\n#define raw_sync_cmpxchg arch_sync_cmpxchg\n\n \nstatic __always_inline int\nraw_atomic_read(const atomic_t *v)\n{\n\treturn arch_atomic_read(v);\n}\n\n \nstatic __always_inline int\nraw_atomic_read_acquire(const atomic_t *v)\n{\n#if defined(arch_atomic_read_acquire)\n\treturn arch_atomic_read_acquire(v);\n#else\n\tint ret;\n\n\tif (__native_word(atomic_t)) {\n\t\tret = smp_load_acquire(&(v)->counter);\n\t} else {\n\t\tret = raw_atomic_read(v);\n\t\t__atomic_acquire_fence();\n\t}\n\n\treturn ret;\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_set(atomic_t *v, int i)\n{\n\tarch_atomic_set(v, i);\n}\n\n \nstatic __always_inline void\nraw_atomic_set_release(atomic_t *v, int i)\n{\n#if defined(arch_atomic_set_release)\n\tarch_atomic_set_release(v, i);\n#else\n\tif (__native_word(atomic_t)) {\n\t\tsmp_store_release(&(v)->counter, i);\n\t} else {\n\t\t__atomic_release_fence();\n\t\traw_atomic_set(v, i);\n\t}\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_add(int i, atomic_t *v)\n{\n\tarch_atomic_add(i, v);\n}\n\n \nstatic __always_inline int\nraw_atomic_add_return(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_return)\n\treturn arch_atomic_add_return(i, v);\n#elif defined(arch_atomic_add_return_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_add_return_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_add_return\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_add_return_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_return_acquire)\n\treturn arch_atomic_add_return_acquire(i, v);\n#elif defined(arch_atomic_add_return_relaxed)\n\tint ret = arch_atomic_add_return_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_add_return)\n\treturn arch_atomic_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic_add_return_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_add_return_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_return_release)\n\treturn arch_atomic_add_return_release(i, v);\n#elif defined(arch_atomic_add_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_add_return_relaxed(i, v);\n#elif defined(arch_atomic_add_return)\n\treturn arch_atomic_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic_add_return_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_add_return_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_return_relaxed)\n\treturn arch_atomic_add_return_relaxed(i, v);\n#elif defined(arch_atomic_add_return)\n\treturn arch_atomic_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic_add_return_relaxed\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_add(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_add)\n\treturn arch_atomic_fetch_add(i, v);\n#elif defined(arch_atomic_fetch_add_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_add_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_fetch_add\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_add_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_add_acquire)\n\treturn arch_atomic_fetch_add_acquire(i, v);\n#elif defined(arch_atomic_fetch_add_relaxed)\n\tint ret = arch_atomic_fetch_add_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_add)\n\treturn arch_atomic_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_add_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_add_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_add_release)\n\treturn arch_atomic_fetch_add_release(i, v);\n#elif defined(arch_atomic_fetch_add_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_add_relaxed(i, v);\n#elif defined(arch_atomic_fetch_add)\n\treturn arch_atomic_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_add_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_add_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_add_relaxed)\n\treturn arch_atomic_fetch_add_relaxed(i, v);\n#elif defined(arch_atomic_fetch_add)\n\treturn arch_atomic_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_add_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_sub(int i, atomic_t *v)\n{\n\tarch_atomic_sub(i, v);\n}\n\n \nstatic __always_inline int\nraw_atomic_sub_return(int i, atomic_t *v)\n{\n#if defined(arch_atomic_sub_return)\n\treturn arch_atomic_sub_return(i, v);\n#elif defined(arch_atomic_sub_return_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_sub_return_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_sub_return\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_sub_return_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_sub_return_acquire)\n\treturn arch_atomic_sub_return_acquire(i, v);\n#elif defined(arch_atomic_sub_return_relaxed)\n\tint ret = arch_atomic_sub_return_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_sub_return)\n\treturn arch_atomic_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic_sub_return_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_sub_return_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_sub_return_release)\n\treturn arch_atomic_sub_return_release(i, v);\n#elif defined(arch_atomic_sub_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_sub_return_relaxed(i, v);\n#elif defined(arch_atomic_sub_return)\n\treturn arch_atomic_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic_sub_return_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_sub_return_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_sub_return_relaxed)\n\treturn arch_atomic_sub_return_relaxed(i, v);\n#elif defined(arch_atomic_sub_return)\n\treturn arch_atomic_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic_sub_return_relaxed\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_sub(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_sub)\n\treturn arch_atomic_fetch_sub(i, v);\n#elif defined(arch_atomic_fetch_sub_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_sub_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_fetch_sub\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_sub_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_sub_acquire)\n\treturn arch_atomic_fetch_sub_acquire(i, v);\n#elif defined(arch_atomic_fetch_sub_relaxed)\n\tint ret = arch_atomic_fetch_sub_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_sub)\n\treturn arch_atomic_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_sub_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_sub_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_sub_release)\n\treturn arch_atomic_fetch_sub_release(i, v);\n#elif defined(arch_atomic_fetch_sub_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_sub_relaxed(i, v);\n#elif defined(arch_atomic_fetch_sub)\n\treturn arch_atomic_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_sub_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_sub_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_sub_relaxed)\n\treturn arch_atomic_fetch_sub_relaxed(i, v);\n#elif defined(arch_atomic_fetch_sub)\n\treturn arch_atomic_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_sub_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_inc(atomic_t *v)\n{\n#if defined(arch_atomic_inc)\n\tarch_atomic_inc(v);\n#else\n\traw_atomic_add(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_inc_return(atomic_t *v)\n{\n#if defined(arch_atomic_inc_return)\n\treturn arch_atomic_inc_return(v);\n#elif defined(arch_atomic_inc_return_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_inc_return_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_add_return(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_inc_return_acquire(atomic_t *v)\n{\n#if defined(arch_atomic_inc_return_acquire)\n\treturn arch_atomic_inc_return_acquire(v);\n#elif defined(arch_atomic_inc_return_relaxed)\n\tint ret = arch_atomic_inc_return_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_inc_return)\n\treturn arch_atomic_inc_return(v);\n#else\n\treturn raw_atomic_add_return_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_inc_return_release(atomic_t *v)\n{\n#if defined(arch_atomic_inc_return_release)\n\treturn arch_atomic_inc_return_release(v);\n#elif defined(arch_atomic_inc_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_inc_return_relaxed(v);\n#elif defined(arch_atomic_inc_return)\n\treturn arch_atomic_inc_return(v);\n#else\n\treturn raw_atomic_add_return_release(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_inc_return_relaxed(atomic_t *v)\n{\n#if defined(arch_atomic_inc_return_relaxed)\n\treturn arch_atomic_inc_return_relaxed(v);\n#elif defined(arch_atomic_inc_return)\n\treturn arch_atomic_inc_return(v);\n#else\n\treturn raw_atomic_add_return_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_inc(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_inc)\n\treturn arch_atomic_fetch_inc(v);\n#elif defined(arch_atomic_fetch_inc_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_inc_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_fetch_add(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_inc_acquire(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_inc_acquire)\n\treturn arch_atomic_fetch_inc_acquire(v);\n#elif defined(arch_atomic_fetch_inc_relaxed)\n\tint ret = arch_atomic_fetch_inc_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_inc)\n\treturn arch_atomic_fetch_inc(v);\n#else\n\treturn raw_atomic_fetch_add_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_inc_release(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_inc_release)\n\treturn arch_atomic_fetch_inc_release(v);\n#elif defined(arch_atomic_fetch_inc_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_inc_relaxed(v);\n#elif defined(arch_atomic_fetch_inc)\n\treturn arch_atomic_fetch_inc(v);\n#else\n\treturn raw_atomic_fetch_add_release(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_inc_relaxed(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_inc_relaxed)\n\treturn arch_atomic_fetch_inc_relaxed(v);\n#elif defined(arch_atomic_fetch_inc)\n\treturn arch_atomic_fetch_inc(v);\n#else\n\treturn raw_atomic_fetch_add_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_dec(atomic_t *v)\n{\n#if defined(arch_atomic_dec)\n\tarch_atomic_dec(v);\n#else\n\traw_atomic_sub(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_dec_return(atomic_t *v)\n{\n#if defined(arch_atomic_dec_return)\n\treturn arch_atomic_dec_return(v);\n#elif defined(arch_atomic_dec_return_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_dec_return_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_sub_return(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_dec_return_acquire(atomic_t *v)\n{\n#if defined(arch_atomic_dec_return_acquire)\n\treturn arch_atomic_dec_return_acquire(v);\n#elif defined(arch_atomic_dec_return_relaxed)\n\tint ret = arch_atomic_dec_return_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_dec_return)\n\treturn arch_atomic_dec_return(v);\n#else\n\treturn raw_atomic_sub_return_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_dec_return_release(atomic_t *v)\n{\n#if defined(arch_atomic_dec_return_release)\n\treturn arch_atomic_dec_return_release(v);\n#elif defined(arch_atomic_dec_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_dec_return_relaxed(v);\n#elif defined(arch_atomic_dec_return)\n\treturn arch_atomic_dec_return(v);\n#else\n\treturn raw_atomic_sub_return_release(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_dec_return_relaxed(atomic_t *v)\n{\n#if defined(arch_atomic_dec_return_relaxed)\n\treturn arch_atomic_dec_return_relaxed(v);\n#elif defined(arch_atomic_dec_return)\n\treturn arch_atomic_dec_return(v);\n#else\n\treturn raw_atomic_sub_return_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_dec(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_dec)\n\treturn arch_atomic_fetch_dec(v);\n#elif defined(arch_atomic_fetch_dec_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_dec_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_fetch_sub(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_dec_acquire(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_dec_acquire)\n\treturn arch_atomic_fetch_dec_acquire(v);\n#elif defined(arch_atomic_fetch_dec_relaxed)\n\tint ret = arch_atomic_fetch_dec_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_dec)\n\treturn arch_atomic_fetch_dec(v);\n#else\n\treturn raw_atomic_fetch_sub_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_dec_release(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_dec_release)\n\treturn arch_atomic_fetch_dec_release(v);\n#elif defined(arch_atomic_fetch_dec_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_dec_relaxed(v);\n#elif defined(arch_atomic_fetch_dec)\n\treturn arch_atomic_fetch_dec(v);\n#else\n\treturn raw_atomic_fetch_sub_release(1, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_dec_relaxed(atomic_t *v)\n{\n#if defined(arch_atomic_fetch_dec_relaxed)\n\treturn arch_atomic_fetch_dec_relaxed(v);\n#elif defined(arch_atomic_fetch_dec)\n\treturn arch_atomic_fetch_dec(v);\n#else\n\treturn raw_atomic_fetch_sub_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_and(int i, atomic_t *v)\n{\n\tarch_atomic_and(i, v);\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_and(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_and)\n\treturn arch_atomic_fetch_and(i, v);\n#elif defined(arch_atomic_fetch_and_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_and_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_fetch_and\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_and_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_and_acquire)\n\treturn arch_atomic_fetch_and_acquire(i, v);\n#elif defined(arch_atomic_fetch_and_relaxed)\n\tint ret = arch_atomic_fetch_and_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_and)\n\treturn arch_atomic_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_and_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_and_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_and_release)\n\treturn arch_atomic_fetch_and_release(i, v);\n#elif defined(arch_atomic_fetch_and_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_and_relaxed(i, v);\n#elif defined(arch_atomic_fetch_and)\n\treturn arch_atomic_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_and_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_and_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_and_relaxed)\n\treturn arch_atomic_fetch_and_relaxed(i, v);\n#elif defined(arch_atomic_fetch_and)\n\treturn arch_atomic_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_and_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_andnot(int i, atomic_t *v)\n{\n#if defined(arch_atomic_andnot)\n\tarch_atomic_andnot(i, v);\n#else\n\traw_atomic_and(~i, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_andnot(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_andnot)\n\treturn arch_atomic_fetch_andnot(i, v);\n#elif defined(arch_atomic_fetch_andnot_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_andnot_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_fetch_and(~i, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_andnot_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_andnot_acquire)\n\treturn arch_atomic_fetch_andnot_acquire(i, v);\n#elif defined(arch_atomic_fetch_andnot_relaxed)\n\tint ret = arch_atomic_fetch_andnot_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_andnot)\n\treturn arch_atomic_fetch_andnot(i, v);\n#else\n\treturn raw_atomic_fetch_and_acquire(~i, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_andnot_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_andnot_release)\n\treturn arch_atomic_fetch_andnot_release(i, v);\n#elif defined(arch_atomic_fetch_andnot_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_andnot_relaxed(i, v);\n#elif defined(arch_atomic_fetch_andnot)\n\treturn arch_atomic_fetch_andnot(i, v);\n#else\n\treturn raw_atomic_fetch_and_release(~i, v);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_andnot_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_andnot_relaxed)\n\treturn arch_atomic_fetch_andnot_relaxed(i, v);\n#elif defined(arch_atomic_fetch_andnot)\n\treturn arch_atomic_fetch_andnot(i, v);\n#else\n\treturn raw_atomic_fetch_and_relaxed(~i, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_or(int i, atomic_t *v)\n{\n\tarch_atomic_or(i, v);\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_or(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_or)\n\treturn arch_atomic_fetch_or(i, v);\n#elif defined(arch_atomic_fetch_or_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_or_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_fetch_or\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_or_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_or_acquire)\n\treturn arch_atomic_fetch_or_acquire(i, v);\n#elif defined(arch_atomic_fetch_or_relaxed)\n\tint ret = arch_atomic_fetch_or_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_or)\n\treturn arch_atomic_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_or_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_or_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_or_release)\n\treturn arch_atomic_fetch_or_release(i, v);\n#elif defined(arch_atomic_fetch_or_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_or_relaxed(i, v);\n#elif defined(arch_atomic_fetch_or)\n\treturn arch_atomic_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_or_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_or_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_or_relaxed)\n\treturn arch_atomic_fetch_or_relaxed(i, v);\n#elif defined(arch_atomic_fetch_or)\n\treturn arch_atomic_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_or_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic_xor(int i, atomic_t *v)\n{\n\tarch_atomic_xor(i, v);\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_xor(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_xor)\n\treturn arch_atomic_fetch_xor(i, v);\n#elif defined(arch_atomic_fetch_xor_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_fetch_xor_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic_fetch_xor\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_xor_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_xor_acquire)\n\treturn arch_atomic_fetch_xor_acquire(i, v);\n#elif defined(arch_atomic_fetch_xor_relaxed)\n\tint ret = arch_atomic_fetch_xor_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_fetch_xor)\n\treturn arch_atomic_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_xor_acquire\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_xor_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_xor_release)\n\treturn arch_atomic_fetch_xor_release(i, v);\n#elif defined(arch_atomic_fetch_xor_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_fetch_xor_relaxed(i, v);\n#elif defined(arch_atomic_fetch_xor)\n\treturn arch_atomic_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_xor_release\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_xor_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_fetch_xor_relaxed)\n\treturn arch_atomic_fetch_xor_relaxed(i, v);\n#elif defined(arch_atomic_fetch_xor)\n\treturn arch_atomic_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic_fetch_xor_relaxed\"\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_xchg(atomic_t *v, int new)\n{\n#if defined(arch_atomic_xchg)\n\treturn arch_atomic_xchg(v, new);\n#elif defined(arch_atomic_xchg_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_xchg_relaxed(v, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_xchg(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_xchg_acquire(atomic_t *v, int new)\n{\n#if defined(arch_atomic_xchg_acquire)\n\treturn arch_atomic_xchg_acquire(v, new);\n#elif defined(arch_atomic_xchg_relaxed)\n\tint ret = arch_atomic_xchg_relaxed(v, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_xchg)\n\treturn arch_atomic_xchg(v, new);\n#else\n\treturn raw_xchg_acquire(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_xchg_release(atomic_t *v, int new)\n{\n#if defined(arch_atomic_xchg_release)\n\treturn arch_atomic_xchg_release(v, new);\n#elif defined(arch_atomic_xchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_xchg_relaxed(v, new);\n#elif defined(arch_atomic_xchg)\n\treturn arch_atomic_xchg(v, new);\n#else\n\treturn raw_xchg_release(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_xchg_relaxed(atomic_t *v, int new)\n{\n#if defined(arch_atomic_xchg_relaxed)\n\treturn arch_atomic_xchg_relaxed(v, new);\n#elif defined(arch_atomic_xchg)\n\treturn arch_atomic_xchg(v, new);\n#else\n\treturn raw_xchg_relaxed(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_cmpxchg(atomic_t *v, int old, int new)\n{\n#if defined(arch_atomic_cmpxchg)\n\treturn arch_atomic_cmpxchg(v, old, new);\n#elif defined(arch_atomic_cmpxchg_relaxed)\n\tint ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_cmpxchg_relaxed(v, old, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_cmpxchg(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_cmpxchg_acquire(atomic_t *v, int old, int new)\n{\n#if defined(arch_atomic_cmpxchg_acquire)\n\treturn arch_atomic_cmpxchg_acquire(v, old, new);\n#elif defined(arch_atomic_cmpxchg_relaxed)\n\tint ret = arch_atomic_cmpxchg_relaxed(v, old, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_cmpxchg)\n\treturn arch_atomic_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_acquire(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_cmpxchg_release(atomic_t *v, int old, int new)\n{\n#if defined(arch_atomic_cmpxchg_release)\n\treturn arch_atomic_cmpxchg_release(v, old, new);\n#elif defined(arch_atomic_cmpxchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic_cmpxchg)\n\treturn arch_atomic_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_release(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_cmpxchg_relaxed(atomic_t *v, int old, int new)\n{\n#if defined(arch_atomic_cmpxchg_relaxed)\n\treturn arch_atomic_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic_cmpxchg)\n\treturn arch_atomic_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_relaxed(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_try_cmpxchg(atomic_t *v, int *old, int new)\n{\n#if defined(arch_atomic_try_cmpxchg)\n\treturn arch_atomic_try_cmpxchg(v, old, new);\n#elif defined(arch_atomic_try_cmpxchg_relaxed)\n\tbool ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_try_cmpxchg_relaxed(v, old, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\tint r, o = *old;\n\tr = raw_atomic_cmpxchg(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_try_cmpxchg_acquire(atomic_t *v, int *old, int new)\n{\n#if defined(arch_atomic_try_cmpxchg_acquire)\n\treturn arch_atomic_try_cmpxchg_acquire(v, old, new);\n#elif defined(arch_atomic_try_cmpxchg_relaxed)\n\tbool ret = arch_atomic_try_cmpxchg_relaxed(v, old, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_try_cmpxchg)\n\treturn arch_atomic_try_cmpxchg(v, old, new);\n#else\n\tint r, o = *old;\n\tr = raw_atomic_cmpxchg_acquire(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_try_cmpxchg_release(atomic_t *v, int *old, int new)\n{\n#if defined(arch_atomic_try_cmpxchg_release)\n\treturn arch_atomic_try_cmpxchg_release(v, old, new);\n#elif defined(arch_atomic_try_cmpxchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_try_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic_try_cmpxchg)\n\treturn arch_atomic_try_cmpxchg(v, old, new);\n#else\n\tint r, o = *old;\n\tr = raw_atomic_cmpxchg_release(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_try_cmpxchg_relaxed(atomic_t *v, int *old, int new)\n{\n#if defined(arch_atomic_try_cmpxchg_relaxed)\n\treturn arch_atomic_try_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic_try_cmpxchg)\n\treturn arch_atomic_try_cmpxchg(v, old, new);\n#else\n\tint r, o = *old;\n\tr = raw_atomic_cmpxchg_relaxed(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_sub_and_test(int i, atomic_t *v)\n{\n#if defined(arch_atomic_sub_and_test)\n\treturn arch_atomic_sub_and_test(i, v);\n#else\n\treturn raw_atomic_sub_return(i, v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_dec_and_test(atomic_t *v)\n{\n#if defined(arch_atomic_dec_and_test)\n\treturn arch_atomic_dec_and_test(v);\n#else\n\treturn raw_atomic_dec_return(v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_inc_and_test(atomic_t *v)\n{\n#if defined(arch_atomic_inc_and_test)\n\treturn arch_atomic_inc_and_test(v);\n#else\n\treturn raw_atomic_inc_return(v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_add_negative(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_negative)\n\treturn arch_atomic_add_negative(i, v);\n#elif defined(arch_atomic_add_negative_relaxed)\n\tbool ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic_add_negative_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic_add_return(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_add_negative_acquire(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_negative_acquire)\n\treturn arch_atomic_add_negative_acquire(i, v);\n#elif defined(arch_atomic_add_negative_relaxed)\n\tbool ret = arch_atomic_add_negative_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic_add_negative)\n\treturn arch_atomic_add_negative(i, v);\n#else\n\treturn raw_atomic_add_return_acquire(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_add_negative_release(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_negative_release)\n\treturn arch_atomic_add_negative_release(i, v);\n#elif defined(arch_atomic_add_negative_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic_add_negative_relaxed(i, v);\n#elif defined(arch_atomic_add_negative)\n\treturn arch_atomic_add_negative(i, v);\n#else\n\treturn raw_atomic_add_return_release(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_add_negative_relaxed(int i, atomic_t *v)\n{\n#if defined(arch_atomic_add_negative_relaxed)\n\treturn arch_atomic_add_negative_relaxed(i, v);\n#elif defined(arch_atomic_add_negative)\n\treturn arch_atomic_add_negative(i, v);\n#else\n\treturn raw_atomic_add_return_relaxed(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_fetch_add_unless(atomic_t *v, int a, int u)\n{\n#if defined(arch_atomic_fetch_add_unless)\n\treturn arch_atomic_fetch_add_unless(v, a, u);\n#else\n\tint c = raw_atomic_read(v);\n\n\tdo {\n\t\tif (unlikely(c == u))\n\t\t\tbreak;\n\t} while (!raw_atomic_try_cmpxchg(v, &c, c + a));\n\n\treturn c;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_add_unless(atomic_t *v, int a, int u)\n{\n#if defined(arch_atomic_add_unless)\n\treturn arch_atomic_add_unless(v, a, u);\n#else\n\treturn raw_atomic_fetch_add_unless(v, a, u) != u;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_inc_not_zero(atomic_t *v)\n{\n#if defined(arch_atomic_inc_not_zero)\n\treturn arch_atomic_inc_not_zero(v);\n#else\n\treturn raw_atomic_add_unless(v, 1, 0);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_inc_unless_negative(atomic_t *v)\n{\n#if defined(arch_atomic_inc_unless_negative)\n\treturn arch_atomic_inc_unless_negative(v);\n#else\n\tint c = raw_atomic_read(v);\n\n\tdo {\n\t\tif (unlikely(c < 0))\n\t\t\treturn false;\n\t} while (!raw_atomic_try_cmpxchg(v, &c, c + 1));\n\n\treturn true;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic_dec_unless_positive(atomic_t *v)\n{\n#if defined(arch_atomic_dec_unless_positive)\n\treturn arch_atomic_dec_unless_positive(v);\n#else\n\tint c = raw_atomic_read(v);\n\n\tdo {\n\t\tif (unlikely(c > 0))\n\t\t\treturn false;\n\t} while (!raw_atomic_try_cmpxchg(v, &c, c - 1));\n\n\treturn true;\n#endif\n}\n\n \nstatic __always_inline int\nraw_atomic_dec_if_positive(atomic_t *v)\n{\n#if defined(arch_atomic_dec_if_positive)\n\treturn arch_atomic_dec_if_positive(v);\n#else\n\tint dec, c = raw_atomic_read(v);\n\n\tdo {\n\t\tdec = c - 1;\n\t\tif (unlikely(dec < 0))\n\t\t\tbreak;\n\t} while (!raw_atomic_try_cmpxchg(v, &c, dec));\n\n\treturn dec;\n#endif\n}\n\n#ifdef CONFIG_GENERIC_ATOMIC64\n#include <asm-generic/atomic64.h>\n#endif\n\n \nstatic __always_inline s64\nraw_atomic64_read(const atomic64_t *v)\n{\n\treturn arch_atomic64_read(v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_read_acquire(const atomic64_t *v)\n{\n#if defined(arch_atomic64_read_acquire)\n\treturn arch_atomic64_read_acquire(v);\n#else\n\ts64 ret;\n\n\tif (__native_word(atomic64_t)) {\n\t\tret = smp_load_acquire(&(v)->counter);\n\t} else {\n\t\tret = raw_atomic64_read(v);\n\t\t__atomic_acquire_fence();\n\t}\n\n\treturn ret;\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_set(atomic64_t *v, s64 i)\n{\n\tarch_atomic64_set(v, i);\n}\n\n \nstatic __always_inline void\nraw_atomic64_set_release(atomic64_t *v, s64 i)\n{\n#if defined(arch_atomic64_set_release)\n\tarch_atomic64_set_release(v, i);\n#else\n\tif (__native_word(atomic64_t)) {\n\t\tsmp_store_release(&(v)->counter, i);\n\t} else {\n\t\t__atomic_release_fence();\n\t\traw_atomic64_set(v, i);\n\t}\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_add(s64 i, atomic64_t *v)\n{\n\tarch_atomic64_add(i, v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_add_return(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_return)\n\treturn arch_atomic64_add_return(i, v);\n#elif defined(arch_atomic64_add_return_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_add_return_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_add_return\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_add_return_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_return_acquire)\n\treturn arch_atomic64_add_return_acquire(i, v);\n#elif defined(arch_atomic64_add_return_relaxed)\n\ts64 ret = arch_atomic64_add_return_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_add_return)\n\treturn arch_atomic64_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_add_return_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_add_return_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_return_release)\n\treturn arch_atomic64_add_return_release(i, v);\n#elif defined(arch_atomic64_add_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_add_return_relaxed(i, v);\n#elif defined(arch_atomic64_add_return)\n\treturn arch_atomic64_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_add_return_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_add_return_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_return_relaxed)\n\treturn arch_atomic64_add_return_relaxed(i, v);\n#elif defined(arch_atomic64_add_return)\n\treturn arch_atomic64_add_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_add_return_relaxed\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_add(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_add)\n\treturn arch_atomic64_fetch_add(i, v);\n#elif defined(arch_atomic64_fetch_add_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_add_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_fetch_add\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_add_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_add_acquire)\n\treturn arch_atomic64_fetch_add_acquire(i, v);\n#elif defined(arch_atomic64_fetch_add_relaxed)\n\ts64 ret = arch_atomic64_fetch_add_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_add)\n\treturn arch_atomic64_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_add_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_add_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_add_release)\n\treturn arch_atomic64_fetch_add_release(i, v);\n#elif defined(arch_atomic64_fetch_add_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_add_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_add)\n\treturn arch_atomic64_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_add_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_add_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_add_relaxed)\n\treturn arch_atomic64_fetch_add_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_add)\n\treturn arch_atomic64_fetch_add(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_add_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_sub(s64 i, atomic64_t *v)\n{\n\tarch_atomic64_sub(i, v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_sub_return(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_sub_return)\n\treturn arch_atomic64_sub_return(i, v);\n#elif defined(arch_atomic64_sub_return_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_sub_return_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_sub_return\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_sub_return_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_sub_return_acquire)\n\treturn arch_atomic64_sub_return_acquire(i, v);\n#elif defined(arch_atomic64_sub_return_relaxed)\n\ts64 ret = arch_atomic64_sub_return_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_sub_return)\n\treturn arch_atomic64_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_sub_return_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_sub_return_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_sub_return_release)\n\treturn arch_atomic64_sub_return_release(i, v);\n#elif defined(arch_atomic64_sub_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_sub_return_relaxed(i, v);\n#elif defined(arch_atomic64_sub_return)\n\treturn arch_atomic64_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_sub_return_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_sub_return_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_sub_return_relaxed)\n\treturn arch_atomic64_sub_return_relaxed(i, v);\n#elif defined(arch_atomic64_sub_return)\n\treturn arch_atomic64_sub_return(i, v);\n#else\n#error \"Unable to define raw_atomic64_sub_return_relaxed\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_sub(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_sub)\n\treturn arch_atomic64_fetch_sub(i, v);\n#elif defined(arch_atomic64_fetch_sub_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_sub_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_fetch_sub\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_sub_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_sub_acquire)\n\treturn arch_atomic64_fetch_sub_acquire(i, v);\n#elif defined(arch_atomic64_fetch_sub_relaxed)\n\ts64 ret = arch_atomic64_fetch_sub_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_sub)\n\treturn arch_atomic64_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_sub_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_sub_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_sub_release)\n\treturn arch_atomic64_fetch_sub_release(i, v);\n#elif defined(arch_atomic64_fetch_sub_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_sub_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_sub)\n\treturn arch_atomic64_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_sub_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_sub_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_sub_relaxed)\n\treturn arch_atomic64_fetch_sub_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_sub)\n\treturn arch_atomic64_fetch_sub(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_sub_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_inc(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc)\n\tarch_atomic64_inc(v);\n#else\n\traw_atomic64_add(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_inc_return(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_return)\n\treturn arch_atomic64_inc_return(v);\n#elif defined(arch_atomic64_inc_return_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_inc_return_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_add_return(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_inc_return_acquire(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_return_acquire)\n\treturn arch_atomic64_inc_return_acquire(v);\n#elif defined(arch_atomic64_inc_return_relaxed)\n\ts64 ret = arch_atomic64_inc_return_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_inc_return)\n\treturn arch_atomic64_inc_return(v);\n#else\n\treturn raw_atomic64_add_return_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_inc_return_release(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_return_release)\n\treturn arch_atomic64_inc_return_release(v);\n#elif defined(arch_atomic64_inc_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_inc_return_relaxed(v);\n#elif defined(arch_atomic64_inc_return)\n\treturn arch_atomic64_inc_return(v);\n#else\n\treturn raw_atomic64_add_return_release(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_inc_return_relaxed(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_return_relaxed)\n\treturn arch_atomic64_inc_return_relaxed(v);\n#elif defined(arch_atomic64_inc_return)\n\treturn arch_atomic64_inc_return(v);\n#else\n\treturn raw_atomic64_add_return_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_inc(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_inc)\n\treturn arch_atomic64_fetch_inc(v);\n#elif defined(arch_atomic64_fetch_inc_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_inc_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_fetch_add(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_inc_acquire(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_inc_acquire)\n\treturn arch_atomic64_fetch_inc_acquire(v);\n#elif defined(arch_atomic64_fetch_inc_relaxed)\n\ts64 ret = arch_atomic64_fetch_inc_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_inc)\n\treturn arch_atomic64_fetch_inc(v);\n#else\n\treturn raw_atomic64_fetch_add_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_inc_release(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_inc_release)\n\treturn arch_atomic64_fetch_inc_release(v);\n#elif defined(arch_atomic64_fetch_inc_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_inc_relaxed(v);\n#elif defined(arch_atomic64_fetch_inc)\n\treturn arch_atomic64_fetch_inc(v);\n#else\n\treturn raw_atomic64_fetch_add_release(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_inc_relaxed(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_inc_relaxed)\n\treturn arch_atomic64_fetch_inc_relaxed(v);\n#elif defined(arch_atomic64_fetch_inc)\n\treturn arch_atomic64_fetch_inc(v);\n#else\n\treturn raw_atomic64_fetch_add_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_dec(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec)\n\tarch_atomic64_dec(v);\n#else\n\traw_atomic64_sub(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_dec_return(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_return)\n\treturn arch_atomic64_dec_return(v);\n#elif defined(arch_atomic64_dec_return_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_dec_return_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_sub_return(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_dec_return_acquire(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_return_acquire)\n\treturn arch_atomic64_dec_return_acquire(v);\n#elif defined(arch_atomic64_dec_return_relaxed)\n\ts64 ret = arch_atomic64_dec_return_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_dec_return)\n\treturn arch_atomic64_dec_return(v);\n#else\n\treturn raw_atomic64_sub_return_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_dec_return_release(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_return_release)\n\treturn arch_atomic64_dec_return_release(v);\n#elif defined(arch_atomic64_dec_return_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_dec_return_relaxed(v);\n#elif defined(arch_atomic64_dec_return)\n\treturn arch_atomic64_dec_return(v);\n#else\n\treturn raw_atomic64_sub_return_release(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_dec_return_relaxed(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_return_relaxed)\n\treturn arch_atomic64_dec_return_relaxed(v);\n#elif defined(arch_atomic64_dec_return)\n\treturn arch_atomic64_dec_return(v);\n#else\n\treturn raw_atomic64_sub_return_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_dec(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_dec)\n\treturn arch_atomic64_fetch_dec(v);\n#elif defined(arch_atomic64_fetch_dec_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_dec_relaxed(v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_fetch_sub(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_dec_acquire(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_dec_acquire)\n\treturn arch_atomic64_fetch_dec_acquire(v);\n#elif defined(arch_atomic64_fetch_dec_relaxed)\n\ts64 ret = arch_atomic64_fetch_dec_relaxed(v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_dec)\n\treturn arch_atomic64_fetch_dec(v);\n#else\n\treturn raw_atomic64_fetch_sub_acquire(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_dec_release(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_dec_release)\n\treturn arch_atomic64_fetch_dec_release(v);\n#elif defined(arch_atomic64_fetch_dec_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_dec_relaxed(v);\n#elif defined(arch_atomic64_fetch_dec)\n\treturn arch_atomic64_fetch_dec(v);\n#else\n\treturn raw_atomic64_fetch_sub_release(1, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_dec_relaxed(atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_dec_relaxed)\n\treturn arch_atomic64_fetch_dec_relaxed(v);\n#elif defined(arch_atomic64_fetch_dec)\n\treturn arch_atomic64_fetch_dec(v);\n#else\n\treturn raw_atomic64_fetch_sub_relaxed(1, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_and(s64 i, atomic64_t *v)\n{\n\tarch_atomic64_and(i, v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_and(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_and)\n\treturn arch_atomic64_fetch_and(i, v);\n#elif defined(arch_atomic64_fetch_and_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_and_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_fetch_and\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_and_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_and_acquire)\n\treturn arch_atomic64_fetch_and_acquire(i, v);\n#elif defined(arch_atomic64_fetch_and_relaxed)\n\ts64 ret = arch_atomic64_fetch_and_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_and)\n\treturn arch_atomic64_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_and_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_and_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_and_release)\n\treturn arch_atomic64_fetch_and_release(i, v);\n#elif defined(arch_atomic64_fetch_and_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_and_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_and)\n\treturn arch_atomic64_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_and_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_and_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_and_relaxed)\n\treturn arch_atomic64_fetch_and_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_and)\n\treturn arch_atomic64_fetch_and(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_and_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_andnot(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_andnot)\n\tarch_atomic64_andnot(i, v);\n#else\n\traw_atomic64_and(~i, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_andnot(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_andnot)\n\treturn arch_atomic64_fetch_andnot(i, v);\n#elif defined(arch_atomic64_fetch_andnot_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_andnot_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_fetch_and(~i, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_andnot_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_andnot_acquire)\n\treturn arch_atomic64_fetch_andnot_acquire(i, v);\n#elif defined(arch_atomic64_fetch_andnot_relaxed)\n\ts64 ret = arch_atomic64_fetch_andnot_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_andnot)\n\treturn arch_atomic64_fetch_andnot(i, v);\n#else\n\treturn raw_atomic64_fetch_and_acquire(~i, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_andnot_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_andnot_release)\n\treturn arch_atomic64_fetch_andnot_release(i, v);\n#elif defined(arch_atomic64_fetch_andnot_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_andnot_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_andnot)\n\treturn arch_atomic64_fetch_andnot(i, v);\n#else\n\treturn raw_atomic64_fetch_and_release(~i, v);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_andnot_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_andnot_relaxed)\n\treturn arch_atomic64_fetch_andnot_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_andnot)\n\treturn arch_atomic64_fetch_andnot(i, v);\n#else\n\treturn raw_atomic64_fetch_and_relaxed(~i, v);\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_or(s64 i, atomic64_t *v)\n{\n\tarch_atomic64_or(i, v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_or(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_or)\n\treturn arch_atomic64_fetch_or(i, v);\n#elif defined(arch_atomic64_fetch_or_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_or_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_fetch_or\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_or_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_or_acquire)\n\treturn arch_atomic64_fetch_or_acquire(i, v);\n#elif defined(arch_atomic64_fetch_or_relaxed)\n\ts64 ret = arch_atomic64_fetch_or_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_or)\n\treturn arch_atomic64_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_or_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_or_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_or_release)\n\treturn arch_atomic64_fetch_or_release(i, v);\n#elif defined(arch_atomic64_fetch_or_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_or_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_or)\n\treturn arch_atomic64_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_or_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_or_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_or_relaxed)\n\treturn arch_atomic64_fetch_or_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_or)\n\treturn arch_atomic64_fetch_or(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_or_relaxed\"\n#endif\n}\n\n \nstatic __always_inline void\nraw_atomic64_xor(s64 i, atomic64_t *v)\n{\n\tarch_atomic64_xor(i, v);\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_xor(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_xor)\n\treturn arch_atomic64_fetch_xor(i, v);\n#elif defined(arch_atomic64_fetch_xor_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_fetch_xor_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n#error \"Unable to define raw_atomic64_fetch_xor\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_xor_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_xor_acquire)\n\treturn arch_atomic64_fetch_xor_acquire(i, v);\n#elif defined(arch_atomic64_fetch_xor_relaxed)\n\ts64 ret = arch_atomic64_fetch_xor_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_fetch_xor)\n\treturn arch_atomic64_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_xor_acquire\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_xor_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_xor_release)\n\treturn arch_atomic64_fetch_xor_release(i, v);\n#elif defined(arch_atomic64_fetch_xor_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_fetch_xor_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_xor)\n\treturn arch_atomic64_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_xor_release\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_xor_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_fetch_xor_relaxed)\n\treturn arch_atomic64_fetch_xor_relaxed(i, v);\n#elif defined(arch_atomic64_fetch_xor)\n\treturn arch_atomic64_fetch_xor(i, v);\n#else\n#error \"Unable to define raw_atomic64_fetch_xor_relaxed\"\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_xchg(atomic64_t *v, s64 new)\n{\n#if defined(arch_atomic64_xchg)\n\treturn arch_atomic64_xchg(v, new);\n#elif defined(arch_atomic64_xchg_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_xchg_relaxed(v, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_xchg(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_xchg_acquire(atomic64_t *v, s64 new)\n{\n#if defined(arch_atomic64_xchg_acquire)\n\treturn arch_atomic64_xchg_acquire(v, new);\n#elif defined(arch_atomic64_xchg_relaxed)\n\ts64 ret = arch_atomic64_xchg_relaxed(v, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_xchg)\n\treturn arch_atomic64_xchg(v, new);\n#else\n\treturn raw_xchg_acquire(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_xchg_release(atomic64_t *v, s64 new)\n{\n#if defined(arch_atomic64_xchg_release)\n\treturn arch_atomic64_xchg_release(v, new);\n#elif defined(arch_atomic64_xchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_xchg_relaxed(v, new);\n#elif defined(arch_atomic64_xchg)\n\treturn arch_atomic64_xchg(v, new);\n#else\n\treturn raw_xchg_release(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_xchg_relaxed(atomic64_t *v, s64 new)\n{\n#if defined(arch_atomic64_xchg_relaxed)\n\treturn arch_atomic64_xchg_relaxed(v, new);\n#elif defined(arch_atomic64_xchg)\n\treturn arch_atomic64_xchg(v, new);\n#else\n\treturn raw_xchg_relaxed(&v->counter, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)\n{\n#if defined(arch_atomic64_cmpxchg)\n\treturn arch_atomic64_cmpxchg(v, old, new);\n#elif defined(arch_atomic64_cmpxchg_relaxed)\n\ts64 ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_cmpxchg_relaxed(v, old, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_cmpxchg(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_cmpxchg_acquire(atomic64_t *v, s64 old, s64 new)\n{\n#if defined(arch_atomic64_cmpxchg_acquire)\n\treturn arch_atomic64_cmpxchg_acquire(v, old, new);\n#elif defined(arch_atomic64_cmpxchg_relaxed)\n\ts64 ret = arch_atomic64_cmpxchg_relaxed(v, old, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_cmpxchg)\n\treturn arch_atomic64_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_acquire(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_cmpxchg_release(atomic64_t *v, s64 old, s64 new)\n{\n#if defined(arch_atomic64_cmpxchg_release)\n\treturn arch_atomic64_cmpxchg_release(v, old, new);\n#elif defined(arch_atomic64_cmpxchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic64_cmpxchg)\n\treturn arch_atomic64_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_release(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_cmpxchg_relaxed(atomic64_t *v, s64 old, s64 new)\n{\n#if defined(arch_atomic64_cmpxchg_relaxed)\n\treturn arch_atomic64_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic64_cmpxchg)\n\treturn arch_atomic64_cmpxchg(v, old, new);\n#else\n\treturn raw_cmpxchg_relaxed(&v->counter, old, new);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)\n{\n#if defined(arch_atomic64_try_cmpxchg)\n\treturn arch_atomic64_try_cmpxchg(v, old, new);\n#elif defined(arch_atomic64_try_cmpxchg_relaxed)\n\tbool ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_try_cmpxchg_relaxed(v, old, new);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\ts64 r, o = *old;\n\tr = raw_atomic64_cmpxchg(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_try_cmpxchg_acquire(atomic64_t *v, s64 *old, s64 new)\n{\n#if defined(arch_atomic64_try_cmpxchg_acquire)\n\treturn arch_atomic64_try_cmpxchg_acquire(v, old, new);\n#elif defined(arch_atomic64_try_cmpxchg_relaxed)\n\tbool ret = arch_atomic64_try_cmpxchg_relaxed(v, old, new);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_try_cmpxchg)\n\treturn arch_atomic64_try_cmpxchg(v, old, new);\n#else\n\ts64 r, o = *old;\n\tr = raw_atomic64_cmpxchg_acquire(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_try_cmpxchg_release(atomic64_t *v, s64 *old, s64 new)\n{\n#if defined(arch_atomic64_try_cmpxchg_release)\n\treturn arch_atomic64_try_cmpxchg_release(v, old, new);\n#elif defined(arch_atomic64_try_cmpxchg_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_try_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic64_try_cmpxchg)\n\treturn arch_atomic64_try_cmpxchg(v, old, new);\n#else\n\ts64 r, o = *old;\n\tr = raw_atomic64_cmpxchg_release(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_try_cmpxchg_relaxed(atomic64_t *v, s64 *old, s64 new)\n{\n#if defined(arch_atomic64_try_cmpxchg_relaxed)\n\treturn arch_atomic64_try_cmpxchg_relaxed(v, old, new);\n#elif defined(arch_atomic64_try_cmpxchg)\n\treturn arch_atomic64_try_cmpxchg(v, old, new);\n#else\n\ts64 r, o = *old;\n\tr = raw_atomic64_cmpxchg_relaxed(v, o, new);\n\tif (unlikely(r != o))\n\t\t*old = r;\n\treturn likely(r == o);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_sub_and_test(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_sub_and_test)\n\treturn arch_atomic64_sub_and_test(i, v);\n#else\n\treturn raw_atomic64_sub_return(i, v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_dec_and_test(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_and_test)\n\treturn arch_atomic64_dec_and_test(v);\n#else\n\treturn raw_atomic64_dec_return(v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_inc_and_test(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_and_test)\n\treturn arch_atomic64_inc_and_test(v);\n#else\n\treturn raw_atomic64_inc_return(v) == 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_add_negative(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_negative)\n\treturn arch_atomic64_add_negative(i, v);\n#elif defined(arch_atomic64_add_negative_relaxed)\n\tbool ret;\n\t__atomic_pre_full_fence();\n\tret = arch_atomic64_add_negative_relaxed(i, v);\n\t__atomic_post_full_fence();\n\treturn ret;\n#else\n\treturn raw_atomic64_add_return(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_add_negative_acquire(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_negative_acquire)\n\treturn arch_atomic64_add_negative_acquire(i, v);\n#elif defined(arch_atomic64_add_negative_relaxed)\n\tbool ret = arch_atomic64_add_negative_relaxed(i, v);\n\t__atomic_acquire_fence();\n\treturn ret;\n#elif defined(arch_atomic64_add_negative)\n\treturn arch_atomic64_add_negative(i, v);\n#else\n\treturn raw_atomic64_add_return_acquire(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_add_negative_release(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_negative_release)\n\treturn arch_atomic64_add_negative_release(i, v);\n#elif defined(arch_atomic64_add_negative_relaxed)\n\t__atomic_release_fence();\n\treturn arch_atomic64_add_negative_relaxed(i, v);\n#elif defined(arch_atomic64_add_negative)\n\treturn arch_atomic64_add_negative(i, v);\n#else\n\treturn raw_atomic64_add_return_release(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_add_negative_relaxed(s64 i, atomic64_t *v)\n{\n#if defined(arch_atomic64_add_negative_relaxed)\n\treturn arch_atomic64_add_negative_relaxed(i, v);\n#elif defined(arch_atomic64_add_negative)\n\treturn arch_atomic64_add_negative(i, v);\n#else\n\treturn raw_atomic64_add_return_relaxed(i, v) < 0;\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_fetch_add_unless(atomic64_t *v, s64 a, s64 u)\n{\n#if defined(arch_atomic64_fetch_add_unless)\n\treturn arch_atomic64_fetch_add_unless(v, a, u);\n#else\n\ts64 c = raw_atomic64_read(v);\n\n\tdo {\n\t\tif (unlikely(c == u))\n\t\t\tbreak;\n\t} while (!raw_atomic64_try_cmpxchg(v, &c, c + a));\n\n\treturn c;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_add_unless(atomic64_t *v, s64 a, s64 u)\n{\n#if defined(arch_atomic64_add_unless)\n\treturn arch_atomic64_add_unless(v, a, u);\n#else\n\treturn raw_atomic64_fetch_add_unless(v, a, u) != u;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_inc_not_zero(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_not_zero)\n\treturn arch_atomic64_inc_not_zero(v);\n#else\n\treturn raw_atomic64_add_unless(v, 1, 0);\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_inc_unless_negative(atomic64_t *v)\n{\n#if defined(arch_atomic64_inc_unless_negative)\n\treturn arch_atomic64_inc_unless_negative(v);\n#else\n\ts64 c = raw_atomic64_read(v);\n\n\tdo {\n\t\tif (unlikely(c < 0))\n\t\t\treturn false;\n\t} while (!raw_atomic64_try_cmpxchg(v, &c, c + 1));\n\n\treturn true;\n#endif\n}\n\n \nstatic __always_inline bool\nraw_atomic64_dec_unless_positive(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_unless_positive)\n\treturn arch_atomic64_dec_unless_positive(v);\n#else\n\ts64 c = raw_atomic64_read(v);\n\n\tdo {\n\t\tif (unlikely(c > 0))\n\t\t\treturn false;\n\t} while (!raw_atomic64_try_cmpxchg(v, &c, c - 1));\n\n\treturn true;\n#endif\n}\n\n \nstatic __always_inline s64\nraw_atomic64_dec_if_positive(atomic64_t *v)\n{\n#if defined(arch_atomic64_dec_if_positive)\n\treturn arch_atomic64_dec_if_positive(v);\n#else\n\ts64 dec, c = raw_atomic64_read(v);\n\n\tdo {\n\t\tdec = c - 1;\n\t\tif (unlikely(dec < 0))\n\t\t\tbreak;\n\t} while (!raw_atomic64_try_cmpxchg(v, &c, dec));\n\n\treturn dec;\n#endif\n}\n\n#endif  \n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}