{
  "module_name": "bcache.h",
  "hash_id": "1ffa421aab93496212b0a7c2c5e176a2914daa6c45846bcb8cdddcc898e48288",
  "original_prompt": "Ingested from linux-6.6.14/include/trace/events/bcache.h",
  "human_readable_source": " \n#undef TRACE_SYSTEM\n#define TRACE_SYSTEM bcache\n\n#if !defined(_TRACE_BCACHE_H) || defined(TRACE_HEADER_MULTI_READ)\n#define _TRACE_BCACHE_H\n\n#include <linux/tracepoint.h>\n\nDECLARE_EVENT_CLASS(bcache_request,\n\tTP_PROTO(struct bcache_device *d, struct bio *bio),\n\tTP_ARGS(d, bio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(unsigned int,\torig_major\t\t)\n\t\t__field(unsigned int,\torig_minor\t\t)\n\t\t__field(sector_t,\tsector\t\t\t)\n\t\t__field(dev_t,\t\torig_sector\t\t)\n\t\t__field(unsigned int,\tnr_sector\t\t)\n\t\t__array(char,\t\trwbs,\t6\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= bio_dev(bio);\n\t\t__entry->orig_major\t= d->disk->major;\n\t\t__entry->orig_minor\t= d->disk->first_minor;\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->orig_sector\t= bio->bi_iter.bi_sector - 16;\n\t\t__entry->nr_sector\t= bio->bi_iter.bi_size >> 9;\n\t\tblk_fill_rwbs(__entry->rwbs, bio->bi_opf);\n\t),\n\n\tTP_printk(\"%d,%d %s %llu + %u (from %d,%d @ %llu)\",\n\t\t  MAJOR(__entry->dev), MINOR(__entry->dev),\n\t\t  __entry->rwbs, (unsigned long long)__entry->sector,\n\t\t  __entry->nr_sector, __entry->orig_major, __entry->orig_minor,\n\t\t  (unsigned long long)__entry->orig_sector)\n);\n\nDECLARE_EVENT_CLASS(bkey,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k),\n\n\tTP_STRUCT__entry(\n\t\t__field(u32,\tsize\t\t\t\t)\n\t\t__field(u32,\tinode\t\t\t\t)\n\t\t__field(u64,\toffset\t\t\t\t)\n\t\t__field(bool,\tdirty\t\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->inode\t= KEY_INODE(k);\n\t\t__entry->offset\t= KEY_OFFSET(k);\n\t\t__entry->size\t= KEY_SIZE(k);\n\t\t__entry->dirty\t= KEY_DIRTY(k);\n\t),\n\n\tTP_printk(\"%u:%llu len %u dirty %u\", __entry->inode,\n\t\t  __entry->offset, __entry->size, __entry->dirty)\n);\n\nDECLARE_EVENT_CLASS(btree_node,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b),\n\n\tTP_STRUCT__entry(\n\t\t__field(size_t,\t\tbucket\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->bucket\t= PTR_BUCKET_NR(b->c, &b->key, 0);\n\t),\n\n\tTP_printk(\"bucket %zu\", __entry->bucket)\n);\n\n \n\nDEFINE_EVENT(bcache_request, bcache_request_start,\n\tTP_PROTO(struct bcache_device *d, struct bio *bio),\n\tTP_ARGS(d, bio)\n);\n\nDEFINE_EVENT(bcache_request, bcache_request_end,\n\tTP_PROTO(struct bcache_device *d, struct bio *bio),\n\tTP_ARGS(d, bio)\n);\n\nDECLARE_EVENT_CLASS(bcache_bio,\n\tTP_PROTO(struct bio *bio),\n\tTP_ARGS(bio),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(sector_t,\tsector\t\t\t)\n\t\t__field(unsigned int,\tnr_sector\t\t)\n\t\t__array(char,\t\trwbs,\t6\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= bio_dev(bio);\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->nr_sector\t= bio->bi_iter.bi_size >> 9;\n\t\tblk_fill_rwbs(__entry->rwbs, bio->bi_opf);\n\t),\n\n\tTP_printk(\"%d,%d  %s %llu + %u\",\n\t\t  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->rwbs,\n\t\t  (unsigned long long)__entry->sector, __entry->nr_sector)\n);\n\nDEFINE_EVENT(bcache_bio, bcache_bypass_sequential,\n\tTP_PROTO(struct bio *bio),\n\tTP_ARGS(bio)\n);\n\nDEFINE_EVENT(bcache_bio, bcache_bypass_congested,\n\tTP_PROTO(struct bio *bio),\n\tTP_ARGS(bio)\n);\n\nTRACE_EVENT(bcache_read,\n\tTP_PROTO(struct bio *bio, bool hit, bool bypass),\n\tTP_ARGS(bio, hit, bypass),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(sector_t,\tsector\t\t\t)\n\t\t__field(unsigned int,\tnr_sector\t\t)\n\t\t__array(char,\t\trwbs,\t6\t\t)\n\t\t__field(bool,\t\tcache_hit\t\t)\n\t\t__field(bool,\t\tbypass\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= bio_dev(bio);\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->nr_sector\t= bio->bi_iter.bi_size >> 9;\n\t\tblk_fill_rwbs(__entry->rwbs, bio->bi_opf);\n\t\t__entry->cache_hit = hit;\n\t\t__entry->bypass = bypass;\n\t),\n\n\tTP_printk(\"%d,%d  %s %llu + %u hit %u bypass %u\",\n\t\t  MAJOR(__entry->dev), MINOR(__entry->dev),\n\t\t  __entry->rwbs, (unsigned long long)__entry->sector,\n\t\t  __entry->nr_sector, __entry->cache_hit, __entry->bypass)\n);\n\nTRACE_EVENT(bcache_write,\n\tTP_PROTO(struct cache_set *c, u64 inode, struct bio *bio,\n\t\tbool writeback, bool bypass),\n\tTP_ARGS(c, inode, bio, writeback, bypass),\n\n\tTP_STRUCT__entry(\n\t\t__array(char,\t\tuuid,\t16\t\t)\n\t\t__field(u64,\t\tinode\t\t\t)\n\t\t__field(sector_t,\tsector\t\t\t)\n\t\t__field(unsigned int,\tnr_sector\t\t)\n\t\t__array(char,\t\trwbs,\t6\t\t)\n\t\t__field(bool,\t\twriteback\t\t)\n\t\t__field(bool,\t\tbypass\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\tmemcpy(__entry->uuid, c->set_uuid, 16);\n\t\t__entry->inode\t\t= inode;\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->nr_sector\t= bio->bi_iter.bi_size >> 9;\n\t\tblk_fill_rwbs(__entry->rwbs, bio->bi_opf);\n\t\t__entry->writeback = writeback;\n\t\t__entry->bypass = bypass;\n\t),\n\n\tTP_printk(\"%pU inode %llu  %s %llu + %u hit %u bypass %u\",\n\t\t  __entry->uuid, __entry->inode,\n\t\t  __entry->rwbs, (unsigned long long)__entry->sector,\n\t\t  __entry->nr_sector, __entry->writeback, __entry->bypass)\n);\n\nDEFINE_EVENT(bcache_bio, bcache_read_retry,\n\tTP_PROTO(struct bio *bio),\n\tTP_ARGS(bio)\n);\n\nDEFINE_EVENT(bkey, bcache_cache_insert,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\n \n\nDECLARE_EVENT_CLASS(cache_set,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c),\n\n\tTP_STRUCT__entry(\n\t\t__array(char,\t\tuuid,\t16 )\n\t),\n\n\tTP_fast_assign(\n\t\tmemcpy(__entry->uuid, c->set_uuid, 16);\n\t),\n\n\tTP_printk(\"%pU\", __entry->uuid)\n);\n\nDEFINE_EVENT(bkey, bcache_journal_replay_key,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\nDEFINE_EVENT(cache_set, bcache_journal_full,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nDEFINE_EVENT(cache_set, bcache_journal_entry_full,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nTRACE_EVENT(bcache_journal_write,\n\tTP_PROTO(struct bio *bio, u32 keys),\n\tTP_ARGS(bio, keys),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(sector_t,\tsector\t\t\t)\n\t\t__field(unsigned int,\tnr_sector\t\t)\n\t\t__array(char,\t\trwbs,\t6\t\t)\n\t\t__field(u32,\t\tnr_keys\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= bio_dev(bio);\n\t\t__entry->sector\t\t= bio->bi_iter.bi_sector;\n\t\t__entry->nr_sector\t= bio->bi_iter.bi_size >> 9;\n\t\t__entry->nr_keys\t= keys;\n\t\tblk_fill_rwbs(__entry->rwbs, bio->bi_opf);\n\t),\n\n\tTP_printk(\"%d,%d  %s %llu + %u keys %u\",\n\t\t  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->rwbs,\n\t\t  (unsigned long long)__entry->sector, __entry->nr_sector,\n\t\t  __entry->nr_keys)\n);\n\n \n\nDEFINE_EVENT(cache_set, bcache_btree_cache_cannibalize,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nDEFINE_EVENT(btree_node, bcache_btree_read,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b)\n);\n\nTRACE_EVENT(bcache_btree_write,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b),\n\n\tTP_STRUCT__entry(\n\t\t__field(size_t,\t\tbucket\t\t\t)\n\t\t__field(unsigned,\tblock\t\t\t)\n\t\t__field(unsigned,\tkeys\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->bucket\t= PTR_BUCKET_NR(b->c, &b->key, 0);\n\t\t__entry->block\t= b->written;\n\t\t__entry->keys\t= b->keys.set[b->keys.nsets].data->keys;\n\t),\n\n\tTP_printk(\"bucket %zu written block %u + %u\",\n\t\t__entry->bucket, __entry->block, __entry->keys)\n);\n\nDEFINE_EVENT(btree_node, bcache_btree_node_alloc,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b)\n);\n\nDEFINE_EVENT(cache_set, bcache_btree_node_alloc_fail,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nDEFINE_EVENT(btree_node, bcache_btree_node_free,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b)\n);\n\nTRACE_EVENT(bcache_btree_gc_coalesce,\n\tTP_PROTO(unsigned nodes),\n\tTP_ARGS(nodes),\n\n\tTP_STRUCT__entry(\n\t\t__field(unsigned,\tnodes\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->nodes\t= nodes;\n\t),\n\n\tTP_printk(\"coalesced %u nodes\", __entry->nodes)\n);\n\nDEFINE_EVENT(cache_set, bcache_gc_start,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nDEFINE_EVENT(cache_set, bcache_gc_end,\n\tTP_PROTO(struct cache_set *c),\n\tTP_ARGS(c)\n);\n\nDEFINE_EVENT(bkey, bcache_gc_copy,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\nDEFINE_EVENT(bkey, bcache_gc_copy_collision,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\nTRACE_EVENT(bcache_btree_insert_key,\n\tTP_PROTO(struct btree *b, struct bkey *k, unsigned op, unsigned status),\n\tTP_ARGS(b, k, op, status),\n\n\tTP_STRUCT__entry(\n\t\t__field(u64,\tbtree_node\t\t\t)\n\t\t__field(u32,\tbtree_level\t\t\t)\n\t\t__field(u32,\tinode\t\t\t\t)\n\t\t__field(u64,\toffset\t\t\t\t)\n\t\t__field(u32,\tsize\t\t\t\t)\n\t\t__field(u8,\tdirty\t\t\t\t)\n\t\t__field(u8,\top\t\t\t\t)\n\t\t__field(u8,\tstatus\t\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->btree_node = PTR_BUCKET_NR(b->c, &b->key, 0);\n\t\t__entry->btree_level = b->level;\n\t\t__entry->inode\t= KEY_INODE(k);\n\t\t__entry->offset\t= KEY_OFFSET(k);\n\t\t__entry->size\t= KEY_SIZE(k);\n\t\t__entry->dirty\t= KEY_DIRTY(k);\n\t\t__entry->op = op;\n\t\t__entry->status = status;\n\t),\n\n\tTP_printk(\"%u for %u at %llu(%u): %u:%llu len %u dirty %u\",\n\t\t  __entry->status, __entry->op,\n\t\t  __entry->btree_node, __entry->btree_level,\n\t\t  __entry->inode, __entry->offset,\n\t\t  __entry->size, __entry->dirty)\n);\n\nDECLARE_EVENT_CLASS(btree_split,\n\tTP_PROTO(struct btree *b, unsigned keys),\n\tTP_ARGS(b, keys),\n\n\tTP_STRUCT__entry(\n\t\t__field(size_t,\t\tbucket\t\t\t)\n\t\t__field(unsigned,\tkeys\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->bucket\t= PTR_BUCKET_NR(b->c, &b->key, 0);\n\t\t__entry->keys\t= keys;\n\t),\n\n\tTP_printk(\"bucket %zu keys %u\", __entry->bucket, __entry->keys)\n);\n\nDEFINE_EVENT(btree_split, bcache_btree_node_split,\n\tTP_PROTO(struct btree *b, unsigned keys),\n\tTP_ARGS(b, keys)\n);\n\nDEFINE_EVENT(btree_split, bcache_btree_node_compact,\n\tTP_PROTO(struct btree *b, unsigned keys),\n\tTP_ARGS(b, keys)\n);\n\nDEFINE_EVENT(btree_node, bcache_btree_set_root,\n\tTP_PROTO(struct btree *b),\n\tTP_ARGS(b)\n);\n\nTRACE_EVENT(bcache_keyscan,\n\tTP_PROTO(unsigned nr_found,\n\t\t unsigned start_inode, uint64_t start_offset,\n\t\t unsigned end_inode, uint64_t end_offset),\n\tTP_ARGS(nr_found,\n\t\tstart_inode, start_offset,\n\t\tend_inode, end_offset),\n\n\tTP_STRUCT__entry(\n\t\t__field(__u32,\tnr_found\t\t\t)\n\t\t__field(__u32,\tstart_inode\t\t\t)\n\t\t__field(__u64,\tstart_offset\t\t\t)\n\t\t__field(__u32,\tend_inode\t\t\t)\n\t\t__field(__u64,\tend_offset\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->nr_found\t= nr_found;\n\t\t__entry->start_inode\t= start_inode;\n\t\t__entry->start_offset\t= start_offset;\n\t\t__entry->end_inode\t= end_inode;\n\t\t__entry->end_offset\t= end_offset;\n\t),\n\n\tTP_printk(\"found %u keys from %u:%llu to %u:%llu\", __entry->nr_found,\n\t\t  __entry->start_inode, __entry->start_offset,\n\t\t  __entry->end_inode, __entry->end_offset)\n);\n\n \n\nTRACE_EVENT(bcache_invalidate,\n\tTP_PROTO(struct cache *ca, size_t bucket),\n\tTP_ARGS(ca, bucket),\n\n\tTP_STRUCT__entry(\n\t\t__field(unsigned,\tsectors\t\t\t)\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(__u64,\t\toffset\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= ca->bdev->bd_dev;\n\t\t__entry->offset\t\t= bucket << ca->set->bucket_bits;\n\t\t__entry->sectors\t= GC_SECTORS_USED(&ca->buckets[bucket]);\n\t),\n\n\tTP_printk(\"invalidated %u sectors at %d,%d sector=%llu\",\n\t\t  __entry->sectors, MAJOR(__entry->dev),\n\t\t  MINOR(__entry->dev), __entry->offset)\n);\n\nTRACE_EVENT(bcache_alloc,\n\tTP_PROTO(struct cache *ca, size_t bucket),\n\tTP_ARGS(ca, bucket),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(__u64,\t\toffset\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= ca->bdev->bd_dev;\n\t\t__entry->offset\t\t= bucket << ca->set->bucket_bits;\n\t),\n\n\tTP_printk(\"allocated %d,%d sector=%llu\", MAJOR(__entry->dev),\n\t\t  MINOR(__entry->dev), __entry->offset)\n);\n\nTRACE_EVENT(bcache_alloc_fail,\n\tTP_PROTO(struct cache *ca, unsigned reserve),\n\tTP_ARGS(ca, reserve),\n\n\tTP_STRUCT__entry(\n\t\t__field(dev_t,\t\tdev\t\t\t)\n\t\t__field(unsigned,\tfree\t\t\t)\n\t\t__field(unsigned,\tfree_inc\t\t)\n\t\t__field(unsigned,\tblocked\t\t\t)\n\t),\n\n\tTP_fast_assign(\n\t\t__entry->dev\t\t= ca->bdev->bd_dev;\n\t\t__entry->free\t\t= fifo_used(&ca->free[reserve]);\n\t\t__entry->free_inc\t= fifo_used(&ca->free_inc);\n\t\t__entry->blocked\t= atomic_read(&ca->set->prio_blocked);\n\t),\n\n\tTP_printk(\"alloc fail %d,%d free %u free_inc %u blocked %u\",\n\t\t  MAJOR(__entry->dev), MINOR(__entry->dev), __entry->free,\n\t\t  __entry->free_inc, __entry->blocked)\n);\n\n \n\nDEFINE_EVENT(bkey, bcache_writeback,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\nDEFINE_EVENT(bkey, bcache_writeback_collision,\n\tTP_PROTO(struct bkey *k),\n\tTP_ARGS(k)\n);\n\n#endif  \n\n \n#include <trace/define_trace.h>\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}