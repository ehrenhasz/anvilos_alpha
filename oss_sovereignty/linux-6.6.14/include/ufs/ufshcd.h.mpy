{
  "module_name": "ufshcd.h",
  "hash_id": "55f1c6f92f2a850fdd9871b62e2fbdea6235433502f1a54072422b9df16cb180",
  "original_prompt": "Ingested from linux-6.6.14/include/ufs/ufshcd.h",
  "human_readable_source": " \n \n\n#ifndef _UFSHCD_H\n#define _UFSHCD_H\n\n#include <linux/bitfield.h>\n#include <linux/blk-crypto-profile.h>\n#include <linux/blk-mq.h>\n#include <linux/devfreq.h>\n#include <linux/msi.h>\n#include <linux/pm_runtime.h>\n#include <linux/dma-direction.h>\n#include <scsi/scsi_device.h>\n#include <scsi/scsi_host.h>\n#include <ufs/unipro.h>\n#include <ufs/ufs.h>\n#include <ufs/ufs_quirks.h>\n#include <ufs/ufshci.h>\n\n#define UFSHCD \"ufshcd\"\n\nstruct ufs_hba;\n\nenum dev_cmd_type {\n\tDEV_CMD_TYPE_NOP\t\t= 0x0,\n\tDEV_CMD_TYPE_QUERY\t\t= 0x1,\n\tDEV_CMD_TYPE_RPMB\t\t= 0x2,\n};\n\nenum ufs_event_type {\n\t \n\tUFS_EVT_PA_ERR = 0,\n\tUFS_EVT_DL_ERR,\n\tUFS_EVT_NL_ERR,\n\tUFS_EVT_TL_ERR,\n\tUFS_EVT_DME_ERR,\n\n\t \n\tUFS_EVT_AUTO_HIBERN8_ERR,\n\tUFS_EVT_FATAL_ERR,\n\tUFS_EVT_LINK_STARTUP_FAIL,\n\tUFS_EVT_RESUME_ERR,\n\tUFS_EVT_SUSPEND_ERR,\n\tUFS_EVT_WL_SUSP_ERR,\n\tUFS_EVT_WL_RES_ERR,\n\n\t \n\tUFS_EVT_DEV_RESET,\n\tUFS_EVT_HOST_RESET,\n\tUFS_EVT_ABORT,\n\n\tUFS_EVT_CNT,\n};\n\n \nstruct uic_command {\n\tu32 command;\n\tu32 argument1;\n\tu32 argument2;\n\tu32 argument3;\n\tint cmd_active;\n\tstruct completion done;\n};\n\n \nenum ufs_pm_op {\n\tUFS_RUNTIME_PM,\n\tUFS_SYSTEM_PM,\n\tUFS_SHUTDOWN_PM,\n};\n\n \nenum uic_link_state {\n\tUIC_LINK_OFF_STATE\t= 0,  \n\tUIC_LINK_ACTIVE_STATE\t= 1,  \n\tUIC_LINK_HIBERN8_STATE\t= 2,  \n\tUIC_LINK_BROKEN_STATE\t= 3,  \n};\n\n#define ufshcd_is_link_off(hba) ((hba)->uic_link_state == UIC_LINK_OFF_STATE)\n#define ufshcd_is_link_active(hba) ((hba)->uic_link_state == \\\n\t\t\t\t    UIC_LINK_ACTIVE_STATE)\n#define ufshcd_is_link_hibern8(hba) ((hba)->uic_link_state == \\\n\t\t\t\t    UIC_LINK_HIBERN8_STATE)\n#define ufshcd_is_link_broken(hba) ((hba)->uic_link_state == \\\n\t\t\t\t   UIC_LINK_BROKEN_STATE)\n#define ufshcd_set_link_off(hba) ((hba)->uic_link_state = UIC_LINK_OFF_STATE)\n#define ufshcd_set_link_active(hba) ((hba)->uic_link_state = \\\n\t\t\t\t    UIC_LINK_ACTIVE_STATE)\n#define ufshcd_set_link_hibern8(hba) ((hba)->uic_link_state = \\\n\t\t\t\t    UIC_LINK_HIBERN8_STATE)\n#define ufshcd_set_link_broken(hba) ((hba)->uic_link_state = \\\n\t\t\t\t    UIC_LINK_BROKEN_STATE)\n\n#define ufshcd_set_ufs_dev_active(h) \\\n\t((h)->curr_dev_pwr_mode = UFS_ACTIVE_PWR_MODE)\n#define ufshcd_set_ufs_dev_sleep(h) \\\n\t((h)->curr_dev_pwr_mode = UFS_SLEEP_PWR_MODE)\n#define ufshcd_set_ufs_dev_poweroff(h) \\\n\t((h)->curr_dev_pwr_mode = UFS_POWERDOWN_PWR_MODE)\n#define ufshcd_set_ufs_dev_deepsleep(h) \\\n\t((h)->curr_dev_pwr_mode = UFS_DEEPSLEEP_PWR_MODE)\n#define ufshcd_is_ufs_dev_active(h) \\\n\t((h)->curr_dev_pwr_mode == UFS_ACTIVE_PWR_MODE)\n#define ufshcd_is_ufs_dev_sleep(h) \\\n\t((h)->curr_dev_pwr_mode == UFS_SLEEP_PWR_MODE)\n#define ufshcd_is_ufs_dev_poweroff(h) \\\n\t((h)->curr_dev_pwr_mode == UFS_POWERDOWN_PWR_MODE)\n#define ufshcd_is_ufs_dev_deepsleep(h) \\\n\t((h)->curr_dev_pwr_mode == UFS_DEEPSLEEP_PWR_MODE)\n\n \nenum ufs_pm_level {\n\tUFS_PM_LVL_0,\n\tUFS_PM_LVL_1,\n\tUFS_PM_LVL_2,\n\tUFS_PM_LVL_3,\n\tUFS_PM_LVL_4,\n\tUFS_PM_LVL_5,\n\tUFS_PM_LVL_6,\n\tUFS_PM_LVL_MAX\n};\n\nstruct ufs_pm_lvl_states {\n\tenum ufs_dev_pwr_mode dev_state;\n\tenum uic_link_state link_state;\n};\n\n \nstruct ufshcd_lrb {\n\tstruct utp_transfer_req_desc *utr_descriptor_ptr;\n\tstruct utp_upiu_req *ucd_req_ptr;\n\tstruct utp_upiu_rsp *ucd_rsp_ptr;\n\tstruct ufshcd_sg_entry *ucd_prdt_ptr;\n\n\tdma_addr_t utrd_dma_addr;\n\tdma_addr_t ucd_req_dma_addr;\n\tdma_addr_t ucd_rsp_dma_addr;\n\tdma_addr_t ucd_prdt_dma_addr;\n\n\tstruct scsi_cmnd *cmd;\n\tint scsi_status;\n\n\tint command_type;\n\tint task_tag;\n\tu8 lun;  \n\tbool intr_cmd;\n\tktime_t issue_time_stamp;\n\tu64 issue_time_stamp_local_clock;\n\tktime_t compl_time_stamp;\n\tu64 compl_time_stamp_local_clock;\n#ifdef CONFIG_SCSI_UFS_CRYPTO\n\tint crypto_key_slot;\n\tu64 data_unit_num;\n#endif\n\n\tbool req_abort_skip;\n};\n\n \nstruct ufs_query_req {\n\tu8 query_func;\n\tstruct utp_upiu_query upiu_req;\n};\n\n \nstruct ufs_query_res {\n\tstruct utp_upiu_query upiu_res;\n};\n\n \nstruct ufs_query {\n\tstruct ufs_query_req request;\n\tu8 *descriptor;\n\tstruct ufs_query_res response;\n};\n\n \nstruct ufs_dev_cmd {\n\tenum dev_cmd_type type;\n\tstruct mutex lock;\n\tstruct completion *complete;\n\tstruct ufs_query query;\n};\n\n \nstruct ufs_clk_info {\n\tstruct list_head list;\n\tstruct clk *clk;\n\tconst char *name;\n\tu32 max_freq;\n\tu32 min_freq;\n\tu32 curr_freq;\n\tbool keep_link_active;\n\tbool enabled;\n};\n\nenum ufs_notify_change_status {\n\tPRE_CHANGE,\n\tPOST_CHANGE,\n};\n\nstruct ufs_pa_layer_attr {\n\tu32 gear_rx;\n\tu32 gear_tx;\n\tu32 lane_rx;\n\tu32 lane_tx;\n\tu32 pwr_rx;\n\tu32 pwr_tx;\n\tu32 hs_rate;\n};\n\nstruct ufs_pwr_mode_info {\n\tbool is_valid;\n\tstruct ufs_pa_layer_attr info;\n};\n\n \nstruct ufs_hba_variant_ops {\n\tconst char *name;\n\tint\t(*init)(struct ufs_hba *);\n\tvoid    (*exit)(struct ufs_hba *);\n\tu32\t(*get_ufs_hci_version)(struct ufs_hba *);\n\tint\t(*clk_scale_notify)(struct ufs_hba *, bool,\n\t\t\t\t    enum ufs_notify_change_status);\n\tint\t(*setup_clocks)(struct ufs_hba *, bool,\n\t\t\t\tenum ufs_notify_change_status);\n\tint\t(*hce_enable_notify)(struct ufs_hba *,\n\t\t\t\t     enum ufs_notify_change_status);\n\tint\t(*link_startup_notify)(struct ufs_hba *,\n\t\t\t\t       enum ufs_notify_change_status);\n\tint\t(*pwr_change_notify)(struct ufs_hba *,\n\t\t\t\t\tenum ufs_notify_change_status status,\n\t\t\t\t\tstruct ufs_pa_layer_attr *,\n\t\t\t\t\tstruct ufs_pa_layer_attr *);\n\tvoid\t(*setup_xfer_req)(struct ufs_hba *hba, int tag,\n\t\t\t\t  bool is_scsi_cmd);\n\tvoid\t(*setup_task_mgmt)(struct ufs_hba *, int, u8);\n\tvoid    (*hibern8_notify)(struct ufs_hba *, enum uic_cmd_dme,\n\t\t\t\t\tenum ufs_notify_change_status);\n\tint\t(*apply_dev_quirks)(struct ufs_hba *hba);\n\tvoid\t(*fixup_dev_quirks)(struct ufs_hba *hba);\n\tint     (*suspend)(struct ufs_hba *, enum ufs_pm_op,\n\t\t\t\t\tenum ufs_notify_change_status);\n\tint     (*resume)(struct ufs_hba *, enum ufs_pm_op);\n\tvoid\t(*dbg_register_dump)(struct ufs_hba *hba);\n\tint\t(*phy_initialization)(struct ufs_hba *);\n\tint\t(*device_reset)(struct ufs_hba *hba);\n\tvoid\t(*config_scaling_param)(struct ufs_hba *hba,\n\t\t\t\tstruct devfreq_dev_profile *profile,\n\t\t\t\tstruct devfreq_simple_ondemand_data *data);\n\tint\t(*program_key)(struct ufs_hba *hba,\n\t\t\t       const union ufs_crypto_cfg_entry *cfg, int slot);\n\tvoid\t(*event_notify)(struct ufs_hba *hba,\n\t\t\t\tenum ufs_event_type evt, void *data);\n\tvoid\t(*reinit_notify)(struct ufs_hba *);\n\tint\t(*mcq_config_resource)(struct ufs_hba *hba);\n\tint\t(*get_hba_mac)(struct ufs_hba *hba);\n\tint\t(*op_runtime_config)(struct ufs_hba *hba);\n\tint\t(*get_outstanding_cqs)(struct ufs_hba *hba,\n\t\t\t\t       unsigned long *ocqs);\n\tint\t(*config_esi)(struct ufs_hba *hba);\n};\n\n \nenum clk_gating_state {\n\tCLKS_OFF,\n\tCLKS_ON,\n\tREQ_CLKS_OFF,\n\tREQ_CLKS_ON,\n};\n\n \nstruct ufs_clk_gating {\n\tstruct delayed_work gate_work;\n\tstruct work_struct ungate_work;\n\tenum clk_gating_state state;\n\tunsigned long delay_ms;\n\tbool is_suspended;\n\tstruct device_attribute delay_attr;\n\tstruct device_attribute enable_attr;\n\tbool is_enabled;\n\tbool is_initialized;\n\tint active_reqs;\n\tstruct workqueue_struct *clk_gating_workq;\n};\n\n \nstruct ufs_clk_scaling {\n\tint active_reqs;\n\tunsigned long tot_busy_t;\n\tktime_t window_start_t;\n\tktime_t busy_start_t;\n\tstruct device_attribute enable_attr;\n\tstruct ufs_pa_layer_attr saved_pwr_info;\n\tstruct workqueue_struct *workq;\n\tstruct work_struct suspend_work;\n\tstruct work_struct resume_work;\n\tu32 min_gear;\n\tbool is_enabled;\n\tbool is_allowed;\n\tbool is_initialized;\n\tbool is_busy_started;\n\tbool is_suspended;\n};\n\n#define UFS_EVENT_HIST_LENGTH 8\n \nstruct ufs_event_hist {\n\tint pos;\n\tu32 val[UFS_EVENT_HIST_LENGTH];\n\tu64 tstamp[UFS_EVENT_HIST_LENGTH];\n\tunsigned long long cnt;\n};\n\n \nstruct ufs_stats {\n\tu32 last_intr_status;\n\tu64 last_intr_ts;\n\n\tu32 hibern8_exit_cnt;\n\tu64 last_hibern8_exit_tstamp;\n\tstruct ufs_event_hist event[UFS_EVT_CNT];\n};\n\n \nenum ufshcd_state {\n\tUFSHCD_STATE_RESET,\n\tUFSHCD_STATE_OPERATIONAL,\n\tUFSHCD_STATE_EH_SCHEDULED_NON_FATAL,\n\tUFSHCD_STATE_EH_SCHEDULED_FATAL,\n\tUFSHCD_STATE_ERROR,\n};\n\nenum ufshcd_quirks {\n\t \n\tUFSHCD_QUIRK_BROKEN_INTR_AGGR\t\t\t= 1 << 0,\n\n\t \n\tUFSHCD_QUIRK_DELAY_BEFORE_DME_CMDS\t\t= 1 << 1,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_LCC\t\t\t\t= 1 << 2,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_PA_RXHSUNTERMCAP\t\t= 1 << 3,\n\n\t \n\tUFSHCD_QUIRK_DME_PEER_ACCESS_AUTO_MODE\t\t= 1 << 4,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_UFS_HCI_VERSION\t\t= 1 << 5,\n\n\t \n\tUFSHCI_QUIRK_BROKEN_REQ_LIST_CLR\t\t= 1 << 6,\n\n\t \n\tUFSHCI_QUIRK_SKIP_RESET_INTR_AGGR\t\t= 1 << 7,\n\n\t \n\tUFSHCI_QUIRK_BROKEN_HCE\t\t\t\t= 1 << 8,\n\n\t \n\tUFSHCD_QUIRK_PRDT_BYTE_GRAN\t\t\t= 1 << 9,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_OCS_FATAL_ERROR\t\t= 1 << 10,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_AUTO_HIBERN8\t\t= 1 << 11,\n\n\t \n\tUFSHCI_QUIRK_SKIP_MANUAL_WB_FLUSH_CTRL\t\t= 1 << 12,\n\n\t \n\tUFSHCD_QUIRK_SKIP_DEF_UNIPRO_TIMEOUT_SETTING = 1 << 13,\n\n\t \n\tUFSHCD_QUIRK_4KB_DMA_ALIGNMENT\t\t\t= 1 << 14,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_UIC_CMD\t\t\t= 1 << 15,\n\n\t \n\tUFSHCD_QUIRK_SKIP_PH_CONFIGURATION\t\t= 1 << 16,\n\n\t \n\tUFSHCD_QUIRK_BROKEN_64BIT_ADDRESS\t\t= 1 << 17,\n\n\t \n\tUFSHCD_QUIRK_HIBERN_FASTAUTO\t\t\t= 1 << 18,\n\n\t \n\tUFSHCD_QUIRK_REINIT_AFTER_MAX_GEAR_SWITCH       = 1 << 19,\n\n\t \n\tUFSHCD_QUIRK_MCQ_BROKEN_INTR\t\t\t= 1 << 20,\n\n\t \n\tUFSHCD_QUIRK_MCQ_BROKEN_RTC\t\t\t= 1 << 21,\n};\n\nenum ufshcd_caps {\n\t \n\tUFSHCD_CAP_CLK_GATING\t\t\t\t= 1 << 0,\n\n\t \n\tUFSHCD_CAP_HIBERN8_WITH_CLK_GATING\t\t= 1 << 1,\n\n\t \n\tUFSHCD_CAP_CLK_SCALING\t\t\t\t= 1 << 2,\n\n\t \n\tUFSHCD_CAP_AUTO_BKOPS_SUSPEND\t\t\t= 1 << 3,\n\n\t \n\tUFSHCD_CAP_INTR_AGGR\t\t\t\t= 1 << 4,\n\n\t \n\tUFSHCD_CAP_KEEP_AUTO_BKOPS_ENABLED_EXCEPT_SUSPEND = 1 << 5,\n\n\t \n\tUFSHCD_CAP_RPM_AUTOSUSPEND\t\t\t= 1 << 6,\n\n\t \n\tUFSHCD_CAP_WB_EN\t\t\t\t= 1 << 7,\n\n\t \n\tUFSHCD_CAP_CRYPTO\t\t\t\t= 1 << 8,\n\n\t \n\tUFSHCD_CAP_AGGR_POWER_COLLAPSE\t\t\t= 1 << 9,\n\n\t \n\tUFSHCD_CAP_DEEPSLEEP\t\t\t\t= 1 << 10,\n\n\t \n\tUFSHCD_CAP_TEMP_NOTIF\t\t\t\t= 1 << 11,\n\n\t \n\tUFSHCD_CAP_WB_WITH_CLK_SCALING\t\t\t= 1 << 12,\n};\n\nstruct ufs_hba_variant_params {\n\tstruct devfreq_dev_profile devfreq_profile;\n\tstruct devfreq_simple_ondemand_data ondemand_data;\n\tu16 hba_enable_delay_us;\n\tu32 wb_flush_threshold;\n};\n\nstruct ufs_hba_monitor {\n\tunsigned long chunk_size;\n\n\tunsigned long nr_sec_rw[2];\n\tktime_t total_busy[2];\n\n\tunsigned long nr_req[2];\n\t \n\tktime_t lat_sum[2];\n\tktime_t lat_max[2];\n\tktime_t lat_min[2];\n\n\tu32 nr_queued[2];\n\tktime_t busy_start_ts[2];\n\n\tktime_t enabled_ts;\n\tbool enabled;\n};\n\n \nstruct ufshcd_res_info {\n\tconst char *name;\n\tstruct resource *resource;\n\tvoid __iomem *base;\n};\n\nenum ufshcd_res {\n\tRES_UFS,\n\tRES_MCQ,\n\tRES_MCQ_SQD,\n\tRES_MCQ_SQIS,\n\tRES_MCQ_CQD,\n\tRES_MCQ_CQIS,\n\tRES_MCQ_VS,\n\tRES_MAX,\n};\n\n \nstruct ufshcd_mcq_opr_info_t {\n\tunsigned long offset;\n\tunsigned long stride;\n\tvoid __iomem *base;\n};\n\nenum ufshcd_mcq_opr {\n\tOPR_SQD,\n\tOPR_SQIS,\n\tOPR_CQD,\n\tOPR_CQIS,\n\tOPR_MAX,\n};\n\n \nstruct ufs_hba {\n\tvoid __iomem *mmio_base;\n\n\t \n\tstruct utp_transfer_cmd_desc *ucdl_base_addr;\n\tstruct utp_transfer_req_desc *utrdl_base_addr;\n\tstruct utp_task_req_desc *utmrdl_base_addr;\n\n\t \n\tdma_addr_t ucdl_dma_addr;\n\tdma_addr_t utrdl_dma_addr;\n\tdma_addr_t utmrdl_dma_addr;\n\n\tstruct Scsi_Host *host;\n\tstruct device *dev;\n\tstruct scsi_device *ufs_device_wlun;\n\n#ifdef CONFIG_SCSI_UFS_HWMON\n\tstruct device *hwmon_device;\n#endif\n\n\tenum ufs_dev_pwr_mode curr_dev_pwr_mode;\n\tenum uic_link_state uic_link_state;\n\t \n\tenum ufs_pm_level rpm_lvl;\n\t \n\tenum ufs_pm_level spm_lvl;\n\tint pm_op_in_progress;\n\n\t \n\tu32 ahit;\n\n\tstruct ufshcd_lrb *lrb;\n\n\tunsigned long outstanding_tasks;\n\tspinlock_t outstanding_lock;\n\tunsigned long outstanding_reqs;\n\n\tu32 capabilities;\n\tint nutrs;\n\tu32 mcq_capabilities;\n\tint nutmrs;\n\tu32 reserved_slot;\n\tu32 ufs_version;\n\tconst struct ufs_hba_variant_ops *vops;\n\tstruct ufs_hba_variant_params *vps;\n\tvoid *priv;\n#ifdef CONFIG_SCSI_UFS_VARIABLE_SG_ENTRY_SIZE\n\tsize_t sg_entry_size;\n#endif\n\tunsigned int irq;\n\tbool is_irq_enabled;\n\tenum ufs_ref_clk_freq dev_ref_clk_freq;\n\n\tunsigned int quirks;\t \n\n\t \n\tunsigned int dev_quirks;\n\n\tstruct blk_mq_tag_set tmf_tag_set;\n\tstruct request_queue *tmf_queue;\n\tstruct request **tmf_rqs;\n\n\tstruct uic_command *active_uic_cmd;\n\tstruct mutex uic_cmd_mutex;\n\tstruct completion *uic_async_done;\n\n\tenum ufshcd_state ufshcd_state;\n\tu32 eh_flags;\n\tu32 intr_mask;\n\tu16 ee_ctrl_mask;\n\tu16 ee_drv_mask;\n\tu16 ee_usr_mask;\n\tstruct mutex ee_ctrl_mutex;\n\tbool is_powered;\n\tbool shutting_down;\n\tstruct semaphore host_sem;\n\n\t \n\tstruct workqueue_struct *eh_wq;\n\tstruct work_struct eh_work;\n\tstruct work_struct eeh_work;\n\n\t \n\tu32 errors;\n\tu32 uic_error;\n\tu32 saved_err;\n\tu32 saved_uic_err;\n\tstruct ufs_stats ufs_stats;\n\tbool force_reset;\n\tbool force_pmc;\n\tbool silence_err_logs;\n\n\t \n\tstruct ufs_dev_cmd dev_cmd;\n\tktime_t last_dme_cmd_tstamp;\n\tint nop_out_timeout;\n\n\t \n\tstruct ufs_dev_info dev_info;\n\tbool auto_bkops_enabled;\n\tstruct ufs_vreg_info vreg_info;\n\tstruct list_head clk_list_head;\n\n\t \n\tint req_abort_count;\n\n\t \n\tu32 lanes_per_direction;\n\tstruct ufs_pa_layer_attr pwr_info;\n\tstruct ufs_pwr_mode_info max_pwr_info;\n\n\tstruct ufs_clk_gating clk_gating;\n\t \n\tu32 caps;\n\n\tstruct devfreq *devfreq;\n\tstruct ufs_clk_scaling clk_scaling;\n\tbool system_suspending;\n\tbool is_sys_suspended;\n\n\tenum bkops_status urgent_bkops_lvl;\n\tbool is_urgent_bkops_lvl_checked;\n\n\tstruct mutex wb_mutex;\n\tstruct rw_semaphore clk_scaling_lock;\n\tatomic_t scsi_block_reqs_cnt;\n\n\tstruct device\t\tbsg_dev;\n\tstruct request_queue\t*bsg_queue;\n\tstruct delayed_work rpm_dev_flush_recheck_work;\n\n\tstruct ufs_hba_monitor\tmonitor;\n\n#ifdef CONFIG_SCSI_UFS_CRYPTO\n\tunion ufs_crypto_capabilities crypto_capabilities;\n\tunion ufs_crypto_cap_entry *crypto_cap_array;\n\tu32 crypto_cfg_register;\n\tstruct blk_crypto_profile crypto_profile;\n#endif\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry *debugfs_root;\n\tstruct delayed_work debugfs_ee_work;\n\tu32 debugfs_ee_rate_limit_ms;\n#endif\n\tu32 luns_avail;\n\tunsigned int nr_hw_queues;\n\tunsigned int nr_queues[HCTX_MAX_TYPES];\n\tbool complete_put;\n\tbool ext_iid_sup;\n\tbool scsi_host_added;\n\tbool mcq_sup;\n\tbool mcq_enabled;\n\tstruct ufshcd_res_info res[RES_MAX];\n\tvoid __iomem *mcq_base;\n\tstruct ufs_hw_queue *uhq;\n\tstruct ufs_hw_queue *dev_cmd_queue;\n\tstruct ufshcd_mcq_opr_info_t mcq_opr[OPR_MAX];\n};\n\n \nstruct ufs_hw_queue {\n\tvoid __iomem *mcq_sq_head;\n\tvoid __iomem *mcq_sq_tail;\n\tvoid __iomem *mcq_cq_head;\n\tvoid __iomem *mcq_cq_tail;\n\n\tstruct utp_transfer_req_desc *sqe_base_addr;\n\tdma_addr_t sqe_dma_addr;\n\tstruct cq_entry *cqe_base_addr;\n\tdma_addr_t cqe_dma_addr;\n\tu32 max_entries;\n\tu32 id;\n\tu32 sq_tail_slot;\n\tspinlock_t sq_lock;\n\tu32 cq_tail_slot;\n\tu32 cq_head_slot;\n\tspinlock_t cq_lock;\n\t \n\tstruct mutex sq_mutex;\n};\n\nstatic inline bool is_mcq_enabled(struct ufs_hba *hba)\n{\n\treturn hba->mcq_enabled;\n}\n\n#ifdef CONFIG_SCSI_UFS_VARIABLE_SG_ENTRY_SIZE\nstatic inline size_t ufshcd_sg_entry_size(const struct ufs_hba *hba)\n{\n\treturn hba->sg_entry_size;\n}\n\nstatic inline void ufshcd_set_sg_entry_size(struct ufs_hba *hba, size_t sg_entry_size)\n{\n\tWARN_ON_ONCE(sg_entry_size < sizeof(struct ufshcd_sg_entry));\n\thba->sg_entry_size = sg_entry_size;\n}\n#else\nstatic inline size_t ufshcd_sg_entry_size(const struct ufs_hba *hba)\n{\n\treturn sizeof(struct ufshcd_sg_entry);\n}\n\n#define ufshcd_set_sg_entry_size(hba, sg_entry_size)                   \\\n\t({ (void)(hba); BUILD_BUG_ON(sg_entry_size != sizeof(struct ufshcd_sg_entry)); })\n#endif\n\nstatic inline size_t ufshcd_get_ucd_size(const struct ufs_hba *hba)\n{\n\treturn sizeof(struct utp_transfer_cmd_desc) + SG_ALL * ufshcd_sg_entry_size(hba);\n}\n\n \nstatic inline bool ufshcd_is_clkgating_allowed(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_CLK_GATING;\n}\nstatic inline bool ufshcd_can_hibern8_during_gating(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_HIBERN8_WITH_CLK_GATING;\n}\nstatic inline int ufshcd_is_clkscaling_supported(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_CLK_SCALING;\n}\nstatic inline bool ufshcd_can_autobkops_during_suspend(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_AUTO_BKOPS_SUSPEND;\n}\nstatic inline bool ufshcd_is_rpm_autosuspend_allowed(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_RPM_AUTOSUSPEND;\n}\n\nstatic inline bool ufshcd_is_intr_aggr_allowed(struct ufs_hba *hba)\n{\n\treturn (hba->caps & UFSHCD_CAP_INTR_AGGR) &&\n\t\t!(hba->quirks & UFSHCD_QUIRK_BROKEN_INTR_AGGR);\n}\n\nstatic inline bool ufshcd_can_aggressive_pc(struct ufs_hba *hba)\n{\n\treturn !!(ufshcd_is_link_hibern8(hba) &&\n\t\t  (hba->caps & UFSHCD_CAP_AGGR_POWER_COLLAPSE));\n}\n\nstatic inline bool ufshcd_is_auto_hibern8_supported(struct ufs_hba *hba)\n{\n\treturn (hba->capabilities & MASK_AUTO_HIBERN8_SUPPORT) &&\n\t\t!(hba->quirks & UFSHCD_QUIRK_BROKEN_AUTO_HIBERN8);\n}\n\nstatic inline bool ufshcd_is_auto_hibern8_enabled(struct ufs_hba *hba)\n{\n\treturn FIELD_GET(UFSHCI_AHIBERN8_TIMER_MASK, hba->ahit);\n}\n\nstatic inline bool ufshcd_is_wb_allowed(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_WB_EN;\n}\n\nstatic inline bool ufshcd_enable_wb_if_scaling_up(struct ufs_hba *hba)\n{\n\treturn hba->caps & UFSHCD_CAP_WB_WITH_CLK_SCALING;\n}\n\n#define ufsmcq_writel(hba, val, reg)\t\\\n\twritel((val), (hba)->mcq_base + (reg))\n#define ufsmcq_readl(hba, reg)\t\\\n\treadl((hba)->mcq_base + (reg))\n\n#define ufsmcq_writelx(hba, val, reg)\t\\\n\twritel_relaxed((val), (hba)->mcq_base + (reg))\n#define ufsmcq_readlx(hba, reg)\t\\\n\treadl_relaxed((hba)->mcq_base + (reg))\n\n#define ufshcd_writel(hba, val, reg)\t\\\n\twritel((val), (hba)->mmio_base + (reg))\n#define ufshcd_readl(hba, reg)\t\\\n\treadl((hba)->mmio_base + (reg))\n\n \nstatic inline void ufshcd_rmwl(struct ufs_hba *hba, u32 mask, u32 val, u32 reg)\n{\n\tu32 tmp;\n\n\ttmp = ufshcd_readl(hba, reg);\n\ttmp &= ~mask;\n\ttmp |= (val & mask);\n\tufshcd_writel(hba, tmp, reg);\n}\n\nint ufshcd_alloc_host(struct device *, struct ufs_hba **);\nvoid ufshcd_dealloc_host(struct ufs_hba *);\nint ufshcd_hba_enable(struct ufs_hba *hba);\nint ufshcd_init(struct ufs_hba *, void __iomem *, unsigned int);\nint ufshcd_link_recovery(struct ufs_hba *hba);\nint ufshcd_make_hba_operational(struct ufs_hba *hba);\nvoid ufshcd_remove(struct ufs_hba *);\nint ufshcd_uic_hibern8_enter(struct ufs_hba *hba);\nint ufshcd_uic_hibern8_exit(struct ufs_hba *hba);\nvoid ufshcd_delay_us(unsigned long us, unsigned long tolerance);\nvoid ufshcd_parse_dev_ref_clk_freq(struct ufs_hba *hba, struct clk *refclk);\nvoid ufshcd_update_evt_hist(struct ufs_hba *hba, u32 id, u32 val);\nvoid ufshcd_hba_stop(struct ufs_hba *hba);\nvoid ufshcd_schedule_eh_work(struct ufs_hba *hba);\nvoid ufshcd_mcq_config_mac(struct ufs_hba *hba, u32 max_active_cmds);\nu32 ufshcd_mcq_read_cqis(struct ufs_hba *hba, int i);\nvoid ufshcd_mcq_write_cqis(struct ufs_hba *hba, u32 val, int i);\nunsigned long ufshcd_mcq_poll_cqe_lock(struct ufs_hba *hba,\n\t\t\t\t\t struct ufs_hw_queue *hwq);\nvoid ufshcd_mcq_make_queues_operational(struct ufs_hba *hba);\nvoid ufshcd_mcq_enable_esi(struct ufs_hba *hba);\nvoid ufshcd_mcq_config_esi(struct ufs_hba *hba, struct msi_msg *msg);\n\n \nstatic inline void ufshcd_set_variant(struct ufs_hba *hba, void *variant)\n{\n\tBUG_ON(!hba);\n\thba->priv = variant;\n}\n\n \nstatic inline void *ufshcd_get_variant(struct ufs_hba *hba)\n{\n\tBUG_ON(!hba);\n\treturn hba->priv;\n}\n\n#ifdef CONFIG_PM\nextern int ufshcd_runtime_suspend(struct device *dev);\nextern int ufshcd_runtime_resume(struct device *dev);\n#endif\n#ifdef CONFIG_PM_SLEEP\nextern int ufshcd_system_suspend(struct device *dev);\nextern int ufshcd_system_resume(struct device *dev);\nextern int ufshcd_system_freeze(struct device *dev);\nextern int ufshcd_system_thaw(struct device *dev);\nextern int ufshcd_system_restore(struct device *dev);\n#endif\n\nextern int ufshcd_dme_configure_adapt(struct ufs_hba *hba,\n\t\t\t\t      int agreed_gear,\n\t\t\t\t      int adapt_val);\nextern int ufshcd_dme_set_attr(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t       u8 attr_set, u32 mib_val, u8 peer);\nextern int ufshcd_dme_get_attr(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t       u32 *mib_val, u8 peer);\nextern int ufshcd_config_pwr_mode(struct ufs_hba *hba,\n\t\t\tstruct ufs_pa_layer_attr *desired_pwr_mode);\nextern int ufshcd_uic_change_pwr_mode(struct ufs_hba *hba, u8 mode);\n\n \n#define DME_LOCAL\t0\n#define DME_PEER\t1\n#define ATTR_SET_NOR\t0\t \n#define ATTR_SET_ST\t1\t \n\nstatic inline int ufshcd_dme_set(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t\t u32 mib_val)\n{\n\treturn ufshcd_dme_set_attr(hba, attr_sel, ATTR_SET_NOR,\n\t\t\t\t   mib_val, DME_LOCAL);\n}\n\nstatic inline int ufshcd_dme_st_set(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t\t    u32 mib_val)\n{\n\treturn ufshcd_dme_set_attr(hba, attr_sel, ATTR_SET_ST,\n\t\t\t\t   mib_val, DME_LOCAL);\n}\n\nstatic inline int ufshcd_dme_peer_set(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t\t      u32 mib_val)\n{\n\treturn ufshcd_dme_set_attr(hba, attr_sel, ATTR_SET_NOR,\n\t\t\t\t   mib_val, DME_PEER);\n}\n\nstatic inline int ufshcd_dme_peer_st_set(struct ufs_hba *hba, u32 attr_sel,\n\t\t\t\t\t u32 mib_val)\n{\n\treturn ufshcd_dme_set_attr(hba, attr_sel, ATTR_SET_ST,\n\t\t\t\t   mib_val, DME_PEER);\n}\n\nstatic inline int ufshcd_dme_get(struct ufs_hba *hba,\n\t\t\t\t u32 attr_sel, u32 *mib_val)\n{\n\treturn ufshcd_dme_get_attr(hba, attr_sel, mib_val, DME_LOCAL);\n}\n\nstatic inline int ufshcd_dme_peer_get(struct ufs_hba *hba,\n\t\t\t\t      u32 attr_sel, u32 *mib_val)\n{\n\treturn ufshcd_dme_get_attr(hba, attr_sel, mib_val, DME_PEER);\n}\n\nstatic inline bool ufshcd_is_hs_mode(struct ufs_pa_layer_attr *pwr_info)\n{\n\treturn (pwr_info->pwr_rx == FAST_MODE ||\n\t\tpwr_info->pwr_rx == FASTAUTO_MODE) &&\n\t\t(pwr_info->pwr_tx == FAST_MODE ||\n\t\tpwr_info->pwr_tx == FASTAUTO_MODE);\n}\n\nstatic inline int ufshcd_disable_host_tx_lcc(struct ufs_hba *hba)\n{\n\treturn ufshcd_dme_set(hba, UIC_ARG_MIB(PA_LOCAL_TX_LCC_ENABLE), 0);\n}\n\nvoid ufshcd_auto_hibern8_enable(struct ufs_hba *hba);\nvoid ufshcd_auto_hibern8_update(struct ufs_hba *hba, u32 ahit);\nvoid ufshcd_fixup_dev_quirks(struct ufs_hba *hba,\n\t\t\t     const struct ufs_dev_quirk *fixups);\n#define SD_ASCII_STD true\n#define SD_RAW false\nint ufshcd_read_string_desc(struct ufs_hba *hba, u8 desc_index,\n\t\t\t    u8 **buf, bool ascii);\n\nvoid ufshcd_hold(struct ufs_hba *hba);\nvoid ufshcd_release(struct ufs_hba *hba);\n\nvoid ufshcd_clkgate_delay_set(struct device *dev, unsigned long value);\n\nu32 ufshcd_get_local_unipro_ver(struct ufs_hba *hba);\n\nint ufshcd_get_vreg(struct device *dev, struct ufs_vreg *vreg);\n\nint ufshcd_send_uic_cmd(struct ufs_hba *hba, struct uic_command *uic_cmd);\n\nint ufshcd_advanced_rpmb_req_handler(struct ufs_hba *hba, struct utp_upiu_req *req_upiu,\n\t\t\t\t     struct utp_upiu_req *rsp_upiu, struct ufs_ehs *ehs_req,\n\t\t\t\t     struct ufs_ehs *ehs_rsp, int sg_cnt,\n\t\t\t\t     struct scatterlist *sg_list, enum dma_data_direction dir);\nint ufshcd_wb_toggle(struct ufs_hba *hba, bool enable);\nint ufshcd_wb_toggle_buf_flush(struct ufs_hba *hba, bool enable);\nint ufshcd_suspend_prepare(struct device *dev);\nint __ufshcd_suspend_prepare(struct device *dev, bool rpm_ok_for_spm);\nvoid ufshcd_resume_complete(struct device *dev);\nbool ufshcd_is_hba_active(struct ufs_hba *hba);\n\n \nstatic inline int ufshcd_vops_init(struct ufs_hba *hba)\n{\n\tif (hba->vops && hba->vops->init)\n\t\treturn hba->vops->init(hba);\n\n\treturn 0;\n}\n\nstatic inline int ufshcd_vops_phy_initialization(struct ufs_hba *hba)\n{\n\tif (hba->vops && hba->vops->phy_initialization)\n\t\treturn hba->vops->phy_initialization(hba);\n\n\treturn 0;\n}\n\nextern const struct ufs_pm_lvl_states ufs_pm_lvl_states[];\n\nint ufshcd_dump_regs(struct ufs_hba *hba, size_t offset, size_t len,\n\t\t     const char *prefix);\n\nint __ufshcd_write_ee_control(struct ufs_hba *hba, u32 ee_ctrl_mask);\nint ufshcd_write_ee_control(struct ufs_hba *hba);\nint ufshcd_update_ee_control(struct ufs_hba *hba, u16 *mask,\n\t\t\t     const u16 *other_mask, u16 set, u16 clr);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}