{
  "module_name": "ip.h",
  "hash_id": "96cfd3f8e21e362594d488ebef329c92143b298e37a69ce5a53a2ca3398c2dce",
  "original_prompt": "Ingested from linux-6.6.14/include/net/ip.h",
  "human_readable_source": " \n \n#ifndef _IP_H\n#define _IP_H\n\n#include <linux/types.h>\n#include <linux/ip.h>\n#include <linux/in.h>\n#include <linux/skbuff.h>\n#include <linux/jhash.h>\n#include <linux/sockptr.h>\n#include <linux/static_key.h>\n\n#include <net/inet_sock.h>\n#include <net/route.h>\n#include <net/snmp.h>\n#include <net/flow.h>\n#include <net/flow_dissector.h>\n#include <net/netns/hash.h>\n#include <net/lwtunnel.h>\n\n#define IPV4_MAX_PMTU\t\t65535U\t\t \n#define IPV4_MIN_MTU\t\t68\t\t\t \n\nextern unsigned int sysctl_fib_sync_mem;\nextern unsigned int sysctl_fib_sync_mem_min;\nextern unsigned int sysctl_fib_sync_mem_max;\n\nstruct sock;\n\nstruct inet_skb_parm {\n\tint\t\t\tiif;\n\tstruct ip_options\topt;\t\t \n\tu16\t\t\tflags;\n\n#define IPSKB_FORWARDED\t\tBIT(0)\n#define IPSKB_XFRM_TUNNEL_SIZE\tBIT(1)\n#define IPSKB_XFRM_TRANSFORMED\tBIT(2)\n#define IPSKB_FRAG_COMPLETE\tBIT(3)\n#define IPSKB_REROUTED\t\tBIT(4)\n#define IPSKB_DOREDIRECT\tBIT(5)\n#define IPSKB_FRAG_PMTU\t\tBIT(6)\n#define IPSKB_L3SLAVE\t\tBIT(7)\n#define IPSKB_NOPOLICY\t\tBIT(8)\n#define IPSKB_MULTIPATH\t\tBIT(9)\n\n\tu16\t\t\tfrag_max_size;\n};\n\nstatic inline bool ipv4_l3mdev_skb(u16 flags)\n{\n\treturn !!(flags & IPSKB_L3SLAVE);\n}\n\nstatic inline unsigned int ip_hdrlen(const struct sk_buff *skb)\n{\n\treturn ip_hdr(skb)->ihl * 4;\n}\n\nstruct ipcm_cookie {\n\tstruct sockcm_cookie\tsockc;\n\t__be32\t\t\taddr;\n\tint\t\t\toif;\n\tstruct ip_options_rcu\t*opt;\n\t__u8\t\t\tprotocol;\n\t__u8\t\t\tttl;\n\t__s16\t\t\ttos;\n\tchar\t\t\tpriority;\n\t__u16\t\t\tgso_size;\n};\n\nstatic inline void ipcm_init(struct ipcm_cookie *ipcm)\n{\n\t*ipcm = (struct ipcm_cookie) { .tos = -1 };\n}\n\nstatic inline void ipcm_init_sk(struct ipcm_cookie *ipcm,\n\t\t\t\tconst struct inet_sock *inet)\n{\n\tipcm_init(ipcm);\n\n\tipcm->sockc.mark = READ_ONCE(inet->sk.sk_mark);\n\tipcm->sockc.tsflags = READ_ONCE(inet->sk.sk_tsflags);\n\tipcm->oif = READ_ONCE(inet->sk.sk_bound_dev_if);\n\tipcm->addr = inet->inet_saddr;\n\tipcm->protocol = inet->inet_num;\n}\n\n#define IPCB(skb) ((struct inet_skb_parm*)((skb)->cb))\n#define PKTINFO_SKB_CB(skb) ((struct in_pktinfo *)((skb)->cb))\n\n \nstatic inline int inet_sdif(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv4_l3mdev_skb(IPCB(skb)->flags))\n\t\treturn IPCB(skb)->iif;\n#endif\n\treturn 0;\n}\n\n \n\nstruct ip_ra_chain {\n\tstruct ip_ra_chain __rcu *next;\n\tstruct sock\t\t*sk;\n\tunion {\n\t\tvoid\t\t\t(*destructor)(struct sock *);\n\t\tstruct sock\t\t*saved_sk;\n\t};\n\tstruct rcu_head\t\trcu;\n};\n\n \n#define IP_CE\t\t0x8000\t\t \n#define IP_DF\t\t0x4000\t\t \n#define IP_MF\t\t0x2000\t\t \n#define IP_OFFSET\t0x1FFF\t\t \n\n#define IP_FRAG_TIME\t(30 * HZ)\t\t \n\nstruct msghdr;\nstruct net_device;\nstruct packet_type;\nstruct rtable;\nstruct sockaddr;\n\nint igmp_mc_init(void);\n\n \n\nint ip_build_and_send_pkt(struct sk_buff *skb, const struct sock *sk,\n\t\t\t  __be32 saddr, __be32 daddr,\n\t\t\t  struct ip_options_rcu *opt, u8 tos);\nint ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,\n\t   struct net_device *orig_dev);\nvoid ip_list_rcv(struct list_head *head, struct packet_type *pt,\n\t\t struct net_device *orig_dev);\nint ip_local_deliver(struct sk_buff *skb);\nvoid ip_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int proto);\nint ip_mr_input(struct sk_buff *skb);\nint ip_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_mc_output(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_do_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,\n\t\t   int (*output)(struct net *, struct sock *, struct sk_buff *));\n\nstruct ip_fraglist_iter {\n\tstruct sk_buff\t*frag;\n\tstruct iphdr\t*iph;\n\tint\t\toffset;\n\tunsigned int\thlen;\n};\n\nvoid ip_fraglist_init(struct sk_buff *skb, struct iphdr *iph,\n\t\t      unsigned int hlen, struct ip_fraglist_iter *iter);\nvoid ip_fraglist_prepare(struct sk_buff *skb, struct ip_fraglist_iter *iter);\n\nstatic inline struct sk_buff *ip_fraglist_next(struct ip_fraglist_iter *iter)\n{\n\tstruct sk_buff *skb = iter->frag;\n\n\titer->frag = skb->next;\n\tskb_mark_not_on_list(skb);\n\n\treturn skb;\n}\n\nstruct ip_frag_state {\n\tbool\t\tDF;\n\tunsigned int\thlen;\n\tunsigned int\tll_rs;\n\tunsigned int\tmtu;\n\tunsigned int\tleft;\n\tint\t\toffset;\n\tint\t\tptr;\n\t__be16\t\tnot_last_frag;\n};\n\nvoid ip_frag_init(struct sk_buff *skb, unsigned int hlen, unsigned int ll_rs,\n\t\t  unsigned int mtu, bool DF, struct ip_frag_state *state);\nstruct sk_buff *ip_frag_next(struct sk_buff *skb,\n\t\t\t     struct ip_frag_state *state);\n\nvoid ip_send_check(struct iphdr *ip);\nint __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\nint ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb);\n\nint __ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl,\n\t\t    __u8 tos);\nvoid ip_init(void);\nint ip_append_data(struct sock *sk, struct flowi4 *fl4,\n\t\t   int getfrag(void *from, char *to, int offset, int len,\n\t\t\t       int odd, struct sk_buff *skb),\n\t\t   void *from, int len, int protolen,\n\t\t   struct ipcm_cookie *ipc,\n\t\t   struct rtable **rt,\n\t\t   unsigned int flags);\nint ip_generic_getfrag(void *from, char *to, int offset, int len, int odd,\n\t\t       struct sk_buff *skb);\nstruct sk_buff *__ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t      struct sk_buff_head *queue,\n\t\t\t      struct inet_cork *cork);\nint ip_send_skb(struct net *net, struct sk_buff *skb);\nint ip_push_pending_frames(struct sock *sk, struct flowi4 *fl4);\nvoid ip_flush_pending_frames(struct sock *sk);\nstruct sk_buff *ip_make_skb(struct sock *sk, struct flowi4 *fl4,\n\t\t\t    int getfrag(void *from, char *to, int offset,\n\t\t\t\t\tint len, int odd, struct sk_buff *skb),\n\t\t\t    void *from, int length, int transhdrlen,\n\t\t\t    struct ipcm_cookie *ipc, struct rtable **rtp,\n\t\t\t    struct inet_cork *cork, unsigned int flags);\n\nint ip_queue_xmit(struct sock *sk, struct sk_buff *skb, struct flowi *fl);\n\nstatic inline struct sk_buff *ip_finish_skb(struct sock *sk, struct flowi4 *fl4)\n{\n\treturn __ip_make_skb(sk, fl4, &sk->sk_write_queue, &inet_sk(sk)->cork.base);\n}\n\n \nstatic inline u8 ip_sendmsg_scope(const struct inet_sock *inet,\n\t\t\t\t  const struct ipcm_cookie *ipc,\n\t\t\t\t  const struct msghdr *msg)\n{\n\tif (sock_flag(&inet->sk, SOCK_LOCALROUTE) ||\n\t    msg->msg_flags & MSG_DONTROUTE ||\n\t    (ipc->opt && ipc->opt->opt.is_strictroute))\n\t\treturn RT_SCOPE_LINK;\n\n\treturn RT_SCOPE_UNIVERSE;\n}\n\nstatic inline __u8 get_rttos(struct ipcm_cookie* ipc, struct inet_sock *inet)\n{\n\treturn (ipc->tos != -1) ? RT_TOS(ipc->tos) : RT_TOS(inet->tos);\n}\n\n \nint __ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\nint ip4_datagram_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\n\nvoid ip4_datagram_release_cb(struct sock *sk);\n\nstruct ip_reply_arg {\n\tstruct kvec iov[1];\n\tint\t    flags;\n\t__wsum \t    csum;\n\tint\t    csumoffset;  \n\t\t\t\t \n\tint\t    bound_dev_if;\n\tu8  \t    tos;\n\tkuid_t\t    uid;\n};\n\n#define IP_REPLY_ARG_NOSRCCHECK 1\n\nstatic inline __u8 ip_reply_arg_flowi_flags(const struct ip_reply_arg *arg)\n{\n\treturn (arg->flags & IP_REPLY_ARG_NOSRCCHECK) ? FLOWI_FLAG_ANYSRC : 0;\n}\n\nvoid ip_send_unicast_reply(struct sock *sk, struct sk_buff *skb,\n\t\t\t   const struct ip_options *sopt,\n\t\t\t   __be32 daddr, __be32 saddr,\n\t\t\t   const struct ip_reply_arg *arg,\n\t\t\t   unsigned int len, u64 transmit_time, u32 txhash);\n\n#define IP_INC_STATS(net, field)\tSNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define __IP_INC_STATS(net, field)\t__SNMP_INC_STATS64((net)->mib.ip_statistics, field)\n#define IP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_ADD_STATS(net, field, val) __SNMP_ADD_STATS64((net)->mib.ip_statistics, field, val)\n#define IP_UPD_PO_STATS(net, field, val) SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define __IP_UPD_PO_STATS(net, field, val) __SNMP_UPD_PO_STATS64((net)->mib.ip_statistics, field, val)\n#define NET_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.net_statistics, field)\n#define __NET_INC_STATS(net, field)\t__SNMP_INC_STATS((net)->mib.net_statistics, field)\n#define NET_ADD_STATS(net, field, adnd)\tSNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n#define __NET_ADD_STATS(net, field, adnd) __SNMP_ADD_STATS((net)->mib.net_statistics, field, adnd)\n\nstatic inline u64 snmp_get_cpu_field(void __percpu *mib, int cpu, int offt)\n{\n\treturn  *(((unsigned long *)per_cpu_ptr(mib, cpu)) + offt);\n}\n\nunsigned long snmp_fold_field(void __percpu *mib, int offt);\n#if BITS_PER_LONG==32\nu64 snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t size_t syncp_offset);\nu64 snmp_fold_field64(void __percpu *mib, int offt, size_t sync_off);\n#else\nstatic inline u64  snmp_get_cpu_field64(void __percpu *mib, int cpu, int offct,\n\t\t\t\t\tsize_t syncp_offset)\n{\n\treturn snmp_get_cpu_field(mib, cpu, offct);\n\n}\n\nstatic inline u64 snmp_fold_field64(void __percpu *mib, int offt, size_t syncp_off)\n{\n\treturn snmp_fold_field(mib, offt);\n}\n#endif\n\n#define snmp_get_cpu_field64_batch(buff64, stats_list, mib_statistic, offset) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff64[i] += snmp_get_cpu_field64( \\\n\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\tc, stats_list[i].entry, \\\n\t\t\t\t\toffset); \\\n\t} \\\n}\n\n#define snmp_get_cpu_field_batch(buff, stats_list, mib_statistic) \\\n{ \\\n\tint i, c; \\\n\tfor_each_possible_cpu(c) { \\\n\t\tfor (i = 0; stats_list[i].name; i++) \\\n\t\t\tbuff[i] += snmp_get_cpu_field( \\\n\t\t\t\t\t\tmib_statistic, \\\n\t\t\t\t\t\tc, stats_list[i].entry); \\\n\t} \\\n}\n\nvoid inet_get_local_port_range(const struct net *net, int *low, int *high);\nvoid inet_sk_get_local_port_range(const struct sock *sk, int *low, int *high);\n\n#ifdef CONFIG_SYSCTL\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\tif (!net->ipv4.sysctl_local_reserved_ports)\n\t\treturn false;\n\treturn test_bit(port, net->ipv4.sysctl_local_reserved_ports);\n}\n\nstatic inline bool sysctl_dev_name_is_allowed(const char *name)\n{\n\treturn strcmp(name, \"default\") != 0  && strcmp(name, \"all\") != 0;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < READ_ONCE(net->ipv4.sysctl_ip_prot_sock);\n}\n\n#else\nstatic inline bool inet_is_local_reserved_port(struct net *net, unsigned short port)\n{\n\treturn false;\n}\n\nstatic inline bool inet_port_requires_bind_service(struct net *net, unsigned short port)\n{\n\treturn port < PROT_SOCK;\n}\n#endif\n\n__be32 inet_current_timestamp(void);\n\n \nextern int inet_peer_threshold;\nextern int inet_peer_minttl;\nextern int inet_peer_maxttl;\n\nvoid ipfrag_init(void);\n\nvoid ip_static_sysctl_init(void);\n\n#define IP4_REPLY_MARK(net, mark) \\\n\t(READ_ONCE((net)->ipv4.sysctl_fwmark_reflect) ? (mark) : 0)\n\nstatic inline bool ip_is_fragment(const struct iphdr *iph)\n{\n\treturn (iph->frag_off & htons(IP_MF | IP_OFFSET)) != 0;\n}\n\n#ifdef CONFIG_INET\n#include <net/dst.h>\n\n \nstatic inline\nint ip_decrease_ttl(struct iphdr *iph)\n{\n\tu32 check = (__force u32)iph->check;\n\tcheck += (__force u32)htons(0x0100);\n\tiph->check = (__force __sum16)(check + (check>=0xFFFF));\n\treturn --iph->ttl;\n}\n\nstatic inline int ip_mtu_locked(const struct dst_entry *dst)\n{\n\tconst struct rtable *rt = (const struct rtable *)dst;\n\n\treturn rt->rt_mtu_locked || dst_metric_locked(dst, RTAX_MTU);\n}\n\nstatic inline\nint ip_dont_fragment(const struct sock *sk, const struct dst_entry *dst)\n{\n\tu8 pmtudisc = READ_ONCE(inet_sk(sk)->pmtudisc);\n\n\treturn  pmtudisc == IP_PMTUDISC_DO ||\n\t\t(pmtudisc == IP_PMTUDISC_WANT &&\n\t\t !ip_mtu_locked(dst));\n}\n\nstatic inline bool ip_sk_accept_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc != IP_PMTUDISC_INTERFACE &&\n\t       inet_sk(sk)->pmtudisc != IP_PMTUDISC_OMIT;\n}\n\nstatic inline bool ip_sk_use_pmtu(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_PROBE;\n}\n\nstatic inline bool ip_sk_ignore_df(const struct sock *sk)\n{\n\treturn inet_sk(sk)->pmtudisc < IP_PMTUDISC_DO ||\n\t       inet_sk(sk)->pmtudisc == IP_PMTUDISC_OMIT;\n}\n\nstatic inline unsigned int ip_dst_mtu_maybe_forward(const struct dst_entry *dst,\n\t\t\t\t\t\t    bool forwarding)\n{\n\tconst struct rtable *rt = container_of(dst, struct rtable, dst);\n\tstruct net *net = dev_net(dst->dev);\n\tunsigned int mtu;\n\n\tif (READ_ONCE(net->ipv4.sysctl_ip_fwd_use_pmtu) ||\n\t    ip_mtu_locked(dst) ||\n\t    !forwarding) {\n\t\tmtu = rt->rt_pmtu;\n\t\tif (mtu && time_before(jiffies, rt->dst.expires))\n\t\t\tgoto out;\n\t}\n\n\t \n\tmtu = dst_metric_raw(dst, RTAX_MTU);\n\tif (mtu)\n\t\tgoto out;\n\n\tmtu = READ_ONCE(dst->dev->mtu);\n\n\tif (unlikely(ip_mtu_locked(dst))) {\n\t\tif (rt->rt_uses_gateway && mtu > 576)\n\t\t\tmtu = 576;\n\t}\n\nout:\n\tmtu = min_t(unsigned int, mtu, IP_MAX_MTU);\n\n\treturn mtu - lwtunnel_headroom(dst->lwtstate, mtu);\n}\n\nstatic inline unsigned int ip_skb_dst_mtu(struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb)\n{\n\tunsigned int mtu;\n\n\tif (!sk || !sk_fullsock(sk) || ip_sk_use_pmtu(sk)) {\n\t\tbool forwarding = IPCB(skb)->flags & IPSKB_FORWARDED;\n\n\t\treturn ip_dst_mtu_maybe_forward(skb_dst(skb), forwarding);\n\t}\n\n\tmtu = min(READ_ONCE(skb_dst(skb)->dev->mtu), IP_MAX_MTU);\n\treturn mtu - lwtunnel_headroom(skb_dst(skb)->lwtstate, mtu);\n}\n\nstruct dst_metrics *ip_fib_metrics_init(struct net *net, struct nlattr *fc_mx,\n\t\t\t\t\tint fc_mx_len,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\nstatic inline void ip_fib_metrics_put(struct dst_metrics *fib_metrics)\n{\n\tif (fib_metrics != &dst_default_metrics &&\n\t    refcount_dec_and_test(&fib_metrics->refcnt))\n\t\tkfree(fib_metrics);\n}\n\n \nstatic inline\nvoid ip_dst_init_metrics(struct dst_entry *dst, struct dst_metrics *fib_metrics)\n{\n\tdst_init_metrics(dst, fib_metrics->metrics, true);\n\n\tif (fib_metrics != &dst_default_metrics) {\n\t\tdst->_metrics |= DST_METRICS_REFCOUNTED;\n\t\trefcount_inc(&fib_metrics->refcnt);\n\t}\n}\n\nstatic inline\nvoid ip_dst_metrics_put(struct dst_entry *dst)\n{\n\tstruct dst_metrics *p = (struct dst_metrics *)DST_METRICS_PTR(dst);\n\n\tif (p != &dst_default_metrics && refcount_dec_and_test(&p->refcnt))\n\t\tkfree(p);\n}\n\nvoid __ip_select_ident(struct net *net, struct iphdr *iph, int segs);\n\nstatic inline void ip_select_ident_segs(struct net *net, struct sk_buff *skb,\n\t\t\t\t\tstruct sock *sk, int segs)\n{\n\tstruct iphdr *iph = ip_hdr(skb);\n\n\t \n\tif (sk && inet_sk(sk)->inet_daddr) {\n\t\tint val;\n\n\t\t \n\t\tif (sk_is_tcp(sk)) {\n\t\t\tsock_owned_by_me(sk);\n\t\t\tval = atomic_read(&inet_sk(sk)->inet_id);\n\t\t\tatomic_set(&inet_sk(sk)->inet_id, val + segs);\n\t\t} else {\n\t\t\tval = atomic_add_return(segs, &inet_sk(sk)->inet_id);\n\t\t}\n\t\tiph->id = htons(val);\n\t\treturn;\n\t}\n\tif ((iph->frag_off & htons(IP_DF)) && !skb->ignore_df) {\n\t\tiph->id = 0;\n\t} else {\n\t\t \n\t\t__ip_select_ident(net, iph, segs);\n\t}\n}\n\nstatic inline void ip_select_ident(struct net *net, struct sk_buff *skb,\n\t\t\t\t   struct sock *sk)\n{\n\tip_select_ident_segs(net, skb, sk, 1);\n}\n\nstatic inline __wsum inet_compute_pseudo(struct sk_buff *skb, int proto)\n{\n\treturn csum_tcpudp_nofold(ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,\n\t\t\t\t  skb->len, proto, 0);\n}\n\n \nstatic inline void iph_to_flow_copy_v4addrs(struct flow_keys *flow,\n\t\t\t\t\t    const struct iphdr *iph)\n{\n\tBUILD_BUG_ON(offsetof(typeof(flow->addrs), v4addrs.dst) !=\n\t\t     offsetof(typeof(flow->addrs), v4addrs.src) +\n\t\t\t      sizeof(flow->addrs.v4addrs.src));\n\tmemcpy(&flow->addrs.v4addrs, &iph->addrs, sizeof(flow->addrs.v4addrs));\n\tflow->control.addr_type = FLOW_DISSECTOR_KEY_IPV4_ADDRS;\n}\n\n \n\nstatic inline void ip_eth_mc_map(__be32 naddr, char *buf)\n{\n\t__u32 addr=ntohl(naddr);\n\tbuf[0]=0x01;\n\tbuf[1]=0x00;\n\tbuf[2]=0x5e;\n\tbuf[5]=addr&0xFF;\n\taddr>>=8;\n\tbuf[4]=addr&0xFF;\n\taddr>>=8;\n\tbuf[3]=addr&0x7F;\n}\n\n \n\nstatic inline void ip_ib_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\t__u32 addr;\n\tunsigned char scope = broadcast[5] & 0xF;\n\n\tbuf[0]  = 0;\t\t \n\tbuf[1]  = 0xff;\t\t \n\tbuf[2]  = 0xff;\n\tbuf[3]  = 0xff;\n\taddr    = ntohl(naddr);\n\tbuf[4]  = 0xff;\n\tbuf[5]  = 0x10 | scope;\t \n\tbuf[6]  = 0x40;\t\t \n\tbuf[7]  = 0x1b;\n\tbuf[8]  = broadcast[8];\t\t \n\tbuf[9]  = broadcast[9];\n\tbuf[10] = 0;\n\tbuf[11] = 0;\n\tbuf[12] = 0;\n\tbuf[13] = 0;\n\tbuf[14] = 0;\n\tbuf[15] = 0;\n\tbuf[19] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[18] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[17] = addr & 0xff;\n\taddr  >>= 8;\n\tbuf[16] = addr & 0x0f;\n}\n\nstatic inline void ip_ipgre_mc_map(__be32 naddr, const unsigned char *broadcast, char *buf)\n{\n\tif ((broadcast[0] | broadcast[1] | broadcast[2] | broadcast[3]) != 0)\n\t\tmemcpy(buf, broadcast, 4);\n\telse\n\t\tmemcpy(buf, &naddr, sizeof(naddr));\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n#include <linux/ipv6.h>\n#endif\n\nstatic __inline__ void inet_reset_saddr(struct sock *sk)\n{\n\tinet_sk(sk)->inet_rcv_saddr = inet_sk(sk)->inet_saddr = 0;\n#if IS_ENABLED(CONFIG_IPV6)\n\tif (sk->sk_family == PF_INET6) {\n\t\tstruct ipv6_pinfo *np = inet6_sk(sk);\n\n\t\tmemset(&np->saddr, 0, sizeof(np->saddr));\n\t\tmemset(&sk->sk_v6_rcv_saddr, 0, sizeof(sk->sk_v6_rcv_saddr));\n\t}\n#endif\n}\n\n#endif\n\nstatic inline unsigned int ipv4_addr_hash(__be32 ip)\n{\n\treturn (__force unsigned int) ip;\n}\n\nstatic inline u32 ipv4_portaddr_hash(const struct net *net,\n\t\t\t\t     __be32 saddr,\n\t\t\t\t     unsigned int port)\n{\n\treturn jhash_1word((__force u32)saddr, net_hash_mix(net)) ^ port;\n}\n\nbool ip_call_ra_chain(struct sk_buff *skb);\n\n \n\nenum ip_defrag_users {\n\tIP_DEFRAG_LOCAL_DELIVER,\n\tIP_DEFRAG_CALL_RA_CHAIN,\n\tIP_DEFRAG_CONNTRACK_IN,\n\t__IP_DEFRAG_CONNTRACK_IN_END\t= IP_DEFRAG_CONNTRACK_IN + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_OUT,\n\t__IP_DEFRAG_CONNTRACK_OUT_END\t= IP_DEFRAG_CONNTRACK_OUT + USHRT_MAX,\n\tIP_DEFRAG_CONNTRACK_BRIDGE_IN,\n\t__IP_DEFRAG_CONNTRACK_BRIDGE_IN = IP_DEFRAG_CONNTRACK_BRIDGE_IN + USHRT_MAX,\n\tIP_DEFRAG_VS_IN,\n\tIP_DEFRAG_VS_OUT,\n\tIP_DEFRAG_VS_FWD,\n\tIP_DEFRAG_AF_PACKET,\n\tIP_DEFRAG_MACVLAN,\n};\n\n \nstatic inline bool ip_defrag_user_in_between(u32 user,\n\t\t\t\t\t     enum ip_defrag_users lower_bond,\n\t\t\t\t\t     enum ip_defrag_users upper_bond)\n{\n\treturn user >= lower_bond && user <= upper_bond;\n}\n\nint ip_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#ifdef CONFIG_INET\nstruct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user);\n#else\nstatic inline struct sk_buff *ip_check_defrag(struct net *net, struct sk_buff *skb, u32 user)\n{\n\treturn skb;\n}\n#endif\n\n \n\nint ip_forward(struct sk_buff *skb);\n\n \n\nvoid ip_options_build(struct sk_buff *skb, struct ip_options *opt,\n\t\t      __be32 daddr, struct rtable *rt);\n\nint __ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t      struct sk_buff *skb, const struct ip_options *sopt);\nstatic inline int ip_options_echo(struct net *net, struct ip_options *dopt,\n\t\t\t\t  struct sk_buff *skb)\n{\n\treturn __ip_options_echo(net, dopt, skb, &IPCB(skb)->opt);\n}\n\nvoid ip_options_fragment(struct sk_buff *skb);\nint __ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t\t struct sk_buff *skb, __be32 *info);\nint ip_options_compile(struct net *net, struct ip_options *opt,\n\t\t       struct sk_buff *skb);\nint ip_options_get(struct net *net, struct ip_options_rcu **optp,\n\t\t   sockptr_t data, int optlen);\nvoid ip_options_undo(struct ip_options *opt);\nvoid ip_forward_options(struct sk_buff *skb);\nint ip_options_rcv_srr(struct sk_buff *skb, struct net_device *dev);\n\n \n\nvoid ipv4_pktinfo_prepare(const struct sock *sk, struct sk_buff *skb);\nvoid ip_cmsg_recv_offset(struct msghdr *msg, struct sock *sk,\n\t\t\t struct sk_buff *skb, int tlen, int offset);\nint ip_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t struct ipcm_cookie *ipc, bool allow_ipv6);\nDECLARE_STATIC_KEY_FALSE(ip4_min_ttl);\nint do_ip_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t     unsigned int optlen);\nint ip_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t  unsigned int optlen);\nint do_ip_getsockopt(struct sock *sk, int level, int optname,\n\t\t     sockptr_t optval, sockptr_t optlen);\nint ip_getsockopt(struct sock *sk, int level, int optname, char __user *optval,\n\t\t  int __user *optlen);\nint ip_ra_control(struct sock *sk, unsigned char on,\n\t\t  void (*destructor)(struct sock *));\n\nint ip_recv_error(struct sock *sk, struct msghdr *msg, int len, int *addr_len);\nvoid ip_icmp_error(struct sock *sk, struct sk_buff *skb, int err, __be16 port,\n\t\t   u32 info, u8 *payload);\nvoid ip_local_error(struct sock *sk, int err, __be32 daddr, __be16 dport,\n\t\t    u32 info);\n\nstatic inline void ip_cmsg_recv(struct msghdr *msg, struct sk_buff *skb)\n{\n\tip_cmsg_recv_offset(msg, skb->sk, skb, 0, 0);\n}\n\nbool icmp_global_allow(void);\nextern int sysctl_icmp_msgs_per_sec;\nextern int sysctl_icmp_msgs_burst;\n\n#ifdef CONFIG_PROC_FS\nint ip_misc_proc_init(void);\n#endif\n\nint rtm_getroute_parse_ip_proto(struct nlattr *attr, u8 *ip_proto, u8 family,\n\t\t\t\tstruct netlink_ext_ack *extack);\n\nstatic inline bool inetdev_valid_mtu(unsigned int mtu)\n{\n\treturn likely(mtu >= IPV4_MIN_MTU);\n}\n\nvoid ip_sock_set_freebind(struct sock *sk);\nint ip_sock_set_mtu_discover(struct sock *sk, int val);\nvoid ip_sock_set_pktinfo(struct sock *sk);\nvoid ip_sock_set_recverr(struct sock *sk);\nvoid ip_sock_set_tos(struct sock *sk, int val);\nvoid  __ip_sock_set_tos(struct sock *sk, int val);\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}