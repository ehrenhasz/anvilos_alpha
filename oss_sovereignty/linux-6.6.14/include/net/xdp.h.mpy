{
  "module_name": "xdp.h",
  "hash_id": "6c59831f1080cbf2c734c20a84d924487ca4b69f00778d5b45e53a8b5b251ff3",
  "original_prompt": "Ingested from linux-6.6.14/include/net/xdp.h",
  "human_readable_source": " \n \n#ifndef __LINUX_NET_XDP_H__\n#define __LINUX_NET_XDP_H__\n\n#include <linux/bitfield.h>\n#include <linux/filter.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>  \n\n \n\nenum xdp_mem_type {\n\tMEM_TYPE_PAGE_SHARED = 0,  \n\tMEM_TYPE_PAGE_ORDER0,      \n\tMEM_TYPE_PAGE_POOL,\n\tMEM_TYPE_XSK_BUFF_POOL,\n\tMEM_TYPE_MAX,\n};\n\n \n#define XDP_XMIT_FLUSH\t\t(1U << 0)\t \n#define XDP_XMIT_FLAGS_MASK\tXDP_XMIT_FLUSH\n\nstruct xdp_mem_info {\n\tu32 type;  \n\tu32 id;\n};\n\nstruct page_pool;\n\nstruct xdp_rxq_info {\n\tstruct net_device *dev;\n\tu32 queue_index;\n\tu32 reg_state;\n\tstruct xdp_mem_info mem;\n\tunsigned int napi_id;\n\tu32 frag_size;\n} ____cacheline_aligned;  \n\nstruct xdp_txq_info {\n\tstruct net_device *dev;\n};\n\nenum xdp_buff_flags {\n\tXDP_FLAGS_HAS_FRAGS\t\t= BIT(0),  \n\tXDP_FLAGS_FRAGS_PF_MEMALLOC\t= BIT(1),  \n};\n\nstruct xdp_buff {\n\tvoid *data;\n\tvoid *data_end;\n\tvoid *data_meta;\n\tvoid *data_hard_start;\n\tstruct xdp_rxq_info *rxq;\n\tstruct xdp_txq_info *txq;\n\tu32 frame_sz;  \n\tu32 flags;  \n};\n\nstatic __always_inline bool xdp_buff_has_frags(struct xdp_buff *xdp)\n{\n\treturn !!(xdp->flags & XDP_FLAGS_HAS_FRAGS);\n}\n\nstatic __always_inline void xdp_buff_set_frags_flag(struct xdp_buff *xdp)\n{\n\txdp->flags |= XDP_FLAGS_HAS_FRAGS;\n}\n\nstatic __always_inline void xdp_buff_clear_frags_flag(struct xdp_buff *xdp)\n{\n\txdp->flags &= ~XDP_FLAGS_HAS_FRAGS;\n}\n\nstatic __always_inline bool xdp_buff_is_frag_pfmemalloc(struct xdp_buff *xdp)\n{\n\treturn !!(xdp->flags & XDP_FLAGS_FRAGS_PF_MEMALLOC);\n}\n\nstatic __always_inline void xdp_buff_set_frag_pfmemalloc(struct xdp_buff *xdp)\n{\n\txdp->flags |= XDP_FLAGS_FRAGS_PF_MEMALLOC;\n}\n\nstatic __always_inline void\nxdp_init_buff(struct xdp_buff *xdp, u32 frame_sz, struct xdp_rxq_info *rxq)\n{\n\txdp->frame_sz = frame_sz;\n\txdp->rxq = rxq;\n\txdp->flags = 0;\n}\n\nstatic __always_inline void\nxdp_prepare_buff(struct xdp_buff *xdp, unsigned char *hard_start,\n\t\t int headroom, int data_len, const bool meta_valid)\n{\n\tunsigned char *data = hard_start + headroom;\n\n\txdp->data_hard_start = hard_start;\n\txdp->data = data;\n\txdp->data_end = data + data_len;\n\txdp->data_meta = meta_valid ? data : data + 1;\n}\n\n \n#define xdp_data_hard_end(xdp)\t\t\t\t\\\n\t((xdp)->data_hard_start + (xdp)->frame_sz -\t\\\n\t SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))\n\nstatic inline struct skb_shared_info *\nxdp_get_shared_info_from_buff(struct xdp_buff *xdp)\n{\n\treturn (struct skb_shared_info *)xdp_data_hard_end(xdp);\n}\n\nstatic __always_inline unsigned int xdp_get_buff_len(struct xdp_buff *xdp)\n{\n\tunsigned int len = xdp->data_end - xdp->data;\n\tstruct skb_shared_info *sinfo;\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_buff(xdp);\n\tlen += sinfo->xdp_frags_size;\nout:\n\treturn len;\n}\n\nstruct xdp_frame {\n\tvoid *data;\n\tu16 len;\n\tu16 headroom;\n\tu32 metasize;  \n\t \n\tstruct xdp_mem_info mem;\n\tstruct net_device *dev_rx;  \n\tu32 frame_sz;\n\tu32 flags;  \n};\n\nstatic __always_inline bool xdp_frame_has_frags(struct xdp_frame *frame)\n{\n\treturn !!(frame->flags & XDP_FLAGS_HAS_FRAGS);\n}\n\nstatic __always_inline bool xdp_frame_is_frag_pfmemalloc(struct xdp_frame *frame)\n{\n\treturn !!(frame->flags & XDP_FLAGS_FRAGS_PF_MEMALLOC);\n}\n\n#define XDP_BULK_QUEUE_SIZE\t16\nstruct xdp_frame_bulk {\n\tint count;\n\tvoid *xa;\n\tvoid *q[XDP_BULK_QUEUE_SIZE];\n};\n\nstatic __always_inline void xdp_frame_bulk_init(struct xdp_frame_bulk *bq)\n{\n\t \n\tbq->xa = NULL;\n}\n\nstatic inline struct skb_shared_info *\nxdp_get_shared_info_from_frame(struct xdp_frame *frame)\n{\n\tvoid *data_hard_start = frame->data - frame->headroom - sizeof(*frame);\n\n\treturn (struct skb_shared_info *)(data_hard_start + frame->frame_sz -\n\t\t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info)));\n}\n\nstruct xdp_cpumap_stats {\n\tunsigned int redirect;\n\tunsigned int pass;\n\tunsigned int drop;\n};\n\n \nstatic inline void xdp_scrub_frame(struct xdp_frame *frame)\n{\n\tframe->data = NULL;\n\tframe->dev_rx = NULL;\n}\n\nstatic inline void\nxdp_update_skb_shared_info(struct sk_buff *skb, u8 nr_frags,\n\t\t\t   unsigned int size, unsigned int truesize,\n\t\t\t   bool pfmemalloc)\n{\n\tskb_shinfo(skb)->nr_frags = nr_frags;\n\n\tskb->len += size;\n\tskb->data_len += size;\n\tskb->truesize += truesize;\n\tskb->pfmemalloc |= pfmemalloc;\n}\n\n \nvoid xdp_warn(const char *msg, const char *func, const int line);\n#define XDP_WARN(msg) xdp_warn(msg, __func__, __LINE__)\n\nstruct xdp_frame *xdp_convert_zc_to_xdp_frame(struct xdp_buff *xdp);\nstruct sk_buff *__xdp_build_skb_from_frame(struct xdp_frame *xdpf,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev);\nstruct sk_buff *xdp_build_skb_from_frame(struct xdp_frame *xdpf,\n\t\t\t\t\t struct net_device *dev);\nint xdp_alloc_skb_bulk(void **skbs, int n_skb, gfp_t gfp);\nstruct xdp_frame *xdpf_clone(struct xdp_frame *xdpf);\n\nstatic inline\nvoid xdp_convert_frame_to_buff(struct xdp_frame *frame, struct xdp_buff *xdp)\n{\n\txdp->data_hard_start = frame->data - frame->headroom - sizeof(*frame);\n\txdp->data = frame->data;\n\txdp->data_end = frame->data + frame->len;\n\txdp->data_meta = frame->data - frame->metasize;\n\txdp->frame_sz = frame->frame_sz;\n\txdp->flags = frame->flags;\n}\n\nstatic inline\nint xdp_update_frame_from_buff(struct xdp_buff *xdp,\n\t\t\t       struct xdp_frame *xdp_frame)\n{\n\tint metasize, headroom;\n\n\t \n\theadroom = xdp->data - xdp->data_hard_start;\n\tmetasize = xdp->data - xdp->data_meta;\n\tmetasize = metasize > 0 ? metasize : 0;\n\tif (unlikely((headroom - metasize) < sizeof(*xdp_frame)))\n\t\treturn -ENOSPC;\n\n\t \n\tif (unlikely(xdp->data_end > xdp_data_hard_end(xdp))) {\n\t\tXDP_WARN(\"Driver BUG: missing reserved tailroom\");\n\t\treturn -ENOSPC;\n\t}\n\n\txdp_frame->data = xdp->data;\n\txdp_frame->len  = xdp->data_end - xdp->data;\n\txdp_frame->headroom = headroom - sizeof(*xdp_frame);\n\txdp_frame->metasize = metasize;\n\txdp_frame->frame_sz = xdp->frame_sz;\n\txdp_frame->flags = xdp->flags;\n\n\treturn 0;\n}\n\n \nstatic inline\nstruct xdp_frame *xdp_convert_buff_to_frame(struct xdp_buff *xdp)\n{\n\tstruct xdp_frame *xdp_frame;\n\n\tif (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL)\n\t\treturn xdp_convert_zc_to_xdp_frame(xdp);\n\n\t \n\txdp_frame = xdp->data_hard_start;\n\tif (unlikely(xdp_update_frame_from_buff(xdp, xdp_frame) < 0))\n\t\treturn NULL;\n\n\t \n\txdp_frame->mem = xdp->rxq->mem;\n\n\treturn xdp_frame;\n}\n\nvoid __xdp_return(void *data, struct xdp_mem_info *mem, bool napi_direct,\n\t\t  struct xdp_buff *xdp);\nvoid xdp_return_frame(struct xdp_frame *xdpf);\nvoid xdp_return_frame_rx_napi(struct xdp_frame *xdpf);\nvoid xdp_return_buff(struct xdp_buff *xdp);\nvoid xdp_flush_frame_bulk(struct xdp_frame_bulk *bq);\nvoid xdp_return_frame_bulk(struct xdp_frame *xdpf,\n\t\t\t   struct xdp_frame_bulk *bq);\n\nstatic __always_inline unsigned int xdp_get_frame_len(struct xdp_frame *xdpf)\n{\n\tstruct skb_shared_info *sinfo;\n\tunsigned int len = xdpf->len;\n\n\tif (likely(!xdp_frame_has_frags(xdpf)))\n\t\tgoto out;\n\n\tsinfo = xdp_get_shared_info_from_frame(xdpf);\n\tlen += sinfo->xdp_frags_size;\nout:\n\treturn len;\n}\n\nint __xdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,\n\t\t       struct net_device *dev, u32 queue_index,\n\t\t       unsigned int napi_id, u32 frag_size);\nstatic inline int\nxdp_rxq_info_reg(struct xdp_rxq_info *xdp_rxq,\n\t\t struct net_device *dev, u32 queue_index,\n\t\t unsigned int napi_id)\n{\n\treturn __xdp_rxq_info_reg(xdp_rxq, dev, queue_index, napi_id, 0);\n}\n\nvoid xdp_rxq_info_unreg(struct xdp_rxq_info *xdp_rxq);\nvoid xdp_rxq_info_unused(struct xdp_rxq_info *xdp_rxq);\nbool xdp_rxq_info_is_reg(struct xdp_rxq_info *xdp_rxq);\nint xdp_rxq_info_reg_mem_model(struct xdp_rxq_info *xdp_rxq,\n\t\t\t       enum xdp_mem_type type, void *allocator);\nvoid xdp_rxq_info_unreg_mem_model(struct xdp_rxq_info *xdp_rxq);\nint xdp_reg_mem_model(struct xdp_mem_info *mem,\n\t\t      enum xdp_mem_type type, void *allocator);\nvoid xdp_unreg_mem_model(struct xdp_mem_info *mem);\n\n \nstatic __always_inline void\nxdp_set_data_meta_invalid(struct xdp_buff *xdp)\n{\n\txdp->data_meta = xdp->data + 1;\n}\n\nstatic __always_inline bool\nxdp_data_meta_unsupported(const struct xdp_buff *xdp)\n{\n\treturn unlikely(xdp->data_meta > xdp->data);\n}\n\nstatic inline bool xdp_metalen_invalid(unsigned long metalen)\n{\n\treturn (metalen & (sizeof(__u32) - 1)) || (metalen > 32);\n}\n\nstruct xdp_attachment_info {\n\tstruct bpf_prog *prog;\n\tu32 flags;\n};\n\nstruct netdev_bpf;\nvoid xdp_attachment_setup(struct xdp_attachment_info *info,\n\t\t\t  struct netdev_bpf *bpf);\n\n#define DEV_MAP_BULK_SIZE XDP_BULK_QUEUE_SIZE\n\n#define XDP_METADATA_KFUNC_xxx\t\\\n\tXDP_METADATA_KFUNC(XDP_METADATA_KFUNC_RX_TIMESTAMP, \\\n\t\t\t   bpf_xdp_metadata_rx_timestamp) \\\n\tXDP_METADATA_KFUNC(XDP_METADATA_KFUNC_RX_HASH, \\\n\t\t\t   bpf_xdp_metadata_rx_hash) \\\n\nenum {\n#define XDP_METADATA_KFUNC(name, _) name,\nXDP_METADATA_KFUNC_xxx\n#undef XDP_METADATA_KFUNC\nMAX_XDP_METADATA_KFUNC,\n};\n\nenum xdp_rss_hash_type {\n\t \n\tXDP_RSS_L3_IPV4\t\t= BIT(0),\n\tXDP_RSS_L3_IPV6\t\t= BIT(1),\n\n\t \n\tXDP_RSS_L3_DYNHDR\t= BIT(2),\n\n\t \n\tXDP_RSS_L4\t\t= BIT(3),  \n\tXDP_RSS_L4_TCP\t\t= BIT(4),\n\tXDP_RSS_L4_UDP\t\t= BIT(5),\n\tXDP_RSS_L4_SCTP\t\t= BIT(6),\n\tXDP_RSS_L4_IPSEC\t= BIT(7),  \n\n\t \n\tXDP_RSS_TYPE_NONE            = 0,\n\tXDP_RSS_TYPE_L2              = XDP_RSS_TYPE_NONE,\n\n\tXDP_RSS_TYPE_L3_IPV4         = XDP_RSS_L3_IPV4,\n\tXDP_RSS_TYPE_L3_IPV6         = XDP_RSS_L3_IPV6,\n\tXDP_RSS_TYPE_L3_IPV4_OPT     = XDP_RSS_L3_IPV4 | XDP_RSS_L3_DYNHDR,\n\tXDP_RSS_TYPE_L3_IPV6_EX      = XDP_RSS_L3_IPV6 | XDP_RSS_L3_DYNHDR,\n\n\tXDP_RSS_TYPE_L4_ANY          = XDP_RSS_L4,\n\tXDP_RSS_TYPE_L4_IPV4_TCP     = XDP_RSS_L3_IPV4 | XDP_RSS_L4 | XDP_RSS_L4_TCP,\n\tXDP_RSS_TYPE_L4_IPV4_UDP     = XDP_RSS_L3_IPV4 | XDP_RSS_L4 | XDP_RSS_L4_UDP,\n\tXDP_RSS_TYPE_L4_IPV4_SCTP    = XDP_RSS_L3_IPV4 | XDP_RSS_L4 | XDP_RSS_L4_SCTP,\n\tXDP_RSS_TYPE_L4_IPV4_IPSEC   = XDP_RSS_L3_IPV4 | XDP_RSS_L4 | XDP_RSS_L4_IPSEC,\n\n\tXDP_RSS_TYPE_L4_IPV6_TCP     = XDP_RSS_L3_IPV6 | XDP_RSS_L4 | XDP_RSS_L4_TCP,\n\tXDP_RSS_TYPE_L4_IPV6_UDP     = XDP_RSS_L3_IPV6 | XDP_RSS_L4 | XDP_RSS_L4_UDP,\n\tXDP_RSS_TYPE_L4_IPV6_SCTP    = XDP_RSS_L3_IPV6 | XDP_RSS_L4 | XDP_RSS_L4_SCTP,\n\tXDP_RSS_TYPE_L4_IPV6_IPSEC   = XDP_RSS_L3_IPV6 | XDP_RSS_L4 | XDP_RSS_L4_IPSEC,\n\n\tXDP_RSS_TYPE_L4_IPV6_TCP_EX  = XDP_RSS_TYPE_L4_IPV6_TCP  | XDP_RSS_L3_DYNHDR,\n\tXDP_RSS_TYPE_L4_IPV6_UDP_EX  = XDP_RSS_TYPE_L4_IPV6_UDP  | XDP_RSS_L3_DYNHDR,\n\tXDP_RSS_TYPE_L4_IPV6_SCTP_EX = XDP_RSS_TYPE_L4_IPV6_SCTP | XDP_RSS_L3_DYNHDR,\n};\n\nstruct xdp_metadata_ops {\n\tint\t(*xmo_rx_timestamp)(const struct xdp_md *ctx, u64 *timestamp);\n\tint\t(*xmo_rx_hash)(const struct xdp_md *ctx, u32 *hash,\n\t\t\t       enum xdp_rss_hash_type *rss_type);\n};\n\n#ifdef CONFIG_NET\nu32 bpf_xdp_metadata_kfunc_id(int id);\nbool bpf_dev_bound_kfunc_id(u32 btf_id);\nvoid xdp_set_features_flag(struct net_device *dev, xdp_features_t val);\nvoid xdp_features_set_redirect_target(struct net_device *dev, bool support_sg);\nvoid xdp_features_clear_redirect_target(struct net_device *dev);\n#else\nstatic inline u32 bpf_xdp_metadata_kfunc_id(int id) { return 0; }\nstatic inline bool bpf_dev_bound_kfunc_id(u32 btf_id) { return false; }\n\nstatic inline void\nxdp_set_features_flag(struct net_device *dev, xdp_features_t val)\n{\n}\n\nstatic inline void\nxdp_features_set_redirect_target(struct net_device *dev, bool support_sg)\n{\n}\n\nstatic inline void\nxdp_features_clear_redirect_target(struct net_device *dev)\n{\n}\n#endif\n\nstatic inline void xdp_clear_features_flag(struct net_device *dev)\n{\n\txdp_set_features_flag(dev, 0);\n}\n\nstatic __always_inline u32 bpf_prog_run_xdp(const struct bpf_prog *prog,\n\t\t\t\t\t    struct xdp_buff *xdp)\n{\n\t \n\tu32 act = __bpf_prog_run(prog, xdp, BPF_DISPATCHER_FUNC(xdp));\n\n\tif (static_branch_unlikely(&bpf_master_redirect_enabled_key)) {\n\t\tif (act == XDP_TX && netif_is_bond_slave(xdp->rxq->dev))\n\t\t\tact = xdp_master_redirect(xdp);\n\t}\n\n\treturn act;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}