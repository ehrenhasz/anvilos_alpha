{
  "module_name": "tls.h",
  "hash_id": "5aa5d48d07f0a159c14c2e5ee77ab072b3616efc760b9094e06783ccde00d361",
  "original_prompt": "Ingested from linux-6.6.14/include/net/tls.h",
  "human_readable_source": " \n\n#ifndef _TLS_OFFLOAD_H\n#define _TLS_OFFLOAD_H\n\n#include <linux/types.h>\n#include <asm/byteorder.h>\n#include <linux/crypto.h>\n#include <linux/socket.h>\n#include <linux/tcp.h>\n#include <linux/mutex.h>\n#include <linux/netdevice.h>\n#include <linux/rcupdate.h>\n\n#include <net/net_namespace.h>\n#include <net/tcp.h>\n#include <net/strparser.h>\n#include <crypto/aead.h>\n#include <uapi/linux/tls.h>\n\nstruct tls_rec;\n\n \n#define TLS_MAX_PAYLOAD_SIZE\t\t((size_t)1 << 14)\n\n#define TLS_HEADER_SIZE\t\t\t5\n#define TLS_NONCE_OFFSET\t\tTLS_HEADER_SIZE\n\n#define TLS_CRYPTO_INFO_READY(info)\t((info)->cipher_type)\n\n#define TLS_AAD_SPACE_SIZE\t\t13\n\n#define MAX_IV_SIZE\t\t\t16\n#define TLS_TAG_SIZE\t\t\t16\n#define TLS_MAX_REC_SEQ_SIZE\t\t8\n#define TLS_MAX_AAD_SIZE\t\tTLS_AAD_SPACE_SIZE\n\n \n#define TLS_AES_CCM_IV_B0_BYTE\t\t2\n#define TLS_SM4_CCM_IV_B0_BYTE\t\t2\n\nenum {\n\tTLS_BASE,\n\tTLS_SW,\n\tTLS_HW,\n\tTLS_HW_RECORD,\n\tTLS_NUM_CONFIG,\n};\n\nstruct tx_work {\n\tstruct delayed_work work;\n\tstruct sock *sk;\n};\n\nstruct tls_sw_context_tx {\n\tstruct crypto_aead *aead_send;\n\tstruct crypto_wait async_wait;\n\tstruct tx_work tx_work;\n\tstruct tls_rec *open_rec;\n\tstruct list_head tx_list;\n\tatomic_t encrypt_pending;\n\t \n\tspinlock_t encrypt_compl_lock;\n\tint async_notify;\n\tu8 async_capable:1;\n\n#define BIT_TX_SCHEDULED\t0\n#define BIT_TX_CLOSING\t\t1\n\tunsigned long tx_bitmask;\n};\n\nstruct tls_strparser {\n\tstruct sock *sk;\n\n\tu32 mark : 8;\n\tu32 stopped : 1;\n\tu32 copy_mode : 1;\n\tu32 mixed_decrypted : 1;\n\tu32 msg_ready : 1;\n\n\tstruct strp_msg stm;\n\n\tstruct sk_buff *anchor;\n\tstruct work_struct work;\n};\n\nstruct tls_sw_context_rx {\n\tstruct crypto_aead *aead_recv;\n\tstruct crypto_wait async_wait;\n\tstruct sk_buff_head rx_list;\t \n\tvoid (*saved_data_ready)(struct sock *sk);\n\n\tu8 reader_present;\n\tu8 async_capable:1;\n\tu8 zc_capable:1;\n\tu8 reader_contended:1;\n\n\tstruct tls_strparser strp;\n\n\tatomic_t decrypt_pending;\n\t \n\tspinlock_t decrypt_compl_lock;\n\tstruct sk_buff_head async_hold;\n\tstruct wait_queue_head wq;\n};\n\nstruct tls_record_info {\n\tstruct list_head list;\n\tu32 end_seq;\n\tint len;\n\tint num_frags;\n\tskb_frag_t frags[MAX_SKB_FRAGS];\n};\n\nstruct tls_offload_context_tx {\n\tstruct crypto_aead *aead_send;\n\tspinlock_t lock;\t \n\tstruct list_head records_list;\n\tstruct tls_record_info *open_record;\n\tstruct tls_record_info *retransmit_hint;\n\tu64 hint_record_sn;\n\tu64 unacked_record_sn;\n\n\tstruct scatterlist sg_tx_data[MAX_SKB_FRAGS];\n\tvoid (*sk_destruct)(struct sock *sk);\n\tstruct work_struct destruct_work;\n\tstruct tls_context *ctx;\n\tu8 driver_state[] __aligned(8);\n\t \n#define TLS_DRIVER_STATE_SIZE_TX\t16\n};\n\n#define TLS_OFFLOAD_CONTEXT_SIZE_TX                                            \\\n\t(sizeof(struct tls_offload_context_tx) + TLS_DRIVER_STATE_SIZE_TX)\n\nenum tls_context_flags {\n\t \n\tTLS_RX_DEV_DEGRADED = 0,\n\t \n\tTLS_TX_SYNC_SCHED = 1,\n\t \n\tTLS_RX_DEV_CLOSED = 2,\n};\n\nstruct cipher_context {\n\tchar *iv;\n\tchar *rec_seq;\n};\n\nunion tls_crypto_context {\n\tstruct tls_crypto_info info;\n\tunion {\n\t\tstruct tls12_crypto_info_aes_gcm_128 aes_gcm_128;\n\t\tstruct tls12_crypto_info_aes_gcm_256 aes_gcm_256;\n\t\tstruct tls12_crypto_info_chacha20_poly1305 chacha20_poly1305;\n\t\tstruct tls12_crypto_info_sm4_gcm sm4_gcm;\n\t\tstruct tls12_crypto_info_sm4_ccm sm4_ccm;\n\t};\n};\n\nstruct tls_prot_info {\n\tu16 version;\n\tu16 cipher_type;\n\tu16 prepend_size;\n\tu16 tag_size;\n\tu16 overhead_size;\n\tu16 iv_size;\n\tu16 salt_size;\n\tu16 rec_seq_size;\n\tu16 aad_size;\n\tu16 tail_size;\n};\n\nstruct tls_context {\n\t \n\tstruct tls_prot_info prot_info;\n\n\tu8 tx_conf:3;\n\tu8 rx_conf:3;\n\tu8 zerocopy_sendfile:1;\n\tu8 rx_no_pad:1;\n\n\tint (*push_pending_record)(struct sock *sk, int flags);\n\tvoid (*sk_write_space)(struct sock *sk);\n\n\tvoid *priv_ctx_tx;\n\tvoid *priv_ctx_rx;\n\n\tstruct net_device __rcu *netdev;\n\n\t \n\tstruct cipher_context tx;\n\tstruct cipher_context rx;\n\n\tstruct scatterlist *partially_sent_record;\n\tu16 partially_sent_offset;\n\n\tbool splicing_pages;\n\tbool pending_open_record_frags;\n\n\tstruct mutex tx_lock;  \n\tunsigned long flags;\n\n\t \n\tstruct proto *sk_proto;\n\tstruct sock *sk;\n\n\tvoid (*sk_destruct)(struct sock *sk);\n\n\tunion tls_crypto_context crypto_send;\n\tunion tls_crypto_context crypto_recv;\n\n\tstruct list_head list;\n\trefcount_t refcount;\n\tstruct rcu_head rcu;\n};\n\nenum tls_offload_ctx_dir {\n\tTLS_OFFLOAD_CTX_DIR_RX,\n\tTLS_OFFLOAD_CTX_DIR_TX,\n};\n\nstruct tlsdev_ops {\n\tint (*tls_dev_add)(struct net_device *netdev, struct sock *sk,\n\t\t\t   enum tls_offload_ctx_dir direction,\n\t\t\t   struct tls_crypto_info *crypto_info,\n\t\t\t   u32 start_offload_tcp_sn);\n\tvoid (*tls_dev_del)(struct net_device *netdev,\n\t\t\t    struct tls_context *ctx,\n\t\t\t    enum tls_offload_ctx_dir direction);\n\tint (*tls_dev_resync)(struct net_device *netdev,\n\t\t\t      struct sock *sk, u32 seq, u8 *rcd_sn,\n\t\t\t      enum tls_offload_ctx_dir direction);\n};\n\nenum tls_offload_sync_type {\n\tTLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ = 0,\n\tTLS_OFFLOAD_SYNC_TYPE_CORE_NEXT_HINT = 1,\n\tTLS_OFFLOAD_SYNC_TYPE_DRIVER_REQ_ASYNC = 2,\n};\n\n#define TLS_DEVICE_RESYNC_NH_START_IVAL\t\t2\n#define TLS_DEVICE_RESYNC_NH_MAX_IVAL\t\t128\n\n#define TLS_DEVICE_RESYNC_ASYNC_LOGMAX\t\t13\nstruct tls_offload_resync_async {\n\tatomic64_t req;\n\tu16 loglen;\n\tu16 rcd_delta;\n\tu32 log[TLS_DEVICE_RESYNC_ASYNC_LOGMAX];\n};\n\nstruct tls_offload_context_rx {\n\t \n\tstruct tls_sw_context_rx sw;\n\tenum tls_offload_sync_type resync_type;\n\t \n\tu8 resync_nh_reset:1;\n\t \n\tu8 resync_nh_do_now:1;\n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tatomic64_t resync_req;\n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tu32 decrypted_failed;\n\t\t\tu32 decrypted_tgt;\n\t\t} resync_nh;\n\t\t \n\t\tstruct {\n\t\t\tstruct tls_offload_resync_async *resync_async;\n\t\t};\n\t};\n\tu8 driver_state[] __aligned(8);\n\t \n#define TLS_DRIVER_STATE_SIZE_RX\t8\n};\n\n#define TLS_OFFLOAD_CONTEXT_SIZE_RX\t\t\t\t\t\\\n\t(sizeof(struct tls_offload_context_rx) + TLS_DRIVER_STATE_SIZE_RX)\n\nstruct tls_record_info *tls_get_record(struct tls_offload_context_tx *context,\n\t\t\t\t       u32 seq, u64 *p_record_sn);\n\nstatic inline bool tls_record_is_start_marker(struct tls_record_info *rec)\n{\n\treturn rec->len == 0;\n}\n\nstatic inline u32 tls_record_start_seq(struct tls_record_info *rec)\n{\n\treturn rec->end_seq - rec->len;\n}\n\nstruct sk_buff *\ntls_validate_xmit_skb(struct sock *sk, struct net_device *dev,\n\t\t      struct sk_buff *skb);\nstruct sk_buff *\ntls_validate_xmit_skb_sw(struct sock *sk, struct net_device *dev,\n\t\t\t struct sk_buff *skb);\n\nstatic inline bool tls_is_skb_tx_device_offloaded(const struct sk_buff *skb)\n{\n#ifdef CONFIG_TLS_DEVICE\n\tstruct sock *sk = skb->sk;\n\n\treturn sk && sk_fullsock(sk) &&\n\t       (smp_load_acquire(&sk->sk_validate_xmit_skb) ==\n\t       &tls_validate_xmit_skb);\n#else\n\treturn false;\n#endif\n}\n\nstatic inline struct tls_context *tls_get_ctx(const struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\t \n\treturn (__force void *)icsk->icsk_ulp_data;\n}\n\nstatic inline struct tls_sw_context_rx *tls_sw_ctx_rx(\n\t\tconst struct tls_context *tls_ctx)\n{\n\treturn (struct tls_sw_context_rx *)tls_ctx->priv_ctx_rx;\n}\n\nstatic inline struct tls_sw_context_tx *tls_sw_ctx_tx(\n\t\tconst struct tls_context *tls_ctx)\n{\n\treturn (struct tls_sw_context_tx *)tls_ctx->priv_ctx_tx;\n}\n\nstatic inline struct tls_offload_context_tx *\ntls_offload_ctx_tx(const struct tls_context *tls_ctx)\n{\n\treturn (struct tls_offload_context_tx *)tls_ctx->priv_ctx_tx;\n}\n\nstatic inline bool tls_sw_has_ctx_tx(const struct sock *sk)\n{\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\n\tif (!ctx)\n\t\treturn false;\n\treturn !!tls_sw_ctx_tx(ctx);\n}\n\nstatic inline bool tls_sw_has_ctx_rx(const struct sock *sk)\n{\n\tstruct tls_context *ctx = tls_get_ctx(sk);\n\n\tif (!ctx)\n\t\treturn false;\n\treturn !!tls_sw_ctx_rx(ctx);\n}\n\nstatic inline struct tls_offload_context_rx *\ntls_offload_ctx_rx(const struct tls_context *tls_ctx)\n{\n\treturn (struct tls_offload_context_rx *)tls_ctx->priv_ctx_rx;\n}\n\nstatic inline void *__tls_driver_ctx(struct tls_context *tls_ctx,\n\t\t\t\t     enum tls_offload_ctx_dir direction)\n{\n\tif (direction == TLS_OFFLOAD_CTX_DIR_TX)\n\t\treturn tls_offload_ctx_tx(tls_ctx)->driver_state;\n\telse\n\t\treturn tls_offload_ctx_rx(tls_ctx)->driver_state;\n}\n\nstatic inline void *\ntls_driver_ctx(const struct sock *sk, enum tls_offload_ctx_dir direction)\n{\n\treturn __tls_driver_ctx(tls_get_ctx(sk), direction);\n}\n\n#define RESYNC_REQ BIT(0)\n#define RESYNC_REQ_ASYNC BIT(1)\n \nstatic inline void tls_offload_rx_resync_request(struct sock *sk, __be32 seq)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);\n\n\tatomic64_set(&rx_ctx->resync_req, ((u64)ntohl(seq) << 32) | RESYNC_REQ);\n}\n\n \nstatic inline void\ntls_offload_rx_resync_async_request_start(struct sock *sk, __be32 seq, u16 len)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);\n\n\tatomic64_set(&rx_ctx->resync_async->req, ((u64)ntohl(seq) << 32) |\n\t\t     ((u64)len << 16) | RESYNC_REQ | RESYNC_REQ_ASYNC);\n\trx_ctx->resync_async->loglen = 0;\n\trx_ctx->resync_async->rcd_delta = 0;\n}\n\nstatic inline void\ntls_offload_rx_resync_async_request_end(struct sock *sk, __be32 seq)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tstruct tls_offload_context_rx *rx_ctx = tls_offload_ctx_rx(tls_ctx);\n\n\tatomic64_set(&rx_ctx->resync_async->req,\n\t\t     ((u64)ntohl(seq) << 32) | RESYNC_REQ);\n}\n\nstatic inline void\ntls_offload_rx_resync_set_type(struct sock *sk, enum tls_offload_sync_type type)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\n\ttls_offload_ctx_rx(tls_ctx)->resync_type = type;\n}\n\n \nstatic inline bool tls_offload_tx_resync_pending(struct sock *sk)\n{\n\tstruct tls_context *tls_ctx = tls_get_ctx(sk);\n\tbool ret;\n\n\tret = test_bit(TLS_TX_SYNC_SCHED, &tls_ctx->flags);\n\tsmp_mb__after_atomic();\n\treturn ret;\n}\n\nstruct sk_buff *tls_encrypt_skb(struct sk_buff *skb);\n\n#ifdef CONFIG_TLS_DEVICE\nvoid tls_device_sk_destruct(struct sock *sk);\nvoid tls_offload_tx_resync_request(struct sock *sk, u32 got_seq, u32 exp_seq);\n\nstatic inline bool tls_is_sk_rx_device_offloaded(struct sock *sk)\n{\n\tif (!sk_fullsock(sk) ||\n\t    smp_load_acquire(&sk->sk_destruct) != tls_device_sk_destruct)\n\t\treturn false;\n\treturn tls_get_ctx(sk)->rx_conf == TLS_HW;\n}\n#endif\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}