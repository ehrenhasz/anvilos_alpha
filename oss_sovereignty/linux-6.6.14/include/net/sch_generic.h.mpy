{
  "module_name": "sch_generic.h",
  "hash_id": "be59626119f16559452a1c9fd88d78de2ce3e604ba2df8fec90647d80d0527d8",
  "original_prompt": "Ingested from linux-6.6.14/include/net/sch_generic.h",
  "human_readable_source": " \n#ifndef __NET_SCHED_GENERIC_H\n#define __NET_SCHED_GENERIC_H\n\n#include <linux/netdevice.h>\n#include <linux/types.h>\n#include <linux/rcupdate.h>\n#include <linux/pkt_sched.h>\n#include <linux/pkt_cls.h>\n#include <linux/percpu.h>\n#include <linux/dynamic_queue_limits.h>\n#include <linux/list.h>\n#include <linux/refcount.h>\n#include <linux/workqueue.h>\n#include <linux/mutex.h>\n#include <linux/rwsem.h>\n#include <linux/atomic.h>\n#include <linux/hashtable.h>\n#include <net/gen_stats.h>\n#include <net/rtnetlink.h>\n#include <net/flow_offload.h>\n\nstruct Qdisc_ops;\nstruct qdisc_walker;\nstruct tcf_walker;\nstruct module;\nstruct bpf_flow_keys;\n\nstruct qdisc_rate_table {\n\tstruct tc_ratespec rate;\n\tu32\t\tdata[256];\n\tstruct qdisc_rate_table *next;\n\tint\t\trefcnt;\n};\n\nenum qdisc_state_t {\n\t__QDISC_STATE_SCHED,\n\t__QDISC_STATE_DEACTIVATED,\n\t__QDISC_STATE_MISSED,\n\t__QDISC_STATE_DRAINING,\n};\n\nenum qdisc_state2_t {\n\t \n\t__QDISC_STATE2_RUNNING,\n};\n\n#define QDISC_STATE_MISSED\tBIT(__QDISC_STATE_MISSED)\n#define QDISC_STATE_DRAINING\tBIT(__QDISC_STATE_DRAINING)\n\n#define QDISC_STATE_NON_EMPTY\t(QDISC_STATE_MISSED | \\\n\t\t\t\t\tQDISC_STATE_DRAINING)\n\nstruct qdisc_size_table {\n\tstruct rcu_head\t\trcu;\n\tstruct list_head\tlist;\n\tstruct tc_sizespec\tszopts;\n\tint\t\t\trefcnt;\n\tu16\t\t\tdata[];\n};\n\n \nstruct qdisc_skb_head {\n\tstruct sk_buff\t*head;\n\tstruct sk_buff\t*tail;\n\t__u32\t\tqlen;\n\tspinlock_t\tlock;\n};\n\nstruct Qdisc {\n\tint \t\t\t(*enqueue)(struct sk_buff *skb,\n\t\t\t\t\t   struct Qdisc *sch,\n\t\t\t\t\t   struct sk_buff **to_free);\n\tstruct sk_buff *\t(*dequeue)(struct Qdisc *sch);\n\tunsigned int\t\tflags;\n#define TCQ_F_BUILTIN\t\t1\n#define TCQ_F_INGRESS\t\t2\n#define TCQ_F_CAN_BYPASS\t4\n#define TCQ_F_MQROOT\t\t8\n#define TCQ_F_ONETXQUEUE\t0x10  \n#define TCQ_F_WARN_NONWC\t(1 << 16)\n#define TCQ_F_CPUSTATS\t\t0x20  \n#define TCQ_F_NOPARENT\t\t0x40  \n#define TCQ_F_INVISIBLE\t\t0x80  \n#define TCQ_F_NOLOCK\t\t0x100  \n#define TCQ_F_OFFLOADED\t\t0x200  \n\tu32\t\t\tlimit;\n\tconst struct Qdisc_ops\t*ops;\n\tstruct qdisc_size_table\t__rcu *stab;\n\tstruct hlist_node       hash;\n\tu32\t\t\thandle;\n\tu32\t\t\tparent;\n\n\tstruct netdev_queue\t*dev_queue;\n\n\tstruct net_rate_estimator __rcu *rate_est;\n\tstruct gnet_stats_basic_sync __percpu *cpu_bstats;\n\tstruct gnet_stats_queue\t__percpu *cpu_qstats;\n\tint\t\t\tpad;\n\trefcount_t\t\trefcnt;\n\n\t \n\tstruct sk_buff_head\tgso_skb ____cacheline_aligned_in_smp;\n\tstruct qdisc_skb_head\tq;\n\tstruct gnet_stats_basic_sync bstats;\n\tstruct gnet_stats_queue\tqstats;\n\tunsigned long\t\tstate;\n\tunsigned long\t\tstate2;  \n\tstruct Qdisc            *next_sched;\n\tstruct sk_buff_head\tskb_bad_txq;\n\n\tspinlock_t\t\tbusylock ____cacheline_aligned_in_smp;\n\tspinlock_t\t\tseqlock;\n\n\tstruct rcu_head\t\trcu;\n\tnetdevice_tracker\tdev_tracker;\n\t \n\tlong privdata[] ____cacheline_aligned;\n};\n\nstatic inline void qdisc_refcount_inc(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN)\n\t\treturn;\n\trefcount_inc(&qdisc->refcnt);\n}\n\nstatic inline bool qdisc_refcount_dec_if_one(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN)\n\t\treturn true;\n\treturn refcount_dec_if_one(&qdisc->refcnt);\n}\n\n \n\nstatic inline struct Qdisc *qdisc_refcount_inc_nz(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_BUILTIN)\n\t\treturn qdisc;\n\tif (refcount_inc_not_zero(&qdisc->refcnt))\n\t\treturn qdisc;\n\treturn NULL;\n}\n\n \nstatic inline bool qdisc_is_running(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_NOLOCK)\n\t\treturn spin_is_locked(&qdisc->seqlock);\n\treturn test_bit(__QDISC_STATE2_RUNNING, &qdisc->state2);\n}\n\nstatic inline bool nolock_qdisc_is_empty(const struct Qdisc *qdisc)\n{\n\treturn !(READ_ONCE(qdisc->state) & QDISC_STATE_NON_EMPTY);\n}\n\nstatic inline bool qdisc_is_percpu_stats(const struct Qdisc *q)\n{\n\treturn q->flags & TCQ_F_CPUSTATS;\n}\n\nstatic inline bool qdisc_is_empty(const struct Qdisc *qdisc)\n{\n\tif (qdisc_is_percpu_stats(qdisc))\n\t\treturn nolock_qdisc_is_empty(qdisc);\n\treturn !READ_ONCE(qdisc->q.qlen);\n}\n\n \nstatic inline bool qdisc_run_begin(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_NOLOCK) {\n\t\tif (spin_trylock(&qdisc->seqlock))\n\t\t\treturn true;\n\n\t\t \n\t\tif (test_and_set_bit(__QDISC_STATE_MISSED, &qdisc->state))\n\t\t\treturn false;\n\n\t\t \n\t\treturn spin_trylock(&qdisc->seqlock);\n\t}\n\treturn !__test_and_set_bit(__QDISC_STATE2_RUNNING, &qdisc->state2);\n}\n\nstatic inline void qdisc_run_end(struct Qdisc *qdisc)\n{\n\tif (qdisc->flags & TCQ_F_NOLOCK) {\n\t\tspin_unlock(&qdisc->seqlock);\n\n\t\t \n\t\tsmp_mb();\n\n\t\tif (unlikely(test_bit(__QDISC_STATE_MISSED,\n\t\t\t\t      &qdisc->state)))\n\t\t\t__netif_schedule(qdisc);\n\t} else {\n\t\t__clear_bit(__QDISC_STATE2_RUNNING, &qdisc->state2);\n\t}\n}\n\nstatic inline bool qdisc_may_bulk(const struct Qdisc *qdisc)\n{\n\treturn qdisc->flags & TCQ_F_ONETXQUEUE;\n}\n\nstatic inline int qdisc_avail_bulklimit(const struct netdev_queue *txq)\n{\n#ifdef CONFIG_BQL\n\t \n\treturn dql_avail(&txq->dql);\n#else\n\treturn 0;\n#endif\n}\n\nstruct Qdisc_class_ops {\n\tunsigned int\t\tflags;\n\t \n\tstruct netdev_queue *\t(*select_queue)(struct Qdisc *, struct tcmsg *);\n\tint\t\t\t(*graft)(struct Qdisc *, unsigned long cl,\n\t\t\t\t\tstruct Qdisc *, struct Qdisc **,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\n\tstruct Qdisc *\t\t(*leaf)(struct Qdisc *, unsigned long cl);\n\tvoid\t\t\t(*qlen_notify)(struct Qdisc *, unsigned long);\n\n\t \n\tunsigned long\t\t(*find)(struct Qdisc *, u32 classid);\n\tint\t\t\t(*change)(struct Qdisc *, u32, u32,\n\t\t\t\t\tstruct nlattr **, unsigned long *,\n\t\t\t\t\tstruct netlink_ext_ack *);\n\tint\t\t\t(*delete)(struct Qdisc *, unsigned long,\n\t\t\t\t\t  struct netlink_ext_ack *);\n\tvoid\t\t\t(*walk)(struct Qdisc *, struct qdisc_walker * arg);\n\n\t \n\tstruct tcf_block *\t(*tcf_block)(struct Qdisc *sch,\n\t\t\t\t\t     unsigned long arg,\n\t\t\t\t\t     struct netlink_ext_ack *extack);\n\tunsigned long\t\t(*bind_tcf)(struct Qdisc *, unsigned long,\n\t\t\t\t\tu32 classid);\n\tvoid\t\t\t(*unbind_tcf)(struct Qdisc *, unsigned long);\n\n\t \n\tint\t\t\t(*dump)(struct Qdisc *, unsigned long,\n\t\t\t\t\tstruct sk_buff *skb, struct tcmsg*);\n\tint\t\t\t(*dump_stats)(struct Qdisc *, unsigned long,\n\t\t\t\t\tstruct gnet_dump *);\n};\n\n \n\n \nenum qdisc_class_ops_flags {\n\tQDISC_CLASS_OPS_DOIT_UNLOCKED = 1,\n};\n\nstruct Qdisc_ops {\n\tstruct Qdisc_ops\t*next;\n\tconst struct Qdisc_class_ops\t*cl_ops;\n\tchar\t\t\tid[IFNAMSIZ];\n\tint\t\t\tpriv_size;\n\tunsigned int\t\tstatic_flags;\n\n\tint \t\t\t(*enqueue)(struct sk_buff *skb,\n\t\t\t\t\t   struct Qdisc *sch,\n\t\t\t\t\t   struct sk_buff **to_free);\n\tstruct sk_buff *\t(*dequeue)(struct Qdisc *);\n\tstruct sk_buff *\t(*peek)(struct Qdisc *);\n\n\tint\t\t\t(*init)(struct Qdisc *sch, struct nlattr *arg,\n\t\t\t\t\tstruct netlink_ext_ack *extack);\n\tvoid\t\t\t(*reset)(struct Qdisc *);\n\tvoid\t\t\t(*destroy)(struct Qdisc *);\n\tint\t\t\t(*change)(struct Qdisc *sch,\n\t\t\t\t\t  struct nlattr *arg,\n\t\t\t\t\t  struct netlink_ext_ack *extack);\n\tvoid\t\t\t(*attach)(struct Qdisc *sch);\n\tint\t\t\t(*change_tx_queue_len)(struct Qdisc *, unsigned int);\n\tvoid\t\t\t(*change_real_num_tx)(struct Qdisc *sch,\n\t\t\t\t\t\t      unsigned int new_real_tx);\n\n\tint\t\t\t(*dump)(struct Qdisc *, struct sk_buff *);\n\tint\t\t\t(*dump_stats)(struct Qdisc *, struct gnet_dump *);\n\n\tvoid\t\t\t(*ingress_block_set)(struct Qdisc *sch,\n\t\t\t\t\t\t     u32 block_index);\n\tvoid\t\t\t(*egress_block_set)(struct Qdisc *sch,\n\t\t\t\t\t\t    u32 block_index);\n\tu32\t\t\t(*ingress_block_get)(struct Qdisc *sch);\n\tu32\t\t\t(*egress_block_get)(struct Qdisc *sch);\n\n\tstruct module\t\t*owner;\n};\n\n\nstruct tcf_result {\n\tunion {\n\t\tstruct {\n\t\t\tunsigned long\tclass;\n\t\t\tu32\t\tclassid;\n\t\t};\n\t\tconst struct tcf_proto *goto_tp;\n\n\t};\n};\n\nstruct tcf_chain;\n\nstruct tcf_proto_ops {\n\tstruct list_head\thead;\n\tchar\t\t\tkind[IFNAMSIZ];\n\n\tint\t\t\t(*classify)(struct sk_buff *,\n\t\t\t\t\t    const struct tcf_proto *,\n\t\t\t\t\t    struct tcf_result *);\n\tint\t\t\t(*init)(struct tcf_proto*);\n\tvoid\t\t\t(*destroy)(struct tcf_proto *tp, bool rtnl_held,\n\t\t\t\t\t   struct netlink_ext_ack *extack);\n\n\tvoid*\t\t\t(*get)(struct tcf_proto*, u32 handle);\n\tvoid\t\t\t(*put)(struct tcf_proto *tp, void *f);\n\tint\t\t\t(*change)(struct net *net, struct sk_buff *,\n\t\t\t\t\tstruct tcf_proto*, unsigned long,\n\t\t\t\t\tu32 handle, struct nlattr **,\n\t\t\t\t\tvoid **, u32,\n\t\t\t\t\tstruct netlink_ext_ack *);\n\tint\t\t\t(*delete)(struct tcf_proto *tp, void *arg,\n\t\t\t\t\t  bool *last, bool rtnl_held,\n\t\t\t\t\t  struct netlink_ext_ack *);\n\tbool\t\t\t(*delete_empty)(struct tcf_proto *tp);\n\tvoid\t\t\t(*walk)(struct tcf_proto *tp,\n\t\t\t\t\tstruct tcf_walker *arg, bool rtnl_held);\n\tint\t\t\t(*reoffload)(struct tcf_proto *tp, bool add,\n\t\t\t\t\t     flow_setup_cb_t *cb, void *cb_priv,\n\t\t\t\t\t     struct netlink_ext_ack *extack);\n\tvoid\t\t\t(*hw_add)(struct tcf_proto *tp,\n\t\t\t\t\t  void *type_data);\n\tvoid\t\t\t(*hw_del)(struct tcf_proto *tp,\n\t\t\t\t\t  void *type_data);\n\tvoid\t\t\t(*bind_class)(void *, u32, unsigned long,\n\t\t\t\t\t      void *, unsigned long);\n\tvoid *\t\t\t(*tmplt_create)(struct net *net,\n\t\t\t\t\t\tstruct tcf_chain *chain,\n\t\t\t\t\t\tstruct nlattr **tca,\n\t\t\t\t\t\tstruct netlink_ext_ack *extack);\n\tvoid\t\t\t(*tmplt_destroy)(void *tmplt_priv);\n\tstruct tcf_exts *\t(*get_exts)(const struct tcf_proto *tp,\n\t\t\t\t\t    u32 handle);\n\n\t \n\tint\t\t\t(*dump)(struct net*, struct tcf_proto*, void *,\n\t\t\t\t\tstruct sk_buff *skb, struct tcmsg*,\n\t\t\t\t\tbool);\n\tint\t\t\t(*terse_dump)(struct net *net,\n\t\t\t\t\t      struct tcf_proto *tp, void *fh,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      struct tcmsg *t, bool rtnl_held);\n\tint\t\t\t(*tmplt_dump)(struct sk_buff *skb,\n\t\t\t\t\t      struct net *net,\n\t\t\t\t\t      void *tmplt_priv);\n\n\tstruct module\t\t*owner;\n\tint\t\t\tflags;\n};\n\n \nenum tcf_proto_ops_flags {\n\tTCF_PROTO_OPS_DOIT_UNLOCKED = 1,\n};\n\nstruct tcf_proto {\n\t \n\tstruct tcf_proto __rcu\t*next;\n\tvoid __rcu\t\t*root;\n\n\t \n\tint\t\t\t(*classify)(struct sk_buff *,\n\t\t\t\t\t    const struct tcf_proto *,\n\t\t\t\t\t    struct tcf_result *);\n\t__be16\t\t\tprotocol;\n\n\t \n\tu32\t\t\tprio;\n\tvoid\t\t\t*data;\n\tconst struct tcf_proto_ops\t*ops;\n\tstruct tcf_chain\t*chain;\n\t \n\tspinlock_t\t\tlock;\n\tbool\t\t\tdeleting;\n\trefcount_t\t\trefcnt;\n\tstruct rcu_head\t\trcu;\n\tstruct hlist_node\tdestroy_ht_node;\n};\n\nstruct qdisc_skb_cb {\n\tstruct {\n\t\tunsigned int\t\tpkt_len;\n\t\tu16\t\t\tslave_dev_queue_mapping;\n\t\tu16\t\t\ttc_classid;\n\t};\n#define QDISC_CB_PRIV_LEN 20\n\tunsigned char\t\tdata[QDISC_CB_PRIV_LEN];\n};\n\ntypedef void tcf_chain_head_change_t(struct tcf_proto *tp_head, void *priv);\n\nstruct tcf_chain {\n\t \n\tstruct mutex filter_chain_lock;\n\tstruct tcf_proto __rcu *filter_chain;\n\tstruct list_head list;\n\tstruct tcf_block *block;\n\tu32 index;  \n\tunsigned int refcnt;\n\tunsigned int action_refcnt;\n\tbool explicitly_created;\n\tbool flushing;\n\tconst struct tcf_proto_ops *tmplt_ops;\n\tvoid *tmplt_priv;\n\tstruct rcu_head rcu;\n};\n\nstruct tcf_block {\n\t \n\tstruct mutex lock;\n\tstruct list_head chain_list;\n\tu32 index;  \n\tu32 classid;  \n\trefcount_t refcnt;\n\tstruct net *net;\n\tstruct Qdisc *q;\n\tstruct rw_semaphore cb_lock;  \n\tstruct flow_block flow_block;\n\tstruct list_head owner_list;\n\tbool keep_dst;\n\tatomic_t offloadcnt;  \n\tunsigned int nooffloaddevcnt;  \n\tunsigned int lockeddevcnt;  \n\tstruct {\n\t\tstruct tcf_chain *chain;\n\t\tstruct list_head filter_chain_list;\n\t} chain0;\n\tstruct rcu_head rcu;\n\tDECLARE_HASHTABLE(proto_destroy_ht, 7);\n\tstruct mutex proto_destroy_lock;  \n};\n\nstatic inline bool lockdep_tcf_chain_is_locked(struct tcf_chain *chain)\n{\n\treturn lockdep_is_held(&chain->filter_chain_lock);\n}\n\nstatic inline bool lockdep_tcf_proto_is_locked(struct tcf_proto *tp)\n{\n\treturn lockdep_is_held(&tp->lock);\n}\n\n#define tcf_chain_dereference(p, chain)\t\t\t\t\t\\\n\trcu_dereference_protected(p, lockdep_tcf_chain_is_locked(chain))\n\n#define tcf_proto_dereference(p, tp)\t\t\t\t\t\\\n\trcu_dereference_protected(p, lockdep_tcf_proto_is_locked(tp))\n\nstatic inline void qdisc_cb_private_validate(const struct sk_buff *skb, int sz)\n{\n\tstruct qdisc_skb_cb *qcb;\n\n\tBUILD_BUG_ON(sizeof(skb->cb) < sizeof(*qcb));\n\tBUILD_BUG_ON(sizeof(qcb->data) < sz);\n}\n\nstatic inline int qdisc_qlen(const struct Qdisc *q)\n{\n\treturn q->q.qlen;\n}\n\nstatic inline int qdisc_qlen_sum(const struct Qdisc *q)\n{\n\t__u32 qlen = q->qstats.qlen;\n\tint i;\n\n\tif (qdisc_is_percpu_stats(q)) {\n\t\tfor_each_possible_cpu(i)\n\t\t\tqlen += per_cpu_ptr(q->cpu_qstats, i)->qlen;\n\t} else {\n\t\tqlen += q->q.qlen;\n\t}\n\n\treturn qlen;\n}\n\nstatic inline struct qdisc_skb_cb *qdisc_skb_cb(const struct sk_buff *skb)\n{\n\treturn (struct qdisc_skb_cb *)skb->cb;\n}\n\nstatic inline spinlock_t *qdisc_lock(struct Qdisc *qdisc)\n{\n\treturn &qdisc->q.lock;\n}\n\nstatic inline struct Qdisc *qdisc_root(const struct Qdisc *qdisc)\n{\n\tstruct Qdisc *q = rcu_dereference_rtnl(qdisc->dev_queue->qdisc);\n\n\treturn q;\n}\n\nstatic inline struct Qdisc *qdisc_root_bh(const struct Qdisc *qdisc)\n{\n\treturn rcu_dereference_bh(qdisc->dev_queue->qdisc);\n}\n\nstatic inline struct Qdisc *qdisc_root_sleeping(const struct Qdisc *qdisc)\n{\n\treturn rcu_dereference_rtnl(qdisc->dev_queue->qdisc_sleeping);\n}\n\nstatic inline spinlock_t *qdisc_root_sleeping_lock(const struct Qdisc *qdisc)\n{\n\tstruct Qdisc *root = qdisc_root_sleeping(qdisc);\n\n\tASSERT_RTNL();\n\treturn qdisc_lock(root);\n}\n\nstatic inline struct net_device *qdisc_dev(const struct Qdisc *qdisc)\n{\n\treturn qdisc->dev_queue->dev;\n}\n\nstatic inline void sch_tree_lock(struct Qdisc *q)\n{\n\tif (q->flags & TCQ_F_MQROOT)\n\t\tspin_lock_bh(qdisc_lock(q));\n\telse\n\t\tspin_lock_bh(qdisc_root_sleeping_lock(q));\n}\n\nstatic inline void sch_tree_unlock(struct Qdisc *q)\n{\n\tif (q->flags & TCQ_F_MQROOT)\n\t\tspin_unlock_bh(qdisc_lock(q));\n\telse\n\t\tspin_unlock_bh(qdisc_root_sleeping_lock(q));\n}\n\nextern struct Qdisc noop_qdisc;\nextern struct Qdisc_ops noop_qdisc_ops;\nextern struct Qdisc_ops pfifo_fast_ops;\nextern struct Qdisc_ops mq_qdisc_ops;\nextern struct Qdisc_ops noqueue_qdisc_ops;\nextern const struct Qdisc_ops *default_qdisc_ops;\nstatic inline const struct Qdisc_ops *\nget_default_qdisc_ops(const struct net_device *dev, int ntx)\n{\n\treturn ntx < dev->real_num_tx_queues ?\n\t\t\tdefault_qdisc_ops : &pfifo_fast_ops;\n}\n\nstruct Qdisc_class_common {\n\tu32\t\t\tclassid;\n\tunsigned int\t\tfilter_cnt;\n\tstruct hlist_node\thnode;\n};\n\nstruct Qdisc_class_hash {\n\tstruct hlist_head\t*hash;\n\tunsigned int\t\thashsize;\n\tunsigned int\t\thashmask;\n\tunsigned int\t\thashelems;\n};\n\nstatic inline unsigned int qdisc_class_hash(u32 id, u32 mask)\n{\n\tid ^= id >> 8;\n\tid ^= id >> 4;\n\treturn id & mask;\n}\n\nstatic inline struct Qdisc_class_common *\nqdisc_class_find(const struct Qdisc_class_hash *hash, u32 id)\n{\n\tstruct Qdisc_class_common *cl;\n\tunsigned int h;\n\n\tif (!id)\n\t\treturn NULL;\n\n\th = qdisc_class_hash(id, hash->hashmask);\n\thlist_for_each_entry(cl, &hash->hash[h], hnode) {\n\t\tif (cl->classid == id)\n\t\t\treturn cl;\n\t}\n\treturn NULL;\n}\n\nstatic inline bool qdisc_class_in_use(const struct Qdisc_class_common *cl)\n{\n\treturn cl->filter_cnt > 0;\n}\n\nstatic inline void qdisc_class_get(struct Qdisc_class_common *cl)\n{\n\tunsigned int res;\n\n\tif (check_add_overflow(cl->filter_cnt, 1, &res))\n\t\tWARN(1, \"Qdisc class overflow\");\n\n\tcl->filter_cnt = res;\n}\n\nstatic inline void qdisc_class_put(struct Qdisc_class_common *cl)\n{\n\tunsigned int res;\n\n\tif (check_sub_overflow(cl->filter_cnt, 1, &res))\n\t\tWARN(1, \"Qdisc class underflow\");\n\n\tcl->filter_cnt = res;\n}\n\nstatic inline int tc_classid_to_hwtc(struct net_device *dev, u32 classid)\n{\n\tu32 hwtc = TC_H_MIN(classid) - TC_H_MIN_PRIORITY;\n\n\treturn (hwtc < netdev_get_num_tc(dev)) ? hwtc : -EINVAL;\n}\n\nint qdisc_class_hash_init(struct Qdisc_class_hash *);\nvoid qdisc_class_hash_insert(struct Qdisc_class_hash *,\n\t\t\t     struct Qdisc_class_common *);\nvoid qdisc_class_hash_remove(struct Qdisc_class_hash *,\n\t\t\t     struct Qdisc_class_common *);\nvoid qdisc_class_hash_grow(struct Qdisc *, struct Qdisc_class_hash *);\nvoid qdisc_class_hash_destroy(struct Qdisc_class_hash *);\n\nint dev_qdisc_change_tx_queue_len(struct net_device *dev);\nvoid dev_qdisc_change_real_num_tx(struct net_device *dev,\n\t\t\t\t  unsigned int new_real_tx);\nvoid dev_init_scheduler(struct net_device *dev);\nvoid dev_shutdown(struct net_device *dev);\nvoid dev_activate(struct net_device *dev);\nvoid dev_deactivate(struct net_device *dev);\nvoid dev_deactivate_many(struct list_head *head);\nstruct Qdisc *dev_graft_qdisc(struct netdev_queue *dev_queue,\n\t\t\t      struct Qdisc *qdisc);\nvoid qdisc_reset(struct Qdisc *qdisc);\nvoid qdisc_destroy(struct Qdisc *qdisc);\nvoid qdisc_put(struct Qdisc *qdisc);\nvoid qdisc_put_unlocked(struct Qdisc *qdisc);\nvoid qdisc_tree_reduce_backlog(struct Qdisc *qdisc, int n, int len);\n#ifdef CONFIG_NET_SCHED\nint qdisc_offload_dump_helper(struct Qdisc *q, enum tc_setup_type type,\n\t\t\t      void *type_data);\nvoid qdisc_offload_graft_helper(struct net_device *dev, struct Qdisc *sch,\n\t\t\t\tstruct Qdisc *new, struct Qdisc *old,\n\t\t\t\tenum tc_setup_type type, void *type_data,\n\t\t\t\tstruct netlink_ext_ack *extack);\n#else\nstatic inline int\nqdisc_offload_dump_helper(struct Qdisc *q, enum tc_setup_type type,\n\t\t\t  void *type_data)\n{\n\tq->flags &= ~TCQ_F_OFFLOADED;\n\treturn 0;\n}\n\nstatic inline void\nqdisc_offload_graft_helper(struct net_device *dev, struct Qdisc *sch,\n\t\t\t   struct Qdisc *new, struct Qdisc *old,\n\t\t\t   enum tc_setup_type type, void *type_data,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n}\n#endif\nvoid qdisc_offload_query_caps(struct net_device *dev,\n\t\t\t      enum tc_setup_type type,\n\t\t\t      void *caps, size_t caps_len);\nstruct Qdisc *qdisc_alloc(struct netdev_queue *dev_queue,\n\t\t\t  const struct Qdisc_ops *ops,\n\t\t\t  struct netlink_ext_ack *extack);\nvoid qdisc_free(struct Qdisc *qdisc);\nstruct Qdisc *qdisc_create_dflt(struct netdev_queue *dev_queue,\n\t\t\t\tconst struct Qdisc_ops *ops, u32 parentid,\n\t\t\t\tstruct netlink_ext_ack *extack);\nvoid __qdisc_calculate_pkt_len(struct sk_buff *skb,\n\t\t\t       const struct qdisc_size_table *stab);\nint skb_do_redirect(struct sk_buff *);\n\nstatic inline bool skb_at_tc_ingress(const struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_XGRESS\n\treturn skb->tc_at_ingress;\n#else\n\treturn false;\n#endif\n}\n\nstatic inline bool skb_skip_tc_classify(struct sk_buff *skb)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tif (skb->tc_skip_classify) {\n\t\tskb->tc_skip_classify = 0;\n\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\n \nstatic inline void qdisc_reset_all_tx_gt(struct net_device *dev, unsigned int i)\n{\n\tstruct Qdisc *qdisc;\n\n\tfor (; i < dev->num_tx_queues; i++) {\n\t\tqdisc = rtnl_dereference(netdev_get_tx_queue(dev, i)->qdisc);\n\t\tif (qdisc) {\n\t\t\tspin_lock_bh(qdisc_lock(qdisc));\n\t\t\tqdisc_reset(qdisc);\n\t\t\tspin_unlock_bh(qdisc_lock(qdisc));\n\t\t}\n\t}\n}\n\n \nstatic inline bool qdisc_all_tx_empty(const struct net_device *dev)\n{\n\tunsigned int i;\n\n\trcu_read_lock();\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tconst struct Qdisc *q = rcu_dereference(txq->qdisc);\n\n\t\tif (!qdisc_is_empty(q)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn false;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn true;\n}\n\n \nstatic inline bool qdisc_tx_changing(const struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\n\t\tif (rcu_access_pointer(txq->qdisc) !=\n\t\t    rcu_access_pointer(txq->qdisc_sleeping))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nstatic inline bool qdisc_tx_is_noop(const struct net_device *dev)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < dev->num_tx_queues; i++) {\n\t\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, i);\n\t\tif (rcu_access_pointer(txq->qdisc) != &noop_qdisc)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic inline unsigned int qdisc_pkt_len(const struct sk_buff *skb)\n{\n\treturn qdisc_skb_cb(skb)->pkt_len;\n}\n\n \nenum net_xmit_qdisc_t {\n\t__NET_XMIT_STOLEN = 0x00010000,\n\t__NET_XMIT_BYPASS = 0x00020000,\n};\n\n#ifdef CONFIG_NET_CLS_ACT\n#define net_xmit_drop_count(e)\t((e) & __NET_XMIT_STOLEN ? 0 : 1)\n#else\n#define net_xmit_drop_count(e)\t(1)\n#endif\n\nstatic inline void qdisc_calculate_pkt_len(struct sk_buff *skb,\n\t\t\t\t\t   const struct Qdisc *sch)\n{\n#ifdef CONFIG_NET_SCHED\n\tstruct qdisc_size_table *stab = rcu_dereference_bh(sch->stab);\n\n\tif (stab)\n\t\t__qdisc_calculate_pkt_len(skb, stab);\n#endif\n}\n\nstatic inline int qdisc_enqueue(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\tstruct sk_buff **to_free)\n{\n\tqdisc_calculate_pkt_len(skb, sch);\n\treturn sch->enqueue(skb, sch, to_free);\n}\n\nstatic inline void _bstats_update(struct gnet_stats_basic_sync *bstats,\n\t\t\t\t  __u64 bytes, __u32 packets)\n{\n\tu64_stats_update_begin(&bstats->syncp);\n\tu64_stats_add(&bstats->bytes, bytes);\n\tu64_stats_add(&bstats->packets, packets);\n\tu64_stats_update_end(&bstats->syncp);\n}\n\nstatic inline void bstats_update(struct gnet_stats_basic_sync *bstats,\n\t\t\t\t const struct sk_buff *skb)\n{\n\t_bstats_update(bstats,\n\t\t       qdisc_pkt_len(skb),\n\t\t       skb_is_gso(skb) ? skb_shinfo(skb)->gso_segs : 1);\n}\n\nstatic inline void qdisc_bstats_cpu_update(struct Qdisc *sch,\n\t\t\t\t\t   const struct sk_buff *skb)\n{\n\tbstats_update(this_cpu_ptr(sch->cpu_bstats), skb);\n}\n\nstatic inline void qdisc_bstats_update(struct Qdisc *sch,\n\t\t\t\t       const struct sk_buff *skb)\n{\n\tbstats_update(&sch->bstats, skb);\n}\n\nstatic inline void qdisc_qstats_backlog_dec(struct Qdisc *sch,\n\t\t\t\t\t    const struct sk_buff *skb)\n{\n\tsch->qstats.backlog -= qdisc_pkt_len(skb);\n}\n\nstatic inline void qdisc_qstats_cpu_backlog_dec(struct Qdisc *sch,\n\t\t\t\t\t\tconst struct sk_buff *skb)\n{\n\tthis_cpu_sub(sch->cpu_qstats->backlog, qdisc_pkt_len(skb));\n}\n\nstatic inline void qdisc_qstats_backlog_inc(struct Qdisc *sch,\n\t\t\t\t\t    const struct sk_buff *skb)\n{\n\tsch->qstats.backlog += qdisc_pkt_len(skb);\n}\n\nstatic inline void qdisc_qstats_cpu_backlog_inc(struct Qdisc *sch,\n\t\t\t\t\t\tconst struct sk_buff *skb)\n{\n\tthis_cpu_add(sch->cpu_qstats->backlog, qdisc_pkt_len(skb));\n}\n\nstatic inline void qdisc_qstats_cpu_qlen_inc(struct Qdisc *sch)\n{\n\tthis_cpu_inc(sch->cpu_qstats->qlen);\n}\n\nstatic inline void qdisc_qstats_cpu_qlen_dec(struct Qdisc *sch)\n{\n\tthis_cpu_dec(sch->cpu_qstats->qlen);\n}\n\nstatic inline void qdisc_qstats_cpu_requeues_inc(struct Qdisc *sch)\n{\n\tthis_cpu_inc(sch->cpu_qstats->requeues);\n}\n\nstatic inline void __qdisc_qstats_drop(struct Qdisc *sch, int count)\n{\n\tsch->qstats.drops += count;\n}\n\nstatic inline void qstats_drop_inc(struct gnet_stats_queue *qstats)\n{\n\tqstats->drops++;\n}\n\nstatic inline void qstats_overlimit_inc(struct gnet_stats_queue *qstats)\n{\n\tqstats->overlimits++;\n}\n\nstatic inline void qdisc_qstats_drop(struct Qdisc *sch)\n{\n\tqstats_drop_inc(&sch->qstats);\n}\n\nstatic inline void qdisc_qstats_cpu_drop(struct Qdisc *sch)\n{\n\tthis_cpu_inc(sch->cpu_qstats->drops);\n}\n\nstatic inline void qdisc_qstats_overlimit(struct Qdisc *sch)\n{\n\tsch->qstats.overlimits++;\n}\n\nstatic inline int qdisc_qstats_copy(struct gnet_dump *d, struct Qdisc *sch)\n{\n\t__u32 qlen = qdisc_qlen_sum(sch);\n\n\treturn gnet_stats_copy_queue(d, sch->cpu_qstats, &sch->qstats, qlen);\n}\n\nstatic inline void qdisc_qstats_qlen_backlog(struct Qdisc *sch,  __u32 *qlen,\n\t\t\t\t\t     __u32 *backlog)\n{\n\tstruct gnet_stats_queue qstats = { 0 };\n\n\tgnet_stats_add_queue(&qstats, sch->cpu_qstats, &sch->qstats);\n\t*qlen = qstats.qlen + qdisc_qlen(sch);\n\t*backlog = qstats.backlog;\n}\n\nstatic inline void qdisc_tree_flush_backlog(struct Qdisc *sch)\n{\n\t__u32 qlen, backlog;\n\n\tqdisc_qstats_qlen_backlog(sch, &qlen, &backlog);\n\tqdisc_tree_reduce_backlog(sch, qlen, backlog);\n}\n\nstatic inline void qdisc_purge_queue(struct Qdisc *sch)\n{\n\t__u32 qlen, backlog;\n\n\tqdisc_qstats_qlen_backlog(sch, &qlen, &backlog);\n\tqdisc_reset(sch);\n\tqdisc_tree_reduce_backlog(sch, qlen, backlog);\n}\n\nstatic inline void __qdisc_enqueue_tail(struct sk_buff *skb,\n\t\t\t\t\tstruct qdisc_skb_head *qh)\n{\n\tstruct sk_buff *last = qh->tail;\n\n\tif (last) {\n\t\tskb->next = NULL;\n\t\tlast->next = skb;\n\t\tqh->tail = skb;\n\t} else {\n\t\tqh->tail = skb;\n\t\tqh->head = skb;\n\t}\n\tqh->qlen++;\n}\n\nstatic inline int qdisc_enqueue_tail(struct sk_buff *skb, struct Qdisc *sch)\n{\n\t__qdisc_enqueue_tail(skb, &sch->q);\n\tqdisc_qstats_backlog_inc(sch, skb);\n\treturn NET_XMIT_SUCCESS;\n}\n\nstatic inline void __qdisc_enqueue_head(struct sk_buff *skb,\n\t\t\t\t\tstruct qdisc_skb_head *qh)\n{\n\tskb->next = qh->head;\n\n\tif (!qh->head)\n\t\tqh->tail = skb;\n\tqh->head = skb;\n\tqh->qlen++;\n}\n\nstatic inline struct sk_buff *__qdisc_dequeue_head(struct qdisc_skb_head *qh)\n{\n\tstruct sk_buff *skb = qh->head;\n\n\tif (likely(skb != NULL)) {\n\t\tqh->head = skb->next;\n\t\tqh->qlen--;\n\t\tif (qh->head == NULL)\n\t\t\tqh->tail = NULL;\n\t\tskb->next = NULL;\n\t}\n\n\treturn skb;\n}\n\nstatic inline struct sk_buff *qdisc_dequeue_head(struct Qdisc *sch)\n{\n\tstruct sk_buff *skb = __qdisc_dequeue_head(&sch->q);\n\n\tif (likely(skb != NULL)) {\n\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\tqdisc_bstats_update(sch, skb);\n\t}\n\n\treturn skb;\n}\n\n \nstatic inline void __qdisc_drop(struct sk_buff *skb, struct sk_buff **to_free)\n{\n\tskb->next = *to_free;\n\t*to_free = skb;\n}\n\nstatic inline void __qdisc_drop_all(struct sk_buff *skb,\n\t\t\t\t    struct sk_buff **to_free)\n{\n\tif (skb->prev)\n\t\tskb->prev->next = *to_free;\n\telse\n\t\tskb->next = *to_free;\n\t*to_free = skb;\n}\n\nstatic inline unsigned int __qdisc_queue_drop_head(struct Qdisc *sch,\n\t\t\t\t\t\t   struct qdisc_skb_head *qh,\n\t\t\t\t\t\t   struct sk_buff **to_free)\n{\n\tstruct sk_buff *skb = __qdisc_dequeue_head(qh);\n\n\tif (likely(skb != NULL)) {\n\t\tunsigned int len = qdisc_pkt_len(skb);\n\n\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\t__qdisc_drop(skb, to_free);\n\t\treturn len;\n\t}\n\n\treturn 0;\n}\n\nstatic inline struct sk_buff *qdisc_peek_head(struct Qdisc *sch)\n{\n\tconst struct qdisc_skb_head *qh = &sch->q;\n\n\treturn qh->head;\n}\n\n \nstatic inline struct sk_buff *qdisc_peek_dequeued(struct Qdisc *sch)\n{\n\tstruct sk_buff *skb = skb_peek(&sch->gso_skb);\n\n\t \n\tif (!skb) {\n\t\tskb = sch->dequeue(sch);\n\n\t\tif (skb) {\n\t\t\t__skb_queue_head(&sch->gso_skb, skb);\n\t\t\t \n\t\t\tqdisc_qstats_backlog_inc(sch, skb);\n\t\t\tsch->q.qlen++;\n\t\t}\n\t}\n\n\treturn skb;\n}\n\nstatic inline void qdisc_update_stats_at_dequeue(struct Qdisc *sch,\n\t\t\t\t\t\t struct sk_buff *skb)\n{\n\tif (qdisc_is_percpu_stats(sch)) {\n\t\tqdisc_qstats_cpu_backlog_dec(sch, skb);\n\t\tqdisc_bstats_cpu_update(sch, skb);\n\t\tqdisc_qstats_cpu_qlen_dec(sch);\n\t} else {\n\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\tqdisc_bstats_update(sch, skb);\n\t\tsch->q.qlen--;\n\t}\n}\n\nstatic inline void qdisc_update_stats_at_enqueue(struct Qdisc *sch,\n\t\t\t\t\t\t unsigned int pkt_len)\n{\n\tif (qdisc_is_percpu_stats(sch)) {\n\t\tqdisc_qstats_cpu_qlen_inc(sch);\n\t\tthis_cpu_add(sch->cpu_qstats->backlog, pkt_len);\n\t} else {\n\t\tsch->qstats.backlog += pkt_len;\n\t\tsch->q.qlen++;\n\t}\n}\n\n \nstatic inline struct sk_buff *qdisc_dequeue_peeked(struct Qdisc *sch)\n{\n\tstruct sk_buff *skb = skb_peek(&sch->gso_skb);\n\n\tif (skb) {\n\t\tskb = __skb_dequeue(&sch->gso_skb);\n\t\tif (qdisc_is_percpu_stats(sch)) {\n\t\t\tqdisc_qstats_cpu_backlog_dec(sch, skb);\n\t\t\tqdisc_qstats_cpu_qlen_dec(sch);\n\t\t} else {\n\t\t\tqdisc_qstats_backlog_dec(sch, skb);\n\t\t\tsch->q.qlen--;\n\t\t}\n\t} else {\n\t\tskb = sch->dequeue(sch);\n\t}\n\n\treturn skb;\n}\n\nstatic inline void __qdisc_reset_queue(struct qdisc_skb_head *qh)\n{\n\t \n\tASSERT_RTNL();\n\tif (qh->qlen) {\n\t\trtnl_kfree_skbs(qh->head, qh->tail);\n\n\t\tqh->head = NULL;\n\t\tqh->tail = NULL;\n\t\tqh->qlen = 0;\n\t}\n}\n\nstatic inline void qdisc_reset_queue(struct Qdisc *sch)\n{\n\t__qdisc_reset_queue(&sch->q);\n}\n\nstatic inline struct Qdisc *qdisc_replace(struct Qdisc *sch, struct Qdisc *new,\n\t\t\t\t\t  struct Qdisc **pold)\n{\n\tstruct Qdisc *old;\n\n\tsch_tree_lock(sch);\n\told = *pold;\n\t*pold = new;\n\tif (old != NULL)\n\t\tqdisc_purge_queue(old);\n\tsch_tree_unlock(sch);\n\n\treturn old;\n}\n\nstatic inline void rtnl_qdisc_drop(struct sk_buff *skb, struct Qdisc *sch)\n{\n\trtnl_kfree_skbs(skb, skb);\n\tqdisc_qstats_drop(sch);\n}\n\nstatic inline int qdisc_drop_cpu(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t struct sk_buff **to_free)\n{\n\t__qdisc_drop(skb, to_free);\n\tqdisc_qstats_cpu_drop(sch);\n\n\treturn NET_XMIT_DROP;\n}\n\nstatic inline int qdisc_drop(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t     struct sk_buff **to_free)\n{\n\t__qdisc_drop(skb, to_free);\n\tqdisc_qstats_drop(sch);\n\n\treturn NET_XMIT_DROP;\n}\n\nstatic inline int qdisc_drop_all(struct sk_buff *skb, struct Qdisc *sch,\n\t\t\t\t struct sk_buff **to_free)\n{\n\t__qdisc_drop_all(skb, to_free);\n\tqdisc_qstats_drop(sch);\n\n\treturn NET_XMIT_DROP;\n}\n\nstruct psched_ratecfg {\n\tu64\trate_bytes_ps;  \n\tu32\tmult;\n\tu16\toverhead;\n\tu16\tmpu;\n\tu8\tlinklayer;\n\tu8\tshift;\n};\n\nstatic inline u64 psched_l2t_ns(const struct psched_ratecfg *r,\n\t\t\t\tunsigned int len)\n{\n\tlen += r->overhead;\n\n\tif (len < r->mpu)\n\t\tlen = r->mpu;\n\n\tif (unlikely(r->linklayer == TC_LINKLAYER_ATM))\n\t\treturn ((u64)(DIV_ROUND_UP(len,48)*53) * r->mult) >> r->shift;\n\n\treturn ((u64)len * r->mult) >> r->shift;\n}\n\nvoid psched_ratecfg_precompute(struct psched_ratecfg *r,\n\t\t\t       const struct tc_ratespec *conf,\n\t\t\t       u64 rate64);\n\nstatic inline void psched_ratecfg_getrate(struct tc_ratespec *res,\n\t\t\t\t\t  const struct psched_ratecfg *r)\n{\n\tmemset(res, 0, sizeof(*res));\n\n\t \n\tres->rate = min_t(u64, r->rate_bytes_ps, ~0U);\n\n\tres->overhead = r->overhead;\n\tres->mpu = r->mpu;\n\tres->linklayer = (r->linklayer & TC_LINKLAYER_MASK);\n}\n\nstruct psched_pktrate {\n\tu64\trate_pkts_ps;  \n\tu32\tmult;\n\tu8\tshift;\n};\n\nstatic inline u64 psched_pkt2t_ns(const struct psched_pktrate *r,\n\t\t\t\t  unsigned int pkt_num)\n{\n\treturn ((u64)pkt_num * r->mult) >> r->shift;\n}\n\nvoid psched_ppscfg_precompute(struct psched_pktrate *r, u64 pktrate64);\n\n \nstruct mini_Qdisc {\n\tstruct tcf_proto *filter_list;\n\tstruct tcf_block *block;\n\tstruct gnet_stats_basic_sync __percpu *cpu_bstats;\n\tstruct gnet_stats_queue\t__percpu *cpu_qstats;\n\tunsigned long rcu_state;\n};\n\nstatic inline void mini_qdisc_bstats_cpu_update(struct mini_Qdisc *miniq,\n\t\t\t\t\t\tconst struct sk_buff *skb)\n{\n\tbstats_update(this_cpu_ptr(miniq->cpu_bstats), skb);\n}\n\nstatic inline void mini_qdisc_qstats_cpu_drop(struct mini_Qdisc *miniq)\n{\n\tthis_cpu_inc(miniq->cpu_qstats->drops);\n}\n\nstruct mini_Qdisc_pair {\n\tstruct mini_Qdisc miniq1;\n\tstruct mini_Qdisc miniq2;\n\tstruct mini_Qdisc __rcu **p_miniq;\n};\n\nvoid mini_qdisc_pair_swap(struct mini_Qdisc_pair *miniqp,\n\t\t\t  struct tcf_proto *tp_head);\nvoid mini_qdisc_pair_init(struct mini_Qdisc_pair *miniqp, struct Qdisc *qdisc,\n\t\t\t  struct mini_Qdisc __rcu **p_miniq);\nvoid mini_qdisc_pair_block_init(struct mini_Qdisc_pair *miniqp,\n\t\t\t\tstruct tcf_block *block);\n\nvoid mq_change_real_num_tx(struct Qdisc *sch, unsigned int new_real_tx);\n\nint sch_frag_xmit_hook(struct sk_buff *skb, int (*xmit)(struct sk_buff *skb));\n\n \nstatic inline void qdisc_synchronize(const struct Qdisc *q)\n{\n\twhile (test_bit(__QDISC_STATE_SCHED, &q->state))\n\t\tmsleep(1);\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}