{
  "module_name": "xdp_sock_drv.h",
  "hash_id": "e6e0c6d2b9c35cd5b7bdd7d8ac4b179d96fd9dad7543c7967c8c0fa037811be4",
  "original_prompt": "Ingested from linux-6.6.14/include/net/xdp_sock_drv.h",
  "human_readable_source": " \n \n\n#ifndef _LINUX_XDP_SOCK_DRV_H\n#define _LINUX_XDP_SOCK_DRV_H\n\n#include <net/xdp_sock.h>\n#include <net/xsk_buff_pool.h>\n\n#define XDP_UMEM_MIN_CHUNK_SHIFT 11\n#define XDP_UMEM_MIN_CHUNK_SIZE (1 << XDP_UMEM_MIN_CHUNK_SHIFT)\n\n#ifdef CONFIG_XDP_SOCKETS\n\nvoid xsk_tx_completed(struct xsk_buff_pool *pool, u32 nb_entries);\nbool xsk_tx_peek_desc(struct xsk_buff_pool *pool, struct xdp_desc *desc);\nu32 xsk_tx_peek_release_desc_batch(struct xsk_buff_pool *pool, u32 max);\nvoid xsk_tx_release(struct xsk_buff_pool *pool);\nstruct xsk_buff_pool *xsk_get_pool_from_qid(struct net_device *dev,\n\t\t\t\t\t    u16 queue_id);\nvoid xsk_set_rx_need_wakeup(struct xsk_buff_pool *pool);\nvoid xsk_set_tx_need_wakeup(struct xsk_buff_pool *pool);\nvoid xsk_clear_rx_need_wakeup(struct xsk_buff_pool *pool);\nvoid xsk_clear_tx_need_wakeup(struct xsk_buff_pool *pool);\nbool xsk_uses_need_wakeup(struct xsk_buff_pool *pool);\n\nstatic inline u32 xsk_pool_get_headroom(struct xsk_buff_pool *pool)\n{\n\treturn XDP_PACKET_HEADROOM + pool->headroom;\n}\n\nstatic inline u32 xsk_pool_get_chunk_size(struct xsk_buff_pool *pool)\n{\n\treturn pool->chunk_size;\n}\n\nstatic inline u32 xsk_pool_get_rx_frame_size(struct xsk_buff_pool *pool)\n{\n\treturn xsk_pool_get_chunk_size(pool) - xsk_pool_get_headroom(pool);\n}\n\nstatic inline void xsk_pool_set_rxq_info(struct xsk_buff_pool *pool,\n\t\t\t\t\t struct xdp_rxq_info *rxq)\n{\n\txp_set_rxq_info(pool, rxq);\n}\n\nstatic inline unsigned int xsk_pool_get_napi_id(struct xsk_buff_pool *pool)\n{\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\treturn pool->heads[0].xdp.rxq->napi_id;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline void xsk_pool_dma_unmap(struct xsk_buff_pool *pool,\n\t\t\t\t      unsigned long attrs)\n{\n\txp_dma_unmap(pool, attrs);\n}\n\nstatic inline int xsk_pool_dma_map(struct xsk_buff_pool *pool,\n\t\t\t\t   struct device *dev, unsigned long attrs)\n{\n\tstruct xdp_umem *umem = pool->umem;\n\n\treturn xp_dma_map(pool, dev, attrs, umem->pgs, umem->npgs);\n}\n\nstatic inline dma_addr_t xsk_buff_xdp_get_dma(struct xdp_buff *xdp)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(xdp, struct xdp_buff_xsk, xdp);\n\n\treturn xp_get_dma(xskb);\n}\n\nstatic inline dma_addr_t xsk_buff_xdp_get_frame_dma(struct xdp_buff *xdp)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(xdp, struct xdp_buff_xsk, xdp);\n\n\treturn xp_get_frame_dma(xskb);\n}\n\nstatic inline struct xdp_buff *xsk_buff_alloc(struct xsk_buff_pool *pool)\n{\n\treturn xp_alloc(pool);\n}\n\nstatic inline bool xsk_is_eop_desc(struct xdp_desc *desc)\n{\n\treturn !xp_mb_desc(desc);\n}\n\n \nstatic inline u32 xsk_buff_alloc_batch(struct xsk_buff_pool *pool, struct xdp_buff **xdp, u32 max)\n{\n\treturn xp_alloc_batch(pool, xdp, max);\n}\n\nstatic inline bool xsk_buff_can_alloc(struct xsk_buff_pool *pool, u32 count)\n{\n\treturn xp_can_alloc(pool, count);\n}\n\nstatic inline void xsk_buff_free(struct xdp_buff *xdp)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(xdp, struct xdp_buff_xsk, xdp);\n\tstruct list_head *xskb_list = &xskb->pool->xskb_list;\n\tstruct xdp_buff_xsk *pos, *tmp;\n\n\tif (likely(!xdp_buff_has_frags(xdp)))\n\t\tgoto out;\n\n\tlist_for_each_entry_safe(pos, tmp, xskb_list, xskb_list_node) {\n\t\tlist_del(&pos->xskb_list_node);\n\t\txp_free(pos);\n\t}\n\n\txdp_get_shared_info_from_buff(xdp)->nr_frags = 0;\nout:\n\txp_free(xskb);\n}\n\nstatic inline void xsk_buff_add_frag(struct xdp_buff *xdp)\n{\n\tstruct xdp_buff_xsk *frag = container_of(xdp, struct xdp_buff_xsk, xdp);\n\n\tlist_add_tail(&frag->xskb_list_node, &frag->pool->xskb_list);\n}\n\nstatic inline struct xdp_buff *xsk_buff_get_frag(struct xdp_buff *first)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(first, struct xdp_buff_xsk, xdp);\n\tstruct xdp_buff *ret = NULL;\n\tstruct xdp_buff_xsk *frag;\n\n\tfrag = list_first_entry_or_null(&xskb->pool->xskb_list,\n\t\t\t\t\tstruct xdp_buff_xsk, xskb_list_node);\n\tif (frag) {\n\t\tlist_del(&frag->xskb_list_node);\n\t\tret = &frag->xdp;\n\t}\n\n\treturn ret;\n}\n\nstatic inline void xsk_buff_set_size(struct xdp_buff *xdp, u32 size)\n{\n\txdp->data = xdp->data_hard_start + XDP_PACKET_HEADROOM;\n\txdp->data_meta = xdp->data;\n\txdp->data_end = xdp->data + size;\n}\n\nstatic inline dma_addr_t xsk_buff_raw_get_dma(struct xsk_buff_pool *pool,\n\t\t\t\t\t      u64 addr)\n{\n\treturn xp_raw_get_dma(pool, addr);\n}\n\nstatic inline void *xsk_buff_raw_get_data(struct xsk_buff_pool *pool, u64 addr)\n{\n\treturn xp_raw_get_data(pool, addr);\n}\n\nstatic inline void xsk_buff_dma_sync_for_cpu(struct xdp_buff *xdp, struct xsk_buff_pool *pool)\n{\n\tstruct xdp_buff_xsk *xskb = container_of(xdp, struct xdp_buff_xsk, xdp);\n\n\tif (!pool->dma_need_sync)\n\t\treturn;\n\n\txp_dma_sync_for_cpu(xskb);\n}\n\nstatic inline void xsk_buff_raw_dma_sync_for_device(struct xsk_buff_pool *pool,\n\t\t\t\t\t\t    dma_addr_t dma,\n\t\t\t\t\t\t    size_t size)\n{\n\txp_dma_sync_for_device(pool, dma, size);\n}\n\n#else\n\nstatic inline void xsk_tx_completed(struct xsk_buff_pool *pool, u32 nb_entries)\n{\n}\n\nstatic inline bool xsk_tx_peek_desc(struct xsk_buff_pool *pool,\n\t\t\t\t    struct xdp_desc *desc)\n{\n\treturn false;\n}\n\nstatic inline u32 xsk_tx_peek_release_desc_batch(struct xsk_buff_pool *pool, u32 max)\n{\n\treturn 0;\n}\n\nstatic inline void xsk_tx_release(struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline struct xsk_buff_pool *\nxsk_get_pool_from_qid(struct net_device *dev, u16 queue_id)\n{\n\treturn NULL;\n}\n\nstatic inline void xsk_set_rx_need_wakeup(struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline void xsk_set_tx_need_wakeup(struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline void xsk_clear_rx_need_wakeup(struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline void xsk_clear_tx_need_wakeup(struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline bool xsk_uses_need_wakeup(struct xsk_buff_pool *pool)\n{\n\treturn false;\n}\n\nstatic inline u32 xsk_pool_get_headroom(struct xsk_buff_pool *pool)\n{\n\treturn 0;\n}\n\nstatic inline u32 xsk_pool_get_chunk_size(struct xsk_buff_pool *pool)\n{\n\treturn 0;\n}\n\nstatic inline u32 xsk_pool_get_rx_frame_size(struct xsk_buff_pool *pool)\n{\n\treturn 0;\n}\n\nstatic inline void xsk_pool_set_rxq_info(struct xsk_buff_pool *pool,\n\t\t\t\t\t struct xdp_rxq_info *rxq)\n{\n}\n\nstatic inline unsigned int xsk_pool_get_napi_id(struct xsk_buff_pool *pool)\n{\n\treturn 0;\n}\n\nstatic inline void xsk_pool_dma_unmap(struct xsk_buff_pool *pool,\n\t\t\t\t      unsigned long attrs)\n{\n}\n\nstatic inline int xsk_pool_dma_map(struct xsk_buff_pool *pool,\n\t\t\t\t   struct device *dev, unsigned long attrs)\n{\n\treturn 0;\n}\n\nstatic inline dma_addr_t xsk_buff_xdp_get_dma(struct xdp_buff *xdp)\n{\n\treturn 0;\n}\n\nstatic inline dma_addr_t xsk_buff_xdp_get_frame_dma(struct xdp_buff *xdp)\n{\n\treturn 0;\n}\n\nstatic inline struct xdp_buff *xsk_buff_alloc(struct xsk_buff_pool *pool)\n{\n\treturn NULL;\n}\n\nstatic inline bool xsk_is_eop_desc(struct xdp_desc *desc)\n{\n\treturn false;\n}\n\nstatic inline u32 xsk_buff_alloc_batch(struct xsk_buff_pool *pool, struct xdp_buff **xdp, u32 max)\n{\n\treturn 0;\n}\n\nstatic inline bool xsk_buff_can_alloc(struct xsk_buff_pool *pool, u32 count)\n{\n\treturn false;\n}\n\nstatic inline void xsk_buff_free(struct xdp_buff *xdp)\n{\n}\n\nstatic inline void xsk_buff_add_frag(struct xdp_buff *xdp)\n{\n}\n\nstatic inline struct xdp_buff *xsk_buff_get_frag(struct xdp_buff *first)\n{\n\treturn NULL;\n}\n\nstatic inline void xsk_buff_set_size(struct xdp_buff *xdp, u32 size)\n{\n}\n\nstatic inline dma_addr_t xsk_buff_raw_get_dma(struct xsk_buff_pool *pool,\n\t\t\t\t\t      u64 addr)\n{\n\treturn 0;\n}\n\nstatic inline void *xsk_buff_raw_get_data(struct xsk_buff_pool *pool, u64 addr)\n{\n\treturn NULL;\n}\n\nstatic inline void xsk_buff_dma_sync_for_cpu(struct xdp_buff *xdp, struct xsk_buff_pool *pool)\n{\n}\n\nstatic inline void xsk_buff_raw_dma_sync_for_device(struct xsk_buff_pool *pool,\n\t\t\t\t\t\t    dma_addr_t dma,\n\t\t\t\t\t\t    size_t size)\n{\n}\n\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}