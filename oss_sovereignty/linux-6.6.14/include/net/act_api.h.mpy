{
  "module_name": "act_api.h",
  "hash_id": "453fbde2a1ef196b71be9613847dcfb40ed691222e6f8f453162f29d6b5a531e",
  "original_prompt": "Ingested from linux-6.6.14/include/net/act_api.h",
  "human_readable_source": " \n#ifndef __NET_ACT_API_H\n#define __NET_ACT_API_H\n\n \n\n#include <linux/refcount.h>\n#include <net/flow_offload.h>\n#include <net/sch_generic.h>\n#include <net/pkt_sched.h>\n#include <net/net_namespace.h>\n#include <net/netns/generic.h>\n\nstruct tcf_idrinfo {\n\tstruct mutex\tlock;\n\tstruct idr\taction_idr;\n\tstruct net\t*net;\n};\n\nstruct tc_action_ops;\n\nstruct tc_action {\n\tconst struct tc_action_ops\t*ops;\n\t__u32\t\t\t\ttype;  \n\tstruct tcf_idrinfo\t\t*idrinfo;\n\n\tu32\t\t\t\ttcfa_index;\n\trefcount_t\t\t\ttcfa_refcnt;\n\tatomic_t\t\t\ttcfa_bindcnt;\n\tint\t\t\t\ttcfa_action;\n\tstruct tcf_t\t\t\ttcfa_tm;\n\tstruct gnet_stats_basic_sync\ttcfa_bstats;\n\tstruct gnet_stats_basic_sync\ttcfa_bstats_hw;\n\tstruct gnet_stats_queue\t\ttcfa_qstats;\n\tstruct net_rate_estimator __rcu *tcfa_rate_est;\n\tspinlock_t\t\t\ttcfa_lock;\n\tstruct gnet_stats_basic_sync __percpu *cpu_bstats;\n\tstruct gnet_stats_basic_sync __percpu *cpu_bstats_hw;\n\tstruct gnet_stats_queue __percpu *cpu_qstats;\n\tstruct tc_cookie\t__rcu *user_cookie;\n\tstruct tcf_chain\t__rcu *goto_chain;\n\tu32\t\t\ttcfa_flags;\n\tu8\t\t\thw_stats;\n\tu8\t\t\tused_hw_stats;\n\tbool\t\t\tused_hw_stats_valid;\n\tu32\t\t\tin_hw_count;\n};\n#define tcf_index\tcommon.tcfa_index\n#define tcf_refcnt\tcommon.tcfa_refcnt\n#define tcf_bindcnt\tcommon.tcfa_bindcnt\n#define tcf_action\tcommon.tcfa_action\n#define tcf_tm\t\tcommon.tcfa_tm\n#define tcf_bstats\tcommon.tcfa_bstats\n#define tcf_qstats\tcommon.tcfa_qstats\n#define tcf_rate_est\tcommon.tcfa_rate_est\n#define tcf_lock\tcommon.tcfa_lock\n\n#define TCA_ACT_HW_STATS_ANY (TCA_ACT_HW_STATS_IMMEDIATE | \\\n\t\t\t      TCA_ACT_HW_STATS_DELAYED)\n\n \n#define TCA_ACT_FLAGS_USER_BITS 16\n#define TCA_ACT_FLAGS_USER_MASK 0xffff\n#define TCA_ACT_FLAGS_POLICE\t(1U << TCA_ACT_FLAGS_USER_BITS)\n#define TCA_ACT_FLAGS_BIND\t(1U << (TCA_ACT_FLAGS_USER_BITS + 1))\n#define TCA_ACT_FLAGS_REPLACE\t(1U << (TCA_ACT_FLAGS_USER_BITS + 2))\n#define TCA_ACT_FLAGS_NO_RTNL\t(1U << (TCA_ACT_FLAGS_USER_BITS + 3))\n#define TCA_ACT_FLAGS_AT_INGRESS\t(1U << (TCA_ACT_FLAGS_USER_BITS + 4))\n\n \nstatic inline void tcf_lastuse_update(struct tcf_t *tm)\n{\n\tunsigned long now = jiffies;\n\n\tif (tm->lastuse != now)\n\t\ttm->lastuse = now;\n\tif (unlikely(!tm->firstuse))\n\t\ttm->firstuse = now;\n}\n\nstatic inline void tcf_tm_dump(struct tcf_t *dtm, const struct tcf_t *stm)\n{\n\tdtm->install = jiffies_to_clock_t(jiffies - stm->install);\n\tdtm->lastuse = jiffies_to_clock_t(jiffies - stm->lastuse);\n\tdtm->firstuse = stm->firstuse ?\n\t\tjiffies_to_clock_t(jiffies - stm->firstuse) : 0;\n\tdtm->expires = jiffies_to_clock_t(stm->expires);\n}\n\nstatic inline enum flow_action_hw_stats tc_act_hw_stats(u8 hw_stats)\n{\n\tif (WARN_ON_ONCE(hw_stats > TCA_ACT_HW_STATS_ANY))\n\t\treturn FLOW_ACTION_HW_STATS_DONT_CARE;\n\telse if (!hw_stats)\n\t\treturn FLOW_ACTION_HW_STATS_DISABLED;\n\n\treturn hw_stats;\n}\n\ntypedef void (*tc_action_priv_destructor)(void *priv);\n\nstruct tc_action_ops {\n\tstruct list_head head;\n\tchar    kind[IFNAMSIZ];\n\tenum tca_id  id;  \n\tunsigned int\tnet_id;\n\tsize_t\tsize;\n\tstruct module\t\t*owner;\n\tint     (*act)(struct sk_buff *, const struct tc_action *,\n\t\t       struct tcf_result *);  \n\tint     (*dump)(struct sk_buff *, struct tc_action *, int, int);\n\tvoid\t(*cleanup)(struct tc_action *);\n\tint     (*lookup)(struct net *net, struct tc_action **a, u32 index);\n\tint     (*init)(struct net *net, struct nlattr *nla,\n\t\t\tstruct nlattr *est, struct tc_action **act,\n\t\t\tstruct tcf_proto *tp,\n\t\t\tu32 flags, struct netlink_ext_ack *extack);\n\tint     (*walk)(struct net *, struct sk_buff *,\n\t\t\tstruct netlink_callback *, int,\n\t\t\tconst struct tc_action_ops *,\n\t\t\tstruct netlink_ext_ack *);\n\tvoid\t(*stats_update)(struct tc_action *, u64, u64, u64, u64, bool);\n\tsize_t  (*get_fill_size)(const struct tc_action *act);\n\tstruct net_device *(*get_dev)(const struct tc_action *a,\n\t\t\t\t      tc_action_priv_destructor *destructor);\n\tstruct psample_group *\n\t(*get_psample_group)(const struct tc_action *a,\n\t\t\t     tc_action_priv_destructor *destructor);\n\tint     (*offload_act_setup)(struct tc_action *act, void *entry_data,\n\t\t\t\t     u32 *index_inc, bool bind,\n\t\t\t\t     struct netlink_ext_ack *extack);\n};\n\n#ifdef CONFIG_NET_CLS_ACT\n\n#define ACT_P_CREATED 1\n#define ACT_P_DELETED 1\n\nstruct tc_action_net {\n\tstruct tcf_idrinfo *idrinfo;\n\tconst struct tc_action_ops *ops;\n};\n\nstatic inline\nint tc_action_net_init(struct net *net, struct tc_action_net *tn,\n\t\t       const struct tc_action_ops *ops)\n{\n\tint err = 0;\n\n\ttn->idrinfo = kmalloc(sizeof(*tn->idrinfo), GFP_KERNEL);\n\tif (!tn->idrinfo)\n\t\treturn -ENOMEM;\n\ttn->ops = ops;\n\ttn->idrinfo->net = net;\n\tmutex_init(&tn->idrinfo->lock);\n\tidr_init(&tn->idrinfo->action_idr);\n\treturn err;\n}\n\nvoid tcf_idrinfo_destroy(const struct tc_action_ops *ops,\n\t\t\t struct tcf_idrinfo *idrinfo);\n\nstatic inline void tc_action_net_exit(struct list_head *net_list,\n\t\t\t\t      unsigned int id)\n{\n\tstruct net *net;\n\n\trtnl_lock();\n\tlist_for_each_entry(net, net_list, exit_list) {\n\t\tstruct tc_action_net *tn = net_generic(net, id);\n\n\t\ttcf_idrinfo_destroy(tn->ops, tn->idrinfo);\n\t\tkfree(tn->idrinfo);\n\t}\n\trtnl_unlock();\n}\n\nint tcf_generic_walker(struct tc_action_net *tn, struct sk_buff *skb,\n\t\t       struct netlink_callback *cb, int type,\n\t\t       const struct tc_action_ops *ops,\n\t\t       struct netlink_ext_ack *extack);\nint tcf_idr_search(struct tc_action_net *tn, struct tc_action **a, u32 index);\nint tcf_idr_create(struct tc_action_net *tn, u32 index, struct nlattr *est,\n\t\t   struct tc_action **a, const struct tc_action_ops *ops,\n\t\t   int bind, bool cpustats, u32 flags);\nint tcf_idr_create_from_flags(struct tc_action_net *tn, u32 index,\n\t\t\t      struct nlattr *est, struct tc_action **a,\n\t\t\t      const struct tc_action_ops *ops, int bind,\n\t\t\t      u32 flags);\nvoid tcf_idr_insert_many(struct tc_action *actions[]);\nvoid tcf_idr_cleanup(struct tc_action_net *tn, u32 index);\nint tcf_idr_check_alloc(struct tc_action_net *tn, u32 *index,\n\t\t\tstruct tc_action **a, int bind);\nint tcf_idr_release(struct tc_action *a, bool bind);\n\nint tcf_register_action(struct tc_action_ops *a, struct pernet_operations *ops);\nint tcf_unregister_action(struct tc_action_ops *a,\n\t\t\t  struct pernet_operations *ops);\nint tcf_action_destroy(struct tc_action *actions[], int bind);\nint tcf_action_exec(struct sk_buff *skb, struct tc_action **actions,\n\t\t    int nr_actions, struct tcf_result *res);\nint tcf_action_init(struct net *net, struct tcf_proto *tp, struct nlattr *nla,\n\t\t    struct nlattr *est,\n\t\t    struct tc_action *actions[], int init_res[], size_t *attr_size,\n\t\t    u32 flags, u32 fl_flags, struct netlink_ext_ack *extack);\nstruct tc_action_ops *tc_action_load_ops(struct nlattr *nla, bool police,\n\t\t\t\t\t bool rtnl_held,\n\t\t\t\t\t struct netlink_ext_ack *extack);\nstruct tc_action *tcf_action_init_1(struct net *net, struct tcf_proto *tp,\n\t\t\t\t    struct nlattr *nla, struct nlattr *est,\n\t\t\t\t    struct tc_action_ops *a_o, int *init_res,\n\t\t\t\t    u32 flags, struct netlink_ext_ack *extack);\nint tcf_action_dump(struct sk_buff *skb, struct tc_action *actions[], int bind,\n\t\t    int ref, bool terse);\nint tcf_action_dump_old(struct sk_buff *skb, struct tc_action *a, int, int);\nint tcf_action_dump_1(struct sk_buff *skb, struct tc_action *a, int, int);\n\nstatic inline void tcf_action_update_bstats(struct tc_action *a,\n\t\t\t\t\t    struct sk_buff *skb)\n{\n\tif (likely(a->cpu_bstats)) {\n\t\tbstats_update(this_cpu_ptr(a->cpu_bstats), skb);\n\t\treturn;\n\t}\n\tspin_lock(&a->tcfa_lock);\n\tbstats_update(&a->tcfa_bstats, skb);\n\tspin_unlock(&a->tcfa_lock);\n}\n\nstatic inline void tcf_action_inc_drop_qstats(struct tc_action *a)\n{\n\tif (likely(a->cpu_qstats)) {\n\t\tqstats_drop_inc(this_cpu_ptr(a->cpu_qstats));\n\t\treturn;\n\t}\n\tspin_lock(&a->tcfa_lock);\n\tqstats_drop_inc(&a->tcfa_qstats);\n\tspin_unlock(&a->tcfa_lock);\n}\n\nstatic inline void tcf_action_inc_overlimit_qstats(struct tc_action *a)\n{\n\tif (likely(a->cpu_qstats)) {\n\t\tqstats_overlimit_inc(this_cpu_ptr(a->cpu_qstats));\n\t\treturn;\n\t}\n\tspin_lock(&a->tcfa_lock);\n\tqstats_overlimit_inc(&a->tcfa_qstats);\n\tspin_unlock(&a->tcfa_lock);\n}\n\nvoid tcf_action_update_stats(struct tc_action *a, u64 bytes, u64 packets,\n\t\t\t     u64 drops, bool hw);\nint tcf_action_copy_stats(struct sk_buff *, struct tc_action *, int);\n\nint tcf_action_update_hw_stats(struct tc_action *action);\nint tcf_action_reoffload_cb(flow_indr_block_bind_cb_t *cb,\n\t\t\t    void *cb_priv, bool add);\nint tcf_action_check_ctrlact(int action, struct tcf_proto *tp,\n\t\t\t     struct tcf_chain **handle,\n\t\t\t     struct netlink_ext_ack *newchain);\nstruct tcf_chain *tcf_action_set_ctrlact(struct tc_action *a, int action,\n\t\t\t\t\t struct tcf_chain *newchain);\n\n#ifdef CONFIG_INET\nDECLARE_STATIC_KEY_FALSE(tcf_frag_xmit_count);\n#endif\n\nint tcf_dev_queue_xmit(struct sk_buff *skb, int (*xmit)(struct sk_buff *skb));\n\n#else  \n\nstatic inline int tcf_action_reoffload_cb(flow_indr_block_bind_cb_t *cb,\n\t\t\t\t\t  void *cb_priv, bool add) {\n\treturn 0;\n}\n\n#endif  \n\nstatic inline void tcf_action_stats_update(struct tc_action *a, u64 bytes,\n\t\t\t\t\t   u64 packets, u64 drops,\n\t\t\t\t\t   u64 lastuse, bool hw)\n{\n#ifdef CONFIG_NET_CLS_ACT\n\tif (!a->ops->stats_update)\n\t\treturn;\n\n\ta->ops->stats_update(a, bytes, packets, drops, lastuse, hw);\n#endif\n}\n\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}