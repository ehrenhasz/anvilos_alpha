{
  "module_name": "tcp.h",
  "hash_id": "5aa6f478bbf2a24c6d01e03b08e94c14b9642686d7b56193361d6de28ba27569",
  "original_prompt": "Ingested from linux-6.6.14/include/net/tcp.h",
  "human_readable_source": " \n \n#ifndef _TCP_H\n#define _TCP_H\n\n#define FASTRETRANS_DEBUG 1\n\n#include <linux/list.h>\n#include <linux/tcp.h>\n#include <linux/bug.h>\n#include <linux/slab.h>\n#include <linux/cache.h>\n#include <linux/percpu.h>\n#include <linux/skbuff.h>\n#include <linux/kref.h>\n#include <linux/ktime.h>\n#include <linux/indirect_call_wrapper.h>\n\n#include <net/inet_connection_sock.h>\n#include <net/inet_timewait_sock.h>\n#include <net/inet_hashtables.h>\n#include <net/checksum.h>\n#include <net/request_sock.h>\n#include <net/sock_reuseport.h>\n#include <net/sock.h>\n#include <net/snmp.h>\n#include <net/ip.h>\n#include <net/tcp_states.h>\n#include <net/inet_ecn.h>\n#include <net/dst.h>\n#include <net/mptcp.h>\n\n#include <linux/seq_file.h>\n#include <linux/memcontrol.h>\n#include <linux/bpf-cgroup.h>\n#include <linux/siphash.h>\n\nextern struct inet_hashinfo tcp_hashinfo;\n\nDECLARE_PER_CPU(unsigned int, tcp_orphan_count);\nint tcp_orphan_count_sum(void);\n\nvoid tcp_time_wait(struct sock *sk, int state, int timeo);\n\n#define MAX_TCP_HEADER\tL1_CACHE_ALIGN(128 + MAX_HEADER)\n#define MAX_TCP_OPTION_SPACE 40\n#define TCP_MIN_SND_MSS\t\t48\n#define TCP_MIN_GSO_SIZE\t(TCP_MIN_SND_MSS - MAX_TCP_OPTION_SPACE)\n\n \n#define MAX_TCP_WINDOW\t\t32767U\n\n \n#define TCP_MIN_MSS\t\t88U\n\n \n#define TCP_BASE_MSS\t\t1024\n\n \n#define TCP_PROBE_INTERVAL\t600\n\n \n#define TCP_PROBE_THRESHOLD\t8\n\n \n#define TCP_FASTRETRANS_THRESH 3\n\n \n#define TCP_MAX_QUICKACKS\t16U\n\n \n#define TCP_MAX_WSCALE\t\t14U\n\n \n#define TCP_URG_VALID\t0x0100\n#define TCP_URG_NOTYET\t0x0200\n#define TCP_URG_READ\t0x0400\n\n#define TCP_RETR1\t3\t \n\n#define TCP_RETR2\t15\t \n\n#define TCP_SYN_RETRIES\t 6\t \n\n#define TCP_SYNACK_RETRIES 5\t \n\n#define TCP_TIMEWAIT_LEN (60*HZ)  \n#define TCP_FIN_TIMEOUT\tTCP_TIMEWAIT_LEN\n                                  \n#define TCP_FIN_TIMEOUT_MAX (120 * HZ)  \n\n#define TCP_DELACK_MAX\t((unsigned)(HZ/5))\t \n#if HZ >= 100\n#define TCP_DELACK_MIN\t((unsigned)(HZ/25))\t \n#define TCP_ATO_MIN\t((unsigned)(HZ/25))\n#else\n#define TCP_DELACK_MIN\t4U\n#define TCP_ATO_MIN\t4U\n#endif\n#define TCP_RTO_MAX\t((unsigned)(120*HZ))\n#define TCP_RTO_MIN\t((unsigned)(HZ/5))\n#define TCP_TIMEOUT_MIN\t(2U)  \n\n#define TCP_TIMEOUT_MIN_US (2*USEC_PER_MSEC)  \n\n#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))\t \n#define TCP_TIMEOUT_FALLBACK ((unsigned)(3*HZ))\t \n\n#define TCP_RESOURCE_PROBE_INTERVAL ((unsigned)(HZ/2U))  \n#define TCP_KEEPALIVE_TIME\t(120*60*HZ)\t \n#define TCP_KEEPALIVE_PROBES\t9\t\t \n#define TCP_KEEPALIVE_INTVL\t(75*HZ)\n\n#define MAX_TCP_KEEPIDLE\t32767\n#define MAX_TCP_KEEPINTVL\t32767\n#define MAX_TCP_KEEPCNT\t\t127\n#define MAX_TCP_SYNCNT\t\t127\n\n#define TCP_PAWS_24DAYS\t(60 * 60 * 24 * 24)\n#define TCP_PAWS_MSL\t60\t\t \n#define TCP_PAWS_WINDOW\t1\t\t \n \n\n#define TCPOPT_NOP\t\t1\t \n#define TCPOPT_EOL\t\t0\t \n#define TCPOPT_MSS\t\t2\t \n#define TCPOPT_WINDOW\t\t3\t \n#define TCPOPT_SACK_PERM        4        \n#define TCPOPT_SACK             5        \n#define TCPOPT_TIMESTAMP\t8\t \n#define TCPOPT_MD5SIG\t\t19\t \n#define TCPOPT_MPTCP\t\t30\t \n#define TCPOPT_FASTOPEN\t\t34\t \n#define TCPOPT_EXP\t\t254\t \n \n#define TCPOPT_FASTOPEN_MAGIC\t0xF989\n#define TCPOPT_SMC_MAGIC\t0xE2D4C3D9\n\n \n\n#define TCPOLEN_MSS            4\n#define TCPOLEN_WINDOW         3\n#define TCPOLEN_SACK_PERM      2\n#define TCPOLEN_TIMESTAMP      10\n#define TCPOLEN_MD5SIG         18\n#define TCPOLEN_FASTOPEN_BASE  2\n#define TCPOLEN_EXP_FASTOPEN_BASE  4\n#define TCPOLEN_EXP_SMC_BASE   6\n\n \n#define TCPOLEN_TSTAMP_ALIGNED\t\t12\n#define TCPOLEN_WSCALE_ALIGNED\t\t4\n#define TCPOLEN_SACKPERM_ALIGNED\t4\n#define TCPOLEN_SACK_BASE\t\t2\n#define TCPOLEN_SACK_BASE_ALIGNED\t4\n#define TCPOLEN_SACK_PERBLOCK\t\t8\n#define TCPOLEN_MD5SIG_ALIGNED\t\t20\n#define TCPOLEN_MSS_ALIGNED\t\t4\n#define TCPOLEN_EXP_SMC_BASE_ALIGNED\t8\n\n \n#define TCP_NAGLE_OFF\t\t1\t \n#define TCP_NAGLE_CORK\t\t2\t \n#define TCP_NAGLE_PUSH\t\t4\t \n\n \n#define TCP_THIN_LINEAR_RETRIES 6        \n\n \n#define TCP_INIT_CWND\t\t10\n\n \n#define\tTFO_CLIENT_ENABLE\t1\n#define\tTFO_SERVER_ENABLE\t2\n#define\tTFO_CLIENT_NO_COOKIE\t4\t \n\n \n#define\tTFO_SERVER_COOKIE_NOT_REQD\t0x200\n\n \n#define\tTFO_SERVER_WO_SOCKOPT1\t0x400\n\n\n \nextern int sysctl_tcp_max_orphans;\nextern long sysctl_tcp_mem[3];\n\n#define TCP_RACK_LOSS_DETECTION  0x1  \n#define TCP_RACK_STATIC_REO_WND  0x2  \n#define TCP_RACK_NO_DUPTHRESH    0x4  \n\nextern atomic_long_t tcp_memory_allocated;\nDECLARE_PER_CPU(int, tcp_memory_per_cpu_fw_alloc);\n\nextern struct percpu_counter tcp_sockets_allocated;\nextern unsigned long tcp_memory_pressure;\n\n \nstatic inline bool tcp_under_memory_pressure(const struct sock *sk)\n{\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg &&\n\t    mem_cgroup_under_socket_pressure(sk->sk_memcg))\n\t\treturn true;\n\n\treturn READ_ONCE(tcp_memory_pressure);\n}\n \n\nstatic inline bool before(__u32 seq1, __u32 seq2)\n{\n        return (__s32)(seq1-seq2) < 0;\n}\n#define after(seq2, seq1) \tbefore(seq1, seq2)\n\n \nstatic inline bool between(__u32 seq1, __u32 seq2, __u32 seq3)\n{\n\treturn seq3 - seq2 >= seq1 - seq2;\n}\n\nstatic inline bool tcp_out_of_memory(struct sock *sk)\n{\n\tif (sk->sk_wmem_queued > SOCK_MIN_SNDBUF &&\n\t    sk_memory_allocated(sk) > sk_prot_mem_limits(sk, 2))\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline void tcp_wmem_free_skb(struct sock *sk, struct sk_buff *skb)\n{\n\tsk_wmem_queued_add(sk, -skb->truesize);\n\tif (!skb_zcopy_pure(skb))\n\t\tsk_mem_uncharge(sk, skb->truesize);\n\telse\n\t\tsk_mem_uncharge(sk, SKB_TRUESIZE(skb_end_offset(skb)));\n\t__kfree_skb(skb);\n}\n\nvoid sk_forced_mem_schedule(struct sock *sk, int size);\n\nbool tcp_check_oom(struct sock *sk, int shift);\n\n\nextern struct proto tcp_prot;\n\n#define TCP_INC_STATS(net, field)\tSNMP_INC_STATS((net)->mib.tcp_statistics, field)\n#define __TCP_INC_STATS(net, field)\t__SNMP_INC_STATS((net)->mib.tcp_statistics, field)\n#define TCP_DEC_STATS(net, field)\tSNMP_DEC_STATS((net)->mib.tcp_statistics, field)\n#define TCP_ADD_STATS(net, field, val)\tSNMP_ADD_STATS((net)->mib.tcp_statistics, field, val)\n\nvoid tcp_tasklet_init(void);\n\nint tcp_v4_err(struct sk_buff *skb, u32);\n\nvoid tcp_shutdown(struct sock *sk, int how);\n\nint tcp_v4_early_demux(struct sk_buff *skb);\nint tcp_v4_rcv(struct sk_buff *skb);\n\nvoid tcp_remove_empty_skb(struct sock *sk);\nint tcp_sendmsg(struct sock *sk, struct msghdr *msg, size_t size);\nint tcp_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t size);\nint tcp_sendmsg_fastopen(struct sock *sk, struct msghdr *msg, int *copied,\n\t\t\t size_t size, struct ubuf_info *uarg);\nvoid tcp_splice_eof(struct socket *sock);\nint tcp_send_mss(struct sock *sk, int *size_goal, int flags);\nint tcp_wmem_schedule(struct sock *sk, int copy);\nvoid tcp_push(struct sock *sk, int flags, int mss_now, int nonagle,\n\t      int size_goal);\nvoid tcp_release_cb(struct sock *sk);\nvoid tcp_wfree(struct sk_buff *skb);\nvoid tcp_write_timer_handler(struct sock *sk);\nvoid tcp_delack_timer_handler(struct sock *sk);\nint tcp_ioctl(struct sock *sk, int cmd, int *karg);\nint tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb);\nvoid tcp_rcv_established(struct sock *sk, struct sk_buff *skb);\nvoid tcp_rcv_space_adjust(struct sock *sk);\nint tcp_twsk_unique(struct sock *sk, struct sock *sktw, void *twp);\nvoid tcp_twsk_destructor(struct sock *sk);\nvoid tcp_twsk_purge(struct list_head *net_exit_list, int family);\nssize_t tcp_splice_read(struct socket *sk, loff_t *ppos,\n\t\t\tstruct pipe_inode_info *pipe, size_t len,\n\t\t\tunsigned int flags);\nstruct sk_buff *tcp_stream_alloc_skb(struct sock *sk, gfp_t gfp,\n\t\t\t\t     bool force_schedule);\n\nstatic inline void tcp_dec_quickack_mode(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ack.quick) {\n\t\t \n\t\tconst unsigned int pkts = inet_csk_ack_scheduled(sk) ? 1 : 0;\n\n\t\tif (pkts >= icsk->icsk_ack.quick) {\n\t\t\ticsk->icsk_ack.quick = 0;\n\t\t\t \n\t\t\ticsk->icsk_ack.ato   = TCP_ATO_MIN;\n\t\t} else\n\t\t\ticsk->icsk_ack.quick -= pkts;\n\t}\n}\n\n#define\tTCP_ECN_OK\t\t1\n#define\tTCP_ECN_QUEUE_CWR\t2\n#define\tTCP_ECN_DEMAND_CWR\t4\n#define\tTCP_ECN_SEEN\t\t8\n\nenum tcp_tw_status {\n\tTCP_TW_SUCCESS = 0,\n\tTCP_TW_RST = 1,\n\tTCP_TW_ACK = 2,\n\tTCP_TW_SYN = 3\n};\n\n\nenum tcp_tw_status tcp_timewait_state_process(struct inet_timewait_sock *tw,\n\t\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t\t      const struct tcphdr *th);\nstruct sock *tcp_check_req(struct sock *sk, struct sk_buff *skb,\n\t\t\t   struct request_sock *req, bool fastopen,\n\t\t\t   bool *lost_race);\nint tcp_child_process(struct sock *parent, struct sock *child,\n\t\t      struct sk_buff *skb);\nvoid tcp_enter_loss(struct sock *sk);\nvoid tcp_cwnd_reduction(struct sock *sk, int newly_acked_sacked, int newly_lost, int flag);\nvoid tcp_clear_retrans(struct tcp_sock *tp);\nvoid tcp_update_metrics(struct sock *sk);\nvoid tcp_init_metrics(struct sock *sk);\nvoid tcp_metrics_init(void);\nbool tcp_peer_is_proven(struct request_sock *req, struct dst_entry *dst);\nvoid __tcp_close(struct sock *sk, long timeout);\nvoid tcp_close(struct sock *sk, long timeout);\nvoid tcp_init_sock(struct sock *sk);\nvoid tcp_init_transfer(struct sock *sk, int bpf_op, struct sk_buff *skb);\n__poll_t tcp_poll(struct file *file, struct socket *sock,\n\t\t      struct poll_table_struct *wait);\nint do_tcp_getsockopt(struct sock *sk, int level,\n\t\t      int optname, sockptr_t optval, sockptr_t optlen);\nint tcp_getsockopt(struct sock *sk, int level, int optname,\n\t\t   char __user *optval, int __user *optlen);\nbool tcp_bpf_bypass_getsockopt(int level, int optname);\nint do_tcp_setsockopt(struct sock *sk, int level, int optname,\n\t\t      sockptr_t optval, unsigned int optlen);\nint tcp_setsockopt(struct sock *sk, int level, int optname, sockptr_t optval,\n\t\t   unsigned int optlen);\nvoid tcp_set_keepalive(struct sock *sk, int val);\nvoid tcp_syn_ack_timeout(const struct request_sock *req);\nint tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,\n\t\tint flags, int *addr_len);\nint tcp_set_rcvlowat(struct sock *sk, int val);\nint tcp_set_window_clamp(struct sock *sk, int val);\nvoid tcp_update_recv_tstamps(struct sk_buff *skb,\n\t\t\t     struct scm_timestamping_internal *tss);\nvoid tcp_recv_timestamp(struct msghdr *msg, const struct sock *sk,\n\t\t\tstruct scm_timestamping_internal *tss);\nvoid tcp_data_ready(struct sock *sk);\n#ifdef CONFIG_MMU\nint tcp_mmap(struct file *file, struct socket *sock,\n\t     struct vm_area_struct *vma);\n#endif\nvoid tcp_parse_options(const struct net *net, const struct sk_buff *skb,\n\t\t       struct tcp_options_received *opt_rx,\n\t\t       int estab, struct tcp_fastopen_cookie *foc);\nconst u8 *tcp_parse_md5sig_option(const struct tcphdr *th);\n\n \nu16 tcp_v4_get_syncookie(struct sock *sk, struct iphdr *iph,\n\t\t\t struct tcphdr *th, u32 *cookie);\nu16 tcp_v6_get_syncookie(struct sock *sk, struct ipv6hdr *iph,\n\t\t\t struct tcphdr *th, u32 *cookie);\nu16 tcp_parse_mss_option(const struct tcphdr *th, u16 user_mss);\nu16 tcp_get_syncookie_mss(struct request_sock_ops *rsk_ops,\n\t\t\t  const struct tcp_request_sock_ops *af_ops,\n\t\t\t  struct sock *sk, struct tcphdr *th);\n \n\nvoid tcp_v4_send_check(struct sock *sk, struct sk_buff *skb);\nvoid tcp_v4_mtu_reduced(struct sock *sk);\nvoid tcp_req_err(struct sock *sk, u32 seq, bool abort);\nvoid tcp_ld_RTO_revert(struct sock *sk, u32 seq);\nint tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb);\nstruct sock *tcp_create_openreq_child(const struct sock *sk,\n\t\t\t\t      struct request_sock *req,\n\t\t\t\t      struct sk_buff *skb);\nvoid tcp_ca_openreq_child(struct sock *sk, const struct dst_entry *dst);\nstruct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t  struct request_sock *req,\n\t\t\t\t  struct dst_entry *dst,\n\t\t\t\t  struct request_sock *req_unhash,\n\t\t\t\t  bool *own_req);\nint tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb);\nint tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len);\nint tcp_connect(struct sock *sk);\nenum tcp_synack_type {\n\tTCP_SYNACK_NORMAL,\n\tTCP_SYNACK_FASTOPEN,\n\tTCP_SYNACK_COOKIE,\n};\nstruct sk_buff *tcp_make_synack(const struct sock *sk, struct dst_entry *dst,\n\t\t\t\tstruct request_sock *req,\n\t\t\t\tstruct tcp_fastopen_cookie *foc,\n\t\t\t\tenum tcp_synack_type synack_type,\n\t\t\t\tstruct sk_buff *syn_skb);\nint tcp_disconnect(struct sock *sk, int flags);\n\nvoid tcp_finish_connect(struct sock *sk, struct sk_buff *skb);\nint tcp_send_rcvq(struct sock *sk, struct msghdr *msg, size_t size);\nvoid inet_sk_rx_dst_set(struct sock *sk, const struct sk_buff *skb);\n\n \nstruct sock *tcp_get_cookie_sock(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t struct request_sock *req,\n\t\t\t\t struct dst_entry *dst, u32 tsoff);\nint __cookie_v4_check(const struct iphdr *iph, const struct tcphdr *th,\n\t\t      u32 cookie);\nstruct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb);\nstruct request_sock *cookie_tcp_reqsk_alloc(const struct request_sock_ops *ops,\n\t\t\t\t\t    const struct tcp_request_sock_ops *af_ops,\n\t\t\t\t\t    struct sock *sk, struct sk_buff *skb);\n#ifdef CONFIG_SYN_COOKIES\n\n \n#define MAX_SYNCOOKIE_AGE\t2\n#define TCP_SYNCOOKIE_PERIOD\t(60 * HZ)\n#define TCP_SYNCOOKIE_VALID\t(MAX_SYNCOOKIE_AGE * TCP_SYNCOOKIE_PERIOD)\n\n \nstatic inline void tcp_synq_overflow(const struct sock *sk)\n{\n\tunsigned int last_overflow;\n\tunsigned int now = jiffies;\n\n\tif (sk->sk_reuseport) {\n\t\tstruct sock_reuseport *reuse;\n\n\t\treuse = rcu_dereference(sk->sk_reuseport_cb);\n\t\tif (likely(reuse)) {\n\t\t\tlast_overflow = READ_ONCE(reuse->synq_overflow_ts);\n\t\t\tif (!time_between32(now, last_overflow,\n\t\t\t\t\t    last_overflow + HZ))\n\t\t\t\tWRITE_ONCE(reuse->synq_overflow_ts, now);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tlast_overflow = READ_ONCE(tcp_sk(sk)->rx_opt.ts_recent_stamp);\n\tif (!time_between32(now, last_overflow, last_overflow + HZ))\n\t\tWRITE_ONCE(tcp_sk_rw(sk)->rx_opt.ts_recent_stamp, now);\n}\n\n \nstatic inline bool tcp_synq_no_recent_overflow(const struct sock *sk)\n{\n\tunsigned int last_overflow;\n\tunsigned int now = jiffies;\n\n\tif (sk->sk_reuseport) {\n\t\tstruct sock_reuseport *reuse;\n\n\t\treuse = rcu_dereference(sk->sk_reuseport_cb);\n\t\tif (likely(reuse)) {\n\t\t\tlast_overflow = READ_ONCE(reuse->synq_overflow_ts);\n\t\t\treturn !time_between32(now, last_overflow - HZ,\n\t\t\t\t\t       last_overflow +\n\t\t\t\t\t       TCP_SYNCOOKIE_VALID);\n\t\t}\n\t}\n\n\tlast_overflow = READ_ONCE(tcp_sk(sk)->rx_opt.ts_recent_stamp);\n\n\t \n\treturn !time_between32(now, last_overflow - HZ,\n\t\t\t       last_overflow + TCP_SYNCOOKIE_VALID);\n}\n\nstatic inline u32 tcp_cookie_time(void)\n{\n\tu64 val = get_jiffies_64();\n\n\tdo_div(val, TCP_SYNCOOKIE_PERIOD);\n\treturn val;\n}\n\nu32 __cookie_v4_init_sequence(const struct iphdr *iph, const struct tcphdr *th,\n\t\t\t      u16 *mssp);\n__u32 cookie_v4_init_sequence(const struct sk_buff *skb, __u16 *mss);\nu64 cookie_init_timestamp(struct request_sock *req, u64 now);\nbool cookie_timestamp_decode(const struct net *net,\n\t\t\t     struct tcp_options_received *opt);\nbool cookie_ecn_ok(const struct tcp_options_received *opt,\n\t\t   const struct net *net, const struct dst_entry *dst);\n\n \nint __cookie_v6_check(const struct ipv6hdr *iph, const struct tcphdr *th,\n\t\t      u32 cookie);\nstruct sock *cookie_v6_check(struct sock *sk, struct sk_buff *skb);\n\nu32 __cookie_v6_init_sequence(const struct ipv6hdr *iph,\n\t\t\t      const struct tcphdr *th, u16 *mssp);\n__u32 cookie_v6_init_sequence(const struct sk_buff *skb, __u16 *mss);\n#endif\n \n\nvoid tcp_skb_entail(struct sock *sk, struct sk_buff *skb);\nvoid tcp_mark_push(struct tcp_sock *tp, struct sk_buff *skb);\nvoid __tcp_push_pending_frames(struct sock *sk, unsigned int cur_mss,\n\t\t\t       int nonagle);\nint __tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);\nint tcp_retransmit_skb(struct sock *sk, struct sk_buff *skb, int segs);\nvoid tcp_retransmit_timer(struct sock *sk);\nvoid tcp_xmit_retransmit_queue(struct sock *);\nvoid tcp_simple_retransmit(struct sock *);\nvoid tcp_enter_recovery(struct sock *sk, bool ece_ack);\nint tcp_trim_head(struct sock *, struct sk_buff *, u32);\nenum tcp_queue {\n\tTCP_FRAG_IN_WRITE_QUEUE,\n\tTCP_FRAG_IN_RTX_QUEUE,\n};\nint tcp_fragment(struct sock *sk, enum tcp_queue tcp_queue,\n\t\t struct sk_buff *skb, u32 len,\n\t\t unsigned int mss_now, gfp_t gfp);\n\nvoid tcp_send_probe0(struct sock *);\nint tcp_write_wakeup(struct sock *, int mib);\nvoid tcp_send_fin(struct sock *sk);\nvoid tcp_send_active_reset(struct sock *sk, gfp_t priority);\nint tcp_send_synack(struct sock *);\nvoid tcp_push_one(struct sock *, unsigned int mss_now);\nvoid __tcp_send_ack(struct sock *sk, u32 rcv_nxt);\nvoid tcp_send_ack(struct sock *sk);\nvoid tcp_send_delayed_ack(struct sock *sk);\nvoid tcp_send_loss_probe(struct sock *sk);\nbool tcp_schedule_loss_probe(struct sock *sk, bool advancing_rto);\nvoid tcp_skb_collapse_tstamp(struct sk_buff *skb,\n\t\t\t     const struct sk_buff *next_skb);\n\n \nvoid tcp_rearm_rto(struct sock *sk);\nvoid tcp_synack_rtt_meas(struct sock *sk, struct request_sock *req);\nvoid tcp_reset(struct sock *sk, struct sk_buff *skb);\nvoid tcp_fin(struct sock *sk);\nvoid tcp_check_space(struct sock *sk);\nvoid tcp_sack_compress_send_ack(struct sock *sk);\n\n \nvoid tcp_init_xmit_timers(struct sock *);\nstatic inline void tcp_clear_xmit_timers(struct sock *sk)\n{\n\tif (hrtimer_try_to_cancel(&tcp_sk(sk)->pacing_timer) == 1)\n\t\t__sock_put(sk);\n\n\tif (hrtimer_try_to_cancel(&tcp_sk(sk)->compressed_ack_timer) == 1)\n\t\t__sock_put(sk);\n\n\tinet_csk_clear_xmit_timers(sk);\n}\n\nunsigned int tcp_sync_mss(struct sock *sk, u32 pmtu);\nunsigned int tcp_current_mss(struct sock *sk);\nu32 tcp_clamp_probe0_to_user_timeout(const struct sock *sk, u32 when);\n\n \nstatic inline int tcp_bound_to_half_wnd(struct tcp_sock *tp, int pktsize)\n{\n\tint cutoff;\n\n\t \n\tif (tp->max_window > TCP_MSS_DEFAULT)\n\t\tcutoff = (tp->max_window >> 1);\n\telse\n\t\tcutoff = tp->max_window;\n\n\tif (cutoff && pktsize > cutoff)\n\t\treturn max_t(int, cutoff, 68U - tp->tcp_header_len);\n\telse\n\t\treturn pktsize;\n}\n\n \nvoid tcp_get_info(struct sock *, struct tcp_info *);\n\n \nint tcp_read_sock(struct sock *sk, read_descriptor_t *desc,\n\t\t  sk_read_actor_t recv_actor);\nint tcp_read_skb(struct sock *sk, skb_read_actor_t recv_actor);\nstruct sk_buff *tcp_recv_skb(struct sock *sk, u32 seq, u32 *off);\nvoid tcp_read_done(struct sock *sk, size_t len);\n\nvoid tcp_initialize_rcv_mss(struct sock *sk);\n\nint tcp_mtu_to_mss(struct sock *sk, int pmtu);\nint tcp_mss_to_mtu(struct sock *sk, int mss);\nvoid tcp_mtup_init(struct sock *sk);\n\nstatic inline void tcp_bound_rto(const struct sock *sk)\n{\n\tif (inet_csk(sk)->icsk_rto > TCP_RTO_MAX)\n\t\tinet_csk(sk)->icsk_rto = TCP_RTO_MAX;\n}\n\nstatic inline u32 __tcp_set_rto(const struct tcp_sock *tp)\n{\n\treturn usecs_to_jiffies((tp->srtt_us >> 3) + tp->rttvar_us);\n}\n\nstatic inline void __tcp_fast_path_on(struct tcp_sock *tp, u32 snd_wnd)\n{\n\t \n\tif (sk_is_mptcp((struct sock *)tp))\n\t\treturn;\n\n\ttp->pred_flags = htonl((tp->tcp_header_len << 26) |\n\t\t\t       ntohl(TCP_FLAG_ACK) |\n\t\t\t       snd_wnd);\n}\n\nstatic inline void tcp_fast_path_on(struct tcp_sock *tp)\n{\n\t__tcp_fast_path_on(tp, tp->snd_wnd >> tp->rx_opt.snd_wscale);\n}\n\nstatic inline void tcp_fast_path_check(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\tif (RB_EMPTY_ROOT(&tp->out_of_order_queue) &&\n\t    tp->rcv_wnd &&\n\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf &&\n\t    !tp->urg_data)\n\t\ttcp_fast_path_on(tp);\n}\n\nu32 tcp_delack_max(const struct sock *sk);\n\n \nstatic inline u32 tcp_rto_min(struct sock *sk)\n{\n\tconst struct dst_entry *dst = __sk_dst_get(sk);\n\tu32 rto_min = inet_csk(sk)->icsk_rto_min;\n\n\tif (dst && dst_metric_locked(dst, RTAX_RTO_MIN))\n\t\trto_min = dst_metric_rtt(dst, RTAX_RTO_MIN);\n\treturn rto_min;\n}\n\nstatic inline u32 tcp_rto_min_us(struct sock *sk)\n{\n\treturn jiffies_to_usecs(tcp_rto_min(sk));\n}\n\nstatic inline bool tcp_ca_dst_locked(const struct dst_entry *dst)\n{\n\treturn dst_metric_locked(dst, RTAX_CC_ALGO);\n}\n\n \nstatic inline u32 tcp_min_rtt(const struct tcp_sock *tp)\n{\n\treturn minmax_get(&tp->rtt_min);\n}\n\n \nstatic inline u32 tcp_receive_window(const struct tcp_sock *tp)\n{\n\ts32 win = tp->rcv_wup + tp->rcv_wnd - tp->rcv_nxt;\n\n\tif (win < 0)\n\t\twin = 0;\n\treturn (u32) win;\n}\n\n \nu32 __tcp_select_window(struct sock *sk);\n\nvoid tcp_send_window_probe(struct sock *sk);\n\n \n#define tcp_jiffies32 ((u32)jiffies)\n\n \n#define TCP_TS_HZ\t1000\n\nstatic inline u64 tcp_clock_ns(void)\n{\n\treturn ktime_get_ns();\n}\n\nstatic inline u64 tcp_clock_us(void)\n{\n\treturn div_u64(tcp_clock_ns(), NSEC_PER_USEC);\n}\n\n \nstatic inline u32 tcp_time_stamp(const struct tcp_sock *tp)\n{\n\treturn div_u64(tp->tcp_mstamp, USEC_PER_SEC / TCP_TS_HZ);\n}\n\n \nstatic inline u64 tcp_ns_to_ts(u64 ns)\n{\n\treturn div_u64(ns, NSEC_PER_SEC / TCP_TS_HZ);\n}\n\n \nstatic inline u32 tcp_time_stamp_raw(void)\n{\n\treturn tcp_ns_to_ts(tcp_clock_ns());\n}\n\nvoid tcp_mstamp_refresh(struct tcp_sock *tp);\n\nstatic inline u32 tcp_stamp_us_delta(u64 t1, u64 t0)\n{\n\treturn max_t(s64, t1 - t0, 0);\n}\n\nstatic inline u32 tcp_skb_timestamp(const struct sk_buff *skb)\n{\n\treturn tcp_ns_to_ts(skb->skb_mstamp_ns);\n}\n\n \nstatic inline u64 tcp_skb_timestamp_us(const struct sk_buff *skb)\n{\n\treturn div_u64(skb->skb_mstamp_ns, NSEC_PER_USEC);\n}\n\n\n#define tcp_flag_byte(th) (((u_int8_t *)th)[13])\n\n#define TCPHDR_FIN 0x01\n#define TCPHDR_SYN 0x02\n#define TCPHDR_RST 0x04\n#define TCPHDR_PSH 0x08\n#define TCPHDR_ACK 0x10\n#define TCPHDR_URG 0x20\n#define TCPHDR_ECE 0x40\n#define TCPHDR_CWR 0x80\n\n#define TCPHDR_SYN_ECN\t(TCPHDR_SYN | TCPHDR_ECE | TCPHDR_CWR)\n\n \nstruct tcp_skb_cb {\n\t__u32\t\tseq;\t\t \n\t__u32\t\tend_seq;\t \n\tunion {\n\t\t \n\t\t__u32\t\ttcp_tw_isn;\n\t\tstruct {\n\t\t\tu16\ttcp_gso_segs;\n\t\t\tu16\ttcp_gso_size;\n\t\t};\n\t};\n\t__u8\t\ttcp_flags;\t \n\n\t__u8\t\tsacked;\t\t \n#define TCPCB_SACKED_ACKED\t0x01\t \n#define TCPCB_SACKED_RETRANS\t0x02\t \n#define TCPCB_LOST\t\t0x04\t \n#define TCPCB_TAGBITS\t\t0x07\t \n#define TCPCB_REPAIRED\t\t0x10\t \n#define TCPCB_EVER_RETRANS\t0x80\t \n#define TCPCB_RETRANS\t\t(TCPCB_SACKED_RETRANS|TCPCB_EVER_RETRANS| \\\n\t\t\t\tTCPCB_REPAIRED)\n\n\t__u8\t\tip_dsfield;\t \n\t__u8\t\ttxstamp_ack:1,\t \n\t\t\teor:1,\t\t \n\t\t\thas_rxtstamp:1,\t \n\t\t\tunused:5;\n\t__u32\t\tack_seq;\t \n\tunion {\n\t\tstruct {\n#define TCPCB_DELIVERED_CE_MASK ((1U<<20) - 1)\n\t\t\t \n\t\t\t__u32 is_app_limited:1,  \n\t\t\t      delivered_ce:20,\n\t\t\t      unused:11;\n\t\t\t \n\t\t\t__u32 delivered;\n\t\t\t \n\t\t\tu64 first_tx_mstamp;\n\t\t\t \n\t\t\tu64 delivered_mstamp;\n\t\t} tx;    \n\t\tunion {\n\t\t\tstruct inet_skb_parm\th4;\n#if IS_ENABLED(CONFIG_IPV6)\n\t\t\tstruct inet6_skb_parm\th6;\n#endif\n\t\t} header;\t \n\t};\n};\n\n#define TCP_SKB_CB(__skb)\t((struct tcp_skb_cb *)&((__skb)->cb[0]))\n\nextern const struct inet_connection_sock_af_ops ipv4_specific;\n\n#if IS_ENABLED(CONFIG_IPV6)\n \nstatic inline int tcp_v6_iif(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->header.h6.iif;\n}\n\nstatic inline int tcp_v6_iif_l3_slave(const struct sk_buff *skb)\n{\n\tbool l3_slave = ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags);\n\n\treturn l3_slave ? skb->skb_iif : TCP_SKB_CB(skb)->header.h6.iif;\n}\n\n \nstatic inline int tcp_v6_sdif(const struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv6_l3mdev_skb(TCP_SKB_CB(skb)->header.h6.flags))\n\t\treturn TCP_SKB_CB(skb)->header.h6.iif;\n#endif\n\treturn 0;\n}\n\nextern const struct inet_connection_sock_af_ops ipv6_specific;\n\nINDIRECT_CALLABLE_DECLARE(void tcp_v6_send_check(struct sock *sk, struct sk_buff *skb));\nINDIRECT_CALLABLE_DECLARE(int tcp_v6_rcv(struct sk_buff *skb));\nvoid tcp_v6_early_demux(struct sk_buff *skb);\n\n#endif\n\n \nstatic inline int tcp_v4_sdif(struct sk_buff *skb)\n{\n#if IS_ENABLED(CONFIG_NET_L3_MASTER_DEV)\n\tif (skb && ipv4_l3mdev_skb(TCP_SKB_CB(skb)->header.h4.flags))\n\t\treturn TCP_SKB_CB(skb)->header.h4.iif;\n#endif\n\treturn 0;\n}\n\n \nstatic inline int tcp_skb_pcount(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->tcp_gso_segs;\n}\n\nstatic inline void tcp_skb_pcount_set(struct sk_buff *skb, int segs)\n{\n\tTCP_SKB_CB(skb)->tcp_gso_segs = segs;\n}\n\nstatic inline void tcp_skb_pcount_add(struct sk_buff *skb, int segs)\n{\n\tTCP_SKB_CB(skb)->tcp_gso_segs += segs;\n}\n\n \nstatic inline int tcp_skb_mss(const struct sk_buff *skb)\n{\n\treturn TCP_SKB_CB(skb)->tcp_gso_size;\n}\n\nstatic inline bool tcp_skb_can_collapse_to(const struct sk_buff *skb)\n{\n\treturn likely(!TCP_SKB_CB(skb)->eor);\n}\n\nstatic inline bool tcp_skb_can_collapse(const struct sk_buff *to,\n\t\t\t\t\tconst struct sk_buff *from)\n{\n\treturn likely(tcp_skb_can_collapse_to(to) &&\n\t\t      mptcp_skb_can_collapse(to, from) &&\n\t\t      skb_pure_zcopy_same(to, from));\n}\n\n \nenum tcp_ca_event {\n\tCA_EVENT_TX_START,\t \n\tCA_EVENT_CWND_RESTART,\t \n\tCA_EVENT_COMPLETE_CWR,\t \n\tCA_EVENT_LOSS,\t\t \n\tCA_EVENT_ECN_NO_CE,\t \n\tCA_EVENT_ECN_IS_CE,\t \n};\n\n \nenum tcp_ca_ack_event_flags {\n\tCA_ACK_SLOWPATH\t\t= (1 << 0),\t \n\tCA_ACK_WIN_UPDATE\t= (1 << 1),\t \n\tCA_ACK_ECE\t\t= (1 << 2),\t \n};\n\n \n#define TCP_CA_NAME_MAX\t16\n#define TCP_CA_MAX\t128\n#define TCP_CA_BUF_MAX\t(TCP_CA_NAME_MAX*TCP_CA_MAX)\n\n#define TCP_CA_UNSPEC\t0\n\n \n#define TCP_CONG_NON_RESTRICTED 0x1\n \n#define TCP_CONG_NEEDS_ECN\t0x2\n#define TCP_CONG_MASK\t(TCP_CONG_NON_RESTRICTED | TCP_CONG_NEEDS_ECN)\n\nunion tcp_cc_info;\n\nstruct ack_sample {\n\tu32 pkts_acked;\n\ts32 rtt_us;\n\tu32 in_flight;\n};\n\n \nstruct rate_sample {\n\tu64  prior_mstamp;  \n\tu32  prior_delivered;\t \n\tu32  prior_delivered_ce; \n\ts32  delivered;\t\t \n\ts32  delivered_ce;\t \n\tlong interval_us;\t \n\tu32 snd_interval_us;\t \n\tu32 rcv_interval_us;\t \n\tlong rtt_us;\t\t \n\tint  losses;\t\t \n\tu32  acked_sacked;\t \n\tu32  prior_in_flight;\t \n\tu32  last_end_seq;\t \n\tbool is_app_limited;\t \n\tbool is_retrans;\t \n\tbool is_ack_delayed;\t \n};\n\nstruct tcp_congestion_ops {\n \n\n\t \n\tu32 (*ssthresh)(struct sock *sk);\n\n\t \n\tvoid (*cong_avoid)(struct sock *sk, u32 ack, u32 acked);\n\n\t \n\tvoid (*set_state)(struct sock *sk, u8 new_state);\n\n\t \n\tvoid (*cwnd_event)(struct sock *sk, enum tcp_ca_event ev);\n\n\t \n\tvoid (*in_ack_event)(struct sock *sk, u32 flags);\n\n\t \n\tvoid (*pkts_acked)(struct sock *sk, const struct ack_sample *sample);\n\n\t \n\tu32 (*min_tso_segs)(struct sock *sk);\n\n\t \n\tvoid (*cong_control)(struct sock *sk, const struct rate_sample *rs);\n\n\n\t \n\tu32  (*undo_cwnd)(struct sock *sk);\n\t \n\tu32 (*sndbuf_expand)(struct sock *sk);\n\n \n\t \n\tsize_t (*get_info)(struct sock *sk, u32 ext, int *attr,\n\t\t\t   union tcp_cc_info *info);\n\n\tchar \t\t\tname[TCP_CA_NAME_MAX];\n\tstruct module\t\t*owner;\n\tstruct list_head\tlist;\n\tu32\t\t\tkey;\n\tu32\t\t\tflags;\n\n\t \n\tvoid (*init)(struct sock *sk);\n\t \n\tvoid (*release)(struct sock *sk);\n} ____cacheline_aligned_in_smp;\n\nint tcp_register_congestion_control(struct tcp_congestion_ops *type);\nvoid tcp_unregister_congestion_control(struct tcp_congestion_ops *type);\nint tcp_update_congestion_control(struct tcp_congestion_ops *type,\n\t\t\t\t  struct tcp_congestion_ops *old_type);\nint tcp_validate_congestion_control(struct tcp_congestion_ops *ca);\n\nvoid tcp_assign_congestion_control(struct sock *sk);\nvoid tcp_init_congestion_control(struct sock *sk);\nvoid tcp_cleanup_congestion_control(struct sock *sk);\nint tcp_set_default_congestion_control(struct net *net, const char *name);\nvoid tcp_get_default_congestion_control(struct net *net, char *name);\nvoid tcp_get_available_congestion_control(char *buf, size_t len);\nvoid tcp_get_allowed_congestion_control(char *buf, size_t len);\nint tcp_set_allowed_congestion_control(char *allowed);\nint tcp_set_congestion_control(struct sock *sk, const char *name, bool load,\n\t\t\t       bool cap_net_admin);\nu32 tcp_slow_start(struct tcp_sock *tp, u32 acked);\nvoid tcp_cong_avoid_ai(struct tcp_sock *tp, u32 w, u32 acked);\n\nu32 tcp_reno_ssthresh(struct sock *sk);\nu32 tcp_reno_undo_cwnd(struct sock *sk);\nvoid tcp_reno_cong_avoid(struct sock *sk, u32 ack, u32 acked);\nextern struct tcp_congestion_ops tcp_reno;\n\nstruct tcp_congestion_ops *tcp_ca_find(const char *name);\nstruct tcp_congestion_ops *tcp_ca_find_key(u32 key);\nu32 tcp_ca_get_key_by_name(struct net *net, const char *name, bool *ecn_ca);\n#ifdef CONFIG_INET\nchar *tcp_ca_get_name_by_key(u32 key, char *buffer);\n#else\nstatic inline char *tcp_ca_get_name_by_key(u32 key, char *buffer)\n{\n\treturn NULL;\n}\n#endif\n\nstatic inline bool tcp_ca_needs_ecn(const struct sock *sk)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\treturn icsk->icsk_ca_ops->flags & TCP_CONG_NEEDS_ECN;\n}\n\nstatic inline void tcp_ca_event(struct sock *sk, const enum tcp_ca_event event)\n{\n\tconst struct inet_connection_sock *icsk = inet_csk(sk);\n\n\tif (icsk->icsk_ca_ops->cwnd_event)\n\t\ticsk->icsk_ca_ops->cwnd_event(sk, event);\n}\n\n \nvoid tcp_set_ca_state(struct sock *sk, const u8 ca_state);\n\n \nvoid tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb);\nvoid tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,\n\t\t\t    struct rate_sample *rs);\nvoid tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,\n\t\t  bool is_sack_reneg, struct rate_sample *rs);\nvoid tcp_rate_check_app_limited(struct sock *sk);\n\nstatic inline bool tcp_skb_sent_after(u64 t1, u64 t2, u32 seq1, u32 seq2)\n{\n\treturn t1 > t2 || (t1 == t2 && after(seq1, seq2));\n}\n\n \nstatic inline int tcp_is_sack(const struct tcp_sock *tp)\n{\n\treturn likely(tp->rx_opt.sack_ok);\n}\n\nstatic inline bool tcp_is_reno(const struct tcp_sock *tp)\n{\n\treturn !tcp_is_sack(tp);\n}\n\nstatic inline unsigned int tcp_left_out(const struct tcp_sock *tp)\n{\n\treturn tp->sacked_out + tp->lost_out;\n}\n\n \nstatic inline unsigned int tcp_packets_in_flight(const struct tcp_sock *tp)\n{\n\treturn tp->packets_out - tcp_left_out(tp) + tp->retrans_out;\n}\n\n#define TCP_INFINITE_SSTHRESH\t0x7fffffff\n\nstatic inline u32 tcp_snd_cwnd(const struct tcp_sock *tp)\n{\n\treturn tp->snd_cwnd;\n}\n\nstatic inline void tcp_snd_cwnd_set(struct tcp_sock *tp, u32 val)\n{\n\tWARN_ON_ONCE((int)val <= 0);\n\ttp->snd_cwnd = val;\n}\n\nstatic inline bool tcp_in_slow_start(const struct tcp_sock *tp)\n{\n\treturn tcp_snd_cwnd(tp) < tp->snd_ssthresh;\n}\n\nstatic inline bool tcp_in_initial_slowstart(const struct tcp_sock *tp)\n{\n\treturn tp->snd_ssthresh >= TCP_INFINITE_SSTHRESH;\n}\n\nstatic inline bool tcp_in_cwnd_reduction(const struct sock *sk)\n{\n\treturn (TCPF_CA_CWR | TCPF_CA_Recovery) &\n\t       (1 << inet_csk(sk)->icsk_ca_state);\n}\n\n \nstatic inline __u32 tcp_current_ssthresh(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tcp_in_cwnd_reduction(sk))\n\t\treturn tp->snd_ssthresh;\n\telse\n\t\treturn max(tp->snd_ssthresh,\n\t\t\t   ((tcp_snd_cwnd(tp) >> 1) +\n\t\t\t    (tcp_snd_cwnd(tp) >> 2)));\n}\n\n \n#define tcp_verify_left_out(tp)\tWARN_ON(tcp_left_out(tp) > tp->packets_out)\n\nvoid tcp_enter_cwr(struct sock *sk);\n__u32 tcp_init_cwnd(const struct tcp_sock *tp, const struct dst_entry *dst);\n\n \nstatic inline __u32 tcp_max_tso_deferred_mss(const struct tcp_sock *tp)\n{\n\treturn 3;\n}\n\n \nstatic inline u32 tcp_wnd_end(const struct tcp_sock *tp)\n{\n\treturn tp->snd_una + tp->snd_wnd;\n}\n\n \nstatic inline bool tcp_is_cwnd_limited(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\tif (tp->is_cwnd_limited)\n\t\treturn true;\n\n\t \n\tif (tcp_in_slow_start(tp))\n\t\treturn tcp_snd_cwnd(tp) < 2 * tp->max_packets_out;\n\n\treturn false;\n}\n\n \nstatic inline bool tcp_needs_internal_pacing(const struct sock *sk)\n{\n\treturn smp_load_acquire(&sk->sk_pacing_status) == SK_PACING_NEEDED;\n}\n\n \nstatic inline unsigned long tcp_pacing_delay(const struct sock *sk)\n{\n\ts64 delay = tcp_sk(sk)->tcp_wstamp_ns - tcp_sk(sk)->tcp_clock_cache;\n\n\treturn delay > 0 ? nsecs_to_jiffies(delay) : 0;\n}\n\nstatic inline void tcp_reset_xmit_timer(struct sock *sk,\n\t\t\t\t\tconst int what,\n\t\t\t\t\tunsigned long when,\n\t\t\t\t\tconst unsigned long max_when)\n{\n\tinet_csk_reset_xmit_timer(sk, what, when + tcp_pacing_delay(sk),\n\t\t\t\t  max_when);\n}\n\n \nstatic inline unsigned long tcp_probe0_base(const struct sock *sk)\n{\n\treturn max_t(unsigned long, inet_csk(sk)->icsk_rto, TCP_RTO_MIN);\n}\n\n \nstatic inline unsigned long tcp_probe0_when(const struct sock *sk,\n\t\t\t\t\t    unsigned long max_when)\n{\n\tu8 backoff = min_t(u8, ilog2(TCP_RTO_MAX / TCP_RTO_MIN) + 1,\n\t\t\t   inet_csk(sk)->icsk_backoff);\n\tu64 when = (u64)tcp_probe0_base(sk) << backoff;\n\n\treturn (unsigned long)min_t(u64, when, max_when);\n}\n\nstatic inline void tcp_check_probe_timer(struct sock *sk)\n{\n\tif (!tcp_sk(sk)->packets_out && !inet_csk(sk)->icsk_pending)\n\t\ttcp_reset_xmit_timer(sk, ICSK_TIME_PROBE0,\n\t\t\t\t     tcp_probe0_base(sk), TCP_RTO_MAX);\n}\n\nstatic inline void tcp_init_wl(struct tcp_sock *tp, u32 seq)\n{\n\ttp->snd_wl1 = seq;\n}\n\nstatic inline void tcp_update_wl(struct tcp_sock *tp, u32 seq)\n{\n\ttp->snd_wl1 = seq;\n}\n\n \nstatic inline __sum16 tcp_v4_check(int len, __be32 saddr,\n\t\t\t\t   __be32 daddr, __wsum base)\n{\n\treturn csum_tcpudp_magic(saddr, daddr, len, IPPROTO_TCP, base);\n}\n\nstatic inline bool tcp_checksum_complete(struct sk_buff *skb)\n{\n\treturn !skb_csum_unnecessary(skb) &&\n\t\t__skb_checksum_complete(skb);\n}\n\nbool tcp_add_backlog(struct sock *sk, struct sk_buff *skb,\n\t\t     enum skb_drop_reason *reason);\n\n\nint tcp_filter(struct sock *sk, struct sk_buff *skb);\nvoid tcp_set_state(struct sock *sk, int state);\nvoid tcp_done(struct sock *sk);\nint tcp_abort(struct sock *sk, int err);\n\nstatic inline void tcp_sack_reset(struct tcp_options_received *rx_opt)\n{\n\trx_opt->dsack = 0;\n\trx_opt->num_sacks = 0;\n}\n\nvoid tcp_cwnd_restart(struct sock *sk, s32 delta);\n\nstatic inline void tcp_slow_start_after_idle_check(struct sock *sk)\n{\n\tconst struct tcp_congestion_ops *ca_ops = inet_csk(sk)->icsk_ca_ops;\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\ts32 delta;\n\n\tif (!READ_ONCE(sock_net(sk)->ipv4.sysctl_tcp_slow_start_after_idle) ||\n\t    tp->packets_out || ca_ops->cong_control)\n\t\treturn;\n\tdelta = tcp_jiffies32 - tp->lsndtime;\n\tif (delta > inet_csk(sk)->icsk_rto)\n\t\ttcp_cwnd_restart(sk, delta);\n}\n\n \nvoid tcp_select_initial_window(const struct sock *sk, int __space,\n\t\t\t       __u32 mss, __u32 *rcv_wnd,\n\t\t\t       __u32 *window_clamp, int wscale_ok,\n\t\t\t       __u8 *rcv_wscale, __u32 init_rcv_wnd);\n\nstatic inline int __tcp_win_from_space(u8 scaling_ratio, int space)\n{\n\ts64 scaled_space = (s64)space * scaling_ratio;\n\n\treturn scaled_space >> TCP_RMEM_TO_WIN_SCALE;\n}\n\nstatic inline int tcp_win_from_space(const struct sock *sk, int space)\n{\n\treturn __tcp_win_from_space(tcp_sk(sk)->scaling_ratio, space);\n}\n\n \nstatic inline int __tcp_space_from_win(u8 scaling_ratio, int win)\n{\n\tu64 val = (u64)win << TCP_RMEM_TO_WIN_SCALE;\n\n\tdo_div(val, scaling_ratio);\n\treturn val;\n}\n\nstatic inline int tcp_space_from_win(const struct sock *sk, int win)\n{\n\treturn __tcp_space_from_win(tcp_sk(sk)->scaling_ratio, win);\n}\n\nstatic inline void tcp_scaling_ratio_init(struct sock *sk)\n{\n\t \n\ttcp_sk(sk)->scaling_ratio = (1200 << TCP_RMEM_TO_WIN_SCALE) /\n\t\t\t\t    SKB_TRUESIZE(4096);\n}\n\n \nstatic inline int tcp_space(const struct sock *sk)\n{\n\treturn tcp_win_from_space(sk, READ_ONCE(sk->sk_rcvbuf) -\n\t\t\t\t  READ_ONCE(sk->sk_backlog.len) -\n\t\t\t\t  atomic_read(&sk->sk_rmem_alloc));\n}\n\nstatic inline int tcp_full_space(const struct sock *sk)\n{\n\treturn tcp_win_from_space(sk, READ_ONCE(sk->sk_rcvbuf));\n}\n\nstatic inline void __tcp_adjust_rcv_ssthresh(struct sock *sk, u32 new_ssthresh)\n{\n\tint unused_mem = sk_unused_reserved_mem(sk);\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\ttp->rcv_ssthresh = min(tp->rcv_ssthresh, new_ssthresh);\n\tif (unused_mem)\n\t\ttp->rcv_ssthresh = max_t(u32, tp->rcv_ssthresh,\n\t\t\t\t\t tcp_win_from_space(sk, unused_mem));\n}\n\nstatic inline void tcp_adjust_rcv_ssthresh(struct sock *sk)\n{\n\t__tcp_adjust_rcv_ssthresh(sk, 4U * tcp_sk(sk)->advmss);\n}\n\nvoid tcp_cleanup_rbuf(struct sock *sk, int copied);\nvoid __tcp_cleanup_rbuf(struct sock *sk, int copied);\n\n\n \nstatic inline bool tcp_rmem_pressure(const struct sock *sk)\n{\n\tint rcvbuf, threshold;\n\n\tif (tcp_under_memory_pressure(sk))\n\t\treturn true;\n\n\trcvbuf = READ_ONCE(sk->sk_rcvbuf);\n\tthreshold = rcvbuf - (rcvbuf >> 3);\n\n\treturn atomic_read(&sk->sk_rmem_alloc) > threshold;\n}\n\nstatic inline bool tcp_epollin_ready(const struct sock *sk, int target)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\tint avail = READ_ONCE(tp->rcv_nxt) - READ_ONCE(tp->copied_seq);\n\n\tif (avail <= 0)\n\t\treturn false;\n\n\treturn (avail >= target) || tcp_rmem_pressure(sk) ||\n\t       (tcp_receive_window(tp) <= inet_csk(sk)->icsk_ack.rcv_mss);\n}\n\nextern void tcp_openreq_init_rwin(struct request_sock *req,\n\t\t\t\t  const struct sock *sk_listener,\n\t\t\t\t  const struct dst_entry *dst);\n\nvoid tcp_enter_memory_pressure(struct sock *sk);\nvoid tcp_leave_memory_pressure(struct sock *sk);\n\nstatic inline int keepalive_intvl_when(const struct tcp_sock *tp)\n{\n\tstruct net *net = sock_net((struct sock *)tp);\n\tint val;\n\n\t \n\tval = READ_ONCE(tp->keepalive_intvl);\n\n\treturn val ? : READ_ONCE(net->ipv4.sysctl_tcp_keepalive_intvl);\n}\n\nstatic inline int keepalive_time_when(const struct tcp_sock *tp)\n{\n\tstruct net *net = sock_net((struct sock *)tp);\n\tint val;\n\n\t \n\tval = READ_ONCE(tp->keepalive_time);\n\n\treturn val ? : READ_ONCE(net->ipv4.sysctl_tcp_keepalive_time);\n}\n\nstatic inline int keepalive_probes(const struct tcp_sock *tp)\n{\n\tstruct net *net = sock_net((struct sock *)tp);\n\tint val;\n\n\t \n\tval = READ_ONCE(tp->keepalive_probes);\n\n\treturn val ? : READ_ONCE(net->ipv4.sysctl_tcp_keepalive_probes);\n}\n\nstatic inline u32 keepalive_time_elapsed(const struct tcp_sock *tp)\n{\n\tconst struct inet_connection_sock *icsk = &tp->inet_conn;\n\n\treturn min_t(u32, tcp_jiffies32 - icsk->icsk_ack.lrcvtime,\n\t\t\t  tcp_jiffies32 - tp->rcv_tstamp);\n}\n\nstatic inline int tcp_fin_time(const struct sock *sk)\n{\n\tint fin_timeout = tcp_sk(sk)->linger2 ? :\n\t\tREAD_ONCE(sock_net(sk)->ipv4.sysctl_tcp_fin_timeout);\n\tconst int rto = inet_csk(sk)->icsk_rto;\n\n\tif (fin_timeout < (rto << 2) - (rto >> 1))\n\t\tfin_timeout = (rto << 2) - (rto >> 1);\n\n\treturn fin_timeout;\n}\n\nstatic inline bool tcp_paws_check(const struct tcp_options_received *rx_opt,\n\t\t\t\t  int paws_win)\n{\n\tif ((s32)(rx_opt->ts_recent - rx_opt->rcv_tsval) <= paws_win)\n\t\treturn true;\n\tif (unlikely(!time_before32(ktime_get_seconds(),\n\t\t\t\t    rx_opt->ts_recent_stamp + TCP_PAWS_24DAYS)))\n\t\treturn true;\n\t \n\tif (!rx_opt->ts_recent)\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline bool tcp_paws_reject(const struct tcp_options_received *rx_opt,\n\t\t\t\t   int rst)\n{\n\tif (tcp_paws_check(rx_opt, 0))\n\t\treturn false;\n\n\t \n\tif (rst && !time_before32(ktime_get_seconds(),\n\t\t\t\t  rx_opt->ts_recent_stamp + TCP_PAWS_MSL))\n\t\treturn false;\n\treturn true;\n}\n\nbool tcp_oow_rate_limited(struct net *net, const struct sk_buff *skb,\n\t\t\t  int mib_idx, u32 *last_oow_ack_time);\n\nstatic inline void tcp_mib_init(struct net *net)\n{\n\t \n\tTCP_ADD_STATS(net, TCP_MIB_RTOALGORITHM, 1);\n\tTCP_ADD_STATS(net, TCP_MIB_RTOMIN, TCP_RTO_MIN*1000/HZ);\n\tTCP_ADD_STATS(net, TCP_MIB_RTOMAX, TCP_RTO_MAX*1000/HZ);\n\tTCP_ADD_STATS(net, TCP_MIB_MAXCONN, -1);\n}\n\n \nstatic inline void tcp_clear_retrans_hints_partial(struct tcp_sock *tp)\n{\n\ttp->lost_skb_hint = NULL;\n}\n\nstatic inline void tcp_clear_all_retrans_hints(struct tcp_sock *tp)\n{\n\ttcp_clear_retrans_hints_partial(tp);\n\ttp->retransmit_skb_hint = NULL;\n}\n\nunion tcp_md5_addr {\n\tstruct in_addr  a4;\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct in6_addr\ta6;\n#endif\n};\n\n \nstruct tcp_md5sig_key {\n\tstruct hlist_node\tnode;\n\tu8\t\t\tkeylen;\n\tu8\t\t\tfamily;  \n\tu8\t\t\tprefixlen;\n\tu8\t\t\tflags;\n\tunion tcp_md5_addr\taddr;\n\tint\t\t\tl3index;  \n\tu8\t\t\tkey[TCP_MD5SIG_MAXKEYLEN];\n\tstruct rcu_head\t\trcu;\n};\n\n \nstruct tcp_md5sig_info {\n\tstruct hlist_head\thead;\n\tstruct rcu_head\t\trcu;\n};\n\n \nstruct tcp4_pseudohdr {\n\t__be32\t\tsaddr;\n\t__be32\t\tdaddr;\n\t__u8\t\tpad;\n\t__u8\t\tprotocol;\n\t__be16\t\tlen;\n};\n\nstruct tcp6_pseudohdr {\n\tstruct in6_addr\tsaddr;\n\tstruct in6_addr daddr;\n\t__be32\t\tlen;\n\t__be32\t\tprotocol;\t \n};\n\nunion tcp_md5sum_block {\n\tstruct tcp4_pseudohdr ip4;\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct tcp6_pseudohdr ip6;\n#endif\n};\n\n \nstruct tcp_md5sig_pool {\n\tstruct ahash_request\t*md5_req;\n\tvoid\t\t\t*scratch;\n};\n\n \nint tcp_v4_md5_hash_skb(char *md5_hash, const struct tcp_md5sig_key *key,\n\t\t\tconst struct sock *sk, const struct sk_buff *skb);\nint tcp_md5_do_add(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t   int family, u8 prefixlen, int l3index, u8 flags,\n\t\t   const u8 *newkey, u8 newkeylen);\nint tcp_md5_key_copy(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t     int family, u8 prefixlen, int l3index,\n\t\t     struct tcp_md5sig_key *key);\n\nint tcp_md5_do_del(struct sock *sk, const union tcp_md5_addr *addr,\n\t\t   int family, u8 prefixlen, int l3index, u8 flags);\nstruct tcp_md5sig_key *tcp_v4_md5_lookup(const struct sock *sk,\n\t\t\t\t\t const struct sock *addr_sk);\n\n#ifdef CONFIG_TCP_MD5SIG\n#include <linux/jump_label.h>\nextern struct static_key_false_deferred tcp_md5_needed;\nstruct tcp_md5sig_key *__tcp_md5_do_lookup(const struct sock *sk, int l3index,\n\t\t\t\t\t   const union tcp_md5_addr *addr,\n\t\t\t\t\t   int family);\nstatic inline struct tcp_md5sig_key *\ntcp_md5_do_lookup(const struct sock *sk, int l3index,\n\t\t  const union tcp_md5_addr *addr, int family)\n{\n\tif (!static_branch_unlikely(&tcp_md5_needed.key))\n\t\treturn NULL;\n\treturn __tcp_md5_do_lookup(sk, l3index, addr, family);\n}\n\nenum skb_drop_reason\ntcp_inbound_md5_hash(const struct sock *sk, const struct sk_buff *skb,\n\t\t     const void *saddr, const void *daddr,\n\t\t     int family, int dif, int sdif);\n\n\n#define tcp_twsk_md5_key(twsk)\t((twsk)->tw_md5_key)\n#else\nstatic inline struct tcp_md5sig_key *\ntcp_md5_do_lookup(const struct sock *sk, int l3index,\n\t\t  const union tcp_md5_addr *addr, int family)\n{\n\treturn NULL;\n}\n\nstatic inline enum skb_drop_reason\ntcp_inbound_md5_hash(const struct sock *sk, const struct sk_buff *skb,\n\t\t     const void *saddr, const void *daddr,\n\t\t     int family, int dif, int sdif)\n{\n\treturn SKB_NOT_DROPPED_YET;\n}\n#define tcp_twsk_md5_key(twsk)\tNULL\n#endif\n\nbool tcp_alloc_md5sig_pool(void);\n\nstruct tcp_md5sig_pool *tcp_get_md5sig_pool(void);\nstatic inline void tcp_put_md5sig_pool(void)\n{\n\tlocal_bh_enable();\n}\n\nint tcp_md5_hash_skb_data(struct tcp_md5sig_pool *, const struct sk_buff *,\n\t\t\t  unsigned int header_len);\nint tcp_md5_hash_key(struct tcp_md5sig_pool *hp,\n\t\t     const struct tcp_md5sig_key *key);\n\n \nvoid tcp_fastopen_cache_get(struct sock *sk, u16 *mss,\n\t\t\t    struct tcp_fastopen_cookie *cookie);\nvoid tcp_fastopen_cache_set(struct sock *sk, u16 mss,\n\t\t\t    struct tcp_fastopen_cookie *cookie, bool syn_lost,\n\t\t\t    u16 try_exp);\nstruct tcp_fastopen_request {\n\t \n\tstruct tcp_fastopen_cookie\tcookie;\n\tstruct msghdr\t\t\t*data;   \n\tsize_t\t\t\t\tsize;\n\tint\t\t\t\tcopied;\t \n\tstruct ubuf_info\t\t*uarg;\n};\nvoid tcp_free_fastopen_req(struct tcp_sock *tp);\nvoid tcp_fastopen_destroy_cipher(struct sock *sk);\nvoid tcp_fastopen_ctx_destroy(struct net *net);\nint tcp_fastopen_reset_cipher(struct net *net, struct sock *sk,\n\t\t\t      void *primary_key, void *backup_key);\nint tcp_fastopen_get_cipher(struct net *net, struct inet_connection_sock *icsk,\n\t\t\t    u64 *key);\nvoid tcp_fastopen_add_skb(struct sock *sk, struct sk_buff *skb);\nstruct sock *tcp_try_fastopen(struct sock *sk, struct sk_buff *skb,\n\t\t\t      struct request_sock *req,\n\t\t\t      struct tcp_fastopen_cookie *foc,\n\t\t\t      const struct dst_entry *dst);\nvoid tcp_fastopen_init_key_once(struct net *net);\nbool tcp_fastopen_cookie_check(struct sock *sk, u16 *mss,\n\t\t\t     struct tcp_fastopen_cookie *cookie);\nbool tcp_fastopen_defer_connect(struct sock *sk, int *err);\n#define TCP_FASTOPEN_KEY_LENGTH sizeof(siphash_key_t)\n#define TCP_FASTOPEN_KEY_MAX 2\n#define TCP_FASTOPEN_KEY_BUF_LENGTH \\\n\t(TCP_FASTOPEN_KEY_LENGTH * TCP_FASTOPEN_KEY_MAX)\n\n \nstruct tcp_fastopen_context {\n\tsiphash_key_t\tkey[TCP_FASTOPEN_KEY_MAX];\n\tint\t\tnum;\n\tstruct rcu_head\trcu;\n};\n\nvoid tcp_fastopen_active_disable(struct sock *sk);\nbool tcp_fastopen_active_should_disable(struct sock *sk);\nvoid tcp_fastopen_active_disable_ofo_check(struct sock *sk);\nvoid tcp_fastopen_active_detect_blackhole(struct sock *sk, bool expired);\n\n \nstatic inline\nstruct tcp_fastopen_context *tcp_fastopen_get_ctx(const struct sock *sk)\n{\n\tstruct tcp_fastopen_context *ctx;\n\n\tctx = rcu_dereference(inet_csk(sk)->icsk_accept_queue.fastopenq.ctx);\n\tif (!ctx)\n\t\tctx = rcu_dereference(sock_net(sk)->ipv4.tcp_fastopen_ctx);\n\treturn ctx;\n}\n\nstatic inline\nbool tcp_fastopen_cookie_match(const struct tcp_fastopen_cookie *foc,\n\t\t\t       const struct tcp_fastopen_cookie *orig)\n{\n\tif (orig->len == TCP_FASTOPEN_COOKIE_SIZE &&\n\t    orig->len == foc->len &&\n\t    !memcmp(orig->val, foc->val, foc->len))\n\t\treturn true;\n\treturn false;\n}\n\nstatic inline\nint tcp_fastopen_context_len(const struct tcp_fastopen_context *ctx)\n{\n\treturn ctx->num;\n}\n\n \nenum tcp_chrono {\n\tTCP_CHRONO_UNSPEC,\n\tTCP_CHRONO_BUSY,  \n\tTCP_CHRONO_RWND_LIMITED,  \n\tTCP_CHRONO_SNDBUF_LIMITED,  \n\t__TCP_CHRONO_MAX,\n};\n\nvoid tcp_chrono_start(struct sock *sk, const enum tcp_chrono type);\nvoid tcp_chrono_stop(struct sock *sk, const enum tcp_chrono type);\n\n \nstatic inline void tcp_skb_tsorted_anchor_cleanup(struct sk_buff *skb)\n{\n\tskb->destructor = NULL;\n\tskb->_skb_refdst = 0UL;\n}\n\n#define tcp_skb_tsorted_save(skb) {\t\t\\\n\tunsigned long _save = skb->_skb_refdst;\t\\\n\tskb->_skb_refdst = 0UL;\n\n#define tcp_skb_tsorted_restore(skb)\t\t\\\n\tskb->_skb_refdst = _save;\t\t\\\n}\n\nvoid tcp_write_queue_purge(struct sock *sk);\n\nstatic inline struct sk_buff *tcp_rtx_queue_head(const struct sock *sk)\n{\n\treturn skb_rb_first(&sk->tcp_rtx_queue);\n}\n\nstatic inline struct sk_buff *tcp_rtx_queue_tail(const struct sock *sk)\n{\n\treturn skb_rb_last(&sk->tcp_rtx_queue);\n}\n\nstatic inline struct sk_buff *tcp_write_queue_tail(const struct sock *sk)\n{\n\treturn skb_peek_tail(&sk->sk_write_queue);\n}\n\n#define tcp_for_write_queue_from_safe(skb, tmp, sk)\t\t\t\\\n\tskb_queue_walk_from_safe(&(sk)->sk_write_queue, skb, tmp)\n\nstatic inline struct sk_buff *tcp_send_head(const struct sock *sk)\n{\n\treturn skb_peek(&sk->sk_write_queue);\n}\n\nstatic inline bool tcp_skb_is_last(const struct sock *sk,\n\t\t\t\t   const struct sk_buff *skb)\n{\n\treturn skb_queue_is_last(&sk->sk_write_queue, skb);\n}\n\n \nstatic inline bool tcp_write_queue_empty(const struct sock *sk)\n{\n\tconst struct tcp_sock *tp = tcp_sk(sk);\n\n\treturn tp->write_seq == tp->snd_nxt;\n}\n\nstatic inline bool tcp_rtx_queue_empty(const struct sock *sk)\n{\n\treturn RB_EMPTY_ROOT(&sk->tcp_rtx_queue);\n}\n\nstatic inline bool tcp_rtx_and_write_queues_empty(const struct sock *sk)\n{\n\treturn tcp_rtx_queue_empty(sk) && tcp_write_queue_empty(sk);\n}\n\nstatic inline void tcp_add_write_queue_tail(struct sock *sk, struct sk_buff *skb)\n{\n\t__skb_queue_tail(&sk->sk_write_queue, skb);\n\n\t \n\tif (sk->sk_write_queue.next == skb)\n\t\ttcp_chrono_start(sk, TCP_CHRONO_BUSY);\n}\n\n \nstatic inline void tcp_insert_write_queue_before(struct sk_buff *new,\n\t\t\t\t\t\t  struct sk_buff *skb,\n\t\t\t\t\t\t  struct sock *sk)\n{\n\t__skb_queue_before(&sk->sk_write_queue, skb, new);\n}\n\nstatic inline void tcp_unlink_write_queue(struct sk_buff *skb, struct sock *sk)\n{\n\ttcp_skb_tsorted_anchor_cleanup(skb);\n\t__skb_unlink(skb, &sk->sk_write_queue);\n}\n\nvoid tcp_rbtree_insert(struct rb_root *root, struct sk_buff *skb);\n\nstatic inline void tcp_rtx_queue_unlink(struct sk_buff *skb, struct sock *sk)\n{\n\ttcp_skb_tsorted_anchor_cleanup(skb);\n\trb_erase(&skb->rbnode, &sk->tcp_rtx_queue);\n}\n\nstatic inline void tcp_rtx_queue_unlink_and_free(struct sk_buff *skb, struct sock *sk)\n{\n\tlist_del(&skb->tcp_tsorted_anchor);\n\ttcp_rtx_queue_unlink(skb, sk);\n\ttcp_wmem_free_skb(sk, skb);\n}\n\nstatic inline void tcp_push_pending_frames(struct sock *sk)\n{\n\tif (tcp_send_head(sk)) {\n\t\tstruct tcp_sock *tp = tcp_sk(sk);\n\n\t\t__tcp_push_pending_frames(sk, tcp_current_mss(sk), tp->nonagle);\n\t}\n}\n\n \nstatic inline u32 tcp_highest_sack_seq(struct tcp_sock *tp)\n{\n\tif (!tp->sacked_out)\n\t\treturn tp->snd_una;\n\n\tif (tp->highest_sack == NULL)\n\t\treturn tp->snd_nxt;\n\n\treturn TCP_SKB_CB(tp->highest_sack)->seq;\n}\n\nstatic inline void tcp_advance_highest_sack(struct sock *sk, struct sk_buff *skb)\n{\n\ttcp_sk(sk)->highest_sack = skb_rb_next(skb);\n}\n\nstatic inline struct sk_buff *tcp_highest_sack(struct sock *sk)\n{\n\treturn tcp_sk(sk)->highest_sack;\n}\n\nstatic inline void tcp_highest_sack_reset(struct sock *sk)\n{\n\ttcp_sk(sk)->highest_sack = tcp_rtx_queue_head(sk);\n}\n\n \nstatic inline void tcp_highest_sack_replace(struct sock *sk,\n\t\t\t\t\t    struct sk_buff *old,\n\t\t\t\t\t    struct sk_buff *new)\n{\n\tif (old == tcp_highest_sack(sk))\n\t\ttcp_sk(sk)->highest_sack = new;\n}\n\n \nstatic inline bool inet_sk_transparent(const struct sock *sk)\n{\n\tswitch (sk->sk_state) {\n\tcase TCP_TIME_WAIT:\n\t\treturn inet_twsk(sk)->tw_transparent;\n\tcase TCP_NEW_SYN_RECV:\n\t\treturn inet_rsk(inet_reqsk(sk))->no_srccheck;\n\t}\n\treturn inet_test_bit(TRANSPARENT, sk);\n}\n\n \nstatic inline bool tcp_stream_is_thin(struct tcp_sock *tp)\n{\n\treturn tp->packets_out < 4 && !tcp_in_initial_slowstart(tp);\n}\n\n \nenum tcp_seq_states {\n\tTCP_SEQ_STATE_LISTENING,\n\tTCP_SEQ_STATE_ESTABLISHED,\n};\n\nvoid *tcp_seq_start(struct seq_file *seq, loff_t *pos);\nvoid *tcp_seq_next(struct seq_file *seq, void *v, loff_t *pos);\nvoid tcp_seq_stop(struct seq_file *seq, void *v);\n\nstruct tcp_seq_afinfo {\n\tsa_family_t\t\t\tfamily;\n};\n\nstruct tcp_iter_state {\n\tstruct seq_net_private\tp;\n\tenum tcp_seq_states\tstate;\n\tstruct sock\t\t*syn_wait_sk;\n\tint\t\t\tbucket, offset, sbucket, num;\n\tloff_t\t\t\tlast_pos;\n};\n\nextern struct request_sock_ops tcp_request_sock_ops;\nextern struct request_sock_ops tcp6_request_sock_ops;\n\nvoid tcp_v4_destroy_sock(struct sock *sk);\n\nstruct sk_buff *tcp_gso_segment(struct sk_buff *skb,\n\t\t\t\tnetdev_features_t features);\nstruct sk_buff *tcp_gro_receive(struct list_head *head, struct sk_buff *skb);\nINDIRECT_CALLABLE_DECLARE(int tcp4_gro_complete(struct sk_buff *skb, int thoff));\nINDIRECT_CALLABLE_DECLARE(struct sk_buff *tcp4_gro_receive(struct list_head *head, struct sk_buff *skb));\nINDIRECT_CALLABLE_DECLARE(int tcp6_gro_complete(struct sk_buff *skb, int thoff));\nINDIRECT_CALLABLE_DECLARE(struct sk_buff *tcp6_gro_receive(struct list_head *head, struct sk_buff *skb));\nvoid tcp_gro_complete(struct sk_buff *skb);\n\nvoid __tcp_v4_send_check(struct sk_buff *skb, __be32 saddr, __be32 daddr);\n\nstatic inline u32 tcp_notsent_lowat(const struct tcp_sock *tp)\n{\n\tstruct net *net = sock_net((struct sock *)tp);\n\tu32 val;\n\n\tval = READ_ONCE(tp->notsent_lowat);\n\n\treturn val ?: READ_ONCE(net->ipv4.sysctl_tcp_notsent_lowat);\n}\n\nbool tcp_stream_memory_free(const struct sock *sk, int wake);\n\n#ifdef CONFIG_PROC_FS\nint tcp4_proc_init(void);\nvoid tcp4_proc_exit(void);\n#endif\n\nint tcp_rtx_synack(const struct sock *sk, struct request_sock *req);\nint tcp_conn_request(struct request_sock_ops *rsk_ops,\n\t\t     const struct tcp_request_sock_ops *af_ops,\n\t\t     struct sock *sk, struct sk_buff *skb);\n\n \nstruct tcp_sock_af_ops {\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key\t*(*md5_lookup) (const struct sock *sk,\n\t\t\t\t\t\tconst struct sock *addr_sk);\n\tint\t\t(*calc_md5_hash)(char *location,\n\t\t\t\t\t const struct tcp_md5sig_key *md5,\n\t\t\t\t\t const struct sock *sk,\n\t\t\t\t\t const struct sk_buff *skb);\n\tint\t\t(*md5_parse)(struct sock *sk,\n\t\t\t\t     int optname,\n\t\t\t\t     sockptr_t optval,\n\t\t\t\t     int optlen);\n#endif\n};\n\nstruct tcp_request_sock_ops {\n\tu16 mss_clamp;\n#ifdef CONFIG_TCP_MD5SIG\n\tstruct tcp_md5sig_key *(*req_md5_lookup)(const struct sock *sk,\n\t\t\t\t\t\t const struct sock *addr_sk);\n\tint\t\t(*calc_md5_hash) (char *location,\n\t\t\t\t\t  const struct tcp_md5sig_key *md5,\n\t\t\t\t\t  const struct sock *sk,\n\t\t\t\t\t  const struct sk_buff *skb);\n#endif\n#ifdef CONFIG_SYN_COOKIES\n\t__u32 (*cookie_init_seq)(const struct sk_buff *skb,\n\t\t\t\t __u16 *mss);\n#endif\n\tstruct dst_entry *(*route_req)(const struct sock *sk,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct flowi *fl,\n\t\t\t\t       struct request_sock *req);\n\tu32 (*init_seq)(const struct sk_buff *skb);\n\tu32 (*init_ts_off)(const struct net *net, const struct sk_buff *skb);\n\tint (*send_synack)(const struct sock *sk, struct dst_entry *dst,\n\t\t\t   struct flowi *fl, struct request_sock *req,\n\t\t\t   struct tcp_fastopen_cookie *foc,\n\t\t\t   enum tcp_synack_type synack_type,\n\t\t\t   struct sk_buff *syn_skb);\n};\n\nextern const struct tcp_request_sock_ops tcp_request_sock_ipv4_ops;\n#if IS_ENABLED(CONFIG_IPV6)\nextern const struct tcp_request_sock_ops tcp_request_sock_ipv6_ops;\n#endif\n\n#ifdef CONFIG_SYN_COOKIES\nstatic inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,\n\t\t\t\t\t const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t __u16 *mss)\n{\n\ttcp_synq_overflow(sk);\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_SYNCOOKIESSENT);\n\treturn ops->cookie_init_seq(skb, mss);\n}\n#else\nstatic inline __u32 cookie_init_sequence(const struct tcp_request_sock_ops *ops,\n\t\t\t\t\t const struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t __u16 *mss)\n{\n\treturn 0;\n}\n#endif\n\nint tcpv4_offload_init(void);\n\nvoid tcp_v4_init(void);\nvoid tcp_init(void);\n\n \nvoid tcp_mark_skb_lost(struct sock *sk, struct sk_buff *skb);\nvoid tcp_newreno_mark_lost(struct sock *sk, bool snd_una_advanced);\nextern s32 tcp_rack_skb_timeout(struct tcp_sock *tp, struct sk_buff *skb,\n\t\t\t\tu32 reo_wnd);\nextern bool tcp_rack_mark_lost(struct sock *sk);\nextern void tcp_rack_advance(struct tcp_sock *tp, u8 sacked, u32 end_seq,\n\t\t\t     u64 xmit_time);\nextern void tcp_rack_reo_timeout(struct sock *sk);\nextern void tcp_rack_update_reo_wnd(struct sock *sk, struct rate_sample *rs);\n\n \n\n \n#define TCP_PLB_SCALE 8\n\n \nstruct tcp_plb_state {\n\tu8\tconsec_cong_rounds:5,  \n\t\tunused:3;\n\tu32\tpause_until;  \n};\n\nstatic inline void tcp_plb_init(const struct sock *sk,\n\t\t\t\tstruct tcp_plb_state *plb)\n{\n\tplb->consec_cong_rounds = 0;\n\tplb->pause_until = 0;\n}\nvoid tcp_plb_update_state(const struct sock *sk, struct tcp_plb_state *plb,\n\t\t\t  const int cong_ratio);\nvoid tcp_plb_check_rehash(struct sock *sk, struct tcp_plb_state *plb);\nvoid tcp_plb_update_state_upon_rto(struct sock *sk, struct tcp_plb_state *plb);\n\n \nstatic inline s64 tcp_rto_delta_us(const struct sock *sk)\n{\n\tconst struct sk_buff *skb = tcp_rtx_queue_head(sk);\n\tu32 rto = inet_csk(sk)->icsk_rto;\n\tu64 rto_time_stamp_us = tcp_skb_timestamp_us(skb) + jiffies_to_usecs(rto);\n\n\treturn rto_time_stamp_us - tcp_sk(sk)->tcp_mstamp;\n}\n\n \nstatic inline struct ip_options_rcu *tcp_v4_save_options(struct net *net,\n\t\t\t\t\t\t\t struct sk_buff *skb)\n{\n\tconst struct ip_options *opt = &TCP_SKB_CB(skb)->header.h4.opt;\n\tstruct ip_options_rcu *dopt = NULL;\n\n\tif (opt->optlen) {\n\t\tint opt_size = sizeof(*dopt) + opt->optlen;\n\n\t\tdopt = kmalloc(opt_size, GFP_ATOMIC);\n\t\tif (dopt && __ip_options_echo(net, &dopt->opt, skb, opt)) {\n\t\t\tkfree(dopt);\n\t\t\tdopt = NULL;\n\t\t}\n\t}\n\treturn dopt;\n}\n\n \nstatic inline bool skb_is_tcp_pure_ack(const struct sk_buff *skb)\n{\n\treturn skb->truesize == 2;\n}\n\nstatic inline void skb_set_tcp_pure_ack(struct sk_buff *skb)\n{\n\tskb->truesize = 2;\n}\n\nstatic inline int tcp_inq(struct sock *sk)\n{\n\tstruct tcp_sock *tp = tcp_sk(sk);\n\tint answ;\n\n\tif ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {\n\t\tansw = 0;\n\t} else if (sock_flag(sk, SOCK_URGINLINE) ||\n\t\t   !tp->urg_data ||\n\t\t   before(tp->urg_seq, tp->copied_seq) ||\n\t\t   !before(tp->urg_seq, tp->rcv_nxt)) {\n\n\t\tansw = tp->rcv_nxt - tp->copied_seq;\n\n\t\t \n\t\tif (answ && sock_flag(sk, SOCK_DONE))\n\t\t\tansw--;\n\t} else {\n\t\tansw = tp->urg_seq - tp->copied_seq;\n\t}\n\n\treturn answ;\n}\n\nint tcp_peek_len(struct socket *sock);\n\nstatic inline void tcp_segs_in(struct tcp_sock *tp, const struct sk_buff *skb)\n{\n\tu16 segs_in;\n\n\tsegs_in = max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\n\t \n\tWRITE_ONCE(tp->segs_in, tp->segs_in + segs_in);\n\tif (skb->len > tcp_hdrlen(skb))\n\t\tWRITE_ONCE(tp->data_segs_in, tp->data_segs_in + segs_in);\n}\n\n \nstatic inline void tcp_listendrop(const struct sock *sk)\n{\n\tatomic_inc(&((struct sock *)sk)->sk_drops);\n\t__NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENDROPS);\n}\n\nenum hrtimer_restart tcp_pace_kick(struct hrtimer *timer);\n\n \n\n#define TCP_ULP_NAME_MAX\t16\n#define TCP_ULP_MAX\t\t128\n#define TCP_ULP_BUF_MAX\t\t(TCP_ULP_NAME_MAX*TCP_ULP_MAX)\n\nstruct tcp_ulp_ops {\n\tstruct list_head\tlist;\n\n\t \n\tint (*init)(struct sock *sk);\n\t \n\tvoid (*update)(struct sock *sk, struct proto *p,\n\t\t       void (*write_space)(struct sock *sk));\n\t \n\tvoid (*release)(struct sock *sk);\n\t \n\tint (*get_info)(const struct sock *sk, struct sk_buff *skb);\n\tsize_t (*get_info_size)(const struct sock *sk);\n\t \n\tvoid (*clone)(const struct request_sock *req, struct sock *newsk,\n\t\t      const gfp_t priority);\n\n\tchar\t\tname[TCP_ULP_NAME_MAX];\n\tstruct module\t*owner;\n};\nint tcp_register_ulp(struct tcp_ulp_ops *type);\nvoid tcp_unregister_ulp(struct tcp_ulp_ops *type);\nint tcp_set_ulp(struct sock *sk, const char *name);\nvoid tcp_get_available_ulp(char *buf, size_t len);\nvoid tcp_cleanup_ulp(struct sock *sk);\nvoid tcp_update_ulp(struct sock *sk, struct proto *p,\n\t\t    void (*write_space)(struct sock *sk));\n\n#define MODULE_ALIAS_TCP_ULP(name)\t\t\t\t\\\n\t__MODULE_INFO(alias, alias_userspace, name);\t\t\\\n\t__MODULE_INFO(alias, alias_tcp_ulp, \"tcp-ulp-\" name)\n\n#ifdef CONFIG_NET_SOCK_MSG\nstruct sk_msg;\nstruct sk_psock;\n\n#ifdef CONFIG_BPF_SYSCALL\nint tcp_bpf_update_proto(struct sock *sk, struct sk_psock *psock, bool restore);\nvoid tcp_bpf_clone(const struct sock *sk, struct sock *newsk);\n#endif  \n\n#ifdef CONFIG_INET\nvoid tcp_eat_skb(struct sock *sk, struct sk_buff *skb);\n#else\nstatic inline void tcp_eat_skb(struct sock *sk, struct sk_buff *skb)\n{\n}\n#endif\n\nint tcp_bpf_sendmsg_redir(struct sock *sk, bool ingress,\n\t\t\t  struct sk_msg *msg, u32 bytes, int flags);\n#endif  \n\n#if !defined(CONFIG_BPF_SYSCALL) || !defined(CONFIG_NET_SOCK_MSG)\nstatic inline void tcp_bpf_clone(const struct sock *sk, struct sock *newsk)\n{\n}\n#endif\n\n#ifdef CONFIG_CGROUP_BPF\nstatic inline void bpf_skops_init_skb(struct bpf_sock_ops_kern *skops,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int end_offset)\n{\n\tskops->skb = skb;\n\tskops->skb_data_end = skb->data + end_offset;\n}\n#else\nstatic inline void bpf_skops_init_skb(struct bpf_sock_ops_kern *skops,\n\t\t\t\t      struct sk_buff *skb,\n\t\t\t\t      unsigned int end_offset)\n{\n}\n#endif\n\n \n#ifdef CONFIG_BPF\nstatic inline int tcp_call_bpf(struct sock *sk, int op, u32 nargs, u32 *args)\n{\n\tstruct bpf_sock_ops_kern sock_ops;\n\tint ret;\n\n\tmemset(&sock_ops, 0, offsetof(struct bpf_sock_ops_kern, temp));\n\tif (sk_fullsock(sk)) {\n\t\tsock_ops.is_fullsock = 1;\n\t\tsock_owned_by_me(sk);\n\t}\n\n\tsock_ops.sk = sk;\n\tsock_ops.op = op;\n\tif (nargs > 0)\n\t\tmemcpy(sock_ops.args, args, nargs * sizeof(*args));\n\n\tret = BPF_CGROUP_RUN_PROG_SOCK_OPS(&sock_ops);\n\tif (ret == 0)\n\t\tret = sock_ops.reply;\n\telse\n\t\tret = -1;\n\treturn ret;\n}\n\nstatic inline int tcp_call_bpf_2arg(struct sock *sk, int op, u32 arg1, u32 arg2)\n{\n\tu32 args[2] = {arg1, arg2};\n\n\treturn tcp_call_bpf(sk, op, 2, args);\n}\n\nstatic inline int tcp_call_bpf_3arg(struct sock *sk, int op, u32 arg1, u32 arg2,\n\t\t\t\t    u32 arg3)\n{\n\tu32 args[3] = {arg1, arg2, arg3};\n\n\treturn tcp_call_bpf(sk, op, 3, args);\n}\n\n#else\nstatic inline int tcp_call_bpf(struct sock *sk, int op, u32 nargs, u32 *args)\n{\n\treturn -EPERM;\n}\n\nstatic inline int tcp_call_bpf_2arg(struct sock *sk, int op, u32 arg1, u32 arg2)\n{\n\treturn -EPERM;\n}\n\nstatic inline int tcp_call_bpf_3arg(struct sock *sk, int op, u32 arg1, u32 arg2,\n\t\t\t\t    u32 arg3)\n{\n\treturn -EPERM;\n}\n\n#endif\n\nstatic inline u32 tcp_timeout_init(struct sock *sk)\n{\n\tint timeout;\n\n\ttimeout = tcp_call_bpf(sk, BPF_SOCK_OPS_TIMEOUT_INIT, 0, NULL);\n\n\tif (timeout <= 0)\n\t\ttimeout = TCP_TIMEOUT_INIT;\n\treturn min_t(int, timeout, TCP_RTO_MAX);\n}\n\nstatic inline u32 tcp_rwnd_init_bpf(struct sock *sk)\n{\n\tint rwnd;\n\n\trwnd = tcp_call_bpf(sk, BPF_SOCK_OPS_RWND_INIT, 0, NULL);\n\n\tif (rwnd < 0)\n\t\trwnd = 0;\n\treturn rwnd;\n}\n\nstatic inline bool tcp_bpf_ca_needs_ecn(struct sock *sk)\n{\n\treturn (tcp_call_bpf(sk, BPF_SOCK_OPS_NEEDS_ECN, 0, NULL) == 1);\n}\n\nstatic inline void tcp_bpf_rtt(struct sock *sk)\n{\n\tif (BPF_SOCK_OPS_TEST_FLAG(tcp_sk(sk), BPF_SOCK_OPS_RTT_CB_FLAG))\n\t\ttcp_call_bpf(sk, BPF_SOCK_OPS_RTT_CB, 0, NULL);\n}\n\n#if IS_ENABLED(CONFIG_SMC)\nextern struct static_key_false tcp_have_smc;\n#endif\n\n#if IS_ENABLED(CONFIG_TLS_DEVICE)\nvoid clean_acked_data_enable(struct inet_connection_sock *icsk,\n\t\t\t     void (*cad)(struct sock *sk, u32 ack_seq));\nvoid clean_acked_data_disable(struct inet_connection_sock *icsk);\nvoid clean_acked_data_flush(void);\n#endif\n\nDECLARE_STATIC_KEY_FALSE(tcp_tx_delay_enabled);\nstatic inline void tcp_add_tx_delay(struct sk_buff *skb,\n\t\t\t\t    const struct tcp_sock *tp)\n{\n\tif (static_branch_unlikely(&tcp_tx_delay_enabled))\n\t\tskb->skb_mstamp_ns += (u64)tp->tcp_tx_delay * NSEC_PER_USEC;\n}\n\n \nstatic inline u64 tcp_transmit_time(const struct sock *sk)\n{\n\tif (static_branch_unlikely(&tcp_tx_delay_enabled)) {\n\t\tu32 delay = (sk->sk_state == TCP_TIME_WAIT) ?\n\t\t\ttcp_twsk(sk)->tw_tx_delay : tcp_sk(sk)->tcp_tx_delay;\n\n\t\treturn tcp_clock_ns() + (u64)delay * NSEC_PER_USEC;\n\t}\n\treturn 0;\n}\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}