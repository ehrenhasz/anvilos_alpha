{
  "module_name": "checksum.h",
  "hash_id": "09ec9e2214d7f14ab1fe064b7fdfe2172d06210e8e85b2c8ffef3cdd8e56a2c3",
  "original_prompt": "Ingested from linux-6.6.14/include/net/checksum.h",
  "human_readable_source": " \n \n\n#ifndef _CHECKSUM_H\n#define _CHECKSUM_H\n\n#include <linux/errno.h>\n#include <asm/types.h>\n#include <asm/byteorder.h>\n#include <asm/checksum.h>\n#if !defined(_HAVE_ARCH_COPY_AND_CSUM_FROM_USER) || !defined(HAVE_CSUM_COPY_USER)\n#include <linux/uaccess.h>\n#endif\n\n#ifndef _HAVE_ARCH_COPY_AND_CSUM_FROM_USER\nstatic __always_inline\n__wsum csum_and_copy_from_user (const void __user *src, void *dst,\n\t\t\t\t      int len)\n{\n\tif (copy_from_user(dst, src, len))\n\t\treturn 0;\n\treturn csum_partial(dst, len, ~0U);\n}\n#endif\n\n#ifndef HAVE_CSUM_COPY_USER\nstatic __always_inline __wsum csum_and_copy_to_user\n(const void *src, void __user *dst, int len)\n{\n\t__wsum sum = csum_partial(src, len, ~0U);\n\n\tif (copy_to_user(dst, src, len) == 0)\n\t\treturn sum;\n\treturn 0;\n}\n#endif\n\n#ifndef _HAVE_ARCH_CSUM_AND_COPY\nstatic __always_inline __wsum\ncsum_partial_copy_nocheck(const void *src, void *dst, int len)\n{\n\tmemcpy(dst, src, len);\n\treturn csum_partial(dst, len, 0);\n}\n#endif\n\n#ifndef HAVE_ARCH_CSUM_ADD\nstatic __always_inline __wsum csum_add(__wsum csum, __wsum addend)\n{\n\tu32 res = (__force u32)csum;\n\tres += (__force u32)addend;\n\treturn (__force __wsum)(res + (res < (__force u32)addend));\n}\n#endif\n\nstatic __always_inline __wsum csum_sub(__wsum csum, __wsum addend)\n{\n\treturn csum_add(csum, ~addend);\n}\n\nstatic __always_inline __sum16 csum16_add(__sum16 csum, __be16 addend)\n{\n\tu16 res = (__force u16)csum;\n\n\tres += (__force u16)addend;\n\treturn (__force __sum16)(res + (res < (__force u16)addend));\n}\n\nstatic __always_inline __sum16 csum16_sub(__sum16 csum, __be16 addend)\n{\n\treturn csum16_add(csum, ~addend);\n}\n\n#ifndef HAVE_ARCH_CSUM_SHIFT\nstatic __always_inline __wsum csum_shift(__wsum sum, int offset)\n{\n\t \n\tif (offset & 1)\n\t\treturn (__force __wsum)ror32((__force u32)sum, 8);\n\treturn sum;\n}\n#endif\n\nstatic __always_inline __wsum\ncsum_block_add(__wsum csum, __wsum csum2, int offset)\n{\n\treturn csum_add(csum, csum_shift(csum2, offset));\n}\n\nstatic __always_inline __wsum\ncsum_block_add_ext(__wsum csum, __wsum csum2, int offset, int len)\n{\n\treturn csum_block_add(csum, csum2, offset);\n}\n\nstatic __always_inline __wsum\ncsum_block_sub(__wsum csum, __wsum csum2, int offset)\n{\n\treturn csum_block_add(csum, ~csum2, offset);\n}\n\nstatic __always_inline __wsum csum_unfold(__sum16 n)\n{\n\treturn (__force __wsum)n;\n}\n\nstatic __always_inline\n__wsum csum_partial_ext(const void *buff, int len, __wsum sum)\n{\n\treturn csum_partial(buff, len, sum);\n}\n\n#define CSUM_MANGLED_0 ((__force __sum16)0xffff)\n\nstatic __always_inline void csum_replace_by_diff(__sum16 *sum, __wsum diff)\n{\n\t*sum = csum_fold(csum_add(diff, ~csum_unfold(*sum)));\n}\n\nstatic __always_inline void csum_replace4(__sum16 *sum, __be32 from, __be32 to)\n{\n\t__wsum tmp = csum_sub(~csum_unfold(*sum), (__force __wsum)from);\n\n\t*sum = csum_fold(csum_add(tmp, (__force __wsum)to));\n}\n\n \nstatic __always_inline void csum_replace2(__sum16 *sum, __be16 old, __be16 new)\n{\n\t*sum = ~csum16_add(csum16_sub(~(*sum), old), new);\n}\n\nstatic inline void csum_replace(__wsum *csum, __wsum old, __wsum new)\n{\n\t*csum = csum_add(csum_sub(*csum, old), new);\n}\n\nstruct sk_buff;\nvoid inet_proto_csum_replace4(__sum16 *sum, struct sk_buff *skb,\n\t\t\t      __be32 from, __be32 to, bool pseudohdr);\nvoid inet_proto_csum_replace16(__sum16 *sum, struct sk_buff *skb,\n\t\t\t       const __be32 *from, const __be32 *to,\n\t\t\t       bool pseudohdr);\nvoid inet_proto_csum_replace_by_diff(__sum16 *sum, struct sk_buff *skb,\n\t\t\t\t     __wsum diff, bool pseudohdr);\n\nstatic __always_inline\nvoid inet_proto_csum_replace2(__sum16 *sum, struct sk_buff *skb,\n\t\t\t      __be16 from, __be16 to, bool pseudohdr)\n{\n\tinet_proto_csum_replace4(sum, skb, (__force __be32)from,\n\t\t\t\t (__force __be32)to, pseudohdr);\n}\n\nstatic __always_inline __wsum remcsum_adjust(void *ptr, __wsum csum,\n\t\t\t\t\t     int start, int offset)\n{\n\t__sum16 *psum = (__sum16 *)(ptr + offset);\n\t__wsum delta;\n\n\t \n\tcsum = csum_sub(csum, csum_partial(ptr, start, 0));\n\n\t \n\tdelta = csum_sub((__force __wsum)csum_fold(csum),\n\t\t\t (__force __wsum)*psum);\n\t*psum = csum_fold(csum);\n\n\treturn delta;\n}\n\nstatic __always_inline void remcsum_unadjust(__sum16 *psum, __wsum delta)\n{\n\t*psum = csum_fold(csum_sub(delta, (__force __wsum)*psum));\n}\n\nstatic __always_inline __wsum wsum_negate(__wsum val)\n{\n\treturn (__force __wsum)-((__force u32)val);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}