{
  "module_name": "sock.h",
  "hash_id": "171665206358c8713e05a698b112bbcb6632e10c4d838368493c92543b44f861",
  "original_prompt": "Ingested from linux-6.6.14/include/net/sock.h",
  "human_readable_source": " \n \n#ifndef _SOCK_H\n#define _SOCK_H\n\n#include <linux/hardirq.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/list_nulls.h>\n#include <linux/timer.h>\n#include <linux/cache.h>\n#include <linux/bitops.h>\n#include <linux/lockdep.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>\t \n#include <linux/mm.h>\n#include <linux/security.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/page_counter.h>\n#include <linux/memcontrol.h>\n#include <linux/static_key.h>\n#include <linux/sched.h>\n#include <linux/wait.h>\n#include <linux/cgroup-defs.h>\n#include <linux/rbtree.h>\n#include <linux/rculist_nulls.h>\n#include <linux/poll.h>\n#include <linux/sockptr.h>\n#include <linux/indirect_call_wrapper.h>\n#include <linux/atomic.h>\n#include <linux/refcount.h>\n#include <linux/llist.h>\n#include <net/dst.h>\n#include <net/checksum.h>\n#include <net/tcp_states.h>\n#include <linux/net_tstamp.h>\n#include <net/l3mdev.h>\n#include <uapi/linux/socket.h>\n\n \n\n \n#define SOCK_DEBUGGING\n#ifdef SOCK_DEBUGGING\n#define SOCK_DEBUG(sk, msg...) do { if ((sk) && sock_flag((sk), SOCK_DBG)) \\\n\t\t\t\t\tprintk(KERN_DEBUG msg); } while (0)\n#else\n \nstatic inline __printf(2, 3)\nvoid SOCK_DEBUG(const struct sock *sk, const char *msg, ...)\n{\n}\n#endif\n\n \ntypedef struct {\n\tspinlock_t\t\tslock;\n\tint\t\t\towned;\n\twait_queue_head_t\twq;\n\t \n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\tstruct lockdep_map dep_map;\n#endif\n} socket_lock_t;\n\nstruct sock;\nstruct proto;\nstruct net;\n\ntypedef __u32 __bitwise __portpair;\ntypedef __u64 __bitwise __addrpair;\n\n \nstruct sock_common {\n\tunion {\n\t\t__addrpair\tskc_addrpair;\n\t\tstruct {\n\t\t\t__be32\tskc_daddr;\n\t\t\t__be32\tskc_rcv_saddr;\n\t\t};\n\t};\n\tunion  {\n\t\tunsigned int\tskc_hash;\n\t\t__u16\t\tskc_u16hashes[2];\n\t};\n\t \n\tunion {\n\t\t__portpair\tskc_portpair;\n\t\tstruct {\n\t\t\t__be16\tskc_dport;\n\t\t\t__u16\tskc_num;\n\t\t};\n\t};\n\n\tunsigned short\t\tskc_family;\n\tvolatile unsigned char\tskc_state;\n\tunsigned char\t\tskc_reuse:4;\n\tunsigned char\t\tskc_reuseport:1;\n\tunsigned char\t\tskc_ipv6only:1;\n\tunsigned char\t\tskc_net_refcnt:1;\n\tint\t\t\tskc_bound_dev_if;\n\tunion {\n\t\tstruct hlist_node\tskc_bind_node;\n\t\tstruct hlist_node\tskc_portaddr_node;\n\t};\n\tstruct proto\t\t*skc_prot;\n\tpossible_net_t\t\tskc_net;\n\n#if IS_ENABLED(CONFIG_IPV6)\n\tstruct in6_addr\t\tskc_v6_daddr;\n\tstruct in6_addr\t\tskc_v6_rcv_saddr;\n#endif\n\n\tatomic64_t\t\tskc_cookie;\n\n\t \n\tunion {\n\t\tunsigned long\tskc_flags;\n\t\tstruct sock\t*skc_listener;  \n\t\tstruct inet_timewait_death_row *skc_tw_dr;  \n\t};\n\t \n\t \n\tint\t\t\tskc_dontcopy_begin[0];\n\t \n\tunion {\n\t\tstruct hlist_node\tskc_node;\n\t\tstruct hlist_nulls_node skc_nulls_node;\n\t};\n\tunsigned short\t\tskc_tx_queue_mapping;\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\tunsigned short\t\tskc_rx_queue_mapping;\n#endif\n\tunion {\n\t\tint\t\tskc_incoming_cpu;\n\t\tu32\t\tskc_rcv_wnd;\n\t\tu32\t\tskc_tw_rcv_nxt;  \n\t};\n\n\trefcount_t\t\tskc_refcnt;\n\t \n\tint                     skc_dontcopy_end[0];\n\tunion {\n\t\tu32\t\tskc_rxhash;\n\t\tu32\t\tskc_window_clamp;\n\t\tu32\t\tskc_tw_snd_nxt;  \n\t};\n\t \n};\n\nstruct bpf_local_storage;\nstruct sk_filter;\n\n \nstruct sock {\n\t \n\tstruct sock_common\t__sk_common;\n#define sk_node\t\t\t__sk_common.skc_node\n#define sk_nulls_node\t\t__sk_common.skc_nulls_node\n#define sk_refcnt\t\t__sk_common.skc_refcnt\n#define sk_tx_queue_mapping\t__sk_common.skc_tx_queue_mapping\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n#define sk_rx_queue_mapping\t__sk_common.skc_rx_queue_mapping\n#endif\n\n#define sk_dontcopy_begin\t__sk_common.skc_dontcopy_begin\n#define sk_dontcopy_end\t\t__sk_common.skc_dontcopy_end\n#define sk_hash\t\t\t__sk_common.skc_hash\n#define sk_portpair\t\t__sk_common.skc_portpair\n#define sk_num\t\t\t__sk_common.skc_num\n#define sk_dport\t\t__sk_common.skc_dport\n#define sk_addrpair\t\t__sk_common.skc_addrpair\n#define sk_daddr\t\t__sk_common.skc_daddr\n#define sk_rcv_saddr\t\t__sk_common.skc_rcv_saddr\n#define sk_family\t\t__sk_common.skc_family\n#define sk_state\t\t__sk_common.skc_state\n#define sk_reuse\t\t__sk_common.skc_reuse\n#define sk_reuseport\t\t__sk_common.skc_reuseport\n#define sk_ipv6only\t\t__sk_common.skc_ipv6only\n#define sk_net_refcnt\t\t__sk_common.skc_net_refcnt\n#define sk_bound_dev_if\t\t__sk_common.skc_bound_dev_if\n#define sk_bind_node\t\t__sk_common.skc_bind_node\n#define sk_prot\t\t\t__sk_common.skc_prot\n#define sk_net\t\t\t__sk_common.skc_net\n#define sk_v6_daddr\t\t__sk_common.skc_v6_daddr\n#define sk_v6_rcv_saddr\t__sk_common.skc_v6_rcv_saddr\n#define sk_cookie\t\t__sk_common.skc_cookie\n#define sk_incoming_cpu\t\t__sk_common.skc_incoming_cpu\n#define sk_flags\t\t__sk_common.skc_flags\n#define sk_rxhash\t\t__sk_common.skc_rxhash\n\n\t \n\tstruct dst_entry __rcu\t*sk_rx_dst;\n\tint\t\t\tsk_rx_dst_ifindex;\n\tu32\t\t\tsk_rx_dst_cookie;\n\n\tsocket_lock_t\t\tsk_lock;\n\tatomic_t\t\tsk_drops;\n\tint\t\t\tsk_rcvlowat;\n\tstruct sk_buff_head\tsk_error_queue;\n\tstruct sk_buff_head\tsk_receive_queue;\n\t \n\tstruct {\n\t\tatomic_t\trmem_alloc;\n\t\tint\t\tlen;\n\t\tstruct sk_buff\t*head;\n\t\tstruct sk_buff\t*tail;\n\t} sk_backlog;\n\n#define sk_rmem_alloc sk_backlog.rmem_alloc\n\n\tint\t\t\tsk_forward_alloc;\n\tu32\t\t\tsk_reserved_mem;\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tunsigned int\t\tsk_ll_usec;\n\t \n\tunsigned int\t\tsk_napi_id;\n#endif\n\tint\t\t\tsk_rcvbuf;\n\tint\t\t\tsk_disconnects;\n\n\tstruct sk_filter __rcu\t*sk_filter;\n\tunion {\n\t\tstruct socket_wq __rcu\t*sk_wq;\n\t\t \n\t\tstruct socket_wq\t*sk_wq_raw;\n\t\t \n\t};\n#ifdef CONFIG_XFRM\n\tstruct xfrm_policy __rcu *sk_policy[2];\n#endif\n\n\tstruct dst_entry __rcu\t*sk_dst_cache;\n\tatomic_t\t\tsk_omem_alloc;\n\tint\t\t\tsk_sndbuf;\n\n\t \n\tint\t\t\tsk_wmem_queued;\n\trefcount_t\t\tsk_wmem_alloc;\n\tunsigned long\t\tsk_tsq_flags;\n\tunion {\n\t\tstruct sk_buff\t*sk_send_head;\n\t\tstruct rb_root\ttcp_rtx_queue;\n\t};\n\tstruct sk_buff_head\tsk_write_queue;\n\t__s32\t\t\tsk_peek_off;\n\tint\t\t\tsk_write_pending;\n\t__u32\t\t\tsk_dst_pending_confirm;\n\tu32\t\t\tsk_pacing_status;  \n\tlong\t\t\tsk_sndtimeo;\n\tstruct timer_list\tsk_timer;\n\t__u32\t\t\tsk_priority;\n\t__u32\t\t\tsk_mark;\n\tunsigned long\t\tsk_pacing_rate;  \n\tunsigned long\t\tsk_max_pacing_rate;\n\tstruct page_frag\tsk_frag;\n\tnetdev_features_t\tsk_route_caps;\n\tint\t\t\tsk_gso_type;\n\tunsigned int\t\tsk_gso_max_size;\n\tgfp_t\t\t\tsk_allocation;\n\t__u32\t\t\tsk_txhash;\n\n\t \n\tu8\t\t\tsk_gso_disabled : 1,\n\t\t\t\tsk_kern_sock : 1,\n\t\t\t\tsk_no_check_tx : 1,\n\t\t\t\tsk_no_check_rx : 1,\n\t\t\t\tsk_userlocks : 4;\n\tu8\t\t\tsk_pacing_shift;\n\tu16\t\t\tsk_type;\n\tu16\t\t\tsk_protocol;\n\tu16\t\t\tsk_gso_max_segs;\n\tunsigned long\t        sk_lingertime;\n\tstruct proto\t\t*sk_prot_creator;\n\trwlock_t\t\tsk_callback_lock;\n\tint\t\t\tsk_err,\n\t\t\t\tsk_err_soft;\n\tu32\t\t\tsk_ack_backlog;\n\tu32\t\t\tsk_max_ack_backlog;\n\tkuid_t\t\t\tsk_uid;\n\tu8\t\t\tsk_txrehash;\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tu8\t\t\tsk_prefer_busy_poll;\n\tu16\t\t\tsk_busy_poll_budget;\n#endif\n\tspinlock_t\t\tsk_peer_lock;\n\tint\t\t\tsk_bind_phc;\n\tstruct pid\t\t*sk_peer_pid;\n\tconst struct cred\t*sk_peer_cred;\n\n\tlong\t\t\tsk_rcvtimeo;\n\tktime_t\t\t\tsk_stamp;\n#if BITS_PER_LONG==32\n\tseqlock_t\t\tsk_stamp_seq;\n#endif\n\tatomic_t\t\tsk_tskey;\n\tatomic_t\t\tsk_zckey;\n\tu32\t\t\tsk_tsflags;\n\tu8\t\t\tsk_shutdown;\n\n\tu8\t\t\tsk_clockid;\n\tu8\t\t\tsk_txtime_deadline_mode : 1,\n\t\t\t\tsk_txtime_report_errors : 1,\n\t\t\t\tsk_txtime_unused : 6;\n\tbool\t\t\tsk_use_task_frag;\n\n\tstruct socket\t\t*sk_socket;\n\tvoid\t\t\t*sk_user_data;\n#ifdef CONFIG_SECURITY\n\tvoid\t\t\t*sk_security;\n#endif\n\tstruct sock_cgroup_data\tsk_cgrp_data;\n\tstruct mem_cgroup\t*sk_memcg;\n\tvoid\t\t\t(*sk_state_change)(struct sock *sk);\n\tvoid\t\t\t(*sk_data_ready)(struct sock *sk);\n\tvoid\t\t\t(*sk_write_space)(struct sock *sk);\n\tvoid\t\t\t(*sk_error_report)(struct sock *sk);\n\tint\t\t\t(*sk_backlog_rcv)(struct sock *sk,\n\t\t\t\t\t\t  struct sk_buff *skb);\n#ifdef CONFIG_SOCK_VALIDATE_XMIT\n\tstruct sk_buff*\t\t(*sk_validate_xmit_skb)(struct sock *sk,\n\t\t\t\t\t\t\tstruct net_device *dev,\n\t\t\t\t\t\t\tstruct sk_buff *skb);\n#endif\n\tvoid                    (*sk_destruct)(struct sock *sk);\n\tstruct sock_reuseport __rcu\t*sk_reuseport_cb;\n#ifdef CONFIG_BPF_SYSCALL\n\tstruct bpf_local_storage __rcu\t*sk_bpf_storage;\n#endif\n\tstruct rcu_head\t\tsk_rcu;\n\tnetns_tracker\t\tns_tracker;\n\tstruct hlist_node\tsk_bind2_node;\n};\n\nenum sk_pacing {\n\tSK_PACING_NONE\t\t= 0,\n\tSK_PACING_NEEDED\t= 1,\n\tSK_PACING_FQ\t\t= 2,\n};\n\n \n#define SK_USER_DATA_NOCOPY\t1UL\n#define SK_USER_DATA_BPF\t2UL\n#define SK_USER_DATA_PSOCK\t4UL\n#define SK_USER_DATA_PTRMASK\t~(SK_USER_DATA_NOCOPY | SK_USER_DATA_BPF |\\\n\t\t\t\t  SK_USER_DATA_PSOCK)\n\n \nstatic inline bool sk_user_data_is_nocopy(const struct sock *sk)\n{\n\treturn ((uintptr_t)sk->sk_user_data & SK_USER_DATA_NOCOPY);\n}\n\n#define __sk_user_data(sk) ((*((void __rcu **)&(sk)->sk_user_data)))\n\n \nstatic inline void *\n__locked_read_sk_user_data_with_flags(const struct sock *sk,\n\t\t\t\t      uintptr_t flags)\n{\n\tuintptr_t sk_user_data =\n\t\t(uintptr_t)rcu_dereference_check(__sk_user_data(sk),\n\t\t\t\t\t\t lockdep_is_held(&sk->sk_callback_lock));\n\n\tWARN_ON_ONCE(flags & SK_USER_DATA_PTRMASK);\n\n\tif ((sk_user_data & flags) == flags)\n\t\treturn (void *)(sk_user_data & SK_USER_DATA_PTRMASK);\n\treturn NULL;\n}\n\n \nstatic inline void *\n__rcu_dereference_sk_user_data_with_flags(const struct sock *sk,\n\t\t\t\t\t  uintptr_t flags)\n{\n\tuintptr_t sk_user_data = (uintptr_t)rcu_dereference(__sk_user_data(sk));\n\n\tWARN_ON_ONCE(flags & SK_USER_DATA_PTRMASK);\n\n\tif ((sk_user_data & flags) == flags)\n\t\treturn (void *)(sk_user_data & SK_USER_DATA_PTRMASK);\n\treturn NULL;\n}\n\n#define rcu_dereference_sk_user_data(sk)\t\t\t\t\\\n\t__rcu_dereference_sk_user_data_with_flags(sk, 0)\n#define __rcu_assign_sk_user_data_with_flags(sk, ptr, flags)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tuintptr_t __tmp1 = (uintptr_t)(ptr),\t\t\t\t\\\n\t\t  __tmp2 = (uintptr_t)(flags);\t\t\t\t\\\n\tWARN_ON_ONCE(__tmp1 & ~SK_USER_DATA_PTRMASK);\t\t\t\\\n\tWARN_ON_ONCE(__tmp2 & SK_USER_DATA_PTRMASK);\t\t\t\\\n\trcu_assign_pointer(__sk_user_data((sk)),\t\t\t\\\n\t\t\t   __tmp1 | __tmp2);\t\t\t\t\\\n})\n#define rcu_assign_sk_user_data(sk, ptr)\t\t\t\t\\\n\t__rcu_assign_sk_user_data_with_flags(sk, ptr, 0)\n\nstatic inline\nstruct net *sock_net(const struct sock *sk)\n{\n\treturn read_pnet(&sk->sk_net);\n}\n\nstatic inline\nvoid sock_net_set(struct sock *sk, struct net *net)\n{\n\twrite_pnet(&sk->sk_net, net);\n}\n\n \n\n#define SK_NO_REUSE\t0\n#define SK_CAN_REUSE\t1\n#define SK_FORCE_REUSE\t2\n\nint sk_set_peek_off(struct sock *sk, int val);\n\nstatic inline int sk_peek_offset(const struct sock *sk, int flags)\n{\n\tif (unlikely(flags & MSG_PEEK)) {\n\t\treturn READ_ONCE(sk->sk_peek_off);\n\t}\n\n\treturn 0;\n}\n\nstatic inline void sk_peek_offset_bwd(struct sock *sk, int val)\n{\n\ts32 off = READ_ONCE(sk->sk_peek_off);\n\n\tif (unlikely(off >= 0)) {\n\t\toff = max_t(s32, off - val, 0);\n\t\tWRITE_ONCE(sk->sk_peek_off, off);\n\t}\n}\n\nstatic inline void sk_peek_offset_fwd(struct sock *sk, int val)\n{\n\tsk_peek_offset_bwd(sk, -val);\n}\n\n \nstatic inline struct sock *sk_entry(const struct hlist_node *node)\n{\n\treturn hlist_entry(node, struct sock, sk_node);\n}\n\nstatic inline struct sock *__sk_head(const struct hlist_head *head)\n{\n\treturn hlist_entry(head->first, struct sock, sk_node);\n}\n\nstatic inline struct sock *sk_head(const struct hlist_head *head)\n{\n\treturn hlist_empty(head) ? NULL : __sk_head(head);\n}\n\nstatic inline struct sock *__sk_nulls_head(const struct hlist_nulls_head *head)\n{\n\treturn hlist_nulls_entry(head->first, struct sock, sk_nulls_node);\n}\n\nstatic inline struct sock *sk_nulls_head(const struct hlist_nulls_head *head)\n{\n\treturn hlist_nulls_empty(head) ? NULL : __sk_nulls_head(head);\n}\n\nstatic inline struct sock *sk_next(const struct sock *sk)\n{\n\treturn hlist_entry_safe(sk->sk_node.next, struct sock, sk_node);\n}\n\nstatic inline struct sock *sk_nulls_next(const struct sock *sk)\n{\n\treturn (!is_a_nulls(sk->sk_nulls_node.next)) ?\n\t\thlist_nulls_entry(sk->sk_nulls_node.next,\n\t\t\t\t  struct sock, sk_nulls_node) :\n\t\tNULL;\n}\n\nstatic inline bool sk_unhashed(const struct sock *sk)\n{\n\treturn hlist_unhashed(&sk->sk_node);\n}\n\nstatic inline bool sk_hashed(const struct sock *sk)\n{\n\treturn !sk_unhashed(sk);\n}\n\nstatic inline void sk_node_init(struct hlist_node *node)\n{\n\tnode->pprev = NULL;\n}\n\nstatic inline void __sk_del_node(struct sock *sk)\n{\n\t__hlist_del(&sk->sk_node);\n}\n\n \nstatic inline bool __sk_del_node_init(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\t__sk_del_node(sk);\n\t\tsk_node_init(&sk->sk_node);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \n\nstatic __always_inline void sock_hold(struct sock *sk)\n{\n\trefcount_inc(&sk->sk_refcnt);\n}\n\n \nstatic __always_inline void __sock_put(struct sock *sk)\n{\n\trefcount_dec(&sk->sk_refcnt);\n}\n\nstatic inline bool sk_del_node_init(struct sock *sk)\n{\n\tbool rc = __sk_del_node_init(sk);\n\n\tif (rc) {\n\t\t \n\t\tWARN_ON(refcount_read(&sk->sk_refcnt) == 1);\n\t\t__sock_put(sk);\n\t}\n\treturn rc;\n}\n#define sk_del_node_init_rcu(sk)\tsk_del_node_init(sk)\n\nstatic inline bool __sk_nulls_del_node_init_rcu(struct sock *sk)\n{\n\tif (sk_hashed(sk)) {\n\t\thlist_nulls_del_init_rcu(&sk->sk_nulls_node);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool sk_nulls_del_node_init_rcu(struct sock *sk)\n{\n\tbool rc = __sk_nulls_del_node_init_rcu(sk);\n\n\tif (rc) {\n\t\t \n\t\tWARN_ON(refcount_read(&sk->sk_refcnt) == 1);\n\t\t__sock_put(sk);\n\t}\n\treturn rc;\n}\n\nstatic inline void __sk_add_node(struct sock *sk, struct hlist_head *list)\n{\n\thlist_add_head(&sk->sk_node, list);\n}\n\nstatic inline void sk_add_node(struct sock *sk, struct hlist_head *list)\n{\n\tsock_hold(sk);\n\t__sk_add_node(sk, list);\n}\n\nstatic inline void sk_add_node_rcu(struct sock *sk, struct hlist_head *list)\n{\n\tsock_hold(sk);\n\tif (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&\n\t    sk->sk_family == AF_INET6)\n\t\thlist_add_tail_rcu(&sk->sk_node, list);\n\telse\n\t\thlist_add_head_rcu(&sk->sk_node, list);\n}\n\nstatic inline void sk_add_node_tail_rcu(struct sock *sk, struct hlist_head *list)\n{\n\tsock_hold(sk);\n\thlist_add_tail_rcu(&sk->sk_node, list);\n}\n\nstatic inline void __sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)\n{\n\thlist_nulls_add_head_rcu(&sk->sk_nulls_node, list);\n}\n\nstatic inline void __sk_nulls_add_node_tail_rcu(struct sock *sk, struct hlist_nulls_head *list)\n{\n\thlist_nulls_add_tail_rcu(&sk->sk_nulls_node, list);\n}\n\nstatic inline void sk_nulls_add_node_rcu(struct sock *sk, struct hlist_nulls_head *list)\n{\n\tsock_hold(sk);\n\t__sk_nulls_add_node_rcu(sk, list);\n}\n\nstatic inline void __sk_del_bind_node(struct sock *sk)\n{\n\t__hlist_del(&sk->sk_bind_node);\n}\n\nstatic inline void sk_add_bind_node(struct sock *sk,\n\t\t\t\t\tstruct hlist_head *list)\n{\n\thlist_add_head(&sk->sk_bind_node, list);\n}\n\nstatic inline void __sk_del_bind2_node(struct sock *sk)\n{\n\t__hlist_del(&sk->sk_bind2_node);\n}\n\nstatic inline void sk_add_bind2_node(struct sock *sk, struct hlist_head *list)\n{\n\thlist_add_head(&sk->sk_bind2_node, list);\n}\n\n#define sk_for_each(__sk, list) \\\n\thlist_for_each_entry(__sk, list, sk_node)\n#define sk_for_each_rcu(__sk, list) \\\n\thlist_for_each_entry_rcu(__sk, list, sk_node)\n#define sk_nulls_for_each(__sk, node, list) \\\n\thlist_nulls_for_each_entry(__sk, node, list, sk_nulls_node)\n#define sk_nulls_for_each_rcu(__sk, node, list) \\\n\thlist_nulls_for_each_entry_rcu(__sk, node, list, sk_nulls_node)\n#define sk_for_each_from(__sk) \\\n\thlist_for_each_entry_from(__sk, sk_node)\n#define sk_nulls_for_each_from(__sk, node) \\\n\tif (__sk && ({ node = &(__sk)->sk_nulls_node; 1; })) \\\n\t\thlist_nulls_for_each_entry_from(__sk, node, sk_nulls_node)\n#define sk_for_each_safe(__sk, tmp, list) \\\n\thlist_for_each_entry_safe(__sk, tmp, list, sk_node)\n#define sk_for_each_bound(__sk, list) \\\n\thlist_for_each_entry(__sk, list, sk_bind_node)\n#define sk_for_each_bound_bhash2(__sk, list) \\\n\thlist_for_each_entry(__sk, list, sk_bind2_node)\n\n \n#define sk_for_each_entry_offset_rcu(tpos, pos, head, offset)\t\t       \\\n\tfor (pos = rcu_dereference(hlist_first_rcu(head));\t\t       \\\n\t     pos != NULL &&\t\t\t\t\t\t       \\\n\t\t({ tpos = (typeof(*tpos) *)((void *)pos - offset); 1;});       \\\n\t     pos = rcu_dereference(hlist_next_rcu(pos)))\n\nstatic inline struct user_namespace *sk_user_ns(const struct sock *sk)\n{\n\t \n\treturn sk->sk_socket->file->f_cred->user_ns;\n}\n\n \nenum sock_flags {\n\tSOCK_DEAD,\n\tSOCK_DONE,\n\tSOCK_URGINLINE,\n\tSOCK_KEEPOPEN,\n\tSOCK_LINGER,\n\tSOCK_DESTROY,\n\tSOCK_BROADCAST,\n\tSOCK_TIMESTAMP,\n\tSOCK_ZAPPED,\n\tSOCK_USE_WRITE_QUEUE,  \n\tSOCK_DBG,  \n\tSOCK_RCVTSTAMP,  \n\tSOCK_RCVTSTAMPNS,  \n\tSOCK_LOCALROUTE,  \n\tSOCK_MEMALLOC,  \n\tSOCK_TIMESTAMPING_RX_SOFTWARE,   \n\tSOCK_FASYNC,  \n\tSOCK_RXQ_OVFL,\n\tSOCK_ZEROCOPY,  \n\tSOCK_WIFI_STATUS,  \n\tSOCK_NOFCS,  \n\tSOCK_FILTER_LOCKED,  \n\tSOCK_SELECT_ERR_QUEUE,  \n\tSOCK_RCU_FREE,  \n\tSOCK_TXTIME,\n\tSOCK_XDP,  \n\tSOCK_TSTAMP_NEW,  \n\tSOCK_RCVMARK,  \n};\n\n#define SK_FLAGS_TIMESTAMP ((1UL << SOCK_TIMESTAMP) | (1UL << SOCK_TIMESTAMPING_RX_SOFTWARE))\n\nstatic inline void sock_copy_flags(struct sock *nsk, const struct sock *osk)\n{\n\tnsk->sk_flags = osk->sk_flags;\n}\n\nstatic inline void sock_set_flag(struct sock *sk, enum sock_flags flag)\n{\n\t__set_bit(flag, &sk->sk_flags);\n}\n\nstatic inline void sock_reset_flag(struct sock *sk, enum sock_flags flag)\n{\n\t__clear_bit(flag, &sk->sk_flags);\n}\n\nstatic inline void sock_valbool_flag(struct sock *sk, enum sock_flags bit,\n\t\t\t\t     int valbool)\n{\n\tif (valbool)\n\t\tsock_set_flag(sk, bit);\n\telse\n\t\tsock_reset_flag(sk, bit);\n}\n\nstatic inline bool sock_flag(const struct sock *sk, enum sock_flags flag)\n{\n\treturn test_bit(flag, &sk->sk_flags);\n}\n\n#ifdef CONFIG_NET\nDECLARE_STATIC_KEY_FALSE(memalloc_socks_key);\nstatic inline int sk_memalloc_socks(void)\n{\n\treturn static_branch_unlikely(&memalloc_socks_key);\n}\n\nvoid __receive_sock(struct file *file);\n#else\n\nstatic inline int sk_memalloc_socks(void)\n{\n\treturn 0;\n}\n\nstatic inline void __receive_sock(struct file *file)\n{ }\n#endif\n\nstatic inline gfp_t sk_gfp_mask(const struct sock *sk, gfp_t gfp_mask)\n{\n\treturn gfp_mask | (sk->sk_allocation & __GFP_MEMALLOC);\n}\n\nstatic inline void sk_acceptq_removed(struct sock *sk)\n{\n\tWRITE_ONCE(sk->sk_ack_backlog, sk->sk_ack_backlog - 1);\n}\n\nstatic inline void sk_acceptq_added(struct sock *sk)\n{\n\tWRITE_ONCE(sk->sk_ack_backlog, sk->sk_ack_backlog + 1);\n}\n\n \nstatic inline bool sk_acceptq_is_full(const struct sock *sk)\n{\n\treturn READ_ONCE(sk->sk_ack_backlog) > READ_ONCE(sk->sk_max_ack_backlog);\n}\n\n \nstatic inline int sk_stream_min_wspace(const struct sock *sk)\n{\n\treturn READ_ONCE(sk->sk_wmem_queued) >> 1;\n}\n\nstatic inline int sk_stream_wspace(const struct sock *sk)\n{\n\treturn READ_ONCE(sk->sk_sndbuf) - READ_ONCE(sk->sk_wmem_queued);\n}\n\nstatic inline void sk_wmem_queued_add(struct sock *sk, int val)\n{\n\tWRITE_ONCE(sk->sk_wmem_queued, sk->sk_wmem_queued + val);\n}\n\nstatic inline void sk_forward_alloc_add(struct sock *sk, int val)\n{\n\t \n\tWRITE_ONCE(sk->sk_forward_alloc, sk->sk_forward_alloc + val);\n}\n\nvoid sk_stream_write_space(struct sock *sk);\n\n \nstatic inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)\n{\n\t \n\tskb_dst_force(skb);\n\n\tif (!sk->sk_backlog.tail)\n\t\tWRITE_ONCE(sk->sk_backlog.head, skb);\n\telse\n\t\tsk->sk_backlog.tail->next = skb;\n\n\tWRITE_ONCE(sk->sk_backlog.tail, skb);\n\tskb->next = NULL;\n}\n\n \nstatic inline bool sk_rcvqueues_full(const struct sock *sk, unsigned int limit)\n{\n\tunsigned int qsize = sk->sk_backlog.len + atomic_read(&sk->sk_rmem_alloc);\n\n\treturn qsize > limit;\n}\n\n \nstatic inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t      unsigned int limit)\n{\n\tif (sk_rcvqueues_full(sk, limit))\n\t\treturn -ENOBUFS;\n\n\t \n\tif (skb_pfmemalloc(skb) && !sock_flag(sk, SOCK_MEMALLOC))\n\t\treturn -ENOMEM;\n\n\t__sk_add_backlog(sk, skb);\n\tsk->sk_backlog.len += skb->truesize;\n\treturn 0;\n}\n\nint __sk_backlog_rcv(struct sock *sk, struct sk_buff *skb);\n\nINDIRECT_CALLABLE_DECLARE(int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb));\nINDIRECT_CALLABLE_DECLARE(int tcp_v6_do_rcv(struct sock *sk, struct sk_buff *skb));\n\nstatic inline int sk_backlog_rcv(struct sock *sk, struct sk_buff *skb)\n{\n\tif (sk_memalloc_socks() && skb_pfmemalloc(skb))\n\t\treturn __sk_backlog_rcv(sk, skb);\n\n\treturn INDIRECT_CALL_INET(sk->sk_backlog_rcv,\n\t\t\t\t  tcp_v6_do_rcv,\n\t\t\t\t  tcp_v4_do_rcv,\n\t\t\t\t  sk, skb);\n}\n\nstatic inline void sk_incoming_cpu_update(struct sock *sk)\n{\n\tint cpu = raw_smp_processor_id();\n\n\tif (unlikely(READ_ONCE(sk->sk_incoming_cpu) != cpu))\n\t\tWRITE_ONCE(sk->sk_incoming_cpu, cpu);\n}\n\nstatic inline void sock_rps_record_flow_hash(__u32 hash)\n{\n#ifdef CONFIG_RPS\n\tstruct rps_sock_flow_table *sock_flow_table;\n\n\trcu_read_lock();\n\tsock_flow_table = rcu_dereference(rps_sock_flow_table);\n\trps_record_sock_flow(sock_flow_table, hash);\n\trcu_read_unlock();\n#endif\n}\n\nstatic inline void sock_rps_record_flow(const struct sock *sk)\n{\n#ifdef CONFIG_RPS\n\tif (static_branch_unlikely(&rfs_needed)) {\n\t\t \n\t\tif (sk->sk_state == TCP_ESTABLISHED) {\n\t\t\t \n\t\t\tsock_rps_record_flow_hash(READ_ONCE(sk->sk_rxhash));\n\t\t}\n\t}\n#endif\n}\n\nstatic inline void sock_rps_save_rxhash(struct sock *sk,\n\t\t\t\t\tconst struct sk_buff *skb)\n{\n#ifdef CONFIG_RPS\n\t \n\tif (unlikely(READ_ONCE(sk->sk_rxhash) != skb->hash))\n\t\tWRITE_ONCE(sk->sk_rxhash, skb->hash);\n#endif\n}\n\nstatic inline void sock_rps_reset_rxhash(struct sock *sk)\n{\n#ifdef CONFIG_RPS\n\t \n\tWRITE_ONCE(sk->sk_rxhash, 0);\n#endif\n}\n\n#define sk_wait_event(__sk, __timeo, __condition, __wait)\t\t\\\n\t({\tint __rc, __dis = __sk->sk_disconnects;\t\t\t\\\n\t\trelease_sock(__sk);\t\t\t\t\t\\\n\t\t__rc = __condition;\t\t\t\t\t\\\n\t\tif (!__rc) {\t\t\t\t\t\t\\\n\t\t\t*(__timeo) = wait_woken(__wait,\t\t\t\\\n\t\t\t\t\t\tTASK_INTERRUPTIBLE,\t\\\n\t\t\t\t\t\t*(__timeo));\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\tsched_annotate_sleep();\t\t\t\t\t\\\n\t\tlock_sock(__sk);\t\t\t\t\t\\\n\t\t__rc = __dis == __sk->sk_disconnects ? __condition : -EPIPE; \\\n\t\t__rc;\t\t\t\t\t\t\t\\\n\t})\n\nint sk_stream_wait_connect(struct sock *sk, long *timeo_p);\nint sk_stream_wait_memory(struct sock *sk, long *timeo_p);\nvoid sk_stream_wait_close(struct sock *sk, long timeo_p);\nint sk_stream_error(struct sock *sk, int flags, int err);\nvoid sk_stream_kill_queues(struct sock *sk);\nvoid sk_set_memalloc(struct sock *sk);\nvoid sk_clear_memalloc(struct sock *sk);\n\nvoid __sk_flush_backlog(struct sock *sk);\n\nstatic inline bool sk_flush_backlog(struct sock *sk)\n{\n\tif (unlikely(READ_ONCE(sk->sk_backlog.tail))) {\n\t\t__sk_flush_backlog(sk);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nint sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb);\n\nstruct request_sock_ops;\nstruct timewait_sock_ops;\nstruct inet_hashinfo;\nstruct raw_hashinfo;\nstruct smc_hashinfo;\nstruct module;\nstruct sk_psock;\n\n \nstatic inline void sk_prot_clear_nulls(struct sock *sk, int size)\n{\n\tif (offsetof(struct sock, sk_node.next) != 0)\n\t\tmemset(sk, 0, offsetof(struct sock, sk_node.next));\n\tmemset(&sk->sk_node.pprev, 0,\n\t       size - offsetof(struct sock, sk_node.pprev));\n}\n\n \nstruct proto {\n\tvoid\t\t\t(*close)(struct sock *sk,\n\t\t\t\t\tlong timeout);\n\tint\t\t\t(*pre_connect)(struct sock *sk,\n\t\t\t\t\tstruct sockaddr *uaddr,\n\t\t\t\t\tint addr_len);\n\tint\t\t\t(*connect)(struct sock *sk,\n\t\t\t\t\tstruct sockaddr *uaddr,\n\t\t\t\t\tint addr_len);\n\tint\t\t\t(*disconnect)(struct sock *sk, int flags);\n\n\tstruct sock *\t\t(*accept)(struct sock *sk, int flags, int *err,\n\t\t\t\t\t  bool kern);\n\n\tint\t\t\t(*ioctl)(struct sock *sk, int cmd,\n\t\t\t\t\t int *karg);\n\tint\t\t\t(*init)(struct sock *sk);\n\tvoid\t\t\t(*destroy)(struct sock *sk);\n\tvoid\t\t\t(*shutdown)(struct sock *sk, int how);\n\tint\t\t\t(*setsockopt)(struct sock *sk, int level,\n\t\t\t\t\tint optname, sockptr_t optval,\n\t\t\t\t\tunsigned int optlen);\n\tint\t\t\t(*getsockopt)(struct sock *sk, int level,\n\t\t\t\t\tint optname, char __user *optval,\n\t\t\t\t\tint __user *option);\n\tvoid\t\t\t(*keepalive)(struct sock *sk, int valbool);\n#ifdef CONFIG_COMPAT\n\tint\t\t\t(*compat_ioctl)(struct sock *sk,\n\t\t\t\t\tunsigned int cmd, unsigned long arg);\n#endif\n\tint\t\t\t(*sendmsg)(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t   size_t len);\n\tint\t\t\t(*recvmsg)(struct sock *sk, struct msghdr *msg,\n\t\t\t\t\t   size_t len, int flags, int *addr_len);\n\tvoid\t\t\t(*splice_eof)(struct socket *sock);\n\tint\t\t\t(*bind)(struct sock *sk,\n\t\t\t\t\tstruct sockaddr *addr, int addr_len);\n\tint\t\t\t(*bind_add)(struct sock *sk,\n\t\t\t\t\tstruct sockaddr *addr, int addr_len);\n\n\tint\t\t\t(*backlog_rcv) (struct sock *sk,\n\t\t\t\t\t\tstruct sk_buff *skb);\n\tbool\t\t\t(*bpf_bypass_getsockopt)(int level,\n\t\t\t\t\t\t\t int optname);\n\n\tvoid\t\t(*release_cb)(struct sock *sk);\n\n\t \n\tint\t\t\t(*hash)(struct sock *sk);\n\tvoid\t\t\t(*unhash)(struct sock *sk);\n\tvoid\t\t\t(*rehash)(struct sock *sk);\n\tint\t\t\t(*get_port)(struct sock *sk, unsigned short snum);\n\tvoid\t\t\t(*put_port)(struct sock *sk);\n#ifdef CONFIG_BPF_SYSCALL\n\tint\t\t\t(*psock_update_sk_prot)(struct sock *sk,\n\t\t\t\t\t\t\tstruct sk_psock *psock,\n\t\t\t\t\t\t\tbool restore);\n#endif\n\n\t \n#ifdef CONFIG_PROC_FS\n\tunsigned int\t\tinuse_idx;\n#endif\n\n#if IS_ENABLED(CONFIG_MPTCP)\n\tint\t\t\t(*forward_alloc_get)(const struct sock *sk);\n#endif\n\n\tbool\t\t\t(*stream_memory_free)(const struct sock *sk, int wake);\n\tbool\t\t\t(*sock_is_readable)(struct sock *sk);\n\t \n\tvoid\t\t\t(*enter_memory_pressure)(struct sock *sk);\n\tvoid\t\t\t(*leave_memory_pressure)(struct sock *sk);\n\tatomic_long_t\t\t*memory_allocated;\t \n\tint  __percpu\t\t*per_cpu_fw_alloc;\n\tstruct percpu_counter\t*sockets_allocated;\t \n\n\t \n\tunsigned long\t\t*memory_pressure;\n\tlong\t\t\t*sysctl_mem;\n\n\tint\t\t\t*sysctl_wmem;\n\tint\t\t\t*sysctl_rmem;\n\tu32\t\t\tsysctl_wmem_offset;\n\tu32\t\t\tsysctl_rmem_offset;\n\n\tint\t\t\tmax_header;\n\tbool\t\t\tno_autobind;\n\n\tstruct kmem_cache\t*slab;\n\tunsigned int\t\tobj_size;\n\tunsigned int\t\tipv6_pinfo_offset;\n\tslab_flags_t\t\tslab_flags;\n\tunsigned int\t\tuseroffset;\t \n\tunsigned int\t\tusersize;\t \n\n\tunsigned int __percpu\t*orphan_count;\n\n\tstruct request_sock_ops\t*rsk_prot;\n\tstruct timewait_sock_ops *twsk_prot;\n\n\tunion {\n\t\tstruct inet_hashinfo\t*hashinfo;\n\t\tstruct udp_table\t*udp_table;\n\t\tstruct raw_hashinfo\t*raw_hash;\n\t\tstruct smc_hashinfo\t*smc_hash;\n\t} h;\n\n\tstruct module\t\t*owner;\n\n\tchar\t\t\tname[32];\n\n\tstruct list_head\tnode;\n\tint\t\t\t(*diag_destroy)(struct sock *sk, int err);\n} __randomize_layout;\n\nint proto_register(struct proto *prot, int alloc_slab);\nvoid proto_unregister(struct proto *prot);\nint sock_load_diag_module(int family, int protocol);\n\nINDIRECT_CALLABLE_DECLARE(bool tcp_stream_memory_free(const struct sock *sk, int wake));\n\nstatic inline int sk_forward_alloc_get(const struct sock *sk)\n{\n#if IS_ENABLED(CONFIG_MPTCP)\n\tif (sk->sk_prot->forward_alloc_get)\n\t\treturn sk->sk_prot->forward_alloc_get(sk);\n#endif\n\treturn READ_ONCE(sk->sk_forward_alloc);\n}\n\nstatic inline bool __sk_stream_memory_free(const struct sock *sk, int wake)\n{\n\tif (READ_ONCE(sk->sk_wmem_queued) >= READ_ONCE(sk->sk_sndbuf))\n\t\treturn false;\n\n\treturn sk->sk_prot->stream_memory_free ?\n\t\tINDIRECT_CALL_INET_1(sk->sk_prot->stream_memory_free,\n\t\t\t\t     tcp_stream_memory_free, sk, wake) : true;\n}\n\nstatic inline bool sk_stream_memory_free(const struct sock *sk)\n{\n\treturn __sk_stream_memory_free(sk, 0);\n}\n\nstatic inline bool __sk_stream_is_writeable(const struct sock *sk, int wake)\n{\n\treturn sk_stream_wspace(sk) >= sk_stream_min_wspace(sk) &&\n\t       __sk_stream_memory_free(sk, wake);\n}\n\nstatic inline bool sk_stream_is_writeable(const struct sock *sk)\n{\n\treturn __sk_stream_is_writeable(sk, 0);\n}\n\nstatic inline int sk_under_cgroup_hierarchy(struct sock *sk,\n\t\t\t\t\t    struct cgroup *ancestor)\n{\n#ifdef CONFIG_SOCK_CGROUP_DATA\n\treturn cgroup_is_descendant(sock_cgroup_ptr(&sk->sk_cgrp_data),\n\t\t\t\t    ancestor);\n#else\n\treturn -ENOTSUPP;\n#endif\n}\n\nstatic inline bool sk_has_memory_pressure(const struct sock *sk)\n{\n\treturn sk->sk_prot->memory_pressure != NULL;\n}\n\nstatic inline bool sk_under_global_memory_pressure(const struct sock *sk)\n{\n\treturn sk->sk_prot->memory_pressure &&\n\t\t!!READ_ONCE(*sk->sk_prot->memory_pressure);\n}\n\nstatic inline bool sk_under_memory_pressure(const struct sock *sk)\n{\n\tif (!sk->sk_prot->memory_pressure)\n\t\treturn false;\n\n\tif (mem_cgroup_sockets_enabled && sk->sk_memcg &&\n\t    mem_cgroup_under_socket_pressure(sk->sk_memcg))\n\t\treturn true;\n\n\treturn !!READ_ONCE(*sk->sk_prot->memory_pressure);\n}\n\nstatic inline long\nproto_memory_allocated(const struct proto *prot)\n{\n\treturn max(0L, atomic_long_read(prot->memory_allocated));\n}\n\nstatic inline long\nsk_memory_allocated(const struct sock *sk)\n{\n\treturn proto_memory_allocated(sk->sk_prot);\n}\n\n \n#define SK_MEMORY_PCPU_RESERVE (1 << (20 - PAGE_SHIFT))\n\nstatic inline void\nsk_memory_allocated_add(struct sock *sk, int amt)\n{\n\tint local_reserve;\n\n\tpreempt_disable();\n\tlocal_reserve = __this_cpu_add_return(*sk->sk_prot->per_cpu_fw_alloc, amt);\n\tif (local_reserve >= SK_MEMORY_PCPU_RESERVE) {\n\t\t__this_cpu_sub(*sk->sk_prot->per_cpu_fw_alloc, local_reserve);\n\t\tatomic_long_add(local_reserve, sk->sk_prot->memory_allocated);\n\t}\n\tpreempt_enable();\n}\n\nstatic inline void\nsk_memory_allocated_sub(struct sock *sk, int amt)\n{\n\tint local_reserve;\n\n\tpreempt_disable();\n\tlocal_reserve = __this_cpu_sub_return(*sk->sk_prot->per_cpu_fw_alloc, amt);\n\tif (local_reserve <= -SK_MEMORY_PCPU_RESERVE) {\n\t\t__this_cpu_sub(*sk->sk_prot->per_cpu_fw_alloc, local_reserve);\n\t\tatomic_long_add(local_reserve, sk->sk_prot->memory_allocated);\n\t}\n\tpreempt_enable();\n}\n\n#define SK_ALLOC_PERCPU_COUNTER_BATCH 16\n\nstatic inline void sk_sockets_allocated_dec(struct sock *sk)\n{\n\tpercpu_counter_add_batch(sk->sk_prot->sockets_allocated, -1,\n\t\t\t\t SK_ALLOC_PERCPU_COUNTER_BATCH);\n}\n\nstatic inline void sk_sockets_allocated_inc(struct sock *sk)\n{\n\tpercpu_counter_add_batch(sk->sk_prot->sockets_allocated, 1,\n\t\t\t\t SK_ALLOC_PERCPU_COUNTER_BATCH);\n}\n\nstatic inline u64\nsk_sockets_allocated_read_positive(struct sock *sk)\n{\n\treturn percpu_counter_read_positive(sk->sk_prot->sockets_allocated);\n}\n\nstatic inline int\nproto_sockets_allocated_sum_positive(struct proto *prot)\n{\n\treturn percpu_counter_sum_positive(prot->sockets_allocated);\n}\n\nstatic inline bool\nproto_memory_pressure(struct proto *prot)\n{\n\tif (!prot->memory_pressure)\n\t\treturn false;\n\treturn !!READ_ONCE(*prot->memory_pressure);\n}\n\n\n#ifdef CONFIG_PROC_FS\n#define PROTO_INUSE_NR\t64\t \nstruct prot_inuse {\n\tint all;\n\tint val[PROTO_INUSE_NR];\n};\n\nstatic inline void sock_prot_inuse_add(const struct net *net,\n\t\t\t\t       const struct proto *prot, int val)\n{\n\tthis_cpu_add(net->core.prot_inuse->val[prot->inuse_idx], val);\n}\n\nstatic inline void sock_inuse_add(const struct net *net, int val)\n{\n\tthis_cpu_add(net->core.prot_inuse->all, val);\n}\n\nint sock_prot_inuse_get(struct net *net, struct proto *proto);\nint sock_inuse_get(struct net *net);\n#else\nstatic inline void sock_prot_inuse_add(const struct net *net,\n\t\t\t\t       const struct proto *prot, int val)\n{\n}\n\nstatic inline void sock_inuse_add(const struct net *net, int val)\n{\n}\n#endif\n\n\n \nstatic inline int __sk_prot_rehash(struct sock *sk)\n{\n\tsk->sk_prot->unhash(sk);\n\treturn sk->sk_prot->hash(sk);\n}\n\n \n#define SOCK_DESTROY_TIME (10*HZ)\n\n \n#define PROT_SOCK\t1024\n\n#define SHUTDOWN_MASK\t3\n#define RCV_SHUTDOWN\t1\n#define SEND_SHUTDOWN\t2\n\n#define SOCK_BINDADDR_LOCK\t4\n#define SOCK_BINDPORT_LOCK\t8\n\nstruct socket_alloc {\n\tstruct socket socket;\n\tstruct inode vfs_inode;\n};\n\nstatic inline struct socket *SOCKET_I(struct inode *inode)\n{\n\treturn &container_of(inode, struct socket_alloc, vfs_inode)->socket;\n}\n\nstatic inline struct inode *SOCK_INODE(struct socket *socket)\n{\n\treturn &container_of(socket, struct socket_alloc, socket)->vfs_inode;\n}\n\n \nint __sk_mem_raise_allocated(struct sock *sk, int size, int amt, int kind);\nint __sk_mem_schedule(struct sock *sk, int size, int kind);\nvoid __sk_mem_reduce_allocated(struct sock *sk, int amount);\nvoid __sk_mem_reclaim(struct sock *sk, int amount);\n\n#define SK_MEM_SEND\t0\n#define SK_MEM_RECV\t1\n\n \nstatic inline long sk_prot_mem_limits(const struct sock *sk, int index)\n{\n\treturn READ_ONCE(sk->sk_prot->sysctl_mem[index]);\n}\n\nstatic inline int sk_mem_pages(int amt)\n{\n\treturn (amt + PAGE_SIZE - 1) >> PAGE_SHIFT;\n}\n\nstatic inline bool sk_has_account(struct sock *sk)\n{\n\t \n\treturn !!sk->sk_prot->memory_allocated;\n}\n\nstatic inline bool sk_wmem_schedule(struct sock *sk, int size)\n{\n\tint delta;\n\n\tif (!sk_has_account(sk))\n\t\treturn true;\n\tdelta = size - sk->sk_forward_alloc;\n\treturn delta <= 0 || __sk_mem_schedule(sk, delta, SK_MEM_SEND);\n}\n\nstatic inline bool\nsk_rmem_schedule(struct sock *sk, struct sk_buff *skb, int size)\n{\n\tint delta;\n\n\tif (!sk_has_account(sk))\n\t\treturn true;\n\tdelta = size - sk->sk_forward_alloc;\n\treturn delta <= 0 || __sk_mem_schedule(sk, delta, SK_MEM_RECV) ||\n\t\tskb_pfmemalloc(skb);\n}\n\nstatic inline int sk_unused_reserved_mem(const struct sock *sk)\n{\n\tint unused_mem;\n\n\tif (likely(!sk->sk_reserved_mem))\n\t\treturn 0;\n\n\tunused_mem = sk->sk_reserved_mem - sk->sk_wmem_queued -\n\t\t\tatomic_read(&sk->sk_rmem_alloc);\n\n\treturn unused_mem > 0 ? unused_mem : 0;\n}\n\nstatic inline void sk_mem_reclaim(struct sock *sk)\n{\n\tint reclaimable;\n\n\tif (!sk_has_account(sk))\n\t\treturn;\n\n\treclaimable = sk->sk_forward_alloc - sk_unused_reserved_mem(sk);\n\n\tif (reclaimable >= (int)PAGE_SIZE)\n\t\t__sk_mem_reclaim(sk, reclaimable);\n}\n\nstatic inline void sk_mem_reclaim_final(struct sock *sk)\n{\n\tsk->sk_reserved_mem = 0;\n\tsk_mem_reclaim(sk);\n}\n\nstatic inline void sk_mem_charge(struct sock *sk, int size)\n{\n\tif (!sk_has_account(sk))\n\t\treturn;\n\tsk_forward_alloc_add(sk, -size);\n}\n\nstatic inline void sk_mem_uncharge(struct sock *sk, int size)\n{\n\tif (!sk_has_account(sk))\n\t\treturn;\n\tsk_forward_alloc_add(sk, size);\n\tsk_mem_reclaim(sk);\n}\n\n \n#define sock_lock_init_class_and_name(sk, sname, skey, name, key)\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tsk->sk_lock.owned = 0;\t\t\t\t\t\t\\\n\tinit_waitqueue_head(&sk->sk_lock.wq);\t\t\t\t\\\n\tspin_lock_init(&(sk)->sk_lock.slock);\t\t\t\t\\\n\tdebug_check_no_locks_freed((void *)&(sk)->sk_lock,\t\t\\\n\t\t\tsizeof((sk)->sk_lock));\t\t\t\t\\\n\tlockdep_set_class_and_name(&(sk)->sk_lock.slock,\t\t\\\n\t\t\t\t(skey), (sname));\t\t\t\t\\\n\tlockdep_init_map(&(sk)->sk_lock.dep_map, (name), (key), 0);\t\\\n} while (0)\n\nstatic inline bool lockdep_sock_is_held(const struct sock *sk)\n{\n\treturn lockdep_is_held(&sk->sk_lock) ||\n\t       lockdep_is_held(&sk->sk_lock.slock);\n}\n\nvoid lock_sock_nested(struct sock *sk, int subclass);\n\nstatic inline void lock_sock(struct sock *sk)\n{\n\tlock_sock_nested(sk, 0);\n}\n\nvoid __lock_sock(struct sock *sk);\nvoid __release_sock(struct sock *sk);\nvoid release_sock(struct sock *sk);\n\n \n#define bh_lock_sock(__sk)\tspin_lock(&((__sk)->sk_lock.slock))\n#define bh_lock_sock_nested(__sk) \\\n\t\t\t\tspin_lock_nested(&((__sk)->sk_lock.slock), \\\n\t\t\t\tSINGLE_DEPTH_NESTING)\n#define bh_unlock_sock(__sk)\tspin_unlock(&((__sk)->sk_lock.slock))\n\nbool __lock_sock_fast(struct sock *sk) __acquires(&sk->sk_lock.slock);\n\n \nstatic inline bool lock_sock_fast(struct sock *sk)\n{\n\t \n\tmutex_acquire(&sk->sk_lock.dep_map, 0, 0, _RET_IP_);\n\n\treturn __lock_sock_fast(sk);\n}\n\n \nstatic inline bool lock_sock_fast_nested(struct sock *sk)\n{\n\tmutex_acquire(&sk->sk_lock.dep_map, SINGLE_DEPTH_NESTING, 0, _RET_IP_);\n\n\treturn __lock_sock_fast(sk);\n}\n\n \nstatic inline void unlock_sock_fast(struct sock *sk, bool slow)\n\t__releases(&sk->sk_lock.slock)\n{\n\tif (slow) {\n\t\trelease_sock(sk);\n\t\t__release(&sk->sk_lock.slock);\n\t} else {\n\t\tmutex_release(&sk->sk_lock.dep_map, _RET_IP_);\n\t\tspin_unlock_bh(&sk->sk_lock.slock);\n\t}\n}\n\nvoid sockopt_lock_sock(struct sock *sk);\nvoid sockopt_release_sock(struct sock *sk);\nbool sockopt_ns_capable(struct user_namespace *ns, int cap);\nbool sockopt_capable(int cap);\n\n \n\nstatic inline void sock_owned_by_me(const struct sock *sk)\n{\n#ifdef CONFIG_LOCKDEP\n\tWARN_ON_ONCE(!lockdep_sock_is_held(sk) && debug_locks);\n#endif\n}\n\nstatic inline bool sock_owned_by_user(const struct sock *sk)\n{\n\tsock_owned_by_me(sk);\n\treturn sk->sk_lock.owned;\n}\n\nstatic inline bool sock_owned_by_user_nocheck(const struct sock *sk)\n{\n\treturn sk->sk_lock.owned;\n}\n\nstatic inline void sock_release_ownership(struct sock *sk)\n{\n\tif (sock_owned_by_user_nocheck(sk)) {\n\t\tsk->sk_lock.owned = 0;\n\n\t\t \n\t\tmutex_release(&sk->sk_lock.dep_map, _RET_IP_);\n\t}\n}\n\n \nstatic inline bool sock_allow_reclassification(const struct sock *csk)\n{\n\tstruct sock *sk = (struct sock *)csk;\n\n\treturn !sock_owned_by_user_nocheck(sk) &&\n\t\t!spin_is_locked(&sk->sk_lock.slock);\n}\n\nstruct sock *sk_alloc(struct net *net, int family, gfp_t priority,\n\t\t      struct proto *prot, int kern);\nvoid sk_free(struct sock *sk);\nvoid sk_destruct(struct sock *sk);\nstruct sock *sk_clone_lock(const struct sock *sk, const gfp_t priority);\nvoid sk_free_unlock_clone(struct sock *sk);\n\nstruct sk_buff *sock_wmalloc(struct sock *sk, unsigned long size, int force,\n\t\t\t     gfp_t priority);\nvoid __sock_wfree(struct sk_buff *skb);\nvoid sock_wfree(struct sk_buff *skb);\nstruct sk_buff *sock_omalloc(struct sock *sk, unsigned long size,\n\t\t\t     gfp_t priority);\nvoid skb_orphan_partial(struct sk_buff *skb);\nvoid sock_rfree(struct sk_buff *skb);\nvoid sock_efree(struct sk_buff *skb);\n#ifdef CONFIG_INET\nvoid sock_edemux(struct sk_buff *skb);\nvoid sock_pfree(struct sk_buff *skb);\n#else\n#define sock_edemux sock_efree\n#endif\n\nint sk_setsockopt(struct sock *sk, int level, int optname,\n\t\t  sockptr_t optval, unsigned int optlen);\nint sock_setsockopt(struct socket *sock, int level, int op,\n\t\t    sockptr_t optval, unsigned int optlen);\n\nint sk_getsockopt(struct sock *sk, int level, int optname,\n\t\t  sockptr_t optval, sockptr_t optlen);\nint sock_getsockopt(struct socket *sock, int level, int op,\n\t\t    char __user *optval, int __user *optlen);\nint sock_gettstamp(struct socket *sock, void __user *userstamp,\n\t\t   bool timeval, bool time32);\nstruct sk_buff *sock_alloc_send_pskb(struct sock *sk, unsigned long header_len,\n\t\t\t\t     unsigned long data_len, int noblock,\n\t\t\t\t     int *errcode, int max_page_order);\n\nstatic inline struct sk_buff *sock_alloc_send_skb(struct sock *sk,\n\t\t\t\t\t\t  unsigned long size,\n\t\t\t\t\t\t  int noblock, int *errcode)\n{\n\treturn sock_alloc_send_pskb(sk, size, 0, noblock, errcode, 0);\n}\n\nvoid *sock_kmalloc(struct sock *sk, int size, gfp_t priority);\nvoid sock_kfree_s(struct sock *sk, void *mem, int size);\nvoid sock_kzfree_s(struct sock *sk, void *mem, int size);\nvoid sk_send_sigurg(struct sock *sk);\n\nstatic inline void sock_replace_proto(struct sock *sk, struct proto *proto)\n{\n\tif (sk->sk_socket)\n\t\tclear_bit(SOCK_SUPPORT_ZC, &sk->sk_socket->flags);\n\tWRITE_ONCE(sk->sk_prot, proto);\n}\n\nstruct sockcm_cookie {\n\tu64 transmit_time;\n\tu32 mark;\n\tu32 tsflags;\n};\n\nstatic inline void sockcm_init(struct sockcm_cookie *sockc,\n\t\t\t       const struct sock *sk)\n{\n\t*sockc = (struct sockcm_cookie) {\n\t\t.tsflags = READ_ONCE(sk->sk_tsflags)\n\t};\n}\n\nint __sock_cmsg_send(struct sock *sk, struct cmsghdr *cmsg,\n\t\t     struct sockcm_cookie *sockc);\nint sock_cmsg_send(struct sock *sk, struct msghdr *msg,\n\t\t   struct sockcm_cookie *sockc);\n\n \nint sock_no_bind(struct socket *, struct sockaddr *, int);\nint sock_no_connect(struct socket *, struct sockaddr *, int, int);\nint sock_no_socketpair(struct socket *, struct socket *);\nint sock_no_accept(struct socket *, struct socket *, int, bool);\nint sock_no_getname(struct socket *, struct sockaddr *, int);\nint sock_no_ioctl(struct socket *, unsigned int, unsigned long);\nint sock_no_listen(struct socket *, int);\nint sock_no_shutdown(struct socket *, int);\nint sock_no_sendmsg(struct socket *, struct msghdr *, size_t);\nint sock_no_sendmsg_locked(struct sock *sk, struct msghdr *msg, size_t len);\nint sock_no_recvmsg(struct socket *, struct msghdr *, size_t, int);\nint sock_no_mmap(struct file *file, struct socket *sock,\n\t\t struct vm_area_struct *vma);\n\n \nint sock_common_getsockopt(struct socket *sock, int level, int optname,\n\t\t\t\t  char __user *optval, int __user *optlen);\nint sock_common_recvmsg(struct socket *sock, struct msghdr *msg, size_t size,\n\t\t\tint flags);\nint sock_common_setsockopt(struct socket *sock, int level, int optname,\n\t\t\t   sockptr_t optval, unsigned int optlen);\n\nvoid sk_common_release(struct sock *sk);\n\n \n\n \nvoid sock_init_data_uid(struct socket *sock, struct sock *sk, kuid_t uid);\n\n \nvoid sock_init_data(struct socket *sock, struct sock *sk);\n\n \n\n \nstatic inline void sock_put(struct sock *sk)\n{\n\tif (refcount_dec_and_test(&sk->sk_refcnt))\n\t\tsk_free(sk);\n}\n \nvoid sock_gen_put(struct sock *sk);\n\nint __sk_receive_skb(struct sock *sk, struct sk_buff *skb, const int nested,\n\t\t     unsigned int trim_cap, bool refcounted);\nstatic inline int sk_receive_skb(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t const int nested)\n{\n\treturn __sk_receive_skb(sk, skb, nested, 1, true);\n}\n\nstatic inline void sk_tx_queue_set(struct sock *sk, int tx_queue)\n{\n\t \n\tif (WARN_ON_ONCE((unsigned short)tx_queue >= USHRT_MAX))\n\t\treturn;\n\t \n\tWRITE_ONCE(sk->sk_tx_queue_mapping, tx_queue);\n}\n\n#define NO_QUEUE_MAPPING\tUSHRT_MAX\n\nstatic inline void sk_tx_queue_clear(struct sock *sk)\n{\n\t \n\tWRITE_ONCE(sk->sk_tx_queue_mapping, NO_QUEUE_MAPPING);\n}\n\nstatic inline int sk_tx_queue_get(const struct sock *sk)\n{\n\tif (sk) {\n\t\t \n\t\tint val = READ_ONCE(sk->sk_tx_queue_mapping);\n\n\t\tif (val != NO_QUEUE_MAPPING)\n\t\t\treturn val;\n\t}\n\treturn -1;\n}\n\nstatic inline void __sk_rx_queue_set(struct sock *sk,\n\t\t\t\t     const struct sk_buff *skb,\n\t\t\t\t     bool force_set)\n{\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\tif (skb_rx_queue_recorded(skb)) {\n\t\tu16 rx_queue = skb_get_rx_queue(skb);\n\n\t\tif (force_set ||\n\t\t    unlikely(READ_ONCE(sk->sk_rx_queue_mapping) != rx_queue))\n\t\t\tWRITE_ONCE(sk->sk_rx_queue_mapping, rx_queue);\n\t}\n#endif\n}\n\nstatic inline void sk_rx_queue_set(struct sock *sk, const struct sk_buff *skb)\n{\n\t__sk_rx_queue_set(sk, skb, true);\n}\n\nstatic inline void sk_rx_queue_update(struct sock *sk, const struct sk_buff *skb)\n{\n\t__sk_rx_queue_set(sk, skb, false);\n}\n\nstatic inline void sk_rx_queue_clear(struct sock *sk)\n{\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\tWRITE_ONCE(sk->sk_rx_queue_mapping, NO_QUEUE_MAPPING);\n#endif\n}\n\nstatic inline int sk_rx_queue_get(const struct sock *sk)\n{\n#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING\n\tif (sk) {\n\t\tint res = READ_ONCE(sk->sk_rx_queue_mapping);\n\n\t\tif (res != NO_QUEUE_MAPPING)\n\t\t\treturn res;\n\t}\n#endif\n\n\treturn -1;\n}\n\nstatic inline void sk_set_socket(struct sock *sk, struct socket *sock)\n{\n\tsk->sk_socket = sock;\n}\n\nstatic inline wait_queue_head_t *sk_sleep(struct sock *sk)\n{\n\tBUILD_BUG_ON(offsetof(struct socket_wq, wait) != 0);\n\treturn &rcu_dereference_raw(sk->sk_wq)->wait;\n}\n \nstatic inline void sock_orphan(struct sock *sk)\n{\n\twrite_lock_bh(&sk->sk_callback_lock);\n\tsock_set_flag(sk, SOCK_DEAD);\n\tsk_set_socket(sk, NULL);\n\tsk->sk_wq  = NULL;\n\twrite_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic inline void sock_graft(struct sock *sk, struct socket *parent)\n{\n\tWARN_ON(parent->sk);\n\twrite_lock_bh(&sk->sk_callback_lock);\n\trcu_assign_pointer(sk->sk_wq, &parent->wq);\n\tparent->sk = sk;\n\tsk_set_socket(sk, parent);\n\tsk->sk_uid = SOCK_INODE(parent)->i_uid;\n\tsecurity_sock_graft(sk, parent);\n\twrite_unlock_bh(&sk->sk_callback_lock);\n}\n\nkuid_t sock_i_uid(struct sock *sk);\nunsigned long __sock_i_ino(struct sock *sk);\nunsigned long sock_i_ino(struct sock *sk);\n\nstatic inline kuid_t sock_net_uid(const struct net *net, const struct sock *sk)\n{\n\treturn sk ? sk->sk_uid : make_kuid(net->user_ns, 0);\n}\n\nstatic inline u32 net_tx_rndhash(void)\n{\n\tu32 v = get_random_u32();\n\n\treturn v ?: 1;\n}\n\nstatic inline void sk_set_txhash(struct sock *sk)\n{\n\t \n\tWRITE_ONCE(sk->sk_txhash, net_tx_rndhash());\n}\n\nstatic inline bool sk_rethink_txhash(struct sock *sk)\n{\n\tif (sk->sk_txhash && sk->sk_txrehash == SOCK_TXREHASH_ENABLED) {\n\t\tsk_set_txhash(sk);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline struct dst_entry *\n__sk_dst_get(const struct sock *sk)\n{\n\treturn rcu_dereference_check(sk->sk_dst_cache,\n\t\t\t\t     lockdep_sock_is_held(sk));\n}\n\nstatic inline struct dst_entry *\nsk_dst_get(const struct sock *sk)\n{\n\tstruct dst_entry *dst;\n\n\trcu_read_lock();\n\tdst = rcu_dereference(sk->sk_dst_cache);\n\tif (dst && !rcuref_get(&dst->__rcuref))\n\t\tdst = NULL;\n\trcu_read_unlock();\n\treturn dst;\n}\n\nstatic inline void __dst_negative_advice(struct sock *sk)\n{\n\tstruct dst_entry *ndst, *dst = __sk_dst_get(sk);\n\n\tif (dst && dst->ops->negative_advice) {\n\t\tndst = dst->ops->negative_advice(dst);\n\n\t\tif (ndst != dst) {\n\t\t\trcu_assign_pointer(sk->sk_dst_cache, ndst);\n\t\t\tsk_tx_queue_clear(sk);\n\t\t\tWRITE_ONCE(sk->sk_dst_pending_confirm, 0);\n\t\t}\n\t}\n}\n\nstatic inline void dst_negative_advice(struct sock *sk)\n{\n\tsk_rethink_txhash(sk);\n\t__dst_negative_advice(sk);\n}\n\nstatic inline void\n__sk_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tstruct dst_entry *old_dst;\n\n\tsk_tx_queue_clear(sk);\n\tWRITE_ONCE(sk->sk_dst_pending_confirm, 0);\n\told_dst = rcu_dereference_protected(sk->sk_dst_cache,\n\t\t\t\t\t    lockdep_sock_is_held(sk));\n\trcu_assign_pointer(sk->sk_dst_cache, dst);\n\tdst_release(old_dst);\n}\n\nstatic inline void\nsk_dst_set(struct sock *sk, struct dst_entry *dst)\n{\n\tstruct dst_entry *old_dst;\n\n\tsk_tx_queue_clear(sk);\n\tWRITE_ONCE(sk->sk_dst_pending_confirm, 0);\n\told_dst = xchg((__force struct dst_entry **)&sk->sk_dst_cache, dst);\n\tdst_release(old_dst);\n}\n\nstatic inline void\n__sk_dst_reset(struct sock *sk)\n{\n\t__sk_dst_set(sk, NULL);\n}\n\nstatic inline void\nsk_dst_reset(struct sock *sk)\n{\n\tsk_dst_set(sk, NULL);\n}\n\nstruct dst_entry *__sk_dst_check(struct sock *sk, u32 cookie);\n\nstruct dst_entry *sk_dst_check(struct sock *sk, u32 cookie);\n\nstatic inline void sk_dst_confirm(struct sock *sk)\n{\n\tif (!READ_ONCE(sk->sk_dst_pending_confirm))\n\t\tWRITE_ONCE(sk->sk_dst_pending_confirm, 1);\n}\n\nstatic inline void sock_confirm_neigh(struct sk_buff *skb, struct neighbour *n)\n{\n\tif (skb_get_dst_pending_confirm(skb)) {\n\t\tstruct sock *sk = skb->sk;\n\n\t\tif (sk && READ_ONCE(sk->sk_dst_pending_confirm))\n\t\t\tWRITE_ONCE(sk->sk_dst_pending_confirm, 0);\n\t\tneigh_confirm(n);\n\t}\n}\n\nbool sk_mc_loop(struct sock *sk);\n\nstatic inline bool sk_can_gso(const struct sock *sk)\n{\n\treturn net_gso_ok(sk->sk_route_caps, sk->sk_gso_type);\n}\n\nvoid sk_setup_caps(struct sock *sk, struct dst_entry *dst);\n\nstatic inline void sk_gso_disable(struct sock *sk)\n{\n\tsk->sk_gso_disabled = 1;\n\tsk->sk_route_caps &= ~NETIF_F_GSO_MASK;\n}\n\nstatic inline int skb_do_copy_data_nocache(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t\t   struct iov_iter *from, char *to,\n\t\t\t\t\t   int copy, int offset)\n{\n\tif (skb->ip_summed == CHECKSUM_NONE) {\n\t\t__wsum csum = 0;\n\t\tif (!csum_and_copy_from_iter_full(to, copy, &csum, from))\n\t\t\treturn -EFAULT;\n\t\tskb->csum = csum_block_add(skb->csum, csum, offset);\n\t} else if (sk->sk_route_caps & NETIF_F_NOCACHE_COPY) {\n\t\tif (!copy_from_iter_full_nocache(to, copy, from))\n\t\t\treturn -EFAULT;\n\t} else if (!copy_from_iter_full(to, copy, from))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic inline int skb_add_data_nocache(struct sock *sk, struct sk_buff *skb,\n\t\t\t\t       struct iov_iter *from, int copy)\n{\n\tint err, offset = skb->len;\n\n\terr = skb_do_copy_data_nocache(sk, skb, from, skb_put(skb, copy),\n\t\t\t\t       copy, offset);\n\tif (err)\n\t\t__skb_trim(skb, offset);\n\n\treturn err;\n}\n\nstatic inline int skb_copy_to_page_nocache(struct sock *sk, struct iov_iter *from,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct page *page,\n\t\t\t\t\t   int off, int copy)\n{\n\tint err;\n\n\terr = skb_do_copy_data_nocache(sk, skb, from, page_address(page) + off,\n\t\t\t\t       copy, skb->len);\n\tif (err)\n\t\treturn err;\n\n\tskb_len_add(skb, copy);\n\tsk_wmem_queued_add(sk, copy);\n\tsk_mem_charge(sk, copy);\n\treturn 0;\n}\n\n \nstatic inline int sk_wmem_alloc_get(const struct sock *sk)\n{\n\treturn refcount_read(&sk->sk_wmem_alloc) - 1;\n}\n\n \nstatic inline int sk_rmem_alloc_get(const struct sock *sk)\n{\n\treturn atomic_read(&sk->sk_rmem_alloc);\n}\n\n \nstatic inline bool sk_has_allocations(const struct sock *sk)\n{\n\treturn sk_wmem_alloc_get(sk) || sk_rmem_alloc_get(sk);\n}\n\n \nstatic inline bool skwq_has_sleeper(struct socket_wq *wq)\n{\n\treturn wq && wq_has_sleeper(&wq->wait);\n}\n\n \nstatic inline void sock_poll_wait(struct file *filp, struct socket *sock,\n\t\t\t\t  poll_table *p)\n{\n\tif (!poll_does_not_wait(p)) {\n\t\tpoll_wait(filp, &sock->wq.wait, p);\n\t\t \n\t\tsmp_mb();\n\t}\n}\n\nstatic inline void skb_set_hash_from_sk(struct sk_buff *skb, struct sock *sk)\n{\n\t \n\tu32 txhash = READ_ONCE(sk->sk_txhash);\n\n\tif (txhash) {\n\t\tskb->l4_hash = 1;\n\t\tskb->hash = txhash;\n\t}\n}\n\nvoid skb_set_owner_w(struct sk_buff *skb, struct sock *sk);\n\n \nstatic inline void skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\n{\n\tskb_orphan(skb);\n\tskb->sk = sk;\n\tskb->destructor = sock_rfree;\n\tatomic_add(skb->truesize, &sk->sk_rmem_alloc);\n\tsk_mem_charge(sk, skb->truesize);\n}\n\nstatic inline __must_check bool skb_set_owner_sk_safe(struct sk_buff *skb, struct sock *sk)\n{\n\tif (sk && refcount_inc_not_zero(&sk->sk_refcnt)) {\n\t\tskb_orphan(skb);\n\t\tskb->destructor = sock_efree;\n\t\tskb->sk = sk;\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline struct sk_buff *skb_clone_and_charge_r(struct sk_buff *skb, struct sock *sk)\n{\n\tskb = skb_clone(skb, sk_gfp_mask(sk, GFP_ATOMIC));\n\tif (skb) {\n\t\tif (sk_rmem_schedule(sk, skb, skb->truesize)) {\n\t\t\tskb_set_owner_r(skb, sk);\n\t\t\treturn skb;\n\t\t}\n\t\t__kfree_skb(skb);\n\t}\n\treturn NULL;\n}\n\nstatic inline void skb_prepare_for_gro(struct sk_buff *skb)\n{\n\tif (skb->destructor != sock_wfree) {\n\t\tskb_orphan(skb);\n\t\treturn;\n\t}\n\tskb->slow_gro = 1;\n}\n\nvoid sk_reset_timer(struct sock *sk, struct timer_list *timer,\n\t\t    unsigned long expires);\n\nvoid sk_stop_timer(struct sock *sk, struct timer_list *timer);\n\nvoid sk_stop_timer_sync(struct sock *sk, struct timer_list *timer);\n\nint __sk_queue_drop_skb(struct sock *sk, struct sk_buff_head *sk_queue,\n\t\t\tstruct sk_buff *skb, unsigned int flags,\n\t\t\tvoid (*destructor)(struct sock *sk,\n\t\t\t\t\t   struct sk_buff *skb));\nint __sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb);\n\nint sock_queue_rcv_skb_reason(struct sock *sk, struct sk_buff *skb,\n\t\t\t      enum skb_drop_reason *reason);\n\nstatic inline int sock_queue_rcv_skb(struct sock *sk, struct sk_buff *skb)\n{\n\treturn sock_queue_rcv_skb_reason(sk, skb, NULL);\n}\n\nint sock_queue_err_skb(struct sock *sk, struct sk_buff *skb);\nstruct sk_buff *sock_dequeue_err_skb(struct sock *sk);\n\n \n\nstatic inline int sock_error(struct sock *sk)\n{\n\tint err;\n\n\t \n\tif (likely(data_race(!sk->sk_err)))\n\t\treturn 0;\n\n\terr = xchg(&sk->sk_err, 0);\n\treturn -err;\n}\n\nvoid sk_error_report(struct sock *sk);\n\nstatic inline unsigned long sock_wspace(struct sock *sk)\n{\n\tint amt = 0;\n\n\tif (!(sk->sk_shutdown & SEND_SHUTDOWN)) {\n\t\tamt = sk->sk_sndbuf - refcount_read(&sk->sk_wmem_alloc);\n\t\tif (amt < 0)\n\t\t\tamt = 0;\n\t}\n\treturn amt;\n}\n\n \nstatic inline void sk_set_bit(int nr, struct sock *sk)\n{\n\tif ((nr == SOCKWQ_ASYNC_NOSPACE || nr == SOCKWQ_ASYNC_WAITDATA) &&\n\t    !sock_flag(sk, SOCK_FASYNC))\n\t\treturn;\n\n\tset_bit(nr, &sk->sk_wq_raw->flags);\n}\n\nstatic inline void sk_clear_bit(int nr, struct sock *sk)\n{\n\tif ((nr == SOCKWQ_ASYNC_NOSPACE || nr == SOCKWQ_ASYNC_WAITDATA) &&\n\t    !sock_flag(sk, SOCK_FASYNC))\n\t\treturn;\n\n\tclear_bit(nr, &sk->sk_wq_raw->flags);\n}\n\nstatic inline void sk_wake_async(const struct sock *sk, int how, int band)\n{\n\tif (sock_flag(sk, SOCK_FASYNC)) {\n\t\trcu_read_lock();\n\t\tsock_wake_async(rcu_dereference(sk->sk_wq), how, band);\n\t\trcu_read_unlock();\n\t}\n}\n\n \n#define TCP_SKB_MIN_TRUESIZE\t(2048 + SKB_DATA_ALIGN(sizeof(struct sk_buff)))\n\n#define SOCK_MIN_SNDBUF\t\t(TCP_SKB_MIN_TRUESIZE * 2)\n#define SOCK_MIN_RCVBUF\t\t TCP_SKB_MIN_TRUESIZE\n\nstatic inline void sk_stream_moderate_sndbuf(struct sock *sk)\n{\n\tu32 val;\n\n\tif (sk->sk_userlocks & SOCK_SNDBUF_LOCK)\n\t\treturn;\n\n\tval = min(sk->sk_sndbuf, sk->sk_wmem_queued >> 1);\n\tval = max_t(u32, val, sk_unused_reserved_mem(sk));\n\n\tWRITE_ONCE(sk->sk_sndbuf, max_t(u32, val, SOCK_MIN_SNDBUF));\n}\n\n \nstatic inline struct page_frag *sk_page_frag(struct sock *sk)\n{\n\tif (sk->sk_use_task_frag)\n\t\treturn &current->task_frag;\n\n\treturn &sk->sk_frag;\n}\n\nbool sk_page_frag_refill(struct sock *sk, struct page_frag *pfrag);\n\n \nstatic inline bool sock_writeable(const struct sock *sk)\n{\n\treturn refcount_read(&sk->sk_wmem_alloc) < (READ_ONCE(sk->sk_sndbuf) >> 1);\n}\n\nstatic inline gfp_t gfp_any(void)\n{\n\treturn in_softirq() ? GFP_ATOMIC : GFP_KERNEL;\n}\n\nstatic inline gfp_t gfp_memcg_charge(void)\n{\n\treturn in_softirq() ? GFP_ATOMIC : GFP_KERNEL;\n}\n\nstatic inline long sock_rcvtimeo(const struct sock *sk, bool noblock)\n{\n\treturn noblock ? 0 : sk->sk_rcvtimeo;\n}\n\nstatic inline long sock_sndtimeo(const struct sock *sk, bool noblock)\n{\n\treturn noblock ? 0 : sk->sk_sndtimeo;\n}\n\nstatic inline int sock_rcvlowat(const struct sock *sk, int waitall, int len)\n{\n\tint v = waitall ? len : min_t(int, READ_ONCE(sk->sk_rcvlowat), len);\n\n\treturn v ?: 1;\n}\n\n \nstatic inline int sock_intr_errno(long timeo)\n{\n\treturn timeo == MAX_SCHEDULE_TIMEOUT ? -ERESTARTSYS : -EINTR;\n}\n\nstruct sock_skb_cb {\n\tu32 dropcount;\n};\n\n \n#define SOCK_SKB_CB_OFFSET ((sizeof_field(struct sk_buff, cb) - \\\n\t\t\t    sizeof(struct sock_skb_cb)))\n\n#define SOCK_SKB_CB(__skb) ((struct sock_skb_cb *)((__skb)->cb + \\\n\t\t\t    SOCK_SKB_CB_OFFSET))\n\n#define sock_skb_cb_check_size(size) \\\n\tBUILD_BUG_ON((size) > SOCK_SKB_CB_OFFSET)\n\nstatic inline void\nsock_skb_set_dropcount(const struct sock *sk, struct sk_buff *skb)\n{\n\tSOCK_SKB_CB(skb)->dropcount = sock_flag(sk, SOCK_RXQ_OVFL) ?\n\t\t\t\t\t\tatomic_read(&sk->sk_drops) : 0;\n}\n\nstatic inline void sk_drops_add(struct sock *sk, const struct sk_buff *skb)\n{\n\tint segs = max_t(u16, 1, skb_shinfo(skb)->gso_segs);\n\n\tatomic_add(segs, &sk->sk_drops);\n}\n\nstatic inline ktime_t sock_read_timestamp(struct sock *sk)\n{\n#if BITS_PER_LONG==32\n\tunsigned int seq;\n\tktime_t kt;\n\n\tdo {\n\t\tseq = read_seqbegin(&sk->sk_stamp_seq);\n\t\tkt = sk->sk_stamp;\n\t} while (read_seqretry(&sk->sk_stamp_seq, seq));\n\n\treturn kt;\n#else\n\treturn READ_ONCE(sk->sk_stamp);\n#endif\n}\n\nstatic inline void sock_write_timestamp(struct sock *sk, ktime_t kt)\n{\n#if BITS_PER_LONG==32\n\twrite_seqlock(&sk->sk_stamp_seq);\n\tsk->sk_stamp = kt;\n\twrite_sequnlock(&sk->sk_stamp_seq);\n#else\n\tWRITE_ONCE(sk->sk_stamp, kt);\n#endif\n}\n\nvoid __sock_recv_timestamp(struct msghdr *msg, struct sock *sk,\n\t\t\t   struct sk_buff *skb);\nvoid __sock_recv_wifi_status(struct msghdr *msg, struct sock *sk,\n\t\t\t     struct sk_buff *skb);\n\nstatic inline void\nsock_recv_timestamp(struct msghdr *msg, struct sock *sk, struct sk_buff *skb)\n{\n\tstruct skb_shared_hwtstamps *hwtstamps = skb_hwtstamps(skb);\n\tu32 tsflags = READ_ONCE(sk->sk_tsflags);\n\tktime_t kt = skb->tstamp;\n\t \n\tif (sock_flag(sk, SOCK_RCVTSTAMP) ||\n\t    (tsflags & SOF_TIMESTAMPING_RX_SOFTWARE) ||\n\t    (kt && tsflags & SOF_TIMESTAMPING_SOFTWARE) ||\n\t    (hwtstamps->hwtstamp &&\n\t     (tsflags & SOF_TIMESTAMPING_RAW_HARDWARE)))\n\t\t__sock_recv_timestamp(msg, sk, skb);\n\telse\n\t\tsock_write_timestamp(sk, kt);\n\n\tif (sock_flag(sk, SOCK_WIFI_STATUS) && skb_wifi_acked_valid(skb))\n\t\t__sock_recv_wifi_status(msg, sk, skb);\n}\n\nvoid __sock_recv_cmsgs(struct msghdr *msg, struct sock *sk,\n\t\t       struct sk_buff *skb);\n\n#define SK_DEFAULT_STAMP (-1L * NSEC_PER_SEC)\nstatic inline void sock_recv_cmsgs(struct msghdr *msg, struct sock *sk,\n\t\t\t\t   struct sk_buff *skb)\n{\n#define FLAGS_RECV_CMSGS ((1UL << SOCK_RXQ_OVFL)\t\t\t| \\\n\t\t\t   (1UL << SOCK_RCVTSTAMP)\t\t\t| \\\n\t\t\t   (1UL << SOCK_RCVMARK))\n#define TSFLAGS_ANY\t  (SOF_TIMESTAMPING_SOFTWARE\t\t\t| \\\n\t\t\t   SOF_TIMESTAMPING_RAW_HARDWARE)\n\n\tif (sk->sk_flags & FLAGS_RECV_CMSGS ||\n\t    READ_ONCE(sk->sk_tsflags) & TSFLAGS_ANY)\n\t\t__sock_recv_cmsgs(msg, sk, skb);\n\telse if (unlikely(sock_flag(sk, SOCK_TIMESTAMP)))\n\t\tsock_write_timestamp(sk, skb->tstamp);\n\telse if (unlikely(sock_read_timestamp(sk) == SK_DEFAULT_STAMP))\n\t\tsock_write_timestamp(sk, 0);\n}\n\nvoid __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags);\n\n \nstatic inline void _sock_tx_timestamp(struct sock *sk, __u16 tsflags,\n\t\t\t\t      __u8 *tx_flags, __u32 *tskey)\n{\n\tif (unlikely(tsflags)) {\n\t\t__sock_tx_timestamp(tsflags, tx_flags);\n\t\tif (tsflags & SOF_TIMESTAMPING_OPT_ID && tskey &&\n\t\t    tsflags & SOF_TIMESTAMPING_TX_RECORD_MASK)\n\t\t\t*tskey = atomic_inc_return(&sk->sk_tskey) - 1;\n\t}\n\tif (unlikely(sock_flag(sk, SOCK_WIFI_STATUS)))\n\t\t*tx_flags |= SKBTX_WIFI_STATUS;\n}\n\nstatic inline void sock_tx_timestamp(struct sock *sk, __u16 tsflags,\n\t\t\t\t     __u8 *tx_flags)\n{\n\t_sock_tx_timestamp(sk, tsflags, tx_flags, NULL);\n}\n\nstatic inline void skb_setup_tx_timestamp(struct sk_buff *skb, __u16 tsflags)\n{\n\t_sock_tx_timestamp(skb->sk, tsflags, &skb_shinfo(skb)->tx_flags,\n\t\t\t   &skb_shinfo(skb)->tskey);\n}\n\nstatic inline bool sk_is_tcp(const struct sock *sk)\n{\n\treturn sk->sk_type == SOCK_STREAM && sk->sk_protocol == IPPROTO_TCP;\n}\n\nstatic inline bool sk_is_stream_unix(const struct sock *sk)\n{\n\treturn sk->sk_family == AF_UNIX && sk->sk_type == SOCK_STREAM;\n}\n\n \nstatic inline void sk_eat_skb(struct sock *sk, struct sk_buff *skb)\n{\n\t__skb_unlink(skb, &sk->sk_receive_queue);\n\t__kfree_skb(skb);\n}\n\nstatic inline bool\nskb_sk_is_prefetched(struct sk_buff *skb)\n{\n#ifdef CONFIG_INET\n\treturn skb->destructor == sock_pfree;\n#else\n\treturn false;\n#endif  \n}\n\n \nstatic inline bool sk_fullsock(const struct sock *sk)\n{\n\treturn (1 << sk->sk_state) & ~(TCPF_TIME_WAIT | TCPF_NEW_SYN_RECV);\n}\n\nstatic inline bool\nsk_is_refcounted(struct sock *sk)\n{\n\t \n\treturn !sk_fullsock(sk) || !sock_flag(sk, SOCK_RCU_FREE);\n}\n\n \nstatic inline struct sock *\nskb_steal_sock(struct sk_buff *skb, bool *refcounted, bool *prefetched)\n{\n\tif (skb->sk) {\n\t\tstruct sock *sk = skb->sk;\n\n\t\t*refcounted = true;\n\t\t*prefetched = skb_sk_is_prefetched(skb);\n\t\tif (*prefetched)\n\t\t\t*refcounted = sk_is_refcounted(sk);\n\t\tskb->destructor = NULL;\n\t\tskb->sk = NULL;\n\t\treturn sk;\n\t}\n\t*prefetched = false;\n\t*refcounted = false;\n\treturn NULL;\n}\n\n \nstatic inline struct sk_buff *sk_validate_xmit_skb(struct sk_buff *skb,\n\t\t\t\t\t\t   struct net_device *dev)\n{\n#ifdef CONFIG_SOCK_VALIDATE_XMIT\n\tstruct sock *sk = skb->sk;\n\n\tif (sk && sk_fullsock(sk) && sk->sk_validate_xmit_skb) {\n\t\tskb = sk->sk_validate_xmit_skb(sk, dev, skb);\n#ifdef CONFIG_TLS_DEVICE\n\t} else if (unlikely(skb->decrypted)) {\n\t\tpr_warn_ratelimited(\"unencrypted skb with no associated socket - dropping\\n\");\n\t\tkfree_skb(skb);\n\t\tskb = NULL;\n#endif\n\t}\n#endif\n\n\treturn skb;\n}\n\n \nstatic inline bool sk_listener(const struct sock *sk)\n{\n\treturn (1 << sk->sk_state) & (TCPF_LISTEN | TCPF_NEW_SYN_RECV);\n}\n\nvoid sock_enable_timestamp(struct sock *sk, enum sock_flags flag);\nint sock_recv_errqueue(struct sock *sk, struct msghdr *msg, int len, int level,\n\t\t       int type);\n\nbool sk_ns_capable(const struct sock *sk,\n\t\t   struct user_namespace *user_ns, int cap);\nbool sk_capable(const struct sock *sk, int cap);\nbool sk_net_capable(const struct sock *sk, int cap);\n\nvoid sk_get_meminfo(const struct sock *sk, u32 *meminfo);\n\n \n#define _SK_MEM_PACKETS\t\t256\n#define _SK_MEM_OVERHEAD\tSKB_TRUESIZE(256)\n#define SK_WMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n#define SK_RMEM_MAX\t\t(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)\n\nextern __u32 sysctl_wmem_max;\nextern __u32 sysctl_rmem_max;\n\nextern int sysctl_tstamp_allow_data;\nextern int sysctl_optmem_max;\n\nextern __u32 sysctl_wmem_default;\nextern __u32 sysctl_rmem_default;\n\n#define SKB_FRAG_PAGE_ORDER\tget_order(32768)\nDECLARE_STATIC_KEY_FALSE(net_high_order_alloc_disable_key);\n\nstatic inline int sk_get_wmem0(const struct sock *sk, const struct proto *proto)\n{\n\t \n\tif (proto->sysctl_wmem_offset)\n\t\treturn READ_ONCE(*(int *)((void *)sock_net(sk) + proto->sysctl_wmem_offset));\n\n\treturn READ_ONCE(*proto->sysctl_wmem);\n}\n\nstatic inline int sk_get_rmem0(const struct sock *sk, const struct proto *proto)\n{\n\t \n\tif (proto->sysctl_rmem_offset)\n\t\treturn READ_ONCE(*(int *)((void *)sock_net(sk) + proto->sysctl_rmem_offset));\n\n\treturn READ_ONCE(*proto->sysctl_rmem);\n}\n\n \nstatic inline void sk_pacing_shift_update(struct sock *sk, int val)\n{\n\tif (!sk || !sk_fullsock(sk) || READ_ONCE(sk->sk_pacing_shift) == val)\n\t\treturn;\n\tWRITE_ONCE(sk->sk_pacing_shift, val);\n}\n\n \nstatic inline bool sk_dev_equal_l3scope(struct sock *sk, int dif)\n{\n\tint bound_dev_if = READ_ONCE(sk->sk_bound_dev_if);\n\tint mdif;\n\n\tif (!bound_dev_if || bound_dev_if == dif)\n\t\treturn true;\n\n\tmdif = l3mdev_master_ifindex_by_index(sock_net(sk), dif);\n\tif (mdif && mdif == bound_dev_if)\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid sock_def_readable(struct sock *sk);\n\nint sock_bindtoindex(struct sock *sk, int ifindex, bool lock_sk);\nvoid sock_set_timestamp(struct sock *sk, int optname, bool valbool);\nint sock_set_timestamping(struct sock *sk, int optname,\n\t\t\t  struct so_timestamping timestamping);\n\nvoid sock_enable_timestamps(struct sock *sk);\nvoid sock_no_linger(struct sock *sk);\nvoid sock_set_keepalive(struct sock *sk);\nvoid sock_set_priority(struct sock *sk, u32 priority);\nvoid sock_set_rcvbuf(struct sock *sk, int val);\nvoid sock_set_mark(struct sock *sk, u32 val);\nvoid sock_set_reuseaddr(struct sock *sk);\nvoid sock_set_reuseport(struct sock *sk);\nvoid sock_set_sndtimeo(struct sock *sk, s64 secs);\n\nint sock_bind_add(struct sock *sk, struct sockaddr *addr, int addr_len);\n\nint sock_get_timeout(long timeo, void *optval, bool old_timeval);\nint sock_copy_user_timeval(struct __kernel_sock_timeval *tv,\n\t\t\t   sockptr_t optval, int optlen, bool old_timeval);\n\nint sock_ioctl_inout(struct sock *sk, unsigned int cmd,\n\t\t     void __user *arg, void *karg, size_t size);\nint sk_ioctl(struct sock *sk, unsigned int cmd, void __user *arg);\nstatic inline bool sk_is_readable(struct sock *sk)\n{\n\tif (sk->sk_prot->sock_is_readable)\n\t\treturn sk->sk_prot->sock_is_readable(sk);\n\treturn false;\n}\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}