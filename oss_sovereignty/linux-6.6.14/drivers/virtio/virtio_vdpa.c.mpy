{
  "module_name": "virtio_vdpa.c",
  "hash_id": "e04a764d1f9cbfa30ed18c299f71ffc4ebb69d16984b7f73b5d88940ede49b5c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/virtio/virtio_vdpa.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/device.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/uuid.h>\n#include <linux/group_cpus.h>\n#include <linux/virtio.h>\n#include <linux/vdpa.h>\n#include <linux/virtio_config.h>\n#include <linux/virtio_ring.h>\n\n#define MOD_VERSION  \"0.1\"\n#define MOD_AUTHOR   \"Jason Wang <jasowang@redhat.com>\"\n#define MOD_DESC     \"vDPA bus driver for virtio devices\"\n#define MOD_LICENSE  \"GPL v2\"\n\nstruct virtio_vdpa_device {\n\tstruct virtio_device vdev;\n\tstruct vdpa_device *vdpa;\n\tu64 features;\n\n\t \n\tspinlock_t lock;\n\t \n\tstruct list_head virtqueues;\n};\n\nstruct virtio_vdpa_vq_info {\n\t \n\tstruct virtqueue *vq;\n\n\t \n\tstruct list_head node;\n};\n\nstatic inline struct virtio_vdpa_device *\nto_virtio_vdpa_device(struct virtio_device *dev)\n{\n\treturn container_of(dev, struct virtio_vdpa_device, vdev);\n}\n\nstatic struct vdpa_device *vd_get_vdpa(struct virtio_device *vdev)\n{\n\treturn to_virtio_vdpa_device(vdev)->vdpa;\n}\n\nstatic void virtio_vdpa_get(struct virtio_device *vdev, unsigned int offset,\n\t\t\t    void *buf, unsigned int len)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\n\tvdpa_get_config(vdpa, offset, buf, len);\n}\n\nstatic void virtio_vdpa_set(struct virtio_device *vdev, unsigned int offset,\n\t\t\t    const void *buf, unsigned int len)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\n\tvdpa_set_config(vdpa, offset, buf, len);\n}\n\nstatic u32 virtio_vdpa_generation(struct virtio_device *vdev)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (ops->get_generation)\n\t\treturn ops->get_generation(vdpa);\n\n\treturn 0;\n}\n\nstatic u8 virtio_vdpa_get_status(struct virtio_device *vdev)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\treturn ops->get_status(vdpa);\n}\n\nstatic void virtio_vdpa_set_status(struct virtio_device *vdev, u8 status)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\n\treturn vdpa_set_status(vdpa, status);\n}\n\nstatic void virtio_vdpa_reset(struct virtio_device *vdev)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\n\tvdpa_reset(vdpa);\n}\n\nstatic bool virtio_vdpa_notify(struct virtqueue *vq)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vq->vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tops->kick_vq(vdpa, vq->index);\n\n\treturn true;\n}\n\nstatic bool virtio_vdpa_notify_with_data(struct virtqueue *vq)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vq->vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu32 data = vring_notification_data(vq);\n\n\tops->kick_vq_with_data(vdpa, data);\n\n\treturn true;\n}\n\nstatic irqreturn_t virtio_vdpa_config_cb(void *private)\n{\n\tstruct virtio_vdpa_device *vd_dev = private;\n\n\tvirtio_config_changed(&vd_dev->vdev);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t virtio_vdpa_virtqueue_cb(void *private)\n{\n\tstruct virtio_vdpa_vq_info *info = private;\n\n\treturn vring_interrupt(0, info->vq);\n}\n\nstatic struct virtqueue *\nvirtio_vdpa_setup_vq(struct virtio_device *vdev, unsigned int index,\n\t\t     void (*callback)(struct virtqueue *vq),\n\t\t     const char *name, bool ctx)\n{\n\tstruct virtio_vdpa_device *vd_dev = to_virtio_vdpa_device(vdev);\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tstruct device *dma_dev;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct virtio_vdpa_vq_info *info;\n\tbool (*notify)(struct virtqueue *vq) = virtio_vdpa_notify;\n\tstruct vdpa_callback cb;\n\tstruct virtqueue *vq;\n\tu64 desc_addr, driver_addr, device_addr;\n\t \n\tstruct vdpa_vq_state state = {0};\n\tunsigned long flags;\n\tu32 align, max_num, min_num = 1;\n\tbool may_reduce_num = true;\n\tint err;\n\n\tif (!name)\n\t\treturn NULL;\n\n\tif (index >= vdpa->nvqs)\n\t\treturn ERR_PTR(-ENOENT);\n\n\t \n\tif (__virtio_test_bit(vdev, VIRTIO_F_NOTIFICATION_DATA)) {\n\t\tif (ops->kick_vq_with_data)\n\t\t\tnotify = virtio_vdpa_notify_with_data;\n\t\telse\n\t\t\t__virtio_clear_bit(vdev, VIRTIO_F_NOTIFICATION_DATA);\n\t}\n\n\t \n\tif (ops->get_vq_ready(vdpa, index))\n\t\treturn ERR_PTR(-ENOENT);\n\n\t \n\tinfo = kmalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmax_num = ops->get_vq_num_max(vdpa);\n\tif (max_num == 0) {\n\t\terr = -ENOENT;\n\t\tgoto error_new_virtqueue;\n\t}\n\n\tif (ops->get_vq_num_min)\n\t\tmin_num = ops->get_vq_num_min(vdpa);\n\n\tmay_reduce_num = (max_num == min_num) ? false : true;\n\n\t \n\talign = ops->get_vq_align(vdpa);\n\n\tif (ops->get_vq_dma_dev)\n\t\tdma_dev = ops->get_vq_dma_dev(vdpa, index);\n\telse\n\t\tdma_dev = vdpa_get_dma_dev(vdpa);\n\tvq = vring_create_virtqueue_dma(index, max_num, align, vdev,\n\t\t\t\t\ttrue, may_reduce_num, ctx,\n\t\t\t\t\tnotify, callback, name, dma_dev);\n\tif (!vq) {\n\t\terr = -ENOMEM;\n\t\tgoto error_new_virtqueue;\n\t}\n\n\tvq->num_max = max_num;\n\n\t \n\tcb.callback = callback ? virtio_vdpa_virtqueue_cb : NULL;\n\tcb.private = info;\n\tcb.trigger = NULL;\n\tops->set_vq_cb(vdpa, index, &cb);\n\tops->set_vq_num(vdpa, index, virtqueue_get_vring_size(vq));\n\n\tdesc_addr = virtqueue_get_desc_addr(vq);\n\tdriver_addr = virtqueue_get_avail_addr(vq);\n\tdevice_addr = virtqueue_get_used_addr(vq);\n\n\tif (ops->set_vq_address(vdpa, index,\n\t\t\t\tdesc_addr, driver_addr,\n\t\t\t\tdevice_addr)) {\n\t\terr = -EINVAL;\n\t\tgoto err_vq;\n\t}\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED)) {\n\t\tstruct vdpa_vq_state_packed *s = &state.packed;\n\n\t\ts->last_avail_counter = 1;\n\t\ts->last_avail_idx = 0;\n\t\ts->last_used_counter = 1;\n\t\ts->last_used_idx = 0;\n\t}\n\terr = ops->set_vq_state(vdpa, index, &state);\n\tif (err)\n\t\tgoto err_vq;\n\n\tops->set_vq_ready(vdpa, index, 1);\n\n\tvq->priv = info;\n\tinfo->vq = vq;\n\n\tspin_lock_irqsave(&vd_dev->lock, flags);\n\tlist_add(&info->node, &vd_dev->virtqueues);\n\tspin_unlock_irqrestore(&vd_dev->lock, flags);\n\n\treturn vq;\n\nerr_vq:\n\tvring_del_virtqueue(vq);\nerror_new_virtqueue:\n\tops->set_vq_ready(vdpa, index, 0);\n\t \n\tWARN_ON(ops->get_vq_ready(vdpa, index));\n\tkfree(info);\n\treturn ERR_PTR(err);\n}\n\nstatic void virtio_vdpa_del_vq(struct virtqueue *vq)\n{\n\tstruct virtio_vdpa_device *vd_dev = to_virtio_vdpa_device(vq->vdev);\n\tstruct vdpa_device *vdpa = vd_dev->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct virtio_vdpa_vq_info *info = vq->priv;\n\tunsigned int index = vq->index;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&vd_dev->lock, flags);\n\tlist_del(&info->node);\n\tspin_unlock_irqrestore(&vd_dev->lock, flags);\n\n\t \n\tops->set_vq_ready(vdpa, index, 0);\n\n\tvring_del_virtqueue(vq);\n\n\tkfree(info);\n}\n\nstatic void virtio_vdpa_del_vqs(struct virtio_device *vdev)\n{\n\tstruct virtqueue *vq, *n;\n\n\tlist_for_each_entry_safe(vq, n, &vdev->vqs, list)\n\t\tvirtio_vdpa_del_vq(vq);\n}\n\nstatic void default_calc_sets(struct irq_affinity *affd, unsigned int affvecs)\n{\n\taffd->nr_sets = 1;\n\taffd->set_size[0] = affvecs;\n}\n\nstatic struct cpumask *\ncreate_affinity_masks(unsigned int nvecs, struct irq_affinity *affd)\n{\n\tunsigned int affvecs = 0, curvec, usedvecs, i;\n\tstruct cpumask *masks = NULL;\n\n\tif (nvecs > affd->pre_vectors + affd->post_vectors)\n\t\taffvecs = nvecs - affd->pre_vectors - affd->post_vectors;\n\n\tif (!affd->calc_sets)\n\t\taffd->calc_sets = default_calc_sets;\n\n\taffd->calc_sets(affd, affvecs);\n\n\tif (!affvecs)\n\t\treturn NULL;\n\n\tmasks = kcalloc(nvecs, sizeof(*masks), GFP_KERNEL);\n\tif (!masks)\n\t\treturn NULL;\n\n\t \n\tfor (curvec = 0; curvec < affd->pre_vectors; curvec++)\n\t\tcpumask_setall(&masks[curvec]);\n\n\tfor (i = 0, usedvecs = 0; i < affd->nr_sets; i++) {\n\t\tunsigned int this_vecs = affd->set_size[i];\n\t\tint j;\n\t\tstruct cpumask *result = group_cpus_evenly(this_vecs);\n\n\t\tif (!result) {\n\t\t\tkfree(masks);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tfor (j = 0; j < this_vecs; j++)\n\t\t\tcpumask_copy(&masks[curvec + j], &result[j]);\n\t\tkfree(result);\n\n\t\tcurvec += this_vecs;\n\t\tusedvecs += this_vecs;\n\t}\n\n\t \n\tif (usedvecs >= affvecs)\n\t\tcurvec = affd->pre_vectors + affvecs;\n\telse\n\t\tcurvec = affd->pre_vectors + usedvecs;\n\tfor (; curvec < nvecs; curvec++)\n\t\tcpumask_setall(&masks[curvec]);\n\n\treturn masks;\n}\n\nstatic int virtio_vdpa_find_vqs(struct virtio_device *vdev, unsigned int nvqs,\n\t\t\t\tstruct virtqueue *vqs[],\n\t\t\t\tvq_callback_t *callbacks[],\n\t\t\t\tconst char * const names[],\n\t\t\t\tconst bool *ctx,\n\t\t\t\tstruct irq_affinity *desc)\n{\n\tstruct virtio_vdpa_device *vd_dev = to_virtio_vdpa_device(vdev);\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct irq_affinity default_affd = { 0 };\n\tstruct cpumask *masks;\n\tstruct vdpa_callback cb;\n\tbool has_affinity = desc && ops->set_vq_affinity;\n\tint i, err, queue_idx = 0;\n\n\tif (has_affinity) {\n\t\tmasks = create_affinity_masks(nvqs, desc ? desc : &default_affd);\n\t\tif (!masks)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < nvqs; ++i) {\n\t\tif (!names[i]) {\n\t\t\tvqs[i] = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tvqs[i] = virtio_vdpa_setup_vq(vdev, queue_idx++,\n\t\t\t\t\t      callbacks[i], names[i], ctx ?\n\t\t\t\t\t      ctx[i] : false);\n\t\tif (IS_ERR(vqs[i])) {\n\t\t\terr = PTR_ERR(vqs[i]);\n\t\t\tgoto err_setup_vq;\n\t\t}\n\n\t\tif (has_affinity)\n\t\t\tops->set_vq_affinity(vdpa, i, &masks[i]);\n\t}\n\n\tcb.callback = virtio_vdpa_config_cb;\n\tcb.private = vd_dev;\n\tops->set_config_cb(vdpa, &cb);\n\tif (has_affinity)\n\t\tkfree(masks);\n\n\treturn 0;\n\nerr_setup_vq:\n\tvirtio_vdpa_del_vqs(vdev);\n\tif (has_affinity)\n\t\tkfree(masks);\n\treturn err;\n}\n\nstatic u64 virtio_vdpa_get_features(struct virtio_device *vdev)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\treturn ops->get_device_features(vdpa);\n}\n\nstatic int virtio_vdpa_finalize_features(struct virtio_device *vdev)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\n\t \n\tvring_transport_features(vdev);\n\n\treturn vdpa_set_features(vdpa, vdev->features);\n}\n\nstatic const char *virtio_vdpa_bus_name(struct virtio_device *vdev)\n{\n\tstruct virtio_vdpa_device *vd_dev = to_virtio_vdpa_device(vdev);\n\tstruct vdpa_device *vdpa = vd_dev->vdpa;\n\n\treturn dev_name(&vdpa->dev);\n}\n\nstatic int virtio_vdpa_set_vq_affinity(struct virtqueue *vq,\n\t\t\t\t       const struct cpumask *cpu_mask)\n{\n\tstruct virtio_vdpa_device *vd_dev = to_virtio_vdpa_device(vq->vdev);\n\tstruct vdpa_device *vdpa = vd_dev->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tunsigned int index = vq->index;\n\n\tif (ops->set_vq_affinity)\n\t\treturn ops->set_vq_affinity(vdpa, index, cpu_mask);\n\n\treturn 0;\n}\n\nstatic const struct cpumask *\nvirtio_vdpa_get_vq_affinity(struct virtio_device *vdev, int index)\n{\n\tstruct vdpa_device *vdpa = vd_get_vdpa(vdev);\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (ops->get_vq_affinity)\n\t\treturn ops->get_vq_affinity(vdpa, index);\n\n\treturn NULL;\n}\n\nstatic const struct virtio_config_ops virtio_vdpa_config_ops = {\n\t.get\t\t= virtio_vdpa_get,\n\t.set\t\t= virtio_vdpa_set,\n\t.generation\t= virtio_vdpa_generation,\n\t.get_status\t= virtio_vdpa_get_status,\n\t.set_status\t= virtio_vdpa_set_status,\n\t.reset\t\t= virtio_vdpa_reset,\n\t.find_vqs\t= virtio_vdpa_find_vqs,\n\t.del_vqs\t= virtio_vdpa_del_vqs,\n\t.get_features\t= virtio_vdpa_get_features,\n\t.finalize_features = virtio_vdpa_finalize_features,\n\t.bus_name\t= virtio_vdpa_bus_name,\n\t.set_vq_affinity = virtio_vdpa_set_vq_affinity,\n\t.get_vq_affinity = virtio_vdpa_get_vq_affinity,\n};\n\nstatic void virtio_vdpa_release_dev(struct device *_d)\n{\n\tstruct virtio_device *vdev =\n\t       container_of(_d, struct virtio_device, dev);\n\tstruct virtio_vdpa_device *vd_dev =\n\t       container_of(vdev, struct virtio_vdpa_device, vdev);\n\n\tkfree(vd_dev);\n}\n\nstatic int virtio_vdpa_probe(struct vdpa_device *vdpa)\n{\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct virtio_vdpa_device *vd_dev, *reg_dev = NULL;\n\tint ret = -EINVAL;\n\n\tvd_dev = kzalloc(sizeof(*vd_dev), GFP_KERNEL);\n\tif (!vd_dev)\n\t\treturn -ENOMEM;\n\n\tvd_dev->vdev.dev.parent = vdpa_get_dma_dev(vdpa);\n\tvd_dev->vdev.dev.release = virtio_vdpa_release_dev;\n\tvd_dev->vdev.config = &virtio_vdpa_config_ops;\n\tvd_dev->vdpa = vdpa;\n\tINIT_LIST_HEAD(&vd_dev->virtqueues);\n\tspin_lock_init(&vd_dev->lock);\n\n\tvd_dev->vdev.id.device = ops->get_device_id(vdpa);\n\tif (vd_dev->vdev.id.device == 0)\n\t\tgoto err;\n\n\tvd_dev->vdev.id.vendor = ops->get_vendor_id(vdpa);\n\tret = register_virtio_device(&vd_dev->vdev);\n\treg_dev = vd_dev;\n\tif (ret)\n\t\tgoto err;\n\n\tvdpa_set_drvdata(vdpa, vd_dev);\n\n\treturn 0;\n\nerr:\n\tif (reg_dev)\n\t\tput_device(&vd_dev->vdev.dev);\n\telse\n\t\tkfree(vd_dev);\n\treturn ret;\n}\n\nstatic void virtio_vdpa_remove(struct vdpa_device *vdpa)\n{\n\tstruct virtio_vdpa_device *vd_dev = vdpa_get_drvdata(vdpa);\n\n\tunregister_virtio_device(&vd_dev->vdev);\n}\n\nstatic struct vdpa_driver virtio_vdpa_driver = {\n\t.driver = {\n\t\t.name\t= \"virtio_vdpa\",\n\t},\n\t.probe\t= virtio_vdpa_probe,\n\t.remove = virtio_vdpa_remove,\n};\n\nmodule_vdpa_driver(virtio_vdpa_driver);\n\nMODULE_VERSION(MOD_VERSION);\nMODULE_LICENSE(MOD_LICENSE);\nMODULE_AUTHOR(MOD_AUTHOR);\nMODULE_DESCRIPTION(MOD_DESC);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}