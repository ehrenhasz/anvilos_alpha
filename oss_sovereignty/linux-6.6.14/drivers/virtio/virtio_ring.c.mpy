{
  "module_name": "virtio_ring.c",
  "hash_id": "652123d2a1ce967bcb8c29047ed9dba33fc1e087e552f0244ec1e704fffac86d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/virtio/virtio_ring.c",
  "human_readable_source": "\n \n#include <linux/virtio.h>\n#include <linux/virtio_ring.h>\n#include <linux/virtio_config.h>\n#include <linux/device.h>\n#include <linux/slab.h>\n#include <linux/module.h>\n#include <linux/hrtimer.h>\n#include <linux/dma-mapping.h>\n#include <linux/kmsan.h>\n#include <linux/spinlock.h>\n#include <xen/xen.h>\n\n#ifdef DEBUG\n \n#define BAD_RING(_vq, fmt, args...)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tdev_err(&(_vq)->vq.vdev->dev,\t\t\t\\\n\t\t\t\"%s:\"fmt, (_vq)->vq.name, ##args);\t\\\n\t\tBUG();\t\t\t\t\t\t\\\n\t} while (0)\n \n#define START_USE(_vq)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif ((_vq)->in_use)\t\t\t\t\\\n\t\t\tpanic(\"%s:in_use = %i\\n\",\t\t\\\n\t\t\t      (_vq)->vq.name, (_vq)->in_use);\t\\\n\t\t(_vq)->in_use = __LINE__;\t\t\t\\\n\t} while (0)\n#define END_USE(_vq) \\\n\tdo { BUG_ON(!(_vq)->in_use); (_vq)->in_use = 0; } while(0)\n#define LAST_ADD_TIME_UPDATE(_vq)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tktime_t now = ktime_get();\t\t\t\\\n\t\t\t\t\t\t\t\t\\\n\t\t  \\\n\t\tif ((_vq)->last_add_time_valid)\t\t\t\\\n\t\t\tWARN_ON(ktime_to_ms(ktime_sub(now,\t\\\n\t\t\t\t(_vq)->last_add_time)) > 100);\t\\\n\t\t(_vq)->last_add_time = now;\t\t\t\\\n\t\t(_vq)->last_add_time_valid = true;\t\t\\\n\t} while (0)\n#define LAST_ADD_TIME_CHECK(_vq)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif ((_vq)->last_add_time_valid) {\t\t\\\n\t\t\tWARN_ON(ktime_to_ms(ktime_sub(ktime_get(), \\\n\t\t\t\t      (_vq)->last_add_time)) > 100); \\\n\t\t}\t\t\t\t\t\t\\\n\t} while (0)\n#define LAST_ADD_TIME_INVALID(_vq)\t\t\t\t\\\n\t((_vq)->last_add_time_valid = false)\n#else\n#define BAD_RING(_vq, fmt, args...)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tdev_err(&_vq->vq.vdev->dev,\t\t\t\\\n\t\t\t\"%s:\"fmt, (_vq)->vq.name, ##args);\t\\\n\t\t(_vq)->broken = true;\t\t\t\t\\\n\t} while (0)\n#define START_USE(vq)\n#define END_USE(vq)\n#define LAST_ADD_TIME_UPDATE(vq)\n#define LAST_ADD_TIME_CHECK(vq)\n#define LAST_ADD_TIME_INVALID(vq)\n#endif\n\nstruct vring_desc_state_split {\n\tvoid *data;\t\t\t \n\tstruct vring_desc *indir_desc;\t \n};\n\nstruct vring_desc_state_packed {\n\tvoid *data;\t\t\t \n\tstruct vring_packed_desc *indir_desc;  \n\tu16 num;\t\t\t \n\tu16 last;\t\t\t \n};\n\nstruct vring_desc_extra {\n\tdma_addr_t addr;\t\t \n\tu32 len;\t\t\t \n\tu16 flags;\t\t\t \n\tu16 next;\t\t\t \n};\n\nstruct vring_virtqueue_split {\n\t \n\tstruct vring vring;\n\n\t \n\tu16 avail_flags_shadow;\n\n\t \n\tu16 avail_idx_shadow;\n\n\t \n\tstruct vring_desc_state_split *desc_state;\n\tstruct vring_desc_extra *desc_extra;\n\n\t \n\tdma_addr_t queue_dma_addr;\n\tsize_t queue_size_in_bytes;\n\n\t \n\tu32 vring_align;\n\tbool may_reduce_num;\n};\n\nstruct vring_virtqueue_packed {\n\t \n\tstruct {\n\t\tunsigned int num;\n\t\tstruct vring_packed_desc *desc;\n\t\tstruct vring_packed_desc_event *driver;\n\t\tstruct vring_packed_desc_event *device;\n\t} vring;\n\n\t \n\tbool avail_wrap_counter;\n\n\t \n\tu16 avail_used_flags;\n\n\t \n\tu16 next_avail_idx;\n\n\t \n\tu16 event_flags_shadow;\n\n\t \n\tstruct vring_desc_state_packed *desc_state;\n\tstruct vring_desc_extra *desc_extra;\n\n\t \n\tdma_addr_t ring_dma_addr;\n\tdma_addr_t driver_event_dma_addr;\n\tdma_addr_t device_event_dma_addr;\n\tsize_t ring_size_in_bytes;\n\tsize_t event_size_in_bytes;\n};\n\nstruct vring_virtqueue {\n\tstruct virtqueue vq;\n\n\t \n\tbool packed_ring;\n\n\t \n\tbool use_dma_api;\n\n\t \n\tbool weak_barriers;\n\n\t \n\tbool broken;\n\n\t \n\tbool indirect;\n\n\t \n\tbool event;\n\n\t \n\tbool premapped;\n\n\t \n\tbool do_unmap;\n\n\t \n\tunsigned int free_head;\n\t \n\tunsigned int num_added;\n\n\t \n\tu16 last_used_idx;\n\n\t \n\tbool event_triggered;\n\n\tunion {\n\t\t \n\t\tstruct vring_virtqueue_split split;\n\n\t\t \n\t\tstruct vring_virtqueue_packed packed;\n\t};\n\n\t \n\tbool (*notify)(struct virtqueue *vq);\n\n\t \n\tbool we_own_ring;\n\n\t \n\tstruct device *dma_dev;\n\n#ifdef DEBUG\n\t \n\tunsigned int in_use;\n\n\t \n\tbool last_add_time_valid;\n\tktime_t last_add_time;\n#endif\n};\n\nstatic struct virtqueue *__vring_new_virtqueue(unsigned int index,\n\t\t\t\t\t       struct vring_virtqueue_split *vring_split,\n\t\t\t\t\t       struct virtio_device *vdev,\n\t\t\t\t\t       bool weak_barriers,\n\t\t\t\t\t       bool context,\n\t\t\t\t\t       bool (*notify)(struct virtqueue *),\n\t\t\t\t\t       void (*callback)(struct virtqueue *),\n\t\t\t\t\t       const char *name,\n\t\t\t\t\t       struct device *dma_dev);\nstatic struct vring_desc_extra *vring_alloc_desc_extra(unsigned int num);\nstatic void vring_free(struct virtqueue *_vq);\n\n \n\n#define to_vvq(_vq) container_of_const(_vq, struct vring_virtqueue, vq)\n\nstatic bool virtqueue_use_indirect(const struct vring_virtqueue *vq,\n\t\t\t\t   unsigned int total_sg)\n{\n\t \n\treturn (vq->indirect && total_sg > 1 && vq->vq.num_free);\n}\n\n \n\nstatic bool vring_use_dma_api(const struct virtio_device *vdev)\n{\n\tif (!virtio_has_dma_quirk(vdev))\n\t\treturn true;\n\n\t \n\t \n\tif (xen_domain())\n\t\treturn true;\n\n\treturn false;\n}\n\nsize_t virtio_max_dma_size(const struct virtio_device *vdev)\n{\n\tsize_t max_segment_size = SIZE_MAX;\n\n\tif (vring_use_dma_api(vdev))\n\t\tmax_segment_size = dma_max_mapping_size(vdev->dev.parent);\n\n\treturn max_segment_size;\n}\nEXPORT_SYMBOL_GPL(virtio_max_dma_size);\n\nstatic void *vring_alloc_queue(struct virtio_device *vdev, size_t size,\n\t\t\t       dma_addr_t *dma_handle, gfp_t flag,\n\t\t\t       struct device *dma_dev)\n{\n\tif (vring_use_dma_api(vdev)) {\n\t\treturn dma_alloc_coherent(dma_dev, size,\n\t\t\t\t\t  dma_handle, flag);\n\t} else {\n\t\tvoid *queue = alloc_pages_exact(PAGE_ALIGN(size), flag);\n\n\t\tif (queue) {\n\t\t\tphys_addr_t phys_addr = virt_to_phys(queue);\n\t\t\t*dma_handle = (dma_addr_t)phys_addr;\n\n\t\t\t \n\t\t\tif (WARN_ON_ONCE(*dma_handle != phys_addr)) {\n\t\t\t\tfree_pages_exact(queue, PAGE_ALIGN(size));\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\t\treturn queue;\n\t}\n}\n\nstatic void vring_free_queue(struct virtio_device *vdev, size_t size,\n\t\t\t     void *queue, dma_addr_t dma_handle,\n\t\t\t     struct device *dma_dev)\n{\n\tif (vring_use_dma_api(vdev))\n\t\tdma_free_coherent(dma_dev, size, queue, dma_handle);\n\telse\n\t\tfree_pages_exact(queue, PAGE_ALIGN(size));\n}\n\n \nstatic struct device *vring_dma_dev(const struct vring_virtqueue *vq)\n{\n\treturn vq->dma_dev;\n}\n\n \nstatic int vring_map_one_sg(const struct vring_virtqueue *vq, struct scatterlist *sg,\n\t\t\t    enum dma_data_direction direction, dma_addr_t *addr)\n{\n\tif (vq->premapped) {\n\t\t*addr = sg_dma_address(sg);\n\t\treturn 0;\n\t}\n\n\tif (!vq->use_dma_api) {\n\t\t \n\t\tkmsan_handle_dma(sg_page(sg), sg->offset, sg->length, direction);\n\t\t*addr = (dma_addr_t)sg_phys(sg);\n\t\treturn 0;\n\t}\n\n\t \n\t*addr = dma_map_page(vring_dma_dev(vq),\n\t\t\t    sg_page(sg), sg->offset, sg->length,\n\t\t\t    direction);\n\n\tif (dma_mapping_error(vring_dma_dev(vq), *addr))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic dma_addr_t vring_map_single(const struct vring_virtqueue *vq,\n\t\t\t\t   void *cpu_addr, size_t size,\n\t\t\t\t   enum dma_data_direction direction)\n{\n\tif (!vq->use_dma_api)\n\t\treturn (dma_addr_t)virt_to_phys(cpu_addr);\n\n\treturn dma_map_single(vring_dma_dev(vq),\n\t\t\t      cpu_addr, size, direction);\n}\n\nstatic int vring_mapping_error(const struct vring_virtqueue *vq,\n\t\t\t       dma_addr_t addr)\n{\n\tif (!vq->use_dma_api)\n\t\treturn 0;\n\n\treturn dma_mapping_error(vring_dma_dev(vq), addr);\n}\n\nstatic void virtqueue_init(struct vring_virtqueue *vq, u32 num)\n{\n\tvq->vq.num_free = num;\n\n\tif (vq->packed_ring)\n\t\tvq->last_used_idx = 0 | (1 << VRING_PACKED_EVENT_F_WRAP_CTR);\n\telse\n\t\tvq->last_used_idx = 0;\n\n\tvq->event_triggered = false;\n\tvq->num_added = 0;\n\n#ifdef DEBUG\n\tvq->in_use = false;\n\tvq->last_add_time_valid = false;\n#endif\n}\n\n\n \n\nstatic void vring_unmap_one_split_indirect(const struct vring_virtqueue *vq,\n\t\t\t\t\t   const struct vring_desc *desc)\n{\n\tu16 flags;\n\n\tif (!vq->do_unmap)\n\t\treturn;\n\n\tflags = virtio16_to_cpu(vq->vq.vdev, desc->flags);\n\n\tdma_unmap_page(vring_dma_dev(vq),\n\t\t       virtio64_to_cpu(vq->vq.vdev, desc->addr),\n\t\t       virtio32_to_cpu(vq->vq.vdev, desc->len),\n\t\t       (flags & VRING_DESC_F_WRITE) ?\n\t\t       DMA_FROM_DEVICE : DMA_TO_DEVICE);\n}\n\nstatic unsigned int vring_unmap_one_split(const struct vring_virtqueue *vq,\n\t\t\t\t\t  unsigned int i)\n{\n\tstruct vring_desc_extra *extra = vq->split.desc_extra;\n\tu16 flags;\n\n\tflags = extra[i].flags;\n\n\tif (flags & VRING_DESC_F_INDIRECT) {\n\t\tif (!vq->use_dma_api)\n\t\t\tgoto out;\n\n\t\tdma_unmap_single(vring_dma_dev(vq),\n\t\t\t\t extra[i].addr,\n\t\t\t\t extra[i].len,\n\t\t\t\t (flags & VRING_DESC_F_WRITE) ?\n\t\t\t\t DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t} else {\n\t\tif (!vq->do_unmap)\n\t\t\tgoto out;\n\n\t\tdma_unmap_page(vring_dma_dev(vq),\n\t\t\t       extra[i].addr,\n\t\t\t       extra[i].len,\n\t\t\t       (flags & VRING_DESC_F_WRITE) ?\n\t\t\t       DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t}\n\nout:\n\treturn extra[i].next;\n}\n\nstatic struct vring_desc *alloc_indirect_split(struct virtqueue *_vq,\n\t\t\t\t\t       unsigned int total_sg,\n\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct vring_desc *desc;\n\tunsigned int i;\n\n\t \n\tgfp &= ~__GFP_HIGHMEM;\n\n\tdesc = kmalloc_array(total_sg, sizeof(struct vring_desc), gfp);\n\tif (!desc)\n\t\treturn NULL;\n\n\tfor (i = 0; i < total_sg; i++)\n\t\tdesc[i].next = cpu_to_virtio16(_vq->vdev, i + 1);\n\treturn desc;\n}\n\nstatic inline unsigned int virtqueue_add_desc_split(struct virtqueue *vq,\n\t\t\t\t\t\t    struct vring_desc *desc,\n\t\t\t\t\t\t    unsigned int i,\n\t\t\t\t\t\t    dma_addr_t addr,\n\t\t\t\t\t\t    unsigned int len,\n\t\t\t\t\t\t    u16 flags,\n\t\t\t\t\t\t    bool indirect)\n{\n\tstruct vring_virtqueue *vring = to_vvq(vq);\n\tstruct vring_desc_extra *extra = vring->split.desc_extra;\n\tu16 next;\n\n\tdesc[i].flags = cpu_to_virtio16(vq->vdev, flags);\n\tdesc[i].addr = cpu_to_virtio64(vq->vdev, addr);\n\tdesc[i].len = cpu_to_virtio32(vq->vdev, len);\n\n\tif (!indirect) {\n\t\tnext = extra[i].next;\n\t\tdesc[i].next = cpu_to_virtio16(vq->vdev, next);\n\n\t\textra[i].addr = addr;\n\t\textra[i].len = len;\n\t\textra[i].flags = flags;\n\t} else\n\t\tnext = virtio16_to_cpu(vq->vdev, desc[i].next);\n\n\treturn next;\n}\n\nstatic inline int virtqueue_add_split(struct virtqueue *_vq,\n\t\t\t\t      struct scatterlist *sgs[],\n\t\t\t\t      unsigned int total_sg,\n\t\t\t\t      unsigned int out_sgs,\n\t\t\t\t      unsigned int in_sgs,\n\t\t\t\t      void *data,\n\t\t\t\t      void *ctx,\n\t\t\t\t      gfp_t gfp)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct scatterlist *sg;\n\tstruct vring_desc *desc;\n\tunsigned int i, n, avail, descs_used, prev, err_idx;\n\tint head;\n\tbool indirect;\n\n\tSTART_USE(vq);\n\n\tBUG_ON(data == NULL);\n\tBUG_ON(ctx && vq->indirect);\n\n\tif (unlikely(vq->broken)) {\n\t\tEND_USE(vq);\n\t\treturn -EIO;\n\t}\n\n\tLAST_ADD_TIME_UPDATE(vq);\n\n\tBUG_ON(total_sg == 0);\n\n\thead = vq->free_head;\n\n\tif (virtqueue_use_indirect(vq, total_sg))\n\t\tdesc = alloc_indirect_split(_vq, total_sg, gfp);\n\telse {\n\t\tdesc = NULL;\n\t\tWARN_ON_ONCE(total_sg > vq->split.vring.num && !vq->indirect);\n\t}\n\n\tif (desc) {\n\t\t \n\t\tindirect = true;\n\t\t \n\t\ti = 0;\n\t\tdescs_used = 1;\n\t} else {\n\t\tindirect = false;\n\t\tdesc = vq->split.vring.desc;\n\t\ti = head;\n\t\tdescs_used = total_sg;\n\t}\n\n\tif (unlikely(vq->vq.num_free < descs_used)) {\n\t\tpr_debug(\"Can't add buf len %i - avail = %i\\n\",\n\t\t\t descs_used, vq->vq.num_free);\n\t\t \n\t\tif (out_sgs)\n\t\t\tvq->notify(&vq->vq);\n\t\tif (indirect)\n\t\t\tkfree(desc);\n\t\tEND_USE(vq);\n\t\treturn -ENOSPC;\n\t}\n\n\tfor (n = 0; n < out_sgs; n++) {\n\t\tfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\tdma_addr_t addr;\n\n\t\t\tif (vring_map_one_sg(vq, sg, DMA_TO_DEVICE, &addr))\n\t\t\t\tgoto unmap_release;\n\n\t\t\tprev = i;\n\t\t\t \n\t\t\ti = virtqueue_add_desc_split(_vq, desc, i, addr, sg->length,\n\t\t\t\t\t\t     VRING_DESC_F_NEXT,\n\t\t\t\t\t\t     indirect);\n\t\t}\n\t}\n\tfor (; n < (out_sgs + in_sgs); n++) {\n\t\tfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\tdma_addr_t addr;\n\n\t\t\tif (vring_map_one_sg(vq, sg, DMA_FROM_DEVICE, &addr))\n\t\t\t\tgoto unmap_release;\n\n\t\t\tprev = i;\n\t\t\t \n\t\t\ti = virtqueue_add_desc_split(_vq, desc, i, addr,\n\t\t\t\t\t\t     sg->length,\n\t\t\t\t\t\t     VRING_DESC_F_NEXT |\n\t\t\t\t\t\t     VRING_DESC_F_WRITE,\n\t\t\t\t\t\t     indirect);\n\t\t}\n\t}\n\t \n\tdesc[prev].flags &= cpu_to_virtio16(_vq->vdev, ~VRING_DESC_F_NEXT);\n\tif (!indirect && vq->do_unmap)\n\t\tvq->split.desc_extra[prev & (vq->split.vring.num - 1)].flags &=\n\t\t\t~VRING_DESC_F_NEXT;\n\n\tif (indirect) {\n\t\t \n\t\tdma_addr_t addr = vring_map_single(\n\t\t\tvq, desc, total_sg * sizeof(struct vring_desc),\n\t\t\tDMA_TO_DEVICE);\n\t\tif (vring_mapping_error(vq, addr)) {\n\t\t\tif (vq->premapped)\n\t\t\t\tgoto free_indirect;\n\n\t\t\tgoto unmap_release;\n\t\t}\n\n\t\tvirtqueue_add_desc_split(_vq, vq->split.vring.desc,\n\t\t\t\t\t head, addr,\n\t\t\t\t\t total_sg * sizeof(struct vring_desc),\n\t\t\t\t\t VRING_DESC_F_INDIRECT,\n\t\t\t\t\t false);\n\t}\n\n\t \n\tvq->vq.num_free -= descs_used;\n\n\t \n\tif (indirect)\n\t\tvq->free_head = vq->split.desc_extra[head].next;\n\telse\n\t\tvq->free_head = i;\n\n\t \n\tvq->split.desc_state[head].data = data;\n\tif (indirect)\n\t\tvq->split.desc_state[head].indir_desc = desc;\n\telse\n\t\tvq->split.desc_state[head].indir_desc = ctx;\n\n\t \n\tavail = vq->split.avail_idx_shadow & (vq->split.vring.num - 1);\n\tvq->split.vring.avail->ring[avail] = cpu_to_virtio16(_vq->vdev, head);\n\n\t \n\tvirtio_wmb(vq->weak_barriers);\n\tvq->split.avail_idx_shadow++;\n\tvq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev,\n\t\t\t\t\t\tvq->split.avail_idx_shadow);\n\tvq->num_added++;\n\n\tpr_debug(\"Added buffer head %i to %p\\n\", head, vq);\n\tEND_USE(vq);\n\n\t \n\tif (unlikely(vq->num_added == (1 << 16) - 1))\n\t\tvirtqueue_kick(_vq);\n\n\treturn 0;\n\nunmap_release:\n\terr_idx = i;\n\n\tif (indirect)\n\t\ti = 0;\n\telse\n\t\ti = head;\n\n\tfor (n = 0; n < total_sg; n++) {\n\t\tif (i == err_idx)\n\t\t\tbreak;\n\t\tif (indirect) {\n\t\t\tvring_unmap_one_split_indirect(vq, &desc[i]);\n\t\t\ti = virtio16_to_cpu(_vq->vdev, desc[i].next);\n\t\t} else\n\t\t\ti = vring_unmap_one_split(vq, i);\n\t}\n\nfree_indirect:\n\tif (indirect)\n\t\tkfree(desc);\n\n\tEND_USE(vq);\n\treturn -ENOMEM;\n}\n\nstatic bool virtqueue_kick_prepare_split(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 new, old;\n\tbool needs_kick;\n\n\tSTART_USE(vq);\n\t \n\tvirtio_mb(vq->weak_barriers);\n\n\told = vq->split.avail_idx_shadow - vq->num_added;\n\tnew = vq->split.avail_idx_shadow;\n\tvq->num_added = 0;\n\n\tLAST_ADD_TIME_CHECK(vq);\n\tLAST_ADD_TIME_INVALID(vq);\n\n\tif (vq->event) {\n\t\tneeds_kick = vring_need_event(virtio16_to_cpu(_vq->vdev,\n\t\t\t\t\tvring_avail_event(&vq->split.vring)),\n\t\t\t\t\t      new, old);\n\t} else {\n\t\tneeds_kick = !(vq->split.vring.used->flags &\n\t\t\t\t\tcpu_to_virtio16(_vq->vdev,\n\t\t\t\t\t\tVRING_USED_F_NO_NOTIFY));\n\t}\n\tEND_USE(vq);\n\treturn needs_kick;\n}\n\nstatic void detach_buf_split(struct vring_virtqueue *vq, unsigned int head,\n\t\t\t     void **ctx)\n{\n\tunsigned int i, j;\n\t__virtio16 nextflag = cpu_to_virtio16(vq->vq.vdev, VRING_DESC_F_NEXT);\n\n\t \n\tvq->split.desc_state[head].data = NULL;\n\n\t \n\ti = head;\n\n\twhile (vq->split.vring.desc[i].flags & nextflag) {\n\t\tvring_unmap_one_split(vq, i);\n\t\ti = vq->split.desc_extra[i].next;\n\t\tvq->vq.num_free++;\n\t}\n\n\tvring_unmap_one_split(vq, i);\n\tvq->split.desc_extra[i].next = vq->free_head;\n\tvq->free_head = head;\n\n\t \n\tvq->vq.num_free++;\n\n\tif (vq->indirect) {\n\t\tstruct vring_desc *indir_desc =\n\t\t\t\tvq->split.desc_state[head].indir_desc;\n\t\tu32 len;\n\n\t\t \n\t\tif (!indir_desc)\n\t\t\treturn;\n\n\t\tlen = vq->split.desc_extra[head].len;\n\n\t\tBUG_ON(!(vq->split.desc_extra[head].flags &\n\t\t\t\tVRING_DESC_F_INDIRECT));\n\t\tBUG_ON(len == 0 || len % sizeof(struct vring_desc));\n\n\t\tif (vq->do_unmap) {\n\t\t\tfor (j = 0; j < len / sizeof(struct vring_desc); j++)\n\t\t\t\tvring_unmap_one_split_indirect(vq, &indir_desc[j]);\n\t\t}\n\n\t\tkfree(indir_desc);\n\t\tvq->split.desc_state[head].indir_desc = NULL;\n\t} else if (ctx) {\n\t\t*ctx = vq->split.desc_state[head].indir_desc;\n\t}\n}\n\nstatic bool more_used_split(const struct vring_virtqueue *vq)\n{\n\treturn vq->last_used_idx != virtio16_to_cpu(vq->vq.vdev,\n\t\t\tvq->split.vring.used->idx);\n}\n\nstatic void *virtqueue_get_buf_ctx_split(struct virtqueue *_vq,\n\t\t\t\t\t unsigned int *len,\n\t\t\t\t\t void **ctx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tvoid *ret;\n\tunsigned int i;\n\tu16 last_used;\n\n\tSTART_USE(vq);\n\n\tif (unlikely(vq->broken)) {\n\t\tEND_USE(vq);\n\t\treturn NULL;\n\t}\n\n\tif (!more_used_split(vq)) {\n\t\tpr_debug(\"No more buffers in queue\\n\");\n\t\tEND_USE(vq);\n\t\treturn NULL;\n\t}\n\n\t \n\tvirtio_rmb(vq->weak_barriers);\n\n\tlast_used = (vq->last_used_idx & (vq->split.vring.num - 1));\n\ti = virtio32_to_cpu(_vq->vdev,\n\t\t\tvq->split.vring.used->ring[last_used].id);\n\t*len = virtio32_to_cpu(_vq->vdev,\n\t\t\tvq->split.vring.used->ring[last_used].len);\n\n\tif (unlikely(i >= vq->split.vring.num)) {\n\t\tBAD_RING(vq, \"id %u out of range\\n\", i);\n\t\treturn NULL;\n\t}\n\tif (unlikely(!vq->split.desc_state[i].data)) {\n\t\tBAD_RING(vq, \"id %u is not a head!\\n\", i);\n\t\treturn NULL;\n\t}\n\n\t \n\tret = vq->split.desc_state[i].data;\n\tdetach_buf_split(vq, i, ctx);\n\tvq->last_used_idx++;\n\t \n\tif (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT))\n\t\tvirtio_store_mb(vq->weak_barriers,\n\t\t\t\t&vring_used_event(&vq->split.vring),\n\t\t\t\tcpu_to_virtio16(_vq->vdev, vq->last_used_idx));\n\n\tLAST_ADD_TIME_INVALID(vq);\n\n\tEND_USE(vq);\n\treturn ret;\n}\n\nstatic void virtqueue_disable_cb_split(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!(vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT)) {\n\t\tvq->split.avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;\n\n\t\t \n\t\tif (vq->event_triggered)\n\t\t\treturn;\n\n\t\tif (vq->event)\n\t\t\t \n\t\t\tvring_used_event(&vq->split.vring) = 0x0;\n\t\telse\n\t\t\tvq->split.vring.avail->flags =\n\t\t\t\tcpu_to_virtio16(_vq->vdev,\n\t\t\t\t\t\tvq->split.avail_flags_shadow);\n\t}\n}\n\nstatic unsigned int virtqueue_enable_cb_prepare_split(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 last_used_idx;\n\n\tSTART_USE(vq);\n\n\t \n\t \n\tif (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {\n\t\tvq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;\n\t\tif (!vq->event)\n\t\t\tvq->split.vring.avail->flags =\n\t\t\t\tcpu_to_virtio16(_vq->vdev,\n\t\t\t\t\t\tvq->split.avail_flags_shadow);\n\t}\n\tvring_used_event(&vq->split.vring) = cpu_to_virtio16(_vq->vdev,\n\t\t\tlast_used_idx = vq->last_used_idx);\n\tEND_USE(vq);\n\treturn last_used_idx;\n}\n\nstatic bool virtqueue_poll_split(struct virtqueue *_vq, unsigned int last_used_idx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn (u16)last_used_idx != virtio16_to_cpu(_vq->vdev,\n\t\t\tvq->split.vring.used->idx);\n}\n\nstatic bool virtqueue_enable_cb_delayed_split(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 bufs;\n\n\tSTART_USE(vq);\n\n\t \n\t \n\tif (vq->split.avail_flags_shadow & VRING_AVAIL_F_NO_INTERRUPT) {\n\t\tvq->split.avail_flags_shadow &= ~VRING_AVAIL_F_NO_INTERRUPT;\n\t\tif (!vq->event)\n\t\t\tvq->split.vring.avail->flags =\n\t\t\t\tcpu_to_virtio16(_vq->vdev,\n\t\t\t\t\t\tvq->split.avail_flags_shadow);\n\t}\n\t \n\tbufs = (u16)(vq->split.avail_idx_shadow - vq->last_used_idx) * 3 / 4;\n\n\tvirtio_store_mb(vq->weak_barriers,\n\t\t\t&vring_used_event(&vq->split.vring),\n\t\t\tcpu_to_virtio16(_vq->vdev, vq->last_used_idx + bufs));\n\n\tif (unlikely((u16)(virtio16_to_cpu(_vq->vdev, vq->split.vring.used->idx)\n\t\t\t\t\t- vq->last_used_idx) > bufs)) {\n\t\tEND_USE(vq);\n\t\treturn false;\n\t}\n\n\tEND_USE(vq);\n\treturn true;\n}\n\nstatic void *virtqueue_detach_unused_buf_split(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tunsigned int i;\n\tvoid *buf;\n\n\tSTART_USE(vq);\n\n\tfor (i = 0; i < vq->split.vring.num; i++) {\n\t\tif (!vq->split.desc_state[i].data)\n\t\t\tcontinue;\n\t\t \n\t\tbuf = vq->split.desc_state[i].data;\n\t\tdetach_buf_split(vq, i, NULL);\n\t\tvq->split.avail_idx_shadow--;\n\t\tvq->split.vring.avail->idx = cpu_to_virtio16(_vq->vdev,\n\t\t\t\tvq->split.avail_idx_shadow);\n\t\tEND_USE(vq);\n\t\treturn buf;\n\t}\n\t \n\tBUG_ON(vq->vq.num_free != vq->split.vring.num);\n\n\tEND_USE(vq);\n\treturn NULL;\n}\n\nstatic void virtqueue_vring_init_split(struct vring_virtqueue_split *vring_split,\n\t\t\t\t       struct vring_virtqueue *vq)\n{\n\tstruct virtio_device *vdev;\n\n\tvdev = vq->vq.vdev;\n\n\tvring_split->avail_flags_shadow = 0;\n\tvring_split->avail_idx_shadow = 0;\n\n\t \n\tif (!vq->vq.callback) {\n\t\tvring_split->avail_flags_shadow |= VRING_AVAIL_F_NO_INTERRUPT;\n\t\tif (!vq->event)\n\t\t\tvring_split->vring.avail->flags = cpu_to_virtio16(vdev,\n\t\t\t\t\tvring_split->avail_flags_shadow);\n\t}\n}\n\nstatic void virtqueue_reinit_split(struct vring_virtqueue *vq)\n{\n\tint num;\n\n\tnum = vq->split.vring.num;\n\n\tvq->split.vring.avail->flags = 0;\n\tvq->split.vring.avail->idx = 0;\n\n\t \n\tvq->split.vring.avail->ring[num] = 0;\n\n\tvq->split.vring.used->flags = 0;\n\tvq->split.vring.used->idx = 0;\n\n\t \n\t*(__virtio16 *)&(vq->split.vring.used->ring[num]) = 0;\n\n\tvirtqueue_init(vq, num);\n\n\tvirtqueue_vring_init_split(&vq->split, vq);\n}\n\nstatic void virtqueue_vring_attach_split(struct vring_virtqueue *vq,\n\t\t\t\t\t struct vring_virtqueue_split *vring_split)\n{\n\tvq->split = *vring_split;\n\n\t \n\tvq->free_head = 0;\n}\n\nstatic int vring_alloc_state_extra_split(struct vring_virtqueue_split *vring_split)\n{\n\tstruct vring_desc_state_split *state;\n\tstruct vring_desc_extra *extra;\n\tu32 num = vring_split->vring.num;\n\n\tstate = kmalloc_array(num, sizeof(struct vring_desc_state_split), GFP_KERNEL);\n\tif (!state)\n\t\tgoto err_state;\n\n\textra = vring_alloc_desc_extra(num);\n\tif (!extra)\n\t\tgoto err_extra;\n\n\tmemset(state, 0, num * sizeof(struct vring_desc_state_split));\n\n\tvring_split->desc_state = state;\n\tvring_split->desc_extra = extra;\n\treturn 0;\n\nerr_extra:\n\tkfree(state);\nerr_state:\n\treturn -ENOMEM;\n}\n\nstatic void vring_free_split(struct vring_virtqueue_split *vring_split,\n\t\t\t     struct virtio_device *vdev, struct device *dma_dev)\n{\n\tvring_free_queue(vdev, vring_split->queue_size_in_bytes,\n\t\t\t vring_split->vring.desc,\n\t\t\t vring_split->queue_dma_addr,\n\t\t\t dma_dev);\n\n\tkfree(vring_split->desc_state);\n\tkfree(vring_split->desc_extra);\n}\n\nstatic int vring_alloc_queue_split(struct vring_virtqueue_split *vring_split,\n\t\t\t\t   struct virtio_device *vdev,\n\t\t\t\t   u32 num,\n\t\t\t\t   unsigned int vring_align,\n\t\t\t\t   bool may_reduce_num,\n\t\t\t\t   struct device *dma_dev)\n{\n\tvoid *queue = NULL;\n\tdma_addr_t dma_addr;\n\n\t \n\tif (!is_power_of_2(num)) {\n\t\tdev_warn(&vdev->dev, \"Bad virtqueue length %u\\n\", num);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (; num && vring_size(num, vring_align) > PAGE_SIZE; num /= 2) {\n\t\tqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\n\t\t\t\t\t  &dma_addr,\n\t\t\t\t\t  GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\n\t\t\t\t\t  dma_dev);\n\t\tif (queue)\n\t\t\tbreak;\n\t\tif (!may_reduce_num)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (!num)\n\t\treturn -ENOMEM;\n\n\tif (!queue) {\n\t\t \n\t\tqueue = vring_alloc_queue(vdev, vring_size(num, vring_align),\n\t\t\t\t\t  &dma_addr, GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t  dma_dev);\n\t}\n\tif (!queue)\n\t\treturn -ENOMEM;\n\n\tvring_init(&vring_split->vring, num, queue, vring_align);\n\n\tvring_split->queue_dma_addr = dma_addr;\n\tvring_split->queue_size_in_bytes = vring_size(num, vring_align);\n\n\tvring_split->vring_align = vring_align;\n\tvring_split->may_reduce_num = may_reduce_num;\n\n\treturn 0;\n}\n\nstatic struct virtqueue *vring_create_virtqueue_split(\n\tunsigned int index,\n\tunsigned int num,\n\tunsigned int vring_align,\n\tstruct virtio_device *vdev,\n\tbool weak_barriers,\n\tbool may_reduce_num,\n\tbool context,\n\tbool (*notify)(struct virtqueue *),\n\tvoid (*callback)(struct virtqueue *),\n\tconst char *name,\n\tstruct device *dma_dev)\n{\n\tstruct vring_virtqueue_split vring_split = {};\n\tstruct virtqueue *vq;\n\tint err;\n\n\terr = vring_alloc_queue_split(&vring_split, vdev, num, vring_align,\n\t\t\t\t      may_reduce_num, dma_dev);\n\tif (err)\n\t\treturn NULL;\n\n\tvq = __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,\n\t\t\t\t   context, notify, callback, name, dma_dev);\n\tif (!vq) {\n\t\tvring_free_split(&vring_split, vdev, dma_dev);\n\t\treturn NULL;\n\t}\n\n\tto_vvq(vq)->we_own_ring = true;\n\n\treturn vq;\n}\n\nstatic int virtqueue_resize_split(struct virtqueue *_vq, u32 num)\n{\n\tstruct vring_virtqueue_split vring_split = {};\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct virtio_device *vdev = _vq->vdev;\n\tint err;\n\n\terr = vring_alloc_queue_split(&vring_split, vdev, num,\n\t\t\t\t      vq->split.vring_align,\n\t\t\t\t      vq->split.may_reduce_num,\n\t\t\t\t      vring_dma_dev(vq));\n\tif (err)\n\t\tgoto err;\n\n\terr = vring_alloc_state_extra_split(&vring_split);\n\tif (err)\n\t\tgoto err_state_extra;\n\n\tvring_free(&vq->vq);\n\n\tvirtqueue_vring_init_split(&vring_split, vq);\n\n\tvirtqueue_init(vq, vring_split.vring.num);\n\tvirtqueue_vring_attach_split(vq, &vring_split);\n\n\treturn 0;\n\nerr_state_extra:\n\tvring_free_split(&vring_split, vdev, vring_dma_dev(vq));\nerr:\n\tvirtqueue_reinit_split(vq);\n\treturn -ENOMEM;\n}\n\n\n \nstatic bool packed_used_wrap_counter(u16 last_used_idx)\n{\n\treturn !!(last_used_idx & (1 << VRING_PACKED_EVENT_F_WRAP_CTR));\n}\n\nstatic u16 packed_last_used(u16 last_used_idx)\n{\n\treturn last_used_idx & ~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR));\n}\n\nstatic void vring_unmap_extra_packed(const struct vring_virtqueue *vq,\n\t\t\t\t     const struct vring_desc_extra *extra)\n{\n\tu16 flags;\n\n\tflags = extra->flags;\n\n\tif (flags & VRING_DESC_F_INDIRECT) {\n\t\tif (!vq->use_dma_api)\n\t\t\treturn;\n\n\t\tdma_unmap_single(vring_dma_dev(vq),\n\t\t\t\t extra->addr, extra->len,\n\t\t\t\t (flags & VRING_DESC_F_WRITE) ?\n\t\t\t\t DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t} else {\n\t\tif (!vq->do_unmap)\n\t\t\treturn;\n\n\t\tdma_unmap_page(vring_dma_dev(vq),\n\t\t\t       extra->addr, extra->len,\n\t\t\t       (flags & VRING_DESC_F_WRITE) ?\n\t\t\t       DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t}\n}\n\nstatic void vring_unmap_desc_packed(const struct vring_virtqueue *vq,\n\t\t\t\t    const struct vring_packed_desc *desc)\n{\n\tu16 flags;\n\n\tif (!vq->do_unmap)\n\t\treturn;\n\n\tflags = le16_to_cpu(desc->flags);\n\n\tdma_unmap_page(vring_dma_dev(vq),\n\t\t       le64_to_cpu(desc->addr),\n\t\t       le32_to_cpu(desc->len),\n\t\t       (flags & VRING_DESC_F_WRITE) ?\n\t\t       DMA_FROM_DEVICE : DMA_TO_DEVICE);\n}\n\nstatic struct vring_packed_desc *alloc_indirect_packed(unsigned int total_sg,\n\t\t\t\t\t\t       gfp_t gfp)\n{\n\tstruct vring_packed_desc *desc;\n\n\t \n\tgfp &= ~__GFP_HIGHMEM;\n\n\tdesc = kmalloc_array(total_sg, sizeof(struct vring_packed_desc), gfp);\n\n\treturn desc;\n}\n\nstatic int virtqueue_add_indirect_packed(struct vring_virtqueue *vq,\n\t\t\t\t\t struct scatterlist *sgs[],\n\t\t\t\t\t unsigned int total_sg,\n\t\t\t\t\t unsigned int out_sgs,\n\t\t\t\t\t unsigned int in_sgs,\n\t\t\t\t\t void *data,\n\t\t\t\t\t gfp_t gfp)\n{\n\tstruct vring_packed_desc *desc;\n\tstruct scatterlist *sg;\n\tunsigned int i, n, err_idx;\n\tu16 head, id;\n\tdma_addr_t addr;\n\n\thead = vq->packed.next_avail_idx;\n\tdesc = alloc_indirect_packed(total_sg, gfp);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\tif (unlikely(vq->vq.num_free < 1)) {\n\t\tpr_debug(\"Can't add buf len 1 - avail = 0\\n\");\n\t\tkfree(desc);\n\t\tEND_USE(vq);\n\t\treturn -ENOSPC;\n\t}\n\n\ti = 0;\n\tid = vq->free_head;\n\tBUG_ON(id == vq->packed.vring.num);\n\n\tfor (n = 0; n < out_sgs + in_sgs; n++) {\n\t\tfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\tif (vring_map_one_sg(vq, sg, n < out_sgs ?\n\t\t\t\t\t     DMA_TO_DEVICE : DMA_FROM_DEVICE, &addr))\n\t\t\t\tgoto unmap_release;\n\n\t\t\tdesc[i].flags = cpu_to_le16(n < out_sgs ?\n\t\t\t\t\t\t0 : VRING_DESC_F_WRITE);\n\t\t\tdesc[i].addr = cpu_to_le64(addr);\n\t\t\tdesc[i].len = cpu_to_le32(sg->length);\n\t\t\ti++;\n\t\t}\n\t}\n\n\t \n\taddr = vring_map_single(vq, desc,\n\t\t\ttotal_sg * sizeof(struct vring_packed_desc),\n\t\t\tDMA_TO_DEVICE);\n\tif (vring_mapping_error(vq, addr)) {\n\t\tif (vq->premapped)\n\t\t\tgoto free_desc;\n\n\t\tgoto unmap_release;\n\t}\n\n\tvq->packed.vring.desc[head].addr = cpu_to_le64(addr);\n\tvq->packed.vring.desc[head].len = cpu_to_le32(total_sg *\n\t\t\t\tsizeof(struct vring_packed_desc));\n\tvq->packed.vring.desc[head].id = cpu_to_le16(id);\n\n\tif (vq->do_unmap) {\n\t\tvq->packed.desc_extra[id].addr = addr;\n\t\tvq->packed.desc_extra[id].len = total_sg *\n\t\t\t\tsizeof(struct vring_packed_desc);\n\t\tvq->packed.desc_extra[id].flags = VRING_DESC_F_INDIRECT |\n\t\t\t\t\t\t  vq->packed.avail_used_flags;\n\t}\n\n\t \n\tvirtio_wmb(vq->weak_barriers);\n\tvq->packed.vring.desc[head].flags = cpu_to_le16(VRING_DESC_F_INDIRECT |\n\t\t\t\t\t\tvq->packed.avail_used_flags);\n\n\t \n\tvq->vq.num_free -= 1;\n\n\t \n\tn = head + 1;\n\tif (n >= vq->packed.vring.num) {\n\t\tn = 0;\n\t\tvq->packed.avail_wrap_counter ^= 1;\n\t\tvq->packed.avail_used_flags ^=\n\t\t\t\t1 << VRING_PACKED_DESC_F_AVAIL |\n\t\t\t\t1 << VRING_PACKED_DESC_F_USED;\n\t}\n\tvq->packed.next_avail_idx = n;\n\tvq->free_head = vq->packed.desc_extra[id].next;\n\n\t \n\tvq->packed.desc_state[id].num = 1;\n\tvq->packed.desc_state[id].data = data;\n\tvq->packed.desc_state[id].indir_desc = desc;\n\tvq->packed.desc_state[id].last = id;\n\n\tvq->num_added += 1;\n\n\tpr_debug(\"Added buffer head %i to %p\\n\", head, vq);\n\tEND_USE(vq);\n\n\treturn 0;\n\nunmap_release:\n\terr_idx = i;\n\n\tfor (i = 0; i < err_idx; i++)\n\t\tvring_unmap_desc_packed(vq, &desc[i]);\n\nfree_desc:\n\tkfree(desc);\n\n\tEND_USE(vq);\n\treturn -ENOMEM;\n}\n\nstatic inline int virtqueue_add_packed(struct virtqueue *_vq,\n\t\t\t\t       struct scatterlist *sgs[],\n\t\t\t\t       unsigned int total_sg,\n\t\t\t\t       unsigned int out_sgs,\n\t\t\t\t       unsigned int in_sgs,\n\t\t\t\t       void *data,\n\t\t\t\t       void *ctx,\n\t\t\t\t       gfp_t gfp)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct vring_packed_desc *desc;\n\tstruct scatterlist *sg;\n\tunsigned int i, n, c, descs_used, err_idx;\n\t__le16 head_flags, flags;\n\tu16 head, id, prev, curr, avail_used_flags;\n\tint err;\n\n\tSTART_USE(vq);\n\n\tBUG_ON(data == NULL);\n\tBUG_ON(ctx && vq->indirect);\n\n\tif (unlikely(vq->broken)) {\n\t\tEND_USE(vq);\n\t\treturn -EIO;\n\t}\n\n\tLAST_ADD_TIME_UPDATE(vq);\n\n\tBUG_ON(total_sg == 0);\n\n\tif (virtqueue_use_indirect(vq, total_sg)) {\n\t\terr = virtqueue_add_indirect_packed(vq, sgs, total_sg, out_sgs,\n\t\t\t\t\t\t    in_sgs, data, gfp);\n\t\tif (err != -ENOMEM) {\n\t\t\tEND_USE(vq);\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t}\n\n\thead = vq->packed.next_avail_idx;\n\tavail_used_flags = vq->packed.avail_used_flags;\n\n\tWARN_ON_ONCE(total_sg > vq->packed.vring.num && !vq->indirect);\n\n\tdesc = vq->packed.vring.desc;\n\ti = head;\n\tdescs_used = total_sg;\n\n\tif (unlikely(vq->vq.num_free < descs_used)) {\n\t\tpr_debug(\"Can't add buf len %i - avail = %i\\n\",\n\t\t\t descs_used, vq->vq.num_free);\n\t\tEND_USE(vq);\n\t\treturn -ENOSPC;\n\t}\n\n\tid = vq->free_head;\n\tBUG_ON(id == vq->packed.vring.num);\n\n\tcurr = id;\n\tc = 0;\n\tfor (n = 0; n < out_sgs + in_sgs; n++) {\n\t\tfor (sg = sgs[n]; sg; sg = sg_next(sg)) {\n\t\t\tdma_addr_t addr;\n\n\t\t\tif (vring_map_one_sg(vq, sg, n < out_sgs ?\n\t\t\t\t\t     DMA_TO_DEVICE : DMA_FROM_DEVICE, &addr))\n\t\t\t\tgoto unmap_release;\n\n\t\t\tflags = cpu_to_le16(vq->packed.avail_used_flags |\n\t\t\t\t    (++c == total_sg ? 0 : VRING_DESC_F_NEXT) |\n\t\t\t\t    (n < out_sgs ? 0 : VRING_DESC_F_WRITE));\n\t\t\tif (i == head)\n\t\t\t\thead_flags = flags;\n\t\t\telse\n\t\t\t\tdesc[i].flags = flags;\n\n\t\t\tdesc[i].addr = cpu_to_le64(addr);\n\t\t\tdesc[i].len = cpu_to_le32(sg->length);\n\t\t\tdesc[i].id = cpu_to_le16(id);\n\n\t\t\tif (unlikely(vq->do_unmap)) {\n\t\t\t\tvq->packed.desc_extra[curr].addr = addr;\n\t\t\t\tvq->packed.desc_extra[curr].len = sg->length;\n\t\t\t\tvq->packed.desc_extra[curr].flags =\n\t\t\t\t\tle16_to_cpu(flags);\n\t\t\t}\n\t\t\tprev = curr;\n\t\t\tcurr = vq->packed.desc_extra[curr].next;\n\n\t\t\tif ((unlikely(++i >= vq->packed.vring.num))) {\n\t\t\t\ti = 0;\n\t\t\t\tvq->packed.avail_used_flags ^=\n\t\t\t\t\t1 << VRING_PACKED_DESC_F_AVAIL |\n\t\t\t\t\t1 << VRING_PACKED_DESC_F_USED;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (i <= head)\n\t\tvq->packed.avail_wrap_counter ^= 1;\n\n\t \n\tvq->vq.num_free -= descs_used;\n\n\t \n\tvq->packed.next_avail_idx = i;\n\tvq->free_head = curr;\n\n\t \n\tvq->packed.desc_state[id].num = descs_used;\n\tvq->packed.desc_state[id].data = data;\n\tvq->packed.desc_state[id].indir_desc = ctx;\n\tvq->packed.desc_state[id].last = prev;\n\n\t \n\tvirtio_wmb(vq->weak_barriers);\n\tvq->packed.vring.desc[head].flags = head_flags;\n\tvq->num_added += descs_used;\n\n\tpr_debug(\"Added buffer head %i to %p\\n\", head, vq);\n\tEND_USE(vq);\n\n\treturn 0;\n\nunmap_release:\n\terr_idx = i;\n\ti = head;\n\tcurr = vq->free_head;\n\n\tvq->packed.avail_used_flags = avail_used_flags;\n\n\tfor (n = 0; n < total_sg; n++) {\n\t\tif (i == err_idx)\n\t\t\tbreak;\n\t\tvring_unmap_extra_packed(vq, &vq->packed.desc_extra[curr]);\n\t\tcurr = vq->packed.desc_extra[curr].next;\n\t\ti++;\n\t\tif (i >= vq->packed.vring.num)\n\t\t\ti = 0;\n\t}\n\n\tEND_USE(vq);\n\treturn -EIO;\n}\n\nstatic bool virtqueue_kick_prepare_packed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 new, old, off_wrap, flags, wrap_counter, event_idx;\n\tbool needs_kick;\n\tunion {\n\t\tstruct {\n\t\t\t__le16 off_wrap;\n\t\t\t__le16 flags;\n\t\t};\n\t\tu32 u32;\n\t} snapshot;\n\n\tSTART_USE(vq);\n\n\t \n\tvirtio_mb(vq->weak_barriers);\n\n\told = vq->packed.next_avail_idx - vq->num_added;\n\tnew = vq->packed.next_avail_idx;\n\tvq->num_added = 0;\n\n\tsnapshot.u32 = *(u32 *)vq->packed.vring.device;\n\tflags = le16_to_cpu(snapshot.flags);\n\n\tLAST_ADD_TIME_CHECK(vq);\n\tLAST_ADD_TIME_INVALID(vq);\n\n\tif (flags != VRING_PACKED_EVENT_FLAG_DESC) {\n\t\tneeds_kick = (flags != VRING_PACKED_EVENT_FLAG_DISABLE);\n\t\tgoto out;\n\t}\n\n\toff_wrap = le16_to_cpu(snapshot.off_wrap);\n\n\twrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;\n\tevent_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);\n\tif (wrap_counter != vq->packed.avail_wrap_counter)\n\t\tevent_idx -= vq->packed.vring.num;\n\n\tneeds_kick = vring_need_event(event_idx, new, old);\nout:\n\tEND_USE(vq);\n\treturn needs_kick;\n}\n\nstatic void detach_buf_packed(struct vring_virtqueue *vq,\n\t\t\t      unsigned int id, void **ctx)\n{\n\tstruct vring_desc_state_packed *state = NULL;\n\tstruct vring_packed_desc *desc;\n\tunsigned int i, curr;\n\n\tstate = &vq->packed.desc_state[id];\n\n\t \n\tstate->data = NULL;\n\n\tvq->packed.desc_extra[state->last].next = vq->free_head;\n\tvq->free_head = id;\n\tvq->vq.num_free += state->num;\n\n\tif (unlikely(vq->do_unmap)) {\n\t\tcurr = id;\n\t\tfor (i = 0; i < state->num; i++) {\n\t\t\tvring_unmap_extra_packed(vq,\n\t\t\t\t\t\t &vq->packed.desc_extra[curr]);\n\t\t\tcurr = vq->packed.desc_extra[curr].next;\n\t\t}\n\t}\n\n\tif (vq->indirect) {\n\t\tu32 len;\n\n\t\t \n\t\tdesc = state->indir_desc;\n\t\tif (!desc)\n\t\t\treturn;\n\n\t\tif (vq->do_unmap) {\n\t\t\tlen = vq->packed.desc_extra[id].len;\n\t\t\tfor (i = 0; i < len / sizeof(struct vring_packed_desc);\n\t\t\t\t\ti++)\n\t\t\t\tvring_unmap_desc_packed(vq, &desc[i]);\n\t\t}\n\t\tkfree(desc);\n\t\tstate->indir_desc = NULL;\n\t} else if (ctx) {\n\t\t*ctx = state->indir_desc;\n\t}\n}\n\nstatic inline bool is_used_desc_packed(const struct vring_virtqueue *vq,\n\t\t\t\t       u16 idx, bool used_wrap_counter)\n{\n\tbool avail, used;\n\tu16 flags;\n\n\tflags = le16_to_cpu(vq->packed.vring.desc[idx].flags);\n\tavail = !!(flags & (1 << VRING_PACKED_DESC_F_AVAIL));\n\tused = !!(flags & (1 << VRING_PACKED_DESC_F_USED));\n\n\treturn avail == used && used == used_wrap_counter;\n}\n\nstatic bool more_used_packed(const struct vring_virtqueue *vq)\n{\n\tu16 last_used;\n\tu16 last_used_idx;\n\tbool used_wrap_counter;\n\n\tlast_used_idx = READ_ONCE(vq->last_used_idx);\n\tlast_used = packed_last_used(last_used_idx);\n\tused_wrap_counter = packed_used_wrap_counter(last_used_idx);\n\treturn is_used_desc_packed(vq, last_used, used_wrap_counter);\n}\n\nstatic void *virtqueue_get_buf_ctx_packed(struct virtqueue *_vq,\n\t\t\t\t\t  unsigned int *len,\n\t\t\t\t\t  void **ctx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 last_used, id, last_used_idx;\n\tbool used_wrap_counter;\n\tvoid *ret;\n\n\tSTART_USE(vq);\n\n\tif (unlikely(vq->broken)) {\n\t\tEND_USE(vq);\n\t\treturn NULL;\n\t}\n\n\tif (!more_used_packed(vq)) {\n\t\tpr_debug(\"No more buffers in queue\\n\");\n\t\tEND_USE(vq);\n\t\treturn NULL;\n\t}\n\n\t \n\tvirtio_rmb(vq->weak_barriers);\n\n\tlast_used_idx = READ_ONCE(vq->last_used_idx);\n\tused_wrap_counter = packed_used_wrap_counter(last_used_idx);\n\tlast_used = packed_last_used(last_used_idx);\n\tid = le16_to_cpu(vq->packed.vring.desc[last_used].id);\n\t*len = le32_to_cpu(vq->packed.vring.desc[last_used].len);\n\n\tif (unlikely(id >= vq->packed.vring.num)) {\n\t\tBAD_RING(vq, \"id %u out of range\\n\", id);\n\t\treturn NULL;\n\t}\n\tif (unlikely(!vq->packed.desc_state[id].data)) {\n\t\tBAD_RING(vq, \"id %u is not a head!\\n\", id);\n\t\treturn NULL;\n\t}\n\n\t \n\tret = vq->packed.desc_state[id].data;\n\tdetach_buf_packed(vq, id, ctx);\n\n\tlast_used += vq->packed.desc_state[id].num;\n\tif (unlikely(last_used >= vq->packed.vring.num)) {\n\t\tlast_used -= vq->packed.vring.num;\n\t\tused_wrap_counter ^= 1;\n\t}\n\n\tlast_used = (last_used | (used_wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));\n\tWRITE_ONCE(vq->last_used_idx, last_used);\n\n\t \n\tif (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DESC)\n\t\tvirtio_store_mb(vq->weak_barriers,\n\t\t\t\t&vq->packed.vring.driver->off_wrap,\n\t\t\t\tcpu_to_le16(vq->last_used_idx));\n\n\tLAST_ADD_TIME_INVALID(vq);\n\n\tEND_USE(vq);\n\treturn ret;\n}\n\nstatic void virtqueue_disable_cb_packed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->packed.event_flags_shadow != VRING_PACKED_EVENT_FLAG_DISABLE) {\n\t\tvq->packed.event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;\n\n\t\t \n\t\tif (vq->event_triggered)\n\t\t\treturn;\n\n\t\tvq->packed.vring.driver->flags =\n\t\t\tcpu_to_le16(vq->packed.event_flags_shadow);\n\t}\n}\n\nstatic unsigned int virtqueue_enable_cb_prepare_packed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tSTART_USE(vq);\n\n\t \n\n\tif (vq->event) {\n\t\tvq->packed.vring.driver->off_wrap =\n\t\t\tcpu_to_le16(vq->last_used_idx);\n\t\t \n\t\tvirtio_wmb(vq->weak_barriers);\n\t}\n\n\tif (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {\n\t\tvq->packed.event_flags_shadow = vq->event ?\n\t\t\t\tVRING_PACKED_EVENT_FLAG_DESC :\n\t\t\t\tVRING_PACKED_EVENT_FLAG_ENABLE;\n\t\tvq->packed.vring.driver->flags =\n\t\t\t\tcpu_to_le16(vq->packed.event_flags_shadow);\n\t}\n\n\tEND_USE(vq);\n\treturn vq->last_used_idx;\n}\n\nstatic bool virtqueue_poll_packed(struct virtqueue *_vq, u16 off_wrap)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tbool wrap_counter;\n\tu16 used_idx;\n\n\twrap_counter = off_wrap >> VRING_PACKED_EVENT_F_WRAP_CTR;\n\tused_idx = off_wrap & ~(1 << VRING_PACKED_EVENT_F_WRAP_CTR);\n\n\treturn is_used_desc_packed(vq, used_idx, wrap_counter);\n}\n\nstatic bool virtqueue_enable_cb_delayed_packed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 used_idx, wrap_counter, last_used_idx;\n\tu16 bufs;\n\n\tSTART_USE(vq);\n\n\t \n\n\tif (vq->event) {\n\t\t \n\t\tbufs = (vq->packed.vring.num - vq->vq.num_free) * 3 / 4;\n\t\tlast_used_idx = READ_ONCE(vq->last_used_idx);\n\t\twrap_counter = packed_used_wrap_counter(last_used_idx);\n\n\t\tused_idx = packed_last_used(last_used_idx) + bufs;\n\t\tif (used_idx >= vq->packed.vring.num) {\n\t\t\tused_idx -= vq->packed.vring.num;\n\t\t\twrap_counter ^= 1;\n\t\t}\n\n\t\tvq->packed.vring.driver->off_wrap = cpu_to_le16(used_idx |\n\t\t\t(wrap_counter << VRING_PACKED_EVENT_F_WRAP_CTR));\n\n\t\t \n\t\tvirtio_wmb(vq->weak_barriers);\n\t}\n\n\tif (vq->packed.event_flags_shadow == VRING_PACKED_EVENT_FLAG_DISABLE) {\n\t\tvq->packed.event_flags_shadow = vq->event ?\n\t\t\t\tVRING_PACKED_EVENT_FLAG_DESC :\n\t\t\t\tVRING_PACKED_EVENT_FLAG_ENABLE;\n\t\tvq->packed.vring.driver->flags =\n\t\t\t\tcpu_to_le16(vq->packed.event_flags_shadow);\n\t}\n\n\t \n\tvirtio_mb(vq->weak_barriers);\n\n\tlast_used_idx = READ_ONCE(vq->last_used_idx);\n\twrap_counter = packed_used_wrap_counter(last_used_idx);\n\tused_idx = packed_last_used(last_used_idx);\n\tif (is_used_desc_packed(vq, used_idx, wrap_counter)) {\n\t\tEND_USE(vq);\n\t\treturn false;\n\t}\n\n\tEND_USE(vq);\n\treturn true;\n}\n\nstatic void *virtqueue_detach_unused_buf_packed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tunsigned int i;\n\tvoid *buf;\n\n\tSTART_USE(vq);\n\n\tfor (i = 0; i < vq->packed.vring.num; i++) {\n\t\tif (!vq->packed.desc_state[i].data)\n\t\t\tcontinue;\n\t\t \n\t\tbuf = vq->packed.desc_state[i].data;\n\t\tdetach_buf_packed(vq, i, NULL);\n\t\tEND_USE(vq);\n\t\treturn buf;\n\t}\n\t \n\tBUG_ON(vq->vq.num_free != vq->packed.vring.num);\n\n\tEND_USE(vq);\n\treturn NULL;\n}\n\nstatic struct vring_desc_extra *vring_alloc_desc_extra(unsigned int num)\n{\n\tstruct vring_desc_extra *desc_extra;\n\tunsigned int i;\n\n\tdesc_extra = kmalloc_array(num, sizeof(struct vring_desc_extra),\n\t\t\t\t   GFP_KERNEL);\n\tif (!desc_extra)\n\t\treturn NULL;\n\n\tmemset(desc_extra, 0, num * sizeof(struct vring_desc_extra));\n\n\tfor (i = 0; i < num - 1; i++)\n\t\tdesc_extra[i].next = i + 1;\n\n\treturn desc_extra;\n}\n\nstatic void vring_free_packed(struct vring_virtqueue_packed *vring_packed,\n\t\t\t      struct virtio_device *vdev,\n\t\t\t      struct device *dma_dev)\n{\n\tif (vring_packed->vring.desc)\n\t\tvring_free_queue(vdev, vring_packed->ring_size_in_bytes,\n\t\t\t\t vring_packed->vring.desc,\n\t\t\t\t vring_packed->ring_dma_addr,\n\t\t\t\t dma_dev);\n\n\tif (vring_packed->vring.driver)\n\t\tvring_free_queue(vdev, vring_packed->event_size_in_bytes,\n\t\t\t\t vring_packed->vring.driver,\n\t\t\t\t vring_packed->driver_event_dma_addr,\n\t\t\t\t dma_dev);\n\n\tif (vring_packed->vring.device)\n\t\tvring_free_queue(vdev, vring_packed->event_size_in_bytes,\n\t\t\t\t vring_packed->vring.device,\n\t\t\t\t vring_packed->device_event_dma_addr,\n\t\t\t\t dma_dev);\n\n\tkfree(vring_packed->desc_state);\n\tkfree(vring_packed->desc_extra);\n}\n\nstatic int vring_alloc_queue_packed(struct vring_virtqueue_packed *vring_packed,\n\t\t\t\t    struct virtio_device *vdev,\n\t\t\t\t    u32 num, struct device *dma_dev)\n{\n\tstruct vring_packed_desc *ring;\n\tstruct vring_packed_desc_event *driver, *device;\n\tdma_addr_t ring_dma_addr, driver_event_dma_addr, device_event_dma_addr;\n\tsize_t ring_size_in_bytes, event_size_in_bytes;\n\n\tring_size_in_bytes = num * sizeof(struct vring_packed_desc);\n\n\tring = vring_alloc_queue(vdev, ring_size_in_bytes,\n\t\t\t\t &ring_dma_addr,\n\t\t\t\t GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\n\t\t\t\t dma_dev);\n\tif (!ring)\n\t\tgoto err;\n\n\tvring_packed->vring.desc         = ring;\n\tvring_packed->ring_dma_addr      = ring_dma_addr;\n\tvring_packed->ring_size_in_bytes = ring_size_in_bytes;\n\n\tevent_size_in_bytes = sizeof(struct vring_packed_desc_event);\n\n\tdriver = vring_alloc_queue(vdev, event_size_in_bytes,\n\t\t\t\t   &driver_event_dma_addr,\n\t\t\t\t   GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\n\t\t\t\t   dma_dev);\n\tif (!driver)\n\t\tgoto err;\n\n\tvring_packed->vring.driver          = driver;\n\tvring_packed->event_size_in_bytes   = event_size_in_bytes;\n\tvring_packed->driver_event_dma_addr = driver_event_dma_addr;\n\n\tdevice = vring_alloc_queue(vdev, event_size_in_bytes,\n\t\t\t\t   &device_event_dma_addr,\n\t\t\t\t   GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO,\n\t\t\t\t   dma_dev);\n\tif (!device)\n\t\tgoto err;\n\n\tvring_packed->vring.device          = device;\n\tvring_packed->device_event_dma_addr = device_event_dma_addr;\n\n\tvring_packed->vring.num = num;\n\n\treturn 0;\n\nerr:\n\tvring_free_packed(vring_packed, vdev, dma_dev);\n\treturn -ENOMEM;\n}\n\nstatic int vring_alloc_state_extra_packed(struct vring_virtqueue_packed *vring_packed)\n{\n\tstruct vring_desc_state_packed *state;\n\tstruct vring_desc_extra *extra;\n\tu32 num = vring_packed->vring.num;\n\n\tstate = kmalloc_array(num, sizeof(struct vring_desc_state_packed), GFP_KERNEL);\n\tif (!state)\n\t\tgoto err_desc_state;\n\n\tmemset(state, 0, num * sizeof(struct vring_desc_state_packed));\n\n\textra = vring_alloc_desc_extra(num);\n\tif (!extra)\n\t\tgoto err_desc_extra;\n\n\tvring_packed->desc_state = state;\n\tvring_packed->desc_extra = extra;\n\n\treturn 0;\n\nerr_desc_extra:\n\tkfree(state);\nerr_desc_state:\n\treturn -ENOMEM;\n}\n\nstatic void virtqueue_vring_init_packed(struct vring_virtqueue_packed *vring_packed,\n\t\t\t\t\tbool callback)\n{\n\tvring_packed->next_avail_idx = 0;\n\tvring_packed->avail_wrap_counter = 1;\n\tvring_packed->event_flags_shadow = 0;\n\tvring_packed->avail_used_flags = 1 << VRING_PACKED_DESC_F_AVAIL;\n\n\t \n\tif (!callback) {\n\t\tvring_packed->event_flags_shadow = VRING_PACKED_EVENT_FLAG_DISABLE;\n\t\tvring_packed->vring.driver->flags =\n\t\t\tcpu_to_le16(vring_packed->event_flags_shadow);\n\t}\n}\n\nstatic void virtqueue_vring_attach_packed(struct vring_virtqueue *vq,\n\t\t\t\t\t  struct vring_virtqueue_packed *vring_packed)\n{\n\tvq->packed = *vring_packed;\n\n\t \n\tvq->free_head = 0;\n}\n\nstatic void virtqueue_reinit_packed(struct vring_virtqueue *vq)\n{\n\tmemset(vq->packed.vring.device, 0, vq->packed.event_size_in_bytes);\n\tmemset(vq->packed.vring.driver, 0, vq->packed.event_size_in_bytes);\n\n\t \n\tmemset(vq->packed.vring.desc, 0, vq->packed.ring_size_in_bytes);\n\n\tvirtqueue_init(vq, vq->packed.vring.num);\n\tvirtqueue_vring_init_packed(&vq->packed, !!vq->vq.callback);\n}\n\nstatic struct virtqueue *vring_create_virtqueue_packed(\n\tunsigned int index,\n\tunsigned int num,\n\tunsigned int vring_align,\n\tstruct virtio_device *vdev,\n\tbool weak_barriers,\n\tbool may_reduce_num,\n\tbool context,\n\tbool (*notify)(struct virtqueue *),\n\tvoid (*callback)(struct virtqueue *),\n\tconst char *name,\n\tstruct device *dma_dev)\n{\n\tstruct vring_virtqueue_packed vring_packed = {};\n\tstruct vring_virtqueue *vq;\n\tint err;\n\n\tif (vring_alloc_queue_packed(&vring_packed, vdev, num, dma_dev))\n\t\tgoto err_ring;\n\n\tvq = kmalloc(sizeof(*vq), GFP_KERNEL);\n\tif (!vq)\n\t\tgoto err_vq;\n\n\tvq->vq.callback = callback;\n\tvq->vq.vdev = vdev;\n\tvq->vq.name = name;\n\tvq->vq.index = index;\n\tvq->vq.reset = false;\n\tvq->we_own_ring = true;\n\tvq->notify = notify;\n\tvq->weak_barriers = weak_barriers;\n#ifdef CONFIG_VIRTIO_HARDEN_NOTIFICATION\n\tvq->broken = true;\n#else\n\tvq->broken = false;\n#endif\n\tvq->packed_ring = true;\n\tvq->dma_dev = dma_dev;\n\tvq->use_dma_api = vring_use_dma_api(vdev);\n\tvq->premapped = false;\n\tvq->do_unmap = vq->use_dma_api;\n\n\tvq->indirect = virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC) &&\n\t\t!context;\n\tvq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_ORDER_PLATFORM))\n\t\tvq->weak_barriers = false;\n\n\terr = vring_alloc_state_extra_packed(&vring_packed);\n\tif (err)\n\t\tgoto err_state_extra;\n\n\tvirtqueue_vring_init_packed(&vring_packed, !!callback);\n\n\tvirtqueue_init(vq, num);\n\tvirtqueue_vring_attach_packed(vq, &vring_packed);\n\n\tspin_lock(&vdev->vqs_list_lock);\n\tlist_add_tail(&vq->vq.list, &vdev->vqs);\n\tspin_unlock(&vdev->vqs_list_lock);\n\treturn &vq->vq;\n\nerr_state_extra:\n\tkfree(vq);\nerr_vq:\n\tvring_free_packed(&vring_packed, vdev, dma_dev);\nerr_ring:\n\treturn NULL;\n}\n\nstatic int virtqueue_resize_packed(struct virtqueue *_vq, u32 num)\n{\n\tstruct vring_virtqueue_packed vring_packed = {};\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct virtio_device *vdev = _vq->vdev;\n\tint err;\n\n\tif (vring_alloc_queue_packed(&vring_packed, vdev, num, vring_dma_dev(vq)))\n\t\tgoto err_ring;\n\n\terr = vring_alloc_state_extra_packed(&vring_packed);\n\tif (err)\n\t\tgoto err_state_extra;\n\n\tvring_free(&vq->vq);\n\n\tvirtqueue_vring_init_packed(&vring_packed, !!vq->vq.callback);\n\n\tvirtqueue_init(vq, vring_packed.vring.num);\n\tvirtqueue_vring_attach_packed(vq, &vring_packed);\n\n\treturn 0;\n\nerr_state_extra:\n\tvring_free_packed(&vring_packed, vdev, vring_dma_dev(vq));\nerr_ring:\n\tvirtqueue_reinit_packed(vq);\n\treturn -ENOMEM;\n}\n\nstatic int virtqueue_disable_and_recycle(struct virtqueue *_vq,\n\t\t\t\t\t void (*recycle)(struct virtqueue *vq, void *buf))\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct virtio_device *vdev = vq->vq.vdev;\n\tvoid *buf;\n\tint err;\n\n\tif (!vq->we_own_ring)\n\t\treturn -EPERM;\n\n\tif (!vdev->config->disable_vq_and_reset)\n\t\treturn -ENOENT;\n\n\tif (!vdev->config->enable_vq_after_reset)\n\t\treturn -ENOENT;\n\n\terr = vdev->config->disable_vq_and_reset(_vq);\n\tif (err)\n\t\treturn err;\n\n\twhile ((buf = virtqueue_detach_unused_buf(_vq)) != NULL)\n\t\trecycle(_vq, buf);\n\n\treturn 0;\n}\n\nstatic int virtqueue_enable_after_reset(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct virtio_device *vdev = vq->vq.vdev;\n\n\tif (vdev->config->enable_vq_after_reset(_vq))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n \n\nstatic inline int virtqueue_add(struct virtqueue *_vq,\n\t\t\t\tstruct scatterlist *sgs[],\n\t\t\t\tunsigned int total_sg,\n\t\t\t\tunsigned int out_sgs,\n\t\t\t\tunsigned int in_sgs,\n\t\t\t\tvoid *data,\n\t\t\t\tvoid *ctx,\n\t\t\t\tgfp_t gfp)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn vq->packed_ring ? virtqueue_add_packed(_vq, sgs, total_sg,\n\t\t\t\t\tout_sgs, in_sgs, data, ctx, gfp) :\n\t\t\t\t virtqueue_add_split(_vq, sgs, total_sg,\n\t\t\t\t\tout_sgs, in_sgs, data, ctx, gfp);\n}\n\n \nint virtqueue_add_sgs(struct virtqueue *_vq,\n\t\t      struct scatterlist *sgs[],\n\t\t      unsigned int out_sgs,\n\t\t      unsigned int in_sgs,\n\t\t      void *data,\n\t\t      gfp_t gfp)\n{\n\tunsigned int i, total_sg = 0;\n\n\t \n\tfor (i = 0; i < out_sgs + in_sgs; i++) {\n\t\tstruct scatterlist *sg;\n\n\t\tfor (sg = sgs[i]; sg; sg = sg_next(sg))\n\t\t\ttotal_sg++;\n\t}\n\treturn virtqueue_add(_vq, sgs, total_sg, out_sgs, in_sgs,\n\t\t\t     data, NULL, gfp);\n}\nEXPORT_SYMBOL_GPL(virtqueue_add_sgs);\n\n \nint virtqueue_add_outbuf(struct virtqueue *vq,\n\t\t\t struct scatterlist *sg, unsigned int num,\n\t\t\t void *data,\n\t\t\t gfp_t gfp)\n{\n\treturn virtqueue_add(vq, &sg, num, 1, 0, data, NULL, gfp);\n}\nEXPORT_SYMBOL_GPL(virtqueue_add_outbuf);\n\n \nint virtqueue_add_inbuf(struct virtqueue *vq,\n\t\t\tstruct scatterlist *sg, unsigned int num,\n\t\t\tvoid *data,\n\t\t\tgfp_t gfp)\n{\n\treturn virtqueue_add(vq, &sg, num, 0, 1, data, NULL, gfp);\n}\nEXPORT_SYMBOL_GPL(virtqueue_add_inbuf);\n\n \nint virtqueue_add_inbuf_ctx(struct virtqueue *vq,\n\t\t\tstruct scatterlist *sg, unsigned int num,\n\t\t\tvoid *data,\n\t\t\tvoid *ctx,\n\t\t\tgfp_t gfp)\n{\n\treturn virtqueue_add(vq, &sg, num, 0, 1, data, ctx, gfp);\n}\nEXPORT_SYMBOL_GPL(virtqueue_add_inbuf_ctx);\n\n \nstruct device *virtqueue_dma_dev(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->use_dma_api)\n\t\treturn vring_dma_dev(vq);\n\telse\n\t\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_dev);\n\n \nbool virtqueue_kick_prepare(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn vq->packed_ring ? virtqueue_kick_prepare_packed(_vq) :\n\t\t\t\t virtqueue_kick_prepare_split(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_kick_prepare);\n\n \nbool virtqueue_notify(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (unlikely(vq->broken))\n\t\treturn false;\n\n\t \n\tif (!vq->notify(_vq)) {\n\t\tvq->broken = true;\n\t\treturn false;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(virtqueue_notify);\n\n \nbool virtqueue_kick(struct virtqueue *vq)\n{\n\tif (virtqueue_kick_prepare(vq))\n\t\treturn virtqueue_notify(vq);\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(virtqueue_kick);\n\n \nvoid *virtqueue_get_buf_ctx(struct virtqueue *_vq, unsigned int *len,\n\t\t\t    void **ctx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn vq->packed_ring ? virtqueue_get_buf_ctx_packed(_vq, len, ctx) :\n\t\t\t\t virtqueue_get_buf_ctx_split(_vq, len, ctx);\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_buf_ctx);\n\nvoid *virtqueue_get_buf(struct virtqueue *_vq, unsigned int *len)\n{\n\treturn virtqueue_get_buf_ctx(_vq, len, NULL);\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_buf);\n \nvoid virtqueue_disable_cb(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->packed_ring)\n\t\tvirtqueue_disable_cb_packed(_vq);\n\telse\n\t\tvirtqueue_disable_cb_split(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_disable_cb);\n\n \nunsigned int virtqueue_enable_cb_prepare(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->event_triggered)\n\t\tvq->event_triggered = false;\n\n\treturn vq->packed_ring ? virtqueue_enable_cb_prepare_packed(_vq) :\n\t\t\t\t virtqueue_enable_cb_prepare_split(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb_prepare);\n\n \nbool virtqueue_poll(struct virtqueue *_vq, unsigned int last_used_idx)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (unlikely(vq->broken))\n\t\treturn false;\n\n\tvirtio_mb(vq->weak_barriers);\n\treturn vq->packed_ring ? virtqueue_poll_packed(_vq, last_used_idx) :\n\t\t\t\t virtqueue_poll_split(_vq, last_used_idx);\n}\nEXPORT_SYMBOL_GPL(virtqueue_poll);\n\n \nbool virtqueue_enable_cb(struct virtqueue *_vq)\n{\n\tunsigned int last_used_idx = virtqueue_enable_cb_prepare(_vq);\n\n\treturn !virtqueue_poll(_vq, last_used_idx);\n}\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb);\n\n \nbool virtqueue_enable_cb_delayed(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->event_triggered)\n\t\tvq->event_triggered = false;\n\n\treturn vq->packed_ring ? virtqueue_enable_cb_delayed_packed(_vq) :\n\t\t\t\t virtqueue_enable_cb_delayed_split(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_enable_cb_delayed);\n\n \nvoid *virtqueue_detach_unused_buf(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn vq->packed_ring ? virtqueue_detach_unused_buf_packed(_vq) :\n\t\t\t\t virtqueue_detach_unused_buf_split(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_detach_unused_buf);\n\nstatic inline bool more_used(const struct vring_virtqueue *vq)\n{\n\treturn vq->packed_ring ? more_used_packed(vq) : more_used_split(vq);\n}\n\n \nirqreturn_t vring_interrupt(int irq, void *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!more_used(vq)) {\n\t\tpr_debug(\"virtqueue interrupt with no work for %p\\n\", vq);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (unlikely(vq->broken)) {\n#ifdef CONFIG_VIRTIO_HARDEN_NOTIFICATION\n\t\tdev_warn_once(&vq->vq.vdev->dev,\n\t\t\t      \"virtio vring IRQ raised before DRIVER_OK\");\n\t\treturn IRQ_NONE;\n#else\n\t\treturn IRQ_HANDLED;\n#endif\n\t}\n\n\t \n\tif (vq->event)\n\t\tvq->event_triggered = true;\n\n\tpr_debug(\"virtqueue callback for %p (%p)\\n\", vq, vq->vq.callback);\n\tif (vq->vq.callback)\n\t\tvq->vq.callback(&vq->vq);\n\n\treturn IRQ_HANDLED;\n}\nEXPORT_SYMBOL_GPL(vring_interrupt);\n\n \nstatic struct virtqueue *__vring_new_virtqueue(unsigned int index,\n\t\t\t\t\t       struct vring_virtqueue_split *vring_split,\n\t\t\t\t\t       struct virtio_device *vdev,\n\t\t\t\t\t       bool weak_barriers,\n\t\t\t\t\t       bool context,\n\t\t\t\t\t       bool (*notify)(struct virtqueue *),\n\t\t\t\t\t       void (*callback)(struct virtqueue *),\n\t\t\t\t\t       const char *name,\n\t\t\t\t\t       struct device *dma_dev)\n{\n\tstruct vring_virtqueue *vq;\n\tint err;\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED))\n\t\treturn NULL;\n\n\tvq = kmalloc(sizeof(*vq), GFP_KERNEL);\n\tif (!vq)\n\t\treturn NULL;\n\n\tvq->packed_ring = false;\n\tvq->vq.callback = callback;\n\tvq->vq.vdev = vdev;\n\tvq->vq.name = name;\n\tvq->vq.index = index;\n\tvq->vq.reset = false;\n\tvq->we_own_ring = false;\n\tvq->notify = notify;\n\tvq->weak_barriers = weak_barriers;\n#ifdef CONFIG_VIRTIO_HARDEN_NOTIFICATION\n\tvq->broken = true;\n#else\n\tvq->broken = false;\n#endif\n\tvq->dma_dev = dma_dev;\n\tvq->use_dma_api = vring_use_dma_api(vdev);\n\tvq->premapped = false;\n\tvq->do_unmap = vq->use_dma_api;\n\n\tvq->indirect = virtio_has_feature(vdev, VIRTIO_RING_F_INDIRECT_DESC) &&\n\t\t!context;\n\tvq->event = virtio_has_feature(vdev, VIRTIO_RING_F_EVENT_IDX);\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_ORDER_PLATFORM))\n\t\tvq->weak_barriers = false;\n\n\terr = vring_alloc_state_extra_split(vring_split);\n\tif (err) {\n\t\tkfree(vq);\n\t\treturn NULL;\n\t}\n\n\tvirtqueue_vring_init_split(vring_split, vq);\n\n\tvirtqueue_init(vq, vring_split->vring.num);\n\tvirtqueue_vring_attach_split(vq, vring_split);\n\n\tspin_lock(&vdev->vqs_list_lock);\n\tlist_add_tail(&vq->vq.list, &vdev->vqs);\n\tspin_unlock(&vdev->vqs_list_lock);\n\treturn &vq->vq;\n}\n\nstruct virtqueue *vring_create_virtqueue(\n\tunsigned int index,\n\tunsigned int num,\n\tunsigned int vring_align,\n\tstruct virtio_device *vdev,\n\tbool weak_barriers,\n\tbool may_reduce_num,\n\tbool context,\n\tbool (*notify)(struct virtqueue *),\n\tvoid (*callback)(struct virtqueue *),\n\tconst char *name)\n{\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED))\n\t\treturn vring_create_virtqueue_packed(index, num, vring_align,\n\t\t\t\tvdev, weak_barriers, may_reduce_num,\n\t\t\t\tcontext, notify, callback, name, vdev->dev.parent);\n\n\treturn vring_create_virtqueue_split(index, num, vring_align,\n\t\t\tvdev, weak_barriers, may_reduce_num,\n\t\t\tcontext, notify, callback, name, vdev->dev.parent);\n}\nEXPORT_SYMBOL_GPL(vring_create_virtqueue);\n\nstruct virtqueue *vring_create_virtqueue_dma(\n\tunsigned int index,\n\tunsigned int num,\n\tunsigned int vring_align,\n\tstruct virtio_device *vdev,\n\tbool weak_barriers,\n\tbool may_reduce_num,\n\tbool context,\n\tbool (*notify)(struct virtqueue *),\n\tvoid (*callback)(struct virtqueue *),\n\tconst char *name,\n\tstruct device *dma_dev)\n{\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED))\n\t\treturn vring_create_virtqueue_packed(index, num, vring_align,\n\t\t\t\tvdev, weak_barriers, may_reduce_num,\n\t\t\t\tcontext, notify, callback, name, dma_dev);\n\n\treturn vring_create_virtqueue_split(index, num, vring_align,\n\t\t\tvdev, weak_barriers, may_reduce_num,\n\t\t\tcontext, notify, callback, name, dma_dev);\n}\nEXPORT_SYMBOL_GPL(vring_create_virtqueue_dma);\n\n \nint virtqueue_resize(struct virtqueue *_vq, u32 num,\n\t\t     void (*recycle)(struct virtqueue *vq, void *buf))\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tint err;\n\n\tif (num > vq->vq.num_max)\n\t\treturn -E2BIG;\n\n\tif (!num)\n\t\treturn -EINVAL;\n\n\tif ((vq->packed_ring ? vq->packed.vring.num : vq->split.vring.num) == num)\n\t\treturn 0;\n\n\terr = virtqueue_disable_and_recycle(_vq, recycle);\n\tif (err)\n\t\treturn err;\n\n\tif (vq->packed_ring)\n\t\terr = virtqueue_resize_packed(_vq, num);\n\telse\n\t\terr = virtqueue_resize_split(_vq, num);\n\n\treturn virtqueue_enable_after_reset(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_resize);\n\n \nint virtqueue_set_dma_premapped(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu32 num;\n\n\tSTART_USE(vq);\n\n\tnum = vq->packed_ring ? vq->packed.vring.num : vq->split.vring.num;\n\n\tif (num != vq->vq.num_free) {\n\t\tEND_USE(vq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!vq->use_dma_api) {\n\t\tEND_USE(vq);\n\t\treturn -EINVAL;\n\t}\n\n\tvq->premapped = true;\n\tvq->do_unmap = false;\n\n\tEND_USE(vq);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(virtqueue_set_dma_premapped);\n\n \nint virtqueue_reset(struct virtqueue *_vq,\n\t\t    void (*recycle)(struct virtqueue *vq, void *buf))\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tint err;\n\n\terr = virtqueue_disable_and_recycle(_vq, recycle);\n\tif (err)\n\t\treturn err;\n\n\tif (vq->packed_ring)\n\t\tvirtqueue_reinit_packed(vq);\n\telse\n\t\tvirtqueue_reinit_split(vq);\n\n\treturn virtqueue_enable_after_reset(_vq);\n}\nEXPORT_SYMBOL_GPL(virtqueue_reset);\n\n \nstruct virtqueue *vring_new_virtqueue(unsigned int index,\n\t\t\t\t      unsigned int num,\n\t\t\t\t      unsigned int vring_align,\n\t\t\t\t      struct virtio_device *vdev,\n\t\t\t\t      bool weak_barriers,\n\t\t\t\t      bool context,\n\t\t\t\t      void *pages,\n\t\t\t\t      bool (*notify)(struct virtqueue *vq),\n\t\t\t\t      void (*callback)(struct virtqueue *vq),\n\t\t\t\t      const char *name)\n{\n\tstruct vring_virtqueue_split vring_split = {};\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_RING_PACKED))\n\t\treturn NULL;\n\n\tvring_init(&vring_split.vring, num, pages, vring_align);\n\treturn __vring_new_virtqueue(index, &vring_split, vdev, weak_barriers,\n\t\t\t\t     context, notify, callback, name,\n\t\t\t\t     vdev->dev.parent);\n}\nEXPORT_SYMBOL_GPL(vring_new_virtqueue);\n\nstatic void vring_free(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (vq->we_own_ring) {\n\t\tif (vq->packed_ring) {\n\t\t\tvring_free_queue(vq->vq.vdev,\n\t\t\t\t\t vq->packed.ring_size_in_bytes,\n\t\t\t\t\t vq->packed.vring.desc,\n\t\t\t\t\t vq->packed.ring_dma_addr,\n\t\t\t\t\t vring_dma_dev(vq));\n\n\t\t\tvring_free_queue(vq->vq.vdev,\n\t\t\t\t\t vq->packed.event_size_in_bytes,\n\t\t\t\t\t vq->packed.vring.driver,\n\t\t\t\t\t vq->packed.driver_event_dma_addr,\n\t\t\t\t\t vring_dma_dev(vq));\n\n\t\t\tvring_free_queue(vq->vq.vdev,\n\t\t\t\t\t vq->packed.event_size_in_bytes,\n\t\t\t\t\t vq->packed.vring.device,\n\t\t\t\t\t vq->packed.device_event_dma_addr,\n\t\t\t\t\t vring_dma_dev(vq));\n\n\t\t\tkfree(vq->packed.desc_state);\n\t\t\tkfree(vq->packed.desc_extra);\n\t\t} else {\n\t\t\tvring_free_queue(vq->vq.vdev,\n\t\t\t\t\t vq->split.queue_size_in_bytes,\n\t\t\t\t\t vq->split.vring.desc,\n\t\t\t\t\t vq->split.queue_dma_addr,\n\t\t\t\t\t vring_dma_dev(vq));\n\t\t}\n\t}\n\tif (!vq->packed_ring) {\n\t\tkfree(vq->split.desc_state);\n\t\tkfree(vq->split.desc_extra);\n\t}\n}\n\nvoid vring_del_virtqueue(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tspin_lock(&vq->vq.vdev->vqs_list_lock);\n\tlist_del(&_vq->list);\n\tspin_unlock(&vq->vq.vdev->vqs_list_lock);\n\n\tvring_free(_vq);\n\n\tkfree(vq);\n}\nEXPORT_SYMBOL_GPL(vring_del_virtqueue);\n\nu32 vring_notification_data(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tu16 next;\n\n\tif (vq->packed_ring)\n\t\tnext = (vq->packed.next_avail_idx &\n\t\t\t\t~(-(1 << VRING_PACKED_EVENT_F_WRAP_CTR))) |\n\t\t\tvq->packed.avail_wrap_counter <<\n\t\t\t\tVRING_PACKED_EVENT_F_WRAP_CTR;\n\telse\n\t\tnext = vq->split.avail_idx_shadow;\n\n\treturn next << 16 | _vq->index;\n}\nEXPORT_SYMBOL_GPL(vring_notification_data);\n\n \nvoid vring_transport_features(struct virtio_device *vdev)\n{\n\tunsigned int i;\n\n\tfor (i = VIRTIO_TRANSPORT_F_START; i < VIRTIO_TRANSPORT_F_END; i++) {\n\t\tswitch (i) {\n\t\tcase VIRTIO_RING_F_INDIRECT_DESC:\n\t\t\tbreak;\n\t\tcase VIRTIO_RING_F_EVENT_IDX:\n\t\t\tbreak;\n\t\tcase VIRTIO_F_VERSION_1:\n\t\t\tbreak;\n\t\tcase VIRTIO_F_ACCESS_PLATFORM:\n\t\t\tbreak;\n\t\tcase VIRTIO_F_RING_PACKED:\n\t\t\tbreak;\n\t\tcase VIRTIO_F_ORDER_PLATFORM:\n\t\t\tbreak;\n\t\tcase VIRTIO_F_NOTIFICATION_DATA:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\t__virtio_clear_bit(vdev, i);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL_GPL(vring_transport_features);\n\n \nunsigned int virtqueue_get_vring_size(const struct virtqueue *_vq)\n{\n\n\tconst struct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn vq->packed_ring ? vq->packed.vring.num : vq->split.vring.num;\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_vring_size);\n\n \nvoid __virtqueue_break(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\t \n\tWRITE_ONCE(vq->broken, true);\n}\nEXPORT_SYMBOL_GPL(__virtqueue_break);\n\n \nvoid __virtqueue_unbreak(struct virtqueue *_vq)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\t \n\tWRITE_ONCE(vq->broken, false);\n}\nEXPORT_SYMBOL_GPL(__virtqueue_unbreak);\n\nbool virtqueue_is_broken(const struct virtqueue *_vq)\n{\n\tconst struct vring_virtqueue *vq = to_vvq(_vq);\n\n\treturn READ_ONCE(vq->broken);\n}\nEXPORT_SYMBOL_GPL(virtqueue_is_broken);\n\n \nvoid virtio_break_device(struct virtio_device *dev)\n{\n\tstruct virtqueue *_vq;\n\n\tspin_lock(&dev->vqs_list_lock);\n\tlist_for_each_entry(_vq, &dev->vqs, list) {\n\t\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\t\t \n\t\tWRITE_ONCE(vq->broken, true);\n\t}\n\tspin_unlock(&dev->vqs_list_lock);\n}\nEXPORT_SYMBOL_GPL(virtio_break_device);\n\n \nvoid __virtio_unbreak_device(struct virtio_device *dev)\n{\n\tstruct virtqueue *_vq;\n\n\tspin_lock(&dev->vqs_list_lock);\n\tlist_for_each_entry(_vq, &dev->vqs, list) {\n\t\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\t\t \n\t\tWRITE_ONCE(vq->broken, false);\n\t}\n\tspin_unlock(&dev->vqs_list_lock);\n}\nEXPORT_SYMBOL_GPL(__virtio_unbreak_device);\n\ndma_addr_t virtqueue_get_desc_addr(const struct virtqueue *_vq)\n{\n\tconst struct vring_virtqueue *vq = to_vvq(_vq);\n\n\tBUG_ON(!vq->we_own_ring);\n\n\tif (vq->packed_ring)\n\t\treturn vq->packed.ring_dma_addr;\n\n\treturn vq->split.queue_dma_addr;\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_desc_addr);\n\ndma_addr_t virtqueue_get_avail_addr(const struct virtqueue *_vq)\n{\n\tconst struct vring_virtqueue *vq = to_vvq(_vq);\n\n\tBUG_ON(!vq->we_own_ring);\n\n\tif (vq->packed_ring)\n\t\treturn vq->packed.driver_event_dma_addr;\n\n\treturn vq->split.queue_dma_addr +\n\t\t((char *)vq->split.vring.avail - (char *)vq->split.vring.desc);\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_avail_addr);\n\ndma_addr_t virtqueue_get_used_addr(const struct virtqueue *_vq)\n{\n\tconst struct vring_virtqueue *vq = to_vvq(_vq);\n\n\tBUG_ON(!vq->we_own_ring);\n\n\tif (vq->packed_ring)\n\t\treturn vq->packed.device_event_dma_addr;\n\n\treturn vq->split.queue_dma_addr +\n\t\t((char *)vq->split.vring.used - (char *)vq->split.vring.desc);\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_used_addr);\n\n \nconst struct vring *virtqueue_get_vring(const struct virtqueue *vq)\n{\n\treturn &to_vvq(vq)->split.vring;\n}\nEXPORT_SYMBOL_GPL(virtqueue_get_vring);\n\n \ndma_addr_t virtqueue_dma_map_single_attrs(struct virtqueue *_vq, void *ptr,\n\t\t\t\t\t  size_t size,\n\t\t\t\t\t  enum dma_data_direction dir,\n\t\t\t\t\t  unsigned long attrs)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn (dma_addr_t)virt_to_phys(ptr);\n\n\treturn dma_map_single_attrs(vring_dma_dev(vq), ptr, size, dir, attrs);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_map_single_attrs);\n\n \nvoid virtqueue_dma_unmap_single_attrs(struct virtqueue *_vq, dma_addr_t addr,\n\t\t\t\t      size_t size, enum dma_data_direction dir,\n\t\t\t\t      unsigned long attrs)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn;\n\n\tdma_unmap_single_attrs(vring_dma_dev(vq), addr, size, dir, attrs);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_unmap_single_attrs);\n\n \nint virtqueue_dma_mapping_error(struct virtqueue *_vq, dma_addr_t addr)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn 0;\n\n\treturn dma_mapping_error(vring_dma_dev(vq), addr);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_mapping_error);\n\n \nbool virtqueue_dma_need_sync(struct virtqueue *_vq, dma_addr_t addr)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn false;\n\n\treturn dma_need_sync(vring_dma_dev(vq), addr);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_need_sync);\n\n \nvoid virtqueue_dma_sync_single_range_for_cpu(struct virtqueue *_vq,\n\t\t\t\t\t     dma_addr_t addr,\n\t\t\t\t\t     unsigned long offset, size_t size,\n\t\t\t\t\t     enum dma_data_direction dir)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct device *dev = vring_dma_dev(vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn;\n\n\tdma_sync_single_range_for_cpu(dev, addr, offset, size, dir);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_sync_single_range_for_cpu);\n\n \nvoid virtqueue_dma_sync_single_range_for_device(struct virtqueue *_vq,\n\t\t\t\t\t\tdma_addr_t addr,\n\t\t\t\t\t\tunsigned long offset, size_t size,\n\t\t\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct vring_virtqueue *vq = to_vvq(_vq);\n\tstruct device *dev = vring_dma_dev(vq);\n\n\tif (!vq->use_dma_api)\n\t\treturn;\n\n\tdma_sync_single_range_for_device(dev, addr, offset, size, dir);\n}\nEXPORT_SYMBOL_GPL(virtqueue_dma_sync_single_range_for_device);\n\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}