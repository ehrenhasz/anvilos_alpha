{
  "module_name": "industrialio-buffer-dma.c",
  "hash_id": "b714a3a5e3cb43a2f6e76b6d07046785da583c60e24dedd55bd244964a74c3e2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iio/buffer/industrialio-buffer-dma.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/device.h>\n#include <linux/workqueue.h>\n#include <linux/mutex.h>\n#include <linux/sched.h>\n#include <linux/poll.h>\n#include <linux/iio/buffer_impl.h>\n#include <linux/iio/buffer-dma.h>\n#include <linux/dma-mapping.h>\n#include <linux/sizes.h>\n\n \n\nstatic void iio_buffer_block_release(struct kref *kref)\n{\n\tstruct iio_dma_buffer_block *block = container_of(kref,\n\t\tstruct iio_dma_buffer_block, kref);\n\n\tWARN_ON(block->state != IIO_BLOCK_STATE_DEAD);\n\n\tdma_free_coherent(block->queue->dev, PAGE_ALIGN(block->size),\n\t\t\t\t\tblock->vaddr, block->phys_addr);\n\n\tiio_buffer_put(&block->queue->buffer);\n\tkfree(block);\n}\n\nstatic void iio_buffer_block_get(struct iio_dma_buffer_block *block)\n{\n\tkref_get(&block->kref);\n}\n\nstatic void iio_buffer_block_put(struct iio_dma_buffer_block *block)\n{\n\tkref_put(&block->kref, iio_buffer_block_release);\n}\n\n \nstatic LIST_HEAD(iio_dma_buffer_dead_blocks);\nstatic DEFINE_SPINLOCK(iio_dma_buffer_dead_blocks_lock);\n\nstatic void iio_dma_buffer_cleanup_worker(struct work_struct *work)\n{\n\tstruct iio_dma_buffer_block *block, *_block;\n\tLIST_HEAD(block_list);\n\n\tspin_lock_irq(&iio_dma_buffer_dead_blocks_lock);\n\tlist_splice_tail_init(&iio_dma_buffer_dead_blocks, &block_list);\n\tspin_unlock_irq(&iio_dma_buffer_dead_blocks_lock);\n\n\tlist_for_each_entry_safe(block, _block, &block_list, head)\n\t\tiio_buffer_block_release(&block->kref);\n}\nstatic DECLARE_WORK(iio_dma_buffer_cleanup_work, iio_dma_buffer_cleanup_worker);\n\nstatic void iio_buffer_block_release_atomic(struct kref *kref)\n{\n\tstruct iio_dma_buffer_block *block;\n\tunsigned long flags;\n\n\tblock = container_of(kref, struct iio_dma_buffer_block, kref);\n\n\tspin_lock_irqsave(&iio_dma_buffer_dead_blocks_lock, flags);\n\tlist_add_tail(&block->head, &iio_dma_buffer_dead_blocks);\n\tspin_unlock_irqrestore(&iio_dma_buffer_dead_blocks_lock, flags);\n\n\tschedule_work(&iio_dma_buffer_cleanup_work);\n}\n\n \nstatic void iio_buffer_block_put_atomic(struct iio_dma_buffer_block *block)\n{\n\tkref_put(&block->kref, iio_buffer_block_release_atomic);\n}\n\nstatic struct iio_dma_buffer_queue *iio_buffer_to_queue(struct iio_buffer *buf)\n{\n\treturn container_of(buf, struct iio_dma_buffer_queue, buffer);\n}\n\nstatic struct iio_dma_buffer_block *iio_dma_buffer_alloc_block(\n\tstruct iio_dma_buffer_queue *queue, size_t size)\n{\n\tstruct iio_dma_buffer_block *block;\n\n\tblock = kzalloc(sizeof(*block), GFP_KERNEL);\n\tif (!block)\n\t\treturn NULL;\n\n\tblock->vaddr = dma_alloc_coherent(queue->dev, PAGE_ALIGN(size),\n\t\t&block->phys_addr, GFP_KERNEL);\n\tif (!block->vaddr) {\n\t\tkfree(block);\n\t\treturn NULL;\n\t}\n\n\tblock->size = size;\n\tblock->state = IIO_BLOCK_STATE_DEQUEUED;\n\tblock->queue = queue;\n\tINIT_LIST_HEAD(&block->head);\n\tkref_init(&block->kref);\n\n\tiio_buffer_get(&queue->buffer);\n\n\treturn block;\n}\n\nstatic void _iio_dma_buffer_block_done(struct iio_dma_buffer_block *block)\n{\n\tstruct iio_dma_buffer_queue *queue = block->queue;\n\n\t \n\tif (block->state != IIO_BLOCK_STATE_DEAD) {\n\t\tblock->state = IIO_BLOCK_STATE_DONE;\n\t\tlist_add_tail(&block->head, &queue->outgoing);\n\t}\n}\n\n \nvoid iio_dma_buffer_block_done(struct iio_dma_buffer_block *block)\n{\n\tstruct iio_dma_buffer_queue *queue = block->queue;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->list_lock, flags);\n\t_iio_dma_buffer_block_done(block);\n\tspin_unlock_irqrestore(&queue->list_lock, flags);\n\n\tiio_buffer_block_put_atomic(block);\n\twake_up_interruptible_poll(&queue->buffer.pollq, EPOLLIN | EPOLLRDNORM);\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_block_done);\n\n \nvoid iio_dma_buffer_block_list_abort(struct iio_dma_buffer_queue *queue,\n\tstruct list_head *list)\n{\n\tstruct iio_dma_buffer_block *block, *_block;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->list_lock, flags);\n\tlist_for_each_entry_safe(block, _block, list, head) {\n\t\tlist_del(&block->head);\n\t\tblock->bytes_used = 0;\n\t\t_iio_dma_buffer_block_done(block);\n\t\tiio_buffer_block_put_atomic(block);\n\t}\n\tspin_unlock_irqrestore(&queue->list_lock, flags);\n\n\twake_up_interruptible_poll(&queue->buffer.pollq, EPOLLIN | EPOLLRDNORM);\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_block_list_abort);\n\nstatic bool iio_dma_block_reusable(struct iio_dma_buffer_block *block)\n{\n\t \n\tswitch (block->state) {\n\tcase IIO_BLOCK_STATE_DEQUEUED:\n\tcase IIO_BLOCK_STATE_QUEUED:\n\tcase IIO_BLOCK_STATE_DONE:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nint iio_dma_buffer_request_update(struct iio_buffer *buffer)\n{\n\tstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\n\tstruct iio_dma_buffer_block *block;\n\tbool try_reuse = false;\n\tsize_t size;\n\tint ret = 0;\n\tint i;\n\n\t \n\tsize = DIV_ROUND_UP(queue->buffer.bytes_per_datum *\n\t\tqueue->buffer.length, 2);\n\n\tmutex_lock(&queue->lock);\n\n\t \n\tif (PAGE_ALIGN(queue->fileio.block_size) == PAGE_ALIGN(size))\n\t\ttry_reuse = true;\n\n\tqueue->fileio.block_size = size;\n\tqueue->fileio.active_block = NULL;\n\n\tspin_lock_irq(&queue->list_lock);\n\tfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\n\t\tblock = queue->fileio.blocks[i];\n\n\t\t \n\t\tif (block && (!iio_dma_block_reusable(block) || !try_reuse))\n\t\t\tblock->state = IIO_BLOCK_STATE_DEAD;\n\t}\n\n\t \n\tINIT_LIST_HEAD(&queue->outgoing);\n\tspin_unlock_irq(&queue->list_lock);\n\n\tINIT_LIST_HEAD(&queue->incoming);\n\n\tfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\n\t\tif (queue->fileio.blocks[i]) {\n\t\t\tblock = queue->fileio.blocks[i];\n\t\t\tif (block->state == IIO_BLOCK_STATE_DEAD) {\n\t\t\t\t \n\t\t\t\tiio_buffer_block_put(block);\n\t\t\t\tblock = NULL;\n\t\t\t} else {\n\t\t\t\tblock->size = size;\n\t\t\t}\n\t\t} else {\n\t\t\tblock = NULL;\n\t\t}\n\n\t\tif (!block) {\n\t\t\tblock = iio_dma_buffer_alloc_block(queue, size);\n\t\t\tif (!block) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tqueue->fileio.blocks[i] = block;\n\t\t}\n\n\t\tblock->state = IIO_BLOCK_STATE_QUEUED;\n\t\tlist_add_tail(&block->head, &queue->incoming);\n\t}\n\nout_unlock:\n\tmutex_unlock(&queue->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_request_update);\n\nstatic void iio_dma_buffer_submit_block(struct iio_dma_buffer_queue *queue,\n\tstruct iio_dma_buffer_block *block)\n{\n\tint ret;\n\n\t \n\tif (!queue->ops)\n\t\treturn;\n\n\tblock->state = IIO_BLOCK_STATE_ACTIVE;\n\tiio_buffer_block_get(block);\n\tret = queue->ops->submit(queue, block);\n\tif (ret) {\n\t\t \n\t\tiio_buffer_block_put(block);\n\t}\n}\n\n \nint iio_dma_buffer_enable(struct iio_buffer *buffer,\n\tstruct iio_dev *indio_dev)\n{\n\tstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\n\tstruct iio_dma_buffer_block *block, *_block;\n\n\tmutex_lock(&queue->lock);\n\tqueue->active = true;\n\tlist_for_each_entry_safe(block, _block, &queue->incoming, head) {\n\t\tlist_del(&block->head);\n\t\tiio_dma_buffer_submit_block(queue, block);\n\t}\n\tmutex_unlock(&queue->lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_enable);\n\n \nint iio_dma_buffer_disable(struct iio_buffer *buffer,\n\tstruct iio_dev *indio_dev)\n{\n\tstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\n\n\tmutex_lock(&queue->lock);\n\tqueue->active = false;\n\n\tif (queue->ops && queue->ops->abort)\n\t\tqueue->ops->abort(queue);\n\tmutex_unlock(&queue->lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_disable);\n\nstatic void iio_dma_buffer_enqueue(struct iio_dma_buffer_queue *queue,\n\tstruct iio_dma_buffer_block *block)\n{\n\tif (block->state == IIO_BLOCK_STATE_DEAD) {\n\t\tiio_buffer_block_put(block);\n\t} else if (queue->active) {\n\t\tiio_dma_buffer_submit_block(queue, block);\n\t} else {\n\t\tblock->state = IIO_BLOCK_STATE_QUEUED;\n\t\tlist_add_tail(&block->head, &queue->incoming);\n\t}\n}\n\nstatic struct iio_dma_buffer_block *iio_dma_buffer_dequeue(\n\tstruct iio_dma_buffer_queue *queue)\n{\n\tstruct iio_dma_buffer_block *block;\n\n\tspin_lock_irq(&queue->list_lock);\n\tblock = list_first_entry_or_null(&queue->outgoing, struct\n\t\tiio_dma_buffer_block, head);\n\tif (block != NULL) {\n\t\tlist_del(&block->head);\n\t\tblock->state = IIO_BLOCK_STATE_DEQUEUED;\n\t}\n\tspin_unlock_irq(&queue->list_lock);\n\n\treturn block;\n}\n\n \nint iio_dma_buffer_read(struct iio_buffer *buffer, size_t n,\n\tchar __user *user_buffer)\n{\n\tstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buffer);\n\tstruct iio_dma_buffer_block *block;\n\tint ret;\n\n\tif (n < buffer->bytes_per_datum)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&queue->lock);\n\n\tif (!queue->fileio.active_block) {\n\t\tblock = iio_dma_buffer_dequeue(queue);\n\t\tif (block == NULL) {\n\t\t\tret = 0;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tqueue->fileio.pos = 0;\n\t\tqueue->fileio.active_block = block;\n\t} else {\n\t\tblock = queue->fileio.active_block;\n\t}\n\n\tn = rounddown(n, buffer->bytes_per_datum);\n\tif (n > block->bytes_used - queue->fileio.pos)\n\t\tn = block->bytes_used - queue->fileio.pos;\n\n\tif (copy_to_user(user_buffer, block->vaddr + queue->fileio.pos, n)) {\n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tqueue->fileio.pos += n;\n\n\tif (queue->fileio.pos == block->bytes_used) {\n\t\tqueue->fileio.active_block = NULL;\n\t\tiio_dma_buffer_enqueue(queue, block);\n\t}\n\n\tret = n;\n\nout_unlock:\n\tmutex_unlock(&queue->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_read);\n\n \nsize_t iio_dma_buffer_data_available(struct iio_buffer *buf)\n{\n\tstruct iio_dma_buffer_queue *queue = iio_buffer_to_queue(buf);\n\tstruct iio_dma_buffer_block *block;\n\tsize_t data_available = 0;\n\n\t \n\n\tmutex_lock(&queue->lock);\n\tif (queue->fileio.active_block)\n\t\tdata_available += queue->fileio.active_block->size;\n\n\tspin_lock_irq(&queue->list_lock);\n\tlist_for_each_entry(block, &queue->outgoing, head)\n\t\tdata_available += block->size;\n\tspin_unlock_irq(&queue->list_lock);\n\tmutex_unlock(&queue->lock);\n\n\treturn data_available;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_data_available);\n\n \nint iio_dma_buffer_set_bytes_per_datum(struct iio_buffer *buffer, size_t bpd)\n{\n\tbuffer->bytes_per_datum = bpd;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_set_bytes_per_datum);\n\n \nint iio_dma_buffer_set_length(struct iio_buffer *buffer, unsigned int length)\n{\n\t \n\tif (length < 2)\n\t\tlength = 2;\n\tbuffer->length = length;\n\tbuffer->watermark = length / 2;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_set_length);\n\n \nint iio_dma_buffer_init(struct iio_dma_buffer_queue *queue,\n\tstruct device *dev, const struct iio_dma_buffer_ops *ops)\n{\n\tiio_buffer_init(&queue->buffer);\n\tqueue->buffer.length = PAGE_SIZE;\n\tqueue->buffer.watermark = queue->buffer.length / 2;\n\tqueue->dev = dev;\n\tqueue->ops = ops;\n\n\tINIT_LIST_HEAD(&queue->incoming);\n\tINIT_LIST_HEAD(&queue->outgoing);\n\n\tmutex_init(&queue->lock);\n\tspin_lock_init(&queue->list_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_init);\n\n \nvoid iio_dma_buffer_exit(struct iio_dma_buffer_queue *queue)\n{\n\tunsigned int i;\n\n\tmutex_lock(&queue->lock);\n\n\tspin_lock_irq(&queue->list_lock);\n\tfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\n\t\tif (!queue->fileio.blocks[i])\n\t\t\tcontinue;\n\t\tqueue->fileio.blocks[i]->state = IIO_BLOCK_STATE_DEAD;\n\t}\n\tINIT_LIST_HEAD(&queue->outgoing);\n\tspin_unlock_irq(&queue->list_lock);\n\n\tINIT_LIST_HEAD(&queue->incoming);\n\n\tfor (i = 0; i < ARRAY_SIZE(queue->fileio.blocks); i++) {\n\t\tif (!queue->fileio.blocks[i])\n\t\t\tcontinue;\n\t\tiio_buffer_block_put(queue->fileio.blocks[i]);\n\t\tqueue->fileio.blocks[i] = NULL;\n\t}\n\tqueue->fileio.active_block = NULL;\n\tqueue->ops = NULL;\n\n\tmutex_unlock(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_exit);\n\n \nvoid iio_dma_buffer_release(struct iio_dma_buffer_queue *queue)\n{\n\tmutex_destroy(&queue->lock);\n}\nEXPORT_SYMBOL_GPL(iio_dma_buffer_release);\n\nMODULE_AUTHOR(\"Lars-Peter Clausen <lars@metafoo.de>\");\nMODULE_DESCRIPTION(\"DMA buffer for the IIO framework\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}