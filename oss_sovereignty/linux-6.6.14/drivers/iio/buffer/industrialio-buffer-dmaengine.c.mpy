{
  "module_name": "industrialio-buffer-dmaengine.c",
  "hash_id": "bb4cff14acd2820e7d7dba9bdf1060e140186cc668c7ccfc068cec74b067224a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/iio/buffer/industrialio-buffer-dmaengine.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/kernel.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-mapping.h>\n#include <linux/spinlock.h>\n#include <linux/err.h>\n#include <linux/module.h>\n\n#include <linux/iio/iio.h>\n#include <linux/iio/sysfs.h>\n#include <linux/iio/buffer.h>\n#include <linux/iio/buffer_impl.h>\n#include <linux/iio/buffer-dma.h>\n#include <linux/iio/buffer-dmaengine.h>\n\n \n\nstruct dmaengine_buffer {\n\tstruct iio_dma_buffer_queue queue;\n\n\tstruct dma_chan *chan;\n\tstruct list_head active;\n\n\tsize_t align;\n\tsize_t max_size;\n};\n\nstatic struct dmaengine_buffer *iio_buffer_to_dmaengine_buffer(\n\t\tstruct iio_buffer *buffer)\n{\n\treturn container_of(buffer, struct dmaengine_buffer, queue.buffer);\n}\n\nstatic void iio_dmaengine_buffer_block_done(void *data,\n\t\tconst struct dmaengine_result *result)\n{\n\tstruct iio_dma_buffer_block *block = data;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&block->queue->list_lock, flags);\n\tlist_del(&block->head);\n\tspin_unlock_irqrestore(&block->queue->list_lock, flags);\n\tblock->bytes_used -= result->residue;\n\tiio_dma_buffer_block_done(block);\n}\n\nstatic int iio_dmaengine_buffer_submit_block(struct iio_dma_buffer_queue *queue,\n\tstruct iio_dma_buffer_block *block)\n{\n\tstruct dmaengine_buffer *dmaengine_buffer =\n\t\tiio_buffer_to_dmaengine_buffer(&queue->buffer);\n\tstruct dma_async_tx_descriptor *desc;\n\tdma_cookie_t cookie;\n\n\tblock->bytes_used = min(block->size, dmaengine_buffer->max_size);\n\tblock->bytes_used = round_down(block->bytes_used,\n\t\t\tdmaengine_buffer->align);\n\n\tdesc = dmaengine_prep_slave_single(dmaengine_buffer->chan,\n\t\tblock->phys_addr, block->bytes_used, DMA_DEV_TO_MEM,\n\t\tDMA_PREP_INTERRUPT);\n\tif (!desc)\n\t\treturn -ENOMEM;\n\n\tdesc->callback_result = iio_dmaengine_buffer_block_done;\n\tdesc->callback_param = block;\n\n\tcookie = dmaengine_submit(desc);\n\tif (dma_submit_error(cookie))\n\t\treturn dma_submit_error(cookie);\n\n\tspin_lock_irq(&dmaengine_buffer->queue.list_lock);\n\tlist_add_tail(&block->head, &dmaengine_buffer->active);\n\tspin_unlock_irq(&dmaengine_buffer->queue.list_lock);\n\n\tdma_async_issue_pending(dmaengine_buffer->chan);\n\n\treturn 0;\n}\n\nstatic void iio_dmaengine_buffer_abort(struct iio_dma_buffer_queue *queue)\n{\n\tstruct dmaengine_buffer *dmaengine_buffer =\n\t\tiio_buffer_to_dmaengine_buffer(&queue->buffer);\n\n\tdmaengine_terminate_sync(dmaengine_buffer->chan);\n\tiio_dma_buffer_block_list_abort(queue, &dmaengine_buffer->active);\n}\n\nstatic void iio_dmaengine_buffer_release(struct iio_buffer *buf)\n{\n\tstruct dmaengine_buffer *dmaengine_buffer =\n\t\tiio_buffer_to_dmaengine_buffer(buf);\n\n\tiio_dma_buffer_release(&dmaengine_buffer->queue);\n\tkfree(dmaengine_buffer);\n}\n\nstatic const struct iio_buffer_access_funcs iio_dmaengine_buffer_ops = {\n\t.read = iio_dma_buffer_read,\n\t.set_bytes_per_datum = iio_dma_buffer_set_bytes_per_datum,\n\t.set_length = iio_dma_buffer_set_length,\n\t.request_update = iio_dma_buffer_request_update,\n\t.enable = iio_dma_buffer_enable,\n\t.disable = iio_dma_buffer_disable,\n\t.data_available = iio_dma_buffer_data_available,\n\t.release = iio_dmaengine_buffer_release,\n\n\t.modes = INDIO_BUFFER_HARDWARE,\n\t.flags = INDIO_BUFFER_FLAG_FIXED_WATERMARK,\n};\n\nstatic const struct iio_dma_buffer_ops iio_dmaengine_default_ops = {\n\t.submit = iio_dmaengine_buffer_submit_block,\n\t.abort = iio_dmaengine_buffer_abort,\n};\n\nstatic ssize_t iio_dmaengine_buffer_get_length_align(struct device *dev,\n\tstruct device_attribute *attr, char *buf)\n{\n\tstruct iio_buffer *buffer = to_iio_dev_attr(attr)->buffer;\n\tstruct dmaengine_buffer *dmaengine_buffer =\n\t\tiio_buffer_to_dmaengine_buffer(buffer);\n\n\treturn sysfs_emit(buf, \"%zu\\n\", dmaengine_buffer->align);\n}\n\nstatic IIO_DEVICE_ATTR(length_align_bytes, 0444,\n\t\t       iio_dmaengine_buffer_get_length_align, NULL, 0);\n\nstatic const struct iio_dev_attr *iio_dmaengine_buffer_attrs[] = {\n\t&iio_dev_attr_length_align_bytes,\n\tNULL,\n};\n\n \nstatic struct iio_buffer *iio_dmaengine_buffer_alloc(struct device *dev,\n\tconst char *channel)\n{\n\tstruct dmaengine_buffer *dmaengine_buffer;\n\tunsigned int width, src_width, dest_width;\n\tstruct dma_slave_caps caps;\n\tstruct dma_chan *chan;\n\tint ret;\n\n\tdmaengine_buffer = kzalloc(sizeof(*dmaengine_buffer), GFP_KERNEL);\n\tif (!dmaengine_buffer)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tchan = dma_request_chan(dev, channel);\n\tif (IS_ERR(chan)) {\n\t\tret = PTR_ERR(chan);\n\t\tgoto err_free;\n\t}\n\n\tret = dma_get_slave_caps(chan, &caps);\n\tif (ret < 0)\n\t\tgoto err_free;\n\n\t \n\tif (caps.src_addr_widths)\n\t\tsrc_width = __ffs(caps.src_addr_widths);\n\telse\n\t\tsrc_width = 1;\n\tif (caps.dst_addr_widths)\n\t\tdest_width = __ffs(caps.dst_addr_widths);\n\telse\n\t\tdest_width = 1;\n\twidth = max(src_width, dest_width);\n\n\tINIT_LIST_HEAD(&dmaengine_buffer->active);\n\tdmaengine_buffer->chan = chan;\n\tdmaengine_buffer->align = width;\n\tdmaengine_buffer->max_size = dma_get_max_seg_size(chan->device->dev);\n\n\tiio_dma_buffer_init(&dmaengine_buffer->queue, chan->device->dev,\n\t\t&iio_dmaengine_default_ops);\n\n\tdmaengine_buffer->queue.buffer.attrs = iio_dmaengine_buffer_attrs;\n\tdmaengine_buffer->queue.buffer.access = &iio_dmaengine_buffer_ops;\n\n\treturn &dmaengine_buffer->queue.buffer;\n\nerr_free:\n\tkfree(dmaengine_buffer);\n\treturn ERR_PTR(ret);\n}\n\n \nstatic void iio_dmaengine_buffer_free(struct iio_buffer *buffer)\n{\n\tstruct dmaengine_buffer *dmaengine_buffer =\n\t\tiio_buffer_to_dmaengine_buffer(buffer);\n\n\tiio_dma_buffer_exit(&dmaengine_buffer->queue);\n\tdma_release_channel(dmaengine_buffer->chan);\n\n\tiio_buffer_put(buffer);\n}\n\nstatic void __devm_iio_dmaengine_buffer_free(void *buffer)\n{\n\tiio_dmaengine_buffer_free(buffer);\n}\n\n \nstatic struct iio_buffer *devm_iio_dmaengine_buffer_alloc(struct device *dev,\n\tconst char *channel)\n{\n\tstruct iio_buffer *buffer;\n\tint ret;\n\n\tbuffer = iio_dmaengine_buffer_alloc(dev, channel);\n\tif (IS_ERR(buffer))\n\t\treturn buffer;\n\n\tret = devm_add_action_or_reset(dev, __devm_iio_dmaengine_buffer_free,\n\t\t\t\t       buffer);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn buffer;\n}\n\n \nint devm_iio_dmaengine_buffer_setup(struct device *dev,\n\t\t\t\t    struct iio_dev *indio_dev,\n\t\t\t\t    const char *channel)\n{\n\tstruct iio_buffer *buffer;\n\n\tbuffer = devm_iio_dmaengine_buffer_alloc(indio_dev->dev.parent,\n\t\t\t\t\t\t channel);\n\tif (IS_ERR(buffer))\n\t\treturn PTR_ERR(buffer);\n\n\tindio_dev->modes |= INDIO_BUFFER_HARDWARE;\n\n\treturn iio_device_attach_buffer(indio_dev, buffer);\n}\nEXPORT_SYMBOL_GPL(devm_iio_dmaengine_buffer_setup);\n\nMODULE_AUTHOR(\"Lars-Peter Clausen <lars@metafoo.de>\");\nMODULE_DESCRIPTION(\"DMA buffer for the IIO framework\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}