{
  "module_name": "arm_spe_pmu.c",
  "hash_id": "5e60d583cbd5f076cdc0923040adcbbc370799f855cc58dfeb716db301a14e07",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/arm_spe_pmu.c",
  "human_readable_source": "\n \n\n#define PMUNAME\t\t\t\t\t\"arm_spe\"\n#define DRVNAME\t\t\t\t\tPMUNAME \"_pmu\"\n#define pr_fmt(fmt)\t\t\t\tDRVNAME \": \" fmt\n\n#include <linux/bitfield.h>\n#include <linux/bitops.h>\n#include <linux/bug.h>\n#include <linux/capability.h>\n#include <linux/cpuhotplug.h>\n#include <linux/cpumask.h>\n#include <linux/device.h>\n#include <linux/errno.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/perf_event.h>\n#include <linux/perf/arm_pmu.h>\n#include <linux/platform_device.h>\n#include <linux/printk.h>\n#include <linux/slab.h>\n#include <linux/smp.h>\n#include <linux/vmalloc.h>\n\n#include <asm/barrier.h>\n#include <asm/cpufeature.h>\n#include <asm/mmu.h>\n#include <asm/sysreg.h>\n\n \n#define SPE_PMU_HW_FLAGS_CX\t\t\t0x00001\n\nstatic_assert((PERF_EVENT_FLAG_ARCH & SPE_PMU_HW_FLAGS_CX) == SPE_PMU_HW_FLAGS_CX);\n\nstatic void set_spe_event_has_cx(struct perf_event *event)\n{\n\tif (IS_ENABLED(CONFIG_PID_IN_CONTEXTIDR) && perfmon_capable())\n\t\tevent->hw.flags |= SPE_PMU_HW_FLAGS_CX;\n}\n\nstatic bool get_spe_event_has_cx(struct perf_event *event)\n{\n\treturn !!(event->hw.flags & SPE_PMU_HW_FLAGS_CX);\n}\n\n#define ARM_SPE_BUF_PAD_BYTE\t\t\t0\n\nstruct arm_spe_pmu_buf {\n\tint\t\t\t\t\tnr_pages;\n\tbool\t\t\t\t\tsnapshot;\n\tvoid\t\t\t\t\t*base;\n};\n\nstruct arm_spe_pmu {\n\tstruct pmu\t\t\t\tpmu;\n\tstruct platform_device\t\t\t*pdev;\n\tcpumask_t\t\t\t\tsupported_cpus;\n\tstruct hlist_node\t\t\thotplug_node;\n\n\tint\t\t\t\t\tirq;  \n\tu16\t\t\t\t\tpmsver;\n\tu16\t\t\t\t\tmin_period;\n\tu16\t\t\t\t\tcounter_sz;\n\n#define SPE_PMU_FEAT_FILT_EVT\t\t\t(1UL << 0)\n#define SPE_PMU_FEAT_FILT_TYP\t\t\t(1UL << 1)\n#define SPE_PMU_FEAT_FILT_LAT\t\t\t(1UL << 2)\n#define SPE_PMU_FEAT_ARCH_INST\t\t\t(1UL << 3)\n#define SPE_PMU_FEAT_LDS\t\t\t(1UL << 4)\n#define SPE_PMU_FEAT_ERND\t\t\t(1UL << 5)\n#define SPE_PMU_FEAT_INV_FILT_EVT\t\t(1UL << 6)\n#define SPE_PMU_FEAT_DEV_PROBED\t\t\t(1UL << 63)\n\tu64\t\t\t\t\tfeatures;\n\n\tu16\t\t\t\t\tmax_record_sz;\n\tu16\t\t\t\t\talign;\n\tstruct perf_output_handle __percpu\t*handle;\n};\n\n#define to_spe_pmu(p) (container_of(p, struct arm_spe_pmu, pmu))\n\n \n#define PERF_IDX2OFF(idx, buf)\t((idx) % ((buf)->nr_pages << PAGE_SHIFT))\n\n \nstatic enum cpuhp_state arm_spe_pmu_online;\n\nenum arm_spe_pmu_buf_fault_action {\n\tSPE_PMU_BUF_FAULT_ACT_SPURIOUS,\n\tSPE_PMU_BUF_FAULT_ACT_FATAL,\n\tSPE_PMU_BUF_FAULT_ACT_OK,\n};\n\n \nenum arm_spe_pmu_capabilities {\n\tSPE_PMU_CAP_ARCH_INST = 0,\n\tSPE_PMU_CAP_ERND,\n\tSPE_PMU_CAP_FEAT_MAX,\n\tSPE_PMU_CAP_CNT_SZ = SPE_PMU_CAP_FEAT_MAX,\n\tSPE_PMU_CAP_MIN_IVAL,\n};\n\nstatic int arm_spe_pmu_feat_caps[SPE_PMU_CAP_FEAT_MAX] = {\n\t[SPE_PMU_CAP_ARCH_INST]\t= SPE_PMU_FEAT_ARCH_INST,\n\t[SPE_PMU_CAP_ERND]\t= SPE_PMU_FEAT_ERND,\n};\n\nstatic u32 arm_spe_pmu_cap_get(struct arm_spe_pmu *spe_pmu, int cap)\n{\n\tif (cap < SPE_PMU_CAP_FEAT_MAX)\n\t\treturn !!(spe_pmu->features & arm_spe_pmu_feat_caps[cap]);\n\n\tswitch (cap) {\n\tcase SPE_PMU_CAP_CNT_SZ:\n\t\treturn spe_pmu->counter_sz;\n\tcase SPE_PMU_CAP_MIN_IVAL:\n\t\treturn spe_pmu->min_period;\n\tdefault:\n\t\tWARN(1, \"unknown cap %d\\n\", cap);\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t arm_spe_pmu_cap_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct arm_spe_pmu *spe_pmu = dev_get_drvdata(dev);\n\tstruct dev_ext_attribute *ea =\n\t\tcontainer_of(attr, struct dev_ext_attribute, attr);\n\tint cap = (long)ea->var;\n\n\treturn sysfs_emit(buf, \"%u\\n\", arm_spe_pmu_cap_get(spe_pmu, cap));\n}\n\n#define SPE_EXT_ATTR_ENTRY(_name, _func, _var)\t\t\t\t\\\n\t&((struct dev_ext_attribute[]) {\t\t\t\t\\\n\t\t{ __ATTR(_name, S_IRUGO, _func, NULL), (void *)_var }\t\\\n\t})[0].attr.attr\n\n#define SPE_CAP_EXT_ATTR_ENTRY(_name, _var)\t\t\t\t\\\n\tSPE_EXT_ATTR_ENTRY(_name, arm_spe_pmu_cap_show, _var)\n\nstatic struct attribute *arm_spe_pmu_cap_attr[] = {\n\tSPE_CAP_EXT_ATTR_ENTRY(arch_inst, SPE_PMU_CAP_ARCH_INST),\n\tSPE_CAP_EXT_ATTR_ENTRY(ernd, SPE_PMU_CAP_ERND),\n\tSPE_CAP_EXT_ATTR_ENTRY(count_size, SPE_PMU_CAP_CNT_SZ),\n\tSPE_CAP_EXT_ATTR_ENTRY(min_interval, SPE_PMU_CAP_MIN_IVAL),\n\tNULL,\n};\n\nstatic const struct attribute_group arm_spe_pmu_cap_group = {\n\t.name\t= \"caps\",\n\t.attrs\t= arm_spe_pmu_cap_attr,\n};\n\n \n#define ATTR_CFG_FLD_ts_enable_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_ts_enable_LO\t\t0\n#define ATTR_CFG_FLD_ts_enable_HI\t\t0\n#define ATTR_CFG_FLD_pa_enable_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_pa_enable_LO\t\t1\n#define ATTR_CFG_FLD_pa_enable_HI\t\t1\n#define ATTR_CFG_FLD_pct_enable_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_pct_enable_LO\t\t2\n#define ATTR_CFG_FLD_pct_enable_HI\t\t2\n#define ATTR_CFG_FLD_jitter_CFG\t\t\tconfig\t \n#define ATTR_CFG_FLD_jitter_LO\t\t\t16\n#define ATTR_CFG_FLD_jitter_HI\t\t\t16\n#define ATTR_CFG_FLD_branch_filter_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_branch_filter_LO\t\t32\n#define ATTR_CFG_FLD_branch_filter_HI\t\t32\n#define ATTR_CFG_FLD_load_filter_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_load_filter_LO\t\t33\n#define ATTR_CFG_FLD_load_filter_HI\t\t33\n#define ATTR_CFG_FLD_store_filter_CFG\t\tconfig\t \n#define ATTR_CFG_FLD_store_filter_LO\t\t34\n#define ATTR_CFG_FLD_store_filter_HI\t\t34\n\n#define ATTR_CFG_FLD_event_filter_CFG\t\tconfig1\t \n#define ATTR_CFG_FLD_event_filter_LO\t\t0\n#define ATTR_CFG_FLD_event_filter_HI\t\t63\n\n#define ATTR_CFG_FLD_min_latency_CFG\t\tconfig2\t \n#define ATTR_CFG_FLD_min_latency_LO\t\t0\n#define ATTR_CFG_FLD_min_latency_HI\t\t11\n\n#define ATTR_CFG_FLD_inv_event_filter_CFG\tconfig3\t \n#define ATTR_CFG_FLD_inv_event_filter_LO\t0\n#define ATTR_CFG_FLD_inv_event_filter_HI\t63\n\n \n#define __GEN_PMU_FORMAT_ATTR(cfg, lo, hi)\t\t\t\t\\\n\t(lo) == (hi) ? #cfg \":\" #lo \"\\n\" : #cfg \":\" #lo \"-\" #hi\n\n#define _GEN_PMU_FORMAT_ATTR(cfg, lo, hi)\t\t\t\t\\\n\t__GEN_PMU_FORMAT_ATTR(cfg, lo, hi)\n\n#define GEN_PMU_FORMAT_ATTR(name)\t\t\t\t\t\\\n\tPMU_FORMAT_ATTR(name,\t\t\t\t\t\t\\\n\t_GEN_PMU_FORMAT_ATTR(ATTR_CFG_FLD_##name##_CFG,\t\t\t\\\n\t\t\t     ATTR_CFG_FLD_##name##_LO,\t\t\t\\\n\t\t\t     ATTR_CFG_FLD_##name##_HI))\n\n#define _ATTR_CFG_GET_FLD(attr, cfg, lo, hi)\t\t\t\t\\\n\t((((attr)->cfg) >> lo) & GENMASK(hi - lo, 0))\n\n#define ATTR_CFG_GET_FLD(attr, name)\t\t\t\t\t\\\n\t_ATTR_CFG_GET_FLD(attr,\t\t\t\t\t\t\\\n\t\t\t  ATTR_CFG_FLD_##name##_CFG,\t\t\t\\\n\t\t\t  ATTR_CFG_FLD_##name##_LO,\t\t\t\\\n\t\t\t  ATTR_CFG_FLD_##name##_HI)\n\nGEN_PMU_FORMAT_ATTR(ts_enable);\nGEN_PMU_FORMAT_ATTR(pa_enable);\nGEN_PMU_FORMAT_ATTR(pct_enable);\nGEN_PMU_FORMAT_ATTR(jitter);\nGEN_PMU_FORMAT_ATTR(branch_filter);\nGEN_PMU_FORMAT_ATTR(load_filter);\nGEN_PMU_FORMAT_ATTR(store_filter);\nGEN_PMU_FORMAT_ATTR(event_filter);\nGEN_PMU_FORMAT_ATTR(inv_event_filter);\nGEN_PMU_FORMAT_ATTR(min_latency);\n\nstatic struct attribute *arm_spe_pmu_formats_attr[] = {\n\t&format_attr_ts_enable.attr,\n\t&format_attr_pa_enable.attr,\n\t&format_attr_pct_enable.attr,\n\t&format_attr_jitter.attr,\n\t&format_attr_branch_filter.attr,\n\t&format_attr_load_filter.attr,\n\t&format_attr_store_filter.attr,\n\t&format_attr_event_filter.attr,\n\t&format_attr_inv_event_filter.attr,\n\t&format_attr_min_latency.attr,\n\tNULL,\n};\n\nstatic umode_t arm_spe_pmu_format_attr_is_visible(struct kobject *kobj,\n\t\t\t\t\t\t  struct attribute *attr,\n\t\t\t\t\t\t  int unused)\n\t{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct arm_spe_pmu *spe_pmu = dev_get_drvdata(dev);\n\n\tif (attr == &format_attr_inv_event_filter.attr && !(spe_pmu->features & SPE_PMU_FEAT_INV_FILT_EVT))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group arm_spe_pmu_format_group = {\n\t.name\t= \"format\",\n\t.is_visible = arm_spe_pmu_format_attr_is_visible,\n\t.attrs\t= arm_spe_pmu_formats_attr,\n};\n\nstatic ssize_t cpumask_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct arm_spe_pmu *spe_pmu = dev_get_drvdata(dev);\n\n\treturn cpumap_print_to_pagebuf(true, buf, &spe_pmu->supported_cpus);\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *arm_spe_pmu_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group arm_spe_pmu_group = {\n\t.attrs\t= arm_spe_pmu_attrs,\n};\n\nstatic const struct attribute_group *arm_spe_pmu_attr_groups[] = {\n\t&arm_spe_pmu_group,\n\t&arm_spe_pmu_cap_group,\n\t&arm_spe_pmu_format_group,\n\tNULL,\n};\n\n \nstatic u64 arm_spe_event_to_pmscr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu64 reg = 0;\n\n\treg |= FIELD_PREP(PMSCR_EL1_TS, ATTR_CFG_GET_FLD(attr, ts_enable));\n\treg |= FIELD_PREP(PMSCR_EL1_PA, ATTR_CFG_GET_FLD(attr, pa_enable));\n\treg |= FIELD_PREP(PMSCR_EL1_PCT, ATTR_CFG_GET_FLD(attr, pct_enable));\n\n\tif (!attr->exclude_user)\n\t\treg |= PMSCR_EL1_E0SPE;\n\n\tif (!attr->exclude_kernel)\n\t\treg |= PMSCR_EL1_E1SPE;\n\n\tif (get_spe_event_has_cx(event))\n\t\treg |= PMSCR_EL1_CX;\n\n\treturn reg;\n}\n\nstatic void arm_spe_event_sanitise_period(struct perf_event *event)\n{\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(event->pmu);\n\tu64 period = event->hw.sample_period;\n\tu64 max_period = PMSIRR_EL1_INTERVAL_MASK;\n\n\tif (period < spe_pmu->min_period)\n\t\tperiod = spe_pmu->min_period;\n\telse if (period > max_period)\n\t\tperiod = max_period;\n\telse\n\t\tperiod &= max_period;\n\n\tevent->hw.sample_period = period;\n}\n\nstatic u64 arm_spe_event_to_pmsirr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu64 reg = 0;\n\n\tarm_spe_event_sanitise_period(event);\n\n\treg |= FIELD_PREP(PMSIRR_EL1_RND, ATTR_CFG_GET_FLD(attr, jitter));\n\treg |= event->hw.sample_period;\n\n\treturn reg;\n}\n\nstatic u64 arm_spe_event_to_pmsfcr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu64 reg = 0;\n\n\treg |= FIELD_PREP(PMSFCR_EL1_LD, ATTR_CFG_GET_FLD(attr, load_filter));\n\treg |= FIELD_PREP(PMSFCR_EL1_ST, ATTR_CFG_GET_FLD(attr, store_filter));\n\treg |= FIELD_PREP(PMSFCR_EL1_B, ATTR_CFG_GET_FLD(attr, branch_filter));\n\n\tif (reg)\n\t\treg |= PMSFCR_EL1_FT;\n\n\tif (ATTR_CFG_GET_FLD(attr, event_filter))\n\t\treg |= PMSFCR_EL1_FE;\n\n\tif (ATTR_CFG_GET_FLD(attr, inv_event_filter))\n\t\treg |= PMSFCR_EL1_FnE;\n\n\tif (ATTR_CFG_GET_FLD(attr, min_latency))\n\t\treg |= PMSFCR_EL1_FL;\n\n\treturn reg;\n}\n\nstatic u64 arm_spe_event_to_pmsevfr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\treturn ATTR_CFG_GET_FLD(attr, event_filter);\n}\n\nstatic u64 arm_spe_event_to_pmsnevfr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\treturn ATTR_CFG_GET_FLD(attr, inv_event_filter);\n}\n\nstatic u64 arm_spe_event_to_pmslatfr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\treturn FIELD_PREP(PMSLATFR_EL1_MINLAT, ATTR_CFG_GET_FLD(attr, min_latency));\n}\n\nstatic void arm_spe_pmu_pad_buf(struct perf_output_handle *handle, int len)\n{\n\tstruct arm_spe_pmu_buf *buf = perf_get_aux(handle);\n\tu64 head = PERF_IDX2OFF(handle->head, buf);\n\n\tmemset(buf->base + head, ARM_SPE_BUF_PAD_BYTE, len);\n\tif (!buf->snapshot)\n\t\tperf_aux_output_skip(handle, len);\n}\n\nstatic u64 arm_spe_pmu_next_snapshot_off(struct perf_output_handle *handle)\n{\n\tstruct arm_spe_pmu_buf *buf = perf_get_aux(handle);\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(handle->event->pmu);\n\tu64 head = PERF_IDX2OFF(handle->head, buf);\n\tu64 limit = buf->nr_pages * PAGE_SIZE;\n\n\t \n\tif (head < limit >> 1)\n\t\tlimit >>= 1;\n\n\t \n\tif (limit - head < spe_pmu->max_record_sz) {\n\t\tarm_spe_pmu_pad_buf(handle, limit - head);\n\t\thandle->head = PERF_IDX2OFF(limit, buf);\n\t\tlimit = ((buf->nr_pages * PAGE_SIZE) >> 1) + handle->head;\n\t}\n\n\treturn limit;\n}\n\nstatic u64 __arm_spe_pmu_next_off(struct perf_output_handle *handle)\n{\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(handle->event->pmu);\n\tstruct arm_spe_pmu_buf *buf = perf_get_aux(handle);\n\tconst u64 bufsize = buf->nr_pages * PAGE_SIZE;\n\tu64 limit = bufsize;\n\tu64 head, tail, wakeup;\n\n\t \n\thead = PERF_IDX2OFF(handle->head, buf);\n\tif (!IS_ALIGNED(head, spe_pmu->align)) {\n\t\tunsigned long delta = roundup(head, spe_pmu->align) - head;\n\n\t\tdelta = min(delta, handle->size);\n\t\tarm_spe_pmu_pad_buf(handle, delta);\n\t\thead = PERF_IDX2OFF(handle->head, buf);\n\t}\n\n\t \n\tif (!handle->size)\n\t\tgoto no_space;\n\n\t \n\ttail = PERF_IDX2OFF(handle->head + handle->size, buf);\n\twakeup = PERF_IDX2OFF(handle->wakeup, buf);\n\n\t \n\tif (head < tail)\n\t\tlimit = round_down(tail, PAGE_SIZE);\n\n\t \n\tif (handle->wakeup < (handle->head + handle->size) && head <= wakeup)\n\t\tlimit = min(limit, round_up(wakeup, PAGE_SIZE));\n\n\tif (limit > head)\n\t\treturn limit;\n\n\tarm_spe_pmu_pad_buf(handle, handle->size);\nno_space:\n\tperf_aux_output_flag(handle, PERF_AUX_FLAG_TRUNCATED);\n\tperf_aux_output_end(handle, 0);\n\treturn 0;\n}\n\nstatic u64 arm_spe_pmu_next_off(struct perf_output_handle *handle)\n{\n\tstruct arm_spe_pmu_buf *buf = perf_get_aux(handle);\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(handle->event->pmu);\n\tu64 limit = __arm_spe_pmu_next_off(handle);\n\tu64 head = PERF_IDX2OFF(handle->head, buf);\n\n\t \n\tif (limit && (limit - head < spe_pmu->max_record_sz)) {\n\t\tarm_spe_pmu_pad_buf(handle, limit - head);\n\t\tlimit = __arm_spe_pmu_next_off(handle);\n\t}\n\n\treturn limit;\n}\n\nstatic void arm_spe_perf_aux_output_begin(struct perf_output_handle *handle,\n\t\t\t\t\t  struct perf_event *event)\n{\n\tu64 base, limit;\n\tstruct arm_spe_pmu_buf *buf;\n\n\t \n\tbuf = perf_aux_output_begin(handle, event);\n\tif (!buf) {\n\t\tevent->hw.state |= PERF_HES_STOPPED;\n\t\t \n\t\tlimit = 0;\n\t\tgoto out_write_limit;\n\t}\n\n\tlimit = buf->snapshot ? arm_spe_pmu_next_snapshot_off(handle)\n\t\t\t      : arm_spe_pmu_next_off(handle);\n\tif (limit)\n\t\tlimit |= PMBLIMITR_EL1_E;\n\n\tlimit += (u64)buf->base;\n\tbase = (u64)buf->base + PERF_IDX2OFF(handle->head, buf);\n\twrite_sysreg_s(base, SYS_PMBPTR_EL1);\n\nout_write_limit:\n\twrite_sysreg_s(limit, SYS_PMBLIMITR_EL1);\n}\n\nstatic void arm_spe_perf_aux_output_end(struct perf_output_handle *handle)\n{\n\tstruct arm_spe_pmu_buf *buf = perf_get_aux(handle);\n\tu64 offset, size;\n\n\toffset = read_sysreg_s(SYS_PMBPTR_EL1) - (u64)buf->base;\n\tsize = offset - PERF_IDX2OFF(handle->head, buf);\n\n\tif (buf->snapshot)\n\t\thandle->head = offset;\n\n\tperf_aux_output_end(handle, size);\n}\n\nstatic void arm_spe_pmu_disable_and_drain_local(void)\n{\n\t \n\twrite_sysreg_s(0, SYS_PMSCR_EL1);\n\tisb();\n\n\t \n\tpsb_csync();\n\tdsb(nsh);\n\n\t \n\twrite_sysreg_s(0, SYS_PMBLIMITR_EL1);\n\tisb();\n}\n\n \nstatic enum arm_spe_pmu_buf_fault_action\narm_spe_pmu_buf_get_fault_act(struct perf_output_handle *handle)\n{\n\tconst char *err_str;\n\tu64 pmbsr;\n\tenum arm_spe_pmu_buf_fault_action ret;\n\n\t \n\tpsb_csync();\n\tdsb(nsh);\n\n\t \n\tisb();\n\n\t \n\tpmbsr = read_sysreg_s(SYS_PMBSR_EL1);\n\tif (!FIELD_GET(PMBSR_EL1_S, pmbsr))\n\t\treturn SPE_PMU_BUF_FAULT_ACT_SPURIOUS;\n\n\t \n\tif (FIELD_GET(PMBSR_EL1_DL, pmbsr))\n\t\tperf_aux_output_flag(handle, PERF_AUX_FLAG_TRUNCATED |\n\t\t\t\t\t     PERF_AUX_FLAG_PARTIAL);\n\n\t \n\tif (FIELD_GET(PMBSR_EL1_COLL, pmbsr))\n\t\tperf_aux_output_flag(handle, PERF_AUX_FLAG_COLLISION);\n\n\t \n\tswitch (FIELD_GET(PMBSR_EL1_EC, pmbsr)) {\n\tcase PMBSR_EL1_EC_BUF:\n\t\t \n\t\tbreak;\n\tcase PMBSR_EL1_EC_FAULT_S1:\n\tcase PMBSR_EL1_EC_FAULT_S2:\n\t\terr_str = \"Unexpected buffer fault\";\n\t\tgoto out_err;\n\tdefault:\n\t\terr_str = \"Unknown error code\";\n\t\tgoto out_err;\n\t}\n\n\t \n\tswitch (FIELD_GET(PMBSR_EL1_BUF_BSC_MASK, pmbsr)) {\n\tcase PMBSR_EL1_BUF_BSC_FULL:\n\t\tret = SPE_PMU_BUF_FAULT_ACT_OK;\n\t\tgoto out_stop;\n\tdefault:\n\t\terr_str = \"Unknown buffer status code\";\n\t}\n\nout_err:\n\tpr_err_ratelimited(\"%s on CPU %d [PMBSR=0x%016llx, PMBPTR=0x%016llx, PMBLIMITR=0x%016llx]\\n\",\n\t\t\t   err_str, smp_processor_id(), pmbsr,\n\t\t\t   read_sysreg_s(SYS_PMBPTR_EL1),\n\t\t\t   read_sysreg_s(SYS_PMBLIMITR_EL1));\n\tret = SPE_PMU_BUF_FAULT_ACT_FATAL;\n\nout_stop:\n\tarm_spe_perf_aux_output_end(handle);\n\treturn ret;\n}\n\nstatic irqreturn_t arm_spe_pmu_irq_handler(int irq, void *dev)\n{\n\tstruct perf_output_handle *handle = dev;\n\tstruct perf_event *event = handle->event;\n\tenum arm_spe_pmu_buf_fault_action act;\n\n\tif (!perf_get_aux(handle))\n\t\treturn IRQ_NONE;\n\n\tact = arm_spe_pmu_buf_get_fault_act(handle);\n\tif (act == SPE_PMU_BUF_FAULT_ACT_SPURIOUS)\n\t\treturn IRQ_NONE;\n\n\t \n\tirq_work_run();\n\n\tswitch (act) {\n\tcase SPE_PMU_BUF_FAULT_ACT_FATAL:\n\t\t \n\t\tarm_spe_pmu_disable_and_drain_local();\n\t\tbreak;\n\tcase SPE_PMU_BUF_FAULT_ACT_OK:\n\t\t \n\t\tif (!(handle->aux_flags & PERF_AUX_FLAG_TRUNCATED)) {\n\t\t\tarm_spe_perf_aux_output_begin(handle, event);\n\t\t\tisb();\n\t\t}\n\t\tbreak;\n\tcase SPE_PMU_BUF_FAULT_ACT_SPURIOUS:\n\t\t \n\t\tbreak;\n\t}\n\n\t \n\twrite_sysreg_s(0, SYS_PMBSR_EL1);\n\treturn IRQ_HANDLED;\n}\n\nstatic u64 arm_spe_pmsevfr_res0(u16 pmsver)\n{\n\tswitch (pmsver) {\n\tcase ID_AA64DFR0_EL1_PMSVer_IMP:\n\t\treturn PMSEVFR_EL1_RES0_IMP;\n\tcase ID_AA64DFR0_EL1_PMSVer_V1P1:\n\t\treturn PMSEVFR_EL1_RES0_V1P1;\n\tcase ID_AA64DFR0_EL1_PMSVer_V1P2:\n\t \n\tdefault:\n\t\treturn PMSEVFR_EL1_RES0_V1P2;\n\t}\n}\n\n \nstatic int arm_spe_pmu_event_init(struct perf_event *event)\n{\n\tu64 reg;\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(event->pmu);\n\n\t \n\tif (attr->type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tif (event->cpu >= 0 &&\n\t    !cpumask_test_cpu(event->cpu, &spe_pmu->supported_cpus))\n\t\treturn -ENOENT;\n\n\tif (arm_spe_event_to_pmsevfr(event) & arm_spe_pmsevfr_res0(spe_pmu->pmsver))\n\t\treturn -EOPNOTSUPP;\n\n\tif (arm_spe_event_to_pmsnevfr(event) & arm_spe_pmsevfr_res0(spe_pmu->pmsver))\n\t\treturn -EOPNOTSUPP;\n\n\tif (attr->exclude_idle)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (attr->freq)\n\t\treturn -EINVAL;\n\n\treg = arm_spe_event_to_pmsfcr(event);\n\tif ((FIELD_GET(PMSFCR_EL1_FE, reg)) &&\n\t    !(spe_pmu->features & SPE_PMU_FEAT_FILT_EVT))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((FIELD_GET(PMSFCR_EL1_FnE, reg)) &&\n\t    !(spe_pmu->features & SPE_PMU_FEAT_INV_FILT_EVT))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((FIELD_GET(PMSFCR_EL1_FT, reg)) &&\n\t    !(spe_pmu->features & SPE_PMU_FEAT_FILT_TYP))\n\t\treturn -EOPNOTSUPP;\n\n\tif ((FIELD_GET(PMSFCR_EL1_FL, reg)) &&\n\t    !(spe_pmu->features & SPE_PMU_FEAT_FILT_LAT))\n\t\treturn -EOPNOTSUPP;\n\n\tset_spe_event_has_cx(event);\n\treg = arm_spe_event_to_pmscr(event);\n\tif (!perfmon_capable() &&\n\t    (reg & (PMSCR_EL1_PA | PMSCR_EL1_PCT)))\n\t\treturn -EACCES;\n\n\treturn 0;\n}\n\nstatic void arm_spe_pmu_start(struct perf_event *event, int flags)\n{\n\tu64 reg;\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_output_handle *handle = this_cpu_ptr(spe_pmu->handle);\n\n\thwc->state = 0;\n\tarm_spe_perf_aux_output_begin(handle, event);\n\tif (hwc->state)\n\t\treturn;\n\n\treg = arm_spe_event_to_pmsfcr(event);\n\twrite_sysreg_s(reg, SYS_PMSFCR_EL1);\n\n\treg = arm_spe_event_to_pmsevfr(event);\n\twrite_sysreg_s(reg, SYS_PMSEVFR_EL1);\n\n\tif (spe_pmu->features & SPE_PMU_FEAT_INV_FILT_EVT) {\n\t\treg = arm_spe_event_to_pmsnevfr(event);\n\t\twrite_sysreg_s(reg, SYS_PMSNEVFR_EL1);\n\t}\n\n\treg = arm_spe_event_to_pmslatfr(event);\n\twrite_sysreg_s(reg, SYS_PMSLATFR_EL1);\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\treg = arm_spe_event_to_pmsirr(event);\n\t\twrite_sysreg_s(reg, SYS_PMSIRR_EL1);\n\t\tisb();\n\t\treg = local64_read(&hwc->period_left);\n\t\twrite_sysreg_s(reg, SYS_PMSICR_EL1);\n\t}\n\n\treg = arm_spe_event_to_pmscr(event);\n\tisb();\n\twrite_sysreg_s(reg, SYS_PMSCR_EL1);\n}\n\nstatic void arm_spe_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_output_handle *handle = this_cpu_ptr(spe_pmu->handle);\n\n\t \n\tif (hwc->state & PERF_HES_STOPPED)\n\t\treturn;\n\n\t \n\tarm_spe_pmu_disable_and_drain_local();\n\n\tif (flags & PERF_EF_UPDATE) {\n\t\t \n\t\tif (perf_get_aux(handle)) {\n\t\t\tenum arm_spe_pmu_buf_fault_action act;\n\n\t\t\tact = arm_spe_pmu_buf_get_fault_act(handle);\n\t\t\tif (act == SPE_PMU_BUF_FAULT_ACT_SPURIOUS)\n\t\t\t\tarm_spe_perf_aux_output_end(handle);\n\t\t\telse\n\t\t\t\twrite_sysreg_s(0, SYS_PMBSR_EL1);\n\t\t}\n\n\t\t \n\t\tlocal64_set(&hwc->period_left, read_sysreg_s(SYS_PMSICR_EL1));\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n\n\thwc->state |= PERF_HES_STOPPED;\n}\n\nstatic int arm_spe_pmu_add(struct perf_event *event, int flags)\n{\n\tint ret = 0;\n\tstruct arm_spe_pmu *spe_pmu = to_spe_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint cpu = event->cpu == -1 ? smp_processor_id() : event->cpu;\n\n\tif (!cpumask_test_cpu(cpu, &spe_pmu->supported_cpus))\n\t\treturn -ENOENT;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\tif (flags & PERF_EF_START) {\n\t\tarm_spe_pmu_start(event, PERF_EF_RELOAD);\n\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n}\n\nstatic void arm_spe_pmu_del(struct perf_event *event, int flags)\n{\n\tarm_spe_pmu_stop(event, PERF_EF_UPDATE);\n}\n\nstatic void arm_spe_pmu_read(struct perf_event *event)\n{\n}\n\nstatic void *arm_spe_pmu_setup_aux(struct perf_event *event, void **pages,\n\t\t\t\t   int nr_pages, bool snapshot)\n{\n\tint i, cpu = event->cpu;\n\tstruct page **pglist;\n\tstruct arm_spe_pmu_buf *buf;\n\n\t \n\tif (nr_pages < 2)\n\t\treturn NULL;\n\n\t \n\tif (snapshot && (nr_pages & 1))\n\t\treturn NULL;\n\n\tif (cpu == -1)\n\t\tcpu = raw_smp_processor_id();\n\n\tbuf = kzalloc_node(sizeof(*buf), GFP_KERNEL, cpu_to_node(cpu));\n\tif (!buf)\n\t\treturn NULL;\n\n\tpglist = kcalloc(nr_pages, sizeof(*pglist), GFP_KERNEL);\n\tif (!pglist)\n\t\tgoto out_free_buf;\n\n\tfor (i = 0; i < nr_pages; ++i)\n\t\tpglist[i] = virt_to_page(pages[i]);\n\n\tbuf->base = vmap(pglist, nr_pages, VM_MAP, PAGE_KERNEL);\n\tif (!buf->base)\n\t\tgoto out_free_pglist;\n\n\tbuf->nr_pages\t= nr_pages;\n\tbuf->snapshot\t= snapshot;\n\n\tkfree(pglist);\n\treturn buf;\n\nout_free_pglist:\n\tkfree(pglist);\nout_free_buf:\n\tkfree(buf);\n\treturn NULL;\n}\n\nstatic void arm_spe_pmu_free_aux(void *aux)\n{\n\tstruct arm_spe_pmu_buf *buf = aux;\n\n\tvunmap(buf->base);\n\tkfree(buf);\n}\n\n \nstatic int arm_spe_pmu_perf_init(struct arm_spe_pmu *spe_pmu)\n{\n\tstatic atomic_t pmu_idx = ATOMIC_INIT(-1);\n\n\tint idx;\n\tchar *name;\n\tstruct device *dev = &spe_pmu->pdev->dev;\n\n\tspe_pmu->pmu = (struct pmu) {\n\t\t.module = THIS_MODULE,\n\t\t.capabilities\t= PERF_PMU_CAP_EXCLUSIVE | PERF_PMU_CAP_ITRACE,\n\t\t.attr_groups\t= arm_spe_pmu_attr_groups,\n\t\t \n\t\t.task_ctx_nr\t= perf_sw_context,\n\t\t.event_init\t= arm_spe_pmu_event_init,\n\t\t.add\t\t= arm_spe_pmu_add,\n\t\t.del\t\t= arm_spe_pmu_del,\n\t\t.start\t\t= arm_spe_pmu_start,\n\t\t.stop\t\t= arm_spe_pmu_stop,\n\t\t.read\t\t= arm_spe_pmu_read,\n\t\t.setup_aux\t= arm_spe_pmu_setup_aux,\n\t\t.free_aux\t= arm_spe_pmu_free_aux,\n\t};\n\n\tidx = atomic_inc_return(&pmu_idx);\n\tname = devm_kasprintf(dev, GFP_KERNEL, \"%s_%d\", PMUNAME, idx);\n\tif (!name) {\n\t\tdev_err(dev, \"failed to allocate name for pmu %d\\n\", idx);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn perf_pmu_register(&spe_pmu->pmu, name, -1);\n}\n\nstatic void arm_spe_pmu_perf_destroy(struct arm_spe_pmu *spe_pmu)\n{\n\tperf_pmu_unregister(&spe_pmu->pmu);\n}\n\nstatic void __arm_spe_pmu_dev_probe(void *info)\n{\n\tint fld;\n\tu64 reg;\n\tstruct arm_spe_pmu *spe_pmu = info;\n\tstruct device *dev = &spe_pmu->pdev->dev;\n\n\tfld = cpuid_feature_extract_unsigned_field(read_cpuid(ID_AA64DFR0_EL1),\n\t\t\t\t\t\t   ID_AA64DFR0_EL1_PMSVer_SHIFT);\n\tif (!fld) {\n\t\tdev_err(dev,\n\t\t\t\"unsupported ID_AA64DFR0_EL1.PMSVer [%d] on CPU %d\\n\",\n\t\t\tfld, smp_processor_id());\n\t\treturn;\n\t}\n\tspe_pmu->pmsver = (u16)fld;\n\n\t \n\treg = read_sysreg_s(SYS_PMBIDR_EL1);\n\tif (FIELD_GET(PMBIDR_EL1_P, reg)) {\n\t\tdev_err(dev,\n\t\t\t\"profiling buffer owned by higher exception level\\n\");\n\t\treturn;\n\t}\n\n\t \n\tfld = FIELD_GET(PMBIDR_EL1_ALIGN, reg);\n\tspe_pmu->align = 1 << fld;\n\tif (spe_pmu->align > SZ_2K) {\n\t\tdev_err(dev, \"unsupported PMBIDR.Align [%d] on CPU %d\\n\",\n\t\t\tfld, smp_processor_id());\n\t\treturn;\n\t}\n\n\t \n\treg = read_sysreg_s(SYS_PMSIDR_EL1);\n\tif (FIELD_GET(PMSIDR_EL1_FE, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_FILT_EVT;\n\n\tif (FIELD_GET(PMSIDR_EL1_FnE, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_INV_FILT_EVT;\n\n\tif (FIELD_GET(PMSIDR_EL1_FT, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_FILT_TYP;\n\n\tif (FIELD_GET(PMSIDR_EL1_FL, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_FILT_LAT;\n\n\tif (FIELD_GET(PMSIDR_EL1_ARCHINST, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_ARCH_INST;\n\n\tif (FIELD_GET(PMSIDR_EL1_LDS, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_LDS;\n\n\tif (FIELD_GET(PMSIDR_EL1_ERND, reg))\n\t\tspe_pmu->features |= SPE_PMU_FEAT_ERND;\n\n\t \n\tfld = FIELD_GET(PMSIDR_EL1_INTERVAL, reg);\n\tswitch (fld) {\n\tcase PMSIDR_EL1_INTERVAL_256:\n\t\tspe_pmu->min_period = 256;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_512:\n\t\tspe_pmu->min_period = 512;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_768:\n\t\tspe_pmu->min_period = 768;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_1024:\n\t\tspe_pmu->min_period = 1024;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_1536:\n\t\tspe_pmu->min_period = 1536;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_2048:\n\t\tspe_pmu->min_period = 2048;\n\t\tbreak;\n\tcase PMSIDR_EL1_INTERVAL_3072:\n\t\tspe_pmu->min_period = 3072;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(dev, \"unknown PMSIDR_EL1.Interval [%d]; assuming 8\\n\",\n\t\t\t fld);\n\t\tfallthrough;\n\tcase PMSIDR_EL1_INTERVAL_4096:\n\t\tspe_pmu->min_period = 4096;\n\t}\n\n\t \n\tfld = FIELD_GET(PMSIDR_EL1_MAXSIZE, reg);\n\tspe_pmu->max_record_sz = 1 << fld;\n\tif (spe_pmu->max_record_sz > SZ_2K || spe_pmu->max_record_sz < 16) {\n\t\tdev_err(dev, \"unsupported PMSIDR_EL1.MaxSize [%d] on CPU %d\\n\",\n\t\t\tfld, smp_processor_id());\n\t\treturn;\n\t}\n\n\tfld = FIELD_GET(PMSIDR_EL1_COUNTSIZE, reg);\n\tswitch (fld) {\n\tdefault:\n\t\tdev_warn(dev, \"unknown PMSIDR_EL1.CountSize [%d]; assuming 2\\n\",\n\t\t\t fld);\n\t\tfallthrough;\n\tcase PMSIDR_EL1_COUNTSIZE_12_BIT_SAT:\n\t\tspe_pmu->counter_sz = 12;\n\t\tbreak;\n\tcase PMSIDR_EL1_COUNTSIZE_16_BIT_SAT:\n\t\tspe_pmu->counter_sz = 16;\n\t}\n\n\tdev_info(dev,\n\t\t \"probed SPEv1.%d for CPUs %*pbl [max_record_sz %u, align %u, features 0x%llx]\\n\",\n\t\t spe_pmu->pmsver - 1, cpumask_pr_args(&spe_pmu->supported_cpus),\n\t\t spe_pmu->max_record_sz, spe_pmu->align, spe_pmu->features);\n\n\tspe_pmu->features |= SPE_PMU_FEAT_DEV_PROBED;\n}\n\nstatic void __arm_spe_pmu_reset_local(void)\n{\n\t \n\tarm_spe_pmu_disable_and_drain_local();\n\n\t \n\twrite_sysreg_s(0, SYS_PMBPTR_EL1);\n\tisb();\n\n\t \n\twrite_sysreg_s(0, SYS_PMBSR_EL1);\n\tisb();\n}\n\nstatic void __arm_spe_pmu_setup_one(void *info)\n{\n\tstruct arm_spe_pmu *spe_pmu = info;\n\n\t__arm_spe_pmu_reset_local();\n\tenable_percpu_irq(spe_pmu->irq, IRQ_TYPE_NONE);\n}\n\nstatic void __arm_spe_pmu_stop_one(void *info)\n{\n\tstruct arm_spe_pmu *spe_pmu = info;\n\n\tdisable_percpu_irq(spe_pmu->irq);\n\t__arm_spe_pmu_reset_local();\n}\n\nstatic int arm_spe_pmu_cpu_startup(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct arm_spe_pmu *spe_pmu;\n\n\tspe_pmu = hlist_entry_safe(node, struct arm_spe_pmu, hotplug_node);\n\tif (!cpumask_test_cpu(cpu, &spe_pmu->supported_cpus))\n\t\treturn 0;\n\n\t__arm_spe_pmu_setup_one(spe_pmu);\n\treturn 0;\n}\n\nstatic int arm_spe_pmu_cpu_teardown(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct arm_spe_pmu *spe_pmu;\n\n\tspe_pmu = hlist_entry_safe(node, struct arm_spe_pmu, hotplug_node);\n\tif (!cpumask_test_cpu(cpu, &spe_pmu->supported_cpus))\n\t\treturn 0;\n\n\t__arm_spe_pmu_stop_one(spe_pmu);\n\treturn 0;\n}\n\nstatic int arm_spe_pmu_dev_init(struct arm_spe_pmu *spe_pmu)\n{\n\tint ret;\n\tcpumask_t *mask = &spe_pmu->supported_cpus;\n\n\t \n\tret = smp_call_function_any(mask,  __arm_spe_pmu_dev_probe, spe_pmu, 1);\n\tif (ret || !(spe_pmu->features & SPE_PMU_FEAT_DEV_PROBED))\n\t\treturn -ENXIO;\n\n\t \n\tret = request_percpu_irq(spe_pmu->irq, arm_spe_pmu_irq_handler, DRVNAME,\n\t\t\t\t spe_pmu->handle);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = cpuhp_state_add_instance(arm_spe_pmu_online,\n\t\t\t\t       &spe_pmu->hotplug_node);\n\tif (ret)\n\t\tfree_percpu_irq(spe_pmu->irq, spe_pmu->handle);\n\n\treturn ret;\n}\n\nstatic void arm_spe_pmu_dev_teardown(struct arm_spe_pmu *spe_pmu)\n{\n\tcpuhp_state_remove_instance(arm_spe_pmu_online, &spe_pmu->hotplug_node);\n\tfree_percpu_irq(spe_pmu->irq, spe_pmu->handle);\n}\n\n \nstatic int arm_spe_pmu_irq_probe(struct arm_spe_pmu *spe_pmu)\n{\n\tstruct platform_device *pdev = spe_pmu->pdev;\n\tint irq = platform_get_irq(pdev, 0);\n\n\tif (irq < 0)\n\t\treturn -ENXIO;\n\n\tif (!irq_is_percpu(irq)) {\n\t\tdev_err(&pdev->dev, \"expected PPI but got SPI (%d)\\n\", irq);\n\t\treturn -EINVAL;\n\t}\n\n\tif (irq_get_percpu_devid_partition(irq, &spe_pmu->supported_cpus)) {\n\t\tdev_err(&pdev->dev, \"failed to get PPI partition (%d)\\n\", irq);\n\t\treturn -EINVAL;\n\t}\n\n\tspe_pmu->irq = irq;\n\treturn 0;\n}\n\nstatic const struct of_device_id arm_spe_pmu_of_match[] = {\n\t{ .compatible = \"arm,statistical-profiling-extension-v1\", .data = (void *)1 },\n\t{   },\n};\nMODULE_DEVICE_TABLE(of, arm_spe_pmu_of_match);\n\nstatic const struct platform_device_id arm_spe_match[] = {\n\t{ ARMV8_SPE_PDEV_NAME, 0},\n\t{ }\n};\nMODULE_DEVICE_TABLE(platform, arm_spe_match);\n\nstatic int arm_spe_pmu_device_probe(struct platform_device *pdev)\n{\n\tint ret;\n\tstruct arm_spe_pmu *spe_pmu;\n\tstruct device *dev = &pdev->dev;\n\n\t \n\tif (arm64_kernel_unmapped_at_el0()) {\n\t\tdev_warn_once(dev, \"profiling buffer inaccessible. Try passing \\\"kpti=off\\\" on the kernel command line\\n\");\n\t\treturn -EPERM;\n\t}\n\n\tspe_pmu = devm_kzalloc(dev, sizeof(*spe_pmu), GFP_KERNEL);\n\tif (!spe_pmu)\n\t\treturn -ENOMEM;\n\n\tspe_pmu->handle = alloc_percpu(typeof(*spe_pmu->handle));\n\tif (!spe_pmu->handle)\n\t\treturn -ENOMEM;\n\n\tspe_pmu->pdev = pdev;\n\tplatform_set_drvdata(pdev, spe_pmu);\n\n\tret = arm_spe_pmu_irq_probe(spe_pmu);\n\tif (ret)\n\t\tgoto out_free_handle;\n\n\tret = arm_spe_pmu_dev_init(spe_pmu);\n\tif (ret)\n\t\tgoto out_free_handle;\n\n\tret = arm_spe_pmu_perf_init(spe_pmu);\n\tif (ret)\n\t\tgoto out_teardown_dev;\n\n\treturn 0;\n\nout_teardown_dev:\n\tarm_spe_pmu_dev_teardown(spe_pmu);\nout_free_handle:\n\tfree_percpu(spe_pmu->handle);\n\treturn ret;\n}\n\nstatic int arm_spe_pmu_device_remove(struct platform_device *pdev)\n{\n\tstruct arm_spe_pmu *spe_pmu = platform_get_drvdata(pdev);\n\n\tarm_spe_pmu_perf_destroy(spe_pmu);\n\tarm_spe_pmu_dev_teardown(spe_pmu);\n\tfree_percpu(spe_pmu->handle);\n\treturn 0;\n}\n\nstatic struct platform_driver arm_spe_pmu_driver = {\n\t.id_table = arm_spe_match,\n\t.driver\t= {\n\t\t.name\t\t= DRVNAME,\n\t\t.of_match_table\t= of_match_ptr(arm_spe_pmu_of_match),\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe\t= arm_spe_pmu_device_probe,\n\t.remove\t= arm_spe_pmu_device_remove,\n};\n\nstatic int __init arm_spe_pmu_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, DRVNAME,\n\t\t\t\t      arm_spe_pmu_cpu_startup,\n\t\t\t\t      arm_spe_pmu_cpu_teardown);\n\tif (ret < 0)\n\t\treturn ret;\n\tarm_spe_pmu_online = ret;\n\n\tret = platform_driver_register(&arm_spe_pmu_driver);\n\tif (ret)\n\t\tcpuhp_remove_multi_state(arm_spe_pmu_online);\n\n\treturn ret;\n}\n\nstatic void __exit arm_spe_pmu_exit(void)\n{\n\tplatform_driver_unregister(&arm_spe_pmu_driver);\n\tcpuhp_remove_multi_state(arm_spe_pmu_online);\n}\n\nmodule_init(arm_spe_pmu_init);\nmodule_exit(arm_spe_pmu_exit);\n\nMODULE_DESCRIPTION(\"Perf driver for the ARMv8.2 Statistical Profiling Extension\");\nMODULE_AUTHOR(\"Will Deacon <will.deacon@arm.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}