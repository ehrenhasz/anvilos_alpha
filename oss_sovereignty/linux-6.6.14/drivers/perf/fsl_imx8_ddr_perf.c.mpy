{
  "module_name": "fsl_imx8_ddr_perf.c",
  "hash_id": "701ae7c59a0b2a9331b680bb9c67023c958fc7b20493c11172956caee031e01b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/fsl_imx8_ddr_perf.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n\n#define COUNTER_CNTL\t\t0x0\n#define COUNTER_READ\t\t0x20\n\n#define COUNTER_DPCR1\t\t0x30\n\n#define CNTL_OVER\t\t0x1\n#define CNTL_CLEAR\t\t0x2\n#define CNTL_EN\t\t\t0x4\n#define CNTL_EN_MASK\t\t0xFFFFFFFB\n#define CNTL_CLEAR_MASK\t\t0xFFFFFFFD\n#define CNTL_OVER_MASK\t\t0xFFFFFFFE\n\n#define CNTL_CP_SHIFT\t\t16\n#define CNTL_CP_MASK\t\t(0xFF << CNTL_CP_SHIFT)\n#define CNTL_CSV_SHIFT\t\t24\n#define CNTL_CSV_MASK\t\t(0xFFU << CNTL_CSV_SHIFT)\n\n#define EVENT_CYCLES_ID\t\t0\n#define EVENT_CYCLES_COUNTER\t0\n#define NUM_COUNTERS\t\t4\n\n \n#define CYCLES_COUNTER_MASK\t0x0FFFFFFF\n#define AXI_MASKING_REVERT\t0xffff0000\t \n\n#define to_ddr_pmu(p)\t\tcontainer_of(p, struct ddr_pmu, pmu)\n\n#define DDR_PERF_DEV_NAME\t\"imx8_ddr\"\n#define DDR_CPUHP_CB_NAME\tDDR_PERF_DEV_NAME \"_perf_pmu\"\n\nstatic DEFINE_IDA(ddr_ida);\n\n \n#define DDR_CAP_AXI_ID_FILTER\t\t\t0x1      \n#define DDR_CAP_AXI_ID_FILTER_ENHANCED\t\t0x3      \n\nstruct fsl_ddr_devtype_data {\n\tunsigned int quirks;     \n\tconst char *identifier;\t \n};\n\nstatic const struct fsl_ddr_devtype_data imx8_devtype_data;\n\nstatic const struct fsl_ddr_devtype_data imx8m_devtype_data = {\n\t.quirks = DDR_CAP_AXI_ID_FILTER,\n};\n\nstatic const struct fsl_ddr_devtype_data imx8mq_devtype_data = {\n\t.quirks = DDR_CAP_AXI_ID_FILTER,\n\t.identifier = \"i.MX8MQ\",\n};\n\nstatic const struct fsl_ddr_devtype_data imx8mm_devtype_data = {\n\t.quirks = DDR_CAP_AXI_ID_FILTER,\n\t.identifier = \"i.MX8MM\",\n};\n\nstatic const struct fsl_ddr_devtype_data imx8mn_devtype_data = {\n\t.quirks = DDR_CAP_AXI_ID_FILTER,\n\t.identifier = \"i.MX8MN\",\n};\n\nstatic const struct fsl_ddr_devtype_data imx8mp_devtype_data = {\n\t.quirks = DDR_CAP_AXI_ID_FILTER_ENHANCED,\n\t.identifier = \"i.MX8MP\",\n};\n\nstatic const struct of_device_id imx_ddr_pmu_dt_ids[] = {\n\t{ .compatible = \"fsl,imx8-ddr-pmu\", .data = &imx8_devtype_data},\n\t{ .compatible = \"fsl,imx8m-ddr-pmu\", .data = &imx8m_devtype_data},\n\t{ .compatible = \"fsl,imx8mq-ddr-pmu\", .data = &imx8mq_devtype_data},\n\t{ .compatible = \"fsl,imx8mm-ddr-pmu\", .data = &imx8mm_devtype_data},\n\t{ .compatible = \"fsl,imx8mn-ddr-pmu\", .data = &imx8mn_devtype_data},\n\t{ .compatible = \"fsl,imx8mp-ddr-pmu\", .data = &imx8mp_devtype_data},\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, imx_ddr_pmu_dt_ids);\n\nstruct ddr_pmu {\n\tstruct pmu pmu;\n\tvoid __iomem *base;\n\tunsigned int cpu;\n\tstruct\thlist_node node;\n\tstruct\tdevice *dev;\n\tstruct perf_event *events[NUM_COUNTERS];\n\tenum cpuhp_state cpuhp_state;\n\tconst struct fsl_ddr_devtype_data *devtype_data;\n\tint irq;\n\tint id;\n\tint active_counter;\n};\n\nstatic ssize_t ddr_perf_identifier_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *page)\n{\n\tstruct ddr_pmu *pmu = dev_get_drvdata(dev);\n\n\treturn sysfs_emit(page, \"%s\\n\", pmu->devtype_data->identifier);\n}\n\nstatic umode_t ddr_perf_identifier_attr_visible(struct kobject *kobj,\n\t\t\t\t\t\tstruct attribute *attr,\n\t\t\t\t\t\tint n)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct ddr_pmu *pmu = dev_get_drvdata(dev);\n\n\tif (!pmu->devtype_data->identifier)\n\t\treturn 0;\n\treturn attr->mode;\n};\n\nstatic struct device_attribute ddr_perf_identifier_attr =\n\t__ATTR(identifier, 0444, ddr_perf_identifier_show, NULL);\n\nstatic struct attribute *ddr_perf_identifier_attrs[] = {\n\t&ddr_perf_identifier_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group ddr_perf_identifier_attr_group = {\n\t.attrs = ddr_perf_identifier_attrs,\n\t.is_visible = ddr_perf_identifier_attr_visible,\n};\n\nenum ddr_perf_filter_capabilities {\n\tPERF_CAP_AXI_ID_FILTER = 0,\n\tPERF_CAP_AXI_ID_FILTER_ENHANCED,\n\tPERF_CAP_AXI_ID_FEAT_MAX,\n};\n\nstatic u32 ddr_perf_filter_cap_get(struct ddr_pmu *pmu, int cap)\n{\n\tu32 quirks = pmu->devtype_data->quirks;\n\n\tswitch (cap) {\n\tcase PERF_CAP_AXI_ID_FILTER:\n\t\treturn !!(quirks & DDR_CAP_AXI_ID_FILTER);\n\tcase PERF_CAP_AXI_ID_FILTER_ENHANCED:\n\t\tquirks &= DDR_CAP_AXI_ID_FILTER_ENHANCED;\n\t\treturn quirks == DDR_CAP_AXI_ID_FILTER_ENHANCED;\n\tdefault:\n\t\tWARN(1, \"unknown filter cap %d\\n\", cap);\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t ddr_perf_filter_cap_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct ddr_pmu *pmu = dev_get_drvdata(dev);\n\tstruct dev_ext_attribute *ea =\n\t\tcontainer_of(attr, struct dev_ext_attribute, attr);\n\tint cap = (long)ea->var;\n\n\treturn sysfs_emit(buf, \"%u\\n\", ddr_perf_filter_cap_get(pmu, cap));\n}\n\n#define PERF_EXT_ATTR_ENTRY(_name, _func, _var)\t\t\t\t\\\n\t(&((struct dev_ext_attribute) {\t\t\t\t\t\\\n\t\t__ATTR(_name, 0444, _func, NULL), (void *)_var\t\t\\\n\t}).attr.attr)\n\n#define PERF_FILTER_EXT_ATTR_ENTRY(_name, _var)\t\t\t\t\\\n\tPERF_EXT_ATTR_ENTRY(_name, ddr_perf_filter_cap_show, _var)\n\nstatic struct attribute *ddr_perf_filter_cap_attr[] = {\n\tPERF_FILTER_EXT_ATTR_ENTRY(filter, PERF_CAP_AXI_ID_FILTER),\n\tPERF_FILTER_EXT_ATTR_ENTRY(enhanced_filter, PERF_CAP_AXI_ID_FILTER_ENHANCED),\n\tNULL,\n};\n\nstatic const struct attribute_group ddr_perf_filter_cap_attr_group = {\n\t.name = \"caps\",\n\t.attrs = ddr_perf_filter_cap_attr,\n};\n\nstatic ssize_t ddr_perf_cpumask_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ddr_pmu *pmu = dev_get_drvdata(dev);\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(pmu->cpu));\n}\n\nstatic struct device_attribute ddr_perf_cpumask_attr =\n\t__ATTR(cpumask, 0444, ddr_perf_cpumask_show, NULL);\n\nstatic struct attribute *ddr_perf_cpumask_attrs[] = {\n\t&ddr_perf_cpumask_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group ddr_perf_cpumask_attr_group = {\n\t.attrs = ddr_perf_cpumask_attrs,\n};\n\nstatic ssize_t\nddr_pmu_event_show(struct device *dev, struct device_attribute *attr,\n\t\t   char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\treturn sysfs_emit(page, \"event=0x%02llx\\n\", pmu_attr->id);\n}\n\n#define IMX8_DDR_PMU_EVENT_ATTR(_name, _id)\t\t\\\n\tPMU_EVENT_ATTR_ID(_name, ddr_pmu_event_show, _id)\n\nstatic struct attribute *ddr_perf_events_attrs[] = {\n\tIMX8_DDR_PMU_EVENT_ATTR(cycles, EVENT_CYCLES_ID),\n\tIMX8_DDR_PMU_EVENT_ATTR(selfresh, 0x01),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-accesses, 0x04),\n\tIMX8_DDR_PMU_EVENT_ATTR(write-accesses, 0x05),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-queue-depth, 0x08),\n\tIMX8_DDR_PMU_EVENT_ATTR(write-queue-depth, 0x09),\n\tIMX8_DDR_PMU_EVENT_ATTR(lp-read-credit-cnt, 0x10),\n\tIMX8_DDR_PMU_EVENT_ATTR(hp-read-credit-cnt, 0x11),\n\tIMX8_DDR_PMU_EVENT_ATTR(write-credit-cnt, 0x12),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-command, 0x20),\n\tIMX8_DDR_PMU_EVENT_ATTR(write-command, 0x21),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-modify-write-command, 0x22),\n\tIMX8_DDR_PMU_EVENT_ATTR(hp-read, 0x23),\n\tIMX8_DDR_PMU_EVENT_ATTR(hp-req-nocredit, 0x24),\n\tIMX8_DDR_PMU_EVENT_ATTR(hp-xact-credit, 0x25),\n\tIMX8_DDR_PMU_EVENT_ATTR(lp-req-nocredit, 0x26),\n\tIMX8_DDR_PMU_EVENT_ATTR(lp-xact-credit, 0x27),\n\tIMX8_DDR_PMU_EVENT_ATTR(wr-xact-credit, 0x29),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-cycles, 0x2a),\n\tIMX8_DDR_PMU_EVENT_ATTR(write-cycles, 0x2b),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-write-transition, 0x30),\n\tIMX8_DDR_PMU_EVENT_ATTR(precharge, 0x31),\n\tIMX8_DDR_PMU_EVENT_ATTR(activate, 0x32),\n\tIMX8_DDR_PMU_EVENT_ATTR(load-mode, 0x33),\n\tIMX8_DDR_PMU_EVENT_ATTR(perf-mwr, 0x34),\n\tIMX8_DDR_PMU_EVENT_ATTR(read, 0x35),\n\tIMX8_DDR_PMU_EVENT_ATTR(read-activate, 0x36),\n\tIMX8_DDR_PMU_EVENT_ATTR(refresh, 0x37),\n\tIMX8_DDR_PMU_EVENT_ATTR(write, 0x38),\n\tIMX8_DDR_PMU_EVENT_ATTR(raw-hazard, 0x39),\n\tIMX8_DDR_PMU_EVENT_ATTR(axid-read, 0x41),\n\tIMX8_DDR_PMU_EVENT_ATTR(axid-write, 0x42),\n\tNULL,\n};\n\nstatic const struct attribute_group ddr_perf_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = ddr_perf_events_attrs,\n};\n\nPMU_FORMAT_ATTR(event, \"config:0-7\");\nPMU_FORMAT_ATTR(axi_id, \"config1:0-15\");\nPMU_FORMAT_ATTR(axi_mask, \"config1:16-31\");\n\nstatic struct attribute *ddr_perf_format_attrs[] = {\n\t&format_attr_event.attr,\n\t&format_attr_axi_id.attr,\n\t&format_attr_axi_mask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group ddr_perf_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = ddr_perf_format_attrs,\n};\n\nstatic const struct attribute_group *attr_groups[] = {\n\t&ddr_perf_events_attr_group,\n\t&ddr_perf_format_attr_group,\n\t&ddr_perf_cpumask_attr_group,\n\t&ddr_perf_filter_cap_attr_group,\n\t&ddr_perf_identifier_attr_group,\n\tNULL,\n};\n\nstatic bool ddr_perf_is_filtered(struct perf_event *event)\n{\n\treturn event->attr.config == 0x41 || event->attr.config == 0x42;\n}\n\nstatic u32 ddr_perf_filter_val(struct perf_event *event)\n{\n\treturn event->attr.config1;\n}\n\nstatic bool ddr_perf_filters_compatible(struct perf_event *a,\n\t\t\t\t\tstruct perf_event *b)\n{\n\tif (!ddr_perf_is_filtered(a))\n\t\treturn true;\n\tif (!ddr_perf_is_filtered(b))\n\t\treturn true;\n\treturn ddr_perf_filter_val(a) == ddr_perf_filter_val(b);\n}\n\nstatic bool ddr_perf_is_enhanced_filtered(struct perf_event *event)\n{\n\tunsigned int filt;\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\n\tfilt = pmu->devtype_data->quirks & DDR_CAP_AXI_ID_FILTER_ENHANCED;\n\treturn (filt == DDR_CAP_AXI_ID_FILTER_ENHANCED) &&\n\t\tddr_perf_is_filtered(event);\n}\n\nstatic u32 ddr_perf_alloc_counter(struct ddr_pmu *pmu, int event)\n{\n\tint i;\n\n\t \n\tif (event == EVENT_CYCLES_ID) {\n\t\tif (pmu->events[EVENT_CYCLES_COUNTER] == NULL)\n\t\t\treturn EVENT_CYCLES_COUNTER;\n\t\telse\n\t\t\treturn -ENOENT;\n\t}\n\n\tfor (i = 1; i < NUM_COUNTERS; i++) {\n\t\tif (pmu->events[i] == NULL)\n\t\t\treturn i;\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic void ddr_perf_free_counter(struct ddr_pmu *pmu, int counter)\n{\n\tpmu->events[counter] = NULL;\n}\n\nstatic u32 ddr_perf_read_counter(struct ddr_pmu *pmu, int counter)\n{\n\tstruct perf_event *event = pmu->events[counter];\n\tvoid __iomem *base = pmu->base;\n\n\t \n\tbase += ddr_perf_is_enhanced_filtered(event) ? COUNTER_DPCR1 :\n\t\t\t\t\t\t       COUNTER_READ;\n\treturn readl_relaxed(base + counter * 4);\n}\n\nstatic int ddr_perf_event_init(struct perf_event *event)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_event *sibling;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EOPNOTSUPP;\n\n\tif (event->cpu < 0) {\n\t\tdev_warn(pmu->dev, \"Can't provide per-task data!\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (event->group_leader->pmu != event->pmu &&\n\t\t\t!is_software_event(event->group_leader))\n\t\treturn -EINVAL;\n\n\tif (pmu->devtype_data->quirks & DDR_CAP_AXI_ID_FILTER) {\n\t\tif (!ddr_perf_filters_compatible(event, event->group_leader))\n\t\t\treturn -EINVAL;\n\t\tfor_each_sibling_event(sibling, event->group_leader) {\n\t\t\tif (!ddr_perf_filters_compatible(event, sibling))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tfor_each_sibling_event(sibling, event->group_leader) {\n\t\tif (sibling->pmu != event->pmu &&\n\t\t\t\t!is_software_event(sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tevent->cpu = pmu->cpu;\n\thwc->idx = -1;\n\n\treturn 0;\n}\n\nstatic void ddr_perf_counter_enable(struct ddr_pmu *pmu, int config,\n\t\t\t\t  int counter, bool enable)\n{\n\tu8 reg = counter * 4 + COUNTER_CNTL;\n\tint val;\n\n\tif (enable) {\n\t\t \n\t\twritel(0, pmu->base + reg);\n\t\tval = CNTL_EN | CNTL_CLEAR;\n\t\tval |= FIELD_PREP(CNTL_CSV_MASK, config);\n\n\t\t \n\t\tif (pmu->devtype_data->quirks & DDR_CAP_AXI_ID_FILTER_ENHANCED) {\n\t\t\tif (counter == EVENT_CYCLES_COUNTER)\n\t\t\t\tval |= FIELD_PREP(CNTL_CP_MASK, 0xf0);\n\t\t}\n\n\t\twritel(val, pmu->base + reg);\n\t} else {\n\t\t \n\t\tval = readl_relaxed(pmu->base + reg) & CNTL_EN_MASK;\n\t\twritel(val, pmu->base + reg);\n\t}\n}\n\nstatic bool ddr_perf_counter_overflow(struct ddr_pmu *pmu, int counter)\n{\n\tint val;\n\n\tval = readl_relaxed(pmu->base + counter * 4 + COUNTER_CNTL);\n\n\treturn val & CNTL_OVER;\n}\n\nstatic void ddr_perf_counter_clear(struct ddr_pmu *pmu, int counter)\n{\n\tu8 reg = counter * 4 + COUNTER_CNTL;\n\tint val;\n\n\tval = readl_relaxed(pmu->base + reg);\n\tval &= ~CNTL_CLEAR;\n\twritel(val, pmu->base + reg);\n\n\tval |= CNTL_CLEAR;\n\twritel(val, pmu->base + reg);\n}\n\nstatic void ddr_perf_event_update(struct perf_event *event)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 new_raw_count;\n\tint counter = hwc->idx;\n\tint ret;\n\n\tnew_raw_count = ddr_perf_read_counter(pmu, counter);\n\t \n\tif (pmu->devtype_data->quirks & DDR_CAP_AXI_ID_FILTER_ENHANCED) {\n\t\tif (counter == EVENT_CYCLES_COUNTER)\n\t\t\tnew_raw_count &= CYCLES_COUNTER_MASK;\n\t}\n\n\tlocal64_add(new_raw_count, &event->count);\n\n\t \n\tif (counter != EVENT_CYCLES_COUNTER) {\n\t\tret = ddr_perf_counter_overflow(pmu, counter);\n\t\tif (ret)\n\t\t\tdev_warn_ratelimited(pmu->dev,  \"events lost due to counter overflow (config 0x%llx)\\n\",\n\t\t\t\t\t     event->attr.config);\n\t}\n\n\t \n\tddr_perf_counter_clear(pmu, counter);\n}\n\nstatic void ddr_perf_event_start(struct perf_event *event, int flags)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tlocal64_set(&hwc->prev_count, 0);\n\n\tddr_perf_counter_enable(pmu, event->attr.config, counter, true);\n\n\tif (!pmu->active_counter++)\n\t\tddr_perf_counter_enable(pmu, EVENT_CYCLES_ID,\n\t\t\tEVENT_CYCLES_COUNTER, true);\n\n\thwc->state = 0;\n}\n\nstatic int ddr_perf_event_add(struct perf_event *event, int flags)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter;\n\tint cfg = event->attr.config;\n\tint cfg1 = event->attr.config1;\n\n\tif (pmu->devtype_data->quirks & DDR_CAP_AXI_ID_FILTER) {\n\t\tint i;\n\n\t\tfor (i = 1; i < NUM_COUNTERS; i++) {\n\t\t\tif (pmu->events[i] &&\n\t\t\t    !ddr_perf_filters_compatible(event, pmu->events[i]))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ddr_perf_is_filtered(event)) {\n\t\t\t \n\t\t\tcfg1 ^= AXI_MASKING_REVERT;\n\t\t\twritel(cfg1, pmu->base + COUNTER_DPCR1);\n\t\t}\n\t}\n\n\tcounter = ddr_perf_alloc_counter(pmu, cfg);\n\tif (counter < 0) {\n\t\tdev_dbg(pmu->dev, \"There are not enough counters\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tpmu->events[counter] = event;\n\thwc->idx = counter;\n\n\thwc->state |= PERF_HES_STOPPED;\n\n\tif (flags & PERF_EF_START)\n\t\tddr_perf_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void ddr_perf_event_stop(struct perf_event *event, int flags)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tddr_perf_counter_enable(pmu, event->attr.config, counter, false);\n\tddr_perf_event_update(event);\n\n\tif (!--pmu->active_counter)\n\t\tddr_perf_counter_enable(pmu, EVENT_CYCLES_ID,\n\t\t\tEVENT_CYCLES_COUNTER, false);\n\n\thwc->state |= PERF_HES_STOPPED;\n}\n\nstatic void ddr_perf_event_del(struct perf_event *event, int flags)\n{\n\tstruct ddr_pmu *pmu = to_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tddr_perf_event_stop(event, PERF_EF_UPDATE);\n\n\tddr_perf_free_counter(pmu, counter);\n\thwc->idx = -1;\n}\n\nstatic void ddr_perf_pmu_enable(struct pmu *pmu)\n{\n}\n\nstatic void ddr_perf_pmu_disable(struct pmu *pmu)\n{\n}\n\nstatic int ddr_perf_init(struct ddr_pmu *pmu, void __iomem *base,\n\t\t\t struct device *dev)\n{\n\t*pmu = (struct ddr_pmu) {\n\t\t.pmu = (struct pmu) {\n\t\t\t.module\t      = THIS_MODULE,\n\t\t\t.capabilities = PERF_PMU_CAP_NO_EXCLUDE,\n\t\t\t.task_ctx_nr = perf_invalid_context,\n\t\t\t.attr_groups = attr_groups,\n\t\t\t.event_init  = ddr_perf_event_init,\n\t\t\t.add\t     = ddr_perf_event_add,\n\t\t\t.del\t     = ddr_perf_event_del,\n\t\t\t.start\t     = ddr_perf_event_start,\n\t\t\t.stop\t     = ddr_perf_event_stop,\n\t\t\t.read\t     = ddr_perf_event_update,\n\t\t\t.pmu_enable  = ddr_perf_pmu_enable,\n\t\t\t.pmu_disable = ddr_perf_pmu_disable,\n\t\t},\n\t\t.base = base,\n\t\t.dev = dev,\n\t};\n\n\tpmu->id = ida_alloc(&ddr_ida, GFP_KERNEL);\n\treturn pmu->id;\n}\n\nstatic irqreturn_t ddr_perf_irq_handler(int irq, void *p)\n{\n\tint i;\n\tstruct ddr_pmu *pmu = (struct ddr_pmu *) p;\n\tstruct perf_event *event;\n\n\t \n\tddr_perf_counter_enable(pmu,\n\t\t\t      EVENT_CYCLES_ID,\n\t\t\t      EVENT_CYCLES_COUNTER,\n\t\t\t      false);\n\t \n\tfor (i = 0; i < NUM_COUNTERS; i++) {\n\n\t\tif (!pmu->events[i])\n\t\t\tcontinue;\n\n\t\tevent = pmu->events[i];\n\n\t\tddr_perf_event_update(event);\n\t}\n\n\tddr_perf_counter_enable(pmu,\n\t\t\t      EVENT_CYCLES_ID,\n\t\t\t      EVENT_CYCLES_COUNTER,\n\t\t\t      true);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int ddr_perf_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct ddr_pmu *pmu = hlist_entry_safe(node, struct ddr_pmu, node);\n\tint target;\n\n\tif (cpu != pmu->cpu)\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids)\n\t\treturn 0;\n\n\tperf_pmu_migrate_context(&pmu->pmu, cpu, target);\n\tpmu->cpu = target;\n\n\tWARN_ON(irq_set_affinity(pmu->irq, cpumask_of(pmu->cpu)));\n\n\treturn 0;\n}\n\nstatic int ddr_perf_probe(struct platform_device *pdev)\n{\n\tstruct ddr_pmu *pmu;\n\tstruct device_node *np;\n\tvoid __iomem *base;\n\tchar *name;\n\tint num;\n\tint ret;\n\tint irq;\n\n\tbase = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(base))\n\t\treturn PTR_ERR(base);\n\n\tnp = pdev->dev.of_node;\n\n\tpmu = devm_kzalloc(&pdev->dev, sizeof(*pmu), GFP_KERNEL);\n\tif (!pmu)\n\t\treturn -ENOMEM;\n\n\tnum = ddr_perf_init(pmu, base, &pdev->dev);\n\n\tplatform_set_drvdata(pdev, pmu);\n\n\tname = devm_kasprintf(&pdev->dev, GFP_KERNEL, DDR_PERF_DEV_NAME \"%d\",\n\t\t\t      num);\n\tif (!name) {\n\t\tret = -ENOMEM;\n\t\tgoto cpuhp_state_err;\n\t}\n\n\tpmu->devtype_data = of_device_get_match_data(&pdev->dev);\n\n\tpmu->cpu = raw_smp_processor_id();\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t      DDR_CPUHP_CB_NAME,\n\t\t\t\t      NULL,\n\t\t\t\t      ddr_perf_offline_cpu);\n\n\tif (ret < 0) {\n\t\tdev_err(&pdev->dev, \"cpuhp_setup_state_multi failed\\n\");\n\t\tgoto cpuhp_state_err;\n\t}\n\n\tpmu->cpuhp_state = ret;\n\n\t \n\tret = cpuhp_state_add_instance_nocalls(pmu->cpuhp_state, &pmu->node);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Error %d registering hotplug\\n\", ret);\n\t\tgoto cpuhp_instance_err;\n\t}\n\n\t \n\tirq = of_irq_get(np, 0);\n\tif (irq < 0) {\n\t\tdev_err(&pdev->dev, \"Failed to get irq: %d\", irq);\n\t\tret = irq;\n\t\tgoto ddr_perf_err;\n\t}\n\n\tret = devm_request_irq(&pdev->dev, irq,\n\t\t\t\t\tddr_perf_irq_handler,\n\t\t\t\t\tIRQF_NOBALANCING | IRQF_NO_THREAD,\n\t\t\t\t\tDDR_CPUHP_CB_NAME,\n\t\t\t\t\tpmu);\n\tif (ret < 0) {\n\t\tdev_err(&pdev->dev, \"Request irq failed: %d\", ret);\n\t\tgoto ddr_perf_err;\n\t}\n\n\tpmu->irq = irq;\n\tret = irq_set_affinity(pmu->irq, cpumask_of(pmu->cpu));\n\tif (ret) {\n\t\tdev_err(pmu->dev, \"Failed to set interrupt affinity!\\n\");\n\t\tgoto ddr_perf_err;\n\t}\n\n\tret = perf_pmu_register(&pmu->pmu, name, -1);\n\tif (ret)\n\t\tgoto ddr_perf_err;\n\n\treturn 0;\n\nddr_perf_err:\n\tcpuhp_state_remove_instance_nocalls(pmu->cpuhp_state, &pmu->node);\ncpuhp_instance_err:\n\tcpuhp_remove_multi_state(pmu->cpuhp_state);\ncpuhp_state_err:\n\tida_free(&ddr_ida, pmu->id);\n\tdev_warn(&pdev->dev, \"i.MX8 DDR Perf PMU failed (%d), disabled\\n\", ret);\n\treturn ret;\n}\n\nstatic int ddr_perf_remove(struct platform_device *pdev)\n{\n\tstruct ddr_pmu *pmu = platform_get_drvdata(pdev);\n\n\tcpuhp_state_remove_instance_nocalls(pmu->cpuhp_state, &pmu->node);\n\tcpuhp_remove_multi_state(pmu->cpuhp_state);\n\n\tperf_pmu_unregister(&pmu->pmu);\n\n\tida_free(&ddr_ida, pmu->id);\n\treturn 0;\n}\n\nstatic struct platform_driver imx_ddr_pmu_driver = {\n\t.driver         = {\n\t\t.name   = \"imx-ddr-pmu\",\n\t\t.of_match_table = imx_ddr_pmu_dt_ids,\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe          = ddr_perf_probe,\n\t.remove         = ddr_perf_remove,\n};\n\nmodule_platform_driver(imx_ddr_pmu_driver);\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}