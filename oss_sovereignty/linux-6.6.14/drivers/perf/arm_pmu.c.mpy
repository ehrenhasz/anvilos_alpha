{
  "module_name": "arm_pmu.c",
  "hash_id": "2cc81f2c7302baf4f4b8fc61477f59eadbe1adb88488e0a2bebd9efeca064ecd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/arm_pmu.c",
  "human_readable_source": "\n#undef DEBUG\n\n \n#define pr_fmt(fmt) \"hw perfevents: \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/cpumask.h>\n#include <linux/cpu_pm.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/perf/arm_pmu.h>\n#include <linux/slab.h>\n#include <linux/sched/clock.h>\n#include <linux/spinlock.h>\n#include <linux/irq.h>\n#include <linux/irqdesc.h>\n\n#include <asm/irq_regs.h>\n\nstatic int armpmu_count_irq_users(const int irq);\n\nstruct pmu_irq_ops {\n\tvoid (*enable_pmuirq)(unsigned int irq);\n\tvoid (*disable_pmuirq)(unsigned int irq);\n\tvoid (*free_pmuirq)(unsigned int irq, int cpu, void __percpu *devid);\n};\n\nstatic void armpmu_free_pmuirq(unsigned int irq, int cpu, void __percpu *devid)\n{\n\tfree_irq(irq, per_cpu_ptr(devid, cpu));\n}\n\nstatic const struct pmu_irq_ops pmuirq_ops = {\n\t.enable_pmuirq = enable_irq,\n\t.disable_pmuirq = disable_irq_nosync,\n\t.free_pmuirq = armpmu_free_pmuirq\n};\n\nstatic void armpmu_free_pmunmi(unsigned int irq, int cpu, void __percpu *devid)\n{\n\tfree_nmi(irq, per_cpu_ptr(devid, cpu));\n}\n\nstatic const struct pmu_irq_ops pmunmi_ops = {\n\t.enable_pmuirq = enable_nmi,\n\t.disable_pmuirq = disable_nmi_nosync,\n\t.free_pmuirq = armpmu_free_pmunmi\n};\n\nstatic void armpmu_enable_percpu_pmuirq(unsigned int irq)\n{\n\tenable_percpu_irq(irq, IRQ_TYPE_NONE);\n}\n\nstatic void armpmu_free_percpu_pmuirq(unsigned int irq, int cpu,\n\t\t\t\t   void __percpu *devid)\n{\n\tif (armpmu_count_irq_users(irq) == 1)\n\t\tfree_percpu_irq(irq, devid);\n}\n\nstatic const struct pmu_irq_ops percpu_pmuirq_ops = {\n\t.enable_pmuirq = armpmu_enable_percpu_pmuirq,\n\t.disable_pmuirq = disable_percpu_irq,\n\t.free_pmuirq = armpmu_free_percpu_pmuirq\n};\n\nstatic void armpmu_enable_percpu_pmunmi(unsigned int irq)\n{\n\tif (!prepare_percpu_nmi(irq))\n\t\tenable_percpu_nmi(irq, IRQ_TYPE_NONE);\n}\n\nstatic void armpmu_disable_percpu_pmunmi(unsigned int irq)\n{\n\tdisable_percpu_nmi(irq);\n\tteardown_percpu_nmi(irq);\n}\n\nstatic void armpmu_free_percpu_pmunmi(unsigned int irq, int cpu,\n\t\t\t\t      void __percpu *devid)\n{\n\tif (armpmu_count_irq_users(irq) == 1)\n\t\tfree_percpu_nmi(irq, devid);\n}\n\nstatic const struct pmu_irq_ops percpu_pmunmi_ops = {\n\t.enable_pmuirq = armpmu_enable_percpu_pmunmi,\n\t.disable_pmuirq = armpmu_disable_percpu_pmunmi,\n\t.free_pmuirq = armpmu_free_percpu_pmunmi\n};\n\nstatic DEFINE_PER_CPU(struct arm_pmu *, cpu_armpmu);\nstatic DEFINE_PER_CPU(int, cpu_irq);\nstatic DEFINE_PER_CPU(const struct pmu_irq_ops *, cpu_irq_ops);\n\nstatic bool has_nmi;\n\nstatic inline u64 arm_pmu_event_max_period(struct perf_event *event)\n{\n\tif (event->hw.flags & ARMPMU_EVT_64BIT)\n\t\treturn GENMASK_ULL(63, 0);\n\telse if (event->hw.flags & ARMPMU_EVT_63BIT)\n\t\treturn GENMASK_ULL(62, 0);\n\telse if (event->hw.flags & ARMPMU_EVT_47BIT)\n\t\treturn GENMASK_ULL(46, 0);\n\telse\n\t\treturn GENMASK_ULL(31, 0);\n}\n\nstatic int\narmpmu_map_cache_event(const unsigned (*cache_map)\n\t\t\t\t      [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t       u64 config)\n{\n\tunsigned int cache_type, cache_op, cache_result, ret;\n\n\tcache_type = (config >>  0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\n\tif (!cache_map)\n\t\treturn -ENOENT;\n\n\tret = (int)(*cache_map)[cache_type][cache_op][cache_result];\n\n\tif (ret == CACHE_OP_UNSUPPORTED)\n\t\treturn -ENOENT;\n\n\treturn ret;\n}\n\nstatic int\narmpmu_map_hw_event(const unsigned (*event_map)[PERF_COUNT_HW_MAX], u64 config)\n{\n\tint mapping;\n\n\tif (config >= PERF_COUNT_HW_MAX)\n\t\treturn -EINVAL;\n\n\tif (!event_map)\n\t\treturn -ENOENT;\n\n\tmapping = (*event_map)[config];\n\treturn mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;\n}\n\nstatic int\narmpmu_map_raw_event(u32 raw_event_mask, u64 config)\n{\n\treturn (int)(config & raw_event_mask);\n}\n\nint\narmpmu_map_event(struct perf_event *event,\n\t\t const unsigned (*event_map)[PERF_COUNT_HW_MAX],\n\t\t const unsigned (*cache_map)\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX],\n\t\t u32 raw_event_mask)\n{\n\tu64 config = event->attr.config;\n\tint type = event->attr.type;\n\n\tif (type == event->pmu->type)\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\n\tswitch (type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\treturn armpmu_map_hw_event(event_map, config);\n\tcase PERF_TYPE_HW_CACHE:\n\t\treturn armpmu_map_cache_event(cache_map, config);\n\tcase PERF_TYPE_RAW:\n\t\treturn armpmu_map_raw_event(raw_event_mask, config);\n\t}\n\n\treturn -ENOENT;\n}\n\nint armpmu_event_set_period(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tu64 max_period;\n\tint ret = 0;\n\n\tmax_period = arm_pmu_event_max_period(event);\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\t \n\tif (left > (max_period >> 1))\n\t\tleft = (max_period >> 1);\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tarmpmu->write_counter(event, (u64)(-left) & max_period);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nu64 armpmu_event_update(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 delta, prev_raw_count, new_raw_count;\n\tu64 max_period = arm_pmu_event_max_period(event);\n\nagain:\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tnew_raw_count = armpmu->read_counter(event);\n\n\tif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t     new_raw_count) != prev_raw_count)\n\t\tgoto again;\n\n\tdelta = (new_raw_count - prev_raw_count) & max_period;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\nstatic void\narmpmu_read(struct perf_event *event)\n{\n\tarmpmu_event_update(event);\n}\n\nstatic void\narmpmu_stop(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tarmpmu->disable(event);\n\t\tarmpmu_event_update(event);\n\t\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void armpmu_start(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\t \n\tarmpmu_event_set_period(event);\n\tarmpmu->enable(event);\n}\n\nstatic void\narmpmu_del(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tarmpmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tarmpmu->clear_event_idx(hw_events, event);\n\tperf_event_update_userpage(event);\n\t \n\thwc->idx = -1;\n}\n\nstatic int\narmpmu_add(struct perf_event *event, int flags)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\t \n\tif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\n\t\treturn -ENOENT;\n\n\t \n\tidx = armpmu->get_event_idx(hw_events, event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\t \n\tevent->hw.idx = idx;\n\tarmpmu->disable(event);\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\n\t \n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic int\nvalidate_event(struct pmu *pmu, struct pmu_hw_events *hw_events,\n\t\t\t       struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu;\n\n\tif (is_software_event(event))\n\t\treturn 1;\n\n\t \n\tif (event->pmu != pmu)\n\t\treturn 0;\n\n\tif (event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\tarmpmu = to_arm_pmu(event->pmu);\n\treturn armpmu->get_event_idx(hw_events, event) >= 0;\n}\n\nstatic int\nvalidate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct pmu_hw_events fake_pmu;\n\n\t \n\tmemset(&fake_pmu.used_mask, 0, sizeof(fake_pmu.used_mask));\n\n\tif (!validate_event(event->pmu, &fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tif (event == leader)\n\t\treturn 0;\n\n\tfor_each_sibling_event(sibling, leader) {\n\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(event->pmu, &fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic irqreturn_t armpmu_dispatch_irq(int irq, void *dev)\n{\n\tstruct arm_pmu *armpmu;\n\tint ret;\n\tu64 start_clock, finish_clock;\n\n\t \n\tarmpmu = *(void **)dev;\n\tif (WARN_ON_ONCE(!armpmu))\n\t\treturn IRQ_NONE;\n\n\tstart_clock = sched_clock();\n\tret = armpmu->handle_irq(armpmu);\n\tfinish_clock = sched_clock();\n\n\tperf_sample_event_took(finish_clock - start_clock);\n\treturn ret;\n}\n\nstatic int\n__hw_perf_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping;\n\n\thwc->flags = 0;\n\tmapping = armpmu->map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t \n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t \n\tif (armpmu->set_event_filter &&\n\t    armpmu->set_event_filter(hwc, &event->attr)) {\n\t\tpr_debug(\"ARM performance counters do not support \"\n\t\t\t \"mode exclusion\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (!is_sampling_event(event)) {\n\t\t \n\t\thwc->sample_period  = arm_pmu_event_max_period(event) >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn validate_group(event);\n}\n\nstatic int armpmu_event_init(struct perf_event *event)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\n\t \n\tif (event->cpu != -1 &&\n\t\t!cpumask_test_cpu(event->cpu, &armpmu->supported_cpus))\n\t\treturn -ENOENT;\n\n\t \n\tif (has_branch_stack(event))\n\t\treturn -EOPNOTSUPP;\n\n\treturn __hw_perf_event_init(event);\n}\n\nstatic void armpmu_enable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\tstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\n\tbool enabled = !bitmap_empty(hw_events->used_mask, armpmu->num_events);\n\n\t \n\tif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\n\t\treturn;\n\n\tif (enabled)\n\t\tarmpmu->start(armpmu);\n}\n\nstatic void armpmu_disable(struct pmu *pmu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\n\t \n\tif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\n\t\treturn;\n\n\tarmpmu->stop(armpmu);\n}\n\n \nstatic bool armpmu_filter(struct pmu *pmu, int cpu)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(pmu);\n\treturn !cpumask_test_cpu(cpu, &armpmu->supported_cpus);\n}\n\nstatic ssize_t cpus_show(struct device *dev,\n\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct arm_pmu *armpmu = to_arm_pmu(dev_get_drvdata(dev));\n\treturn cpumap_print_to_pagebuf(true, buf, &armpmu->supported_cpus);\n}\n\nstatic DEVICE_ATTR_RO(cpus);\n\nstatic struct attribute *armpmu_common_attrs[] = {\n\t&dev_attr_cpus.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group armpmu_common_attr_group = {\n\t.attrs = armpmu_common_attrs,\n};\n\nstatic int armpmu_count_irq_users(const int irq)\n{\n\tint cpu, count = 0;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (per_cpu(cpu_irq, cpu) == irq)\n\t\t\tcount++;\n\t}\n\n\treturn count;\n}\n\nstatic const struct pmu_irq_ops *armpmu_find_irq_ops(int irq)\n{\n\tconst struct pmu_irq_ops *ops = NULL;\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (per_cpu(cpu_irq, cpu) != irq)\n\t\t\tcontinue;\n\n\t\tops = per_cpu(cpu_irq_ops, cpu);\n\t\tif (ops)\n\t\t\tbreak;\n\t}\n\n\treturn ops;\n}\n\nvoid armpmu_free_irq(int irq, int cpu)\n{\n\tif (per_cpu(cpu_irq, cpu) == 0)\n\t\treturn;\n\tif (WARN_ON(irq != per_cpu(cpu_irq, cpu)))\n\t\treturn;\n\n\tper_cpu(cpu_irq_ops, cpu)->free_pmuirq(irq, cpu, &cpu_armpmu);\n\n\tper_cpu(cpu_irq, cpu) = 0;\n\tper_cpu(cpu_irq_ops, cpu) = NULL;\n}\n\nint armpmu_request_irq(int irq, int cpu)\n{\n\tint err = 0;\n\tconst irq_handler_t handler = armpmu_dispatch_irq;\n\tconst struct pmu_irq_ops *irq_ops;\n\n\tif (!irq)\n\t\treturn 0;\n\n\tif (!irq_is_percpu_devid(irq)) {\n\t\tunsigned long irq_flags;\n\n\t\terr = irq_force_affinity(irq, cpumask_of(cpu));\n\n\t\tif (err && num_possible_cpus() > 1) {\n\t\t\tpr_warn(\"unable to set irq affinity (irq=%d, cpu=%u)\\n\",\n\t\t\t\tirq, cpu);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tirq_flags = IRQF_PERCPU |\n\t\t\t    IRQF_NOBALANCING | IRQF_NO_AUTOEN |\n\t\t\t    IRQF_NO_THREAD;\n\n\t\terr = request_nmi(irq, handler, irq_flags, \"arm-pmu\",\n\t\t\t\t  per_cpu_ptr(&cpu_armpmu, cpu));\n\n\t\t \n\t\tif (err) {\n\t\t\terr = request_irq(irq, handler, irq_flags, \"arm-pmu\",\n\t\t\t\t\t  per_cpu_ptr(&cpu_armpmu, cpu));\n\t\t\tirq_ops = &pmuirq_ops;\n\t\t} else {\n\t\t\thas_nmi = true;\n\t\t\tirq_ops = &pmunmi_ops;\n\t\t}\n\t} else if (armpmu_count_irq_users(irq) == 0) {\n\t\terr = request_percpu_nmi(irq, handler, \"arm-pmu\", &cpu_armpmu);\n\n\t\t \n\t\tif (err) {\n\t\t\terr = request_percpu_irq(irq, handler, \"arm-pmu\",\n\t\t\t\t\t\t &cpu_armpmu);\n\t\t\tirq_ops = &percpu_pmuirq_ops;\n\t\t} else {\n\t\t\thas_nmi = true;\n\t\t\tirq_ops = &percpu_pmunmi_ops;\n\t\t}\n\t} else {\n\t\t \n\t\tirq_ops = armpmu_find_irq_ops(irq);\n\n\t\tif (WARN_ON(!irq_ops))\n\t\t\terr = -EINVAL;\n\t}\n\n\tif (err)\n\t\tgoto err_out;\n\n\tper_cpu(cpu_irq, cpu) = irq;\n\tper_cpu(cpu_irq_ops, cpu) = irq_ops;\n\treturn 0;\n\nerr_out:\n\tpr_err(\"unable to request IRQ%d for ARM PMU counters\\n\", irq);\n\treturn err;\n}\n\nstatic int armpmu_get_cpu_irq(struct arm_pmu *pmu, int cpu)\n{\n\tstruct pmu_hw_events __percpu *hw_events = pmu->hw_events;\n\treturn per_cpu(hw_events->irq, cpu);\n}\n\nbool arm_pmu_irq_is_nmi(void)\n{\n\treturn has_nmi;\n}\n\n \nstatic int arm_perf_starting_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct arm_pmu *pmu = hlist_entry_safe(node, struct arm_pmu, node);\n\tint irq;\n\n\tif (!cpumask_test_cpu(cpu, &pmu->supported_cpus))\n\t\treturn 0;\n\tif (pmu->reset)\n\t\tpmu->reset(pmu);\n\n\tper_cpu(cpu_armpmu, cpu) = pmu;\n\n\tirq = armpmu_get_cpu_irq(pmu, cpu);\n\tif (irq)\n\t\tper_cpu(cpu_irq_ops, cpu)->enable_pmuirq(irq);\n\n\treturn 0;\n}\n\nstatic int arm_perf_teardown_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct arm_pmu *pmu = hlist_entry_safe(node, struct arm_pmu, node);\n\tint irq;\n\n\tif (!cpumask_test_cpu(cpu, &pmu->supported_cpus))\n\t\treturn 0;\n\n\tirq = armpmu_get_cpu_irq(pmu, cpu);\n\tif (irq)\n\t\tper_cpu(cpu_irq_ops, cpu)->disable_pmuirq(irq);\n\n\tper_cpu(cpu_armpmu, cpu) = NULL;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_CPU_PM\nstatic void cpu_pm_pmu_setup(struct arm_pmu *armpmu, unsigned long cmd)\n{\n\tstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\n\tstruct perf_event *event;\n\tint idx;\n\n\tfor (idx = 0; idx < armpmu->num_events; idx++) {\n\t\tevent = hw_events->events[idx];\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\tswitch (cmd) {\n\t\tcase CPU_PM_ENTER:\n\t\t\t \n\t\t\tarmpmu_stop(event, PERF_EF_UPDATE);\n\t\t\tbreak;\n\t\tcase CPU_PM_EXIT:\n\t\tcase CPU_PM_ENTER_FAILED:\n\t\t\t  \n\t\t\tarmpmu_start(event, PERF_EF_RELOAD);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int cpu_pm_pmu_notify(struct notifier_block *b, unsigned long cmd,\n\t\t\t     void *v)\n{\n\tstruct arm_pmu *armpmu = container_of(b, struct arm_pmu, cpu_pm_nb);\n\tstruct pmu_hw_events *hw_events = this_cpu_ptr(armpmu->hw_events);\n\tbool enabled = !bitmap_empty(hw_events->used_mask, armpmu->num_events);\n\n\tif (!cpumask_test_cpu(smp_processor_id(), &armpmu->supported_cpus))\n\t\treturn NOTIFY_DONE;\n\n\t \n\tif (cmd == CPU_PM_EXIT && armpmu->reset)\n\t\tarmpmu->reset(armpmu);\n\n\tif (!enabled)\n\t\treturn NOTIFY_OK;\n\n\tswitch (cmd) {\n\tcase CPU_PM_ENTER:\n\t\tarmpmu->stop(armpmu);\n\t\tcpu_pm_pmu_setup(armpmu, cmd);\n\t\tbreak;\n\tcase CPU_PM_EXIT:\n\tcase CPU_PM_ENTER_FAILED:\n\t\tcpu_pm_pmu_setup(armpmu, cmd);\n\t\tarmpmu->start(armpmu);\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic int cpu_pm_pmu_register(struct arm_pmu *cpu_pmu)\n{\n\tcpu_pmu->cpu_pm_nb.notifier_call = cpu_pm_pmu_notify;\n\treturn cpu_pm_register_notifier(&cpu_pmu->cpu_pm_nb);\n}\n\nstatic void cpu_pm_pmu_unregister(struct arm_pmu *cpu_pmu)\n{\n\tcpu_pm_unregister_notifier(&cpu_pmu->cpu_pm_nb);\n}\n#else\nstatic inline int cpu_pm_pmu_register(struct arm_pmu *cpu_pmu) { return 0; }\nstatic inline void cpu_pm_pmu_unregister(struct arm_pmu *cpu_pmu) { }\n#endif\n\nstatic int cpu_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\tint err;\n\n\terr = cpuhp_state_add_instance(CPUHP_AP_PERF_ARM_STARTING,\n\t\t\t\t       &cpu_pmu->node);\n\tif (err)\n\t\tgoto out;\n\n\terr = cpu_pm_pmu_register(cpu_pmu);\n\tif (err)\n\t\tgoto out_unregister;\n\n\treturn 0;\n\nout_unregister:\n\tcpuhp_state_remove_instance_nocalls(CPUHP_AP_PERF_ARM_STARTING,\n\t\t\t\t\t    &cpu_pmu->node);\nout:\n\treturn err;\n}\n\nstatic void cpu_pmu_destroy(struct arm_pmu *cpu_pmu)\n{\n\tcpu_pm_pmu_unregister(cpu_pmu);\n\tcpuhp_state_remove_instance_nocalls(CPUHP_AP_PERF_ARM_STARTING,\n\t\t\t\t\t    &cpu_pmu->node);\n}\n\nstruct arm_pmu *armpmu_alloc(void)\n{\n\tstruct arm_pmu *pmu;\n\tint cpu;\n\n\tpmu = kzalloc(sizeof(*pmu), GFP_KERNEL);\n\tif (!pmu)\n\t\tgoto out;\n\n\tpmu->hw_events = alloc_percpu_gfp(struct pmu_hw_events, GFP_KERNEL);\n\tif (!pmu->hw_events) {\n\t\tpr_info(\"failed to allocate per-cpu PMU data.\\n\");\n\t\tgoto out_free_pmu;\n\t}\n\n\tpmu->pmu = (struct pmu) {\n\t\t.pmu_enable\t= armpmu_enable,\n\t\t.pmu_disable\t= armpmu_disable,\n\t\t.event_init\t= armpmu_event_init,\n\t\t.add\t\t= armpmu_add,\n\t\t.del\t\t= armpmu_del,\n\t\t.start\t\t= armpmu_start,\n\t\t.stop\t\t= armpmu_stop,\n\t\t.read\t\t= armpmu_read,\n\t\t.filter\t\t= armpmu_filter,\n\t\t.attr_groups\t= pmu->attr_groups,\n\t\t \n\t\t.capabilities\t= PERF_PMU_CAP_EXTENDED_REGS |\n\t\t\t\t  PERF_PMU_CAP_EXTENDED_HW_TYPE,\n\t};\n\n\tpmu->attr_groups[ARMPMU_ATTR_GROUP_COMMON] =\n\t\t&armpmu_common_attr_group;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct pmu_hw_events *events;\n\n\t\tevents = per_cpu_ptr(pmu->hw_events, cpu);\n\t\traw_spin_lock_init(&events->pmu_lock);\n\t\tevents->percpu_pmu = pmu;\n\t}\n\n\treturn pmu;\n\nout_free_pmu:\n\tkfree(pmu);\nout:\n\treturn NULL;\n}\n\nvoid armpmu_free(struct arm_pmu *pmu)\n{\n\tfree_percpu(pmu->hw_events);\n\tkfree(pmu);\n}\n\nint armpmu_register(struct arm_pmu *pmu)\n{\n\tint ret;\n\n\tret = cpu_pmu_init(pmu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!pmu->set_event_filter)\n\t\tpmu->pmu.capabilities |= PERF_PMU_CAP_NO_EXCLUDE;\n\n\tret = perf_pmu_register(&pmu->pmu, pmu->name, -1);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tpr_info(\"enabled with %s PMU driver, %d counters available%s\\n\",\n\t\tpmu->name, pmu->num_events,\n\t\thas_nmi ? \", using NMIs\" : \"\");\n\n\tkvm_host_pmu_init(pmu);\n\n\treturn 0;\n\nout_destroy:\n\tcpu_pmu_destroy(pmu);\n\treturn ret;\n}\n\nstatic int arm_pmu_hp_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_PERF_ARM_STARTING,\n\t\t\t\t      \"perf/arm/pmu:starting\",\n\t\t\t\t      arm_perf_starting_cpu,\n\t\t\t\t      arm_perf_teardown_cpu);\n\tif (ret)\n\t\tpr_err(\"CPU hotplug notifier for ARM PMU could not be registered: %d\\n\",\n\t\t       ret);\n\treturn ret;\n}\nsubsys_initcall(arm_pmu_hp_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}