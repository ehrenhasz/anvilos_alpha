{
  "module_name": "arm_dsu_pmu.c",
  "hash_id": "1fbe6b1aaf9d50035d5c3439c83c8a8f03ac085a53e4ff687c45b1d78505a3f6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/arm_dsu_pmu.c",
  "human_readable_source": "\n \n\n#define PMUNAME\t\t\"arm_dsu\"\n#define DRVNAME\t\tPMUNAME \"_pmu\"\n#define pr_fmt(fmt)\tDRVNAME \": \" fmt\n\n#include <linux/acpi.h>\n#include <linux/bitmap.h>\n#include <linux/bitops.h>\n#include <linux/bug.h>\n#include <linux/cpumask.h>\n#include <linux/device.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/sysfs.h>\n#include <linux/types.h>\n\n#include <asm/arm_dsu_pmu.h>\n#include <asm/local64.h>\n\n \n#define DSU_PMU_EVT_CYCLES\t\t0x11\n#define DSU_PMU_EVT_CHAIN\t\t0x1e\n\n#define DSU_PMU_MAX_COMMON_EVENTS\t0x40\n\n#define DSU_PMU_MAX_HW_CNTRS\t\t32\n#define DSU_PMU_HW_COUNTER_MASK\t\t(DSU_PMU_MAX_HW_CNTRS - 1)\n\n#define CLUSTERPMCR_E\t\t\tBIT(0)\n#define CLUSTERPMCR_P\t\t\tBIT(1)\n#define CLUSTERPMCR_C\t\t\tBIT(2)\n#define CLUSTERPMCR_N_SHIFT\t\t11\n#define CLUSTERPMCR_N_MASK\t\t0x1f\n#define CLUSTERPMCR_IDCODE_SHIFT\t16\n#define CLUSTERPMCR_IDCODE_MASK\t\t0xff\n#define CLUSTERPMCR_IMP_SHIFT\t\t24\n#define CLUSTERPMCR_IMP_MASK\t\t0xff\n#define CLUSTERPMCR_RES_MASK\t\t0x7e8\n#define CLUSTERPMCR_RES_VAL\t\t0x40\n\n#define DSU_ACTIVE_CPU_MASK\t\t0x0\n#define DSU_ASSOCIATED_CPU_MASK\t\t0x1\n\n \n#define DSU_PMU_IDX_CYCLE_COUNTER\t31\n\n \n#define DSU_PMU_COUNTER_WIDTH(idx)\t\\\n\t(((idx) == DSU_PMU_IDX_CYCLE_COUNTER) ? 64 : 32)\n\n#define DSU_PMU_COUNTER_MASK(idx)\t\\\n\tGENMASK_ULL((DSU_PMU_COUNTER_WIDTH((idx)) - 1), 0)\n\n#define DSU_EXT_ATTR(_name, _func, _config)\t\t\\\n\t(&((struct dev_ext_attribute[]) {\t\t\t\t\\\n\t\t{\t\t\t\t\t\t\t\\\n\t\t\t.attr = __ATTR(_name, 0444, _func, NULL),\t\\\n\t\t\t.var = (void *)_config\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t})[0].attr.attr)\n\n#define DSU_EVENT_ATTR(_name, _config)\t\t\\\n\tDSU_EXT_ATTR(_name, dsu_pmu_sysfs_event_show, (unsigned long)_config)\n\n#define DSU_FORMAT_ATTR(_name, _config)\t\t\\\n\tDSU_EXT_ATTR(_name, dsu_pmu_sysfs_format_show, (char *)_config)\n\n#define DSU_CPUMASK_ATTR(_name, _config)\t\\\n\tDSU_EXT_ATTR(_name, dsu_pmu_cpumask_show, (unsigned long)_config)\n\nstruct dsu_hw_events {\n\tDECLARE_BITMAP(used_mask, DSU_PMU_MAX_HW_CNTRS);\n\tstruct perf_event\t*events[DSU_PMU_MAX_HW_CNTRS];\n};\n\n \nstruct dsu_pmu {\n\tstruct pmu\t\t\tpmu;\n\tstruct device\t\t\t*dev;\n\traw_spinlock_t\t\t\tpmu_lock;\n\tstruct dsu_hw_events\t\thw_events;\n\tcpumask_t\t\t\tassociated_cpus;\n\tcpumask_t\t\t\tactive_cpu;\n\tstruct hlist_node\t\tcpuhp_node;\n\ts8\t\t\t\tnum_counters;\n\tint\t\t\t\tirq;\n\tDECLARE_BITMAP(cpmceid_bitmap, DSU_PMU_MAX_COMMON_EVENTS);\n};\n\nstatic unsigned long dsu_pmu_cpuhp_state;\n\nstatic inline struct dsu_pmu *to_dsu_pmu(struct pmu *pmu)\n{\n\treturn container_of(pmu, struct dsu_pmu, pmu);\n}\n\nstatic ssize_t dsu_pmu_sysfs_event_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\t\tstruct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"event=0x%lx\\n\", (unsigned long)eattr->var);\n}\n\nstatic ssize_t dsu_pmu_sysfs_format_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\t\tstruct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"%s\\n\", (char *)eattr->var);\n}\n\nstatic ssize_t dsu_pmu_cpumask_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    char *buf)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(pmu);\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\t\tstruct dev_ext_attribute, attr);\n\tunsigned long mask_id = (unsigned long)eattr->var;\n\tconst cpumask_t *cpumask;\n\n\tswitch (mask_id) {\n\tcase DSU_ACTIVE_CPU_MASK:\n\t\tcpumask = &dsu_pmu->active_cpu;\n\t\tbreak;\n\tcase DSU_ASSOCIATED_CPU_MASK:\n\t\tcpumask = &dsu_pmu->associated_cpus;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask);\n}\n\nstatic struct attribute *dsu_pmu_format_attrs[] = {\n\tDSU_FORMAT_ATTR(event, \"config:0-31\"),\n\tNULL,\n};\n\nstatic const struct attribute_group dsu_pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = dsu_pmu_format_attrs,\n};\n\nstatic struct attribute *dsu_pmu_event_attrs[] = {\n\tDSU_EVENT_ATTR(cycles, 0x11),\n\tDSU_EVENT_ATTR(bus_access, 0x19),\n\tDSU_EVENT_ATTR(memory_error, 0x1a),\n\tDSU_EVENT_ATTR(bus_cycles, 0x1d),\n\tDSU_EVENT_ATTR(l3d_cache_allocate, 0x29),\n\tDSU_EVENT_ATTR(l3d_cache_refill, 0x2a),\n\tDSU_EVENT_ATTR(l3d_cache, 0x2b),\n\tDSU_EVENT_ATTR(l3d_cache_wb, 0x2c),\n\tNULL,\n};\n\nstatic umode_t\ndsu_pmu_event_attr_is_visible(struct kobject *kobj, struct attribute *attr,\n\t\t\t\tint unused)\n{\n\tstruct pmu *pmu = dev_get_drvdata(kobj_to_dev(kobj));\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(pmu);\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\t\tstruct dev_ext_attribute, attr.attr);\n\tunsigned long evt = (unsigned long)eattr->var;\n\n\treturn test_bit(evt, dsu_pmu->cpmceid_bitmap) ? attr->mode : 0;\n}\n\nstatic const struct attribute_group dsu_pmu_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = dsu_pmu_event_attrs,\n\t.is_visible = dsu_pmu_event_attr_is_visible,\n};\n\nstatic struct attribute *dsu_pmu_cpumask_attrs[] = {\n\tDSU_CPUMASK_ATTR(cpumask, DSU_ACTIVE_CPU_MASK),\n\tDSU_CPUMASK_ATTR(associated_cpus, DSU_ASSOCIATED_CPU_MASK),\n\tNULL,\n};\n\nstatic const struct attribute_group dsu_pmu_cpumask_attr_group = {\n\t.attrs = dsu_pmu_cpumask_attrs,\n};\n\nstatic const struct attribute_group *dsu_pmu_attr_groups[] = {\n\t&dsu_pmu_cpumask_attr_group,\n\t&dsu_pmu_events_attr_group,\n\t&dsu_pmu_format_attr_group,\n\tNULL,\n};\n\nstatic int dsu_pmu_get_online_cpu_any_but(struct dsu_pmu *dsu_pmu, int cpu)\n{\n\tstruct cpumask online_supported;\n\n\tcpumask_and(&online_supported,\n\t\t\t &dsu_pmu->associated_cpus, cpu_online_mask);\n\treturn cpumask_any_but(&online_supported, cpu);\n}\n\nstatic inline bool dsu_pmu_counter_valid(struct dsu_pmu *dsu_pmu, u32 idx)\n{\n\treturn (idx < dsu_pmu->num_counters) ||\n\t       (idx == DSU_PMU_IDX_CYCLE_COUNTER);\n}\n\nstatic inline u64 dsu_pmu_read_counter(struct perf_event *event)\n{\n\tu64 val;\n\tunsigned long flags;\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON(!cpumask_test_cpu(smp_processor_id(),\n\t\t\t\t &dsu_pmu->associated_cpus)))\n\t\treturn 0;\n\n\tif (!dsu_pmu_counter_valid(dsu_pmu, idx)) {\n\t\tdev_err(event->pmu->dev,\n\t\t\t\"Trying reading invalid counter %d\\n\", idx);\n\t\treturn 0;\n\t}\n\n\traw_spin_lock_irqsave(&dsu_pmu->pmu_lock, flags);\n\tif (idx == DSU_PMU_IDX_CYCLE_COUNTER)\n\t\tval = __dsu_pmu_read_pmccntr();\n\telse\n\t\tval = __dsu_pmu_read_counter(idx);\n\traw_spin_unlock_irqrestore(&dsu_pmu->pmu_lock, flags);\n\n\treturn val;\n}\n\nstatic void dsu_pmu_write_counter(struct perf_event *event, u64 val)\n{\n\tunsigned long flags;\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON(!cpumask_test_cpu(smp_processor_id(),\n\t\t\t &dsu_pmu->associated_cpus)))\n\t\treturn;\n\n\tif (!dsu_pmu_counter_valid(dsu_pmu, idx)) {\n\t\tdev_err(event->pmu->dev,\n\t\t\t\"writing to invalid counter %d\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&dsu_pmu->pmu_lock, flags);\n\tif (idx == DSU_PMU_IDX_CYCLE_COUNTER)\n\t\t__dsu_pmu_write_pmccntr(val);\n\telse\n\t\t__dsu_pmu_write_counter(idx, val);\n\traw_spin_unlock_irqrestore(&dsu_pmu->pmu_lock, flags);\n}\n\nstatic int dsu_pmu_get_event_idx(struct dsu_hw_events *hw_events,\n\t\t\t\t struct perf_event *event)\n{\n\tint idx;\n\tunsigned long evtype = event->attr.config;\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\tunsigned long *used_mask = hw_events->used_mask;\n\n\tif (evtype == DSU_PMU_EVT_CYCLES) {\n\t\tif (test_and_set_bit(DSU_PMU_IDX_CYCLE_COUNTER, used_mask))\n\t\t\treturn -EAGAIN;\n\t\treturn DSU_PMU_IDX_CYCLE_COUNTER;\n\t}\n\n\tidx = find_first_zero_bit(used_mask, dsu_pmu->num_counters);\n\tif (idx >= dsu_pmu->num_counters)\n\t\treturn -EAGAIN;\n\tset_bit(idx, hw_events->used_mask);\n\treturn idx;\n}\n\nstatic void dsu_pmu_enable_counter(struct dsu_pmu *dsu_pmu, int idx)\n{\n\t__dsu_pmu_counter_interrupt_enable(idx);\n\t__dsu_pmu_enable_counter(idx);\n}\n\nstatic void dsu_pmu_disable_counter(struct dsu_pmu *dsu_pmu, int idx)\n{\n\t__dsu_pmu_disable_counter(idx);\n\t__dsu_pmu_counter_interrupt_disable(idx);\n}\n\nstatic inline void dsu_pmu_set_event(struct dsu_pmu *dsu_pmu,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tint idx = event->hw.idx;\n\tunsigned long flags;\n\n\tif (!dsu_pmu_counter_valid(dsu_pmu, idx)) {\n\t\tdev_err(event->pmu->dev,\n\t\t\t\"Trying to set invalid counter %d\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&dsu_pmu->pmu_lock, flags);\n\t__dsu_pmu_set_event(idx, event->hw.config_base);\n\traw_spin_unlock_irqrestore(&dsu_pmu->pmu_lock, flags);\n}\n\nstatic void dsu_pmu_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 delta, prev_count, new_count;\n\n\tdo {\n\t\t \n\t\tprev_count = local64_read(&hwc->prev_count);\n\t\tnew_count = dsu_pmu_read_counter(event);\n\t} while (local64_cmpxchg(&hwc->prev_count, prev_count, new_count) !=\n\t\t\tprev_count);\n\tdelta = (new_count - prev_count) & DSU_PMU_COUNTER_MASK(hwc->idx);\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void dsu_pmu_read(struct perf_event *event)\n{\n\tdsu_pmu_event_update(event);\n}\n\nstatic inline u32 dsu_pmu_get_reset_overflow(void)\n{\n\treturn __dsu_pmu_get_reset_overflow();\n}\n\n \nstatic void dsu_pmu_set_event_period(struct perf_event *event)\n{\n\tint idx = event->hw.idx;\n\tu64 val = DSU_PMU_COUNTER_MASK(idx) >> 1;\n\n\tlocal64_set(&event->hw.prev_count, val);\n\tdsu_pmu_write_counter(event, val);\n}\n\nstatic irqreturn_t dsu_pmu_handle_irq(int irq_num, void *dev)\n{\n\tint i;\n\tbool handled = false;\n\tstruct dsu_pmu *dsu_pmu = dev;\n\tstruct dsu_hw_events *hw_events = &dsu_pmu->hw_events;\n\tunsigned long overflow;\n\n\toverflow = dsu_pmu_get_reset_overflow();\n\tif (!overflow)\n\t\treturn IRQ_NONE;\n\n\tfor_each_set_bit(i, &overflow, DSU_PMU_MAX_HW_CNTRS) {\n\t\tstruct perf_event *event = hw_events->events[i];\n\n\t\tif (!event)\n\t\t\tcontinue;\n\t\tdsu_pmu_event_update(event);\n\t\tdsu_pmu_set_event_period(event);\n\t\thandled = true;\n\t}\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic void dsu_pmu_start(struct perf_event *event, int pmu_flags)\n{\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\n\t \n\tif (pmu_flags & PERF_EF_RELOAD)\n\t\tWARN_ON(!(event->hw.state & PERF_HES_UPTODATE));\n\tdsu_pmu_set_event_period(event);\n\tif (event->hw.idx != DSU_PMU_IDX_CYCLE_COUNTER)\n\t\tdsu_pmu_set_event(dsu_pmu, event);\n\tevent->hw.state = 0;\n\tdsu_pmu_enable_counter(dsu_pmu, event->hw.idx);\n}\n\nstatic void dsu_pmu_stop(struct perf_event *event, int pmu_flags)\n{\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\n\tif (event->hw.state & PERF_HES_STOPPED)\n\t\treturn;\n\tdsu_pmu_disable_counter(dsu_pmu, event->hw.idx);\n\tdsu_pmu_event_update(event);\n\tevent->hw.state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n}\n\nstatic int dsu_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\tstruct dsu_hw_events *hw_events = &dsu_pmu->hw_events;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\tif (WARN_ON_ONCE(!cpumask_test_cpu(smp_processor_id(),\n\t\t\t\t\t   &dsu_pmu->associated_cpus)))\n\t\treturn -ENOENT;\n\n\tidx = dsu_pmu_get_event_idx(hw_events, event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\thwc->idx = idx;\n\thw_events->events[idx] = event;\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\tif (flags & PERF_EF_START)\n\t\tdsu_pmu_start(event, PERF_EF_RELOAD);\n\n\tperf_event_update_userpage(event);\n\treturn 0;\n}\n\nstatic void dsu_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\tstruct dsu_hw_events *hw_events = &dsu_pmu->hw_events;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tdsu_pmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\tperf_event_update_userpage(event);\n}\n\nstatic void dsu_pmu_enable(struct pmu *pmu)\n{\n\tu32 pmcr;\n\tunsigned long flags;\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(pmu);\n\n\t \n\tif (bitmap_empty(dsu_pmu->hw_events.used_mask, DSU_PMU_MAX_HW_CNTRS))\n\t\treturn;\n\n\traw_spin_lock_irqsave(&dsu_pmu->pmu_lock, flags);\n\tpmcr = __dsu_pmu_read_pmcr();\n\tpmcr |= CLUSTERPMCR_E;\n\t__dsu_pmu_write_pmcr(pmcr);\n\traw_spin_unlock_irqrestore(&dsu_pmu->pmu_lock, flags);\n}\n\nstatic void dsu_pmu_disable(struct pmu *pmu)\n{\n\tu32 pmcr;\n\tunsigned long flags;\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(pmu);\n\n\traw_spin_lock_irqsave(&dsu_pmu->pmu_lock, flags);\n\tpmcr = __dsu_pmu_read_pmcr();\n\tpmcr &= ~CLUSTERPMCR_E;\n\t__dsu_pmu_write_pmcr(pmcr);\n\traw_spin_unlock_irqrestore(&dsu_pmu->pmu_lock, flags);\n}\n\nstatic bool dsu_pmu_validate_event(struct pmu *pmu,\n\t\t\t\t  struct dsu_hw_events *hw_events,\n\t\t\t\t  struct perf_event *event)\n{\n\tif (is_software_event(event))\n\t\treturn true;\n\t \n\tif (event->pmu != pmu)\n\t\treturn false;\n\treturn dsu_pmu_get_event_idx(hw_events, event) >= 0;\n}\n\n \nstatic bool dsu_pmu_validate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct dsu_hw_events fake_hw;\n\n\tif (event->group_leader == event)\n\t\treturn true;\n\n\tmemset(fake_hw.used_mask, 0, sizeof(fake_hw.used_mask));\n\tif (!dsu_pmu_validate_event(event->pmu, &fake_hw, leader))\n\t\treturn false;\n\tfor_each_sibling_event(sibling, leader) {\n\t\tif (!dsu_pmu_validate_event(event->pmu, &fake_hw, sibling))\n\t\t\treturn false;\n\t}\n\treturn dsu_pmu_validate_event(event->pmu, &fake_hw, event);\n}\n\nstatic int dsu_pmu_event_init(struct perf_event *event)\n{\n\tstruct dsu_pmu *dsu_pmu = to_dsu_pmu(event->pmu);\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (is_sampling_event(event)) {\n\t\tdev_dbg(dsu_pmu->pmu.dev, \"Can't support sampling events\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (event->cpu < 0 || event->attach_state & PERF_ATTACH_TASK) {\n\t\tdev_dbg(dsu_pmu->pmu.dev, \"Can't support per-task counters\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (has_branch_stack(event)) {\n\t\tdev_dbg(dsu_pmu->pmu.dev, \"Can't support filtering\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!cpumask_test_cpu(event->cpu, &dsu_pmu->associated_cpus)) {\n\t\tdev_dbg(dsu_pmu->pmu.dev,\n\t\t\t \"Requested cpu is not associated with the DSU\\n\");\n\t\treturn -EINVAL;\n\t}\n\t \n\tevent->cpu = cpumask_first(&dsu_pmu->active_cpu);\n\tif (event->cpu >= nr_cpu_ids)\n\t\treturn -EINVAL;\n\tif (!dsu_pmu_validate_group(event))\n\t\treturn -EINVAL;\n\n\tevent->hw.config_base = event->attr.config;\n\treturn 0;\n}\n\nstatic struct dsu_pmu *dsu_pmu_alloc(struct platform_device *pdev)\n{\n\tstruct dsu_pmu *dsu_pmu;\n\n\tdsu_pmu = devm_kzalloc(&pdev->dev, sizeof(*dsu_pmu), GFP_KERNEL);\n\tif (!dsu_pmu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\traw_spin_lock_init(&dsu_pmu->pmu_lock);\n\t \n\tdsu_pmu->num_counters = -1;\n\treturn dsu_pmu;\n}\n\n \nstatic int dsu_pmu_dt_get_cpus(struct device *dev, cpumask_t *mask)\n{\n\tint i = 0, n, cpu;\n\tstruct device_node *cpu_node;\n\n\tn = of_count_phandle_with_args(dev->of_node, \"cpus\", NULL);\n\tif (n <= 0)\n\t\treturn -ENODEV;\n\tfor (; i < n; i++) {\n\t\tcpu_node = of_parse_phandle(dev->of_node, \"cpus\", i);\n\t\tif (!cpu_node)\n\t\t\tbreak;\n\t\tcpu = of_cpu_node_to_id(cpu_node);\n\t\tof_node_put(cpu_node);\n\t\t \n\t\tif (cpu < 0)\n\t\t\tcontinue;\n\t\tcpumask_set_cpu(cpu, mask);\n\t}\n\treturn 0;\n}\n\n \nstatic int dsu_pmu_acpi_get_cpus(struct device *dev, cpumask_t *mask)\n{\n#ifdef CONFIG_ACPI\n\tstruct acpi_device *parent_adev = acpi_dev_parent(ACPI_COMPANION(dev));\n\tint cpu;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct acpi_device *acpi_dev;\n\t\tstruct device *cpu_dev = get_cpu_device(cpu);\n\n\t\tif (!cpu_dev)\n\t\t\tcontinue;\n\n\t\tacpi_dev = ACPI_COMPANION(cpu_dev);\n\t\tif (acpi_dev && acpi_dev_parent(acpi_dev) == parent_adev)\n\t\t\tcpumask_set_cpu(cpu, mask);\n\t}\n#endif\n\n\treturn 0;\n}\n\n \nstatic void dsu_pmu_probe_pmu(struct dsu_pmu *dsu_pmu)\n{\n\tu64 num_counters;\n\tu32 cpmceid[2];\n\n\tnum_counters = (__dsu_pmu_read_pmcr() >> CLUSTERPMCR_N_SHIFT) &\n\t\t\t\t\t\tCLUSTERPMCR_N_MASK;\n\t \n\tif (WARN_ON(num_counters > 31))\n\t\tnum_counters = 31;\n\tdsu_pmu->num_counters = num_counters;\n\tif (!dsu_pmu->num_counters)\n\t\treturn;\n\tcpmceid[0] = __dsu_pmu_read_pmceid(0);\n\tcpmceid[1] = __dsu_pmu_read_pmceid(1);\n\tbitmap_from_arr32(dsu_pmu->cpmceid_bitmap, cpmceid,\n\t\t\t  DSU_PMU_MAX_COMMON_EVENTS);\n}\n\nstatic void dsu_pmu_set_active_cpu(int cpu, struct dsu_pmu *dsu_pmu)\n{\n\tcpumask_set_cpu(cpu, &dsu_pmu->active_cpu);\n\tif (irq_set_affinity(dsu_pmu->irq, &dsu_pmu->active_cpu))\n\t\tpr_warn(\"Failed to set irq affinity to %d\\n\", cpu);\n}\n\n \nstatic void dsu_pmu_init_pmu(struct dsu_pmu *dsu_pmu)\n{\n\tif (dsu_pmu->num_counters == -1)\n\t\tdsu_pmu_probe_pmu(dsu_pmu);\n\t \n\tdsu_pmu_get_reset_overflow();\n}\n\nstatic int dsu_pmu_device_probe(struct platform_device *pdev)\n{\n\tint irq, rc;\n\tstruct dsu_pmu *dsu_pmu;\n\tstruct fwnode_handle *fwnode = dev_fwnode(&pdev->dev);\n\tchar *name;\n\tstatic atomic_t pmu_idx = ATOMIC_INIT(-1);\n\n\tdsu_pmu = dsu_pmu_alloc(pdev);\n\tif (IS_ERR(dsu_pmu))\n\t\treturn PTR_ERR(dsu_pmu);\n\n\tif (is_of_node(fwnode))\n\t\trc = dsu_pmu_dt_get_cpus(&pdev->dev, &dsu_pmu->associated_cpus);\n\telse if (is_acpi_device_node(fwnode))\n\t\trc = dsu_pmu_acpi_get_cpus(&pdev->dev, &dsu_pmu->associated_cpus);\n\telse\n\t\treturn -ENOENT;\n\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"Failed to parse the CPUs\\n\");\n\t\treturn rc;\n\t}\n\n\tirq = platform_get_irq(pdev, 0);\n\tif (irq < 0)\n\t\treturn -EINVAL;\n\n\tname = devm_kasprintf(&pdev->dev, GFP_KERNEL, \"%s_%d\",\n\t\t\t\tPMUNAME, atomic_inc_return(&pmu_idx));\n\tif (!name)\n\t\treturn -ENOMEM;\n\trc = devm_request_irq(&pdev->dev, irq, dsu_pmu_handle_irq,\n\t\t\t      IRQF_NOBALANCING, name, dsu_pmu);\n\tif (rc) {\n\t\tdev_warn(&pdev->dev, \"Failed to request IRQ %d\\n\", irq);\n\t\treturn rc;\n\t}\n\n\tdsu_pmu->irq = irq;\n\tplatform_set_drvdata(pdev, dsu_pmu);\n\trc = cpuhp_state_add_instance(dsu_pmu_cpuhp_state,\n\t\t\t\t\t\t&dsu_pmu->cpuhp_node);\n\tif (rc)\n\t\treturn rc;\n\n\tdsu_pmu->pmu = (struct pmu) {\n\t\t.task_ctx_nr\t= perf_invalid_context,\n\t\t.module\t\t= THIS_MODULE,\n\t\t.pmu_enable\t= dsu_pmu_enable,\n\t\t.pmu_disable\t= dsu_pmu_disable,\n\t\t.event_init\t= dsu_pmu_event_init,\n\t\t.add\t\t= dsu_pmu_add,\n\t\t.del\t\t= dsu_pmu_del,\n\t\t.start\t\t= dsu_pmu_start,\n\t\t.stop\t\t= dsu_pmu_stop,\n\t\t.read\t\t= dsu_pmu_read,\n\n\t\t.attr_groups\t= dsu_pmu_attr_groups,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\trc = perf_pmu_register(&dsu_pmu->pmu, name, -1);\n\tif (rc) {\n\t\tcpuhp_state_remove_instance(dsu_pmu_cpuhp_state,\n\t\t\t\t\t\t &dsu_pmu->cpuhp_node);\n\t}\n\n\treturn rc;\n}\n\nstatic int dsu_pmu_device_remove(struct platform_device *pdev)\n{\n\tstruct dsu_pmu *dsu_pmu = platform_get_drvdata(pdev);\n\n\tperf_pmu_unregister(&dsu_pmu->pmu);\n\tcpuhp_state_remove_instance(dsu_pmu_cpuhp_state, &dsu_pmu->cpuhp_node);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id dsu_pmu_of_match[] = {\n\t{ .compatible = \"arm,dsu-pmu\", },\n\t{},\n};\nMODULE_DEVICE_TABLE(of, dsu_pmu_of_match);\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id dsu_pmu_acpi_match[] = {\n\t{ \"ARMHD500\", 0},\n\t{},\n};\nMODULE_DEVICE_TABLE(acpi, dsu_pmu_acpi_match);\n#endif\n\nstatic struct platform_driver dsu_pmu_driver = {\n\t.driver = {\n\t\t.name\t= DRVNAME,\n\t\t.of_match_table = of_match_ptr(dsu_pmu_of_match),\n\t\t.acpi_match_table = ACPI_PTR(dsu_pmu_acpi_match),\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe = dsu_pmu_device_probe,\n\t.remove = dsu_pmu_device_remove,\n};\n\nstatic int dsu_pmu_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct dsu_pmu *dsu_pmu = hlist_entry_safe(node, struct dsu_pmu,\n\t\t\t\t\t\t   cpuhp_node);\n\n\tif (!cpumask_test_cpu(cpu, &dsu_pmu->associated_cpus))\n\t\treturn 0;\n\n\t \n\tif (!cpumask_empty(&dsu_pmu->active_cpu))\n\t\treturn 0;\n\n\tdsu_pmu_init_pmu(dsu_pmu);\n\tdsu_pmu_set_active_cpu(cpu, dsu_pmu);\n\n\treturn 0;\n}\n\nstatic int dsu_pmu_cpu_teardown(unsigned int cpu, struct hlist_node *node)\n{\n\tint dst;\n\tstruct dsu_pmu *dsu_pmu = hlist_entry_safe(node, struct dsu_pmu,\n\t\t\t\t\t\t   cpuhp_node);\n\n\tif (!cpumask_test_and_clear_cpu(cpu, &dsu_pmu->active_cpu))\n\t\treturn 0;\n\n\tdst = dsu_pmu_get_online_cpu_any_but(dsu_pmu, cpu);\n\t \n\tif (dst >= nr_cpu_ids)\n\t\treturn 0;\n\n\tperf_pmu_migrate_context(&dsu_pmu->pmu, cpu, dst);\n\tdsu_pmu_set_active_cpu(dst, dsu_pmu);\n\n\treturn 0;\n}\n\nstatic int __init dsu_pmu_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t\tDRVNAME,\n\t\t\t\t\tdsu_pmu_cpu_online,\n\t\t\t\t\tdsu_pmu_cpu_teardown);\n\tif (ret < 0)\n\t\treturn ret;\n\tdsu_pmu_cpuhp_state = ret;\n\tret = platform_driver_register(&dsu_pmu_driver);\n\tif (ret)\n\t\tcpuhp_remove_multi_state(dsu_pmu_cpuhp_state);\n\n\treturn ret;\n}\n\nstatic void __exit dsu_pmu_exit(void)\n{\n\tplatform_driver_unregister(&dsu_pmu_driver);\n\tcpuhp_remove_multi_state(dsu_pmu_cpuhp_state);\n}\n\nmodule_init(dsu_pmu_init);\nmodule_exit(dsu_pmu_exit);\n\nMODULE_DESCRIPTION(\"Perf driver for ARM DynamIQ Shared Unit\");\nMODULE_AUTHOR(\"Suzuki K Poulose <suzuki.poulose@arm.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}