{
  "module_name": "arm_pmuv3.c",
  "hash_id": "a5c2e16bc4c107cf4419a0e6b56226847abf1773cd01a65bee870272ff006be6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/arm_pmuv3.c",
  "human_readable_source": "\n \n\n#include <asm/irq_regs.h>\n#include <asm/perf_event.h>\n#include <asm/virt.h>\n\n#include <clocksource/arm_arch_timer.h>\n\n#include <linux/acpi.h>\n#include <linux/clocksource.h>\n#include <linux/of.h>\n#include <linux/perf/arm_pmu.h>\n#include <linux/perf/arm_pmuv3.h>\n#include <linux/platform_device.h>\n#include <linux/sched_clock.h>\n#include <linux/smp.h>\n#include <linux/nmi.h>\n\n#include <asm/arm_pmuv3.h>\n\n \n#define ARMV8_A53_PERFCTR_PREF_LINEFILL\t\t\t\t0xC2\n\n \n#define ARMV8_THUNDER_PERFCTR_L1D_CACHE_MISS_ST\t\t\t0xE9\n#define ARMV8_THUNDER_PERFCTR_L1D_CACHE_PREF_ACCESS\t\t0xEA\n#define ARMV8_THUNDER_PERFCTR_L1D_CACHE_PREF_MISS\t\t0xEB\n#define ARMV8_THUNDER_PERFCTR_L1I_CACHE_PREF_ACCESS\t\t0xEC\n#define ARMV8_THUNDER_PERFCTR_L1I_CACHE_PREF_MISS\t\t0xED\n\n \nstatic const unsigned armv8_pmuv3_perf_map[PERF_COUNT_HW_MAX] = {\n\tPERF_MAP_ALL_UNSUPPORTED,\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= ARMV8_PMUV3_PERFCTR_CPU_CYCLES,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= ARMV8_PMUV3_PERFCTR_INST_RETIRED,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= ARMV8_PMUV3_PERFCTR_L1D_CACHE,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_L1D_CACHE_REFILL,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= ARMV8_PMUV3_PERFCTR_BR_MIS_PRED,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t\t= ARMV8_PMUV3_PERFCTR_BUS_CYCLES,\n\t[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]\t= ARMV8_PMUV3_PERFCTR_STALL_FRONTEND,\n\t[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]\t= ARMV8_PMUV3_PERFCTR_STALL_BACKEND,\n};\n\nstatic const unsigned armv8_pmuv3_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1D_CACHE,\n\t[C(L1D)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1D_CACHE_REFILL,\n\n\t[C(L1I)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1I_CACHE,\n\t[C(L1I)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1I_CACHE_REFILL,\n\n\t[C(DTLB)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1D_TLB_REFILL,\n\t[C(DTLB)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1D_TLB,\n\n\t[C(ITLB)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_L1I_TLB_REFILL,\n\t[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_L1I_TLB,\n\n\t[C(LL)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_LL_CACHE_MISS_RD,\n\t[C(LL)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_LL_CACHE_RD,\n\n\t[C(BPU)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_PMUV3_PERFCTR_BR_PRED,\n\t[C(BPU)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_PMUV3_PERFCTR_BR_MIS_PRED,\n};\n\nstatic const unsigned armv8_a53_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_PREFETCH)][C(RESULT_MISS)] = ARMV8_A53_PERFCTR_PREF_LINEFILL,\n\n\t[C(NODE)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_RD,\n\t[C(NODE)][C(OP_WRITE)][C(RESULT_ACCESS)] = ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_WR,\n};\n\nstatic const unsigned armv8_a57_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_RD,\n\t[C(L1D)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_REFILL_RD,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_WR,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_REFILL_WR,\n\n\t[C(DTLB)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_RD,\n\t[C(DTLB)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_WR,\n\n\t[C(NODE)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_RD,\n\t[C(NODE)][C(OP_WRITE)][C(RESULT_ACCESS)] = ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_WR,\n};\n\nstatic const unsigned armv8_a73_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_RD,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_WR,\n};\n\nstatic const unsigned armv8_thunder_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t\t   [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t\t   [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_RD,\n\t[C(L1D)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_REFILL_RD,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_WR,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_THUNDER_PERFCTR_L1D_CACHE_MISS_ST,\n\t[C(L1D)][C(OP_PREFETCH)][C(RESULT_ACCESS)] = ARMV8_THUNDER_PERFCTR_L1D_CACHE_PREF_ACCESS,\n\t[C(L1D)][C(OP_PREFETCH)][C(RESULT_MISS)] = ARMV8_THUNDER_PERFCTR_L1D_CACHE_PREF_MISS,\n\n\t[C(L1I)][C(OP_PREFETCH)][C(RESULT_ACCESS)] = ARMV8_THUNDER_PERFCTR_L1I_CACHE_PREF_ACCESS,\n\t[C(L1I)][C(OP_PREFETCH)][C(RESULT_MISS)] = ARMV8_THUNDER_PERFCTR_L1I_CACHE_PREF_MISS,\n\n\t[C(DTLB)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_RD,\n\t[C(DTLB)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_RD,\n\t[C(DTLB)][C(OP_WRITE)][C(RESULT_ACCESS)] = ARMV8_IMPDEF_PERFCTR_L1D_TLB_WR,\n\t[C(DTLB)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_WR,\n};\n\nstatic const unsigned armv8_vulcan_perf_cache_map[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t      [PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\tPERF_CACHE_MAP_ALL_UNSUPPORTED,\n\n\t[C(L1D)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_RD,\n\t[C(L1D)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_REFILL_RD,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_WR,\n\t[C(L1D)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_CACHE_REFILL_WR,\n\n\t[C(DTLB)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_RD,\n\t[C(DTLB)][C(OP_WRITE)][C(RESULT_ACCESS)] = ARMV8_IMPDEF_PERFCTR_L1D_TLB_WR,\n\t[C(DTLB)][C(OP_READ)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_RD,\n\t[C(DTLB)][C(OP_WRITE)][C(RESULT_MISS)]\t= ARMV8_IMPDEF_PERFCTR_L1D_TLB_REFILL_WR,\n\n\t[C(NODE)][C(OP_READ)][C(RESULT_ACCESS)]\t= ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_RD,\n\t[C(NODE)][C(OP_WRITE)][C(RESULT_ACCESS)] = ARMV8_IMPDEF_PERFCTR_BUS_ACCESS_WR,\n};\n\nstatic ssize_t\narmv8pmu_events_sysfs_show(struct device *dev,\n\t\t\t   struct device_attribute *attr, char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\n\treturn sprintf(page, \"event=0x%04llx\\n\", pmu_attr->id);\n}\n\n#define ARMV8_EVENT_ATTR(name, config)\t\t\t\t\t\t\\\n\tPMU_EVENT_ATTR_ID(name, armv8pmu_events_sysfs_show, config)\n\nstatic struct attribute *armv8_pmuv3_event_attrs[] = {\n\tARMV8_EVENT_ATTR(sw_incr, ARMV8_PMUV3_PERFCTR_SW_INCR),\n\tARMV8_EVENT_ATTR(l1i_cache_refill, ARMV8_PMUV3_PERFCTR_L1I_CACHE_REFILL),\n\tARMV8_EVENT_ATTR(l1i_tlb_refill, ARMV8_PMUV3_PERFCTR_L1I_TLB_REFILL),\n\tARMV8_EVENT_ATTR(l1d_cache_refill, ARMV8_PMUV3_PERFCTR_L1D_CACHE_REFILL),\n\tARMV8_EVENT_ATTR(l1d_cache, ARMV8_PMUV3_PERFCTR_L1D_CACHE),\n\tARMV8_EVENT_ATTR(l1d_tlb_refill, ARMV8_PMUV3_PERFCTR_L1D_TLB_REFILL),\n\tARMV8_EVENT_ATTR(ld_retired, ARMV8_PMUV3_PERFCTR_LD_RETIRED),\n\tARMV8_EVENT_ATTR(st_retired, ARMV8_PMUV3_PERFCTR_ST_RETIRED),\n\tARMV8_EVENT_ATTR(inst_retired, ARMV8_PMUV3_PERFCTR_INST_RETIRED),\n\tARMV8_EVENT_ATTR(exc_taken, ARMV8_PMUV3_PERFCTR_EXC_TAKEN),\n\tARMV8_EVENT_ATTR(exc_return, ARMV8_PMUV3_PERFCTR_EXC_RETURN),\n\tARMV8_EVENT_ATTR(cid_write_retired, ARMV8_PMUV3_PERFCTR_CID_WRITE_RETIRED),\n\tARMV8_EVENT_ATTR(pc_write_retired, ARMV8_PMUV3_PERFCTR_PC_WRITE_RETIRED),\n\tARMV8_EVENT_ATTR(br_immed_retired, ARMV8_PMUV3_PERFCTR_BR_IMMED_RETIRED),\n\tARMV8_EVENT_ATTR(br_return_retired, ARMV8_PMUV3_PERFCTR_BR_RETURN_RETIRED),\n\tARMV8_EVENT_ATTR(unaligned_ldst_retired, ARMV8_PMUV3_PERFCTR_UNALIGNED_LDST_RETIRED),\n\tARMV8_EVENT_ATTR(br_mis_pred, ARMV8_PMUV3_PERFCTR_BR_MIS_PRED),\n\tARMV8_EVENT_ATTR(cpu_cycles, ARMV8_PMUV3_PERFCTR_CPU_CYCLES),\n\tARMV8_EVENT_ATTR(br_pred, ARMV8_PMUV3_PERFCTR_BR_PRED),\n\tARMV8_EVENT_ATTR(mem_access, ARMV8_PMUV3_PERFCTR_MEM_ACCESS),\n\tARMV8_EVENT_ATTR(l1i_cache, ARMV8_PMUV3_PERFCTR_L1I_CACHE),\n\tARMV8_EVENT_ATTR(l1d_cache_wb, ARMV8_PMUV3_PERFCTR_L1D_CACHE_WB),\n\tARMV8_EVENT_ATTR(l2d_cache, ARMV8_PMUV3_PERFCTR_L2D_CACHE),\n\tARMV8_EVENT_ATTR(l2d_cache_refill, ARMV8_PMUV3_PERFCTR_L2D_CACHE_REFILL),\n\tARMV8_EVENT_ATTR(l2d_cache_wb, ARMV8_PMUV3_PERFCTR_L2D_CACHE_WB),\n\tARMV8_EVENT_ATTR(bus_access, ARMV8_PMUV3_PERFCTR_BUS_ACCESS),\n\tARMV8_EVENT_ATTR(memory_error, ARMV8_PMUV3_PERFCTR_MEMORY_ERROR),\n\tARMV8_EVENT_ATTR(inst_spec, ARMV8_PMUV3_PERFCTR_INST_SPEC),\n\tARMV8_EVENT_ATTR(ttbr_write_retired, ARMV8_PMUV3_PERFCTR_TTBR_WRITE_RETIRED),\n\tARMV8_EVENT_ATTR(bus_cycles, ARMV8_PMUV3_PERFCTR_BUS_CYCLES),\n\t \n\tARMV8_EVENT_ATTR(l1d_cache_allocate, ARMV8_PMUV3_PERFCTR_L1D_CACHE_ALLOCATE),\n\tARMV8_EVENT_ATTR(l2d_cache_allocate, ARMV8_PMUV3_PERFCTR_L2D_CACHE_ALLOCATE),\n\tARMV8_EVENT_ATTR(br_retired, ARMV8_PMUV3_PERFCTR_BR_RETIRED),\n\tARMV8_EVENT_ATTR(br_mis_pred_retired, ARMV8_PMUV3_PERFCTR_BR_MIS_PRED_RETIRED),\n\tARMV8_EVENT_ATTR(stall_frontend, ARMV8_PMUV3_PERFCTR_STALL_FRONTEND),\n\tARMV8_EVENT_ATTR(stall_backend, ARMV8_PMUV3_PERFCTR_STALL_BACKEND),\n\tARMV8_EVENT_ATTR(l1d_tlb, ARMV8_PMUV3_PERFCTR_L1D_TLB),\n\tARMV8_EVENT_ATTR(l1i_tlb, ARMV8_PMUV3_PERFCTR_L1I_TLB),\n\tARMV8_EVENT_ATTR(l2i_cache, ARMV8_PMUV3_PERFCTR_L2I_CACHE),\n\tARMV8_EVENT_ATTR(l2i_cache_refill, ARMV8_PMUV3_PERFCTR_L2I_CACHE_REFILL),\n\tARMV8_EVENT_ATTR(l3d_cache_allocate, ARMV8_PMUV3_PERFCTR_L3D_CACHE_ALLOCATE),\n\tARMV8_EVENT_ATTR(l3d_cache_refill, ARMV8_PMUV3_PERFCTR_L3D_CACHE_REFILL),\n\tARMV8_EVENT_ATTR(l3d_cache, ARMV8_PMUV3_PERFCTR_L3D_CACHE),\n\tARMV8_EVENT_ATTR(l3d_cache_wb, ARMV8_PMUV3_PERFCTR_L3D_CACHE_WB),\n\tARMV8_EVENT_ATTR(l2d_tlb_refill, ARMV8_PMUV3_PERFCTR_L2D_TLB_REFILL),\n\tARMV8_EVENT_ATTR(l2i_tlb_refill, ARMV8_PMUV3_PERFCTR_L2I_TLB_REFILL),\n\tARMV8_EVENT_ATTR(l2d_tlb, ARMV8_PMUV3_PERFCTR_L2D_TLB),\n\tARMV8_EVENT_ATTR(l2i_tlb, ARMV8_PMUV3_PERFCTR_L2I_TLB),\n\tARMV8_EVENT_ATTR(remote_access, ARMV8_PMUV3_PERFCTR_REMOTE_ACCESS),\n\tARMV8_EVENT_ATTR(ll_cache, ARMV8_PMUV3_PERFCTR_LL_CACHE),\n\tARMV8_EVENT_ATTR(ll_cache_miss, ARMV8_PMUV3_PERFCTR_LL_CACHE_MISS),\n\tARMV8_EVENT_ATTR(dtlb_walk, ARMV8_PMUV3_PERFCTR_DTLB_WALK),\n\tARMV8_EVENT_ATTR(itlb_walk, ARMV8_PMUV3_PERFCTR_ITLB_WALK),\n\tARMV8_EVENT_ATTR(ll_cache_rd, ARMV8_PMUV3_PERFCTR_LL_CACHE_RD),\n\tARMV8_EVENT_ATTR(ll_cache_miss_rd, ARMV8_PMUV3_PERFCTR_LL_CACHE_MISS_RD),\n\tARMV8_EVENT_ATTR(remote_access_rd, ARMV8_PMUV3_PERFCTR_REMOTE_ACCESS_RD),\n\tARMV8_EVENT_ATTR(l1d_cache_lmiss_rd, ARMV8_PMUV3_PERFCTR_L1D_CACHE_LMISS_RD),\n\tARMV8_EVENT_ATTR(op_retired, ARMV8_PMUV3_PERFCTR_OP_RETIRED),\n\tARMV8_EVENT_ATTR(op_spec, ARMV8_PMUV3_PERFCTR_OP_SPEC),\n\tARMV8_EVENT_ATTR(stall, ARMV8_PMUV3_PERFCTR_STALL),\n\tARMV8_EVENT_ATTR(stall_slot_backend, ARMV8_PMUV3_PERFCTR_STALL_SLOT_BACKEND),\n\tARMV8_EVENT_ATTR(stall_slot_frontend, ARMV8_PMUV3_PERFCTR_STALL_SLOT_FRONTEND),\n\tARMV8_EVENT_ATTR(stall_slot, ARMV8_PMUV3_PERFCTR_STALL_SLOT),\n\tARMV8_EVENT_ATTR(sample_pop, ARMV8_SPE_PERFCTR_SAMPLE_POP),\n\tARMV8_EVENT_ATTR(sample_feed, ARMV8_SPE_PERFCTR_SAMPLE_FEED),\n\tARMV8_EVENT_ATTR(sample_filtrate, ARMV8_SPE_PERFCTR_SAMPLE_FILTRATE),\n\tARMV8_EVENT_ATTR(sample_collision, ARMV8_SPE_PERFCTR_SAMPLE_COLLISION),\n\tARMV8_EVENT_ATTR(cnt_cycles, ARMV8_AMU_PERFCTR_CNT_CYCLES),\n\tARMV8_EVENT_ATTR(stall_backend_mem, ARMV8_AMU_PERFCTR_STALL_BACKEND_MEM),\n\tARMV8_EVENT_ATTR(l1i_cache_lmiss, ARMV8_PMUV3_PERFCTR_L1I_CACHE_LMISS),\n\tARMV8_EVENT_ATTR(l2d_cache_lmiss_rd, ARMV8_PMUV3_PERFCTR_L2D_CACHE_LMISS_RD),\n\tARMV8_EVENT_ATTR(l2i_cache_lmiss, ARMV8_PMUV3_PERFCTR_L2I_CACHE_LMISS),\n\tARMV8_EVENT_ATTR(l3d_cache_lmiss_rd, ARMV8_PMUV3_PERFCTR_L3D_CACHE_LMISS_RD),\n\tARMV8_EVENT_ATTR(trb_wrap, ARMV8_PMUV3_PERFCTR_TRB_WRAP),\n\tARMV8_EVENT_ATTR(trb_trig, ARMV8_PMUV3_PERFCTR_TRB_TRIG),\n\tARMV8_EVENT_ATTR(trcextout0, ARMV8_PMUV3_PERFCTR_TRCEXTOUT0),\n\tARMV8_EVENT_ATTR(trcextout1, ARMV8_PMUV3_PERFCTR_TRCEXTOUT1),\n\tARMV8_EVENT_ATTR(trcextout2, ARMV8_PMUV3_PERFCTR_TRCEXTOUT2),\n\tARMV8_EVENT_ATTR(trcextout3, ARMV8_PMUV3_PERFCTR_TRCEXTOUT3),\n\tARMV8_EVENT_ATTR(cti_trigout4, ARMV8_PMUV3_PERFCTR_CTI_TRIGOUT4),\n\tARMV8_EVENT_ATTR(cti_trigout5, ARMV8_PMUV3_PERFCTR_CTI_TRIGOUT5),\n\tARMV8_EVENT_ATTR(cti_trigout6, ARMV8_PMUV3_PERFCTR_CTI_TRIGOUT6),\n\tARMV8_EVENT_ATTR(cti_trigout7, ARMV8_PMUV3_PERFCTR_CTI_TRIGOUT7),\n\tARMV8_EVENT_ATTR(ldst_align_lat, ARMV8_PMUV3_PERFCTR_LDST_ALIGN_LAT),\n\tARMV8_EVENT_ATTR(ld_align_lat, ARMV8_PMUV3_PERFCTR_LD_ALIGN_LAT),\n\tARMV8_EVENT_ATTR(st_align_lat, ARMV8_PMUV3_PERFCTR_ST_ALIGN_LAT),\n\tARMV8_EVENT_ATTR(mem_access_checked, ARMV8_MTE_PERFCTR_MEM_ACCESS_CHECKED),\n\tARMV8_EVENT_ATTR(mem_access_checked_rd, ARMV8_MTE_PERFCTR_MEM_ACCESS_CHECKED_RD),\n\tARMV8_EVENT_ATTR(mem_access_checked_wr, ARMV8_MTE_PERFCTR_MEM_ACCESS_CHECKED_WR),\n\tNULL,\n};\n\nstatic umode_t\narmv8pmu_event_attr_is_visible(struct kobject *kobj,\n\t\t\t       struct attribute *attr, int unused)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct arm_pmu *cpu_pmu = container_of(pmu, struct arm_pmu, pmu);\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr.attr);\n\n\tif (pmu_attr->id < ARMV8_PMUV3_MAX_COMMON_EVENTS &&\n\t    test_bit(pmu_attr->id, cpu_pmu->pmceid_bitmap))\n\t\treturn attr->mode;\n\n\tif (pmu_attr->id >= ARMV8_PMUV3_EXT_COMMON_EVENT_BASE) {\n\t\tu64 id = pmu_attr->id - ARMV8_PMUV3_EXT_COMMON_EVENT_BASE;\n\n\t\tif (id < ARMV8_PMUV3_MAX_COMMON_EVENTS &&\n\t\t    test_bit(id, cpu_pmu->pmceid_ext_bitmap))\n\t\t\treturn attr->mode;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct attribute_group armv8_pmuv3_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = armv8_pmuv3_event_attrs,\n\t.is_visible = armv8pmu_event_attr_is_visible,\n};\n\nPMU_FORMAT_ATTR(event, \"config:0-15\");\nPMU_FORMAT_ATTR(long, \"config1:0\");\nPMU_FORMAT_ATTR(rdpmc, \"config1:1\");\n\nstatic int sysctl_perf_user_access __read_mostly;\n\nstatic inline bool armv8pmu_event_is_64bit(struct perf_event *event)\n{\n\treturn event->attr.config1 & 0x1;\n}\n\nstatic inline bool armv8pmu_event_want_user_access(struct perf_event *event)\n{\n\treturn event->attr.config1 & 0x2;\n}\n\nstatic struct attribute *armv8_pmuv3_format_attrs[] = {\n\t&format_attr_event.attr,\n\t&format_attr_long.attr,\n\t&format_attr_rdpmc.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group armv8_pmuv3_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = armv8_pmuv3_format_attrs,\n};\n\nstatic ssize_t slots_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct arm_pmu *cpu_pmu = container_of(pmu, struct arm_pmu, pmu);\n\tu32 slots = cpu_pmu->reg_pmmir & ARMV8_PMU_SLOTS_MASK;\n\n\treturn sysfs_emit(page, \"0x%08x\\n\", slots);\n}\n\nstatic DEVICE_ATTR_RO(slots);\n\nstatic ssize_t bus_slots_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct arm_pmu *cpu_pmu = container_of(pmu, struct arm_pmu, pmu);\n\tu32 bus_slots = (cpu_pmu->reg_pmmir >> ARMV8_PMU_BUS_SLOTS_SHIFT)\n\t\t\t& ARMV8_PMU_BUS_SLOTS_MASK;\n\n\treturn sysfs_emit(page, \"0x%08x\\n\", bus_slots);\n}\n\nstatic DEVICE_ATTR_RO(bus_slots);\n\nstatic ssize_t bus_width_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *page)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct arm_pmu *cpu_pmu = container_of(pmu, struct arm_pmu, pmu);\n\tu32 bus_width = (cpu_pmu->reg_pmmir >> ARMV8_PMU_BUS_WIDTH_SHIFT)\n\t\t\t& ARMV8_PMU_BUS_WIDTH_MASK;\n\tu32 val = 0;\n\n\t \n\tif (bus_width > 2 && bus_width < 13)\n\t\tval = 1 << (bus_width - 1);\n\n\treturn sysfs_emit(page, \"0x%08x\\n\", val);\n}\n\nstatic DEVICE_ATTR_RO(bus_width);\n\nstatic struct attribute *armv8_pmuv3_caps_attrs[] = {\n\t&dev_attr_slots.attr,\n\t&dev_attr_bus_slots.attr,\n\t&dev_attr_bus_width.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group armv8_pmuv3_caps_attr_group = {\n\t.name = \"caps\",\n\t.attrs = armv8_pmuv3_caps_attrs,\n};\n\n \n#define\tARMV8_IDX_CYCLE_COUNTER\t0\n#define\tARMV8_IDX_COUNTER0\t1\n#define\tARMV8_IDX_CYCLE_COUNTER_USER\t32\n\n \nstatic bool armv8pmu_has_long_event(struct arm_pmu *cpu_pmu)\n{\n\treturn (IS_ENABLED(CONFIG_ARM64) && is_pmuv3p5(cpu_pmu->pmuver));\n}\n\nstatic inline bool armv8pmu_event_has_user_read(struct perf_event *event)\n{\n\treturn event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT;\n}\n\n \nstatic inline bool armv8pmu_event_is_chained(struct perf_event *event)\n{\n\tint idx = event->hw.idx;\n\tstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\n\n\treturn !armv8pmu_event_has_user_read(event) &&\n\t       armv8pmu_event_is_64bit(event) &&\n\t       !armv8pmu_has_long_event(cpu_pmu) &&\n\t       (idx != ARMV8_IDX_CYCLE_COUNTER);\n}\n\n \n\n \n#define\tARMV8_IDX_TO_COUNTER(x)\t\\\n\t(((x) - ARMV8_IDX_COUNTER0) & ARMV8_PMU_COUNTER_MASK)\n\nstatic inline u64 armv8pmu_pmcr_read(void)\n{\n\treturn read_pmcr();\n}\n\nstatic inline void armv8pmu_pmcr_write(u64 val)\n{\n\tval &= ARMV8_PMU_PMCR_MASK;\n\tisb();\n\twrite_pmcr(val);\n}\n\nstatic inline int armv8pmu_has_overflowed(u32 pmovsr)\n{\n\treturn pmovsr & ARMV8_PMU_OVERFLOWED_MASK;\n}\n\nstatic inline int armv8pmu_counter_has_overflowed(u32 pmnc, int idx)\n{\n\treturn pmnc & BIT(ARMV8_IDX_TO_COUNTER(idx));\n}\n\nstatic inline u64 armv8pmu_read_evcntr(int idx)\n{\n\tu32 counter = ARMV8_IDX_TO_COUNTER(idx);\n\n\treturn read_pmevcntrn(counter);\n}\n\nstatic inline u64 armv8pmu_read_hw_counter(struct perf_event *event)\n{\n\tint idx = event->hw.idx;\n\tu64 val = armv8pmu_read_evcntr(idx);\n\n\tif (armv8pmu_event_is_chained(event))\n\t\tval = (val << 32) | armv8pmu_read_evcntr(idx - 1);\n\treturn val;\n}\n\n \nstatic bool armv8pmu_event_needs_bias(struct perf_event *event)\n{\n\tstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tif (armv8pmu_event_is_64bit(event))\n\t\treturn false;\n\n\tif (armv8pmu_has_long_event(cpu_pmu) ||\n\t    idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic u64 armv8pmu_bias_long_counter(struct perf_event *event, u64 value)\n{\n\tif (armv8pmu_event_needs_bias(event))\n\t\tvalue |= GENMASK_ULL(63, 32);\n\n\treturn value;\n}\n\nstatic u64 armv8pmu_unbias_long_counter(struct perf_event *event, u64 value)\n{\n\tif (armv8pmu_event_needs_bias(event))\n\t\tvalue &= ~GENMASK_ULL(63, 32);\n\n\treturn value;\n}\n\nstatic u64 armv8pmu_read_counter(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\tu64 value;\n\n\tif (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\tvalue = read_pmccntr();\n\telse\n\t\tvalue = armv8pmu_read_hw_counter(event);\n\n\treturn  armv8pmu_unbias_long_counter(event, value);\n}\n\nstatic inline void armv8pmu_write_evcntr(int idx, u64 value)\n{\n\tu32 counter = ARMV8_IDX_TO_COUNTER(idx);\n\n\twrite_pmevcntrn(counter, value);\n}\n\nstatic inline void armv8pmu_write_hw_counter(struct perf_event *event,\n\t\t\t\t\t     u64 value)\n{\n\tint idx = event->hw.idx;\n\n\tif (armv8pmu_event_is_chained(event)) {\n\t\tarmv8pmu_write_evcntr(idx, upper_32_bits(value));\n\t\tarmv8pmu_write_evcntr(idx - 1, lower_32_bits(value));\n\t} else {\n\t\tarmv8pmu_write_evcntr(idx, value);\n\t}\n}\n\nstatic void armv8pmu_write_counter(struct perf_event *event, u64 value)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tvalue = armv8pmu_bias_long_counter(event, value);\n\n\tif (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\twrite_pmccntr(value);\n\telse\n\t\tarmv8pmu_write_hw_counter(event, value);\n}\n\nstatic inline void armv8pmu_write_evtype(int idx, u32 val)\n{\n\tu32 counter = ARMV8_IDX_TO_COUNTER(idx);\n\n\tval &= ARMV8_PMU_EVTYPE_MASK;\n\twrite_pmevtypern(counter, val);\n}\n\nstatic inline void armv8pmu_write_event_type(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\t \n\tif (armv8pmu_event_is_chained(event)) {\n\t\tu32 chain_evt = ARMV8_PMUV3_PERFCTR_CHAIN |\n\t\t\t\tARMV8_PMU_INCLUDE_EL2;\n\n\t\tarmv8pmu_write_evtype(idx - 1, hwc->config_base);\n\t\tarmv8pmu_write_evtype(idx, chain_evt);\n\t} else {\n\t\tif (idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\t\twrite_pmccfiltr(hwc->config_base);\n\t\telse\n\t\t\tarmv8pmu_write_evtype(idx, hwc->config_base);\n\t}\n}\n\nstatic u32 armv8pmu_event_cnten_mask(struct perf_event *event)\n{\n\tint counter = ARMV8_IDX_TO_COUNTER(event->hw.idx);\n\tu32 mask = BIT(counter);\n\n\tif (armv8pmu_event_is_chained(event))\n\t\tmask |= BIT(counter - 1);\n\treturn mask;\n}\n\nstatic inline void armv8pmu_enable_counter(u32 mask)\n{\n\t \n\tisb();\n\twrite_pmcntenset(mask);\n}\n\nstatic inline void armv8pmu_enable_event_counter(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu32 mask = armv8pmu_event_cnten_mask(event);\n\n\tkvm_set_pmu_events(mask, attr);\n\n\t \n\tif (!kvm_pmu_counter_deferred(attr))\n\t\tarmv8pmu_enable_counter(mask);\n}\n\nstatic inline void armv8pmu_disable_counter(u32 mask)\n{\n\twrite_pmcntenclr(mask);\n\t \n\tisb();\n}\n\nstatic inline void armv8pmu_disable_event_counter(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu32 mask = armv8pmu_event_cnten_mask(event);\n\n\tkvm_clr_pmu_events(mask);\n\n\t \n\tif (!kvm_pmu_counter_deferred(attr))\n\t\tarmv8pmu_disable_counter(mask);\n}\n\nstatic inline void armv8pmu_enable_intens(u32 mask)\n{\n\twrite_pmintenset(mask);\n}\n\nstatic inline void armv8pmu_enable_event_irq(struct perf_event *event)\n{\n\tu32 counter = ARMV8_IDX_TO_COUNTER(event->hw.idx);\n\tarmv8pmu_enable_intens(BIT(counter));\n}\n\nstatic inline void armv8pmu_disable_intens(u32 mask)\n{\n\twrite_pmintenclr(mask);\n\tisb();\n\t \n\twrite_pmovsclr(mask);\n\tisb();\n}\n\nstatic inline void armv8pmu_disable_event_irq(struct perf_event *event)\n{\n\tu32 counter = ARMV8_IDX_TO_COUNTER(event->hw.idx);\n\tarmv8pmu_disable_intens(BIT(counter));\n}\n\nstatic inline u32 armv8pmu_getreset_flags(void)\n{\n\tu32 value;\n\n\t \n\tvalue = read_pmovsclr();\n\n\t \n\tvalue &= ARMV8_PMU_OVSR_MASK;\n\twrite_pmovsclr(value);\n\n\treturn value;\n}\n\nstatic void update_pmuserenr(u64 val)\n{\n\tlockdep_assert_irqs_disabled();\n\n\t \n\tif (kvm_set_pmuserenr(val))\n\t\treturn;\n\n\twrite_pmuserenr(val);\n}\n\nstatic void armv8pmu_disable_user_access(void)\n{\n\tupdate_pmuserenr(0);\n}\n\nstatic void armv8pmu_enable_user_access(struct arm_pmu *cpu_pmu)\n{\n\tint i;\n\tstruct pmu_hw_events *cpuc = this_cpu_ptr(cpu_pmu->hw_events);\n\n\t \n\tfor_each_clear_bit(i, cpuc->used_mask, cpu_pmu->num_events) {\n\t\tif (i == ARMV8_IDX_CYCLE_COUNTER)\n\t\t\twrite_pmccntr(0);\n\t\telse\n\t\t\tarmv8pmu_write_evcntr(i, 0);\n\t}\n\n\tupdate_pmuserenr(ARMV8_PMU_USERENR_ER | ARMV8_PMU_USERENR_CR);\n}\n\nstatic void armv8pmu_enable_event(struct perf_event *event)\n{\n\t \n\tarmv8pmu_disable_event_counter(event);\n\tarmv8pmu_write_event_type(event);\n\tarmv8pmu_enable_event_irq(event);\n\tarmv8pmu_enable_event_counter(event);\n}\n\nstatic void armv8pmu_disable_event(struct perf_event *event)\n{\n\tarmv8pmu_disable_event_counter(event);\n\tarmv8pmu_disable_event_irq(event);\n}\n\nstatic void armv8pmu_start(struct arm_pmu *cpu_pmu)\n{\n\tstruct perf_event_context *ctx;\n\tint nr_user = 0;\n\n\tctx = perf_cpu_task_ctx();\n\tif (ctx)\n\t\tnr_user = ctx->nr_user;\n\n\tif (sysctl_perf_user_access && nr_user)\n\t\tarmv8pmu_enable_user_access(cpu_pmu);\n\telse\n\t\tarmv8pmu_disable_user_access();\n\n\t \n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() | ARMV8_PMU_PMCR_E);\n\n\tkvm_vcpu_pmu_resync_el0();\n}\n\nstatic void armv8pmu_stop(struct arm_pmu *cpu_pmu)\n{\n\t \n\tarmv8pmu_pmcr_write(armv8pmu_pmcr_read() & ~ARMV8_PMU_PMCR_E);\n}\n\nstatic irqreturn_t armv8pmu_handle_irq(struct arm_pmu *cpu_pmu)\n{\n\tu32 pmovsr;\n\tstruct perf_sample_data data;\n\tstruct pmu_hw_events *cpuc = this_cpu_ptr(cpu_pmu->hw_events);\n\tstruct pt_regs *regs;\n\tint idx;\n\n\t \n\tpmovsr = armv8pmu_getreset_flags();\n\n\t \n\tif (!armv8pmu_has_overflowed(pmovsr))\n\t\treturn IRQ_NONE;\n\n\t \n\tregs = get_irq_regs();\n\n\t \n\tarmv8pmu_stop(cpu_pmu);\n\tfor (idx = 0; idx < cpu_pmu->num_events; ++idx) {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\t\tstruct hw_perf_event *hwc;\n\n\t\t \n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!armv8pmu_counter_has_overflowed(pmovsr, idx))\n\t\t\tcontinue;\n\n\t\thwc = &event->hw;\n\t\tarmpmu_event_update(event);\n\t\tperf_sample_data_init(&data, 0, hwc->last_period);\n\t\tif (!armpmu_event_set_period(event))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tcpu_pmu->disable(event);\n\t}\n\tarmv8pmu_start(cpu_pmu);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int armv8pmu_get_single_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t    struct arm_pmu *cpu_pmu)\n{\n\tint idx;\n\n\tfor (idx = ARMV8_IDX_COUNTER0; idx < cpu_pmu->num_events; idx++) {\n\t\tif (!test_and_set_bit(idx, cpuc->used_mask))\n\t\t\treturn idx;\n\t}\n\treturn -EAGAIN;\n}\n\nstatic int armv8pmu_get_chain_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t   struct arm_pmu *cpu_pmu)\n{\n\tint idx;\n\n\t \n\tfor (idx = ARMV8_IDX_COUNTER0 + 1; idx < cpu_pmu->num_events; idx += 2) {\n\t\tif (!test_and_set_bit(idx, cpuc->used_mask)) {\n\t\t\t \n\t\t\tif (!test_and_set_bit(idx - 1, cpuc->used_mask))\n\t\t\t\treturn idx;\n\t\t\t \n\t\t\tclear_bit(idx, cpuc->used_mask);\n\t\t}\n\t}\n\treturn -EAGAIN;\n}\n\nstatic int armv8pmu_get_event_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t  struct perf_event *event)\n{\n\tstruct arm_pmu *cpu_pmu = to_arm_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned long evtype = hwc->config_base & ARMV8_PMU_EVTYPE_EVENT;\n\n\t \n\tif (evtype == ARMV8_PMUV3_PERFCTR_CPU_CYCLES) {\n\t\tif (!test_and_set_bit(ARMV8_IDX_CYCLE_COUNTER, cpuc->used_mask))\n\t\t\treturn ARMV8_IDX_CYCLE_COUNTER;\n\t\telse if (armv8pmu_event_is_64bit(event) &&\n\t\t\t   armv8pmu_event_want_user_access(event) &&\n\t\t\t   !armv8pmu_has_long_event(cpu_pmu))\n\t\t\t\treturn -EAGAIN;\n\t}\n\n\t \n\tif (armv8pmu_event_is_chained(event))\n\t\treturn\tarmv8pmu_get_chain_idx(cpuc, cpu_pmu);\n\telse\n\t\treturn armv8pmu_get_single_idx(cpuc, cpu_pmu);\n}\n\nstatic void armv8pmu_clear_event_idx(struct pmu_hw_events *cpuc,\n\t\t\t\t     struct perf_event *event)\n{\n\tint idx = event->hw.idx;\n\n\tclear_bit(idx, cpuc->used_mask);\n\tif (armv8pmu_event_is_chained(event))\n\t\tclear_bit(idx - 1, cpuc->used_mask);\n}\n\nstatic int armv8pmu_user_event_idx(struct perf_event *event)\n{\n\tif (!sysctl_perf_user_access || !armv8pmu_event_has_user_read(event))\n\t\treturn 0;\n\n\t \n\tif (event->hw.idx == ARMV8_IDX_CYCLE_COUNTER)\n\t\treturn ARMV8_IDX_CYCLE_COUNTER_USER;\n\n\treturn event->hw.idx;\n}\n\n \nstatic int armv8pmu_set_event_filter(struct hw_perf_event *event,\n\t\t\t\t     struct perf_event_attr *attr)\n{\n\tunsigned long config_base = 0;\n\n\tif (attr->exclude_idle)\n\t\treturn -EPERM;\n\n\t \n\tif (is_kernel_in_hyp_mode()) {\n\t\tif (!attr->exclude_kernel && !attr->exclude_host)\n\t\t\tconfig_base |= ARMV8_PMU_INCLUDE_EL2;\n\t\tif (attr->exclude_guest)\n\t\t\tconfig_base |= ARMV8_PMU_EXCLUDE_EL1;\n\t\tif (attr->exclude_host)\n\t\t\tconfig_base |= ARMV8_PMU_EXCLUDE_EL0;\n\t} else {\n\t\tif (!attr->exclude_hv && !attr->exclude_host)\n\t\t\tconfig_base |= ARMV8_PMU_INCLUDE_EL2;\n\t}\n\n\t \n\tif (attr->exclude_kernel)\n\t\tconfig_base |= ARMV8_PMU_EXCLUDE_EL1;\n\n\tif (attr->exclude_user)\n\t\tconfig_base |= ARMV8_PMU_EXCLUDE_EL0;\n\n\t \n\tevent->config_base = config_base;\n\n\treturn 0;\n}\n\nstatic void armv8pmu_reset(void *info)\n{\n\tstruct arm_pmu *cpu_pmu = (struct arm_pmu *)info;\n\tu64 pmcr;\n\n\t \n\tarmv8pmu_disable_counter(U32_MAX);\n\tarmv8pmu_disable_intens(U32_MAX);\n\n\t \n\tkvm_clr_pmu_events(U32_MAX);\n\n\t \n\tpmcr = ARMV8_PMU_PMCR_P | ARMV8_PMU_PMCR_C | ARMV8_PMU_PMCR_LC;\n\n\t \n\tif (armv8pmu_has_long_event(cpu_pmu))\n\t\tpmcr |= ARMV8_PMU_PMCR_LP;\n\n\tarmv8pmu_pmcr_write(pmcr);\n}\n\nstatic int __armv8_pmuv3_map_event_id(struct arm_pmu *armpmu,\n\t\t\t\t      struct perf_event *event)\n{\n\tif (event->attr.type == PERF_TYPE_HARDWARE &&\n\t    event->attr.config == PERF_COUNT_HW_BRANCH_INSTRUCTIONS) {\n\n\t\tif (test_bit(ARMV8_PMUV3_PERFCTR_PC_WRITE_RETIRED,\n\t\t\t     armpmu->pmceid_bitmap))\n\t\t\treturn ARMV8_PMUV3_PERFCTR_PC_WRITE_RETIRED;\n\n\t\tif (test_bit(ARMV8_PMUV3_PERFCTR_BR_RETIRED,\n\t\t\t     armpmu->pmceid_bitmap))\n\t\t\treturn ARMV8_PMUV3_PERFCTR_BR_RETIRED;\n\n\t\treturn HW_OP_UNSUPPORTED;\n\t}\n\n\treturn armpmu_map_event(event, &armv8_pmuv3_perf_map,\n\t\t\t\t&armv8_pmuv3_perf_cache_map,\n\t\t\t\tARMV8_PMU_EVTYPE_EVENT);\n}\n\nstatic int __armv8_pmuv3_map_event(struct perf_event *event,\n\t\t\t\t   const unsigned (*extra_event_map)\n\t\t\t\t\t\t  [PERF_COUNT_HW_MAX],\n\t\t\t\t   const unsigned (*extra_cache_map)\n\t\t\t\t\t\t  [PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t\t  [PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t\t  [PERF_COUNT_HW_CACHE_RESULT_MAX])\n{\n\tint hw_event_id;\n\tstruct arm_pmu *armpmu = to_arm_pmu(event->pmu);\n\n\thw_event_id = __armv8_pmuv3_map_event_id(armpmu, event);\n\n\t \n\tif (hw_event_id == ARMV8_PMUV3_PERFCTR_CHAIN)\n\t\treturn -EINVAL;\n\n\tif (armv8pmu_event_is_64bit(event))\n\t\tevent->hw.flags |= ARMPMU_EVT_64BIT;\n\n\t \n\tif (armv8pmu_event_want_user_access(event)) {\n\t\tif (!(event->attach_state & PERF_ATTACH_TASK))\n\t\t\treturn -EINVAL;\n\t\tif (armv8pmu_event_is_64bit(event) &&\n\t\t    (hw_event_id != ARMV8_PMUV3_PERFCTR_CPU_CYCLES) &&\n\t\t    !armv8pmu_has_long_event(armpmu))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tevent->hw.flags |= PERF_EVENT_FLAG_USER_READ_CNT;\n\t}\n\n\t \n\tif ((hw_event_id > 0) && (hw_event_id < ARMV8_PMUV3_MAX_COMMON_EVENTS)\n\t    && test_bit(hw_event_id, armpmu->pmceid_bitmap)) {\n\t\treturn hw_event_id;\n\t}\n\n\treturn armpmu_map_event(event, extra_event_map, extra_cache_map,\n\t\t\t\tARMV8_PMU_EVTYPE_EVENT);\n}\n\nstatic int armv8_pmuv3_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL, NULL);\n}\n\nstatic int armv8_a53_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL, &armv8_a53_perf_cache_map);\n}\n\nstatic int armv8_a57_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL, &armv8_a57_perf_cache_map);\n}\n\nstatic int armv8_a73_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL, &armv8_a73_perf_cache_map);\n}\n\nstatic int armv8_thunder_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL,\n\t\t\t\t       &armv8_thunder_perf_cache_map);\n}\n\nstatic int armv8_vulcan_map_event(struct perf_event *event)\n{\n\treturn __armv8_pmuv3_map_event(event, NULL,\n\t\t\t\t       &armv8_vulcan_perf_cache_map);\n}\n\nstruct armv8pmu_probe_info {\n\tstruct arm_pmu *pmu;\n\tbool present;\n};\n\nstatic void __armv8pmu_probe_pmu(void *info)\n{\n\tstruct armv8pmu_probe_info *probe = info;\n\tstruct arm_pmu *cpu_pmu = probe->pmu;\n\tu64 pmceid_raw[2];\n\tu32 pmceid[2];\n\tint pmuver;\n\n\tpmuver = read_pmuver();\n\tif (!pmuv3_implemented(pmuver))\n\t\treturn;\n\n\tcpu_pmu->pmuver = pmuver;\n\tprobe->present = true;\n\n\t \n\tcpu_pmu->num_events = (armv8pmu_pmcr_read() >> ARMV8_PMU_PMCR_N_SHIFT)\n\t\t& ARMV8_PMU_PMCR_N_MASK;\n\n\t \n\tcpu_pmu->num_events += 1;\n\n\tpmceid[0] = pmceid_raw[0] = read_pmceid0();\n\tpmceid[1] = pmceid_raw[1] = read_pmceid1();\n\n\tbitmap_from_arr32(cpu_pmu->pmceid_bitmap,\n\t\t\t     pmceid, ARMV8_PMUV3_MAX_COMMON_EVENTS);\n\n\tpmceid[0] = pmceid_raw[0] >> 32;\n\tpmceid[1] = pmceid_raw[1] >> 32;\n\n\tbitmap_from_arr32(cpu_pmu->pmceid_ext_bitmap,\n\t\t\t     pmceid, ARMV8_PMUV3_MAX_COMMON_EVENTS);\n\n\t \n\tif (is_pmuv3p4(pmuver) && (pmceid_raw[1] & BIT(31)))\n\t\tcpu_pmu->reg_pmmir = read_pmmir();\n\telse\n\t\tcpu_pmu->reg_pmmir = 0;\n}\n\nstatic int armv8pmu_probe_pmu(struct arm_pmu *cpu_pmu)\n{\n\tstruct armv8pmu_probe_info probe = {\n\t\t.pmu = cpu_pmu,\n\t\t.present = false,\n\t};\n\tint ret;\n\n\tret = smp_call_function_any(&cpu_pmu->supported_cpus,\n\t\t\t\t    __armv8pmu_probe_pmu,\n\t\t\t\t    &probe, 1);\n\tif (ret)\n\t\treturn ret;\n\n\treturn probe.present ? 0 : -ENODEV;\n}\n\nstatic void armv8pmu_disable_user_access_ipi(void *unused)\n{\n\tarmv8pmu_disable_user_access();\n}\n\nstatic int armv8pmu_proc_user_access_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret || !write || sysctl_perf_user_access)\n\t\treturn ret;\n\n\ton_each_cpu(armv8pmu_disable_user_access_ipi, NULL, 1);\n\treturn 0;\n}\n\nstatic struct ctl_table armv8_pmu_sysctl_table[] = {\n\t{\n\t\t.procname       = \"perf_user_access\",\n\t\t.data\t\t= &sysctl_perf_user_access,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode           = 0644,\n\t\t.proc_handler\t= armv8pmu_proc_user_access_handler,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{ }\n};\n\nstatic void armv8_pmu_register_sysctl_table(void)\n{\n\tstatic u32 tbl_registered = 0;\n\n\tif (!cmpxchg_relaxed(&tbl_registered, 0, 1))\n\t\tregister_sysctl(\"kernel\", armv8_pmu_sysctl_table);\n}\n\nstatic int armv8_pmu_init(struct arm_pmu *cpu_pmu, char *name,\n\t\t\t  int (*map_event)(struct perf_event *event),\n\t\t\t  const struct attribute_group *events,\n\t\t\t  const struct attribute_group *format,\n\t\t\t  const struct attribute_group *caps)\n{\n\tint ret = armv8pmu_probe_pmu(cpu_pmu);\n\tif (ret)\n\t\treturn ret;\n\n\tcpu_pmu->handle_irq\t\t= armv8pmu_handle_irq;\n\tcpu_pmu->enable\t\t\t= armv8pmu_enable_event;\n\tcpu_pmu->disable\t\t= armv8pmu_disable_event;\n\tcpu_pmu->read_counter\t\t= armv8pmu_read_counter;\n\tcpu_pmu->write_counter\t\t= armv8pmu_write_counter;\n\tcpu_pmu->get_event_idx\t\t= armv8pmu_get_event_idx;\n\tcpu_pmu->clear_event_idx\t= armv8pmu_clear_event_idx;\n\tcpu_pmu->start\t\t\t= armv8pmu_start;\n\tcpu_pmu->stop\t\t\t= armv8pmu_stop;\n\tcpu_pmu->reset\t\t\t= armv8pmu_reset;\n\tcpu_pmu->set_event_filter\t= armv8pmu_set_event_filter;\n\n\tcpu_pmu->pmu.event_idx\t\t= armv8pmu_user_event_idx;\n\n\tcpu_pmu->name\t\t\t= name;\n\tcpu_pmu->map_event\t\t= map_event;\n\tcpu_pmu->attr_groups[ARMPMU_ATTR_GROUP_EVENTS] = events ?\n\t\t\tevents : &armv8_pmuv3_events_attr_group;\n\tcpu_pmu->attr_groups[ARMPMU_ATTR_GROUP_FORMATS] = format ?\n\t\t\tformat : &armv8_pmuv3_format_attr_group;\n\tcpu_pmu->attr_groups[ARMPMU_ATTR_GROUP_CAPS] = caps ?\n\t\t\tcaps : &armv8_pmuv3_caps_attr_group;\n\n\tarmv8_pmu_register_sysctl_table();\n\treturn 0;\n}\n\nstatic int armv8_pmu_init_nogroups(struct arm_pmu *cpu_pmu, char *name,\n\t\t\t\t   int (*map_event)(struct perf_event *event))\n{\n\treturn armv8_pmu_init(cpu_pmu, name, map_event, NULL, NULL, NULL);\n}\n\n#define PMUV3_INIT_SIMPLE(name)\t\t\t\t\t\t\\\nstatic int name##_pmu_init(struct arm_pmu *cpu_pmu)\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn armv8_pmu_init_nogroups(cpu_pmu, #name, armv8_pmuv3_map_event);\\\n}\n\nPMUV3_INIT_SIMPLE(armv8_pmuv3)\n\nPMUV3_INIT_SIMPLE(armv8_cortex_a34)\nPMUV3_INIT_SIMPLE(armv8_cortex_a55)\nPMUV3_INIT_SIMPLE(armv8_cortex_a65)\nPMUV3_INIT_SIMPLE(armv8_cortex_a75)\nPMUV3_INIT_SIMPLE(armv8_cortex_a76)\nPMUV3_INIT_SIMPLE(armv8_cortex_a77)\nPMUV3_INIT_SIMPLE(armv8_cortex_a78)\nPMUV3_INIT_SIMPLE(armv9_cortex_a510)\nPMUV3_INIT_SIMPLE(armv9_cortex_a520)\nPMUV3_INIT_SIMPLE(armv9_cortex_a710)\nPMUV3_INIT_SIMPLE(armv9_cortex_a715)\nPMUV3_INIT_SIMPLE(armv9_cortex_a720)\nPMUV3_INIT_SIMPLE(armv8_cortex_x1)\nPMUV3_INIT_SIMPLE(armv9_cortex_x2)\nPMUV3_INIT_SIMPLE(armv9_cortex_x3)\nPMUV3_INIT_SIMPLE(armv9_cortex_x4)\nPMUV3_INIT_SIMPLE(armv8_neoverse_e1)\nPMUV3_INIT_SIMPLE(armv8_neoverse_n1)\nPMUV3_INIT_SIMPLE(armv9_neoverse_n2)\nPMUV3_INIT_SIMPLE(armv8_neoverse_v1)\n\nPMUV3_INIT_SIMPLE(armv8_nvidia_carmel)\nPMUV3_INIT_SIMPLE(armv8_nvidia_denver)\n\nstatic int armv8_a35_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cortex_a35\",\n\t\t\t\t       armv8_a53_map_event);\n}\n\nstatic int armv8_a53_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cortex_a53\",\n\t\t\t\t       armv8_a53_map_event);\n}\n\nstatic int armv8_a57_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cortex_a57\",\n\t\t\t\t       armv8_a57_map_event);\n}\n\nstatic int armv8_a72_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cortex_a72\",\n\t\t\t\t       armv8_a57_map_event);\n}\n\nstatic int armv8_a73_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cortex_a73\",\n\t\t\t\t       armv8_a73_map_event);\n}\n\nstatic int armv8_thunder_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_cavium_thunder\",\n\t\t\t\t       armv8_thunder_map_event);\n}\n\nstatic int armv8_vulcan_pmu_init(struct arm_pmu *cpu_pmu)\n{\n\treturn armv8_pmu_init_nogroups(cpu_pmu, \"armv8_brcm_vulcan\",\n\t\t\t\t       armv8_vulcan_map_event);\n}\n\nstatic const struct of_device_id armv8_pmu_of_device_ids[] = {\n\t{.compatible = \"arm,armv8-pmuv3\",\t.data = armv8_pmuv3_pmu_init},\n\t{.compatible = \"arm,cortex-a34-pmu\",\t.data = armv8_cortex_a34_pmu_init},\n\t{.compatible = \"arm,cortex-a35-pmu\",\t.data = armv8_a35_pmu_init},\n\t{.compatible = \"arm,cortex-a53-pmu\",\t.data = armv8_a53_pmu_init},\n\t{.compatible = \"arm,cortex-a55-pmu\",\t.data = armv8_cortex_a55_pmu_init},\n\t{.compatible = \"arm,cortex-a57-pmu\",\t.data = armv8_a57_pmu_init},\n\t{.compatible = \"arm,cortex-a65-pmu\",\t.data = armv8_cortex_a65_pmu_init},\n\t{.compatible = \"arm,cortex-a72-pmu\",\t.data = armv8_a72_pmu_init},\n\t{.compatible = \"arm,cortex-a73-pmu\",\t.data = armv8_a73_pmu_init},\n\t{.compatible = \"arm,cortex-a75-pmu\",\t.data = armv8_cortex_a75_pmu_init},\n\t{.compatible = \"arm,cortex-a76-pmu\",\t.data = armv8_cortex_a76_pmu_init},\n\t{.compatible = \"arm,cortex-a77-pmu\",\t.data = armv8_cortex_a77_pmu_init},\n\t{.compatible = \"arm,cortex-a78-pmu\",\t.data = armv8_cortex_a78_pmu_init},\n\t{.compatible = \"arm,cortex-a510-pmu\",\t.data = armv9_cortex_a510_pmu_init},\n\t{.compatible = \"arm,cortex-a520-pmu\",\t.data = armv9_cortex_a520_pmu_init},\n\t{.compatible = \"arm,cortex-a710-pmu\",\t.data = armv9_cortex_a710_pmu_init},\n\t{.compatible = \"arm,cortex-a715-pmu\",\t.data = armv9_cortex_a715_pmu_init},\n\t{.compatible = \"arm,cortex-a720-pmu\",\t.data = armv9_cortex_a720_pmu_init},\n\t{.compatible = \"arm,cortex-x1-pmu\",\t.data = armv8_cortex_x1_pmu_init},\n\t{.compatible = \"arm,cortex-x2-pmu\",\t.data = armv9_cortex_x2_pmu_init},\n\t{.compatible = \"arm,cortex-x3-pmu\",\t.data = armv9_cortex_x3_pmu_init},\n\t{.compatible = \"arm,cortex-x4-pmu\",\t.data = armv9_cortex_x4_pmu_init},\n\t{.compatible = \"arm,neoverse-e1-pmu\",\t.data = armv8_neoverse_e1_pmu_init},\n\t{.compatible = \"arm,neoverse-n1-pmu\",\t.data = armv8_neoverse_n1_pmu_init},\n\t{.compatible = \"arm,neoverse-n2-pmu\",\t.data = armv9_neoverse_n2_pmu_init},\n\t{.compatible = \"arm,neoverse-v1-pmu\",\t.data = armv8_neoverse_v1_pmu_init},\n\t{.compatible = \"cavium,thunder-pmu\",\t.data = armv8_thunder_pmu_init},\n\t{.compatible = \"brcm,vulcan-pmu\",\t.data = armv8_vulcan_pmu_init},\n\t{.compatible = \"nvidia,carmel-pmu\",\t.data = armv8_nvidia_carmel_pmu_init},\n\t{.compatible = \"nvidia,denver-pmu\",\t.data = armv8_nvidia_denver_pmu_init},\n\t{},\n};\n\nstatic int armv8_pmu_device_probe(struct platform_device *pdev)\n{\n\treturn arm_pmu_device_probe(pdev, armv8_pmu_of_device_ids, NULL);\n}\n\nstatic struct platform_driver armv8_pmu_driver = {\n\t.driver\t\t= {\n\t\t.name\t= ARMV8_PMU_PDEV_NAME,\n\t\t.of_match_table = armv8_pmu_of_device_ids,\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe\t\t= armv8_pmu_device_probe,\n};\n\nstatic int __init armv8_pmu_driver_init(void)\n{\n\tint ret;\n\n\tif (acpi_disabled)\n\t\tret = platform_driver_register(&armv8_pmu_driver);\n\telse\n\t\tret = arm_pmu_acpi_probe(armv8_pmuv3_pmu_init);\n\n\tif (!ret)\n\t\tlockup_detector_retry_init();\n\n\treturn ret;\n}\ndevice_initcall(armv8_pmu_driver_init)\n\nvoid arch_perf_update_userpage(struct perf_event *event,\n\t\t\t       struct perf_event_mmap_page *userpg, u64 now)\n{\n\tstruct clock_read_data *rd;\n\tunsigned int seq;\n\tu64 ns;\n\n\tuserpg->cap_user_time = 0;\n\tuserpg->cap_user_time_zero = 0;\n\tuserpg->cap_user_time_short = 0;\n\tuserpg->cap_user_rdpmc = armv8pmu_event_has_user_read(event);\n\n\tif (userpg->cap_user_rdpmc) {\n\t\tif (event->hw.flags & ARMPMU_EVT_64BIT)\n\t\t\tuserpg->pmc_width = 64;\n\t\telse\n\t\t\tuserpg->pmc_width = 32;\n\t}\n\n\tdo {\n\t\trd = sched_clock_read_begin(&seq);\n\n\t\tif (rd->read_sched_clock != arch_timer_read_counter)\n\t\t\treturn;\n\n\t\tuserpg->time_mult = rd->mult;\n\t\tuserpg->time_shift = rd->shift;\n\t\tuserpg->time_zero = rd->epoch_ns;\n\t\tuserpg->time_cycles = rd->epoch_cyc;\n\t\tuserpg->time_mask = rd->sched_clock_mask;\n\n\t\t \n\t\tns = mul_u64_u32_shr(rd->epoch_cyc, rd->mult, rd->shift);\n\t\tuserpg->time_zero -= ns;\n\n\t} while (sched_clock_read_retry(seq));\n\n\tuserpg->time_offset = userpg->time_zero - now;\n\n\t \n\tif (userpg->time_shift == 32) {\n\t\tuserpg->time_shift = 31;\n\t\tuserpg->time_mult >>= 1;\n\t}\n\n\t \n\tuserpg->cap_user_time = 1;\n\tuserpg->cap_user_time_zero = 1;\n\tuserpg->cap_user_time_short = 1;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}