{
  "module_name": "hisi_pcie_pmu.c",
  "hash_id": "d5cbff3218be58f6ac8acf48e8c5616e940797a3d0d5f7a65f4dea758adac07f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/hisilicon/hisi_pcie_pmu.c",
  "human_readable_source": "\n \n#include <linux/bitfield.h>\n#include <linux/bitmap.h>\n#include <linux/bug.h>\n#include <linux/device.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/perf_event.h>\n\n#define DRV_NAME \"hisi_pcie_pmu\"\n \n#define HISI_PCIE_GLOBAL_CTRL\t\t0x00\n#define HISI_PCIE_EVENT_CTRL\t\t0x010\n#define HISI_PCIE_CNT\t\t\t0x090\n#define HISI_PCIE_EXT_CNT\t\t0x110\n#define HISI_PCIE_INT_STAT\t\t0x150\n#define HISI_PCIE_INT_MASK\t\t0x154\n#define HISI_PCIE_REG_BDF\t\t0xfe0\n#define HISI_PCIE_REG_VERSION\t\t0xfe4\n#define HISI_PCIE_REG_INFO\t\t0xfe8\n\n \n#define HISI_PCIE_GLOBAL_EN\t\t0x01\n#define HISI_PCIE_GLOBAL_NONE\t\t0\n\n \n#define HISI_PCIE_EVENT_EN\t\tBIT_ULL(20)\n#define HISI_PCIE_RESET_CNT\t\tBIT_ULL(22)\n#define HISI_PCIE_INIT_SET\t\tBIT_ULL(34)\n#define HISI_PCIE_THR_EN\t\tBIT_ULL(26)\n#define HISI_PCIE_TARGET_EN\t\tBIT_ULL(32)\n#define HISI_PCIE_TRIG_EN\t\tBIT_ULL(52)\n\n \n#define HISI_PCIE_EVENT_M\t\tGENMASK_ULL(15, 0)\n#define HISI_PCIE_THR_MODE_M\t\tGENMASK_ULL(27, 27)\n#define HISI_PCIE_THR_M\t\t\tGENMASK_ULL(31, 28)\n#define HISI_PCIE_LEN_M\t\t\tGENMASK_ULL(35, 34)\n#define HISI_PCIE_TARGET_M\t\tGENMASK_ULL(52, 36)\n#define HISI_PCIE_TRIG_MODE_M\t\tGENMASK_ULL(53, 53)\n#define HISI_PCIE_TRIG_M\t\tGENMASK_ULL(59, 56)\n\n \n#define HISI_PCIE_LEN_M_DEFAULT\t\t3ULL\n\n#define HISI_PCIE_MAX_COUNTERS\t\t8\n#define HISI_PCIE_REG_STEP\t\t8\n#define HISI_PCIE_THR_MAX_VAL\t\t10\n#define HISI_PCIE_TRIG_MAX_VAL\t\t10\n#define HISI_PCIE_MAX_PERIOD\t\t(GENMASK_ULL(63, 0))\n#define HISI_PCIE_INIT_VAL\t\tBIT_ULL(63)\n\nstruct hisi_pcie_pmu {\n\tstruct perf_event *hw_events[HISI_PCIE_MAX_COUNTERS];\n\tstruct hlist_node node;\n\tstruct pci_dev *pdev;\n\tstruct pmu pmu;\n\tvoid __iomem *base;\n\tint irq;\n\tu32 identifier;\n\t \n\tu16 bdf_min;\n\tu16 bdf_max;\n\tint on_cpu;\n};\n\nstruct hisi_pcie_reg_pair {\n\tu16 lo;\n\tu16 hi;\n};\n\n#define to_pcie_pmu(p)  (container_of((p), struct hisi_pcie_pmu, pmu))\n#define GET_PCI_DEVFN(bdf)  ((bdf) & 0xff)\n\n#define HISI_PCIE_PMU_FILTER_ATTR(_name, _config, _hi, _lo)\t\t  \\\n\tstatic u64 hisi_pcie_get_##_name(struct perf_event *event)\t  \\\n\t{\t\t\t\t\t\t\t\t  \\\n\t\treturn FIELD_GET(GENMASK(_hi, _lo), event->attr._config); \\\n\t}\t\t\t\t\t\t\t\t  \\\n\nHISI_PCIE_PMU_FILTER_ATTR(event, config, 16, 0);\nHISI_PCIE_PMU_FILTER_ATTR(thr_len, config1, 3, 0);\nHISI_PCIE_PMU_FILTER_ATTR(thr_mode, config1, 4, 4);\nHISI_PCIE_PMU_FILTER_ATTR(trig_len, config1, 8, 5);\nHISI_PCIE_PMU_FILTER_ATTR(trig_mode, config1, 9, 9);\nHISI_PCIE_PMU_FILTER_ATTR(len_mode, config1, 11, 10);\nHISI_PCIE_PMU_FILTER_ATTR(port, config2, 15, 0);\nHISI_PCIE_PMU_FILTER_ATTR(bdf, config2, 31, 16);\n\nstatic ssize_t hisi_pcie_format_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct dev_ext_attribute *eattr;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr);\n\n\treturn sysfs_emit(buf, \"%s\\n\", (char *)eattr->var);\n}\n\nstatic ssize_t hisi_pcie_event_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_attr, attr);\n\n\treturn sysfs_emit(buf, \"config=0x%llx\\n\", pmu_attr->id);\n}\n\n#define HISI_PCIE_PMU_FORMAT_ATTR(_name, _format)                              \\\n\t(&((struct dev_ext_attribute[]){                                       \\\n\t\t{ .attr = __ATTR(_name, 0444, hisi_pcie_format_sysfs_show,     \\\n\t\t\t\t NULL),                                        \\\n\t\t  .var = (void *)_format }                                     \\\n\t})[0].attr.attr)\n\n#define HISI_PCIE_PMU_EVENT_ATTR(_name, _id)\t\t\t\\\n\tPMU_EVENT_ATTR_ID(_name, hisi_pcie_event_sysfs_show, _id)\n\nstatic ssize_t cpumask_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(dev_get_drvdata(dev));\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(pcie_pmu->on_cpu));\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic ssize_t identifier_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"%#x\\n\", pcie_pmu->identifier);\n}\nstatic DEVICE_ATTR_RO(identifier);\n\nstatic ssize_t bus_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"%#04x\\n\", PCI_BUS_NUM(pcie_pmu->bdf_min));\n}\nstatic DEVICE_ATTR_RO(bus);\n\nstatic struct hisi_pcie_reg_pair\nhisi_pcie_parse_reg_value(struct hisi_pcie_pmu *pcie_pmu, u32 reg_off)\n{\n\tu32 val = readl_relaxed(pcie_pmu->base + reg_off);\n\tstruct hisi_pcie_reg_pair regs = {\n\t\t.lo = val,\n\t\t.hi = val >> 16,\n\t};\n\n\treturn regs;\n}\n\n \n#define EXT_COUNTER_IS_USED(idx)\t\t((idx) & BIT(16))\n\nstatic u32 hisi_pcie_get_real_event(struct perf_event *event)\n{\n\treturn hisi_pcie_get_event(event) & GENMASK(15, 0);\n}\n\nstatic u32 hisi_pcie_pmu_get_offset(u32 offset, u32 idx)\n{\n\treturn offset + HISI_PCIE_REG_STEP * idx;\n}\n\nstatic u32 hisi_pcie_pmu_readl(struct hisi_pcie_pmu *pcie_pmu, u32 reg_offset,\n\t\t\t       u32 idx)\n{\n\tu32 offset = hisi_pcie_pmu_get_offset(reg_offset, idx);\n\n\treturn readl_relaxed(pcie_pmu->base + offset);\n}\n\nstatic void hisi_pcie_pmu_writel(struct hisi_pcie_pmu *pcie_pmu, u32 reg_offset, u32 idx, u32 val)\n{\n\tu32 offset = hisi_pcie_pmu_get_offset(reg_offset, idx);\n\n\twritel_relaxed(val, pcie_pmu->base + offset);\n}\n\nstatic u64 hisi_pcie_pmu_readq(struct hisi_pcie_pmu *pcie_pmu, u32 reg_offset, u32 idx)\n{\n\tu32 offset = hisi_pcie_pmu_get_offset(reg_offset, idx);\n\n\treturn readq_relaxed(pcie_pmu->base + offset);\n}\n\nstatic void hisi_pcie_pmu_writeq(struct hisi_pcie_pmu *pcie_pmu, u32 reg_offset, u32 idx, u64 val)\n{\n\tu32 offset = hisi_pcie_pmu_get_offset(reg_offset, idx);\n\n\twriteq_relaxed(val, pcie_pmu->base + offset);\n}\n\nstatic void hisi_pcie_pmu_config_filter(struct perf_event *event)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 port, trig_len, thr_len, len_mode;\n\tu64 reg = HISI_PCIE_INIT_SET;\n\n\t \n\treg |= FIELD_PREP(HISI_PCIE_EVENT_M, hisi_pcie_get_real_event(event));\n\n\t \n\tport = hisi_pcie_get_port(event);\n\tif (port)\n\t\treg |= FIELD_PREP(HISI_PCIE_TARGET_M, port);\n\telse\n\t\treg |= HISI_PCIE_TARGET_EN |\n\t\t       FIELD_PREP(HISI_PCIE_TARGET_M, hisi_pcie_get_bdf(event));\n\n\t \n\ttrig_len = hisi_pcie_get_trig_len(event);\n\tif (trig_len) {\n\t\treg |= FIELD_PREP(HISI_PCIE_TRIG_M, trig_len);\n\t\treg |= FIELD_PREP(HISI_PCIE_TRIG_MODE_M, hisi_pcie_get_trig_mode(event));\n\t\treg |= HISI_PCIE_TRIG_EN;\n\t}\n\n\t \n\tthr_len = hisi_pcie_get_thr_len(event);\n\tif (thr_len) {\n\t\treg |= FIELD_PREP(HISI_PCIE_THR_M, thr_len);\n\t\treg |= FIELD_PREP(HISI_PCIE_THR_MODE_M, hisi_pcie_get_thr_mode(event));\n\t\treg |= HISI_PCIE_THR_EN;\n\t}\n\n\tlen_mode = hisi_pcie_get_len_mode(event);\n\tif (len_mode)\n\t\treg |= FIELD_PREP(HISI_PCIE_LEN_M, len_mode);\n\telse\n\t\treg |= FIELD_PREP(HISI_PCIE_LEN_M, HISI_PCIE_LEN_M_DEFAULT);\n\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, hwc->idx, reg);\n}\n\nstatic void hisi_pcie_pmu_clear_filter(struct perf_event *event)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, hwc->idx, HISI_PCIE_INIT_SET);\n}\n\nstatic bool hisi_pcie_pmu_valid_requester_id(struct hisi_pcie_pmu *pcie_pmu, u32 bdf)\n{\n\tstruct pci_dev *root_port, *pdev;\n\tu16 rp_bdf;\n\n\tpdev = pci_get_domain_bus_and_slot(pci_domain_nr(pcie_pmu->pdev->bus), PCI_BUS_NUM(bdf),\n\t\t\t\t\t   GET_PCI_DEVFN(bdf));\n\tif (!pdev)\n\t\treturn false;\n\n\troot_port = pcie_find_root_port(pdev);\n\tif (!root_port) {\n\t\tpci_dev_put(pdev);\n\t\treturn false;\n\t}\n\n\tpci_dev_put(pdev);\n\trp_bdf = pci_dev_id(root_port);\n\treturn rp_bdf >= pcie_pmu->bdf_min && rp_bdf <= pcie_pmu->bdf_max;\n}\n\nstatic bool hisi_pcie_pmu_valid_filter(struct perf_event *event,\n\t\t\t\t       struct hisi_pcie_pmu *pcie_pmu)\n{\n\tu32 requester_id = hisi_pcie_get_bdf(event);\n\n\tif (hisi_pcie_get_thr_len(event) > HISI_PCIE_THR_MAX_VAL)\n\t\treturn false;\n\n\tif (hisi_pcie_get_trig_len(event) > HISI_PCIE_TRIG_MAX_VAL)\n\t\treturn false;\n\n\tif (requester_id) {\n\t\tif (!hisi_pcie_pmu_valid_requester_id(pcie_pmu, requester_id))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool hisi_pcie_pmu_cmp_event(struct perf_event *target,\n\t\t\t\t\tstruct perf_event *event)\n{\n\treturn hisi_pcie_get_real_event(target) == hisi_pcie_get_real_event(event);\n}\n\nstatic bool hisi_pcie_pmu_validate_event_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct perf_event *event_group[HISI_PCIE_MAX_COUNTERS];\n\tint counters = 1;\n\tint num;\n\n\tevent_group[0] = leader;\n\tif (!is_software_event(leader)) {\n\t\tif (leader->pmu != event->pmu)\n\t\t\treturn false;\n\n\t\tif (leader != event && !hisi_pcie_pmu_cmp_event(leader, event))\n\t\t\tevent_group[counters++] = event;\n\t}\n\n\tfor_each_sibling_event(sibling, event->group_leader) {\n\t\tif (is_software_event(sibling))\n\t\t\tcontinue;\n\n\t\tif (sibling->pmu != event->pmu)\n\t\t\treturn false;\n\n\t\tfor (num = 0; num < counters; num++) {\n\t\t\tif (hisi_pcie_pmu_cmp_event(event_group[num], sibling))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (num == counters)\n\t\t\tevent_group[counters++] = sibling;\n\t}\n\n\treturn counters <= HISI_PCIE_MAX_COUNTERS;\n}\n\nstatic int hisi_pcie_pmu_event_init(struct perf_event *event)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tevent->cpu = pcie_pmu->on_cpu;\n\n\tif (EXT_COUNTER_IS_USED(hisi_pcie_get_event(event)))\n\t\thwc->event_base = HISI_PCIE_EXT_CNT;\n\telse\n\t\thwc->event_base = HISI_PCIE_CNT;\n\n\t \n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!hisi_pcie_pmu_valid_filter(event, pcie_pmu))\n\t\treturn -EINVAL;\n\n\tif (!hisi_pcie_pmu_validate_event_group(event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic u64 hisi_pcie_pmu_read_counter(struct perf_event *event)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tu32 idx = event->hw.idx;\n\n\treturn hisi_pcie_pmu_readq(pcie_pmu, event->hw.event_base, idx);\n}\n\nstatic int hisi_pcie_pmu_find_related_event(struct hisi_pcie_pmu *pcie_pmu,\n\t\t\t\t\t    struct perf_event *event)\n{\n\tstruct perf_event *sibling;\n\tint idx;\n\n\tfor (idx = 0; idx < HISI_PCIE_MAX_COUNTERS; idx++) {\n\t\tsibling = pcie_pmu->hw_events[idx];\n\t\tif (!sibling)\n\t\t\tcontinue;\n\n\t\tif (!hisi_pcie_pmu_cmp_event(sibling, event))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sibling->group_leader == event->group_leader)\n\t\t\treturn idx;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn idx;\n}\n\nstatic int hisi_pcie_pmu_get_event_idx(struct hisi_pcie_pmu *pcie_pmu)\n{\n\tint idx;\n\n\tfor (idx = 0; idx < HISI_PCIE_MAX_COUNTERS; idx++) {\n\t\tif (!pcie_pmu->hw_events[idx])\n\t\t\treturn idx;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void hisi_pcie_pmu_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 new_cnt, prev_cnt, delta;\n\n\tdo {\n\t\tprev_cnt = local64_read(&hwc->prev_count);\n\t\tnew_cnt = hisi_pcie_pmu_read_counter(event);\n\t} while (local64_cmpxchg(&hwc->prev_count, prev_cnt,\n\t\t\t\t new_cnt) != prev_cnt);\n\n\tdelta = (new_cnt - prev_cnt) & HISI_PCIE_MAX_PERIOD;\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void hisi_pcie_pmu_read(struct perf_event *event)\n{\n\thisi_pcie_pmu_event_update(event);\n}\n\nstatic void hisi_pcie_pmu_set_period(struct perf_event *event)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tlocal64_set(&hwc->prev_count, HISI_PCIE_INIT_VAL);\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_CNT, idx, HISI_PCIE_INIT_VAL);\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EXT_CNT, idx, HISI_PCIE_INIT_VAL);\n}\n\nstatic void hisi_pcie_pmu_enable_counter(struct hisi_pcie_pmu *pcie_pmu, struct hw_perf_event *hwc)\n{\n\tu32 idx = hwc->idx;\n\tu64 val;\n\n\tval = hisi_pcie_pmu_readq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx);\n\tval |= HISI_PCIE_EVENT_EN;\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx, val);\n}\n\nstatic void hisi_pcie_pmu_disable_counter(struct hisi_pcie_pmu *pcie_pmu, struct hw_perf_event *hwc)\n{\n\tu32 idx = hwc->idx;\n\tu64 val;\n\n\tval = hisi_pcie_pmu_readq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx);\n\tval &= ~HISI_PCIE_EVENT_EN;\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx, val);\n}\n\nstatic void hisi_pcie_pmu_enable_int(struct hisi_pcie_pmu *pcie_pmu, struct hw_perf_event *hwc)\n{\n\tu32 idx = hwc->idx;\n\n\thisi_pcie_pmu_writel(pcie_pmu, HISI_PCIE_INT_MASK, idx, 0);\n}\n\nstatic void hisi_pcie_pmu_disable_int(struct hisi_pcie_pmu *pcie_pmu, struct hw_perf_event *hwc)\n{\n\tu32 idx = hwc->idx;\n\n\thisi_pcie_pmu_writel(pcie_pmu, HISI_PCIE_INT_MASK, idx, 1);\n}\n\nstatic void hisi_pcie_pmu_reset_counter(struct hisi_pcie_pmu *pcie_pmu, int idx)\n{\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx, HISI_PCIE_RESET_CNT);\n\thisi_pcie_pmu_writeq(pcie_pmu, HISI_PCIE_EVENT_CTRL, idx, HISI_PCIE_INIT_SET);\n}\n\nstatic void hisi_pcie_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\tu64 prev_cnt;\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\thwc->state = 0;\n\n\thisi_pcie_pmu_config_filter(event);\n\thisi_pcie_pmu_enable_counter(pcie_pmu, hwc);\n\thisi_pcie_pmu_enable_int(pcie_pmu, hwc);\n\thisi_pcie_pmu_set_period(event);\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tprev_cnt = local64_read(&hwc->prev_count);\n\t\thisi_pcie_pmu_writeq(pcie_pmu, hwc->event_base, idx, prev_cnt);\n\t}\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void hisi_pcie_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thisi_pcie_pmu_event_update(event);\n\thisi_pcie_pmu_disable_int(pcie_pmu, hwc);\n\thisi_pcie_pmu_disable_counter(pcie_pmu, hwc);\n\thisi_pcie_pmu_clear_filter(event);\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\thwc->state |= PERF_HES_STOPPED;\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\thwc->state |= PERF_HES_UPTODATE;\n}\n\nstatic int hisi_pcie_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\t \n\tidx = hisi_pcie_pmu_find_related_event(pcie_pmu, event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\t \n\tif (idx < HISI_PCIE_MAX_COUNTERS) {\n\t\thwc->idx = idx;\n\t\tgoto start_count;\n\t}\n\n\tidx = hisi_pcie_pmu_get_event_idx(pcie_pmu);\n\tif (idx < 0)\n\t\treturn idx;\n\n\thwc->idx = idx;\n\tpcie_pmu->hw_events[idx] = event;\n\t \n\thisi_pcie_pmu_reset_counter(pcie_pmu, idx);\n\nstart_count:\n\tif (flags & PERF_EF_START)\n\t\thisi_pcie_pmu_start(event, PERF_EF_RELOAD);\n\n\treturn 0;\n}\n\nstatic void hisi_pcie_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thisi_pcie_pmu_stop(event, PERF_EF_UPDATE);\n\tpcie_pmu->hw_events[hwc->idx] = NULL;\n\tperf_event_update_userpage(event);\n}\n\nstatic void hisi_pcie_pmu_enable(struct pmu *pmu)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(pmu);\n\tint num;\n\n\tfor (num = 0; num < HISI_PCIE_MAX_COUNTERS; num++) {\n\t\tif (pcie_pmu->hw_events[num])\n\t\t\tbreak;\n\t}\n\n\tif (num == HISI_PCIE_MAX_COUNTERS)\n\t\treturn;\n\n\twritel(HISI_PCIE_GLOBAL_EN, pcie_pmu->base + HISI_PCIE_GLOBAL_CTRL);\n}\n\nstatic void hisi_pcie_pmu_disable(struct pmu *pmu)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = to_pcie_pmu(pmu);\n\n\twritel(HISI_PCIE_GLOBAL_NONE, pcie_pmu->base + HISI_PCIE_GLOBAL_CTRL);\n}\n\nstatic irqreturn_t hisi_pcie_pmu_irq(int irq, void *data)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = data;\n\tirqreturn_t ret = IRQ_NONE;\n\tstruct perf_event *event;\n\tu32 overflown;\n\tint idx;\n\n\tfor (idx = 0; idx < HISI_PCIE_MAX_COUNTERS; idx++) {\n\t\toverflown = hisi_pcie_pmu_readl(pcie_pmu, HISI_PCIE_INT_STAT, idx);\n\t\tif (!overflown)\n\t\t\tcontinue;\n\n\t\t \n\t\thisi_pcie_pmu_writel(pcie_pmu, HISI_PCIE_INT_STAT, idx, 1);\n\t\tevent = pcie_pmu->hw_events[idx];\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\thisi_pcie_pmu_event_update(event);\n\t\thisi_pcie_pmu_set_period(event);\n\t\tret = IRQ_HANDLED;\n\t}\n\n\treturn ret;\n}\n\nstatic int hisi_pcie_pmu_irq_register(struct pci_dev *pdev, struct hisi_pcie_pmu *pcie_pmu)\n{\n\tint irq, ret;\n\n\tret = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);\n\tif (ret < 0) {\n\t\tpci_err(pdev, \"Failed to enable MSI vectors: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tirq = pci_irq_vector(pdev, 0);\n\tret = request_irq(irq, hisi_pcie_pmu_irq, IRQF_NOBALANCING | IRQF_NO_THREAD, DRV_NAME,\n\t\t\t  pcie_pmu);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to register IRQ: %d\\n\", ret);\n\t\tpci_free_irq_vectors(pdev);\n\t\treturn ret;\n\t}\n\n\tpcie_pmu->irq = irq;\n\n\treturn 0;\n}\n\nstatic void hisi_pcie_pmu_irq_unregister(struct pci_dev *pdev, struct hisi_pcie_pmu *pcie_pmu)\n{\n\tfree_irq(pcie_pmu->irq, pcie_pmu);\n\tpci_free_irq_vectors(pdev);\n}\n\nstatic int hisi_pcie_pmu_online_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = hlist_entry_safe(node, struct hisi_pcie_pmu, node);\n\n\tif (pcie_pmu->on_cpu == -1) {\n\t\tpcie_pmu->on_cpu = cpumask_local_spread(0, dev_to_node(&pcie_pmu->pdev->dev));\n\t\tWARN_ON(irq_set_affinity(pcie_pmu->irq, cpumask_of(pcie_pmu->on_cpu)));\n\t}\n\n\treturn 0;\n}\n\nstatic int hisi_pcie_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = hlist_entry_safe(node, struct hisi_pcie_pmu, node);\n\tunsigned int target;\n\tcpumask_t mask;\n\tint numa_node;\n\n\t \n\tif (pcie_pmu->on_cpu != cpu)\n\t\treturn 0;\n\n\tpcie_pmu->on_cpu = -1;\n\n\t \n\tnuma_node = dev_to_node(&pcie_pmu->pdev->dev);\n\tif (cpumask_and(&mask, cpumask_of_node(numa_node), cpu_online_mask) &&\n\t    cpumask_andnot(&mask, &mask, cpumask_of(cpu)))\n\t\ttarget = cpumask_any(&mask);\n\telse\n\t\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\n\tif (target >= nr_cpu_ids) {\n\t\tpci_err(pcie_pmu->pdev, \"There is no CPU to set\\n\");\n\t\treturn 0;\n\t}\n\n\tperf_pmu_migrate_context(&pcie_pmu->pmu, cpu, target);\n\t \n\tpcie_pmu->on_cpu = target;\n\tWARN_ON(irq_set_affinity(pcie_pmu->irq, cpumask_of(target)));\n\n\treturn 0;\n}\n\nstatic struct attribute *hisi_pcie_pmu_events_attr[] = {\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mwr_latency, 0x0010),\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mwr_cnt, 0x10010),\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mrd_latency, 0x0210),\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mrd_cnt, 0x10210),\n\tHISI_PCIE_PMU_EVENT_ATTR(tx_mrd_latency, 0x0011),\n\tHISI_PCIE_PMU_EVENT_ATTR(tx_mrd_cnt, 0x10011),\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mrd_flux, 0x0804),\n\tHISI_PCIE_PMU_EVENT_ATTR(rx_mrd_time, 0x10804),\n\tHISI_PCIE_PMU_EVENT_ATTR(tx_mrd_flux, 0x0405),\n\tHISI_PCIE_PMU_EVENT_ATTR(tx_mrd_time, 0x10405),\n\tNULL\n};\n\nstatic struct attribute_group hisi_pcie_pmu_events_group = {\n\t.name = \"events\",\n\t.attrs = hisi_pcie_pmu_events_attr,\n};\n\nstatic struct attribute *hisi_pcie_pmu_format_attr[] = {\n\tHISI_PCIE_PMU_FORMAT_ATTR(event, \"config:0-16\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(thr_len, \"config1:0-3\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(thr_mode, \"config1:4\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(trig_len, \"config1:5-8\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(trig_mode, \"config1:9\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(len_mode, \"config1:10-11\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(port, \"config2:0-15\"),\n\tHISI_PCIE_PMU_FORMAT_ATTR(bdf, \"config2:16-31\"),\n\tNULL\n};\n\nstatic const struct attribute_group hisi_pcie_pmu_format_group = {\n\t.name = \"format\",\n\t.attrs = hisi_pcie_pmu_format_attr,\n};\n\nstatic struct attribute *hisi_pcie_pmu_bus_attrs[] = {\n\t&dev_attr_bus.attr,\n\tNULL\n};\n\nstatic const struct attribute_group hisi_pcie_pmu_bus_attr_group = {\n\t.attrs = hisi_pcie_pmu_bus_attrs,\n};\n\nstatic struct attribute *hisi_pcie_pmu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL\n};\n\nstatic const struct attribute_group hisi_pcie_pmu_cpumask_attr_group = {\n\t.attrs = hisi_pcie_pmu_cpumask_attrs,\n};\n\nstatic struct attribute *hisi_pcie_pmu_identifier_attrs[] = {\n\t&dev_attr_identifier.attr,\n\tNULL\n};\n\nstatic const struct attribute_group hisi_pcie_pmu_identifier_attr_group = {\n\t.attrs = hisi_pcie_pmu_identifier_attrs,\n};\n\nstatic const struct attribute_group *hisi_pcie_pmu_attr_groups[] = {\n\t&hisi_pcie_pmu_events_group,\n\t&hisi_pcie_pmu_format_group,\n\t&hisi_pcie_pmu_bus_attr_group,\n\t&hisi_pcie_pmu_cpumask_attr_group,\n\t&hisi_pcie_pmu_identifier_attr_group,\n\tNULL\n};\n\nstatic int hisi_pcie_alloc_pmu(struct pci_dev *pdev, struct hisi_pcie_pmu *pcie_pmu)\n{\n\tstruct hisi_pcie_reg_pair regs;\n\tu16 sicl_id, core_id;\n\tchar *name;\n\n\tregs = hisi_pcie_parse_reg_value(pcie_pmu, HISI_PCIE_REG_BDF);\n\tpcie_pmu->bdf_min = regs.lo;\n\tpcie_pmu->bdf_max = regs.hi;\n\n\tregs = hisi_pcie_parse_reg_value(pcie_pmu, HISI_PCIE_REG_INFO);\n\tsicl_id = regs.hi;\n\tcore_id = regs.lo;\n\n\tname = devm_kasprintf(&pdev->dev, GFP_KERNEL, \"hisi_pcie%u_core%u\", sicl_id, core_id);\n\tif (!name)\n\t\treturn -ENOMEM;\n\n\tpcie_pmu->pdev = pdev;\n\tpcie_pmu->on_cpu = -1;\n\tpcie_pmu->identifier = readl(pcie_pmu->base + HISI_PCIE_REG_VERSION);\n\tpcie_pmu->pmu = (struct pmu) {\n\t\t.name\t\t= name,\n\t\t.module\t\t= THIS_MODULE,\n\t\t.event_init\t= hisi_pcie_pmu_event_init,\n\t\t.pmu_enable\t= hisi_pcie_pmu_enable,\n\t\t.pmu_disable\t= hisi_pcie_pmu_disable,\n\t\t.add\t\t= hisi_pcie_pmu_add,\n\t\t.del\t\t= hisi_pcie_pmu_del,\n\t\t.start\t\t= hisi_pcie_pmu_start,\n\t\t.stop\t\t= hisi_pcie_pmu_stop,\n\t\t.read\t\t= hisi_pcie_pmu_read,\n\t\t.task_ctx_nr\t= perf_invalid_context,\n\t\t.attr_groups\t= hisi_pcie_pmu_attr_groups,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\treturn 0;\n}\n\nstatic int hisi_pcie_init_pmu(struct pci_dev *pdev, struct hisi_pcie_pmu *pcie_pmu)\n{\n\tint ret;\n\n\tpcie_pmu->base = pci_ioremap_bar(pdev, 2);\n\tif (!pcie_pmu->base) {\n\t\tpci_err(pdev, \"Ioremap failed for pcie_pmu resource\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tret = hisi_pcie_alloc_pmu(pdev, pcie_pmu);\n\tif (ret)\n\t\tgoto err_iounmap;\n\n\tret = hisi_pcie_pmu_irq_register(pdev, pcie_pmu);\n\tif (ret)\n\t\tgoto err_iounmap;\n\n\tret = cpuhp_state_add_instance(CPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE, &pcie_pmu->node);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to register hotplug: %d\\n\", ret);\n\t\tgoto err_irq_unregister;\n\t}\n\n\tret = perf_pmu_register(&pcie_pmu->pmu, pcie_pmu->pmu.name, -1);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to register PCIe PMU: %d\\n\", ret);\n\t\tgoto err_hotplug_unregister;\n\t}\n\n\treturn ret;\n\nerr_hotplug_unregister:\n\tcpuhp_state_remove_instance_nocalls(\n\t\tCPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE, &pcie_pmu->node);\n\nerr_irq_unregister:\n\thisi_pcie_pmu_irq_unregister(pdev, pcie_pmu);\n\nerr_iounmap:\n\tiounmap(pcie_pmu->base);\n\n\treturn ret;\n}\n\nstatic void hisi_pcie_uninit_pmu(struct pci_dev *pdev)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu = pci_get_drvdata(pdev);\n\n\tperf_pmu_unregister(&pcie_pmu->pmu);\n\tcpuhp_state_remove_instance_nocalls(\n\t\tCPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE, &pcie_pmu->node);\n\thisi_pcie_pmu_irq_unregister(pdev, pcie_pmu);\n\tiounmap(pcie_pmu->base);\n}\n\nstatic int hisi_pcie_init_dev(struct pci_dev *pdev)\n{\n\tint ret;\n\n\tret = pcim_enable_device(pdev);\n\tif (ret) {\n\t\tpci_err(pdev, \"Failed to enable PCI device: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = pcim_iomap_regions(pdev, BIT(2), DRV_NAME);\n\tif (ret < 0) {\n\t\tpci_err(pdev, \"Failed to request PCI mem regions: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tpci_set_master(pdev);\n\n\treturn 0;\n}\n\nstatic int hisi_pcie_pmu_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct hisi_pcie_pmu *pcie_pmu;\n\tint ret;\n\n\tpcie_pmu = devm_kzalloc(&pdev->dev, sizeof(*pcie_pmu), GFP_KERNEL);\n\tif (!pcie_pmu)\n\t\treturn -ENOMEM;\n\n\tret = hisi_pcie_init_dev(pdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = hisi_pcie_init_pmu(pdev, pcie_pmu);\n\tif (ret)\n\t\treturn ret;\n\n\tpci_set_drvdata(pdev, pcie_pmu);\n\n\treturn ret;\n}\n\nstatic void hisi_pcie_pmu_remove(struct pci_dev *pdev)\n{\n\thisi_pcie_uninit_pmu(pdev);\n\tpci_set_drvdata(pdev, NULL);\n}\n\nstatic const struct pci_device_id hisi_pcie_pmu_ids[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_HUAWEI, 0xa12d) },\n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, hisi_pcie_pmu_ids);\n\nstatic struct pci_driver hisi_pcie_pmu_driver = {\n\t.name = DRV_NAME,\n\t.id_table = hisi_pcie_pmu_ids,\n\t.probe = hisi_pcie_pmu_probe,\n\t.remove = hisi_pcie_pmu_remove,\n};\n\nstatic int __init hisi_pcie_module_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE,\n\t\t\t\t      \"AP_PERF_ARM_HISI_PCIE_PMU_ONLINE\",\n\t\t\t\t      hisi_pcie_pmu_online_cpu,\n\t\t\t\t      hisi_pcie_pmu_offline_cpu);\n\tif (ret) {\n\t\tpr_err(\"Failed to setup PCIe PMU hotplug: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = pci_register_driver(&hisi_pcie_pmu_driver);\n\tif (ret)\n\t\tcpuhp_remove_multi_state(CPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE);\n\n\treturn ret;\n}\nmodule_init(hisi_pcie_module_init);\n\nstatic void __exit hisi_pcie_module_exit(void)\n{\n\tpci_unregister_driver(&hisi_pcie_pmu_driver);\n\tcpuhp_remove_multi_state(CPUHP_AP_PERF_ARM_HISI_PCIE_PMU_ONLINE);\n}\nmodule_exit(hisi_pcie_module_exit);\n\nMODULE_DESCRIPTION(\"HiSilicon PCIe PMU driver\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Qi Liu <liuqi115@huawei.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}