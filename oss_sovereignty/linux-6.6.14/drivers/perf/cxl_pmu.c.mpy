{
  "module_name": "cxl_pmu.c",
  "hash_id": "592250ba1f7b8cca2f1c79c357040cc2dfb1edb0b76183735efea9ef1012b089",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/cxl_pmu.c",
  "human_readable_source": "\n\n \n\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/perf_event.h>\n#include <linux/bitops.h>\n#include <linux/device.h>\n#include <linux/bits.h>\n#include <linux/list.h>\n#include <linux/bug.h>\n#include <linux/pci.h>\n\n#include \"../cxl/cxlpci.h\"\n#include \"../cxl/cxl.h\"\n#include \"../cxl/pmu.h\"\n\n#define CXL_PMU_CAP_REG\t\t\t0x0\n#define   CXL_PMU_CAP_NUM_COUNTERS_MSK\t\t\tGENMASK_ULL(5, 0)\n#define   CXL_PMU_CAP_COUNTER_WIDTH_MSK\t\t\tGENMASK_ULL(15, 8)\n#define   CXL_PMU_CAP_NUM_EVN_CAP_REG_SUP_MSK\t\tGENMASK_ULL(24, 20)\n#define   CXL_PMU_CAP_FILTERS_SUP_MSK\t\t\tGENMASK_ULL(39, 32)\n#define     CXL_PMU_FILTER_HDM\t\t\t\tBIT(0)\n#define     CXL_PMU_FILTER_CHAN_RANK_BANK\t\tBIT(1)\n#define   CXL_PMU_CAP_MSI_N_MSK\t\t\t\tGENMASK_ULL(47, 44)\n#define   CXL_PMU_CAP_WRITEABLE_WHEN_FROZEN\t\tBIT_ULL(48)\n#define   CXL_PMU_CAP_FREEZE\t\t\t\tBIT_ULL(49)\n#define   CXL_PMU_CAP_INT\t\t\t\tBIT_ULL(50)\n#define   CXL_PMU_CAP_VERSION_MSK\t\t\tGENMASK_ULL(63, 60)\n\n#define CXL_PMU_OVERFLOW_REG\t\t0x10\n#define CXL_PMU_FREEZE_REG\t\t0x18\n#define CXL_PMU_EVENT_CAP_REG(n)\t(0x100 + 8 * (n))\n#define   CXL_PMU_EVENT_CAP_SUPPORTED_EVENTS_MSK\tGENMASK_ULL(31, 0)\n#define   CXL_PMU_EVENT_CAP_GROUP_ID_MSK\t\tGENMASK_ULL(47, 32)\n#define   CXL_PMU_EVENT_CAP_VENDOR_ID_MSK\t\tGENMASK_ULL(63, 48)\n\n#define CXL_PMU_COUNTER_CFG_REG(n)\t(0x200 + 8 * (n))\n#define   CXL_PMU_COUNTER_CFG_TYPE_MSK\t\t\tGENMASK_ULL(1, 0)\n#define     CXL_PMU_COUNTER_CFG_TYPE_FREE_RUN\t\t0\n#define     CXL_PMU_COUNTER_CFG_TYPE_FIXED_FUN\t\t1\n#define     CXL_PMU_COUNTER_CFG_TYPE_CONFIGURABLE\t2\n#define   CXL_PMU_COUNTER_CFG_ENABLE\t\t\tBIT_ULL(8)\n#define   CXL_PMU_COUNTER_CFG_INT_ON_OVRFLW\t\tBIT_ULL(9)\n#define   CXL_PMU_COUNTER_CFG_FREEZE_ON_OVRFLW\t\tBIT_ULL(10)\n#define   CXL_PMU_COUNTER_CFG_EDGE\t\t\tBIT_ULL(11)\n#define   CXL_PMU_COUNTER_CFG_INVERT\t\t\tBIT_ULL(12)\n#define   CXL_PMU_COUNTER_CFG_THRESHOLD_MSK\t\tGENMASK_ULL(23, 16)\n#define   CXL_PMU_COUNTER_CFG_EVENTS_MSK\t\tGENMASK_ULL(55, 24)\n#define   CXL_PMU_COUNTER_CFG_EVENT_GRP_ID_IDX_MSK\tGENMASK_ULL(63, 59)\n\n#define CXL_PMU_FILTER_CFG_REG(n, f)\t(0x400 + 4 * ((f) + (n) * 8))\n#define   CXL_PMU_FILTER_CFG_VALUE_MSK\t\t\tGENMASK(15, 0)\n\n#define CXL_PMU_COUNTER_REG(n)\t\t(0xc00 + 8 * (n))\n\n \n#define CXL_PMU_GID_CLOCK_TICKS\t\t0x00\n#define CXL_PMU_GID_D2H_REQ\t\t0x0010\n#define CXL_PMU_GID_D2H_RSP\t\t0x0011\n#define CXL_PMU_GID_H2D_REQ\t\t0x0012\n#define CXL_PMU_GID_H2D_RSP\t\t0x0013\n#define CXL_PMU_GID_CACHE_DATA\t\t0x0014\n#define CXL_PMU_GID_M2S_REQ\t\t0x0020\n#define CXL_PMU_GID_M2S_RWD\t\t0x0021\n#define CXL_PMU_GID_M2S_BIRSP\t\t0x0022\n#define CXL_PMU_GID_S2M_BISNP\t\t0x0023\n#define CXL_PMU_GID_S2M_NDR\t\t0x0024\n#define CXL_PMU_GID_S2M_DRS\t\t0x0025\n#define CXL_PMU_GID_DDR\t\t\t0x8000\n\nstatic int cxl_pmu_cpuhp_state_num;\n\nstruct cxl_pmu_ev_cap {\n\tu16 vid;\n\tu16 gid;\n\tu32 msk;\n\tunion {\n\t\tint counter_idx;  \n\t\tint event_idx;  \n\t};\n\tstruct list_head node;\n};\n\n#define CXL_PMU_MAX_COUNTERS 64\nstruct cxl_pmu_info {\n\tstruct pmu pmu;\n\tvoid __iomem *base;\n\tstruct perf_event **hw_events;\n\tstruct list_head event_caps_configurable;\n\tstruct list_head event_caps_fixed;\n\tDECLARE_BITMAP(used_counter_bm, CXL_PMU_MAX_COUNTERS);\n\tDECLARE_BITMAP(conf_counter_bm, CXL_PMU_MAX_COUNTERS);\n\tu16 counter_width;\n\tu8 num_counters;\n\tu8 num_event_capabilities;\n\tint on_cpu;\n\tstruct hlist_node node;\n\tbool filter_hdm;\n\tint irq;\n};\n\n#define pmu_to_cxl_pmu_info(_pmu) container_of(_pmu, struct cxl_pmu_info, pmu)\n\n \nstatic int cxl_pmu_parse_caps(struct device *dev, struct cxl_pmu_info *info)\n{\n\tunsigned long fixed_counter_event_cap_bm = 0;\n\tvoid __iomem *base = info->base;\n\tbool freeze_for_enable;\n\tu64 val, eval;\n\tint i;\n\n\tval = readq(base + CXL_PMU_CAP_REG);\n\tfreeze_for_enable = FIELD_GET(CXL_PMU_CAP_WRITEABLE_WHEN_FROZEN, val) &&\n\t\tFIELD_GET(CXL_PMU_CAP_FREEZE, val);\n\tif (!freeze_for_enable) {\n\t\tdev_err(dev, \"Counters not writable while frozen\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tinfo->num_counters = FIELD_GET(CXL_PMU_CAP_NUM_COUNTERS_MSK, val) + 1;\n\tinfo->counter_width = FIELD_GET(CXL_PMU_CAP_COUNTER_WIDTH_MSK, val);\n\tinfo->num_event_capabilities = FIELD_GET(CXL_PMU_CAP_NUM_EVN_CAP_REG_SUP_MSK, val) + 1;\n\n\tinfo->filter_hdm = FIELD_GET(CXL_PMU_CAP_FILTERS_SUP_MSK, val) & CXL_PMU_FILTER_HDM;\n\tif (FIELD_GET(CXL_PMU_CAP_INT, val))\n\t\tinfo->irq = FIELD_GET(CXL_PMU_CAP_MSI_N_MSK, val);\n\telse\n\t\tinfo->irq = -1;\n\n\t \n\tfor (i = 0; i < info->num_counters; i++) {\n\t\tstruct cxl_pmu_ev_cap *pmu_ev;\n\t\tu32 events_msk;\n\t\tu8 group_idx;\n\n\t\tval = readq(base + CXL_PMU_COUNTER_CFG_REG(i));\n\n\t\tif (FIELD_GET(CXL_PMU_COUNTER_CFG_TYPE_MSK, val) ==\n\t\t\tCXL_PMU_COUNTER_CFG_TYPE_CONFIGURABLE) {\n\t\t\tset_bit(i, info->conf_counter_bm);\n\t\t}\n\n\t\tif (FIELD_GET(CXL_PMU_COUNTER_CFG_TYPE_MSK, val) !=\n\t\t    CXL_PMU_COUNTER_CFG_TYPE_FIXED_FUN)\n\t\t\tcontinue;\n\n\t\t \n\t\tgroup_idx = FIELD_GET(CXL_PMU_COUNTER_CFG_EVENT_GRP_ID_IDX_MSK, val);\n\t\tevents_msk = FIELD_GET(CXL_PMU_COUNTER_CFG_EVENTS_MSK, val);\n\t\teval = readq(base + CXL_PMU_EVENT_CAP_REG(group_idx));\n\t\tpmu_ev = devm_kzalloc(dev, sizeof(*pmu_ev), GFP_KERNEL);\n\t\tif (!pmu_ev)\n\t\t\treturn -ENOMEM;\n\n\t\tpmu_ev->vid = FIELD_GET(CXL_PMU_EVENT_CAP_VENDOR_ID_MSK, eval);\n\t\tpmu_ev->gid = FIELD_GET(CXL_PMU_EVENT_CAP_GROUP_ID_MSK, eval);\n\t\t \n\t\tpmu_ev->msk = events_msk;\n\t\tpmu_ev->counter_idx = i;\n\t\t \n\t\tlist_add(&pmu_ev->node, &info->event_caps_fixed);\n\t\t \n\t\tset_bit(group_idx, &fixed_counter_event_cap_bm);\n\t}\n\n\tif (!bitmap_empty(info->conf_counter_bm, CXL_PMU_MAX_COUNTERS)) {\n\t\tstruct cxl_pmu_ev_cap *pmu_ev;\n\t\tint j;\n\t\t \n\t\tfor_each_clear_bit(j, &fixed_counter_event_cap_bm,\n\t\t\t\t   info->num_event_capabilities) {\n\t\t\tpmu_ev = devm_kzalloc(dev, sizeof(*pmu_ev), GFP_KERNEL);\n\t\t\tif (!pmu_ev)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\teval = readq(base + CXL_PMU_EVENT_CAP_REG(j));\n\t\t\tpmu_ev->vid = FIELD_GET(CXL_PMU_EVENT_CAP_VENDOR_ID_MSK, eval);\n\t\t\tpmu_ev->gid = FIELD_GET(CXL_PMU_EVENT_CAP_GROUP_ID_MSK, eval);\n\t\t\tpmu_ev->msk = FIELD_GET(CXL_PMU_EVENT_CAP_SUPPORTED_EVENTS_MSK, eval);\n\t\t\tpmu_ev->event_idx = j;\n\t\t\tlist_add(&pmu_ev->node, &info->event_caps_configurable);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t cxl_pmu_format_sysfs_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr);\n\n\treturn sysfs_emit(buf, \"%s\\n\", (char *)eattr->var);\n}\n\n#define CXL_PMU_FORMAT_ATTR(_name, _format)\\\n\t(&((struct dev_ext_attribute[]) {\t\t\t\t\t\\\n\t\t{\t\t\t\t\t\t\t\t\\\n\t\t\t.attr = __ATTR(_name, 0444,\t\t\t\t\\\n\t\t\t\t       cxl_pmu_format_sysfs_show, NULL),\t\\\n\t\t\t.var = (void *)_format\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\t\\\n\t\t})[0].attr.attr)\n\nenum {\n\tcxl_pmu_mask_attr,\n\tcxl_pmu_gid_attr,\n\tcxl_pmu_vid_attr,\n\tcxl_pmu_threshold_attr,\n\tcxl_pmu_invert_attr,\n\tcxl_pmu_edge_attr,\n\tcxl_pmu_hdm_filter_en_attr,\n\tcxl_pmu_hdm_attr,\n};\n\nstatic struct attribute *cxl_pmu_format_attr[] = {\n\t[cxl_pmu_mask_attr] = CXL_PMU_FORMAT_ATTR(mask, \"config:0-31\"),\n\t[cxl_pmu_gid_attr] = CXL_PMU_FORMAT_ATTR(gid, \"config:32-47\"),\n\t[cxl_pmu_vid_attr] = CXL_PMU_FORMAT_ATTR(vid, \"config:48-63\"),\n\t[cxl_pmu_threshold_attr] = CXL_PMU_FORMAT_ATTR(threshold, \"config1:0-15\"),\n\t[cxl_pmu_invert_attr] = CXL_PMU_FORMAT_ATTR(invert, \"config1:16\"),\n\t[cxl_pmu_edge_attr] = CXL_PMU_FORMAT_ATTR(edge, \"config1:17\"),\n\t[cxl_pmu_hdm_filter_en_attr] = CXL_PMU_FORMAT_ATTR(hdm_filter_en, \"config1:18\"),\n\t[cxl_pmu_hdm_attr] = CXL_PMU_FORMAT_ATTR(hdm, \"config2:0-15\"),\n\tNULL\n};\n\n#define CXL_PMU_ATTR_CONFIG_MASK_MSK\t\tGENMASK_ULL(31, 0)\n#define CXL_PMU_ATTR_CONFIG_GID_MSK\t\tGENMASK_ULL(47, 32)\n#define CXL_PMU_ATTR_CONFIG_VID_MSK\t\tGENMASK_ULL(63, 48)\n#define CXL_PMU_ATTR_CONFIG1_THRESHOLD_MSK\tGENMASK_ULL(15, 0)\n#define CXL_PMU_ATTR_CONFIG1_INVERT_MSK\t\tBIT(16)\n#define CXL_PMU_ATTR_CONFIG1_EDGE_MSK\t\tBIT(17)\n#define CXL_PMU_ATTR_CONFIG1_FILTER_EN_MSK\tBIT(18)\n#define CXL_PMU_ATTR_CONFIG2_HDM_MSK\t\tGENMASK(15, 0)\n\nstatic umode_t cxl_pmu_format_is_visible(struct kobject *kobj,\n\t\t\t\t\t struct attribute *attr, int a)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct cxl_pmu_info *info = dev_get_drvdata(dev);\n\n\t \n\tif (!info->filter_hdm &&\n\t    (attr == cxl_pmu_format_attr[cxl_pmu_hdm_filter_en_attr] ||\n\t     attr == cxl_pmu_format_attr[cxl_pmu_hdm_attr]))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group cxl_pmu_format_group = {\n\t.name = \"format\",\n\t.attrs = cxl_pmu_format_attr,\n\t.is_visible = cxl_pmu_format_is_visible,\n};\n\nstatic u32 cxl_pmu_config_get_mask(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG_MASK_MSK, event->attr.config);\n}\n\nstatic u16 cxl_pmu_config_get_gid(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG_GID_MSK, event->attr.config);\n}\n\nstatic u16 cxl_pmu_config_get_vid(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG_VID_MSK, event->attr.config);\n}\n\nstatic u8 cxl_pmu_config1_get_threshold(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG1_THRESHOLD_MSK, event->attr.config1);\n}\n\nstatic bool cxl_pmu_config1_get_invert(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG1_INVERT_MSK, event->attr.config1);\n}\n\nstatic bool cxl_pmu_config1_get_edge(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG1_EDGE_MSK, event->attr.config1);\n}\n\n \n\nstatic bool cxl_pmu_config1_hdm_filter_en(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG1_FILTER_EN_MSK, event->attr.config1);\n}\n\nstatic u16 cxl_pmu_config2_get_hdm_decoder(struct perf_event *event)\n{\n\treturn FIELD_GET(CXL_PMU_ATTR_CONFIG2_HDM_MSK, event->attr.config2);\n}\n\nstatic ssize_t cxl_pmu_event_sysfs_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_attr, attr);\n\n\treturn sysfs_emit(buf, \"config=%#llx\\n\", pmu_attr->id);\n}\n\n#define CXL_PMU_EVENT_ATTR(_name, _vid, _gid, _msk)\t\t\t\\\n\tPMU_EVENT_ATTR_ID(_name, cxl_pmu_event_sysfs_show,\t\t\\\n\t\t\t  ((u64)(_vid) << 48) | ((u64)(_gid) << 32) | (u64)(_msk))\n\n \n#define CXL_PMU_EVENT_CXL_ATTR(_name, _gid, _msk)\t\t\t\\\n\tCXL_PMU_EVENT_ATTR(_name, PCI_DVSEC_VENDOR_ID_CXL, _gid, _msk)\n\nstatic struct attribute *cxl_pmu_event_attrs[] = {\n\tCXL_PMU_EVENT_CXL_ATTR(clock_ticks,\t\t\tCXL_PMU_GID_CLOCK_TICKS, BIT(0)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_rdcurr,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_rdown,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_rdshared,\t\tCXL_PMU_GID_D2H_REQ, BIT(3)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_rdany,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_rdownnodata,\t\tCXL_PMU_GID_D2H_REQ, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_itomwr,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(6)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_wrcurr,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(7)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_clflush,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(8)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_cleanevict,\t\tCXL_PMU_GID_D2H_REQ, BIT(9)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_dirtyevict,\t\tCXL_PMU_GID_D2H_REQ, BIT(10)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_cleanevictnodata,\tCXL_PMU_GID_D2H_REQ, BIT(11)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_wowrinv,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(12)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_wowrinvf,\t\tCXL_PMU_GID_D2H_REQ, BIT(13)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_wrinv,\t\t\tCXL_PMU_GID_D2H_REQ, BIT(14)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_req_cacheflushed,\t\tCXL_PMU_GID_D2H_REQ, BIT(16)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspihiti,\t\tCXL_PMU_GID_D2H_RSP, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspvhitv,\t\tCXL_PMU_GID_D2H_RSP, BIT(6)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspihitse,\t\tCXL_PMU_GID_D2H_RSP, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspshitse,\t\tCXL_PMU_GID_D2H_RSP, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspsfwdm,\t\tCXL_PMU_GID_D2H_RSP, BIT(7)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspifwdm,\t\tCXL_PMU_GID_D2H_RSP, BIT(15)),\n\tCXL_PMU_EVENT_CXL_ATTR(d2h_rsp_rspvfwdv,\t\tCXL_PMU_GID_D2H_RSP, BIT(22)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(h2d_req_snpdata,\t\t\tCXL_PMU_GID_H2D_REQ, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_req_snpinv,\t\t\tCXL_PMU_GID_H2D_REQ, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_req_snpcur,\t\t\tCXL_PMU_GID_H2D_REQ, BIT(3)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_writepull,\t\tCXL_PMU_GID_H2D_RSP, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_go,\t\t\tCXL_PMU_GID_H2D_RSP, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_gowritepull,\t\tCXL_PMU_GID_H2D_RSP, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_extcmp,\t\t\tCXL_PMU_GID_H2D_RSP, BIT(6)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_gowritepulldrop,\t\tCXL_PMU_GID_H2D_RSP, BIT(8)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_fastgowritepull,\t\tCXL_PMU_GID_H2D_RSP, BIT(13)),\n\tCXL_PMU_EVENT_CXL_ATTR(h2d_rsp_goerrwritepull,\t\tCXL_PMU_GID_H2D_RSP, BIT(15)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(cachedata_d2h_data,\t\tCXL_PMU_GID_CACHE_DATA, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(cachedata_h2d_data,\t\tCXL_PMU_GID_CACHE_DATA, BIT(1)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_meminv,\t\t\tCXL_PMU_GID_M2S_REQ, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memrd,\t\t\tCXL_PMU_GID_M2S_REQ, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memrddata,\t\tCXL_PMU_GID_M2S_REQ, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memrdfwd,\t\tCXL_PMU_GID_M2S_REQ, BIT(3)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memwrfwd,\t\tCXL_PMU_GID_M2S_REQ, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memspecrd,\t\tCXL_PMU_GID_M2S_REQ, BIT(8)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_meminvnt,\t\tCXL_PMU_GID_M2S_REQ, BIT(9)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_req_memcleanevict,\t\tCXL_PMU_GID_M2S_REQ, BIT(10)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(m2s_rwd_memwr,\t\t\tCXL_PMU_GID_M2S_RWD, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_rwd_memwrptl,\t\tCXL_PMU_GID_M2S_RWD, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_rwd_biconflict,\t\tCXL_PMU_GID_M2S_RWD, BIT(4)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_i,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_s,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_e,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_iblk,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_sblk,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(m2s_birsp_eblk,\t\t\tCXL_PMU_GID_M2S_BIRSP, BIT(6)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_cur,\t\t\tCXL_PMU_GID_S2M_BISNP, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_data,\t\t\tCXL_PMU_GID_S2M_BISNP, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_inv,\t\t\tCXL_PMU_GID_S2M_BISNP, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_curblk,\t\tCXL_PMU_GID_S2M_BISNP, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_datblk,\t\tCXL_PMU_GID_S2M_BISNP, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_bisnp_invblk,\t\tCXL_PMU_GID_S2M_BISNP, BIT(6)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(s2m_ndr_cmp,\t\t\tCXL_PMU_GID_S2M_NDR, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_ndr_cmps,\t\t\tCXL_PMU_GID_S2M_NDR, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_ndr_cmpe,\t\t\tCXL_PMU_GID_S2M_NDR, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_ndr_biconflictack,\t\tCXL_PMU_GID_S2M_NDR, BIT(3)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(s2m_drs_memdata,\t\t\tCXL_PMU_GID_S2M_DRS, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(s2m_drs_memdatanxm,\t\tCXL_PMU_GID_S2M_DRS, BIT(1)),\n\t \n\tCXL_PMU_EVENT_CXL_ATTR(ddr_act,\t\t\t\tCXL_PMU_GID_DDR, BIT(0)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_pre,\t\t\t\tCXL_PMU_GID_DDR, BIT(1)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_casrd,\t\t\tCXL_PMU_GID_DDR, BIT(2)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_caswr,\t\t\tCXL_PMU_GID_DDR, BIT(3)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_refresh,\t\t\tCXL_PMU_GID_DDR, BIT(4)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_selfrefreshent,\t\tCXL_PMU_GID_DDR, BIT(5)),\n\tCXL_PMU_EVENT_CXL_ATTR(ddr_rfm,\t\t\t\tCXL_PMU_GID_DDR, BIT(6)),\n\tNULL\n};\n\nstatic struct cxl_pmu_ev_cap *cxl_pmu_find_fixed_counter_ev_cap(struct cxl_pmu_info *info,\n\t\t\t\t\t\t\t\tint vid, int gid, int msk)\n{\n\tstruct cxl_pmu_ev_cap *pmu_ev;\n\n\tlist_for_each_entry(pmu_ev, &info->event_caps_fixed, node) {\n\t\tif (vid != pmu_ev->vid || gid != pmu_ev->gid)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (msk == pmu_ev->msk)\n\t\t\treturn pmu_ev;\n\t}\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct cxl_pmu_ev_cap *cxl_pmu_find_config_counter_ev_cap(struct cxl_pmu_info *info,\n\t\t\t\t\t\t\t\t int vid, int gid, int msk)\n{\n\tstruct cxl_pmu_ev_cap *pmu_ev;\n\n\tlist_for_each_entry(pmu_ev, &info->event_caps_configurable, node) {\n\t\tif (vid != pmu_ev->vid || gid != pmu_ev->gid)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (msk & ~pmu_ev->msk)\n\t\t\tcontinue;\n\n\t\treturn pmu_ev;\n\t}\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic umode_t cxl_pmu_event_is_visible(struct kobject *kobj, struct attribute *attr, int a)\n{\n\tstruct device_attribute *dev_attr = container_of(attr, struct device_attribute, attr);\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(dev_attr, struct perf_pmu_events_attr, attr);\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct cxl_pmu_info *info = dev_get_drvdata(dev);\n\tint vid = FIELD_GET(CXL_PMU_ATTR_CONFIG_VID_MSK, pmu_attr->id);\n\tint gid = FIELD_GET(CXL_PMU_ATTR_CONFIG_GID_MSK, pmu_attr->id);\n\tint msk = FIELD_GET(CXL_PMU_ATTR_CONFIG_MASK_MSK, pmu_attr->id);\n\n\tif (!IS_ERR(cxl_pmu_find_fixed_counter_ev_cap(info, vid, gid, msk)))\n\t\treturn attr->mode;\n\n\tif (!IS_ERR(cxl_pmu_find_config_counter_ev_cap(info, vid, gid, msk)))\n\t\treturn attr->mode;\n\n\treturn 0;\n}\n\nstatic const struct attribute_group cxl_pmu_events = {\n\t.name = \"events\",\n\t.attrs = cxl_pmu_event_attrs,\n\t.is_visible = cxl_pmu_event_is_visible,\n};\n\nstatic ssize_t cpumask_show(struct device *dev, struct device_attribute *attr,\n\t\t\t    char *buf)\n{\n\tstruct cxl_pmu_info *info = dev_get_drvdata(dev);\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(info->on_cpu));\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *cxl_pmu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL\n};\n\nstatic const struct attribute_group cxl_pmu_cpumask_group = {\n\t.attrs = cxl_pmu_cpumask_attrs,\n};\n\nstatic const struct attribute_group *cxl_pmu_attr_groups[] = {\n\t&cxl_pmu_events,\n\t&cxl_pmu_format_group,\n\t&cxl_pmu_cpumask_group,\n\tNULL\n};\n\n \nstatic int cxl_pmu_get_event_idx(struct perf_event *event, int *counter_idx,\n\t\t\t\t int *event_idx)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tDECLARE_BITMAP(configurable_and_free, CXL_PMU_MAX_COUNTERS);\n\tstruct cxl_pmu_ev_cap *pmu_ev;\n\tu32 mask;\n\tu16 gid, vid;\n\tint i;\n\n\tvid = cxl_pmu_config_get_vid(event);\n\tgid = cxl_pmu_config_get_gid(event);\n\tmask = cxl_pmu_config_get_mask(event);\n\n\tpmu_ev = cxl_pmu_find_fixed_counter_ev_cap(info, vid, gid, mask);\n\tif (!IS_ERR(pmu_ev)) {\n\t\tif (!counter_idx)\n\t\t\treturn 0;\n\t\tif (!test_bit(pmu_ev->counter_idx, info->used_counter_bm)) {\n\t\t\t*counter_idx = pmu_ev->counter_idx;\n\t\t\treturn 0;\n\t\t}\n\t\t \n\t}\n\n\tpmu_ev = cxl_pmu_find_config_counter_ev_cap(info, vid, gid, mask);\n\tif (!IS_ERR(pmu_ev)) {\n\t\tif (!counter_idx)\n\t\t\treturn 0;\n\n\t\tbitmap_andnot(configurable_and_free, info->conf_counter_bm,\n\t\t\tinfo->used_counter_bm, CXL_PMU_MAX_COUNTERS);\n\n\t\ti = find_first_bit(configurable_and_free, CXL_PMU_MAX_COUNTERS);\n\t\tif (i == CXL_PMU_MAX_COUNTERS)\n\t\t\treturn -EINVAL;\n\n\t\t*counter_idx = i;\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int cxl_pmu_event_init(struct perf_event *event)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tint rc;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EOPNOTSUPP;\n\t \n\n\t \n\trc = cxl_pmu_get_event_idx(event, NULL, NULL);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tevent->cpu = info->on_cpu;\n\n\treturn 0;\n}\n\nstatic void cxl_pmu_enable(struct pmu *pmu)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(pmu);\n\tvoid __iomem *base = info->base;\n\n\t \n\twriteq(0, base + CXL_PMU_FREEZE_REG);\n}\n\nstatic void cxl_pmu_disable(struct pmu *pmu)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(pmu);\n\tvoid __iomem *base = info->base;\n\n\t \n\twriteq(GENMASK_ULL(63, 0), base + CXL_PMU_FREEZE_REG);\n}\n\nstatic void cxl_pmu_event_start(struct perf_event *event, int flags)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tvoid __iomem *base = info->base;\n\tu64 cfg;\n\n\t \n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\thwc->state = 0;\n\n\t \n\tif (info->filter_hdm) {\n\t\tif (cxl_pmu_config1_hdm_filter_en(event))\n\t\t\tcfg = cxl_pmu_config2_get_hdm_decoder(event);\n\t\telse\n\t\t\tcfg = GENMASK(15, 0);  \n\t\twriteq(cfg, base + CXL_PMU_FILTER_CFG_REG(hwc->idx, 0));\n\t}\n\n\tcfg = readq(base + CXL_PMU_COUNTER_CFG_REG(hwc->idx));\n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_INT_ON_OVRFLW, 1);\n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_FREEZE_ON_OVRFLW, 1);\n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_ENABLE, 1);\n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_EDGE,\n\t\t\t  cxl_pmu_config1_get_edge(event) ? 1 : 0);\n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_INVERT,\n\t\t\t  cxl_pmu_config1_get_invert(event) ? 1 : 0);\n\n\t \n\tif (test_bit(hwc->idx, info->conf_counter_bm)) {\n\t\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_EVENT_GRP_ID_IDX_MSK,\n\t\t\t\t  hwc->event_base);\n\t\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_EVENTS_MSK,\n\t\t\t\t  cxl_pmu_config_get_mask(event));\n\t}\n\tcfg &= ~CXL_PMU_COUNTER_CFG_THRESHOLD_MSK;\n\t \n\tcfg |= FIELD_PREP(CXL_PMU_COUNTER_CFG_THRESHOLD_MSK,\n\t\t\t  cxl_pmu_config1_get_threshold(event));\n\twriteq(cfg, base + CXL_PMU_COUNTER_CFG_REG(hwc->idx));\n\n\tlocal64_set(&hwc->prev_count, 0);\n\twriteq(0, base + CXL_PMU_COUNTER_REG(hwc->idx));\n\n\tperf_event_update_userpage(event);\n}\n\nstatic u64 cxl_pmu_read_counter(struct perf_event *event)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tvoid __iomem *base = info->base;\n\n\treturn readq(base + CXL_PMU_COUNTER_REG(event->hw.idx));\n}\n\nstatic void __cxl_pmu_read(struct perf_event *event, bool overflow)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 new_cnt, prev_cnt, delta;\n\n\tdo {\n\t\tprev_cnt = local64_read(&hwc->prev_count);\n\t\tnew_cnt = cxl_pmu_read_counter(event);\n\t} while (local64_cmpxchg(&hwc->prev_count, prev_cnt, new_cnt) != prev_cnt);\n\n\t \n\tdelta = (new_cnt - prev_cnt) & GENMASK_ULL(info->counter_width - 1, 0);\n\tif (overflow && delta < GENMASK_ULL(info->counter_width - 1, 0))\n\t\tdelta += (1UL << info->counter_width);\n\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void cxl_pmu_read(struct perf_event *event)\n{\n\t__cxl_pmu_read(event, false);\n}\n\nstatic void cxl_pmu_event_stop(struct perf_event *event, int flags)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tvoid __iomem *base = info->base;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 cfg;\n\n\tcxl_pmu_read(event);\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\thwc->state |= PERF_HES_STOPPED;\n\n\tcfg = readq(base + CXL_PMU_COUNTER_CFG_REG(hwc->idx));\n\tcfg &= ~(FIELD_PREP(CXL_PMU_COUNTER_CFG_INT_ON_OVRFLW, 1) |\n\t\t FIELD_PREP(CXL_PMU_COUNTER_CFG_ENABLE, 1));\n\twriteq(cfg, base + CXL_PMU_COUNTER_CFG_REG(hwc->idx));\n\n\thwc->state |= PERF_HES_UPTODATE;\n}\n\nstatic int cxl_pmu_event_add(struct perf_event *event, int flags)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx, rc;\n\tint event_idx = 0;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\n\trc = cxl_pmu_get_event_idx(event, &idx, &event_idx);\n\tif (rc < 0)\n\t\treturn rc;\n\n\thwc->idx = idx;\n\n\t \n\thwc->event_base = event_idx;\n\tinfo->hw_events[idx] = event;\n\tset_bit(idx, info->used_counter_bm);\n\n\tif (flags & PERF_EF_START)\n\t\tcxl_pmu_event_start(event, PERF_EF_RELOAD);\n\n\treturn 0;\n}\n\nstatic void cxl_pmu_event_del(struct perf_event *event, int flags)\n{\n\tstruct cxl_pmu_info *info = pmu_to_cxl_pmu_info(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tcxl_pmu_event_stop(event, PERF_EF_UPDATE);\n\tclear_bit(hwc->idx, info->used_counter_bm);\n\tinfo->hw_events[hwc->idx] = NULL;\n\tperf_event_update_userpage(event);\n}\n\nstatic irqreturn_t cxl_pmu_irq(int irq, void *data)\n{\n\tstruct cxl_pmu_info *info = data;\n\tvoid __iomem *base = info->base;\n\tu64 overflowed;\n\tDECLARE_BITMAP(overflowedbm, 64);\n\tint i;\n\n\toverflowed = readq(base + CXL_PMU_OVERFLOW_REG);\n\n\t \n\tif (!overflowed)\n\t\treturn IRQ_NONE;\n\n\tbitmap_from_arr64(overflowedbm, &overflowed, 64);\n\tfor_each_set_bit(i, overflowedbm, info->num_counters) {\n\t\tstruct perf_event *event = info->hw_events[i];\n\n\t\tif (!event) {\n\t\t\tdev_dbg(info->pmu.dev,\n\t\t\t\t\"overflow but on non enabled counter %d\\n\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__cxl_pmu_read(event, true);\n\t}\n\n\twriteq(overflowed, base + CXL_PMU_OVERFLOW_REG);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void cxl_pmu_perf_unregister(void *_info)\n{\n\tstruct cxl_pmu_info *info = _info;\n\n\tperf_pmu_unregister(&info->pmu);\n}\n\nstatic void cxl_pmu_cpuhp_remove(void *_info)\n{\n\tstruct cxl_pmu_info *info = _info;\n\n\tcpuhp_state_remove_instance_nocalls(cxl_pmu_cpuhp_state_num, &info->node);\n}\n\nstatic int cxl_pmu_probe(struct device *dev)\n{\n\tstruct cxl_pmu *pmu = to_cxl_pmu(dev);\n\tstruct pci_dev *pdev = to_pci_dev(dev->parent);\n\tstruct cxl_pmu_info *info;\n\tchar *irq_name;\n\tchar *dev_name;\n\tint rc, irq;\n\n\tinfo = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tdev_set_drvdata(dev, info);\n\tINIT_LIST_HEAD(&info->event_caps_fixed);\n\tINIT_LIST_HEAD(&info->event_caps_configurable);\n\n\tinfo->base = pmu->base;\n\n\tinfo->on_cpu = -1;\n\trc = cxl_pmu_parse_caps(dev, info);\n\tif (rc)\n\t\treturn rc;\n\n\tinfo->hw_events = devm_kcalloc(dev, sizeof(*info->hw_events),\n\t\t\t\t       info->num_counters, GFP_KERNEL);\n\tif (!info->hw_events)\n\t\treturn -ENOMEM;\n\n\tswitch (pmu->type) {\n\tcase CXL_PMU_MEMDEV:\n\t\tdev_name = devm_kasprintf(dev, GFP_KERNEL, \"cxl_pmu_mem%d.%d\",\n\t\t\t\t\t  pmu->assoc_id, pmu->index);\n\t\tbreak;\n\t}\n\tif (!dev_name)\n\t\treturn -ENOMEM;\n\n\tinfo->pmu = (struct pmu) {\n\t\t.name = dev_name,\n\t\t.parent = dev,\n\t\t.module = THIS_MODULE,\n\t\t.event_init = cxl_pmu_event_init,\n\t\t.pmu_enable = cxl_pmu_enable,\n\t\t.pmu_disable = cxl_pmu_disable,\n\t\t.add = cxl_pmu_event_add,\n\t\t.del = cxl_pmu_event_del,\n\t\t.start = cxl_pmu_event_start,\n\t\t.stop = cxl_pmu_event_stop,\n\t\t.read = cxl_pmu_read,\n\t\t.task_ctx_nr = perf_invalid_context,\n\t\t.attr_groups = cxl_pmu_attr_groups,\n\t\t.capabilities = PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\tif (info->irq <= 0)\n\t\treturn -EINVAL;\n\n\trc = pci_irq_vector(pdev, info->irq);\n\tif (rc < 0)\n\t\treturn rc;\n\tirq = rc;\n\n\tirq_name = devm_kasprintf(dev, GFP_KERNEL, \"%s_overflow\\n\", dev_name);\n\tif (!irq_name)\n\t\treturn -ENOMEM;\n\n\trc = devm_request_irq(dev, irq, cxl_pmu_irq, IRQF_SHARED | IRQF_ONESHOT,\n\t\t\t      irq_name, info);\n\tif (rc)\n\t\treturn rc;\n\tinfo->irq = irq;\n\n\trc = cpuhp_state_add_instance(cxl_pmu_cpuhp_state_num, &info->node);\n\tif (rc)\n\t\treturn rc;\n\n\trc = devm_add_action_or_reset(dev, cxl_pmu_cpuhp_remove, info);\n\tif (rc)\n\t\treturn rc;\n\n\trc = perf_pmu_register(&info->pmu, info->pmu.name, -1);\n\tif (rc)\n\t\treturn rc;\n\n\trc = devm_add_action_or_reset(dev, cxl_pmu_perf_unregister, info);\n\tif (rc)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic struct cxl_driver cxl_pmu_driver = {\n\t.name = \"cxl_pmu\",\n\t.probe = cxl_pmu_probe,\n\t.id = CXL_DEVICE_PMU,\n};\n\nstatic int cxl_pmu_online_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct cxl_pmu_info *info = hlist_entry_safe(node, struct cxl_pmu_info, node);\n\n\tif (info->on_cpu != -1)\n\t\treturn 0;\n\n\tinfo->on_cpu = cpu;\n\t \n\tWARN_ON(irq_set_affinity(info->irq, cpumask_of(cpu)));\n\n\treturn 0;\n}\n\nstatic int cxl_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct cxl_pmu_info *info = hlist_entry_safe(node, struct cxl_pmu_info, node);\n\tunsigned int target;\n\n\tif (info->on_cpu != cpu)\n\t\treturn 0;\n\n\tinfo->on_cpu = -1;\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids) {\n\t\tdev_err(info->pmu.dev, \"Unable to find a suitable CPU\\n\");\n\t\treturn 0;\n\t}\n\n\tperf_pmu_migrate_context(&info->pmu, cpu, target);\n\tinfo->on_cpu = target;\n\t \n\tWARN_ON(irq_set_affinity(info->irq, cpumask_of(target)));\n\n\treturn 0;\n}\n\nstatic __init int cxl_pmu_init(void)\n{\n\tint rc;\n\n\trc = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t     \"AP_PERF_CXL_PMU_ONLINE\",\n\t\t\t\t     cxl_pmu_online_cpu, cxl_pmu_offline_cpu);\n\tif (rc < 0)\n\t\treturn rc;\n\tcxl_pmu_cpuhp_state_num = rc;\n\n\trc = cxl_driver_register(&cxl_pmu_driver);\n\tif (rc)\n\t\tcpuhp_remove_multi_state(cxl_pmu_cpuhp_state_num);\n\n\treturn rc;\n}\n\nstatic __exit void cxl_pmu_exit(void)\n{\n\tcxl_driver_unregister(&cxl_pmu_driver);\n\tcpuhp_remove_multi_state(cxl_pmu_cpuhp_state_num);\n}\n\nMODULE_LICENSE(\"GPL\");\nMODULE_IMPORT_NS(CXL);\nmodule_init(cxl_pmu_init);\nmodule_exit(cxl_pmu_exit);\nMODULE_ALIAS_CXL(CXL_DEVICE_PMU);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}