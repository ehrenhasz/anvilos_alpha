{
  "module_name": "marvell_cn10k_ddr_pmu.c",
  "hash_id": "812fe0b3eedaa5ddb9b4525ad02fa666134acf3637302e22e6669b7d2ab9722c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/marvell_cn10k_ddr_pmu.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/perf_event.h>\n#include <linux/hrtimer.h>\n#include <linux/acpi.h>\n#include <linux/platform_device.h>\n\n \n#define DDRC_PERF_CNT_OP_MODE_CTRL\t0x8020\n#define OP_MODE_CTRL_VAL_MANNUAL\t0x1\n\n \n#define DDRC_PERF_CNT_START_OP_CTRL\t0x8028\n#define START_OP_CTRL_VAL_START\t\t0x1ULL\n#define START_OP_CTRL_VAL_ACTIVE\t0x2\n\n \n#define DDRC_PERF_CNT_END_OP_CTRL\t0x8030\n#define END_OP_CTRL_VAL_END\t\t0x1ULL\n\n \n#define DDRC_PERF_CNT_END_STATUS\t\t0x8038\n#define END_STATUS_VAL_END_TIMER_MODE_END\t0x1\n\n \n#define DDRC_PERF_CFG_BASE\t\t0x8040\n\n \n#define DDRC_PERF_NUM_GEN_COUNTERS\t8\n#define DDRC_PERF_NUM_FIX_COUNTERS\t2\n#define DDRC_PERF_READ_COUNTER_IDX\tDDRC_PERF_NUM_GEN_COUNTERS\n#define DDRC_PERF_WRITE_COUNTER_IDX\t(DDRC_PERF_NUM_GEN_COUNTERS + 1)\n#define DDRC_PERF_NUM_COUNTERS\t\t(DDRC_PERF_NUM_GEN_COUNTERS + \\\n\t\t\t\t\t DDRC_PERF_NUM_FIX_COUNTERS)\n\n \n#define DDRC_PERF_CFG(n)\t\t(DDRC_PERF_CFG_BASE + 8 * (n))\n#define EVENT_ENABLE\t\t\tBIT_ULL(63)\n\n \n#define EVENT_DDR_READS\t\t\t101\n#define EVENT_DDR_WRITES\t\t100\n\n \n#define EVENT_OP_IS_ZQLATCH\t\t\t55\n#define EVENT_OP_IS_ZQSTART\t\t\t54\n#define EVENT_OP_IS_TCR_MRR\t\t\t53\n#define EVENT_OP_IS_DQSOSC_MRR\t\t\t52\n#define EVENT_OP_IS_DQSOSC_MPC\t\t\t51\n#define EVENT_VISIBLE_WIN_LIMIT_REACHED_WR\t50\n#define EVENT_VISIBLE_WIN_LIMIT_REACHED_RD\t49\n#define EVENT_BSM_STARVATION\t\t\t48\n#define EVENT_BSM_ALLOC\t\t\t\t47\n#define EVENT_LPR_REQ_WITH_NOCREDIT\t\t46\n#define EVENT_HPR_REQ_WITH_NOCREDIT\t\t45\n#define EVENT_OP_IS_ZQCS\t\t\t44\n#define EVENT_OP_IS_ZQCL\t\t\t43\n#define EVENT_OP_IS_LOAD_MODE\t\t\t42\n#define EVENT_OP_IS_SPEC_REF\t\t\t41\n#define EVENT_OP_IS_CRIT_REF\t\t\t40\n#define EVENT_OP_IS_REFRESH\t\t\t39\n#define EVENT_OP_IS_ENTER_MPSM\t\t\t35\n#define EVENT_OP_IS_ENTER_POWERDOWN\t\t31\n#define EVENT_OP_IS_ENTER_SELFREF\t\t27\n#define EVENT_WAW_HAZARD\t\t\t26\n#define EVENT_RAW_HAZARD\t\t\t25\n#define EVENT_WAR_HAZARD\t\t\t24\n#define EVENT_WRITE_COMBINE\t\t\t23\n#define EVENT_RDWR_TRANSITIONS\t\t\t22\n#define EVENT_PRECHARGE_FOR_OTHER\t\t21\n#define EVENT_PRECHARGE_FOR_RDWR\t\t20\n#define EVENT_OP_IS_PRECHARGE\t\t\t19\n#define EVENT_OP_IS_MWR\t\t\t\t18\n#define EVENT_OP_IS_WR\t\t\t\t17\n#define EVENT_OP_IS_RD\t\t\t\t16\n#define EVENT_OP_IS_RD_ACTIVATE\t\t\t15\n#define EVENT_OP_IS_RD_OR_WR\t\t\t14\n#define EVENT_OP_IS_ACTIVATE\t\t\t13\n#define EVENT_WR_XACT_WHEN_CRITICAL\t\t12\n#define EVENT_LPR_XACT_WHEN_CRITICAL\t\t11\n#define EVENT_HPR_XACT_WHEN_CRITICAL\t\t10\n#define EVENT_DFI_RD_DATA_CYCLES\t\t9\n#define EVENT_DFI_WR_DATA_CYCLES\t\t8\n#define EVENT_ACT_BYPASS\t\t\t7\n#define EVENT_READ_BYPASS\t\t\t6\n#define EVENT_HIF_HI_PRI_RD\t\t\t5\n#define EVENT_HIF_RMW\t\t\t\t4\n#define EVENT_HIF_RD\t\t\t\t3\n#define EVENT_HIF_WR\t\t\t\t2\n#define EVENT_HIF_RD_OR_WR\t\t\t1\n\n \n#define DDRC_PERF_CNT_VALUE_BASE\t\t0x8080\n#define DDRC_PERF_CNT_VALUE(n)\t(DDRC_PERF_CNT_VALUE_BASE + 8 * (n))\n\n \n#define DDRC_PERF_CNT_FREERUN_EN\t0x80C0\n#define DDRC_PERF_FREERUN_WRITE_EN\t0x1\n#define DDRC_PERF_FREERUN_READ_EN\t0x2\n\n \n#define DDRC_PERF_CNT_FREERUN_CTRL\t0x80C8\n#define DDRC_FREERUN_WRITE_CNT_CLR\t0x1\n#define DDRC_FREERUN_READ_CNT_CLR\t0x2\n\n \n#define DDRC_PERF_CNT_VALUE_WR_OP\t0x80D0\n#define DDRC_PERF_CNT_VALUE_RD_OP\t0x80D8\n#define DDRC_PERF_CNT_VALUE_OVERFLOW\tBIT_ULL(48)\n#define DDRC_PERF_CNT_MAX_VALUE\t\tGENMASK_ULL(48, 0)\n\nstruct cn10k_ddr_pmu {\n\tstruct pmu pmu;\n\tvoid __iomem *base;\n\tunsigned int cpu;\n\tstruct\tdevice *dev;\n\tint active_events;\n\tstruct perf_event *events[DDRC_PERF_NUM_COUNTERS];\n\tstruct hrtimer hrtimer;\n\tstruct hlist_node node;\n};\n\n#define to_cn10k_ddr_pmu(p)\tcontainer_of(p, struct cn10k_ddr_pmu, pmu)\n\nstatic ssize_t cn10k_ddr_pmu_event_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\treturn sysfs_emit(page, \"event=0x%02llx\\n\", pmu_attr->id);\n\n}\n\n#define CN10K_DDR_PMU_EVENT_ATTR(_name, _id)\t\t\t\t     \\\n\tPMU_EVENT_ATTR_ID(_name, cn10k_ddr_pmu_event_show, _id)\n\nstatic struct attribute *cn10k_ddr_perf_events_attrs[] = {\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hif_rd_or_wr_access, EVENT_HIF_RD_OR_WR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hif_wr_access, EVENT_HIF_WR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hif_rd_access, EVENT_HIF_RD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hif_rmw_access, EVENT_HIF_RMW),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hif_pri_rdaccess, EVENT_HIF_HI_PRI_RD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_rd_bypass_access, EVENT_READ_BYPASS),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_act_bypass_access, EVENT_ACT_BYPASS),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_dif_wr_data_access, EVENT_DFI_WR_DATA_CYCLES),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_dif_rd_data_access, EVENT_DFI_RD_DATA_CYCLES),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hpri_sched_rd_crit_access,\n\t\t\t\t\tEVENT_HPR_XACT_WHEN_CRITICAL),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_lpri_sched_rd_crit_access,\n\t\t\t\t\tEVENT_LPR_XACT_WHEN_CRITICAL),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_wr_trxn_crit_access,\n\t\t\t\t\tEVENT_WR_XACT_WHEN_CRITICAL),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_active_access, EVENT_OP_IS_ACTIVATE),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_rd_or_wr_access, EVENT_OP_IS_RD_OR_WR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_rd_active_access, EVENT_OP_IS_RD_ACTIVATE),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_read, EVENT_OP_IS_RD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_write, EVENT_OP_IS_WR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_mwr, EVENT_OP_IS_MWR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_precharge, EVENT_OP_IS_PRECHARGE),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_precharge_for_rdwr, EVENT_PRECHARGE_FOR_RDWR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_precharge_for_other,\n\t\t\t\t\tEVENT_PRECHARGE_FOR_OTHER),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_rdwr_transitions, EVENT_RDWR_TRANSITIONS),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_write_combine, EVENT_WRITE_COMBINE),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_war_hazard, EVENT_WAR_HAZARD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_raw_hazard, EVENT_RAW_HAZARD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_waw_hazard, EVENT_WAW_HAZARD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_enter_selfref, EVENT_OP_IS_ENTER_SELFREF),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_enter_powerdown, EVENT_OP_IS_ENTER_POWERDOWN),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_enter_mpsm, EVENT_OP_IS_ENTER_MPSM),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_refresh, EVENT_OP_IS_REFRESH),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_crit_ref, EVENT_OP_IS_CRIT_REF),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_spec_ref, EVENT_OP_IS_SPEC_REF),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_load_mode, EVENT_OP_IS_LOAD_MODE),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_zqcl, EVENT_OP_IS_ZQCL),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_cam_wr_access, EVENT_OP_IS_ZQCS),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_hpr_req_with_nocredit,\n\t\t\t\t\tEVENT_HPR_REQ_WITH_NOCREDIT),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_lpr_req_with_nocredit,\n\t\t\t\t\tEVENT_LPR_REQ_WITH_NOCREDIT),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_bsm_alloc, EVENT_BSM_ALLOC),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_bsm_starvation, EVENT_BSM_STARVATION),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_win_limit_reached_rd,\n\t\t\t\t\tEVENT_VISIBLE_WIN_LIMIT_REACHED_RD),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_win_limit_reached_wr,\n\t\t\t\t\tEVENT_VISIBLE_WIN_LIMIT_REACHED_WR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_dqsosc_mpc, EVENT_OP_IS_DQSOSC_MPC),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_dqsosc_mrr, EVENT_OP_IS_DQSOSC_MRR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_tcr_mrr, EVENT_OP_IS_TCR_MRR),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_zqstart, EVENT_OP_IS_ZQSTART),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_zqlatch, EVENT_OP_IS_ZQLATCH),\n\t \n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_ddr_reads, EVENT_DDR_READS),\n\tCN10K_DDR_PMU_EVENT_ATTR(ddr_ddr_writes, EVENT_DDR_WRITES),\n\tNULL\n};\n\nstatic struct attribute_group cn10k_ddr_perf_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = cn10k_ddr_perf_events_attrs,\n};\n\nPMU_FORMAT_ATTR(event, \"config:0-8\");\n\nstatic struct attribute *cn10k_ddr_perf_format_attrs[] = {\n\t&format_attr_event.attr,\n\tNULL,\n};\n\nstatic struct attribute_group cn10k_ddr_perf_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = cn10k_ddr_perf_format_attrs,\n};\n\nstatic ssize_t cn10k_ddr_perf_cpumask_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct cn10k_ddr_pmu *pmu = dev_get_drvdata(dev);\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(pmu->cpu));\n}\n\nstatic struct device_attribute cn10k_ddr_perf_cpumask_attr =\n\t__ATTR(cpumask, 0444, cn10k_ddr_perf_cpumask_show, NULL);\n\nstatic struct attribute *cn10k_ddr_perf_cpumask_attrs[] = {\n\t&cn10k_ddr_perf_cpumask_attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group cn10k_ddr_perf_cpumask_attr_group = {\n\t.attrs = cn10k_ddr_perf_cpumask_attrs,\n};\n\nstatic const struct attribute_group *cn10k_attr_groups[] = {\n\t&cn10k_ddr_perf_events_attr_group,\n\t&cn10k_ddr_perf_format_attr_group,\n\t&cn10k_ddr_perf_cpumask_attr_group,\n\tNULL,\n};\n\n \nstatic unsigned long cn10k_ddr_pmu_poll_period_sec = 100;\nmodule_param_named(poll_period_sec, cn10k_ddr_pmu_poll_period_sec, ulong, 0644);\n\nstatic ktime_t cn10k_ddr_pmu_timer_period(void)\n{\n\treturn ms_to_ktime((u64)cn10k_ddr_pmu_poll_period_sec * USEC_PER_SEC);\n}\n\nstatic int ddr_perf_get_event_bitmap(int eventid, u64 *event_bitmap)\n{\n\tswitch (eventid) {\n\tcase EVENT_HIF_RD_OR_WR ... EVENT_WAW_HAZARD:\n\tcase EVENT_OP_IS_REFRESH ... EVENT_OP_IS_ZQLATCH:\n\t\t*event_bitmap = (1ULL << (eventid - 1));\n\t\tbreak;\n\tcase EVENT_OP_IS_ENTER_SELFREF:\n\tcase EVENT_OP_IS_ENTER_POWERDOWN:\n\tcase EVENT_OP_IS_ENTER_MPSM:\n\t\t*event_bitmap = (0xFULL << (eventid - 1));\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s Invalid eventid %d\\n\", __func__, eventid);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int cn10k_ddr_perf_alloc_counter(struct cn10k_ddr_pmu *pmu,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tu8 config = event->attr.config;\n\tint i;\n\n\t \n\tif (config == EVENT_DDR_READS) {\n\t\tpmu->events[DDRC_PERF_READ_COUNTER_IDX] = event;\n\t\treturn DDRC_PERF_READ_COUNTER_IDX;\n\t}\n\n\t \n\tif (config == EVENT_DDR_WRITES) {\n\t\tpmu->events[DDRC_PERF_WRITE_COUNTER_IDX] = event;\n\t\treturn DDRC_PERF_WRITE_COUNTER_IDX;\n\t}\n\n\t \n\tfor (i = 0; i < DDRC_PERF_NUM_GEN_COUNTERS; i++) {\n\t\tif (pmu->events[i] == NULL) {\n\t\t\tpmu->events[i] = event;\n\t\t\treturn i;\n\t\t}\n\t}\n\n\treturn -ENOENT;\n}\n\nstatic void cn10k_ddr_perf_free_counter(struct cn10k_ddr_pmu *pmu, int counter)\n{\n\tpmu->events[counter] = NULL;\n}\n\nstatic int cn10k_ddr_perf_event_init(struct perf_event *event)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tif (is_sampling_event(event)) {\n\t\tdev_info(pmu->dev, \"Sampling not supported!\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (event->cpu < 0) {\n\t\tdev_warn(pmu->dev, \"Can't provide per-task data!\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (event->group_leader->pmu != event->pmu &&\n\t    !is_software_event(event->group_leader))\n\t\treturn -EINVAL;\n\n\t \n\tevent->cpu = pmu->cpu;\n\thwc->idx = -1;\n\treturn 0;\n}\n\nstatic void cn10k_ddr_perf_counter_enable(struct cn10k_ddr_pmu *pmu,\n\t\t\t\t\t  int counter, bool enable)\n{\n\tu32 reg;\n\tu64 val;\n\n\tif (counter > DDRC_PERF_NUM_COUNTERS) {\n\t\tpr_err(\"Error: unsupported counter %d\\n\", counter);\n\t\treturn;\n\t}\n\n\tif (counter < DDRC_PERF_NUM_GEN_COUNTERS) {\n\t\treg = DDRC_PERF_CFG(counter);\n\t\tval = readq_relaxed(pmu->base + reg);\n\n\t\tif (enable)\n\t\t\tval |= EVENT_ENABLE;\n\t\telse\n\t\t\tval &= ~EVENT_ENABLE;\n\n\t\twriteq_relaxed(val, pmu->base + reg);\n\t} else {\n\t\tval = readq_relaxed(pmu->base + DDRC_PERF_CNT_FREERUN_EN);\n\t\tif (enable) {\n\t\t\tif (counter == DDRC_PERF_READ_COUNTER_IDX)\n\t\t\t\tval |= DDRC_PERF_FREERUN_READ_EN;\n\t\t\telse\n\t\t\t\tval |= DDRC_PERF_FREERUN_WRITE_EN;\n\t\t} else {\n\t\t\tif (counter == DDRC_PERF_READ_COUNTER_IDX)\n\t\t\t\tval &= ~DDRC_PERF_FREERUN_READ_EN;\n\t\t\telse\n\t\t\t\tval &= ~DDRC_PERF_FREERUN_WRITE_EN;\n\t\t}\n\t\twriteq_relaxed(val, pmu->base + DDRC_PERF_CNT_FREERUN_EN);\n\t}\n}\n\nstatic u64 cn10k_ddr_perf_read_counter(struct cn10k_ddr_pmu *pmu, int counter)\n{\n\tu64 val;\n\n\tif (counter == DDRC_PERF_READ_COUNTER_IDX)\n\t\treturn readq_relaxed(pmu->base + DDRC_PERF_CNT_VALUE_RD_OP);\n\n\tif (counter == DDRC_PERF_WRITE_COUNTER_IDX)\n\t\treturn readq_relaxed(pmu->base + DDRC_PERF_CNT_VALUE_WR_OP);\n\n\tval = readq_relaxed(pmu->base + DDRC_PERF_CNT_VALUE(counter));\n\treturn val;\n}\n\nstatic void cn10k_ddr_perf_event_update(struct perf_event *event)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 prev_count, new_count, mask;\n\n\tdo {\n\t\tprev_count = local64_read(&hwc->prev_count);\n\t\tnew_count = cn10k_ddr_perf_read_counter(pmu, hwc->idx);\n\t} while (local64_xchg(&hwc->prev_count, new_count) != prev_count);\n\n\tmask = DDRC_PERF_CNT_MAX_VALUE;\n\n\tlocal64_add((new_count - prev_count) & mask, &event->count);\n}\n\nstatic void cn10k_ddr_perf_event_start(struct perf_event *event, int flags)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tlocal64_set(&hwc->prev_count, 0);\n\n\tcn10k_ddr_perf_counter_enable(pmu, counter, true);\n\n\thwc->state = 0;\n}\n\nstatic int cn10k_ddr_perf_event_add(struct perf_event *event, int flags)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu8 config = event->attr.config;\n\tint counter, ret;\n\tu32 reg_offset;\n\tu64 val;\n\n\tcounter = cn10k_ddr_perf_alloc_counter(pmu, event);\n\tif (counter < 0)\n\t\treturn -EAGAIN;\n\n\tpmu->active_events++;\n\thwc->idx = counter;\n\n\tif (pmu->active_events == 1)\n\t\thrtimer_start(&pmu->hrtimer, cn10k_ddr_pmu_timer_period(),\n\t\t\t      HRTIMER_MODE_REL_PINNED);\n\n\tif (counter < DDRC_PERF_NUM_GEN_COUNTERS) {\n\t\t \n\t\treg_offset = DDRC_PERF_CFG(counter);\n\t\tret = ddr_perf_get_event_bitmap(config, &val);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\twriteq_relaxed(val, pmu->base + reg_offset);\n\t} else {\n\t\t \n\t\tif (counter == DDRC_PERF_READ_COUNTER_IDX)\n\t\t\tval = DDRC_FREERUN_READ_CNT_CLR;\n\t\telse\n\t\t\tval = DDRC_FREERUN_WRITE_CNT_CLR;\n\n\t\twriteq_relaxed(val, pmu->base + DDRC_PERF_CNT_FREERUN_CTRL);\n\t}\n\n\thwc->state |= PERF_HES_STOPPED;\n\n\tif (flags & PERF_EF_START)\n\t\tcn10k_ddr_perf_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void cn10k_ddr_perf_event_stop(struct perf_event *event, int flags)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tcn10k_ddr_perf_counter_enable(pmu, counter, false);\n\n\tif (flags & PERF_EF_UPDATE)\n\t\tcn10k_ddr_perf_event_update(event);\n\n\thwc->state |= PERF_HES_STOPPED;\n}\n\nstatic void cn10k_ddr_perf_event_del(struct perf_event *event, int flags)\n{\n\tstruct cn10k_ddr_pmu *pmu = to_cn10k_ddr_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint counter = hwc->idx;\n\n\tcn10k_ddr_perf_event_stop(event, PERF_EF_UPDATE);\n\n\tcn10k_ddr_perf_free_counter(pmu, counter);\n\tpmu->active_events--;\n\thwc->idx = -1;\n\n\t \n\tif (pmu->active_events == 0)\n\t\thrtimer_cancel(&pmu->hrtimer);\n}\n\nstatic void cn10k_ddr_perf_pmu_enable(struct pmu *pmu)\n{\n\tstruct cn10k_ddr_pmu *ddr_pmu = to_cn10k_ddr_pmu(pmu);\n\n\twriteq_relaxed(START_OP_CTRL_VAL_START, ddr_pmu->base +\n\t\t       DDRC_PERF_CNT_START_OP_CTRL);\n}\n\nstatic void cn10k_ddr_perf_pmu_disable(struct pmu *pmu)\n{\n\tstruct cn10k_ddr_pmu *ddr_pmu = to_cn10k_ddr_pmu(pmu);\n\n\twriteq_relaxed(END_OP_CTRL_VAL_END, ddr_pmu->base +\n\t\t       DDRC_PERF_CNT_END_OP_CTRL);\n}\n\nstatic void cn10k_ddr_perf_event_update_all(struct cn10k_ddr_pmu *pmu)\n{\n\tstruct hw_perf_event *hwc;\n\tint i;\n\n\tfor (i = 0; i < DDRC_PERF_NUM_GEN_COUNTERS; i++) {\n\t\tif (pmu->events[i] == NULL)\n\t\t\tcontinue;\n\n\t\tcn10k_ddr_perf_event_update(pmu->events[i]);\n\t}\n\n\t \n\tfor (i = 0; i < DDRC_PERF_NUM_GEN_COUNTERS; i++) {\n\t\tif (pmu->events[i] == NULL)\n\t\t\tcontinue;\n\n\t\thwc = &pmu->events[i]->hw;\n\t\tlocal64_set(&hwc->prev_count, 0);\n\t}\n}\n\nstatic irqreturn_t cn10k_ddr_pmu_overflow_handler(struct cn10k_ddr_pmu *pmu)\n{\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tu64 prev_count, new_count;\n\tu64 value;\n\tint i;\n\n\tevent = pmu->events[DDRC_PERF_READ_COUNTER_IDX];\n\tif (event) {\n\t\thwc = &event->hw;\n\t\tprev_count = local64_read(&hwc->prev_count);\n\t\tnew_count = cn10k_ddr_perf_read_counter(pmu, hwc->idx);\n\n\t\t \n\t\tif (new_count < prev_count)\n\t\t\tcn10k_ddr_perf_event_update(event);\n\t}\n\n\tevent = pmu->events[DDRC_PERF_WRITE_COUNTER_IDX];\n\tif (event) {\n\t\thwc = &event->hw;\n\t\tprev_count = local64_read(&hwc->prev_count);\n\t\tnew_count = cn10k_ddr_perf_read_counter(pmu, hwc->idx);\n\n\t\t \n\t\tif (new_count < prev_count)\n\t\t\tcn10k_ddr_perf_event_update(event);\n\t}\n\n\tfor (i = 0; i < DDRC_PERF_NUM_GEN_COUNTERS; i++) {\n\t\tif (pmu->events[i] == NULL)\n\t\t\tcontinue;\n\n\t\tvalue = cn10k_ddr_perf_read_counter(pmu, i);\n\t\tif (value == DDRC_PERF_CNT_MAX_VALUE) {\n\t\t\tpr_info(\"Counter-(%d) reached max value\\n\", i);\n\t\t\tcn10k_ddr_perf_event_update_all(pmu);\n\t\t\tcn10k_ddr_perf_pmu_disable(&pmu->pmu);\n\t\t\tcn10k_ddr_perf_pmu_enable(&pmu->pmu);\n\t\t}\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic enum hrtimer_restart cn10k_ddr_pmu_timer_handler(struct hrtimer *hrtimer)\n{\n\tstruct cn10k_ddr_pmu *pmu = container_of(hrtimer, struct cn10k_ddr_pmu,\n\t\t\t\t\t\t hrtimer);\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcn10k_ddr_pmu_overflow_handler(pmu);\n\tlocal_irq_restore(flags);\n\n\thrtimer_forward_now(hrtimer, cn10k_ddr_pmu_timer_period());\n\treturn HRTIMER_RESTART;\n}\n\nstatic int cn10k_ddr_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct cn10k_ddr_pmu *pmu = hlist_entry_safe(node, struct cn10k_ddr_pmu,\n\t\t\t\t\t\t     node);\n\tunsigned int target;\n\n\tif (cpu != pmu->cpu)\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids)\n\t\treturn 0;\n\n\tperf_pmu_migrate_context(&pmu->pmu, cpu, target);\n\tpmu->cpu = target;\n\treturn 0;\n}\n\nstatic int cn10k_ddr_perf_probe(struct platform_device *pdev)\n{\n\tstruct cn10k_ddr_pmu *ddr_pmu;\n\tstruct resource *res;\n\tvoid __iomem *base;\n\tchar *name;\n\tint ret;\n\n\tddr_pmu = devm_kzalloc(&pdev->dev, sizeof(*ddr_pmu), GFP_KERNEL);\n\tif (!ddr_pmu)\n\t\treturn -ENOMEM;\n\n\tddr_pmu->dev = &pdev->dev;\n\tplatform_set_drvdata(pdev, ddr_pmu);\n\n\tbase = devm_platform_get_and_ioremap_resource(pdev, 0, &res);\n\tif (IS_ERR(base))\n\t\treturn PTR_ERR(base);\n\n\tddr_pmu->base = base;\n\n\t \n\twriteq_relaxed(OP_MODE_CTRL_VAL_MANNUAL, ddr_pmu->base +\n\t\t       DDRC_PERF_CNT_OP_MODE_CTRL);\n\n\tddr_pmu->pmu = (struct pmu) {\n\t\t.module\t      = THIS_MODULE,\n\t\t.capabilities = PERF_PMU_CAP_NO_EXCLUDE,\n\t\t.task_ctx_nr = perf_invalid_context,\n\t\t.attr_groups = cn10k_attr_groups,\n\t\t.event_init  = cn10k_ddr_perf_event_init,\n\t\t.add\t     = cn10k_ddr_perf_event_add,\n\t\t.del\t     = cn10k_ddr_perf_event_del,\n\t\t.start\t     = cn10k_ddr_perf_event_start,\n\t\t.stop\t     = cn10k_ddr_perf_event_stop,\n\t\t.read\t     = cn10k_ddr_perf_event_update,\n\t\t.pmu_enable  = cn10k_ddr_perf_pmu_enable,\n\t\t.pmu_disable = cn10k_ddr_perf_pmu_disable,\n\t};\n\n\t \n\tddr_pmu->cpu = raw_smp_processor_id();\n\n\tname = devm_kasprintf(ddr_pmu->dev, GFP_KERNEL, \"mrvl_ddr_pmu_%llx\",\n\t\t\t      res->start);\n\tif (!name)\n\t\treturn -ENOMEM;\n\n\thrtimer_init(&ddr_pmu->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tddr_pmu->hrtimer.function = cn10k_ddr_pmu_timer_handler;\n\n\tcpuhp_state_add_instance_nocalls(\n\t\t\t\tCPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE,\n\t\t\t\t&ddr_pmu->node);\n\n\tret = perf_pmu_register(&ddr_pmu->pmu, name, -1);\n\tif (ret)\n\t\tgoto error;\n\n\tpr_info(\"CN10K DDR PMU Driver for ddrc@%llx\\n\", res->start);\n\treturn 0;\nerror:\n\tcpuhp_state_remove_instance_nocalls(\n\t\t\t\tCPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE,\n\t\t\t\t&ddr_pmu->node);\n\treturn ret;\n}\n\nstatic int cn10k_ddr_perf_remove(struct platform_device *pdev)\n{\n\tstruct cn10k_ddr_pmu *ddr_pmu = platform_get_drvdata(pdev);\n\n\tcpuhp_state_remove_instance_nocalls(\n\t\t\t\tCPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE,\n\t\t\t\t&ddr_pmu->node);\n\n\tperf_pmu_unregister(&ddr_pmu->pmu);\n\treturn 0;\n}\n\n#ifdef CONFIG_OF\nstatic const struct of_device_id cn10k_ddr_pmu_of_match[] = {\n\t{ .compatible = \"marvell,cn10k-ddr-pmu\", },\n\t{ },\n};\nMODULE_DEVICE_TABLE(of, cn10k_ddr_pmu_of_match);\n#endif\n\n#ifdef CONFIG_ACPI\nstatic const struct acpi_device_id cn10k_ddr_pmu_acpi_match[] = {\n\t{\"MRVL000A\", 0},\n\t{},\n};\nMODULE_DEVICE_TABLE(acpi, cn10k_ddr_pmu_acpi_match);\n#endif\n\nstatic struct platform_driver cn10k_ddr_pmu_driver = {\n\t.driver\t= {\n\t\t.name   = \"cn10k-ddr-pmu\",\n\t\t.of_match_table = of_match_ptr(cn10k_ddr_pmu_of_match),\n\t\t.acpi_match_table  = ACPI_PTR(cn10k_ddr_pmu_acpi_match),\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe\t\t= cn10k_ddr_perf_probe,\n\t.remove\t\t= cn10k_ddr_perf_remove,\n};\n\nstatic int __init cn10k_ddr_pmu_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(\n\t\t\t\tCPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE,\n\t\t\t\t\"perf/marvell/cn10k/ddr:online\", NULL,\n\t\t\t\tcn10k_ddr_pmu_offline_cpu);\n\tif (ret)\n\t\treturn ret;\n\n\tret = platform_driver_register(&cn10k_ddr_pmu_driver);\n\tif (ret)\n\t\tcpuhp_remove_multi_state(\n\t\t\t\tCPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE);\n\treturn ret;\n}\n\nstatic void __exit cn10k_ddr_pmu_exit(void)\n{\n\tplatform_driver_unregister(&cn10k_ddr_pmu_driver);\n\tcpuhp_remove_multi_state(CPUHP_AP_PERF_ARM_MARVELL_CN10K_DDR_ONLINE);\n}\n\nmodule_init(cn10k_ddr_pmu_init);\nmodule_exit(cn10k_ddr_pmu_exit);\n\nMODULE_AUTHOR(\"Bharat Bhushan <bbhushan2@marvell.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}