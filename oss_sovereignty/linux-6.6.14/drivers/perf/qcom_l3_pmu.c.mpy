{
  "module_name": "qcom_l3_pmu.c",
  "hash_id": "36308ef6aa194d7f112accb462060b975ae47fc520ba1a8259bdc9cbe6e8da40",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/qcom_l3_pmu.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/bitops.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n\n \n\n \n#define L3_NUM_COUNTERS  8\n \n#define L3_EVTYPE_MASK   0xFF\n \n#define L3_EVENT_LC_BIT  32\n\n \n\n \n#define L3_HML3_PM_CR       0x000\n#define L3_HML3_PM_EVCNTR(__cntr) (0x420 + ((__cntr) & 0x7) * 8)\n#define L3_HML3_PM_CNTCTL(__cntr) (0x120 + ((__cntr) & 0x7) * 8)\n#define L3_HML3_PM_EVTYPE(__cntr) (0x220 + ((__cntr) & 0x7) * 8)\n#define L3_HML3_PM_FILTRA   0x300\n#define L3_HML3_PM_FILTRB   0x308\n#define L3_HML3_PM_FILTRC   0x310\n#define L3_HML3_PM_FILTRAM  0x304\n#define L3_HML3_PM_FILTRBM  0x30C\n#define L3_HML3_PM_FILTRCM  0x314\n\n \n#define L3_M_BC_CR         0x500\n#define L3_M_BC_SATROLL_CR 0x504\n#define L3_M_BC_CNTENSET   0x508\n#define L3_M_BC_CNTENCLR   0x50C\n#define L3_M_BC_INTENSET   0x510\n#define L3_M_BC_INTENCLR   0x514\n#define L3_M_BC_GANG       0x718\n#define L3_M_BC_OVSR       0x740\n#define L3_M_BC_IRQCTL     0x96C\n\n \n\n \n#define PM_CR_RESET           (0)\n\n \n#define PMCNT_RESET           (0)\n\n \n#define EVSEL(__val)          ((__val) & L3_EVTYPE_MASK)\n\n \n#define PM_FLTR_RESET         (0)\n\n \n#define BC_RESET              (1UL << 1)\n#define BC_ENABLE             (1UL << 0)\n\n \n#define BC_SATROLL_CR_RESET   (0)\n\n \n#define PMCNTENSET(__cntr)    (1UL << ((__cntr) & 0x7))\n\n \n#define PMCNTENCLR(__cntr)    (1UL << ((__cntr) & 0x7))\n#define BC_CNTENCLR_RESET     (0xFF)\n\n \n#define PMINTENSET(__cntr)    (1UL << ((__cntr) & 0x7))\n\n \n#define PMINTENCLR(__cntr)    (1UL << ((__cntr) & 0x7))\n#define BC_INTENCLR_RESET     (0xFF)\n\n \n#define GANG_EN(__cntr)       (1UL << ((__cntr) & 0x7))\n#define BC_GANG_RESET         (0)\n\n \n#define PMOVSRCLR(__cntr)     (1UL << ((__cntr) & 0x7))\n#define PMOVSRCLR_RESET       (0xFF)\n\n \n#define PMIRQONMSBEN(__cntr)  (1UL << ((__cntr) & 0x7))\n#define BC_IRQCTL_RESET       (0x0)\n\n \n\n#define L3_EVENT_CYCLES\t\t0x01\n#define L3_EVENT_READ_HIT\t\t0x20\n#define L3_EVENT_READ_MISS\t\t0x21\n#define L3_EVENT_READ_HIT_D\t\t0x22\n#define L3_EVENT_READ_MISS_D\t\t0x23\n#define L3_EVENT_WRITE_HIT\t\t0x24\n#define L3_EVENT_WRITE_MISS\t\t0x25\n\n \n\nstatic inline u32 get_event_type(struct perf_event *event)\n{\n\treturn (event->attr.config) & L3_EVTYPE_MASK;\n}\n\nstatic inline bool event_uses_long_counter(struct perf_event *event)\n{\n\treturn !!(event->attr.config & BIT_ULL(L3_EVENT_LC_BIT));\n}\n\nstatic inline int event_num_counters(struct perf_event *event)\n{\n\treturn event_uses_long_counter(event) ? 2 : 1;\n}\n\n \nstruct l3cache_pmu {\n\tstruct pmu\t\tpmu;\n\tstruct hlist_node\tnode;\n\tvoid __iomem\t\t*regs;\n\tstruct perf_event\t*events[L3_NUM_COUNTERS];\n\tunsigned long\t\tused_mask[BITS_TO_LONGS(L3_NUM_COUNTERS)];\n\tcpumask_t\t\tcpumask;\n};\n\n#define to_l3cache_pmu(p) (container_of(p, struct l3cache_pmu, pmu))\n\n \nstruct l3cache_event_ops {\n\t \n\tvoid (*start)(struct perf_event *event);\n\t \n\tvoid (*stop)(struct perf_event *event, int flags);\n\t \n\tvoid (*update)(struct perf_event *event);\n};\n\n \n\nstatic void qcom_l3_cache__64bit_counter_start(struct perf_event *event)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 evsel = get_event_type(event);\n\tu32 gang;\n\n\t \n\tgang = readl_relaxed(l3pmu->regs + L3_M_BC_GANG);\n\tgang |= GANG_EN(idx + 1);\n\twritel_relaxed(gang, l3pmu->regs + L3_M_BC_GANG);\n\n\t \n\tlocal64_set(&event->hw.prev_count, 0);\n\twritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1));\n\twritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\n\n\t \n\twritel_relaxed(EVSEL(0), l3pmu->regs + L3_HML3_PM_EVTYPE(idx + 1));\n\twritel_relaxed(EVSEL(evsel), l3pmu->regs + L3_HML3_PM_EVTYPE(idx));\n\n\t \n\twritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx + 1));\n\twritel_relaxed(PMCNTENSET(idx + 1), l3pmu->regs + L3_M_BC_CNTENSET);\n\twritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx));\n\twritel_relaxed(PMCNTENSET(idx), l3pmu->regs + L3_M_BC_CNTENSET);\n}\n\nstatic void qcom_l3_cache__64bit_counter_stop(struct perf_event *event,\n\t\t\t\t\t      int flags)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 gang = readl_relaxed(l3pmu->regs + L3_M_BC_GANG);\n\n\t \n\twritel_relaxed(PMCNTENCLR(idx), l3pmu->regs + L3_M_BC_CNTENCLR);\n\twritel_relaxed(PMCNTENCLR(idx + 1), l3pmu->regs + L3_M_BC_CNTENCLR);\n\n\t \n\twritel_relaxed(gang & ~GANG_EN(idx + 1), l3pmu->regs + L3_M_BC_GANG);\n}\n\nstatic void qcom_l3_cache__64bit_counter_update(struct perf_event *event)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 hi, lo;\n\tu64 prev, new;\n\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tdo {\n\t\t\thi = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1));\n\t\t\tlo = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\n\t\t} while (hi != readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx + 1)));\n\t\tnew = ((u64)hi << 32) | lo;\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, new) != prev);\n\n\tlocal64_add(new - prev, &event->count);\n}\n\nstatic const struct l3cache_event_ops event_ops_long = {\n\t.start = qcom_l3_cache__64bit_counter_start,\n\t.stop = qcom_l3_cache__64bit_counter_stop,\n\t.update = qcom_l3_cache__64bit_counter_update,\n};\n\n \n\nstatic void qcom_l3_cache__32bit_counter_start(struct perf_event *event)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 evsel = get_event_type(event);\n\tu32 irqctl = readl_relaxed(l3pmu->regs + L3_M_BC_IRQCTL);\n\n\t \n\twritel_relaxed(irqctl | PMIRQONMSBEN(idx), l3pmu->regs + L3_M_BC_IRQCTL);\n\n\t \n\tlocal64_set(&event->hw.prev_count, 0);\n\twritel_relaxed(0, l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\n\n\t \n\twritel_relaxed(EVSEL(evsel), l3pmu->regs + L3_HML3_PM_EVTYPE(idx));\n\n\t \n\twritel_relaxed(PMINTENSET(idx), l3pmu->regs + L3_M_BC_INTENSET);\n\n\t \n\twritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(idx));\n\twritel_relaxed(PMCNTENSET(idx), l3pmu->regs + L3_M_BC_CNTENSET);\n}\n\nstatic void qcom_l3_cache__32bit_counter_stop(struct perf_event *event,\n\t\t\t\t\t      int flags)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 irqctl = readl_relaxed(l3pmu->regs + L3_M_BC_IRQCTL);\n\n\t \n\twritel_relaxed(PMCNTENCLR(idx), l3pmu->regs + L3_M_BC_CNTENCLR);\n\n\t \n\twritel_relaxed(PMINTENCLR(idx), l3pmu->regs + L3_M_BC_INTENCLR);\n\n\t \n\twritel_relaxed(irqctl & ~PMIRQONMSBEN(idx), l3pmu->regs + L3_M_BC_IRQCTL);\n}\n\nstatic void qcom_l3_cache__32bit_counter_update(struct perf_event *event)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tint idx = event->hw.idx;\n\tu32 prev, new;\n\n\tdo {\n\t\tprev = local64_read(&event->hw.prev_count);\n\t\tnew = readl_relaxed(l3pmu->regs + L3_HML3_PM_EVCNTR(idx));\n\t} while (local64_cmpxchg(&event->hw.prev_count, prev, new) != prev);\n\n\tlocal64_add(new - prev, &event->count);\n}\n\nstatic const struct l3cache_event_ops event_ops_std = {\n\t.start = qcom_l3_cache__32bit_counter_start,\n\t.stop = qcom_l3_cache__32bit_counter_stop,\n\t.update = qcom_l3_cache__32bit_counter_update,\n};\n\n \nstatic\nconst struct l3cache_event_ops *l3cache_event_get_ops(struct perf_event *event)\n{\n\tif (event_uses_long_counter(event))\n\t\treturn &event_ops_long;\n\telse\n\t\treturn &event_ops_std;\n}\n\n \n\nstatic inline void qcom_l3_cache__init(struct l3cache_pmu *l3pmu)\n{\n\tint i;\n\n\twritel_relaxed(BC_RESET, l3pmu->regs + L3_M_BC_CR);\n\n\t \n\twritel(BC_SATROLL_CR_RESET, l3pmu->regs + L3_M_BC_SATROLL_CR);\n\n\twritel_relaxed(BC_CNTENCLR_RESET, l3pmu->regs + L3_M_BC_CNTENCLR);\n\twritel_relaxed(BC_INTENCLR_RESET, l3pmu->regs + L3_M_BC_INTENCLR);\n\twritel_relaxed(PMOVSRCLR_RESET, l3pmu->regs + L3_M_BC_OVSR);\n\twritel_relaxed(BC_GANG_RESET, l3pmu->regs + L3_M_BC_GANG);\n\twritel_relaxed(BC_IRQCTL_RESET, l3pmu->regs + L3_M_BC_IRQCTL);\n\twritel_relaxed(PM_CR_RESET, l3pmu->regs + L3_HML3_PM_CR);\n\n\tfor (i = 0; i < L3_NUM_COUNTERS; ++i) {\n\t\twritel_relaxed(PMCNT_RESET, l3pmu->regs + L3_HML3_PM_CNTCTL(i));\n\t\twritel_relaxed(EVSEL(0), l3pmu->regs + L3_HML3_PM_EVTYPE(i));\n\t}\n\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRA);\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRAM);\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRB);\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRBM);\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRC);\n\twritel_relaxed(PM_FLTR_RESET, l3pmu->regs + L3_HML3_PM_FILTRCM);\n\n\t \n\twritel(BC_ENABLE, l3pmu->regs + L3_M_BC_CR);\n}\n\nstatic irqreturn_t qcom_l3_cache__handle_irq(int irq_num, void *data)\n{\n\tstruct l3cache_pmu *l3pmu = data;\n\t \n\tlong status = readl_relaxed(l3pmu->regs + L3_M_BC_OVSR);\n\tint idx;\n\n\tif (status == 0)\n\t\treturn IRQ_NONE;\n\n\t \n\twritel_relaxed(status, l3pmu->regs + L3_M_BC_OVSR);\n\n\tfor_each_set_bit(idx, &status, L3_NUM_COUNTERS) {\n\t\tstruct perf_event *event;\n\t\tconst struct l3cache_event_ops *ops;\n\n\t\tevent = l3pmu->events[idx];\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\t \n\n\t\tops = l3cache_event_get_ops(event);\n\t\tops->update(event);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic void qcom_l3_cache__pmu_enable(struct pmu *pmu)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(pmu);\n\n\t \n\twmb();\n\n\twritel_relaxed(BC_ENABLE, l3pmu->regs + L3_M_BC_CR);\n}\n\nstatic void qcom_l3_cache__pmu_disable(struct pmu *pmu)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(pmu);\n\n\twritel_relaxed(0, l3pmu->regs + L3_M_BC_CR);\n\n\t \n\twmb();\n}\n\n \nstatic bool qcom_l3_cache__validate_event_group(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling;\n\tint counters = 0;\n\n\tif (leader->pmu != event->pmu && !is_software_event(leader))\n\t\treturn false;\n\n\tcounters = event_num_counters(event);\n\tcounters += event_num_counters(leader);\n\n\tfor_each_sibling_event(sibling, leader) {\n\t\tif (is_software_event(sibling))\n\t\t\tcontinue;\n\t\tif (sibling->pmu != event->pmu)\n\t\t\treturn false;\n\t\tcounters += event_num_counters(sibling);\n\t}\n\n\t \n\treturn counters <= L3_NUM_COUNTERS;\n}\n\nstatic int qcom_l3_cache__event_init(struct perf_event *event)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (hwc->sample_period)\n\t\treturn -EINVAL;\n\n\t \n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (!qcom_l3_cache__validate_event_group(event))\n\t\treturn -EINVAL;\n\n\thwc->idx = -1;\n\n\t \n\tevent->cpu = cpumask_first(&l3pmu->cpumask);\n\n\treturn 0;\n}\n\nstatic void qcom_l3_cache__event_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\n\n\thwc->state = 0;\n\tops->start(event);\n}\n\nstatic void qcom_l3_cache__event_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\n\n\tif (hwc->state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tops->stop(event, flags);\n\tif (flags & PERF_EF_UPDATE)\n\t\tops->update(event);\n\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n}\n\nstatic int qcom_l3_cache__event_add(struct perf_event *event, int flags)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint order = event_uses_long_counter(event) ? 1 : 0;\n\tint idx;\n\n\t \n\tidx = bitmap_find_free_region(l3pmu->used_mask, L3_NUM_COUNTERS, order);\n\tif (idx < 0)\n\t\t \n\t\treturn -EAGAIN;\n\n\thwc->idx = idx;\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tl3pmu->events[idx] = event;\n\n\tif (flags & PERF_EF_START)\n\t\tqcom_l3_cache__event_start(event, 0);\n\n\t \n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void qcom_l3_cache__event_del(struct perf_event *event, int flags)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint order = event_uses_long_counter(event) ? 1 : 0;\n\n\t \n\tqcom_l3_cache__event_stop(event,  flags | PERF_EF_UPDATE);\n\tl3pmu->events[hwc->idx] = NULL;\n\tbitmap_release_region(l3pmu->used_mask, hwc->idx, order);\n\n\t \n\tperf_event_update_userpage(event);\n}\n\nstatic void qcom_l3_cache__event_read(struct perf_event *event)\n{\n\tconst struct l3cache_event_ops *ops = l3cache_event_get_ops(event);\n\n\tops->update(event);\n}\n\n \n\n \n\nstatic ssize_t l3cache_pmu_format_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"%s\\n\", (char *) eattr->var);\n}\n\n#define L3CACHE_PMU_FORMAT_ATTR(_name, _config)\t\t\t\t      \\\n\t(&((struct dev_ext_attribute[]) {\t\t\t\t      \\\n\t\t{ .attr = __ATTR(_name, 0444, l3cache_pmu_format_show, NULL), \\\n\t\t  .var = (void *) _config, }\t\t\t\t      \\\n\t})[0].attr.attr)\n\nstatic struct attribute *qcom_l3_cache_pmu_formats[] = {\n\tL3CACHE_PMU_FORMAT_ATTR(event, \"config:0-7\"),\n\tL3CACHE_PMU_FORMAT_ATTR(lc, \"config:\" __stringify(L3_EVENT_LC_BIT)),\n\tNULL,\n};\n\nstatic const struct attribute_group qcom_l3_cache_pmu_format_group = {\n\t.name = \"format\",\n\t.attrs = qcom_l3_cache_pmu_formats,\n};\n\n \n\nstatic ssize_t l3cache_pmu_event_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr);\n\treturn sysfs_emit(page, \"event=0x%02llx\\n\", pmu_attr->id);\n}\n\n#define L3CACHE_EVENT_ATTR(_name, _id)\t\t\t\t\t     \\\n\tPMU_EVENT_ATTR_ID(_name, l3cache_pmu_event_show, _id)\n\nstatic struct attribute *qcom_l3_cache_pmu_events[] = {\n\tL3CACHE_EVENT_ATTR(cycles, L3_EVENT_CYCLES),\n\tL3CACHE_EVENT_ATTR(read-hit, L3_EVENT_READ_HIT),\n\tL3CACHE_EVENT_ATTR(read-miss, L3_EVENT_READ_MISS),\n\tL3CACHE_EVENT_ATTR(read-hit-d-side, L3_EVENT_READ_HIT_D),\n\tL3CACHE_EVENT_ATTR(read-miss-d-side, L3_EVENT_READ_MISS_D),\n\tL3CACHE_EVENT_ATTR(write-hit, L3_EVENT_WRITE_HIT),\n\tL3CACHE_EVENT_ATTR(write-miss, L3_EVENT_WRITE_MISS),\n\tNULL\n};\n\nstatic const struct attribute_group qcom_l3_cache_pmu_events_group = {\n\t.name = \"events\",\n\t.attrs = qcom_l3_cache_pmu_events,\n};\n\n \n\nstatic ssize_t cpumask_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct l3cache_pmu *l3pmu = to_l3cache_pmu(dev_get_drvdata(dev));\n\n\treturn cpumap_print_to_pagebuf(true, buf, &l3pmu->cpumask);\n}\n\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *qcom_l3_cache_pmu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group qcom_l3_cache_pmu_cpumask_attr_group = {\n\t.attrs = qcom_l3_cache_pmu_cpumask_attrs,\n};\n\n \nstatic const struct attribute_group *qcom_l3_cache_pmu_attr_grps[] = {\n\t&qcom_l3_cache_pmu_format_group,\n\t&qcom_l3_cache_pmu_events_group,\n\t&qcom_l3_cache_pmu_cpumask_attr_group,\n\tNULL,\n};\n\n \n\nstatic int qcom_l3_cache_pmu_online_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct l3cache_pmu *l3pmu = hlist_entry_safe(node, struct l3cache_pmu, node);\n\n\t \n\tif (cpumask_empty(&l3pmu->cpumask))\n\t\tcpumask_set_cpu(cpu, &l3pmu->cpumask);\n\n\treturn 0;\n}\n\nstatic int qcom_l3_cache_pmu_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct l3cache_pmu *l3pmu = hlist_entry_safe(node, struct l3cache_pmu, node);\n\tunsigned int target;\n\n\tif (!cpumask_test_and_clear_cpu(cpu, &l3pmu->cpumask))\n\t\treturn 0;\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids)\n\t\treturn 0;\n\tperf_pmu_migrate_context(&l3pmu->pmu, cpu, target);\n\tcpumask_set_cpu(target, &l3pmu->cpumask);\n\treturn 0;\n}\n\nstatic int qcom_l3_cache_pmu_probe(struct platform_device *pdev)\n{\n\tstruct l3cache_pmu *l3pmu;\n\tstruct acpi_device *acpi_dev;\n\tstruct resource *memrc;\n\tint ret;\n\tchar *name;\n\n\t \n\n\tacpi_dev = ACPI_COMPANION(&pdev->dev);\n\tif (!acpi_dev)\n\t\treturn -ENODEV;\n\n\tl3pmu = devm_kzalloc(&pdev->dev, sizeof(*l3pmu), GFP_KERNEL);\n\tname = devm_kasprintf(&pdev->dev, GFP_KERNEL, \"l3cache_%s_%s\",\n\t\t      acpi_dev_parent(acpi_dev)->pnp.unique_id,\n\t\t      acpi_dev->pnp.unique_id);\n\tif (!l3pmu || !name)\n\t\treturn -ENOMEM;\n\n\tl3pmu->pmu = (struct pmu) {\n\t\t.task_ctx_nr\t= perf_invalid_context,\n\n\t\t.pmu_enable\t= qcom_l3_cache__pmu_enable,\n\t\t.pmu_disable\t= qcom_l3_cache__pmu_disable,\n\t\t.event_init\t= qcom_l3_cache__event_init,\n\t\t.add\t\t= qcom_l3_cache__event_add,\n\t\t.del\t\t= qcom_l3_cache__event_del,\n\t\t.start\t\t= qcom_l3_cache__event_start,\n\t\t.stop\t\t= qcom_l3_cache__event_stop,\n\t\t.read\t\t= qcom_l3_cache__event_read,\n\n\t\t.attr_groups\t= qcom_l3_cache_pmu_attr_grps,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\tl3pmu->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &memrc);\n\tif (IS_ERR(l3pmu->regs))\n\t\treturn PTR_ERR(l3pmu->regs);\n\n\tqcom_l3_cache__init(l3pmu);\n\n\tret = platform_get_irq(pdev, 0);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tret = devm_request_irq(&pdev->dev, ret, qcom_l3_cache__handle_irq, 0,\n\t\t\t       name, l3pmu);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Request for IRQ failed for slice @%pa\\n\",\n\t\t\t&memrc->start);\n\t\treturn ret;\n\t}\n\n\t \n\tret = cpuhp_state_add_instance(CPUHP_AP_PERF_ARM_QCOM_L3_ONLINE, &l3pmu->node);\n\tif (ret) {\n\t\tdev_err(&pdev->dev, \"Error %d registering hotplug\", ret);\n\t\treturn ret;\n\t}\n\n\tret = perf_pmu_register(&l3pmu->pmu, name, -1);\n\tif (ret < 0) {\n\t\tdev_err(&pdev->dev, \"Failed to register L3 cache PMU (%d)\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tdev_info(&pdev->dev, \"Registered %s, type: %d\\n\", name, l3pmu->pmu.type);\n\n\treturn 0;\n}\n\nstatic const struct acpi_device_id qcom_l3_cache_pmu_acpi_match[] = {\n\t{ \"QCOM8081\", },\n\t{ }\n};\nMODULE_DEVICE_TABLE(acpi, qcom_l3_cache_pmu_acpi_match);\n\nstatic struct platform_driver qcom_l3_cache_pmu_driver = {\n\t.driver = {\n\t\t.name = \"qcom-l3cache-pmu\",\n\t\t.acpi_match_table = ACPI_PTR(qcom_l3_cache_pmu_acpi_match),\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe = qcom_l3_cache_pmu_probe,\n};\n\nstatic int __init register_qcom_l3_cache_pmu_driver(void)\n{\n\tint ret;\n\n\t \n\tret = cpuhp_setup_state_multi(CPUHP_AP_PERF_ARM_QCOM_L3_ONLINE,\n\t\t\t\t      \"perf/qcom/l3cache:online\",\n\t\t\t\t      qcom_l3_cache_pmu_online_cpu,\n\t\t\t\t      qcom_l3_cache_pmu_offline_cpu);\n\tif (ret)\n\t\treturn ret;\n\n\treturn platform_driver_register(&qcom_l3_cache_pmu_driver);\n}\ndevice_initcall(register_qcom_l3_cache_pmu_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}