{
  "module_name": "riscv_pmu.c",
  "hash_id": "c60df37a3af55e3efb997c94cb421afff517dc6c285f579f43ac2f43e9d86808",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/riscv_pmu.c",
  "human_readable_source": "\n \n\n#include <linux/cpumask.h>\n#include <linux/irq.h>\n#include <linux/irqdesc.h>\n#include <linux/perf/riscv_pmu.h>\n#include <linux/printk.h>\n#include <linux/smp.h>\n#include <linux/sched_clock.h>\n\n#include <asm/sbi.h>\n\nstatic bool riscv_perf_user_access(struct perf_event *event)\n{\n\treturn ((event->attr.type == PERF_TYPE_HARDWARE) ||\n\t\t(event->attr.type == PERF_TYPE_HW_CACHE) ||\n\t\t(event->attr.type == PERF_TYPE_RAW)) &&\n\t\t!!(event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT) &&\n\t\t(event->hw.idx != -1);\n}\n\nvoid arch_perf_update_userpage(struct perf_event *event,\n\t\t\t       struct perf_event_mmap_page *userpg, u64 now)\n{\n\tstruct clock_read_data *rd;\n\tunsigned int seq;\n\tu64 ns;\n\n\tuserpg->cap_user_time = 0;\n\tuserpg->cap_user_time_zero = 0;\n\tuserpg->cap_user_time_short = 0;\n\tuserpg->cap_user_rdpmc = riscv_perf_user_access(event);\n\n#ifdef CONFIG_RISCV_PMU\n\t \n\tif (userpg->cap_user_rdpmc)\n\t\tuserpg->pmc_width = to_riscv_pmu(event->pmu)->ctr_get_width(event->hw.idx) + 1;\n#endif\n\n\tdo {\n\t\trd = sched_clock_read_begin(&seq);\n\n\t\tuserpg->time_mult = rd->mult;\n\t\tuserpg->time_shift = rd->shift;\n\t\tuserpg->time_zero = rd->epoch_ns;\n\t\tuserpg->time_cycles = rd->epoch_cyc;\n\t\tuserpg->time_mask = rd->sched_clock_mask;\n\n\t\t \n\t\tns = mul_u64_u32_shr(rd->epoch_cyc, rd->mult, rd->shift);\n\t\tuserpg->time_zero -= ns;\n\n\t} while (sched_clock_read_retry(seq));\n\n\tuserpg->time_offset = userpg->time_zero - now;\n\n\t \n\tif (userpg->time_shift == 32) {\n\t\tuserpg->time_shift = 31;\n\t\tuserpg->time_mult >>= 1;\n\t}\n\n\t \n\tuserpg->cap_user_time = 1;\n\tuserpg->cap_user_time_zero = 1;\n\tuserpg->cap_user_time_short = 1;\n}\n\nstatic unsigned long csr_read_num(int csr_num)\n{\n#define switchcase_csr_read(__csr_num, __val)\t\t{\\\n\tcase __csr_num:\t\t\t\t\t\\\n\t\t__val = csr_read(__csr_num);\t\t\\\n\t\tbreak; }\n#define switchcase_csr_read_2(__csr_num, __val)\t\t{\\\n\tswitchcase_csr_read(__csr_num + 0, __val)\t \\\n\tswitchcase_csr_read(__csr_num + 1, __val)}\n#define switchcase_csr_read_4(__csr_num, __val)\t\t{\\\n\tswitchcase_csr_read_2(__csr_num + 0, __val)\t \\\n\tswitchcase_csr_read_2(__csr_num + 2, __val)}\n#define switchcase_csr_read_8(__csr_num, __val)\t\t{\\\n\tswitchcase_csr_read_4(__csr_num + 0, __val)\t \\\n\tswitchcase_csr_read_4(__csr_num + 4, __val)}\n#define switchcase_csr_read_16(__csr_num, __val)\t{\\\n\tswitchcase_csr_read_8(__csr_num + 0, __val)\t \\\n\tswitchcase_csr_read_8(__csr_num + 8, __val)}\n#define switchcase_csr_read_32(__csr_num, __val)\t{\\\n\tswitchcase_csr_read_16(__csr_num + 0, __val)\t \\\n\tswitchcase_csr_read_16(__csr_num + 16, __val)}\n\n\tunsigned long ret = 0;\n\n\tswitch (csr_num) {\n\tswitchcase_csr_read_32(CSR_CYCLE, ret)\n\tswitchcase_csr_read_32(CSR_CYCLEH, ret)\n\tdefault :\n\t\tbreak;\n\t}\n\n\treturn ret;\n#undef switchcase_csr_read_32\n#undef switchcase_csr_read_16\n#undef switchcase_csr_read_8\n#undef switchcase_csr_read_4\n#undef switchcase_csr_read_2\n#undef switchcase_csr_read\n}\n\n \nunsigned long riscv_pmu_ctr_read_csr(unsigned long csr)\n{\n\tif (csr < CSR_CYCLE || csr > CSR_HPMCOUNTER31H ||\n\t   (csr > CSR_HPMCOUNTER31 && csr < CSR_CYCLEH)) {\n\t\tpr_err(\"Invalid performance counter csr %lx\\n\", csr);\n\t\treturn -EINVAL;\n\t}\n\n\treturn csr_read_num(csr);\n}\n\nu64 riscv_pmu_ctr_get_width_mask(struct perf_event *event)\n{\n\tint cwidth;\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!rvpmu->ctr_get_width)\n\t \n\t\tcwidth = 63;\n\telse {\n\t\tif (hwc->idx == -1)\n\t\t\t \n\t\t\tcwidth = rvpmu->ctr_get_width(0);\n\t\telse\n\t\t\tcwidth = rvpmu->ctr_get_width(hwc->idx);\n\t}\n\n\treturn GENMASK_ULL(cwidth, 0);\n}\n\nu64 riscv_pmu_event_update(struct perf_event *event)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 prev_raw_count, new_raw_count;\n\tunsigned long cmask;\n\tu64 oldval, delta;\n\n\tif (!rvpmu->ctr_read)\n\t\treturn 0;\n\n\tcmask = riscv_pmu_ctr_get_width_mask(event);\n\n\tdo {\n\t\tprev_raw_count = local64_read(&hwc->prev_count);\n\t\tnew_raw_count = rvpmu->ctr_read(event);\n\t\toldval = local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t\t\t\t new_raw_count);\n\t} while (oldval != prev_raw_count);\n\n\tdelta = (new_raw_count - prev_raw_count) & cmask;\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn delta;\n}\n\nvoid riscv_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\n\tif (!(hwc->state & PERF_HES_STOPPED)) {\n\t\tif (rvpmu->ctr_stop) {\n\t\t\trvpmu->ctr_stop(event, 0);\n\t\t\thwc->state |= PERF_HES_STOPPED;\n\t\t}\n\t\triscv_pmu_event_update(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nint riscv_pmu_event_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint overflow = 0;\n\tuint64_t max_period = riscv_pmu_ctr_get_width_mask(event);\n\n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\toverflow = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\toverflow = 1;\n\t}\n\n\t \n\tif (left > (max_period >> 1))\n\t\tleft = (max_period >> 1);\n\n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\tperf_event_update_userpage(event);\n\n\treturn overflow;\n}\n\nvoid riscv_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tuint64_t max_period = riscv_pmu_ctr_get_width_mask(event);\n\tu64 init_val;\n\n\tif (flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\triscv_pmu_event_set_period(event);\n\tinit_val = local64_read(&hwc->prev_count) & max_period;\n\trvpmu->ctr_start(event, init_val);\n\tperf_event_update_userpage(event);\n}\n\nstatic int riscv_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(rvpmu->hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\tidx = rvpmu->ctr_get_idx(event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\thwc->idx = idx;\n\tcpuc->events[idx] = event;\n\tcpuc->n_events++;\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (flags & PERF_EF_START)\n\t\triscv_pmu_start(event, PERF_EF_RELOAD);\n\n\t \n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void riscv_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(rvpmu->hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\triscv_pmu_stop(event, PERF_EF_UPDATE);\n\tcpuc->events[hwc->idx] = NULL;\n\t \n\tif (rvpmu->ctr_stop)\n\t\trvpmu->ctr_stop(event, RISCV_PMU_STOP_FLAG_RESET);\n\tcpuc->n_events--;\n\tif (rvpmu->ctr_clear_idx)\n\t\trvpmu->ctr_clear_idx(event);\n\tperf_event_update_userpage(event);\n\thwc->idx = -1;\n}\n\nstatic void riscv_pmu_read(struct perf_event *event)\n{\n\triscv_pmu_event_update(event);\n}\n\nstatic int riscv_pmu_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\tint mapped_event;\n\tu64 event_config = 0;\n\tuint64_t cmask;\n\n\thwc->flags = 0;\n\tmapped_event = rvpmu->event_map(event, &event_config);\n\tif (mapped_event < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapped_event;\n\t}\n\n\t \n\thwc->config = event_config;\n\thwc->idx = -1;\n\thwc->event_base = mapped_event;\n\n\tif (rvpmu->event_init)\n\t\trvpmu->event_init(event);\n\n\tif (!is_sampling_event(event)) {\n\t\t \n\t\tcmask = riscv_pmu_ctr_get_width_mask(event);\n\t\thwc->sample_period  =  cmask >> 1;\n\t\thwc->last_period    = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\treturn 0;\n}\n\nstatic int riscv_pmu_event_idx(struct perf_event *event)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\n\tif (!(event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT))\n\t\treturn 0;\n\n\tif (rvpmu->csr_index)\n\t\treturn rvpmu->csr_index(event) + 1;\n\n\treturn 0;\n}\n\nstatic void riscv_pmu_event_mapped(struct perf_event *event, struct mm_struct *mm)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\n\tif (rvpmu->event_mapped) {\n\t\trvpmu->event_mapped(event, mm);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\nstatic void riscv_pmu_event_unmapped(struct perf_event *event, struct mm_struct *mm)\n{\n\tstruct riscv_pmu *rvpmu = to_riscv_pmu(event->pmu);\n\n\tif (rvpmu->event_unmapped) {\n\t\trvpmu->event_unmapped(event, mm);\n\t\tperf_event_update_userpage(event);\n\t}\n}\n\nstruct riscv_pmu *riscv_pmu_alloc(void)\n{\n\tstruct riscv_pmu *pmu;\n\tint cpuid, i;\n\tstruct cpu_hw_events *cpuc;\n\n\tpmu = kzalloc(sizeof(*pmu), GFP_KERNEL);\n\tif (!pmu)\n\t\tgoto out;\n\n\tpmu->hw_events = alloc_percpu_gfp(struct cpu_hw_events, GFP_KERNEL);\n\tif (!pmu->hw_events) {\n\t\tpr_info(\"failed to allocate per-cpu PMU data.\\n\");\n\t\tgoto out_free_pmu;\n\t}\n\n\tfor_each_possible_cpu(cpuid) {\n\t\tcpuc = per_cpu_ptr(pmu->hw_events, cpuid);\n\t\tcpuc->n_events = 0;\n\t\tfor (i = 0; i < RISCV_MAX_COUNTERS; i++)\n\t\t\tcpuc->events[i] = NULL;\n\t}\n\tpmu->pmu = (struct pmu) {\n\t\t.event_init\t= riscv_pmu_event_init,\n\t\t.event_mapped\t= riscv_pmu_event_mapped,\n\t\t.event_unmapped\t= riscv_pmu_event_unmapped,\n\t\t.event_idx\t= riscv_pmu_event_idx,\n\t\t.add\t\t= riscv_pmu_add,\n\t\t.del\t\t= riscv_pmu_del,\n\t\t.start\t\t= riscv_pmu_start,\n\t\t.stop\t\t= riscv_pmu_stop,\n\t\t.read\t\t= riscv_pmu_read,\n\t};\n\n\treturn pmu;\n\nout_free_pmu:\n\tkfree(pmu);\nout:\n\treturn NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}