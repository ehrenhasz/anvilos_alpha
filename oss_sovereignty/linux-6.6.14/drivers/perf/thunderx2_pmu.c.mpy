{
  "module_name": "thunderx2_pmu.c",
  "hash_id": "2523dc13278ad04e445ed69b4079d43aa0e4f4e67eb295a3df422fbae01c59ed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/thunderx2_pmu.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/cpuhotplug.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n\n \n\n#define TX2_PMU_DMC_L3C_MAX_COUNTERS\t4\n#define TX2_PMU_CCPI2_MAX_COUNTERS\t8\n#define TX2_PMU_MAX_COUNTERS\t\tTX2_PMU_CCPI2_MAX_COUNTERS\n\n\n#define TX2_PMU_DMC_CHANNELS\t\t8\n#define TX2_PMU_L3_TILES\t\t16\n\n#define TX2_PMU_HRTIMER_INTERVAL\t(2 * NSEC_PER_SEC)\n#define GET_EVENTID(ev, mask)\t\t((ev->hw.config) & mask)\n#define GET_COUNTERID(ev, mask)\t\t((ev->hw.idx) & mask)\n  \n#define DMC_EVENT_CFG(idx, val)\t\t((val) << (((idx) * 8) + 1))\n\n \n#define CCPI2_COUNTER_OFFSET\t\t8\n\n#define L3C_COUNTER_CTL\t\t\t0xA8\n#define L3C_COUNTER_DATA\t\t0xAC\n#define DMC_COUNTER_CTL\t\t\t0x234\n#define DMC_COUNTER_DATA\t\t0x240\n\n#define CCPI2_PERF_CTL\t\t\t0x108\n#define CCPI2_COUNTER_CTL\t\t0x10C\n#define CCPI2_COUNTER_SEL\t\t0x12c\n#define CCPI2_COUNTER_DATA_L\t\t0x130\n#define CCPI2_COUNTER_DATA_H\t\t0x134\n\n \n#define L3_EVENT_READ_REQ\t\t0xD\n#define L3_EVENT_WRITEBACK_REQ\t\t0xE\n#define L3_EVENT_INV_N_WRITE_REQ\t0xF\n#define L3_EVENT_INV_REQ\t\t0x10\n#define L3_EVENT_EVICT_REQ\t\t0x13\n#define L3_EVENT_INV_N_WRITE_HIT\t0x14\n#define L3_EVENT_INV_HIT\t\t0x15\n#define L3_EVENT_READ_HIT\t\t0x17\n#define L3_EVENT_MAX\t\t\t0x18\n\n \n#define DMC_EVENT_COUNT_CYCLES\t\t0x1\n#define DMC_EVENT_WRITE_TXNS\t\t0xB\n#define DMC_EVENT_DATA_TRANSFERS\t0xD\n#define DMC_EVENT_READ_TXNS\t\t0xF\n#define DMC_EVENT_MAX\t\t\t0x10\n\n#define CCPI2_EVENT_REQ_PKT_SENT\t0x3D\n#define CCPI2_EVENT_SNOOP_PKT_SENT\t0x65\n#define CCPI2_EVENT_DATA_PKT_SENT\t0x105\n#define CCPI2_EVENT_GIC_PKT_SENT\t0x12D\n#define CCPI2_EVENT_MAX\t\t\t0x200\n\n#define CCPI2_PERF_CTL_ENABLE\t\tBIT(0)\n#define CCPI2_PERF_CTL_START\t\tBIT(1)\n#define CCPI2_PERF_CTL_RESET\t\tBIT(4)\n#define CCPI2_EVENT_LEVEL_RISING_EDGE\tBIT(10)\n#define CCPI2_EVENT_TYPE_EDGE_SENSITIVE\tBIT(11)\n\nenum tx2_uncore_type {\n\tPMU_TYPE_L3C,\n\tPMU_TYPE_DMC,\n\tPMU_TYPE_CCPI2,\n\tPMU_TYPE_INVALID,\n};\n\n \nstruct tx2_uncore_pmu {\n\tstruct hlist_node hpnode;\n\tstruct list_head  entry;\n\tstruct pmu pmu;\n\tchar *name;\n\tint node;\n\tint cpu;\n\tu32 max_counters;\n\tu32 counters_mask;\n\tu32 prorate_factor;\n\tu32 max_events;\n\tu32 events_mask;\n\tu64 hrtimer_interval;\n\tvoid __iomem *base;\n\tDECLARE_BITMAP(active_counters, TX2_PMU_MAX_COUNTERS);\n\tstruct perf_event *events[TX2_PMU_MAX_COUNTERS];\n\tstruct device *dev;\n\tstruct hrtimer hrtimer;\n\tconst struct attribute_group **attr_groups;\n\tenum tx2_uncore_type type;\n\tenum hrtimer_restart (*hrtimer_callback)(struct hrtimer *cb);\n\tvoid (*init_cntr_base)(struct perf_event *event,\n\t\t\tstruct tx2_uncore_pmu *tx2_pmu);\n\tvoid (*stop_event)(struct perf_event *event);\n\tvoid (*start_event)(struct perf_event *event, int flags);\n};\n\nstatic LIST_HEAD(tx2_pmus);\n\nstatic inline struct tx2_uncore_pmu *pmu_to_tx2_pmu(struct pmu *pmu)\n{\n\treturn container_of(pmu, struct tx2_uncore_pmu, pmu);\n}\n\n#define TX2_PMU_FORMAT_ATTR(_var, _name, _format)\t\t\t\\\nstatic ssize_t\t\t\t\t\t\t\t\t\\\n__tx2_pmu_##_var##_show(struct device *dev,\t\t\t\t\\\n\t\t\t       struct device_attribute *attr,\t\t\\\n\t\t\t       char *page)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);\t\t\t\\\n\treturn sysfs_emit(page, _format \"\\n\");\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute format_attr_##_var =\t\t\t\\\n\t__ATTR(_name, 0444, __tx2_pmu_##_var##_show, NULL)\n\nTX2_PMU_FORMAT_ATTR(event, event, \"config:0-4\");\nTX2_PMU_FORMAT_ATTR(event_ccpi2, event, \"config:0-9\");\n\nstatic struct attribute *l3c_pmu_format_attrs[] = {\n\t&format_attr_event.attr,\n\tNULL,\n};\n\nstatic struct attribute *dmc_pmu_format_attrs[] = {\n\t&format_attr_event.attr,\n\tNULL,\n};\n\nstatic struct attribute *ccpi2_pmu_format_attrs[] = {\n\t&format_attr_event_ccpi2.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group l3c_pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = l3c_pmu_format_attrs,\n};\n\nstatic const struct attribute_group dmc_pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = dmc_pmu_format_attrs,\n};\n\nstatic const struct attribute_group ccpi2_pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = ccpi2_pmu_format_attrs,\n};\n\n \nstatic ssize_t tx2_pmu_event_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"event=0x%lx\\n\", (unsigned long) eattr->var);\n}\n\n#define TX2_EVENT_ATTR(name, config) \\\n\tPMU_EVENT_ATTR(name, tx2_pmu_event_attr_##name, \\\n\t\t\tconfig, tx2_pmu_event_show)\n\nTX2_EVENT_ATTR(read_request, L3_EVENT_READ_REQ);\nTX2_EVENT_ATTR(writeback_request, L3_EVENT_WRITEBACK_REQ);\nTX2_EVENT_ATTR(inv_nwrite_request, L3_EVENT_INV_N_WRITE_REQ);\nTX2_EVENT_ATTR(inv_request, L3_EVENT_INV_REQ);\nTX2_EVENT_ATTR(evict_request, L3_EVENT_EVICT_REQ);\nTX2_EVENT_ATTR(inv_nwrite_hit, L3_EVENT_INV_N_WRITE_HIT);\nTX2_EVENT_ATTR(inv_hit, L3_EVENT_INV_HIT);\nTX2_EVENT_ATTR(read_hit, L3_EVENT_READ_HIT);\n\nstatic struct attribute *l3c_pmu_events_attrs[] = {\n\t&tx2_pmu_event_attr_read_request.attr.attr,\n\t&tx2_pmu_event_attr_writeback_request.attr.attr,\n\t&tx2_pmu_event_attr_inv_nwrite_request.attr.attr,\n\t&tx2_pmu_event_attr_inv_request.attr.attr,\n\t&tx2_pmu_event_attr_evict_request.attr.attr,\n\t&tx2_pmu_event_attr_inv_nwrite_hit.attr.attr,\n\t&tx2_pmu_event_attr_inv_hit.attr.attr,\n\t&tx2_pmu_event_attr_read_hit.attr.attr,\n\tNULL,\n};\n\nTX2_EVENT_ATTR(cnt_cycles, DMC_EVENT_COUNT_CYCLES);\nTX2_EVENT_ATTR(write_txns, DMC_EVENT_WRITE_TXNS);\nTX2_EVENT_ATTR(data_transfers, DMC_EVENT_DATA_TRANSFERS);\nTX2_EVENT_ATTR(read_txns, DMC_EVENT_READ_TXNS);\n\nstatic struct attribute *dmc_pmu_events_attrs[] = {\n\t&tx2_pmu_event_attr_cnt_cycles.attr.attr,\n\t&tx2_pmu_event_attr_write_txns.attr.attr,\n\t&tx2_pmu_event_attr_data_transfers.attr.attr,\n\t&tx2_pmu_event_attr_read_txns.attr.attr,\n\tNULL,\n};\n\nTX2_EVENT_ATTR(req_pktsent, CCPI2_EVENT_REQ_PKT_SENT);\nTX2_EVENT_ATTR(snoop_pktsent, CCPI2_EVENT_SNOOP_PKT_SENT);\nTX2_EVENT_ATTR(data_pktsent, CCPI2_EVENT_DATA_PKT_SENT);\nTX2_EVENT_ATTR(gic_pktsent, CCPI2_EVENT_GIC_PKT_SENT);\n\nstatic struct attribute *ccpi2_pmu_events_attrs[] = {\n\t&tx2_pmu_event_attr_req_pktsent.attr.attr,\n\t&tx2_pmu_event_attr_snoop_pktsent.attr.attr,\n\t&tx2_pmu_event_attr_data_pktsent.attr.attr,\n\t&tx2_pmu_event_attr_gic_pktsent.attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group l3c_pmu_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = l3c_pmu_events_attrs,\n};\n\nstatic const struct attribute_group dmc_pmu_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = dmc_pmu_events_attrs,\n};\n\nstatic const struct attribute_group ccpi2_pmu_events_attr_group = {\n\t.name = \"events\",\n\t.attrs = ccpi2_pmu_events_attrs,\n};\n\n \nstatic ssize_t cpumask_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\ttx2_pmu = pmu_to_tx2_pmu(dev_get_drvdata(dev));\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(tx2_pmu->cpu));\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *tx2_pmu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group pmu_cpumask_attr_group = {\n\t.attrs = tx2_pmu_cpumask_attrs,\n};\n\n \nstatic const struct attribute_group *l3c_pmu_attr_groups[] = {\n\t&l3c_pmu_format_attr_group,\n\t&pmu_cpumask_attr_group,\n\t&l3c_pmu_events_attr_group,\n\tNULL\n};\n\nstatic const struct attribute_group *dmc_pmu_attr_groups[] = {\n\t&dmc_pmu_format_attr_group,\n\t&pmu_cpumask_attr_group,\n\t&dmc_pmu_events_attr_group,\n\tNULL\n};\n\nstatic const struct attribute_group *ccpi2_pmu_attr_groups[] = {\n\t&ccpi2_pmu_format_attr_group,\n\t&pmu_cpumask_attr_group,\n\t&ccpi2_pmu_events_attr_group,\n\tNULL\n};\n\nstatic inline u32 reg_readl(unsigned long addr)\n{\n\treturn readl((void __iomem *)addr);\n}\n\nstatic inline void reg_writel(u32 val, unsigned long addr)\n{\n\twritel(val, (void __iomem *)addr);\n}\n\nstatic int alloc_counter(struct tx2_uncore_pmu *tx2_pmu)\n{\n\tint counter;\n\n\tcounter = find_first_zero_bit(tx2_pmu->active_counters,\n\t\t\t\ttx2_pmu->max_counters);\n\tif (counter == tx2_pmu->max_counters)\n\t\treturn -ENOSPC;\n\n\tset_bit(counter, tx2_pmu->active_counters);\n\treturn counter;\n}\n\nstatic inline void free_counter(struct tx2_uncore_pmu *tx2_pmu, int counter)\n{\n\tclear_bit(counter, tx2_pmu->active_counters);\n}\n\nstatic void init_cntr_base_l3c(struct perf_event *event,\n\t\tstruct tx2_uncore_pmu *tx2_pmu)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu32 cmask;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tcmask = tx2_pmu->counters_mask;\n\n\t \n\thwc->config_base = (unsigned long)tx2_pmu->base\n\t\t+ L3C_COUNTER_CTL + (8 * GET_COUNTERID(event, cmask));\n\thwc->event_base =  (unsigned long)tx2_pmu->base\n\t\t+ L3C_COUNTER_DATA + (8 * GET_COUNTERID(event, cmask));\n}\n\nstatic void init_cntr_base_dmc(struct perf_event *event,\n\t\tstruct tx2_uncore_pmu *tx2_pmu)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu32 cmask;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tcmask = tx2_pmu->counters_mask;\n\n\thwc->config_base = (unsigned long)tx2_pmu->base\n\t\t+ DMC_COUNTER_CTL;\n\t \n\thwc->event_base = (unsigned long)tx2_pmu->base\n\t\t+ DMC_COUNTER_DATA + (0xc * GET_COUNTERID(event, cmask));\n}\n\nstatic void init_cntr_base_ccpi2(struct perf_event *event,\n\t\tstruct tx2_uncore_pmu *tx2_pmu)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu32 cmask;\n\n\tcmask = tx2_pmu->counters_mask;\n\n\thwc->config_base = (unsigned long)tx2_pmu->base\n\t\t+ CCPI2_COUNTER_CTL + (4 * GET_COUNTERID(event, cmask));\n\thwc->event_base =  (unsigned long)tx2_pmu->base;\n}\n\nstatic void uncore_start_event_l3c(struct perf_event *event, int flags)\n{\n\tu32 val, emask;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\temask = tx2_pmu->events_mask;\n\n\t \n\tval = GET_EVENTID(event, emask) << 3;\n\treg_writel(val, hwc->config_base);\n\tlocal64_set(&hwc->prev_count, 0);\n\treg_writel(0, hwc->event_base);\n}\n\nstatic inline void uncore_stop_event_l3c(struct perf_event *event)\n{\n\treg_writel(0, event->hw.config_base);\n}\n\nstatic void uncore_start_event_dmc(struct perf_event *event, int flags)\n{\n\tu32 val, cmask, emask;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tint idx, event_id;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tcmask = tx2_pmu->counters_mask;\n\temask = tx2_pmu->events_mask;\n\n\tidx = GET_COUNTERID(event, cmask);\n\tevent_id = GET_EVENTID(event, emask);\n\n\t \n\tval = reg_readl(hwc->config_base);\n\tval &= ~DMC_EVENT_CFG(idx, 0x1f);\n\tval |= DMC_EVENT_CFG(idx, event_id);\n\treg_writel(val, hwc->config_base);\n\tlocal64_set(&hwc->prev_count, 0);\n\treg_writel(0, hwc->event_base);\n}\n\nstatic void uncore_stop_event_dmc(struct perf_event *event)\n{\n\tu32 val, cmask;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tint idx;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tcmask = tx2_pmu->counters_mask;\n\tidx = GET_COUNTERID(event, cmask);\n\n\t \n\tval = reg_readl(hwc->config_base);\n\tval &= ~DMC_EVENT_CFG(idx, 0x1f);\n\treg_writel(val, hwc->config_base);\n}\n\nstatic void uncore_start_event_ccpi2(struct perf_event *event, int flags)\n{\n\tu32 emask;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\temask = tx2_pmu->events_mask;\n\n\t \n\treg_writel((CCPI2_EVENT_TYPE_EDGE_SENSITIVE |\n\t\t\tCCPI2_EVENT_LEVEL_RISING_EDGE |\n\t\t\tGET_EVENTID(event, emask)), hwc->config_base);\n\n\t \n\treg_writel(CCPI2_PERF_CTL_RESET |\n\t\t\tCCPI2_PERF_CTL_START |\n\t\t\tCCPI2_PERF_CTL_ENABLE,\n\t\t\thwc->event_base + CCPI2_PERF_CTL);\n\tlocal64_set(&event->hw.prev_count, 0ULL);\n}\n\nstatic void uncore_stop_event_ccpi2(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\treg_writel(0, hwc->event_base + CCPI2_PERF_CTL);\n}\n\nstatic void tx2_uncore_event_update(struct perf_event *event)\n{\n\tu64 prev, delta, new = 0;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tenum tx2_uncore_type type;\n\tu32 prorate_factor;\n\tu32 cmask, emask;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\ttype = tx2_pmu->type;\n\tcmask = tx2_pmu->counters_mask;\n\temask = tx2_pmu->events_mask;\n\tprorate_factor = tx2_pmu->prorate_factor;\n\tif (type == PMU_TYPE_CCPI2) {\n\t\treg_writel(CCPI2_COUNTER_OFFSET +\n\t\t\t\tGET_COUNTERID(event, cmask),\n\t\t\t\thwc->event_base + CCPI2_COUNTER_SEL);\n\t\tnew = reg_readl(hwc->event_base + CCPI2_COUNTER_DATA_H);\n\t\tnew = (new << 32) +\n\t\t\treg_readl(hwc->event_base + CCPI2_COUNTER_DATA_L);\n\t\tprev = local64_xchg(&hwc->prev_count, new);\n\t\tdelta = new - prev;\n\t} else {\n\t\tnew = reg_readl(hwc->event_base);\n\t\tprev = local64_xchg(&hwc->prev_count, new);\n\t\t \n\t\tdelta = (u32)(((1ULL << 32) - prev) + new);\n\t}\n\n\t \n\tif (type == PMU_TYPE_DMC &&\n\t\t\tGET_EVENTID(event, emask) == DMC_EVENT_DATA_TRANSFERS)\n\t\tdelta = delta/4;\n\n\t \n\tlocal64_add(delta * prorate_factor, &event->count);\n}\n\nstatic enum tx2_uncore_type get_tx2_pmu_type(struct acpi_device *adev)\n{\n\tint i = 0;\n\tstruct acpi_tx2_pmu_device {\n\t\t__u8 id[ACPI_ID_LEN];\n\t\tenum tx2_uncore_type type;\n\t} devices[] = {\n\t\t{\"CAV901D\", PMU_TYPE_L3C},\n\t\t{\"CAV901F\", PMU_TYPE_DMC},\n\t\t{\"CAV901E\", PMU_TYPE_CCPI2},\n\t\t{\"\", PMU_TYPE_INVALID}\n\t};\n\n\twhile (devices[i].type != PMU_TYPE_INVALID) {\n\t\tif (!strcmp(acpi_device_hid(adev), devices[i].id))\n\t\t\tbreak;\n\t\ti++;\n\t}\n\n\treturn devices[i].type;\n}\n\nstatic bool tx2_uncore_validate_event(struct pmu *pmu,\n\t\t\t\t  struct perf_event *event, int *counters)\n{\n\tif (is_software_event(event))\n\t\treturn true;\n\t \n\tif (event->pmu != pmu)\n\t\treturn false;\n\n\t*counters = *counters + 1;\n\treturn true;\n}\n\n \nstatic bool tx2_uncore_validate_event_group(struct perf_event *event,\n\t\tint max_counters)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tint counters = 0;\n\n\tif (event->group_leader == event)\n\t\treturn true;\n\n\tif (!tx2_uncore_validate_event(event->pmu, leader, &counters))\n\t\treturn false;\n\n\tfor_each_sibling_event(sibling, leader) {\n\t\tif (!tx2_uncore_validate_event(event->pmu, sibling, &counters))\n\t\t\treturn false;\n\t}\n\n\tif (!tx2_uncore_validate_event(event->pmu, event, &counters))\n\t\treturn false;\n\n\t \n\treturn counters <= max_counters;\n}\n\n\nstatic int tx2_uncore_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EINVAL;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tif (tx2_pmu->cpu >= nr_cpu_ids)\n\t\treturn -EINVAL;\n\tevent->cpu = tx2_pmu->cpu;\n\n\tif (event->attr.config >= tx2_pmu->max_events)\n\t\treturn -EINVAL;\n\n\t \n\thwc->config = event->attr.config;\n\n\t \n\tif (!tx2_uncore_validate_event_group(event, tx2_pmu->max_counters))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void tx2_uncore_event_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\thwc->state = 0;\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\n\ttx2_pmu->start_event(event, flags);\n\tperf_event_update_userpage(event);\n\n\t \n\tif (!tx2_pmu->hrtimer_callback)\n\t\treturn;\n\n\t \n\tif (bitmap_weight(tx2_pmu->active_counters,\n\t\t\t\ttx2_pmu->max_counters) == 1) {\n\t\thrtimer_start(&tx2_pmu->hrtimer,\n\t\t\tns_to_ktime(tx2_pmu->hrtimer_interval),\n\t\t\tHRTIMER_MODE_REL_PINNED);\n\t}\n}\n\nstatic void tx2_uncore_event_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\ttx2_pmu->stop_event(event);\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\thwc->state |= PERF_HES_STOPPED;\n\tif (flags & PERF_EF_UPDATE) {\n\t\ttx2_uncore_event_update(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic int tx2_uncore_event_add(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\ttx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\n\t \n\thwc->idx  = alloc_counter(tx2_pmu);\n\tif (hwc->idx < 0)\n\t\treturn -EAGAIN;\n\n\ttx2_pmu->events[hwc->idx] = event;\n\t \n\ttx2_pmu->init_cntr_base(event, tx2_pmu);\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (flags & PERF_EF_START)\n\t\ttx2_uncore_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void tx2_uncore_event_del(struct perf_event *event, int flags)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu = pmu_to_tx2_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu32 cmask;\n\n\tcmask = tx2_pmu->counters_mask;\n\ttx2_uncore_event_stop(event, PERF_EF_UPDATE);\n\n\t \n\tfree_counter(tx2_pmu, GET_COUNTERID(event, cmask));\n\n\tperf_event_update_userpage(event);\n\ttx2_pmu->events[hwc->idx] = NULL;\n\thwc->idx = -1;\n\n\tif (!tx2_pmu->hrtimer_callback)\n\t\treturn;\n\n\tif (bitmap_empty(tx2_pmu->active_counters, tx2_pmu->max_counters))\n\t\thrtimer_cancel(&tx2_pmu->hrtimer);\n}\n\nstatic void tx2_uncore_event_read(struct perf_event *event)\n{\n\ttx2_uncore_event_update(event);\n}\n\nstatic enum hrtimer_restart tx2_hrtimer_callback(struct hrtimer *timer)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tint max_counters, idx;\n\n\ttx2_pmu = container_of(timer, struct tx2_uncore_pmu, hrtimer);\n\tmax_counters = tx2_pmu->max_counters;\n\n\tif (bitmap_empty(tx2_pmu->active_counters, max_counters))\n\t\treturn HRTIMER_NORESTART;\n\n\tfor_each_set_bit(idx, tx2_pmu->active_counters, max_counters) {\n\t\tstruct perf_event *event = tx2_pmu->events[idx];\n\n\t\ttx2_uncore_event_update(event);\n\t}\n\thrtimer_forward_now(timer, ns_to_ktime(tx2_pmu->hrtimer_interval));\n\treturn HRTIMER_RESTART;\n}\n\nstatic int tx2_uncore_pmu_register(\n\t\tstruct tx2_uncore_pmu *tx2_pmu)\n{\n\tstruct device *dev = tx2_pmu->dev;\n\tchar *name = tx2_pmu->name;\n\n\t \n\ttx2_pmu->pmu = (struct pmu) {\n\t\t.module         = THIS_MODULE,\n\t\t.attr_groups\t= tx2_pmu->attr_groups,\n\t\t.task_ctx_nr\t= perf_invalid_context,\n\t\t.event_init\t= tx2_uncore_event_init,\n\t\t.add\t\t= tx2_uncore_event_add,\n\t\t.del\t\t= tx2_uncore_event_del,\n\t\t.start\t\t= tx2_uncore_event_start,\n\t\t.stop\t\t= tx2_uncore_event_stop,\n\t\t.read\t\t= tx2_uncore_event_read,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\ttx2_pmu->pmu.name = devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t\"%s\", name);\n\n\treturn perf_pmu_register(&tx2_pmu->pmu, tx2_pmu->pmu.name, -1);\n}\n\nstatic int tx2_uncore_pmu_add_dev(struct tx2_uncore_pmu *tx2_pmu)\n{\n\tint ret, cpu;\n\n\tcpu = cpumask_any_and(cpumask_of_node(tx2_pmu->node),\n\t\t\tcpu_online_mask);\n\n\ttx2_pmu->cpu = cpu;\n\n\tif (tx2_pmu->hrtimer_callback) {\n\t\thrtimer_init(&tx2_pmu->hrtimer,\n\t\t\t\tCLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\t\ttx2_pmu->hrtimer.function = tx2_pmu->hrtimer_callback;\n\t}\n\n\tret = tx2_uncore_pmu_register(tx2_pmu);\n\tif (ret) {\n\t\tdev_err(tx2_pmu->dev, \"%s PMU: Failed to init driver\\n\",\n\t\t\t\ttx2_pmu->name);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tret = cpuhp_state_add_instance(\n\t\t\tCPUHP_AP_PERF_ARM_CAVIUM_TX2_UNCORE_ONLINE,\n\t\t\t&tx2_pmu->hpnode);\n\tif (ret) {\n\t\tdev_err(tx2_pmu->dev, \"Error %d registering hotplug\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tlist_add(&tx2_pmu->entry, &tx2_pmus);\n\n\tdev_dbg(tx2_pmu->dev, \"%s PMU UNCORE registered\\n\",\n\t\t\ttx2_pmu->pmu.name);\n\treturn ret;\n}\n\nstatic struct tx2_uncore_pmu *tx2_uncore_pmu_init_dev(struct device *dev,\n\t\tacpi_handle handle, struct acpi_device *adev, u32 type)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tvoid __iomem *base;\n\tstruct resource res;\n\tstruct resource_entry *rentry;\n\tstruct list_head list;\n\tint ret;\n\n\tINIT_LIST_HEAD(&list);\n\tret = acpi_dev_get_resources(adev, &list, NULL, NULL);\n\tif (ret <= 0) {\n\t\tdev_err(dev, \"failed to parse _CRS method, error %d\\n\", ret);\n\t\treturn NULL;\n\t}\n\n\tlist_for_each_entry(rentry, &list, node) {\n\t\tif (resource_type(rentry->res) == IORESOURCE_MEM) {\n\t\t\tres = *rentry->res;\n\t\t\trentry = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\tacpi_dev_free_resource_list(&list);\n\n\tif (rentry) {\n\t\tdev_err(dev, \"PMU type %d: Fail to find resource\\n\", type);\n\t\treturn NULL;\n\t}\n\n\tbase = devm_ioremap_resource(dev, &res);\n\tif (IS_ERR(base))\n\t\treturn NULL;\n\n\ttx2_pmu = devm_kzalloc(dev, sizeof(*tx2_pmu), GFP_KERNEL);\n\tif (!tx2_pmu)\n\t\treturn NULL;\n\n\ttx2_pmu->dev = dev;\n\ttx2_pmu->type = type;\n\ttx2_pmu->base = base;\n\ttx2_pmu->node = dev_to_node(dev);\n\tINIT_LIST_HEAD(&tx2_pmu->entry);\n\n\tswitch (tx2_pmu->type) {\n\tcase PMU_TYPE_L3C:\n\t\ttx2_pmu->max_counters = TX2_PMU_DMC_L3C_MAX_COUNTERS;\n\t\ttx2_pmu->counters_mask = 0x3;\n\t\ttx2_pmu->prorate_factor = TX2_PMU_L3_TILES;\n\t\ttx2_pmu->max_events = L3_EVENT_MAX;\n\t\ttx2_pmu->events_mask = 0x1f;\n\t\ttx2_pmu->hrtimer_interval = TX2_PMU_HRTIMER_INTERVAL;\n\t\ttx2_pmu->hrtimer_callback = tx2_hrtimer_callback;\n\t\ttx2_pmu->attr_groups = l3c_pmu_attr_groups;\n\t\ttx2_pmu->name = devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t\t\"uncore_l3c_%d\", tx2_pmu->node);\n\t\ttx2_pmu->init_cntr_base = init_cntr_base_l3c;\n\t\ttx2_pmu->start_event = uncore_start_event_l3c;\n\t\ttx2_pmu->stop_event = uncore_stop_event_l3c;\n\t\tbreak;\n\tcase PMU_TYPE_DMC:\n\t\ttx2_pmu->max_counters = TX2_PMU_DMC_L3C_MAX_COUNTERS;\n\t\ttx2_pmu->counters_mask = 0x3;\n\t\ttx2_pmu->prorate_factor = TX2_PMU_DMC_CHANNELS;\n\t\ttx2_pmu->max_events = DMC_EVENT_MAX;\n\t\ttx2_pmu->events_mask = 0x1f;\n\t\ttx2_pmu->hrtimer_interval = TX2_PMU_HRTIMER_INTERVAL;\n\t\ttx2_pmu->hrtimer_callback = tx2_hrtimer_callback;\n\t\ttx2_pmu->attr_groups = dmc_pmu_attr_groups;\n\t\ttx2_pmu->name = devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t\t\"uncore_dmc_%d\", tx2_pmu->node);\n\t\ttx2_pmu->init_cntr_base = init_cntr_base_dmc;\n\t\ttx2_pmu->start_event = uncore_start_event_dmc;\n\t\ttx2_pmu->stop_event = uncore_stop_event_dmc;\n\t\tbreak;\n\tcase PMU_TYPE_CCPI2:\n\t\t \n\t\ttx2_pmu->max_counters = TX2_PMU_CCPI2_MAX_COUNTERS;\n\t\ttx2_pmu->counters_mask = 0x7;\n\t\ttx2_pmu->prorate_factor = 1;\n\t\ttx2_pmu->max_events = CCPI2_EVENT_MAX;\n\t\ttx2_pmu->events_mask = 0x1ff;\n\t\ttx2_pmu->attr_groups = ccpi2_pmu_attr_groups;\n\t\ttx2_pmu->name = devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t\t\"uncore_ccpi2_%d\", tx2_pmu->node);\n\t\ttx2_pmu->init_cntr_base = init_cntr_base_ccpi2;\n\t\ttx2_pmu->start_event = uncore_start_event_ccpi2;\n\t\ttx2_pmu->stop_event = uncore_stop_event_ccpi2;\n\t\ttx2_pmu->hrtimer_callback = NULL;\n\t\tbreak;\n\tcase PMU_TYPE_INVALID:\n\t\tdevm_kfree(dev, tx2_pmu);\n\t\treturn NULL;\n\t}\n\n\treturn tx2_pmu;\n}\n\nstatic acpi_status tx2_uncore_pmu_add(acpi_handle handle, u32 level,\n\t\t\t\t    void *data, void **return_value)\n{\n\tstruct acpi_device *adev = acpi_fetch_acpi_dev(handle);\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tenum tx2_uncore_type type;\n\n\tif (!adev || acpi_bus_get_status(adev) || !adev->status.present)\n\t\treturn AE_OK;\n\n\ttype = get_tx2_pmu_type(adev);\n\tif (type == PMU_TYPE_INVALID)\n\t\treturn AE_OK;\n\n\ttx2_pmu = tx2_uncore_pmu_init_dev((struct device *)data,\n\t\t\thandle, adev, type);\n\n\tif (!tx2_pmu)\n\t\treturn AE_ERROR;\n\n\tif (tx2_uncore_pmu_add_dev(tx2_pmu)) {\n\t\t \n\t\treturn AE_ERROR;\n\t}\n\treturn AE_OK;\n}\n\nstatic int tx2_uncore_pmu_online_cpu(unsigned int cpu,\n\t\tstruct hlist_node *hpnode)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\n\ttx2_pmu = hlist_entry_safe(hpnode,\n\t\t\tstruct tx2_uncore_pmu, hpnode);\n\n\t \n\tif ((tx2_pmu->cpu >= nr_cpu_ids) &&\n\t\t(tx2_pmu->node == cpu_to_node(cpu)))\n\t\ttx2_pmu->cpu = cpu;\n\n\treturn 0;\n}\n\nstatic int tx2_uncore_pmu_offline_cpu(unsigned int cpu,\n\t\tstruct hlist_node *hpnode)\n{\n\tint new_cpu;\n\tstruct tx2_uncore_pmu *tx2_pmu;\n\tstruct cpumask cpu_online_mask_temp;\n\n\ttx2_pmu = hlist_entry_safe(hpnode,\n\t\t\tstruct tx2_uncore_pmu, hpnode);\n\n\tif (cpu != tx2_pmu->cpu)\n\t\treturn 0;\n\n\tif (tx2_pmu->hrtimer_callback)\n\t\thrtimer_cancel(&tx2_pmu->hrtimer);\n\n\tcpumask_copy(&cpu_online_mask_temp, cpu_online_mask);\n\tcpumask_clear_cpu(cpu, &cpu_online_mask_temp);\n\tnew_cpu = cpumask_any_and(\n\t\t\tcpumask_of_node(tx2_pmu->node),\n\t\t\t&cpu_online_mask_temp);\n\n\ttx2_pmu->cpu = new_cpu;\n\tif (new_cpu >= nr_cpu_ids)\n\t\treturn 0;\n\tperf_pmu_migrate_context(&tx2_pmu->pmu, cpu, new_cpu);\n\n\treturn 0;\n}\n\nstatic const struct acpi_device_id tx2_uncore_acpi_match[] = {\n\t{\"CAV901C\", 0},\n\t{},\n};\nMODULE_DEVICE_TABLE(acpi, tx2_uncore_acpi_match);\n\nstatic int tx2_uncore_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tacpi_handle handle;\n\tacpi_status status;\n\n\tset_dev_node(dev, acpi_get_node(ACPI_HANDLE(dev)));\n\n\tif (!has_acpi_companion(dev))\n\t\treturn -ENODEV;\n\n\thandle = ACPI_HANDLE(dev);\n\tif (!handle)\n\t\treturn -EINVAL;\n\n\t \n\tstatus = acpi_walk_namespace(ACPI_TYPE_DEVICE, handle, 1,\n\t\t\t\t     tx2_uncore_pmu_add,\n\t\t\t\t     NULL, dev, NULL);\n\tif (ACPI_FAILURE(status)) {\n\t\tdev_err(dev, \"failed to probe PMU devices\\n\");\n\t\treturn_ACPI_STATUS(status);\n\t}\n\n\tdev_info(dev, \"node%d: pmu uncore registered\\n\", dev_to_node(dev));\n\treturn 0;\n}\n\nstatic int tx2_uncore_remove(struct platform_device *pdev)\n{\n\tstruct tx2_uncore_pmu *tx2_pmu, *temp;\n\tstruct device *dev = &pdev->dev;\n\n\tif (!list_empty(&tx2_pmus)) {\n\t\tlist_for_each_entry_safe(tx2_pmu, temp, &tx2_pmus, entry) {\n\t\t\tif (tx2_pmu->node == dev_to_node(dev)) {\n\t\t\t\tcpuhp_state_remove_instance_nocalls(\n\t\t\t\t\tCPUHP_AP_PERF_ARM_CAVIUM_TX2_UNCORE_ONLINE,\n\t\t\t\t\t&tx2_pmu->hpnode);\n\t\t\t\tperf_pmu_unregister(&tx2_pmu->pmu);\n\t\t\t\tlist_del(&tx2_pmu->entry);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic struct platform_driver tx2_uncore_driver = {\n\t.driver = {\n\t\t.name\t\t= \"tx2-uncore-pmu\",\n\t\t.acpi_match_table = ACPI_PTR(tx2_uncore_acpi_match),\n\t\t.suppress_bind_attrs = true,\n\t},\n\t.probe = tx2_uncore_probe,\n\t.remove = tx2_uncore_remove,\n};\n\nstatic int __init tx2_uncore_driver_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_PERF_ARM_CAVIUM_TX2_UNCORE_ONLINE,\n\t\t\t\t      \"perf/tx2/uncore:online\",\n\t\t\t\t      tx2_uncore_pmu_online_cpu,\n\t\t\t\t      tx2_uncore_pmu_offline_cpu);\n\tif (ret) {\n\t\tpr_err(\"TX2 PMU: setup hotplug failed(%d)\\n\", ret);\n\t\treturn ret;\n\t}\n\tret = platform_driver_register(&tx2_uncore_driver);\n\tif (ret)\n\t\tcpuhp_remove_multi_state(CPUHP_AP_PERF_ARM_CAVIUM_TX2_UNCORE_ONLINE);\n\n\treturn ret;\n}\nmodule_init(tx2_uncore_driver_init);\n\nstatic void __exit tx2_uncore_driver_exit(void)\n{\n\tplatform_driver_unregister(&tx2_uncore_driver);\n\tcpuhp_remove_multi_state(CPUHP_AP_PERF_ARM_CAVIUM_TX2_UNCORE_ONLINE);\n}\nmodule_exit(tx2_uncore_driver_exit);\n\nMODULE_DESCRIPTION(\"ThunderX2 UNCORE PMU driver\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Ganapatrao Kulkarni <gkulkarni@cavium.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}