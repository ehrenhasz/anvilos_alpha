{
  "module_name": "arm-cci.c",
  "hash_id": "d311ef5bd433bf86a78ee4176106a03ec1fcf46ff9634031a0e81d3e311856dc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/perf/arm-cci.c",
  "human_readable_source": "\n\n\n\n\n#include <linux/arm-cci.h>\n#include <linux/io.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/perf_event.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n\n#define DRIVER_NAME\t\t\"ARM-CCI PMU\"\n\n#define CCI_PMCR\t\t0x0100\n#define CCI_PID2\t\t0x0fe8\n\n#define CCI_PMCR_CEN\t\t0x00000001\n#define CCI_PMCR_NCNT_MASK\t0x0000f800\n#define CCI_PMCR_NCNT_SHIFT\t11\n\n#define CCI_PID2_REV_MASK\t0xf0\n#define CCI_PID2_REV_SHIFT\t4\n\n#define CCI_PMU_EVT_SEL\t\t0x000\n#define CCI_PMU_CNTR\t\t0x004\n#define CCI_PMU_CNTR_CTRL\t0x008\n#define CCI_PMU_OVRFLW\t\t0x00c\n\n#define CCI_PMU_OVRFLW_FLAG\t1\n\n#define CCI_PMU_CNTR_SIZE(model)\t((model)->cntr_size)\n#define CCI_PMU_CNTR_BASE(model, idx)\t((idx) * CCI_PMU_CNTR_SIZE(model))\n#define CCI_PMU_CNTR_MASK\t\t((1ULL << 32) - 1)\n#define CCI_PMU_CNTR_LAST(cci_pmu)\t(cci_pmu->num_cntrs - 1)\n\n#define CCI_PMU_MAX_HW_CNTRS(model) \\\n\t((model)->num_hw_cntrs + (model)->fixed_hw_cntrs)\n\n \nenum {\n\tCCI_IF_SLAVE,\n\tCCI_IF_MASTER,\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\tCCI_IF_GLOBAL,\n#endif\n\tCCI_IF_MAX,\n};\n\n#define NUM_HW_CNTRS_CII_4XX\t4\n#define NUM_HW_CNTRS_CII_5XX\t8\n#define NUM_HW_CNTRS_MAX\tNUM_HW_CNTRS_CII_5XX\n\n#define FIXED_HW_CNTRS_CII_4XX\t1\n#define FIXED_HW_CNTRS_CII_5XX\t0\n#define FIXED_HW_CNTRS_MAX\tFIXED_HW_CNTRS_CII_4XX\n\n#define HW_CNTRS_MAX\t\t(NUM_HW_CNTRS_MAX + FIXED_HW_CNTRS_MAX)\n\nstruct event_range {\n\tu32 min;\n\tu32 max;\n};\n\nstruct cci_pmu_hw_events {\n\tstruct perf_event **events;\n\tunsigned long *used_mask;\n\traw_spinlock_t pmu_lock;\n};\n\nstruct cci_pmu;\n \nstruct cci_pmu_model {\n\tchar *name;\n\tu32 fixed_hw_cntrs;\n\tu32 num_hw_cntrs;\n\tu32 cntr_size;\n\tstruct attribute **format_attrs;\n\tstruct attribute **event_attrs;\n\tstruct event_range event_ranges[CCI_IF_MAX];\n\tint (*validate_hw_event)(struct cci_pmu *, unsigned long);\n\tint (*get_event_idx)(struct cci_pmu *, struct cci_pmu_hw_events *, unsigned long);\n\tvoid (*write_counters)(struct cci_pmu *, unsigned long *);\n};\n\nstatic struct cci_pmu_model cci_pmu_models[];\n\nstruct cci_pmu {\n\tvoid __iomem *base;\n\tvoid __iomem *ctrl_base;\n\tstruct pmu pmu;\n\tint cpu;\n\tint nr_irqs;\n\tint *irqs;\n\tunsigned long active_irqs;\n\tconst struct cci_pmu_model *model;\n\tstruct cci_pmu_hw_events hw_events;\n\tstruct platform_device *plat_device;\n\tint num_cntrs;\n\tatomic_t active_events;\n\tstruct mutex reserve_mutex;\n};\n\n#define to_cci_pmu(c)\t(container_of(c, struct cci_pmu, pmu))\n\nstatic struct cci_pmu *g_cci_pmu;\n\nenum cci_models {\n#ifdef CONFIG_ARM_CCI400_PMU\n\tCCI400_R0,\n\tCCI400_R1,\n#endif\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\tCCI500_R0,\n\tCCI550_R0,\n#endif\n\tCCI_MODEL_MAX\n};\n\nstatic void pmu_write_counters(struct cci_pmu *cci_pmu,\n\t\t\t\t unsigned long *mask);\nstatic ssize_t __maybe_unused cci_pmu_format_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf);\nstatic ssize_t __maybe_unused cci_pmu_event_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf);\n\n#define CCI_EXT_ATTR_ENTRY(_name, _func, _config) \t\t\t\t\\\n\t&((struct dev_ext_attribute[]) {\t\t\t\t\t\\\n\t\t{ __ATTR(_name, S_IRUGO, _func, NULL), (void *)_config }\t\\\n\t})[0].attr.attr\n\n#define CCI_FORMAT_EXT_ATTR_ENTRY(_name, _config) \\\n\tCCI_EXT_ATTR_ENTRY(_name, cci_pmu_format_show, (char *)_config)\n#define CCI_EVENT_EXT_ATTR_ENTRY(_name, _config) \\\n\tCCI_EXT_ATTR_ENTRY(_name, cci_pmu_event_show, (unsigned long)_config)\n\n \n\n#ifdef CONFIG_ARM_CCI400_PMU\n\n \n#define CCI400_PORT_S0\t\t0\n#define CCI400_PORT_S1\t\t1\n#define CCI400_PORT_S2\t\t2\n#define CCI400_PORT_S3\t\t3\n#define CCI400_PORT_S4\t\t4\n#define CCI400_PORT_M0\t\t5\n#define CCI400_PORT_M1\t\t6\n#define CCI400_PORT_M2\t\t7\n\n#define CCI400_R1_PX\t\t5\n\n \nenum cci400_perf_events {\n\tCCI400_PMU_CYCLES = 0xff\n};\n\n#define CCI400_PMU_CYCLE_CNTR_IDX\t0\n#define CCI400_PMU_CNTR0_IDX\t\t1\n\n \n\n#define CCI400_PMU_EVENT_MASK\t\t0xffUL\n#define CCI400_PMU_EVENT_SOURCE_SHIFT\t5\n#define CCI400_PMU_EVENT_SOURCE_MASK\t0x7\n#define CCI400_PMU_EVENT_CODE_SHIFT\t0\n#define CCI400_PMU_EVENT_CODE_MASK\t0x1f\n#define CCI400_PMU_EVENT_SOURCE(event) \\\n\t((event >> CCI400_PMU_EVENT_SOURCE_SHIFT) & \\\n\t\t\tCCI400_PMU_EVENT_SOURCE_MASK)\n#define CCI400_PMU_EVENT_CODE(event) \\\n\t((event >> CCI400_PMU_EVENT_CODE_SHIFT) & CCI400_PMU_EVENT_CODE_MASK)\n\n#define CCI400_R0_SLAVE_PORT_MIN_EV\t0x00\n#define CCI400_R0_SLAVE_PORT_MAX_EV\t0x13\n#define CCI400_R0_MASTER_PORT_MIN_EV\t0x14\n#define CCI400_R0_MASTER_PORT_MAX_EV\t0x1a\n\n#define CCI400_R1_SLAVE_PORT_MIN_EV\t0x00\n#define CCI400_R1_SLAVE_PORT_MAX_EV\t0x14\n#define CCI400_R1_MASTER_PORT_MIN_EV\t0x00\n#define CCI400_R1_MASTER_PORT_MAX_EV\t0x11\n\n#define CCI400_CYCLE_EVENT_EXT_ATTR_ENTRY(_name, _config) \\\n\tCCI_EXT_ATTR_ENTRY(_name, cci400_pmu_cycle_event_show, \\\n\t\t\t\t\t(unsigned long)_config)\n\nstatic ssize_t cci400_pmu_cycle_event_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf);\n\nstatic struct attribute *cci400_pmu_format_attrs[] = {\n\tCCI_FORMAT_EXT_ATTR_ENTRY(event, \"config:0-4\"),\n\tCCI_FORMAT_EXT_ATTR_ENTRY(source, \"config:5-7\"),\n\tNULL\n};\n\nstatic struct attribute *cci400_r0_pmu_event_attrs[] = {\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_any, 0x0),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_device, 0x01),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_normal_or_nonshareable, 0x2),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_inner_or_outershareable, 0x3),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_cache_maintenance, 0x4),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_mem_barrier, 0x5),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_sync_barrier, 0x6),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_dvm_msg, 0x7),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_dvm_msg_sync, 0x8),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_stall_tt_full, 0x9),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_data_last_hs_snoop, 0xA),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_data_stall_rvalids_h_rready_l, 0xB),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_any, 0xC),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_device, 0xD),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_normal_or_nonshareable, 0xE),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_inner_or_outershare_wback_wclean, 0xF),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_write_unique, 0x10),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_write_line_unique, 0x11),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_evict, 0x12),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_stall_tt_full, 0x13),\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_retry_speculative_fetch, 0x14),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_addr_hazard, 0x15),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_id_hazard, 0x16),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_tt_full, 0x17),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_barrier_hazard, 0x18),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_barrier_hazard, 0x19),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_tt_full, 0x1A),\n\t \n\tCCI400_CYCLE_EVENT_EXT_ATTR_ENTRY(cycles, 0xff),\n\tNULL\n};\n\nstatic struct attribute *cci400_r1_pmu_event_attrs[] = {\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_any, 0x0),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_device, 0x01),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_normal_or_nonshareable, 0x2),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_inner_or_outershareable, 0x3),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_cache_maintenance, 0x4),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_mem_barrier, 0x5),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_sync_barrier, 0x6),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_dvm_msg, 0x7),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_dvm_msg_sync, 0x8),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_stall_tt_full, 0x9),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_data_last_hs_snoop, 0xA),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_data_stall_rvalids_h_rready_l, 0xB),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_any, 0xC),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_device, 0xD),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_normal_or_nonshareable, 0xE),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_inner_or_outershare_wback_wclean, 0xF),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_write_unique, 0x10),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_write_line_unique, 0x11),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_evict, 0x12),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_stall_tt_full, 0x13),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_stall_slave_id_hazard, 0x14),\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_retry_speculative_fetch, 0x0),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_stall_cycle_addr_hazard, 0x1),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_master_id_hazard, 0x2),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_hi_prio_rtq_full, 0x3),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_barrier_hazard, 0x4),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_barrier_hazard, 0x5),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_wtq_full, 0x6),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_low_prio_rtq_full, 0x7),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_mid_prio_rtq_full, 0x8),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_qvn_vn0, 0x9),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_qvn_vn1, 0xA),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_qvn_vn2, 0xB),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall_qvn_vn3, 0xC),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_qvn_vn0, 0xD),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_qvn_vn1, 0xE),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_qvn_vn2, 0xF),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall_qvn_vn3, 0x10),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_unique_or_line_unique_addr_hazard, 0x11),\n\t \n\tCCI400_CYCLE_EVENT_EXT_ATTR_ENTRY(cycles, 0xff),\n\tNULL\n};\n\nstatic ssize_t cci400_pmu_cycle_event_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\tstruct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"config=0x%lx\\n\", (unsigned long)eattr->var);\n}\n\nstatic int cci400_get_event_idx(struct cci_pmu *cci_pmu,\n\t\t\t\tstruct cci_pmu_hw_events *hw,\n\t\t\t\tunsigned long cci_event)\n{\n\tint idx;\n\n\t \n\tif (cci_event == CCI400_PMU_CYCLES) {\n\t\tif (test_and_set_bit(CCI400_PMU_CYCLE_CNTR_IDX, hw->used_mask))\n\t\t\treturn -EAGAIN;\n\n\t\treturn CCI400_PMU_CYCLE_CNTR_IDX;\n\t}\n\n\tfor (idx = CCI400_PMU_CNTR0_IDX; idx <= CCI_PMU_CNTR_LAST(cci_pmu); ++idx)\n\t\tif (!test_and_set_bit(idx, hw->used_mask))\n\t\t\treturn idx;\n\n\t \n\treturn -EAGAIN;\n}\n\nstatic int cci400_validate_hw_event(struct cci_pmu *cci_pmu, unsigned long hw_event)\n{\n\tu8 ev_source = CCI400_PMU_EVENT_SOURCE(hw_event);\n\tu8 ev_code = CCI400_PMU_EVENT_CODE(hw_event);\n\tint if_type;\n\n\tif (hw_event & ~CCI400_PMU_EVENT_MASK)\n\t\treturn -ENOENT;\n\n\tif (hw_event == CCI400_PMU_CYCLES)\n\t\treturn hw_event;\n\n\tswitch (ev_source) {\n\tcase CCI400_PORT_S0:\n\tcase CCI400_PORT_S1:\n\tcase CCI400_PORT_S2:\n\tcase CCI400_PORT_S3:\n\tcase CCI400_PORT_S4:\n\t\t \n\t\tif_type = CCI_IF_SLAVE;\n\t\tbreak;\n\tcase CCI400_PORT_M0:\n\tcase CCI400_PORT_M1:\n\tcase CCI400_PORT_M2:\n\t\t \n\t\tif_type = CCI_IF_MASTER;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\n\t\tev_code <= cci_pmu->model->event_ranges[if_type].max)\n\t\treturn hw_event;\n\n\treturn -ENOENT;\n}\n\nstatic int probe_cci400_revision(struct cci_pmu *cci_pmu)\n{\n\tint rev;\n\trev = readl_relaxed(cci_pmu->ctrl_base + CCI_PID2) & CCI_PID2_REV_MASK;\n\trev >>= CCI_PID2_REV_SHIFT;\n\n\tif (rev < CCI400_R1_PX)\n\t\treturn CCI400_R0;\n\telse\n\t\treturn CCI400_R1;\n}\n\nstatic const struct cci_pmu_model *probe_cci_model(struct cci_pmu *cci_pmu)\n{\n\tif (platform_has_secure_cci_access())\n\t\treturn &cci_pmu_models[probe_cci400_revision(cci_pmu)];\n\treturn NULL;\n}\n#else\t \nstatic inline struct cci_pmu_model *probe_cci_model(struct cci_pmu *cci_pmu)\n{\n\treturn NULL;\n}\n#endif\t \n\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\n \n\n \n#define CCI5xx_PORT_S0\t\t\t0x0\n#define CCI5xx_PORT_S1\t\t\t0x1\n#define CCI5xx_PORT_S2\t\t\t0x2\n#define CCI5xx_PORT_S3\t\t\t0x3\n#define CCI5xx_PORT_S4\t\t\t0x4\n#define CCI5xx_PORT_S5\t\t\t0x5\n#define CCI5xx_PORT_S6\t\t\t0x6\n\n#define CCI5xx_PORT_M0\t\t\t0x8\n#define CCI5xx_PORT_M1\t\t\t0x9\n#define CCI5xx_PORT_M2\t\t\t0xa\n#define CCI5xx_PORT_M3\t\t\t0xb\n#define CCI5xx_PORT_M4\t\t\t0xc\n#define CCI5xx_PORT_M5\t\t\t0xd\n#define CCI5xx_PORT_M6\t\t\t0xe\n\n#define CCI5xx_PORT_GLOBAL\t\t0xf\n\n#define CCI5xx_PMU_EVENT_MASK\t\t0x1ffUL\n#define CCI5xx_PMU_EVENT_SOURCE_SHIFT\t0x5\n#define CCI5xx_PMU_EVENT_SOURCE_MASK\t0xf\n#define CCI5xx_PMU_EVENT_CODE_SHIFT\t0x0\n#define CCI5xx_PMU_EVENT_CODE_MASK\t0x1f\n\n#define CCI5xx_PMU_EVENT_SOURCE(event)\t\\\n\t((event >> CCI5xx_PMU_EVENT_SOURCE_SHIFT) & CCI5xx_PMU_EVENT_SOURCE_MASK)\n#define CCI5xx_PMU_EVENT_CODE(event)\t\\\n\t((event >> CCI5xx_PMU_EVENT_CODE_SHIFT) & CCI5xx_PMU_EVENT_CODE_MASK)\n\n#define CCI5xx_SLAVE_PORT_MIN_EV\t0x00\n#define CCI5xx_SLAVE_PORT_MAX_EV\t0x1f\n#define CCI5xx_MASTER_PORT_MIN_EV\t0x00\n#define CCI5xx_MASTER_PORT_MAX_EV\t0x06\n#define CCI5xx_GLOBAL_PORT_MIN_EV\t0x00\n#define CCI5xx_GLOBAL_PORT_MAX_EV\t0x0f\n\n\n#define CCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(_name, _config) \\\n\tCCI_EXT_ATTR_ENTRY(_name, cci5xx_pmu_global_event_show, \\\n\t\t\t\t\t(unsigned long) _config)\n\nstatic ssize_t cci5xx_pmu_global_event_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf);\n\nstatic struct attribute *cci5xx_pmu_format_attrs[] = {\n\tCCI_FORMAT_EXT_ATTR_ENTRY(event, \"config:0-4\"),\n\tCCI_FORMAT_EXT_ATTR_ENTRY(source, \"config:5-8\"),\n\tNULL,\n};\n\nstatic struct attribute *cci5xx_pmu_event_attrs[] = {\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_arvalid, 0x0),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_dev, 0x1),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_nonshareable, 0x2),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_shareable_non_alloc, 0x3),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_shareable_alloc, 0x4),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_invalidate, 0x5),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_cache_maint, 0x6),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_dvm_msg, 0x7),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_rval, 0x8),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_hs_rlast_snoop, 0x9),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_hs_awalid, 0xA),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_dev, 0xB),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_non_shareable, 0xC),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_share_wb, 0xD),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_share_wlu, 0xE),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_share_wunique, 0xF),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_evict, 0x10),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_wrevict, 0x11),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_w_data_beat, 0x12),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_srq_acvalid, 0x13),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_srq_read, 0x14),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_srq_clean, 0x15),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_srq_data_transfer_low, 0x16),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rrq_stall_arvalid, 0x17),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_data_stall, 0x18),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_wrq_stall, 0x19),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_w_data_stall, 0x1A),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_w_resp_stall, 0x1B),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_srq_stall, 0x1C),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_s_data_stall, 0x1D),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_rq_stall_ot_limit, 0x1E),\n\tCCI_EVENT_EXT_ATTR_ENTRY(si_r_stall_arbit, 0x1F),\n\n\t \n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_r_data_beat_any, 0x0),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_w_data_beat_any, 0x1),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_rrq_stall, 0x2),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_r_data_stall, 0x3),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_wrq_stall, 0x4),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_w_data_stall, 0x5),\n\tCCI_EVENT_EXT_ATTR_ENTRY(mi_w_resp_stall, 0x6),\n\n\t \n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_filter_bank_0_1, 0x0),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_filter_bank_2_3, 0x1),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_filter_bank_4_5, 0x2),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_filter_bank_6_7, 0x3),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_miss_filter_bank_0_1, 0x4),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_miss_filter_bank_2_3, 0x5),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_miss_filter_bank_4_5, 0x6),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_access_miss_filter_bank_6_7, 0x7),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_back_invalidation, 0x8),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_stall_alloc_busy, 0x9),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_stall_tt_full, 0xA),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_wrq, 0xB),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_cd_hs, 0xC),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_rq_stall_addr_hazard, 0xD),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_rq_stall_tt_full, 0xE),\n\tCCI5xx_GLOBAL_EVENT_EXT_ATTR_ENTRY(cci_snoop_rq_tzmp1_prot, 0xF),\n\tNULL\n};\n\nstatic ssize_t cci5xx_pmu_global_event_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\t\tstruct dev_ext_attribute, attr);\n\t \n\treturn sysfs_emit(buf, \"event=0x%lx,source=0x%x\\n\",\n\t\t\t  (unsigned long)eattr->var, CCI5xx_PORT_GLOBAL);\n}\n\n \nstatic int cci500_validate_hw_event(struct cci_pmu *cci_pmu,\n\t\t\t\t\tunsigned long hw_event)\n{\n\tu32 ev_source = CCI5xx_PMU_EVENT_SOURCE(hw_event);\n\tu32 ev_code = CCI5xx_PMU_EVENT_CODE(hw_event);\n\tint if_type;\n\n\tif (hw_event & ~CCI5xx_PMU_EVENT_MASK)\n\t\treturn -ENOENT;\n\n\tswitch (ev_source) {\n\tcase CCI5xx_PORT_S0:\n\tcase CCI5xx_PORT_S1:\n\tcase CCI5xx_PORT_S2:\n\tcase CCI5xx_PORT_S3:\n\tcase CCI5xx_PORT_S4:\n\tcase CCI5xx_PORT_S5:\n\tcase CCI5xx_PORT_S6:\n\t\tif_type = CCI_IF_SLAVE;\n\t\tbreak;\n\tcase CCI5xx_PORT_M0:\n\tcase CCI5xx_PORT_M1:\n\tcase CCI5xx_PORT_M2:\n\tcase CCI5xx_PORT_M3:\n\tcase CCI5xx_PORT_M4:\n\tcase CCI5xx_PORT_M5:\n\t\tif_type = CCI_IF_MASTER;\n\t\tbreak;\n\tcase CCI5xx_PORT_GLOBAL:\n\t\tif_type = CCI_IF_GLOBAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\n\t\tev_code <= cci_pmu->model->event_ranges[if_type].max)\n\t\treturn hw_event;\n\n\treturn -ENOENT;\n}\n\n \nstatic int cci550_validate_hw_event(struct cci_pmu *cci_pmu,\n\t\t\t\t\tunsigned long hw_event)\n{\n\tu32 ev_source = CCI5xx_PMU_EVENT_SOURCE(hw_event);\n\tu32 ev_code = CCI5xx_PMU_EVENT_CODE(hw_event);\n\tint if_type;\n\n\tif (hw_event & ~CCI5xx_PMU_EVENT_MASK)\n\t\treturn -ENOENT;\n\n\tswitch (ev_source) {\n\tcase CCI5xx_PORT_S0:\n\tcase CCI5xx_PORT_S1:\n\tcase CCI5xx_PORT_S2:\n\tcase CCI5xx_PORT_S3:\n\tcase CCI5xx_PORT_S4:\n\tcase CCI5xx_PORT_S5:\n\tcase CCI5xx_PORT_S6:\n\t\tif_type = CCI_IF_SLAVE;\n\t\tbreak;\n\tcase CCI5xx_PORT_M0:\n\tcase CCI5xx_PORT_M1:\n\tcase CCI5xx_PORT_M2:\n\tcase CCI5xx_PORT_M3:\n\tcase CCI5xx_PORT_M4:\n\tcase CCI5xx_PORT_M5:\n\tcase CCI5xx_PORT_M6:\n\t\tif_type = CCI_IF_MASTER;\n\t\tbreak;\n\tcase CCI5xx_PORT_GLOBAL:\n\t\tif_type = CCI_IF_GLOBAL;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\tif (ev_code >= cci_pmu->model->event_ranges[if_type].min &&\n\t\tev_code <= cci_pmu->model->event_ranges[if_type].max)\n\t\treturn hw_event;\n\n\treturn -ENOENT;\n}\n\n#endif\t \n\n \nstatic void cci_pmu_sync_counters(struct cci_pmu *cci_pmu)\n{\n\tint i;\n\tstruct cci_pmu_hw_events *cci_hw = &cci_pmu->hw_events;\n\tDECLARE_BITMAP(mask, HW_CNTRS_MAX);\n\n\tbitmap_zero(mask, HW_CNTRS_MAX);\n\tfor_each_set_bit(i, cci_pmu->hw_events.used_mask, cci_pmu->num_cntrs) {\n\t\tstruct perf_event *event = cci_hw->events[i];\n\n\t\tif (WARN_ON(!event))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (event->hw.state & PERF_HES_STOPPED)\n\t\t\tcontinue;\n\t\tif (event->hw.state & PERF_HES_ARCH) {\n\t\t\t__set_bit(i, mask);\n\t\t\tevent->hw.state &= ~PERF_HES_ARCH;\n\t\t}\n\t}\n\n\tpmu_write_counters(cci_pmu, mask);\n}\n\n \nstatic void __cci_pmu_enable_nosync(struct cci_pmu *cci_pmu)\n{\n\tu32 val;\n\n\t \n\tval = readl_relaxed(cci_pmu->ctrl_base + CCI_PMCR) | CCI_PMCR_CEN;\n\twritel(val, cci_pmu->ctrl_base + CCI_PMCR);\n}\n\n \nstatic void __cci_pmu_enable_sync(struct cci_pmu *cci_pmu)\n{\n\tcci_pmu_sync_counters(cci_pmu);\n\t__cci_pmu_enable_nosync(cci_pmu);\n}\n\n \nstatic void __cci_pmu_disable(struct cci_pmu *cci_pmu)\n{\n\tu32 val;\n\n\t \n\tval = readl_relaxed(cci_pmu->ctrl_base + CCI_PMCR) & ~CCI_PMCR_CEN;\n\twritel(val, cci_pmu->ctrl_base + CCI_PMCR);\n}\n\nstatic ssize_t cci_pmu_format_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\tstruct dev_ext_attribute, attr);\n\treturn sysfs_emit(buf, \"%s\\n\", (char *)eattr->var);\n}\n\nstatic ssize_t cci_pmu_event_show(struct device *dev,\n\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr = container_of(attr,\n\t\t\t\tstruct dev_ext_attribute, attr);\n\t \n\treturn sysfs_emit(buf, \"source=?,event=0x%lx\\n\",\n\t\t\t  (unsigned long)eattr->var);\n}\n\nstatic int pmu_is_valid_counter(struct cci_pmu *cci_pmu, int idx)\n{\n\treturn 0 <= idx && idx <= CCI_PMU_CNTR_LAST(cci_pmu);\n}\n\nstatic u32 pmu_read_register(struct cci_pmu *cci_pmu, int idx, unsigned int offset)\n{\n\treturn readl_relaxed(cci_pmu->base +\n\t\t\t     CCI_PMU_CNTR_BASE(cci_pmu->model, idx) + offset);\n}\n\nstatic void pmu_write_register(struct cci_pmu *cci_pmu, u32 value,\n\t\t\t       int idx, unsigned int offset)\n{\n\twritel_relaxed(value, cci_pmu->base +\n\t\t       CCI_PMU_CNTR_BASE(cci_pmu->model, idx) + offset);\n}\n\nstatic void pmu_disable_counter(struct cci_pmu *cci_pmu, int idx)\n{\n\tpmu_write_register(cci_pmu, 0, idx, CCI_PMU_CNTR_CTRL);\n}\n\nstatic void pmu_enable_counter(struct cci_pmu *cci_pmu, int idx)\n{\n\tpmu_write_register(cci_pmu, 1, idx, CCI_PMU_CNTR_CTRL);\n}\n\nstatic bool __maybe_unused\npmu_counter_is_enabled(struct cci_pmu *cci_pmu, int idx)\n{\n\treturn (pmu_read_register(cci_pmu, idx, CCI_PMU_CNTR_CTRL) & 0x1) != 0;\n}\n\nstatic void pmu_set_event(struct cci_pmu *cci_pmu, int idx, unsigned long event)\n{\n\tpmu_write_register(cci_pmu, event, idx, CCI_PMU_EVT_SEL);\n}\n\n \nstatic void __maybe_unused\npmu_save_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\n{\n\tint i;\n\n\tfor (i = 0; i < cci_pmu->num_cntrs; i++) {\n\t\tif (pmu_counter_is_enabled(cci_pmu, i)) {\n\t\t\tset_bit(i, mask);\n\t\t\tpmu_disable_counter(cci_pmu, i);\n\t\t}\n\t}\n}\n\n \nstatic void __maybe_unused\npmu_restore_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\n{\n\tint i;\n\n\tfor_each_set_bit(i, mask, cci_pmu->num_cntrs)\n\t\tpmu_enable_counter(cci_pmu, i);\n}\n\n \nstatic u32 pmu_get_max_counters(struct cci_pmu *cci_pmu)\n{\n\treturn (readl_relaxed(cci_pmu->ctrl_base + CCI_PMCR) &\n\t\tCCI_PMCR_NCNT_MASK) >> CCI_PMCR_NCNT_SHIFT;\n}\n\nstatic int pmu_get_event_idx(struct cci_pmu_hw_events *hw, struct perf_event *event)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tunsigned long cci_event = event->hw.config_base;\n\tint idx;\n\n\tif (cci_pmu->model->get_event_idx)\n\t\treturn cci_pmu->model->get_event_idx(cci_pmu, hw, cci_event);\n\n\t \n\tfor (idx = 0; idx <= CCI_PMU_CNTR_LAST(cci_pmu); idx++)\n\t\tif (!test_and_set_bit(idx, hw->used_mask))\n\t\t\treturn idx;\n\n\t \n\treturn -EAGAIN;\n}\n\nstatic int pmu_map_event(struct perf_event *event)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\n\tif (event->attr.type < PERF_TYPE_MAX ||\n\t\t\t!cci_pmu->model->validate_hw_event)\n\t\treturn -ENOENT;\n\n\treturn\tcci_pmu->model->validate_hw_event(cci_pmu, event->attr.config);\n}\n\nstatic int pmu_request_irq(struct cci_pmu *cci_pmu, irq_handler_t handler)\n{\n\tint i;\n\tstruct platform_device *pmu_device = cci_pmu->plat_device;\n\n\tif (unlikely(!pmu_device))\n\t\treturn -ENODEV;\n\n\tif (cci_pmu->nr_irqs < 1) {\n\t\tdev_err(&pmu_device->dev, \"no irqs for CCI PMUs defined\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tfor (i = 0; i < cci_pmu->nr_irqs; i++) {\n\t\tint err = request_irq(cci_pmu->irqs[i], handler, IRQF_SHARED,\n\t\t\t\t\"arm-cci-pmu\", cci_pmu);\n\t\tif (err) {\n\t\t\tdev_err(&pmu_device->dev, \"unable to request IRQ%d for ARM CCI PMU counters\\n\",\n\t\t\t\tcci_pmu->irqs[i]);\n\t\t\treturn err;\n\t\t}\n\n\t\tset_bit(i, &cci_pmu->active_irqs);\n\t}\n\n\treturn 0;\n}\n\nstatic void pmu_free_irq(struct cci_pmu *cci_pmu)\n{\n\tint i;\n\n\tfor (i = 0; i < cci_pmu->nr_irqs; i++) {\n\t\tif (!test_and_clear_bit(i, &cci_pmu->active_irqs))\n\t\t\tcontinue;\n\n\t\tfree_irq(cci_pmu->irqs[i], cci_pmu);\n\t}\n}\n\nstatic u32 pmu_read_counter(struct perf_event *event)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tstruct hw_perf_event *hw_counter = &event->hw;\n\tint idx = hw_counter->idx;\n\tu32 value;\n\n\tif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\n\t\tdev_err(&cci_pmu->plat_device->dev, \"Invalid CCI PMU counter %d\\n\", idx);\n\t\treturn 0;\n\t}\n\tvalue = pmu_read_register(cci_pmu, idx, CCI_PMU_CNTR);\n\n\treturn value;\n}\n\nstatic void pmu_write_counter(struct cci_pmu *cci_pmu, u32 value, int idx)\n{\n\tpmu_write_register(cci_pmu, value, idx, CCI_PMU_CNTR);\n}\n\nstatic void __pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\n{\n\tint i;\n\tstruct cci_pmu_hw_events *cci_hw = &cci_pmu->hw_events;\n\n\tfor_each_set_bit(i, mask, cci_pmu->num_cntrs) {\n\t\tstruct perf_event *event = cci_hw->events[i];\n\n\t\tif (WARN_ON(!event))\n\t\t\tcontinue;\n\t\tpmu_write_counter(cci_pmu, local64_read(&event->hw.prev_count), i);\n\t}\n}\n\nstatic void pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\n{\n\tif (cci_pmu->model->write_counters)\n\t\tcci_pmu->model->write_counters(cci_pmu, mask);\n\telse\n\t\t__pmu_write_counters(cci_pmu, mask);\n}\n\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\n \n#define CCI5xx_INVALID_EVENT\t((CCI5xx_PORT_M0 << CCI5xx_PMU_EVENT_SOURCE_SHIFT) | \\\n\t\t\t\t (CCI5xx_PMU_EVENT_CODE_MASK << CCI5xx_PMU_EVENT_CODE_SHIFT))\nstatic void cci5xx_pmu_write_counters(struct cci_pmu *cci_pmu, unsigned long *mask)\n{\n\tint i;\n\tDECLARE_BITMAP(saved_mask, HW_CNTRS_MAX);\n\n\tbitmap_zero(saved_mask, cci_pmu->num_cntrs);\n\tpmu_save_counters(cci_pmu, saved_mask);\n\n\t \n\t__cci_pmu_enable_nosync(cci_pmu);\n\n\tfor_each_set_bit(i, mask, cci_pmu->num_cntrs) {\n\t\tstruct perf_event *event = cci_pmu->hw_events.events[i];\n\n\t\tif (WARN_ON(!event))\n\t\t\tcontinue;\n\n\t\tpmu_set_event(cci_pmu, i, CCI5xx_INVALID_EVENT);\n\t\tpmu_enable_counter(cci_pmu, i);\n\t\tpmu_write_counter(cci_pmu, local64_read(&event->hw.prev_count), i);\n\t\tpmu_disable_counter(cci_pmu, i);\n\t\tpmu_set_event(cci_pmu, i, event->hw.config_base);\n\t}\n\n\t__cci_pmu_disable(cci_pmu);\n\n\tpmu_restore_counters(cci_pmu, saved_mask);\n}\n\n#endif\t \n\nstatic u64 pmu_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 delta, prev_raw_count, new_raw_count;\n\n\tdo {\n\t\tprev_raw_count = local64_read(&hwc->prev_count);\n\t\tnew_raw_count = pmu_read_counter(event);\n\t} while (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\n\t\t new_raw_count) != prev_raw_count);\n\n\tdelta = (new_raw_count - prev_raw_count) & CCI_PMU_CNTR_MASK;\n\n\tlocal64_add(delta, &event->count);\n\n\treturn new_raw_count;\n}\n\nstatic void pmu_read(struct perf_event *event)\n{\n\tpmu_event_update(event);\n}\n\nstatic void pmu_event_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\t \n\tu64 val = 1ULL << 31;\n\tlocal64_set(&hwc->prev_count, val);\n\n\t \n\thwc->state |= PERF_HES_ARCH;\n}\n\nstatic irqreturn_t pmu_handle_irq(int irq_num, void *dev)\n{\n\tstruct cci_pmu *cci_pmu = dev;\n\tstruct cci_pmu_hw_events *events = &cci_pmu->hw_events;\n\tint idx, handled = IRQ_NONE;\n\n\traw_spin_lock(&events->pmu_lock);\n\n\t \n\t__cci_pmu_disable(cci_pmu);\n\t \n\tfor (idx = 0; idx <= CCI_PMU_CNTR_LAST(cci_pmu); idx++) {\n\t\tstruct perf_event *event = events->events[idx];\n\n\t\tif (!event)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(pmu_read_register(cci_pmu, idx, CCI_PMU_OVRFLW) &\n\t\t      CCI_PMU_OVRFLW_FLAG))\n\t\t\tcontinue;\n\n\t\tpmu_write_register(cci_pmu, CCI_PMU_OVRFLW_FLAG, idx,\n\t\t\t\t\t\t\tCCI_PMU_OVRFLW);\n\n\t\tpmu_event_update(event);\n\t\tpmu_event_set_period(event);\n\t\thandled = IRQ_HANDLED;\n\t}\n\n\t \n\t__cci_pmu_enable_sync(cci_pmu);\n\traw_spin_unlock(&events->pmu_lock);\n\n\treturn IRQ_RETVAL(handled);\n}\n\nstatic int cci_pmu_get_hw(struct cci_pmu *cci_pmu)\n{\n\tint ret = pmu_request_irq(cci_pmu, pmu_handle_irq);\n\tif (ret) {\n\t\tpmu_free_irq(cci_pmu);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void cci_pmu_put_hw(struct cci_pmu *cci_pmu)\n{\n\tpmu_free_irq(cci_pmu);\n}\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tatomic_t *active_events = &cci_pmu->active_events;\n\tstruct mutex *reserve_mutex = &cci_pmu->reserve_mutex;\n\n\tif (atomic_dec_and_mutex_lock(active_events, reserve_mutex)) {\n\t\tcci_pmu_put_hw(cci_pmu);\n\t\tmutex_unlock(reserve_mutex);\n\t}\n}\n\nstatic void cci_pmu_enable(struct pmu *pmu)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\n\tstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\n\tbool enabled = !bitmap_empty(hw_events->used_mask, cci_pmu->num_cntrs);\n\tunsigned long flags;\n\n\tif (!enabled)\n\t\treturn;\n\n\traw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\n\t__cci_pmu_enable_sync(cci_pmu);\n\traw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\n\n}\n\nstatic void cci_pmu_disable(struct pmu *pmu)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\n\tstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\n\t__cci_pmu_disable(cci_pmu);\n\traw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\n}\n\n \nstatic bool pmu_fixed_hw_idx(struct cci_pmu *cci_pmu, int idx)\n{\n\treturn (idx >= 0) && (idx < cci_pmu->model->fixed_hw_cntrs);\n}\n\nstatic void cci_pmu_start(struct perf_event *event, int pmu_flags)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\tunsigned long flags;\n\n\t \n\tif (pmu_flags & PERF_EF_RELOAD)\n\t\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\n\thwc->state = 0;\n\n\tif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\n\t\tdev_err(&cci_pmu->plat_device->dev, \"Invalid CCI PMU counter %d\\n\", idx);\n\t\treturn;\n\t}\n\n\traw_spin_lock_irqsave(&hw_events->pmu_lock, flags);\n\n\t \n\tif (!pmu_fixed_hw_idx(cci_pmu, idx))\n\t\tpmu_set_event(cci_pmu, idx, hwc->config_base);\n\n\tpmu_event_set_period(event);\n\tpmu_enable_counter(cci_pmu, idx);\n\n\traw_spin_unlock_irqrestore(&hw_events->pmu_lock, flags);\n}\n\nstatic void cci_pmu_stop(struct perf_event *event, int pmu_flags)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tif (hwc->state & PERF_HES_STOPPED)\n\t\treturn;\n\n\tif (unlikely(!pmu_is_valid_counter(cci_pmu, idx))) {\n\t\tdev_err(&cci_pmu->plat_device->dev, \"Invalid CCI PMU counter %d\\n\", idx);\n\t\treturn;\n\t}\n\n\t \n\tpmu_disable_counter(cci_pmu, idx);\n\tpmu_event_update(event);\n\thwc->state |= PERF_HES_STOPPED | PERF_HES_UPTODATE;\n}\n\nstatic int cci_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\t \n\tidx = pmu_get_event_idx(hw_events, event);\n\tif (idx < 0)\n\t\treturn idx;\n\n\tevent->hw.idx = idx;\n\thw_events->events[idx] = event;\n\n\thwc->state = PERF_HES_STOPPED | PERF_HES_UPTODATE;\n\tif (flags & PERF_EF_START)\n\t\tcci_pmu_start(event, PERF_EF_RELOAD);\n\n\t \n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic void cci_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tstruct cci_pmu_hw_events *hw_events = &cci_pmu->hw_events;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tcci_pmu_stop(event, PERF_EF_UPDATE);\n\thw_events->events[idx] = NULL;\n\tclear_bit(idx, hw_events->used_mask);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic int validate_event(struct pmu *cci_pmu,\n\t\t\t  struct cci_pmu_hw_events *hw_events,\n\t\t\t  struct perf_event *event)\n{\n\tif (is_software_event(event))\n\t\treturn 1;\n\n\t \n\tif (event->pmu != cci_pmu)\n\t\treturn 0;\n\n\tif (event->state < PERF_EVENT_STATE_OFF)\n\t\treturn 1;\n\n\tif (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)\n\t\treturn 1;\n\n\treturn pmu_get_event_idx(hw_events, event) >= 0;\n}\n\nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *leader = event->group_leader;\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tunsigned long mask[BITS_TO_LONGS(HW_CNTRS_MAX)];\n\tstruct cci_pmu_hw_events fake_pmu = {\n\t\t \n\t\t.used_mask = mask,\n\t};\n\tbitmap_zero(mask, cci_pmu->num_cntrs);\n\n\tif (!validate_event(event->pmu, &fake_pmu, leader))\n\t\treturn -EINVAL;\n\n\tfor_each_sibling_event(sibling, leader) {\n\t\tif (!validate_event(event->pmu, &fake_pmu, sibling))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!validate_event(event->pmu, &fake_pmu, event))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int __hw_perf_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint mapping;\n\n\tmapping = pmu_map_event(event);\n\n\tif (mapping < 0) {\n\t\tpr_debug(\"event %x:%llx not supported\\n\", event->attr.type,\n\t\t\t event->attr.config);\n\t\treturn mapping;\n\t}\n\n\t \n\thwc->idx\t\t= -1;\n\thwc->config_base\t= 0;\n\thwc->config\t\t= 0;\n\thwc->event_base\t\t= 0;\n\n\t \n\thwc->config_base\t    |= (unsigned long)mapping;\n\n\tif (event->group_leader != event) {\n\t\tif (validate_group(event) != 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int cci_pmu_event_init(struct perf_event *event)\n{\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(event->pmu);\n\tatomic_t *active_events = &cci_pmu->active_events;\n\tint err = 0;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\tevent->cpu = cci_pmu->cpu;\n\n\tevent->destroy = hw_perf_event_destroy;\n\tif (!atomic_inc_not_zero(active_events)) {\n\t\tmutex_lock(&cci_pmu->reserve_mutex);\n\t\tif (atomic_read(active_events) == 0)\n\t\t\terr = cci_pmu_get_hw(cci_pmu);\n\t\tif (!err)\n\t\t\tatomic_inc(active_events);\n\t\tmutex_unlock(&cci_pmu->reserve_mutex);\n\t}\n\tif (err)\n\t\treturn err;\n\n\terr = __hw_perf_event_init(event);\n\tif (err)\n\t\thw_perf_event_destroy(event);\n\n\treturn err;\n}\n\nstatic ssize_t pmu_cpumask_attr_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct cci_pmu *cci_pmu = to_cci_pmu(pmu);\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(cci_pmu->cpu));\n}\n\nstatic struct device_attribute pmu_cpumask_attr =\n\t__ATTR(cpumask, S_IRUGO, pmu_cpumask_attr_show, NULL);\n\nstatic struct attribute *pmu_attrs[] = {\n\t&pmu_cpumask_attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group pmu_attr_group = {\n\t.attrs = pmu_attrs,\n};\n\nstatic struct attribute_group pmu_format_attr_group = {\n\t.name = \"format\",\n\t.attrs = NULL,\t\t \n};\n\nstatic struct attribute_group pmu_event_attr_group = {\n\t.name = \"events\",\n\t.attrs = NULL,\t\t \n};\n\nstatic const struct attribute_group *pmu_attr_groups[] = {\n\t&pmu_attr_group,\n\t&pmu_format_attr_group,\n\t&pmu_event_attr_group,\n\tNULL\n};\n\nstatic int cci_pmu_init(struct cci_pmu *cci_pmu, struct platform_device *pdev)\n{\n\tconst struct cci_pmu_model *model = cci_pmu->model;\n\tchar *name = model->name;\n\tu32 num_cntrs;\n\n\tif (WARN_ON(model->num_hw_cntrs > NUM_HW_CNTRS_MAX))\n\t\treturn -EINVAL;\n\tif (WARN_ON(model->fixed_hw_cntrs > FIXED_HW_CNTRS_MAX))\n\t\treturn -EINVAL;\n\n\tpmu_event_attr_group.attrs = model->event_attrs;\n\tpmu_format_attr_group.attrs = model->format_attrs;\n\n\tcci_pmu->pmu = (struct pmu) {\n\t\t.module\t\t= THIS_MODULE,\n\t\t.name\t\t= cci_pmu->model->name,\n\t\t.task_ctx_nr\t= perf_invalid_context,\n\t\t.pmu_enable\t= cci_pmu_enable,\n\t\t.pmu_disable\t= cci_pmu_disable,\n\t\t.event_init\t= cci_pmu_event_init,\n\t\t.add\t\t= cci_pmu_add,\n\t\t.del\t\t= cci_pmu_del,\n\t\t.start\t\t= cci_pmu_start,\n\t\t.stop\t\t= cci_pmu_stop,\n\t\t.read\t\t= pmu_read,\n\t\t.attr_groups\t= pmu_attr_groups,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t};\n\n\tcci_pmu->plat_device = pdev;\n\tnum_cntrs = pmu_get_max_counters(cci_pmu);\n\tif (num_cntrs > cci_pmu->model->num_hw_cntrs) {\n\t\tdev_warn(&pdev->dev,\n\t\t\t\"PMU implements more counters(%d) than supported by\"\n\t\t\t\" the model(%d), truncated.\",\n\t\t\tnum_cntrs, cci_pmu->model->num_hw_cntrs);\n\t\tnum_cntrs = cci_pmu->model->num_hw_cntrs;\n\t}\n\tcci_pmu->num_cntrs = num_cntrs + cci_pmu->model->fixed_hw_cntrs;\n\n\treturn perf_pmu_register(&cci_pmu->pmu, name, -1);\n}\n\nstatic int cci_pmu_offline_cpu(unsigned int cpu)\n{\n\tint target;\n\n\tif (!g_cci_pmu || cpu != g_cci_pmu->cpu)\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids)\n\t\treturn 0;\n\n\tperf_pmu_migrate_context(&g_cci_pmu->pmu, cpu, target);\n\tg_cci_pmu->cpu = target;\n\treturn 0;\n}\n\nstatic __maybe_unused struct cci_pmu_model cci_pmu_models[] = {\n#ifdef CONFIG_ARM_CCI400_PMU\n\t[CCI400_R0] = {\n\t\t.name = \"CCI_400\",\n\t\t.fixed_hw_cntrs = FIXED_HW_CNTRS_CII_4XX,  \n\t\t.num_hw_cntrs = NUM_HW_CNTRS_CII_4XX,\n\t\t.cntr_size = SZ_4K,\n\t\t.format_attrs = cci400_pmu_format_attrs,\n\t\t.event_attrs = cci400_r0_pmu_event_attrs,\n\t\t.event_ranges = {\n\t\t\t[CCI_IF_SLAVE] = {\n\t\t\t\tCCI400_R0_SLAVE_PORT_MIN_EV,\n\t\t\t\tCCI400_R0_SLAVE_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_MASTER] = {\n\t\t\t\tCCI400_R0_MASTER_PORT_MIN_EV,\n\t\t\t\tCCI400_R0_MASTER_PORT_MAX_EV,\n\t\t\t},\n\t\t},\n\t\t.validate_hw_event = cci400_validate_hw_event,\n\t\t.get_event_idx = cci400_get_event_idx,\n\t},\n\t[CCI400_R1] = {\n\t\t.name = \"CCI_400_r1\",\n\t\t.fixed_hw_cntrs = FIXED_HW_CNTRS_CII_4XX,  \n\t\t.num_hw_cntrs = NUM_HW_CNTRS_CII_4XX,\n\t\t.cntr_size = SZ_4K,\n\t\t.format_attrs = cci400_pmu_format_attrs,\n\t\t.event_attrs = cci400_r1_pmu_event_attrs,\n\t\t.event_ranges = {\n\t\t\t[CCI_IF_SLAVE] = {\n\t\t\t\tCCI400_R1_SLAVE_PORT_MIN_EV,\n\t\t\t\tCCI400_R1_SLAVE_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_MASTER] = {\n\t\t\t\tCCI400_R1_MASTER_PORT_MIN_EV,\n\t\t\t\tCCI400_R1_MASTER_PORT_MAX_EV,\n\t\t\t},\n\t\t},\n\t\t.validate_hw_event = cci400_validate_hw_event,\n\t\t.get_event_idx = cci400_get_event_idx,\n\t},\n#endif\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\t[CCI500_R0] = {\n\t\t.name = \"CCI_500\",\n\t\t.fixed_hw_cntrs = FIXED_HW_CNTRS_CII_5XX,\n\t\t.num_hw_cntrs = NUM_HW_CNTRS_CII_5XX,\n\t\t.cntr_size = SZ_64K,\n\t\t.format_attrs = cci5xx_pmu_format_attrs,\n\t\t.event_attrs = cci5xx_pmu_event_attrs,\n\t\t.event_ranges = {\n\t\t\t[CCI_IF_SLAVE] = {\n\t\t\t\tCCI5xx_SLAVE_PORT_MIN_EV,\n\t\t\t\tCCI5xx_SLAVE_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_MASTER] = {\n\t\t\t\tCCI5xx_MASTER_PORT_MIN_EV,\n\t\t\t\tCCI5xx_MASTER_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_GLOBAL] = {\n\t\t\t\tCCI5xx_GLOBAL_PORT_MIN_EV,\n\t\t\t\tCCI5xx_GLOBAL_PORT_MAX_EV,\n\t\t\t},\n\t\t},\n\t\t.validate_hw_event = cci500_validate_hw_event,\n\t\t.write_counters\t= cci5xx_pmu_write_counters,\n\t},\n\t[CCI550_R0] = {\n\t\t.name = \"CCI_550\",\n\t\t.fixed_hw_cntrs = FIXED_HW_CNTRS_CII_5XX,\n\t\t.num_hw_cntrs = NUM_HW_CNTRS_CII_5XX,\n\t\t.cntr_size = SZ_64K,\n\t\t.format_attrs = cci5xx_pmu_format_attrs,\n\t\t.event_attrs = cci5xx_pmu_event_attrs,\n\t\t.event_ranges = {\n\t\t\t[CCI_IF_SLAVE] = {\n\t\t\t\tCCI5xx_SLAVE_PORT_MIN_EV,\n\t\t\t\tCCI5xx_SLAVE_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_MASTER] = {\n\t\t\t\tCCI5xx_MASTER_PORT_MIN_EV,\n\t\t\t\tCCI5xx_MASTER_PORT_MAX_EV,\n\t\t\t},\n\t\t\t[CCI_IF_GLOBAL] = {\n\t\t\t\tCCI5xx_GLOBAL_PORT_MIN_EV,\n\t\t\t\tCCI5xx_GLOBAL_PORT_MAX_EV,\n\t\t\t},\n\t\t},\n\t\t.validate_hw_event = cci550_validate_hw_event,\n\t\t.write_counters\t= cci5xx_pmu_write_counters,\n\t},\n#endif\n};\n\nstatic const struct of_device_id arm_cci_pmu_matches[] = {\n#ifdef CONFIG_ARM_CCI400_PMU\n\t{\n\t\t.compatible = \"arm,cci-400-pmu\",\n\t\t.data\t= NULL,\n\t},\n\t{\n\t\t.compatible = \"arm,cci-400-pmu,r0\",\n\t\t.data\t= &cci_pmu_models[CCI400_R0],\n\t},\n\t{\n\t\t.compatible = \"arm,cci-400-pmu,r1\",\n\t\t.data\t= &cci_pmu_models[CCI400_R1],\n\t},\n#endif\n#ifdef CONFIG_ARM_CCI5xx_PMU\n\t{\n\t\t.compatible = \"arm,cci-500-pmu,r0\",\n\t\t.data = &cci_pmu_models[CCI500_R0],\n\t},\n\t{\n\t\t.compatible = \"arm,cci-550-pmu,r0\",\n\t\t.data = &cci_pmu_models[CCI550_R0],\n\t},\n#endif\n\t{},\n};\nMODULE_DEVICE_TABLE(of, arm_cci_pmu_matches);\n\nstatic bool is_duplicate_irq(int irq, int *irqs, int nr_irqs)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_irqs; i++)\n\t\tif (irq == irqs[i])\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic struct cci_pmu *cci_pmu_alloc(struct device *dev)\n{\n\tstruct cci_pmu *cci_pmu;\n\tconst struct cci_pmu_model *model;\n\n\t \n\tcci_pmu = devm_kzalloc(dev, sizeof(*cci_pmu), GFP_KERNEL);\n\tif (!cci_pmu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tcci_pmu->ctrl_base = *(void __iomem **)dev->platform_data;\n\n\tmodel = of_device_get_match_data(dev);\n\tif (!model) {\n\t\tdev_warn(dev,\n\t\t\t \"DEPRECATED compatible property, requires secure access to CCI registers\");\n\t\tmodel = probe_cci_model(cci_pmu);\n\t}\n\tif (!model) {\n\t\tdev_warn(dev, \"CCI PMU version not supported\\n\");\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\tcci_pmu->model = model;\n\tcci_pmu->irqs = devm_kcalloc(dev, CCI_PMU_MAX_HW_CNTRS(model),\n\t\t\t\t\tsizeof(*cci_pmu->irqs), GFP_KERNEL);\n\tif (!cci_pmu->irqs)\n\t\treturn ERR_PTR(-ENOMEM);\n\tcci_pmu->hw_events.events = devm_kcalloc(dev,\n\t\t\t\t\t     CCI_PMU_MAX_HW_CNTRS(model),\n\t\t\t\t\t     sizeof(*cci_pmu->hw_events.events),\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!cci_pmu->hw_events.events)\n\t\treturn ERR_PTR(-ENOMEM);\n\tcci_pmu->hw_events.used_mask = devm_bitmap_zalloc(dev,\n\t\t\t\t\t\t\t  CCI_PMU_MAX_HW_CNTRS(model),\n\t\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!cci_pmu->hw_events.used_mask)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn cci_pmu;\n}\n\nstatic int cci_pmu_probe(struct platform_device *pdev)\n{\n\tstruct cci_pmu *cci_pmu;\n\tint i, ret, irq;\n\n\tcci_pmu = cci_pmu_alloc(&pdev->dev);\n\tif (IS_ERR(cci_pmu))\n\t\treturn PTR_ERR(cci_pmu);\n\n\tcci_pmu->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(cci_pmu->base))\n\t\treturn -ENOMEM;\n\n\t \n\tcci_pmu->nr_irqs = 0;\n\tfor (i = 0; i < CCI_PMU_MAX_HW_CNTRS(cci_pmu->model); i++) {\n\t\tirq = platform_get_irq(pdev, i);\n\t\tif (irq < 0)\n\t\t\tbreak;\n\n\t\tif (is_duplicate_irq(irq, cci_pmu->irqs, cci_pmu->nr_irqs))\n\t\t\tcontinue;\n\n\t\tcci_pmu->irqs[cci_pmu->nr_irqs++] = irq;\n\t}\n\n\t \n\tif (i < CCI_PMU_MAX_HW_CNTRS(cci_pmu->model)) {\n\t\tdev_warn(&pdev->dev, \"In-correct number of interrupts: %d, should be %d\\n\",\n\t\t\ti, CCI_PMU_MAX_HW_CNTRS(cci_pmu->model));\n\t\treturn -EINVAL;\n\t}\n\n\traw_spin_lock_init(&cci_pmu->hw_events.pmu_lock);\n\tmutex_init(&cci_pmu->reserve_mutex);\n\tatomic_set(&cci_pmu->active_events, 0);\n\n\tcci_pmu->cpu = raw_smp_processor_id();\n\tg_cci_pmu = cci_pmu;\n\tcpuhp_setup_state_nocalls(CPUHP_AP_PERF_ARM_CCI_ONLINE,\n\t\t\t\t  \"perf/arm/cci:online\", NULL,\n\t\t\t\t  cci_pmu_offline_cpu);\n\n\tret = cci_pmu_init(cci_pmu, pdev);\n\tif (ret)\n\t\tgoto error_pmu_init;\n\n\tpr_info(\"ARM %s PMU driver probed\", cci_pmu->model->name);\n\treturn 0;\n\nerror_pmu_init:\n\tcpuhp_remove_state(CPUHP_AP_PERF_ARM_CCI_ONLINE);\n\tg_cci_pmu = NULL;\n\treturn ret;\n}\n\nstatic int cci_pmu_remove(struct platform_device *pdev)\n{\n\tif (!g_cci_pmu)\n\t\treturn 0;\n\n\tcpuhp_remove_state(CPUHP_AP_PERF_ARM_CCI_ONLINE);\n\tperf_pmu_unregister(&g_cci_pmu->pmu);\n\tg_cci_pmu = NULL;\n\n\treturn 0;\n}\n\nstatic struct platform_driver cci_pmu_driver = {\n\t.driver = {\n\t\t   .name = DRIVER_NAME,\n\t\t   .of_match_table = arm_cci_pmu_matches,\n\t\t   .suppress_bind_attrs = true,\n\t\t  },\n\t.probe = cci_pmu_probe,\n\t.remove = cci_pmu_remove,\n};\n\nmodule_platform_driver(cci_pmu_driver);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"ARM CCI PMU support\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}