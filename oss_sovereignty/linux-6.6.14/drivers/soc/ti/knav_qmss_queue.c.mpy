{
  "module_name": "knav_qmss_queue.c",
  "hash_id": "bb92c8fdd8f791e5114d9895757a15d5aa913100ae25d89a5abf1020749d8d03",
  "original_prompt": "Ingested from linux-6.6.14/drivers/soc/ti/knav_qmss_queue.c",
  "human_readable_source": "\n \n\n#include <linux/debugfs.h>\n#include <linux/dma-mapping.h>\n#include <linux/firmware.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/of_address.h>\n#include <linux/of_device.h>\n#include <linux/of_irq.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/soc/ti/knav_qmss.h>\n\n#include \"knav_qmss.h\"\n\nstatic struct knav_device *kdev;\nstatic DEFINE_MUTEX(knav_dev_lock);\n#define knav_dev_lock_held() \\\n\tlockdep_is_held(&knav_dev_lock)\n\n \n#define KNAV_QUEUE_PEEK_REG_INDEX\t0\n#define KNAV_QUEUE_STATUS_REG_INDEX\t1\n#define KNAV_QUEUE_CONFIG_REG_INDEX\t2\n#define KNAV_QUEUE_REGION_REG_INDEX\t3\n#define KNAV_QUEUE_PUSH_REG_INDEX\t4\n#define KNAV_QUEUE_POP_REG_INDEX\t5\n\n \n#define KNAV_L_QUEUE_CONFIG_REG_INDEX\t1\n#define KNAV_L_QUEUE_REGION_REG_INDEX\t2\n#define KNAV_L_QUEUE_PUSH_REG_INDEX\t3\n\n \n#define KNAV_QUEUE_PDSP_IRAM_REG_INDEX\t0\n#define KNAV_QUEUE_PDSP_REGS_REG_INDEX\t1\n#define KNAV_QUEUE_PDSP_INTD_REG_INDEX\t2\n#define KNAV_QUEUE_PDSP_CMD_REG_INDEX\t3\n\n#define knav_queue_idx_to_inst(kdev, idx)\t\t\t\\\n\t(kdev->instances + (idx << kdev->inst_shift))\n\n#define for_each_handle_rcu(qh, inst)\t\t\t\t\\\n\tlist_for_each_entry_rcu(qh, &inst->handles, list,\t\\\n\t\t\t\tknav_dev_lock_held())\n\n#define for_each_instance(idx, inst, kdev)\t\t\\\n\tfor (idx = 0, inst = kdev->instances;\t\t\\\n\t     idx < (kdev)->num_queues_in_use;\t\t\t\\\n\t     idx++, inst = knav_queue_idx_to_inst(kdev, idx))\n\n \nstatic const char * const knav_acc_firmwares[] = {\"ks2_qmss_pdsp_acc48.bin\"};\n\nstatic bool device_ready;\nbool knav_qmss_device_ready(void)\n{\n\treturn device_ready;\n}\nEXPORT_SYMBOL_GPL(knav_qmss_device_ready);\n\n \nvoid knav_queue_notify(struct knav_queue_inst *inst)\n{\n\tstruct knav_queue *qh;\n\n\tif (!inst)\n\t\treturn;\n\n\trcu_read_lock();\n\tfor_each_handle_rcu(qh, inst) {\n\t\tif (atomic_read(&qh->notifier_enabled) <= 0)\n\t\t\tcontinue;\n\t\tif (WARN_ON(!qh->notifier_fn))\n\t\t\tcontinue;\n\t\tthis_cpu_inc(qh->stats->notifies);\n\t\tqh->notifier_fn(qh->notifier_fn_arg);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(knav_queue_notify);\n\nstatic irqreturn_t knav_queue_int_handler(int irq, void *_instdata)\n{\n\tstruct knav_queue_inst *inst = _instdata;\n\n\tknav_queue_notify(inst);\n\treturn IRQ_HANDLED;\n}\n\nstatic int knav_queue_setup_irq(struct knav_range_info *range,\n\t\t\t  struct knav_queue_inst *inst)\n{\n\tunsigned queue = inst->id - range->queue_base;\n\tint ret = 0, irq;\n\n\tif (range->flags & RANGE_HAS_IRQ) {\n\t\tirq = range->irqs[queue].irq;\n\t\tret = request_irq(irq, knav_queue_int_handler, 0,\n\t\t\t\t\tinst->irq_name, inst);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tdisable_irq(irq);\n\t\tif (range->irqs[queue].cpu_mask) {\n\t\t\tret = irq_set_affinity_hint(irq, range->irqs[queue].cpu_mask);\n\t\t\tif (ret) {\n\t\t\t\tdev_warn(range->kdev->dev,\n\t\t\t\t\t \"Failed to set IRQ affinity\\n\");\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic void knav_queue_free_irq(struct knav_queue_inst *inst)\n{\n\tstruct knav_range_info *range = inst->range;\n\tunsigned queue = inst->id - inst->range->queue_base;\n\tint irq;\n\n\tif (range->flags & RANGE_HAS_IRQ) {\n\t\tirq = range->irqs[queue].irq;\n\t\tirq_set_affinity_hint(irq, NULL);\n\t\tfree_irq(irq, inst);\n\t}\n}\n\nstatic inline bool knav_queue_is_busy(struct knav_queue_inst *inst)\n{\n\treturn !list_empty(&inst->handles);\n}\n\nstatic inline bool knav_queue_is_reserved(struct knav_queue_inst *inst)\n{\n\treturn inst->range->flags & RANGE_RESERVED;\n}\n\nstatic inline bool knav_queue_is_shared(struct knav_queue_inst *inst)\n{\n\tstruct knav_queue *tmp;\n\n\trcu_read_lock();\n\tfor_each_handle_rcu(tmp, inst) {\n\t\tif (tmp->flags & KNAV_QUEUE_SHARED) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic inline bool knav_queue_match_type(struct knav_queue_inst *inst,\n\t\t\t\t\t\tunsigned type)\n{\n\tif ((type == KNAV_QUEUE_QPEND) &&\n\t    (inst->range->flags & RANGE_HAS_IRQ)) {\n\t\treturn true;\n\t} else if ((type == KNAV_QUEUE_ACC) &&\n\t\t(inst->range->flags & RANGE_HAS_ACCUMULATOR)) {\n\t\treturn true;\n\t} else if ((type == KNAV_QUEUE_GP) &&\n\t\t!(inst->range->flags &\n\t\t\t(RANGE_HAS_ACCUMULATOR | RANGE_HAS_IRQ))) {\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline struct knav_queue_inst *\nknav_queue_match_id_to_inst(struct knav_device *kdev, unsigned id)\n{\n\tstruct knav_queue_inst *inst;\n\tint idx;\n\n\tfor_each_instance(idx, inst, kdev) {\n\t\tif (inst->id == id)\n\t\t\treturn inst;\n\t}\n\treturn NULL;\n}\n\nstatic inline struct knav_queue_inst *knav_queue_find_by_id(int id)\n{\n\tif (kdev->base_id <= id &&\n\t    kdev->base_id + kdev->num_queues > id) {\n\t\tid -= kdev->base_id;\n\t\treturn knav_queue_match_id_to_inst(kdev, id);\n\t}\n\treturn NULL;\n}\n\nstatic struct knav_queue *__knav_queue_open(struct knav_queue_inst *inst,\n\t\t\t\t      const char *name, unsigned flags)\n{\n\tstruct knav_queue *qh;\n\tunsigned id;\n\tint ret = 0;\n\n\tqh = devm_kzalloc(inst->kdev->dev, sizeof(*qh), GFP_KERNEL);\n\tif (!qh)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tqh->stats = alloc_percpu(struct knav_queue_stats);\n\tif (!qh->stats) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tqh->flags = flags;\n\tqh->inst = inst;\n\tid = inst->id - inst->qmgr->start_queue;\n\tqh->reg_push = &inst->qmgr->reg_push[id];\n\tqh->reg_pop = &inst->qmgr->reg_pop[id];\n\tqh->reg_peek = &inst->qmgr->reg_peek[id];\n\n\t \n\tif (!knav_queue_is_busy(inst)) {\n\t\tstruct knav_range_info *range = inst->range;\n\n\t\tinst->name = kstrndup(name, KNAV_NAME_SIZE - 1, GFP_KERNEL);\n\t\tif (range->ops && range->ops->open_queue)\n\t\t\tret = range->ops->open_queue(range, inst, flags);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\tlist_add_tail_rcu(&qh->list, &inst->handles);\n\treturn qh;\n\nerr:\n\tif (qh->stats)\n\t\tfree_percpu(qh->stats);\n\tdevm_kfree(inst->kdev->dev, qh);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct knav_queue *\nknav_queue_open_by_id(const char *name, unsigned id, unsigned flags)\n{\n\tstruct knav_queue_inst *inst;\n\tstruct knav_queue *qh;\n\n\tmutex_lock(&knav_dev_lock);\n\n\tqh = ERR_PTR(-ENODEV);\n\tinst = knav_queue_find_by_id(id);\n\tif (!inst)\n\t\tgoto unlock_ret;\n\n\tqh = ERR_PTR(-EEXIST);\n\tif (!(flags & KNAV_QUEUE_SHARED) && knav_queue_is_busy(inst))\n\t\tgoto unlock_ret;\n\n\tqh = ERR_PTR(-EBUSY);\n\tif ((flags & KNAV_QUEUE_SHARED) &&\n\t    (knav_queue_is_busy(inst) && !knav_queue_is_shared(inst)))\n\t\tgoto unlock_ret;\n\n\tqh = __knav_queue_open(inst, name, flags);\n\nunlock_ret:\n\tmutex_unlock(&knav_dev_lock);\n\n\treturn qh;\n}\n\nstatic struct knav_queue *knav_queue_open_by_type(const char *name,\n\t\t\t\t\t\tunsigned type, unsigned flags)\n{\n\tstruct knav_queue_inst *inst;\n\tstruct knav_queue *qh = ERR_PTR(-EINVAL);\n\tint idx;\n\n\tmutex_lock(&knav_dev_lock);\n\n\tfor_each_instance(idx, inst, kdev) {\n\t\tif (knav_queue_is_reserved(inst))\n\t\t\tcontinue;\n\t\tif (!knav_queue_match_type(inst, type))\n\t\t\tcontinue;\n\t\tif (knav_queue_is_busy(inst))\n\t\t\tcontinue;\n\t\tqh = __knav_queue_open(inst, name, flags);\n\t\tgoto unlock_ret;\n\t}\n\nunlock_ret:\n\tmutex_unlock(&knav_dev_lock);\n\treturn qh;\n}\n\nstatic void knav_queue_set_notify(struct knav_queue_inst *inst, bool enabled)\n{\n\tstruct knav_range_info *range = inst->range;\n\n\tif (range->ops && range->ops->set_notify)\n\t\trange->ops->set_notify(range, inst, enabled);\n}\n\nstatic int knav_queue_enable_notifier(struct knav_queue *qh)\n{\n\tstruct knav_queue_inst *inst = qh->inst;\n\tbool first;\n\n\tif (WARN_ON(!qh->notifier_fn))\n\t\treturn -EINVAL;\n\n\t \n\tfirst = (atomic_inc_return(&qh->notifier_enabled) == 1);\n\tif (!first)\n\t\treturn 0;  \n\n\t \n\tfirst = (atomic_inc_return(&inst->num_notifiers) == 1);\n\tif (first)\n\t\tknav_queue_set_notify(inst, true);\n\n\treturn 0;\n}\n\nstatic int knav_queue_disable_notifier(struct knav_queue *qh)\n{\n\tstruct knav_queue_inst *inst = qh->inst;\n\tbool last;\n\n\tlast = (atomic_dec_return(&qh->notifier_enabled) == 0);\n\tif (!last)\n\t\treturn 0;  \n\n\tlast = (atomic_dec_return(&inst->num_notifiers) == 0);\n\tif (last)\n\t\tknav_queue_set_notify(inst, false);\n\n\treturn 0;\n}\n\nstatic int knav_queue_set_notifier(struct knav_queue *qh,\n\t\t\t\tstruct knav_queue_notify_config *cfg)\n{\n\tknav_queue_notify_fn old_fn = qh->notifier_fn;\n\n\tif (!cfg)\n\t\treturn -EINVAL;\n\n\tif (!(qh->inst->range->flags & (RANGE_HAS_ACCUMULATOR | RANGE_HAS_IRQ)))\n\t\treturn -ENOTSUPP;\n\n\tif (!cfg->fn && old_fn)\n\t\tknav_queue_disable_notifier(qh);\n\n\tqh->notifier_fn = cfg->fn;\n\tqh->notifier_fn_arg = cfg->fn_arg;\n\n\tif (cfg->fn && !old_fn)\n\t\tknav_queue_enable_notifier(qh);\n\n\treturn 0;\n}\n\nstatic int knav_gp_set_notify(struct knav_range_info *range,\n\t\t\t       struct knav_queue_inst *inst,\n\t\t\t       bool enabled)\n{\n\tunsigned queue;\n\n\tif (range->flags & RANGE_HAS_IRQ) {\n\t\tqueue = inst->id - range->queue_base;\n\t\tif (enabled)\n\t\t\tenable_irq(range->irqs[queue].irq);\n\t\telse\n\t\t\tdisable_irq_nosync(range->irqs[queue].irq);\n\t}\n\treturn 0;\n}\n\nstatic int knav_gp_open_queue(struct knav_range_info *range,\n\t\t\t\tstruct knav_queue_inst *inst, unsigned flags)\n{\n\treturn knav_queue_setup_irq(range, inst);\n}\n\nstatic int knav_gp_close_queue(struct knav_range_info *range,\n\t\t\t\tstruct knav_queue_inst *inst)\n{\n\tknav_queue_free_irq(inst);\n\treturn 0;\n}\n\nstatic struct knav_range_ops knav_gp_range_ops = {\n\t.set_notify\t= knav_gp_set_notify,\n\t.open_queue\t= knav_gp_open_queue,\n\t.close_queue\t= knav_gp_close_queue,\n};\n\n\nstatic int knav_queue_get_count(void *qhandle)\n{\n\tstruct knav_queue *qh = qhandle;\n\tstruct knav_queue_inst *inst = qh->inst;\n\n\treturn readl_relaxed(&qh->reg_peek[0].entry_count) +\n\t\tatomic_read(&inst->desc_count);\n}\n\nstatic void knav_queue_debug_show_instance(struct seq_file *s,\n\t\t\t\t\tstruct knav_queue_inst *inst)\n{\n\tstruct knav_device *kdev = inst->kdev;\n\tstruct knav_queue *qh;\n\tint cpu = 0;\n\tint pushes = 0;\n\tint pops = 0;\n\tint push_errors = 0;\n\tint pop_errors = 0;\n\tint notifies = 0;\n\n\tif (!knav_queue_is_busy(inst))\n\t\treturn;\n\n\tseq_printf(s, \"\\tqueue id %d (%s)\\n\",\n\t\t   kdev->base_id + inst->id, inst->name);\n\tfor_each_handle_rcu(qh, inst) {\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tpushes += per_cpu_ptr(qh->stats, cpu)->pushes;\n\t\t\tpops += per_cpu_ptr(qh->stats, cpu)->pops;\n\t\t\tpush_errors += per_cpu_ptr(qh->stats, cpu)->push_errors;\n\t\t\tpop_errors += per_cpu_ptr(qh->stats, cpu)->pop_errors;\n\t\t\tnotifies += per_cpu_ptr(qh->stats, cpu)->notifies;\n\t\t}\n\n\t\tseq_printf(s, \"\\t\\thandle %p: pushes %8d, pops %8d, count %8d, notifies %8d, push errors %8d, pop errors %8d\\n\",\n\t\t\t\tqh,\n\t\t\t\tpushes,\n\t\t\t\tpops,\n\t\t\t\tknav_queue_get_count(qh),\n\t\t\t\tnotifies,\n\t\t\t\tpush_errors,\n\t\t\t\tpop_errors);\n\t}\n}\n\nstatic int knav_queue_debug_show(struct seq_file *s, void *v)\n{\n\tstruct knav_queue_inst *inst;\n\tint idx;\n\n\tmutex_lock(&knav_dev_lock);\n\tseq_printf(s, \"%s: %u-%u\\n\",\n\t\t   dev_name(kdev->dev), kdev->base_id,\n\t\t   kdev->base_id + kdev->num_queues - 1);\n\tfor_each_instance(idx, inst, kdev)\n\t\tknav_queue_debug_show_instance(s, inst);\n\tmutex_unlock(&knav_dev_lock);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(knav_queue_debug);\n\nstatic inline int knav_queue_pdsp_wait(u32 * __iomem addr, unsigned timeout,\n\t\t\t\t\tu32 flags)\n{\n\tunsigned long end;\n\tu32 val = 0;\n\n\tend = jiffies + msecs_to_jiffies(timeout);\n\twhile (time_after(end, jiffies)) {\n\t\tval = readl_relaxed(addr);\n\t\tif (flags)\n\t\t\tval &= flags;\n\t\tif (!val)\n\t\t\tbreak;\n\t\tcpu_relax();\n\t}\n\treturn val ? -ETIMEDOUT : 0;\n}\n\n\nstatic int knav_queue_flush(struct knav_queue *qh)\n{\n\tstruct knav_queue_inst *inst = qh->inst;\n\tunsigned id = inst->id - inst->qmgr->start_queue;\n\n\tatomic_set(&inst->desc_count, 0);\n\twritel_relaxed(0, &inst->qmgr->reg_push[id].ptr_size_thresh);\n\treturn 0;\n}\n\n \nvoid *knav_queue_open(const char *name, unsigned id,\n\t\t\t\t\tunsigned flags)\n{\n\tstruct knav_queue *qh = ERR_PTR(-EINVAL);\n\n\tswitch (id) {\n\tcase KNAV_QUEUE_QPEND:\n\tcase KNAV_QUEUE_ACC:\n\tcase KNAV_QUEUE_GP:\n\t\tqh = knav_queue_open_by_type(name, id, flags);\n\t\tbreak;\n\n\tdefault:\n\t\tqh = knav_queue_open_by_id(name, id, flags);\n\t\tbreak;\n\t}\n\treturn qh;\n}\nEXPORT_SYMBOL_GPL(knav_queue_open);\n\n \nvoid knav_queue_close(void *qhandle)\n{\n\tstruct knav_queue *qh = qhandle;\n\tstruct knav_queue_inst *inst = qh->inst;\n\n\twhile (atomic_read(&qh->notifier_enabled) > 0)\n\t\tknav_queue_disable_notifier(qh);\n\n\tmutex_lock(&knav_dev_lock);\n\tlist_del_rcu(&qh->list);\n\tmutex_unlock(&knav_dev_lock);\n\tsynchronize_rcu();\n\tif (!knav_queue_is_busy(inst)) {\n\t\tstruct knav_range_info *range = inst->range;\n\n\t\tif (range->ops && range->ops->close_queue)\n\t\t\trange->ops->close_queue(range, inst);\n\t}\n\tfree_percpu(qh->stats);\n\tdevm_kfree(inst->kdev->dev, qh);\n}\nEXPORT_SYMBOL_GPL(knav_queue_close);\n\n \nint knav_queue_device_control(void *qhandle, enum knav_queue_ctrl_cmd cmd,\n\t\t\t\tunsigned long arg)\n{\n\tstruct knav_queue *qh = qhandle;\n\tstruct knav_queue_notify_config *cfg;\n\tint ret;\n\n\tswitch ((int)cmd) {\n\tcase KNAV_QUEUE_GET_ID:\n\t\tret = qh->inst->kdev->base_id + qh->inst->id;\n\t\tbreak;\n\n\tcase KNAV_QUEUE_FLUSH:\n\t\tret = knav_queue_flush(qh);\n\t\tbreak;\n\n\tcase KNAV_QUEUE_SET_NOTIFIER:\n\t\tcfg = (void *)arg;\n\t\tret = knav_queue_set_notifier(qh, cfg);\n\t\tbreak;\n\n\tcase KNAV_QUEUE_ENABLE_NOTIFY:\n\t\tret = knav_queue_enable_notifier(qh);\n\t\tbreak;\n\n\tcase KNAV_QUEUE_DISABLE_NOTIFY:\n\t\tret = knav_queue_disable_notifier(qh);\n\t\tbreak;\n\n\tcase KNAV_QUEUE_GET_COUNT:\n\t\tret = knav_queue_get_count(qh);\n\t\tbreak;\n\n\tdefault:\n\t\tret = -ENOTSUPP;\n\t\tbreak;\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(knav_queue_device_control);\n\n\n\n \nint knav_queue_push(void *qhandle, dma_addr_t dma,\n\t\t\t\t\tunsigned size, unsigned flags)\n{\n\tstruct knav_queue *qh = qhandle;\n\tu32 val;\n\n\tval = (u32)dma | ((size / 16) - 1);\n\twritel_relaxed(val, &qh->reg_push[0].ptr_size_thresh);\n\n\tthis_cpu_inc(qh->stats->pushes);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(knav_queue_push);\n\n \ndma_addr_t knav_queue_pop(void *qhandle, unsigned *size)\n{\n\tstruct knav_queue *qh = qhandle;\n\tstruct knav_queue_inst *inst = qh->inst;\n\tdma_addr_t dma;\n\tu32 val, idx;\n\n\t \n\tif (inst->descs) {\n\t\tif (unlikely(atomic_dec_return(&inst->desc_count) < 0)) {\n\t\t\tatomic_inc(&inst->desc_count);\n\t\t\treturn 0;\n\t\t}\n\t\tidx  = atomic_inc_return(&inst->desc_head);\n\t\tidx &= ACC_DESCS_MASK;\n\t\tval = inst->descs[idx];\n\t} else {\n\t\tval = readl_relaxed(&qh->reg_pop[0].ptr_size_thresh);\n\t\tif (unlikely(!val))\n\t\t\treturn 0;\n\t}\n\n\tdma = val & DESC_PTR_MASK;\n\tif (size)\n\t\t*size = ((val & DESC_SIZE_MASK) + 1) * 16;\n\n\tthis_cpu_inc(qh->stats->pops);\n\treturn dma;\n}\nEXPORT_SYMBOL_GPL(knav_queue_pop);\n\n \nstatic void kdesc_fill_pool(struct knav_pool *pool)\n{\n\tstruct knav_region *region;\n\tint i;\n\n\tregion = pool->region;\n\tpool->desc_size = region->desc_size;\n\tfor (i = 0; i < pool->num_desc; i++) {\n\t\tint index = pool->region_offset + i;\n\t\tdma_addr_t dma_addr;\n\t\tunsigned dma_size;\n\t\tdma_addr = region->dma_start + (region->desc_size * index);\n\t\tdma_size = ALIGN(pool->desc_size, SMP_CACHE_BYTES);\n\t\tdma_sync_single_for_device(pool->dev, dma_addr, dma_size,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tknav_queue_push(pool->queue, dma_addr, dma_size, 0);\n\t}\n}\n\n \nstatic void kdesc_empty_pool(struct knav_pool *pool)\n{\n\tdma_addr_t dma;\n\tunsigned size;\n\tvoid *desc;\n\tint i;\n\n\tif (!pool->queue)\n\t\treturn;\n\n\tfor (i = 0;; i++) {\n\t\tdma = knav_queue_pop(pool->queue, &size);\n\t\tif (!dma)\n\t\t\tbreak;\n\t\tdesc = knav_pool_desc_dma_to_virt(pool, dma);\n\t\tif (!desc) {\n\t\t\tdev_dbg(pool->kdev->dev,\n\t\t\t\t\"couldn't unmap desc, continuing\\n\");\n\t\t\tcontinue;\n\t\t}\n\t}\n\tWARN_ON(i != pool->num_desc);\n\tknav_queue_close(pool->queue);\n}\n\n\n \ndma_addr_t knav_pool_desc_virt_to_dma(void *ph, void *virt)\n{\n\tstruct knav_pool *pool = ph;\n\treturn pool->region->dma_start + (virt - pool->region->virt_start);\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_virt_to_dma);\n\nvoid *knav_pool_desc_dma_to_virt(void *ph, dma_addr_t dma)\n{\n\tstruct knav_pool *pool = ph;\n\treturn pool->region->virt_start + (dma - pool->region->dma_start);\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_dma_to_virt);\n\n \nvoid *knav_pool_create(const char *name,\n\t\t\t\t\tint num_desc, int region_id)\n{\n\tstruct knav_region *reg_itr, *region = NULL;\n\tstruct knav_pool *pool, *pi = NULL, *iter;\n\tstruct list_head *node;\n\tunsigned last_offset;\n\tint ret;\n\n\tif (!kdev)\n\t\treturn ERR_PTR(-EPROBE_DEFER);\n\n\tif (!kdev->dev)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tpool = devm_kzalloc(kdev->dev, sizeof(*pool), GFP_KERNEL);\n\tif (!pool) {\n\t\tdev_err(kdev->dev, \"out of memory allocating pool\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tfor_each_region(kdev, reg_itr) {\n\t\tif (reg_itr->id != region_id)\n\t\t\tcontinue;\n\t\tregion = reg_itr;\n\t\tbreak;\n\t}\n\n\tif (!region) {\n\t\tdev_err(kdev->dev, \"region-id(%d) not found\\n\", region_id);\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tpool->queue = knav_queue_open(name, KNAV_QUEUE_GP, 0);\n\tif (IS_ERR(pool->queue)) {\n\t\tdev_err(kdev->dev,\n\t\t\t\"failed to open queue for pool(%s), error %ld\\n\",\n\t\t\tname, PTR_ERR(pool->queue));\n\t\tret = PTR_ERR(pool->queue);\n\t\tgoto err;\n\t}\n\n\tpool->name = kstrndup(name, KNAV_NAME_SIZE - 1, GFP_KERNEL);\n\tpool->kdev = kdev;\n\tpool->dev = kdev->dev;\n\n\tmutex_lock(&knav_dev_lock);\n\n\tif (num_desc > (region->num_desc - region->used_desc)) {\n\t\tdev_err(kdev->dev, \"out of descs in region(%d) for pool(%s)\\n\",\n\t\t\tregion_id, name);\n\t\tret = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\t \n\tlast_offset = 0;\n\tnode = &region->pools;\n\tlist_for_each_entry(iter, &region->pools, region_inst) {\n\t\tif ((iter->region_offset - last_offset) >= num_desc) {\n\t\t\tpi = iter;\n\t\t\tbreak;\n\t\t}\n\t\tlast_offset = iter->region_offset + iter->num_desc;\n\t}\n\n\tif (pi) {\n\t\tnode = &pi->region_inst;\n\t\tpool->region = region;\n\t\tpool->num_desc = num_desc;\n\t\tpool->region_offset = last_offset;\n\t\tregion->used_desc += num_desc;\n\t\tlist_add_tail(&pool->list, &kdev->pools);\n\t\tlist_add_tail(&pool->region_inst, node);\n\t} else {\n\t\tdev_err(kdev->dev, \"pool(%s) create failed: fragmented desc pool in region(%d)\\n\",\n\t\t\tname, region_id);\n\t\tret = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\tmutex_unlock(&knav_dev_lock);\n\tkdesc_fill_pool(pool);\n\treturn pool;\n\nerr_unlock:\n\tmutex_unlock(&knav_dev_lock);\nerr:\n\tkfree(pool->name);\n\tdevm_kfree(kdev->dev, pool);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(knav_pool_create);\n\n \nvoid knav_pool_destroy(void *ph)\n{\n\tstruct knav_pool *pool = ph;\n\n\tif (!pool)\n\t\treturn;\n\n\tif (!pool->region)\n\t\treturn;\n\n\tkdesc_empty_pool(pool);\n\tmutex_lock(&knav_dev_lock);\n\n\tpool->region->used_desc -= pool->num_desc;\n\tlist_del(&pool->region_inst);\n\tlist_del(&pool->list);\n\n\tmutex_unlock(&knav_dev_lock);\n\tkfree(pool->name);\n\tdevm_kfree(kdev->dev, pool);\n}\nEXPORT_SYMBOL_GPL(knav_pool_destroy);\n\n\n \nvoid *knav_pool_desc_get(void *ph)\n{\n\tstruct knav_pool *pool = ph;\n\tdma_addr_t dma;\n\tunsigned size;\n\tvoid *data;\n\n\tdma = knav_queue_pop(pool->queue, &size);\n\tif (unlikely(!dma))\n\t\treturn ERR_PTR(-ENOMEM);\n\tdata = knav_pool_desc_dma_to_virt(pool, dma);\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_get);\n\n \nvoid knav_pool_desc_put(void *ph, void *desc)\n{\n\tstruct knav_pool *pool = ph;\n\tdma_addr_t dma;\n\tdma = knav_pool_desc_virt_to_dma(pool, desc);\n\tknav_queue_push(pool->queue, dma, pool->region->desc_size, 0);\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_put);\n\n \nint knav_pool_desc_map(void *ph, void *desc, unsigned size,\n\t\t\t\t\tdma_addr_t *dma, unsigned *dma_sz)\n{\n\tstruct knav_pool *pool = ph;\n\t*dma = knav_pool_desc_virt_to_dma(pool, desc);\n\tsize = min(size, pool->region->desc_size);\n\tsize = ALIGN(size, SMP_CACHE_BYTES);\n\t*dma_sz = size;\n\tdma_sync_single_for_device(pool->dev, *dma, size, DMA_TO_DEVICE);\n\n\t \n\t__iowmb();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_map);\n\n \nvoid *knav_pool_desc_unmap(void *ph, dma_addr_t dma, unsigned dma_sz)\n{\n\tstruct knav_pool *pool = ph;\n\tunsigned desc_sz;\n\tvoid *desc;\n\n\tdesc_sz = min(dma_sz, pool->region->desc_size);\n\tdesc = knav_pool_desc_dma_to_virt(pool, dma);\n\tdma_sync_single_for_cpu(pool->dev, dma, desc_sz, DMA_FROM_DEVICE);\n\tprefetch(desc);\n\treturn desc;\n}\nEXPORT_SYMBOL_GPL(knav_pool_desc_unmap);\n\n \nint knav_pool_count(void *ph)\n{\n\tstruct knav_pool *pool = ph;\n\treturn knav_queue_get_count(pool->queue);\n}\nEXPORT_SYMBOL_GPL(knav_pool_count);\n\nstatic void knav_queue_setup_region(struct knav_device *kdev,\n\t\t\t\t\tstruct knav_region *region)\n{\n\tunsigned hw_num_desc, hw_desc_size, size;\n\tstruct knav_reg_region __iomem  *regs;\n\tstruct knav_qmgr_info *qmgr;\n\tstruct knav_pool *pool;\n\tint id = region->id;\n\tstruct page *page;\n\n\t \n\tif (!region->num_desc) {\n\t\tdev_warn(kdev->dev, \"unused region %s\\n\", region->name);\n\t\treturn;\n\t}\n\n\t \n\thw_num_desc = ilog2(region->num_desc - 1) + 1;\n\n\t \n\tif (region->num_desc < 32) {\n\t\tregion->num_desc = 0;\n\t\tdev_warn(kdev->dev, \"too few descriptors in region %s\\n\",\n\t\t\t region->name);\n\t\treturn;\n\t}\n\n\tsize = region->num_desc * region->desc_size;\n\tregion->virt_start = alloc_pages_exact(size, GFP_KERNEL | GFP_DMA |\n\t\t\t\t\t\tGFP_DMA32);\n\tif (!region->virt_start) {\n\t\tregion->num_desc = 0;\n\t\tdev_err(kdev->dev, \"memory alloc failed for region %s\\n\",\n\t\t\tregion->name);\n\t\treturn;\n\t}\n\tregion->virt_end = region->virt_start + size;\n\tpage = virt_to_page(region->virt_start);\n\n\tregion->dma_start = dma_map_page(kdev->dev, page, 0, size,\n\t\t\t\t\t DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(kdev->dev, region->dma_start)) {\n\t\tdev_err(kdev->dev, \"dma map failed for region %s\\n\",\n\t\t\tregion->name);\n\t\tgoto fail;\n\t}\n\tregion->dma_end = region->dma_start + size;\n\n\tpool = devm_kzalloc(kdev->dev, sizeof(*pool), GFP_KERNEL);\n\tif (!pool) {\n\t\tdev_err(kdev->dev, \"out of memory allocating dummy pool\\n\");\n\t\tgoto fail;\n\t}\n\tpool->num_desc = 0;\n\tpool->region_offset = region->num_desc;\n\tlist_add(&pool->region_inst, &region->pools);\n\n\tdev_dbg(kdev->dev,\n\t\t\"region %s (%d): size:%d, link:%d@%d, dma:%pad-%pad, virt:%p-%p\\n\",\n\t\tregion->name, id, region->desc_size, region->num_desc,\n\t\tregion->link_index, &region->dma_start, &region->dma_end,\n\t\tregion->virt_start, region->virt_end);\n\n\thw_desc_size = (region->desc_size / 16) - 1;\n\thw_num_desc -= 5;\n\n\tfor_each_qmgr(kdev, qmgr) {\n\t\tregs = qmgr->reg_region + id;\n\t\twritel_relaxed((u32)region->dma_start, &regs->base);\n\t\twritel_relaxed(region->link_index, &regs->start_index);\n\t\twritel_relaxed(hw_desc_size << 16 | hw_num_desc,\n\t\t\t       &regs->size_count);\n\t}\n\treturn;\n\nfail:\n\tif (region->dma_start)\n\t\tdma_unmap_page(kdev->dev, region->dma_start, size,\n\t\t\t\tDMA_BIDIRECTIONAL);\n\tif (region->virt_start)\n\t\tfree_pages_exact(region->virt_start, size);\n\tregion->num_desc = 0;\n\treturn;\n}\n\nstatic const char *knav_queue_find_name(struct device_node *node)\n{\n\tconst char *name;\n\n\tif (of_property_read_string(node, \"label\", &name) < 0)\n\t\tname = node->name;\n\tif (!name)\n\t\tname = \"unknown\";\n\treturn name;\n}\n\nstatic int knav_queue_setup_regions(struct knav_device *kdev,\n\t\t\t\t\tstruct device_node *regions)\n{\n\tstruct device *dev = kdev->dev;\n\tstruct knav_region *region;\n\tstruct device_node *child;\n\tu32 temp[2];\n\tint ret;\n\n\tfor_each_child_of_node(regions, child) {\n\t\tregion = devm_kzalloc(dev, sizeof(*region), GFP_KERNEL);\n\t\tif (!region) {\n\t\t\tof_node_put(child);\n\t\t\tdev_err(dev, \"out of memory allocating region\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tregion->name = knav_queue_find_name(child);\n\t\tof_property_read_u32(child, \"id\", &region->id);\n\t\tret = of_property_read_u32_array(child, \"region-spec\", temp, 2);\n\t\tif (!ret) {\n\t\t\tregion->num_desc  = temp[0];\n\t\t\tregion->desc_size = temp[1];\n\t\t} else {\n\t\t\tdev_err(dev, \"invalid region info %s\\n\", region->name);\n\t\t\tdevm_kfree(dev, region);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!of_get_property(child, \"link-index\", NULL)) {\n\t\t\tdev_err(dev, \"No link info for %s\\n\", region->name);\n\t\t\tdevm_kfree(dev, region);\n\t\t\tcontinue;\n\t\t}\n\t\tret = of_property_read_u32(child, \"link-index\",\n\t\t\t\t\t   &region->link_index);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"link index not found for %s\\n\",\n\t\t\t\tregion->name);\n\t\t\tdevm_kfree(dev, region);\n\t\t\tcontinue;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&region->pools);\n\t\tlist_add_tail(&region->list, &kdev->regions);\n\t}\n\tif (list_empty(&kdev->regions)) {\n\t\tdev_err(dev, \"no valid region information found\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tfor_each_region(kdev, region)\n\t\tknav_queue_setup_region(kdev, region);\n\n\treturn 0;\n}\n\nstatic int knav_get_link_ram(struct knav_device *kdev,\n\t\t\t\t       const char *name,\n\t\t\t\t       struct knav_link_ram_block *block)\n{\n\tstruct platform_device *pdev = to_platform_device(kdev->dev);\n\tstruct device_node *node = pdev->dev.of_node;\n\tu32 temp[2];\n\n\t \n\tif (!of_property_read_u32_array(node, name , temp, 2)) {\n\t\tif (temp[0]) {\n\t\t\t \n\t\t\tblock->dma = (dma_addr_t)temp[0];\n\t\t\tblock->virt = NULL;\n\t\t\tblock->size = temp[1];\n\t\t} else {\n\t\t\tblock->size = temp[1];\n\t\t\t \n\t\t\tblock->virt = dmam_alloc_coherent(kdev->dev,\n\t\t\t\t\t\t  8 * block->size, &block->dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (!block->virt) {\n\t\t\t\tdev_err(kdev->dev, \"failed to alloc linkram\\n\");\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\t} else {\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic int knav_queue_setup_link_ram(struct knav_device *kdev)\n{\n\tstruct knav_link_ram_block *block;\n\tstruct knav_qmgr_info *qmgr;\n\n\tfor_each_qmgr(kdev, qmgr) {\n\t\tblock = &kdev->link_rams[0];\n\t\tdev_dbg(kdev->dev, \"linkram0: dma:%pad, virt:%p, size:%x\\n\",\n\t\t\t&block->dma, block->virt, block->size);\n\t\twritel_relaxed((u32)block->dma, &qmgr->reg_config->link_ram_base0);\n\t\tif (kdev->version == QMSS_66AK2G)\n\t\t\twritel_relaxed(block->size,\n\t\t\t\t       &qmgr->reg_config->link_ram_size0);\n\t\telse\n\t\t\twritel_relaxed(block->size - 1,\n\t\t\t\t       &qmgr->reg_config->link_ram_size0);\n\t\tblock++;\n\t\tif (!block->size)\n\t\t\tcontinue;\n\n\t\tdev_dbg(kdev->dev, \"linkram1: dma:%pad, virt:%p, size:%x\\n\",\n\t\t\t&block->dma, block->virt, block->size);\n\t\twritel_relaxed(block->dma, &qmgr->reg_config->link_ram_base1);\n\t}\n\n\treturn 0;\n}\n\nstatic int knav_setup_queue_range(struct knav_device *kdev,\n\t\t\t\t\tstruct device_node *node)\n{\n\tstruct device *dev = kdev->dev;\n\tstruct knav_range_info *range;\n\tstruct knav_qmgr_info *qmgr;\n\tu32 temp[2], start, end, id, index;\n\tint ret, i;\n\n\trange = devm_kzalloc(dev, sizeof(*range), GFP_KERNEL);\n\tif (!range) {\n\t\tdev_err(dev, \"out of memory allocating range\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trange->kdev = kdev;\n\trange->name = knav_queue_find_name(node);\n\tret = of_property_read_u32_array(node, \"qrange\", temp, 2);\n\tif (!ret) {\n\t\trange->queue_base = temp[0] - kdev->base_id;\n\t\trange->num_queues = temp[1];\n\t} else {\n\t\tdev_err(dev, \"invalid queue range %s\\n\", range->name);\n\t\tdevm_kfree(dev, range);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < RANGE_MAX_IRQS; i++) {\n\t\tstruct of_phandle_args oirq;\n\n\t\tif (of_irq_parse_one(node, i, &oirq))\n\t\t\tbreak;\n\n\t\trange->irqs[i].irq = irq_create_of_mapping(&oirq);\n\t\tif (range->irqs[i].irq == IRQ_NONE)\n\t\t\tbreak;\n\n\t\trange->num_irqs++;\n\n\t\tif (IS_ENABLED(CONFIG_SMP) && oirq.args_count == 3) {\n\t\t\tunsigned long mask;\n\t\t\tint bit;\n\n\t\t\trange->irqs[i].cpu_mask = devm_kzalloc(dev,\n\t\t\t\t\t\t\t       cpumask_size(), GFP_KERNEL);\n\t\t\tif (!range->irqs[i].cpu_mask)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tmask = (oirq.args[2] & 0x0000ff00) >> 8;\n\t\t\tfor_each_set_bit(bit, &mask, BITS_PER_LONG)\n\t\t\t\tcpumask_set_cpu(bit, range->irqs[i].cpu_mask);\n\t\t}\n\t}\n\n\trange->num_irqs = min(range->num_irqs, range->num_queues);\n\tif (range->num_irqs)\n\t\trange->flags |= RANGE_HAS_IRQ;\n\n\tif (of_property_read_bool(node, \"qalloc-by-id\"))\n\t\trange->flags |= RANGE_RESERVED;\n\n\tif (of_property_present(node, \"accumulator\")) {\n\t\tret = knav_init_acc_range(kdev, node, range);\n\t\tif (ret < 0) {\n\t\t\tdevm_kfree(dev, range);\n\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\trange->ops = &knav_gp_range_ops;\n\t}\n\n\t \n\tfor_each_qmgr(kdev, qmgr) {\n\t\tstart = max(qmgr->start_queue, range->queue_base);\n\t\tend   = min(qmgr->start_queue + qmgr->num_queues,\n\t\t\t    range->queue_base + range->num_queues);\n\t\tfor (id = start; id < end; id++) {\n\t\t\tindex = id - qmgr->start_queue;\n\t\t\twritel_relaxed(THRESH_GTE | 1,\n\t\t\t\t       &qmgr->reg_peek[index].ptr_size_thresh);\n\t\t\twritel_relaxed(0,\n\t\t\t\t       &qmgr->reg_push[index].ptr_size_thresh);\n\t\t}\n\t}\n\n\tlist_add_tail(&range->list, &kdev->queue_ranges);\n\tdev_dbg(dev, \"added range %s: %d-%d, %d irqs%s%s%s\\n\",\n\t\trange->name, range->queue_base,\n\t\trange->queue_base + range->num_queues - 1,\n\t\trange->num_irqs,\n\t\t(range->flags & RANGE_HAS_IRQ) ? \", has irq\" : \"\",\n\t\t(range->flags & RANGE_RESERVED) ? \", reserved\" : \"\",\n\t\t(range->flags & RANGE_HAS_ACCUMULATOR) ? \", acc\" : \"\");\n\tkdev->num_queues_in_use += range->num_queues;\n\treturn 0;\n}\n\nstatic int knav_setup_queue_pools(struct knav_device *kdev,\n\t\t\t\t   struct device_node *queue_pools)\n{\n\tstruct device_node *type, *range;\n\n\tfor_each_child_of_node(queue_pools, type) {\n\t\tfor_each_child_of_node(type, range) {\n\t\t\t \n\t\t\tknav_setup_queue_range(kdev, range);\n\t\t}\n\t}\n\n\t \n\tif (list_empty(&kdev->queue_ranges)) {\n\t\tdev_err(kdev->dev, \"no valid queue range found\\n\");\n\t\treturn -ENODEV;\n\t}\n\treturn 0;\n}\n\nstatic void knav_free_queue_range(struct knav_device *kdev,\n\t\t\t\t  struct knav_range_info *range)\n{\n\tif (range->ops && range->ops->free_range)\n\t\trange->ops->free_range(range);\n\tlist_del(&range->list);\n\tdevm_kfree(kdev->dev, range);\n}\n\nstatic void knav_free_queue_ranges(struct knav_device *kdev)\n{\n\tstruct knav_range_info *range;\n\n\tfor (;;) {\n\t\trange = first_queue_range(kdev);\n\t\tif (!range)\n\t\t\tbreak;\n\t\tknav_free_queue_range(kdev, range);\n\t}\n}\n\nstatic void knav_queue_free_regions(struct knav_device *kdev)\n{\n\tstruct knav_region *region;\n\tstruct knav_pool *pool, *tmp;\n\tunsigned size;\n\n\tfor (;;) {\n\t\tregion = first_region(kdev);\n\t\tif (!region)\n\t\t\tbreak;\n\t\tlist_for_each_entry_safe(pool, tmp, &region->pools, region_inst)\n\t\t\tknav_pool_destroy(pool);\n\n\t\tsize = region->virt_end - region->virt_start;\n\t\tif (size)\n\t\t\tfree_pages_exact(region->virt_start, size);\n\t\tlist_del(&region->list);\n\t\tdevm_kfree(kdev->dev, region);\n\t}\n}\n\nstatic void __iomem *knav_queue_map_reg(struct knav_device *kdev,\n\t\t\t\t\tstruct device_node *node, int index)\n{\n\tstruct resource res;\n\tvoid __iomem *regs;\n\tint ret;\n\n\tret = of_address_to_resource(node, index, &res);\n\tif (ret) {\n\t\tdev_err(kdev->dev, \"Can't translate of node(%pOFn) address for index(%d)\\n\",\n\t\t\tnode, index);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tregs = devm_ioremap_resource(kdev->dev, &res);\n\tif (IS_ERR(regs))\n\t\tdev_err(kdev->dev, \"Failed to map register base for index(%d) node(%pOFn)\\n\",\n\t\t\tindex, node);\n\treturn regs;\n}\n\nstatic int knav_queue_init_qmgrs(struct knav_device *kdev,\n\t\t\t\t\tstruct device_node *qmgrs)\n{\n\tstruct device *dev = kdev->dev;\n\tstruct knav_qmgr_info *qmgr;\n\tstruct device_node *child;\n\tu32 temp[2];\n\tint ret;\n\n\tfor_each_child_of_node(qmgrs, child) {\n\t\tqmgr = devm_kzalloc(dev, sizeof(*qmgr), GFP_KERNEL);\n\t\tif (!qmgr) {\n\t\t\tof_node_put(child);\n\t\t\tdev_err(dev, \"out of memory allocating qmgr\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tret = of_property_read_u32_array(child, \"managed-queues\",\n\t\t\t\t\t\t temp, 2);\n\t\tif (!ret) {\n\t\t\tqmgr->start_queue = temp[0];\n\t\t\tqmgr->num_queues = temp[1];\n\t\t} else {\n\t\t\tdev_err(dev, \"invalid qmgr queue range\\n\");\n\t\t\tdevm_kfree(dev, qmgr);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev_info(dev, \"qmgr start queue %d, number of queues %d\\n\",\n\t\t\t qmgr->start_queue, qmgr->num_queues);\n\n\t\tqmgr->reg_peek =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   KNAV_QUEUE_PEEK_REG_INDEX);\n\n\t\tif (kdev->version == QMSS) {\n\t\t\tqmgr->reg_status =\n\t\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t\t   KNAV_QUEUE_STATUS_REG_INDEX);\n\t\t}\n\n\t\tqmgr->reg_config =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   (kdev->version == QMSS_66AK2G) ?\n\t\t\t\t\t   KNAV_L_QUEUE_CONFIG_REG_INDEX :\n\t\t\t\t\t   KNAV_QUEUE_CONFIG_REG_INDEX);\n\t\tqmgr->reg_region =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   (kdev->version == QMSS_66AK2G) ?\n\t\t\t\t\t   KNAV_L_QUEUE_REGION_REG_INDEX :\n\t\t\t\t\t   KNAV_QUEUE_REGION_REG_INDEX);\n\n\t\tqmgr->reg_push =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   (kdev->version == QMSS_66AK2G) ?\n\t\t\t\t\t    KNAV_L_QUEUE_PUSH_REG_INDEX :\n\t\t\t\t\t    KNAV_QUEUE_PUSH_REG_INDEX);\n\n\t\tif (kdev->version == QMSS) {\n\t\t\tqmgr->reg_pop =\n\t\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t\t   KNAV_QUEUE_POP_REG_INDEX);\n\t\t}\n\n\t\tif (IS_ERR(qmgr->reg_peek) ||\n\t\t    ((kdev->version == QMSS) &&\n\t\t    (IS_ERR(qmgr->reg_status) || IS_ERR(qmgr->reg_pop))) ||\n\t\t    IS_ERR(qmgr->reg_config) || IS_ERR(qmgr->reg_region) ||\n\t\t    IS_ERR(qmgr->reg_push)) {\n\t\t\tdev_err(dev, \"failed to map qmgr regs\\n\");\n\t\t\tif (kdev->version == QMSS) {\n\t\t\t\tif (!IS_ERR(qmgr->reg_status))\n\t\t\t\t\tdevm_iounmap(dev, qmgr->reg_status);\n\t\t\t\tif (!IS_ERR(qmgr->reg_pop))\n\t\t\t\t\tdevm_iounmap(dev, qmgr->reg_pop);\n\t\t\t}\n\t\t\tif (!IS_ERR(qmgr->reg_peek))\n\t\t\t\tdevm_iounmap(dev, qmgr->reg_peek);\n\t\t\tif (!IS_ERR(qmgr->reg_config))\n\t\t\t\tdevm_iounmap(dev, qmgr->reg_config);\n\t\t\tif (!IS_ERR(qmgr->reg_region))\n\t\t\t\tdevm_iounmap(dev, qmgr->reg_region);\n\t\t\tif (!IS_ERR(qmgr->reg_push))\n\t\t\t\tdevm_iounmap(dev, qmgr->reg_push);\n\t\t\tdevm_kfree(dev, qmgr);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (kdev->version == QMSS_66AK2G)\n\t\t\tqmgr->reg_pop = qmgr->reg_push;\n\n\t\tlist_add_tail(&qmgr->list, &kdev->qmgrs);\n\t\tdev_info(dev, \"added qmgr start queue %d, num of queues %d, reg_peek %p, reg_status %p, reg_config %p, reg_region %p, reg_push %p, reg_pop %p\\n\",\n\t\t\t qmgr->start_queue, qmgr->num_queues,\n\t\t\t qmgr->reg_peek, qmgr->reg_status,\n\t\t\t qmgr->reg_config, qmgr->reg_region,\n\t\t\t qmgr->reg_push, qmgr->reg_pop);\n\t}\n\treturn 0;\n}\n\nstatic int knav_queue_init_pdsps(struct knav_device *kdev,\n\t\t\t\t\tstruct device_node *pdsps)\n{\n\tstruct device *dev = kdev->dev;\n\tstruct knav_pdsp_info *pdsp;\n\tstruct device_node *child;\n\n\tfor_each_child_of_node(pdsps, child) {\n\t\tpdsp = devm_kzalloc(dev, sizeof(*pdsp), GFP_KERNEL);\n\t\tif (!pdsp) {\n\t\t\tof_node_put(child);\n\t\t\tdev_err(dev, \"out of memory allocating pdsp\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tpdsp->name = knav_queue_find_name(child);\n\t\tpdsp->iram =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   KNAV_QUEUE_PDSP_IRAM_REG_INDEX);\n\t\tpdsp->regs =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   KNAV_QUEUE_PDSP_REGS_REG_INDEX);\n\t\tpdsp->intd =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   KNAV_QUEUE_PDSP_INTD_REG_INDEX);\n\t\tpdsp->command =\n\t\t\tknav_queue_map_reg(kdev, child,\n\t\t\t\t\t   KNAV_QUEUE_PDSP_CMD_REG_INDEX);\n\n\t\tif (IS_ERR(pdsp->command) || IS_ERR(pdsp->iram) ||\n\t\t    IS_ERR(pdsp->regs) || IS_ERR(pdsp->intd)) {\n\t\t\tdev_err(dev, \"failed to map pdsp %s regs\\n\",\n\t\t\t\tpdsp->name);\n\t\t\tif (!IS_ERR(pdsp->command))\n\t\t\t\tdevm_iounmap(dev, pdsp->command);\n\t\t\tif (!IS_ERR(pdsp->iram))\n\t\t\t\tdevm_iounmap(dev, pdsp->iram);\n\t\t\tif (!IS_ERR(pdsp->regs))\n\t\t\t\tdevm_iounmap(dev, pdsp->regs);\n\t\t\tif (!IS_ERR(pdsp->intd))\n\t\t\t\tdevm_iounmap(dev, pdsp->intd);\n\t\t\tdevm_kfree(dev, pdsp);\n\t\t\tcontinue;\n\t\t}\n\t\tof_property_read_u32(child, \"id\", &pdsp->id);\n\t\tlist_add_tail(&pdsp->list, &kdev->pdsps);\n\t\tdev_dbg(dev, \"added pdsp %s: command %p, iram %p, regs %p, intd %p\\n\",\n\t\t\tpdsp->name, pdsp->command, pdsp->iram, pdsp->regs,\n\t\t\tpdsp->intd);\n\t}\n\treturn 0;\n}\n\nstatic int knav_queue_stop_pdsp(struct knav_device *kdev,\n\t\t\t  struct knav_pdsp_info *pdsp)\n{\n\tu32 val, timeout = 1000;\n\tint ret;\n\n\tval = readl_relaxed(&pdsp->regs->control) & ~PDSP_CTRL_ENABLE;\n\twritel_relaxed(val, &pdsp->regs->control);\n\tret = knav_queue_pdsp_wait(&pdsp->regs->control, timeout,\n\t\t\t\t\tPDSP_CTRL_RUNNING);\n\tif (ret < 0) {\n\t\tdev_err(kdev->dev, \"timed out on pdsp %s stop\\n\", pdsp->name);\n\t\treturn ret;\n\t}\n\tpdsp->loaded = false;\n\tpdsp->started = false;\n\treturn 0;\n}\n\nstatic int knav_queue_load_pdsp(struct knav_device *kdev,\n\t\t\t  struct knav_pdsp_info *pdsp)\n{\n\tint i, ret, fwlen;\n\tconst struct firmware *fw;\n\tbool found = false;\n\tu32 *fwdata;\n\n\tfor (i = 0; i < ARRAY_SIZE(knav_acc_firmwares); i++) {\n\t\tif (knav_acc_firmwares[i]) {\n\t\t\tret = request_firmware_direct(&fw,\n\t\t\t\t\t\t      knav_acc_firmwares[i],\n\t\t\t\t\t\t      kdev->dev);\n\t\t\tif (!ret) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!found) {\n\t\tdev_err(kdev->dev, \"failed to get firmware for pdsp\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tdev_info(kdev->dev, \"firmware file %s downloaded for PDSP\\n\",\n\t\t knav_acc_firmwares[i]);\n\n\twritel_relaxed(pdsp->id + 1, pdsp->command + 0x18);\n\t \n\tfwdata = (u32 *)fw->data;\n\tfwlen = (fw->size + sizeof(u32) - 1) / sizeof(u32);\n\tfor (i = 0; i < fwlen; i++)\n\t\twritel_relaxed(be32_to_cpu(fwdata[i]), pdsp->iram + i);\n\n\trelease_firmware(fw);\n\treturn 0;\n}\n\nstatic int knav_queue_start_pdsp(struct knav_device *kdev,\n\t\t\t   struct knav_pdsp_info *pdsp)\n{\n\tu32 val, timeout = 1000;\n\tint ret;\n\n\t \n\twritel_relaxed(0xffffffff, pdsp->command);\n\twhile (readl_relaxed(pdsp->command) != 0xffffffff)\n\t\tcpu_relax();\n\n\t \n\tval  = readl_relaxed(&pdsp->regs->control);\n\tval &= ~(PDSP_CTRL_PC_MASK | PDSP_CTRL_SOFT_RESET);\n\twritel_relaxed(val, &pdsp->regs->control);\n\n\t \n\tval = readl_relaxed(&pdsp->regs->control) | PDSP_CTRL_ENABLE;\n\twritel_relaxed(val, &pdsp->regs->control);\n\n\t \n\tret = knav_queue_pdsp_wait(pdsp->command, timeout, 0);\n\tif (ret < 0) {\n\t\tdev_err(kdev->dev,\n\t\t\t\"timed out on pdsp %s command register wait\\n\",\n\t\t\tpdsp->name);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void knav_queue_stop_pdsps(struct knav_device *kdev)\n{\n\tstruct knav_pdsp_info *pdsp;\n\n\t \n\tfor_each_pdsp(kdev, pdsp)\n\t\tknav_queue_stop_pdsp(kdev, pdsp);\n}\n\nstatic int knav_queue_start_pdsps(struct knav_device *kdev)\n{\n\tstruct knav_pdsp_info *pdsp;\n\tint ret;\n\n\tknav_queue_stop_pdsps(kdev);\n\t \n\tfor_each_pdsp(kdev, pdsp) {\n\t\tret = knav_queue_load_pdsp(kdev, pdsp);\n\t\tif (!ret)\n\t\t\tpdsp->loaded = true;\n\t}\n\n\tfor_each_pdsp(kdev, pdsp) {\n\t\tif (pdsp->loaded) {\n\t\t\tret = knav_queue_start_pdsp(kdev, pdsp);\n\t\t\tif (!ret)\n\t\t\t\tpdsp->started = true;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic inline struct knav_qmgr_info *knav_find_qmgr(unsigned id)\n{\n\tstruct knav_qmgr_info *qmgr;\n\n\tfor_each_qmgr(kdev, qmgr) {\n\t\tif ((id >= qmgr->start_queue) &&\n\t\t    (id < qmgr->start_queue + qmgr->num_queues))\n\t\t\treturn qmgr;\n\t}\n\treturn NULL;\n}\n\nstatic int knav_queue_init_queue(struct knav_device *kdev,\n\t\t\t\t\tstruct knav_range_info *range,\n\t\t\t\t\tstruct knav_queue_inst *inst,\n\t\t\t\t\tunsigned id)\n{\n\tchar irq_name[KNAV_NAME_SIZE];\n\tinst->qmgr = knav_find_qmgr(id);\n\tif (!inst->qmgr)\n\t\treturn -1;\n\n\tINIT_LIST_HEAD(&inst->handles);\n\tinst->kdev = kdev;\n\tinst->range = range;\n\tinst->irq_num = -1;\n\tinst->id = id;\n\tscnprintf(irq_name, sizeof(irq_name), \"hwqueue-%d\", id);\n\tinst->irq_name = kstrndup(irq_name, sizeof(irq_name), GFP_KERNEL);\n\n\tif (range->ops && range->ops->init_queue)\n\t\treturn range->ops->init_queue(range, inst);\n\telse\n\t\treturn 0;\n}\n\nstatic int knav_queue_init_queues(struct knav_device *kdev)\n{\n\tstruct knav_range_info *range;\n\tint size, id, base_idx;\n\tint idx = 0, ret = 0;\n\n\t \n\tsize = sizeof(struct knav_queue_inst);\n\n\t \n\tkdev->inst_shift = order_base_2(size);\n\tsize = (1 << kdev->inst_shift) * kdev->num_queues_in_use;\n\tkdev->instances = devm_kzalloc(kdev->dev, size, GFP_KERNEL);\n\tif (!kdev->instances)\n\t\treturn -ENOMEM;\n\n\tfor_each_queue_range(kdev, range) {\n\t\tif (range->ops && range->ops->init_range)\n\t\t\trange->ops->init_range(range);\n\t\tbase_idx = idx;\n\t\tfor (id = range->queue_base;\n\t\t     id < range->queue_base + range->num_queues; id++, idx++) {\n\t\t\tret = knav_queue_init_queue(kdev, range,\n\t\t\t\t\tknav_queue_idx_to_inst(kdev, idx), id);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t}\n\t\trange->queue_base_inst =\n\t\t\tknav_queue_idx_to_inst(kdev, base_idx);\n\t}\n\treturn 0;\n}\n\n \nstatic const struct of_device_id keystone_qmss_of_match[] = {\n\t{\n\t\t.compatible = \"ti,keystone-navigator-qmss\",\n\t},\n\t{\n\t\t.compatible = \"ti,66ak2g-navss-qm\",\n\t\t.data\t= (void *)QMSS_66AK2G,\n\t},\n\t{},\n};\nMODULE_DEVICE_TABLE(of, keystone_qmss_of_match);\n\nstatic int knav_queue_probe(struct platform_device *pdev)\n{\n\tstruct device_node *node = pdev->dev.of_node;\n\tstruct device_node *qmgrs, *queue_pools, *regions, *pdsps;\n\tconst struct of_device_id *match;\n\tstruct device *dev = &pdev->dev;\n\tu32 temp[2];\n\tint ret;\n\n\tif (!node) {\n\t\tdev_err(dev, \"device tree info unavailable\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tkdev = devm_kzalloc(dev, sizeof(struct knav_device), GFP_KERNEL);\n\tif (!kdev) {\n\t\tdev_err(dev, \"memory allocation failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmatch = of_match_device(of_match_ptr(keystone_qmss_of_match), dev);\n\tif (match && match->data)\n\t\tkdev->version = QMSS_66AK2G;\n\n\tplatform_set_drvdata(pdev, kdev);\n\tkdev->dev = dev;\n\tINIT_LIST_HEAD(&kdev->queue_ranges);\n\tINIT_LIST_HEAD(&kdev->qmgrs);\n\tINIT_LIST_HEAD(&kdev->pools);\n\tINIT_LIST_HEAD(&kdev->regions);\n\tINIT_LIST_HEAD(&kdev->pdsps);\n\n\tpm_runtime_enable(&pdev->dev);\n\tret = pm_runtime_resume_and_get(&pdev->dev);\n\tif (ret < 0) {\n\t\tpm_runtime_disable(&pdev->dev);\n\t\tdev_err(dev, \"Failed to enable QMSS\\n\");\n\t\treturn ret;\n\t}\n\n\tif (of_property_read_u32_array(node, \"queue-range\", temp, 2)) {\n\t\tdev_err(dev, \"queue-range not specified\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\tkdev->base_id    = temp[0];\n\tkdev->num_queues = temp[1];\n\n\t \n\tqmgrs =  of_get_child_by_name(node, \"qmgrs\");\n\tif (!qmgrs) {\n\t\tdev_err(dev, \"queue manager info not specified\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\tret = knav_queue_init_qmgrs(kdev, qmgrs);\n\tof_node_put(qmgrs);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tpdsps =  of_get_child_by_name(node, \"pdsps\");\n\tif (pdsps) {\n\t\tret = knav_queue_init_pdsps(kdev, pdsps);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tret = knav_queue_start_pdsps(kdev);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\tof_node_put(pdsps);\n\n\t \n\tqueue_pools = of_get_child_by_name(node, \"queue-pools\");\n\tif (!queue_pools) {\n\t\tdev_err(dev, \"queue-pools not specified\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\tret = knav_setup_queue_pools(kdev, queue_pools);\n\tof_node_put(queue_pools);\n\tif (ret)\n\t\tgoto err;\n\n\tret = knav_get_link_ram(kdev, \"linkram0\", &kdev->link_rams[0]);\n\tif (ret) {\n\t\tdev_err(kdev->dev, \"could not setup linking ram\\n\");\n\t\tgoto err;\n\t}\n\n\tret = knav_get_link_ram(kdev, \"linkram1\", &kdev->link_rams[1]);\n\tif (ret) {\n\t\t \n\t}\n\n\tret = knav_queue_setup_link_ram(kdev);\n\tif (ret)\n\t\tgoto err;\n\n\tregions = of_get_child_by_name(node, \"descriptor-regions\");\n\tif (!regions) {\n\t\tdev_err(dev, \"descriptor-regions not specified\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err;\n\t}\n\tret = knav_queue_setup_regions(kdev, regions);\n\tof_node_put(regions);\n\tif (ret)\n\t\tgoto err;\n\n\tret = knav_queue_init_queues(kdev);\n\tif (ret < 0) {\n\t\tdev_err(dev, \"hwqueue initialization failed\\n\");\n\t\tgoto err;\n\t}\n\n\tdebugfs_create_file(\"qmss\", S_IFREG | S_IRUGO, NULL, NULL,\n\t\t\t    &knav_queue_debug_fops);\n\tdevice_ready = true;\n\treturn 0;\n\nerr:\n\tknav_queue_stop_pdsps(kdev);\n\tknav_queue_free_regions(kdev);\n\tknav_free_queue_ranges(kdev);\n\tpm_runtime_put_sync(&pdev->dev);\n\tpm_runtime_disable(&pdev->dev);\n\treturn ret;\n}\n\nstatic int knav_queue_remove(struct platform_device *pdev)\n{\n\t \n\tpm_runtime_put_sync(&pdev->dev);\n\tpm_runtime_disable(&pdev->dev);\n\treturn 0;\n}\n\nstatic struct platform_driver keystone_qmss_driver = {\n\t.probe\t\t= knav_queue_probe,\n\t.remove\t\t= knav_queue_remove,\n\t.driver\t\t= {\n\t\t.name\t= \"keystone-navigator-qmss\",\n\t\t.of_match_table = keystone_qmss_of_match,\n\t},\n};\nmodule_platform_driver(keystone_qmss_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"TI QMSS driver for Keystone SOCs\");\nMODULE_AUTHOR(\"Sandeep Nair <sandeep_n@ti.com>\");\nMODULE_AUTHOR(\"Santosh Shilimkar <santosh.shilimkar@ti.com>\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}