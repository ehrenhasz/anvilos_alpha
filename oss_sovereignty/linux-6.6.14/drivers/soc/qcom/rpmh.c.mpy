{
  "module_name": "rpmh.c",
  "hash_id": "c1c1b3062fd9ca4fd01f0e11650b42a41daad9f10b6138a5382c9fb27b1a2a79",
  "original_prompt": "Ingested from linux-6.6.14/drivers/soc/qcom/rpmh.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/bug.h>\n#include <linux/interrupt.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/lockdep.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n\n#include <soc/qcom/rpmh.h>\n\n#include \"rpmh-internal.h\"\n\n#define RPMH_TIMEOUT_MS\t\t\tmsecs_to_jiffies(10000)\n\n#define DEFINE_RPMH_MSG_ONSTACK(device, s, q, name)\t\\\n\tstruct rpmh_request name = {\t\t\t\\\n\t\t.msg = {\t\t\t\t\\\n\t\t\t.state = s,\t\t\t\\\n\t\t\t.cmds = name.cmd,\t\t\\\n\t\t\t.num_cmds = 0,\t\t\t\\\n\t\t\t.wait_for_compl = true,\t\t\\\n\t\t},\t\t\t\t\t\\\n\t\t.cmd = { { 0 } },\t\t\t\\\n\t\t.completion = q,\t\t\t\\\n\t\t.dev = device,\t\t\t\t\\\n\t\t.needs_free = false,\t\t\t\t\\\n\t}\n\n#define ctrlr_to_drv(ctrlr) container_of(ctrlr, struct rsc_drv, client)\n\n \nstruct cache_req {\n\tu32 addr;\n\tu32 sleep_val;\n\tu32 wake_val;\n\tstruct list_head list;\n};\n\n \n\nstruct batch_cache_req {\n\tstruct list_head list;\n\tint count;\n\tstruct rpmh_request rpm_msgs[];\n};\n\nstatic struct rpmh_ctrlr *get_rpmh_ctrlr(const struct device *dev)\n{\n\tstruct rsc_drv *drv = dev_get_drvdata(dev->parent);\n\n\treturn &drv->client;\n}\n\nvoid rpmh_tx_done(const struct tcs_request *msg)\n{\n\tstruct rpmh_request *rpm_msg = container_of(msg, struct rpmh_request,\n\t\t\t\t\t\t    msg);\n\tstruct completion *compl = rpm_msg->completion;\n\tbool free = rpm_msg->needs_free;\n\n\tif (!compl)\n\t\tgoto exit;\n\n\t \n\tcomplete(compl);\n\nexit:\n\tif (free)\n\t\tkfree(rpm_msg);\n}\n\nstatic struct cache_req *__find_req(struct rpmh_ctrlr *ctrlr, u32 addr)\n{\n\tstruct cache_req *p, *req = NULL;\n\n\tlist_for_each_entry(p, &ctrlr->cache, list) {\n\t\tif (p->addr == addr) {\n\t\t\treq = p;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn req;\n}\n\nstatic struct cache_req *cache_rpm_request(struct rpmh_ctrlr *ctrlr,\n\t\t\t\t\t   enum rpmh_state state,\n\t\t\t\t\t   struct tcs_cmd *cmd)\n{\n\tstruct cache_req *req;\n\tunsigned long flags;\n\tu32 old_sleep_val, old_wake_val;\n\n\tspin_lock_irqsave(&ctrlr->cache_lock, flags);\n\treq = __find_req(ctrlr, cmd->addr);\n\tif (req)\n\t\tgoto existing;\n\n\treq = kzalloc(sizeof(*req), GFP_ATOMIC);\n\tif (!req) {\n\t\treq = ERR_PTR(-ENOMEM);\n\t\tgoto unlock;\n\t}\n\n\treq->addr = cmd->addr;\n\treq->sleep_val = req->wake_val = UINT_MAX;\n\tlist_add_tail(&req->list, &ctrlr->cache);\n\nexisting:\n\told_sleep_val = req->sleep_val;\n\told_wake_val = req->wake_val;\n\n\tswitch (state) {\n\tcase RPMH_ACTIVE_ONLY_STATE:\n\tcase RPMH_WAKE_ONLY_STATE:\n\t\treq->wake_val = cmd->data;\n\t\tbreak;\n\tcase RPMH_SLEEP_STATE:\n\t\treq->sleep_val = cmd->data;\n\t\tbreak;\n\t}\n\n\tctrlr->dirty |= (req->sleep_val != old_sleep_val ||\n\t\t\t req->wake_val != old_wake_val) &&\n\t\t\t req->sleep_val != UINT_MAX &&\n\t\t\t req->wake_val != UINT_MAX;\n\nunlock:\n\tspin_unlock_irqrestore(&ctrlr->cache_lock, flags);\n\n\treturn req;\n}\n\n \nstatic int __rpmh_write(const struct device *dev, enum rpmh_state state,\n\t\t\tstruct rpmh_request *rpm_msg)\n{\n\tstruct rpmh_ctrlr *ctrlr = get_rpmh_ctrlr(dev);\n\tint ret = -EINVAL;\n\tstruct cache_req *req;\n\tint i;\n\n\t \n\tfor (i = 0; i < rpm_msg->msg.num_cmds; i++) {\n\t\treq = cache_rpm_request(ctrlr, state, &rpm_msg->msg.cmds[i]);\n\t\tif (IS_ERR(req))\n\t\t\treturn PTR_ERR(req);\n\t}\n\n\tif (state == RPMH_ACTIVE_ONLY_STATE) {\n\t\tWARN_ON(irqs_disabled());\n\t\tret = rpmh_rsc_send_data(ctrlr_to_drv(ctrlr), &rpm_msg->msg);\n\t} else {\n\t\t \n\t\tret = 0;\n\t\trpmh_tx_done(&rpm_msg->msg);\n\t}\n\n\treturn ret;\n}\n\nstatic int __fill_rpmh_msg(struct rpmh_request *req, enum rpmh_state state,\n\t\tconst struct tcs_cmd *cmd, u32 n)\n{\n\tif (!cmd || !n || n > MAX_RPMH_PAYLOAD)\n\t\treturn -EINVAL;\n\n\tmemcpy(req->cmd, cmd, n * sizeof(*cmd));\n\n\treq->msg.state = state;\n\treq->msg.cmds = req->cmd;\n\treq->msg.num_cmds = n;\n\n\treturn 0;\n}\n\n \nint rpmh_write_async(const struct device *dev, enum rpmh_state state,\n\t\t     const struct tcs_cmd *cmd, u32 n)\n{\n\tstruct rpmh_request *rpm_msg;\n\tint ret;\n\n\trpm_msg = kzalloc(sizeof(*rpm_msg), GFP_ATOMIC);\n\tif (!rpm_msg)\n\t\treturn -ENOMEM;\n\trpm_msg->needs_free = true;\n\n\tret = __fill_rpmh_msg(rpm_msg, state, cmd, n);\n\tif (ret) {\n\t\tkfree(rpm_msg);\n\t\treturn ret;\n\t}\n\n\treturn __rpmh_write(dev, state, rpm_msg);\n}\nEXPORT_SYMBOL(rpmh_write_async);\n\n \nint rpmh_write(const struct device *dev, enum rpmh_state state,\n\t       const struct tcs_cmd *cmd, u32 n)\n{\n\tDECLARE_COMPLETION_ONSTACK(compl);\n\tDEFINE_RPMH_MSG_ONSTACK(dev, state, &compl, rpm_msg);\n\tint ret;\n\n\tret = __fill_rpmh_msg(&rpm_msg, state, cmd, n);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __rpmh_write(dev, state, &rpm_msg);\n\tif (ret)\n\t\treturn ret;\n\n\tret = wait_for_completion_timeout(&compl, RPMH_TIMEOUT_MS);\n\tWARN_ON(!ret);\n\treturn (ret > 0) ? 0 : -ETIMEDOUT;\n}\nEXPORT_SYMBOL(rpmh_write);\n\nstatic void cache_batch(struct rpmh_ctrlr *ctrlr, struct batch_cache_req *req)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctrlr->cache_lock, flags);\n\tlist_add_tail(&req->list, &ctrlr->batch_cache);\n\tctrlr->dirty = true;\n\tspin_unlock_irqrestore(&ctrlr->cache_lock, flags);\n}\n\nstatic int flush_batch(struct rpmh_ctrlr *ctrlr)\n{\n\tstruct batch_cache_req *req;\n\tconst struct rpmh_request *rpm_msg;\n\tint ret = 0;\n\tint i;\n\n\t \n\tlist_for_each_entry(req, &ctrlr->batch_cache, list) {\n\t\tfor (i = 0; i < req->count; i++) {\n\t\t\trpm_msg = req->rpm_msgs + i;\n\t\t\tret = rpmh_rsc_write_ctrl_data(ctrlr_to_drv(ctrlr),\n\t\t\t\t\t\t       &rpm_msg->msg);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nint rpmh_write_batch(const struct device *dev, enum rpmh_state state,\n\t\t     const struct tcs_cmd *cmd, u32 *n)\n{\n\tstruct batch_cache_req *req;\n\tstruct rpmh_request *rpm_msgs;\n\tstruct completion *compls;\n\tstruct rpmh_ctrlr *ctrlr = get_rpmh_ctrlr(dev);\n\tunsigned long time_left;\n\tint count = 0;\n\tint ret, i;\n\tvoid *ptr;\n\n\tif (!cmd || !n)\n\t\treturn -EINVAL;\n\n\twhile (n[count] > 0)\n\t\tcount++;\n\tif (!count)\n\t\treturn -EINVAL;\n\n\tptr = kzalloc(sizeof(*req) +\n\t\t      count * (sizeof(req->rpm_msgs[0]) + sizeof(*compls)),\n\t\t      GFP_ATOMIC);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\treq = ptr;\n\tcompls = ptr + sizeof(*req) + count * sizeof(*rpm_msgs);\n\n\treq->count = count;\n\trpm_msgs = req->rpm_msgs;\n\n\tfor (i = 0; i < count; i++) {\n\t\t__fill_rpmh_msg(rpm_msgs + i, state, cmd, n[i]);\n\t\tcmd += n[i];\n\t}\n\n\tif (state != RPMH_ACTIVE_ONLY_STATE) {\n\t\tcache_batch(ctrlr, req);\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct completion *compl = &compls[i];\n\n\t\tinit_completion(compl);\n\t\trpm_msgs[i].completion = compl;\n\t\tret = rpmh_rsc_send_data(ctrlr_to_drv(ctrlr), &rpm_msgs[i].msg);\n\t\tif (ret) {\n\t\t\tpr_err(\"Error(%d) sending RPMH message addr=%#x\\n\",\n\t\t\t       ret, rpm_msgs[i].msg.cmds[0].addr);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttime_left = RPMH_TIMEOUT_MS;\n\twhile (i--) {\n\t\ttime_left = wait_for_completion_timeout(&compls[i], time_left);\n\t\tif (!time_left) {\n\t\t\t \n\t\t\tWARN_ON(1);\n\t\t\tret = -ETIMEDOUT;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\nexit:\n\tkfree(ptr);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(rpmh_write_batch);\n\nstatic int is_req_valid(struct cache_req *req)\n{\n\treturn (req->sleep_val != UINT_MAX &&\n\t\treq->wake_val != UINT_MAX &&\n\t\treq->sleep_val != req->wake_val);\n}\n\nstatic int send_single(struct rpmh_ctrlr *ctrlr, enum rpmh_state state,\n\t\t       u32 addr, u32 data)\n{\n\tDEFINE_RPMH_MSG_ONSTACK(NULL, state, NULL, rpm_msg);\n\n\t \n\trpm_msg.msg.wait_for_compl = (state == RPMH_WAKE_ONLY_STATE);\n\trpm_msg.cmd[0].addr = addr;\n\trpm_msg.cmd[0].data = data;\n\trpm_msg.msg.num_cmds = 1;\n\n\treturn rpmh_rsc_write_ctrl_data(ctrlr_to_drv(ctrlr), &rpm_msg.msg);\n}\n\n \nint rpmh_flush(struct rpmh_ctrlr *ctrlr)\n{\n\tstruct cache_req *p;\n\tint ret = 0;\n\n\tlockdep_assert_irqs_disabled();\n\n\t \n\tif (!spin_trylock(&ctrlr->cache_lock))\n\t\treturn -EBUSY;\n\n\tif (!ctrlr->dirty) {\n\t\tpr_debug(\"Skipping flush, TCS has latest data.\\n\");\n\t\tgoto write_next_wakeup;\n\t}\n\n\t \n\trpmh_rsc_invalidate(ctrlr_to_drv(ctrlr));\n\n\t \n\tret = flush_batch(ctrlr);\n\tif (ret)\n\t\tgoto exit;\n\n\tlist_for_each_entry(p, &ctrlr->cache, list) {\n\t\tif (!is_req_valid(p)) {\n\t\t\tpr_debug(\"%s: skipping RPMH req: a:%#x s:%#x w:%#x\",\n\t\t\t\t __func__, p->addr, p->sleep_val, p->wake_val);\n\t\t\tcontinue;\n\t\t}\n\t\tret = send_single(ctrlr, RPMH_SLEEP_STATE, p->addr,\n\t\t\t\t  p->sleep_val);\n\t\tif (ret)\n\t\t\tgoto exit;\n\t\tret = send_single(ctrlr, RPMH_WAKE_ONLY_STATE, p->addr,\n\t\t\t\t  p->wake_val);\n\t\tif (ret)\n\t\t\tgoto exit;\n\t}\n\n\tctrlr->dirty = false;\n\nwrite_next_wakeup:\n\trpmh_rsc_write_next_wakeup(ctrlr_to_drv(ctrlr));\nexit:\n\tspin_unlock(&ctrlr->cache_lock);\n\treturn ret;\n}\n\n \nvoid rpmh_invalidate(const struct device *dev)\n{\n\tstruct rpmh_ctrlr *ctrlr = get_rpmh_ctrlr(dev);\n\tstruct batch_cache_req *req, *tmp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctrlr->cache_lock, flags);\n\tlist_for_each_entry_safe(req, tmp, &ctrlr->batch_cache, list)\n\t\tkfree(req);\n\tINIT_LIST_HEAD(&ctrlr->batch_cache);\n\tctrlr->dirty = true;\n\tspin_unlock_irqrestore(&ctrlr->cache_lock, flags);\n}\nEXPORT_SYMBOL(rpmh_invalidate);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}