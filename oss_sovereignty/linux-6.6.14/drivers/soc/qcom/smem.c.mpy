{
  "module_name": "smem.c",
  "hash_id": "676cca50945cd8ad8736e6b02e6a55da666b1c7500ea047b16b20caac864b6a6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/soc/qcom/smem.c",
  "human_readable_source": "\n \n\n#include <linux/hwspinlock.h>\n#include <linux/io.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/of_reserved_mem.h>\n#include <linux/platform_device.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/soc/qcom/smem.h>\n#include <linux/soc/qcom/socinfo.h>\n\n \n\n \n#define SMEM_MASTER_SBL_VERSION_INDEX\t7\n#define SMEM_GLOBAL_HEAP_VERSION\t11\n#define SMEM_GLOBAL_PART_VERSION\t12\n\n \n#define SMEM_ITEM_LAST_FIXED\t8\n\n \n#define SMEM_ITEM_COUNT\t\t512\n\n \n#define SMEM_HOST_APPS\t\t0\n\n \n#define SMEM_GLOBAL_HOST\t0xfffe\n\n \n#define SMEM_HOST_COUNT\t\t20\n\n \nstruct smem_proc_comm {\n\t__le32 command;\n\t__le32 status;\n\t__le32 params[2];\n};\n\n \nstruct smem_global_entry {\n\t__le32 allocated;\n\t__le32 offset;\n\t__le32 size;\n\t__le32 aux_base;  \n};\n#define AUX_BASE_MASK\t\t0xfffffffc\n\n \nstruct smem_header {\n\tstruct smem_proc_comm proc_comm[4];\n\t__le32 version[32];\n\t__le32 initialized;\n\t__le32 free_offset;\n\t__le32 available;\n\t__le32 reserved;\n\tstruct smem_global_entry toc[SMEM_ITEM_COUNT];\n};\n\n \nstruct smem_ptable_entry {\n\t__le32 offset;\n\t__le32 size;\n\t__le32 flags;\n\t__le16 host0;\n\t__le16 host1;\n\t__le32 cacheline;\n\t__le32 reserved[7];\n};\n\n \nstruct smem_ptable {\n\tu8 magic[4];\n\t__le32 version;\n\t__le32 num_entries;\n\t__le32 reserved[5];\n\tstruct smem_ptable_entry entry[];\n};\n\nstatic const u8 SMEM_PTABLE_MAGIC[] = { 0x24, 0x54, 0x4f, 0x43 };  \n\n \nstruct smem_partition_header {\n\tu8 magic[4];\n\t__le16 host0;\n\t__le16 host1;\n\t__le32 size;\n\t__le32 offset_free_uncached;\n\t__le32 offset_free_cached;\n\t__le32 reserved[3];\n};\n\n \nstruct smem_partition {\n\tvoid __iomem *virt_base;\n\tphys_addr_t phys_base;\n\tsize_t cacheline;\n\tsize_t size;\n};\n\nstatic const u8 SMEM_PART_MAGIC[] = { 0x24, 0x50, 0x52, 0x54 };\n\n \nstruct smem_private_entry {\n\tu16 canary;  \n\t__le16 item;\n\t__le32 size;  \n\t__le16 padding_data;\n\t__le16 padding_hdr;\n\t__le32 reserved;\n};\n#define SMEM_PRIVATE_CANARY\t0xa5a5\n\n \nstruct smem_info {\n\tu8 magic[4];\n\t__le32 size;\n\t__le32 base_addr;\n\t__le32 reserved;\n\t__le16 num_items;\n};\n\nstatic const u8 SMEM_INFO_MAGIC[] = { 0x53, 0x49, 0x49, 0x49 };  \n\n \nstruct smem_region {\n\tphys_addr_t aux_base;\n\tvoid __iomem *virt_base;\n\tsize_t size;\n};\n\n \nstruct qcom_smem {\n\tstruct device *dev;\n\n\tstruct hwspinlock *hwlock;\n\n\tu32 item_count;\n\tstruct platform_device *socinfo;\n\tstruct smem_ptable *ptable;\n\tstruct smem_partition global_partition;\n\tstruct smem_partition partitions[SMEM_HOST_COUNT];\n\n\tunsigned num_regions;\n\tstruct smem_region regions[];\n};\n\nstatic void *\nphdr_to_last_uncached_entry(struct smem_partition_header *phdr)\n{\n\tvoid *p = phdr;\n\n\treturn p + le32_to_cpu(phdr->offset_free_uncached);\n}\n\nstatic struct smem_private_entry *\nphdr_to_first_cached_entry(struct smem_partition_header *phdr,\n\t\t\t\t\tsize_t cacheline)\n{\n\tvoid *p = phdr;\n\tstruct smem_private_entry *e;\n\n\treturn p + le32_to_cpu(phdr->size) - ALIGN(sizeof(*e), cacheline);\n}\n\nstatic void *\nphdr_to_last_cached_entry(struct smem_partition_header *phdr)\n{\n\tvoid *p = phdr;\n\n\treturn p + le32_to_cpu(phdr->offset_free_cached);\n}\n\nstatic struct smem_private_entry *\nphdr_to_first_uncached_entry(struct smem_partition_header *phdr)\n{\n\tvoid *p = phdr;\n\n\treturn p + sizeof(*phdr);\n}\n\nstatic struct smem_private_entry *\nuncached_entry_next(struct smem_private_entry *e)\n{\n\tvoid *p = e;\n\n\treturn p + sizeof(*e) + le16_to_cpu(e->padding_hdr) +\n\t       le32_to_cpu(e->size);\n}\n\nstatic struct smem_private_entry *\ncached_entry_next(struct smem_private_entry *e, size_t cacheline)\n{\n\tvoid *p = e;\n\n\treturn p - le32_to_cpu(e->size) - ALIGN(sizeof(*e), cacheline);\n}\n\nstatic void *uncached_entry_to_item(struct smem_private_entry *e)\n{\n\tvoid *p = e;\n\n\treturn p + sizeof(*e) + le16_to_cpu(e->padding_hdr);\n}\n\nstatic void *cached_entry_to_item(struct smem_private_entry *e)\n{\n\tvoid *p = e;\n\n\treturn p - le32_to_cpu(e->size);\n}\n\n \nstatic struct qcom_smem *__smem;\n\n \n#define HWSPINLOCK_TIMEOUT\t1000\n\n \nbool qcom_smem_is_available(void)\n{\n\treturn !!__smem;\n}\nEXPORT_SYMBOL(qcom_smem_is_available);\n\nstatic int qcom_smem_alloc_private(struct qcom_smem *smem,\n\t\t\t\t   struct smem_partition *part,\n\t\t\t\t   unsigned item,\n\t\t\t\t   size_t size)\n{\n\tstruct smem_private_entry *hdr, *end;\n\tstruct smem_partition_header *phdr;\n\tsize_t alloc_size;\n\tvoid *cached;\n\tvoid *p_end;\n\n\tphdr = (struct smem_partition_header __force *)part->virt_base;\n\tp_end = (void *)phdr + part->size;\n\n\thdr = phdr_to_first_uncached_entry(phdr);\n\tend = phdr_to_last_uncached_entry(phdr);\n\tcached = phdr_to_last_cached_entry(phdr);\n\n\tif (WARN_ON((void *)end > p_end || cached > p_end))\n\t\treturn -EINVAL;\n\n\twhile (hdr < end) {\n\t\tif (hdr->canary != SMEM_PRIVATE_CANARY)\n\t\t\tgoto bad_canary;\n\t\tif (le16_to_cpu(hdr->item) == item)\n\t\t\treturn -EEXIST;\n\n\t\thdr = uncached_entry_next(hdr);\n\t}\n\n\tif (WARN_ON((void *)hdr > p_end))\n\t\treturn -EINVAL;\n\n\t \n\talloc_size = sizeof(*hdr) + ALIGN(size, 8);\n\tif ((void *)hdr + alloc_size > cached) {\n\t\tdev_err(smem->dev, \"Out of memory\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\thdr->canary = SMEM_PRIVATE_CANARY;\n\thdr->item = cpu_to_le16(item);\n\thdr->size = cpu_to_le32(ALIGN(size, 8));\n\thdr->padding_data = cpu_to_le16(le32_to_cpu(hdr->size) - size);\n\thdr->padding_hdr = 0;\n\n\t \n\twmb();\n\tle32_add_cpu(&phdr->offset_free_uncached, alloc_size);\n\n\treturn 0;\nbad_canary:\n\tdev_err(smem->dev, \"Found invalid canary in hosts %hu:%hu partition\\n\",\n\t\tle16_to_cpu(phdr->host0), le16_to_cpu(phdr->host1));\n\n\treturn -EINVAL;\n}\n\nstatic int qcom_smem_alloc_global(struct qcom_smem *smem,\n\t\t\t\t  unsigned item,\n\t\t\t\t  size_t size)\n{\n\tstruct smem_global_entry *entry;\n\tstruct smem_header *header;\n\n\theader = smem->regions[0].virt_base;\n\tentry = &header->toc[item];\n\tif (entry->allocated)\n\t\treturn -EEXIST;\n\n\tsize = ALIGN(size, 8);\n\tif (WARN_ON(size > le32_to_cpu(header->available)))\n\t\treturn -ENOMEM;\n\n\tentry->offset = header->free_offset;\n\tentry->size = cpu_to_le32(size);\n\n\t \n\twmb();\n\tentry->allocated = cpu_to_le32(1);\n\n\tle32_add_cpu(&header->free_offset, size);\n\tle32_add_cpu(&header->available, -size);\n\n\treturn 0;\n}\n\n \nint qcom_smem_alloc(unsigned host, unsigned item, size_t size)\n{\n\tstruct smem_partition *part;\n\tunsigned long flags;\n\tint ret;\n\n\tif (!__smem)\n\t\treturn -EPROBE_DEFER;\n\n\tif (item < SMEM_ITEM_LAST_FIXED) {\n\t\tdev_err(__smem->dev,\n\t\t\t\"Rejecting allocation of static entry %d\\n\", item);\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN_ON(item >= __smem->item_count))\n\t\treturn -EINVAL;\n\n\tret = hwspin_lock_timeout_irqsave(__smem->hwlock,\n\t\t\t\t\t  HWSPINLOCK_TIMEOUT,\n\t\t\t\t\t  &flags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (host < SMEM_HOST_COUNT && __smem->partitions[host].virt_base) {\n\t\tpart = &__smem->partitions[host];\n\t\tret = qcom_smem_alloc_private(__smem, part, item, size);\n\t} else if (__smem->global_partition.virt_base) {\n\t\tpart = &__smem->global_partition;\n\t\tret = qcom_smem_alloc_private(__smem, part, item, size);\n\t} else {\n\t\tret = qcom_smem_alloc_global(__smem, item, size);\n\t}\n\n\thwspin_unlock_irqrestore(__smem->hwlock, &flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(qcom_smem_alloc);\n\nstatic void *qcom_smem_get_global(struct qcom_smem *smem,\n\t\t\t\t  unsigned item,\n\t\t\t\t  size_t *size)\n{\n\tstruct smem_header *header;\n\tstruct smem_region *region;\n\tstruct smem_global_entry *entry;\n\tu64 entry_offset;\n\tu32 e_size;\n\tu32 aux_base;\n\tunsigned i;\n\n\theader = smem->regions[0].virt_base;\n\tentry = &header->toc[item];\n\tif (!entry->allocated)\n\t\treturn ERR_PTR(-ENXIO);\n\n\taux_base = le32_to_cpu(entry->aux_base) & AUX_BASE_MASK;\n\n\tfor (i = 0; i < smem->num_regions; i++) {\n\t\tregion = &smem->regions[i];\n\n\t\tif ((u32)region->aux_base == aux_base || !aux_base) {\n\t\t\te_size = le32_to_cpu(entry->size);\n\t\t\tentry_offset = le32_to_cpu(entry->offset);\n\n\t\t\tif (WARN_ON(e_size + entry_offset > region->size))\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\tif (size != NULL)\n\t\t\t\t*size = e_size;\n\n\t\t\treturn region->virt_base + entry_offset;\n\t\t}\n\t}\n\n\treturn ERR_PTR(-ENOENT);\n}\n\nstatic void *qcom_smem_get_private(struct qcom_smem *smem,\n\t\t\t\t   struct smem_partition *part,\n\t\t\t\t   unsigned item,\n\t\t\t\t   size_t *size)\n{\n\tstruct smem_private_entry *e, *end;\n\tstruct smem_partition_header *phdr;\n\tvoid *item_ptr, *p_end;\n\tu32 padding_data;\n\tu32 e_size;\n\n\tphdr = (struct smem_partition_header __force *)part->virt_base;\n\tp_end = (void *)phdr + part->size;\n\n\te = phdr_to_first_uncached_entry(phdr);\n\tend = phdr_to_last_uncached_entry(phdr);\n\n\twhile (e < end) {\n\t\tif (e->canary != SMEM_PRIVATE_CANARY)\n\t\t\tgoto invalid_canary;\n\n\t\tif (le16_to_cpu(e->item) == item) {\n\t\t\tif (size != NULL) {\n\t\t\t\te_size = le32_to_cpu(e->size);\n\t\t\t\tpadding_data = le16_to_cpu(e->padding_data);\n\n\t\t\t\tif (WARN_ON(e_size > part->size || padding_data > e_size))\n\t\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\t\t*size = e_size - padding_data;\n\t\t\t}\n\n\t\t\titem_ptr = uncached_entry_to_item(e);\n\t\t\tif (WARN_ON(item_ptr > p_end))\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\treturn item_ptr;\n\t\t}\n\n\t\te = uncached_entry_next(e);\n\t}\n\n\tif (WARN_ON((void *)e > p_end))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\n\te = phdr_to_first_cached_entry(phdr, part->cacheline);\n\tend = phdr_to_last_cached_entry(phdr);\n\n\tif (WARN_ON((void *)e < (void *)phdr || (void *)end > p_end))\n\t\treturn ERR_PTR(-EINVAL);\n\n\twhile (e > end) {\n\t\tif (e->canary != SMEM_PRIVATE_CANARY)\n\t\t\tgoto invalid_canary;\n\n\t\tif (le16_to_cpu(e->item) == item) {\n\t\t\tif (size != NULL) {\n\t\t\t\te_size = le32_to_cpu(e->size);\n\t\t\t\tpadding_data = le16_to_cpu(e->padding_data);\n\n\t\t\t\tif (WARN_ON(e_size > part->size || padding_data > e_size))\n\t\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\t\t*size = e_size - padding_data;\n\t\t\t}\n\n\t\t\titem_ptr = cached_entry_to_item(e);\n\t\t\tif (WARN_ON(item_ptr < (void *)phdr))\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t\treturn item_ptr;\n\t\t}\n\n\t\te = cached_entry_next(e, part->cacheline);\n\t}\n\n\tif (WARN_ON((void *)e < (void *)phdr))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn ERR_PTR(-ENOENT);\n\ninvalid_canary:\n\tdev_err(smem->dev, \"Found invalid canary in hosts %hu:%hu partition\\n\",\n\t\t\tle16_to_cpu(phdr->host0), le16_to_cpu(phdr->host1));\n\n\treturn ERR_PTR(-EINVAL);\n}\n\n \nvoid *qcom_smem_get(unsigned host, unsigned item, size_t *size)\n{\n\tstruct smem_partition *part;\n\tunsigned long flags;\n\tint ret;\n\tvoid *ptr = ERR_PTR(-EPROBE_DEFER);\n\n\tif (!__smem)\n\t\treturn ptr;\n\n\tif (WARN_ON(item >= __smem->item_count))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tret = hwspin_lock_timeout_irqsave(__smem->hwlock,\n\t\t\t\t\t  HWSPINLOCK_TIMEOUT,\n\t\t\t\t\t  &flags);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (host < SMEM_HOST_COUNT && __smem->partitions[host].virt_base) {\n\t\tpart = &__smem->partitions[host];\n\t\tptr = qcom_smem_get_private(__smem, part, item, size);\n\t} else if (__smem->global_partition.virt_base) {\n\t\tpart = &__smem->global_partition;\n\t\tptr = qcom_smem_get_private(__smem, part, item, size);\n\t} else {\n\t\tptr = qcom_smem_get_global(__smem, item, size);\n\t}\n\n\thwspin_unlock_irqrestore(__smem->hwlock, &flags);\n\n\treturn ptr;\n\n}\nEXPORT_SYMBOL_GPL(qcom_smem_get);\n\n \nint qcom_smem_get_free_space(unsigned host)\n{\n\tstruct smem_partition *part;\n\tstruct smem_partition_header *phdr;\n\tstruct smem_header *header;\n\tunsigned ret;\n\n\tif (!__smem)\n\t\treturn -EPROBE_DEFER;\n\n\tif (host < SMEM_HOST_COUNT && __smem->partitions[host].virt_base) {\n\t\tpart = &__smem->partitions[host];\n\t\tphdr = part->virt_base;\n\t\tret = le32_to_cpu(phdr->offset_free_cached) -\n\t\t      le32_to_cpu(phdr->offset_free_uncached);\n\n\t\tif (ret > le32_to_cpu(part->size))\n\t\t\treturn -EINVAL;\n\t} else if (__smem->global_partition.virt_base) {\n\t\tpart = &__smem->global_partition;\n\t\tphdr = part->virt_base;\n\t\tret = le32_to_cpu(phdr->offset_free_cached) -\n\t\t      le32_to_cpu(phdr->offset_free_uncached);\n\n\t\tif (ret > le32_to_cpu(part->size))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\theader = __smem->regions[0].virt_base;\n\t\tret = le32_to_cpu(header->available);\n\n\t\tif (ret > __smem->regions[0].size)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(qcom_smem_get_free_space);\n\nstatic bool addr_in_range(void __iomem *base, size_t size, void *addr)\n{\n\treturn base && ((void __iomem *)addr >= base && (void __iomem *)addr < base + size);\n}\n\n \nphys_addr_t qcom_smem_virt_to_phys(void *p)\n{\n\tstruct smem_partition *part;\n\tstruct smem_region *area;\n\tu64 offset;\n\tu32 i;\n\n\tfor (i = 0; i < SMEM_HOST_COUNT; i++) {\n\t\tpart = &__smem->partitions[i];\n\n\t\tif (addr_in_range(part->virt_base, part->size, p)) {\n\t\t\toffset = p - part->virt_base;\n\n\t\t\treturn (phys_addr_t)part->phys_base + offset;\n\t\t}\n\t}\n\n\tpart = &__smem->global_partition;\n\n\tif (addr_in_range(part->virt_base, part->size, p)) {\n\t\toffset = p - part->virt_base;\n\n\t\treturn (phys_addr_t)part->phys_base + offset;\n\t}\n\n\tfor (i = 0; i < __smem->num_regions; i++) {\n\t\tarea = &__smem->regions[i];\n\n\t\tif (addr_in_range(area->virt_base, area->size, p)) {\n\t\t\toffset = p - area->virt_base;\n\n\t\t\treturn (phys_addr_t)area->aux_base + offset;\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(qcom_smem_virt_to_phys);\n\n \nint qcom_smem_get_soc_id(u32 *id)\n{\n\tstruct socinfo *info;\n\n\tinfo = qcom_smem_get(QCOM_SMEM_HOST_ANY, SMEM_HW_SW_BUILD_ID, NULL);\n\tif (IS_ERR(info))\n\t\treturn PTR_ERR(info);\n\n\t*id = __le32_to_cpu(info->id);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(qcom_smem_get_soc_id);\n\nstatic int qcom_smem_get_sbl_version(struct qcom_smem *smem)\n{\n\tstruct smem_header *header;\n\t__le32 *versions;\n\n\theader = smem->regions[0].virt_base;\n\tversions = header->version;\n\n\treturn le32_to_cpu(versions[SMEM_MASTER_SBL_VERSION_INDEX]);\n}\n\nstatic struct smem_ptable *qcom_smem_get_ptable(struct qcom_smem *smem)\n{\n\tstruct smem_ptable *ptable;\n\tu32 version;\n\n\tptable = smem->ptable;\n\tif (memcmp(ptable->magic, SMEM_PTABLE_MAGIC, sizeof(ptable->magic)))\n\t\treturn ERR_PTR(-ENOENT);\n\n\tversion = le32_to_cpu(ptable->version);\n\tif (version != 1) {\n\t\tdev_err(smem->dev,\n\t\t\t\"Unsupported partition header version %d\\n\", version);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\treturn ptable;\n}\n\nstatic u32 qcom_smem_get_item_count(struct qcom_smem *smem)\n{\n\tstruct smem_ptable *ptable;\n\tstruct smem_info *info;\n\n\tptable = qcom_smem_get_ptable(smem);\n\tif (IS_ERR_OR_NULL(ptable))\n\t\treturn SMEM_ITEM_COUNT;\n\n\tinfo = (struct smem_info *)&ptable->entry[ptable->num_entries];\n\tif (memcmp(info->magic, SMEM_INFO_MAGIC, sizeof(info->magic)))\n\t\treturn SMEM_ITEM_COUNT;\n\n\treturn le16_to_cpu(info->num_items);\n}\n\n \nstatic struct smem_partition_header *\nqcom_smem_partition_header(struct qcom_smem *smem,\n\t\tstruct smem_ptable_entry *entry, u16 host0, u16 host1)\n{\n\tstruct smem_partition_header *header;\n\tu32 phys_addr;\n\tu32 size;\n\n\tphys_addr = smem->regions[0].aux_base + le32_to_cpu(entry->offset);\n\theader = devm_ioremap_wc(smem->dev, phys_addr, le32_to_cpu(entry->size));\n\n\tif (!header)\n\t\treturn NULL;\n\n\tif (memcmp(header->magic, SMEM_PART_MAGIC, sizeof(header->magic))) {\n\t\tdev_err(smem->dev, \"bad partition magic %4ph\\n\", header->magic);\n\t\treturn NULL;\n\t}\n\n\tif (host0 != le16_to_cpu(header->host0)) {\n\t\tdev_err(smem->dev, \"bad host0 (%hu != %hu)\\n\",\n\t\t\t\thost0, le16_to_cpu(header->host0));\n\t\treturn NULL;\n\t}\n\tif (host1 != le16_to_cpu(header->host1)) {\n\t\tdev_err(smem->dev, \"bad host1 (%hu != %hu)\\n\",\n\t\t\t\thost1, le16_to_cpu(header->host1));\n\t\treturn NULL;\n\t}\n\n\tsize = le32_to_cpu(header->size);\n\tif (size != le32_to_cpu(entry->size)) {\n\t\tdev_err(smem->dev, \"bad partition size (%u != %u)\\n\",\n\t\t\tsize, le32_to_cpu(entry->size));\n\t\treturn NULL;\n\t}\n\n\tif (le32_to_cpu(header->offset_free_uncached) > size) {\n\t\tdev_err(smem->dev, \"bad partition free uncached (%u > %u)\\n\",\n\t\t\tle32_to_cpu(header->offset_free_uncached), size);\n\t\treturn NULL;\n\t}\n\n\treturn header;\n}\n\nstatic int qcom_smem_set_global_partition(struct qcom_smem *smem)\n{\n\tstruct smem_partition_header *header;\n\tstruct smem_ptable_entry *entry;\n\tstruct smem_ptable *ptable;\n\tbool found = false;\n\tint i;\n\n\tif (smem->global_partition.virt_base) {\n\t\tdev_err(smem->dev, \"Already found the global partition\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tptable = qcom_smem_get_ptable(smem);\n\tif (IS_ERR(ptable))\n\t\treturn PTR_ERR(ptable);\n\n\tfor (i = 0; i < le32_to_cpu(ptable->num_entries); i++) {\n\t\tentry = &ptable->entry[i];\n\t\tif (!le32_to_cpu(entry->offset))\n\t\t\tcontinue;\n\t\tif (!le32_to_cpu(entry->size))\n\t\t\tcontinue;\n\n\t\tif (le16_to_cpu(entry->host0) != SMEM_GLOBAL_HOST)\n\t\t\tcontinue;\n\n\t\tif (le16_to_cpu(entry->host1) == SMEM_GLOBAL_HOST) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found) {\n\t\tdev_err(smem->dev, \"Missing entry for global partition\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\theader = qcom_smem_partition_header(smem, entry,\n\t\t\t\tSMEM_GLOBAL_HOST, SMEM_GLOBAL_HOST);\n\tif (!header)\n\t\treturn -EINVAL;\n\n\tsmem->global_partition.virt_base = (void __iomem *)header;\n\tsmem->global_partition.phys_base = smem->regions[0].aux_base +\n\t\t\t\t\t\t\t\tle32_to_cpu(entry->offset);\n\tsmem->global_partition.size = le32_to_cpu(entry->size);\n\tsmem->global_partition.cacheline = le32_to_cpu(entry->cacheline);\n\n\treturn 0;\n}\n\nstatic int\nqcom_smem_enumerate_partitions(struct qcom_smem *smem, u16 local_host)\n{\n\tstruct smem_partition_header *header;\n\tstruct smem_ptable_entry *entry;\n\tstruct smem_ptable *ptable;\n\tu16 remote_host;\n\tu16 host0, host1;\n\tint i;\n\n\tptable = qcom_smem_get_ptable(smem);\n\tif (IS_ERR(ptable))\n\t\treturn PTR_ERR(ptable);\n\n\tfor (i = 0; i < le32_to_cpu(ptable->num_entries); i++) {\n\t\tentry = &ptable->entry[i];\n\t\tif (!le32_to_cpu(entry->offset))\n\t\t\tcontinue;\n\t\tif (!le32_to_cpu(entry->size))\n\t\t\tcontinue;\n\n\t\thost0 = le16_to_cpu(entry->host0);\n\t\thost1 = le16_to_cpu(entry->host1);\n\t\tif (host0 == local_host)\n\t\t\tremote_host = host1;\n\t\telse if (host1 == local_host)\n\t\t\tremote_host = host0;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (remote_host >= SMEM_HOST_COUNT) {\n\t\t\tdev_err(smem->dev, \"bad host %u\\n\", remote_host);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (smem->partitions[remote_host].virt_base) {\n\t\t\tdev_err(smem->dev, \"duplicate host %u\\n\", remote_host);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\theader = qcom_smem_partition_header(smem, entry, host0, host1);\n\t\tif (!header)\n\t\t\treturn -EINVAL;\n\n\t\tsmem->partitions[remote_host].virt_base = (void __iomem *)header;\n\t\tsmem->partitions[remote_host].phys_base = smem->regions[0].aux_base +\n\t\t\t\t\t\t\t\t\t\tle32_to_cpu(entry->offset);\n\t\tsmem->partitions[remote_host].size = le32_to_cpu(entry->size);\n\t\tsmem->partitions[remote_host].cacheline = le32_to_cpu(entry->cacheline);\n\t}\n\n\treturn 0;\n}\n\nstatic int qcom_smem_map_toc(struct qcom_smem *smem, struct smem_region *region)\n{\n\tu32 ptable_start;\n\n\t \n\tregion->virt_base = devm_ioremap_wc(smem->dev, region->aux_base, SZ_4K);\n\tptable_start = region->aux_base + region->size - SZ_4K;\n\t \n\tsmem->ptable = devm_ioremap_wc(smem->dev, ptable_start, SZ_4K);\n\n\tif (!region->virt_base || !smem->ptable)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int qcom_smem_map_global(struct qcom_smem *smem, u32 size)\n{\n\tu32 phys_addr;\n\n\tphys_addr = smem->regions[0].aux_base;\n\n\tsmem->regions[0].size = size;\n\tsmem->regions[0].virt_base = devm_ioremap_wc(smem->dev, phys_addr, size);\n\n\tif (!smem->regions[0].virt_base)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int qcom_smem_resolve_mem(struct qcom_smem *smem, const char *name,\n\t\t\t\t struct smem_region *region)\n{\n\tstruct device *dev = smem->dev;\n\tstruct device_node *np;\n\tstruct resource r;\n\tint ret;\n\n\tnp = of_parse_phandle(dev->of_node, name, 0);\n\tif (!np) {\n\t\tdev_err(dev, \"No %s specified\\n\", name);\n\t\treturn -EINVAL;\n\t}\n\n\tret = of_address_to_resource(np, 0, &r);\n\tof_node_put(np);\n\tif (ret)\n\t\treturn ret;\n\n\tregion->aux_base = r.start;\n\tregion->size = resource_size(&r);\n\n\treturn 0;\n}\n\nstatic int qcom_smem_probe(struct platform_device *pdev)\n{\n\tstruct smem_header *header;\n\tstruct reserved_mem *rmem;\n\tstruct qcom_smem *smem;\n\tunsigned long flags;\n\tint num_regions;\n\tint hwlock_id;\n\tu32 version;\n\tu32 size;\n\tint ret;\n\tint i;\n\n\tnum_regions = 1;\n\tif (of_property_present(pdev->dev.of_node, \"qcom,rpm-msg-ram\"))\n\t\tnum_regions++;\n\n\tsmem = devm_kzalloc(&pdev->dev, struct_size(smem, regions, num_regions),\n\t\t\t    GFP_KERNEL);\n\tif (!smem)\n\t\treturn -ENOMEM;\n\n\tsmem->dev = &pdev->dev;\n\tsmem->num_regions = num_regions;\n\n\trmem = of_reserved_mem_lookup(pdev->dev.of_node);\n\tif (rmem) {\n\t\tsmem->regions[0].aux_base = rmem->base;\n\t\tsmem->regions[0].size = rmem->size;\n\t} else {\n\t\t \n\t\tret = qcom_smem_resolve_mem(smem, \"memory-region\", &smem->regions[0]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (num_regions > 1) {\n\t\tret = qcom_smem_resolve_mem(smem, \"qcom,rpm-msg-ram\", &smem->regions[1]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\n\tret = qcom_smem_map_toc(smem, &smem->regions[0]);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 1; i < num_regions; i++) {\n\t\tsmem->regions[i].virt_base = devm_ioremap_wc(&pdev->dev,\n\t\t\t\t\t\t\t     smem->regions[i].aux_base,\n\t\t\t\t\t\t\t     smem->regions[i].size);\n\t\tif (!smem->regions[i].virt_base) {\n\t\t\tdev_err(&pdev->dev, \"failed to remap %pa\\n\", &smem->regions[i].aux_base);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\theader = smem->regions[0].virt_base;\n\tif (le32_to_cpu(header->initialized) != 1 ||\n\t    le32_to_cpu(header->reserved)) {\n\t\tdev_err(&pdev->dev, \"SMEM is not initialized by SBL\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\thwlock_id = of_hwspin_lock_get_id(pdev->dev.of_node, 0);\n\tif (hwlock_id < 0) {\n\t\tif (hwlock_id != -EPROBE_DEFER)\n\t\t\tdev_err(&pdev->dev, \"failed to retrieve hwlock\\n\");\n\t\treturn hwlock_id;\n\t}\n\n\tsmem->hwlock = hwspin_lock_request_specific(hwlock_id);\n\tif (!smem->hwlock)\n\t\treturn -ENXIO;\n\n\tret = hwspin_lock_timeout_irqsave(smem->hwlock, HWSPINLOCK_TIMEOUT, &flags);\n\tif (ret)\n\t\treturn ret;\n\tsize = readl_relaxed(&header->available) + readl_relaxed(&header->free_offset);\n\thwspin_unlock_irqrestore(smem->hwlock, &flags);\n\n\tversion = qcom_smem_get_sbl_version(smem);\n\t \n\tdevm_iounmap(smem->dev, smem->regions[0].virt_base);\n\tswitch (version >> 16) {\n\tcase SMEM_GLOBAL_PART_VERSION:\n\t\tret = qcom_smem_set_global_partition(smem);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tsmem->item_count = qcom_smem_get_item_count(smem);\n\t\tbreak;\n\tcase SMEM_GLOBAL_HEAP_VERSION:\n\t\tqcom_smem_map_global(smem, size);\n\t\tsmem->item_count = SMEM_ITEM_COUNT;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&pdev->dev, \"Unsupported SMEM version 0x%x\\n\", version);\n\t\treturn -EINVAL;\n\t}\n\n\tBUILD_BUG_ON(SMEM_HOST_APPS >= SMEM_HOST_COUNT);\n\tret = qcom_smem_enumerate_partitions(smem, SMEM_HOST_APPS);\n\tif (ret < 0 && ret != -ENOENT)\n\t\treturn ret;\n\n\t__smem = smem;\n\n\tsmem->socinfo = platform_device_register_data(&pdev->dev, \"qcom-socinfo\",\n\t\t\t\t\t\t      PLATFORM_DEVID_NONE, NULL,\n\t\t\t\t\t\t      0);\n\tif (IS_ERR(smem->socinfo))\n\t\tdev_dbg(&pdev->dev, \"failed to register socinfo device\\n\");\n\n\treturn 0;\n}\n\nstatic int qcom_smem_remove(struct platform_device *pdev)\n{\n\tplatform_device_unregister(__smem->socinfo);\n\n\thwspin_lock_free(__smem->hwlock);\n\t__smem = NULL;\n\n\treturn 0;\n}\n\nstatic const struct of_device_id qcom_smem_of_match[] = {\n\t{ .compatible = \"qcom,smem\" },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, qcom_smem_of_match);\n\nstatic struct platform_driver qcom_smem_driver = {\n\t.probe = qcom_smem_probe,\n\t.remove = qcom_smem_remove,\n\t.driver  = {\n\t\t.name = \"qcom-smem\",\n\t\t.of_match_table = qcom_smem_of_match,\n\t\t.suppress_bind_attrs = true,\n\t},\n};\n\nstatic int __init qcom_smem_init(void)\n{\n\treturn platform_driver_register(&qcom_smem_driver);\n}\narch_initcall(qcom_smem_init);\n\nstatic void __exit qcom_smem_exit(void)\n{\n\tplatform_driver_unregister(&qcom_smem_driver);\n}\nmodule_exit(qcom_smem_exit)\n\nMODULE_AUTHOR(\"Bjorn Andersson <bjorn.andersson@sonymobile.com>\");\nMODULE_DESCRIPTION(\"Qualcomm Shared Memory Manager\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}