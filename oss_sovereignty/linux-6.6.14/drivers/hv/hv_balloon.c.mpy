{
  "module_name": "hv_balloon.c",
  "hash_id": "60c72971c353167742c8977d944ed780ebbeca5c891a1a7460dcad11be89390f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/hv/hv_balloon.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/cleanup.h>\n#include <linux/kernel.h>\n#include <linux/jiffies.h>\n#include <linux/mman.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/kthread.h>\n#include <linux/completion.h>\n#include <linux/count_zeros.h>\n#include <linux/memory_hotplug.h>\n#include <linux/memory.h>\n#include <linux/notifier.h>\n#include <linux/percpu_counter.h>\n#include <linux/page_reporting.h>\n\n#include <linux/hyperv.h>\n#include <asm/hyperv-tlfs.h>\n\n#include <asm/mshyperv.h>\n\n#define CREATE_TRACE_POINTS\n#include \"hv_trace_balloon.h\"\n\n \n\n\n\n \n\n#define DYNMEM_MAKE_VERSION(Major, Minor) ((__u32)(((Major) << 16) | (Minor)))\n#define DYNMEM_MAJOR_VERSION(Version) ((__u32)(Version) >> 16)\n#define DYNMEM_MINOR_VERSION(Version) ((__u32)(Version) & 0xff)\n\nenum {\n\tDYNMEM_PROTOCOL_VERSION_1 = DYNMEM_MAKE_VERSION(0, 3),\n\tDYNMEM_PROTOCOL_VERSION_2 = DYNMEM_MAKE_VERSION(1, 0),\n\tDYNMEM_PROTOCOL_VERSION_3 = DYNMEM_MAKE_VERSION(2, 0),\n\n\tDYNMEM_PROTOCOL_VERSION_WIN7 = DYNMEM_PROTOCOL_VERSION_1,\n\tDYNMEM_PROTOCOL_VERSION_WIN8 = DYNMEM_PROTOCOL_VERSION_2,\n\tDYNMEM_PROTOCOL_VERSION_WIN10 = DYNMEM_PROTOCOL_VERSION_3,\n\n\tDYNMEM_PROTOCOL_VERSION_CURRENT = DYNMEM_PROTOCOL_VERSION_WIN10\n};\n\n\n\n \n\nenum dm_message_type {\n\t \n\tDM_ERROR\t\t\t= 0,\n\tDM_VERSION_REQUEST\t\t= 1,\n\tDM_VERSION_RESPONSE\t\t= 2,\n\tDM_CAPABILITIES_REPORT\t\t= 3,\n\tDM_CAPABILITIES_RESPONSE\t= 4,\n\tDM_STATUS_REPORT\t\t= 5,\n\tDM_BALLOON_REQUEST\t\t= 6,\n\tDM_BALLOON_RESPONSE\t\t= 7,\n\tDM_UNBALLOON_REQUEST\t\t= 8,\n\tDM_UNBALLOON_RESPONSE\t\t= 9,\n\tDM_MEM_HOT_ADD_REQUEST\t\t= 10,\n\tDM_MEM_HOT_ADD_RESPONSE\t\t= 11,\n\tDM_VERSION_03_MAX\t\t= 11,\n\t \n\tDM_INFO_MESSAGE\t\t\t= 12,\n\tDM_VERSION_1_MAX\t\t= 12\n};\n\n\n \n\nunion dm_version {\n\tstruct {\n\t\t__u16 minor_version;\n\t\t__u16 major_version;\n\t};\n\t__u32 version;\n} __packed;\n\n\nunion dm_caps {\n\tstruct {\n\t\t__u64 balloon:1;\n\t\t__u64 hot_add:1;\n\t\t \n\t\t__u64 hot_add_alignment:4;\n\t\t__u64 reservedz:58;\n\t} cap_bits;\n\t__u64 caps;\n} __packed;\n\nunion dm_mem_page_range {\n\tstruct  {\n\t\t \n\t\t__u64 start_page:40;\n\t\t \n\t\t__u64 page_cnt:24;\n\t} finfo;\n\t__u64  page_range;\n} __packed;\n\n\n\n \n\nstruct dm_header {\n\t__u16 type;\n\t__u16 size;\n\t__u32 trans_id;\n} __packed;\n\n \n\nstruct dm_message {\n\tstruct dm_header hdr;\n\t__u8 data[];  \n} __packed;\n\n\n \n\n \n\nstruct dm_version_request {\n\tstruct dm_header hdr;\n\tunion dm_version version;\n\t__u32 is_last_attempt:1;\n\t__u32 reservedz:31;\n} __packed;\n\n \n\nstruct dm_version_response {\n\tstruct dm_header hdr;\n\t__u64 is_accepted:1;\n\t__u64 reservedz:63;\n} __packed;\n\n \n\nstruct dm_capabilities {\n\tstruct dm_header hdr;\n\tunion dm_caps caps;\n\t__u64 min_page_cnt;\n\t__u64 max_page_number;\n} __packed;\n\n \n\nstruct dm_capabilities_resp_msg {\n\tstruct dm_header hdr;\n\t__u64 is_accepted:1;\n\t__u64 reservedz:63;\n} __packed;\n\n \n\nstruct dm_status {\n\tstruct dm_header hdr;\n\t__u64 num_avail;\n\t__u64 num_committed;\n\t__u64 page_file_size;\n\t__u64 zero_free;\n\t__u32 page_file_writes;\n\t__u32 io_diff;\n} __packed;\n\n\n \n\nstruct dm_balloon {\n\tstruct dm_header hdr;\n\t__u32 num_pages;\n\t__u32 reservedz;\n} __packed;\n\n\n \n\nstruct dm_balloon_response {\n\tstruct dm_header hdr;\n\t__u32 reservedz;\n\t__u32 more_pages:1;\n\t__u32 range_count:31;\n\tunion dm_mem_page_range range_array[];\n} __packed;\n\n \n\nstruct dm_unballoon_request {\n\tstruct dm_header hdr;\n\t__u32 more_pages:1;\n\t__u32 reservedz:31;\n\t__u32 range_count;\n\tunion dm_mem_page_range range_array[];\n} __packed;\n\n \n\nstruct dm_unballoon_response {\n\tstruct dm_header hdr;\n} __packed;\n\n\n \n\nstruct dm_hot_add {\n\tstruct dm_header hdr;\n\tunion dm_mem_page_range range;\n} __packed;\n\n \n\nstruct dm_hot_add_response {\n\tstruct dm_header hdr;\n\t__u32 page_count;\n\t__u32 result;\n} __packed;\n\n \n\nenum dm_info_type {\n\tINFO_TYPE_MAX_PAGE_CNT = 0,\n\tMAX_INFO_TYPE\n};\n\n\n \n\nstruct dm_info_header {\n\tenum dm_info_type type;\n\t__u32 data_size;\n} __packed;\n\n \n\nstruct dm_info_msg {\n\tstruct dm_header hdr;\n\t__u32 reserved;\n\t__u32 info_size;\n\t__u8  info[];\n};\n\n \n\n \n\nstruct hv_hotadd_state {\n\tstruct list_head list;\n\tunsigned long start_pfn;\n\tunsigned long covered_start_pfn;\n\tunsigned long covered_end_pfn;\n\tunsigned long ha_end_pfn;\n\tunsigned long end_pfn;\n\t \n\tstruct list_head gap_list;\n};\n\nstruct hv_hotadd_gap {\n\tstruct list_head list;\n\tunsigned long start_pfn;\n\tunsigned long end_pfn;\n};\n\nstruct balloon_state {\n\t__u32 num_pages;\n\tstruct work_struct wrk;\n};\n\nstruct hot_add_wrk {\n\tunion dm_mem_page_range ha_page_range;\n\tunion dm_mem_page_range ha_region_range;\n\tstruct work_struct wrk;\n};\n\nstatic bool allow_hibernation;\nstatic bool hot_add = true;\nstatic bool do_hot_add;\n \nstatic uint pressure_report_delay = 45;\nextern unsigned int page_reporting_order;\n#define HV_MAX_FAILURES\t2\n\n \nstatic unsigned long last_post_time;\n\nstatic int hv_hypercall_multi_failure;\n\nmodule_param(hot_add, bool, (S_IRUGO | S_IWUSR));\nMODULE_PARM_DESC(hot_add, \"If set attempt memory hot_add\");\n\nmodule_param(pressure_report_delay, uint, (S_IRUGO | S_IWUSR));\nMODULE_PARM_DESC(pressure_report_delay, \"Delay in secs in reporting pressure\");\nstatic atomic_t trans_id = ATOMIC_INIT(0);\n\nstatic int dm_ring_size = VMBUS_RING_SIZE(16 * 1024);\n\n \n\nenum hv_dm_state {\n\tDM_INITIALIZING = 0,\n\tDM_INITIALIZED,\n\tDM_BALLOON_UP,\n\tDM_BALLOON_DOWN,\n\tDM_HOT_ADD,\n\tDM_INIT_ERROR\n};\n\n\nstatic __u8 recv_buffer[HV_HYP_PAGE_SIZE];\nstatic __u8 balloon_up_send_buffer[HV_HYP_PAGE_SIZE];\n#define PAGES_IN_2M (2 * 1024 * 1024 / PAGE_SIZE)\n#define HA_CHUNK (128 * 1024 * 1024 / PAGE_SIZE)\n\nstruct hv_dynmem_device {\n\tstruct hv_device *dev;\n\tenum hv_dm_state state;\n\tstruct completion host_event;\n\tstruct completion config_event;\n\n\t \n\tunsigned int num_pages_ballooned;\n\tunsigned int num_pages_onlined;\n\tunsigned int num_pages_added;\n\n\t \n\tstruct balloon_state balloon_wrk;\n\n\t \n\tstruct hot_add_wrk ha_wrk;\n\n\t \n\tbool host_specified_ha_region;\n\n\t \n\tstruct completion  ol_waitevent;\n\t \n\tstruct task_struct *thread;\n\n\t \n\tspinlock_t ha_lock;\n\n\t \n\tstruct list_head ha_region_list;\n\n\t \n\t__u32 next_version;\n\n\t \n\t__u32 version;\n\n\tstruct page_reporting_dev_info pr_dev_info;\n\n\t \n\t__u64 max_dynamic_page_count;\n};\n\nstatic struct hv_dynmem_device dm_device;\n\nstatic void post_status(struct hv_dynmem_device *dm);\n\nstatic void enable_page_reporting(void);\n\nstatic void disable_page_reporting(void);\n\n#ifdef CONFIG_MEMORY_HOTPLUG\nstatic inline bool has_pfn_is_backed(struct hv_hotadd_state *has,\n\t\t\t\t     unsigned long pfn)\n{\n\tstruct hv_hotadd_gap *gap;\n\n\t \n\tif ((pfn < has->covered_start_pfn) || (pfn >= has->covered_end_pfn))\n\t\treturn false;\n\n\t \n\tlist_for_each_entry(gap, &has->gap_list, list) {\n\t\tif ((pfn >= gap->start_pfn) && (pfn < gap->end_pfn))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic unsigned long hv_page_offline_check(unsigned long start_pfn,\n\t\t\t\t\t   unsigned long nr_pages)\n{\n\tunsigned long pfn = start_pfn, count = 0;\n\tstruct hv_hotadd_state *has;\n\tbool found;\n\n\twhile (pfn < start_pfn + nr_pages) {\n\t\t \n\t\tfound = false;\n\t\tlist_for_each_entry(has, &dm_device.ha_region_list, list) {\n\t\t\twhile ((pfn >= has->start_pfn) &&\n\t\t\t       (pfn < has->end_pfn) &&\n\t\t\t       (pfn < start_pfn + nr_pages)) {\n\t\t\t\tfound = true;\n\t\t\t\tif (has_pfn_is_backed(has, pfn))\n\t\t\t\t\tcount++;\n\t\t\t\tpfn++;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!found)\n\t\t\tpfn++;\n\t}\n\n\treturn count;\n}\n\nstatic int hv_memory_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t      void *v)\n{\n\tstruct memory_notify *mem = (struct memory_notify *)v;\n\tunsigned long pfn_count;\n\n\tswitch (val) {\n\tcase MEM_ONLINE:\n\tcase MEM_CANCEL_ONLINE:\n\t\tcomplete(&dm_device.ol_waitevent);\n\t\tbreak;\n\n\tcase MEM_OFFLINE:\n\t\tscoped_guard(spinlock_irqsave, &dm_device.ha_lock) {\n\t\t\tpfn_count = hv_page_offline_check(mem->start_pfn,\n\t\t\t\t\t\t\t  mem->nr_pages);\n\t\t\tif (pfn_count <= dm_device.num_pages_onlined) {\n\t\t\t\tdm_device.num_pages_onlined -= pfn_count;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\tdm_device.num_pages_onlined = 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase MEM_GOING_ONLINE:\n\tcase MEM_GOING_OFFLINE:\n\tcase MEM_CANCEL_OFFLINE:\n\t\tbreak;\n\t}\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block hv_memory_nb = {\n\t.notifier_call = hv_memory_notifier,\n\t.priority = 0\n};\n\n \nstatic void hv_page_online_one(struct hv_hotadd_state *has, struct page *pg)\n{\n\tif (!has_pfn_is_backed(has, page_to_pfn(pg))) {\n\t\tif (!PageOffline(pg))\n\t\t\t__SetPageOffline(pg);\n\t\treturn;\n\t}\n\tif (PageOffline(pg))\n\t\t__ClearPageOffline(pg);\n\n\t \n\tgeneric_online_page(pg, 0);\n\n\tlockdep_assert_held(&dm_device.ha_lock);\n\tdm_device.num_pages_onlined++;\n}\n\nstatic void hv_bring_pgs_online(struct hv_hotadd_state *has,\n\t\t\t\tunsigned long start_pfn, unsigned long size)\n{\n\tint i;\n\n\tpr_debug(\"Online %lu pages starting at pfn 0x%lx\\n\", size, start_pfn);\n\tfor (i = 0; i < size; i++)\n\t\thv_page_online_one(has, pfn_to_page(start_pfn + i));\n}\n\nstatic void hv_mem_hot_add(unsigned long start, unsigned long size,\n\t\t\t\tunsigned long pfn_count,\n\t\t\t\tstruct hv_hotadd_state *has)\n{\n\tint ret = 0;\n\tint i, nid;\n\tunsigned long start_pfn;\n\tunsigned long processed_pfn;\n\tunsigned long total_pfn = pfn_count;\n\n\tfor (i = 0; i < (size/HA_CHUNK); i++) {\n\t\tstart_pfn = start + (i * HA_CHUNK);\n\n\t\tscoped_guard(spinlock_irqsave, &dm_device.ha_lock) {\n\t\t\thas->ha_end_pfn +=  HA_CHUNK;\n\n\t\t\tif (total_pfn > HA_CHUNK) {\n\t\t\t\tprocessed_pfn = HA_CHUNK;\n\t\t\t\ttotal_pfn -= HA_CHUNK;\n\t\t\t} else {\n\t\t\t\tprocessed_pfn = total_pfn;\n\t\t\t\ttotal_pfn = 0;\n\t\t\t}\n\n\t\t\thas->covered_end_pfn +=  processed_pfn;\n\t\t}\n\n\t\treinit_completion(&dm_device.ol_waitevent);\n\n\t\tnid = memory_add_physaddr_to_nid(PFN_PHYS(start_pfn));\n\t\tret = add_memory(nid, PFN_PHYS((start_pfn)),\n\t\t\t\t(HA_CHUNK << PAGE_SHIFT), MHP_MERGE_RESOURCE);\n\n\t\tif (ret) {\n\t\t\tpr_err(\"hot_add memory failed error is %d\\n\", ret);\n\t\t\tif (ret == -EEXIST) {\n\t\t\t\t \n\t\t\t\tdo_hot_add = false;\n\t\t\t}\n\t\t\tscoped_guard(spinlock_irqsave, &dm_device.ha_lock) {\n\t\t\t\thas->ha_end_pfn -= HA_CHUNK;\n\t\t\t\thas->covered_end_pfn -=  processed_pfn;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\twait_for_completion_timeout(&dm_device.ol_waitevent, 5 * HZ);\n\t\tpost_status(&dm_device);\n\t}\n}\n\nstatic void hv_online_page(struct page *pg, unsigned int order)\n{\n\tstruct hv_hotadd_state *has;\n\tunsigned long pfn = page_to_pfn(pg);\n\n\tguard(spinlock_irqsave)(&dm_device.ha_lock);\n\tlist_for_each_entry(has, &dm_device.ha_region_list, list) {\n\t\t \n\t\tif ((pfn < has->start_pfn) ||\n\t\t\t\t(pfn + (1UL << order) > has->end_pfn))\n\t\t\tcontinue;\n\n\t\thv_bring_pgs_online(has, pfn, 1UL << order);\n\t\tbreak;\n\t}\n}\n\nstatic int pfn_covered(unsigned long start_pfn, unsigned long pfn_cnt)\n{\n\tstruct hv_hotadd_state *has;\n\tstruct hv_hotadd_gap *gap;\n\tunsigned long residual, new_inc;\n\tint ret = 0;\n\n\tguard(spinlock_irqsave)(&dm_device.ha_lock);\n\tlist_for_each_entry(has, &dm_device.ha_region_list, list) {\n\t\t \n\t\tif (start_pfn < has->start_pfn || start_pfn >= has->end_pfn)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (has->covered_end_pfn != start_pfn) {\n\t\t\tgap = kzalloc(sizeof(struct hv_hotadd_gap), GFP_ATOMIC);\n\t\t\tif (!gap) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tINIT_LIST_HEAD(&gap->list);\n\t\t\tgap->start_pfn = has->covered_end_pfn;\n\t\t\tgap->end_pfn = start_pfn;\n\t\t\tlist_add_tail(&gap->list, &has->gap_list);\n\n\t\t\thas->covered_end_pfn = start_pfn;\n\t\t}\n\n\t\t \n\t\tif ((start_pfn + pfn_cnt) > has->end_pfn) {\n\t\t\tresidual = (start_pfn + pfn_cnt - has->end_pfn);\n\t\t\t \n\t\t\tnew_inc = (residual / HA_CHUNK) * HA_CHUNK;\n\t\t\tif (residual % HA_CHUNK)\n\t\t\t\tnew_inc += HA_CHUNK;\n\n\t\t\thas->end_pfn += new_inc;\n\t\t}\n\n\t\tret = 1;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic unsigned long handle_pg_range(unsigned long pg_start,\n\t\t\t\t\tunsigned long pg_count)\n{\n\tunsigned long start_pfn = pg_start;\n\tunsigned long pfn_cnt = pg_count;\n\tunsigned long size;\n\tstruct hv_hotadd_state *has;\n\tunsigned long pgs_ol = 0;\n\tunsigned long old_covered_state;\n\tunsigned long res = 0, flags;\n\n\tpr_debug(\"Hot adding %lu pages starting at pfn 0x%lx.\\n\", pg_count,\n\t\tpg_start);\n\n\tspin_lock_irqsave(&dm_device.ha_lock, flags);\n\tlist_for_each_entry(has, &dm_device.ha_region_list, list) {\n\t\t \n\t\tif (start_pfn < has->start_pfn || start_pfn >= has->end_pfn)\n\t\t\tcontinue;\n\n\t\told_covered_state = has->covered_end_pfn;\n\n\t\tif (start_pfn < has->ha_end_pfn) {\n\t\t\t \n\t\t\tpgs_ol = has->ha_end_pfn - start_pfn;\n\t\t\tif (pgs_ol > pfn_cnt)\n\t\t\t\tpgs_ol = pfn_cnt;\n\n\t\t\thas->covered_end_pfn +=  pgs_ol;\n\t\t\tpfn_cnt -= pgs_ol;\n\t\t\t \n\t\t\tif (start_pfn > has->start_pfn &&\n\t\t\t    online_section_nr(pfn_to_section_nr(start_pfn)))\n\t\t\t\thv_bring_pgs_online(has, start_pfn, pgs_ol);\n\n\t\t}\n\n\t\tif ((has->ha_end_pfn < has->end_pfn) && (pfn_cnt > 0)) {\n\t\t\t \n\t\t\tsize = (has->end_pfn - has->ha_end_pfn);\n\t\t\tif (pfn_cnt <= size) {\n\t\t\t\tsize = ((pfn_cnt / HA_CHUNK) * HA_CHUNK);\n\t\t\t\tif (pfn_cnt % HA_CHUNK)\n\t\t\t\t\tsize += HA_CHUNK;\n\t\t\t} else {\n\t\t\t\tpfn_cnt = size;\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&dm_device.ha_lock, flags);\n\t\t\thv_mem_hot_add(has->ha_end_pfn, size, pfn_cnt, has);\n\t\t\tspin_lock_irqsave(&dm_device.ha_lock, flags);\n\t\t}\n\t\t \n\t\tres = has->covered_end_pfn - old_covered_state;\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&dm_device.ha_lock, flags);\n\n\treturn res;\n}\n\nstatic unsigned long process_hot_add(unsigned long pg_start,\n\t\t\t\t\tunsigned long pfn_cnt,\n\t\t\t\t\tunsigned long rg_start,\n\t\t\t\t\tunsigned long rg_size)\n{\n\tstruct hv_hotadd_state *ha_region = NULL;\n\tint covered;\n\n\tif (pfn_cnt == 0)\n\t\treturn 0;\n\n\tif (!dm_device.host_specified_ha_region) {\n\t\tcovered = pfn_covered(pg_start, pfn_cnt);\n\t\tif (covered < 0)\n\t\t\treturn 0;\n\n\t\tif (covered)\n\t\t\tgoto do_pg_range;\n\t}\n\n\t \n\n\tif (rg_size != 0) {\n\t\tha_region = kzalloc(sizeof(struct hv_hotadd_state), GFP_KERNEL);\n\t\tif (!ha_region)\n\t\t\treturn 0;\n\n\t\tINIT_LIST_HEAD(&ha_region->list);\n\t\tINIT_LIST_HEAD(&ha_region->gap_list);\n\n\t\tha_region->start_pfn = rg_start;\n\t\tha_region->ha_end_pfn = rg_start;\n\t\tha_region->covered_start_pfn = pg_start;\n\t\tha_region->covered_end_pfn = pg_start;\n\t\tha_region->end_pfn = rg_start + rg_size;\n\n\t\tscoped_guard(spinlock_irqsave, &dm_device.ha_lock) {\n\t\t\tlist_add_tail(&ha_region->list, &dm_device.ha_region_list);\n\t\t}\n\t}\n\ndo_pg_range:\n\t \n\treturn handle_pg_range(pg_start, pfn_cnt);\n}\n\n#endif\n\nstatic void hot_add_req(struct work_struct *dummy)\n{\n\tstruct dm_hot_add_response resp;\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tunsigned long pg_start, pfn_cnt;\n\tunsigned long rg_start, rg_sz;\n#endif\n\tstruct hv_dynmem_device *dm = &dm_device;\n\n\tmemset(&resp, 0, sizeof(struct dm_hot_add_response));\n\tresp.hdr.type = DM_MEM_HOT_ADD_RESPONSE;\n\tresp.hdr.size = sizeof(struct dm_hot_add_response);\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tpg_start = dm->ha_wrk.ha_page_range.finfo.start_page;\n\tpfn_cnt = dm->ha_wrk.ha_page_range.finfo.page_cnt;\n\n\trg_start = dm->ha_wrk.ha_region_range.finfo.start_page;\n\trg_sz = dm->ha_wrk.ha_region_range.finfo.page_cnt;\n\n\tif ((rg_start == 0) && (!dm->host_specified_ha_region)) {\n\t\tunsigned long region_size;\n\t\tunsigned long region_start;\n\n\t\t \n\t\tregion_size = (pfn_cnt / HA_CHUNK) * HA_CHUNK;\n\t\tif (pfn_cnt % HA_CHUNK)\n\t\t\tregion_size += HA_CHUNK;\n\n\t\tregion_start = (pg_start / HA_CHUNK) * HA_CHUNK;\n\n\t\trg_start = region_start;\n\t\trg_sz = region_size;\n\t}\n\n\tif (do_hot_add)\n\t\tresp.page_count = process_hot_add(pg_start, pfn_cnt,\n\t\t\t\t\t\trg_start, rg_sz);\n\n\tdm->num_pages_added += resp.page_count;\n#endif\n\t \n\tif (resp.page_count > 0)\n\t\tresp.result = 1;\n\telse if (!do_hot_add)\n\t\tresp.result = 1;\n\telse\n\t\tresp.result = 0;\n\n\tif (!do_hot_add || resp.page_count == 0) {\n\t\tif (!allow_hibernation)\n\t\t\tpr_err(\"Memory hot add failed\\n\");\n\t\telse\n\t\t\tpr_info(\"Ignore hot-add request!\\n\");\n\t}\n\n\tdm->state = DM_INITIALIZED;\n\tresp.hdr.trans_id = atomic_inc_return(&trans_id);\n\tvmbus_sendpacket(dm->dev->channel, &resp,\n\t\t\tsizeof(struct dm_hot_add_response),\n\t\t\t(unsigned long)NULL,\n\t\t\tVM_PKT_DATA_INBAND, 0);\n}\n\nstatic void process_info(struct hv_dynmem_device *dm, struct dm_info_msg *msg)\n{\n\tstruct dm_info_header *info_hdr;\n\n\tinfo_hdr = (struct dm_info_header *)msg->info;\n\n\tswitch (info_hdr->type) {\n\tcase INFO_TYPE_MAX_PAGE_CNT:\n\t\tif (info_hdr->data_size == sizeof(__u64)) {\n\t\t\t__u64 *max_page_count = (__u64 *)&info_hdr[1];\n\n\t\t\tpr_info(\"Max. dynamic memory size: %llu MB\\n\",\n\t\t\t\t(*max_page_count) >> (20 - HV_HYP_PAGE_SHIFT));\n\t\t\tdm->max_dynamic_page_count = *max_page_count;\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"Received Unknown type: %d\\n\", info_hdr->type);\n\t}\n}\n\nstatic unsigned long compute_balloon_floor(void)\n{\n\tunsigned long min_pages;\n\tunsigned long nr_pages = totalram_pages();\n#define MB2PAGES(mb) ((mb) << (20 - PAGE_SHIFT))\n\t \n\tif (nr_pages < MB2PAGES(128))\n\t\tmin_pages = MB2PAGES(8) + (nr_pages >> 1);\n\telse if (nr_pages < MB2PAGES(512))\n\t\tmin_pages = MB2PAGES(40) + (nr_pages >> 2);\n\telse if (nr_pages < MB2PAGES(2048))\n\t\tmin_pages = MB2PAGES(104) + (nr_pages >> 3);\n\telse if (nr_pages < MB2PAGES(8192))\n\t\tmin_pages = MB2PAGES(232) + (nr_pages >> 4);\n\telse\n\t\tmin_pages = MB2PAGES(488) + (nr_pages >> 5);\n#undef MB2PAGES\n\treturn min_pages;\n}\n\n \n\nstatic unsigned long get_pages_committed(struct hv_dynmem_device *dm)\n{\n\treturn vm_memory_committed() +\n\t\tdm->num_pages_ballooned +\n\t\t(dm->num_pages_added > dm->num_pages_onlined ?\n\t\t dm->num_pages_added - dm->num_pages_onlined : 0) +\n\t\tcompute_balloon_floor();\n}\n\n \n\nstatic void post_status(struct hv_dynmem_device *dm)\n{\n\tstruct dm_status status;\n\tunsigned long now = jiffies;\n\tunsigned long last_post = last_post_time;\n\tunsigned long num_pages_avail, num_pages_committed;\n\n\tif (pressure_report_delay > 0) {\n\t\t--pressure_report_delay;\n\t\treturn;\n\t}\n\n\tif (!time_after(now, (last_post_time + HZ)))\n\t\treturn;\n\n\tmemset(&status, 0, sizeof(struct dm_status));\n\tstatus.hdr.type = DM_STATUS_REPORT;\n\tstatus.hdr.size = sizeof(struct dm_status);\n\tstatus.hdr.trans_id = atomic_inc_return(&trans_id);\n\n\t \n\tnum_pages_avail = si_mem_available();\n\tnum_pages_committed = get_pages_committed(dm);\n\n\ttrace_balloon_status(num_pages_avail, num_pages_committed,\n\t\t\t     vm_memory_committed(), dm->num_pages_ballooned,\n\t\t\t     dm->num_pages_added, dm->num_pages_onlined);\n\n\t \n\tstatus.num_avail = num_pages_avail * NR_HV_HYP_PAGES_IN_PAGE;\n\tstatus.num_committed = num_pages_committed * NR_HV_HYP_PAGES_IN_PAGE;\n\n\t \n\tif (status.hdr.trans_id != atomic_read(&trans_id))\n\t\treturn;\n\n\t \n\tif (last_post != last_post_time)\n\t\treturn;\n\n\tlast_post_time = jiffies;\n\tvmbus_sendpacket(dm->dev->channel, &status,\n\t\t\t\tsizeof(struct dm_status),\n\t\t\t\t(unsigned long)NULL,\n\t\t\t\tVM_PKT_DATA_INBAND, 0);\n\n}\n\nstatic void free_balloon_pages(struct hv_dynmem_device *dm,\n\t\t\t union dm_mem_page_range *range_array)\n{\n\tint num_pages = range_array->finfo.page_cnt;\n\t__u64 start_frame = range_array->finfo.start_page;\n\tstruct page *pg;\n\tint i;\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tpg = pfn_to_page(i + start_frame);\n\t\t__ClearPageOffline(pg);\n\t\t__free_page(pg);\n\t\tdm->num_pages_ballooned--;\n\t\tadjust_managed_page_count(pg, 1);\n\t}\n}\n\n\n\nstatic unsigned int alloc_balloon_pages(struct hv_dynmem_device *dm,\n\t\t\t\t\tunsigned int num_pages,\n\t\t\t\t\tstruct dm_balloon_response *bl_resp,\n\t\t\t\t\tint alloc_unit)\n{\n\tunsigned int i, j;\n\tstruct page *pg;\n\n\tfor (i = 0; i < num_pages / alloc_unit; i++) {\n\t\tif (bl_resp->hdr.size + sizeof(union dm_mem_page_range) >\n\t\t\tHV_HYP_PAGE_SIZE)\n\t\t\treturn i * alloc_unit;\n\n\t\t \n\t\tpg = alloc_pages(GFP_HIGHUSER | __GFP_NORETRY |\n\t\t\t\t__GFP_NOMEMALLOC | __GFP_NOWARN,\n\t\t\t\tget_order(alloc_unit << PAGE_SHIFT));\n\n\t\tif (!pg)\n\t\t\treturn i * alloc_unit;\n\n\t\tdm->num_pages_ballooned += alloc_unit;\n\n\t\t \n\n\t\tif (alloc_unit != 1)\n\t\t\tsplit_page(pg, get_order(alloc_unit << PAGE_SHIFT));\n\n\t\t \n\t\tfor (j = 0; j < alloc_unit; j++) {\n\t\t\t__SetPageOffline(pg + j);\n\t\t\tadjust_managed_page_count(pg + j, -1);\n\t\t}\n\n\t\tbl_resp->range_count++;\n\t\tbl_resp->range_array[i].finfo.start_page =\n\t\t\tpage_to_pfn(pg);\n\t\tbl_resp->range_array[i].finfo.page_cnt = alloc_unit;\n\t\tbl_resp->hdr.size += sizeof(union dm_mem_page_range);\n\n\t}\n\n\treturn i * alloc_unit;\n}\n\nstatic void balloon_up(struct work_struct *dummy)\n{\n\tunsigned int num_pages = dm_device.balloon_wrk.num_pages;\n\tunsigned int num_ballooned = 0;\n\tstruct dm_balloon_response *bl_resp;\n\tint alloc_unit;\n\tint ret;\n\tbool done = false;\n\tint i;\n\tlong avail_pages;\n\tunsigned long floor;\n\n\t \n\talloc_unit = PAGES_IN_2M;\n\n\tavail_pages = si_mem_available();\n\tfloor = compute_balloon_floor();\n\n\t \n\tif (avail_pages < num_pages || avail_pages - num_pages < floor) {\n\t\tpr_info(\"Balloon request will be partially fulfilled. %s\\n\",\n\t\t\tavail_pages < num_pages ? \"Not enough memory.\" :\n\t\t\t\"Balloon floor reached.\");\n\n\t\tnum_pages = avail_pages > floor ? (avail_pages - floor) : 0;\n\t}\n\n\twhile (!done) {\n\t\tmemset(balloon_up_send_buffer, 0, HV_HYP_PAGE_SIZE);\n\t\tbl_resp = (struct dm_balloon_response *)balloon_up_send_buffer;\n\t\tbl_resp->hdr.type = DM_BALLOON_RESPONSE;\n\t\tbl_resp->hdr.size = sizeof(struct dm_balloon_response);\n\t\tbl_resp->more_pages = 1;\n\n\t\tnum_pages -= num_ballooned;\n\t\tnum_ballooned = alloc_balloon_pages(&dm_device, num_pages,\n\t\t\t\t\t\t    bl_resp, alloc_unit);\n\n\t\tif (alloc_unit != 1 && num_ballooned == 0) {\n\t\t\talloc_unit = 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (num_ballooned == 0 || num_ballooned == num_pages) {\n\t\t\tpr_debug(\"Ballooned %u out of %u requested pages.\\n\",\n\t\t\t\tnum_pages, dm_device.balloon_wrk.num_pages);\n\n\t\t\tbl_resp->more_pages = 0;\n\t\t\tdone = true;\n\t\t\tdm_device.state = DM_INITIALIZED;\n\t\t}\n\n\t\t \n\n\t\tdo {\n\t\t\tbl_resp->hdr.trans_id = atomic_inc_return(&trans_id);\n\t\t\tret = vmbus_sendpacket(dm_device.dev->channel,\n\t\t\t\t\t\tbl_resp,\n\t\t\t\t\t\tbl_resp->hdr.size,\n\t\t\t\t\t\t(unsigned long)NULL,\n\t\t\t\t\t\tVM_PKT_DATA_INBAND, 0);\n\n\t\t\tif (ret == -EAGAIN)\n\t\t\t\tmsleep(20);\n\t\t\tpost_status(&dm_device);\n\t\t} while (ret == -EAGAIN);\n\n\t\tif (ret) {\n\t\t\t \n\t\t\tpr_err(\"Balloon response failed\\n\");\n\n\t\t\tfor (i = 0; i < bl_resp->range_count; i++)\n\t\t\t\tfree_balloon_pages(&dm_device,\n\t\t\t\t\t\t &bl_resp->range_array[i]);\n\n\t\t\tdone = true;\n\t\t}\n\t}\n\n}\n\nstatic void balloon_down(struct hv_dynmem_device *dm,\n\t\t\tstruct dm_unballoon_request *req)\n{\n\tunion dm_mem_page_range *range_array = req->range_array;\n\tint range_count = req->range_count;\n\tstruct dm_unballoon_response resp;\n\tint i;\n\tunsigned int prev_pages_ballooned = dm->num_pages_ballooned;\n\n\tfor (i = 0; i < range_count; i++) {\n\t\tfree_balloon_pages(dm, &range_array[i]);\n\t\tcomplete(&dm_device.config_event);\n\t}\n\n\tpr_debug(\"Freed %u ballooned pages.\\n\",\n\t\tprev_pages_ballooned - dm->num_pages_ballooned);\n\n\tif (req->more_pages == 1)\n\t\treturn;\n\n\tmemset(&resp, 0, sizeof(struct dm_unballoon_response));\n\tresp.hdr.type = DM_UNBALLOON_RESPONSE;\n\tresp.hdr.trans_id = atomic_inc_return(&trans_id);\n\tresp.hdr.size = sizeof(struct dm_unballoon_response);\n\n\tvmbus_sendpacket(dm_device.dev->channel, &resp,\n\t\t\t\tsizeof(struct dm_unballoon_response),\n\t\t\t\t(unsigned long)NULL,\n\t\t\t\tVM_PKT_DATA_INBAND, 0);\n\n\tdm->state = DM_INITIALIZED;\n}\n\nstatic void balloon_onchannelcallback(void *context);\n\nstatic int dm_thread_func(void *dm_dev)\n{\n\tstruct hv_dynmem_device *dm = dm_dev;\n\n\twhile (!kthread_should_stop()) {\n\t\twait_for_completion_interruptible_timeout(\n\t\t\t\t\t\t&dm_device.config_event, 1*HZ);\n\t\t \n\t\treinit_completion(&dm_device.config_event);\n\t\tpost_status(dm);\n\t\t \n\t\tif (hv_hypercall_multi_failure >= HV_MAX_FAILURES) {\n\t\t\tpr_err(\"Multiple failures in cold memory discard hypercall, disabling page reporting\\n\");\n\t\t\tdisable_page_reporting();\n\t\t\t \n\t\t\thv_hypercall_multi_failure = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n\nstatic void version_resp(struct hv_dynmem_device *dm,\n\t\t\tstruct dm_version_response *vresp)\n{\n\tstruct dm_version_request version_req;\n\tint ret;\n\n\tif (vresp->is_accepted) {\n\t\t \n\t\tcomplete(&dm->host_event);\n\t\treturn;\n\t}\n\t \n\tif (dm->next_version == 0)\n\t\tgoto version_error;\n\n\tmemset(&version_req, 0, sizeof(struct dm_version_request));\n\tversion_req.hdr.type = DM_VERSION_REQUEST;\n\tversion_req.hdr.size = sizeof(struct dm_version_request);\n\tversion_req.hdr.trans_id = atomic_inc_return(&trans_id);\n\tversion_req.version.version = dm->next_version;\n\tdm->version = version_req.version.version;\n\n\t \n\tswitch (version_req.version.version) {\n\tcase DYNMEM_PROTOCOL_VERSION_WIN8:\n\t\tdm->next_version = DYNMEM_PROTOCOL_VERSION_WIN7;\n\t\tversion_req.is_last_attempt = 0;\n\t\tbreak;\n\tdefault:\n\t\tdm->next_version = 0;\n\t\tversion_req.is_last_attempt = 1;\n\t}\n\n\tret = vmbus_sendpacket(dm->dev->channel, &version_req,\n\t\t\t\tsizeof(struct dm_version_request),\n\t\t\t\t(unsigned long)NULL,\n\t\t\t\tVM_PKT_DATA_INBAND, 0);\n\n\tif (ret)\n\t\tgoto version_error;\n\n\treturn;\n\nversion_error:\n\tdm->state = DM_INIT_ERROR;\n\tcomplete(&dm->host_event);\n}\n\nstatic void cap_resp(struct hv_dynmem_device *dm,\n\t\t\tstruct dm_capabilities_resp_msg *cap_resp)\n{\n\tif (!cap_resp->is_accepted) {\n\t\tpr_err(\"Capabilities not accepted by host\\n\");\n\t\tdm->state = DM_INIT_ERROR;\n\t}\n\tcomplete(&dm->host_event);\n}\n\nstatic void balloon_onchannelcallback(void *context)\n{\n\tstruct hv_device *dev = context;\n\tu32 recvlen;\n\tu64 requestid;\n\tstruct dm_message *dm_msg;\n\tstruct dm_header *dm_hdr;\n\tstruct hv_dynmem_device *dm = hv_get_drvdata(dev);\n\tstruct dm_balloon *bal_msg;\n\tstruct dm_hot_add *ha_msg;\n\tunion dm_mem_page_range *ha_pg_range;\n\tunion dm_mem_page_range *ha_region;\n\n\tmemset(recv_buffer, 0, sizeof(recv_buffer));\n\tvmbus_recvpacket(dev->channel, recv_buffer,\n\t\t\t HV_HYP_PAGE_SIZE, &recvlen, &requestid);\n\n\tif (recvlen > 0) {\n\t\tdm_msg = (struct dm_message *)recv_buffer;\n\t\tdm_hdr = &dm_msg->hdr;\n\n\t\tswitch (dm_hdr->type) {\n\t\tcase DM_VERSION_RESPONSE:\n\t\t\tversion_resp(dm,\n\t\t\t\t (struct dm_version_response *)dm_msg);\n\t\t\tbreak;\n\n\t\tcase DM_CAPABILITIES_RESPONSE:\n\t\t\tcap_resp(dm,\n\t\t\t\t (struct dm_capabilities_resp_msg *)dm_msg);\n\t\t\tbreak;\n\n\t\tcase DM_BALLOON_REQUEST:\n\t\t\tif (allow_hibernation) {\n\t\t\t\tpr_info(\"Ignore balloon-up request!\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (dm->state == DM_BALLOON_UP)\n\t\t\t\tpr_warn(\"Currently ballooning\\n\");\n\t\t\tbal_msg = (struct dm_balloon *)recv_buffer;\n\t\t\tdm->state = DM_BALLOON_UP;\n\t\t\tdm_device.balloon_wrk.num_pages = bal_msg->num_pages;\n\t\t\tschedule_work(&dm_device.balloon_wrk.wrk);\n\t\t\tbreak;\n\n\t\tcase DM_UNBALLOON_REQUEST:\n\t\t\tif (allow_hibernation) {\n\t\t\t\tpr_info(\"Ignore balloon-down request!\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tdm->state = DM_BALLOON_DOWN;\n\t\t\tballoon_down(dm,\n\t\t\t\t (struct dm_unballoon_request *)recv_buffer);\n\t\t\tbreak;\n\n\t\tcase DM_MEM_HOT_ADD_REQUEST:\n\t\t\tif (dm->state == DM_HOT_ADD)\n\t\t\t\tpr_warn(\"Currently hot-adding\\n\");\n\t\t\tdm->state = DM_HOT_ADD;\n\t\t\tha_msg = (struct dm_hot_add *)recv_buffer;\n\t\t\tif (ha_msg->hdr.size == sizeof(struct dm_hot_add)) {\n\t\t\t\t \n\t\t\t\tdm->host_specified_ha_region = false;\n\t\t\t\tha_pg_range = &ha_msg->range;\n\t\t\t\tdm->ha_wrk.ha_page_range = *ha_pg_range;\n\t\t\t\tdm->ha_wrk.ha_region_range.page_range = 0;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tdm->host_specified_ha_region = true;\n\t\t\t\tha_pg_range = &ha_msg->range;\n\t\t\t\tha_region = &ha_pg_range[1];\n\t\t\t\tdm->ha_wrk.ha_page_range = *ha_pg_range;\n\t\t\t\tdm->ha_wrk.ha_region_range = *ha_region;\n\t\t\t}\n\t\t\tschedule_work(&dm_device.ha_wrk.wrk);\n\t\t\tbreak;\n\n\t\tcase DM_INFO_MESSAGE:\n\t\t\tprocess_info(dm, (struct dm_info_msg *)dm_msg);\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpr_warn_ratelimited(\"Unhandled message: type: %d\\n\", dm_hdr->type);\n\n\t\t}\n\t}\n\n}\n\n#define HV_LARGE_REPORTING_ORDER\t9\n#define HV_LARGE_REPORTING_LEN (HV_HYP_PAGE_SIZE << \\\n\t\tHV_LARGE_REPORTING_ORDER)\nstatic int hv_free_page_report(struct page_reporting_dev_info *pr_dev_info,\n\t\t    struct scatterlist *sgl, unsigned int nents)\n{\n\tunsigned long flags;\n\tstruct hv_memory_hint *hint;\n\tint i, order;\n\tu64 status;\n\tstruct scatterlist *sg;\n\n\tWARN_ON_ONCE(nents > HV_MEMORY_HINT_MAX_GPA_PAGE_RANGES);\n\tWARN_ON_ONCE(sgl->length < (HV_HYP_PAGE_SIZE << page_reporting_order));\n\tlocal_irq_save(flags);\n\thint = *this_cpu_ptr(hyperv_pcpu_input_arg);\n\tif (!hint) {\n\t\tlocal_irq_restore(flags);\n\t\treturn -ENOSPC;\n\t}\n\n\thint->type = HV_EXT_MEMORY_HEAT_HINT_TYPE_COLD_DISCARD;\n\thint->reserved = 0;\n\tfor_each_sg(sgl, sg, nents, i) {\n\t\tunion hv_gpa_page_range *range;\n\n\t\trange = &hint->ranges[i];\n\t\trange->address_space = 0;\n\t\torder = get_order(sg->length);\n\t\t \n\n\t\t \n\t\tif (order >= HV_LARGE_REPORTING_ORDER ) {\n\t\t\trange->page.largepage = 1;\n\t\t\trange->page_size = HV_GPA_PAGE_RANGE_PAGE_SIZE_2MB;\n\t\t\trange->base_large_pfn = page_to_hvpfn(\n\t\t\t\t\tsg_page(sg)) >> HV_LARGE_REPORTING_ORDER;\n\t\t\trange->page.additional_pages =\n\t\t\t\t(sg->length / HV_LARGE_REPORTING_LEN) - 1;\n\t\t} else {\n\t\t\t \n\t\t\trange->page.basepfn = page_to_hvpfn(sg_page(sg));\n\t\t\trange->page.largepage = false;\n\t\t\trange->page.additional_pages =\n\t\t\t\t(sg->length / HV_HYP_PAGE_SIZE) - 1;\n\t\t}\n\n\t}\n\n\tstatus = hv_do_rep_hypercall(HV_EXT_CALL_MEMORY_HEAT_HINT, nents, 0,\n\t\t\t\t     hint, NULL);\n\tlocal_irq_restore(flags);\n\tif (!hv_result_success(status)) {\n\n\t\tpr_err(\"Cold memory discard hypercall failed with status %llx\\n\",\n\t\t\t\tstatus);\n\t\tif (hv_hypercall_multi_failure > 0)\n\t\t\thv_hypercall_multi_failure++;\n\n\t\tif (hv_result(status) == HV_STATUS_INVALID_PARAMETER) {\n\t\t\tpr_err(\"Underlying Hyper-V does not support order less than 9. Hypercall failed\\n\");\n\t\t\tpr_err(\"Defaulting to page_reporting_order %d\\n\",\n\t\t\t\t\tpageblock_order);\n\t\t\tpage_reporting_order = pageblock_order;\n\t\t\thv_hypercall_multi_failure++;\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void enable_page_reporting(void)\n{\n\tint ret;\n\n\tif (!hv_query_ext_cap(HV_EXT_CAPABILITY_MEMORY_COLD_DISCARD_HINT)) {\n\t\tpr_debug(\"Cold memory discard hint not supported by Hyper-V\\n\");\n\t\treturn;\n\t}\n\n\tBUILD_BUG_ON(PAGE_REPORTING_CAPACITY > HV_MEMORY_HINT_MAX_GPA_PAGE_RANGES);\n\tdm_device.pr_dev_info.report = hv_free_page_report;\n\t \n\tdm_device.pr_dev_info.order = 0;\n\tret = page_reporting_register(&dm_device.pr_dev_info);\n\tif (ret < 0) {\n\t\tdm_device.pr_dev_info.report = NULL;\n\t\tpr_err(\"Failed to enable cold memory discard: %d\\n\", ret);\n\t} else {\n\t\tpr_info(\"Cold memory discard hint enabled with order %d\\n\",\n\t\t\t\tpage_reporting_order);\n\t}\n}\n\nstatic void disable_page_reporting(void)\n{\n\tif (dm_device.pr_dev_info.report) {\n\t\tpage_reporting_unregister(&dm_device.pr_dev_info);\n\t\tdm_device.pr_dev_info.report = NULL;\n\t}\n}\n\nstatic int ballooning_enabled(void)\n{\n\t \n\tif (PAGE_SIZE != HV_HYP_PAGE_SIZE) {\n\t\tpr_info(\"Ballooning disabled because page size is not 4096 bytes\\n\");\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int hot_add_enabled(void)\n{\n\t \n\tif (IS_ENABLED(CONFIG_ARM64)) {\n\t\tpr_info(\"Memory hot add disabled on ARM64\\n\");\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int balloon_connect_vsp(struct hv_device *dev)\n{\n\tstruct dm_version_request version_req;\n\tstruct dm_capabilities cap_msg;\n\tunsigned long t;\n\tint ret;\n\n\t \n\tdev->channel->max_pkt_size = HV_HYP_PAGE_SIZE * 2;\n\n\tret = vmbus_open(dev->channel, dm_ring_size, dm_ring_size, NULL, 0,\n\t\t\t balloon_onchannelcallback, dev);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tmemset(&version_req, 0, sizeof(struct dm_version_request));\n\tversion_req.hdr.type = DM_VERSION_REQUEST;\n\tversion_req.hdr.size = sizeof(struct dm_version_request);\n\tversion_req.hdr.trans_id = atomic_inc_return(&trans_id);\n\tversion_req.version.version = DYNMEM_PROTOCOL_VERSION_WIN10;\n\tversion_req.is_last_attempt = 0;\n\tdm_device.version = version_req.version.version;\n\n\tret = vmbus_sendpacket(dev->channel, &version_req,\n\t\t\t       sizeof(struct dm_version_request),\n\t\t\t       (unsigned long)NULL, VM_PKT_DATA_INBAND, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tt = wait_for_completion_timeout(&dm_device.host_event, 5*HZ);\n\tif (t == 0) {\n\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t \n\tif (dm_device.state == DM_INIT_ERROR) {\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\tpr_info(\"Using Dynamic Memory protocol version %u.%u\\n\",\n\t\tDYNMEM_MAJOR_VERSION(dm_device.version),\n\t\tDYNMEM_MINOR_VERSION(dm_device.version));\n\n\t \n\tmemset(&cap_msg, 0, sizeof(struct dm_capabilities));\n\tcap_msg.hdr.type = DM_CAPABILITIES_REPORT;\n\tcap_msg.hdr.size = sizeof(struct dm_capabilities);\n\tcap_msg.hdr.trans_id = atomic_inc_return(&trans_id);\n\n\t \n\tcap_msg.caps.cap_bits.balloon = ballooning_enabled();\n\tcap_msg.caps.cap_bits.hot_add = hot_add_enabled();\n\n\t \n\tcap_msg.caps.cap_bits.hot_add_alignment = 7;\n\n\t \n\tcap_msg.min_page_cnt = 0;\n\tcap_msg.max_page_number = -1;\n\n\tret = vmbus_sendpacket(dev->channel, &cap_msg,\n\t\t\t       sizeof(struct dm_capabilities),\n\t\t\t       (unsigned long)NULL, VM_PKT_DATA_INBAND, 0);\n\tif (ret)\n\t\tgoto out;\n\n\tt = wait_for_completion_timeout(&dm_device.host_event, 5*HZ);\n\tif (t == 0) {\n\t\tret = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\t \n\tif (dm_device.state == DM_INIT_ERROR) {\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\treturn 0;\nout:\n\tvmbus_close(dev->channel);\n\treturn ret;\n}\n\n \n#ifdef CONFIG_DEBUG_FS\n\n \nstatic int hv_balloon_debug_show(struct seq_file *f, void *offset)\n{\n\tstruct hv_dynmem_device *dm = f->private;\n\tchar *sname;\n\n\tseq_printf(f, \"%-22s: %u.%u\\n\", \"host_version\",\n\t\t\t\tDYNMEM_MAJOR_VERSION(dm->version),\n\t\t\t\tDYNMEM_MINOR_VERSION(dm->version));\n\n\tseq_printf(f, \"%-22s:\", \"capabilities\");\n\tif (ballooning_enabled())\n\t\tseq_puts(f, \" enabled\");\n\n\tif (hot_add_enabled())\n\t\tseq_puts(f, \" hot_add\");\n\n\tseq_puts(f, \"\\n\");\n\n\tseq_printf(f, \"%-22s: %u\", \"state\", dm->state);\n\tswitch (dm->state) {\n\tcase DM_INITIALIZING:\n\t\t\tsname = \"Initializing\";\n\t\t\tbreak;\n\tcase DM_INITIALIZED:\n\t\t\tsname = \"Initialized\";\n\t\t\tbreak;\n\tcase DM_BALLOON_UP:\n\t\t\tsname = \"Balloon Up\";\n\t\t\tbreak;\n\tcase DM_BALLOON_DOWN:\n\t\t\tsname = \"Balloon Down\";\n\t\t\tbreak;\n\tcase DM_HOT_ADD:\n\t\t\tsname = \"Hot Add\";\n\t\t\tbreak;\n\tcase DM_INIT_ERROR:\n\t\t\tsname = \"Error\";\n\t\t\tbreak;\n\tdefault:\n\t\t\tsname = \"Unknown\";\n\t}\n\tseq_printf(f, \" (%s)\\n\", sname);\n\n\t \n\tseq_printf(f, \"%-22s: %ld\\n\", \"page_size\", HV_HYP_PAGE_SIZE);\n\n\t \n\tseq_printf(f, \"%-22s: %u\\n\", \"pages_added\", dm->num_pages_added);\n\n\t \n\tseq_printf(f, \"%-22s: %u\\n\", \"pages_onlined\", dm->num_pages_onlined);\n\n\t \n\tseq_printf(f, \"%-22s: %u\\n\", \"pages_ballooned\", dm->num_pages_ballooned);\n\n\tseq_printf(f, \"%-22s: %lu\\n\", \"total_pages_committed\",\n\t\t\t\tget_pages_committed(dm));\n\n\tseq_printf(f, \"%-22s: %llu\\n\", \"max_dynamic_page_count\",\n\t\t\t\tdm->max_dynamic_page_count);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(hv_balloon_debug);\n\nstatic void  hv_balloon_debugfs_init(struct hv_dynmem_device *b)\n{\n\tdebugfs_create_file(\"hv-balloon\", 0444, NULL, b,\n\t\t\t&hv_balloon_debug_fops);\n}\n\nstatic void  hv_balloon_debugfs_exit(struct hv_dynmem_device *b)\n{\n\tdebugfs_lookup_and_remove(\"hv-balloon\", NULL);\n}\n\n#else\n\nstatic inline void hv_balloon_debugfs_init(struct hv_dynmem_device  *b)\n{\n}\n\nstatic inline void hv_balloon_debugfs_exit(struct hv_dynmem_device *b)\n{\n}\n\n#endif\t \n\nstatic int balloon_probe(struct hv_device *dev,\n\t\t\t const struct hv_vmbus_device_id *dev_id)\n{\n\tint ret;\n\n\tallow_hibernation = hv_is_hibernation_supported();\n\tif (allow_hibernation)\n\t\thot_add = false;\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tdo_hot_add = hot_add;\n#else\n\tdo_hot_add = false;\n#endif\n\tdm_device.dev = dev;\n\tdm_device.state = DM_INITIALIZING;\n\tdm_device.next_version = DYNMEM_PROTOCOL_VERSION_WIN8;\n\tinit_completion(&dm_device.host_event);\n\tinit_completion(&dm_device.config_event);\n\tINIT_LIST_HEAD(&dm_device.ha_region_list);\n\tspin_lock_init(&dm_device.ha_lock);\n\tINIT_WORK(&dm_device.balloon_wrk.wrk, balloon_up);\n\tINIT_WORK(&dm_device.ha_wrk.wrk, hot_add_req);\n\tdm_device.host_specified_ha_region = false;\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tset_online_page_callback(&hv_online_page);\n\tinit_completion(&dm_device.ol_waitevent);\n\tregister_memory_notifier(&hv_memory_nb);\n#endif\n\n\thv_set_drvdata(dev, &dm_device);\n\n\tret = balloon_connect_vsp(dev);\n\tif (ret != 0)\n\t\tgoto connect_error;\n\n\tenable_page_reporting();\n\tdm_device.state = DM_INITIALIZED;\n\n\tdm_device.thread =\n\t\t kthread_run(dm_thread_func, &dm_device, \"hv_balloon\");\n\tif (IS_ERR(dm_device.thread)) {\n\t\tret = PTR_ERR(dm_device.thread);\n\t\tgoto probe_error;\n\t}\n\n\thv_balloon_debugfs_init(&dm_device);\n\n\treturn 0;\n\nprobe_error:\n\tdm_device.state = DM_INIT_ERROR;\n\tdm_device.thread  = NULL;\n\tdisable_page_reporting();\n\tvmbus_close(dev->channel);\nconnect_error:\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tunregister_memory_notifier(&hv_memory_nb);\n\trestore_online_page_callback(&hv_online_page);\n#endif\n\treturn ret;\n}\n\nstatic void balloon_remove(struct hv_device *dev)\n{\n\tstruct hv_dynmem_device *dm = hv_get_drvdata(dev);\n\tstruct hv_hotadd_state *has, *tmp;\n\tstruct hv_hotadd_gap *gap, *tmp_gap;\n\n\tif (dm->num_pages_ballooned != 0)\n\t\tpr_warn(\"Ballooned pages: %d\\n\", dm->num_pages_ballooned);\n\n\thv_balloon_debugfs_exit(dm);\n\n\tcancel_work_sync(&dm->balloon_wrk.wrk);\n\tcancel_work_sync(&dm->ha_wrk.wrk);\n\n\tkthread_stop(dm->thread);\n\n\t \n\tif (dm_device.state != DM_INIT_ERROR) {\n\t\tdisable_page_reporting();\n\t\tvmbus_close(dev->channel);\n#ifdef CONFIG_MEMORY_HOTPLUG\n\t\tunregister_memory_notifier(&hv_memory_nb);\n\t\trestore_online_page_callback(&hv_online_page);\n#endif\n\t}\n\n\tguard(spinlock_irqsave)(&dm_device.ha_lock);\n\tlist_for_each_entry_safe(has, tmp, &dm->ha_region_list, list) {\n\t\tlist_for_each_entry_safe(gap, tmp_gap, &has->gap_list, list) {\n\t\t\tlist_del(&gap->list);\n\t\t\tkfree(gap);\n\t\t}\n\t\tlist_del(&has->list);\n\t\tkfree(has);\n\t}\n}\n\nstatic int balloon_suspend(struct hv_device *hv_dev)\n{\n\tstruct hv_dynmem_device *dm = hv_get_drvdata(hv_dev);\n\n\ttasklet_disable(&hv_dev->channel->callback_event);\n\n\tcancel_work_sync(&dm->balloon_wrk.wrk);\n\tcancel_work_sync(&dm->ha_wrk.wrk);\n\n\tif (dm->thread) {\n\t\tkthread_stop(dm->thread);\n\t\tdm->thread = NULL;\n\t\tvmbus_close(hv_dev->channel);\n\t}\n\n\ttasklet_enable(&hv_dev->channel->callback_event);\n\n\treturn 0;\n\n}\n\nstatic int balloon_resume(struct hv_device *dev)\n{\n\tint ret;\n\n\tdm_device.state = DM_INITIALIZING;\n\n\tret = balloon_connect_vsp(dev);\n\n\tif (ret != 0)\n\t\tgoto out;\n\n\tdm_device.thread =\n\t\t kthread_run(dm_thread_func, &dm_device, \"hv_balloon\");\n\tif (IS_ERR(dm_device.thread)) {\n\t\tret = PTR_ERR(dm_device.thread);\n\t\tdm_device.thread = NULL;\n\t\tgoto close_channel;\n\t}\n\n\tdm_device.state = DM_INITIALIZED;\n\treturn 0;\nclose_channel:\n\tvmbus_close(dev->channel);\nout:\n\tdm_device.state = DM_INIT_ERROR;\n\tdisable_page_reporting();\n#ifdef CONFIG_MEMORY_HOTPLUG\n\tunregister_memory_notifier(&hv_memory_nb);\n\trestore_online_page_callback(&hv_online_page);\n#endif\n\treturn ret;\n}\n\nstatic const struct hv_vmbus_device_id id_table[] = {\n\t \n\t \n\t{ HV_DM_GUID, },\n\t{ },\n};\n\nMODULE_DEVICE_TABLE(vmbus, id_table);\n\nstatic  struct hv_driver balloon_drv = {\n\t.name = \"hv_balloon\",\n\t.id_table = id_table,\n\t.probe =  balloon_probe,\n\t.remove =  balloon_remove,\n\t.suspend = balloon_suspend,\n\t.resume = balloon_resume,\n\t.driver = {\n\t\t.probe_type = PROBE_PREFER_ASYNCHRONOUS,\n\t},\n};\n\nstatic int __init init_balloon_drv(void)\n{\n\n\treturn vmbus_driver_register(&balloon_drv);\n}\n\nmodule_init(init_balloon_drv);\n\nMODULE_DESCRIPTION(\"Hyper-V Balloon\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}