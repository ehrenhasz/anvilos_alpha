{
  "module_name": "vmbus_drv.c",
  "hash_id": "9493bc321ad2a6e185a8ce7ae322227a5ed1630af35c34704c4d5939707bbd21",
  "original_prompt": "Ingested from linux-6.6.14/drivers/hv/vmbus_drv.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/device.h>\n#include <linux/platform_device.h>\n#include <linux/interrupt.h>\n#include <linux/sysctl.h>\n#include <linux/slab.h>\n#include <linux/acpi.h>\n#include <linux/completion.h>\n#include <linux/hyperv.h>\n#include <linux/kernel_stat.h>\n#include <linux/of_address.h>\n#include <linux/clockchips.h>\n#include <linux/cpu.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/task_stack.h>\n\n#include <linux/delay.h>\n#include <linux/panic_notifier.h>\n#include <linux/ptrace.h>\n#include <linux/screen_info.h>\n#include <linux/efi.h>\n#include <linux/random.h>\n#include <linux/kernel.h>\n#include <linux/syscore_ops.h>\n#include <linux/dma-map-ops.h>\n#include <linux/pci.h>\n#include <clocksource/hyperv_timer.h>\n#include <asm/mshyperv.h>\n#include \"hyperv_vmbus.h\"\n\nstruct vmbus_dynid {\n\tstruct list_head node;\n\tstruct hv_vmbus_device_id id;\n};\n\nstatic struct device  *hv_dev;\n\nstatic int hyperv_cpuhp_online;\n\nstatic long __percpu *vmbus_evt;\n\n \nint vmbus_irq;\nint vmbus_interrupt;\n\n \nstatic int hv_panic_vmbus_unload(struct notifier_block *nb, unsigned long val,\n\t\t\t      void *args)\n{\n\tvmbus_initiate_unload(true);\n\treturn NOTIFY_DONE;\n}\nstatic struct notifier_block hyperv_panic_vmbus_unload_block = {\n\t.notifier_call\t= hv_panic_vmbus_unload,\n\t.priority\t= INT_MIN + 1,  \n};\n\nstatic const char *fb_mmio_name = \"fb_range\";\nstatic struct resource *fb_mmio;\nstatic struct resource *hyperv_mmio;\nstatic DEFINE_MUTEX(hyperv_mmio_lock);\n\nstatic int vmbus_exists(void)\n{\n\tif (hv_dev == NULL)\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\nstatic u8 channel_monitor_group(const struct vmbus_channel *channel)\n{\n\treturn (u8)channel->offermsg.monitorid / 32;\n}\n\nstatic u8 channel_monitor_offset(const struct vmbus_channel *channel)\n{\n\treturn (u8)channel->offermsg.monitorid % 32;\n}\n\nstatic u32 channel_pending(const struct vmbus_channel *channel,\n\t\t\t   const struct hv_monitor_page *monitor_page)\n{\n\tu8 monitor_group = channel_monitor_group(channel);\n\n\treturn monitor_page->trigger_group[monitor_group].pending;\n}\n\nstatic u32 channel_latency(const struct vmbus_channel *channel,\n\t\t\t   const struct hv_monitor_page *monitor_page)\n{\n\tu8 monitor_group = channel_monitor_group(channel);\n\tu8 monitor_offset = channel_monitor_offset(channel);\n\n\treturn monitor_page->latency[monitor_group][monitor_offset];\n}\n\nstatic u32 channel_conn_id(struct vmbus_channel *channel,\n\t\t\t   struct hv_monitor_page *monitor_page)\n{\n\tu8 monitor_group = channel_monitor_group(channel);\n\tu8 monitor_offset = channel_monitor_offset(channel);\n\n\treturn monitor_page->parameter[monitor_group][monitor_offset].connectionid.u.id;\n}\n\nstatic ssize_t id_show(struct device *dev, struct device_attribute *dev_attr,\n\t\t       char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\", hv_dev->channel->offermsg.child_relid);\n}\nstatic DEVICE_ATTR_RO(id);\n\nstatic ssize_t state_show(struct device *dev, struct device_attribute *dev_attr,\n\t\t\t  char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\", hv_dev->channel->state);\n}\nstatic DEVICE_ATTR_RO(state);\n\nstatic ssize_t monitor_id_show(struct device *dev,\n\t\t\t       struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\", hv_dev->channel->offermsg.monitorid);\n}\nstatic DEVICE_ATTR_RO(monitor_id);\n\nstatic ssize_t class_id_show(struct device *dev,\n\t\t\t       struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"{%pUl}\\n\",\n\t\t       &hv_dev->channel->offermsg.offer.if_type);\n}\nstatic DEVICE_ATTR_RO(class_id);\n\nstatic ssize_t device_id_show(struct device *dev,\n\t\t\t      struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"{%pUl}\\n\",\n\t\t       &hv_dev->channel->offermsg.offer.if_instance);\n}\nstatic DEVICE_ATTR_RO(device_id);\n\nstatic ssize_t modalias_show(struct device *dev,\n\t\t\t     struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\treturn sprintf(buf, \"vmbus:%*phN\\n\", UUID_SIZE, &hv_dev->dev_type);\n}\nstatic DEVICE_ATTR_RO(modalias);\n\n#ifdef CONFIG_NUMA\nstatic ssize_t numa_node_show(struct device *dev,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\treturn sprintf(buf, \"%d\\n\", cpu_to_node(hv_dev->channel->target_cpu));\n}\nstatic DEVICE_ATTR_RO(numa_node);\n#endif\n\nstatic ssize_t server_monitor_pending_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_pending(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[0]));\n}\nstatic DEVICE_ATTR_RO(server_monitor_pending);\n\nstatic ssize_t client_monitor_pending_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_pending(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[1]));\n}\nstatic DEVICE_ATTR_RO(client_monitor_pending);\n\nstatic ssize_t server_monitor_latency_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_latency(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[0]));\n}\nstatic DEVICE_ATTR_RO(server_monitor_latency);\n\nstatic ssize_t client_monitor_latency_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_latency(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[1]));\n}\nstatic DEVICE_ATTR_RO(client_monitor_latency);\n\nstatic ssize_t server_monitor_conn_id_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_conn_id(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[0]));\n}\nstatic DEVICE_ATTR_RO(server_monitor_conn_id);\n\nstatic ssize_t client_monitor_conn_id_show(struct device *dev,\n\t\t\t\t\t   struct device_attribute *dev_attr,\n\t\t\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_conn_id(hv_dev->channel,\n\t\t\t\t       vmbus_connection.monitor_pages[1]));\n}\nstatic DEVICE_ATTR_RO(client_monitor_conn_id);\n\nstatic ssize_t out_intr_mask_show(struct device *dev,\n\t\t\t\t  struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info outbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->outbound,\n\t\t\t\t\t  &outbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", outbound.current_interrupt_mask);\n}\nstatic DEVICE_ATTR_RO(out_intr_mask);\n\nstatic ssize_t out_read_index_show(struct device *dev,\n\t\t\t\t   struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info outbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->outbound,\n\t\t\t\t\t  &outbound);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn sprintf(buf, \"%d\\n\", outbound.current_read_index);\n}\nstatic DEVICE_ATTR_RO(out_read_index);\n\nstatic ssize_t out_write_index_show(struct device *dev,\n\t\t\t\t    struct device_attribute *dev_attr,\n\t\t\t\t    char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info outbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->outbound,\n\t\t\t\t\t  &outbound);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn sprintf(buf, \"%d\\n\", outbound.current_write_index);\n}\nstatic DEVICE_ATTR_RO(out_write_index);\n\nstatic ssize_t out_read_bytes_avail_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *dev_attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info outbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->outbound,\n\t\t\t\t\t  &outbound);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn sprintf(buf, \"%d\\n\", outbound.bytes_avail_toread);\n}\nstatic DEVICE_ATTR_RO(out_read_bytes_avail);\n\nstatic ssize_t out_write_bytes_avail_show(struct device *dev,\n\t\t\t\t\t  struct device_attribute *dev_attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info outbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->outbound,\n\t\t\t\t\t  &outbound);\n\tif (ret < 0)\n\t\treturn ret;\n\treturn sprintf(buf, \"%d\\n\", outbound.bytes_avail_towrite);\n}\nstatic DEVICE_ATTR_RO(out_write_bytes_avail);\n\nstatic ssize_t in_intr_mask_show(struct device *dev,\n\t\t\t\t struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info inbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->inbound, &inbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", inbound.current_interrupt_mask);\n}\nstatic DEVICE_ATTR_RO(in_intr_mask);\n\nstatic ssize_t in_read_index_show(struct device *dev,\n\t\t\t\t  struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info inbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->inbound, &inbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", inbound.current_read_index);\n}\nstatic DEVICE_ATTR_RO(in_read_index);\n\nstatic ssize_t in_write_index_show(struct device *dev,\n\t\t\t\t   struct device_attribute *dev_attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info inbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->inbound, &inbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", inbound.current_write_index);\n}\nstatic DEVICE_ATTR_RO(in_write_index);\n\nstatic ssize_t in_read_bytes_avail_show(struct device *dev,\n\t\t\t\t\tstruct device_attribute *dev_attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info inbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->inbound, &inbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", inbound.bytes_avail_toread);\n}\nstatic DEVICE_ATTR_RO(in_read_bytes_avail);\n\nstatic ssize_t in_write_bytes_avail_show(struct device *dev,\n\t\t\t\t\t struct device_attribute *dev_attr,\n\t\t\t\t\t char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct hv_ring_buffer_debug_info inbound;\n\tint ret;\n\n\tif (!hv_dev->channel)\n\t\treturn -ENODEV;\n\n\tret = hv_ringbuffer_get_debuginfo(&hv_dev->channel->inbound, &inbound);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn sprintf(buf, \"%d\\n\", inbound.bytes_avail_towrite);\n}\nstatic DEVICE_ATTR_RO(in_write_bytes_avail);\n\nstatic ssize_t channel_vp_mapping_show(struct device *dev,\n\t\t\t\t       struct device_attribute *dev_attr,\n\t\t\t\t       char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tstruct vmbus_channel *channel = hv_dev->channel, *cur_sc;\n\tint buf_size = PAGE_SIZE, n_written, tot_written;\n\tstruct list_head *cur;\n\n\tif (!channel)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&vmbus_connection.channel_mutex);\n\n\ttot_written = snprintf(buf, buf_size, \"%u:%u\\n\",\n\t\tchannel->offermsg.child_relid, channel->target_cpu);\n\n\tlist_for_each(cur, &channel->sc_list) {\n\t\tif (tot_written >= buf_size - 1)\n\t\t\tbreak;\n\n\t\tcur_sc = list_entry(cur, struct vmbus_channel, sc_list);\n\t\tn_written = scnprintf(buf + tot_written,\n\t\t\t\t     buf_size - tot_written,\n\t\t\t\t     \"%u:%u\\n\",\n\t\t\t\t     cur_sc->offermsg.child_relid,\n\t\t\t\t     cur_sc->target_cpu);\n\t\ttot_written += n_written;\n\t}\n\n\tmutex_unlock(&vmbus_connection.channel_mutex);\n\n\treturn tot_written;\n}\nstatic DEVICE_ATTR_RO(channel_vp_mapping);\n\nstatic ssize_t vendor_show(struct device *dev,\n\t\t\t   struct device_attribute *dev_attr,\n\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\treturn sprintf(buf, \"0x%x\\n\", hv_dev->vendor_id);\n}\nstatic DEVICE_ATTR_RO(vendor);\n\nstatic ssize_t device_show(struct device *dev,\n\t\t\t   struct device_attribute *dev_attr,\n\t\t\t   char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\n\treturn sprintf(buf, \"0x%x\\n\", hv_dev->device_id);\n}\nstatic DEVICE_ATTR_RO(device);\n\nstatic ssize_t driver_override_store(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tint ret;\n\n\tret = driver_set_override(dev, &hv_dev->driver_override, buf, count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn count;\n}\n\nstatic ssize_t driver_override_show(struct device *dev,\n\t\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(dev);\n\tssize_t len;\n\n\tdevice_lock(dev);\n\tlen = snprintf(buf, PAGE_SIZE, \"%s\\n\", hv_dev->driver_override);\n\tdevice_unlock(dev);\n\n\treturn len;\n}\nstatic DEVICE_ATTR_RW(driver_override);\n\n \nstatic struct attribute *vmbus_dev_attrs[] = {\n\t&dev_attr_id.attr,\n\t&dev_attr_state.attr,\n\t&dev_attr_monitor_id.attr,\n\t&dev_attr_class_id.attr,\n\t&dev_attr_device_id.attr,\n\t&dev_attr_modalias.attr,\n#ifdef CONFIG_NUMA\n\t&dev_attr_numa_node.attr,\n#endif\n\t&dev_attr_server_monitor_pending.attr,\n\t&dev_attr_client_monitor_pending.attr,\n\t&dev_attr_server_monitor_latency.attr,\n\t&dev_attr_client_monitor_latency.attr,\n\t&dev_attr_server_monitor_conn_id.attr,\n\t&dev_attr_client_monitor_conn_id.attr,\n\t&dev_attr_out_intr_mask.attr,\n\t&dev_attr_out_read_index.attr,\n\t&dev_attr_out_write_index.attr,\n\t&dev_attr_out_read_bytes_avail.attr,\n\t&dev_attr_out_write_bytes_avail.attr,\n\t&dev_attr_in_intr_mask.attr,\n\t&dev_attr_in_read_index.attr,\n\t&dev_attr_in_write_index.attr,\n\t&dev_attr_in_read_bytes_avail.attr,\n\t&dev_attr_in_write_bytes_avail.attr,\n\t&dev_attr_channel_vp_mapping.attr,\n\t&dev_attr_vendor.attr,\n\t&dev_attr_device.attr,\n\t&dev_attr_driver_override.attr,\n\tNULL,\n};\n\n \nstatic umode_t vmbus_dev_attr_is_visible(struct kobject *kobj,\n\t\t\t\t\t struct attribute *attr, int idx)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tconst struct hv_device *hv_dev = device_to_hv_device(dev);\n\n\t \n\tif (!hv_dev->channel->offermsg.monitor_allocated &&\n\t    (attr == &dev_attr_monitor_id.attr ||\n\t     attr == &dev_attr_server_monitor_pending.attr ||\n\t     attr == &dev_attr_client_monitor_pending.attr ||\n\t     attr == &dev_attr_server_monitor_latency.attr ||\n\t     attr == &dev_attr_client_monitor_latency.attr ||\n\t     attr == &dev_attr_server_monitor_conn_id.attr ||\n\t     attr == &dev_attr_client_monitor_conn_id.attr))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic const struct attribute_group vmbus_dev_group = {\n\t.attrs = vmbus_dev_attrs,\n\t.is_visible = vmbus_dev_attr_is_visible\n};\n__ATTRIBUTE_GROUPS(vmbus_dev);\n\n \nstatic ssize_t hibernation_show(const struct bus_type *bus, char *buf)\n{\n\treturn sprintf(buf, \"%d\\n\", !!hv_is_hibernation_supported());\n}\n\nstatic BUS_ATTR_RO(hibernation);\n\nstatic struct attribute *vmbus_bus_attrs[] = {\n\t&bus_attr_hibernation.attr,\n\tNULL,\n};\nstatic const struct attribute_group vmbus_bus_group = {\n\t.attrs = vmbus_bus_attrs,\n};\n__ATTRIBUTE_GROUPS(vmbus_bus);\n\n \nstatic int vmbus_uevent(const struct device *device, struct kobj_uevent_env *env)\n{\n\tconst struct hv_device *dev = device_to_hv_device(device);\n\tconst char *format = \"MODALIAS=vmbus:%*phN\";\n\n\treturn add_uevent_var(env, format, UUID_SIZE, &dev->dev_type);\n}\n\nstatic const struct hv_vmbus_device_id *\nhv_vmbus_dev_match(const struct hv_vmbus_device_id *id, const guid_t *guid)\n{\n\tif (id == NULL)\n\t\treturn NULL;  \n\n\tfor (; !guid_is_null(&id->guid); id++)\n\t\tif (guid_equal(&id->guid, guid))\n\t\t\treturn id;\n\n\treturn NULL;\n}\n\nstatic const struct hv_vmbus_device_id *\nhv_vmbus_dynid_match(struct hv_driver *drv, const guid_t *guid)\n{\n\tconst struct hv_vmbus_device_id *id = NULL;\n\tstruct vmbus_dynid *dynid;\n\n\tspin_lock(&drv->dynids.lock);\n\tlist_for_each_entry(dynid, &drv->dynids.list, node) {\n\t\tif (guid_equal(&dynid->id.guid, guid)) {\n\t\t\tid = &dynid->id;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&drv->dynids.lock);\n\n\treturn id;\n}\n\nstatic const struct hv_vmbus_device_id vmbus_device_null;\n\n \nstatic const struct hv_vmbus_device_id *hv_vmbus_get_id(struct hv_driver *drv,\n\t\t\t\t\t\t\tstruct hv_device *dev)\n{\n\tconst guid_t *guid = &dev->dev_type;\n\tconst struct hv_vmbus_device_id *id;\n\n\t \n\tif (dev->driver_override && strcmp(dev->driver_override, drv->name))\n\t\treturn NULL;\n\n\t \n\tid = hv_vmbus_dynid_match(drv, guid);\n\tif (!id)\n\t\tid = hv_vmbus_dev_match(drv->id_table, guid);\n\n\t \n\tif (!id && dev->driver_override)\n\t\tid = &vmbus_device_null;\n\n\treturn id;\n}\n\n \nstatic int vmbus_add_dynid(struct hv_driver *drv, guid_t *guid)\n{\n\tstruct vmbus_dynid *dynid;\n\n\tdynid = kzalloc(sizeof(*dynid), GFP_KERNEL);\n\tif (!dynid)\n\t\treturn -ENOMEM;\n\n\tdynid->id.guid = *guid;\n\n\tspin_lock(&drv->dynids.lock);\n\tlist_add_tail(&dynid->node, &drv->dynids.list);\n\tspin_unlock(&drv->dynids.lock);\n\n\treturn driver_attach(&drv->driver);\n}\n\nstatic void vmbus_free_dynids(struct hv_driver *drv)\n{\n\tstruct vmbus_dynid *dynid, *n;\n\n\tspin_lock(&drv->dynids.lock);\n\tlist_for_each_entry_safe(dynid, n, &drv->dynids.list, node) {\n\t\tlist_del(&dynid->node);\n\t\tkfree(dynid);\n\t}\n\tspin_unlock(&drv->dynids.lock);\n}\n\n \nstatic ssize_t new_id_store(struct device_driver *driver, const char *buf,\n\t\t\t    size_t count)\n{\n\tstruct hv_driver *drv = drv_to_hv_drv(driver);\n\tguid_t guid;\n\tssize_t retval;\n\n\tretval = guid_parse(buf, &guid);\n\tif (retval)\n\t\treturn retval;\n\n\tif (hv_vmbus_dynid_match(drv, &guid))\n\t\treturn -EEXIST;\n\n\tretval = vmbus_add_dynid(drv, &guid);\n\tif (retval)\n\t\treturn retval;\n\treturn count;\n}\nstatic DRIVER_ATTR_WO(new_id);\n\n \nstatic ssize_t remove_id_store(struct device_driver *driver, const char *buf,\n\t\t\t       size_t count)\n{\n\tstruct hv_driver *drv = drv_to_hv_drv(driver);\n\tstruct vmbus_dynid *dynid, *n;\n\tguid_t guid;\n\tssize_t retval;\n\n\tretval = guid_parse(buf, &guid);\n\tif (retval)\n\t\treturn retval;\n\n\tretval = -ENODEV;\n\tspin_lock(&drv->dynids.lock);\n\tlist_for_each_entry_safe(dynid, n, &drv->dynids.list, node) {\n\t\tstruct hv_vmbus_device_id *id = &dynid->id;\n\n\t\tif (guid_equal(&id->guid, &guid)) {\n\t\t\tlist_del(&dynid->node);\n\t\t\tkfree(dynid);\n\t\t\tretval = count;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&drv->dynids.lock);\n\n\treturn retval;\n}\nstatic DRIVER_ATTR_WO(remove_id);\n\nstatic struct attribute *vmbus_drv_attrs[] = {\n\t&driver_attr_new_id.attr,\n\t&driver_attr_remove_id.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(vmbus_drv);\n\n\n \nstatic int vmbus_match(struct device *device, struct device_driver *driver)\n{\n\tstruct hv_driver *drv = drv_to_hv_drv(driver);\n\tstruct hv_device *hv_dev = device_to_hv_device(device);\n\n\t \n\tif (is_hvsock_channel(hv_dev->channel))\n\t\treturn drv->hvsock;\n\n\tif (hv_vmbus_get_id(drv, hv_dev))\n\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int vmbus_probe(struct device *child_device)\n{\n\tint ret = 0;\n\tstruct hv_driver *drv =\n\t\t\tdrv_to_hv_drv(child_device->driver);\n\tstruct hv_device *dev = device_to_hv_device(child_device);\n\tconst struct hv_vmbus_device_id *dev_id;\n\n\tdev_id = hv_vmbus_get_id(drv, dev);\n\tif (drv->probe) {\n\t\tret = drv->probe(dev, dev_id);\n\t\tif (ret != 0)\n\t\t\tpr_err(\"probe failed for device %s (%d)\\n\",\n\t\t\t       dev_name(child_device), ret);\n\n\t} else {\n\t\tpr_err(\"probe not set for driver %s\\n\",\n\t\t       dev_name(child_device));\n\t\tret = -ENODEV;\n\t}\n\treturn ret;\n}\n\n \nstatic int vmbus_dma_configure(struct device *child_device)\n{\n\t \n\thv_setup_dma_ops(child_device,\n\t\tdevice_get_dma_attr(hv_dev) == DEV_DMA_COHERENT);\n\treturn 0;\n}\n\n \nstatic void vmbus_remove(struct device *child_device)\n{\n\tstruct hv_driver *drv;\n\tstruct hv_device *dev = device_to_hv_device(child_device);\n\n\tif (child_device->driver) {\n\t\tdrv = drv_to_hv_drv(child_device->driver);\n\t\tif (drv->remove)\n\t\t\tdrv->remove(dev);\n\t}\n}\n\n \nstatic void vmbus_shutdown(struct device *child_device)\n{\n\tstruct hv_driver *drv;\n\tstruct hv_device *dev = device_to_hv_device(child_device);\n\n\n\t \n\tif (!child_device->driver)\n\t\treturn;\n\n\tdrv = drv_to_hv_drv(child_device->driver);\n\n\tif (drv->shutdown)\n\t\tdrv->shutdown(dev);\n}\n\n#ifdef CONFIG_PM_SLEEP\n \nstatic int vmbus_suspend(struct device *child_device)\n{\n\tstruct hv_driver *drv;\n\tstruct hv_device *dev = device_to_hv_device(child_device);\n\n\t \n\tif (!child_device->driver)\n\t\treturn 0;\n\n\tdrv = drv_to_hv_drv(child_device->driver);\n\tif (!drv->suspend)\n\t\treturn -EOPNOTSUPP;\n\n\treturn drv->suspend(dev);\n}\n\n \nstatic int vmbus_resume(struct device *child_device)\n{\n\tstruct hv_driver *drv;\n\tstruct hv_device *dev = device_to_hv_device(child_device);\n\n\t \n\tif (!child_device->driver)\n\t\treturn 0;\n\n\tdrv = drv_to_hv_drv(child_device->driver);\n\tif (!drv->resume)\n\t\treturn -EOPNOTSUPP;\n\n\treturn drv->resume(dev);\n}\n#else\n#define vmbus_suspend NULL\n#define vmbus_resume NULL\n#endif  \n\n \nstatic void vmbus_device_release(struct device *device)\n{\n\tstruct hv_device *hv_dev = device_to_hv_device(device);\n\tstruct vmbus_channel *channel = hv_dev->channel;\n\n\thv_debug_rm_dev_dir(hv_dev);\n\n\tmutex_lock(&vmbus_connection.channel_mutex);\n\thv_process_channel_removal(channel);\n\tmutex_unlock(&vmbus_connection.channel_mutex);\n\tkfree(hv_dev);\n}\n\n \n\nstatic const struct dev_pm_ops vmbus_pm = {\n\t.suspend_noirq\t= NULL,\n\t.resume_noirq\t= NULL,\n\t.freeze_noirq\t= vmbus_suspend,\n\t.thaw_noirq\t= vmbus_resume,\n\t.poweroff_noirq\t= vmbus_suspend,\n\t.restore_noirq\t= vmbus_resume,\n};\n\n \nstatic struct bus_type  hv_bus = {\n\t.name =\t\t\"vmbus\",\n\t.match =\t\tvmbus_match,\n\t.shutdown =\t\tvmbus_shutdown,\n\t.remove =\t\tvmbus_remove,\n\t.probe =\t\tvmbus_probe,\n\t.uevent =\t\tvmbus_uevent,\n\t.dma_configure =\tvmbus_dma_configure,\n\t.dev_groups =\t\tvmbus_dev_groups,\n\t.drv_groups =\t\tvmbus_drv_groups,\n\t.bus_groups =\t\tvmbus_bus_groups,\n\t.pm =\t\t\t&vmbus_pm,\n};\n\nstruct onmessage_work_context {\n\tstruct work_struct work;\n\tstruct {\n\t\tstruct hv_message_header header;\n\t\tu8 payload[];\n\t} msg;\n};\n\nstatic void vmbus_onmessage_work(struct work_struct *work)\n{\n\tstruct onmessage_work_context *ctx;\n\n\t \n\tif (vmbus_connection.conn_state == DISCONNECTED)\n\t\treturn;\n\n\tctx = container_of(work, struct onmessage_work_context,\n\t\t\t   work);\n\tvmbus_onmessage((struct vmbus_channel_message_header *)\n\t\t\t&ctx->msg.payload);\n\tkfree(ctx);\n}\n\nvoid vmbus_on_msg_dpc(unsigned long data)\n{\n\tstruct hv_per_cpu_context *hv_cpu = (void *)data;\n\tvoid *page_addr = hv_cpu->synic_message_page;\n\tstruct hv_message msg_copy, *msg = (struct hv_message *)page_addr +\n\t\t\t\t  VMBUS_MESSAGE_SINT;\n\tstruct vmbus_channel_message_header *hdr;\n\tenum vmbus_channel_message_type msgtype;\n\tconst struct vmbus_channel_message_table_entry *entry;\n\tstruct onmessage_work_context *ctx;\n\t__u8 payload_size;\n\tu32 message_type;\n\n\t \n\tBUILD_BUG_ON(sizeof(enum vmbus_channel_message_type) != sizeof(u32));\n\n\t \n\tmemcpy(&msg_copy, msg, sizeof(struct hv_message));\n\n\tmessage_type = msg_copy.header.message_type;\n\tif (message_type == HVMSG_NONE)\n\t\t \n\t\treturn;\n\n\thdr = (struct vmbus_channel_message_header *)msg_copy.u.payload;\n\tmsgtype = hdr->msgtype;\n\n\ttrace_vmbus_on_msg_dpc(hdr);\n\n\tif (msgtype >= CHANNELMSG_COUNT) {\n\t\tWARN_ONCE(1, \"unknown msgtype=%d\\n\", msgtype);\n\t\tgoto msg_handled;\n\t}\n\n\tpayload_size = msg_copy.header.payload_size;\n\tif (payload_size > HV_MESSAGE_PAYLOAD_BYTE_COUNT) {\n\t\tWARN_ONCE(1, \"payload size is too large (%d)\\n\", payload_size);\n\t\tgoto msg_handled;\n\t}\n\n\tentry = &channel_message_table[msgtype];\n\n\tif (!entry->message_handler)\n\t\tgoto msg_handled;\n\n\tif (payload_size < entry->min_payload_len) {\n\t\tWARN_ONCE(1, \"message too short: msgtype=%d len=%d\\n\", msgtype, payload_size);\n\t\tgoto msg_handled;\n\t}\n\n\tif (entry->handler_type\t== VMHT_BLOCKING) {\n\t\tctx = kmalloc(struct_size(ctx, msg.payload, payload_size), GFP_ATOMIC);\n\t\tif (ctx == NULL)\n\t\t\treturn;\n\n\t\tINIT_WORK(&ctx->work, vmbus_onmessage_work);\n\t\tctx->msg.header = msg_copy.header;\n\t\tmemcpy(&ctx->msg.payload, msg_copy.u.payload, payload_size);\n\n\t\t \n\t\tswitch (msgtype) {\n\t\tcase CHANNELMSG_RESCIND_CHANNELOFFER:\n\t\t\t \n\t\t\tif (vmbus_connection.ignore_any_offer_msg)\n\t\t\t\tbreak;\n\t\t\tqueue_work(vmbus_connection.rescind_work_queue, &ctx->work);\n\t\t\tbreak;\n\n\t\tcase CHANNELMSG_OFFERCHANNEL:\n\t\t\t \n\t\t\tif (vmbus_connection.ignore_any_offer_msg)\n\t\t\t\tbreak;\n\t\t\tatomic_inc(&vmbus_connection.offer_in_progress);\n\t\t\tfallthrough;\n\n\t\tdefault:\n\t\t\tqueue_work(vmbus_connection.work_queue, &ctx->work);\n\t\t}\n\t} else\n\t\tentry->message_handler(hdr);\n\nmsg_handled:\n\tvmbus_signal_eom(msg, message_type);\n}\n\n#ifdef CONFIG_PM_SLEEP\n \nstatic void vmbus_force_channel_rescinded(struct vmbus_channel *channel)\n{\n\tstruct onmessage_work_context *ctx;\n\tstruct vmbus_channel_rescind_offer *rescind;\n\n\tWARN_ON(!is_hvsock_channel(channel));\n\n\t \n\tctx = kzalloc(sizeof(*ctx) + sizeof(*rescind),\n\t\t      GFP_KERNEL | __GFP_NOFAIL);\n\n\t \n\tctx->msg.header.message_type = 1;\n\tctx->msg.header.payload_size = sizeof(*rescind);\n\n\t \n\trescind = (struct vmbus_channel_rescind_offer *)ctx->msg.payload;\n\trescind->header.msgtype = CHANNELMSG_RESCIND_CHANNELOFFER;\n\trescind->child_relid = channel->offermsg.child_relid;\n\n\tINIT_WORK(&ctx->work, vmbus_onmessage_work);\n\n\tqueue_work(vmbus_connection.work_queue, &ctx->work);\n}\n#endif  \n\n \nstatic void vmbus_chan_sched(struct hv_per_cpu_context *hv_cpu)\n{\n\tunsigned long *recv_int_page;\n\tu32 maxbits, relid;\n\n\t \n\tvoid *page_addr = hv_cpu->synic_event_page;\n\tunion hv_synic_event_flags *event\n\t\t= (union hv_synic_event_flags *)page_addr +\n\t\t\t\t\t VMBUS_MESSAGE_SINT;\n\n\tmaxbits = HV_EVENT_FLAGS_COUNT;\n\trecv_int_page = event->flags;\n\n\tif (unlikely(!recv_int_page))\n\t\treturn;\n\n\tfor_each_set_bit(relid, recv_int_page, maxbits) {\n\t\tvoid (*callback_fn)(void *context);\n\t\tstruct vmbus_channel *channel;\n\n\t\tif (!sync_test_and_clear_bit(relid, recv_int_page))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (relid == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\trcu_read_lock();\n\n\t\t \n\t\tchannel = relid2channel(relid);\n\t\tif (channel == NULL)\n\t\t\tgoto sched_unlock_rcu;\n\n\t\tif (channel->rescind)\n\t\t\tgoto sched_unlock_rcu;\n\n\t\t \n\t\tspin_lock(&channel->sched_lock);\n\n\t\tcallback_fn = channel->onchannel_callback;\n\t\tif (unlikely(callback_fn == NULL))\n\t\t\tgoto sched_unlock;\n\n\t\ttrace_vmbus_chan_sched(channel);\n\n\t\t++channel->interrupts;\n\n\t\tswitch (channel->callback_mode) {\n\t\tcase HV_CALL_ISR:\n\t\t\t(*callback_fn)(channel->channel_callback_context);\n\t\t\tbreak;\n\n\t\tcase HV_CALL_BATCHED:\n\t\t\thv_begin_read(&channel->inbound);\n\t\t\tfallthrough;\n\t\tcase HV_CALL_DIRECT:\n\t\t\ttasklet_schedule(&channel->callback_event);\n\t\t}\n\nsched_unlock:\n\t\tspin_unlock(&channel->sched_lock);\nsched_unlock_rcu:\n\t\trcu_read_unlock();\n\t}\n}\n\nstatic void vmbus_isr(void)\n{\n\tstruct hv_per_cpu_context *hv_cpu\n\t\t= this_cpu_ptr(hv_context.cpu_context);\n\tvoid *page_addr;\n\tstruct hv_message *msg;\n\n\tvmbus_chan_sched(hv_cpu);\n\n\tpage_addr = hv_cpu->synic_message_page;\n\tmsg = (struct hv_message *)page_addr + VMBUS_MESSAGE_SINT;\n\n\t \n\tif (msg->header.message_type != HVMSG_NONE) {\n\t\tif (msg->header.message_type == HVMSG_TIMER_EXPIRED) {\n\t\t\thv_stimer0_isr();\n\t\t\tvmbus_signal_eom(msg, HVMSG_TIMER_EXPIRED);\n\t\t} else\n\t\t\ttasklet_schedule(&hv_cpu->msg_dpc);\n\t}\n\n\tadd_interrupt_randomness(vmbus_interrupt);\n}\n\nstatic irqreturn_t vmbus_percpu_isr(int irq, void *dev_id)\n{\n\tvmbus_isr();\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int vmbus_bus_init(void)\n{\n\tint ret;\n\n\tret = hv_init();\n\tif (ret != 0) {\n\t\tpr_err(\"Unable to initialize the hypervisor - 0x%x\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = bus_register(&hv_bus);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\n\tif (vmbus_irq == -1) {\n\t\thv_setup_vmbus_handler(vmbus_isr);\n\t} else {\n\t\tvmbus_evt = alloc_percpu(long);\n\t\tret = request_percpu_irq(vmbus_irq, vmbus_percpu_isr,\n\t\t\t\t\"Hyper-V VMbus\", vmbus_evt);\n\t\tif (ret) {\n\t\t\tpr_err(\"Can't request Hyper-V VMbus IRQ %d, Err %d\",\n\t\t\t\t\tvmbus_irq, ret);\n\t\t\tfree_percpu(vmbus_evt);\n\t\t\tgoto err_setup;\n\t\t}\n\t}\n\n\tret = hv_synic_alloc();\n\tif (ret)\n\t\tgoto err_alloc;\n\n\t \n\tret = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"hyperv/vmbus:online\",\n\t\t\t\thv_synic_init, hv_synic_cleanup);\n\tif (ret < 0)\n\t\tgoto err_alloc;\n\thyperv_cpuhp_online = ret;\n\n\tret = vmbus_connect();\n\tif (ret)\n\t\tgoto err_connect;\n\n\t \n\tatomic_notifier_chain_register(&panic_notifier_list,\n\t\t\t       &hyperv_panic_vmbus_unload_block);\n\n\tvmbus_request_offers();\n\n\treturn 0;\n\nerr_connect:\n\tcpuhp_remove_state(hyperv_cpuhp_online);\nerr_alloc:\n\thv_synic_free();\n\tif (vmbus_irq == -1) {\n\t\thv_remove_vmbus_handler();\n\t} else {\n\t\tfree_percpu_irq(vmbus_irq, vmbus_evt);\n\t\tfree_percpu(vmbus_evt);\n\t}\nerr_setup:\n\tbus_unregister(&hv_bus);\n\treturn ret;\n}\n\n \nint __vmbus_driver_register(struct hv_driver *hv_driver, struct module *owner, const char *mod_name)\n{\n\tint ret;\n\n\tpr_info(\"registering driver %s\\n\", hv_driver->name);\n\n\tret = vmbus_exists();\n\tif (ret < 0)\n\t\treturn ret;\n\n\thv_driver->driver.name = hv_driver->name;\n\thv_driver->driver.owner = owner;\n\thv_driver->driver.mod_name = mod_name;\n\thv_driver->driver.bus = &hv_bus;\n\n\tspin_lock_init(&hv_driver->dynids.lock);\n\tINIT_LIST_HEAD(&hv_driver->dynids.list);\n\n\tret = driver_register(&hv_driver->driver);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__vmbus_driver_register);\n\n \nvoid vmbus_driver_unregister(struct hv_driver *hv_driver)\n{\n\tpr_info(\"unregistering driver %s\\n\", hv_driver->name);\n\n\tif (!vmbus_exists()) {\n\t\tdriver_unregister(&hv_driver->driver);\n\t\tvmbus_free_dynids(hv_driver);\n\t}\n}\nEXPORT_SYMBOL_GPL(vmbus_driver_unregister);\n\n\n \nstatic void vmbus_chan_release(struct kobject *kobj)\n{\n\tstruct vmbus_channel *channel\n\t\t= container_of(kobj, struct vmbus_channel, kobj);\n\n\tkfree_rcu(channel, rcu);\n}\n\nstruct vmbus_chan_attribute {\n\tstruct attribute attr;\n\tssize_t (*show)(struct vmbus_channel *chan, char *buf);\n\tssize_t (*store)(struct vmbus_channel *chan,\n\t\t\t const char *buf, size_t count);\n};\n#define VMBUS_CHAN_ATTR(_name, _mode, _show, _store) \\\n\tstruct vmbus_chan_attribute chan_attr_##_name \\\n\t\t= __ATTR(_name, _mode, _show, _store)\n#define VMBUS_CHAN_ATTR_RW(_name) \\\n\tstruct vmbus_chan_attribute chan_attr_##_name = __ATTR_RW(_name)\n#define VMBUS_CHAN_ATTR_RO(_name) \\\n\tstruct vmbus_chan_attribute chan_attr_##_name = __ATTR_RO(_name)\n#define VMBUS_CHAN_ATTR_WO(_name) \\\n\tstruct vmbus_chan_attribute chan_attr_##_name = __ATTR_WO(_name)\n\nstatic ssize_t vmbus_chan_attr_show(struct kobject *kobj,\n\t\t\t\t    struct attribute *attr, char *buf)\n{\n\tconst struct vmbus_chan_attribute *attribute\n\t\t= container_of(attr, struct vmbus_chan_attribute, attr);\n\tstruct vmbus_channel *chan\n\t\t= container_of(kobj, struct vmbus_channel, kobj);\n\n\tif (!attribute->show)\n\t\treturn -EIO;\n\n\treturn attribute->show(chan, buf);\n}\n\nstatic ssize_t vmbus_chan_attr_store(struct kobject *kobj,\n\t\t\t\t     struct attribute *attr, const char *buf,\n\t\t\t\t     size_t count)\n{\n\tconst struct vmbus_chan_attribute *attribute\n\t\t= container_of(attr, struct vmbus_chan_attribute, attr);\n\tstruct vmbus_channel *chan\n\t\t= container_of(kobj, struct vmbus_channel, kobj);\n\n\tif (!attribute->store)\n\t\treturn -EIO;\n\n\treturn attribute->store(chan, buf, count);\n}\n\nstatic const struct sysfs_ops vmbus_chan_sysfs_ops = {\n\t.show = vmbus_chan_attr_show,\n\t.store = vmbus_chan_attr_store,\n};\n\nstatic ssize_t out_mask_show(struct vmbus_channel *channel, char *buf)\n{\n\tstruct hv_ring_buffer_info *rbi = &channel->outbound;\n\tssize_t ret;\n\n\tmutex_lock(&rbi->ring_buffer_mutex);\n\tif (!rbi->ring_buffer) {\n\t\tmutex_unlock(&rbi->ring_buffer_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tret = sprintf(buf, \"%u\\n\", rbi->ring_buffer->interrupt_mask);\n\tmutex_unlock(&rbi->ring_buffer_mutex);\n\treturn ret;\n}\nstatic VMBUS_CHAN_ATTR_RO(out_mask);\n\nstatic ssize_t in_mask_show(struct vmbus_channel *channel, char *buf)\n{\n\tstruct hv_ring_buffer_info *rbi = &channel->inbound;\n\tssize_t ret;\n\n\tmutex_lock(&rbi->ring_buffer_mutex);\n\tif (!rbi->ring_buffer) {\n\t\tmutex_unlock(&rbi->ring_buffer_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tret = sprintf(buf, \"%u\\n\", rbi->ring_buffer->interrupt_mask);\n\tmutex_unlock(&rbi->ring_buffer_mutex);\n\treturn ret;\n}\nstatic VMBUS_CHAN_ATTR_RO(in_mask);\n\nstatic ssize_t read_avail_show(struct vmbus_channel *channel, char *buf)\n{\n\tstruct hv_ring_buffer_info *rbi = &channel->inbound;\n\tssize_t ret;\n\n\tmutex_lock(&rbi->ring_buffer_mutex);\n\tif (!rbi->ring_buffer) {\n\t\tmutex_unlock(&rbi->ring_buffer_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tret = sprintf(buf, \"%u\\n\", hv_get_bytes_to_read(rbi));\n\tmutex_unlock(&rbi->ring_buffer_mutex);\n\treturn ret;\n}\nstatic VMBUS_CHAN_ATTR_RO(read_avail);\n\nstatic ssize_t write_avail_show(struct vmbus_channel *channel, char *buf)\n{\n\tstruct hv_ring_buffer_info *rbi = &channel->outbound;\n\tssize_t ret;\n\n\tmutex_lock(&rbi->ring_buffer_mutex);\n\tif (!rbi->ring_buffer) {\n\t\tmutex_unlock(&rbi->ring_buffer_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tret = sprintf(buf, \"%u\\n\", hv_get_bytes_to_write(rbi));\n\tmutex_unlock(&rbi->ring_buffer_mutex);\n\treturn ret;\n}\nstatic VMBUS_CHAN_ATTR_RO(write_avail);\n\nstatic ssize_t target_cpu_show(struct vmbus_channel *channel, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", channel->target_cpu);\n}\nstatic ssize_t target_cpu_store(struct vmbus_channel *channel,\n\t\t\t\tconst char *buf, size_t count)\n{\n\tu32 target_cpu, origin_cpu;\n\tssize_t ret = count;\n\n\tif (vmbus_proto_version < VERSION_WIN10_V4_1)\n\t\treturn -EIO;\n\n\tif (sscanf(buf, \"%uu\", &target_cpu) != 1)\n\t\treturn -EIO;\n\n\t \n\tif (target_cpu >= nr_cpumask_bits)\n\t\treturn -EINVAL;\n\n\tif (!cpumask_test_cpu(target_cpu, housekeeping_cpumask(HK_TYPE_MANAGED_IRQ)))\n\t\treturn -EINVAL;\n\n\t \n\tcpus_read_lock();\n\n\tif (!cpu_online(target_cpu)) {\n\t\tcpus_read_unlock();\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmutex_lock(&vmbus_connection.channel_mutex);\n\n\t \n\tif (channel->state != CHANNEL_OPENED_STATE) {\n\t\tret = -EIO;\n\t\tgoto cpu_store_unlock;\n\t}\n\n\torigin_cpu = channel->target_cpu;\n\tif (target_cpu == origin_cpu)\n\t\tgoto cpu_store_unlock;\n\n\tif (vmbus_send_modifychannel(channel,\n\t\t\t\t     hv_cpu_number_to_vp_number(target_cpu))) {\n\t\tret = -EIO;\n\t\tgoto cpu_store_unlock;\n\t}\n\n\t \n\n\tchannel->target_cpu = target_cpu;\n\n\t \n\tif (hv_is_perf_channel(channel))\n\t\thv_update_allocated_cpus(origin_cpu, target_cpu);\n\n\t \n\tif (channel->change_target_cpu_callback) {\n\t\t(*channel->change_target_cpu_callback)(channel,\n\t\t\t\torigin_cpu, target_cpu);\n\t}\n\ncpu_store_unlock:\n\tmutex_unlock(&vmbus_connection.channel_mutex);\n\tcpus_read_unlock();\n\treturn ret;\n}\nstatic VMBUS_CHAN_ATTR(cpu, 0644, target_cpu_show, target_cpu_store);\n\nstatic ssize_t channel_pending_show(struct vmbus_channel *channel,\n\t\t\t\t    char *buf)\n{\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_pending(channel,\n\t\t\t\t       vmbus_connection.monitor_pages[1]));\n}\nstatic VMBUS_CHAN_ATTR(pending, 0444, channel_pending_show, NULL);\n\nstatic ssize_t channel_latency_show(struct vmbus_channel *channel,\n\t\t\t\t    char *buf)\n{\n\treturn sprintf(buf, \"%d\\n\",\n\t\t       channel_latency(channel,\n\t\t\t\t       vmbus_connection.monitor_pages[1]));\n}\nstatic VMBUS_CHAN_ATTR(latency, 0444, channel_latency_show, NULL);\n\nstatic ssize_t channel_interrupts_show(struct vmbus_channel *channel, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", channel->interrupts);\n}\nstatic VMBUS_CHAN_ATTR(interrupts, 0444, channel_interrupts_show, NULL);\n\nstatic ssize_t channel_events_show(struct vmbus_channel *channel, char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\", channel->sig_events);\n}\nstatic VMBUS_CHAN_ATTR(events, 0444, channel_events_show, NULL);\n\nstatic ssize_t channel_intr_in_full_show(struct vmbus_channel *channel,\n\t\t\t\t\t char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t       (unsigned long long)channel->intr_in_full);\n}\nstatic VMBUS_CHAN_ATTR(intr_in_full, 0444, channel_intr_in_full_show, NULL);\n\nstatic ssize_t channel_intr_out_empty_show(struct vmbus_channel *channel,\n\t\t\t\t\t   char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t       (unsigned long long)channel->intr_out_empty);\n}\nstatic VMBUS_CHAN_ATTR(intr_out_empty, 0444, channel_intr_out_empty_show, NULL);\n\nstatic ssize_t channel_out_full_first_show(struct vmbus_channel *channel,\n\t\t\t\t\t   char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t       (unsigned long long)channel->out_full_first);\n}\nstatic VMBUS_CHAN_ATTR(out_full_first, 0444, channel_out_full_first_show, NULL);\n\nstatic ssize_t channel_out_full_total_show(struct vmbus_channel *channel,\n\t\t\t\t\t   char *buf)\n{\n\treturn sprintf(buf, \"%llu\\n\",\n\t\t       (unsigned long long)channel->out_full_total);\n}\nstatic VMBUS_CHAN_ATTR(out_full_total, 0444, channel_out_full_total_show, NULL);\n\nstatic ssize_t subchannel_monitor_id_show(struct vmbus_channel *channel,\n\t\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", channel->offermsg.monitorid);\n}\nstatic VMBUS_CHAN_ATTR(monitor_id, 0444, subchannel_monitor_id_show, NULL);\n\nstatic ssize_t subchannel_id_show(struct vmbus_channel *channel,\n\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\",\n\t\t       channel->offermsg.offer.sub_channel_index);\n}\nstatic VMBUS_CHAN_ATTR_RO(subchannel_id);\n\nstatic struct attribute *vmbus_chan_attrs[] = {\n\t&chan_attr_out_mask.attr,\n\t&chan_attr_in_mask.attr,\n\t&chan_attr_read_avail.attr,\n\t&chan_attr_write_avail.attr,\n\t&chan_attr_cpu.attr,\n\t&chan_attr_pending.attr,\n\t&chan_attr_latency.attr,\n\t&chan_attr_interrupts.attr,\n\t&chan_attr_events.attr,\n\t&chan_attr_intr_in_full.attr,\n\t&chan_attr_intr_out_empty.attr,\n\t&chan_attr_out_full_first.attr,\n\t&chan_attr_out_full_total.attr,\n\t&chan_attr_monitor_id.attr,\n\t&chan_attr_subchannel_id.attr,\n\tNULL\n};\n\n \nstatic umode_t vmbus_chan_attr_is_visible(struct kobject *kobj,\n\t\t\t\t\t  struct attribute *attr, int idx)\n{\n\tconst struct vmbus_channel *channel =\n\t\tcontainer_of(kobj, struct vmbus_channel, kobj);\n\n\t \n\tif (!channel->offermsg.monitor_allocated &&\n\t    (attr == &chan_attr_pending.attr ||\n\t     attr == &chan_attr_latency.attr ||\n\t     attr == &chan_attr_monitor_id.attr))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic struct attribute_group vmbus_chan_group = {\n\t.attrs = vmbus_chan_attrs,\n\t.is_visible = vmbus_chan_attr_is_visible\n};\n\nstatic struct kobj_type vmbus_chan_ktype = {\n\t.sysfs_ops = &vmbus_chan_sysfs_ops,\n\t.release = vmbus_chan_release,\n};\n\n \nint vmbus_add_channel_kobj(struct hv_device *dev, struct vmbus_channel *channel)\n{\n\tconst struct device *device = &dev->device;\n\tstruct kobject *kobj = &channel->kobj;\n\tu32 relid = channel->offermsg.child_relid;\n\tint ret;\n\n\tkobj->kset = dev->channels_kset;\n\tret = kobject_init_and_add(kobj, &vmbus_chan_ktype, NULL,\n\t\t\t\t   \"%u\", relid);\n\tif (ret) {\n\t\tkobject_put(kobj);\n\t\treturn ret;\n\t}\n\n\tret = sysfs_create_group(kobj, &vmbus_chan_group);\n\n\tif (ret) {\n\t\t \n\t\tkobject_put(kobj);\n\t\tdev_err(device, \"Unable to set up channel sysfs files\\n\");\n\t\treturn ret;\n\t}\n\n\tkobject_uevent(kobj, KOBJ_ADD);\n\n\treturn 0;\n}\n\n \nvoid vmbus_remove_channel_attr_group(struct vmbus_channel *channel)\n{\n\tsysfs_remove_group(&channel->kobj, &vmbus_chan_group);\n}\n\n \nstruct hv_device *vmbus_device_create(const guid_t *type,\n\t\t\t\t      const guid_t *instance,\n\t\t\t\t      struct vmbus_channel *channel)\n{\n\tstruct hv_device *child_device_obj;\n\n\tchild_device_obj = kzalloc(sizeof(struct hv_device), GFP_KERNEL);\n\tif (!child_device_obj) {\n\t\tpr_err(\"Unable to allocate device object for child device\\n\");\n\t\treturn NULL;\n\t}\n\n\tchild_device_obj->channel = channel;\n\tguid_copy(&child_device_obj->dev_type, type);\n\tguid_copy(&child_device_obj->dev_instance, instance);\n\tchild_device_obj->vendor_id = PCI_VENDOR_ID_MICROSOFT;\n\n\treturn child_device_obj;\n}\n\n \nint vmbus_device_register(struct hv_device *child_device_obj)\n{\n\tstruct kobject *kobj = &child_device_obj->device.kobj;\n\tint ret;\n\n\tdev_set_name(&child_device_obj->device, \"%pUl\",\n\t\t     &child_device_obj->channel->offermsg.offer.if_instance);\n\n\tchild_device_obj->device.bus = &hv_bus;\n\tchild_device_obj->device.parent = hv_dev;\n\tchild_device_obj->device.release = vmbus_device_release;\n\n\tchild_device_obj->device.dma_parms = &child_device_obj->dma_parms;\n\tchild_device_obj->device.dma_mask = &child_device_obj->dma_mask;\n\tdma_set_mask(&child_device_obj->device, DMA_BIT_MASK(64));\n\n\t \n\tret = device_register(&child_device_obj->device);\n\tif (ret) {\n\t\tpr_err(\"Unable to register child device\\n\");\n\t\tput_device(&child_device_obj->device);\n\t\treturn ret;\n\t}\n\n\tchild_device_obj->channels_kset = kset_create_and_add(\"channels\",\n\t\t\t\t\t\t\t      NULL, kobj);\n\tif (!child_device_obj->channels_kset) {\n\t\tret = -ENOMEM;\n\t\tgoto err_dev_unregister;\n\t}\n\n\tret = vmbus_add_channel_kobj(child_device_obj,\n\t\t\t\t     child_device_obj->channel);\n\tif (ret) {\n\t\tpr_err(\"Unable to register primary channeln\");\n\t\tgoto err_kset_unregister;\n\t}\n\thv_debug_add_dev_dir(child_device_obj);\n\n\treturn 0;\n\nerr_kset_unregister:\n\tkset_unregister(child_device_obj->channels_kset);\n\nerr_dev_unregister:\n\tdevice_unregister(&child_device_obj->device);\n\treturn ret;\n}\n\n \nvoid vmbus_device_unregister(struct hv_device *device_obj)\n{\n\tpr_debug(\"child device %s unregistered\\n\",\n\t\tdev_name(&device_obj->device));\n\n\tkset_unregister(device_obj->channels_kset);\n\n\t \n\tdevice_unregister(&device_obj->device);\n}\n\n#ifdef CONFIG_ACPI\n \nstatic acpi_status vmbus_walk_resources(struct acpi_resource *res, void *ctx)\n{\n\tresource_size_t start = 0;\n\tresource_size_t end = 0;\n\tstruct resource *new_res;\n\tstruct resource **old_res = &hyperv_mmio;\n\tstruct resource **prev_res = NULL;\n\tstruct resource r;\n\n\tswitch (res->type) {\n\n\t \n\tcase ACPI_RESOURCE_TYPE_ADDRESS32:\n\t\tstart = res->data.address32.address.minimum;\n\t\tend = res->data.address32.address.maximum;\n\t\tbreak;\n\n\tcase ACPI_RESOURCE_TYPE_ADDRESS64:\n\t\tstart = res->data.address64.address.minimum;\n\t\tend = res->data.address64.address.maximum;\n\t\tbreak;\n\n\t \n\tcase ACPI_RESOURCE_TYPE_EXTENDED_IRQ:\n\t\tif (!acpi_dev_resource_interrupt(res, 0, &r)) {\n\t\t\tpr_err(\"Unable to parse Hyper-V ACPI interrupt\\n\");\n\t\t\treturn AE_ERROR;\n\t\t}\n\t\t \n\t\tvmbus_interrupt = res->data.extended_irq.interrupts[0];\n\t\t \n\t\tvmbus_irq = r.start;\n\t\treturn AE_OK;\n\n\tdefault:\n\t\t \n\t\treturn AE_OK;\n\n\t}\n\t \n\tif (end < 0x100000)\n\t\treturn AE_OK;\n\n\tnew_res = kzalloc(sizeof(*new_res), GFP_ATOMIC);\n\tif (!new_res)\n\t\treturn AE_NO_MEMORY;\n\n\t \n\tif (end > VTPM_BASE_ADDRESS && start < VTPM_BASE_ADDRESS)\n\t\tend = VTPM_BASE_ADDRESS;\n\n\tnew_res->name = \"hyperv mmio\";\n\tnew_res->flags = IORESOURCE_MEM;\n\tnew_res->start = start;\n\tnew_res->end = end;\n\n\t \n\tdo {\n\t\tif (!*old_res) {\n\t\t\t*old_res = new_res;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (((*old_res)->end + 1) == new_res->start) {\n\t\t\t(*old_res)->end = new_res->end;\n\t\t\tkfree(new_res);\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((*old_res)->start == new_res->end + 1) {\n\t\t\t(*old_res)->start = new_res->start;\n\t\t\tkfree(new_res);\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((*old_res)->start > new_res->end) {\n\t\t\tnew_res->sibling = *old_res;\n\t\t\tif (prev_res)\n\t\t\t\t(*prev_res)->sibling = new_res;\n\t\t\t*old_res = new_res;\n\t\t\tbreak;\n\t\t}\n\n\t\tprev_res = old_res;\n\t\told_res = &(*old_res)->sibling;\n\n\t} while (1);\n\n\treturn AE_OK;\n}\n#endif\n\nstatic void vmbus_mmio_remove(void)\n{\n\tstruct resource *cur_res;\n\tstruct resource *next_res;\n\n\tif (hyperv_mmio) {\n\t\tif (fb_mmio) {\n\t\t\t__release_region(hyperv_mmio, fb_mmio->start,\n\t\t\t\t\t resource_size(fb_mmio));\n\t\t\tfb_mmio = NULL;\n\t\t}\n\n\t\tfor (cur_res = hyperv_mmio; cur_res; cur_res = next_res) {\n\t\t\tnext_res = cur_res->sibling;\n\t\t\tkfree(cur_res);\n\t\t}\n\t}\n}\n\nstatic void __maybe_unused vmbus_reserve_fb(void)\n{\n\tresource_size_t start = 0, size;\n\tstruct pci_dev *pdev;\n\n\tif (efi_enabled(EFI_BOOT)) {\n\t\t \n\t\tstart = screen_info.lfb_base;\n\t\tsize = max_t(__u32, screen_info.lfb_size, 0x800000);\n\t} else {\n\t\t \n\t\tpdev = pci_get_device(PCI_VENDOR_ID_MICROSOFT,\n\t\t\t\t      PCI_DEVICE_ID_HYPERV_VIDEO, NULL);\n\t\tif (!pdev)\n\t\t\treturn;\n\n\t\tif (pdev->resource[0].flags & IORESOURCE_MEM) {\n\t\t\tstart = pci_resource_start(pdev, 0);\n\t\t\tsize = pci_resource_len(pdev, 0);\n\t\t}\n\n\t\t \n\t\tpci_dev_put(pdev);\n\t}\n\n\tif (!start)\n\t\treturn;\n\n\t \n\tfor (; !fb_mmio && (size >= 0x100000); size >>= 1)\n\t\tfb_mmio = __request_region(hyperv_mmio, start, size, fb_mmio_name, 0);\n}\n\n \nint vmbus_allocate_mmio(struct resource **new, struct hv_device *device_obj,\n\t\t\tresource_size_t min, resource_size_t max,\n\t\t\tresource_size_t size, resource_size_t align,\n\t\t\tbool fb_overlap_ok)\n{\n\tstruct resource *iter, *shadow;\n\tresource_size_t range_min, range_max, start, end;\n\tconst char *dev_n = dev_name(&device_obj->device);\n\tint retval;\n\n\tretval = -ENXIO;\n\tmutex_lock(&hyperv_mmio_lock);\n\n\t \n\tif (fb_overlap_ok && fb_mmio && !(min > fb_mmio->end) &&\n\t    !(max < fb_mmio->start)) {\n\n\t\trange_min = fb_mmio->start;\n\t\trange_max = fb_mmio->end;\n\t\tstart = (range_min + align - 1) & ~(align - 1);\n\t\tfor (; start + size - 1 <= range_max; start += align) {\n\t\t\t*new = request_mem_region_exclusive(start, size, dev_n);\n\t\t\tif (*new) {\n\t\t\t\tretval = 0;\n\t\t\t\tgoto exit;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (iter = hyperv_mmio; iter; iter = iter->sibling) {\n\t\tif ((iter->start >= max) || (iter->end <= min))\n\t\t\tcontinue;\n\n\t\trange_min = iter->start;\n\t\trange_max = iter->end;\n\t\tstart = (range_min + align - 1) & ~(align - 1);\n\t\tfor (; start + size - 1 <= range_max; start += align) {\n\t\t\tend = start + size - 1;\n\n\t\t\t \n\t\t\tif (!fb_overlap_ok && fb_mmio &&\n\t\t\t    (((start >= fb_mmio->start) && (start <= fb_mmio->end)) ||\n\t\t\t     ((end >= fb_mmio->start) && (end <= fb_mmio->end))))\n\t\t\t\tcontinue;\n\n\t\t\tshadow = __request_region(iter, start, size, NULL,\n\t\t\t\t\t\t  IORESOURCE_BUSY);\n\t\t\tif (!shadow)\n\t\t\t\tcontinue;\n\n\t\t\t*new = request_mem_region_exclusive(start, size, dev_n);\n\t\t\tif (*new) {\n\t\t\t\tshadow->name = (char *)*new;\n\t\t\t\tretval = 0;\n\t\t\t\tgoto exit;\n\t\t\t}\n\n\t\t\t__release_region(iter, start, size);\n\t\t}\n\t}\n\nexit:\n\tmutex_unlock(&hyperv_mmio_lock);\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(vmbus_allocate_mmio);\n\n \nvoid vmbus_free_mmio(resource_size_t start, resource_size_t size)\n{\n\tstruct resource *iter;\n\n\tmutex_lock(&hyperv_mmio_lock);\n\tfor (iter = hyperv_mmio; iter; iter = iter->sibling) {\n\t\tif ((iter->start >= start + size) || (iter->end <= start))\n\t\t\tcontinue;\n\n\t\t__release_region(iter, start, size);\n\t}\n\trelease_mem_region(start, size);\n\tmutex_unlock(&hyperv_mmio_lock);\n\n}\nEXPORT_SYMBOL_GPL(vmbus_free_mmio);\n\n#ifdef CONFIG_ACPI\nstatic int vmbus_acpi_add(struct platform_device *pdev)\n{\n\tacpi_status result;\n\tint ret_val = -ENODEV;\n\tstruct acpi_device *ancestor;\n\tstruct acpi_device *device = ACPI_COMPANION(&pdev->dev);\n\n\thv_dev = &device->dev;\n\n\t \n\tACPI_COMPANION_SET(&device->dev, device);\n\tif (IS_ENABLED(CONFIG_ACPI_CCA_REQUIRED) &&\n\t    device_get_dma_attr(&device->dev) == DEV_DMA_NOT_SUPPORTED) {\n\t\tpr_info(\"No ACPI _CCA found; assuming coherent device I/O\\n\");\n\t\tdevice->flags.cca_seen = true;\n\t\tdevice->flags.coherent_dma = true;\n\t}\n\n\tresult = acpi_walk_resources(device->handle, METHOD_NAME__CRS,\n\t\t\t\t\tvmbus_walk_resources, NULL);\n\n\tif (ACPI_FAILURE(result))\n\t\tgoto acpi_walk_err;\n\t \n\tfor (ancestor = acpi_dev_parent(device);\n\t     ancestor && ancestor->handle != ACPI_ROOT_OBJECT;\n\t     ancestor = acpi_dev_parent(ancestor)) {\n\t\tresult = acpi_walk_resources(ancestor->handle, METHOD_NAME__CRS,\n\t\t\t\t\t     vmbus_walk_resources, NULL);\n\n\t\tif (ACPI_FAILURE(result))\n\t\t\tcontinue;\n\t\tif (hyperv_mmio) {\n\t\t\tvmbus_reserve_fb();\n\t\t\tbreak;\n\t\t}\n\t}\n\tret_val = 0;\n\nacpi_walk_err:\n\tif (ret_val)\n\t\tvmbus_mmio_remove();\n\treturn ret_val;\n}\n#else\nstatic int vmbus_acpi_add(struct platform_device *pdev)\n{\n\treturn 0;\n}\n#endif\n\nstatic int vmbus_device_add(struct platform_device *pdev)\n{\n\tstruct resource **cur_res = &hyperv_mmio;\n\tstruct of_range range;\n\tstruct of_range_parser parser;\n\tstruct device_node *np = pdev->dev.of_node;\n\tint ret;\n\n\thv_dev = &pdev->dev;\n\n\tret = of_range_parser_init(&parser, np);\n\tif (ret)\n\t\treturn ret;\n\n\tfor_each_of_range(&parser, &range) {\n\t\tstruct resource *res;\n\n\t\tres = kzalloc(sizeof(*res), GFP_KERNEL);\n\t\tif (!res) {\n\t\t\tvmbus_mmio_remove();\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tres->name = \"hyperv mmio\";\n\t\tres->flags = range.flags;\n\t\tres->start = range.cpu_addr;\n\t\tres->end = range.cpu_addr + range.size;\n\n\t\t*cur_res = res;\n\t\tcur_res = &res->sibling;\n\t}\n\n\treturn ret;\n}\n\nstatic int vmbus_platform_driver_probe(struct platform_device *pdev)\n{\n\tif (acpi_disabled)\n\t\treturn vmbus_device_add(pdev);\n\telse\n\t\treturn vmbus_acpi_add(pdev);\n}\n\nstatic int vmbus_platform_driver_remove(struct platform_device *pdev)\n{\n\tvmbus_mmio_remove();\n\treturn 0;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int vmbus_bus_suspend(struct device *dev)\n{\n\tstruct hv_per_cpu_context *hv_cpu = per_cpu_ptr(\n\t\t\thv_context.cpu_context, VMBUS_CONNECT_CPU);\n\tstruct vmbus_channel *channel, *sc;\n\n\ttasklet_disable(&hv_cpu->msg_dpc);\n\tvmbus_connection.ignore_any_offer_msg = true;\n\t \n\ttasklet_enable(&hv_cpu->msg_dpc);\n\n\t \n\tdrain_workqueue(vmbus_connection.rescind_work_queue);\n\tdrain_workqueue(vmbus_connection.work_queue);\n\tdrain_workqueue(vmbus_connection.handle_primary_chan_wq);\n\tdrain_workqueue(vmbus_connection.handle_sub_chan_wq);\n\n\tmutex_lock(&vmbus_connection.channel_mutex);\n\tlist_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {\n\t\tif (!is_hvsock_channel(channel))\n\t\t\tcontinue;\n\n\t\tvmbus_force_channel_rescinded(channel);\n\t}\n\tmutex_unlock(&vmbus_connection.channel_mutex);\n\n\t \n\tif (atomic_read(&vmbus_connection.nr_chan_close_on_suspend) > 0)\n\t\twait_for_completion(&vmbus_connection.ready_for_suspend_event);\n\n\tif (atomic_read(&vmbus_connection.nr_chan_fixup_on_resume) != 0) {\n\t\tpr_err(\"Can not suspend due to a previous failed resuming\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tmutex_lock(&vmbus_connection.channel_mutex);\n\n\tlist_for_each_entry(channel, &vmbus_connection.chn_list, listentry) {\n\t\t \n\t\tvmbus_channel_unmap_relid(channel);\n\t\tchannel->offermsg.child_relid = INVALID_RELID;\n\n\t\tif (is_hvsock_channel(channel)) {\n\t\t\tif (!channel->rescind) {\n\t\t\t\tpr_err(\"hv_sock channel not rescinded!\\n\");\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_for_each_entry(sc, &channel->sc_list, sc_list) {\n\t\t\tpr_err(\"Sub-channel not deleted!\\n\");\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\n\t\tatomic_inc(&vmbus_connection.nr_chan_fixup_on_resume);\n\t}\n\n\tmutex_unlock(&vmbus_connection.channel_mutex);\n\n\tvmbus_initiate_unload(false);\n\n\t \n\treinit_completion(&vmbus_connection.ready_for_resume_event);\n\n\treturn 0;\n}\n\nstatic int vmbus_bus_resume(struct device *dev)\n{\n\tstruct vmbus_channel_msginfo *msginfo;\n\tsize_t msgsize;\n\tint ret;\n\n\tvmbus_connection.ignore_any_offer_msg = false;\n\n\t \n\tif (!vmbus_proto_version) {\n\t\tpr_err(\"Invalid proto version = 0x%x\\n\", vmbus_proto_version);\n\t\treturn -EINVAL;\n\t}\n\n\tmsgsize = sizeof(*msginfo) +\n\t\t  sizeof(struct vmbus_channel_initiate_contact);\n\n\tmsginfo = kzalloc(msgsize, GFP_KERNEL);\n\n\tif (msginfo == NULL)\n\t\treturn -ENOMEM;\n\n\tret = vmbus_negotiate_version(msginfo, vmbus_proto_version);\n\n\tkfree(msginfo);\n\n\tif (ret != 0)\n\t\treturn ret;\n\n\tWARN_ON(atomic_read(&vmbus_connection.nr_chan_fixup_on_resume) == 0);\n\n\tvmbus_request_offers();\n\n\tif (wait_for_completion_timeout(\n\t\t&vmbus_connection.ready_for_resume_event, 10 * HZ) == 0)\n\t\tpr_err(\"Some vmbus device is missing after suspending?\\n\");\n\n\t \n\treinit_completion(&vmbus_connection.ready_for_suspend_event);\n\n\treturn 0;\n}\n#else\n#define vmbus_bus_suspend NULL\n#define vmbus_bus_resume NULL\n#endif  \n\nstatic const __maybe_unused struct of_device_id vmbus_of_match[] = {\n\t{\n\t\t.compatible = \"microsoft,vmbus\",\n\t},\n\t{\n\t\t \n\t},\n};\nMODULE_DEVICE_TABLE(of, vmbus_of_match);\n\nstatic const __maybe_unused struct acpi_device_id vmbus_acpi_device_ids[] = {\n\t{\"VMBUS\", 0},\n\t{\"VMBus\", 0},\n\t{\"\", 0},\n};\nMODULE_DEVICE_TABLE(acpi, vmbus_acpi_device_ids);\n\n \n\nstatic const struct dev_pm_ops vmbus_bus_pm = {\n\t.suspend_noirq\t= NULL,\n\t.resume_noirq\t= NULL,\n\t.freeze_noirq\t= vmbus_bus_suspend,\n\t.thaw_noirq\t= vmbus_bus_resume,\n\t.poweroff_noirq\t= vmbus_bus_suspend,\n\t.restore_noirq\t= vmbus_bus_resume\n};\n\nstatic struct platform_driver vmbus_platform_driver = {\n\t.probe = vmbus_platform_driver_probe,\n\t.remove = vmbus_platform_driver_remove,\n\t.driver = {\n\t\t.name = \"vmbus\",\n\t\t.acpi_match_table = ACPI_PTR(vmbus_acpi_device_ids),\n\t\t.of_match_table = of_match_ptr(vmbus_of_match),\n\t\t.pm = &vmbus_bus_pm,\n\t\t.probe_type = PROBE_FORCE_SYNCHRONOUS,\n\t}\n};\n\nstatic void hv_kexec_handler(void)\n{\n\thv_stimer_global_cleanup();\n\tvmbus_initiate_unload(false);\n\t \n\tmb();\n\tcpuhp_remove_state(hyperv_cpuhp_online);\n};\n\nstatic void hv_crash_handler(struct pt_regs *regs)\n{\n\tint cpu;\n\n\tvmbus_initiate_unload(true);\n\t \n\tcpu = smp_processor_id();\n\thv_stimer_cleanup(cpu);\n\thv_synic_disable_regs(cpu);\n};\n\nstatic int hv_synic_suspend(void)\n{\n\t \n\n\thv_synic_disable_regs(0);\n\n\treturn 0;\n}\n\nstatic void hv_synic_resume(void)\n{\n\thv_synic_enable_regs(0);\n\n\t \n}\n\n \nstatic struct syscore_ops hv_synic_syscore_ops = {\n\t.suspend = hv_synic_suspend,\n\t.resume = hv_synic_resume,\n};\n\nstatic int __init hv_acpi_init(void)\n{\n\tint ret;\n\n\tif (!hv_is_hyperv_initialized())\n\t\treturn -ENODEV;\n\n\tif (hv_root_partition && !hv_nested)\n\t\treturn 0;\n\n\t \n\tret = platform_driver_register(&vmbus_platform_driver);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!hv_dev) {\n\t\tret = -ENODEV;\n\t\tgoto cleanup;\n\t}\n\n\t \n#ifdef HYPERVISOR_CALLBACK_VECTOR\n\tvmbus_interrupt = HYPERVISOR_CALLBACK_VECTOR;\n\tvmbus_irq = -1;\n#endif\n\n\thv_debug_init();\n\n\tret = vmbus_bus_init();\n\tif (ret)\n\t\tgoto cleanup;\n\n\thv_setup_kexec_handler(hv_kexec_handler);\n\thv_setup_crash_handler(hv_crash_handler);\n\n\tregister_syscore_ops(&hv_synic_syscore_ops);\n\n\treturn 0;\n\ncleanup:\n\tplatform_driver_unregister(&vmbus_platform_driver);\n\thv_dev = NULL;\n\treturn ret;\n}\n\nstatic void __exit vmbus_exit(void)\n{\n\tint cpu;\n\n\tunregister_syscore_ops(&hv_synic_syscore_ops);\n\n\thv_remove_kexec_handler();\n\thv_remove_crash_handler();\n\tvmbus_connection.conn_state = DISCONNECTED;\n\thv_stimer_global_cleanup();\n\tvmbus_disconnect();\n\tif (vmbus_irq == -1) {\n\t\thv_remove_vmbus_handler();\n\t} else {\n\t\tfree_percpu_irq(vmbus_irq, vmbus_evt);\n\t\tfree_percpu(vmbus_evt);\n\t}\n\tfor_each_online_cpu(cpu) {\n\t\tstruct hv_per_cpu_context *hv_cpu\n\t\t\t= per_cpu_ptr(hv_context.cpu_context, cpu);\n\n\t\ttasklet_kill(&hv_cpu->msg_dpc);\n\t}\n\thv_debug_rm_all_dir();\n\n\tvmbus_free_channels();\n\tkfree(vmbus_connection.channels);\n\n\t \n\tatomic_notifier_chain_unregister(&panic_notifier_list,\n\t\t\t\t\t&hyperv_panic_vmbus_unload_block);\n\n\tbus_unregister(&hv_bus);\n\n\tcpuhp_remove_state(hyperv_cpuhp_online);\n\thv_synic_free();\n\tplatform_driver_unregister(&vmbus_platform_driver);\n}\n\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Microsoft Hyper-V VMBus Driver\");\n\nsubsys_initcall(hv_acpi_init);\nmodule_exit(vmbus_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}