{
  "module_name": "clk.c",
  "hash_id": "fd7e09114f71172831ea1f352e3d601e3af61a3f3d594cb694b7974f7c958ea9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/clk/clk.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/clk-provider.h>\n#include <linux/clk/clk-conf.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/spinlock.h>\n#include <linux/err.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/of.h>\n#include <linux/device.h>\n#include <linux/init.h>\n#include <linux/pm_runtime.h>\n#include <linux/sched.h>\n#include <linux/clkdev.h>\n\n#include \"clk.h\"\n\nstatic DEFINE_SPINLOCK(enable_lock);\nstatic DEFINE_MUTEX(prepare_lock);\n\nstatic struct task_struct *prepare_owner;\nstatic struct task_struct *enable_owner;\n\nstatic int prepare_refcnt;\nstatic int enable_refcnt;\n\nstatic HLIST_HEAD(clk_root_list);\nstatic HLIST_HEAD(clk_orphan_list);\nstatic LIST_HEAD(clk_notifier_list);\n\nstatic const struct hlist_head *all_lists[] = {\n\t&clk_root_list,\n\t&clk_orphan_list,\n\tNULL,\n};\n\n \n\nstruct clk_parent_map {\n\tconst struct clk_hw\t*hw;\n\tstruct clk_core\t\t*core;\n\tconst char\t\t*fw_name;\n\tconst char\t\t*name;\n\tint\t\t\tindex;\n};\n\nstruct clk_core {\n\tconst char\t\t*name;\n\tconst struct clk_ops\t*ops;\n\tstruct clk_hw\t\t*hw;\n\tstruct module\t\t*owner;\n\tstruct device\t\t*dev;\n\tstruct device_node\t*of_node;\n\tstruct clk_core\t\t*parent;\n\tstruct clk_parent_map\t*parents;\n\tu8\t\t\tnum_parents;\n\tu8\t\t\tnew_parent_index;\n\tunsigned long\t\trate;\n\tunsigned long\t\treq_rate;\n\tunsigned long\t\tnew_rate;\n\tstruct clk_core\t\t*new_parent;\n\tstruct clk_core\t\t*new_child;\n\tunsigned long\t\tflags;\n\tbool\t\t\torphan;\n\tbool\t\t\trpm_enabled;\n\tunsigned int\t\tenable_count;\n\tunsigned int\t\tprepare_count;\n\tunsigned int\t\tprotect_count;\n\tunsigned long\t\tmin_rate;\n\tunsigned long\t\tmax_rate;\n\tunsigned long\t\taccuracy;\n\tint\t\t\tphase;\n\tstruct clk_duty\t\tduty;\n\tstruct hlist_head\tchildren;\n\tstruct hlist_node\tchild_node;\n\tstruct hlist_head\tclks;\n\tunsigned int\t\tnotifier_count;\n#ifdef CONFIG_DEBUG_FS\n\tstruct dentry\t\t*dentry;\n\tstruct hlist_node\tdebug_node;\n#endif\n\tstruct kref\t\tref;\n};\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/clk.h>\n\nstruct clk {\n\tstruct clk_core\t*core;\n\tstruct device *dev;\n\tconst char *dev_id;\n\tconst char *con_id;\n\tunsigned long min_rate;\n\tunsigned long max_rate;\n\tunsigned int exclusive_count;\n\tstruct hlist_node clks_node;\n};\n\n \nstatic int clk_pm_runtime_get(struct clk_core *core)\n{\n\tif (!core->rpm_enabled)\n\t\treturn 0;\n\n\treturn pm_runtime_resume_and_get(core->dev);\n}\n\nstatic void clk_pm_runtime_put(struct clk_core *core)\n{\n\tif (!core->rpm_enabled)\n\t\treturn;\n\n\tpm_runtime_put_sync(core->dev);\n}\n\n \nstatic void clk_prepare_lock(void)\n{\n\tif (!mutex_trylock(&prepare_lock)) {\n\t\tif (prepare_owner == current) {\n\t\t\tprepare_refcnt++;\n\t\t\treturn;\n\t\t}\n\t\tmutex_lock(&prepare_lock);\n\t}\n\tWARN_ON_ONCE(prepare_owner != NULL);\n\tWARN_ON_ONCE(prepare_refcnt != 0);\n\tprepare_owner = current;\n\tprepare_refcnt = 1;\n}\n\nstatic void clk_prepare_unlock(void)\n{\n\tWARN_ON_ONCE(prepare_owner != current);\n\tWARN_ON_ONCE(prepare_refcnt == 0);\n\n\tif (--prepare_refcnt)\n\t\treturn;\n\tprepare_owner = NULL;\n\tmutex_unlock(&prepare_lock);\n}\n\nstatic unsigned long clk_enable_lock(void)\n\t__acquires(enable_lock)\n{\n\tunsigned long flags;\n\n\t \n\tif (!IS_ENABLED(CONFIG_SMP) ||\n\t    !spin_trylock_irqsave(&enable_lock, flags)) {\n\t\tif (enable_owner == current) {\n\t\t\tenable_refcnt++;\n\t\t\t__acquire(enable_lock);\n\t\t\tif (!IS_ENABLED(CONFIG_SMP))\n\t\t\t\tlocal_save_flags(flags);\n\t\t\treturn flags;\n\t\t}\n\t\tspin_lock_irqsave(&enable_lock, flags);\n\t}\n\tWARN_ON_ONCE(enable_owner != NULL);\n\tWARN_ON_ONCE(enable_refcnt != 0);\n\tenable_owner = current;\n\tenable_refcnt = 1;\n\treturn flags;\n}\n\nstatic void clk_enable_unlock(unsigned long flags)\n\t__releases(enable_lock)\n{\n\tWARN_ON_ONCE(enable_owner != current);\n\tWARN_ON_ONCE(enable_refcnt == 0);\n\n\tif (--enable_refcnt) {\n\t\t__release(enable_lock);\n\t\treturn;\n\t}\n\tenable_owner = NULL;\n\tspin_unlock_irqrestore(&enable_lock, flags);\n}\n\nstatic bool clk_core_rate_is_protected(struct clk_core *core)\n{\n\treturn core->protect_count;\n}\n\nstatic bool clk_core_is_prepared(struct clk_core *core)\n{\n\tbool ret = false;\n\n\t \n\tif (!core->ops->is_prepared)\n\t\treturn core->prepare_count;\n\n\tif (!clk_pm_runtime_get(core)) {\n\t\tret = core->ops->is_prepared(core->hw);\n\t\tclk_pm_runtime_put(core);\n\t}\n\n\treturn ret;\n}\n\nstatic bool clk_core_is_enabled(struct clk_core *core)\n{\n\tbool ret = false;\n\n\t \n\tif (!core->ops->is_enabled)\n\t\treturn core->enable_count;\n\n\t \n\tif (core->rpm_enabled) {\n\t\tpm_runtime_get_noresume(core->dev);\n\t\tif (!pm_runtime_active(core->dev)) {\n\t\t\tret = false;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tif ((core->flags & CLK_OPS_PARENT_ENABLE) && core->parent)\n\t\tif (!clk_core_is_enabled(core->parent)) {\n\t\t\tret = false;\n\t\t\tgoto done;\n\t\t}\n\n\tret = core->ops->is_enabled(core->hw);\ndone:\n\tif (core->rpm_enabled)\n\t\tpm_runtime_put(core->dev);\n\n\treturn ret;\n}\n\n \n\nconst char *__clk_get_name(const struct clk *clk)\n{\n\treturn !clk ? NULL : clk->core->name;\n}\nEXPORT_SYMBOL_GPL(__clk_get_name);\n\nconst char *clk_hw_get_name(const struct clk_hw *hw)\n{\n\treturn hw->core->name;\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_name);\n\nstruct clk_hw *__clk_get_hw(struct clk *clk)\n{\n\treturn !clk ? NULL : clk->core->hw;\n}\nEXPORT_SYMBOL_GPL(__clk_get_hw);\n\nunsigned int clk_hw_get_num_parents(const struct clk_hw *hw)\n{\n\treturn hw->core->num_parents;\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_num_parents);\n\nstruct clk_hw *clk_hw_get_parent(const struct clk_hw *hw)\n{\n\treturn hw->core->parent ? hw->core->parent->hw : NULL;\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_parent);\n\nstatic struct clk_core *__clk_lookup_subtree(const char *name,\n\t\t\t\t\t     struct clk_core *core)\n{\n\tstruct clk_core *child;\n\tstruct clk_core *ret;\n\n\tif (!strcmp(core->name, name))\n\t\treturn core;\n\n\thlist_for_each_entry(child, &core->children, child_node) {\n\t\tret = __clk_lookup_subtree(name, child);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct clk_core *clk_core_lookup(const char *name)\n{\n\tstruct clk_core *root_clk;\n\tstruct clk_core *ret;\n\n\tif (!name)\n\t\treturn NULL;\n\n\t \n\thlist_for_each_entry(root_clk, &clk_root_list, child_node) {\n\t\tret = __clk_lookup_subtree(name, root_clk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\thlist_for_each_entry(root_clk, &clk_orphan_list, child_node) {\n\t\tret = __clk_lookup_subtree(name, root_clk);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn NULL;\n}\n\n#ifdef CONFIG_OF\nstatic int of_parse_clkspec(const struct device_node *np, int index,\n\t\t\t    const char *name, struct of_phandle_args *out_args);\nstatic struct clk_hw *\nof_clk_get_hw_from_clkspec(struct of_phandle_args *clkspec);\n#else\nstatic inline int of_parse_clkspec(const struct device_node *np, int index,\n\t\t\t\t   const char *name,\n\t\t\t\t   struct of_phandle_args *out_args)\n{\n\treturn -ENOENT;\n}\nstatic inline struct clk_hw *\nof_clk_get_hw_from_clkspec(struct of_phandle_args *clkspec)\n{\n\treturn ERR_PTR(-ENOENT);\n}\n#endif\n\n \nstatic struct clk_core *clk_core_get(struct clk_core *core, u8 p_index)\n{\n\tconst char *name = core->parents[p_index].fw_name;\n\tint index = core->parents[p_index].index;\n\tstruct clk_hw *hw = ERR_PTR(-ENOENT);\n\tstruct device *dev = core->dev;\n\tconst char *dev_id = dev ? dev_name(dev) : NULL;\n\tstruct device_node *np = core->of_node;\n\tstruct of_phandle_args clkspec;\n\n\tif (np && (name || index >= 0) &&\n\t    !of_parse_clkspec(np, index, name, &clkspec)) {\n\t\thw = of_clk_get_hw_from_clkspec(&clkspec);\n\t\tof_node_put(clkspec.np);\n\t} else if (name) {\n\t\t \n\t\thw = clk_find_hw(dev_id, name);\n\t}\n\n\tif (IS_ERR(hw))\n\t\treturn ERR_CAST(hw);\n\n\treturn hw->core;\n}\n\nstatic void clk_core_fill_parent_index(struct clk_core *core, u8 index)\n{\n\tstruct clk_parent_map *entry = &core->parents[index];\n\tstruct clk_core *parent;\n\n\tif (entry->hw) {\n\t\tparent = entry->hw->core;\n\t} else {\n\t\tparent = clk_core_get(core, index);\n\t\tif (PTR_ERR(parent) == -ENOENT && entry->name)\n\t\t\tparent = clk_core_lookup(entry->name);\n\t}\n\n\t \n\tif (!parent)\n\t\tparent = ERR_PTR(-EPROBE_DEFER);\n\n\t \n\tif (!IS_ERR(parent))\n\t\tentry->core = parent;\n}\n\nstatic struct clk_core *clk_core_get_parent_by_index(struct clk_core *core,\n\t\t\t\t\t\t\t u8 index)\n{\n\tif (!core || index >= core->num_parents || !core->parents)\n\t\treturn NULL;\n\n\tif (!core->parents[index].core)\n\t\tclk_core_fill_parent_index(core, index);\n\n\treturn core->parents[index].core;\n}\n\nstruct clk_hw *\nclk_hw_get_parent_by_index(const struct clk_hw *hw, unsigned int index)\n{\n\tstruct clk_core *parent;\n\n\tparent = clk_core_get_parent_by_index(hw->core, index);\n\n\treturn !parent ? NULL : parent->hw;\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_parent_by_index);\n\nunsigned int __clk_get_enable_count(struct clk *clk)\n{\n\treturn !clk ? 0 : clk->core->enable_count;\n}\n\nstatic unsigned long clk_core_get_rate_nolock(struct clk_core *core)\n{\n\tif (!core)\n\t\treturn 0;\n\n\tif (!core->num_parents || core->parent)\n\t\treturn core->rate;\n\n\t \n\treturn 0;\n}\n\nunsigned long clk_hw_get_rate(const struct clk_hw *hw)\n{\n\treturn clk_core_get_rate_nolock(hw->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_rate);\n\nstatic unsigned long clk_core_get_accuracy_no_lock(struct clk_core *core)\n{\n\tif (!core)\n\t\treturn 0;\n\n\treturn core->accuracy;\n}\n\nunsigned long clk_hw_get_flags(const struct clk_hw *hw)\n{\n\treturn hw->core->flags;\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_flags);\n\nbool clk_hw_is_prepared(const struct clk_hw *hw)\n{\n\treturn clk_core_is_prepared(hw->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_is_prepared);\n\nbool clk_hw_rate_is_protected(const struct clk_hw *hw)\n{\n\treturn clk_core_rate_is_protected(hw->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_rate_is_protected);\n\nbool clk_hw_is_enabled(const struct clk_hw *hw)\n{\n\treturn clk_core_is_enabled(hw->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_is_enabled);\n\nbool __clk_is_enabled(struct clk *clk)\n{\n\tif (!clk)\n\t\treturn false;\n\n\treturn clk_core_is_enabled(clk->core);\n}\nEXPORT_SYMBOL_GPL(__clk_is_enabled);\n\nstatic bool mux_is_better_rate(unsigned long rate, unsigned long now,\n\t\t\t   unsigned long best, unsigned long flags)\n{\n\tif (flags & CLK_MUX_ROUND_CLOSEST)\n\t\treturn abs(now - rate) < abs(best - rate);\n\n\treturn now <= rate && now > best;\n}\n\nstatic void clk_core_init_rate_req(struct clk_core * const core,\n\t\t\t\t   struct clk_rate_request *req,\n\t\t\t\t   unsigned long rate);\n\nstatic int clk_core_round_rate_nolock(struct clk_core *core,\n\t\t\t\t      struct clk_rate_request *req);\n\nstatic bool clk_core_has_parent(struct clk_core *core, const struct clk_core *parent)\n{\n\tstruct clk_core *tmp;\n\tunsigned int i;\n\n\t \n\tif (core->parent == parent)\n\t\treturn true;\n\n\tfor (i = 0; i < core->num_parents; i++) {\n\t\ttmp = clk_core_get_parent_by_index(core, i);\n\t\tif (!tmp)\n\t\t\tcontinue;\n\n\t\tif (tmp == parent)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void\nclk_core_forward_rate_req(struct clk_core *core,\n\t\t\t  const struct clk_rate_request *old_req,\n\t\t\t  struct clk_core *parent,\n\t\t\t  struct clk_rate_request *req,\n\t\t\t  unsigned long parent_rate)\n{\n\tif (WARN_ON(!clk_core_has_parent(core, parent)))\n\t\treturn;\n\n\tclk_core_init_rate_req(parent, req, parent_rate);\n\n\tif (req->min_rate < old_req->min_rate)\n\t\treq->min_rate = old_req->min_rate;\n\n\tif (req->max_rate > old_req->max_rate)\n\t\treq->max_rate = old_req->max_rate;\n}\n\nstatic int\nclk_core_determine_rate_no_reparent(struct clk_hw *hw,\n\t\t\t\t    struct clk_rate_request *req)\n{\n\tstruct clk_core *core = hw->core;\n\tstruct clk_core *parent = core->parent;\n\tunsigned long best;\n\tint ret;\n\n\tif (core->flags & CLK_SET_RATE_PARENT) {\n\t\tstruct clk_rate_request parent_req;\n\n\t\tif (!parent) {\n\t\t\treq->rate = 0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tclk_core_forward_rate_req(core, req, parent, &parent_req,\n\t\t\t\t\t  req->rate);\n\n\t\ttrace_clk_rate_request_start(&parent_req);\n\n\t\tret = clk_core_round_rate_nolock(parent, &parent_req);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttrace_clk_rate_request_done(&parent_req);\n\n\t\tbest = parent_req.rate;\n\t} else if (parent) {\n\t\tbest = clk_core_get_rate_nolock(parent);\n\t} else {\n\t\tbest = clk_core_get_rate_nolock(core);\n\t}\n\n\treq->best_parent_rate = best;\n\treq->rate = best;\n\n\treturn 0;\n}\n\nint clk_mux_determine_rate_flags(struct clk_hw *hw,\n\t\t\t\t struct clk_rate_request *req,\n\t\t\t\t unsigned long flags)\n{\n\tstruct clk_core *core = hw->core, *parent, *best_parent = NULL;\n\tint i, num_parents, ret;\n\tunsigned long best = 0;\n\n\t \n\tif (core->flags & CLK_SET_RATE_NO_REPARENT)\n\t\treturn clk_core_determine_rate_no_reparent(hw, req);\n\n\t \n\tnum_parents = core->num_parents;\n\tfor (i = 0; i < num_parents; i++) {\n\t\tunsigned long parent_rate;\n\n\t\tparent = clk_core_get_parent_by_index(core, i);\n\t\tif (!parent)\n\t\t\tcontinue;\n\n\t\tif (core->flags & CLK_SET_RATE_PARENT) {\n\t\t\tstruct clk_rate_request parent_req;\n\n\t\t\tclk_core_forward_rate_req(core, req, parent, &parent_req, req->rate);\n\n\t\t\ttrace_clk_rate_request_start(&parent_req);\n\n\t\t\tret = clk_core_round_rate_nolock(parent, &parent_req);\n\t\t\tif (ret)\n\t\t\t\tcontinue;\n\n\t\t\ttrace_clk_rate_request_done(&parent_req);\n\n\t\t\tparent_rate = parent_req.rate;\n\t\t} else {\n\t\t\tparent_rate = clk_core_get_rate_nolock(parent);\n\t\t}\n\n\t\tif (mux_is_better_rate(req->rate, parent_rate,\n\t\t\t\t       best, flags)) {\n\t\t\tbest_parent = parent;\n\t\t\tbest = parent_rate;\n\t\t}\n\t}\n\n\tif (!best_parent)\n\t\treturn -EINVAL;\n\n\treq->best_parent_hw = best_parent->hw;\n\treq->best_parent_rate = best;\n\treq->rate = best;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(clk_mux_determine_rate_flags);\n\nstruct clk *__clk_lookup(const char *name)\n{\n\tstruct clk_core *core = clk_core_lookup(name);\n\n\treturn !core ? NULL : core->hw->clk;\n}\n\nstatic void clk_core_get_boundaries(struct clk_core *core,\n\t\t\t\t    unsigned long *min_rate,\n\t\t\t\t    unsigned long *max_rate)\n{\n\tstruct clk *clk_user;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\t*min_rate = core->min_rate;\n\t*max_rate = core->max_rate;\n\n\thlist_for_each_entry(clk_user, &core->clks, clks_node)\n\t\t*min_rate = max(*min_rate, clk_user->min_rate);\n\n\thlist_for_each_entry(clk_user, &core->clks, clks_node)\n\t\t*max_rate = min(*max_rate, clk_user->max_rate);\n}\n\n \nvoid clk_hw_get_rate_range(struct clk_hw *hw, unsigned long *min_rate,\n\t\t\t   unsigned long *max_rate)\n{\n\tclk_core_get_boundaries(hw->core, min_rate, max_rate);\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_rate_range);\n\nstatic bool clk_core_check_boundaries(struct clk_core *core,\n\t\t\t\t      unsigned long min_rate,\n\t\t\t\t      unsigned long max_rate)\n{\n\tstruct clk *user;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (min_rate > core->max_rate || max_rate < core->min_rate)\n\t\treturn false;\n\n\thlist_for_each_entry(user, &core->clks, clks_node)\n\t\tif (min_rate > user->max_rate || max_rate < user->min_rate)\n\t\t\treturn false;\n\n\treturn true;\n}\n\nvoid clk_hw_set_rate_range(struct clk_hw *hw, unsigned long min_rate,\n\t\t\t   unsigned long max_rate)\n{\n\thw->core->min_rate = min_rate;\n\thw->core->max_rate = max_rate;\n}\nEXPORT_SYMBOL_GPL(clk_hw_set_rate_range);\n\n \nint __clk_mux_determine_rate(struct clk_hw *hw,\n\t\t\t     struct clk_rate_request *req)\n{\n\treturn clk_mux_determine_rate_flags(hw, req, 0);\n}\nEXPORT_SYMBOL_GPL(__clk_mux_determine_rate);\n\nint __clk_mux_determine_rate_closest(struct clk_hw *hw,\n\t\t\t\t     struct clk_rate_request *req)\n{\n\treturn clk_mux_determine_rate_flags(hw, req, CLK_MUX_ROUND_CLOSEST);\n}\nEXPORT_SYMBOL_GPL(__clk_mux_determine_rate_closest);\n\n \nint clk_hw_determine_rate_no_reparent(struct clk_hw *hw,\n\t\t\t\t      struct clk_rate_request *req)\n{\n\treturn clk_core_determine_rate_no_reparent(hw, req);\n}\nEXPORT_SYMBOL_GPL(clk_hw_determine_rate_no_reparent);\n\n \n\nstatic void clk_core_rate_unprotect(struct clk_core *core)\n{\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn;\n\n\tif (WARN(core->protect_count == 0,\n\t    \"%s already unprotected\\n\", core->name))\n\t\treturn;\n\n\tif (--core->protect_count > 0)\n\t\treturn;\n\n\tclk_core_rate_unprotect(core->parent);\n}\n\nstatic int clk_core_rate_nuke_protect(struct clk_core *core)\n{\n\tint ret;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn -EINVAL;\n\n\tif (core->protect_count == 0)\n\t\treturn 0;\n\n\tret = core->protect_count;\n\tcore->protect_count = 1;\n\tclk_core_rate_unprotect(core);\n\n\treturn ret;\n}\n\n \nvoid clk_rate_exclusive_put(struct clk *clk)\n{\n\tif (!clk)\n\t\treturn;\n\n\tclk_prepare_lock();\n\n\t \n\tif (WARN_ON(clk->exclusive_count <= 0))\n\t\tgoto out;\n\n\tclk_core_rate_unprotect(clk->core);\n\tclk->exclusive_count--;\nout:\n\tclk_prepare_unlock();\n}\nEXPORT_SYMBOL_GPL(clk_rate_exclusive_put);\n\nstatic void clk_core_rate_protect(struct clk_core *core)\n{\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn;\n\n\tif (core->protect_count == 0)\n\t\tclk_core_rate_protect(core->parent);\n\n\tcore->protect_count++;\n}\n\nstatic void clk_core_rate_restore_protect(struct clk_core *core, int count)\n{\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn;\n\n\tif (count == 0)\n\t\treturn;\n\n\tclk_core_rate_protect(core);\n\tcore->protect_count = count;\n}\n\n \nint clk_rate_exclusive_get(struct clk *clk)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\tclk_core_rate_protect(clk->core);\n\tclk->exclusive_count++;\n\tclk_prepare_unlock();\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(clk_rate_exclusive_get);\n\nstatic void clk_core_unprepare(struct clk_core *core)\n{\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn;\n\n\tif (WARN(core->prepare_count == 0,\n\t    \"%s already unprepared\\n\", core->name))\n\t\treturn;\n\n\tif (WARN(core->prepare_count == 1 && core->flags & CLK_IS_CRITICAL,\n\t    \"Unpreparing critical %s\\n\", core->name))\n\t\treturn;\n\n\tif (core->flags & CLK_SET_RATE_GATE)\n\t\tclk_core_rate_unprotect(core);\n\n\tif (--core->prepare_count > 0)\n\t\treturn;\n\n\tWARN(core->enable_count > 0, \"Unpreparing enabled %s\\n\", core->name);\n\n\ttrace_clk_unprepare(core);\n\n\tif (core->ops->unprepare)\n\t\tcore->ops->unprepare(core->hw);\n\n\ttrace_clk_unprepare_complete(core);\n\tclk_core_unprepare(core->parent);\n\tclk_pm_runtime_put(core);\n}\n\nstatic void clk_core_unprepare_lock(struct clk_core *core)\n{\n\tclk_prepare_lock();\n\tclk_core_unprepare(core);\n\tclk_prepare_unlock();\n}\n\n \nvoid clk_unprepare(struct clk *clk)\n{\n\tif (IS_ERR_OR_NULL(clk))\n\t\treturn;\n\n\tclk_core_unprepare_lock(clk->core);\n}\nEXPORT_SYMBOL_GPL(clk_unprepare);\n\nstatic int clk_core_prepare(struct clk_core *core)\n{\n\tint ret = 0;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\tif (core->prepare_count == 0) {\n\t\tret = clk_pm_runtime_get(core);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = clk_core_prepare(core->parent);\n\t\tif (ret)\n\t\t\tgoto runtime_put;\n\n\t\ttrace_clk_prepare(core);\n\n\t\tif (core->ops->prepare)\n\t\t\tret = core->ops->prepare(core->hw);\n\n\t\ttrace_clk_prepare_complete(core);\n\n\t\tif (ret)\n\t\t\tgoto unprepare;\n\t}\n\n\tcore->prepare_count++;\n\n\t \n\tif (core->flags & CLK_SET_RATE_GATE)\n\t\tclk_core_rate_protect(core);\n\n\treturn 0;\nunprepare:\n\tclk_core_unprepare(core->parent);\nruntime_put:\n\tclk_pm_runtime_put(core);\n\treturn ret;\n}\n\nstatic int clk_core_prepare_lock(struct clk_core *core)\n{\n\tint ret;\n\n\tclk_prepare_lock();\n\tret = clk_core_prepare(core);\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\n\n \nint clk_prepare(struct clk *clk)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\treturn clk_core_prepare_lock(clk->core);\n}\nEXPORT_SYMBOL_GPL(clk_prepare);\n\nstatic void clk_core_disable(struct clk_core *core)\n{\n\tlockdep_assert_held(&enable_lock);\n\n\tif (!core)\n\t\treturn;\n\n\tif (WARN(core->enable_count == 0, \"%s already disabled\\n\", core->name))\n\t\treturn;\n\n\tif (WARN(core->enable_count == 1 && core->flags & CLK_IS_CRITICAL,\n\t    \"Disabling critical %s\\n\", core->name))\n\t\treturn;\n\n\tif (--core->enable_count > 0)\n\t\treturn;\n\n\ttrace_clk_disable(core);\n\n\tif (core->ops->disable)\n\t\tcore->ops->disable(core->hw);\n\n\ttrace_clk_disable_complete(core);\n\n\tclk_core_disable(core->parent);\n}\n\nstatic void clk_core_disable_lock(struct clk_core *core)\n{\n\tunsigned long flags;\n\n\tflags = clk_enable_lock();\n\tclk_core_disable(core);\n\tclk_enable_unlock(flags);\n}\n\n \nvoid clk_disable(struct clk *clk)\n{\n\tif (IS_ERR_OR_NULL(clk))\n\t\treturn;\n\n\tclk_core_disable_lock(clk->core);\n}\nEXPORT_SYMBOL_GPL(clk_disable);\n\nstatic int clk_core_enable(struct clk_core *core)\n{\n\tint ret = 0;\n\n\tlockdep_assert_held(&enable_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\tif (WARN(core->prepare_count == 0,\n\t    \"Enabling unprepared %s\\n\", core->name))\n\t\treturn -ESHUTDOWN;\n\n\tif (core->enable_count == 0) {\n\t\tret = clk_core_enable(core->parent);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttrace_clk_enable(core);\n\n\t\tif (core->ops->enable)\n\t\t\tret = core->ops->enable(core->hw);\n\n\t\ttrace_clk_enable_complete(core);\n\n\t\tif (ret) {\n\t\t\tclk_core_disable(core->parent);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tcore->enable_count++;\n\treturn 0;\n}\n\nstatic int clk_core_enable_lock(struct clk_core *core)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tflags = clk_enable_lock();\n\tret = clk_core_enable(core);\n\tclk_enable_unlock(flags);\n\n\treturn ret;\n}\n\n \nvoid clk_gate_restore_context(struct clk_hw *hw)\n{\n\tstruct clk_core *core = hw->core;\n\n\tif (core->enable_count)\n\t\tcore->ops->enable(hw);\n\telse\n\t\tcore->ops->disable(hw);\n}\nEXPORT_SYMBOL_GPL(clk_gate_restore_context);\n\nstatic int clk_core_save_context(struct clk_core *core)\n{\n\tstruct clk_core *child;\n\tint ret = 0;\n\n\thlist_for_each_entry(child, &core->children, child_node) {\n\t\tret = clk_core_save_context(child);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (core->ops && core->ops->save_context)\n\t\tret = core->ops->save_context(core->hw);\n\n\treturn ret;\n}\n\nstatic void clk_core_restore_context(struct clk_core *core)\n{\n\tstruct clk_core *child;\n\n\tif (core->ops && core->ops->restore_context)\n\t\tcore->ops->restore_context(core->hw);\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\tclk_core_restore_context(child);\n}\n\n \nint clk_save_context(void)\n{\n\tstruct clk_core *clk;\n\tint ret;\n\n\thlist_for_each_entry(clk, &clk_root_list, child_node) {\n\t\tret = clk_core_save_context(clk);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\thlist_for_each_entry(clk, &clk_orphan_list, child_node) {\n\t\tret = clk_core_save_context(clk);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(clk_save_context);\n\n \nvoid clk_restore_context(void)\n{\n\tstruct clk_core *core;\n\n\thlist_for_each_entry(core, &clk_root_list, child_node)\n\t\tclk_core_restore_context(core);\n\n\thlist_for_each_entry(core, &clk_orphan_list, child_node)\n\t\tclk_core_restore_context(core);\n}\nEXPORT_SYMBOL_GPL(clk_restore_context);\n\n \nint clk_enable(struct clk *clk)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\treturn clk_core_enable_lock(clk->core);\n}\nEXPORT_SYMBOL_GPL(clk_enable);\n\n \nbool clk_is_enabled_when_prepared(struct clk *clk)\n{\n\treturn clk && !(clk->core->ops->enable && clk->core->ops->disable);\n}\nEXPORT_SYMBOL_GPL(clk_is_enabled_when_prepared);\n\nstatic int clk_core_prepare_enable(struct clk_core *core)\n{\n\tint ret;\n\n\tret = clk_core_prepare_lock(core);\n\tif (ret)\n\t\treturn ret;\n\n\tret = clk_core_enable_lock(core);\n\tif (ret)\n\t\tclk_core_unprepare_lock(core);\n\n\treturn ret;\n}\n\nstatic void clk_core_disable_unprepare(struct clk_core *core)\n{\n\tclk_core_disable_lock(core);\n\tclk_core_unprepare_lock(core);\n}\n\nstatic void __init clk_unprepare_unused_subtree(struct clk_core *core)\n{\n\tstruct clk_core *child;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\tclk_unprepare_unused_subtree(child);\n\n\tif (core->prepare_count)\n\t\treturn;\n\n\tif (core->flags & CLK_IGNORE_UNUSED)\n\t\treturn;\n\n\tif (clk_pm_runtime_get(core))\n\t\treturn;\n\n\tif (clk_core_is_prepared(core)) {\n\t\ttrace_clk_unprepare(core);\n\t\tif (core->ops->unprepare_unused)\n\t\t\tcore->ops->unprepare_unused(core->hw);\n\t\telse if (core->ops->unprepare)\n\t\t\tcore->ops->unprepare(core->hw);\n\t\ttrace_clk_unprepare_complete(core);\n\t}\n\n\tclk_pm_runtime_put(core);\n}\n\nstatic void __init clk_disable_unused_subtree(struct clk_core *core)\n{\n\tstruct clk_core *child;\n\tunsigned long flags;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\tclk_disable_unused_subtree(child);\n\n\tif (core->flags & CLK_OPS_PARENT_ENABLE)\n\t\tclk_core_prepare_enable(core->parent);\n\n\tif (clk_pm_runtime_get(core))\n\t\tgoto unprepare_out;\n\n\tflags = clk_enable_lock();\n\n\tif (core->enable_count)\n\t\tgoto unlock_out;\n\n\tif (core->flags & CLK_IGNORE_UNUSED)\n\t\tgoto unlock_out;\n\n\t \n\tif (clk_core_is_enabled(core)) {\n\t\ttrace_clk_disable(core);\n\t\tif (core->ops->disable_unused)\n\t\t\tcore->ops->disable_unused(core->hw);\n\t\telse if (core->ops->disable)\n\t\t\tcore->ops->disable(core->hw);\n\t\ttrace_clk_disable_complete(core);\n\t}\n\nunlock_out:\n\tclk_enable_unlock(flags);\n\tclk_pm_runtime_put(core);\nunprepare_out:\n\tif (core->flags & CLK_OPS_PARENT_ENABLE)\n\t\tclk_core_disable_unprepare(core->parent);\n}\n\nstatic bool clk_ignore_unused __initdata;\nstatic int __init clk_ignore_unused_setup(char *__unused)\n{\n\tclk_ignore_unused = true;\n\treturn 1;\n}\n__setup(\"clk_ignore_unused\", clk_ignore_unused_setup);\n\nstatic int __init clk_disable_unused(void)\n{\n\tstruct clk_core *core;\n\n\tif (clk_ignore_unused) {\n\t\tpr_warn(\"clk: Not disabling unused clocks\\n\");\n\t\treturn 0;\n\t}\n\n\tpr_info(\"clk: Disabling unused clocks\\n\");\n\n\tclk_prepare_lock();\n\n\thlist_for_each_entry(core, &clk_root_list, child_node)\n\t\tclk_disable_unused_subtree(core);\n\n\thlist_for_each_entry(core, &clk_orphan_list, child_node)\n\t\tclk_disable_unused_subtree(core);\n\n\thlist_for_each_entry(core, &clk_root_list, child_node)\n\t\tclk_unprepare_unused_subtree(core);\n\n\thlist_for_each_entry(core, &clk_orphan_list, child_node)\n\t\tclk_unprepare_unused_subtree(core);\n\n\tclk_prepare_unlock();\n\n\treturn 0;\n}\nlate_initcall_sync(clk_disable_unused);\n\nstatic int clk_core_determine_round_nolock(struct clk_core *core,\n\t\t\t\t\t   struct clk_rate_request *req)\n{\n\tlong rate;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\t \n\tif (!req->min_rate && !req->max_rate)\n\t\tpr_warn(\"%s: %s: clk_rate_request has initialized min or max rate.\\n\",\n\t\t\t__func__, core->name);\n\telse\n\t\treq->rate = clamp(req->rate, req->min_rate, req->max_rate);\n\n\t \n\tif (clk_core_rate_is_protected(core)) {\n\t\treq->rate = core->rate;\n\t} else if (core->ops->determine_rate) {\n\t\treturn core->ops->determine_rate(core->hw, req);\n\t} else if (core->ops->round_rate) {\n\t\trate = core->ops->round_rate(core->hw, req->rate,\n\t\t\t\t\t     &req->best_parent_rate);\n\t\tif (rate < 0)\n\t\t\treturn rate;\n\n\t\treq->rate = rate;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void clk_core_init_rate_req(struct clk_core * const core,\n\t\t\t\t   struct clk_rate_request *req,\n\t\t\t\t   unsigned long rate)\n{\n\tstruct clk_core *parent;\n\n\tif (WARN_ON(!req))\n\t\treturn;\n\n\tmemset(req, 0, sizeof(*req));\n\treq->max_rate = ULONG_MAX;\n\n\tif (!core)\n\t\treturn;\n\n\treq->core = core;\n\treq->rate = rate;\n\tclk_core_get_boundaries(core, &req->min_rate, &req->max_rate);\n\n\tparent = core->parent;\n\tif (parent) {\n\t\treq->best_parent_hw = parent->hw;\n\t\treq->best_parent_rate = parent->rate;\n\t} else {\n\t\treq->best_parent_hw = NULL;\n\t\treq->best_parent_rate = 0;\n\t}\n}\n\n \nvoid clk_hw_init_rate_request(const struct clk_hw *hw,\n\t\t\t      struct clk_rate_request *req,\n\t\t\t      unsigned long rate)\n{\n\tif (WARN_ON(!hw || !req))\n\t\treturn;\n\n\tclk_core_init_rate_req(hw->core, req, rate);\n}\nEXPORT_SYMBOL_GPL(clk_hw_init_rate_request);\n\n \nvoid clk_hw_forward_rate_request(const struct clk_hw *hw,\n\t\t\t\t const struct clk_rate_request *old_req,\n\t\t\t\t const struct clk_hw *parent,\n\t\t\t\t struct clk_rate_request *req,\n\t\t\t\t unsigned long parent_rate)\n{\n\tif (WARN_ON(!hw || !old_req || !parent || !req))\n\t\treturn;\n\n\tclk_core_forward_rate_req(hw->core, old_req,\n\t\t\t\t  parent->core, req,\n\t\t\t\t  parent_rate);\n}\nEXPORT_SYMBOL_GPL(clk_hw_forward_rate_request);\n\nstatic bool clk_core_can_round(struct clk_core * const core)\n{\n\treturn core->ops->determine_rate || core->ops->round_rate;\n}\n\nstatic int clk_core_round_rate_nolock(struct clk_core *core,\n\t\t\t\t      struct clk_rate_request *req)\n{\n\tint ret;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core) {\n\t\treq->rate = 0;\n\t\treturn 0;\n\t}\n\n\tif (clk_core_can_round(core))\n\t\treturn clk_core_determine_round_nolock(core, req);\n\n\tif (core->flags & CLK_SET_RATE_PARENT) {\n\t\tstruct clk_rate_request parent_req;\n\n\t\tclk_core_forward_rate_req(core, req, core->parent, &parent_req, req->rate);\n\n\t\ttrace_clk_rate_request_start(&parent_req);\n\n\t\tret = clk_core_round_rate_nolock(core->parent, &parent_req);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ttrace_clk_rate_request_done(&parent_req);\n\n\t\treq->best_parent_rate = parent_req.rate;\n\t\treq->rate = parent_req.rate;\n\n\t\treturn 0;\n\t}\n\n\treq->rate = core->rate;\n\treturn 0;\n}\n\n \nint __clk_determine_rate(struct clk_hw *hw, struct clk_rate_request *req)\n{\n\tif (!hw) {\n\t\treq->rate = 0;\n\t\treturn 0;\n\t}\n\n\treturn clk_core_round_rate_nolock(hw->core, req);\n}\nEXPORT_SYMBOL_GPL(__clk_determine_rate);\n\n \nunsigned long clk_hw_round_rate(struct clk_hw *hw, unsigned long rate)\n{\n\tint ret;\n\tstruct clk_rate_request req;\n\n\tclk_core_init_rate_req(hw->core, &req, rate);\n\n\ttrace_clk_rate_request_start(&req);\n\n\tret = clk_core_round_rate_nolock(hw->core, &req);\n\tif (ret)\n\t\treturn 0;\n\n\ttrace_clk_rate_request_done(&req);\n\n\treturn req.rate;\n}\nEXPORT_SYMBOL_GPL(clk_hw_round_rate);\n\n \nlong clk_round_rate(struct clk *clk, unsigned long rate)\n{\n\tstruct clk_rate_request req;\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\tclk_core_init_rate_req(clk->core, &req, rate);\n\n\ttrace_clk_rate_request_start(&req);\n\n\tret = clk_core_round_rate_nolock(clk->core, &req);\n\n\ttrace_clk_rate_request_done(&req);\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\tclk_prepare_unlock();\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn req.rate;\n}\nEXPORT_SYMBOL_GPL(clk_round_rate);\n\n \nstatic int __clk_notify(struct clk_core *core, unsigned long msg,\n\t\tunsigned long old_rate, unsigned long new_rate)\n{\n\tstruct clk_notifier *cn;\n\tstruct clk_notifier_data cnd;\n\tint ret = NOTIFY_DONE;\n\n\tcnd.old_rate = old_rate;\n\tcnd.new_rate = new_rate;\n\n\tlist_for_each_entry(cn, &clk_notifier_list, node) {\n\t\tif (cn->clk->core == core) {\n\t\t\tcnd.clk = cn->clk;\n\t\t\tret = srcu_notifier_call_chain(&cn->notifier_head, msg,\n\t\t\t\t\t&cnd);\n\t\t\tif (ret & NOTIFY_STOP_MASK)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic void __clk_recalc_accuracies(struct clk_core *core)\n{\n\tunsigned long parent_accuracy = 0;\n\tstruct clk_core *child;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (core->parent)\n\t\tparent_accuracy = core->parent->accuracy;\n\n\tif (core->ops->recalc_accuracy)\n\t\tcore->accuracy = core->ops->recalc_accuracy(core->hw,\n\t\t\t\t\t\t\t  parent_accuracy);\n\telse\n\t\tcore->accuracy = parent_accuracy;\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\t__clk_recalc_accuracies(child);\n}\n\nstatic long clk_core_get_accuracy_recalc(struct clk_core *core)\n{\n\tif (core && (core->flags & CLK_GET_ACCURACY_NOCACHE))\n\t\t__clk_recalc_accuracies(core);\n\n\treturn clk_core_get_accuracy_no_lock(core);\n}\n\n \nlong clk_get_accuracy(struct clk *clk)\n{\n\tlong accuracy;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\taccuracy = clk_core_get_accuracy_recalc(clk->core);\n\tclk_prepare_unlock();\n\n\treturn accuracy;\n}\nEXPORT_SYMBOL_GPL(clk_get_accuracy);\n\nstatic unsigned long clk_recalc(struct clk_core *core,\n\t\t\t\tunsigned long parent_rate)\n{\n\tunsigned long rate = parent_rate;\n\n\tif (core->ops->recalc_rate && !clk_pm_runtime_get(core)) {\n\t\trate = core->ops->recalc_rate(core->hw, parent_rate);\n\t\tclk_pm_runtime_put(core);\n\t}\n\treturn rate;\n}\n\n \nstatic void __clk_recalc_rates(struct clk_core *core, bool update_req,\n\t\t\t       unsigned long msg)\n{\n\tunsigned long old_rate;\n\tunsigned long parent_rate = 0;\n\tstruct clk_core *child;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\told_rate = core->rate;\n\n\tif (core->parent)\n\t\tparent_rate = core->parent->rate;\n\n\tcore->rate = clk_recalc(core, parent_rate);\n\tif (update_req)\n\t\tcore->req_rate = core->rate;\n\n\t \n\tif (core->notifier_count && msg)\n\t\t__clk_notify(core, msg, old_rate, core->rate);\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\t__clk_recalc_rates(child, update_req, msg);\n}\n\nstatic unsigned long clk_core_get_rate_recalc(struct clk_core *core)\n{\n\tif (core && (core->flags & CLK_GET_RATE_NOCACHE))\n\t\t__clk_recalc_rates(core, false, 0);\n\n\treturn clk_core_get_rate_nolock(core);\n}\n\n \nunsigned long clk_get_rate(struct clk *clk)\n{\n\tunsigned long rate;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\trate = clk_core_get_rate_recalc(clk->core);\n\tclk_prepare_unlock();\n\n\treturn rate;\n}\nEXPORT_SYMBOL_GPL(clk_get_rate);\n\nstatic int clk_fetch_parent_index(struct clk_core *core,\n\t\t\t\t  struct clk_core *parent)\n{\n\tint i;\n\n\tif (!parent)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < core->num_parents; i++) {\n\t\t \n\t\tif (core->parents[i].core == parent)\n\t\t\treturn i;\n\n\t\t \n\t\tif (core->parents[i].core)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (core->parents[i].hw) {\n\t\t\tif (core->parents[i].hw == parent->hw)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (parent == clk_core_get(core, i))\n\t\t\tbreak;\n\n\t\t \n\t\tif (core->parents[i].name &&\n\t\t    !strcmp(parent->name, core->parents[i].name))\n\t\t\tbreak;\n\t}\n\n\tif (i == core->num_parents)\n\t\treturn -EINVAL;\n\n\tcore->parents[i].core = parent;\n\treturn i;\n}\n\n \nint clk_hw_get_parent_index(struct clk_hw *hw)\n{\n\tstruct clk_hw *parent = clk_hw_get_parent(hw);\n\n\tif (WARN_ON(parent == NULL))\n\t\treturn -EINVAL;\n\n\treturn clk_fetch_parent_index(hw->core, parent->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_get_parent_index);\n\n \nstatic void clk_core_update_orphan_status(struct clk_core *core, bool is_orphan)\n{\n\tstruct clk_core *child;\n\n\tcore->orphan = is_orphan;\n\n\thlist_for_each_entry(child, &core->children, child_node)\n\t\tclk_core_update_orphan_status(child, is_orphan);\n}\n\nstatic void clk_reparent(struct clk_core *core, struct clk_core *new_parent)\n{\n\tbool was_orphan = core->orphan;\n\n\thlist_del(&core->child_node);\n\n\tif (new_parent) {\n\t\tbool becomes_orphan = new_parent->orphan;\n\n\t\t \n\t\tif (new_parent->new_child == core)\n\t\t\tnew_parent->new_child = NULL;\n\n\t\thlist_add_head(&core->child_node, &new_parent->children);\n\n\t\tif (was_orphan != becomes_orphan)\n\t\t\tclk_core_update_orphan_status(core, becomes_orphan);\n\t} else {\n\t\thlist_add_head(&core->child_node, &clk_orphan_list);\n\t\tif (!was_orphan)\n\t\t\tclk_core_update_orphan_status(core, true);\n\t}\n\n\tcore->parent = new_parent;\n}\n\nstatic struct clk_core *__clk_set_parent_before(struct clk_core *core,\n\t\t\t\t\t   struct clk_core *parent)\n{\n\tunsigned long flags;\n\tstruct clk_core *old_parent = core->parent;\n\n\t \n\n\t \n\tif (core->flags & CLK_OPS_PARENT_ENABLE) {\n\t\tclk_core_prepare_enable(old_parent);\n\t\tclk_core_prepare_enable(parent);\n\t}\n\n\t \n\tif (core->prepare_count) {\n\t\tclk_core_prepare_enable(parent);\n\t\tclk_core_enable_lock(core);\n\t}\n\n\t \n\tflags = clk_enable_lock();\n\tclk_reparent(core, parent);\n\tclk_enable_unlock(flags);\n\n\treturn old_parent;\n}\n\nstatic void __clk_set_parent_after(struct clk_core *core,\n\t\t\t\t   struct clk_core *parent,\n\t\t\t\t   struct clk_core *old_parent)\n{\n\t \n\tif (core->prepare_count) {\n\t\tclk_core_disable_lock(core);\n\t\tclk_core_disable_unprepare(old_parent);\n\t}\n\n\t \n\tif (core->flags & CLK_OPS_PARENT_ENABLE) {\n\t\tclk_core_disable_unprepare(parent);\n\t\tclk_core_disable_unprepare(old_parent);\n\t}\n}\n\nstatic int __clk_set_parent(struct clk_core *core, struct clk_core *parent,\n\t\t\t    u8 p_index)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\tstruct clk_core *old_parent;\n\n\told_parent = __clk_set_parent_before(core, parent);\n\n\ttrace_clk_set_parent(core, parent);\n\n\t \n\tif (parent && core->ops->set_parent)\n\t\tret = core->ops->set_parent(core->hw, p_index);\n\n\ttrace_clk_set_parent_complete(core, parent);\n\n\tif (ret) {\n\t\tflags = clk_enable_lock();\n\t\tclk_reparent(core, old_parent);\n\t\tclk_enable_unlock(flags);\n\n\t\t__clk_set_parent_after(core, old_parent, parent);\n\n\t\treturn ret;\n\t}\n\n\t__clk_set_parent_after(core, parent, old_parent);\n\n\treturn 0;\n}\n\n \nstatic int __clk_speculate_rates(struct clk_core *core,\n\t\t\t\t unsigned long parent_rate)\n{\n\tstruct clk_core *child;\n\tunsigned long new_rate;\n\tint ret = NOTIFY_DONE;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tnew_rate = clk_recalc(core, parent_rate);\n\n\t \n\tif (core->notifier_count)\n\t\tret = __clk_notify(core, PRE_RATE_CHANGE, core->rate, new_rate);\n\n\tif (ret & NOTIFY_STOP_MASK) {\n\t\tpr_debug(\"%s: clk notifier callback for clock %s aborted with error %d\\n\",\n\t\t\t\t__func__, core->name, ret);\n\t\tgoto out;\n\t}\n\n\thlist_for_each_entry(child, &core->children, child_node) {\n\t\tret = __clk_speculate_rates(child, new_rate);\n\t\tif (ret & NOTIFY_STOP_MASK)\n\t\t\tbreak;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void clk_calc_subtree(struct clk_core *core, unsigned long new_rate,\n\t\t\t     struct clk_core *new_parent, u8 p_index)\n{\n\tstruct clk_core *child;\n\n\tcore->new_rate = new_rate;\n\tcore->new_parent = new_parent;\n\tcore->new_parent_index = p_index;\n\t \n\tcore->new_child = NULL;\n\tif (new_parent && new_parent != core->parent)\n\t\tnew_parent->new_child = core;\n\n\thlist_for_each_entry(child, &core->children, child_node) {\n\t\tchild->new_rate = clk_recalc(child, new_rate);\n\t\tclk_calc_subtree(child, child->new_rate, NULL, 0);\n\t}\n}\n\n \nstatic struct clk_core *clk_calc_new_rates(struct clk_core *core,\n\t\t\t\t\t   unsigned long rate)\n{\n\tstruct clk_core *top = core;\n\tstruct clk_core *old_parent, *parent;\n\tunsigned long best_parent_rate = 0;\n\tunsigned long new_rate;\n\tunsigned long min_rate;\n\tunsigned long max_rate;\n\tint p_index = 0;\n\tlong ret;\n\n\t \n\tif (IS_ERR_OR_NULL(core))\n\t\treturn NULL;\n\n\t \n\tparent = old_parent = core->parent;\n\tif (parent)\n\t\tbest_parent_rate = parent->rate;\n\n\tclk_core_get_boundaries(core, &min_rate, &max_rate);\n\n\t \n\tif (clk_core_can_round(core)) {\n\t\tstruct clk_rate_request req;\n\n\t\tclk_core_init_rate_req(core, &req, rate);\n\n\t\ttrace_clk_rate_request_start(&req);\n\n\t\tret = clk_core_determine_round_nolock(core, &req);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\n\t\ttrace_clk_rate_request_done(&req);\n\n\t\tbest_parent_rate = req.best_parent_rate;\n\t\tnew_rate = req.rate;\n\t\tparent = req.best_parent_hw ? req.best_parent_hw->core : NULL;\n\n\t\tif (new_rate < min_rate || new_rate > max_rate)\n\t\t\treturn NULL;\n\t} else if (!parent || !(core->flags & CLK_SET_RATE_PARENT)) {\n\t\t \n\t\tcore->new_rate = core->rate;\n\t\treturn NULL;\n\t} else {\n\t\t \n\t\ttop = clk_calc_new_rates(parent, rate);\n\t\tnew_rate = parent->new_rate;\n\t\tgoto out;\n\t}\n\n\t \n\tif (parent != old_parent &&\n\t    (core->flags & CLK_SET_PARENT_GATE) && core->prepare_count) {\n\t\tpr_debug(\"%s: %s not gated but wants to reparent\\n\",\n\t\t\t __func__, core->name);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (parent && core->num_parents > 1) {\n\t\tp_index = clk_fetch_parent_index(core, parent);\n\t\tif (p_index < 0) {\n\t\t\tpr_debug(\"%s: clk %s can not be parent of clk %s\\n\",\n\t\t\t\t __func__, parent->name, core->name);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif ((core->flags & CLK_SET_RATE_PARENT) && parent &&\n\t    best_parent_rate != parent->rate)\n\t\ttop = clk_calc_new_rates(parent, best_parent_rate);\n\nout:\n\tclk_calc_subtree(core, new_rate, parent, p_index);\n\n\treturn top;\n}\n\n \nstatic struct clk_core *clk_propagate_rate_change(struct clk_core *core,\n\t\t\t\t\t\t  unsigned long event)\n{\n\tstruct clk_core *child, *tmp_clk, *fail_clk = NULL;\n\tint ret = NOTIFY_DONE;\n\n\tif (core->rate == core->new_rate)\n\t\treturn NULL;\n\n\tif (core->notifier_count) {\n\t\tret = __clk_notify(core, event, core->rate, core->new_rate);\n\t\tif (ret & NOTIFY_STOP_MASK)\n\t\t\tfail_clk = core;\n\t}\n\n\thlist_for_each_entry(child, &core->children, child_node) {\n\t\t \n\t\tif (child->new_parent && child->new_parent != core)\n\t\t\tcontinue;\n\t\ttmp_clk = clk_propagate_rate_change(child, event);\n\t\tif (tmp_clk)\n\t\t\tfail_clk = tmp_clk;\n\t}\n\n\t \n\tif (core->new_child) {\n\t\ttmp_clk = clk_propagate_rate_change(core->new_child, event);\n\t\tif (tmp_clk)\n\t\t\tfail_clk = tmp_clk;\n\t}\n\n\treturn fail_clk;\n}\n\n \nstatic void clk_change_rate(struct clk_core *core)\n{\n\tstruct clk_core *child;\n\tstruct hlist_node *tmp;\n\tunsigned long old_rate;\n\tunsigned long best_parent_rate = 0;\n\tbool skip_set_rate = false;\n\tstruct clk_core *old_parent;\n\tstruct clk_core *parent = NULL;\n\n\told_rate = core->rate;\n\n\tif (core->new_parent) {\n\t\tparent = core->new_parent;\n\t\tbest_parent_rate = core->new_parent->rate;\n\t} else if (core->parent) {\n\t\tparent = core->parent;\n\t\tbest_parent_rate = core->parent->rate;\n\t}\n\n\tif (clk_pm_runtime_get(core))\n\t\treturn;\n\n\tif (core->flags & CLK_SET_RATE_UNGATE) {\n\t\tclk_core_prepare(core);\n\t\tclk_core_enable_lock(core);\n\t}\n\n\tif (core->new_parent && core->new_parent != core->parent) {\n\t\told_parent = __clk_set_parent_before(core, core->new_parent);\n\t\ttrace_clk_set_parent(core, core->new_parent);\n\n\t\tif (core->ops->set_rate_and_parent) {\n\t\t\tskip_set_rate = true;\n\t\t\tcore->ops->set_rate_and_parent(core->hw, core->new_rate,\n\t\t\t\t\tbest_parent_rate,\n\t\t\t\t\tcore->new_parent_index);\n\t\t} else if (core->ops->set_parent) {\n\t\t\tcore->ops->set_parent(core->hw, core->new_parent_index);\n\t\t}\n\n\t\ttrace_clk_set_parent_complete(core, core->new_parent);\n\t\t__clk_set_parent_after(core, core->new_parent, old_parent);\n\t}\n\n\tif (core->flags & CLK_OPS_PARENT_ENABLE)\n\t\tclk_core_prepare_enable(parent);\n\n\ttrace_clk_set_rate(core, core->new_rate);\n\n\tif (!skip_set_rate && core->ops->set_rate)\n\t\tcore->ops->set_rate(core->hw, core->new_rate, best_parent_rate);\n\n\ttrace_clk_set_rate_complete(core, core->new_rate);\n\n\tcore->rate = clk_recalc(core, best_parent_rate);\n\n\tif (core->flags & CLK_SET_RATE_UNGATE) {\n\t\tclk_core_disable_lock(core);\n\t\tclk_core_unprepare(core);\n\t}\n\n\tif (core->flags & CLK_OPS_PARENT_ENABLE)\n\t\tclk_core_disable_unprepare(parent);\n\n\tif (core->notifier_count && old_rate != core->rate)\n\t\t__clk_notify(core, POST_RATE_CHANGE, old_rate, core->rate);\n\n\tif (core->flags & CLK_RECALC_NEW_RATES)\n\t\t(void)clk_calc_new_rates(core, core->new_rate);\n\n\t \n\thlist_for_each_entry_safe(child, tmp, &core->children, child_node) {\n\t\t \n\t\tif (child->new_parent && child->new_parent != core)\n\t\t\tcontinue;\n\t\tclk_change_rate(child);\n\t}\n\n\t \n\tif (core->new_child)\n\t\tclk_change_rate(core->new_child);\n\n\tclk_pm_runtime_put(core);\n}\n\nstatic unsigned long clk_core_req_round_rate_nolock(struct clk_core *core,\n\t\t\t\t\t\t     unsigned long req_rate)\n{\n\tint ret, cnt;\n\tstruct clk_rate_request req;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\t \n\tcnt = clk_core_rate_nuke_protect(core);\n\tif (cnt < 0)\n\t\treturn cnt;\n\n\tclk_core_init_rate_req(core, &req, req_rate);\n\n\ttrace_clk_rate_request_start(&req);\n\n\tret = clk_core_round_rate_nolock(core, &req);\n\n\ttrace_clk_rate_request_done(&req);\n\n\t \n\tclk_core_rate_restore_protect(core, cnt);\n\n\treturn ret ? 0 : req.rate;\n}\n\nstatic int clk_core_set_rate_nolock(struct clk_core *core,\n\t\t\t\t    unsigned long req_rate)\n{\n\tstruct clk_core *top, *fail_clk;\n\tunsigned long rate;\n\tint ret;\n\n\tif (!core)\n\t\treturn 0;\n\n\trate = clk_core_req_round_rate_nolock(core, req_rate);\n\n\t \n\tif (rate == clk_core_get_rate_nolock(core))\n\t\treturn 0;\n\n\t \n\tif (clk_core_rate_is_protected(core))\n\t\treturn -EBUSY;\n\n\t \n\ttop = clk_calc_new_rates(core, req_rate);\n\tif (!top)\n\t\treturn -EINVAL;\n\n\tret = clk_pm_runtime_get(core);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfail_clk = clk_propagate_rate_change(top, PRE_RATE_CHANGE);\n\tif (fail_clk) {\n\t\tpr_debug(\"%s: failed to set %s rate\\n\", __func__,\n\t\t\t\tfail_clk->name);\n\t\tclk_propagate_rate_change(top, ABORT_RATE_CHANGE);\n\t\tret = -EBUSY;\n\t\tgoto err;\n\t}\n\n\t \n\tclk_change_rate(top);\n\n\tcore->req_rate = req_rate;\nerr:\n\tclk_pm_runtime_put(core);\n\n\treturn ret;\n}\n\n \nint clk_set_rate(struct clk *clk, unsigned long rate)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\t \n\tclk_prepare_lock();\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\tret = clk_core_set_rate_nolock(clk->core, rate);\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_rate);\n\n \nint clk_set_rate_exclusive(struct clk *clk, unsigned long rate)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\t \n\tclk_prepare_lock();\n\n\t \n\n\tret = clk_core_set_rate_nolock(clk->core, rate);\n\tif (!ret) {\n\t\tclk_core_rate_protect(clk->core);\n\t\tclk->exclusive_count++;\n\t}\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_rate_exclusive);\n\nstatic int clk_set_rate_range_nolock(struct clk *clk,\n\t\t\t\t     unsigned long min,\n\t\t\t\t     unsigned long max)\n{\n\tint ret = 0;\n\tunsigned long old_min, old_max, rate;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!clk)\n\t\treturn 0;\n\n\ttrace_clk_set_rate_range(clk->core, min, max);\n\n\tif (min > max) {\n\t\tpr_err(\"%s: clk %s dev %s con %s: invalid range [%lu, %lu]\\n\",\n\t\t       __func__, clk->core->name, clk->dev_id, clk->con_id,\n\t\t       min, max);\n\t\treturn -EINVAL;\n\t}\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\t \n\told_min = clk->min_rate;\n\told_max = clk->max_rate;\n\tclk->min_rate = min;\n\tclk->max_rate = max;\n\n\tif (!clk_core_check_boundaries(clk->core, min, max)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\trate = clk->core->req_rate;\n\tif (clk->core->flags & CLK_GET_RATE_NOCACHE)\n\t\trate = clk_core_get_rate_recalc(clk->core);\n\n\t \n\trate = clamp(rate, min, max);\n\tret = clk_core_set_rate_nolock(clk->core, rate);\n\tif (ret) {\n\t\t \n\t\tclk->min_rate = old_min;\n\t\tclk->max_rate = old_max;\n\t}\n\nout:\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\treturn ret;\n}\n\n \nint clk_set_rate_range(struct clk *clk, unsigned long min, unsigned long max)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\n\tret = clk_set_rate_range_nolock(clk, min, max);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_rate_range);\n\n \nint clk_set_min_rate(struct clk *clk, unsigned long rate)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\ttrace_clk_set_min_rate(clk->core, rate);\n\n\treturn clk_set_rate_range(clk, rate, clk->max_rate);\n}\nEXPORT_SYMBOL_GPL(clk_set_min_rate);\n\n \nint clk_set_max_rate(struct clk *clk, unsigned long rate)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\ttrace_clk_set_max_rate(clk->core, rate);\n\n\treturn clk_set_rate_range(clk, clk->min_rate, rate);\n}\nEXPORT_SYMBOL_GPL(clk_set_max_rate);\n\n \nstruct clk *clk_get_parent(struct clk *clk)\n{\n\tstruct clk *parent;\n\n\tif (!clk)\n\t\treturn NULL;\n\n\tclk_prepare_lock();\n\t \n\tparent = !clk->core->parent ? NULL : clk->core->parent->hw->clk;\n\tclk_prepare_unlock();\n\n\treturn parent;\n}\nEXPORT_SYMBOL_GPL(clk_get_parent);\n\nstatic struct clk_core *__clk_init_parent(struct clk_core *core)\n{\n\tu8 index = 0;\n\n\tif (core->num_parents > 1 && core->ops->get_parent)\n\t\tindex = core->ops->get_parent(core->hw);\n\n\treturn clk_core_get_parent_by_index(core, index);\n}\n\nstatic void clk_core_reparent(struct clk_core *core,\n\t\t\t\t  struct clk_core *new_parent)\n{\n\tclk_reparent(core, new_parent);\n\t__clk_recalc_accuracies(core);\n\t__clk_recalc_rates(core, true, POST_RATE_CHANGE);\n}\n\nvoid clk_hw_reparent(struct clk_hw *hw, struct clk_hw *new_parent)\n{\n\tif (!hw)\n\t\treturn;\n\n\tclk_core_reparent(hw->core, !new_parent ? NULL : new_parent->core);\n}\n\n \nbool clk_has_parent(const struct clk *clk, const struct clk *parent)\n{\n\t \n\tif (!clk || !parent)\n\t\treturn true;\n\n\treturn clk_core_has_parent(clk->core, parent->core);\n}\nEXPORT_SYMBOL_GPL(clk_has_parent);\n\nstatic int clk_core_set_parent_nolock(struct clk_core *core,\n\t\t\t\t      struct clk_core *parent)\n{\n\tint ret = 0;\n\tint p_index = 0;\n\tunsigned long p_rate = 0;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\tif (core->parent == parent)\n\t\treturn 0;\n\n\t \n\tif (core->num_parents > 1 && !core->ops->set_parent)\n\t\treturn -EPERM;\n\n\t \n\tif ((core->flags & CLK_SET_PARENT_GATE) && core->prepare_count)\n\t\treturn -EBUSY;\n\n\tif (clk_core_rate_is_protected(core))\n\t\treturn -EBUSY;\n\n\t \n\tif (parent) {\n\t\tp_index = clk_fetch_parent_index(core, parent);\n\t\tif (p_index < 0) {\n\t\t\tpr_debug(\"%s: clk %s can not be parent of clk %s\\n\",\n\t\t\t\t\t__func__, parent->name, core->name);\n\t\t\treturn p_index;\n\t\t}\n\t\tp_rate = parent->rate;\n\t}\n\n\tret = clk_pm_runtime_get(core);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = __clk_speculate_rates(core, p_rate);\n\n\t \n\tif (ret & NOTIFY_STOP_MASK)\n\t\tgoto runtime_put;\n\n\t \n\tret = __clk_set_parent(core, parent, p_index);\n\n\t \n\tif (ret) {\n\t\t__clk_recalc_rates(core, true, ABORT_RATE_CHANGE);\n\t} else {\n\t\t__clk_recalc_rates(core, true, POST_RATE_CHANGE);\n\t\t__clk_recalc_accuracies(core);\n\t}\n\nruntime_put:\n\tclk_pm_runtime_put(core);\n\n\treturn ret;\n}\n\nint clk_hw_set_parent(struct clk_hw *hw, struct clk_hw *parent)\n{\n\treturn clk_core_set_parent_nolock(hw->core, parent->core);\n}\nEXPORT_SYMBOL_GPL(clk_hw_set_parent);\n\n \nint clk_set_parent(struct clk *clk, struct clk *parent)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\tret = clk_core_set_parent_nolock(clk->core,\n\t\t\t\t\t parent ? parent->core : NULL);\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_parent);\n\nstatic int clk_core_set_phase_nolock(struct clk_core *core, int degrees)\n{\n\tint ret = -EINVAL;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (!core)\n\t\treturn 0;\n\n\tif (clk_core_rate_is_protected(core))\n\t\treturn -EBUSY;\n\n\ttrace_clk_set_phase(core, degrees);\n\n\tif (core->ops->set_phase) {\n\t\tret = core->ops->set_phase(core->hw, degrees);\n\t\tif (!ret)\n\t\t\tcore->phase = degrees;\n\t}\n\n\ttrace_clk_set_phase_complete(core, degrees);\n\n\treturn ret;\n}\n\n \nint clk_set_phase(struct clk *clk, int degrees)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\t \n\tdegrees %= 360;\n\tif (degrees < 0)\n\t\tdegrees += 360;\n\n\tclk_prepare_lock();\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\tret = clk_core_set_phase_nolock(clk->core, degrees);\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_phase);\n\nstatic int clk_core_get_phase(struct clk_core *core)\n{\n\tint ret;\n\n\tlockdep_assert_held(&prepare_lock);\n\tif (!core->ops->get_phase)\n\t\treturn 0;\n\n\t \n\tret = core->ops->get_phase(core->hw);\n\tif (ret >= 0)\n\t\tcore->phase = ret;\n\n\treturn ret;\n}\n\n \nint clk_get_phase(struct clk *clk)\n{\n\tint ret;\n\n\tif (!clk)\n\t\treturn 0;\n\n\tclk_prepare_lock();\n\tret = clk_core_get_phase(clk->core);\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_get_phase);\n\nstatic void clk_core_reset_duty_cycle_nolock(struct clk_core *core)\n{\n\t \n\tcore->duty.num = 1;\n\tcore->duty.den = 2;\n}\n\nstatic int clk_core_update_duty_cycle_parent_nolock(struct clk_core *core);\n\nstatic int clk_core_update_duty_cycle_nolock(struct clk_core *core)\n{\n\tstruct clk_duty *duty = &core->duty;\n\tint ret = 0;\n\n\tif (!core->ops->get_duty_cycle)\n\t\treturn clk_core_update_duty_cycle_parent_nolock(core);\n\n\tret = core->ops->get_duty_cycle(core->hw, duty);\n\tif (ret)\n\t\tgoto reset;\n\n\t \n\tif (duty->den == 0 || duty->num > duty->den) {\n\t\tret = -EINVAL;\n\t\tgoto reset;\n\t}\n\n\treturn 0;\n\nreset:\n\tclk_core_reset_duty_cycle_nolock(core);\n\treturn ret;\n}\n\nstatic int clk_core_update_duty_cycle_parent_nolock(struct clk_core *core)\n{\n\tint ret = 0;\n\n\tif (core->parent &&\n\t    core->flags & CLK_DUTY_CYCLE_PARENT) {\n\t\tret = clk_core_update_duty_cycle_nolock(core->parent);\n\t\tmemcpy(&core->duty, &core->parent->duty, sizeof(core->duty));\n\t} else {\n\t\tclk_core_reset_duty_cycle_nolock(core);\n\t}\n\n\treturn ret;\n}\n\nstatic int clk_core_set_duty_cycle_parent_nolock(struct clk_core *core,\n\t\t\t\t\t\t struct clk_duty *duty);\n\nstatic int clk_core_set_duty_cycle_nolock(struct clk_core *core,\n\t\t\t\t\t  struct clk_duty *duty)\n{\n\tint ret;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tif (clk_core_rate_is_protected(core))\n\t\treturn -EBUSY;\n\n\ttrace_clk_set_duty_cycle(core, duty);\n\n\tif (!core->ops->set_duty_cycle)\n\t\treturn clk_core_set_duty_cycle_parent_nolock(core, duty);\n\n\tret = core->ops->set_duty_cycle(core->hw, duty);\n\tif (!ret)\n\t\tmemcpy(&core->duty, duty, sizeof(*duty));\n\n\ttrace_clk_set_duty_cycle_complete(core, duty);\n\n\treturn ret;\n}\n\nstatic int clk_core_set_duty_cycle_parent_nolock(struct clk_core *core,\n\t\t\t\t\t\t struct clk_duty *duty)\n{\n\tint ret = 0;\n\n\tif (core->parent &&\n\t    core->flags & (CLK_DUTY_CYCLE_PARENT | CLK_SET_RATE_PARENT)) {\n\t\tret = clk_core_set_duty_cycle_nolock(core->parent, duty);\n\t\tmemcpy(&core->duty, &core->parent->duty, sizeof(core->duty));\n\t}\n\n\treturn ret;\n}\n\n \nint clk_set_duty_cycle(struct clk *clk, unsigned int num, unsigned int den)\n{\n\tint ret;\n\tstruct clk_duty duty;\n\n\tif (!clk)\n\t\treturn 0;\n\n\t \n\tif (den == 0 || num > den)\n\t\treturn -EINVAL;\n\n\tduty.num = num;\n\tduty.den = den;\n\n\tclk_prepare_lock();\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_unprotect(clk->core);\n\n\tret = clk_core_set_duty_cycle_nolock(clk->core, &duty);\n\n\tif (clk->exclusive_count)\n\t\tclk_core_rate_protect(clk->core);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_set_duty_cycle);\n\nstatic int clk_core_get_scaled_duty_cycle(struct clk_core *core,\n\t\t\t\t\t  unsigned int scale)\n{\n\tstruct clk_duty *duty = &core->duty;\n\tint ret;\n\n\tclk_prepare_lock();\n\n\tret = clk_core_update_duty_cycle_nolock(core);\n\tif (!ret)\n\t\tret = mult_frac(scale, duty->num, duty->den);\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\n\n \nint clk_get_scaled_duty_cycle(struct clk *clk, unsigned int scale)\n{\n\tif (!clk)\n\t\treturn 0;\n\n\treturn clk_core_get_scaled_duty_cycle(clk->core, scale);\n}\nEXPORT_SYMBOL_GPL(clk_get_scaled_duty_cycle);\n\n \nbool clk_is_match(const struct clk *p, const struct clk *q)\n{\n\t \n\tif (p == q)\n\t\treturn true;\n\n\t \n\tif (!IS_ERR_OR_NULL(p) && !IS_ERR_OR_NULL(q))\n\t\tif (p->core == q->core)\n\t\t\treturn true;\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(clk_is_match);\n\n \n\n#ifdef CONFIG_DEBUG_FS\n#include <linux/debugfs.h>\n\nstatic struct dentry *rootdir;\nstatic int inited = 0;\nstatic DEFINE_MUTEX(clk_debug_lock);\nstatic HLIST_HEAD(clk_debug_list);\n\nstatic struct hlist_head *orphan_list[] = {\n\t&clk_orphan_list,\n\tNULL,\n};\n\nstatic void clk_summary_show_one(struct seq_file *s, struct clk_core *c,\n\t\t\t\t int level)\n{\n\tint phase;\n\n\tseq_printf(s, \"%*s%-*s %7d %8d %8d %11lu %10lu \",\n\t\t   level * 3 + 1, \"\",\n\t\t   30 - level * 3, c->name,\n\t\t   c->enable_count, c->prepare_count, c->protect_count,\n\t\t   clk_core_get_rate_recalc(c),\n\t\t   clk_core_get_accuracy_recalc(c));\n\n\tphase = clk_core_get_phase(c);\n\tif (phase >= 0)\n\t\tseq_printf(s, \"%5d\", phase);\n\telse\n\t\tseq_puts(s, \"-----\");\n\n\tseq_printf(s, \" %6d\", clk_core_get_scaled_duty_cycle(c, 100000));\n\n\tif (c->ops->is_enabled)\n\t\tseq_printf(s, \" %9c\\n\", clk_core_is_enabled(c) ? 'Y' : 'N');\n\telse if (!c->ops->enable)\n\t\tseq_printf(s, \" %9c\\n\", 'Y');\n\telse\n\t\tseq_printf(s, \" %9c\\n\", '?');\n}\n\nstatic void clk_summary_show_subtree(struct seq_file *s, struct clk_core *c,\n\t\t\t\t     int level)\n{\n\tstruct clk_core *child;\n\n\tclk_pm_runtime_get(c);\n\tclk_summary_show_one(s, c, level);\n\tclk_pm_runtime_put(c);\n\n\thlist_for_each_entry(child, &c->children, child_node)\n\t\tclk_summary_show_subtree(s, child, level + 1);\n}\n\nstatic int clk_summary_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *c;\n\tstruct hlist_head **lists = s->private;\n\n\tseq_puts(s, \"                                 enable  prepare  protect                                duty  hardware\\n\");\n\tseq_puts(s, \"   clock                          count    count    count        rate   accuracy phase  cycle    enable\\n\");\n\tseq_puts(s, \"-------------------------------------------------------------------------------------------------------\\n\");\n\n\tclk_prepare_lock();\n\n\tfor (; *lists; lists++)\n\t\thlist_for_each_entry(c, *lists, child_node)\n\t\t\tclk_summary_show_subtree(s, c, 0);\n\n\tclk_prepare_unlock();\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_summary);\n\nstatic void clk_dump_one(struct seq_file *s, struct clk_core *c, int level)\n{\n\tint phase;\n\tunsigned long min_rate, max_rate;\n\n\tclk_core_get_boundaries(c, &min_rate, &max_rate);\n\n\t \n\tseq_printf(s, \"\\\"%s\\\": { \", c->name);\n\tseq_printf(s, \"\\\"enable_count\\\": %d,\", c->enable_count);\n\tseq_printf(s, \"\\\"prepare_count\\\": %d,\", c->prepare_count);\n\tseq_printf(s, \"\\\"protect_count\\\": %d,\", c->protect_count);\n\tseq_printf(s, \"\\\"rate\\\": %lu,\", clk_core_get_rate_recalc(c));\n\tseq_printf(s, \"\\\"min_rate\\\": %lu,\", min_rate);\n\tseq_printf(s, \"\\\"max_rate\\\": %lu,\", max_rate);\n\tseq_printf(s, \"\\\"accuracy\\\": %lu,\", clk_core_get_accuracy_recalc(c));\n\tphase = clk_core_get_phase(c);\n\tif (phase >= 0)\n\t\tseq_printf(s, \"\\\"phase\\\": %d,\", phase);\n\tseq_printf(s, \"\\\"duty_cycle\\\": %u\",\n\t\t   clk_core_get_scaled_duty_cycle(c, 100000));\n}\n\nstatic void clk_dump_subtree(struct seq_file *s, struct clk_core *c, int level)\n{\n\tstruct clk_core *child;\n\n\tclk_dump_one(s, c, level);\n\n\thlist_for_each_entry(child, &c->children, child_node) {\n\t\tseq_putc(s, ',');\n\t\tclk_dump_subtree(s, child, level + 1);\n\t}\n\n\tseq_putc(s, '}');\n}\n\nstatic int clk_dump_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *c;\n\tbool first_node = true;\n\tstruct hlist_head **lists = s->private;\n\n\tseq_putc(s, '{');\n\tclk_prepare_lock();\n\n\tfor (; *lists; lists++) {\n\t\thlist_for_each_entry(c, *lists, child_node) {\n\t\t\tif (!first_node)\n\t\t\t\tseq_putc(s, ',');\n\t\t\tfirst_node = false;\n\t\t\tclk_dump_subtree(s, c, 0);\n\t\t}\n\t}\n\n\tclk_prepare_unlock();\n\n\tseq_puts(s, \"}\\n\");\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_dump);\n\n#undef CLOCK_ALLOW_WRITE_DEBUGFS\n#ifdef CLOCK_ALLOW_WRITE_DEBUGFS\n \nstatic int clk_rate_set(void *data, u64 val)\n{\n\tstruct clk_core *core = data;\n\tint ret;\n\n\tclk_prepare_lock();\n\tret = clk_core_set_rate_nolock(core, val);\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\n\n#define clk_rate_mode\t0644\n\nstatic int clk_prepare_enable_set(void *data, u64 val)\n{\n\tstruct clk_core *core = data;\n\tint ret = 0;\n\n\tif (val)\n\t\tret = clk_prepare_enable(core->hw->clk);\n\telse\n\t\tclk_disable_unprepare(core->hw->clk);\n\n\treturn ret;\n}\n\nstatic int clk_prepare_enable_get(void *data, u64 *val)\n{\n\tstruct clk_core *core = data;\n\n\t*val = core->enable_count && core->prepare_count;\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(clk_prepare_enable_fops, clk_prepare_enable_get,\n\t\t\t clk_prepare_enable_set, \"%llu\\n\");\n\n#else\n#define clk_rate_set\tNULL\n#define clk_rate_mode\t0444\n#endif\n\nstatic int clk_rate_get(void *data, u64 *val)\n{\n\tstruct clk_core *core = data;\n\n\tclk_prepare_lock();\n\t*val = clk_core_get_rate_recalc(core);\n\tclk_prepare_unlock();\n\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(clk_rate_fops, clk_rate_get, clk_rate_set, \"%llu\\n\");\n\nstatic const struct {\n\tunsigned long flag;\n\tconst char *name;\n} clk_flags[] = {\n#define ENTRY(f) { f, #f }\n\tENTRY(CLK_SET_RATE_GATE),\n\tENTRY(CLK_SET_PARENT_GATE),\n\tENTRY(CLK_SET_RATE_PARENT),\n\tENTRY(CLK_IGNORE_UNUSED),\n\tENTRY(CLK_GET_RATE_NOCACHE),\n\tENTRY(CLK_SET_RATE_NO_REPARENT),\n\tENTRY(CLK_GET_ACCURACY_NOCACHE),\n\tENTRY(CLK_RECALC_NEW_RATES),\n\tENTRY(CLK_SET_RATE_UNGATE),\n\tENTRY(CLK_IS_CRITICAL),\n\tENTRY(CLK_OPS_PARENT_ENABLE),\n\tENTRY(CLK_DUTY_CYCLE_PARENT),\n#undef ENTRY\n};\n\nstatic int clk_flags_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\tunsigned long flags = core->flags;\n\tunsigned int i;\n\n\tfor (i = 0; flags && i < ARRAY_SIZE(clk_flags); i++) {\n\t\tif (flags & clk_flags[i].flag) {\n\t\t\tseq_printf(s, \"%s\\n\", clk_flags[i].name);\n\t\t\tflags &= ~clk_flags[i].flag;\n\t\t}\n\t}\n\tif (flags) {\n\t\t \n\t\tseq_printf(s, \"0x%lx\\n\", flags);\n\t}\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_flags);\n\nstatic void possible_parent_show(struct seq_file *s, struct clk_core *core,\n\t\t\t\t unsigned int i, char terminator)\n{\n\tstruct clk_core *parent;\n\tconst char *name = NULL;\n\n\t \n\tparent = clk_core_get_parent_by_index(core, i);\n\tif (parent) {\n\t\tseq_puts(s, parent->name);\n\t} else if (core->parents[i].name) {\n\t\tseq_puts(s, core->parents[i].name);\n\t} else if (core->parents[i].fw_name) {\n\t\tseq_printf(s, \"<%s>(fw)\", core->parents[i].fw_name);\n\t} else {\n\t\tif (core->parents[i].index >= 0)\n\t\t\tname = of_clk_get_parent_name(core->of_node, core->parents[i].index);\n\t\tif (!name)\n\t\t\tname = \"(missing)\";\n\n\t\tseq_puts(s, name);\n\t}\n\n\tseq_putc(s, terminator);\n}\n\nstatic int possible_parents_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\tint i;\n\n\tfor (i = 0; i < core->num_parents - 1; i++)\n\t\tpossible_parent_show(s, core, i, ' ');\n\n\tpossible_parent_show(s, core, i, '\\n');\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(possible_parents);\n\nstatic int current_parent_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\n\tif (core->parent)\n\t\tseq_printf(s, \"%s\\n\", core->parent->name);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(current_parent);\n\n#ifdef CLOCK_ALLOW_WRITE_DEBUGFS\nstatic ssize_t current_parent_write(struct file *file, const char __user *ubuf,\n\t\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct seq_file *s = file->private_data;\n\tstruct clk_core *core = s->private;\n\tstruct clk_core *parent;\n\tu8 idx;\n\tint err;\n\n\terr = kstrtou8_from_user(ubuf, count, 0, &idx);\n\tif (err < 0)\n\t\treturn err;\n\n\tparent = clk_core_get_parent_by_index(core, idx);\n\tif (!parent)\n\t\treturn -ENOENT;\n\n\tclk_prepare_lock();\n\terr = clk_core_set_parent_nolock(core, parent);\n\tclk_prepare_unlock();\n\tif (err)\n\t\treturn err;\n\n\treturn count;\n}\n\nstatic const struct file_operations current_parent_rw_fops = {\n\t.open\t\t= current_parent_open,\n\t.write\t\t= current_parent_write,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n#endif\n\nstatic int clk_duty_cycle_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\tstruct clk_duty *duty = &core->duty;\n\n\tseq_printf(s, \"%u/%u\\n\", duty->num, duty->den);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_duty_cycle);\n\nstatic int clk_min_rate_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\tunsigned long min_rate, max_rate;\n\n\tclk_prepare_lock();\n\tclk_core_get_boundaries(core, &min_rate, &max_rate);\n\tclk_prepare_unlock();\n\tseq_printf(s, \"%lu\\n\", min_rate);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_min_rate);\n\nstatic int clk_max_rate_show(struct seq_file *s, void *data)\n{\n\tstruct clk_core *core = s->private;\n\tunsigned long min_rate, max_rate;\n\n\tclk_prepare_lock();\n\tclk_core_get_boundaries(core, &min_rate, &max_rate);\n\tclk_prepare_unlock();\n\tseq_printf(s, \"%lu\\n\", max_rate);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(clk_max_rate);\n\nstatic void clk_debug_create_one(struct clk_core *core, struct dentry *pdentry)\n{\n\tstruct dentry *root;\n\n\tif (!core || !pdentry)\n\t\treturn;\n\n\troot = debugfs_create_dir(core->name, pdentry);\n\tcore->dentry = root;\n\n\tdebugfs_create_file(\"clk_rate\", clk_rate_mode, root, core,\n\t\t\t    &clk_rate_fops);\n\tdebugfs_create_file(\"clk_min_rate\", 0444, root, core, &clk_min_rate_fops);\n\tdebugfs_create_file(\"clk_max_rate\", 0444, root, core, &clk_max_rate_fops);\n\tdebugfs_create_ulong(\"clk_accuracy\", 0444, root, &core->accuracy);\n\tdebugfs_create_u32(\"clk_phase\", 0444, root, &core->phase);\n\tdebugfs_create_file(\"clk_flags\", 0444, root, core, &clk_flags_fops);\n\tdebugfs_create_u32(\"clk_prepare_count\", 0444, root, &core->prepare_count);\n\tdebugfs_create_u32(\"clk_enable_count\", 0444, root, &core->enable_count);\n\tdebugfs_create_u32(\"clk_protect_count\", 0444, root, &core->protect_count);\n\tdebugfs_create_u32(\"clk_notifier_count\", 0444, root, &core->notifier_count);\n\tdebugfs_create_file(\"clk_duty_cycle\", 0444, root, core,\n\t\t\t    &clk_duty_cycle_fops);\n#ifdef CLOCK_ALLOW_WRITE_DEBUGFS\n\tdebugfs_create_file(\"clk_prepare_enable\", 0644, root, core,\n\t\t\t    &clk_prepare_enable_fops);\n\n\tif (core->num_parents > 1)\n\t\tdebugfs_create_file(\"clk_parent\", 0644, root, core,\n\t\t\t\t    &current_parent_rw_fops);\n\telse\n#endif\n\tif (core->num_parents > 0)\n\t\tdebugfs_create_file(\"clk_parent\", 0444, root, core,\n\t\t\t\t    &current_parent_fops);\n\n\tif (core->num_parents > 1)\n\t\tdebugfs_create_file(\"clk_possible_parents\", 0444, root, core,\n\t\t\t\t    &possible_parents_fops);\n\n\tif (core->ops->debug_init)\n\t\tcore->ops->debug_init(core->hw, core->dentry);\n}\n\n \nstatic void clk_debug_register(struct clk_core *core)\n{\n\tmutex_lock(&clk_debug_lock);\n\thlist_add_head(&core->debug_node, &clk_debug_list);\n\tif (inited)\n\t\tclk_debug_create_one(core, rootdir);\n\tmutex_unlock(&clk_debug_lock);\n}\n\n  \nstatic void clk_debug_unregister(struct clk_core *core)\n{\n\tmutex_lock(&clk_debug_lock);\n\thlist_del_init(&core->debug_node);\n\tdebugfs_remove_recursive(core->dentry);\n\tcore->dentry = NULL;\n\tmutex_unlock(&clk_debug_lock);\n}\n\n \nstatic int __init clk_debug_init(void)\n{\n\tstruct clk_core *core;\n\n#ifdef CLOCK_ALLOW_WRITE_DEBUGFS\n\tpr_warn(\"\\n\");\n\tpr_warn(\"********************************************************************\\n\");\n\tpr_warn(\"**     NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE           **\\n\");\n\tpr_warn(\"**                                                                **\\n\");\n\tpr_warn(\"**  WRITEABLE clk DebugFS SUPPORT HAS BEEN ENABLED IN THIS KERNEL **\\n\");\n\tpr_warn(\"**                                                                **\\n\");\n\tpr_warn(\"** This means that this kernel is built to expose clk operations  **\\n\");\n\tpr_warn(\"** such as parent or rate setting, enabling, disabling, etc.      **\\n\");\n\tpr_warn(\"** to userspace, which may compromise security on your system.    **\\n\");\n\tpr_warn(\"**                                                                **\\n\");\n\tpr_warn(\"** If you see this message and you are not debugging the          **\\n\");\n\tpr_warn(\"** kernel, report this immediately to your vendor!                **\\n\");\n\tpr_warn(\"**                                                                **\\n\");\n\tpr_warn(\"**     NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE NOTICE           **\\n\");\n\tpr_warn(\"********************************************************************\\n\");\n#endif\n\n\trootdir = debugfs_create_dir(\"clk\", NULL);\n\n\tdebugfs_create_file(\"clk_summary\", 0444, rootdir, &all_lists,\n\t\t\t    &clk_summary_fops);\n\tdebugfs_create_file(\"clk_dump\", 0444, rootdir, &all_lists,\n\t\t\t    &clk_dump_fops);\n\tdebugfs_create_file(\"clk_orphan_summary\", 0444, rootdir, &orphan_list,\n\t\t\t    &clk_summary_fops);\n\tdebugfs_create_file(\"clk_orphan_dump\", 0444, rootdir, &orphan_list,\n\t\t\t    &clk_dump_fops);\n\n\tmutex_lock(&clk_debug_lock);\n\thlist_for_each_entry(core, &clk_debug_list, debug_node)\n\t\tclk_debug_create_one(core, rootdir);\n\n\tinited = 1;\n\tmutex_unlock(&clk_debug_lock);\n\n\treturn 0;\n}\nlate_initcall(clk_debug_init);\n#else\nstatic inline void clk_debug_register(struct clk_core *core) { }\nstatic inline void clk_debug_unregister(struct clk_core *core)\n{\n}\n#endif\n\nstatic void clk_core_reparent_orphans_nolock(void)\n{\n\tstruct clk_core *orphan;\n\tstruct hlist_node *tmp2;\n\n\t \n\thlist_for_each_entry_safe(orphan, tmp2, &clk_orphan_list, child_node) {\n\t\tstruct clk_core *parent = __clk_init_parent(orphan);\n\n\t\t \n\t\tif (parent) {\n\t\t\t \n\t\t\t__clk_set_parent_before(orphan, parent);\n\t\t\t__clk_set_parent_after(orphan, parent, NULL);\n\t\t\t__clk_recalc_accuracies(orphan);\n\t\t\t__clk_recalc_rates(orphan, true, 0);\n\n\t\t\t \n\t\t\torphan->req_rate = orphan->rate;\n\t\t}\n\t}\n}\n\n \nstatic int __clk_core_init(struct clk_core *core)\n{\n\tint ret;\n\tstruct clk_core *parent;\n\tunsigned long rate;\n\tint phase;\n\n\tclk_prepare_lock();\n\n\t \n\tcore->hw->core = core;\n\n\tret = clk_pm_runtime_get(core);\n\tif (ret)\n\t\tgoto unlock;\n\n\t \n\tif (clk_core_lookup(core->name)) {\n\t\tpr_debug(\"%s: clk %s already initialized\\n\",\n\t\t\t\t__func__, core->name);\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\t \n\tif (core->ops->set_rate &&\n\t    !((core->ops->round_rate || core->ops->determine_rate) &&\n\t      core->ops->recalc_rate)) {\n\t\tpr_err(\"%s: %s must implement .round_rate or .determine_rate in addition to .recalc_rate\\n\",\n\t\t       __func__, core->name);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (core->ops->set_parent && !core->ops->get_parent) {\n\t\tpr_err(\"%s: %s must implement .get_parent & .set_parent\\n\",\n\t\t       __func__, core->name);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (core->ops->set_parent && !core->ops->determine_rate) {\n\t\tpr_err(\"%s: %s must implement .set_parent & .determine_rate\\n\",\n\t\t\t__func__, core->name);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (core->num_parents > 1 && !core->ops->get_parent) {\n\t\tpr_err(\"%s: %s must implement .get_parent as it has multi parents\\n\",\n\t\t       __func__, core->name);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (core->ops->set_rate_and_parent &&\n\t\t\t!(core->ops->set_parent && core->ops->set_rate)) {\n\t\tpr_err(\"%s: %s must implement .set_parent & .set_rate\\n\",\n\t\t\t\t__func__, core->name);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (core->ops->init) {\n\t\tret = core->ops->init(core->hw);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tparent = core->parent = __clk_init_parent(core);\n\n\t \n\tif (parent) {\n\t\thlist_add_head(&core->child_node, &parent->children);\n\t\tcore->orphan = parent->orphan;\n\t} else if (!core->num_parents) {\n\t\thlist_add_head(&core->child_node, &clk_root_list);\n\t\tcore->orphan = false;\n\t} else {\n\t\thlist_add_head(&core->child_node, &clk_orphan_list);\n\t\tcore->orphan = true;\n\t}\n\n\t \n\tif (core->ops->recalc_accuracy)\n\t\tcore->accuracy = core->ops->recalc_accuracy(core->hw,\n\t\t\t\t\tclk_core_get_accuracy_no_lock(parent));\n\telse if (parent)\n\t\tcore->accuracy = parent->accuracy;\n\telse\n\t\tcore->accuracy = 0;\n\n\t \n\tphase = clk_core_get_phase(core);\n\tif (phase < 0) {\n\t\tret = phase;\n\t\tpr_warn(\"%s: Failed to get phase for clk '%s'\\n\", __func__,\n\t\t\tcore->name);\n\t\tgoto out;\n\t}\n\n\t \n\tclk_core_update_duty_cycle_nolock(core);\n\n\t \n\tif (core->ops->recalc_rate)\n\t\trate = core->ops->recalc_rate(core->hw,\n\t\t\t\tclk_core_get_rate_nolock(parent));\n\telse if (parent)\n\t\trate = parent->rate;\n\telse\n\t\trate = 0;\n\tcore->rate = core->req_rate = rate;\n\n\t \n\tif (core->flags & CLK_IS_CRITICAL) {\n\t\tret = clk_core_prepare(core);\n\t\tif (ret) {\n\t\t\tpr_warn(\"%s: critical clk '%s' failed to prepare\\n\",\n\t\t\t       __func__, core->name);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = clk_core_enable_lock(core);\n\t\tif (ret) {\n\t\t\tpr_warn(\"%s: critical clk '%s' failed to enable\\n\",\n\t\t\t       __func__, core->name);\n\t\t\tclk_core_unprepare(core);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tclk_core_reparent_orphans_nolock();\n\n\tkref_init(&core->ref);\nout:\n\tclk_pm_runtime_put(core);\nunlock:\n\tif (ret) {\n\t\thlist_del_init(&core->child_node);\n\t\tcore->hw->core = NULL;\n\t}\n\n\tclk_prepare_unlock();\n\n\tif (!ret)\n\t\tclk_debug_register(core);\n\n\treturn ret;\n}\n\n \nstatic void clk_core_link_consumer(struct clk_core *core, struct clk *clk)\n{\n\tclk_prepare_lock();\n\thlist_add_head(&clk->clks_node, &core->clks);\n\tclk_prepare_unlock();\n}\n\n \nstatic void clk_core_unlink_consumer(struct clk *clk)\n{\n\tlockdep_assert_held(&prepare_lock);\n\thlist_del(&clk->clks_node);\n}\n\n \nstatic struct clk *alloc_clk(struct clk_core *core, const char *dev_id,\n\t\t\t     const char *con_id)\n{\n\tstruct clk *clk;\n\n\tclk = kzalloc(sizeof(*clk), GFP_KERNEL);\n\tif (!clk)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclk->core = core;\n\tclk->dev_id = dev_id;\n\tclk->con_id = kstrdup_const(con_id, GFP_KERNEL);\n\tclk->max_rate = ULONG_MAX;\n\n\treturn clk;\n}\n\n \nstatic void free_clk(struct clk *clk)\n{\n\tkfree_const(clk->con_id);\n\tkfree(clk);\n}\n\n \nstruct clk *clk_hw_create_clk(struct device *dev, struct clk_hw *hw,\n\t\t\t      const char *dev_id, const char *con_id)\n{\n\tstruct clk *clk;\n\tstruct clk_core *core;\n\n\t \n\tif (IS_ERR_OR_NULL(hw))\n\t\treturn ERR_CAST(hw);\n\n\tcore = hw->core;\n\tclk = alloc_clk(core, dev_id, con_id);\n\tif (IS_ERR(clk))\n\t\treturn clk;\n\tclk->dev = dev;\n\n\tif (!try_module_get(core->owner)) {\n\t\tfree_clk(clk);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\tkref_get(&core->ref);\n\tclk_core_link_consumer(core, clk);\n\n\treturn clk;\n}\n\n \nstruct clk *clk_hw_get_clk(struct clk_hw *hw, const char *con_id)\n{\n\tstruct device *dev = hw->core->dev;\n\tconst char *name = dev ? dev_name(dev) : NULL;\n\n\treturn clk_hw_create_clk(dev, hw, name, con_id);\n}\nEXPORT_SYMBOL(clk_hw_get_clk);\n\nstatic int clk_cpy_name(const char **dst_p, const char *src, bool must_exist)\n{\n\tconst char *dst;\n\n\tif (!src) {\n\t\tif (must_exist)\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\t*dst_p = dst = kstrdup_const(src, GFP_KERNEL);\n\tif (!dst)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int clk_core_populate_parent_map(struct clk_core *core,\n\t\t\t\t\tconst struct clk_init_data *init)\n{\n\tu8 num_parents = init->num_parents;\n\tconst char * const *parent_names = init->parent_names;\n\tconst struct clk_hw **parent_hws = init->parent_hws;\n\tconst struct clk_parent_data *parent_data = init->parent_data;\n\tint i, ret = 0;\n\tstruct clk_parent_map *parents, *parent;\n\n\tif (!num_parents)\n\t\treturn 0;\n\n\t \n\tparents = kcalloc(num_parents, sizeof(*parents), GFP_KERNEL);\n\tcore->parents = parents;\n\tif (!parents)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0, parent = parents; i < num_parents; i++, parent++) {\n\t\tparent->index = -1;\n\t\tif (parent_names) {\n\t\t\t \n\t\t\tWARN(!parent_names[i],\n\t\t\t\t\"%s: invalid NULL in %s's .parent_names\\n\",\n\t\t\t\t__func__, core->name);\n\t\t\tret = clk_cpy_name(&parent->name, parent_names[i],\n\t\t\t\t\t   true);\n\t\t} else if (parent_data) {\n\t\t\tparent->hw = parent_data[i].hw;\n\t\t\tparent->index = parent_data[i].index;\n\t\t\tret = clk_cpy_name(&parent->fw_name,\n\t\t\t\t\t   parent_data[i].fw_name, false);\n\t\t\tif (!ret)\n\t\t\t\tret = clk_cpy_name(&parent->name,\n\t\t\t\t\t\t   parent_data[i].name,\n\t\t\t\t\t\t   false);\n\t\t} else if (parent_hws) {\n\t\t\tparent->hw = parent_hws[i];\n\t\t} else {\n\t\t\tret = -EINVAL;\n\t\t\tWARN(1, \"Must specify parents if num_parents > 0\\n\");\n\t\t}\n\n\t\tif (ret) {\n\t\t\tdo {\n\t\t\t\tkfree_const(parents[i].name);\n\t\t\t\tkfree_const(parents[i].fw_name);\n\t\t\t} while (--i >= 0);\n\t\t\tkfree(parents);\n\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void clk_core_free_parent_map(struct clk_core *core)\n{\n\tint i = core->num_parents;\n\n\tif (!core->num_parents)\n\t\treturn;\n\n\twhile (--i >= 0) {\n\t\tkfree_const(core->parents[i].name);\n\t\tkfree_const(core->parents[i].fw_name);\n\t}\n\n\tkfree(core->parents);\n}\n\nstatic struct clk *\n__clk_register(struct device *dev, struct device_node *np, struct clk_hw *hw)\n{\n\tint ret;\n\tstruct clk_core *core;\n\tconst struct clk_init_data *init = hw->init;\n\n\t \n\thw->init = NULL;\n\n\tcore = kzalloc(sizeof(*core), GFP_KERNEL);\n\tif (!core) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_out;\n\t}\n\n\tcore->name = kstrdup_const(init->name, GFP_KERNEL);\n\tif (!core->name) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_name;\n\t}\n\n\tif (WARN_ON(!init->ops)) {\n\t\tret = -EINVAL;\n\t\tgoto fail_ops;\n\t}\n\tcore->ops = init->ops;\n\n\tif (dev && pm_runtime_enabled(dev))\n\t\tcore->rpm_enabled = true;\n\tcore->dev = dev;\n\tcore->of_node = np;\n\tif (dev && dev->driver)\n\t\tcore->owner = dev->driver->owner;\n\tcore->hw = hw;\n\tcore->flags = init->flags;\n\tcore->num_parents = init->num_parents;\n\tcore->min_rate = 0;\n\tcore->max_rate = ULONG_MAX;\n\n\tret = clk_core_populate_parent_map(core, init);\n\tif (ret)\n\t\tgoto fail_parents;\n\n\tINIT_HLIST_HEAD(&core->clks);\n\n\t \n\thw->clk = alloc_clk(core, NULL, NULL);\n\tif (IS_ERR(hw->clk)) {\n\t\tret = PTR_ERR(hw->clk);\n\t\tgoto fail_create_clk;\n\t}\n\n\tclk_core_link_consumer(core, hw->clk);\n\n\tret = __clk_core_init(core);\n\tif (!ret)\n\t\treturn hw->clk;\n\n\tclk_prepare_lock();\n\tclk_core_unlink_consumer(hw->clk);\n\tclk_prepare_unlock();\n\n\tfree_clk(hw->clk);\n\thw->clk = NULL;\n\nfail_create_clk:\n\tclk_core_free_parent_map(core);\nfail_parents:\nfail_ops:\n\tkfree_const(core->name);\nfail_name:\n\tkfree(core);\nfail_out:\n\treturn ERR_PTR(ret);\n}\n\n \nstatic struct device_node *dev_or_parent_of_node(struct device *dev)\n{\n\tstruct device_node *np;\n\n\tif (!dev)\n\t\treturn NULL;\n\n\tnp = dev_of_node(dev);\n\tif (!np)\n\t\tnp = dev_of_node(dev->parent);\n\n\treturn np;\n}\n\n \nstruct clk *clk_register(struct device *dev, struct clk_hw *hw)\n{\n\treturn __clk_register(dev, dev_or_parent_of_node(dev), hw);\n}\nEXPORT_SYMBOL_GPL(clk_register);\n\n \nint clk_hw_register(struct device *dev, struct clk_hw *hw)\n{\n\treturn PTR_ERR_OR_ZERO(__clk_register(dev, dev_or_parent_of_node(dev),\n\t\t\t       hw));\n}\nEXPORT_SYMBOL_GPL(clk_hw_register);\n\n \nint of_clk_hw_register(struct device_node *node, struct clk_hw *hw)\n{\n\treturn PTR_ERR_OR_ZERO(__clk_register(NULL, node, hw));\n}\nEXPORT_SYMBOL_GPL(of_clk_hw_register);\n\n \nstatic void __clk_release(struct kref *ref)\n{\n\tstruct clk_core *core = container_of(ref, struct clk_core, ref);\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tclk_core_free_parent_map(core);\n\tkfree_const(core->name);\n\tkfree(core);\n}\n\n \nstatic int clk_nodrv_prepare_enable(struct clk_hw *hw)\n{\n\treturn -ENXIO;\n}\n\nstatic void clk_nodrv_disable_unprepare(struct clk_hw *hw)\n{\n\tWARN_ON_ONCE(1);\n}\n\nstatic int clk_nodrv_set_rate(struct clk_hw *hw, unsigned long rate,\n\t\t\t\t\tunsigned long parent_rate)\n{\n\treturn -ENXIO;\n}\n\nstatic int clk_nodrv_set_parent(struct clk_hw *hw, u8 index)\n{\n\treturn -ENXIO;\n}\n\nstatic int clk_nodrv_determine_rate(struct clk_hw *hw,\n\t\t\t\t    struct clk_rate_request *req)\n{\n\treturn -ENXIO;\n}\n\nstatic const struct clk_ops clk_nodrv_ops = {\n\t.enable\t\t= clk_nodrv_prepare_enable,\n\t.disable\t= clk_nodrv_disable_unprepare,\n\t.prepare\t= clk_nodrv_prepare_enable,\n\t.unprepare\t= clk_nodrv_disable_unprepare,\n\t.determine_rate\t= clk_nodrv_determine_rate,\n\t.set_rate\t= clk_nodrv_set_rate,\n\t.set_parent\t= clk_nodrv_set_parent,\n};\n\nstatic void clk_core_evict_parent_cache_subtree(struct clk_core *root,\n\t\t\t\t\t\tconst struct clk_core *target)\n{\n\tint i;\n\tstruct clk_core *child;\n\n\tfor (i = 0; i < root->num_parents; i++)\n\t\tif (root->parents[i].core == target)\n\t\t\troot->parents[i].core = NULL;\n\n\thlist_for_each_entry(child, &root->children, child_node)\n\t\tclk_core_evict_parent_cache_subtree(child, target);\n}\n\n \nstatic void clk_core_evict_parent_cache(struct clk_core *core)\n{\n\tconst struct hlist_head **lists;\n\tstruct clk_core *root;\n\n\tlockdep_assert_held(&prepare_lock);\n\n\tfor (lists = all_lists; *lists; lists++)\n\t\thlist_for_each_entry(root, *lists, child_node)\n\t\t\tclk_core_evict_parent_cache_subtree(root, core);\n\n}\n\n \nvoid clk_unregister(struct clk *clk)\n{\n\tunsigned long flags;\n\tconst struct clk_ops *ops;\n\n\tif (!clk || WARN_ON_ONCE(IS_ERR(clk)))\n\t\treturn;\n\n\tclk_debug_unregister(clk->core);\n\n\tclk_prepare_lock();\n\n\tops = clk->core->ops;\n\tif (ops == &clk_nodrv_ops) {\n\t\tpr_err(\"%s: unregistered clock: %s\\n\", __func__,\n\t\t       clk->core->name);\n\t\tgoto unlock;\n\t}\n\t \n\tflags = clk_enable_lock();\n\tclk->core->ops = &clk_nodrv_ops;\n\tclk_enable_unlock(flags);\n\n\tif (ops->terminate)\n\t\tops->terminate(clk->core->hw);\n\n\tif (!hlist_empty(&clk->core->children)) {\n\t\tstruct clk_core *child;\n\t\tstruct hlist_node *t;\n\n\t\t \n\t\thlist_for_each_entry_safe(child, t, &clk->core->children,\n\t\t\t\t\t  child_node)\n\t\t\tclk_core_set_parent_nolock(child, NULL);\n\t}\n\n\tclk_core_evict_parent_cache(clk->core);\n\n\thlist_del_init(&clk->core->child_node);\n\n\tif (clk->core->prepare_count)\n\t\tpr_warn(\"%s: unregistering prepared clock: %s\\n\",\n\t\t\t\t\t__func__, clk->core->name);\n\n\tif (clk->core->protect_count)\n\t\tpr_warn(\"%s: unregistering protected clock: %s\\n\",\n\t\t\t\t\t__func__, clk->core->name);\n\n\tkref_put(&clk->core->ref, __clk_release);\n\tfree_clk(clk);\nunlock:\n\tclk_prepare_unlock();\n}\nEXPORT_SYMBOL_GPL(clk_unregister);\n\n \nvoid clk_hw_unregister(struct clk_hw *hw)\n{\n\tclk_unregister(hw->clk);\n}\nEXPORT_SYMBOL_GPL(clk_hw_unregister);\n\nstatic void devm_clk_unregister_cb(struct device *dev, void *res)\n{\n\tclk_unregister(*(struct clk **)res);\n}\n\nstatic void devm_clk_hw_unregister_cb(struct device *dev, void *res)\n{\n\tclk_hw_unregister(*(struct clk_hw **)res);\n}\n\n \nstruct clk *devm_clk_register(struct device *dev, struct clk_hw *hw)\n{\n\tstruct clk *clk;\n\tstruct clk **clkp;\n\n\tclkp = devres_alloc(devm_clk_unregister_cb, sizeof(*clkp), GFP_KERNEL);\n\tif (!clkp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclk = clk_register(dev, hw);\n\tif (!IS_ERR(clk)) {\n\t\t*clkp = clk;\n\t\tdevres_add(dev, clkp);\n\t} else {\n\t\tdevres_free(clkp);\n\t}\n\n\treturn clk;\n}\nEXPORT_SYMBOL_GPL(devm_clk_register);\n\n \nint devm_clk_hw_register(struct device *dev, struct clk_hw *hw)\n{\n\tstruct clk_hw **hwp;\n\tint ret;\n\n\thwp = devres_alloc(devm_clk_hw_unregister_cb, sizeof(*hwp), GFP_KERNEL);\n\tif (!hwp)\n\t\treturn -ENOMEM;\n\n\tret = clk_hw_register(dev, hw);\n\tif (!ret) {\n\t\t*hwp = hw;\n\t\tdevres_add(dev, hwp);\n\t} else {\n\t\tdevres_free(hwp);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(devm_clk_hw_register);\n\nstatic void devm_clk_release(struct device *dev, void *res)\n{\n\tclk_put(*(struct clk **)res);\n}\n\n \nstruct clk *devm_clk_hw_get_clk(struct device *dev, struct clk_hw *hw,\n\t\t\t\tconst char *con_id)\n{\n\tstruct clk *clk;\n\tstruct clk **clkp;\n\n\t \n\tWARN_ON_ONCE(dev != hw->core->dev);\n\n\tclkp = devres_alloc(devm_clk_release, sizeof(*clkp), GFP_KERNEL);\n\tif (!clkp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tclk = clk_hw_get_clk(hw, con_id);\n\tif (!IS_ERR(clk)) {\n\t\t*clkp = clk;\n\t\tdevres_add(dev, clkp);\n\t} else {\n\t\tdevres_free(clkp);\n\t}\n\n\treturn clk;\n}\nEXPORT_SYMBOL_GPL(devm_clk_hw_get_clk);\n\n \n\nvoid __clk_put(struct clk *clk)\n{\n\tstruct module *owner;\n\n\tif (!clk || WARN_ON_ONCE(IS_ERR(clk)))\n\t\treturn;\n\n\tclk_prepare_lock();\n\n\t \n\tif (WARN_ON(clk->exclusive_count)) {\n\t\t \n\t\tclk->core->protect_count -= (clk->exclusive_count - 1);\n\t\tclk_core_rate_unprotect(clk->core);\n\t\tclk->exclusive_count = 0;\n\t}\n\n\thlist_del(&clk->clks_node);\n\n\t \n\tif (clk->min_rate > 0 || clk->max_rate < ULONG_MAX)\n\t\tclk_set_rate_range_nolock(clk, 0, ULONG_MAX);\n\n\towner = clk->core->owner;\n\tkref_put(&clk->core->ref, __clk_release);\n\n\tclk_prepare_unlock();\n\n\tmodule_put(owner);\n\n\tfree_clk(clk);\n}\n\n \n\n \nint clk_notifier_register(struct clk *clk, struct notifier_block *nb)\n{\n\tstruct clk_notifier *cn;\n\tint ret = -ENOMEM;\n\n\tif (!clk || !nb)\n\t\treturn -EINVAL;\n\n\tclk_prepare_lock();\n\n\t \n\tlist_for_each_entry(cn, &clk_notifier_list, node)\n\t\tif (cn->clk == clk)\n\t\t\tgoto found;\n\n\t \n\tcn = kzalloc(sizeof(*cn), GFP_KERNEL);\n\tif (!cn)\n\t\tgoto out;\n\n\tcn->clk = clk;\n\tsrcu_init_notifier_head(&cn->notifier_head);\n\n\tlist_add(&cn->node, &clk_notifier_list);\n\nfound:\n\tret = srcu_notifier_chain_register(&cn->notifier_head, nb);\n\n\tclk->core->notifier_count++;\n\nout:\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_notifier_register);\n\n \nint clk_notifier_unregister(struct clk *clk, struct notifier_block *nb)\n{\n\tstruct clk_notifier *cn;\n\tint ret = -ENOENT;\n\n\tif (!clk || !nb)\n\t\treturn -EINVAL;\n\n\tclk_prepare_lock();\n\n\tlist_for_each_entry(cn, &clk_notifier_list, node) {\n\t\tif (cn->clk == clk) {\n\t\t\tret = srcu_notifier_chain_unregister(&cn->notifier_head, nb);\n\n\t\t\tclk->core->notifier_count--;\n\n\t\t\t \n\t\t\tif (!cn->notifier_head.head) {\n\t\t\t\tsrcu_cleanup_notifier_head(&cn->notifier_head);\n\t\t\t\tlist_del(&cn->node);\n\t\t\t\tkfree(cn);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tclk_prepare_unlock();\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(clk_notifier_unregister);\n\nstruct clk_notifier_devres {\n\tstruct clk *clk;\n\tstruct notifier_block *nb;\n};\n\nstatic void devm_clk_notifier_release(struct device *dev, void *res)\n{\n\tstruct clk_notifier_devres *devres = res;\n\n\tclk_notifier_unregister(devres->clk, devres->nb);\n}\n\nint devm_clk_notifier_register(struct device *dev, struct clk *clk,\n\t\t\t       struct notifier_block *nb)\n{\n\tstruct clk_notifier_devres *devres;\n\tint ret;\n\n\tdevres = devres_alloc(devm_clk_notifier_release,\n\t\t\t      sizeof(*devres), GFP_KERNEL);\n\n\tif (!devres)\n\t\treturn -ENOMEM;\n\n\tret = clk_notifier_register(clk, nb);\n\tif (!ret) {\n\t\tdevres->clk = clk;\n\t\tdevres->nb = nb;\n\t\tdevres_add(dev, devres);\n\t} else {\n\t\tdevres_free(devres);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(devm_clk_notifier_register);\n\n#ifdef CONFIG_OF\nstatic void clk_core_reparent_orphans(void)\n{\n\tclk_prepare_lock();\n\tclk_core_reparent_orphans_nolock();\n\tclk_prepare_unlock();\n}\n\n \nstruct of_clk_provider {\n\tstruct list_head link;\n\n\tstruct device_node *node;\n\tstruct clk *(*get)(struct of_phandle_args *clkspec, void *data);\n\tstruct clk_hw *(*get_hw)(struct of_phandle_args *clkspec, void *data);\n\tvoid *data;\n};\n\nextern struct of_device_id __clk_of_table;\nstatic const struct of_device_id __clk_of_table_sentinel\n\t__used __section(\"__clk_of_table_end\");\n\nstatic LIST_HEAD(of_clk_providers);\nstatic DEFINE_MUTEX(of_clk_mutex);\n\nstruct clk *of_clk_src_simple_get(struct of_phandle_args *clkspec,\n\t\t\t\t     void *data)\n{\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(of_clk_src_simple_get);\n\nstruct clk_hw *of_clk_hw_simple_get(struct of_phandle_args *clkspec, void *data)\n{\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(of_clk_hw_simple_get);\n\nstruct clk *of_clk_src_onecell_get(struct of_phandle_args *clkspec, void *data)\n{\n\tstruct clk_onecell_data *clk_data = data;\n\tunsigned int idx = clkspec->args[0];\n\n\tif (idx >= clk_data->clk_num) {\n\t\tpr_err(\"%s: invalid clock index %u\\n\", __func__, idx);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn clk_data->clks[idx];\n}\nEXPORT_SYMBOL_GPL(of_clk_src_onecell_get);\n\nstruct clk_hw *\nof_clk_hw_onecell_get(struct of_phandle_args *clkspec, void *data)\n{\n\tstruct clk_hw_onecell_data *hw_data = data;\n\tunsigned int idx = clkspec->args[0];\n\n\tif (idx >= hw_data->num) {\n\t\tpr_err(\"%s: invalid index %u\\n\", __func__, idx);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn hw_data->hws[idx];\n}\nEXPORT_SYMBOL_GPL(of_clk_hw_onecell_get);\n\n \nint of_clk_add_provider(struct device_node *np,\n\t\t\tstruct clk *(*clk_src_get)(struct of_phandle_args *clkspec,\n\t\t\t\t\t\t   void *data),\n\t\t\tvoid *data)\n{\n\tstruct of_clk_provider *cp;\n\tint ret;\n\n\tif (!np)\n\t\treturn 0;\n\n\tcp = kzalloc(sizeof(*cp), GFP_KERNEL);\n\tif (!cp)\n\t\treturn -ENOMEM;\n\n\tcp->node = of_node_get(np);\n\tcp->data = data;\n\tcp->get = clk_src_get;\n\n\tmutex_lock(&of_clk_mutex);\n\tlist_add(&cp->link, &of_clk_providers);\n\tmutex_unlock(&of_clk_mutex);\n\tpr_debug(\"Added clock from %pOF\\n\", np);\n\n\tclk_core_reparent_orphans();\n\n\tret = of_clk_set_defaults(np, true);\n\tif (ret < 0)\n\t\tof_clk_del_provider(np);\n\n\tfwnode_dev_initialized(&np->fwnode, true);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(of_clk_add_provider);\n\n \nint of_clk_add_hw_provider(struct device_node *np,\n\t\t\t   struct clk_hw *(*get)(struct of_phandle_args *clkspec,\n\t\t\t\t\t\t void *data),\n\t\t\t   void *data)\n{\n\tstruct of_clk_provider *cp;\n\tint ret;\n\n\tif (!np)\n\t\treturn 0;\n\n\tcp = kzalloc(sizeof(*cp), GFP_KERNEL);\n\tif (!cp)\n\t\treturn -ENOMEM;\n\n\tcp->node = of_node_get(np);\n\tcp->data = data;\n\tcp->get_hw = get;\n\n\tmutex_lock(&of_clk_mutex);\n\tlist_add(&cp->link, &of_clk_providers);\n\tmutex_unlock(&of_clk_mutex);\n\tpr_debug(\"Added clk_hw provider from %pOF\\n\", np);\n\n\tclk_core_reparent_orphans();\n\n\tret = of_clk_set_defaults(np, true);\n\tif (ret < 0)\n\t\tof_clk_del_provider(np);\n\n\tfwnode_dev_initialized(&np->fwnode, true);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(of_clk_add_hw_provider);\n\nstatic void devm_of_clk_release_provider(struct device *dev, void *res)\n{\n\tof_clk_del_provider(*(struct device_node **)res);\n}\n\n \nstatic struct device_node *get_clk_provider_node(struct device *dev)\n{\n\tstruct device_node *np, *parent_np;\n\n\tnp = dev->of_node;\n\tparent_np = dev->parent ? dev->parent->of_node : NULL;\n\n\tif (!of_property_present(np, \"#clock-cells\"))\n\t\tif (of_property_present(parent_np, \"#clock-cells\"))\n\t\t\tnp = parent_np;\n\n\treturn np;\n}\n\n \nint devm_of_clk_add_hw_provider(struct device *dev,\n\t\t\tstruct clk_hw *(*get)(struct of_phandle_args *clkspec,\n\t\t\t\t\t      void *data),\n\t\t\tvoid *data)\n{\n\tstruct device_node **ptr, *np;\n\tint ret;\n\n\tptr = devres_alloc(devm_of_clk_release_provider, sizeof(*ptr),\n\t\t\t   GFP_KERNEL);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\tnp = get_clk_provider_node(dev);\n\tret = of_clk_add_hw_provider(np, get, data);\n\tif (!ret) {\n\t\t*ptr = np;\n\t\tdevres_add(dev, ptr);\n\t} else {\n\t\tdevres_free(ptr);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(devm_of_clk_add_hw_provider);\n\n \nvoid of_clk_del_provider(struct device_node *np)\n{\n\tstruct of_clk_provider *cp;\n\n\tif (!np)\n\t\treturn;\n\n\tmutex_lock(&of_clk_mutex);\n\tlist_for_each_entry(cp, &of_clk_providers, link) {\n\t\tif (cp->node == np) {\n\t\t\tlist_del(&cp->link);\n\t\t\tfwnode_dev_initialized(&np->fwnode, false);\n\t\t\tof_node_put(cp->node);\n\t\t\tkfree(cp);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&of_clk_mutex);\n}\nEXPORT_SYMBOL_GPL(of_clk_del_provider);\n\n \nstatic int of_parse_clkspec(const struct device_node *np, int index,\n\t\t\t    const char *name, struct of_phandle_args *out_args)\n{\n\tint ret = -ENOENT;\n\n\t \n\twhile (np) {\n\t\t \n\t\tif (name)\n\t\t\tindex = of_property_match_string(np, \"clock-names\", name);\n\t\tret = of_parse_phandle_with_args(np, \"clocks\", \"#clock-cells\",\n\t\t\t\t\t\t index, out_args);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\tif (name && index >= 0)\n\t\t\tbreak;\n\n\t\t \n\t\tnp = np->parent;\n\t\tif (np && !of_get_property(np, \"clock-ranges\", NULL))\n\t\t\tbreak;\n\t\tindex = 0;\n\t}\n\n\treturn ret;\n}\n\nstatic struct clk_hw *\n__of_clk_get_hw_from_provider(struct of_clk_provider *provider,\n\t\t\t      struct of_phandle_args *clkspec)\n{\n\tstruct clk *clk;\n\n\tif (provider->get_hw)\n\t\treturn provider->get_hw(clkspec, provider->data);\n\n\tclk = provider->get(clkspec, provider->data);\n\tif (IS_ERR(clk))\n\t\treturn ERR_CAST(clk);\n\treturn __clk_get_hw(clk);\n}\n\nstatic struct clk_hw *\nof_clk_get_hw_from_clkspec(struct of_phandle_args *clkspec)\n{\n\tstruct of_clk_provider *provider;\n\tstruct clk_hw *hw = ERR_PTR(-EPROBE_DEFER);\n\n\tif (!clkspec)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmutex_lock(&of_clk_mutex);\n\tlist_for_each_entry(provider, &of_clk_providers, link) {\n\t\tif (provider->node == clkspec->np) {\n\t\t\thw = __of_clk_get_hw_from_provider(provider, clkspec);\n\t\t\tif (!IS_ERR(hw))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&of_clk_mutex);\n\n\treturn hw;\n}\n\n \nstruct clk *of_clk_get_from_provider(struct of_phandle_args *clkspec)\n{\n\tstruct clk_hw *hw = of_clk_get_hw_from_clkspec(clkspec);\n\n\treturn clk_hw_create_clk(NULL, hw, NULL, __func__);\n}\nEXPORT_SYMBOL_GPL(of_clk_get_from_provider);\n\nstruct clk_hw *of_clk_get_hw(struct device_node *np, int index,\n\t\t\t     const char *con_id)\n{\n\tint ret;\n\tstruct clk_hw *hw;\n\tstruct of_phandle_args clkspec;\n\n\tret = of_parse_clkspec(np, index, con_id, &clkspec);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\thw = of_clk_get_hw_from_clkspec(&clkspec);\n\tof_node_put(clkspec.np);\n\n\treturn hw;\n}\n\nstatic struct clk *__of_clk_get(struct device_node *np,\n\t\t\t\tint index, const char *dev_id,\n\t\t\t\tconst char *con_id)\n{\n\tstruct clk_hw *hw = of_clk_get_hw(np, index, con_id);\n\n\treturn clk_hw_create_clk(NULL, hw, dev_id, con_id);\n}\n\nstruct clk *of_clk_get(struct device_node *np, int index)\n{\n\treturn __of_clk_get(np, index, np->full_name, NULL);\n}\nEXPORT_SYMBOL(of_clk_get);\n\n \nstruct clk *of_clk_get_by_name(struct device_node *np, const char *name)\n{\n\tif (!np)\n\t\treturn ERR_PTR(-ENOENT);\n\n\treturn __of_clk_get(np, 0, np->full_name, name);\n}\nEXPORT_SYMBOL(of_clk_get_by_name);\n\n \nunsigned int of_clk_get_parent_count(const struct device_node *np)\n{\n\tint count;\n\n\tcount = of_count_phandle_with_args(np, \"clocks\", \"#clock-cells\");\n\tif (count < 0)\n\t\treturn 0;\n\n\treturn count;\n}\nEXPORT_SYMBOL_GPL(of_clk_get_parent_count);\n\nconst char *of_clk_get_parent_name(const struct device_node *np, int index)\n{\n\tstruct of_phandle_args clkspec;\n\tstruct property *prop;\n\tconst char *clk_name;\n\tconst __be32 *vp;\n\tu32 pv;\n\tint rc;\n\tint count;\n\tstruct clk *clk;\n\n\trc = of_parse_phandle_with_args(np, \"clocks\", \"#clock-cells\", index,\n\t\t\t\t\t&clkspec);\n\tif (rc)\n\t\treturn NULL;\n\n\tindex = clkspec.args_count ? clkspec.args[0] : 0;\n\tcount = 0;\n\n\t \n\tof_property_for_each_u32(clkspec.np, \"clock-indices\", prop, vp, pv) {\n\t\tif (index == pv) {\n\t\t\tindex = count;\n\t\t\tbreak;\n\t\t}\n\t\tcount++;\n\t}\n\t \n\tif (prop && !vp)\n\t\treturn NULL;\n\n\tif (of_property_read_string_index(clkspec.np, \"clock-output-names\",\n\t\t\t\t\t  index,\n\t\t\t\t\t  &clk_name) < 0) {\n\t\t \n\t\tclk = of_clk_get_from_provider(&clkspec);\n\t\tif (IS_ERR(clk)) {\n\t\t\tif (clkspec.args_count == 0)\n\t\t\t\tclk_name = clkspec.np->name;\n\t\t\telse\n\t\t\t\tclk_name = NULL;\n\t\t} else {\n\t\t\tclk_name = __clk_get_name(clk);\n\t\t\tclk_put(clk);\n\t\t}\n\t}\n\n\n\tof_node_put(clkspec.np);\n\treturn clk_name;\n}\nEXPORT_SYMBOL_GPL(of_clk_get_parent_name);\n\n \nint of_clk_parent_fill(struct device_node *np, const char **parents,\n\t\t       unsigned int size)\n{\n\tunsigned int i = 0;\n\n\twhile (i < size && (parents[i] = of_clk_get_parent_name(np, i)) != NULL)\n\t\ti++;\n\n\treturn i;\n}\nEXPORT_SYMBOL_GPL(of_clk_parent_fill);\n\nstruct clock_provider {\n\tvoid (*clk_init_cb)(struct device_node *);\n\tstruct device_node *np;\n\tstruct list_head node;\n};\n\n \nstatic int parent_ready(struct device_node *np)\n{\n\tint i = 0;\n\n\twhile (true) {\n\t\tstruct clk *clk = of_clk_get(np, i);\n\n\t\t \n\t\tif (!IS_ERR(clk)) {\n\t\t\tclk_put(clk);\n\t\t\ti++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (PTR_ERR(clk) == -EPROBE_DEFER)\n\t\t\treturn 0;\n\n\t\t \n\t\treturn 1;\n\t}\n}\n\n \nint of_clk_detect_critical(struct device_node *np, int index,\n\t\t\t   unsigned long *flags)\n{\n\tstruct property *prop;\n\tconst __be32 *cur;\n\tuint32_t idx;\n\n\tif (!np || !flags)\n\t\treturn -EINVAL;\n\n\tof_property_for_each_u32(np, \"clock-critical\", prop, cur, idx)\n\t\tif (index == idx)\n\t\t\t*flags |= CLK_IS_CRITICAL;\n\n\treturn 0;\n}\n\n \nvoid __init of_clk_init(const struct of_device_id *matches)\n{\n\tconst struct of_device_id *match;\n\tstruct device_node *np;\n\tstruct clock_provider *clk_provider, *next;\n\tbool is_init_done;\n\tbool force = false;\n\tLIST_HEAD(clk_provider_list);\n\n\tif (!matches)\n\t\tmatches = &__clk_of_table;\n\n\t \n\tfor_each_matching_node_and_match(np, matches, &match) {\n\t\tstruct clock_provider *parent;\n\n\t\tif (!of_device_is_available(np))\n\t\t\tcontinue;\n\n\t\tparent = kzalloc(sizeof(*parent), GFP_KERNEL);\n\t\tif (!parent) {\n\t\t\tlist_for_each_entry_safe(clk_provider, next,\n\t\t\t\t\t\t &clk_provider_list, node) {\n\t\t\t\tlist_del(&clk_provider->node);\n\t\t\t\tof_node_put(clk_provider->np);\n\t\t\t\tkfree(clk_provider);\n\t\t\t}\n\t\t\tof_node_put(np);\n\t\t\treturn;\n\t\t}\n\n\t\tparent->clk_init_cb = match->data;\n\t\tparent->np = of_node_get(np);\n\t\tlist_add_tail(&parent->node, &clk_provider_list);\n\t}\n\n\twhile (!list_empty(&clk_provider_list)) {\n\t\tis_init_done = false;\n\t\tlist_for_each_entry_safe(clk_provider, next,\n\t\t\t\t\t&clk_provider_list, node) {\n\t\t\tif (force || parent_ready(clk_provider->np)) {\n\n\t\t\t\t \n\t\t\t\tof_node_set_flag(clk_provider->np,\n\t\t\t\t\t\t OF_POPULATED);\n\n\t\t\t\tclk_provider->clk_init_cb(clk_provider->np);\n\t\t\t\tof_clk_set_defaults(clk_provider->np, true);\n\n\t\t\t\tlist_del(&clk_provider->node);\n\t\t\t\tof_node_put(clk_provider->np);\n\t\t\t\tkfree(clk_provider);\n\t\t\t\tis_init_done = true;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!is_init_done)\n\t\t\tforce = true;\n\t}\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}