{
  "module_name": "of_reserved_mem.c",
  "hash_id": "96ccabc8296c1e9cc9a47e02e42005090b8308dc09fcd113cb209c6c27f68807",
  "original_prompt": "Ingested from linux-6.6.14/drivers/of/of_reserved_mem.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"OF: reserved mem: \" fmt\n\n#include <linux/err.h>\n#include <linux/of.h>\n#include <linux/of_fdt.h>\n#include <linux/of_platform.h>\n#include <linux/mm.h>\n#include <linux/sizes.h>\n#include <linux/of_reserved_mem.h>\n#include <linux/sort.h>\n#include <linux/slab.h>\n#include <linux/memblock.h>\n#include <linux/kmemleak.h>\n#include <linux/cma.h>\n\n#include \"of_private.h\"\n\n#define MAX_RESERVED_REGIONS\t64\nstatic struct reserved_mem reserved_mem[MAX_RESERVED_REGIONS];\nstatic int reserved_mem_count;\n\nstatic int __init early_init_dt_alloc_reserved_memory_arch(phys_addr_t size,\n\tphys_addr_t align, phys_addr_t start, phys_addr_t end, bool nomap,\n\tphys_addr_t *res_base)\n{\n\tphys_addr_t base;\n\tint err = 0;\n\n\tend = !end ? MEMBLOCK_ALLOC_ANYWHERE : end;\n\talign = !align ? SMP_CACHE_BYTES : align;\n\tbase = memblock_phys_alloc_range(size, align, start, end);\n\tif (!base)\n\t\treturn -ENOMEM;\n\n\t*res_base = base;\n\tif (nomap) {\n\t\terr = memblock_mark_nomap(base, size);\n\t\tif (err)\n\t\t\tmemblock_phys_free(base, size);\n\t}\n\n\tkmemleak_ignore_phys(base);\n\n\treturn err;\n}\n\n \nvoid __init fdt_reserved_mem_save_node(unsigned long node, const char *uname,\n\t\t\t\t      phys_addr_t base, phys_addr_t size)\n{\n\tstruct reserved_mem *rmem = &reserved_mem[reserved_mem_count];\n\n\tif (reserved_mem_count == ARRAY_SIZE(reserved_mem)) {\n\t\tpr_err(\"not enough space for all defined regions.\\n\");\n\t\treturn;\n\t}\n\n\trmem->fdt_node = node;\n\trmem->name = uname;\n\trmem->base = base;\n\trmem->size = size;\n\n\treserved_mem_count++;\n\treturn;\n}\n\n \nstatic int __init __reserved_mem_alloc_in_range(phys_addr_t size,\n\tphys_addr_t align, phys_addr_t start, phys_addr_t end, bool nomap,\n\tphys_addr_t *res_base)\n{\n\tbool prev_bottom_up = memblock_bottom_up();\n\tbool bottom_up = false, top_down = false;\n\tint ret, i;\n\n\tfor (i = 0; i < reserved_mem_count; i++) {\n\t\tstruct reserved_mem *rmem = &reserved_mem[i];\n\n\t\t \n\t\tif (rmem->size == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (start >= rmem->base && start <= (rmem->base + rmem->size))\n\t\t\tbottom_up = true;\n\n\t\t \n\t\tif (end >= rmem->base && end <= (rmem->base + rmem->size))\n\t\t\ttop_down = true;\n\t}\n\n\t \n\tif (bottom_up != top_down)\n\t\tmemblock_set_bottom_up(bottom_up);\n\n\tret = early_init_dt_alloc_reserved_memory_arch(size, align,\n\t\t\tstart, end, nomap, res_base);\n\n\t \n\tif (bottom_up != top_down)\n\t\tmemblock_set_bottom_up(prev_bottom_up);\n\n\treturn ret;\n}\n\n \nstatic int __init __reserved_mem_alloc_size(unsigned long node,\n\tconst char *uname, phys_addr_t *res_base, phys_addr_t *res_size)\n{\n\tint t_len = (dt_root_addr_cells + dt_root_size_cells) * sizeof(__be32);\n\tphys_addr_t start = 0, end = 0;\n\tphys_addr_t base = 0, align = 0, size;\n\tint len;\n\tconst __be32 *prop;\n\tbool nomap;\n\tint ret;\n\n\tprop = of_get_flat_dt_prop(node, \"size\", &len);\n\tif (!prop)\n\t\treturn -EINVAL;\n\n\tif (len != dt_root_size_cells * sizeof(__be32)) {\n\t\tpr_err(\"invalid size property in '%s' node.\\n\", uname);\n\t\treturn -EINVAL;\n\t}\n\tsize = dt_mem_next_cell(dt_root_size_cells, &prop);\n\n\tprop = of_get_flat_dt_prop(node, \"alignment\", &len);\n\tif (prop) {\n\t\tif (len != dt_root_addr_cells * sizeof(__be32)) {\n\t\t\tpr_err(\"invalid alignment property in '%s' node.\\n\",\n\t\t\t\tuname);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = dt_mem_next_cell(dt_root_addr_cells, &prop);\n\t}\n\n\tnomap = of_get_flat_dt_prop(node, \"no-map\", NULL) != NULL;\n\n\t \n\tif (IS_ENABLED(CONFIG_CMA)\n\t    && of_flat_dt_is_compatible(node, \"shared-dma-pool\")\n\t    && of_get_flat_dt_prop(node, \"reusable\", NULL)\n\t    && !nomap)\n\t\talign = max_t(phys_addr_t, align, CMA_MIN_ALIGNMENT_BYTES);\n\n\tprop = of_get_flat_dt_prop(node, \"alloc-ranges\", &len);\n\tif (prop) {\n\n\t\tif (len % t_len != 0) {\n\t\t\tpr_err(\"invalid alloc-ranges property in '%s', skipping node.\\n\",\n\t\t\t       uname);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tbase = 0;\n\n\t\twhile (len > 0) {\n\t\t\tstart = dt_mem_next_cell(dt_root_addr_cells, &prop);\n\t\t\tend = start + dt_mem_next_cell(dt_root_size_cells,\n\t\t\t\t\t\t       &prop);\n\n\t\t\tret = __reserved_mem_alloc_in_range(size, align,\n\t\t\t\t\tstart, end, nomap, &base);\n\t\t\tif (ret == 0) {\n\t\t\t\tpr_debug(\"allocated memory for '%s' node: base %pa, size %lu MiB\\n\",\n\t\t\t\t\tuname, &base,\n\t\t\t\t\t(unsigned long)(size / SZ_1M));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tlen -= t_len;\n\t\t}\n\n\t} else {\n\t\tret = early_init_dt_alloc_reserved_memory_arch(size, align,\n\t\t\t\t\t\t\t0, 0, nomap, &base);\n\t\tif (ret == 0)\n\t\t\tpr_debug(\"allocated memory for '%s' node: base %pa, size %lu MiB\\n\",\n\t\t\t\tuname, &base, (unsigned long)(size / SZ_1M));\n\t}\n\n\tif (base == 0) {\n\t\tpr_err(\"failed to allocate memory for node '%s': size %lu MiB\\n\",\n\t\t       uname, (unsigned long)(size / SZ_1M));\n\t\treturn -ENOMEM;\n\t}\n\n\t*res_base = base;\n\t*res_size = size;\n\n\treturn 0;\n}\n\nstatic const struct of_device_id __rmem_of_table_sentinel\n\t__used __section(\"__reservedmem_of_table_end\");\n\n \nstatic int __init __reserved_mem_init_node(struct reserved_mem *rmem)\n{\n\textern const struct of_device_id __reservedmem_of_table[];\n\tconst struct of_device_id *i;\n\tint ret = -ENOENT;\n\n\tfor (i = __reservedmem_of_table; i < &__rmem_of_table_sentinel; i++) {\n\t\treservedmem_of_init_fn initfn = i->data;\n\t\tconst char *compat = i->compatible;\n\n\t\tif (!of_flat_dt_is_compatible(rmem->fdt_node, compat))\n\t\t\tcontinue;\n\n\t\tret = initfn(rmem);\n\t\tif (ret == 0) {\n\t\t\tpr_info(\"initialized node %s, compatible id %s\\n\",\n\t\t\t\trmem->name, compat);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int __init __rmem_cmp(const void *a, const void *b)\n{\n\tconst struct reserved_mem *ra = a, *rb = b;\n\n\tif (ra->base < rb->base)\n\t\treturn -1;\n\n\tif (ra->base > rb->base)\n\t\treturn 1;\n\n\t \n\tif (ra->size < rb->size)\n\t\treturn -1;\n\tif (ra->size > rb->size)\n\t\treturn 1;\n\n\tif (ra->fdt_node < rb->fdt_node)\n\t\treturn -1;\n\tif (ra->fdt_node > rb->fdt_node)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void __init __rmem_check_for_overlap(void)\n{\n\tint i;\n\n\tif (reserved_mem_count < 2)\n\t\treturn;\n\n\tsort(reserved_mem, reserved_mem_count, sizeof(reserved_mem[0]),\n\t     __rmem_cmp, NULL);\n\tfor (i = 0; i < reserved_mem_count - 1; i++) {\n\t\tstruct reserved_mem *this, *next;\n\n\t\tthis = &reserved_mem[i];\n\t\tnext = &reserved_mem[i + 1];\n\n\t\tif (this->base + this->size > next->base) {\n\t\t\tphys_addr_t this_end, next_end;\n\n\t\t\tthis_end = this->base + this->size;\n\t\t\tnext_end = next->base + next->size;\n\t\t\tpr_err(\"OVERLAP DETECTED!\\n%s (%pa--%pa) overlaps with %s (%pa--%pa)\\n\",\n\t\t\t       this->name, &this->base, &this_end,\n\t\t\t       next->name, &next->base, &next_end);\n\t\t}\n\t}\n}\n\n \nvoid __init fdt_init_reserved_mem(void)\n{\n\tint i;\n\n\t \n\t__rmem_check_for_overlap();\n\n\tfor (i = 0; i < reserved_mem_count; i++) {\n\t\tstruct reserved_mem *rmem = &reserved_mem[i];\n\t\tunsigned long node = rmem->fdt_node;\n\t\tint len;\n\t\tconst __be32 *prop;\n\t\tint err = 0;\n\t\tbool nomap;\n\n\t\tnomap = of_get_flat_dt_prop(node, \"no-map\", NULL) != NULL;\n\t\tprop = of_get_flat_dt_prop(node, \"phandle\", &len);\n\t\tif (!prop)\n\t\t\tprop = of_get_flat_dt_prop(node, \"linux,phandle\", &len);\n\t\tif (prop)\n\t\t\trmem->phandle = of_read_number(prop, len/4);\n\n\t\tif (rmem->size == 0)\n\t\t\terr = __reserved_mem_alloc_size(node, rmem->name,\n\t\t\t\t\t\t &rmem->base, &rmem->size);\n\t\tif (err == 0) {\n\t\t\terr = __reserved_mem_init_node(rmem);\n\t\t\tif (err != 0 && err != -ENOENT) {\n\t\t\t\tpr_info(\"node %s compatible matching fail\\n\",\n\t\t\t\t\trmem->name);\n\t\t\t\tif (nomap)\n\t\t\t\t\tmemblock_clear_nomap(rmem->base, rmem->size);\n\t\t\t\telse\n\t\t\t\t\tmemblock_phys_free(rmem->base,\n\t\t\t\t\t\t\t   rmem->size);\n\t\t\t} else {\n\t\t\t\tphys_addr_t end = rmem->base + rmem->size - 1;\n\t\t\t\tbool reusable =\n\t\t\t\t\t(of_get_flat_dt_prop(node, \"reusable\", NULL)) != NULL;\n\n\t\t\t\tpr_info(\"%pa..%pa (%lu KiB) %s %s %s\\n\",\n\t\t\t\t\t&rmem->base, &end, (unsigned long)(rmem->size / SZ_1K),\n\t\t\t\t\tnomap ? \"nomap\" : \"map\",\n\t\t\t\t\treusable ? \"reusable\" : \"non-reusable\",\n\t\t\t\t\trmem->name ? rmem->name : \"unknown\");\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic inline struct reserved_mem *__find_rmem(struct device_node *node)\n{\n\tunsigned int i;\n\n\tif (!node->phandle)\n\t\treturn NULL;\n\n\tfor (i = 0; i < reserved_mem_count; i++)\n\t\tif (reserved_mem[i].phandle == node->phandle)\n\t\t\treturn &reserved_mem[i];\n\treturn NULL;\n}\n\nstruct rmem_assigned_device {\n\tstruct device *dev;\n\tstruct reserved_mem *rmem;\n\tstruct list_head list;\n};\n\nstatic LIST_HEAD(of_rmem_assigned_device_list);\nstatic DEFINE_MUTEX(of_rmem_assigned_device_mutex);\n\n \nint of_reserved_mem_device_init_by_idx(struct device *dev,\n\t\t\t\t       struct device_node *np, int idx)\n{\n\tstruct rmem_assigned_device *rd;\n\tstruct device_node *target;\n\tstruct reserved_mem *rmem;\n\tint ret;\n\n\tif (!np || !dev)\n\t\treturn -EINVAL;\n\n\ttarget = of_parse_phandle(np, \"memory-region\", idx);\n\tif (!target)\n\t\treturn -ENODEV;\n\n\tif (!of_device_is_available(target)) {\n\t\tof_node_put(target);\n\t\treturn 0;\n\t}\n\n\trmem = __find_rmem(target);\n\tof_node_put(target);\n\n\tif (!rmem || !rmem->ops || !rmem->ops->device_init)\n\t\treturn -EINVAL;\n\n\trd = kmalloc(sizeof(struct rmem_assigned_device), GFP_KERNEL);\n\tif (!rd)\n\t\treturn -ENOMEM;\n\n\tret = rmem->ops->device_init(rmem, dev);\n\tif (ret == 0) {\n\t\trd->dev = dev;\n\t\trd->rmem = rmem;\n\n\t\tmutex_lock(&of_rmem_assigned_device_mutex);\n\t\tlist_add(&rd->list, &of_rmem_assigned_device_list);\n\t\tmutex_unlock(&of_rmem_assigned_device_mutex);\n\n\t\tdev_info(dev, \"assigned reserved memory node %s\\n\", rmem->name);\n\t} else {\n\t\tkfree(rd);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(of_reserved_mem_device_init_by_idx);\n\n \nint of_reserved_mem_device_init_by_name(struct device *dev,\n\t\t\t\t\tstruct device_node *np,\n\t\t\t\t\tconst char *name)\n{\n\tint idx = of_property_match_string(np, \"memory-region-names\", name);\n\n\treturn of_reserved_mem_device_init_by_idx(dev, np, idx);\n}\nEXPORT_SYMBOL_GPL(of_reserved_mem_device_init_by_name);\n\n \nvoid of_reserved_mem_device_release(struct device *dev)\n{\n\tstruct rmem_assigned_device *rd, *tmp;\n\tLIST_HEAD(release_list);\n\n\tmutex_lock(&of_rmem_assigned_device_mutex);\n\tlist_for_each_entry_safe(rd, tmp, &of_rmem_assigned_device_list, list) {\n\t\tif (rd->dev == dev)\n\t\t\tlist_move_tail(&rd->list, &release_list);\n\t}\n\tmutex_unlock(&of_rmem_assigned_device_mutex);\n\n\tlist_for_each_entry_safe(rd, tmp, &release_list, list) {\n\t\tif (rd->rmem && rd->rmem->ops && rd->rmem->ops->device_release)\n\t\t\trd->rmem->ops->device_release(rd->rmem, dev);\n\n\t\tkfree(rd);\n\t}\n}\nEXPORT_SYMBOL_GPL(of_reserved_mem_device_release);\n\n \nstruct reserved_mem *of_reserved_mem_lookup(struct device_node *np)\n{\n\tconst char *name;\n\tint i;\n\n\tif (!np->full_name)\n\t\treturn NULL;\n\n\tname = kbasename(np->full_name);\n\tfor (i = 0; i < reserved_mem_count; i++)\n\t\tif (!strcmp(reserved_mem[i].name, name))\n\t\t\treturn &reserved_mem[i];\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(of_reserved_mem_lookup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}