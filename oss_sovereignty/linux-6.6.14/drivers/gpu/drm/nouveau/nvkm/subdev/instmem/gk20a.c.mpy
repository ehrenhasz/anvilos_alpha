{
  "module_name": "gk20a.c",
  "hash_id": "a34961bbcbbf1d72c9ec04405d9ef81866ce2175212a44a57c575a875ddb203c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/gk20a.c",
  "human_readable_source": " \n\n \n#include \"priv.h\"\n\n#include <core/memory.h>\n#include <core/tegra.h>\n#include <subdev/ltc.h>\n#include <subdev/mmu.h>\n\nstruct gk20a_instobj {\n\tstruct nvkm_memory memory;\n\tstruct nvkm_mm_node *mn;\n\tstruct gk20a_instmem *imem;\n\n\t \n\tu32 *vaddr;\n};\n#define gk20a_instobj(p) container_of((p), struct gk20a_instobj, memory)\n\n \nstruct gk20a_instobj_dma {\n\tstruct gk20a_instobj base;\n\n\tdma_addr_t handle;\n\tstruct nvkm_mm_node r;\n};\n#define gk20a_instobj_dma(p) \\\n\tcontainer_of(gk20a_instobj(p), struct gk20a_instobj_dma, base)\n\n \nstruct gk20a_instobj_iommu {\n\tstruct gk20a_instobj base;\n\n\t \n\tstruct list_head vaddr_node;\n\t \n\tu32 use_cpt;\n\n\t \n\tdma_addr_t *dma_addrs;\n\t \n\tstruct page *pages[];\n};\n#define gk20a_instobj_iommu(p) \\\n\tcontainer_of(gk20a_instobj(p), struct gk20a_instobj_iommu, base)\n\nstruct gk20a_instmem {\n\tstruct nvkm_instmem base;\n\n\t \n\tstruct mutex lock;\n\n\t \n\tunsigned int vaddr_use;\n\tunsigned int vaddr_max;\n\tstruct list_head vaddr_lru;\n\n\t \n\tstruct mutex *mm_mutex;\n\tstruct nvkm_mm *mm;\n\tstruct iommu_domain *domain;\n\tunsigned long iommu_pgshift;\n\tu16 iommu_bit;\n\n\t \n\tunsigned long attrs;\n};\n#define gk20a_instmem(p) container_of((p), struct gk20a_instmem, base)\n\nstatic enum nvkm_memory_target\ngk20a_instobj_target(struct nvkm_memory *memory)\n{\n\treturn NVKM_MEM_TARGET_NCOH;\n}\n\nstatic u8\ngk20a_instobj_page(struct nvkm_memory *memory)\n{\n\treturn 12;\n}\n\nstatic u64\ngk20a_instobj_addr(struct nvkm_memory *memory)\n{\n\treturn (u64)gk20a_instobj(memory)->mn->offset << 12;\n}\n\nstatic u64\ngk20a_instobj_size(struct nvkm_memory *memory)\n{\n\treturn (u64)gk20a_instobj(memory)->mn->length << 12;\n}\n\n \nstatic void\ngk20a_instobj_iommu_recycle_vaddr(struct gk20a_instobj_iommu *obj)\n{\n\tstruct gk20a_instmem *imem = obj->base.imem;\n\t \n\tWARN_ON(obj->use_cpt);\n\tlist_del(&obj->vaddr_node);\n\tvunmap(obj->base.vaddr);\n\tobj->base.vaddr = NULL;\n\timem->vaddr_use -= nvkm_memory_size(&obj->base.memory);\n\tnvkm_debug(&imem->base.subdev, \"vaddr used: %x/%x\\n\", imem->vaddr_use,\n\t\t   imem->vaddr_max);\n}\n\n \nstatic void\ngk20a_instmem_vaddr_gc(struct gk20a_instmem *imem, const u64 size)\n{\n\twhile (imem->vaddr_use + size > imem->vaddr_max) {\n\t\t \n\t\tif (list_empty(&imem->vaddr_lru))\n\t\t\tbreak;\n\n\t\tgk20a_instobj_iommu_recycle_vaddr(\n\t\t\t\tlist_first_entry(&imem->vaddr_lru,\n\t\t\t\tstruct gk20a_instobj_iommu, vaddr_node));\n\t}\n}\n\nstatic void __iomem *\ngk20a_instobj_acquire_dma(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj *node = gk20a_instobj(memory);\n\tstruct gk20a_instmem *imem = node->imem;\n\tstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\n\n\tnvkm_ltc_flush(ltc);\n\n\treturn node->vaddr;\n}\n\nstatic void __iomem *\ngk20a_instobj_acquire_iommu(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\n\tstruct gk20a_instmem *imem = node->base.imem;\n\tstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\n\tconst u64 size = nvkm_memory_size(memory);\n\n\tnvkm_ltc_flush(ltc);\n\n\tmutex_lock(&imem->lock);\n\n\tif (node->base.vaddr) {\n\t\tif (!node->use_cpt) {\n\t\t\t \n\t\t\tlist_del(&node->vaddr_node);\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tgk20a_instmem_vaddr_gc(imem, size);\n\n\t \n\tnode->base.vaddr = vmap(node->pages, size >> PAGE_SHIFT, VM_MAP,\n\t\t\t\tpgprot_writecombine(PAGE_KERNEL));\n\tif (!node->base.vaddr) {\n\t\tnvkm_error(&imem->base.subdev, \"cannot map instobj - \"\n\t\t\t   \"this is not going to end well...\\n\");\n\t\tgoto out;\n\t}\n\n\timem->vaddr_use += size;\n\tnvkm_debug(&imem->base.subdev, \"vaddr used: %x/%x\\n\",\n\t\t   imem->vaddr_use, imem->vaddr_max);\n\nout:\n\tnode->use_cpt++;\n\tmutex_unlock(&imem->lock);\n\n\treturn node->base.vaddr;\n}\n\nstatic void\ngk20a_instobj_release_dma(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj *node = gk20a_instobj(memory);\n\tstruct gk20a_instmem *imem = node->imem;\n\tstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\n\n\t \n\twmb();\n\tnvkm_ltc_invalidate(ltc);\n}\n\nstatic void\ngk20a_instobj_release_iommu(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\n\tstruct gk20a_instmem *imem = node->base.imem;\n\tstruct nvkm_ltc *ltc = imem->base.subdev.device->ltc;\n\n\tmutex_lock(&imem->lock);\n\n\t \n\tif (WARN_ON(node->use_cpt == 0))\n\t\tgoto out;\n\n\t \n\tif (--node->use_cpt == 0)\n\t\tlist_add_tail(&node->vaddr_node, &imem->vaddr_lru);\n\nout:\n\tmutex_unlock(&imem->lock);\n\n\twmb();\n\tnvkm_ltc_invalidate(ltc);\n}\n\nstatic u32\ngk20a_instobj_rd32(struct nvkm_memory *memory, u64 offset)\n{\n\tstruct gk20a_instobj *node = gk20a_instobj(memory);\n\n\treturn node->vaddr[offset / 4];\n}\n\nstatic void\ngk20a_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)\n{\n\tstruct gk20a_instobj *node = gk20a_instobj(memory);\n\n\tnode->vaddr[offset / 4] = data;\n}\n\nstatic int\ngk20a_instobj_map(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,\n\t\t  struct nvkm_vma *vma, void *argv, u32 argc)\n{\n\tstruct gk20a_instobj *node = gk20a_instobj(memory);\n\tstruct nvkm_vmm_map map = {\n\t\t.memory = &node->memory,\n\t\t.offset = offset,\n\t\t.mem = node->mn,\n\t};\n\n\treturn nvkm_vmm_map(vmm, vma, argv, argc, &map);\n}\n\nstatic void *\ngk20a_instobj_dtor_dma(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj_dma *node = gk20a_instobj_dma(memory);\n\tstruct gk20a_instmem *imem = node->base.imem;\n\tstruct device *dev = imem->base.subdev.device->dev;\n\n\tif (unlikely(!node->base.vaddr))\n\t\tgoto out;\n\n\tdma_free_attrs(dev, (u64)node->base.mn->length << PAGE_SHIFT,\n\t\t       node->base.vaddr, node->handle, imem->attrs);\n\nout:\n\treturn node;\n}\n\nstatic void *\ngk20a_instobj_dtor_iommu(struct nvkm_memory *memory)\n{\n\tstruct gk20a_instobj_iommu *node = gk20a_instobj_iommu(memory);\n\tstruct gk20a_instmem *imem = node->base.imem;\n\tstruct device *dev = imem->base.subdev.device->dev;\n\tstruct nvkm_mm_node *r = node->base.mn;\n\tint i;\n\n\tif (unlikely(!r))\n\t\tgoto out;\n\n\tmutex_lock(&imem->lock);\n\n\t \n\tif (node->base.vaddr)\n\t\tgk20a_instobj_iommu_recycle_vaddr(node);\n\n\tmutex_unlock(&imem->lock);\n\n\t \n\tr->offset &= ~BIT(imem->iommu_bit - imem->iommu_pgshift);\n\n\t \n\tfor (i = 0; i < node->base.mn->length; i++) {\n\t\tiommu_unmap(imem->domain,\n\t\t\t    (r->offset + i) << imem->iommu_pgshift, PAGE_SIZE);\n\t\tdma_unmap_page(dev, node->dma_addrs[i], PAGE_SIZE,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t\t__free_page(node->pages[i]);\n\t}\n\n\t \n\tmutex_lock(imem->mm_mutex);\n\tnvkm_mm_free(imem->mm, &r);\n\tmutex_unlock(imem->mm_mutex);\n\nout:\n\treturn node;\n}\n\nstatic const struct nvkm_memory_func\ngk20a_instobj_func_dma = {\n\t.dtor = gk20a_instobj_dtor_dma,\n\t.target = gk20a_instobj_target,\n\t.page = gk20a_instobj_page,\n\t.addr = gk20a_instobj_addr,\n\t.size = gk20a_instobj_size,\n\t.acquire = gk20a_instobj_acquire_dma,\n\t.release = gk20a_instobj_release_dma,\n\t.map = gk20a_instobj_map,\n};\n\nstatic const struct nvkm_memory_func\ngk20a_instobj_func_iommu = {\n\t.dtor = gk20a_instobj_dtor_iommu,\n\t.target = gk20a_instobj_target,\n\t.page = gk20a_instobj_page,\n\t.addr = gk20a_instobj_addr,\n\t.size = gk20a_instobj_size,\n\t.acquire = gk20a_instobj_acquire_iommu,\n\t.release = gk20a_instobj_release_iommu,\n\t.map = gk20a_instobj_map,\n};\n\nstatic const struct nvkm_memory_ptrs\ngk20a_instobj_ptrs = {\n\t.rd32 = gk20a_instobj_rd32,\n\t.wr32 = gk20a_instobj_wr32,\n};\n\nstatic int\ngk20a_instobj_ctor_dma(struct gk20a_instmem *imem, u32 npages, u32 align,\n\t\t       struct gk20a_instobj **_node)\n{\n\tstruct gk20a_instobj_dma *node;\n\tstruct nvkm_subdev *subdev = &imem->base.subdev;\n\tstruct device *dev = subdev->device->dev;\n\n\tif (!(node = kzalloc(sizeof(*node), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\t*_node = &node->base;\n\n\tnvkm_memory_ctor(&gk20a_instobj_func_dma, &node->base.memory);\n\tnode->base.memory.ptrs = &gk20a_instobj_ptrs;\n\n\tnode->base.vaddr = dma_alloc_attrs(dev, npages << PAGE_SHIFT,\n\t\t\t\t\t   &node->handle, GFP_KERNEL,\n\t\t\t\t\t   imem->attrs);\n\tif (!node->base.vaddr) {\n\t\tnvkm_error(subdev, \"cannot allocate DMA memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (unlikely(node->handle & (align - 1)))\n\t\tnvkm_warn(subdev,\n\t\t\t  \"memory not aligned as requested: %pad (0x%x)\\n\",\n\t\t\t  &node->handle, align);\n\n\t \n\tnode->r.type = 12;\n\tnode->r.offset = node->handle >> 12;\n\tnode->r.length = (npages << PAGE_SHIFT) >> 12;\n\n\tnode->base.mn = &node->r;\n\treturn 0;\n}\n\nstatic int\ngk20a_instobj_ctor_iommu(struct gk20a_instmem *imem, u32 npages, u32 align,\n\t\t\t struct gk20a_instobj **_node)\n{\n\tstruct gk20a_instobj_iommu *node;\n\tstruct nvkm_subdev *subdev = &imem->base.subdev;\n\tstruct device *dev = subdev->device->dev;\n\tstruct nvkm_mm_node *r;\n\tint ret;\n\tint i;\n\n\t \n\tif (!(node = kzalloc(sizeof(*node) + ((sizeof(node->pages[0]) +\n\t\t\t     sizeof(*node->dma_addrs)) * npages), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\t*_node = &node->base;\n\tnode->dma_addrs = (void *)(node->pages + npages);\n\n\tnvkm_memory_ctor(&gk20a_instobj_func_iommu, &node->base.memory);\n\tnode->base.memory.ptrs = &gk20a_instobj_ptrs;\n\n\t \n\tfor (i = 0; i < npages; i++) {\n\t\tstruct page *p = alloc_page(GFP_KERNEL);\n\t\tdma_addr_t dma_adr;\n\n\t\tif (p == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_pages;\n\t\t}\n\t\tnode->pages[i] = p;\n\t\tdma_adr = dma_map_page(dev, p, 0, PAGE_SIZE, DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, dma_adr)) {\n\t\t\tnvkm_error(subdev, \"DMA mapping error!\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_pages;\n\t\t}\n\t\tnode->dma_addrs[i] = dma_adr;\n\t}\n\n\tmutex_lock(imem->mm_mutex);\n\t \n\tret = nvkm_mm_head(imem->mm, 0, 1, npages, npages,\n\t\t\t   align >> imem->iommu_pgshift, &r);\n\tmutex_unlock(imem->mm_mutex);\n\tif (ret) {\n\t\tnvkm_error(subdev, \"IOMMU space is full!\\n\");\n\t\tgoto free_pages;\n\t}\n\n\t \n\tfor (i = 0; i < npages; i++) {\n\t\tu32 offset = (r->offset + i) << imem->iommu_pgshift;\n\n\t\tret = iommu_map(imem->domain, offset, node->dma_addrs[i],\n\t\t\t\tPAGE_SIZE, IOMMU_READ | IOMMU_WRITE,\n\t\t\t\tGFP_KERNEL);\n\t\tif (ret < 0) {\n\t\t\tnvkm_error(subdev, \"IOMMU mapping failure: %d\\n\", ret);\n\n\t\t\twhile (i-- > 0) {\n\t\t\t\toffset -= PAGE_SIZE;\n\t\t\t\tiommu_unmap(imem->domain, offset, PAGE_SIZE);\n\t\t\t}\n\t\t\tgoto release_area;\n\t\t}\n\t}\n\n\t \n\tr->offset |= BIT(imem->iommu_bit - imem->iommu_pgshift);\n\n\tnode->base.mn = r;\n\treturn 0;\n\nrelease_area:\n\tmutex_lock(imem->mm_mutex);\n\tnvkm_mm_free(imem->mm, &r);\n\tmutex_unlock(imem->mm_mutex);\n\nfree_pages:\n\tfor (i = 0; i < npages && node->pages[i] != NULL; i++) {\n\t\tdma_addr_t dma_addr = node->dma_addrs[i];\n\t\tif (dma_addr)\n\t\t\tdma_unmap_page(dev, dma_addr, PAGE_SIZE,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\t__free_page(node->pages[i]);\n\t}\n\n\treturn ret;\n}\n\nstatic int\ngk20a_instobj_new(struct nvkm_instmem *base, u32 size, u32 align, bool zero,\n\t\t  struct nvkm_memory **pmemory)\n{\n\tstruct gk20a_instmem *imem = gk20a_instmem(base);\n\tstruct nvkm_subdev *subdev = &imem->base.subdev;\n\tstruct gk20a_instobj *node = NULL;\n\tint ret;\n\n\tnvkm_debug(subdev, \"%s (%s): size: %x align: %x\\n\", __func__,\n\t\t   imem->domain ? \"IOMMU\" : \"DMA\", size, align);\n\n\t \n\tsize = max(roundup(size, PAGE_SIZE), PAGE_SIZE);\n\talign = max(roundup(align, PAGE_SIZE), PAGE_SIZE);\n\n\tif (imem->domain)\n\t\tret = gk20a_instobj_ctor_iommu(imem, size >> PAGE_SHIFT,\n\t\t\t\t\t       align, &node);\n\telse\n\t\tret = gk20a_instobj_ctor_dma(imem, size >> PAGE_SHIFT,\n\t\t\t\t\t     align, &node);\n\t*pmemory = node ? &node->memory : NULL;\n\tif (ret)\n\t\treturn ret;\n\n\tnode->imem = imem;\n\n\tnvkm_debug(subdev, \"alloc size: 0x%x, align: 0x%x, gaddr: 0x%llx\\n\",\n\t\t   size, align, (u64)node->mn->offset << 12);\n\n\treturn 0;\n}\n\nstatic void *\ngk20a_instmem_dtor(struct nvkm_instmem *base)\n{\n\tstruct gk20a_instmem *imem = gk20a_instmem(base);\n\n\t \n\tif (!list_empty(&imem->vaddr_lru))\n\t\tnvkm_warn(&base->subdev, \"instobj LRU not empty!\\n\");\n\n\tif (imem->vaddr_use != 0)\n\t\tnvkm_warn(&base->subdev, \"instobj vmap area not empty! \"\n\t\t\t  \"0x%x bytes still mapped\\n\", imem->vaddr_use);\n\n\treturn imem;\n}\n\nstatic const struct nvkm_instmem_func\ngk20a_instmem = {\n\t.dtor = gk20a_instmem_dtor,\n\t.memory_new = gk20a_instobj_new,\n\t.zero = false,\n};\n\nint\ngk20a_instmem_new(struct nvkm_device *device, enum nvkm_subdev_type type, int inst,\n\t\t  struct nvkm_instmem **pimem)\n{\n\tstruct nvkm_device_tegra *tdev = device->func->tegra(device);\n\tstruct gk20a_instmem *imem;\n\n\tif (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tnvkm_instmem_ctor(&gk20a_instmem, device, type, inst, &imem->base);\n\tmutex_init(&imem->lock);\n\t*pimem = &imem->base;\n\n\t \n\timem->vaddr_use = 0;\n\timem->vaddr_max = 0x100000;\n\tINIT_LIST_HEAD(&imem->vaddr_lru);\n\n\tif (tdev->iommu.domain) {\n\t\timem->mm_mutex = &tdev->iommu.mutex;\n\t\timem->mm = &tdev->iommu.mm;\n\t\timem->domain = tdev->iommu.domain;\n\t\timem->iommu_pgshift = tdev->iommu.pgshift;\n\t\timem->iommu_bit = tdev->func->iommu_bit;\n\n\t\tnvkm_info(&imem->base.subdev, \"using IOMMU\\n\");\n\t} else {\n\t\timem->attrs = DMA_ATTR_WEAK_ORDERING |\n\t\t\t      DMA_ATTR_WRITE_COMBINE;\n\n\t\tnvkm_info(&imem->base.subdev, \"using DMA API\\n\");\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}