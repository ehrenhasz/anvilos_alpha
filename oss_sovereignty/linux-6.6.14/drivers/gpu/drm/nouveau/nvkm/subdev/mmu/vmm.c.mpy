{
  "module_name": "vmm.c",
  "hash_id": "65478c22bc38332f9c3d1c81e82fab0667e4dbefe837dc999d87fb74245dbbed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.c",
  "human_readable_source": " \n#define NVKM_VMM_LEVELS_MAX 5\n#include \"vmm.h\"\n\n#include <subdev/fb.h>\n\nstatic void\nnvkm_vmm_pt_del(struct nvkm_vmm_pt **ppgt)\n{\n\tstruct nvkm_vmm_pt *pgt = *ppgt;\n\tif (pgt) {\n\t\tkvfree(pgt->pde);\n\t\tkfree(pgt);\n\t\t*ppgt = NULL;\n\t}\n}\n\n\nstatic struct nvkm_vmm_pt *\nnvkm_vmm_pt_new(const struct nvkm_vmm_desc *desc, bool sparse,\n\t\tconst struct nvkm_vmm_page *page)\n{\n\tconst u32 pten = 1 << desc->bits;\n\tstruct nvkm_vmm_pt *pgt;\n\tu32 lpte = 0;\n\n\tif (desc->type > PGT) {\n\t\tif (desc->type == SPT) {\n\t\t\tconst struct nvkm_vmm_desc *pair = page[-1].desc;\n\t\t\tlpte = pten >> (desc->bits - pair->bits);\n\t\t} else {\n\t\t\tlpte = pten;\n\t\t}\n\t}\n\n\tif (!(pgt = kzalloc(sizeof(*pgt) + lpte, GFP_KERNEL)))\n\t\treturn NULL;\n\tpgt->page = page ? page->shift : 0;\n\tpgt->sparse = sparse;\n\n\tif (desc->type == PGD) {\n\t\tpgt->pde = kvcalloc(pten, sizeof(*pgt->pde), GFP_KERNEL);\n\t\tif (!pgt->pde) {\n\t\t\tkfree(pgt);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn pgt;\n}\n\nstruct nvkm_vmm_iter {\n\tconst struct nvkm_vmm_page *page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vmm *vmm;\n\tu64 cnt;\n\tu16 max, lvl;\n\tu32 pte[NVKM_VMM_LEVELS_MAX];\n\tstruct nvkm_vmm_pt *pt[NVKM_VMM_LEVELS_MAX];\n\tint flush;\n};\n\n#ifdef CONFIG_NOUVEAU_DEBUG_MMU\nstatic const char *\nnvkm_vmm_desc_type(const struct nvkm_vmm_desc *desc)\n{\n\tswitch (desc->type) {\n\tcase PGD: return \"PGD\";\n\tcase PGT: return \"PGT\";\n\tcase SPT: return \"SPT\";\n\tcase LPT: return \"LPT\";\n\tdefault:\n\t\treturn \"UNKNOWN\";\n\t}\n}\n\nstatic void\nnvkm_vmm_trace(struct nvkm_vmm_iter *it, char *buf)\n{\n\tint lvl;\n\tfor (lvl = it->max; lvl >= 0; lvl--) {\n\t\tif (lvl >= it->lvl)\n\t\t\tbuf += sprintf(buf,  \"%05x:\", it->pte[lvl]);\n\t\telse\n\t\t\tbuf += sprintf(buf, \"xxxxx:\");\n\t}\n}\n\n#define TRA(i,f,a...) do {                                                     \\\n\tchar _buf[NVKM_VMM_LEVELS_MAX * 7];                                    \\\n\tstruct nvkm_vmm_iter *_it = (i);                                       \\\n\tnvkm_vmm_trace(_it, _buf);                                             \\\n\tVMM_TRACE(_it->vmm, \"%s \"f, _buf, ##a);                                \\\n} while(0)\n#else\n#define TRA(i,f,a...)\n#endif\n\nstatic inline void\nnvkm_vmm_flush_mark(struct nvkm_vmm_iter *it)\n{\n\tit->flush = min(it->flush, it->max - it->lvl);\n}\n\nstatic inline void\nnvkm_vmm_flush(struct nvkm_vmm_iter *it)\n{\n\tif (it->flush != NVKM_VMM_LEVELS_MAX) {\n\t\tif (it->vmm->func->flush) {\n\t\t\tTRA(it, \"flush: %d\", it->flush);\n\t\t\tit->vmm->func->flush(it->vmm, it->flush);\n\t\t}\n\t\tit->flush = NVKM_VMM_LEVELS_MAX;\n\t}\n}\n\nstatic void\nnvkm_vmm_unref_pdes(struct nvkm_vmm_iter *it)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc[it->lvl].type == SPT;\n\tstruct nvkm_vmm_pt *pgd = it->pt[it->lvl + 1];\n\tstruct nvkm_vmm_pt *pgt = it->pt[it->lvl];\n\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 pdei = it->pte[it->lvl + 1];\n\n\t \n\tit->lvl++;\n\tif (--pgd->refs[0]) {\n\t\tconst struct nvkm_vmm_desc_func *func = desc[it->lvl].func;\n\t\t \n\t\tTRA(it, \"PDE unmap %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\t\tpgt->pt[type] = NULL;\n\t\tif (!pgt->refs[!type]) {\n\t\t\t \n\t\t\tif (pgd->pt[0]) {\n\t\t\t\tif (pgt->sparse) {\n\t\t\t\t\tfunc->sparse(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NVKM_VMM_PDE_SPARSE;\n\t\t\t\t} else {\n\t\t\t\t\tfunc->unmap(vmm, pgd->pt[0], pdei, 1);\n\t\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t\t\tpgd->pde[pdei] = NULL;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tfunc->pde(vmm, pgd, pdei);\n\t\t}\n\n\t\t \n\t\tnvkm_vmm_flush_mark(it);\n\t\tnvkm_vmm_flush(it);\n\t} else {\n\t\t \n\t\tnvkm_vmm_unref_pdes(it);\n\t}\n\n\t \n\tTRA(it, \"PDE free %s\", nvkm_vmm_desc_type(&desc[it->lvl - 1]));\n\tnvkm_mmu_ptc_put(vmm->mmu, vmm->bootstrapped, &pt);\n\tif (!pgt->refs[!type])\n\t\tnvkm_vmm_pt_del(&pgt);\n\tit->lvl--;\n}\n\nstatic void\nnvkm_vmm_unref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t     const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t \n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] -= pten;\n\t\tptes -= pten;\n\t}\n\n\t \n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t \n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPTES) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_SPTES))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_SPTES)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] &= ~NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tTRA(it, \"LPTE %05x: U -> S %d PTEs\", pteb, ptes);\n\t\t\tpair->func->sparse(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t \n\t\t\tTRA(it, \"LPTE %05x: U -> I %d PTEs\", pteb, ptes);\n\t\t\tpair->func->invalid(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_unref_ptes(struct nvkm_vmm_iter *it, bool pfn, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\tbool dma;\n\n\tif (pfn) {\n\t\t \n\t\tdma = desc->func->pfn_clear(it->vmm, pgt->pt[type], ptei, ptes);\n\t\tif (dma) {\n\t\t\t \n\t\t\tnvkm_vmm_flush_mark(it);\n\t\t\tnvkm_vmm_flush(it);\n\t\t\tdesc->func->pfn_unmap(it->vmm, pgt->pt[type], ptei, ptes);\n\t\t}\n\t}\n\n\t \n\tpgt->refs[type] -= ptes;\n\n\t \n\tif (desc->type == SPT && (pgt->refs[0] || pgt->refs[1]))\n\t\tnvkm_vmm_unref_sptes(it, pgt, desc, ptei, ptes);\n\n\t \n\tif (!pgt->refs[type]) {\n\t\tit->lvl++;\n\t\tTRA(it, \"%s empty\", nvkm_vmm_desc_type(desc));\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;  \n\t}\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_ref_sptes(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgt,\n\t\t   const struct nvkm_vmm_desc *desc, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *pair = it->page[-1].desc;\n\tconst u32 sptb = desc->bits - pair->bits;\n\tconst u32 sptn = 1 << sptb;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tu32 spti = ptei & (sptn - 1), lpti, pteb;\n\n\t \n\tfor (lpti = ptei >> sptb; ptes; spti = 0, lpti++) {\n\t\tconst u32 pten = min(sptn - spti, ptes);\n\t\tpgt->pte[lpti] += pten;\n\t\tptes -= pten;\n\t}\n\n\t \n\tif (!pgt->refs[0])\n\t\treturn;\n\n\tfor (ptei = pteb = ptei >> sptb; ptei < lpti; pteb = ptei) {\n\t\t \n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_VALID) {\n\t\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\t\tif (!(pgt->pte[ptei] & NVKM_VMM_PTE_VALID))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\tfor (ptes = 1, ptei++; ptei < lpti; ptes++, ptei++) {\n\t\t\tif (pgt->pte[ptei] & NVKM_VMM_PTE_VALID)\n\t\t\t\tbreak;\n\t\t\tpgt->pte[ptei] |= NVKM_VMM_PTE_VALID;\n\t\t}\n\n\t\tif (pgt->pte[pteb] & NVKM_VMM_PTE_SPARSE) {\n\t\t\tconst u32 spti = pteb * sptn;\n\t\t\tconst u32 sptc = ptes * sptn;\n\t\t\t \n\t\t\tTRA(it, \"SPTE %05x: U -> S %d PTEs\", spti, sptc);\n\t\t\tdesc->func->sparse(vmm, pgt->pt[1], spti, sptc);\n\t\t\t \n\t\t\tTRA(it, \"LPTE %05x: S -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t} else\n\t\tif (pair->func->invalid) {\n\t\t\t \n\t\t\tTRA(it, \"LPTE %05x: I -> U %d PTEs\", pteb, ptes);\n\t\t\tpair->func->unmap(vmm, pgt->pt[0], pteb, ptes);\n\t\t}\n\t}\n}\n\nstatic bool\nnvkm_vmm_ref_ptes(struct nvkm_vmm_iter *it, bool pfn, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = it->pt[0];\n\n\t \n\tpgt->refs[type] += ptes;\n\n\t \n\tif (desc->type == SPT)\n\t\tnvkm_vmm_ref_sptes(it, pgt, desc, ptei, ptes);\n\n\treturn true;\n}\n\nstatic void\nnvkm_vmm_sparse_ptes(const struct nvkm_vmm_desc *desc,\n\t\t     struct nvkm_vmm_pt *pgt, u32 ptei, u32 ptes)\n{\n\tif (desc->type == PGD) {\n\t\twhile (ptes--)\n\t\t\tpgt->pde[ptei++] = NVKM_VMM_PDE_SPARSE;\n\t} else\n\tif (desc->type == LPT) {\n\t\tmemset(&pgt->pte[ptei], NVKM_VMM_PTE_SPARSE, ptes);\n\t}\n}\n\nstatic bool\nnvkm_vmm_sparse_unref_ptes(struct nvkm_vmm_iter *it, bool pfn, u32 ptei, u32 ptes)\n{\n\tstruct nvkm_vmm_pt *pt = it->pt[0];\n\tif (it->desc->type == PGD)\n\t\tmemset(&pt->pde[ptei], 0x00, sizeof(pt->pde[0]) * ptes);\n\telse\n\tif (it->desc->type == LPT)\n\t\tmemset(&pt->pte[ptei], 0x00, sizeof(pt->pte[0]) * ptes);\n\treturn nvkm_vmm_unref_ptes(it, pfn, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_sparse_ref_ptes(struct nvkm_vmm_iter *it, bool pfn, u32 ptei, u32 ptes)\n{\n\tnvkm_vmm_sparse_ptes(it->desc, it->pt[0], ptei, ptes);\n\treturn nvkm_vmm_ref_ptes(it, pfn, ptei, ptes);\n}\n\nstatic bool\nnvkm_vmm_ref_hwpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tconst int type = desc->type == SPT;\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\tconst bool zero = !pgt->sparse && !desc->func->invalid;\n\tstruct nvkm_vmm *vmm = it->vmm;\n\tstruct nvkm_mmu *mmu = vmm->mmu;\n\tstruct nvkm_mmu_pt *pt;\n\tu32 pten = 1 << desc->bits;\n\tu32 pteb, ptei, ptes;\n\tu32 size = desc->size * pten;\n\n\tpgd->refs[0]++;\n\n\tpgt->pt[type] = nvkm_mmu_ptc_get(mmu, size, desc->align, zero);\n\tif (!pgt->pt[type]) {\n\t\tit->lvl--;\n\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tif (zero)\n\t\tgoto done;\n\n\tpt = pgt->pt[type];\n\n\tif (desc->type == LPT && pgt->refs[1]) {\n\t\t \n\t\tfor (ptei = pteb = 0; ptei < pten; pteb = ptei) {\n\t\t\tbool spte = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\tfor (ptes = 1, ptei++; ptei < pten; ptes++, ptei++) {\n\t\t\t\tbool next = pgt->pte[ptei] & NVKM_VMM_PTE_SPTES;\n\t\t\t\tif (spte != next)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!spte) {\n\t\t\t\tif (pgt->sparse)\n\t\t\t\t\tdesc->func->sparse(vmm, pt, pteb, ptes);\n\t\t\t\telse\n\t\t\t\t\tdesc->func->invalid(vmm, pt, pteb, ptes);\n\t\t\t\tmemset(&pgt->pte[pteb], 0x00, ptes);\n\t\t\t} else {\n\t\t\t\tdesc->func->unmap(vmm, pt, pteb, ptes);\n\t\t\t\twhile (ptes--)\n\t\t\t\t\tpgt->pte[pteb++] |= NVKM_VMM_PTE_VALID;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (pgt->sparse) {\n\t\t\tnvkm_vmm_sparse_ptes(desc, pgt, 0, pten);\n\t\t\tdesc->func->sparse(vmm, pt, 0, pten);\n\t\t} else {\n\t\t\tdesc->func->invalid(vmm, pt, 0, pten);\n\t\t}\n\t}\n\ndone:\n\tTRA(it, \"PDE write %s\", nvkm_vmm_desc_type(desc));\n\tit->desc[it->lvl].func->pde(it->vmm, pgd, pdei);\n\tnvkm_vmm_flush_mark(it);\n\treturn true;\n}\n\nstatic bool\nnvkm_vmm_ref_swpt(struct nvkm_vmm_iter *it, struct nvkm_vmm_pt *pgd, u32 pdei)\n{\n\tconst struct nvkm_vmm_desc *desc = &it->desc[it->lvl - 1];\n\tstruct nvkm_vmm_pt *pgt = pgd->pde[pdei];\n\n\tpgt = nvkm_vmm_pt_new(desc, NVKM_VMM_PDE_SPARSED(pgt), it->page);\n\tif (!pgt) {\n\t\tif (!pgd->refs[0])\n\t\t\tnvkm_vmm_unref_pdes(it);\n\t\treturn false;\n\t}\n\n\tpgd->pde[pdei] = pgt;\n\treturn true;\n}\n\nstatic inline u64\nnvkm_vmm_iter(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t      u64 addr, u64 size, const char *name, bool ref, bool pfn,\n\t      bool (*REF_PTES)(struct nvkm_vmm_iter *, bool pfn, u32, u32),\n\t      nvkm_vmm_pte_func MAP_PTES, struct nvkm_vmm_map *map,\n\t      nvkm_vmm_pxe_func CLR_PTES)\n{\n\tconst struct nvkm_vmm_desc *desc = page->desc;\n\tstruct nvkm_vmm_iter it;\n\tu64 bits = addr >> page->shift;\n\n\tit.page = page;\n\tit.desc = desc;\n\tit.vmm = vmm;\n\tit.cnt = size >> page->shift;\n\tit.flush = NVKM_VMM_LEVELS_MAX;\n\n\t \n\tfor (it.lvl = 0; desc[it.lvl].bits; it.lvl++) {\n\t\tit.pte[it.lvl] = bits & ((1 << desc[it.lvl].bits) - 1);\n\t\tbits >>= desc[it.lvl].bits;\n\t}\n\tit.max = --it.lvl;\n\tit.pt[it.max] = vmm->pd;\n\n\tit.lvl = 0;\n\tTRA(&it, \"%s: %016llx %016llx %d %lld PTEs\", name,\n\t         addr, size, page->shift, it.cnt);\n\tit.lvl = it.max;\n\n\t \n\twhile (it.cnt) {\n\t\tstruct nvkm_vmm_pt *pgt = it.pt[it.lvl];\n\t\tconst int type = desc->type == SPT;\n\t\tconst u32 pten = 1 << desc->bits;\n\t\tconst u32 ptei = it.pte[0];\n\t\tconst u32 ptes = min_t(u64, it.cnt, pten - ptei);\n\n\t\t \n\t\tfor (; it.lvl; it.lvl--) {\n\t\t\tconst u32 pdei = it.pte[it.lvl];\n\t\t\tstruct nvkm_vmm_pt *pgd = pgt;\n\n\t\t\t \n\t\t\tif (ref && NVKM_VMM_PDE_INVALID(pgd->pde[pdei])) {\n\t\t\t\tif (!nvkm_vmm_ref_swpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tit.pt[it.lvl - 1] = pgt = pgd->pde[pdei];\n\n\t\t\t \n\t\t\tif (ref && !pgt->refs[desc[it.lvl - 1].type == SPT]) {\n\t\t\t\tif (!nvkm_vmm_ref_hwpt(&it, pgd, pdei))\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!REF_PTES || REF_PTES(&it, pfn, ptei, ptes)) {\n\t\t\tstruct nvkm_mmu_pt *pt = pgt->pt[type];\n\t\t\tif (MAP_PTES || CLR_PTES) {\n\t\t\t\tif (MAP_PTES)\n\t\t\t\t\tMAP_PTES(vmm, pt, ptei, ptes, map);\n\t\t\t\telse\n\t\t\t\t\tCLR_PTES(vmm, pt, ptei, ptes);\n\t\t\t\tnvkm_vmm_flush_mark(&it);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tit.pte[it.lvl] += ptes;\n\t\tit.cnt -= ptes;\n\t\tif (it.cnt) {\n\t\t\twhile (it.pte[it.lvl] == (1 << desc[it.lvl].bits)) {\n\t\t\t\tit.pte[it.lvl++] = 0;\n\t\t\t\tit.pte[it.lvl]++;\n\t\t\t}\n\t\t}\n\t}\n\n\tnvkm_vmm_flush(&it);\n\treturn ~0ULL;\n\nfail:\n\t \n\taddr = it.pte[it.max--];\n\tdo {\n\t\taddr  = addr << desc[it.max].bits;\n\t\taddr |= it.pte[it.max];\n\t} while (it.max--);\n\n\treturn addr << page->shift;\n}\n\nstatic void\nnvkm_vmm_ptes_sparse_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"sparse unref\", false, false,\n\t\t      nvkm_vmm_sparse_unref_ptes, NULL, NULL,\n\t\t      page->desc->func->invalid ?\n\t\t      page->desc->func->invalid : page->desc->func->unmap);\n}\n\nstatic int\nnvkm_vmm_ptes_sparse_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tif ((page->type & NVKM_VMM_PAGE_SPARSE)) {\n\t\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"sparse ref\",\n\t\t\t\t\t true, false, nvkm_vmm_sparse_ref_ptes,\n\t\t\t\t\t NULL, NULL, page->desc->func->sparse);\n\t\tif (fail != ~0ULL) {\n\t\t\tif ((size = fail - addr))\n\t\t\t\tnvkm_vmm_ptes_sparse_put(vmm, page, addr, size);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_ptes_sparse(struct nvkm_vmm *vmm, u64 addr, u64 size, bool ref)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tint m = 0, i;\n\tu64 start = addr;\n\tu64 block;\n\n\twhile (size) {\n\t\t \n\t\twhile (size < (1ULL << page[m].shift))\n\t\t\tm++;\n\t\ti = m;\n\n\t\t \n\t\twhile (!IS_ALIGNED(addr, 1ULL << page[i].shift))\n\t\t\ti++;\n\n\t\t \n\t\tif (i != m) {\n\t\t\t \n\t\t\tu64 next = 1ULL << page[i - 1].shift;\n\t\t\tu64 part = ALIGN(addr, next) - addr;\n\t\t\tif (size - part >= next)\n\t\t\t\tblock = (part >> page[i].shift) << page[i].shift;\n\t\t\telse\n\t\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t} else {\n\t\t\tblock = (size >> page[i].shift) << page[i].shift;\n\t\t}\n\n\t\t \n\t\tif (ref) {\n\t\t\tint ret = nvkm_vmm_ptes_sparse_get(vmm, &page[i], addr, block);\n\t\t\tif (ret) {\n\t\t\t\tif ((size = addr - start))\n\t\t\t\t\tnvkm_vmm_ptes_sparse(vmm, start, size, false);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t} else {\n\t\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[i], addr, block);\n\t\t}\n\n\t\tsize -= block;\n\t\taddr += block;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_ptes_unmap(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t    u64 addr, u64 size, bool sparse, bool pfn)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\n\tmutex_lock(&vmm->mutex.map);\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap\", false, pfn,\n\t\t      NULL, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n\tmutex_unlock(&vmm->mutex.map);\n}\n\nstatic void\nnvkm_vmm_ptes_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t  nvkm_vmm_pte_func func)\n{\n\tmutex_lock(&vmm->mutex.map);\n\tnvkm_vmm_iter(vmm, page, addr, size, \"map\", false, false,\n\t\t      NULL, func, map, NULL);\n\tmutex_unlock(&vmm->mutex.map);\n}\n\nstatic void\nnvkm_vmm_ptes_put_locked(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t u64 addr, u64 size)\n{\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unref\", false, false,\n\t\t      nvkm_vmm_unref_ptes, NULL, NULL, NULL);\n}\n\nstatic void\nnvkm_vmm_ptes_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tmutex_lock(&vmm->mutex.ref);\n\tnvkm_vmm_ptes_put_locked(vmm, page, addr, size);\n\tmutex_unlock(&vmm->mutex.ref);\n}\n\nstatic int\nnvkm_vmm_ptes_get(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t  u64 addr, u64 size)\n{\n\tu64 fail;\n\n\tmutex_lock(&vmm->mutex.ref);\n\tfail = nvkm_vmm_iter(vmm, page, addr, size, \"ref\", true, false,\n\t\t\t     nvkm_vmm_ref_ptes, NULL, NULL, NULL);\n\tif (fail != ~0ULL) {\n\t\tif (fail != addr)\n\t\t\tnvkm_vmm_ptes_put_locked(vmm, page, addr, fail - addr);\n\t\tmutex_unlock(&vmm->mutex.ref);\n\t\treturn -ENOMEM;\n\t}\n\tmutex_unlock(&vmm->mutex.ref);\n\treturn 0;\n}\n\nstatic void\n__nvkm_vmm_ptes_unmap_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\t  u64 addr, u64 size, bool sparse, bool pfn)\n{\n\tconst struct nvkm_vmm_desc_func *func = page->desc->func;\n\n\tnvkm_vmm_iter(vmm, page, addr, size, \"unmap + unref\",\n\t\t      false, pfn, nvkm_vmm_unref_ptes, NULL, NULL,\n\t\t      sparse ? func->sparse : func->invalid ? func->invalid :\n\t\t\t\t\t\t\t      func->unmap);\n}\n\nstatic void\nnvkm_vmm_ptes_unmap_put(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\tu64 addr, u64 size, bool sparse, bool pfn)\n{\n\tif (vmm->managed.raw) {\n\t\tnvkm_vmm_ptes_unmap(vmm, page, addr, size, sparse, pfn);\n\t\tnvkm_vmm_ptes_put(vmm, page, addr, size);\n\t} else {\n\t\t__nvkm_vmm_ptes_unmap_put(vmm, page, addr, size, sparse, pfn);\n\t}\n}\n\nstatic int\n__nvkm_vmm_ptes_get_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t\tu64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t\tnvkm_vmm_pte_func func)\n{\n\tu64 fail = nvkm_vmm_iter(vmm, page, addr, size, \"ref + map\", true,\n\t\t\t\t false, nvkm_vmm_ref_ptes, func, map, NULL);\n\tif (fail != ~0ULL) {\n\t\tif ((size = fail - addr))\n\t\t\tnvkm_vmm_ptes_unmap_put(vmm, page, addr, size, false, false);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int\nnvkm_vmm_ptes_get_map(struct nvkm_vmm *vmm, const struct nvkm_vmm_page *page,\n\t\t      u64 addr, u64 size, struct nvkm_vmm_map *map,\n\t\t      nvkm_vmm_pte_func func)\n{\n\tint ret;\n\n\tif (vmm->managed.raw) {\n\t\tret = nvkm_vmm_ptes_get(vmm, page, addr, size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tnvkm_vmm_ptes_map(vmm, page, addr, size, map, func);\n\n\t\treturn 0;\n\t} else {\n\t\treturn __nvkm_vmm_ptes_get_map(vmm, page, addr, size, map, func);\n\t}\n}\n\nstruct nvkm_vma *\nnvkm_vma_new(u64 addr, u64 size)\n{\n\tstruct nvkm_vma *vma = kzalloc(sizeof(*vma), GFP_KERNEL);\n\tif (vma) {\n\t\tvma->addr = addr;\n\t\tvma->size = size;\n\t\tvma->page = NVKM_VMA_PAGE_NONE;\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t}\n\treturn vma;\n}\n\nstruct nvkm_vma *\nnvkm_vma_tail(struct nvkm_vma *vma, u64 tail)\n{\n\tstruct nvkm_vma *new;\n\n\tBUG_ON(vma->size == tail);\n\n\tif (!(new = nvkm_vma_new(vma->addr + (vma->size - tail), tail)))\n\t\treturn NULL;\n\tvma->size -= tail;\n\n\tnew->mapref = vma->mapref;\n\tnew->sparse = vma->sparse;\n\tnew->page = vma->page;\n\tnew->refd = vma->refd;\n\tnew->used = vma->used;\n\tnew->part = vma->part;\n\tnew->busy = vma->busy;\n\tnew->mapped = vma->mapped;\n\tlist_add(&new->head, &vma->head);\n\treturn new;\n}\n\nstatic inline void\nnvkm_vmm_free_remove(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\trb_erase(&vma->tree, &vmm->free);\n}\n\nstatic inline void\nnvkm_vmm_free_delete(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tnvkm_vmm_free_remove(vmm, vma);\n\tlist_del(&vma->head);\n\tkfree(vma);\n}\n\nstatic void\nnvkm_vmm_free_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->free.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->size < this->size)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->size > this->size)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->free);\n}\n\nstatic inline void\nnvkm_vmm_node_remove(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\trb_erase(&vma->tree, &vmm->root);\n}\n\nstatic inline void\nnvkm_vmm_node_delete(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tnvkm_vmm_node_remove(vmm, vma);\n\tlist_del(&vma->head);\n\tkfree(vma);\n}\n\nstatic void\nnvkm_vmm_node_insert(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct rb_node **ptr = &vmm->root.rb_node;\n\tstruct rb_node *parent = NULL;\n\n\twhile (*ptr) {\n\t\tstruct nvkm_vma *this = rb_entry(*ptr, typeof(*this), tree);\n\t\tparent = *ptr;\n\t\tif (vma->addr < this->addr)\n\t\t\tptr = &parent->rb_left;\n\t\telse\n\t\tif (vma->addr > this->addr)\n\t\t\tptr = &parent->rb_right;\n\t\telse\n\t\t\tBUG();\n\t}\n\n\trb_link_node(&vma->tree, parent, ptr);\n\trb_insert_color(&vma->tree, &vmm->root);\n}\n\nstruct nvkm_vma *\nnvkm_vmm_node_search(struct nvkm_vmm *vmm, u64 addr)\n{\n\tstruct rb_node *node = vmm->root.rb_node;\n\twhile (node) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tif (addr < vma->addr)\n\t\t\tnode = node->rb_left;\n\t\telse\n\t\tif (addr >= vma->addr + vma->size)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn vma;\n\t}\n\treturn NULL;\n}\n\n#define node(root, dir) (((root)->head.dir == &vmm->list) ? NULL :             \\\n\tlist_entry((root)->head.dir, struct nvkm_vma, head))\n\nstatic struct nvkm_vma *\nnvkm_vmm_node_merge(struct nvkm_vmm *vmm, struct nvkm_vma *prev,\n\t\t    struct nvkm_vma *vma, struct nvkm_vma *next, u64 size)\n{\n\tif (next) {\n\t\tif (vma->size == size) {\n\t\t\tvma->size += next->size;\n\t\t\tnvkm_vmm_node_delete(vmm, next);\n\t\t\tif (prev) {\n\t\t\t\tprev->size += vma->size;\n\t\t\t\tnvkm_vmm_node_delete(vmm, vma);\n\t\t\t\treturn prev;\n\t\t\t}\n\t\t\treturn vma;\n\t\t}\n\t\tBUG_ON(prev);\n\n\t\tnvkm_vmm_node_remove(vmm, next);\n\t\tvma->size -= size;\n\t\tnext->addr -= size;\n\t\tnext->size += size;\n\t\tnvkm_vmm_node_insert(vmm, next);\n\t\treturn next;\n\t}\n\n\tif (prev) {\n\t\tif (vma->size != size) {\n\t\t\tnvkm_vmm_node_remove(vmm, vma);\n\t\t\tprev->size += size;\n\t\t\tvma->addr += size;\n\t\t\tvma->size -= size;\n\t\t\tnvkm_vmm_node_insert(vmm, vma);\n\t\t} else {\n\t\t\tprev->size += vma->size;\n\t\t\tnvkm_vmm_node_delete(vmm, vma);\n\t\t}\n\t\treturn prev;\n\t}\n\n\treturn vma;\n}\n\nstruct nvkm_vma *\nnvkm_vmm_node_split(struct nvkm_vmm *vmm,\n\t\t    struct nvkm_vma *vma, u64 addr, u64 size)\n{\n\tstruct nvkm_vma *prev = NULL;\n\n\tif (vma->addr != addr) {\n\t\tprev = vma;\n\t\tif (!(vma = nvkm_vma_tail(vma, vma->size + vma->addr - addr)))\n\t\t\treturn NULL;\n\t\tvma->part = true;\n\t\tnvkm_vmm_node_insert(vmm, vma);\n\t}\n\n\tif (vma->size != size) {\n\t\tstruct nvkm_vma *tmp;\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_node_merge(vmm, prev, vma, NULL, vma->size);\n\t\t\treturn NULL;\n\t\t}\n\t\ttmp->part = true;\n\t\tnvkm_vmm_node_insert(vmm, tmp);\n\t}\n\n\treturn vma;\n}\n\nstatic void\nnvkm_vma_dump(struct nvkm_vma *vma)\n{\n\tprintk(KERN_ERR \"%016llx %016llx %c%c%c%c%c%c%c%c %p\\n\",\n\t       vma->addr, (u64)vma->size,\n\t       vma->used ? '-' : 'F',\n\t       vma->mapref ? 'R' : '-',\n\t       vma->sparse ? 'S' : '-',\n\t       vma->page != NVKM_VMA_PAGE_NONE ? '0' + vma->page : '-',\n\t       vma->refd != NVKM_VMA_PAGE_NONE ? '0' + vma->refd : '-',\n\t       vma->part ? 'P' : '-',\n\t       vma->busy ? 'B' : '-',\n\t       vma->mapped ? 'M' : '-',\n\t       vma->memory);\n}\n\nstatic void\nnvkm_vmm_dump(struct nvkm_vmm *vmm)\n{\n\tstruct nvkm_vma *vma;\n\tlist_for_each_entry(vma, &vmm->list, head) {\n\t\tnvkm_vma_dump(vma);\n\t}\n}\n\nstatic void\nnvkm_vmm_dtor(struct nvkm_vmm *vmm)\n{\n\tstruct nvkm_vma *vma;\n\tstruct rb_node *node;\n\n\tif (0)\n\t\tnvkm_vmm_dump(vmm);\n\n\twhile ((node = rb_first(&vmm->root))) {\n\t\tstruct nvkm_vma *vma = rb_entry(node, typeof(*vma), tree);\n\t\tnvkm_vmm_put(vmm, &vma);\n\t}\n\n\tif (vmm->bootstrapped) {\n\t\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\t\tconst u64 limit = vmm->limit - vmm->start;\n\n\t\twhile (page[1].shift)\n\t\t\tpage++;\n\n\t\tnvkm_mmu_ptc_dump(vmm->mmu);\n\t\tnvkm_vmm_ptes_put(vmm, page, vmm->start, limit);\n\t}\n\n\tvma = list_first_entry(&vmm->list, typeof(*vma), head);\n\tlist_del(&vma->head);\n\tkfree(vma);\n\tWARN_ON(!list_empty(&vmm->list));\n\n\tif (vmm->nullp) {\n\t\tdma_free_coherent(vmm->mmu->subdev.device->dev, 16 * 1024,\n\t\t\t\t  vmm->nullp, vmm->null);\n\t}\n\n\tif (vmm->pd) {\n\t\tnvkm_mmu_ptc_put(vmm->mmu, true, &vmm->pd->pt[0]);\n\t\tnvkm_vmm_pt_del(&vmm->pd);\n\t}\n}\n\nstatic int\nnvkm_vmm_ctor_managed(struct nvkm_vmm *vmm, u64 addr, u64 size)\n{\n\tstruct nvkm_vma *vma;\n\tif (!(vma = nvkm_vma_new(addr, size)))\n\t\treturn -ENOMEM;\n\tvma->mapref = true;\n\tvma->sparse = false;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\tlist_add_tail(&vma->head, &vmm->list);\n\treturn 0;\n}\n\nstatic int\nnvkm_vmm_ctor(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 pd_header, bool managed, u64 addr, u64 size,\n\t      struct lock_class_key *key, const char *name,\n\t      struct nvkm_vmm *vmm)\n{\n\tstatic struct lock_class_key _key;\n\tconst struct nvkm_vmm_page *page = func->page;\n\tconst struct nvkm_vmm_desc *desc;\n\tstruct nvkm_vma *vma;\n\tint levels, bits = 0, ret;\n\n\tvmm->func = func;\n\tvmm->mmu = mmu;\n\tvmm->name = name;\n\tvmm->debug = mmu->subdev.debug;\n\tkref_init(&vmm->kref);\n\n\t__mutex_init(&vmm->mutex.vmm, \"&vmm->mutex.vmm\", key ? key : &_key);\n\tmutex_init(&vmm->mutex.ref);\n\tmutex_init(&vmm->mutex.map);\n\n\t \n\twhile (page[1].shift)\n\t\tpage++;\n\n\t \n\tfor (levels = 0, desc = page->desc; desc->bits; desc++, levels++)\n\t\tbits += desc->bits;\n\tbits += page->shift;\n\tdesc--;\n\n\tif (WARN_ON(levels > NVKM_VMM_LEVELS_MAX))\n\t\treturn -EINVAL;\n\n\t \n\tvmm->pd = nvkm_vmm_pt_new(desc, false, NULL);\n\tif (!vmm->pd)\n\t\treturn -ENOMEM;\n\tvmm->pd->refs[0] = 1;\n\tINIT_LIST_HEAD(&vmm->join);\n\n\t \n\tif (desc->size) {\n\t\tconst u32 size = pd_header + desc->size * (1 << desc->bits);\n\t\tvmm->pd->pt[0] = nvkm_mmu_ptc_get(mmu, size, desc->align, true);\n\t\tif (!vmm->pd->pt[0])\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tINIT_LIST_HEAD(&vmm->list);\n\tvmm->free = RB_ROOT;\n\tvmm->root = RB_ROOT;\n\n\tif (managed) {\n\t\t \n\t\tvmm->start = 0;\n\t\tvmm->limit = 1ULL << bits;\n\t\tif (addr + size < addr || addr + size > vmm->limit)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (addr && (ret = nvkm_vmm_ctor_managed(vmm, 0, addr)))\n\t\t\treturn ret;\n\n\t\tvmm->managed.p.addr = 0;\n\t\tvmm->managed.p.size = addr;\n\n\t\t \n\t\tif (size) {\n\t\t\tif (!(vma = nvkm_vma_new(addr, size)))\n\t\t\t\treturn -ENOMEM;\n\t\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\t\tlist_add_tail(&vma->head, &vmm->list);\n\t\t}\n\n\t\t \n\t\taddr = addr + size;\n\t\tsize = vmm->limit - addr;\n\t\tif (size && (ret = nvkm_vmm_ctor_managed(vmm, addr, size)))\n\t\t\treturn ret;\n\n\t\tvmm->managed.n.addr = addr;\n\t\tvmm->managed.n.size = size;\n\t} else {\n\t\t \n\t\tvmm->start = addr;\n\t\tvmm->limit = size ? (addr + size) : (1ULL << bits);\n\t\tif (vmm->start > vmm->limit || vmm->limit > (1ULL << bits))\n\t\t\treturn -EINVAL;\n\n\t\tif (!(vma = nvkm_vma_new(vmm->start, vmm->limit - vmm->start)))\n\t\t\treturn -ENOMEM;\n\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tlist_add(&vma->head, &vmm->list);\n\t}\n\n\treturn 0;\n}\n\nint\nnvkm_vmm_new_(const struct nvkm_vmm_func *func, struct nvkm_mmu *mmu,\n\t      u32 hdr, bool managed, u64 addr, u64 size,\n\t      struct lock_class_key *key, const char *name,\n\t      struct nvkm_vmm **pvmm)\n{\n\tif (!(*pvmm = kzalloc(sizeof(**pvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\treturn nvkm_vmm_ctor(func, mmu, hdr, managed, addr, size, key, name, *pvmm);\n}\n\nstatic struct nvkm_vma *\nnvkm_vmm_pfn_split_merge(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t\t u64 addr, u64 size, u8 page, bool map)\n{\n\tstruct nvkm_vma *prev = NULL;\n\tstruct nvkm_vma *next = NULL;\n\n\tif (vma->addr == addr && vma->part && (prev = node(vma, prev))) {\n\t\tif (prev->memory || prev->mapped != map)\n\t\t\tprev = NULL;\n\t}\n\n\tif (vma->addr + vma->size == addr + size && (next = node(vma, next))) {\n\t\tif (!next->part ||\n\t\t    next->memory || next->mapped != map)\n\t\t\tnext = NULL;\n\t}\n\n\tif (prev || next)\n\t\treturn nvkm_vmm_node_merge(vmm, prev, vma, next, size);\n\treturn nvkm_vmm_node_split(vmm, vma, addr, size);\n}\n\nint\nnvkm_vmm_pfn_unmap(struct nvkm_vmm *vmm, u64 addr, u64 size)\n{\n\tstruct nvkm_vma *vma = nvkm_vmm_node_search(vmm, addr);\n\tstruct nvkm_vma *next;\n\tu64 limit = addr + size;\n\tu64 start = addr;\n\n\tif (!vma)\n\t\treturn -EINVAL;\n\n\tdo {\n\t\tif (!vma->mapped || vma->memory)\n\t\t\tcontinue;\n\n\t\tsize = min(limit - start, vma->size - (start - vma->addr));\n\n\t\tnvkm_vmm_ptes_unmap_put(vmm, &vmm->func->page[vma->refd],\n\t\t\t\t\tstart, size, false, true);\n\n\t\tnext = nvkm_vmm_pfn_split_merge(vmm, vma, start, size, 0, false);\n\t\tif (!WARN_ON(!next)) {\n\t\t\tvma = next;\n\t\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t\t\tvma->mapped = false;\n\t\t}\n\t} while ((vma = node(vma, next)) && (start = vma->addr) < limit);\n\n\treturn 0;\n}\n\n \nint\nnvkm_vmm_pfn_map(struct nvkm_vmm *vmm, u8 shift, u64 addr, u64 size, u64 *pfn)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *vma, *tmp;\n\tu64 limit = addr + size;\n\tu64 start = addr;\n\tint pm = size >> shift;\n\tint pi = 0;\n\n\t \n\twhile (page->shift && (page->shift != shift ||\n\t       page->desc->func->pfn == NULL))\n\t\tpage++;\n\n\tif (!page->shift || !IS_ALIGNED(addr, 1ULL << shift) ||\n\t\t\t    !IS_ALIGNED(size, 1ULL << shift) ||\n\t    addr + size < addr || addr + size > vmm->limit) {\n\t\tVMM_DEBUG(vmm, \"paged map %d %d %016llx %016llx\\n\",\n\t\t\t  shift, page->shift, addr, size);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(vma = nvkm_vmm_node_search(vmm, addr)))\n\t\treturn -ENOENT;\n\n\tdo {\n\t\tbool map = !!(pfn[pi] & NVKM_VMM_PFN_V);\n\t\tbool mapped = vma->mapped;\n\t\tu64 size = limit - start;\n\t\tu64 addr = start;\n\t\tint pn, ret = 0;\n\n\t\t \n\t\tfor (pn = 0; pi + pn < pm; pn++) {\n\t\t\tif (map != !!(pfn[pi + pn] & NVKM_VMM_PFN_V))\n\t\t\t\tbreak;\n\t\t}\n\t\tsize = min_t(u64, size, pn << page->shift);\n\t\tsize = min_t(u64, size, vma->size + vma->addr - addr);\n\n\t\t \n\t\tif (!vma->mapref || vma->memory) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (map != mapped) {\n\t\t\ttmp = nvkm_vmm_pfn_split_merge(vmm, vma, addr, size,\n\t\t\t\t\t\t       page -\n\t\t\t\t\t\t       vmm->func->page, map);\n\t\t\tif (WARN_ON(!tmp)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tif ((tmp->mapped = map))\n\t\t\t\ttmp->refd = page - vmm->func->page;\n\t\t\telse\n\t\t\t\ttmp->refd = NVKM_VMA_PAGE_NONE;\n\t\t\tvma = tmp;\n\t\t}\n\n\t\t \n\t\tif (map) {\n\t\t\tstruct nvkm_vmm_map args;\n\t\t\targs.page = page;\n\t\t\targs.pfn = &pfn[pi];\n\n\t\t\tif (!mapped) {\n\t\t\t\tret = nvkm_vmm_ptes_get_map(vmm, page, addr,\n\t\t\t\t\t\t\t    size, &args, page->\n\t\t\t\t\t\t\t    desc->func->pfn);\n\t\t\t} else {\n\t\t\t\tnvkm_vmm_ptes_map(vmm, page, addr, size, &args,\n\t\t\t\t\t\t  page->desc->func->pfn);\n\t\t\t}\n\t\t} else {\n\t\t\tif (mapped) {\n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, page, addr, size,\n\t\t\t\t\t\t\tfalse, true);\n\t\t\t}\n\t\t}\n\nnext:\n\t\t \n\t\tif (vma->addr + vma->size == addr + size)\n\t\t\tvma = node(vma, next);\n\t\tstart += size;\n\n\t\tif (ret) {\n\t\t\t \n\t\t\twhile (size) {\n\t\t\t\tpfn[pi++] = NVKM_VMM_PFN_NONE;\n\t\t\t\tsize -= 1 << page->shift;\n\t\t\t}\n\t\t} else {\n\t\t\tpi += size >> page->shift;\n\t\t}\n\t} while (vma && start < limit);\n\n\treturn 0;\n}\n\nvoid\nnvkm_vmm_unmap_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev = NULL;\n\tstruct nvkm_vma *next;\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\tvma->mapped = false;\n\n\tif (vma->part && (prev = node(vma, prev)) && prev->mapped)\n\t\tprev = NULL;\n\tif ((next = node(vma, next)) && (!next->part || next->mapped))\n\t\tnext = NULL;\n\tnvkm_vmm_node_merge(vmm, prev, vma, next, vma->size);\n}\n\nvoid\nnvkm_vmm_unmap_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma, bool pfn)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[vma->refd];\n\n\tif (vma->mapref) {\n\t\tnvkm_vmm_ptes_unmap_put(vmm, page, vma->addr, vma->size, vma->sparse, pfn);\n\t\tvma->refd = NVKM_VMA_PAGE_NONE;\n\t} else {\n\t\tnvkm_vmm_ptes_unmap(vmm, page, vma->addr, vma->size, vma->sparse, pfn);\n\t}\n\n\tnvkm_vmm_unmap_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_unmap(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tif (vma->memory) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tnvkm_vmm_unmap_locked(vmm, vma, false);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t}\n}\n\nstatic int\nnvkm_vmm_map_valid(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t   void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tswitch (nvkm_memory_target(map->memory)) {\n\tcase NVKM_MEM_TARGET_VRAM:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_VRAM)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !VRAM\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tcase NVKM_MEM_TARGET_HOST:\n\tcase NVKM_MEM_TARGET_NCOH:\n\t\tif (!(map->page->type & NVKM_VMM_PAGE_HOST)) {\n\t\t\tVMM_DEBUG(vmm, \"%d !HOST\", map->page->shift);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOSYS;\n\t}\n\n\tif (!IS_ALIGNED(     vma->addr, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED((u64)vma->size, 1ULL << map->page->shift) ||\n\t    !IS_ALIGNED(   map->offset, 1ULL << map->page->shift) ||\n\t    nvkm_memory_page(map->memory) < map->page->shift) {\n\t\tVMM_DEBUG(vmm, \"alignment %016llx %016llx %016llx %d %d\",\n\t\t    vma->addr, (u64)vma->size, map->offset, map->page->shift,\n\t\t    nvkm_memory_page(map->memory));\n\t\treturn -EINVAL;\n\t}\n\n\treturn vmm->func->valid(vmm, argv, argc, map);\n}\n\nstatic int\nnvkm_vmm_map_choose(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tfor (map->page = vmm->func->page; map->page->shift; map->page++) {\n\t\tVMM_DEBUG(vmm, \"trying %d\", map->page->shift);\n\t\tif (!nvkm_vmm_map_valid(vmm, vma, argv, argc, map))\n\t\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_vmm_map_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma,\n\t\t    void *argv, u32 argc, struct nvkm_vmm_map *map)\n{\n\tnvkm_vmm_pte_func func;\n\tint ret;\n\n\tmap->no_comp = vma->no_comp;\n\n\t \n\tif (unlikely(nvkm_memory_size(map->memory) < map->offset + vma->size)) {\n\t\tVMM_DEBUG(vmm, \"overrun %016llx %016llx %016llx\",\n\t\t\t  nvkm_memory_size(map->memory),\n\t\t\t  map->offset, (u64)vma->size);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (vma->page == NVKM_VMA_PAGE_NONE &&\n\t    vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\t \n\t\tconst u32 debug = vmm->debug;\n\t\tvmm->debug = 0;\n\t\tret = nvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\tvmm->debug = debug;\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid at any page size\");\n\t\t\tnvkm_vmm_map_choose(vmm, vma, argv, argc, map);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\t \n\t\tif (vma->refd != NVKM_VMA_PAGE_NONE)\n\t\t\tmap->page = &vmm->func->page[vma->refd];\n\t\telse\n\t\t\tmap->page = &vmm->func->page[vma->page];\n\n\t\tret = nvkm_vmm_map_valid(vmm, vma, argv, argc, map);\n\t\tif (ret) {\n\t\t\tVMM_DEBUG(vmm, \"invalid %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tmap->off = map->offset;\n\tif (map->mem) {\n\t\tfor (; map->off; map->mem = map->mem->next) {\n\t\t\tu64 size = (u64)map->mem->length << NVKM_RAM_MM_SHIFT;\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->mem;\n\t} else\n\tif (map->sgl) {\n\t\tfor (; map->off; map->sgl = sg_next(map->sgl)) {\n\t\t\tu64 size = sg_dma_len(map->sgl);\n\t\t\tif (size > map->off)\n\t\t\t\tbreak;\n\t\t\tmap->off -= size;\n\t\t}\n\t\tfunc = map->page->desc->func->sgl;\n\t} else {\n\t\tmap->dma += map->offset >> PAGE_SHIFT;\n\t\tmap->off  = map->offset & PAGE_MASK;\n\t\tfunc = map->page->desc->func->dma;\n\t}\n\n\t \n\tif (vma->refd == NVKM_VMA_PAGE_NONE) {\n\t\tret = nvkm_vmm_ptes_get_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvma->refd = map->page - vmm->func->page;\n\t} else {\n\t\tnvkm_vmm_ptes_map(vmm, map->page, vma->addr, vma->size, map, func);\n\t}\n\n\tnvkm_memory_tags_put(vma->memory, vmm->mmu->subdev.device, &vma->tags);\n\tnvkm_memory_unref(&vma->memory);\n\tvma->memory = nvkm_memory_ref(map->memory);\n\tvma->mapped = true;\n\tvma->tags = map->tags;\n\treturn 0;\n}\n\nint\nnvkm_vmm_map(struct nvkm_vmm *vmm, struct nvkm_vma *vma, void *argv, u32 argc,\n\t     struct nvkm_vmm_map *map)\n{\n\tint ret;\n\n\tif (nvkm_vmm_in_managed_range(vmm, vma->addr, vma->size) &&\n\t    vmm->managed.raw)\n\t\treturn nvkm_vmm_map_locked(vmm, vma, argv, argc, map);\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tret = nvkm_vmm_map_locked(vmm, vma, argv, argc, map);\n\tvma->busy = false;\n\tmutex_unlock(&vmm->mutex.vmm);\n\treturn ret;\n}\n\nstatic void\nnvkm_vmm_put_region(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tstruct nvkm_vma *prev, *next;\n\n\tif ((prev = node(vma, prev)) && !prev->used) {\n\t\tvma->addr  = prev->addr;\n\t\tvma->size += prev->size;\n\t\tnvkm_vmm_free_delete(vmm, prev);\n\t}\n\n\tif ((next = node(vma, next)) && !next->used) {\n\t\tvma->size += next->size;\n\t\tnvkm_vmm_free_delete(vmm, next);\n\t}\n\n\tnvkm_vmm_free_insert(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put_locked(struct nvkm_vmm *vmm, struct nvkm_vma *vma)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tstruct nvkm_vma *next = vma;\n\n\tBUG_ON(vma->part);\n\n\tif (vma->mapref || !vma->sparse) {\n\t\tdo {\n\t\t\tconst bool mem = next->memory != NULL;\n\t\t\tconst bool map = next->mapped;\n\t\t\tconst u8  refd = next->refd;\n\t\t\tconst u64 addr = next->addr;\n\t\t\tu64 size = next->size;\n\n\t\t\t \n\t\t\twhile ((next = node(next, next)) && next->part &&\n\t\t\t       (next->mapped == map) &&\n\t\t\t       (next->memory != NULL) == mem &&\n\t\t\t       (next->refd == refd))\n\t\t\t\tsize += next->size;\n\n\t\t\tif (map) {\n\t\t\t\t \n\t\t\t\tnvkm_vmm_ptes_unmap_put(vmm, &page[refd], addr,\n\t\t\t\t\t\t\tsize, vma->sparse,\n\t\t\t\t\t\t\t!mem);\n\t\t\t} else\n\t\t\tif (refd != NVKM_VMA_PAGE_NONE) {\n\t\t\t\t \n\t\t\t\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n\t\t\t}\n\t\t} while (next && next->part);\n\t}\n\n\t \n\tnext = vma;\n\tdo {\n\t\tif (next->mapped)\n\t\t\tnvkm_vmm_unmap_region(vmm, next);\n\t} while ((next = node(vma, next)) && next->part);\n\n\tif (vma->sparse && !vma->mapref) {\n\t\t \n\t\tnvkm_vmm_ptes_sparse_put(vmm, &page[vma->refd], vma->addr, vma->size);\n\t} else\n\tif (vma->sparse) {\n\t\t \n\t\tnvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, false);\n\t}\n\n\t \n\tnvkm_vmm_node_remove(vmm, vma);\n\n\t \n\tvma->page = NVKM_VMA_PAGE_NONE;\n\tvma->refd = NVKM_VMA_PAGE_NONE;\n\tvma->used = false;\n\tnvkm_vmm_put_region(vmm, vma);\n}\n\nvoid\nnvkm_vmm_put(struct nvkm_vmm *vmm, struct nvkm_vma **pvma)\n{\n\tstruct nvkm_vma *vma = *pvma;\n\tif (vma) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tnvkm_vmm_put_locked(vmm, vma);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t\t*pvma = NULL;\n\t}\n}\n\nint\nnvkm_vmm_get_locked(struct nvkm_vmm *vmm, bool getref, bool mapref, bool sparse,\n\t\t    u8 shift, u8 align, u64 size, struct nvkm_vma **pvma)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[NVKM_VMA_PAGE_NONE];\n\tstruct rb_node *node = NULL, *temp;\n\tstruct nvkm_vma *vma = NULL, *tmp;\n\tu64 addr, tail;\n\tint ret;\n\n\tVMM_TRACE(vmm, \"getref %d mapref %d sparse %d \"\n\t\t       \"shift: %d align: %d size: %016llx\",\n\t\t  getref, mapref, sparse, shift, align, size);\n\n\t \n\tif (unlikely(!size || (!getref && !mapref && sparse))) {\n\t\tVMM_DEBUG(vmm, \"args %016llx %d %d %d\",\n\t\t\t  size, getref, mapref, sparse);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (unlikely((getref || vmm->func->page_block) && !shift)) {\n\t\tVMM_DEBUG(vmm, \"page size required: %d %016llx\",\n\t\t\t  getref, vmm->func->page_block);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (shift) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\talign = max_t(u8, align, shift);\n\t} else {\n\t\talign = max_t(u8, align, 12);\n\t}\n\n\t \n\ttemp = vmm->free.rb_node;\n\twhile (temp) {\n\t\tstruct nvkm_vma *this = rb_entry(temp, typeof(*this), tree);\n\t\tif (this->size < size) {\n\t\t\ttemp = temp->rb_right;\n\t\t} else {\n\t\t\tnode = temp;\n\t\t\ttemp = temp->rb_left;\n\t\t}\n\t}\n\n\tif (unlikely(!node))\n\t\treturn -ENOSPC;\n\n\t \n\tdo {\n\t\tstruct nvkm_vma *this = rb_entry(node, typeof(*this), tree);\n\t\tstruct nvkm_vma *prev = node(this, prev);\n\t\tstruct nvkm_vma *next = node(this, next);\n\t\tconst int p = page - vmm->func->page;\n\n\t\taddr = this->addr;\n\t\tif (vmm->func->page_block && prev && prev->page != p)\n\t\t\taddr = ALIGN(addr, vmm->func->page_block);\n\t\taddr = ALIGN(addr, 1ULL << align);\n\n\t\ttail = this->addr + this->size;\n\t\tif (vmm->func->page_block && next && next->page != p)\n\t\t\ttail = ALIGN_DOWN(tail, vmm->func->page_block);\n\n\t\tif (addr <= tail && tail - addr >= size) {\n\t\t\tnvkm_vmm_free_remove(vmm, this);\n\t\t\tvma = this;\n\t\t\tbreak;\n\t\t}\n\t} while ((node = rb_next(node)));\n\n\tif (unlikely(!vma))\n\t\treturn -ENOSPC;\n\n\t \n\tif (addr != vma->addr) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size + vma->addr - addr))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, vma);\n\t\tvma = tmp;\n\t}\n\n\tif (size != vma->size) {\n\t\tif (!(tmp = nvkm_vma_tail(vma, vma->size - size))) {\n\t\t\tnvkm_vmm_put_region(vmm, vma);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tnvkm_vmm_free_insert(vmm, tmp);\n\t}\n\n\t \n\tif (sparse && getref)\n\t\tret = nvkm_vmm_ptes_sparse_get(vmm, page, vma->addr, vma->size);\n\telse if (sparse)\n\t\tret = nvkm_vmm_ptes_sparse(vmm, vma->addr, vma->size, true);\n\telse if (getref)\n\t\tret = nvkm_vmm_ptes_get(vmm, page, vma->addr, vma->size);\n\telse\n\t\tret = 0;\n\tif (ret) {\n\t\tnvkm_vmm_put_region(vmm, vma);\n\t\treturn ret;\n\t}\n\n\tvma->mapref = mapref && !getref;\n\tvma->sparse = sparse;\n\tvma->page = page - vmm->func->page;\n\tvma->refd = getref ? vma->page : NVKM_VMA_PAGE_NONE;\n\tvma->used = true;\n\tnvkm_vmm_node_insert(vmm, vma);\n\t*pvma = vma;\n\treturn 0;\n}\n\nint\nnvkm_vmm_get(struct nvkm_vmm *vmm, u8 page, u64 size, struct nvkm_vma **pvma)\n{\n\tint ret;\n\tmutex_lock(&vmm->mutex.vmm);\n\tret = nvkm_vmm_get_locked(vmm, false, true, false, page, 0, size, pvma);\n\tmutex_unlock(&vmm->mutex.vmm);\n\treturn ret;\n}\n\nvoid\nnvkm_vmm_raw_unmap(struct nvkm_vmm *vmm, u64 addr, u64 size,\n\t\t   bool sparse, u8 refd)\n{\n\tconst struct nvkm_vmm_page *page = &vmm->func->page[refd];\n\n\tnvkm_vmm_ptes_unmap(vmm, page, addr, size, sparse, false);\n}\n\nvoid\nnvkm_vmm_raw_put(struct nvkm_vmm *vmm, u64 addr, u64 size, u8 refd)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\n\tnvkm_vmm_ptes_put(vmm, &page[refd], addr, size);\n}\n\nint\nnvkm_vmm_raw_get(struct nvkm_vmm *vmm, u64 addr, u64 size, u8 refd)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\n\tif (unlikely(!size))\n\t\treturn -EINVAL;\n\n\treturn nvkm_vmm_ptes_get(vmm, &page[refd], addr, size);\n}\n\nint\nnvkm_vmm_raw_sparse(struct nvkm_vmm *vmm, u64 addr, u64 size, bool ref)\n{\n\tint ret;\n\n\tmutex_lock(&vmm->mutex.ref);\n\tret = nvkm_vmm_ptes_sparse(vmm, addr, size, ref);\n\tmutex_unlock(&vmm->mutex.ref);\n\n\treturn ret;\n}\n\nvoid\nnvkm_vmm_part(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tif (inst && vmm && vmm->func->part) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tvmm->func->part(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t}\n}\n\nint\nnvkm_vmm_join(struct nvkm_vmm *vmm, struct nvkm_memory *inst)\n{\n\tint ret = 0;\n\tif (vmm->func->join) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tret = vmm->func->join(vmm, inst);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t}\n\treturn ret;\n}\n\nstatic bool\nnvkm_vmm_boot_ptes(struct nvkm_vmm_iter *it, bool pfn, u32 ptei, u32 ptes)\n{\n\tconst struct nvkm_vmm_desc *desc = it->desc;\n\tconst int type = desc->type == SPT;\n\tnvkm_memory_boot(it->pt[0]->pt[type]->memory, it->vmm);\n\treturn false;\n}\n\nint\nnvkm_vmm_boot(struct nvkm_vmm *vmm)\n{\n\tconst struct nvkm_vmm_page *page = vmm->func->page;\n\tconst u64 limit = vmm->limit - vmm->start;\n\tint ret;\n\n\twhile (page[1].shift)\n\t\tpage++;\n\n\tret = nvkm_vmm_ptes_get(vmm, page, vmm->start, limit);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_vmm_iter(vmm, page, vmm->start, limit, \"bootstrap\", false, false,\n\t\t      nvkm_vmm_boot_ptes, NULL, NULL, NULL);\n\tvmm->bootstrapped = true;\n\treturn 0;\n}\n\nstatic void\nnvkm_vmm_del(struct kref *kref)\n{\n\tstruct nvkm_vmm *vmm = container_of(kref, typeof(*vmm), kref);\n\tnvkm_vmm_dtor(vmm);\n\tkfree(vmm);\n}\n\nvoid\nnvkm_vmm_unref(struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_vmm *vmm = *pvmm;\n\tif (vmm) {\n\t\tkref_put(&vmm->kref, nvkm_vmm_del);\n\t\t*pvmm = NULL;\n\t}\n}\n\nstruct nvkm_vmm *\nnvkm_vmm_ref(struct nvkm_vmm *vmm)\n{\n\tif (vmm)\n\t\tkref_get(&vmm->kref);\n\treturn vmm;\n}\n\nint\nnvkm_vmm_new(struct nvkm_device *device, u64 addr, u64 size, void *argv,\n\t     u32 argc, struct lock_class_key *key, const char *name,\n\t     struct nvkm_vmm **pvmm)\n{\n\tstruct nvkm_mmu *mmu = device->mmu;\n\tstruct nvkm_vmm *vmm = NULL;\n\tint ret;\n\tret = mmu->func->vmm.ctor(mmu, false, addr, size, argv, argc,\n\t\t\t\t  key, name, &vmm);\n\tif (ret)\n\t\tnvkm_vmm_unref(&vmm);\n\t*pvmm = vmm;\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}