{
  "module_name": "uvmm.c",
  "hash_id": "157c14e3cc4a971745edb1c46ef1c33eb8571a069a75fd50451d767c6af48b47",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/uvmm.c",
  "human_readable_source": " \n#include \"uvmm.h\"\n#include \"umem.h\"\n#include \"ummu.h\"\n\n#include <core/client.h>\n#include <core/memory.h>\n\n#include <nvif/if000c.h>\n#include <nvif/unpack.h>\n\nstatic const struct nvkm_object_func nvkm_uvmm;\nstruct nvkm_vmm *\nnvkm_uvmm_search(struct nvkm_client *client, u64 handle)\n{\n\tstruct nvkm_object *object;\n\n\tobject = nvkm_object_search(client, handle, &nvkm_uvmm);\n\tif (IS_ERR(object))\n\t\treturn (void *)object;\n\n\treturn nvkm_vmm_ref(nvkm_uvmm(object)->vmm);\n}\n\nstatic int\nnvkm_uvmm_mthd_pfnclr(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_pfnclr_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tint ret = -ENOSYS;\n\tu64 addr, size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tif (nvkm_vmm_in_managed_range(vmm, addr, size) && vmm->managed.raw)\n\t\treturn -EINVAL;\n\n\tif (size) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tret = nvkm_vmm_pfn_unmap(vmm, addr, size);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t}\n\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_pfnmap(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_pfnmap_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tint ret = -ENOSYS;\n\tu64 addr, size, *phys;\n\tu8  page;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\tpage = args->v0.page;\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\tphys = args->v0.phys;\n\t\tif (argc != (size >> page) * sizeof(args->v0.phys[0]))\n\t\t\treturn -EINVAL;\n\t} else\n\t\treturn ret;\n\n\tif (nvkm_vmm_in_managed_range(vmm, addr, size) && vmm->managed.raw)\n\t\treturn -EINVAL;\n\n\tif (size) {\n\t\tmutex_lock(&vmm->mutex.vmm);\n\t\tret = nvkm_vmm_pfn_map(vmm, page, addr, size, phys);\n\t\tmutex_unlock(&vmm->mutex.vmm);\n\t}\n\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_unmap(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_unmap_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tif (nvkm_vmm_in_managed_range(vmm, addr, 0) && vmm->managed.raw)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tvma = nvkm_vmm_node_search(vmm, addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx\",\n\t\t\t  addr, vma ? vma->addr : ~0ULL);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d\", addr, vma->busy);\n\t\tgoto done;\n\t}\n\n\tif (ret = -EINVAL, !vma->memory) {\n\t\tVMM_DEBUG(vmm, \"unmapped\");\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_unmap_locked(vmm, vma, false);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex.vmm);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_map(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tunion {\n\t\tstruct nvif_vmm_map_v0 v0;\n\t} *args = argv;\n\tu64 addr, size, handle, offset;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tstruct nvkm_memory *memory;\n\tint ret = -ENOSYS;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true))) {\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t\thandle = args->v0.memory;\n\t\toffset = args->v0.offset;\n\t} else\n\t\treturn ret;\n\n\tif (nvkm_vmm_in_managed_range(vmm, addr, size) && vmm->managed.raw)\n\t\treturn -EINVAL;\n\n\tmemory = nvkm_umem_search(client, handle);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tif (ret = -ENOENT, !(vma = nvkm_vmm_node_search(vmm, addr))) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -ENOENT, vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d\", addr, vma->busy);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->mapped && !vma->memory) {\n\t\tVMM_DEBUG(vmm, \"pfnmap %016llx\", addr);\n\t\tgoto fail;\n\t}\n\n\tif (ret = -EINVAL, vma->addr != addr || vma->size != size) {\n\t\tif (addr + size > vma->addr + vma->size || vma->memory ||\n\t\t    (vma->refd == NVKM_VMA_PAGE_NONE && !vma->mapref)) {\n\t\t\tVMM_DEBUG(vmm, \"split %d %d %d \"\n\t\t\t\t       \"%016llx %016llx %016llx %016llx\",\n\t\t\t\t  !!vma->memory, vma->refd, vma->mapref,\n\t\t\t\t  addr, size, vma->addr, (u64)vma->size);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tvma = nvkm_vmm_node_split(vmm, vma, addr, size);\n\t\tif (!vma) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tvma->busy = true;\n\tmutex_unlock(&vmm->mutex.vmm);\n\n\tret = nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n\tif (ret == 0) {\n\t\t \n\t\tnvkm_memory_unref(&memory);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tvma->busy = false;\n\tnvkm_vmm_unmap_region(vmm, vma);\nfail:\n\tmutex_unlock(&vmm->mutex.vmm);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_put(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_put_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tu64 addr;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\taddr = args->v0.addr;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tvma = nvkm_vmm_node_search(vmm, args->v0.addr);\n\tif (ret = -ENOENT, !vma || vma->addr != addr || vma->part) {\n\t\tVMM_DEBUG(vmm, \"lookup %016llx: %016llx %d\", addr,\n\t\t\t  vma ? vma->addr : ~0ULL, vma ? vma->part : 0);\n\t\tgoto done;\n\t}\n\n\tif (ret = -ENOENT, vma->busy) {\n\t\tVMM_DEBUG(vmm, \"denied %016llx: %d\", addr, vma->busy);\n\t\tgoto done;\n\t}\n\n\tnvkm_vmm_put_locked(vmm, vma);\n\tret = 0;\ndone:\n\tmutex_unlock(&vmm->mutex.vmm);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_get(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_get_v0 v0;\n\t} *args = argv;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma *vma;\n\tint ret = -ENOSYS;\n\tbool getref, mapref, sparse;\n\tu8 page, align;\n\tu64 size;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tgetref = args->v0.type == NVIF_VMM_GET_V0_PTES;\n\t\tmapref = args->v0.type == NVIF_VMM_GET_V0_ADDR;\n\t\tsparse = args->v0.sparse;\n\t\tpage = args->v0.page;\n\t\talign = args->v0.align;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tmutex_lock(&vmm->mutex.vmm);\n\tret = nvkm_vmm_get_locked(vmm, getref, mapref, sparse,\n\t\t\t\t  page, align, size, &vma);\n\tmutex_unlock(&vmm->mutex.vmm);\n\tif (ret)\n\t\treturn ret;\n\n\targs->v0.addr = vma->addr;\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_page(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_page_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tint ret = -ENOSYS;\n\tu8 type, index, nr;\n\n\tpage = uvmm->vmm->func->page;\n\tfor (nr = 0; page[nr].shift; nr++);\n\n\tif (!(nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tif ((index = args->v0.index) >= nr)\n\t\t\treturn -EINVAL;\n\t\ttype = page[index].type;\n\t\targs->v0.shift = page[index].shift;\n\t\targs->v0.sparse = !!(type & NVKM_VMM_PAGE_SPARSE);\n\t\targs->v0.vram = !!(type & NVKM_VMM_PAGE_VRAM);\n\t\targs->v0.host = !!(type & NVKM_VMM_PAGE_HOST);\n\t\targs->v0.comp = !!(type & NVKM_VMM_PAGE_COMP);\n\t} else\n\t\treturn -ENOSYS;\n\n\treturn 0;\n}\n\nstatic inline int\nnvkm_uvmm_page_index(struct nvkm_uvmm *uvmm, u64 size, u8 shift, u8 *refd)\n{\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tconst struct nvkm_vmm_page *page;\n\n\tif (likely(shift)) {\n\t\tfor (page = vmm->func->page; page->shift; page++) {\n\t\t\tif (shift == page->shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!page->shift || !IS_ALIGNED(size, 1ULL << page->shift)) {\n\t\t\tVMM_DEBUG(vmm, \"page %d %016llx\", shift, size);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\t*refd = page - vmm->func->page;\n\n\treturn 0;\n}\n\nstatic int\nnvkm_uvmm_mthd_raw_get(struct nvkm_uvmm *uvmm, struct nvif_vmm_raw_v0 *args)\n{\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tu8 refd;\n\tint ret;\n\n\tif (!nvkm_vmm_in_managed_range(vmm, args->addr, args->size))\n\t\treturn -EINVAL;\n\n\tret = nvkm_uvmm_page_index(uvmm, args->size, args->shift, &refd);\n\tif (ret)\n\t\treturn ret;\n\n\treturn nvkm_vmm_raw_get(vmm, args->addr, args->size, refd);\n}\n\nstatic int\nnvkm_uvmm_mthd_raw_put(struct nvkm_uvmm *uvmm, struct nvif_vmm_raw_v0 *args)\n{\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tu8 refd;\n\tint ret;\n\n\tif (!nvkm_vmm_in_managed_range(vmm, args->addr, args->size))\n\t\treturn -EINVAL;\n\n\tret = nvkm_uvmm_page_index(uvmm, args->size, args->shift, &refd);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_vmm_raw_put(vmm, args->addr, args->size, refd);\n\n\treturn 0;\n}\n\nstatic int\nnvkm_uvmm_mthd_raw_map(struct nvkm_uvmm *uvmm, struct nvif_vmm_raw_v0 *args)\n{\n\tstruct nvkm_client *client = uvmm->object.client;\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tstruct nvkm_vma vma = {\n\t\t.addr = args->addr,\n\t\t.size = args->size,\n\t\t.used = true,\n\t\t.mapref = false,\n\t\t.no_comp = true,\n\t};\n\tstruct nvkm_memory *memory;\n\tvoid *argv = (void *)(uintptr_t)args->argv;\n\tunsigned int argc = args->argc;\n\tu64 handle = args->memory;\n\tu8 refd;\n\tint ret;\n\n\tif (!nvkm_vmm_in_managed_range(vmm, args->addr, args->size))\n\t\treturn -EINVAL;\n\n\tret = nvkm_uvmm_page_index(uvmm, args->size, args->shift, &refd);\n\tif (ret)\n\t\treturn ret;\n\n\tvma.page = vma.refd = refd;\n\n\tmemory = nvkm_umem_search(client, args->memory);\n\tif (IS_ERR(memory)) {\n\t\tVMM_DEBUG(vmm, \"memory %016llx %ld\\n\", handle, PTR_ERR(memory));\n\t\treturn PTR_ERR(memory);\n\t}\n\n\tret = nvkm_memory_map(memory, args->offset, vmm, &vma, argv, argc);\n\n\tnvkm_memory_unref(&vma.memory);\n\tnvkm_memory_unref(&memory);\n\treturn ret;\n}\n\nstatic int\nnvkm_uvmm_mthd_raw_unmap(struct nvkm_uvmm *uvmm, struct nvif_vmm_raw_v0 *args)\n{\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\tu8 refd;\n\tint ret;\n\n\tif (!nvkm_vmm_in_managed_range(vmm, args->addr, args->size))\n\t\treturn -EINVAL;\n\n\tret = nvkm_uvmm_page_index(uvmm, args->size, args->shift, &refd);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_vmm_raw_unmap(vmm, args->addr, args->size,\n\t\t\t   args->sparse, refd);\n\n\treturn 0;\n}\n\nstatic int\nnvkm_uvmm_mthd_raw_sparse(struct nvkm_uvmm *uvmm, struct nvif_vmm_raw_v0 *args)\n{\n\tstruct nvkm_vmm *vmm = uvmm->vmm;\n\n\tif (!nvkm_vmm_in_managed_range(vmm, args->addr, args->size))\n\t\treturn -EINVAL;\n\n\treturn nvkm_vmm_raw_sparse(vmm, args->addr, args->size, args->ref);\n}\n\nstatic int\nnvkm_uvmm_mthd_raw(struct nvkm_uvmm *uvmm, void *argv, u32 argc)\n{\n\tunion {\n\t\tstruct nvif_vmm_raw_v0 v0;\n\t} *args = argv;\n\tint ret = -ENOSYS;\n\n\tif (!uvmm->vmm->managed.raw)\n\t\treturn -EINVAL;\n\n\tif ((ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, true)))\n\t\treturn ret;\n\n\tswitch (args->v0.op) {\n\tcase NVIF_VMM_RAW_V0_GET:\n\t\treturn nvkm_uvmm_mthd_raw_get(uvmm, &args->v0);\n\tcase NVIF_VMM_RAW_V0_PUT:\n\t\treturn nvkm_uvmm_mthd_raw_put(uvmm, &args->v0);\n\tcase NVIF_VMM_RAW_V0_MAP:\n\t\treturn nvkm_uvmm_mthd_raw_map(uvmm, &args->v0);\n\tcase NVIF_VMM_RAW_V0_UNMAP:\n\t\treturn nvkm_uvmm_mthd_raw_unmap(uvmm, &args->v0);\n\tcase NVIF_VMM_RAW_V0_SPARSE:\n\t\treturn nvkm_uvmm_mthd_raw_sparse(uvmm, &args->v0);\n\tdefault:\n\t\treturn -EINVAL;\n\t};\n}\n\nstatic int\nnvkm_uvmm_mthd(struct nvkm_object *object, u32 mthd, void *argv, u32 argc)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tswitch (mthd) {\n\tcase NVIF_VMM_V0_PAGE  : return nvkm_uvmm_mthd_page  (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_GET   : return nvkm_uvmm_mthd_get   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_PUT   : return nvkm_uvmm_mthd_put   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_MAP   : return nvkm_uvmm_mthd_map   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_UNMAP : return nvkm_uvmm_mthd_unmap (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_PFNMAP: return nvkm_uvmm_mthd_pfnmap(uvmm, argv, argc);\n\tcase NVIF_VMM_V0_PFNCLR: return nvkm_uvmm_mthd_pfnclr(uvmm, argv, argc);\n\tcase NVIF_VMM_V0_RAW   : return nvkm_uvmm_mthd_raw   (uvmm, argv, argc);\n\tcase NVIF_VMM_V0_MTHD(0x00) ... NVIF_VMM_V0_MTHD(0x7f):\n\t\tif (uvmm->vmm->func->mthd) {\n\t\t\treturn uvmm->vmm->func->mthd(uvmm->vmm,\n\t\t\t\t\t\t     uvmm->object.client,\n\t\t\t\t\t\t     mthd, argv, argc);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic void *\nnvkm_uvmm_dtor(struct nvkm_object *object)\n{\n\tstruct nvkm_uvmm *uvmm = nvkm_uvmm(object);\n\tnvkm_vmm_unref(&uvmm->vmm);\n\treturn uvmm;\n}\n\nstatic const struct nvkm_object_func\nnvkm_uvmm = {\n\t.dtor = nvkm_uvmm_dtor,\n\t.mthd = nvkm_uvmm_mthd,\n};\n\nint\nnvkm_uvmm_new(const struct nvkm_oclass *oclass, void *argv, u32 argc,\n\t      struct nvkm_object **pobject)\n{\n\tstruct nvkm_mmu *mmu = nvkm_ummu(oclass->parent)->mmu;\n\tconst bool more = oclass->base.maxver >= 0;\n\tunion {\n\t\tstruct nvif_vmm_v0 v0;\n\t} *args = argv;\n\tconst struct nvkm_vmm_page *page;\n\tstruct nvkm_uvmm *uvmm;\n\tint ret = -ENOSYS;\n\tu64 addr, size;\n\tbool managed, raw;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, more))) {\n\t\tmanaged = args->v0.type == NVIF_VMM_V0_TYPE_MANAGED;\n\t\traw = args->v0.type == NVIF_VMM_V0_TYPE_RAW;\n\t\taddr = args->v0.addr;\n\t\tsize = args->v0.size;\n\t} else\n\t\treturn ret;\n\n\tif (!(uvmm = kzalloc(sizeof(*uvmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\tnvkm_object_ctor(&nvkm_uvmm, oclass, &uvmm->object);\n\t*pobject = &uvmm->object;\n\n\tif (!mmu->vmm) {\n\t\tret = mmu->func->vmm.ctor(mmu, managed || raw, addr, size,\n\t\t\t\t\t  argv, argc, NULL, \"user\", &uvmm->vmm);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tuvmm->vmm->debug = max(uvmm->vmm->debug, oclass->client->debug);\n\t} else {\n\t\tif (size)\n\t\t\treturn -EINVAL;\n\n\t\tuvmm->vmm = nvkm_vmm_ref(mmu->vmm);\n\t}\n\tuvmm->vmm->managed.raw = raw;\n\n\tpage = uvmm->vmm->func->page;\n\targs->v0.page_nr = 0;\n\twhile (page && (page++)->shift)\n\t\targs->v0.page_nr++;\n\targs->v0.addr = uvmm->vmm->start;\n\targs->v0.size = uvmm->vmm->limit;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}