{
  "module_name": "base.c",
  "hash_id": "a872f6d141c101b0da83d1e9a5027562dd390962be3aaf98580572fbe4835f94",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/base.c",
  "human_readable_source": " \n#include \"ummu.h\"\n#include \"vmm.h\"\n\n#include <subdev/bar.h>\n#include <subdev/fb.h>\n\n#include <nvif/if500d.h>\n#include <nvif/if900d.h>\n\nstruct nvkm_mmu_ptp {\n\tstruct nvkm_mmu_pt *pt;\n\tstruct list_head head;\n\tu8  shift;\n\tu16 mask;\n\tu16 free;\n};\n\nstatic void\nnvkm_mmu_ptp_put(struct nvkm_mmu *mmu, bool force, struct nvkm_mmu_pt *pt)\n{\n\tconst int slot = pt->base >> pt->ptp->shift;\n\tstruct nvkm_mmu_ptp *ptp = pt->ptp;\n\n\t \n\tif (!ptp->free)\n\t\tlist_add(&ptp->head, &mmu->ptp.list);\n\tptp->free |= BIT(slot);\n\n\t \n\tif (ptp->free == ptp->mask) {\n\t\tnvkm_mmu_ptc_put(mmu, force, &ptp->pt);\n\t\tlist_del(&ptp->head);\n\t\tkfree(ptp);\n\t}\n\n\tkfree(pt);\n}\n\nstatic struct nvkm_mmu_pt *\nnvkm_mmu_ptp_get(struct nvkm_mmu *mmu, u32 size, bool zero)\n{\n\tstruct nvkm_mmu_pt *pt;\n\tstruct nvkm_mmu_ptp *ptp;\n\tint slot;\n\n\tif (!(pt = kzalloc(sizeof(*pt), GFP_KERNEL)))\n\t\treturn NULL;\n\n\tptp = list_first_entry_or_null(&mmu->ptp.list, typeof(*ptp), head);\n\tif (!ptp) {\n\t\t \n\t\tif (!(ptp = kmalloc(sizeof(*ptp), GFP_KERNEL))) {\n\t\t\tkfree(pt);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tptp->pt = nvkm_mmu_ptc_get(mmu, 0x1000, 0x1000, false);\n\t\tif (!ptp->pt) {\n\t\t\tkfree(ptp);\n\t\t\tkfree(pt);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tptp->shift = order_base_2(size);\n\t\tslot = nvkm_memory_size(ptp->pt->memory) >> ptp->shift;\n\t\tptp->mask = (1 << slot) - 1;\n\t\tptp->free = ptp->mask;\n\t\tlist_add(&ptp->head, &mmu->ptp.list);\n\t}\n\tpt->ptp = ptp;\n\tpt->sub = true;\n\n\t \n\tslot = __ffs(ptp->free);\n\tptp->free &= ~BIT(slot);\n\tif (!ptp->free)\n\t\tlist_del(&ptp->head);\n\n\tpt->memory = pt->ptp->pt->memory;\n\tpt->base = slot << ptp->shift;\n\tpt->addr = pt->ptp->pt->addr + pt->base;\n\treturn pt;\n}\n\nstruct nvkm_mmu_ptc {\n\tstruct list_head head;\n\tstruct list_head item;\n\tu32 size;\n\tu32 refs;\n};\n\nstatic inline struct nvkm_mmu_ptc *\nnvkm_mmu_ptc_find(struct nvkm_mmu *mmu, u32 size)\n{\n\tstruct nvkm_mmu_ptc *ptc;\n\n\tlist_for_each_entry(ptc, &mmu->ptc.list, head) {\n\t\tif (ptc->size == size)\n\t\t\treturn ptc;\n\t}\n\n\tptc = kmalloc(sizeof(*ptc), GFP_KERNEL);\n\tif (ptc) {\n\t\tINIT_LIST_HEAD(&ptc->item);\n\t\tptc->size = size;\n\t\tptc->refs = 0;\n\t\tlist_add(&ptc->head, &mmu->ptc.list);\n\t}\n\n\treturn ptc;\n}\n\nvoid\nnvkm_mmu_ptc_put(struct nvkm_mmu *mmu, bool force, struct nvkm_mmu_pt **ppt)\n{\n\tstruct nvkm_mmu_pt *pt = *ppt;\n\tif (pt) {\n\t\t \n\t\tif (pt->sub) {\n\t\t\tmutex_lock(&mmu->ptp.mutex);\n\t\t\tnvkm_mmu_ptp_put(mmu, force, pt);\n\t\t\tmutex_unlock(&mmu->ptp.mutex);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tmutex_lock(&mmu->ptc.mutex);\n\t\tif (pt->ptc->refs < 8   && !force) {\n\t\t\tlist_add_tail(&pt->head, &pt->ptc->item);\n\t\t\tpt->ptc->refs++;\n\t\t} else {\n\t\t\tnvkm_memory_unref(&pt->memory);\n\t\t\tkfree(pt);\n\t\t}\n\t\tmutex_unlock(&mmu->ptc.mutex);\n\t}\n}\n\nstruct nvkm_mmu_pt *\nnvkm_mmu_ptc_get(struct nvkm_mmu *mmu, u32 size, u32 align, bool zero)\n{\n\tstruct nvkm_mmu_ptc *ptc;\n\tstruct nvkm_mmu_pt *pt;\n\tint ret;\n\n\t \n\tif (align < 0x1000) {\n\t\tmutex_lock(&mmu->ptp.mutex);\n\t\tpt = nvkm_mmu_ptp_get(mmu, align, zero);\n\t\tmutex_unlock(&mmu->ptp.mutex);\n\t\treturn pt;\n\t}\n\n\t \n\tmutex_lock(&mmu->ptc.mutex);\n\tptc = nvkm_mmu_ptc_find(mmu, size);\n\tif (!ptc) {\n\t\tmutex_unlock(&mmu->ptc.mutex);\n\t\treturn NULL;\n\t}\n\n\t \n\tpt = list_first_entry_or_null(&ptc->item, typeof(*pt), head);\n\tif (pt) {\n\t\tif (zero)\n\t\t\tnvkm_fo64(pt->memory, 0, 0, size >> 3);\n\t\tlist_del(&pt->head);\n\t\tptc->refs--;\n\t\tmutex_unlock(&mmu->ptc.mutex);\n\t\treturn pt;\n\t}\n\tmutex_unlock(&mmu->ptc.mutex);\n\n\t \n\tif (!(pt = kmalloc(sizeof(*pt), GFP_KERNEL)))\n\t\treturn NULL;\n\tpt->ptc = ptc;\n\tpt->sub = false;\n\n\tret = nvkm_memory_new(mmu->subdev.device, NVKM_MEM_TARGET_INST,\n\t\t\t      size, align, zero, &pt->memory);\n\tif (ret) {\n\t\tkfree(pt);\n\t\treturn NULL;\n\t}\n\n\tpt->base = 0;\n\tpt->addr = nvkm_memory_addr(pt->memory);\n\treturn pt;\n}\n\nvoid\nnvkm_mmu_ptc_dump(struct nvkm_mmu *mmu)\n{\n\tstruct nvkm_mmu_ptc *ptc;\n\tlist_for_each_entry(ptc, &mmu->ptc.list, head) {\n\t\tstruct nvkm_mmu_pt *pt, *tt;\n\t\tlist_for_each_entry_safe(pt, tt, &ptc->item, head) {\n\t\t\tnvkm_memory_unref(&pt->memory);\n\t\t\tlist_del(&pt->head);\n\t\t\tkfree(pt);\n\t\t}\n\t}\n}\n\nstatic void\nnvkm_mmu_ptc_fini(struct nvkm_mmu *mmu)\n{\n\tstruct nvkm_mmu_ptc *ptc, *ptct;\n\n\tlist_for_each_entry_safe(ptc, ptct, &mmu->ptc.list, head) {\n\t\tWARN_ON(!list_empty(&ptc->item));\n\t\tlist_del(&ptc->head);\n\t\tkfree(ptc);\n\t}\n}\n\nstatic void\nnvkm_mmu_ptc_init(struct nvkm_mmu *mmu)\n{\n\tmutex_init(&mmu->ptc.mutex);\n\tINIT_LIST_HEAD(&mmu->ptc.list);\n\tmutex_init(&mmu->ptp.mutex);\n\tINIT_LIST_HEAD(&mmu->ptp.list);\n}\n\nstatic void\nnvkm_mmu_type(struct nvkm_mmu *mmu, int heap, u8 type)\n{\n\tif (heap >= 0 && !WARN_ON(mmu->type_nr == ARRAY_SIZE(mmu->type))) {\n\t\tmmu->type[mmu->type_nr].type = type | mmu->heap[heap].type;\n\t\tmmu->type[mmu->type_nr].heap = heap;\n\t\tmmu->type_nr++;\n\t}\n}\n\nstatic int\nnvkm_mmu_heap(struct nvkm_mmu *mmu, u8 type, u64 size)\n{\n\tif (size) {\n\t\tif (!WARN_ON(mmu->heap_nr == ARRAY_SIZE(mmu->heap))) {\n\t\t\tmmu->heap[mmu->heap_nr].type = type;\n\t\t\tmmu->heap[mmu->heap_nr].size = size;\n\t\t\treturn mmu->heap_nr++;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nnvkm_mmu_host(struct nvkm_mmu *mmu)\n{\n\tstruct nvkm_device *device = mmu->subdev.device;\n\tu8 type = NVKM_MEM_KIND * !!mmu->func->kind_sys;\n\tint heap;\n\n\t \n\theap = nvkm_mmu_heap(mmu, NVKM_MEM_HOST, ~0ULL);\n\tnvkm_mmu_type(mmu, heap, type);\n\n\t \n\ttype |= NVKM_MEM_MAPPABLE;\n\tif (!device->bar || device->bar->iomap_uncached)\n\t\tnvkm_mmu_type(mmu, heap, type & ~NVKM_MEM_KIND);\n\telse\n\t\tnvkm_mmu_type(mmu, heap, type);\n\n\t \n\ttype |= NVKM_MEM_COHERENT;\n\tif (device->func->cpu_coherent)\n\t\tnvkm_mmu_type(mmu, heap, type & ~NVKM_MEM_KIND);\n\n\t \n\tnvkm_mmu_type(mmu, heap, type |= NVKM_MEM_UNCACHED);\n}\n\nstatic void\nnvkm_mmu_vram(struct nvkm_mmu *mmu)\n{\n\tstruct nvkm_device *device = mmu->subdev.device;\n\tstruct nvkm_mm *mm = &device->fb->ram->vram;\n\tconst u64 sizeN = nvkm_mm_heap_size(mm, NVKM_RAM_MM_NORMAL);\n\tconst u64 sizeU = nvkm_mm_heap_size(mm, NVKM_RAM_MM_NOMAP);\n\tconst u64 sizeM = nvkm_mm_heap_size(mm, NVKM_RAM_MM_MIXED);\n\tu8 type = NVKM_MEM_KIND * !!mmu->func->kind;\n\tu8 heap = NVKM_MEM_VRAM;\n\tint heapM, heapN, heapU;\n\n\t \n\theapM = nvkm_mmu_heap(mmu, heap, sizeM << NVKM_RAM_MM_SHIFT);\n\n\theap |= NVKM_MEM_COMP;\n\theap |= NVKM_MEM_DISP;\n\theapN = nvkm_mmu_heap(mmu, heap, sizeN << NVKM_RAM_MM_SHIFT);\n\theapU = nvkm_mmu_heap(mmu, heap, sizeU << NVKM_RAM_MM_SHIFT);\n\n\t \n\tnvkm_mmu_type(mmu, heapU, type);\n\tnvkm_mmu_type(mmu, heapN, type);\n\tnvkm_mmu_type(mmu, heapM, type);\n\n\t \n\tnvkm_mmu_host(mmu);\n\n\t \n\tif (device->bar) {\n\t\t \n\t\ttype |= NVKM_MEM_MAPPABLE;\n\t\tif (!device->bar->iomap_uncached) {\n\t\t\tnvkm_mmu_type(mmu, heapN, type);\n\t\t\tnvkm_mmu_type(mmu, heapM, type);\n\t\t}\n\n\t\t \n\t\ttype |= NVKM_MEM_COHERENT;\n\t\ttype |= NVKM_MEM_UNCACHED;\n\t\tnvkm_mmu_type(mmu, heapN, type);\n\t\tnvkm_mmu_type(mmu, heapM, type);\n\t}\n}\n\nstatic int\nnvkm_mmu_oneinit(struct nvkm_subdev *subdev)\n{\n\tstruct nvkm_mmu *mmu = nvkm_mmu(subdev);\n\n\t \n\tif (mmu->subdev.device->fb && mmu->subdev.device->fb->ram)\n\t\tnvkm_mmu_vram(mmu);\n\telse\n\t\tnvkm_mmu_host(mmu);\n\n\tif (mmu->func->vmm.global) {\n\t\tint ret = nvkm_vmm_new(subdev->device, 0, 0, NULL, 0, NULL,\n\t\t\t\t       \"gart\", &mmu->vmm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nnvkm_mmu_init(struct nvkm_subdev *subdev)\n{\n\tstruct nvkm_mmu *mmu = nvkm_mmu(subdev);\n\tif (mmu->func->init)\n\t\tmmu->func->init(mmu);\n\treturn 0;\n}\n\nstatic void *\nnvkm_mmu_dtor(struct nvkm_subdev *subdev)\n{\n\tstruct nvkm_mmu *mmu = nvkm_mmu(subdev);\n\n\tnvkm_vmm_unref(&mmu->vmm);\n\n\tnvkm_mmu_ptc_fini(mmu);\n\tmutex_destroy(&mmu->mutex);\n\treturn mmu;\n}\n\nstatic const struct nvkm_subdev_func\nnvkm_mmu = {\n\t.dtor = nvkm_mmu_dtor,\n\t.oneinit = nvkm_mmu_oneinit,\n\t.init = nvkm_mmu_init,\n};\n\nvoid\nnvkm_mmu_ctor(const struct nvkm_mmu_func *func, struct nvkm_device *device,\n\t      enum nvkm_subdev_type type, int inst, struct nvkm_mmu *mmu)\n{\n\tnvkm_subdev_ctor(&nvkm_mmu, device, type, inst, &mmu->subdev);\n\tmmu->func = func;\n\tmmu->dma_bits = func->dma_bits;\n\tnvkm_mmu_ptc_init(mmu);\n\tmutex_init(&mmu->mutex);\n\tmmu->user.ctor = nvkm_ummu_new;\n\tmmu->user.base = func->mmu.user;\n}\n\nint\nnvkm_mmu_new_(const struct nvkm_mmu_func *func, struct nvkm_device *device,\n\t      enum nvkm_subdev_type type, int inst, struct nvkm_mmu **pmmu)\n{\n\tif (!(*pmmu = kzalloc(sizeof(**pmmu), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tnvkm_mmu_ctor(func, device, type, inst, *pmmu);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}