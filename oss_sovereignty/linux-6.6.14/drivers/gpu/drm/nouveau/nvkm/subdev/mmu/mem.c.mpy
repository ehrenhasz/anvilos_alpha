{
  "module_name": "mem.c",
  "hash_id": "fb73bb51c8c637a4669539d2e5e8fe7404f61a6c3c7282f4348afb55ba6a912a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/mem.c",
  "human_readable_source": " \n#define nvkm_mem(p) container_of((p), struct nvkm_mem, memory)\n#include \"mem.h\"\n\n#include <core/memory.h>\n\n#include <nvif/if000a.h>\n#include <nvif/unpack.h>\n\nstruct nvkm_mem {\n\tstruct nvkm_memory memory;\n\tenum nvkm_memory_target target;\n\tstruct nvkm_mmu *mmu;\n\tu64 pages;\n\tstruct page **mem;\n\tunion {\n\t\tstruct scatterlist *sgl;\n\t\tdma_addr_t *dma;\n\t};\n};\n\nstatic enum nvkm_memory_target\nnvkm_mem_target(struct nvkm_memory *memory)\n{\n\treturn nvkm_mem(memory)->target;\n}\n\nstatic u8\nnvkm_mem_page(struct nvkm_memory *memory)\n{\n\treturn PAGE_SHIFT;\n}\n\nstatic u64\nnvkm_mem_addr(struct nvkm_memory *memory)\n{\n\tstruct nvkm_mem *mem = nvkm_mem(memory);\n\tif (mem->pages == 1 && mem->mem)\n\t\treturn mem->dma[0];\n\treturn ~0ULL;\n}\n\nstatic u64\nnvkm_mem_size(struct nvkm_memory *memory)\n{\n\treturn nvkm_mem(memory)->pages << PAGE_SHIFT;\n}\n\nstatic int\nnvkm_mem_map_dma(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,\n\t\t struct nvkm_vma *vma, void *argv, u32 argc)\n{\n\tstruct nvkm_mem *mem = nvkm_mem(memory);\n\tstruct nvkm_vmm_map map = {\n\t\t.memory = &mem->memory,\n\t\t.offset = offset,\n\t\t.dma = mem->dma,\n\t};\n\treturn nvkm_vmm_map(vmm, vma, argv, argc, &map);\n}\n\nstatic void *\nnvkm_mem_dtor(struct nvkm_memory *memory)\n{\n\tstruct nvkm_mem *mem = nvkm_mem(memory);\n\tif (mem->mem) {\n\t\twhile (mem->pages--) {\n\t\t\tdma_unmap_page(mem->mmu->subdev.device->dev,\n\t\t\t\t       mem->dma[mem->pages], PAGE_SIZE,\n\t\t\t\t       DMA_BIDIRECTIONAL);\n\t\t\t__free_page(mem->mem[mem->pages]);\n\t\t}\n\t\tkvfree(mem->dma);\n\t\tkvfree(mem->mem);\n\t}\n\treturn mem;\n}\n\nstatic const struct nvkm_memory_func\nnvkm_mem_dma = {\n\t.dtor = nvkm_mem_dtor,\n\t.target = nvkm_mem_target,\n\t.page = nvkm_mem_page,\n\t.addr = nvkm_mem_addr,\n\t.size = nvkm_mem_size,\n\t.map = nvkm_mem_map_dma,\n};\n\nstatic int\nnvkm_mem_map_sgl(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,\n\t\t struct nvkm_vma *vma, void *argv, u32 argc)\n{\n\tstruct nvkm_mem *mem = nvkm_mem(memory);\n\tstruct nvkm_vmm_map map = {\n\t\t.memory = &mem->memory,\n\t\t.offset = offset,\n\t\t.sgl = mem->sgl,\n\t};\n\treturn nvkm_vmm_map(vmm, vma, argv, argc, &map);\n}\n\nstatic const struct nvkm_memory_func\nnvkm_mem_sgl = {\n\t.dtor = nvkm_mem_dtor,\n\t.target = nvkm_mem_target,\n\t.page = nvkm_mem_page,\n\t.addr = nvkm_mem_addr,\n\t.size = nvkm_mem_size,\n\t.map = nvkm_mem_map_sgl,\n};\n\nint\nnvkm_mem_map_host(struct nvkm_memory *memory, void **pmap)\n{\n\tstruct nvkm_mem *mem = nvkm_mem(memory);\n\tif (mem->mem) {\n\t\t*pmap = vmap(mem->mem, mem->pages, VM_MAP, PAGE_KERNEL);\n\t\treturn *pmap ? 0 : -EFAULT;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int\nnvkm_mem_new_host(struct nvkm_mmu *mmu, int type, u8 page, u64 size,\n\t\t  void *argv, u32 argc, struct nvkm_memory **pmemory)\n{\n\tstruct device *dev = mmu->subdev.device->dev;\n\tunion {\n\t\tstruct nvif_mem_ram_vn vn;\n\t\tstruct nvif_mem_ram_v0 v0;\n\t} *args = argv;\n\tint ret = -ENOSYS;\n\tenum nvkm_memory_target target;\n\tstruct nvkm_mem *mem;\n\tgfp_t gfp = GFP_USER | __GFP_ZERO;\n\n\tif ( (mmu->type[type].type & NVKM_MEM_COHERENT) &&\n\t    !(mmu->type[type].type & NVKM_MEM_UNCACHED))\n\t\ttarget = NVKM_MEM_TARGET_HOST;\n\telse\n\t\ttarget = NVKM_MEM_TARGET_NCOH;\n\n\tif (page != PAGE_SHIFT)\n\t\treturn -EINVAL;\n\n\tif (!(mem = kzalloc(sizeof(*mem), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tmem->target = target;\n\tmem->mmu = mmu;\n\t*pmemory = &mem->memory;\n\n\tif (!(ret = nvif_unpack(ret, &argv, &argc, args->v0, 0, 0, false))) {\n\t\tif (args->v0.dma) {\n\t\t\tnvkm_memory_ctor(&nvkm_mem_dma, &mem->memory);\n\t\t\tmem->dma = args->v0.dma;\n\t\t} else {\n\t\t\tnvkm_memory_ctor(&nvkm_mem_sgl, &mem->memory);\n\t\t\tmem->sgl = args->v0.sgl;\n\t\t}\n\n\t\tif (!IS_ALIGNED(size, PAGE_SIZE))\n\t\t\treturn -EINVAL;\n\t\tmem->pages = size >> PAGE_SHIFT;\n\t\treturn 0;\n\t} else\n\tif ( (ret = nvif_unvers(ret, &argv, &argc, args->vn))) {\n\t\tkfree(mem);\n\t\treturn ret;\n\t}\n\n\tnvkm_memory_ctor(&nvkm_mem_dma, &mem->memory);\n\tsize = ALIGN(size, PAGE_SIZE) >> PAGE_SHIFT;\n\n\tif (!(mem->mem = kvmalloc_array(size, sizeof(*mem->mem), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tif (!(mem->dma = kvmalloc_array(size, sizeof(*mem->dma), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\tif (mmu->dma_bits > 32)\n\t\tgfp |= GFP_HIGHUSER;\n\telse\n\t\tgfp |= GFP_DMA32;\n\n\tfor (mem->pages = 0; size; size--, mem->pages++) {\n\t\tstruct page *p = alloc_page(gfp);\n\t\tif (!p)\n\t\t\treturn -ENOMEM;\n\n\t\tmem->dma[mem->pages] = dma_map_page(mmu->subdev.device->dev,\n\t\t\t\t\t\t    p, 0, PAGE_SIZE,\n\t\t\t\t\t\t    DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(dev, mem->dma[mem->pages])) {\n\t\t\t__free_page(p);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tmem->mem[mem->pages] = p;\n\t}\n\n\treturn 0;\n}\n\nint\nnvkm_mem_new_type(struct nvkm_mmu *mmu, int type, u8 page, u64 size,\n\t\t  void *argv, u32 argc, struct nvkm_memory **pmemory)\n{\n\tstruct nvkm_memory *memory = NULL;\n\tint ret;\n\n\tif (mmu->type[type].type & NVKM_MEM_VRAM) {\n\t\tret = mmu->func->mem.vram(mmu, type, page, size,\n\t\t\t\t\t  argv, argc, &memory);\n\t} else {\n\t\tret = nvkm_mem_new_host(mmu, type, page, size,\n\t\t\t\t\targv, argc, &memory);\n\t}\n\n\tif (ret)\n\t\tnvkm_memory_unref(&memory);\n\t*pmemory = memory;\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}