{
  "module_name": "nv50.c",
  "hash_id": "23418c806f375eecb376484b186d2150bc61e20ff33c90fb25ef6c0df46ae209",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/instmem/nv50.c",
  "human_readable_source": " \n#define nv50_instmem(p) container_of((p), struct nv50_instmem, base)\n#include \"priv.h\"\n\n#include <core/memory.h>\n#include <subdev/bar.h>\n#include <subdev/fb.h>\n#include <subdev/mmu.h>\n\nstruct nv50_instmem {\n\tstruct nvkm_instmem base;\n\tu64 addr;\n\n\t \n\tstruct list_head lru;\n};\n\n \n#define nv50_instobj(p) container_of((p), struct nv50_instobj, base.memory)\n\nstruct nv50_instobj {\n\tstruct nvkm_instobj base;\n\tstruct nv50_instmem *imem;\n\tstruct nvkm_memory *ram;\n\tstruct nvkm_vma *bar;\n\trefcount_t maps;\n\tvoid *map;\n\tstruct list_head lru;\n};\n\nstatic void\nnv50_instobj_wr32_slow(struct nvkm_memory *memory, u64 offset, u32 data)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nv50_instmem *imem = iobj->imem;\n\tstruct nvkm_device *device = imem->base.subdev.device;\n\tu64 base = (nvkm_memory_addr(iobj->ram) + offset) & 0xffffff00000ULL;\n\tu64 addr = (nvkm_memory_addr(iobj->ram) + offset) & 0x000000fffffULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&imem->base.lock, flags);\n\tif (unlikely(imem->addr != base)) {\n\t\tnvkm_wr32(device, 0x001700, base >> 16);\n\t\timem->addr = base;\n\t}\n\tnvkm_wr32(device, 0x700000 + addr, data);\n\tspin_unlock_irqrestore(&imem->base.lock, flags);\n}\n\nstatic u32\nnv50_instobj_rd32_slow(struct nvkm_memory *memory, u64 offset)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nv50_instmem *imem = iobj->imem;\n\tstruct nvkm_device *device = imem->base.subdev.device;\n\tu64 base = (nvkm_memory_addr(iobj->ram) + offset) & 0xffffff00000ULL;\n\tu64 addr = (nvkm_memory_addr(iobj->ram) + offset) & 0x000000fffffULL;\n\tu32 data;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&imem->base.lock, flags);\n\tif (unlikely(imem->addr != base)) {\n\t\tnvkm_wr32(device, 0x001700, base >> 16);\n\t\timem->addr = base;\n\t}\n\tdata = nvkm_rd32(device, 0x700000 + addr);\n\tspin_unlock_irqrestore(&imem->base.lock, flags);\n\treturn data;\n}\n\nstatic const struct nvkm_memory_ptrs\nnv50_instobj_slow = {\n\t.rd32 = nv50_instobj_rd32_slow,\n\t.wr32 = nv50_instobj_wr32_slow,\n};\n\nstatic void\nnv50_instobj_wr32(struct nvkm_memory *memory, u64 offset, u32 data)\n{\n\tiowrite32_native(data, nv50_instobj(memory)->map + offset);\n}\n\nstatic u32\nnv50_instobj_rd32(struct nvkm_memory *memory, u64 offset)\n{\n\treturn ioread32_native(nv50_instobj(memory)->map + offset);\n}\n\nstatic const struct nvkm_memory_ptrs\nnv50_instobj_fast = {\n\t.rd32 = nv50_instobj_rd32,\n\t.wr32 = nv50_instobj_wr32,\n};\n\nstatic void\nnv50_instobj_kmap(struct nv50_instobj *iobj, struct nvkm_vmm *vmm)\n{\n\tstruct nv50_instmem *imem = iobj->imem;\n\tstruct nv50_instobj *eobj;\n\tstruct nvkm_memory *memory = &iobj->base.memory;\n\tstruct nvkm_subdev *subdev = &imem->base.subdev;\n\tstruct nvkm_device *device = subdev->device;\n\tstruct nvkm_vma *bar = NULL, *ebar;\n\tu64 size = nvkm_memory_size(memory);\n\tvoid *emap;\n\tint ret;\n\n\t \n\tmutex_unlock(&imem->base.mutex);\n\twhile ((ret = nvkm_vmm_get(vmm, 12, size, &bar))) {\n\t\t \n\t\tmutex_lock(&imem->base.mutex);\n\t\teobj = list_first_entry_or_null(&imem->lru, typeof(*eobj), lru);\n\t\tif (eobj) {\n\t\t\tnvkm_debug(subdev, \"evict %016llx %016llx @ %016llx\\n\",\n\t\t\t\t   nvkm_memory_addr(&eobj->base.memory),\n\t\t\t\t   nvkm_memory_size(&eobj->base.memory),\n\t\t\t\t   eobj->bar->addr);\n\t\t\tlist_del_init(&eobj->lru);\n\t\t\tebar = eobj->bar;\n\t\t\teobj->bar = NULL;\n\t\t\temap = eobj->map;\n\t\t\teobj->map = NULL;\n\t\t}\n\t\tmutex_unlock(&imem->base.mutex);\n\t\tif (!eobj)\n\t\t\tbreak;\n\t\tiounmap(emap);\n\t\tnvkm_vmm_put(vmm, &ebar);\n\t}\n\n\tif (ret == 0)\n\t\tret = nvkm_memory_map(memory, 0, vmm, bar, NULL, 0);\n\tmutex_lock(&imem->base.mutex);\n\tif (ret || iobj->bar) {\n\t\t \n\t\tmutex_unlock(&imem->base.mutex);\n\t\tnvkm_vmm_put(vmm, &bar);\n\t\tmutex_lock(&imem->base.mutex);\n\t\treturn;\n\t}\n\n\t \n\tiobj->bar = bar;\n\tiobj->map = ioremap_wc(device->func->resource_addr(device, 3) +\n\t\t\t       (u32)iobj->bar->addr, size);\n\tif (!iobj->map) {\n\t\tnvkm_warn(subdev, \"PRAMIN ioremap failed\\n\");\n\t\tnvkm_vmm_put(vmm, &iobj->bar);\n\t}\n}\n\nstatic int\nnv50_instobj_map(struct nvkm_memory *memory, u64 offset, struct nvkm_vmm *vmm,\n\t\t struct nvkm_vma *vma, void *argv, u32 argc)\n{\n\tmemory = nv50_instobj(memory)->ram;\n\treturn nvkm_memory_map(memory, offset, vmm, vma, argv, argc);\n}\n\nstatic void\nnv50_instobj_release(struct nvkm_memory *memory)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nv50_instmem *imem = iobj->imem;\n\tstruct nvkm_subdev *subdev = &imem->base.subdev;\n\n\twmb();\n\tnvkm_bar_flush(subdev->device->bar);\n\n\tif (refcount_dec_and_mutex_lock(&iobj->maps, &imem->base.mutex)) {\n\t\t \n\t\tif (likely(iobj->lru.next) && iobj->map) {\n\t\t\tBUG_ON(!list_empty(&iobj->lru));\n\t\t\tlist_add_tail(&iobj->lru, &imem->lru);\n\t\t}\n\n\t\t \n\t\tiobj->base.memory.ptrs = NULL;\n\t\tmutex_unlock(&imem->base.mutex);\n\t}\n}\n\nstatic void __iomem *\nnv50_instobj_acquire(struct nvkm_memory *memory)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nvkm_instmem *imem = &iobj->imem->base;\n\tstruct nvkm_vmm *vmm;\n\tvoid __iomem *map = NULL;\n\n\t \n\tif (refcount_inc_not_zero(&iobj->maps))\n\t\treturn iobj->map;\n\n\t \n\tmutex_lock(&imem->mutex);\n\tif (refcount_inc_not_zero(&iobj->maps)) {\n\t\tmutex_unlock(&imem->mutex);\n\t\treturn iobj->map;\n\t}\n\n\t \n\tif ((vmm = nvkm_bar_bar2_vmm(imem->subdev.device))) {\n\t\tif (!iobj->map)\n\t\t\tnv50_instobj_kmap(iobj, vmm);\n\t\tmap = iobj->map;\n\t}\n\n\tif (!refcount_inc_not_zero(&iobj->maps)) {\n\t\t \n\t\tif (likely(iobj->lru.next))\n\t\t\tlist_del_init(&iobj->lru);\n\n\t\tif (map)\n\t\t\tiobj->base.memory.ptrs = &nv50_instobj_fast;\n\t\telse\n\t\t\tiobj->base.memory.ptrs = &nv50_instobj_slow;\n\t\trefcount_set(&iobj->maps, 1);\n\t}\n\n\tmutex_unlock(&imem->mutex);\n\treturn map;\n}\n\nstatic void\nnv50_instobj_boot(struct nvkm_memory *memory, struct nvkm_vmm *vmm)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nvkm_instmem *imem = &iobj->imem->base;\n\n\t \n\tmutex_lock(&imem->mutex);\n\tif (likely(iobj->lru.next)) {\n\t\tlist_del_init(&iobj->lru);\n\t\tiobj->lru.next = NULL;\n\t}\n\n\tnv50_instobj_kmap(iobj, vmm);\n\tnvkm_instmem_boot(imem);\n\tmutex_unlock(&imem->mutex);\n}\n\nstatic u64\nnv50_instobj_size(struct nvkm_memory *memory)\n{\n\treturn nvkm_memory_size(nv50_instobj(memory)->ram);\n}\n\nstatic u64\nnv50_instobj_addr(struct nvkm_memory *memory)\n{\n\treturn nvkm_memory_addr(nv50_instobj(memory)->ram);\n}\n\nstatic u64\nnv50_instobj_bar2(struct nvkm_memory *memory)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tu64 addr = ~0ULL;\n\tif (nv50_instobj_acquire(&iobj->base.memory)) {\n\t\tiobj->lru.next = NULL;  \n\t\taddr = iobj->bar->addr;\n\t}\n\tnv50_instobj_release(&iobj->base.memory);\n\treturn addr;\n}\n\nstatic enum nvkm_memory_target\nnv50_instobj_target(struct nvkm_memory *memory)\n{\n\treturn nvkm_memory_target(nv50_instobj(memory)->ram);\n}\n\nstatic void *\nnv50_instobj_dtor(struct nvkm_memory *memory)\n{\n\tstruct nv50_instobj *iobj = nv50_instobj(memory);\n\tstruct nvkm_instmem *imem = &iobj->imem->base;\n\tstruct nvkm_vma *bar;\n\tvoid *map;\n\n\tmutex_lock(&imem->mutex);\n\tif (likely(iobj->lru.next))\n\t\tlist_del(&iobj->lru);\n\tmap = iobj->map;\n\tbar = iobj->bar;\n\tmutex_unlock(&imem->mutex);\n\n\tif (map) {\n\t\tstruct nvkm_vmm *vmm = nvkm_bar_bar2_vmm(imem->subdev.device);\n\t\tiounmap(map);\n\t\tif (likely(vmm))  \n\t\t\tnvkm_vmm_put(vmm, &bar);\n\t}\n\n\tnvkm_memory_unref(&iobj->ram);\n\tnvkm_instobj_dtor(imem, &iobj->base);\n\treturn iobj;\n}\n\nstatic const struct nvkm_memory_func\nnv50_instobj_func = {\n\t.dtor = nv50_instobj_dtor,\n\t.target = nv50_instobj_target,\n\t.bar2 = nv50_instobj_bar2,\n\t.addr = nv50_instobj_addr,\n\t.size = nv50_instobj_size,\n\t.boot = nv50_instobj_boot,\n\t.acquire = nv50_instobj_acquire,\n\t.release = nv50_instobj_release,\n\t.map = nv50_instobj_map,\n};\n\nstatic int\nnv50_instobj_wrap(struct nvkm_instmem *base,\n\t\t  struct nvkm_memory *memory, struct nvkm_memory **pmemory)\n{\n\tstruct nv50_instmem *imem = nv50_instmem(base);\n\tstruct nv50_instobj *iobj;\n\n\tif (!(iobj = kzalloc(sizeof(*iobj), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\t*pmemory = &iobj->base.memory;\n\n\tnvkm_instobj_ctor(&nv50_instobj_func, &imem->base, &iobj->base);\n\tiobj->imem = imem;\n\trefcount_set(&iobj->maps, 0);\n\tINIT_LIST_HEAD(&iobj->lru);\n\n\tiobj->ram = nvkm_memory_ref(memory);\n\treturn 0;\n}\n\nstatic int\nnv50_instobj_new(struct nvkm_instmem *imem, u32 size, u32 align, bool zero,\n\t\t struct nvkm_memory **pmemory)\n{\n\tu8 page = max(order_base_2(align), 12);\n\tstruct nvkm_memory *ram;\n\tint ret;\n\n\tret = nvkm_ram_get(imem->subdev.device, 0, 1, page, size, true, true, &ram);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nv50_instobj_wrap(imem, ram, pmemory);\n\tnvkm_memory_unref(&ram);\n\treturn ret;\n}\n\n \n\nstatic void\nnv50_instmem_fini(struct nvkm_instmem *base)\n{\n\tnv50_instmem(base)->addr = ~0ULL;\n}\n\nstatic const struct nvkm_instmem_func\nnv50_instmem = {\n\t.fini = nv50_instmem_fini,\n\t.memory_new = nv50_instobj_new,\n\t.memory_wrap = nv50_instobj_wrap,\n\t.zero = false,\n};\n\nint\nnv50_instmem_new(struct nvkm_device *device, enum nvkm_subdev_type type, int inst,\n\t\t struct nvkm_instmem **pimem)\n{\n\tstruct nv50_instmem *imem;\n\n\tif (!(imem = kzalloc(sizeof(*imem), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tnvkm_instmem_ctor(&nv50_instmem, device, type, inst, &imem->base);\n\tINIT_LIST_HEAD(&imem->lru);\n\t*pimem = &imem->base;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}