{
  "module_name": "vmm.h",
  "hash_id": "216995cb15718817461656ad80b018acca2e9694d024c3a4ba00945addccb537",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/subdev/mmu/vmm.h",
  "human_readable_source": "#ifndef __NVKM_VMM_H__\n#define __NVKM_VMM_H__\n#include \"priv.h\"\n#include <core/memory.h>\nenum nvkm_memory_target;\n\nstruct nvkm_vmm_pt {\n\t \n\tstruct nvkm_mmu_pt *pt[2];\n\tu32 refs[2];\n\n\t \n\tu8 page;\n\n\t \n\tbool sparse:1;\n\n\t \n#define NVKM_VMM_PDE_INVALID(pde) IS_ERR_OR_NULL(pde)\n#define NVKM_VMM_PDE_SPARSED(pde) IS_ERR(pde)\n#define NVKM_VMM_PDE_SPARSE       ERR_PTR(-EBUSY)\n\tstruct nvkm_vmm_pt **pde;\n\n\t \n#define NVKM_VMM_PTE_SPARSE 0x80\n#define NVKM_VMM_PTE_VALID  0x40\n#define NVKM_VMM_PTE_SPTES  0x3f\n\tu8 pte[];\n};\n\ntypedef void (*nvkm_vmm_pxe_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_mmu_pt *, u32 ptei, u32 ptes);\ntypedef void (*nvkm_vmm_pde_func)(struct nvkm_vmm *,\n\t\t\t\t  struct nvkm_vmm_pt *, u32 pdei);\ntypedef void (*nvkm_vmm_pte_func)(struct nvkm_vmm *, struct nvkm_mmu_pt *,\n\t\t\t\t  u32 ptei, u32 ptes, struct nvkm_vmm_map *);\n\nstruct nvkm_vmm_desc_func {\n\tnvkm_vmm_pxe_func invalid;\n\tnvkm_vmm_pxe_func unmap;\n\tnvkm_vmm_pxe_func sparse;\n\n\tnvkm_vmm_pde_func pde;\n\n\tnvkm_vmm_pte_func mem;\n\tnvkm_vmm_pte_func dma;\n\tnvkm_vmm_pte_func sgl;\n\n\tnvkm_vmm_pte_func pfn;\n\tbool (*pfn_clear)(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32 ptei, u32 ptes);\n\tnvkm_vmm_pxe_func pfn_unmap;\n};\n\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgd;\nvoid gf100_vmm_pgd_pde(struct nvkm_vmm *, struct nvkm_vmm_pt *, u32);\nextern const struct nvkm_vmm_desc_func gf100_vmm_pgt;\nvoid gf100_vmm_pgt_unmap(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\nvoid gf100_vmm_pgt_mem(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_dma(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\nvoid gf100_vmm_pgt_sgl(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32,\n\t\t       struct nvkm_vmm_map *);\n\nvoid gk104_vmm_lpt_invalid(struct nvkm_vmm *, struct nvkm_mmu_pt *, u32, u32);\n\nstruct nvkm_vmm_desc {\n\tenum {\n\t\tPGD,\n\t\tPGT,\n\t\tSPT,\n\t\tLPT,\n\t} type;\n\tu8 bits;\t \n\tu8 size;\t \n\tu32 align;\t \n\tconst struct nvkm_vmm_desc_func *func;\n};\n\nextern const struct nvkm_vmm_desc nv50_vmm_desc_12[];\nextern const struct nvkm_vmm_desc nv50_vmm_desc_16[];\n\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gk104_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_16_16[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_12[];\nextern const struct nvkm_vmm_desc gm200_vmm_desc_17_17[];\n\nextern const struct nvkm_vmm_desc gp100_vmm_desc_12[];\nextern const struct nvkm_vmm_desc gp100_vmm_desc_16[];\n\nstruct nvkm_vmm_page {\n\tu8 shift;\n\tconst struct nvkm_vmm_desc *desc;\n#define NVKM_VMM_PAGE_SPARSE                                               0x01\n#define NVKM_VMM_PAGE_VRAM                                                 0x02\n#define NVKM_VMM_PAGE_HOST                                                 0x04\n#define NVKM_VMM_PAGE_COMP                                                 0x08\n#define NVKM_VMM_PAGE_Sxxx                                (NVKM_VMM_PAGE_SPARSE)\n#define NVKM_VMM_PAGE_xVxx                                  (NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_SVxx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_VRAM)\n#define NVKM_VMM_PAGE_xxHx                                  (NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SxHx             (NVKM_VMM_PAGE_Sxxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVHx             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_SVHx             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_HOST)\n#define NVKM_VMM_PAGE_xVxC             (NVKM_VMM_PAGE_xVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SVxC             (NVKM_VMM_PAGE_SVxx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_xxHC             (NVKM_VMM_PAGE_xxHx | NVKM_VMM_PAGE_COMP)\n#define NVKM_VMM_PAGE_SxHC             (NVKM_VMM_PAGE_SxHx | NVKM_VMM_PAGE_COMP)\n\tu8 type;\n};\n\nstruct nvkm_vmm_func {\n\tint (*join)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\tvoid (*part)(struct nvkm_vmm *, struct nvkm_memory *inst);\n\n\tint (*aper)(enum nvkm_memory_target);\n\tint (*valid)(struct nvkm_vmm *, void *argv, u32 argc,\n\t\t     struct nvkm_vmm_map *);\n\tvoid (*flush)(struct nvkm_vmm *, int depth);\n\n\tint (*mthd)(struct nvkm_vmm *, struct nvkm_client *,\n\t\t    u32 mthd, void *argv, u32 argc);\n\n\tvoid (*invalidate_pdb)(struct nvkm_vmm *, u64 addr);\n\n\tu64 page_block;\n\tconst struct nvkm_vmm_page page[];\n};\n\nstruct nvkm_vmm_join {\n\tstruct nvkm_memory *inst;\n\tstruct list_head head;\n};\n\nint nvkm_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *,\n\t\t  u32 pd_header, bool managed, u64 addr, u64 size,\n\t\t  struct lock_class_key *, const char *name,\n\t\t  struct nvkm_vmm **);\nstruct nvkm_vma *nvkm_vma_new(u64 addr, u64 size);\nstruct nvkm_vma *nvkm_vmm_node_search(struct nvkm_vmm *, u64 addr);\nstruct nvkm_vma *nvkm_vmm_node_split(struct nvkm_vmm *, struct nvkm_vma *,\n\t\t\t\t     u64 addr, u64 size);\nint nvkm_vmm_get_locked(struct nvkm_vmm *, bool getref, bool mapref,\n\t\t\tbool sparse, u8 page, u8 align, u64 size,\n\t\t\tstruct nvkm_vma **pvma);\nvoid nvkm_vmm_put_locked(struct nvkm_vmm *, struct nvkm_vma *);\nvoid nvkm_vmm_unmap_locked(struct nvkm_vmm *, struct nvkm_vma *, bool pfn);\nvoid nvkm_vmm_unmap_region(struct nvkm_vmm *, struct nvkm_vma *);\n\nint nvkm_vmm_raw_get(struct nvkm_vmm *vmm, u64 addr, u64 size, u8 refd);\nvoid nvkm_vmm_raw_put(struct nvkm_vmm *vmm, u64 addr, u64 size, u8 refd);\nvoid nvkm_vmm_raw_unmap(struct nvkm_vmm *vmm, u64 addr, u64 size,\n\t\t\tbool sparse, u8 refd);\nint nvkm_vmm_raw_sparse(struct nvkm_vmm *, u64 addr, u64 size, bool ref);\n\nstatic inline bool\nnvkm_vmm_in_managed_range(struct nvkm_vmm *vmm, u64 start, u64 size)\n{\n\tu64 p_start = vmm->managed.p.addr;\n\tu64 p_end = p_start + vmm->managed.p.size;\n\tu64 n_start = vmm->managed.n.addr;\n\tu64 n_end = n_start + vmm->managed.n.size;\n\tu64 end = start + size;\n\n\tif (start >= p_start && end <= p_end)\n\t\treturn true;\n\n\tif (start >= n_start && end <= n_end)\n\t\treturn true;\n\n\treturn false;\n}\n\n#define NVKM_VMM_PFN_ADDR                                 0xfffffffffffff000ULL\n#define NVKM_VMM_PFN_ADDR_SHIFT                                              12\n#define NVKM_VMM_PFN_APER                                 0x00000000000000f0ULL\n#define NVKM_VMM_PFN_HOST                                 0x0000000000000000ULL\n#define NVKM_VMM_PFN_VRAM                                 0x0000000000000010ULL\n#define NVKM_VMM_PFN_A\t\t\t\t\t  0x0000000000000004ULL\n#define NVKM_VMM_PFN_W                                    0x0000000000000002ULL\n#define NVKM_VMM_PFN_V                                    0x0000000000000001ULL\n#define NVKM_VMM_PFN_NONE                                 0x0000000000000000ULL\n\nint nvkm_vmm_pfn_map(struct nvkm_vmm *, u8 page, u64 addr, u64 size, u64 *pfn);\nint nvkm_vmm_pfn_unmap(struct nvkm_vmm *, u64 addr, u64 size);\n\nstruct nvkm_vma *nvkm_vma_tail(struct nvkm_vma *, u64 tail);\n\nint nv04_vmm_new_(const struct nvkm_vmm_func *, struct nvkm_mmu *, u32,\n\t\t  bool, u64, u64, void *, u32, struct lock_class_key *,\n\t\t  const char *, struct nvkm_vmm **);\nint nv04_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\n\nint nv50_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid nv50_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint nv50_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid nv50_vmm_flush(struct nvkm_vmm *, int);\n\nint gf100_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gf100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nvoid gf100_vmm_part(struct nvkm_vmm *, struct nvkm_memory *);\nint gf100_vmm_aper(enum nvkm_memory_target);\nint gf100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gf100_vmm_flush(struct nvkm_vmm *, int);\nvoid gf100_vmm_invalidate(struct nvkm_vmm *, u32 type);\nvoid gf100_vmm_invalidate_pdb(struct nvkm_vmm *, u64 addr);\n\nint gk20a_vmm_aper(enum nvkm_memory_target);\n\nint gm200_vmm_new_(const struct nvkm_vmm_func *, const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_join_(struct nvkm_vmm *, struct nvkm_memory *, u64 base);\nint gm200_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\n\nint gp100_vmm_new_(const struct nvkm_vmm_func *,\n\t\t   struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t   struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gp100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\nint gp100_vmm_valid(struct nvkm_vmm *, void *, u32, struct nvkm_vmm_map *);\nvoid gp100_vmm_flush(struct nvkm_vmm *, int);\nint gp100_vmm_mthd(struct nvkm_vmm *, struct nvkm_client *, u32, void *, u32);\nvoid gp100_vmm_invalidate_pdb(struct nvkm_vmm *, u64 addr);\n\nint gv100_vmm_join(struct nvkm_vmm *, struct nvkm_memory *);\n\nint nv04_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv41_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv44_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint nv50_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t struct lock_class_key *, const char *, struct nvkm_vmm **);\nint mcp77_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint g84_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\tstruct lock_class_key *, const char *, struct nvkm_vmm **);\nint gf100_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk104_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gk20a_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *, struct nvkm_vmm **);\nint gm200_vmm_new_fixed(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm200_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gm20b_vmm_new_fixed(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t\tstruct lock_class_key *, const char *,\n\t\t\tstruct nvkm_vmm **);\nint gm20b_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp100_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gp10b_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint gv100_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\nint tu102_vmm_new(struct nvkm_mmu *, bool, u64, u64, void *, u32,\n\t\t  struct lock_class_key *, const char *,\n\t\t  struct nvkm_vmm **);\n\n#define VMM_PRINT(l,v,p,f,a...) do {                                           \\\n\tstruct nvkm_vmm *_vmm = (v);                                           \\\n\tif (CONFIG_NOUVEAU_DEBUG >= (l) && _vmm->debug >= (l)) {               \\\n\t\tnvkm_printk_(&_vmm->mmu->subdev, 0, p, \"%s: \"f\"\\n\",            \\\n\t\t\t     _vmm->name, ##a);                                 \\\n\t}                                                                      \\\n} while(0)\n#define VMM_DEBUG(v,f,a...) VMM_PRINT(NV_DBG_DEBUG, (v), info, f, ##a)\n#define VMM_TRACE(v,f,a...) VMM_PRINT(NV_DBG_TRACE, (v), info, f, ##a)\n#define VMM_SPAM(v,f,a...)  VMM_PRINT(NV_DBG_SPAM , (v),  dbg, f, ##a)\n\n#define VMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,BASE,SIZE,NEXT) do {            \\\n\tnvkm_kmap((PT)->memory);                                               \\\n\twhile (PTEN) {                                                         \\\n\t\tu64 _ptes = ((SIZE) - MAP->off) >> MAP->page->shift;           \\\n\t\tu64 _addr = ((BASE) + MAP->off);                               \\\n                                                                               \\\n\t\tif (_ptes > PTEN) {                                            \\\n\t\t\tMAP->off += PTEN << MAP->page->shift;                  \\\n\t\t\t_ptes = PTEN;                                          \\\n\t\t} else {                                                       \\\n\t\t\tMAP->off = 0;                                          \\\n\t\t\tNEXT;                                                  \\\n\t\t}                                                              \\\n                                                                               \\\n\t\tVMM_SPAM(VMM, \"ITER %08x %08x PTE(s)\", PTEI, (u32)_ptes);      \\\n                                                                               \\\n\t\tFILL(VMM, PT, PTEI, _ptes, MAP, _addr);                        \\\n\t\tPTEI += _ptes;                                                 \\\n\t\tPTEN -= _ptes;                                                 \\\n\t}                                                                      \\\n\tnvkm_done((PT)->memory);                                               \\\n} while(0)\n\n#define VMM_MAP_ITER_MEM(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     ((u64)MAP->mem->offset << NVKM_RAM_MM_SHIFT),             \\\n\t\t     ((u64)MAP->mem->length << NVKM_RAM_MM_SHIFT),             \\\n\t\t     (MAP->mem = MAP->mem->next))\n#define VMM_MAP_ITER_DMA(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     *MAP->dma, PAGE_SIZE, MAP->dma++)\n#define VMM_MAP_ITER_SGL(VMM,PT,PTEI,PTEN,MAP,FILL)                            \\\n\tVMM_MAP_ITER(VMM,PT,PTEI,PTEN,MAP,FILL,                                \\\n\t\t     sg_dma_address(MAP->sgl), sg_dma_len(MAP->sgl),           \\\n\t\t     (MAP->sgl = sg_next(MAP->sgl)))\n\n#define VMM_FO(m,o,d,c,b) nvkm_fo##b((m)->memory, (o), (d), (c))\n#define VMM_WO(m,o,d,c,b) nvkm_wo##b((m)->memory, (o), (d))\n#define VMM_XO(m,v,o,d,c,b,fn,f,a...) do {                                     \\\n\tconst u32 _pteo = (o); u##b _data = (d);                               \\\n\tVMM_SPAM((v), \"   %010llx \"f, (m)->addr + _pteo, _data, ##a);          \\\n\tVMM_##fn((m), (m)->base + _pteo, _data, (c), b);                       \\\n} while(0)\n\n#define VMM_WO032(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 32, WO, \"%08x\")\n#define VMM_FO032(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 32, FO, \"%08x %08x\", (c))\n\n#define VMM_WO064(m,v,o,d) VMM_XO((m),(v),(o),(d),  1, 64, WO, \"%016llx\")\n#define VMM_FO064(m,v,o,d,c)                                                   \\\n\tVMM_XO((m),(v),(o),(d),(c), 64, FO, \"%016llx %08x\", (c))\n\n#define VMM_XO128(m,v,o,lo,hi,c,f,a...) do {                                   \\\n\tu32 _pteo = (o), _ptes = (c);                                          \\\n\tconst u64 _addr = (m)->addr + _pteo;                                   \\\n\tVMM_SPAM((v), \"   %010llx %016llx%016llx\"f, _addr, (hi), (lo), ##a);   \\\n\twhile (_ptes--) {                                                      \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 0, (lo));           \\\n\t\tnvkm_wo64((m)->memory, (m)->base + _pteo + 8, (hi));           \\\n\t\t_pteo += 0x10;                                                 \\\n\t}                                                                      \\\n} while(0)\n\n#define VMM_WO128(m,v,o,lo,hi) VMM_XO128((m),(v),(o),(lo),(hi), 1, \"\")\n#define VMM_FO128(m,v,o,lo,hi,c) do {                                          \\\n\tnvkm_kmap((m)->memory);                                                \\\n\tVMM_XO128((m),(v),(o),(lo),(hi),(c), \" %08x\", (c));                    \\\n\tnvkm_done((m)->memory);                                                \\\n} while(0)\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}