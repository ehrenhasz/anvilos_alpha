{
  "module_name": "intr.c",
  "hash_id": "c647a0f50f197ced3f1a6ee352626acb34f52d257e141fc0bb07f1e82276ed7d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/core/intr.c",
  "human_readable_source": " \n#include <core/intr.h>\n#include <core/device.h>\n#include <core/subdev.h>\n#include <subdev/pci.h>\n#include <subdev/top.h>\n\nstatic int\nnvkm_intr_xlat(struct nvkm_subdev *subdev, struct nvkm_intr *intr,\n\t       enum nvkm_intr_type type, int *leaf, u32 *mask)\n{\n\tstruct nvkm_device *device = subdev->device;\n\n\tif (type < NVKM_INTR_VECTOR_0) {\n\t\tif (type == NVKM_INTR_SUBDEV) {\n\t\t\tconst struct nvkm_intr_data *data = intr->data;\n\t\t\tstruct nvkm_top_device *tdev;\n\n\t\t\twhile (data && data->mask) {\n\t\t\t\tif (data->type == NVKM_SUBDEV_TOP) {\n\t\t\t\t\tlist_for_each_entry(tdev, &device->top->device, head) {\n\t\t\t\t\t\tif (tdev->intr >= 0 &&\n\t\t\t\t\t\t    tdev->type == subdev->type &&\n\t\t\t\t\t\t    tdev->inst == subdev->inst) {\n\t\t\t\t\t\t\tif (data->mask & BIT(tdev->intr)) {\n\t\t\t\t\t\t\t\t*leaf = data->leaf;\n\t\t\t\t\t\t\t\t*mask = BIT(tdev->intr);\n\t\t\t\t\t\t\t\treturn 0;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\tif (data->type == subdev->type && data->inst == subdev->inst) {\n\t\t\t\t\t*leaf = data->leaf;\n\t\t\t\t\t*mask = data->mask;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\t\t\tdata++;\n\t\t\t}\n\t\t} else {\n\t\t\treturn -ENOSYS;\n\t\t}\n\t} else {\n\t\tif (type < intr->leaves * sizeof(*intr->stat) * 8) {\n\t\t\t*leaf = type / 32;\n\t\t\t*mask = BIT(type % 32);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic struct nvkm_intr *\nnvkm_intr_find(struct nvkm_subdev *subdev, enum nvkm_intr_type type, int *leaf, u32 *mask)\n{\n\tstruct nvkm_intr *intr;\n\tint ret;\n\n\tlist_for_each_entry(intr, &subdev->device->intr.intr, head) {\n\t\tret = nvkm_intr_xlat(subdev, intr, type, leaf, mask);\n\t\tif (ret == 0)\n\t\t\treturn intr;\n\t}\n\n\treturn NULL;\n}\n\nstatic void\nnvkm_intr_allow_locked(struct nvkm_intr *intr, int leaf, u32 mask)\n{\n\tintr->mask[leaf] |= mask;\n\tif (intr->func->allow) {\n\t\tif (intr->func->reset)\n\t\t\tintr->func->reset(intr, leaf, mask);\n\t\tintr->func->allow(intr, leaf, mask);\n\t}\n}\n\nvoid\nnvkm_intr_allow(struct nvkm_subdev *subdev, enum nvkm_intr_type type)\n{\n\tstruct nvkm_device *device = subdev->device;\n\tstruct nvkm_intr *intr;\n\tunsigned long flags;\n\tint leaf;\n\tu32 mask;\n\n\tintr = nvkm_intr_find(subdev, type, &leaf, &mask);\n\tif (intr) {\n\t\tnvkm_debug(intr->subdev, \"intr %d/%08x allowed by %s\\n\", leaf, mask, subdev->name);\n\t\tspin_lock_irqsave(&device->intr.lock, flags);\n\t\tnvkm_intr_allow_locked(intr, leaf, mask);\n\t\tspin_unlock_irqrestore(&device->intr.lock, flags);\n\t}\n}\n\nstatic void\nnvkm_intr_block_locked(struct nvkm_intr *intr, int leaf, u32 mask)\n{\n\tintr->mask[leaf] &= ~mask;\n\tif (intr->func->block)\n\t\tintr->func->block(intr, leaf, mask);\n}\n\nvoid\nnvkm_intr_block(struct nvkm_subdev *subdev, enum nvkm_intr_type type)\n{\n\tstruct nvkm_device *device = subdev->device;\n\tstruct nvkm_intr *intr;\n\tunsigned long flags;\n\tint leaf;\n\tu32 mask;\n\n\tintr = nvkm_intr_find(subdev, type, &leaf, &mask);\n\tif (intr) {\n\t\tnvkm_debug(intr->subdev, \"intr %d/%08x blocked by %s\\n\", leaf, mask, subdev->name);\n\t\tspin_lock_irqsave(&device->intr.lock, flags);\n\t\tnvkm_intr_block_locked(intr, leaf, mask);\n\t\tspin_unlock_irqrestore(&device->intr.lock, flags);\n\t}\n}\n\nstatic void\nnvkm_intr_rearm_locked(struct nvkm_device *device)\n{\n\tstruct nvkm_intr *intr;\n\n\tlist_for_each_entry(intr, &device->intr.intr, head)\n\t\tintr->func->rearm(intr);\n}\n\nstatic void\nnvkm_intr_unarm_locked(struct nvkm_device *device)\n{\n\tstruct nvkm_intr *intr;\n\n\tlist_for_each_entry(intr, &device->intr.intr, head)\n\t\tintr->func->unarm(intr);\n}\n\nstatic irqreturn_t\nnvkm_intr(int irq, void *arg)\n{\n\tstruct nvkm_device *device = arg;\n\tstruct nvkm_intr *intr;\n\tstruct nvkm_inth *inth;\n\tirqreturn_t ret = IRQ_NONE;\n\tbool pending = false;\n\tint prio, leaf;\n\n\t \n\tspin_lock(&device->intr.lock);\n\tif (!device->intr.armed)\n\t\tgoto done_unlock;\n\n\tnvkm_intr_unarm_locked(device);\n\tnvkm_pci_msi_rearm(device);\n\n\t \n\tlist_for_each_entry(intr, &device->intr.intr, head) {\n\t\tif (intr->func->pending(intr))\n\t\t\tpending = true;\n\t}\n\n\tif (!pending)\n\t\tgoto done;\n\n\t \n\tif (WARN_ON(nvkm_rd32(device, 0x000000) == 0xffffffff))\n\t\tgoto done;\n\n\t \n\tfor (prio = 0; prio < ARRAY_SIZE(device->intr.prio); prio++) {\n\t\tlist_for_each_entry(inth, &device->intr.prio[prio], head) {\n\t\t\tstruct nvkm_intr *intr = inth->intr;\n\n\t\t\tif (intr->stat[inth->leaf] & inth->mask) {\n\t\t\t\tif (atomic_read(&inth->allowed)) {\n\t\t\t\t\tif (intr->func->reset)\n\t\t\t\t\t\tintr->func->reset(intr, inth->leaf, inth->mask);\n\t\t\t\t\tif (inth->func(inth) == IRQ_HANDLED)\n\t\t\t\t\t\tret = IRQ_HANDLED;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (ret == IRQ_NONE) {\n\t\tlist_for_each_entry(intr, &device->intr.intr, head) {\n\t\t\tfor (leaf = 0; leaf < intr->leaves; leaf++) {\n\t\t\t\tif (intr->stat[leaf]) {\n\t\t\t\t\tnvkm_debug(intr->subdev, \"intr%d: %08x\\n\",\n\t\t\t\t\t\t   leaf, intr->stat[leaf]);\n\t\t\t\t\tnvkm_intr_block_locked(intr, leaf, intr->stat[leaf]);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\ndone:\n\t \n\tnvkm_intr_rearm_locked(device);\ndone_unlock:\n\tspin_unlock(&device->intr.lock);\n\treturn ret;\n}\n\nint\nnvkm_intr_add(const struct nvkm_intr_func *func, const struct nvkm_intr_data *data,\n\t      struct nvkm_subdev *subdev, int leaves, struct nvkm_intr *intr)\n{\n\tstruct nvkm_device *device = subdev->device;\n\tint i;\n\n\tintr->func = func;\n\tintr->data = data;\n\tintr->subdev = subdev;\n\tintr->leaves = leaves;\n\tintr->stat = kcalloc(leaves, sizeof(*intr->stat), GFP_KERNEL);\n\tintr->mask = kcalloc(leaves, sizeof(*intr->mask), GFP_KERNEL);\n\tif (!intr->stat || !intr->mask) {\n\t\tkfree(intr->stat);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (intr->subdev->debug >= NV_DBG_DEBUG) {\n\t\tfor (i = 0; i < intr->leaves; i++)\n\t\t\tintr->mask[i] = ~0;\n\t}\n\n\tspin_lock_irq(&device->intr.lock);\n\tlist_add_tail(&intr->head, &device->intr.intr);\n\tspin_unlock_irq(&device->intr.lock);\n\treturn 0;\n}\n\nstatic irqreturn_t\nnvkm_intr_subdev(struct nvkm_inth *inth)\n{\n\tstruct nvkm_subdev *subdev = container_of(inth, typeof(*subdev), inth);\n\n\tnvkm_subdev_intr(subdev);\n\treturn IRQ_HANDLED;\n}\n\nstatic void\nnvkm_intr_subdev_add_dev(struct nvkm_intr *intr, enum nvkm_subdev_type type, int inst)\n{\n\tstruct nvkm_subdev *subdev;\n\tenum nvkm_intr_prio prio;\n\tint ret;\n\n\tsubdev = nvkm_device_subdev(intr->subdev->device, type, inst);\n\tif (!subdev || !subdev->func->intr)\n\t\treturn;\n\n\tif (type == NVKM_ENGINE_DISP)\n\t\tprio = NVKM_INTR_PRIO_VBLANK;\n\telse\n\t\tprio = NVKM_INTR_PRIO_NORMAL;\n\n\tret = nvkm_inth_add(intr, NVKM_INTR_SUBDEV, prio, subdev, nvkm_intr_subdev, &subdev->inth);\n\tif (WARN_ON(ret))\n\t\treturn;\n\n\tnvkm_inth_allow(&subdev->inth);\n}\n\nstatic void\nnvkm_intr_subdev_add(struct nvkm_intr *intr)\n{\n\tconst struct nvkm_intr_data *data;\n\tstruct nvkm_device *device = intr->subdev->device;\n\tstruct nvkm_top_device *tdev;\n\n\tfor (data = intr->data; data && data->mask; data++) {\n\t\tif (data->legacy) {\n\t\t\tif (data->type == NVKM_SUBDEV_TOP) {\n\t\t\t\tlist_for_each_entry(tdev, &device->top->device, head) {\n\t\t\t\t\tif (tdev->intr < 0 || !(data->mask & BIT(tdev->intr)))\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\tnvkm_intr_subdev_add_dev(intr, tdev->type, tdev->inst);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tnvkm_intr_subdev_add_dev(intr, data->type, data->inst);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid\nnvkm_intr_rearm(struct nvkm_device *device)\n{\n\tstruct nvkm_intr *intr;\n\tint i;\n\n\tif (unlikely(!device->intr.legacy_done)) {\n\t\tlist_for_each_entry(intr, &device->intr.intr, head)\n\t\t\tnvkm_intr_subdev_add(intr);\n\t\tdevice->intr.legacy_done = true;\n\t}\n\n\tspin_lock_irq(&device->intr.lock);\n\tlist_for_each_entry(intr, &device->intr.intr, head) {\n\t\tfor (i = 0; intr->func->block && i < intr->leaves; i++) {\n\t\t\tintr->func->block(intr, i, ~0);\n\t\t\tintr->func->allow(intr, i, intr->mask[i]);\n\t\t}\n\t}\n\n\tnvkm_intr_rearm_locked(device);\n\tdevice->intr.armed = true;\n\tspin_unlock_irq(&device->intr.lock);\n}\n\nvoid\nnvkm_intr_unarm(struct nvkm_device *device)\n{\n\tspin_lock_irq(&device->intr.lock);\n\tnvkm_intr_unarm_locked(device);\n\tdevice->intr.armed = false;\n\tspin_unlock_irq(&device->intr.lock);\n}\n\nint\nnvkm_intr_install(struct nvkm_device *device)\n{\n\tint ret;\n\n\tdevice->intr.irq = device->func->irq(device);\n\tif (device->intr.irq < 0)\n\t\treturn device->intr.irq;\n\n\tret = request_irq(device->intr.irq, nvkm_intr, IRQF_SHARED, \"nvkm\", device);\n\tif (ret)\n\t\treturn ret;\n\n\tdevice->intr.alloc = true;\n\treturn 0;\n}\n\nvoid\nnvkm_intr_dtor(struct nvkm_device *device)\n{\n\tstruct nvkm_intr *intr, *intt;\n\n\tlist_for_each_entry_safe(intr, intt, &device->intr.intr, head) {\n\t\tlist_del(&intr->head);\n\t\tkfree(intr->mask);\n\t\tkfree(intr->stat);\n\t}\n\n\tif (device->intr.alloc)\n\t\tfree_irq(device->intr.irq, device);\n}\n\nvoid\nnvkm_intr_ctor(struct nvkm_device *device)\n{\n\tint i;\n\n\tINIT_LIST_HEAD(&device->intr.intr);\n\tfor (i = 0; i < ARRAY_SIZE(device->intr.prio); i++)\n\t\tINIT_LIST_HEAD(&device->intr.prio[i]);\n\n\tspin_lock_init(&device->intr.lock);\n\tdevice->intr.armed = false;\n}\n\nvoid\nnvkm_inth_block(struct nvkm_inth *inth)\n{\n\tif (unlikely(!inth->intr))\n\t\treturn;\n\n\tatomic_set(&inth->allowed, 0);\n}\n\nvoid\nnvkm_inth_allow(struct nvkm_inth *inth)\n{\n\tstruct nvkm_intr *intr = inth->intr;\n\tunsigned long flags;\n\n\tif (unlikely(!inth->intr))\n\t\treturn;\n\n\tspin_lock_irqsave(&intr->subdev->device->intr.lock, flags);\n\tif (!atomic_xchg(&inth->allowed, 1)) {\n\t\tif ((intr->mask[inth->leaf] & inth->mask) != inth->mask)\n\t\t\tnvkm_intr_allow_locked(intr, inth->leaf, inth->mask);\n\t}\n\tspin_unlock_irqrestore(&intr->subdev->device->intr.lock, flags);\n}\n\nint\nnvkm_inth_add(struct nvkm_intr *intr, enum nvkm_intr_type type, enum nvkm_intr_prio prio,\n\t      struct nvkm_subdev *subdev, nvkm_inth_func func, struct nvkm_inth *inth)\n{\n\tstruct nvkm_device *device = subdev->device;\n\tint ret;\n\n\tif (WARN_ON(inth->mask))\n\t\treturn -EBUSY;\n\n\tret = nvkm_intr_xlat(subdev, intr, type, &inth->leaf, &inth->mask);\n\tif (ret)\n\t\treturn ret;\n\n\tnvkm_debug(intr->subdev, \"intr %d/%08x requested by %s\\n\",\n\t\t   inth->leaf, inth->mask, subdev->name);\n\n\tinth->intr = intr;\n\tinth->func = func;\n\tatomic_set(&inth->allowed, 0);\n\tlist_add_tail(&inth->head, &device->intr.prio[prio]);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}