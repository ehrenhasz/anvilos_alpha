{
  "module_name": "runl.c",
  "hash_id": "baee234ba2c8f77b89289979f3ea9214fca2d468a22503f870ec234910bbc119",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nvkm/engine/fifo/runl.c",
  "human_readable_source": " \n#include \"runl.h\"\n#include \"cgrp.h\"\n#include \"chan.h\"\n#include \"chid.h\"\n#include \"priv.h\"\n#include \"runq.h\"\n\n#include <core/gpuobj.h>\n#include <subdev/timer.h>\n#include <subdev/top.h>\n\nstatic struct nvkm_cgrp *\nnvkm_engn_cgrp_get(struct nvkm_engn *engn, unsigned long *pirqflags)\n{\n\tstruct nvkm_cgrp *cgrp = NULL;\n\tstruct nvkm_chan *chan;\n\tbool cgid;\n\tint id;\n\n\tid = engn->func->cxid(engn, &cgid);\n\tif (id < 0)\n\t\treturn NULL;\n\n\tif (!cgid) {\n\t\tchan = nvkm_runl_chan_get_chid(engn->runl, id, pirqflags);\n\t\tif (chan)\n\t\t\tcgrp = chan->cgrp;\n\t} else {\n\t\tcgrp = nvkm_runl_cgrp_get_cgid(engn->runl, id, pirqflags);\n\t}\n\n\tWARN_ON(!cgrp);\n\treturn cgrp;\n}\n\nstatic void\nnvkm_runl_rc(struct nvkm_runl *runl)\n{\n\tstruct nvkm_fifo *fifo = runl->fifo;\n\tstruct nvkm_cgrp *cgrp, *gtmp;\n\tstruct nvkm_chan *chan, *ctmp;\n\tstruct nvkm_engn *engn;\n\tunsigned long flags;\n\tint rc, state, i;\n\tbool reset;\n\n\t \n\tBUG_ON(!mutex_is_locked(&runl->mutex));\n\trc = atomic_xchg(&runl->rc_pending, 0);\n\tif (!rc)\n\t\treturn;\n\n\t \n\tnvkm_runl_foreach_cgrp_safe(cgrp, gtmp, runl) {\n\t\tstate = atomic_cmpxchg(&cgrp->rc, NVKM_CGRP_RC_PENDING, NVKM_CGRP_RC_RUNNING);\n\t\tif (state == NVKM_CGRP_RC_PENDING) {\n\t\t\t \n\t\t\tnvkm_cgrp_foreach_chan_safe(chan, ctmp, cgrp) {\n\t\t\t\tnvkm_chan_error(chan, false);\n\t\t\t\tnvkm_chan_remove_locked(chan);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (runl->func->preempt) {\n\t\tfor (i = 0; i < runl->runq_nr; i++) {\n\t\t\tstruct nvkm_runq *runq = runl->runq[i];\n\n\t\t\tif (runq) {\n\t\t\t\tnvkm_msec(fifo->engine.subdev.device, 2000,\n\t\t\t\t\tif (runq->func->idle(runq))\n\t\t\t\t\t\tbreak;\n\t\t\t\t);\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tnvkm_runl_foreach_engn_cond(engn, runl, engn->func->cxid) {\n\t\tcgrp = nvkm_engn_cgrp_get(engn, &flags);\n\t\tif (!cgrp) {\n\t\t\tENGN_DEBUG(engn, \"cxid not valid\");\n\t\t\tcontinue;\n\t\t}\n\n\t\treset = atomic_read(&cgrp->rc) == NVKM_CGRP_RC_RUNNING;\n\t\tnvkm_cgrp_put(&cgrp, flags);\n\t\tif (!reset) {\n\t\t\tENGN_DEBUG(engn, \"cxid not in recovery\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tENGN_DEBUG(engn, \"resetting...\");\n\t\t \n\t\tWARN_ON(nvkm_engine_reset(engn->engine));\n\t}\n\n\t \n\trunl->func->update(runl);\n\tif (runl->func->fault_clear)\n\t\trunl->func->fault_clear(runl);\n\n\t \n\twhile (rc--)\n\t\tnvkm_runl_allow(runl);\n\trunl->func->wait(runl);\n}\n\nstatic void\nnvkm_runl_rc_runl(struct nvkm_runl *runl)\n{\n\tRUNL_ERROR(runl, \"rc scheduled\");\n\n\tnvkm_runl_block(runl);\n\tif (runl->func->preempt)\n\t\trunl->func->preempt(runl);\n\n\tatomic_inc(&runl->rc_pending);\n\tschedule_work(&runl->work);\n}\n\nvoid\nnvkm_runl_rc_cgrp(struct nvkm_cgrp *cgrp)\n{\n\tif (atomic_cmpxchg(&cgrp->rc, NVKM_CGRP_RC_NONE, NVKM_CGRP_RC_PENDING) != NVKM_CGRP_RC_NONE)\n\t\treturn;\n\n\tCGRP_ERROR(cgrp, \"rc scheduled\");\n\tnvkm_runl_rc_runl(cgrp->runl);\n}\n\nvoid\nnvkm_runl_rc_engn(struct nvkm_runl *runl, struct nvkm_engn *engn)\n{\n\tstruct nvkm_cgrp *cgrp;\n\tunsigned long flags;\n\n\t \n\tcgrp = nvkm_engn_cgrp_get(engn, &flags);\n\tif (!cgrp) {\n\t\tENGN_DEBUG(engn, \"rc skipped, not on channel\");\n\t\treturn;\n\t}\n\n\tnvkm_runl_rc_cgrp(cgrp);\n\tnvkm_cgrp_put(&cgrp, flags);\n}\n\nstatic void\nnvkm_runl_work(struct work_struct *work)\n{\n\tstruct nvkm_runl *runl = container_of(work, typeof(*runl), work);\n\n\tmutex_lock(&runl->mutex);\n\tnvkm_runl_rc(runl);\n\tmutex_unlock(&runl->mutex);\n\n}\n\nstruct nvkm_chan *\nnvkm_runl_chan_get_inst(struct nvkm_runl *runl, u64 inst, unsigned long *pirqflags)\n{\n\tstruct nvkm_chid *chid = runl->chid;\n\tstruct nvkm_chan *chan;\n\tunsigned long flags;\n\tint id;\n\n\tspin_lock_irqsave(&chid->lock, flags);\n\tfor_each_set_bit(id, chid->used, chid->nr) {\n\t\tchan = chid->data[id];\n\t\tif (likely(chan)) {\n\t\t\tif (chan->inst->addr == inst) {\n\t\t\t\tspin_lock(&chan->cgrp->lock);\n\t\t\t\t*pirqflags = flags;\n\t\t\t\tspin_unlock(&chid->lock);\n\t\t\t\treturn chan;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&chid->lock, flags);\n\treturn NULL;\n}\n\nstruct nvkm_chan *\nnvkm_runl_chan_get_chid(struct nvkm_runl *runl, int id, unsigned long *pirqflags)\n{\n\tstruct nvkm_chid *chid = runl->chid;\n\tstruct nvkm_chan *chan;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&chid->lock, flags);\n\tif (!WARN_ON(id >= chid->nr)) {\n\t\tchan = chid->data[id];\n\t\tif (likely(chan)) {\n\t\t\tspin_lock(&chan->cgrp->lock);\n\t\t\t*pirqflags = flags;\n\t\t\tspin_unlock(&chid->lock);\n\t\t\treturn chan;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&chid->lock, flags);\n\treturn NULL;\n}\n\nstruct nvkm_cgrp *\nnvkm_runl_cgrp_get_cgid(struct nvkm_runl *runl, int id, unsigned long *pirqflags)\n{\n\tstruct nvkm_chid *cgid = runl->cgid;\n\tstruct nvkm_cgrp *cgrp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cgid->lock, flags);\n\tif (!WARN_ON(id >= cgid->nr)) {\n\t\tcgrp = cgid->data[id];\n\t\tif (likely(cgrp)) {\n\t\t\tspin_lock(&cgrp->lock);\n\t\t\t*pirqflags = flags;\n\t\t\tspin_unlock(&cgid->lock);\n\t\t\treturn cgrp;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&cgid->lock, flags);\n\treturn NULL;\n}\n\nint\nnvkm_runl_preempt_wait(struct nvkm_runl *runl)\n{\n\treturn nvkm_msec(runl->fifo->engine.subdev.device, runl->fifo->timeout.chan_msec,\n\t\tif (!runl->func->preempt_pending(runl))\n\t\t\tbreak;\n\n\t\tnvkm_runl_rc(runl);\n\t\tusleep_range(1, 2);\n\t) < 0 ? -ETIMEDOUT : 0;\n}\n\nbool\nnvkm_runl_update_pending(struct nvkm_runl *runl)\n{\n\tif (!runl->func->pending(runl))\n\t\treturn false;\n\n\tnvkm_runl_rc(runl);\n\treturn true;\n}\n\nvoid\nnvkm_runl_update_locked(struct nvkm_runl *runl, bool wait)\n{\n\tif (atomic_xchg(&runl->changed, 0) && runl->func->update) {\n\t\trunl->func->update(runl);\n\t\tif (wait)\n\t\t\trunl->func->wait(runl);\n\t}\n}\n\nvoid\nnvkm_runl_allow(struct nvkm_runl *runl)\n{\n\tstruct nvkm_fifo *fifo = runl->fifo;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fifo->lock, flags);\n\tif (!--runl->blocked) {\n\t\tRUNL_TRACE(runl, \"running\");\n\t\trunl->func->allow(runl, ~0);\n\t}\n\tspin_unlock_irqrestore(&fifo->lock, flags);\n}\n\nvoid\nnvkm_runl_block(struct nvkm_runl *runl)\n{\n\tstruct nvkm_fifo *fifo = runl->fifo;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fifo->lock, flags);\n\tif (!runl->blocked++) {\n\t\tRUNL_TRACE(runl, \"stopped\");\n\t\trunl->func->block(runl, ~0);\n\t}\n\tspin_unlock_irqrestore(&fifo->lock, flags);\n}\n\nvoid\nnvkm_runl_fini(struct nvkm_runl *runl)\n{\n\tif (runl->func->fini)\n\t\trunl->func->fini(runl);\n\n\tflush_work(&runl->work);\n}\n\nvoid\nnvkm_runl_del(struct nvkm_runl *runl)\n{\n\tstruct nvkm_engn *engn, *engt;\n\n\tnvkm_memory_unref(&runl->mem);\n\n\tlist_for_each_entry_safe(engn, engt, &runl->engns, head) {\n\t\tlist_del(&engn->head);\n\t\tkfree(engn);\n\t}\n\n\tnvkm_chid_unref(&runl->chid);\n\tnvkm_chid_unref(&runl->cgid);\n\n\tlist_del(&runl->head);\n\tmutex_destroy(&runl->mutex);\n\tkfree(runl);\n}\n\nstruct nvkm_engn *\nnvkm_runl_add(struct nvkm_runl *runl, int engi, const struct nvkm_engn_func *func,\n\t      enum nvkm_subdev_type type, int inst)\n{\n\tstruct nvkm_fifo *fifo = runl->fifo;\n\tstruct nvkm_device *device = fifo->engine.subdev.device;\n\tstruct nvkm_engine *engine;\n\tstruct nvkm_engn *engn;\n\n\tengine = nvkm_device_engine(device, type, inst);\n\tif (!engine) {\n\t\tRUNL_DEBUG(runl, \"engn %d.%d[%s] not found\", engi, inst, nvkm_subdev_type[type]);\n\t\treturn NULL;\n\t}\n\n\tif (!(engn = kzalloc(sizeof(*engn), GFP_KERNEL)))\n\t\treturn NULL;\n\n\tengn->func = func;\n\tengn->runl = runl;\n\tengn->id = engi;\n\tengn->engine = engine;\n\tengn->fault = -1;\n\tlist_add_tail(&engn->head, &runl->engns);\n\n\t \n\tif (device->top)\n\t\tengn->fault = nvkm_top_fault_id(device, engine->subdev.type, engine->subdev.inst);\n\n\tif (engn->fault < 0 && fifo->func->mmu_fault) {\n\t\tconst struct nvkm_enum *map = fifo->func->mmu_fault->engine;\n\n\t\twhile (map->name) {\n\t\t\tif (map->data2 == engine->subdev.type && map->inst == engine->subdev.inst) {\n\t\t\t\tengn->fault = map->value;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmap++;\n\t\t}\n\t}\n\n\treturn engn;\n}\n\nstruct nvkm_runl *\nnvkm_runl_get(struct nvkm_fifo *fifo, int runi, u32 addr)\n{\n\tstruct nvkm_runl *runl;\n\n\tnvkm_runl_foreach(runl, fifo) {\n\t\tif ((runi >= 0 && runl->id == runi) || (runi < 0 && runl->addr == addr))\n\t\t\treturn runl;\n\t}\n\n\treturn NULL;\n}\n\nstruct nvkm_runl *\nnvkm_runl_new(struct nvkm_fifo *fifo, int runi, u32 addr, int id_nr)\n{\n\tstruct nvkm_subdev *subdev = &fifo->engine.subdev;\n\tstruct nvkm_runl *runl;\n\tint ret;\n\n\tif (!(runl = kzalloc(sizeof(*runl), GFP_KERNEL)))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trunl->func = fifo->func->runl;\n\trunl->fifo = fifo;\n\trunl->id = runi;\n\trunl->addr = addr;\n\tINIT_LIST_HEAD(&runl->engns);\n\tINIT_LIST_HEAD(&runl->cgrps);\n\tatomic_set(&runl->changed, 0);\n\tmutex_init(&runl->mutex);\n\tINIT_WORK(&runl->work, nvkm_runl_work);\n\tatomic_set(&runl->rc_triggered, 0);\n\tatomic_set(&runl->rc_pending, 0);\n\tlist_add_tail(&runl->head, &fifo->runls);\n\n\tif (!fifo->chid) {\n\t\tif ((ret = nvkm_chid_new(&nvkm_chan_event, subdev, id_nr, 0, id_nr, &runl->cgid)) ||\n\t\t    (ret = nvkm_chid_new(&nvkm_chan_event, subdev, id_nr, 0, id_nr, &runl->chid))) {\n\t\t\tRUNL_ERROR(runl, \"cgid/chid: %d\", ret);\n\t\t\tnvkm_runl_del(runl);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t} else {\n\t\trunl->cgid = nvkm_chid_ref(fifo->cgid);\n\t\trunl->chid = nvkm_chid_ref(fifo->chid);\n\t}\n\n\treturn runl;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}