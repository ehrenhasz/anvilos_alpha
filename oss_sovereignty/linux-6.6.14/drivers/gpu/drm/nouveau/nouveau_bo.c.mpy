{
  "module_name": "nouveau_bo.c",
  "hash_id": "1a03c0e37a0e63f9cc1411d9f10243794d18520a7f3a8d7c3acf6eee7b22bc22",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nouveau_bo.c",
  "human_readable_source": " \n \n\n#include <linux/dma-mapping.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include \"nouveau_drv.h\"\n#include \"nouveau_chan.h\"\n#include \"nouveau_fence.h\"\n\n#include \"nouveau_bo.h\"\n#include \"nouveau_ttm.h\"\n#include \"nouveau_gem.h\"\n#include \"nouveau_mem.h\"\n#include \"nouveau_vmm.h\"\n\n#include <nvif/class.h>\n#include <nvif/if500b.h>\n#include <nvif/if900b.h>\n\nstatic int nouveau_ttm_tt_bind(struct ttm_device *bdev, struct ttm_tt *ttm,\n\t\t\t       struct ttm_resource *reg);\nstatic void nouveau_ttm_tt_unbind(struct ttm_device *bdev, struct ttm_tt *ttm);\n\n \n\nstatic void\nnv10_bo_update_tile_region(struct drm_device *dev, struct nouveau_drm_tile *reg,\n\t\t\t   u32 addr, u32 size, u32 pitch, u32 flags)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\tint i = reg - drm->tile.reg;\n\tstruct nvkm_fb *fb = nvxx_fb(&drm->client.device);\n\tstruct nvkm_fb_tile *tile = &fb->tile.region[i];\n\n\tnouveau_fence_unref(&reg->fence);\n\n\tif (tile->pitch)\n\t\tnvkm_fb_tile_fini(fb, i, tile);\n\n\tif (pitch)\n\t\tnvkm_fb_tile_init(fb, i, addr, size, pitch, flags, tile);\n\n\tnvkm_fb_tile_prog(fb, i, tile);\n}\n\nstatic struct nouveau_drm_tile *\nnv10_bo_get_tile_region(struct drm_device *dev, int i)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\tstruct nouveau_drm_tile *tile = &drm->tile.reg[i];\n\n\tspin_lock(&drm->tile.lock);\n\n\tif (!tile->used &&\n\t    (!tile->fence || nouveau_fence_done(tile->fence)))\n\t\ttile->used = true;\n\telse\n\t\ttile = NULL;\n\n\tspin_unlock(&drm->tile.lock);\n\treturn tile;\n}\n\nstatic void\nnv10_bo_put_tile_region(struct drm_device *dev, struct nouveau_drm_tile *tile,\n\t\t\tstruct dma_fence *fence)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\n\tif (tile) {\n\t\tspin_lock(&drm->tile.lock);\n\t\ttile->fence = (struct nouveau_fence *)dma_fence_get(fence);\n\t\ttile->used = false;\n\t\tspin_unlock(&drm->tile.lock);\n\t}\n}\n\nstatic struct nouveau_drm_tile *\nnv10_bo_set_tiling(struct drm_device *dev, u32 addr,\n\t\t   u32 size, u32 pitch, u32 zeta)\n{\n\tstruct nouveau_drm *drm = nouveau_drm(dev);\n\tstruct nvkm_fb *fb = nvxx_fb(&drm->client.device);\n\tstruct nouveau_drm_tile *tile, *found = NULL;\n\tint i;\n\n\tfor (i = 0; i < fb->tile.regions; i++) {\n\t\ttile = nv10_bo_get_tile_region(dev, i);\n\n\t\tif (pitch && !found) {\n\t\t\tfound = tile;\n\t\t\tcontinue;\n\n\t\t} else if (tile && fb->tile.region[i].pitch) {\n\t\t\t \n\t\t\tnv10_bo_update_tile_region(dev, tile, 0, 0, 0, 0);\n\t\t}\n\n\t\tnv10_bo_put_tile_region(dev, tile, NULL);\n\t}\n\n\tif (found)\n\t\tnv10_bo_update_tile_region(dev, found, addr, size, pitch, zeta);\n\treturn found;\n}\n\nstatic void\nnouveau_bo_del_ttm(struct ttm_buffer_object *bo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct drm_device *dev = drm->dev;\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\n\tWARN_ON(nvbo->bo.pin_count > 0);\n\tnouveau_bo_del_io_reserve_lru(bo);\n\tnv10_bo_put_tile_region(dev, nvbo->tile, NULL);\n\n\t \n\tif (bo->base.dev)\n\t\tdrm_gem_object_release(&bo->base);\n\telse\n\t\tdma_resv_fini(&bo->base._resv);\n\n\tkfree(nvbo);\n}\n\nstatic inline u64\nroundup_64(u64 x, u32 y)\n{\n\tx += y - 1;\n\tdo_div(x, y);\n\treturn x * y;\n}\n\nstatic void\nnouveau_bo_fixup_align(struct nouveau_bo *nvbo, int *align, u64 *size)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tstruct nvif_device *device = &drm->client.device;\n\n\tif (device->info.family < NV_DEVICE_INFO_V0_TESLA) {\n\t\tif (nvbo->mode) {\n\t\t\tif (device->info.chipset >= 0x40) {\n\t\t\t\t*align = 65536;\n\t\t\t\t*size = roundup_64(*size, 64 * nvbo->mode);\n\n\t\t\t} else if (device->info.chipset >= 0x30) {\n\t\t\t\t*align = 32768;\n\t\t\t\t*size = roundup_64(*size, 64 * nvbo->mode);\n\n\t\t\t} else if (device->info.chipset >= 0x20) {\n\t\t\t\t*align = 16384;\n\t\t\t\t*size = roundup_64(*size, 64 * nvbo->mode);\n\n\t\t\t} else if (device->info.chipset >= 0x10) {\n\t\t\t\t*align = 16384;\n\t\t\t\t*size = roundup_64(*size, 32 * nvbo->mode);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t*size = roundup_64(*size, (1 << nvbo->page));\n\t\t*align = max((1 <<  nvbo->page), *align);\n\t}\n\n\t*size = roundup_64(*size, PAGE_SIZE);\n}\n\nstruct nouveau_bo *\nnouveau_bo_alloc(struct nouveau_cli *cli, u64 *size, int *align, u32 domain,\n\t\t u32 tile_mode, u32 tile_flags, bool internal)\n{\n\tstruct nouveau_drm *drm = cli->drm;\n\tstruct nouveau_bo *nvbo;\n\tstruct nvif_mmu *mmu = &cli->mmu;\n\tstruct nvif_vmm *vmm = &nouveau_cli_vmm(cli)->vmm;\n\tint i, pi = -1;\n\n\tif (!*size) {\n\t\tNV_WARN(drm, \"skipped size %016llx\\n\", *size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tnvbo = kzalloc(sizeof(struct nouveau_bo), GFP_KERNEL);\n\tif (!nvbo)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&nvbo->head);\n\tINIT_LIST_HEAD(&nvbo->entry);\n\tINIT_LIST_HEAD(&nvbo->vma_list);\n\tnvbo->bo.bdev = &drm->ttm.bdev;\n\n\t \n\tif (domain & NOUVEAU_GEM_DOMAIN_COHERENT) {\n\t\t \n\t\tif (!nouveau_drm_use_coherent_gpu_mapping(drm))\n\t\t\tnvbo->force_coherent = true;\n\t}\n\n\tnvbo->contig = !(tile_flags & NOUVEAU_GEM_TILE_NONCONTIG);\n\tif (!nouveau_cli_uvmm(cli) || internal) {\n\t\t \n\t\tif (cli->device.info.family >= NV_DEVICE_INFO_V0_FERMI) {\n\t\t\tnvbo->kind = (tile_flags & 0x0000ff00) >> 8;\n\t\t\tif (!nvif_mmu_kind_valid(mmu, nvbo->kind)) {\n\t\t\t\tkfree(nvbo);\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\n\t\t\tnvbo->comp = mmu->kind[nvbo->kind] != nvbo->kind;\n\t\t} else if (cli->device.info.family >= NV_DEVICE_INFO_V0_TESLA) {\n\t\t\tnvbo->kind = (tile_flags & 0x00007f00) >> 8;\n\t\t\tnvbo->comp = (tile_flags & 0x00030000) >> 16;\n\t\t\tif (!nvif_mmu_kind_valid(mmu, nvbo->kind)) {\n\t\t\t\tkfree(nvbo);\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t} else {\n\t\t\tnvbo->zeta = (tile_flags & 0x00000007);\n\t\t}\n\t\tnvbo->mode = tile_mode;\n\n\t\t \n\t\tfor (i = 0; i < vmm->page_nr; i++) {\n\t\t\t \n\t\t\tif (cli->device.info.family > NV_DEVICE_INFO_V0_CURIE &&\n\t\t\t    (domain & NOUVEAU_GEM_DOMAIN_VRAM) && !vmm->page[i].vram)\n\t\t\t\tcontinue;\n\t\t\tif ((domain & NOUVEAU_GEM_DOMAIN_GART) &&\n\t\t\t    (!vmm->page[i].host || vmm->page[i].shift > PAGE_SHIFT))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (pi < 0 || !nvbo->comp || vmm->page[i].comp)\n\t\t\t\tpi = i;\n\n\t\t\t \n\t\t\tif (*size >= 1ULL << vmm->page[i].shift)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (WARN_ON(pi < 0)) {\n\t\t\tkfree(nvbo);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\t \n\t\tif (nvbo->comp && !vmm->page[pi].comp) {\n\t\t\tif (mmu->object.oclass >= NVIF_CLASS_MMU_GF100)\n\t\t\t\tnvbo->kind = mmu->kind[nvbo->kind];\n\t\t\tnvbo->comp = 0;\n\t\t}\n\t\tnvbo->page = vmm->page[pi].shift;\n\t} else {\n\t\t \n\t\tif (tile_mode)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tif (tile_flags & ~NOUVEAU_GEM_TILE_NONCONTIG)\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\t \n\t\tfor (i = 0; i < vmm->page_nr; i++) {\n\t\t\t \n\t\t\tif ((domain & NOUVEAU_GEM_DOMAIN_VRAM) && !vmm->page[i].vram)\n\t\t\t\tcontinue;\n\t\t\tif ((domain & NOUVEAU_GEM_DOMAIN_GART) &&\n\t\t\t    (!vmm->page[i].host || vmm->page[i].shift > PAGE_SHIFT))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tpi = i;\n\n\t\t\t \n\t\t\tif (*size >= 1ULL << vmm->page[i].shift)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (WARN_ON(pi < 0)) {\n\t\t\tkfree(nvbo);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\tnvbo->page = vmm->page[pi].shift;\n\t}\n\n\tnouveau_bo_fixup_align(nvbo, align, size);\n\n\treturn nvbo;\n}\n\nint\nnouveau_bo_init(struct nouveau_bo *nvbo, u64 size, int align, u32 domain,\n\t\tstruct sg_table *sg, struct dma_resv *robj)\n{\n\tint type = sg ? ttm_bo_type_sg : ttm_bo_type_device;\n\tint ret;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = false,\n\t\t.no_wait_gpu = false,\n\t\t.resv = robj,\n\t};\n\n\tnouveau_bo_placement_set(nvbo, domain, 0);\n\tINIT_LIST_HEAD(&nvbo->io_reserve_lru);\n\n\tret = ttm_bo_init_reserved(nvbo->bo.bdev, &nvbo->bo, type,\n\t\t\t\t   &nvbo->placement, align >> PAGE_SHIFT, &ctx,\n\t\t\t\t   sg, robj, nouveau_bo_del_ttm);\n\tif (ret) {\n\t\t \n\t\treturn ret;\n\t}\n\n\tif (!robj)\n\t\tttm_bo_unreserve(&nvbo->bo);\n\n\treturn 0;\n}\n\nint\nnouveau_bo_new(struct nouveau_cli *cli, u64 size, int align,\n\t       uint32_t domain, uint32_t tile_mode, uint32_t tile_flags,\n\t       struct sg_table *sg, struct dma_resv *robj,\n\t       struct nouveau_bo **pnvbo)\n{\n\tstruct nouveau_bo *nvbo;\n\tint ret;\n\n\tnvbo = nouveau_bo_alloc(cli, &size, &align, domain, tile_mode,\n\t\t\t\ttile_flags, true);\n\tif (IS_ERR(nvbo))\n\t\treturn PTR_ERR(nvbo);\n\n\tnvbo->bo.base.size = size;\n\tdma_resv_init(&nvbo->bo.base._resv);\n\tdrm_vma_node_reset(&nvbo->bo.base.vma_node);\n\n\t \n\tdrm_gem_gpuva_init(&nvbo->bo.base);\n\n\tret = nouveau_bo_init(nvbo, size, align, domain, sg, robj);\n\tif (ret)\n\t\treturn ret;\n\n\t*pnvbo = nvbo;\n\treturn 0;\n}\n\nstatic void\nset_placement_list(struct ttm_place *pl, unsigned *n, uint32_t domain)\n{\n\t*n = 0;\n\n\tif (domain & NOUVEAU_GEM_DOMAIN_VRAM) {\n\t\tpl[*n].mem_type = TTM_PL_VRAM;\n\t\tpl[*n].flags = 0;\n\t\t(*n)++;\n\t}\n\tif (domain & NOUVEAU_GEM_DOMAIN_GART) {\n\t\tpl[*n].mem_type = TTM_PL_TT;\n\t\tpl[*n].flags = 0;\n\t\t(*n)++;\n\t}\n\tif (domain & NOUVEAU_GEM_DOMAIN_CPU) {\n\t\tpl[*n].mem_type = TTM_PL_SYSTEM;\n\t\tpl[(*n)++].flags = 0;\n\t}\n}\n\nstatic void\nset_placement_range(struct nouveau_bo *nvbo, uint32_t domain)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tu64 vram_size = drm->client.device.info.ram_size;\n\tunsigned i, fpfn, lpfn;\n\n\tif (drm->client.device.info.family == NV_DEVICE_INFO_V0_CELSIUS &&\n\t    nvbo->mode && (domain & NOUVEAU_GEM_DOMAIN_VRAM) &&\n\t    nvbo->bo.base.size < vram_size / 4) {\n\t\t \n\t\tif (nvbo->zeta) {\n\t\t\tfpfn = (vram_size / 2) >> PAGE_SHIFT;\n\t\t\tlpfn = ~0;\n\t\t} else {\n\t\t\tfpfn = 0;\n\t\t\tlpfn = (vram_size / 2) >> PAGE_SHIFT;\n\t\t}\n\t\tfor (i = 0; i < nvbo->placement.num_placement; ++i) {\n\t\t\tnvbo->placements[i].fpfn = fpfn;\n\t\t\tnvbo->placements[i].lpfn = lpfn;\n\t\t}\n\t\tfor (i = 0; i < nvbo->placement.num_busy_placement; ++i) {\n\t\t\tnvbo->busy_placements[i].fpfn = fpfn;\n\t\t\tnvbo->busy_placements[i].lpfn = lpfn;\n\t\t}\n\t}\n}\n\nvoid\nnouveau_bo_placement_set(struct nouveau_bo *nvbo, uint32_t domain,\n\t\t\t uint32_t busy)\n{\n\tstruct ttm_placement *pl = &nvbo->placement;\n\n\tpl->placement = nvbo->placements;\n\tset_placement_list(nvbo->placements, &pl->num_placement, domain);\n\n\tpl->busy_placement = nvbo->busy_placements;\n\tset_placement_list(nvbo->busy_placements, &pl->num_busy_placement,\n\t\t\t   domain | busy);\n\n\tset_placement_range(nvbo, domain);\n}\n\nint\nnouveau_bo_pin(struct nouveau_bo *nvbo, uint32_t domain, bool contig)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tstruct ttm_buffer_object *bo = &nvbo->bo;\n\tbool force = false, evict = false;\n\tint ret;\n\n\tret = ttm_bo_reserve(bo, false, false, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA &&\n\t    domain == NOUVEAU_GEM_DOMAIN_VRAM && contig) {\n\t\tif (!nvbo->contig) {\n\t\t\tnvbo->contig = true;\n\t\t\tforce = true;\n\t\t\tevict = true;\n\t\t}\n\t}\n\n\tif (nvbo->bo.pin_count) {\n\t\tbool error = evict;\n\n\t\tswitch (bo->resource->mem_type) {\n\t\tcase TTM_PL_VRAM:\n\t\t\terror |= !(domain & NOUVEAU_GEM_DOMAIN_VRAM);\n\t\t\tbreak;\n\t\tcase TTM_PL_TT:\n\t\t\terror |= !(domain & NOUVEAU_GEM_DOMAIN_GART);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (error) {\n\t\t\tNV_ERROR(drm, \"bo %p pinned elsewhere: \"\n\t\t\t\t      \"0x%08x vs 0x%08x\\n\", bo,\n\t\t\t\t bo->resource->mem_type, domain);\n\t\t\tret = -EBUSY;\n\t\t}\n\t\tttm_bo_pin(&nvbo->bo);\n\t\tgoto out;\n\t}\n\n\tif (evict) {\n\t\tnouveau_bo_placement_set(nvbo, NOUVEAU_GEM_DOMAIN_GART, 0);\n\t\tret = nouveau_bo_validate(nvbo, false, false);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tnouveau_bo_placement_set(nvbo, domain, 0);\n\tret = nouveau_bo_validate(nvbo, false, false);\n\tif (ret)\n\t\tgoto out;\n\n\tttm_bo_pin(&nvbo->bo);\n\n\tswitch (bo->resource->mem_type) {\n\tcase TTM_PL_VRAM:\n\t\tdrm->gem.vram_available -= bo->base.size;\n\t\tbreak;\n\tcase TTM_PL_TT:\n\t\tdrm->gem.gart_available -= bo->base.size;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\nout:\n\tif (force && ret)\n\t\tnvbo->contig = false;\n\tttm_bo_unreserve(bo);\n\treturn ret;\n}\n\nint\nnouveau_bo_unpin(struct nouveau_bo *nvbo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tstruct ttm_buffer_object *bo = &nvbo->bo;\n\tint ret;\n\n\tret = ttm_bo_reserve(bo, false, false, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tttm_bo_unpin(&nvbo->bo);\n\tif (!nvbo->bo.pin_count) {\n\t\tswitch (bo->resource->mem_type) {\n\t\tcase TTM_PL_VRAM:\n\t\t\tdrm->gem.vram_available += bo->base.size;\n\t\t\tbreak;\n\t\tcase TTM_PL_TT:\n\t\t\tdrm->gem.gart_available += bo->base.size;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tttm_bo_unreserve(bo);\n\treturn 0;\n}\n\nint\nnouveau_bo_map(struct nouveau_bo *nvbo)\n{\n\tint ret;\n\n\tret = ttm_bo_reserve(&nvbo->bo, false, false, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ttm_bo_kmap(&nvbo->bo, 0, PFN_UP(nvbo->bo.base.size), &nvbo->kmap);\n\n\tttm_bo_unreserve(&nvbo->bo);\n\treturn ret;\n}\n\nvoid\nnouveau_bo_unmap(struct nouveau_bo *nvbo)\n{\n\tif (!nvbo)\n\t\treturn;\n\n\tttm_bo_kunmap(&nvbo->kmap);\n}\n\nvoid\nnouveau_bo_sync_for_device(struct nouveau_bo *nvbo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tstruct ttm_tt *ttm_dma = (struct ttm_tt *)nvbo->bo.ttm;\n\tint i, j;\n\n\tif (!ttm_dma || !ttm_dma->dma_address)\n\t\treturn;\n\tif (!ttm_dma->pages) {\n\t\tNV_DEBUG(drm, \"ttm_dma 0x%p: pages NULL\\n\", ttm_dma);\n\t\treturn;\n\t}\n\n\t \n\tif (nvbo->force_coherent)\n\t\treturn;\n\n\ti = 0;\n\twhile (i < ttm_dma->num_pages) {\n\t\tstruct page *p = ttm_dma->pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tfor (j = i + 1; j < ttm_dma->num_pages; ++j) {\n\t\t\tif (++p != ttm_dma->pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\t\tdma_sync_single_for_device(drm->dev->dev,\n\t\t\t\t\t   ttm_dma->dma_address[i],\n\t\t\t\t\t   num_pages * PAGE_SIZE, DMA_TO_DEVICE);\n\t\ti += num_pages;\n\t}\n}\n\nvoid\nnouveau_bo_sync_for_cpu(struct nouveau_bo *nvbo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(nvbo->bo.bdev);\n\tstruct ttm_tt *ttm_dma = (struct ttm_tt *)nvbo->bo.ttm;\n\tint i, j;\n\n\tif (!ttm_dma || !ttm_dma->dma_address)\n\t\treturn;\n\tif (!ttm_dma->pages) {\n\t\tNV_DEBUG(drm, \"ttm_dma 0x%p: pages NULL\\n\", ttm_dma);\n\t\treturn;\n\t}\n\n\t \n\tif (nvbo->force_coherent)\n\t\treturn;\n\n\ti = 0;\n\twhile (i < ttm_dma->num_pages) {\n\t\tstruct page *p = ttm_dma->pages[i];\n\t\tsize_t num_pages = 1;\n\n\t\tfor (j = i + 1; j < ttm_dma->num_pages; ++j) {\n\t\t\tif (++p != ttm_dma->pages[j])\n\t\t\t\tbreak;\n\n\t\t\t++num_pages;\n\t\t}\n\n\t\tdma_sync_single_for_cpu(drm->dev->dev, ttm_dma->dma_address[i],\n\t\t\t\t\tnum_pages * PAGE_SIZE, DMA_FROM_DEVICE);\n\t\ti += num_pages;\n\t}\n}\n\nvoid nouveau_bo_add_io_reserve_lru(struct ttm_buffer_object *bo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\n\tmutex_lock(&drm->ttm.io_reserve_mutex);\n\tlist_move_tail(&nvbo->io_reserve_lru, &drm->ttm.io_reserve_lru);\n\tmutex_unlock(&drm->ttm.io_reserve_mutex);\n}\n\nvoid nouveau_bo_del_io_reserve_lru(struct ttm_buffer_object *bo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\n\tmutex_lock(&drm->ttm.io_reserve_mutex);\n\tlist_del_init(&nvbo->io_reserve_lru);\n\tmutex_unlock(&drm->ttm.io_reserve_mutex);\n}\n\nint\nnouveau_bo_validate(struct nouveau_bo *nvbo, bool interruptible,\n\t\t    bool no_wait_gpu)\n{\n\tstruct ttm_operation_ctx ctx = { interruptible, no_wait_gpu };\n\tint ret;\n\n\tret = ttm_bo_validate(&nvbo->bo, &nvbo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tnouveau_bo_sync_for_device(nvbo);\n\n\treturn 0;\n}\n\nvoid\nnouveau_bo_wr16(struct nouveau_bo *nvbo, unsigned index, u16 val)\n{\n\tbool is_iomem;\n\tu16 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\n\n\tmem += index;\n\n\tif (is_iomem)\n\t\tiowrite16_native(val, (void __force __iomem *)mem);\n\telse\n\t\t*mem = val;\n}\n\nu32\nnouveau_bo_rd32(struct nouveau_bo *nvbo, unsigned index)\n{\n\tbool is_iomem;\n\tu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\n\n\tmem += index;\n\n\tif (is_iomem)\n\t\treturn ioread32_native((void __force __iomem *)mem);\n\telse\n\t\treturn *mem;\n}\n\nvoid\nnouveau_bo_wr32(struct nouveau_bo *nvbo, unsigned index, u32 val)\n{\n\tbool is_iomem;\n\tu32 *mem = ttm_kmap_obj_virtual(&nvbo->kmap, &is_iomem);\n\n\tmem += index;\n\n\tif (is_iomem)\n\t\tiowrite32_native(val, (void __force __iomem *)mem);\n\telse\n\t\t*mem = val;\n}\n\nstatic struct ttm_tt *\nnouveau_ttm_tt_create(struct ttm_buffer_object *bo, uint32_t page_flags)\n{\n#if IS_ENABLED(CONFIG_AGP)\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\n\tif (drm->agp.bridge) {\n\t\treturn ttm_agp_tt_create(bo, drm->agp.bridge, page_flags);\n\t}\n#endif\n\n\treturn nouveau_sgdma_create_ttm(bo, page_flags);\n}\n\nstatic int\nnouveau_ttm_tt_bind(struct ttm_device *bdev, struct ttm_tt *ttm,\n\t\t    struct ttm_resource *reg)\n{\n#if IS_ENABLED(CONFIG_AGP)\n\tstruct nouveau_drm *drm = nouveau_bdev(bdev);\n#endif\n\tif (!reg)\n\t\treturn -EINVAL;\n#if IS_ENABLED(CONFIG_AGP)\n\tif (drm->agp.bridge)\n\t\treturn ttm_agp_bind(ttm, reg);\n#endif\n\treturn nouveau_sgdma_bind(bdev, ttm, reg);\n}\n\nstatic void\nnouveau_ttm_tt_unbind(struct ttm_device *bdev, struct ttm_tt *ttm)\n{\n#if IS_ENABLED(CONFIG_AGP)\n\tstruct nouveau_drm *drm = nouveau_bdev(bdev);\n\n\tif (drm->agp.bridge) {\n\t\tttm_agp_unbind(ttm);\n\t\treturn;\n\t}\n#endif\n\tnouveau_sgdma_unbind(bdev, ttm);\n}\n\nstatic void\nnouveau_bo_evict_flags(struct ttm_buffer_object *bo, struct ttm_placement *pl)\n{\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\n\tswitch (bo->resource->mem_type) {\n\tcase TTM_PL_VRAM:\n\t\tnouveau_bo_placement_set(nvbo, NOUVEAU_GEM_DOMAIN_GART,\n\t\t\t\t\t NOUVEAU_GEM_DOMAIN_CPU);\n\t\tbreak;\n\tdefault:\n\t\tnouveau_bo_placement_set(nvbo, NOUVEAU_GEM_DOMAIN_CPU, 0);\n\t\tbreak;\n\t}\n\n\t*pl = nvbo->placement;\n}\n\nstatic int\nnouveau_bo_move_prep(struct nouveau_drm *drm, struct ttm_buffer_object *bo,\n\t\t     struct ttm_resource *reg)\n{\n\tstruct nouveau_mem *old_mem = nouveau_mem(bo->resource);\n\tstruct nouveau_mem *new_mem = nouveau_mem(reg);\n\tstruct nvif_vmm *vmm = &drm->client.vmm.vmm;\n\tint ret;\n\n\tret = nvif_vmm_get(vmm, LAZY, false, old_mem->mem.page, 0,\n\t\t\t   old_mem->mem.size, &old_mem->vma[0]);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvif_vmm_get(vmm, LAZY, false, new_mem->mem.page, 0,\n\t\t\t   new_mem->mem.size, &old_mem->vma[1]);\n\tif (ret)\n\t\tgoto done;\n\n\tret = nouveau_mem_map(old_mem, vmm, &old_mem->vma[0]);\n\tif (ret)\n\t\tgoto done;\n\n\tret = nouveau_mem_map(new_mem, vmm, &old_mem->vma[1]);\ndone:\n\tif (ret) {\n\t\tnvif_vmm_put(vmm, &old_mem->vma[1]);\n\t\tnvif_vmm_put(vmm, &old_mem->vma[0]);\n\t}\n\treturn 0;\n}\n\nstatic int\nnouveau_bo_move_m2mf(struct ttm_buffer_object *bo, int evict,\n\t\t     struct ttm_operation_ctx *ctx,\n\t\t     struct ttm_resource *new_reg)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_channel *chan = drm->ttm.chan;\n\tstruct nouveau_cli *cli = (void *)chan->user.client;\n\tstruct nouveau_fence *fence;\n\tint ret;\n\n\t \n\tif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA) {\n\t\tret = nouveau_bo_move_prep(drm, bo, new_reg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (drm_drv_uses_atomic_modeset(drm->dev))\n\t\tmutex_lock(&cli->mutex);\n\telse\n\t\tmutex_lock_nested(&cli->mutex, SINGLE_DEPTH_NESTING);\n\n\tret = nouveau_fence_sync(nouveau_bo(bo), chan, true, ctx->interruptible);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = drm->ttm.move(chan, bo, bo->resource, new_reg);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = nouveau_fence_new(&fence, chan);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\t \n\tnouveau_fence_wait(fence, false, false);\n\tret = ttm_bo_move_accel_cleanup(bo, &fence->base, evict, false,\n\t\t\t\t\tnew_reg);\n\tnouveau_fence_unref(&fence);\n\nout_unlock:\n\tmutex_unlock(&cli->mutex);\n\treturn ret;\n}\n\nvoid\nnouveau_bo_move_init(struct nouveau_drm *drm)\n{\n\tstatic const struct _method_table {\n\t\tconst char *name;\n\t\tint engine;\n\t\ts32 oclass;\n\t\tint (*exec)(struct nouveau_channel *,\n\t\t\t    struct ttm_buffer_object *,\n\t\t\t    struct ttm_resource *, struct ttm_resource *);\n\t\tint (*init)(struct nouveau_channel *, u32 handle);\n\t} _methods[] = {\n\t\t{  \"COPY\", 4, 0xc7b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc7b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xc6b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc6b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xc5b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc5b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xc3b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc3b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xc1b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc1b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xc0b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xc0b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xb0b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xb0b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 4, 0xa0b5, nve0_bo_move_copy, nve0_bo_move_init },\n\t\t{  \"GRCE\", 0, 0xa0b5, nve0_bo_move_copy, nvc0_bo_move_init },\n\t\t{ \"COPY1\", 5, 0x90b8, nvc0_bo_move_copy, nvc0_bo_move_init },\n\t\t{ \"COPY0\", 4, 0x90b5, nvc0_bo_move_copy, nvc0_bo_move_init },\n\t\t{  \"COPY\", 0, 0x85b5, nva3_bo_move_copy, nv50_bo_move_init },\n\t\t{ \"CRYPT\", 0, 0x74c1, nv84_bo_move_exec, nv50_bo_move_init },\n\t\t{  \"M2MF\", 0, 0x9039, nvc0_bo_move_m2mf, nvc0_bo_move_init },\n\t\t{  \"M2MF\", 0, 0x5039, nv50_bo_move_m2mf, nv50_bo_move_init },\n\t\t{  \"M2MF\", 0, 0x0039, nv04_bo_move_m2mf, nv04_bo_move_init },\n\t\t{},\n\t};\n\tconst struct _method_table *mthd = _methods;\n\tconst char *name = \"CPU\";\n\tint ret;\n\n\tdo {\n\t\tstruct nouveau_channel *chan;\n\n\t\tif (mthd->engine)\n\t\t\tchan = drm->cechan;\n\t\telse\n\t\t\tchan = drm->channel;\n\t\tif (chan == NULL)\n\t\t\tcontinue;\n\n\t\tret = nvif_object_ctor(&chan->user, \"ttmBoMove\",\n\t\t\t\t       mthd->oclass | (mthd->engine << 16),\n\t\t\t\t       mthd->oclass, NULL, 0,\n\t\t\t\t       &drm->ttm.copy);\n\t\tif (ret == 0) {\n\t\t\tret = mthd->init(chan, drm->ttm.copy.handle);\n\t\t\tif (ret) {\n\t\t\t\tnvif_object_dtor(&drm->ttm.copy);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdrm->ttm.move = mthd->exec;\n\t\t\tdrm->ttm.chan = chan;\n\t\t\tname = mthd->name;\n\t\t\tbreak;\n\t\t}\n\t} while ((++mthd)->exec);\n\n\tNV_INFO(drm, \"MM: using %s for buffer copies\\n\", name);\n}\n\nstatic void nouveau_bo_move_ntfy(struct ttm_buffer_object *bo,\n\t\t\t\t struct ttm_resource *new_reg)\n{\n\tstruct nouveau_mem *mem = new_reg ? nouveau_mem(new_reg) : NULL;\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\tstruct nouveau_vma *vma;\n\tlong ret;\n\n\t \n\tif (bo->destroy != nouveau_bo_del_ttm)\n\t\treturn;\n\n\tnouveau_bo_del_io_reserve_lru(bo);\n\n\tif (mem && new_reg->mem_type != TTM_PL_SYSTEM &&\n\t    mem->mem.page == nvbo->page) {\n\t\tlist_for_each_entry(vma, &nvbo->vma_list, head) {\n\t\t\tnouveau_vma_map(vma, mem);\n\t\t}\n\t\tnouveau_uvmm_bo_map_all(nvbo, mem);\n\t} else {\n\t\tlist_for_each_entry(vma, &nvbo->vma_list, head) {\n\t\t\tret = dma_resv_wait_timeout(bo->base.resv,\n\t\t\t\t\t\t    DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t\t\t    false, 15 * HZ);\n\t\t\tWARN_ON(ret <= 0);\n\t\t\tnouveau_vma_unmap(vma);\n\t\t}\n\t\tnouveau_uvmm_bo_unmap_all(nvbo);\n\t}\n\n\tif (new_reg)\n\t\tnvbo->offset = (new_reg->start << PAGE_SHIFT);\n\n}\n\nstatic int\nnouveau_bo_vm_bind(struct ttm_buffer_object *bo, struct ttm_resource *new_reg,\n\t\t   struct nouveau_drm_tile **new_tile)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct drm_device *dev = drm->dev;\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\tu64 offset = new_reg->start << PAGE_SHIFT;\n\n\t*new_tile = NULL;\n\tif (new_reg->mem_type != TTM_PL_VRAM)\n\t\treturn 0;\n\n\tif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_CELSIUS) {\n\t\t*new_tile = nv10_bo_set_tiling(dev, offset, bo->base.size,\n\t\t\t\t\t       nvbo->mode, nvbo->zeta);\n\t}\n\n\treturn 0;\n}\n\nstatic void\nnouveau_bo_vm_cleanup(struct ttm_buffer_object *bo,\n\t\t      struct nouveau_drm_tile *new_tile,\n\t\t      struct nouveau_drm_tile **old_tile)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct drm_device *dev = drm->dev;\n\tstruct dma_fence *fence;\n\tint ret;\n\n\tret = dma_resv_get_singleton(bo->base.resv, DMA_RESV_USAGE_WRITE,\n\t\t\t\t     &fence);\n\tif (ret)\n\t\tdma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_WRITE,\n\t\t\t\t      false, MAX_SCHEDULE_TIMEOUT);\n\n\tnv10_bo_put_tile_region(dev, *old_tile, fence);\n\t*old_tile = new_tile;\n}\n\nstatic int\nnouveau_bo_move(struct ttm_buffer_object *bo, bool evict,\n\t\tstruct ttm_operation_ctx *ctx,\n\t\tstruct ttm_resource *new_reg,\n\t\tstruct ttm_place *hop)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\tstruct ttm_resource *old_reg = bo->resource;\n\tstruct nouveau_drm_tile *new_tile = NULL;\n\tint ret = 0;\n\n\n\tif (new_reg->mem_type == TTM_PL_TT) {\n\t\tret = nouveau_ttm_tt_bind(bo->bdev, bo->ttm, new_reg);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tnouveau_bo_move_ntfy(bo, new_reg);\n\tret = ttm_bo_wait_ctx(bo, ctx);\n\tif (ret)\n\t\tgoto out_ntfy;\n\n\tif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA) {\n\t\tret = nouveau_bo_vm_bind(bo, new_reg, &new_tile);\n\t\tif (ret)\n\t\t\tgoto out_ntfy;\n\t}\n\n\t \n\tif (!old_reg || (old_reg->mem_type == TTM_PL_SYSTEM &&\n\t\t\t !bo->ttm)) {\n\t\tttm_bo_move_null(bo, new_reg);\n\t\tgoto out;\n\t}\n\n\tif (old_reg->mem_type == TTM_PL_SYSTEM &&\n\t    new_reg->mem_type == TTM_PL_TT) {\n\t\tttm_bo_move_null(bo, new_reg);\n\t\tgoto out;\n\t}\n\n\tif (old_reg->mem_type == TTM_PL_TT &&\n\t    new_reg->mem_type == TTM_PL_SYSTEM) {\n\t\tnouveau_ttm_tt_unbind(bo->bdev, bo->ttm);\n\t\tttm_resource_free(bo, &bo->resource);\n\t\tttm_bo_assign_mem(bo, new_reg);\n\t\tgoto out;\n\t}\n\n\t \n\tif (drm->ttm.move) {\n\t\tif ((old_reg->mem_type == TTM_PL_SYSTEM &&\n\t\t     new_reg->mem_type == TTM_PL_VRAM) ||\n\t\t    (old_reg->mem_type == TTM_PL_VRAM &&\n\t\t     new_reg->mem_type == TTM_PL_SYSTEM)) {\n\t\t\thop->fpfn = 0;\n\t\t\thop->lpfn = 0;\n\t\t\thop->mem_type = TTM_PL_TT;\n\t\t\thop->flags = 0;\n\t\t\treturn -EMULTIHOP;\n\t\t}\n\t\tret = nouveau_bo_move_m2mf(bo, evict, ctx,\n\t\t\t\t\t   new_reg);\n\t} else\n\t\tret = -ENODEV;\n\n\tif (ret) {\n\t\t \n\t\tret = ttm_bo_move_memcpy(bo, ctx, new_reg);\n\t}\n\nout:\n\tif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA) {\n\t\tif (ret)\n\t\t\tnouveau_bo_vm_cleanup(bo, NULL, &new_tile);\n\t\telse\n\t\t\tnouveau_bo_vm_cleanup(bo, new_tile, &nvbo->tile);\n\t}\nout_ntfy:\n\tif (ret) {\n\t\tnouveau_bo_move_ntfy(bo, bo->resource);\n\t}\n\treturn ret;\n}\n\nstatic void\nnouveau_ttm_io_mem_free_locked(struct nouveau_drm *drm,\n\t\t\t       struct ttm_resource *reg)\n{\n\tstruct nouveau_mem *mem = nouveau_mem(reg);\n\n\tif (drm->client.mem->oclass >= NVIF_CLASS_MEM_NV50) {\n\t\tswitch (reg->mem_type) {\n\t\tcase TTM_PL_TT:\n\t\t\tif (mem->kind)\n\t\t\t\tnvif_object_unmap_handle(&mem->mem.object);\n\t\t\tbreak;\n\t\tcase TTM_PL_VRAM:\n\t\t\tnvif_object_unmap_handle(&mem->mem.object);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int\nnouveau_ttm_io_mem_reserve(struct ttm_device *bdev, struct ttm_resource *reg)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bdev);\n\tstruct nvkm_device *device = nvxx_device(&drm->client.device);\n\tstruct nouveau_mem *mem = nouveau_mem(reg);\n\tstruct nvif_mmu *mmu = &drm->client.mmu;\n\tint ret;\n\n\tmutex_lock(&drm->ttm.io_reserve_mutex);\nretry:\n\tswitch (reg->mem_type) {\n\tcase TTM_PL_SYSTEM:\n\t\t \n\t\tret = 0;\n\t\tgoto out;\n\tcase TTM_PL_TT:\n#if IS_ENABLED(CONFIG_AGP)\n\t\tif (drm->agp.bridge) {\n\t\t\treg->bus.offset = (reg->start << PAGE_SHIFT) +\n\t\t\t\tdrm->agp.base;\n\t\t\treg->bus.is_iomem = !drm->agp.cma;\n\t\t\treg->bus.caching = ttm_write_combined;\n\t\t}\n#endif\n\t\tif (drm->client.mem->oclass < NVIF_CLASS_MEM_NV50 ||\n\t\t    !mem->kind) {\n\t\t\t \n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\t \n\tcase TTM_PL_VRAM:\n\t\treg->bus.offset = (reg->start << PAGE_SHIFT) +\n\t\t\tdevice->func->resource_addr(device, 1);\n\t\treg->bus.is_iomem = true;\n\n\t\t \n\t\tif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA &&\n\t\t    mmu->type[drm->ttm.type_vram].type & NVIF_MEM_UNCACHED)\n\t\t\treg->bus.caching = ttm_uncached;\n\t\telse\n\t\t\treg->bus.caching = ttm_write_combined;\n\n\t\tif (drm->client.mem->oclass >= NVIF_CLASS_MEM_NV50) {\n\t\t\tunion {\n\t\t\t\tstruct nv50_mem_map_v0 nv50;\n\t\t\t\tstruct gf100_mem_map_v0 gf100;\n\t\t\t} args;\n\t\t\tu64 handle, length;\n\t\t\tu32 argc = 0;\n\n\t\t\tswitch (mem->mem.object.oclass) {\n\t\t\tcase NVIF_CLASS_MEM_NV50:\n\t\t\t\targs.nv50.version = 0;\n\t\t\t\targs.nv50.ro = 0;\n\t\t\t\targs.nv50.kind = mem->kind;\n\t\t\t\targs.nv50.comp = mem->comp;\n\t\t\t\targc = sizeof(args.nv50);\n\t\t\t\tbreak;\n\t\t\tcase NVIF_CLASS_MEM_GF100:\n\t\t\t\targs.gf100.version = 0;\n\t\t\t\targs.gf100.ro = 0;\n\t\t\t\targs.gf100.kind = mem->kind;\n\t\t\t\targc = sizeof(args.gf100);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tWARN_ON(1);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tret = nvif_object_map_handle(&mem->mem.object,\n\t\t\t\t\t\t     &args, argc,\n\t\t\t\t\t\t     &handle, &length);\n\t\t\tif (ret != 1) {\n\t\t\t\tif (WARN_ON(ret == 0))\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\treg->bus.offset = handle;\n\t\t}\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\nout:\n\tif (ret == -ENOSPC) {\n\t\tstruct nouveau_bo *nvbo;\n\n\t\tnvbo = list_first_entry_or_null(&drm->ttm.io_reserve_lru,\n\t\t\t\t\t\ttypeof(*nvbo),\n\t\t\t\t\t\tio_reserve_lru);\n\t\tif (nvbo) {\n\t\t\tlist_del_init(&nvbo->io_reserve_lru);\n\t\t\tdrm_vma_node_unmap(&nvbo->bo.base.vma_node,\n\t\t\t\t\t   bdev->dev_mapping);\n\t\t\tnouveau_ttm_io_mem_free_locked(drm, nvbo->bo.resource);\n\t\t\tgoto retry;\n\t\t}\n\n\t}\n\tmutex_unlock(&drm->ttm.io_reserve_mutex);\n\treturn ret;\n}\n\nstatic void\nnouveau_ttm_io_mem_free(struct ttm_device *bdev, struct ttm_resource *reg)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bdev);\n\n\tmutex_lock(&drm->ttm.io_reserve_mutex);\n\tnouveau_ttm_io_mem_free_locked(drm, reg);\n\tmutex_unlock(&drm->ttm.io_reserve_mutex);\n}\n\nvm_fault_t nouveau_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\n{\n\tstruct nouveau_drm *drm = nouveau_bdev(bo->bdev);\n\tstruct nouveau_bo *nvbo = nouveau_bo(bo);\n\tstruct nvkm_device *device = nvxx_device(&drm->client.device);\n\tu32 mappable = device->func->resource_size(device, 1) >> PAGE_SHIFT;\n\tint i, ret;\n\n\t \n\tif (bo->resource->mem_type != TTM_PL_VRAM) {\n\t\tif (drm->client.device.info.family < NV_DEVICE_INFO_V0_TESLA ||\n\t\t    !nvbo->kind)\n\t\t\treturn 0;\n\n\t\tif (bo->resource->mem_type != TTM_PL_SYSTEM)\n\t\t\treturn 0;\n\n\t\tnouveau_bo_placement_set(nvbo, NOUVEAU_GEM_DOMAIN_GART, 0);\n\n\t} else {\n\t\t \n\t\tif (drm->client.device.info.family >= NV_DEVICE_INFO_V0_TESLA ||\n\t\t    bo->resource->start + PFN_UP(bo->resource->size) < mappable)\n\t\t\treturn 0;\n\n\t\tfor (i = 0; i < nvbo->placement.num_placement; ++i) {\n\t\t\tnvbo->placements[i].fpfn = 0;\n\t\t\tnvbo->placements[i].lpfn = mappable;\n\t\t}\n\n\t\tfor (i = 0; i < nvbo->placement.num_busy_placement; ++i) {\n\t\t\tnvbo->busy_placements[i].fpfn = 0;\n\t\t\tnvbo->busy_placements[i].lpfn = mappable;\n\t\t}\n\n\t\tnouveau_bo_placement_set(nvbo, NOUVEAU_GEM_DOMAIN_VRAM, 0);\n\t}\n\n\tret = nouveau_bo_validate(nvbo, false, false);\n\tif (unlikely(ret == -EBUSY || ret == -ERESTARTSYS))\n\t\treturn VM_FAULT_NOPAGE;\n\telse if (unlikely(ret))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tttm_bo_move_to_lru_tail_unlocked(bo);\n\treturn 0;\n}\n\nstatic int\nnouveau_ttm_tt_populate(struct ttm_device *bdev,\n\t\t\tstruct ttm_tt *ttm, struct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_tt *ttm_dma = (void *)ttm;\n\tstruct nouveau_drm *drm;\n\tbool slave = !!(ttm->page_flags & TTM_TT_FLAG_EXTERNAL);\n\n\tif (ttm_tt_is_populated(ttm))\n\t\treturn 0;\n\n\tif (slave && ttm->sg) {\n\t\tdrm_prime_sg_to_dma_addr_array(ttm->sg, ttm_dma->dma_address,\n\t\t\t\t\t       ttm->num_pages);\n\t\treturn 0;\n\t}\n\n\tdrm = nouveau_bdev(bdev);\n\n\treturn ttm_pool_alloc(&drm->ttm.bdev.pool, ttm, ctx);\n}\n\nstatic void\nnouveau_ttm_tt_unpopulate(struct ttm_device *bdev,\n\t\t\t  struct ttm_tt *ttm)\n{\n\tstruct nouveau_drm *drm;\n\tbool slave = !!(ttm->page_flags & TTM_TT_FLAG_EXTERNAL);\n\n\tif (slave)\n\t\treturn;\n\n\tnouveau_ttm_tt_unbind(bdev, ttm);\n\n\tdrm = nouveau_bdev(bdev);\n\n\treturn ttm_pool_free(&drm->ttm.bdev.pool, ttm);\n}\n\nstatic void\nnouveau_ttm_tt_destroy(struct ttm_device *bdev,\n\t\t       struct ttm_tt *ttm)\n{\n#if IS_ENABLED(CONFIG_AGP)\n\tstruct nouveau_drm *drm = nouveau_bdev(bdev);\n\tif (drm->agp.bridge) {\n\t\tttm_agp_destroy(ttm);\n\t\treturn;\n\t}\n#endif\n\tnouveau_sgdma_destroy(bdev, ttm);\n}\n\nvoid\nnouveau_bo_fence(struct nouveau_bo *nvbo, struct nouveau_fence *fence, bool exclusive)\n{\n\tstruct dma_resv *resv = nvbo->bo.base.resv;\n\n\tif (!fence)\n\t\treturn;\n\n\tdma_resv_add_fence(resv, &fence->base, exclusive ?\n\t\t\t   DMA_RESV_USAGE_WRITE : DMA_RESV_USAGE_READ);\n}\n\nstatic void\nnouveau_bo_delete_mem_notify(struct ttm_buffer_object *bo)\n{\n\tnouveau_bo_move_ntfy(bo, NULL);\n}\n\nstruct ttm_device_funcs nouveau_bo_driver = {\n\t.ttm_tt_create = &nouveau_ttm_tt_create,\n\t.ttm_tt_populate = &nouveau_ttm_tt_populate,\n\t.ttm_tt_unpopulate = &nouveau_ttm_tt_unpopulate,\n\t.ttm_tt_destroy = &nouveau_ttm_tt_destroy,\n\t.eviction_valuable = ttm_bo_eviction_valuable,\n\t.evict_flags = nouveau_bo_evict_flags,\n\t.delete_mem_notify = nouveau_bo_delete_mem_notify,\n\t.move = nouveau_bo_move,\n\t.io_mem_reserve = &nouveau_ttm_io_mem_reserve,\n\t.io_mem_free = &nouveau_ttm_io_mem_free,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}