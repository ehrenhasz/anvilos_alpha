{
  "module_name": "nouveau_svm.c",
  "hash_id": "4d5bca1efecf9e2bec13d74e8f70c4e03867ea0075e87695aa276f0291e3196a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nouveau_svm.c",
  "human_readable_source": " \n#include \"nouveau_svm.h\"\n#include \"nouveau_drv.h\"\n#include \"nouveau_chan.h\"\n#include \"nouveau_dmem.h\"\n\n#include <nvif/event.h>\n#include <nvif/object.h>\n#include <nvif/vmm.h>\n\n#include <nvif/class.h>\n#include <nvif/clb069.h>\n#include <nvif/ifc00d.h>\n\n#include <linux/sched/mm.h>\n#include <linux/sort.h>\n#include <linux/hmm.h>\n#include <linux/memremap.h>\n#include <linux/rmap.h>\n\nstruct nouveau_svm {\n\tstruct nouveau_drm *drm;\n\tstruct mutex mutex;\n\tstruct list_head inst;\n\n\tstruct nouveau_svm_fault_buffer {\n\t\tint id;\n\t\tstruct nvif_object object;\n\t\tu32 entries;\n\t\tu32 getaddr;\n\t\tu32 putaddr;\n\t\tu32 get;\n\t\tu32 put;\n\t\tstruct nvif_event notify;\n\t\tstruct work_struct work;\n\n\t\tstruct nouveau_svm_fault {\n\t\t\tu64 inst;\n\t\t\tu64 addr;\n\t\t\tu64 time;\n\t\t\tu32 engine;\n\t\t\tu8  gpc;\n\t\t\tu8  hub;\n\t\t\tu8  access;\n\t\t\tu8  client;\n\t\t\tu8  fault;\n\t\t\tstruct nouveau_svmm *svmm;\n\t\t} **fault;\n\t\tint fault_nr;\n\t} buffer[1];\n};\n\n#define FAULT_ACCESS_READ 0\n#define FAULT_ACCESS_WRITE 1\n#define FAULT_ACCESS_ATOMIC 2\n#define FAULT_ACCESS_PREFETCH 3\n\n#define SVM_DBG(s,f,a...) NV_DEBUG((s)->drm, \"svm: \"f\"\\n\", ##a)\n#define SVM_ERR(s,f,a...) NV_WARN((s)->drm, \"svm: \"f\"\\n\", ##a)\n\nstruct nouveau_pfnmap_args {\n\tstruct nvif_ioctl_v0 i;\n\tstruct nvif_ioctl_mthd_v0 m;\n\tstruct nvif_vmm_pfnmap_v0 p;\n};\n\nstruct nouveau_ivmm {\n\tstruct nouveau_svmm *svmm;\n\tu64 inst;\n\tstruct list_head head;\n};\n\nstatic struct nouveau_ivmm *\nnouveau_ivmm_find(struct nouveau_svm *svm, u64 inst)\n{\n\tstruct nouveau_ivmm *ivmm;\n\tlist_for_each_entry(ivmm, &svm->inst, head) {\n\t\tif (ivmm->inst == inst)\n\t\t\treturn ivmm;\n\t}\n\treturn NULL;\n}\n\n#define SVMM_DBG(s,f,a...)                                                     \\\n\tNV_DEBUG((s)->vmm->cli->drm, \"svm-%p: \"f\"\\n\", (s), ##a)\n#define SVMM_ERR(s,f,a...)                                                     \\\n\tNV_WARN((s)->vmm->cli->drm, \"svm-%p: \"f\"\\n\", (s), ##a)\n\nint\nnouveau_svmm_bind(struct drm_device *dev, void *data,\n\t\t  struct drm_file *file_priv)\n{\n\tstruct nouveau_cli *cli = nouveau_cli(file_priv);\n\tstruct drm_nouveau_svm_bind *args = data;\n\tunsigned target, cmd, priority;\n\tunsigned long addr, end;\n\tstruct mm_struct *mm;\n\n\targs->va_start &= PAGE_MASK;\n\targs->va_end = ALIGN(args->va_end, PAGE_SIZE);\n\n\t \n\tif (args->reserved0 || args->reserved1)\n\t\treturn -EINVAL;\n\tif (args->header & (~NOUVEAU_SVM_BIND_VALID_MASK))\n\t\treturn -EINVAL;\n\tif (args->va_start >= args->va_end)\n\t\treturn -EINVAL;\n\n\tcmd = args->header >> NOUVEAU_SVM_BIND_COMMAND_SHIFT;\n\tcmd &= NOUVEAU_SVM_BIND_COMMAND_MASK;\n\tswitch (cmd) {\n\tcase NOUVEAU_SVM_BIND_COMMAND__MIGRATE:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tpriority = args->header >> NOUVEAU_SVM_BIND_PRIORITY_SHIFT;\n\tpriority &= NOUVEAU_SVM_BIND_PRIORITY_MASK;\n\n\t \n\ttarget = args->header >> NOUVEAU_SVM_BIND_TARGET_SHIFT;\n\ttarget &= NOUVEAU_SVM_BIND_TARGET_MASK;\n\tswitch (target) {\n\tcase NOUVEAU_SVM_BIND_TARGET__GPU_VRAM:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (args->stride)\n\t\treturn -EINVAL;\n\n\t \n\n\tmm = get_task_mm(current);\n\tif (!mm) {\n\t\treturn -EINVAL;\n\t}\n\tmmap_read_lock(mm);\n\n\tif (!cli->svm.svmm) {\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (addr = args->va_start, end = args->va_end; addr < end;) {\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned long next;\n\n\t\tvma = find_vma_intersection(mm, addr, end);\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\taddr = max(addr, vma->vm_start);\n\t\tnext = min(vma->vm_end, end);\n\t\t \n\t\tnouveau_dmem_migrate_vma(cli->drm, cli->svm.svmm, vma, addr,\n\t\t\t\t\t next);\n\t\taddr = next;\n\t}\n\n\t \n\targs->result = 0;\n\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\treturn 0;\n}\n\n \nvoid\nnouveau_svmm_part(struct nouveau_svmm *svmm, u64 inst)\n{\n\tstruct nouveau_ivmm *ivmm;\n\tif (svmm) {\n\t\tmutex_lock(&svmm->vmm->cli->drm->svm->mutex);\n\t\tivmm = nouveau_ivmm_find(svmm->vmm->cli->drm->svm, inst);\n\t\tif (ivmm) {\n\t\t\tlist_del(&ivmm->head);\n\t\t\tkfree(ivmm);\n\t\t}\n\t\tmutex_unlock(&svmm->vmm->cli->drm->svm->mutex);\n\t}\n}\n\n \nint\nnouveau_svmm_join(struct nouveau_svmm *svmm, u64 inst)\n{\n\tstruct nouveau_ivmm *ivmm;\n\tif (svmm) {\n\t\tif (!(ivmm = kmalloc(sizeof(*ivmm), GFP_KERNEL)))\n\t\t\treturn -ENOMEM;\n\t\tivmm->svmm = svmm;\n\t\tivmm->inst = inst;\n\n\t\tmutex_lock(&svmm->vmm->cli->drm->svm->mutex);\n\t\tlist_add(&ivmm->head, &svmm->vmm->cli->drm->svm->inst);\n\t\tmutex_unlock(&svmm->vmm->cli->drm->svm->mutex);\n\t}\n\treturn 0;\n}\n\n \nvoid\nnouveau_svmm_invalidate(struct nouveau_svmm *svmm, u64 start, u64 limit)\n{\n\tif (limit > start) {\n\t\tnvif_object_mthd(&svmm->vmm->vmm.object, NVIF_VMM_V0_PFNCLR,\n\t\t\t\t &(struct nvif_vmm_pfnclr_v0) {\n\t\t\t\t\t.addr = start,\n\t\t\t\t\t.size = limit - start,\n\t\t\t\t }, sizeof(struct nvif_vmm_pfnclr_v0));\n\t}\n}\n\nstatic int\nnouveau_svmm_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\t\t    const struct mmu_notifier_range *update)\n{\n\tstruct nouveau_svmm *svmm =\n\t\tcontainer_of(mn, struct nouveau_svmm, notifier);\n\tunsigned long start = update->start;\n\tunsigned long limit = update->end;\n\n\tif (!mmu_notifier_range_blockable(update))\n\t\treturn -EAGAIN;\n\n\tSVMM_DBG(svmm, \"invalidate %016lx-%016lx\", start, limit);\n\n\tmutex_lock(&svmm->mutex);\n\tif (unlikely(!svmm->vmm))\n\t\tgoto out;\n\n\t \n\tif (update->event == MMU_NOTIFY_MIGRATE &&\n\t    update->owner == svmm->vmm->cli->drm->dev)\n\t\tgoto out;\n\n\tif (limit > svmm->unmanaged.start && start < svmm->unmanaged.limit) {\n\t\tif (start < svmm->unmanaged.start) {\n\t\t\tnouveau_svmm_invalidate(svmm, start,\n\t\t\t\t\t\tsvmm->unmanaged.limit);\n\t\t}\n\t\tstart = svmm->unmanaged.limit;\n\t}\n\n\tnouveau_svmm_invalidate(svmm, start, limit);\n\nout:\n\tmutex_unlock(&svmm->mutex);\n\treturn 0;\n}\n\nstatic void nouveau_svmm_free_notifier(struct mmu_notifier *mn)\n{\n\tkfree(container_of(mn, struct nouveau_svmm, notifier));\n}\n\nstatic const struct mmu_notifier_ops nouveau_mn_ops = {\n\t.invalidate_range_start = nouveau_svmm_invalidate_range_start,\n\t.free_notifier = nouveau_svmm_free_notifier,\n};\n\nvoid\nnouveau_svmm_fini(struct nouveau_svmm **psvmm)\n{\n\tstruct nouveau_svmm *svmm = *psvmm;\n\tif (svmm) {\n\t\tmutex_lock(&svmm->mutex);\n\t\tsvmm->vmm = NULL;\n\t\tmutex_unlock(&svmm->mutex);\n\t\tmmu_notifier_put(&svmm->notifier);\n\t\t*psvmm = NULL;\n\t}\n}\n\nint\nnouveau_svmm_init(struct drm_device *dev, void *data,\n\t\t  struct drm_file *file_priv)\n{\n\tstruct nouveau_cli *cli = nouveau_cli(file_priv);\n\tstruct nouveau_svmm *svmm;\n\tstruct drm_nouveau_svm_init *args = data;\n\tint ret;\n\n\t \n\tif (!cli->drm->svm)\n\t\treturn -ENOSYS;\n\n\t \n\tif (!(svmm = kzalloc(sizeof(*svmm), GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\tsvmm->vmm = &cli->svm;\n\tsvmm->unmanaged.start = args->unmanaged_addr;\n\tsvmm->unmanaged.limit = args->unmanaged_addr + args->unmanaged_size;\n\tmutex_init(&svmm->mutex);\n\n\t \n\tmutex_lock(&cli->mutex);\n\tif (cli->svm.cli) {\n\t\tret = -EBUSY;\n\t\tgoto out_free;\n\t}\n\n\t \n\tret = nvif_vmm_ctor(&cli->mmu, \"svmVmm\",\n\t\t\t    cli->vmm.vmm.object.oclass, MANAGED,\n\t\t\t    args->unmanaged_addr, args->unmanaged_size,\n\t\t\t    &(struct gp100_vmm_v0) {\n\t\t\t\t.fault_replay = true,\n\t\t\t    }, sizeof(struct gp100_vmm_v0), &cli->svm.vmm);\n\tif (ret)\n\t\tgoto out_free;\n\n\tmmap_write_lock(current->mm);\n\tsvmm->notifier.ops = &nouveau_mn_ops;\n\tret = __mmu_notifier_register(&svmm->notifier, current->mm);\n\tif (ret)\n\t\tgoto out_mm_unlock;\n\t \n\n\tcli->svm.svmm = svmm;\n\tcli->svm.cli = cli;\n\tmmap_write_unlock(current->mm);\n\tmutex_unlock(&cli->mutex);\n\treturn 0;\n\nout_mm_unlock:\n\tmmap_write_unlock(current->mm);\nout_free:\n\tmutex_unlock(&cli->mutex);\n\tkfree(svmm);\n\treturn ret;\n}\n\n \nstatic void\nnouveau_svm_fault_replay(struct nouveau_svm *svm)\n{\n\tSVM_DBG(svm, \"replay\");\n\tWARN_ON(nvif_object_mthd(&svm->drm->client.vmm.vmm.object,\n\t\t\t\t GP100_VMM_VN_FAULT_REPLAY,\n\t\t\t\t &(struct gp100_vmm_fault_replay_vn) {},\n\t\t\t\t sizeof(struct gp100_vmm_fault_replay_vn)));\n}\n\n \nstatic void\nnouveau_svm_fault_cancel(struct nouveau_svm *svm,\n\t\t\t u64 inst, u8 hub, u8 gpc, u8 client)\n{\n\tSVM_DBG(svm, \"cancel %016llx %d %02x %02x\", inst, hub, gpc, client);\n\tWARN_ON(nvif_object_mthd(&svm->drm->client.vmm.vmm.object,\n\t\t\t\t GP100_VMM_VN_FAULT_CANCEL,\n\t\t\t\t &(struct gp100_vmm_fault_cancel_v0) {\n\t\t\t\t\t.hub = hub,\n\t\t\t\t\t.gpc = gpc,\n\t\t\t\t\t.client = client,\n\t\t\t\t\t.inst = inst,\n\t\t\t\t }, sizeof(struct gp100_vmm_fault_cancel_v0)));\n}\n\nstatic void\nnouveau_svm_fault_cancel_fault(struct nouveau_svm *svm,\n\t\t\t       struct nouveau_svm_fault *fault)\n{\n\tnouveau_svm_fault_cancel(svm, fault->inst,\n\t\t\t\t      fault->hub,\n\t\t\t\t      fault->gpc,\n\t\t\t\t      fault->client);\n}\n\nstatic int\nnouveau_svm_fault_priority(u8 fault)\n{\n\tswitch (fault) {\n\tcase FAULT_ACCESS_PREFETCH:\n\t\treturn 0;\n\tcase FAULT_ACCESS_READ:\n\t\treturn 1;\n\tcase FAULT_ACCESS_WRITE:\n\t\treturn 2;\n\tcase FAULT_ACCESS_ATOMIC:\n\t\treturn 3;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -1;\n\t}\n}\n\nstatic int\nnouveau_svm_fault_cmp(const void *a, const void *b)\n{\n\tconst struct nouveau_svm_fault *fa = *(struct nouveau_svm_fault **)a;\n\tconst struct nouveau_svm_fault *fb = *(struct nouveau_svm_fault **)b;\n\tint ret;\n\tif ((ret = (s64)fa->inst - fb->inst))\n\t\treturn ret;\n\tif ((ret = (s64)fa->addr - fb->addr))\n\t\treturn ret;\n\treturn nouveau_svm_fault_priority(fa->access) -\n\t\tnouveau_svm_fault_priority(fb->access);\n}\n\nstatic void\nnouveau_svm_fault_cache(struct nouveau_svm *svm,\n\t\t\tstruct nouveau_svm_fault_buffer *buffer, u32 offset)\n{\n\tstruct nvif_object *memory = &buffer->object;\n\tconst u32 instlo = nvif_rd32(memory, offset + 0x00);\n\tconst u32 insthi = nvif_rd32(memory, offset + 0x04);\n\tconst u32 addrlo = nvif_rd32(memory, offset + 0x08);\n\tconst u32 addrhi = nvif_rd32(memory, offset + 0x0c);\n\tconst u32 timelo = nvif_rd32(memory, offset + 0x10);\n\tconst u32 timehi = nvif_rd32(memory, offset + 0x14);\n\tconst u32 engine = nvif_rd32(memory, offset + 0x18);\n\tconst u32   info = nvif_rd32(memory, offset + 0x1c);\n\tconst u64   inst = (u64)insthi << 32 | instlo;\n\tconst u8     gpc = (info & 0x1f000000) >> 24;\n\tconst u8     hub = (info & 0x00100000) >> 20;\n\tconst u8  client = (info & 0x00007f00) >> 8;\n\tstruct nouveau_svm_fault *fault;\n\n\t \n\tif (WARN_ON(!(info & 0x80000000)))\n\t\treturn;\n\n\tnvif_mask(memory, offset + 0x1c, 0x80000000, 0x00000000);\n\n\tif (!buffer->fault[buffer->fault_nr]) {\n\t\tfault = kmalloc(sizeof(*fault), GFP_KERNEL);\n\t\tif (WARN_ON(!fault)) {\n\t\t\tnouveau_svm_fault_cancel(svm, inst, hub, gpc, client);\n\t\t\treturn;\n\t\t}\n\t\tbuffer->fault[buffer->fault_nr] = fault;\n\t}\n\n\tfault = buffer->fault[buffer->fault_nr++];\n\tfault->inst   = inst;\n\tfault->addr   = (u64)addrhi << 32 | addrlo;\n\tfault->time   = (u64)timehi << 32 | timelo;\n\tfault->engine = engine;\n\tfault->gpc    = gpc;\n\tfault->hub    = hub;\n\tfault->access = (info & 0x000f0000) >> 16;\n\tfault->client = client;\n\tfault->fault  = (info & 0x0000001f);\n\n\tSVM_DBG(svm, \"fault %016llx %016llx %02x\",\n\t\tfault->inst, fault->addr, fault->access);\n}\n\nstruct svm_notifier {\n\tstruct mmu_interval_notifier notifier;\n\tstruct nouveau_svmm *svmm;\n};\n\nstatic bool nouveau_svm_range_invalidate(struct mmu_interval_notifier *mni,\n\t\t\t\t\t const struct mmu_notifier_range *range,\n\t\t\t\t\t unsigned long cur_seq)\n{\n\tstruct svm_notifier *sn =\n\t\tcontainer_of(mni, struct svm_notifier, notifier);\n\n\tif (range->event == MMU_NOTIFY_EXCLUSIVE &&\n\t    range->owner == sn->svmm->vmm->cli->drm->dev)\n\t\treturn true;\n\n\t \n\tif (mmu_notifier_range_blockable(range))\n\t\tmutex_lock(&sn->svmm->mutex);\n\telse if (!mutex_trylock(&sn->svmm->mutex))\n\t\treturn false;\n\tmmu_interval_set_seq(mni, cur_seq);\n\tmutex_unlock(&sn->svmm->mutex);\n\treturn true;\n}\n\nstatic const struct mmu_interval_notifier_ops nouveau_svm_mni_ops = {\n\t.invalidate = nouveau_svm_range_invalidate,\n};\n\nstatic void nouveau_hmm_convert_pfn(struct nouveau_drm *drm,\n\t\t\t\t    struct hmm_range *range,\n\t\t\t\t    struct nouveau_pfnmap_args *args)\n{\n\tstruct page *page;\n\n\t \n\tif (!(range->hmm_pfns[0] & HMM_PFN_VALID)) {\n\t\targs->p.phys[0] = 0;\n\t\treturn;\n\t}\n\n\tpage = hmm_pfn_to_page(range->hmm_pfns[0]);\n\t \n\tif (hmm_pfn_to_map_order(range->hmm_pfns[0])) {\n\t\tunsigned long addr = args->p.addr;\n\n\t\targs->p.page = hmm_pfn_to_map_order(range->hmm_pfns[0]) +\n\t\t\t\tPAGE_SHIFT;\n\t\targs->p.size = 1UL << args->p.page;\n\t\targs->p.addr &= ~(args->p.size - 1);\n\t\tpage -= (addr - args->p.addr) >> PAGE_SHIFT;\n\t}\n\tif (is_device_private_page(page))\n\t\targs->p.phys[0] = nouveau_dmem_page_addr(page) |\n\t\t\t\tNVIF_VMM_PFNMAP_V0_V |\n\t\t\t\tNVIF_VMM_PFNMAP_V0_VRAM;\n\telse\n\t\targs->p.phys[0] = page_to_phys(page) |\n\t\t\t\tNVIF_VMM_PFNMAP_V0_V |\n\t\t\t\tNVIF_VMM_PFNMAP_V0_HOST;\n\tif (range->hmm_pfns[0] & HMM_PFN_WRITE)\n\t\targs->p.phys[0] |= NVIF_VMM_PFNMAP_V0_W;\n}\n\nstatic int nouveau_atomic_range_fault(struct nouveau_svmm *svmm,\n\t\t\t       struct nouveau_drm *drm,\n\t\t\t       struct nouveau_pfnmap_args *args, u32 size,\n\t\t\t       struct svm_notifier *notifier)\n{\n\tunsigned long timeout =\n\t\tjiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);\n\tstruct mm_struct *mm = svmm->notifier.mm;\n\tstruct page *page;\n\tunsigned long start = args->p.addr;\n\tunsigned long notifier_seq;\n\tint ret = 0;\n\n\tret = mmu_interval_notifier_insert(&notifier->notifier, mm,\n\t\t\t\t\targs->p.addr, args->p.size,\n\t\t\t\t\t&nouveau_svm_mni_ops);\n\tif (ret)\n\t\treturn ret;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\tnotifier_seq = mmu_interval_read_begin(&notifier->notifier);\n\t\tmmap_read_lock(mm);\n\t\tret = make_device_exclusive_range(mm, start, start + PAGE_SIZE,\n\t\t\t\t\t    &page, drm->dev);\n\t\tmmap_read_unlock(mm);\n\t\tif (ret <= 0 || !page) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmutex_lock(&svmm->mutex);\n\t\tif (!mmu_interval_read_retry(&notifier->notifier,\n\t\t\t\t\t     notifier_seq))\n\t\t\tbreak;\n\t\tmutex_unlock(&svmm->mutex);\n\t}\n\n\t \n\targs->p.page = 12;\n\targs->p.size = PAGE_SIZE;\n\targs->p.addr = start;\n\targs->p.phys[0] = page_to_phys(page) |\n\t\tNVIF_VMM_PFNMAP_V0_V |\n\t\tNVIF_VMM_PFNMAP_V0_W |\n\t\tNVIF_VMM_PFNMAP_V0_A |\n\t\tNVIF_VMM_PFNMAP_V0_HOST;\n\n\tret = nvif_object_ioctl(&svmm->vmm->vmm.object, args, size, NULL);\n\tmutex_unlock(&svmm->mutex);\n\n\tunlock_page(page);\n\tput_page(page);\n\nout:\n\tmmu_interval_notifier_remove(&notifier->notifier);\n\treturn ret;\n}\n\nstatic int nouveau_range_fault(struct nouveau_svmm *svmm,\n\t\t\t       struct nouveau_drm *drm,\n\t\t\t       struct nouveau_pfnmap_args *args, u32 size,\n\t\t\t       unsigned long hmm_flags,\n\t\t\t       struct svm_notifier *notifier)\n{\n\tunsigned long timeout =\n\t\tjiffies + msecs_to_jiffies(HMM_RANGE_DEFAULT_TIMEOUT);\n\t \n\tunsigned long hmm_pfns[1];\n\tstruct hmm_range range = {\n\t\t.notifier = &notifier->notifier,\n\t\t.default_flags = hmm_flags,\n\t\t.hmm_pfns = hmm_pfns,\n\t\t.dev_private_owner = drm->dev,\n\t};\n\tstruct mm_struct *mm = svmm->notifier.mm;\n\tint ret;\n\n\tret = mmu_interval_notifier_insert(&notifier->notifier, mm,\n\t\t\t\t\targs->p.addr, args->p.size,\n\t\t\t\t\t&nouveau_svm_mni_ops);\n\tif (ret)\n\t\treturn ret;\n\n\trange.start = notifier->notifier.interval_tree.start;\n\trange.end = notifier->notifier.interval_tree.last + 1;\n\n\twhile (true) {\n\t\tif (time_after(jiffies, timeout)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\n\t\trange.notifier_seq = mmu_interval_read_begin(range.notifier);\n\t\tmmap_read_lock(mm);\n\t\tret = hmm_range_fault(&range);\n\t\tmmap_read_unlock(mm);\n\t\tif (ret) {\n\t\t\tif (ret == -EBUSY)\n\t\t\t\tcontinue;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmutex_lock(&svmm->mutex);\n\t\tif (mmu_interval_read_retry(range.notifier,\n\t\t\t\t\t    range.notifier_seq)) {\n\t\t\tmutex_unlock(&svmm->mutex);\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\tnouveau_hmm_convert_pfn(drm, &range, args);\n\n\tret = nvif_object_ioctl(&svmm->vmm->vmm.object, args, size, NULL);\n\tmutex_unlock(&svmm->mutex);\n\nout:\n\tmmu_interval_notifier_remove(&notifier->notifier);\n\n\treturn ret;\n}\n\nstatic void\nnouveau_svm_fault(struct work_struct *work)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = container_of(work, typeof(*buffer), work);\n\tstruct nouveau_svm *svm = container_of(buffer, typeof(*svm), buffer[buffer->id]);\n\tstruct nvif_object *device = &svm->drm->client.device.object;\n\tstruct nouveau_svmm *svmm;\n\tstruct {\n\t\tstruct nouveau_pfnmap_args i;\n\t\tu64 phys[1];\n\t} args;\n\tunsigned long hmm_flags;\n\tu64 inst, start, limit;\n\tint fi, fn;\n\tint replay = 0, atomic = 0, ret;\n\n\t \n\tSVM_DBG(svm, \"fault handler\");\n\tif (buffer->get == buffer->put) {\n\t\tbuffer->put = nvif_rd32(device, buffer->putaddr);\n\t\tbuffer->get = nvif_rd32(device, buffer->getaddr);\n\t\tif (buffer->get == buffer->put)\n\t\t\treturn;\n\t}\n\tbuffer->fault_nr = 0;\n\n\tSVM_DBG(svm, \"get %08x put %08x\", buffer->get, buffer->put);\n\twhile (buffer->get != buffer->put) {\n\t\tnouveau_svm_fault_cache(svm, buffer, buffer->get * 0x20);\n\t\tif (++buffer->get == buffer->entries)\n\t\t\tbuffer->get = 0;\n\t}\n\tnvif_wr32(device, buffer->getaddr, buffer->get);\n\tSVM_DBG(svm, \"%d fault(s) pending\", buffer->fault_nr);\n\n\t \n\tsort(buffer->fault, buffer->fault_nr, sizeof(*buffer->fault),\n\t     nouveau_svm_fault_cmp, NULL);\n\n\t \n\tmutex_lock(&svm->mutex);\n\tfor (fi = 0, svmm = NULL; fi < buffer->fault_nr; fi++) {\n\t\tif (!svmm || buffer->fault[fi]->inst != inst) {\n\t\t\tstruct nouveau_ivmm *ivmm =\n\t\t\t\tnouveau_ivmm_find(svm, buffer->fault[fi]->inst);\n\t\t\tsvmm = ivmm ? ivmm->svmm : NULL;\n\t\t\tinst = buffer->fault[fi]->inst;\n\t\t\tSVM_DBG(svm, \"inst %016llx -> svm-%p\", inst, svmm);\n\t\t}\n\t\tbuffer->fault[fi]->svmm = svmm;\n\t}\n\tmutex_unlock(&svm->mutex);\n\n\t \n\targs.i.i.version = 0;\n\targs.i.i.type = NVIF_IOCTL_V0_MTHD;\n\targs.i.m.version = 0;\n\targs.i.m.method = NVIF_VMM_V0_PFNMAP;\n\targs.i.p.version = 0;\n\n\tfor (fi = 0; fn = fi + 1, fi < buffer->fault_nr; fi = fn) {\n\t\tstruct svm_notifier notifier;\n\t\tstruct mm_struct *mm;\n\n\t\t \n\t\tif (!(svmm = buffer->fault[fi]->svmm)) {\n\t\t\tnouveau_svm_fault_cancel_fault(svm, buffer->fault[fi]);\n\t\t\tcontinue;\n\t\t}\n\t\tSVMM_DBG(svmm, \"addr %016llx\", buffer->fault[fi]->addr);\n\n\t\t \n\t\tstart = buffer->fault[fi]->addr;\n\t\tlimit = start + PAGE_SIZE;\n\t\tif (start < svmm->unmanaged.limit)\n\t\t\tlimit = min_t(u64, limit, svmm->unmanaged.start);\n\n\t\t \n\t\targs.i.p.addr = start;\n\t\targs.i.p.page = PAGE_SHIFT;\n\t\targs.i.p.size = PAGE_SIZE;\n\t\t \n\t\tswitch (buffer->fault[fi]->access) {\n\t\tcase 0:  \n\t\t\thmm_flags = HMM_PFN_REQ_FAULT;\n\t\t\tbreak;\n\t\tcase 2:  \n\t\t\tatomic = true;\n\t\t\tbreak;\n\t\tcase 3:  \n\t\t\thmm_flags = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\thmm_flags = HMM_PFN_REQ_FAULT | HMM_PFN_REQ_WRITE;\n\t\t\tbreak;\n\t\t}\n\n\t\tmm = svmm->notifier.mm;\n\t\tif (!mmget_not_zero(mm)) {\n\t\t\tnouveau_svm_fault_cancel_fault(svm, buffer->fault[fi]);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnotifier.svmm = svmm;\n\t\tif (atomic)\n\t\t\tret = nouveau_atomic_range_fault(svmm, svm->drm,\n\t\t\t\t\t\t\t &args.i, sizeof(args),\n\t\t\t\t\t\t\t &notifier);\n\t\telse\n\t\t\tret = nouveau_range_fault(svmm, svm->drm, &args.i,\n\t\t\t\t\t\t  sizeof(args), hmm_flags,\n\t\t\t\t\t\t  &notifier);\n\t\tmmput(mm);\n\n\t\tlimit = args.i.p.addr + args.i.p.size;\n\t\tfor (fn = fi; ++fn < buffer->fault_nr; ) {\n\t\t\t \n\t\t\tif (buffer->fault[fn]->svmm != svmm ||\n\t\t\t    buffer->fault[fn]->addr >= limit ||\n\t\t\t    (buffer->fault[fi]->access == FAULT_ACCESS_READ &&\n\t\t\t     !(args.phys[0] & NVIF_VMM_PFNMAP_V0_V)) ||\n\t\t\t    (buffer->fault[fi]->access != FAULT_ACCESS_READ &&\n\t\t\t     buffer->fault[fi]->access != FAULT_ACCESS_PREFETCH &&\n\t\t\t     !(args.phys[0] & NVIF_VMM_PFNMAP_V0_W)) ||\n\t\t\t    (buffer->fault[fi]->access != FAULT_ACCESS_READ &&\n\t\t\t     buffer->fault[fi]->access != FAULT_ACCESS_WRITE &&\n\t\t\t     buffer->fault[fi]->access != FAULT_ACCESS_PREFETCH &&\n\t\t\t     !(args.phys[0] & NVIF_VMM_PFNMAP_V0_A)))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (ret) {\n\t\t\twhile (fi < fn) {\n\t\t\t\tstruct nouveau_svm_fault *fault =\n\t\t\t\t\tbuffer->fault[fi++];\n\n\t\t\t\tnouveau_svm_fault_cancel_fault(svm, fault);\n\t\t\t}\n\t\t} else\n\t\t\treplay++;\n\t}\n\n\t \n\tif (replay)\n\t\tnouveau_svm_fault_replay(svm);\n}\n\nstatic int\nnouveau_svm_event(struct nvif_event *event, void *argv, u32 argc)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = container_of(event, typeof(*buffer), notify);\n\n\tschedule_work(&buffer->work);\n\treturn NVIF_EVENT_KEEP;\n}\n\nstatic struct nouveau_pfnmap_args *\nnouveau_pfns_to_args(void *pfns)\n{\n\treturn container_of(pfns, struct nouveau_pfnmap_args, p.phys);\n}\n\nu64 *\nnouveau_pfns_alloc(unsigned long npages)\n{\n\tstruct nouveau_pfnmap_args *args;\n\n\targs = kzalloc(struct_size(args, p.phys, npages), GFP_KERNEL);\n\tif (!args)\n\t\treturn NULL;\n\n\targs->i.type = NVIF_IOCTL_V0_MTHD;\n\targs->m.method = NVIF_VMM_V0_PFNMAP;\n\targs->p.page = PAGE_SHIFT;\n\n\treturn args->p.phys;\n}\n\nvoid\nnouveau_pfns_free(u64 *pfns)\n{\n\tstruct nouveau_pfnmap_args *args = nouveau_pfns_to_args(pfns);\n\n\tkfree(args);\n}\n\nvoid\nnouveau_pfns_map(struct nouveau_svmm *svmm, struct mm_struct *mm,\n\t\t unsigned long addr, u64 *pfns, unsigned long npages)\n{\n\tstruct nouveau_pfnmap_args *args = nouveau_pfns_to_args(pfns);\n\tint ret;\n\n\targs->p.addr = addr;\n\targs->p.size = npages << PAGE_SHIFT;\n\n\tmutex_lock(&svmm->mutex);\n\n\tret = nvif_object_ioctl(&svmm->vmm->vmm.object, args,\n\t\t\t\tstruct_size(args, p.phys, npages), NULL);\n\n\tmutex_unlock(&svmm->mutex);\n}\n\nstatic void\nnouveau_svm_fault_buffer_fini(struct nouveau_svm *svm, int id)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = &svm->buffer[id];\n\n\tnvif_event_block(&buffer->notify);\n\tflush_work(&buffer->work);\n}\n\nstatic int\nnouveau_svm_fault_buffer_init(struct nouveau_svm *svm, int id)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = &svm->buffer[id];\n\tstruct nvif_object *device = &svm->drm->client.device.object;\n\n\tbuffer->get = nvif_rd32(device, buffer->getaddr);\n\tbuffer->put = nvif_rd32(device, buffer->putaddr);\n\tSVM_DBG(svm, \"get %08x put %08x (init)\", buffer->get, buffer->put);\n\n\treturn nvif_event_allow(&buffer->notify);\n}\n\nstatic void\nnouveau_svm_fault_buffer_dtor(struct nouveau_svm *svm, int id)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = &svm->buffer[id];\n\tint i;\n\n\tif (!nvif_object_constructed(&buffer->object))\n\t\treturn;\n\n\tnouveau_svm_fault_buffer_fini(svm, id);\n\n\tif (buffer->fault) {\n\t\tfor (i = 0; buffer->fault[i] && i < buffer->entries; i++)\n\t\t\tkfree(buffer->fault[i]);\n\t\tkvfree(buffer->fault);\n\t}\n\n\tnvif_event_dtor(&buffer->notify);\n\tnvif_object_dtor(&buffer->object);\n}\n\nstatic int\nnouveau_svm_fault_buffer_ctor(struct nouveau_svm *svm, s32 oclass, int id)\n{\n\tstruct nouveau_svm_fault_buffer *buffer = &svm->buffer[id];\n\tstruct nouveau_drm *drm = svm->drm;\n\tstruct nvif_object *device = &drm->client.device.object;\n\tstruct nvif_clb069_v0 args = {};\n\tint ret;\n\n\tbuffer->id = id;\n\n\tret = nvif_object_ctor(device, \"svmFaultBuffer\", 0, oclass, &args,\n\t\t\t       sizeof(args), &buffer->object);\n\tif (ret < 0) {\n\t\tSVM_ERR(svm, \"Fault buffer allocation failed: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tnvif_object_map(&buffer->object, NULL, 0);\n\tbuffer->entries = args.entries;\n\tbuffer->getaddr = args.get;\n\tbuffer->putaddr = args.put;\n\tINIT_WORK(&buffer->work, nouveau_svm_fault);\n\n\tret = nvif_event_ctor(&buffer->object, \"svmFault\", id, nouveau_svm_event, true, NULL, 0,\n\t\t\t      &buffer->notify);\n\tif (ret)\n\t\treturn ret;\n\n\tbuffer->fault = kvcalloc(sizeof(*buffer->fault), buffer->entries, GFP_KERNEL);\n\tif (!buffer->fault)\n\t\treturn -ENOMEM;\n\n\treturn nouveau_svm_fault_buffer_init(svm, id);\n}\n\nvoid\nnouveau_svm_resume(struct nouveau_drm *drm)\n{\n\tstruct nouveau_svm *svm = drm->svm;\n\tif (svm)\n\t\tnouveau_svm_fault_buffer_init(svm, 0);\n}\n\nvoid\nnouveau_svm_suspend(struct nouveau_drm *drm)\n{\n\tstruct nouveau_svm *svm = drm->svm;\n\tif (svm)\n\t\tnouveau_svm_fault_buffer_fini(svm, 0);\n}\n\nvoid\nnouveau_svm_fini(struct nouveau_drm *drm)\n{\n\tstruct nouveau_svm *svm = drm->svm;\n\tif (svm) {\n\t\tnouveau_svm_fault_buffer_dtor(svm, 0);\n\t\tkfree(drm->svm);\n\t\tdrm->svm = NULL;\n\t}\n}\n\nvoid\nnouveau_svm_init(struct nouveau_drm *drm)\n{\n\tstatic const struct nvif_mclass buffers[] = {\n\t\t{   VOLTA_FAULT_BUFFER_A, 0 },\n\t\t{ MAXWELL_FAULT_BUFFER_A, 0 },\n\t\t{}\n\t};\n\tstruct nouveau_svm *svm;\n\tint ret;\n\n\t \n\tif (drm->client.device.info.family > NV_DEVICE_INFO_V0_PASCAL)\n\t\treturn;\n\n\tif (!(drm->svm = svm = kzalloc(sizeof(*drm->svm), GFP_KERNEL)))\n\t\treturn;\n\n\tdrm->svm->drm = drm;\n\tmutex_init(&drm->svm->mutex);\n\tINIT_LIST_HEAD(&drm->svm->inst);\n\n\tret = nvif_mclass(&drm->client.device.object, buffers);\n\tif (ret < 0) {\n\t\tSVM_DBG(svm, \"No supported fault buffer class\");\n\t\tnouveau_svm_fini(drm);\n\t\treturn;\n\t}\n\n\tret = nouveau_svm_fault_buffer_ctor(svm, buffers[ret].oclass, 0);\n\tif (ret) {\n\t\tnouveau_svm_fini(drm);\n\t\treturn;\n\t}\n\n\tSVM_DBG(svm, \"Initialised\");\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}