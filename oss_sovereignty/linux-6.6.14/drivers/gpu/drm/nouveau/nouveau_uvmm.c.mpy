{
  "module_name": "nouveau_uvmm.c",
  "hash_id": "58e1b9fe7d39adf5478781d0b2fa977801e9a6cf1702e70dd821f7a99fa960fc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/nouveau/nouveau_uvmm.c",
  "human_readable_source": "\n\n \n\n#include \"nouveau_drv.h\"\n#include \"nouveau_gem.h\"\n#include \"nouveau_mem.h\"\n#include \"nouveau_uvmm.h\"\n\n#include <nvif/vmm.h>\n#include <nvif/mem.h>\n\n#include <nvif/class.h>\n#include <nvif/if000c.h>\n#include <nvif/if900d.h>\n\n#define NOUVEAU_VA_SPACE_BITS\t\t47  \n#define NOUVEAU_VA_SPACE_START\t\t0x0\n#define NOUVEAU_VA_SPACE_END\t\t(1ULL << NOUVEAU_VA_SPACE_BITS)\n\n#define list_last_op(_ops) list_last_entry(_ops, struct bind_job_op, entry)\n#define list_prev_op(_op) list_prev_entry(_op, entry)\n#define list_for_each_op(_op, _ops) list_for_each_entry(_op, _ops, entry)\n#define list_for_each_op_from_reverse(_op, _ops) \\\n\tlist_for_each_entry_from_reverse(_op, _ops, entry)\n#define list_for_each_op_safe(_op, _n, _ops) list_for_each_entry_safe(_op, _n, _ops, entry)\n\nenum vm_bind_op {\n\tOP_MAP = DRM_NOUVEAU_VM_BIND_OP_MAP,\n\tOP_UNMAP = DRM_NOUVEAU_VM_BIND_OP_UNMAP,\n\tOP_MAP_SPARSE,\n\tOP_UNMAP_SPARSE,\n};\n\nstruct nouveau_uvma_prealloc {\n\tstruct nouveau_uvma *map;\n\tstruct nouveau_uvma *prev;\n\tstruct nouveau_uvma *next;\n};\n\nstruct bind_job_op {\n\tstruct list_head entry;\n\n\tenum vm_bind_op op;\n\tu32 flags;\n\n\tstruct {\n\t\tu64 addr;\n\t\tu64 range;\n\t} va;\n\n\tstruct {\n\t\tu32 handle;\n\t\tu64 offset;\n\t\tstruct drm_gem_object *obj;\n\t} gem;\n\n\tstruct nouveau_uvma_region *reg;\n\tstruct nouveau_uvma_prealloc new;\n\tstruct drm_gpuva_ops *ops;\n};\n\nstruct uvmm_map_args {\n\tstruct nouveau_uvma_region *region;\n\tu64 addr;\n\tu64 range;\n\tu8 kind;\n};\n\nstatic int\nnouveau_uvmm_vmm_sparse_ref(struct nouveau_uvmm *uvmm,\n\t\t\t    u64 addr, u64 range)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\n\treturn nvif_vmm_raw_sparse(vmm, addr, range, true);\n}\n\nstatic int\nnouveau_uvmm_vmm_sparse_unref(struct nouveau_uvmm *uvmm,\n\t\t\t      u64 addr, u64 range)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\n\treturn nvif_vmm_raw_sparse(vmm, addr, range, false);\n}\n\nstatic int\nnouveau_uvmm_vmm_get(struct nouveau_uvmm *uvmm,\n\t\t     u64 addr, u64 range)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\n\treturn nvif_vmm_raw_get(vmm, addr, range, PAGE_SHIFT);\n}\n\nstatic int\nnouveau_uvmm_vmm_put(struct nouveau_uvmm *uvmm,\n\t\t     u64 addr, u64 range)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\n\treturn nvif_vmm_raw_put(vmm, addr, range, PAGE_SHIFT);\n}\n\nstatic int\nnouveau_uvmm_vmm_unmap(struct nouveau_uvmm *uvmm,\n\t\t       u64 addr, u64 range, bool sparse)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\n\treturn nvif_vmm_raw_unmap(vmm, addr, range, PAGE_SHIFT, sparse);\n}\n\nstatic int\nnouveau_uvmm_vmm_map(struct nouveau_uvmm *uvmm,\n\t\t     u64 addr, u64 range,\n\t\t     u64 bo_offset, u8 kind,\n\t\t     struct nouveau_mem *mem)\n{\n\tstruct nvif_vmm *vmm = &uvmm->vmm.vmm;\n\tunion {\n\t\tstruct gf100_vmm_map_v0 gf100;\n\t} args;\n\tu32 argc = 0;\n\n\tswitch (vmm->object.oclass) {\n\tcase NVIF_CLASS_VMM_GF100:\n\tcase NVIF_CLASS_VMM_GM200:\n\tcase NVIF_CLASS_VMM_GP100:\n\t\targs.gf100.version = 0;\n\t\tif (mem->mem.type & NVIF_MEM_VRAM)\n\t\t\targs.gf100.vol = 0;\n\t\telse\n\t\t\targs.gf100.vol = 1;\n\t\targs.gf100.ro = 0;\n\t\targs.gf100.priv = 0;\n\t\targs.gf100.kind = kind;\n\t\targc = sizeof(args.gf100);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -ENOSYS;\n\t}\n\n\treturn nvif_vmm_raw_map(vmm, addr, range, PAGE_SHIFT,\n\t\t\t\t&args, argc,\n\t\t\t\t&mem->mem, bo_offset);\n}\n\nstatic int\nnouveau_uvma_region_sparse_unref(struct nouveau_uvma_region *reg)\n{\n\tu64 addr = reg->va.addr;\n\tu64 range = reg->va.range;\n\n\treturn nouveau_uvmm_vmm_sparse_unref(reg->uvmm, addr, range);\n}\n\nstatic int\nnouveau_uvma_vmm_put(struct nouveau_uvma *uvma)\n{\n\tu64 addr = uvma->va.va.addr;\n\tu64 range = uvma->va.va.range;\n\n\treturn nouveau_uvmm_vmm_put(to_uvmm(uvma), addr, range);\n}\n\nstatic int\nnouveau_uvma_map(struct nouveau_uvma *uvma,\n\t\t struct nouveau_mem *mem)\n{\n\tu64 addr = uvma->va.va.addr;\n\tu64 offset = uvma->va.gem.offset;\n\tu64 range = uvma->va.va.range;\n\n\treturn nouveau_uvmm_vmm_map(to_uvmm(uvma), addr, range,\n\t\t\t\t    offset, uvma->kind, mem);\n}\n\nstatic int\nnouveau_uvma_unmap(struct nouveau_uvma *uvma)\n{\n\tu64 addr = uvma->va.va.addr;\n\tu64 range = uvma->va.va.range;\n\tbool sparse = !!uvma->region;\n\n\tif (drm_gpuva_invalidated(&uvma->va))\n\t\treturn 0;\n\n\treturn nouveau_uvmm_vmm_unmap(to_uvmm(uvma), addr, range, sparse);\n}\n\nstatic int\nnouveau_uvma_alloc(struct nouveau_uvma **puvma)\n{\n\t*puvma = kzalloc(sizeof(**puvma), GFP_KERNEL);\n\tif (!*puvma)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void\nnouveau_uvma_free(struct nouveau_uvma *uvma)\n{\n\tkfree(uvma);\n}\n\nstatic void\nnouveau_uvma_gem_get(struct nouveau_uvma *uvma)\n{\n\tdrm_gem_object_get(uvma->va.gem.obj);\n}\n\nstatic void\nnouveau_uvma_gem_put(struct nouveau_uvma *uvma)\n{\n\tdrm_gem_object_put(uvma->va.gem.obj);\n}\n\nstatic int\nnouveau_uvma_region_alloc(struct nouveau_uvma_region **preg)\n{\n\t*preg = kzalloc(sizeof(**preg), GFP_KERNEL);\n\tif (!*preg)\n\t\treturn -ENOMEM;\n\n\tkref_init(&(*preg)->kref);\n\n\treturn 0;\n}\n\nstatic void\nnouveau_uvma_region_free(struct kref *kref)\n{\n\tstruct nouveau_uvma_region *reg =\n\t\tcontainer_of(kref, struct nouveau_uvma_region, kref);\n\n\tkfree(reg);\n}\n\nstatic void\nnouveau_uvma_region_get(struct nouveau_uvma_region *reg)\n{\n\tkref_get(&reg->kref);\n}\n\nstatic void\nnouveau_uvma_region_put(struct nouveau_uvma_region *reg)\n{\n\tkref_put(&reg->kref, nouveau_uvma_region_free);\n}\n\nstatic int\n__nouveau_uvma_region_insert(struct nouveau_uvmm *uvmm,\n\t\t\t     struct nouveau_uvma_region *reg)\n{\n\tu64 addr = reg->va.addr;\n\tu64 range = reg->va.range;\n\tu64 last = addr + range - 1;\n\tMA_STATE(mas, &uvmm->region_mt, addr, addr);\n\n\tif (unlikely(mas_walk(&mas)))\n\t\treturn -EEXIST;\n\n\tif (unlikely(mas.last < last))\n\t\treturn -EEXIST;\n\n\tmas.index = addr;\n\tmas.last = last;\n\n\tmas_store_gfp(&mas, reg, GFP_KERNEL);\n\n\treg->uvmm = uvmm;\n\n\treturn 0;\n}\n\nstatic int\nnouveau_uvma_region_insert(struct nouveau_uvmm *uvmm,\n\t\t\t   struct nouveau_uvma_region *reg,\n\t\t\t   u64 addr, u64 range)\n{\n\tint ret;\n\n\treg->uvmm = uvmm;\n\treg->va.addr = addr;\n\treg->va.range = range;\n\n\tret = __nouveau_uvma_region_insert(uvmm, reg);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic void\nnouveau_uvma_region_remove(struct nouveau_uvma_region *reg)\n{\n\tstruct nouveau_uvmm *uvmm = reg->uvmm;\n\tMA_STATE(mas, &uvmm->region_mt, reg->va.addr, 0);\n\n\tmas_erase(&mas);\n}\n\nstatic int\nnouveau_uvma_region_create(struct nouveau_uvmm *uvmm,\n\t\t\t   u64 addr, u64 range)\n{\n\tstruct nouveau_uvma_region *reg;\n\tint ret;\n\n\tif (!drm_gpuva_interval_empty(&uvmm->umgr, addr, range))\n\t\treturn -ENOSPC;\n\n\tret = nouveau_uvma_region_alloc(&reg);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nouveau_uvma_region_insert(uvmm, reg, addr, range);\n\tif (ret)\n\t\tgoto err_free_region;\n\n\tret = nouveau_uvmm_vmm_sparse_ref(uvmm, addr, range);\n\tif (ret)\n\t\tgoto err_region_remove;\n\n\treturn 0;\n\nerr_region_remove:\n\tnouveau_uvma_region_remove(reg);\nerr_free_region:\n\tnouveau_uvma_region_put(reg);\n\treturn ret;\n}\n\nstatic struct nouveau_uvma_region *\nnouveau_uvma_region_find_first(struct nouveau_uvmm *uvmm,\n\t\t\t       u64 addr, u64 range)\n{\n\tMA_STATE(mas, &uvmm->region_mt, addr, 0);\n\n\treturn mas_find(&mas, addr + range - 1);\n}\n\nstatic struct nouveau_uvma_region *\nnouveau_uvma_region_find(struct nouveau_uvmm *uvmm,\n\t\t\t u64 addr, u64 range)\n{\n\tstruct nouveau_uvma_region *reg;\n\n\treg = nouveau_uvma_region_find_first(uvmm, addr, range);\n\tif (!reg)\n\t\treturn NULL;\n\n\tif (reg->va.addr != addr ||\n\t    reg->va.range != range)\n\t\treturn NULL;\n\n\treturn reg;\n}\n\nstatic bool\nnouveau_uvma_region_empty(struct nouveau_uvma_region *reg)\n{\n\tstruct nouveau_uvmm *uvmm = reg->uvmm;\n\n\treturn drm_gpuva_interval_empty(&uvmm->umgr,\n\t\t\t\t\treg->va.addr,\n\t\t\t\t\treg->va.range);\n}\n\nstatic int\n__nouveau_uvma_region_destroy(struct nouveau_uvma_region *reg)\n{\n\tstruct nouveau_uvmm *uvmm = reg->uvmm;\n\tu64 addr = reg->va.addr;\n\tu64 range = reg->va.range;\n\n\tif (!nouveau_uvma_region_empty(reg))\n\t\treturn -EBUSY;\n\n\tnouveau_uvma_region_remove(reg);\n\tnouveau_uvmm_vmm_sparse_unref(uvmm, addr, range);\n\tnouveau_uvma_region_put(reg);\n\n\treturn 0;\n}\n\nstatic int\nnouveau_uvma_region_destroy(struct nouveau_uvmm *uvmm,\n\t\t\t    u64 addr, u64 range)\n{\n\tstruct nouveau_uvma_region *reg;\n\n\treg = nouveau_uvma_region_find(uvmm, addr, range);\n\tif (!reg)\n\t\treturn -ENOENT;\n\n\treturn __nouveau_uvma_region_destroy(reg);\n}\n\nstatic void\nnouveau_uvma_region_dirty(struct nouveau_uvma_region *reg)\n{\n\n\tinit_completion(&reg->complete);\n\treg->dirty = true;\n}\n\nstatic void\nnouveau_uvma_region_complete(struct nouveau_uvma_region *reg)\n{\n\tcomplete_all(&reg->complete);\n}\n\nstatic void\nop_map_prepare_unwind(struct nouveau_uvma *uvma)\n{\n\tnouveau_uvma_gem_put(uvma);\n\tdrm_gpuva_remove(&uvma->va);\n\tnouveau_uvma_free(uvma);\n}\n\nstatic void\nop_unmap_prepare_unwind(struct drm_gpuva *va)\n{\n\tdrm_gpuva_insert(va->mgr, va);\n}\n\nstatic void\nnouveau_uvmm_sm_prepare_unwind(struct nouveau_uvmm *uvmm,\n\t\t\t       struct nouveau_uvma_prealloc *new,\n\t\t\t       struct drm_gpuva_ops *ops,\n\t\t\t       struct drm_gpuva_op *last,\n\t\t\t       struct uvmm_map_args *args)\n{\n\tstruct drm_gpuva_op *op = last;\n\tu64 vmm_get_start = args ? args->addr : 0;\n\tu64 vmm_get_end = args ? args->addr + args->range : 0;\n\n\t \n\tdrm_gpuva_for_each_op_from_reverse(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP:\n\t\t\top_map_prepare_unwind(new->map);\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_REMAP: {\n\t\t\tstruct drm_gpuva_op_remap *r = &op->remap;\n\n\t\t\tif (r->next)\n\t\t\t\top_map_prepare_unwind(new->next);\n\n\t\t\tif (r->prev)\n\t\t\t\top_map_prepare_unwind(new->prev);\n\n\t\t\top_unmap_prepare_unwind(r->unmap->va);\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_UNMAP:\n\t\t\top_unmap_prepare_unwind(op->unmap.va);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (!args)\n\t\treturn;\n\n\tdrm_gpuva_for_each_op(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP: {\n\t\t\tu64 vmm_get_range = vmm_get_end - vmm_get_start;\n\n\t\t\tif (vmm_get_range)\n\t\t\t\tnouveau_uvmm_vmm_put(uvmm, vmm_get_start,\n\t\t\t\t\t\t     vmm_get_range);\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_REMAP: {\n\t\t\tstruct drm_gpuva_op_remap *r = &op->remap;\n\t\t\tstruct drm_gpuva *va = r->unmap->va;\n\t\t\tu64 ustart = va->va.addr;\n\t\t\tu64 urange = va->va.range;\n\t\t\tu64 uend = ustart + urange;\n\n\t\t\tif (r->prev)\n\t\t\t\tvmm_get_start = uend;\n\n\t\t\tif (r->next)\n\t\t\t\tvmm_get_end = ustart;\n\n\t\t\tif (r->prev && r->next)\n\t\t\t\tvmm_get_start = vmm_get_end = 0;\n\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_UNMAP: {\n\t\t\tstruct drm_gpuva_op_unmap *u = &op->unmap;\n\t\t\tstruct drm_gpuva *va = u->va;\n\t\t\tu64 ustart = va->va.addr;\n\t\t\tu64 urange = va->va.range;\n\t\t\tu64 uend = ustart + urange;\n\n\t\t\t \n\t\t\tif (uend == vmm_get_start ||\n\t\t\t    ustart == vmm_get_end)\n\t\t\t\tbreak;\n\n\t\t\tif (ustart > vmm_get_start) {\n\t\t\t\tu64 vmm_get_range = ustart - vmm_get_start;\n\n\t\t\t\tnouveau_uvmm_vmm_put(uvmm, vmm_get_start,\n\t\t\t\t\t\t     vmm_get_range);\n\t\t\t}\n\t\t\tvmm_get_start = uend;\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (op == last)\n\t\t\tbreak;\n\t}\n}\n\nstatic void\nnouveau_uvmm_sm_map_prepare_unwind(struct nouveau_uvmm *uvmm,\n\t\t\t\t   struct nouveau_uvma_prealloc *new,\n\t\t\t\t   struct drm_gpuva_ops *ops,\n\t\t\t\t   u64 addr, u64 range)\n{\n\tstruct drm_gpuva_op *last = drm_gpuva_last_op(ops);\n\tstruct uvmm_map_args args = {\n\t\t.addr = addr,\n\t\t.range = range,\n\t};\n\n\tnouveau_uvmm_sm_prepare_unwind(uvmm, new, ops, last, &args);\n}\n\nstatic void\nnouveau_uvmm_sm_unmap_prepare_unwind(struct nouveau_uvmm *uvmm,\n\t\t\t\t     struct nouveau_uvma_prealloc *new,\n\t\t\t\t     struct drm_gpuva_ops *ops)\n{\n\tstruct drm_gpuva_op *last = drm_gpuva_last_op(ops);\n\n\tnouveau_uvmm_sm_prepare_unwind(uvmm, new, ops, last, NULL);\n}\n\nstatic int\nop_map_prepare(struct nouveau_uvmm *uvmm,\n\t       struct nouveau_uvma **puvma,\n\t       struct drm_gpuva_op_map *op,\n\t       struct uvmm_map_args *args)\n{\n\tstruct nouveau_uvma *uvma;\n\tint ret;\n\n\tret = nouveau_uvma_alloc(&uvma);\n\tif (ret)\n\t\treturn ret;\n\n\tuvma->region = args->region;\n\tuvma->kind = args->kind;\n\n\tdrm_gpuva_map(&uvmm->umgr, &uvma->va, op);\n\n\t \n\tnouveau_uvma_gem_get(uvma);\n\n\t*puvma = uvma;\n\treturn 0;\n}\n\nstatic void\nop_unmap_prepare(struct drm_gpuva_op_unmap *u)\n{\n\tdrm_gpuva_unmap(u);\n}\n\nstatic int\nnouveau_uvmm_sm_prepare(struct nouveau_uvmm *uvmm,\n\t\t\tstruct nouveau_uvma_prealloc *new,\n\t\t\tstruct drm_gpuva_ops *ops,\n\t\t\tstruct uvmm_map_args *args)\n{\n\tstruct drm_gpuva_op *op;\n\tu64 vmm_get_start = args ? args->addr : 0;\n\tu64 vmm_get_end = args ? args->addr + args->range : 0;\n\tint ret;\n\n\tdrm_gpuva_for_each_op(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP: {\n\t\t\tu64 vmm_get_range = vmm_get_end - vmm_get_start;\n\n\t\t\tret = op_map_prepare(uvmm, &new->map, &op->map, args);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\n\t\t\tif (args && vmm_get_range) {\n\t\t\t\tret = nouveau_uvmm_vmm_get(uvmm, vmm_get_start,\n\t\t\t\t\t\t\t   vmm_get_range);\n\t\t\t\tif (ret) {\n\t\t\t\t\top_map_prepare_unwind(new->map);\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_REMAP: {\n\t\t\tstruct drm_gpuva_op_remap *r = &op->remap;\n\t\t\tstruct drm_gpuva *va = r->unmap->va;\n\t\t\tstruct uvmm_map_args remap_args = {\n\t\t\t\t.kind = uvma_from_va(va)->kind,\n\t\t\t\t.region = uvma_from_va(va)->region,\n\t\t\t};\n\t\t\tu64 ustart = va->va.addr;\n\t\t\tu64 urange = va->va.range;\n\t\t\tu64 uend = ustart + urange;\n\n\t\t\top_unmap_prepare(r->unmap);\n\n\t\t\tif (r->prev) {\n\t\t\t\tret = op_map_prepare(uvmm, &new->prev, r->prev,\n\t\t\t\t\t\t     &remap_args);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto unwind;\n\n\t\t\t\tif (args)\n\t\t\t\t\tvmm_get_start = uend;\n\t\t\t}\n\n\t\t\tif (r->next) {\n\t\t\t\tret = op_map_prepare(uvmm, &new->next, r->next,\n\t\t\t\t\t\t     &remap_args);\n\t\t\t\tif (ret) {\n\t\t\t\t\tif (r->prev)\n\t\t\t\t\t\top_map_prepare_unwind(new->prev);\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\n\t\t\t\tif (args)\n\t\t\t\t\tvmm_get_end = ustart;\n\t\t\t}\n\n\t\t\tif (args && (r->prev && r->next))\n\t\t\t\tvmm_get_start = vmm_get_end = 0;\n\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_UNMAP: {\n\t\t\tstruct drm_gpuva_op_unmap *u = &op->unmap;\n\t\t\tstruct drm_gpuva *va = u->va;\n\t\t\tu64 ustart = va->va.addr;\n\t\t\tu64 urange = va->va.range;\n\t\t\tu64 uend = ustart + urange;\n\n\t\t\top_unmap_prepare(u);\n\n\t\t\tif (!args)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (uend == vmm_get_start ||\n\t\t\t    ustart == vmm_get_end)\n\t\t\t\tbreak;\n\n\t\t\tif (ustart > vmm_get_start) {\n\t\t\t\tu64 vmm_get_range = ustart - vmm_get_start;\n\n\t\t\t\tret = nouveau_uvmm_vmm_get(uvmm, vmm_get_start,\n\t\t\t\t\t\t\t   vmm_get_range);\n\t\t\t\tif (ret) {\n\t\t\t\t\top_unmap_prepare_unwind(va);\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\t\t\t}\n\t\t\tvmm_get_start = uend;\n\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\t}\n\n\treturn 0;\n\nunwind:\n\tif (op != drm_gpuva_first_op(ops))\n\t\tnouveau_uvmm_sm_prepare_unwind(uvmm, new, ops,\n\t\t\t\t\t       drm_gpuva_prev_op(op),\n\t\t\t\t\t       args);\n\treturn ret;\n}\n\nstatic int\nnouveau_uvmm_sm_map_prepare(struct nouveau_uvmm *uvmm,\n\t\t\t    struct nouveau_uvma_prealloc *new,\n\t\t\t    struct nouveau_uvma_region *region,\n\t\t\t    struct drm_gpuva_ops *ops,\n\t\t\t    u64 addr, u64 range, u8 kind)\n{\n\tstruct uvmm_map_args args = {\n\t\t.region = region,\n\t\t.addr = addr,\n\t\t.range = range,\n\t\t.kind = kind,\n\t};\n\n\treturn nouveau_uvmm_sm_prepare(uvmm, new, ops, &args);\n}\n\nstatic int\nnouveau_uvmm_sm_unmap_prepare(struct nouveau_uvmm *uvmm,\n\t\t\t      struct nouveau_uvma_prealloc *new,\n\t\t\t      struct drm_gpuva_ops *ops)\n{\n\treturn nouveau_uvmm_sm_prepare(uvmm, new, ops, NULL);\n}\n\nstatic struct drm_gem_object *\nop_gem_obj(struct drm_gpuva_op *op)\n{\n\tswitch (op->op) {\n\tcase DRM_GPUVA_OP_MAP:\n\t\treturn op->map.gem.obj;\n\tcase DRM_GPUVA_OP_REMAP:\n\t\t \n\t\treturn op->remap.unmap->va->gem.obj;\n\tcase DRM_GPUVA_OP_UNMAP:\n\t\treturn op->unmap.va->gem.obj;\n\tdefault:\n\t\tWARN(1, \"Unknown operation.\\n\");\n\t\treturn NULL;\n\t}\n}\n\nstatic void\nop_map(struct nouveau_uvma *uvma)\n{\n\tstruct nouveau_bo *nvbo = nouveau_gem_object(uvma->va.gem.obj);\n\n\tnouveau_uvma_map(uvma, nouveau_mem(nvbo->bo.resource));\n}\n\nstatic void\nop_unmap(struct drm_gpuva_op_unmap *u)\n{\n\tstruct drm_gpuva *va = u->va;\n\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\n\t \n\tif (!u->keep)\n\t\tnouveau_uvma_unmap(uvma);\n}\n\nstatic void\nop_unmap_range(struct drm_gpuva_op_unmap *u,\n\t       u64 addr, u64 range)\n{\n\tstruct nouveau_uvma *uvma = uvma_from_va(u->va);\n\tbool sparse = !!uvma->region;\n\n\tif (!drm_gpuva_invalidated(u->va))\n\t\tnouveau_uvmm_vmm_unmap(to_uvmm(uvma), addr, range, sparse);\n}\n\nstatic void\nop_remap(struct drm_gpuva_op_remap *r,\n\t struct nouveau_uvma_prealloc *new)\n{\n\tstruct drm_gpuva_op_unmap *u = r->unmap;\n\tstruct nouveau_uvma *uvma = uvma_from_va(u->va);\n\tu64 addr = uvma->va.va.addr;\n\tu64 range = uvma->va.va.range;\n\n\tif (r->prev)\n\t\taddr = r->prev->va.addr + r->prev->va.range;\n\n\tif (r->next)\n\t\trange = r->next->va.addr - addr;\n\n\top_unmap_range(u, addr, range);\n}\n\nstatic int\nnouveau_uvmm_sm(struct nouveau_uvmm *uvmm,\n\t\tstruct nouveau_uvma_prealloc *new,\n\t\tstruct drm_gpuva_ops *ops)\n{\n\tstruct drm_gpuva_op *op;\n\n\tdrm_gpuva_for_each_op(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP:\n\t\t\top_map(new->map);\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_REMAP:\n\t\t\top_remap(&op->remap, new);\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_UNMAP:\n\t\t\top_unmap(&op->unmap);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int\nnouveau_uvmm_sm_map(struct nouveau_uvmm *uvmm,\n\t\t    struct nouveau_uvma_prealloc *new,\n\t\t    struct drm_gpuva_ops *ops)\n{\n\treturn nouveau_uvmm_sm(uvmm, new, ops);\n}\n\nstatic int\nnouveau_uvmm_sm_unmap(struct nouveau_uvmm *uvmm,\n\t\t      struct nouveau_uvma_prealloc *new,\n\t\t      struct drm_gpuva_ops *ops)\n{\n\treturn nouveau_uvmm_sm(uvmm, new, ops);\n}\n\nstatic void\nnouveau_uvmm_sm_cleanup(struct nouveau_uvmm *uvmm,\n\t\t\tstruct nouveau_uvma_prealloc *new,\n\t\t\tstruct drm_gpuva_ops *ops, bool unmap)\n{\n\tstruct drm_gpuva_op *op;\n\n\tdrm_gpuva_for_each_op(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP:\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_REMAP: {\n\t\t\tstruct drm_gpuva_op_remap *r = &op->remap;\n\t\t\tstruct drm_gpuva_op_map *p = r->prev;\n\t\t\tstruct drm_gpuva_op_map *n = r->next;\n\t\t\tstruct drm_gpuva *va = r->unmap->va;\n\t\t\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\n\t\t\tif (unmap) {\n\t\t\t\tu64 addr = va->va.addr;\n\t\t\t\tu64 end = addr + va->va.range;\n\n\t\t\t\tif (p)\n\t\t\t\t\taddr = p->va.addr + p->va.range;\n\n\t\t\t\tif (n)\n\t\t\t\t\tend = n->va.addr;\n\n\t\t\t\tnouveau_uvmm_vmm_put(uvmm, addr, end - addr);\n\t\t\t}\n\n\t\t\tnouveau_uvma_gem_put(uvma);\n\t\t\tnouveau_uvma_free(uvma);\n\t\t\tbreak;\n\t\t}\n\t\tcase DRM_GPUVA_OP_UNMAP: {\n\t\t\tstruct drm_gpuva_op_unmap *u = &op->unmap;\n\t\t\tstruct drm_gpuva *va = u->va;\n\t\t\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\n\t\t\tif (unmap)\n\t\t\t\tnouveau_uvma_vmm_put(uvma);\n\n\t\t\tnouveau_uvma_gem_put(uvma);\n\t\t\tnouveau_uvma_free(uvma);\n\t\t\tbreak;\n\t\t}\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void\nnouveau_uvmm_sm_map_cleanup(struct nouveau_uvmm *uvmm,\n\t\t\t    struct nouveau_uvma_prealloc *new,\n\t\t\t    struct drm_gpuva_ops *ops)\n{\n\tnouveau_uvmm_sm_cleanup(uvmm, new, ops, false);\n}\n\nstatic void\nnouveau_uvmm_sm_unmap_cleanup(struct nouveau_uvmm *uvmm,\n\t\t\t      struct nouveau_uvma_prealloc *new,\n\t\t\t      struct drm_gpuva_ops *ops)\n{\n\tnouveau_uvmm_sm_cleanup(uvmm, new, ops, true);\n}\n\nstatic int\nnouveau_uvmm_validate_range(struct nouveau_uvmm *uvmm, u64 addr, u64 range)\n{\n\tu64 end = addr + range;\n\tu64 kernel_managed_end = uvmm->kernel_managed_addr +\n\t\t\t\t uvmm->kernel_managed_size;\n\n\tif (addr & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tif (range & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tif (end <= addr)\n\t\treturn -EINVAL;\n\n\tif (addr < NOUVEAU_VA_SPACE_START ||\n\t    end > NOUVEAU_VA_SPACE_END)\n\t\treturn -EINVAL;\n\n\tif (addr < kernel_managed_end &&\n\t    end > uvmm->kernel_managed_addr)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int\nnouveau_uvmm_bind_job_alloc(struct nouveau_uvmm_bind_job **pjob)\n{\n\t*pjob = kzalloc(sizeof(**pjob), GFP_KERNEL);\n\tif (!*pjob)\n\t\treturn -ENOMEM;\n\n\tkref_init(&(*pjob)->kref);\n\n\treturn 0;\n}\n\nstatic void\nnouveau_uvmm_bind_job_free(struct kref *kref)\n{\n\tstruct nouveau_uvmm_bind_job *job =\n\t\tcontainer_of(kref, struct nouveau_uvmm_bind_job, kref);\n\n\tnouveau_job_free(&job->base);\n\tkfree(job);\n}\n\nstatic void\nnouveau_uvmm_bind_job_get(struct nouveau_uvmm_bind_job *job)\n{\n\tkref_get(&job->kref);\n}\n\nstatic void\nnouveau_uvmm_bind_job_put(struct nouveau_uvmm_bind_job *job)\n{\n\tkref_put(&job->kref, nouveau_uvmm_bind_job_free);\n}\n\nstatic int\nbind_validate_op(struct nouveau_job *job,\n\t\t struct bind_job_op *op)\n{\n\tstruct nouveau_uvmm *uvmm = nouveau_cli_uvmm(job->cli);\n\tstruct drm_gem_object *obj = op->gem.obj;\n\n\tif (op->op == OP_MAP) {\n\t\tif (op->gem.offset & ~PAGE_MASK)\n\t\t\treturn -EINVAL;\n\n\t\tif (obj->size <= op->gem.offset)\n\t\t\treturn -EINVAL;\n\n\t\tif (op->va.range > (obj->size - op->gem.offset))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn nouveau_uvmm_validate_range(uvmm, op->va.addr, op->va.range);\n}\n\nstatic void\nbind_validate_map_sparse(struct nouveau_job *job, u64 addr, u64 range)\n{\n\tstruct nouveau_uvmm_bind_job *bind_job;\n\tstruct nouveau_sched_entity *entity = job->entity;\n\tstruct bind_job_op *op;\n\tu64 end = addr + range;\n\nagain:\n\tspin_lock(&entity->job.list.lock);\n\tlist_for_each_entry(bind_job, &entity->job.list.head, entry) {\n\t\tlist_for_each_op(op, &bind_job->ops) {\n\t\t\tif (op->op == OP_UNMAP) {\n\t\t\t\tu64 op_addr = op->va.addr;\n\t\t\t\tu64 op_end = op_addr + op->va.range;\n\n\t\t\t\tif (!(end <= op_addr || addr >= op_end)) {\n\t\t\t\t\tnouveau_uvmm_bind_job_get(bind_job);\n\t\t\t\t\tspin_unlock(&entity->job.list.lock);\n\t\t\t\t\twait_for_completion(&bind_job->complete);\n\t\t\t\t\tnouveau_uvmm_bind_job_put(bind_job);\n\t\t\t\t\tgoto again;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&entity->job.list.lock);\n}\n\nstatic int\nbind_validate_map_common(struct nouveau_job *job, u64 addr, u64 range,\n\t\t\t bool sparse)\n{\n\tstruct nouveau_uvmm *uvmm = nouveau_cli_uvmm(job->cli);\n\tstruct nouveau_uvma_region *reg;\n\tu64 reg_addr, reg_end;\n\tu64 end = addr + range;\n\nagain:\n\tnouveau_uvmm_lock(uvmm);\n\treg = nouveau_uvma_region_find_first(uvmm, addr, range);\n\tif (!reg) {\n\t\tnouveau_uvmm_unlock(uvmm);\n\t\treturn 0;\n\t}\n\n\t \n\tif (reg->dirty) {\n\t\tnouveau_uvma_region_get(reg);\n\t\tnouveau_uvmm_unlock(uvmm);\n\t\twait_for_completion(&reg->complete);\n\t\tnouveau_uvma_region_put(reg);\n\t\tgoto again;\n\t}\n\tnouveau_uvmm_unlock(uvmm);\n\n\tif (sparse)\n\t\treturn -ENOSPC;\n\n\treg_addr = reg->va.addr;\n\treg_end = reg_addr + reg->va.range;\n\n\t \n\tif (reg_addr > addr || reg_end < end)\n\t\treturn -ENOSPC;\n\n\treturn 0;\n}\n\nstatic int\nbind_validate_region(struct nouveau_job *job)\n{\n\tstruct nouveau_uvmm_bind_job *bind_job = to_uvmm_bind_job(job);\n\tstruct bind_job_op *op;\n\tint ret;\n\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tu64 op_addr = op->va.addr;\n\t\tu64 op_range = op->va.range;\n\t\tbool sparse = false;\n\n\t\tswitch (op->op) {\n\t\tcase OP_MAP_SPARSE:\n\t\t\tsparse = true;\n\t\t\tbind_validate_map_sparse(job, op_addr, op_range);\n\t\t\tfallthrough;\n\t\tcase OP_MAP:\n\t\t\tret = bind_validate_map_common(job, op_addr, op_range,\n\t\t\t\t\t\t       sparse);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nbind_link_gpuvas(struct drm_gpuva_ops *ops, struct nouveau_uvma_prealloc *new)\n{\n\tstruct drm_gpuva_op *op;\n\n\tdrm_gpuva_for_each_op(op, ops) {\n\t\tswitch (op->op) {\n\t\tcase DRM_GPUVA_OP_MAP:\n\t\t\tdrm_gpuva_link(&new->map->va);\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_REMAP:\n\t\t\tif (op->remap.prev)\n\t\t\t\tdrm_gpuva_link(&new->prev->va);\n\t\t\tif (op->remap.next)\n\t\t\t\tdrm_gpuva_link(&new->next->va);\n\t\t\tdrm_gpuva_unlink(op->remap.unmap->va);\n\t\t\tbreak;\n\t\tcase DRM_GPUVA_OP_UNMAP:\n\t\t\tdrm_gpuva_unlink(op->unmap.va);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int\nnouveau_uvmm_bind_job_submit(struct nouveau_job *job)\n{\n\tstruct nouveau_uvmm *uvmm = nouveau_cli_uvmm(job->cli);\n\tstruct nouveau_uvmm_bind_job *bind_job = to_uvmm_bind_job(job);\n\tstruct nouveau_sched_entity *entity = job->entity;\n\tstruct drm_exec *exec = &job->exec;\n\tstruct bind_job_op *op;\n\tint ret;\n\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tif (op->op == OP_MAP) {\n\t\t\top->gem.obj = drm_gem_object_lookup(job->file_priv,\n\t\t\t\t\t\t\t    op->gem.handle);\n\t\t\tif (!op->gem.obj)\n\t\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tret = bind_validate_op(job, op);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tret = bind_validate_region(job);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tnouveau_uvmm_lock(uvmm);\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tswitch (op->op) {\n\t\tcase OP_MAP_SPARSE:\n\t\t\tret = nouveau_uvma_region_create(uvmm,\n\t\t\t\t\t\t\t op->va.addr,\n\t\t\t\t\t\t\t op->va.range);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind_continue;\n\n\t\t\tbreak;\n\t\tcase OP_UNMAP_SPARSE:\n\t\t\top->reg = nouveau_uvma_region_find(uvmm, op->va.addr,\n\t\t\t\t\t\t\t   op->va.range);\n\t\t\tif (!op->reg || op->reg->dirty) {\n\t\t\t\tret = -ENOENT;\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\top->ops = drm_gpuva_sm_unmap_ops_create(&uvmm->umgr,\n\t\t\t\t\t\t\t\top->va.addr,\n\t\t\t\t\t\t\t\top->va.range);\n\t\t\tif (IS_ERR(op->ops)) {\n\t\t\t\tret = PTR_ERR(op->ops);\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tret = nouveau_uvmm_sm_unmap_prepare(uvmm, &op->new,\n\t\t\t\t\t\t\t    op->ops);\n\t\t\tif (ret) {\n\t\t\t\tdrm_gpuva_ops_free(&uvmm->umgr, op->ops);\n\t\t\t\top->ops = NULL;\n\t\t\t\top->reg = NULL;\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tnouveau_uvma_region_dirty(op->reg);\n\n\t\t\tbreak;\n\t\tcase OP_MAP: {\n\t\t\tstruct nouveau_uvma_region *reg;\n\n\t\t\treg = nouveau_uvma_region_find_first(uvmm,\n\t\t\t\t\t\t\t     op->va.addr,\n\t\t\t\t\t\t\t     op->va.range);\n\t\t\tif (reg) {\n\t\t\t\tu64 reg_addr = reg->va.addr;\n\t\t\t\tu64 reg_end = reg_addr + reg->va.range;\n\t\t\t\tu64 op_addr = op->va.addr;\n\t\t\t\tu64 op_end = op_addr + op->va.range;\n\n\t\t\t\tif (unlikely(reg->dirty)) {\n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto unwind_continue;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tif (reg_addr > op_addr || reg_end < op_end) {\n\t\t\t\t\tret = -ENOSPC;\n\t\t\t\t\tgoto unwind_continue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\top->ops = drm_gpuva_sm_map_ops_create(&uvmm->umgr,\n\t\t\t\t\t\t\t      op->va.addr,\n\t\t\t\t\t\t\t      op->va.range,\n\t\t\t\t\t\t\t      op->gem.obj,\n\t\t\t\t\t\t\t      op->gem.offset);\n\t\t\tif (IS_ERR(op->ops)) {\n\t\t\t\tret = PTR_ERR(op->ops);\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tret = nouveau_uvmm_sm_map_prepare(uvmm, &op->new,\n\t\t\t\t\t\t\t  reg, op->ops,\n\t\t\t\t\t\t\t  op->va.addr,\n\t\t\t\t\t\t\t  op->va.range,\n\t\t\t\t\t\t\t  op->flags & 0xff);\n\t\t\tif (ret) {\n\t\t\t\tdrm_gpuva_ops_free(&uvmm->umgr, op->ops);\n\t\t\t\top->ops = NULL;\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t\tcase OP_UNMAP:\n\t\t\top->ops = drm_gpuva_sm_unmap_ops_create(&uvmm->umgr,\n\t\t\t\t\t\t\t\top->va.addr,\n\t\t\t\t\t\t\t\top->va.range);\n\t\t\tif (IS_ERR(op->ops)) {\n\t\t\t\tret = PTR_ERR(op->ops);\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tret = nouveau_uvmm_sm_unmap_prepare(uvmm, &op->new,\n\t\t\t\t\t\t\t    op->ops);\n\t\t\tif (ret) {\n\t\t\t\tdrm_gpuva_ops_free(&uvmm->umgr, op->ops);\n\t\t\t\top->ops = NULL;\n\t\t\t\tgoto unwind_continue;\n\t\t\t}\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind_continue;\n\t\t}\n\t}\n\n\tdrm_exec_init(exec, DRM_EXEC_INTERRUPTIBLE_WAIT |\n\t\t\t    DRM_EXEC_IGNORE_DUPLICATES);\n\tdrm_exec_until_all_locked(exec) {\n\t\tlist_for_each_op(op, &bind_job->ops) {\n\t\t\tstruct drm_gpuva_op *va_op;\n\n\t\t\tif (IS_ERR_OR_NULL(op->ops))\n\t\t\t\tcontinue;\n\n\t\t\tdrm_gpuva_for_each_op(va_op, op->ops) {\n\t\t\t\tstruct drm_gem_object *obj = op_gem_obj(va_op);\n\n\t\t\t\tif (unlikely(!obj))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tret = drm_exec_prepare_obj(exec, obj, 1);\n\t\t\t\tdrm_exec_retry_on_contention(exec);\n\t\t\t\tif (ret) {\n\t\t\t\t\top = list_last_op(&bind_job->ops);\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tstruct drm_gpuva_op *va_op;\n\n\t\tif (IS_ERR_OR_NULL(op->ops))\n\t\t\tcontinue;\n\n\t\tdrm_gpuva_for_each_op(va_op, op->ops) {\n\t\t\tstruct drm_gem_object *obj = op_gem_obj(va_op);\n\n\t\t\tif (unlikely(!obj))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (unlikely(va_op->op == DRM_GPUVA_OP_UNMAP))\n\t\t\t\tcontinue;\n\n\t\t\tret = nouveau_bo_validate(nouveau_gem_object(obj),\n\t\t\t\t\t\t  true, false);\n\t\t\tif (ret) {\n\t\t\t\top = list_last_op(&bind_job->ops);\n\t\t\t\tgoto unwind;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tswitch (op->op) {\n\t\tcase OP_UNMAP_SPARSE:\n\t\tcase OP_MAP:\n\t\tcase OP_UNMAP:\n\t\t\tbind_link_gpuvas(op->ops, &op->new);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\tnouveau_uvmm_unlock(uvmm);\n\n\tspin_lock(&entity->job.list.lock);\n\tlist_add(&bind_job->entry, &entity->job.list.head);\n\tspin_unlock(&entity->job.list.lock);\n\n\treturn 0;\n\nunwind_continue:\n\top = list_prev_op(op);\nunwind:\n\tlist_for_each_op_from_reverse(op, &bind_job->ops) {\n\t\tswitch (op->op) {\n\t\tcase OP_MAP_SPARSE:\n\t\t\tnouveau_uvma_region_destroy(uvmm, op->va.addr,\n\t\t\t\t\t\t    op->va.range);\n\t\t\tbreak;\n\t\tcase OP_UNMAP_SPARSE:\n\t\t\t__nouveau_uvma_region_insert(uvmm, op->reg);\n\t\t\tnouveau_uvmm_sm_unmap_prepare_unwind(uvmm, &op->new,\n\t\t\t\t\t\t\t     op->ops);\n\t\t\tbreak;\n\t\tcase OP_MAP:\n\t\t\tnouveau_uvmm_sm_map_prepare_unwind(uvmm, &op->new,\n\t\t\t\t\t\t\t   op->ops,\n\t\t\t\t\t\t\t   op->va.addr,\n\t\t\t\t\t\t\t   op->va.range);\n\t\t\tbreak;\n\t\tcase OP_UNMAP:\n\t\t\tnouveau_uvmm_sm_unmap_prepare_unwind(uvmm, &op->new,\n\t\t\t\t\t\t\t     op->ops);\n\t\t\tbreak;\n\t\t}\n\n\t\tdrm_gpuva_ops_free(&uvmm->umgr, op->ops);\n\t\top->ops = NULL;\n\t\top->reg = NULL;\n\t}\n\n\tnouveau_uvmm_unlock(uvmm);\n\tdrm_exec_fini(exec);\n\treturn ret;\n}\n\nstatic void\nnouveau_uvmm_bind_job_armed_submit(struct nouveau_job *job)\n{\n\tstruct drm_exec *exec = &job->exec;\n\tstruct drm_gem_object *obj;\n\tunsigned long index;\n\n\tdrm_exec_for_each_locked_object(exec, index, obj)\n\t\tdma_resv_add_fence(obj->resv, job->done_fence, job->resv_usage);\n\n\tdrm_exec_fini(exec);\n}\n\nstatic struct dma_fence *\nnouveau_uvmm_bind_job_run(struct nouveau_job *job)\n{\n\tstruct nouveau_uvmm_bind_job *bind_job = to_uvmm_bind_job(job);\n\tstruct nouveau_uvmm *uvmm = nouveau_cli_uvmm(job->cli);\n\tstruct bind_job_op *op;\n\tint ret = 0;\n\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tswitch (op->op) {\n\t\tcase OP_MAP_SPARSE:\n\t\t\t \n\t\t\tbreak;\n\t\tcase OP_MAP:\n\t\t\tret = nouveau_uvmm_sm_map(uvmm, &op->new, op->ops);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase OP_UNMAP_SPARSE:\n\t\t\tfallthrough;\n\t\tcase OP_UNMAP:\n\t\t\tret = nouveau_uvmm_sm_unmap(uvmm, &op->new, op->ops);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (ret)\n\t\tNV_PRINTK(err, job->cli, \"bind job failed: %d\\n\", ret);\n\treturn ERR_PTR(ret);\n}\n\nstatic void\nnouveau_uvmm_bind_job_free_work_fn(struct work_struct *work)\n{\n\tstruct nouveau_uvmm_bind_job *bind_job =\n\t\tcontainer_of(work, struct nouveau_uvmm_bind_job, work);\n\tstruct nouveau_job *job = &bind_job->base;\n\tstruct nouveau_uvmm *uvmm = nouveau_cli_uvmm(job->cli);\n\tstruct nouveau_sched_entity *entity = job->entity;\n\tstruct bind_job_op *op, *next;\n\n\tlist_for_each_op(op, &bind_job->ops) {\n\t\tstruct drm_gem_object *obj = op->gem.obj;\n\n\t\t \n\t\tswitch (op->op) {\n\t\tcase OP_MAP_SPARSE:\n\t\t\t \n\t\t\tbreak;\n\t\tcase OP_UNMAP_SPARSE:\n\t\t\tif (!IS_ERR_OR_NULL(op->ops))\n\t\t\t\tnouveau_uvmm_sm_unmap_cleanup(uvmm, &op->new,\n\t\t\t\t\t\t\t      op->ops);\n\n\t\t\tif (op->reg) {\n\t\t\t\tnouveau_uvma_region_sparse_unref(op->reg);\n\t\t\t\tnouveau_uvmm_lock(uvmm);\n\t\t\t\tnouveau_uvma_region_remove(op->reg);\n\t\t\t\tnouveau_uvmm_unlock(uvmm);\n\t\t\t\tnouveau_uvma_region_complete(op->reg);\n\t\t\t\tnouveau_uvma_region_put(op->reg);\n\t\t\t}\n\n\t\t\tbreak;\n\t\tcase OP_MAP:\n\t\t\tif (!IS_ERR_OR_NULL(op->ops))\n\t\t\t\tnouveau_uvmm_sm_map_cleanup(uvmm, &op->new,\n\t\t\t\t\t\t\t    op->ops);\n\t\t\tbreak;\n\t\tcase OP_UNMAP:\n\t\t\tif (!IS_ERR_OR_NULL(op->ops))\n\t\t\t\tnouveau_uvmm_sm_unmap_cleanup(uvmm, &op->new,\n\t\t\t\t\t\t\t      op->ops);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!IS_ERR_OR_NULL(op->ops))\n\t\t\tdrm_gpuva_ops_free(&uvmm->umgr, op->ops);\n\n\t\tif (obj)\n\t\t\tdrm_gem_object_put(obj);\n\t}\n\n\tspin_lock(&entity->job.list.lock);\n\tlist_del(&bind_job->entry);\n\tspin_unlock(&entity->job.list.lock);\n\n\tcomplete_all(&bind_job->complete);\n\twake_up(&entity->job.wq);\n\n\t \n\tlist_for_each_op_safe(op, next, &bind_job->ops) {\n\t\tlist_del(&op->entry);\n\t\tkfree(op);\n\t}\n\n\tnouveau_uvmm_bind_job_put(bind_job);\n}\n\nstatic void\nnouveau_uvmm_bind_job_free_qwork(struct nouveau_job *job)\n{\n\tstruct nouveau_uvmm_bind_job *bind_job = to_uvmm_bind_job(job);\n\tstruct nouveau_sched_entity *entity = job->entity;\n\n\tnouveau_sched_entity_qwork(entity, &bind_job->work);\n}\n\nstatic struct nouveau_job_ops nouveau_bind_job_ops = {\n\t.submit = nouveau_uvmm_bind_job_submit,\n\t.armed_submit = nouveau_uvmm_bind_job_armed_submit,\n\t.run = nouveau_uvmm_bind_job_run,\n\t.free = nouveau_uvmm_bind_job_free_qwork,\n};\n\nstatic int\nbind_job_op_from_uop(struct bind_job_op **pop,\n\t\t     struct drm_nouveau_vm_bind_op *uop)\n{\n\tstruct bind_job_op *op;\n\n\top = *pop = kzalloc(sizeof(*op), GFP_KERNEL);\n\tif (!op)\n\t\treturn -ENOMEM;\n\n\tswitch (uop->op) {\n\tcase OP_MAP:\n\t\top->op = uop->flags & DRM_NOUVEAU_VM_BIND_SPARSE ?\n\t\t\t OP_MAP_SPARSE : OP_MAP;\n\t\tbreak;\n\tcase OP_UNMAP:\n\t\top->op = uop->flags & DRM_NOUVEAU_VM_BIND_SPARSE ?\n\t\t\t OP_UNMAP_SPARSE : OP_UNMAP;\n\t\tbreak;\n\tdefault:\n\t\top->op = uop->op;\n\t\tbreak;\n\t}\n\n\top->flags = uop->flags;\n\top->va.addr = uop->addr;\n\top->va.range = uop->range;\n\top->gem.handle = uop->handle;\n\top->gem.offset = uop->bo_offset;\n\n\treturn 0;\n}\n\nstatic void\nbind_job_ops_free(struct list_head *ops)\n{\n\tstruct bind_job_op *op, *next;\n\n\tlist_for_each_op_safe(op, next, ops) {\n\t\tlist_del(&op->entry);\n\t\tkfree(op);\n\t}\n}\n\nstatic int\nnouveau_uvmm_bind_job_init(struct nouveau_uvmm_bind_job **pjob,\n\t\t\t   struct nouveau_uvmm_bind_job_args *__args)\n{\n\tstruct nouveau_uvmm_bind_job *job;\n\tstruct nouveau_job_args args = {};\n\tstruct bind_job_op *op;\n\tint i, ret;\n\n\tret = nouveau_uvmm_bind_job_alloc(&job);\n\tif (ret)\n\t\treturn ret;\n\n\tINIT_LIST_HEAD(&job->ops);\n\tINIT_LIST_HEAD(&job->entry);\n\n\tfor (i = 0; i < __args->op.count; i++) {\n\t\tret = bind_job_op_from_uop(&op, &__args->op.s[i]);\n\t\tif (ret)\n\t\t\tgoto err_free;\n\n\t\tlist_add_tail(&op->entry, &job->ops);\n\t}\n\n\tinit_completion(&job->complete);\n\tINIT_WORK(&job->work, nouveau_uvmm_bind_job_free_work_fn);\n\n\targs.sched_entity = __args->sched_entity;\n\targs.file_priv = __args->file_priv;\n\n\targs.in_sync.count = __args->in_sync.count;\n\targs.in_sync.s = __args->in_sync.s;\n\n\targs.out_sync.count = __args->out_sync.count;\n\targs.out_sync.s = __args->out_sync.s;\n\n\targs.sync = !(__args->flags & DRM_NOUVEAU_VM_BIND_RUN_ASYNC);\n\targs.ops = &nouveau_bind_job_ops;\n\targs.resv_usage = DMA_RESV_USAGE_BOOKKEEP;\n\n\tret = nouveau_job_init(&job->base, &args);\n\tif (ret)\n\t\tgoto err_free;\n\n\t*pjob = job;\n\treturn 0;\n\nerr_free:\n\tbind_job_ops_free(&job->ops);\n\tkfree(job);\n\t*pjob = NULL;\n\n\treturn ret;\n}\n\nint\nnouveau_uvmm_ioctl_vm_init(struct drm_device *dev,\n\t\t\t   void *data,\n\t\t\t   struct drm_file *file_priv)\n{\n\tstruct nouveau_cli *cli = nouveau_cli(file_priv);\n\tstruct drm_nouveau_vm_init *init = data;\n\n\treturn nouveau_uvmm_init(&cli->uvmm, cli, init->kernel_managed_addr,\n\t\t\t\t init->kernel_managed_size);\n}\n\nstatic int\nnouveau_uvmm_vm_bind(struct nouveau_uvmm_bind_job_args *args)\n{\n\tstruct nouveau_uvmm_bind_job *job;\n\tint ret;\n\n\tret = nouveau_uvmm_bind_job_init(&job, args);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nouveau_job_submit(&job->base);\n\tif (ret)\n\t\tgoto err_job_fini;\n\n\treturn 0;\n\nerr_job_fini:\n\tnouveau_job_fini(&job->base);\n\treturn ret;\n}\n\nstatic int\nnouveau_uvmm_vm_bind_ucopy(struct nouveau_uvmm_bind_job_args *args,\n\t\t\t   struct drm_nouveau_vm_bind *req)\n{\n\tstruct drm_nouveau_sync **s;\n\tu32 inc = req->wait_count;\n\tu64 ins = req->wait_ptr;\n\tu32 outc = req->sig_count;\n\tu64 outs = req->sig_ptr;\n\tu32 opc = req->op_count;\n\tu64 ops = req->op_ptr;\n\tint ret;\n\n\targs->flags = req->flags;\n\n\tif (opc) {\n\t\targs->op.count = opc;\n\t\targs->op.s = u_memcpya(ops, opc,\n\t\t\t\t       sizeof(*args->op.s));\n\t\tif (IS_ERR(args->op.s))\n\t\t\treturn PTR_ERR(args->op.s);\n\t}\n\n\tif (inc) {\n\t\ts = &args->in_sync.s;\n\n\t\targs->in_sync.count = inc;\n\t\t*s = u_memcpya(ins, inc, sizeof(**s));\n\t\tif (IS_ERR(*s)) {\n\t\t\tret = PTR_ERR(*s);\n\t\t\tgoto err_free_ops;\n\t\t}\n\t}\n\n\tif (outc) {\n\t\ts = &args->out_sync.s;\n\n\t\targs->out_sync.count = outc;\n\t\t*s = u_memcpya(outs, outc, sizeof(**s));\n\t\tif (IS_ERR(*s)) {\n\t\t\tret = PTR_ERR(*s);\n\t\t\tgoto err_free_ins;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_free_ops:\n\tu_free(args->op.s);\nerr_free_ins:\n\tu_free(args->in_sync.s);\n\treturn ret;\n}\n\nstatic void\nnouveau_uvmm_vm_bind_ufree(struct nouveau_uvmm_bind_job_args *args)\n{\n\tu_free(args->op.s);\n\tu_free(args->in_sync.s);\n\tu_free(args->out_sync.s);\n}\n\nint\nnouveau_uvmm_ioctl_vm_bind(struct drm_device *dev,\n\t\t\t   void *data,\n\t\t\t   struct drm_file *file_priv)\n{\n\tstruct nouveau_cli *cli = nouveau_cli(file_priv);\n\tstruct nouveau_uvmm_bind_job_args args = {};\n\tstruct drm_nouveau_vm_bind *req = data;\n\tint ret = 0;\n\n\tif (unlikely(!nouveau_cli_uvmm_locked(cli)))\n\t\treturn -ENOSYS;\n\n\tret = nouveau_uvmm_vm_bind_ucopy(&args, req);\n\tif (ret)\n\t\treturn ret;\n\n\targs.sched_entity = &cli->sched_entity;\n\targs.file_priv = file_priv;\n\n\tret = nouveau_uvmm_vm_bind(&args);\n\tif (ret)\n\t\tgoto out_free_args;\n\nout_free_args:\n\tnouveau_uvmm_vm_bind_ufree(&args);\n\treturn ret;\n}\n\nvoid\nnouveau_uvmm_bo_map_all(struct nouveau_bo *nvbo, struct nouveau_mem *mem)\n{\n\tstruct drm_gem_object *obj = &nvbo->bo.base;\n\tstruct drm_gpuva *va;\n\n\tdma_resv_assert_held(obj->resv);\n\n\tdrm_gem_for_each_gpuva(va, obj) {\n\t\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\n\t\tnouveau_uvma_map(uvma, mem);\n\t\tdrm_gpuva_invalidate(va, false);\n\t}\n}\n\nvoid\nnouveau_uvmm_bo_unmap_all(struct nouveau_bo *nvbo)\n{\n\tstruct drm_gem_object *obj = &nvbo->bo.base;\n\tstruct drm_gpuva *va;\n\n\tdma_resv_assert_held(obj->resv);\n\n\tdrm_gem_for_each_gpuva(va, obj) {\n\t\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\n\t\tnouveau_uvma_unmap(uvma);\n\t\tdrm_gpuva_invalidate(va, true);\n\t}\n}\n\nint\nnouveau_uvmm_init(struct nouveau_uvmm *uvmm, struct nouveau_cli *cli,\n\t\t  u64 kernel_managed_addr, u64 kernel_managed_size)\n{\n\tint ret;\n\tu64 kernel_managed_end = kernel_managed_addr + kernel_managed_size;\n\n\tmutex_init(&uvmm->mutex);\n\tdma_resv_init(&uvmm->resv);\n\tmt_init_flags(&uvmm->region_mt, MT_FLAGS_LOCK_EXTERN);\n\tmt_set_external_lock(&uvmm->region_mt, &uvmm->mutex);\n\n\tmutex_lock(&cli->mutex);\n\n\tif (unlikely(cli->uvmm.disabled)) {\n\t\tret = -ENOSYS;\n\t\tgoto out_unlock;\n\t}\n\n\tif (kernel_managed_end <= kernel_managed_addr) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (kernel_managed_end > NOUVEAU_VA_SPACE_END) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tuvmm->kernel_managed_addr = kernel_managed_addr;\n\tuvmm->kernel_managed_size = kernel_managed_size;\n\n\tdrm_gpuva_manager_init(&uvmm->umgr, cli->name,\n\t\t\t       NOUVEAU_VA_SPACE_START,\n\t\t\t       NOUVEAU_VA_SPACE_END,\n\t\t\t       kernel_managed_addr, kernel_managed_size,\n\t\t\t       NULL);\n\n\tret = nvif_vmm_ctor(&cli->mmu, \"uvmm\",\n\t\t\t    cli->vmm.vmm.object.oclass, RAW,\n\t\t\t    kernel_managed_addr, kernel_managed_size,\n\t\t\t    NULL, 0, &cli->uvmm.vmm.vmm);\n\tif (ret)\n\t\tgoto out_free_gpuva_mgr;\n\n\tcli->uvmm.vmm.cli = cli;\n\tmutex_unlock(&cli->mutex);\n\n\treturn 0;\n\nout_free_gpuva_mgr:\n\tdrm_gpuva_manager_destroy(&uvmm->umgr);\nout_unlock:\n\tmutex_unlock(&cli->mutex);\n\treturn ret;\n}\n\nvoid\nnouveau_uvmm_fini(struct nouveau_uvmm *uvmm)\n{\n\tMA_STATE(mas, &uvmm->region_mt, 0, 0);\n\tstruct nouveau_uvma_region *reg;\n\tstruct nouveau_cli *cli = uvmm->vmm.cli;\n\tstruct nouveau_sched_entity *entity = &cli->sched_entity;\n\tstruct drm_gpuva *va, *next;\n\n\tif (!cli)\n\t\treturn;\n\n\trmb();  \n\twait_event(entity->job.wq, list_empty(&entity->job.list.head));\n\n\tnouveau_uvmm_lock(uvmm);\n\tdrm_gpuva_for_each_va_safe(va, next, &uvmm->umgr) {\n\t\tstruct nouveau_uvma *uvma = uvma_from_va(va);\n\t\tstruct drm_gem_object *obj = va->gem.obj;\n\n\t\tif (unlikely(va == &uvmm->umgr.kernel_alloc_node))\n\t\t\tcontinue;\n\n\t\tdrm_gpuva_remove(va);\n\n\t\tdma_resv_lock(obj->resv, NULL);\n\t\tdrm_gpuva_unlink(va);\n\t\tdma_resv_unlock(obj->resv);\n\n\t\tnouveau_uvma_unmap(uvma);\n\t\tnouveau_uvma_vmm_put(uvma);\n\n\t\tnouveau_uvma_gem_put(uvma);\n\t\tnouveau_uvma_free(uvma);\n\t}\n\n\tmas_for_each(&mas, reg, ULONG_MAX) {\n\t\tmas_erase(&mas);\n\t\tnouveau_uvma_region_sparse_unref(reg);\n\t\tnouveau_uvma_region_put(reg);\n\t}\n\n\tWARN(!mtree_empty(&uvmm->region_mt),\n\t     \"nouveau_uvma_region tree not empty, potentially leaking memory.\");\n\t__mt_destroy(&uvmm->region_mt);\n\tnouveau_uvmm_unlock(uvmm);\n\n\tmutex_lock(&cli->mutex);\n\tnouveau_vmm_fini(&uvmm->vmm);\n\tdrm_gpuva_manager_destroy(&uvmm->umgr);\n\tmutex_unlock(&cli->mutex);\n\n\tdma_resv_fini(&uvmm->resv);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}