{
  "module_name": "vc4_bo.c",
  "hash_id": "95592f28e90ca2ccf7ebdad048901f33e18c5c9ef514ef2c284cb1aa6d1f7a63",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/vc4/vc4_bo.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/dma-buf.h>\n\n#include <drm/drm_fourcc.h>\n\n#include \"vc4_drv.h\"\n#include \"uapi/drm/vc4_drm.h\"\n\nstatic const struct drm_gem_object_funcs vc4_gem_object_funcs;\n\nstatic const char * const bo_type_names[] = {\n\t\"kernel\",\n\t\"V3D\",\n\t\"V3D shader\",\n\t\"dumb\",\n\t\"binner\",\n\t\"RCL\",\n\t\"BCL\",\n\t\"kernel BO cache\",\n};\n\nstatic bool is_user_label(int label)\n{\n\treturn label >= VC4_BO_TYPE_COUNT;\n}\n\nstatic void vc4_bo_stats_print(struct drm_printer *p, struct vc4_dev *vc4)\n{\n\tint i;\n\n\tfor (i = 0; i < vc4->num_labels; i++) {\n\t\tif (!vc4->bo_labels[i].num_allocated)\n\t\t\tcontinue;\n\n\t\tdrm_printf(p, \"%30s: %6dkb BOs (%d)\\n\",\n\t\t\t   vc4->bo_labels[i].name,\n\t\t\t   vc4->bo_labels[i].size_allocated / 1024,\n\t\t\t   vc4->bo_labels[i].num_allocated);\n\t}\n\n\tmutex_lock(&vc4->purgeable.lock);\n\tif (vc4->purgeable.num)\n\t\tdrm_printf(p, \"%30s: %6zdkb BOs (%d)\\n\", \"userspace BO cache\",\n\t\t\t   vc4->purgeable.size / 1024, vc4->purgeable.num);\n\n\tif (vc4->purgeable.purged_num)\n\t\tdrm_printf(p, \"%30s: %6zdkb BOs (%d)\\n\", \"total purged BO\",\n\t\t\t   vc4->purgeable.purged_size / 1024,\n\t\t\t   vc4->purgeable.purged_num);\n\tmutex_unlock(&vc4->purgeable.lock);\n}\n\nstatic int vc4_bo_stats_debugfs(struct seq_file *m, void *unused)\n{\n\tstruct drm_debugfs_entry *entry = m->private;\n\tstruct drm_device *dev = entry->dev;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_printer p = drm_seq_file_printer(m);\n\n\tvc4_bo_stats_print(&p, vc4);\n\n\treturn 0;\n}\n\n \nstatic int vc4_get_user_label(struct vc4_dev *vc4, const char *name)\n{\n\tint i;\n\tint free_slot = -1;\n\n\tfor (i = 0; i < vc4->num_labels; i++) {\n\t\tif (!vc4->bo_labels[i].name) {\n\t\t\tfree_slot = i;\n\t\t} else if (strcmp(vc4->bo_labels[i].name, name) == 0) {\n\t\t\tkfree(name);\n\t\t\treturn i;\n\t\t}\n\t}\n\n\tif (free_slot != -1) {\n\t\tWARN_ON(vc4->bo_labels[free_slot].num_allocated != 0);\n\t\tvc4->bo_labels[free_slot].name = name;\n\t\treturn free_slot;\n\t} else {\n\t\tu32 new_label_count = vc4->num_labels + 1;\n\t\tstruct vc4_label *new_labels =\n\t\t\tkrealloc(vc4->bo_labels,\n\t\t\t\t new_label_count * sizeof(*new_labels),\n\t\t\t\t GFP_KERNEL);\n\n\t\tif (!new_labels) {\n\t\t\tkfree(name);\n\t\t\treturn -1;\n\t\t}\n\n\t\tfree_slot = vc4->num_labels;\n\t\tvc4->bo_labels = new_labels;\n\t\tvc4->num_labels = new_label_count;\n\n\t\tvc4->bo_labels[free_slot].name = name;\n\t\tvc4->bo_labels[free_slot].num_allocated = 0;\n\t\tvc4->bo_labels[free_slot].size_allocated = 0;\n\n\t\treturn free_slot;\n\t}\n}\n\nstatic void vc4_bo_set_label(struct drm_gem_object *gem_obj, int label)\n{\n\tstruct vc4_bo *bo = to_vc4_bo(gem_obj);\n\tstruct vc4_dev *vc4 = to_vc4_dev(gem_obj->dev);\n\n\tlockdep_assert_held(&vc4->bo_lock);\n\n\tif (label != -1) {\n\t\tvc4->bo_labels[label].num_allocated++;\n\t\tvc4->bo_labels[label].size_allocated += gem_obj->size;\n\t}\n\n\tvc4->bo_labels[bo->label].num_allocated--;\n\tvc4->bo_labels[bo->label].size_allocated -= gem_obj->size;\n\n\tif (vc4->bo_labels[bo->label].num_allocated == 0 &&\n\t    is_user_label(bo->label)) {\n\t\t \n\t\tkfree(vc4->bo_labels[bo->label].name);\n\t\tvc4->bo_labels[bo->label].name = NULL;\n\t}\n\n\tbo->label = label;\n}\n\nstatic uint32_t bo_page_index(size_t size)\n{\n\treturn (size / PAGE_SIZE) - 1;\n}\n\nstatic void vc4_bo_destroy(struct vc4_bo *bo)\n{\n\tstruct drm_gem_object *obj = &bo->base.base;\n\tstruct vc4_dev *vc4 = to_vc4_dev(obj->dev);\n\n\tlockdep_assert_held(&vc4->bo_lock);\n\n\tvc4_bo_set_label(obj, -1);\n\n\tif (bo->validated_shader) {\n\t\tkfree(bo->validated_shader->uniform_addr_offsets);\n\t\tkfree(bo->validated_shader->texture_samples);\n\t\tkfree(bo->validated_shader);\n\t\tbo->validated_shader = NULL;\n\t}\n\n\tmutex_destroy(&bo->madv_lock);\n\tdrm_gem_dma_free(&bo->base);\n}\n\nstatic void vc4_bo_remove_from_cache(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\n\tlockdep_assert_held(&vc4->bo_lock);\n\tlist_del(&bo->unref_head);\n\tlist_del(&bo->size_head);\n}\n\nstatic struct list_head *vc4_get_cache_list_for_size(struct drm_device *dev,\n\t\t\t\t\t\t     size_t size)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint32_t page_index = bo_page_index(size);\n\n\tif (vc4->bo_cache.size_list_size <= page_index) {\n\t\tuint32_t new_size = max(vc4->bo_cache.size_list_size * 2,\n\t\t\t\t\tpage_index + 1);\n\t\tstruct list_head *new_list;\n\t\tuint32_t i;\n\n\t\tnew_list = kmalloc_array(new_size, sizeof(struct list_head),\n\t\t\t\t\t GFP_KERNEL);\n\t\tif (!new_list)\n\t\t\treturn NULL;\n\n\t\t \n\t\tfor (i = 0; i < vc4->bo_cache.size_list_size; i++) {\n\t\t\tstruct list_head *old_list =\n\t\t\t\t&vc4->bo_cache.size_list[i];\n\n\t\t\tif (list_empty(old_list))\n\t\t\t\tINIT_LIST_HEAD(&new_list[i]);\n\t\t\telse\n\t\t\t\tlist_replace(old_list, &new_list[i]);\n\t\t}\n\t\t \n\t\tfor (i = vc4->bo_cache.size_list_size; i < new_size; i++)\n\t\t\tINIT_LIST_HEAD(&new_list[i]);\n\n\t\tkfree(vc4->bo_cache.size_list);\n\t\tvc4->bo_cache.size_list = new_list;\n\t\tvc4->bo_cache.size_list_size = new_size;\n\t}\n\n\treturn &vc4->bo_cache.size_list[page_index];\n}\n\nstatic void vc4_bo_cache_purge(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tmutex_lock(&vc4->bo_lock);\n\twhile (!list_empty(&vc4->bo_cache.time_list)) {\n\t\tstruct vc4_bo *bo = list_last_entry(&vc4->bo_cache.time_list,\n\t\t\t\t\t\t    struct vc4_bo, unref_head);\n\t\tvc4_bo_remove_from_cache(bo);\n\t\tvc4_bo_destroy(bo);\n\t}\n\tmutex_unlock(&vc4->bo_lock);\n}\n\nvoid vc4_bo_add_to_purgeable_pool(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\tmutex_lock(&vc4->purgeable.lock);\n\tlist_add_tail(&bo->size_head, &vc4->purgeable.list);\n\tvc4->purgeable.num++;\n\tvc4->purgeable.size += bo->base.base.size;\n\tmutex_unlock(&vc4->purgeable.lock);\n}\n\nstatic void vc4_bo_remove_from_purgeable_pool_locked(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\t \n\tlist_del_init(&bo->size_head);\n\tvc4->purgeable.num--;\n\tvc4->purgeable.size -= bo->base.base.size;\n}\n\nvoid vc4_bo_remove_from_purgeable_pool(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\n\tmutex_lock(&vc4->purgeable.lock);\n\tvc4_bo_remove_from_purgeable_pool_locked(bo);\n\tmutex_unlock(&vc4->purgeable.lock);\n}\n\nstatic void vc4_bo_purge(struct drm_gem_object *obj)\n{\n\tstruct vc4_bo *bo = to_vc4_bo(obj);\n\tstruct drm_device *dev = obj->dev;\n\n\tWARN_ON(!mutex_is_locked(&bo->madv_lock));\n\tWARN_ON(bo->madv != VC4_MADV_DONTNEED);\n\n\tdrm_vma_node_unmap(&obj->vma_node, dev->anon_inode->i_mapping);\n\n\tdma_free_wc(dev->dev, obj->size, bo->base.vaddr, bo->base.dma_addr);\n\tbo->base.vaddr = NULL;\n\tbo->madv = __VC4_MADV_PURGED;\n}\n\nstatic void vc4_bo_userspace_cache_purge(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tmutex_lock(&vc4->purgeable.lock);\n\twhile (!list_empty(&vc4->purgeable.list)) {\n\t\tstruct vc4_bo *bo = list_first_entry(&vc4->purgeable.list,\n\t\t\t\t\t\t     struct vc4_bo, size_head);\n\t\tstruct drm_gem_object *obj = &bo->base.base;\n\t\tsize_t purged_size = 0;\n\n\t\tvc4_bo_remove_from_purgeable_pool_locked(bo);\n\n\t\t \n\t\tmutex_unlock(&vc4->purgeable.lock);\n\t\tmutex_lock(&bo->madv_lock);\n\n\t\t \n\t\tif (bo->madv == VC4_MADV_DONTNEED &&\n\t\t    list_empty(&bo->size_head) &&\n\t\t    !refcount_read(&bo->usecnt)) {\n\t\t\tpurged_size = bo->base.base.size;\n\t\t\tvc4_bo_purge(obj);\n\t\t}\n\t\tmutex_unlock(&bo->madv_lock);\n\t\tmutex_lock(&vc4->purgeable.lock);\n\n\t\tif (purged_size) {\n\t\t\tvc4->purgeable.purged_size += purged_size;\n\t\t\tvc4->purgeable.purged_num++;\n\t\t}\n\t}\n\tmutex_unlock(&vc4->purgeable.lock);\n}\n\nstatic struct vc4_bo *vc4_bo_get_from_cache(struct drm_device *dev,\n\t\t\t\t\t    uint32_t size,\n\t\t\t\t\t    enum vc4_kernel_bo_type type)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tuint32_t page_index = bo_page_index(size);\n\tstruct vc4_bo *bo = NULL;\n\n\tmutex_lock(&vc4->bo_lock);\n\tif (page_index >= vc4->bo_cache.size_list_size)\n\t\tgoto out;\n\n\tif (list_empty(&vc4->bo_cache.size_list[page_index]))\n\t\tgoto out;\n\n\tbo = list_first_entry(&vc4->bo_cache.size_list[page_index],\n\t\t\t      struct vc4_bo, size_head);\n\tvc4_bo_remove_from_cache(bo);\n\tkref_init(&bo->base.base.refcount);\n\nout:\n\tif (bo)\n\t\tvc4_bo_set_label(&bo->base.base, type);\n\tmutex_unlock(&vc4->bo_lock);\n\treturn bo;\n}\n\n \nstruct drm_gem_object *vc4_create_object(struct drm_device *dev, size_t size)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_bo *bo;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tbo = kzalloc(sizeof(*bo), GFP_KERNEL);\n\tif (!bo)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbo->madv = VC4_MADV_WILLNEED;\n\trefcount_set(&bo->usecnt, 0);\n\n\tmutex_init(&bo->madv_lock);\n\n\tmutex_lock(&vc4->bo_lock);\n\tbo->label = VC4_BO_TYPE_KERNEL;\n\tvc4->bo_labels[VC4_BO_TYPE_KERNEL].num_allocated++;\n\tvc4->bo_labels[VC4_BO_TYPE_KERNEL].size_allocated += size;\n\tmutex_unlock(&vc4->bo_lock);\n\n\tbo->base.base.funcs = &vc4_gem_object_funcs;\n\n\treturn &bo->base.base;\n}\n\nstruct vc4_bo *vc4_bo_create(struct drm_device *dev, size_t unaligned_size,\n\t\t\t     bool allow_unzeroed, enum vc4_kernel_bo_type type)\n{\n\tsize_t size = roundup(unaligned_size, PAGE_SIZE);\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_gem_dma_object *dma_obj;\n\tstruct vc4_bo *bo;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tif (size == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tbo = vc4_bo_get_from_cache(dev, size, type);\n\tif (bo) {\n\t\tif (!allow_unzeroed)\n\t\t\tmemset(bo->base.vaddr, 0, bo->base.base.size);\n\t\treturn bo;\n\t}\n\n\tdma_obj = drm_gem_dma_create(dev, size);\n\tif (IS_ERR(dma_obj)) {\n\t\t \n\t\tvc4_bo_cache_purge(dev);\n\t\tdma_obj = drm_gem_dma_create(dev, size);\n\t}\n\n\tif (IS_ERR(dma_obj)) {\n\t\t \n\t\tvc4_bo_userspace_cache_purge(dev);\n\t\tdma_obj = drm_gem_dma_create(dev, size);\n\t}\n\n\tif (IS_ERR(dma_obj)) {\n\t\tstruct drm_printer p = drm_info_printer(vc4->base.dev);\n\t\tDRM_ERROR(\"Failed to allocate from GEM DMA helper:\\n\");\n\t\tvc4_bo_stats_print(&p, vc4);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tbo = to_vc4_bo(&dma_obj->base);\n\n\t \n\tbo->madv = __VC4_MADV_NOTSUPP;\n\n\tmutex_lock(&vc4->bo_lock);\n\tvc4_bo_set_label(&dma_obj->base, type);\n\tmutex_unlock(&vc4->bo_lock);\n\n\treturn bo;\n}\n\nint vc4_bo_dumb_create(struct drm_file *file_priv,\n\t\t       struct drm_device *dev,\n\t\t       struct drm_mode_create_dumb *args)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_bo *bo = NULL;\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tret = vc4_dumb_fixup_args(args);\n\tif (ret)\n\t\treturn ret;\n\n\tbo = vc4_bo_create(dev, args->size, false, VC4_BO_TYPE_DUMB);\n\tif (IS_ERR(bo))\n\t\treturn PTR_ERR(bo);\n\n\tbo->madv = VC4_MADV_WILLNEED;\n\n\tret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\n\tdrm_gem_object_put(&bo->base.base);\n\n\treturn ret;\n}\n\nstatic void vc4_bo_cache_free_old(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long expire_time = jiffies - msecs_to_jiffies(1000);\n\n\tlockdep_assert_held(&vc4->bo_lock);\n\n\twhile (!list_empty(&vc4->bo_cache.time_list)) {\n\t\tstruct vc4_bo *bo = list_last_entry(&vc4->bo_cache.time_list,\n\t\t\t\t\t\t    struct vc4_bo, unref_head);\n\t\tif (time_before(expire_time, bo->free_time)) {\n\t\t\tmod_timer(&vc4->bo_cache.time_timer,\n\t\t\t\t  round_jiffies_up(jiffies +\n\t\t\t\t\t\t   msecs_to_jiffies(1000)));\n\t\t\treturn;\n\t\t}\n\n\t\tvc4_bo_remove_from_cache(bo);\n\t\tvc4_bo_destroy(bo);\n\t}\n}\n\n \nstatic void vc4_free_object(struct drm_gem_object *gem_bo)\n{\n\tstruct drm_device *dev = gem_bo->dev;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_bo *bo = to_vc4_bo(gem_bo);\n\tstruct list_head *cache_list;\n\n\t \n\tmutex_lock(&bo->madv_lock);\n\tif (bo->madv == VC4_MADV_DONTNEED && !refcount_read(&bo->usecnt))\n\t\tvc4_bo_remove_from_purgeable_pool(bo);\n\tmutex_unlock(&bo->madv_lock);\n\n\tmutex_lock(&vc4->bo_lock);\n\t \n\tif (gem_bo->import_attach) {\n\t\tvc4_bo_destroy(bo);\n\t\tgoto out;\n\t}\n\n\t \n\tif (gem_bo->name) {\n\t\tvc4_bo_destroy(bo);\n\t\tgoto out;\n\t}\n\n\t \n\tif (!bo->base.vaddr) {\n\t\tvc4_bo_destroy(bo);\n\t\tgoto out;\n\t}\n\n\tcache_list = vc4_get_cache_list_for_size(dev, gem_bo->size);\n\tif (!cache_list) {\n\t\tvc4_bo_destroy(bo);\n\t\tgoto out;\n\t}\n\n\tif (bo->validated_shader) {\n\t\tkfree(bo->validated_shader->uniform_addr_offsets);\n\t\tkfree(bo->validated_shader->texture_samples);\n\t\tkfree(bo->validated_shader);\n\t\tbo->validated_shader = NULL;\n\t}\n\n\t \n\tbo->madv = __VC4_MADV_NOTSUPP;\n\trefcount_set(&bo->usecnt, 0);\n\n\tbo->t_format = false;\n\tbo->free_time = jiffies;\n\tlist_add(&bo->size_head, cache_list);\n\tlist_add(&bo->unref_head, &vc4->bo_cache.time_list);\n\n\tvc4_bo_set_label(&bo->base.base, VC4_BO_TYPE_KERNEL_CACHE);\n\n\tvc4_bo_cache_free_old(dev);\n\nout:\n\tmutex_unlock(&vc4->bo_lock);\n}\n\nstatic void vc4_bo_cache_time_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, bo_cache.time_work);\n\tstruct drm_device *dev = &vc4->base;\n\n\tmutex_lock(&vc4->bo_lock);\n\tvc4_bo_cache_free_old(dev);\n\tmutex_unlock(&vc4->bo_lock);\n}\n\nint vc4_bo_inc_usecnt(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\t \n\tif (refcount_inc_not_zero(&bo->usecnt))\n\t\treturn 0;\n\n\tmutex_lock(&bo->madv_lock);\n\tswitch (bo->madv) {\n\tcase VC4_MADV_WILLNEED:\n\t\tif (!refcount_inc_not_zero(&bo->usecnt))\n\t\t\trefcount_set(&bo->usecnt, 1);\n\t\tret = 0;\n\t\tbreak;\n\tcase VC4_MADV_DONTNEED:\n\t\t \n\tcase __VC4_MADV_PURGED:\n\t\t \n\tdefault:\n\t\t \n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\tmutex_unlock(&bo->madv_lock);\n\n\treturn ret;\n}\n\nvoid vc4_bo_dec_usecnt(struct vc4_bo *bo)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(bo->base.base.dev);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\t \n\tif (refcount_dec_not_one(&bo->usecnt))\n\t\treturn;\n\n\tmutex_lock(&bo->madv_lock);\n\tif (refcount_dec_and_test(&bo->usecnt) &&\n\t    bo->madv == VC4_MADV_DONTNEED)\n\t\tvc4_bo_add_to_purgeable_pool(bo);\n\tmutex_unlock(&bo->madv_lock);\n}\n\nstatic void vc4_bo_cache_time_timer(struct timer_list *t)\n{\n\tstruct vc4_dev *vc4 = from_timer(vc4, t, bo_cache.time_timer);\n\n\tschedule_work(&vc4->bo_cache.time_work);\n}\n\nstatic struct dma_buf *vc4_prime_export(struct drm_gem_object *obj, int flags)\n{\n\tstruct vc4_bo *bo = to_vc4_bo(obj);\n\tstruct dma_buf *dmabuf;\n\tint ret;\n\n\tif (bo->validated_shader) {\n\t\tDRM_DEBUG(\"Attempting to export shader BO\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tret = vc4_bo_inc_usecnt(bo);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to increment BO usecnt\\n\");\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tdmabuf = drm_gem_prime_export(obj, flags);\n\tif (IS_ERR(dmabuf))\n\t\tvc4_bo_dec_usecnt(bo);\n\n\treturn dmabuf;\n}\n\nstatic vm_fault_t vc4_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct drm_gem_object *obj = vma->vm_private_data;\n\tstruct vc4_bo *bo = to_vc4_bo(obj);\n\n\t \n\tmutex_lock(&bo->madv_lock);\n\tWARN_ON(bo->madv != __VC4_MADV_PURGED);\n\tmutex_unlock(&bo->madv_lock);\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic int vc4_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct vc4_bo *bo = to_vc4_bo(obj);\n\n\tif (bo->validated_shader && (vma->vm_flags & VM_WRITE)) {\n\t\tDRM_DEBUG(\"mmapping of shader BOs for writing not allowed.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (bo->madv != VC4_MADV_WILLNEED) {\n\t\tDRM_DEBUG(\"mmapping of %s BO not allowed\\n\",\n\t\t\t  bo->madv == VC4_MADV_DONTNEED ?\n\t\t\t  \"purgeable\" : \"purged\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn drm_gem_dma_mmap(&bo->base, vma);\n}\n\nstatic const struct vm_operations_struct vc4_vm_ops = {\n\t.fault = vc4_fault,\n\t.open = drm_gem_vm_open,\n\t.close = drm_gem_vm_close,\n};\n\nstatic const struct drm_gem_object_funcs vc4_gem_object_funcs = {\n\t.free = vc4_free_object,\n\t.export = vc4_prime_export,\n\t.get_sg_table = drm_gem_dma_object_get_sg_table,\n\t.vmap = drm_gem_dma_object_vmap,\n\t.mmap = vc4_gem_object_mmap,\n\t.vm_ops = &vc4_vm_ops,\n};\n\nstatic int vc4_grab_bin_bo(struct vc4_dev *vc4, struct vc4_file *vc4file)\n{\n\tif (!vc4->v3d)\n\t\treturn -ENODEV;\n\n\tif (vc4file->bin_bo_used)\n\t\treturn 0;\n\n\treturn vc4_v3d_bin_bo_get(vc4, &vc4file->bin_bo_used);\n}\n\nint vc4_create_bo_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *file_priv)\n{\n\tstruct drm_vc4_create_bo *args = data;\n\tstruct vc4_file *vc4file = file_priv->driver_priv;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_bo *bo = NULL;\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tret = vc4_grab_bin_bo(vc4, vc4file);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tbo = vc4_bo_create(dev, args->size, false, VC4_BO_TYPE_V3D);\n\tif (IS_ERR(bo))\n\t\treturn PTR_ERR(bo);\n\n\tbo->madv = VC4_MADV_WILLNEED;\n\n\tret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\n\tdrm_gem_object_put(&bo->base.base);\n\n\treturn ret;\n}\n\nint vc4_mmap_bo_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_mmap_bo *args = data;\n\tstruct drm_gem_object *gem_obj;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_DEBUG(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\targs->offset = drm_vma_node_offset_addr(&gem_obj->vma_node);\n\n\tdrm_gem_object_put(gem_obj);\n\treturn 0;\n}\n\nint\nvc4_create_shader_bo_ioctl(struct drm_device *dev, void *data,\n\t\t\t   struct drm_file *file_priv)\n{\n\tstruct drm_vc4_create_shader_bo *args = data;\n\tstruct vc4_file *vc4file = file_priv->driver_priv;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_bo *bo = NULL;\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (args->size == 0)\n\t\treturn -EINVAL;\n\n\tif (args->size % sizeof(u64) != 0)\n\t\treturn -EINVAL;\n\n\tif (args->flags != 0) {\n\t\tDRM_INFO(\"Unknown flags set: 0x%08x\\n\", args->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->pad != 0) {\n\t\tDRM_INFO(\"Pad set: 0x%08x\\n\", args->pad);\n\t\treturn -EINVAL;\n\t}\n\n\tret = vc4_grab_bin_bo(vc4, vc4file);\n\tif (ret)\n\t\treturn ret;\n\n\tbo = vc4_bo_create(dev, args->size, true, VC4_BO_TYPE_V3D_SHADER);\n\tif (IS_ERR(bo))\n\t\treturn PTR_ERR(bo);\n\n\tbo->madv = VC4_MADV_WILLNEED;\n\n\tif (copy_from_user(bo->base.vaddr,\n\t\t\t     (void __user *)(uintptr_t)args->data,\n\t\t\t     args->size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\t \n\tmemset(bo->base.vaddr + args->size, 0,\n\t       bo->base.base.size - args->size);\n\n\tbo->validated_shader = vc4_validate_shader(&bo->base);\n\tif (!bo->validated_shader) {\n\t\tret = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\t \n\tret = drm_gem_handle_create(file_priv, &bo->base.base, &args->handle);\n\nfail:\n\tdrm_gem_object_put(&bo->base.base);\n\n\treturn ret;\n}\n\n \nint vc4_set_tiling_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_set_tiling *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\tbool t_format;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (args->flags != 0)\n\t\treturn -EINVAL;\n\n\tswitch (args->modifier) {\n\tcase DRM_FORMAT_MOD_NONE:\n\t\tt_format = false;\n\t\tbreak;\n\tcase DRM_FORMAT_MOD_BROADCOM_VC4_T_TILED:\n\t\tt_format = true;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_DEBUG(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -ENOENT;\n\t}\n\tbo = to_vc4_bo(gem_obj);\n\tbo->t_format = t_format;\n\n\tdrm_gem_object_put(gem_obj);\n\n\treturn 0;\n}\n\n \nint vc4_get_tiling_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_get_tiling *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (args->flags != 0 || args->modifier != 0)\n\t\treturn -EINVAL;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_DEBUG(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -ENOENT;\n\t}\n\tbo = to_vc4_bo(gem_obj);\n\n\tif (bo->t_format)\n\t\targs->modifier = DRM_FORMAT_MOD_BROADCOM_VC4_T_TILED;\n\telse\n\t\targs->modifier = DRM_FORMAT_MOD_NONE;\n\n\tdrm_gem_object_put(gem_obj);\n\n\treturn 0;\n}\n\nint vc4_bo_debugfs_init(struct drm_minor *minor)\n{\n\tstruct drm_device *drm = minor->dev;\n\tstruct vc4_dev *vc4 = to_vc4_dev(drm);\n\n\tif (!vc4->v3d)\n\t\treturn -ENODEV;\n\n\tdrm_debugfs_add_file(drm, \"bo_stats\", vc4_bo_stats_debugfs, NULL);\n\n\treturn 0;\n}\n\nstatic void vc4_bo_cache_destroy(struct drm_device *dev, void *unused);\nint vc4_bo_cache_init(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret;\n\tint i;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\t \n\tvc4->bo_labels = kcalloc(VC4_BO_TYPE_COUNT, sizeof(*vc4->bo_labels),\n\t\t\t\t GFP_KERNEL);\n\tif (!vc4->bo_labels)\n\t\treturn -ENOMEM;\n\tvc4->num_labels = VC4_BO_TYPE_COUNT;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(bo_type_names) != VC4_BO_TYPE_COUNT);\n\tfor (i = 0; i < VC4_BO_TYPE_COUNT; i++)\n\t\tvc4->bo_labels[i].name = bo_type_names[i];\n\n\tret = drmm_mutex_init(dev, &vc4->bo_lock);\n\tif (ret) {\n\t\tkfree(vc4->bo_labels);\n\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&vc4->bo_cache.time_list);\n\n\tINIT_WORK(&vc4->bo_cache.time_work, vc4_bo_cache_time_work);\n\ttimer_setup(&vc4->bo_cache.time_timer, vc4_bo_cache_time_timer, 0);\n\n\treturn drmm_add_action_or_reset(dev, vc4_bo_cache_destroy, NULL);\n}\n\nstatic void vc4_bo_cache_destroy(struct drm_device *dev, void *unused)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint i;\n\n\tdel_timer(&vc4->bo_cache.time_timer);\n\tcancel_work_sync(&vc4->bo_cache.time_work);\n\n\tvc4_bo_cache_purge(dev);\n\n\tfor (i = 0; i < vc4->num_labels; i++) {\n\t\tif (vc4->bo_labels[i].num_allocated) {\n\t\t\tDRM_ERROR(\"Destroying BO cache with %d %s \"\n\t\t\t\t  \"BOs still allocated\\n\",\n\t\t\t\t  vc4->bo_labels[i].num_allocated,\n\t\t\t\t  vc4->bo_labels[i].name);\n\t\t}\n\n\t\tif (is_user_label(i))\n\t\t\tkfree(vc4->bo_labels[i].name);\n\t}\n\tkfree(vc4->bo_labels);\n}\n\nint vc4_label_bo_ioctl(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_label_bo *args = data;\n\tchar *name;\n\tstruct drm_gem_object *gem_obj;\n\tint ret = 0, label;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (!args->len)\n\t\treturn -EINVAL;\n\n\tname = strndup_user(u64_to_user_ptr(args->name), args->len + 1);\n\tif (IS_ERR(name))\n\t\treturn PTR_ERR(name);\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_ERROR(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\tkfree(name);\n\t\treturn -ENOENT;\n\t}\n\n\tmutex_lock(&vc4->bo_lock);\n\tlabel = vc4_get_user_label(vc4, name);\n\tif (label != -1)\n\t\tvc4_bo_set_label(gem_obj, label);\n\telse\n\t\tret = -ENOMEM;\n\tmutex_unlock(&vc4->bo_lock);\n\n\tdrm_gem_object_put(gem_obj);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}