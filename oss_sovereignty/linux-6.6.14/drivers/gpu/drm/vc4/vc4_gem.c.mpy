{
  "module_name": "vc4_gem.c",
  "hash_id": "0bcec1c693d5ed5b4883150d1e100085bb341ee642bcfdb31cc0e6c84af17d64",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/vc4/vc4_gem.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/pm_runtime.h>\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/sched/signal.h>\n#include <linux/dma-fence-array.h>\n\n#include <drm/drm_syncobj.h>\n\n#include \"uapi/drm/vc4_drm.h\"\n#include \"vc4_drv.h\"\n#include \"vc4_regs.h\"\n#include \"vc4_trace.h\"\n\nstatic void\nvc4_queue_hangcheck(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tmod_timer(&vc4->hangcheck.timer,\n\t\t  round_jiffies_up(jiffies + msecs_to_jiffies(100)));\n}\n\nstruct vc4_hang_state {\n\tstruct drm_vc4_get_hang_state user_state;\n\n\tu32 bo_count;\n\tstruct drm_gem_object **bo;\n};\n\nstatic void\nvc4_free_hang_state(struct drm_device *dev, struct vc4_hang_state *state)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < state->user_state.bo_count; i++)\n\t\tdrm_gem_object_put(state->bo[i]);\n\n\tkfree(state);\n}\n\nint\nvc4_get_hang_state_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file_priv)\n{\n\tstruct drm_vc4_get_hang_state *get_state = data;\n\tstruct drm_vc4_get_hang_state_bo *bo_state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long irqflags;\n\tu32 i;\n\tint ret = 0;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (!vc4->v3d) {\n\t\tDRM_DEBUG(\"VC4_GET_HANG_STATE with no VC4 V3D probed\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tkernel_state = vc4->hang_state;\n\tif (!kernel_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn -ENOENT;\n\t}\n\tstate = &kernel_state->user_state;\n\n\t \n\tif (get_state->bo_count < state->bo_count) {\n\t\tget_state->bo_count = state->bo_count;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn 0;\n\t}\n\n\tvc4->hang_state = NULL;\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t \n\tstate->bo = get_state->bo;\n\tmemcpy(get_state, state, sizeof(*state));\n\n\tbo_state = kcalloc(state->bo_count, sizeof(*bo_state), GFP_KERNEL);\n\tif (!bo_state) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\n\tfor (i = 0; i < state->bo_count; i++) {\n\t\tstruct vc4_bo *vc4_bo = to_vc4_bo(kernel_state->bo[i]);\n\t\tu32 handle;\n\n\t\tret = drm_gem_handle_create(file_priv, kernel_state->bo[i],\n\t\t\t\t\t    &handle);\n\n\t\tif (ret) {\n\t\t\tstate->bo_count = i;\n\t\t\tgoto err_delete_handle;\n\t\t}\n\t\tbo_state[i].handle = handle;\n\t\tbo_state[i].paddr = vc4_bo->base.dma_addr;\n\t\tbo_state[i].size = vc4_bo->base.base.size;\n\t}\n\n\tif (copy_to_user(u64_to_user_ptr(get_state->bo),\n\t\t\t bo_state,\n\t\t\t state->bo_count * sizeof(*bo_state)))\n\t\tret = -EFAULT;\n\nerr_delete_handle:\n\tif (ret) {\n\t\tfor (i = 0; i < state->bo_count; i++)\n\t\t\tdrm_gem_handle_delete(file_priv, bo_state[i].handle);\n\t}\n\nerr_free:\n\tvc4_free_hang_state(dev, kernel_state);\n\tkfree(bo_state);\n\n\treturn ret;\n}\n\nstatic void\nvc4_save_hang_state(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_get_hang_state *state;\n\tstruct vc4_hang_state *kernel_state;\n\tstruct vc4_exec_info *exec[2];\n\tstruct vc4_bo *bo;\n\tunsigned long irqflags;\n\tunsigned int i, j, k, unref_list_count;\n\n\tkernel_state = kcalloc(1, sizeof(*kernel_state), GFP_KERNEL);\n\tif (!kernel_state)\n\t\treturn;\n\n\tstate = &kernel_state->user_state;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\texec[0] = vc4_first_bin_job(vc4);\n\texec[1] = vc4_first_render_job(vc4);\n\tif (!exec[0] && !exec[1]) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\t \n\tstate->bo_count = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tunref_list_count = 0;\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head)\n\t\t\tunref_list_count++;\n\t\tstate->bo_count += exec[i]->bo_count + unref_list_count;\n\t}\n\n\tkernel_state->bo = kcalloc(state->bo_count,\n\t\t\t\t   sizeof(*kernel_state->bo), GFP_ATOMIC);\n\n\tif (!kernel_state->bo) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tk = 0;\n\tfor (i = 0; i < 2; i++) {\n\t\tif (!exec[i])\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < exec[i]->bo_count; j++) {\n\t\t\tbo = to_vc4_bo(exec[i]->bo[j]);\n\n\t\t\t \n\t\t\tWARN_ON(!refcount_read(&bo->usecnt));\n\t\t\trefcount_inc(&bo->usecnt);\n\t\t\tdrm_gem_object_get(exec[i]->bo[j]);\n\t\t\tkernel_state->bo[k++] = exec[i]->bo[j];\n\t\t}\n\n\t\tlist_for_each_entry(bo, &exec[i]->unref_list, unref_head) {\n\t\t\t \n\t\t\tdrm_gem_object_get(&bo->base.base);\n\t\t\tkernel_state->bo[k++] = &bo->base.base;\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(k != state->bo_count);\n\n\tif (exec[0])\n\t\tstate->start_bin = exec[0]->ct0ca;\n\tif (exec[1])\n\t\tstate->start_render = exec[1]->ct1ca;\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\tstate->ct0ca = V3D_READ(V3D_CTNCA(0));\n\tstate->ct0ea = V3D_READ(V3D_CTNEA(0));\n\n\tstate->ct1ca = V3D_READ(V3D_CTNCA(1));\n\tstate->ct1ea = V3D_READ(V3D_CTNEA(1));\n\n\tstate->ct0cs = V3D_READ(V3D_CTNCS(0));\n\tstate->ct1cs = V3D_READ(V3D_CTNCS(1));\n\n\tstate->ct0ra0 = V3D_READ(V3D_CT00RA0);\n\tstate->ct1ra0 = V3D_READ(V3D_CT01RA0);\n\n\tstate->bpca = V3D_READ(V3D_BPCA);\n\tstate->bpcs = V3D_READ(V3D_BPCS);\n\tstate->bpoa = V3D_READ(V3D_BPOA);\n\tstate->bpos = V3D_READ(V3D_BPOS);\n\n\tstate->vpmbase = V3D_READ(V3D_VPMBASE);\n\n\tstate->dbge = V3D_READ(V3D_DBGE);\n\tstate->fdbgo = V3D_READ(V3D_FDBGO);\n\tstate->fdbgb = V3D_READ(V3D_FDBGB);\n\tstate->fdbgr = V3D_READ(V3D_FDBGR);\n\tstate->fdbgs = V3D_READ(V3D_FDBGS);\n\tstate->errstat = V3D_READ(V3D_ERRSTAT);\n\n\t \n\tfor (i = 0; i < kernel_state->user_state.bo_count; i++) {\n\t\tstruct vc4_bo *bo = to_vc4_bo(kernel_state->bo[i]);\n\n\t\tif (bo->madv == __VC4_MADV_NOTSUPP)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&bo->madv_lock);\n\t\tif (!WARN_ON(bo->madv == __VC4_MADV_PURGED))\n\t\t\tbo->madv = VC4_MADV_WILLNEED;\n\t\trefcount_dec(&bo->usecnt);\n\t\tmutex_unlock(&bo->madv_lock);\n\t}\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (vc4->hang_state) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_free_hang_state(dev, kernel_state);\n\t} else {\n\t\tvc4->hang_state = kernel_state;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t}\n}\n\nstatic void\nvc4_reset(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tDRM_INFO(\"Resetting GPU.\\n\");\n\n\tmutex_lock(&vc4->power_lock);\n\tif (vc4->power_refcount) {\n\t\t \n\t\tpm_runtime_put_sync_suspend(&vc4->v3d->pdev->dev);\n\t\tpm_runtime_get_sync(&vc4->v3d->pdev->dev);\n\t}\n\tmutex_unlock(&vc4->power_lock);\n\n\tvc4_irq_reset(dev);\n\n\t \n\tvc4_queue_hangcheck(dev);\n}\n\nstatic void\nvc4_reset_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, hangcheck.reset_work);\n\n\tvc4_save_hang_state(&vc4->base);\n\n\tvc4_reset(&vc4->base);\n}\n\nstatic void\nvc4_hangcheck_elapsed(struct timer_list *t)\n{\n\tstruct vc4_dev *vc4 = from_timer(vc4, t, hangcheck.timer);\n\tstruct drm_device *dev = &vc4->base;\n\tuint32_t ct0ca, ct1ca;\n\tunsigned long irqflags;\n\tstruct vc4_exec_info *bin_exec, *render_exec;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tbin_exec = vc4_first_bin_job(vc4);\n\trender_exec = vc4_first_render_job(vc4);\n\n\t \n\tif (!bin_exec && !render_exec) {\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\treturn;\n\t}\n\n\tct0ca = V3D_READ(V3D_CTNCA(0));\n\tct1ca = V3D_READ(V3D_CTNCA(1));\n\n\t \n\tif ((bin_exec && ct0ca != bin_exec->last_ct0ca) ||\n\t    (render_exec && ct1ca != render_exec->last_ct1ca)) {\n\t\tif (bin_exec)\n\t\t\tbin_exec->last_ct0ca = ct0ca;\n\t\tif (render_exec)\n\t\t\trender_exec->last_ct1ca = ct1ca;\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_queue_hangcheck(dev);\n\t\treturn;\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t \n\tschedule_work(&vc4->hangcheck.reset_work);\n}\n\nstatic void\nsubmit_cl(struct drm_device *dev, uint32_t thread, uint32_t start, uint32_t end)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t \n\tV3D_WRITE(V3D_CTNCA(thread), start);\n\tV3D_WRITE(V3D_CTNEA(thread), end);\n}\n\nint\nvc4_wait_for_seqno(struct drm_device *dev, uint64_t seqno, uint64_t timeout_ns,\n\t\t   bool interruptible)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret = 0;\n\tunsigned long timeout_expire;\n\tDEFINE_WAIT(wait);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (vc4->finished_seqno >= seqno)\n\t\treturn 0;\n\n\tif (timeout_ns == 0)\n\t\treturn -ETIME;\n\n\ttimeout_expire = jiffies + nsecs_to_jiffies(timeout_ns);\n\n\ttrace_vc4_wait_for_seqno_begin(dev, seqno, timeout_ns);\n\tfor (;;) {\n\t\tprepare_to_wait(&vc4->job_wait_queue, &wait,\n\t\t\t\tinterruptible ? TASK_INTERRUPTIBLE :\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tif (interruptible && signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (vc4->finished_seqno >= seqno)\n\t\t\tbreak;\n\n\t\tif (timeout_ns != ~0ull) {\n\t\t\tif (time_after_eq(jiffies, timeout_expire)) {\n\t\t\t\tret = -ETIME;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tschedule_timeout(timeout_expire - jiffies);\n\t\t} else {\n\t\t\tschedule();\n\t\t}\n\t}\n\n\tfinish_wait(&vc4->job_wait_queue, &wait);\n\ttrace_vc4_wait_for_seqno_end(dev, seqno);\n\n\treturn ret;\n}\n\nstatic void\nvc4_flush_caches(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t \n\tV3D_WRITE(V3D_L2CACTL,\n\t\t  V3D_L2CACTL_L2CCLR);\n\n\tV3D_WRITE(V3D_SLCACTL,\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T1CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T0CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_UCC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_ICC));\n}\n\nstatic void\nvc4_flush_texture_caches(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\tV3D_WRITE(V3D_L2CACTL,\n\t\t  V3D_L2CACTL_L2CCLR);\n\n\tV3D_WRITE(V3D_SLCACTL,\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T1CC) |\n\t\t  VC4_SET_FIELD(0xf, V3D_SLCACTL_T0CC));\n}\n\n \nvoid\nvc4_submit_next_bin_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\nagain:\n\texec = vc4_first_bin_job(vc4);\n\tif (!exec)\n\t\treturn;\n\n\tvc4_flush_caches(dev);\n\n\t \n\tif (exec->perfmon && vc4->active_perfmon != exec->perfmon)\n\t\tvc4_perfmon_start(vc4, exec->perfmon);\n\n\t \n\tif (exec->ct0ca != exec->ct0ea) {\n\t\ttrace_vc4_submit_cl(dev, false, exec->seqno, exec->ct0ca,\n\t\t\t\t    exec->ct0ea);\n\t\tsubmit_cl(dev, 0, exec->ct0ca, exec->ct0ea);\n\t} else {\n\t\tstruct vc4_exec_info *next;\n\n\t\tvc4_move_job_to_render(dev, exec);\n\t\tnext = vc4_first_bin_job(vc4);\n\n\t\t \n\t\tif (next && next->perfmon == exec->perfmon)\n\t\t\tgoto again;\n\t}\n}\n\nvoid\nvc4_submit_next_render_job(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *exec = vc4_first_render_job(vc4);\n\n\tif (!exec)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\t \n\tvc4_flush_texture_caches(dev);\n\n\ttrace_vc4_submit_cl(dev, true, exec->seqno, exec->ct1ca, exec->ct1ea);\n\tsubmit_cl(dev, 1, exec->ct1ca, exec->ct1ea);\n}\n\nvoid\nvc4_move_job_to_render(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tbool was_empty = list_empty(&vc4->render_job_list);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\tlist_move_tail(&exec->head, &vc4->render_job_list);\n\tif (was_empty)\n\t\tvc4_submit_next_render_job(dev);\n}\n\nstatic void\nvc4_update_bo_seqnos(struct vc4_exec_info *exec, uint64_t seqno)\n{\n\tstruct vc4_bo *bo;\n\tunsigned i;\n\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tbo = to_vc4_bo(exec->bo[i]);\n\t\tbo->seqno = seqno;\n\n\t\tdma_resv_add_fence(bo->base.base.resv, exec->fence,\n\t\t\t\t   DMA_RESV_USAGE_READ);\n\t}\n\n\tlist_for_each_entry(bo, &exec->unref_list, unref_head) {\n\t\tbo->seqno = seqno;\n\t}\n\n\tfor (i = 0; i < exec->rcl_write_bo_count; i++) {\n\t\tbo = to_vc4_bo(&exec->rcl_write_bo[i]->base);\n\t\tbo->write_seqno = seqno;\n\n\t\tdma_resv_add_fence(bo->base.base.resv, exec->fence,\n\t\t\t\t   DMA_RESV_USAGE_WRITE);\n\t}\n}\n\nstatic void\nvc4_unlock_bo_reservations(struct drm_device *dev,\n\t\t\t   struct vc4_exec_info *exec,\n\t\t\t   struct ww_acquire_ctx *acquire_ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < exec->bo_count; i++)\n\t\tdma_resv_unlock(exec->bo[i]->resv);\n\n\tww_acquire_fini(acquire_ctx);\n}\n\n \nstatic int\nvc4_lock_bo_reservations(struct drm_device *dev,\n\t\t\t struct vc4_exec_info *exec,\n\t\t\t struct ww_acquire_ctx *acquire_ctx)\n{\n\tint contended_lock = -1;\n\tint i, ret;\n\tstruct drm_gem_object *bo;\n\n\tww_acquire_init(acquire_ctx, &reservation_ww_class);\n\nretry:\n\tif (contended_lock != -1) {\n\t\tbo = exec->bo[contended_lock];\n\t\tret = dma_resv_lock_slow_interruptible(bo->resv, acquire_ctx);\n\t\tif (ret) {\n\t\t\tww_acquire_done(acquire_ctx);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tif (i == contended_lock)\n\t\t\tcontinue;\n\n\t\tbo = exec->bo[i];\n\n\t\tret = dma_resv_lock_interruptible(bo->resv, acquire_ctx);\n\t\tif (ret) {\n\t\t\tint j;\n\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tbo = exec->bo[j];\n\t\t\t\tdma_resv_unlock(bo->resv);\n\t\t\t}\n\n\t\t\tif (contended_lock != -1 && contended_lock >= i) {\n\t\t\t\tbo = exec->bo[contended_lock];\n\n\t\t\t\tdma_resv_unlock(bo->resv);\n\t\t\t}\n\n\t\t\tif (ret == -EDEADLK) {\n\t\t\t\tcontended_lock = i;\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tww_acquire_done(acquire_ctx);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tww_acquire_done(acquire_ctx);\n\n\t \n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tbo = exec->bo[i];\n\n\t\tret = dma_resv_reserve_fences(bo->resv, 1);\n\t\tif (ret) {\n\t\t\tvc4_unlock_bo_reservations(dev, exec, acquire_ctx);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nvc4_queue_submit(struct drm_device *dev, struct vc4_exec_info *exec,\n\t\t struct ww_acquire_ctx *acquire_ctx,\n\t\t struct drm_syncobj *out_sync)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_exec_info *renderjob;\n\tuint64_t seqno;\n\tunsigned long irqflags;\n\tstruct vc4_fence *fence;\n\n\tfence = kzalloc(sizeof(*fence), GFP_KERNEL);\n\tif (!fence)\n\t\treturn -ENOMEM;\n\tfence->dev = dev;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\n\tseqno = ++vc4->emit_seqno;\n\texec->seqno = seqno;\n\n\tdma_fence_init(&fence->base, &vc4_fence_ops, &vc4->job_lock,\n\t\t       vc4->dma_fence_context, exec->seqno);\n\tfence->seqno = exec->seqno;\n\texec->fence = &fence->base;\n\n\tif (out_sync)\n\t\tdrm_syncobj_replace_fence(out_sync, exec->fence);\n\n\tvc4_update_bo_seqnos(exec, seqno);\n\n\tvc4_unlock_bo_reservations(dev, exec, acquire_ctx);\n\n\tlist_add_tail(&exec->head, &vc4->bin_job_list);\n\n\t \n\trenderjob = vc4_first_render_job(vc4);\n\tif (vc4_first_bin_job(vc4) == exec &&\n\t    (!renderjob || renderjob->perfmon == exec->perfmon)) {\n\t\tvc4_submit_next_bin_job(dev);\n\t\tvc4_queue_hangcheck(dev);\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\treturn 0;\n}\n\n \nstatic int\nvc4_cl_lookup_bos(struct drm_device *dev,\n\t\t  struct drm_file *file_priv,\n\t\t  struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tint ret = 0;\n\tint i;\n\n\texec->bo_count = args->bo_handle_count;\n\n\tif (!exec->bo_count) {\n\t\t \n\t\tDRM_DEBUG(\"Rendering requires BOs to validate\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = drm_gem_objects_lookup(file_priv, u64_to_user_ptr(args->bo_handles),\n\t\t\t\t     exec->bo_count, &exec->bo);\n\n\tif (ret)\n\t\tgoto fail_put_bo;\n\n\tfor (i = 0; i < exec->bo_count; i++) {\n\t\tret = vc4_bo_inc_usecnt(to_vc4_bo(exec->bo[i]));\n\t\tif (ret)\n\t\t\tgoto fail_dec_usecnt;\n\t}\n\n\treturn 0;\n\nfail_dec_usecnt:\n\t \n\tfor (i-- ; i >= 0; i--)\n\t\tvc4_bo_dec_usecnt(to_vc4_bo(exec->bo[i]));\n\nfail_put_bo:\n\t \n\tfor (i = 0; i < exec->bo_count && exec->bo[i]; i++)\n\t\tdrm_gem_object_put(exec->bo[i]);\n\n\tkvfree(exec->bo);\n\texec->bo = NULL;\n\treturn ret;\n}\n\nstatic int\nvc4_get_bcl(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct drm_vc4_submit_cl *args = exec->args;\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tvoid *temp = NULL;\n\tvoid *bin;\n\tint ret = 0;\n\tuint32_t bin_offset = 0;\n\tuint32_t shader_rec_offset = roundup(bin_offset + args->bin_cl_size,\n\t\t\t\t\t     16);\n\tuint32_t uniforms_offset = shader_rec_offset + args->shader_rec_size;\n\tuint32_t exec_size = uniforms_offset + args->uniforms_size;\n\tuint32_t temp_size = exec_size + (sizeof(struct vc4_shader_state) *\n\t\t\t\t\t  args->shader_rec_count);\n\tstruct vc4_bo *bo;\n\n\tif (shader_rec_offset < args->bin_cl_size ||\n\t    uniforms_offset < shader_rec_offset ||\n\t    exec_size < uniforms_offset ||\n\t    args->shader_rec_count >= (UINT_MAX /\n\t\t\t\t\t  sizeof(struct vc4_shader_state)) ||\n\t    temp_size < exec_size) {\n\t\tDRM_DEBUG(\"overflow in exec arguments\\n\");\n\t\tret = -EINVAL;\n\t\tgoto fail;\n\t}\n\n\t \n\ttemp = kvmalloc_array(temp_size, 1, GFP_KERNEL);\n\tif (!temp) {\n\t\tDRM_ERROR(\"Failed to allocate storage for copying \"\n\t\t\t  \"in bin/render CLs.\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tbin = temp + bin_offset;\n\texec->shader_rec_u = temp + shader_rec_offset;\n\texec->uniforms_u = temp + uniforms_offset;\n\texec->shader_state = temp + exec_size;\n\texec->shader_state_size = args->shader_rec_count;\n\n\tif (copy_from_user(bin,\n\t\t\t   u64_to_user_ptr(args->bin_cl),\n\t\t\t   args->bin_cl_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->shader_rec_u,\n\t\t\t   u64_to_user_ptr(args->shader_rec),\n\t\t\t   args->shader_rec_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tif (copy_from_user(exec->uniforms_u,\n\t\t\t   u64_to_user_ptr(args->uniforms),\n\t\t\t   args->uniforms_size)) {\n\t\tret = -EFAULT;\n\t\tgoto fail;\n\t}\n\n\tbo = vc4_bo_create(dev, exec_size, true, VC4_BO_TYPE_BCL);\n\tif (IS_ERR(bo)) {\n\t\tDRM_ERROR(\"Couldn't allocate BO for binning\\n\");\n\t\tret = PTR_ERR(bo);\n\t\tgoto fail;\n\t}\n\texec->exec_bo = &bo->base;\n\n\tlist_add_tail(&to_vc4_bo(&exec->exec_bo->base)->unref_head,\n\t\t      &exec->unref_list);\n\n\texec->ct0ca = exec->exec_bo->dma_addr + bin_offset;\n\n\texec->bin_u = bin;\n\n\texec->shader_rec_v = exec->exec_bo->vaddr + shader_rec_offset;\n\texec->shader_rec_p = exec->exec_bo->dma_addr + shader_rec_offset;\n\texec->shader_rec_size = args->shader_rec_size;\n\n\texec->uniforms_v = exec->exec_bo->vaddr + uniforms_offset;\n\texec->uniforms_p = exec->exec_bo->dma_addr + uniforms_offset;\n\texec->uniforms_size = args->uniforms_size;\n\n\tret = vc4_validate_bin_cl(dev,\n\t\t\t\t  exec->exec_bo->vaddr + bin_offset,\n\t\t\t\t  bin,\n\t\t\t\t  exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_validate_shader_recs(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (exec->found_tile_binning_mode_config_packet) {\n\t\tret = vc4_v3d_bin_bo_get(vc4, &exec->bin_bo_used);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\n\t \n\tret = vc4_wait_for_seqno(dev, exec->bin_dep_seqno, ~0ull, true);\n\nfail:\n\tkvfree(temp);\n\treturn ret;\n}\n\nstatic void\nvc4_complete_exec(struct drm_device *dev, struct vc4_exec_info *exec)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long irqflags;\n\tunsigned i;\n\n\t \n\tif (exec->fence) {\n\t\tdma_fence_signal(exec->fence);\n\t\tdma_fence_put(exec->fence);\n\t}\n\n\tif (exec->bo) {\n\t\tfor (i = 0; i < exec->bo_count; i++) {\n\t\t\tstruct vc4_bo *bo = to_vc4_bo(exec->bo[i]);\n\n\t\t\tvc4_bo_dec_usecnt(bo);\n\t\t\tdrm_gem_object_put(exec->bo[i]);\n\t\t}\n\t\tkvfree(exec->bo);\n\t}\n\n\twhile (!list_empty(&exec->unref_list)) {\n\t\tstruct vc4_bo *bo = list_first_entry(&exec->unref_list,\n\t\t\t\t\t\t     struct vc4_bo, unref_head);\n\t\tlist_del(&bo->unref_head);\n\t\tdrm_gem_object_put(&bo->base.base);\n\t}\n\n\t \n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tvc4->bin_alloc_used &= ~exec->bin_slots;\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\t \n\tif (exec->bin_bo_used)\n\t\tvc4_v3d_bin_bo_put(vc4);\n\n\t \n\tvc4_perfmon_put(exec->perfmon);\n\n\tvc4_v3d_pm_put(vc4);\n\n\tkfree(exec);\n}\n\nvoid\nvc4_job_handle_completed(struct vc4_dev *vc4)\n{\n\tunsigned long irqflags;\n\tstruct vc4_seqno_cb *cb, *cb_temp;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn;\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\twhile (!list_empty(&vc4->job_done_list)) {\n\t\tstruct vc4_exec_info *exec =\n\t\t\tlist_first_entry(&vc4->job_done_list,\n\t\t\t\t\t struct vc4_exec_info, head);\n\t\tlist_del(&exec->head);\n\n\t\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\t\tvc4_complete_exec(&vc4->base, exec);\n\t\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\t}\n\n\tlist_for_each_entry_safe(cb, cb_temp, &vc4->seqno_cb_list, work.entry) {\n\t\tif (cb->seqno <= vc4->finished_seqno) {\n\t\t\tlist_del_init(&cb->work.entry);\n\t\t\tschedule_work(&cb->work);\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n}\n\nstatic void vc4_seqno_cb_work(struct work_struct *work)\n{\n\tstruct vc4_seqno_cb *cb = container_of(work, struct vc4_seqno_cb, work);\n\n\tcb->func(cb);\n}\n\nint vc4_queue_seqno_cb(struct drm_device *dev,\n\t\t       struct vc4_seqno_cb *cb, uint64_t seqno,\n\t\t       void (*func)(struct vc4_seqno_cb *cb))\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tunsigned long irqflags;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tcb->func = func;\n\tINIT_WORK(&cb->work, vc4_seqno_cb_work);\n\n\tspin_lock_irqsave(&vc4->job_lock, irqflags);\n\tif (seqno > vc4->finished_seqno) {\n\t\tcb->seqno = seqno;\n\t\tlist_add_tail(&cb->work.entry, &vc4->seqno_cb_list);\n\t} else {\n\t\tschedule_work(&cb->work);\n\t}\n\tspin_unlock_irqrestore(&vc4->job_lock, irqflags);\n\n\treturn 0;\n}\n\n \nstatic void\nvc4_job_done_work(struct work_struct *work)\n{\n\tstruct vc4_dev *vc4 =\n\t\tcontainer_of(work, struct vc4_dev, job_done_work);\n\n\tvc4_job_handle_completed(vc4);\n}\n\nstatic int\nvc4_wait_for_seqno_ioctl_helper(struct drm_device *dev,\n\t\t\t\tuint64_t seqno,\n\t\t\t\tuint64_t *timeout_ns)\n{\n\tunsigned long start = jiffies;\n\tint ret = vc4_wait_for_seqno(dev, seqno, *timeout_ns, true);\n\n\tif ((ret == -EINTR || ret == -ERESTARTSYS) && *timeout_ns != ~0ull) {\n\t\tuint64_t delta = jiffies_to_nsecs(jiffies - start);\n\n\t\tif (*timeout_ns >= delta)\n\t\t\t*timeout_ns -= delta;\n\t}\n\n\treturn ret;\n}\n\nint\nvc4_wait_seqno_ioctl(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_wait_seqno *args = data;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\treturn vc4_wait_for_seqno_ioctl_helper(dev, args->seqno,\n\t\t\t\t\t       &args->timeout_ns);\n}\n\nint\nvc4_wait_bo_ioctl(struct drm_device *dev, void *data,\n\t\t  struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret;\n\tstruct drm_vc4_wait_bo *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_DEBUG(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -EINVAL;\n\t}\n\tbo = to_vc4_bo(gem_obj);\n\n\tret = vc4_wait_for_seqno_ioctl_helper(dev, bo->seqno,\n\t\t\t\t\t      &args->timeout_ns);\n\n\tdrm_gem_object_put(gem_obj);\n\treturn ret;\n}\n\n \nint\nvc4_submit_cl_ioctl(struct drm_device *dev, void *data,\n\t\t    struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct vc4_file *vc4file = file_priv->driver_priv;\n\tstruct drm_vc4_submit_cl *args = data;\n\tstruct drm_syncobj *out_sync = NULL;\n\tstruct vc4_exec_info *exec;\n\tstruct ww_acquire_ctx acquire_ctx;\n\tstruct dma_fence *in_fence;\n\tint ret = 0;\n\n\ttrace_vc4_submit_cl_ioctl(dev, args->bin_cl_size,\n\t\t\t\t  args->shader_rec_size,\n\t\t\t\t  args->bo_handle_count);\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tif (!vc4->v3d) {\n\t\tDRM_DEBUG(\"VC4_SUBMIT_CL with no VC4 V3D probed\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif ((args->flags & ~(VC4_SUBMIT_CL_USE_CLEAR_COLOR |\n\t\t\t     VC4_SUBMIT_CL_FIXED_RCL_ORDER |\n\t\t\t     VC4_SUBMIT_CL_RCL_ORDER_INCREASING_X |\n\t\t\t     VC4_SUBMIT_CL_RCL_ORDER_INCREASING_Y)) != 0) {\n\t\tDRM_DEBUG(\"Unknown flags: 0x%02x\\n\", args->flags);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->pad2 != 0) {\n\t\tDRM_DEBUG(\"Invalid pad: 0x%08x\\n\", args->pad2);\n\t\treturn -EINVAL;\n\t}\n\n\texec = kcalloc(1, sizeof(*exec), GFP_KERNEL);\n\tif (!exec) {\n\t\tDRM_ERROR(\"malloc failure on exec struct\\n\");\n\t\treturn -ENOMEM;\n\t}\n\texec->dev = vc4;\n\n\tret = vc4_v3d_pm_get(vc4);\n\tif (ret) {\n\t\tkfree(exec);\n\t\treturn ret;\n\t}\n\n\texec->args = args;\n\tINIT_LIST_HEAD(&exec->unref_list);\n\n\tret = vc4_cl_lookup_bos(dev, file_priv, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (args->perfmonid) {\n\t\texec->perfmon = vc4_perfmon_find(vc4file,\n\t\t\t\t\t\t args->perfmonid);\n\t\tif (!exec->perfmon) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tif (args->in_sync) {\n\t\tret = drm_syncobj_find_fence(file_priv, args->in_sync,\n\t\t\t\t\t     0, 0, &in_fence);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t\t \n\t\tif (!dma_fence_match_context(in_fence,\n\t\t\t\t\t     vc4->dma_fence_context)) {\n\t\t\tret = dma_fence_wait(in_fence, true);\n\t\t\tif (ret) {\n\t\t\t\tdma_fence_put(in_fence);\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t}\n\n\t\tdma_fence_put(in_fence);\n\t}\n\n\tif (exec->args->bin_cl_size != 0) {\n\t\tret = vc4_get_bcl(dev, exec);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t} else {\n\t\texec->ct0ca = 0;\n\t\texec->ct0ea = 0;\n\t}\n\n\tret = vc4_get_rcl(dev, exec);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = vc4_lock_bo_reservations(dev, exec, &acquire_ctx);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (args->out_sync) {\n\t\tout_sync = drm_syncobj_find(file_priv, args->out_sync);\n\t\tif (!out_sync) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\n\t\t \n\t}\n\n\t \n\texec->args = NULL;\n\n\tret = vc4_queue_submit(dev, exec, &acquire_ctx, out_sync);\n\n\t \n\tif (out_sync)\n\t\tdrm_syncobj_put(out_sync);\n\n\tif (ret)\n\t\tgoto fail;\n\n\t \n\targs->seqno = vc4->emit_seqno;\n\n\treturn 0;\n\nfail:\n\tvc4_complete_exec(&vc4->base, exec);\n\n\treturn ret;\n}\n\nstatic void vc4_gem_destroy(struct drm_device *dev, void *unused);\nint vc4_gem_init(struct drm_device *dev)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tvc4->dma_fence_context = dma_fence_context_alloc(1);\n\n\tINIT_LIST_HEAD(&vc4->bin_job_list);\n\tINIT_LIST_HEAD(&vc4->render_job_list);\n\tINIT_LIST_HEAD(&vc4->job_done_list);\n\tINIT_LIST_HEAD(&vc4->seqno_cb_list);\n\tspin_lock_init(&vc4->job_lock);\n\n\tINIT_WORK(&vc4->hangcheck.reset_work, vc4_reset_work);\n\ttimer_setup(&vc4->hangcheck.timer, vc4_hangcheck_elapsed, 0);\n\n\tINIT_WORK(&vc4->job_done_work, vc4_job_done_work);\n\n\tret = drmm_mutex_init(dev, &vc4->power_lock);\n\tif (ret)\n\t\treturn ret;\n\n\tINIT_LIST_HEAD(&vc4->purgeable.list);\n\n\tret = drmm_mutex_init(dev, &vc4->purgeable.lock);\n\tif (ret)\n\t\treturn ret;\n\n\treturn drmm_add_action_or_reset(dev, vc4_gem_destroy, NULL);\n}\n\nstatic void vc4_gem_destroy(struct drm_device *dev, void *unused)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\n\t \n\tWARN_ON(vc4->emit_seqno != vc4->finished_seqno);\n\n\t \n\tif (vc4->bin_bo) {\n\t\tdrm_gem_object_put(&vc4->bin_bo->base.base);\n\t\tvc4->bin_bo = NULL;\n\t}\n\n\tif (vc4->hang_state)\n\t\tvc4_free_hang_state(dev, vc4->hang_state);\n}\n\nint vc4_gem_madvise_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *file_priv)\n{\n\tstruct vc4_dev *vc4 = to_vc4_dev(dev);\n\tstruct drm_vc4_gem_madvise *args = data;\n\tstruct drm_gem_object *gem_obj;\n\tstruct vc4_bo *bo;\n\tint ret;\n\n\tif (WARN_ON_ONCE(vc4->is_vc5))\n\t\treturn -ENODEV;\n\n\tswitch (args->madv) {\n\tcase VC4_MADV_DONTNEED:\n\tcase VC4_MADV_WILLNEED:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tgem_obj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!gem_obj) {\n\t\tDRM_DEBUG(\"Failed to look up GEM BO %d\\n\", args->handle);\n\t\treturn -ENOENT;\n\t}\n\n\tbo = to_vc4_bo(gem_obj);\n\n\t \n\tif (bo->madv == __VC4_MADV_NOTSUPP) {\n\t\tDRM_DEBUG(\"madvise not supported on this BO\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_put_gem;\n\t}\n\n\t \n\tif (gem_obj->import_attach) {\n\t\tDRM_DEBUG(\"madvise not supported on imported BOs\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out_put_gem;\n\t}\n\n\tmutex_lock(&bo->madv_lock);\n\n\tif (args->madv == VC4_MADV_DONTNEED && bo->madv == VC4_MADV_WILLNEED &&\n\t    !refcount_read(&bo->usecnt)) {\n\t\t \n\t\tvc4_bo_add_to_purgeable_pool(bo);\n\t} else if (args->madv == VC4_MADV_WILLNEED &&\n\t\t   bo->madv == VC4_MADV_DONTNEED &&\n\t\t   !refcount_read(&bo->usecnt)) {\n\t\t \n\t\tvc4_bo_remove_from_purgeable_pool(bo);\n\t}\n\n\t \n\targs->retained = bo->madv != __VC4_MADV_PURGED;\n\n\t \n\tif (bo->madv != __VC4_MADV_PURGED)\n\t\tbo->madv = args->madv;\n\n\tmutex_unlock(&bo->madv_lock);\n\n\tret = 0;\n\nout_put_gem:\n\tdrm_gem_object_put(gem_obj);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}