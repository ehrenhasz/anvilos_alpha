{
  "module_name": "sched_main.c",
  "hash_id": "271c4ca8281cfea8ab3004fd38a9b4e878d71def140fb6f02af6167ffba7381d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/scheduler/sched_main.c",
  "human_readable_source": " \n\n \n\n#include <linux/kthread.h>\n#include <linux/wait.h>\n#include <linux/sched.h>\n#include <linux/completion.h>\n#include <linux/dma-resv.h>\n#include <uapi/linux/sched/types.h>\n\n#include <drm/drm_print.h>\n#include <drm/drm_gem.h>\n#include <drm/drm_syncobj.h>\n#include <drm/gpu_scheduler.h>\n#include <drm/spsc_queue.h>\n\n#define CREATE_TRACE_POINTS\n#include \"gpu_scheduler_trace.h\"\n\n#define to_drm_sched_job(sched_job)\t\t\\\n\t\tcontainer_of((sched_job), struct drm_sched_job, queue_node)\n\nint drm_sched_policy = DRM_SCHED_POLICY_FIFO;\n\n \nMODULE_PARM_DESC(sched_policy, \"Specify the scheduling policy for entities on a run-queue, \" __stringify(DRM_SCHED_POLICY_RR) \" = Round Robin, \" __stringify(DRM_SCHED_POLICY_FIFO) \" = FIFO (default).\");\nmodule_param_named(sched_policy, drm_sched_policy, int, 0444);\n\nstatic __always_inline bool drm_sched_entity_compare_before(struct rb_node *a,\n\t\t\t\t\t\t\t    const struct rb_node *b)\n{\n\tstruct drm_sched_entity *ent_a =  rb_entry((a), struct drm_sched_entity, rb_tree_node);\n\tstruct drm_sched_entity *ent_b =  rb_entry((b), struct drm_sched_entity, rb_tree_node);\n\n\treturn ktime_before(ent_a->oldest_job_waiting, ent_b->oldest_job_waiting);\n}\n\nstatic inline void drm_sched_rq_remove_fifo_locked(struct drm_sched_entity *entity)\n{\n\tstruct drm_sched_rq *rq = entity->rq;\n\n\tif (!RB_EMPTY_NODE(&entity->rb_tree_node)) {\n\t\trb_erase_cached(&entity->rb_tree_node, &rq->rb_tree_root);\n\t\tRB_CLEAR_NODE(&entity->rb_tree_node);\n\t}\n}\n\nvoid drm_sched_rq_update_fifo(struct drm_sched_entity *entity, ktime_t ts)\n{\n\t \n\tspin_lock(&entity->rq_lock);\n\tspin_lock(&entity->rq->lock);\n\n\tdrm_sched_rq_remove_fifo_locked(entity);\n\n\tentity->oldest_job_waiting = ts;\n\n\trb_add_cached(&entity->rb_tree_node, &entity->rq->rb_tree_root,\n\t\t      drm_sched_entity_compare_before);\n\n\tspin_unlock(&entity->rq->lock);\n\tspin_unlock(&entity->rq_lock);\n}\n\n \nstatic void drm_sched_rq_init(struct drm_gpu_scheduler *sched,\n\t\t\t      struct drm_sched_rq *rq)\n{\n\tspin_lock_init(&rq->lock);\n\tINIT_LIST_HEAD(&rq->entities);\n\trq->rb_tree_root = RB_ROOT_CACHED;\n\trq->current_entity = NULL;\n\trq->sched = sched;\n}\n\n \nvoid drm_sched_rq_add_entity(struct drm_sched_rq *rq,\n\t\t\t     struct drm_sched_entity *entity)\n{\n\tif (!list_empty(&entity->list))\n\t\treturn;\n\n\tspin_lock(&rq->lock);\n\n\tatomic_inc(rq->sched->score);\n\tlist_add_tail(&entity->list, &rq->entities);\n\n\tspin_unlock(&rq->lock);\n}\n\n \nvoid drm_sched_rq_remove_entity(struct drm_sched_rq *rq,\n\t\t\t\tstruct drm_sched_entity *entity)\n{\n\tif (list_empty(&entity->list))\n\t\treturn;\n\n\tspin_lock(&rq->lock);\n\n\tatomic_dec(rq->sched->score);\n\tlist_del_init(&entity->list);\n\n\tif (rq->current_entity == entity)\n\t\trq->current_entity = NULL;\n\n\tif (drm_sched_policy == DRM_SCHED_POLICY_FIFO)\n\t\tdrm_sched_rq_remove_fifo_locked(entity);\n\n\tspin_unlock(&rq->lock);\n}\n\n \nstatic struct drm_sched_entity *\ndrm_sched_rq_select_entity_rr(struct drm_sched_rq *rq)\n{\n\tstruct drm_sched_entity *entity;\n\n\tspin_lock(&rq->lock);\n\n\tentity = rq->current_entity;\n\tif (entity) {\n\t\tlist_for_each_entry_continue(entity, &rq->entities, list) {\n\t\t\tif (drm_sched_entity_is_ready(entity)) {\n\t\t\t\trq->current_entity = entity;\n\t\t\t\treinit_completion(&entity->entity_idle);\n\t\t\t\tspin_unlock(&rq->lock);\n\t\t\t\treturn entity;\n\t\t\t}\n\t\t}\n\t}\n\n\tlist_for_each_entry(entity, &rq->entities, list) {\n\n\t\tif (drm_sched_entity_is_ready(entity)) {\n\t\t\trq->current_entity = entity;\n\t\t\treinit_completion(&entity->entity_idle);\n\t\t\tspin_unlock(&rq->lock);\n\t\t\treturn entity;\n\t\t}\n\n\t\tif (entity == rq->current_entity)\n\t\t\tbreak;\n\t}\n\n\tspin_unlock(&rq->lock);\n\n\treturn NULL;\n}\n\n \nstatic struct drm_sched_entity *\ndrm_sched_rq_select_entity_fifo(struct drm_sched_rq *rq)\n{\n\tstruct rb_node *rb;\n\n\tspin_lock(&rq->lock);\n\tfor (rb = rb_first_cached(&rq->rb_tree_root); rb; rb = rb_next(rb)) {\n\t\tstruct drm_sched_entity *entity;\n\n\t\tentity = rb_entry(rb, struct drm_sched_entity, rb_tree_node);\n\t\tif (drm_sched_entity_is_ready(entity)) {\n\t\t\trq->current_entity = entity;\n\t\t\treinit_completion(&entity->entity_idle);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&rq->lock);\n\n\treturn rb ? rb_entry(rb, struct drm_sched_entity, rb_tree_node) : NULL;\n}\n\n \nstatic void drm_sched_job_done(struct drm_sched_job *s_job, int result)\n{\n\tstruct drm_sched_fence *s_fence = s_job->s_fence;\n\tstruct drm_gpu_scheduler *sched = s_fence->sched;\n\n\tatomic_dec(&sched->hw_rq_count);\n\tatomic_dec(sched->score);\n\n\ttrace_drm_sched_process_job(s_fence);\n\n\tdma_fence_get(&s_fence->finished);\n\tdrm_sched_fence_finished(s_fence, result);\n\tdma_fence_put(&s_fence->finished);\n\twake_up_interruptible(&sched->wake_up_worker);\n}\n\n \nstatic void drm_sched_job_done_cb(struct dma_fence *f, struct dma_fence_cb *cb)\n{\n\tstruct drm_sched_job *s_job = container_of(cb, struct drm_sched_job, cb);\n\n\tdrm_sched_job_done(s_job, f->error);\n}\n\n \nstatic void drm_sched_start_timeout(struct drm_gpu_scheduler *sched)\n{\n\tif (sched->timeout != MAX_SCHEDULE_TIMEOUT &&\n\t    !list_empty(&sched->pending_list))\n\t\tqueue_delayed_work(sched->timeout_wq, &sched->work_tdr, sched->timeout);\n}\n\n \nvoid drm_sched_fault(struct drm_gpu_scheduler *sched)\n{\n\tif (sched->timeout_wq)\n\t\tmod_delayed_work(sched->timeout_wq, &sched->work_tdr, 0);\n}\nEXPORT_SYMBOL(drm_sched_fault);\n\n \nunsigned long drm_sched_suspend_timeout(struct drm_gpu_scheduler *sched)\n{\n\tunsigned long sched_timeout, now = jiffies;\n\n\tsched_timeout = sched->work_tdr.timer.expires;\n\n\t \n\tif (mod_delayed_work(sched->timeout_wq, &sched->work_tdr, MAX_SCHEDULE_TIMEOUT)\n\t\t\t&& time_after(sched_timeout, now))\n\t\treturn sched_timeout - now;\n\telse\n\t\treturn sched->timeout;\n}\nEXPORT_SYMBOL(drm_sched_suspend_timeout);\n\n \nvoid drm_sched_resume_timeout(struct drm_gpu_scheduler *sched,\n\t\tunsigned long remaining)\n{\n\tspin_lock(&sched->job_list_lock);\n\n\tif (list_empty(&sched->pending_list))\n\t\tcancel_delayed_work(&sched->work_tdr);\n\telse\n\t\tmod_delayed_work(sched->timeout_wq, &sched->work_tdr, remaining);\n\n\tspin_unlock(&sched->job_list_lock);\n}\nEXPORT_SYMBOL(drm_sched_resume_timeout);\n\nstatic void drm_sched_job_begin(struct drm_sched_job *s_job)\n{\n\tstruct drm_gpu_scheduler *sched = s_job->sched;\n\n\tspin_lock(&sched->job_list_lock);\n\tlist_add_tail(&s_job->list, &sched->pending_list);\n\tdrm_sched_start_timeout(sched);\n\tspin_unlock(&sched->job_list_lock);\n}\n\nstatic void drm_sched_job_timedout(struct work_struct *work)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_sched_job *job;\n\tenum drm_gpu_sched_stat status = DRM_GPU_SCHED_STAT_NOMINAL;\n\n\tsched = container_of(work, struct drm_gpu_scheduler, work_tdr.work);\n\n\t \n\tspin_lock(&sched->job_list_lock);\n\tjob = list_first_entry_or_null(&sched->pending_list,\n\t\t\t\t       struct drm_sched_job, list);\n\n\tif (job) {\n\t\t \n\t\tlist_del_init(&job->list);\n\t\tspin_unlock(&sched->job_list_lock);\n\n\t\tstatus = job->sched->ops->timedout_job(job);\n\n\t\t \n\t\tif (sched->free_guilty) {\n\t\t\tjob->sched->ops->free_job(job);\n\t\t\tsched->free_guilty = false;\n\t\t}\n\t} else {\n\t\tspin_unlock(&sched->job_list_lock);\n\t}\n\n\tif (status != DRM_GPU_SCHED_STAT_ENODEV) {\n\t\tspin_lock(&sched->job_list_lock);\n\t\tdrm_sched_start_timeout(sched);\n\t\tspin_unlock(&sched->job_list_lock);\n\t}\n}\n\n \nvoid drm_sched_stop(struct drm_gpu_scheduler *sched, struct drm_sched_job *bad)\n{\n\tstruct drm_sched_job *s_job, *tmp;\n\n\tkthread_park(sched->thread);\n\n\t \n\tif (bad && bad->sched == sched)\n\t\t \n\t\tlist_add(&bad->list, &sched->pending_list);\n\n\t \n\tlist_for_each_entry_safe_reverse(s_job, tmp, &sched->pending_list,\n\t\t\t\t\t list) {\n\t\tif (s_job->s_fence->parent &&\n\t\t    dma_fence_remove_callback(s_job->s_fence->parent,\n\t\t\t\t\t      &s_job->cb)) {\n\t\t\tdma_fence_put(s_job->s_fence->parent);\n\t\t\ts_job->s_fence->parent = NULL;\n\t\t\tatomic_dec(&sched->hw_rq_count);\n\t\t} else {\n\t\t\t \n\t\t\tspin_lock(&sched->job_list_lock);\n\t\t\tlist_del_init(&s_job->list);\n\t\t\tspin_unlock(&sched->job_list_lock);\n\n\t\t\t \n\t\t\tdma_fence_wait(&s_job->s_fence->finished, false);\n\n\t\t\t \n\t\t\tif (bad != s_job)\n\t\t\t\tsched->ops->free_job(s_job);\n\t\t\telse\n\t\t\t\tsched->free_guilty = true;\n\t\t}\n\t}\n\n\t \n\tcancel_delayed_work(&sched->work_tdr);\n}\n\nEXPORT_SYMBOL(drm_sched_stop);\n\n \nvoid drm_sched_start(struct drm_gpu_scheduler *sched, bool full_recovery)\n{\n\tstruct drm_sched_job *s_job, *tmp;\n\tint r;\n\n\t \n\tlist_for_each_entry_safe(s_job, tmp, &sched->pending_list, list) {\n\t\tstruct dma_fence *fence = s_job->s_fence->parent;\n\n\t\tatomic_inc(&sched->hw_rq_count);\n\n\t\tif (!full_recovery)\n\t\t\tcontinue;\n\n\t\tif (fence) {\n\t\t\tr = dma_fence_add_callback(fence, &s_job->cb,\n\t\t\t\t\t\t   drm_sched_job_done_cb);\n\t\t\tif (r == -ENOENT)\n\t\t\t\tdrm_sched_job_done(s_job, fence->error);\n\t\t\telse if (r)\n\t\t\t\tDRM_DEV_ERROR(sched->dev, \"fence add callback failed (%d)\\n\",\n\t\t\t\t\t  r);\n\t\t} else\n\t\t\tdrm_sched_job_done(s_job, -ECANCELED);\n\t}\n\n\tif (full_recovery) {\n\t\tspin_lock(&sched->job_list_lock);\n\t\tdrm_sched_start_timeout(sched);\n\t\tspin_unlock(&sched->job_list_lock);\n\t}\n\n\tkthread_unpark(sched->thread);\n}\nEXPORT_SYMBOL(drm_sched_start);\n\n \nvoid drm_sched_resubmit_jobs(struct drm_gpu_scheduler *sched)\n{\n\tstruct drm_sched_job *s_job, *tmp;\n\tuint64_t guilty_context;\n\tbool found_guilty = false;\n\tstruct dma_fence *fence;\n\n\tlist_for_each_entry_safe(s_job, tmp, &sched->pending_list, list) {\n\t\tstruct drm_sched_fence *s_fence = s_job->s_fence;\n\n\t\tif (!found_guilty && atomic_read(&s_job->karma) > sched->hang_limit) {\n\t\t\tfound_guilty = true;\n\t\t\tguilty_context = s_job->s_fence->scheduled.context;\n\t\t}\n\n\t\tif (found_guilty && s_job->s_fence->scheduled.context == guilty_context)\n\t\t\tdma_fence_set_error(&s_fence->finished, -ECANCELED);\n\n\t\tfence = sched->ops->run_job(s_job);\n\n\t\tif (IS_ERR_OR_NULL(fence)) {\n\t\t\tif (IS_ERR(fence))\n\t\t\t\tdma_fence_set_error(&s_fence->finished, PTR_ERR(fence));\n\n\t\t\ts_job->s_fence->parent = NULL;\n\t\t} else {\n\n\t\t\ts_job->s_fence->parent = dma_fence_get(fence);\n\n\t\t\t \n\t\t\tdma_fence_put(fence);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(drm_sched_resubmit_jobs);\n\n \nint drm_sched_job_init(struct drm_sched_job *job,\n\t\t       struct drm_sched_entity *entity,\n\t\t       void *owner)\n{\n\tif (!entity->rq)\n\t\treturn -ENOENT;\n\n\tjob->entity = entity;\n\tjob->s_fence = drm_sched_fence_alloc(entity, owner);\n\tif (!job->s_fence)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&job->list);\n\n\txa_init_flags(&job->dependencies, XA_FLAGS_ALLOC);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_sched_job_init);\n\n \nvoid drm_sched_job_arm(struct drm_sched_job *job)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_sched_entity *entity = job->entity;\n\n\tBUG_ON(!entity);\n\tdrm_sched_entity_select_rq(entity);\n\tsched = entity->rq->sched;\n\n\tjob->sched = sched;\n\tjob->s_priority = entity->rq - sched->sched_rq;\n\tjob->id = atomic64_inc_return(&sched->job_id_count);\n\n\tdrm_sched_fence_init(job->s_fence, job->entity);\n}\nEXPORT_SYMBOL(drm_sched_job_arm);\n\n \nint drm_sched_job_add_dependency(struct drm_sched_job *job,\n\t\t\t\t struct dma_fence *fence)\n{\n\tstruct dma_fence *entry;\n\tunsigned long index;\n\tu32 id = 0;\n\tint ret;\n\n\tif (!fence)\n\t\treturn 0;\n\n\t \n\txa_for_each(&job->dependencies, index, entry) {\n\t\tif (entry->context != fence->context)\n\t\t\tcontinue;\n\n\t\tif (dma_fence_is_later(fence, entry)) {\n\t\t\tdma_fence_put(entry);\n\t\t\txa_store(&job->dependencies, index, fence, GFP_KERNEL);\n\t\t} else {\n\t\t\tdma_fence_put(fence);\n\t\t}\n\t\treturn 0;\n\t}\n\n\tret = xa_alloc(&job->dependencies, &id, fence, xa_limit_32b, GFP_KERNEL);\n\tif (ret != 0)\n\t\tdma_fence_put(fence);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_sched_job_add_dependency);\n\n \nint drm_sched_job_add_syncobj_dependency(struct drm_sched_job *job,\n\t\t\t\t\t struct drm_file *file,\n\t\t\t\t\t u32 handle,\n\t\t\t\t\t u32 point)\n{\n\tstruct dma_fence *fence;\n\tint ret;\n\n\tret = drm_syncobj_find_fence(file, handle, point, 0, &fence);\n\tif (ret)\n\t\treturn ret;\n\n\treturn drm_sched_job_add_dependency(job, fence);\n}\nEXPORT_SYMBOL(drm_sched_job_add_syncobj_dependency);\n\n \nint drm_sched_job_add_resv_dependencies(struct drm_sched_job *job,\n\t\t\t\t\tstruct dma_resv *resv,\n\t\t\t\t\tenum dma_resv_usage usage)\n{\n\tstruct dma_resv_iter cursor;\n\tstruct dma_fence *fence;\n\tint ret;\n\n\tdma_resv_assert_held(resv);\n\n\tdma_resv_for_each_fence(&cursor, resv, usage, fence) {\n\t\t \n\t\tdma_fence_get(fence);\n\t\tret = drm_sched_job_add_dependency(job, fence);\n\t\tif (ret) {\n\t\t\tdma_fence_put(fence);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_sched_job_add_resv_dependencies);\n\n \nint drm_sched_job_add_implicit_dependencies(struct drm_sched_job *job,\n\t\t\t\t\t    struct drm_gem_object *obj,\n\t\t\t\t\t    bool write)\n{\n\treturn drm_sched_job_add_resv_dependencies(job, obj->resv,\n\t\t\t\t\t\t   dma_resv_usage_rw(write));\n}\nEXPORT_SYMBOL(drm_sched_job_add_implicit_dependencies);\n\n \nvoid drm_sched_job_cleanup(struct drm_sched_job *job)\n{\n\tstruct dma_fence *fence;\n\tunsigned long index;\n\n\tif (kref_read(&job->s_fence->finished.refcount)) {\n\t\t \n\t\tdma_fence_put(&job->s_fence->finished);\n\t} else {\n\t\t \n\t\tdrm_sched_fence_free(job->s_fence);\n\t}\n\n\tjob->s_fence = NULL;\n\n\txa_for_each(&job->dependencies, index, fence) {\n\t\tdma_fence_put(fence);\n\t}\n\txa_destroy(&job->dependencies);\n\n}\nEXPORT_SYMBOL(drm_sched_job_cleanup);\n\n \nstatic bool drm_sched_can_queue(struct drm_gpu_scheduler *sched)\n{\n\treturn atomic_read(&sched->hw_rq_count) <\n\t\tsched->hw_submission_limit;\n}\n\n \nvoid drm_sched_wakeup_if_can_queue(struct drm_gpu_scheduler *sched)\n{\n\tif (drm_sched_can_queue(sched))\n\t\twake_up_interruptible(&sched->wake_up_worker);\n}\n\n \nstatic struct drm_sched_entity *\ndrm_sched_select_entity(struct drm_gpu_scheduler *sched)\n{\n\tstruct drm_sched_entity *entity;\n\tint i;\n\n\tif (!drm_sched_can_queue(sched))\n\t\treturn NULL;\n\n\t \n\tfor (i = DRM_SCHED_PRIORITY_COUNT - 1; i >= DRM_SCHED_PRIORITY_MIN; i--) {\n\t\tentity = drm_sched_policy == DRM_SCHED_POLICY_FIFO ?\n\t\t\tdrm_sched_rq_select_entity_fifo(&sched->sched_rq[i]) :\n\t\t\tdrm_sched_rq_select_entity_rr(&sched->sched_rq[i]);\n\t\tif (entity)\n\t\t\tbreak;\n\t}\n\n\treturn entity;\n}\n\n \nstatic struct drm_sched_job *\ndrm_sched_get_cleanup_job(struct drm_gpu_scheduler *sched)\n{\n\tstruct drm_sched_job *job, *next;\n\n\tspin_lock(&sched->job_list_lock);\n\n\tjob = list_first_entry_or_null(&sched->pending_list,\n\t\t\t\t       struct drm_sched_job, list);\n\n\tif (job && dma_fence_is_signaled(&job->s_fence->finished)) {\n\t\t \n\t\tlist_del_init(&job->list);\n\n\t\t \n\t\tcancel_delayed_work(&sched->work_tdr);\n\t\t \n\t\tnext = list_first_entry_or_null(&sched->pending_list,\n\t\t\t\t\t\ttypeof(*next), list);\n\n\t\tif (next) {\n\t\t\tnext->s_fence->scheduled.timestamp =\n\t\t\t\tdma_fence_timestamp(&job->s_fence->finished);\n\t\t\t \n\t\t\tdrm_sched_start_timeout(sched);\n\t\t}\n\t} else {\n\t\tjob = NULL;\n\t}\n\n\tspin_unlock(&sched->job_list_lock);\n\n\treturn job;\n}\n\n \nstruct drm_gpu_scheduler *\ndrm_sched_pick_best(struct drm_gpu_scheduler **sched_list,\n\t\t     unsigned int num_sched_list)\n{\n\tstruct drm_gpu_scheduler *sched, *picked_sched = NULL;\n\tint i;\n\tunsigned int min_score = UINT_MAX, num_score;\n\n\tfor (i = 0; i < num_sched_list; ++i) {\n\t\tsched = sched_list[i];\n\n\t\tif (!sched->ready) {\n\t\t\tDRM_WARN(\"scheduler %s is not ready, skipping\",\n\t\t\t\t sched->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tnum_score = atomic_read(sched->score);\n\t\tif (num_score < min_score) {\n\t\t\tmin_score = num_score;\n\t\t\tpicked_sched = sched;\n\t\t}\n\t}\n\n\treturn picked_sched;\n}\nEXPORT_SYMBOL(drm_sched_pick_best);\n\n \nstatic bool drm_sched_blocked(struct drm_gpu_scheduler *sched)\n{\n\tif (kthread_should_park()) {\n\t\tkthread_parkme();\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int drm_sched_main(void *param)\n{\n\tstruct drm_gpu_scheduler *sched = (struct drm_gpu_scheduler *)param;\n\tint r;\n\n\tsched_set_fifo_low(current);\n\n\twhile (!kthread_should_stop()) {\n\t\tstruct drm_sched_entity *entity = NULL;\n\t\tstruct drm_sched_fence *s_fence;\n\t\tstruct drm_sched_job *sched_job;\n\t\tstruct dma_fence *fence;\n\t\tstruct drm_sched_job *cleanup_job = NULL;\n\n\t\twait_event_interruptible(sched->wake_up_worker,\n\t\t\t\t\t (cleanup_job = drm_sched_get_cleanup_job(sched)) ||\n\t\t\t\t\t (!drm_sched_blocked(sched) &&\n\t\t\t\t\t  (entity = drm_sched_select_entity(sched))) ||\n\t\t\t\t\t kthread_should_stop());\n\n\t\tif (cleanup_job)\n\t\t\tsched->ops->free_job(cleanup_job);\n\n\t\tif (!entity)\n\t\t\tcontinue;\n\n\t\tsched_job = drm_sched_entity_pop_job(entity);\n\n\t\tif (!sched_job) {\n\t\t\tcomplete_all(&entity->entity_idle);\n\t\t\tcontinue;\n\t\t}\n\n\t\ts_fence = sched_job->s_fence;\n\n\t\tatomic_inc(&sched->hw_rq_count);\n\t\tdrm_sched_job_begin(sched_job);\n\n\t\ttrace_drm_run_job(sched_job, entity);\n\t\tfence = sched->ops->run_job(sched_job);\n\t\tcomplete_all(&entity->entity_idle);\n\t\tdrm_sched_fence_scheduled(s_fence, fence);\n\n\t\tif (!IS_ERR_OR_NULL(fence)) {\n\t\t\t \n\t\t\tdma_fence_put(fence);\n\n\t\t\tr = dma_fence_add_callback(fence, &sched_job->cb,\n\t\t\t\t\t\t   drm_sched_job_done_cb);\n\t\t\tif (r == -ENOENT)\n\t\t\t\tdrm_sched_job_done(sched_job, fence->error);\n\t\t\telse if (r)\n\t\t\t\tDRM_DEV_ERROR(sched->dev, \"fence add callback failed (%d)\\n\",\n\t\t\t\t\t  r);\n\t\t} else {\n\t\t\tdrm_sched_job_done(sched_job, IS_ERR(fence) ?\n\t\t\t\t\t   PTR_ERR(fence) : 0);\n\t\t}\n\n\t\twake_up(&sched->job_scheduled);\n\t}\n\treturn 0;\n}\n\n \nint drm_sched_init(struct drm_gpu_scheduler *sched,\n\t\t   const struct drm_sched_backend_ops *ops,\n\t\t   unsigned hw_submission, unsigned hang_limit,\n\t\t   long timeout, struct workqueue_struct *timeout_wq,\n\t\t   atomic_t *score, const char *name, struct device *dev)\n{\n\tint i, ret;\n\tsched->ops = ops;\n\tsched->hw_submission_limit = hw_submission;\n\tsched->name = name;\n\tsched->timeout = timeout;\n\tsched->timeout_wq = timeout_wq ? : system_wq;\n\tsched->hang_limit = hang_limit;\n\tsched->score = score ? score : &sched->_score;\n\tsched->dev = dev;\n\tfor (i = DRM_SCHED_PRIORITY_MIN; i < DRM_SCHED_PRIORITY_COUNT; i++)\n\t\tdrm_sched_rq_init(sched, &sched->sched_rq[i]);\n\n\tinit_waitqueue_head(&sched->wake_up_worker);\n\tinit_waitqueue_head(&sched->job_scheduled);\n\tINIT_LIST_HEAD(&sched->pending_list);\n\tspin_lock_init(&sched->job_list_lock);\n\tatomic_set(&sched->hw_rq_count, 0);\n\tINIT_DELAYED_WORK(&sched->work_tdr, drm_sched_job_timedout);\n\tatomic_set(&sched->_score, 0);\n\tatomic64_set(&sched->job_id_count, 0);\n\n\t \n\tsched->thread = kthread_run(drm_sched_main, sched, sched->name);\n\tif (IS_ERR(sched->thread)) {\n\t\tret = PTR_ERR(sched->thread);\n\t\tsched->thread = NULL;\n\t\tDRM_DEV_ERROR(sched->dev, \"Failed to create scheduler for %s.\\n\", name);\n\t\treturn ret;\n\t}\n\n\tsched->ready = true;\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_sched_init);\n\n \nvoid drm_sched_fini(struct drm_gpu_scheduler *sched)\n{\n\tstruct drm_sched_entity *s_entity;\n\tint i;\n\n\tif (sched->thread)\n\t\tkthread_stop(sched->thread);\n\n\tfor (i = DRM_SCHED_PRIORITY_COUNT - 1; i >= DRM_SCHED_PRIORITY_MIN; i--) {\n\t\tstruct drm_sched_rq *rq = &sched->sched_rq[i];\n\n\t\tspin_lock(&rq->lock);\n\t\tlist_for_each_entry(s_entity, &rq->entities, list)\n\t\t\t \n\t\t\ts_entity->stopped = true;\n\t\tspin_unlock(&rq->lock);\n\n\t}\n\n\t \n\twake_up_all(&sched->job_scheduled);\n\n\t \n\tcancel_delayed_work_sync(&sched->work_tdr);\n\n\tsched->ready = false;\n}\nEXPORT_SYMBOL(drm_sched_fini);\n\n \nvoid drm_sched_increase_karma(struct drm_sched_job *bad)\n{\n\tint i;\n\tstruct drm_sched_entity *tmp;\n\tstruct drm_sched_entity *entity;\n\tstruct drm_gpu_scheduler *sched = bad->sched;\n\n\t \n\tif (bad->s_priority != DRM_SCHED_PRIORITY_KERNEL) {\n\t\tatomic_inc(&bad->karma);\n\n\t\tfor (i = DRM_SCHED_PRIORITY_MIN; i < DRM_SCHED_PRIORITY_KERNEL;\n\t\t     i++) {\n\t\t\tstruct drm_sched_rq *rq = &sched->sched_rq[i];\n\n\t\t\tspin_lock(&rq->lock);\n\t\t\tlist_for_each_entry_safe(entity, tmp, &rq->entities, list) {\n\t\t\t\tif (bad->s_fence->scheduled.context ==\n\t\t\t\t    entity->fence_context) {\n\t\t\t\t\tif (entity->guilty)\n\t\t\t\t\t\tatomic_set(entity->guilty, 1);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock(&rq->lock);\n\t\t\tif (&entity->list != &rq->entities)\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(drm_sched_increase_karma);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}