{
  "module_name": "sched_fence.c",
  "hash_id": "a494c1c730bf88aa85969c27a63af91f7b88caa9d1304c8d8a967c8bf671b564",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/scheduler/sched_fence.c",
  "human_readable_source": " \n\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/wait.h>\n\n#include <drm/gpu_scheduler.h>\n\nstatic struct kmem_cache *sched_fence_slab;\n\nstatic int __init drm_sched_fence_slab_init(void)\n{\n\tsched_fence_slab = kmem_cache_create(\n\t\t\"drm_sched_fence\", sizeof(struct drm_sched_fence), 0,\n\t\tSLAB_HWCACHE_ALIGN, NULL);\n\tif (!sched_fence_slab)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void __exit drm_sched_fence_slab_fini(void)\n{\n\trcu_barrier();\n\tkmem_cache_destroy(sched_fence_slab);\n}\n\nstatic void drm_sched_fence_set_parent(struct drm_sched_fence *s_fence,\n\t\t\t\t       struct dma_fence *fence)\n{\n\t \n\tsmp_store_release(&s_fence->parent, dma_fence_get(fence));\n\tif (test_bit(DRM_SCHED_FENCE_FLAG_HAS_DEADLINE_BIT,\n\t\t     &s_fence->finished.flags))\n\t\tdma_fence_set_deadline(fence, s_fence->deadline);\n}\n\nvoid drm_sched_fence_scheduled(struct drm_sched_fence *fence,\n\t\t\t       struct dma_fence *parent)\n{\n\t \n\tif (!IS_ERR_OR_NULL(parent))\n\t\tdrm_sched_fence_set_parent(fence, parent);\n\n\tdma_fence_signal(&fence->scheduled);\n}\n\nvoid drm_sched_fence_finished(struct drm_sched_fence *fence, int result)\n{\n\tif (result)\n\t\tdma_fence_set_error(&fence->finished, result);\n\tdma_fence_signal(&fence->finished);\n}\n\nstatic const char *drm_sched_fence_get_driver_name(struct dma_fence *fence)\n{\n\treturn \"drm_sched\";\n}\n\nstatic const char *drm_sched_fence_get_timeline_name(struct dma_fence *f)\n{\n\tstruct drm_sched_fence *fence = to_drm_sched_fence(f);\n\treturn (const char *)fence->sched->name;\n}\n\nstatic void drm_sched_fence_free_rcu(struct rcu_head *rcu)\n{\n\tstruct dma_fence *f = container_of(rcu, struct dma_fence, rcu);\n\tstruct drm_sched_fence *fence = to_drm_sched_fence(f);\n\n\tif (!WARN_ON_ONCE(!fence))\n\t\tkmem_cache_free(sched_fence_slab, fence);\n}\n\n \nvoid drm_sched_fence_free(struct drm_sched_fence *fence)\n{\n\t \n\tif (!WARN_ON_ONCE(fence->sched))\n\t\tkmem_cache_free(sched_fence_slab, fence);\n}\n\n \nstatic void drm_sched_fence_release_scheduled(struct dma_fence *f)\n{\n\tstruct drm_sched_fence *fence = to_drm_sched_fence(f);\n\n\tdma_fence_put(fence->parent);\n\tcall_rcu(&fence->finished.rcu, drm_sched_fence_free_rcu);\n}\n\n \nstatic void drm_sched_fence_release_finished(struct dma_fence *f)\n{\n\tstruct drm_sched_fence *fence = to_drm_sched_fence(f);\n\n\tdma_fence_put(&fence->scheduled);\n}\n\nstatic void drm_sched_fence_set_deadline_finished(struct dma_fence *f,\n\t\t\t\t\t\t  ktime_t deadline)\n{\n\tstruct drm_sched_fence *fence = to_drm_sched_fence(f);\n\tstruct dma_fence *parent;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fence->lock, flags);\n\n\t \n\tif (test_bit(DRM_SCHED_FENCE_FLAG_HAS_DEADLINE_BIT, &f->flags) &&\n\t    ktime_before(fence->deadline, deadline)) {\n\t\tspin_unlock_irqrestore(&fence->lock, flags);\n\t\treturn;\n\t}\n\n\tfence->deadline = deadline;\n\tset_bit(DRM_SCHED_FENCE_FLAG_HAS_DEADLINE_BIT, &f->flags);\n\n\tspin_unlock_irqrestore(&fence->lock, flags);\n\n\t \n\tparent = smp_load_acquire(&fence->parent);\n\tif (parent)\n\t\tdma_fence_set_deadline(parent, deadline);\n}\n\nstatic const struct dma_fence_ops drm_sched_fence_ops_scheduled = {\n\t.get_driver_name = drm_sched_fence_get_driver_name,\n\t.get_timeline_name = drm_sched_fence_get_timeline_name,\n\t.release = drm_sched_fence_release_scheduled,\n};\n\nstatic const struct dma_fence_ops drm_sched_fence_ops_finished = {\n\t.get_driver_name = drm_sched_fence_get_driver_name,\n\t.get_timeline_name = drm_sched_fence_get_timeline_name,\n\t.release = drm_sched_fence_release_finished,\n\t.set_deadline = drm_sched_fence_set_deadline_finished,\n};\n\nstruct drm_sched_fence *to_drm_sched_fence(struct dma_fence *f)\n{\n\tif (f->ops == &drm_sched_fence_ops_scheduled)\n\t\treturn container_of(f, struct drm_sched_fence, scheduled);\n\n\tif (f->ops == &drm_sched_fence_ops_finished)\n\t\treturn container_of(f, struct drm_sched_fence, finished);\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(to_drm_sched_fence);\n\nstruct drm_sched_fence *drm_sched_fence_alloc(struct drm_sched_entity *entity,\n\t\t\t\t\t      void *owner)\n{\n\tstruct drm_sched_fence *fence = NULL;\n\n\tfence = kmem_cache_zalloc(sched_fence_slab, GFP_KERNEL);\n\tif (fence == NULL)\n\t\treturn NULL;\n\n\tfence->owner = owner;\n\tspin_lock_init(&fence->lock);\n\n\treturn fence;\n}\n\nvoid drm_sched_fence_init(struct drm_sched_fence *fence,\n\t\t\t  struct drm_sched_entity *entity)\n{\n\tunsigned seq;\n\n\tfence->sched = entity->rq->sched;\n\tseq = atomic_inc_return(&entity->fence_seq);\n\tdma_fence_init(&fence->scheduled, &drm_sched_fence_ops_scheduled,\n\t\t       &fence->lock, entity->fence_context, seq);\n\tdma_fence_init(&fence->finished, &drm_sched_fence_ops_finished,\n\t\t       &fence->lock, entity->fence_context + 1, seq);\n}\n\nmodule_init(drm_sched_fence_slab_init);\nmodule_exit(drm_sched_fence_slab_fini);\n\nMODULE_DESCRIPTION(\"DRM GPU scheduler\");\nMODULE_LICENSE(\"GPL and additional rights\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}