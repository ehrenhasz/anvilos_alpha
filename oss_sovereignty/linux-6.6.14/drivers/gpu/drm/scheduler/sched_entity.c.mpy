{
  "module_name": "sched_entity.c",
  "hash_id": "3aff34f30918501e19fd3442c22119cf3b6aea3e85d6b72c471e1f99985393cf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/scheduler/sched_entity.c",
  "human_readable_source": " \n\n#include <linux/kthread.h>\n#include <linux/slab.h>\n#include <linux/completion.h>\n\n#include <drm/drm_print.h>\n#include <drm/gpu_scheduler.h>\n\n#include \"gpu_scheduler_trace.h\"\n\n#define to_drm_sched_job(sched_job)\t\t\\\n\t\tcontainer_of((sched_job), struct drm_sched_job, queue_node)\n\n \nint drm_sched_entity_init(struct drm_sched_entity *entity,\n\t\t\t  enum drm_sched_priority priority,\n\t\t\t  struct drm_gpu_scheduler **sched_list,\n\t\t\t  unsigned int num_sched_list,\n\t\t\t  atomic_t *guilty)\n{\n\tif (!(entity && sched_list && (num_sched_list == 0 || sched_list[0])))\n\t\treturn -EINVAL;\n\n\tmemset(entity, 0, sizeof(struct drm_sched_entity));\n\tINIT_LIST_HEAD(&entity->list);\n\tentity->rq = NULL;\n\tentity->guilty = guilty;\n\tentity->num_sched_list = num_sched_list;\n\tentity->priority = priority;\n\tentity->sched_list = num_sched_list > 1 ? sched_list : NULL;\n\tRCU_INIT_POINTER(entity->last_scheduled, NULL);\n\tRB_CLEAR_NODE(&entity->rb_tree_node);\n\n\tif(num_sched_list)\n\t\tentity->rq = &sched_list[0]->sched_rq[entity->priority];\n\n\tinit_completion(&entity->entity_idle);\n\n\t \n\tcomplete_all(&entity->entity_idle);\n\n\tspin_lock_init(&entity->rq_lock);\n\tspsc_queue_init(&entity->job_queue);\n\n\tatomic_set(&entity->fence_seq, 0);\n\tentity->fence_context = dma_fence_context_alloc(2);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_sched_entity_init);\n\n \nvoid drm_sched_entity_modify_sched(struct drm_sched_entity *entity,\n\t\t\t\t    struct drm_gpu_scheduler **sched_list,\n\t\t\t\t    unsigned int num_sched_list)\n{\n\tWARN_ON(!num_sched_list || !sched_list);\n\n\tentity->sched_list = sched_list;\n\tentity->num_sched_list = num_sched_list;\n}\nEXPORT_SYMBOL(drm_sched_entity_modify_sched);\n\nstatic bool drm_sched_entity_is_idle(struct drm_sched_entity *entity)\n{\n\trmb();  \n\n\tif (list_empty(&entity->list) ||\n\t    spsc_queue_count(&entity->job_queue) == 0 ||\n\t    entity->stopped)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nbool drm_sched_entity_is_ready(struct drm_sched_entity *entity)\n{\n\tif (spsc_queue_peek(&entity->job_queue) == NULL)\n\t\treturn false;\n\n\tif (READ_ONCE(entity->dependency))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nint drm_sched_entity_error(struct drm_sched_entity *entity)\n{\n\tstruct dma_fence *fence;\n\tint r;\n\n\trcu_read_lock();\n\tfence = rcu_dereference(entity->last_scheduled);\n\tr = fence ? fence->error : 0;\n\trcu_read_unlock();\n\n\treturn r;\n}\nEXPORT_SYMBOL(drm_sched_entity_error);\n\nstatic void drm_sched_entity_kill_jobs_work(struct work_struct *wrk)\n{\n\tstruct drm_sched_job *job = container_of(wrk, typeof(*job), work);\n\n\tdrm_sched_fence_finished(job->s_fence, -ESRCH);\n\tWARN_ON(job->s_fence->parent);\n\tjob->sched->ops->free_job(job);\n}\n\n \nstatic void drm_sched_entity_kill_jobs_cb(struct dma_fence *f,\n\t\t\t\t\t  struct dma_fence_cb *cb)\n{\n\tstruct drm_sched_job *job = container_of(cb, struct drm_sched_job,\n\t\t\t\t\t\t finish_cb);\n\tunsigned long index;\n\n\tdma_fence_put(f);\n\n\t \n\txa_for_each(&job->dependencies, index, f) {\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(f);\n\n\t\tif (s_fence && f == &s_fence->scheduled) {\n\t\t\t \n\t\t\tf = dma_fence_get_rcu(&s_fence->finished);\n\n\t\t\t \n\t\t\tdma_fence_put(&s_fence->scheduled);\n\t\t}\n\n\t\txa_erase(&job->dependencies, index);\n\t\tif (f && !dma_fence_add_callback(f, &job->finish_cb,\n\t\t\t\t\t\t drm_sched_entity_kill_jobs_cb))\n\t\t\treturn;\n\n\t\tdma_fence_put(f);\n\t}\n\n\tINIT_WORK(&job->work, drm_sched_entity_kill_jobs_work);\n\tschedule_work(&job->work);\n}\n\n \nstatic void drm_sched_entity_kill(struct drm_sched_entity *entity)\n{\n\tstruct drm_sched_job *job;\n\tstruct dma_fence *prev;\n\n\tif (!entity->rq)\n\t\treturn;\n\n\tspin_lock(&entity->rq_lock);\n\tentity->stopped = true;\n\tdrm_sched_rq_remove_entity(entity->rq, entity);\n\tspin_unlock(&entity->rq_lock);\n\n\t \n\twait_for_completion(&entity->entity_idle);\n\n\t \n\tprev = rcu_dereference_check(entity->last_scheduled, true);\n\tdma_fence_get(prev);\n\twhile ((job = to_drm_sched_job(spsc_queue_pop(&entity->job_queue)))) {\n\t\tstruct drm_sched_fence *s_fence = job->s_fence;\n\n\t\tdma_fence_get(&s_fence->finished);\n\t\tif (!prev || dma_fence_add_callback(prev, &job->finish_cb,\n\t\t\t\t\t   drm_sched_entity_kill_jobs_cb))\n\t\t\tdrm_sched_entity_kill_jobs_cb(NULL, &job->finish_cb);\n\n\t\tprev = &s_fence->finished;\n\t}\n\tdma_fence_put(prev);\n}\n\n \nlong drm_sched_entity_flush(struct drm_sched_entity *entity, long timeout)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tstruct task_struct *last_user;\n\tlong ret = timeout;\n\n\tif (!entity->rq)\n\t\treturn 0;\n\n\tsched = entity->rq->sched;\n\t \n\tif (current->flags & PF_EXITING) {\n\t\tif (timeout)\n\t\t\tret = wait_event_timeout(\n\t\t\t\t\tsched->job_scheduled,\n\t\t\t\t\tdrm_sched_entity_is_idle(entity),\n\t\t\t\t\ttimeout);\n\t} else {\n\t\twait_event_killable(sched->job_scheduled,\n\t\t\t\t    drm_sched_entity_is_idle(entity));\n\t}\n\n\t \n\tlast_user = cmpxchg(&entity->last_user, current->group_leader, NULL);\n\tif ((!last_user || last_user == current->group_leader) &&\n\t    (current->flags & PF_EXITING) && (current->exit_code == SIGKILL))\n\t\tdrm_sched_entity_kill(entity);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_sched_entity_flush);\n\n \nvoid drm_sched_entity_fini(struct drm_sched_entity *entity)\n{\n\t \n\tdrm_sched_entity_kill(entity);\n\n\tif (entity->dependency) {\n\t\tdma_fence_remove_callback(entity->dependency, &entity->cb);\n\t\tdma_fence_put(entity->dependency);\n\t\tentity->dependency = NULL;\n\t}\n\n\tdma_fence_put(rcu_dereference_check(entity->last_scheduled, true));\n\tRCU_INIT_POINTER(entity->last_scheduled, NULL);\n}\nEXPORT_SYMBOL(drm_sched_entity_fini);\n\n \nvoid drm_sched_entity_destroy(struct drm_sched_entity *entity)\n{\n\tdrm_sched_entity_flush(entity, MAX_WAIT_SCHED_ENTITY_Q_EMPTY);\n\tdrm_sched_entity_fini(entity);\n}\nEXPORT_SYMBOL(drm_sched_entity_destroy);\n\n \nstatic void drm_sched_entity_clear_dep(struct dma_fence *f,\n\t\t\t\t       struct dma_fence_cb *cb)\n{\n\tstruct drm_sched_entity *entity =\n\t\tcontainer_of(cb, struct drm_sched_entity, cb);\n\n\tentity->dependency = NULL;\n\tdma_fence_put(f);\n}\n\n \nstatic void drm_sched_entity_wakeup(struct dma_fence *f,\n\t\t\t\t    struct dma_fence_cb *cb)\n{\n\tstruct drm_sched_entity *entity =\n\t\tcontainer_of(cb, struct drm_sched_entity, cb);\n\n\tdrm_sched_entity_clear_dep(f, cb);\n\tdrm_sched_wakeup_if_can_queue(entity->rq->sched);\n}\n\n \nvoid drm_sched_entity_set_priority(struct drm_sched_entity *entity,\n\t\t\t\t   enum drm_sched_priority priority)\n{\n\tspin_lock(&entity->rq_lock);\n\tentity->priority = priority;\n\tspin_unlock(&entity->rq_lock);\n}\nEXPORT_SYMBOL(drm_sched_entity_set_priority);\n\n \nstatic bool drm_sched_entity_add_dependency_cb(struct drm_sched_entity *entity)\n{\n\tstruct drm_gpu_scheduler *sched = entity->rq->sched;\n\tstruct dma_fence *fence = entity->dependency;\n\tstruct drm_sched_fence *s_fence;\n\n\tif (fence->context == entity->fence_context ||\n\t    fence->context == entity->fence_context + 1) {\n\t\t \n\t\tdma_fence_put(entity->dependency);\n\t\treturn false;\n\t}\n\n\ts_fence = to_drm_sched_fence(fence);\n\tif (!fence->error && s_fence && s_fence->sched == sched &&\n\t    !test_bit(DRM_SCHED_FENCE_DONT_PIPELINE, &fence->flags)) {\n\n\t\t \n\t\tfence = dma_fence_get(&s_fence->scheduled);\n\t\tdma_fence_put(entity->dependency);\n\t\tentity->dependency = fence;\n\t\tif (!dma_fence_add_callback(fence, &entity->cb,\n\t\t\t\t\t    drm_sched_entity_clear_dep))\n\t\t\treturn true;\n\n\t\t \n\t\tdma_fence_put(fence);\n\t\treturn false;\n\t}\n\n\tif (!dma_fence_add_callback(entity->dependency, &entity->cb,\n\t\t\t\t    drm_sched_entity_wakeup))\n\t\treturn true;\n\n\tdma_fence_put(entity->dependency);\n\treturn false;\n}\n\nstatic struct dma_fence *\ndrm_sched_job_dependency(struct drm_sched_job *job,\n\t\t\t struct drm_sched_entity *entity)\n{\n\tstruct dma_fence *f;\n\n\t \n\tf = xa_load(&job->dependencies, job->last_dependency);\n\tif (f) {\n\t\tjob->last_dependency++;\n\t\treturn dma_fence_get(f);\n\t}\n\n\tif (job->sched->ops->prepare_job)\n\t\treturn job->sched->ops->prepare_job(job, entity);\n\n\treturn NULL;\n}\n\nstruct drm_sched_job *drm_sched_entity_pop_job(struct drm_sched_entity *entity)\n{\n\tstruct drm_sched_job *sched_job;\n\n\tsched_job = to_drm_sched_job(spsc_queue_peek(&entity->job_queue));\n\tif (!sched_job)\n\t\treturn NULL;\n\n\twhile ((entity->dependency =\n\t\t\tdrm_sched_job_dependency(sched_job, entity))) {\n\t\ttrace_drm_sched_job_wait_dep(sched_job, entity->dependency);\n\n\t\tif (drm_sched_entity_add_dependency_cb(entity))\n\t\t\treturn NULL;\n\t}\n\n\t \n\tif (entity->guilty && atomic_read(entity->guilty))\n\t\tdma_fence_set_error(&sched_job->s_fence->finished, -ECANCELED);\n\n\tdma_fence_put(rcu_dereference_check(entity->last_scheduled, true));\n\trcu_assign_pointer(entity->last_scheduled,\n\t\t\t   dma_fence_get(&sched_job->s_fence->finished));\n\n\t \n\tsmp_wmb();\n\n\tspsc_queue_pop(&entity->job_queue);\n\n\t \n\tif (drm_sched_policy == DRM_SCHED_POLICY_FIFO) {\n\t\tstruct drm_sched_job *next;\n\n\t\tnext = to_drm_sched_job(spsc_queue_peek(&entity->job_queue));\n\t\tif (next)\n\t\t\tdrm_sched_rq_update_fifo(entity, next->submit_ts);\n\t}\n\n\t \n\tsched_job->entity = NULL;\n\n\treturn sched_job;\n}\n\nvoid drm_sched_entity_select_rq(struct drm_sched_entity *entity)\n{\n\tstruct dma_fence *fence;\n\tstruct drm_gpu_scheduler *sched;\n\tstruct drm_sched_rq *rq;\n\n\t \n\tif (!entity->sched_list)\n\t\treturn;\n\n\t \n\tif (spsc_queue_count(&entity->job_queue))\n\t\treturn;\n\n\t \n\tsmp_rmb();\n\n\tfence = rcu_dereference_check(entity->last_scheduled, true);\n\n\t \n\tif (fence && !dma_fence_is_signaled(fence))\n\t\treturn;\n\n\tspin_lock(&entity->rq_lock);\n\tsched = drm_sched_pick_best(entity->sched_list, entity->num_sched_list);\n\trq = sched ? &sched->sched_rq[entity->priority] : NULL;\n\tif (rq != entity->rq) {\n\t\tdrm_sched_rq_remove_entity(entity->rq, entity);\n\t\tentity->rq = rq;\n\t}\n\tspin_unlock(&entity->rq_lock);\n\n\tif (entity->num_sched_list == 1)\n\t\tentity->sched_list = NULL;\n}\n\n \nvoid drm_sched_entity_push_job(struct drm_sched_job *sched_job)\n{\n\tstruct drm_sched_entity *entity = sched_job->entity;\n\tbool first;\n\tktime_t submit_ts;\n\n\ttrace_drm_sched_job(sched_job, entity);\n\tatomic_inc(entity->rq->sched->score);\n\tWRITE_ONCE(entity->last_user, current->group_leader);\n\n\t \n\tsched_job->submit_ts = submit_ts = ktime_get();\n\tfirst = spsc_queue_push(&entity->job_queue, &sched_job->queue_node);\n\n\t \n\tif (first) {\n\t\t \n\t\tspin_lock(&entity->rq_lock);\n\t\tif (entity->stopped) {\n\t\t\tspin_unlock(&entity->rq_lock);\n\n\t\t\tDRM_ERROR(\"Trying to push to a killed entity\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tdrm_sched_rq_add_entity(entity->rq, entity);\n\t\tspin_unlock(&entity->rq_lock);\n\n\t\tif (drm_sched_policy == DRM_SCHED_POLICY_FIFO)\n\t\t\tdrm_sched_rq_update_fifo(entity, submit_ts);\n\n\t\tdrm_sched_wakeup_if_can_queue(entity->rq->sched);\n\t}\n}\nEXPORT_SYMBOL(drm_sched_entity_push_job);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}