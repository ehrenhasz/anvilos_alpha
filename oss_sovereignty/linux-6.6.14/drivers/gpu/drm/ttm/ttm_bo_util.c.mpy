{
  "module_name": "ttm_bo_util.c",
  "hash_id": "32153eab520104ce3b9d66f4a0c3626902eb90d4e8374e93eb099c8d932ac66f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/ttm/ttm_bo_util.c",
  "human_readable_source": " \n \n \n\n#include <linux/vmalloc.h>\n\n#include <drm/ttm/ttm_bo.h>\n#include <drm/ttm/ttm_placement.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_cache.h>\n\nstruct ttm_transfer_obj {\n\tstruct ttm_buffer_object base;\n\tstruct ttm_buffer_object *bo;\n};\n\nint ttm_mem_io_reserve(struct ttm_device *bdev,\n\t\t       struct ttm_resource *mem)\n{\n\tif (mem->bus.offset || mem->bus.addr)\n\t\treturn 0;\n\n\tmem->bus.is_iomem = false;\n\tif (!bdev->funcs->io_mem_reserve)\n\t\treturn 0;\n\n\treturn bdev->funcs->io_mem_reserve(bdev, mem);\n}\n\nvoid ttm_mem_io_free(struct ttm_device *bdev,\n\t\t     struct ttm_resource *mem)\n{\n\tif (!mem)\n\t\treturn;\n\n\tif (!mem->bus.offset && !mem->bus.addr)\n\t\treturn;\n\n\tif (bdev->funcs->io_mem_free)\n\t\tbdev->funcs->io_mem_free(bdev, mem);\n\n\tmem->bus.offset = 0;\n\tmem->bus.addr = NULL;\n}\n\n \nvoid ttm_move_memcpy(bool clear,\n\t\t     u32 num_pages,\n\t\t     struct ttm_kmap_iter *dst_iter,\n\t\t     struct ttm_kmap_iter *src_iter)\n{\n\tconst struct ttm_kmap_iter_ops *dst_ops = dst_iter->ops;\n\tconst struct ttm_kmap_iter_ops *src_ops = src_iter->ops;\n\tstruct iosys_map src_map, dst_map;\n\tpgoff_t i;\n\n\t \n\tif (dst_ops->maps_tt && src_ops->maps_tt)\n\t\treturn;\n\n\t \n\tif (clear) {\n\t\tfor (i = 0; i < num_pages; ++i) {\n\t\t\tdst_ops->map_local(dst_iter, &dst_map, i);\n\t\t\tif (dst_map.is_iomem)\n\t\t\t\tmemset_io(dst_map.vaddr_iomem, 0, PAGE_SIZE);\n\t\t\telse\n\t\t\t\tmemset(dst_map.vaddr, 0, PAGE_SIZE);\n\t\t\tif (dst_ops->unmap_local)\n\t\t\t\tdst_ops->unmap_local(dst_iter, &dst_map);\n\t\t}\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < num_pages; ++i) {\n\t\tdst_ops->map_local(dst_iter, &dst_map, i);\n\t\tsrc_ops->map_local(src_iter, &src_map, i);\n\n\t\tdrm_memcpy_from_wc(&dst_map, &src_map, PAGE_SIZE);\n\n\t\tif (src_ops->unmap_local)\n\t\t\tsrc_ops->unmap_local(src_iter, &src_map);\n\t\tif (dst_ops->unmap_local)\n\t\t\tdst_ops->unmap_local(dst_iter, &dst_map);\n\t}\n}\nEXPORT_SYMBOL(ttm_move_memcpy);\n\n \nint ttm_bo_move_memcpy(struct ttm_buffer_object *bo,\n\t\t       struct ttm_operation_ctx *ctx,\n\t\t       struct ttm_resource *dst_mem)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource_manager *dst_man =\n\t\tttm_manager_type(bo->bdev, dst_mem->mem_type);\n\tstruct ttm_tt *ttm = bo->ttm;\n\tstruct ttm_resource *src_mem = bo->resource;\n\tstruct ttm_resource_manager *src_man;\n\tunion {\n\t\tstruct ttm_kmap_iter_tt tt;\n\t\tstruct ttm_kmap_iter_linear_io io;\n\t} _dst_iter, _src_iter;\n\tstruct ttm_kmap_iter *dst_iter, *src_iter;\n\tbool clear;\n\tint ret = 0;\n\n\tif (WARN_ON(!src_mem))\n\t\treturn -EINVAL;\n\n\tsrc_man = ttm_manager_type(bdev, src_mem->mem_type);\n\tif (ttm && ((ttm->page_flags & TTM_TT_FLAG_SWAPPED) ||\n\t\t    dst_man->use_tt)) {\n\t\tret = ttm_tt_populate(bdev, ttm, ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdst_iter = ttm_kmap_iter_linear_io_init(&_dst_iter.io, bdev, dst_mem);\n\tif (PTR_ERR(dst_iter) == -EINVAL && dst_man->use_tt)\n\t\tdst_iter = ttm_kmap_iter_tt_init(&_dst_iter.tt, bo->ttm);\n\tif (IS_ERR(dst_iter))\n\t\treturn PTR_ERR(dst_iter);\n\n\tsrc_iter = ttm_kmap_iter_linear_io_init(&_src_iter.io, bdev, src_mem);\n\tif (PTR_ERR(src_iter) == -EINVAL && src_man->use_tt)\n\t\tsrc_iter = ttm_kmap_iter_tt_init(&_src_iter.tt, bo->ttm);\n\tif (IS_ERR(src_iter)) {\n\t\tret = PTR_ERR(src_iter);\n\t\tgoto out_src_iter;\n\t}\n\n\tclear = src_iter->ops->maps_tt && (!ttm || !ttm_tt_is_populated(ttm));\n\tif (!(clear && ttm && !(ttm->page_flags & TTM_TT_FLAG_ZERO_ALLOC)))\n\t\tttm_move_memcpy(clear, PFN_UP(dst_mem->size), dst_iter, src_iter);\n\n\tif (!src_iter->ops->maps_tt)\n\t\tttm_kmap_iter_linear_io_fini(&_src_iter.io, bdev, src_mem);\n\tttm_bo_move_sync_cleanup(bo, dst_mem);\n\nout_src_iter:\n\tif (!dst_iter->ops->maps_tt)\n\t\tttm_kmap_iter_linear_io_fini(&_dst_iter.io, bdev, dst_mem);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_move_memcpy);\n\nstatic void ttm_transfered_destroy(struct ttm_buffer_object *bo)\n{\n\tstruct ttm_transfer_obj *fbo;\n\n\tfbo = container_of(bo, struct ttm_transfer_obj, base);\n\tdma_resv_fini(&fbo->base.base._resv);\n\tttm_bo_put(fbo->bo);\n\tkfree(fbo);\n}\n\n \n\nstatic int ttm_buffer_object_transfer(struct ttm_buffer_object *bo,\n\t\t\t\t      struct ttm_buffer_object **new_obj)\n{\n\tstruct ttm_transfer_obj *fbo;\n\tint ret;\n\n\tfbo = kmalloc(sizeof(*fbo), GFP_KERNEL);\n\tif (!fbo)\n\t\treturn -ENOMEM;\n\n\tfbo->base = *bo;\n\n\t \n\n\tatomic_inc(&ttm_glob.bo_count);\n\tdrm_vma_node_reset(&fbo->base.base.vma_node);\n\n\tkref_init(&fbo->base.kref);\n\tfbo->base.destroy = &ttm_transfered_destroy;\n\tfbo->base.pin_count = 0;\n\tif (bo->type != ttm_bo_type_sg)\n\t\tfbo->base.base.resv = &fbo->base.base._resv;\n\n\tdma_resv_init(&fbo->base.base._resv);\n\tfbo->base.base.dev = NULL;\n\tret = dma_resv_trylock(&fbo->base.base._resv);\n\tWARN_ON(!ret);\n\n\tif (fbo->base.resource) {\n\t\tttm_resource_set_bo(fbo->base.resource, &fbo->base);\n\t\tbo->resource = NULL;\n\t\tttm_bo_set_bulk_move(&fbo->base, NULL);\n\t} else {\n\t\tfbo->base.bulk_move = NULL;\n\t}\n\n\tret = dma_resv_reserve_fences(&fbo->base.base._resv, 1);\n\tif (ret) {\n\t\tkfree(fbo);\n\t\treturn ret;\n\t}\n\n\tttm_bo_get(bo);\n\tfbo->bo = bo;\n\n\tttm_bo_move_to_lru_tail_unlocked(&fbo->base);\n\n\t*new_obj = &fbo->base;\n\treturn 0;\n}\n\n \npgprot_t ttm_io_prot(struct ttm_buffer_object *bo, struct ttm_resource *res,\n\t\t     pgprot_t tmp)\n{\n\tstruct ttm_resource_manager *man;\n\tenum ttm_caching caching;\n\n\tman = ttm_manager_type(bo->bdev, res->mem_type);\n\tcaching = man->use_tt ? bo->ttm->caching : res->bus.caching;\n\n\treturn ttm_prot_from_caching(caching, tmp);\n}\nEXPORT_SYMBOL(ttm_io_prot);\n\nstatic int ttm_bo_ioremap(struct ttm_buffer_object *bo,\n\t\t\t  unsigned long offset,\n\t\t\t  unsigned long size,\n\t\t\t  struct ttm_bo_kmap_obj *map)\n{\n\tstruct ttm_resource *mem = bo->resource;\n\n\tif (bo->resource->bus.addr) {\n\t\tmap->bo_kmap_type = ttm_bo_map_premapped;\n\t\tmap->virtual = ((u8 *)bo->resource->bus.addr) + offset;\n\t} else {\n\t\tresource_size_t res = bo->resource->bus.offset + offset;\n\n\t\tmap->bo_kmap_type = ttm_bo_map_iomap;\n\t\tif (mem->bus.caching == ttm_write_combined)\n\t\t\tmap->virtual = ioremap_wc(res, size);\n#ifdef CONFIG_X86\n\t\telse if (mem->bus.caching == ttm_cached)\n\t\t\tmap->virtual = ioremap_cache(res, size);\n#endif\n\t\telse\n\t\t\tmap->virtual = ioremap(res, size);\n\t}\n\treturn (!map->virtual) ? -ENOMEM : 0;\n}\n\nstatic int ttm_bo_kmap_ttm(struct ttm_buffer_object *bo,\n\t\t\t   unsigned long start_page,\n\t\t\t   unsigned long num_pages,\n\t\t\t   struct ttm_bo_kmap_obj *map)\n{\n\tstruct ttm_resource *mem = bo->resource;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = false,\n\t\t.no_wait_gpu = false\n\t};\n\tstruct ttm_tt *ttm = bo->ttm;\n\tpgprot_t prot;\n\tint ret;\n\n\tBUG_ON(!ttm);\n\n\tret = ttm_tt_populate(bo->bdev, ttm, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tif (num_pages == 1 && ttm->caching == ttm_cached) {\n\t\t \n\n\t\tmap->bo_kmap_type = ttm_bo_map_kmap;\n\t\tmap->page = ttm->pages[start_page];\n\t\tmap->virtual = kmap(map->page);\n\t} else {\n\t\t \n\t\tprot = ttm_io_prot(bo, mem, PAGE_KERNEL);\n\t\tmap->bo_kmap_type = ttm_bo_map_vmap;\n\t\tmap->virtual = vmap(ttm->pages + start_page, num_pages,\n\t\t\t\t    0, prot);\n\t}\n\treturn (!map->virtual) ? -ENOMEM : 0;\n}\n\n \nint ttm_bo_kmap(struct ttm_buffer_object *bo,\n\t\tunsigned long start_page, unsigned long num_pages,\n\t\tstruct ttm_bo_kmap_obj *map)\n{\n\tunsigned long offset, size;\n\tint ret;\n\n\tmap->virtual = NULL;\n\tmap->bo = bo;\n\tif (num_pages > PFN_UP(bo->resource->size))\n\t\treturn -EINVAL;\n\tif ((start_page + num_pages) > PFN_UP(bo->resource->size))\n\t\treturn -EINVAL;\n\n\tret = ttm_mem_io_reserve(bo->bdev, bo->resource);\n\tif (ret)\n\t\treturn ret;\n\tif (!bo->resource->bus.is_iomem) {\n\t\treturn ttm_bo_kmap_ttm(bo, start_page, num_pages, map);\n\t} else {\n\t\toffset = start_page << PAGE_SHIFT;\n\t\tsize = num_pages << PAGE_SHIFT;\n\t\treturn ttm_bo_ioremap(bo, offset, size, map);\n\t}\n}\nEXPORT_SYMBOL(ttm_bo_kmap);\n\n \nvoid ttm_bo_kunmap(struct ttm_bo_kmap_obj *map)\n{\n\tif (!map->virtual)\n\t\treturn;\n\tswitch (map->bo_kmap_type) {\n\tcase ttm_bo_map_iomap:\n\t\tiounmap(map->virtual);\n\t\tbreak;\n\tcase ttm_bo_map_vmap:\n\t\tvunmap(map->virtual);\n\t\tbreak;\n\tcase ttm_bo_map_kmap:\n\t\tkunmap(map->page);\n\t\tbreak;\n\tcase ttm_bo_map_premapped:\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\tttm_mem_io_free(map->bo->bdev, map->bo->resource);\n\tmap->virtual = NULL;\n\tmap->page = NULL;\n}\nEXPORT_SYMBOL(ttm_bo_kunmap);\n\n \nint ttm_bo_vmap(struct ttm_buffer_object *bo, struct iosys_map *map)\n{\n\tstruct ttm_resource *mem = bo->resource;\n\tint ret;\n\n\tdma_resv_assert_held(bo->base.resv);\n\n\tret = ttm_mem_io_reserve(bo->bdev, mem);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->bus.is_iomem) {\n\t\tvoid __iomem *vaddr_iomem;\n\n\t\tif (mem->bus.addr)\n\t\t\tvaddr_iomem = (void __iomem *)mem->bus.addr;\n\t\telse if (mem->bus.caching == ttm_write_combined)\n\t\t\tvaddr_iomem = ioremap_wc(mem->bus.offset,\n\t\t\t\t\t\t bo->base.size);\n#ifdef CONFIG_X86\n\t\telse if (mem->bus.caching == ttm_cached)\n\t\t\tvaddr_iomem = ioremap_cache(mem->bus.offset,\n\t\t\t\t\t\t  bo->base.size);\n#endif\n\t\telse\n\t\t\tvaddr_iomem = ioremap(mem->bus.offset, bo->base.size);\n\n\t\tif (!vaddr_iomem)\n\t\t\treturn -ENOMEM;\n\n\t\tiosys_map_set_vaddr_iomem(map, vaddr_iomem);\n\n\t} else {\n\t\tstruct ttm_operation_ctx ctx = {\n\t\t\t.interruptible = false,\n\t\t\t.no_wait_gpu = false\n\t\t};\n\t\tstruct ttm_tt *ttm = bo->ttm;\n\t\tpgprot_t prot;\n\t\tvoid *vaddr;\n\n\t\tret = ttm_tt_populate(bo->bdev, ttm, &ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tprot = ttm_io_prot(bo, mem, PAGE_KERNEL);\n\t\tvaddr = vmap(ttm->pages, ttm->num_pages, 0, prot);\n\t\tif (!vaddr)\n\t\t\treturn -ENOMEM;\n\n\t\tiosys_map_set_vaddr(map, vaddr);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_vmap);\n\n \nvoid ttm_bo_vunmap(struct ttm_buffer_object *bo, struct iosys_map *map)\n{\n\tstruct ttm_resource *mem = bo->resource;\n\n\tdma_resv_assert_held(bo->base.resv);\n\n\tif (iosys_map_is_null(map))\n\t\treturn;\n\n\tif (!map->is_iomem)\n\t\tvunmap(map->vaddr);\n\telse if (!mem->bus.addr)\n\t\tiounmap(map->vaddr_iomem);\n\tiosys_map_clear(map);\n\n\tttm_mem_io_free(bo->bdev, bo->resource);\n}\nEXPORT_SYMBOL(ttm_bo_vunmap);\n\nstatic int ttm_bo_wait_free_node(struct ttm_buffer_object *bo,\n\t\t\t\t bool dst_use_tt)\n{\n\tlong ret;\n\n\tret = dma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t    false, 15 * HZ);\n\tif (ret == 0)\n\t\treturn -EBUSY;\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (!dst_use_tt)\n\t\tttm_bo_tt_destroy(bo);\n\tttm_resource_free(bo, &bo->resource);\n\treturn 0;\n}\n\nstatic int ttm_bo_move_to_ghost(struct ttm_buffer_object *bo,\n\t\t\t\tstruct dma_fence *fence,\n\t\t\t\tbool dst_use_tt)\n{\n\tstruct ttm_buffer_object *ghost_obj;\n\tint ret;\n\n\t \n\n\tret = ttm_buffer_object_transfer(bo, &ghost_obj);\n\tif (ret)\n\t\treturn ret;\n\n\tdma_resv_add_fence(&ghost_obj->base._resv, fence,\n\t\t\t   DMA_RESV_USAGE_KERNEL);\n\n\t \n\n\tif (dst_use_tt)\n\t\tghost_obj->ttm = NULL;\n\telse\n\t\tbo->ttm = NULL;\n\n\tdma_resv_unlock(&ghost_obj->base._resv);\n\tttm_bo_put(ghost_obj);\n\treturn 0;\n}\n\nstatic void ttm_bo_move_pipeline_evict(struct ttm_buffer_object *bo,\n\t\t\t\t       struct dma_fence *fence)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource_manager *from;\n\n\tfrom = ttm_manager_type(bdev, bo->resource->mem_type);\n\n\t \n\tspin_lock(&from->move_lock);\n\tif (!from->move || dma_fence_is_later(fence, from->move)) {\n\t\tdma_fence_put(from->move);\n\t\tfrom->move = dma_fence_get(fence);\n\t}\n\tspin_unlock(&from->move_lock);\n\n\tttm_resource_free(bo, &bo->resource);\n}\n\n \nint ttm_bo_move_accel_cleanup(struct ttm_buffer_object *bo,\n\t\t\t      struct dma_fence *fence,\n\t\t\t      bool evict,\n\t\t\t      bool pipeline,\n\t\t\t      struct ttm_resource *new_mem)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource_manager *from = ttm_manager_type(bdev, bo->resource->mem_type);\n\tstruct ttm_resource_manager *man = ttm_manager_type(bdev, new_mem->mem_type);\n\tint ret = 0;\n\n\tdma_resv_add_fence(bo->base.resv, fence, DMA_RESV_USAGE_KERNEL);\n\tif (!evict)\n\t\tret = ttm_bo_move_to_ghost(bo, fence, man->use_tt);\n\telse if (!from->use_tt && pipeline)\n\t\tttm_bo_move_pipeline_evict(bo, fence);\n\telse\n\t\tret = ttm_bo_wait_free_node(bo, man->use_tt);\n\n\tif (ret)\n\t\treturn ret;\n\n\tttm_bo_assign_mem(bo, new_mem);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_move_accel_cleanup);\n\n \nvoid ttm_bo_move_sync_cleanup(struct ttm_buffer_object *bo,\n\t\t\t      struct ttm_resource *new_mem)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource_manager *man = ttm_manager_type(bdev, new_mem->mem_type);\n\tint ret;\n\n\tret = ttm_bo_wait_free_node(bo, man->use_tt);\n\tif (WARN_ON(ret))\n\t\treturn;\n\n\tttm_bo_assign_mem(bo, new_mem);\n}\nEXPORT_SYMBOL(ttm_bo_move_sync_cleanup);\n\n \nint ttm_bo_pipeline_gutting(struct ttm_buffer_object *bo)\n{\n\tstruct ttm_buffer_object *ghost;\n\tstruct ttm_tt *ttm;\n\tint ret;\n\n\t \n\tif (dma_resv_test_signaled(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP)) {\n\t\tif (!bo->ttm) {\n\t\t\t \n\t\t\tret = ttm_tt_create(bo, true);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else {\n\t\t\tttm_tt_unpopulate(bo->bdev, bo->ttm);\n\t\t\tif (bo->type == ttm_bo_type_device)\n\t\t\t\tttm_tt_mark_for_clear(bo->ttm);\n\t\t}\n\t\tttm_resource_free(bo, &bo->resource);\n\t\treturn 0;\n\t}\n\n\t \n\n\tttm = bo->ttm;\n\tbo->ttm = NULL;\n\tret = ttm_tt_create(bo, true);\n\tswap(bo->ttm, ttm);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ttm_buffer_object_transfer(bo, &ghost);\n\tif (ret)\n\t\tgoto error_destroy_tt;\n\n\tret = dma_resv_copy_fences(&ghost->base._resv, bo->base.resv);\n\t \n\tif (ret) {\n\t\tdma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t      false, MAX_SCHEDULE_TIMEOUT);\n\t}\n\n\tdma_resv_unlock(&ghost->base._resv);\n\tttm_bo_put(ghost);\n\tbo->ttm = ttm;\n\treturn 0;\n\nerror_destroy_tt:\n\tttm_tt_destroy(bo->bdev, ttm);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}