{
  "module_name": "ttm_pool.c",
  "hash_id": "accec3c4544da3eb044e22e7bf2087b9de85a64a581adb365b9e14e99d44a778",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/ttm/ttm_pool.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/module.h>\n#include <linux/dma-mapping.h>\n#include <linux/debugfs.h>\n#include <linux/highmem.h>\n#include <linux/sched/mm.h>\n\n#ifdef CONFIG_X86\n#include <asm/set_memory.h>\n#endif\n\n#include <drm/ttm/ttm_pool.h>\n#include <drm/ttm/ttm_tt.h>\n#include <drm/ttm/ttm_bo.h>\n\n#include \"ttm_module.h\"\n\n \nstruct ttm_pool_dma {\n\tdma_addr_t addr;\n\tunsigned long vaddr;\n};\n\nstatic unsigned long page_pool_size;\n\nMODULE_PARM_DESC(page_pool_size, \"Number of pages in the WC/UC/DMA pool\");\nmodule_param(page_pool_size, ulong, 0644);\n\nstatic atomic_long_t allocated_pages;\n\nstatic struct ttm_pool_type global_write_combined[MAX_ORDER + 1];\nstatic struct ttm_pool_type global_uncached[MAX_ORDER + 1];\n\nstatic struct ttm_pool_type global_dma32_write_combined[MAX_ORDER + 1];\nstatic struct ttm_pool_type global_dma32_uncached[MAX_ORDER + 1];\n\nstatic spinlock_t shrinker_lock;\nstatic struct list_head shrinker_list;\nstatic struct shrinker mm_shrinker;\n\n \nstatic struct page *ttm_pool_alloc_page(struct ttm_pool *pool, gfp_t gfp_flags,\n\t\t\t\t\tunsigned int order)\n{\n\tunsigned long attr = DMA_ATTR_FORCE_CONTIGUOUS;\n\tstruct ttm_pool_dma *dma;\n\tstruct page *p;\n\tvoid *vaddr;\n\n\t \n\tif (order)\n\t\tgfp_flags |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN |\n\t\t\t__GFP_KSWAPD_RECLAIM;\n\n\tif (!pool->use_dma_alloc) {\n\t\tp = alloc_pages_node(pool->nid, gfp_flags, order);\n\t\tif (p)\n\t\t\tp->private = order;\n\t\treturn p;\n\t}\n\n\tdma = kmalloc(sizeof(*dma), GFP_KERNEL);\n\tif (!dma)\n\t\treturn NULL;\n\n\tif (order)\n\t\tattr |= DMA_ATTR_NO_WARN;\n\n\tvaddr = dma_alloc_attrs(pool->dev, (1ULL << order) * PAGE_SIZE,\n\t\t\t\t&dma->addr, gfp_flags, attr);\n\tif (!vaddr)\n\t\tgoto error_free;\n\n\t \n\tif (is_vmalloc_addr(vaddr))\n\t\tp = vmalloc_to_page(vaddr);\n\telse\n\t\tp = virt_to_page(vaddr);\n\n\tdma->vaddr = (unsigned long)vaddr | order;\n\tp->private = (unsigned long)dma;\n\treturn p;\n\nerror_free:\n\tkfree(dma);\n\treturn NULL;\n}\n\n \nstatic void ttm_pool_free_page(struct ttm_pool *pool, enum ttm_caching caching,\n\t\t\t       unsigned int order, struct page *p)\n{\n\tunsigned long attr = DMA_ATTR_FORCE_CONTIGUOUS;\n\tstruct ttm_pool_dma *dma;\n\tvoid *vaddr;\n\n#ifdef CONFIG_X86\n\t \n\tif (caching != ttm_cached && !PageHighMem(p))\n\t\tset_pages_wb(p, 1 << order);\n#endif\n\n\tif (!pool || !pool->use_dma_alloc) {\n\t\t__free_pages(p, order);\n\t\treturn;\n\t}\n\n\tif (order)\n\t\tattr |= DMA_ATTR_NO_WARN;\n\n\tdma = (void *)p->private;\n\tvaddr = (void *)(dma->vaddr & PAGE_MASK);\n\tdma_free_attrs(pool->dev, (1UL << order) * PAGE_SIZE, vaddr, dma->addr,\n\t\t       attr);\n\tkfree(dma);\n}\n\n \nstatic int ttm_pool_apply_caching(struct page **first, struct page **last,\n\t\t\t\t  enum ttm_caching caching)\n{\n#ifdef CONFIG_X86\n\tunsigned int num_pages = last - first;\n\n\tif (!num_pages)\n\t\treturn 0;\n\n\tswitch (caching) {\n\tcase ttm_cached:\n\t\tbreak;\n\tcase ttm_write_combined:\n\t\treturn set_pages_array_wc(first, num_pages);\n\tcase ttm_uncached:\n\t\treturn set_pages_array_uc(first, num_pages);\n\t}\n#endif\n\treturn 0;\n}\n\n \nstatic int ttm_pool_map(struct ttm_pool *pool, unsigned int order,\n\t\t\tstruct page *p, dma_addr_t **dma_addr)\n{\n\tdma_addr_t addr;\n\tunsigned int i;\n\n\tif (pool->use_dma_alloc) {\n\t\tstruct ttm_pool_dma *dma = (void *)p->private;\n\n\t\taddr = dma->addr;\n\t} else {\n\t\tsize_t size = (1ULL << order) * PAGE_SIZE;\n\n\t\taddr = dma_map_page(pool->dev, p, 0, size, DMA_BIDIRECTIONAL);\n\t\tif (dma_mapping_error(pool->dev, addr))\n\t\t\treturn -EFAULT;\n\t}\n\n\tfor (i = 1 << order; i ; --i) {\n\t\t*(*dma_addr)++ = addr;\n\t\taddr += PAGE_SIZE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ttm_pool_unmap(struct ttm_pool *pool, dma_addr_t dma_addr,\n\t\t\t   unsigned int num_pages)\n{\n\t \n\tif (pool->use_dma_alloc)\n\t\treturn;\n\n\tdma_unmap_page(pool->dev, dma_addr, (long)num_pages << PAGE_SHIFT,\n\t\t       DMA_BIDIRECTIONAL);\n}\n\n \nstatic void ttm_pool_type_give(struct ttm_pool_type *pt, struct page *p)\n{\n\tunsigned int i, num_pages = 1 << pt->order;\n\n\tfor (i = 0; i < num_pages; ++i) {\n\t\tif (PageHighMem(p))\n\t\t\tclear_highpage(p + i);\n\t\telse\n\t\t\tclear_page(page_address(p + i));\n\t}\n\n\tspin_lock(&pt->lock);\n\tlist_add(&p->lru, &pt->pages);\n\tspin_unlock(&pt->lock);\n\tatomic_long_add(1 << pt->order, &allocated_pages);\n}\n\n \nstatic struct page *ttm_pool_type_take(struct ttm_pool_type *pt)\n{\n\tstruct page *p;\n\n\tspin_lock(&pt->lock);\n\tp = list_first_entry_or_null(&pt->pages, typeof(*p), lru);\n\tif (p) {\n\t\tatomic_long_sub(1 << pt->order, &allocated_pages);\n\t\tlist_del(&p->lru);\n\t}\n\tspin_unlock(&pt->lock);\n\n\treturn p;\n}\n\n \nstatic void ttm_pool_type_init(struct ttm_pool_type *pt, struct ttm_pool *pool,\n\t\t\t       enum ttm_caching caching, unsigned int order)\n{\n\tpt->pool = pool;\n\tpt->caching = caching;\n\tpt->order = order;\n\tspin_lock_init(&pt->lock);\n\tINIT_LIST_HEAD(&pt->pages);\n\n\tspin_lock(&shrinker_lock);\n\tlist_add_tail(&pt->shrinker_list, &shrinker_list);\n\tspin_unlock(&shrinker_lock);\n}\n\n \nstatic void ttm_pool_type_fini(struct ttm_pool_type *pt)\n{\n\tstruct page *p;\n\n\tspin_lock(&shrinker_lock);\n\tlist_del(&pt->shrinker_list);\n\tspin_unlock(&shrinker_lock);\n\n\twhile ((p = ttm_pool_type_take(pt)))\n\t\tttm_pool_free_page(pt->pool, pt->caching, pt->order, p);\n}\n\n \nstatic struct ttm_pool_type *ttm_pool_select_type(struct ttm_pool *pool,\n\t\t\t\t\t\t  enum ttm_caching caching,\n\t\t\t\t\t\t  unsigned int order)\n{\n\tif (pool->use_dma_alloc || pool->nid != NUMA_NO_NODE)\n\t\treturn &pool->caching[caching].orders[order];\n\n#ifdef CONFIG_X86\n\tswitch (caching) {\n\tcase ttm_write_combined:\n\t\tif (pool->use_dma32)\n\t\t\treturn &global_dma32_write_combined[order];\n\n\t\treturn &global_write_combined[order];\n\tcase ttm_uncached:\n\t\tif (pool->use_dma32)\n\t\t\treturn &global_dma32_uncached[order];\n\n\t\treturn &global_uncached[order];\n\tdefault:\n\t\tbreak;\n\t}\n#endif\n\n\treturn NULL;\n}\n\n \nstatic unsigned int ttm_pool_shrink(void)\n{\n\tstruct ttm_pool_type *pt;\n\tunsigned int num_pages;\n\tstruct page *p;\n\n\tspin_lock(&shrinker_lock);\n\tpt = list_first_entry(&shrinker_list, typeof(*pt), shrinker_list);\n\tlist_move_tail(&pt->shrinker_list, &shrinker_list);\n\tspin_unlock(&shrinker_lock);\n\n\tp = ttm_pool_type_take(pt);\n\tif (p) {\n\t\tttm_pool_free_page(pt->pool, pt->caching, pt->order, p);\n\t\tnum_pages = 1 << pt->order;\n\t} else {\n\t\tnum_pages = 0;\n\t}\n\n\treturn num_pages;\n}\n\n \nstatic unsigned int ttm_pool_page_order(struct ttm_pool *pool, struct page *p)\n{\n\tif (pool->use_dma_alloc) {\n\t\tstruct ttm_pool_dma *dma = (void *)p->private;\n\n\t\treturn dma->vaddr & ~PAGE_MASK;\n\t}\n\n\treturn p->private;\n}\n\n \nstatic int ttm_pool_page_allocated(struct ttm_pool *pool, unsigned int order,\n\t\t\t\t   struct page *p, dma_addr_t **dma_addr,\n\t\t\t\t   unsigned long *num_pages,\n\t\t\t\t   struct page ***pages)\n{\n\tunsigned int i;\n\tint r;\n\n\tif (*dma_addr) {\n\t\tr = ttm_pool_map(pool, order, p, dma_addr);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t*num_pages -= 1 << order;\n\tfor (i = 1 << order; i; --i, ++(*pages), ++p)\n\t\t**pages = p;\n\n\treturn 0;\n}\n\n \nstatic void ttm_pool_free_range(struct ttm_pool *pool, struct ttm_tt *tt,\n\t\t\t\tenum ttm_caching caching,\n\t\t\t\tpgoff_t start_page, pgoff_t end_page)\n{\n\tstruct page **pages = tt->pages;\n\tunsigned int order;\n\tpgoff_t i, nr;\n\n\tfor (i = start_page; i < end_page; i += nr, pages += nr) {\n\t\tstruct ttm_pool_type *pt = NULL;\n\n\t\torder = ttm_pool_page_order(pool, *pages);\n\t\tnr = (1UL << order);\n\t\tif (tt->dma_address)\n\t\t\tttm_pool_unmap(pool, tt->dma_address[i], nr);\n\n\t\tpt = ttm_pool_select_type(pool, caching, order);\n\t\tif (pt)\n\t\t\tttm_pool_type_give(pt, *pages);\n\t\telse\n\t\t\tttm_pool_free_page(pool, caching, order, *pages);\n\t}\n}\n\n \nint ttm_pool_alloc(struct ttm_pool *pool, struct ttm_tt *tt,\n\t\t   struct ttm_operation_ctx *ctx)\n{\n\tpgoff_t num_pages = tt->num_pages;\n\tdma_addr_t *dma_addr = tt->dma_address;\n\tstruct page **caching = tt->pages;\n\tstruct page **pages = tt->pages;\n\tenum ttm_caching page_caching;\n\tgfp_t gfp_flags = GFP_USER;\n\tpgoff_t caching_divide;\n\tunsigned int order;\n\tstruct page *p;\n\tint r;\n\n\tWARN_ON(!num_pages || ttm_tt_is_populated(tt));\n\tWARN_ON(dma_addr && !pool->dev);\n\n\tif (tt->page_flags & TTM_TT_FLAG_ZERO_ALLOC)\n\t\tgfp_flags |= __GFP_ZERO;\n\n\tif (ctx->gfp_retry_mayfail)\n\t\tgfp_flags |= __GFP_RETRY_MAYFAIL;\n\n\tif (pool->use_dma32)\n\t\tgfp_flags |= GFP_DMA32;\n\telse\n\t\tgfp_flags |= GFP_HIGHUSER;\n\n\tfor (order = min_t(unsigned int, MAX_ORDER, __fls(num_pages));\n\t     num_pages;\n\t     order = min_t(unsigned int, order, __fls(num_pages))) {\n\t\tstruct ttm_pool_type *pt;\n\n\t\tpage_caching = tt->caching;\n\t\tpt = ttm_pool_select_type(pool, tt->caching, order);\n\t\tp = pt ? ttm_pool_type_take(pt) : NULL;\n\t\tif (p) {\n\t\t\tr = ttm_pool_apply_caching(caching, pages,\n\t\t\t\t\t\t   tt->caching);\n\t\t\tif (r)\n\t\t\t\tgoto error_free_page;\n\n\t\t\tcaching = pages;\n\t\t\tdo {\n\t\t\t\tr = ttm_pool_page_allocated(pool, order, p,\n\t\t\t\t\t\t\t    &dma_addr,\n\t\t\t\t\t\t\t    &num_pages,\n\t\t\t\t\t\t\t    &pages);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto error_free_page;\n\n\t\t\t\tcaching = pages;\n\t\t\t\tif (num_pages < (1 << order))\n\t\t\t\t\tbreak;\n\n\t\t\t\tp = ttm_pool_type_take(pt);\n\t\t\t} while (p);\n\t\t}\n\n\t\tpage_caching = ttm_cached;\n\t\twhile (num_pages >= (1 << order) &&\n\t\t       (p = ttm_pool_alloc_page(pool, gfp_flags, order))) {\n\n\t\t\tif (PageHighMem(p)) {\n\t\t\t\tr = ttm_pool_apply_caching(caching, pages,\n\t\t\t\t\t\t\t   tt->caching);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto error_free_page;\n\t\t\t\tcaching = pages;\n\t\t\t}\n\t\t\tr = ttm_pool_page_allocated(pool, order, p, &dma_addr,\n\t\t\t\t\t\t    &num_pages, &pages);\n\t\t\tif (r)\n\t\t\t\tgoto error_free_page;\n\t\t\tif (PageHighMem(p))\n\t\t\t\tcaching = pages;\n\t\t}\n\n\t\tif (!p) {\n\t\t\tif (order) {\n\t\t\t\t--order;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tr = -ENOMEM;\n\t\t\tgoto error_free_all;\n\t\t}\n\t}\n\n\tr = ttm_pool_apply_caching(caching, pages, tt->caching);\n\tif (r)\n\t\tgoto error_free_all;\n\n\treturn 0;\n\nerror_free_page:\n\tttm_pool_free_page(pool, page_caching, order, p);\n\nerror_free_all:\n\tnum_pages = tt->num_pages - num_pages;\n\tcaching_divide = caching - tt->pages;\n\tttm_pool_free_range(pool, tt, tt->caching, 0, caching_divide);\n\tttm_pool_free_range(pool, tt, ttm_cached, caching_divide, num_pages);\n\n\treturn r;\n}\nEXPORT_SYMBOL(ttm_pool_alloc);\n\n \nvoid ttm_pool_free(struct ttm_pool *pool, struct ttm_tt *tt)\n{\n\tttm_pool_free_range(pool, tt, tt->caching, 0, tt->num_pages);\n\n\twhile (atomic_long_read(&allocated_pages) > page_pool_size)\n\t\tttm_pool_shrink();\n}\nEXPORT_SYMBOL(ttm_pool_free);\n\n \nvoid ttm_pool_init(struct ttm_pool *pool, struct device *dev,\n\t\t   int nid, bool use_dma_alloc, bool use_dma32)\n{\n\tunsigned int i, j;\n\n\tWARN_ON(!dev && use_dma_alloc);\n\n\tpool->dev = dev;\n\tpool->nid = nid;\n\tpool->use_dma_alloc = use_dma_alloc;\n\tpool->use_dma32 = use_dma32;\n\n\tif (use_dma_alloc || nid != NUMA_NO_NODE) {\n\t\tfor (i = 0; i < TTM_NUM_CACHING_TYPES; ++i)\n\t\t\tfor (j = 0; j <= MAX_ORDER; ++j)\n\t\t\t\tttm_pool_type_init(&pool->caching[i].orders[j],\n\t\t\t\t\t\t   pool, i, j);\n\t}\n}\nEXPORT_SYMBOL(ttm_pool_init);\n\n \nvoid ttm_pool_fini(struct ttm_pool *pool)\n{\n\tunsigned int i, j;\n\n\tif (pool->use_dma_alloc || pool->nid != NUMA_NO_NODE) {\n\t\tfor (i = 0; i < TTM_NUM_CACHING_TYPES; ++i)\n\t\t\tfor (j = 0; j <= MAX_ORDER; ++j)\n\t\t\t\tttm_pool_type_fini(&pool->caching[i].orders[j]);\n\t}\n\n\t \n\tsynchronize_shrinkers();\n}\nEXPORT_SYMBOL(ttm_pool_fini);\n\n \nstatic unsigned long ttm_pool_shrinker_scan(struct shrinker *shrink,\n\t\t\t\t\t    struct shrink_control *sc)\n{\n\tunsigned long num_freed = 0;\n\n\tdo\n\t\tnum_freed += ttm_pool_shrink();\n\twhile (!num_freed && atomic_long_read(&allocated_pages));\n\n\treturn num_freed;\n}\n\n \nstatic unsigned long ttm_pool_shrinker_count(struct shrinker *shrink,\n\t\t\t\t\t     struct shrink_control *sc)\n{\n\tunsigned long num_pages = atomic_long_read(&allocated_pages);\n\n\treturn num_pages ? num_pages : SHRINK_EMPTY;\n}\n\n#ifdef CONFIG_DEBUG_FS\n \nstatic unsigned int ttm_pool_type_count(struct ttm_pool_type *pt)\n{\n\tunsigned int count = 0;\n\tstruct page *p;\n\n\tspin_lock(&pt->lock);\n\t \n\tlist_for_each_entry(p, &pt->pages, lru)\n\t\t++count;\n\tspin_unlock(&pt->lock);\n\n\treturn count;\n}\n\n \nstatic void ttm_pool_debugfs_header(struct seq_file *m)\n{\n\tunsigned int i;\n\n\tseq_puts(m, \"\\t \");\n\tfor (i = 0; i <= MAX_ORDER; ++i)\n\t\tseq_printf(m, \" ---%2u---\", i);\n\tseq_puts(m, \"\\n\");\n}\n\n \nstatic void ttm_pool_debugfs_orders(struct ttm_pool_type *pt,\n\t\t\t\t    struct seq_file *m)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i <= MAX_ORDER; ++i)\n\t\tseq_printf(m, \" %8u\", ttm_pool_type_count(&pt[i]));\n\tseq_puts(m, \"\\n\");\n}\n\n \nstatic void ttm_pool_debugfs_footer(struct seq_file *m)\n{\n\tseq_printf(m, \"\\ntotal\\t: %8lu of %8lu\\n\",\n\t\t   atomic_long_read(&allocated_pages), page_pool_size);\n}\n\n \nstatic int ttm_pool_debugfs_globals_show(struct seq_file *m, void *data)\n{\n\tttm_pool_debugfs_header(m);\n\n\tspin_lock(&shrinker_lock);\n\tseq_puts(m, \"wc\\t:\");\n\tttm_pool_debugfs_orders(global_write_combined, m);\n\tseq_puts(m, \"uc\\t:\");\n\tttm_pool_debugfs_orders(global_uncached, m);\n\tseq_puts(m, \"wc 32\\t:\");\n\tttm_pool_debugfs_orders(global_dma32_write_combined, m);\n\tseq_puts(m, \"uc 32\\t:\");\n\tttm_pool_debugfs_orders(global_dma32_uncached, m);\n\tspin_unlock(&shrinker_lock);\n\n\tttm_pool_debugfs_footer(m);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(ttm_pool_debugfs_globals);\n\n \nint ttm_pool_debugfs(struct ttm_pool *pool, struct seq_file *m)\n{\n\tunsigned int i;\n\n\tif (!pool->use_dma_alloc) {\n\t\tseq_puts(m, \"unused\\n\");\n\t\treturn 0;\n\t}\n\n\tttm_pool_debugfs_header(m);\n\n\tspin_lock(&shrinker_lock);\n\tfor (i = 0; i < TTM_NUM_CACHING_TYPES; ++i) {\n\t\tseq_puts(m, \"DMA \");\n\t\tswitch (i) {\n\t\tcase ttm_cached:\n\t\t\tseq_puts(m, \"\\t:\");\n\t\t\tbreak;\n\t\tcase ttm_write_combined:\n\t\t\tseq_puts(m, \"wc\\t:\");\n\t\t\tbreak;\n\t\tcase ttm_uncached:\n\t\t\tseq_puts(m, \"uc\\t:\");\n\t\t\tbreak;\n\t\t}\n\t\tttm_pool_debugfs_orders(pool->caching[i].orders, m);\n\t}\n\tspin_unlock(&shrinker_lock);\n\n\tttm_pool_debugfs_footer(m);\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_pool_debugfs);\n\n \nstatic int ttm_pool_debugfs_shrink_show(struct seq_file *m, void *data)\n{\n\tstruct shrink_control sc = { .gfp_mask = GFP_NOFS };\n\n\tfs_reclaim_acquire(GFP_KERNEL);\n\tseq_printf(m, \"%lu/%lu\\n\", ttm_pool_shrinker_count(&mm_shrinker, &sc),\n\t\t   ttm_pool_shrinker_scan(&mm_shrinker, &sc));\n\tfs_reclaim_release(GFP_KERNEL);\n\n\treturn 0;\n}\nDEFINE_SHOW_ATTRIBUTE(ttm_pool_debugfs_shrink);\n\n#endif\n\n \nint ttm_pool_mgr_init(unsigned long num_pages)\n{\n\tunsigned int i;\n\n\tif (!page_pool_size)\n\t\tpage_pool_size = num_pages;\n\n\tspin_lock_init(&shrinker_lock);\n\tINIT_LIST_HEAD(&shrinker_list);\n\n\tfor (i = 0; i <= MAX_ORDER; ++i) {\n\t\tttm_pool_type_init(&global_write_combined[i], NULL,\n\t\t\t\t   ttm_write_combined, i);\n\t\tttm_pool_type_init(&global_uncached[i], NULL, ttm_uncached, i);\n\n\t\tttm_pool_type_init(&global_dma32_write_combined[i], NULL,\n\t\t\t\t   ttm_write_combined, i);\n\t\tttm_pool_type_init(&global_dma32_uncached[i], NULL,\n\t\t\t\t   ttm_uncached, i);\n\t}\n\n#ifdef CONFIG_DEBUG_FS\n\tdebugfs_create_file(\"page_pool\", 0444, ttm_debugfs_root, NULL,\n\t\t\t    &ttm_pool_debugfs_globals_fops);\n\tdebugfs_create_file(\"page_pool_shrink\", 0400, ttm_debugfs_root, NULL,\n\t\t\t    &ttm_pool_debugfs_shrink_fops);\n#endif\n\n\tmm_shrinker.count_objects = ttm_pool_shrinker_count;\n\tmm_shrinker.scan_objects = ttm_pool_shrinker_scan;\n\tmm_shrinker.seeks = 1;\n\treturn register_shrinker(&mm_shrinker, \"drm-ttm_pool\");\n}\n\n \nvoid ttm_pool_mgr_fini(void)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i <= MAX_ORDER; ++i) {\n\t\tttm_pool_type_fini(&global_write_combined[i]);\n\t\tttm_pool_type_fini(&global_uncached[i]);\n\n\t\tttm_pool_type_fini(&global_dma32_write_combined[i]);\n\t\tttm_pool_type_fini(&global_dma32_uncached[i]);\n\t}\n\n\tunregister_shrinker(&mm_shrinker);\n\tWARN_ON(!list_empty(&shrinker_list));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}