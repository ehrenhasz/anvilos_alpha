{
  "module_name": "ttm_bo_vm.c",
  "hash_id": "4f5ad25f1ee9127a31583553dff1d2d6a52bee39b4beb812b4108006306c46ac",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/ttm/ttm_bo_vm.c",
  "human_readable_source": " \n \n \n\n#define pr_fmt(fmt) \"[TTM] \" fmt\n\n#include <drm/ttm/ttm_bo.h>\n#include <drm/ttm/ttm_placement.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_drv.h>\n#include <drm/drm_managed.h>\n\nstatic vm_fault_t ttm_bo_vm_fault_idle(struct ttm_buffer_object *bo,\n\t\t\t\tstruct vm_fault *vmf)\n{\n\tlong err = 0;\n\n\t \n\tif (dma_resv_test_signaled(bo->base.resv, DMA_RESV_USAGE_KERNEL))\n\t\treturn 0;\n\n\t \n\tif (fault_flag_allow_retry_first(vmf->flags)) {\n\t\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\t\treturn VM_FAULT_RETRY;\n\n\t\tttm_bo_get(bo);\n\t\tmmap_read_unlock(vmf->vma->vm_mm);\n\t\t(void)dma_resv_wait_timeout(bo->base.resv,\n\t\t\t\t\t    DMA_RESV_USAGE_KERNEL, true,\n\t\t\t\t\t    MAX_SCHEDULE_TIMEOUT);\n\t\tdma_resv_unlock(bo->base.resv);\n\t\tttm_bo_put(bo);\n\t\treturn VM_FAULT_RETRY;\n\t}\n\n\t \n\terr = dma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_KERNEL, true,\n\t\t\t\t    MAX_SCHEDULE_TIMEOUT);\n\tif (unlikely(err < 0)) {\n\t\treturn (err != -ERESTARTSYS) ? VM_FAULT_SIGBUS :\n\t\t\tVM_FAULT_NOPAGE;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned long ttm_bo_io_mem_pfn(struct ttm_buffer_object *bo,\n\t\t\t\t       unsigned long page_offset)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\n\tif (bdev->funcs->io_mem_pfn)\n\t\treturn bdev->funcs->io_mem_pfn(bo, page_offset);\n\n\treturn (bo->resource->bus.offset >> PAGE_SHIFT) + page_offset;\n}\n\n \nvm_fault_t ttm_bo_vm_reserve(struct ttm_buffer_object *bo,\n\t\t\t     struct vm_fault *vmf)\n{\n\t \n\tif (unlikely(!dma_resv_trylock(bo->base.resv))) {\n\t\t \n\t\tif (fault_flag_allow_retry_first(vmf->flags)) {\n\t\t\tif (!(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {\n\t\t\t\tttm_bo_get(bo);\n\t\t\t\tmmap_read_unlock(vmf->vma->vm_mm);\n\t\t\t\tif (!dma_resv_lock_interruptible(bo->base.resv,\n\t\t\t\t\t\t\t\t NULL))\n\t\t\t\t\tdma_resv_unlock(bo->base.resv);\n\t\t\t\tttm_bo_put(bo);\n\t\t\t}\n\n\t\t\treturn VM_FAULT_RETRY;\n\t\t}\n\n\t\tif (dma_resv_lock_interruptible(bo->base.resv, NULL))\n\t\t\treturn VM_FAULT_NOPAGE;\n\t}\n\n\t \n\tif (bo->ttm && (bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL)) {\n\t\tif (!(bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL_MAPPABLE)) {\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\t\treturn VM_FAULT_SIGBUS;\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_vm_reserve);\n\n \nvm_fault_t ttm_bo_vm_fault_reserved(struct vm_fault *vmf,\n\t\t\t\t    pgprot_t prot,\n\t\t\t\t    pgoff_t num_prefault)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\tstruct ttm_device *bdev = bo->bdev;\n\tunsigned long page_offset;\n\tunsigned long page_last;\n\tunsigned long pfn;\n\tstruct ttm_tt *ttm = NULL;\n\tstruct page *page;\n\tint err;\n\tpgoff_t i;\n\tvm_fault_t ret = VM_FAULT_NOPAGE;\n\tunsigned long address = vmf->address;\n\n\t \n\tret = ttm_bo_vm_fault_idle(bo, vmf);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\terr = ttm_mem_io_reserve(bdev, bo->resource);\n\tif (unlikely(err != 0))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tpage_offset = ((address - vma->vm_start) >> PAGE_SHIFT) +\n\t\tvma->vm_pgoff - drm_vma_node_start(&bo->base.vma_node);\n\tpage_last = vma_pages(vma) + vma->vm_pgoff -\n\t\tdrm_vma_node_start(&bo->base.vma_node);\n\n\tif (unlikely(page_offset >= PFN_UP(bo->base.size)))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tprot = ttm_io_prot(bo, bo->resource, prot);\n\tif (!bo->resource->bus.is_iomem) {\n\t\tstruct ttm_operation_ctx ctx = {\n\t\t\t.interruptible = true,\n\t\t\t.no_wait_gpu = false,\n\t\t\t.force_alloc = true\n\t\t};\n\n\t\tttm = bo->ttm;\n\t\terr = ttm_tt_populate(bdev, bo->ttm, &ctx);\n\t\tif (err) {\n\t\t\tif (err == -EINTR || err == -ERESTARTSYS ||\n\t\t\t    err == -EAGAIN)\n\t\t\t\treturn VM_FAULT_NOPAGE;\n\n\t\t\tpr_debug(\"TTM fault hit %pe.\\n\", ERR_PTR(err));\n\t\t\treturn VM_FAULT_SIGBUS;\n\t\t}\n\t} else {\n\t\t \n\t\tprot = pgprot_decrypted(prot);\n\t}\n\n\t \n\tfor (i = 0; i < num_prefault; ++i) {\n\t\tif (bo->resource->bus.is_iomem) {\n\t\t\tpfn = ttm_bo_io_mem_pfn(bo, page_offset);\n\t\t} else {\n\t\t\tpage = ttm->pages[page_offset];\n\t\t\tif (unlikely(!page && i == 0)) {\n\t\t\t\treturn VM_FAULT_OOM;\n\t\t\t} else if (unlikely(!page)) {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpfn = page_to_pfn(page);\n\t\t}\n\n\t\t \n\t\tret = vmf_insert_pfn_prot(vma, address, pfn, prot);\n\n\t\t \n\t\tif (unlikely((ret & VM_FAULT_ERROR))) {\n\t\t\tif (i == 0)\n\t\t\t\treturn VM_FAULT_NOPAGE;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\taddress += PAGE_SIZE;\n\t\tif (unlikely(++page_offset >= page_last))\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_vm_fault_reserved);\n\nstatic void ttm_bo_release_dummy_page(struct drm_device *dev, void *res)\n{\n\tstruct page *dummy_page = (struct page *)res;\n\n\t__free_page(dummy_page);\n}\n\nvm_fault_t ttm_bo_vm_dummy_page(struct vm_fault *vmf, pgprot_t prot)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret = VM_FAULT_NOPAGE;\n\tunsigned long address;\n\tunsigned long pfn;\n\tstruct page *page;\n\n\t \n\tpage = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!page)\n\t\treturn VM_FAULT_OOM;\n\n\t \n\tif (drmm_add_action_or_reset(ddev, ttm_bo_release_dummy_page, page))\n\t\treturn VM_FAULT_OOM;\n\n\tpfn = page_to_pfn(page);\n\n\t \n\tfor (address = vma->vm_start; address < vma->vm_end;\n\t     address += PAGE_SIZE)\n\t\tret = vmf_insert_pfn_prot(vma, address, pfn, prot);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_vm_dummy_page);\n\nvm_fault_t ttm_bo_vm_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tpgprot_t prot;\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\tstruct drm_device *ddev = bo->base.dev;\n\tvm_fault_t ret;\n\tint idx;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tprot = vma->vm_page_prot;\n\tif (drm_dev_enter(ddev, &idx)) {\n\t\tret = ttm_bo_vm_fault_reserved(vmf, prot, TTM_BO_VM_NUM_PREFAULT);\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, prot);\n\t}\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\n\tdma_resv_unlock(bo->base.resv);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_vm_fault);\n\nvoid ttm_bo_vm_open(struct vm_area_struct *vma)\n{\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\n\tWARN_ON(bo->bdev->dev_mapping != vma->vm_file->f_mapping);\n\n\tttm_bo_get(bo);\n}\nEXPORT_SYMBOL(ttm_bo_vm_open);\n\nvoid ttm_bo_vm_close(struct vm_area_struct *vma)\n{\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\n\tttm_bo_put(bo);\n\tvma->vm_private_data = NULL;\n}\nEXPORT_SYMBOL(ttm_bo_vm_close);\n\nstatic int ttm_bo_vm_access_kmap(struct ttm_buffer_object *bo,\n\t\t\t\t unsigned long offset,\n\t\t\t\t uint8_t *buf, int len, int write)\n{\n\tunsigned long page = offset >> PAGE_SHIFT;\n\tunsigned long bytes_left = len;\n\tint ret;\n\n\t \n\toffset -= page << PAGE_SHIFT;\n\tdo {\n\t\tunsigned long bytes = min(bytes_left, PAGE_SIZE - offset);\n\t\tstruct ttm_bo_kmap_obj map;\n\t\tvoid *ptr;\n\t\tbool is_iomem;\n\n\t\tret = ttm_bo_kmap(bo, page, 1, &map);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tptr = (uint8_t *)ttm_kmap_obj_virtual(&map, &is_iomem) + offset;\n\t\tWARN_ON_ONCE(is_iomem);\n\t\tif (write)\n\t\t\tmemcpy(ptr, buf, bytes);\n\t\telse\n\t\t\tmemcpy(buf, ptr, bytes);\n\t\tttm_bo_kunmap(&map);\n\n\t\tpage++;\n\t\tbuf += bytes;\n\t\tbytes_left -= bytes;\n\t\toffset = 0;\n\t} while (bytes_left);\n\n\treturn len;\n}\n\nint ttm_bo_vm_access(struct vm_area_struct *vma, unsigned long addr,\n\t\t     void *buf, int len, int write)\n{\n\tstruct ttm_buffer_object *bo = vma->vm_private_data;\n\tunsigned long offset = (addr) - vma->vm_start +\n\t\t((vma->vm_pgoff - drm_vma_node_start(&bo->base.vma_node))\n\t\t << PAGE_SHIFT);\n\tint ret;\n\n\tif (len < 1 || (offset + len) > bo->base.size)\n\t\treturn -EIO;\n\n\tret = ttm_bo_reserve(bo, true, false, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (bo->resource->mem_type) {\n\tcase TTM_PL_SYSTEM:\n\t\tfallthrough;\n\tcase TTM_PL_TT:\n\t\tret = ttm_bo_vm_access_kmap(bo, offset, buf, len, write);\n\t\tbreak;\n\tdefault:\n\t\tif (bo->bdev->funcs->access_memory)\n\t\t\tret = bo->bdev->funcs->access_memory(\n\t\t\t\tbo, offset, buf, len, write);\n\t\telse\n\t\t\tret = -EIO;\n\t}\n\n\tttm_bo_unreserve(bo);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_vm_access);\n\nstatic const struct vm_operations_struct ttm_bo_vm_ops = {\n\t.fault = ttm_bo_vm_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access,\n};\n\n \nint ttm_bo_mmap_obj(struct vm_area_struct *vma, struct ttm_buffer_object *bo)\n{\n\t \n\tif (is_cow_mapping(vma->vm_flags))\n\t\treturn -EINVAL;\n\n\tttm_bo_get(bo);\n\n\t \n\tif (!vma->vm_ops)\n\t\tvma->vm_ops = &ttm_bo_vm_ops;\n\n\t \n\n\tvma->vm_private_data = bo;\n\n\tvm_flags_set(vma, VM_PFNMAP | VM_IO | VM_DONTEXPAND | VM_DONTDUMP);\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_mmap_obj);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}