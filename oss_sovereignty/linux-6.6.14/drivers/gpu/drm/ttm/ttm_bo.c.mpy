{
  "module_name": "ttm_bo.c",
  "hash_id": "2426ea4a63aa25d05b8759d84dc78b8463a9fd03ad18b64941577a15baa0fd06",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/ttm/ttm_bo.c",
  "human_readable_source": " \n \n \n\n#define pr_fmt(fmt) \"[TTM] \" fmt\n\n#include <drm/ttm/ttm_bo.h>\n#include <drm/ttm/ttm_placement.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <linux/jiffies.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/file.h>\n#include <linux/module.h>\n#include <linux/atomic.h>\n#include <linux/dma-resv.h>\n\n#include \"ttm_module.h\"\n\nstatic void ttm_bo_mem_space_debug(struct ttm_buffer_object *bo,\n\t\t\t\t\tstruct ttm_placement *placement)\n{\n\tstruct drm_printer p = drm_debug_printer(TTM_PFX);\n\tstruct ttm_resource_manager *man;\n\tint i, mem_type;\n\n\tfor (i = 0; i < placement->num_placement; i++) {\n\t\tmem_type = placement->placement[i].mem_type;\n\t\tdrm_printf(&p, \"  placement[%d]=0x%08X (%d)\\n\",\n\t\t\t   i, placement->placement[i].flags, mem_type);\n\t\tman = ttm_manager_type(bo->bdev, mem_type);\n\t\tttm_resource_manager_debug(man, &p);\n\t}\n}\n\n \nvoid ttm_bo_move_to_lru_tail(struct ttm_buffer_object *bo)\n{\n\tdma_resv_assert_held(bo->base.resv);\n\n\tif (bo->resource)\n\t\tttm_resource_move_to_lru_tail(bo->resource);\n}\nEXPORT_SYMBOL(ttm_bo_move_to_lru_tail);\n\n \nvoid ttm_bo_set_bulk_move(struct ttm_buffer_object *bo,\n\t\t\t  struct ttm_lru_bulk_move *bulk)\n{\n\tdma_resv_assert_held(bo->base.resv);\n\n\tif (bo->bulk_move == bulk)\n\t\treturn;\n\n\tspin_lock(&bo->bdev->lru_lock);\n\tif (bo->resource)\n\t\tttm_resource_del_bulk_move(bo->resource, bo);\n\tbo->bulk_move = bulk;\n\tif (bo->resource)\n\t\tttm_resource_add_bulk_move(bo->resource, bo);\n\tspin_unlock(&bo->bdev->lru_lock);\n}\nEXPORT_SYMBOL(ttm_bo_set_bulk_move);\n\nstatic int ttm_bo_handle_move_mem(struct ttm_buffer_object *bo,\n\t\t\t\t  struct ttm_resource *mem, bool evict,\n\t\t\t\t  struct ttm_operation_ctx *ctx,\n\t\t\t\t  struct ttm_place *hop)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tbool old_use_tt, new_use_tt;\n\tint ret;\n\n\told_use_tt = !bo->resource || ttm_manager_type(bdev, bo->resource->mem_type)->use_tt;\n\tnew_use_tt = ttm_manager_type(bdev, mem->mem_type)->use_tt;\n\n\tttm_bo_unmap_virtual(bo);\n\n\t \n\n\tif (new_use_tt) {\n\t\t \n\t\tret = ttm_tt_create(bo, old_use_tt);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\n\t\tif (mem->mem_type != TTM_PL_SYSTEM) {\n\t\t\tret = ttm_tt_populate(bo->bdev, bo->ttm, ctx);\n\t\t\tif (ret)\n\t\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = bdev->funcs->move(bo, evict, ctx, mem, hop);\n\tif (ret) {\n\t\tif (ret == -EMULTIHOP)\n\t\t\treturn ret;\n\t\tgoto out_err;\n\t}\n\n\tctx->bytes_moved += bo->base.size;\n\treturn 0;\n\nout_err:\n\tif (!old_use_tt)\n\t\tttm_bo_tt_destroy(bo);\n\n\treturn ret;\n}\n\n \n\nstatic void ttm_bo_cleanup_memtype_use(struct ttm_buffer_object *bo)\n{\n\tif (bo->bdev->funcs->delete_mem_notify)\n\t\tbo->bdev->funcs->delete_mem_notify(bo);\n\n\tttm_bo_tt_destroy(bo);\n\tttm_resource_free(bo, &bo->resource);\n}\n\nstatic int ttm_bo_individualize_resv(struct ttm_buffer_object *bo)\n{\n\tint r;\n\n\tif (bo->base.resv == &bo->base._resv)\n\t\treturn 0;\n\n\tBUG_ON(!dma_resv_trylock(&bo->base._resv));\n\n\tr = dma_resv_copy_fences(&bo->base._resv, bo->base.resv);\n\tdma_resv_unlock(&bo->base._resv);\n\tif (r)\n\t\treturn r;\n\n\tif (bo->type != ttm_bo_type_sg) {\n\t\t \n\t\tspin_lock(&bo->bdev->lru_lock);\n\t\tbo->base.resv = &bo->base._resv;\n\t\tspin_unlock(&bo->bdev->lru_lock);\n\t}\n\n\treturn r;\n}\n\nstatic void ttm_bo_flush_all_fences(struct ttm_buffer_object *bo)\n{\n\tstruct dma_resv *resv = &bo->base._resv;\n\tstruct dma_resv_iter cursor;\n\tstruct dma_fence *fence;\n\n\tdma_resv_iter_begin(&cursor, resv, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_resv_for_each_fence_unlocked(&cursor, fence) {\n\t\tif (!fence->ops->signaled)\n\t\t\tdma_fence_enable_sw_signaling(fence);\n\t}\n\tdma_resv_iter_end(&cursor);\n}\n\n \n\nstatic int ttm_bo_cleanup_refs(struct ttm_buffer_object *bo,\n\t\t\t       bool interruptible, bool no_wait_gpu,\n\t\t\t       bool unlock_resv)\n{\n\tstruct dma_resv *resv = &bo->base._resv;\n\tint ret;\n\n\tif (dma_resv_test_signaled(resv, DMA_RESV_USAGE_BOOKKEEP))\n\t\tret = 0;\n\telse\n\t\tret = -EBUSY;\n\n\tif (ret && !no_wait_gpu) {\n\t\tlong lret;\n\n\t\tif (unlock_resv)\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\tspin_unlock(&bo->bdev->lru_lock);\n\n\t\tlret = dma_resv_wait_timeout(resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t\t     interruptible,\n\t\t\t\t\t     30 * HZ);\n\n\t\tif (lret < 0)\n\t\t\treturn lret;\n\t\telse if (lret == 0)\n\t\t\treturn -EBUSY;\n\n\t\tspin_lock(&bo->bdev->lru_lock);\n\t\tif (unlock_resv && !dma_resv_trylock(bo->base.resv)) {\n\t\t\t \n\t\t\tspin_unlock(&bo->bdev->lru_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tret = 0;\n\t}\n\n\tif (ret) {\n\t\tif (unlock_resv)\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\tspin_unlock(&bo->bdev->lru_lock);\n\t\treturn ret;\n\t}\n\n\tspin_unlock(&bo->bdev->lru_lock);\n\tttm_bo_cleanup_memtype_use(bo);\n\n\tif (unlock_resv)\n\t\tdma_resv_unlock(bo->base.resv);\n\n\treturn 0;\n}\n\n \nstatic void ttm_bo_delayed_delete(struct work_struct *work)\n{\n\tstruct ttm_buffer_object *bo;\n\n\tbo = container_of(work, typeof(*bo), delayed_delete);\n\n\tdma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP, false,\n\t\t\t      MAX_SCHEDULE_TIMEOUT);\n\tdma_resv_lock(bo->base.resv, NULL);\n\tttm_bo_cleanup_memtype_use(bo);\n\tdma_resv_unlock(bo->base.resv);\n\tttm_bo_put(bo);\n}\n\nstatic void ttm_bo_release(struct kref *kref)\n{\n\tstruct ttm_buffer_object *bo =\n\t    container_of(kref, struct ttm_buffer_object, kref);\n\tstruct ttm_device *bdev = bo->bdev;\n\tint ret;\n\n\tWARN_ON_ONCE(bo->pin_count);\n\tWARN_ON_ONCE(bo->bulk_move);\n\n\tif (!bo->deleted) {\n\t\tret = ttm_bo_individualize_resv(bo);\n\t\tif (ret) {\n\t\t\t \n\t\t\tdma_resv_wait_timeout(bo->base.resv,\n\t\t\t\t\t      DMA_RESV_USAGE_BOOKKEEP, false,\n\t\t\t\t\t      30 * HZ);\n\t\t}\n\n\t\tif (bo->bdev->funcs->release_notify)\n\t\t\tbo->bdev->funcs->release_notify(bo);\n\n\t\tdrm_vma_offset_remove(bdev->vma_manager, &bo->base.vma_node);\n\t\tttm_mem_io_free(bdev, bo->resource);\n\n\t\tif (!dma_resv_test_signaled(bo->base.resv,\n\t\t\t\t\t    DMA_RESV_USAGE_BOOKKEEP) ||\n\t\t    (want_init_on_free() && (bo->ttm != NULL)) ||\n\t\t    !dma_resv_trylock(bo->base.resv)) {\n\t\t\t \n\t\t\tttm_bo_flush_all_fences(bo);\n\t\t\tbo->deleted = true;\n\n\t\t\tspin_lock(&bo->bdev->lru_lock);\n\n\t\t\t \n\t\t\tif (bo->pin_count) {\n\t\t\t\tbo->pin_count = 0;\n\t\t\t\tttm_resource_move_to_lru_tail(bo->resource);\n\t\t\t}\n\n\t\t\tkref_init(&bo->kref);\n\t\t\tspin_unlock(&bo->bdev->lru_lock);\n\n\t\t\tINIT_WORK(&bo->delayed_delete, ttm_bo_delayed_delete);\n\t\t\tqueue_work(bdev->wq, &bo->delayed_delete);\n\t\t\treturn;\n\t\t}\n\n\t\tttm_bo_cleanup_memtype_use(bo);\n\t\tdma_resv_unlock(bo->base.resv);\n\t}\n\n\tatomic_dec(&ttm_glob.bo_count);\n\tbo->destroy(bo);\n}\n\n \nvoid ttm_bo_put(struct ttm_buffer_object *bo)\n{\n\tkref_put(&bo->kref, ttm_bo_release);\n}\nEXPORT_SYMBOL(ttm_bo_put);\n\nstatic int ttm_bo_bounce_temp_buffer(struct ttm_buffer_object *bo,\n\t\t\t\t     struct ttm_resource **mem,\n\t\t\t\t     struct ttm_operation_ctx *ctx,\n\t\t\t\t     struct ttm_place *hop)\n{\n\tstruct ttm_placement hop_placement;\n\tstruct ttm_resource *hop_mem;\n\tint ret;\n\n\thop_placement.num_placement = hop_placement.num_busy_placement = 1;\n\thop_placement.placement = hop_placement.busy_placement = hop;\n\n\t \n\tret = ttm_bo_mem_space(bo, &hop_placement, &hop_mem, ctx);\n\tif (ret)\n\t\treturn ret;\n\t \n\tret = ttm_bo_handle_move_mem(bo, hop_mem, false, ctx, NULL);\n\tif (ret) {\n\t\tttm_resource_free(bo, &hop_mem);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int ttm_bo_evict(struct ttm_buffer_object *bo,\n\t\t\tstruct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource *evict_mem;\n\tstruct ttm_placement placement;\n\tstruct ttm_place hop;\n\tint ret = 0;\n\n\tmemset(&hop, 0, sizeof(hop));\n\n\tdma_resv_assert_held(bo->base.resv);\n\n\tplacement.num_placement = 0;\n\tplacement.num_busy_placement = 0;\n\tbdev->funcs->evict_flags(bo, &placement);\n\n\tif (!placement.num_placement && !placement.num_busy_placement) {\n\t\tret = ttm_bo_wait_ctx(bo, ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\treturn ttm_bo_pipeline_gutting(bo);\n\t}\n\n\tret = ttm_bo_mem_space(bo, &placement, &evict_mem, ctx);\n\tif (ret) {\n\t\tif (ret != -ERESTARTSYS) {\n\t\t\tpr_err(\"Failed to find memory space for buffer 0x%p eviction\\n\",\n\t\t\t       bo);\n\t\t\tttm_bo_mem_space_debug(bo, &placement);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tret = ttm_bo_handle_move_mem(bo, evict_mem, true, ctx, &hop);\n\t\tif (ret != -EMULTIHOP)\n\t\t\tbreak;\n\n\t\tret = ttm_bo_bounce_temp_buffer(bo, &evict_mem, ctx, &hop);\n\t} while (!ret);\n\n\tif (ret) {\n\t\tttm_resource_free(bo, &evict_mem);\n\t\tif (ret != -ERESTARTSYS && ret != -EINTR)\n\t\t\tpr_err(\"Buffer eviction failed\\n\");\n\t}\nout:\n\treturn ret;\n}\n\n \nbool ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,\n\t\t\t      const struct ttm_place *place)\n{\n\tstruct ttm_resource *res = bo->resource;\n\tstruct ttm_device *bdev = bo->bdev;\n\n\tdma_resv_assert_held(bo->base.resv);\n\tif (bo->resource->mem_type == TTM_PL_SYSTEM)\n\t\treturn true;\n\n\t \n\treturn ttm_resource_intersects(bdev, res, place, bo->base.size);\n}\nEXPORT_SYMBOL(ttm_bo_eviction_valuable);\n\n \nstatic bool ttm_bo_evict_swapout_allowable(struct ttm_buffer_object *bo,\n\t\t\t\t\t   struct ttm_operation_ctx *ctx,\n\t\t\t\t\t   const struct ttm_place *place,\n\t\t\t\t\t   bool *locked, bool *busy)\n{\n\tbool ret = false;\n\n\tif (bo->pin_count) {\n\t\t*locked = false;\n\t\tif (busy)\n\t\t\t*busy = false;\n\t\treturn false;\n\t}\n\n\tif (bo->base.resv == ctx->resv) {\n\t\tdma_resv_assert_held(bo->base.resv);\n\t\tif (ctx->allow_res_evict)\n\t\t\tret = true;\n\t\t*locked = false;\n\t\tif (busy)\n\t\t\t*busy = false;\n\t} else {\n\t\tret = dma_resv_trylock(bo->base.resv);\n\t\t*locked = ret;\n\t\tif (busy)\n\t\t\t*busy = !ret;\n\t}\n\n\tif (ret && place && (bo->resource->mem_type != place->mem_type ||\n\t\t!bo->bdev->funcs->eviction_valuable(bo, place))) {\n\t\tret = false;\n\t\tif (*locked) {\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\t\t*locked = false;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int ttm_mem_evict_wait_busy(struct ttm_buffer_object *busy_bo,\n\t\t\t\t   struct ttm_operation_ctx *ctx,\n\t\t\t\t   struct ww_acquire_ctx *ticket)\n{\n\tint r;\n\n\tif (!busy_bo || !ticket)\n\t\treturn -EBUSY;\n\n\tif (ctx->interruptible)\n\t\tr = dma_resv_lock_interruptible(busy_bo->base.resv,\n\t\t\t\t\t\t\t  ticket);\n\telse\n\t\tr = dma_resv_lock(busy_bo->base.resv, ticket);\n\n\t \n\tif (!r)\n\t\tdma_resv_unlock(busy_bo->base.resv);\n\n\treturn r == -EDEADLK ? -EBUSY : r;\n}\n\nint ttm_mem_evict_first(struct ttm_device *bdev,\n\t\t\tstruct ttm_resource_manager *man,\n\t\t\tconst struct ttm_place *place,\n\t\t\tstruct ttm_operation_ctx *ctx,\n\t\t\tstruct ww_acquire_ctx *ticket)\n{\n\tstruct ttm_buffer_object *bo = NULL, *busy_bo = NULL;\n\tstruct ttm_resource_cursor cursor;\n\tstruct ttm_resource *res;\n\tbool locked = false;\n\tint ret;\n\n\tspin_lock(&bdev->lru_lock);\n\tttm_resource_manager_for_each_res(man, &cursor, res) {\n\t\tbool busy;\n\n\t\tif (!ttm_bo_evict_swapout_allowable(res->bo, ctx, place,\n\t\t\t\t\t\t    &locked, &busy)) {\n\t\t\tif (busy && !busy_bo && ticket !=\n\t\t\t    dma_resv_locking_ctx(res->bo->base.resv))\n\t\t\t\tbusy_bo = res->bo;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (ttm_bo_get_unless_zero(res->bo)) {\n\t\t\tbo = res->bo;\n\t\t\tbreak;\n\t\t}\n\t\tif (locked)\n\t\t\tdma_resv_unlock(res->bo->base.resv);\n\t}\n\n\tif (!bo) {\n\t\tif (busy_bo && !ttm_bo_get_unless_zero(busy_bo))\n\t\t\tbusy_bo = NULL;\n\t\tspin_unlock(&bdev->lru_lock);\n\t\tret = ttm_mem_evict_wait_busy(busy_bo, ctx, ticket);\n\t\tif (busy_bo)\n\t\t\tttm_bo_put(busy_bo);\n\t\treturn ret;\n\t}\n\n\tif (bo->deleted) {\n\t\tret = ttm_bo_cleanup_refs(bo, ctx->interruptible,\n\t\t\t\t\t  ctx->no_wait_gpu, locked);\n\t\tttm_bo_put(bo);\n\t\treturn ret;\n\t}\n\n\tspin_unlock(&bdev->lru_lock);\n\n\tret = ttm_bo_evict(bo, ctx);\n\tif (locked)\n\t\tttm_bo_unreserve(bo);\n\telse\n\t\tttm_bo_move_to_lru_tail_unlocked(bo);\n\n\tttm_bo_put(bo);\n\treturn ret;\n}\n\n \nvoid ttm_bo_pin(struct ttm_buffer_object *bo)\n{\n\tdma_resv_assert_held(bo->base.resv);\n\tWARN_ON_ONCE(!kref_read(&bo->kref));\n\tspin_lock(&bo->bdev->lru_lock);\n\tif (bo->resource)\n\t\tttm_resource_del_bulk_move(bo->resource, bo);\n\t++bo->pin_count;\n\tspin_unlock(&bo->bdev->lru_lock);\n}\nEXPORT_SYMBOL(ttm_bo_pin);\n\n \nvoid ttm_bo_unpin(struct ttm_buffer_object *bo)\n{\n\tdma_resv_assert_held(bo->base.resv);\n\tWARN_ON_ONCE(!kref_read(&bo->kref));\n\tif (WARN_ON_ONCE(!bo->pin_count))\n\t\treturn;\n\n\tspin_lock(&bo->bdev->lru_lock);\n\t--bo->pin_count;\n\tif (bo->resource)\n\t\tttm_resource_add_bulk_move(bo->resource, bo);\n\tspin_unlock(&bo->bdev->lru_lock);\n}\nEXPORT_SYMBOL(ttm_bo_unpin);\n\n \nstatic int ttm_bo_add_move_fence(struct ttm_buffer_object *bo,\n\t\t\t\t struct ttm_resource_manager *man,\n\t\t\t\t struct ttm_resource *mem,\n\t\t\t\t bool no_wait_gpu)\n{\n\tstruct dma_fence *fence;\n\tint ret;\n\n\tspin_lock(&man->move_lock);\n\tfence = dma_fence_get(man->move);\n\tspin_unlock(&man->move_lock);\n\n\tif (!fence)\n\t\treturn 0;\n\n\tif (no_wait_gpu) {\n\t\tret = dma_fence_is_signaled(fence) ? 0 : -EBUSY;\n\t\tdma_fence_put(fence);\n\t\treturn ret;\n\t}\n\n\tdma_resv_add_fence(bo->base.resv, fence, DMA_RESV_USAGE_KERNEL);\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tdma_fence_put(fence);\n\treturn ret;\n}\n\n \nstatic int ttm_bo_mem_force_space(struct ttm_buffer_object *bo,\n\t\t\t\t  const struct ttm_place *place,\n\t\t\t\t  struct ttm_resource **mem,\n\t\t\t\t  struct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct ttm_resource_manager *man;\n\tstruct ww_acquire_ctx *ticket;\n\tint ret;\n\n\tman = ttm_manager_type(bdev, place->mem_type);\n\tticket = dma_resv_locking_ctx(bo->base.resv);\n\tdo {\n\t\tret = ttm_resource_alloc(bo, place, mem);\n\t\tif (likely(!ret))\n\t\t\tbreak;\n\t\tif (unlikely(ret != -ENOSPC))\n\t\t\treturn ret;\n\t\tret = ttm_mem_evict_first(bdev, man, place, ctx,\n\t\t\t\t\t  ticket);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t} while (1);\n\n\treturn ttm_bo_add_move_fence(bo, man, *mem, ctx->no_wait_gpu);\n}\n\n \nint ttm_bo_mem_space(struct ttm_buffer_object *bo,\n\t\t\tstruct ttm_placement *placement,\n\t\t\tstruct ttm_resource **mem,\n\t\t\tstruct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\tbool type_found = false;\n\tint i, ret;\n\n\tret = dma_resv_reserve_fences(bo->base.resv, 1);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tfor (i = 0; i < placement->num_placement; ++i) {\n\t\tconst struct ttm_place *place = &placement->placement[i];\n\t\tstruct ttm_resource_manager *man;\n\n\t\tman = ttm_manager_type(bdev, place->mem_type);\n\t\tif (!man || !ttm_resource_manager_used(man))\n\t\t\tcontinue;\n\n\t\ttype_found = true;\n\t\tret = ttm_resource_alloc(bo, place, mem);\n\t\tif (ret == -ENOSPC)\n\t\t\tcontinue;\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = ttm_bo_add_move_fence(bo, man, *mem, ctx->no_wait_gpu);\n\t\tif (unlikely(ret)) {\n\t\t\tttm_resource_free(bo, mem);\n\t\t\tif (ret == -EBUSY)\n\t\t\t\tcontinue;\n\n\t\t\tgoto error;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < placement->num_busy_placement; ++i) {\n\t\tconst struct ttm_place *place = &placement->busy_placement[i];\n\t\tstruct ttm_resource_manager *man;\n\n\t\tman = ttm_manager_type(bdev, place->mem_type);\n\t\tif (!man || !ttm_resource_manager_used(man))\n\t\t\tcontinue;\n\n\t\ttype_found = true;\n\t\tret = ttm_bo_mem_force_space(bo, place, mem, ctx);\n\t\tif (likely(!ret))\n\t\t\treturn 0;\n\n\t\tif (ret && ret != -EBUSY)\n\t\t\tgoto error;\n\t}\n\n\tret = -ENOMEM;\n\tif (!type_found) {\n\t\tpr_err(TTM_PFX \"No compatible memory type found\\n\");\n\t\tret = -EINVAL;\n\t}\n\nerror:\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_mem_space);\n\nstatic int ttm_bo_move_buffer(struct ttm_buffer_object *bo,\n\t\t\t      struct ttm_placement *placement,\n\t\t\t      struct ttm_operation_ctx *ctx)\n{\n\tstruct ttm_resource *mem;\n\tstruct ttm_place hop;\n\tint ret;\n\n\tdma_resv_assert_held(bo->base.resv);\n\n\t \n\tret = ttm_bo_mem_space(bo, placement, &mem, ctx);\n\tif (ret)\n\t\treturn ret;\nbounce:\n\tret = ttm_bo_handle_move_mem(bo, mem, false, ctx, &hop);\n\tif (ret == -EMULTIHOP) {\n\t\tret = ttm_bo_bounce_temp_buffer(bo, &mem, ctx, &hop);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\t \n\t\tgoto bounce;\n\t}\nout:\n\tif (ret)\n\t\tttm_resource_free(bo, &mem);\n\treturn ret;\n}\n\n \nint ttm_bo_validate(struct ttm_buffer_object *bo,\n\t\t    struct ttm_placement *placement,\n\t\t    struct ttm_operation_ctx *ctx)\n{\n\tint ret;\n\n\tdma_resv_assert_held(bo->base.resv);\n\n\t \n\tif (!placement->num_placement && !placement->num_busy_placement)\n\t\treturn ttm_bo_pipeline_gutting(bo);\n\n\t \n\tif (bo->resource && ttm_resource_compat(bo->resource, placement))\n\t\treturn 0;\n\n\t \n\tif (bo->pin_count)\n\t\treturn -EINVAL;\n\n\tret = ttm_bo_move_buffer(bo, placement, ctx);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!bo->resource || bo->resource->mem_type == TTM_PL_SYSTEM) {\n\t\tret = ttm_tt_create(bo, true);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_validate);\n\n \nint ttm_bo_init_reserved(struct ttm_device *bdev, struct ttm_buffer_object *bo,\n\t\t\t enum ttm_bo_type type, struct ttm_placement *placement,\n\t\t\t uint32_t alignment, struct ttm_operation_ctx *ctx,\n\t\t\t struct sg_table *sg, struct dma_resv *resv,\n\t\t\t void (*destroy) (struct ttm_buffer_object *))\n{\n\tint ret;\n\n\tkref_init(&bo->kref);\n\tbo->bdev = bdev;\n\tbo->type = type;\n\tbo->page_alignment = alignment;\n\tbo->destroy = destroy;\n\tbo->pin_count = 0;\n\tbo->sg = sg;\n\tbo->bulk_move = NULL;\n\tif (resv)\n\t\tbo->base.resv = resv;\n\telse\n\t\tbo->base.resv = &bo->base._resv;\n\tatomic_inc(&ttm_glob.bo_count);\n\n\t \n\tif (bo->type == ttm_bo_type_device || bo->type == ttm_bo_type_sg) {\n\t\tret = drm_vma_offset_add(bdev->vma_manager, &bo->base.vma_node,\n\t\t\t\t\t PFN_UP(bo->base.size));\n\t\tif (ret)\n\t\t\tgoto err_put;\n\t}\n\n\t \n\tif (!resv)\n\t\tWARN_ON(!dma_resv_trylock(bo->base.resv));\n\telse\n\t\tdma_resv_assert_held(resv);\n\n\tret = ttm_bo_validate(bo, placement, ctx);\n\tif (unlikely(ret))\n\t\tgoto err_unlock;\n\n\treturn 0;\n\nerr_unlock:\n\tif (!resv)\n\t\tdma_resv_unlock(bo->base.resv);\n\nerr_put:\n\tttm_bo_put(bo);\n\treturn ret;\n}\nEXPORT_SYMBOL(ttm_bo_init_reserved);\n\n \nint ttm_bo_init_validate(struct ttm_device *bdev, struct ttm_buffer_object *bo,\n\t\t\t enum ttm_bo_type type, struct ttm_placement *placement,\n\t\t\t uint32_t alignment, bool interruptible,\n\t\t\t struct sg_table *sg, struct dma_resv *resv,\n\t\t\t void (*destroy) (struct ttm_buffer_object *))\n{\n\tstruct ttm_operation_ctx ctx = { interruptible, false };\n\tint ret;\n\n\tret = ttm_bo_init_reserved(bdev, bo, type, placement, alignment, &ctx,\n\t\t\t\t   sg, resv, destroy);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!resv)\n\t\tttm_bo_unreserve(bo);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_init_validate);\n\n \n\n \nvoid ttm_bo_unmap_virtual(struct ttm_buffer_object *bo)\n{\n\tstruct ttm_device *bdev = bo->bdev;\n\n\tdrm_vma_node_unmap(&bo->base.vma_node, bdev->dev_mapping);\n\tttm_mem_io_free(bdev, bo->resource);\n}\nEXPORT_SYMBOL(ttm_bo_unmap_virtual);\n\n \nint ttm_bo_wait_ctx(struct ttm_buffer_object *bo, struct ttm_operation_ctx *ctx)\n{\n\tlong ret;\n\n\tif (ctx->no_wait_gpu) {\n\t\tif (dma_resv_test_signaled(bo->base.resv,\n\t\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP))\n\t\t\treturn 0;\n\t\telse\n\t\t\treturn -EBUSY;\n\t}\n\n\tret = dma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t    ctx->interruptible, 15 * HZ);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\tif (unlikely(ret == 0))\n\t\treturn -EBUSY;\n\treturn 0;\n}\nEXPORT_SYMBOL(ttm_bo_wait_ctx);\n\nint ttm_bo_swapout(struct ttm_buffer_object *bo, struct ttm_operation_ctx *ctx,\n\t\t   gfp_t gfp_flags)\n{\n\tstruct ttm_place place;\n\tbool locked;\n\tlong ret;\n\n\t \n\tmemset(&place, 0, sizeof(place));\n\tplace.mem_type = bo->resource->mem_type;\n\tif (!ttm_bo_evict_swapout_allowable(bo, ctx, &place, &locked, NULL))\n\t\treturn -EBUSY;\n\n\tif (!bo->ttm || !ttm_tt_is_populated(bo->ttm) ||\n\t    bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL ||\n\t    bo->ttm->page_flags & TTM_TT_FLAG_SWAPPED ||\n\t    !ttm_bo_get_unless_zero(bo)) {\n\t\tif (locked)\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\treturn -EBUSY;\n\t}\n\n\tif (bo->deleted) {\n\t\tret = ttm_bo_cleanup_refs(bo, false, false, locked);\n\t\tttm_bo_put(bo);\n\t\treturn ret == -EBUSY ? -ENOSPC : ret;\n\t}\n\n\t \n\tspin_unlock(&bo->bdev->lru_lock);\n\n\t \n\tif (bo->resource->mem_type != TTM_PL_SYSTEM) {\n\t\tstruct ttm_resource *evict_mem;\n\t\tstruct ttm_place hop;\n\n\t\tmemset(&hop, 0, sizeof(hop));\n\t\tplace.mem_type = TTM_PL_SYSTEM;\n\t\tret = ttm_resource_alloc(bo, &place, &evict_mem);\n\t\tif (unlikely(ret))\n\t\t\tgoto out;\n\n\t\tret = ttm_bo_handle_move_mem(bo, evict_mem, true, ctx, &hop);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tWARN(ret == -EMULTIHOP, \"Unexpected multihop in swaput - likely driver bug.\\n\");\n\t\t\tttm_resource_free(bo, &evict_mem);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tret = ttm_bo_wait_ctx(bo, ctx);\n\tif (unlikely(ret != 0))\n\t\tgoto out;\n\n\tttm_bo_unmap_virtual(bo);\n\n\t \n\tif (bo->bdev->funcs->swap_notify)\n\t\tbo->bdev->funcs->swap_notify(bo);\n\n\tif (ttm_tt_is_populated(bo->ttm))\n\t\tret = ttm_tt_swapout(bo->bdev, bo->ttm, gfp_flags);\nout:\n\n\t \n\tif (locked)\n\t\tdma_resv_unlock(bo->base.resv);\n\tttm_bo_put(bo);\n\treturn ret == -EBUSY ? -ENOSPC : ret;\n}\n\nvoid ttm_bo_tt_destroy(struct ttm_buffer_object *bo)\n{\n\tif (bo->ttm == NULL)\n\t\treturn;\n\n\tttm_tt_unpopulate(bo->bdev, bo->ttm);\n\tttm_tt_destroy(bo->bdev, bo->ttm);\n\tbo->ttm = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}