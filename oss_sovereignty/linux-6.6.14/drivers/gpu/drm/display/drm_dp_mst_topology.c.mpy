{
  "module_name": "drm_dp_mst_topology.c",
  "hash_id": "c2cfa88ec6673da0ab3857395d4ec085875514c752d6878e4a759ef854ef05ed",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/display/drm_dp_mst_topology.c",
  "human_readable_source": " \n\n#include <linux/bitfield.h>\n#include <linux/delay.h>\n#include <linux/errno.h>\n#include <linux/i2c.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/random.h>\n#include <linux/sched.h>\n#include <linux/seq_file.h>\n#include <linux/iopoll.h>\n\n#if IS_ENABLED(CONFIG_DRM_DEBUG_DP_MST_TOPOLOGY_REFS)\n#include <linux/stacktrace.h>\n#include <linux/sort.h>\n#include <linux/timekeeping.h>\n#include <linux/math64.h>\n#endif\n\n#include <drm/display/drm_dp_mst_helper.h>\n#include <drm/drm_atomic.h>\n#include <drm/drm_atomic_helper.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_edid.h>\n#include <drm/drm_print.h>\n#include <drm/drm_probe_helper.h>\n\n#include \"drm_dp_helper_internal.h\"\n#include \"drm_dp_mst_topology_internal.h\"\n\n \nstruct drm_dp_pending_up_req {\n\tstruct drm_dp_sideband_msg_hdr hdr;\n\tstruct drm_dp_sideband_msg_req_body msg;\n\tstruct list_head next;\n};\n\nstatic bool dump_dp_payload_table(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t  char *buf);\n\nstatic void drm_dp_mst_topology_put_port(struct drm_dp_mst_port *port);\n\nstatic int drm_dp_dpcd_write_payload(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t     int id, u8 start_slot, u8 num_slots);\n\nstatic int drm_dp_send_dpcd_read(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_dp_mst_port *port,\n\t\t\t\t int offset, int size, u8 *bytes);\nstatic int drm_dp_send_dpcd_write(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t  struct drm_dp_mst_port *port,\n\t\t\t\t  int offset, int size, u8 *bytes);\n\nstatic int drm_dp_send_link_address(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t    struct drm_dp_mst_branch *mstb);\n\nstatic void\ndrm_dp_send_clear_payload_id_table(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t   struct drm_dp_mst_branch *mstb);\n\nstatic int drm_dp_send_enum_path_resources(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t   struct drm_dp_mst_branch *mstb,\n\t\t\t\t\t   struct drm_dp_mst_port *port);\nstatic bool drm_dp_validate_guid(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t u8 *guid);\n\nstatic int drm_dp_mst_register_i2c_bus(struct drm_dp_mst_port *port);\nstatic void drm_dp_mst_unregister_i2c_bus(struct drm_dp_mst_port *port);\nstatic void drm_dp_mst_kick_tx(struct drm_dp_mst_topology_mgr *mgr);\n\nstatic bool drm_dp_mst_port_downstream_of_branch(struct drm_dp_mst_port *port,\n\t\t\t\t\t\t struct drm_dp_mst_branch *branch);\n\n#define DBG_PREFIX \"[dp_mst]\"\n\n#define DP_STR(x) [DP_ ## x] = #x\n\nstatic const char *drm_dp_mst_req_type_str(u8 req_type)\n{\n\tstatic const char * const req_type_str[] = {\n\t\tDP_STR(GET_MSG_TRANSACTION_VERSION),\n\t\tDP_STR(LINK_ADDRESS),\n\t\tDP_STR(CONNECTION_STATUS_NOTIFY),\n\t\tDP_STR(ENUM_PATH_RESOURCES),\n\t\tDP_STR(ALLOCATE_PAYLOAD),\n\t\tDP_STR(QUERY_PAYLOAD),\n\t\tDP_STR(RESOURCE_STATUS_NOTIFY),\n\t\tDP_STR(CLEAR_PAYLOAD_ID_TABLE),\n\t\tDP_STR(REMOTE_DPCD_READ),\n\t\tDP_STR(REMOTE_DPCD_WRITE),\n\t\tDP_STR(REMOTE_I2C_READ),\n\t\tDP_STR(REMOTE_I2C_WRITE),\n\t\tDP_STR(POWER_UP_PHY),\n\t\tDP_STR(POWER_DOWN_PHY),\n\t\tDP_STR(SINK_EVENT_NOTIFY),\n\t\tDP_STR(QUERY_STREAM_ENC_STATUS),\n\t};\n\n\tif (req_type >= ARRAY_SIZE(req_type_str) ||\n\t    !req_type_str[req_type])\n\t\treturn \"unknown\";\n\n\treturn req_type_str[req_type];\n}\n\n#undef DP_STR\n#define DP_STR(x) [DP_NAK_ ## x] = #x\n\nstatic const char *drm_dp_mst_nak_reason_str(u8 nak_reason)\n{\n\tstatic const char * const nak_reason_str[] = {\n\t\tDP_STR(WRITE_FAILURE),\n\t\tDP_STR(INVALID_READ),\n\t\tDP_STR(CRC_FAILURE),\n\t\tDP_STR(BAD_PARAM),\n\t\tDP_STR(DEFER),\n\t\tDP_STR(LINK_FAILURE),\n\t\tDP_STR(NO_RESOURCES),\n\t\tDP_STR(DPCD_FAIL),\n\t\tDP_STR(I2C_NAK),\n\t\tDP_STR(ALLOCATE_FAIL),\n\t};\n\n\tif (nak_reason >= ARRAY_SIZE(nak_reason_str) ||\n\t    !nak_reason_str[nak_reason])\n\t\treturn \"unknown\";\n\n\treturn nak_reason_str[nak_reason];\n}\n\n#undef DP_STR\n#define DP_STR(x) [DRM_DP_SIDEBAND_TX_ ## x] = #x\n\nstatic const char *drm_dp_mst_sideband_tx_state_str(int state)\n{\n\tstatic const char * const sideband_reason_str[] = {\n\t\tDP_STR(QUEUED),\n\t\tDP_STR(START_SEND),\n\t\tDP_STR(SENT),\n\t\tDP_STR(RX),\n\t\tDP_STR(TIMEOUT),\n\t};\n\n\tif (state >= ARRAY_SIZE(sideband_reason_str) ||\n\t    !sideband_reason_str[state])\n\t\treturn \"unknown\";\n\n\treturn sideband_reason_str[state];\n}\n\nstatic int\ndrm_dp_mst_rad_to_str(const u8 rad[8], u8 lct, char *out, size_t len)\n{\n\tint i;\n\tu8 unpacked_rad[16];\n\n\tfor (i = 0; i < lct; i++) {\n\t\tif (i % 2)\n\t\t\tunpacked_rad[i] = rad[i / 2] >> 4;\n\t\telse\n\t\t\tunpacked_rad[i] = rad[i / 2] & BIT_MASK(4);\n\t}\n\n\t \n\treturn snprintf(out, len, \"%*phC\", lct, unpacked_rad);\n}\n\n \nstatic u8 drm_dp_msg_header_crc4(const uint8_t *data, size_t num_nibbles)\n{\n\tu8 bitmask = 0x80;\n\tu8 bitshift = 7;\n\tu8 array_index = 0;\n\tint number_of_bits = num_nibbles * 4;\n\tu8 remainder = 0;\n\n\twhile (number_of_bits != 0) {\n\t\tnumber_of_bits--;\n\t\tremainder <<= 1;\n\t\tremainder |= (data[array_index] & bitmask) >> bitshift;\n\t\tbitmask >>= 1;\n\t\tbitshift--;\n\t\tif (bitmask == 0) {\n\t\t\tbitmask = 0x80;\n\t\t\tbitshift = 7;\n\t\t\tarray_index++;\n\t\t}\n\t\tif ((remainder & 0x10) == 0x10)\n\t\t\tremainder ^= 0x13;\n\t}\n\n\tnumber_of_bits = 4;\n\twhile (number_of_bits != 0) {\n\t\tnumber_of_bits--;\n\t\tremainder <<= 1;\n\t\tif ((remainder & 0x10) != 0)\n\t\t\tremainder ^= 0x13;\n\t}\n\n\treturn remainder;\n}\n\nstatic u8 drm_dp_msg_data_crc4(const uint8_t *data, u8 number_of_bytes)\n{\n\tu8 bitmask = 0x80;\n\tu8 bitshift = 7;\n\tu8 array_index = 0;\n\tint number_of_bits = number_of_bytes * 8;\n\tu16 remainder = 0;\n\n\twhile (number_of_bits != 0) {\n\t\tnumber_of_bits--;\n\t\tremainder <<= 1;\n\t\tremainder |= (data[array_index] & bitmask) >> bitshift;\n\t\tbitmask >>= 1;\n\t\tbitshift--;\n\t\tif (bitmask == 0) {\n\t\t\tbitmask = 0x80;\n\t\t\tbitshift = 7;\n\t\t\tarray_index++;\n\t\t}\n\t\tif ((remainder & 0x100) == 0x100)\n\t\t\tremainder ^= 0xd5;\n\t}\n\n\tnumber_of_bits = 8;\n\twhile (number_of_bits != 0) {\n\t\tnumber_of_bits--;\n\t\tremainder <<= 1;\n\t\tif ((remainder & 0x100) != 0)\n\t\t\tremainder ^= 0xd5;\n\t}\n\n\treturn remainder & 0xff;\n}\nstatic inline u8 drm_dp_calc_sb_hdr_size(struct drm_dp_sideband_msg_hdr *hdr)\n{\n\tu8 size = 3;\n\n\tsize += (hdr->lct / 2);\n\treturn size;\n}\n\nstatic void drm_dp_encode_sideband_msg_hdr(struct drm_dp_sideband_msg_hdr *hdr,\n\t\t\t\t\t   u8 *buf, int *len)\n{\n\tint idx = 0;\n\tint i;\n\tu8 crc4;\n\n\tbuf[idx++] = ((hdr->lct & 0xf) << 4) | (hdr->lcr & 0xf);\n\tfor (i = 0; i < (hdr->lct / 2); i++)\n\t\tbuf[idx++] = hdr->rad[i];\n\tbuf[idx++] = (hdr->broadcast << 7) | (hdr->path_msg << 6) |\n\t\t(hdr->msg_len & 0x3f);\n\tbuf[idx++] = (hdr->somt << 7) | (hdr->eomt << 6) | (hdr->seqno << 4);\n\n\tcrc4 = drm_dp_msg_header_crc4(buf, (idx * 2) - 1);\n\tbuf[idx - 1] |= (crc4 & 0xf);\n\n\t*len = idx;\n}\n\nstatic bool drm_dp_decode_sideband_msg_hdr(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t   struct drm_dp_sideband_msg_hdr *hdr,\n\t\t\t\t\t   u8 *buf, int buflen, u8 *hdrlen)\n{\n\tu8 crc4;\n\tu8 len;\n\tint i;\n\tu8 idx;\n\n\tif (buf[0] == 0)\n\t\treturn false;\n\tlen = 3;\n\tlen += ((buf[0] & 0xf0) >> 4) / 2;\n\tif (len > buflen)\n\t\treturn false;\n\tcrc4 = drm_dp_msg_header_crc4(buf, (len * 2) - 1);\n\n\tif ((crc4 & 0xf) != (buf[len - 1] & 0xf)) {\n\t\tdrm_dbg_kms(mgr->dev, \"crc4 mismatch 0x%x 0x%x\\n\", crc4, buf[len - 1]);\n\t\treturn false;\n\t}\n\n\thdr->lct = (buf[0] & 0xf0) >> 4;\n\thdr->lcr = (buf[0] & 0xf);\n\tidx = 1;\n\tfor (i = 0; i < (hdr->lct / 2); i++)\n\t\thdr->rad[i] = buf[idx++];\n\thdr->broadcast = (buf[idx] >> 7) & 0x1;\n\thdr->path_msg = (buf[idx] >> 6) & 0x1;\n\thdr->msg_len = buf[idx] & 0x3f;\n\tidx++;\n\thdr->somt = (buf[idx] >> 7) & 0x1;\n\thdr->eomt = (buf[idx] >> 6) & 0x1;\n\thdr->seqno = (buf[idx] >> 4) & 0x1;\n\tidx++;\n\t*hdrlen = idx;\n\treturn true;\n}\n\nvoid\ndrm_dp_encode_sideband_req(const struct drm_dp_sideband_msg_req_body *req,\n\t\t\t   struct drm_dp_sideband_msg_tx *raw)\n{\n\tint idx = 0;\n\tint i;\n\tu8 *buf = raw->msg;\n\n\tbuf[idx++] = req->req_type & 0x7f;\n\n\tswitch (req->req_type) {\n\tcase DP_ENUM_PATH_RESOURCES:\n\tcase DP_POWER_DOWN_PHY:\n\tcase DP_POWER_UP_PHY:\n\t\tbuf[idx] = (req->u.port_num.port_number & 0xf) << 4;\n\t\tidx++;\n\t\tbreak;\n\tcase DP_ALLOCATE_PAYLOAD:\n\t\tbuf[idx] = (req->u.allocate_payload.port_number & 0xf) << 4 |\n\t\t\t(req->u.allocate_payload.number_sdp_streams & 0xf);\n\t\tidx++;\n\t\tbuf[idx] = (req->u.allocate_payload.vcpi & 0x7f);\n\t\tidx++;\n\t\tbuf[idx] = (req->u.allocate_payload.pbn >> 8);\n\t\tidx++;\n\t\tbuf[idx] = (req->u.allocate_payload.pbn & 0xff);\n\t\tidx++;\n\t\tfor (i = 0; i < req->u.allocate_payload.number_sdp_streams / 2; i++) {\n\t\t\tbuf[idx] = ((req->u.allocate_payload.sdp_stream_sink[i * 2] & 0xf) << 4) |\n\t\t\t\t(req->u.allocate_payload.sdp_stream_sink[i * 2 + 1] & 0xf);\n\t\t\tidx++;\n\t\t}\n\t\tif (req->u.allocate_payload.number_sdp_streams & 1) {\n\t\t\ti = req->u.allocate_payload.number_sdp_streams - 1;\n\t\t\tbuf[idx] = (req->u.allocate_payload.sdp_stream_sink[i] & 0xf) << 4;\n\t\t\tidx++;\n\t\t}\n\t\tbreak;\n\tcase DP_QUERY_PAYLOAD:\n\t\tbuf[idx] = (req->u.query_payload.port_number & 0xf) << 4;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.query_payload.vcpi & 0x7f);\n\t\tidx++;\n\t\tbreak;\n\tcase DP_REMOTE_DPCD_READ:\n\t\tbuf[idx] = (req->u.dpcd_read.port_number & 0xf) << 4;\n\t\tbuf[idx] |= ((req->u.dpcd_read.dpcd_address & 0xf0000) >> 16) & 0xf;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_read.dpcd_address & 0xff00) >> 8;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_read.dpcd_address & 0xff);\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_read.num_bytes);\n\t\tidx++;\n\t\tbreak;\n\n\tcase DP_REMOTE_DPCD_WRITE:\n\t\tbuf[idx] = (req->u.dpcd_write.port_number & 0xf) << 4;\n\t\tbuf[idx] |= ((req->u.dpcd_write.dpcd_address & 0xf0000) >> 16) & 0xf;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_write.dpcd_address & 0xff00) >> 8;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_write.dpcd_address & 0xff);\n\t\tidx++;\n\t\tbuf[idx] = (req->u.dpcd_write.num_bytes);\n\t\tidx++;\n\t\tmemcpy(&buf[idx], req->u.dpcd_write.bytes, req->u.dpcd_write.num_bytes);\n\t\tidx += req->u.dpcd_write.num_bytes;\n\t\tbreak;\n\tcase DP_REMOTE_I2C_READ:\n\t\tbuf[idx] = (req->u.i2c_read.port_number & 0xf) << 4;\n\t\tbuf[idx] |= (req->u.i2c_read.num_transactions & 0x3);\n\t\tidx++;\n\t\tfor (i = 0; i < (req->u.i2c_read.num_transactions & 0x3); i++) {\n\t\t\tbuf[idx] = req->u.i2c_read.transactions[i].i2c_dev_id & 0x7f;\n\t\t\tidx++;\n\t\t\tbuf[idx] = req->u.i2c_read.transactions[i].num_bytes;\n\t\t\tidx++;\n\t\t\tmemcpy(&buf[idx], req->u.i2c_read.transactions[i].bytes, req->u.i2c_read.transactions[i].num_bytes);\n\t\t\tidx += req->u.i2c_read.transactions[i].num_bytes;\n\n\t\t\tbuf[idx] = (req->u.i2c_read.transactions[i].no_stop_bit & 0x1) << 4;\n\t\t\tbuf[idx] |= (req->u.i2c_read.transactions[i].i2c_transaction_delay & 0xf);\n\t\t\tidx++;\n\t\t}\n\t\tbuf[idx] = (req->u.i2c_read.read_i2c_device_id) & 0x7f;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.i2c_read.num_bytes_read);\n\t\tidx++;\n\t\tbreak;\n\n\tcase DP_REMOTE_I2C_WRITE:\n\t\tbuf[idx] = (req->u.i2c_write.port_number & 0xf) << 4;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.i2c_write.write_i2c_device_id) & 0x7f;\n\t\tidx++;\n\t\tbuf[idx] = (req->u.i2c_write.num_bytes);\n\t\tidx++;\n\t\tmemcpy(&buf[idx], req->u.i2c_write.bytes, req->u.i2c_write.num_bytes);\n\t\tidx += req->u.i2c_write.num_bytes;\n\t\tbreak;\n\tcase DP_QUERY_STREAM_ENC_STATUS: {\n\t\tconst struct drm_dp_query_stream_enc_status *msg;\n\n\t\tmsg = &req->u.enc_status;\n\t\tbuf[idx] = msg->stream_id;\n\t\tidx++;\n\t\tmemcpy(&buf[idx], msg->client_id, sizeof(msg->client_id));\n\t\tidx += sizeof(msg->client_id);\n\t\tbuf[idx] = 0;\n\t\tbuf[idx] |= FIELD_PREP(GENMASK(1, 0), msg->stream_event);\n\t\tbuf[idx] |= msg->valid_stream_event ? BIT(2) : 0;\n\t\tbuf[idx] |= FIELD_PREP(GENMASK(4, 3), msg->stream_behavior);\n\t\tbuf[idx] |= msg->valid_stream_behavior ? BIT(5) : 0;\n\t\tidx++;\n\t\t}\n\t\tbreak;\n\t}\n\traw->cur_len = idx;\n}\nEXPORT_SYMBOL_FOR_TESTS_ONLY(drm_dp_encode_sideband_req);\n\n \nint\ndrm_dp_decode_sideband_req(const struct drm_dp_sideband_msg_tx *raw,\n\t\t\t   struct drm_dp_sideband_msg_req_body *req)\n{\n\tconst u8 *buf = raw->msg;\n\tint i, idx = 0;\n\n\treq->req_type = buf[idx++] & 0x7f;\n\tswitch (req->req_type) {\n\tcase DP_ENUM_PATH_RESOURCES:\n\tcase DP_POWER_DOWN_PHY:\n\tcase DP_POWER_UP_PHY:\n\t\treq->u.port_num.port_number = (buf[idx] >> 4) & 0xf;\n\t\tbreak;\n\tcase DP_ALLOCATE_PAYLOAD:\n\t\t{\n\t\t\tstruct drm_dp_allocate_payload *a =\n\t\t\t\t&req->u.allocate_payload;\n\n\t\t\ta->number_sdp_streams = buf[idx] & 0xf;\n\t\t\ta->port_number = (buf[idx] >> 4) & 0xf;\n\n\t\t\tWARN_ON(buf[++idx] & 0x80);\n\t\t\ta->vcpi = buf[idx] & 0x7f;\n\n\t\t\ta->pbn = buf[++idx] << 8;\n\t\t\ta->pbn |= buf[++idx];\n\n\t\t\tidx++;\n\t\t\tfor (i = 0; i < a->number_sdp_streams; i++) {\n\t\t\t\ta->sdp_stream_sink[i] =\n\t\t\t\t\t(buf[idx + (i / 2)] >> ((i % 2) ? 0 : 4)) & 0xf;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase DP_QUERY_PAYLOAD:\n\t\treq->u.query_payload.port_number = (buf[idx] >> 4) & 0xf;\n\t\tWARN_ON(buf[++idx] & 0x80);\n\t\treq->u.query_payload.vcpi = buf[idx] & 0x7f;\n\t\tbreak;\n\tcase DP_REMOTE_DPCD_READ:\n\t\t{\n\t\t\tstruct drm_dp_remote_dpcd_read *r = &req->u.dpcd_read;\n\n\t\t\tr->port_number = (buf[idx] >> 4) & 0xf;\n\n\t\t\tr->dpcd_address = (buf[idx] << 16) & 0xf0000;\n\t\t\tr->dpcd_address |= (buf[++idx] << 8) & 0xff00;\n\t\t\tr->dpcd_address |= buf[++idx] & 0xff;\n\n\t\t\tr->num_bytes = buf[++idx];\n\t\t}\n\t\tbreak;\n\tcase DP_REMOTE_DPCD_WRITE:\n\t\t{\n\t\t\tstruct drm_dp_remote_dpcd_write *w =\n\t\t\t\t&req->u.dpcd_write;\n\n\t\t\tw->port_number = (buf[idx] >> 4) & 0xf;\n\n\t\t\tw->dpcd_address = (buf[idx] << 16) & 0xf0000;\n\t\t\tw->dpcd_address |= (buf[++idx] << 8) & 0xff00;\n\t\t\tw->dpcd_address |= buf[++idx] & 0xff;\n\n\t\t\tw->num_bytes = buf[++idx];\n\n\t\t\tw->bytes = kmemdup(&buf[++idx], w->num_bytes,\n\t\t\t\t\t   GFP_KERNEL);\n\t\t\tif (!w->bytes)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tbreak;\n\tcase DP_REMOTE_I2C_READ:\n\t\t{\n\t\t\tstruct drm_dp_remote_i2c_read *r = &req->u.i2c_read;\n\t\t\tstruct drm_dp_remote_i2c_read_tx *tx;\n\t\t\tbool failed = false;\n\n\t\t\tr->num_transactions = buf[idx] & 0x3;\n\t\t\tr->port_number = (buf[idx] >> 4) & 0xf;\n\t\t\tfor (i = 0; i < r->num_transactions; i++) {\n\t\t\t\ttx = &r->transactions[i];\n\n\t\t\t\ttx->i2c_dev_id = buf[++idx] & 0x7f;\n\t\t\t\ttx->num_bytes = buf[++idx];\n\t\t\t\ttx->bytes = kmemdup(&buf[++idx],\n\t\t\t\t\t\t    tx->num_bytes,\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\t\t\tif (!tx->bytes) {\n\t\t\t\t\tfailed = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tidx += tx->num_bytes;\n\t\t\t\ttx->no_stop_bit = (buf[idx] >> 5) & 0x1;\n\t\t\t\ttx->i2c_transaction_delay = buf[idx] & 0xf;\n\t\t\t}\n\n\t\t\tif (failed) {\n\t\t\t\tfor (i = 0; i < r->num_transactions; i++) {\n\t\t\t\t\ttx = &r->transactions[i];\n\t\t\t\t\tkfree(tx->bytes);\n\t\t\t\t}\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\tr->read_i2c_device_id = buf[++idx] & 0x7f;\n\t\t\tr->num_bytes_read = buf[++idx];\n\t\t}\n\t\tbreak;\n\tcase DP_REMOTE_I2C_WRITE:\n\t\t{\n\t\t\tstruct drm_dp_remote_i2c_write *w = &req->u.i2c_write;\n\n\t\t\tw->port_number = (buf[idx] >> 4) & 0xf;\n\t\t\tw->write_i2c_device_id = buf[++idx] & 0x7f;\n\t\t\tw->num_bytes = buf[++idx];\n\t\t\tw->bytes = kmemdup(&buf[++idx], w->num_bytes,\n\t\t\t\t\t   GFP_KERNEL);\n\t\t\tif (!w->bytes)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\t\tbreak;\n\tcase DP_QUERY_STREAM_ENC_STATUS:\n\t\treq->u.enc_status.stream_id = buf[idx++];\n\t\tfor (i = 0; i < sizeof(req->u.enc_status.client_id); i++)\n\t\t\treq->u.enc_status.client_id[i] = buf[idx++];\n\n\t\treq->u.enc_status.stream_event = FIELD_GET(GENMASK(1, 0),\n\t\t\t\t\t\t\t   buf[idx]);\n\t\treq->u.enc_status.valid_stream_event = FIELD_GET(BIT(2),\n\t\t\t\t\t\t\t\t buf[idx]);\n\t\treq->u.enc_status.stream_behavior = FIELD_GET(GENMASK(4, 3),\n\t\t\t\t\t\t\t      buf[idx]);\n\t\treq->u.enc_status.valid_stream_behavior = FIELD_GET(BIT(5),\n\t\t\t\t\t\t\t\t    buf[idx]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_FOR_TESTS_ONLY(drm_dp_decode_sideband_req);\n\nvoid\ndrm_dp_dump_sideband_msg_req_body(const struct drm_dp_sideband_msg_req_body *req,\n\t\t\t\t  int indent, struct drm_printer *printer)\n{\n\tint i;\n\n#define P(f, ...) drm_printf_indent(printer, indent, f, ##__VA_ARGS__)\n\tif (req->req_type == DP_LINK_ADDRESS) {\n\t\t \n\t\tP(\"type=%s\\n\", drm_dp_mst_req_type_str(req->req_type));\n\t\treturn;\n\t}\n\n\tP(\"type=%s contents:\\n\", drm_dp_mst_req_type_str(req->req_type));\n\tindent++;\n\n\tswitch (req->req_type) {\n\tcase DP_ENUM_PATH_RESOURCES:\n\tcase DP_POWER_DOWN_PHY:\n\tcase DP_POWER_UP_PHY:\n\t\tP(\"port=%d\\n\", req->u.port_num.port_number);\n\t\tbreak;\n\tcase DP_ALLOCATE_PAYLOAD:\n\t\tP(\"port=%d vcpi=%d pbn=%d sdp_streams=%d %*ph\\n\",\n\t\t  req->u.allocate_payload.port_number,\n\t\t  req->u.allocate_payload.vcpi, req->u.allocate_payload.pbn,\n\t\t  req->u.allocate_payload.number_sdp_streams,\n\t\t  req->u.allocate_payload.number_sdp_streams,\n\t\t  req->u.allocate_payload.sdp_stream_sink);\n\t\tbreak;\n\tcase DP_QUERY_PAYLOAD:\n\t\tP(\"port=%d vcpi=%d\\n\",\n\t\t  req->u.query_payload.port_number,\n\t\t  req->u.query_payload.vcpi);\n\t\tbreak;\n\tcase DP_REMOTE_DPCD_READ:\n\t\tP(\"port=%d dpcd_addr=%05x len=%d\\n\",\n\t\t  req->u.dpcd_read.port_number, req->u.dpcd_read.dpcd_address,\n\t\t  req->u.dpcd_read.num_bytes);\n\t\tbreak;\n\tcase DP_REMOTE_DPCD_WRITE:\n\t\tP(\"port=%d addr=%05x len=%d: %*ph\\n\",\n\t\t  req->u.dpcd_write.port_number,\n\t\t  req->u.dpcd_write.dpcd_address,\n\t\t  req->u.dpcd_write.num_bytes, req->u.dpcd_write.num_bytes,\n\t\t  req->u.dpcd_write.bytes);\n\t\tbreak;\n\tcase DP_REMOTE_I2C_READ:\n\t\tP(\"port=%d num_tx=%d id=%d size=%d:\\n\",\n\t\t  req->u.i2c_read.port_number,\n\t\t  req->u.i2c_read.num_transactions,\n\t\t  req->u.i2c_read.read_i2c_device_id,\n\t\t  req->u.i2c_read.num_bytes_read);\n\n\t\tindent++;\n\t\tfor (i = 0; i < req->u.i2c_read.num_transactions; i++) {\n\t\t\tconst struct drm_dp_remote_i2c_read_tx *rtx =\n\t\t\t\t&req->u.i2c_read.transactions[i];\n\n\t\t\tP(\"%d: id=%03d size=%03d no_stop_bit=%d tx_delay=%03d: %*ph\\n\",\n\t\t\t  i, rtx->i2c_dev_id, rtx->num_bytes,\n\t\t\t  rtx->no_stop_bit, rtx->i2c_transaction_delay,\n\t\t\t  rtx->num_bytes, rtx->bytes);\n\t\t}\n\t\tbreak;\n\tcase DP_REMOTE_I2C_WRITE:\n\t\tP(\"port=%d id=%d size=%d: %*ph\\n\",\n\t\t  req->u.i2c_write.port_number,\n\t\t  req->u.i2c_write.write_i2c_device_id,\n\t\t  req->u.i2c_write.num_bytes, req->u.i2c_write.num_bytes,\n\t\t  req->u.i2c_write.bytes);\n\t\tbreak;\n\tcase DP_QUERY_STREAM_ENC_STATUS:\n\t\tP(\"stream_id=%u client_id=%*ph stream_event=%x \"\n\t\t  \"valid_event=%d stream_behavior=%x valid_behavior=%d\",\n\t\t  req->u.enc_status.stream_id,\n\t\t  (int)ARRAY_SIZE(req->u.enc_status.client_id),\n\t\t  req->u.enc_status.client_id, req->u.enc_status.stream_event,\n\t\t  req->u.enc_status.valid_stream_event,\n\t\t  req->u.enc_status.stream_behavior,\n\t\t  req->u.enc_status.valid_stream_behavior);\n\t\tbreak;\n\tdefault:\n\t\tP(\"???\\n\");\n\t\tbreak;\n\t}\n#undef P\n}\nEXPORT_SYMBOL_FOR_TESTS_ONLY(drm_dp_dump_sideband_msg_req_body);\n\nstatic inline void\ndrm_dp_mst_dump_sideband_msg_tx(struct drm_printer *p,\n\t\t\t\tconst struct drm_dp_sideband_msg_tx *txmsg)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\tchar buf[64];\n\tint ret;\n\tint i;\n\n\tdrm_dp_mst_rad_to_str(txmsg->dst->rad, txmsg->dst->lct, buf,\n\t\t\t      sizeof(buf));\n\tdrm_printf(p, \"txmsg cur_offset=%x cur_len=%x seqno=%x state=%s path_msg=%d dst=%s\\n\",\n\t\t   txmsg->cur_offset, txmsg->cur_len, txmsg->seqno,\n\t\t   drm_dp_mst_sideband_tx_state_str(txmsg->state),\n\t\t   txmsg->path_msg, buf);\n\n\tret = drm_dp_decode_sideband_req(txmsg, &req);\n\tif (ret) {\n\t\tdrm_printf(p, \"<failed to decode sideband req: %d>\\n\", ret);\n\t\treturn;\n\t}\n\tdrm_dp_dump_sideband_msg_req_body(&req, 1, p);\n\n\tswitch (req.req_type) {\n\tcase DP_REMOTE_DPCD_WRITE:\n\t\tkfree(req.u.dpcd_write.bytes);\n\t\tbreak;\n\tcase DP_REMOTE_I2C_READ:\n\t\tfor (i = 0; i < req.u.i2c_read.num_transactions; i++)\n\t\t\tkfree(req.u.i2c_read.transactions[i].bytes);\n\t\tbreak;\n\tcase DP_REMOTE_I2C_WRITE:\n\t\tkfree(req.u.i2c_write.bytes);\n\t\tbreak;\n\t}\n}\n\nstatic void drm_dp_crc_sideband_chunk_req(u8 *msg, u8 len)\n{\n\tu8 crc4;\n\n\tcrc4 = drm_dp_msg_data_crc4(msg, len);\n\tmsg[len] = crc4;\n}\n\nstatic void drm_dp_encode_sideband_reply(struct drm_dp_sideband_msg_reply_body *rep,\n\t\t\t\t\t struct drm_dp_sideband_msg_tx *raw)\n{\n\tint idx = 0;\n\tu8 *buf = raw->msg;\n\n\tbuf[idx++] = (rep->reply_type & 0x1) << 7 | (rep->req_type & 0x7f);\n\n\traw->cur_len = idx;\n}\n\nstatic int drm_dp_sideband_msg_set_header(struct drm_dp_sideband_msg_rx *msg,\n\t\t\t\t\t  struct drm_dp_sideband_msg_hdr *hdr,\n\t\t\t\t\t  u8 hdrlen)\n{\n\t \n\tif (!hdr->somt && !msg->have_somt)\n\t\treturn false;\n\n\t \n\tmsg->curchunk_idx = 0;\n\tmsg->curchunk_len = hdr->msg_len;\n\tmsg->curchunk_hdrlen = hdrlen;\n\n\t \n\tif (hdr->somt && msg->have_somt)\n\t\treturn false;\n\n\tif (hdr->somt) {\n\t\tmemcpy(&msg->initial_hdr, hdr,\n\t\t       sizeof(struct drm_dp_sideband_msg_hdr));\n\t\tmsg->have_somt = true;\n\t}\n\tif (hdr->eomt)\n\t\tmsg->have_eomt = true;\n\n\treturn true;\n}\n\n \nstatic bool drm_dp_sideband_append_payload(struct drm_dp_sideband_msg_rx *msg,\n\t\t\t\t\t   u8 *replybuf, u8 replybuflen)\n{\n\tu8 crc4;\n\n\tmemcpy(&msg->chunk[msg->curchunk_idx], replybuf, replybuflen);\n\tmsg->curchunk_idx += replybuflen;\n\n\tif (msg->curchunk_idx >= msg->curchunk_len) {\n\t\t \n\t\tcrc4 = drm_dp_msg_data_crc4(msg->chunk, msg->curchunk_len - 1);\n\t\tif (crc4 != msg->chunk[msg->curchunk_len - 1])\n\t\t\tprint_hex_dump(KERN_DEBUG, \"wrong crc\",\n\t\t\t\t       DUMP_PREFIX_NONE, 16, 1,\n\t\t\t\t       msg->chunk,  msg->curchunk_len, false);\n\t\t \n\t\tmemcpy(&msg->msg[msg->curlen], msg->chunk, msg->curchunk_len - 1);\n\t\tmsg->curlen += msg->curchunk_len - 1;\n\t}\n\treturn true;\n}\n\nstatic bool drm_dp_sideband_parse_link_address(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t       struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t       struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\tint i;\n\n\tmemcpy(repmsg->u.link_addr.guid, &raw->msg[idx], 16);\n\tidx += 16;\n\trepmsg->u.link_addr.nports = raw->msg[idx] & 0xf;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\tfor (i = 0; i < repmsg->u.link_addr.nports; i++) {\n\t\tif (raw->msg[idx] & 0x80)\n\t\t\trepmsg->u.link_addr.ports[i].input_port = 1;\n\n\t\trepmsg->u.link_addr.ports[i].peer_device_type = (raw->msg[idx] >> 4) & 0x7;\n\t\trepmsg->u.link_addr.ports[i].port_number = (raw->msg[idx] & 0xf);\n\n\t\tidx++;\n\t\tif (idx > raw->curlen)\n\t\t\tgoto fail_len;\n\t\trepmsg->u.link_addr.ports[i].mcs = (raw->msg[idx] >> 7) & 0x1;\n\t\trepmsg->u.link_addr.ports[i].ddps = (raw->msg[idx] >> 6) & 0x1;\n\t\tif (repmsg->u.link_addr.ports[i].input_port == 0)\n\t\t\trepmsg->u.link_addr.ports[i].legacy_device_plug_status = (raw->msg[idx] >> 5) & 0x1;\n\t\tidx++;\n\t\tif (idx > raw->curlen)\n\t\t\tgoto fail_len;\n\t\tif (repmsg->u.link_addr.ports[i].input_port == 0) {\n\t\t\trepmsg->u.link_addr.ports[i].dpcd_revision = (raw->msg[idx]);\n\t\t\tidx++;\n\t\t\tif (idx > raw->curlen)\n\t\t\t\tgoto fail_len;\n\t\t\tmemcpy(repmsg->u.link_addr.ports[i].peer_guid, &raw->msg[idx], 16);\n\t\t\tidx += 16;\n\t\t\tif (idx > raw->curlen)\n\t\t\t\tgoto fail_len;\n\t\t\trepmsg->u.link_addr.ports[i].num_sdp_streams = (raw->msg[idx] >> 4) & 0xf;\n\t\t\trepmsg->u.link_addr.ports[i].num_sdp_stream_sinks = (raw->msg[idx] & 0xf);\n\t\t\tidx++;\n\n\t\t}\n\t\tif (idx > raw->curlen)\n\t\t\tgoto fail_len;\n\t}\n\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"link address reply parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_remote_dpcd_read(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t   struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.remote_dpcd_read_ack.port_number = raw->msg[idx] & 0xf;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.remote_dpcd_read_ack.num_bytes = raw->msg[idx];\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\n\tmemcpy(repmsg->u.remote_dpcd_read_ack.bytes, &raw->msg[idx], repmsg->u.remote_dpcd_read_ack.num_bytes);\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"link address reply parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_remote_dpcd_write(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t      struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.remote_dpcd_write_ack.port_number = raw->msg[idx] & 0xf;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_remote_i2c_read_ack(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t      struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.remote_i2c_read_ack.port_number = (raw->msg[idx] & 0xf);\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.remote_i2c_read_ack.num_bytes = raw->msg[idx];\n\tidx++;\n\t \n\tmemcpy(repmsg->u.remote_i2c_read_ack.bytes, &raw->msg[idx], repmsg->u.remote_i2c_read_ack.num_bytes);\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"remote i2c reply parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_enum_path_resources_ack(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t\t  struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.path_resources.port_number = (raw->msg[idx] >> 4) & 0xf;\n\trepmsg->u.path_resources.fec_capable = raw->msg[idx] & 0x1;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.path_resources.full_payload_bw_number = (raw->msg[idx] << 8) | (raw->msg[idx+1]);\n\tidx += 2;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.path_resources.avail_payload_bw_number = (raw->msg[idx] << 8) | (raw->msg[idx+1]);\n\tidx += 2;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"enum resource parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_allocate_payload_ack(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t\t  struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.allocate_payload.port_number = (raw->msg[idx] >> 4) & 0xf;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.allocate_payload.vcpi = raw->msg[idx];\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.allocate_payload.allocated_pbn = (raw->msg[idx] << 8) | (raw->msg[idx+1]);\n\tidx += 2;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"allocate payload parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_query_payload_ack(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t    struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.query_payload.port_number = (raw->msg[idx] >> 4) & 0xf;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\trepmsg->u.query_payload.allocated_pbn = (raw->msg[idx] << 8) | (raw->msg[idx + 1]);\n\tidx += 2;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\treturn true;\nfail_len:\n\tDRM_DEBUG_KMS(\"query payload parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_power_updown_phy_ack(struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t       struct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tint idx = 1;\n\n\trepmsg->u.port_number.port_number = (raw->msg[idx] >> 4) & 0xf;\n\tidx++;\n\tif (idx > raw->curlen) {\n\t\tDRM_DEBUG_KMS(\"power up/down phy parse length fail %d %d\\n\",\n\t\t\t      idx, raw->curlen);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool\ndrm_dp_sideband_parse_query_stream_enc_status(\n\t\t\t\tstruct drm_dp_sideband_msg_rx *raw,\n\t\t\t\tstruct drm_dp_sideband_msg_reply_body *repmsg)\n{\n\tstruct drm_dp_query_stream_enc_status_ack_reply *reply;\n\n\treply = &repmsg->u.enc_status;\n\n\treply->stream_id = raw->msg[3];\n\n\treply->reply_signed = raw->msg[2] & BIT(0);\n\n\t \n\treply->hdcp_1x_device_present = raw->msg[2] & BIT(4);\n\treply->hdcp_2x_device_present = raw->msg[2] & BIT(3);\n\n\treply->query_capable_device_present = raw->msg[2] & BIT(5);\n\treply->legacy_device_present = raw->msg[2] & BIT(6);\n\treply->unauthorizable_device_present = raw->msg[2] & BIT(7);\n\n\treply->auth_completed = !!(raw->msg[1] & BIT(3));\n\treply->encryption_enabled = !!(raw->msg[1] & BIT(4));\n\treply->repeater_present = !!(raw->msg[1] & BIT(5));\n\treply->state = (raw->msg[1] & GENMASK(7, 6)) >> 6;\n\n\treturn true;\n}\n\nstatic bool drm_dp_sideband_parse_reply(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\tstruct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\tstruct drm_dp_sideband_msg_reply_body *msg)\n{\n\tmemset(msg, 0, sizeof(*msg));\n\tmsg->reply_type = (raw->msg[0] & 0x80) >> 7;\n\tmsg->req_type = (raw->msg[0] & 0x7f);\n\n\tif (msg->reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\tmemcpy(msg->u.nak.guid, &raw->msg[1], 16);\n\t\tmsg->u.nak.reason = raw->msg[17];\n\t\tmsg->u.nak.nak_data = raw->msg[18];\n\t\treturn false;\n\t}\n\n\tswitch (msg->req_type) {\n\tcase DP_LINK_ADDRESS:\n\t\treturn drm_dp_sideband_parse_link_address(mgr, raw, msg);\n\tcase DP_QUERY_PAYLOAD:\n\t\treturn drm_dp_sideband_parse_query_payload_ack(raw, msg);\n\tcase DP_REMOTE_DPCD_READ:\n\t\treturn drm_dp_sideband_parse_remote_dpcd_read(raw, msg);\n\tcase DP_REMOTE_DPCD_WRITE:\n\t\treturn drm_dp_sideband_parse_remote_dpcd_write(raw, msg);\n\tcase DP_REMOTE_I2C_READ:\n\t\treturn drm_dp_sideband_parse_remote_i2c_read_ack(raw, msg);\n\tcase DP_REMOTE_I2C_WRITE:\n\t\treturn true;  \n\tcase DP_ENUM_PATH_RESOURCES:\n\t\treturn drm_dp_sideband_parse_enum_path_resources_ack(raw, msg);\n\tcase DP_ALLOCATE_PAYLOAD:\n\t\treturn drm_dp_sideband_parse_allocate_payload_ack(raw, msg);\n\tcase DP_POWER_DOWN_PHY:\n\tcase DP_POWER_UP_PHY:\n\t\treturn drm_dp_sideband_parse_power_updown_phy_ack(raw, msg);\n\tcase DP_CLEAR_PAYLOAD_ID_TABLE:\n\t\treturn true;  \n\tcase DP_QUERY_STREAM_ENC_STATUS:\n\t\treturn drm_dp_sideband_parse_query_stream_enc_status(raw, msg);\n\tdefault:\n\t\tdrm_err(mgr->dev, \"Got unknown reply 0x%02x (%s)\\n\",\n\t\t\tmsg->req_type, drm_dp_mst_req_type_str(msg->req_type));\n\t\treturn false;\n\t}\n}\n\nstatic bool\ndrm_dp_sideband_parse_connection_status_notify(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t       struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t       struct drm_dp_sideband_msg_req_body *msg)\n{\n\tint idx = 1;\n\n\tmsg->u.conn_stat.port_number = (raw->msg[idx] & 0xf0) >> 4;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\n\tmemcpy(msg->u.conn_stat.guid, &raw->msg[idx], 16);\n\tidx += 16;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\n\tmsg->u.conn_stat.legacy_device_plug_status = (raw->msg[idx] >> 6) & 0x1;\n\tmsg->u.conn_stat.displayport_device_plug_status = (raw->msg[idx] >> 5) & 0x1;\n\tmsg->u.conn_stat.message_capability_status = (raw->msg[idx] >> 4) & 0x1;\n\tmsg->u.conn_stat.input_port = (raw->msg[idx] >> 3) & 0x1;\n\tmsg->u.conn_stat.peer_device_type = (raw->msg[idx] & 0x7);\n\tidx++;\n\treturn true;\nfail_len:\n\tdrm_dbg_kms(mgr->dev, \"connection status reply parse length fail %d %d\\n\",\n\t\t    idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_resource_status_notify(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t\t\t struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t\t\t\t struct drm_dp_sideband_msg_req_body *msg)\n{\n\tint idx = 1;\n\n\tmsg->u.resource_stat.port_number = (raw->msg[idx] & 0xf0) >> 4;\n\tidx++;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\n\tmemcpy(msg->u.resource_stat.guid, &raw->msg[idx], 16);\n\tidx += 16;\n\tif (idx > raw->curlen)\n\t\tgoto fail_len;\n\n\tmsg->u.resource_stat.available_pbn = (raw->msg[idx] << 8) | (raw->msg[idx + 1]);\n\tidx++;\n\treturn true;\nfail_len:\n\tdrm_dbg_kms(mgr->dev, \"resource status reply parse length fail %d %d\\n\", idx, raw->curlen);\n\treturn false;\n}\n\nstatic bool drm_dp_sideband_parse_req(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t      struct drm_dp_sideband_msg_rx *raw,\n\t\t\t\t      struct drm_dp_sideband_msg_req_body *msg)\n{\n\tmemset(msg, 0, sizeof(*msg));\n\tmsg->req_type = (raw->msg[0] & 0x7f);\n\n\tswitch (msg->req_type) {\n\tcase DP_CONNECTION_STATUS_NOTIFY:\n\t\treturn drm_dp_sideband_parse_connection_status_notify(mgr, raw, msg);\n\tcase DP_RESOURCE_STATUS_NOTIFY:\n\t\treturn drm_dp_sideband_parse_resource_status_notify(mgr, raw, msg);\n\tdefault:\n\t\tdrm_err(mgr->dev, \"Got unknown request 0x%02x (%s)\\n\",\n\t\t\tmsg->req_type, drm_dp_mst_req_type_str(msg->req_type));\n\t\treturn false;\n\t}\n}\n\nstatic void build_dpcd_write(struct drm_dp_sideband_msg_tx *msg,\n\t\t\t     u8 port_num, u32 offset, u8 num_bytes, u8 *bytes)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_REMOTE_DPCD_WRITE;\n\treq.u.dpcd_write.port_number = port_num;\n\treq.u.dpcd_write.dpcd_address = offset;\n\treq.u.dpcd_write.num_bytes = num_bytes;\n\treq.u.dpcd_write.bytes = bytes;\n\tdrm_dp_encode_sideband_req(&req, msg);\n}\n\nstatic void build_link_address(struct drm_dp_sideband_msg_tx *msg)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_LINK_ADDRESS;\n\tdrm_dp_encode_sideband_req(&req, msg);\n}\n\nstatic void build_clear_payload_id_table(struct drm_dp_sideband_msg_tx *msg)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_CLEAR_PAYLOAD_ID_TABLE;\n\tdrm_dp_encode_sideband_req(&req, msg);\n\tmsg->path_msg = true;\n}\n\nstatic int build_enum_path_resources(struct drm_dp_sideband_msg_tx *msg,\n\t\t\t\t     int port_num)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_ENUM_PATH_RESOURCES;\n\treq.u.port_num.port_number = port_num;\n\tdrm_dp_encode_sideband_req(&req, msg);\n\tmsg->path_msg = true;\n\treturn 0;\n}\n\nstatic void build_allocate_payload(struct drm_dp_sideband_msg_tx *msg,\n\t\t\t\t   int port_num,\n\t\t\t\t   u8 vcpi, uint16_t pbn,\n\t\t\t\t   u8 number_sdp_streams,\n\t\t\t\t   u8 *sdp_stream_sink)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\tmemset(&req, 0, sizeof(req));\n\treq.req_type = DP_ALLOCATE_PAYLOAD;\n\treq.u.allocate_payload.port_number = port_num;\n\treq.u.allocate_payload.vcpi = vcpi;\n\treq.u.allocate_payload.pbn = pbn;\n\treq.u.allocate_payload.number_sdp_streams = number_sdp_streams;\n\tmemcpy(req.u.allocate_payload.sdp_stream_sink, sdp_stream_sink,\n\t\t   number_sdp_streams);\n\tdrm_dp_encode_sideband_req(&req, msg);\n\tmsg->path_msg = true;\n}\n\nstatic void build_power_updown_phy(struct drm_dp_sideband_msg_tx *msg,\n\t\t\t\t   int port_num, bool power_up)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\tif (power_up)\n\t\treq.req_type = DP_POWER_UP_PHY;\n\telse\n\t\treq.req_type = DP_POWER_DOWN_PHY;\n\n\treq.u.port_num.port_number = port_num;\n\tdrm_dp_encode_sideband_req(&req, msg);\n\tmsg->path_msg = true;\n}\n\nstatic int\nbuild_query_stream_enc_status(struct drm_dp_sideband_msg_tx *msg, u8 stream_id,\n\t\t\t      u8 *q_id)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_QUERY_STREAM_ENC_STATUS;\n\treq.u.enc_status.stream_id = stream_id;\n\tmemcpy(req.u.enc_status.client_id, q_id,\n\t       sizeof(req.u.enc_status.client_id));\n\treq.u.enc_status.stream_event = 0;\n\treq.u.enc_status.valid_stream_event = false;\n\treq.u.enc_status.stream_behavior = 0;\n\treq.u.enc_status.valid_stream_behavior = false;\n\n\tdrm_dp_encode_sideband_req(&req, msg);\n\treturn 0;\n}\n\nstatic bool check_txmsg_state(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t      struct drm_dp_sideband_msg_tx *txmsg)\n{\n\tunsigned int state;\n\n\t \n\tstate = READ_ONCE(txmsg->state);\n\treturn (state == DRM_DP_SIDEBAND_TX_RX ||\n\t\tstate == DRM_DP_SIDEBAND_TX_TIMEOUT);\n}\n\nstatic int drm_dp_mst_wait_tx_reply(struct drm_dp_mst_branch *mstb,\n\t\t\t\t    struct drm_dp_sideband_msg_tx *txmsg)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = mstb->mgr;\n\tunsigned long wait_timeout = msecs_to_jiffies(4000);\n\tunsigned long wait_expires = jiffies + wait_timeout;\n\tint ret;\n\n\tfor (;;) {\n\t\t \n\t\tret = wait_event_timeout(mgr->tx_waitq,\n\t\t\t\t\t check_txmsg_state(mgr, txmsg),\n\t\t\t\t\t mgr->cbs->poll_hpd_irq ?\n\t\t\t\t\t\tmsecs_to_jiffies(50) :\n\t\t\t\t\t\twait_timeout);\n\n\t\tif (ret || !mgr->cbs->poll_hpd_irq ||\n\t\t    time_after(jiffies, wait_expires))\n\t\t\tbreak;\n\n\t\tmgr->cbs->poll_hpd_irq(mgr);\n\t}\n\n\tmutex_lock(&mgr->qlock);\n\tif (ret > 0) {\n\t\tif (txmsg->state == DRM_DP_SIDEBAND_TX_TIMEOUT) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tdrm_dbg_kms(mgr->dev, \"timedout msg send %p %d %d\\n\",\n\t\t\t    txmsg, txmsg->state, txmsg->seqno);\n\n\t\t \n\t\tret = -EIO;\n\n\t\t \n\t\tif (txmsg->state == DRM_DP_SIDEBAND_TX_QUEUED ||\n\t\t    txmsg->state == DRM_DP_SIDEBAND_TX_START_SEND ||\n\t\t    txmsg->state == DRM_DP_SIDEBAND_TX_SENT)\n\t\t\tlist_del(&txmsg->next);\n\t}\nout:\n\tif (unlikely(ret == -EIO) && drm_debug_enabled(DRM_UT_DP)) {\n\t\tstruct drm_printer p = drm_debug_printer(DBG_PREFIX);\n\n\t\tdrm_dp_mst_dump_sideband_msg_tx(&p, txmsg);\n\t}\n\tmutex_unlock(&mgr->qlock);\n\n\tdrm_dp_mst_kick_tx(mgr);\n\treturn ret;\n}\n\nstatic struct drm_dp_mst_branch *drm_dp_add_mst_branch_device(u8 lct, u8 *rad)\n{\n\tstruct drm_dp_mst_branch *mstb;\n\n\tmstb = kzalloc(sizeof(*mstb), GFP_KERNEL);\n\tif (!mstb)\n\t\treturn NULL;\n\n\tmstb->lct = lct;\n\tif (lct > 1)\n\t\tmemcpy(mstb->rad, rad, lct / 2);\n\tINIT_LIST_HEAD(&mstb->ports);\n\tkref_init(&mstb->topology_kref);\n\tkref_init(&mstb->malloc_kref);\n\treturn mstb;\n}\n\nstatic void drm_dp_free_mst_branch_device(struct kref *kref)\n{\n\tstruct drm_dp_mst_branch *mstb =\n\t\tcontainer_of(kref, struct drm_dp_mst_branch, malloc_kref);\n\n\tif (mstb->port_parent)\n\t\tdrm_dp_mst_put_port_malloc(mstb->port_parent);\n\n\tkfree(mstb);\n}\n\n \n\n \nstatic void\ndrm_dp_mst_get_mstb_malloc(struct drm_dp_mst_branch *mstb)\n{\n\tkref_get(&mstb->malloc_kref);\n\tdrm_dbg(mstb->mgr->dev, \"mstb %p (%d)\\n\", mstb, kref_read(&mstb->malloc_kref));\n}\n\n \nstatic void\ndrm_dp_mst_put_mstb_malloc(struct drm_dp_mst_branch *mstb)\n{\n\tdrm_dbg(mstb->mgr->dev, \"mstb %p (%d)\\n\", mstb, kref_read(&mstb->malloc_kref) - 1);\n\tkref_put(&mstb->malloc_kref, drm_dp_free_mst_branch_device);\n}\n\nstatic void drm_dp_free_mst_port(struct kref *kref)\n{\n\tstruct drm_dp_mst_port *port =\n\t\tcontainer_of(kref, struct drm_dp_mst_port, malloc_kref);\n\n\tdrm_dp_mst_put_mstb_malloc(port->parent);\n\tkfree(port);\n}\n\n \nvoid\ndrm_dp_mst_get_port_malloc(struct drm_dp_mst_port *port)\n{\n\tkref_get(&port->malloc_kref);\n\tdrm_dbg(port->mgr->dev, \"port %p (%d)\\n\", port, kref_read(&port->malloc_kref));\n}\nEXPORT_SYMBOL(drm_dp_mst_get_port_malloc);\n\n \nvoid\ndrm_dp_mst_put_port_malloc(struct drm_dp_mst_port *port)\n{\n\tdrm_dbg(port->mgr->dev, \"port %p (%d)\\n\", port, kref_read(&port->malloc_kref) - 1);\n\tkref_put(&port->malloc_kref, drm_dp_free_mst_port);\n}\nEXPORT_SYMBOL(drm_dp_mst_put_port_malloc);\n\n#if IS_ENABLED(CONFIG_DRM_DEBUG_DP_MST_TOPOLOGY_REFS)\n\n#define STACK_DEPTH 8\n\nstatic noinline void\n__topology_ref_save(struct drm_dp_mst_topology_mgr *mgr,\n\t\t    struct drm_dp_mst_topology_ref_history *history,\n\t\t    enum drm_dp_mst_topology_ref_type type)\n{\n\tstruct drm_dp_mst_topology_ref_entry *entry = NULL;\n\tdepot_stack_handle_t backtrace;\n\tulong stack_entries[STACK_DEPTH];\n\tuint n;\n\tint i;\n\n\tn = stack_trace_save(stack_entries, ARRAY_SIZE(stack_entries), 1);\n\tbacktrace = stack_depot_save(stack_entries, n, GFP_KERNEL);\n\tif (!backtrace)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < history->len; i++) {\n\t\tif (history->entries[i].backtrace == backtrace) {\n\t\t\tentry = &history->entries[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (!entry) {\n\t\tstruct drm_dp_mst_topology_ref_entry *new;\n\t\tint new_len = history->len + 1;\n\n\t\tnew = krealloc(history->entries, sizeof(*new) * new_len,\n\t\t\t       GFP_KERNEL);\n\t\tif (!new)\n\t\t\treturn;\n\n\t\tentry = &new[history->len];\n\t\thistory->len = new_len;\n\t\thistory->entries = new;\n\n\t\tentry->backtrace = backtrace;\n\t\tentry->type = type;\n\t\tentry->count = 0;\n\t}\n\tentry->count++;\n\tentry->ts_nsec = ktime_get_ns();\n}\n\nstatic int\ntopology_ref_history_cmp(const void *a, const void *b)\n{\n\tconst struct drm_dp_mst_topology_ref_entry *entry_a = a, *entry_b = b;\n\n\tif (entry_a->ts_nsec > entry_b->ts_nsec)\n\t\treturn 1;\n\telse if (entry_a->ts_nsec < entry_b->ts_nsec)\n\t\treturn -1;\n\telse\n\t\treturn 0;\n}\n\nstatic inline const char *\ntopology_ref_type_to_str(enum drm_dp_mst_topology_ref_type type)\n{\n\tif (type == DRM_DP_MST_TOPOLOGY_REF_GET)\n\t\treturn \"get\";\n\telse\n\t\treturn \"put\";\n}\n\nstatic void\n__dump_topology_ref_history(struct drm_dp_mst_topology_ref_history *history,\n\t\t\t    void *ptr, const char *type_str)\n{\n\tstruct drm_printer p = drm_debug_printer(DBG_PREFIX);\n\tchar *buf = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\tint i;\n\n\tif (!buf)\n\t\treturn;\n\n\tif (!history->len)\n\t\tgoto out;\n\n\t \n\tsort(history->entries, history->len, sizeof(*history->entries),\n\t     topology_ref_history_cmp, NULL);\n\n\tdrm_printf(&p, \"%s (%p) topology count reached 0, dumping history:\\n\",\n\t\t   type_str, ptr);\n\n\tfor (i = 0; i < history->len; i++) {\n\t\tconst struct drm_dp_mst_topology_ref_entry *entry =\n\t\t\t&history->entries[i];\n\t\tu64 ts_nsec = entry->ts_nsec;\n\t\tu32 rem_nsec = do_div(ts_nsec, 1000000000);\n\n\t\tstack_depot_snprint(entry->backtrace, buf, PAGE_SIZE, 4);\n\n\t\tdrm_printf(&p, \"  %d %ss (last at %5llu.%06u):\\n%s\",\n\t\t\t   entry->count,\n\t\t\t   topology_ref_type_to_str(entry->type),\n\t\t\t   ts_nsec, rem_nsec / 1000, buf);\n\t}\n\n\t \n\tkfree(history->entries);\nout:\n\tkfree(buf);\n}\n\nstatic __always_inline void\ndrm_dp_mst_dump_mstb_topology_history(struct drm_dp_mst_branch *mstb)\n{\n\t__dump_topology_ref_history(&mstb->topology_ref_history, mstb,\n\t\t\t\t    \"MSTB\");\n}\n\nstatic __always_inline void\ndrm_dp_mst_dump_port_topology_history(struct drm_dp_mst_port *port)\n{\n\t__dump_topology_ref_history(&port->topology_ref_history, port,\n\t\t\t\t    \"Port\");\n}\n\nstatic __always_inline void\nsave_mstb_topology_ref(struct drm_dp_mst_branch *mstb,\n\t\t       enum drm_dp_mst_topology_ref_type type)\n{\n\t__topology_ref_save(mstb->mgr, &mstb->topology_ref_history, type);\n}\n\nstatic __always_inline void\nsave_port_topology_ref(struct drm_dp_mst_port *port,\n\t\t       enum drm_dp_mst_topology_ref_type type)\n{\n\t__topology_ref_save(port->mgr, &port->topology_ref_history, type);\n}\n\nstatic inline void\ntopology_ref_history_lock(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tmutex_lock(&mgr->topology_ref_history_lock);\n}\n\nstatic inline void\ntopology_ref_history_unlock(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tmutex_unlock(&mgr->topology_ref_history_lock);\n}\n#else\nstatic inline void\ntopology_ref_history_lock(struct drm_dp_mst_topology_mgr *mgr) {}\nstatic inline void\ntopology_ref_history_unlock(struct drm_dp_mst_topology_mgr *mgr) {}\nstatic inline void\ndrm_dp_mst_dump_mstb_topology_history(struct drm_dp_mst_branch *mstb) {}\nstatic inline void\ndrm_dp_mst_dump_port_topology_history(struct drm_dp_mst_port *port) {}\n#define save_mstb_topology_ref(mstb, type)\n#define save_port_topology_ref(port, type)\n#endif\n\nstruct drm_dp_mst_atomic_payload *\ndrm_atomic_get_mst_payload_state(struct drm_dp_mst_topology_state *state,\n\t\t\t\t struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_atomic_payload *payload;\n\n\tlist_for_each_entry(payload, &state->payloads, next)\n\t\tif (payload->port == port)\n\t\t\treturn payload;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(drm_atomic_get_mst_payload_state);\n\nstatic void drm_dp_destroy_mst_branch_device(struct kref *kref)\n{\n\tstruct drm_dp_mst_branch *mstb =\n\t\tcontainer_of(kref, struct drm_dp_mst_branch, topology_kref);\n\tstruct drm_dp_mst_topology_mgr *mgr = mstb->mgr;\n\n\tdrm_dp_mst_dump_mstb_topology_history(mstb);\n\n\tINIT_LIST_HEAD(&mstb->destroy_next);\n\n\t \n\tmutex_lock(&mgr->delayed_destroy_lock);\n\tlist_add(&mstb->destroy_next, &mgr->destroy_branch_device_list);\n\tmutex_unlock(&mgr->delayed_destroy_lock);\n\tqueue_work(mgr->delayed_destroy_wq, &mgr->delayed_destroy_work);\n}\n\n \nstatic int __must_check\ndrm_dp_mst_topology_try_get_mstb(struct drm_dp_mst_branch *mstb)\n{\n\tint ret;\n\n\ttopology_ref_history_lock(mstb->mgr);\n\tret = kref_get_unless_zero(&mstb->topology_kref);\n\tif (ret) {\n\t\tdrm_dbg(mstb->mgr->dev, \"mstb %p (%d)\\n\", mstb, kref_read(&mstb->topology_kref));\n\t\tsave_mstb_topology_ref(mstb, DRM_DP_MST_TOPOLOGY_REF_GET);\n\t}\n\n\ttopology_ref_history_unlock(mstb->mgr);\n\n\treturn ret;\n}\n\n \nstatic void drm_dp_mst_topology_get_mstb(struct drm_dp_mst_branch *mstb)\n{\n\ttopology_ref_history_lock(mstb->mgr);\n\n\tsave_mstb_topology_ref(mstb, DRM_DP_MST_TOPOLOGY_REF_GET);\n\tWARN_ON(kref_read(&mstb->topology_kref) == 0);\n\tkref_get(&mstb->topology_kref);\n\tdrm_dbg(mstb->mgr->dev, \"mstb %p (%d)\\n\", mstb, kref_read(&mstb->topology_kref));\n\n\ttopology_ref_history_unlock(mstb->mgr);\n}\n\n \nstatic void\ndrm_dp_mst_topology_put_mstb(struct drm_dp_mst_branch *mstb)\n{\n\ttopology_ref_history_lock(mstb->mgr);\n\n\tdrm_dbg(mstb->mgr->dev, \"mstb %p (%d)\\n\", mstb, kref_read(&mstb->topology_kref) - 1);\n\tsave_mstb_topology_ref(mstb, DRM_DP_MST_TOPOLOGY_REF_PUT);\n\n\ttopology_ref_history_unlock(mstb->mgr);\n\tkref_put(&mstb->topology_kref, drm_dp_destroy_mst_branch_device);\n}\n\nstatic void drm_dp_destroy_port(struct kref *kref)\n{\n\tstruct drm_dp_mst_port *port =\n\t\tcontainer_of(kref, struct drm_dp_mst_port, topology_kref);\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\n\tdrm_dp_mst_dump_port_topology_history(port);\n\n\t \n\tif (port->input) {\n\t\tdrm_dp_mst_put_port_malloc(port);\n\t\treturn;\n\t}\n\n\tdrm_edid_free(port->cached_edid);\n\n\t \n\tmutex_lock(&mgr->delayed_destroy_lock);\n\tlist_add(&port->next, &mgr->destroy_port_list);\n\tmutex_unlock(&mgr->delayed_destroy_lock);\n\tqueue_work(mgr->delayed_destroy_wq, &mgr->delayed_destroy_work);\n}\n\n \nstatic int __must_check\ndrm_dp_mst_topology_try_get_port(struct drm_dp_mst_port *port)\n{\n\tint ret;\n\n\ttopology_ref_history_lock(port->mgr);\n\tret = kref_get_unless_zero(&port->topology_kref);\n\tif (ret) {\n\t\tdrm_dbg(port->mgr->dev, \"port %p (%d)\\n\", port, kref_read(&port->topology_kref));\n\t\tsave_port_topology_ref(port, DRM_DP_MST_TOPOLOGY_REF_GET);\n\t}\n\n\ttopology_ref_history_unlock(port->mgr);\n\treturn ret;\n}\n\n \nstatic void drm_dp_mst_topology_get_port(struct drm_dp_mst_port *port)\n{\n\ttopology_ref_history_lock(port->mgr);\n\n\tWARN_ON(kref_read(&port->topology_kref) == 0);\n\tkref_get(&port->topology_kref);\n\tdrm_dbg(port->mgr->dev, \"port %p (%d)\\n\", port, kref_read(&port->topology_kref));\n\tsave_port_topology_ref(port, DRM_DP_MST_TOPOLOGY_REF_GET);\n\n\ttopology_ref_history_unlock(port->mgr);\n}\n\n \nstatic void drm_dp_mst_topology_put_port(struct drm_dp_mst_port *port)\n{\n\ttopology_ref_history_lock(port->mgr);\n\n\tdrm_dbg(port->mgr->dev, \"port %p (%d)\\n\", port, kref_read(&port->topology_kref) - 1);\n\tsave_port_topology_ref(port, DRM_DP_MST_TOPOLOGY_REF_PUT);\n\n\ttopology_ref_history_unlock(port->mgr);\n\tkref_put(&port->topology_kref, drm_dp_destroy_port);\n}\n\nstatic struct drm_dp_mst_branch *\ndrm_dp_mst_topology_get_mstb_validated_locked(struct drm_dp_mst_branch *mstb,\n\t\t\t\t\t      struct drm_dp_mst_branch *to_find)\n{\n\tstruct drm_dp_mst_port *port;\n\tstruct drm_dp_mst_branch *rmstb;\n\n\tif (to_find == mstb)\n\t\treturn mstb;\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tif (port->mstb) {\n\t\t\trmstb = drm_dp_mst_topology_get_mstb_validated_locked(\n\t\t\t    port->mstb, to_find);\n\t\t\tif (rmstb)\n\t\t\t\treturn rmstb;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct drm_dp_mst_branch *\ndrm_dp_mst_topology_get_mstb_validated(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t       struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_mst_branch *rmstb = NULL;\n\n\tmutex_lock(&mgr->lock);\n\tif (mgr->mst_primary) {\n\t\trmstb = drm_dp_mst_topology_get_mstb_validated_locked(\n\t\t    mgr->mst_primary, mstb);\n\n\t\tif (rmstb && !drm_dp_mst_topology_try_get_mstb(rmstb))\n\t\t\trmstb = NULL;\n\t}\n\tmutex_unlock(&mgr->lock);\n\treturn rmstb;\n}\n\nstatic struct drm_dp_mst_port *\ndrm_dp_mst_topology_get_port_validated_locked(struct drm_dp_mst_branch *mstb,\n\t\t\t\t\t      struct drm_dp_mst_port *to_find)\n{\n\tstruct drm_dp_mst_port *port, *mport;\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tif (port == to_find)\n\t\t\treturn port;\n\n\t\tif (port->mstb) {\n\t\t\tmport = drm_dp_mst_topology_get_port_validated_locked(\n\t\t\t    port->mstb, to_find);\n\t\t\tif (mport)\n\t\t\t\treturn mport;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct drm_dp_mst_port *\ndrm_dp_mst_topology_get_port_validated(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t       struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_port *rport = NULL;\n\n\tmutex_lock(&mgr->lock);\n\tif (mgr->mst_primary) {\n\t\trport = drm_dp_mst_topology_get_port_validated_locked(\n\t\t    mgr->mst_primary, port);\n\n\t\tif (rport && !drm_dp_mst_topology_try_get_port(rport))\n\t\t\trport = NULL;\n\t}\n\tmutex_unlock(&mgr->lock);\n\treturn rport;\n}\n\nstatic struct drm_dp_mst_port *drm_dp_get_port(struct drm_dp_mst_branch *mstb, u8 port_num)\n{\n\tstruct drm_dp_mst_port *port;\n\tint ret;\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tif (port->port_num == port_num) {\n\t\t\tret = drm_dp_mst_topology_try_get_port(port);\n\t\t\treturn ret ? port : NULL;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic u8 drm_dp_calculate_rad(struct drm_dp_mst_port *port,\n\t\t\t\t u8 *rad)\n{\n\tint parent_lct = port->parent->lct;\n\tint shift = 4;\n\tint idx = (parent_lct - 1) / 2;\n\n\tif (parent_lct > 1) {\n\t\tmemcpy(rad, port->parent->rad, idx + 1);\n\t\tshift = (parent_lct % 2) ? 4 : 0;\n\t} else\n\t\trad[0] = 0;\n\n\trad[idx] |= port->port_num << shift;\n\treturn parent_lct + 1;\n}\n\nstatic bool drm_dp_mst_is_end_device(u8 pdt, bool mcs)\n{\n\tswitch (pdt) {\n\tcase DP_PEER_DEVICE_DP_LEGACY_CONV:\n\tcase DP_PEER_DEVICE_SST_SINK:\n\t\treturn true;\n\tcase DP_PEER_DEVICE_MST_BRANCHING:\n\t\t \n\t\tif (!mcs)\n\t\t\treturn true;\n\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int\ndrm_dp_port_set_pdt(struct drm_dp_mst_port *port, u8 new_pdt,\n\t\t    bool new_mcs)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\tstruct drm_dp_mst_branch *mstb;\n\tu8 rad[8], lct;\n\tint ret = 0;\n\n\tif (port->pdt == new_pdt && port->mcs == new_mcs)\n\t\treturn 0;\n\n\t \n\tif (port->pdt != DP_PEER_DEVICE_NONE) {\n\t\tif (drm_dp_mst_is_end_device(port->pdt, port->mcs)) {\n\t\t\t \n\t\t\tif (new_pdt != DP_PEER_DEVICE_NONE &&\n\t\t\t    drm_dp_mst_is_end_device(new_pdt, new_mcs)) {\n\t\t\t\tport->pdt = new_pdt;\n\t\t\t\tport->mcs = new_mcs;\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tdrm_dp_mst_unregister_i2c_bus(port);\n\t\t} else {\n\t\t\tmutex_lock(&mgr->lock);\n\t\t\tdrm_dp_mst_topology_put_mstb(port->mstb);\n\t\t\tport->mstb = NULL;\n\t\t\tmutex_unlock(&mgr->lock);\n\t\t}\n\t}\n\n\tport->pdt = new_pdt;\n\tport->mcs = new_mcs;\n\n\tif (port->pdt != DP_PEER_DEVICE_NONE) {\n\t\tif (drm_dp_mst_is_end_device(port->pdt, port->mcs)) {\n\t\t\t \n\t\t\tret = drm_dp_mst_register_i2c_bus(port);\n\t\t} else {\n\t\t\tlct = drm_dp_calculate_rad(port, rad);\n\t\t\tmstb = drm_dp_add_mst_branch_device(lct, rad);\n\t\t\tif (!mstb) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tdrm_err(mgr->dev, \"Failed to create MSTB for port %p\", port);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmutex_lock(&mgr->lock);\n\t\t\tport->mstb = mstb;\n\t\t\tmstb->mgr = port->mgr;\n\t\t\tmstb->port_parent = port;\n\n\t\t\t \n\t\t\tdrm_dp_mst_get_port_malloc(port);\n\t\t\tmutex_unlock(&mgr->lock);\n\n\t\t\t \n\t\t\tret = 1;\n\t\t}\n\t}\n\nout:\n\tif (ret < 0)\n\t\tport->pdt = DP_PEER_DEVICE_NONE;\n\treturn ret;\n}\n\n \nssize_t drm_dp_mst_dpcd_read(struct drm_dp_aux *aux,\n\t\t\t     unsigned int offset, void *buffer, size_t size)\n{\n\tstruct drm_dp_mst_port *port = container_of(aux, struct drm_dp_mst_port,\n\t\t\t\t\t\t    aux);\n\n\treturn drm_dp_send_dpcd_read(port->mgr, port,\n\t\t\t\t     offset, size, buffer);\n}\n\n \nssize_t drm_dp_mst_dpcd_write(struct drm_dp_aux *aux,\n\t\t\t      unsigned int offset, void *buffer, size_t size)\n{\n\tstruct drm_dp_mst_port *port = container_of(aux, struct drm_dp_mst_port,\n\t\t\t\t\t\t    aux);\n\n\treturn drm_dp_send_dpcd_write(port->mgr, port,\n\t\t\t\t      offset, size, buffer);\n}\n\nstatic int drm_dp_check_mstb_guid(struct drm_dp_mst_branch *mstb, u8 *guid)\n{\n\tint ret = 0;\n\n\tmemcpy(mstb->guid, guid, 16);\n\n\tif (!drm_dp_validate_guid(mstb->mgr, mstb->guid)) {\n\t\tif (mstb->port_parent) {\n\t\t\tret = drm_dp_send_dpcd_write(mstb->mgr,\n\t\t\t\t\t\t     mstb->port_parent,\n\t\t\t\t\t\t     DP_GUID, 16, mstb->guid);\n\t\t} else {\n\t\t\tret = drm_dp_dpcd_write(mstb->mgr->aux,\n\t\t\t\t\t\tDP_GUID, mstb->guid, 16);\n\t\t}\n\t}\n\n\tif (ret < 16 && ret > 0)\n\t\treturn -EPROTO;\n\n\treturn ret == 16 ? 0 : ret;\n}\n\nstatic void build_mst_prop_path(const struct drm_dp_mst_branch *mstb,\n\t\t\t\tint pnum,\n\t\t\t\tchar *proppath,\n\t\t\t\tsize_t proppath_size)\n{\n\tint i;\n\tchar temp[8];\n\n\tsnprintf(proppath, proppath_size, \"mst:%d\", mstb->mgr->conn_base_id);\n\tfor (i = 0; i < (mstb->lct - 1); i++) {\n\t\tint shift = (i % 2) ? 0 : 4;\n\t\tint port_num = (mstb->rad[i / 2] >> shift) & 0xf;\n\n\t\tsnprintf(temp, sizeof(temp), \"-%d\", port_num);\n\t\tstrlcat(proppath, temp, proppath_size);\n\t}\n\tsnprintf(temp, sizeof(temp), \"-%d\", pnum);\n\tstrlcat(proppath, temp, proppath_size);\n}\n\n \nint drm_dp_mst_connector_late_register(struct drm_connector *connector,\n\t\t\t\t       struct drm_dp_mst_port *port)\n{\n\tdrm_dbg_kms(port->mgr->dev, \"registering %s remote bus for %s\\n\",\n\t\t    port->aux.name, connector->kdev->kobj.name);\n\n\tport->aux.dev = connector->kdev;\n\treturn drm_dp_aux_register_devnode(&port->aux);\n}\nEXPORT_SYMBOL(drm_dp_mst_connector_late_register);\n\n \nvoid drm_dp_mst_connector_early_unregister(struct drm_connector *connector,\n\t\t\t\t\t   struct drm_dp_mst_port *port)\n{\n\tdrm_dbg_kms(port->mgr->dev, \"unregistering %s remote bus for %s\\n\",\n\t\t    port->aux.name, connector->kdev->kobj.name);\n\tdrm_dp_aux_unregister_devnode(&port->aux);\n}\nEXPORT_SYMBOL(drm_dp_mst_connector_early_unregister);\n\nstatic void\ndrm_dp_mst_port_add_connector(struct drm_dp_mst_branch *mstb,\n\t\t\t      struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\tchar proppath[255];\n\tint ret;\n\n\tbuild_mst_prop_path(mstb, port->port_num, proppath, sizeof(proppath));\n\tport->connector = mgr->cbs->add_connector(mgr, port, proppath);\n\tif (!port->connector) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tif (port->pdt != DP_PEER_DEVICE_NONE &&\n\t    drm_dp_mst_is_end_device(port->pdt, port->mcs) &&\n\t    port->port_num >= DP_MST_LOGICAL_PORT_0)\n\t\tport->cached_edid = drm_edid_read_ddc(port->connector,\n\t\t\t\t\t\t      &port->aux.ddc);\n\n\tdrm_connector_register(port->connector);\n\treturn;\n\nerror:\n\tdrm_err(mgr->dev, \"Failed to create connector for port %p: %d\\n\", port, ret);\n}\n\n \nstatic void\ndrm_dp_mst_topology_unlink_port(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\tstruct drm_dp_mst_port *port)\n{\n\tmutex_lock(&mgr->lock);\n\tport->parent->num_ports--;\n\tlist_del(&port->next);\n\tmutex_unlock(&mgr->lock);\n\tdrm_dp_mst_topology_put_port(port);\n}\n\nstatic struct drm_dp_mst_port *\ndrm_dp_mst_add_port(struct drm_device *dev,\n\t\t    struct drm_dp_mst_topology_mgr *mgr,\n\t\t    struct drm_dp_mst_branch *mstb, u8 port_number)\n{\n\tstruct drm_dp_mst_port *port = kzalloc(sizeof(*port), GFP_KERNEL);\n\n\tif (!port)\n\t\treturn NULL;\n\n\tkref_init(&port->topology_kref);\n\tkref_init(&port->malloc_kref);\n\tport->parent = mstb;\n\tport->port_num = port_number;\n\tport->mgr = mgr;\n\tport->aux.name = \"DPMST\";\n\tport->aux.dev = dev->dev;\n\tport->aux.is_remote = true;\n\n\t \n\tport->aux.drm_dev = dev;\n\tdrm_dp_remote_aux_init(&port->aux);\n\n\t \n\tdrm_dp_mst_get_mstb_malloc(mstb);\n\n\treturn port;\n}\n\nstatic int\ndrm_dp_mst_handle_link_address_port(struct drm_dp_mst_branch *mstb,\n\t\t\t\t    struct drm_device *dev,\n\t\t\t\t    struct drm_dp_link_addr_reply_port *port_msg)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = mstb->mgr;\n\tstruct drm_dp_mst_port *port;\n\tint old_ddps = 0, ret;\n\tu8 new_pdt = DP_PEER_DEVICE_NONE;\n\tbool new_mcs = 0;\n\tbool created = false, send_link_addr = false, changed = false;\n\n\tport = drm_dp_get_port(mstb, port_msg->port_number);\n\tif (!port) {\n\t\tport = drm_dp_mst_add_port(dev, mgr, mstb,\n\t\t\t\t\t   port_msg->port_number);\n\t\tif (!port)\n\t\t\treturn -ENOMEM;\n\t\tcreated = true;\n\t\tchanged = true;\n\t} else if (!port->input && port_msg->input_port && port->connector) {\n\t\t \n\t\tdrm_dp_mst_topology_unlink_port(mgr, port);\n\t\tdrm_dp_mst_topology_put_port(port);\n\t\tport = drm_dp_mst_add_port(dev, mgr, mstb,\n\t\t\t\t\t   port_msg->port_number);\n\t\tif (!port)\n\t\t\treturn -ENOMEM;\n\t\tchanged = true;\n\t\tcreated = true;\n\t} else if (port->input && !port_msg->input_port) {\n\t\tchanged = true;\n\t} else if (port->connector) {\n\t\t \n\t\tdrm_modeset_lock(&mgr->base.lock, NULL);\n\n\t\told_ddps = port->ddps;\n\t\tchanged = port->ddps != port_msg->ddps ||\n\t\t\t(port->ddps &&\n\t\t\t (port->ldps != port_msg->legacy_device_plug_status ||\n\t\t\t  port->dpcd_rev != port_msg->dpcd_revision ||\n\t\t\t  port->mcs != port_msg->mcs ||\n\t\t\t  port->pdt != port_msg->peer_device_type ||\n\t\t\t  port->num_sdp_stream_sinks !=\n\t\t\t  port_msg->num_sdp_stream_sinks));\n\t}\n\n\tport->input = port_msg->input_port;\n\tif (!port->input)\n\t\tnew_pdt = port_msg->peer_device_type;\n\tnew_mcs = port_msg->mcs;\n\tport->ddps = port_msg->ddps;\n\tport->ldps = port_msg->legacy_device_plug_status;\n\tport->dpcd_rev = port_msg->dpcd_revision;\n\tport->num_sdp_streams = port_msg->num_sdp_streams;\n\tport->num_sdp_stream_sinks = port_msg->num_sdp_stream_sinks;\n\n\t \n\tif (created) {\n\t\tmutex_lock(&mgr->lock);\n\t\tdrm_dp_mst_topology_get_port(port);\n\t\tlist_add(&port->next, &mstb->ports);\n\t\tmstb->num_ports++;\n\t\tmutex_unlock(&mgr->lock);\n\t}\n\n\t \n\tif (old_ddps != port->ddps || !created) {\n\t\tif (port->ddps && !port->input) {\n\t\t\tret = drm_dp_send_enum_path_resources(mgr, mstb,\n\t\t\t\t\t\t\t      port);\n\t\t\tif (ret == 1)\n\t\t\t\tchanged = true;\n\t\t} else {\n\t\t\tport->full_pbn = 0;\n\t\t}\n\t}\n\n\tret = drm_dp_port_set_pdt(port, new_pdt, new_mcs);\n\tif (ret == 1) {\n\t\tsend_link_addr = true;\n\t} else if (ret < 0) {\n\t\tdrm_err(dev, \"Failed to change PDT on port %p: %d\\n\", port, ret);\n\t\tgoto fail;\n\t}\n\n\t \n\tif (!created && port->pdt == DP_PEER_DEVICE_MST_BRANCHING &&\n\t    port->mcs)\n\t\tsend_link_addr = true;\n\n\tif (port->connector)\n\t\tdrm_modeset_unlock(&mgr->base.lock);\n\telse if (!port->input)\n\t\tdrm_dp_mst_port_add_connector(mstb, port);\n\n\tif (send_link_addr && port->mstb) {\n\t\tret = drm_dp_send_link_address(mgr, port->mstb);\n\t\tif (ret == 1)  \n\t\t\tchanged = true;\n\t\telse if (ret < 0)\n\t\t\tgoto fail_put;\n\t}\n\n\t \n\tdrm_dp_mst_topology_put_port(port);\n\treturn changed;\n\nfail:\n\tdrm_dp_mst_topology_unlink_port(mgr, port);\n\tif (port->connector)\n\t\tdrm_modeset_unlock(&mgr->base.lock);\nfail_put:\n\tdrm_dp_mst_topology_put_port(port);\n\treturn ret;\n}\n\nstatic int\ndrm_dp_mst_handle_conn_stat(struct drm_dp_mst_branch *mstb,\n\t\t\t    struct drm_dp_connection_status_notify *conn_stat)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = mstb->mgr;\n\tstruct drm_dp_mst_port *port;\n\tint old_ddps, ret;\n\tu8 new_pdt;\n\tbool new_mcs;\n\tbool dowork = false, create_connector = false;\n\n\tport = drm_dp_get_port(mstb, conn_stat->port_number);\n\tif (!port)\n\t\treturn 0;\n\n\tif (port->connector) {\n\t\tif (!port->input && conn_stat->input_port) {\n\t\t\t \n\t\t\tdrm_dp_mst_topology_unlink_port(mgr, port);\n\t\t\tmstb->link_address_sent = false;\n\t\t\tdowork = true;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tdrm_modeset_lock(&mgr->base.lock, NULL);\n\t} else if (port->input && !conn_stat->input_port) {\n\t\tcreate_connector = true;\n\t\t \n\t\tmstb->link_address_sent = false;\n\t\tdowork = true;\n\t}\n\n\told_ddps = port->ddps;\n\tport->input = conn_stat->input_port;\n\tport->ldps = conn_stat->legacy_device_plug_status;\n\tport->ddps = conn_stat->displayport_device_plug_status;\n\n\tif (old_ddps != port->ddps) {\n\t\tif (port->ddps && !port->input)\n\t\t\tdrm_dp_send_enum_path_resources(mgr, mstb, port);\n\t\telse\n\t\t\tport->full_pbn = 0;\n\t}\n\n\tnew_pdt = port->input ? DP_PEER_DEVICE_NONE : conn_stat->peer_device_type;\n\tnew_mcs = conn_stat->message_capability_status;\n\tret = drm_dp_port_set_pdt(port, new_pdt, new_mcs);\n\tif (ret == 1) {\n\t\tdowork = true;\n\t} else if (ret < 0) {\n\t\tdrm_err(mgr->dev, \"Failed to change PDT for port %p: %d\\n\", port, ret);\n\t\tdowork = false;\n\t}\n\n\tif (port->connector)\n\t\tdrm_modeset_unlock(&mgr->base.lock);\n\telse if (create_connector)\n\t\tdrm_dp_mst_port_add_connector(mstb, port);\n\nout:\n\tdrm_dp_mst_topology_put_port(port);\n\treturn dowork;\n}\n\nstatic struct drm_dp_mst_branch *drm_dp_get_mst_branch_device(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t\t\t       u8 lct, u8 *rad)\n{\n\tstruct drm_dp_mst_branch *mstb;\n\tstruct drm_dp_mst_port *port;\n\tint i, ret;\n\t \n\n\tmutex_lock(&mgr->lock);\n\tmstb = mgr->mst_primary;\n\n\tif (!mstb)\n\t\tgoto out;\n\n\tfor (i = 0; i < lct - 1; i++) {\n\t\tint shift = (i % 2) ? 0 : 4;\n\t\tint port_num = (rad[i / 2] >> shift) & 0xf;\n\n\t\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\t\tif (port->port_num == port_num) {\n\t\t\t\tmstb = port->mstb;\n\t\t\t\tif (!mstb) {\n\t\t\t\t\tdrm_err(mgr->dev,\n\t\t\t\t\t\t\"failed to lookup MSTB with lct %d, rad %02x\\n\",\n\t\t\t\t\t\tlct, rad[0]);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tret = drm_dp_mst_topology_try_get_mstb(mstb);\n\tif (!ret)\n\t\tmstb = NULL;\nout:\n\tmutex_unlock(&mgr->lock);\n\treturn mstb;\n}\n\nstatic struct drm_dp_mst_branch *get_mst_branch_device_by_guid_helper(\n\tstruct drm_dp_mst_branch *mstb,\n\tconst uint8_t *guid)\n{\n\tstruct drm_dp_mst_branch *found_mstb;\n\tstruct drm_dp_mst_port *port;\n\n\tif (!mstb)\n\t\treturn NULL;\n\n\tif (memcmp(mstb->guid, guid, 16) == 0)\n\t\treturn mstb;\n\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tfound_mstb = get_mst_branch_device_by_guid_helper(port->mstb, guid);\n\n\t\tif (found_mstb)\n\t\t\treturn found_mstb;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct drm_dp_mst_branch *\ndrm_dp_get_mst_branch_device_by_guid(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t     const uint8_t *guid)\n{\n\tstruct drm_dp_mst_branch *mstb;\n\tint ret;\n\n\t \n\tmutex_lock(&mgr->lock);\n\n\tmstb = get_mst_branch_device_by_guid_helper(mgr->mst_primary, guid);\n\tif (mstb) {\n\t\tret = drm_dp_mst_topology_try_get_mstb(mstb);\n\t\tif (!ret)\n\t\t\tmstb = NULL;\n\t}\n\n\tmutex_unlock(&mgr->lock);\n\treturn mstb;\n}\n\nstatic int drm_dp_check_and_send_link_address(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t       struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_mst_port *port;\n\tint ret;\n\tbool changed = false;\n\n\tif (!mstb->link_address_sent) {\n\t\tret = drm_dp_send_link_address(mgr, mstb);\n\t\tif (ret == 1)\n\t\t\tchanged = true;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tif (port->input || !port->ddps || !port->mstb)\n\t\t\tcontinue;\n\n\t\tret = drm_dp_check_and_send_link_address(mgr, port->mstb);\n\t\tif (ret == 1)\n\t\t\tchanged = true;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn changed;\n}\n\nstatic void drm_dp_mst_link_probe_work(struct work_struct *work)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr =\n\t\tcontainer_of(work, struct drm_dp_mst_topology_mgr, work);\n\tstruct drm_device *dev = mgr->dev;\n\tstruct drm_dp_mst_branch *mstb;\n\tint ret;\n\tbool clear_payload_id_table;\n\n\tmutex_lock(&mgr->probe_lock);\n\n\tmutex_lock(&mgr->lock);\n\tclear_payload_id_table = !mgr->payload_id_table_cleared;\n\tmgr->payload_id_table_cleared = true;\n\n\tmstb = mgr->mst_primary;\n\tif (mstb) {\n\t\tret = drm_dp_mst_topology_try_get_mstb(mstb);\n\t\tif (!ret)\n\t\t\tmstb = NULL;\n\t}\n\tmutex_unlock(&mgr->lock);\n\tif (!mstb) {\n\t\tmutex_unlock(&mgr->probe_lock);\n\t\treturn;\n\t}\n\n\t \n\tif (clear_payload_id_table) {\n\t\tdrm_dbg_kms(dev, \"Clearing payload ID table\\n\");\n\t\tdrm_dp_send_clear_payload_id_table(mgr, mstb);\n\t}\n\n\tret = drm_dp_check_and_send_link_address(mgr, mstb);\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\n\tmutex_unlock(&mgr->probe_lock);\n\tif (ret > 0)\n\t\tdrm_kms_helper_hotplug_event(dev);\n}\n\nstatic bool drm_dp_validate_guid(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t u8 *guid)\n{\n\tu64 salt;\n\n\tif (memchr_inv(guid, 0, 16))\n\t\treturn true;\n\n\tsalt = get_jiffies_64();\n\n\tmemcpy(&guid[0], &salt, sizeof(u64));\n\tmemcpy(&guid[8], &salt, sizeof(u64));\n\n\treturn false;\n}\n\nstatic void build_dpcd_read(struct drm_dp_sideband_msg_tx *msg,\n\t\t\t    u8 port_num, u32 offset, u8 num_bytes)\n{\n\tstruct drm_dp_sideband_msg_req_body req;\n\n\treq.req_type = DP_REMOTE_DPCD_READ;\n\treq.u.dpcd_read.port_number = port_num;\n\treq.u.dpcd_read.dpcd_address = offset;\n\treq.u.dpcd_read.num_bytes = num_bytes;\n\tdrm_dp_encode_sideband_req(&req, msg);\n}\n\nstatic int drm_dp_send_sideband_msg(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t    bool up, u8 *msg, int len)\n{\n\tint ret;\n\tint regbase = up ? DP_SIDEBAND_MSG_UP_REP_BASE : DP_SIDEBAND_MSG_DOWN_REQ_BASE;\n\tint tosend, total, offset;\n\tint retries = 0;\n\nretry:\n\ttotal = len;\n\toffset = 0;\n\tdo {\n\t\ttosend = min3(mgr->max_dpcd_transaction_bytes, 16, total);\n\n\t\tret = drm_dp_dpcd_write(mgr->aux, regbase + offset,\n\t\t\t\t\t&msg[offset],\n\t\t\t\t\ttosend);\n\t\tif (ret != tosend) {\n\t\t\tif (ret == -EIO && retries < 5) {\n\t\t\t\tretries++;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tdrm_dbg_kms(mgr->dev, \"failed to dpcd write %d %d\\n\", tosend, ret);\n\n\t\t\treturn -EIO;\n\t\t}\n\t\toffset += tosend;\n\t\ttotal -= tosend;\n\t} while (total > 0);\n\treturn 0;\n}\n\nstatic int set_hdr_from_dst_qlock(struct drm_dp_sideband_msg_hdr *hdr,\n\t\t\t\t  struct drm_dp_sideband_msg_tx *txmsg)\n{\n\tstruct drm_dp_mst_branch *mstb = txmsg->dst;\n\tu8 req_type;\n\n\treq_type = txmsg->msg[0] & 0x7f;\n\tif (req_type == DP_CONNECTION_STATUS_NOTIFY ||\n\t\treq_type == DP_RESOURCE_STATUS_NOTIFY ||\n\t\treq_type == DP_CLEAR_PAYLOAD_ID_TABLE)\n\t\thdr->broadcast = 1;\n\telse\n\t\thdr->broadcast = 0;\n\thdr->path_msg = txmsg->path_msg;\n\tif (hdr->broadcast) {\n\t\thdr->lct = 1;\n\t\thdr->lcr = 6;\n\t} else {\n\t\thdr->lct = mstb->lct;\n\t\thdr->lcr = mstb->lct - 1;\n\t}\n\n\tmemcpy(hdr->rad, mstb->rad, hdr->lct / 2);\n\n\treturn 0;\n}\n \nstatic int process_single_tx_qlock(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t   struct drm_dp_sideband_msg_tx *txmsg,\n\t\t\t\t   bool up)\n{\n\tu8 chunk[48];\n\tstruct drm_dp_sideband_msg_hdr hdr;\n\tint len, space, idx, tosend;\n\tint ret;\n\n\tif (txmsg->state == DRM_DP_SIDEBAND_TX_SENT)\n\t\treturn 0;\n\n\tmemset(&hdr, 0, sizeof(struct drm_dp_sideband_msg_hdr));\n\n\tif (txmsg->state == DRM_DP_SIDEBAND_TX_QUEUED)\n\t\ttxmsg->state = DRM_DP_SIDEBAND_TX_START_SEND;\n\n\t \n\tret = set_hdr_from_dst_qlock(&hdr, txmsg);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tlen = txmsg->cur_len - txmsg->cur_offset;\n\n\t \n\tspace = 48 - 1 - drm_dp_calc_sb_hdr_size(&hdr);\n\n\ttosend = min(len, space);\n\tif (len == txmsg->cur_len)\n\t\thdr.somt = 1;\n\tif (space >= len)\n\t\thdr.eomt = 1;\n\n\n\thdr.msg_len = tosend + 1;\n\tdrm_dp_encode_sideband_msg_hdr(&hdr, chunk, &idx);\n\tmemcpy(&chunk[idx], &txmsg->msg[txmsg->cur_offset], tosend);\n\t \n\tdrm_dp_crc_sideband_chunk_req(&chunk[idx], tosend);\n\tidx += tosend + 1;\n\n\tret = drm_dp_send_sideband_msg(mgr, up, chunk, idx);\n\tif (ret) {\n\t\tif (drm_debug_enabled(DRM_UT_DP)) {\n\t\t\tstruct drm_printer p = drm_debug_printer(DBG_PREFIX);\n\n\t\t\tdrm_printf(&p, \"sideband msg failed to send\\n\");\n\t\t\tdrm_dp_mst_dump_sideband_msg_tx(&p, txmsg);\n\t\t}\n\t\treturn ret;\n\t}\n\n\ttxmsg->cur_offset += tosend;\n\tif (txmsg->cur_offset == txmsg->cur_len) {\n\t\ttxmsg->state = DRM_DP_SIDEBAND_TX_SENT;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void process_single_down_tx_qlock(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tint ret;\n\n\tWARN_ON(!mutex_is_locked(&mgr->qlock));\n\n\t \n\tif (list_empty(&mgr->tx_msg_downq))\n\t\treturn;\n\n\ttxmsg = list_first_entry(&mgr->tx_msg_downq,\n\t\t\t\t struct drm_dp_sideband_msg_tx, next);\n\tret = process_single_tx_qlock(mgr, txmsg, false);\n\tif (ret < 0) {\n\t\tdrm_dbg_kms(mgr->dev, \"failed to send msg in q %d\\n\", ret);\n\t\tlist_del(&txmsg->next);\n\t\ttxmsg->state = DRM_DP_SIDEBAND_TX_TIMEOUT;\n\t\twake_up_all(&mgr->tx_waitq);\n\t}\n}\n\nstatic void drm_dp_queue_down_tx(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_dp_sideband_msg_tx *txmsg)\n{\n\tmutex_lock(&mgr->qlock);\n\tlist_add_tail(&txmsg->next, &mgr->tx_msg_downq);\n\n\tif (drm_debug_enabled(DRM_UT_DP)) {\n\t\tstruct drm_printer p = drm_debug_printer(DBG_PREFIX);\n\n\t\tdrm_dp_mst_dump_sideband_msg_tx(&p, txmsg);\n\t}\n\n\tif (list_is_singular(&mgr->tx_msg_downq))\n\t\tprocess_single_down_tx_qlock(mgr);\n\tmutex_unlock(&mgr->qlock);\n}\n\nstatic void\ndrm_dp_dump_link_address(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t struct drm_dp_link_address_ack_reply *reply)\n{\n\tstruct drm_dp_link_addr_reply_port *port_reply;\n\tint i;\n\n\tfor (i = 0; i < reply->nports; i++) {\n\t\tport_reply = &reply->ports[i];\n\t\tdrm_dbg_kms(mgr->dev,\n\t\t\t    \"port %d: input %d, pdt: %d, pn: %d, dpcd_rev: %02x, mcs: %d, ddps: %d, ldps %d, sdp %d/%d\\n\",\n\t\t\t    i,\n\t\t\t    port_reply->input_port,\n\t\t\t    port_reply->peer_device_type,\n\t\t\t    port_reply->port_number,\n\t\t\t    port_reply->dpcd_revision,\n\t\t\t    port_reply->mcs,\n\t\t\t    port_reply->ddps,\n\t\t\t    port_reply->legacy_device_plug_status,\n\t\t\t    port_reply->num_sdp_streams,\n\t\t\t    port_reply->num_sdp_stream_sinks);\n\t}\n}\n\nstatic int drm_dp_send_link_address(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t     struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tstruct drm_dp_link_address_ack_reply *reply;\n\tstruct drm_dp_mst_port *port, *tmp;\n\tint i, ret, port_mask = 0;\n\tbool changed = false;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg)\n\t\treturn -ENOMEM;\n\n\ttxmsg->dst = mstb;\n\tbuild_link_address(txmsg);\n\n\tmstb->link_address_sent = true;\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\t \n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret <= 0) {\n\t\tdrm_err(mgr->dev, \"Sending link address failed with %d\\n\", ret);\n\t\tgoto out;\n\t}\n\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\tdrm_err(mgr->dev, \"link address NAK received\\n\");\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\treply = &txmsg->reply.u.link_addr;\n\tdrm_dbg_kms(mgr->dev, \"link address reply: %d\\n\", reply->nports);\n\tdrm_dp_dump_link_address(mgr, reply);\n\n\tret = drm_dp_check_mstb_guid(mstb, reply->guid);\n\tif (ret) {\n\t\tchar buf[64];\n\n\t\tdrm_dp_mst_rad_to_str(mstb->rad, mstb->lct, buf, sizeof(buf));\n\t\tdrm_err(mgr->dev, \"GUID check on %s failed: %d\\n\", buf, ret);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < reply->nports; i++) {\n\t\tport_mask |= BIT(reply->ports[i].port_number);\n\t\tret = drm_dp_mst_handle_link_address_port(mstb, mgr->dev,\n\t\t\t\t\t\t\t  &reply->ports[i]);\n\t\tif (ret == 1)\n\t\t\tchanged = true;\n\t\telse if (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\t \n\tmutex_lock(&mgr->lock);\n\tlist_for_each_entry_safe(port, tmp, &mstb->ports, next) {\n\t\tif (port_mask & BIT(port->port_num))\n\t\t\tcontinue;\n\n\t\tdrm_dbg_kms(mgr->dev, \"port %d was not in link address, removing\\n\",\n\t\t\t    port->port_num);\n\t\tlist_del(&port->next);\n\t\tdrm_dp_mst_topology_put_port(port);\n\t\tchanged = true;\n\t}\n\tmutex_unlock(&mgr->lock);\n\nout:\n\tif (ret <= 0)\n\t\tmstb->link_address_sent = false;\n\tkfree(txmsg);\n\treturn ret < 0 ? ret : changed;\n}\n\nstatic void\ndrm_dp_send_clear_payload_id_table(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t   struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tint ret;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg)\n\t\treturn;\n\n\ttxmsg->dst = mstb;\n\tbuild_clear_payload_id_table(txmsg);\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret > 0 && txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK)\n\t\tdrm_dbg_kms(mgr->dev, \"clear payload table id nak received\\n\");\n\n\tkfree(txmsg);\n}\n\nstatic int\ndrm_dp_send_enum_path_resources(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\tstruct drm_dp_mst_branch *mstb,\n\t\t\t\tstruct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_enum_path_resources_ack_reply *path_res;\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tint ret;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg)\n\t\treturn -ENOMEM;\n\n\ttxmsg->dst = mstb;\n\tbuild_enum_path_resources(txmsg, port->port_num);\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret > 0) {\n\t\tret = 0;\n\t\tpath_res = &txmsg->reply.u.path_resources;\n\n\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\t\tdrm_dbg_kms(mgr->dev, \"enum path resources nak received\\n\");\n\t\t} else {\n\t\t\tif (port->port_num != path_res->port_number)\n\t\t\t\tDRM_ERROR(\"got incorrect port in response\\n\");\n\n\t\t\tdrm_dbg_kms(mgr->dev, \"enum path resources %d: %d %d\\n\",\n\t\t\t\t    path_res->port_number,\n\t\t\t\t    path_res->full_payload_bw_number,\n\t\t\t\t    path_res->avail_payload_bw_number);\n\n\t\t\t \n\t\t\tif (port->full_pbn != path_res->full_payload_bw_number ||\n\t\t\t    port->fec_capable != path_res->fec_capable)\n\t\t\t\tret = 1;\n\n\t\t\tport->full_pbn = path_res->full_payload_bw_number;\n\t\t\tport->fec_capable = path_res->fec_capable;\n\t\t}\n\t}\n\n\tkfree(txmsg);\n\treturn ret;\n}\n\nstatic struct drm_dp_mst_port *drm_dp_get_last_connected_port_to_mstb(struct drm_dp_mst_branch *mstb)\n{\n\tif (!mstb->port_parent)\n\t\treturn NULL;\n\n\tif (mstb->port_parent->mstb != mstb)\n\t\treturn mstb->port_parent;\n\n\treturn drm_dp_get_last_connected_port_to_mstb(mstb->port_parent->parent);\n}\n\n \nstatic struct drm_dp_mst_branch *\ndrm_dp_get_last_connected_port_and_mstb(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\tstruct drm_dp_mst_branch *mstb,\n\t\t\t\t\tint *port_num)\n{\n\tstruct drm_dp_mst_branch *rmstb = NULL;\n\tstruct drm_dp_mst_port *found_port;\n\n\tmutex_lock(&mgr->lock);\n\tif (!mgr->mst_primary)\n\t\tgoto out;\n\n\tdo {\n\t\tfound_port = drm_dp_get_last_connected_port_to_mstb(mstb);\n\t\tif (!found_port)\n\t\t\tbreak;\n\n\t\tif (drm_dp_mst_topology_try_get_mstb(found_port->parent)) {\n\t\t\trmstb = found_port->parent;\n\t\t\t*port_num = found_port->port_num;\n\t\t} else {\n\t\t\t \n\t\t\tmstb = found_port->parent;\n\t\t}\n\t} while (!rmstb);\nout:\n\tmutex_unlock(&mgr->lock);\n\treturn rmstb;\n}\n\nstatic int drm_dp_payload_send_msg(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t   struct drm_dp_mst_port *port,\n\t\t\t\t   int id,\n\t\t\t\t   int pbn)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tstruct drm_dp_mst_branch *mstb;\n\tint ret, port_num;\n\tu8 sinks[DRM_DP_MAX_SDP_STREAMS];\n\tint i;\n\n\tport_num = port->port_num;\n\tmstb = drm_dp_mst_topology_get_mstb_validated(mgr, port->parent);\n\tif (!mstb) {\n\t\tmstb = drm_dp_get_last_connected_port_and_mstb(mgr,\n\t\t\t\t\t\t\t       port->parent,\n\t\t\t\t\t\t\t       &port_num);\n\n\t\tif (!mstb)\n\t\t\treturn -EINVAL;\n\t}\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_put;\n\t}\n\n\tfor (i = 0; i < port->num_sdp_streams; i++)\n\t\tsinks[i] = i;\n\n\ttxmsg->dst = mstb;\n\tbuild_allocate_payload(txmsg, port_num,\n\t\t\t       id,\n\t\t\t       pbn, port->num_sdp_streams, sinks);\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\t \n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret > 0) {\n\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tret = 0;\n\t}\n\tkfree(txmsg);\nfail_put:\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\treturn ret;\n}\n\nint drm_dp_send_power_updown_phy(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_dp_mst_port *port, bool power_up)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tint ret;\n\n\tport = drm_dp_mst_topology_get_port_validated(mgr, port);\n\tif (!port)\n\t\treturn -EINVAL;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tdrm_dp_mst_topology_put_port(port);\n\t\treturn -ENOMEM;\n\t}\n\n\ttxmsg->dst = port->parent;\n\tbuild_power_updown_phy(txmsg, port->port_num, power_up);\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(port->parent, txmsg);\n\tif (ret > 0) {\n\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tret = 0;\n\t}\n\tkfree(txmsg);\n\tdrm_dp_mst_topology_put_port(port);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_send_power_updown_phy);\n\nint drm_dp_send_query_stream_enc_status(struct drm_dp_mst_topology_mgr *mgr,\n\t\tstruct drm_dp_mst_port *port,\n\t\tstruct drm_dp_query_stream_enc_status_ack_reply *status)\n{\n\tstruct drm_dp_mst_topology_state *state;\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tu8 nonce[7];\n\tint ret;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg)\n\t\treturn -ENOMEM;\n\n\tport = drm_dp_mst_topology_get_port_validated(mgr, port);\n\tif (!port) {\n\t\tret = -EINVAL;\n\t\tgoto out_get_port;\n\t}\n\n\tget_random_bytes(nonce, sizeof(nonce));\n\n\tdrm_modeset_lock(&mgr->base.lock, NULL);\n\tstate = to_drm_dp_mst_topology_state(mgr->base.state);\n\tpayload = drm_atomic_get_mst_payload_state(state, port);\n\n\t \n\ttxmsg->dst = mgr->mst_primary;\n\n\tbuild_query_stream_enc_status(txmsg, payload->vcpi, nonce);\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mgr->mst_primary, txmsg);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\tdrm_dbg_kms(mgr->dev, \"query encryption status nak received\\n\");\n\t\tret = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tret = 0;\n\tmemcpy(status, &txmsg->reply.u.enc_status, sizeof(*status));\n\nout:\n\tdrm_modeset_unlock(&mgr->base.lock);\n\tdrm_dp_mst_topology_put_port(port);\nout_get_port:\n\tkfree(txmsg);\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_send_query_stream_enc_status);\n\nstatic int drm_dp_create_payload_step1(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t       struct drm_dp_mst_atomic_payload *payload)\n{\n\treturn drm_dp_dpcd_write_payload(mgr, payload->vcpi, payload->vc_start_slot,\n\t\t\t\t\t payload->time_slots);\n}\n\nstatic int drm_dp_create_payload_step2(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t       struct drm_dp_mst_atomic_payload *payload)\n{\n\tint ret;\n\tstruct drm_dp_mst_port *port = drm_dp_mst_topology_get_port_validated(mgr, payload->port);\n\n\tif (!port)\n\t\treturn -EIO;\n\n\tret = drm_dp_payload_send_msg(mgr, port, payload->vcpi, payload->pbn);\n\tdrm_dp_mst_topology_put_port(port);\n\treturn ret;\n}\n\nstatic int drm_dp_destroy_payload_step1(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\tstruct drm_dp_mst_topology_state *mst_state,\n\t\t\t\t\tstruct drm_dp_mst_atomic_payload *payload)\n{\n\tdrm_dbg_kms(mgr->dev, \"\\n\");\n\n\t \n\tdrm_dp_payload_send_msg(mgr, payload->port, payload->vcpi, 0);\n\tdrm_dp_dpcd_write_payload(mgr, payload->vcpi, payload->vc_start_slot, 0);\n\n\treturn 0;\n}\n\n \nint drm_dp_add_payload_part1(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t     struct drm_dp_mst_topology_state *mst_state,\n\t\t\t     struct drm_dp_mst_atomic_payload *payload)\n{\n\tstruct drm_dp_mst_port *port;\n\tint ret;\n\n\tport = drm_dp_mst_topology_get_port_validated(mgr, payload->port);\n\tif (!port) {\n\t\tdrm_dbg_kms(mgr->dev,\n\t\t\t    \"VCPI %d for port %p not in topology, not creating a payload\\n\",\n\t\t\t    payload->vcpi, payload->port);\n\t\tpayload->vc_start_slot = -1;\n\t\treturn 0;\n\t}\n\n\tif (mgr->payload_count == 0)\n\t\tmgr->next_start_slot = mst_state->start_slot;\n\n\tpayload->vc_start_slot = mgr->next_start_slot;\n\n\tret = drm_dp_create_payload_step1(mgr, payload);\n\tdrm_dp_mst_topology_put_port(port);\n\tif (ret < 0) {\n\t\tdrm_warn(mgr->dev, \"Failed to create MST payload for port %p: %d\\n\",\n\t\t\t payload->port, ret);\n\t\tpayload->vc_start_slot = -1;\n\t\treturn ret;\n\t}\n\n\tmgr->payload_count++;\n\tmgr->next_start_slot += payload->time_slots;\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_add_payload_part1);\n\n \nvoid drm_dp_remove_payload(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t   struct drm_dp_mst_topology_state *mst_state,\n\t\t\t   const struct drm_dp_mst_atomic_payload *old_payload,\n\t\t\t   struct drm_dp_mst_atomic_payload *new_payload)\n{\n\tstruct drm_dp_mst_atomic_payload *pos;\n\tbool send_remove = false;\n\n\t \n\tif (new_payload->vc_start_slot == -1)\n\t\treturn;\n\n\tmutex_lock(&mgr->lock);\n\tsend_remove = drm_dp_mst_port_downstream_of_branch(new_payload->port, mgr->mst_primary);\n\tmutex_unlock(&mgr->lock);\n\n\tif (send_remove)\n\t\tdrm_dp_destroy_payload_step1(mgr, mst_state, new_payload);\n\telse\n\t\tdrm_dbg_kms(mgr->dev, \"Payload for VCPI %d not in topology, not sending remove\\n\",\n\t\t\t    new_payload->vcpi);\n\n\tlist_for_each_entry(pos, &mst_state->payloads, next) {\n\t\tif (pos != new_payload && pos->vc_start_slot > new_payload->vc_start_slot)\n\t\t\tpos->vc_start_slot -= old_payload->time_slots;\n\t}\n\tnew_payload->vc_start_slot = -1;\n\n\tmgr->payload_count--;\n\tmgr->next_start_slot -= old_payload->time_slots;\n\n\tif (new_payload->delete)\n\t\tdrm_dp_mst_put_port_malloc(new_payload->port);\n}\nEXPORT_SYMBOL(drm_dp_remove_payload);\n\n \nint drm_dp_add_payload_part2(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t     struct drm_atomic_state *state,\n\t\t\t     struct drm_dp_mst_atomic_payload *payload)\n{\n\tint ret = 0;\n\n\t \n\tif (payload->vc_start_slot == -1) {\n\t\tdrm_dbg_kms(mgr->dev, \"Part 1 of payload creation for %s failed, skipping part 2\\n\",\n\t\t\t    payload->port->connector->name);\n\t\treturn -EIO;\n\t}\n\n\tret = drm_dp_create_payload_step2(mgr, payload);\n\tif (ret < 0) {\n\t\tif (!payload->delete)\n\t\t\tdrm_err(mgr->dev, \"Step 2 of creating MST payload for %p failed: %d\\n\",\n\t\t\t\tpayload->port, ret);\n\t\telse\n\t\t\tdrm_dbg_kms(mgr->dev, \"Step 2 of removing MST payload for %p failed: %d\\n\",\n\t\t\t\t    payload->port, ret);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_add_payload_part2);\n\nstatic int drm_dp_send_dpcd_read(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_dp_mst_port *port,\n\t\t\t\t int offset, int size, u8 *bytes)\n{\n\tint ret = 0;\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tstruct drm_dp_mst_branch *mstb;\n\n\tmstb = drm_dp_mst_topology_get_mstb_validated(mgr, port->parent);\n\tif (!mstb)\n\t\treturn -EINVAL;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_put;\n\t}\n\n\tbuild_dpcd_read(txmsg, port->port_num, offset, size);\n\ttxmsg->dst = port->parent;\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret < 0)\n\t\tgoto fail_free;\n\n\tif (txmsg->reply.reply_type == 1) {\n\t\tdrm_dbg_kms(mgr->dev, \"mstb %p port %d: DPCD read on addr 0x%x for %d bytes NAKed\\n\",\n\t\t\t    mstb, port->port_num, offset, size);\n\t\tret = -EIO;\n\t\tgoto fail_free;\n\t}\n\n\tif (txmsg->reply.u.remote_dpcd_read_ack.num_bytes != size) {\n\t\tret = -EPROTO;\n\t\tgoto fail_free;\n\t}\n\n\tret = min_t(size_t, txmsg->reply.u.remote_dpcd_read_ack.num_bytes,\n\t\t    size);\n\tmemcpy(bytes, txmsg->reply.u.remote_dpcd_read_ack.bytes, ret);\n\nfail_free:\n\tkfree(txmsg);\nfail_put:\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\n\treturn ret;\n}\n\nstatic int drm_dp_send_dpcd_write(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t  struct drm_dp_mst_port *port,\n\t\t\t\t  int offset, int size, u8 *bytes)\n{\n\tint ret;\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tstruct drm_dp_mst_branch *mstb;\n\n\tmstb = drm_dp_mst_topology_get_mstb_validated(mgr, port->parent);\n\tif (!mstb)\n\t\treturn -EINVAL;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_put;\n\t}\n\n\tbuild_dpcd_write(txmsg, port->port_num, offset, size, bytes);\n\ttxmsg->dst = mstb;\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret > 0) {\n\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK)\n\t\t\tret = -EIO;\n\t\telse\n\t\t\tret = size;\n\t}\n\n\tkfree(txmsg);\nfail_put:\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\treturn ret;\n}\n\nstatic int drm_dp_encode_up_ack_reply(struct drm_dp_sideband_msg_tx *msg, u8 req_type)\n{\n\tstruct drm_dp_sideband_msg_reply_body reply;\n\n\treply.reply_type = DP_SIDEBAND_REPLY_ACK;\n\treply.req_type = req_type;\n\tdrm_dp_encode_sideband_reply(&reply, msg);\n\treturn 0;\n}\n\nstatic int drm_dp_send_up_ack_reply(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t    struct drm_dp_mst_branch *mstb,\n\t\t\t\t    int req_type, bool broadcast)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg)\n\t\treturn -ENOMEM;\n\n\ttxmsg->dst = mstb;\n\tdrm_dp_encode_up_ack_reply(txmsg, req_type);\n\n\tmutex_lock(&mgr->qlock);\n\t \n\tprocess_single_tx_qlock(mgr, txmsg, true);\n\tmutex_unlock(&mgr->qlock);\n\n\tkfree(txmsg);\n\treturn 0;\n}\n\n \nint drm_dp_get_vc_payload_bw(const struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t     int link_rate, int link_lane_count)\n{\n\tif (link_rate == 0 || link_lane_count == 0)\n\t\tdrm_dbg_kms(mgr->dev, \"invalid link rate/lane count: (%d / %d)\\n\",\n\t\t\t    link_rate, link_lane_count);\n\n\t \n\treturn link_rate * link_lane_count / 54000;\n}\nEXPORT_SYMBOL(drm_dp_get_vc_payload_bw);\n\n \nbool drm_dp_read_mst_cap(struct drm_dp_aux *aux,\n\t\t\t const u8 dpcd[DP_RECEIVER_CAP_SIZE])\n{\n\tu8 mstm_cap;\n\n\tif (dpcd[DP_DPCD_REV] < DP_DPCD_REV_12)\n\t\treturn false;\n\n\tif (drm_dp_dpcd_readb(aux, DP_MSTM_CAP, &mstm_cap) != 1)\n\t\treturn false;\n\n\treturn mstm_cap & DP_MST_CAP;\n}\nEXPORT_SYMBOL(drm_dp_read_mst_cap);\n\n \nint drm_dp_mst_topology_mgr_set_mst(struct drm_dp_mst_topology_mgr *mgr, bool mst_state)\n{\n\tint ret = 0;\n\tstruct drm_dp_mst_branch *mstb = NULL;\n\n\tmutex_lock(&mgr->lock);\n\tif (mst_state == mgr->mst_state)\n\t\tgoto out_unlock;\n\n\tmgr->mst_state = mst_state;\n\t \n\tif (mst_state) {\n\t\tWARN_ON(mgr->mst_primary);\n\n\t\t \n\t\tret = drm_dp_read_dpcd_caps(mgr->aux, mgr->dpcd);\n\t\tif (ret < 0) {\n\t\t\tdrm_dbg_kms(mgr->dev, \"%s: failed to read DPCD, ret %d\\n\",\n\t\t\t\t    mgr->aux->name, ret);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tmstb = drm_dp_add_mst_branch_device(1, NULL);\n\t\tif (mstb == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tmstb->mgr = mgr;\n\n\t\t \n\t\tmgr->mst_primary = mstb;\n\t\tdrm_dp_mst_topology_get_mstb(mgr->mst_primary);\n\n\t\tret = drm_dp_dpcd_writeb(mgr->aux, DP_MSTM_CTRL,\n\t\t\t\t\t DP_MST_EN |\n\t\t\t\t\t DP_UP_REQ_EN |\n\t\t\t\t\t DP_UPSTREAM_IS_SRC);\n\t\tif (ret < 0)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tdrm_dp_dpcd_write_payload(mgr, 0, 0, 0x3f);\n\n\t\tqueue_work(system_long_wq, &mgr->work);\n\n\t\tret = 0;\n\t} else {\n\t\t \n\t\tmstb = mgr->mst_primary;\n\t\tmgr->mst_primary = NULL;\n\t\t \n\t\tdrm_dp_dpcd_writeb(mgr->aux, DP_MSTM_CTRL, 0);\n\t\tret = 0;\n\t\tmgr->payload_id_table_cleared = false;\n\n\t\tmemset(&mgr->down_rep_recv, 0, sizeof(mgr->down_rep_recv));\n\t\tmemset(&mgr->up_req_recv, 0, sizeof(mgr->up_req_recv));\n\t}\n\nout_unlock:\n\tmutex_unlock(&mgr->lock);\n\tif (mstb)\n\t\tdrm_dp_mst_topology_put_mstb(mstb);\n\treturn ret;\n\n}\nEXPORT_SYMBOL(drm_dp_mst_topology_mgr_set_mst);\n\nstatic void\ndrm_dp_mst_topology_mgr_invalidate_mstb(struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_mst_port *port;\n\n\t \n\tmstb->link_address_sent = false;\n\n\tlist_for_each_entry(port, &mstb->ports, next)\n\t\tif (port->mstb)\n\t\t\tdrm_dp_mst_topology_mgr_invalidate_mstb(port->mstb);\n}\n\n \nvoid drm_dp_mst_topology_mgr_suspend(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tmutex_lock(&mgr->lock);\n\tdrm_dp_dpcd_writeb(mgr->aux, DP_MSTM_CTRL,\n\t\t\t   DP_MST_EN | DP_UPSTREAM_IS_SRC);\n\tmutex_unlock(&mgr->lock);\n\tflush_work(&mgr->up_req_work);\n\tflush_work(&mgr->work);\n\tflush_work(&mgr->delayed_destroy_work);\n\n\tmutex_lock(&mgr->lock);\n\tif (mgr->mst_state && mgr->mst_primary)\n\t\tdrm_dp_mst_topology_mgr_invalidate_mstb(mgr->mst_primary);\n\tmutex_unlock(&mgr->lock);\n}\nEXPORT_SYMBOL(drm_dp_mst_topology_mgr_suspend);\n\n \nint drm_dp_mst_topology_mgr_resume(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t   bool sync)\n{\n\tint ret;\n\tu8 guid[16];\n\n\tmutex_lock(&mgr->lock);\n\tif (!mgr->mst_primary)\n\t\tgoto out_fail;\n\n\tif (drm_dp_read_dpcd_caps(mgr->aux, mgr->dpcd) < 0) {\n\t\tdrm_dbg_kms(mgr->dev, \"dpcd read failed - undocked during suspend?\\n\");\n\t\tgoto out_fail;\n\t}\n\n\tret = drm_dp_dpcd_writeb(mgr->aux, DP_MSTM_CTRL,\n\t\t\t\t DP_MST_EN |\n\t\t\t\t DP_UP_REQ_EN |\n\t\t\t\t DP_UPSTREAM_IS_SRC);\n\tif (ret < 0) {\n\t\tdrm_dbg_kms(mgr->dev, \"mst write failed - undocked during suspend?\\n\");\n\t\tgoto out_fail;\n\t}\n\n\t \n\tret = drm_dp_dpcd_read(mgr->aux, DP_GUID, guid, 16);\n\tif (ret != 16) {\n\t\tdrm_dbg_kms(mgr->dev, \"dpcd read failed - undocked during suspend?\\n\");\n\t\tgoto out_fail;\n\t}\n\n\tret = drm_dp_check_mstb_guid(mgr->mst_primary, guid);\n\tif (ret) {\n\t\tdrm_dbg_kms(mgr->dev, \"check mstb failed - undocked during suspend?\\n\");\n\t\tgoto out_fail;\n\t}\n\n\t \n\tqueue_work(system_long_wq, &mgr->work);\n\tmutex_unlock(&mgr->lock);\n\n\tif (sync) {\n\t\tdrm_dbg_kms(mgr->dev,\n\t\t\t    \"Waiting for link probe work to finish re-syncing topology...\\n\");\n\t\tflush_work(&mgr->work);\n\t}\n\n\treturn 0;\n\nout_fail:\n\tmutex_unlock(&mgr->lock);\n\treturn -1;\n}\nEXPORT_SYMBOL(drm_dp_mst_topology_mgr_resume);\n\nstatic bool\ndrm_dp_get_one_sb_msg(struct drm_dp_mst_topology_mgr *mgr, bool up,\n\t\t      struct drm_dp_mst_branch **mstb)\n{\n\tint len;\n\tu8 replyblock[32];\n\tint replylen, curreply;\n\tint ret;\n\tu8 hdrlen;\n\tstruct drm_dp_sideband_msg_hdr hdr;\n\tstruct drm_dp_sideband_msg_rx *msg =\n\t\tup ? &mgr->up_req_recv : &mgr->down_rep_recv;\n\tint basereg = up ? DP_SIDEBAND_MSG_UP_REQ_BASE :\n\t\t\t   DP_SIDEBAND_MSG_DOWN_REP_BASE;\n\n\tif (!up)\n\t\t*mstb = NULL;\n\n\tlen = min(mgr->max_dpcd_transaction_bytes, 16);\n\tret = drm_dp_dpcd_read(mgr->aux, basereg, replyblock, len);\n\tif (ret != len) {\n\t\tdrm_dbg_kms(mgr->dev, \"failed to read DPCD down rep %d %d\\n\", len, ret);\n\t\treturn false;\n\t}\n\n\tret = drm_dp_decode_sideband_msg_hdr(mgr, &hdr, replyblock, len, &hdrlen);\n\tif (ret == false) {\n\t\tprint_hex_dump(KERN_DEBUG, \"failed hdr\", DUMP_PREFIX_NONE, 16,\n\t\t\t       1, replyblock, len, false);\n\t\tdrm_dbg_kms(mgr->dev, \"ERROR: failed header\\n\");\n\t\treturn false;\n\t}\n\n\tif (!up) {\n\t\t \n\t\t*mstb = drm_dp_get_mst_branch_device(mgr, hdr.lct, hdr.rad);\n\t\tif (!*mstb) {\n\t\t\tdrm_dbg_kms(mgr->dev, \"Got MST reply from unknown device %d\\n\", hdr.lct);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (!drm_dp_sideband_msg_set_header(msg, &hdr, hdrlen)) {\n\t\tdrm_dbg_kms(mgr->dev, \"sideband msg set header failed %d\\n\", replyblock[0]);\n\t\treturn false;\n\t}\n\n\treplylen = min(msg->curchunk_len, (u8)(len - hdrlen));\n\tret = drm_dp_sideband_append_payload(msg, replyblock + hdrlen, replylen);\n\tif (!ret) {\n\t\tdrm_dbg_kms(mgr->dev, \"sideband msg build failed %d\\n\", replyblock[0]);\n\t\treturn false;\n\t}\n\n\treplylen = msg->curchunk_len + msg->curchunk_hdrlen - len;\n\tcurreply = len;\n\twhile (replylen > 0) {\n\t\tlen = min3(replylen, mgr->max_dpcd_transaction_bytes, 16);\n\t\tret = drm_dp_dpcd_read(mgr->aux, basereg + curreply,\n\t\t\t\t    replyblock, len);\n\t\tif (ret != len) {\n\t\t\tdrm_dbg_kms(mgr->dev, \"failed to read a chunk (len %d, ret %d)\\n\",\n\t\t\t\t    len, ret);\n\t\t\treturn false;\n\t\t}\n\n\t\tret = drm_dp_sideband_append_payload(msg, replyblock, len);\n\t\tif (!ret) {\n\t\t\tdrm_dbg_kms(mgr->dev, \"failed to build sideband msg\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tcurreply += len;\n\t\treplylen -= len;\n\t}\n\treturn true;\n}\n\nstatic int drm_dp_mst_handle_down_rep(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tstruct drm_dp_mst_branch *mstb = NULL;\n\tstruct drm_dp_sideband_msg_rx *msg = &mgr->down_rep_recv;\n\n\tif (!drm_dp_get_one_sb_msg(mgr, false, &mstb))\n\t\tgoto out_clear_reply;\n\n\t \n\tif (!msg->have_eomt)\n\t\tgoto out;\n\n\t \n\tmutex_lock(&mgr->qlock);\n\ttxmsg = list_first_entry_or_null(&mgr->tx_msg_downq,\n\t\t\t\t\t struct drm_dp_sideband_msg_tx, next);\n\tmutex_unlock(&mgr->qlock);\n\n\t \n\tif (!txmsg || txmsg->dst != mstb) {\n\t\tstruct drm_dp_sideband_msg_hdr *hdr;\n\n\t\thdr = &msg->initial_hdr;\n\t\tdrm_dbg_kms(mgr->dev, \"Got MST reply with no msg %p %d %d %02x %02x\\n\",\n\t\t\t    mstb, hdr->seqno, hdr->lct, hdr->rad[0], msg->msg[0]);\n\t\tgoto out_clear_reply;\n\t}\n\n\tdrm_dp_sideband_parse_reply(mgr, msg, &txmsg->reply);\n\n\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\tdrm_dbg_kms(mgr->dev,\n\t\t\t    \"Got NAK reply: req 0x%02x (%s), reason 0x%02x (%s), nak data 0x%02x\\n\",\n\t\t\t    txmsg->reply.req_type,\n\t\t\t    drm_dp_mst_req_type_str(txmsg->reply.req_type),\n\t\t\t    txmsg->reply.u.nak.reason,\n\t\t\t    drm_dp_mst_nak_reason_str(txmsg->reply.u.nak.reason),\n\t\t\t    txmsg->reply.u.nak.nak_data);\n\t}\n\n\tmemset(msg, 0, sizeof(struct drm_dp_sideband_msg_rx));\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\n\tmutex_lock(&mgr->qlock);\n\ttxmsg->state = DRM_DP_SIDEBAND_TX_RX;\n\tlist_del(&txmsg->next);\n\tmutex_unlock(&mgr->qlock);\n\n\twake_up_all(&mgr->tx_waitq);\n\n\treturn 0;\n\nout_clear_reply:\n\tmemset(msg, 0, sizeof(struct drm_dp_sideband_msg_rx));\nout:\n\tif (mstb)\n\t\tdrm_dp_mst_topology_put_mstb(mstb);\n\n\treturn 0;\n}\n\nstatic inline bool\ndrm_dp_mst_process_up_req(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t  struct drm_dp_pending_up_req *up_req)\n{\n\tstruct drm_dp_mst_branch *mstb = NULL;\n\tstruct drm_dp_sideband_msg_req_body *msg = &up_req->msg;\n\tstruct drm_dp_sideband_msg_hdr *hdr = &up_req->hdr;\n\tbool hotplug = false, dowork = false;\n\n\tif (hdr->broadcast) {\n\t\tconst u8 *guid = NULL;\n\n\t\tif (msg->req_type == DP_CONNECTION_STATUS_NOTIFY)\n\t\t\tguid = msg->u.conn_stat.guid;\n\t\telse if (msg->req_type == DP_RESOURCE_STATUS_NOTIFY)\n\t\t\tguid = msg->u.resource_stat.guid;\n\n\t\tif (guid)\n\t\t\tmstb = drm_dp_get_mst_branch_device_by_guid(mgr, guid);\n\t} else {\n\t\tmstb = drm_dp_get_mst_branch_device(mgr, hdr->lct, hdr->rad);\n\t}\n\n\tif (!mstb) {\n\t\tdrm_dbg_kms(mgr->dev, \"Got MST reply from unknown device %d\\n\", hdr->lct);\n\t\treturn false;\n\t}\n\n\t \n\tif (msg->req_type == DP_CONNECTION_STATUS_NOTIFY) {\n\t\tdowork = drm_dp_mst_handle_conn_stat(mstb, &msg->u.conn_stat);\n\t\thotplug = true;\n\t}\n\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\n\tif (dowork)\n\t\tqueue_work(system_long_wq, &mgr->work);\n\treturn hotplug;\n}\n\nstatic void drm_dp_mst_up_req_work(struct work_struct *work)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr =\n\t\tcontainer_of(work, struct drm_dp_mst_topology_mgr,\n\t\t\t     up_req_work);\n\tstruct drm_dp_pending_up_req *up_req;\n\tbool send_hotplug = false;\n\n\tmutex_lock(&mgr->probe_lock);\n\twhile (true) {\n\t\tmutex_lock(&mgr->up_req_lock);\n\t\tup_req = list_first_entry_or_null(&mgr->up_req_list,\n\t\t\t\t\t\t  struct drm_dp_pending_up_req,\n\t\t\t\t\t\t  next);\n\t\tif (up_req)\n\t\t\tlist_del(&up_req->next);\n\t\tmutex_unlock(&mgr->up_req_lock);\n\n\t\tif (!up_req)\n\t\t\tbreak;\n\n\t\tsend_hotplug |= drm_dp_mst_process_up_req(mgr, up_req);\n\t\tkfree(up_req);\n\t}\n\tmutex_unlock(&mgr->probe_lock);\n\n\tif (send_hotplug)\n\t\tdrm_kms_helper_hotplug_event(mgr->dev);\n}\n\nstatic int drm_dp_mst_handle_up_req(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_pending_up_req *up_req;\n\n\tif (!drm_dp_get_one_sb_msg(mgr, true, NULL))\n\t\tgoto out;\n\n\tif (!mgr->up_req_recv.have_eomt)\n\t\treturn 0;\n\n\tup_req = kzalloc(sizeof(*up_req), GFP_KERNEL);\n\tif (!up_req)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&up_req->next);\n\n\tdrm_dp_sideband_parse_req(mgr, &mgr->up_req_recv, &up_req->msg);\n\n\tif (up_req->msg.req_type != DP_CONNECTION_STATUS_NOTIFY &&\n\t    up_req->msg.req_type != DP_RESOURCE_STATUS_NOTIFY) {\n\t\tdrm_dbg_kms(mgr->dev, \"Received unknown up req type, ignoring: %x\\n\",\n\t\t\t    up_req->msg.req_type);\n\t\tkfree(up_req);\n\t\tgoto out;\n\t}\n\n\tdrm_dp_send_up_ack_reply(mgr, mgr->mst_primary, up_req->msg.req_type,\n\t\t\t\t false);\n\n\tif (up_req->msg.req_type == DP_CONNECTION_STATUS_NOTIFY) {\n\t\tconst struct drm_dp_connection_status_notify *conn_stat =\n\t\t\t&up_req->msg.u.conn_stat;\n\n\t\tdrm_dbg_kms(mgr->dev, \"Got CSN: pn: %d ldps:%d ddps: %d mcs: %d ip: %d pdt: %d\\n\",\n\t\t\t    conn_stat->port_number,\n\t\t\t    conn_stat->legacy_device_plug_status,\n\t\t\t    conn_stat->displayport_device_plug_status,\n\t\t\t    conn_stat->message_capability_status,\n\t\t\t    conn_stat->input_port,\n\t\t\t    conn_stat->peer_device_type);\n\t} else if (up_req->msg.req_type == DP_RESOURCE_STATUS_NOTIFY) {\n\t\tconst struct drm_dp_resource_status_notify *res_stat =\n\t\t\t&up_req->msg.u.resource_stat;\n\n\t\tdrm_dbg_kms(mgr->dev, \"Got RSN: pn: %d avail_pbn %d\\n\",\n\t\t\t    res_stat->port_number,\n\t\t\t    res_stat->available_pbn);\n\t}\n\n\tup_req->hdr = mgr->up_req_recv.initial_hdr;\n\tmutex_lock(&mgr->up_req_lock);\n\tlist_add_tail(&up_req->next, &mgr->up_req_list);\n\tmutex_unlock(&mgr->up_req_lock);\n\tqueue_work(system_long_wq, &mgr->up_req_work);\n\nout:\n\tmemset(&mgr->up_req_recv, 0, sizeof(struct drm_dp_sideband_msg_rx));\n\treturn 0;\n}\n\n \nint drm_dp_mst_hpd_irq_handle_event(struct drm_dp_mst_topology_mgr *mgr, const u8 *esi,\n\t\t\t\t    u8 *ack, bool *handled)\n{\n\tint ret = 0;\n\tint sc;\n\t*handled = false;\n\tsc = DP_GET_SINK_COUNT(esi[0]);\n\n\tif (sc != mgr->sink_count) {\n\t\tmgr->sink_count = sc;\n\t\t*handled = true;\n\t}\n\n\tif (esi[1] & DP_DOWN_REP_MSG_RDY) {\n\t\tret = drm_dp_mst_handle_down_rep(mgr);\n\t\t*handled = true;\n\t\tack[1] |= DP_DOWN_REP_MSG_RDY;\n\t}\n\n\tif (esi[1] & DP_UP_REQ_MSG_RDY) {\n\t\tret |= drm_dp_mst_handle_up_req(mgr);\n\t\t*handled = true;\n\t\tack[1] |= DP_UP_REQ_MSG_RDY;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_mst_hpd_irq_handle_event);\n\n \nvoid drm_dp_mst_hpd_irq_send_new_request(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_sideband_msg_tx *txmsg;\n\tbool kick = true;\n\n\tmutex_lock(&mgr->qlock);\n\ttxmsg = list_first_entry_or_null(&mgr->tx_msg_downq,\n\t\t\t\t\t struct drm_dp_sideband_msg_tx, next);\n\t \n\tif (!txmsg ||\n\t    txmsg->state == DRM_DP_SIDEBAND_TX_START_SEND ||\n\t    txmsg->state == DRM_DP_SIDEBAND_TX_SENT)\n\t\tkick = false;\n\tmutex_unlock(&mgr->qlock);\n\n\tif (kick)\n\t\tdrm_dp_mst_kick_tx(mgr);\n}\nEXPORT_SYMBOL(drm_dp_mst_hpd_irq_send_new_request);\n \nint\ndrm_dp_mst_detect_port(struct drm_connector *connector,\n\t\t       struct drm_modeset_acquire_ctx *ctx,\n\t\t       struct drm_dp_mst_topology_mgr *mgr,\n\t\t       struct drm_dp_mst_port *port)\n{\n\tint ret;\n\n\t \n\tport = drm_dp_mst_topology_get_port_validated(mgr, port);\n\tif (!port)\n\t\treturn connector_status_disconnected;\n\n\tret = drm_modeset_lock(&mgr->base.lock, ctx);\n\tif (ret)\n\t\tgoto out;\n\n\tret = connector_status_disconnected;\n\n\tif (!port->ddps)\n\t\tgoto out;\n\n\tswitch (port->pdt) {\n\tcase DP_PEER_DEVICE_NONE:\n\t\tbreak;\n\tcase DP_PEER_DEVICE_MST_BRANCHING:\n\t\tif (!port->mcs)\n\t\t\tret = connector_status_connected;\n\t\tbreak;\n\n\tcase DP_PEER_DEVICE_SST_SINK:\n\t\tret = connector_status_connected;\n\t\t \n\t\tif (port->port_num >= DP_MST_LOGICAL_PORT_0 && !port->cached_edid)\n\t\t\tport->cached_edid = drm_edid_read_ddc(connector, &port->aux.ddc);\n\t\tbreak;\n\tcase DP_PEER_DEVICE_DP_LEGACY_CONV:\n\t\tif (port->ldps)\n\t\t\tret = connector_status_connected;\n\t\tbreak;\n\t}\nout:\n\tdrm_dp_mst_topology_put_port(port);\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_mst_detect_port);\n\n \nconst struct drm_edid *drm_dp_mst_edid_read(struct drm_connector *connector,\n\t\t\t\t\t    struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t    struct drm_dp_mst_port *port)\n{\n\tconst struct drm_edid *drm_edid;\n\n\t \n\tport = drm_dp_mst_topology_get_port_validated(mgr, port);\n\tif (!port)\n\t\treturn NULL;\n\n\tif (port->cached_edid)\n\t\tdrm_edid = drm_edid_dup(port->cached_edid);\n\telse\n\t\tdrm_edid = drm_edid_read_ddc(connector, &port->aux.ddc);\n\n\tdrm_dp_mst_topology_put_port(port);\n\n\treturn drm_edid;\n}\nEXPORT_SYMBOL(drm_dp_mst_edid_read);\n\n \nstruct edid *drm_dp_mst_get_edid(struct drm_connector *connector,\n\t\t\t\t struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_dp_mst_port *port)\n{\n\tconst struct drm_edid *drm_edid;\n\tstruct edid *edid;\n\n\tdrm_edid = drm_dp_mst_edid_read(connector, mgr, port);\n\n\tedid = drm_edid_duplicate(drm_edid_raw(drm_edid));\n\n\tdrm_edid_free(drm_edid);\n\n\treturn edid;\n}\nEXPORT_SYMBOL(drm_dp_mst_get_edid);\n\n \nint drm_dp_atomic_find_time_slots(struct drm_atomic_state *state,\n\t\t\t\t  struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t  struct drm_dp_mst_port *port, int pbn)\n{\n\tstruct drm_dp_mst_topology_state *topology_state;\n\tstruct drm_dp_mst_atomic_payload *payload = NULL;\n\tstruct drm_connector_state *conn_state;\n\tint prev_slots = 0, prev_bw = 0, req_slots;\n\n\ttopology_state = drm_atomic_get_mst_topology_state(state, mgr);\n\tif (IS_ERR(topology_state))\n\t\treturn PTR_ERR(topology_state);\n\n\tconn_state = drm_atomic_get_new_connector_state(state, port->connector);\n\ttopology_state->pending_crtc_mask |= drm_crtc_mask(conn_state->crtc);\n\n\t \n\tpayload = drm_atomic_get_mst_payload_state(topology_state, port);\n\tif (payload) {\n\t\tprev_slots = payload->time_slots;\n\t\tprev_bw = payload->pbn;\n\n\t\t \n\t\tif (drm_WARN_ON(mgr->dev, payload->delete)) {\n\t\t\tdrm_err(mgr->dev,\n\t\t\t\t\"cannot allocate and release time slots on [MST PORT:%p] in the same state\\n\",\n\t\t\t\tport);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treq_slots = DIV_ROUND_UP(pbn, topology_state->pbn_div);\n\n\tdrm_dbg_atomic(mgr->dev, \"[CONNECTOR:%d:%s] [MST PORT:%p] TU %d -> %d\\n\",\n\t\t       port->connector->base.id, port->connector->name,\n\t\t       port, prev_slots, req_slots);\n\tdrm_dbg_atomic(mgr->dev, \"[CONNECTOR:%d:%s] [MST PORT:%p] PBN %d -> %d\\n\",\n\t\t       port->connector->base.id, port->connector->name,\n\t\t       port, prev_bw, pbn);\n\n\t \n\tif (!payload) {\n\t\tpayload = kzalloc(sizeof(*payload), GFP_KERNEL);\n\t\tif (!payload)\n\t\t\treturn -ENOMEM;\n\n\t\tdrm_dp_mst_get_port_malloc(port);\n\t\tpayload->port = port;\n\t\tpayload->vc_start_slot = -1;\n\t\tlist_add(&payload->next, &topology_state->payloads);\n\t}\n\tpayload->time_slots = req_slots;\n\tpayload->pbn = pbn;\n\n\treturn req_slots;\n}\nEXPORT_SYMBOL(drm_dp_atomic_find_time_slots);\n\n \nint drm_dp_atomic_release_time_slots(struct drm_atomic_state *state,\n\t\t\t\t     struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t     struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_topology_state *topology_state;\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tstruct drm_connector_state *old_conn_state, *new_conn_state;\n\tbool update_payload = true;\n\n\told_conn_state = drm_atomic_get_old_connector_state(state, port->connector);\n\tif (!old_conn_state->crtc)\n\t\treturn 0;\n\n\t \n\tnew_conn_state = drm_atomic_get_new_connector_state(state, port->connector);\n\tif (new_conn_state->crtc) {\n\t\tstruct drm_crtc_state *crtc_state =\n\t\t\tdrm_atomic_get_new_crtc_state(state, new_conn_state->crtc);\n\n\t\t \n\t\tif (!crtc_state || !drm_atomic_crtc_needs_modeset(crtc_state))\n\t\t\treturn 0;\n\n\t\tif (!crtc_state->mode_changed && !crtc_state->connectors_changed)\n\t\t\tupdate_payload = false;\n\t}\n\n\ttopology_state = drm_atomic_get_mst_topology_state(state, mgr);\n\tif (IS_ERR(topology_state))\n\t\treturn PTR_ERR(topology_state);\n\n\ttopology_state->pending_crtc_mask |= drm_crtc_mask(old_conn_state->crtc);\n\tif (!update_payload)\n\t\treturn 0;\n\n\tpayload = drm_atomic_get_mst_payload_state(topology_state, port);\n\tif (WARN_ON(!payload)) {\n\t\tdrm_err(mgr->dev, \"No payload for [MST PORT:%p] found in mst state %p\\n\",\n\t\t\tport, &topology_state->base);\n\t\treturn -EINVAL;\n\t}\n\n\tif (new_conn_state->crtc)\n\t\treturn 0;\n\n\tdrm_dbg_atomic(mgr->dev, \"[MST PORT:%p] TU %d -> 0\\n\", port, payload->time_slots);\n\tif (!payload->delete) {\n\t\tpayload->pbn = 0;\n\t\tpayload->delete = true;\n\t\ttopology_state->payload_mask &= ~BIT(payload->vcpi - 1);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_atomic_release_time_slots);\n\n \nint drm_dp_mst_atomic_setup_commit(struct drm_atomic_state *state)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr;\n\tstruct drm_dp_mst_topology_state *mst_state;\n\tstruct drm_crtc *crtc;\n\tstruct drm_crtc_state *crtc_state;\n\tint i, j, commit_idx, num_commit_deps;\n\n\tfor_each_new_mst_mgr_in_state(state, mgr, mst_state, i) {\n\t\tif (!mst_state->pending_crtc_mask)\n\t\t\tcontinue;\n\n\t\tnum_commit_deps = hweight32(mst_state->pending_crtc_mask);\n\t\tmst_state->commit_deps = kmalloc_array(num_commit_deps,\n\t\t\t\t\t\t       sizeof(*mst_state->commit_deps), GFP_KERNEL);\n\t\tif (!mst_state->commit_deps)\n\t\t\treturn -ENOMEM;\n\t\tmst_state->num_commit_deps = num_commit_deps;\n\n\t\tcommit_idx = 0;\n\t\tfor_each_new_crtc_in_state(state, crtc, crtc_state, j) {\n\t\t\tif (mst_state->pending_crtc_mask & drm_crtc_mask(crtc)) {\n\t\t\t\tmst_state->commit_deps[commit_idx++] =\n\t\t\t\t\tdrm_crtc_commit_get(crtc_state->commit);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_mst_atomic_setup_commit);\n\n \nvoid drm_dp_mst_atomic_wait_for_dependencies(struct drm_atomic_state *state)\n{\n\tstruct drm_dp_mst_topology_state *old_mst_state, *new_mst_state;\n\tstruct drm_dp_mst_topology_mgr *mgr;\n\tstruct drm_dp_mst_atomic_payload *old_payload, *new_payload;\n\tint i, j, ret;\n\n\tfor_each_oldnew_mst_mgr_in_state(state, mgr, old_mst_state, new_mst_state, i) {\n\t\tfor (j = 0; j < old_mst_state->num_commit_deps; j++) {\n\t\t\tret = drm_crtc_commit_wait(old_mst_state->commit_deps[j]);\n\t\t\tif (ret < 0)\n\t\t\t\tdrm_err(state->dev, \"Failed to wait for %s: %d\\n\",\n\t\t\t\t\told_mst_state->commit_deps[j]->crtc->name, ret);\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(old_payload, &old_mst_state->payloads, next) {\n\t\t\tif (old_payload->delete)\n\t\t\t\tcontinue;\n\n\t\t\tnew_payload = drm_atomic_get_mst_payload_state(new_mst_state,\n\t\t\t\t\t\t\t\t       old_payload->port);\n\t\t\tnew_payload->vc_start_slot = old_payload->vc_start_slot;\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(drm_dp_mst_atomic_wait_for_dependencies);\n\n \nint drm_dp_mst_root_conn_atomic_check(struct drm_connector_state *new_conn_state,\n\t\t\t\t      struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_atomic_state *state = new_conn_state->state;\n\tstruct drm_connector_state *old_conn_state =\n\t\tdrm_atomic_get_old_connector_state(state, new_conn_state->connector);\n\tstruct drm_crtc_state *crtc_state;\n\tstruct drm_dp_mst_topology_state *mst_state = NULL;\n\n\tif (new_conn_state->crtc) {\n\t\tcrtc_state = drm_atomic_get_new_crtc_state(state, new_conn_state->crtc);\n\t\tif (crtc_state && drm_atomic_crtc_needs_modeset(crtc_state)) {\n\t\t\tmst_state = drm_atomic_get_mst_topology_state(state, mgr);\n\t\t\tif (IS_ERR(mst_state))\n\t\t\t\treturn PTR_ERR(mst_state);\n\n\t\t\tmst_state->pending_crtc_mask |= drm_crtc_mask(new_conn_state->crtc);\n\t\t}\n\t}\n\n\tif (old_conn_state->crtc) {\n\t\tcrtc_state = drm_atomic_get_new_crtc_state(state, old_conn_state->crtc);\n\t\tif (crtc_state && drm_atomic_crtc_needs_modeset(crtc_state)) {\n\t\t\tif (!mst_state) {\n\t\t\t\tmst_state = drm_atomic_get_mst_topology_state(state, mgr);\n\t\t\t\tif (IS_ERR(mst_state))\n\t\t\t\t\treturn PTR_ERR(mst_state);\n\t\t\t}\n\n\t\t\tmst_state->pending_crtc_mask |= drm_crtc_mask(old_conn_state->crtc);\n\t\t}\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_mst_root_conn_atomic_check);\n\n \nvoid drm_dp_mst_update_slots(struct drm_dp_mst_topology_state *mst_state, uint8_t link_encoding_cap)\n{\n\tif (link_encoding_cap == DP_CAP_ANSI_128B132B) {\n\t\tmst_state->total_avail_slots = 64;\n\t\tmst_state->start_slot = 0;\n\t} else {\n\t\tmst_state->total_avail_slots = 63;\n\t\tmst_state->start_slot = 1;\n\t}\n\n\tDRM_DEBUG_KMS(\"%s encoding format on mst_state 0x%p\\n\",\n\t\t      (link_encoding_cap == DP_CAP_ANSI_128B132B) ? \"128b/132b\":\"8b/10b\",\n\t\t      mst_state);\n}\nEXPORT_SYMBOL(drm_dp_mst_update_slots);\n\nstatic int drm_dp_dpcd_write_payload(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t     int id, u8 start_slot, u8 num_slots)\n{\n\tu8 payload_alloc[3], status;\n\tint ret;\n\tint retries = 0;\n\n\tdrm_dp_dpcd_writeb(mgr->aux, DP_PAYLOAD_TABLE_UPDATE_STATUS,\n\t\t\t   DP_PAYLOAD_TABLE_UPDATED);\n\n\tpayload_alloc[0] = id;\n\tpayload_alloc[1] = start_slot;\n\tpayload_alloc[2] = num_slots;\n\n\tret = drm_dp_dpcd_write(mgr->aux, DP_PAYLOAD_ALLOCATE_SET, payload_alloc, 3);\n\tif (ret != 3) {\n\t\tdrm_dbg_kms(mgr->dev, \"failed to write payload allocation %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\nretry:\n\tret = drm_dp_dpcd_readb(mgr->aux, DP_PAYLOAD_TABLE_UPDATE_STATUS, &status);\n\tif (ret < 0) {\n\t\tdrm_dbg_kms(mgr->dev, \"failed to read payload table status %d\\n\", ret);\n\t\tgoto fail;\n\t}\n\n\tif (!(status & DP_PAYLOAD_TABLE_UPDATED)) {\n\t\tretries++;\n\t\tif (retries < 20) {\n\t\t\tusleep_range(10000, 20000);\n\t\t\tgoto retry;\n\t\t}\n\t\tdrm_dbg_kms(mgr->dev, \"status not set after read payload table status %d\\n\",\n\t\t\t    status);\n\t\tret = -EINVAL;\n\t\tgoto fail;\n\t}\n\tret = 0;\nfail:\n\treturn ret;\n}\n\nstatic int do_get_act_status(struct drm_dp_aux *aux)\n{\n\tint ret;\n\tu8 status;\n\n\tret = drm_dp_dpcd_readb(aux, DP_PAYLOAD_TABLE_UPDATE_STATUS, &status);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn status;\n}\n\n \nint drm_dp_check_act_status(struct drm_dp_mst_topology_mgr *mgr)\n{\n\t \n\tconst int timeout_ms = 3000;\n\tint ret, status;\n\n\tret = readx_poll_timeout(do_get_act_status, mgr->aux, status,\n\t\t\t\t status & DP_PAYLOAD_ACT_HANDLED || status < 0,\n\t\t\t\t 200, timeout_ms * USEC_PER_MSEC);\n\tif (ret < 0 && status >= 0) {\n\t\tdrm_err(mgr->dev, \"Failed to get ACT after %dms, last status: %02x\\n\",\n\t\t\ttimeout_ms, status);\n\t\treturn -EINVAL;\n\t} else if (status < 0) {\n\t\t \n\t\tdrm_dbg_kms(mgr->dev, \"Failed to read payload table status: %d\\n\", status);\n\t\treturn status;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_check_act_status);\n\n \nint drm_dp_calc_pbn_mode(int clock, int bpp)\n{\n\t \n\treturn DIV_ROUND_UP_ULL(mul_u32_u32(clock * bpp, 64 * 1006 >> 4),\n\t\t\t\t1000 * 8 * 54 * 1000);\n}\nEXPORT_SYMBOL(drm_dp_calc_pbn_mode);\n\n \nstatic void drm_dp_mst_kick_tx(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tqueue_work(system_long_wq, &mgr->tx_work);\n}\n\n \nstatic const char *pdt_to_string(u8 pdt)\n{\n\tswitch (pdt) {\n\tcase DP_PEER_DEVICE_NONE:\n\t\treturn \"NONE\";\n\tcase DP_PEER_DEVICE_SOURCE_OR_SST:\n\t\treturn \"SOURCE OR SST\";\n\tcase DP_PEER_DEVICE_MST_BRANCHING:\n\t\treturn \"MST BRANCHING\";\n\tcase DP_PEER_DEVICE_SST_SINK:\n\t\treturn \"SST SINK\";\n\tcase DP_PEER_DEVICE_DP_LEGACY_CONV:\n\t\treturn \"DP LEGACY CONV\";\n\tdefault:\n\t\treturn \"ERR\";\n\t}\n}\n\nstatic void drm_dp_mst_dump_mstb(struct seq_file *m,\n\t\t\t\t struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_mst_port *port;\n\tint tabs = mstb->lct;\n\tchar prefix[10];\n\tint i;\n\n\tfor (i = 0; i < tabs; i++)\n\t\tprefix[i] = '\\t';\n\tprefix[i] = '\\0';\n\n\tseq_printf(m, \"%smstb - [%p]: num_ports: %d\\n\", prefix, mstb, mstb->num_ports);\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tseq_printf(m, \"%sport %d - [%p] (%s - %s): ddps: %d, ldps: %d, sdp: %d/%d, fec: %s, conn: %p\\n\",\n\t\t\t   prefix,\n\t\t\t   port->port_num,\n\t\t\t   port,\n\t\t\t   port->input ? \"input\" : \"output\",\n\t\t\t   pdt_to_string(port->pdt),\n\t\t\t   port->ddps,\n\t\t\t   port->ldps,\n\t\t\t   port->num_sdp_streams,\n\t\t\t   port->num_sdp_stream_sinks,\n\t\t\t   port->fec_capable ? \"true\" : \"false\",\n\t\t\t   port->connector);\n\t\tif (port->mstb)\n\t\t\tdrm_dp_mst_dump_mstb(m, port->mstb);\n\t}\n}\n\n#define DP_PAYLOAD_TABLE_SIZE\t\t64\n\nstatic bool dump_dp_payload_table(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t  char *buf)\n{\n\tint i;\n\n\tfor (i = 0; i < DP_PAYLOAD_TABLE_SIZE; i += 16) {\n\t\tif (drm_dp_dpcd_read(mgr->aux,\n\t\t\t\t     DP_PAYLOAD_TABLE_UPDATE_STATUS + i,\n\t\t\t\t     &buf[i], 16) != 16)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic void fetch_monitor_name(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t       struct drm_dp_mst_port *port, char *name,\n\t\t\t       int namelen)\n{\n\tstruct edid *mst_edid;\n\n\tmst_edid = drm_dp_mst_get_edid(port->connector, mgr, port);\n\tdrm_edid_get_monitor_name(mst_edid, name, namelen);\n\tkfree(mst_edid);\n}\n\n \nvoid drm_dp_mst_dump_topology(struct seq_file *m,\n\t\t\t      struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_mst_topology_state *state;\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tint i, ret;\n\n\tmutex_lock(&mgr->lock);\n\tif (mgr->mst_primary)\n\t\tdrm_dp_mst_dump_mstb(m, mgr->mst_primary);\n\n\t \n\tmutex_unlock(&mgr->lock);\n\n\tret = drm_modeset_lock_single_interruptible(&mgr->base.lock);\n\tif (ret < 0)\n\t\treturn;\n\n\tstate = to_drm_dp_mst_topology_state(mgr->base.state);\n\tseq_printf(m, \"\\n*** Atomic state info ***\\n\");\n\tseq_printf(m, \"payload_mask: %x, max_payloads: %d, start_slot: %u, pbn_div: %d\\n\",\n\t\t   state->payload_mask, mgr->max_payloads, state->start_slot, state->pbn_div);\n\n\tseq_printf(m, \"\\n| idx | port | vcpi | slots | pbn | dsc |     sink name     |\\n\");\n\tfor (i = 0; i < mgr->max_payloads; i++) {\n\t\tlist_for_each_entry(payload, &state->payloads, next) {\n\t\t\tchar name[14];\n\n\t\t\tif (payload->vcpi != i || payload->delete)\n\t\t\t\tcontinue;\n\n\t\t\tfetch_monitor_name(mgr, payload->port, name, sizeof(name));\n\t\t\tseq_printf(m, \" %5d %6d %6d %02d - %02d %5d %5s %19s\\n\",\n\t\t\t\t   i,\n\t\t\t\t   payload->port->port_num,\n\t\t\t\t   payload->vcpi,\n\t\t\t\t   payload->vc_start_slot,\n\t\t\t\t   payload->vc_start_slot + payload->time_slots - 1,\n\t\t\t\t   payload->pbn,\n\t\t\t\t   payload->dsc_enabled ? \"Y\" : \"N\",\n\t\t\t\t   (*name != 0) ? name : \"Unknown\");\n\t\t}\n\t}\n\n\tseq_printf(m, \"\\n*** DPCD Info ***\\n\");\n\tmutex_lock(&mgr->lock);\n\tif (mgr->mst_primary) {\n\t\tu8 buf[DP_PAYLOAD_TABLE_SIZE];\n\t\tint ret;\n\n\t\tif (drm_dp_read_dpcd_caps(mgr->aux, buf) < 0) {\n\t\t\tseq_printf(m, \"dpcd read failed\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tseq_printf(m, \"dpcd: %*ph\\n\", DP_RECEIVER_CAP_SIZE, buf);\n\n\t\tret = drm_dp_dpcd_read(mgr->aux, DP_FAUX_CAP, buf, 2);\n\t\tif (ret != 2) {\n\t\t\tseq_printf(m, \"faux/mst read failed\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tseq_printf(m, \"faux/mst: %*ph\\n\", 2, buf);\n\n\t\tret = drm_dp_dpcd_read(mgr->aux, DP_MSTM_CTRL, buf, 1);\n\t\tif (ret != 1) {\n\t\t\tseq_printf(m, \"mst ctrl read failed\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tseq_printf(m, \"mst ctrl: %*ph\\n\", 1, buf);\n\n\t\t \n\t\tret = drm_dp_dpcd_read(mgr->aux, DP_BRANCH_OUI, buf, DP_BRANCH_OUI_HEADER_SIZE);\n\t\tif (ret != DP_BRANCH_OUI_HEADER_SIZE) {\n\t\t\tseq_printf(m, \"branch oui read failed\\n\");\n\t\t\tgoto out;\n\t\t}\n\t\tseq_printf(m, \"branch oui: %*phN devid: \", 3, buf);\n\n\t\tfor (i = 0x3; i < 0x8 && buf[i]; i++)\n\t\t\tseq_printf(m, \"%c\", buf[i]);\n\t\tseq_printf(m, \" revision: hw: %x.%x sw: %x.%x\\n\",\n\t\t\t   buf[0x9] >> 4, buf[0x9] & 0xf, buf[0xa], buf[0xb]);\n\t\tif (dump_dp_payload_table(mgr, buf))\n\t\t\tseq_printf(m, \"payload table: %*ph\\n\", DP_PAYLOAD_TABLE_SIZE, buf);\n\t}\n\nout:\n\tmutex_unlock(&mgr->lock);\n\tdrm_modeset_unlock(&mgr->base.lock);\n}\nEXPORT_SYMBOL(drm_dp_mst_dump_topology);\n\nstatic void drm_dp_tx_work(struct work_struct *work)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = container_of(work, struct drm_dp_mst_topology_mgr, tx_work);\n\n\tmutex_lock(&mgr->qlock);\n\tif (!list_empty(&mgr->tx_msg_downq))\n\t\tprocess_single_down_tx_qlock(mgr);\n\tmutex_unlock(&mgr->qlock);\n}\n\nstatic inline void\ndrm_dp_delayed_destroy_port(struct drm_dp_mst_port *port)\n{\n\tdrm_dp_port_set_pdt(port, DP_PEER_DEVICE_NONE, port->mcs);\n\n\tif (port->connector) {\n\t\tdrm_connector_unregister(port->connector);\n\t\tdrm_connector_put(port->connector);\n\t}\n\n\tdrm_dp_mst_put_port_malloc(port);\n}\n\nstatic inline void\ndrm_dp_delayed_destroy_mstb(struct drm_dp_mst_branch *mstb)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = mstb->mgr;\n\tstruct drm_dp_mst_port *port, *port_tmp;\n\tstruct drm_dp_sideband_msg_tx *txmsg, *txmsg_tmp;\n\tbool wake_tx = false;\n\n\tmutex_lock(&mgr->lock);\n\tlist_for_each_entry_safe(port, port_tmp, &mstb->ports, next) {\n\t\tlist_del(&port->next);\n\t\tdrm_dp_mst_topology_put_port(port);\n\t}\n\tmutex_unlock(&mgr->lock);\n\n\t \n\tmutex_lock(&mstb->mgr->qlock);\n\tlist_for_each_entry_safe(txmsg, txmsg_tmp, &mgr->tx_msg_downq, next) {\n\t\tif (txmsg->dst != mstb)\n\t\t\tcontinue;\n\n\t\ttxmsg->state = DRM_DP_SIDEBAND_TX_TIMEOUT;\n\t\tlist_del(&txmsg->next);\n\t\twake_tx = true;\n\t}\n\tmutex_unlock(&mstb->mgr->qlock);\n\n\tif (wake_tx)\n\t\twake_up_all(&mstb->mgr->tx_waitq);\n\n\tdrm_dp_mst_put_mstb_malloc(mstb);\n}\n\nstatic void drm_dp_delayed_destroy_work(struct work_struct *work)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr =\n\t\tcontainer_of(work, struct drm_dp_mst_topology_mgr,\n\t\t\t     delayed_destroy_work);\n\tbool send_hotplug = false, go_again;\n\n\t \n\tdo {\n\t\tgo_again = false;\n\n\t\tfor (;;) {\n\t\t\tstruct drm_dp_mst_branch *mstb;\n\n\t\t\tmutex_lock(&mgr->delayed_destroy_lock);\n\t\t\tmstb = list_first_entry_or_null(&mgr->destroy_branch_device_list,\n\t\t\t\t\t\t\tstruct drm_dp_mst_branch,\n\t\t\t\t\t\t\tdestroy_next);\n\t\t\tif (mstb)\n\t\t\t\tlist_del(&mstb->destroy_next);\n\t\t\tmutex_unlock(&mgr->delayed_destroy_lock);\n\n\t\t\tif (!mstb)\n\t\t\t\tbreak;\n\n\t\t\tdrm_dp_delayed_destroy_mstb(mstb);\n\t\t\tgo_again = true;\n\t\t}\n\n\t\tfor (;;) {\n\t\t\tstruct drm_dp_mst_port *port;\n\n\t\t\tmutex_lock(&mgr->delayed_destroy_lock);\n\t\t\tport = list_first_entry_or_null(&mgr->destroy_port_list,\n\t\t\t\t\t\t\tstruct drm_dp_mst_port,\n\t\t\t\t\t\t\tnext);\n\t\t\tif (port)\n\t\t\t\tlist_del(&port->next);\n\t\t\tmutex_unlock(&mgr->delayed_destroy_lock);\n\n\t\t\tif (!port)\n\t\t\t\tbreak;\n\n\t\t\tdrm_dp_delayed_destroy_port(port);\n\t\t\tsend_hotplug = true;\n\t\t\tgo_again = true;\n\t\t}\n\t} while (go_again);\n\n\tif (send_hotplug)\n\t\tdrm_kms_helper_hotplug_event(mgr->dev);\n}\n\nstatic struct drm_private_state *\ndrm_dp_mst_duplicate_state(struct drm_private_obj *obj)\n{\n\tstruct drm_dp_mst_topology_state *state, *old_state =\n\t\tto_dp_mst_topology_state(obj->state);\n\tstruct drm_dp_mst_atomic_payload *pos, *payload;\n\n\tstate = kmemdup(old_state, sizeof(*state), GFP_KERNEL);\n\tif (!state)\n\t\treturn NULL;\n\n\t__drm_atomic_helper_private_obj_duplicate_state(obj, &state->base);\n\n\tINIT_LIST_HEAD(&state->payloads);\n\tstate->commit_deps = NULL;\n\tstate->num_commit_deps = 0;\n\tstate->pending_crtc_mask = 0;\n\n\tlist_for_each_entry(pos, &old_state->payloads, next) {\n\t\t \n\t\tif (pos->delete)\n\t\t\tcontinue;\n\n\t\tpayload = kmemdup(pos, sizeof(*payload), GFP_KERNEL);\n\t\tif (!payload)\n\t\t\tgoto fail;\n\n\t\tdrm_dp_mst_get_port_malloc(payload->port);\n\t\tlist_add(&payload->next, &state->payloads);\n\t}\n\n\treturn &state->base;\n\nfail:\n\tlist_for_each_entry_safe(pos, payload, &state->payloads, next) {\n\t\tdrm_dp_mst_put_port_malloc(pos->port);\n\t\tkfree(pos);\n\t}\n\tkfree(state);\n\n\treturn NULL;\n}\n\nstatic void drm_dp_mst_destroy_state(struct drm_private_obj *obj,\n\t\t\t\t     struct drm_private_state *state)\n{\n\tstruct drm_dp_mst_topology_state *mst_state =\n\t\tto_dp_mst_topology_state(state);\n\tstruct drm_dp_mst_atomic_payload *pos, *tmp;\n\tint i;\n\n\tlist_for_each_entry_safe(pos, tmp, &mst_state->payloads, next) {\n\t\t \n\t\tif (!pos->delete)\n\t\t\tdrm_dp_mst_put_port_malloc(pos->port);\n\t\tkfree(pos);\n\t}\n\n\tfor (i = 0; i < mst_state->num_commit_deps; i++)\n\t\tdrm_crtc_commit_put(mst_state->commit_deps[i]);\n\n\tkfree(mst_state->commit_deps);\n\tkfree(mst_state);\n}\n\nstatic bool drm_dp_mst_port_downstream_of_branch(struct drm_dp_mst_port *port,\n\t\t\t\t\t\t struct drm_dp_mst_branch *branch)\n{\n\twhile (port->parent) {\n\t\tif (port->parent == branch)\n\t\t\treturn true;\n\n\t\tif (port->parent->port_parent)\n\t\t\tport = port->parent->port_parent;\n\t\telse\n\t\t\tbreak;\n\t}\n\treturn false;\n}\n\nstatic int\ndrm_dp_mst_atomic_check_port_bw_limit(struct drm_dp_mst_port *port,\n\t\t\t\t      struct drm_dp_mst_topology_state *state);\n\nstatic int\ndrm_dp_mst_atomic_check_mstb_bw_limit(struct drm_dp_mst_branch *mstb,\n\t\t\t\t      struct drm_dp_mst_topology_state *state)\n{\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tstruct drm_dp_mst_port *port;\n\tint pbn_used = 0, ret;\n\tbool found = false;\n\n\t \n\tlist_for_each_entry(payload, &state->payloads, next) {\n\t\tif (!payload->pbn ||\n\t\t    !drm_dp_mst_port_downstream_of_branch(payload->port, mstb))\n\t\t\tcontinue;\n\n\t\tfound = true;\n\t\tbreak;\n\t}\n\tif (!found)\n\t\treturn 0;\n\n\tif (mstb->port_parent)\n\t\tdrm_dbg_atomic(mstb->mgr->dev,\n\t\t\t       \"[MSTB:%p] [MST PORT:%p] Checking bandwidth limits on [MSTB:%p]\\n\",\n\t\t\t       mstb->port_parent->parent, mstb->port_parent, mstb);\n\telse\n\t\tdrm_dbg_atomic(mstb->mgr->dev, \"[MSTB:%p] Checking bandwidth limits\\n\", mstb);\n\n\tlist_for_each_entry(port, &mstb->ports, next) {\n\t\tret = drm_dp_mst_atomic_check_port_bw_limit(port, state);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tpbn_used += ret;\n\t}\n\n\treturn pbn_used;\n}\n\nstatic int\ndrm_dp_mst_atomic_check_port_bw_limit(struct drm_dp_mst_port *port,\n\t\t\t\t      struct drm_dp_mst_topology_state *state)\n{\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tint pbn_used = 0;\n\n\tif (port->pdt == DP_PEER_DEVICE_NONE)\n\t\treturn 0;\n\n\tif (drm_dp_mst_is_end_device(port->pdt, port->mcs)) {\n\t\tpayload = drm_atomic_get_mst_payload_state(state, port);\n\t\tif (!payload)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!port->full_pbn) {\n\t\t\tdrm_dbg_atomic(port->mgr->dev,\n\t\t\t\t       \"[MSTB:%p] [MST PORT:%p] no BW available for the port\\n\",\n\t\t\t\t       port->parent, port);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpbn_used = payload->pbn;\n\t} else {\n\t\tpbn_used = drm_dp_mst_atomic_check_mstb_bw_limit(port->mstb,\n\t\t\t\t\t\t\t\t state);\n\t\tif (pbn_used <= 0)\n\t\t\treturn pbn_used;\n\t}\n\n\tif (pbn_used > port->full_pbn) {\n\t\tdrm_dbg_atomic(port->mgr->dev,\n\t\t\t       \"[MSTB:%p] [MST PORT:%p] required PBN of %d exceeds port limit of %d\\n\",\n\t\t\t       port->parent, port, pbn_used, port->full_pbn);\n\t\treturn -ENOSPC;\n\t}\n\n\tdrm_dbg_atomic(port->mgr->dev, \"[MSTB:%p] [MST PORT:%p] uses %d out of %d PBN\\n\",\n\t\t       port->parent, port, pbn_used, port->full_pbn);\n\n\treturn pbn_used;\n}\n\nstatic inline int\ndrm_dp_mst_atomic_check_payload_alloc_limits(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t\t     struct drm_dp_mst_topology_state *mst_state)\n{\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tint avail_slots = mst_state->total_avail_slots, payload_count = 0;\n\n\tlist_for_each_entry(payload, &mst_state->payloads, next) {\n\t\t \n\t\tif (payload->delete) {\n\t\t\tdrm_dbg_atomic(mgr->dev, \"[MST PORT:%p] releases all time slots\\n\",\n\t\t\t\t       payload->port);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdrm_dbg_atomic(mgr->dev, \"[MST PORT:%p] requires %d time slots\\n\",\n\t\t\t       payload->port, payload->time_slots);\n\n\t\tavail_slots -= payload->time_slots;\n\t\tif (avail_slots < 0) {\n\t\t\tdrm_dbg_atomic(mgr->dev,\n\t\t\t\t       \"[MST PORT:%p] not enough time slots in mst state %p (avail=%d)\\n\",\n\t\t\t\t       payload->port, mst_state, avail_slots + payload->time_slots);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\tif (++payload_count > mgr->max_payloads) {\n\t\t\tdrm_dbg_atomic(mgr->dev,\n\t\t\t\t       \"[MST MGR:%p] state %p has too many payloads (max=%d)\\n\",\n\t\t\t\t       mgr, mst_state, mgr->max_payloads);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (!payload->vcpi) {\n\t\t\tpayload->vcpi = ffz(mst_state->payload_mask) + 1;\n\t\t\tdrm_dbg_atomic(mgr->dev, \"[MST PORT:%p] assigned VCPI #%d\\n\",\n\t\t\t\t       payload->port, payload->vcpi);\n\t\t\tmst_state->payload_mask |= BIT(payload->vcpi - 1);\n\t\t}\n\t}\n\n\tif (!payload_count)\n\t\tmst_state->pbn_div = 0;\n\n\tdrm_dbg_atomic(mgr->dev, \"[MST MGR:%p] mst state %p TU pbn_div=%d avail=%d used=%d\\n\",\n\t\t       mgr, mst_state, mst_state->pbn_div, avail_slots,\n\t\t       mst_state->total_avail_slots - avail_slots);\n\n\treturn 0;\n}\n\n \nint drm_dp_mst_add_affected_dsc_crtcs(struct drm_atomic_state *state, struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_dp_mst_topology_state *mst_state;\n\tstruct drm_dp_mst_atomic_payload *pos;\n\tstruct drm_connector *connector;\n\tstruct drm_connector_state *conn_state;\n\tstruct drm_crtc *crtc;\n\tstruct drm_crtc_state *crtc_state;\n\n\tmst_state = drm_atomic_get_mst_topology_state(state, mgr);\n\n\tif (IS_ERR(mst_state))\n\t\treturn PTR_ERR(mst_state);\n\n\tlist_for_each_entry(pos, &mst_state->payloads, next) {\n\n\t\tconnector = pos->port->connector;\n\n\t\tif (!connector)\n\t\t\treturn -EINVAL;\n\n\t\tconn_state = drm_atomic_get_connector_state(state, connector);\n\n\t\tif (IS_ERR(conn_state))\n\t\t\treturn PTR_ERR(conn_state);\n\n\t\tcrtc = conn_state->crtc;\n\n\t\tif (!crtc)\n\t\t\tcontinue;\n\n\t\tif (!drm_dp_mst_dsc_aux_for_port(pos->port))\n\t\t\tcontinue;\n\n\t\tcrtc_state = drm_atomic_get_crtc_state(mst_state->base.state, crtc);\n\n\t\tif (IS_ERR(crtc_state))\n\t\t\treturn PTR_ERR(crtc_state);\n\n\t\tdrm_dbg_atomic(mgr->dev, \"[MST MGR:%p] Setting mode_changed flag on CRTC %p\\n\",\n\t\t\t       mgr, crtc);\n\n\t\tcrtc_state->mode_changed = true;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_mst_add_affected_dsc_crtcs);\n\n \nint drm_dp_mst_atomic_enable_dsc(struct drm_atomic_state *state,\n\t\t\t\t struct drm_dp_mst_port *port,\n\t\t\t\t int pbn, bool enable)\n{\n\tstruct drm_dp_mst_topology_state *mst_state;\n\tstruct drm_dp_mst_atomic_payload *payload;\n\tint time_slots = 0;\n\n\tmst_state = drm_atomic_get_mst_topology_state(state, port->mgr);\n\tif (IS_ERR(mst_state))\n\t\treturn PTR_ERR(mst_state);\n\n\tpayload = drm_atomic_get_mst_payload_state(mst_state, port);\n\tif (!payload) {\n\t\tdrm_dbg_atomic(state->dev,\n\t\t\t       \"[MST PORT:%p] Couldn't find payload in mst state %p\\n\",\n\t\t\t       port, mst_state);\n\t\treturn -EINVAL;\n\t}\n\n\tif (payload->dsc_enabled == enable) {\n\t\tdrm_dbg_atomic(state->dev,\n\t\t\t       \"[MST PORT:%p] DSC flag is already set to %d, returning %d time slots\\n\",\n\t\t\t       port, enable, payload->time_slots);\n\t\ttime_slots = payload->time_slots;\n\t}\n\n\tif (enable) {\n\t\ttime_slots = drm_dp_atomic_find_time_slots(state, port->mgr, port, pbn);\n\t\tdrm_dbg_atomic(state->dev,\n\t\t\t       \"[MST PORT:%p] Enabling DSC flag, reallocating %d time slots on the port\\n\",\n\t\t\t       port, time_slots);\n\t\tif (time_slots < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tpayload->dsc_enabled = enable;\n\n\treturn time_slots;\n}\nEXPORT_SYMBOL(drm_dp_mst_atomic_enable_dsc);\n\n \nint drm_dp_mst_atomic_check(struct drm_atomic_state *state)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr;\n\tstruct drm_dp_mst_topology_state *mst_state;\n\tint i, ret = 0;\n\n\tfor_each_new_mst_mgr_in_state(state, mgr, mst_state, i) {\n\t\tif (!mgr->mst_state)\n\t\t\tcontinue;\n\n\t\tret = drm_dp_mst_atomic_check_payload_alloc_limits(mgr, mst_state);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tmutex_lock(&mgr->lock);\n\t\tret = drm_dp_mst_atomic_check_mstb_bw_limit(mgr->mst_primary,\n\t\t\t\t\t\t\t    mst_state);\n\t\tmutex_unlock(&mgr->lock);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\telse\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL(drm_dp_mst_atomic_check);\n\nconst struct drm_private_state_funcs drm_dp_mst_topology_state_funcs = {\n\t.atomic_duplicate_state = drm_dp_mst_duplicate_state,\n\t.atomic_destroy_state = drm_dp_mst_destroy_state,\n};\nEXPORT_SYMBOL(drm_dp_mst_topology_state_funcs);\n\n \nstruct drm_dp_mst_topology_state *drm_atomic_get_mst_topology_state(struct drm_atomic_state *state,\n\t\t\t\t\t\t\t\t    struct drm_dp_mst_topology_mgr *mgr)\n{\n\treturn to_dp_mst_topology_state(drm_atomic_get_private_obj_state(state, &mgr->base));\n}\nEXPORT_SYMBOL(drm_atomic_get_mst_topology_state);\n\n \nstruct drm_dp_mst_topology_state *\ndrm_atomic_get_old_mst_topology_state(struct drm_atomic_state *state,\n\t\t\t\t      struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_private_state *old_priv_state =\n\t\tdrm_atomic_get_old_private_obj_state(state, &mgr->base);\n\n\treturn old_priv_state ? to_dp_mst_topology_state(old_priv_state) : NULL;\n}\nEXPORT_SYMBOL(drm_atomic_get_old_mst_topology_state);\n\n \nstruct drm_dp_mst_topology_state *\ndrm_atomic_get_new_mst_topology_state(struct drm_atomic_state *state,\n\t\t\t\t      struct drm_dp_mst_topology_mgr *mgr)\n{\n\tstruct drm_private_state *new_priv_state =\n\t\tdrm_atomic_get_new_private_obj_state(state, &mgr->base);\n\n\treturn new_priv_state ? to_dp_mst_topology_state(new_priv_state) : NULL;\n}\nEXPORT_SYMBOL(drm_atomic_get_new_mst_topology_state);\n\n \nint drm_dp_mst_topology_mgr_init(struct drm_dp_mst_topology_mgr *mgr,\n\t\t\t\t struct drm_device *dev, struct drm_dp_aux *aux,\n\t\t\t\t int max_dpcd_transaction_bytes, int max_payloads,\n\t\t\t\t int conn_base_id)\n{\n\tstruct drm_dp_mst_topology_state *mst_state;\n\n\tmutex_init(&mgr->lock);\n\tmutex_init(&mgr->qlock);\n\tmutex_init(&mgr->delayed_destroy_lock);\n\tmutex_init(&mgr->up_req_lock);\n\tmutex_init(&mgr->probe_lock);\n#if IS_ENABLED(CONFIG_DRM_DEBUG_DP_MST_TOPOLOGY_REFS)\n\tmutex_init(&mgr->topology_ref_history_lock);\n\tstack_depot_init();\n#endif\n\tINIT_LIST_HEAD(&mgr->tx_msg_downq);\n\tINIT_LIST_HEAD(&mgr->destroy_port_list);\n\tINIT_LIST_HEAD(&mgr->destroy_branch_device_list);\n\tINIT_LIST_HEAD(&mgr->up_req_list);\n\n\t \n\tmgr->delayed_destroy_wq = alloc_ordered_workqueue(\"drm_dp_mst_wq\", 0);\n\tif (mgr->delayed_destroy_wq == NULL)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&mgr->work, drm_dp_mst_link_probe_work);\n\tINIT_WORK(&mgr->tx_work, drm_dp_tx_work);\n\tINIT_WORK(&mgr->delayed_destroy_work, drm_dp_delayed_destroy_work);\n\tINIT_WORK(&mgr->up_req_work, drm_dp_mst_up_req_work);\n\tinit_waitqueue_head(&mgr->tx_waitq);\n\tmgr->dev = dev;\n\tmgr->aux = aux;\n\tmgr->max_dpcd_transaction_bytes = max_dpcd_transaction_bytes;\n\tmgr->max_payloads = max_payloads;\n\tmgr->conn_base_id = conn_base_id;\n\n\tmst_state = kzalloc(sizeof(*mst_state), GFP_KERNEL);\n\tif (mst_state == NULL)\n\t\treturn -ENOMEM;\n\n\tmst_state->total_avail_slots = 63;\n\tmst_state->start_slot = 1;\n\n\tmst_state->mgr = mgr;\n\tINIT_LIST_HEAD(&mst_state->payloads);\n\n\tdrm_atomic_private_obj_init(dev, &mgr->base,\n\t\t\t\t    &mst_state->base,\n\t\t\t\t    &drm_dp_mst_topology_state_funcs);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(drm_dp_mst_topology_mgr_init);\n\n \nvoid drm_dp_mst_topology_mgr_destroy(struct drm_dp_mst_topology_mgr *mgr)\n{\n\tdrm_dp_mst_topology_mgr_set_mst(mgr, false);\n\tflush_work(&mgr->work);\n\t \n\tif (mgr->delayed_destroy_wq) {\n\t\tdestroy_workqueue(mgr->delayed_destroy_wq);\n\t\tmgr->delayed_destroy_wq = NULL;\n\t}\n\tmgr->dev = NULL;\n\tmgr->aux = NULL;\n\tdrm_atomic_private_obj_fini(&mgr->base);\n\tmgr->funcs = NULL;\n\n\tmutex_destroy(&mgr->delayed_destroy_lock);\n\tmutex_destroy(&mgr->qlock);\n\tmutex_destroy(&mgr->lock);\n\tmutex_destroy(&mgr->up_req_lock);\n\tmutex_destroy(&mgr->probe_lock);\n#if IS_ENABLED(CONFIG_DRM_DEBUG_DP_MST_TOPOLOGY_REFS)\n\tmutex_destroy(&mgr->topology_ref_history_lock);\n#endif\n}\nEXPORT_SYMBOL(drm_dp_mst_topology_mgr_destroy);\n\nstatic bool remote_i2c_read_ok(const struct i2c_msg msgs[], int num)\n{\n\tint i;\n\n\tif (num - 1 > DP_REMOTE_I2C_READ_MAX_TRANSACTIONS)\n\t\treturn false;\n\n\tfor (i = 0; i < num - 1; i++) {\n\t\tif (msgs[i].flags & I2C_M_RD ||\n\t\t    msgs[i].len > 0xff)\n\t\t\treturn false;\n\t}\n\n\treturn msgs[num - 1].flags & I2C_M_RD &&\n\t\tmsgs[num - 1].len <= 0xff;\n}\n\nstatic bool remote_i2c_write_ok(const struct i2c_msg msgs[], int num)\n{\n\tint i;\n\n\tfor (i = 0; i < num - 1; i++) {\n\t\tif (msgs[i].flags & I2C_M_RD || !(msgs[i].flags & I2C_M_STOP) ||\n\t\t    msgs[i].len > 0xff)\n\t\t\treturn false;\n\t}\n\n\treturn !(msgs[num - 1].flags & I2C_M_RD) && msgs[num - 1].len <= 0xff;\n}\n\nstatic int drm_dp_mst_i2c_read(struct drm_dp_mst_branch *mstb,\n\t\t\t       struct drm_dp_mst_port *port,\n\t\t\t       struct i2c_msg *msgs, int num)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\tunsigned int i;\n\tstruct drm_dp_sideband_msg_req_body msg;\n\tstruct drm_dp_sideband_msg_tx *txmsg = NULL;\n\tint ret;\n\n\tmemset(&msg, 0, sizeof(msg));\n\tmsg.req_type = DP_REMOTE_I2C_READ;\n\tmsg.u.i2c_read.num_transactions = num - 1;\n\tmsg.u.i2c_read.port_number = port->port_num;\n\tfor (i = 0; i < num - 1; i++) {\n\t\tmsg.u.i2c_read.transactions[i].i2c_dev_id = msgs[i].addr;\n\t\tmsg.u.i2c_read.transactions[i].num_bytes = msgs[i].len;\n\t\tmsg.u.i2c_read.transactions[i].bytes = msgs[i].buf;\n\t\tmsg.u.i2c_read.transactions[i].no_stop_bit = !(msgs[i].flags & I2C_M_STOP);\n\t}\n\tmsg.u.i2c_read.read_i2c_device_id = msgs[num - 1].addr;\n\tmsg.u.i2c_read.num_bytes_read = msgs[num - 1].len;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\ttxmsg->dst = mstb;\n\tdrm_dp_encode_sideband_req(&msg, txmsg);\n\n\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\tif (ret > 0) {\n\n\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\t\tret = -EREMOTEIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (txmsg->reply.u.remote_i2c_read_ack.num_bytes != msgs[num - 1].len) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tmemcpy(msgs[num - 1].buf, txmsg->reply.u.remote_i2c_read_ack.bytes, msgs[num - 1].len);\n\t\tret = num;\n\t}\nout:\n\tkfree(txmsg);\n\treturn ret;\n}\n\nstatic int drm_dp_mst_i2c_write(struct drm_dp_mst_branch *mstb,\n\t\t\t\tstruct drm_dp_mst_port *port,\n\t\t\t\tstruct i2c_msg *msgs, int num)\n{\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\tunsigned int i;\n\tstruct drm_dp_sideband_msg_req_body msg;\n\tstruct drm_dp_sideband_msg_tx *txmsg = NULL;\n\tint ret;\n\n\ttxmsg = kzalloc(sizeof(*txmsg), GFP_KERNEL);\n\tif (!txmsg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tfor (i = 0; i < num; i++) {\n\t\tmemset(&msg, 0, sizeof(msg));\n\t\tmsg.req_type = DP_REMOTE_I2C_WRITE;\n\t\tmsg.u.i2c_write.port_number = port->port_num;\n\t\tmsg.u.i2c_write.write_i2c_device_id = msgs[i].addr;\n\t\tmsg.u.i2c_write.num_bytes = msgs[i].len;\n\t\tmsg.u.i2c_write.bytes = msgs[i].buf;\n\n\t\tmemset(txmsg, 0, sizeof(*txmsg));\n\t\ttxmsg->dst = mstb;\n\n\t\tdrm_dp_encode_sideband_req(&msg, txmsg);\n\t\tdrm_dp_queue_down_tx(mgr, txmsg);\n\n\t\tret = drm_dp_mst_wait_tx_reply(mstb, txmsg);\n\t\tif (ret > 0) {\n\t\t\tif (txmsg->reply.reply_type == DP_SIDEBAND_REPLY_NAK) {\n\t\t\t\tret = -EREMOTEIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\tgoto out;\n\t\t}\n\t}\n\tret = num;\nout:\n\tkfree(txmsg);\n\treturn ret;\n}\n\n \nstatic int drm_dp_mst_i2c_xfer(struct i2c_adapter *adapter,\n\t\t\t       struct i2c_msg *msgs, int num)\n{\n\tstruct drm_dp_aux *aux = adapter->algo_data;\n\tstruct drm_dp_mst_port *port =\n\t\tcontainer_of(aux, struct drm_dp_mst_port, aux);\n\tstruct drm_dp_mst_branch *mstb;\n\tstruct drm_dp_mst_topology_mgr *mgr = port->mgr;\n\tint ret;\n\n\tmstb = drm_dp_mst_topology_get_mstb_validated(mgr, port->parent);\n\tif (!mstb)\n\t\treturn -EREMOTEIO;\n\n\tif (remote_i2c_read_ok(msgs, num)) {\n\t\tret = drm_dp_mst_i2c_read(mstb, port, msgs, num);\n\t} else if (remote_i2c_write_ok(msgs, num)) {\n\t\tret = drm_dp_mst_i2c_write(mstb, port, msgs, num);\n\t} else {\n\t\tdrm_dbg_kms(mgr->dev, \"Unsupported I2C transaction for MST device\\n\");\n\t\tret = -EIO;\n\t}\n\n\tdrm_dp_mst_topology_put_mstb(mstb);\n\treturn ret;\n}\n\nstatic u32 drm_dp_mst_i2c_functionality(struct i2c_adapter *adapter)\n{\n\treturn I2C_FUNC_I2C | I2C_FUNC_SMBUS_EMUL |\n\t       I2C_FUNC_SMBUS_READ_BLOCK_DATA |\n\t       I2C_FUNC_SMBUS_BLOCK_PROC_CALL |\n\t       I2C_FUNC_10BIT_ADDR;\n}\n\nstatic const struct i2c_algorithm drm_dp_mst_i2c_algo = {\n\t.functionality = drm_dp_mst_i2c_functionality,\n\t.master_xfer = drm_dp_mst_i2c_xfer,\n};\n\n \nstatic int drm_dp_mst_register_i2c_bus(struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_aux *aux = &port->aux;\n\tstruct device *parent_dev = port->mgr->dev->dev;\n\n\taux->ddc.algo = &drm_dp_mst_i2c_algo;\n\taux->ddc.algo_data = aux;\n\taux->ddc.retries = 3;\n\n\taux->ddc.class = I2C_CLASS_DDC;\n\taux->ddc.owner = THIS_MODULE;\n\t \n\taux->ddc.dev.parent = parent_dev;\n\taux->ddc.dev.of_node = parent_dev->of_node;\n\n\tstrscpy(aux->ddc.name, aux->name ? aux->name : dev_name(parent_dev),\n\t\tsizeof(aux->ddc.name));\n\n\treturn i2c_add_adapter(&aux->ddc);\n}\n\n \nstatic void drm_dp_mst_unregister_i2c_bus(struct drm_dp_mst_port *port)\n{\n\ti2c_del_adapter(&port->aux.ddc);\n}\n\n \nstatic bool drm_dp_mst_is_virtual_dpcd(struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_port *downstream_port;\n\n\tif (!port || port->dpcd_rev < DP_DPCD_REV_14)\n\t\treturn false;\n\n\t \n\tif (port->port_num >= 8)\n\t\treturn true;\n\n\t \n\tif (port->pdt == DP_PEER_DEVICE_DP_LEGACY_CONV &&\n\t    !port->mcs &&\n\t    port->ldps)\n\t\treturn true;\n\n\t \n\tmutex_lock(&port->mgr->lock);\n\tif (port->pdt == DP_PEER_DEVICE_MST_BRANCHING &&\n\t    port->mstb &&\n\t    port->mstb->num_ports == 2) {\n\t\tlist_for_each_entry(downstream_port, &port->mstb->ports, next) {\n\t\t\tif (downstream_port->pdt == DP_PEER_DEVICE_SST_SINK &&\n\t\t\t    !downstream_port->input) {\n\t\t\t\tmutex_unlock(&port->mgr->lock);\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&port->mgr->lock);\n\n\treturn false;\n}\n\n \nstruct drm_dp_aux *drm_dp_mst_dsc_aux_for_port(struct drm_dp_mst_port *port)\n{\n\tstruct drm_dp_mst_port *immediate_upstream_port;\n\tstruct drm_dp_mst_port *fec_port;\n\tstruct drm_dp_desc desc = {};\n\tu8 endpoint_fec;\n\tu8 endpoint_dsc;\n\n\tif (!port)\n\t\treturn NULL;\n\n\tif (port->parent->port_parent)\n\t\timmediate_upstream_port = port->parent->port_parent;\n\telse\n\t\timmediate_upstream_port = NULL;\n\n\tfec_port = immediate_upstream_port;\n\twhile (fec_port) {\n\t\t \n\t\tif (!drm_dp_mst_is_virtual_dpcd(fec_port) &&\n\t\t    !fec_port->fec_capable)\n\t\t\treturn NULL;\n\n\t\tfec_port = fec_port->parent->port_parent;\n\t}\n\n\t \n\tif (drm_dp_mst_is_virtual_dpcd(immediate_upstream_port)) {\n\t\tu8 upstream_dsc;\n\n\t\tif (drm_dp_dpcd_read(&port->aux,\n\t\t\t\t     DP_DSC_SUPPORT, &endpoint_dsc, 1) != 1)\n\t\t\treturn NULL;\n\t\tif (drm_dp_dpcd_read(&port->aux,\n\t\t\t\t     DP_FEC_CAPABILITY, &endpoint_fec, 1) != 1)\n\t\t\treturn NULL;\n\t\tif (drm_dp_dpcd_read(&immediate_upstream_port->aux,\n\t\t\t\t     DP_DSC_SUPPORT, &upstream_dsc, 1) != 1)\n\t\t\treturn NULL;\n\n\t\t \n\t\tif ((endpoint_dsc & DP_DSC_DECOMPRESSION_IS_SUPPORTED) &&\n\t\t    (endpoint_fec & DP_FEC_CAPABLE) &&\n\t\t    (upstream_dsc & DP_DSC_PASSTHROUGH_IS_SUPPORTED)) {\n\t\t\tport->passthrough_aux = &immediate_upstream_port->aux;\n\t\t\treturn &port->aux;\n\t\t}\n\n\t\t \n\t\treturn &immediate_upstream_port->aux;\n\t}\n\n\t \n\tif (drm_dp_mst_is_virtual_dpcd(port))\n\t\treturn &port->aux;\n\n\t \n\tif (drm_dp_read_desc(port->mgr->aux, &desc, true))\n\t\treturn NULL;\n\n\tif (drm_dp_has_quirk(&desc, DP_DPCD_QUIRK_DSC_WITHOUT_VIRTUAL_DPCD) &&\n\t    port->mgr->dpcd[DP_DPCD_REV] >= DP_DPCD_REV_14 &&\n\t    port->parent == port->mgr->mst_primary) {\n\t\tu8 dpcd_ext[DP_RECEIVER_CAP_SIZE];\n\n\t\tif (drm_dp_read_dpcd_caps(port->mgr->aux, dpcd_ext) < 0)\n\t\t\treturn NULL;\n\n\t\tif ((dpcd_ext[DP_DOWNSTREAMPORT_PRESENT] & DP_DWN_STRM_PORT_PRESENT) &&\n\t\t    ((dpcd_ext[DP_DOWNSTREAMPORT_PRESENT] & DP_DWN_STRM_PORT_TYPE_MASK)\n\t\t     != DP_DWN_STRM_PORT_TYPE_ANALOG))\n\t\t\treturn port->mgr->aux;\n\t}\n\n\t \n\tif (drm_dp_dpcd_read(&port->aux,\n\t   DP_DSC_SUPPORT, &endpoint_dsc, 1) != 1)\n\t\treturn NULL;\n\tif (drm_dp_dpcd_read(&port->aux,\n\t   DP_FEC_CAPABILITY, &endpoint_fec, 1) != 1)\n\t\treturn NULL;\n\tif ((endpoint_dsc & DP_DSC_DECOMPRESSION_IS_SUPPORTED) &&\n\t   (endpoint_fec & DP_FEC_CAPABLE))\n\t\treturn &port->aux;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(drm_dp_mst_dsc_aux_for_port);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}