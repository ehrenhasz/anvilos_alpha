{
  "module_name": "drm_gpuva_mgr.c",
  "hash_id": "7d1b773a3d4eb190f003ff73ac91b933dbed20b52d9f6dab0f439515005034a5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/drm_gpuva_mgr.c",
  "human_readable_source": "\n \n\n#include <drm/drm_gpuva_mgr.h>\n\n#include <linux/interval_tree_generic.h>\n#include <linux/mm.h>\n\n \n\n \n\n \n\n \n\n#define to_drm_gpuva(__node)\tcontainer_of((__node), struct drm_gpuva, rb.node)\n\n#define GPUVA_START(node) ((node)->va.addr)\n#define GPUVA_LAST(node) ((node)->va.addr + (node)->va.range - 1)\n\n \nINTERVAL_TREE_DEFINE(struct drm_gpuva, rb.node, u64, rb.__subtree_last,\n\t\t     GPUVA_START, GPUVA_LAST, static __maybe_unused,\n\t\t     drm_gpuva_it)\n\nstatic int __drm_gpuva_insert(struct drm_gpuva_manager *mgr,\n\t\t\t      struct drm_gpuva *va);\nstatic void __drm_gpuva_remove(struct drm_gpuva *va);\n\nstatic bool\ndrm_gpuva_check_overflow(u64 addr, u64 range)\n{\n\tu64 end;\n\n\treturn WARN(check_add_overflow(addr, range, &end),\n\t\t    \"GPUVA address limited to %zu bytes.\\n\", sizeof(end));\n}\n\nstatic bool\ndrm_gpuva_in_mm_range(struct drm_gpuva_manager *mgr, u64 addr, u64 range)\n{\n\tu64 end = addr + range;\n\tu64 mm_start = mgr->mm_start;\n\tu64 mm_end = mm_start + mgr->mm_range;\n\n\treturn addr >= mm_start && end <= mm_end;\n}\n\nstatic bool\ndrm_gpuva_in_kernel_node(struct drm_gpuva_manager *mgr, u64 addr, u64 range)\n{\n\tu64 end = addr + range;\n\tu64 kstart = mgr->kernel_alloc_node.va.addr;\n\tu64 krange = mgr->kernel_alloc_node.va.range;\n\tu64 kend = kstart + krange;\n\n\treturn krange && addr < kend && kstart < end;\n}\n\nstatic bool\ndrm_gpuva_range_valid(struct drm_gpuva_manager *mgr,\n\t\t      u64 addr, u64 range)\n{\n\treturn !drm_gpuva_check_overflow(addr, range) &&\n\t       drm_gpuva_in_mm_range(mgr, addr, range) &&\n\t       !drm_gpuva_in_kernel_node(mgr, addr, range);\n}\n\n \nvoid\ndrm_gpuva_manager_init(struct drm_gpuva_manager *mgr,\n\t\t       const char *name,\n\t\t       u64 start_offset, u64 range,\n\t\t       u64 reserve_offset, u64 reserve_range,\n\t\t       const struct drm_gpuva_fn_ops *ops)\n{\n\tmgr->rb.tree = RB_ROOT_CACHED;\n\tINIT_LIST_HEAD(&mgr->rb.list);\n\n\tdrm_gpuva_check_overflow(start_offset, range);\n\tmgr->mm_start = start_offset;\n\tmgr->mm_range = range;\n\n\tmgr->name = name ? name : \"unknown\";\n\tmgr->ops = ops;\n\n\tmemset(&mgr->kernel_alloc_node, 0, sizeof(struct drm_gpuva));\n\n\tif (reserve_range) {\n\t\tmgr->kernel_alloc_node.va.addr = reserve_offset;\n\t\tmgr->kernel_alloc_node.va.range = reserve_range;\n\n\t\tif (likely(!drm_gpuva_check_overflow(reserve_offset,\n\t\t\t\t\t\t     reserve_range)))\n\t\t\t__drm_gpuva_insert(mgr, &mgr->kernel_alloc_node);\n\t}\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_manager_init);\n\n \nvoid\ndrm_gpuva_manager_destroy(struct drm_gpuva_manager *mgr)\n{\n\tmgr->name = NULL;\n\n\tif (mgr->kernel_alloc_node.va.range)\n\t\t__drm_gpuva_remove(&mgr->kernel_alloc_node);\n\n\tWARN(!RB_EMPTY_ROOT(&mgr->rb.tree.rb_root),\n\t     \"GPUVA tree is not empty, potentially leaking memory.\");\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_manager_destroy);\n\nstatic int\n__drm_gpuva_insert(struct drm_gpuva_manager *mgr,\n\t\t   struct drm_gpuva *va)\n{\n\tstruct rb_node *node;\n\tstruct list_head *head;\n\n\tif (drm_gpuva_it_iter_first(&mgr->rb.tree,\n\t\t\t\t    GPUVA_START(va),\n\t\t\t\t    GPUVA_LAST(va)))\n\t\treturn -EEXIST;\n\n\tva->mgr = mgr;\n\n\tdrm_gpuva_it_insert(va, &mgr->rb.tree);\n\n\tnode = rb_prev(&va->rb.node);\n\tif (node)\n\t\thead = &(to_drm_gpuva(node))->rb.entry;\n\telse\n\t\thead = &mgr->rb.list;\n\n\tlist_add(&va->rb.entry, head);\n\n\treturn 0;\n}\n\n \nint\ndrm_gpuva_insert(struct drm_gpuva_manager *mgr,\n\t\t struct drm_gpuva *va)\n{\n\tu64 addr = va->va.addr;\n\tu64 range = va->va.range;\n\n\tif (unlikely(!drm_gpuva_range_valid(mgr, addr, range)))\n\t\treturn -EINVAL;\n\n\treturn __drm_gpuva_insert(mgr, va);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_insert);\n\nstatic void\n__drm_gpuva_remove(struct drm_gpuva *va)\n{\n\tdrm_gpuva_it_remove(va, &va->mgr->rb.tree);\n\tlist_del_init(&va->rb.entry);\n}\n\n \nvoid\ndrm_gpuva_remove(struct drm_gpuva *va)\n{\n\tstruct drm_gpuva_manager *mgr = va->mgr;\n\n\tif (unlikely(va == &mgr->kernel_alloc_node)) {\n\t\tWARN(1, \"Can't destroy kernel reserved node.\\n\");\n\t\treturn;\n\t}\n\n\t__drm_gpuva_remove(va);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_remove);\n\n \nvoid\ndrm_gpuva_link(struct drm_gpuva *va)\n{\n\tstruct drm_gem_object *obj = va->gem.obj;\n\n\tif (unlikely(!obj))\n\t\treturn;\n\n\tdrm_gem_gpuva_assert_lock_held(obj);\n\n\tlist_add_tail(&va->gem.entry, &obj->gpuva.list);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_link);\n\n \nvoid\ndrm_gpuva_unlink(struct drm_gpuva *va)\n{\n\tstruct drm_gem_object *obj = va->gem.obj;\n\n\tif (unlikely(!obj))\n\t\treturn;\n\n\tdrm_gem_gpuva_assert_lock_held(obj);\n\n\tlist_del_init(&va->gem.entry);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_unlink);\n\n \nstruct drm_gpuva *\ndrm_gpuva_find_first(struct drm_gpuva_manager *mgr,\n\t\t     u64 addr, u64 range)\n{\n\tu64 last = addr + range - 1;\n\n\treturn drm_gpuva_it_iter_first(&mgr->rb.tree, addr, last);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_find_first);\n\n \nstruct drm_gpuva *\ndrm_gpuva_find(struct drm_gpuva_manager *mgr,\n\t       u64 addr, u64 range)\n{\n\tstruct drm_gpuva *va;\n\n\tva = drm_gpuva_find_first(mgr, addr, range);\n\tif (!va)\n\t\tgoto out;\n\n\tif (va->va.addr != addr ||\n\t    va->va.range != range)\n\t\tgoto out;\n\n\treturn va;\n\nout:\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_find);\n\n \nstruct drm_gpuva *\ndrm_gpuva_find_prev(struct drm_gpuva_manager *mgr, u64 start)\n{\n\tif (!drm_gpuva_range_valid(mgr, start - 1, 1))\n\t\treturn NULL;\n\n\treturn drm_gpuva_it_iter_first(&mgr->rb.tree, start - 1, start);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_find_prev);\n\n \nstruct drm_gpuva *\ndrm_gpuva_find_next(struct drm_gpuva_manager *mgr, u64 end)\n{\n\tif (!drm_gpuva_range_valid(mgr, end, 1))\n\t\treturn NULL;\n\n\treturn drm_gpuva_it_iter_first(&mgr->rb.tree, end, end + 1);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_find_next);\n\n \nbool\ndrm_gpuva_interval_empty(struct drm_gpuva_manager *mgr, u64 addr, u64 range)\n{\n\treturn !drm_gpuva_find_first(mgr, addr, range);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_interval_empty);\n\n \nvoid\ndrm_gpuva_map(struct drm_gpuva_manager *mgr,\n\t      struct drm_gpuva *va,\n\t      struct drm_gpuva_op_map *op)\n{\n\tdrm_gpuva_init_from_op(va, op);\n\tdrm_gpuva_insert(mgr, va);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_map);\n\n \nvoid\ndrm_gpuva_remap(struct drm_gpuva *prev,\n\t\tstruct drm_gpuva *next,\n\t\tstruct drm_gpuva_op_remap *op)\n{\n\tstruct drm_gpuva *curr = op->unmap->va;\n\tstruct drm_gpuva_manager *mgr = curr->mgr;\n\n\tdrm_gpuva_remove(curr);\n\n\tif (op->prev) {\n\t\tdrm_gpuva_init_from_op(prev, op->prev);\n\t\tdrm_gpuva_insert(mgr, prev);\n\t}\n\n\tif (op->next) {\n\t\tdrm_gpuva_init_from_op(next, op->next);\n\t\tdrm_gpuva_insert(mgr, next);\n\t}\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_remap);\n\n \nvoid\ndrm_gpuva_unmap(struct drm_gpuva_op_unmap *op)\n{\n\tdrm_gpuva_remove(op->va);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_unmap);\n\nstatic int\nop_map_cb(const struct drm_gpuva_fn_ops *fn, void *priv,\n\t  u64 addr, u64 range,\n\t  struct drm_gem_object *obj, u64 offset)\n{\n\tstruct drm_gpuva_op op = {};\n\n\top.op = DRM_GPUVA_OP_MAP;\n\top.map.va.addr = addr;\n\top.map.va.range = range;\n\top.map.gem.obj = obj;\n\top.map.gem.offset = offset;\n\n\treturn fn->sm_step_map(&op, priv);\n}\n\nstatic int\nop_remap_cb(const struct drm_gpuva_fn_ops *fn, void *priv,\n\t    struct drm_gpuva_op_map *prev,\n\t    struct drm_gpuva_op_map *next,\n\t    struct drm_gpuva_op_unmap *unmap)\n{\n\tstruct drm_gpuva_op op = {};\n\tstruct drm_gpuva_op_remap *r;\n\n\top.op = DRM_GPUVA_OP_REMAP;\n\tr = &op.remap;\n\tr->prev = prev;\n\tr->next = next;\n\tr->unmap = unmap;\n\n\treturn fn->sm_step_remap(&op, priv);\n}\n\nstatic int\nop_unmap_cb(const struct drm_gpuva_fn_ops *fn, void *priv,\n\t    struct drm_gpuva *va, bool merge)\n{\n\tstruct drm_gpuva_op op = {};\n\n\top.op = DRM_GPUVA_OP_UNMAP;\n\top.unmap.va = va;\n\top.unmap.keep = merge;\n\n\treturn fn->sm_step_unmap(&op, priv);\n}\n\nstatic int\n__drm_gpuva_sm_map(struct drm_gpuva_manager *mgr,\n\t\t   const struct drm_gpuva_fn_ops *ops, void *priv,\n\t\t   u64 req_addr, u64 req_range,\n\t\t   struct drm_gem_object *req_obj, u64 req_offset)\n{\n\tstruct drm_gpuva *va, *next;\n\tu64 req_end = req_addr + req_range;\n\tint ret;\n\n\tif (unlikely(!drm_gpuva_range_valid(mgr, req_addr, req_range)))\n\t\treturn -EINVAL;\n\n\tdrm_gpuva_for_each_va_range_safe(va, next, mgr, req_addr, req_end) {\n\t\tstruct drm_gem_object *obj = va->gem.obj;\n\t\tu64 offset = va->gem.offset;\n\t\tu64 addr = va->va.addr;\n\t\tu64 range = va->va.range;\n\t\tu64 end = addr + range;\n\t\tbool merge = !!va->gem.obj;\n\n\t\tif (addr == req_addr) {\n\t\t\tmerge &= obj == req_obj &&\n\t\t\t\t offset == req_offset;\n\n\t\t\tif (end == req_end) {\n\t\t\t\tret = op_unmap_cb(ops, priv, va, merge);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (end < req_end) {\n\t\t\t\tret = op_unmap_cb(ops, priv, va, merge);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (end > req_end) {\n\t\t\t\tstruct drm_gpuva_op_map n = {\n\t\t\t\t\t.va.addr = req_end,\n\t\t\t\t\t.va.range = range - req_range,\n\t\t\t\t\t.gem.obj = obj,\n\t\t\t\t\t.gem.offset = offset + req_range,\n\t\t\t\t};\n\t\t\t\tstruct drm_gpuva_op_unmap u = {\n\t\t\t\t\t.va = va,\n\t\t\t\t\t.keep = merge,\n\t\t\t\t};\n\n\t\t\t\tret = op_remap_cb(ops, priv, NULL, &n, &u);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (addr < req_addr) {\n\t\t\tu64 ls_range = req_addr - addr;\n\t\t\tstruct drm_gpuva_op_map p = {\n\t\t\t\t.va.addr = addr,\n\t\t\t\t.va.range = ls_range,\n\t\t\t\t.gem.obj = obj,\n\t\t\t\t.gem.offset = offset,\n\t\t\t};\n\t\t\tstruct drm_gpuva_op_unmap u = { .va = va };\n\n\t\t\tmerge &= obj == req_obj &&\n\t\t\t\t offset + ls_range == req_offset;\n\t\t\tu.keep = merge;\n\n\t\t\tif (end == req_end) {\n\t\t\t\tret = op_remap_cb(ops, priv, &p, NULL, &u);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (end < req_end) {\n\t\t\t\tret = op_remap_cb(ops, priv, &p, NULL, &u);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (end > req_end) {\n\t\t\t\tstruct drm_gpuva_op_map n = {\n\t\t\t\t\t.va.addr = req_end,\n\t\t\t\t\t.va.range = end - req_end,\n\t\t\t\t\t.gem.obj = obj,\n\t\t\t\t\t.gem.offset = offset + ls_range +\n\t\t\t\t\t\t      req_range,\n\t\t\t\t};\n\n\t\t\t\tret = op_remap_cb(ops, priv, &p, &n, &u);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (addr > req_addr) {\n\t\t\tmerge &= obj == req_obj &&\n\t\t\t\t offset == req_offset +\n\t\t\t\t\t   (addr - req_addr);\n\n\t\t\tif (end == req_end) {\n\t\t\t\tret = op_unmap_cb(ops, priv, va, merge);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (end < req_end) {\n\t\t\t\tret = op_unmap_cb(ops, priv, va, merge);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (end > req_end) {\n\t\t\t\tstruct drm_gpuva_op_map n = {\n\t\t\t\t\t.va.addr = req_end,\n\t\t\t\t\t.va.range = end - req_end,\n\t\t\t\t\t.gem.obj = obj,\n\t\t\t\t\t.gem.offset = offset + req_end - addr,\n\t\t\t\t};\n\t\t\t\tstruct drm_gpuva_op_unmap u = {\n\t\t\t\t\t.va = va,\n\t\t\t\t\t.keep = merge,\n\t\t\t\t};\n\n\t\t\t\tret = op_remap_cb(ops, priv, NULL, &n, &u);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn op_map_cb(ops, priv,\n\t\t\t req_addr, req_range,\n\t\t\t req_obj, req_offset);\n}\n\nstatic int\n__drm_gpuva_sm_unmap(struct drm_gpuva_manager *mgr,\n\t\t     const struct drm_gpuva_fn_ops *ops, void *priv,\n\t\t     u64 req_addr, u64 req_range)\n{\n\tstruct drm_gpuva *va, *next;\n\tu64 req_end = req_addr + req_range;\n\tint ret;\n\n\tif (unlikely(!drm_gpuva_range_valid(mgr, req_addr, req_range)))\n\t\treturn -EINVAL;\n\n\tdrm_gpuva_for_each_va_range_safe(va, next, mgr, req_addr, req_end) {\n\t\tstruct drm_gpuva_op_map prev = {}, next = {};\n\t\tbool prev_split = false, next_split = false;\n\t\tstruct drm_gem_object *obj = va->gem.obj;\n\t\tu64 offset = va->gem.offset;\n\t\tu64 addr = va->va.addr;\n\t\tu64 range = va->va.range;\n\t\tu64 end = addr + range;\n\n\t\tif (addr < req_addr) {\n\t\t\tprev.va.addr = addr;\n\t\t\tprev.va.range = req_addr - addr;\n\t\t\tprev.gem.obj = obj;\n\t\t\tprev.gem.offset = offset;\n\n\t\t\tprev_split = true;\n\t\t}\n\n\t\tif (end > req_end) {\n\t\t\tnext.va.addr = req_end;\n\t\t\tnext.va.range = end - req_end;\n\t\t\tnext.gem.obj = obj;\n\t\t\tnext.gem.offset = offset + (req_end - addr);\n\n\t\t\tnext_split = true;\n\t\t}\n\n\t\tif (prev_split || next_split) {\n\t\t\tstruct drm_gpuva_op_unmap unmap = { .va = va };\n\n\t\t\tret = op_remap_cb(ops, priv,\n\t\t\t\t\t  prev_split ? &prev : NULL,\n\t\t\t\t\t  next_split ? &next : NULL,\n\t\t\t\t\t  &unmap);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else {\n\t\t\tret = op_unmap_cb(ops, priv, va, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint\ndrm_gpuva_sm_map(struct drm_gpuva_manager *mgr, void *priv,\n\t\t u64 req_addr, u64 req_range,\n\t\t struct drm_gem_object *req_obj, u64 req_offset)\n{\n\tconst struct drm_gpuva_fn_ops *ops = mgr->ops;\n\n\tif (unlikely(!(ops && ops->sm_step_map &&\n\t\t       ops->sm_step_remap &&\n\t\t       ops->sm_step_unmap)))\n\t\treturn -EINVAL;\n\n\treturn __drm_gpuva_sm_map(mgr, ops, priv,\n\t\t\t\t  req_addr, req_range,\n\t\t\t\t  req_obj, req_offset);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_sm_map);\n\n \nint\ndrm_gpuva_sm_unmap(struct drm_gpuva_manager *mgr, void *priv,\n\t\t   u64 req_addr, u64 req_range)\n{\n\tconst struct drm_gpuva_fn_ops *ops = mgr->ops;\n\n\tif (unlikely(!(ops && ops->sm_step_remap &&\n\t\t       ops->sm_step_unmap)))\n\t\treturn -EINVAL;\n\n\treturn __drm_gpuva_sm_unmap(mgr, ops, priv,\n\t\t\t\t    req_addr, req_range);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_sm_unmap);\n\nstatic struct drm_gpuva_op *\ngpuva_op_alloc(struct drm_gpuva_manager *mgr)\n{\n\tconst struct drm_gpuva_fn_ops *fn = mgr->ops;\n\tstruct drm_gpuva_op *op;\n\n\tif (fn && fn->op_alloc)\n\t\top = fn->op_alloc();\n\telse\n\t\top = kzalloc(sizeof(*op), GFP_KERNEL);\n\n\tif (unlikely(!op))\n\t\treturn NULL;\n\n\treturn op;\n}\n\nstatic void\ngpuva_op_free(struct drm_gpuva_manager *mgr,\n\t      struct drm_gpuva_op *op)\n{\n\tconst struct drm_gpuva_fn_ops *fn = mgr->ops;\n\n\tif (fn && fn->op_free)\n\t\tfn->op_free(op);\n\telse\n\t\tkfree(op);\n}\n\nstatic int\ndrm_gpuva_sm_step(struct drm_gpuva_op *__op,\n\t\t  void *priv)\n{\n\tstruct {\n\t\tstruct drm_gpuva_manager *mgr;\n\t\tstruct drm_gpuva_ops *ops;\n\t} *args = priv;\n\tstruct drm_gpuva_manager *mgr = args->mgr;\n\tstruct drm_gpuva_ops *ops = args->ops;\n\tstruct drm_gpuva_op *op;\n\n\top = gpuva_op_alloc(mgr);\n\tif (unlikely(!op))\n\t\tgoto err;\n\n\tmemcpy(op, __op, sizeof(*op));\n\n\tif (op->op == DRM_GPUVA_OP_REMAP) {\n\t\tstruct drm_gpuva_op_remap *__r = &__op->remap;\n\t\tstruct drm_gpuva_op_remap *r = &op->remap;\n\n\t\tr->unmap = kmemdup(__r->unmap, sizeof(*r->unmap),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (unlikely(!r->unmap))\n\t\t\tgoto err_free_op;\n\n\t\tif (__r->prev) {\n\t\t\tr->prev = kmemdup(__r->prev, sizeof(*r->prev),\n\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (unlikely(!r->prev))\n\t\t\t\tgoto err_free_unmap;\n\t\t}\n\n\t\tif (__r->next) {\n\t\t\tr->next = kmemdup(__r->next, sizeof(*r->next),\n\t\t\t\t\t  GFP_KERNEL);\n\t\t\tif (unlikely(!r->next))\n\t\t\t\tgoto err_free_prev;\n\t\t}\n\t}\n\n\tlist_add_tail(&op->entry, &ops->list);\n\n\treturn 0;\n\nerr_free_unmap:\n\tkfree(op->remap.unmap);\nerr_free_prev:\n\tkfree(op->remap.prev);\nerr_free_op:\n\tgpuva_op_free(mgr, op);\nerr:\n\treturn -ENOMEM;\n}\n\nstatic const struct drm_gpuva_fn_ops gpuva_list_ops = {\n\t.sm_step_map = drm_gpuva_sm_step,\n\t.sm_step_remap = drm_gpuva_sm_step,\n\t.sm_step_unmap = drm_gpuva_sm_step,\n};\n\n \nstruct drm_gpuva_ops *\ndrm_gpuva_sm_map_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t    u64 req_addr, u64 req_range,\n\t\t\t    struct drm_gem_object *req_obj, u64 req_offset)\n{\n\tstruct drm_gpuva_ops *ops;\n\tstruct {\n\t\tstruct drm_gpuva_manager *mgr;\n\t\tstruct drm_gpuva_ops *ops;\n\t} args;\n\tint ret;\n\n\tops = kzalloc(sizeof(*ops), GFP_KERNEL);\n\tif (unlikely(!ops))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ops->list);\n\n\targs.mgr = mgr;\n\targs.ops = ops;\n\n\tret = __drm_gpuva_sm_map(mgr, &gpuva_list_ops, &args,\n\t\t\t\t req_addr, req_range,\n\t\t\t\t req_obj, req_offset);\n\tif (ret)\n\t\tgoto err_free_ops;\n\n\treturn ops;\n\nerr_free_ops:\n\tdrm_gpuva_ops_free(mgr, ops);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_sm_map_ops_create);\n\n \nstruct drm_gpuva_ops *\ndrm_gpuva_sm_unmap_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t      u64 req_addr, u64 req_range)\n{\n\tstruct drm_gpuva_ops *ops;\n\tstruct {\n\t\tstruct drm_gpuva_manager *mgr;\n\t\tstruct drm_gpuva_ops *ops;\n\t} args;\n\tint ret;\n\n\tops = kzalloc(sizeof(*ops), GFP_KERNEL);\n\tif (unlikely(!ops))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ops->list);\n\n\targs.mgr = mgr;\n\targs.ops = ops;\n\n\tret = __drm_gpuva_sm_unmap(mgr, &gpuva_list_ops, &args,\n\t\t\t\t   req_addr, req_range);\n\tif (ret)\n\t\tgoto err_free_ops;\n\n\treturn ops;\n\nerr_free_ops:\n\tdrm_gpuva_ops_free(mgr, ops);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_sm_unmap_ops_create);\n\n \nstruct drm_gpuva_ops *\ndrm_gpuva_prefetch_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t      u64 addr, u64 range)\n{\n\tstruct drm_gpuva_ops *ops;\n\tstruct drm_gpuva_op *op;\n\tstruct drm_gpuva *va;\n\tu64 end = addr + range;\n\tint ret;\n\n\tops = kzalloc(sizeof(*ops), GFP_KERNEL);\n\tif (!ops)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ops->list);\n\n\tdrm_gpuva_for_each_va_range(va, mgr, addr, end) {\n\t\top = gpuva_op_alloc(mgr);\n\t\tif (!op) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_ops;\n\t\t}\n\n\t\top->op = DRM_GPUVA_OP_PREFETCH;\n\t\top->prefetch.va = va;\n\t\tlist_add_tail(&op->entry, &ops->list);\n\t}\n\n\treturn ops;\n\nerr_free_ops:\n\tdrm_gpuva_ops_free(mgr, ops);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_prefetch_ops_create);\n\n \nstruct drm_gpuva_ops *\ndrm_gpuva_gem_unmap_ops_create(struct drm_gpuva_manager *mgr,\n\t\t\t       struct drm_gem_object *obj)\n{\n\tstruct drm_gpuva_ops *ops;\n\tstruct drm_gpuva_op *op;\n\tstruct drm_gpuva *va;\n\tint ret;\n\n\tdrm_gem_gpuva_assert_lock_held(obj);\n\n\tops = kzalloc(sizeof(*ops), GFP_KERNEL);\n\tif (!ops)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ops->list);\n\n\tdrm_gem_for_each_gpuva(va, obj) {\n\t\top = gpuva_op_alloc(mgr);\n\t\tif (!op) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_ops;\n\t\t}\n\n\t\top->op = DRM_GPUVA_OP_UNMAP;\n\t\top->unmap.va = va;\n\t\tlist_add_tail(&op->entry, &ops->list);\n\t}\n\n\treturn ops;\n\nerr_free_ops:\n\tdrm_gpuva_ops_free(mgr, ops);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_gem_unmap_ops_create);\n\n \nvoid\ndrm_gpuva_ops_free(struct drm_gpuva_manager *mgr,\n\t\t   struct drm_gpuva_ops *ops)\n{\n\tstruct drm_gpuva_op *op, *next;\n\n\tdrm_gpuva_for_each_op_safe(op, next, ops) {\n\t\tlist_del(&op->entry);\n\n\t\tif (op->op == DRM_GPUVA_OP_REMAP) {\n\t\t\tkfree(op->remap.prev);\n\t\t\tkfree(op->remap.next);\n\t\t\tkfree(op->remap.unmap);\n\t\t}\n\n\t\tgpuva_op_free(mgr, op);\n\t}\n\n\tkfree(ops);\n}\nEXPORT_SYMBOL_GPL(drm_gpuva_ops_free);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}