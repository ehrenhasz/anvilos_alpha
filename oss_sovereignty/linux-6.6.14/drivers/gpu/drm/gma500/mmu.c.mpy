{
  "module_name": "mmu.c",
  "hash_id": "09b53b2c1098656601e88481e7d9c87bb94f951072d13399475631204e9315a7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/gma500/mmu.c",
  "human_readable_source": "\n \n\n#include <linux/highmem.h>\n\n#include \"mmu.h\"\n#include \"psb_drv.h\"\n#include \"psb_reg.h\"\n\n \n\n \n\n \n\n \n\nstatic inline uint32_t psb_mmu_pt_index(uint32_t offset)\n{\n\treturn (offset >> PSB_PTE_SHIFT) & 0x3FF;\n}\n\nstatic inline uint32_t psb_mmu_pd_index(uint32_t offset)\n{\n\treturn offset >> PSB_PDE_SHIFT;\n}\n\nstatic inline void psb_clflush(void *addr)\n{\n\t__asm__ __volatile__(\"clflush (%0)\\n\" : : \"r\"(addr) : \"memory\");\n}\n\nstatic inline void psb_mmu_clflush(struct psb_mmu_driver *driver, void *addr)\n{\n\tif (!driver->has_clflush)\n\t\treturn;\n\n\tmb();\n\tpsb_clflush(addr);\n\tmb();\n}\n\nstatic void psb_mmu_flush_pd_locked(struct psb_mmu_driver *driver, int force)\n{\n\tstruct drm_device *dev = driver->dev;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\n\tif (atomic_read(&driver->needs_tlbflush) || force) {\n\t\tuint32_t val = PSB_RSGX32(PSB_CR_BIF_CTRL);\n\t\tPSB_WSGX32(val | _PSB_CB_CTRL_INVALDC, PSB_CR_BIF_CTRL);\n\n\t\t \n\t\twmb();\n\t\tPSB_WSGX32(val & ~_PSB_CB_CTRL_INVALDC, PSB_CR_BIF_CTRL);\n\t\t(void)PSB_RSGX32(PSB_CR_BIF_CTRL);\n\t\tif (driver->msvdx_mmu_invaldc)\n\t\t\tatomic_set(driver->msvdx_mmu_invaldc, 1);\n\t}\n\tatomic_set(&driver->needs_tlbflush, 0);\n}\n\n#if 0\nstatic void psb_mmu_flush_pd(struct psb_mmu_driver *driver, int force)\n{\n\tdown_write(&driver->sem);\n\tpsb_mmu_flush_pd_locked(driver, force);\n\tup_write(&driver->sem);\n}\n#endif\n\nvoid psb_mmu_flush(struct psb_mmu_driver *driver)\n{\n\tstruct drm_device *dev = driver->dev;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\tuint32_t val;\n\n\tdown_write(&driver->sem);\n\tval = PSB_RSGX32(PSB_CR_BIF_CTRL);\n\tif (atomic_read(&driver->needs_tlbflush))\n\t\tPSB_WSGX32(val | _PSB_CB_CTRL_INVALDC, PSB_CR_BIF_CTRL);\n\telse\n\t\tPSB_WSGX32(val | _PSB_CB_CTRL_FLUSH, PSB_CR_BIF_CTRL);\n\n\t \n\twmb();\n\tPSB_WSGX32(val & ~(_PSB_CB_CTRL_FLUSH | _PSB_CB_CTRL_INVALDC),\n\t\t   PSB_CR_BIF_CTRL);\n\t(void)PSB_RSGX32(PSB_CR_BIF_CTRL);\n\n\tatomic_set(&driver->needs_tlbflush, 0);\n\tif (driver->msvdx_mmu_invaldc)\n\t\tatomic_set(driver->msvdx_mmu_invaldc, 1);\n\tup_write(&driver->sem);\n}\n\nvoid psb_mmu_set_pd_context(struct psb_mmu_pd *pd, int hw_context)\n{\n\tstruct drm_device *dev = pd->driver->dev;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\tuint32_t offset = (hw_context == 0) ? PSB_CR_BIF_DIR_LIST_BASE0 :\n\t\t\t  PSB_CR_BIF_DIR_LIST_BASE1 + hw_context * 4;\n\n\tdown_write(&pd->driver->sem);\n\tPSB_WSGX32(page_to_pfn(pd->p) << PAGE_SHIFT, offset);\n\twmb();\n\tpsb_mmu_flush_pd_locked(pd->driver, 1);\n\tpd->hw_context = hw_context;\n\tup_write(&pd->driver->sem);\n\n}\n\nstatic inline unsigned long psb_pd_addr_end(unsigned long addr,\n\t\t\t\t\t    unsigned long end)\n{\n\taddr = (addr + PSB_PDE_MASK + 1) & ~PSB_PDE_MASK;\n\treturn (addr < end) ? addr : end;\n}\n\nstatic inline uint32_t psb_mmu_mask_pte(uint32_t pfn, int type)\n{\n\tuint32_t mask = PSB_PTE_VALID;\n\n\tif (type & PSB_MMU_CACHED_MEMORY)\n\t\tmask |= PSB_PTE_CACHED;\n\tif (type & PSB_MMU_RO_MEMORY)\n\t\tmask |= PSB_PTE_RO;\n\tif (type & PSB_MMU_WO_MEMORY)\n\t\tmask |= PSB_PTE_WO;\n\n\treturn (pfn << PAGE_SHIFT) | mask;\n}\n\nstruct psb_mmu_pd *psb_mmu_alloc_pd(struct psb_mmu_driver *driver,\n\t\t\t\t    int trap_pagefaults, int invalid_type)\n{\n\tstruct psb_mmu_pd *pd = kmalloc(sizeof(*pd), GFP_KERNEL);\n\tuint32_t *v;\n\tint i;\n\n\tif (!pd)\n\t\treturn NULL;\n\n\tpd->p = alloc_page(GFP_DMA32);\n\tif (!pd->p)\n\t\tgoto out_err1;\n\tpd->dummy_pt = alloc_page(GFP_DMA32);\n\tif (!pd->dummy_pt)\n\t\tgoto out_err2;\n\tpd->dummy_page = alloc_page(GFP_DMA32);\n\tif (!pd->dummy_page)\n\t\tgoto out_err3;\n\n\tif (!trap_pagefaults) {\n\t\tpd->invalid_pde = psb_mmu_mask_pte(page_to_pfn(pd->dummy_pt),\n\t\t\t\t\t\t   invalid_type);\n\t\tpd->invalid_pte = psb_mmu_mask_pte(page_to_pfn(pd->dummy_page),\n\t\t\t\t\t\t   invalid_type);\n\t} else {\n\t\tpd->invalid_pde = 0;\n\t\tpd->invalid_pte = 0;\n\t}\n\n\tv = kmap_local_page(pd->dummy_pt);\n\tfor (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i)\n\t\tv[i] = pd->invalid_pte;\n\n\tkunmap_local(v);\n\n\tv = kmap_local_page(pd->p);\n\tfor (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i)\n\t\tv[i] = pd->invalid_pde;\n\n\tkunmap_local(v);\n\n\tclear_page(kmap(pd->dummy_page));\n\tkunmap(pd->dummy_page);\n\n\tpd->tables = vmalloc_user(sizeof(struct psb_mmu_pt *) * 1024);\n\tif (!pd->tables)\n\t\tgoto out_err4;\n\n\tpd->hw_context = -1;\n\tpd->pd_mask = PSB_PTE_VALID;\n\tpd->driver = driver;\n\n\treturn pd;\n\nout_err4:\n\t__free_page(pd->dummy_page);\nout_err3:\n\t__free_page(pd->dummy_pt);\nout_err2:\n\t__free_page(pd->p);\nout_err1:\n\tkfree(pd);\n\treturn NULL;\n}\n\nstatic void psb_mmu_free_pt(struct psb_mmu_pt *pt)\n{\n\t__free_page(pt->p);\n\tkfree(pt);\n}\n\nvoid psb_mmu_free_pagedir(struct psb_mmu_pd *pd)\n{\n\tstruct psb_mmu_driver *driver = pd->driver;\n\tstruct drm_device *dev = driver->dev;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\tstruct psb_mmu_pt *pt;\n\tint i;\n\n\tdown_write(&driver->sem);\n\tif (pd->hw_context != -1) {\n\t\tPSB_WSGX32(0, PSB_CR_BIF_DIR_LIST_BASE0 + pd->hw_context * 4);\n\t\tpsb_mmu_flush_pd_locked(driver, 1);\n\t}\n\n\t \n\n\tfor (i = 0; i < 1024; ++i) {\n\t\tpt = pd->tables[i];\n\t\tif (pt)\n\t\t\tpsb_mmu_free_pt(pt);\n\t}\n\n\tvfree(pd->tables);\n\t__free_page(pd->dummy_page);\n\t__free_page(pd->dummy_pt);\n\t__free_page(pd->p);\n\tkfree(pd);\n\tup_write(&driver->sem);\n}\n\nstatic struct psb_mmu_pt *psb_mmu_alloc_pt(struct psb_mmu_pd *pd)\n{\n\tstruct psb_mmu_pt *pt = kmalloc(sizeof(*pt), GFP_KERNEL);\n\tvoid *v;\n\tuint32_t clflush_add = pd->driver->clflush_add >> PAGE_SHIFT;\n\tuint32_t clflush_count = PAGE_SIZE / clflush_add;\n\tspinlock_t *lock = &pd->driver->lock;\n\tuint8_t *clf;\n\tuint32_t *ptes;\n\tint i;\n\n\tif (!pt)\n\t\treturn NULL;\n\n\tpt->p = alloc_page(GFP_DMA32);\n\tif (!pt->p) {\n\t\tkfree(pt);\n\t\treturn NULL;\n\t}\n\n\tspin_lock(lock);\n\n\tv = kmap_atomic(pt->p);\n\tclf = (uint8_t *) v;\n\tptes = (uint32_t *) v;\n\tfor (i = 0; i < (PAGE_SIZE / sizeof(uint32_t)); ++i)\n\t\t*ptes++ = pd->invalid_pte;\n\n\tif (pd->driver->has_clflush && pd->hw_context != -1) {\n\t\tmb();\n\t\tfor (i = 0; i < clflush_count; ++i) {\n\t\t\tpsb_clflush(clf);\n\t\t\tclf += clflush_add;\n\t\t}\n\t\tmb();\n\t}\n\tkunmap_atomic(v);\n\tspin_unlock(lock);\n\n\tpt->count = 0;\n\tpt->pd = pd;\n\tpt->index = 0;\n\n\treturn pt;\n}\n\nstatic struct psb_mmu_pt *psb_mmu_pt_alloc_map_lock(struct psb_mmu_pd *pd,\n\t\t\t\t\t\t    unsigned long addr)\n{\n\tuint32_t index = psb_mmu_pd_index(addr);\n\tstruct psb_mmu_pt *pt;\n\tuint32_t *v;\n\tspinlock_t *lock = &pd->driver->lock;\n\n\tspin_lock(lock);\n\tpt = pd->tables[index];\n\twhile (!pt) {\n\t\tspin_unlock(lock);\n\t\tpt = psb_mmu_alloc_pt(pd);\n\t\tif (!pt)\n\t\t\treturn NULL;\n\t\tspin_lock(lock);\n\n\t\tif (pd->tables[index]) {\n\t\t\tspin_unlock(lock);\n\t\t\tpsb_mmu_free_pt(pt);\n\t\t\tspin_lock(lock);\n\t\t\tpt = pd->tables[index];\n\t\t\tcontinue;\n\t\t}\n\n\t\tv = kmap_atomic(pd->p);\n\t\tpd->tables[index] = pt;\n\t\tv[index] = (page_to_pfn(pt->p) << 12) | pd->pd_mask;\n\t\tpt->index = index;\n\t\tkunmap_atomic((void *) v);\n\n\t\tif (pd->hw_context != -1) {\n\t\t\tpsb_mmu_clflush(pd->driver, (void *)&v[index]);\n\t\t\tatomic_set(&pd->driver->needs_tlbflush, 1);\n\t\t}\n\t}\n\tpt->v = kmap_atomic(pt->p);\n\treturn pt;\n}\n\nstatic struct psb_mmu_pt *psb_mmu_pt_map_lock(struct psb_mmu_pd *pd,\n\t\t\t\t\t      unsigned long addr)\n{\n\tuint32_t index = psb_mmu_pd_index(addr);\n\tstruct psb_mmu_pt *pt;\n\tspinlock_t *lock = &pd->driver->lock;\n\n\tspin_lock(lock);\n\tpt = pd->tables[index];\n\tif (!pt) {\n\t\tspin_unlock(lock);\n\t\treturn NULL;\n\t}\n\tpt->v = kmap_atomic(pt->p);\n\treturn pt;\n}\n\nstatic void psb_mmu_pt_unmap_unlock(struct psb_mmu_pt *pt)\n{\n\tstruct psb_mmu_pd *pd = pt->pd;\n\tuint32_t *v;\n\n\tkunmap_atomic(pt->v);\n\tif (pt->count == 0) {\n\t\tv = kmap_atomic(pd->p);\n\t\tv[pt->index] = pd->invalid_pde;\n\t\tpd->tables[pt->index] = NULL;\n\n\t\tif (pd->hw_context != -1) {\n\t\t\tpsb_mmu_clflush(pd->driver, (void *)&v[pt->index]);\n\t\t\tatomic_set(&pd->driver->needs_tlbflush, 1);\n\t\t}\n\t\tkunmap_atomic(v);\n\t\tspin_unlock(&pd->driver->lock);\n\t\tpsb_mmu_free_pt(pt);\n\t\treturn;\n\t}\n\tspin_unlock(&pd->driver->lock);\n}\n\nstatic inline void psb_mmu_set_pte(struct psb_mmu_pt *pt, unsigned long addr,\n\t\t\t\t   uint32_t pte)\n{\n\tpt->v[psb_mmu_pt_index(addr)] = pte;\n}\n\nstatic inline void psb_mmu_invalidate_pte(struct psb_mmu_pt *pt,\n\t\t\t\t\t  unsigned long addr)\n{\n\tpt->v[psb_mmu_pt_index(addr)] = pt->pd->invalid_pte;\n}\n\nstruct psb_mmu_pd *psb_mmu_get_default_pd(struct psb_mmu_driver *driver)\n{\n\tstruct psb_mmu_pd *pd;\n\n\tdown_read(&driver->sem);\n\tpd = driver->default_pd;\n\tup_read(&driver->sem);\n\n\treturn pd;\n}\n\nvoid psb_mmu_driver_takedown(struct psb_mmu_driver *driver)\n{\n\tstruct drm_device *dev = driver->dev;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\n\tPSB_WSGX32(driver->bif_ctrl, PSB_CR_BIF_CTRL);\n\tpsb_mmu_free_pagedir(driver->default_pd);\n\tkfree(driver);\n}\n\nstruct psb_mmu_driver *psb_mmu_driver_init(struct drm_device *dev,\n\t\t\t\t\t   int trap_pagefaults,\n\t\t\t\t\t   int invalid_type,\n\t\t\t\t\t   atomic_t *msvdx_mmu_invaldc)\n{\n\tstruct psb_mmu_driver *driver;\n\tstruct drm_psb_private *dev_priv = to_drm_psb_private(dev);\n\n\tdriver = kmalloc(sizeof(*driver), GFP_KERNEL);\n\n\tif (!driver)\n\t\treturn NULL;\n\n\tdriver->dev = dev;\n\tdriver->default_pd = psb_mmu_alloc_pd(driver, trap_pagefaults,\n\t\t\t\t\t      invalid_type);\n\tif (!driver->default_pd)\n\t\tgoto out_err1;\n\n\tspin_lock_init(&driver->lock);\n\tinit_rwsem(&driver->sem);\n\tdown_write(&driver->sem);\n\tatomic_set(&driver->needs_tlbflush, 1);\n\tdriver->msvdx_mmu_invaldc = msvdx_mmu_invaldc;\n\n\tdriver->bif_ctrl = PSB_RSGX32(PSB_CR_BIF_CTRL);\n\tPSB_WSGX32(driver->bif_ctrl | _PSB_CB_CTRL_CLEAR_FAULT,\n\t\t   PSB_CR_BIF_CTRL);\n\tPSB_WSGX32(driver->bif_ctrl & ~_PSB_CB_CTRL_CLEAR_FAULT,\n\t\t   PSB_CR_BIF_CTRL);\n\n\tdriver->has_clflush = 0;\n\n\tif (boot_cpu_has(X86_FEATURE_CLFLUSH)) {\n\t\tuint32_t tfms, misc, cap0, cap4, clflush_size;\n\n\t\t \n\n\t\tcpuid(0x00000001, &tfms, &misc, &cap0, &cap4);\n\t\tclflush_size = ((misc >> 8) & 0xff) * 8;\n\t\tdriver->has_clflush = 1;\n\t\tdriver->clflush_add =\n\t\t    PAGE_SIZE * clflush_size / sizeof(uint32_t);\n\t\tdriver->clflush_mask = driver->clflush_add - 1;\n\t\tdriver->clflush_mask = ~driver->clflush_mask;\n\t}\n\n\tup_write(&driver->sem);\n\treturn driver;\n\nout_err1:\n\tkfree(driver);\n\treturn NULL;\n}\n\nstatic void psb_mmu_flush_ptes(struct psb_mmu_pd *pd, unsigned long address,\n\t\t\t       uint32_t num_pages, uint32_t desired_tile_stride,\n\t\t\t       uint32_t hw_tile_stride)\n{\n\tstruct psb_mmu_pt *pt;\n\tuint32_t rows = 1;\n\tuint32_t i;\n\tunsigned long addr;\n\tunsigned long end;\n\tunsigned long next;\n\tunsigned long add;\n\tunsigned long row_add;\n\tunsigned long clflush_add = pd->driver->clflush_add;\n\tunsigned long clflush_mask = pd->driver->clflush_mask;\n\n\tif (!pd->driver->has_clflush)\n\t\treturn;\n\n\tif (hw_tile_stride)\n\t\trows = num_pages / desired_tile_stride;\n\telse\n\t\tdesired_tile_stride = num_pages;\n\n\tadd = desired_tile_stride << PAGE_SHIFT;\n\trow_add = hw_tile_stride << PAGE_SHIFT;\n\tmb();\n\tfor (i = 0; i < rows; ++i) {\n\n\t\taddr = address;\n\t\tend = addr + add;\n\n\t\tdo {\n\t\t\tnext = psb_pd_addr_end(addr, end);\n\t\t\tpt = psb_mmu_pt_map_lock(pd, addr);\n\t\t\tif (!pt)\n\t\t\t\tcontinue;\n\t\t\tdo {\n\t\t\t\tpsb_clflush(&pt->v[psb_mmu_pt_index(addr)]);\n\t\t\t} while (addr += clflush_add,\n\t\t\t\t (addr & clflush_mask) < next);\n\n\t\t\tpsb_mmu_pt_unmap_unlock(pt);\n\t\t} while (addr = next, next != end);\n\t\taddress += row_add;\n\t}\n\tmb();\n}\n\nvoid psb_mmu_remove_pfn_sequence(struct psb_mmu_pd *pd,\n\t\t\t\t unsigned long address, uint32_t num_pages)\n{\n\tstruct psb_mmu_pt *pt;\n\tunsigned long addr;\n\tunsigned long end;\n\tunsigned long next;\n\tunsigned long f_address = address;\n\n\tdown_read(&pd->driver->sem);\n\n\taddr = address;\n\tend = addr + (num_pages << PAGE_SHIFT);\n\n\tdo {\n\t\tnext = psb_pd_addr_end(addr, end);\n\t\tpt = psb_mmu_pt_alloc_map_lock(pd, addr);\n\t\tif (!pt)\n\t\t\tgoto out;\n\t\tdo {\n\t\t\tpsb_mmu_invalidate_pte(pt, addr);\n\t\t\t--pt->count;\n\t\t} while (addr += PAGE_SIZE, addr < next);\n\t\tpsb_mmu_pt_unmap_unlock(pt);\n\n\t} while (addr = next, next != end);\n\nout:\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush_ptes(pd, f_address, num_pages, 1, 1);\n\n\tup_read(&pd->driver->sem);\n\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush(pd->driver);\n\n\treturn;\n}\n\nvoid psb_mmu_remove_pages(struct psb_mmu_pd *pd, unsigned long address,\n\t\t\t  uint32_t num_pages, uint32_t desired_tile_stride,\n\t\t\t  uint32_t hw_tile_stride)\n{\n\tstruct psb_mmu_pt *pt;\n\tuint32_t rows = 1;\n\tuint32_t i;\n\tunsigned long addr;\n\tunsigned long end;\n\tunsigned long next;\n\tunsigned long add;\n\tunsigned long row_add;\n\tunsigned long f_address = address;\n\n\tif (hw_tile_stride)\n\t\trows = num_pages / desired_tile_stride;\n\telse\n\t\tdesired_tile_stride = num_pages;\n\n\tadd = desired_tile_stride << PAGE_SHIFT;\n\trow_add = hw_tile_stride << PAGE_SHIFT;\n\n\tdown_read(&pd->driver->sem);\n\n\t \n\n\tfor (i = 0; i < rows; ++i) {\n\n\t\taddr = address;\n\t\tend = addr + add;\n\n\t\tdo {\n\t\t\tnext = psb_pd_addr_end(addr, end);\n\t\t\tpt = psb_mmu_pt_map_lock(pd, addr);\n\t\t\tif (!pt)\n\t\t\t\tcontinue;\n\t\t\tdo {\n\t\t\t\tpsb_mmu_invalidate_pte(pt, addr);\n\t\t\t\t--pt->count;\n\n\t\t\t} while (addr += PAGE_SIZE, addr < next);\n\t\t\tpsb_mmu_pt_unmap_unlock(pt);\n\n\t\t} while (addr = next, next != end);\n\t\taddress += row_add;\n\t}\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush_ptes(pd, f_address, num_pages,\n\t\t\t\t   desired_tile_stride, hw_tile_stride);\n\n\tup_read(&pd->driver->sem);\n\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush(pd->driver);\n}\n\nint psb_mmu_insert_pfn_sequence(struct psb_mmu_pd *pd, uint32_t start_pfn,\n\t\t\t\tunsigned long address, uint32_t num_pages,\n\t\t\t\tint type)\n{\n\tstruct psb_mmu_pt *pt;\n\tuint32_t pte;\n\tunsigned long addr;\n\tunsigned long end;\n\tunsigned long next;\n\tunsigned long f_address = address;\n\tint ret = -ENOMEM;\n\n\tdown_read(&pd->driver->sem);\n\n\taddr = address;\n\tend = addr + (num_pages << PAGE_SHIFT);\n\n\tdo {\n\t\tnext = psb_pd_addr_end(addr, end);\n\t\tpt = psb_mmu_pt_alloc_map_lock(pd, addr);\n\t\tif (!pt) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tdo {\n\t\t\tpte = psb_mmu_mask_pte(start_pfn++, type);\n\t\t\tpsb_mmu_set_pte(pt, addr, pte);\n\t\t\tpt->count++;\n\t\t} while (addr += PAGE_SIZE, addr < next);\n\t\tpsb_mmu_pt_unmap_unlock(pt);\n\n\t} while (addr = next, next != end);\n\tret = 0;\n\nout:\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush_ptes(pd, f_address, num_pages, 1, 1);\n\n\tup_read(&pd->driver->sem);\n\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush(pd->driver);\n\n\treturn ret;\n}\n\nint psb_mmu_insert_pages(struct psb_mmu_pd *pd, struct page **pages,\n\t\t\t unsigned long address, uint32_t num_pages,\n\t\t\t uint32_t desired_tile_stride, uint32_t hw_tile_stride,\n\t\t\t int type)\n{\n\tstruct psb_mmu_pt *pt;\n\tuint32_t rows = 1;\n\tuint32_t i;\n\tuint32_t pte;\n\tunsigned long addr;\n\tunsigned long end;\n\tunsigned long next;\n\tunsigned long add;\n\tunsigned long row_add;\n\tunsigned long f_address = address;\n\tint ret = -ENOMEM;\n\n\tif (hw_tile_stride) {\n\t\tif (num_pages % desired_tile_stride != 0)\n\t\t\treturn -EINVAL;\n\t\trows = num_pages / desired_tile_stride;\n\t} else {\n\t\tdesired_tile_stride = num_pages;\n\t}\n\n\tadd = desired_tile_stride << PAGE_SHIFT;\n\trow_add = hw_tile_stride << PAGE_SHIFT;\n\n\tdown_read(&pd->driver->sem);\n\n\tfor (i = 0; i < rows; ++i) {\n\n\t\taddr = address;\n\t\tend = addr + add;\n\n\t\tdo {\n\t\t\tnext = psb_pd_addr_end(addr, end);\n\t\t\tpt = psb_mmu_pt_alloc_map_lock(pd, addr);\n\t\t\tif (!pt)\n\t\t\t\tgoto out;\n\t\t\tdo {\n\t\t\t\tpte = psb_mmu_mask_pte(page_to_pfn(*pages++),\n\t\t\t\t\t\t       type);\n\t\t\t\tpsb_mmu_set_pte(pt, addr, pte);\n\t\t\t\tpt->count++;\n\t\t\t} while (addr += PAGE_SIZE, addr < next);\n\t\t\tpsb_mmu_pt_unmap_unlock(pt);\n\n\t\t} while (addr = next, next != end);\n\n\t\taddress += row_add;\n\t}\n\n\tret = 0;\nout:\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush_ptes(pd, f_address, num_pages,\n\t\t\t\t   desired_tile_stride, hw_tile_stride);\n\n\tup_read(&pd->driver->sem);\n\n\tif (pd->hw_context != -1)\n\t\tpsb_mmu_flush(pd->driver);\n\n\treturn ret;\n}\n\nint psb_mmu_virtual_to_pfn(struct psb_mmu_pd *pd, uint32_t virtual,\n\t\t\t   unsigned long *pfn)\n{\n\tint ret;\n\tstruct psb_mmu_pt *pt;\n\tuint32_t tmp;\n\tspinlock_t *lock = &pd->driver->lock;\n\n\tdown_read(&pd->driver->sem);\n\tpt = psb_mmu_pt_map_lock(pd, virtual);\n\tif (!pt) {\n\t\tuint32_t *v;\n\n\t\tspin_lock(lock);\n\t\tv = kmap_atomic(pd->p);\n\t\ttmp = v[psb_mmu_pd_index(virtual)];\n\t\tkunmap_atomic(v);\n\t\tspin_unlock(lock);\n\n\t\tif (tmp != pd->invalid_pde || !(tmp & PSB_PTE_VALID) ||\n\t\t    !(pd->invalid_pte & PSB_PTE_VALID)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tret = 0;\n\t\t*pfn = pd->invalid_pte >> PAGE_SHIFT;\n\t\tgoto out;\n\t}\n\ttmp = pt->v[psb_mmu_pt_index(virtual)];\n\tif (!(tmp & PSB_PTE_VALID)) {\n\t\tret = -EINVAL;\n\t} else {\n\t\tret = 0;\n\t\t*pfn = tmp >> PAGE_SHIFT;\n\t}\n\tpsb_mmu_pt_unmap_unlock(pt);\nout:\n\tup_read(&pd->driver->sem);\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}