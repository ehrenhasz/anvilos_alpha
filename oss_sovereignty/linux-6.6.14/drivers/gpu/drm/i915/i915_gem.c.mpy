{
  "module_name": "i915_gem.c",
  "hash_id": "0b281452b13b5abb9ce4f8f7796c7a58c84d3f77b72c71de4545a3fdce1fac4e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/i915_gem.c",
  "human_readable_source": " \n\n#include <linux/dma-fence-array.h>\n#include <linux/kthread.h>\n#include <linux/dma-resv.h>\n#include <linux/shmem_fs.h>\n#include <linux/slab.h>\n#include <linux/stop_machine.h>\n#include <linux/swap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n#include <linux/mman.h>\n\n#include <drm/drm_cache.h>\n#include <drm/drm_vma_manager.h>\n\n#include \"display/intel_display.h\"\n#include \"display/intel_frontbuffer.h\"\n\n#include \"gem/i915_gem_clflush.h\"\n#include \"gem/i915_gem_context.h\"\n#include \"gem/i915_gem_ioctls.h\"\n#include \"gem/i915_gem_mman.h\"\n#include \"gem/i915_gem_pm.h\"\n#include \"gem/i915_gem_region.h\"\n#include \"gem/i915_gem_userptr.h\"\n#include \"gt/intel_engine_user.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_pm.h\"\n#include \"gt/intel_workarounds.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_file_private.h\"\n#include \"i915_trace.h\"\n#include \"i915_vgpu.h\"\n#include \"intel_clock_gating.h\"\n\nstatic int\ninsert_mappable_node(struct i915_ggtt *ggtt, struct drm_mm_node *node, u32 size)\n{\n\tint err;\n\n\terr = mutex_lock_interruptible(&ggtt->vm.mutex);\n\tif (err)\n\t\treturn err;\n\n\tmemset(node, 0, sizeof(*node));\n\terr = drm_mm_insert_node_in_range(&ggtt->vm.mm, node,\n\t\t\t\t\t  size, 0, I915_COLOR_UNEVICTABLE,\n\t\t\t\t\t  0, ggtt->mappable_end,\n\t\t\t\t\t  DRM_MM_INSERT_LOW);\n\n\tmutex_unlock(&ggtt->vm.mutex);\n\n\treturn err;\n}\n\nstatic void\nremove_mappable_node(struct i915_ggtt *ggtt, struct drm_mm_node *node)\n{\n\tmutex_lock(&ggtt->vm.mutex);\n\tdrm_mm_remove_node(node);\n\tmutex_unlock(&ggtt->vm.mutex);\n}\n\nint\ni915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\tstruct drm_i915_gem_get_aperture *args = data;\n\tstruct i915_vma *vma;\n\tu64 pinned;\n\n\tif (mutex_lock_interruptible(&ggtt->vm.mutex))\n\t\treturn -EINTR;\n\n\tpinned = ggtt->vm.reserved;\n\tlist_for_each_entry(vma, &ggtt->vm.bound_list, vm_link)\n\t\tif (i915_vma_is_pinned(vma))\n\t\t\tpinned += vma->node.size;\n\n\tmutex_unlock(&ggtt->vm.mutex);\n\n\targs->aper_size = ggtt->vm.total;\n\targs->aper_available_size = args->aper_size - pinned;\n\n\treturn 0;\n}\n\nint i915_gem_object_unbind(struct drm_i915_gem_object *obj,\n\t\t\t   unsigned long flags)\n{\n\tstruct intel_runtime_pm *rpm = &to_i915(obj->base.dev)->runtime_pm;\n\tbool vm_trylock = !!(flags & I915_GEM_OBJECT_UNBIND_VM_TRYLOCK);\n\tLIST_HEAD(still_in_list);\n\tintel_wakeref_t wakeref;\n\tstruct i915_vma *vma;\n\tint ret;\n\n\tassert_object_held(obj);\n\n\tif (list_empty(&obj->vma.list))\n\t\treturn 0;\n\n\t \n\twakeref = intel_runtime_pm_get(rpm);\n\ntry_again:\n\tret = 0;\n\tspin_lock(&obj->vma.lock);\n\twhile (!ret && (vma = list_first_entry_or_null(&obj->vma.list,\n\t\t\t\t\t\t       struct i915_vma,\n\t\t\t\t\t\t       obj_link))) {\n\t\tlist_move_tail(&vma->obj_link, &still_in_list);\n\t\tif (!i915_vma_is_bound(vma, I915_VMA_BIND_MASK))\n\t\t\tcontinue;\n\n\t\tif (flags & I915_GEM_OBJECT_UNBIND_TEST) {\n\t\t\tret = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tret = -EAGAIN;\n\t\tif (!i915_vm_tryget(vma->vm))\n\t\t\tbreak;\n\n\t\tspin_unlock(&obj->vma.lock);\n\n\t\t \n\n\t\tret = -EBUSY;\n\t\tif (flags & I915_GEM_OBJECT_UNBIND_ASYNC) {\n\t\t\tassert_object_held(vma->obj);\n\t\t\tret = i915_vma_unbind_async(vma, vm_trylock);\n\t\t}\n\n\t\tif (ret == -EBUSY && (flags & I915_GEM_OBJECT_UNBIND_ACTIVE ||\n\t\t\t\t      !i915_vma_is_active(vma))) {\n\t\t\tif (vm_trylock) {\n\t\t\t\tif (mutex_trylock(&vma->vm->mutex)) {\n\t\t\t\t\tret = __i915_vma_unbind(vma);\n\t\t\t\t\tmutex_unlock(&vma->vm->mutex);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tret = i915_vma_unbind(vma);\n\t\t\t}\n\t\t}\n\n\t\ti915_vm_put(vma->vm);\n\t\tspin_lock(&obj->vma.lock);\n\t}\n\tlist_splice_init(&still_in_list, &obj->vma.list);\n\tspin_unlock(&obj->vma.lock);\n\n\tif (ret == -EAGAIN && flags & I915_GEM_OBJECT_UNBIND_BARRIER) {\n\t\trcu_barrier();  \n\t\tgoto try_again;\n\t}\n\n\tintel_runtime_pm_put(rpm, wakeref);\n\n\treturn ret;\n}\n\nstatic int\nshmem_pread(struct page *page, int offset, int len, char __user *user_data,\n\t    bool needs_clflush)\n{\n\tchar *vaddr;\n\tint ret;\n\n\tvaddr = kmap(page);\n\n\tif (needs_clflush)\n\t\tdrm_clflush_virt_range(vaddr + offset, len);\n\n\tret = __copy_to_user(user_data, vaddr + offset, len);\n\n\tkunmap(page);\n\n\treturn ret ? -EFAULT : 0;\n}\n\nstatic int\ni915_gem_shmem_pread(struct drm_i915_gem_object *obj,\n\t\t     struct drm_i915_gem_pread *args)\n{\n\tunsigned int needs_clflush;\n\tchar __user *user_data;\n\tunsigned long offset;\n\tpgoff_t idx;\n\tu64 remain;\n\tint ret;\n\n\tret = i915_gem_object_lock_interruptible(obj, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tret = i915_gem_object_pin_pages(obj);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tret = i915_gem_object_prepare_read(obj, &needs_clflush);\n\tif (ret)\n\t\tgoto err_unpin;\n\n\ti915_gem_object_finish_access(obj);\n\ti915_gem_object_unlock(obj);\n\n\tremain = args->size;\n\tuser_data = u64_to_user_ptr(args->data_ptr);\n\toffset = offset_in_page(args->offset);\n\tfor (idx = args->offset >> PAGE_SHIFT; remain; idx++) {\n\t\tstruct page *page = i915_gem_object_get_page(obj, idx);\n\t\tunsigned int length = min_t(u64, remain, PAGE_SIZE - offset);\n\n\t\tret = shmem_pread(page, offset, length, user_data,\n\t\t\t\t  needs_clflush);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tremain -= length;\n\t\tuser_data += length;\n\t\toffset = 0;\n\t}\n\n\ti915_gem_object_unpin_pages(obj);\n\treturn ret;\n\nerr_unpin:\n\ti915_gem_object_unpin_pages(obj);\nerr_unlock:\n\ti915_gem_object_unlock(obj);\n\treturn ret;\n}\n\nstatic inline bool\ngtt_user_read(struct io_mapping *mapping,\n\t      loff_t base, int offset,\n\t      char __user *user_data, int length)\n{\n\tvoid __iomem *vaddr;\n\tunsigned long unwritten;\n\n\t \n\tvaddr = io_mapping_map_atomic_wc(mapping, base);\n\tunwritten = __copy_to_user_inatomic(user_data,\n\t\t\t\t\t    (void __force *)vaddr + offset,\n\t\t\t\t\t    length);\n\tio_mapping_unmap_atomic(vaddr);\n\tif (unwritten) {\n\t\tvaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);\n\t\tunwritten = copy_to_user(user_data,\n\t\t\t\t\t (void __force *)vaddr + offset,\n\t\t\t\t\t length);\n\t\tio_mapping_unmap(vaddr);\n\t}\n\treturn unwritten;\n}\n\nstatic struct i915_vma *i915_gem_gtt_prepare(struct drm_i915_gem_object *obj,\n\t\t\t\t\t     struct drm_mm_node *node,\n\t\t\t\t\t     bool write)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\tstruct i915_vma *vma;\n\tstruct i915_gem_ww_ctx ww;\n\tint ret;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\tvma = ERR_PTR(-ENODEV);\n\tret = i915_gem_object_lock(obj, &ww);\n\tif (ret)\n\t\tgoto err_ww;\n\n\tret = i915_gem_object_set_to_gtt_domain(obj, write);\n\tif (ret)\n\t\tgoto err_ww;\n\n\tif (!i915_gem_object_is_tiled(obj))\n\t\tvma = i915_gem_object_ggtt_pin_ww(obj, &ww, NULL, 0, 0,\n\t\t\t\t\t\t  PIN_MAPPABLE |\n\t\t\t\t\t\t  PIN_NONBLOCK   |\n\t\t\t\t\t\t  PIN_NOEVICT);\n\tif (vma == ERR_PTR(-EDEADLK)) {\n\t\tret = -EDEADLK;\n\t\tgoto err_ww;\n\t} else if (!IS_ERR(vma)) {\n\t\tnode->start = i915_ggtt_offset(vma);\n\t\tnode->flags = 0;\n\t} else {\n\t\tret = insert_mappable_node(ggtt, node, PAGE_SIZE);\n\t\tif (ret)\n\t\t\tgoto err_ww;\n\t\tGEM_BUG_ON(!drm_mm_node_allocated(node));\n\t\tvma = NULL;\n\t}\n\n\tret = i915_gem_object_pin_pages(obj);\n\tif (ret) {\n\t\tif (drm_mm_node_allocated(node)) {\n\t\t\tggtt->vm.clear_range(&ggtt->vm, node->start, node->size);\n\t\t\tremove_mappable_node(ggtt, node);\n\t\t} else {\n\t\t\ti915_vma_unpin(vma);\n\t\t}\n\t}\n\nerr_ww:\n\tif (ret == -EDEADLK) {\n\t\tret = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!ret)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\n\treturn ret ? ERR_PTR(ret) : vma;\n}\n\nstatic void i915_gem_gtt_cleanup(struct drm_i915_gem_object *obj,\n\t\t\t\t struct drm_mm_node *node,\n\t\t\t\t struct i915_vma *vma)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\n\ti915_gem_object_unpin_pages(obj);\n\tif (drm_mm_node_allocated(node)) {\n\t\tggtt->vm.clear_range(&ggtt->vm, node->start, node->size);\n\t\tremove_mappable_node(ggtt, node);\n\t} else {\n\t\ti915_vma_unpin(vma);\n\t}\n}\n\nstatic int\ni915_gem_gtt_pread(struct drm_i915_gem_object *obj,\n\t\t   const struct drm_i915_gem_pread *args)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\tunsigned long remain, offset;\n\tintel_wakeref_t wakeref;\n\tstruct drm_mm_node node;\n\tvoid __user *user_data;\n\tstruct i915_vma *vma;\n\tint ret = 0;\n\n\tif (overflows_type(args->size, remain) ||\n\t    overflows_type(args->offset, offset))\n\t\treturn -EINVAL;\n\n\twakeref = intel_runtime_pm_get(&i915->runtime_pm);\n\n\tvma = i915_gem_gtt_prepare(obj, &node, false);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out_rpm;\n\t}\n\n\tuser_data = u64_to_user_ptr(args->data_ptr);\n\tremain = args->size;\n\toffset = args->offset;\n\n\twhile (remain > 0) {\n\t\t \n\t\tu32 page_base = node.start;\n\t\tunsigned page_offset = offset_in_page(offset);\n\t\tunsigned page_length = PAGE_SIZE - page_offset;\n\t\tpage_length = remain < page_length ? remain : page_length;\n\t\tif (drm_mm_node_allocated(&node)) {\n\t\t\tggtt->vm.insert_page(&ggtt->vm,\n\t\t\t\t\t     i915_gem_object_get_dma_address(obj,\n\t\t\t\t\t\t\t\t\t     offset >> PAGE_SHIFT),\n\t\t\t\t\t     node.start,\n\t\t\t\t\t     i915_gem_get_pat_index(i915,\n\t\t\t\t\t\t\t\t    I915_CACHE_NONE), 0);\n\t\t} else {\n\t\t\tpage_base += offset & PAGE_MASK;\n\t\t}\n\n\t\tif (gtt_user_read(&ggtt->iomap, page_base, page_offset,\n\t\t\t\t  user_data, page_length)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tremain -= page_length;\n\t\tuser_data += page_length;\n\t\toffset += page_length;\n\t}\n\n\ti915_gem_gtt_cleanup(obj, &node, vma);\nout_rpm:\n\tintel_runtime_pm_put(&i915->runtime_pm, wakeref);\n\treturn ret;\n}\n\n \nint\ni915_gem_pread_ioctl(struct drm_device *dev, void *data,\n\t\t     struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_pread *args = data;\n\tstruct drm_i915_gem_object *obj;\n\tint ret;\n\n\t \n\tif (GRAPHICS_VER(i915) >= 12 && !IS_TIGERLAKE(i915))\n\t\treturn -EOPNOTSUPP;\n\n\tif (args->size == 0)\n\t\treturn 0;\n\n\tif (!access_ok(u64_to_user_ptr(args->data_ptr),\n\t\t       args->size))\n\t\treturn -EFAULT;\n\n\tobj = i915_gem_object_lookup(file, args->handle);\n\tif (!obj)\n\t\treturn -ENOENT;\n\n\t \n\tif (range_overflows_t(u64, args->offset, args->size, obj->base.size)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\ttrace_i915_gem_object_pread(obj, args->offset, args->size);\n\tret = -ENODEV;\n\tif (obj->ops->pread)\n\t\tret = obj->ops->pread(obj, args);\n\tif (ret != -ENODEV)\n\t\tgoto out;\n\n\tret = i915_gem_object_wait(obj,\n\t\t\t\t   I915_WAIT_INTERRUPTIBLE,\n\t\t\t\t   MAX_SCHEDULE_TIMEOUT);\n\tif (ret)\n\t\tgoto out;\n\n\tret = i915_gem_shmem_pread(obj, args);\n\tif (ret == -EFAULT || ret == -ENODEV)\n\t\tret = i915_gem_gtt_pread(obj, args);\n\nout:\n\ti915_gem_object_put(obj);\n\treturn ret;\n}\n\n \n\nstatic inline bool\nggtt_write(struct io_mapping *mapping,\n\t   loff_t base, int offset,\n\t   char __user *user_data, int length)\n{\n\tvoid __iomem *vaddr;\n\tunsigned long unwritten;\n\n\t \n\tvaddr = io_mapping_map_atomic_wc(mapping, base);\n\tunwritten = __copy_from_user_inatomic_nocache((void __force *)vaddr + offset,\n\t\t\t\t\t\t      user_data, length);\n\tio_mapping_unmap_atomic(vaddr);\n\tif (unwritten) {\n\t\tvaddr = io_mapping_map_wc(mapping, base, PAGE_SIZE);\n\t\tunwritten = copy_from_user((void __force *)vaddr + offset,\n\t\t\t\t\t   user_data, length);\n\t\tio_mapping_unmap(vaddr);\n\t}\n\n\treturn unwritten;\n}\n\n \nstatic int\ni915_gem_gtt_pwrite_fast(struct drm_i915_gem_object *obj,\n\t\t\t const struct drm_i915_gem_pwrite *args)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\tstruct intel_runtime_pm *rpm = &i915->runtime_pm;\n\tunsigned long remain, offset;\n\tintel_wakeref_t wakeref;\n\tstruct drm_mm_node node;\n\tstruct i915_vma *vma;\n\tvoid __user *user_data;\n\tint ret = 0;\n\n\tif (overflows_type(args->size, remain) ||\n\t    overflows_type(args->offset, offset))\n\t\treturn -EINVAL;\n\n\tif (i915_gem_object_has_struct_page(obj)) {\n\t\t \n\t\twakeref = intel_runtime_pm_get_if_in_use(rpm);\n\t\tif (!wakeref)\n\t\t\treturn -EFAULT;\n\t} else {\n\t\t \n\t\twakeref = intel_runtime_pm_get(rpm);\n\t}\n\n\tvma = i915_gem_gtt_prepare(obj, &node, true);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out_rpm;\n\t}\n\n\ti915_gem_object_invalidate_frontbuffer(obj, ORIGIN_CPU);\n\n\tuser_data = u64_to_user_ptr(args->data_ptr);\n\toffset = args->offset;\n\tremain = args->size;\n\twhile (remain) {\n\t\t \n\t\tu32 page_base = node.start;\n\t\tunsigned int page_offset = offset_in_page(offset);\n\t\tunsigned int page_length = PAGE_SIZE - page_offset;\n\t\tpage_length = remain < page_length ? remain : page_length;\n\t\tif (drm_mm_node_allocated(&node)) {\n\t\t\t \n\t\t\tintel_gt_flush_ggtt_writes(ggtt->vm.gt);\n\t\t\tggtt->vm.insert_page(&ggtt->vm,\n\t\t\t\t\t     i915_gem_object_get_dma_address(obj,\n\t\t\t\t\t\t\t\t\t     offset >> PAGE_SHIFT),\n\t\t\t\t\t     node.start,\n\t\t\t\t\t     i915_gem_get_pat_index(i915,\n\t\t\t\t\t\t\t\t    I915_CACHE_NONE), 0);\n\t\t\twmb();  \n\t\t} else {\n\t\t\tpage_base += offset & PAGE_MASK;\n\t\t}\n\t\t \n\t\tif (ggtt_write(&ggtt->iomap, page_base, page_offset,\n\t\t\t       user_data, page_length)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tremain -= page_length;\n\t\tuser_data += page_length;\n\t\toffset += page_length;\n\t}\n\n\tintel_gt_flush_ggtt_writes(ggtt->vm.gt);\n\ti915_gem_object_flush_frontbuffer(obj, ORIGIN_CPU);\n\n\ti915_gem_gtt_cleanup(obj, &node, vma);\nout_rpm:\n\tintel_runtime_pm_put(rpm, wakeref);\n\treturn ret;\n}\n\n \nstatic int\nshmem_pwrite(struct page *page, int offset, int len, char __user *user_data,\n\t     bool needs_clflush_before,\n\t     bool needs_clflush_after)\n{\n\tchar *vaddr;\n\tint ret;\n\n\tvaddr = kmap(page);\n\n\tif (needs_clflush_before)\n\t\tdrm_clflush_virt_range(vaddr + offset, len);\n\n\tret = __copy_from_user(vaddr + offset, user_data, len);\n\tif (!ret && needs_clflush_after)\n\t\tdrm_clflush_virt_range(vaddr + offset, len);\n\n\tkunmap(page);\n\n\treturn ret ? -EFAULT : 0;\n}\n\nstatic int\ni915_gem_shmem_pwrite(struct drm_i915_gem_object *obj,\n\t\t      const struct drm_i915_gem_pwrite *args)\n{\n\tunsigned int partial_cacheline_write;\n\tunsigned int needs_clflush;\n\tvoid __user *user_data;\n\tunsigned long offset;\n\tpgoff_t idx;\n\tu64 remain;\n\tint ret;\n\n\tret = i915_gem_object_lock_interruptible(obj, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tret = i915_gem_object_pin_pages(obj);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tret = i915_gem_object_prepare_write(obj, &needs_clflush);\n\tif (ret)\n\t\tgoto err_unpin;\n\n\ti915_gem_object_finish_access(obj);\n\ti915_gem_object_unlock(obj);\n\n\t \n\tpartial_cacheline_write = 0;\n\tif (needs_clflush & CLFLUSH_BEFORE)\n\t\tpartial_cacheline_write = boot_cpu_data.x86_clflush_size - 1;\n\n\tuser_data = u64_to_user_ptr(args->data_ptr);\n\tremain = args->size;\n\toffset = offset_in_page(args->offset);\n\tfor (idx = args->offset >> PAGE_SHIFT; remain; idx++) {\n\t\tstruct page *page = i915_gem_object_get_page(obj, idx);\n\t\tunsigned int length = min_t(u64, remain, PAGE_SIZE - offset);\n\n\t\tret = shmem_pwrite(page, offset, length, user_data,\n\t\t\t\t   (offset | length) & partial_cacheline_write,\n\t\t\t\t   needs_clflush & CLFLUSH_AFTER);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tremain -= length;\n\t\tuser_data += length;\n\t\toffset = 0;\n\t}\n\n\ti915_gem_object_flush_frontbuffer(obj, ORIGIN_CPU);\n\n\ti915_gem_object_unpin_pages(obj);\n\treturn ret;\n\nerr_unpin:\n\ti915_gem_object_unpin_pages(obj);\nerr_unlock:\n\ti915_gem_object_unlock(obj);\n\treturn ret;\n}\n\n \nint\ni915_gem_pwrite_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_pwrite *args = data;\n\tstruct drm_i915_gem_object *obj;\n\tint ret;\n\n\t \n\tif (GRAPHICS_VER(i915) >= 12 && !IS_TIGERLAKE(i915))\n\t\treturn -EOPNOTSUPP;\n\n\tif (args->size == 0)\n\t\treturn 0;\n\n\tif (!access_ok(u64_to_user_ptr(args->data_ptr), args->size))\n\t\treturn -EFAULT;\n\n\tobj = i915_gem_object_lookup(file, args->handle);\n\tif (!obj)\n\t\treturn -ENOENT;\n\n\t \n\tif (range_overflows_t(u64, args->offset, args->size, obj->base.size)) {\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\t \n\tif (i915_gem_object_is_readonly(obj)) {\n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\ttrace_i915_gem_object_pwrite(obj, args->offset, args->size);\n\n\tret = -ENODEV;\n\tif (obj->ops->pwrite)\n\t\tret = obj->ops->pwrite(obj, args);\n\tif (ret != -ENODEV)\n\t\tgoto err;\n\n\tret = i915_gem_object_wait(obj,\n\t\t\t\t   I915_WAIT_INTERRUPTIBLE |\n\t\t\t\t   I915_WAIT_ALL,\n\t\t\t\t   MAX_SCHEDULE_TIMEOUT);\n\tif (ret)\n\t\tgoto err;\n\n\tret = -EFAULT;\n\t \n\tif (!i915_gem_object_has_struct_page(obj) ||\n\t    i915_gem_cpu_write_needs_clflush(obj))\n\t\t \n\t\tret = i915_gem_gtt_pwrite_fast(obj, args);\n\n\tif (ret == -EFAULT || ret == -ENOSPC) {\n\t\tif (i915_gem_object_has_struct_page(obj))\n\t\t\tret = i915_gem_shmem_pwrite(obj, args);\n\t}\n\nerr:\n\ti915_gem_object_put(obj);\n\treturn ret;\n}\n\n \nint\ni915_gem_sw_finish_ioctl(struct drm_device *dev, void *data,\n\t\t\t struct drm_file *file)\n{\n\tstruct drm_i915_gem_sw_finish *args = data;\n\tstruct drm_i915_gem_object *obj;\n\n\tobj = i915_gem_object_lookup(file, args->handle);\n\tif (!obj)\n\t\treturn -ENOENT;\n\n\t \n\n\t \n\ti915_gem_object_flush_if_display(obj);\n\ti915_gem_object_put(obj);\n\n\treturn 0;\n}\n\nvoid i915_gem_runtime_suspend(struct drm_i915_private *i915)\n{\n\tstruct drm_i915_gem_object *obj, *on;\n\tint i;\n\n\t \n\n\tlist_for_each_entry_safe(obj, on,\n\t\t\t\t &to_gt(i915)->ggtt->userfault_list, userfault_link)\n\t\t__i915_gem_object_release_mmap_gtt(obj);\n\n\tlist_for_each_entry_safe(obj, on,\n\t\t\t\t &i915->runtime_pm.lmem_userfault_list, userfault_link)\n\t\ti915_gem_object_runtime_pm_release_mmap_offset(obj);\n\n\t \n\tfor (i = 0; i < to_gt(i915)->ggtt->num_fences; i++) {\n\t\tstruct i915_fence_reg *reg = &to_gt(i915)->ggtt->fence_regs[i];\n\n\t\t \n\n\t\tif (!reg->vma)\n\t\t\tcontinue;\n\n\t\tGEM_BUG_ON(i915_vma_has_userfault(reg->vma));\n\t\treg->dirty = true;\n\t}\n}\n\nstatic void discard_ggtt_vma(struct i915_vma *vma)\n{\n\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\tspin_lock(&obj->vma.lock);\n\tif (!RB_EMPTY_NODE(&vma->obj_node)) {\n\t\trb_erase(&vma->obj_node, &obj->vma.tree);\n\t\tRB_CLEAR_NODE(&vma->obj_node);\n\t}\n\tspin_unlock(&obj->vma.lock);\n}\n\nstruct i915_vma *\ni915_gem_object_ggtt_pin_ww(struct drm_i915_gem_object *obj,\n\t\t\t    struct i915_gem_ww_ctx *ww,\n\t\t\t    const struct i915_gtt_view *view,\n\t\t\t    u64 size, u64 alignment, u64 flags)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\tstruct i915_vma *vma;\n\tint ret;\n\n\tGEM_WARN_ON(!ww);\n\n\tif (flags & PIN_MAPPABLE &&\n\t    (!view || view->type == I915_GTT_VIEW_NORMAL)) {\n\t\t \n\t\tif (obj->base.size > ggtt->mappable_end)\n\t\t\treturn ERR_PTR(-E2BIG);\n\n\t\t \n\t\tif (flags & PIN_NONBLOCK &&\n\t\t    obj->base.size > ggtt->mappable_end / 2)\n\t\t\treturn ERR_PTR(-ENOSPC);\n\t}\n\nnew_vma:\n\tvma = i915_vma_instance(obj, &ggtt->vm, view);\n\tif (IS_ERR(vma))\n\t\treturn vma;\n\n\tif (i915_vma_misplaced(vma, size, alignment, flags)) {\n\t\tif (flags & PIN_NONBLOCK) {\n\t\t\tif (i915_vma_is_pinned(vma) || i915_vma_is_active(vma))\n\t\t\t\treturn ERR_PTR(-ENOSPC);\n\n\t\t\t \n\t\t\tif (flags & PIN_MAPPABLE &&\n\t\t\t    (vma->fence_size > ggtt->mappable_end / 2 ||\n\t\t\t    !i915_vma_is_map_and_fenceable(vma)))\n\t\t\t\treturn ERR_PTR(-ENOSPC);\n\t\t}\n\n\t\tif (i915_vma_is_pinned(vma) || i915_vma_is_active(vma)) {\n\t\t\tdiscard_ggtt_vma(vma);\n\t\t\tgoto new_vma;\n\t\t}\n\n\t\tret = i915_vma_unbind(vma);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = i915_vma_pin_ww(vma, ww, size, alignment, flags | PIN_GLOBAL);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tif (vma->fence && !i915_gem_object_is_tiled(obj)) {\n\t\tmutex_lock(&ggtt->vm.mutex);\n\t\ti915_vma_revoke_fence(vma);\n\t\tmutex_unlock(&ggtt->vm.mutex);\n\t}\n\n\tret = i915_vma_wait_for_bind(vma);\n\tif (ret) {\n\t\ti915_vma_unpin(vma);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn vma;\n}\n\nstruct i915_vma * __must_check\ni915_gem_object_ggtt_pin(struct drm_i915_gem_object *obj,\n\t\t\t const struct i915_gtt_view *view,\n\t\t\t u64 size, u64 alignment, u64 flags)\n{\n\tstruct i915_gem_ww_ctx ww;\n\tstruct i915_vma *ret;\n\tint err;\n\n\tfor_i915_gem_ww(&ww, err, true) {\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tret = i915_gem_object_ggtt_pin_ww(obj, &ww, view, size,\n\t\t\t\t\t\t  alignment, flags);\n\t\tif (IS_ERR(ret))\n\t\t\terr = PTR_ERR(ret);\n\t}\n\n\treturn err ? ERR_PTR(err) : ret;\n}\n\nint\ni915_gem_madvise_ioctl(struct drm_device *dev, void *data,\n\t\t       struct drm_file *file_priv)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_madvise *args = data;\n\tstruct drm_i915_gem_object *obj;\n\tint err;\n\n\tswitch (args->madv) {\n\tcase I915_MADV_DONTNEED:\n\tcase I915_MADV_WILLNEED:\n\t    break;\n\tdefault:\n\t    return -EINVAL;\n\t}\n\n\tobj = i915_gem_object_lookup(file_priv, args->handle);\n\tif (!obj)\n\t\treturn -ENOENT;\n\n\terr = i915_gem_object_lock_interruptible(obj, NULL);\n\tif (err)\n\t\tgoto out;\n\n\tif (i915_gem_object_has_pages(obj) &&\n\t    i915_gem_object_is_tiled(obj) &&\n\t    i915->gem_quirks & GEM_QUIRK_PIN_SWIZZLED_PAGES) {\n\t\tif (obj->mm.madv == I915_MADV_WILLNEED) {\n\t\t\tGEM_BUG_ON(!i915_gem_object_has_tiling_quirk(obj));\n\t\t\ti915_gem_object_clear_tiling_quirk(obj);\n\t\t\ti915_gem_object_make_shrinkable(obj);\n\t\t}\n\t\tif (args->madv == I915_MADV_WILLNEED) {\n\t\t\tGEM_BUG_ON(i915_gem_object_has_tiling_quirk(obj));\n\t\t\ti915_gem_object_make_unshrinkable(obj);\n\t\t\ti915_gem_object_set_tiling_quirk(obj);\n\t\t}\n\t}\n\n\tif (obj->mm.madv != __I915_MADV_PURGED) {\n\t\tobj->mm.madv = args->madv;\n\t\tif (obj->ops->adjust_lru)\n\t\t\tobj->ops->adjust_lru(obj);\n\t}\n\n\tif (i915_gem_object_has_pages(obj) ||\n\t    i915_gem_object_has_self_managed_shrink_list(obj)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\t\tif (!list_empty(&obj->mm.link)) {\n\t\t\tstruct list_head *list;\n\n\t\t\tif (obj->mm.madv != I915_MADV_WILLNEED)\n\t\t\t\tlist = &i915->mm.purge_list;\n\t\t\telse\n\t\t\t\tlist = &i915->mm.shrink_list;\n\t\t\tlist_move_tail(&obj->mm.link, list);\n\n\t\t}\n\t\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n\t}\n\n\t \n\tif (obj->mm.madv == I915_MADV_DONTNEED &&\n\t    !i915_gem_object_has_pages(obj))\n\t\ti915_gem_object_truncate(obj);\n\n\targs->retained = obj->mm.madv != __I915_MADV_PURGED;\n\n\ti915_gem_object_unlock(obj);\nout:\n\ti915_gem_object_put(obj);\n\treturn err;\n}\n\n \nvoid i915_gem_drain_freed_objects(struct drm_i915_private *i915)\n{\n\twhile (atomic_read(&i915->mm.free_count)) {\n\t\tflush_work(&i915->mm.free_work);\n\t\tdrain_workqueue(i915->bdev.wq);\n\t\trcu_barrier();\n\t}\n}\n\n \nvoid i915_gem_drain_workqueue(struct drm_i915_private *i915)\n{\n\tint i;\n\n\tfor (i = 0; i < 3; i++) {\n\t\tflush_workqueue(i915->wq);\n\t\trcu_barrier();\n\t\ti915_gem_drain_freed_objects(i915);\n\t}\n\n\tdrain_workqueue(i915->wq);\n}\n\nint i915_gem_init(struct drm_i915_private *dev_priv)\n{\n\tstruct intel_gt *gt;\n\tunsigned int i;\n\tint ret;\n\n\t \n\tBUILD_BUG_ON(I915_CACHE_NONE != 0 ||\n\t\t     I915_CACHE_LLC != 1 ||\n\t\t     I915_CACHE_L3_LLC != 2 ||\n\t\t     I915_CACHE_WT != 3 ||\n\t\t     I915_MAX_CACHE_LEVEL != 4);\n\n\t \n\tif (intel_vgpu_active(dev_priv) && !intel_vgpu_has_huge_gtt(dev_priv))\n\t\tRUNTIME_INFO(dev_priv)->page_sizes = I915_GTT_PAGE_SIZE_4K;\n\n\tret = i915_gem_init_userptr(dev_priv);\n\tif (ret)\n\t\treturn ret;\n\n\tfor_each_gt(gt, dev_priv, i) {\n\t\tintel_uc_fetch_firmwares(&gt->uc);\n\t\tintel_wopcm_init(&gt->wopcm);\n\t\tif (GRAPHICS_VER(dev_priv) >= 8)\n\t\t\tsetup_private_pat(gt);\n\t}\n\n\tret = i915_init_ggtt(dev_priv);\n\tif (ret) {\n\t\tGEM_BUG_ON(ret == -EIO);\n\t\tgoto err_unlock;\n\t}\n\n\t \n\tintel_clock_gating_init(dev_priv);\n\n\tfor_each_gt(gt, dev_priv, i) {\n\t\tret = intel_gt_init(gt);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\t}\n\n\t \n\tintel_engines_driver_register(dev_priv);\n\n\treturn 0;\n\n\t \nerr_unlock:\n\ti915_gem_drain_workqueue(dev_priv);\n\n\tif (ret != -EIO) {\n\t\tfor_each_gt(gt, dev_priv, i) {\n\t\t\tintel_gt_driver_remove(gt);\n\t\t\tintel_gt_driver_release(gt);\n\t\t\tintel_uc_cleanup_firmwares(&gt->uc);\n\t\t}\n\t}\n\n\tif (ret == -EIO) {\n\t\t \n\t\tfor_each_gt(gt, dev_priv, i) {\n\t\t\tif (!intel_gt_is_wedged(gt)) {\n\t\t\t\ti915_probe_error(dev_priv,\n\t\t\t\t\t\t \"Failed to initialize GPU, declaring it wedged!\\n\");\n\t\t\t\tintel_gt_set_wedged(gt);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tret = i915_ggtt_enable_hw(dev_priv);\n\t\ti915_ggtt_resume(to_gt(dev_priv)->ggtt);\n\t\tintel_clock_gating_init(dev_priv);\n\t}\n\n\ti915_gem_drain_freed_objects(dev_priv);\n\n\treturn ret;\n}\n\nvoid i915_gem_driver_register(struct drm_i915_private *i915)\n{\n\ti915_gem_driver_register__shrinker(i915);\n}\n\nvoid i915_gem_driver_unregister(struct drm_i915_private *i915)\n{\n\ti915_gem_driver_unregister__shrinker(i915);\n}\n\nvoid i915_gem_driver_remove(struct drm_i915_private *dev_priv)\n{\n\tstruct intel_gt *gt;\n\tunsigned int i;\n\n\ti915_gem_suspend_late(dev_priv);\n\tfor_each_gt(gt, dev_priv, i)\n\t\tintel_gt_driver_remove(gt);\n\tdev_priv->uabi_engines = RB_ROOT;\n\n\t \n\ti915_gem_drain_workqueue(dev_priv);\n}\n\nvoid i915_gem_driver_release(struct drm_i915_private *dev_priv)\n{\n\tstruct intel_gt *gt;\n\tunsigned int i;\n\n\tfor_each_gt(gt, dev_priv, i) {\n\t\tintel_gt_driver_release(gt);\n\t\tintel_uc_cleanup_firmwares(&gt->uc);\n\t}\n\n\t \n\ti915_gem_drain_workqueue(dev_priv);\n\n\tdrm_WARN_ON(&dev_priv->drm, !list_empty(&dev_priv->gem.contexts.list));\n}\n\nstatic void i915_gem_init__mm(struct drm_i915_private *i915)\n{\n\tspin_lock_init(&i915->mm.obj_lock);\n\n\tinit_llist_head(&i915->mm.free_list);\n\n\tINIT_LIST_HEAD(&i915->mm.purge_list);\n\tINIT_LIST_HEAD(&i915->mm.shrink_list);\n\n\ti915_gem_init__objects(i915);\n}\n\nvoid i915_gem_init_early(struct drm_i915_private *dev_priv)\n{\n\ti915_gem_init__mm(dev_priv);\n\ti915_gem_init__contexts(dev_priv);\n\n\tspin_lock_init(&dev_priv->display.fb_tracking.lock);\n}\n\nvoid i915_gem_cleanup_early(struct drm_i915_private *dev_priv)\n{\n\ti915_gem_drain_workqueue(dev_priv);\n\tGEM_BUG_ON(!llist_empty(&dev_priv->mm.free_list));\n\tGEM_BUG_ON(atomic_read(&dev_priv->mm.free_count));\n\tdrm_WARN_ON(&dev_priv->drm, dev_priv->mm.shrink_count);\n}\n\nint i915_gem_open(struct drm_i915_private *i915, struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv;\n\tstruct i915_drm_client *client;\n\tint ret = -ENOMEM;\n\n\tdrm_dbg(&i915->drm, \"\\n\");\n\n\tfile_priv = kzalloc(sizeof(*file_priv), GFP_KERNEL);\n\tif (!file_priv)\n\t\tgoto err_alloc;\n\n\tclient = i915_drm_client_alloc();\n\tif (!client)\n\t\tgoto err_client;\n\n\tfile->driver_priv = file_priv;\n\tfile_priv->i915 = i915;\n\tfile_priv->file = file;\n\tfile_priv->client = client;\n\n\tfile_priv->bsd_engine = -1;\n\tfile_priv->hang_timestamp = jiffies;\n\n\tret = i915_gem_context_open(i915, file);\n\tif (ret)\n\t\tgoto err_context;\n\n\treturn 0;\n\nerr_context:\n\ti915_drm_client_put(client);\nerr_client:\n\tkfree(file_priv);\nerr_alloc:\n\treturn ret;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/mock_gem_device.c\"\n#include \"selftests/i915_gem.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}