{
  "module_name": "i915_request.c",
  "hash_id": "3d3ff23ef4121e680928c975770eb2db29eca7aa7782f34ca3e88d66acc8e2c4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/i915_request.c",
  "human_readable_source": " \n\n#include <linux/dma-fence-array.h>\n#include <linux/dma-fence-chain.h>\n#include <linux/irq_work.h>\n#include <linux/prefetch.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n\n#include \"gem/i915_gem_context.h\"\n#include \"gt/intel_breadcrumbs.h\"\n#include \"gt/intel_context.h\"\n#include \"gt/intel_engine.h\"\n#include \"gt/intel_engine_heartbeat.h\"\n#include \"gt/intel_engine_regs.h\"\n#include \"gt/intel_gpu_commands.h\"\n#include \"gt/intel_reset.h\"\n#include \"gt/intel_ring.h\"\n#include \"gt/intel_rps.h\"\n\n#include \"i915_active.h\"\n#include \"i915_config.h\"\n#include \"i915_deps.h\"\n#include \"i915_driver.h\"\n#include \"i915_drv.h\"\n#include \"i915_trace.h\"\n\nstruct execute_cb {\n\tstruct irq_work work;\n\tstruct i915_sw_fence *fence;\n\tstruct i915_request *signal;\n};\n\nstatic struct kmem_cache *slab_requests;\nstatic struct kmem_cache *slab_execute_cbs;\n\nstatic const char *i915_fence_get_driver_name(struct dma_fence *fence)\n{\n\treturn dev_name(to_request(fence)->i915->drm.dev);\n}\n\nstatic const char *i915_fence_get_timeline_name(struct dma_fence *fence)\n{\n\tconst struct i915_gem_context *ctx;\n\n\t \n\tif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))\n\t\treturn \"signaled\";\n\n\tctx = i915_request_gem_context(to_request(fence));\n\tif (!ctx)\n\t\treturn \"[\" DRIVER_NAME \"]\";\n\n\treturn ctx->name;\n}\n\nstatic bool i915_fence_signaled(struct dma_fence *fence)\n{\n\treturn i915_request_completed(to_request(fence));\n}\n\nstatic bool i915_fence_enable_signaling(struct dma_fence *fence)\n{\n\treturn i915_request_enable_breadcrumb(to_request(fence));\n}\n\nstatic signed long i915_fence_wait(struct dma_fence *fence,\n\t\t\t\t   bool interruptible,\n\t\t\t\t   signed long timeout)\n{\n\treturn i915_request_wait_timeout(to_request(fence),\n\t\t\t\t\t interruptible | I915_WAIT_PRIORITY,\n\t\t\t\t\t timeout);\n}\n\nstruct kmem_cache *i915_request_slab_cache(void)\n{\n\treturn slab_requests;\n}\n\nstatic void i915_fence_release(struct dma_fence *fence)\n{\n\tstruct i915_request *rq = to_request(fence);\n\n\tGEM_BUG_ON(rq->guc_prio != GUC_PRIO_INIT &&\n\t\t   rq->guc_prio != GUC_PRIO_FINI);\n\n\ti915_request_free_capture_list(fetch_and_zero(&rq->capture_list));\n\tif (rq->batch_res) {\n\t\ti915_vma_resource_put(rq->batch_res);\n\t\trq->batch_res = NULL;\n\t}\n\n\t \n\ti915_sw_fence_fini(&rq->submit);\n\ti915_sw_fence_fini(&rq->semaphore);\n\n\t \n\tif (is_power_of_2(rq->execution_mask) &&\n\t    !cmpxchg(&rq->engine->request_pool, NULL, rq))\n\t\treturn;\n\n\tkmem_cache_free(slab_requests, rq);\n}\n\nconst struct dma_fence_ops i915_fence_ops = {\n\t.get_driver_name = i915_fence_get_driver_name,\n\t.get_timeline_name = i915_fence_get_timeline_name,\n\t.enable_signaling = i915_fence_enable_signaling,\n\t.signaled = i915_fence_signaled,\n\t.wait = i915_fence_wait,\n\t.release = i915_fence_release,\n};\n\nstatic void irq_execute_cb(struct irq_work *wrk)\n{\n\tstruct execute_cb *cb = container_of(wrk, typeof(*cb), work);\n\n\ti915_sw_fence_complete(cb->fence);\n\tkmem_cache_free(slab_execute_cbs, cb);\n}\n\nstatic __always_inline void\n__notify_execute_cb(struct i915_request *rq, bool (*fn)(struct irq_work *wrk))\n{\n\tstruct execute_cb *cb, *cn;\n\n\tif (llist_empty(&rq->execute_cb))\n\t\treturn;\n\n\tllist_for_each_entry_safe(cb, cn,\n\t\t\t\t  llist_del_all(&rq->execute_cb),\n\t\t\t\t  work.node.llist)\n\t\tfn(&cb->work);\n}\n\nstatic void __notify_execute_cb_irq(struct i915_request *rq)\n{\n\t__notify_execute_cb(rq, irq_work_queue);\n}\n\nstatic bool irq_work_imm(struct irq_work *wrk)\n{\n\twrk->func(wrk);\n\treturn false;\n}\n\nvoid i915_request_notify_execute_cb_imm(struct i915_request *rq)\n{\n\t__notify_execute_cb(rq, irq_work_imm);\n}\n\nstatic void __i915_request_fill(struct i915_request *rq, u8 val)\n{\n\tvoid *vaddr = rq->ring->vaddr;\n\tu32 head;\n\n\thead = rq->infix;\n\tif (rq->postfix < head) {\n\t\tmemset(vaddr + head, val, rq->ring->size - head);\n\t\thead = 0;\n\t}\n\tmemset(vaddr + head, val, rq->postfix - head);\n}\n\n \nbool\ni915_request_active_engine(struct i915_request *rq,\n\t\t\t   struct intel_engine_cs **active)\n{\n\tstruct intel_engine_cs *engine, *locked;\n\tbool ret = false;\n\n\t \n\tlocked = READ_ONCE(rq->engine);\n\tspin_lock_irq(&locked->sched_engine->lock);\n\twhile (unlikely(locked != (engine = READ_ONCE(rq->engine)))) {\n\t\tspin_unlock(&locked->sched_engine->lock);\n\t\tlocked = engine;\n\t\tspin_lock(&locked->sched_engine->lock);\n\t}\n\n\tif (i915_request_is_active(rq)) {\n\t\tif (!__i915_request_is_complete(rq))\n\t\t\t*active = locked;\n\t\tret = true;\n\t}\n\n\tspin_unlock_irq(&locked->sched_engine->lock);\n\n\treturn ret;\n}\n\nstatic void __rq_init_watchdog(struct i915_request *rq)\n{\n\trq->watchdog.timer.function = NULL;\n}\n\nstatic enum hrtimer_restart __rq_watchdog_expired(struct hrtimer *hrtimer)\n{\n\tstruct i915_request *rq =\n\t\tcontainer_of(hrtimer, struct i915_request, watchdog.timer);\n\tstruct intel_gt *gt = rq->engine->gt;\n\n\tif (!i915_request_completed(rq)) {\n\t\tif (llist_add(&rq->watchdog.link, &gt->watchdog.list))\n\t\t\tqueue_work(gt->i915->unordered_wq, &gt->watchdog.work);\n\t} else {\n\t\ti915_request_put(rq);\n\t}\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void __rq_arm_watchdog(struct i915_request *rq)\n{\n\tstruct i915_request_watchdog *wdg = &rq->watchdog;\n\tstruct intel_context *ce = rq->context;\n\n\tif (!ce->watchdog.timeout_us)\n\t\treturn;\n\n\ti915_request_get(rq);\n\n\thrtimer_init(&wdg->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\twdg->timer.function = __rq_watchdog_expired;\n\thrtimer_start_range_ns(&wdg->timer,\n\t\t\t       ns_to_ktime(ce->watchdog.timeout_us *\n\t\t\t\t\t   NSEC_PER_USEC),\n\t\t\t       NSEC_PER_MSEC,\n\t\t\t       HRTIMER_MODE_REL);\n}\n\nstatic void __rq_cancel_watchdog(struct i915_request *rq)\n{\n\tstruct i915_request_watchdog *wdg = &rq->watchdog;\n\n\tif (wdg->timer.function && hrtimer_try_to_cancel(&wdg->timer) > 0)\n\t\ti915_request_put(rq);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_CAPTURE_ERROR)\n\n \nvoid i915_request_free_capture_list(struct i915_capture_list *capture)\n{\n\twhile (capture) {\n\t\tstruct i915_capture_list *next = capture->next;\n\n\t\ti915_vma_resource_put(capture->vma_res);\n\t\tkfree(capture);\n\t\tcapture = next;\n\t}\n}\n\n#define assert_capture_list_is_null(_rq) GEM_BUG_ON((_rq)->capture_list)\n\n#define clear_capture_list(_rq) ((_rq)->capture_list = NULL)\n\n#else\n\n#define i915_request_free_capture_list(_a) do {} while (0)\n\n#define assert_capture_list_is_null(_a) do {} while (0)\n\n#define clear_capture_list(_rq) do {} while (0)\n\n#endif\n\nbool i915_request_retire(struct i915_request *rq)\n{\n\tif (!__i915_request_is_complete(rq))\n\t\treturn false;\n\n\tRQ_TRACE(rq, \"\\n\");\n\n\tGEM_BUG_ON(!i915_sw_fence_signaled(&rq->submit));\n\ttrace_i915_request_retire(rq);\n\ti915_request_mark_complete(rq);\n\n\t__rq_cancel_watchdog(rq);\n\n\t \n\tGEM_BUG_ON(!list_is_first(&rq->link,\n\t\t\t\t  &i915_request_timeline(rq)->requests));\n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\t \n\t\t__i915_request_fill(rq, POISON_FREE);\n\trq->ring->head = rq->postfix;\n\n\tif (!i915_request_signaled(rq)) {\n\t\tspin_lock_irq(&rq->lock);\n\t\tdma_fence_signal_locked(&rq->fence);\n\t\tspin_unlock_irq(&rq->lock);\n\t}\n\n\tif (test_and_set_bit(I915_FENCE_FLAG_BOOST, &rq->fence.flags))\n\t\tintel_rps_dec_waiters(&rq->engine->gt->rps);\n\n\t \n\trq->engine->remove_active_request(rq);\n\tGEM_BUG_ON(!llist_empty(&rq->execute_cb));\n\n\t__list_del_entry(&rq->link);  \n\n\tintel_context_exit(rq->context);\n\tintel_context_unpin(rq->context);\n\n\ti915_sched_node_fini(&rq->sched);\n\ti915_request_put(rq);\n\n\treturn true;\n}\n\nvoid i915_request_retire_upto(struct i915_request *rq)\n{\n\tstruct intel_timeline * const tl = i915_request_timeline(rq);\n\tstruct i915_request *tmp;\n\n\tRQ_TRACE(rq, \"\\n\");\n\tGEM_BUG_ON(!__i915_request_is_complete(rq));\n\n\tdo {\n\t\ttmp = list_first_entry(&tl->requests, typeof(*tmp), link);\n\t\tGEM_BUG_ON(!i915_request_completed(tmp));\n\t} while (i915_request_retire(tmp) && tmp != rq);\n}\n\nstatic struct i915_request * const *\n__engine_active(struct intel_engine_cs *engine)\n{\n\treturn READ_ONCE(engine->execlists.active);\n}\n\nstatic bool __request_in_flight(const struct i915_request *signal)\n{\n\tstruct i915_request * const *port, *rq;\n\tbool inflight = false;\n\n\tif (!i915_request_is_ready(signal))\n\t\treturn false;\n\n\t \n\tif (!intel_context_inflight(signal->context))\n\t\treturn false;\n\n\trcu_read_lock();\n\tfor (port = __engine_active(signal->engine);\n\t     (rq = READ_ONCE(*port));  \n\t     port++) {\n\t\tif (rq->context == signal->context) {\n\t\t\tinflight = i915_seqno_passed(rq->fence.seqno,\n\t\t\t\t\t\t     signal->fence.seqno);\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn inflight;\n}\n\nstatic int\n__await_execution(struct i915_request *rq,\n\t\t  struct i915_request *signal,\n\t\t  gfp_t gfp)\n{\n\tstruct execute_cb *cb;\n\n\tif (i915_request_is_active(signal))\n\t\treturn 0;\n\n\tcb = kmem_cache_alloc(slab_execute_cbs, gfp);\n\tif (!cb)\n\t\treturn -ENOMEM;\n\n\tcb->fence = &rq->submit;\n\ti915_sw_fence_await(cb->fence);\n\tinit_irq_work(&cb->work, irq_execute_cb);\n\n\t \n\tif (llist_add(&cb->work.node.llist, &signal->execute_cb)) {\n\t\tif (i915_request_is_active(signal) ||\n\t\t    __request_in_flight(signal))\n\t\t\ti915_request_notify_execute_cb_imm(signal);\n\t}\n\n\treturn 0;\n}\n\nstatic bool fatal_error(int error)\n{\n\tswitch (error) {\n\tcase 0:  \n\tcase -EAGAIN:  \n\tcase -ETIMEDOUT:  \n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nvoid __i915_request_skip(struct i915_request *rq)\n{\n\tGEM_BUG_ON(!fatal_error(rq->fence.error));\n\n\tif (rq->infix == rq->postfix)\n\t\treturn;\n\n\tRQ_TRACE(rq, \"error: %d\\n\", rq->fence.error);\n\n\t \n\t__i915_request_fill(rq, 0);\n\trq->infix = rq->postfix;\n}\n\nbool i915_request_set_error_once(struct i915_request *rq, int error)\n{\n\tint old;\n\n\tGEM_BUG_ON(!IS_ERR_VALUE((long)error));\n\n\tif (i915_request_signaled(rq))\n\t\treturn false;\n\n\told = READ_ONCE(rq->fence.error);\n\tdo {\n\t\tif (fatal_error(old))\n\t\t\treturn false;\n\t} while (!try_cmpxchg(&rq->fence.error, &old, error));\n\n\treturn true;\n}\n\nstruct i915_request *i915_request_mark_eio(struct i915_request *rq)\n{\n\tif (__i915_request_is_complete(rq))\n\t\treturn NULL;\n\n\tGEM_BUG_ON(i915_request_signaled(rq));\n\n\t \n\trq = i915_request_get(rq);\n\n\ti915_request_set_error_once(rq, -EIO);\n\ti915_request_mark_complete(rq);\n\n\treturn rq;\n}\n\nbool __i915_request_submit(struct i915_request *request)\n{\n\tstruct intel_engine_cs *engine = request->engine;\n\tbool result = false;\n\n\tRQ_TRACE(request, \"\\n\");\n\n\tGEM_BUG_ON(!irqs_disabled());\n\tlockdep_assert_held(&engine->sched_engine->lock);\n\n\t \n\tif (__i915_request_is_complete(request)) {\n\t\tlist_del_init(&request->sched.link);\n\t\tgoto active;\n\t}\n\n\tif (unlikely(!intel_context_is_schedulable(request->context)))\n\t\ti915_request_set_error_once(request, -EIO);\n\n\tif (unlikely(fatal_error(request->fence.error)))\n\t\t__i915_request_skip(request);\n\n\t \n\tif (request->sched.semaphores &&\n\t    i915_sw_fence_signaled(&request->semaphore))\n\t\tengine->saturated |= request->sched.semaphores;\n\n\tengine->emit_fini_breadcrumb(request,\n\t\t\t\t     request->ring->vaddr + request->postfix);\n\n\ttrace_i915_request_execute(request);\n\tif (engine->bump_serial)\n\t\tengine->bump_serial(engine);\n\telse\n\t\tengine->serial++;\n\n\tresult = true;\n\n\tGEM_BUG_ON(test_bit(I915_FENCE_FLAG_ACTIVE, &request->fence.flags));\n\tengine->add_active_request(request);\nactive:\n\tclear_bit(I915_FENCE_FLAG_PQUEUE, &request->fence.flags);\n\tset_bit(I915_FENCE_FLAG_ACTIVE, &request->fence.flags);\n\n\t \n\t__notify_execute_cb_irq(request);\n\n\t \n\tif (test_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT, &request->fence.flags))\n\t\ti915_request_enable_breadcrumb(request);\n\n\treturn result;\n}\n\nvoid i915_request_submit(struct i915_request *request)\n{\n\tstruct intel_engine_cs *engine = request->engine;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\t__i915_request_submit(request);\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\nvoid __i915_request_unsubmit(struct i915_request *request)\n{\n\tstruct intel_engine_cs *engine = request->engine;\n\n\t \n\tRQ_TRACE(request, \"\\n\");\n\n\tGEM_BUG_ON(!irqs_disabled());\n\tlockdep_assert_held(&engine->sched_engine->lock);\n\n\t \n\tGEM_BUG_ON(!test_bit(I915_FENCE_FLAG_ACTIVE, &request->fence.flags));\n\tclear_bit_unlock(I915_FENCE_FLAG_ACTIVE, &request->fence.flags);\n\tif (test_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT, &request->fence.flags))\n\t\ti915_request_cancel_breadcrumb(request);\n\n\t \n\tif (request->sched.semaphores && __i915_request_has_started(request))\n\t\trequest->sched.semaphores = 0;\n\n\t \n}\n\nvoid i915_request_unsubmit(struct i915_request *request)\n{\n\tstruct intel_engine_cs *engine = request->engine;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\t__i915_request_unsubmit(request);\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\nvoid i915_request_cancel(struct i915_request *rq, int error)\n{\n\tif (!i915_request_set_error_once(rq, error))\n\t\treturn;\n\n\tset_bit(I915_FENCE_FLAG_SENTINEL, &rq->fence.flags);\n\n\tintel_context_cancel_request(rq->context, rq);\n}\n\nstatic int\nsubmit_notify(struct i915_sw_fence *fence, enum i915_sw_fence_notify state)\n{\n\tstruct i915_request *request =\n\t\tcontainer_of(fence, typeof(*request), submit);\n\n\tswitch (state) {\n\tcase FENCE_COMPLETE:\n\t\ttrace_i915_request_submit(request);\n\n\t\tif (unlikely(fence->error))\n\t\t\ti915_request_set_error_once(request, fence->error);\n\t\telse\n\t\t\t__rq_arm_watchdog(request);\n\n\t\t \n\t\trcu_read_lock();\n\t\trequest->engine->submit_request(request);\n\t\trcu_read_unlock();\n\t\tbreak;\n\n\tcase FENCE_FREE:\n\t\ti915_request_put(request);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic int\nsemaphore_notify(struct i915_sw_fence *fence, enum i915_sw_fence_notify state)\n{\n\tstruct i915_request *rq = container_of(fence, typeof(*rq), semaphore);\n\n\tswitch (state) {\n\tcase FENCE_COMPLETE:\n\t\tbreak;\n\n\tcase FENCE_FREE:\n\t\ti915_request_put(rq);\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic void retire_requests(struct intel_timeline *tl)\n{\n\tstruct i915_request *rq, *rn;\n\n\tlist_for_each_entry_safe(rq, rn, &tl->requests, link)\n\t\tif (!i915_request_retire(rq))\n\t\t\tbreak;\n}\n\nstatic noinline struct i915_request *\nrequest_alloc_slow(struct intel_timeline *tl,\n\t\t   struct i915_request **rsvd,\n\t\t   gfp_t gfp)\n{\n\tstruct i915_request *rq;\n\n\t \n\tif (!gfpflags_allow_blocking(gfp)) {\n\t\trq = xchg(rsvd, NULL);\n\t\tif (!rq)  \n\t\t\tgoto out;\n\n\t\treturn rq;\n\t}\n\n\tif (list_empty(&tl->requests))\n\t\tgoto out;\n\n\t \n\trq = list_first_entry(&tl->requests, typeof(*rq), link);\n\ti915_request_retire(rq);\n\n\trq = kmem_cache_alloc(slab_requests,\n\t\t\t      gfp | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);\n\tif (rq)\n\t\treturn rq;\n\n\t \n\trq = list_last_entry(&tl->requests, typeof(*rq), link);\n\tcond_synchronize_rcu(rq->rcustate);\n\n\t \n\tretire_requests(tl);\n\nout:\n\treturn kmem_cache_alloc(slab_requests, gfp);\n}\n\nstatic void __i915_request_ctor(void *arg)\n{\n\tstruct i915_request *rq = arg;\n\n\tspin_lock_init(&rq->lock);\n\ti915_sched_node_init(&rq->sched);\n\ti915_sw_fence_init(&rq->submit, submit_notify);\n\ti915_sw_fence_init(&rq->semaphore, semaphore_notify);\n\n\tclear_capture_list(rq);\n\trq->batch_res = NULL;\n\n\tinit_llist_head(&rq->execute_cb);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#define clear_batch_ptr(_rq) ((_rq)->batch = NULL)\n#else\n#define clear_batch_ptr(_a) do {} while (0)\n#endif\n\nstruct i915_request *\n__i915_request_create(struct intel_context *ce, gfp_t gfp)\n{\n\tstruct intel_timeline *tl = ce->timeline;\n\tstruct i915_request *rq;\n\tu32 seqno;\n\tint ret;\n\n\tmight_alloc(gfp);\n\n\t \n\t__intel_context_pin(ce);\n\n\t \n\trq = kmem_cache_alloc(slab_requests,\n\t\t\t      gfp | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);\n\tif (unlikely(!rq)) {\n\t\trq = request_alloc_slow(tl, &ce->engine->request_pool, gfp);\n\t\tif (!rq) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_unreserve;\n\t\t}\n\t}\n\n\trq->context = ce;\n\trq->engine = ce->engine;\n\trq->ring = ce->ring;\n\trq->execution_mask = ce->engine->mask;\n\trq->i915 = ce->engine->i915;\n\n\tret = intel_timeline_get_seqno(tl, rq, &seqno);\n\tif (ret)\n\t\tgoto err_free;\n\n\tdma_fence_init(&rq->fence, &i915_fence_ops, &rq->lock,\n\t\t       tl->fence_context, seqno);\n\n\tRCU_INIT_POINTER(rq->timeline, tl);\n\trq->hwsp_seqno = tl->hwsp_seqno;\n\tGEM_BUG_ON(__i915_request_is_complete(rq));\n\n\trq->rcustate = get_state_synchronize_rcu();  \n\n\trq->guc_prio = GUC_PRIO_INIT;\n\n\t \n\ti915_sw_fence_reinit(&i915_request_get(rq)->submit);\n\ti915_sw_fence_reinit(&i915_request_get(rq)->semaphore);\n\n\ti915_sched_node_reinit(&rq->sched);\n\n\t \n\tclear_batch_ptr(rq);\n\t__rq_init_watchdog(rq);\n\tassert_capture_list_is_null(rq);\n\tGEM_BUG_ON(!llist_empty(&rq->execute_cb));\n\tGEM_BUG_ON(rq->batch_res);\n\n\t \n\trq->reserved_space =\n\t\t2 * rq->engine->emit_fini_breadcrumb_dw * sizeof(u32);\n\n\t \n\trq->head = rq->ring->emit;\n\n\tret = rq->engine->request_alloc(rq);\n\tif (ret)\n\t\tgoto err_unwind;\n\n\trq->infix = rq->ring->emit;  \n\n\tintel_context_mark_active(ce);\n\tlist_add_tail_rcu(&rq->link, &tl->requests);\n\n\treturn rq;\n\nerr_unwind:\n\tce->ring->emit = rq->head;\n\n\t \n\tGEM_BUG_ON(!list_empty(&rq->sched.signalers_list));\n\tGEM_BUG_ON(!list_empty(&rq->sched.waiters_list));\n\nerr_free:\n\tkmem_cache_free(slab_requests, rq);\nerr_unreserve:\n\tintel_context_unpin(ce);\n\treturn ERR_PTR(ret);\n}\n\nstruct i915_request *\ni915_request_create(struct intel_context *ce)\n{\n\tstruct i915_request *rq;\n\tstruct intel_timeline *tl;\n\n\ttl = intel_context_timeline_lock(ce);\n\tif (IS_ERR(tl))\n\t\treturn ERR_CAST(tl);\n\n\t \n\trq = list_first_entry(&tl->requests, typeof(*rq), link);\n\tif (!list_is_last(&rq->link, &tl->requests))\n\t\ti915_request_retire(rq);\n\n\tintel_context_enter(ce);\n\trq = __i915_request_create(ce, GFP_KERNEL);\n\tintel_context_exit(ce);  \n\tif (IS_ERR(rq))\n\t\tgoto err_unlock;\n\n\t \n\trq->cookie = lockdep_pin_lock(&tl->mutex);\n\n\treturn rq;\n\nerr_unlock:\n\tintel_context_timeline_unlock(tl);\n\treturn rq;\n}\n\nstatic int\ni915_request_await_start(struct i915_request *rq, struct i915_request *signal)\n{\n\tstruct dma_fence *fence;\n\tint err;\n\n\tif (i915_request_timeline(rq) == rcu_access_pointer(signal->timeline))\n\t\treturn 0;\n\n\tif (i915_request_started(signal))\n\t\treturn 0;\n\n\t \n\tfence = NULL;\n\trcu_read_lock();\n\tdo {\n\t\tstruct list_head *pos = READ_ONCE(signal->link.prev);\n\t\tstruct i915_request *prev;\n\n\t\t \n\t\tif (unlikely(__i915_request_has_started(signal)))\n\t\t\tbreak;\n\n\t\t \n\t\tif (pos == &rcu_dereference(signal->timeline)->requests)\n\t\t\tbreak;\n\n\t\t \n\t\tprev = list_entry(pos, typeof(*prev), link);\n\t\tif (!i915_request_get_rcu(prev))\n\t\t\tbreak;\n\n\t\t \n\t\tif (unlikely(READ_ONCE(prev->link.next) != &signal->link)) {\n\t\t\ti915_request_put(prev);\n\t\t\tbreak;\n\t\t}\n\n\t\tfence = &prev->fence;\n\t} while (0);\n\trcu_read_unlock();\n\tif (!fence)\n\t\treturn 0;\n\n\terr = 0;\n\tif (!intel_timeline_sync_is_later(i915_request_timeline(rq), fence))\n\t\terr = i915_sw_fence_await_dma_fence(&rq->submit,\n\t\t\t\t\t\t    fence, 0,\n\t\t\t\t\t\t    I915_FENCE_GFP);\n\tdma_fence_put(fence);\n\n\treturn err;\n}\n\nstatic intel_engine_mask_t\nalready_busywaiting(struct i915_request *rq)\n{\n\t \n\treturn rq->sched.semaphores | READ_ONCE(rq->engine->saturated);\n}\n\nstatic int\n__emit_semaphore_wait(struct i915_request *to,\n\t\t      struct i915_request *from,\n\t\t      u32 seqno)\n{\n\tconst int has_token = GRAPHICS_VER(to->engine->i915) >= 12;\n\tu32 hwsp_offset;\n\tint len, err;\n\tu32 *cs;\n\n\tGEM_BUG_ON(GRAPHICS_VER(to->engine->i915) < 8);\n\tGEM_BUG_ON(i915_request_has_initial_breadcrumb(to));\n\n\t \n\terr = intel_timeline_read_hwsp(from, to, &hwsp_offset);\n\tif (err)\n\t\treturn err;\n\n\tlen = 4;\n\tif (has_token)\n\t\tlen += 2;\n\n\tcs = intel_ring_begin(to, len);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\t*cs++ = (MI_SEMAPHORE_WAIT |\n\t\t MI_SEMAPHORE_GLOBAL_GTT |\n\t\t MI_SEMAPHORE_POLL |\n\t\t MI_SEMAPHORE_SAD_GTE_SDD) +\n\t\thas_token;\n\t*cs++ = seqno;\n\t*cs++ = hwsp_offset;\n\t*cs++ = 0;\n\tif (has_token) {\n\t\t*cs++ = 0;\n\t\t*cs++ = MI_NOOP;\n\t}\n\n\tintel_ring_advance(to, cs);\n\treturn 0;\n}\n\nstatic bool\ncan_use_semaphore_wait(struct i915_request *to, struct i915_request *from)\n{\n\treturn to->engine->gt->ggtt == from->engine->gt->ggtt;\n}\n\nstatic int\nemit_semaphore_wait(struct i915_request *to,\n\t\t    struct i915_request *from,\n\t\t    gfp_t gfp)\n{\n\tconst intel_engine_mask_t mask = READ_ONCE(from->engine)->mask;\n\tstruct i915_sw_fence *wait = &to->submit;\n\n\tif (!can_use_semaphore_wait(to, from))\n\t\tgoto await_fence;\n\n\tif (!intel_context_use_semaphores(to->context))\n\t\tgoto await_fence;\n\n\tif (i915_request_has_initial_breadcrumb(to))\n\t\tgoto await_fence;\n\n\t \n\tif (from->sched.flags & I915_SCHED_HAS_EXTERNAL_CHAIN)\n\t\tgoto await_fence;\n\n\t \n\tif (already_busywaiting(to) & mask)\n\t\tgoto await_fence;\n\n\tif (i915_request_await_start(to, from) < 0)\n\t\tgoto await_fence;\n\n\t \n\tif (__await_execution(to, from, gfp))\n\t\tgoto await_fence;\n\n\tif (__emit_semaphore_wait(to, from, from->fence.seqno))\n\t\tgoto await_fence;\n\n\tto->sched.semaphores |= mask;\n\twait = &to->semaphore;\n\nawait_fence:\n\treturn i915_sw_fence_await_dma_fence(wait,\n\t\t\t\t\t     &from->fence, 0,\n\t\t\t\t\t     I915_FENCE_GFP);\n}\n\nstatic bool intel_timeline_sync_has_start(struct intel_timeline *tl,\n\t\t\t\t\t  struct dma_fence *fence)\n{\n\treturn __intel_timeline_sync_is_later(tl,\n\t\t\t\t\t      fence->context,\n\t\t\t\t\t      fence->seqno - 1);\n}\n\nstatic int intel_timeline_sync_set_start(struct intel_timeline *tl,\n\t\t\t\t\t const struct dma_fence *fence)\n{\n\treturn __intel_timeline_sync_set(tl, fence->context, fence->seqno - 1);\n}\n\nstatic int\n__i915_request_await_execution(struct i915_request *to,\n\t\t\t       struct i915_request *from)\n{\n\tint err;\n\n\tGEM_BUG_ON(intel_context_is_barrier(from->context));\n\n\t \n\terr = __await_execution(to, from, I915_FENCE_GFP);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (intel_timeline_sync_has_start(i915_request_timeline(to),\n\t\t\t\t\t  &from->fence))\n\t\treturn 0;\n\n\t \n\terr = i915_request_await_start(to, from);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tif (can_use_semaphore_wait(to, from) &&\n\t    intel_engine_has_semaphores(to->engine) &&\n\t    !i915_request_has_initial_breadcrumb(to)) {\n\t\terr = __emit_semaphore_wait(to, from, from->fence.seqno - 1);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\t \n\tif (to->engine->sched_engine->schedule) {\n\t\terr = i915_sched_node_add_dependency(&to->sched,\n\t\t\t\t\t\t     &from->sched,\n\t\t\t\t\t\t     I915_DEPENDENCY_WEAK);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn intel_timeline_sync_set_start(i915_request_timeline(to),\n\t\t\t\t\t     &from->fence);\n}\n\nstatic void mark_external(struct i915_request *rq)\n{\n\t \n\trq->sched.flags |= I915_SCHED_HAS_EXTERNAL_CHAIN;\n}\n\nstatic int\n__i915_request_await_external(struct i915_request *rq, struct dma_fence *fence)\n{\n\tmark_external(rq);\n\treturn i915_sw_fence_await_dma_fence(&rq->submit, fence,\n\t\t\t\t\t     i915_fence_context_timeout(rq->i915,\n\t\t\t\t\t\t\t\t\tfence->context),\n\t\t\t\t\t     I915_FENCE_GFP);\n}\n\nstatic int\ni915_request_await_external(struct i915_request *rq, struct dma_fence *fence)\n{\n\tstruct dma_fence *iter;\n\tint err = 0;\n\n\tif (!to_dma_fence_chain(fence))\n\t\treturn __i915_request_await_external(rq, fence);\n\n\tdma_fence_chain_for_each(iter, fence) {\n\t\tstruct dma_fence_chain *chain = to_dma_fence_chain(iter);\n\n\t\tif (!dma_fence_is_i915(chain->fence)) {\n\t\t\terr = __i915_request_await_external(rq, iter);\n\t\t\tbreak;\n\t\t}\n\n\t\terr = i915_request_await_dma_fence(rq, chain->fence);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t}\n\n\tdma_fence_put(iter);\n\treturn err;\n}\n\nstatic inline bool is_parallel_rq(struct i915_request *rq)\n{\n\treturn intel_context_is_parallel(rq->context);\n}\n\nstatic inline struct intel_context *request_to_parent(struct i915_request *rq)\n{\n\treturn intel_context_to_parent(rq->context);\n}\n\nstatic bool is_same_parallel_context(struct i915_request *to,\n\t\t\t\t     struct i915_request *from)\n{\n\tif (is_parallel_rq(to))\n\t\treturn request_to_parent(to) == request_to_parent(from);\n\n\treturn false;\n}\n\nint\ni915_request_await_execution(struct i915_request *rq,\n\t\t\t     struct dma_fence *fence)\n{\n\tstruct dma_fence **child = &fence;\n\tunsigned int nchild = 1;\n\tint ret;\n\n\tif (dma_fence_is_array(fence)) {\n\t\tstruct dma_fence_array *array = to_dma_fence_array(fence);\n\n\t\t \n\n\t\tchild = array->fences;\n\t\tnchild = array->num_fences;\n\t\tGEM_BUG_ON(!nchild);\n\t}\n\n\tdo {\n\t\tfence = *child++;\n\t\tif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))\n\t\t\tcontinue;\n\n\t\tif (fence->context == rq->fence.context)\n\t\t\tcontinue;\n\n\t\t \n\n\t\tif (dma_fence_is_i915(fence)) {\n\t\t\tif (is_same_parallel_context(rq, to_request(fence)))\n\t\t\t\tcontinue;\n\t\t\tret = __i915_request_await_execution(rq,\n\t\t\t\t\t\t\t     to_request(fence));\n\t\t} else {\n\t\t\tret = i915_request_await_external(rq, fence);\n\t\t}\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t} while (--nchild);\n\n\treturn 0;\n}\n\nstatic int\nawait_request_submit(struct i915_request *to, struct i915_request *from)\n{\n\t \n\tif (to->engine == READ_ONCE(from->engine))\n\t\treturn i915_sw_fence_await_sw_fence_gfp(&to->submit,\n\t\t\t\t\t\t\t&from->submit,\n\t\t\t\t\t\t\tI915_FENCE_GFP);\n\telse\n\t\treturn __i915_request_await_execution(to, from);\n}\n\nstatic int\ni915_request_await_request(struct i915_request *to, struct i915_request *from)\n{\n\tint ret;\n\n\tGEM_BUG_ON(to == from);\n\tGEM_BUG_ON(to->timeline == from->timeline);\n\n\tif (i915_request_completed(from)) {\n\t\ti915_sw_fence_set_error_once(&to->submit, from->fence.error);\n\t\treturn 0;\n\t}\n\n\tif (to->engine->sched_engine->schedule) {\n\t\tret = i915_sched_node_add_dependency(&to->sched,\n\t\t\t\t\t\t     &from->sched,\n\t\t\t\t\t\t     I915_DEPENDENCY_EXTERNAL);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (!intel_engine_uses_guc(to->engine) &&\n\t    is_power_of_2(to->execution_mask | READ_ONCE(from->execution_mask)))\n\t\tret = await_request_submit(to, from);\n\telse\n\t\tret = emit_semaphore_wait(to, from, I915_FENCE_GFP);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint\ni915_request_await_dma_fence(struct i915_request *rq, struct dma_fence *fence)\n{\n\tstruct dma_fence **child = &fence;\n\tunsigned int nchild = 1;\n\tint ret;\n\n\t \n\tif (dma_fence_is_array(fence)) {\n\t\tstruct dma_fence_array *array = to_dma_fence_array(fence);\n\n\t\tchild = array->fences;\n\t\tnchild = array->num_fences;\n\t\tGEM_BUG_ON(!nchild);\n\t}\n\n\tdo {\n\t\tfence = *child++;\n\t\tif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (fence->context == rq->fence.context)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (fence->context &&\n\t\t    intel_timeline_sync_is_later(i915_request_timeline(rq),\n\t\t\t\t\t\t fence))\n\t\t\tcontinue;\n\n\t\tif (dma_fence_is_i915(fence)) {\n\t\t\tif (is_same_parallel_context(rq, to_request(fence)))\n\t\t\t\tcontinue;\n\t\t\tret = i915_request_await_request(rq, to_request(fence));\n\t\t} else {\n\t\t\tret = i915_request_await_external(rq, fence);\n\t\t}\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\t \n\t\tif (fence->context)\n\t\t\tintel_timeline_sync_set(i915_request_timeline(rq),\n\t\t\t\t\t\tfence);\n\t} while (--nchild);\n\n\treturn 0;\n}\n\n \nint i915_request_await_deps(struct i915_request *rq, const struct i915_deps *deps)\n{\n\tint i, err;\n\n\tfor (i = 0; i < deps->num_deps; ++i) {\n\t\terr = i915_request_await_dma_fence(rq, deps->fences[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nint\ni915_request_await_object(struct i915_request *to,\n\t\t\t  struct drm_i915_gem_object *obj,\n\t\t\t  bool write)\n{\n\tstruct dma_resv_iter cursor;\n\tstruct dma_fence *fence;\n\tint ret = 0;\n\n\tdma_resv_for_each_fence(&cursor, obj->base.resv,\n\t\t\t\tdma_resv_usage_rw(write), fence) {\n\t\tret = i915_request_await_dma_fence(to, fence);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void i915_request_await_huc(struct i915_request *rq)\n{\n\tstruct intel_huc *huc = &rq->context->engine->gt->uc.huc;\n\n\t \n\tif (!rcu_access_pointer(rq->context->gem_context))\n\t\treturn;\n\n\tif (intel_huc_wait_required(huc))\n\t\ti915_sw_fence_await_sw_fence(&rq->submit,\n\t\t\t\t\t     &huc->delayed_load.fence,\n\t\t\t\t\t     &rq->hucq);\n}\n\nstatic struct i915_request *\n__i915_request_ensure_parallel_ordering(struct i915_request *rq,\n\t\t\t\t\tstruct intel_timeline *timeline)\n{\n\tstruct i915_request *prev;\n\n\tGEM_BUG_ON(!is_parallel_rq(rq));\n\n\tprev = request_to_parent(rq)->parallel.last_rq;\n\tif (prev) {\n\t\tif (!__i915_request_is_complete(prev)) {\n\t\t\ti915_sw_fence_await_sw_fence(&rq->submit,\n\t\t\t\t\t\t     &prev->submit,\n\t\t\t\t\t\t     &rq->submitq);\n\n\t\t\tif (rq->engine->sched_engine->schedule)\n\t\t\t\t__i915_sched_node_add_dependency(&rq->sched,\n\t\t\t\t\t\t\t\t &prev->sched,\n\t\t\t\t\t\t\t\t &rq->dep,\n\t\t\t\t\t\t\t\t 0);\n\t\t}\n\t\ti915_request_put(prev);\n\t}\n\n\trequest_to_parent(rq)->parallel.last_rq = i915_request_get(rq);\n\n\t \n\treturn to_request(__i915_active_fence_set(&timeline->last_request,\n\t\t\t\t\t\t  &rq->fence));\n}\n\nstatic struct i915_request *\n__i915_request_ensure_ordering(struct i915_request *rq,\n\t\t\t       struct intel_timeline *timeline)\n{\n\tstruct i915_request *prev;\n\n\tGEM_BUG_ON(is_parallel_rq(rq));\n\n\tprev = to_request(__i915_active_fence_set(&timeline->last_request,\n\t\t\t\t\t\t  &rq->fence));\n\n\tif (prev && !__i915_request_is_complete(prev)) {\n\t\tbool uses_guc = intel_engine_uses_guc(rq->engine);\n\t\tbool pow2 = is_power_of_2(READ_ONCE(prev->engine)->mask |\n\t\t\t\t\t  rq->engine->mask);\n\t\tbool same_context = prev->context == rq->context;\n\n\t\t \n\t\tGEM_BUG_ON(same_context &&\n\t\t\t   i915_seqno_passed(prev->fence.seqno,\n\t\t\t\t\t     rq->fence.seqno));\n\n\t\tif ((same_context && uses_guc) || (!uses_guc && pow2))\n\t\t\ti915_sw_fence_await_sw_fence(&rq->submit,\n\t\t\t\t\t\t     &prev->submit,\n\t\t\t\t\t\t     &rq->submitq);\n\t\telse\n\t\t\t__i915_sw_fence_await_dma_fence(&rq->submit,\n\t\t\t\t\t\t\t&prev->fence,\n\t\t\t\t\t\t\t&rq->dmaq);\n\t\tif (rq->engine->sched_engine->schedule)\n\t\t\t__i915_sched_node_add_dependency(&rq->sched,\n\t\t\t\t\t\t\t &prev->sched,\n\t\t\t\t\t\t\t &rq->dep,\n\t\t\t\t\t\t\t 0);\n\t}\n\n\t \n\treturn prev;\n}\n\nstatic struct i915_request *\n__i915_request_add_to_timeline(struct i915_request *rq)\n{\n\tstruct intel_timeline *timeline = i915_request_timeline(rq);\n\tstruct i915_request *prev;\n\n\t \n\tif (rq->engine->class == VIDEO_DECODE_CLASS)\n\t\ti915_request_await_huc(rq);\n\n\t \n\tif (likely(!is_parallel_rq(rq)))\n\t\tprev = __i915_request_ensure_ordering(rq, timeline);\n\telse\n\t\tprev = __i915_request_ensure_parallel_ordering(rq, timeline);\n\tif (prev)\n\t\ti915_request_put(prev);\n\n\t \n\tGEM_BUG_ON(timeline->seqno != rq->fence.seqno);\n\n\treturn prev;\n}\n\n \nstruct i915_request *__i915_request_commit(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tstruct intel_ring *ring = rq->ring;\n\tu32 *cs;\n\n\tRQ_TRACE(rq, \"\\n\");\n\n\t \n\tGEM_BUG_ON(rq->reserved_space > ring->space);\n\trq->reserved_space = 0;\n\trq->emitted_jiffies = jiffies;\n\n\t \n\tcs = intel_ring_begin(rq, engine->emit_fini_breadcrumb_dw);\n\tGEM_BUG_ON(IS_ERR(cs));\n\trq->postfix = intel_ring_offset(rq, cs);\n\n\treturn __i915_request_add_to_timeline(rq);\n}\n\nvoid __i915_request_queue_bh(struct i915_request *rq)\n{\n\ti915_sw_fence_commit(&rq->semaphore);\n\ti915_sw_fence_commit(&rq->submit);\n}\n\nvoid __i915_request_queue(struct i915_request *rq,\n\t\t\t  const struct i915_sched_attr *attr)\n{\n\t \n\tif (attr && rq->engine->sched_engine->schedule)\n\t\trq->engine->sched_engine->schedule(rq, attr);\n\n\tlocal_bh_disable();\n\t__i915_request_queue_bh(rq);\n\tlocal_bh_enable();  \n}\n\nvoid i915_request_add(struct i915_request *rq)\n{\n\tstruct intel_timeline * const tl = i915_request_timeline(rq);\n\tstruct i915_sched_attr attr = {};\n\tstruct i915_gem_context *ctx;\n\n\tlockdep_assert_held(&tl->mutex);\n\tlockdep_unpin_lock(&tl->mutex, rq->cookie);\n\n\ttrace_i915_request_add(rq);\n\t__i915_request_commit(rq);\n\n\t \n\trcu_read_lock();\n\tctx = rcu_dereference(rq->context->gem_context);\n\tif (ctx)\n\t\tattr = ctx->sched;\n\trcu_read_unlock();\n\n\t__i915_request_queue(rq, &attr);\n\n\tmutex_unlock(&tl->mutex);\n}\n\nstatic unsigned long local_clock_ns(unsigned int *cpu)\n{\n\tunsigned long t;\n\n\t \n\t*cpu = get_cpu();\n\tt = local_clock();\n\tput_cpu();\n\n\treturn t;\n}\n\nstatic bool busywait_stop(unsigned long timeout, unsigned int cpu)\n{\n\tunsigned int this_cpu;\n\n\tif (time_after(local_clock_ns(&this_cpu), timeout))\n\t\treturn true;\n\n\treturn this_cpu != cpu;\n}\n\nstatic bool __i915_spin_request(struct i915_request * const rq, int state)\n{\n\tunsigned long timeout_ns;\n\tunsigned int cpu;\n\n\t \n\tif (!i915_request_is_running(rq))\n\t\treturn false;\n\n\t \n\n\ttimeout_ns = READ_ONCE(rq->engine->props.max_busywait_duration_ns);\n\ttimeout_ns += local_clock_ns(&cpu);\n\tdo {\n\t\tif (dma_fence_is_signaled(&rq->fence))\n\t\t\treturn true;\n\n\t\tif (signal_pending_state(state, current))\n\t\t\tbreak;\n\n\t\tif (busywait_stop(timeout_ns, cpu))\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t} while (!need_resched());\n\n\treturn false;\n}\n\nstruct request_wait {\n\tstruct dma_fence_cb cb;\n\tstruct task_struct *tsk;\n};\n\nstatic void request_wait_wake(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tstruct request_wait *wait = container_of(cb, typeof(*wait), cb);\n\n\twake_up_process(fetch_and_zero(&wait->tsk));\n}\n\n \nlong i915_request_wait_timeout(struct i915_request *rq,\n\t\t\t       unsigned int flags,\n\t\t\t       long timeout)\n{\n\tconst int state = flags & I915_WAIT_INTERRUPTIBLE ?\n\t\tTASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE;\n\tstruct request_wait wait;\n\n\tmight_sleep();\n\tGEM_BUG_ON(timeout < 0);\n\n\tif (dma_fence_is_signaled(&rq->fence))\n\t\treturn timeout ?: 1;\n\n\tif (!timeout)\n\t\treturn -ETIME;\n\n\ttrace_i915_request_wait_begin(rq, flags);\n\n\t \n\tmutex_acquire(&rq->engine->gt->reset.mutex.dep_map, 0, 0, _THIS_IP_);\n\n\t \n\tif (CONFIG_DRM_I915_MAX_REQUEST_BUSYWAIT &&\n\t    __i915_spin_request(rq, state))\n\t\tgoto out;\n\n\t \n\tif (flags & I915_WAIT_PRIORITY && !i915_request_started(rq))\n\t\tintel_rps_boost(rq);\n\n\twait.tsk = current;\n\tif (dma_fence_add_callback(&rq->fence, &wait.cb, request_wait_wake))\n\t\tgoto out;\n\n\t \n\tif (i915_request_is_ready(rq))\n\t\t__intel_engine_flush_submission(rq->engine, false);\n\n\tfor (;;) {\n\t\tset_current_state(state);\n\n\t\tif (dma_fence_is_signaled(&rq->fence))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(state, current)) {\n\t\t\ttimeout = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!timeout) {\n\t\t\ttimeout = -ETIME;\n\t\t\tbreak;\n\t\t}\n\n\t\ttimeout = io_schedule_timeout(timeout);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\n\tif (READ_ONCE(wait.tsk))\n\t\tdma_fence_remove_callback(&rq->fence, &wait.cb);\n\tGEM_BUG_ON(!list_empty(&wait.cb.node));\n\nout:\n\tmutex_release(&rq->engine->gt->reset.mutex.dep_map, _THIS_IP_);\n\ttrace_i915_request_wait_end(rq);\n\treturn timeout;\n}\n\n \nlong i915_request_wait(struct i915_request *rq,\n\t\t       unsigned int flags,\n\t\t       long timeout)\n{\n\tlong ret = i915_request_wait_timeout(rq, flags, timeout);\n\n\tif (!ret)\n\t\treturn -ETIME;\n\n\tif (ret > 0 && !timeout)\n\t\treturn 0;\n\n\treturn ret;\n}\n\nstatic int print_sched_attr(const struct i915_sched_attr *attr,\n\t\t\t    char *buf, int x, int len)\n{\n\tif (attr->priority == I915_PRIORITY_INVALID)\n\t\treturn x;\n\n\tx += snprintf(buf + x, len - x,\n\t\t      \" prio=%d\", attr->priority);\n\n\treturn x;\n}\n\nstatic char queue_status(const struct i915_request *rq)\n{\n\tif (i915_request_is_active(rq))\n\t\treturn 'E';\n\n\tif (i915_request_is_ready(rq))\n\t\treturn intel_engine_is_virtual(rq->engine) ? 'V' : 'R';\n\n\treturn 'U';\n}\n\nstatic const char *run_status(const struct i915_request *rq)\n{\n\tif (__i915_request_is_complete(rq))\n\t\treturn \"!\";\n\n\tif (__i915_request_has_started(rq))\n\t\treturn \"*\";\n\n\tif (!i915_sw_fence_signaled(&rq->semaphore))\n\t\treturn \"&\";\n\n\treturn \"\";\n}\n\nstatic const char *fence_status(const struct i915_request *rq)\n{\n\tif (test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &rq->fence.flags))\n\t\treturn \"+\";\n\n\tif (test_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT, &rq->fence.flags))\n\t\treturn \"-\";\n\n\treturn \"\";\n}\n\nvoid i915_request_show(struct drm_printer *m,\n\t\t       const struct i915_request *rq,\n\t\t       const char *prefix,\n\t\t       int indent)\n{\n\tconst char *name = rq->fence.ops->get_timeline_name((struct dma_fence *)&rq->fence);\n\tchar buf[80] = \"\";\n\tint x = 0;\n\n\t \n\n\tx = print_sched_attr(&rq->sched.attr, buf, x, sizeof(buf));\n\n\tdrm_printf(m, \"%s%.*s%c %llx:%lld%s%s %s @ %dms: %s\\n\",\n\t\t   prefix, indent, \"                \",\n\t\t   queue_status(rq),\n\t\t   rq->fence.context, rq->fence.seqno,\n\t\t   run_status(rq),\n\t\t   fence_status(rq),\n\t\t   buf,\n\t\t   jiffies_to_msecs(jiffies - rq->emitted_jiffies),\n\t\t   name);\n}\n\nstatic bool engine_match_ring(struct intel_engine_cs *engine, struct i915_request *rq)\n{\n\tu32 ring = ENGINE_READ(engine, RING_START);\n\n\treturn ring == i915_ggtt_offset(rq->ring->vma);\n}\n\nstatic bool match_ring(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine;\n\tbool found;\n\tint i;\n\n\tif (!intel_engine_is_virtual(rq->engine))\n\t\treturn engine_match_ring(rq->engine, rq);\n\n\tfound = false;\n\ti = 0;\n\twhile ((engine = intel_engine_get_sibling(rq->engine, i++))) {\n\t\tfound = engine_match_ring(engine, rq);\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\n\treturn found;\n}\n\nenum i915_request_state i915_test_request_state(struct i915_request *rq)\n{\n\tif (i915_request_completed(rq))\n\t\treturn I915_REQUEST_COMPLETE;\n\n\tif (!i915_request_started(rq))\n\t\treturn I915_REQUEST_PENDING;\n\n\tif (match_ring(rq))\n\t\treturn I915_REQUEST_ACTIVE;\n\n\treturn I915_REQUEST_QUEUED;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/mock_request.c\"\n#include \"selftests/i915_request.c\"\n#endif\n\nvoid i915_request_module_exit(void)\n{\n\tkmem_cache_destroy(slab_execute_cbs);\n\tkmem_cache_destroy(slab_requests);\n}\n\nint __init i915_request_module_init(void)\n{\n\tslab_requests =\n\t\tkmem_cache_create(\"i915_request\",\n\t\t\t\t  sizeof(struct i915_request),\n\t\t\t\t  __alignof__(struct i915_request),\n\t\t\t\t  SLAB_HWCACHE_ALIGN |\n\t\t\t\t  SLAB_RECLAIM_ACCOUNT |\n\t\t\t\t  SLAB_TYPESAFE_BY_RCU,\n\t\t\t\t  __i915_request_ctor);\n\tif (!slab_requests)\n\t\treturn -ENOMEM;\n\n\tslab_execute_cbs = KMEM_CACHE(execute_cb,\n\t\t\t\t\t     SLAB_HWCACHE_ALIGN |\n\t\t\t\t\t     SLAB_RECLAIM_ACCOUNT |\n\t\t\t\t\t     SLAB_TYPESAFE_BY_RCU);\n\tif (!slab_execute_cbs)\n\t\tgoto err_requests;\n\n\treturn 0;\n\nerr_requests:\n\tkmem_cache_destroy(slab_requests);\n\treturn -ENOMEM;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}