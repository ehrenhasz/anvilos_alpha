{
  "module_name": "i915_request.c",
  "hash_id": "b6427470edc5df941b783b2be143c04e711cf3792fe86042cbc5db66cced445b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/selftests/i915_request.c",
  "human_readable_source": " \n\n#include <linux/prime_numbers.h>\n#include <linux/pm_qos.h>\n#include <linux/sort.h>\n\n#include \"gem/i915_gem_internal.h\"\n#include \"gem/i915_gem_pm.h\"\n#include \"gem/selftests/mock_context.h\"\n\n#include \"gt/intel_engine_heartbeat.h\"\n#include \"gt/intel_engine_pm.h\"\n#include \"gt/intel_engine_user.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_clock_utils.h\"\n#include \"gt/intel_gt_requests.h\"\n#include \"gt/selftest_engine_heartbeat.h\"\n\n#include \"i915_random.h\"\n#include \"i915_selftest.h\"\n#include \"igt_flush_test.h\"\n#include \"igt_live_test.h\"\n#include \"igt_spinner.h\"\n#include \"lib_sw_fence.h\"\n\n#include \"mock_drm.h\"\n#include \"mock_gem_device.h\"\n\nstatic unsigned int num_uabi_engines(struct drm_i915_private *i915)\n{\n\tstruct intel_engine_cs *engine;\n\tunsigned int count;\n\n\tcount = 0;\n\tfor_each_uabi_engine(engine, i915)\n\t\tcount++;\n\n\treturn count;\n}\n\nstatic struct intel_engine_cs *rcs0(struct drm_i915_private *i915)\n{\n\treturn intel_engine_lookup_user(i915, I915_ENGINE_CLASS_RENDER, 0);\n}\n\nstatic int igt_add_request(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct i915_request *request;\n\n\t \n\n\trequest = mock_request(rcs0(i915)->kernel_context, HZ / 10);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\ti915_request_add(request);\n\n\treturn 0;\n}\n\nstatic int igt_wait_request(void *arg)\n{\n\tconst long T = HZ / 4;\n\tstruct drm_i915_private *i915 = arg;\n\tstruct i915_request *request;\n\tint err = -EINVAL;\n\n\t \n\n\trequest = mock_request(rcs0(i915)->kernel_context, T);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\ti915_request_get(request);\n\n\tif (i915_request_wait(request, 0, 0) != -ETIME) {\n\t\tpr_err(\"request wait (busy query) succeeded (expected timeout before submit!)\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_wait(request, 0, T) != -ETIME) {\n\t\tpr_err(\"request wait succeeded (expected timeout before submit!)\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_completed(request)) {\n\t\tpr_err(\"request completed before submit!!\\n\");\n\t\tgoto out_request;\n\t}\n\n\ti915_request_add(request);\n\n\tif (i915_request_wait(request, 0, 0) != -ETIME) {\n\t\tpr_err(\"request wait (busy query) succeeded (expected timeout after submit!)\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_completed(request)) {\n\t\tpr_err(\"request completed immediately!\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_wait(request, 0, T / 2) != -ETIME) {\n\t\tpr_err(\"request wait succeeded (expected timeout!)\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_wait(request, 0, T) == -ETIME) {\n\t\tpr_err(\"request wait timed out!\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (!i915_request_completed(request)) {\n\t\tpr_err(\"request not complete after waiting!\\n\");\n\t\tgoto out_request;\n\t}\n\n\tif (i915_request_wait(request, 0, T) == -ETIME) {\n\t\tpr_err(\"request wait timed out when already complete!\\n\");\n\t\tgoto out_request;\n\t}\n\n\terr = 0;\nout_request:\n\ti915_request_put(request);\n\tmock_device_flush(i915);\n\treturn err;\n}\n\nstatic int igt_fence_wait(void *arg)\n{\n\tconst long T = HZ / 4;\n\tstruct drm_i915_private *i915 = arg;\n\tstruct i915_request *request;\n\tint err = -EINVAL;\n\n\t \n\n\trequest = mock_request(rcs0(i915)->kernel_context, T);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\tif (dma_fence_wait_timeout(&request->fence, false, T) != -ETIME) {\n\t\tpr_err(\"fence wait success before submit (expected timeout)!\\n\");\n\t\tgoto out;\n\t}\n\n\ti915_request_add(request);\n\n\tif (dma_fence_is_signaled(&request->fence)) {\n\t\tpr_err(\"fence signaled immediately!\\n\");\n\t\tgoto out;\n\t}\n\n\tif (dma_fence_wait_timeout(&request->fence, false, T / 2) != -ETIME) {\n\t\tpr_err(\"fence wait success after submit (expected timeout)!\\n\");\n\t\tgoto out;\n\t}\n\n\tif (dma_fence_wait_timeout(&request->fence, false, T) <= 0) {\n\t\tpr_err(\"fence wait timed out (expected success)!\\n\");\n\t\tgoto out;\n\t}\n\n\tif (!dma_fence_is_signaled(&request->fence)) {\n\t\tpr_err(\"fence unsignaled after waiting!\\n\");\n\t\tgoto out;\n\t}\n\n\tif (dma_fence_wait_timeout(&request->fence, false, T) <= 0) {\n\t\tpr_err(\"fence wait timed out when complete (expected success)!\\n\");\n\t\tgoto out;\n\t}\n\n\terr = 0;\nout:\n\tmock_device_flush(i915);\n\treturn err;\n}\n\nstatic int igt_request_rewind(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct i915_request *request, *vip;\n\tstruct i915_gem_context *ctx[2];\n\tstruct intel_context *ce;\n\tint err = -EINVAL;\n\n\tctx[0] = mock_context(i915, \"A\");\n\tif (!ctx[0]) {\n\t\terr = -ENOMEM;\n\t\tgoto err_ctx_0;\n\t}\n\n\tce = i915_gem_context_get_engine(ctx[0], RCS0);\n\tGEM_BUG_ON(IS_ERR(ce));\n\trequest = mock_request(ce, 2 * HZ);\n\tintel_context_put(ce);\n\tif (!request) {\n\t\terr = -ENOMEM;\n\t\tgoto err_context_0;\n\t}\n\n\ti915_request_get(request);\n\ti915_request_add(request);\n\n\tctx[1] = mock_context(i915, \"B\");\n\tif (!ctx[1]) {\n\t\terr = -ENOMEM;\n\t\tgoto err_ctx_1;\n\t}\n\n\tce = i915_gem_context_get_engine(ctx[1], RCS0);\n\tGEM_BUG_ON(IS_ERR(ce));\n\tvip = mock_request(ce, 0);\n\tintel_context_put(ce);\n\tif (!vip) {\n\t\terr = -ENOMEM;\n\t\tgoto err_context_1;\n\t}\n\n\t \n\tif (!mock_cancel_request(request)) {\n\t\tpr_err(\"failed to cancel request (already executed)!\\n\");\n\t\ti915_request_add(vip);\n\t\tgoto err_context_1;\n\t}\n\ti915_request_get(vip);\n\ti915_request_add(vip);\n\trcu_read_lock();\n\trequest->engine->submit_request(request);\n\trcu_read_unlock();\n\n\n\tif (i915_request_wait(vip, 0, HZ) == -ETIME) {\n\t\tpr_err(\"timed out waiting for high priority request\\n\");\n\t\tgoto err;\n\t}\n\n\tif (i915_request_completed(request)) {\n\t\tpr_err(\"low priority request already completed\\n\");\n\t\tgoto err;\n\t}\n\n\terr = 0;\nerr:\n\ti915_request_put(vip);\nerr_context_1:\n\tmock_context_close(ctx[1]);\nerr_ctx_1:\n\ti915_request_put(request);\nerr_context_0:\n\tmock_context_close(ctx[0]);\nerr_ctx_0:\n\tmock_device_flush(i915);\n\treturn err;\n}\n\nstruct smoketest {\n\tstruct intel_engine_cs *engine;\n\tstruct i915_gem_context **contexts;\n\tatomic_long_t num_waits, num_fences;\n\tint ncontexts, max_batch;\n\tstruct i915_request *(*request_alloc)(struct intel_context *ce);\n};\n\nstatic struct i915_request *\n__mock_request_alloc(struct intel_context *ce)\n{\n\treturn mock_request(ce, 0);\n}\n\nstatic struct i915_request *\n__live_request_alloc(struct intel_context *ce)\n{\n\treturn intel_context_create_request(ce);\n}\n\nstruct smoke_thread {\n\tstruct kthread_worker *worker;\n\tstruct kthread_work work;\n\tstruct smoketest *t;\n\tbool stop;\n\tint result;\n};\n\nstatic void __igt_breadcrumbs_smoketest(struct kthread_work *work)\n{\n\tstruct smoke_thread *thread = container_of(work, typeof(*thread), work);\n\tstruct smoketest *t = thread->t;\n\tconst unsigned int max_batch = min(t->ncontexts, t->max_batch) - 1;\n\tconst unsigned int total = 4 * t->ncontexts + 1;\n\tunsigned int num_waits = 0, num_fences = 0;\n\tstruct i915_request **requests;\n\tI915_RND_STATE(prng);\n\tunsigned int *order;\n\tint err = 0;\n\n\t \n\n\trequests = kcalloc(total, sizeof(*requests), GFP_KERNEL);\n\tif (!requests) {\n\t\tthread->result = -ENOMEM;\n\t\treturn;\n\t}\n\n\torder = i915_random_order(total, &prng);\n\tif (!order) {\n\t\terr = -ENOMEM;\n\t\tgoto out_requests;\n\t}\n\n\twhile (!READ_ONCE(thread->stop)) {\n\t\tstruct i915_sw_fence *submit, *wait;\n\t\tunsigned int n, count;\n\n\t\tsubmit = heap_fence_create(GFP_KERNEL);\n\t\tif (!submit) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\twait = heap_fence_create(GFP_KERNEL);\n\t\tif (!wait) {\n\t\t\ti915_sw_fence_commit(submit);\n\t\t\theap_fence_put(submit);\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_random_reorder(order, total, &prng);\n\t\tcount = 1 + i915_prandom_u32_max_state(max_batch, &prng);\n\n\t\tfor (n = 0; n < count; n++) {\n\t\t\tstruct i915_gem_context *ctx =\n\t\t\t\tt->contexts[order[n] % t->ncontexts];\n\t\t\tstruct i915_request *rq;\n\t\t\tstruct intel_context *ce;\n\n\t\t\tce = i915_gem_context_get_engine(ctx, t->engine->legacy_idx);\n\t\t\tGEM_BUG_ON(IS_ERR(ce));\n\t\t\trq = t->request_alloc(ce);\n\t\t\tintel_context_put(ce);\n\t\t\tif (IS_ERR(rq)) {\n\t\t\t\terr = PTR_ERR(rq);\n\t\t\t\tcount = n;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = i915_sw_fence_await_sw_fence_gfp(&rq->submit,\n\t\t\t\t\t\t\t       submit,\n\t\t\t\t\t\t\t       GFP_KERNEL);\n\n\t\t\trequests[n] = i915_request_get(rq);\n\t\t\ti915_request_add(rq);\n\n\t\t\tif (err >= 0)\n\t\t\t\terr = i915_sw_fence_await_dma_fence(wait,\n\t\t\t\t\t\t\t\t    &rq->fence,\n\t\t\t\t\t\t\t\t    0,\n\t\t\t\t\t\t\t\t    GFP_KERNEL);\n\n\t\t\tif (err < 0) {\n\t\t\t\ti915_request_put(rq);\n\t\t\t\tcount = n;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\ti915_sw_fence_commit(submit);\n\t\ti915_sw_fence_commit(wait);\n\n\t\tif (!wait_event_timeout(wait->wait,\n\t\t\t\t\ti915_sw_fence_done(wait),\n\t\t\t\t\t5 * HZ)) {\n\t\t\tstruct i915_request *rq = requests[count - 1];\n\n\t\t\tpr_err(\"waiting for %d/%d fences (last %llx:%lld) on %s timed out!\\n\",\n\t\t\t       atomic_read(&wait->pending), count,\n\t\t\t       rq->fence.context, rq->fence.seqno,\n\t\t\t       t->engine->name);\n\t\t\tGEM_TRACE_DUMP();\n\n\t\t\tintel_gt_set_wedged(t->engine->gt);\n\t\t\tGEM_BUG_ON(!i915_request_completed(rq));\n\t\t\ti915_sw_fence_wait(wait);\n\t\t\terr = -EIO;\n\t\t}\n\n\t\tfor (n = 0; n < count; n++) {\n\t\t\tstruct i915_request *rq = requests[n];\n\n\t\t\tif (!test_bit(DMA_FENCE_FLAG_SIGNALED_BIT,\n\t\t\t\t      &rq->fence.flags)) {\n\t\t\t\tpr_err(\"%llu:%llu was not signaled!\\n\",\n\t\t\t\t       rq->fence.context, rq->fence.seqno);\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\n\t\t\ti915_request_put(rq);\n\t\t}\n\n\t\theap_fence_put(wait);\n\t\theap_fence_put(submit);\n\n\t\tif (err < 0)\n\t\t\tbreak;\n\n\t\tnum_fences += count;\n\t\tnum_waits++;\n\n\t\tcond_resched();\n\t}\n\n\tatomic_long_add(num_fences, &t->num_fences);\n\tatomic_long_add(num_waits, &t->num_waits);\n\n\tkfree(order);\nout_requests:\n\tkfree(requests);\n\tthread->result = err;\n}\n\nstatic int mock_breadcrumbs_smoketest(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct smoketest t = {\n\t\t.engine = rcs0(i915),\n\t\t.ncontexts = 1024,\n\t\t.max_batch = 1024,\n\t\t.request_alloc = __mock_request_alloc\n\t};\n\tunsigned int ncpus = num_online_cpus();\n\tstruct smoke_thread *threads;\n\tunsigned int n;\n\tint ret = 0;\n\n\t \n\n\tthreads = kcalloc(ncpus, sizeof(*threads), GFP_KERNEL);\n\tif (!threads)\n\t\treturn -ENOMEM;\n\n\tt.contexts = kcalloc(t.ncontexts, sizeof(*t.contexts), GFP_KERNEL);\n\tif (!t.contexts) {\n\t\tret = -ENOMEM;\n\t\tgoto out_threads;\n\t}\n\n\tfor (n = 0; n < t.ncontexts; n++) {\n\t\tt.contexts[n] = mock_context(t.engine->i915, \"mock\");\n\t\tif (!t.contexts[n]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_contexts;\n\t\t}\n\t}\n\n\tfor (n = 0; n < ncpus; n++) {\n\t\tstruct kthread_worker *worker;\n\n\t\tworker = kthread_create_worker(0, \"igt/%d\", n);\n\t\tif (IS_ERR(worker)) {\n\t\t\tret = PTR_ERR(worker);\n\t\t\tncpus = n;\n\t\t\tbreak;\n\t\t}\n\n\t\tthreads[n].worker = worker;\n\t\tthreads[n].t = &t;\n\t\tthreads[n].stop = false;\n\t\tthreads[n].result = 0;\n\n\t\tkthread_init_work(&threads[n].work,\n\t\t\t\t  __igt_breadcrumbs_smoketest);\n\t\tkthread_queue_work(worker, &threads[n].work);\n\t}\n\n\tmsleep(jiffies_to_msecs(i915_selftest.timeout_jiffies));\n\n\tfor (n = 0; n < ncpus; n++) {\n\t\tint err;\n\n\t\tWRITE_ONCE(threads[n].stop, true);\n\t\tkthread_flush_work(&threads[n].work);\n\t\terr = READ_ONCE(threads[n].result);\n\t\tif (err < 0 && !ret)\n\t\t\tret = err;\n\n\t\tkthread_destroy_worker(threads[n].worker);\n\t}\n\tpr_info(\"Completed %lu waits for %lu fence across %d cpus\\n\",\n\t\tatomic_long_read(&t.num_waits),\n\t\tatomic_long_read(&t.num_fences),\n\t\tncpus);\n\nout_contexts:\n\tfor (n = 0; n < t.ncontexts; n++) {\n\t\tif (!t.contexts[n])\n\t\t\tbreak;\n\t\tmock_context_close(t.contexts[n]);\n\t}\n\tkfree(t.contexts);\nout_threads:\n\tkfree(threads);\n\treturn ret;\n}\n\nint i915_request_mock_selftests(void)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(igt_add_request),\n\t\tSUBTEST(igt_wait_request),\n\t\tSUBTEST(igt_fence_wait),\n\t\tSUBTEST(igt_request_rewind),\n\t\tSUBTEST(mock_breadcrumbs_smoketest),\n\t};\n\tstruct drm_i915_private *i915;\n\tintel_wakeref_t wakeref;\n\tint err = 0;\n\n\ti915 = mock_gem_device();\n\tif (!i915)\n\t\treturn -ENOMEM;\n\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref)\n\t\terr = i915_subtests(tests, i915);\n\n\tmock_destroy_device(i915);\n\n\treturn err;\n}\n\nstatic int live_nop_request(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_engine_cs *engine;\n\tstruct igt_live_test t;\n\tint err = -ENODEV;\n\n\t \n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tunsigned long n, prime;\n\t\tIGT_TIMEOUT(end_time);\n\t\tktime_t times[2] = {};\n\n\t\terr = igt_live_test_begin(&t, i915, __func__, engine->name);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tintel_engine_pm_get(engine);\n\t\tfor_each_prime_number_from(prime, 1, 8192) {\n\t\t\tstruct i915_request *request = NULL;\n\n\t\t\ttimes[1] = ktime_get_raw();\n\n\t\t\tfor (n = 0; n < prime; n++) {\n\t\t\t\ti915_request_put(request);\n\t\t\t\trequest = i915_request_create(engine->kernel_context);\n\t\t\t\tif (IS_ERR(request))\n\t\t\t\t\treturn PTR_ERR(request);\n\n\t\t\t\t \n\n\t\t\t\ti915_request_get(request);\n\t\t\t\ti915_request_add(request);\n\t\t\t}\n\t\t\ti915_request_wait(request, 0, MAX_SCHEDULE_TIMEOUT);\n\t\t\ti915_request_put(request);\n\n\t\t\ttimes[1] = ktime_sub(ktime_get_raw(), times[1]);\n\t\t\tif (prime == 1)\n\t\t\t\ttimes[0] = times[1];\n\n\t\t\tif (__igt_timeout(end_time, NULL))\n\t\t\t\tbreak;\n\t\t}\n\t\tintel_engine_pm_put(engine);\n\n\t\terr = igt_live_test_end(&t);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tpr_info(\"Request latencies on %s: 1 = %lluns, %lu = %lluns\\n\",\n\t\t\tengine->name,\n\t\t\tktime_to_ns(times[0]),\n\t\t\tprime, div64_u64(ktime_to_ns(times[1]), prime));\n\t}\n\n\treturn err;\n}\n\nstatic int __cancel_inactive(struct intel_engine_cs *engine)\n{\n\tstruct intel_context *ce;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tint err = 0;\n\n\tif (igt_spinner_init(&spin, engine->gt))\n\t\treturn -ENOMEM;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\terr = PTR_ERR(ce);\n\t\tgoto out_spin;\n\t}\n\n\trq = igt_spinner_create_request(&spin, ce, MI_ARB_CHECK);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto out_ce;\n\t}\n\n\tpr_debug(\"%s: Cancelling inactive request\\n\", engine->name);\n\ti915_request_cancel(rq, -EINTR);\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\n\tif (i915_request_wait(rq, 0, HZ / 5) < 0) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"%s: Failed to cancel inactive request\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_rq;\n\t}\n\n\tif (rq->fence.error != -EINTR) {\n\t\tpr_err(\"%s: fence not cancelled (%u)\\n\",\n\t\t       engine->name, rq->fence.error);\n\t\terr = -EINVAL;\n\t}\n\nout_rq:\n\ti915_request_put(rq);\nout_ce:\n\tintel_context_put(ce);\nout_spin:\n\tigt_spinner_fini(&spin);\n\tif (err)\n\t\tpr_err(\"%s: %s error %d\\n\", __func__, engine->name, err);\n\treturn err;\n}\n\nstatic int __cancel_active(struct intel_engine_cs *engine)\n{\n\tstruct intel_context *ce;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tint err = 0;\n\n\tif (igt_spinner_init(&spin, engine->gt))\n\t\treturn -ENOMEM;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\terr = PTR_ERR(ce);\n\t\tgoto out_spin;\n\t}\n\n\trq = igt_spinner_create_request(&spin, ce, MI_ARB_CHECK);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto out_ce;\n\t}\n\n\tpr_debug(\"%s: Cancelling active request\\n\", engine->name);\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\tif (!igt_wait_for_spinner(&spin, rq)) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"Failed to start spinner on %s\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_rq;\n\t}\n\ti915_request_cancel(rq, -EINTR);\n\n\tif (i915_request_wait(rq, 0, HZ / 5) < 0) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"%s: Failed to cancel active request\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_rq;\n\t}\n\n\tif (rq->fence.error != -EINTR) {\n\t\tpr_err(\"%s: fence not cancelled (%u)\\n\",\n\t\t       engine->name, rq->fence.error);\n\t\terr = -EINVAL;\n\t}\n\nout_rq:\n\ti915_request_put(rq);\nout_ce:\n\tintel_context_put(ce);\nout_spin:\n\tigt_spinner_fini(&spin);\n\tif (err)\n\t\tpr_err(\"%s: %s error %d\\n\", __func__, engine->name, err);\n\treturn err;\n}\n\nstatic int __cancel_completed(struct intel_engine_cs *engine)\n{\n\tstruct intel_context *ce;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tint err = 0;\n\n\tif (igt_spinner_init(&spin, engine->gt))\n\t\treturn -ENOMEM;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\terr = PTR_ERR(ce);\n\t\tgoto out_spin;\n\t}\n\n\trq = igt_spinner_create_request(&spin, ce, MI_ARB_CHECK);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto out_ce;\n\t}\n\tigt_spinner_end(&spin);\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\n\tif (i915_request_wait(rq, 0, HZ / 5) < 0) {\n\t\terr = -ETIME;\n\t\tgoto out_rq;\n\t}\n\n\tpr_debug(\"%s: Cancelling completed request\\n\", engine->name);\n\ti915_request_cancel(rq, -EINTR);\n\tif (rq->fence.error) {\n\t\tpr_err(\"%s: fence not cancelled (%u)\\n\",\n\t\t       engine->name, rq->fence.error);\n\t\terr = -EINVAL;\n\t}\n\nout_rq:\n\ti915_request_put(rq);\nout_ce:\n\tintel_context_put(ce);\nout_spin:\n\tigt_spinner_fini(&spin);\n\tif (err)\n\t\tpr_err(\"%s: %s error %d\\n\", __func__, engine->name, err);\n\treturn err;\n}\n\n \nstatic int __cancel_reset(struct drm_i915_private *i915,\n\t\t\t  struct intel_engine_cs *engine)\n{\n\tstruct intel_context *ce;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq, *nop;\n\tunsigned long preempt_timeout_ms;\n\tint err = 0;\n\n\tif (!CONFIG_DRM_I915_PREEMPT_TIMEOUT ||\n\t    !intel_has_reset_engine(engine->gt))\n\t\treturn 0;\n\n\tpreempt_timeout_ms = engine->props.preempt_timeout_ms;\n\tengine->props.preempt_timeout_ms = 100;\n\n\tif (igt_spinner_init(&spin, engine->gt))\n\t\tgoto out_restore;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\terr = PTR_ERR(ce);\n\t\tgoto out_spin;\n\t}\n\n\trq = igt_spinner_create_request(&spin, ce, MI_NOOP);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto out_ce;\n\t}\n\n\tpr_debug(\"%s: Cancelling active non-preemptable request\\n\",\n\t\t engine->name);\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\tif (!igt_wait_for_spinner(&spin, rq)) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"Failed to start spinner on %s\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_rq;\n\t}\n\n\tnop = intel_context_create_request(ce);\n\tif (IS_ERR(nop))\n\t\tgoto out_rq;\n\ti915_request_get(nop);\n\ti915_request_add(nop);\n\n\ti915_request_cancel(rq, -EINTR);\n\n\tif (i915_request_wait(rq, 0, HZ) < 0) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"%s: Failed to cancel hung request\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_nop;\n\t}\n\n\tif (rq->fence.error != -EINTR) {\n\t\tpr_err(\"%s: fence not cancelled (%u)\\n\",\n\t\t       engine->name, rq->fence.error);\n\t\terr = -EINVAL;\n\t\tgoto out_nop;\n\t}\n\n\tif (i915_request_wait(nop, 0, HZ) < 0) {\n\t\tstruct drm_printer p = drm_info_printer(engine->i915->drm.dev);\n\n\t\tpr_err(\"%s: Failed to complete nop request\\n\", engine->name);\n\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\terr = -ETIME;\n\t\tgoto out_nop;\n\t}\n\n\tif (nop->fence.error != 0) {\n\t\tpr_err(\"%s: Nop request errored (%u)\\n\",\n\t\t       engine->name, nop->fence.error);\n\t\terr = -EINVAL;\n\t}\n\nout_nop:\n\ti915_request_put(nop);\nout_rq:\n\ti915_request_put(rq);\nout_ce:\n\tintel_context_put(ce);\nout_spin:\n\tigt_spinner_fini(&spin);\nout_restore:\n\tengine->props.preempt_timeout_ms = preempt_timeout_ms;\n\tif (err)\n\t\tpr_err(\"%s: %s error %d\\n\", __func__, engine->name, err);\n\treturn err;\n}\n\nstatic int live_cancel_request(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_engine_cs *engine;\n\n\t \n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct igt_live_test t;\n\t\tint err, err2;\n\n\t\tif (!intel_engine_has_preemption(engine))\n\t\t\tcontinue;\n\n\t\terr = igt_live_test_begin(&t, i915, __func__, engine->name);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = __cancel_inactive(engine);\n\t\tif (err == 0)\n\t\t\terr = __cancel_active(engine);\n\t\tif (err == 0)\n\t\t\terr = __cancel_completed(engine);\n\n\t\terr2 = igt_live_test_end(&t);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (err2)\n\t\t\treturn err2;\n\n\t\t \n\t\terr = __cancel_reset(i915, engine);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (igt_flush_test(i915))\n\t\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic struct i915_vma *empty_batch(struct intel_gt *gt)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tu32 *cmd;\n\tint err;\n\n\tobj = i915_gem_object_create_internal(gt->i915, PAGE_SIZE);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tcmd = i915_gem_object_pin_map_unlocked(obj, I915_MAP_WC);\n\tif (IS_ERR(cmd)) {\n\t\terr = PTR_ERR(cmd);\n\t\tgoto err;\n\t}\n\n\t*cmd = MI_BATCH_BUFFER_END;\n\n\t__i915_gem_object_flush_map(obj, 0, 64);\n\ti915_gem_object_unpin_map(obj);\n\n\tintel_gt_chipset_flush(gt);\n\n\tvma = i915_vma_instance(obj, gt->vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\terr = PTR_ERR(vma);\n\t\tgoto err;\n\t}\n\n\terr = i915_vma_pin(vma, 0, 0, PIN_USER);\n\tif (err)\n\t\tgoto err;\n\n\t \n\terr = i915_vma_sync(vma);\n\tif (err)\n\t\tgoto err_pin;\n\n\treturn vma;\n\nerr_pin:\n\ti915_vma_unpin(vma);\nerr:\n\ti915_gem_object_put(obj);\n\treturn ERR_PTR(err);\n}\n\nstatic int emit_bb_start(struct i915_request *rq, struct i915_vma *batch)\n{\n\treturn rq->engine->emit_bb_start(rq,\n\t\t\t\t\t i915_vma_offset(batch),\n\t\t\t\t\t i915_vma_size(batch),\n\t\t\t\t\t 0);\n}\n\nstatic struct i915_request *\nempty_request(struct intel_engine_cs *engine,\n\t      struct i915_vma *batch)\n{\n\tstruct i915_request *request;\n\tint err;\n\n\trequest = i915_request_create(engine->kernel_context);\n\tif (IS_ERR(request))\n\t\treturn request;\n\n\terr = emit_bb_start(request, batch);\n\tif (err)\n\t\tgoto out_request;\n\n\ti915_request_get(request);\nout_request:\n\ti915_request_add(request);\n\treturn err ? ERR_PTR(err) : request;\n}\n\nstatic int live_empty_request(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_engine_cs *engine;\n\tstruct igt_live_test t;\n\tint err;\n\n\t \n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tIGT_TIMEOUT(end_time);\n\t\tstruct i915_request *request;\n\t\tstruct i915_vma *batch;\n\t\tunsigned long n, prime;\n\t\tktime_t times[2] = {};\n\n\t\tbatch = empty_batch(engine->gt);\n\t\tif (IS_ERR(batch))\n\t\t\treturn PTR_ERR(batch);\n\n\t\terr = igt_live_test_begin(&t, i915, __func__, engine->name);\n\t\tif (err)\n\t\t\tgoto out_batch;\n\n\t\tintel_engine_pm_get(engine);\n\n\t\t \n\t\trequest = empty_request(engine, batch);\n\t\tif (IS_ERR(request)) {\n\t\t\terr = PTR_ERR(request);\n\t\t\tintel_engine_pm_put(engine);\n\t\t\tgoto out_batch;\n\t\t}\n\t\ti915_request_wait(request, 0, MAX_SCHEDULE_TIMEOUT);\n\n\t\tfor_each_prime_number_from(prime, 1, 8192) {\n\t\t\ttimes[1] = ktime_get_raw();\n\n\t\t\tfor (n = 0; n < prime; n++) {\n\t\t\t\ti915_request_put(request);\n\t\t\t\trequest = empty_request(engine, batch);\n\t\t\t\tif (IS_ERR(request)) {\n\t\t\t\t\terr = PTR_ERR(request);\n\t\t\t\t\tintel_engine_pm_put(engine);\n\t\t\t\t\tgoto out_batch;\n\t\t\t\t}\n\t\t\t}\n\t\t\ti915_request_wait(request, 0, MAX_SCHEDULE_TIMEOUT);\n\n\t\t\ttimes[1] = ktime_sub(ktime_get_raw(), times[1]);\n\t\t\tif (prime == 1)\n\t\t\t\ttimes[0] = times[1];\n\n\t\t\tif (__igt_timeout(end_time, NULL))\n\t\t\t\tbreak;\n\t\t}\n\t\ti915_request_put(request);\n\t\tintel_engine_pm_put(engine);\n\n\t\terr = igt_live_test_end(&t);\n\t\tif (err)\n\t\t\tgoto out_batch;\n\n\t\tpr_info(\"Batch latencies on %s: 1 = %lluns, %lu = %lluns\\n\",\n\t\t\tengine->name,\n\t\t\tktime_to_ns(times[0]),\n\t\t\tprime, div64_u64(ktime_to_ns(times[1]), prime));\nout_batch:\n\t\ti915_vma_unpin(batch);\n\t\ti915_vma_put(batch);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic struct i915_vma *recursive_batch(struct intel_gt *gt)\n{\n\tstruct drm_i915_gem_object *obj;\n\tconst int ver = GRAPHICS_VER(gt->i915);\n\tstruct i915_vma *vma;\n\tu32 *cmd;\n\tint err;\n\n\tobj = i915_gem_object_create_internal(gt->i915, PAGE_SIZE);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tvma = i915_vma_instance(obj, gt->vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\terr = PTR_ERR(vma);\n\t\tgoto err;\n\t}\n\n\terr = i915_vma_pin(vma, 0, 0, PIN_USER);\n\tif (err)\n\t\tgoto err;\n\n\tcmd = i915_gem_object_pin_map_unlocked(obj, I915_MAP_WC);\n\tif (IS_ERR(cmd)) {\n\t\terr = PTR_ERR(cmd);\n\t\tgoto err;\n\t}\n\n\tif (ver >= 8) {\n\t\t*cmd++ = MI_BATCH_BUFFER_START | 1 << 8 | 1;\n\t\t*cmd++ = lower_32_bits(i915_vma_offset(vma));\n\t\t*cmd++ = upper_32_bits(i915_vma_offset(vma));\n\t} else if (ver >= 6) {\n\t\t*cmd++ = MI_BATCH_BUFFER_START | 1 << 8;\n\t\t*cmd++ = lower_32_bits(i915_vma_offset(vma));\n\t} else {\n\t\t*cmd++ = MI_BATCH_BUFFER_START | MI_BATCH_GTT;\n\t\t*cmd++ = lower_32_bits(i915_vma_offset(vma));\n\t}\n\t*cmd++ = MI_BATCH_BUFFER_END;  \n\n\t__i915_gem_object_flush_map(obj, 0, 64);\n\ti915_gem_object_unpin_map(obj);\n\n\tintel_gt_chipset_flush(gt);\n\n\treturn vma;\n\nerr:\n\ti915_gem_object_put(obj);\n\treturn ERR_PTR(err);\n}\n\nstatic int recursive_batch_resolve(struct i915_vma *batch)\n{\n\tu32 *cmd;\n\n\tcmd = i915_gem_object_pin_map_unlocked(batch->obj, I915_MAP_WC);\n\tif (IS_ERR(cmd))\n\t\treturn PTR_ERR(cmd);\n\n\t*cmd = MI_BATCH_BUFFER_END;\n\n\t__i915_gem_object_flush_map(batch->obj, 0, sizeof(*cmd));\n\ti915_gem_object_unpin_map(batch->obj);\n\n\tintel_gt_chipset_flush(batch->vm->gt);\n\n\treturn 0;\n}\n\nstatic int live_all_engines(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tstruct intel_engine_cs *engine;\n\tstruct i915_request **request;\n\tstruct igt_live_test t;\n\tunsigned int idx;\n\tint err;\n\n\t \n\n\trequest = kcalloc(nengines, sizeof(*request), GFP_KERNEL);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\terr = igt_live_test_begin(&t, i915, __func__, \"\");\n\tif (err)\n\t\tgoto out_free;\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_vma *batch;\n\n\t\tbatch = recursive_batch(engine->gt);\n\t\tif (IS_ERR(batch)) {\n\t\t\terr = PTR_ERR(batch);\n\t\t\tpr_err(\"%s: Unable to create batch, err=%d\\n\",\n\t\t\t       __func__, err);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\ti915_vma_lock(batch);\n\t\trequest[idx] = intel_engine_create_kernel_request(engine);\n\t\tif (IS_ERR(request[idx])) {\n\t\t\terr = PTR_ERR(request[idx]);\n\t\t\tpr_err(\"%s: Request allocation failed with err=%d\\n\",\n\t\t\t       __func__, err);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tGEM_BUG_ON(request[idx]->context->vm != batch->vm);\n\n\t\terr = i915_vma_move_to_active(batch, request[idx], 0);\n\t\tGEM_BUG_ON(err);\n\n\t\terr = emit_bb_start(request[idx], batch);\n\t\tGEM_BUG_ON(err);\n\t\trequest[idx]->batch = batch;\n\n\t\ti915_request_get(request[idx]);\n\t\ti915_request_add(request[idx]);\n\t\tidx++;\nout_unlock:\n\t\ti915_vma_unlock(batch);\n\t\tif (err)\n\t\t\tgoto out_request;\n\t}\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tif (i915_request_completed(request[idx])) {\n\t\t\tpr_err(\"%s(%s): request completed too early!\\n\",\n\t\t\t       __func__, engine->name);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_request;\n\t\t}\n\t\tidx++;\n\t}\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\terr = recursive_batch_resolve(request[idx]->batch);\n\t\tif (err) {\n\t\t\tpr_err(\"%s: failed to resolve batch, err=%d\\n\",\n\t\t\t       __func__, err);\n\t\t\tgoto out_request;\n\t\t}\n\t\tidx++;\n\t}\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_request *rq = request[idx];\n\t\tlong timeout;\n\n\t\ttimeout = i915_request_wait(rq, 0,\n\t\t\t\t\t    MAX_SCHEDULE_TIMEOUT);\n\t\tif (timeout < 0) {\n\t\t\terr = timeout;\n\t\t\tpr_err(\"%s: error waiting for request on %s, err=%d\\n\",\n\t\t\t       __func__, engine->name, err);\n\t\t\tgoto out_request;\n\t\t}\n\n\t\tGEM_BUG_ON(!i915_request_completed(rq));\n\t\ti915_vma_unpin(rq->batch);\n\t\ti915_vma_put(rq->batch);\n\t\ti915_request_put(rq);\n\t\trequest[idx] = NULL;\n\t\tidx++;\n\t}\n\n\terr = igt_live_test_end(&t);\n\nout_request:\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_request *rq = request[idx];\n\n\t\tif (!rq)\n\t\t\tcontinue;\n\n\t\tif (rq->batch) {\n\t\t\ti915_vma_unpin(rq->batch);\n\t\t\ti915_vma_put(rq->batch);\n\t\t}\n\t\ti915_request_put(rq);\n\t\tidx++;\n\t}\nout_free:\n\tkfree(request);\n\treturn err;\n}\n\nstatic int live_sequential_engines(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tstruct i915_request **request;\n\tstruct i915_request *prev = NULL;\n\tstruct intel_engine_cs *engine;\n\tstruct igt_live_test t;\n\tunsigned int idx;\n\tint err;\n\n\t \n\n\trequest = kcalloc(nengines, sizeof(*request), GFP_KERNEL);\n\tif (!request)\n\t\treturn -ENOMEM;\n\n\terr = igt_live_test_begin(&t, i915, __func__, \"\");\n\tif (err)\n\t\tgoto out_free;\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_vma *batch;\n\n\t\tbatch = recursive_batch(engine->gt);\n\t\tif (IS_ERR(batch)) {\n\t\t\terr = PTR_ERR(batch);\n\t\t\tpr_err(\"%s: Unable to create batch for %s, err=%d\\n\",\n\t\t\t       __func__, engine->name, err);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\ti915_vma_lock(batch);\n\t\trequest[idx] = intel_engine_create_kernel_request(engine);\n\t\tif (IS_ERR(request[idx])) {\n\t\t\terr = PTR_ERR(request[idx]);\n\t\t\tpr_err(\"%s: Request allocation failed for %s with err=%d\\n\",\n\t\t\t       __func__, engine->name, err);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tGEM_BUG_ON(request[idx]->context->vm != batch->vm);\n\n\t\tif (prev) {\n\t\t\terr = i915_request_await_dma_fence(request[idx],\n\t\t\t\t\t\t\t   &prev->fence);\n\t\t\tif (err) {\n\t\t\t\ti915_request_add(request[idx]);\n\t\t\t\tpr_err(\"%s: Request await failed for %s with err=%d\\n\",\n\t\t\t\t       __func__, engine->name, err);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\terr = i915_vma_move_to_active(batch, request[idx], 0);\n\t\tGEM_BUG_ON(err);\n\n\t\terr = emit_bb_start(request[idx], batch);\n\t\tGEM_BUG_ON(err);\n\t\trequest[idx]->batch = batch;\n\n\t\ti915_request_get(request[idx]);\n\t\ti915_request_add(request[idx]);\n\n\t\tprev = request[idx];\n\t\tidx++;\n\nout_unlock:\n\t\ti915_vma_unlock(batch);\n\t\tif (err)\n\t\t\tgoto out_request;\n\t}\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tlong timeout;\n\n\t\tif (i915_request_completed(request[idx])) {\n\t\t\tpr_err(\"%s(%s): request completed too early!\\n\",\n\t\t\t       __func__, engine->name);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_request;\n\t\t}\n\n\t\terr = recursive_batch_resolve(request[idx]->batch);\n\t\tif (err) {\n\t\t\tpr_err(\"%s: failed to resolve batch, err=%d\\n\",\n\t\t\t       __func__, err);\n\t\t\tgoto out_request;\n\t\t}\n\n\t\ttimeout = i915_request_wait(request[idx], 0,\n\t\t\t\t\t    MAX_SCHEDULE_TIMEOUT);\n\t\tif (timeout < 0) {\n\t\t\terr = timeout;\n\t\t\tpr_err(\"%s: error waiting for request on %s, err=%d\\n\",\n\t\t\t       __func__, engine->name, err);\n\t\t\tgoto out_request;\n\t\t}\n\n\t\tGEM_BUG_ON(!i915_request_completed(request[idx]));\n\t\tidx++;\n\t}\n\n\terr = igt_live_test_end(&t);\n\nout_request:\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tu32 *cmd;\n\n\t\tif (!request[idx])\n\t\t\tbreak;\n\n\t\tcmd = i915_gem_object_pin_map_unlocked(request[idx]->batch->obj,\n\t\t\t\t\t\t       I915_MAP_WC);\n\t\tif (!IS_ERR(cmd)) {\n\t\t\t*cmd = MI_BATCH_BUFFER_END;\n\n\t\t\t__i915_gem_object_flush_map(request[idx]->batch->obj,\n\t\t\t\t\t\t    0, sizeof(*cmd));\n\t\t\ti915_gem_object_unpin_map(request[idx]->batch->obj);\n\n\t\t\tintel_gt_chipset_flush(engine->gt);\n\t\t}\n\n\t\ti915_vma_put(request[idx]->batch);\n\t\ti915_request_put(request[idx]);\n\t\tidx++;\n\t}\nout_free:\n\tkfree(request);\n\treturn err;\n}\n\nstruct parallel_thread {\n\tstruct kthread_worker *worker;\n\tstruct kthread_work work;\n\tstruct intel_engine_cs *engine;\n\tint result;\n};\n\nstatic void __live_parallel_engine1(struct kthread_work *work)\n{\n\tstruct parallel_thread *thread =\n\t\tcontainer_of(work, typeof(*thread), work);\n\tstruct intel_engine_cs *engine = thread->engine;\n\tIGT_TIMEOUT(end_time);\n\tunsigned long count;\n\tint err = 0;\n\n\tcount = 0;\n\tintel_engine_pm_get(engine);\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(engine->kernel_context);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\terr = 0;\n\t\tif (i915_request_wait(rq, 0, HZ) < 0)\n\t\t\terr = -ETIME;\n\t\ti915_request_put(rq);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcount++;\n\t} while (!__igt_timeout(end_time, NULL));\n\tintel_engine_pm_put(engine);\n\n\tpr_info(\"%s: %lu request + sync\\n\", engine->name, count);\n\tthread->result = err;\n}\n\nstatic void __live_parallel_engineN(struct kthread_work *work)\n{\n\tstruct parallel_thread *thread =\n\t\tcontainer_of(work, typeof(*thread), work);\n\tstruct intel_engine_cs *engine = thread->engine;\n\tIGT_TIMEOUT(end_time);\n\tunsigned long count;\n\tint err = 0;\n\n\tcount = 0;\n\tintel_engine_pm_get(engine);\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(engine->kernel_context);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_add(rq);\n\t\tcount++;\n\t} while (!__igt_timeout(end_time, NULL));\n\tintel_engine_pm_put(engine);\n\n\tpr_info(\"%s: %lu requests\\n\", engine->name, count);\n\tthread->result = err;\n}\n\nstatic bool wake_all(struct drm_i915_private *i915)\n{\n\tif (atomic_dec_and_test(&i915->selftest.counter)) {\n\t\twake_up_var(&i915->selftest.counter);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int wait_for_all(struct drm_i915_private *i915)\n{\n\tif (wake_all(i915))\n\t\treturn 0;\n\n\tif (wait_var_event_timeout(&i915->selftest.counter,\n\t\t\t\t   !atomic_read(&i915->selftest.counter),\n\t\t\t\t   i915_selftest.timeout_jiffies))\n\t\treturn 0;\n\n\treturn -ETIME;\n}\n\nstatic void __live_parallel_spin(struct kthread_work *work)\n{\n\tstruct parallel_thread *thread =\n\t\tcontainer_of(work, typeof(*thread), work);\n\tstruct intel_engine_cs *engine = thread->engine;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tint err = 0;\n\n\t \n\n\tif (igt_spinner_init(&spin, engine->gt)) {\n\t\twake_all(engine->i915);\n\t\tthread->result = -ENOMEM;\n\t\treturn;\n\t}\n\n\tintel_engine_pm_get(engine);\n\trq = igt_spinner_create_request(&spin,\n\t\t\t\t\tengine->kernel_context,\n\t\t\t\t\tMI_NOOP);  \n\tintel_engine_pm_put(engine);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tif (err == -ENODEV)\n\t\t\terr = 0;\n\t\twake_all(engine->i915);\n\t\tgoto out_spin;\n\t}\n\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\tif (igt_wait_for_spinner(&spin, rq)) {\n\t\t \n\t\terr = wait_for_all(engine->i915);\n\t} else {\n\t\tpr_err(\"Failed to start spinner on %s\\n\", engine->name);\n\t\terr = -EINVAL;\n\t}\n\tigt_spinner_end(&spin);\n\n\tif (err == 0 && i915_request_wait(rq, 0, HZ) < 0)\n\t\terr = -EIO;\n\ti915_request_put(rq);\n\nout_spin:\n\tigt_spinner_fini(&spin);\n\tthread->result = err;\n}\n\nstatic int live_parallel_engines(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstatic void (* const func[])(struct kthread_work *) = {\n\t\t__live_parallel_engine1,\n\t\t__live_parallel_engineN,\n\t\t__live_parallel_spin,\n\t\tNULL,\n\t};\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tstruct parallel_thread *threads;\n\tstruct intel_engine_cs *engine;\n\tvoid (* const *fn)(struct kthread_work *);\n\tint err = 0;\n\n\t \n\n\tthreads = kcalloc(nengines, sizeof(*threads), GFP_KERNEL);\n\tif (!threads)\n\t\treturn -ENOMEM;\n\n\tfor (fn = func; !err && *fn; fn++) {\n\t\tchar name[KSYM_NAME_LEN];\n\t\tstruct igt_live_test t;\n\t\tunsigned int idx;\n\n\t\tsnprintf(name, sizeof(name), \"%ps\", *fn);\n\t\terr = igt_live_test_begin(&t, i915, __func__, name);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tatomic_set(&i915->selftest.counter, nengines);\n\n\t\tidx = 0;\n\t\tfor_each_uabi_engine(engine, i915) {\n\t\t\tstruct kthread_worker *worker;\n\n\t\t\tworker = kthread_create_worker(0, \"igt/parallel:%s\",\n\t\t\t\t\t\t       engine->name);\n\t\t\tif (IS_ERR(worker)) {\n\t\t\t\terr = PTR_ERR(worker);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tthreads[idx].worker = worker;\n\t\t\tthreads[idx].result = 0;\n\t\t\tthreads[idx].engine = engine;\n\n\t\t\tkthread_init_work(&threads[idx].work, *fn);\n\t\t\tkthread_queue_work(worker, &threads[idx].work);\n\t\t\tidx++;\n\t\t}\n\n\t\tidx = 0;\n\t\tfor_each_uabi_engine(engine, i915) {\n\t\t\tint status;\n\n\t\t\tif (!threads[idx].worker)\n\t\t\t\tbreak;\n\n\t\t\tkthread_flush_work(&threads[idx].work);\n\t\t\tstatus = READ_ONCE(threads[idx].result);\n\t\t\tif (status && !err)\n\t\t\t\terr = status;\n\n\t\t\tkthread_destroy_worker(threads[idx++].worker);\n\t\t}\n\n\t\tif (igt_live_test_end(&t))\n\t\t\terr = -EIO;\n\t}\n\n\tkfree(threads);\n\treturn err;\n}\n\nstatic int\nmax_batches(struct i915_gem_context *ctx, struct intel_engine_cs *engine)\n{\n\tstruct i915_request *rq;\n\tint ret;\n\n\t \n\tif (HAS_EXECLISTS(ctx->i915))\n\t\treturn INT_MAX;\n\n\trq = igt_request_alloc(ctx, engine);\n\tif (IS_ERR(rq)) {\n\t\tret = PTR_ERR(rq);\n\t} else {\n\t\tint sz;\n\n\t\tret = rq->ring->size - rq->reserved_space;\n\t\ti915_request_add(rq);\n\n\t\tsz = rq->ring->emit - rq->head;\n\t\tif (sz < 0)\n\t\t\tsz += rq->ring->size;\n\t\tret /= sz;\n\t\tret /= 2;  \n\t}\n\n\treturn ret;\n}\n\nstatic int live_breadcrumbs_smoketest(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tconst unsigned int ncpus =  \n\t\tmax_t(int, 2, DIV_ROUND_UP(num_online_cpus(), nengines));\n\tunsigned long num_waits, num_fences;\n\tstruct intel_engine_cs *engine;\n\tstruct smoke_thread *threads;\n\tstruct igt_live_test live;\n\tintel_wakeref_t wakeref;\n\tstruct smoketest *smoke;\n\tunsigned int n, idx;\n\tstruct file *file;\n\tint ret = 0;\n\n\t \n\n\twakeref = intel_runtime_pm_get(&i915->runtime_pm);\n\n\tfile = mock_file(i915);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto out_rpm;\n\t}\n\n\tsmoke = kcalloc(nengines, sizeof(*smoke), GFP_KERNEL);\n\tif (!smoke) {\n\t\tret = -ENOMEM;\n\t\tgoto out_file;\n\t}\n\n\tthreads = kcalloc(ncpus * nengines, sizeof(*threads), GFP_KERNEL);\n\tif (!threads) {\n\t\tret = -ENOMEM;\n\t\tgoto out_smoke;\n\t}\n\n\tsmoke[0].request_alloc = __live_request_alloc;\n\tsmoke[0].ncontexts = 64;\n\tsmoke[0].contexts = kcalloc(smoke[0].ncontexts,\n\t\t\t\t    sizeof(*smoke[0].contexts),\n\t\t\t\t    GFP_KERNEL);\n\tif (!smoke[0].contexts) {\n\t\tret = -ENOMEM;\n\t\tgoto out_threads;\n\t}\n\n\tfor (n = 0; n < smoke[0].ncontexts; n++) {\n\t\tsmoke[0].contexts[n] = live_context(i915, file);\n\t\tif (IS_ERR(smoke[0].contexts[n])) {\n\t\t\tret = PTR_ERR(smoke[0].contexts[n]);\n\t\t\tgoto out_contexts;\n\t\t}\n\t}\n\n\tret = igt_live_test_begin(&live, i915, __func__, \"\");\n\tif (ret)\n\t\tgoto out_contexts;\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tsmoke[idx] = smoke[0];\n\t\tsmoke[idx].engine = engine;\n\t\tsmoke[idx].max_batch =\n\t\t\tmax_batches(smoke[0].contexts[0], engine);\n\t\tif (smoke[idx].max_batch < 0) {\n\t\t\tret = smoke[idx].max_batch;\n\t\t\tgoto out_flush;\n\t\t}\n\t\t \n\t\tsmoke[idx].max_batch /= ncpus + 1;\n\t\tpr_debug(\"Limiting batches to %d requests on %s\\n\",\n\t\t\t smoke[idx].max_batch, engine->name);\n\n\t\tfor (n = 0; n < ncpus; n++) {\n\t\t\tunsigned int i = idx * ncpus + n;\n\t\t\tstruct kthread_worker *worker;\n\n\t\t\tworker = kthread_create_worker(0, \"igt/%d.%d\", idx, n);\n\t\t\tif (IS_ERR(worker)) {\n\t\t\t\tret = PTR_ERR(worker);\n\t\t\t\tgoto out_flush;\n\t\t\t}\n\n\t\t\tthreads[i].worker = worker;\n\t\t\tthreads[i].t = &smoke[idx];\n\n\t\t\tkthread_init_work(&threads[i].work,\n\t\t\t\t\t  __igt_breadcrumbs_smoketest);\n\t\t\tkthread_queue_work(worker, &threads[i].work);\n\t\t}\n\n\t\tidx++;\n\t}\n\n\tmsleep(jiffies_to_msecs(i915_selftest.timeout_jiffies));\n\nout_flush:\n\tidx = 0;\n\tnum_waits = 0;\n\tnum_fences = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tfor (n = 0; n < ncpus; n++) {\n\t\t\tunsigned int i = idx * ncpus + n;\n\t\t\tint err;\n\n\t\t\tif (!threads[i].worker)\n\t\t\t\tcontinue;\n\n\t\t\tWRITE_ONCE(threads[i].stop, true);\n\t\t\tkthread_flush_work(&threads[i].work);\n\t\t\terr = READ_ONCE(threads[i].result);\n\t\t\tif (err < 0 && !ret)\n\t\t\t\tret = err;\n\n\t\t\tkthread_destroy_worker(threads[i].worker);\n\t\t}\n\n\t\tnum_waits += atomic_long_read(&smoke[idx].num_waits);\n\t\tnum_fences += atomic_long_read(&smoke[idx].num_fences);\n\t\tidx++;\n\t}\n\tpr_info(\"Completed %lu waits for %lu fences across %d engines and %d cpus\\n\",\n\t\tnum_waits, num_fences, idx, ncpus);\n\n\tret = igt_live_test_end(&live) ?: ret;\nout_contexts:\n\tkfree(smoke[0].contexts);\nout_threads:\n\tkfree(threads);\nout_smoke:\n\tkfree(smoke);\nout_file:\n\tfput(file);\nout_rpm:\n\tintel_runtime_pm_put(&i915->runtime_pm, wakeref);\n\n\treturn ret;\n}\n\nint i915_request_live_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(live_nop_request),\n\t\tSUBTEST(live_all_engines),\n\t\tSUBTEST(live_sequential_engines),\n\t\tSUBTEST(live_parallel_engines),\n\t\tSUBTEST(live_empty_request),\n\t\tSUBTEST(live_cancel_request),\n\t\tSUBTEST(live_breadcrumbs_smoketest),\n\t};\n\n\tif (intel_gt_is_wedged(to_gt(i915)))\n\t\treturn 0;\n\n\treturn i915_live_subtests(tests, i915);\n}\n\nstatic int switch_to_kernel_sync(struct intel_context *ce, int err)\n{\n\tstruct i915_request *rq;\n\tstruct dma_fence *fence;\n\n\trq = intel_engine_create_kernel_request(ce->engine);\n\tif (IS_ERR(rq))\n\t\treturn PTR_ERR(rq);\n\n\tfence = i915_active_fence_get(&ce->timeline->last_request);\n\tif (fence) {\n\t\ti915_request_await_dma_fence(rq, fence);\n\t\tdma_fence_put(fence);\n\t}\n\n\trq = i915_request_get(rq);\n\ti915_request_add(rq);\n\tif (i915_request_wait(rq, 0, HZ / 2) < 0 && !err)\n\t\terr = -ETIME;\n\ti915_request_put(rq);\n\n\twhile (!err && !intel_engine_is_idle(ce->engine))\n\t\tintel_engine_flush_submission(ce->engine);\n\n\treturn err;\n}\n\nstruct perf_stats {\n\tstruct intel_engine_cs *engine;\n\tunsigned long count;\n\tktime_t time;\n\tktime_t busy;\n\tu64 runtime;\n};\n\nstruct perf_series {\n\tstruct drm_i915_private *i915;\n\tunsigned int nengines;\n\tstruct intel_context *ce[];\n};\n\nstatic int cmp_u32(const void *A, const void *B)\n{\n\tconst u32 *a = A, *b = B;\n\n\treturn *a - *b;\n}\n\nstatic u32 trifilter(u32 *a)\n{\n\tu64 sum;\n\n#define TF_COUNT 5\n\tsort(a, TF_COUNT, sizeof(*a), cmp_u32, NULL);\n\n\tsum = mul_u32_u32(a[2], 2);\n\tsum += a[1];\n\tsum += a[3];\n\n\tGEM_BUG_ON(sum > U32_MAX);\n\treturn sum;\n#define TF_BIAS 2\n}\n\nstatic u64 cycles_to_ns(struct intel_engine_cs *engine, u32 cycles)\n{\n\tu64 ns = intel_gt_clock_interval_to_ns(engine->gt, cycles);\n\n\treturn DIV_ROUND_CLOSEST(ns, 1 << TF_BIAS);\n}\n\nstatic u32 *emit_timestamp_store(u32 *cs, struct intel_context *ce, u32 offset)\n{\n\t*cs++ = MI_STORE_REGISTER_MEM_GEN8 | MI_USE_GGTT;\n\t*cs++ = i915_mmio_reg_offset(RING_TIMESTAMP((ce->engine->mmio_base)));\n\t*cs++ = offset;\n\t*cs++ = 0;\n\n\treturn cs;\n}\n\nstatic u32 *emit_store_dw(u32 *cs, u32 offset, u32 value)\n{\n\t*cs++ = MI_STORE_DWORD_IMM_GEN4 | MI_USE_GGTT;\n\t*cs++ = offset;\n\t*cs++ = 0;\n\t*cs++ = value;\n\n\treturn cs;\n}\n\nstatic u32 *emit_semaphore_poll(u32 *cs, u32 mode, u32 value, u32 offset)\n{\n\t*cs++ = MI_SEMAPHORE_WAIT |\n\t\tMI_SEMAPHORE_GLOBAL_GTT |\n\t\tMI_SEMAPHORE_POLL |\n\t\tmode;\n\t*cs++ = value;\n\t*cs++ = offset;\n\t*cs++ = 0;\n\n\treturn cs;\n}\n\nstatic u32 *emit_semaphore_poll_until(u32 *cs, u32 offset, u32 value)\n{\n\treturn emit_semaphore_poll(cs, MI_SEMAPHORE_SAD_EQ_SDD, value, offset);\n}\n\nstatic void semaphore_set(u32 *sema, u32 value)\n{\n\tWRITE_ONCE(*sema, value);\n\twmb();  \n}\n\nstatic u32 *hwsp_scratch(const struct intel_context *ce)\n{\n\treturn memset32(ce->engine->status_page.addr + 1000, 0, 21);\n}\n\nstatic u32 hwsp_offset(const struct intel_context *ce, u32 *dw)\n{\n\treturn (i915_ggtt_offset(ce->engine->status_page.vma) +\n\t\toffset_in_page(dw));\n}\n\nstatic int measure_semaphore_response(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT], cycles;\n\tstruct i915_request *rq;\n\tu32 *cs;\n\tint err;\n\tint i;\n\n\t \n\n\tsemaphore_set(sema, -1);\n\n\trq = i915_request_create(ce);\n\tif (IS_ERR(rq))\n\t\treturn PTR_ERR(rq);\n\n\tcs = intel_ring_begin(rq, 4 + 12 * ARRAY_SIZE(elapsed));\n\tif (IS_ERR(cs)) {\n\t\ti915_request_add(rq);\n\t\terr = PTR_ERR(cs);\n\t\tgoto err;\n\t}\n\n\tcs = emit_store_dw(cs, offset, 0);\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tcs = emit_semaphore_poll_until(cs, offset, i);\n\t\tcs = emit_timestamp_store(cs, ce, offset + i * sizeof(u32));\n\t\tcs = emit_store_dw(cs, offset, 0);\n\t}\n\n\tintel_ring_advance(rq, cs);\n\ti915_request_add(rq);\n\n\tif (wait_for(READ_ONCE(*sema) == 0, 50)) {\n\t\terr = -EIO;\n\t\tgoto err;\n\t}\n\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tpreempt_disable();\n\t\tcycles = ENGINE_READ_FW(ce->engine, RING_TIMESTAMP);\n\t\tsemaphore_set(sema, i);\n\t\tpreempt_enable();\n\n\t\tif (wait_for(READ_ONCE(*sema) == 0, 50)) {\n\t\t\terr = -EIO;\n\t\t\tgoto err;\n\t\t}\n\n\t\telapsed[i - 1] = sema[i] - cycles;\n\t}\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: semaphore response %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic int measure_idle_dispatch(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT], cycles;\n\tu32 *cs;\n\tint err;\n\tint i;\n\n\t \n\n\tfor (i = 0; i < ARRAY_SIZE(elapsed); i++) {\n\t\tstruct i915_request *rq;\n\n\t\terr = intel_gt_wait_for_idle(ce->engine->gt, HZ / 2);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 4);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = emit_timestamp_store(cs, ce, offset + i * sizeof(u32));\n\n\t\tintel_ring_advance(rq, cs);\n\n\t\tpreempt_disable();\n\t\tlocal_bh_disable();\n\t\telapsed[i] = ENGINE_READ_FW(ce->engine, RING_TIMESTAMP);\n\t\ti915_request_add(rq);\n\t\tlocal_bh_enable();\n\t\tpreempt_enable();\n\t}\n\n\terr = intel_gt_wait_for_idle(ce->engine->gt, HZ / 2);\n\tif (err)\n\t\tgoto err;\n\n\tfor (i = 0; i < ARRAY_SIZE(elapsed); i++)\n\t\telapsed[i] = sema[i] - elapsed[i];\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: idle dispatch latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic int measure_busy_dispatch(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT + 1], cycles;\n\tu32 *cs;\n\tint err;\n\tint i;\n\n\t \n\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 12);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = emit_store_dw(cs, offset + i * sizeof(u32), -1);\n\t\tcs = emit_semaphore_poll_until(cs, offset, i);\n\t\tcs = emit_timestamp_store(cs, ce, offset + i * sizeof(u32));\n\n\t\tintel_ring_advance(rq, cs);\n\n\t\tif (i > 1 && wait_for(READ_ONCE(sema[i - 1]), 500)) {\n\t\t\terr = -EIO;\n\t\t\tgoto err;\n\t\t}\n\n\t\tpreempt_disable();\n\t\tlocal_bh_disable();\n\t\telapsed[i - 1] = ENGINE_READ_FW(ce->engine, RING_TIMESTAMP);\n\t\ti915_request_add(rq);\n\t\tlocal_bh_enable();\n\t\tsemaphore_set(sema, i - 1);\n\t\tpreempt_enable();\n\t}\n\n\twait_for(READ_ONCE(sema[i - 1]), 500);\n\tsemaphore_set(sema, i - 1);\n\n\tfor (i = 1; i <= TF_COUNT; i++) {\n\t\tGEM_BUG_ON(sema[i] == -1);\n\t\telapsed[i - 1] = sema[i] - elapsed[i];\n\t}\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: busy dispatch latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic int plug(struct intel_engine_cs *engine, u32 *sema, u32 mode, int value)\n{\n\tconst u32 offset =\n\t\ti915_ggtt_offset(engine->status_page.vma) +\n\t\toffset_in_page(sema);\n\tstruct i915_request *rq;\n\tu32 *cs;\n\n\trq = i915_request_create(engine->kernel_context);\n\tif (IS_ERR(rq))\n\t\treturn PTR_ERR(rq);\n\n\tcs = intel_ring_begin(rq, 4);\n\tif (IS_ERR(cs)) {\n\t\ti915_request_add(rq);\n\t\treturn PTR_ERR(cs);\n\t}\n\n\tcs = emit_semaphore_poll(cs, mode, value, offset);\n\n\tintel_ring_advance(rq, cs);\n\ti915_request_add(rq);\n\n\treturn 0;\n}\n\nstatic int measure_inter_request(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT + 1], cycles;\n\tstruct i915_sw_fence *submit;\n\tint i, err;\n\n\t \n\n\terr = plug(ce->engine, sema, MI_SEMAPHORE_SAD_NEQ_SDD, 0);\n\tif (err)\n\t\treturn err;\n\n\tsubmit = heap_fence_create(GFP_KERNEL);\n\tif (!submit) {\n\t\tsemaphore_set(sema, 1);\n\t\treturn -ENOMEM;\n\t}\n\n\tintel_engine_flush_submission(ce->engine);\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tstruct i915_request *rq;\n\t\tu32 *cs;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err_submit;\n\t\t}\n\n\t\terr = i915_sw_fence_await_sw_fence_gfp(&rq->submit,\n\t\t\t\t\t\t       submit,\n\t\t\t\t\t\t       GFP_KERNEL);\n\t\tif (err < 0) {\n\t\t\ti915_request_add(rq);\n\t\t\tgoto err_submit;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 4);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err_submit;\n\t\t}\n\n\t\tcs = emit_timestamp_store(cs, ce, offset + i * sizeof(u32));\n\n\t\tintel_ring_advance(rq, cs);\n\t\ti915_request_add(rq);\n\t}\n\ti915_sw_fence_commit(submit);\n\tintel_engine_flush_submission(ce->engine);\n\theap_fence_put(submit);\n\n\tsemaphore_set(sema, 1);\n\terr = intel_gt_wait_for_idle(ce->engine->gt, HZ / 2);\n\tif (err)\n\t\tgoto err;\n\n\tfor (i = 1; i <= TF_COUNT; i++)\n\t\telapsed[i - 1] = sema[i + 1] - sema[i];\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: inter-request latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr_submit:\n\ti915_sw_fence_commit(submit);\n\theap_fence_put(submit);\n\tsemaphore_set(sema, 1);\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic int measure_context_switch(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tstruct i915_request *fence = NULL;\n\tu32 elapsed[TF_COUNT + 1], cycles;\n\tint i, j, err;\n\tu32 *cs;\n\n\t \n\n\terr = plug(ce->engine, sema, MI_SEMAPHORE_SAD_NEQ_SDD, 0);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tstruct intel_context *arr[] = {\n\t\t\tce, ce->engine->kernel_context\n\t\t};\n\t\tu32 addr = offset + ARRAY_SIZE(arr) * i * sizeof(u32);\n\n\t\tfor (j = 0; j < ARRAY_SIZE(arr); j++) {\n\t\t\tstruct i915_request *rq;\n\n\t\t\trq = i915_request_create(arr[j]);\n\t\t\tif (IS_ERR(rq)) {\n\t\t\t\terr = PTR_ERR(rq);\n\t\t\t\tgoto err_fence;\n\t\t\t}\n\n\t\t\tif (fence) {\n\t\t\t\terr = i915_request_await_dma_fence(rq,\n\t\t\t\t\t\t\t\t   &fence->fence);\n\t\t\t\tif (err) {\n\t\t\t\t\ti915_request_add(rq);\n\t\t\t\t\tgoto err_fence;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tcs = intel_ring_begin(rq, 4);\n\t\t\tif (IS_ERR(cs)) {\n\t\t\t\ti915_request_add(rq);\n\t\t\t\terr = PTR_ERR(cs);\n\t\t\t\tgoto err_fence;\n\t\t\t}\n\n\t\t\tcs = emit_timestamp_store(cs, ce, addr);\n\t\t\taddr += sizeof(u32);\n\n\t\t\tintel_ring_advance(rq, cs);\n\n\t\t\ti915_request_put(fence);\n\t\t\tfence = i915_request_get(rq);\n\n\t\t\ti915_request_add(rq);\n\t\t}\n\t}\n\ti915_request_put(fence);\n\tintel_engine_flush_submission(ce->engine);\n\n\tsemaphore_set(sema, 1);\n\terr = intel_gt_wait_for_idle(ce->engine->gt, HZ / 2);\n\tif (err)\n\t\tgoto err;\n\n\tfor (i = 1; i <= TF_COUNT; i++)\n\t\telapsed[i - 1] = sema[2 * i + 2] - sema[2 * i + 1];\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: context switch latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr_fence:\n\ti915_request_put(fence);\n\tsemaphore_set(sema, 1);\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic int measure_preemption(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT], cycles;\n\tu32 *cs;\n\tint err;\n\tint i;\n\n\t \n\n\tif (!intel_engine_has_preemption(ce->engine))\n\t\treturn 0;\n\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tu32 addr = offset + 2 * i * sizeof(u32);\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 12);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = emit_store_dw(cs, addr, -1);\n\t\tcs = emit_semaphore_poll_until(cs, offset, i);\n\t\tcs = emit_timestamp_store(cs, ce, addr + sizeof(u32));\n\n\t\tintel_ring_advance(rq, cs);\n\t\ti915_request_add(rq);\n\n\t\tif (wait_for(READ_ONCE(sema[2 * i]) == -1, 500)) {\n\t\t\terr = -EIO;\n\t\t\tgoto err;\n\t\t}\n\n\t\trq = i915_request_create(ce->engine->kernel_context);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 8);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = emit_timestamp_store(cs, ce, addr);\n\t\tcs = emit_store_dw(cs, offset, i);\n\n\t\tintel_ring_advance(rq, cs);\n\t\trq->sched.attr.priority = I915_PRIORITY_BARRIER;\n\n\t\telapsed[i - 1] = ENGINE_READ_FW(ce->engine, RING_TIMESTAMP);\n\t\ti915_request_add(rq);\n\t}\n\n\tif (wait_for(READ_ONCE(sema[2 * i - 2]) != -1, 500)) {\n\t\terr = -EIO;\n\t\tgoto err;\n\t}\n\n\tfor (i = 1; i <= TF_COUNT; i++)\n\t\telapsed[i - 1] = sema[2 * i + 0] - elapsed[i - 1];\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: preemption dispatch latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\tfor (i = 1; i <= TF_COUNT; i++)\n\t\telapsed[i - 1] = sema[2 * i + 1] - sema[2 * i + 0];\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: preemption switch latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstruct signal_cb {\n\tstruct dma_fence_cb base;\n\tbool seen;\n};\n\nstatic void signal_cb(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tstruct signal_cb *s = container_of(cb, typeof(*s), base);\n\n\tsmp_store_mb(s->seen, true);  \n}\n\nstatic int measure_completion(struct intel_context *ce)\n{\n\tu32 *sema = hwsp_scratch(ce);\n\tconst u32 offset = hwsp_offset(ce, sema);\n\tu32 elapsed[TF_COUNT], cycles;\n\tu32 *cs;\n\tint err;\n\tint i;\n\n\t \n\n\tfor (i = 1; i <= ARRAY_SIZE(elapsed); i++) {\n\t\tstruct signal_cb cb = { .seen = false };\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = intel_ring_begin(rq, 12);\n\t\tif (IS_ERR(cs)) {\n\t\t\ti915_request_add(rq);\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto err;\n\t\t}\n\n\t\tcs = emit_store_dw(cs, offset + i * sizeof(u32), -1);\n\t\tcs = emit_semaphore_poll_until(cs, offset, i);\n\t\tcs = emit_timestamp_store(cs, ce, offset + i * sizeof(u32));\n\n\t\tintel_ring_advance(rq, cs);\n\n\t\tdma_fence_add_callback(&rq->fence, &cb.base, signal_cb);\n\t\ti915_request_add(rq);\n\n\t\tintel_engine_flush_submission(ce->engine);\n\t\tif (wait_for(READ_ONCE(sema[i]) == -1, 50)) {\n\t\t\terr = -EIO;\n\t\t\tgoto err;\n\t\t}\n\n\t\tpreempt_disable();\n\t\tsemaphore_set(sema, i);\n\t\twhile (!READ_ONCE(cb.seen))\n\t\t\tcpu_relax();\n\n\t\telapsed[i - 1] = ENGINE_READ_FW(ce->engine, RING_TIMESTAMP);\n\t\tpreempt_enable();\n\t}\n\n\terr = intel_gt_wait_for_idle(ce->engine->gt, HZ / 2);\n\tif (err)\n\t\tgoto err;\n\n\tfor (i = 0; i < ARRAY_SIZE(elapsed); i++) {\n\t\tGEM_BUG_ON(sema[i + 1] == -1);\n\t\telapsed[i] = elapsed[i] - sema[i + 1];\n\t}\n\n\tcycles = trifilter(elapsed);\n\tpr_info(\"%s: completion latency %d cycles, %lluns\\n\",\n\t\tce->engine->name, cycles >> TF_BIAS,\n\t\tcycles_to_ns(ce->engine, cycles));\n\n\treturn intel_gt_wait_for_idle(ce->engine->gt, HZ);\n\nerr:\n\tintel_gt_set_wedged(ce->engine->gt);\n\treturn err;\n}\n\nstatic void rps_pin(struct intel_gt *gt)\n{\n\t \n\tatomic_inc(&gt->rps.num_waiters);\n\tintel_uncore_forcewake_get(gt->uncore, FORCEWAKE_ALL);\n\n\tmutex_lock(&gt->rps.lock);\n\tintel_rps_set(&gt->rps, gt->rps.max_freq);\n\tmutex_unlock(&gt->rps.lock);\n}\n\nstatic void rps_unpin(struct intel_gt *gt)\n{\n\tintel_uncore_forcewake_put(gt->uncore, FORCEWAKE_ALL);\n\tatomic_dec(&gt->rps.num_waiters);\n}\n\nstatic int perf_request_latency(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_engine_cs *engine;\n\tstruct pm_qos_request qos;\n\tint err = 0;\n\n\tif (GRAPHICS_VER(i915) < 8)  \n\t\treturn 0;\n\n\tcpu_latency_qos_add_request(&qos, 0);  \n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct intel_context *ce;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce)) {\n\t\t\terr = PTR_ERR(ce);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = intel_context_pin(ce);\n\t\tif (err) {\n\t\t\tintel_context_put(ce);\n\t\t\tgoto out;\n\t\t}\n\n\t\tst_engine_heartbeat_disable(engine);\n\t\trps_pin(engine->gt);\n\n\t\tif (err == 0)\n\t\t\terr = measure_semaphore_response(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_idle_dispatch(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_busy_dispatch(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_inter_request(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_context_switch(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_preemption(ce);\n\t\tif (err == 0)\n\t\t\terr = measure_completion(ce);\n\n\t\trps_unpin(engine->gt);\n\t\tst_engine_heartbeat_enable(engine);\n\n\t\tintel_context_unpin(ce);\n\t\tintel_context_put(ce);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\nout:\n\tif (igt_flush_test(i915))\n\t\terr = -EIO;\n\n\tcpu_latency_qos_remove_request(&qos);\n\treturn err;\n}\n\nstatic int s_sync0(void *arg)\n{\n\tstruct perf_series *ps = arg;\n\tIGT_TIMEOUT(end_time);\n\tunsigned int idx = 0;\n\tint err = 0;\n\n\tGEM_BUG_ON(!ps->nengines);\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ps->ce[idx]);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\tif (i915_request_wait(rq, 0, HZ / 5) < 0)\n\t\t\terr = -ETIME;\n\t\ti915_request_put(rq);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (++idx == ps->nengines)\n\t\t\tidx = 0;\n\t} while (!__igt_timeout(end_time, NULL));\n\n\treturn err;\n}\n\nstatic int s_sync1(void *arg)\n{\n\tstruct perf_series *ps = arg;\n\tstruct i915_request *prev = NULL;\n\tIGT_TIMEOUT(end_time);\n\tunsigned int idx = 0;\n\tint err = 0;\n\n\tGEM_BUG_ON(!ps->nengines);\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ps->ce[idx]);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\tif (prev && i915_request_wait(prev, 0, HZ / 5) < 0)\n\t\t\terr = -ETIME;\n\t\ti915_request_put(prev);\n\t\tprev = rq;\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (++idx == ps->nengines)\n\t\t\tidx = 0;\n\t} while (!__igt_timeout(end_time, NULL));\n\ti915_request_put(prev);\n\n\treturn err;\n}\n\nstatic int s_many(void *arg)\n{\n\tstruct perf_series *ps = arg;\n\tIGT_TIMEOUT(end_time);\n\tunsigned int idx = 0;\n\n\tGEM_BUG_ON(!ps->nengines);\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ps->ce[idx]);\n\t\tif (IS_ERR(rq))\n\t\t\treturn PTR_ERR(rq);\n\n\t\ti915_request_add(rq);\n\n\t\tif (++idx == ps->nengines)\n\t\t\tidx = 0;\n\t} while (!__igt_timeout(end_time, NULL));\n\n\treturn 0;\n}\n\nstatic int perf_series_engines(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstatic int (* const func[])(void *arg) = {\n\t\ts_sync0,\n\t\ts_sync1,\n\t\ts_many,\n\t\tNULL,\n\t};\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tstruct intel_engine_cs *engine;\n\tint (* const *fn)(void *arg);\n\tstruct pm_qos_request qos;\n\tstruct perf_stats *stats;\n\tstruct perf_series *ps;\n\tunsigned int idx;\n\tint err = 0;\n\n\tstats = kcalloc(nengines, sizeof(*stats), GFP_KERNEL);\n\tif (!stats)\n\t\treturn -ENOMEM;\n\n\tps = kzalloc(struct_size(ps, ce, nengines), GFP_KERNEL);\n\tif (!ps) {\n\t\tkfree(stats);\n\t\treturn -ENOMEM;\n\t}\n\n\tcpu_latency_qos_add_request(&qos, 0);  \n\n\tps->i915 = i915;\n\tps->nengines = nengines;\n\n\tidx = 0;\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct intel_context *ce;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce)) {\n\t\t\terr = PTR_ERR(ce);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = intel_context_pin(ce);\n\t\tif (err) {\n\t\t\tintel_context_put(ce);\n\t\t\tgoto out;\n\t\t}\n\n\t\tps->ce[idx++] = ce;\n\t}\n\tGEM_BUG_ON(idx != ps->nengines);\n\n\tfor (fn = func; *fn && !err; fn++) {\n\t\tchar name[KSYM_NAME_LEN];\n\t\tstruct igt_live_test t;\n\n\t\tsnprintf(name, sizeof(name), \"%ps\", *fn);\n\t\terr = igt_live_test_begin(&t, i915, __func__, name);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tfor (idx = 0; idx < nengines; idx++) {\n\t\t\tstruct perf_stats *p =\n\t\t\t\tmemset(&stats[idx], 0, sizeof(stats[idx]));\n\t\t\tstruct intel_context *ce = ps->ce[idx];\n\n\t\t\tp->engine = ps->ce[idx]->engine;\n\t\t\tintel_engine_pm_get(p->engine);\n\n\t\t\tif (intel_engine_supports_stats(p->engine))\n\t\t\t\tp->busy = intel_engine_get_busy_time(p->engine,\n\t\t\t\t\t\t\t\t     &p->time) + 1;\n\t\t\telse\n\t\t\t\tp->time = ktime_get();\n\t\t\tp->runtime = -intel_context_get_total_runtime_ns(ce);\n\t\t}\n\n\t\terr = (*fn)(ps);\n\t\tif (igt_live_test_end(&t))\n\t\t\terr = -EIO;\n\n\t\tfor (idx = 0; idx < nengines; idx++) {\n\t\t\tstruct perf_stats *p = &stats[idx];\n\t\t\tstruct intel_context *ce = ps->ce[idx];\n\t\t\tint integer, decimal;\n\t\t\tu64 busy, dt, now;\n\n\t\t\tif (p->busy)\n\t\t\t\tp->busy = ktime_sub(intel_engine_get_busy_time(p->engine,\n\t\t\t\t\t\t\t\t\t       &now),\n\t\t\t\t\t\t    p->busy - 1);\n\t\t\telse\n\t\t\t\tnow = ktime_get();\n\t\t\tp->time = ktime_sub(now, p->time);\n\n\t\t\terr = switch_to_kernel_sync(ce, err);\n\t\t\tp->runtime += intel_context_get_total_runtime_ns(ce);\n\t\t\tintel_engine_pm_put(p->engine);\n\n\t\t\tbusy = 100 * ktime_to_ns(p->busy);\n\t\t\tdt = ktime_to_ns(p->time);\n\t\t\tif (dt) {\n\t\t\t\tinteger = div64_u64(busy, dt);\n\t\t\t\tbusy -= integer * dt;\n\t\t\t\tdecimal = div64_u64(100 * busy, dt);\n\t\t\t} else {\n\t\t\t\tinteger = 0;\n\t\t\t\tdecimal = 0;\n\t\t\t}\n\n\t\t\tpr_info(\"%s %5s: { seqno:%d, busy:%d.%02d%%, runtime:%lldms, walltime:%lldms }\\n\",\n\t\t\t\tname, p->engine->name, ce->timeline->seqno,\n\t\t\t\tinteger, decimal,\n\t\t\t\tdiv_u64(p->runtime, 1000 * 1000),\n\t\t\t\tdiv_u64(ktime_to_ns(p->time), 1000 * 1000));\n\t\t}\n\t}\n\nout:\n\tfor (idx = 0; idx < nengines; idx++) {\n\t\tif (IS_ERR_OR_NULL(ps->ce[idx]))\n\t\t\tbreak;\n\n\t\tintel_context_unpin(ps->ce[idx]);\n\t\tintel_context_put(ps->ce[idx]);\n\t}\n\tkfree(ps);\n\n\tcpu_latency_qos_remove_request(&qos);\n\tkfree(stats);\n\treturn err;\n}\n\nstruct p_thread {\n\tstruct perf_stats p;\n\tstruct kthread_worker *worker;\n\tstruct kthread_work work;\n\tstruct intel_engine_cs *engine;\n\tint result;\n};\n\nstatic void p_sync0(struct kthread_work *work)\n{\n\tstruct p_thread *thread = container_of(work, typeof(*thread), work);\n\tstruct perf_stats *p = &thread->p;\n\tstruct intel_engine_cs *engine = p->engine;\n\tstruct intel_context *ce;\n\tIGT_TIMEOUT(end_time);\n\tunsigned long count;\n\tbool busy;\n\tint err = 0;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\tthread->result = PTR_ERR(ce);\n\t\treturn;\n\t}\n\n\terr = intel_context_pin(ce);\n\tif (err) {\n\t\tintel_context_put(ce);\n\t\tthread->result = err;\n\t\treturn;\n\t}\n\n\tif (intel_engine_supports_stats(engine)) {\n\t\tp->busy = intel_engine_get_busy_time(engine, &p->time);\n\t\tbusy = true;\n\t} else {\n\t\tp->time = ktime_get();\n\t\tbusy = false;\n\t}\n\n\tcount = 0;\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\terr = 0;\n\t\tif (i915_request_wait(rq, 0, HZ) < 0)\n\t\t\terr = -ETIME;\n\t\ti915_request_put(rq);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcount++;\n\t} while (!__igt_timeout(end_time, NULL));\n\n\tif (busy) {\n\t\tktime_t now;\n\n\t\tp->busy = ktime_sub(intel_engine_get_busy_time(engine, &now),\n\t\t\t\t    p->busy);\n\t\tp->time = ktime_sub(now, p->time);\n\t} else {\n\t\tp->time = ktime_sub(ktime_get(), p->time);\n\t}\n\n\terr = switch_to_kernel_sync(ce, err);\n\tp->runtime = intel_context_get_total_runtime_ns(ce);\n\tp->count = count;\n\n\tintel_context_unpin(ce);\n\tintel_context_put(ce);\n\tthread->result = err;\n}\n\nstatic void p_sync1(struct kthread_work *work)\n{\n\tstruct p_thread *thread = container_of(work, typeof(*thread), work);\n\tstruct perf_stats *p = &thread->p;\n\tstruct intel_engine_cs *engine = p->engine;\n\tstruct i915_request *prev = NULL;\n\tstruct intel_context *ce;\n\tIGT_TIMEOUT(end_time);\n\tunsigned long count;\n\tbool busy;\n\tint err = 0;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\tthread->result = PTR_ERR(ce);\n\t\treturn;\n\t}\n\n\terr = intel_context_pin(ce);\n\tif (err) {\n\t\tintel_context_put(ce);\n\t\tthread->result = err;\n\t\treturn;\n\t}\n\n\tif (intel_engine_supports_stats(engine)) {\n\t\tp->busy = intel_engine_get_busy_time(engine, &p->time);\n\t\tbusy = true;\n\t} else {\n\t\tp->time = ktime_get();\n\t\tbusy = false;\n\t}\n\n\tcount = 0;\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\terr = 0;\n\t\tif (prev && i915_request_wait(prev, 0, HZ) < 0)\n\t\t\terr = -ETIME;\n\t\ti915_request_put(prev);\n\t\tprev = rq;\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcount++;\n\t} while (!__igt_timeout(end_time, NULL));\n\ti915_request_put(prev);\n\n\tif (busy) {\n\t\tktime_t now;\n\n\t\tp->busy = ktime_sub(intel_engine_get_busy_time(engine, &now),\n\t\t\t\t    p->busy);\n\t\tp->time = ktime_sub(now, p->time);\n\t} else {\n\t\tp->time = ktime_sub(ktime_get(), p->time);\n\t}\n\n\terr = switch_to_kernel_sync(ce, err);\n\tp->runtime = intel_context_get_total_runtime_ns(ce);\n\tp->count = count;\n\n\tintel_context_unpin(ce);\n\tintel_context_put(ce);\n\tthread->result = err;\n}\n\nstatic void p_many(struct kthread_work *work)\n{\n\tstruct p_thread *thread = container_of(work, typeof(*thread), work);\n\tstruct perf_stats *p = &thread->p;\n\tstruct intel_engine_cs *engine = p->engine;\n\tstruct intel_context *ce;\n\tIGT_TIMEOUT(end_time);\n\tunsigned long count;\n\tint err = 0;\n\tbool busy;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce)) {\n\t\tthread->result = PTR_ERR(ce);\n\t\treturn;\n\t}\n\n\terr = intel_context_pin(ce);\n\tif (err) {\n\t\tintel_context_put(ce);\n\t\tthread->result = err;\n\t\treturn;\n\t}\n\n\tif (intel_engine_supports_stats(engine)) {\n\t\tp->busy = intel_engine_get_busy_time(engine, &p->time);\n\t\tbusy = true;\n\t} else {\n\t\tp->time = ktime_get();\n\t\tbusy = false;\n\t}\n\n\tcount = 0;\n\tdo {\n\t\tstruct i915_request *rq;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tbreak;\n\t\t}\n\n\t\ti915_request_add(rq);\n\t\tcount++;\n\t} while (!__igt_timeout(end_time, NULL));\n\n\tif (busy) {\n\t\tktime_t now;\n\n\t\tp->busy = ktime_sub(intel_engine_get_busy_time(engine, &now),\n\t\t\t\t    p->busy);\n\t\tp->time = ktime_sub(now, p->time);\n\t} else {\n\t\tp->time = ktime_sub(ktime_get(), p->time);\n\t}\n\n\terr = switch_to_kernel_sync(ce, err);\n\tp->runtime = intel_context_get_total_runtime_ns(ce);\n\tp->count = count;\n\n\tintel_context_unpin(ce);\n\tintel_context_put(ce);\n\tthread->result = err;\n}\n\nstatic int perf_parallel_engines(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstatic void (* const func[])(struct kthread_work *) = {\n\t\tp_sync0,\n\t\tp_sync1,\n\t\tp_many,\n\t\tNULL,\n\t};\n\tconst unsigned int nengines = num_uabi_engines(i915);\n\tvoid (* const *fn)(struct kthread_work *);\n\tstruct intel_engine_cs *engine;\n\tstruct pm_qos_request qos;\n\tstruct p_thread *engines;\n\tint err = 0;\n\n\tengines = kcalloc(nengines, sizeof(*engines), GFP_KERNEL);\n\tif (!engines)\n\t\treturn -ENOMEM;\n\n\tcpu_latency_qos_add_request(&qos, 0);\n\n\tfor (fn = func; *fn; fn++) {\n\t\tchar name[KSYM_NAME_LEN];\n\t\tstruct igt_live_test t;\n\t\tunsigned int idx;\n\n\t\tsnprintf(name, sizeof(name), \"%ps\", *fn);\n\t\terr = igt_live_test_begin(&t, i915, __func__, name);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tatomic_set(&i915->selftest.counter, nengines);\n\n\t\tidx = 0;\n\t\tfor_each_uabi_engine(engine, i915) {\n\t\t\tstruct kthread_worker *worker;\n\n\t\t\tintel_engine_pm_get(engine);\n\n\t\t\tmemset(&engines[idx].p, 0, sizeof(engines[idx].p));\n\n\t\t\tworker = kthread_create_worker(0, \"igt:%s\",\n\t\t\t\t\t\t       engine->name);\n\t\t\tif (IS_ERR(worker)) {\n\t\t\t\terr = PTR_ERR(worker);\n\t\t\t\tintel_engine_pm_put(engine);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tengines[idx].worker = worker;\n\t\t\tengines[idx].result = 0;\n\t\t\tengines[idx].p.engine = engine;\n\t\t\tengines[idx].engine = engine;\n\n\t\t\tkthread_init_work(&engines[idx].work, *fn);\n\t\t\tkthread_queue_work(worker, &engines[idx].work);\n\t\t\tidx++;\n\t\t}\n\n\t\tidx = 0;\n\t\tfor_each_uabi_engine(engine, i915) {\n\t\t\tint status;\n\n\t\t\tif (!engines[idx].worker)\n\t\t\t\tbreak;\n\n\t\t\tkthread_flush_work(&engines[idx].work);\n\t\t\tstatus = READ_ONCE(engines[idx].result);\n\t\t\tif (status && !err)\n\t\t\t\terr = status;\n\n\t\t\tintel_engine_pm_put(engine);\n\n\t\t\tkthread_destroy_worker(engines[idx].worker);\n\t\t\tidx++;\n\t\t}\n\n\t\tif (igt_live_test_end(&t))\n\t\t\terr = -EIO;\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tidx = 0;\n\t\tfor_each_uabi_engine(engine, i915) {\n\t\t\tstruct perf_stats *p = &engines[idx].p;\n\t\t\tu64 busy = 100 * ktime_to_ns(p->busy);\n\t\t\tu64 dt = ktime_to_ns(p->time);\n\t\t\tint integer, decimal;\n\n\t\t\tif (dt) {\n\t\t\t\tinteger = div64_u64(busy, dt);\n\t\t\t\tbusy -= integer * dt;\n\t\t\t\tdecimal = div64_u64(100 * busy, dt);\n\t\t\t} else {\n\t\t\t\tinteger = 0;\n\t\t\t\tdecimal = 0;\n\t\t\t}\n\n\t\t\tGEM_BUG_ON(engine != p->engine);\n\t\t\tpr_info(\"%s %5s: { count:%lu, busy:%d.%02d%%, runtime:%lldms, walltime:%lldms }\\n\",\n\t\t\t\tname, engine->name, p->count, integer, decimal,\n\t\t\t\tdiv_u64(p->runtime, 1000 * 1000),\n\t\t\t\tdiv_u64(ktime_to_ns(p->time), 1000 * 1000));\n\t\t\tidx++;\n\t\t}\n\t}\n\n\tcpu_latency_qos_remove_request(&qos);\n\tkfree(engines);\n\treturn err;\n}\n\nint i915_request_perf_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(perf_request_latency),\n\t\tSUBTEST(perf_series_engines),\n\t\tSUBTEST(perf_parallel_engines),\n\t};\n\n\tif (intel_gt_is_wedged(to_gt(i915)))\n\t\treturn 0;\n\n\treturn i915_subtests(tests, i915);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}