{
  "module_name": "sched_policy.c",
  "hash_id": "c1d14df516c3346f53511742aa0b67d5acce4f55c98963cd0c1480dbe7caa4d4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/sched_policy.c",
  "human_readable_source": " \n\n#include \"i915_drv.h\"\n#include \"gvt.h\"\n\nstatic bool vgpu_has_pending_workload(struct intel_vgpu *vgpu)\n{\n\tenum intel_engine_id i;\n\tstruct intel_engine_cs *engine;\n\n\tfor_each_engine(engine, vgpu->gvt->gt, i) {\n\t\tif (!list_empty(workload_q_head(vgpu, engine)))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \n#define GVT_SCHED_VGPU_PRI_TIME  2\n\nstruct vgpu_sched_data {\n\tstruct list_head lru_list;\n\tstruct intel_vgpu *vgpu;\n\tbool active;\n\tbool pri_sched;\n\tktime_t pri_time;\n\tktime_t sched_in_time;\n\tktime_t sched_time;\n\tktime_t left_ts;\n\tktime_t allocated_ts;\n\n\tstruct vgpu_sched_ctl sched_ctl;\n};\n\nstruct gvt_sched_data {\n\tstruct intel_gvt *gvt;\n\tstruct hrtimer timer;\n\tunsigned long period;\n\tstruct list_head lru_runq_head;\n\tktime_t expire_time;\n};\n\nstatic void vgpu_update_timeslice(struct intel_vgpu *vgpu, ktime_t cur_time)\n{\n\tktime_t delta_ts;\n\tstruct vgpu_sched_data *vgpu_data;\n\n\tif (!vgpu || vgpu == vgpu->gvt->idle_vgpu)\n\t\treturn;\n\n\tvgpu_data = vgpu->sched_data;\n\tdelta_ts = ktime_sub(cur_time, vgpu_data->sched_in_time);\n\tvgpu_data->sched_time = ktime_add(vgpu_data->sched_time, delta_ts);\n\tvgpu_data->left_ts = ktime_sub(vgpu_data->left_ts, delta_ts);\n\tvgpu_data->sched_in_time = cur_time;\n}\n\n#define GVT_TS_BALANCE_PERIOD_MS 100\n#define GVT_TS_BALANCE_STAGE_NUM 10\n\nstatic void gvt_balance_timeslice(struct gvt_sched_data *sched_data)\n{\n\tstruct vgpu_sched_data *vgpu_data;\n\tstruct list_head *pos;\n\tstatic u64 stage_check;\n\tint stage = stage_check++ % GVT_TS_BALANCE_STAGE_NUM;\n\n\t \n\tif (stage == 0) {\n\t\tint total_weight = 0;\n\t\tktime_t fair_timeslice;\n\n\t\tlist_for_each(pos, &sched_data->lru_runq_head) {\n\t\t\tvgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);\n\t\t\ttotal_weight += vgpu_data->sched_ctl.weight;\n\t\t}\n\n\t\tlist_for_each(pos, &sched_data->lru_runq_head) {\n\t\t\tvgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);\n\t\t\tfair_timeslice = ktime_divns(ms_to_ktime(GVT_TS_BALANCE_PERIOD_MS),\n\t\t\t\t\t\t     total_weight) * vgpu_data->sched_ctl.weight;\n\n\t\t\tvgpu_data->allocated_ts = fair_timeslice;\n\t\t\tvgpu_data->left_ts = vgpu_data->allocated_ts;\n\t\t}\n\t} else {\n\t\tlist_for_each(pos, &sched_data->lru_runq_head) {\n\t\t\tvgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);\n\n\t\t\t \n\t\t\tvgpu_data->left_ts += vgpu_data->allocated_ts;\n\t\t}\n\t}\n}\n\nstatic void try_to_schedule_next_vgpu(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tenum intel_engine_id i;\n\tstruct intel_engine_cs *engine;\n\tstruct vgpu_sched_data *vgpu_data;\n\tktime_t cur_time;\n\n\t \n\tif (scheduler->next_vgpu == scheduler->current_vgpu) {\n\t\tscheduler->next_vgpu = NULL;\n\t\treturn;\n\t}\n\n\t \n\tscheduler->need_reschedule = true;\n\n\t \n\tfor_each_engine(engine, gvt->gt, i) {\n\t\tif (scheduler->current_workload[engine->id])\n\t\t\treturn;\n\t}\n\n\tcur_time = ktime_get();\n\tvgpu_update_timeslice(scheduler->current_vgpu, cur_time);\n\tvgpu_data = scheduler->next_vgpu->sched_data;\n\tvgpu_data->sched_in_time = cur_time;\n\n\t \n\tscheduler->current_vgpu = scheduler->next_vgpu;\n\tscheduler->next_vgpu = NULL;\n\n\tscheduler->need_reschedule = false;\n\n\t \n\tfor_each_engine(engine, gvt->gt, i)\n\t\twake_up(&scheduler->waitq[engine->id]);\n}\n\nstatic struct intel_vgpu *find_busy_vgpu(struct gvt_sched_data *sched_data)\n{\n\tstruct vgpu_sched_data *vgpu_data;\n\tstruct intel_vgpu *vgpu = NULL;\n\tstruct list_head *head = &sched_data->lru_runq_head;\n\tstruct list_head *pos;\n\n\t \n\tlist_for_each(pos, head) {\n\n\t\tvgpu_data = container_of(pos, struct vgpu_sched_data, lru_list);\n\t\tif (!vgpu_has_pending_workload(vgpu_data->vgpu))\n\t\t\tcontinue;\n\n\t\tif (vgpu_data->pri_sched) {\n\t\t\tif (ktime_before(ktime_get(), vgpu_data->pri_time)) {\n\t\t\t\tvgpu = vgpu_data->vgpu;\n\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\tvgpu_data->pri_sched = false;\n\t\t}\n\n\t\t \n\t\tif (vgpu_data->left_ts > 0) {\n\t\t\tvgpu = vgpu_data->vgpu;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn vgpu;\n}\n\n \n#define GVT_DEFAULT_TIME_SLICE 1000000\n\nstatic void tbs_sched_func(struct gvt_sched_data *sched_data)\n{\n\tstruct intel_gvt *gvt = sched_data->gvt;\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct vgpu_sched_data *vgpu_data;\n\tstruct intel_vgpu *vgpu = NULL;\n\n\t \n\tif (list_empty(&sched_data->lru_runq_head) || scheduler->next_vgpu)\n\t\tgoto out;\n\n\tvgpu = find_busy_vgpu(sched_data);\n\tif (vgpu) {\n\t\tscheduler->next_vgpu = vgpu;\n\t\tvgpu_data = vgpu->sched_data;\n\t\tif (!vgpu_data->pri_sched) {\n\t\t\t \n\t\t\tlist_del_init(&vgpu_data->lru_list);\n\t\t\tlist_add_tail(&vgpu_data->lru_list,\n\t\t\t\t      &sched_data->lru_runq_head);\n\t\t}\n\t} else {\n\t\tscheduler->next_vgpu = gvt->idle_vgpu;\n\t}\nout:\n\tif (scheduler->next_vgpu)\n\t\ttry_to_schedule_next_vgpu(gvt);\n}\n\nvoid intel_gvt_schedule(struct intel_gvt *gvt)\n{\n\tstruct gvt_sched_data *sched_data = gvt->scheduler.sched_data;\n\tktime_t cur_time;\n\n\tmutex_lock(&gvt->sched_lock);\n\tcur_time = ktime_get();\n\n\tif (test_and_clear_bit(INTEL_GVT_REQUEST_SCHED,\n\t\t\t\t(void *)&gvt->service_request)) {\n\t\tif (cur_time >= sched_data->expire_time) {\n\t\t\tgvt_balance_timeslice(sched_data);\n\t\t\tsched_data->expire_time = ktime_add_ms(\n\t\t\t\tcur_time, GVT_TS_BALANCE_PERIOD_MS);\n\t\t}\n\t}\n\tclear_bit(INTEL_GVT_REQUEST_EVENT_SCHED, (void *)&gvt->service_request);\n\n\tvgpu_update_timeslice(gvt->scheduler.current_vgpu, cur_time);\n\ttbs_sched_func(sched_data);\n\n\tmutex_unlock(&gvt->sched_lock);\n}\n\nstatic enum hrtimer_restart tbs_timer_fn(struct hrtimer *timer_data)\n{\n\tstruct gvt_sched_data *data;\n\n\tdata = container_of(timer_data, struct gvt_sched_data, timer);\n\n\tintel_gvt_request_service(data->gvt, INTEL_GVT_REQUEST_SCHED);\n\n\thrtimer_add_expires_ns(&data->timer, data->period);\n\n\treturn HRTIMER_RESTART;\n}\n\nstatic int tbs_sched_init(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler =\n\t\t&gvt->scheduler;\n\n\tstruct gvt_sched_data *data;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&data->lru_runq_head);\n\thrtimer_init(&data->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);\n\tdata->timer.function = tbs_timer_fn;\n\tdata->period = GVT_DEFAULT_TIME_SLICE;\n\tdata->gvt = gvt;\n\n\tscheduler->sched_data = data;\n\n\treturn 0;\n}\n\nstatic void tbs_sched_clean(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler =\n\t\t&gvt->scheduler;\n\tstruct gvt_sched_data *data = scheduler->sched_data;\n\n\thrtimer_cancel(&data->timer);\n\n\tkfree(data);\n\tscheduler->sched_data = NULL;\n}\n\nstatic int tbs_sched_init_vgpu(struct intel_vgpu *vgpu)\n{\n\tstruct vgpu_sched_data *data;\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tdata->sched_ctl.weight = vgpu->sched_ctl.weight;\n\tdata->vgpu = vgpu;\n\tINIT_LIST_HEAD(&data->lru_list);\n\n\tvgpu->sched_data = data;\n\n\treturn 0;\n}\n\nstatic void tbs_sched_clean_vgpu(struct intel_vgpu *vgpu)\n{\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct gvt_sched_data *sched_data = gvt->scheduler.sched_data;\n\n\tkfree(vgpu->sched_data);\n\tvgpu->sched_data = NULL;\n\n\t \n\tif (idr_is_empty(&gvt->vgpu_idr))\n\t\thrtimer_cancel(&sched_data->timer);\n}\n\nstatic void tbs_sched_start_schedule(struct intel_vgpu *vgpu)\n{\n\tstruct gvt_sched_data *sched_data = vgpu->gvt->scheduler.sched_data;\n\tstruct vgpu_sched_data *vgpu_data = vgpu->sched_data;\n\tktime_t now;\n\n\tif (!list_empty(&vgpu_data->lru_list))\n\t\treturn;\n\n\tnow = ktime_get();\n\tvgpu_data->pri_time = ktime_add(now,\n\t\t\t\t\tktime_set(GVT_SCHED_VGPU_PRI_TIME, 0));\n\tvgpu_data->pri_sched = true;\n\n\tlist_add(&vgpu_data->lru_list, &sched_data->lru_runq_head);\n\n\tif (!hrtimer_active(&sched_data->timer))\n\t\thrtimer_start(&sched_data->timer, ktime_add_ns(ktime_get(),\n\t\t\tsched_data->period), HRTIMER_MODE_ABS);\n\tvgpu_data->active = true;\n}\n\nstatic void tbs_sched_stop_schedule(struct intel_vgpu *vgpu)\n{\n\tstruct vgpu_sched_data *vgpu_data = vgpu->sched_data;\n\n\tlist_del_init(&vgpu_data->lru_list);\n\tvgpu_data->active = false;\n}\n\nstatic const struct intel_gvt_sched_policy_ops tbs_schedule_ops = {\n\t.init = tbs_sched_init,\n\t.clean = tbs_sched_clean,\n\t.init_vgpu = tbs_sched_init_vgpu,\n\t.clean_vgpu = tbs_sched_clean_vgpu,\n\t.start_schedule = tbs_sched_start_schedule,\n\t.stop_schedule = tbs_sched_stop_schedule,\n};\n\nint intel_gvt_init_sched_policy(struct intel_gvt *gvt)\n{\n\tint ret;\n\n\tmutex_lock(&gvt->sched_lock);\n\tgvt->scheduler.sched_ops = &tbs_schedule_ops;\n\tret = gvt->scheduler.sched_ops->init(gvt);\n\tmutex_unlock(&gvt->sched_lock);\n\n\treturn ret;\n}\n\nvoid intel_gvt_clean_sched_policy(struct intel_gvt *gvt)\n{\n\tmutex_lock(&gvt->sched_lock);\n\tgvt->scheduler.sched_ops->clean(gvt);\n\tmutex_unlock(&gvt->sched_lock);\n}\n\n \n\nint intel_vgpu_init_sched_policy(struct intel_vgpu *vgpu)\n{\n\tint ret;\n\n\tmutex_lock(&vgpu->gvt->sched_lock);\n\tret = vgpu->gvt->scheduler.sched_ops->init_vgpu(vgpu);\n\tmutex_unlock(&vgpu->gvt->sched_lock);\n\n\treturn ret;\n}\n\nvoid intel_vgpu_clean_sched_policy(struct intel_vgpu *vgpu)\n{\n\tmutex_lock(&vgpu->gvt->sched_lock);\n\tvgpu->gvt->scheduler.sched_ops->clean_vgpu(vgpu);\n\tmutex_unlock(&vgpu->gvt->sched_lock);\n}\n\nvoid intel_vgpu_start_schedule(struct intel_vgpu *vgpu)\n{\n\tstruct vgpu_sched_data *vgpu_data = vgpu->sched_data;\n\n\tmutex_lock(&vgpu->gvt->sched_lock);\n\tif (!vgpu_data->active) {\n\t\tgvt_dbg_core(\"vgpu%d: start schedule\\n\", vgpu->id);\n\t\tvgpu->gvt->scheduler.sched_ops->start_schedule(vgpu);\n\t}\n\tmutex_unlock(&vgpu->gvt->sched_lock);\n}\n\nvoid intel_gvt_kick_schedule(struct intel_gvt *gvt)\n{\n\tmutex_lock(&gvt->sched_lock);\n\tintel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);\n\tmutex_unlock(&gvt->sched_lock);\n}\n\nvoid intel_vgpu_stop_schedule(struct intel_vgpu *vgpu)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler =\n\t\t&vgpu->gvt->scheduler;\n\tstruct vgpu_sched_data *vgpu_data = vgpu->sched_data;\n\tstruct drm_i915_private *dev_priv = vgpu->gvt->gt->i915;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tif (!vgpu_data->active)\n\t\treturn;\n\n\tgvt_dbg_core(\"vgpu%d: stop schedule\\n\", vgpu->id);\n\n\tmutex_lock(&vgpu->gvt->sched_lock);\n\tscheduler->sched_ops->stop_schedule(vgpu);\n\n\tif (scheduler->next_vgpu == vgpu)\n\t\tscheduler->next_vgpu = NULL;\n\n\tif (scheduler->current_vgpu == vgpu) {\n\t\t \n\t\tscheduler->need_reschedule = true;\n\t\tscheduler->current_vgpu = NULL;\n\t}\n\n\tintel_runtime_pm_get(&dev_priv->runtime_pm);\n\tspin_lock_bh(&scheduler->mmio_context_lock);\n\tfor_each_engine(engine, vgpu->gvt->gt, id) {\n\t\tif (scheduler->engine_owner[engine->id] == vgpu) {\n\t\t\tintel_gvt_switch_mmio(vgpu, NULL, engine);\n\t\t\tscheduler->engine_owner[engine->id] = NULL;\n\t\t}\n\t}\n\tspin_unlock_bh(&scheduler->mmio_context_lock);\n\tintel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);\n\tmutex_unlock(&vgpu->gvt->sched_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}