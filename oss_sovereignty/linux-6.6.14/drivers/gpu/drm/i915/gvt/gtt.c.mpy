{
  "module_name": "gtt.c",
  "hash_id": "1dcf3dd34f45a63b98ded5f71127fba263a3f3276fa7af448519a38fbd2ec804",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/gtt.c",
  "human_readable_source": " \n\n#include \"i915_drv.h\"\n#include \"gvt.h\"\n#include \"i915_pvinfo.h\"\n#include \"trace.h\"\n\n#include \"gt/intel_gt_regs.h\"\n\n#if defined(VERBOSE_DEBUG)\n#define gvt_vdbg_mm(fmt, args...) gvt_dbg_mm(fmt, ##args)\n#else\n#define gvt_vdbg_mm(fmt, args...)\n#endif\n\nstatic bool enable_out_of_sync = false;\nstatic int preallocated_oos_pages = 8192;\n\n \nbool intel_gvt_ggtt_validate_range(struct intel_vgpu *vgpu, u64 addr, u32 size)\n{\n\tif (size == 0)\n\t\treturn vgpu_gmadr_is_valid(vgpu, addr);\n\n\tif (vgpu_gmadr_is_aperture(vgpu, addr) &&\n\t    vgpu_gmadr_is_aperture(vgpu, addr + size - 1))\n\t\treturn true;\n\telse if (vgpu_gmadr_is_hidden(vgpu, addr) &&\n\t\t vgpu_gmadr_is_hidden(vgpu, addr + size - 1))\n\t\treturn true;\n\n\tgvt_dbg_mm(\"Invalid ggtt range at 0x%llx, size: 0x%x\\n\",\n\t\t     addr, size);\n\treturn false;\n}\n\n \nint intel_gvt_ggtt_gmadr_g2h(struct intel_vgpu *vgpu, u64 g_addr, u64 *h_addr)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\n\tif (drm_WARN(&i915->drm, !vgpu_gmadr_is_valid(vgpu, g_addr),\n\t\t     \"invalid guest gmadr %llx\\n\", g_addr))\n\t\treturn -EACCES;\n\n\tif (vgpu_gmadr_is_aperture(vgpu, g_addr))\n\t\t*h_addr = vgpu_aperture_gmadr_base(vgpu)\n\t\t\t  + (g_addr - vgpu_aperture_offset(vgpu));\n\telse\n\t\t*h_addr = vgpu_hidden_gmadr_base(vgpu)\n\t\t\t  + (g_addr - vgpu_hidden_offset(vgpu));\n\treturn 0;\n}\n\n \nint intel_gvt_ggtt_gmadr_h2g(struct intel_vgpu *vgpu, u64 h_addr, u64 *g_addr)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\n\tif (drm_WARN(&i915->drm, !gvt_gmadr_is_valid(vgpu->gvt, h_addr),\n\t\t     \"invalid host gmadr %llx\\n\", h_addr))\n\t\treturn -EACCES;\n\n\tif (gvt_gmadr_is_aperture(vgpu->gvt, h_addr))\n\t\t*g_addr = vgpu_aperture_gmadr_base(vgpu)\n\t\t\t+ (h_addr - gvt_aperture_gmadr_base(vgpu->gvt));\n\telse\n\t\t*g_addr = vgpu_hidden_gmadr_base(vgpu)\n\t\t\t+ (h_addr - gvt_hidden_gmadr_base(vgpu->gvt));\n\treturn 0;\n}\n\nint intel_gvt_ggtt_index_g2h(struct intel_vgpu *vgpu, unsigned long g_index,\n\t\t\t     unsigned long *h_index)\n{\n\tu64 h_addr;\n\tint ret;\n\n\tret = intel_gvt_ggtt_gmadr_g2h(vgpu, g_index << I915_GTT_PAGE_SHIFT,\n\t\t\t\t       &h_addr);\n\tif (ret)\n\t\treturn ret;\n\n\t*h_index = h_addr >> I915_GTT_PAGE_SHIFT;\n\treturn 0;\n}\n\nint intel_gvt_ggtt_h2g_index(struct intel_vgpu *vgpu, unsigned long h_index,\n\t\t\t     unsigned long *g_index)\n{\n\tu64 g_addr;\n\tint ret;\n\n\tret = intel_gvt_ggtt_gmadr_h2g(vgpu, h_index << I915_GTT_PAGE_SHIFT,\n\t\t\t\t       &g_addr);\n\tif (ret)\n\t\treturn ret;\n\n\t*g_index = g_addr >> I915_GTT_PAGE_SHIFT;\n\treturn 0;\n}\n\n#define gtt_type_is_entry(type) \\\n\t(type > GTT_TYPE_INVALID && type < GTT_TYPE_PPGTT_ENTRY \\\n\t && type != GTT_TYPE_PPGTT_PTE_ENTRY \\\n\t && type != GTT_TYPE_PPGTT_ROOT_ENTRY)\n\n#define gtt_type_is_pt(type) \\\n\t(type >= GTT_TYPE_PPGTT_PTE_PT && type < GTT_TYPE_MAX)\n\n#define gtt_type_is_pte_pt(type) \\\n\t(type == GTT_TYPE_PPGTT_PTE_PT)\n\n#define gtt_type_is_root_pointer(type) \\\n\t(gtt_type_is_entry(type) && type > GTT_TYPE_PPGTT_ROOT_ENTRY)\n\n#define gtt_init_entry(e, t, p, v) do { \\\n\t(e)->type = t; \\\n\t(e)->pdev = p; \\\n\tmemcpy(&(e)->val64, &v, sizeof(v)); \\\n} while (0)\n\n \n\nstruct gtt_type_table_entry {\n\tint entry_type;\n\tint pt_type;\n\tint next_pt_type;\n\tint pse_entry_type;\n};\n\n#define GTT_TYPE_TABLE_ENTRY(type, e_type, cpt_type, npt_type, pse_type) \\\n\t[type] = { \\\n\t\t.entry_type = e_type, \\\n\t\t.pt_type = cpt_type, \\\n\t\t.next_pt_type = npt_type, \\\n\t\t.pse_entry_type = pse_type, \\\n\t}\n\nstatic const struct gtt_type_table_entry gtt_type_table[] = {\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_ROOT_L4_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_ROOT_L4_ENTRY,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PML4_PT,\n\t\t\tGTT_TYPE_INVALID),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PML4_PT,\n\t\t\tGTT_TYPE_PPGTT_PML4_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PML4_PT,\n\t\t\tGTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_INVALID),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PML4_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PML4_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PML4_PT,\n\t\t\tGTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_INVALID),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_PPGTT_PDP_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_1G_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_ROOT_L3_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_ROOT_L3_ENTRY,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_1G_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDP_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDP_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_1G_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PDE_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_2M_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PDE_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDE_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_2M_ENTRY),\n\t \n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_PPGTT_PTE_4K_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PTE_64K_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_4K_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PTE_4K_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PTE_64K_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_64K_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PTE_4K_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PTE_PT,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PTE_64K_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_2M_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDE_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDE_PT,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PTE_2M_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_PPGTT_PTE_1G_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDP_ENTRY,\n\t\t\tGTT_TYPE_PPGTT_PDP_PT,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_PPGTT_PTE_1G_ENTRY),\n\tGTT_TYPE_TABLE_ENTRY(GTT_TYPE_GGTT_PTE,\n\t\t\tGTT_TYPE_GGTT_PTE,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_INVALID,\n\t\t\tGTT_TYPE_INVALID),\n};\n\nstatic inline int get_next_pt_type(int type)\n{\n\treturn gtt_type_table[type].next_pt_type;\n}\n\nstatic inline int get_entry_type(int type)\n{\n\treturn gtt_type_table[type].entry_type;\n}\n\nstatic inline int get_pse_type(int type)\n{\n\treturn gtt_type_table[type].pse_entry_type;\n}\n\nstatic u64 read_pte64(struct i915_ggtt *ggtt, unsigned long index)\n{\n\tvoid __iomem *addr = (gen8_pte_t __iomem *)ggtt->gsm + index;\n\n\treturn readq(addr);\n}\n\nstatic void ggtt_invalidate(struct intel_gt *gt)\n{\n\tmmio_hw_access_pre(gt);\n\tintel_uncore_write(gt->uncore, GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);\n\tmmio_hw_access_post(gt);\n}\n\nstatic void write_pte64(struct i915_ggtt *ggtt, unsigned long index, u64 pte)\n{\n\tvoid __iomem *addr = (gen8_pte_t __iomem *)ggtt->gsm + index;\n\n\twriteq(pte, addr);\n}\n\nstatic inline int gtt_get_entry64(void *pt,\n\t\tstruct intel_gvt_gtt_entry *e,\n\t\tunsigned long index, bool hypervisor_access, unsigned long gpa,\n\t\tstruct intel_vgpu *vgpu)\n{\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tint ret;\n\n\tif (WARN_ON(info->gtt_entry_size != 8))\n\t\treturn -EINVAL;\n\n\tif (hypervisor_access) {\n\t\tret = intel_gvt_read_gpa(vgpu, gpa +\n\t\t\t\t(index << info->gtt_entry_size_shift),\n\t\t\t\t&e->val64, 8);\n\t\tif (WARN_ON(ret))\n\t\t\treturn ret;\n\t} else if (!pt) {\n\t\te->val64 = read_pte64(vgpu->gvt->gt->ggtt, index);\n\t} else {\n\t\te->val64 = *((u64 *)pt + index);\n\t}\n\treturn 0;\n}\n\nstatic inline int gtt_set_entry64(void *pt,\n\t\tstruct intel_gvt_gtt_entry *e,\n\t\tunsigned long index, bool hypervisor_access, unsigned long gpa,\n\t\tstruct intel_vgpu *vgpu)\n{\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tint ret;\n\n\tif (WARN_ON(info->gtt_entry_size != 8))\n\t\treturn -EINVAL;\n\n\tif (hypervisor_access) {\n\t\tret = intel_gvt_write_gpa(vgpu, gpa +\n\t\t\t\t(index << info->gtt_entry_size_shift),\n\t\t\t\t&e->val64, 8);\n\t\tif (WARN_ON(ret))\n\t\t\treturn ret;\n\t} else if (!pt) {\n\t\twrite_pte64(vgpu->gvt->gt->ggtt, index, e->val64);\n\t} else {\n\t\t*((u64 *)pt + index) = e->val64;\n\t}\n\treturn 0;\n}\n\n#define GTT_HAW 46\n\n#define ADDR_1G_MASK\tGENMASK_ULL(GTT_HAW - 1, 30)\n#define ADDR_2M_MASK\tGENMASK_ULL(GTT_HAW - 1, 21)\n#define ADDR_64K_MASK\tGENMASK_ULL(GTT_HAW - 1, 16)\n#define ADDR_4K_MASK\tGENMASK_ULL(GTT_HAW - 1, 12)\n\n#define GTT_SPTE_FLAG_MASK GENMASK_ULL(62, 52)\n#define GTT_SPTE_FLAG_64K_SPLITED BIT(52)  \n\n#define GTT_64K_PTE_STRIDE 16\n\nstatic unsigned long gen8_gtt_get_pfn(struct intel_gvt_gtt_entry *e)\n{\n\tunsigned long pfn;\n\n\tif (e->type == GTT_TYPE_PPGTT_PTE_1G_ENTRY)\n\t\tpfn = (e->val64 & ADDR_1G_MASK) >> PAGE_SHIFT;\n\telse if (e->type == GTT_TYPE_PPGTT_PTE_2M_ENTRY)\n\t\tpfn = (e->val64 & ADDR_2M_MASK) >> PAGE_SHIFT;\n\telse if (e->type == GTT_TYPE_PPGTT_PTE_64K_ENTRY)\n\t\tpfn = (e->val64 & ADDR_64K_MASK) >> PAGE_SHIFT;\n\telse\n\t\tpfn = (e->val64 & ADDR_4K_MASK) >> PAGE_SHIFT;\n\treturn pfn;\n}\n\nstatic void gen8_gtt_set_pfn(struct intel_gvt_gtt_entry *e, unsigned long pfn)\n{\n\tif (e->type == GTT_TYPE_PPGTT_PTE_1G_ENTRY) {\n\t\te->val64 &= ~ADDR_1G_MASK;\n\t\tpfn &= (ADDR_1G_MASK >> PAGE_SHIFT);\n\t} else if (e->type == GTT_TYPE_PPGTT_PTE_2M_ENTRY) {\n\t\te->val64 &= ~ADDR_2M_MASK;\n\t\tpfn &= (ADDR_2M_MASK >> PAGE_SHIFT);\n\t} else if (e->type == GTT_TYPE_PPGTT_PTE_64K_ENTRY) {\n\t\te->val64 &= ~ADDR_64K_MASK;\n\t\tpfn &= (ADDR_64K_MASK >> PAGE_SHIFT);\n\t} else {\n\t\te->val64 &= ~ADDR_4K_MASK;\n\t\tpfn &= (ADDR_4K_MASK >> PAGE_SHIFT);\n\t}\n\n\te->val64 |= (pfn << PAGE_SHIFT);\n}\n\nstatic bool gen8_gtt_test_pse(struct intel_gvt_gtt_entry *e)\n{\n\treturn !!(e->val64 & _PAGE_PSE);\n}\n\nstatic void gen8_gtt_clear_pse(struct intel_gvt_gtt_entry *e)\n{\n\tif (gen8_gtt_test_pse(e)) {\n\t\tswitch (e->type) {\n\t\tcase GTT_TYPE_PPGTT_PTE_2M_ENTRY:\n\t\t\te->val64 &= ~_PAGE_PSE;\n\t\t\te->type = GTT_TYPE_PPGTT_PDE_ENTRY;\n\t\t\tbreak;\n\t\tcase GTT_TYPE_PPGTT_PTE_1G_ENTRY:\n\t\t\te->type = GTT_TYPE_PPGTT_PDP_ENTRY;\n\t\t\te->val64 &= ~_PAGE_PSE;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON(1);\n\t\t}\n\t}\n}\n\nstatic bool gen8_gtt_test_ips(struct intel_gvt_gtt_entry *e)\n{\n\tif (GEM_WARN_ON(e->type != GTT_TYPE_PPGTT_PDE_ENTRY))\n\t\treturn false;\n\n\treturn !!(e->val64 & GEN8_PDE_IPS_64K);\n}\n\nstatic void gen8_gtt_clear_ips(struct intel_gvt_gtt_entry *e)\n{\n\tif (GEM_WARN_ON(e->type != GTT_TYPE_PPGTT_PDE_ENTRY))\n\t\treturn;\n\n\te->val64 &= ~GEN8_PDE_IPS_64K;\n}\n\nstatic bool gen8_gtt_test_present(struct intel_gvt_gtt_entry *e)\n{\n\t \n\tif (e->type == GTT_TYPE_PPGTT_ROOT_L3_ENTRY\n\t\t\t|| e->type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY)\n\t\treturn (e->val64 != 0);\n\telse\n\t\treturn (e->val64 & GEN8_PAGE_PRESENT);\n}\n\nstatic void gtt_entry_clear_present(struct intel_gvt_gtt_entry *e)\n{\n\te->val64 &= ~GEN8_PAGE_PRESENT;\n}\n\nstatic void gtt_entry_set_present(struct intel_gvt_gtt_entry *e)\n{\n\te->val64 |= GEN8_PAGE_PRESENT;\n}\n\nstatic bool gen8_gtt_test_64k_splited(struct intel_gvt_gtt_entry *e)\n{\n\treturn !!(e->val64 & GTT_SPTE_FLAG_64K_SPLITED);\n}\n\nstatic void gen8_gtt_set_64k_splited(struct intel_gvt_gtt_entry *e)\n{\n\te->val64 |= GTT_SPTE_FLAG_64K_SPLITED;\n}\n\nstatic void gen8_gtt_clear_64k_splited(struct intel_gvt_gtt_entry *e)\n{\n\te->val64 &= ~GTT_SPTE_FLAG_64K_SPLITED;\n}\n\n \nstatic unsigned long gma_to_ggtt_pte_index(unsigned long gma)\n{\n\tunsigned long x = (gma >> I915_GTT_PAGE_SHIFT);\n\n\ttrace_gma_index(__func__, gma, x);\n\treturn x;\n}\n\n#define DEFINE_PPGTT_GMA_TO_INDEX(prefix, ename, exp) \\\nstatic unsigned long prefix##_gma_to_##ename##_index(unsigned long gma) \\\n{ \\\n\tunsigned long x = (exp); \\\n\ttrace_gma_index(__func__, gma, x); \\\n\treturn x; \\\n}\n\nDEFINE_PPGTT_GMA_TO_INDEX(gen8, pte, (gma >> 12 & 0x1ff));\nDEFINE_PPGTT_GMA_TO_INDEX(gen8, pde, (gma >> 21 & 0x1ff));\nDEFINE_PPGTT_GMA_TO_INDEX(gen8, l3_pdp, (gma >> 30 & 0x3));\nDEFINE_PPGTT_GMA_TO_INDEX(gen8, l4_pdp, (gma >> 30 & 0x1ff));\nDEFINE_PPGTT_GMA_TO_INDEX(gen8, pml4, (gma >> 39 & 0x1ff));\n\nstatic const struct intel_gvt_gtt_pte_ops gen8_gtt_pte_ops = {\n\t.get_entry = gtt_get_entry64,\n\t.set_entry = gtt_set_entry64,\n\t.clear_present = gtt_entry_clear_present,\n\t.set_present = gtt_entry_set_present,\n\t.test_present = gen8_gtt_test_present,\n\t.test_pse = gen8_gtt_test_pse,\n\t.clear_pse = gen8_gtt_clear_pse,\n\t.clear_ips = gen8_gtt_clear_ips,\n\t.test_ips = gen8_gtt_test_ips,\n\t.clear_64k_splited = gen8_gtt_clear_64k_splited,\n\t.set_64k_splited = gen8_gtt_set_64k_splited,\n\t.test_64k_splited = gen8_gtt_test_64k_splited,\n\t.get_pfn = gen8_gtt_get_pfn,\n\t.set_pfn = gen8_gtt_set_pfn,\n};\n\nstatic const struct intel_gvt_gtt_gma_ops gen8_gtt_gma_ops = {\n\t.gma_to_ggtt_pte_index = gma_to_ggtt_pte_index,\n\t.gma_to_pte_index = gen8_gma_to_pte_index,\n\t.gma_to_pde_index = gen8_gma_to_pde_index,\n\t.gma_to_l3_pdp_index = gen8_gma_to_l3_pdp_index,\n\t.gma_to_l4_pdp_index = gen8_gma_to_l4_pdp_index,\n\t.gma_to_pml4_index = gen8_gma_to_pml4_index,\n};\n\n \nstatic void update_entry_type_for_real(const struct intel_gvt_gtt_pte_ops *pte_ops,\n\tstruct intel_gvt_gtt_entry *entry, bool ips)\n{\n\tswitch (entry->type) {\n\tcase GTT_TYPE_PPGTT_PDE_ENTRY:\n\tcase GTT_TYPE_PPGTT_PDP_ENTRY:\n\t\tif (pte_ops->test_pse(entry))\n\t\t\tentry->type = get_pse_type(entry->type);\n\t\tbreak;\n\tcase GTT_TYPE_PPGTT_PTE_4K_ENTRY:\n\t\tif (ips)\n\t\t\tentry->type = get_pse_type(entry->type);\n\t\tbreak;\n\tdefault:\n\t\tGEM_BUG_ON(!gtt_type_is_entry(entry->type));\n\t}\n\n\tGEM_BUG_ON(entry->type == GTT_TYPE_INVALID);\n}\n\n \nstatic void _ppgtt_get_root_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index,\n\t\tbool guest)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_PPGTT);\n\n\tentry->type = mm->ppgtt_mm.root_entry_type;\n\tpte_ops->get_entry(guest ? mm->ppgtt_mm.guest_pdps :\n\t\t\t   mm->ppgtt_mm.shadow_pdps,\n\t\t\t   entry, index, false, 0, mm->vgpu);\n\tupdate_entry_type_for_real(pte_ops, entry, false);\n}\n\nstatic inline void ppgtt_get_guest_root_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\t_ppgtt_get_root_entry(mm, entry, index, true);\n}\n\nstatic inline void ppgtt_get_shadow_root_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\t_ppgtt_get_root_entry(mm, entry, index, false);\n}\n\nstatic void _ppgtt_set_root_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index,\n\t\tbool guest)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\n\tpte_ops->set_entry(guest ? mm->ppgtt_mm.guest_pdps :\n\t\t\t   mm->ppgtt_mm.shadow_pdps,\n\t\t\t   entry, index, false, 0, mm->vgpu);\n}\n\nstatic inline void ppgtt_set_shadow_root_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\t_ppgtt_set_root_entry(mm, entry, index, false);\n}\n\nstatic void ggtt_get_guest_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_GGTT);\n\n\tentry->type = GTT_TYPE_GGTT_PTE;\n\tpte_ops->get_entry(mm->ggtt_mm.virtual_ggtt, entry, index,\n\t\t\t   false, 0, mm->vgpu);\n}\n\nstatic void ggtt_set_guest_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_GGTT);\n\n\tpte_ops->set_entry(mm->ggtt_mm.virtual_ggtt, entry, index,\n\t\t\t   false, 0, mm->vgpu);\n}\n\nstatic void ggtt_get_host_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_GGTT);\n\n\tpte_ops->get_entry(NULL, entry, index, false, 0, mm->vgpu);\n}\n\nstatic void ggtt_set_host_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *entry, unsigned long index)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = mm->vgpu->gvt->gtt.pte_ops;\n\tunsigned long offset = index;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_GGTT);\n\n\tif (vgpu_gmadr_is_aperture(mm->vgpu, index << I915_GTT_PAGE_SHIFT)) {\n\t\toffset -= (vgpu_aperture_gmadr_base(mm->vgpu) >> PAGE_SHIFT);\n\t\tmm->ggtt_mm.host_ggtt_aperture[offset] = entry->val64;\n\t} else if (vgpu_gmadr_is_hidden(mm->vgpu, index << I915_GTT_PAGE_SHIFT)) {\n\t\toffset -= (vgpu_hidden_gmadr_base(mm->vgpu) >> PAGE_SHIFT);\n\t\tmm->ggtt_mm.host_ggtt_hidden[offset] = entry->val64;\n\t}\n\n\tpte_ops->set_entry(NULL, entry, index, false, 0, mm->vgpu);\n}\n\n \nstatic inline int ppgtt_spt_get_entry(\n\t\tstruct intel_vgpu_ppgtt_spt *spt,\n\t\tvoid *page_table, int type,\n\t\tstruct intel_gvt_gtt_entry *e, unsigned long index,\n\t\tbool guest)\n{\n\tstruct intel_gvt *gvt = spt->vgpu->gvt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gvt->gtt.pte_ops;\n\tint ret;\n\n\te->type = get_entry_type(type);\n\n\tif (WARN(!gtt_type_is_entry(e->type), \"invalid entry type\\n\"))\n\t\treturn -EINVAL;\n\n\tret = ops->get_entry(page_table, e, index, guest,\n\t\t\tspt->guest_page.gfn << I915_GTT_PAGE_SHIFT,\n\t\t\tspt->vgpu);\n\tif (ret)\n\t\treturn ret;\n\n\tupdate_entry_type_for_real(ops, e, guest ?\n\t\t\t\t   spt->guest_page.pde_ips : false);\n\n\tgvt_vdbg_mm(\"read ppgtt entry, spt type %d, entry type %d, index %lu, value %llx\\n\",\n\t\t    type, e->type, index, e->val64);\n\treturn 0;\n}\n\nstatic inline int ppgtt_spt_set_entry(\n\t\tstruct intel_vgpu_ppgtt_spt *spt,\n\t\tvoid *page_table, int type,\n\t\tstruct intel_gvt_gtt_entry *e, unsigned long index,\n\t\tbool guest)\n{\n\tstruct intel_gvt *gvt = spt->vgpu->gvt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gvt->gtt.pte_ops;\n\n\tif (WARN(!gtt_type_is_entry(e->type), \"invalid entry type\\n\"))\n\t\treturn -EINVAL;\n\n\tgvt_vdbg_mm(\"set ppgtt entry, spt type %d, entry type %d, index %lu, value %llx\\n\",\n\t\t    type, e->type, index, e->val64);\n\n\treturn ops->set_entry(page_table, e, index, guest,\n\t\t\tspt->guest_page.gfn << I915_GTT_PAGE_SHIFT,\n\t\t\tspt->vgpu);\n}\n\n#define ppgtt_get_guest_entry(spt, e, index) \\\n\tppgtt_spt_get_entry(spt, NULL, \\\n\t\tspt->guest_page.type, e, index, true)\n\n#define ppgtt_set_guest_entry(spt, e, index) \\\n\tppgtt_spt_set_entry(spt, NULL, \\\n\t\tspt->guest_page.type, e, index, true)\n\n#define ppgtt_get_shadow_entry(spt, e, index) \\\n\tppgtt_spt_get_entry(spt, spt->shadow_page.vaddr, \\\n\t\tspt->shadow_page.type, e, index, false)\n\n#define ppgtt_set_shadow_entry(spt, e, index) \\\n\tppgtt_spt_set_entry(spt, spt->shadow_page.vaddr, \\\n\t\tspt->shadow_page.type, e, index, false)\n\nstatic void *alloc_spt(gfp_t gfp_mask)\n{\n\tstruct intel_vgpu_ppgtt_spt *spt;\n\n\tspt = kzalloc(sizeof(*spt), gfp_mask);\n\tif (!spt)\n\t\treturn NULL;\n\n\tspt->shadow_page.page = alloc_page(gfp_mask);\n\tif (!spt->shadow_page.page) {\n\t\tkfree(spt);\n\t\treturn NULL;\n\t}\n\treturn spt;\n}\n\nstatic void free_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\t__free_page(spt->shadow_page.page);\n\tkfree(spt);\n}\n\nstatic int detach_oos_page(struct intel_vgpu *vgpu,\n\t\tstruct intel_vgpu_oos_page *oos_page);\n\nstatic void ppgtt_free_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct device *kdev = spt->vgpu->gvt->gt->i915->drm.dev;\n\n\ttrace_spt_free(spt->vgpu->id, spt, spt->guest_page.type);\n\n\tdma_unmap_page(kdev, spt->shadow_page.mfn << I915_GTT_PAGE_SHIFT, 4096,\n\t\t       DMA_BIDIRECTIONAL);\n\n\tradix_tree_delete(&spt->vgpu->gtt.spt_tree, spt->shadow_page.mfn);\n\n\tif (spt->guest_page.gfn) {\n\t\tif (spt->guest_page.oos_page)\n\t\t\tdetach_oos_page(spt->vgpu, spt->guest_page.oos_page);\n\n\t\tintel_vgpu_unregister_page_track(spt->vgpu, spt->guest_page.gfn);\n\t}\n\n\tlist_del_init(&spt->post_shadow_list);\n\tfree_spt(spt);\n}\n\nstatic void ppgtt_free_all_spt(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_ppgtt_spt *spt, *spn;\n\tstruct radix_tree_iter iter;\n\tLIST_HEAD(all_spt);\n\tvoid __rcu **slot;\n\n\trcu_read_lock();\n\tradix_tree_for_each_slot(slot, &vgpu->gtt.spt_tree, &iter, 0) {\n\t\tspt = radix_tree_deref_slot(slot);\n\t\tlist_move(&spt->post_shadow_list, &all_spt);\n\t}\n\trcu_read_unlock();\n\n\tlist_for_each_entry_safe(spt, spn, &all_spt, post_shadow_list)\n\t\tppgtt_free_spt(spt);\n}\n\nstatic int ppgtt_handle_guest_write_page_table_bytes(\n\t\tstruct intel_vgpu_ppgtt_spt *spt,\n\t\tu64 pa, void *p_data, int bytes);\n\nstatic int ppgtt_write_protection_handler(\n\t\tstruct intel_vgpu_page_track *page_track,\n\t\tu64 gpa, void *data, int bytes)\n{\n\tstruct intel_vgpu_ppgtt_spt *spt = page_track->priv_data;\n\n\tint ret;\n\n\tif (bytes != 4 && bytes != 8)\n\t\treturn -EINVAL;\n\n\tret = ppgtt_handle_guest_write_page_table_bytes(spt, gpa, data, bytes);\n\tif (ret)\n\t\treturn ret;\n\treturn ret;\n}\n\n \nstatic struct intel_vgpu_ppgtt_spt *intel_vgpu_find_spt_by_gfn(\n\t\tstruct intel_vgpu *vgpu, unsigned long gfn)\n{\n\tstruct intel_vgpu_page_track *track;\n\n\ttrack = intel_vgpu_find_page_track(vgpu, gfn);\n\tif (track && track->handler == ppgtt_write_protection_handler)\n\t\treturn track->priv_data;\n\n\treturn NULL;\n}\n\n \nstatic inline struct intel_vgpu_ppgtt_spt *intel_vgpu_find_spt_by_mfn(\n\t\tstruct intel_vgpu *vgpu, unsigned long mfn)\n{\n\treturn radix_tree_lookup(&vgpu->gtt.spt_tree, mfn);\n}\n\nstatic int reclaim_one_ppgtt_mm(struct intel_gvt *gvt);\n\n \nstatic struct intel_vgpu_ppgtt_spt *ppgtt_alloc_spt(\n\t\tstruct intel_vgpu *vgpu, enum intel_gvt_gtt_type type)\n{\n\tstruct device *kdev = vgpu->gvt->gt->i915->drm.dev;\n\tstruct intel_vgpu_ppgtt_spt *spt = NULL;\n\tdma_addr_t daddr;\n\tint ret;\n\nretry:\n\tspt = alloc_spt(GFP_KERNEL | __GFP_ZERO);\n\tif (!spt) {\n\t\tif (reclaim_one_ppgtt_mm(vgpu->gvt))\n\t\t\tgoto retry;\n\n\t\tgvt_vgpu_err(\"fail to allocate ppgtt shadow page\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tspt->vgpu = vgpu;\n\tatomic_set(&spt->refcount, 1);\n\tINIT_LIST_HEAD(&spt->post_shadow_list);\n\n\t \n\tspt->shadow_page.type = type;\n\tdaddr = dma_map_page(kdev, spt->shadow_page.page,\n\t\t\t     0, 4096, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(kdev, daddr)) {\n\t\tgvt_vgpu_err(\"fail to map dma addr\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_free_spt;\n\t}\n\tspt->shadow_page.vaddr = page_address(spt->shadow_page.page);\n\tspt->shadow_page.mfn = daddr >> I915_GTT_PAGE_SHIFT;\n\n\tret = radix_tree_insert(&vgpu->gtt.spt_tree, spt->shadow_page.mfn, spt);\n\tif (ret)\n\t\tgoto err_unmap_dma;\n\n\treturn spt;\n\nerr_unmap_dma:\n\tdma_unmap_page(kdev, daddr, PAGE_SIZE, DMA_BIDIRECTIONAL);\nerr_free_spt:\n\tfree_spt(spt);\n\treturn ERR_PTR(ret);\n}\n\n \nstatic struct intel_vgpu_ppgtt_spt *ppgtt_alloc_spt_gfn(\n\t\tstruct intel_vgpu *vgpu, enum intel_gvt_gtt_type type,\n\t\tunsigned long gfn, bool guest_pde_ips)\n{\n\tstruct intel_vgpu_ppgtt_spt *spt;\n\tint ret;\n\n\tspt = ppgtt_alloc_spt(vgpu, type);\n\tif (IS_ERR(spt))\n\t\treturn spt;\n\n\t \n\tret = intel_vgpu_register_page_track(vgpu, gfn,\n\t\t\tppgtt_write_protection_handler, spt);\n\tif (ret) {\n\t\tppgtt_free_spt(spt);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tspt->guest_page.type = type;\n\tspt->guest_page.gfn = gfn;\n\tspt->guest_page.pde_ips = guest_pde_ips;\n\n\ttrace_spt_alloc(vgpu->id, spt, type, spt->shadow_page.mfn, gfn);\n\n\treturn spt;\n}\n\n#define pt_entry_size_shift(spt) \\\n\t((spt)->vgpu->gvt->device_info.gtt_entry_size_shift)\n\n#define pt_entries(spt) \\\n\t(I915_GTT_PAGE_SIZE >> pt_entry_size_shift(spt))\n\n#define for_each_present_guest_entry(spt, e, i) \\\n\tfor (i = 0; i < pt_entries(spt); \\\n\t     i += spt->guest_page.pde_ips ? GTT_64K_PTE_STRIDE : 1) \\\n\t\tif (!ppgtt_get_guest_entry(spt, e, i) && \\\n\t\t    spt->vgpu->gvt->gtt.pte_ops->test_present(e))\n\n#define for_each_present_shadow_entry(spt, e, i) \\\n\tfor (i = 0; i < pt_entries(spt); \\\n\t     i += spt->shadow_page.pde_ips ? GTT_64K_PTE_STRIDE : 1) \\\n\t\tif (!ppgtt_get_shadow_entry(spt, e, i) && \\\n\t\t    spt->vgpu->gvt->gtt.pte_ops->test_present(e))\n\n#define for_each_shadow_entry(spt, e, i) \\\n\tfor (i = 0; i < pt_entries(spt); \\\n\t     i += (spt->shadow_page.pde_ips ? GTT_64K_PTE_STRIDE : 1)) \\\n\t\tif (!ppgtt_get_shadow_entry(spt, e, i))\n\nstatic inline void ppgtt_get_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tint v = atomic_read(&spt->refcount);\n\n\ttrace_spt_refcount(spt->vgpu->id, \"inc\", spt, v, (v + 1));\n\tatomic_inc(&spt->refcount);\n}\n\nstatic inline int ppgtt_put_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tint v = atomic_read(&spt->refcount);\n\n\ttrace_spt_refcount(spt->vgpu->id, \"dec\", spt, v, (v - 1));\n\treturn atomic_dec_return(&spt->refcount);\n}\n\nstatic int ppgtt_invalidate_spt(struct intel_vgpu_ppgtt_spt *spt);\n\nstatic int ppgtt_invalidate_spt_by_shadow_entry(struct intel_vgpu *vgpu,\n\t\tstruct intel_gvt_gtt_entry *e)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *s;\n\tenum intel_gvt_gtt_type cur_pt_type;\n\n\tGEM_BUG_ON(!gtt_type_is_pt(get_next_pt_type(e->type)));\n\n\tif (e->type != GTT_TYPE_PPGTT_ROOT_L3_ENTRY\n\t\t&& e->type != GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {\n\t\tcur_pt_type = get_next_pt_type(e->type);\n\n\t\tif (!gtt_type_is_pt(cur_pt_type) ||\n\t\t\t\t!gtt_type_is_pt(cur_pt_type + 1)) {\n\t\t\tdrm_WARN(&i915->drm, 1,\n\t\t\t\t \"Invalid page table type, cur_pt_type is: %d\\n\",\n\t\t\t\t cur_pt_type);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tcur_pt_type += 1;\n\n\t\tif (ops->get_pfn(e) ==\n\t\t\tvgpu->gtt.scratch_pt[cur_pt_type].page_mfn)\n\t\t\treturn 0;\n\t}\n\ts = intel_vgpu_find_spt_by_mfn(vgpu, ops->get_pfn(e));\n\tif (!s) {\n\t\tgvt_vgpu_err(\"fail to find shadow page: mfn: 0x%lx\\n\",\n\t\t\t\tops->get_pfn(e));\n\t\treturn -ENXIO;\n\t}\n\treturn ppgtt_invalidate_spt(s);\n}\n\nstatic inline void ppgtt_invalidate_pte(struct intel_vgpu_ppgtt_spt *spt,\n\t\tstruct intel_gvt_gtt_entry *entry)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tunsigned long pfn;\n\tint type;\n\n\tpfn = ops->get_pfn(entry);\n\ttype = spt->shadow_page.type;\n\n\t \n\tif (!pfn || pfn == vgpu->gtt.scratch_pt[type].page_mfn)\n\t\treturn;\n\n\tintel_gvt_dma_unmap_guest_page(vgpu, pfn << PAGE_SHIFT);\n}\n\nstatic int ppgtt_invalidate_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tstruct intel_gvt_gtt_entry e;\n\tunsigned long index;\n\tint ret;\n\n\ttrace_spt_change(spt->vgpu->id, \"die\", spt,\n\t\t\tspt->guest_page.gfn, spt->shadow_page.type);\n\n\tif (ppgtt_put_spt(spt) > 0)\n\t\treturn 0;\n\n\tfor_each_present_shadow_entry(spt, &e, index) {\n\t\tswitch (e.type) {\n\t\tcase GTT_TYPE_PPGTT_PTE_4K_ENTRY:\n\t\t\tgvt_vdbg_mm(\"invalidate 4K entry\\n\");\n\t\t\tppgtt_invalidate_pte(spt, &e);\n\t\t\tbreak;\n\t\tcase GTT_TYPE_PPGTT_PTE_64K_ENTRY:\n\t\t\t \n\t\t\tWARN(1, \"suspicious 64K gtt entry\\n\");\n\t\t\tcontinue;\n\t\tcase GTT_TYPE_PPGTT_PTE_2M_ENTRY:\n\t\t\tgvt_vdbg_mm(\"invalidate 2M entry\\n\");\n\t\t\tcontinue;\n\t\tcase GTT_TYPE_PPGTT_PTE_1G_ENTRY:\n\t\t\tWARN(1, \"GVT doesn't support 1GB page\\n\");\n\t\t\tcontinue;\n\t\tcase GTT_TYPE_PPGTT_PML4_ENTRY:\n\t\tcase GTT_TYPE_PPGTT_PDP_ENTRY:\n\t\tcase GTT_TYPE_PPGTT_PDE_ENTRY:\n\t\t\tgvt_vdbg_mm(\"invalidate PMUL4/PDP/PDE entry\\n\");\n\t\t\tret = ppgtt_invalidate_spt_by_shadow_entry(\n\t\t\t\t\tspt->vgpu, &e);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tGEM_BUG_ON(1);\n\t\t}\n\t}\n\n\ttrace_spt_change(spt->vgpu->id, \"release\", spt,\n\t\t\t spt->guest_page.gfn, spt->shadow_page.type);\n\tppgtt_free_spt(spt);\n\treturn 0;\nfail:\n\tgvt_vgpu_err(\"fail: shadow page %p shadow entry 0x%llx type %d\\n\",\n\t\t\tspt, e.val64, e.type);\n\treturn ret;\n}\n\nstatic bool vgpu_ips_enabled(struct intel_vgpu *vgpu)\n{\n\tstruct drm_i915_private *dev_priv = vgpu->gvt->gt->i915;\n\n\tif (GRAPHICS_VER(dev_priv) == 9) {\n\t\tu32 ips = vgpu_vreg_t(vgpu, GEN8_GAMW_ECO_DEV_RW_IA) &\n\t\t\tGAMW_ECO_ENABLE_64K_IPS_FIELD;\n\n\t\treturn ips == GAMW_ECO_ENABLE_64K_IPS_FIELD;\n\t} else if (GRAPHICS_VER(dev_priv) >= 11) {\n\t\t \n\t\treturn true;\n\t} else\n\t\treturn false;\n}\n\nstatic int ppgtt_populate_spt(struct intel_vgpu_ppgtt_spt *spt);\n\nstatic struct intel_vgpu_ppgtt_spt *ppgtt_populate_spt_by_guest_entry(\n\t\tstruct intel_vgpu *vgpu, struct intel_gvt_gtt_entry *we)\n{\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *spt = NULL;\n\tbool ips = false;\n\tint ret;\n\n\tGEM_BUG_ON(!gtt_type_is_pt(get_next_pt_type(we->type)));\n\n\tif (we->type == GTT_TYPE_PPGTT_PDE_ENTRY)\n\t\tips = vgpu_ips_enabled(vgpu) && ops->test_ips(we);\n\n\tspt = intel_vgpu_find_spt_by_gfn(vgpu, ops->get_pfn(we));\n\tif (spt) {\n\t\tppgtt_get_spt(spt);\n\n\t\tif (ips != spt->guest_page.pde_ips) {\n\t\t\tspt->guest_page.pde_ips = ips;\n\n\t\t\tgvt_dbg_mm(\"reshadow PDE since ips changed\\n\");\n\t\t\tclear_page(spt->shadow_page.vaddr);\n\t\t\tret = ppgtt_populate_spt(spt);\n\t\t\tif (ret) {\n\t\t\t\tppgtt_put_spt(spt);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tint type = get_next_pt_type(we->type);\n\n\t\tif (!gtt_type_is_pt(type)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tspt = ppgtt_alloc_spt_gfn(vgpu, type, ops->get_pfn(we), ips);\n\t\tif (IS_ERR(spt)) {\n\t\t\tret = PTR_ERR(spt);\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = intel_vgpu_enable_page_track(vgpu, spt->guest_page.gfn);\n\t\tif (ret)\n\t\t\tgoto err_free_spt;\n\n\t\tret = ppgtt_populate_spt(spt);\n\t\tif (ret)\n\t\t\tgoto err_free_spt;\n\n\t\ttrace_spt_change(vgpu->id, \"new\", spt, spt->guest_page.gfn,\n\t\t\t\t spt->shadow_page.type);\n\t}\n\treturn spt;\n\nerr_free_spt:\n\tppgtt_free_spt(spt);\n\tspt = NULL;\nerr:\n\tgvt_vgpu_err(\"fail: shadow page %p guest entry 0x%llx type %d\\n\",\n\t\t     spt, we->val64, we->type);\n\treturn ERR_PTR(ret);\n}\n\nstatic inline void ppgtt_generate_shadow_entry(struct intel_gvt_gtt_entry *se,\n\t\tstruct intel_vgpu_ppgtt_spt *s, struct intel_gvt_gtt_entry *ge)\n{\n\tconst struct intel_gvt_gtt_pte_ops *ops = s->vgpu->gvt->gtt.pte_ops;\n\n\tse->type = ge->type;\n\tse->val64 = ge->val64;\n\n\t \n\tif (se->type == GTT_TYPE_PPGTT_PDE_ENTRY)\n\t\tops->clear_ips(se);\n\n\tops->set_pfn(se, s->shadow_page.mfn);\n}\n\nstatic int split_2MB_gtt_entry(struct intel_vgpu *vgpu,\n\tstruct intel_vgpu_ppgtt_spt *spt, unsigned long index,\n\tstruct intel_gvt_gtt_entry *se)\n{\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *sub_spt;\n\tstruct intel_gvt_gtt_entry sub_se;\n\tunsigned long start_gfn;\n\tdma_addr_t dma_addr;\n\tunsigned long sub_index;\n\tint ret;\n\n\tgvt_dbg_mm(\"Split 2M gtt entry, index %lu\\n\", index);\n\n\tstart_gfn = ops->get_pfn(se);\n\n\tsub_spt = ppgtt_alloc_spt(vgpu, GTT_TYPE_PPGTT_PTE_PT);\n\tif (IS_ERR(sub_spt))\n\t\treturn PTR_ERR(sub_spt);\n\n\tfor_each_shadow_entry(sub_spt, &sub_se, sub_index) {\n\t\tret = intel_gvt_dma_map_guest_page(vgpu, start_gfn + sub_index,\n\t\t\t\t\t\t   PAGE_SIZE, &dma_addr);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tsub_se.val64 = se->val64;\n\n\t\t \n\t\tsub_se.val64 &= ~_PAGE_PAT;\n\t\tsub_se.val64 |= (se->val64 & _PAGE_PAT_LARGE) >> 5;\n\n\t\tops->set_pfn(&sub_se, dma_addr >> PAGE_SHIFT);\n\t\tppgtt_set_shadow_entry(sub_spt, &sub_se, sub_index);\n\t}\n\n\t \n\tse->val64 &= ~_PAGE_DIRTY;\n\n\tops->clear_pse(se);\n\tops->clear_ips(se);\n\tops->set_pfn(se, sub_spt->shadow_page.mfn);\n\tppgtt_set_shadow_entry(spt, se, index);\n\treturn 0;\nerr:\n\t \n\tfor_each_present_shadow_entry(sub_spt, &sub_se, sub_index) {\n\t\tgvt_vdbg_mm(\"invalidate 4K entry\\n\");\n\t\tppgtt_invalidate_pte(sub_spt, &sub_se);\n\t}\n\t \n\ttrace_spt_change(sub_spt->vgpu->id, \"release\", sub_spt,\n\t\tsub_spt->guest_page.gfn, sub_spt->shadow_page.type);\n\tppgtt_free_spt(sub_spt);\n\treturn ret;\n}\n\nstatic int split_64KB_gtt_entry(struct intel_vgpu *vgpu,\n\tstruct intel_vgpu_ppgtt_spt *spt, unsigned long index,\n\tstruct intel_gvt_gtt_entry *se)\n{\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_gvt_gtt_entry entry = *se;\n\tunsigned long start_gfn;\n\tdma_addr_t dma_addr;\n\tint i, ret;\n\n\tgvt_vdbg_mm(\"Split 64K gtt entry, index %lu\\n\", index);\n\n\tGEM_BUG_ON(index % GTT_64K_PTE_STRIDE);\n\n\tstart_gfn = ops->get_pfn(se);\n\n\tentry.type = GTT_TYPE_PPGTT_PTE_4K_ENTRY;\n\tops->set_64k_splited(&entry);\n\n\tfor (i = 0; i < GTT_64K_PTE_STRIDE; i++) {\n\t\tret = intel_gvt_dma_map_guest_page(vgpu, start_gfn + i,\n\t\t\t\t\t\t   PAGE_SIZE, &dma_addr);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tops->set_pfn(&entry, dma_addr >> PAGE_SHIFT);\n\t\tppgtt_set_shadow_entry(spt, &entry, index + i);\n\t}\n\treturn 0;\n}\n\nstatic int ppgtt_populate_shadow_entry(struct intel_vgpu *vgpu,\n\tstruct intel_vgpu_ppgtt_spt *spt, unsigned long index,\n\tstruct intel_gvt_gtt_entry *ge)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_gvt_gtt_entry se = *ge;\n\tunsigned long gfn;\n\tdma_addr_t dma_addr;\n\tint ret;\n\n\tif (!pte_ops->test_present(ge))\n\t\treturn 0;\n\n\tgfn = pte_ops->get_pfn(ge);\n\n\tswitch (ge->type) {\n\tcase GTT_TYPE_PPGTT_PTE_4K_ENTRY:\n\t\tgvt_vdbg_mm(\"shadow 4K gtt entry\\n\");\n\t\tret = intel_gvt_dma_map_guest_page(vgpu, gfn, PAGE_SIZE, &dma_addr);\n\t\tif (ret)\n\t\t\treturn -ENXIO;\n\t\tbreak;\n\tcase GTT_TYPE_PPGTT_PTE_64K_ENTRY:\n\t\tgvt_vdbg_mm(\"shadow 64K gtt entry\\n\");\n\t\t \n\t\treturn split_64KB_gtt_entry(vgpu, spt, index, &se);\n\tcase GTT_TYPE_PPGTT_PTE_2M_ENTRY:\n\t\tgvt_vdbg_mm(\"shadow 2M gtt entry\\n\");\n\t\tif (!HAS_PAGE_SIZES(vgpu->gvt->gt->i915, I915_GTT_PAGE_SIZE_2M) ||\n\t\t    intel_gvt_dma_map_guest_page(vgpu, gfn,\n\t\t\t\t\t\t I915_GTT_PAGE_SIZE_2M, &dma_addr))\n\t\t\treturn split_2MB_gtt_entry(vgpu, spt, index, &se);\n\t\tbreak;\n\tcase GTT_TYPE_PPGTT_PTE_1G_ENTRY:\n\t\tgvt_vgpu_err(\"GVT doesn't support 1GB entry\\n\");\n\t\treturn -EINVAL;\n\tdefault:\n\t\tGEM_BUG_ON(1);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tpte_ops->set_pfn(&se, dma_addr >> PAGE_SHIFT);\n\tppgtt_set_shadow_entry(spt, &se, index);\n\treturn 0;\n}\n\nstatic int ppgtt_populate_spt(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tstruct intel_vgpu_ppgtt_spt *s;\n\tstruct intel_gvt_gtt_entry se, ge;\n\tunsigned long i;\n\tint ret;\n\n\ttrace_spt_change(spt->vgpu->id, \"born\", spt,\n\t\t\t spt->guest_page.gfn, spt->shadow_page.type);\n\n\tfor_each_present_guest_entry(spt, &ge, i) {\n\t\tif (gtt_type_is_pt(get_next_pt_type(ge.type))) {\n\t\t\ts = ppgtt_populate_spt_by_guest_entry(vgpu, &ge);\n\t\t\tif (IS_ERR(s)) {\n\t\t\t\tret = PTR_ERR(s);\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tppgtt_get_shadow_entry(spt, &se, i);\n\t\t\tppgtt_generate_shadow_entry(&se, s, &ge);\n\t\t\tppgtt_set_shadow_entry(spt, &se, i);\n\t\t} else {\n\t\t\tret = ppgtt_populate_shadow_entry(vgpu, spt, i, &ge);\n\t\t\tif (ret)\n\t\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\nfail:\n\tgvt_vgpu_err(\"fail: shadow page %p guest entry 0x%llx type %d\\n\",\n\t\t\tspt, ge.val64, ge.type);\n\treturn ret;\n}\n\nstatic int ppgtt_handle_guest_entry_removal(struct intel_vgpu_ppgtt_spt *spt,\n\t\tstruct intel_gvt_gtt_entry *se, unsigned long index)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tint ret;\n\n\ttrace_spt_guest_change(spt->vgpu->id, \"remove\", spt,\n\t\t\t       spt->shadow_page.type, se->val64, index);\n\n\tgvt_vdbg_mm(\"destroy old shadow entry, type %d, index %lu, value %llx\\n\",\n\t\t    se->type, index, se->val64);\n\n\tif (!ops->test_present(se))\n\t\treturn 0;\n\n\tif (ops->get_pfn(se) ==\n\t    vgpu->gtt.scratch_pt[spt->shadow_page.type].page_mfn)\n\t\treturn 0;\n\n\tif (gtt_type_is_pt(get_next_pt_type(se->type))) {\n\t\tstruct intel_vgpu_ppgtt_spt *s =\n\t\t\tintel_vgpu_find_spt_by_mfn(vgpu, ops->get_pfn(se));\n\t\tif (!s) {\n\t\t\tgvt_vgpu_err(\"fail to find guest page\\n\");\n\t\t\tret = -ENXIO;\n\t\t\tgoto fail;\n\t\t}\n\t\tret = ppgtt_invalidate_spt(s);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t} else {\n\t\t \n\t\tWARN(se->type == GTT_TYPE_PPGTT_PTE_64K_ENTRY,\n\t\t     \"suspicious 64K entry\\n\");\n\t\tppgtt_invalidate_pte(spt, se);\n\t}\n\n\treturn 0;\nfail:\n\tgvt_vgpu_err(\"fail: shadow page %p guest entry 0x%llx type %d\\n\",\n\t\t\tspt, se->val64, se->type);\n\treturn ret;\n}\n\nstatic int ppgtt_handle_guest_entry_add(struct intel_vgpu_ppgtt_spt *spt,\n\t\tstruct intel_gvt_gtt_entry *we, unsigned long index)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tstruct intel_gvt_gtt_entry m;\n\tstruct intel_vgpu_ppgtt_spt *s;\n\tint ret;\n\n\ttrace_spt_guest_change(spt->vgpu->id, \"add\", spt, spt->shadow_page.type,\n\t\t\t       we->val64, index);\n\n\tgvt_vdbg_mm(\"add shadow entry: type %d, index %lu, value %llx\\n\",\n\t\t    we->type, index, we->val64);\n\n\tif (gtt_type_is_pt(get_next_pt_type(we->type))) {\n\t\ts = ppgtt_populate_spt_by_guest_entry(vgpu, we);\n\t\tif (IS_ERR(s)) {\n\t\t\tret = PTR_ERR(s);\n\t\t\tgoto fail;\n\t\t}\n\t\tppgtt_get_shadow_entry(spt, &m, index);\n\t\tppgtt_generate_shadow_entry(&m, s, we);\n\t\tppgtt_set_shadow_entry(spt, &m, index);\n\t} else {\n\t\tret = ppgtt_populate_shadow_entry(vgpu, spt, index, we);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\treturn 0;\nfail:\n\tgvt_vgpu_err(\"fail: spt %p guest entry 0x%llx type %d\\n\",\n\t\tspt, we->val64, we->type);\n\treturn ret;\n}\n\nstatic int sync_oos_page(struct intel_vgpu *vgpu,\n\t\tstruct intel_vgpu_oos_page *oos_page)\n{\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gvt->gtt.pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *spt = oos_page->spt;\n\tstruct intel_gvt_gtt_entry old, new;\n\tint index;\n\tint ret;\n\n\ttrace_oos_change(vgpu->id, \"sync\", oos_page->id,\n\t\t\t spt, spt->guest_page.type);\n\n\told.type = new.type = get_entry_type(spt->guest_page.type);\n\told.val64 = new.val64 = 0;\n\n\tfor (index = 0; index < (I915_GTT_PAGE_SIZE >>\n\t\t\t\tinfo->gtt_entry_size_shift); index++) {\n\t\tops->get_entry(oos_page->mem, &old, index, false, 0, vgpu);\n\t\tops->get_entry(NULL, &new, index, true,\n\t\t\t       spt->guest_page.gfn << PAGE_SHIFT, vgpu);\n\n\t\tif (old.val64 == new.val64\n\t\t\t&& !test_and_clear_bit(index, spt->post_shadow_bitmap))\n\t\t\tcontinue;\n\n\t\ttrace_oos_sync(vgpu->id, oos_page->id,\n\t\t\t\tspt, spt->guest_page.type,\n\t\t\t\tnew.val64, index);\n\n\t\tret = ppgtt_populate_shadow_entry(vgpu, spt, index, &new);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tops->set_entry(oos_page->mem, &new, index, false, 0, vgpu);\n\t}\n\n\tspt->guest_page.write_cnt = 0;\n\tlist_del_init(&spt->post_shadow_list);\n\treturn 0;\n}\n\nstatic int detach_oos_page(struct intel_vgpu *vgpu,\n\t\tstruct intel_vgpu_oos_page *oos_page)\n{\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_vgpu_ppgtt_spt *spt = oos_page->spt;\n\n\ttrace_oos_change(vgpu->id, \"detach\", oos_page->id,\n\t\t\t spt, spt->guest_page.type);\n\n\tspt->guest_page.write_cnt = 0;\n\tspt->guest_page.oos_page = NULL;\n\toos_page->spt = NULL;\n\n\tlist_del_init(&oos_page->vm_list);\n\tlist_move_tail(&oos_page->list, &gvt->gtt.oos_page_free_list_head);\n\n\treturn 0;\n}\n\nstatic int attach_oos_page(struct intel_vgpu_oos_page *oos_page,\n\t\tstruct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_gvt *gvt = spt->vgpu->gvt;\n\tint ret;\n\n\tret = intel_gvt_read_gpa(spt->vgpu,\n\t\t\tspt->guest_page.gfn << I915_GTT_PAGE_SHIFT,\n\t\t\toos_page->mem, I915_GTT_PAGE_SIZE);\n\tif (ret)\n\t\treturn ret;\n\n\toos_page->spt = spt;\n\tspt->guest_page.oos_page = oos_page;\n\n\tlist_move_tail(&oos_page->list, &gvt->gtt.oos_page_use_list_head);\n\n\ttrace_oos_change(spt->vgpu->id, \"attach\", oos_page->id,\n\t\t\t spt, spt->guest_page.type);\n\treturn 0;\n}\n\nstatic int ppgtt_set_guest_page_sync(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_vgpu_oos_page *oos_page = spt->guest_page.oos_page;\n\tint ret;\n\n\tret = intel_vgpu_enable_page_track(spt->vgpu, spt->guest_page.gfn);\n\tif (ret)\n\t\treturn ret;\n\n\ttrace_oos_change(spt->vgpu->id, \"set page sync\", oos_page->id,\n\t\t\t spt, spt->guest_page.type);\n\n\tlist_del_init(&oos_page->vm_list);\n\treturn sync_oos_page(spt->vgpu, oos_page);\n}\n\nstatic int ppgtt_allocate_oos_page(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_gvt *gvt = spt->vgpu->gvt;\n\tstruct intel_gvt_gtt *gtt = &gvt->gtt;\n\tstruct intel_vgpu_oos_page *oos_page = spt->guest_page.oos_page;\n\tint ret;\n\n\tWARN(oos_page, \"shadow PPGTT page has already has a oos page\\n\");\n\n\tif (list_empty(&gtt->oos_page_free_list_head)) {\n\t\toos_page = container_of(gtt->oos_page_use_list_head.next,\n\t\t\tstruct intel_vgpu_oos_page, list);\n\t\tret = ppgtt_set_guest_page_sync(oos_page->spt);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = detach_oos_page(spt->vgpu, oos_page);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else\n\t\toos_page = container_of(gtt->oos_page_free_list_head.next,\n\t\t\tstruct intel_vgpu_oos_page, list);\n\treturn attach_oos_page(oos_page, spt);\n}\n\nstatic int ppgtt_set_guest_page_oos(struct intel_vgpu_ppgtt_spt *spt)\n{\n\tstruct intel_vgpu_oos_page *oos_page = spt->guest_page.oos_page;\n\n\tif (WARN(!oos_page, \"shadow PPGTT page should have a oos page\\n\"))\n\t\treturn -EINVAL;\n\n\ttrace_oos_change(spt->vgpu->id, \"set page out of sync\", oos_page->id,\n\t\t\t spt, spt->guest_page.type);\n\n\tlist_add_tail(&oos_page->vm_list, &spt->vgpu->gtt.oos_page_list_head);\n\treturn intel_vgpu_disable_page_track(spt->vgpu, spt->guest_page.gfn);\n}\n\n \nint intel_vgpu_sync_oos_pages(struct intel_vgpu *vgpu)\n{\n\tstruct list_head *pos, *n;\n\tstruct intel_vgpu_oos_page *oos_page;\n\tint ret;\n\n\tif (!enable_out_of_sync)\n\t\treturn 0;\n\n\tlist_for_each_safe(pos, n, &vgpu->gtt.oos_page_list_head) {\n\t\toos_page = container_of(pos,\n\t\t\t\tstruct intel_vgpu_oos_page, vm_list);\n\t\tret = ppgtt_set_guest_page_sync(oos_page->spt);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nstatic int ppgtt_handle_guest_write_page_table(\n\t\tstruct intel_vgpu_ppgtt_spt *spt,\n\t\tstruct intel_gvt_gtt_entry *we, unsigned long index)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tint type = spt->shadow_page.type;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_gvt_gtt_entry old_se;\n\tint new_present;\n\tint i, ret;\n\n\tnew_present = ops->test_present(we);\n\n\t \n\tppgtt_get_shadow_entry(spt, &old_se, index);\n\n\tif (new_present) {\n\t\tret = ppgtt_handle_guest_entry_add(spt, we, index);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\n\tret = ppgtt_handle_guest_entry_removal(spt, &old_se, index);\n\tif (ret)\n\t\tgoto fail;\n\n\tif (!new_present) {\n\t\t \n\t\tif (ops->test_64k_splited(&old_se) &&\n\t\t    !(index % GTT_64K_PTE_STRIDE)) {\n\t\t\tgvt_vdbg_mm(\"remove splited 64K shadow entries\\n\");\n\t\t\tfor (i = 0; i < GTT_64K_PTE_STRIDE; i++) {\n\t\t\t\tops->clear_64k_splited(&old_se);\n\t\t\t\tops->set_pfn(&old_se,\n\t\t\t\t\tvgpu->gtt.scratch_pt[type].page_mfn);\n\t\t\t\tppgtt_set_shadow_entry(spt, &old_se, index + i);\n\t\t\t}\n\t\t} else if (old_se.type == GTT_TYPE_PPGTT_PTE_2M_ENTRY ||\n\t\t\t   old_se.type == GTT_TYPE_PPGTT_PTE_1G_ENTRY) {\n\t\t\tops->clear_pse(&old_se);\n\t\t\tops->set_pfn(&old_se,\n\t\t\t\t     vgpu->gtt.scratch_pt[type].page_mfn);\n\t\t\tppgtt_set_shadow_entry(spt, &old_se, index);\n\t\t} else {\n\t\t\tops->set_pfn(&old_se,\n\t\t\t\t     vgpu->gtt.scratch_pt[type].page_mfn);\n\t\t\tppgtt_set_shadow_entry(spt, &old_se, index);\n\t\t}\n\t}\n\n\treturn 0;\nfail:\n\tgvt_vgpu_err(\"fail: shadow page %p guest entry 0x%llx type %d.\\n\",\n\t\t\tspt, we->val64, we->type);\n\treturn ret;\n}\n\n\n\nstatic inline bool can_do_out_of_sync(struct intel_vgpu_ppgtt_spt *spt)\n{\n\treturn enable_out_of_sync\n\t\t&& gtt_type_is_pte_pt(spt->guest_page.type)\n\t\t&& spt->guest_page.write_cnt >= 2;\n}\n\nstatic void ppgtt_set_post_shadow(struct intel_vgpu_ppgtt_spt *spt,\n\t\tunsigned long index)\n{\n\tset_bit(index, spt->post_shadow_bitmap);\n\tif (!list_empty(&spt->post_shadow_list))\n\t\treturn;\n\n\tlist_add_tail(&spt->post_shadow_list,\n\t\t\t&spt->vgpu->gtt.post_shadow_list_head);\n}\n\n \nint intel_vgpu_flush_post_shadow(struct intel_vgpu *vgpu)\n{\n\tstruct list_head *pos, *n;\n\tstruct intel_vgpu_ppgtt_spt *spt;\n\tstruct intel_gvt_gtt_entry ge;\n\tunsigned long index;\n\tint ret;\n\n\tlist_for_each_safe(pos, n, &vgpu->gtt.post_shadow_list_head) {\n\t\tspt = container_of(pos, struct intel_vgpu_ppgtt_spt,\n\t\t\t\tpost_shadow_list);\n\n\t\tfor_each_set_bit(index, spt->post_shadow_bitmap,\n\t\t\t\tGTT_ENTRY_NUM_IN_ONE_PAGE) {\n\t\t\tppgtt_get_guest_entry(spt, &ge, index);\n\n\t\t\tret = ppgtt_handle_guest_write_page_table(spt,\n\t\t\t\t\t\t\t&ge, index);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tclear_bit(index, spt->post_shadow_bitmap);\n\t\t}\n\t\tlist_del_init(&spt->post_shadow_list);\n\t}\n\treturn 0;\n}\n\nstatic int ppgtt_handle_guest_write_page_table_bytes(\n\t\tstruct intel_vgpu_ppgtt_spt *spt,\n\t\tu64 pa, void *p_data, int bytes)\n{\n\tstruct intel_vgpu *vgpu = spt->vgpu;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tstruct intel_gvt_gtt_entry we, se;\n\tunsigned long index;\n\tint ret;\n\n\tindex = (pa & (PAGE_SIZE - 1)) >> info->gtt_entry_size_shift;\n\n\tppgtt_get_guest_entry(spt, &we, index);\n\n\t \n\tif (we.type == GTT_TYPE_PPGTT_PTE_64K_ENTRY &&\n\t    (index % GTT_64K_PTE_STRIDE)) {\n\t\tgvt_vdbg_mm(\"Ignore write to unused PTE entry, index %lu\\n\",\n\t\t\t    index);\n\t\treturn 0;\n\t}\n\n\tif (bytes == info->gtt_entry_size) {\n\t\tret = ppgtt_handle_guest_write_page_table(spt, &we, index);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tif (!test_bit(index, spt->post_shadow_bitmap)) {\n\t\t\tint type = spt->shadow_page.type;\n\n\t\t\tppgtt_get_shadow_entry(spt, &se, index);\n\t\t\tret = ppgtt_handle_guest_entry_removal(spt, &se, index);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tops->set_pfn(&se, vgpu->gtt.scratch_pt[type].page_mfn);\n\t\t\tppgtt_set_shadow_entry(spt, &se, index);\n\t\t}\n\t\tppgtt_set_post_shadow(spt, index);\n\t}\n\n\tif (!enable_out_of_sync)\n\t\treturn 0;\n\n\tspt->guest_page.write_cnt++;\n\n\tif (spt->guest_page.oos_page)\n\t\tops->set_entry(spt->guest_page.oos_page->mem, &we, index,\n\t\t\t\tfalse, 0, vgpu);\n\n\tif (can_do_out_of_sync(spt)) {\n\t\tif (!spt->guest_page.oos_page)\n\t\t\tppgtt_allocate_oos_page(spt);\n\n\t\tret = ppgtt_set_guest_page_oos(spt);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void invalidate_ppgtt_mm(struct intel_vgpu_mm *mm)\n{\n\tstruct intel_vgpu *vgpu = mm->vgpu;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_gvt_gtt *gtt = &gvt->gtt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gtt->pte_ops;\n\tstruct intel_gvt_gtt_entry se;\n\tint index;\n\n\tif (!mm->ppgtt_mm.shadowed)\n\t\treturn;\n\n\tfor (index = 0; index < ARRAY_SIZE(mm->ppgtt_mm.shadow_pdps); index++) {\n\t\tppgtt_get_shadow_root_entry(mm, &se, index);\n\n\t\tif (!ops->test_present(&se))\n\t\t\tcontinue;\n\n\t\tppgtt_invalidate_spt_by_shadow_entry(vgpu, &se);\n\t\tse.val64 = 0;\n\t\tppgtt_set_shadow_root_entry(mm, &se, index);\n\n\t\ttrace_spt_guest_change(vgpu->id, \"destroy root pointer\",\n\t\t\t\t       NULL, se.type, se.val64, index);\n\t}\n\n\tmm->ppgtt_mm.shadowed = false;\n}\n\n\nstatic int shadow_ppgtt_mm(struct intel_vgpu_mm *mm)\n{\n\tstruct intel_vgpu *vgpu = mm->vgpu;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_gvt_gtt *gtt = &gvt->gtt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gtt->pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *spt;\n\tstruct intel_gvt_gtt_entry ge, se;\n\tint index, ret;\n\n\tif (mm->ppgtt_mm.shadowed)\n\t\treturn 0;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn -EINVAL;\n\n\tmm->ppgtt_mm.shadowed = true;\n\n\tfor (index = 0; index < ARRAY_SIZE(mm->ppgtt_mm.guest_pdps); index++) {\n\t\tppgtt_get_guest_root_entry(mm, &ge, index);\n\n\t\tif (!ops->test_present(&ge))\n\t\t\tcontinue;\n\n\t\ttrace_spt_guest_change(vgpu->id, __func__, NULL,\n\t\t\t\t       ge.type, ge.val64, index);\n\n\t\tspt = ppgtt_populate_spt_by_guest_entry(vgpu, &ge);\n\t\tif (IS_ERR(spt)) {\n\t\t\tgvt_vgpu_err(\"fail to populate guest root pointer\\n\");\n\t\t\tret = PTR_ERR(spt);\n\t\t\tgoto fail;\n\t\t}\n\t\tppgtt_generate_shadow_entry(&se, spt, &ge);\n\t\tppgtt_set_shadow_root_entry(mm, &se, index);\n\n\t\ttrace_spt_guest_change(vgpu->id, \"populate root pointer\",\n\t\t\t\t       NULL, se.type, se.val64, index);\n\t}\n\n\treturn 0;\nfail:\n\tinvalidate_ppgtt_mm(mm);\n\treturn ret;\n}\n\nstatic struct intel_vgpu_mm *vgpu_alloc_mm(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_mm *mm;\n\n\tmm = kzalloc(sizeof(*mm), GFP_KERNEL);\n\tif (!mm)\n\t\treturn NULL;\n\n\tmm->vgpu = vgpu;\n\tkref_init(&mm->ref);\n\tatomic_set(&mm->pincount, 0);\n\n\treturn mm;\n}\n\nstatic void vgpu_free_mm(struct intel_vgpu_mm *mm)\n{\n\tkfree(mm);\n}\n\n \nstruct intel_vgpu_mm *intel_vgpu_create_ppgtt_mm(struct intel_vgpu *vgpu,\n\t\tenum intel_gvt_gtt_type root_entry_type, u64 pdps[])\n{\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_vgpu_mm *mm;\n\tint ret;\n\n\tmm = vgpu_alloc_mm(vgpu);\n\tif (!mm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmm->type = INTEL_GVT_MM_PPGTT;\n\n\tGEM_BUG_ON(root_entry_type != GTT_TYPE_PPGTT_ROOT_L3_ENTRY &&\n\t\t   root_entry_type != GTT_TYPE_PPGTT_ROOT_L4_ENTRY);\n\tmm->ppgtt_mm.root_entry_type = root_entry_type;\n\n\tINIT_LIST_HEAD(&mm->ppgtt_mm.list);\n\tINIT_LIST_HEAD(&mm->ppgtt_mm.lru_list);\n\tINIT_LIST_HEAD(&mm->ppgtt_mm.link);\n\n\tif (root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY)\n\t\tmm->ppgtt_mm.guest_pdps[0] = pdps[0];\n\telse\n\t\tmemcpy(mm->ppgtt_mm.guest_pdps, pdps,\n\t\t       sizeof(mm->ppgtt_mm.guest_pdps));\n\n\tret = shadow_ppgtt_mm(mm);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"failed to shadow ppgtt mm\\n\");\n\t\tvgpu_free_mm(mm);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tlist_add_tail(&mm->ppgtt_mm.list, &vgpu->gtt.ppgtt_mm_list_head);\n\n\tmutex_lock(&gvt->gtt.ppgtt_mm_lock);\n\tlist_add_tail(&mm->ppgtt_mm.lru_list, &gvt->gtt.ppgtt_mm_lru_list_head);\n\tmutex_unlock(&gvt->gtt.ppgtt_mm_lock);\n\n\treturn mm;\n}\n\nstatic struct intel_vgpu_mm *intel_vgpu_create_ggtt_mm(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_mm *mm;\n\tunsigned long nr_entries;\n\n\tmm = vgpu_alloc_mm(vgpu);\n\tif (!mm)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmm->type = INTEL_GVT_MM_GGTT;\n\n\tnr_entries = gvt_ggtt_gm_sz(vgpu->gvt) >> I915_GTT_PAGE_SHIFT;\n\tmm->ggtt_mm.virtual_ggtt =\n\t\tvzalloc(array_size(nr_entries,\n\t\t\t\t   vgpu->gvt->device_info.gtt_entry_size));\n\tif (!mm->ggtt_mm.virtual_ggtt) {\n\t\tvgpu_free_mm(mm);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tmm->ggtt_mm.host_ggtt_aperture = vzalloc((vgpu_aperture_sz(vgpu) >> PAGE_SHIFT) * sizeof(u64));\n\tif (!mm->ggtt_mm.host_ggtt_aperture) {\n\t\tvfree(mm->ggtt_mm.virtual_ggtt);\n\t\tvgpu_free_mm(mm);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tmm->ggtt_mm.host_ggtt_hidden = vzalloc((vgpu_hidden_sz(vgpu) >> PAGE_SHIFT) * sizeof(u64));\n\tif (!mm->ggtt_mm.host_ggtt_hidden) {\n\t\tvfree(mm->ggtt_mm.host_ggtt_aperture);\n\t\tvfree(mm->ggtt_mm.virtual_ggtt);\n\t\tvgpu_free_mm(mm);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn mm;\n}\n\n \nvoid _intel_vgpu_mm_release(struct kref *mm_ref)\n{\n\tstruct intel_vgpu_mm *mm = container_of(mm_ref, typeof(*mm), ref);\n\n\tif (GEM_WARN_ON(atomic_read(&mm->pincount)))\n\t\tgvt_err(\"vgpu mm pin count bug detected\\n\");\n\n\tif (mm->type == INTEL_GVT_MM_PPGTT) {\n\t\tlist_del(&mm->ppgtt_mm.list);\n\n\t\tmutex_lock(&mm->vgpu->gvt->gtt.ppgtt_mm_lock);\n\t\tlist_del(&mm->ppgtt_mm.lru_list);\n\t\tmutex_unlock(&mm->vgpu->gvt->gtt.ppgtt_mm_lock);\n\n\t\tinvalidate_ppgtt_mm(mm);\n\t} else {\n\t\tvfree(mm->ggtt_mm.virtual_ggtt);\n\t\tvfree(mm->ggtt_mm.host_ggtt_aperture);\n\t\tvfree(mm->ggtt_mm.host_ggtt_hidden);\n\t}\n\n\tvgpu_free_mm(mm);\n}\n\n \nvoid intel_vgpu_unpin_mm(struct intel_vgpu_mm *mm)\n{\n\tatomic_dec_if_positive(&mm->pincount);\n}\n\n \nint intel_vgpu_pin_mm(struct intel_vgpu_mm *mm)\n{\n\tint ret;\n\n\tatomic_inc(&mm->pincount);\n\n\tif (mm->type == INTEL_GVT_MM_PPGTT) {\n\t\tret = shadow_ppgtt_mm(mm);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmutex_lock(&mm->vgpu->gvt->gtt.ppgtt_mm_lock);\n\t\tlist_move_tail(&mm->ppgtt_mm.lru_list,\n\t\t\t       &mm->vgpu->gvt->gtt.ppgtt_mm_lru_list_head);\n\t\tmutex_unlock(&mm->vgpu->gvt->gtt.ppgtt_mm_lock);\n\t}\n\n\treturn 0;\n}\n\nstatic int reclaim_one_ppgtt_mm(struct intel_gvt *gvt)\n{\n\tstruct intel_vgpu_mm *mm;\n\tstruct list_head *pos, *n;\n\n\tmutex_lock(&gvt->gtt.ppgtt_mm_lock);\n\n\tlist_for_each_safe(pos, n, &gvt->gtt.ppgtt_mm_lru_list_head) {\n\t\tmm = container_of(pos, struct intel_vgpu_mm, ppgtt_mm.lru_list);\n\n\t\tif (atomic_read(&mm->pincount))\n\t\t\tcontinue;\n\n\t\tlist_del_init(&mm->ppgtt_mm.lru_list);\n\t\tmutex_unlock(&gvt->gtt.ppgtt_mm_lock);\n\t\tinvalidate_ppgtt_mm(mm);\n\t\treturn 1;\n\t}\n\tmutex_unlock(&gvt->gtt.ppgtt_mm_lock);\n\treturn 0;\n}\n\n \nstatic inline int ppgtt_get_next_level_entry(struct intel_vgpu_mm *mm,\n\t\tstruct intel_gvt_gtt_entry *e, unsigned long index, bool guest)\n{\n\tstruct intel_vgpu *vgpu = mm->vgpu;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_vgpu_ppgtt_spt *s;\n\n\ts = intel_vgpu_find_spt_by_mfn(vgpu, ops->get_pfn(e));\n\tif (!s)\n\t\treturn -ENXIO;\n\n\tif (!guest)\n\t\tppgtt_get_shadow_entry(s, e, index);\n\telse\n\t\tppgtt_get_guest_entry(s, e, index);\n\treturn 0;\n}\n\n \nunsigned long intel_vgpu_gma_to_gpa(struct intel_vgpu_mm *mm, unsigned long gma)\n{\n\tstruct intel_vgpu *vgpu = mm->vgpu;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = gvt->gtt.pte_ops;\n\tconst struct intel_gvt_gtt_gma_ops *gma_ops = gvt->gtt.gma_ops;\n\tunsigned long gpa = INTEL_GVT_INVALID_ADDR;\n\tunsigned long gma_index[4];\n\tstruct intel_gvt_gtt_entry e;\n\tint i, levels = 0;\n\tint ret;\n\n\tGEM_BUG_ON(mm->type != INTEL_GVT_MM_GGTT &&\n\t\t   mm->type != INTEL_GVT_MM_PPGTT);\n\n\tif (mm->type == INTEL_GVT_MM_GGTT) {\n\t\tif (!vgpu_gmadr_is_valid(vgpu, gma))\n\t\t\tgoto err;\n\n\t\tggtt_get_guest_entry(mm, &e,\n\t\t\tgma_ops->gma_to_ggtt_pte_index(gma));\n\n\t\tgpa = (pte_ops->get_pfn(&e) << I915_GTT_PAGE_SHIFT)\n\t\t\t+ (gma & ~I915_GTT_PAGE_MASK);\n\n\t\ttrace_gma_translate(vgpu->id, \"ggtt\", 0, 0, gma, gpa);\n\t} else {\n\t\tswitch (mm->ppgtt_mm.root_entry_type) {\n\t\tcase GTT_TYPE_PPGTT_ROOT_L4_ENTRY:\n\t\t\tppgtt_get_shadow_root_entry(mm, &e, 0);\n\n\t\t\tgma_index[0] = gma_ops->gma_to_pml4_index(gma);\n\t\t\tgma_index[1] = gma_ops->gma_to_l4_pdp_index(gma);\n\t\t\tgma_index[2] = gma_ops->gma_to_pde_index(gma);\n\t\t\tgma_index[3] = gma_ops->gma_to_pte_index(gma);\n\t\t\tlevels = 4;\n\t\t\tbreak;\n\t\tcase GTT_TYPE_PPGTT_ROOT_L3_ENTRY:\n\t\t\tppgtt_get_shadow_root_entry(mm, &e,\n\t\t\t\t\tgma_ops->gma_to_l3_pdp_index(gma));\n\n\t\t\tgma_index[0] = gma_ops->gma_to_pde_index(gma);\n\t\t\tgma_index[1] = gma_ops->gma_to_pte_index(gma);\n\t\t\tlevels = 2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tGEM_BUG_ON(1);\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < levels; i++) {\n\t\t\tret = ppgtt_get_next_level_entry(mm, &e, gma_index[i],\n\t\t\t\t(i == levels - 1));\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tif (!pte_ops->test_present(&e)) {\n\t\t\t\tgvt_dbg_core(\"GMA 0x%lx is not present\\n\", gma);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tgpa = (pte_ops->get_pfn(&e) << I915_GTT_PAGE_SHIFT) +\n\t\t\t\t\t(gma & ~I915_GTT_PAGE_MASK);\n\t\ttrace_gma_translate(vgpu->id, \"ppgtt\", 0,\n\t\t\t\t    mm->ppgtt_mm.root_entry_type, gma, gpa);\n\t}\n\n\treturn gpa;\nerr:\n\tgvt_vgpu_err(\"invalid mm type: %d gma %lx\\n\", mm->type, gma);\n\treturn INTEL_GVT_INVALID_ADDR;\n}\n\nstatic int emulate_ggtt_mmio_read(struct intel_vgpu *vgpu,\n\tunsigned int off, void *p_data, unsigned int bytes)\n{\n\tstruct intel_vgpu_mm *ggtt_mm = vgpu->gtt.ggtt_mm;\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tunsigned long index = off >> info->gtt_entry_size_shift;\n\tunsigned long gma;\n\tstruct intel_gvt_gtt_entry e;\n\n\tif (bytes != 4 && bytes != 8)\n\t\treturn -EINVAL;\n\n\tgma = index << I915_GTT_PAGE_SHIFT;\n\tif (!intel_gvt_ggtt_validate_range(vgpu,\n\t\t\t\t\t   gma, 1 << I915_GTT_PAGE_SHIFT)) {\n\t\tgvt_dbg_mm(\"read invalid ggtt at 0x%lx\\n\", gma);\n\t\tmemset(p_data, 0, bytes);\n\t\treturn 0;\n\t}\n\n\tggtt_get_guest_entry(ggtt_mm, &e, index);\n\tmemcpy(p_data, (void *)&e.val64 + (off & (info->gtt_entry_size - 1)),\n\t\t\tbytes);\n\treturn 0;\n}\n\n \nint intel_vgpu_emulate_ggtt_mmio_read(struct intel_vgpu *vgpu, unsigned int off,\n\tvoid *p_data, unsigned int bytes)\n{\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tint ret;\n\n\tif (bytes != 4 && bytes != 8)\n\t\treturn -EINVAL;\n\n\toff -= info->gtt_start_offset;\n\tret = emulate_ggtt_mmio_read(vgpu, off, p_data, bytes);\n\treturn ret;\n}\n\nstatic void ggtt_invalidate_pte(struct intel_vgpu *vgpu,\n\t\tstruct intel_gvt_gtt_entry *entry)\n{\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = vgpu->gvt->gtt.pte_ops;\n\tunsigned long pfn;\n\n\tpfn = pte_ops->get_pfn(entry);\n\tif (pfn != vgpu->gvt->gtt.scratch_mfn)\n\t\tintel_gvt_dma_unmap_guest_page(vgpu, pfn << PAGE_SHIFT);\n}\n\nstatic int emulate_ggtt_mmio_write(struct intel_vgpu *vgpu, unsigned int off,\n\tvoid *p_data, unsigned int bytes)\n{\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tconst struct intel_gvt_device_info *info = &gvt->device_info;\n\tstruct intel_vgpu_mm *ggtt_mm = vgpu->gtt.ggtt_mm;\n\tconst struct intel_gvt_gtt_pte_ops *ops = gvt->gtt.pte_ops;\n\tunsigned long g_gtt_index = off >> info->gtt_entry_size_shift;\n\tunsigned long gma, gfn;\n\tstruct intel_gvt_gtt_entry e = {.val64 = 0, .type = GTT_TYPE_GGTT_PTE};\n\tstruct intel_gvt_gtt_entry m = {.val64 = 0, .type = GTT_TYPE_GGTT_PTE};\n\tdma_addr_t dma_addr;\n\tint ret;\n\tstruct intel_gvt_partial_pte *partial_pte, *pos, *n;\n\tbool partial_update = false;\n\n\tif (bytes != 4 && bytes != 8)\n\t\treturn -EINVAL;\n\n\tgma = g_gtt_index << I915_GTT_PAGE_SHIFT;\n\n\t \n\tif (!vgpu_gmadr_is_valid(vgpu, gma))\n\t\treturn 0;\n\n\te.type = GTT_TYPE_GGTT_PTE;\n\tmemcpy((void *)&e.val64 + (off & (info->gtt_entry_size - 1)), p_data,\n\t\t\tbytes);\n\n\t \n\tif (bytes < info->gtt_entry_size) {\n\t\tbool found = false;\n\n\t\tlist_for_each_entry_safe(pos, n,\n\t\t\t\t&ggtt_mm->ggtt_mm.partial_pte_list, list) {\n\t\t\tif (g_gtt_index == pos->offset >>\n\t\t\t\t\tinfo->gtt_entry_size_shift) {\n\t\t\t\tif (off != pos->offset) {\n\t\t\t\t\t \n\t\t\t\t\tint last_off = pos->offset &\n\t\t\t\t\t\t(info->gtt_entry_size - 1);\n\n\t\t\t\t\tmemcpy((void *)&e.val64 + last_off,\n\t\t\t\t\t\t(void *)&pos->data + last_off,\n\t\t\t\t\t\tbytes);\n\n\t\t\t\t\tlist_del(&pos->list);\n\t\t\t\t\tkfree(pos);\n\t\t\t\t\tfound = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tpos->data = e.val64;\n\t\t\t\tggtt_set_guest_entry(ggtt_mm, &e, g_gtt_index);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\t \n\t\t\tpartial_pte = kzalloc(sizeof(*partial_pte), GFP_KERNEL);\n\t\t\tif (!partial_pte)\n\t\t\t\treturn -ENOMEM;\n\t\t\tpartial_pte->offset = off;\n\t\t\tpartial_pte->data = e.val64;\n\t\t\tlist_add_tail(&partial_pte->list,\n\t\t\t\t&ggtt_mm->ggtt_mm.partial_pte_list);\n\t\t\tpartial_update = true;\n\t\t}\n\t}\n\n\tif (!partial_update && (ops->test_present(&e))) {\n\t\tgfn = ops->get_pfn(&e);\n\t\tm.val64 = e.val64;\n\t\tm.type = e.type;\n\n\t\tret = intel_gvt_dma_map_guest_page(vgpu, gfn, PAGE_SIZE,\n\t\t\t\t\t\t   &dma_addr);\n\t\tif (ret) {\n\t\t\tgvt_vgpu_err(\"fail to populate guest ggtt entry\\n\");\n\t\t\t \n\t\t\tops->set_pfn(&m, gvt->gtt.scratch_mfn);\n\t\t} else\n\t\t\tops->set_pfn(&m, dma_addr >> PAGE_SHIFT);\n\t} else {\n\t\tops->set_pfn(&m, gvt->gtt.scratch_mfn);\n\t\tops->clear_present(&m);\n\t}\n\n\tggtt_set_guest_entry(ggtt_mm, &e, g_gtt_index);\n\n\tggtt_get_host_entry(ggtt_mm, &e, g_gtt_index);\n\tggtt_invalidate_pte(vgpu, &e);\n\n\tggtt_set_host_entry(ggtt_mm, &m, g_gtt_index);\n\tggtt_invalidate(gvt->gt);\n\treturn 0;\n}\n\n \nint intel_vgpu_emulate_ggtt_mmio_write(struct intel_vgpu *vgpu,\n\t\tunsigned int off, void *p_data, unsigned int bytes)\n{\n\tconst struct intel_gvt_device_info *info = &vgpu->gvt->device_info;\n\tint ret;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_engine_cs *engine;\n\tint i;\n\n\tif (bytes != 4 && bytes != 8)\n\t\treturn -EINVAL;\n\n\toff -= info->gtt_start_offset;\n\tret = emulate_ggtt_mmio_write(vgpu, off, p_data, bytes);\n\n\t \n\tfor_each_engine(engine, vgpu->gvt->gt, i) {\n\t\tif (!s->last_ctx[i].valid)\n\t\t\tcontinue;\n\n\t\tif (s->last_ctx[i].lrca == (off >> info->gtt_entry_size_shift))\n\t\t\ts->last_ctx[i].valid = false;\n\t}\n\treturn ret;\n}\n\nstatic int alloc_scratch_pages(struct intel_vgpu *vgpu,\n\t\tenum intel_gvt_gtt_type type)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\tstruct intel_vgpu_gtt *gtt = &vgpu->gtt;\n\tconst struct intel_gvt_gtt_pte_ops *ops = vgpu->gvt->gtt.pte_ops;\n\tint page_entry_num = I915_GTT_PAGE_SIZE >>\n\t\t\t\tvgpu->gvt->device_info.gtt_entry_size_shift;\n\tvoid *scratch_pt;\n\tint i;\n\tstruct device *dev = vgpu->gvt->gt->i915->drm.dev;\n\tdma_addr_t daddr;\n\n\tif (drm_WARN_ON(&i915->drm,\n\t\t\ttype < GTT_TYPE_PPGTT_PTE_PT || type >= GTT_TYPE_MAX))\n\t\treturn -EINVAL;\n\n\tscratch_pt = (void *)get_zeroed_page(GFP_KERNEL);\n\tif (!scratch_pt) {\n\t\tgvt_vgpu_err(\"fail to allocate scratch page\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tdaddr = dma_map_page(dev, virt_to_page(scratch_pt), 0, 4096, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, daddr)) {\n\t\tgvt_vgpu_err(\"fail to dmamap scratch_pt\\n\");\n\t\t__free_page(virt_to_page(scratch_pt));\n\t\treturn -ENOMEM;\n\t}\n\tgtt->scratch_pt[type].page_mfn =\n\t\t(unsigned long)(daddr >> I915_GTT_PAGE_SHIFT);\n\tgtt->scratch_pt[type].page = virt_to_page(scratch_pt);\n\tgvt_dbg_mm(\"vgpu%d create scratch_pt: type %d mfn=0x%lx\\n\",\n\t\t\tvgpu->id, type, gtt->scratch_pt[type].page_mfn);\n\n\t \n\tif (type > GTT_TYPE_PPGTT_PTE_PT) {\n\t\tstruct intel_gvt_gtt_entry se;\n\n\t\tmemset(&se, 0, sizeof(struct intel_gvt_gtt_entry));\n\t\tse.type = get_entry_type(type - 1);\n\t\tops->set_pfn(&se, gtt->scratch_pt[type - 1].page_mfn);\n\n\t\t \n\t\tse.val64 |= GEN8_PAGE_PRESENT | GEN8_PAGE_RW;\n\t\tif (type == GTT_TYPE_PPGTT_PDE_PT)\n\t\t\tse.val64 |= PPAT_CACHED;\n\n\t\tfor (i = 0; i < page_entry_num; i++)\n\t\t\tops->set_entry(scratch_pt, &se, i, false, 0, vgpu);\n\t}\n\n\treturn 0;\n}\n\nstatic int release_scratch_page_tree(struct intel_vgpu *vgpu)\n{\n\tint i;\n\tstruct device *dev = vgpu->gvt->gt->i915->drm.dev;\n\tdma_addr_t daddr;\n\n\tfor (i = GTT_TYPE_PPGTT_PTE_PT; i < GTT_TYPE_MAX; i++) {\n\t\tif (vgpu->gtt.scratch_pt[i].page != NULL) {\n\t\t\tdaddr = (dma_addr_t)(vgpu->gtt.scratch_pt[i].page_mfn <<\n\t\t\t\t\tI915_GTT_PAGE_SHIFT);\n\t\t\tdma_unmap_page(dev, daddr, 4096, DMA_BIDIRECTIONAL);\n\t\t\t__free_page(vgpu->gtt.scratch_pt[i].page);\n\t\t\tvgpu->gtt.scratch_pt[i].page = NULL;\n\t\t\tvgpu->gtt.scratch_pt[i].page_mfn = 0;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int create_scratch_page_tree(struct intel_vgpu *vgpu)\n{\n\tint i, ret;\n\n\tfor (i = GTT_TYPE_PPGTT_PTE_PT; i < GTT_TYPE_MAX; i++) {\n\t\tret = alloc_scratch_pages(vgpu, i);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\n\nerr:\n\trelease_scratch_page_tree(vgpu);\n\treturn ret;\n}\n\n \nint intel_vgpu_init_gtt(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_gtt *gtt = &vgpu->gtt;\n\n\tINIT_RADIX_TREE(&gtt->spt_tree, GFP_KERNEL);\n\n\tINIT_LIST_HEAD(&gtt->ppgtt_mm_list_head);\n\tINIT_LIST_HEAD(&gtt->oos_page_list_head);\n\tINIT_LIST_HEAD(&gtt->post_shadow_list_head);\n\n\tgtt->ggtt_mm = intel_vgpu_create_ggtt_mm(vgpu);\n\tif (IS_ERR(gtt->ggtt_mm)) {\n\t\tgvt_vgpu_err(\"fail to create mm for ggtt.\\n\");\n\t\treturn PTR_ERR(gtt->ggtt_mm);\n\t}\n\n\tintel_vgpu_reset_ggtt(vgpu, false);\n\n\tINIT_LIST_HEAD(&gtt->ggtt_mm->ggtt_mm.partial_pte_list);\n\n\treturn create_scratch_page_tree(vgpu);\n}\n\nvoid intel_vgpu_destroy_all_ppgtt_mm(struct intel_vgpu *vgpu)\n{\n\tstruct list_head *pos, *n;\n\tstruct intel_vgpu_mm *mm;\n\n\tlist_for_each_safe(pos, n, &vgpu->gtt.ppgtt_mm_list_head) {\n\t\tmm = container_of(pos, struct intel_vgpu_mm, ppgtt_mm.list);\n\t\tintel_vgpu_destroy_mm(mm);\n\t}\n\n\tif (GEM_WARN_ON(!list_empty(&vgpu->gtt.ppgtt_mm_list_head)))\n\t\tgvt_err(\"vgpu ppgtt mm is not fully destroyed\\n\");\n\n\tif (GEM_WARN_ON(!radix_tree_empty(&vgpu->gtt.spt_tree))) {\n\t\tgvt_err(\"Why we still has spt not freed?\\n\");\n\t\tppgtt_free_all_spt(vgpu);\n\t}\n}\n\nstatic void intel_vgpu_destroy_ggtt_mm(struct intel_vgpu *vgpu)\n{\n\tstruct intel_gvt_partial_pte *pos, *next;\n\n\tlist_for_each_entry_safe(pos, next,\n\t\t\t\t &vgpu->gtt.ggtt_mm->ggtt_mm.partial_pte_list,\n\t\t\t\t list) {\n\t\tgvt_dbg_mm(\"partial PTE update on hold 0x%lx : 0x%llx\\n\",\n\t\t\tpos->offset, pos->data);\n\t\tkfree(pos);\n\t}\n\tintel_vgpu_destroy_mm(vgpu->gtt.ggtt_mm);\n\tvgpu->gtt.ggtt_mm = NULL;\n}\n\n \nvoid intel_vgpu_clean_gtt(struct intel_vgpu *vgpu)\n{\n\tintel_vgpu_destroy_all_ppgtt_mm(vgpu);\n\tintel_vgpu_destroy_ggtt_mm(vgpu);\n\trelease_scratch_page_tree(vgpu);\n}\n\nstatic void clean_spt_oos(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_gtt *gtt = &gvt->gtt;\n\tstruct list_head *pos, *n;\n\tstruct intel_vgpu_oos_page *oos_page;\n\n\tWARN(!list_empty(&gtt->oos_page_use_list_head),\n\t\t\"someone is still using oos page\\n\");\n\n\tlist_for_each_safe(pos, n, &gtt->oos_page_free_list_head) {\n\t\toos_page = container_of(pos, struct intel_vgpu_oos_page, list);\n\t\tlist_del(&oos_page->list);\n\t\tfree_page((unsigned long)oos_page->mem);\n\t\tkfree(oos_page);\n\t}\n}\n\nstatic int setup_spt_oos(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_gtt *gtt = &gvt->gtt;\n\tstruct intel_vgpu_oos_page *oos_page;\n\tint i;\n\tint ret;\n\n\tINIT_LIST_HEAD(&gtt->oos_page_free_list_head);\n\tINIT_LIST_HEAD(&gtt->oos_page_use_list_head);\n\n\tfor (i = 0; i < preallocated_oos_pages; i++) {\n\t\toos_page = kzalloc(sizeof(*oos_page), GFP_KERNEL);\n\t\tif (!oos_page) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t\toos_page->mem = (void *)__get_free_pages(GFP_KERNEL, 0);\n\t\tif (!oos_page->mem) {\n\t\t\tret = -ENOMEM;\n\t\t\tkfree(oos_page);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&oos_page->list);\n\t\tINIT_LIST_HEAD(&oos_page->vm_list);\n\t\toos_page->id = i;\n\t\tlist_add_tail(&oos_page->list, &gtt->oos_page_free_list_head);\n\t}\n\n\tgvt_dbg_mm(\"%d oos pages preallocated\\n\", i);\n\n\treturn 0;\nfail:\n\tclean_spt_oos(gvt);\n\treturn ret;\n}\n\n \nstruct intel_vgpu_mm *intel_vgpu_find_ppgtt_mm(struct intel_vgpu *vgpu,\n\t\tu64 pdps[])\n{\n\tstruct intel_vgpu_mm *mm;\n\tstruct list_head *pos;\n\n\tlist_for_each(pos, &vgpu->gtt.ppgtt_mm_list_head) {\n\t\tmm = container_of(pos, struct intel_vgpu_mm, ppgtt_mm.list);\n\n\t\tswitch (mm->ppgtt_mm.root_entry_type) {\n\t\tcase GTT_TYPE_PPGTT_ROOT_L4_ENTRY:\n\t\t\tif (pdps[0] == mm->ppgtt_mm.guest_pdps[0])\n\t\t\t\treturn mm;\n\t\t\tbreak;\n\t\tcase GTT_TYPE_PPGTT_ROOT_L3_ENTRY:\n\t\t\tif (!memcmp(pdps, mm->ppgtt_mm.guest_pdps,\n\t\t\t\t    sizeof(mm->ppgtt_mm.guest_pdps)))\n\t\t\t\treturn mm;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tGEM_BUG_ON(1);\n\t\t}\n\t}\n\treturn NULL;\n}\n\n \nstruct intel_vgpu_mm *intel_vgpu_get_ppgtt_mm(struct intel_vgpu *vgpu,\n\t\tenum intel_gvt_gtt_type root_entry_type, u64 pdps[])\n{\n\tstruct intel_vgpu_mm *mm;\n\n\tmm = intel_vgpu_find_ppgtt_mm(vgpu, pdps);\n\tif (mm) {\n\t\tintel_vgpu_mm_get(mm);\n\t} else {\n\t\tmm = intel_vgpu_create_ppgtt_mm(vgpu, root_entry_type, pdps);\n\t\tif (IS_ERR(mm))\n\t\t\tgvt_vgpu_err(\"fail to create mm\\n\");\n\t}\n\treturn mm;\n}\n\n \nint intel_vgpu_put_ppgtt_mm(struct intel_vgpu *vgpu, u64 pdps[])\n{\n\tstruct intel_vgpu_mm *mm;\n\n\tmm = intel_vgpu_find_ppgtt_mm(vgpu, pdps);\n\tif (!mm) {\n\t\tgvt_vgpu_err(\"fail to find ppgtt instance.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tintel_vgpu_mm_put(mm);\n\treturn 0;\n}\n\n \nint intel_gvt_init_gtt(struct intel_gvt *gvt)\n{\n\tint ret;\n\tvoid *page;\n\tstruct device *dev = gvt->gt->i915->drm.dev;\n\tdma_addr_t daddr;\n\n\tgvt_dbg_core(\"init gtt\\n\");\n\n\tgvt->gtt.pte_ops = &gen8_gtt_pte_ops;\n\tgvt->gtt.gma_ops = &gen8_gtt_gma_ops;\n\n\tpage = (void *)get_zeroed_page(GFP_KERNEL);\n\tif (!page) {\n\t\tgvt_err(\"fail to allocate scratch ggtt page\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tdaddr = dma_map_page(dev, virt_to_page(page), 0,\n\t\t\t4096, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, daddr)) {\n\t\tgvt_err(\"fail to dmamap scratch ggtt page\\n\");\n\t\t__free_page(virt_to_page(page));\n\t\treturn -ENOMEM;\n\t}\n\n\tgvt->gtt.scratch_page = virt_to_page(page);\n\tgvt->gtt.scratch_mfn = (unsigned long)(daddr >> I915_GTT_PAGE_SHIFT);\n\n\tif (enable_out_of_sync) {\n\t\tret = setup_spt_oos(gvt);\n\t\tif (ret) {\n\t\t\tgvt_err(\"fail to initialize SPT oos\\n\");\n\t\t\tdma_unmap_page(dev, daddr, 4096, DMA_BIDIRECTIONAL);\n\t\t\t__free_page(gvt->gtt.scratch_page);\n\t\t\treturn ret;\n\t\t}\n\t}\n\tINIT_LIST_HEAD(&gvt->gtt.ppgtt_mm_lru_list_head);\n\tmutex_init(&gvt->gtt.ppgtt_mm_lock);\n\treturn 0;\n}\n\n \nvoid intel_gvt_clean_gtt(struct intel_gvt *gvt)\n{\n\tstruct device *dev = gvt->gt->i915->drm.dev;\n\tdma_addr_t daddr = (dma_addr_t)(gvt->gtt.scratch_mfn <<\n\t\t\t\t\tI915_GTT_PAGE_SHIFT);\n\n\tdma_unmap_page(dev, daddr, 4096, DMA_BIDIRECTIONAL);\n\n\t__free_page(gvt->gtt.scratch_page);\n\n\tif (enable_out_of_sync)\n\t\tclean_spt_oos(gvt);\n}\n\n \nvoid intel_vgpu_invalidate_ppgtt(struct intel_vgpu *vgpu)\n{\n\tstruct list_head *pos, *n;\n\tstruct intel_vgpu_mm *mm;\n\n\tlist_for_each_safe(pos, n, &vgpu->gtt.ppgtt_mm_list_head) {\n\t\tmm = container_of(pos, struct intel_vgpu_mm, ppgtt_mm.list);\n\t\tif (mm->type == INTEL_GVT_MM_PPGTT) {\n\t\t\tmutex_lock(&vgpu->gvt->gtt.ppgtt_mm_lock);\n\t\t\tlist_del_init(&mm->ppgtt_mm.lru_list);\n\t\t\tmutex_unlock(&vgpu->gvt->gtt.ppgtt_mm_lock);\n\t\t\tif (mm->ppgtt_mm.shadowed)\n\t\t\t\tinvalidate_ppgtt_mm(mm);\n\t\t}\n\t}\n}\n\n \nvoid intel_vgpu_reset_ggtt(struct intel_vgpu *vgpu, bool invalidate_old)\n{\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tconst struct intel_gvt_gtt_pte_ops *pte_ops = vgpu->gvt->gtt.pte_ops;\n\tstruct intel_gvt_gtt_entry entry = {.type = GTT_TYPE_GGTT_PTE};\n\tstruct intel_gvt_gtt_entry old_entry;\n\tu32 index;\n\tu32 num_entries;\n\n\tpte_ops->set_pfn(&entry, gvt->gtt.scratch_mfn);\n\tpte_ops->set_present(&entry);\n\n\tindex = vgpu_aperture_gmadr_base(vgpu) >> PAGE_SHIFT;\n\tnum_entries = vgpu_aperture_sz(vgpu) >> PAGE_SHIFT;\n\twhile (num_entries--) {\n\t\tif (invalidate_old) {\n\t\t\tggtt_get_host_entry(vgpu->gtt.ggtt_mm, &old_entry, index);\n\t\t\tggtt_invalidate_pte(vgpu, &old_entry);\n\t\t}\n\t\tggtt_set_host_entry(vgpu->gtt.ggtt_mm, &entry, index++);\n\t}\n\n\tindex = vgpu_hidden_gmadr_base(vgpu) >> PAGE_SHIFT;\n\tnum_entries = vgpu_hidden_sz(vgpu) >> PAGE_SHIFT;\n\twhile (num_entries--) {\n\t\tif (invalidate_old) {\n\t\t\tggtt_get_host_entry(vgpu->gtt.ggtt_mm, &old_entry, index);\n\t\t\tggtt_invalidate_pte(vgpu, &old_entry);\n\t\t}\n\t\tggtt_set_host_entry(vgpu->gtt.ggtt_mm, &entry, index++);\n\t}\n\n\tggtt_invalidate(gvt->gt);\n}\n\n \nvoid intel_gvt_restore_ggtt(struct intel_gvt *gvt)\n{\n\tstruct intel_vgpu *vgpu;\n\tstruct intel_vgpu_mm *mm;\n\tint id;\n\tgen8_pte_t pte;\n\tu32 idx, num_low, num_hi, offset;\n\n\t \n\tidr_for_each_entry(&(gvt)->vgpu_idr, vgpu, id) {\n\t\tmm = vgpu->gtt.ggtt_mm;\n\n\t\tnum_low = vgpu_aperture_sz(vgpu) >> PAGE_SHIFT;\n\t\toffset = vgpu_aperture_gmadr_base(vgpu) >> PAGE_SHIFT;\n\t\tfor (idx = 0; idx < num_low; idx++) {\n\t\t\tpte = mm->ggtt_mm.host_ggtt_aperture[idx];\n\t\t\tif (pte & GEN8_PAGE_PRESENT)\n\t\t\t\twrite_pte64(vgpu->gvt->gt->ggtt, offset + idx, pte);\n\t\t}\n\n\t\tnum_hi = vgpu_hidden_sz(vgpu) >> PAGE_SHIFT;\n\t\toffset = vgpu_hidden_gmadr_base(vgpu) >> PAGE_SHIFT;\n\t\tfor (idx = 0; idx < num_hi; idx++) {\n\t\t\tpte = mm->ggtt_mm.host_ggtt_hidden[idx];\n\t\t\tif (pte & GEN8_PAGE_PRESENT)\n\t\t\t\twrite_pte64(vgpu->gvt->gt->ggtt, offset + idx, pte);\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}