{
  "module_name": "gvt.h",
  "hash_id": "2ea500533c8c2e339899225998d999691f80fe089b6816791a21cf9da6b9724c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/gvt.h",
  "human_readable_source": " \n\n#ifndef _GVT_H_\n#define _GVT_H_\n\n#include <uapi/linux/pci_regs.h>\n#include <linux/vfio.h>\n#include <linux/mdev.h>\n\n#include <asm/kvm_page_track.h>\n\n#include \"i915_drv.h\"\n#include \"intel_gvt.h\"\n\n#include \"debug.h\"\n#include \"mmio.h\"\n#include \"reg.h\"\n#include \"interrupt.h\"\n#include \"gtt.h\"\n#include \"display.h\"\n#include \"edid.h\"\n#include \"execlist.h\"\n#include \"scheduler.h\"\n#include \"sched_policy.h\"\n#include \"mmio_context.h\"\n#include \"cmd_parser.h\"\n#include \"fb_decoder.h\"\n#include \"dmabuf.h\"\n#include \"page_track.h\"\n\n#define GVT_MAX_VGPU 8\n\n \nstruct intel_gvt_device_info {\n\tu32 max_support_vgpus;\n\tu32 cfg_space_size;\n\tu32 mmio_size;\n\tu32 mmio_bar;\n\tunsigned long msi_cap_offset;\n\tu32 gtt_start_offset;\n\tu32 gtt_entry_size;\n\tu32 gtt_entry_size_shift;\n\tint gmadr_bytes_in_cmd;\n\tu32 max_surface_size;\n};\n\n \nstruct intel_vgpu_gm {\n\tu64 aperture_sz;\n\tu64 hidden_sz;\n\tstruct drm_mm_node low_gm_node;\n\tstruct drm_mm_node high_gm_node;\n};\n\n#define INTEL_GVT_MAX_NUM_FENCES 32\n\n \nstruct intel_vgpu_fence {\n\tstruct i915_fence_reg *regs[INTEL_GVT_MAX_NUM_FENCES];\n\tu32 base;\n\tu32 size;\n};\n\nstruct intel_vgpu_mmio {\n\tvoid *vreg;\n};\n\n#define INTEL_GVT_MAX_BAR_NUM 4\n\nstruct intel_vgpu_pci_bar {\n\tu64 size;\n\tbool tracked;\n};\n\nstruct intel_vgpu_cfg_space {\n\tunsigned char virtual_cfg_space[PCI_CFG_SPACE_EXP_SIZE];\n\tstruct intel_vgpu_pci_bar bar[INTEL_GVT_MAX_BAR_NUM];\n\tu32 pmcsr_off;\n};\n\n#define vgpu_cfg_space(vgpu) ((vgpu)->cfg_space.virtual_cfg_space)\n\nstruct intel_vgpu_irq {\n\tbool irq_warn_once[INTEL_GVT_EVENT_MAX];\n\tDECLARE_BITMAP(flip_done_event[I915_MAX_PIPES],\n\t\t       INTEL_GVT_EVENT_MAX);\n};\n\nstruct intel_vgpu_opregion {\n\tbool mapped;\n\tvoid *va;\n\tu32 gfn[INTEL_GVT_OPREGION_PAGES];\n};\n\n#define vgpu_opregion(vgpu) (&(vgpu->opregion))\n\nstruct intel_vgpu_display {\n\tstruct intel_vgpu_i2c_edid i2c_edid;\n\tstruct intel_vgpu_port ports[I915_MAX_PORTS];\n\tstruct intel_vgpu_sbi sbi;\n\tenum port port_num;\n};\n\nstruct vgpu_sched_ctl {\n\tint weight;\n};\n\nenum {\n\tINTEL_VGPU_EXECLIST_SUBMISSION = 1,\n\tINTEL_VGPU_GUC_SUBMISSION,\n};\n\nstruct intel_vgpu_submission_ops {\n\tconst char *name;\n\tint (*init)(struct intel_vgpu *vgpu, intel_engine_mask_t engine_mask);\n\tvoid (*clean)(struct intel_vgpu *vgpu, intel_engine_mask_t engine_mask);\n\tvoid (*reset)(struct intel_vgpu *vgpu, intel_engine_mask_t engine_mask);\n};\n\nstruct intel_vgpu_submission {\n\tstruct intel_vgpu_execlist execlist[I915_NUM_ENGINES];\n\tstruct list_head workload_q_head[I915_NUM_ENGINES];\n\tstruct intel_context *shadow[I915_NUM_ENGINES];\n\tstruct kmem_cache *workloads;\n\tatomic_t running_workload_num;\n\tunion {\n\t\tu64 i915_context_pml4;\n\t\tu64 i915_context_pdps[GEN8_3LVL_PDPES];\n\t};\n\tDECLARE_BITMAP(shadow_ctx_desc_updated, I915_NUM_ENGINES);\n\tDECLARE_BITMAP(tlb_handle_pending, I915_NUM_ENGINES);\n\tvoid *ring_scan_buffer[I915_NUM_ENGINES];\n\tint ring_scan_buffer_size[I915_NUM_ENGINES];\n\tconst struct intel_vgpu_submission_ops *ops;\n\tint virtual_submission_interface;\n\tbool active;\n\tstruct {\n\t\tu32 lrca;\n\t\tbool valid;\n\t\tu64 ring_context_gpa;\n\t} last_ctx[I915_NUM_ENGINES];\n};\n\n#define KVMGT_DEBUGFS_FILENAME\t\t\"kvmgt_nr_cache_entries\"\n\nenum {\n\tINTEL_VGPU_STATUS_ATTACHED = 0,\n\tINTEL_VGPU_STATUS_ACTIVE,\n\tINTEL_VGPU_STATUS_NR_BITS,\n};\n\nstruct intel_vgpu {\n\tstruct vfio_device vfio_device;\n\tstruct intel_gvt *gvt;\n\tstruct mutex vgpu_lock;\n\tint id;\n\tDECLARE_BITMAP(status, INTEL_VGPU_STATUS_NR_BITS);\n\tbool pv_notified;\n\tbool failsafe;\n\tunsigned int resetting_eng;\n\n\t \n\tvoid *sched_data;\n\tstruct vgpu_sched_ctl sched_ctl;\n\n\tstruct intel_vgpu_fence fence;\n\tstruct intel_vgpu_gm gm;\n\tstruct intel_vgpu_cfg_space cfg_space;\n\tstruct intel_vgpu_mmio mmio;\n\tstruct intel_vgpu_irq irq;\n\tstruct intel_vgpu_gtt gtt;\n\tstruct intel_vgpu_opregion opregion;\n\tstruct intel_vgpu_display display;\n\tstruct intel_vgpu_submission submission;\n\tstruct radix_tree_root page_track_tree;\n\tu32 hws_pga[I915_NUM_ENGINES];\n\t \n\tbool d3_entered;\n\n\tstruct dentry *debugfs;\n\n\tstruct list_head dmabuf_obj_list_head;\n\tstruct mutex dmabuf_lock;\n\tstruct idr object_idr;\n\tstruct intel_vgpu_vblank_timer vblank_timer;\n\n\tu32 scan_nonprivbb;\n\n\tstruct vfio_region *region;\n\tint num_regions;\n\tstruct eventfd_ctx *intx_trigger;\n\tstruct eventfd_ctx *msi_trigger;\n\n\t \n\tstruct rb_root gfn_cache;\n\tstruct rb_root dma_addr_cache;\n\tunsigned long nr_cache_entries;\n\tstruct mutex cache_lock;\n\n\tstruct kvm_page_track_notifier_node track_node;\n#define NR_BKT (1 << 18)\n\tstruct hlist_head ptable[NR_BKT];\n#undef NR_BKT\n};\n\n \n#define vgpu_is_vm_unhealthy(ret_val) \\\n\t(((ret_val) == -EBADRQC) || ((ret_val) == -EFAULT))\n\nstruct intel_gvt_gm {\n\tunsigned long vgpu_allocated_low_gm_size;\n\tunsigned long vgpu_allocated_high_gm_size;\n};\n\nstruct intel_gvt_fence {\n\tunsigned long vgpu_allocated_fence_num;\n};\n\n \nstruct gvt_mmio_block {\n\tunsigned int device;\n\ti915_reg_t   offset;\n\tunsigned int size;\n\tgvt_mmio_func read;\n\tgvt_mmio_func write;\n};\n\n#define INTEL_GVT_MMIO_HASH_BITS 11\n\nstruct intel_gvt_mmio {\n\tu16 *mmio_attribute;\n \n#define F_RO\t\t(1 << 0)\n \n#define F_GMADR\t\t(1 << 1)\n \n#define F_MODE_MASK\t(1 << 2)\n \n#define F_CMD_ACCESS\t(1 << 3)\n \n#define F_ACCESSED\t(1 << 4)\n \n#define F_PM_SAVE\t(1 << 5)\n \n#define F_UNALIGN\t(1 << 6)\n \n#define F_SR_IN_CTX\t(1 << 7)\n \n#define F_CMD_WRITE_PATCH\t(1 << 8)\n\n\tstruct gvt_mmio_block *mmio_block;\n\tunsigned int num_mmio_block;\n\n\tDECLARE_HASHTABLE(mmio_info_table, INTEL_GVT_MMIO_HASH_BITS);\n\tunsigned long num_tracked_mmio;\n};\n\nstruct intel_gvt_firmware {\n\tvoid *cfg_space;\n\tvoid *mmio;\n\tbool firmware_loaded;\n};\n\nstruct intel_vgpu_config {\n\tunsigned int low_mm;\n\tunsigned int high_mm;\n\tunsigned int fence;\n\n\t \n\tunsigned int weight;\n\tenum intel_vgpu_edid edid;\n\tconst char *name;\n};\n\nstruct intel_vgpu_type {\n\tstruct mdev_type type;\n\tchar name[16];\n\tconst struct intel_vgpu_config *conf;\n};\n\nstruct intel_gvt {\n\t \n\tstruct mutex lock;\n\t \n\tstruct mutex sched_lock;\n\n\tstruct intel_gt *gt;\n\tstruct idr vgpu_idr;\t \n\n\tstruct intel_gvt_device_info device_info;\n\tstruct intel_gvt_gm gm;\n\tstruct intel_gvt_fence fence;\n\tstruct intel_gvt_mmio mmio;\n\tstruct intel_gvt_firmware firmware;\n\tstruct intel_gvt_irq irq;\n\tstruct intel_gvt_gtt gtt;\n\tstruct intel_gvt_workload_scheduler scheduler;\n\tstruct notifier_block shadow_ctx_notifier_block[I915_NUM_ENGINES];\n\tDECLARE_HASHTABLE(cmd_table, GVT_CMD_HASH_BITS);\n\tstruct mdev_parent parent;\n\tstruct mdev_type **mdev_types;\n\tstruct intel_vgpu_type *types;\n\tunsigned int num_types;\n\tstruct intel_vgpu *idle_vgpu;\n\n\tstruct task_struct *service_thread;\n\twait_queue_head_t service_thread_wq;\n\n\t \n\tunsigned long service_request;\n\n\tstruct {\n\t\tstruct engine_mmio *mmio;\n\t\tint ctx_mmio_count[I915_NUM_ENGINES];\n\t\tu32 *tlb_mmio_offset_list;\n\t\tu32 tlb_mmio_offset_list_cnt;\n\t\tu32 *mocs_mmio_offset_list;\n\t\tu32 mocs_mmio_offset_list_cnt;\n\t} engine_mmio_list;\n\tbool is_reg_whitelist_updated;\n\n\tstruct dentry *debugfs_root;\n};\n\nstatic inline struct intel_gvt *to_gvt(struct drm_i915_private *i915)\n{\n\treturn i915->gvt;\n}\n\nenum {\n\t \n\tINTEL_GVT_REQUEST_SCHED = 0,\n\n\t \n\tINTEL_GVT_REQUEST_EVENT_SCHED = 1,\n\n\t \n\tINTEL_GVT_REQUEST_EMULATE_VBLANK = 2,\n\tINTEL_GVT_REQUEST_EMULATE_VBLANK_MAX = INTEL_GVT_REQUEST_EMULATE_VBLANK\n\t\t+ GVT_MAX_VGPU,\n};\n\nstatic inline void intel_gvt_request_service(struct intel_gvt *gvt,\n\t\tint service)\n{\n\tset_bit(service, (void *)&gvt->service_request);\n\twake_up(&gvt->service_thread_wq);\n}\n\nvoid intel_gvt_free_firmware(struct intel_gvt *gvt);\nint intel_gvt_load_firmware(struct intel_gvt *gvt);\n\n \n#define MB_TO_BYTES(mb) ((mb) << 20ULL)\n#define BYTES_TO_MB(b) ((b) >> 20ULL)\n\n#define HOST_LOW_GM_SIZE MB_TO_BYTES(128)\n#define HOST_HIGH_GM_SIZE MB_TO_BYTES(384)\n#define HOST_FENCE 4\n\n#define gvt_to_ggtt(gvt)\t((gvt)->gt->ggtt)\n\n \n#define gvt_aperture_sz(gvt)\t  gvt_to_ggtt(gvt)->mappable_end\n#define gvt_aperture_pa_base(gvt) gvt_to_ggtt(gvt)->gmadr.start\n\n#define gvt_ggtt_gm_sz(gvt)\tgvt_to_ggtt(gvt)->vm.total\n#define gvt_ggtt_sz(gvt)\t(gvt_to_ggtt(gvt)->vm.total >> PAGE_SHIFT << 3)\n#define gvt_hidden_sz(gvt)\t(gvt_ggtt_gm_sz(gvt) - gvt_aperture_sz(gvt))\n\n#define gvt_aperture_gmadr_base(gvt) (0)\n#define gvt_aperture_gmadr_end(gvt) (gvt_aperture_gmadr_base(gvt) \\\n\t\t\t\t     + gvt_aperture_sz(gvt) - 1)\n\n#define gvt_hidden_gmadr_base(gvt) (gvt_aperture_gmadr_base(gvt) \\\n\t\t\t\t    + gvt_aperture_sz(gvt))\n#define gvt_hidden_gmadr_end(gvt) (gvt_hidden_gmadr_base(gvt) \\\n\t\t\t\t   + gvt_hidden_sz(gvt) - 1)\n\n#define gvt_fence_sz(gvt) (gvt_to_ggtt(gvt)->num_fences)\n\n \n#define vgpu_aperture_offset(vgpu)\t((vgpu)->gm.low_gm_node.start)\n#define vgpu_hidden_offset(vgpu)\t((vgpu)->gm.high_gm_node.start)\n#define vgpu_aperture_sz(vgpu)\t\t((vgpu)->gm.aperture_sz)\n#define vgpu_hidden_sz(vgpu)\t\t((vgpu)->gm.hidden_sz)\n\n#define vgpu_aperture_pa_base(vgpu) \\\n\t(gvt_aperture_pa_base(vgpu->gvt) + vgpu_aperture_offset(vgpu))\n\n#define vgpu_ggtt_gm_sz(vgpu) ((vgpu)->gm.aperture_sz + (vgpu)->gm.hidden_sz)\n\n#define vgpu_aperture_pa_end(vgpu) \\\n\t(vgpu_aperture_pa_base(vgpu) + vgpu_aperture_sz(vgpu) - 1)\n\n#define vgpu_aperture_gmadr_base(vgpu) (vgpu_aperture_offset(vgpu))\n#define vgpu_aperture_gmadr_end(vgpu) \\\n\t(vgpu_aperture_gmadr_base(vgpu) + vgpu_aperture_sz(vgpu) - 1)\n\n#define vgpu_hidden_gmadr_base(vgpu) (vgpu_hidden_offset(vgpu))\n#define vgpu_hidden_gmadr_end(vgpu) \\\n\t(vgpu_hidden_gmadr_base(vgpu) + vgpu_hidden_sz(vgpu) - 1)\n\n#define vgpu_fence_base(vgpu) (vgpu->fence.base)\n#define vgpu_fence_sz(vgpu) (vgpu->fence.size)\n\n \n#define RING_CTX_SIZE 320\n\nint intel_vgpu_alloc_resource(struct intel_vgpu *vgpu,\n\t\t\t      const struct intel_vgpu_config *conf);\nvoid intel_vgpu_reset_resource(struct intel_vgpu *vgpu);\nvoid intel_vgpu_free_resource(struct intel_vgpu *vgpu);\nvoid intel_vgpu_write_fence(struct intel_vgpu *vgpu,\n\tu32 fence, u64 value);\n\n \n#define vgpu_vreg_t(vgpu, reg) \\\n\t(*(u32 *)(vgpu->mmio.vreg + i915_mmio_reg_offset(reg)))\n#define vgpu_vreg(vgpu, offset) \\\n\t(*(u32 *)(vgpu->mmio.vreg + (offset)))\n#define vgpu_vreg64_t(vgpu, reg) \\\n\t(*(u64 *)(vgpu->mmio.vreg + i915_mmio_reg_offset(reg)))\n#define vgpu_vreg64(vgpu, offset) \\\n\t(*(u64 *)(vgpu->mmio.vreg + (offset)))\n\n#define for_each_active_vgpu(gvt, vgpu, id) \\\n\tidr_for_each_entry((&(gvt)->vgpu_idr), (vgpu), (id)) \\\n\t\tfor_each_if(test_bit(INTEL_VGPU_STATUS_ACTIVE, vgpu->status))\n\nstatic inline void intel_vgpu_write_pci_bar(struct intel_vgpu *vgpu,\n\t\t\t\t\t    u32 offset, u32 val, bool low)\n{\n\tu32 *pval;\n\n\t \n\toffset = rounddown(offset, 4);\n\tpval = (u32 *)(vgpu_cfg_space(vgpu) + offset);\n\n\tif (low) {\n\t\t \n\t\t*pval = (val & GENMASK(31, 4)) | (*pval & GENMASK(3, 0));\n\t} else {\n\t\t*pval = val;\n\t}\n}\n\nint intel_gvt_init_vgpu_types(struct intel_gvt *gvt);\nvoid intel_gvt_clean_vgpu_types(struct intel_gvt *gvt);\n\nstruct intel_vgpu *intel_gvt_create_idle_vgpu(struct intel_gvt *gvt);\nvoid intel_gvt_destroy_idle_vgpu(struct intel_vgpu *vgpu);\nint intel_gvt_create_vgpu(struct intel_vgpu *vgpu,\n\t\t\t  const struct intel_vgpu_config *conf);\nvoid intel_gvt_destroy_vgpu(struct intel_vgpu *vgpu);\nvoid intel_gvt_release_vgpu(struct intel_vgpu *vgpu);\nvoid intel_gvt_reset_vgpu_locked(struct intel_vgpu *vgpu, bool dmlr,\n\t\t\t\t intel_engine_mask_t engine_mask);\nvoid intel_gvt_reset_vgpu(struct intel_vgpu *vgpu);\nvoid intel_gvt_activate_vgpu(struct intel_vgpu *vgpu);\nvoid intel_gvt_deactivate_vgpu(struct intel_vgpu *vgpu);\n\nint intel_gvt_set_opregion(struct intel_vgpu *vgpu);\nint intel_gvt_set_edid(struct intel_vgpu *vgpu, int port_num);\n\n \n#define vgpu_gmadr_is_aperture(vgpu, gmadr) \\\n\t((gmadr >= vgpu_aperture_gmadr_base(vgpu)) && \\\n\t (gmadr <= vgpu_aperture_gmadr_end(vgpu)))\n\n#define vgpu_gmadr_is_hidden(vgpu, gmadr) \\\n\t((gmadr >= vgpu_hidden_gmadr_base(vgpu)) && \\\n\t (gmadr <= vgpu_hidden_gmadr_end(vgpu)))\n\n#define vgpu_gmadr_is_valid(vgpu, gmadr) \\\n\t ((vgpu_gmadr_is_aperture(vgpu, gmadr) || \\\n\t  (vgpu_gmadr_is_hidden(vgpu, gmadr))))\n\n#define gvt_gmadr_is_aperture(gvt, gmadr) \\\n\t ((gmadr >= gvt_aperture_gmadr_base(gvt)) && \\\n\t  (gmadr <= gvt_aperture_gmadr_end(gvt)))\n\n#define gvt_gmadr_is_hidden(gvt, gmadr) \\\n\t  ((gmadr >= gvt_hidden_gmadr_base(gvt)) && \\\n\t   (gmadr <= gvt_hidden_gmadr_end(gvt)))\n\n#define gvt_gmadr_is_valid(gvt, gmadr) \\\n\t  (gvt_gmadr_is_aperture(gvt, gmadr) || \\\n\t    gvt_gmadr_is_hidden(gvt, gmadr))\n\nbool intel_gvt_ggtt_validate_range(struct intel_vgpu *vgpu, u64 addr, u32 size);\nint intel_gvt_ggtt_gmadr_g2h(struct intel_vgpu *vgpu, u64 g_addr, u64 *h_addr);\nint intel_gvt_ggtt_gmadr_h2g(struct intel_vgpu *vgpu, u64 h_addr, u64 *g_addr);\nint intel_gvt_ggtt_index_g2h(struct intel_vgpu *vgpu, unsigned long g_index,\n\t\t\t     unsigned long *h_index);\nint intel_gvt_ggtt_h2g_index(struct intel_vgpu *vgpu, unsigned long h_index,\n\t\t\t     unsigned long *g_index);\n\nvoid intel_vgpu_init_cfg_space(struct intel_vgpu *vgpu,\n\t\tbool primary);\nvoid intel_vgpu_reset_cfg_space(struct intel_vgpu *vgpu);\n\nint intel_vgpu_emulate_cfg_read(struct intel_vgpu *vgpu, unsigned int offset,\n\t\tvoid *p_data, unsigned int bytes);\n\nint intel_vgpu_emulate_cfg_write(struct intel_vgpu *vgpu, unsigned int offset,\n\t\tvoid *p_data, unsigned int bytes);\n\nvoid intel_vgpu_emulate_hotplug(struct intel_vgpu *vgpu, bool connected);\n\nstatic inline u64 intel_vgpu_get_bar_gpa(struct intel_vgpu *vgpu, int bar)\n{\n\t \n\treturn (*(u64 *)(vgpu->cfg_space.virtual_cfg_space + bar)) &\n\t\t\tPCI_BASE_ADDRESS_MEM_MASK;\n}\n\nvoid intel_vgpu_clean_opregion(struct intel_vgpu *vgpu);\nint intel_vgpu_init_opregion(struct intel_vgpu *vgpu);\nint intel_vgpu_opregion_base_write_handler(struct intel_vgpu *vgpu, u32 gpa);\n\nint intel_vgpu_emulate_opregion_request(struct intel_vgpu *vgpu, u32 swsci);\nvoid populate_pvinfo_page(struct intel_vgpu *vgpu);\n\nint intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload);\nvoid enter_failsafe_mode(struct intel_vgpu *vgpu, int reason);\nvoid intel_vgpu_detach_regions(struct intel_vgpu *vgpu);\n\nenum {\n\tGVT_FAILSAFE_UNSUPPORTED_GUEST,\n\tGVT_FAILSAFE_INSUFFICIENT_RESOURCE,\n\tGVT_FAILSAFE_GUEST_ERR,\n};\n\nstatic inline void mmio_hw_access_pre(struct intel_gt *gt)\n{\n\tintel_runtime_pm_get(gt->uncore->rpm);\n}\n\nstatic inline void mmio_hw_access_post(struct intel_gt *gt)\n{\n\tintel_runtime_pm_put_unchecked(gt->uncore->rpm);\n}\n\n \nstatic inline void intel_gvt_mmio_set_accessed(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\tgvt->mmio.mmio_attribute[offset >> 2] |= F_ACCESSED;\n}\n\n \nstatic inline bool intel_gvt_mmio_is_cmd_accessible(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\treturn gvt->mmio.mmio_attribute[offset >> 2] & F_CMD_ACCESS;\n}\n\n \nstatic inline void intel_gvt_mmio_set_cmd_accessible(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\tgvt->mmio.mmio_attribute[offset >> 2] |= F_CMD_ACCESS;\n}\n\n \nstatic inline bool intel_gvt_mmio_is_unalign(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\treturn gvt->mmio.mmio_attribute[offset >> 2] & F_UNALIGN;\n}\n\n \nstatic inline bool intel_gvt_mmio_has_mode_mask(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\treturn gvt->mmio.mmio_attribute[offset >> 2] & F_MODE_MASK;\n}\n\n \nstatic inline bool intel_gvt_mmio_is_sr_in_ctx(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\treturn gvt->mmio.mmio_attribute[offset >> 2] & F_SR_IN_CTX;\n}\n\n \nstatic inline void intel_gvt_mmio_set_sr_in_ctx(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\tgvt->mmio.mmio_attribute[offset >> 2] |= F_SR_IN_CTX;\n}\n\nvoid intel_gvt_debugfs_add_vgpu(struct intel_vgpu *vgpu);\n \nstatic inline void intel_gvt_mmio_set_cmd_write_patch(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\tgvt->mmio.mmio_attribute[offset >> 2] |= F_CMD_WRITE_PATCH;\n}\n\n \nstatic inline bool intel_gvt_mmio_is_cmd_write_patch(\n\t\t\tstruct intel_gvt *gvt, unsigned int offset)\n{\n\treturn gvt->mmio.mmio_attribute[offset >> 2] & F_CMD_WRITE_PATCH;\n}\n\n \nstatic inline int intel_gvt_read_gpa(struct intel_vgpu *vgpu, unsigned long gpa,\n\t\tvoid *buf, unsigned long len)\n{\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn -ESRCH;\n\treturn vfio_dma_rw(&vgpu->vfio_device, gpa, buf, len, false);\n}\n\n \nstatic inline int intel_gvt_write_gpa(struct intel_vgpu *vgpu,\n\t\tunsigned long gpa, void *buf, unsigned long len)\n{\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn -ESRCH;\n\treturn vfio_dma_rw(&vgpu->vfio_device, gpa, buf, len, true);\n}\n\nvoid intel_gvt_debugfs_remove_vgpu(struct intel_vgpu *vgpu);\nvoid intel_gvt_debugfs_init(struct intel_gvt *gvt);\nvoid intel_gvt_debugfs_clean(struct intel_gvt *gvt);\n\nint intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn);\nint intel_gvt_page_track_remove(struct intel_vgpu *info, u64 gfn);\nint intel_gvt_dma_pin_guest_page(struct intel_vgpu *vgpu, dma_addr_t dma_addr);\nint intel_gvt_dma_map_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tunsigned long size, dma_addr_t *dma_addr);\nvoid intel_gvt_dma_unmap_guest_page(struct intel_vgpu *vgpu,\n\t\tdma_addr_t dma_addr);\n\n#include \"trace.h\"\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}