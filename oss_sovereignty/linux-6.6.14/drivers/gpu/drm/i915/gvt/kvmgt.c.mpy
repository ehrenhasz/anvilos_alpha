{
  "module_name": "kvmgt.c",
  "hash_id": "f18cc36911815e1850e3c00975d32a01725701ea8ae80e41a21eb145e45f2279",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/kvmgt.c",
  "human_readable_source": " \n\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/kthread.h>\n#include <linux/sched/mm.h>\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/rbtree.h>\n#include <linux/spinlock.h>\n#include <linux/eventfd.h>\n#include <linux/mdev.h>\n#include <linux/debugfs.h>\n\n#include <linux/nospec.h>\n\n#include <drm/drm_edid.h>\n\n#include \"i915_drv.h\"\n#include \"intel_gvt.h\"\n#include \"gvt.h\"\n\nMODULE_IMPORT_NS(DMA_BUF);\nMODULE_IMPORT_NS(I915_GVT);\n\n \n#define VFIO_PCI_OFFSET_SHIFT   40\n#define VFIO_PCI_OFFSET_TO_INDEX(off)   (off >> VFIO_PCI_OFFSET_SHIFT)\n#define VFIO_PCI_INDEX_TO_OFFSET(index) ((u64)(index) << VFIO_PCI_OFFSET_SHIFT)\n#define VFIO_PCI_OFFSET_MASK    (((u64)(1) << VFIO_PCI_OFFSET_SHIFT) - 1)\n\n#define EDID_BLOB_OFFSET (PAGE_SIZE/2)\n\n#define OPREGION_SIGNATURE \"IntelGraphicsMem\"\n\nstruct vfio_region;\nstruct intel_vgpu_regops {\n\tsize_t (*rw)(struct intel_vgpu *vgpu, char *buf,\n\t\t\tsize_t count, loff_t *ppos, bool iswrite);\n\tvoid (*release)(struct intel_vgpu *vgpu,\n\t\t\tstruct vfio_region *region);\n};\n\nstruct vfio_region {\n\tu32\t\t\t\ttype;\n\tu32\t\t\t\tsubtype;\n\tsize_t\t\t\t\tsize;\n\tu32\t\t\t\tflags;\n\tconst struct intel_vgpu_regops\t*ops;\n\tvoid\t\t\t\t*data;\n};\n\nstruct vfio_edid_region {\n\tstruct vfio_region_gfx_edid vfio_edid_regs;\n\tvoid *edid_blob;\n};\n\nstruct kvmgt_pgfn {\n\tgfn_t gfn;\n\tstruct hlist_node hnode;\n};\n\nstruct gvt_dma {\n\tstruct intel_vgpu *vgpu;\n\tstruct rb_node gfn_node;\n\tstruct rb_node dma_addr_node;\n\tgfn_t gfn;\n\tdma_addr_t dma_addr;\n\tunsigned long size;\n\tstruct kref ref;\n};\n\n#define vfio_dev_to_vgpu(vfio_dev) \\\n\tcontainer_of((vfio_dev), struct intel_vgpu, vfio_device)\n\nstatic void kvmgt_page_track_write(gpa_t gpa, const u8 *val, int len,\n\t\t\t\t   struct kvm_page_track_notifier_node *node);\nstatic void kvmgt_page_track_remove_region(gfn_t gfn, unsigned long nr_pages,\n\t\t\t\t\t   struct kvm_page_track_notifier_node *node);\n\nstatic ssize_t intel_vgpu_show_description(struct mdev_type *mtype, char *buf)\n{\n\tstruct intel_vgpu_type *type =\n\t\tcontainer_of(mtype, struct intel_vgpu_type, type);\n\n\treturn sprintf(buf, \"low_gm_size: %dMB\\nhigh_gm_size: %dMB\\n\"\n\t\t       \"fence: %d\\nresolution: %s\\n\"\n\t\t       \"weight: %d\\n\",\n\t\t       BYTES_TO_MB(type->conf->low_mm),\n\t\t       BYTES_TO_MB(type->conf->high_mm),\n\t\t       type->conf->fence, vgpu_edid_str(type->conf->edid),\n\t\t       type->conf->weight);\n}\n\nstatic void gvt_unpin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tunsigned long size)\n{\n\tvfio_unpin_pages(&vgpu->vfio_device, gfn << PAGE_SHIFT,\n\t\t\t DIV_ROUND_UP(size, PAGE_SIZE));\n}\n\n \nstatic int gvt_pin_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tunsigned long size, struct page **page)\n{\n\tint total_pages = DIV_ROUND_UP(size, PAGE_SIZE);\n\tstruct page *base_page = NULL;\n\tint npage;\n\tint ret;\n\n\t \n\tfor (npage = 0; npage < total_pages; npage++) {\n\t\tdma_addr_t cur_iova = (gfn + npage) << PAGE_SHIFT;\n\t\tstruct page *cur_page;\n\n\t\tret = vfio_pin_pages(&vgpu->vfio_device, cur_iova, 1,\n\t\t\t\t     IOMMU_READ | IOMMU_WRITE, &cur_page);\n\t\tif (ret != 1) {\n\t\t\tgvt_vgpu_err(\"vfio_pin_pages failed for iova %pad, ret %d\\n\",\n\t\t\t\t     &cur_iova, ret);\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (npage == 0)\n\t\t\tbase_page = cur_page;\n\t\telse if (page_to_pfn(base_page) + npage != page_to_pfn(cur_page)) {\n\t\t\tret = -EINVAL;\n\t\t\tnpage++;\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\t*page = base_page;\n\treturn 0;\nerr:\n\tif (npage)\n\t\tgvt_unpin_guest_page(vgpu, gfn, npage * PAGE_SIZE);\n\treturn ret;\n}\n\nstatic int gvt_dma_map_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tdma_addr_t *dma_addr, unsigned long size)\n{\n\tstruct device *dev = vgpu->gvt->gt->i915->drm.dev;\n\tstruct page *page = NULL;\n\tint ret;\n\n\tret = gvt_pin_guest_page(vgpu, gfn, size, &page);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\t*dma_addr = dma_map_page(dev, page, 0, size, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(dev, *dma_addr)) {\n\t\tgvt_vgpu_err(\"DMA mapping failed for pfn 0x%lx, ret %d\\n\",\n\t\t\t     page_to_pfn(page), ret);\n\t\tgvt_unpin_guest_page(vgpu, gfn, size);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void gvt_dma_unmap_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tdma_addr_t dma_addr, unsigned long size)\n{\n\tstruct device *dev = vgpu->gvt->gt->i915->drm.dev;\n\n\tdma_unmap_page(dev, dma_addr, size, DMA_BIDIRECTIONAL);\n\tgvt_unpin_guest_page(vgpu, gfn, size);\n}\n\nstatic struct gvt_dma *__gvt_cache_find_dma_addr(struct intel_vgpu *vgpu,\n\t\tdma_addr_t dma_addr)\n{\n\tstruct rb_node *node = vgpu->dma_addr_cache.rb_node;\n\tstruct gvt_dma *itr;\n\n\twhile (node) {\n\t\titr = rb_entry(node, struct gvt_dma, dma_addr_node);\n\n\t\tif (dma_addr < itr->dma_addr)\n\t\t\tnode = node->rb_left;\n\t\telse if (dma_addr > itr->dma_addr)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn itr;\n\t}\n\treturn NULL;\n}\n\nstatic struct gvt_dma *__gvt_cache_find_gfn(struct intel_vgpu *vgpu, gfn_t gfn)\n{\n\tstruct rb_node *node = vgpu->gfn_cache.rb_node;\n\tstruct gvt_dma *itr;\n\n\twhile (node) {\n\t\titr = rb_entry(node, struct gvt_dma, gfn_node);\n\n\t\tif (gfn < itr->gfn)\n\t\t\tnode = node->rb_left;\n\t\telse if (gfn > itr->gfn)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn itr;\n\t}\n\treturn NULL;\n}\n\nstatic int __gvt_cache_add(struct intel_vgpu *vgpu, gfn_t gfn,\n\t\tdma_addr_t dma_addr, unsigned long size)\n{\n\tstruct gvt_dma *new, *itr;\n\tstruct rb_node **link, *parent = NULL;\n\n\tnew = kzalloc(sizeof(struct gvt_dma), GFP_KERNEL);\n\tif (!new)\n\t\treturn -ENOMEM;\n\n\tnew->vgpu = vgpu;\n\tnew->gfn = gfn;\n\tnew->dma_addr = dma_addr;\n\tnew->size = size;\n\tkref_init(&new->ref);\n\n\t \n\tlink = &vgpu->gfn_cache.rb_node;\n\twhile (*link) {\n\t\tparent = *link;\n\t\titr = rb_entry(parent, struct gvt_dma, gfn_node);\n\n\t\tif (gfn < itr->gfn)\n\t\t\tlink = &parent->rb_left;\n\t\telse\n\t\t\tlink = &parent->rb_right;\n\t}\n\trb_link_node(&new->gfn_node, parent, link);\n\trb_insert_color(&new->gfn_node, &vgpu->gfn_cache);\n\n\t \n\tparent = NULL;\n\tlink = &vgpu->dma_addr_cache.rb_node;\n\twhile (*link) {\n\t\tparent = *link;\n\t\titr = rb_entry(parent, struct gvt_dma, dma_addr_node);\n\n\t\tif (dma_addr < itr->dma_addr)\n\t\t\tlink = &parent->rb_left;\n\t\telse\n\t\t\tlink = &parent->rb_right;\n\t}\n\trb_link_node(&new->dma_addr_node, parent, link);\n\trb_insert_color(&new->dma_addr_node, &vgpu->dma_addr_cache);\n\n\tvgpu->nr_cache_entries++;\n\treturn 0;\n}\n\nstatic void __gvt_cache_remove_entry(struct intel_vgpu *vgpu,\n\t\t\t\tstruct gvt_dma *entry)\n{\n\trb_erase(&entry->gfn_node, &vgpu->gfn_cache);\n\trb_erase(&entry->dma_addr_node, &vgpu->dma_addr_cache);\n\tkfree(entry);\n\tvgpu->nr_cache_entries--;\n}\n\nstatic void gvt_cache_destroy(struct intel_vgpu *vgpu)\n{\n\tstruct gvt_dma *dma;\n\tstruct rb_node *node = NULL;\n\n\tfor (;;) {\n\t\tmutex_lock(&vgpu->cache_lock);\n\t\tnode = rb_first(&vgpu->gfn_cache);\n\t\tif (!node) {\n\t\t\tmutex_unlock(&vgpu->cache_lock);\n\t\t\tbreak;\n\t\t}\n\t\tdma = rb_entry(node, struct gvt_dma, gfn_node);\n\t\tgvt_dma_unmap_page(vgpu, dma->gfn, dma->dma_addr, dma->size);\n\t\t__gvt_cache_remove_entry(vgpu, dma);\n\t\tmutex_unlock(&vgpu->cache_lock);\n\t}\n}\n\nstatic void gvt_cache_init(struct intel_vgpu *vgpu)\n{\n\tvgpu->gfn_cache = RB_ROOT;\n\tvgpu->dma_addr_cache = RB_ROOT;\n\tvgpu->nr_cache_entries = 0;\n\tmutex_init(&vgpu->cache_lock);\n}\n\nstatic void kvmgt_protect_table_init(struct intel_vgpu *info)\n{\n\thash_init(info->ptable);\n}\n\nstatic void kvmgt_protect_table_destroy(struct intel_vgpu *info)\n{\n\tstruct kvmgt_pgfn *p;\n\tstruct hlist_node *tmp;\n\tint i;\n\n\thash_for_each_safe(info->ptable, i, tmp, p, hnode) {\n\t\thash_del(&p->hnode);\n\t\tkfree(p);\n\t}\n}\n\nstatic struct kvmgt_pgfn *\n__kvmgt_protect_table_find(struct intel_vgpu *info, gfn_t gfn)\n{\n\tstruct kvmgt_pgfn *p, *res = NULL;\n\n\tlockdep_assert_held(&info->vgpu_lock);\n\n\thash_for_each_possible(info->ptable, p, hnode, gfn) {\n\t\tif (gfn == p->gfn) {\n\t\t\tres = p;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn res;\n}\n\nstatic bool kvmgt_gfn_is_write_protected(struct intel_vgpu *info, gfn_t gfn)\n{\n\tstruct kvmgt_pgfn *p;\n\n\tp = __kvmgt_protect_table_find(info, gfn);\n\treturn !!p;\n}\n\nstatic void kvmgt_protect_table_add(struct intel_vgpu *info, gfn_t gfn)\n{\n\tstruct kvmgt_pgfn *p;\n\n\tif (kvmgt_gfn_is_write_protected(info, gfn))\n\t\treturn;\n\n\tp = kzalloc(sizeof(struct kvmgt_pgfn), GFP_ATOMIC);\n\tif (WARN(!p, \"gfn: 0x%llx\\n\", gfn))\n\t\treturn;\n\n\tp->gfn = gfn;\n\thash_add(info->ptable, &p->hnode, gfn);\n}\n\nstatic void kvmgt_protect_table_del(struct intel_vgpu *info, gfn_t gfn)\n{\n\tstruct kvmgt_pgfn *p;\n\n\tp = __kvmgt_protect_table_find(info, gfn);\n\tif (p) {\n\t\thash_del(&p->hnode);\n\t\tkfree(p);\n\t}\n}\n\nstatic size_t intel_vgpu_reg_rw_opregion(struct intel_vgpu *vgpu, char *buf,\n\t\tsize_t count, loff_t *ppos, bool iswrite)\n{\n\tunsigned int i = VFIO_PCI_OFFSET_TO_INDEX(*ppos) -\n\t\t\tVFIO_PCI_NUM_REGIONS;\n\tvoid *base = vgpu->region[i].data;\n\tloff_t pos = *ppos & VFIO_PCI_OFFSET_MASK;\n\n\n\tif (pos >= vgpu->region[i].size || iswrite) {\n\t\tgvt_vgpu_err(\"invalid op or offset for Intel vgpu OpRegion\\n\");\n\t\treturn -EINVAL;\n\t}\n\tcount = min(count, (size_t)(vgpu->region[i].size - pos));\n\tmemcpy(buf, base + pos, count);\n\n\treturn count;\n}\n\nstatic void intel_vgpu_reg_release_opregion(struct intel_vgpu *vgpu,\n\t\tstruct vfio_region *region)\n{\n}\n\nstatic const struct intel_vgpu_regops intel_vgpu_regops_opregion = {\n\t.rw = intel_vgpu_reg_rw_opregion,\n\t.release = intel_vgpu_reg_release_opregion,\n};\n\nstatic int handle_edid_regs(struct intel_vgpu *vgpu,\n\t\t\tstruct vfio_edid_region *region, char *buf,\n\t\t\tsize_t count, u16 offset, bool is_write)\n{\n\tstruct vfio_region_gfx_edid *regs = &region->vfio_edid_regs;\n\tunsigned int data;\n\n\tif (offset + count > sizeof(*regs))\n\t\treturn -EINVAL;\n\n\tif (count != 4)\n\t\treturn -EINVAL;\n\n\tif (is_write) {\n\t\tdata = *((unsigned int *)buf);\n\t\tswitch (offset) {\n\t\tcase offsetof(struct vfio_region_gfx_edid, link_state):\n\t\t\tif (data == VFIO_DEVICE_GFX_LINK_STATE_UP) {\n\t\t\t\tif (!drm_edid_block_valid(\n\t\t\t\t\t(u8 *)region->edid_blob,\n\t\t\t\t\t0,\n\t\t\t\t\ttrue,\n\t\t\t\t\tNULL)) {\n\t\t\t\t\tgvt_vgpu_err(\"invalid EDID blob\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\tintel_vgpu_emulate_hotplug(vgpu, true);\n\t\t\t} else if (data == VFIO_DEVICE_GFX_LINK_STATE_DOWN)\n\t\t\t\tintel_vgpu_emulate_hotplug(vgpu, false);\n\t\t\telse {\n\t\t\t\tgvt_vgpu_err(\"invalid EDID link state %d\\n\",\n\t\t\t\t\tregs->link_state);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tregs->link_state = data;\n\t\t\tbreak;\n\t\tcase offsetof(struct vfio_region_gfx_edid, edid_size):\n\t\t\tif (data > regs->edid_max_size) {\n\t\t\t\tgvt_vgpu_err(\"EDID size is bigger than %d!\\n\",\n\t\t\t\t\tregs->edid_max_size);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tregs->edid_size = data;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tgvt_vgpu_err(\"write read-only EDID region at offset %d\\n\",\n\t\t\t\toffset);\n\t\t\treturn -EPERM;\n\t\t}\n\t} else {\n\t\tmemcpy(buf, (char *)regs + offset, count);\n\t}\n\n\treturn count;\n}\n\nstatic int handle_edid_blob(struct vfio_edid_region *region, char *buf,\n\t\t\tsize_t count, u16 offset, bool is_write)\n{\n\tif (offset + count > region->vfio_edid_regs.edid_size)\n\t\treturn -EINVAL;\n\n\tif (is_write)\n\t\tmemcpy(region->edid_blob + offset, buf, count);\n\telse\n\t\tmemcpy(buf, region->edid_blob + offset, count);\n\n\treturn count;\n}\n\nstatic size_t intel_vgpu_reg_rw_edid(struct intel_vgpu *vgpu, char *buf,\n\t\tsize_t count, loff_t *ppos, bool iswrite)\n{\n\tint ret;\n\tunsigned int i = VFIO_PCI_OFFSET_TO_INDEX(*ppos) -\n\t\t\tVFIO_PCI_NUM_REGIONS;\n\tstruct vfio_edid_region *region = vgpu->region[i].data;\n\tloff_t pos = *ppos & VFIO_PCI_OFFSET_MASK;\n\n\tif (pos < region->vfio_edid_regs.edid_offset) {\n\t\tret = handle_edid_regs(vgpu, region, buf, count, pos, iswrite);\n\t} else {\n\t\tpos -= EDID_BLOB_OFFSET;\n\t\tret = handle_edid_blob(region, buf, count, pos, iswrite);\n\t}\n\n\tif (ret < 0)\n\t\tgvt_vgpu_err(\"failed to access EDID region\\n\");\n\n\treturn ret;\n}\n\nstatic void intel_vgpu_reg_release_edid(struct intel_vgpu *vgpu,\n\t\t\t\t\tstruct vfio_region *region)\n{\n\tkfree(region->data);\n}\n\nstatic const struct intel_vgpu_regops intel_vgpu_regops_edid = {\n\t.rw = intel_vgpu_reg_rw_edid,\n\t.release = intel_vgpu_reg_release_edid,\n};\n\nstatic int intel_vgpu_register_reg(struct intel_vgpu *vgpu,\n\t\tunsigned int type, unsigned int subtype,\n\t\tconst struct intel_vgpu_regops *ops,\n\t\tsize_t size, u32 flags, void *data)\n{\n\tstruct vfio_region *region;\n\n\tregion = krealloc(vgpu->region,\n\t\t\t(vgpu->num_regions + 1) * sizeof(*region),\n\t\t\tGFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tvgpu->region = region;\n\tvgpu->region[vgpu->num_regions].type = type;\n\tvgpu->region[vgpu->num_regions].subtype = subtype;\n\tvgpu->region[vgpu->num_regions].ops = ops;\n\tvgpu->region[vgpu->num_regions].size = size;\n\tvgpu->region[vgpu->num_regions].flags = flags;\n\tvgpu->region[vgpu->num_regions].data = data;\n\tvgpu->num_regions++;\n\treturn 0;\n}\n\nint intel_gvt_set_opregion(struct intel_vgpu *vgpu)\n{\n\tvoid *base;\n\tint ret;\n\n\t \n\tbase = vgpu_opregion(vgpu)->va;\n\tif (!base)\n\t\treturn -ENOMEM;\n\n\tif (memcmp(base, OPREGION_SIGNATURE, 16)) {\n\t\tmemunmap(base);\n\t\treturn -EINVAL;\n\t}\n\n\tret = intel_vgpu_register_reg(vgpu,\n\t\t\tPCI_VENDOR_ID_INTEL | VFIO_REGION_TYPE_PCI_VENDOR_TYPE,\n\t\t\tVFIO_REGION_SUBTYPE_INTEL_IGD_OPREGION,\n\t\t\t&intel_vgpu_regops_opregion, OPREGION_SIZE,\n\t\t\tVFIO_REGION_INFO_FLAG_READ, base);\n\n\treturn ret;\n}\n\nint intel_gvt_set_edid(struct intel_vgpu *vgpu, int port_num)\n{\n\tstruct intel_vgpu_port *port = intel_vgpu_port(vgpu, port_num);\n\tstruct vfio_edid_region *base;\n\tint ret;\n\n\tbase = kzalloc(sizeof(*base), GFP_KERNEL);\n\tif (!base)\n\t\treturn -ENOMEM;\n\n\t \n\tbase->vfio_edid_regs.edid_offset = EDID_BLOB_OFFSET;\n\tbase->vfio_edid_regs.edid_max_size = EDID_SIZE;\n\tbase->vfio_edid_regs.edid_size = EDID_SIZE;\n\tbase->vfio_edid_regs.max_xres = vgpu_edid_xres(port->id);\n\tbase->vfio_edid_regs.max_yres = vgpu_edid_yres(port->id);\n\tbase->edid_blob = port->edid->edid_block;\n\n\tret = intel_vgpu_register_reg(vgpu,\n\t\t\tVFIO_REGION_TYPE_GFX,\n\t\t\tVFIO_REGION_SUBTYPE_GFX_EDID,\n\t\t\t&intel_vgpu_regops_edid, EDID_SIZE,\n\t\t\tVFIO_REGION_INFO_FLAG_READ |\n\t\t\tVFIO_REGION_INFO_FLAG_WRITE |\n\t\t\tVFIO_REGION_INFO_FLAG_CAPS, base);\n\n\treturn ret;\n}\n\nstatic void intel_vgpu_dma_unmap(struct vfio_device *vfio_dev, u64 iova,\n\t\t\t\t u64 length)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tstruct gvt_dma *entry;\n\tu64 iov_pfn = iova >> PAGE_SHIFT;\n\tu64 end_iov_pfn = iov_pfn + length / PAGE_SIZE;\n\n\tmutex_lock(&vgpu->cache_lock);\n\tfor (; iov_pfn < end_iov_pfn; iov_pfn++) {\n\t\tentry = __gvt_cache_find_gfn(vgpu, iov_pfn);\n\t\tif (!entry)\n\t\t\tcontinue;\n\n\t\tgvt_dma_unmap_page(vgpu, entry->gfn, entry->dma_addr,\n\t\t\t\t   entry->size);\n\t\t__gvt_cache_remove_entry(vgpu, entry);\n\t}\n\tmutex_unlock(&vgpu->cache_lock);\n}\n\nstatic bool __kvmgt_vgpu_exist(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu *itr;\n\tint id;\n\tbool ret = false;\n\n\tmutex_lock(&vgpu->gvt->lock);\n\tfor_each_active_vgpu(vgpu->gvt, itr, id) {\n\t\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, itr->status))\n\t\t\tcontinue;\n\n\t\tif (vgpu->vfio_device.kvm == itr->vfio_device.kvm) {\n\t\t\tret = true;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vgpu->gvt->lock);\n\treturn ret;\n}\n\nstatic int intel_vgpu_open_device(struct vfio_device *vfio_dev)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tint ret;\n\n\tif (__kvmgt_vgpu_exist(vgpu))\n\t\treturn -EEXIST;\n\n\tvgpu->track_node.track_write = kvmgt_page_track_write;\n\tvgpu->track_node.track_remove_region = kvmgt_page_track_remove_region;\n\tret = kvm_page_track_register_notifier(vgpu->vfio_device.kvm,\n\t\t\t\t\t       &vgpu->track_node);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"KVM is required to use Intel vGPU\\n\");\n\t\treturn ret;\n\t}\n\n\tset_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status);\n\n\tdebugfs_create_ulong(KVMGT_DEBUGFS_FILENAME, 0444, vgpu->debugfs,\n\t\t\t     &vgpu->nr_cache_entries);\n\n\tintel_gvt_activate_vgpu(vgpu);\n\n\treturn 0;\n}\n\nstatic void intel_vgpu_release_msi_eventfd_ctx(struct intel_vgpu *vgpu)\n{\n\tstruct eventfd_ctx *trigger;\n\n\ttrigger = vgpu->msi_trigger;\n\tif (trigger) {\n\t\teventfd_ctx_put(trigger);\n\t\tvgpu->msi_trigger = NULL;\n\t}\n}\n\nstatic void intel_vgpu_close_device(struct vfio_device *vfio_dev)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\n\tintel_gvt_release_vgpu(vgpu);\n\n\tclear_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status);\n\n\tdebugfs_lookup_and_remove(KVMGT_DEBUGFS_FILENAME, vgpu->debugfs);\n\n\tkvm_page_track_unregister_notifier(vgpu->vfio_device.kvm,\n\t\t\t\t\t   &vgpu->track_node);\n\n\tkvmgt_protect_table_destroy(vgpu);\n\tgvt_cache_destroy(vgpu);\n\n\tWARN_ON(vgpu->nr_cache_entries);\n\n\tvgpu->gfn_cache = RB_ROOT;\n\tvgpu->dma_addr_cache = RB_ROOT;\n\n\tintel_vgpu_release_msi_eventfd_ctx(vgpu);\n}\n\nstatic u64 intel_vgpu_get_bar_addr(struct intel_vgpu *vgpu, int bar)\n{\n\tu32 start_lo, start_hi;\n\tu32 mem_type;\n\n\tstart_lo = (*(u32 *)(vgpu->cfg_space.virtual_cfg_space + bar)) &\n\t\t\tPCI_BASE_ADDRESS_MEM_MASK;\n\tmem_type = (*(u32 *)(vgpu->cfg_space.virtual_cfg_space + bar)) &\n\t\t\tPCI_BASE_ADDRESS_MEM_TYPE_MASK;\n\n\tswitch (mem_type) {\n\tcase PCI_BASE_ADDRESS_MEM_TYPE_64:\n\t\tstart_hi = (*(u32 *)(vgpu->cfg_space.virtual_cfg_space\n\t\t\t\t\t\t+ bar + 4));\n\t\tbreak;\n\tcase PCI_BASE_ADDRESS_MEM_TYPE_32:\n\tcase PCI_BASE_ADDRESS_MEM_TYPE_1M:\n\t\t \n\tdefault:\n\t\t \n\t\tstart_hi = 0;\n\t\tbreak;\n\t}\n\n\treturn ((u64)start_hi << 32) | start_lo;\n}\n\nstatic int intel_vgpu_bar_rw(struct intel_vgpu *vgpu, int bar, u64 off,\n\t\t\t     void *buf, unsigned int count, bool is_write)\n{\n\tu64 bar_start = intel_vgpu_get_bar_addr(vgpu, bar);\n\tint ret;\n\n\tif (is_write)\n\t\tret = intel_vgpu_emulate_mmio_write(vgpu,\n\t\t\t\t\tbar_start + off, buf, count);\n\telse\n\t\tret = intel_vgpu_emulate_mmio_read(vgpu,\n\t\t\t\t\tbar_start + off, buf, count);\n\treturn ret;\n}\n\nstatic inline bool intel_vgpu_in_aperture(struct intel_vgpu *vgpu, u64 off)\n{\n\treturn off >= vgpu_aperture_offset(vgpu) &&\n\t       off < vgpu_aperture_offset(vgpu) + vgpu_aperture_sz(vgpu);\n}\n\nstatic int intel_vgpu_aperture_rw(struct intel_vgpu *vgpu, u64 off,\n\t\tvoid *buf, unsigned long count, bool is_write)\n{\n\tvoid __iomem *aperture_va;\n\n\tif (!intel_vgpu_in_aperture(vgpu, off) ||\n\t    !intel_vgpu_in_aperture(vgpu, off + count)) {\n\t\tgvt_vgpu_err(\"Invalid aperture offset %llu\\n\", off);\n\t\treturn -EINVAL;\n\t}\n\n\taperture_va = io_mapping_map_wc(&vgpu->gvt->gt->ggtt->iomap,\n\t\t\t\t\tALIGN_DOWN(off, PAGE_SIZE),\n\t\t\t\t\tcount + offset_in_page(off));\n\tif (!aperture_va)\n\t\treturn -EIO;\n\n\tif (is_write)\n\t\tmemcpy_toio(aperture_va + offset_in_page(off), buf, count);\n\telse\n\t\tmemcpy_fromio(buf, aperture_va + offset_in_page(off), count);\n\n\tio_mapping_unmap(aperture_va);\n\n\treturn 0;\n}\n\nstatic ssize_t intel_vgpu_rw(struct intel_vgpu *vgpu, char *buf,\n\t\t\tsize_t count, loff_t *ppos, bool is_write)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tu64 pos = *ppos & VFIO_PCI_OFFSET_MASK;\n\tint ret = -EINVAL;\n\n\n\tif (index >= VFIO_PCI_NUM_REGIONS + vgpu->num_regions) {\n\t\tgvt_vgpu_err(\"invalid index: %u\\n\", index);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (index) {\n\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\tif (is_write)\n\t\t\tret = intel_vgpu_emulate_cfg_write(vgpu, pos,\n\t\t\t\t\t\tbuf, count);\n\t\telse\n\t\t\tret = intel_vgpu_emulate_cfg_read(vgpu, pos,\n\t\t\t\t\t\tbuf, count);\n\t\tbreak;\n\tcase VFIO_PCI_BAR0_REGION_INDEX:\n\t\tret = intel_vgpu_bar_rw(vgpu, PCI_BASE_ADDRESS_0, pos,\n\t\t\t\t\tbuf, count, is_write);\n\t\tbreak;\n\tcase VFIO_PCI_BAR2_REGION_INDEX:\n\t\tret = intel_vgpu_aperture_rw(vgpu, pos, buf, count, is_write);\n\t\tbreak;\n\tcase VFIO_PCI_BAR1_REGION_INDEX:\n\tcase VFIO_PCI_BAR3_REGION_INDEX:\n\tcase VFIO_PCI_BAR4_REGION_INDEX:\n\tcase VFIO_PCI_BAR5_REGION_INDEX:\n\tcase VFIO_PCI_VGA_REGION_INDEX:\n\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\tbreak;\n\tdefault:\n\t\tif (index >= VFIO_PCI_NUM_REGIONS + vgpu->num_regions)\n\t\t\treturn -EINVAL;\n\n\t\tindex -= VFIO_PCI_NUM_REGIONS;\n\t\treturn vgpu->region[index].ops->rw(vgpu, buf, count,\n\t\t\t\tppos, is_write);\n\t}\n\n\treturn ret == 0 ? count : ret;\n}\n\nstatic bool gtt_entry(struct intel_vgpu *vgpu, loff_t *ppos)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tint offset;\n\n\t \n\tif (index != PCI_BASE_ADDRESS_0)\n\t\treturn false;\n\n\toffset = (u64)(*ppos & VFIO_PCI_OFFSET_MASK) -\n\t\tintel_vgpu_get_bar_gpa(vgpu, PCI_BASE_ADDRESS_0);\n\n\treturn (offset >= gvt->device_info.gtt_start_offset &&\n\t\toffset < gvt->device_info.gtt_start_offset + gvt_ggtt_sz(gvt)) ?\n\t\t\ttrue : false;\n}\n\nstatic ssize_t intel_vgpu_read(struct vfio_device *vfio_dev, char __user *buf,\n\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tunsigned int done = 0;\n\tint ret;\n\n\twhile (count) {\n\t\tsize_t filled;\n\n\t\t \n\t\tif (count >= 8 && !(*ppos % 8) &&\n\t\t\tgtt_entry(vgpu, ppos)) {\n\t\t\tu64 val;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val, sizeof(val),\n\t\t\t\t\tppos, false);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto read_err;\n\n\t\t\tif (copy_to_user(buf, &val, sizeof(val)))\n\t\t\t\tgoto read_err;\n\n\t\t\tfilled = 8;\n\t\t} else if (count >= 4 && !(*ppos % 4)) {\n\t\t\tu32 val;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val, sizeof(val),\n\t\t\t\t\tppos, false);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto read_err;\n\n\t\t\tif (copy_to_user(buf, &val, sizeof(val)))\n\t\t\t\tgoto read_err;\n\n\t\t\tfilled = 4;\n\t\t} else if (count >= 2 && !(*ppos % 2)) {\n\t\t\tu16 val;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val, sizeof(val),\n\t\t\t\t\tppos, false);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto read_err;\n\n\t\t\tif (copy_to_user(buf, &val, sizeof(val)))\n\t\t\t\tgoto read_err;\n\n\t\t\tfilled = 2;\n\t\t} else {\n\t\t\tu8 val;\n\n\t\t\tret = intel_vgpu_rw(vgpu, &val, sizeof(val), ppos,\n\t\t\t\t\tfalse);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto read_err;\n\n\t\t\tif (copy_to_user(buf, &val, sizeof(val)))\n\t\t\t\tgoto read_err;\n\n\t\t\tfilled = 1;\n\t\t}\n\n\t\tcount -= filled;\n\t\tdone += filled;\n\t\t*ppos += filled;\n\t\tbuf += filled;\n\t}\n\n\treturn done;\n\nread_err:\n\treturn -EFAULT;\n}\n\nstatic ssize_t intel_vgpu_write(struct vfio_device *vfio_dev,\n\t\t\t\tconst char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tunsigned int done = 0;\n\tint ret;\n\n\twhile (count) {\n\t\tsize_t filled;\n\n\t\t \n\t\tif (count >= 8 && !(*ppos % 8) &&\n\t\t\tgtt_entry(vgpu, ppos)) {\n\t\t\tu64 val;\n\n\t\t\tif (copy_from_user(&val, buf, sizeof(val)))\n\t\t\t\tgoto write_err;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val, sizeof(val),\n\t\t\t\t\tppos, true);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto write_err;\n\n\t\t\tfilled = 8;\n\t\t} else if (count >= 4 && !(*ppos % 4)) {\n\t\t\tu32 val;\n\n\t\t\tif (copy_from_user(&val, buf, sizeof(val)))\n\t\t\t\tgoto write_err;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val, sizeof(val),\n\t\t\t\t\tppos, true);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto write_err;\n\n\t\t\tfilled = 4;\n\t\t} else if (count >= 2 && !(*ppos % 2)) {\n\t\t\tu16 val;\n\n\t\t\tif (copy_from_user(&val, buf, sizeof(val)))\n\t\t\t\tgoto write_err;\n\n\t\t\tret = intel_vgpu_rw(vgpu, (char *)&val,\n\t\t\t\t\tsizeof(val), ppos, true);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto write_err;\n\n\t\t\tfilled = 2;\n\t\t} else {\n\t\t\tu8 val;\n\n\t\t\tif (copy_from_user(&val, buf, sizeof(val)))\n\t\t\t\tgoto write_err;\n\n\t\t\tret = intel_vgpu_rw(vgpu, &val, sizeof(val),\n\t\t\t\t\tppos, true);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto write_err;\n\n\t\t\tfilled = 1;\n\t\t}\n\n\t\tcount -= filled;\n\t\tdone += filled;\n\t\t*ppos += filled;\n\t\tbuf += filled;\n\t}\n\n\treturn done;\nwrite_err:\n\treturn -EFAULT;\n}\n\nstatic int intel_vgpu_mmap(struct vfio_device *vfio_dev,\n\t\tstruct vm_area_struct *vma)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tunsigned int index;\n\tu64 virtaddr;\n\tunsigned long req_size, pgoff, req_start;\n\tpgprot_t pg_prot;\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index != VFIO_PCI_BAR2_REGION_INDEX)\n\t\treturn -EINVAL;\n\n\tpg_prot = vma->vm_page_prot;\n\tvirtaddr = vma->vm_start;\n\treq_size = vma->vm_end - vma->vm_start;\n\tpgoff = vma->vm_pgoff &\n\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\treq_start = pgoff << PAGE_SHIFT;\n\n\tif (!intel_vgpu_in_aperture(vgpu, req_start))\n\t\treturn -EINVAL;\n\tif (req_start + req_size >\n\t    vgpu_aperture_offset(vgpu) + vgpu_aperture_sz(vgpu))\n\t\treturn -EINVAL;\n\n\tpgoff = (gvt_aperture_pa_base(vgpu->gvt) >> PAGE_SHIFT) + pgoff;\n\n\treturn remap_pfn_range(vma, virtaddr, pgoff, req_size, pg_prot);\n}\n\nstatic int intel_vgpu_get_irq_count(struct intel_vgpu *vgpu, int type)\n{\n\tif (type == VFIO_PCI_INTX_IRQ_INDEX || type == VFIO_PCI_MSI_IRQ_INDEX)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int intel_vgpu_set_intx_mask(struct intel_vgpu *vgpu,\n\t\t\tunsigned int index, unsigned int start,\n\t\t\tunsigned int count, u32 flags,\n\t\t\tvoid *data)\n{\n\treturn 0;\n}\n\nstatic int intel_vgpu_set_intx_unmask(struct intel_vgpu *vgpu,\n\t\t\tunsigned int index, unsigned int start,\n\t\t\tunsigned int count, u32 flags, void *data)\n{\n\treturn 0;\n}\n\nstatic int intel_vgpu_set_intx_trigger(struct intel_vgpu *vgpu,\n\t\tunsigned int index, unsigned int start, unsigned int count,\n\t\tu32 flags, void *data)\n{\n\treturn 0;\n}\n\nstatic int intel_vgpu_set_msi_trigger(struct intel_vgpu *vgpu,\n\t\tunsigned int index, unsigned int start, unsigned int count,\n\t\tu32 flags, void *data)\n{\n\tstruct eventfd_ctx *trigger;\n\n\tif (flags & VFIO_IRQ_SET_DATA_EVENTFD) {\n\t\tint fd = *(int *)data;\n\n\t\ttrigger = eventfd_ctx_fdget(fd);\n\t\tif (IS_ERR(trigger)) {\n\t\t\tgvt_vgpu_err(\"eventfd_ctx_fdget failed\\n\");\n\t\t\treturn PTR_ERR(trigger);\n\t\t}\n\t\tvgpu->msi_trigger = trigger;\n\t} else if ((flags & VFIO_IRQ_SET_DATA_NONE) && !count)\n\t\tintel_vgpu_release_msi_eventfd_ctx(vgpu);\n\n\treturn 0;\n}\n\nstatic int intel_vgpu_set_irqs(struct intel_vgpu *vgpu, u32 flags,\n\t\tunsigned int index, unsigned int start, unsigned int count,\n\t\tvoid *data)\n{\n\tint (*func)(struct intel_vgpu *vgpu, unsigned int index,\n\t\t\tunsigned int start, unsigned int count, u32 flags,\n\t\t\tvoid *data) = NULL;\n\n\tswitch (index) {\n\tcase VFIO_PCI_INTX_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\t\tfunc = intel_vgpu_set_intx_mask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\tfunc = intel_vgpu_set_intx_unmask;\n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = intel_vgpu_set_intx_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase VFIO_PCI_MSI_IRQ_INDEX:\n\t\tswitch (flags & VFIO_IRQ_SET_ACTION_TYPE_MASK) {\n\t\tcase VFIO_IRQ_SET_ACTION_MASK:\n\t\tcase VFIO_IRQ_SET_ACTION_UNMASK:\n\t\t\t \n\t\t\tbreak;\n\t\tcase VFIO_IRQ_SET_ACTION_TRIGGER:\n\t\t\tfunc = intel_vgpu_set_msi_trigger;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (!func)\n\t\treturn -ENOTTY;\n\n\treturn func(vgpu, index, start, count, flags, data);\n}\n\nstatic long intel_vgpu_ioctl(struct vfio_device *vfio_dev, unsigned int cmd,\n\t\t\t     unsigned long arg)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tunsigned long minsz;\n\n\tgvt_dbg_core(\"vgpu%d ioctl, cmd: %d\\n\", vgpu->id, cmd);\n\n\tif (cmd == VFIO_DEVICE_GET_INFO) {\n\t\tstruct vfio_device_info info;\n\n\t\tminsz = offsetofend(struct vfio_device_info, num_irqs);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tinfo.flags = VFIO_DEVICE_FLAGS_PCI;\n\t\tinfo.flags |= VFIO_DEVICE_FLAGS_RESET;\n\t\tinfo.num_regions = VFIO_PCI_NUM_REGIONS +\n\t\t\t\tvgpu->num_regions;\n\t\tinfo.num_irqs = VFIO_PCI_NUM_IRQS;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\n\t} else if (cmd == VFIO_DEVICE_GET_REGION_INFO) {\n\t\tstruct vfio_region_info info;\n\t\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\t\tunsigned int i;\n\t\tint ret;\n\t\tstruct vfio_region_info_cap_sparse_mmap *sparse = NULL;\n\t\tint nr_areas = 1;\n\t\tint cap_type_id;\n\n\t\tminsz = offsetofend(struct vfio_region_info, offset);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = vgpu->gvt->device_info.cfg_space_size;\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tbreak;\n\t\tcase VFIO_PCI_BAR0_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = vgpu->cfg_space.bar[info.index].size;\n\t\t\tif (!info.size) {\n\t\t\t\tinfo.flags = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tbreak;\n\t\tcase VFIO_PCI_BAR1_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = 0;\n\t\t\tinfo.flags = 0;\n\t\t\tbreak;\n\t\tcase VFIO_PCI_BAR2_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_CAPS |\n\t\t\t\t\tVFIO_REGION_INFO_FLAG_MMAP |\n\t\t\t\t\tVFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t\tVFIO_REGION_INFO_FLAG_WRITE;\n\t\t\tinfo.size = gvt_aperture_sz(vgpu->gvt);\n\n\t\t\tsparse = kzalloc(struct_size(sparse, areas, nr_areas),\n\t\t\t\t\t GFP_KERNEL);\n\t\t\tif (!sparse)\n\t\t\t\treturn -ENOMEM;\n\n\t\t\tsparse->header.id = VFIO_REGION_INFO_CAP_SPARSE_MMAP;\n\t\t\tsparse->header.version = 1;\n\t\t\tsparse->nr_areas = nr_areas;\n\t\t\tcap_type_id = VFIO_REGION_INFO_CAP_SPARSE_MMAP;\n\t\t\tsparse->areas[0].offset =\n\t\t\t\t\tPAGE_ALIGN(vgpu_aperture_offset(vgpu));\n\t\t\tsparse->areas[0].size = vgpu_aperture_sz(vgpu);\n\t\t\tbreak;\n\n\t\tcase VFIO_PCI_BAR3_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = 0;\n\t\t\tinfo.flags = 0;\n\n\t\t\tgvt_dbg_core(\"get region info bar:%d\\n\", info.index);\n\t\t\tbreak;\n\n\t\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\tinfo.size = 0;\n\t\t\tinfo.flags = 0;\n\n\t\t\tgvt_dbg_core(\"get region info index:%d\\n\", info.index);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t{\n\t\t\t\tstruct vfio_region_info_cap_type cap_type = {\n\t\t\t\t\t.header.id = VFIO_REGION_INFO_CAP_TYPE,\n\t\t\t\t\t.header.version = 1 };\n\n\t\t\t\tif (info.index >= VFIO_PCI_NUM_REGIONS +\n\t\t\t\t\t\tvgpu->num_regions)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\tinfo.index =\n\t\t\t\t\tarray_index_nospec(info.index,\n\t\t\t\t\t\t\tVFIO_PCI_NUM_REGIONS +\n\t\t\t\t\t\t\tvgpu->num_regions);\n\n\t\t\t\ti = info.index - VFIO_PCI_NUM_REGIONS;\n\n\t\t\t\tinfo.offset =\n\t\t\t\t\tVFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\t\t\tinfo.size = vgpu->region[i].size;\n\t\t\t\tinfo.flags = vgpu->region[i].flags;\n\n\t\t\t\tcap_type.type = vgpu->region[i].type;\n\t\t\t\tcap_type.subtype = vgpu->region[i].subtype;\n\n\t\t\t\tret = vfio_info_add_capability(&caps,\n\t\t\t\t\t\t\t&cap_type.header,\n\t\t\t\t\t\t\tsizeof(cap_type));\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tif ((info.flags & VFIO_REGION_INFO_FLAG_CAPS) && sparse) {\n\t\t\tswitch (cap_type_id) {\n\t\t\tcase VFIO_REGION_INFO_CAP_SPARSE_MMAP:\n\t\t\t\tret = vfio_info_add_capability(&caps,\n\t\t\t\t\t&sparse->header,\n\t\t\t\t\tstruct_size(sparse, areas,\n\t\t\t\t\t\t    sparse->nr_areas));\n\t\t\t\tif (ret) {\n\t\t\t\t\tkfree(sparse);\n\t\t\t\t\treturn ret;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tkfree(sparse);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tif (caps.size) {\n\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_CAPS;\n\t\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t\t\tinfo.cap_offset = 0;\n\t\t\t} else {\n\t\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\t\tif (copy_to_user((void __user *)arg +\n\t\t\t\t\t\t  sizeof(info), caps.buf,\n\t\t\t\t\t\t  caps.size)) {\n\t\t\t\t\tkfree(caps.buf);\n\t\t\t\t\tkfree(sparse);\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tinfo.cap_offset = sizeof(info);\n\t\t\t}\n\n\t\t\tkfree(caps.buf);\n\t\t}\n\n\t\tkfree(sparse);\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\t} else if (cmd == VFIO_DEVICE_GET_IRQ_INFO) {\n\t\tstruct vfio_irq_info info;\n\n\t\tminsz = offsetofend(struct vfio_irq_info, count);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (info.index) {\n\t\tcase VFIO_PCI_INTX_IRQ_INDEX:\n\t\tcase VFIO_PCI_MSI_IRQ_INDEX:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinfo.flags = VFIO_IRQ_INFO_EVENTFD;\n\n\t\tinfo.count = intel_vgpu_get_irq_count(vgpu, info.index);\n\n\t\tif (info.index == VFIO_PCI_INTX_IRQ_INDEX)\n\t\t\tinfo.flags |= (VFIO_IRQ_INFO_MASKABLE |\n\t\t\t\t       VFIO_IRQ_INFO_AUTOMASKED);\n\t\telse\n\t\t\tinfo.flags |= VFIO_IRQ_INFO_NORESIZE;\n\n\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n\t} else if (cmd == VFIO_DEVICE_SET_IRQS) {\n\t\tstruct vfio_irq_set hdr;\n\t\tu8 *data = NULL;\n\t\tint ret = 0;\n\t\tsize_t data_size = 0;\n\n\t\tminsz = offsetofend(struct vfio_irq_set, count);\n\n\t\tif (copy_from_user(&hdr, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (!(hdr.flags & VFIO_IRQ_SET_DATA_NONE)) {\n\t\t\tint max = intel_vgpu_get_irq_count(vgpu, hdr.index);\n\n\t\t\tret = vfio_set_irqs_validate_and_prepare(&hdr, max,\n\t\t\t\t\t\tVFIO_PCI_NUM_IRQS, &data_size);\n\t\t\tif (ret) {\n\t\t\t\tgvt_vgpu_err(\"intel:vfio_set_irqs_validate_and_prepare failed\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (data_size) {\n\t\t\t\tdata = memdup_user((void __user *)(arg + minsz),\n\t\t\t\t\t\t   data_size);\n\t\t\t\tif (IS_ERR(data))\n\t\t\t\t\treturn PTR_ERR(data);\n\t\t\t}\n\t\t}\n\n\t\tret = intel_vgpu_set_irqs(vgpu, hdr.flags, hdr.index,\n\t\t\t\t\thdr.start, hdr.count, data);\n\t\tkfree(data);\n\n\t\treturn ret;\n\t} else if (cmd == VFIO_DEVICE_RESET) {\n\t\tintel_gvt_reset_vgpu(vgpu);\n\t\treturn 0;\n\t} else if (cmd == VFIO_DEVICE_QUERY_GFX_PLANE) {\n\t\tstruct vfio_device_gfx_plane_info dmabuf;\n\t\tint ret = 0;\n\n\t\tminsz = offsetofend(struct vfio_device_gfx_plane_info,\n\t\t\t\t    dmabuf_id);\n\t\tif (copy_from_user(&dmabuf, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\t\tif (dmabuf.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tret = intel_vgpu_query_plane(vgpu, &dmabuf);\n\t\tif (ret != 0)\n\t\t\treturn ret;\n\n\t\treturn copy_to_user((void __user *)arg, &dmabuf, minsz) ?\n\t\t\t\t\t\t\t\t-EFAULT : 0;\n\t} else if (cmd == VFIO_DEVICE_GET_GFX_DMABUF) {\n\t\t__u32 dmabuf_id;\n\n\t\tif (get_user(dmabuf_id, (__u32 __user *)arg))\n\t\t\treturn -EFAULT;\n\t\treturn intel_vgpu_get_dmabuf(vgpu, dmabuf_id);\n\t}\n\n\treturn -ENOTTY;\n}\n\nstatic ssize_t\nvgpu_id_show(struct device *dev, struct device_attribute *attr,\n\t     char *buf)\n{\n\tstruct intel_vgpu *vgpu = dev_get_drvdata(dev);\n\n\treturn sprintf(buf, \"%d\\n\", vgpu->id);\n}\n\nstatic DEVICE_ATTR_RO(vgpu_id);\n\nstatic struct attribute *intel_vgpu_attrs[] = {\n\t&dev_attr_vgpu_id.attr,\n\tNULL\n};\n\nstatic const struct attribute_group intel_vgpu_group = {\n\t.name = \"intel_vgpu\",\n\t.attrs = intel_vgpu_attrs,\n};\n\nstatic const struct attribute_group *intel_vgpu_groups[] = {\n\t&intel_vgpu_group,\n\tNULL,\n};\n\nstatic int intel_vgpu_init_dev(struct vfio_device *vfio_dev)\n{\n\tstruct mdev_device *mdev = to_mdev_device(vfio_dev->dev);\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\tstruct intel_vgpu_type *type =\n\t\tcontainer_of(mdev->type, struct intel_vgpu_type, type);\n\tint ret;\n\n\tvgpu->gvt = kdev_to_i915(mdev->type->parent->dev)->gvt;\n\tret = intel_gvt_create_vgpu(vgpu, type->conf);\n\tif (ret)\n\t\treturn ret;\n\n\tkvmgt_protect_table_init(vgpu);\n\tgvt_cache_init(vgpu);\n\n\treturn 0;\n}\n\nstatic void intel_vgpu_release_dev(struct vfio_device *vfio_dev)\n{\n\tstruct intel_vgpu *vgpu = vfio_dev_to_vgpu(vfio_dev);\n\n\tintel_gvt_destroy_vgpu(vgpu);\n}\n\nstatic const struct vfio_device_ops intel_vgpu_dev_ops = {\n\t.init\t\t= intel_vgpu_init_dev,\n\t.release\t= intel_vgpu_release_dev,\n\t.open_device\t= intel_vgpu_open_device,\n\t.close_device\t= intel_vgpu_close_device,\n\t.read\t\t= intel_vgpu_read,\n\t.write\t\t= intel_vgpu_write,\n\t.mmap\t\t= intel_vgpu_mmap,\n\t.ioctl\t\t= intel_vgpu_ioctl,\n\t.dma_unmap\t= intel_vgpu_dma_unmap,\n\t.bind_iommufd\t= vfio_iommufd_emulated_bind,\n\t.unbind_iommufd = vfio_iommufd_emulated_unbind,\n\t.attach_ioas\t= vfio_iommufd_emulated_attach_ioas,\n\t.detach_ioas\t= vfio_iommufd_emulated_detach_ioas,\n};\n\nstatic int intel_vgpu_probe(struct mdev_device *mdev)\n{\n\tstruct intel_vgpu *vgpu;\n\tint ret;\n\n\tvgpu = vfio_alloc_device(intel_vgpu, vfio_device, &mdev->dev,\n\t\t\t\t &intel_vgpu_dev_ops);\n\tif (IS_ERR(vgpu)) {\n\t\tgvt_err(\"failed to create intel vgpu: %ld\\n\", PTR_ERR(vgpu));\n\t\treturn PTR_ERR(vgpu);\n\t}\n\n\tdev_set_drvdata(&mdev->dev, vgpu);\n\tret = vfio_register_emulated_iommu_dev(&vgpu->vfio_device);\n\tif (ret)\n\t\tgoto out_put_vdev;\n\n\tgvt_dbg_core(\"intel_vgpu_create succeeded for mdev: %s\\n\",\n\t\t     dev_name(mdev_dev(mdev)));\n\treturn 0;\n\nout_put_vdev:\n\tvfio_put_device(&vgpu->vfio_device);\n\treturn ret;\n}\n\nstatic void intel_vgpu_remove(struct mdev_device *mdev)\n{\n\tstruct intel_vgpu *vgpu = dev_get_drvdata(&mdev->dev);\n\n\tvfio_unregister_group_dev(&vgpu->vfio_device);\n\tvfio_put_device(&vgpu->vfio_device);\n}\n\nstatic unsigned int intel_vgpu_get_available(struct mdev_type *mtype)\n{\n\tstruct intel_vgpu_type *type =\n\t\tcontainer_of(mtype, struct intel_vgpu_type, type);\n\tstruct intel_gvt *gvt = kdev_to_i915(mtype->parent->dev)->gvt;\n\tunsigned int low_gm_avail, high_gm_avail, fence_avail;\n\n\tmutex_lock(&gvt->lock);\n\tlow_gm_avail = gvt_aperture_sz(gvt) - HOST_LOW_GM_SIZE -\n\t\tgvt->gm.vgpu_allocated_low_gm_size;\n\thigh_gm_avail = gvt_hidden_sz(gvt) - HOST_HIGH_GM_SIZE -\n\t\tgvt->gm.vgpu_allocated_high_gm_size;\n\tfence_avail = gvt_fence_sz(gvt) - HOST_FENCE -\n\t\tgvt->fence.vgpu_allocated_fence_num;\n\tmutex_unlock(&gvt->lock);\n\n\treturn min3(low_gm_avail / type->conf->low_mm,\n\t\t    high_gm_avail / type->conf->high_mm,\n\t\t    fence_avail / type->conf->fence);\n}\n\nstatic struct mdev_driver intel_vgpu_mdev_driver = {\n\t.device_api\t= VFIO_DEVICE_API_PCI_STRING,\n\t.driver = {\n\t\t.name\t\t= \"intel_vgpu_mdev\",\n\t\t.owner\t\t= THIS_MODULE,\n\t\t.dev_groups\t= intel_vgpu_groups,\n\t},\n\t.probe\t\t\t= intel_vgpu_probe,\n\t.remove\t\t\t= intel_vgpu_remove,\n\t.get_available\t\t= intel_vgpu_get_available,\n\t.show_description\t= intel_vgpu_show_description,\n};\n\nint intel_gvt_page_track_add(struct intel_vgpu *info, u64 gfn)\n{\n\tint r;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, info->status))\n\t\treturn -ESRCH;\n\n\tif (kvmgt_gfn_is_write_protected(info, gfn))\n\t\treturn 0;\n\n\tr = kvm_write_track_add_gfn(info->vfio_device.kvm, gfn);\n\tif (r)\n\t\treturn r;\n\n\tkvmgt_protect_table_add(info, gfn);\n\treturn 0;\n}\n\nint intel_gvt_page_track_remove(struct intel_vgpu *info, u64 gfn)\n{\n\tint r;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, info->status))\n\t\treturn -ESRCH;\n\n\tif (!kvmgt_gfn_is_write_protected(info, gfn))\n\t\treturn 0;\n\n\tr = kvm_write_track_remove_gfn(info->vfio_device.kvm, gfn);\n\tif (r)\n\t\treturn r;\n\n\tkvmgt_protect_table_del(info, gfn);\n\treturn 0;\n}\n\nstatic void kvmgt_page_track_write(gpa_t gpa, const u8 *val, int len,\n\t\t\t\t   struct kvm_page_track_notifier_node *node)\n{\n\tstruct intel_vgpu *info =\n\t\tcontainer_of(node, struct intel_vgpu, track_node);\n\n\tmutex_lock(&info->vgpu_lock);\n\n\tif (kvmgt_gfn_is_write_protected(info, gpa >> PAGE_SHIFT))\n\t\tintel_vgpu_page_track_handler(info, gpa,\n\t\t\t\t\t\t     (void *)val, len);\n\n\tmutex_unlock(&info->vgpu_lock);\n}\n\nstatic void kvmgt_page_track_remove_region(gfn_t gfn, unsigned long nr_pages,\n\t\t\t\t\t   struct kvm_page_track_notifier_node *node)\n{\n\tunsigned long i;\n\tstruct intel_vgpu *info =\n\t\tcontainer_of(node, struct intel_vgpu, track_node);\n\n\tmutex_lock(&info->vgpu_lock);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tif (kvmgt_gfn_is_write_protected(info, gfn + i))\n\t\t\tkvmgt_protect_table_del(info, gfn + i);\n\t}\n\n\tmutex_unlock(&info->vgpu_lock);\n}\n\nvoid intel_vgpu_detach_regions(struct intel_vgpu *vgpu)\n{\n\tint i;\n\n\tif (!vgpu->region)\n\t\treturn;\n\n\tfor (i = 0; i < vgpu->num_regions; i++)\n\t\tif (vgpu->region[i].ops->release)\n\t\t\tvgpu->region[i].ops->release(vgpu,\n\t\t\t\t\t&vgpu->region[i]);\n\tvgpu->num_regions = 0;\n\tkfree(vgpu->region);\n\tvgpu->region = NULL;\n}\n\nint intel_gvt_dma_map_guest_page(struct intel_vgpu *vgpu, unsigned long gfn,\n\t\tunsigned long size, dma_addr_t *dma_addr)\n{\n\tstruct gvt_dma *entry;\n\tint ret;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&vgpu->cache_lock);\n\n\tentry = __gvt_cache_find_gfn(vgpu, gfn);\n\tif (!entry) {\n\t\tret = gvt_dma_map_page(vgpu, gfn, dma_addr, size);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\n\t\tret = __gvt_cache_add(vgpu, gfn, *dma_addr, size);\n\t\tif (ret)\n\t\t\tgoto err_unmap;\n\t} else if (entry->size != size) {\n\t\t \n\t\tgvt_dma_unmap_page(vgpu, gfn, entry->dma_addr, entry->size);\n\t\t__gvt_cache_remove_entry(vgpu, entry);\n\n\t\tret = gvt_dma_map_page(vgpu, gfn, dma_addr, size);\n\t\tif (ret)\n\t\t\tgoto err_unlock;\n\n\t\tret = __gvt_cache_add(vgpu, gfn, *dma_addr, size);\n\t\tif (ret)\n\t\t\tgoto err_unmap;\n\t} else {\n\t\tkref_get(&entry->ref);\n\t\t*dma_addr = entry->dma_addr;\n\t}\n\n\tmutex_unlock(&vgpu->cache_lock);\n\treturn 0;\n\nerr_unmap:\n\tgvt_dma_unmap_page(vgpu, gfn, *dma_addr, size);\nerr_unlock:\n\tmutex_unlock(&vgpu->cache_lock);\n\treturn ret;\n}\n\nint intel_gvt_dma_pin_guest_page(struct intel_vgpu *vgpu, dma_addr_t dma_addr)\n{\n\tstruct gvt_dma *entry;\n\tint ret = 0;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&vgpu->cache_lock);\n\tentry = __gvt_cache_find_dma_addr(vgpu, dma_addr);\n\tif (entry)\n\t\tkref_get(&entry->ref);\n\telse\n\t\tret = -ENOMEM;\n\tmutex_unlock(&vgpu->cache_lock);\n\n\treturn ret;\n}\n\nstatic void __gvt_dma_release(struct kref *ref)\n{\n\tstruct gvt_dma *entry = container_of(ref, typeof(*entry), ref);\n\n\tgvt_dma_unmap_page(entry->vgpu, entry->gfn, entry->dma_addr,\n\t\t\t   entry->size);\n\t__gvt_cache_remove_entry(entry->vgpu, entry);\n}\n\nvoid intel_gvt_dma_unmap_guest_page(struct intel_vgpu *vgpu,\n\t\tdma_addr_t dma_addr)\n{\n\tstruct gvt_dma *entry;\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ATTACHED, vgpu->status))\n\t\treturn;\n\n\tmutex_lock(&vgpu->cache_lock);\n\tentry = __gvt_cache_find_dma_addr(vgpu, dma_addr);\n\tif (entry)\n\t\tkref_put(&entry->ref, __gvt_dma_release);\n\tmutex_unlock(&vgpu->cache_lock);\n}\n\nstatic void init_device_info(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_device_info *info = &gvt->device_info;\n\tstruct pci_dev *pdev = to_pci_dev(gvt->gt->i915->drm.dev);\n\n\tinfo->max_support_vgpus = 8;\n\tinfo->cfg_space_size = PCI_CFG_SPACE_EXP_SIZE;\n\tinfo->mmio_size = 2 * 1024 * 1024;\n\tinfo->mmio_bar = 0;\n\tinfo->gtt_start_offset = 8 * 1024 * 1024;\n\tinfo->gtt_entry_size = 8;\n\tinfo->gtt_entry_size_shift = 3;\n\tinfo->gmadr_bytes_in_cmd = 8;\n\tinfo->max_surface_size = 36 * 1024 * 1024;\n\tinfo->msi_cap_offset = pdev->msi_cap;\n}\n\nstatic void intel_gvt_test_and_emulate_vblank(struct intel_gvt *gvt)\n{\n\tstruct intel_vgpu *vgpu;\n\tint id;\n\n\tmutex_lock(&gvt->lock);\n\tidr_for_each_entry((&(gvt)->vgpu_idr), (vgpu), (id)) {\n\t\tif (test_and_clear_bit(INTEL_GVT_REQUEST_EMULATE_VBLANK + id,\n\t\t\t\t       (void *)&gvt->service_request)) {\n\t\t\tif (test_bit(INTEL_VGPU_STATUS_ACTIVE, vgpu->status))\n\t\t\t\tintel_vgpu_emulate_vblank(vgpu);\n\t\t}\n\t}\n\tmutex_unlock(&gvt->lock);\n}\n\nstatic int gvt_service_thread(void *data)\n{\n\tstruct intel_gvt *gvt = (struct intel_gvt *)data;\n\tint ret;\n\n\tgvt_dbg_core(\"service thread start\\n\");\n\n\twhile (!kthread_should_stop()) {\n\t\tret = wait_event_interruptible(gvt->service_thread_wq,\n\t\t\t\tkthread_should_stop() || gvt->service_request);\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (WARN_ONCE(ret, \"service thread is waken up by signal.\\n\"))\n\t\t\tcontinue;\n\n\t\tintel_gvt_test_and_emulate_vblank(gvt);\n\n\t\tif (test_bit(INTEL_GVT_REQUEST_SCHED,\n\t\t\t\t(void *)&gvt->service_request) ||\n\t\t\ttest_bit(INTEL_GVT_REQUEST_EVENT_SCHED,\n\t\t\t\t\t(void *)&gvt->service_request)) {\n\t\t\tintel_gvt_schedule(gvt);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void clean_service_thread(struct intel_gvt *gvt)\n{\n\tkthread_stop(gvt->service_thread);\n}\n\nstatic int init_service_thread(struct intel_gvt *gvt)\n{\n\tinit_waitqueue_head(&gvt->service_thread_wq);\n\n\tgvt->service_thread = kthread_run(gvt_service_thread,\n\t\t\tgvt, \"gvt_service_thread\");\n\tif (IS_ERR(gvt->service_thread)) {\n\t\tgvt_err(\"fail to start service thread.\\n\");\n\t\treturn PTR_ERR(gvt->service_thread);\n\t}\n\treturn 0;\n}\n\n \nstatic void intel_gvt_clean_device(struct drm_i915_private *i915)\n{\n\tstruct intel_gvt *gvt = fetch_and_zero(&i915->gvt);\n\n\tif (drm_WARN_ON(&i915->drm, !gvt))\n\t\treturn;\n\n\tmdev_unregister_parent(&gvt->parent);\n\tintel_gvt_destroy_idle_vgpu(gvt->idle_vgpu);\n\tintel_gvt_clean_vgpu_types(gvt);\n\n\tintel_gvt_debugfs_clean(gvt);\n\tclean_service_thread(gvt);\n\tintel_gvt_clean_cmd_parser(gvt);\n\tintel_gvt_clean_sched_policy(gvt);\n\tintel_gvt_clean_workload_scheduler(gvt);\n\tintel_gvt_clean_gtt(gvt);\n\tintel_gvt_free_firmware(gvt);\n\tintel_gvt_clean_mmio_info(gvt);\n\tidr_destroy(&gvt->vgpu_idr);\n\n\tkfree(i915->gvt);\n}\n\n \nstatic int intel_gvt_init_device(struct drm_i915_private *i915)\n{\n\tstruct intel_gvt *gvt;\n\tstruct intel_vgpu *vgpu;\n\tint ret;\n\n\tif (drm_WARN_ON(&i915->drm, i915->gvt))\n\t\treturn -EEXIST;\n\n\tgvt = kzalloc(sizeof(struct intel_gvt), GFP_KERNEL);\n\tif (!gvt)\n\t\treturn -ENOMEM;\n\n\tgvt_dbg_core(\"init gvt device\\n\");\n\n\tidr_init_base(&gvt->vgpu_idr, 1);\n\tspin_lock_init(&gvt->scheduler.mmio_context_lock);\n\tmutex_init(&gvt->lock);\n\tmutex_init(&gvt->sched_lock);\n\tgvt->gt = to_gt(i915);\n\ti915->gvt = gvt;\n\n\tinit_device_info(gvt);\n\n\tret = intel_gvt_setup_mmio_info(gvt);\n\tif (ret)\n\t\tgoto out_clean_idr;\n\n\tintel_gvt_init_engine_mmio_context(gvt);\n\n\tret = intel_gvt_load_firmware(gvt);\n\tif (ret)\n\t\tgoto out_clean_mmio_info;\n\n\tret = intel_gvt_init_irq(gvt);\n\tif (ret)\n\t\tgoto out_free_firmware;\n\n\tret = intel_gvt_init_gtt(gvt);\n\tif (ret)\n\t\tgoto out_free_firmware;\n\n\tret = intel_gvt_init_workload_scheduler(gvt);\n\tif (ret)\n\t\tgoto out_clean_gtt;\n\n\tret = intel_gvt_init_sched_policy(gvt);\n\tif (ret)\n\t\tgoto out_clean_workload_scheduler;\n\n\tret = intel_gvt_init_cmd_parser(gvt);\n\tif (ret)\n\t\tgoto out_clean_sched_policy;\n\n\tret = init_service_thread(gvt);\n\tif (ret)\n\t\tgoto out_clean_cmd_parser;\n\n\tret = intel_gvt_init_vgpu_types(gvt);\n\tif (ret)\n\t\tgoto out_clean_thread;\n\n\tvgpu = intel_gvt_create_idle_vgpu(gvt);\n\tif (IS_ERR(vgpu)) {\n\t\tret = PTR_ERR(vgpu);\n\t\tgvt_err(\"failed to create idle vgpu\\n\");\n\t\tgoto out_clean_types;\n\t}\n\tgvt->idle_vgpu = vgpu;\n\n\tintel_gvt_debugfs_init(gvt);\n\n\tret = mdev_register_parent(&gvt->parent, i915->drm.dev,\n\t\t\t\t   &intel_vgpu_mdev_driver,\n\t\t\t\t   gvt->mdev_types, gvt->num_types);\n\tif (ret)\n\t\tgoto out_destroy_idle_vgpu;\n\n\tgvt_dbg_core(\"gvt device initialization is done\\n\");\n\treturn 0;\n\nout_destroy_idle_vgpu:\n\tintel_gvt_destroy_idle_vgpu(gvt->idle_vgpu);\n\tintel_gvt_debugfs_clean(gvt);\nout_clean_types:\n\tintel_gvt_clean_vgpu_types(gvt);\nout_clean_thread:\n\tclean_service_thread(gvt);\nout_clean_cmd_parser:\n\tintel_gvt_clean_cmd_parser(gvt);\nout_clean_sched_policy:\n\tintel_gvt_clean_sched_policy(gvt);\nout_clean_workload_scheduler:\n\tintel_gvt_clean_workload_scheduler(gvt);\nout_clean_gtt:\n\tintel_gvt_clean_gtt(gvt);\nout_free_firmware:\n\tintel_gvt_free_firmware(gvt);\nout_clean_mmio_info:\n\tintel_gvt_clean_mmio_info(gvt);\nout_clean_idr:\n\tidr_destroy(&gvt->vgpu_idr);\n\tkfree(gvt);\n\ti915->gvt = NULL;\n\treturn ret;\n}\n\nstatic void intel_gvt_pm_resume(struct drm_i915_private *i915)\n{\n\tstruct intel_gvt *gvt = i915->gvt;\n\n\tintel_gvt_restore_fence(gvt);\n\tintel_gvt_restore_mmio(gvt);\n\tintel_gvt_restore_ggtt(gvt);\n}\n\nstatic const struct intel_vgpu_ops intel_gvt_vgpu_ops = {\n\t.init_device\t= intel_gvt_init_device,\n\t.clean_device\t= intel_gvt_clean_device,\n\t.pm_resume\t= intel_gvt_pm_resume,\n};\n\nstatic int __init kvmgt_init(void)\n{\n\tint ret;\n\n\tret = intel_gvt_set_ops(&intel_gvt_vgpu_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mdev_register_driver(&intel_vgpu_mdev_driver);\n\tif (ret)\n\t\tintel_gvt_clear_ops(&intel_gvt_vgpu_ops);\n\treturn ret;\n}\n\nstatic void __exit kvmgt_exit(void)\n{\n\tmdev_unregister_driver(&intel_vgpu_mdev_driver);\n\tintel_gvt_clear_ops(&intel_gvt_vgpu_ops);\n}\n\nmodule_init(kvmgt_init);\nmodule_exit(kvmgt_exit);\n\nMODULE_LICENSE(\"GPL and additional rights\");\nMODULE_AUTHOR(\"Intel Corporation\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}