{
  "module_name": "scheduler.h",
  "hash_id": "c05be9253aa64d9e447a03166743efad6ba92fbc81c52ed736b265ab914e2752",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/scheduler.h",
  "human_readable_source": " \n\n#ifndef _GVT_SCHEDULER_H_\n#define _GVT_SCHEDULER_H_\n\n#include \"gt/intel_engine_types.h\"\n\n#include \"execlist.h\"\n#include \"interrupt.h\"\n\nstruct intel_gvt_workload_scheduler {\n\tstruct intel_vgpu *current_vgpu;\n\tstruct intel_vgpu *next_vgpu;\n\tstruct intel_vgpu_workload *current_workload[I915_NUM_ENGINES];\n\tbool need_reschedule;\n\n\tspinlock_t mmio_context_lock;\n\t \n\tstruct intel_vgpu *engine_owner[I915_NUM_ENGINES];\n\n\twait_queue_head_t workload_complete_wq;\n\tstruct task_struct *thread[I915_NUM_ENGINES];\n\twait_queue_head_t waitq[I915_NUM_ENGINES];\n\n\tvoid *sched_data;\n\tconst struct intel_gvt_sched_policy_ops *sched_ops;\n};\n\n#define INDIRECT_CTX_ADDR_MASK 0xffffffc0\n#define INDIRECT_CTX_SIZE_MASK 0x3f\nstruct shadow_indirect_ctx {\n\tstruct drm_i915_gem_object *obj;\n\tunsigned long guest_gma;\n\tunsigned long shadow_gma;\n\tvoid *shadow_va;\n\tu32 size;\n};\n\n#define PER_CTX_ADDR_MASK 0xfffff000\nstruct shadow_per_ctx {\n\tunsigned long guest_gma;\n\tunsigned long shadow_gma;\n\tunsigned valid;\n};\n\nstruct intel_shadow_wa_ctx {\n\tstruct shadow_indirect_ctx indirect_ctx;\n\tstruct shadow_per_ctx per_ctx;\n\n};\n\nstruct intel_vgpu_workload {\n\tstruct intel_vgpu *vgpu;\n\tconst struct intel_engine_cs *engine;\n\tstruct i915_request *req;\n\t \n\tbool dispatched;\n\tbool shadow;       \n\tint status;\n\n\tstruct intel_vgpu_mm *shadow_mm;\n\tstruct list_head lri_shadow_mm;  \n\n\t \n\tint (*prepare)(struct intel_vgpu_workload *);\n\tint (*complete)(struct intel_vgpu_workload *);\n\tstruct list_head list;\n\n\tDECLARE_BITMAP(pending_events, INTEL_GVT_EVENT_MAX);\n\tvoid *shadow_ring_buffer_va;\n\n\t \n\tstruct execlist_ctx_descriptor_format ctx_desc;\n\tstruct execlist_ring_context *ring_context;\n\tunsigned long rb_head, rb_tail, rb_ctl, rb_start, rb_len;\n\tunsigned long guest_rb_head;\n\tbool restore_inhibit;\n\tstruct intel_vgpu_elsp_dwords elsp_dwords;\n\tbool emulate_schedule_in;\n\tatomic_t shadow_ctx_active;\n\twait_queue_head_t shadow_ctx_status_wq;\n\tu64 ring_context_gpa;\n\n\t \n\tstruct list_head shadow_bb;\n\tstruct intel_shadow_wa_ctx wa_ctx;\n\n\t \n\tu32 oactxctrl;\n\tu32 flex_mmio[7];\n};\n\nstruct intel_vgpu_shadow_bb {\n\tstruct list_head list;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tvoid *va;\n\tu32 *bb_start_cmd_va;\n\tunsigned long bb_offset;\n\tbool ppgtt;\n};\n\n#define workload_q_head(vgpu, e) \\\n\t(&(vgpu)->submission.workload_q_head[(e)->id])\n\nvoid intel_vgpu_queue_workload(struct intel_vgpu_workload *workload);\n\nint intel_gvt_init_workload_scheduler(struct intel_gvt *gvt);\n\nvoid intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt);\n\nvoid intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu);\n\nint intel_vgpu_setup_submission(struct intel_vgpu *vgpu);\n\nvoid intel_vgpu_reset_submission(struct intel_vgpu *vgpu,\n\t\t\t\t intel_engine_mask_t engine_mask);\n\nvoid intel_vgpu_clean_submission(struct intel_vgpu *vgpu);\n\nint intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,\n\t\t\t\t     intel_engine_mask_t engine_mask,\n\t\t\t\t     unsigned int interface);\n\nextern const struct intel_vgpu_submission_ops\nintel_vgpu_execlist_submission_ops;\n\nstruct intel_vgpu_workload *\nintel_vgpu_create_workload(struct intel_vgpu *vgpu,\n\t\t\t   const struct intel_engine_cs *engine,\n\t\t\t   struct execlist_ctx_descriptor_format *desc);\n\nvoid intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload);\n\nvoid intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,\n\t\t\t\tintel_engine_mask_t engine_mask);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}