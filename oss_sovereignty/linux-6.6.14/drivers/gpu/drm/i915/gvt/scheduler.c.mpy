{
  "module_name": "scheduler.c",
  "hash_id": "db1924406c268705e21081b3c5b5854a394dde920edf2a307202ff10773ec918",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gvt/scheduler.c",
  "human_readable_source": " \n\n#include <linux/kthread.h>\n\n#include \"gem/i915_gem_pm.h\"\n#include \"gt/intel_context.h\"\n#include \"gt/intel_execlists_submission.h\"\n#include \"gt/intel_gt_regs.h\"\n#include \"gt/intel_lrc.h\"\n#include \"gt/intel_ring.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_gem_gtt.h\"\n#include \"i915_perf_oa_regs.h\"\n#include \"gvt.h\"\n\n#define RING_CTX_OFF(x) \\\n\toffsetof(struct execlist_ring_context, x)\n\nstatic void set_context_pdp_root_pointer(\n\t\tstruct execlist_ring_context *ring_context,\n\t\tu32 pdp[8])\n{\n\tint i;\n\n\tfor (i = 0; i < 8; i++)\n\t\tring_context->pdps[i].val = pdp[7 - i];\n}\n\nstatic void update_shadow_pdps(struct intel_vgpu_workload *workload)\n{\n\tstruct execlist_ring_context *shadow_ring_context;\n\tstruct intel_context *ctx = workload->req->context;\n\n\tif (WARN_ON(!workload->shadow_mm))\n\t\treturn;\n\n\tif (WARN_ON(!atomic_read(&workload->shadow_mm->pincount)))\n\t\treturn;\n\n\tshadow_ring_context = (struct execlist_ring_context *)ctx->lrc_reg_state;\n\tset_context_pdp_root_pointer(shadow_ring_context,\n\t\t\t(void *)workload->shadow_mm->ppgtt_mm.shadow_pdps);\n}\n\n \nstatic void sr_oa_regs(struct intel_vgpu_workload *workload,\n\t\tu32 *reg_state, bool save)\n{\n\tstruct drm_i915_private *dev_priv = workload->vgpu->gvt->gt->i915;\n\tu32 ctx_oactxctrl = dev_priv->perf.ctx_oactxctrl_offset;\n\tu32 ctx_flexeu0 = dev_priv->perf.ctx_flexeu0_offset;\n\tint i = 0;\n\tu32 flex_mmio[] = {\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL0),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL1),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL2),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL3),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL4),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL5),\n\t\ti915_mmio_reg_offset(EU_PERF_CNTL6),\n\t};\n\n\tif (workload->engine->id != RCS0)\n\t\treturn;\n\n\tif (save) {\n\t\tworkload->oactxctrl = reg_state[ctx_oactxctrl + 1];\n\n\t\tfor (i = 0; i < ARRAY_SIZE(workload->flex_mmio); i++) {\n\t\t\tu32 state_offset = ctx_flexeu0 + i * 2;\n\n\t\t\tworkload->flex_mmio[i] = reg_state[state_offset + 1];\n\t\t}\n\t} else {\n\t\treg_state[ctx_oactxctrl] =\n\t\t\ti915_mmio_reg_offset(GEN8_OACTXCONTROL);\n\t\treg_state[ctx_oactxctrl + 1] = workload->oactxctrl;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(workload->flex_mmio); i++) {\n\t\t\tu32 state_offset = ctx_flexeu0 + i * 2;\n\t\t\tu32 mmio = flex_mmio[i];\n\n\t\t\treg_state[state_offset] = mmio;\n\t\t\treg_state[state_offset + 1] = workload->flex_mmio[i];\n\t\t}\n\t}\n}\n\nstatic int populate_shadow_context(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_context *ctx = workload->req->context;\n\tstruct execlist_ring_context *shadow_ring_context;\n\tvoid *dst;\n\tvoid *context_base;\n\tunsigned long context_gpa, context_page_num;\n\tunsigned long gpa_base;  \n\tunsigned long gpa_size;  \n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tint i;\n\tbool skip = false;\n\tint ring_id = workload->engine->id;\n\tint ret;\n\n\tGEM_BUG_ON(!intel_context_is_pinned(ctx));\n\n\tcontext_base = (void *) ctx->lrc_reg_state -\n\t\t\t\t(LRC_STATE_PN << I915_GTT_PAGE_SHIFT);\n\n\tshadow_ring_context = (void *) ctx->lrc_reg_state;\n\n\tsr_oa_regs(workload, (u32 *)shadow_ring_context, true);\n#define COPY_REG(name) \\\n\tintel_gvt_read_gpa(vgpu, workload->ring_context_gpa \\\n\t\t+ RING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)\n#define COPY_REG_MASKED(name) {\\\n\t\tintel_gvt_read_gpa(vgpu, workload->ring_context_gpa \\\n\t\t\t\t\t      + RING_CTX_OFF(name.val),\\\n\t\t\t\t\t      &shadow_ring_context->name.val, 4);\\\n\t\tshadow_ring_context->name.val |= 0xffff << 16;\\\n\t}\n\n\tCOPY_REG_MASKED(ctx_ctrl);\n\tCOPY_REG(ctx_timestamp);\n\n\tif (workload->engine->id == RCS0) {\n\t\tCOPY_REG(bb_per_ctx_ptr);\n\t\tCOPY_REG(rcs_indirect_ctx);\n\t\tCOPY_REG(rcs_indirect_ctx_offset);\n\t} else if (workload->engine->id == BCS0)\n\t\tintel_gvt_read_gpa(vgpu,\n\t\t\t\tworkload->ring_context_gpa +\n\t\t\t\tBCS_TILE_REGISTER_VAL_OFFSET,\n\t\t\t\t(void *)shadow_ring_context +\n\t\t\t\tBCS_TILE_REGISTER_VAL_OFFSET, 4);\n#undef COPY_REG\n#undef COPY_REG_MASKED\n\n\t \n\tintel_gvt_read_gpa(vgpu,\n\t\t\tworkload->ring_context_gpa +\n\t\t\tRING_CTX_SIZE,\n\t\t\t(void *)shadow_ring_context +\n\t\t\tRING_CTX_SIZE,\n\t\t\tI915_GTT_PAGE_SIZE - RING_CTX_SIZE);\n\n\tsr_oa_regs(workload, (u32 *)shadow_ring_context, false);\n\n\tgvt_dbg_sched(\"ring %s workload lrca %x, ctx_id %x, ctx gpa %llx\",\n\t\t\tworkload->engine->name, workload->ctx_desc.lrca,\n\t\t\tworkload->ctx_desc.context_id,\n\t\t\tworkload->ring_context_gpa);\n\n\t \n\tif (s->last_ctx[ring_id].valid &&\n\t\t\t(s->last_ctx[ring_id].lrca ==\n\t\t\t\tworkload->ctx_desc.lrca) &&\n\t\t\t(s->last_ctx[ring_id].ring_context_gpa ==\n\t\t\t\tworkload->ring_context_gpa))\n\t\tskip = true;\n\n\ts->last_ctx[ring_id].lrca = workload->ctx_desc.lrca;\n\ts->last_ctx[ring_id].ring_context_gpa = workload->ring_context_gpa;\n\n\tif (IS_RESTORE_INHIBIT(shadow_ring_context->ctx_ctrl.val) || skip)\n\t\treturn 0;\n\n\ts->last_ctx[ring_id].valid = false;\n\tcontext_page_num = workload->engine->context_size;\n\tcontext_page_num = context_page_num >> PAGE_SHIFT;\n\n\tif (IS_BROADWELL(gvt->gt->i915) && workload->engine->id == RCS0)\n\t\tcontext_page_num = 19;\n\n\t \n\tgpa_size = 0;\n\tfor (i = 2; i < context_page_num; i++) {\n\t\tcontext_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,\n\t\t\t\t(u32)((workload->ctx_desc.lrca + i) <<\n\t\t\t\tI915_GTT_PAGE_SHIFT));\n\t\tif (context_gpa == INTEL_GVT_INVALID_ADDR) {\n\t\t\tgvt_vgpu_err(\"Invalid guest context descriptor\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (gpa_size == 0) {\n\t\t\tgpa_base = context_gpa;\n\t\t\tdst = context_base + (i << I915_GTT_PAGE_SHIFT);\n\t\t} else if (context_gpa != gpa_base + gpa_size)\n\t\t\tgoto read;\n\n\t\tgpa_size += I915_GTT_PAGE_SIZE;\n\n\t\tif (i == context_page_num - 1)\n\t\t\tgoto read;\n\n\t\tcontinue;\n\nread:\n\t\tintel_gvt_read_gpa(vgpu, gpa_base, dst, gpa_size);\n\t\tgpa_base = context_gpa;\n\t\tgpa_size = I915_GTT_PAGE_SIZE;\n\t\tdst = context_base + (i << I915_GTT_PAGE_SHIFT);\n\t}\n\tret = intel_gvt_scan_engine_context(workload);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"invalid cmd found in guest context pages\\n\");\n\t\treturn ret;\n\t}\n\ts->last_ctx[ring_id].valid = true;\n\treturn 0;\n}\n\nstatic inline bool is_gvt_request(struct i915_request *rq)\n{\n\treturn intel_context_force_single_submission(rq->context);\n}\n\nstatic void save_ring_hw_state(struct intel_vgpu *vgpu,\n\t\t\t       const struct intel_engine_cs *engine)\n{\n\tstruct intel_uncore *uncore = engine->uncore;\n\ti915_reg_t reg;\n\n\treg = RING_INSTDONE(engine->mmio_base);\n\tvgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =\n\t\tintel_uncore_read(uncore, reg);\n\n\treg = RING_ACTHD(engine->mmio_base);\n\tvgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =\n\t\tintel_uncore_read(uncore, reg);\n\n\treg = RING_ACTHD_UDW(engine->mmio_base);\n\tvgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) =\n\t\tintel_uncore_read(uncore, reg);\n}\n\nstatic int shadow_context_status_change(struct notifier_block *nb,\n\t\tunsigned long action, void *data)\n{\n\tstruct i915_request *rq = data;\n\tstruct intel_gvt *gvt = container_of(nb, struct intel_gvt,\n\t\t\t\tshadow_ctx_notifier_block[rq->engine->id]);\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tenum intel_engine_id ring_id = rq->engine->id;\n\tstruct intel_vgpu_workload *workload;\n\tunsigned long flags;\n\n\tif (!is_gvt_request(rq)) {\n\t\tspin_lock_irqsave(&scheduler->mmio_context_lock, flags);\n\t\tif (action == INTEL_CONTEXT_SCHEDULE_IN &&\n\t\t    scheduler->engine_owner[ring_id]) {\n\t\t\t \n\t\t\tintel_gvt_switch_mmio(scheduler->engine_owner[ring_id],\n\t\t\t\t\t      NULL, rq->engine);\n\t\t\tscheduler->engine_owner[ring_id] = NULL;\n\t\t}\n\t\tspin_unlock_irqrestore(&scheduler->mmio_context_lock, flags);\n\n\t\treturn NOTIFY_OK;\n\t}\n\n\tworkload = scheduler->current_workload[ring_id];\n\tif (unlikely(!workload))\n\t\treturn NOTIFY_OK;\n\n\tswitch (action) {\n\tcase INTEL_CONTEXT_SCHEDULE_IN:\n\t\tspin_lock_irqsave(&scheduler->mmio_context_lock, flags);\n\t\tif (workload->vgpu != scheduler->engine_owner[ring_id]) {\n\t\t\t \n\t\t\tintel_gvt_switch_mmio(scheduler->engine_owner[ring_id],\n\t\t\t\t\t      workload->vgpu, rq->engine);\n\t\t\tscheduler->engine_owner[ring_id] = workload->vgpu;\n\t\t} else\n\t\t\tgvt_dbg_sched(\"skip ring %d mmio switch for vgpu%d\\n\",\n\t\t\t\t      ring_id, workload->vgpu->id);\n\t\tspin_unlock_irqrestore(&scheduler->mmio_context_lock, flags);\n\t\tatomic_set(&workload->shadow_ctx_active, 1);\n\t\tbreak;\n\tcase INTEL_CONTEXT_SCHEDULE_OUT:\n\t\tsave_ring_hw_state(workload->vgpu, rq->engine);\n\t\tatomic_set(&workload->shadow_ctx_active, 0);\n\t\tbreak;\n\tcase INTEL_CONTEXT_SCHEDULE_PREEMPTED:\n\t\tsave_ring_hw_state(workload->vgpu, rq->engine);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn NOTIFY_OK;\n\t}\n\twake_up(&workload->shadow_ctx_status_wq);\n\treturn NOTIFY_OK;\n}\n\nstatic void\nshadow_context_descriptor_update(struct intel_context *ce,\n\t\t\t\t struct intel_vgpu_workload *workload)\n{\n\tu64 desc = ce->lrc.desc;\n\n\t \n\tdesc &= ~(0x3ull << GEN8_CTX_ADDRESSING_MODE_SHIFT);\n\tdesc |= (u64)workload->ctx_desc.addressing_mode <<\n\t\tGEN8_CTX_ADDRESSING_MODE_SHIFT;\n\n\tce->lrc.desc = desc;\n}\n\nstatic int copy_workload_to_ring_buffer(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct i915_request *req = workload->req;\n\tvoid *shadow_ring_buffer_va;\n\tu32 *cs;\n\tint err;\n\n\tif (GRAPHICS_VER(req->engine->i915) == 9 && is_inhibit_context(req->context))\n\t\tintel_vgpu_restore_inhibit_context(vgpu, req);\n\n\t \n\tif (req->engine->emit_init_breadcrumb) {\n\t\terr = req->engine->emit_init_breadcrumb(req);\n\t\tif (err) {\n\t\t\tgvt_vgpu_err(\"fail to emit init breadcrumb\\n\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tcs = intel_ring_begin(workload->req, workload->rb_len / sizeof(u32));\n\tif (IS_ERR(cs)) {\n\t\tgvt_vgpu_err(\"fail to alloc size =%ld shadow  ring buffer\\n\",\n\t\t\tworkload->rb_len);\n\t\treturn PTR_ERR(cs);\n\t}\n\n\tshadow_ring_buffer_va = workload->shadow_ring_buffer_va;\n\n\t \n\tworkload->shadow_ring_buffer_va = cs;\n\n\tmemcpy(cs, shadow_ring_buffer_va,\n\t\t\tworkload->rb_len);\n\n\tcs += workload->rb_len / sizeof(u32);\n\tintel_ring_advance(workload->req, cs);\n\n\treturn 0;\n}\n\nstatic void release_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)\n{\n\tif (!wa_ctx->indirect_ctx.obj)\n\t\treturn;\n\n\ti915_gem_object_lock(wa_ctx->indirect_ctx.obj, NULL);\n\ti915_gem_object_unpin_map(wa_ctx->indirect_ctx.obj);\n\ti915_gem_object_unlock(wa_ctx->indirect_ctx.obj);\n\ti915_gem_object_put(wa_ctx->indirect_ctx.obj);\n\n\twa_ctx->indirect_ctx.obj = NULL;\n\twa_ctx->indirect_ctx.shadow_va = NULL;\n}\n\nstatic void set_dma_address(struct i915_page_directory *pd, dma_addr_t addr)\n{\n\tstruct scatterlist *sg = pd->pt.base->mm.pages->sgl;\n\n\t \n\tsg->dma_address = addr;\n}\n\nstatic void set_context_ppgtt_from_shadow(struct intel_vgpu_workload *workload,\n\t\t\t\t\t  struct intel_context *ce)\n{\n\tstruct intel_vgpu_mm *mm = workload->shadow_mm;\n\tstruct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(ce->vm);\n\tint i = 0;\n\n\tif (mm->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {\n\t\tset_dma_address(ppgtt->pd, mm->ppgtt_mm.shadow_pdps[0]);\n\t} else {\n\t\tfor (i = 0; i < GVT_RING_CTX_NR_PDPS; i++) {\n\t\t\tstruct i915_page_directory * const pd =\n\t\t\t\ti915_pd_entry(ppgtt->pd, i);\n\t\t\t \n\t\t\tif (!pd)\n\t\t\t\tbreak;\n\n\t\t\tset_dma_address(pd, mm->ppgtt_mm.shadow_pdps[i]);\n\t\t}\n\t}\n}\n\nstatic int\nintel_gvt_workload_req_alloc(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct i915_request *rq;\n\n\tif (workload->req)\n\t\treturn 0;\n\n\trq = i915_request_create(s->shadow[workload->engine->id]);\n\tif (IS_ERR(rq)) {\n\t\tgvt_vgpu_err(\"fail to allocate gem request\\n\");\n\t\treturn PTR_ERR(rq);\n\t}\n\n\tworkload->req = i915_request_get(rq);\n\treturn 0;\n}\n\n \nint intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tint ret;\n\n\tlockdep_assert_held(&vgpu->vgpu_lock);\n\n\tif (workload->shadow)\n\t\treturn 0;\n\n\tif (!test_and_set_bit(workload->engine->id, s->shadow_ctx_desc_updated))\n\t\tshadow_context_descriptor_update(s->shadow[workload->engine->id],\n\t\t\t\t\t\t workload);\n\n\tret = intel_gvt_scan_and_shadow_ringbuffer(workload);\n\tif (ret)\n\t\treturn ret;\n\n\tif (workload->engine->id == RCS0 &&\n\t    workload->wa_ctx.indirect_ctx.size) {\n\t\tret = intel_gvt_scan_and_shadow_wa_ctx(&workload->wa_ctx);\n\t\tif (ret)\n\t\t\tgoto err_shadow;\n\t}\n\n\tworkload->shadow = true;\n\treturn 0;\n\nerr_shadow:\n\trelease_shadow_wa_ctx(&workload->wa_ctx);\n\treturn ret;\n}\n\nstatic void release_shadow_batch_buffer(struct intel_vgpu_workload *workload);\n\nstatic int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_gvt *gvt = workload->vgpu->gvt;\n\tconst int gmadr_bytes = gvt->device_info.gmadr_bytes_in_cmd;\n\tstruct intel_vgpu_shadow_bb *bb;\n\tstruct i915_gem_ww_ctx ww;\n\tint ret;\n\n\tlist_for_each_entry(bb, &workload->shadow_bb, list) {\n\t\t \n\n\t\tif (bb->bb_offset)\n\t\t\tbb->bb_start_cmd_va = workload->shadow_ring_buffer_va\n\t\t\t\t+ bb->bb_offset;\n\n\t\t \n\t\tif (!bb->ppgtt) {\n\t\t\ti915_gem_ww_ctx_init(&ww, false);\nretry:\n\t\t\ti915_gem_object_lock(bb->obj, &ww);\n\n\t\t\tbb->vma = i915_gem_object_ggtt_pin_ww(bb->obj, &ww,\n\t\t\t\t\t\t\t      NULL, 0, 0, 0);\n\t\t\tif (IS_ERR(bb->vma)) {\n\t\t\t\tret = PTR_ERR(bb->vma);\n\t\t\t\tif (ret == -EDEADLK) {\n\t\t\t\t\tret = i915_gem_ww_ctx_backoff(&ww);\n\t\t\t\t\tif (!ret)\n\t\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\t \n\t\t\tbb->bb_start_cmd_va[1] = i915_ggtt_offset(bb->vma);\n\t\t\tif (gmadr_bytes == 8)\n\t\t\t\tbb->bb_start_cmd_va[2] = 0;\n\n\t\t\tret = i915_vma_move_to_active(bb->vma, workload->req,\n\t\t\t\t\t\t      __EXEC_OBJECT_NO_REQUEST_AWAIT);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\t \n\t\t\ti915_gem_object_flush_map(bb->obj);\n\t\t\ti915_gem_ww_ctx_fini(&ww);\n\t\t}\n\t}\n\treturn 0;\nerr:\n\ti915_gem_ww_ctx_fini(&ww);\n\trelease_shadow_batch_buffer(workload);\n\treturn ret;\n}\n\nstatic void update_wa_ctx_2_shadow_ctx(struct intel_shadow_wa_ctx *wa_ctx)\n{\n\tstruct intel_vgpu_workload *workload =\n\t\tcontainer_of(wa_ctx, struct intel_vgpu_workload, wa_ctx);\n\tstruct i915_request *rq = workload->req;\n\tstruct execlist_ring_context *shadow_ring_context =\n\t\t(struct execlist_ring_context *)rq->context->lrc_reg_state;\n\n\tshadow_ring_context->bb_per_ctx_ptr.val =\n\t\t(shadow_ring_context->bb_per_ctx_ptr.val &\n\t\t(~PER_CTX_ADDR_MASK)) | wa_ctx->per_ctx.shadow_gma;\n\tshadow_ring_context->rcs_indirect_ctx.val =\n\t\t(shadow_ring_context->rcs_indirect_ctx.val &\n\t\t(~INDIRECT_CTX_ADDR_MASK)) | wa_ctx->indirect_ctx.shadow_gma;\n}\n\nstatic int prepare_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx)\n{\n\tstruct i915_vma *vma;\n\tunsigned char *per_ctx_va =\n\t\t(unsigned char *)wa_ctx->indirect_ctx.shadow_va +\n\t\twa_ctx->indirect_ctx.size;\n\tstruct i915_gem_ww_ctx ww;\n\tint ret;\n\n\tif (wa_ctx->indirect_ctx.size == 0)\n\t\treturn 0;\n\n\ti915_gem_ww_ctx_init(&ww, false);\nretry:\n\ti915_gem_object_lock(wa_ctx->indirect_ctx.obj, &ww);\n\n\tvma = i915_gem_object_ggtt_pin_ww(wa_ctx->indirect_ctx.obj, &ww, NULL,\n\t\t\t\t\t  0, CACHELINE_BYTES, 0);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tif (ret == -EDEADLK) {\n\t\t\tret = i915_gem_ww_ctx_backoff(&ww);\n\t\t\tif (!ret)\n\t\t\t\tgoto retry;\n\t\t}\n\t\treturn ret;\n\t}\n\n\ti915_gem_ww_ctx_fini(&ww);\n\n\t \n\n\twa_ctx->indirect_ctx.shadow_gma = i915_ggtt_offset(vma);\n\n\twa_ctx->per_ctx.shadow_gma = *((unsigned int *)per_ctx_va + 1);\n\tmemset(per_ctx_va, 0, CACHELINE_BYTES);\n\n\tupdate_wa_ctx_2_shadow_ctx(wa_ctx);\n\treturn 0;\n}\n\nstatic void update_vreg_in_ctx(struct intel_vgpu_workload *workload)\n{\n\tvgpu_vreg_t(workload->vgpu, RING_START(workload->engine->mmio_base)) =\n\t\tworkload->rb_start;\n}\n\nstatic void release_shadow_batch_buffer(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu_shadow_bb *bb, *pos;\n\n\tif (list_empty(&workload->shadow_bb))\n\t\treturn;\n\n\tbb = list_first_entry(&workload->shadow_bb,\n\t\t\tstruct intel_vgpu_shadow_bb, list);\n\n\tlist_for_each_entry_safe(bb, pos, &workload->shadow_bb, list) {\n\t\tif (bb->obj) {\n\t\t\ti915_gem_object_lock(bb->obj, NULL);\n\t\t\tif (bb->va && !IS_ERR(bb->va))\n\t\t\t\ti915_gem_object_unpin_map(bb->obj);\n\n\t\t\tif (bb->vma && !IS_ERR(bb->vma))\n\t\t\t\ti915_vma_unpin(bb->vma);\n\n\t\t\ti915_gem_object_unlock(bb->obj);\n\t\t\ti915_gem_object_put(bb->obj);\n\t\t}\n\t\tlist_del(&bb->list);\n\t\tkfree(bb);\n\t}\n}\n\nstatic int\nintel_vgpu_shadow_mm_pin(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_vgpu_mm *m;\n\tint ret = 0;\n\n\tret = intel_vgpu_pin_mm(workload->shadow_mm);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to vgpu pin mm\\n\");\n\t\treturn ret;\n\t}\n\n\tif (workload->shadow_mm->type != INTEL_GVT_MM_PPGTT ||\n\t    !workload->shadow_mm->ppgtt_mm.shadowed) {\n\t\tintel_vgpu_unpin_mm(workload->shadow_mm);\n\t\tgvt_vgpu_err(\"workload shadow ppgtt isn't ready\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!list_empty(&workload->lri_shadow_mm)) {\n\t\tlist_for_each_entry(m, &workload->lri_shadow_mm,\n\t\t\t\t    ppgtt_mm.link) {\n\t\t\tret = intel_vgpu_pin_mm(m);\n\t\t\tif (ret) {\n\t\t\t\tlist_for_each_entry_from_reverse(m,\n\t\t\t\t\t\t\t\t &workload->lri_shadow_mm,\n\t\t\t\t\t\t\t\t ppgtt_mm.link)\n\t\t\t\t\tintel_vgpu_unpin_mm(m);\n\t\t\t\tgvt_vgpu_err(\"LRI shadow ppgtt fail to pin\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (ret)\n\t\tintel_vgpu_unpin_mm(workload->shadow_mm);\n\n\treturn ret;\n}\n\nstatic void\nintel_vgpu_shadow_mm_unpin(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu_mm *m;\n\n\tif (!list_empty(&workload->lri_shadow_mm)) {\n\t\tlist_for_each_entry(m, &workload->lri_shadow_mm,\n\t\t\t\t    ppgtt_mm.link)\n\t\t\tintel_vgpu_unpin_mm(m);\n\t}\n\tintel_vgpu_unpin_mm(workload->shadow_mm);\n}\n\nstatic int prepare_workload(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tint ret = 0;\n\n\tret = intel_vgpu_shadow_mm_pin(workload);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to pin shadow mm\\n\");\n\t\treturn ret;\n\t}\n\n\tupdate_shadow_pdps(workload);\n\n\tset_context_ppgtt_from_shadow(workload, s->shadow[workload->engine->id]);\n\n\tret = intel_vgpu_sync_oos_pages(workload->vgpu);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to vgpu sync oos pages\\n\");\n\t\tgoto err_unpin_mm;\n\t}\n\n\tret = intel_vgpu_flush_post_shadow(workload->vgpu);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to flush post shadow\\n\");\n\t\tgoto err_unpin_mm;\n\t}\n\n\tret = copy_workload_to_ring_buffer(workload);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to generate request\\n\");\n\t\tgoto err_unpin_mm;\n\t}\n\n\tret = prepare_shadow_batch_buffer(workload);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to prepare_shadow_batch_buffer\\n\");\n\t\tgoto err_unpin_mm;\n\t}\n\n\tret = prepare_shadow_wa_ctx(&workload->wa_ctx);\n\tif (ret) {\n\t\tgvt_vgpu_err(\"fail to prepare_shadow_wa_ctx\\n\");\n\t\tgoto err_shadow_batch;\n\t}\n\n\tif (workload->prepare) {\n\t\tret = workload->prepare(workload);\n\t\tif (ret)\n\t\t\tgoto err_shadow_wa_ctx;\n\t}\n\n\treturn 0;\nerr_shadow_wa_ctx:\n\trelease_shadow_wa_ctx(&workload->wa_ctx);\nerr_shadow_batch:\n\trelease_shadow_batch_buffer(workload);\nerr_unpin_mm:\n\tintel_vgpu_shadow_mm_unpin(workload);\n\treturn ret;\n}\n\nstatic int dispatch_workload(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct i915_request *rq;\n\tint ret;\n\n\tgvt_dbg_sched(\"ring id %s prepare to dispatch workload %p\\n\",\n\t\t      workload->engine->name, workload);\n\n\tmutex_lock(&vgpu->vgpu_lock);\n\n\tret = intel_gvt_workload_req_alloc(workload);\n\tif (ret)\n\t\tgoto err_req;\n\n\tret = intel_gvt_scan_and_shadow_workload(workload);\n\tif (ret)\n\t\tgoto out;\n\n\tret = populate_shadow_context(workload);\n\tif (ret) {\n\t\trelease_shadow_wa_ctx(&workload->wa_ctx);\n\t\tgoto out;\n\t}\n\n\tret = prepare_workload(workload);\nout:\n\tif (ret) {\n\t\t \n\t\trq = fetch_and_zero(&workload->req);\n\t\ti915_request_put(rq);\n\t}\n\n\tif (!IS_ERR_OR_NULL(workload->req)) {\n\t\tgvt_dbg_sched(\"ring id %s submit workload to i915 %p\\n\",\n\t\t\t      workload->engine->name, workload->req);\n\t\ti915_request_add(workload->req);\n\t\tworkload->dispatched = true;\n\t}\nerr_req:\n\tif (ret)\n\t\tworkload->status = ret;\n\tmutex_unlock(&vgpu->vgpu_lock);\n\treturn ret;\n}\n\nstatic struct intel_vgpu_workload *\npick_next_workload(struct intel_gvt *gvt, struct intel_engine_cs *engine)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct intel_vgpu_workload *workload = NULL;\n\n\tmutex_lock(&gvt->sched_lock);\n\n\t \n\tif (!scheduler->current_vgpu) {\n\t\tgvt_dbg_sched(\"ring %s stop - no current vgpu\\n\", engine->name);\n\t\tgoto out;\n\t}\n\n\tif (scheduler->need_reschedule) {\n\t\tgvt_dbg_sched(\"ring %s stop - will reschedule\\n\", engine->name);\n\t\tgoto out;\n\t}\n\n\tif (!test_bit(INTEL_VGPU_STATUS_ACTIVE,\n\t\t      scheduler->current_vgpu->status) ||\n\t    list_empty(workload_q_head(scheduler->current_vgpu, engine)))\n\t\tgoto out;\n\n\t \n\tif (scheduler->current_workload[engine->id]) {\n\t\tworkload = scheduler->current_workload[engine->id];\n\t\tgvt_dbg_sched(\"ring %s still have current workload %p\\n\",\n\t\t\t      engine->name, workload);\n\t\tgoto out;\n\t}\n\n\t \n\tscheduler->current_workload[engine->id] =\n\t\tlist_first_entry(workload_q_head(scheduler->current_vgpu,\n\t\t\t\t\t\t engine),\n\t\t\t\t struct intel_vgpu_workload, list);\n\n\tworkload = scheduler->current_workload[engine->id];\n\n\tgvt_dbg_sched(\"ring %s pick new workload %p\\n\", engine->name, workload);\n\n\tatomic_inc(&workload->vgpu->submission.running_workload_num);\nout:\n\tmutex_unlock(&gvt->sched_lock);\n\treturn workload;\n}\n\nstatic void update_guest_pdps(struct intel_vgpu *vgpu,\n\t\t\t      u64 ring_context_gpa, u32 pdp[8])\n{\n\tu64 gpa;\n\tint i;\n\n\tgpa = ring_context_gpa + RING_CTX_OFF(pdps[0].val);\n\n\tfor (i = 0; i < 8; i++)\n\t\tintel_gvt_write_gpa(vgpu, gpa + i * 8, &pdp[7 - i], 4);\n}\n\nstatic __maybe_unused bool\ncheck_shadow_context_ppgtt(struct execlist_ring_context *c, struct intel_vgpu_mm *m)\n{\n\tif (m->ppgtt_mm.root_entry_type == GTT_TYPE_PPGTT_ROOT_L4_ENTRY) {\n\t\tu64 shadow_pdp = c->pdps[7].val | (u64) c->pdps[6].val << 32;\n\n\t\tif (shadow_pdp != m->ppgtt_mm.shadow_pdps[0]) {\n\t\t\tgvt_dbg_mm(\"4-level context ppgtt not match LRI command\\n\");\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t} else {\n\t\t \n\t\tgvt_dbg_mm(\"invalid shadow mm type\\n\");\n\t\treturn false;\n\t}\n}\n\nstatic void update_guest_context(struct intel_vgpu_workload *workload)\n{\n\tstruct i915_request *rq = workload->req;\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct execlist_ring_context *shadow_ring_context;\n\tstruct intel_context *ctx = workload->req->context;\n\tvoid *context_base;\n\tvoid *src;\n\tunsigned long context_gpa, context_page_num;\n\tunsigned long gpa_base;  \n\tunsigned long gpa_size;  \n\tint i;\n\tu32 ring_base;\n\tu32 head, tail;\n\tu16 wrap_count;\n\n\tgvt_dbg_sched(\"ring id %d workload lrca %x\\n\", rq->engine->id,\n\t\t      workload->ctx_desc.lrca);\n\n\tGEM_BUG_ON(!intel_context_is_pinned(ctx));\n\n\thead = workload->rb_head;\n\ttail = workload->rb_tail;\n\twrap_count = workload->guest_rb_head >> RB_HEAD_WRAP_CNT_OFF;\n\n\tif (tail < head) {\n\t\tif (wrap_count == RB_HEAD_WRAP_CNT_MAX)\n\t\t\twrap_count = 0;\n\t\telse\n\t\t\twrap_count += 1;\n\t}\n\n\thead = (wrap_count << RB_HEAD_WRAP_CNT_OFF) | tail;\n\n\tring_base = rq->engine->mmio_base;\n\tvgpu_vreg_t(vgpu, RING_TAIL(ring_base)) = tail;\n\tvgpu_vreg_t(vgpu, RING_HEAD(ring_base)) = head;\n\n\tcontext_page_num = rq->engine->context_size;\n\tcontext_page_num = context_page_num >> PAGE_SHIFT;\n\n\tif (IS_BROADWELL(rq->i915) && rq->engine->id == RCS0)\n\t\tcontext_page_num = 19;\n\n\tcontext_base = (void *) ctx->lrc_reg_state -\n\t\t\t(LRC_STATE_PN << I915_GTT_PAGE_SHIFT);\n\n\t \n\tgpa_size = 0;\n\tfor (i = 2; i < context_page_num; i++) {\n\t\tcontext_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,\n\t\t\t\t(u32)((workload->ctx_desc.lrca + i) <<\n\t\t\t\t\tI915_GTT_PAGE_SHIFT));\n\t\tif (context_gpa == INTEL_GVT_INVALID_ADDR) {\n\t\t\tgvt_vgpu_err(\"invalid guest context descriptor\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tif (gpa_size == 0) {\n\t\t\tgpa_base = context_gpa;\n\t\t\tsrc = context_base + (i << I915_GTT_PAGE_SHIFT);\n\t\t} else if (context_gpa != gpa_base + gpa_size)\n\t\t\tgoto write;\n\n\t\tgpa_size += I915_GTT_PAGE_SIZE;\n\n\t\tif (i == context_page_num - 1)\n\t\t\tgoto write;\n\n\t\tcontinue;\n\nwrite:\n\t\tintel_gvt_write_gpa(vgpu, gpa_base, src, gpa_size);\n\t\tgpa_base = context_gpa;\n\t\tgpa_size = I915_GTT_PAGE_SIZE;\n\t\tsrc = context_base + (i << I915_GTT_PAGE_SHIFT);\n\t}\n\n\tintel_gvt_write_gpa(vgpu, workload->ring_context_gpa +\n\t\tRING_CTX_OFF(ring_header.val), &workload->rb_tail, 4);\n\n\tshadow_ring_context = (void *) ctx->lrc_reg_state;\n\n\tif (!list_empty(&workload->lri_shadow_mm)) {\n\t\tstruct intel_vgpu_mm *m = list_last_entry(&workload->lri_shadow_mm,\n\t\t\t\t\t\t\t  struct intel_vgpu_mm,\n\t\t\t\t\t\t\t  ppgtt_mm.link);\n\t\tGEM_BUG_ON(!check_shadow_context_ppgtt(shadow_ring_context, m));\n\t\tupdate_guest_pdps(vgpu, workload->ring_context_gpa,\n\t\t\t\t  (void *)m->ppgtt_mm.guest_pdps);\n\t}\n\n#define COPY_REG(name) \\\n\tintel_gvt_write_gpa(vgpu, workload->ring_context_gpa + \\\n\t\tRING_CTX_OFF(name.val), &shadow_ring_context->name.val, 4)\n\n\tCOPY_REG(ctx_ctrl);\n\tCOPY_REG(ctx_timestamp);\n\n#undef COPY_REG\n\n\tintel_gvt_write_gpa(vgpu,\n\t\t\tworkload->ring_context_gpa +\n\t\t\tsizeof(*shadow_ring_context),\n\t\t\t(void *)shadow_ring_context +\n\t\t\tsizeof(*shadow_ring_context),\n\t\t\tI915_GTT_PAGE_SIZE - sizeof(*shadow_ring_context));\n}\n\nvoid intel_vgpu_clean_workloads(struct intel_vgpu *vgpu,\n\t\t\t\tintel_engine_mask_t engine_mask)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_engine_cs *engine;\n\tstruct intel_vgpu_workload *pos, *n;\n\tintel_engine_mask_t tmp;\n\n\t \n\tfor_each_engine_masked(engine, vgpu->gvt->gt, engine_mask, tmp) {\n\t\tlist_for_each_entry_safe(pos, n,\n\t\t\t&s->workload_q_head[engine->id], list) {\n\t\t\tlist_del_init(&pos->list);\n\t\t\tintel_vgpu_destroy_workload(pos);\n\t\t}\n\t\tclear_bit(engine->id, s->shadow_ctx_desc_updated);\n\t}\n}\n\nstatic void complete_current_workload(struct intel_gvt *gvt, int ring_id)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct intel_vgpu_workload *workload =\n\t\tscheduler->current_workload[ring_id];\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct i915_request *rq = workload->req;\n\tint event;\n\n\tmutex_lock(&vgpu->vgpu_lock);\n\tmutex_lock(&gvt->sched_lock);\n\n\t \n\tif (rq) {\n\t\twait_event(workload->shadow_ctx_status_wq,\n\t\t\t   !atomic_read(&workload->shadow_ctx_active));\n\n\t\t \n\t\tif (likely(workload->status == -EINPROGRESS)) {\n\t\t\tif (workload->req->fence.error == -EIO)\n\t\t\t\tworkload->status = -EIO;\n\t\t\telse\n\t\t\t\tworkload->status = 0;\n\t\t}\n\n\t\tif (!workload->status &&\n\t\t    !(vgpu->resetting_eng & BIT(ring_id))) {\n\t\t\tupdate_guest_context(workload);\n\n\t\t\tfor_each_set_bit(event, workload->pending_events,\n\t\t\t\t\t INTEL_GVT_EVENT_MAX)\n\t\t\t\tintel_vgpu_trigger_virtual_event(vgpu, event);\n\t\t}\n\n\t\ti915_request_put(fetch_and_zero(&workload->req));\n\t}\n\n\tgvt_dbg_sched(\"ring id %d complete workload %p status %d\\n\",\n\t\t\tring_id, workload, workload->status);\n\n\tscheduler->current_workload[ring_id] = NULL;\n\n\tlist_del_init(&workload->list);\n\n\tif (workload->status || vgpu->resetting_eng & BIT(ring_id)) {\n\t\t \n\t\tintel_vgpu_clean_workloads(vgpu, BIT(ring_id));\n\t}\n\n\tworkload->complete(workload);\n\n\tintel_vgpu_shadow_mm_unpin(workload);\n\tintel_vgpu_destroy_workload(workload);\n\n\tatomic_dec(&s->running_workload_num);\n\twake_up(&scheduler->workload_complete_wq);\n\n\tif (gvt->scheduler.need_reschedule)\n\t\tintel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EVENT_SCHED);\n\n\tmutex_unlock(&gvt->sched_lock);\n\tmutex_unlock(&vgpu->vgpu_lock);\n}\n\nstatic int workload_thread(void *arg)\n{\n\tstruct intel_engine_cs *engine = arg;\n\tconst bool need_force_wake = GRAPHICS_VER(engine->i915) >= 9;\n\tstruct intel_gvt *gvt = engine->i915->gvt;\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct intel_vgpu_workload *workload = NULL;\n\tstruct intel_vgpu *vgpu = NULL;\n\tint ret;\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\n\tgvt_dbg_core(\"workload thread for ring %s started\\n\", engine->name);\n\n\twhile (!kthread_should_stop()) {\n\t\tintel_wakeref_t wakeref;\n\n\t\tadd_wait_queue(&scheduler->waitq[engine->id], &wait);\n\t\tdo {\n\t\t\tworkload = pick_next_workload(gvt, engine);\n\t\t\tif (workload)\n\t\t\t\tbreak;\n\t\t\twait_woken(&wait, TASK_INTERRUPTIBLE,\n\t\t\t\t   MAX_SCHEDULE_TIMEOUT);\n\t\t} while (!kthread_should_stop());\n\t\tremove_wait_queue(&scheduler->waitq[engine->id], &wait);\n\n\t\tif (!workload)\n\t\t\tbreak;\n\n\t\tgvt_dbg_sched(\"ring %s next workload %p vgpu %d\\n\",\n\t\t\t      engine->name, workload,\n\t\t\t      workload->vgpu->id);\n\n\t\twakeref = intel_runtime_pm_get(engine->uncore->rpm);\n\n\t\tgvt_dbg_sched(\"ring %s will dispatch workload %p\\n\",\n\t\t\t      engine->name, workload);\n\n\t\tif (need_force_wake)\n\t\t\tintel_uncore_forcewake_get(engine->uncore,\n\t\t\t\t\t\t   FORCEWAKE_ALL);\n\t\t \n\t\tupdate_vreg_in_ctx(workload);\n\n\t\tret = dispatch_workload(workload);\n\n\t\tif (ret) {\n\t\t\tvgpu = workload->vgpu;\n\t\t\tgvt_vgpu_err(\"fail to dispatch workload, skip\\n\");\n\t\t\tgoto complete;\n\t\t}\n\n\t\tgvt_dbg_sched(\"ring %s wait workload %p\\n\",\n\t\t\t      engine->name, workload);\n\t\ti915_request_wait(workload->req, 0, MAX_SCHEDULE_TIMEOUT);\n\ncomplete:\n\t\tgvt_dbg_sched(\"will complete workload %p, status: %d\\n\",\n\t\t\t      workload, workload->status);\n\n\t\tcomplete_current_workload(gvt, engine->id);\n\n\t\tif (need_force_wake)\n\t\t\tintel_uncore_forcewake_put(engine->uncore,\n\t\t\t\t\t\t   FORCEWAKE_ALL);\n\n\t\tintel_runtime_pm_put(engine->uncore->rpm, wakeref);\n\t\tif (ret && (vgpu_is_vm_unhealthy(ret)))\n\t\t\tenter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);\n\t}\n\treturn 0;\n}\n\nvoid intel_gvt_wait_vgpu_idle(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_gvt *gvt = vgpu->gvt;\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\n\tif (atomic_read(&s->running_workload_num)) {\n\t\tgvt_dbg_sched(\"wait vgpu idle\\n\");\n\n\t\twait_event(scheduler->workload_complete_wq,\n\t\t\t\t!atomic_read(&s->running_workload_num));\n\t}\n}\n\nvoid intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id i;\n\n\tgvt_dbg_core(\"clean workload scheduler\\n\");\n\n\tfor_each_engine(engine, gvt->gt, i) {\n\t\tatomic_notifier_chain_unregister(\n\t\t\t\t\t&engine->context_status_notifier,\n\t\t\t\t\t&gvt->shadow_ctx_notifier_block[i]);\n\t\tkthread_stop(scheduler->thread[i]);\n\t}\n}\n\nint intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)\n{\n\tstruct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id i;\n\tint ret;\n\n\tgvt_dbg_core(\"init workload scheduler\\n\");\n\n\tinit_waitqueue_head(&scheduler->workload_complete_wq);\n\n\tfor_each_engine(engine, gvt->gt, i) {\n\t\tinit_waitqueue_head(&scheduler->waitq[i]);\n\n\t\tscheduler->thread[i] = kthread_run(workload_thread, engine,\n\t\t\t\t\t\t   \"gvt:%s\", engine->name);\n\t\tif (IS_ERR(scheduler->thread[i])) {\n\t\t\tgvt_err(\"fail to create workload thread\\n\");\n\t\t\tret = PTR_ERR(scheduler->thread[i]);\n\t\t\tgoto err;\n\t\t}\n\n\t\tgvt->shadow_ctx_notifier_block[i].notifier_call =\n\t\t\t\t\tshadow_context_status_change;\n\t\tatomic_notifier_chain_register(&engine->context_status_notifier,\n\t\t\t\t\t&gvt->shadow_ctx_notifier_block[i]);\n\t}\n\n\treturn 0;\n\nerr:\n\tintel_gvt_clean_workload_scheduler(gvt);\n\treturn ret;\n}\n\nstatic void\ni915_context_ppgtt_root_restore(struct intel_vgpu_submission *s,\n\t\t\t\tstruct i915_ppgtt *ppgtt)\n{\n\tint i;\n\n\tif (i915_vm_is_4lvl(&ppgtt->vm)) {\n\t\tset_dma_address(ppgtt->pd, s->i915_context_pml4);\n\t} else {\n\t\tfor (i = 0; i < GEN8_3LVL_PDPES; i++) {\n\t\t\tstruct i915_page_directory * const pd =\n\t\t\t\ti915_pd_entry(ppgtt->pd, i);\n\n\t\t\tset_dma_address(pd, s->i915_context_pdps[i]);\n\t\t}\n\t}\n}\n\n \nvoid intel_vgpu_clean_submission(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tintel_vgpu_select_submission_ops(vgpu, ALL_ENGINES, 0);\n\n\ti915_context_ppgtt_root_restore(s, i915_vm_to_ppgtt(s->shadow[0]->vm));\n\tfor_each_engine(engine, vgpu->gvt->gt, id)\n\t\tintel_context_put(s->shadow[id]);\n\n\tkmem_cache_destroy(s->workloads);\n}\n\n\n \nvoid intel_vgpu_reset_submission(struct intel_vgpu *vgpu,\n\t\t\t\t intel_engine_mask_t engine_mask)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\n\tif (!s->active)\n\t\treturn;\n\n\tintel_vgpu_clean_workloads(vgpu, engine_mask);\n\ts->ops->reset(vgpu, engine_mask);\n}\n\nstatic void\ni915_context_ppgtt_root_save(struct intel_vgpu_submission *s,\n\t\t\t     struct i915_ppgtt *ppgtt)\n{\n\tint i;\n\n\tif (i915_vm_is_4lvl(&ppgtt->vm)) {\n\t\ts->i915_context_pml4 = px_dma(ppgtt->pd);\n\t} else {\n\t\tfor (i = 0; i < GEN8_3LVL_PDPES; i++) {\n\t\t\tstruct i915_page_directory * const pd =\n\t\t\t\ti915_pd_entry(ppgtt->pd, i);\n\n\t\t\ts->i915_context_pdps[i] = px_dma(pd);\n\t\t}\n\t}\n}\n\n \nint intel_vgpu_setup_submission(struct intel_vgpu *vgpu)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_engine_cs *engine;\n\tstruct i915_ppgtt *ppgtt;\n\tenum intel_engine_id i;\n\tint ret;\n\n\tppgtt = i915_ppgtt_create(to_gt(i915), I915_BO_ALLOC_PM_EARLY);\n\tif (IS_ERR(ppgtt))\n\t\treturn PTR_ERR(ppgtt);\n\n\ti915_context_ppgtt_root_save(s, ppgtt);\n\n\tfor_each_engine(engine, vgpu->gvt->gt, i) {\n\t\tstruct intel_context *ce;\n\n\t\tINIT_LIST_HEAD(&s->workload_q_head[i]);\n\t\ts->shadow[i] = ERR_PTR(-EINVAL);\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce)) {\n\t\t\tret = PTR_ERR(ce);\n\t\t\tgoto out_shadow_ctx;\n\t\t}\n\n\t\ti915_vm_put(ce->vm);\n\t\tce->vm = i915_vm_get(&ppgtt->vm);\n\t\tintel_context_set_single_submission(ce);\n\n\t\t \n\t\tif (!intel_uc_wants_guc_submission(&engine->gt->uc))\n\t\t\tce->ring_size = SZ_2M;\n\n\t\ts->shadow[i] = ce;\n\t}\n\n\tbitmap_zero(s->shadow_ctx_desc_updated, I915_NUM_ENGINES);\n\n\ts->workloads = kmem_cache_create_usercopy(\"gvt-g_vgpu_workload\",\n\t\t\t\t\t\t  sizeof(struct intel_vgpu_workload), 0,\n\t\t\t\t\t\t  SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t\t  offsetof(struct intel_vgpu_workload, rb_tail),\n\t\t\t\t\t\t  sizeof_field(struct intel_vgpu_workload, rb_tail),\n\t\t\t\t\t\t  NULL);\n\n\tif (!s->workloads) {\n\t\tret = -ENOMEM;\n\t\tgoto out_shadow_ctx;\n\t}\n\n\tatomic_set(&s->running_workload_num, 0);\n\tbitmap_zero(s->tlb_handle_pending, I915_NUM_ENGINES);\n\n\tmemset(s->last_ctx, 0, sizeof(s->last_ctx));\n\n\ti915_vm_put(&ppgtt->vm);\n\treturn 0;\n\nout_shadow_ctx:\n\ti915_context_ppgtt_root_restore(s, ppgtt);\n\tfor_each_engine(engine, vgpu->gvt->gt, i) {\n\t\tif (IS_ERR(s->shadow[i]))\n\t\t\tbreak;\n\n\t\tintel_context_put(s->shadow[i]);\n\t}\n\ti915_vm_put(&ppgtt->vm);\n\treturn ret;\n}\n\n \nint intel_vgpu_select_submission_ops(struct intel_vgpu *vgpu,\n\t\t\t\t     intel_engine_mask_t engine_mask,\n\t\t\t\t     unsigned int interface)\n{\n\tstruct drm_i915_private *i915 = vgpu->gvt->gt->i915;\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tconst struct intel_vgpu_submission_ops *ops[] = {\n\t\t[INTEL_VGPU_EXECLIST_SUBMISSION] =\n\t\t\t&intel_vgpu_execlist_submission_ops,\n\t};\n\tint ret;\n\n\tif (drm_WARN_ON(&i915->drm, interface >= ARRAY_SIZE(ops)))\n\t\treturn -EINVAL;\n\n\tif (drm_WARN_ON(&i915->drm,\n\t\t\tinterface == 0 && engine_mask != ALL_ENGINES))\n\t\treturn -EINVAL;\n\n\tif (s->active)\n\t\ts->ops->clean(vgpu, engine_mask);\n\n\tif (interface == 0) {\n\t\ts->ops = NULL;\n\t\ts->virtual_submission_interface = 0;\n\t\ts->active = false;\n\t\tgvt_dbg_core(\"vgpu%d: remove submission ops\\n\", vgpu->id);\n\t\treturn 0;\n\t}\n\n\tret = ops[interface]->init(vgpu, engine_mask);\n\tif (ret)\n\t\treturn ret;\n\n\ts->ops = ops[interface];\n\ts->virtual_submission_interface = interface;\n\ts->active = true;\n\n\tgvt_dbg_core(\"vgpu%d: activate ops [ %s ]\\n\",\n\t\t\tvgpu->id, s->ops->name);\n\n\treturn 0;\n}\n\n \nvoid intel_vgpu_destroy_workload(struct intel_vgpu_workload *workload)\n{\n\tstruct intel_vgpu_submission *s = &workload->vgpu->submission;\n\n\tintel_context_unpin(s->shadow[workload->engine->id]);\n\trelease_shadow_batch_buffer(workload);\n\trelease_shadow_wa_ctx(&workload->wa_ctx);\n\n\tif (!list_empty(&workload->lri_shadow_mm)) {\n\t\tstruct intel_vgpu_mm *m, *mm;\n\t\tlist_for_each_entry_safe(m, mm, &workload->lri_shadow_mm,\n\t\t\t\t\t ppgtt_mm.link) {\n\t\t\tlist_del(&m->ppgtt_mm.link);\n\t\t\tintel_vgpu_mm_put(m);\n\t\t}\n\t}\n\n\tGEM_BUG_ON(!list_empty(&workload->lri_shadow_mm));\n\tif (workload->shadow_mm)\n\t\tintel_vgpu_mm_put(workload->shadow_mm);\n\n\tkmem_cache_free(s->workloads, workload);\n}\n\nstatic struct intel_vgpu_workload *\nalloc_workload(struct intel_vgpu *vgpu)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct intel_vgpu_workload *workload;\n\n\tworkload = kmem_cache_zalloc(s->workloads, GFP_KERNEL);\n\tif (!workload)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&workload->list);\n\tINIT_LIST_HEAD(&workload->shadow_bb);\n\tINIT_LIST_HEAD(&workload->lri_shadow_mm);\n\n\tinit_waitqueue_head(&workload->shadow_ctx_status_wq);\n\tatomic_set(&workload->shadow_ctx_active, 0);\n\n\tworkload->status = -EINPROGRESS;\n\tworkload->vgpu = vgpu;\n\n\treturn workload;\n}\n\n#define RING_CTX_OFF(x) \\\n\toffsetof(struct execlist_ring_context, x)\n\nstatic void read_guest_pdps(struct intel_vgpu *vgpu,\n\t\tu64 ring_context_gpa, u32 pdp[8])\n{\n\tu64 gpa;\n\tint i;\n\n\tgpa = ring_context_gpa + RING_CTX_OFF(pdps[0].val);\n\n\tfor (i = 0; i < 8; i++)\n\t\tintel_gvt_read_gpa(vgpu,\n\t\t\t\tgpa + i * 8, &pdp[7 - i], 4);\n}\n\nstatic int prepare_mm(struct intel_vgpu_workload *workload)\n{\n\tstruct execlist_ctx_descriptor_format *desc = &workload->ctx_desc;\n\tstruct intel_vgpu_mm *mm;\n\tstruct intel_vgpu *vgpu = workload->vgpu;\n\tenum intel_gvt_gtt_type root_entry_type;\n\tu64 pdps[GVT_RING_CTX_NR_PDPS];\n\n\tswitch (desc->addressing_mode) {\n\tcase 1:  \n\t\troot_entry_type = GTT_TYPE_PPGTT_ROOT_L3_ENTRY;\n\t\tbreak;\n\tcase 3:  \n\t\troot_entry_type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;\n\t\tbreak;\n\tdefault:\n\t\tgvt_vgpu_err(\"Advanced Context mode(SVM) is not supported!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tread_guest_pdps(workload->vgpu, workload->ring_context_gpa, (void *)pdps);\n\n\tmm = intel_vgpu_get_ppgtt_mm(workload->vgpu, root_entry_type, pdps);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\n\tworkload->shadow_mm = mm;\n\treturn 0;\n}\n\n#define same_context(a, b) (((a)->context_id == (b)->context_id) && \\\n\t\t((a)->lrca == (b)->lrca))\n\n \nstruct intel_vgpu_workload *\nintel_vgpu_create_workload(struct intel_vgpu *vgpu,\n\t\t\t   const struct intel_engine_cs *engine,\n\t\t\t   struct execlist_ctx_descriptor_format *desc)\n{\n\tstruct intel_vgpu_submission *s = &vgpu->submission;\n\tstruct list_head *q = workload_q_head(vgpu, engine);\n\tstruct intel_vgpu_workload *last_workload = NULL;\n\tstruct intel_vgpu_workload *workload = NULL;\n\tu64 ring_context_gpa;\n\tu32 head, tail, start, ctl, ctx_ctl, per_ctx, indirect_ctx;\n\tu32 guest_head;\n\tint ret;\n\n\tring_context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,\n\t\t\t(u32)((desc->lrca + 1) << I915_GTT_PAGE_SHIFT));\n\tif (ring_context_gpa == INTEL_GVT_INVALID_ADDR) {\n\t\tgvt_vgpu_err(\"invalid guest context LRCA: %x\\n\", desc->lrca);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(ring_header.val), &head, 4);\n\n\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(ring_tail.val), &tail, 4);\n\n\tguest_head = head;\n\n\thead &= RB_HEAD_OFF_MASK;\n\ttail &= RB_TAIL_OFF_MASK;\n\n\tlist_for_each_entry_reverse(last_workload, q, list) {\n\n\t\tif (same_context(&last_workload->ctx_desc, desc)) {\n\t\t\tgvt_dbg_el(\"ring %s cur workload == last\\n\",\n\t\t\t\t   engine->name);\n\t\t\tgvt_dbg_el(\"ctx head %x real head %lx\\n\", head,\n\t\t\t\t   last_workload->rb_tail);\n\t\t\t \n\t\t\thead = last_workload->rb_tail;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tgvt_dbg_el(\"ring %s begin a new workload\\n\", engine->name);\n\n\t \n\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(rb_start.val), &start, 4);\n\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(rb_ctrl.val), &ctl, 4);\n\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(ctx_ctrl.val), &ctx_ctl, 4);\n\n\tif (!intel_gvt_ggtt_validate_range(vgpu, start,\n\t\t\t\t_RING_CTL_BUF_SIZE(ctl))) {\n\t\tgvt_vgpu_err(\"context contain invalid rb at: 0x%x\\n\", start);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tworkload = alloc_workload(vgpu);\n\tif (IS_ERR(workload))\n\t\treturn workload;\n\n\tworkload->engine = engine;\n\tworkload->ctx_desc = *desc;\n\tworkload->ring_context_gpa = ring_context_gpa;\n\tworkload->rb_head = head;\n\tworkload->guest_rb_head = guest_head;\n\tworkload->rb_tail = tail;\n\tworkload->rb_start = start;\n\tworkload->rb_ctl = ctl;\n\n\tif (engine->id == RCS0) {\n\t\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(bb_per_ctx_ptr.val), &per_ctx, 4);\n\t\tintel_gvt_read_gpa(vgpu, ring_context_gpa +\n\t\t\tRING_CTX_OFF(rcs_indirect_ctx.val), &indirect_ctx, 4);\n\n\t\tworkload->wa_ctx.indirect_ctx.guest_gma =\n\t\t\tindirect_ctx & INDIRECT_CTX_ADDR_MASK;\n\t\tworkload->wa_ctx.indirect_ctx.size =\n\t\t\t(indirect_ctx & INDIRECT_CTX_SIZE_MASK) *\n\t\t\tCACHELINE_BYTES;\n\n\t\tif (workload->wa_ctx.indirect_ctx.size != 0) {\n\t\t\tif (!intel_gvt_ggtt_validate_range(vgpu,\n\t\t\t\tworkload->wa_ctx.indirect_ctx.guest_gma,\n\t\t\t\tworkload->wa_ctx.indirect_ctx.size)) {\n\t\t\t\tgvt_vgpu_err(\"invalid wa_ctx at: 0x%lx\\n\",\n\t\t\t\t    workload->wa_ctx.indirect_ctx.guest_gma);\n\t\t\t\tkmem_cache_free(s->workloads, workload);\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t}\n\n\t\tworkload->wa_ctx.per_ctx.guest_gma =\n\t\t\tper_ctx & PER_CTX_ADDR_MASK;\n\t\tworkload->wa_ctx.per_ctx.valid = per_ctx & 1;\n\t\tif (workload->wa_ctx.per_ctx.valid) {\n\t\t\tif (!intel_gvt_ggtt_validate_range(vgpu,\n\t\t\t\tworkload->wa_ctx.per_ctx.guest_gma,\n\t\t\t\tCACHELINE_BYTES)) {\n\t\t\t\tgvt_vgpu_err(\"invalid per_ctx at: 0x%lx\\n\",\n\t\t\t\t\tworkload->wa_ctx.per_ctx.guest_gma);\n\t\t\t\tkmem_cache_free(s->workloads, workload);\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t}\n\t}\n\n\tgvt_dbg_el(\"workload %p ring %s head %x tail %x start %x ctl %x\\n\",\n\t\t   workload, engine->name, head, tail, start, ctl);\n\n\tret = prepare_mm(workload);\n\tif (ret) {\n\t\tkmem_cache_free(s->workloads, workload);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\t \n\tif (list_empty(q)) {\n\t\tintel_wakeref_t wakeref;\n\n\t\twith_intel_runtime_pm(engine->gt->uncore->rpm, wakeref)\n\t\t\tret = intel_gvt_scan_and_shadow_workload(workload);\n\t}\n\n\tif (ret) {\n\t\tif (vgpu_is_vm_unhealthy(ret))\n\t\t\tenter_failsafe_mode(vgpu, GVT_FAILSAFE_GUEST_ERR);\n\t\tintel_vgpu_destroy_workload(workload);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = intel_context_pin(s->shadow[engine->id]);\n\tif (ret) {\n\t\tintel_vgpu_destroy_workload(workload);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn workload;\n}\n\n \nvoid intel_vgpu_queue_workload(struct intel_vgpu_workload *workload)\n{\n\tlist_add_tail(&workload->list,\n\t\t      workload_q_head(workload->vgpu, workload->engine));\n\tintel_gvt_kick_schedule(workload->vgpu->gvt);\n\twake_up(&workload->vgpu->gvt->scheduler.waitq[workload->engine->id]);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}