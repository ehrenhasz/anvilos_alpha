{
  "module_name": "i915_vma.c",
  "hash_id": "d9bad0fc646ac2285919f81434521337546edfe86ffed2ecc39b91157ae92a99",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/i915_vma.c",
  "human_readable_source": " \n\n#include <linux/sched/mm.h>\n#include <linux/dma-fence-array.h>\n#include <drm/drm_gem.h>\n\n#include \"display/intel_display.h\"\n#include \"display/intel_frontbuffer.h\"\n#include \"gem/i915_gem_lmem.h\"\n#include \"gem/i915_gem_tiling.h\"\n#include \"gt/intel_engine.h\"\n#include \"gt/intel_engine_heartbeat.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_requests.h\"\n#include \"gt/intel_tlb.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_gem_evict.h\"\n#include \"i915_sw_fence_work.h\"\n#include \"i915_trace.h\"\n#include \"i915_vma.h\"\n#include \"i915_vma_resource.h\"\n\nstatic inline void assert_vma_held_evict(const struct i915_vma *vma)\n{\n\t \n\tif (kref_read(&vma->vm->ref))\n\t\tassert_object_held_shared(vma->obj);\n}\n\nstatic struct kmem_cache *slab_vmas;\n\nstatic struct i915_vma *i915_vma_alloc(void)\n{\n\treturn kmem_cache_zalloc(slab_vmas, GFP_KERNEL);\n}\n\nstatic void i915_vma_free(struct i915_vma *vma)\n{\n\treturn kmem_cache_free(slab_vmas, vma);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_ERRLOG_GEM) && IS_ENABLED(CONFIG_DRM_DEBUG_MM)\n\n#include <linux/stackdepot.h>\n\nstatic void vma_print_allocator(struct i915_vma *vma, const char *reason)\n{\n\tchar buf[512];\n\n\tif (!vma->node.stack) {\n\t\tdrm_dbg(vma->obj->base.dev,\n\t\t\t\"vma.node [%08llx + %08llx] %s: unknown owner\\n\",\n\t\t\tvma->node.start, vma->node.size, reason);\n\t\treturn;\n\t}\n\n\tstack_depot_snprint(vma->node.stack, buf, sizeof(buf), 0);\n\tdrm_dbg(vma->obj->base.dev,\n\t\t\"vma.node [%08llx + %08llx] %s: inserted at %s\\n\",\n\t\tvma->node.start, vma->node.size, reason, buf);\n}\n\n#else\n\nstatic void vma_print_allocator(struct i915_vma *vma, const char *reason)\n{\n}\n\n#endif\n\nstatic inline struct i915_vma *active_to_vma(struct i915_active *ref)\n{\n\treturn container_of(ref, typeof(struct i915_vma), active);\n}\n\nstatic int __i915_vma_active(struct i915_active *ref)\n{\n\treturn i915_vma_tryget(active_to_vma(ref)) ? 0 : -ENOENT;\n}\n\nstatic void __i915_vma_retire(struct i915_active *ref)\n{\n\ti915_vma_put(active_to_vma(ref));\n}\n\nstatic struct i915_vma *\nvma_create(struct drm_i915_gem_object *obj,\n\t   struct i915_address_space *vm,\n\t   const struct i915_gtt_view *view)\n{\n\tstruct i915_vma *pos = ERR_PTR(-E2BIG);\n\tstruct i915_vma *vma;\n\tstruct rb_node *rb, **p;\n\tint err;\n\n\t \n\tGEM_BUG_ON(vm == &vm->gt->ggtt->alias->vm);\n\n\tvma = i915_vma_alloc();\n\tif (vma == NULL)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tvma->ops = &vm->vma_ops;\n\tvma->obj = obj;\n\tvma->size = obj->base.size;\n\tvma->display_alignment = I915_GTT_MIN_ALIGNMENT;\n\n\ti915_active_init(&vma->active, __i915_vma_active, __i915_vma_retire, 0);\n\n\t \n\tif (IS_ENABLED(CONFIG_LOCKDEP)) {\n\t\tfs_reclaim_acquire(GFP_KERNEL);\n\t\tmight_lock(&vma->active.mutex);\n\t\tfs_reclaim_release(GFP_KERNEL);\n\t}\n\n\tINIT_LIST_HEAD(&vma->closed_link);\n\tINIT_LIST_HEAD(&vma->obj_link);\n\tRB_CLEAR_NODE(&vma->obj_node);\n\n\tif (view && view->type != I915_GTT_VIEW_NORMAL) {\n\t\tvma->gtt_view = *view;\n\t\tif (view->type == I915_GTT_VIEW_PARTIAL) {\n\t\t\tGEM_BUG_ON(range_overflows_t(u64,\n\t\t\t\t\t\t     view->partial.offset,\n\t\t\t\t\t\t     view->partial.size,\n\t\t\t\t\t\t     obj->base.size >> PAGE_SHIFT));\n\t\t\tvma->size = view->partial.size;\n\t\t\tvma->size <<= PAGE_SHIFT;\n\t\t\tGEM_BUG_ON(vma->size > obj->base.size);\n\t\t} else if (view->type == I915_GTT_VIEW_ROTATED) {\n\t\t\tvma->size = intel_rotation_info_size(&view->rotated);\n\t\t\tvma->size <<= PAGE_SHIFT;\n\t\t} else if (view->type == I915_GTT_VIEW_REMAPPED) {\n\t\t\tvma->size = intel_remapped_info_size(&view->remapped);\n\t\t\tvma->size <<= PAGE_SHIFT;\n\t\t}\n\t}\n\n\tif (unlikely(vma->size > vm->total))\n\t\tgoto err_vma;\n\n\tGEM_BUG_ON(!IS_ALIGNED(vma->size, I915_GTT_PAGE_SIZE));\n\n\terr = mutex_lock_interruptible(&vm->mutex);\n\tif (err) {\n\t\tpos = ERR_PTR(err);\n\t\tgoto err_vma;\n\t}\n\n\tvma->vm = vm;\n\tlist_add_tail(&vma->vm_link, &vm->unbound_list);\n\n\tspin_lock(&obj->vma.lock);\n\tif (i915_is_ggtt(vm)) {\n\t\tif (unlikely(overflows_type(vma->size, u32)))\n\t\t\tgoto err_unlock;\n\n\t\tvma->fence_size = i915_gem_fence_size(vm->i915, vma->size,\n\t\t\t\t\t\t      i915_gem_object_get_tiling(obj),\n\t\t\t\t\t\t      i915_gem_object_get_stride(obj));\n\t\tif (unlikely(vma->fence_size < vma->size ||  \n\t\t\t     vma->fence_size > vm->total))\n\t\t\tgoto err_unlock;\n\n\t\tGEM_BUG_ON(!IS_ALIGNED(vma->fence_size, I915_GTT_MIN_ALIGNMENT));\n\n\t\tvma->fence_alignment = i915_gem_fence_alignment(vm->i915, vma->size,\n\t\t\t\t\t\t\t\ti915_gem_object_get_tiling(obj),\n\t\t\t\t\t\t\t\ti915_gem_object_get_stride(obj));\n\t\tGEM_BUG_ON(!is_power_of_2(vma->fence_alignment));\n\n\t\t__set_bit(I915_VMA_GGTT_BIT, __i915_vma_flags(vma));\n\t}\n\n\trb = NULL;\n\tp = &obj->vma.tree.rb_node;\n\twhile (*p) {\n\t\tlong cmp;\n\n\t\trb = *p;\n\t\tpos = rb_entry(rb, struct i915_vma, obj_node);\n\n\t\t \n\t\tcmp = i915_vma_compare(pos, vm, view);\n\t\tif (cmp < 0)\n\t\t\tp = &rb->rb_right;\n\t\telse if (cmp > 0)\n\t\t\tp = &rb->rb_left;\n\t\telse\n\t\t\tgoto err_unlock;\n\t}\n\trb_link_node(&vma->obj_node, rb, p);\n\trb_insert_color(&vma->obj_node, &obj->vma.tree);\n\n\tif (i915_vma_is_ggtt(vma))\n\t\t \n\t\tlist_add(&vma->obj_link, &obj->vma.list);\n\telse\n\t\tlist_add_tail(&vma->obj_link, &obj->vma.list);\n\n\tspin_unlock(&obj->vma.lock);\n\tmutex_unlock(&vm->mutex);\n\n\treturn vma;\n\nerr_unlock:\n\tspin_unlock(&obj->vma.lock);\n\tlist_del_init(&vma->vm_link);\n\tmutex_unlock(&vm->mutex);\nerr_vma:\n\ti915_vma_free(vma);\n\treturn pos;\n}\n\nstatic struct i915_vma *\ni915_vma_lookup(struct drm_i915_gem_object *obj,\n\t   struct i915_address_space *vm,\n\t   const struct i915_gtt_view *view)\n{\n\tstruct rb_node *rb;\n\n\trb = obj->vma.tree.rb_node;\n\twhile (rb) {\n\t\tstruct i915_vma *vma = rb_entry(rb, struct i915_vma, obj_node);\n\t\tlong cmp;\n\n\t\tcmp = i915_vma_compare(vma, vm, view);\n\t\tif (cmp == 0)\n\t\t\treturn vma;\n\n\t\tif (cmp < 0)\n\t\t\trb = rb->rb_right;\n\t\telse\n\t\t\trb = rb->rb_left;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct i915_vma *\ni915_vma_instance(struct drm_i915_gem_object *obj,\n\t\t  struct i915_address_space *vm,\n\t\t  const struct i915_gtt_view *view)\n{\n\tstruct i915_vma *vma;\n\n\tGEM_BUG_ON(view && !i915_is_ggtt_or_dpt(vm));\n\tGEM_BUG_ON(!kref_read(&vm->ref));\n\n\tspin_lock(&obj->vma.lock);\n\tvma = i915_vma_lookup(obj, vm, view);\n\tspin_unlock(&obj->vma.lock);\n\n\t \n\tif (unlikely(!vma))\n\t\tvma = vma_create(obj, vm, view);\n\n\tGEM_BUG_ON(!IS_ERR(vma) && i915_vma_compare(vma, vm, view));\n\treturn vma;\n}\n\nstruct i915_vma_work {\n\tstruct dma_fence_work base;\n\tstruct i915_address_space *vm;\n\tstruct i915_vm_pt_stash stash;\n\tstruct i915_vma_resource *vma_res;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_sw_dma_fence_cb cb;\n\tunsigned int pat_index;\n\tunsigned int flags;\n};\n\nstatic void __vma_bind(struct dma_fence_work *work)\n{\n\tstruct i915_vma_work *vw = container_of(work, typeof(*vw), base);\n\tstruct i915_vma_resource *vma_res = vw->vma_res;\n\n\t \n\tif (i915_gem_object_has_unknown_state(vw->obj))\n\t\treturn;\n\n\tvma_res->ops->bind_vma(vma_res->vm, &vw->stash,\n\t\t\t       vma_res, vw->pat_index, vw->flags);\n}\n\nstatic void __vma_release(struct dma_fence_work *work)\n{\n\tstruct i915_vma_work *vw = container_of(work, typeof(*vw), base);\n\n\tif (vw->obj)\n\t\ti915_gem_object_put(vw->obj);\n\n\ti915_vm_free_pt_stash(vw->vm, &vw->stash);\n\tif (vw->vma_res)\n\t\ti915_vma_resource_put(vw->vma_res);\n}\n\nstatic const struct dma_fence_work_ops bind_ops = {\n\t.name = \"bind\",\n\t.work = __vma_bind,\n\t.release = __vma_release,\n};\n\nstruct i915_vma_work *i915_vma_work(void)\n{\n\tstruct i915_vma_work *vw;\n\n\tvw = kzalloc(sizeof(*vw), GFP_KERNEL);\n\tif (!vw)\n\t\treturn NULL;\n\n\tdma_fence_work_init(&vw->base, &bind_ops);\n\tvw->base.dma.error = -EAGAIN;  \n\n\treturn vw;\n}\n\nint i915_vma_wait_for_bind(struct i915_vma *vma)\n{\n\tint err = 0;\n\n\tif (rcu_access_pointer(vma->active.excl.fence)) {\n\t\tstruct dma_fence *fence;\n\n\t\trcu_read_lock();\n\t\tfence = dma_fence_get_rcu_safe(&vma->active.excl.fence);\n\t\trcu_read_unlock();\n\t\tif (fence) {\n\t\t\terr = dma_fence_wait(fence, true);\n\t\t\tdma_fence_put(fence);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\nstatic int i915_vma_verify_bind_complete(struct i915_vma *vma)\n{\n\tstruct dma_fence *fence = i915_active_fence_get(&vma->active.excl);\n\tint err;\n\n\tif (!fence)\n\t\treturn 0;\n\n\tif (dma_fence_is_signaled(fence))\n\t\terr = fence->error;\n\telse\n\t\terr = -EBUSY;\n\n\tdma_fence_put(fence);\n\n\treturn err;\n}\n#else\n#define i915_vma_verify_bind_complete(_vma) 0\n#endif\n\nI915_SELFTEST_EXPORT void\ni915_vma_resource_init_from_vma(struct i915_vma_resource *vma_res,\n\t\t\t\tstruct i915_vma *vma)\n{\n\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\ti915_vma_resource_init(vma_res, vma->vm, vma->pages, &vma->page_sizes,\n\t\t\t       obj->mm.rsgt, i915_gem_object_is_readonly(obj),\n\t\t\t       i915_gem_object_is_lmem(obj), obj->mm.region,\n\t\t\t       vma->ops, vma->private, __i915_vma_offset(vma),\n\t\t\t       __i915_vma_size(vma), vma->size, vma->guard);\n}\n\n \nint i915_vma_bind(struct i915_vma *vma,\n\t\t  unsigned int pat_index,\n\t\t  u32 flags,\n\t\t  struct i915_vma_work *work,\n\t\t  struct i915_vma_resource *vma_res)\n{\n\tu32 bind_flags;\n\tu32 vma_flags;\n\tint ret;\n\n\tlockdep_assert_held(&vma->vm->mutex);\n\tGEM_BUG_ON(!drm_mm_node_allocated(&vma->node));\n\tGEM_BUG_ON(vma->size > i915_vma_size(vma));\n\n\tif (GEM_DEBUG_WARN_ON(range_overflows(vma->node.start,\n\t\t\t\t\t      vma->node.size,\n\t\t\t\t\t      vma->vm->total))) {\n\t\ti915_vma_resource_free(vma_res);\n\t\treturn -ENODEV;\n\t}\n\n\tif (GEM_DEBUG_WARN_ON(!flags)) {\n\t\ti915_vma_resource_free(vma_res);\n\t\treturn -EINVAL;\n\t}\n\n\tbind_flags = flags;\n\tbind_flags &= I915_VMA_GLOBAL_BIND | I915_VMA_LOCAL_BIND;\n\n\tvma_flags = atomic_read(&vma->flags);\n\tvma_flags &= I915_VMA_GLOBAL_BIND | I915_VMA_LOCAL_BIND;\n\n\tbind_flags &= ~vma_flags;\n\tif (bind_flags == 0) {\n\t\ti915_vma_resource_free(vma_res);\n\t\treturn 0;\n\t}\n\n\tGEM_BUG_ON(!atomic_read(&vma->pages_count));\n\n\t \n\tif (work && bind_flags & vma->vm->bind_async_flags)\n\t\tret = i915_vma_resource_bind_dep_await(vma->vm,\n\t\t\t\t\t\t       &work->base.chain,\n\t\t\t\t\t\t       vma->node.start,\n\t\t\t\t\t\t       vma->node.size,\n\t\t\t\t\t\t       true,\n\t\t\t\t\t\t       GFP_NOWAIT |\n\t\t\t\t\t\t       __GFP_RETRY_MAYFAIL |\n\t\t\t\t\t\t       __GFP_NOWARN);\n\telse\n\t\tret = i915_vma_resource_bind_dep_sync(vma->vm, vma->node.start,\n\t\t\t\t\t\t      vma->node.size, true);\n\tif (ret) {\n\t\ti915_vma_resource_free(vma_res);\n\t\treturn ret;\n\t}\n\n\tif (vma->resource || !vma_res) {\n\t\t \n\t\tGEM_WARN_ON(!vma_flags);\n\t\ti915_vma_resource_free(vma_res);\n\t} else {\n\t\ti915_vma_resource_init_from_vma(vma_res, vma);\n\t\tvma->resource = vma_res;\n\t}\n\ttrace_i915_vma_bind(vma, bind_flags);\n\tif (work && bind_flags & vma->vm->bind_async_flags) {\n\t\tstruct dma_fence *prev;\n\n\t\twork->vma_res = i915_vma_resource_get(vma->resource);\n\t\twork->pat_index = pat_index;\n\t\twork->flags = bind_flags;\n\n\t\t \n\t\tprev = i915_active_set_exclusive(&vma->active, &work->base.dma);\n\t\tif (prev) {\n\t\t\t__i915_sw_fence_await_dma_fence(&work->base.chain,\n\t\t\t\t\t\t\tprev,\n\t\t\t\t\t\t\t&work->cb);\n\t\t\tdma_fence_put(prev);\n\t\t}\n\n\t\twork->base.dma.error = 0;  \n\t\twork->obj = i915_gem_object_get(vma->obj);\n\t} else {\n\t\tret = i915_gem_object_wait_moving_fence(vma->obj, true);\n\t\tif (ret) {\n\t\t\ti915_vma_resource_free(vma->resource);\n\t\t\tvma->resource = NULL;\n\n\t\t\treturn ret;\n\t\t}\n\t\tvma->ops->bind_vma(vma->vm, NULL, vma->resource, pat_index,\n\t\t\t\t   bind_flags);\n\t}\n\n\tatomic_or(bind_flags, &vma->flags);\n\treturn 0;\n}\n\nvoid __iomem *i915_vma_pin_iomap(struct i915_vma *vma)\n{\n\tvoid __iomem *ptr;\n\tint err;\n\n\tif (WARN_ON_ONCE(vma->obj->flags & I915_BO_ALLOC_GPU_ONLY))\n\t\treturn IOMEM_ERR_PTR(-EINVAL);\n\n\tGEM_BUG_ON(!i915_vma_is_ggtt(vma));\n\tGEM_BUG_ON(!i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND));\n\tGEM_BUG_ON(i915_vma_verify_bind_complete(vma));\n\n\tptr = READ_ONCE(vma->iomap);\n\tif (ptr == NULL) {\n\t\t \n\t\tif (i915_gem_object_is_lmem(vma->obj)) {\n\t\t\tptr = i915_gem_object_lmem_io_map(vma->obj, 0,\n\t\t\t\t\t\t\t  vma->obj->base.size);\n\t\t} else if (i915_vma_is_map_and_fenceable(vma)) {\n\t\t\tptr = io_mapping_map_wc(&i915_vm_to_ggtt(vma->vm)->iomap,\n\t\t\t\t\t\ti915_vma_offset(vma),\n\t\t\t\t\t\ti915_vma_size(vma));\n\t\t} else {\n\t\t\tptr = (void __iomem *)\n\t\t\t\ti915_gem_object_pin_map(vma->obj, I915_MAP_WC);\n\t\t\tif (IS_ERR(ptr)) {\n\t\t\t\terr = PTR_ERR(ptr);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tptr = page_pack_bits(ptr, 1);\n\t\t}\n\n\t\tif (ptr == NULL) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\tif (unlikely(cmpxchg(&vma->iomap, NULL, ptr))) {\n\t\t\tif (page_unmask_bits(ptr))\n\t\t\t\t__i915_gem_object_release_map(vma->obj);\n\t\t\telse\n\t\t\t\tio_mapping_unmap(ptr);\n\t\t\tptr = vma->iomap;\n\t\t}\n\t}\n\n\t__i915_vma_pin(vma);\n\n\terr = i915_vma_pin_fence(vma);\n\tif (err)\n\t\tgoto err_unpin;\n\n\ti915_vma_set_ggtt_write(vma);\n\n\t \n\treturn page_mask_bits(ptr);\n\nerr_unpin:\n\t__i915_vma_unpin(vma);\nerr:\n\treturn IOMEM_ERR_PTR(err);\n}\n\nvoid i915_vma_flush_writes(struct i915_vma *vma)\n{\n\tif (i915_vma_unset_ggtt_write(vma))\n\t\tintel_gt_flush_ggtt_writes(vma->vm->gt);\n}\n\nvoid i915_vma_unpin_iomap(struct i915_vma *vma)\n{\n\tGEM_BUG_ON(vma->iomap == NULL);\n\n\t \n\n\ti915_vma_flush_writes(vma);\n\n\ti915_vma_unpin_fence(vma);\n\ti915_vma_unpin(vma);\n}\n\nvoid i915_vma_unpin_and_release(struct i915_vma **p_vma, unsigned int flags)\n{\n\tstruct i915_vma *vma;\n\tstruct drm_i915_gem_object *obj;\n\n\tvma = fetch_and_zero(p_vma);\n\tif (!vma)\n\t\treturn;\n\n\tobj = vma->obj;\n\tGEM_BUG_ON(!obj);\n\n\ti915_vma_unpin(vma);\n\n\tif (flags & I915_VMA_RELEASE_MAP)\n\t\ti915_gem_object_unpin_map(obj);\n\n\ti915_gem_object_put(obj);\n}\n\nbool i915_vma_misplaced(const struct i915_vma *vma,\n\t\t\tu64 size, u64 alignment, u64 flags)\n{\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn false;\n\n\tif (test_bit(I915_VMA_ERROR_BIT, __i915_vma_flags(vma)))\n\t\treturn true;\n\n\tif (i915_vma_size(vma) < size)\n\t\treturn true;\n\n\tGEM_BUG_ON(alignment && !is_power_of_2(alignment));\n\tif (alignment && !IS_ALIGNED(i915_vma_offset(vma), alignment))\n\t\treturn true;\n\n\tif (flags & PIN_MAPPABLE && !i915_vma_is_map_and_fenceable(vma))\n\t\treturn true;\n\n\tif (flags & PIN_OFFSET_BIAS &&\n\t    i915_vma_offset(vma) < (flags & PIN_OFFSET_MASK))\n\t\treturn true;\n\n\tif (flags & PIN_OFFSET_FIXED &&\n\t    i915_vma_offset(vma) != (flags & PIN_OFFSET_MASK))\n\t\treturn true;\n\n\tif (flags & PIN_OFFSET_GUARD &&\n\t    vma->guard < (flags & PIN_OFFSET_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid __i915_vma_set_map_and_fenceable(struct i915_vma *vma)\n{\n\tbool mappable, fenceable;\n\n\tGEM_BUG_ON(!i915_vma_is_ggtt(vma));\n\tGEM_BUG_ON(!vma->fence_size);\n\n\tfenceable = (i915_vma_size(vma) >= vma->fence_size &&\n\t\t     IS_ALIGNED(i915_vma_offset(vma), vma->fence_alignment));\n\n\tmappable = i915_ggtt_offset(vma) + vma->fence_size <=\n\t\t   i915_vm_to_ggtt(vma->vm)->mappable_end;\n\n\tif (mappable && fenceable)\n\t\tset_bit(I915_VMA_CAN_FENCE_BIT, __i915_vma_flags(vma));\n\telse\n\t\tclear_bit(I915_VMA_CAN_FENCE_BIT, __i915_vma_flags(vma));\n}\n\nbool i915_gem_valid_gtt_space(struct i915_vma *vma, unsigned long color)\n{\n\tstruct drm_mm_node *node = &vma->node;\n\tstruct drm_mm_node *other;\n\n\t \n\tif (!i915_vm_has_cache_coloring(vma->vm))\n\t\treturn true;\n\n\t \n\tGEM_BUG_ON(!drm_mm_node_allocated(node));\n\tGEM_BUG_ON(list_empty(&node->node_list));\n\n\tother = list_prev_entry(node, node_list);\n\tif (i915_node_color_differs(other, color) &&\n\t    !drm_mm_hole_follows(other))\n\t\treturn false;\n\n\tother = list_next_entry(node, node_list);\n\tif (i915_node_color_differs(other, color) &&\n\t    !drm_mm_hole_follows(node))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic int\ni915_vma_insert(struct i915_vma *vma, struct i915_gem_ww_ctx *ww,\n\t\tu64 size, u64 alignment, u64 flags)\n{\n\tunsigned long color, guard;\n\tu64 start, end;\n\tint ret;\n\n\tGEM_BUG_ON(i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND | I915_VMA_LOCAL_BIND));\n\tGEM_BUG_ON(drm_mm_node_allocated(&vma->node));\n\tGEM_BUG_ON(hweight64(flags & (PIN_OFFSET_GUARD | PIN_OFFSET_FIXED | PIN_OFFSET_BIAS)) > 1);\n\n\tsize = max(size, vma->size);\n\talignment = max_t(typeof(alignment), alignment, vma->display_alignment);\n\tif (flags & PIN_MAPPABLE) {\n\t\tsize = max_t(typeof(size), size, vma->fence_size);\n\t\talignment = max_t(typeof(alignment),\n\t\t\t\t  alignment, vma->fence_alignment);\n\t}\n\n\tGEM_BUG_ON(!IS_ALIGNED(size, I915_GTT_PAGE_SIZE));\n\tGEM_BUG_ON(!IS_ALIGNED(alignment, I915_GTT_MIN_ALIGNMENT));\n\tGEM_BUG_ON(!is_power_of_2(alignment));\n\n\tguard = vma->guard;  \n\tif (flags & PIN_OFFSET_GUARD) {\n\t\tGEM_BUG_ON(overflows_type(flags & PIN_OFFSET_MASK, u32));\n\t\tguard = max_t(u32, guard, flags & PIN_OFFSET_MASK);\n\t}\n\t \n\tguard = ALIGN(guard, alignment);\n\n\tstart = flags & PIN_OFFSET_BIAS ? flags & PIN_OFFSET_MASK : 0;\n\tGEM_BUG_ON(!IS_ALIGNED(start, I915_GTT_PAGE_SIZE));\n\n\tend = vma->vm->total;\n\tif (flags & PIN_MAPPABLE)\n\t\tend = min_t(u64, end, i915_vm_to_ggtt(vma->vm)->mappable_end);\n\tif (flags & PIN_ZONE_4G)\n\t\tend = min_t(u64, end, (1ULL << 32) - I915_GTT_PAGE_SIZE);\n\tGEM_BUG_ON(!IS_ALIGNED(end, I915_GTT_PAGE_SIZE));\n\n\talignment = max(alignment, i915_vm_obj_min_alignment(vma->vm, vma->obj));\n\n\t \n\tif (size > end - 2 * guard) {\n\t\tdrm_dbg(vma->obj->base.dev,\n\t\t\t\"Attempting to bind an object larger than the aperture: request=%llu > %s aperture=%llu\\n\",\n\t\t\tsize, flags & PIN_MAPPABLE ? \"mappable\" : \"total\", end);\n\t\treturn -ENOSPC;\n\t}\n\n\tcolor = 0;\n\n\tif (i915_vm_has_cache_coloring(vma->vm))\n\t\tcolor = vma->obj->pat_index;\n\n\tif (flags & PIN_OFFSET_FIXED) {\n\t\tu64 offset = flags & PIN_OFFSET_MASK;\n\t\tif (!IS_ALIGNED(offset, alignment) ||\n\t\t    range_overflows(offset, size, end))\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (offset < guard || offset + size > end - guard)\n\t\t\treturn -ENOSPC;\n\n\t\tret = i915_gem_gtt_reserve(vma->vm, ww, &vma->node,\n\t\t\t\t\t   size + 2 * guard,\n\t\t\t\t\t   offset - guard,\n\t\t\t\t\t   color, flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tsize += 2 * guard;\n\t\t \n\t\tif (upper_32_bits(end - 1) &&\n\t\t    vma->page_sizes.sg > I915_GTT_PAGE_SIZE &&\n\t\t    !HAS_64K_PAGES(vma->vm->i915)) {\n\t\t\t \n\t\t\tu64 page_alignment =\n\t\t\t\trounddown_pow_of_two(vma->page_sizes.sg |\n\t\t\t\t\t\t     I915_GTT_PAGE_SIZE_2M);\n\n\t\t\t \n\t\t\tGEM_BUG_ON(i915_vma_is_ggtt(vma));\n\n\t\t\talignment = max(alignment, page_alignment);\n\n\t\t\tif (vma->page_sizes.sg & I915_GTT_PAGE_SIZE_64K)\n\t\t\t\tsize = round_up(size, I915_GTT_PAGE_SIZE_2M);\n\t\t}\n\n\t\tret = i915_gem_gtt_insert(vma->vm, ww, &vma->node,\n\t\t\t\t\t  size, alignment, color,\n\t\t\t\t\t  start, end, flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tGEM_BUG_ON(vma->node.start < start);\n\t\tGEM_BUG_ON(vma->node.start + vma->node.size > end);\n\t}\n\tGEM_BUG_ON(!drm_mm_node_allocated(&vma->node));\n\tGEM_BUG_ON(!i915_gem_valid_gtt_space(vma, color));\n\n\tlist_move_tail(&vma->vm_link, &vma->vm->bound_list);\n\tvma->guard = guard;\n\n\treturn 0;\n}\n\nstatic void\ni915_vma_detach(struct i915_vma *vma)\n{\n\tGEM_BUG_ON(!drm_mm_node_allocated(&vma->node));\n\tGEM_BUG_ON(i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND | I915_VMA_LOCAL_BIND));\n\n\t \n\tlist_move_tail(&vma->vm_link, &vma->vm->unbound_list);\n}\n\nstatic bool try_qad_pin(struct i915_vma *vma, unsigned int flags)\n{\n\tunsigned int bound;\n\n\tbound = atomic_read(&vma->flags);\n\n\tif (flags & PIN_VALIDATE) {\n\t\tflags &= I915_VMA_BIND_MASK;\n\n\t\treturn (flags & bound) == flags;\n\t}\n\n\t \n\tflags &= I915_VMA_BIND_MASK;\n\tdo {\n\t\tif (unlikely(flags & ~bound))\n\t\t\treturn false;\n\n\t\tif (unlikely(bound & (I915_VMA_OVERFLOW | I915_VMA_ERROR)))\n\t\t\treturn false;\n\n\t\tGEM_BUG_ON(((bound + 1) & I915_VMA_PIN_MASK) == 0);\n\t} while (!atomic_try_cmpxchg(&vma->flags, &bound, bound + 1));\n\n\treturn true;\n}\n\nstatic struct scatterlist *\nrotate_pages(struct drm_i915_gem_object *obj, unsigned int offset,\n\t     unsigned int width, unsigned int height,\n\t     unsigned int src_stride, unsigned int dst_stride,\n\t     struct sg_table *st, struct scatterlist *sg)\n{\n\tunsigned int column, row;\n\tpgoff_t src_idx;\n\n\tfor (column = 0; column < width; column++) {\n\t\tunsigned int left;\n\n\t\tsrc_idx = src_stride * (height - 1) + column + offset;\n\t\tfor (row = 0; row < height; row++) {\n\t\t\tst->nents++;\n\t\t\t \n\t\t\tsg_set_page(sg, NULL, I915_GTT_PAGE_SIZE, 0);\n\t\t\tsg_dma_address(sg) =\n\t\t\t\ti915_gem_object_get_dma_address(obj, src_idx);\n\t\t\tsg_dma_len(sg) = I915_GTT_PAGE_SIZE;\n\t\t\tsg = sg_next(sg);\n\t\t\tsrc_idx -= src_stride;\n\t\t}\n\n\t\tleft = (dst_stride - height) * I915_GTT_PAGE_SIZE;\n\n\t\tif (!left)\n\t\t\tcontinue;\n\n\t\tst->nents++;\n\n\t\t \n\t\tsg_set_page(sg, NULL, left, 0);\n\t\tsg_dma_address(sg) = 0;\n\t\tsg_dma_len(sg) = left;\n\t\tsg = sg_next(sg);\n\t}\n\n\treturn sg;\n}\n\nstatic noinline struct sg_table *\nintel_rotate_pages(struct intel_rotation_info *rot_info,\n\t\t   struct drm_i915_gem_object *obj)\n{\n\tunsigned int size = intel_rotation_info_size(rot_info);\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct sg_table *st;\n\tstruct scatterlist *sg;\n\tint ret = -ENOMEM;\n\tint i;\n\n\t \n\tst = kmalloc(sizeof(*st), GFP_KERNEL);\n\tif (!st)\n\t\tgoto err_st_alloc;\n\n\tret = sg_alloc_table(st, size, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err_sg_alloc;\n\n\tst->nents = 0;\n\tsg = st->sgl;\n\n\tfor (i = 0 ; i < ARRAY_SIZE(rot_info->plane); i++)\n\t\tsg = rotate_pages(obj, rot_info->plane[i].offset,\n\t\t\t\t  rot_info->plane[i].width, rot_info->plane[i].height,\n\t\t\t\t  rot_info->plane[i].src_stride,\n\t\t\t\t  rot_info->plane[i].dst_stride,\n\t\t\t\t  st, sg);\n\n\treturn st;\n\nerr_sg_alloc:\n\tkfree(st);\nerr_st_alloc:\n\n\tdrm_dbg(&i915->drm, \"Failed to create rotated mapping for object size %zu! (%ux%u tiles, %u pages)\\n\",\n\t\tobj->base.size, rot_info->plane[0].width,\n\t\trot_info->plane[0].height, size);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic struct scatterlist *\nadd_padding_pages(unsigned int count,\n\t\t  struct sg_table *st, struct scatterlist *sg)\n{\n\tst->nents++;\n\n\t \n\tsg_set_page(sg, NULL, count * I915_GTT_PAGE_SIZE, 0);\n\tsg_dma_address(sg) = 0;\n\tsg_dma_len(sg) = count * I915_GTT_PAGE_SIZE;\n\tsg = sg_next(sg);\n\n\treturn sg;\n}\n\nstatic struct scatterlist *\nremap_tiled_color_plane_pages(struct drm_i915_gem_object *obj,\n\t\t\t      unsigned long offset, unsigned int alignment_pad,\n\t\t\t      unsigned int width, unsigned int height,\n\t\t\t      unsigned int src_stride, unsigned int dst_stride,\n\t\t\t      struct sg_table *st, struct scatterlist *sg,\n\t\t\t      unsigned int *gtt_offset)\n{\n\tunsigned int row;\n\n\tif (!width || !height)\n\t\treturn sg;\n\n\tif (alignment_pad)\n\t\tsg = add_padding_pages(alignment_pad, st, sg);\n\n\tfor (row = 0; row < height; row++) {\n\t\tunsigned int left = width * I915_GTT_PAGE_SIZE;\n\n\t\twhile (left) {\n\t\t\tdma_addr_t addr;\n\t\t\tunsigned int length;\n\n\t\t\t \n\n\t\t\taddr = i915_gem_object_get_dma_address_len(obj, offset, &length);\n\n\t\t\tlength = min(left, length);\n\n\t\t\tst->nents++;\n\n\t\t\tsg_set_page(sg, NULL, length, 0);\n\t\t\tsg_dma_address(sg) = addr;\n\t\t\tsg_dma_len(sg) = length;\n\t\t\tsg = sg_next(sg);\n\n\t\t\toffset += length / I915_GTT_PAGE_SIZE;\n\t\t\tleft -= length;\n\t\t}\n\n\t\toffset += src_stride - width;\n\n\t\tleft = (dst_stride - width) * I915_GTT_PAGE_SIZE;\n\n\t\tif (!left)\n\t\t\tcontinue;\n\n\t\tsg = add_padding_pages(left >> PAGE_SHIFT, st, sg);\n\t}\n\n\t*gtt_offset += alignment_pad + dst_stride * height;\n\n\treturn sg;\n}\n\nstatic struct scatterlist *\nremap_contiguous_pages(struct drm_i915_gem_object *obj,\n\t\t       pgoff_t obj_offset,\n\t\t       unsigned int count,\n\t\t       struct sg_table *st, struct scatterlist *sg)\n{\n\tstruct scatterlist *iter;\n\tunsigned int offset;\n\n\titer = i915_gem_object_get_sg_dma(obj, obj_offset, &offset);\n\tGEM_BUG_ON(!iter);\n\n\tdo {\n\t\tunsigned int len;\n\n\t\tlen = min(sg_dma_len(iter) - (offset << PAGE_SHIFT),\n\t\t\t  count << PAGE_SHIFT);\n\t\tsg_set_page(sg, NULL, len, 0);\n\t\tsg_dma_address(sg) =\n\t\t\tsg_dma_address(iter) + (offset << PAGE_SHIFT);\n\t\tsg_dma_len(sg) = len;\n\n\t\tst->nents++;\n\t\tcount -= len >> PAGE_SHIFT;\n\t\tif (count == 0)\n\t\t\treturn sg;\n\n\t\tsg = __sg_next(sg);\n\t\titer = __sg_next(iter);\n\t\toffset = 0;\n\t} while (1);\n}\n\nstatic struct scatterlist *\nremap_linear_color_plane_pages(struct drm_i915_gem_object *obj,\n\t\t\t       pgoff_t obj_offset, unsigned int alignment_pad,\n\t\t\t       unsigned int size,\n\t\t\t       struct sg_table *st, struct scatterlist *sg,\n\t\t\t       unsigned int *gtt_offset)\n{\n\tif (!size)\n\t\treturn sg;\n\n\tif (alignment_pad)\n\t\tsg = add_padding_pages(alignment_pad, st, sg);\n\n\tsg = remap_contiguous_pages(obj, obj_offset, size, st, sg);\n\tsg = sg_next(sg);\n\n\t*gtt_offset += alignment_pad + size;\n\n\treturn sg;\n}\n\nstatic struct scatterlist *\nremap_color_plane_pages(const struct intel_remapped_info *rem_info,\n\t\t\tstruct drm_i915_gem_object *obj,\n\t\t\tint color_plane,\n\t\t\tstruct sg_table *st, struct scatterlist *sg,\n\t\t\tunsigned int *gtt_offset)\n{\n\tunsigned int alignment_pad = 0;\n\n\tif (rem_info->plane_alignment)\n\t\talignment_pad = ALIGN(*gtt_offset, rem_info->plane_alignment) - *gtt_offset;\n\n\tif (rem_info->plane[color_plane].linear)\n\t\tsg = remap_linear_color_plane_pages(obj,\n\t\t\t\t\t\t    rem_info->plane[color_plane].offset,\n\t\t\t\t\t\t    alignment_pad,\n\t\t\t\t\t\t    rem_info->plane[color_plane].size,\n\t\t\t\t\t\t    st, sg,\n\t\t\t\t\t\t    gtt_offset);\n\n\telse\n\t\tsg = remap_tiled_color_plane_pages(obj,\n\t\t\t\t\t\t   rem_info->plane[color_plane].offset,\n\t\t\t\t\t\t   alignment_pad,\n\t\t\t\t\t\t   rem_info->plane[color_plane].width,\n\t\t\t\t\t\t   rem_info->plane[color_plane].height,\n\t\t\t\t\t\t   rem_info->plane[color_plane].src_stride,\n\t\t\t\t\t\t   rem_info->plane[color_plane].dst_stride,\n\t\t\t\t\t\t   st, sg,\n\t\t\t\t\t\t   gtt_offset);\n\n\treturn sg;\n}\n\nstatic noinline struct sg_table *\nintel_remap_pages(struct intel_remapped_info *rem_info,\n\t\t  struct drm_i915_gem_object *obj)\n{\n\tunsigned int size = intel_remapped_info_size(rem_info);\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct sg_table *st;\n\tstruct scatterlist *sg;\n\tunsigned int gtt_offset = 0;\n\tint ret = -ENOMEM;\n\tint i;\n\n\t \n\tst = kmalloc(sizeof(*st), GFP_KERNEL);\n\tif (!st)\n\t\tgoto err_st_alloc;\n\n\tret = sg_alloc_table(st, size, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err_sg_alloc;\n\n\tst->nents = 0;\n\tsg = st->sgl;\n\n\tfor (i = 0 ; i < ARRAY_SIZE(rem_info->plane); i++)\n\t\tsg = remap_color_plane_pages(rem_info, obj, i, st, sg, &gtt_offset);\n\n\ti915_sg_trim(st);\n\n\treturn st;\n\nerr_sg_alloc:\n\tkfree(st);\nerr_st_alloc:\n\n\tdrm_dbg(&i915->drm, \"Failed to create remapped mapping for object size %zu! (%ux%u tiles, %u pages)\\n\",\n\t\tobj->base.size, rem_info->plane[0].width,\n\t\trem_info->plane[0].height, size);\n\n\treturn ERR_PTR(ret);\n}\n\nstatic noinline struct sg_table *\nintel_partial_pages(const struct i915_gtt_view *view,\n\t\t    struct drm_i915_gem_object *obj)\n{\n\tstruct sg_table *st;\n\tstruct scatterlist *sg;\n\tunsigned int count = view->partial.size;\n\tint ret = -ENOMEM;\n\n\tst = kmalloc(sizeof(*st), GFP_KERNEL);\n\tif (!st)\n\t\tgoto err_st_alloc;\n\n\tret = sg_alloc_table(st, count, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err_sg_alloc;\n\n\tst->nents = 0;\n\n\tsg = remap_contiguous_pages(obj, view->partial.offset, count, st, st->sgl);\n\n\tsg_mark_end(sg);\n\ti915_sg_trim(st);  \n\n\treturn st;\n\nerr_sg_alloc:\n\tkfree(st);\nerr_st_alloc:\n\treturn ERR_PTR(ret);\n}\n\nstatic int\n__i915_vma_get_pages(struct i915_vma *vma)\n{\n\tstruct sg_table *pages;\n\n\t \n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(vma->obj));\n\n\tswitch (vma->gtt_view.type) {\n\tdefault:\n\t\tGEM_BUG_ON(vma->gtt_view.type);\n\t\tfallthrough;\n\tcase I915_GTT_VIEW_NORMAL:\n\t\tpages = vma->obj->mm.pages;\n\t\tbreak;\n\n\tcase I915_GTT_VIEW_ROTATED:\n\t\tpages =\n\t\t\tintel_rotate_pages(&vma->gtt_view.rotated, vma->obj);\n\t\tbreak;\n\n\tcase I915_GTT_VIEW_REMAPPED:\n\t\tpages =\n\t\t\tintel_remap_pages(&vma->gtt_view.remapped, vma->obj);\n\t\tbreak;\n\n\tcase I915_GTT_VIEW_PARTIAL:\n\t\tpages = intel_partial_pages(&vma->gtt_view, vma->obj);\n\t\tbreak;\n\t}\n\n\tif (IS_ERR(pages)) {\n\t\tdrm_err(&vma->vm->i915->drm,\n\t\t\t\"Failed to get pages for VMA view type %u (%ld)!\\n\",\n\t\t\tvma->gtt_view.type, PTR_ERR(pages));\n\t\treturn PTR_ERR(pages);\n\t}\n\n\tvma->pages = pages;\n\n\treturn 0;\n}\n\nI915_SELFTEST_EXPORT int i915_vma_get_pages(struct i915_vma *vma)\n{\n\tint err;\n\n\tif (atomic_add_unless(&vma->pages_count, 1, 0))\n\t\treturn 0;\n\n\terr = i915_gem_object_pin_pages(vma->obj);\n\tif (err)\n\t\treturn err;\n\n\terr = __i915_vma_get_pages(vma);\n\tif (err)\n\t\tgoto err_unpin;\n\n\tvma->page_sizes = vma->obj->mm.page_sizes;\n\tatomic_inc(&vma->pages_count);\n\n\treturn 0;\n\nerr_unpin:\n\t__i915_gem_object_unpin_pages(vma->obj);\n\n\treturn err;\n}\n\nvoid vma_invalidate_tlb(struct i915_address_space *vm, u32 *tlb)\n{\n\tstruct intel_gt *gt;\n\tint id;\n\n\tif (!tlb)\n\t\treturn;\n\n\t \n\tfor_each_gt(gt, vm->i915, id)\n\t\tWRITE_ONCE(tlb[id],\n\t\t\t   intel_gt_next_invalidate_tlb_full(gt));\n}\n\nstatic void __vma_put_pages(struct i915_vma *vma, unsigned int count)\n{\n\t \n\tGEM_BUG_ON(atomic_read(&vma->pages_count) < count);\n\n\tif (atomic_sub_return(count, &vma->pages_count) == 0) {\n\t\tif (vma->pages != vma->obj->mm.pages) {\n\t\t\tsg_free_table(vma->pages);\n\t\t\tkfree(vma->pages);\n\t\t}\n\t\tvma->pages = NULL;\n\n\t\ti915_gem_object_unpin_pages(vma->obj);\n\t}\n}\n\nI915_SELFTEST_EXPORT void i915_vma_put_pages(struct i915_vma *vma)\n{\n\tif (atomic_add_unless(&vma->pages_count, -1, 1))\n\t\treturn;\n\n\t__vma_put_pages(vma, 1);\n}\n\nstatic void vma_unbind_pages(struct i915_vma *vma)\n{\n\tunsigned int count;\n\n\tlockdep_assert_held(&vma->vm->mutex);\n\n\t \n\tcount = atomic_read(&vma->pages_count);\n\tcount >>= I915_VMA_PAGES_BIAS;\n\tGEM_BUG_ON(!count);\n\n\t__vma_put_pages(vma, count | count << I915_VMA_PAGES_BIAS);\n}\n\nint i915_vma_pin_ww(struct i915_vma *vma, struct i915_gem_ww_ctx *ww,\n\t\t    u64 size, u64 alignment, u64 flags)\n{\n\tstruct i915_vma_work *work = NULL;\n\tstruct dma_fence *moving = NULL;\n\tstruct i915_vma_resource *vma_res = NULL;\n\tintel_wakeref_t wakeref = 0;\n\tunsigned int bound;\n\tint err;\n\n\tassert_vma_held(vma);\n\tGEM_BUG_ON(!ww);\n\n\tBUILD_BUG_ON(PIN_GLOBAL != I915_VMA_GLOBAL_BIND);\n\tBUILD_BUG_ON(PIN_USER != I915_VMA_LOCAL_BIND);\n\n\tGEM_BUG_ON(!(flags & (PIN_USER | PIN_GLOBAL)));\n\n\t \n\tif (try_qad_pin(vma, flags))\n\t\treturn 0;\n\n\terr = i915_vma_get_pages(vma);\n\tif (err)\n\t\treturn err;\n\n\tif (flags & PIN_GLOBAL)\n\t\twakeref = intel_runtime_pm_get(&vma->vm->i915->runtime_pm);\n\n\tif (flags & vma->vm->bind_async_flags) {\n\t\t \n\t\terr = i915_vm_lock_objects(vma->vm, ww);\n\t\tif (err)\n\t\t\tgoto err_rpm;\n\n\t\twork = i915_vma_work();\n\t\tif (!work) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_rpm;\n\t\t}\n\n\t\twork->vm = vma->vm;\n\n\t\terr = i915_gem_object_get_moving_fence(vma->obj, &moving);\n\t\tif (err)\n\t\t\tgoto err_rpm;\n\n\t\tdma_fence_work_chain(&work->base, moving);\n\n\t\t \n\t\tif (vma->vm->allocate_va_range) {\n\t\t\terr = i915_vm_alloc_pt_stash(vma->vm,\n\t\t\t\t\t\t     &work->stash,\n\t\t\t\t\t\t     vma->size);\n\t\t\tif (err)\n\t\t\t\tgoto err_fence;\n\n\t\t\terr = i915_vm_map_pt_stash(vma->vm, &work->stash);\n\t\t\tif (err)\n\t\t\t\tgoto err_fence;\n\t\t}\n\t}\n\n\tvma_res = i915_vma_resource_alloc();\n\tif (IS_ERR(vma_res)) {\n\t\terr = PTR_ERR(vma_res);\n\t\tgoto err_fence;\n\t}\n\n\t \n\terr = mutex_lock_interruptible_nested(&vma->vm->mutex,\n\t\t\t\t\t      !(flags & PIN_GLOBAL));\n\tif (err)\n\t\tgoto err_vma_res;\n\n\t \n\n\tif (unlikely(i915_vma_is_closed(vma))) {\n\t\terr = -ENOENT;\n\t\tgoto err_unlock;\n\t}\n\n\tbound = atomic_read(&vma->flags);\n\tif (unlikely(bound & I915_VMA_ERROR)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_unlock;\n\t}\n\n\tif (unlikely(!((bound + 1) & I915_VMA_PIN_MASK))) {\n\t\terr = -EAGAIN;  \n\t\tgoto err_unlock;\n\t}\n\n\tif (unlikely(!(flags & ~bound & I915_VMA_BIND_MASK))) {\n\t\tif (!(flags & PIN_VALIDATE))\n\t\t\t__i915_vma_pin(vma);\n\t\tgoto err_unlock;\n\t}\n\n\terr = i915_active_acquire(&vma->active);\n\tif (err)\n\t\tgoto err_unlock;\n\n\tif (!(bound & I915_VMA_BIND_MASK)) {\n\t\terr = i915_vma_insert(vma, ww, size, alignment, flags);\n\t\tif (err)\n\t\t\tgoto err_active;\n\n\t\tif (i915_is_ggtt(vma->vm))\n\t\t\t__i915_vma_set_map_and_fenceable(vma);\n\t}\n\n\tGEM_BUG_ON(!vma->pages);\n\terr = i915_vma_bind(vma,\n\t\t\t    vma->obj->pat_index,\n\t\t\t    flags, work, vma_res);\n\tvma_res = NULL;\n\tif (err)\n\t\tgoto err_remove;\n\n\t \n\tGEM_BUG_ON(bound + I915_VMA_PAGES_ACTIVE < bound);\n\tatomic_add(I915_VMA_PAGES_ACTIVE, &vma->pages_count);\n\tlist_move_tail(&vma->vm_link, &vma->vm->bound_list);\n\n\tif (!(flags & PIN_VALIDATE)) {\n\t\t__i915_vma_pin(vma);\n\t\tGEM_BUG_ON(!i915_vma_is_pinned(vma));\n\t}\n\tGEM_BUG_ON(!i915_vma_is_bound(vma, flags));\n\tGEM_BUG_ON(i915_vma_misplaced(vma, size, alignment, flags));\n\nerr_remove:\n\tif (!i915_vma_is_bound(vma, I915_VMA_BIND_MASK)) {\n\t\ti915_vma_detach(vma);\n\t\tdrm_mm_remove_node(&vma->node);\n\t}\nerr_active:\n\ti915_active_release(&vma->active);\nerr_unlock:\n\tmutex_unlock(&vma->vm->mutex);\nerr_vma_res:\n\ti915_vma_resource_free(vma_res);\nerr_fence:\n\tif (work)\n\t\tdma_fence_work_commit_imm(&work->base);\nerr_rpm:\n\tif (wakeref)\n\t\tintel_runtime_pm_put(&vma->vm->i915->runtime_pm, wakeref);\n\n\tif (moving)\n\t\tdma_fence_put(moving);\n\n\ti915_vma_put_pages(vma);\n\treturn err;\n}\n\nstatic void flush_idle_contexts(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tfor_each_engine(engine, gt, id)\n\t\tintel_engine_flush_barriers(engine);\n\n\tintel_gt_wait_for_idle(gt, MAX_SCHEDULE_TIMEOUT);\n}\n\nstatic int __i915_ggtt_pin(struct i915_vma *vma, struct i915_gem_ww_ctx *ww,\n\t\t\t   u32 align, unsigned int flags)\n{\n\tstruct i915_address_space *vm = vma->vm;\n\tstruct intel_gt *gt;\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tint err;\n\n\tdo {\n\t\terr = i915_vma_pin_ww(vma, ww, 0, align, flags | PIN_GLOBAL);\n\n\t\tif (err != -ENOSPC) {\n\t\t\tif (!err) {\n\t\t\t\terr = i915_vma_wait_for_bind(vma);\n\t\t\t\tif (err)\n\t\t\t\t\ti915_vma_unpin(vma);\n\t\t\t}\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(gt, &ggtt->gt_list, ggtt_link)\n\t\t\tflush_idle_contexts(gt);\n\t\tif (mutex_lock_interruptible(&vm->mutex) == 0) {\n\t\t\t \n\t\t\ti915_gem_evict_vm(vm, NULL, NULL);\n\t\t\tmutex_unlock(&vm->mutex);\n\t\t}\n\t} while (1);\n}\n\nint i915_ggtt_pin(struct i915_vma *vma, struct i915_gem_ww_ctx *ww,\n\t\t  u32 align, unsigned int flags)\n{\n\tstruct i915_gem_ww_ctx _ww;\n\tint err;\n\n\tGEM_BUG_ON(!i915_vma_is_ggtt(vma));\n\n\tif (ww)\n\t\treturn __i915_ggtt_pin(vma, ww, align, flags);\n\n\tlockdep_assert_not_held(&vma->obj->base.resv->lock.base);\n\n\tfor_i915_gem_ww(&_ww, err, true) {\n\t\terr = i915_gem_object_lock(vma->obj, &_ww);\n\t\tif (!err)\n\t\t\terr = __i915_ggtt_pin(vma, &_ww, align, flags);\n\t}\n\n\treturn err;\n}\n\n \nvoid i915_ggtt_clear_scanout(struct drm_i915_gem_object *obj)\n{\n\tstruct i915_vma *vma;\n\n\tspin_lock(&obj->vma.lock);\n\tfor_each_ggtt_vma(vma, obj) {\n\t\ti915_vma_clear_scanout(vma);\n\t\tvma->display_alignment = I915_GTT_MIN_ALIGNMENT;\n\t}\n\tspin_unlock(&obj->vma.lock);\n}\n\nstatic void __vma_close(struct i915_vma *vma, struct intel_gt *gt)\n{\n\t \n\tGEM_BUG_ON(i915_vma_is_closed(vma));\n\tlist_add(&vma->closed_link, &gt->closed_vma);\n}\n\nvoid i915_vma_close(struct i915_vma *vma)\n{\n\tstruct intel_gt *gt = vma->vm->gt;\n\tunsigned long flags;\n\n\tif (i915_vma_is_ggtt(vma))\n\t\treturn;\n\n\tGEM_BUG_ON(!atomic_read(&vma->open_count));\n\tif (atomic_dec_and_lock_irqsave(&vma->open_count,\n\t\t\t\t\t&gt->closed_lock,\n\t\t\t\t\tflags)) {\n\t\t__vma_close(vma, gt);\n\t\tspin_unlock_irqrestore(&gt->closed_lock, flags);\n\t}\n}\n\nstatic void __i915_vma_remove_closed(struct i915_vma *vma)\n{\n\tlist_del_init(&vma->closed_link);\n}\n\nvoid i915_vma_reopen(struct i915_vma *vma)\n{\n\tstruct intel_gt *gt = vma->vm->gt;\n\n\tspin_lock_irq(&gt->closed_lock);\n\tif (i915_vma_is_closed(vma))\n\t\t__i915_vma_remove_closed(vma);\n\tspin_unlock_irq(&gt->closed_lock);\n}\n\nstatic void force_unbind(struct i915_vma *vma)\n{\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn;\n\n\tatomic_and(~I915_VMA_PIN_MASK, &vma->flags);\n\tWARN_ON(__i915_vma_unbind(vma));\n\tGEM_BUG_ON(drm_mm_node_allocated(&vma->node));\n}\n\nstatic void release_references(struct i915_vma *vma, struct intel_gt *gt,\n\t\t\t       bool vm_ddestroy)\n{\n\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\tGEM_BUG_ON(i915_vma_is_active(vma));\n\n\tspin_lock(&obj->vma.lock);\n\tlist_del(&vma->obj_link);\n\tif (!RB_EMPTY_NODE(&vma->obj_node))\n\t\trb_erase(&vma->obj_node, &obj->vma.tree);\n\n\tspin_unlock(&obj->vma.lock);\n\n\tspin_lock_irq(&gt->closed_lock);\n\t__i915_vma_remove_closed(vma);\n\tspin_unlock_irq(&gt->closed_lock);\n\n\tif (vm_ddestroy)\n\t\ti915_vm_resv_put(vma->vm);\n\n\t \n\ti915_active_wait(&vma->active);\n\ti915_active_fini(&vma->active);\n\tGEM_WARN_ON(vma->resource);\n\ti915_vma_free(vma);\n}\n\n \nvoid i915_vma_destroy_locked(struct i915_vma *vma)\n{\n\tlockdep_assert_held(&vma->vm->mutex);\n\n\tforce_unbind(vma);\n\tlist_del_init(&vma->vm_link);\n\trelease_references(vma, vma->vm->gt, false);\n}\n\nvoid i915_vma_destroy(struct i915_vma *vma)\n{\n\tstruct intel_gt *gt;\n\tbool vm_ddestroy;\n\n\tmutex_lock(&vma->vm->mutex);\n\tforce_unbind(vma);\n\tlist_del_init(&vma->vm_link);\n\tvm_ddestroy = vma->vm_ddestroy;\n\tvma->vm_ddestroy = false;\n\n\t \n\tgt = vma->vm->gt;\n\tmutex_unlock(&vma->vm->mutex);\n\trelease_references(vma, gt, vm_ddestroy);\n}\n\nvoid i915_vma_parked(struct intel_gt *gt)\n{\n\tstruct i915_vma *vma, *next;\n\tLIST_HEAD(closed);\n\n\tspin_lock_irq(&gt->closed_lock);\n\tlist_for_each_entry_safe(vma, next, &gt->closed_vma, closed_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\t\tstruct i915_address_space *vm = vma->vm;\n\n\t\t \n\n\t\tif (!kref_get_unless_zero(&obj->base.refcount))\n\t\t\tcontinue;\n\n\t\tif (!i915_vm_tryget(vm)) {\n\t\t\ti915_gem_object_put(obj);\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move(&vma->closed_link, &closed);\n\t}\n\tspin_unlock_irq(&gt->closed_lock);\n\n\t \n\tlist_for_each_entry_safe(vma, next, &closed, closed_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\t\tstruct i915_address_space *vm = vma->vm;\n\n\t\tif (i915_gem_object_trylock(obj, NULL)) {\n\t\t\tINIT_LIST_HEAD(&vma->closed_link);\n\t\t\ti915_vma_destroy(vma);\n\t\t\ti915_gem_object_unlock(obj);\n\t\t} else {\n\t\t\t \n\t\t\tspin_lock_irq(&gt->closed_lock);\n\t\t\tlist_add(&vma->closed_link, &gt->closed_vma);\n\t\t\tspin_unlock_irq(&gt->closed_lock);\n\t\t}\n\n\t\ti915_gem_object_put(obj);\n\t\ti915_vm_put(vm);\n\t}\n}\n\nstatic void __i915_vma_iounmap(struct i915_vma *vma)\n{\n\tGEM_BUG_ON(i915_vma_is_pinned(vma));\n\n\tif (vma->iomap == NULL)\n\t\treturn;\n\n\tif (page_unmask_bits(vma->iomap))\n\t\t__i915_gem_object_release_map(vma->obj);\n\telse\n\t\tio_mapping_unmap(vma->iomap);\n\tvma->iomap = NULL;\n}\n\nvoid i915_vma_revoke_mmap(struct i915_vma *vma)\n{\n\tstruct drm_vma_offset_node *node;\n\tu64 vma_offset;\n\n\tif (!i915_vma_has_userfault(vma))\n\t\treturn;\n\n\tGEM_BUG_ON(!i915_vma_is_map_and_fenceable(vma));\n\tGEM_BUG_ON(!vma->obj->userfault_count);\n\n\tnode = &vma->mmo->vma_node;\n\tvma_offset = vma->gtt_view.partial.offset << PAGE_SHIFT;\n\tunmap_mapping_range(vma->vm->i915->drm.anon_inode->i_mapping,\n\t\t\t    drm_vma_node_offset_addr(node) + vma_offset,\n\t\t\t    vma->size,\n\t\t\t    1);\n\n\ti915_vma_unset_userfault(vma);\n\tif (!--vma->obj->userfault_count)\n\t\tlist_del(&vma->obj->userfault_link);\n}\n\nstatic int\n__i915_request_await_bind(struct i915_request *rq, struct i915_vma *vma)\n{\n\treturn __i915_request_await_exclusive(rq, &vma->active);\n}\n\nstatic int __i915_vma_move_to_active(struct i915_vma *vma, struct i915_request *rq)\n{\n\tint err;\n\n\t \n\terr = __i915_request_await_bind(rq, vma);\n\tif (err)\n\t\treturn err;\n\n\treturn i915_active_add_request(&vma->active, rq);\n}\n\nint _i915_vma_move_to_active(struct i915_vma *vma,\n\t\t\t     struct i915_request *rq,\n\t\t\t     struct dma_fence *fence,\n\t\t\t     unsigned int flags)\n{\n\tstruct drm_i915_gem_object *obj = vma->obj;\n\tint err;\n\n\tassert_object_held(obj);\n\n\tGEM_BUG_ON(!vma->pages);\n\n\tif (!(flags & __EXEC_OBJECT_NO_REQUEST_AWAIT)) {\n\t\terr = i915_request_await_object(rq, vma->obj, flags & EXEC_OBJECT_WRITE);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\terr = __i915_vma_move_to_active(vma, rq);\n\tif (unlikely(err))\n\t\treturn err;\n\n\t \n\tif (fence && !(flags & __EXEC_OBJECT_NO_RESERVE)) {\n\t\tstruct dma_fence *curr;\n\t\tint idx;\n\n\t\tdma_fence_array_for_each(curr, idx, fence)\n\t\t\t;\n\t\terr = dma_resv_reserve_fences(vma->obj->base.resv, idx);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\tif (flags & EXEC_OBJECT_WRITE) {\n\t\tstruct intel_frontbuffer *front;\n\n\t\tfront = i915_gem_object_get_frontbuffer(obj);\n\t\tif (unlikely(front)) {\n\t\t\tif (intel_frontbuffer_invalidate(front, ORIGIN_CS))\n\t\t\t\ti915_active_add_request(&front->write, rq);\n\t\t\tintel_frontbuffer_put(front);\n\t\t}\n\t}\n\n\tif (fence) {\n\t\tstruct dma_fence *curr;\n\t\tenum dma_resv_usage usage;\n\t\tint idx;\n\n\t\tif (flags & EXEC_OBJECT_WRITE) {\n\t\t\tusage = DMA_RESV_USAGE_WRITE;\n\t\t\tobj->write_domain = I915_GEM_DOMAIN_RENDER;\n\t\t\tobj->read_domains = 0;\n\t\t} else {\n\t\t\tusage = DMA_RESV_USAGE_READ;\n\t\t\tobj->write_domain = 0;\n\t\t}\n\n\t\tdma_fence_array_for_each(curr, idx, fence)\n\t\t\tdma_resv_add_fence(vma->obj->base.resv, curr, usage);\n\t}\n\n\tif (flags & EXEC_OBJECT_NEEDS_FENCE && vma->fence)\n\t\ti915_active_add_request(&vma->fence->active, rq);\n\n\tobj->read_domains |= I915_GEM_GPU_DOMAINS;\n\tobj->mm.dirty = true;\n\n\tGEM_BUG_ON(!i915_vma_is_active(vma));\n\treturn 0;\n}\n\nstruct dma_fence *__i915_vma_evict(struct i915_vma *vma, bool async)\n{\n\tstruct i915_vma_resource *vma_res = vma->resource;\n\tstruct dma_fence *unbind_fence;\n\n\tGEM_BUG_ON(i915_vma_is_pinned(vma));\n\tassert_vma_held_evict(vma);\n\n\tif (i915_vma_is_map_and_fenceable(vma)) {\n\t\t \n\t\ti915_vma_revoke_mmap(vma);\n\n\t\t \n\t\ti915_vma_flush_writes(vma);\n\n\t\t \n\t\ti915_vma_revoke_fence(vma);\n\n\t\tclear_bit(I915_VMA_CAN_FENCE_BIT, __i915_vma_flags(vma));\n\t}\n\n\t__i915_vma_iounmap(vma);\n\n\tGEM_BUG_ON(vma->fence);\n\tGEM_BUG_ON(i915_vma_has_userfault(vma));\n\n\t \n\tGEM_WARN_ON(async && !vma->resource->bi.pages_rsgt);\n\n\t \n\tvma_res->needs_wakeref = i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND) &&\n\t\tkref_read(&vma->vm->ref);\n\tvma_res->skip_pte_rewrite = !kref_read(&vma->vm->ref) ||\n\t\tvma->vm->skip_pte_rewrite;\n\ttrace_i915_vma_unbind(vma);\n\n\tif (async)\n\t\tunbind_fence = i915_vma_resource_unbind(vma_res,\n\t\t\t\t\t\t\tvma->obj->mm.tlb);\n\telse\n\t\tunbind_fence = i915_vma_resource_unbind(vma_res, NULL);\n\n\tvma->resource = NULL;\n\n\tatomic_and(~(I915_VMA_BIND_MASK | I915_VMA_ERROR | I915_VMA_GGTT_WRITE),\n\t\t   &vma->flags);\n\n\ti915_vma_detach(vma);\n\n\tif (!async) {\n\t\tif (unbind_fence) {\n\t\t\tdma_fence_wait(unbind_fence, false);\n\t\t\tdma_fence_put(unbind_fence);\n\t\t\tunbind_fence = NULL;\n\t\t}\n\t\tvma_invalidate_tlb(vma->vm, vma->obj->mm.tlb);\n\t}\n\n\t \n\n\tvma_unbind_pages(vma);\n\treturn unbind_fence;\n}\n\nint __i915_vma_unbind(struct i915_vma *vma)\n{\n\tint ret;\n\n\tlockdep_assert_held(&vma->vm->mutex);\n\tassert_vma_held_evict(vma);\n\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn 0;\n\n\tif (i915_vma_is_pinned(vma)) {\n\t\tvma_print_allocator(vma, \"is pinned\");\n\t\treturn -EAGAIN;\n\t}\n\n\t \n\tret = i915_vma_sync(vma);\n\tif (ret)\n\t\treturn ret;\n\n\tGEM_BUG_ON(i915_vma_is_active(vma));\n\t__i915_vma_evict(vma, false);\n\n\tdrm_mm_remove_node(&vma->node);  \n\treturn 0;\n}\n\nstatic struct dma_fence *__i915_vma_unbind_async(struct i915_vma *vma)\n{\n\tstruct dma_fence *fence;\n\n\tlockdep_assert_held(&vma->vm->mutex);\n\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn NULL;\n\n\tif (i915_vma_is_pinned(vma) ||\n\t    &vma->obj->mm.rsgt->table != vma->resource->bi.pages)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\t \n\tif (i915_sw_fence_await_active(&vma->resource->chain, &vma->active,\n\t\t\t\t       I915_ACTIVE_AWAIT_EXCL |\n\t\t\t\t       I915_ACTIVE_AWAIT_ACTIVE) < 0) {\n\t\treturn ERR_PTR(-EBUSY);\n\t}\n\n\tfence = __i915_vma_evict(vma, true);\n\n\tdrm_mm_remove_node(&vma->node);  \n\n\treturn fence;\n}\n\nint i915_vma_unbind(struct i915_vma *vma)\n{\n\tstruct i915_address_space *vm = vma->vm;\n\tintel_wakeref_t wakeref = 0;\n\tint err;\n\n\tassert_object_held_shared(vma->obj);\n\n\t \n\terr = i915_vma_sync(vma);\n\tif (err)\n\t\treturn err;\n\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn 0;\n\n\tif (i915_vma_is_pinned(vma)) {\n\t\tvma_print_allocator(vma, \"is pinned\");\n\t\treturn -EAGAIN;\n\t}\n\n\tif (i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND))\n\t\t \n\t\twakeref = intel_runtime_pm_get(&vm->i915->runtime_pm);\n\n\terr = mutex_lock_interruptible_nested(&vma->vm->mutex, !wakeref);\n\tif (err)\n\t\tgoto out_rpm;\n\n\terr = __i915_vma_unbind(vma);\n\tmutex_unlock(&vm->mutex);\n\nout_rpm:\n\tif (wakeref)\n\t\tintel_runtime_pm_put(&vm->i915->runtime_pm, wakeref);\n\treturn err;\n}\n\nint i915_vma_unbind_async(struct i915_vma *vma, bool trylock_vm)\n{\n\tstruct drm_i915_gem_object *obj = vma->obj;\n\tstruct i915_address_space *vm = vma->vm;\n\tintel_wakeref_t wakeref = 0;\n\tstruct dma_fence *fence;\n\tint err;\n\n\t \n\tassert_object_held(obj);\n\n\tif (!drm_mm_node_allocated(&vma->node))\n\t\treturn 0;\n\n\tif (i915_vma_is_pinned(vma)) {\n\t\tvma_print_allocator(vma, \"is pinned\");\n\t\treturn -EAGAIN;\n\t}\n\n\tif (!obj->mm.rsgt)\n\t\treturn -EBUSY;\n\n\terr = dma_resv_reserve_fences(obj->base.resv, 2);\n\tif (err)\n\t\treturn -EBUSY;\n\n\t \n\tif (i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND))\n\t\twakeref = intel_runtime_pm_get(&vm->i915->runtime_pm);\n\n\tif (trylock_vm && !mutex_trylock(&vm->mutex)) {\n\t\terr = -EBUSY;\n\t\tgoto out_rpm;\n\t} else if (!trylock_vm) {\n\t\terr = mutex_lock_interruptible_nested(&vm->mutex, !wakeref);\n\t\tif (err)\n\t\t\tgoto out_rpm;\n\t}\n\n\tfence = __i915_vma_unbind_async(vma);\n\tmutex_unlock(&vm->mutex);\n\tif (IS_ERR_OR_NULL(fence)) {\n\t\terr = PTR_ERR_OR_ZERO(fence);\n\t\tgoto out_rpm;\n\t}\n\n\tdma_resv_add_fence(obj->base.resv, fence, DMA_RESV_USAGE_READ);\n\tdma_fence_put(fence);\n\nout_rpm:\n\tif (wakeref)\n\t\tintel_runtime_pm_put(&vm->i915->runtime_pm, wakeref);\n\treturn err;\n}\n\nint i915_vma_unbind_unlocked(struct i915_vma *vma)\n{\n\tint err;\n\n\ti915_gem_object_lock(vma->obj, NULL);\n\terr = i915_vma_unbind(vma);\n\ti915_gem_object_unlock(vma->obj);\n\n\treturn err;\n}\n\nstruct i915_vma *i915_vma_make_unshrinkable(struct i915_vma *vma)\n{\n\ti915_gem_object_make_unshrinkable(vma->obj);\n\treturn vma;\n}\n\nvoid i915_vma_make_shrinkable(struct i915_vma *vma)\n{\n\ti915_gem_object_make_shrinkable(vma->obj);\n}\n\nvoid i915_vma_make_purgeable(struct i915_vma *vma)\n{\n\ti915_gem_object_make_purgeable(vma->obj);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/i915_vma.c\"\n#endif\n\nvoid i915_vma_module_exit(void)\n{\n\tkmem_cache_destroy(slab_vmas);\n}\n\nint __init i915_vma_module_init(void)\n{\n\tslab_vmas = KMEM_CACHE(i915_vma, SLAB_HWCACHE_ALIGN);\n\tif (!slab_vmas)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}