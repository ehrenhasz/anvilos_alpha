{
  "module_name": "i915_gem_object.c",
  "hash_id": "219307ccfce43488e6a07f03521afc9073e320190aa75121a65a705a0619ad4f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_object.c",
  "human_readable_source": " \n\n#include <linux/highmem.h>\n#include <linux/sched/mm.h>\n\n#include <drm/drm_cache.h>\n\n#include \"display/intel_frontbuffer.h\"\n#include \"pxp/intel_pxp.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_file_private.h\"\n#include \"i915_gem_clflush.h\"\n#include \"i915_gem_context.h\"\n#include \"i915_gem_dmabuf.h\"\n#include \"i915_gem_mman.h\"\n#include \"i915_gem_object.h\"\n#include \"i915_gem_ttm.h\"\n#include \"i915_memcpy.h\"\n#include \"i915_trace.h\"\n\nstatic struct kmem_cache *slab_objects;\n\nstatic const struct drm_gem_object_funcs i915_gem_object_funcs;\n\nunsigned int i915_gem_get_pat_index(struct drm_i915_private *i915,\n\t\t\t\t    enum i915_cache_level level)\n{\n\tif (drm_WARN_ON(&i915->drm, level >= I915_MAX_CACHE_LEVEL))\n\t\treturn 0;\n\n\treturn INTEL_INFO(i915)->cachelevel_to_pat[level];\n}\n\nbool i915_gem_object_has_cache_level(const struct drm_i915_gem_object *obj,\n\t\t\t\t     enum i915_cache_level lvl)\n{\n\t \n\tif (obj->pat_set_by_user)\n\t\treturn true;\n\n\t \n\treturn obj->pat_index == i915_gem_get_pat_index(obj_to_i915(obj), lvl);\n}\n\nstruct drm_i915_gem_object *i915_gem_object_alloc(void)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tobj = kmem_cache_zalloc(slab_objects, GFP_KERNEL);\n\tif (!obj)\n\t\treturn NULL;\n\tobj->base.funcs = &i915_gem_object_funcs;\n\n\treturn obj;\n}\n\nvoid i915_gem_object_free(struct drm_i915_gem_object *obj)\n{\n\treturn kmem_cache_free(slab_objects, obj);\n}\n\nvoid i915_gem_object_init(struct drm_i915_gem_object *obj,\n\t\t\t  const struct drm_i915_gem_object_ops *ops,\n\t\t\t  struct lock_class_key *key, unsigned flags)\n{\n\t \n\tBUILD_BUG_ON(offsetof(typeof(*obj), base) !=\n\t\t     offsetof(typeof(*obj), __do_not_access.base));\n\n\tspin_lock_init(&obj->vma.lock);\n\tINIT_LIST_HEAD(&obj->vma.list);\n\n\tINIT_LIST_HEAD(&obj->mm.link);\n\n\tINIT_LIST_HEAD(&obj->lut_list);\n\tspin_lock_init(&obj->lut_lock);\n\n\tspin_lock_init(&obj->mmo.lock);\n\tobj->mmo.offsets = RB_ROOT;\n\n\tinit_rcu_head(&obj->rcu);\n\n\tobj->ops = ops;\n\tGEM_BUG_ON(flags & ~I915_BO_ALLOC_FLAGS);\n\tobj->flags = flags;\n\n\tobj->mm.madv = I915_MADV_WILLNEED;\n\tINIT_RADIX_TREE(&obj->mm.get_page.radix, GFP_KERNEL | __GFP_NOWARN);\n\tmutex_init(&obj->mm.get_page.lock);\n\tINIT_RADIX_TREE(&obj->mm.get_dma_page.radix, GFP_KERNEL | __GFP_NOWARN);\n\tmutex_init(&obj->mm.get_dma_page.lock);\n}\n\n \nvoid __i915_gem_object_fini(struct drm_i915_gem_object *obj)\n{\n\tmutex_destroy(&obj->mm.get_page.lock);\n\tmutex_destroy(&obj->mm.get_dma_page.lock);\n\tdma_resv_fini(&obj->base._resv);\n}\n\n \nvoid i915_gem_object_set_cache_coherency(struct drm_i915_gem_object *obj,\n\t\t\t\t\t unsigned int cache_level)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\tobj->pat_index = i915_gem_get_pat_index(i915, cache_level);\n\n\tif (cache_level != I915_CACHE_NONE)\n\t\tobj->cache_coherent = (I915_BO_CACHE_COHERENT_FOR_READ |\n\t\t\t\t       I915_BO_CACHE_COHERENT_FOR_WRITE);\n\telse if (HAS_LLC(i915))\n\t\tobj->cache_coherent = I915_BO_CACHE_COHERENT_FOR_READ;\n\telse\n\t\tobj->cache_coherent = 0;\n\n\tobj->cache_dirty =\n\t\t!(obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_WRITE) &&\n\t\t!IS_DGFX(i915);\n}\n\n \nvoid i915_gem_object_set_pat_index(struct drm_i915_gem_object *obj,\n\t\t\t\t   unsigned int pat_index)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\tif (obj->pat_index == pat_index)\n\t\treturn;\n\n\tobj->pat_index = pat_index;\n\n\tif (pat_index != i915_gem_get_pat_index(i915, I915_CACHE_NONE))\n\t\tobj->cache_coherent = (I915_BO_CACHE_COHERENT_FOR_READ |\n\t\t\t\t       I915_BO_CACHE_COHERENT_FOR_WRITE);\n\telse if (HAS_LLC(i915))\n\t\tobj->cache_coherent = I915_BO_CACHE_COHERENT_FOR_READ;\n\telse\n\t\tobj->cache_coherent = 0;\n\n\tobj->cache_dirty =\n\t\t!(obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_WRITE) &&\n\t\t!IS_DGFX(i915);\n}\n\nbool i915_gem_object_can_bypass_llc(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\t \n\tif (!(obj->flags & I915_BO_ALLOC_USER))\n\t\treturn false;\n\n\t \n\tif (obj->pat_set_by_user)\n\t\treturn true;\n\n\t \n\treturn (IS_JASPERLAKE(i915) || IS_ELKHARTLAKE(i915));\n}\n\nstatic void i915_gem_close_object(struct drm_gem_object *gem, struct drm_file *file)\n{\n\tstruct drm_i915_gem_object *obj = to_intel_bo(gem);\n\tstruct drm_i915_file_private *fpriv = file->driver_priv;\n\tstruct i915_lut_handle bookmark = {};\n\tstruct i915_mmap_offset *mmo, *mn;\n\tstruct i915_lut_handle *lut, *ln;\n\tLIST_HEAD(close);\n\n\tspin_lock(&obj->lut_lock);\n\tlist_for_each_entry_safe(lut, ln, &obj->lut_list, obj_link) {\n\t\tstruct i915_gem_context *ctx = lut->ctx;\n\n\t\tif (ctx && ctx->file_priv == fpriv) {\n\t\t\ti915_gem_context_get(ctx);\n\t\t\tlist_move(&lut->obj_link, &close);\n\t\t}\n\n\t\t \n\t\tif (&ln->obj_link != &obj->lut_list) {\n\t\t\tlist_add_tail(&bookmark.obj_link, &ln->obj_link);\n\t\t\tif (cond_resched_lock(&obj->lut_lock))\n\t\t\t\tlist_safe_reset_next(&bookmark, ln, obj_link);\n\t\t\t__list_del_entry(&bookmark.obj_link);\n\t\t}\n\t}\n\tspin_unlock(&obj->lut_lock);\n\n\tspin_lock(&obj->mmo.lock);\n\trbtree_postorder_for_each_entry_safe(mmo, mn, &obj->mmo.offsets, offset)\n\t\tdrm_vma_node_revoke(&mmo->vma_node, file);\n\tspin_unlock(&obj->mmo.lock);\n\n\tlist_for_each_entry_safe(lut, ln, &close, obj_link) {\n\t\tstruct i915_gem_context *ctx = lut->ctx;\n\t\tstruct i915_vma *vma;\n\n\t\t \n\n\t\tmutex_lock(&ctx->lut_mutex);\n\t\tvma = radix_tree_delete(&ctx->handles_vma, lut->handle);\n\t\tif (vma) {\n\t\t\tGEM_BUG_ON(vma->obj != obj);\n\t\t\tGEM_BUG_ON(!atomic_read(&vma->open_count));\n\t\t\ti915_vma_close(vma);\n\t\t}\n\t\tmutex_unlock(&ctx->lut_mutex);\n\n\t\ti915_gem_context_put(lut->ctx);\n\t\ti915_lut_handle_free(lut);\n\t\ti915_gem_object_put(obj);\n\t}\n}\n\nvoid __i915_gem_free_object_rcu(struct rcu_head *head)\n{\n\tstruct drm_i915_gem_object *obj =\n\t\tcontainer_of(head, typeof(*obj), rcu);\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\ti915_gem_object_free(obj);\n\n\tGEM_BUG_ON(!atomic_read(&i915->mm.free_count));\n\tatomic_dec(&i915->mm.free_count);\n}\n\nstatic void __i915_gem_object_free_mmaps(struct drm_i915_gem_object *obj)\n{\n\t \n\n\tif (obj->userfault_count && !IS_DGFX(to_i915(obj->base.dev)))\n\t\ti915_gem_object_release_mmap_gtt(obj);\n\n\tif (!RB_EMPTY_ROOT(&obj->mmo.offsets)) {\n\t\tstruct i915_mmap_offset *mmo, *mn;\n\n\t\ti915_gem_object_release_mmap_offset(obj);\n\n\t\trbtree_postorder_for_each_entry_safe(mmo, mn,\n\t\t\t\t\t\t     &obj->mmo.offsets,\n\t\t\t\t\t\t     offset) {\n\t\t\tdrm_vma_offset_remove(obj->base.dev->vma_offset_manager,\n\t\t\t\t\t      &mmo->vma_node);\n\t\t\tkfree(mmo);\n\t\t}\n\t\tobj->mmo.offsets = RB_ROOT;\n\t}\n}\n\n \nvoid __i915_gem_object_pages_fini(struct drm_i915_gem_object *obj)\n{\n\tassert_object_held_shared(obj);\n\n\tif (!list_empty(&obj->vma.list)) {\n\t\tstruct i915_vma *vma;\n\n\t\tspin_lock(&obj->vma.lock);\n\t\twhile ((vma = list_first_entry_or_null(&obj->vma.list,\n\t\t\t\t\t\t       struct i915_vma,\n\t\t\t\t\t\t       obj_link))) {\n\t\t\tGEM_BUG_ON(vma->obj != obj);\n\t\t\tspin_unlock(&obj->vma.lock);\n\n\t\t\ti915_vma_destroy(vma);\n\n\t\t\tspin_lock(&obj->vma.lock);\n\t\t}\n\t\tspin_unlock(&obj->vma.lock);\n\t}\n\n\t__i915_gem_object_free_mmaps(obj);\n\n\tatomic_set(&obj->mm.pages_pin_count, 0);\n\n\t \n\tif (obj->base.import_attach)\n\t\ti915_gem_object_lock(obj, NULL);\n\n\t__i915_gem_object_put_pages(obj);\n\n\tif (obj->base.import_attach)\n\t\ti915_gem_object_unlock(obj);\n\n\tGEM_BUG_ON(i915_gem_object_has_pages(obj));\n}\n\nvoid __i915_gem_free_object(struct drm_i915_gem_object *obj)\n{\n\ttrace_i915_gem_object_destroy(obj);\n\n\tGEM_BUG_ON(!list_empty(&obj->lut_list));\n\n\tbitmap_free(obj->bit_17);\n\n\tif (obj->base.import_attach)\n\t\tdrm_prime_gem_destroy(&obj->base, NULL);\n\n\tdrm_gem_free_mmap_offset(&obj->base);\n\n\tif (obj->ops->release)\n\t\tobj->ops->release(obj);\n\n\tif (obj->mm.n_placements > 1)\n\t\tkfree(obj->mm.placements);\n\n\tif (obj->shares_resv_from)\n\t\ti915_vm_resv_put(obj->shares_resv_from);\n\n\t__i915_gem_object_fini(obj);\n}\n\nstatic void __i915_gem_free_objects(struct drm_i915_private *i915,\n\t\t\t\t    struct llist_node *freed)\n{\n\tstruct drm_i915_gem_object *obj, *on;\n\n\tllist_for_each_entry_safe(obj, on, freed, freed) {\n\t\tmight_sleep();\n\t\tif (obj->ops->delayed_free) {\n\t\t\tobj->ops->delayed_free(obj);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__i915_gem_object_pages_fini(obj);\n\t\t__i915_gem_free_object(obj);\n\n\t\t \n\t\tcall_rcu(&obj->rcu, __i915_gem_free_object_rcu);\n\t\tcond_resched();\n\t}\n}\n\nvoid i915_gem_flush_free_objects(struct drm_i915_private *i915)\n{\n\tstruct llist_node *freed = llist_del_all(&i915->mm.free_list);\n\n\tif (unlikely(freed))\n\t\t__i915_gem_free_objects(i915, freed);\n}\n\nstatic void __i915_gem_free_work(struct work_struct *work)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(work, struct drm_i915_private, mm.free_work);\n\n\ti915_gem_flush_free_objects(i915);\n}\n\nstatic void i915_gem_free_object(struct drm_gem_object *gem_obj)\n{\n\tstruct drm_i915_gem_object *obj = to_intel_bo(gem_obj);\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\tGEM_BUG_ON(i915_gem_object_is_framebuffer(obj));\n\n\t \n\tatomic_inc(&i915->mm.free_count);\n\n\t \n\n\tif (llist_add(&obj->freed, &i915->mm.free_list))\n\t\tqueue_work(i915->wq, &i915->mm.free_work);\n}\n\nvoid __i915_gem_object_flush_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t\t enum fb_op_origin origin)\n{\n\tstruct intel_frontbuffer *front;\n\n\tfront = i915_gem_object_get_frontbuffer(obj);\n\tif (front) {\n\t\tintel_frontbuffer_flush(front, origin);\n\t\tintel_frontbuffer_put(front);\n\t}\n}\n\nvoid __i915_gem_object_invalidate_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t\t      enum fb_op_origin origin)\n{\n\tstruct intel_frontbuffer *front;\n\n\tfront = i915_gem_object_get_frontbuffer(obj);\n\tif (front) {\n\t\tintel_frontbuffer_invalidate(front, origin);\n\t\tintel_frontbuffer_put(front);\n\t}\n}\n\nstatic void\ni915_gem_object_read_from_page_kmap(struct drm_i915_gem_object *obj, u64 offset, void *dst, int size)\n{\n\tpgoff_t idx = offset >> PAGE_SHIFT;\n\tvoid *src_map;\n\tvoid *src_ptr;\n\n\tsrc_map = kmap_atomic(i915_gem_object_get_page(obj, idx));\n\n\tsrc_ptr = src_map + offset_in_page(offset);\n\tif (!(obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_READ))\n\t\tdrm_clflush_virt_range(src_ptr, size);\n\tmemcpy(dst, src_ptr, size);\n\n\tkunmap_atomic(src_map);\n}\n\nstatic void\ni915_gem_object_read_from_page_iomap(struct drm_i915_gem_object *obj, u64 offset, void *dst, int size)\n{\n\tpgoff_t idx = offset >> PAGE_SHIFT;\n\tdma_addr_t dma = i915_gem_object_get_dma_address(obj, idx);\n\tvoid __iomem *src_map;\n\tvoid __iomem *src_ptr;\n\n\tsrc_map = io_mapping_map_wc(&obj->mm.region->iomap,\n\t\t\t\t    dma - obj->mm.region->region.start,\n\t\t\t\t    PAGE_SIZE);\n\n\tsrc_ptr = src_map + offset_in_page(offset);\n\tif (!i915_memcpy_from_wc(dst, (void __force *)src_ptr, size))\n\t\tmemcpy_fromio(dst, src_ptr, size);\n\n\tio_mapping_unmap(src_map);\n}\n\nstatic bool object_has_mappable_iomem(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_iomem(obj));\n\n\tif (IS_DGFX(to_i915(obj->base.dev)))\n\t\treturn i915_ttm_resource_mappable(i915_gem_to_ttm(obj)->resource);\n\n\treturn true;\n}\n\n \nint i915_gem_object_read_from_page(struct drm_i915_gem_object *obj, u64 offset, void *dst, int size)\n{\n\tGEM_BUG_ON(overflows_type(offset >> PAGE_SHIFT, pgoff_t));\n\tGEM_BUG_ON(offset >= obj->base.size);\n\tGEM_BUG_ON(offset_in_page(offset) > PAGE_SIZE - size);\n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));\n\n\tif (i915_gem_object_has_struct_page(obj))\n\t\ti915_gem_object_read_from_page_kmap(obj, offset, dst, size);\n\telse if (i915_gem_object_has_iomem(obj) && object_has_mappable_iomem(obj))\n\t\ti915_gem_object_read_from_page_iomap(obj, offset, dst, size);\n\telse\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\n \nbool i915_gem_object_evictable(struct drm_i915_gem_object *obj)\n{\n\tstruct i915_vma *vma;\n\tint pin_count = atomic_read(&obj->mm.pages_pin_count);\n\n\tif (!pin_count)\n\t\treturn true;\n\n\tspin_lock(&obj->vma.lock);\n\tlist_for_each_entry(vma, &obj->vma.list, obj_link) {\n\t\tif (i915_vma_is_pinned(vma)) {\n\t\t\tspin_unlock(&obj->vma.lock);\n\t\t\treturn false;\n\t\t}\n\t\tif (atomic_read(&vma->pages_count))\n\t\t\tpin_count--;\n\t}\n\tspin_unlock(&obj->vma.lock);\n\tGEM_WARN_ON(pin_count < 0);\n\n\treturn pin_count == 0;\n}\n\n \nbool i915_gem_object_migratable(struct drm_i915_gem_object *obj)\n{\n\tstruct intel_memory_region *mr = READ_ONCE(obj->mm.region);\n\n\tif (!mr)\n\t\treturn false;\n\n\treturn obj->mm.n_placements > 1;\n}\n\n \nbool i915_gem_object_has_struct_page(const struct drm_i915_gem_object *obj)\n{\n#ifdef CONFIG_LOCKDEP\n\tif (IS_DGFX(to_i915(obj->base.dev)) &&\n\t    i915_gem_object_evictable((void __force *)obj))\n\t\tassert_object_held_shared(obj);\n#endif\n\treturn obj->mem_flags & I915_BO_FLAG_STRUCT_PAGE;\n}\n\n \nbool i915_gem_object_has_iomem(const struct drm_i915_gem_object *obj)\n{\n#ifdef CONFIG_LOCKDEP\n\tif (IS_DGFX(to_i915(obj->base.dev)) &&\n\t    i915_gem_object_evictable((void __force *)obj))\n\t\tassert_object_held_shared(obj);\n#endif\n\treturn obj->mem_flags & I915_BO_FLAG_IOMEM;\n}\n\n \nbool i915_gem_object_can_migrate(struct drm_i915_gem_object *obj,\n\t\t\t\t enum intel_region_id id)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tunsigned int num_allowed = obj->mm.n_placements;\n\tstruct intel_memory_region *mr;\n\tunsigned int i;\n\n\tGEM_BUG_ON(id >= INTEL_REGION_UNKNOWN);\n\tGEM_BUG_ON(obj->mm.madv != I915_MADV_WILLNEED);\n\n\tmr = i915->mm.regions[id];\n\tif (!mr)\n\t\treturn false;\n\n\tif (!IS_ALIGNED(obj->base.size, mr->min_page_size))\n\t\treturn false;\n\n\tif (obj->mm.region == mr)\n\t\treturn true;\n\n\tif (!i915_gem_object_evictable(obj))\n\t\treturn false;\n\n\tif (!obj->ops->migrate)\n\t\treturn false;\n\n\tif (!(obj->flags & I915_BO_ALLOC_USER))\n\t\treturn true;\n\n\tif (num_allowed == 0)\n\t\treturn false;\n\n\tfor (i = 0; i < num_allowed; ++i) {\n\t\tif (mr == obj->mm.placements[i])\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nint i915_gem_object_migrate(struct drm_i915_gem_object *obj,\n\t\t\t    struct i915_gem_ww_ctx *ww,\n\t\t\t    enum intel_region_id id)\n{\n\treturn __i915_gem_object_migrate(obj, ww, id, obj->flags);\n}\n\n \nint __i915_gem_object_migrate(struct drm_i915_gem_object *obj,\n\t\t\t      struct i915_gem_ww_ctx *ww,\n\t\t\t      enum intel_region_id id,\n\t\t\t      unsigned int flags)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct intel_memory_region *mr;\n\n\tGEM_BUG_ON(id >= INTEL_REGION_UNKNOWN);\n\tGEM_BUG_ON(obj->mm.madv != I915_MADV_WILLNEED);\n\tassert_object_held(obj);\n\n\tmr = i915->mm.regions[id];\n\tGEM_BUG_ON(!mr);\n\n\tif (!i915_gem_object_can_migrate(obj, id))\n\t\treturn -EINVAL;\n\n\tif (!obj->ops->migrate) {\n\t\tif (GEM_WARN_ON(obj->mm.region != mr))\n\t\t\treturn -EINVAL;\n\t\treturn 0;\n\t}\n\n\treturn obj->ops->migrate(obj, mr, flags);\n}\n\n \nbool i915_gem_object_placement_possible(struct drm_i915_gem_object *obj,\n\t\t\t\t\tenum intel_memory_type type)\n{\n\tunsigned int i;\n\n\tif (!obj->mm.n_placements) {\n\t\tswitch (type) {\n\t\tcase INTEL_MEMORY_LOCAL:\n\t\t\treturn i915_gem_object_has_iomem(obj);\n\t\tcase INTEL_MEMORY_SYSTEM:\n\t\t\treturn i915_gem_object_has_pages(obj);\n\t\tdefault:\n\t\t\t \n\t\t\tGEM_BUG_ON(1);\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tfor (i = 0; i < obj->mm.n_placements; i++) {\n\t\tif (obj->mm.placements[i]->type == type)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nbool i915_gem_object_needs_ccs_pages(struct drm_i915_gem_object *obj)\n{\n\tbool lmem_placement = false;\n\tint i;\n\n\tif (!HAS_FLAT_CCS(to_i915(obj->base.dev)))\n\t\treturn false;\n\n\tif (obj->flags & I915_BO_ALLOC_CCS_AUX)\n\t\treturn true;\n\n\tfor (i = 0; i < obj->mm.n_placements; i++) {\n\t\t \n\t\tif (obj->mm.placements[i]->type == INTEL_MEMORY_SYSTEM)\n\t\t\treturn false;\n\t\tif (!lmem_placement &&\n\t\t    obj->mm.placements[i]->type == INTEL_MEMORY_LOCAL)\n\t\t\tlmem_placement = true;\n\t}\n\n\treturn lmem_placement;\n}\n\nvoid i915_gem_init__objects(struct drm_i915_private *i915)\n{\n\tINIT_WORK(&i915->mm.free_work, __i915_gem_free_work);\n}\n\nvoid i915_objects_module_exit(void)\n{\n\tkmem_cache_destroy(slab_objects);\n}\n\nint __init i915_objects_module_init(void)\n{\n\tslab_objects = KMEM_CACHE(drm_i915_gem_object, SLAB_HWCACHE_ALIGN);\n\tif (!slab_objects)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic const struct drm_gem_object_funcs i915_gem_object_funcs = {\n\t.free = i915_gem_free_object,\n\t.close = i915_gem_close_object,\n\t.export = i915_gem_prime_export,\n};\n\n \nint i915_gem_object_get_moving_fence(struct drm_i915_gem_object *obj,\n\t\t\t\t     struct dma_fence **fence)\n{\n\treturn dma_resv_get_singleton(obj->base.resv, DMA_RESV_USAGE_KERNEL,\n\t\t\t\t      fence);\n}\n\n \nint i915_gem_object_wait_moving_fence(struct drm_i915_gem_object *obj,\n\t\t\t\t      bool intr)\n{\n\tlong ret;\n\n\tassert_object_held(obj);\n\n\tret = dma_resv_wait_timeout(obj->base. resv, DMA_RESV_USAGE_KERNEL,\n\t\t\t\t    intr, MAX_SCHEDULE_TIMEOUT);\n\tif (!ret)\n\t\tret = -ETIME;\n\telse if (ret > 0 && i915_gem_object_has_unknown_state(obj))\n\t\tret = -EIO;\n\n\treturn ret < 0 ? ret : 0;\n}\n\n \nbool i915_gem_object_has_unknown_state(struct drm_i915_gem_object *obj)\n{\n\t \n\tsmp_rmb();\n\treturn obj->mm.unknown_state;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/huge_gem_object.c\"\n#include \"selftests/huge_pages.c\"\n#include \"selftests/i915_gem_migrate.c\"\n#include \"selftests/i915_gem_object.c\"\n#include \"selftests/i915_gem_coherency.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}