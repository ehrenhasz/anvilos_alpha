{
  "module_name": "i915_gem_mman.c",
  "hash_id": "e39f329d634c240a1119f41023b64b5a5d2d922558bcce156abb92d7a3da0725",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/selftests/i915_gem_mman.c",
  "human_readable_source": " \n\n#include <linux/highmem.h>\n#include <linux/prime_numbers.h>\n\n#include \"gem/i915_gem_internal.h\"\n#include \"gem/i915_gem_lmem.h\"\n#include \"gem/i915_gem_region.h\"\n#include \"gem/i915_gem_ttm.h\"\n#include \"gem/i915_gem_ttm_move.h\"\n#include \"gt/intel_engine_pm.h\"\n#include \"gt/intel_gpu_commands.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_pm.h\"\n#include \"gt/intel_migrate.h\"\n#include \"i915_reg.h\"\n#include \"i915_ttm_buddy_manager.h\"\n\n#include \"huge_gem_object.h\"\n#include \"i915_selftest.h\"\n#include \"selftests/i915_random.h\"\n#include \"selftests/igt_flush_test.h\"\n#include \"selftests/igt_reset.h\"\n#include \"selftests/igt_mmap.h\"\n\nstruct tile {\n\tunsigned int width;\n\tunsigned int height;\n\tunsigned int stride;\n\tunsigned int size;\n\tunsigned int tiling;\n\tunsigned int swizzle;\n};\n\nstatic u64 swizzle_bit(unsigned int bit, u64 offset)\n{\n\treturn (offset & BIT_ULL(bit)) >> (bit - 6);\n}\n\nstatic u64 tiled_offset(const struct tile *tile, u64 v)\n{\n\tu64 x, y;\n\n\tif (tile->tiling == I915_TILING_NONE)\n\t\treturn v;\n\n\ty = div64_u64_rem(v, tile->stride, &x);\n\tv = div64_u64_rem(y, tile->height, &y) * tile->stride * tile->height;\n\n\tif (tile->tiling == I915_TILING_X) {\n\t\tv += y * tile->width;\n\t\tv += div64_u64_rem(x, tile->width, &x) << tile->size;\n\t\tv += x;\n\t} else if (tile->width == 128) {\n\t\tconst unsigned int ytile_span = 16;\n\t\tconst unsigned int ytile_height = 512;\n\n\t\tv += y * ytile_span;\n\t\tv += div64_u64_rem(x, ytile_span, &x) * ytile_height;\n\t\tv += x;\n\t} else {\n\t\tconst unsigned int ytile_span = 32;\n\t\tconst unsigned int ytile_height = 256;\n\n\t\tv += y * ytile_span;\n\t\tv += div64_u64_rem(x, ytile_span, &x) * ytile_height;\n\t\tv += x;\n\t}\n\n\tswitch (tile->swizzle) {\n\tcase I915_BIT_6_SWIZZLE_9:\n\t\tv ^= swizzle_bit(9, v);\n\t\tbreak;\n\tcase I915_BIT_6_SWIZZLE_9_10:\n\t\tv ^= swizzle_bit(9, v) ^ swizzle_bit(10, v);\n\t\tbreak;\n\tcase I915_BIT_6_SWIZZLE_9_11:\n\t\tv ^= swizzle_bit(9, v) ^ swizzle_bit(11, v);\n\t\tbreak;\n\tcase I915_BIT_6_SWIZZLE_9_10_11:\n\t\tv ^= swizzle_bit(9, v) ^ swizzle_bit(10, v) ^ swizzle_bit(11, v);\n\t\tbreak;\n\t}\n\n\treturn v;\n}\n\nstatic int check_partial_mapping(struct drm_i915_gem_object *obj,\n\t\t\t\t const struct tile *tile,\n\t\t\t\t struct rnd_state *prng)\n{\n\tconst unsigned long npages = obj->base.size / PAGE_SIZE;\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_gtt_view view;\n\tstruct i915_vma *vma;\n\tunsigned long offset;\n\tunsigned long page;\n\tu32 __iomem *io;\n\tstruct page *p;\n\tunsigned int n;\n\tu32 *cpu;\n\tint err;\n\n\terr = i915_gem_object_set_tiling(obj, tile->tiling, tile->stride);\n\tif (err) {\n\t\tpr_err(\"Failed to set tiling mode=%u, stride=%u, err=%d\\n\",\n\t\t       tile->tiling, tile->stride, err);\n\t\treturn err;\n\t}\n\n\tGEM_BUG_ON(i915_gem_object_get_tiling(obj) != tile->tiling);\n\tGEM_BUG_ON(i915_gem_object_get_stride(obj) != tile->stride);\n\n\ti915_gem_object_lock(obj, NULL);\n\terr = i915_gem_object_set_to_gtt_domain(obj, true);\n\ti915_gem_object_unlock(obj);\n\tif (err) {\n\t\tpr_err(\"Failed to flush to GTT write domain; err=%d\\n\", err);\n\t\treturn err;\n\t}\n\n\tpage = i915_prandom_u32_max_state(npages, prng);\n\tview = compute_partial_view(obj, page, MIN_CHUNK_PAGES);\n\n\tvma = i915_gem_object_ggtt_pin(obj, &view, 0, 0, PIN_MAPPABLE);\n\tif (IS_ERR(vma)) {\n\t\tpr_err(\"Failed to pin partial view: offset=%lu; err=%d\\n\",\n\t\t       page, (int)PTR_ERR(vma));\n\t\treturn PTR_ERR(vma);\n\t}\n\n\tn = page - view.partial.offset;\n\tGEM_BUG_ON(n >= view.partial.size);\n\n\tio = i915_vma_pin_iomap(vma);\n\ti915_vma_unpin(vma);\n\tif (IS_ERR(io)) {\n\t\tpr_err(\"Failed to iomap partial view: offset=%lu; err=%d\\n\",\n\t\t       page, (int)PTR_ERR(io));\n\t\terr = PTR_ERR(io);\n\t\tgoto out;\n\t}\n\n\tiowrite32(page, io + n * PAGE_SIZE / sizeof(*io));\n\ti915_vma_unpin_iomap(vma);\n\n\toffset = tiled_offset(tile, page << PAGE_SHIFT);\n\tif (offset >= obj->base.size)\n\t\tgoto out;\n\n\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\tp = i915_gem_object_get_page(obj, offset >> PAGE_SHIFT);\n\tcpu = kmap(p) + offset_in_page(offset);\n\tdrm_clflush_virt_range(cpu, sizeof(*cpu));\n\tif (*cpu != (u32)page) {\n\t\tpr_err(\"Partial view for %lu [%u] (offset=%llu, size=%u [%llu, row size %u], fence=%d, tiling=%d, stride=%d) misalignment, expected write to page (%lu + %u [0x%lx]) of 0x%x, found 0x%x\\n\",\n\t\t       page, n,\n\t\t       view.partial.offset,\n\t\t       view.partial.size,\n\t\t       vma->size >> PAGE_SHIFT,\n\t\t       tile->tiling ? tile_row_pages(obj) : 0,\n\t\t       vma->fence ? vma->fence->id : -1, tile->tiling, tile->stride,\n\t\t       offset >> PAGE_SHIFT,\n\t\t       (unsigned int)offset_in_page(offset),\n\t\t       offset,\n\t\t       (u32)page, *cpu);\n\t\terr = -EINVAL;\n\t}\n\t*cpu = 0;\n\tdrm_clflush_virt_range(cpu, sizeof(*cpu));\n\tkunmap(p);\n\nout:\n\ti915_gem_object_lock(obj, NULL);\n\ti915_vma_destroy(vma);\n\ti915_gem_object_unlock(obj);\n\treturn err;\n}\n\nstatic int check_partial_mappings(struct drm_i915_gem_object *obj,\n\t\t\t\t  const struct tile *tile,\n\t\t\t\t  unsigned long end_time)\n{\n\tconst unsigned int nreal = obj->scratch / PAGE_SIZE;\n\tconst unsigned long npages = obj->base.size / PAGE_SIZE;\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct i915_vma *vma;\n\tunsigned long page;\n\tint err;\n\n\terr = i915_gem_object_set_tiling(obj, tile->tiling, tile->stride);\n\tif (err) {\n\t\tpr_err(\"Failed to set tiling mode=%u, stride=%u, err=%d\\n\",\n\t\t       tile->tiling, tile->stride, err);\n\t\treturn err;\n\t}\n\n\tGEM_BUG_ON(i915_gem_object_get_tiling(obj) != tile->tiling);\n\tGEM_BUG_ON(i915_gem_object_get_stride(obj) != tile->stride);\n\n\ti915_gem_object_lock(obj, NULL);\n\terr = i915_gem_object_set_to_gtt_domain(obj, true);\n\ti915_gem_object_unlock(obj);\n\tif (err) {\n\t\tpr_err(\"Failed to flush to GTT write domain; err=%d\\n\", err);\n\t\treturn err;\n\t}\n\n\tfor_each_prime_number_from(page, 1, npages) {\n\t\tstruct i915_gtt_view view =\n\t\t\tcompute_partial_view(obj, page, MIN_CHUNK_PAGES);\n\t\tunsigned long offset;\n\t\tu32 __iomem *io;\n\t\tstruct page *p;\n\t\tunsigned int n;\n\t\tu32 *cpu;\n\n\t\tGEM_BUG_ON(view.partial.size > nreal);\n\t\tcond_resched();\n\n\t\tvma = i915_gem_object_ggtt_pin(obj, &view, 0, 0, PIN_MAPPABLE);\n\t\tif (IS_ERR(vma)) {\n\t\t\tpr_err(\"Failed to pin partial view: offset=%lu; err=%d\\n\",\n\t\t\t       page, (int)PTR_ERR(vma));\n\t\t\treturn PTR_ERR(vma);\n\t\t}\n\n\t\tn = page - view.partial.offset;\n\t\tGEM_BUG_ON(n >= view.partial.size);\n\n\t\tio = i915_vma_pin_iomap(vma);\n\t\ti915_vma_unpin(vma);\n\t\tif (IS_ERR(io)) {\n\t\t\tpr_err(\"Failed to iomap partial view: offset=%lu; err=%d\\n\",\n\t\t\t       page, (int)PTR_ERR(io));\n\t\t\treturn PTR_ERR(io);\n\t\t}\n\n\t\tiowrite32(page, io + n * PAGE_SIZE / sizeof(*io));\n\t\ti915_vma_unpin_iomap(vma);\n\n\t\toffset = tiled_offset(tile, page << PAGE_SHIFT);\n\t\tif (offset >= obj->base.size)\n\t\t\tcontinue;\n\n\t\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\t\tp = i915_gem_object_get_page(obj, offset >> PAGE_SHIFT);\n\t\tcpu = kmap(p) + offset_in_page(offset);\n\t\tdrm_clflush_virt_range(cpu, sizeof(*cpu));\n\t\tif (*cpu != (u32)page) {\n\t\t\tpr_err(\"Partial view for %lu [%u] (offset=%llu, size=%u [%llu, row size %u], fence=%d, tiling=%d, stride=%d) misalignment, expected write to page (%lu + %u [0x%lx]) of 0x%x, found 0x%x\\n\",\n\t\t\t       page, n,\n\t\t\t       view.partial.offset,\n\t\t\t       view.partial.size,\n\t\t\t       vma->size >> PAGE_SHIFT,\n\t\t\t       tile->tiling ? tile_row_pages(obj) : 0,\n\t\t\t       vma->fence ? vma->fence->id : -1, tile->tiling, tile->stride,\n\t\t\t       offset >> PAGE_SHIFT,\n\t\t\t       (unsigned int)offset_in_page(offset),\n\t\t\t       offset,\n\t\t\t       (u32)page, *cpu);\n\t\t\terr = -EINVAL;\n\t\t}\n\t\t*cpu = 0;\n\t\tdrm_clflush_virt_range(cpu, sizeof(*cpu));\n\t\tkunmap(p);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\ti915_gem_object_lock(obj, NULL);\n\t\ti915_vma_destroy(vma);\n\t\ti915_gem_object_unlock(obj);\n\n\t\tif (igt_timeout(end_time,\n\t\t\t\t\"%s: timed out after tiling=%d stride=%d\\n\",\n\t\t\t\t__func__, tile->tiling, tile->stride))\n\t\t\treturn -EINTR;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned int\nsetup_tile_size(struct tile *tile, struct drm_i915_private *i915)\n{\n\tif (GRAPHICS_VER(i915) <= 2) {\n\t\ttile->height = 16;\n\t\ttile->width = 128;\n\t\ttile->size = 11;\n\t} else if (tile->tiling == I915_TILING_Y &&\n\t\t   HAS_128_BYTE_Y_TILING(i915)) {\n\t\ttile->height = 32;\n\t\ttile->width = 128;\n\t\ttile->size = 12;\n\t} else {\n\t\ttile->height = 8;\n\t\ttile->width = 512;\n\t\ttile->size = 12;\n\t}\n\n\tif (GRAPHICS_VER(i915) < 4)\n\t\treturn 8192 / tile->width;\n\telse if (GRAPHICS_VER(i915) < 7)\n\t\treturn 128 * I965_FENCE_MAX_PITCH_VAL / tile->width;\n\telse\n\t\treturn 128 * GEN7_FENCE_MAX_PITCH_VAL / tile->width;\n}\n\nstatic int igt_partial_tiling(void *arg)\n{\n\tconst unsigned int nreal = 1 << 12;  \n\tstruct drm_i915_private *i915 = arg;\n\tstruct drm_i915_gem_object *obj;\n\tintel_wakeref_t wakeref;\n\tint tiling;\n\tint err;\n\n\tif (!i915_ggtt_has_aperture(to_gt(i915)->ggtt))\n\t\treturn 0;\n\n\t \n\n\tobj = huge_gem_object(i915,\n\t\t\t      nreal << PAGE_SHIFT,\n\t\t\t      (1 + next_prime_number(to_gt(i915)->ggtt->vm.total >> PAGE_SHIFT)) << PAGE_SHIFT);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\terr = i915_gem_object_pin_pages_unlocked(obj);\n\tif (err) {\n\t\tpr_err(\"Failed to allocate %u pages (%lu total), err=%d\\n\",\n\t\t       nreal, obj->base.size / PAGE_SIZE, err);\n\t\tgoto out;\n\t}\n\n\twakeref = intel_runtime_pm_get(&i915->runtime_pm);\n\n\tif (1) {\n\t\tIGT_TIMEOUT(end);\n\t\tstruct tile tile;\n\n\t\ttile.height = 1;\n\t\ttile.width = 1;\n\t\ttile.size = 0;\n\t\ttile.stride = 0;\n\t\ttile.swizzle = I915_BIT_6_SWIZZLE_NONE;\n\t\ttile.tiling = I915_TILING_NONE;\n\n\t\terr = check_partial_mappings(obj, &tile, end);\n\t\tif (err && err != -EINTR)\n\t\t\tgoto out_unlock;\n\t}\n\n\tfor (tiling = I915_TILING_X; tiling <= I915_TILING_Y; tiling++) {\n\t\tIGT_TIMEOUT(end);\n\t\tunsigned int max_pitch;\n\t\tunsigned int pitch;\n\t\tstruct tile tile;\n\n\t\tif (i915->gem_quirks & GEM_QUIRK_PIN_SWIZZLED_PAGES)\n\t\t\t \n\t\t\tbreak;\n\n\t\ttile.tiling = tiling;\n\t\tswitch (tiling) {\n\t\tcase I915_TILING_X:\n\t\t\ttile.swizzle = to_gt(i915)->ggtt->bit_6_swizzle_x;\n\t\t\tbreak;\n\t\tcase I915_TILING_Y:\n\t\t\ttile.swizzle = to_gt(i915)->ggtt->bit_6_swizzle_y;\n\t\t\tbreak;\n\t\t}\n\n\t\tGEM_BUG_ON(tile.swizzle == I915_BIT_6_SWIZZLE_UNKNOWN);\n\t\tif (tile.swizzle == I915_BIT_6_SWIZZLE_9_17 ||\n\t\t    tile.swizzle == I915_BIT_6_SWIZZLE_9_10_17)\n\t\t\tcontinue;\n\n\t\tmax_pitch = setup_tile_size(&tile, i915);\n\n\t\tfor (pitch = max_pitch; pitch; pitch >>= 1) {\n\t\t\ttile.stride = tile.width * pitch;\n\t\t\terr = check_partial_mappings(obj, &tile, end);\n\t\t\tif (err == -EINTR)\n\t\t\t\tgoto next_tiling;\n\t\t\tif (err)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tif (pitch > 2 && GRAPHICS_VER(i915) >= 4) {\n\t\t\t\ttile.stride = tile.width * (pitch - 1);\n\t\t\t\terr = check_partial_mappings(obj, &tile, end);\n\t\t\t\tif (err == -EINTR)\n\t\t\t\t\tgoto next_tiling;\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tif (pitch < max_pitch && GRAPHICS_VER(i915) >= 4) {\n\t\t\t\ttile.stride = tile.width * (pitch + 1);\n\t\t\t\terr = check_partial_mappings(obj, &tile, end);\n\t\t\t\tif (err == -EINTR)\n\t\t\t\t\tgoto next_tiling;\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\n\t\tif (GRAPHICS_VER(i915) >= 4) {\n\t\t\tfor_each_prime_number(pitch, max_pitch) {\n\t\t\t\ttile.stride = tile.width * pitch;\n\t\t\t\terr = check_partial_mappings(obj, &tile, end);\n\t\t\t\tif (err == -EINTR)\n\t\t\t\t\tgoto next_tiling;\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t}\n\nnext_tiling: ;\n\t}\n\nout_unlock:\n\tintel_runtime_pm_put(&i915->runtime_pm, wakeref);\n\ti915_gem_object_unpin_pages(obj);\nout:\n\ti915_gem_object_put(obj);\n\treturn err;\n}\n\nstatic int igt_smoke_tiling(void *arg)\n{\n\tconst unsigned int nreal = 1 << 12;  \n\tstruct drm_i915_private *i915 = arg;\n\tstruct drm_i915_gem_object *obj;\n\tintel_wakeref_t wakeref;\n\tI915_RND_STATE(prng);\n\tunsigned long count;\n\tIGT_TIMEOUT(end);\n\tint err;\n\n\tif (!i915_ggtt_has_aperture(to_gt(i915)->ggtt))\n\t\treturn 0;\n\n\t \n\n\tif (i915->gem_quirks & GEM_QUIRK_PIN_SWIZZLED_PAGES)\n\t\treturn 0;\n\n\tobj = huge_gem_object(i915,\n\t\t\t      nreal << PAGE_SHIFT,\n\t\t\t      (1 + next_prime_number(to_gt(i915)->ggtt->vm.total >> PAGE_SHIFT)) << PAGE_SHIFT);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\terr = i915_gem_object_pin_pages_unlocked(obj);\n\tif (err) {\n\t\tpr_err(\"Failed to allocate %u pages (%lu total), err=%d\\n\",\n\t\t       nreal, obj->base.size / PAGE_SIZE, err);\n\t\tgoto out;\n\t}\n\n\twakeref = intel_runtime_pm_get(&i915->runtime_pm);\n\n\tcount = 0;\n\tdo {\n\t\tstruct tile tile;\n\n\t\ttile.tiling =\n\t\t\ti915_prandom_u32_max_state(I915_TILING_Y + 1, &prng);\n\t\tswitch (tile.tiling) {\n\t\tcase I915_TILING_NONE:\n\t\t\ttile.height = 1;\n\t\t\ttile.width = 1;\n\t\t\ttile.size = 0;\n\t\t\ttile.stride = 0;\n\t\t\ttile.swizzle = I915_BIT_6_SWIZZLE_NONE;\n\t\t\tbreak;\n\n\t\tcase I915_TILING_X:\n\t\t\ttile.swizzle = to_gt(i915)->ggtt->bit_6_swizzle_x;\n\t\t\tbreak;\n\t\tcase I915_TILING_Y:\n\t\t\ttile.swizzle = to_gt(i915)->ggtt->bit_6_swizzle_y;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tile.swizzle == I915_BIT_6_SWIZZLE_9_17 ||\n\t\t    tile.swizzle == I915_BIT_6_SWIZZLE_9_10_17)\n\t\t\tcontinue;\n\n\t\tif (tile.tiling != I915_TILING_NONE) {\n\t\t\tunsigned int max_pitch = setup_tile_size(&tile, i915);\n\n\t\t\ttile.stride =\n\t\t\t\ti915_prandom_u32_max_state(max_pitch, &prng);\n\t\t\ttile.stride = (1 + tile.stride) * tile.width;\n\t\t\tif (GRAPHICS_VER(i915) < 4)\n\t\t\t\ttile.stride = rounddown_pow_of_two(tile.stride);\n\t\t}\n\n\t\terr = check_partial_mapping(obj, &tile, &prng);\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tcount++;\n\t} while (!__igt_timeout(end, NULL));\n\n\tpr_info(\"%s: Completed %lu trials\\n\", __func__, count);\n\n\tintel_runtime_pm_put(&i915->runtime_pm, wakeref);\n\ti915_gem_object_unpin_pages(obj);\nout:\n\ti915_gem_object_put(obj);\n\treturn err;\n}\n\nstatic int make_obj_busy(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct intel_engine_cs *engine;\n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_request *rq;\n\t\tstruct i915_vma *vma;\n\t\tstruct i915_gem_ww_ctx ww;\n\t\tint err;\n\n\t\tvma = i915_vma_instance(obj, &engine->gt->ggtt->vm, NULL);\n\t\tif (IS_ERR(vma))\n\t\t\treturn PTR_ERR(vma);\n\n\t\ti915_gem_ww_ctx_init(&ww, false);\nretry:\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (!err)\n\t\t\terr = i915_vma_pin_ww(vma, &ww, 0, 0, PIN_USER);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\trq = intel_engine_create_kernel_request(engine);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto err_unpin;\n\t\t}\n\n\t\terr = i915_vma_move_to_active(vma, rq,\n\t\t\t\t\t      EXEC_OBJECT_WRITE);\n\n\t\ti915_request_add(rq);\nerr_unpin:\n\t\ti915_vma_unpin(vma);\nerr:\n\t\tif (err == -EDEADLK) {\n\t\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\t\tif (!err)\n\t\t\t\tgoto retry;\n\t\t}\n\t\ti915_gem_ww_ctx_fini(&ww);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\ti915_gem_object_put(obj);  \n\treturn 0;\n}\n\nstatic enum i915_mmap_type default_mapping(struct drm_i915_private *i915)\n{\n\tif (HAS_LMEM(i915))\n\t\treturn I915_MMAP_TYPE_FIXED;\n\n\treturn I915_MMAP_TYPE_GTT;\n}\n\nstatic struct drm_i915_gem_object *\ncreate_sys_or_internal(struct drm_i915_private *i915,\n\t\t       unsigned long size)\n{\n\tif (HAS_LMEM(i915)) {\n\t\tstruct intel_memory_region *sys_region =\n\t\t\ti915->mm.regions[INTEL_REGION_SMEM];\n\n\t\treturn __i915_gem_object_create_user(i915, size, &sys_region, 1);\n\t}\n\n\treturn i915_gem_object_create_internal(i915, size);\n}\n\nstatic bool assert_mmap_offset(struct drm_i915_private *i915,\n\t\t\t       unsigned long size,\n\t\t\t       int expected)\n{\n\tstruct drm_i915_gem_object *obj;\n\tu64 offset;\n\tint ret;\n\n\tobj = create_sys_or_internal(i915, size);\n\tif (IS_ERR(obj))\n\t\treturn expected && expected == PTR_ERR(obj);\n\n\tret = __assign_mmap_offset(obj, default_mapping(i915), &offset, NULL);\n\ti915_gem_object_put(obj);\n\n\treturn ret == expected;\n}\n\nstatic void disable_retire_worker(struct drm_i915_private *i915)\n{\n\ti915_gem_driver_unregister__shrinker(i915);\n\tintel_gt_pm_get(to_gt(i915));\n\tcancel_delayed_work_sync(&to_gt(i915)->requests.retire_work);\n}\n\nstatic void restore_retire_worker(struct drm_i915_private *i915)\n{\n\tigt_flush_test(i915);\n\tintel_gt_pm_put(to_gt(i915));\n\ti915_gem_driver_register__shrinker(i915);\n}\n\nstatic void mmap_offset_lock(struct drm_i915_private *i915)\n\t__acquires(&i915->drm.vma_offset_manager->vm_lock)\n{\n\twrite_lock(&i915->drm.vma_offset_manager->vm_lock);\n}\n\nstatic void mmap_offset_unlock(struct drm_i915_private *i915)\n\t__releases(&i915->drm.vma_offset_manager->vm_lock)\n{\n\twrite_unlock(&i915->drm.vma_offset_manager->vm_lock);\n}\n\nstatic int igt_mmap_offset_exhaustion(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct drm_mm *mm = &i915->drm.vma_offset_manager->vm_addr_space_mm;\n\tstruct drm_i915_gem_object *obj;\n\tstruct drm_mm_node *hole, *next;\n\tint loop, err = 0;\n\tu64 offset;\n\tint enospc = HAS_LMEM(i915) ? -ENXIO : -ENOSPC;\n\n\t \n\tdisable_retire_worker(i915);\n\tGEM_BUG_ON(!to_gt(i915)->awake);\n\tintel_gt_retire_requests(to_gt(i915));\n\ti915_gem_drain_freed_objects(i915);\n\n\t \n\tmmap_offset_lock(i915);\n\tloop = 1;  \n\tlist_for_each_entry_safe(hole, next, &mm->hole_stack, hole_stack) {\n\t\tstruct drm_mm_node *resv;\n\n\t\tresv = kzalloc(sizeof(*resv), GFP_NOWAIT);\n\t\tif (!resv) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_park;\n\t\t}\n\n\t\tresv->start = drm_mm_hole_node_start(hole) + loop;\n\t\tresv->size = hole->hole_size - loop;\n\t\tresv->color = -1ul;\n\t\tloop = 0;\n\n\t\tif (!resv->size) {\n\t\t\tkfree(resv);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_debug(\"Reserving hole [%llx + %llx]\\n\",\n\t\t\t resv->start, resv->size);\n\n\t\terr = drm_mm_reserve_node(mm, resv);\n\t\tif (err) {\n\t\t\tpr_err(\"Failed to trim VMA manager, err=%d\\n\", err);\n\t\t\tkfree(resv);\n\t\t\tgoto out_park;\n\t\t}\n\t}\n\tGEM_BUG_ON(!list_is_singular(&mm->hole_stack));\n\tmmap_offset_unlock(i915);\n\n\t \n\tif (!assert_mmap_offset(i915, PAGE_SIZE, 0)) {\n\t\tpr_err(\"Unable to insert object into single page hole\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!assert_mmap_offset(i915, 2 * PAGE_SIZE, enospc)) {\n\t\tpr_err(\"Unexpectedly succeeded in inserting too large object into single page hole\\n\");\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tobj = create_sys_or_internal(i915, PAGE_SIZE);\n\tif (IS_ERR(obj)) {\n\t\terr = PTR_ERR(obj);\n\t\tpr_err(\"Unable to create object for reclaimed hole\\n\");\n\t\tgoto out;\n\t}\n\n\terr = __assign_mmap_offset(obj, default_mapping(i915), &offset, NULL);\n\tif (err) {\n\t\tpr_err(\"Unable to insert object into reclaimed hole\\n\");\n\t\tgoto err_obj;\n\t}\n\n\tif (!assert_mmap_offset(i915, PAGE_SIZE, enospc)) {\n\t\tpr_err(\"Unexpectedly succeeded in inserting object into no holes!\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_obj;\n\t}\n\n\ti915_gem_object_put(obj);\n\n\t \n\tfor (loop = 0; loop < 3; loop++) {\n\t\tif (intel_gt_is_wedged(to_gt(i915)))\n\t\t\tbreak;\n\n\t\tobj = i915_gem_object_create_internal(i915, PAGE_SIZE);\n\t\tif (IS_ERR(obj)) {\n\t\t\terr = PTR_ERR(obj);\n\t\t\tgoto out;\n\t\t}\n\n\t\terr = make_obj_busy(obj);\n\t\tif (err) {\n\t\t\tpr_err(\"[loop %d] Failed to busy the object\\n\", loop);\n\t\t\tgoto err_obj;\n\t\t}\n\t}\n\nout:\n\tmmap_offset_lock(i915);\nout_park:\n\tdrm_mm_for_each_node_safe(hole, next, mm) {\n\t\tif (hole->color != -1ul)\n\t\t\tcontinue;\n\n\t\tdrm_mm_remove_node(hole);\n\t\tkfree(hole);\n\t}\n\tmmap_offset_unlock(i915);\n\trestore_retire_worker(i915);\n\treturn err;\nerr_obj:\n\ti915_gem_object_put(obj);\n\tgoto out;\n}\n\nstatic int gtt_set(struct drm_i915_gem_object *obj)\n{\n\tstruct i915_vma *vma;\n\tvoid __iomem *map;\n\tint err = 0;\n\n\tvma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0, PIN_MAPPABLE);\n\tif (IS_ERR(vma))\n\t\treturn PTR_ERR(vma);\n\n\tintel_gt_pm_get(vma->vm->gt);\n\tmap = i915_vma_pin_iomap(vma);\n\ti915_vma_unpin(vma);\n\tif (IS_ERR(map)) {\n\t\terr = PTR_ERR(map);\n\t\tgoto out;\n\t}\n\n\tmemset_io(map, POISON_INUSE, obj->base.size);\n\ti915_vma_unpin_iomap(vma);\n\nout:\n\tintel_gt_pm_put(vma->vm->gt);\n\treturn err;\n}\n\nstatic int gtt_check(struct drm_i915_gem_object *obj)\n{\n\tstruct i915_vma *vma;\n\tvoid __iomem *map;\n\tint err = 0;\n\n\tvma = i915_gem_object_ggtt_pin(obj, NULL, 0, 0, PIN_MAPPABLE);\n\tif (IS_ERR(vma))\n\t\treturn PTR_ERR(vma);\n\n\tintel_gt_pm_get(vma->vm->gt);\n\tmap = i915_vma_pin_iomap(vma);\n\ti915_vma_unpin(vma);\n\tif (IS_ERR(map)) {\n\t\terr = PTR_ERR(map);\n\t\tgoto out;\n\t}\n\n\tif (memchr_inv((void __force *)map, POISON_FREE, obj->base.size)) {\n\t\tpr_err(\"%s: Write via mmap did not land in backing store (GTT)\\n\",\n\t\t       obj->mm.region->name);\n\t\terr = -EINVAL;\n\t}\n\ti915_vma_unpin_iomap(vma);\n\nout:\n\tintel_gt_pm_put(vma->vm->gt);\n\treturn err;\n}\n\nstatic int wc_set(struct drm_i915_gem_object *obj)\n{\n\tvoid *vaddr;\n\n\tvaddr = i915_gem_object_pin_map_unlocked(obj, I915_MAP_WC);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\tmemset(vaddr, POISON_INUSE, obj->base.size);\n\ti915_gem_object_flush_map(obj);\n\ti915_gem_object_unpin_map(obj);\n\n\treturn 0;\n}\n\nstatic int wc_check(struct drm_i915_gem_object *obj)\n{\n\tvoid *vaddr;\n\tint err = 0;\n\n\tvaddr = i915_gem_object_pin_map_unlocked(obj, I915_MAP_WC);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\tif (memchr_inv(vaddr, POISON_FREE, obj->base.size)) {\n\t\tpr_err(\"%s: Write via mmap did not land in backing store (WC)\\n\",\n\t\t       obj->mm.region->name);\n\t\terr = -EINVAL;\n\t}\n\ti915_gem_object_unpin_map(obj);\n\n\treturn err;\n}\n\nstatic bool can_mmap(struct drm_i915_gem_object *obj, enum i915_mmap_type type)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tbool no_map;\n\n\tif (obj->ops->mmap_offset)\n\t\treturn type == I915_MMAP_TYPE_FIXED;\n\telse if (type == I915_MMAP_TYPE_FIXED)\n\t\treturn false;\n\n\tif (type == I915_MMAP_TYPE_GTT &&\n\t    !i915_ggtt_has_aperture(to_gt(i915)->ggtt))\n\t\treturn false;\n\n\ti915_gem_object_lock(obj, NULL);\n\tno_map = (type != I915_MMAP_TYPE_GTT &&\n\t\t  !i915_gem_object_has_struct_page(obj) &&\n\t\t  !i915_gem_object_has_iomem(obj));\n\ti915_gem_object_unlock(obj);\n\n\treturn !no_map;\n}\n\n#define expand32(x) (((x) << 0) | ((x) << 8) | ((x) << 16) | ((x) << 24))\nstatic int __igt_mmap(struct drm_i915_private *i915,\n\t\t      struct drm_i915_gem_object *obj,\n\t\t      enum i915_mmap_type type)\n{\n\tstruct vm_area_struct *area;\n\tunsigned long addr;\n\tint err, i;\n\tu64 offset;\n\n\tif (!can_mmap(obj, type))\n\t\treturn 0;\n\n\terr = wc_set(obj);\n\tif (err == -ENXIO)\n\t\terr = gtt_set(obj);\n\tif (err)\n\t\treturn err;\n\n\terr = __assign_mmap_offset(obj, type, &offset, NULL);\n\tif (err)\n\t\treturn err;\n\n\taddr = igt_mmap_offset(i915, offset, obj->base.size, PROT_WRITE, MAP_SHARED);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tpr_debug(\"igt_mmap(%s, %d) @ %lx\\n\", obj->mm.region->name, type, addr);\n\n\tmmap_read_lock(current->mm);\n\tarea = vma_lookup(current->mm, addr);\n\tmmap_read_unlock(current->mm);\n\tif (!area) {\n\t\tpr_err(\"%s: Did not create a vm_area_struct for the mmap\\n\",\n\t\t       obj->mm.region->name);\n\t\terr = -EINVAL;\n\t\tgoto out_unmap;\n\t}\n\n\tfor (i = 0; i < obj->base.size / sizeof(u32); i++) {\n\t\tu32 __user *ux = u64_to_user_ptr((u64)(addr + i * sizeof(*ux)));\n\t\tu32 x;\n\n\t\tif (get_user(x, ux)) {\n\t\t\tpr_err(\"%s: Unable to read from mmap, offset:%zd\\n\",\n\t\t\t       obj->mm.region->name, i * sizeof(x));\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\tif (x != expand32(POISON_INUSE)) {\n\t\t\tpr_err(\"%s: Read incorrect value from mmap, offset:%zd, found:%x, expected:%x\\n\",\n\t\t\t       obj->mm.region->name,\n\t\t\t       i * sizeof(x), x, expand32(POISON_INUSE));\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\tx = expand32(POISON_FREE);\n\t\tif (put_user(x, ux)) {\n\t\t\tpr_err(\"%s: Unable to write to mmap, offset:%zd\\n\",\n\t\t\t       obj->mm.region->name, i * sizeof(x));\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\t}\n\n\tif (type == I915_MMAP_TYPE_GTT)\n\t\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\terr = wc_check(obj);\n\tif (err == -ENXIO)\n\t\terr = gtt_check(obj);\nout_unmap:\n\tvm_munmap(addr, obj->base.size);\n\treturn err;\n}\n\nstatic int igt_mmap(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_memory_region *mr;\n\tenum intel_region_id id;\n\n\tfor_each_memory_region(mr, i915, id) {\n\t\tunsigned long sizes[] = {\n\t\t\tPAGE_SIZE,\n\t\t\tmr->min_page_size,\n\t\t\tSZ_4M,\n\t\t};\n\t\tint i;\n\n\t\tif (mr->private)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(sizes); i++) {\n\t\t\tstruct drm_i915_gem_object *obj;\n\t\t\tint err;\n\n\t\t\tobj = __i915_gem_object_create_user(i915, sizes[i], &mr, 1);\n\t\t\tif (obj == ERR_PTR(-ENODEV))\n\t\t\t\tcontinue;\n\n\t\t\tif (IS_ERR(obj))\n\t\t\t\treturn PTR_ERR(obj);\n\n\t\t\terr = __igt_mmap(i915, obj, I915_MMAP_TYPE_GTT);\n\t\t\tif (err == 0)\n\t\t\t\terr = __igt_mmap(i915, obj, I915_MMAP_TYPE_WC);\n\t\t\tif (err == 0)\n\t\t\t\terr = __igt_mmap(i915, obj, I915_MMAP_TYPE_FIXED);\n\n\t\t\ti915_gem_object_put(obj);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void igt_close_objects(struct drm_i915_private *i915,\n\t\t\t      struct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj, *on;\n\n\tlist_for_each_entry_safe(obj, on, objects, st_link) {\n\t\ti915_gem_object_lock(obj, NULL);\n\t\tif (i915_gem_object_has_pinned_pages(obj))\n\t\t\ti915_gem_object_unpin_pages(obj);\n\t\t \n\t\t__i915_gem_object_put_pages(obj);\n\t\ti915_gem_object_unlock(obj);\n\t\tlist_del(&obj->st_link);\n\t\ti915_gem_object_put(obj);\n\t}\n\n\tcond_resched();\n\n\ti915_gem_drain_freed_objects(i915);\n}\n\nstatic void igt_make_evictable(struct list_head *objects)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tlist_for_each_entry(obj, objects, st_link) {\n\t\ti915_gem_object_lock(obj, NULL);\n\t\tif (i915_gem_object_has_pinned_pages(obj))\n\t\t\ti915_gem_object_unpin_pages(obj);\n\t\ti915_gem_object_unlock(obj);\n\t}\n\n\tcond_resched();\n}\n\nstatic int igt_fill_mappable(struct intel_memory_region *mr,\n\t\t\t     struct list_head *objects)\n{\n\tu64 size, total;\n\tint err;\n\n\ttotal = 0;\n\tsize = mr->io_size;\n\tdo {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = i915_gem_object_create_region(mr, size, 0, 0);\n\t\tif (IS_ERR(obj)) {\n\t\t\terr = PTR_ERR(obj);\n\t\t\tgoto err_close;\n\t\t}\n\n\t\tlist_add(&obj->st_link, objects);\n\n\t\terr = i915_gem_object_pin_pages_unlocked(obj);\n\t\tif (err) {\n\t\t\tif (err != -ENXIO && err != -ENOMEM)\n\t\t\t\tgoto err_close;\n\n\t\t\tif (size == mr->min_page_size) {\n\t\t\t\terr = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tsize >>= 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\ttotal += obj->base.size;\n\t} while (1);\n\n\tpr_info(\"%s filled=%lluMiB\\n\", __func__, total >> 20);\n\treturn 0;\n\nerr_close:\n\tigt_close_objects(mr->i915, objects);\n\treturn err;\n}\n\nstatic int ___igt_mmap_migrate(struct drm_i915_private *i915,\n\t\t\t       struct drm_i915_gem_object *obj,\n\t\t\t       unsigned long addr,\n\t\t\t       bool unfaultable)\n{\n\tstruct vm_area_struct *area;\n\tint err = 0, i;\n\n\tpr_info(\"igt_mmap(%s, %d) @ %lx\\n\",\n\t\tobj->mm.region->name, I915_MMAP_TYPE_FIXED, addr);\n\n\tmmap_read_lock(current->mm);\n\tarea = vma_lookup(current->mm, addr);\n\tmmap_read_unlock(current->mm);\n\tif (!area) {\n\t\tpr_err(\"%s: Did not create a vm_area_struct for the mmap\\n\",\n\t\t       obj->mm.region->name);\n\t\terr = -EINVAL;\n\t\tgoto out_unmap;\n\t}\n\n\tfor (i = 0; i < obj->base.size / sizeof(u32); i++) {\n\t\tu32 __user *ux = u64_to_user_ptr((u64)(addr + i * sizeof(*ux)));\n\t\tu32 x;\n\n\t\tif (get_user(x, ux)) {\n\t\t\terr = -EFAULT;\n\t\t\tif (!unfaultable) {\n\t\t\t\tpr_err(\"%s: Unable to read from mmap, offset:%zd\\n\",\n\t\t\t\t       obj->mm.region->name, i * sizeof(x));\n\t\t\t\tgoto out_unmap;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (unfaultable) {\n\t\t\tpr_err(\"%s: Faulted unmappable memory\\n\",\n\t\t\t       obj->mm.region->name);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\tif (x != expand32(POISON_INUSE)) {\n\t\t\tpr_err(\"%s: Read incorrect value from mmap, offset:%zd, found:%x, expected:%x\\n\",\n\t\t\t       obj->mm.region->name,\n\t\t\t       i * sizeof(x), x, expand32(POISON_INUSE));\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\tx = expand32(POISON_FREE);\n\t\tif (put_user(x, ux)) {\n\t\t\tpr_err(\"%s: Unable to write to mmap, offset:%zd\\n\",\n\t\t\t       obj->mm.region->name, i * sizeof(x));\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_unmap;\n\t\t}\n\t}\n\n\tif (unfaultable) {\n\t\tif (err == -EFAULT)\n\t\t\terr = 0;\n\t} else {\n\t\tobj->flags &= ~I915_BO_ALLOC_GPU_ONLY;\n\t\terr = wc_check(obj);\n\t}\nout_unmap:\n\tvm_munmap(addr, obj->base.size);\n\treturn err;\n}\n\n#define IGT_MMAP_MIGRATE_TOPDOWN     (1 << 0)\n#define IGT_MMAP_MIGRATE_FILL        (1 << 1)\n#define IGT_MMAP_MIGRATE_EVICTABLE   (1 << 2)\n#define IGT_MMAP_MIGRATE_UNFAULTABLE (1 << 3)\n#define IGT_MMAP_MIGRATE_FAIL_GPU    (1 << 4)\nstatic int __igt_mmap_migrate(struct intel_memory_region **placements,\n\t\t\t      int n_placements,\n\t\t\t      struct intel_memory_region *expected_mr,\n\t\t\t      unsigned int flags)\n{\n\tstruct drm_i915_private *i915 = placements[0]->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_request *rq = NULL;\n\tunsigned long addr;\n\tLIST_HEAD(objects);\n\tu64 offset;\n\tint err;\n\n\tobj = __i915_gem_object_create_user(i915, PAGE_SIZE,\n\t\t\t\t\t    placements,\n\t\t\t\t\t    n_placements);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\tif (flags & IGT_MMAP_MIGRATE_TOPDOWN)\n\t\tobj->flags |= I915_BO_ALLOC_GPU_ONLY;\n\n\terr = __assign_mmap_offset(obj, I915_MMAP_TYPE_FIXED, &offset, NULL);\n\tif (err)\n\t\tgoto out_put;\n\n\t \n\taddr = igt_mmap_offset(i915, offset, obj->base.size,\n\t\t\t       PROT_WRITE, MAP_SHARED);\n\tif (IS_ERR_VALUE(addr)) {\n\t\terr = addr;\n\t\tgoto out_put;\n\t}\n\n\tif (flags & IGT_MMAP_MIGRATE_FILL) {\n\t\terr = igt_fill_mappable(placements[0], &objects);\n\t\tif (err)\n\t\t\tgoto out_put;\n\t}\n\n\terr = i915_gem_object_lock(obj, NULL);\n\tif (err)\n\t\tgoto out_put;\n\n\terr = i915_gem_object_pin_pages(obj);\n\tif (err) {\n\t\ti915_gem_object_unlock(obj);\n\t\tgoto out_put;\n\t}\n\n\terr = intel_context_migrate_clear(to_gt(i915)->migrate.context, NULL,\n\t\t\t\t\t  obj->mm.pages->sgl, obj->pat_index,\n\t\t\t\t\t  i915_gem_object_is_lmem(obj),\n\t\t\t\t\t  expand32(POISON_INUSE), &rq);\n\ti915_gem_object_unpin_pages(obj);\n\tif (rq) {\n\t\terr = dma_resv_reserve_fences(obj->base.resv, 1);\n\t\tif (!err)\n\t\t\tdma_resv_add_fence(obj->base.resv, &rq->fence,\n\t\t\t\t\t   DMA_RESV_USAGE_KERNEL);\n\t\ti915_request_put(rq);\n\t}\n\ti915_gem_object_unlock(obj);\n\tif (err)\n\t\tgoto out_put;\n\n\tif (flags & IGT_MMAP_MIGRATE_EVICTABLE)\n\t\tigt_make_evictable(&objects);\n\n\tif (flags & IGT_MMAP_MIGRATE_FAIL_GPU) {\n\t\terr = i915_gem_object_lock(obj, NULL);\n\t\tif (err)\n\t\t\tgoto out_put;\n\n\t\t \n\t\terr = i915_gem_object_wait_moving_fence(obj, true);\n\t\ti915_gem_object_unlock(obj);\n\t\tif (err)\n\t\t\tgoto out_put;\n\t\ti915_ttm_migrate_set_failure_modes(true, false);\n\t}\n\n\terr = ___igt_mmap_migrate(i915, obj, addr,\n\t\t\t\t  flags & IGT_MMAP_MIGRATE_UNFAULTABLE);\n\n\tif (!err && obj->mm.region != expected_mr) {\n\t\tpr_err(\"%s region mismatch %s\\n\", __func__, expected_mr->name);\n\t\terr = -EINVAL;\n\t}\n\n\tif (flags & IGT_MMAP_MIGRATE_FAIL_GPU) {\n\t\tstruct intel_gt *gt;\n\t\tunsigned int id;\n\n\t\ti915_ttm_migrate_set_failure_modes(false, false);\n\n\t\tfor_each_gt(gt, i915, id) {\n\t\t\tintel_wakeref_t wakeref;\n\t\t\tbool wedged;\n\n\t\t\tmutex_lock(&gt->reset.mutex);\n\t\t\twedged = test_bit(I915_WEDGED, &gt->reset.flags);\n\t\t\tmutex_unlock(&gt->reset.mutex);\n\t\t\tif (!wedged) {\n\t\t\t\tpr_err(\"gt(%u) not wedged\\n\", id);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\twakeref = intel_runtime_pm_get(gt->uncore->rpm);\n\t\t\tigt_global_reset_lock(gt);\n\t\t\tintel_gt_reset(gt, ALL_ENGINES, NULL);\n\t\t\tigt_global_reset_unlock(gt);\n\t\t\tintel_runtime_pm_put(gt->uncore->rpm, wakeref);\n\t\t}\n\n\t\tif (!i915_gem_object_has_unknown_state(obj)) {\n\t\t\tpr_err(\"object missing unknown_state\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\nout_put:\n\ti915_gem_object_put(obj);\n\tigt_close_objects(i915, &objects);\n\treturn err;\n}\n\nstatic int igt_mmap_migrate(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_memory_region *system = i915->mm.regions[INTEL_REGION_SMEM];\n\tstruct intel_memory_region *mr;\n\tenum intel_region_id id;\n\n\tfor_each_memory_region(mr, i915, id) {\n\t\tstruct intel_memory_region *mixed[] = { mr, system };\n\t\tstruct intel_memory_region *single[] = { mr };\n\t\tstruct ttm_resource_manager *man = mr->region_private;\n\t\tresource_size_t saved_io_size;\n\t\tint err;\n\n\t\tif (mr->private)\n\t\t\tcontinue;\n\n\t\tif (!mr->io_size)\n\t\t\tcontinue;\n\n\t\t \n\t\tsaved_io_size = mr->io_size;\n\t\tif (mr->io_size == mr->total) {\n\t\t\tresource_size_t io_size = mr->io_size;\n\n\t\t\tio_size = rounddown_pow_of_two(io_size >> 1);\n\t\t\tif (io_size < PAGE_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tmr->io_size = io_size;\n\t\t\ti915_ttm_buddy_man_force_visible_size(man,\n\t\t\t\t\t\t\t      io_size >> PAGE_SHIFT);\n\t\t}\n\n\t\t \n\t\terr = __igt_mmap_migrate(mixed, ARRAY_SIZE(mixed), mr, 0);\n\t\tif (err)\n\t\t\tgoto out_io_size;\n\n\t\t \n\t\terr = __igt_mmap_migrate(single, ARRAY_SIZE(single), mr,\n\t\t\t\t\t IGT_MMAP_MIGRATE_TOPDOWN |\n\t\t\t\t\t IGT_MMAP_MIGRATE_FILL |\n\t\t\t\t\t IGT_MMAP_MIGRATE_EVICTABLE);\n\t\tif (err)\n\t\t\tgoto out_io_size;\n\n\t\t \n\t\terr = __igt_mmap_migrate(mixed, ARRAY_SIZE(mixed), system,\n\t\t\t\t\t IGT_MMAP_MIGRATE_TOPDOWN |\n\t\t\t\t\t IGT_MMAP_MIGRATE_FILL);\n\t\tif (err)\n\t\t\tgoto out_io_size;\n\n\t\t \n\t\terr = __igt_mmap_migrate(single, ARRAY_SIZE(single), mr,\n\t\t\t\t\t IGT_MMAP_MIGRATE_TOPDOWN |\n\t\t\t\t\t IGT_MMAP_MIGRATE_FILL |\n\t\t\t\t\t IGT_MMAP_MIGRATE_UNFAULTABLE);\n\t\tif (err)\n\t\t\tgoto out_io_size;\n\n\t\t \n\t\terr = __igt_mmap_migrate(single, ARRAY_SIZE(single), mr,\n\t\t\t\t\t IGT_MMAP_MIGRATE_TOPDOWN |\n\t\t\t\t\t IGT_MMAP_MIGRATE_FILL |\n\t\t\t\t\t IGT_MMAP_MIGRATE_EVICTABLE |\n\t\t\t\t\t IGT_MMAP_MIGRATE_FAIL_GPU |\n\t\t\t\t\t IGT_MMAP_MIGRATE_UNFAULTABLE);\nout_io_size:\n\t\tmr->io_size = saved_io_size;\n\t\ti915_ttm_buddy_man_force_visible_size(man,\n\t\t\t\t\t\t      mr->io_size >> PAGE_SHIFT);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic const char *repr_mmap_type(enum i915_mmap_type type)\n{\n\tswitch (type) {\n\tcase I915_MMAP_TYPE_GTT: return \"gtt\";\n\tcase I915_MMAP_TYPE_WB: return \"wb\";\n\tcase I915_MMAP_TYPE_WC: return \"wc\";\n\tcase I915_MMAP_TYPE_UC: return \"uc\";\n\tcase I915_MMAP_TYPE_FIXED: return \"fixed\";\n\tdefault: return \"unknown\";\n\t}\n}\n\nstatic bool can_access(struct drm_i915_gem_object *obj)\n{\n\tbool access;\n\n\ti915_gem_object_lock(obj, NULL);\n\taccess = i915_gem_object_has_struct_page(obj) ||\n\t\ti915_gem_object_has_iomem(obj);\n\ti915_gem_object_unlock(obj);\n\n\treturn access;\n}\n\nstatic int __igt_mmap_access(struct drm_i915_private *i915,\n\t\t\t     struct drm_i915_gem_object *obj,\n\t\t\t     enum i915_mmap_type type)\n{\n\tunsigned long __user *ptr;\n\tunsigned long A, B;\n\tunsigned long x, y;\n\tunsigned long addr;\n\tint err;\n\tu64 offset;\n\n\tmemset(&A, 0xAA, sizeof(A));\n\tmemset(&B, 0xBB, sizeof(B));\n\n\tif (!can_mmap(obj, type) || !can_access(obj))\n\t\treturn 0;\n\n\terr = __assign_mmap_offset(obj, type, &offset, NULL);\n\tif (err)\n\t\treturn err;\n\n\taddr = igt_mmap_offset(i915, offset, obj->base.size, PROT_WRITE, MAP_SHARED);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\tptr = (unsigned long __user *)addr;\n\n\terr = __put_user(A, ptr);\n\tif (err) {\n\t\tpr_err(\"%s(%s): failed to write into user mmap\\n\",\n\t\t       obj->mm.region->name, repr_mmap_type(type));\n\t\tgoto out_unmap;\n\t}\n\n\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\terr = access_process_vm(current, addr, &x, sizeof(x), 0);\n\tif (err != sizeof(x)) {\n\t\tpr_err(\"%s(%s): access_process_vm() read failed\\n\",\n\t\t       obj->mm.region->name, repr_mmap_type(type));\n\t\tgoto out_unmap;\n\t}\n\n\terr = access_process_vm(current, addr, &B, sizeof(B), FOLL_WRITE);\n\tif (err != sizeof(B)) {\n\t\tpr_err(\"%s(%s): access_process_vm() write failed\\n\",\n\t\t       obj->mm.region->name, repr_mmap_type(type));\n\t\tgoto out_unmap;\n\t}\n\n\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\terr = __get_user(y, ptr);\n\tif (err) {\n\t\tpr_err(\"%s(%s): failed to read from user mmap\\n\",\n\t\t       obj->mm.region->name, repr_mmap_type(type));\n\t\tgoto out_unmap;\n\t}\n\n\tif (x != A || y != B) {\n\t\tpr_err(\"%s(%s): failed to read/write values, found (%lx, %lx)\\n\",\n\t\t       obj->mm.region->name, repr_mmap_type(type),\n\t\t       x, y);\n\t\terr = -EINVAL;\n\t\tgoto out_unmap;\n\t}\n\nout_unmap:\n\tvm_munmap(addr, obj->base.size);\n\treturn err;\n}\n\nstatic int igt_mmap_access(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_memory_region *mr;\n\tenum intel_region_id id;\n\n\tfor_each_memory_region(mr, i915, id) {\n\t\tstruct drm_i915_gem_object *obj;\n\t\tint err;\n\n\t\tif (mr->private)\n\t\t\tcontinue;\n\n\t\tobj = __i915_gem_object_create_user(i915, PAGE_SIZE, &mr, 1);\n\t\tif (obj == ERR_PTR(-ENODEV))\n\t\t\tcontinue;\n\n\t\tif (IS_ERR(obj))\n\t\t\treturn PTR_ERR(obj);\n\n\t\terr = __igt_mmap_access(i915, obj, I915_MMAP_TYPE_GTT);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_access(i915, obj, I915_MMAP_TYPE_WB);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_access(i915, obj, I915_MMAP_TYPE_WC);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_access(i915, obj, I915_MMAP_TYPE_UC);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_access(i915, obj, I915_MMAP_TYPE_FIXED);\n\n\t\ti915_gem_object_put(obj);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int __igt_mmap_gpu(struct drm_i915_private *i915,\n\t\t\t  struct drm_i915_gem_object *obj,\n\t\t\t  enum i915_mmap_type type)\n{\n\tstruct intel_engine_cs *engine;\n\tunsigned long addr;\n\tu32 __user *ux;\n\tu32 bbe;\n\tint err;\n\tu64 offset;\n\n\t \n\n\tif (!can_mmap(obj, type))\n\t\treturn 0;\n\n\terr = wc_set(obj);\n\tif (err == -ENXIO)\n\t\terr = gtt_set(obj);\n\tif (err)\n\t\treturn err;\n\n\terr = __assign_mmap_offset(obj, type, &offset, NULL);\n\tif (err)\n\t\treturn err;\n\n\taddr = igt_mmap_offset(i915, offset, obj->base.size, PROT_WRITE, MAP_SHARED);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\tux = u64_to_user_ptr((u64)addr);\n\tbbe = MI_BATCH_BUFFER_END;\n\tif (put_user(bbe, ux)) {\n\t\tpr_err(\"%s: Unable to write to mmap\\n\", obj->mm.region->name);\n\t\terr = -EFAULT;\n\t\tgoto out_unmap;\n\t}\n\n\tif (type == I915_MMAP_TYPE_GTT)\n\t\tintel_gt_flush_ggtt_writes(to_gt(i915));\n\n\tfor_each_uabi_engine(engine, i915) {\n\t\tstruct i915_request *rq;\n\t\tstruct i915_vma *vma;\n\t\tstruct i915_gem_ww_ctx ww;\n\n\t\tvma = i915_vma_instance(obj, engine->kernel_context->vm, NULL);\n\t\tif (IS_ERR(vma)) {\n\t\t\terr = PTR_ERR(vma);\n\t\t\tgoto out_unmap;\n\t\t}\n\n\t\ti915_gem_ww_ctx_init(&ww, false);\nretry:\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (!err)\n\t\t\terr = i915_vma_pin_ww(vma, &ww, 0, 0, PIN_USER);\n\t\tif (err)\n\t\t\tgoto out_ww;\n\n\t\trq = i915_request_create(engine->kernel_context);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_unpin;\n\t\t}\n\n\t\terr = i915_vma_move_to_active(vma, rq, 0);\n\n\t\terr = engine->emit_bb_start(rq, i915_vma_offset(vma), 0, 0);\n\t\ti915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\tif (i915_request_wait(rq, 0, HZ / 5) < 0) {\n\t\t\tstruct drm_printer p =\n\t\t\t\tdrm_info_printer(engine->i915->drm.dev);\n\n\t\t\tpr_err(\"%s(%s, %s): Failed to execute batch\\n\",\n\t\t\t       __func__, engine->name, obj->mm.region->name);\n\t\t\tintel_engine_dump(engine, &p,\n\t\t\t\t\t  \"%s\\n\", engine->name);\n\n\t\t\tintel_gt_set_wedged(engine->gt);\n\t\t\terr = -EIO;\n\t\t}\n\t\ti915_request_put(rq);\n\nout_unpin:\n\t\ti915_vma_unpin(vma);\nout_ww:\n\t\tif (err == -EDEADLK) {\n\t\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\t\tif (!err)\n\t\t\t\tgoto retry;\n\t\t}\n\t\ti915_gem_ww_ctx_fini(&ww);\n\t\tif (err)\n\t\t\tgoto out_unmap;\n\t}\n\nout_unmap:\n\tvm_munmap(addr, obj->base.size);\n\treturn err;\n}\n\nstatic int igt_mmap_gpu(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_memory_region *mr;\n\tenum intel_region_id id;\n\n\tfor_each_memory_region(mr, i915, id) {\n\t\tstruct drm_i915_gem_object *obj;\n\t\tint err;\n\n\t\tif (mr->private)\n\t\t\tcontinue;\n\n\t\tobj = __i915_gem_object_create_user(i915, PAGE_SIZE, &mr, 1);\n\t\tif (obj == ERR_PTR(-ENODEV))\n\t\t\tcontinue;\n\n\t\tif (IS_ERR(obj))\n\t\t\treturn PTR_ERR(obj);\n\n\t\terr = __igt_mmap_gpu(i915, obj, I915_MMAP_TYPE_GTT);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_gpu(i915, obj, I915_MMAP_TYPE_WC);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_gpu(i915, obj, I915_MMAP_TYPE_FIXED);\n\n\t\ti915_gem_object_put(obj);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_present_pte(pte_t *pte, unsigned long addr, void *data)\n{\n\tpte_t ptent = ptep_get(pte);\n\n\tif (!pte_present(ptent) || pte_none(ptent)) {\n\t\tpr_err(\"missing PTE:%lx\\n\",\n\t\t       (addr - (unsigned long)data) >> PAGE_SHIFT);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_absent_pte(pte_t *pte, unsigned long addr, void *data)\n{\n\tpte_t ptent = ptep_get(pte);\n\n\tif (pte_present(ptent) && !pte_none(ptent)) {\n\t\tpr_err(\"present PTE:%lx; expected to be revoked\\n\",\n\t\t       (addr - (unsigned long)data) >> PAGE_SHIFT);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int check_present(unsigned long addr, unsigned long len)\n{\n\treturn apply_to_page_range(current->mm, addr, len,\n\t\t\t\t   check_present_pte, (void *)addr);\n}\n\nstatic int check_absent(unsigned long addr, unsigned long len)\n{\n\treturn apply_to_page_range(current->mm, addr, len,\n\t\t\t\t   check_absent_pte, (void *)addr);\n}\n\nstatic int prefault_range(u64 start, u64 len)\n{\n\tconst char __user *addr, *end;\n\tchar __maybe_unused c;\n\tint err;\n\n\taddr = u64_to_user_ptr(start);\n\tend = addr + len;\n\n\tfor (; addr < end; addr += PAGE_SIZE) {\n\t\terr = __get_user(c, addr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn __get_user(c, end - 1);\n}\n\nstatic int __igt_mmap_revoke(struct drm_i915_private *i915,\n\t\t\t     struct drm_i915_gem_object *obj,\n\t\t\t     enum i915_mmap_type type)\n{\n\tunsigned long addr;\n\tint err;\n\tu64 offset;\n\n\tif (!can_mmap(obj, type))\n\t\treturn 0;\n\n\terr = __assign_mmap_offset(obj, type, &offset, NULL);\n\tif (err)\n\t\treturn err;\n\n\taddr = igt_mmap_offset(i915, offset, obj->base.size, PROT_WRITE, MAP_SHARED);\n\tif (IS_ERR_VALUE(addr))\n\t\treturn addr;\n\n\terr = prefault_range(addr, obj->base.size);\n\tif (err)\n\t\tgoto out_unmap;\n\n\terr = check_present(addr, obj->base.size);\n\tif (err) {\n\t\tpr_err(\"%s: was not present\\n\", obj->mm.region->name);\n\t\tgoto out_unmap;\n\t}\n\n\t \n\ti915_gem_object_lock(obj, NULL);\n\terr = i915_gem_object_unbind(obj, I915_GEM_OBJECT_UNBIND_ACTIVE);\n\ti915_gem_object_unlock(obj);\n\tif (err) {\n\t\tpr_err(\"Failed to unbind object!\\n\");\n\t\tgoto out_unmap;\n\t}\n\n\tif (type != I915_MMAP_TYPE_GTT) {\n\t\ti915_gem_object_lock(obj, NULL);\n\t\t__i915_gem_object_put_pages(obj);\n\t\ti915_gem_object_unlock(obj);\n\t\tif (i915_gem_object_has_pages(obj)) {\n\t\t\tpr_err(\"Failed to put-pages object!\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_unmap;\n\t\t}\n\t}\n\n\terr = check_absent(addr, obj->base.size);\n\tif (err) {\n\t\tpr_err(\"%s: was not absent\\n\", obj->mm.region->name);\n\t\tgoto out_unmap;\n\t}\n\nout_unmap:\n\tvm_munmap(addr, obj->base.size);\n\treturn err;\n}\n\nstatic int igt_mmap_revoke(void *arg)\n{\n\tstruct drm_i915_private *i915 = arg;\n\tstruct intel_memory_region *mr;\n\tenum intel_region_id id;\n\n\tfor_each_memory_region(mr, i915, id) {\n\t\tstruct drm_i915_gem_object *obj;\n\t\tint err;\n\n\t\tif (mr->private)\n\t\t\tcontinue;\n\n\t\tobj = __i915_gem_object_create_user(i915, PAGE_SIZE, &mr, 1);\n\t\tif (obj == ERR_PTR(-ENODEV))\n\t\t\tcontinue;\n\n\t\tif (IS_ERR(obj))\n\t\t\treturn PTR_ERR(obj);\n\n\t\terr = __igt_mmap_revoke(i915, obj, I915_MMAP_TYPE_GTT);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_revoke(i915, obj, I915_MMAP_TYPE_WC);\n\t\tif (err == 0)\n\t\t\terr = __igt_mmap_revoke(i915, obj, I915_MMAP_TYPE_FIXED);\n\n\t\ti915_gem_object_put(obj);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint i915_gem_mman_live_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(igt_partial_tiling),\n\t\tSUBTEST(igt_smoke_tiling),\n\t\tSUBTEST(igt_mmap_offset_exhaustion),\n\t\tSUBTEST(igt_mmap),\n\t\tSUBTEST(igt_mmap_migrate),\n\t\tSUBTEST(igt_mmap_access),\n\t\tSUBTEST(igt_mmap_revoke),\n\t\tSUBTEST(igt_mmap_gpu),\n\t};\n\n\treturn i915_live_subtests(tests, i915);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}