{
  "module_name": "i915_gem_migrate.c",
  "hash_id": "41e11a2b6680d72686b08db7e7835145d7a7be7800e1eed8ce24ea23e820141f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/selftests/i915_gem_migrate.c",
  "human_readable_source": "\n \n\n#include \"gt/intel_migrate.h\"\n#include \"gt/intel_gpu_commands.h\"\n#include \"gem/i915_gem_ttm_move.h\"\n\n#include \"i915_deps.h\"\n\n#include \"selftests/igt_reset.h\"\n#include \"selftests/igt_spinner.h\"\n\nstatic int igt_fill_check_buffer(struct drm_i915_gem_object *obj,\n\t\t\t\t struct intel_gt *gt,\n\t\t\t\t bool fill)\n{\n\tunsigned int i, count = obj->base.size / sizeof(u32);\n\tenum i915_map_type map_type =\n\t\tintel_gt_coherent_map_type(gt, obj, false);\n\tu32 *cur;\n\tint err = 0;\n\n\tassert_object_held(obj);\n\tcur = i915_gem_object_pin_map(obj, map_type);\n\tif (IS_ERR(cur))\n\t\treturn PTR_ERR(cur);\n\n\tif (fill)\n\t\tfor (i = 0; i < count; ++i)\n\t\t\t*cur++ = i;\n\telse\n\t\tfor (i = 0; i < count; ++i)\n\t\t\tif (*cur++ != i) {\n\t\t\t\tpr_err(\"Object content mismatch at location %d of %d\\n\", i, count);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\ti915_gem_object_unpin_map(obj);\n\n\treturn err;\n}\n\nstatic int igt_create_migrate(struct intel_gt *gt, enum intel_region_id src,\n\t\t\t      enum intel_region_id dst)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_memory_region *src_mr = i915->mm.regions[src];\n\tstruct intel_memory_region *dst_mr = i915->mm.regions[dst];\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_gem_ww_ctx ww;\n\tint err = 0;\n\n\tGEM_BUG_ON(!src_mr);\n\tGEM_BUG_ON(!dst_mr);\n\n\t \n\tobj = i915_gem_object_create_region(src_mr, dst_mr->min_page_size, 0, 0);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\tfor_i915_gem_ww(&ww, err, true) {\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = igt_fill_check_buffer(obj, gt, true);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = i915_gem_object_migrate(obj, &ww, dst);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = i915_gem_object_pin_pages(obj);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tif (i915_gem_object_can_migrate(obj, src))\n\t\t\terr = -EINVAL;\n\n\t\ti915_gem_object_unpin_pages(obj);\n\t\terr = i915_gem_object_wait_migration(obj, true);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = igt_fill_check_buffer(obj, gt, false);\n\t}\n\ti915_gem_object_put(obj);\n\n\treturn err;\n}\n\nstatic int igt_smem_create_migrate(void *arg)\n{\n\treturn igt_create_migrate(arg, INTEL_REGION_LMEM_0, INTEL_REGION_SMEM);\n}\n\nstatic int igt_lmem_create_migrate(void *arg)\n{\n\treturn igt_create_migrate(arg, INTEL_REGION_SMEM, INTEL_REGION_LMEM_0);\n}\n\nstatic int igt_same_create_migrate(void *arg)\n{\n\treturn igt_create_migrate(arg, INTEL_REGION_LMEM_0, INTEL_REGION_LMEM_0);\n}\n\nstatic int lmem_pages_migrate_one(struct i915_gem_ww_ctx *ww,\n\t\t\t\t  struct drm_i915_gem_object *obj,\n\t\t\t\t  struct i915_vma *vma,\n\t\t\t\t  bool silent_migrate)\n{\n\tint err;\n\n\terr = i915_gem_object_lock(obj, ww);\n\tif (err)\n\t\treturn err;\n\n\tif (vma) {\n\t\terr = i915_vma_pin_ww(vma, ww, obj->base.size, 0,\n\t\t\t\t      0UL | PIN_OFFSET_FIXED |\n\t\t\t\t      PIN_USER);\n\t\tif (err) {\n\t\t\tif (err != -EINTR && err != ERESTARTSYS &&\n\t\t\t    err != -EDEADLK)\n\t\t\t\tpr_err(\"Failed to pin vma.\\n\");\n\t\t\treturn err;\n\t\t}\n\n\t\ti915_vma_unpin(vma);\n\t}\n\n\t \n\tif (i915_gem_object_is_lmem(obj)) {\n\t\terr = i915_gem_object_migrate(obj, ww, INTEL_REGION_SMEM);\n\t\tif (err) {\n\t\t\tif (!silent_migrate)\n\t\t\t\tpr_err(\"Object failed migration to smem\\n\");\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (i915_gem_object_is_lmem(obj)) {\n\t\t\tpr_err(\"object still backed by lmem\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (!i915_gem_object_has_struct_page(obj)) {\n\t\t\tpr_err(\"object not backed by struct page\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t} else {\n\t\terr = i915_gem_object_migrate(obj, ww, INTEL_REGION_LMEM_0);\n\t\tif (err) {\n\t\t\tif (!silent_migrate)\n\t\t\t\tpr_err(\"Object failed migration to lmem\\n\");\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (i915_gem_object_has_struct_page(obj)) {\n\t\t\tpr_err(\"object still backed by struct page\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\n\t\tif (!i915_gem_object_is_lmem(obj)) {\n\t\t\tpr_err(\"object not backed by lmem\\n\");\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int __igt_lmem_pages_migrate(struct intel_gt *gt,\n\t\t\t\t    struct i915_address_space *vm,\n\t\t\t\t    struct i915_deps *deps,\n\t\t\t\t    struct igt_spinner *spin,\n\t\t\t\t    struct dma_fence *spin_fence,\n\t\t\t\t    bool borked_migrate)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma = NULL;\n\tstruct i915_gem_ww_ctx ww;\n\tstruct i915_request *rq;\n\tint err;\n\tint i;\n\n\t \n\n\tobj = i915_gem_object_create_lmem(i915, SZ_2M, 0);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\tif (vm) {\n\t\tvma = i915_vma_instance(obj, vm, NULL);\n\t\tif (IS_ERR(vma)) {\n\t\t\terr = PTR_ERR(vma);\n\t\t\tgoto out_put;\n\t\t}\n\t}\n\n\t \n\tfor_i915_gem_ww(&ww, err, true) {\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = ____i915_gem_object_get_pages(obj);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = intel_migrate_clear(&gt->migrate, &ww, deps,\n\t\t\t\t\t  obj->mm.pages->sgl, obj->pat_index,\n\t\t\t\t\t  i915_gem_object_is_lmem(obj),\n\t\t\t\t\t  0xdeadbeaf, &rq);\n\t\tif (rq) {\n\t\t\terr = dma_resv_reserve_fences(obj->base.resv, 1);\n\t\t\tif (!err)\n\t\t\t\tdma_resv_add_fence(obj->base.resv, &rq->fence,\n\t\t\t\t\t\t   DMA_RESV_USAGE_KERNEL);\n\t\t\ti915_request_put(rq);\n\t\t}\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tif (!vma) {\n\t\t\terr = igt_fill_check_buffer(obj, gt, true);\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\t\t}\n\t}\n\tif (err)\n\t\tgoto out_put;\n\n\t \n\tfor (i = 1; i <= 5; ++i) {\n\t\tfor_i915_gem_ww(&ww, err, true)\n\t\t\terr = lmem_pages_migrate_one(&ww, obj, vma,\n\t\t\t\t\t\t     borked_migrate);\n\t\tif (err)\n\t\t\tgoto out_put;\n\t}\n\n\terr = i915_gem_object_lock_interruptible(obj, NULL);\n\tif (err)\n\t\tgoto out_put;\n\n\tif (spin) {\n\t\tif (dma_fence_is_signaled(spin_fence)) {\n\t\t\tpr_err(\"Spinner was terminated by hangcheck.\\n\");\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tigt_spinner_end(spin);\n\t}\n\n\t \n\terr = i915_gem_object_wait_migration(obj, true);\n\tif (err)\n\t\tgoto out_unlock;\n\n\tif (vma) {\n\t\terr = i915_vma_wait_for_bind(vma);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t} else {\n\t\terr = igt_fill_check_buffer(obj, gt, false);\n\t}\n\nout_unlock:\n\ti915_gem_object_unlock(obj);\nout_put:\n\ti915_gem_object_put(obj);\n\n\treturn err;\n}\n\nstatic int igt_lmem_pages_failsafe_migrate(void *arg)\n{\n\tint fail_gpu, fail_alloc, ban_memcpy, ret;\n\tstruct intel_gt *gt = arg;\n\n\tfor (fail_gpu = 0; fail_gpu < 2; ++fail_gpu) {\n\t\tfor (fail_alloc = 0; fail_alloc < 2; ++fail_alloc) {\n\t\t\tfor (ban_memcpy = 0; ban_memcpy < 2; ++ban_memcpy) {\n\t\t\t\tpr_info(\"Simulated failure modes: gpu: %d, alloc:%d, ban_memcpy: %d\\n\",\n\t\t\t\t\tfail_gpu, fail_alloc, ban_memcpy);\n\t\t\t\ti915_ttm_migrate_set_ban_memcpy(ban_memcpy);\n\t\t\t\ti915_ttm_migrate_set_failure_modes(fail_gpu,\n\t\t\t\t\t\t\t\t   fail_alloc);\n\t\t\t\tret = __igt_lmem_pages_migrate(gt, NULL, NULL,\n\t\t\t\t\t\t\t       NULL, NULL,\n\t\t\t\t\t\t\t       ban_memcpy &&\n\t\t\t\t\t\t\t       fail_gpu);\n\n\t\t\t\tif (ban_memcpy && fail_gpu) {\n\t\t\t\t\tstruct intel_gt *__gt;\n\t\t\t\t\tunsigned int id;\n\n\t\t\t\t\tif (ret != -EIO) {\n\t\t\t\t\t\tpr_err(\"expected -EIO, got (%d)\\n\", ret);\n\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tret = 0;\n\t\t\t\t\t}\n\n\t\t\t\t\tfor_each_gt(__gt, gt->i915, id) {\n\t\t\t\t\t\tintel_wakeref_t wakeref;\n\t\t\t\t\t\tbool wedged;\n\n\t\t\t\t\t\tmutex_lock(&__gt->reset.mutex);\n\t\t\t\t\t\twedged = test_bit(I915_WEDGED, &__gt->reset.flags);\n\t\t\t\t\t\tmutex_unlock(&__gt->reset.mutex);\n\n\t\t\t\t\t\tif (fail_gpu && !fail_alloc) {\n\t\t\t\t\t\t\tif (!wedged) {\n\t\t\t\t\t\t\t\tpr_err(\"gt(%u) not wedged\\n\", id);\n\t\t\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else if (wedged) {\n\t\t\t\t\t\t\tpr_err(\"gt(%u) incorrectly wedged\\n\", id);\n\t\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\twakeref = intel_runtime_pm_get(__gt->uncore->rpm);\n\t\t\t\t\t\tigt_global_reset_lock(__gt);\n\t\t\t\t\t\tintel_gt_reset(__gt, ALL_ENGINES, NULL);\n\t\t\t\t\t\tigt_global_reset_unlock(__gt);\n\t\t\t\t\t\tintel_runtime_pm_put(__gt->uncore->rpm, wakeref);\n\t\t\t\t\t}\n\t\t\t\t\tif (ret)\n\t\t\t\t\t\tgoto out_err;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\nout_err:\n\ti915_ttm_migrate_set_failure_modes(false, false);\n\ti915_ttm_migrate_set_ban_memcpy(false);\n\treturn ret;\n}\n\n \nstatic int igt_async_migrate(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tstruct i915_ppgtt *ppgtt;\n\tstruct igt_spinner spin;\n\tint err;\n\n\tppgtt = i915_ppgtt_create(gt, 0);\n\tif (IS_ERR(ppgtt))\n\t\treturn PTR_ERR(ppgtt);\n\n\tif (igt_spinner_init(&spin, gt)) {\n\t\terr = -ENOMEM;\n\t\tgoto out_spin;\n\t}\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct ttm_operation_ctx ctx = {\n\t\t\t.interruptible = true\n\t\t};\n\t\tstruct dma_fence *spin_fence;\n\t\tstruct intel_context *ce;\n\t\tstruct i915_request *rq;\n\t\tstruct i915_deps deps;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce)) {\n\t\t\terr = PTR_ERR(ce);\n\t\t\tgoto out_ce;\n\t\t}\n\n\t\t \n\t\trq = igt_spinner_create_request(&spin, ce, MI_NOOP);\n\t\tintel_context_put(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_ce;\n\t\t}\n\n\t\ti915_deps_init(&deps, GFP_KERNEL);\n\t\terr = i915_deps_add_dependency(&deps, &rq->fence, &ctx);\n\t\tspin_fence = dma_fence_get(&rq->fence);\n\t\ti915_request_add(rq);\n\t\tif (err)\n\t\t\tgoto out_ce;\n\n\t\terr = __igt_lmem_pages_migrate(gt, &ppgtt->vm, &deps, &spin,\n\t\t\t\t\t       spin_fence, false);\n\t\ti915_deps_fini(&deps);\n\t\tdma_fence_put(spin_fence);\n\t\tif (err)\n\t\t\tgoto out_ce;\n\t}\n\nout_ce:\n\tigt_spinner_fini(&spin);\nout_spin:\n\ti915_vm_put(&ppgtt->vm);\n\n\treturn err;\n}\n\n \n#define ASYNC_FAIL_ALLOC 1\nstatic int igt_lmem_async_migrate(void *arg)\n{\n\tint fail_gpu, fail_alloc, ban_memcpy, ret;\n\tstruct intel_gt *gt = arg;\n\n\tfor (fail_gpu = 0; fail_gpu < 2; ++fail_gpu) {\n\t\tfor (fail_alloc = 0; fail_alloc < ASYNC_FAIL_ALLOC; ++fail_alloc) {\n\t\t\tfor (ban_memcpy = 0; ban_memcpy < 2; ++ban_memcpy) {\n\t\t\t\tpr_info(\"Simulated failure modes: gpu: %d, alloc: %d, ban_memcpy: %d\\n\",\n\t\t\t\t\tfail_gpu, fail_alloc, ban_memcpy);\n\t\t\t\ti915_ttm_migrate_set_ban_memcpy(ban_memcpy);\n\t\t\t\ti915_ttm_migrate_set_failure_modes(fail_gpu,\n\t\t\t\t\t\t\t\t   fail_alloc);\n\t\t\t\tret = igt_async_migrate(gt);\n\n\t\t\t\tif (fail_gpu && ban_memcpy) {\n\t\t\t\t\tstruct intel_gt *__gt;\n\t\t\t\t\tunsigned int id;\n\n\t\t\t\t\tif (ret != -EIO) {\n\t\t\t\t\t\tpr_err(\"expected -EIO, got (%d)\\n\", ret);\n\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tret = 0;\n\t\t\t\t\t}\n\n\t\t\t\t\tfor_each_gt(__gt, gt->i915, id) {\n\t\t\t\t\t\tintel_wakeref_t wakeref;\n\t\t\t\t\t\tbool wedged;\n\n\t\t\t\t\t\tmutex_lock(&__gt->reset.mutex);\n\t\t\t\t\t\twedged = test_bit(I915_WEDGED, &__gt->reset.flags);\n\t\t\t\t\t\tmutex_unlock(&__gt->reset.mutex);\n\n\t\t\t\t\t\tif (fail_gpu && !fail_alloc) {\n\t\t\t\t\t\t\tif (!wedged) {\n\t\t\t\t\t\t\t\tpr_err(\"gt(%u) not wedged\\n\", id);\n\t\t\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} else if (wedged) {\n\t\t\t\t\t\t\tpr_err(\"gt(%u) incorrectly wedged\\n\", id);\n\t\t\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\twakeref = intel_runtime_pm_get(__gt->uncore->rpm);\n\t\t\t\t\t\tigt_global_reset_lock(__gt);\n\t\t\t\t\t\tintel_gt_reset(__gt, ALL_ENGINES, NULL);\n\t\t\t\t\t\tigt_global_reset_unlock(__gt);\n\t\t\t\t\t\tintel_runtime_pm_put(__gt->uncore->rpm, wakeref);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t}\n\t}\n\nout_err:\n\ti915_ttm_migrate_set_failure_modes(false, false);\n\ti915_ttm_migrate_set_ban_memcpy(false);\n\treturn ret;\n}\n\nint i915_gem_migrate_live_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(igt_smem_create_migrate),\n\t\tSUBTEST(igt_lmem_create_migrate),\n\t\tSUBTEST(igt_same_create_migrate),\n\t\tSUBTEST(igt_lmem_pages_failsafe_migrate),\n\t\tSUBTEST(igt_lmem_async_migrate),\n\t};\n\n\tif (!HAS_LMEM(i915))\n\t\treturn 0;\n\n\treturn intel_gt_live_subtests(tests, to_gt(i915));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}