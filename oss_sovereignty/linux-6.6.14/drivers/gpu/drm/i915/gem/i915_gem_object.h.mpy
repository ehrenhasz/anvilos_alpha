{
  "module_name": "i915_gem_object.h",
  "hash_id": "8f57eab133683138c42b2ecf69b80cccaae3941a17a3a39fb20ddf2af7d96bf0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_object.h",
  "human_readable_source": " \n\n#ifndef __I915_GEM_OBJECT_H__\n#define __I915_GEM_OBJECT_H__\n\n#include <drm/drm_gem.h>\n#include <drm/drm_file.h>\n#include <drm/drm_device.h>\n\n#include \"display/intel_frontbuffer.h\"\n#include \"intel_memory_region.h\"\n#include \"i915_gem_object_types.h\"\n#include \"i915_gem_gtt.h\"\n#include \"i915_gem_ww.h\"\n#include \"i915_vma_types.h\"\n\nenum intel_region_id;\n\n#define obj_to_i915(obj__) to_i915((obj__)->base.dev)\n\nstatic inline bool i915_gem_object_size_2big(u64 size)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tif (overflows_type(size, obj->base.size))\n\t\treturn true;\n\n\treturn false;\n}\n\nunsigned int i915_gem_get_pat_index(struct drm_i915_private *i915,\n\t\t\t\t    enum i915_cache_level level);\nbool i915_gem_object_has_cache_level(const struct drm_i915_gem_object *obj,\n\t\t\t\t     enum i915_cache_level lvl);\nvoid i915_gem_init__objects(struct drm_i915_private *i915);\n\nvoid i915_objects_module_exit(void);\nint i915_objects_module_init(void);\n\nstruct drm_i915_gem_object *i915_gem_object_alloc(void);\nvoid i915_gem_object_free(struct drm_i915_gem_object *obj);\n\nvoid i915_gem_object_init(struct drm_i915_gem_object *obj,\n\t\t\t  const struct drm_i915_gem_object_ops *ops,\n\t\t\t  struct lock_class_key *key,\n\t\t\t  unsigned alloc_flags);\n\nvoid __i915_gem_object_fini(struct drm_i915_gem_object *obj);\n\nstruct drm_i915_gem_object *\ni915_gem_object_create_shmem(struct drm_i915_private *i915,\n\t\t\t     resource_size_t size);\nstruct drm_i915_gem_object *\ni915_gem_object_create_shmem_from_data(struct drm_i915_private *i915,\n\t\t\t\t       const void *data, resource_size_t size);\nstruct drm_i915_gem_object *\n__i915_gem_object_create_user(struct drm_i915_private *i915, u64 size,\n\t\t\t      struct intel_memory_region **placements,\n\t\t\t      unsigned int n_placements);\n\nextern const struct drm_i915_gem_object_ops i915_gem_shmem_ops;\n\nvoid __i915_gem_object_release_shmem(struct drm_i915_gem_object *obj,\n\t\t\t\t     struct sg_table *pages,\n\t\t\t\t     bool needs_clflush);\n\nint i915_gem_object_pwrite_phys(struct drm_i915_gem_object *obj,\n\t\t\t\tconst struct drm_i915_gem_pwrite *args);\nint i915_gem_object_pread_phys(struct drm_i915_gem_object *obj,\n\t\t\t       const struct drm_i915_gem_pread *args);\n\nint i915_gem_object_attach_phys(struct drm_i915_gem_object *obj, int align);\nvoid i915_gem_object_put_pages_shmem(struct drm_i915_gem_object *obj,\n\t\t\t\t     struct sg_table *pages);\nvoid i915_gem_object_put_pages_phys(struct drm_i915_gem_object *obj,\n\t\t\t\t    struct sg_table *pages);\n\nvoid i915_gem_flush_free_objects(struct drm_i915_private *i915);\n\nstruct sg_table *\n__i915_gem_object_unset_pages(struct drm_i915_gem_object *obj);\n\n \nstatic inline struct drm_i915_gem_object *\ni915_gem_object_lookup_rcu(struct drm_file *file, u32 handle)\n{\n#ifdef CONFIG_LOCKDEP\n\tWARN_ON(debug_locks && !lock_is_held(&rcu_lock_map));\n#endif\n\treturn idr_find(&file->object_idr, handle);\n}\n\nstatic inline struct drm_i915_gem_object *\ni915_gem_object_get_rcu(struct drm_i915_gem_object *obj)\n{\n\tif (obj && !kref_get_unless_zero(&obj->base.refcount))\n\t\tobj = NULL;\n\n\treturn obj;\n}\n\nstatic inline struct drm_i915_gem_object *\ni915_gem_object_lookup(struct drm_file *file, u32 handle)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\trcu_read_lock();\n\tobj = i915_gem_object_lookup_rcu(file, handle);\n\tobj = i915_gem_object_get_rcu(obj);\n\trcu_read_unlock();\n\n\treturn obj;\n}\n\n__deprecated\nstruct drm_gem_object *\ndrm_gem_object_lookup(struct drm_file *file, u32 handle);\n\n__attribute__((nonnull))\nstatic inline struct drm_i915_gem_object *\ni915_gem_object_get(struct drm_i915_gem_object *obj)\n{\n\tdrm_gem_object_get(&obj->base);\n\treturn obj;\n}\n\n__attribute__((nonnull))\nstatic inline void\ni915_gem_object_put(struct drm_i915_gem_object *obj)\n{\n\t__drm_gem_object_put(&obj->base);\n}\n\n#define assert_object_held(obj) dma_resv_assert_held((obj)->base.resv)\n\n \nstatic inline void assert_object_held_shared(const struct drm_i915_gem_object *obj)\n{\n\t \n\tif (IS_ENABLED(CONFIG_LOCKDEP) &&\n\t    kref_read(&obj->base.refcount) > 0)\n\t\tassert_object_held(obj);\n}\n\nstatic inline int __i915_gem_object_lock(struct drm_i915_gem_object *obj,\n\t\t\t\t\t struct i915_gem_ww_ctx *ww,\n\t\t\t\t\t bool intr)\n{\n\tint ret;\n\n\tif (intr)\n\t\tret = dma_resv_lock_interruptible(obj->base.resv, ww ? &ww->ctx : NULL);\n\telse\n\t\tret = dma_resv_lock(obj->base.resv, ww ? &ww->ctx : NULL);\n\n\tif (!ret && ww) {\n\t\ti915_gem_object_get(obj);\n\t\tlist_add_tail(&obj->obj_link, &ww->obj_list);\n\t}\n\tif (ret == -EALREADY)\n\t\tret = 0;\n\n\tif (ret == -EDEADLK) {\n\t\ti915_gem_object_get(obj);\n\t\tww->contended = obj;\n\t}\n\n\treturn ret;\n}\n\nstatic inline int i915_gem_object_lock(struct drm_i915_gem_object *obj,\n\t\t\t\t       struct i915_gem_ww_ctx *ww)\n{\n\treturn __i915_gem_object_lock(obj, ww, ww && ww->intr);\n}\n\nstatic inline int i915_gem_object_lock_interruptible(struct drm_i915_gem_object *obj,\n\t\t\t\t\t\t     struct i915_gem_ww_ctx *ww)\n{\n\tWARN_ON(ww && !ww->intr);\n\treturn __i915_gem_object_lock(obj, ww, true);\n}\n\nstatic inline bool i915_gem_object_trylock(struct drm_i915_gem_object *obj,\n\t\t\t\t\t   struct i915_gem_ww_ctx *ww)\n{\n\tif (!ww)\n\t\treturn dma_resv_trylock(obj->base.resv);\n\telse\n\t\treturn ww_mutex_trylock(&obj->base.resv->lock, &ww->ctx);\n}\n\nstatic inline void i915_gem_object_unlock(struct drm_i915_gem_object *obj)\n{\n\tif (obj->ops->adjust_lru)\n\t\tobj->ops->adjust_lru(obj);\n\n\tdma_resv_unlock(obj->base.resv);\n}\n\nstatic inline void\ni915_gem_object_set_readonly(struct drm_i915_gem_object *obj)\n{\n\tobj->flags |= I915_BO_READONLY;\n}\n\nstatic inline bool\ni915_gem_object_is_readonly(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->flags & I915_BO_READONLY;\n}\n\nstatic inline bool\ni915_gem_object_is_contiguous(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->flags & I915_BO_ALLOC_CONTIGUOUS;\n}\n\nstatic inline bool\ni915_gem_object_is_volatile(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->flags & I915_BO_ALLOC_VOLATILE;\n}\n\nstatic inline void\ni915_gem_object_set_volatile(struct drm_i915_gem_object *obj)\n{\n\tobj->flags |= I915_BO_ALLOC_VOLATILE;\n}\n\nstatic inline bool\ni915_gem_object_has_tiling_quirk(struct drm_i915_gem_object *obj)\n{\n\treturn test_bit(I915_TILING_QUIRK_BIT, &obj->flags);\n}\n\nstatic inline void\ni915_gem_object_set_tiling_quirk(struct drm_i915_gem_object *obj)\n{\n\tset_bit(I915_TILING_QUIRK_BIT, &obj->flags);\n}\n\nstatic inline void\ni915_gem_object_clear_tiling_quirk(struct drm_i915_gem_object *obj)\n{\n\tclear_bit(I915_TILING_QUIRK_BIT, &obj->flags);\n}\n\nstatic inline bool\ni915_gem_object_is_protected(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->flags & I915_BO_PROTECTED;\n}\n\nstatic inline bool\ni915_gem_object_type_has(const struct drm_i915_gem_object *obj,\n\t\t\t unsigned long flags)\n{\n\treturn obj->ops->flags & flags;\n}\n\nbool i915_gem_object_has_struct_page(const struct drm_i915_gem_object *obj);\n\nbool i915_gem_object_has_iomem(const struct drm_i915_gem_object *obj);\n\nstatic inline bool\ni915_gem_object_is_shrinkable(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_object_type_has(obj, I915_GEM_OBJECT_IS_SHRINKABLE);\n}\n\nstatic inline bool\ni915_gem_object_has_self_managed_shrink_list(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_object_type_has(obj, I915_GEM_OBJECT_SELF_MANAGED_SHRINK_LIST);\n}\n\nstatic inline bool\ni915_gem_object_is_proxy(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_object_type_has(obj, I915_GEM_OBJECT_IS_PROXY);\n}\n\nstatic inline bool\ni915_gem_object_never_mmap(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_object_type_has(obj, I915_GEM_OBJECT_NO_MMAP);\n}\n\nstatic inline bool\ni915_gem_object_is_framebuffer(const struct drm_i915_gem_object *obj)\n{\n\treturn READ_ONCE(obj->frontbuffer) || obj->is_dpt;\n}\n\nstatic inline unsigned int\ni915_gem_object_get_tiling(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->tiling_and_stride & TILING_MASK;\n}\n\nstatic inline bool\ni915_gem_object_is_tiled(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_object_get_tiling(obj) != I915_TILING_NONE;\n}\n\nstatic inline unsigned int\ni915_gem_object_get_stride(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->tiling_and_stride & STRIDE_MASK;\n}\n\nstatic inline unsigned int\ni915_gem_tile_height(unsigned int tiling)\n{\n\tGEM_BUG_ON(!tiling);\n\treturn tiling == I915_TILING_Y ? 32 : 8;\n}\n\nstatic inline unsigned int\ni915_gem_object_get_tile_height(const struct drm_i915_gem_object *obj)\n{\n\treturn i915_gem_tile_height(i915_gem_object_get_tiling(obj));\n}\n\nstatic inline unsigned int\ni915_gem_object_get_tile_row_size(const struct drm_i915_gem_object *obj)\n{\n\treturn (i915_gem_object_get_stride(obj) *\n\t\ti915_gem_object_get_tile_height(obj));\n}\n\nint i915_gem_object_set_tiling(struct drm_i915_gem_object *obj,\n\t\t\t       unsigned int tiling, unsigned int stride);\n\n \nstruct scatterlist *\n__i915_gem_object_page_iter_get_sg(struct drm_i915_gem_object *obj,\n\t\t\t\t   struct i915_gem_object_page_iter *iter,\n\t\t\t\t   pgoff_t  n,\n\t\t\t\t   unsigned int *offset);\n\n \n#define i915_gem_object_page_iter_get_sg(obj, it, n, offset) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\t\\\n\t__i915_gem_object_page_iter_get_sg(obj, it, n, offset);\t\\\n})\n\n \nstatic inline struct scatterlist *\n__i915_gem_object_get_sg(struct drm_i915_gem_object *obj, pgoff_t n,\n\t\t\t unsigned int *offset)\n{\n\treturn __i915_gem_object_page_iter_get_sg(obj, &obj->mm.get_page, n, offset);\n}\n\n \n#define i915_gem_object_get_sg(obj, n, offset) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\\\n\t__i915_gem_object_get_sg(obj, n, offset);\t\\\n})\n\n \nstatic inline struct scatterlist *\n__i915_gem_object_get_sg_dma(struct drm_i915_gem_object *obj, pgoff_t n,\n\t\t\t     unsigned int *offset)\n{\n\treturn __i915_gem_object_page_iter_get_sg(obj, &obj->mm.get_dma_page, n, offset);\n}\n\n \n#define i915_gem_object_get_sg_dma(obj, n, offset) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\\\n\t__i915_gem_object_get_sg_dma(obj, n, offset);\t\\\n})\n\n \nstruct page *\n__i915_gem_object_get_page(struct drm_i915_gem_object *obj, pgoff_t n);\n\n \n#define i915_gem_object_get_page(obj, n) ({\t\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\\\n\t__i915_gem_object_get_page(obj, n);\t\t\\\n})\n\n \nstruct page *\n__i915_gem_object_get_dirty_page(struct drm_i915_gem_object *obj, pgoff_t n);\n\n \n#define i915_gem_object_get_dirty_page(obj, n) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\\\n\t__i915_gem_object_get_dirty_page(obj, n);\t\\\n})\n\n \ndma_addr_t\n__i915_gem_object_get_dma_address_len(struct drm_i915_gem_object *obj, pgoff_t n,\n\t\t\t\t      unsigned int *len);\n\n \n#define i915_gem_object_get_dma_address_len(obj, n, len) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\t\\\n\t__i915_gem_object_get_dma_address_len(obj, n, len);\t\\\n})\n\n \ndma_addr_t\n__i915_gem_object_get_dma_address(struct drm_i915_gem_object *obj, pgoff_t n);\n\n \n#define i915_gem_object_get_dma_address(obj, n) ({\t\\\n\tstatic_assert(castable_to_type(n, pgoff_t));\t\\\n\t__i915_gem_object_get_dma_address(obj, n);\t\\\n})\n\nvoid __i915_gem_object_set_pages(struct drm_i915_gem_object *obj,\n\t\t\t\t struct sg_table *pages);\n\nint ____i915_gem_object_get_pages(struct drm_i915_gem_object *obj);\nint __i915_gem_object_get_pages(struct drm_i915_gem_object *obj);\n\nstatic inline int __must_check\ni915_gem_object_pin_pages(struct drm_i915_gem_object *obj)\n{\n\tassert_object_held(obj);\n\n\tif (atomic_inc_not_zero(&obj->mm.pages_pin_count))\n\t\treturn 0;\n\n\treturn __i915_gem_object_get_pages(obj);\n}\n\nint i915_gem_object_pin_pages_unlocked(struct drm_i915_gem_object *obj);\n\nstatic inline bool\ni915_gem_object_has_pages(struct drm_i915_gem_object *obj)\n{\n\treturn !IS_ERR_OR_NULL(READ_ONCE(obj->mm.pages));\n}\n\nstatic inline void\n__i915_gem_object_pin_pages(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(obj));\n\n\tatomic_inc(&obj->mm.pages_pin_count);\n}\n\nstatic inline bool\ni915_gem_object_has_pinned_pages(struct drm_i915_gem_object *obj)\n{\n\treturn atomic_read(&obj->mm.pages_pin_count);\n}\n\nstatic inline void\n__i915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(obj));\n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));\n\n\tatomic_dec(&obj->mm.pages_pin_count);\n}\n\nstatic inline void\ni915_gem_object_unpin_pages(struct drm_i915_gem_object *obj)\n{\n\t__i915_gem_object_unpin_pages(obj);\n}\n\nint __i915_gem_object_put_pages(struct drm_i915_gem_object *obj);\nint i915_gem_object_truncate(struct drm_i915_gem_object *obj);\n\n \nvoid *__must_check i915_gem_object_pin_map(struct drm_i915_gem_object *obj,\n\t\t\t\t\t   enum i915_map_type type);\n\nvoid *__must_check i915_gem_object_pin_map_unlocked(struct drm_i915_gem_object *obj,\n\t\t\t\t\t\t    enum i915_map_type type);\n\nvoid __i915_gem_object_flush_map(struct drm_i915_gem_object *obj,\n\t\t\t\t unsigned long offset,\n\t\t\t\t unsigned long size);\nstatic inline void i915_gem_object_flush_map(struct drm_i915_gem_object *obj)\n{\n\t__i915_gem_object_flush_map(obj, 0, obj->base.size);\n}\n\n \nstatic inline void i915_gem_object_unpin_map(struct drm_i915_gem_object *obj)\n{\n\ti915_gem_object_unpin_pages(obj);\n}\n\nvoid __i915_gem_object_release_map(struct drm_i915_gem_object *obj);\n\nint i915_gem_object_prepare_read(struct drm_i915_gem_object *obj,\n\t\t\t\t unsigned int *needs_clflush);\nint i915_gem_object_prepare_write(struct drm_i915_gem_object *obj,\n\t\t\t\t  unsigned int *needs_clflush);\n#define CLFLUSH_BEFORE\tBIT(0)\n#define CLFLUSH_AFTER\tBIT(1)\n#define CLFLUSH_FLAGS\t(CLFLUSH_BEFORE | CLFLUSH_AFTER)\n\nstatic inline void\ni915_gem_object_finish_access(struct drm_i915_gem_object *obj)\n{\n\ti915_gem_object_unpin_pages(obj);\n}\n\nint i915_gem_object_get_moving_fence(struct drm_i915_gem_object *obj,\n\t\t\t\t     struct dma_fence **fence);\nint i915_gem_object_wait_moving_fence(struct drm_i915_gem_object *obj,\n\t\t\t\t      bool intr);\nbool i915_gem_object_has_unknown_state(struct drm_i915_gem_object *obj);\n\nvoid i915_gem_object_set_cache_coherency(struct drm_i915_gem_object *obj,\n\t\t\t\t\t unsigned int cache_level);\nvoid i915_gem_object_set_pat_index(struct drm_i915_gem_object *obj,\n\t\t\t\t   unsigned int pat_index);\nbool i915_gem_object_can_bypass_llc(struct drm_i915_gem_object *obj);\nvoid i915_gem_object_flush_if_display(struct drm_i915_gem_object *obj);\nvoid i915_gem_object_flush_if_display_locked(struct drm_i915_gem_object *obj);\nbool i915_gem_cpu_write_needs_clflush(struct drm_i915_gem_object *obj);\n\nint __must_check\ni915_gem_object_set_to_wc_domain(struct drm_i915_gem_object *obj, bool write);\nint __must_check\ni915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write);\nint __must_check\ni915_gem_object_set_to_cpu_domain(struct drm_i915_gem_object *obj, bool write);\nstruct i915_vma * __must_check\ni915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,\n\t\t\t\t     struct i915_gem_ww_ctx *ww,\n\t\t\t\t     u32 alignment,\n\t\t\t\t     const struct i915_gtt_view *view,\n\t\t\t\t     unsigned int flags);\n\nvoid i915_gem_object_make_unshrinkable(struct drm_i915_gem_object *obj);\nvoid i915_gem_object_make_shrinkable(struct drm_i915_gem_object *obj);\nvoid __i915_gem_object_make_shrinkable(struct drm_i915_gem_object *obj);\nvoid __i915_gem_object_make_purgeable(struct drm_i915_gem_object *obj);\nvoid i915_gem_object_make_purgeable(struct drm_i915_gem_object *obj);\n\nstatic inline void __start_cpu_write(struct drm_i915_gem_object *obj)\n{\n\tobj->read_domains = I915_GEM_DOMAIN_CPU;\n\tobj->write_domain = I915_GEM_DOMAIN_CPU;\n\tif (i915_gem_cpu_write_needs_clflush(obj))\n\t\tobj->cache_dirty = true;\n}\n\nvoid i915_gem_fence_wait_priority(struct dma_fence *fence,\n\t\t\t\t  const struct i915_sched_attr *attr);\n\nint i915_gem_object_wait(struct drm_i915_gem_object *obj,\n\t\t\t unsigned int flags,\n\t\t\t long timeout);\nint i915_gem_object_wait_priority(struct drm_i915_gem_object *obj,\n\t\t\t\t  unsigned int flags,\n\t\t\t\t  const struct i915_sched_attr *attr);\n\nvoid __i915_gem_object_flush_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t\t enum fb_op_origin origin);\nvoid __i915_gem_object_invalidate_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t\t      enum fb_op_origin origin);\n\nstatic inline void\ni915_gem_object_flush_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t  enum fb_op_origin origin)\n{\n\tif (unlikely(rcu_access_pointer(obj->frontbuffer)))\n\t\t__i915_gem_object_flush_frontbuffer(obj, origin);\n}\n\nstatic inline void\ni915_gem_object_invalidate_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\t       enum fb_op_origin origin)\n{\n\tif (unlikely(rcu_access_pointer(obj->frontbuffer)))\n\t\t__i915_gem_object_invalidate_frontbuffer(obj, origin);\n}\n\nint i915_gem_object_read_from_page(struct drm_i915_gem_object *obj, u64 offset, void *dst, int size);\n\nbool i915_gem_object_is_shmem(const struct drm_i915_gem_object *obj);\n\nvoid __i915_gem_free_object_rcu(struct rcu_head *head);\n\nvoid __i915_gem_object_pages_fini(struct drm_i915_gem_object *obj);\n\nvoid __i915_gem_free_object(struct drm_i915_gem_object *obj);\n\nbool i915_gem_object_evictable(struct drm_i915_gem_object *obj);\n\nbool i915_gem_object_migratable(struct drm_i915_gem_object *obj);\n\nint i915_gem_object_migrate(struct drm_i915_gem_object *obj,\n\t\t\t    struct i915_gem_ww_ctx *ww,\n\t\t\t    enum intel_region_id id);\nint __i915_gem_object_migrate(struct drm_i915_gem_object *obj,\n\t\t\t      struct i915_gem_ww_ctx *ww,\n\t\t\t      enum intel_region_id id,\n\t\t\t      unsigned int flags);\n\nbool i915_gem_object_can_migrate(struct drm_i915_gem_object *obj,\n\t\t\t\t enum intel_region_id id);\n\nint i915_gem_object_wait_migration(struct drm_i915_gem_object *obj,\n\t\t\t\t   unsigned int flags);\n\nbool i915_gem_object_placement_possible(struct drm_i915_gem_object *obj,\n\t\t\t\t\tenum intel_memory_type type);\n\nbool i915_gem_object_needs_ccs_pages(struct drm_i915_gem_object *obj);\n\nint shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,\n\t\t\t size_t size, struct intel_memory_region *mr,\n\t\t\t struct address_space *mapping,\n\t\t\t unsigned int max_segment);\nvoid shmem_sg_free_table(struct sg_table *st, struct address_space *mapping,\n\t\t\t bool dirty, bool backup);\nvoid __shmem_writeback(size_t size, struct address_space *mapping);\n\n#ifdef CONFIG_MMU_NOTIFIER\nstatic inline bool\ni915_gem_object_is_userptr(struct drm_i915_gem_object *obj)\n{\n\treturn obj->userptr.notifier.mm;\n}\n\nint i915_gem_object_userptr_submit_init(struct drm_i915_gem_object *obj);\nint i915_gem_object_userptr_submit_done(struct drm_i915_gem_object *obj);\nint i915_gem_object_userptr_validate(struct drm_i915_gem_object *obj);\n#else\nstatic inline bool i915_gem_object_is_userptr(struct drm_i915_gem_object *obj) { return false; }\n\nstatic inline int i915_gem_object_userptr_submit_init(struct drm_i915_gem_object *obj) { GEM_BUG_ON(1); return -ENODEV; }\nstatic inline int i915_gem_object_userptr_submit_done(struct drm_i915_gem_object *obj) { GEM_BUG_ON(1); return -ENODEV; }\nstatic inline int i915_gem_object_userptr_validate(struct drm_i915_gem_object *obj) { GEM_BUG_ON(1); return -ENODEV; }\n\n#endif\n\n \nstatic inline struct intel_frontbuffer *\ni915_gem_object_get_frontbuffer(const struct drm_i915_gem_object *obj)\n{\n\tstruct intel_frontbuffer *front;\n\n\tif (likely(!rcu_access_pointer(obj->frontbuffer)))\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tdo {\n\t\tfront = rcu_dereference(obj->frontbuffer);\n\t\tif (!front)\n\t\t\tbreak;\n\n\t\tif (unlikely(!kref_get_unless_zero(&front->ref)))\n\t\t\tcontinue;\n\n\t\tif (likely(front == rcu_access_pointer(obj->frontbuffer)))\n\t\t\tbreak;\n\n\t\tintel_frontbuffer_put(front);\n\t} while (1);\n\trcu_read_unlock();\n\n\treturn front;\n}\n\n \nstatic inline struct intel_frontbuffer *\ni915_gem_object_set_frontbuffer(struct drm_i915_gem_object *obj,\n\t\t\t\tstruct intel_frontbuffer *front)\n{\n\tstruct intel_frontbuffer *cur = front;\n\n\tif (!front) {\n\t\tRCU_INIT_POINTER(obj->frontbuffer, NULL);\n\t} else if (rcu_access_pointer(obj->frontbuffer)) {\n\t\tcur = rcu_dereference_protected(obj->frontbuffer, true);\n\t\tkref_get(&cur->ref);\n\t} else {\n\t\tdrm_gem_object_get(intel_bo_to_drm_bo(obj));\n\t\trcu_assign_pointer(obj->frontbuffer, front);\n\t}\n\n\treturn cur;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}