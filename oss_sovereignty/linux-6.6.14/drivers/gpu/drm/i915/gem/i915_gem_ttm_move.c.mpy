{
  "module_name": "i915_gem_ttm_move.c",
  "hash_id": "1a3dfc8abe1bafd06e2998477f7c2738242d671a8b3ce6aee333ed9fe6ca0fab",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_ttm_move.c",
  "human_readable_source": "\n \n\n#include <drm/ttm/ttm_tt.h>\n\n#include \"i915_deps.h\"\n#include \"i915_drv.h\"\n#include \"intel_memory_region.h\"\n#include \"intel_region_ttm.h\"\n\n#include \"gem/i915_gem_object.h\"\n#include \"gem/i915_gem_region.h\"\n#include \"gem/i915_gem_ttm.h\"\n#include \"gem/i915_gem_ttm_move.h\"\n\n#include \"gt/intel_engine_pm.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_migrate.h\"\n\n \n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\nstatic bool fail_gpu_migration;\nstatic bool fail_work_allocation;\nstatic bool ban_memcpy;\n\nvoid i915_ttm_migrate_set_failure_modes(bool gpu_migration,\n\t\t\t\t\tbool work_allocation)\n{\n\tfail_gpu_migration = gpu_migration;\n\tfail_work_allocation = work_allocation;\n}\n\nvoid i915_ttm_migrate_set_ban_memcpy(bool ban)\n{\n\tban_memcpy = ban;\n}\n#endif\n\nstatic enum i915_cache_level\ni915_ttm_cache_level(struct drm_i915_private *i915, struct ttm_resource *res,\n\t\t     struct ttm_tt *ttm)\n{\n\treturn ((HAS_LLC(i915) || HAS_SNOOP(i915)) &&\n\t\t!i915_ttm_gtt_binds_lmem(res) &&\n\t\tttm->caching == ttm_cached) ? I915_CACHE_LLC :\n\t\tI915_CACHE_NONE;\n}\n\nstatic struct intel_memory_region *\ni915_ttm_region(struct ttm_device *bdev, int ttm_mem_type)\n{\n\tstruct drm_i915_private *i915 = container_of(bdev, typeof(*i915), bdev);\n\n\t \n\tGEM_BUG_ON(ttm_mem_type != I915_PL_SYSTEM &&\n\t\t   ttm_mem_type < I915_PL_LMEM0);\n\tif (ttm_mem_type == I915_PL_SYSTEM)\n\t\treturn intel_memory_region_lookup(i915, INTEL_MEMORY_SYSTEM,\n\t\t\t\t\t\t  0);\n\n\treturn intel_memory_region_lookup(i915, INTEL_MEMORY_LOCAL,\n\t\t\t\t\t  ttm_mem_type - I915_PL_LMEM0);\n}\n\n \nvoid i915_ttm_adjust_domains_after_move(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\n\tif (i915_ttm_cpu_maps_iomem(bo->resource) || bo->ttm->caching != ttm_cached) {\n\t\tobj->write_domain = I915_GEM_DOMAIN_WC;\n\t\tobj->read_domains = I915_GEM_DOMAIN_WC;\n\t} else {\n\t\tobj->write_domain = I915_GEM_DOMAIN_CPU;\n\t\tobj->read_domains = I915_GEM_DOMAIN_CPU;\n\t}\n}\n\n \nvoid i915_ttm_adjust_gem_after_move(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tunsigned int cache_level;\n\tunsigned int mem_flags;\n\tunsigned int i;\n\tint mem_type;\n\n\t \n\tif (!bo->resource) {\n\t\tmem_flags = I915_BO_FLAG_STRUCT_PAGE;\n\t\tmem_type = I915_PL_SYSTEM;\n\t\tcache_level = I915_CACHE_NONE;\n\t} else {\n\t\tmem_flags = i915_ttm_cpu_maps_iomem(bo->resource) ? I915_BO_FLAG_IOMEM :\n\t\t\tI915_BO_FLAG_STRUCT_PAGE;\n\t\tmem_type = bo->resource->mem_type;\n\t\tcache_level = i915_ttm_cache_level(to_i915(bo->base.dev), bo->resource,\n\t\t\t\t\t\t   bo->ttm);\n\t}\n\n\t \n\tif (intel_region_to_ttm_type(obj->mm.region) != mem_type) {\n\t\tfor (i = 0; i < obj->mm.n_placements; ++i) {\n\t\t\tstruct intel_memory_region *mr = obj->mm.placements[i];\n\n\t\t\tif (intel_region_to_ttm_type(mr) == mem_type &&\n\t\t\t    mr != obj->mm.region) {\n\t\t\t\ti915_gem_object_release_memory_region(obj);\n\t\t\t\ti915_gem_object_init_memory_region(obj, mr);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tobj->mem_flags &= ~(I915_BO_FLAG_STRUCT_PAGE | I915_BO_FLAG_IOMEM);\n\tobj->mem_flags |= mem_flags;\n\n\ti915_gem_object_set_cache_coherency(obj, cache_level);\n}\n\n \nint i915_ttm_move_notify(struct ttm_buffer_object *bo)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tint ret;\n\n\t \n\tret = i915_gem_object_unbind(obj, I915_GEM_OBJECT_UNBIND_ACTIVE |\n\t\t\t\t     I915_GEM_OBJECT_UNBIND_ASYNC);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __i915_gem_object_put_pages(obj);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic struct dma_fence *i915_ttm_accel_move(struct ttm_buffer_object *bo,\n\t\t\t\t\t     bool clear,\n\t\t\t\t\t     struct ttm_resource *dst_mem,\n\t\t\t\t\t     struct ttm_tt *dst_ttm,\n\t\t\t\t\t     struct sg_table *dst_st,\n\t\t\t\t\t     const struct i915_deps *deps)\n{\n\tstruct drm_i915_private *i915 = container_of(bo->bdev, typeof(*i915),\n\t\t\t\t\t\t     bdev);\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tstruct i915_request *rq;\n\tstruct ttm_tt *src_ttm = bo->ttm;\n\tenum i915_cache_level src_level, dst_level;\n\tint ret;\n\n\tif (!to_gt(i915)->migrate.context || intel_gt_is_wedged(to_gt(i915)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (I915_SELFTEST_ONLY(fail_gpu_migration))\n\t\tclear = true;\n\n\tdst_level = i915_ttm_cache_level(i915, dst_mem, dst_ttm);\n\tif (clear) {\n\t\tif (bo->type == ttm_bo_type_kernel &&\n\t\t    !I915_SELFTEST_ONLY(fail_gpu_migration))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tintel_engine_pm_get(to_gt(i915)->migrate.context->engine);\n\t\tret = intel_context_migrate_clear(to_gt(i915)->migrate.context, deps,\n\t\t\t\t\t\t  dst_st->sgl,\n\t\t\t\t\t\t  i915_gem_get_pat_index(i915, dst_level),\n\t\t\t\t\t\t  i915_ttm_gtt_binds_lmem(dst_mem),\n\t\t\t\t\t\t  0, &rq);\n\t} else {\n\t\tstruct i915_refct_sgt *src_rsgt =\n\t\t\ti915_ttm_resource_get_st(obj, bo->resource);\n\n\t\tif (IS_ERR(src_rsgt))\n\t\t\treturn ERR_CAST(src_rsgt);\n\n\t\tsrc_level = i915_ttm_cache_level(i915, bo->resource, src_ttm);\n\t\tintel_engine_pm_get(to_gt(i915)->migrate.context->engine);\n\t\tret = intel_context_migrate_copy(to_gt(i915)->migrate.context,\n\t\t\t\t\t\t deps, src_rsgt->table.sgl,\n\t\t\t\t\t\t i915_gem_get_pat_index(i915, src_level),\n\t\t\t\t\t\t i915_ttm_gtt_binds_lmem(bo->resource),\n\t\t\t\t\t\t dst_st->sgl,\n\t\t\t\t\t\t i915_gem_get_pat_index(i915, dst_level),\n\t\t\t\t\t\t i915_ttm_gtt_binds_lmem(dst_mem),\n\t\t\t\t\t\t &rq);\n\n\t\ti915_refct_sgt_put(src_rsgt);\n\t}\n\n\tintel_engine_pm_put(to_gt(i915)->migrate.context->engine);\n\n\tif (ret && rq) {\n\t\ti915_request_wait(rq, 0, MAX_SCHEDULE_TIMEOUT);\n\t\ti915_request_put(rq);\n\t}\n\n\treturn ret ? ERR_PTR(ret) : &rq->fence;\n}\n\n \nstruct i915_ttm_memcpy_arg {\n\tunion {\n\t\tstruct ttm_kmap_iter_tt tt;\n\t\tstruct ttm_kmap_iter_iomap io;\n\t} _dst_iter,\n\t_src_iter;\n\tstruct ttm_kmap_iter *dst_iter;\n\tstruct ttm_kmap_iter *src_iter;\n\tunsigned long num_pages;\n\tbool clear;\n\tstruct i915_refct_sgt *src_rsgt;\n\tstruct i915_refct_sgt *dst_rsgt;\n};\n\n \nstruct i915_ttm_memcpy_work {\n\tstruct dma_fence fence;\n\tstruct work_struct work;\n\tspinlock_t lock;\n\tstruct irq_work irq_work;\n\tstruct dma_fence_cb cb;\n\tstruct i915_ttm_memcpy_arg arg;\n\tstruct drm_i915_private *i915;\n\tstruct drm_i915_gem_object *obj;\n\tbool memcpy_allowed;\n};\n\nstatic void i915_ttm_move_memcpy(struct i915_ttm_memcpy_arg *arg)\n{\n\tttm_move_memcpy(arg->clear, arg->num_pages,\n\t\t\targ->dst_iter, arg->src_iter);\n}\n\nstatic void i915_ttm_memcpy_init(struct i915_ttm_memcpy_arg *arg,\n\t\t\t\t struct ttm_buffer_object *bo, bool clear,\n\t\t\t\t struct ttm_resource *dst_mem,\n\t\t\t\t struct ttm_tt *dst_ttm,\n\t\t\t\t struct i915_refct_sgt *dst_rsgt)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tstruct intel_memory_region *dst_reg, *src_reg;\n\n\tdst_reg = i915_ttm_region(bo->bdev, dst_mem->mem_type);\n\tsrc_reg = i915_ttm_region(bo->bdev, bo->resource->mem_type);\n\tGEM_BUG_ON(!dst_reg || !src_reg);\n\n\targ->dst_iter = !i915_ttm_cpu_maps_iomem(dst_mem) ?\n\t\tttm_kmap_iter_tt_init(&arg->_dst_iter.tt, dst_ttm) :\n\t\tttm_kmap_iter_iomap_init(&arg->_dst_iter.io, &dst_reg->iomap,\n\t\t\t\t\t &dst_rsgt->table, dst_reg->region.start);\n\n\targ->src_iter = !i915_ttm_cpu_maps_iomem(bo->resource) ?\n\t\tttm_kmap_iter_tt_init(&arg->_src_iter.tt, bo->ttm) :\n\t\tttm_kmap_iter_iomap_init(&arg->_src_iter.io, &src_reg->iomap,\n\t\t\t\t\t &obj->ttm.cached_io_rsgt->table,\n\t\t\t\t\t src_reg->region.start);\n\targ->clear = clear;\n\targ->num_pages = bo->base.size >> PAGE_SHIFT;\n\n\targ->dst_rsgt = i915_refct_sgt_get(dst_rsgt);\n\targ->src_rsgt = clear ? NULL :\n\t\ti915_ttm_resource_get_st(obj, bo->resource);\n}\n\nstatic void i915_ttm_memcpy_release(struct i915_ttm_memcpy_arg *arg)\n{\n\ti915_refct_sgt_put(arg->src_rsgt);\n\ti915_refct_sgt_put(arg->dst_rsgt);\n}\n\nstatic void __memcpy_work(struct work_struct *work)\n{\n\tstruct i915_ttm_memcpy_work *copy_work =\n\t\tcontainer_of(work, typeof(*copy_work), work);\n\tstruct i915_ttm_memcpy_arg *arg = &copy_work->arg;\n\tbool cookie;\n\n\t \n\tif (!copy_work->memcpy_allowed) {\n\t\tstruct intel_gt *gt;\n\t\tunsigned int id;\n\n\t\tfor_each_gt(gt, copy_work->i915, id)\n\t\t\tintel_gt_set_wedged(gt);\n\t}\n\n\tcookie = dma_fence_begin_signalling();\n\n\tif (copy_work->memcpy_allowed) {\n\t\ti915_ttm_move_memcpy(arg);\n\t} else {\n\t\t \n\t\tcopy_work->obj->mm.unknown_state = true;\n\t}\n\n\tdma_fence_end_signalling(cookie);\n\n\tdma_fence_signal(&copy_work->fence);\n\n\ti915_ttm_memcpy_release(arg);\n\ti915_gem_object_put(copy_work->obj);\n\tdma_fence_put(&copy_work->fence);\n}\n\nstatic void __memcpy_irq_work(struct irq_work *irq_work)\n{\n\tstruct i915_ttm_memcpy_work *copy_work =\n\t\tcontainer_of(irq_work, typeof(*copy_work), irq_work);\n\tstruct i915_ttm_memcpy_arg *arg = &copy_work->arg;\n\n\tdma_fence_signal(&copy_work->fence);\n\ti915_ttm_memcpy_release(arg);\n\ti915_gem_object_put(copy_work->obj);\n\tdma_fence_put(&copy_work->fence);\n}\n\nstatic void __memcpy_cb(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tstruct i915_ttm_memcpy_work *copy_work =\n\t\tcontainer_of(cb, typeof(*copy_work), cb);\n\n\tif (unlikely(fence->error || I915_SELFTEST_ONLY(fail_gpu_migration))) {\n\t\tINIT_WORK(&copy_work->work, __memcpy_work);\n\t\tqueue_work(system_unbound_wq, &copy_work->work);\n\t} else {\n\t\tinit_irq_work(&copy_work->irq_work, __memcpy_irq_work);\n\t\tirq_work_queue(&copy_work->irq_work);\n\t}\n}\n\nstatic const char *get_driver_name(struct dma_fence *fence)\n{\n\treturn \"i915_ttm_memcpy_work\";\n}\n\nstatic const char *get_timeline_name(struct dma_fence *fence)\n{\n\treturn \"unbound\";\n}\n\nstatic const struct dma_fence_ops dma_fence_memcpy_ops = {\n\t.get_driver_name = get_driver_name,\n\t.get_timeline_name = get_timeline_name,\n};\n\nstatic struct dma_fence *\ni915_ttm_memcpy_work_arm(struct i915_ttm_memcpy_work *work,\n\t\t\t struct dma_fence *dep)\n{\n\tint ret;\n\n\tspin_lock_init(&work->lock);\n\tdma_fence_init(&work->fence, &dma_fence_memcpy_ops, &work->lock, 0, 0);\n\tdma_fence_get(&work->fence);\n\tret = dma_fence_add_callback(dep, &work->cb, __memcpy_cb);\n\tif (ret) {\n\t\tif (ret != -ENOENT)\n\t\t\tdma_fence_wait(dep, false);\n\n\t\treturn ERR_PTR(I915_SELFTEST_ONLY(fail_gpu_migration) ? -EINVAL :\n\t\t\t       dep->error);\n\t}\n\n\treturn &work->fence;\n}\n\nstatic bool i915_ttm_memcpy_allowed(struct ttm_buffer_object *bo,\n\t\t\t\t    struct ttm_resource *dst_mem)\n{\n\tif (i915_gem_object_needs_ccs_pages(i915_ttm_to_gem(bo)))\n\t\treturn false;\n\n\tif (!(i915_ttm_resource_mappable(bo->resource) &&\n\t      i915_ttm_resource_mappable(dst_mem)))\n\t\treturn false;\n\n\treturn I915_SELFTEST_ONLY(ban_memcpy) ? false : true;\n}\n\nstatic struct dma_fence *\n__i915_ttm_move(struct ttm_buffer_object *bo,\n\t\tconst struct ttm_operation_ctx *ctx, bool clear,\n\t\tstruct ttm_resource *dst_mem, struct ttm_tt *dst_ttm,\n\t\tstruct i915_refct_sgt *dst_rsgt, bool allow_accel,\n\t\tconst struct i915_deps *move_deps)\n{\n\tconst bool memcpy_allowed = i915_ttm_memcpy_allowed(bo, dst_mem);\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tstruct drm_i915_private *i915 = to_i915(bo->base.dev);\n\tstruct i915_ttm_memcpy_work *copy_work = NULL;\n\tstruct i915_ttm_memcpy_arg _arg, *arg = &_arg;\n\tstruct dma_fence *fence = ERR_PTR(-EINVAL);\n\n\tif (allow_accel) {\n\t\tfence = i915_ttm_accel_move(bo, clear, dst_mem, dst_ttm,\n\t\t\t\t\t    &dst_rsgt->table, move_deps);\n\n\t\t \n\t\tif (!IS_ERR(fence) && !i915_ttm_gtt_binds_lmem(dst_mem) &&\n\t\t    !I915_SELFTEST_ONLY(fail_gpu_migration ||\n\t\t\t\t\tfail_work_allocation))\n\t\t\tgoto out;\n\t}\n\n\t \n\tif (!IS_ERR(fence)) {\n\t\tstruct dma_fence *dep = fence;\n\n\t\tif (!I915_SELFTEST_ONLY(fail_work_allocation))\n\t\t\tcopy_work = kzalloc(sizeof(*copy_work), GFP_KERNEL);\n\n\t\tif (copy_work) {\n\t\t\tcopy_work->i915 = i915;\n\t\t\tcopy_work->memcpy_allowed = memcpy_allowed;\n\t\t\tcopy_work->obj = i915_gem_object_get(obj);\n\t\t\targ = &copy_work->arg;\n\t\t\tif (memcpy_allowed)\n\t\t\t\ti915_ttm_memcpy_init(arg, bo, clear, dst_mem,\n\t\t\t\t\t\t     dst_ttm, dst_rsgt);\n\n\t\t\tfence = i915_ttm_memcpy_work_arm(copy_work, dep);\n\t\t} else {\n\t\t\tdma_fence_wait(dep, false);\n\t\t\tfence = ERR_PTR(I915_SELFTEST_ONLY(fail_gpu_migration) ?\n\t\t\t\t\t-EINVAL : fence->error);\n\t\t}\n\t\tdma_fence_put(dep);\n\n\t\tif (!IS_ERR(fence))\n\t\t\tgoto out;\n\t} else {\n\t\tint err = PTR_ERR(fence);\n\n\t\tif (err == -EINTR || err == -ERESTARTSYS || err == -EAGAIN)\n\t\t\treturn fence;\n\n\t\tif (move_deps) {\n\t\t\terr = i915_deps_sync(move_deps, ctx);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\t \n\n\tif (memcpy_allowed) {\n\t\tif (!copy_work)\n\t\t\ti915_ttm_memcpy_init(arg, bo, clear, dst_mem, dst_ttm,\n\t\t\t\t\t     dst_rsgt);\n\t\ti915_ttm_move_memcpy(arg);\n\t\ti915_ttm_memcpy_release(arg);\n\t}\n\tif (copy_work)\n\t\ti915_gem_object_put(copy_work->obj);\n\tkfree(copy_work);\n\n\treturn memcpy_allowed ? NULL : ERR_PTR(-EIO);\nout:\n\tif (!fence && copy_work) {\n\t\ti915_ttm_memcpy_release(arg);\n\t\ti915_gem_object_put(copy_work->obj);\n\t\tkfree(copy_work);\n\t}\n\n\treturn fence;\n}\n\n \nint i915_ttm_move(struct ttm_buffer_object *bo, bool evict,\n\t\t  struct ttm_operation_ctx *ctx,\n\t\t  struct ttm_resource *dst_mem,\n\t\t  struct ttm_place *hop)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tstruct ttm_resource_manager *dst_man =\n\t\tttm_manager_type(bo->bdev, dst_mem->mem_type);\n\tstruct dma_fence *migration_fence = NULL;\n\tstruct ttm_tt *ttm = bo->ttm;\n\tstruct i915_refct_sgt *dst_rsgt;\n\tbool clear, prealloc_bo;\n\tint ret;\n\n\tif (GEM_WARN_ON(i915_ttm_is_ghost_object(bo))) {\n\t\tttm_bo_move_null(bo, dst_mem);\n\t\treturn 0;\n\t}\n\n\tif (!bo->resource) {\n\t\tif (dst_mem->mem_type != TTM_PL_SYSTEM) {\n\t\t\thop->mem_type = TTM_PL_SYSTEM;\n\t\t\thop->flags = TTM_PL_FLAG_TEMPORARY;\n\t\t\treturn -EMULTIHOP;\n\t\t}\n\n\t\t \n\t\tttm_bo_move_null(bo, dst_mem);\n\t\treturn 0;\n\t}\n\n\tret = i915_ttm_move_notify(bo);\n\tif (ret)\n\t\treturn ret;\n\n\tif (obj->mm.madv != I915_MADV_WILLNEED) {\n\t\ti915_ttm_purge(obj);\n\t\tttm_resource_free(bo, &dst_mem);\n\t\treturn 0;\n\t}\n\n\t \n\tif (ttm && (dst_man->use_tt || (ttm->page_flags & TTM_TT_FLAG_SWAPPED))) {\n\t\tret = ttm_tt_populate(bo->bdev, ttm, ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tdst_rsgt = i915_ttm_resource_get_st(obj, dst_mem);\n\tif (IS_ERR(dst_rsgt))\n\t\treturn PTR_ERR(dst_rsgt);\n\n\tclear = !i915_ttm_cpu_maps_iomem(bo->resource) && (!ttm || !ttm_tt_is_populated(ttm));\n\tprealloc_bo = obj->flags & I915_BO_PREALLOC;\n\tif (!(clear && ttm && !((ttm->page_flags & TTM_TT_FLAG_ZERO_ALLOC) && !prealloc_bo))) {\n\t\tstruct i915_deps deps;\n\n\t\ti915_deps_init(&deps, GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN);\n\t\tret = i915_deps_add_resv(&deps, bo->base.resv, ctx);\n\t\tif (ret) {\n\t\t\ti915_refct_sgt_put(dst_rsgt);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmigration_fence = __i915_ttm_move(bo, ctx, clear, dst_mem, ttm,\n\t\t\t\t\t\t  dst_rsgt, true, &deps);\n\t\ti915_deps_fini(&deps);\n\t}\n\n\t \n\tif (IS_ERR(migration_fence)) {\n\t\ti915_refct_sgt_put(dst_rsgt);\n\t\treturn PTR_ERR(migration_fence);\n\t}\n\n\tif (migration_fence) {\n\t\tif (I915_SELFTEST_ONLY(evict && fail_gpu_migration))\n\t\t\tret = -EIO;  \n\t\telse\n\t\t\tret = ttm_bo_move_accel_cleanup(bo, migration_fence, evict,\n\t\t\t\t\t\t\ttrue, dst_mem);\n\t\tif (ret) {\n\t\t\tdma_fence_wait(migration_fence, false);\n\t\t\tttm_bo_move_sync_cleanup(bo, dst_mem);\n\t\t}\n\t\tdma_fence_put(migration_fence);\n\t} else {\n\t\tttm_bo_move_sync_cleanup(bo, dst_mem);\n\t}\n\n\ti915_ttm_adjust_domains_after_move(obj);\n\ti915_ttm_free_cached_io_rsgt(obj);\n\n\tif (i915_ttm_gtt_binds_lmem(dst_mem) || i915_ttm_cpu_maps_iomem(dst_mem)) {\n\t\tobj->ttm.cached_io_rsgt = dst_rsgt;\n\t\tobj->ttm.get_io_page.sg_pos = dst_rsgt->table.sgl;\n\t\tobj->ttm.get_io_page.sg_idx = 0;\n\t} else {\n\t\ti915_refct_sgt_put(dst_rsgt);\n\t}\n\n\ti915_ttm_adjust_lru(obj);\n\ti915_ttm_adjust_gem_after_move(obj);\n\treturn 0;\n}\n\n \nint i915_gem_obj_copy_ttm(struct drm_i915_gem_object *dst,\n\t\t\t  struct drm_i915_gem_object *src,\n\t\t\t  bool allow_accel, bool intr)\n{\n\tstruct ttm_buffer_object *dst_bo = i915_gem_to_ttm(dst);\n\tstruct ttm_buffer_object *src_bo = i915_gem_to_ttm(src);\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = intr,\n\t};\n\tstruct i915_refct_sgt *dst_rsgt;\n\tstruct dma_fence *copy_fence;\n\tstruct i915_deps deps;\n\tint ret;\n\n\tassert_object_held(dst);\n\tassert_object_held(src);\n\n\tif (GEM_WARN_ON(!src_bo->resource || !dst_bo->resource))\n\t\treturn -EINVAL;\n\n\ti915_deps_init(&deps, GFP_KERNEL | __GFP_NORETRY | __GFP_NOWARN);\n\n\tret = dma_resv_reserve_fences(src_bo->base.resv, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = dma_resv_reserve_fences(dst_bo->base.resv, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tret = i915_deps_add_resv(&deps, dst_bo->base.resv, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tret = i915_deps_add_resv(&deps, src_bo->base.resv, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tdst_rsgt = i915_ttm_resource_get_st(dst, dst_bo->resource);\n\tcopy_fence = __i915_ttm_move(src_bo, &ctx, false, dst_bo->resource,\n\t\t\t\t     dst_bo->ttm, dst_rsgt, allow_accel,\n\t\t\t\t     &deps);\n\n\ti915_deps_fini(&deps);\n\ti915_refct_sgt_put(dst_rsgt);\n\tif (IS_ERR_OR_NULL(copy_fence))\n\t\treturn PTR_ERR_OR_ZERO(copy_fence);\n\n\tdma_resv_add_fence(dst_bo->base.resv, copy_fence, DMA_RESV_USAGE_WRITE);\n\tdma_resv_add_fence(src_bo->base.resv, copy_fence, DMA_RESV_USAGE_READ);\n\tdma_fence_put(copy_fence);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}