{
  "module_name": "i915_gem_execbuffer.c",
  "hash_id": "315bd75c290b96d8b61d30e9d59d18e2d554909209d87d17ecb78572c28cb4f9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c",
  "human_readable_source": " \n\n#include <linux/dma-resv.h>\n#include <linux/highmem.h>\n#include <linux/sync_file.h>\n#include <linux/uaccess.h>\n\n#include <drm/drm_syncobj.h>\n\n#include \"display/intel_frontbuffer.h\"\n\n#include \"gem/i915_gem_ioctls.h\"\n#include \"gt/intel_context.h\"\n#include \"gt/intel_gpu_commands.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_buffer_pool.h\"\n#include \"gt/intel_gt_pm.h\"\n#include \"gt/intel_ring.h\"\n\n#include \"pxp/intel_pxp.h\"\n\n#include \"i915_cmd_parser.h\"\n#include \"i915_drv.h\"\n#include \"i915_file_private.h\"\n#include \"i915_gem_clflush.h\"\n#include \"i915_gem_context.h\"\n#include \"i915_gem_evict.h\"\n#include \"i915_gem_ioctls.h\"\n#include \"i915_reg.h\"\n#include \"i915_trace.h\"\n#include \"i915_user_extensions.h\"\n\nstruct eb_vma {\n\tstruct i915_vma *vma;\n\tunsigned int flags;\n\n\t \n\tstruct drm_i915_gem_exec_object2 *exec;\n\tstruct list_head bind_link;\n\tstruct list_head reloc_link;\n\n\tstruct hlist_node node;\n\tu32 handle;\n};\n\nenum {\n\tFORCE_CPU_RELOC = 1,\n\tFORCE_GTT_RELOC,\n\tFORCE_GPU_RELOC,\n#define DBG_FORCE_RELOC 0  \n};\n\n \n#define __EXEC_OBJECT_HAS_PIN\t\tBIT(29)\n#define __EXEC_OBJECT_HAS_FENCE\t\tBIT(28)\n#define __EXEC_OBJECT_USERPTR_INIT\tBIT(27)\n#define __EXEC_OBJECT_NEEDS_MAP\t\tBIT(26)\n#define __EXEC_OBJECT_NEEDS_BIAS\tBIT(25)\n#define __EXEC_OBJECT_INTERNAL_FLAGS\t(~0u << 25)  \n#define __EXEC_OBJECT_RESERVED (__EXEC_OBJECT_HAS_PIN | __EXEC_OBJECT_HAS_FENCE)\n\n#define __EXEC_HAS_RELOC\tBIT(31)\n#define __EXEC_ENGINE_PINNED\tBIT(30)\n#define __EXEC_USERPTR_USED\tBIT(29)\n#define __EXEC_INTERNAL_FLAGS\t(~0u << 29)\n#define UPDATE\t\t\tPIN_OFFSET_FIXED\n\n#define BATCH_OFFSET_BIAS (256*1024)\n\n#define __I915_EXEC_ILLEGAL_FLAGS \\\n\t(__I915_EXEC_UNKNOWN_FLAGS | \\\n\t I915_EXEC_CONSTANTS_MASK  | \\\n\t I915_EXEC_RESOURCE_STREAMER)\n\n \n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\n#undef EINVAL\n#define EINVAL ({ \\\n\tDRM_DEBUG_DRIVER(\"EINVAL at %s:%d\\n\", __func__, __LINE__); \\\n\t22; \\\n})\n#endif\n\n \n\nstruct eb_fence {\n\tstruct drm_syncobj *syncobj;  \n\tstruct dma_fence *dma_fence;\n\tu64 value;\n\tstruct dma_fence_chain *chain_fence;\n};\n\nstruct i915_execbuffer {\n\tstruct drm_i915_private *i915;  \n\tstruct drm_file *file;  \n\tstruct drm_i915_gem_execbuffer2 *args;  \n\tstruct drm_i915_gem_exec_object2 *exec;  \n\tstruct eb_vma *vma;\n\n\tstruct intel_gt *gt;  \n\tstruct intel_context *context;  \n\tstruct i915_gem_context *gem_context;  \n\n\t \n\tstruct i915_request *requests[MAX_ENGINE_INSTANCE + 1];\n\t \n\tstruct eb_vma *batches[MAX_ENGINE_INSTANCE + 1];\n\tstruct i915_vma *trampoline;  \n\n\t \n\tstruct dma_fence *composite_fence;\n\n\t \n\tunsigned int buffer_count;\n\n\t \n\tunsigned int num_batches;\n\n\t \n\tstruct list_head unbound;\n\n\t \n\tstruct list_head relocs;\n\n\tstruct i915_gem_ww_ctx ww;\n\n\t \n\tstruct reloc_cache {\n\t\tstruct drm_mm_node node;  \n\t\tunsigned long vaddr;  \n\t\tunsigned long page;  \n\t\tunsigned int graphics_ver;  \n\t\tbool use_64bit_reloc : 1;\n\t\tbool has_llc : 1;\n\t\tbool has_fence : 1;\n\t\tbool needs_unfenced : 1;\n\t} reloc_cache;\n\n\tu64 invalid_flags;  \n\n\t \n\tu64 batch_len[MAX_ENGINE_INSTANCE + 1];\n\tu32 batch_start_offset;  \n\tu32 batch_flags;  \n\tstruct intel_gt_buffer_pool_node *batch_pool;  \n\n\t \n\tint lut_size;\n\tstruct hlist_head *buckets;  \n\n\tstruct eb_fence *fences;\n\tunsigned long num_fences;\n#if IS_ENABLED(CONFIG_DRM_I915_CAPTURE_ERROR)\n\tstruct i915_capture_list *capture_lists[MAX_ENGINE_INSTANCE + 1];\n#endif\n};\n\nstatic int eb_parse(struct i915_execbuffer *eb);\nstatic int eb_pin_engine(struct i915_execbuffer *eb, bool throttle);\nstatic void eb_unpin_engine(struct i915_execbuffer *eb);\nstatic void eb_capture_release(struct i915_execbuffer *eb);\n\nstatic inline bool eb_use_cmdparser(const struct i915_execbuffer *eb)\n{\n\treturn intel_engine_requires_cmd_parser(eb->context->engine) ||\n\t\t(intel_engine_using_cmd_parser(eb->context->engine) &&\n\t\t eb->args->batch_len);\n}\n\nstatic int eb_create(struct i915_execbuffer *eb)\n{\n\tif (!(eb->args->flags & I915_EXEC_HANDLE_LUT)) {\n\t\tunsigned int size = 1 + ilog2(eb->buffer_count);\n\n\t\t \n\t\tdo {\n\t\t\tgfp_t flags;\n\n\t\t\t \n\t\t\tflags = GFP_KERNEL;\n\t\t\tif (size > 1)\n\t\t\t\tflags |= __GFP_NORETRY | __GFP_NOWARN;\n\n\t\t\teb->buckets = kzalloc(sizeof(struct hlist_head) << size,\n\t\t\t\t\t      flags);\n\t\t\tif (eb->buckets)\n\t\t\t\tbreak;\n\t\t} while (--size);\n\n\t\tif (unlikely(!size))\n\t\t\treturn -ENOMEM;\n\n\t\teb->lut_size = size;\n\t} else {\n\t\teb->lut_size = -eb->buffer_count;\n\t}\n\n\treturn 0;\n}\n\nstatic bool\neb_vma_misplaced(const struct drm_i915_gem_exec_object2 *entry,\n\t\t const struct i915_vma *vma,\n\t\t unsigned int flags)\n{\n\tconst u64 start = i915_vma_offset(vma);\n\tconst u64 size = i915_vma_size(vma);\n\n\tif (size < entry->pad_to_size)\n\t\treturn true;\n\n\tif (entry->alignment && !IS_ALIGNED(start, entry->alignment))\n\t\treturn true;\n\n\tif (flags & EXEC_OBJECT_PINNED &&\n\t    start != entry->offset)\n\t\treturn true;\n\n\tif (flags & __EXEC_OBJECT_NEEDS_BIAS &&\n\t    start < BATCH_OFFSET_BIAS)\n\t\treturn true;\n\n\tif (!(flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS) &&\n\t    (start + size + 4095) >> 32)\n\t\treturn true;\n\n\tif (flags & __EXEC_OBJECT_NEEDS_MAP &&\n\t    !i915_vma_is_map_and_fenceable(vma))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic u64 eb_pin_flags(const struct drm_i915_gem_exec_object2 *entry,\n\t\t\tunsigned int exec_flags)\n{\n\tu64 pin_flags = 0;\n\n\tif (exec_flags & EXEC_OBJECT_NEEDS_GTT)\n\t\tpin_flags |= PIN_GLOBAL;\n\n\t \n\tif (!(exec_flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS))\n\t\tpin_flags |= PIN_ZONE_4G;\n\n\tif (exec_flags & __EXEC_OBJECT_NEEDS_MAP)\n\t\tpin_flags |= PIN_MAPPABLE;\n\n\tif (exec_flags & EXEC_OBJECT_PINNED)\n\t\tpin_flags |= entry->offset | PIN_OFFSET_FIXED;\n\telse if (exec_flags & __EXEC_OBJECT_NEEDS_BIAS)\n\t\tpin_flags |= BATCH_OFFSET_BIAS | PIN_OFFSET_BIAS;\n\n\treturn pin_flags;\n}\n\nstatic inline int\neb_pin_vma(struct i915_execbuffer *eb,\n\t   const struct drm_i915_gem_exec_object2 *entry,\n\t   struct eb_vma *ev)\n{\n\tstruct i915_vma *vma = ev->vma;\n\tu64 pin_flags;\n\tint err;\n\n\tif (vma->node.size)\n\t\tpin_flags =  __i915_vma_offset(vma);\n\telse\n\t\tpin_flags = entry->offset & PIN_OFFSET_MASK;\n\n\tpin_flags |= PIN_USER | PIN_NOEVICT | PIN_OFFSET_FIXED | PIN_VALIDATE;\n\tif (unlikely(ev->flags & EXEC_OBJECT_NEEDS_GTT))\n\t\tpin_flags |= PIN_GLOBAL;\n\n\t \n\terr = i915_vma_pin_ww(vma, &eb->ww, 0, 0, pin_flags);\n\tif (err == -EDEADLK)\n\t\treturn err;\n\n\tif (unlikely(err)) {\n\t\tif (entry->flags & EXEC_OBJECT_PINNED)\n\t\t\treturn err;\n\n\t\t \n\t\terr = i915_vma_pin_ww(vma, &eb->ww,\n\t\t\t\t\t     entry->pad_to_size,\n\t\t\t\t\t     entry->alignment,\n\t\t\t\t\t     eb_pin_flags(entry, ev->flags) |\n\t\t\t\t\t     PIN_USER | PIN_NOEVICT | PIN_VALIDATE);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\tif (unlikely(ev->flags & EXEC_OBJECT_NEEDS_FENCE)) {\n\t\terr = i915_vma_pin_fence(vma);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tif (vma->fence)\n\t\t\tev->flags |= __EXEC_OBJECT_HAS_FENCE;\n\t}\n\n\tev->flags |= __EXEC_OBJECT_HAS_PIN;\n\tif (eb_vma_misplaced(entry, vma, ev->flags))\n\t\treturn -EBADSLT;\n\n\treturn 0;\n}\n\nstatic inline void\neb_unreserve_vma(struct eb_vma *ev)\n{\n\tif (unlikely(ev->flags & __EXEC_OBJECT_HAS_FENCE))\n\t\t__i915_vma_unpin_fence(ev->vma);\n\n\tev->flags &= ~__EXEC_OBJECT_RESERVED;\n}\n\nstatic int\neb_validate_vma(struct i915_execbuffer *eb,\n\t\tstruct drm_i915_gem_exec_object2 *entry,\n\t\tstruct i915_vma *vma)\n{\n\t \n\tif (entry->relocation_count &&\n\t    GRAPHICS_VER(eb->i915) >= 12 && !IS_TIGERLAKE(eb->i915))\n\t\treturn -EINVAL;\n\n\tif (unlikely(entry->flags & eb->invalid_flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(entry->alignment &&\n\t\t     !is_power_of_2_u64(entry->alignment)))\n\t\treturn -EINVAL;\n\n\t \n\tif (unlikely(entry->flags & EXEC_OBJECT_PINNED &&\n\t\t     entry->offset != gen8_canonical_addr(entry->offset & I915_GTT_PAGE_MASK)))\n\t\treturn -EINVAL;\n\n\t \n\tif (entry->flags & EXEC_OBJECT_PAD_TO_SIZE) {\n\t\tif (unlikely(offset_in_page(entry->pad_to_size)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tentry->pad_to_size = 0;\n\t}\n\t \n\tentry->offset = gen8_noncanonical_addr(entry->offset);\n\n\tif (!eb->reloc_cache.has_fence) {\n\t\tentry->flags &= ~EXEC_OBJECT_NEEDS_FENCE;\n\t} else {\n\t\tif ((entry->flags & EXEC_OBJECT_NEEDS_FENCE ||\n\t\t     eb->reloc_cache.needs_unfenced) &&\n\t\t    i915_gem_object_is_tiled(vma->obj))\n\t\t\tentry->flags |= EXEC_OBJECT_NEEDS_GTT | __EXEC_OBJECT_NEEDS_MAP;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool\nis_batch_buffer(struct i915_execbuffer *eb, unsigned int buffer_idx)\n{\n\treturn eb->args->flags & I915_EXEC_BATCH_FIRST ?\n\t\tbuffer_idx < eb->num_batches :\n\t\tbuffer_idx >= eb->args->buffer_count - eb->num_batches;\n}\n\nstatic int\neb_add_vma(struct i915_execbuffer *eb,\n\t   unsigned int *current_batch,\n\t   unsigned int i,\n\t   struct i915_vma *vma)\n{\n\tstruct drm_i915_private *i915 = eb->i915;\n\tstruct drm_i915_gem_exec_object2 *entry = &eb->exec[i];\n\tstruct eb_vma *ev = &eb->vma[i];\n\n\tev->vma = vma;\n\tev->exec = entry;\n\tev->flags = entry->flags;\n\n\tif (eb->lut_size > 0) {\n\t\tev->handle = entry->handle;\n\t\thlist_add_head(&ev->node,\n\t\t\t       &eb->buckets[hash_32(entry->handle,\n\t\t\t\t\t\t    eb->lut_size)]);\n\t}\n\n\tif (entry->relocation_count)\n\t\tlist_add_tail(&ev->reloc_link, &eb->relocs);\n\n\t \n\tif (is_batch_buffer(eb, i)) {\n\t\tif (entry->relocation_count &&\n\t\t    !(ev->flags & EXEC_OBJECT_PINNED))\n\t\t\tev->flags |= __EXEC_OBJECT_NEEDS_BIAS;\n\t\tif (eb->reloc_cache.has_fence)\n\t\t\tev->flags |= EXEC_OBJECT_NEEDS_FENCE;\n\n\t\teb->batches[*current_batch] = ev;\n\n\t\tif (unlikely(ev->flags & EXEC_OBJECT_WRITE)) {\n\t\t\tdrm_dbg(&i915->drm,\n\t\t\t\t\"Attempting to use self-modifying batch buffer\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (range_overflows_t(u64,\n\t\t\t\t      eb->batch_start_offset,\n\t\t\t\t      eb->args->batch_len,\n\t\t\t\t      ev->vma->size)) {\n\t\t\tdrm_dbg(&i915->drm, \"Attempting to use out-of-bounds batch\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (eb->args->batch_len == 0)\n\t\t\teb->batch_len[*current_batch] = ev->vma->size -\n\t\t\t\teb->batch_start_offset;\n\t\telse\n\t\t\teb->batch_len[*current_batch] = eb->args->batch_len;\n\t\tif (unlikely(eb->batch_len[*current_batch] == 0)) {  \n\t\t\tdrm_dbg(&i915->drm, \"Invalid batch length\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t++*current_batch;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int use_cpu_reloc(const struct reloc_cache *cache,\n\t\t\t\tconst struct drm_i915_gem_object *obj)\n{\n\tif (!i915_gem_object_has_struct_page(obj))\n\t\treturn false;\n\n\tif (DBG_FORCE_RELOC == FORCE_CPU_RELOC)\n\t\treturn true;\n\n\tif (DBG_FORCE_RELOC == FORCE_GTT_RELOC)\n\t\treturn false;\n\n\t \n\treturn (cache->has_llc ||\n\t\tobj->cache_dirty ||\n\t\t!i915_gem_object_has_cache_level(obj, I915_CACHE_NONE));\n}\n\nstatic int eb_reserve_vma(struct i915_execbuffer *eb,\n\t\t\t  struct eb_vma *ev,\n\t\t\t  u64 pin_flags)\n{\n\tstruct drm_i915_gem_exec_object2 *entry = ev->exec;\n\tstruct i915_vma *vma = ev->vma;\n\tint err;\n\n\tif (drm_mm_node_allocated(&vma->node) &&\n\t    eb_vma_misplaced(entry, vma, ev->flags)) {\n\t\terr = i915_vma_unbind(vma);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = i915_vma_pin_ww(vma, &eb->ww,\n\t\t\t   entry->pad_to_size, entry->alignment,\n\t\t\t   eb_pin_flags(entry, ev->flags) | pin_flags);\n\tif (err)\n\t\treturn err;\n\n\tif (entry->offset != i915_vma_offset(vma)) {\n\t\tentry->offset = i915_vma_offset(vma) | UPDATE;\n\t\teb->args->flags |= __EXEC_HAS_RELOC;\n\t}\n\n\tif (unlikely(ev->flags & EXEC_OBJECT_NEEDS_FENCE)) {\n\t\terr = i915_vma_pin_fence(vma);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\n\t\tif (vma->fence)\n\t\t\tev->flags |= __EXEC_OBJECT_HAS_FENCE;\n\t}\n\n\tev->flags |= __EXEC_OBJECT_HAS_PIN;\n\tGEM_BUG_ON(eb_vma_misplaced(entry, vma, ev->flags));\n\n\treturn 0;\n}\n\nstatic bool eb_unbind(struct i915_execbuffer *eb, bool force)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\tstruct list_head last;\n\tbool unpinned = false;\n\n\t \n\tINIT_LIST_HEAD(&eb->unbound);\n\tINIT_LIST_HEAD(&last);\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tunsigned int flags = ev->flags;\n\n\t\tif (!force && flags & EXEC_OBJECT_PINNED &&\n\t\t    flags & __EXEC_OBJECT_HAS_PIN)\n\t\t\tcontinue;\n\n\t\tunpinned = true;\n\t\teb_unreserve_vma(ev);\n\n\t\tif (flags & EXEC_OBJECT_PINNED)\n\t\t\t \n\t\t\tlist_add(&ev->bind_link, &eb->unbound);\n\t\telse if (flags & __EXEC_OBJECT_NEEDS_MAP)\n\t\t\t \n\t\t\tlist_add_tail(&ev->bind_link, &eb->unbound);\n\t\telse if (!(flags & EXEC_OBJECT_SUPPORTS_48B_ADDRESS))\n\t\t\t \n\t\t\tlist_add(&ev->bind_link, &last);\n\t\telse\n\t\t\tlist_add_tail(&ev->bind_link, &last);\n\t}\n\n\tlist_splice_tail(&last, &eb->unbound);\n\treturn unpinned;\n}\n\nstatic int eb_reserve(struct i915_execbuffer *eb)\n{\n\tstruct eb_vma *ev;\n\tunsigned int pass;\n\tint err = 0;\n\n\t \n\tfor (pass = 0; pass <= 3; pass++) {\n\t\tint pin_flags = PIN_USER | PIN_VALIDATE;\n\n\t\tif (pass == 0)\n\t\t\tpin_flags |= PIN_NONBLOCK;\n\n\t\tif (pass >= 1)\n\t\t\teb_unbind(eb, pass >= 2);\n\n\t\tif (pass == 2) {\n\t\t\terr = mutex_lock_interruptible(&eb->context->vm->mutex);\n\t\t\tif (!err) {\n\t\t\t\terr = i915_gem_evict_vm(eb->context->vm, &eb->ww, NULL);\n\t\t\t\tmutex_unlock(&eb->context->vm->mutex);\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tif (pass == 3) {\nretry:\n\t\t\terr = mutex_lock_interruptible(&eb->context->vm->mutex);\n\t\t\tif (!err) {\n\t\t\t\tstruct drm_i915_gem_object *busy_bo = NULL;\n\n\t\t\t\terr = i915_gem_evict_vm(eb->context->vm, &eb->ww, &busy_bo);\n\t\t\t\tmutex_unlock(&eb->context->vm->mutex);\n\t\t\t\tif (err && busy_bo) {\n\t\t\t\t\terr = i915_gem_object_lock(busy_bo, &eb->ww);\n\t\t\t\t\ti915_gem_object_put(busy_bo);\n\t\t\t\t\tif (!err)\n\t\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\n\t\tlist_for_each_entry(ev, &eb->unbound, bind_link) {\n\t\t\terr = eb_reserve_vma(eb, ev, pin_flags);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (err != -ENOSPC)\n\t\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic int eb_select_context(struct i915_execbuffer *eb)\n{\n\tstruct i915_gem_context *ctx;\n\n\tctx = i915_gem_context_lookup(eb->file->driver_priv, eb->args->rsvd1);\n\tif (unlikely(IS_ERR(ctx)))\n\t\treturn PTR_ERR(ctx);\n\n\teb->gem_context = ctx;\n\tif (i915_gem_context_has_full_ppgtt(ctx))\n\t\teb->invalid_flags |= EXEC_OBJECT_NEEDS_GTT;\n\n\treturn 0;\n}\n\nstatic int __eb_add_lut(struct i915_execbuffer *eb,\n\t\t\tu32 handle, struct i915_vma *vma)\n{\n\tstruct i915_gem_context *ctx = eb->gem_context;\n\tstruct i915_lut_handle *lut;\n\tint err;\n\n\tlut = i915_lut_handle_alloc();\n\tif (unlikely(!lut))\n\t\treturn -ENOMEM;\n\n\ti915_vma_get(vma);\n\tif (!atomic_fetch_inc(&vma->open_count))\n\t\ti915_vma_reopen(vma);\n\tlut->handle = handle;\n\tlut->ctx = ctx;\n\n\t \n\terr = -EINTR;\n\tif (!mutex_lock_interruptible(&ctx->lut_mutex)) {\n\t\tif (likely(!i915_gem_context_is_closed(ctx)))\n\t\t\terr = radix_tree_insert(&ctx->handles_vma, handle, vma);\n\t\telse\n\t\t\terr = -ENOENT;\n\t\tif (err == 0) {  \n\t\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\t\t\tspin_lock(&obj->lut_lock);\n\t\t\tif (idr_find(&eb->file->object_idr, handle) == obj) {\n\t\t\t\tlist_add(&lut->obj_link, &obj->lut_list);\n\t\t\t} else {\n\t\t\t\tradix_tree_delete(&ctx->handles_vma, handle);\n\t\t\t\terr = -ENOENT;\n\t\t\t}\n\t\t\tspin_unlock(&obj->lut_lock);\n\t\t}\n\t\tmutex_unlock(&ctx->lut_mutex);\n\t}\n\tif (unlikely(err))\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\ti915_vma_close(vma);\n\ti915_vma_put(vma);\n\ti915_lut_handle_free(lut);\n\treturn err;\n}\n\nstatic struct i915_vma *eb_lookup_vma(struct i915_execbuffer *eb, u32 handle)\n{\n\tstruct i915_address_space *vm = eb->context->vm;\n\n\tdo {\n\t\tstruct drm_i915_gem_object *obj;\n\t\tstruct i915_vma *vma;\n\t\tint err;\n\n\t\trcu_read_lock();\n\t\tvma = radix_tree_lookup(&eb->gem_context->handles_vma, handle);\n\t\tif (likely(vma && vma->vm == vm))\n\t\t\tvma = i915_vma_tryget(vma);\n\t\trcu_read_unlock();\n\t\tif (likely(vma))\n\t\t\treturn vma;\n\n\t\tobj = i915_gem_object_lookup(eb->file, handle);\n\t\tif (unlikely(!obj))\n\t\t\treturn ERR_PTR(-ENOENT);\n\n\t\t \n\t\tif (i915_gem_context_uses_protected_content(eb->gem_context) &&\n\t\t    i915_gem_object_is_protected(obj)) {\n\t\t\terr = intel_pxp_key_check(eb->i915->pxp, obj, true);\n\t\t\tif (err) {\n\t\t\t\ti915_gem_object_put(obj);\n\t\t\t\treturn ERR_PTR(err);\n\t\t\t}\n\t\t}\n\n\t\tvma = i915_vma_instance(obj, vm, NULL);\n\t\tif (IS_ERR(vma)) {\n\t\t\ti915_gem_object_put(obj);\n\t\t\treturn vma;\n\t\t}\n\n\t\terr = __eb_add_lut(eb, handle, vma);\n\t\tif (likely(!err))\n\t\t\treturn vma;\n\n\t\ti915_gem_object_put(obj);\n\t\tif (err != -EEXIST)\n\t\t\treturn ERR_PTR(err);\n\t} while (1);\n}\n\nstatic int eb_lookup_vmas(struct i915_execbuffer *eb)\n{\n\tunsigned int i, current_batch = 0;\n\tint err = 0;\n\n\tINIT_LIST_HEAD(&eb->relocs);\n\n\tfor (i = 0; i < eb->buffer_count; i++) {\n\t\tstruct i915_vma *vma;\n\n\t\tvma = eb_lookup_vma(eb, eb->exec[i].handle);\n\t\tif (IS_ERR(vma)) {\n\t\t\terr = PTR_ERR(vma);\n\t\t\tgoto err;\n\t\t}\n\n\t\terr = eb_validate_vma(eb, &eb->exec[i], vma);\n\t\tif (unlikely(err)) {\n\t\t\ti915_vma_put(vma);\n\t\t\tgoto err;\n\t\t}\n\n\t\terr = eb_add_vma(eb, &current_batch, i, vma);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (i915_gem_object_is_userptr(vma->obj)) {\n\t\t\terr = i915_gem_object_userptr_submit_init(vma->obj);\n\t\t\tif (err) {\n\t\t\t\tif (i + 1 < eb->buffer_count) {\n\t\t\t\t\t \n\t\t\t\t\teb->vma[i + 1].vma = NULL;\n\t\t\t\t}\n\n\t\t\t\treturn err;\n\t\t\t}\n\n\t\t\teb->vma[i].flags |= __EXEC_OBJECT_USERPTR_INIT;\n\t\t\teb->args->flags |= __EXEC_USERPTR_USED;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\teb->vma[i].vma = NULL;\n\treturn err;\n}\n\nstatic int eb_lock_vmas(struct i915_execbuffer *eb)\n{\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < eb->buffer_count; i++) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tstruct i915_vma *vma = ev->vma;\n\n\t\terr = i915_gem_object_lock(vma->obj, &eb->ww);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int eb_validate_vmas(struct i915_execbuffer *eb)\n{\n\tunsigned int i;\n\tint err;\n\n\tINIT_LIST_HEAD(&eb->unbound);\n\n\terr = eb_lock_vmas(eb);\n\tif (err)\n\t\treturn err;\n\n\tfor (i = 0; i < eb->buffer_count; i++) {\n\t\tstruct drm_i915_gem_exec_object2 *entry = &eb->exec[i];\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tstruct i915_vma *vma = ev->vma;\n\n\t\terr = eb_pin_vma(eb, entry, ev);\n\t\tif (err == -EDEADLK)\n\t\t\treturn err;\n\n\t\tif (!err) {\n\t\t\tif (entry->offset != i915_vma_offset(vma)) {\n\t\t\t\tentry->offset = i915_vma_offset(vma) | UPDATE;\n\t\t\t\teb->args->flags |= __EXEC_HAS_RELOC;\n\t\t\t}\n\t\t} else {\n\t\t\teb_unreserve_vma(ev);\n\n\t\t\tlist_add_tail(&ev->bind_link, &eb->unbound);\n\t\t\tif (drm_mm_node_allocated(&vma->node)) {\n\t\t\t\terr = i915_vma_unbind(vma);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = dma_resv_reserve_fences(vma->obj->base.resv, eb->num_batches);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tGEM_BUG_ON(drm_mm_node_allocated(&vma->node) &&\n\t\t\t   eb_vma_misplaced(&eb->exec[i], vma, ev->flags));\n\t}\n\n\tif (!list_empty(&eb->unbound))\n\t\treturn eb_reserve(eb);\n\n\treturn 0;\n}\n\nstatic struct eb_vma *\neb_get_vma(const struct i915_execbuffer *eb, unsigned long handle)\n{\n\tif (eb->lut_size < 0) {\n\t\tif (handle >= -eb->lut_size)\n\t\t\treturn NULL;\n\t\treturn &eb->vma[handle];\n\t} else {\n\t\tstruct hlist_head *head;\n\t\tstruct eb_vma *ev;\n\n\t\thead = &eb->buckets[hash_32(handle, eb->lut_size)];\n\t\thlist_for_each_entry(ev, head, node) {\n\t\t\tif (ev->handle == handle)\n\t\t\t\treturn ev;\n\t\t}\n\t\treturn NULL;\n\t}\n}\n\nstatic void eb_release_vmas(struct i915_execbuffer *eb, bool final)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tstruct i915_vma *vma = ev->vma;\n\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\teb_unreserve_vma(ev);\n\n\t\tif (final)\n\t\t\ti915_vma_put(vma);\n\t}\n\n\teb_capture_release(eb);\n\teb_unpin_engine(eb);\n}\n\nstatic void eb_destroy(const struct i915_execbuffer *eb)\n{\n\tif (eb->lut_size > 0)\n\t\tkfree(eb->buckets);\n}\n\nstatic inline u64\nrelocation_target(const struct drm_i915_gem_relocation_entry *reloc,\n\t\t  const struct i915_vma *target)\n{\n\treturn gen8_canonical_addr((int)reloc->delta + i915_vma_offset(target));\n}\n\nstatic void reloc_cache_init(struct reloc_cache *cache,\n\t\t\t     struct drm_i915_private *i915)\n{\n\tcache->page = -1;\n\tcache->vaddr = 0;\n\t \n\tcache->graphics_ver = GRAPHICS_VER(i915);\n\tcache->has_llc = HAS_LLC(i915);\n\tcache->use_64bit_reloc = HAS_64BIT_RELOC(i915);\n\tcache->has_fence = cache->graphics_ver < 4;\n\tcache->needs_unfenced = INTEL_INFO(i915)->unfenced_needs_alignment;\n\tcache->node.flags = 0;\n}\n\nstatic inline void *unmask_page(unsigned long p)\n{\n\treturn (void *)(uintptr_t)(p & PAGE_MASK);\n}\n\nstatic inline unsigned int unmask_flags(unsigned long p)\n{\n\treturn p & ~PAGE_MASK;\n}\n\n#define KMAP 0x4  \n\nstatic inline struct i915_ggtt *cache_to_ggtt(struct reloc_cache *cache)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(cache, struct i915_execbuffer, reloc_cache)->i915;\n\treturn to_gt(i915)->ggtt;\n}\n\nstatic void reloc_cache_unmap(struct reloc_cache *cache)\n{\n\tvoid *vaddr;\n\n\tif (!cache->vaddr)\n\t\treturn;\n\n\tvaddr = unmask_page(cache->vaddr);\n\tif (cache->vaddr & KMAP)\n\t\tkunmap_atomic(vaddr);\n\telse\n\t\tio_mapping_unmap_atomic((void __iomem *)vaddr);\n}\n\nstatic void reloc_cache_remap(struct reloc_cache *cache,\n\t\t\t      struct drm_i915_gem_object *obj)\n{\n\tvoid *vaddr;\n\n\tif (!cache->vaddr)\n\t\treturn;\n\n\tif (cache->vaddr & KMAP) {\n\t\tstruct page *page = i915_gem_object_get_page(obj, cache->page);\n\n\t\tvaddr = kmap_atomic(page);\n\t\tcache->vaddr = unmask_flags(cache->vaddr) |\n\t\t\t(unsigned long)vaddr;\n\t} else {\n\t\tstruct i915_ggtt *ggtt = cache_to_ggtt(cache);\n\t\tunsigned long offset;\n\n\t\toffset = cache->node.start;\n\t\tif (!drm_mm_node_allocated(&cache->node))\n\t\t\toffset += cache->page << PAGE_SHIFT;\n\n\t\tcache->vaddr = (unsigned long)\n\t\t\tio_mapping_map_atomic_wc(&ggtt->iomap, offset);\n\t}\n}\n\nstatic void reloc_cache_reset(struct reloc_cache *cache, struct i915_execbuffer *eb)\n{\n\tvoid *vaddr;\n\n\tif (!cache->vaddr)\n\t\treturn;\n\n\tvaddr = unmask_page(cache->vaddr);\n\tif (cache->vaddr & KMAP) {\n\t\tstruct drm_i915_gem_object *obj =\n\t\t\t(struct drm_i915_gem_object *)cache->node.mm;\n\t\tif (cache->vaddr & CLFLUSH_AFTER)\n\t\t\tmb();\n\n\t\tkunmap_atomic(vaddr);\n\t\ti915_gem_object_finish_access(obj);\n\t} else {\n\t\tstruct i915_ggtt *ggtt = cache_to_ggtt(cache);\n\n\t\tintel_gt_flush_ggtt_writes(ggtt->vm.gt);\n\t\tio_mapping_unmap_atomic((void __iomem *)vaddr);\n\n\t\tif (drm_mm_node_allocated(&cache->node)) {\n\t\t\tggtt->vm.clear_range(&ggtt->vm,\n\t\t\t\t\t     cache->node.start,\n\t\t\t\t\t     cache->node.size);\n\t\t\tmutex_lock(&ggtt->vm.mutex);\n\t\t\tdrm_mm_remove_node(&cache->node);\n\t\t\tmutex_unlock(&ggtt->vm.mutex);\n\t\t} else {\n\t\t\ti915_vma_unpin((struct i915_vma *)cache->node.mm);\n\t\t}\n\t}\n\n\tcache->vaddr = 0;\n\tcache->page = -1;\n}\n\nstatic void *reloc_kmap(struct drm_i915_gem_object *obj,\n\t\t\tstruct reloc_cache *cache,\n\t\t\tunsigned long pageno)\n{\n\tvoid *vaddr;\n\tstruct page *page;\n\n\tif (cache->vaddr) {\n\t\tkunmap_atomic(unmask_page(cache->vaddr));\n\t} else {\n\t\tunsigned int flushes;\n\t\tint err;\n\n\t\terr = i915_gem_object_prepare_write(obj, &flushes);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\n\t\tBUILD_BUG_ON(KMAP & CLFLUSH_FLAGS);\n\t\tBUILD_BUG_ON((KMAP | CLFLUSH_FLAGS) & PAGE_MASK);\n\n\t\tcache->vaddr = flushes | KMAP;\n\t\tcache->node.mm = (void *)obj;\n\t\tif (flushes)\n\t\t\tmb();\n\t}\n\n\tpage = i915_gem_object_get_page(obj, pageno);\n\tif (!obj->mm.dirty)\n\t\tset_page_dirty(page);\n\n\tvaddr = kmap_atomic(page);\n\tcache->vaddr = unmask_flags(cache->vaddr) | (unsigned long)vaddr;\n\tcache->page = pageno;\n\n\treturn vaddr;\n}\n\nstatic void *reloc_iomap(struct i915_vma *batch,\n\t\t\t struct i915_execbuffer *eb,\n\t\t\t unsigned long page)\n{\n\tstruct drm_i915_gem_object *obj = batch->obj;\n\tstruct reloc_cache *cache = &eb->reloc_cache;\n\tstruct i915_ggtt *ggtt = cache_to_ggtt(cache);\n\tunsigned long offset;\n\tvoid *vaddr;\n\n\tif (cache->vaddr) {\n\t\tintel_gt_flush_ggtt_writes(ggtt->vm.gt);\n\t\tio_mapping_unmap_atomic((void __force __iomem *) unmask_page(cache->vaddr));\n\t} else {\n\t\tstruct i915_vma *vma = ERR_PTR(-ENODEV);\n\t\tint err;\n\n\t\tif (i915_gem_object_is_tiled(obj))\n\t\t\treturn ERR_PTR(-EINVAL);\n\n\t\tif (use_cpu_reloc(cache, obj))\n\t\t\treturn NULL;\n\n\t\terr = i915_gem_object_set_to_gtt_domain(obj, true);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\n\t\t \n\t\tif (!i915_is_ggtt(batch->vm) ||\n\t\t    !i915_vma_misplaced(batch, 0, 0, PIN_MAPPABLE)) {\n\t\t\tvma = i915_gem_object_ggtt_pin_ww(obj, &eb->ww, NULL, 0, 0,\n\t\t\t\t\t\t\t  PIN_MAPPABLE |\n\t\t\t\t\t\t\t  PIN_NONBLOCK   |\n\t\t\t\t\t\t\t  PIN_NOEVICT);\n\t\t}\n\n\t\tif (vma == ERR_PTR(-EDEADLK))\n\t\t\treturn vma;\n\n\t\tif (IS_ERR(vma)) {\n\t\t\tmemset(&cache->node, 0, sizeof(cache->node));\n\t\t\tmutex_lock(&ggtt->vm.mutex);\n\t\t\terr = drm_mm_insert_node_in_range\n\t\t\t\t(&ggtt->vm.mm, &cache->node,\n\t\t\t\t PAGE_SIZE, 0, I915_COLOR_UNEVICTABLE,\n\t\t\t\t 0, ggtt->mappable_end,\n\t\t\t\t DRM_MM_INSERT_LOW);\n\t\t\tmutex_unlock(&ggtt->vm.mutex);\n\t\t\tif (err)  \n\t\t\t\treturn NULL;\n\t\t} else {\n\t\t\tcache->node.start = i915_ggtt_offset(vma);\n\t\t\tcache->node.mm = (void *)vma;\n\t\t}\n\t}\n\n\toffset = cache->node.start;\n\tif (drm_mm_node_allocated(&cache->node)) {\n\t\tggtt->vm.insert_page(&ggtt->vm,\n\t\t\t\t     i915_gem_object_get_dma_address(obj, page),\n\t\t\t\t     offset,\n\t\t\t\t     i915_gem_get_pat_index(ggtt->vm.i915,\n\t\t\t\t\t\t\t    I915_CACHE_NONE),\n\t\t\t\t     0);\n\t} else {\n\t\toffset += page << PAGE_SHIFT;\n\t}\n\n\tvaddr = (void __force *)io_mapping_map_atomic_wc(&ggtt->iomap,\n\t\t\t\t\t\t\t offset);\n\tcache->page = page;\n\tcache->vaddr = (unsigned long)vaddr;\n\n\treturn vaddr;\n}\n\nstatic void *reloc_vaddr(struct i915_vma *vma,\n\t\t\t struct i915_execbuffer *eb,\n\t\t\t unsigned long page)\n{\n\tstruct reloc_cache *cache = &eb->reloc_cache;\n\tvoid *vaddr;\n\n\tif (cache->page == page) {\n\t\tvaddr = unmask_page(cache->vaddr);\n\t} else {\n\t\tvaddr = NULL;\n\t\tif ((cache->vaddr & KMAP) == 0)\n\t\t\tvaddr = reloc_iomap(vma, eb, page);\n\t\tif (!vaddr)\n\t\t\tvaddr = reloc_kmap(vma->obj, cache, page);\n\t}\n\n\treturn vaddr;\n}\n\nstatic void clflush_write32(u32 *addr, u32 value, unsigned int flushes)\n{\n\tif (unlikely(flushes & (CLFLUSH_BEFORE | CLFLUSH_AFTER))) {\n\t\tif (flushes & CLFLUSH_BEFORE)\n\t\t\tdrm_clflush_virt_range(addr, sizeof(*addr));\n\n\t\t*addr = value;\n\n\t\t \n\t\tif (flushes & CLFLUSH_AFTER)\n\t\t\tdrm_clflush_virt_range(addr, sizeof(*addr));\n\t} else\n\t\t*addr = value;\n}\n\nstatic u64\nrelocate_entry(struct i915_vma *vma,\n\t       const struct drm_i915_gem_relocation_entry *reloc,\n\t       struct i915_execbuffer *eb,\n\t       const struct i915_vma *target)\n{\n\tu64 target_addr = relocation_target(reloc, target);\n\tu64 offset = reloc->offset;\n\tbool wide = eb->reloc_cache.use_64bit_reloc;\n\tvoid *vaddr;\n\nrepeat:\n\tvaddr = reloc_vaddr(vma, eb,\n\t\t\t    offset >> PAGE_SHIFT);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\tGEM_BUG_ON(!IS_ALIGNED(offset, sizeof(u32)));\n\tclflush_write32(vaddr + offset_in_page(offset),\n\t\t\tlower_32_bits(target_addr),\n\t\t\teb->reloc_cache.vaddr);\n\n\tif (wide) {\n\t\toffset += sizeof(u32);\n\t\ttarget_addr >>= 32;\n\t\twide = false;\n\t\tgoto repeat;\n\t}\n\n\treturn target->node.start | UPDATE;\n}\n\nstatic u64\neb_relocate_entry(struct i915_execbuffer *eb,\n\t\t  struct eb_vma *ev,\n\t\t  const struct drm_i915_gem_relocation_entry *reloc)\n{\n\tstruct drm_i915_private *i915 = eb->i915;\n\tstruct eb_vma *target;\n\tint err;\n\n\t \n\ttarget = eb_get_vma(eb, reloc->target_handle);\n\tif (unlikely(!target))\n\t\treturn -ENOENT;\n\n\t \n\tif (unlikely(reloc->write_domain & (reloc->write_domain - 1))) {\n\t\tdrm_dbg(&i915->drm, \"reloc with multiple write domains: \"\n\t\t\t  \"target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn -EINVAL;\n\t}\n\tif (unlikely((reloc->write_domain | reloc->read_domains)\n\t\t     & ~I915_GEM_GPU_DOMAINS)) {\n\t\tdrm_dbg(&i915->drm, \"reloc with read/write non-GPU domains: \"\n\t\t\t  \"target %d offset %d \"\n\t\t\t  \"read %08x write %08x\",\n\t\t\t  reloc->target_handle,\n\t\t\t  (int) reloc->offset,\n\t\t\t  reloc->read_domains,\n\t\t\t  reloc->write_domain);\n\t\treturn -EINVAL;\n\t}\n\n\tif (reloc->write_domain) {\n\t\ttarget->flags |= EXEC_OBJECT_WRITE;\n\n\t\t \n\t\tif (reloc->write_domain == I915_GEM_DOMAIN_INSTRUCTION &&\n\t\t    GRAPHICS_VER(eb->i915) == 6 &&\n\t\t    !i915_vma_is_bound(target->vma, I915_VMA_GLOBAL_BIND)) {\n\t\t\tstruct i915_vma *vma = target->vma;\n\n\t\t\treloc_cache_unmap(&eb->reloc_cache);\n\t\t\tmutex_lock(&vma->vm->mutex);\n\t\t\terr = i915_vma_bind(target->vma,\n\t\t\t\t\t    target->vma->obj->pat_index,\n\t\t\t\t\t    PIN_GLOBAL, NULL, NULL);\n\t\t\tmutex_unlock(&vma->vm->mutex);\n\t\t\treloc_cache_remap(&eb->reloc_cache, ev->vma->obj);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tif (!DBG_FORCE_RELOC &&\n\t    gen8_canonical_addr(i915_vma_offset(target->vma)) == reloc->presumed_offset)\n\t\treturn 0;\n\n\t \n\tif (unlikely(reloc->offset >\n\t\t     ev->vma->size - (eb->reloc_cache.use_64bit_reloc ? 8 : 4))) {\n\t\tdrm_dbg(&i915->drm, \"Relocation beyond object bounds: \"\n\t\t\t  \"target %d offset %d size %d.\\n\",\n\t\t\t  reloc->target_handle,\n\t\t\t  (int)reloc->offset,\n\t\t\t  (int)ev->vma->size);\n\t\treturn -EINVAL;\n\t}\n\tif (unlikely(reloc->offset & 3)) {\n\t\tdrm_dbg(&i915->drm, \"Relocation not 4-byte aligned: \"\n\t\t\t  \"target %d offset %d.\\n\",\n\t\t\t  reloc->target_handle,\n\t\t\t  (int)reloc->offset);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tev->flags &= ~EXEC_OBJECT_ASYNC;\n\n\t \n\treturn relocate_entry(ev->vma, reloc, eb, target->vma);\n}\n\nstatic int eb_relocate_vma(struct i915_execbuffer *eb, struct eb_vma *ev)\n{\n#define N_RELOC(x) ((x) / sizeof(struct drm_i915_gem_relocation_entry))\n\tstruct drm_i915_gem_relocation_entry stack[N_RELOC(512)];\n\tconst struct drm_i915_gem_exec_object2 *entry = ev->exec;\n\tstruct drm_i915_gem_relocation_entry __user *urelocs =\n\t\tu64_to_user_ptr(entry->relocs_ptr);\n\tunsigned long remain = entry->relocation_count;\n\n\tif (unlikely(remain > N_RELOC(ULONG_MAX)))\n\t\treturn -EINVAL;\n\n\t \n\tif (unlikely(!access_ok(urelocs, remain * sizeof(*urelocs))))\n\t\treturn -EFAULT;\n\n\tdo {\n\t\tstruct drm_i915_gem_relocation_entry *r = stack;\n\t\tunsigned int count =\n\t\t\tmin_t(unsigned long, remain, ARRAY_SIZE(stack));\n\t\tunsigned int copied;\n\n\t\t \n\t\tpagefault_disable();\n\t\tcopied = __copy_from_user_inatomic(r, urelocs, count * sizeof(r[0]));\n\t\tpagefault_enable();\n\t\tif (unlikely(copied)) {\n\t\t\tremain = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tremain -= count;\n\t\tdo {\n\t\t\tu64 offset = eb_relocate_entry(eb, ev, r);\n\n\t\t\tif (likely(offset == 0)) {\n\t\t\t} else if ((s64)offset < 0) {\n\t\t\t\tremain = (int)offset;\n\t\t\t\tgoto out;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\toffset = gen8_canonical_addr(offset & ~UPDATE);\n\t\t\t\t__put_user(offset,\n\t\t\t\t\t   &urelocs[r - stack].presumed_offset);\n\t\t\t}\n\t\t} while (r++, --count);\n\t\turelocs += ARRAY_SIZE(stack);\n\t} while (remain);\nout:\n\treloc_cache_reset(&eb->reloc_cache, eb);\n\treturn remain;\n}\n\nstatic int\neb_relocate_vma_slow(struct i915_execbuffer *eb, struct eb_vma *ev)\n{\n\tconst struct drm_i915_gem_exec_object2 *entry = ev->exec;\n\tstruct drm_i915_gem_relocation_entry *relocs =\n\t\tu64_to_ptr(typeof(*relocs), entry->relocs_ptr);\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < entry->relocation_count; i++) {\n\t\tu64 offset = eb_relocate_entry(eb, ev, &relocs[i]);\n\n\t\tif ((s64)offset < 0) {\n\t\t\terr = (int)offset;\n\t\t\tgoto err;\n\t\t}\n\t}\n\terr = 0;\nerr:\n\treloc_cache_reset(&eb->reloc_cache, eb);\n\treturn err;\n}\n\nstatic int check_relocations(const struct drm_i915_gem_exec_object2 *entry)\n{\n\tconst char __user *addr, *end;\n\tunsigned long size;\n\tchar __maybe_unused c;\n\n\tsize = entry->relocation_count;\n\tif (size == 0)\n\t\treturn 0;\n\n\tif (size > N_RELOC(ULONG_MAX))\n\t\treturn -EINVAL;\n\n\taddr = u64_to_user_ptr(entry->relocs_ptr);\n\tsize *= sizeof(struct drm_i915_gem_relocation_entry);\n\tif (!access_ok(addr, size))\n\t\treturn -EFAULT;\n\n\tend = addr + size;\n\tfor (; addr < end; addr += PAGE_SIZE) {\n\t\tint err = __get_user(c, addr);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\treturn __get_user(c, end - 1);\n}\n\nstatic int eb_copy_relocations(const struct i915_execbuffer *eb)\n{\n\tstruct drm_i915_gem_relocation_entry *relocs;\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\tint err;\n\n\tfor (i = 0; i < count; i++) {\n\t\tconst unsigned int nreloc = eb->exec[i].relocation_count;\n\t\tstruct drm_i915_gem_relocation_entry __user *urelocs;\n\t\tunsigned long size;\n\t\tunsigned long copied;\n\n\t\tif (nreloc == 0)\n\t\t\tcontinue;\n\n\t\terr = check_relocations(&eb->exec[i]);\n\t\tif (err)\n\t\t\tgoto err;\n\n\t\turelocs = u64_to_user_ptr(eb->exec[i].relocs_ptr);\n\t\tsize = nreloc * sizeof(*relocs);\n\n\t\trelocs = kvmalloc_array(size, 1, GFP_KERNEL);\n\t\tif (!relocs) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\tcopied = 0;\n\t\tdo {\n\t\t\tunsigned int len =\n\t\t\t\tmin_t(u64, BIT_ULL(31), size - copied);\n\n\t\t\tif (__copy_from_user((char *)relocs + copied,\n\t\t\t\t\t     (char __user *)urelocs + copied,\n\t\t\t\t\t     len))\n\t\t\t\tgoto end;\n\n\t\t\tcopied += len;\n\t\t} while (copied < size);\n\n\t\t \n\t\tif (!user_access_begin(urelocs, size))\n\t\t\tgoto end;\n\n\t\tfor (copied = 0; copied < nreloc; copied++)\n\t\t\tunsafe_put_user(-1,\n\t\t\t\t\t&urelocs[copied].presumed_offset,\n\t\t\t\t\tend_user);\n\t\tuser_access_end();\n\n\t\teb->exec[i].relocs_ptr = (uintptr_t)relocs;\n\t}\n\n\treturn 0;\n\nend_user:\n\tuser_access_end();\nend:\n\tkvfree(relocs);\n\terr = -EFAULT;\nerr:\n\twhile (i--) {\n\t\trelocs = u64_to_ptr(typeof(*relocs), eb->exec[i].relocs_ptr);\n\t\tif (eb->exec[i].relocation_count)\n\t\t\tkvfree(relocs);\n\t}\n\treturn err;\n}\n\nstatic int eb_prefault_relocations(const struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tint err;\n\n\t\terr = check_relocations(&eb->exec[i]);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int eb_reinit_userptr(struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i;\n\tint ret;\n\n\tif (likely(!(eb->args->flags & __EXEC_USERPTR_USED)))\n\t\treturn 0;\n\n\tfor (i = 0; i < count; i++) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\n\t\tif (!i915_gem_object_is_userptr(ev->vma->obj))\n\t\t\tcontinue;\n\n\t\tret = i915_gem_object_userptr_submit_init(ev->vma->obj);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tev->flags |= __EXEC_OBJECT_USERPTR_INIT;\n\t}\n\n\treturn 0;\n}\n\nstatic noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)\n{\n\tbool have_copy = false;\n\tstruct eb_vma *ev;\n\tint err = 0;\n\nrepeat:\n\tif (signal_pending(current)) {\n\t\terr = -ERESTARTSYS;\n\t\tgoto out;\n\t}\n\n\t \n\teb_release_vmas(eb, false);\n\ti915_gem_ww_ctx_fini(&eb->ww);\n\n\t \n\tif (!err) {\n\t\terr = eb_prefault_relocations(eb);\n\t} else if (!have_copy) {\n\t\terr = eb_copy_relocations(eb);\n\t\thave_copy = err == 0;\n\t} else {\n\t\tcond_resched();\n\t\terr = 0;\n\t}\n\n\tif (!err)\n\t\terr = eb_reinit_userptr(eb);\n\n\ti915_gem_ww_ctx_init(&eb->ww, true);\n\tif (err)\n\t\tgoto out;\n\n\t \nrepeat_validate:\n\terr = eb_pin_engine(eb, false);\n\tif (err)\n\t\tgoto err;\n\n\terr = eb_validate_vmas(eb);\n\tif (err)\n\t\tgoto err;\n\n\tGEM_BUG_ON(!eb->batches[0]);\n\n\tlist_for_each_entry(ev, &eb->relocs, reloc_link) {\n\t\tif (!have_copy) {\n\t\t\terr = eb_relocate_vma(eb, ev);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\terr = eb_relocate_vma_slow(eb, ev);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (err == -EDEADLK)\n\t\tgoto err;\n\n\tif (err && !have_copy)\n\t\tgoto repeat;\n\n\tif (err)\n\t\tgoto err;\n\n\t \n\terr = eb_parse(eb);\n\tif (err)\n\t\tgoto err;\n\n\t \n\nerr:\n\tif (err == -EDEADLK) {\n\t\teb_release_vmas(eb, false);\n\t\terr = i915_gem_ww_ctx_backoff(&eb->ww);\n\t\tif (!err)\n\t\t\tgoto repeat_validate;\n\t}\n\n\tif (err == -EAGAIN)\n\t\tgoto repeat;\n\nout:\n\tif (have_copy) {\n\t\tconst unsigned int count = eb->buffer_count;\n\t\tunsigned int i;\n\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tconst struct drm_i915_gem_exec_object2 *entry =\n\t\t\t\t&eb->exec[i];\n\t\t\tstruct drm_i915_gem_relocation_entry *relocs;\n\n\t\t\tif (!entry->relocation_count)\n\t\t\t\tcontinue;\n\n\t\t\trelocs = u64_to_ptr(typeof(*relocs), entry->relocs_ptr);\n\t\t\tkvfree(relocs);\n\t\t}\n\t}\n\n\treturn err;\n}\n\nstatic int eb_relocate_parse(struct i915_execbuffer *eb)\n{\n\tint err;\n\tbool throttle = true;\n\nretry:\n\terr = eb_pin_engine(eb, throttle);\n\tif (err) {\n\t\tif (err != -EDEADLK)\n\t\t\treturn err;\n\n\t\tgoto err;\n\t}\n\n\t \n\tthrottle = false;\n\n\terr = eb_validate_vmas(eb);\n\tif (err == -EAGAIN)\n\t\tgoto slow;\n\telse if (err)\n\t\tgoto err;\n\n\t \n\tif (eb->args->flags & __EXEC_HAS_RELOC) {\n\t\tstruct eb_vma *ev;\n\n\t\tlist_for_each_entry(ev, &eb->relocs, reloc_link) {\n\t\t\terr = eb_relocate_vma(eb, ev);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (err == -EDEADLK)\n\t\t\tgoto err;\n\t\telse if (err)\n\t\t\tgoto slow;\n\t}\n\n\tif (!err)\n\t\terr = eb_parse(eb);\n\nerr:\n\tif (err == -EDEADLK) {\n\t\teb_release_vmas(eb, false);\n\t\terr = i915_gem_ww_ctx_backoff(&eb->ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\n\treturn err;\n\nslow:\n\terr = eb_relocate_parse_slow(eb);\n\tif (err)\n\t\t \n\t\teb->args->flags &= ~__EXEC_HAS_RELOC;\n\n\treturn err;\n}\n\n \n#define for_each_batch_create_order(_eb, _i) \\\n\tfor ((_i) = 0; (_i) < (_eb)->num_batches; ++(_i))\n#define for_each_batch_add_order(_eb, _i) \\\n\tBUILD_BUG_ON(!typecheck(int, _i)); \\\n\tfor ((_i) = (_eb)->num_batches - 1; (_i) >= 0; --(_i))\n\nstatic struct i915_request *\neb_find_first_request_added(struct i915_execbuffer *eb)\n{\n\tint i;\n\n\tfor_each_batch_add_order(eb, i)\n\t\tif (eb->requests[i])\n\t\t\treturn eb->requests[i];\n\n\tGEM_BUG_ON(\"Request not found\");\n\n\treturn NULL;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_CAPTURE_ERROR)\n\n \nstatic int eb_capture_stage(struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i = count, j;\n\n\twhile (i--) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tstruct i915_vma *vma = ev->vma;\n\t\tunsigned int flags = ev->flags;\n\n\t\tif (!(flags & EXEC_OBJECT_CAPTURE))\n\t\t\tcontinue;\n\n\t\tif (i915_gem_context_is_recoverable(eb->gem_context) &&\n\t\t    (IS_DGFX(eb->i915) || GRAPHICS_VER_FULL(eb->i915) > IP_VER(12, 0)))\n\t\t\treturn -EINVAL;\n\n\t\tfor_each_batch_create_order(eb, j) {\n\t\t\tstruct i915_capture_list *capture;\n\n\t\t\tcapture = kmalloc(sizeof(*capture), GFP_KERNEL);\n\t\t\tif (!capture)\n\t\t\t\tcontinue;\n\n\t\t\tcapture->next = eb->capture_lists[j];\n\t\t\tcapture->vma_res = i915_vma_resource_get(vma->resource);\n\t\t\teb->capture_lists[j] = capture;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void eb_capture_commit(struct i915_execbuffer *eb)\n{\n\tunsigned int j;\n\n\tfor_each_batch_create_order(eb, j) {\n\t\tstruct i915_request *rq = eb->requests[j];\n\n\t\tif (!rq)\n\t\t\tbreak;\n\n\t\trq->capture_list = eb->capture_lists[j];\n\t\teb->capture_lists[j] = NULL;\n\t}\n}\n\n \nstatic void eb_capture_release(struct i915_execbuffer *eb)\n{\n\tunsigned int j;\n\n\tfor_each_batch_create_order(eb, j) {\n\t\tif (eb->capture_lists[j]) {\n\t\t\ti915_request_free_capture_list(eb->capture_lists[j]);\n\t\t\teb->capture_lists[j] = NULL;\n\t\t}\n\t}\n}\n\nstatic void eb_capture_list_clear(struct i915_execbuffer *eb)\n{\n\tmemset(eb->capture_lists, 0, sizeof(eb->capture_lists));\n}\n\n#else\n\nstatic int eb_capture_stage(struct i915_execbuffer *eb)\n{\n\treturn 0;\n}\n\nstatic void eb_capture_commit(struct i915_execbuffer *eb)\n{\n}\n\nstatic void eb_capture_release(struct i915_execbuffer *eb)\n{\n}\n\nstatic void eb_capture_list_clear(struct i915_execbuffer *eb)\n{\n}\n\n#endif\n\nstatic int eb_move_to_gpu(struct i915_execbuffer *eb)\n{\n\tconst unsigned int count = eb->buffer_count;\n\tunsigned int i = count;\n\tint err = 0, j;\n\n\twhile (i--) {\n\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\tstruct i915_vma *vma = ev->vma;\n\t\tunsigned int flags = ev->flags;\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\t\tassert_vma_held(vma);\n\n\t\t \n\t\tif (unlikely(obj->cache_dirty & ~obj->cache_coherent)) {\n\t\t\tif (i915_gem_clflush_object(obj, 0))\n\t\t\t\tflags &= ~EXEC_OBJECT_ASYNC;\n\t\t}\n\n\t\t \n\t\tif (err == 0 && !(flags & EXEC_OBJECT_ASYNC)) {\n\t\t\terr = i915_request_await_object\n\t\t\t\t(eb_find_first_request_added(eb), obj,\n\t\t\t\t flags & EXEC_OBJECT_WRITE);\n\t\t}\n\n\t\tfor_each_batch_add_order(eb, j) {\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (!eb->requests[j])\n\t\t\t\tcontinue;\n\n\t\t\terr = _i915_vma_move_to_active(vma, eb->requests[j],\n\t\t\t\t\t\t       j ? NULL :\n\t\t\t\t\t\t       eb->composite_fence ?\n\t\t\t\t\t\t       eb->composite_fence :\n\t\t\t\t\t\t       &eb->requests[j]->fence,\n\t\t\t\t\t\t       flags | __EXEC_OBJECT_NO_RESERVE |\n\t\t\t\t\t\t       __EXEC_OBJECT_NO_REQUEST_AWAIT);\n\t\t}\n\t}\n\n#ifdef CONFIG_MMU_NOTIFIER\n\tif (!err && (eb->args->flags & __EXEC_USERPTR_USED)) {\n\t\tread_lock(&eb->i915->mm.notifier_lock);\n\n\t\t \n\t\tfor (i = 0; i < count; i++) {\n\t\t\tstruct eb_vma *ev = &eb->vma[i];\n\t\t\tstruct drm_i915_gem_object *obj = ev->vma->obj;\n\n\t\t\tif (!i915_gem_object_is_userptr(obj))\n\t\t\t\tcontinue;\n\n\t\t\terr = i915_gem_object_userptr_submit_done(obj);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tread_unlock(&eb->i915->mm.notifier_lock);\n\t}\n#endif\n\n\tif (unlikely(err))\n\t\tgoto err_skip;\n\n\t \n\tintel_gt_chipset_flush(eb->gt);\n\teb_capture_commit(eb);\n\n\treturn 0;\n\nerr_skip:\n\tfor_each_batch_create_order(eb, j) {\n\t\tif (!eb->requests[j])\n\t\t\tbreak;\n\n\t\ti915_request_set_error_once(eb->requests[j], err);\n\t}\n\treturn err;\n}\n\nstatic int i915_gem_check_execbuffer(struct drm_i915_private *i915,\n\t\t\t\t     struct drm_i915_gem_execbuffer2 *exec)\n{\n\tif (exec->flags & __I915_EXEC_ILLEGAL_FLAGS)\n\t\treturn -EINVAL;\n\n\t \n\tif (!(exec->flags & (I915_EXEC_FENCE_ARRAY |\n\t\t\t     I915_EXEC_USE_EXTENSIONS))) {\n\t\tif (exec->num_cliprects || exec->cliprects_ptr)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (exec->DR4 == 0xffffffff) {\n\t\tdrm_dbg(&i915->drm, \"UXA submitting garbage DR4, fixing up\\n\");\n\t\texec->DR4 = 0;\n\t}\n\tif (exec->DR1 || exec->DR4)\n\t\treturn -EINVAL;\n\n\tif ((exec->batch_start_offset | exec->batch_len) & 0x7)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int i915_reset_gen7_sol_offsets(struct i915_request *rq)\n{\n\tu32 *cs;\n\tint i;\n\n\tif (GRAPHICS_VER(rq->i915) != 7 || rq->engine->id != RCS0) {\n\t\tdrm_dbg(&rq->i915->drm, \"sol reset is gen7/rcs only\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcs = intel_ring_begin(rq, 4 * 2 + 2);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t*cs++ = MI_LOAD_REGISTER_IMM(4);\n\tfor (i = 0; i < 4; i++) {\n\t\t*cs++ = i915_mmio_reg_offset(GEN7_SO_WRITE_OFFSET(i));\n\t\t*cs++ = 0;\n\t}\n\t*cs++ = MI_NOOP;\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic struct i915_vma *\nshadow_batch_pin(struct i915_execbuffer *eb,\n\t\t struct drm_i915_gem_object *obj,\n\t\t struct i915_address_space *vm,\n\t\t unsigned int flags)\n{\n\tstruct i915_vma *vma;\n\tint err;\n\n\tvma = i915_vma_instance(obj, vm, NULL);\n\tif (IS_ERR(vma))\n\t\treturn vma;\n\n\terr = i915_vma_pin_ww(vma, &eb->ww, 0, 0, flags | PIN_VALIDATE);\n\tif (err)\n\t\treturn ERR_PTR(err);\n\n\treturn vma;\n}\n\nstatic struct i915_vma *eb_dispatch_secure(struct i915_execbuffer *eb, struct i915_vma *vma)\n{\n\t \n\tif (eb->batch_flags & I915_DISPATCH_SECURE)\n\t\treturn i915_gem_object_ggtt_pin_ww(vma->obj, &eb->ww, NULL, 0, 0, PIN_VALIDATE);\n\n\treturn NULL;\n}\n\nstatic int eb_parse(struct i915_execbuffer *eb)\n{\n\tstruct drm_i915_private *i915 = eb->i915;\n\tstruct intel_gt_buffer_pool_node *pool = eb->batch_pool;\n\tstruct i915_vma *shadow, *trampoline, *batch;\n\tunsigned long len;\n\tint err;\n\n\tif (!eb_use_cmdparser(eb)) {\n\t\tbatch = eb_dispatch_secure(eb, eb->batches[0]->vma);\n\t\tif (IS_ERR(batch))\n\t\t\treturn PTR_ERR(batch);\n\n\t\tgoto secure_batch;\n\t}\n\n\tif (intel_context_is_parallel(eb->context))\n\t\treturn -EINVAL;\n\n\tlen = eb->batch_len[0];\n\tif (!CMDPARSER_USES_GGTT(eb->i915)) {\n\t\t \n\t\tif (!eb->context->vm->has_read_only) {\n\t\t\tdrm_dbg(&i915->drm,\n\t\t\t\t\"Cannot prevent post-scan tampering without RO capable vm\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tlen += I915_CMD_PARSER_TRAMPOLINE_SIZE;\n\t}\n\tif (unlikely(len < eb->batch_len[0]))  \n\t\treturn -EINVAL;\n\n\tif (!pool) {\n\t\tpool = intel_gt_get_buffer_pool(eb->gt, len,\n\t\t\t\t\t\tI915_MAP_WB);\n\t\tif (IS_ERR(pool))\n\t\t\treturn PTR_ERR(pool);\n\t\teb->batch_pool = pool;\n\t}\n\n\terr = i915_gem_object_lock(pool->obj, &eb->ww);\n\tif (err)\n\t\treturn err;\n\n\tshadow = shadow_batch_pin(eb, pool->obj, eb->context->vm, PIN_USER);\n\tif (IS_ERR(shadow))\n\t\treturn PTR_ERR(shadow);\n\n\tintel_gt_buffer_pool_mark_used(pool);\n\ti915_gem_object_set_readonly(shadow->obj);\n\tshadow->private = pool;\n\n\ttrampoline = NULL;\n\tif (CMDPARSER_USES_GGTT(eb->i915)) {\n\t\ttrampoline = shadow;\n\n\t\tshadow = shadow_batch_pin(eb, pool->obj,\n\t\t\t\t\t  &eb->gt->ggtt->vm,\n\t\t\t\t\t  PIN_GLOBAL);\n\t\tif (IS_ERR(shadow))\n\t\t\treturn PTR_ERR(shadow);\n\n\t\tshadow->private = pool;\n\n\t\teb->batch_flags |= I915_DISPATCH_SECURE;\n\t}\n\n\tbatch = eb_dispatch_secure(eb, shadow);\n\tif (IS_ERR(batch))\n\t\treturn PTR_ERR(batch);\n\n\terr = dma_resv_reserve_fences(shadow->obj->base.resv, 1);\n\tif (err)\n\t\treturn err;\n\n\terr = intel_engine_cmd_parser(eb->context->engine,\n\t\t\t\t      eb->batches[0]->vma,\n\t\t\t\t      eb->batch_start_offset,\n\t\t\t\t      eb->batch_len[0],\n\t\t\t\t      shadow, trampoline);\n\tif (err)\n\t\treturn err;\n\n\teb->batches[0] = &eb->vma[eb->buffer_count++];\n\teb->batches[0]->vma = i915_vma_get(shadow);\n\teb->batches[0]->flags = __EXEC_OBJECT_HAS_PIN;\n\n\teb->trampoline = trampoline;\n\teb->batch_start_offset = 0;\n\nsecure_batch:\n\tif (batch) {\n\t\tif (intel_context_is_parallel(eb->context))\n\t\t\treturn -EINVAL;\n\n\t\teb->batches[0] = &eb->vma[eb->buffer_count++];\n\t\teb->batches[0]->flags = __EXEC_OBJECT_HAS_PIN;\n\t\teb->batches[0]->vma = i915_vma_get(batch);\n\t}\n\treturn 0;\n}\n\nstatic int eb_request_submit(struct i915_execbuffer *eb,\n\t\t\t     struct i915_request *rq,\n\t\t\t     struct i915_vma *batch,\n\t\t\t     u64 batch_len)\n{\n\tint err;\n\n\tif (intel_context_nopreempt(rq->context))\n\t\t__set_bit(I915_FENCE_FLAG_NOPREEMPT, &rq->fence.flags);\n\n\tif (eb->args->flags & I915_EXEC_GEN7_SOL_RESET) {\n\t\terr = i915_reset_gen7_sol_offsets(rq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tif (rq->context->engine->emit_init_breadcrumb) {\n\t\terr = rq->context->engine->emit_init_breadcrumb(rq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = rq->context->engine->emit_bb_start(rq,\n\t\t\t\t\t\t i915_vma_offset(batch) +\n\t\t\t\t\t\t eb->batch_start_offset,\n\t\t\t\t\t\t batch_len,\n\t\t\t\t\t\t eb->batch_flags);\n\tif (err)\n\t\treturn err;\n\n\tif (eb->trampoline) {\n\t\tGEM_BUG_ON(intel_context_is_parallel(rq->context));\n\t\tGEM_BUG_ON(eb->batch_start_offset);\n\t\terr = rq->context->engine->emit_bb_start(rq,\n\t\t\t\t\t\t\t i915_vma_offset(eb->trampoline) +\n\t\t\t\t\t\t\t batch_len, 0, 0);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int eb_submit(struct i915_execbuffer *eb)\n{\n\tunsigned int i;\n\tint err;\n\n\terr = eb_move_to_gpu(eb);\n\n\tfor_each_batch_create_order(eb, i) {\n\t\tif (!eb->requests[i])\n\t\t\tbreak;\n\n\t\ttrace_i915_request_queue(eb->requests[i], eb->batch_flags);\n\t\tif (!err)\n\t\t\terr = eb_request_submit(eb, eb->requests[i],\n\t\t\t\t\t\teb->batches[i]->vma,\n\t\t\t\t\t\teb->batch_len[i]);\n\t}\n\n\treturn err;\n}\n\n \nstatic unsigned int\ngen8_dispatch_bsd_engine(struct drm_i915_private *dev_priv,\n\t\t\t struct drm_file *file)\n{\n\tstruct drm_i915_file_private *file_priv = file->driver_priv;\n\n\t \n\tif ((int)file_priv->bsd_engine < 0)\n\t\tfile_priv->bsd_engine =\n\t\t\tget_random_u32_below(dev_priv->engine_uabi_class_count[I915_ENGINE_CLASS_VIDEO]);\n\n\treturn file_priv->bsd_engine;\n}\n\nstatic const enum intel_engine_id user_ring_map[] = {\n\t[I915_EXEC_DEFAULT]\t= RCS0,\n\t[I915_EXEC_RENDER]\t= RCS0,\n\t[I915_EXEC_BLT]\t\t= BCS0,\n\t[I915_EXEC_BSD]\t\t= VCS0,\n\t[I915_EXEC_VEBOX]\t= VECS0\n};\n\nstatic struct i915_request *eb_throttle(struct i915_execbuffer *eb, struct intel_context *ce)\n{\n\tstruct intel_ring *ring = ce->ring;\n\tstruct intel_timeline *tl = ce->timeline;\n\tstruct i915_request *rq;\n\n\t \n\tif (intel_ring_update_space(ring) >= PAGE_SIZE)\n\t\treturn NULL;\n\n\t \n\tlist_for_each_entry(rq, &tl->requests, link) {\n\t\tif (rq->ring != ring)\n\t\t\tcontinue;\n\n\t\tif (__intel_ring_space(rq->postfix,\n\t\t\t\t       ring->emit, ring->size) > ring->size / 2)\n\t\t\tbreak;\n\t}\n\tif (&rq->link == &tl->requests)\n\t\treturn NULL;  \n\n\treturn i915_request_get(rq);\n}\n\nstatic int eb_pin_timeline(struct i915_execbuffer *eb, struct intel_context *ce,\n\t\t\t   bool throttle)\n{\n\tstruct intel_timeline *tl;\n\tstruct i915_request *rq = NULL;\n\n\t \n\ttl = intel_context_timeline_lock(ce);\n\tif (IS_ERR(tl))\n\t\treturn PTR_ERR(tl);\n\n\tintel_context_enter(ce);\n\tif (throttle)\n\t\trq = eb_throttle(eb, ce);\n\tintel_context_timeline_unlock(tl);\n\n\tif (rq) {\n\t\tbool nonblock = eb->file->filp->f_flags & O_NONBLOCK;\n\t\tlong timeout = nonblock ? 0 : MAX_SCHEDULE_TIMEOUT;\n\n\t\tif (i915_request_wait(rq, I915_WAIT_INTERRUPTIBLE,\n\t\t\t\t      timeout) < 0) {\n\t\t\ti915_request_put(rq);\n\n\t\t\t \n\t\t\tmutex_lock(&ce->timeline->mutex);\n\t\t\tintel_context_exit(ce);\n\t\t\tmutex_unlock(&ce->timeline->mutex);\n\n\t\t\tif (nonblock)\n\t\t\t\treturn -EWOULDBLOCK;\n\t\t\telse\n\t\t\t\treturn -EINTR;\n\t\t}\n\t\ti915_request_put(rq);\n\t}\n\n\treturn 0;\n}\n\nstatic int eb_pin_engine(struct i915_execbuffer *eb, bool throttle)\n{\n\tstruct intel_context *ce = eb->context, *child;\n\tint err;\n\tint i = 0, j = 0;\n\n\tGEM_BUG_ON(eb->args->flags & __EXEC_ENGINE_PINNED);\n\n\tif (unlikely(intel_context_is_banned(ce)))\n\t\treturn -EIO;\n\n\t \n\terr = intel_context_pin_ww(ce, &eb->ww);\n\tif (err)\n\t\treturn err;\n\tfor_each_child(ce, child) {\n\t\terr = intel_context_pin_ww(child, &eb->ww);\n\t\tGEM_BUG_ON(err);\t \n\t}\n\n\tfor_each_child(ce, child) {\n\t\terr = eb_pin_timeline(eb, child, throttle);\n\t\tif (err)\n\t\t\tgoto unwind;\n\t\t++i;\n\t}\n\terr = eb_pin_timeline(eb, ce, throttle);\n\tif (err)\n\t\tgoto unwind;\n\n\teb->args->flags |= __EXEC_ENGINE_PINNED;\n\treturn 0;\n\nunwind:\n\tfor_each_child(ce, child) {\n\t\tif (j++ < i) {\n\t\t\tmutex_lock(&child->timeline->mutex);\n\t\t\tintel_context_exit(child);\n\t\t\tmutex_unlock(&child->timeline->mutex);\n\t\t}\n\t}\n\tfor_each_child(ce, child)\n\t\tintel_context_unpin(child);\n\tintel_context_unpin(ce);\n\treturn err;\n}\n\nstatic void eb_unpin_engine(struct i915_execbuffer *eb)\n{\n\tstruct intel_context *ce = eb->context, *child;\n\n\tif (!(eb->args->flags & __EXEC_ENGINE_PINNED))\n\t\treturn;\n\n\teb->args->flags &= ~__EXEC_ENGINE_PINNED;\n\n\tfor_each_child(ce, child) {\n\t\tmutex_lock(&child->timeline->mutex);\n\t\tintel_context_exit(child);\n\t\tmutex_unlock(&child->timeline->mutex);\n\n\t\tintel_context_unpin(child);\n\t}\n\n\tmutex_lock(&ce->timeline->mutex);\n\tintel_context_exit(ce);\n\tmutex_unlock(&ce->timeline->mutex);\n\n\tintel_context_unpin(ce);\n}\n\nstatic unsigned int\neb_select_legacy_ring(struct i915_execbuffer *eb)\n{\n\tstruct drm_i915_private *i915 = eb->i915;\n\tstruct drm_i915_gem_execbuffer2 *args = eb->args;\n\tunsigned int user_ring_id = args->flags & I915_EXEC_RING_MASK;\n\n\tif (user_ring_id != I915_EXEC_BSD &&\n\t    (args->flags & I915_EXEC_BSD_MASK)) {\n\t\tdrm_dbg(&i915->drm,\n\t\t\t\"execbuf with non bsd ring but with invalid \"\n\t\t\t\"bsd dispatch flags: %d\\n\", (int)(args->flags));\n\t\treturn -1;\n\t}\n\n\tif (user_ring_id == I915_EXEC_BSD &&\n\t    i915->engine_uabi_class_count[I915_ENGINE_CLASS_VIDEO] > 1) {\n\t\tunsigned int bsd_idx = args->flags & I915_EXEC_BSD_MASK;\n\n\t\tif (bsd_idx == I915_EXEC_BSD_DEFAULT) {\n\t\t\tbsd_idx = gen8_dispatch_bsd_engine(i915, eb->file);\n\t\t} else if (bsd_idx >= I915_EXEC_BSD_RING1 &&\n\t\t\t   bsd_idx <= I915_EXEC_BSD_RING2) {\n\t\t\tbsd_idx >>= I915_EXEC_BSD_SHIFT;\n\t\t\tbsd_idx--;\n\t\t} else {\n\t\t\tdrm_dbg(&i915->drm,\n\t\t\t\t\"execbuf with unknown bsd ring: %u\\n\",\n\t\t\t\tbsd_idx);\n\t\t\treturn -1;\n\t\t}\n\n\t\treturn _VCS(bsd_idx);\n\t}\n\n\tif (user_ring_id >= ARRAY_SIZE(user_ring_map)) {\n\t\tdrm_dbg(&i915->drm, \"execbuf with unknown ring: %u\\n\",\n\t\t\tuser_ring_id);\n\t\treturn -1;\n\t}\n\n\treturn user_ring_map[user_ring_id];\n}\n\nstatic int\neb_select_engine(struct i915_execbuffer *eb)\n{\n\tstruct intel_context *ce, *child;\n\tstruct intel_gt *gt;\n\tunsigned int idx;\n\tint err;\n\n\tif (i915_gem_context_user_engines(eb->gem_context))\n\t\tidx = eb->args->flags & I915_EXEC_RING_MASK;\n\telse\n\t\tidx = eb_select_legacy_ring(eb);\n\n\tce = i915_gem_context_get_engine(eb->gem_context, idx);\n\tif (IS_ERR(ce))\n\t\treturn PTR_ERR(ce);\n\n\tif (intel_context_is_parallel(ce)) {\n\t\tif (eb->buffer_count < ce->parallel.number_children + 1) {\n\t\t\tintel_context_put(ce);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (eb->batch_start_offset || eb->args->batch_len) {\n\t\t\tintel_context_put(ce);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\teb->num_batches = ce->parallel.number_children + 1;\n\tgt = ce->engine->gt;\n\n\tfor_each_child(ce, child)\n\t\tintel_context_get(child);\n\tintel_gt_pm_get(gt);\n\t \n\tif (gt->info.id)\n\t\tintel_gt_pm_get(to_gt(gt->i915));\n\n\tif (!test_bit(CONTEXT_ALLOC_BIT, &ce->flags)) {\n\t\terr = intel_context_alloc_state(ce);\n\t\tif (err)\n\t\t\tgoto err;\n\t}\n\tfor_each_child(ce, child) {\n\t\tif (!test_bit(CONTEXT_ALLOC_BIT, &child->flags)) {\n\t\t\terr = intel_context_alloc_state(child);\n\t\t\tif (err)\n\t\t\t\tgoto err;\n\t\t}\n\t}\n\n\t \n\terr = intel_gt_terminally_wedged(ce->engine->gt);\n\tif (err)\n\t\tgoto err;\n\n\tif (!i915_vm_tryget(ce->vm)) {\n\t\terr = -ENOENT;\n\t\tgoto err;\n\t}\n\n\teb->context = ce;\n\teb->gt = ce->engine->gt;\n\n\t \n\treturn err;\n\nerr:\n\tif (gt->info.id)\n\t\tintel_gt_pm_put(to_gt(gt->i915));\n\n\tintel_gt_pm_put(gt);\n\tfor_each_child(ce, child)\n\t\tintel_context_put(child);\n\tintel_context_put(ce);\n\treturn err;\n}\n\nstatic void\neb_put_engine(struct i915_execbuffer *eb)\n{\n\tstruct intel_context *child;\n\n\ti915_vm_put(eb->context->vm);\n\t \n\tif (eb->gt->info.id)\n\t\tintel_gt_pm_put(to_gt(eb->gt->i915));\n\tintel_gt_pm_put(eb->gt);\n\tfor_each_child(eb->context, child)\n\t\tintel_context_put(child);\n\tintel_context_put(eb->context);\n}\n\nstatic void\n__free_fence_array(struct eb_fence *fences, unsigned int n)\n{\n\twhile (n--) {\n\t\tdrm_syncobj_put(ptr_mask_bits(fences[n].syncobj, 2));\n\t\tdma_fence_put(fences[n].dma_fence);\n\t\tdma_fence_chain_free(fences[n].chain_fence);\n\t}\n\tkvfree(fences);\n}\n\nstatic int\nadd_timeline_fence_array(struct i915_execbuffer *eb,\n\t\t\t const struct drm_i915_gem_execbuffer_ext_timeline_fences *timeline_fences)\n{\n\tstruct drm_i915_gem_exec_fence __user *user_fences;\n\tu64 __user *user_values;\n\tstruct eb_fence *f;\n\tu64 nfences;\n\tint err = 0;\n\n\tnfences = timeline_fences->fence_count;\n\tif (!nfences)\n\t\treturn 0;\n\n\t \n\tBUILD_BUG_ON(sizeof(size_t) > sizeof(unsigned long));\n\tif (nfences > min_t(unsigned long,\n\t\t\t    ULONG_MAX / sizeof(*user_fences),\n\t\t\t    SIZE_MAX / sizeof(*f)) - eb->num_fences)\n\t\treturn -EINVAL;\n\n\tuser_fences = u64_to_user_ptr(timeline_fences->handles_ptr);\n\tif (!access_ok(user_fences, nfences * sizeof(*user_fences)))\n\t\treturn -EFAULT;\n\n\tuser_values = u64_to_user_ptr(timeline_fences->values_ptr);\n\tif (!access_ok(user_values, nfences * sizeof(*user_values)))\n\t\treturn -EFAULT;\n\n\tf = krealloc(eb->fences,\n\t\t     (eb->num_fences + nfences) * sizeof(*f),\n\t\t     __GFP_NOWARN | GFP_KERNEL);\n\tif (!f)\n\t\treturn -ENOMEM;\n\n\teb->fences = f;\n\tf += eb->num_fences;\n\n\tBUILD_BUG_ON(~(ARCH_KMALLOC_MINALIGN - 1) &\n\t\t     ~__I915_EXEC_FENCE_UNKNOWN_FLAGS);\n\n\twhile (nfences--) {\n\t\tstruct drm_i915_gem_exec_fence user_fence;\n\t\tstruct drm_syncobj *syncobj;\n\t\tstruct dma_fence *fence = NULL;\n\t\tu64 point;\n\n\t\tif (__copy_from_user(&user_fence,\n\t\t\t\t     user_fences++,\n\t\t\t\t     sizeof(user_fence)))\n\t\t\treturn -EFAULT;\n\n\t\tif (user_fence.flags & __I915_EXEC_FENCE_UNKNOWN_FLAGS)\n\t\t\treturn -EINVAL;\n\n\t\tif (__get_user(point, user_values++))\n\t\t\treturn -EFAULT;\n\n\t\tsyncobj = drm_syncobj_find(eb->file, user_fence.handle);\n\t\tif (!syncobj) {\n\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\"Invalid syncobj handle provided\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tfence = drm_syncobj_fence_get(syncobj);\n\n\t\tif (!fence && user_fence.flags &&\n\t\t    !(user_fence.flags & I915_EXEC_FENCE_SIGNAL)) {\n\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\"Syncobj handle has no fence\\n\");\n\t\t\tdrm_syncobj_put(syncobj);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (fence)\n\t\t\terr = dma_fence_chain_find_seqno(&fence, point);\n\n\t\tif (err && !(user_fence.flags & I915_EXEC_FENCE_SIGNAL)) {\n\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\"Syncobj handle missing requested point %llu\\n\",\n\t\t\t\tpoint);\n\t\t\tdma_fence_put(fence);\n\t\t\tdrm_syncobj_put(syncobj);\n\t\t\treturn err;\n\t\t}\n\n\t\t \n\t\tif (!fence && !(user_fence.flags & I915_EXEC_FENCE_SIGNAL)) {\n\t\t\tdrm_syncobj_put(syncobj);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (point != 0 && user_fence.flags & I915_EXEC_FENCE_SIGNAL) {\n\t\t\t \n\t\t\tif (user_fence.flags & I915_EXEC_FENCE_WAIT) {\n\t\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\t\"Trying to wait & signal the same timeline point.\\n\");\n\t\t\t\tdma_fence_put(fence);\n\t\t\t\tdrm_syncobj_put(syncobj);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tf->chain_fence = dma_fence_chain_alloc();\n\t\t\tif (!f->chain_fence) {\n\t\t\t\tdrm_syncobj_put(syncobj);\n\t\t\t\tdma_fence_put(fence);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t} else {\n\t\t\tf->chain_fence = NULL;\n\t\t}\n\n\t\tf->syncobj = ptr_pack_bits(syncobj, user_fence.flags, 2);\n\t\tf->dma_fence = fence;\n\t\tf->value = point;\n\t\tf++;\n\t\teb->num_fences++;\n\t}\n\n\treturn 0;\n}\n\nstatic int add_fence_array(struct i915_execbuffer *eb)\n{\n\tstruct drm_i915_gem_execbuffer2 *args = eb->args;\n\tstruct drm_i915_gem_exec_fence __user *user;\n\tunsigned long num_fences = args->num_cliprects;\n\tstruct eb_fence *f;\n\n\tif (!(args->flags & I915_EXEC_FENCE_ARRAY))\n\t\treturn 0;\n\n\tif (!num_fences)\n\t\treturn 0;\n\n\t \n\tBUILD_BUG_ON(sizeof(size_t) > sizeof(unsigned long));\n\tif (num_fences > min_t(unsigned long,\n\t\t\t       ULONG_MAX / sizeof(*user),\n\t\t\t       SIZE_MAX / sizeof(*f) - eb->num_fences))\n\t\treturn -EINVAL;\n\n\tuser = u64_to_user_ptr(args->cliprects_ptr);\n\tif (!access_ok(user, num_fences * sizeof(*user)))\n\t\treturn -EFAULT;\n\n\tf = krealloc(eb->fences,\n\t\t     (eb->num_fences + num_fences) * sizeof(*f),\n\t\t     __GFP_NOWARN | GFP_KERNEL);\n\tif (!f)\n\t\treturn -ENOMEM;\n\n\teb->fences = f;\n\tf += eb->num_fences;\n\twhile (num_fences--) {\n\t\tstruct drm_i915_gem_exec_fence user_fence;\n\t\tstruct drm_syncobj *syncobj;\n\t\tstruct dma_fence *fence = NULL;\n\n\t\tif (__copy_from_user(&user_fence, user++, sizeof(user_fence)))\n\t\t\treturn -EFAULT;\n\n\t\tif (user_fence.flags & __I915_EXEC_FENCE_UNKNOWN_FLAGS)\n\t\t\treturn -EINVAL;\n\n\t\tsyncobj = drm_syncobj_find(eb->file, user_fence.handle);\n\t\tif (!syncobj) {\n\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\"Invalid syncobj handle provided\\n\");\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tif (user_fence.flags & I915_EXEC_FENCE_WAIT) {\n\t\t\tfence = drm_syncobj_fence_get(syncobj);\n\t\t\tif (!fence) {\n\t\t\t\tdrm_dbg(&eb->i915->drm,\n\t\t\t\t\t\"Syncobj handle has no fence\\n\");\n\t\t\t\tdrm_syncobj_put(syncobj);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\t\tBUILD_BUG_ON(~(ARCH_KMALLOC_MINALIGN - 1) &\n\t\t\t     ~__I915_EXEC_FENCE_UNKNOWN_FLAGS);\n\n\t\tf->syncobj = ptr_pack_bits(syncobj, user_fence.flags, 2);\n\t\tf->dma_fence = fence;\n\t\tf->value = 0;\n\t\tf->chain_fence = NULL;\n\t\tf++;\n\t\teb->num_fences++;\n\t}\n\n\treturn 0;\n}\n\nstatic void put_fence_array(struct eb_fence *fences, int num_fences)\n{\n\tif (fences)\n\t\t__free_fence_array(fences, num_fences);\n}\n\nstatic int\nawait_fence_array(struct i915_execbuffer *eb,\n\t\t  struct i915_request *rq)\n{\n\tunsigned int n;\n\tint err;\n\n\tfor (n = 0; n < eb->num_fences; n++) {\n\t\tif (!eb->fences[n].dma_fence)\n\t\t\tcontinue;\n\n\t\terr = i915_request_await_dma_fence(rq, eb->fences[n].dma_fence);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void signal_fence_array(const struct i915_execbuffer *eb,\n\t\t\t       struct dma_fence * const fence)\n{\n\tunsigned int n;\n\n\tfor (n = 0; n < eb->num_fences; n++) {\n\t\tstruct drm_syncobj *syncobj;\n\t\tunsigned int flags;\n\n\t\tsyncobj = ptr_unpack_bits(eb->fences[n].syncobj, &flags, 2);\n\t\tif (!(flags & I915_EXEC_FENCE_SIGNAL))\n\t\t\tcontinue;\n\n\t\tif (eb->fences[n].chain_fence) {\n\t\t\tdrm_syncobj_add_point(syncobj,\n\t\t\t\t\t      eb->fences[n].chain_fence,\n\t\t\t\t\t      fence,\n\t\t\t\t\t      eb->fences[n].value);\n\t\t\t \n\t\t\teb->fences[n].chain_fence = NULL;\n\t\t} else {\n\t\t\tdrm_syncobj_replace_fence(syncobj, fence);\n\t\t}\n\t}\n}\n\nstatic int\nparse_timeline_fences(struct i915_user_extension __user *ext, void *data)\n{\n\tstruct i915_execbuffer *eb = data;\n\tstruct drm_i915_gem_execbuffer_ext_timeline_fences timeline_fences;\n\n\tif (copy_from_user(&timeline_fences, ext, sizeof(timeline_fences)))\n\t\treturn -EFAULT;\n\n\treturn add_timeline_fence_array(eb, &timeline_fences);\n}\n\nstatic void retire_requests(struct intel_timeline *tl, struct i915_request *end)\n{\n\tstruct i915_request *rq, *rn;\n\n\tlist_for_each_entry_safe(rq, rn, &tl->requests, link)\n\t\tif (rq == end || !i915_request_retire(rq))\n\t\t\tbreak;\n}\n\nstatic int eb_request_add(struct i915_execbuffer *eb, struct i915_request *rq,\n\t\t\t  int err, bool last_parallel)\n{\n\tstruct intel_timeline * const tl = i915_request_timeline(rq);\n\tstruct i915_sched_attr attr = {};\n\tstruct i915_request *prev;\n\n\tlockdep_assert_held(&tl->mutex);\n\tlockdep_unpin_lock(&tl->mutex, rq->cookie);\n\n\ttrace_i915_request_add(rq);\n\n\tprev = __i915_request_commit(rq);\n\n\t \n\tif (likely(!intel_context_is_closed(eb->context))) {\n\t\tattr = eb->gem_context->sched;\n\t} else {\n\t\t \n\t\ti915_request_set_error_once(rq, -ENOENT);\n\t\t__i915_request_skip(rq);\n\t\terr = -ENOENT;  \n\t}\n\n\tif (intel_context_is_parallel(eb->context)) {\n\t\tif (err) {\n\t\t\t__i915_request_skip(rq);\n\t\t\tset_bit(I915_FENCE_FLAG_SKIP_PARALLEL,\n\t\t\t\t&rq->fence.flags);\n\t\t}\n\t\tif (last_parallel)\n\t\t\tset_bit(I915_FENCE_FLAG_SUBMIT_PARALLEL,\n\t\t\t\t&rq->fence.flags);\n\t}\n\n\t__i915_request_queue(rq, &attr);\n\n\t \n\tif (prev)\n\t\tretire_requests(tl, prev);\n\n\tmutex_unlock(&tl->mutex);\n\n\treturn err;\n}\n\nstatic int eb_requests_add(struct i915_execbuffer *eb, int err)\n{\n\tint i;\n\n\t \n\tfor_each_batch_add_order(eb, i) {\n\t\tstruct i915_request *rq = eb->requests[i];\n\n\t\tif (!rq)\n\t\t\tcontinue;\n\t\terr |= eb_request_add(eb, rq, err, i == 0);\n\t}\n\n\treturn err;\n}\n\nstatic const i915_user_extension_fn execbuf_extensions[] = {\n\t[DRM_I915_GEM_EXECBUFFER_EXT_TIMELINE_FENCES] = parse_timeline_fences,\n};\n\nstatic int\nparse_execbuf2_extensions(struct drm_i915_gem_execbuffer2 *args,\n\t\t\t  struct i915_execbuffer *eb)\n{\n\tif (!(args->flags & I915_EXEC_USE_EXTENSIONS))\n\t\treturn 0;\n\n\t \n\tif (eb->args->flags & I915_EXEC_FENCE_ARRAY)\n\t\treturn -EINVAL;\n\n\tif (args->num_cliprects != 0)\n\t\treturn -EINVAL;\n\n\treturn i915_user_extensions(u64_to_user_ptr(args->cliprects_ptr),\n\t\t\t\t    execbuf_extensions,\n\t\t\t\t    ARRAY_SIZE(execbuf_extensions),\n\t\t\t\t    eb);\n}\n\nstatic void eb_requests_get(struct i915_execbuffer *eb)\n{\n\tunsigned int i;\n\n\tfor_each_batch_create_order(eb, i) {\n\t\tif (!eb->requests[i])\n\t\t\tbreak;\n\n\t\ti915_request_get(eb->requests[i]);\n\t}\n}\n\nstatic void eb_requests_put(struct i915_execbuffer *eb)\n{\n\tunsigned int i;\n\n\tfor_each_batch_create_order(eb, i) {\n\t\tif (!eb->requests[i])\n\t\t\tbreak;\n\n\t\ti915_request_put(eb->requests[i]);\n\t}\n}\n\nstatic struct sync_file *\neb_composite_fence_create(struct i915_execbuffer *eb, int out_fence_fd)\n{\n\tstruct sync_file *out_fence = NULL;\n\tstruct dma_fence_array *fence_array;\n\tstruct dma_fence **fences;\n\tunsigned int i;\n\n\tGEM_BUG_ON(!intel_context_is_parent(eb->context));\n\n\tfences = kmalloc_array(eb->num_batches, sizeof(*fences), GFP_KERNEL);\n\tif (!fences)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor_each_batch_create_order(eb, i) {\n\t\tfences[i] = &eb->requests[i]->fence;\n\t\t__set_bit(I915_FENCE_FLAG_COMPOSITE,\n\t\t\t  &eb->requests[i]->fence.flags);\n\t}\n\n\tfence_array = dma_fence_array_create(eb->num_batches,\n\t\t\t\t\t     fences,\n\t\t\t\t\t     eb->context->parallel.fence_context,\n\t\t\t\t\t     eb->context->parallel.seqno++,\n\t\t\t\t\t     false);\n\tif (!fence_array) {\n\t\tkfree(fences);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\tfor_each_batch_create_order(eb, i)\n\t\tdma_fence_get(fences[i]);\n\n\tif (out_fence_fd != -1) {\n\t\tout_fence = sync_file_create(&fence_array->base);\n\t\t \n\t\tdma_fence_put(&fence_array->base);\n\t\tif (!out_fence)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\teb->composite_fence = &fence_array->base;\n\n\treturn out_fence;\n}\n\nstatic struct sync_file *\neb_fences_add(struct i915_execbuffer *eb, struct i915_request *rq,\n\t      struct dma_fence *in_fence, int out_fence_fd)\n{\n\tstruct sync_file *out_fence = NULL;\n\tint err;\n\n\tif (unlikely(eb->gem_context->syncobj)) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = drm_syncobj_fence_get(eb->gem_context->syncobj);\n\t\terr = i915_request_await_dma_fence(rq, fence);\n\t\tdma_fence_put(fence);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\tif (in_fence) {\n\t\tif (eb->args->flags & I915_EXEC_FENCE_SUBMIT)\n\t\t\terr = i915_request_await_execution(rq, in_fence);\n\t\telse\n\t\t\terr = i915_request_await_dma_fence(rq, in_fence);\n\t\tif (err < 0)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\tif (eb->fences) {\n\t\terr = await_fence_array(eb, rq);\n\t\tif (err)\n\t\t\treturn ERR_PTR(err);\n\t}\n\n\tif (intel_context_is_parallel(eb->context)) {\n\t\tout_fence = eb_composite_fence_create(eb, out_fence_fd);\n\t\tif (IS_ERR(out_fence))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t} else if (out_fence_fd != -1) {\n\t\tout_fence = sync_file_create(&rq->fence);\n\t\tif (!out_fence)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\treturn out_fence;\n}\n\nstatic struct intel_context *\neb_find_context(struct i915_execbuffer *eb, unsigned int context_number)\n{\n\tstruct intel_context *child;\n\n\tif (likely(context_number == 0))\n\t\treturn eb->context;\n\n\tfor_each_child(eb->context, child)\n\t\tif (!--context_number)\n\t\t\treturn child;\n\n\tGEM_BUG_ON(\"Context not found\");\n\n\treturn NULL;\n}\n\nstatic struct sync_file *\neb_requests_create(struct i915_execbuffer *eb, struct dma_fence *in_fence,\n\t\t   int out_fence_fd)\n{\n\tstruct sync_file *out_fence = NULL;\n\tunsigned int i;\n\n\tfor_each_batch_create_order(eb, i) {\n\t\t \n\t\teb->requests[i] = i915_request_create(eb_find_context(eb, i));\n\t\tif (IS_ERR(eb->requests[i])) {\n\t\t\tout_fence = ERR_CAST(eb->requests[i]);\n\t\t\teb->requests[i] = NULL;\n\t\t\treturn out_fence;\n\t\t}\n\n\t\t \n\t\tif (i + 1 == eb->num_batches) {\n\t\t\tout_fence = eb_fences_add(eb, eb->requests[i],\n\t\t\t\t\t\t  in_fence, out_fence_fd);\n\t\t\tif (IS_ERR(out_fence))\n\t\t\t\treturn out_fence;\n\t\t}\n\n\t\t \n\t\tif (eb->batches[i]->vma)\n\t\t\teb->requests[i]->batch_res =\n\t\t\t\ti915_vma_resource_get(eb->batches[i]->vma->resource);\n\t\tif (eb->batch_pool) {\n\t\t\tGEM_BUG_ON(intel_context_is_parallel(eb->context));\n\t\t\tintel_gt_buffer_pool_mark_active(eb->batch_pool,\n\t\t\t\t\t\t\t eb->requests[i]);\n\t\t}\n\t}\n\n\treturn out_fence;\n}\n\nstatic int\ni915_gem_do_execbuffer(struct drm_device *dev,\n\t\t       struct drm_file *file,\n\t\t       struct drm_i915_gem_execbuffer2 *args,\n\t\t       struct drm_i915_gem_exec_object2 *exec)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct i915_execbuffer eb;\n\tstruct dma_fence *in_fence = NULL;\n\tstruct sync_file *out_fence = NULL;\n\tint out_fence_fd = -1;\n\tint err;\n\n\tBUILD_BUG_ON(__EXEC_INTERNAL_FLAGS & ~__I915_EXEC_ILLEGAL_FLAGS);\n\tBUILD_BUG_ON(__EXEC_OBJECT_INTERNAL_FLAGS &\n\t\t     ~__EXEC_OBJECT_UNKNOWN_FLAGS);\n\n\teb.i915 = i915;\n\teb.file = file;\n\teb.args = args;\n\tif (DBG_FORCE_RELOC || !(args->flags & I915_EXEC_NO_RELOC))\n\t\targs->flags |= __EXEC_HAS_RELOC;\n\n\teb.exec = exec;\n\teb.vma = (struct eb_vma *)(exec + args->buffer_count + 1);\n\teb.vma[0].vma = NULL;\n\teb.batch_pool = NULL;\n\n\teb.invalid_flags = __EXEC_OBJECT_UNKNOWN_FLAGS;\n\treloc_cache_init(&eb.reloc_cache, eb.i915);\n\n\teb.buffer_count = args->buffer_count;\n\teb.batch_start_offset = args->batch_start_offset;\n\teb.trampoline = NULL;\n\n\teb.fences = NULL;\n\teb.num_fences = 0;\n\n\teb_capture_list_clear(&eb);\n\n\tmemset(eb.requests, 0, sizeof(struct i915_request *) *\n\t       ARRAY_SIZE(eb.requests));\n\teb.composite_fence = NULL;\n\n\teb.batch_flags = 0;\n\tif (args->flags & I915_EXEC_SECURE) {\n\t\tif (GRAPHICS_VER(i915) >= 11)\n\t\t\treturn -ENODEV;\n\n\t\t \n\t\tif (!HAS_SECURE_BATCHES(i915))\n\t\t\treturn -EPERM;\n\n\t\tif (!drm_is_current_master(file) || !capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\n\t\teb.batch_flags |= I915_DISPATCH_SECURE;\n\t}\n\tif (args->flags & I915_EXEC_IS_PINNED)\n\t\teb.batch_flags |= I915_DISPATCH_PINNED;\n\n\terr = parse_execbuf2_extensions(args, &eb);\n\tif (err)\n\t\tgoto err_ext;\n\n\terr = add_fence_array(&eb);\n\tif (err)\n\t\tgoto err_ext;\n\n#define IN_FENCES (I915_EXEC_FENCE_IN | I915_EXEC_FENCE_SUBMIT)\n\tif (args->flags & IN_FENCES) {\n\t\tif ((args->flags & IN_FENCES) == IN_FENCES)\n\t\t\treturn -EINVAL;\n\n\t\tin_fence = sync_file_get_fence(lower_32_bits(args->rsvd2));\n\t\tif (!in_fence) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_ext;\n\t\t}\n\t}\n#undef IN_FENCES\n\n\tif (args->flags & I915_EXEC_FENCE_OUT) {\n\t\tout_fence_fd = get_unused_fd_flags(O_CLOEXEC);\n\t\tif (out_fence_fd < 0) {\n\t\t\terr = out_fence_fd;\n\t\t\tgoto err_in_fence;\n\t\t}\n\t}\n\n\terr = eb_create(&eb);\n\tif (err)\n\t\tgoto err_out_fence;\n\n\tGEM_BUG_ON(!eb.lut_size);\n\n\terr = eb_select_context(&eb);\n\tif (unlikely(err))\n\t\tgoto err_destroy;\n\n\terr = eb_select_engine(&eb);\n\tif (unlikely(err))\n\t\tgoto err_context;\n\n\terr = eb_lookup_vmas(&eb);\n\tif (err) {\n\t\teb_release_vmas(&eb, true);\n\t\tgoto err_engine;\n\t}\n\n\ti915_gem_ww_ctx_init(&eb.ww, true);\n\n\terr = eb_relocate_parse(&eb);\n\tif (err) {\n\t\t \n\t\targs->flags &= ~__EXEC_HAS_RELOC;\n\t\tgoto err_vma;\n\t}\n\n\tww_acquire_done(&eb.ww.ctx);\n\terr = eb_capture_stage(&eb);\n\tif (err)\n\t\tgoto err_vma;\n\n\tout_fence = eb_requests_create(&eb, in_fence, out_fence_fd);\n\tif (IS_ERR(out_fence)) {\n\t\terr = PTR_ERR(out_fence);\n\t\tout_fence = NULL;\n\t\tif (eb.requests[0])\n\t\t\tgoto err_request;\n\t\telse\n\t\t\tgoto err_vma;\n\t}\n\n\terr = eb_submit(&eb);\n\nerr_request:\n\teb_requests_get(&eb);\n\terr = eb_requests_add(&eb, err);\n\n\tif (eb.fences)\n\t\tsignal_fence_array(&eb, eb.composite_fence ?\n\t\t\t\t   eb.composite_fence :\n\t\t\t\t   &eb.requests[0]->fence);\n\n\tif (unlikely(eb.gem_context->syncobj)) {\n\t\tdrm_syncobj_replace_fence(eb.gem_context->syncobj,\n\t\t\t\t\t  eb.composite_fence ?\n\t\t\t\t\t  eb.composite_fence :\n\t\t\t\t\t  &eb.requests[0]->fence);\n\t}\n\n\tif (out_fence) {\n\t\tif (err == 0) {\n\t\t\tfd_install(out_fence_fd, out_fence->file);\n\t\t\targs->rsvd2 &= GENMASK_ULL(31, 0);  \n\t\t\targs->rsvd2 |= (u64)out_fence_fd << 32;\n\t\t\tout_fence_fd = -1;\n\t\t} else {\n\t\t\tfput(out_fence->file);\n\t\t}\n\t}\n\n\tif (!out_fence && eb.composite_fence)\n\t\tdma_fence_put(eb.composite_fence);\n\n\teb_requests_put(&eb);\n\nerr_vma:\n\teb_release_vmas(&eb, true);\n\tWARN_ON(err == -EDEADLK);\n\ti915_gem_ww_ctx_fini(&eb.ww);\n\n\tif (eb.batch_pool)\n\t\tintel_gt_buffer_pool_put(eb.batch_pool);\nerr_engine:\n\teb_put_engine(&eb);\nerr_context:\n\ti915_gem_context_put(eb.gem_context);\nerr_destroy:\n\teb_destroy(&eb);\nerr_out_fence:\n\tif (out_fence_fd != -1)\n\t\tput_unused_fd(out_fence_fd);\nerr_in_fence:\n\tdma_fence_put(in_fence);\nerr_ext:\n\tput_fence_array(eb.fences, eb.num_fences);\n\treturn err;\n}\n\nstatic size_t eb_element_size(void)\n{\n\treturn sizeof(struct drm_i915_gem_exec_object2) + sizeof(struct eb_vma);\n}\n\nstatic bool check_buffer_count(size_t count)\n{\n\tconst size_t sz = eb_element_size();\n\n\t \n\n\treturn !(count < 1 || count > INT_MAX || count > SIZE_MAX / sz - 1);\n}\n\nint\ni915_gem_execbuffer2_ioctl(struct drm_device *dev, void *data,\n\t\t\t   struct drm_file *file)\n{\n\tstruct drm_i915_private *i915 = to_i915(dev);\n\tstruct drm_i915_gem_execbuffer2 *args = data;\n\tstruct drm_i915_gem_exec_object2 *exec2_list;\n\tconst size_t count = args->buffer_count;\n\tint err;\n\n\tif (!check_buffer_count(count)) {\n\t\tdrm_dbg(&i915->drm, \"execbuf2 with %zd buffers\\n\", count);\n\t\treturn -EINVAL;\n\t}\n\n\terr = i915_gem_check_execbuffer(i915, args);\n\tif (err)\n\t\treturn err;\n\n\t \n\texec2_list = kvmalloc_array(count + 2, eb_element_size(),\n\t\t\t\t    __GFP_NOWARN | GFP_KERNEL);\n\tif (exec2_list == NULL) {\n\t\tdrm_dbg(&i915->drm, \"Failed to allocate exec list for %zd buffers\\n\",\n\t\t\tcount);\n\t\treturn -ENOMEM;\n\t}\n\tif (copy_from_user(exec2_list,\n\t\t\t   u64_to_user_ptr(args->buffers_ptr),\n\t\t\t   sizeof(*exec2_list) * count)) {\n\t\tdrm_dbg(&i915->drm, \"copy %zd exec entries failed\\n\", count);\n\t\tkvfree(exec2_list);\n\t\treturn -EFAULT;\n\t}\n\n\terr = i915_gem_do_execbuffer(dev, file, args, exec2_list);\n\n\t \n\tif (args->flags & __EXEC_HAS_RELOC) {\n\t\tstruct drm_i915_gem_exec_object2 __user *user_exec_list =\n\t\t\tu64_to_user_ptr(args->buffers_ptr);\n\t\tunsigned int i;\n\n\t\t \n\t\t \n\t\tif (!user_write_access_begin(user_exec_list,\n\t\t\t\t\t     count * sizeof(*user_exec_list)))\n\t\t\tgoto end;\n\n\t\tfor (i = 0; i < args->buffer_count; i++) {\n\t\t\tif (!(exec2_list[i].offset & UPDATE))\n\t\t\t\tcontinue;\n\n\t\t\texec2_list[i].offset =\n\t\t\t\tgen8_canonical_addr(exec2_list[i].offset & PIN_OFFSET_MASK);\n\t\t\tunsafe_put_user(exec2_list[i].offset,\n\t\t\t\t\t&user_exec_list[i].offset,\n\t\t\t\t\tend_user);\n\t\t}\nend_user:\n\t\tuser_write_access_end();\nend:;\n\t}\n\n\targs->flags &= ~__I915_EXEC_UNKNOWN_FLAGS;\n\tkvfree(exec2_list);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}