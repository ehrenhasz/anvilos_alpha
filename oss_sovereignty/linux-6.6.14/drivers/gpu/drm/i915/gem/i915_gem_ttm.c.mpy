{
  "module_name": "i915_gem_ttm.c",
  "hash_id": "94e5acda57ef0c60e63cd8486d82194f21eeb7fdea24255d9801a3352e13c1a2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_ttm.c",
  "human_readable_source": "\n \n\n#include <linux/shmem_fs.h>\n\n#include <drm/ttm/ttm_placement.h>\n#include <drm/ttm/ttm_tt.h>\n#include <drm/drm_buddy.h>\n\n#include \"i915_drv.h\"\n#include \"i915_ttm_buddy_manager.h\"\n#include \"intel_memory_region.h\"\n#include \"intel_region_ttm.h\"\n\n#include \"gem/i915_gem_mman.h\"\n#include \"gem/i915_gem_object.h\"\n#include \"gem/i915_gem_region.h\"\n#include \"gem/i915_gem_ttm.h\"\n#include \"gem/i915_gem_ttm_move.h\"\n#include \"gem/i915_gem_ttm_pm.h\"\n#include \"gt/intel_gpu_commands.h\"\n\n#define I915_TTM_PRIO_PURGE     0\n#define I915_TTM_PRIO_NO_PAGES  1\n#define I915_TTM_PRIO_HAS_PAGES 2\n#define I915_TTM_PRIO_NEEDS_CPU_ACCESS 3\n\n \n#define I915_TTM_MAX_PLACEMENTS INTEL_REGION_UNKNOWN\n\n \nstruct i915_ttm_tt {\n\tstruct ttm_tt ttm;\n\tstruct device *dev;\n\tstruct i915_refct_sgt cached_rsgt;\n\n\tbool is_shmem;\n\tstruct file *filp;\n};\n\nstatic const struct ttm_place sys_placement_flags = {\n\t.fpfn = 0,\n\t.lpfn = 0,\n\t.mem_type = I915_PL_SYSTEM,\n\t.flags = 0,\n};\n\nstatic struct ttm_placement i915_sys_placement = {\n\t.num_placement = 1,\n\t.placement = &sys_placement_flags,\n\t.num_busy_placement = 1,\n\t.busy_placement = &sys_placement_flags,\n};\n\n \nstruct ttm_placement *i915_ttm_sys_placement(void)\n{\n\treturn &i915_sys_placement;\n}\n\nstatic int i915_ttm_err_to_gem(int err)\n{\n\t \n\tif (likely(!err))\n\t\treturn 0;\n\n\tswitch (err) {\n\tcase -EBUSY:\n\t\t \n\t\treturn -EAGAIN;\n\tcase -ENOSPC:\n\t\t \n\t\treturn -ENXIO;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\nstatic enum ttm_caching\ni915_ttm_select_tt_caching(const struct drm_i915_gem_object *obj)\n{\n\t \n\tif (obj->mm.n_placements <= 1)\n\t\treturn ttm_cached;\n\n\treturn ttm_write_combined;\n}\n\nstatic void\ni915_ttm_place_from_region(const struct intel_memory_region *mr,\n\t\t\t   struct ttm_place *place,\n\t\t\t   resource_size_t offset,\n\t\t\t   resource_size_t size,\n\t\t\t   unsigned int flags)\n{\n\tmemset(place, 0, sizeof(*place));\n\tplace->mem_type = intel_region_to_ttm_type(mr);\n\n\tif (mr->type == INTEL_MEMORY_SYSTEM)\n\t\treturn;\n\n\tif (flags & I915_BO_ALLOC_CONTIGUOUS)\n\t\tplace->flags |= TTM_PL_FLAG_CONTIGUOUS;\n\tif (offset != I915_BO_INVALID_OFFSET) {\n\t\tWARN_ON(overflows_type(offset >> PAGE_SHIFT, place->fpfn));\n\t\tplace->fpfn = offset >> PAGE_SHIFT;\n\t\tWARN_ON(overflows_type(place->fpfn + (size >> PAGE_SHIFT), place->lpfn));\n\t\tplace->lpfn = place->fpfn + (size >> PAGE_SHIFT);\n\t} else if (mr->io_size && mr->io_size < mr->total) {\n\t\tif (flags & I915_BO_ALLOC_GPU_ONLY) {\n\t\t\tplace->flags |= TTM_PL_FLAG_TOPDOWN;\n\t\t} else {\n\t\t\tplace->fpfn = 0;\n\t\t\tWARN_ON(overflows_type(mr->io_size >> PAGE_SHIFT, place->lpfn));\n\t\t\tplace->lpfn = mr->io_size >> PAGE_SHIFT;\n\t\t}\n\t}\n}\n\nstatic void\ni915_ttm_placement_from_obj(const struct drm_i915_gem_object *obj,\n\t\t\t    struct ttm_place *requested,\n\t\t\t    struct ttm_place *busy,\n\t\t\t    struct ttm_placement *placement)\n{\n\tunsigned int num_allowed = obj->mm.n_placements;\n\tunsigned int flags = obj->flags;\n\tunsigned int i;\n\n\tplacement->num_placement = 1;\n\ti915_ttm_place_from_region(num_allowed ? obj->mm.placements[0] :\n\t\t\t\t   obj->mm.region, requested, obj->bo_offset,\n\t\t\t\t   obj->base.size, flags);\n\n\t \n\tplacement->num_busy_placement = num_allowed;\n\tfor (i = 0; i < placement->num_busy_placement; ++i)\n\t\ti915_ttm_place_from_region(obj->mm.placements[i], busy + i,\n\t\t\t\t\t   obj->bo_offset, obj->base.size, flags);\n\n\tif (num_allowed == 0) {\n\t\t*busy = *requested;\n\t\tplacement->num_busy_placement = 1;\n\t}\n\n\tplacement->placement = requested;\n\tplacement->busy_placement = busy;\n}\n\nstatic int i915_ttm_tt_shmem_populate(struct ttm_device *bdev,\n\t\t\t\t      struct ttm_tt *ttm,\n\t\t\t\t      struct ttm_operation_ctx *ctx)\n{\n\tstruct drm_i915_private *i915 = container_of(bdev, typeof(*i915), bdev);\n\tstruct intel_memory_region *mr = i915->mm.regions[INTEL_MEMORY_SYSTEM];\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\tconst unsigned int max_segment = i915_sg_segment_size(i915->drm.dev);\n\tconst size_t size = (size_t)ttm->num_pages << PAGE_SHIFT;\n\tstruct file *filp = i915_tt->filp;\n\tstruct sgt_iter sgt_iter;\n\tstruct sg_table *st;\n\tstruct page *page;\n\tunsigned long i;\n\tint err;\n\n\tif (!filp) {\n\t\tstruct address_space *mapping;\n\t\tgfp_t mask;\n\n\t\tfilp = shmem_file_setup(\"i915-shmem-tt\", size, VM_NORESERVE);\n\t\tif (IS_ERR(filp))\n\t\t\treturn PTR_ERR(filp);\n\n\t\tmask = GFP_HIGHUSER | __GFP_RECLAIMABLE;\n\n\t\tmapping = filp->f_mapping;\n\t\tmapping_set_gfp_mask(mapping, mask);\n\t\tGEM_BUG_ON(!(mapping_gfp_mask(mapping) & __GFP_RECLAIM));\n\n\t\ti915_tt->filp = filp;\n\t}\n\n\tst = &i915_tt->cached_rsgt.table;\n\terr = shmem_sg_alloc_table(i915, st, size, mr, filp->f_mapping,\n\t\t\t\t   max_segment);\n\tif (err)\n\t\treturn err;\n\n\terr = dma_map_sgtable(i915_tt->dev, st, DMA_BIDIRECTIONAL,\n\t\t\t      DMA_ATTR_SKIP_CPU_SYNC);\n\tif (err)\n\t\tgoto err_free_st;\n\n\ti = 0;\n\tfor_each_sgt_page(page, sgt_iter, st)\n\t\tttm->pages[i++] = page;\n\n\tif (ttm->page_flags & TTM_TT_FLAG_SWAPPED)\n\t\tttm->page_flags &= ~TTM_TT_FLAG_SWAPPED;\n\n\treturn 0;\n\nerr_free_st:\n\tshmem_sg_free_table(st, filp->f_mapping, false, false);\n\n\treturn err;\n}\n\nstatic void i915_ttm_tt_shmem_unpopulate(struct ttm_tt *ttm)\n{\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\tbool backup = ttm->page_flags & TTM_TT_FLAG_SWAPPED;\n\tstruct sg_table *st = &i915_tt->cached_rsgt.table;\n\n\tshmem_sg_free_table(st, file_inode(i915_tt->filp)->i_mapping,\n\t\t\t    backup, backup);\n}\n\nstatic void i915_ttm_tt_release(struct kref *ref)\n{\n\tstruct i915_ttm_tt *i915_tt =\n\t\tcontainer_of(ref, typeof(*i915_tt), cached_rsgt.kref);\n\tstruct sg_table *st = &i915_tt->cached_rsgt.table;\n\n\tGEM_WARN_ON(st->sgl);\n\n\tkfree(i915_tt);\n}\n\nstatic const struct i915_refct_sgt_ops tt_rsgt_ops = {\n\t.release = i915_ttm_tt_release\n};\n\nstatic struct ttm_tt *i915_ttm_tt_create(struct ttm_buffer_object *bo,\n\t\t\t\t\t uint32_t page_flags)\n{\n\tstruct drm_i915_private *i915 = container_of(bo->bdev, typeof(*i915),\n\t\t\t\t\t\t     bdev);\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tunsigned long ccs_pages = 0;\n\tenum ttm_caching caching;\n\tstruct i915_ttm_tt *i915_tt;\n\tint ret;\n\n\tif (i915_ttm_is_ghost_object(bo))\n\t\treturn NULL;\n\n\ti915_tt = kzalloc(sizeof(*i915_tt), GFP_KERNEL);\n\tif (!i915_tt)\n\t\treturn NULL;\n\n\tif (obj->flags & I915_BO_ALLOC_CPU_CLEAR && (!bo->resource ||\n\t    ttm_manager_type(bo->bdev, bo->resource->mem_type)->use_tt))\n\t\tpage_flags |= TTM_TT_FLAG_ZERO_ALLOC;\n\n\tcaching = i915_ttm_select_tt_caching(obj);\n\tif (i915_gem_object_is_shrinkable(obj) && caching == ttm_cached) {\n\t\tpage_flags |= TTM_TT_FLAG_EXTERNAL |\n\t\t\t      TTM_TT_FLAG_EXTERNAL_MAPPABLE;\n\t\ti915_tt->is_shmem = true;\n\t}\n\n\tif (i915_gem_object_needs_ccs_pages(obj))\n\t\tccs_pages = DIV_ROUND_UP(DIV_ROUND_UP(bo->base.size,\n\t\t\t\t\t\t      NUM_BYTES_PER_CCS_BYTE),\n\t\t\t\t\t PAGE_SIZE);\n\n\tret = ttm_tt_init(&i915_tt->ttm, bo, page_flags, caching, ccs_pages);\n\tif (ret)\n\t\tgoto err_free;\n\n\t__i915_refct_sgt_init(&i915_tt->cached_rsgt, bo->base.size,\n\t\t\t      &tt_rsgt_ops);\n\n\ti915_tt->dev = obj->base.dev->dev;\n\n\treturn &i915_tt->ttm;\n\nerr_free:\n\tkfree(i915_tt);\n\treturn NULL;\n}\n\nstatic int i915_ttm_tt_populate(struct ttm_device *bdev,\n\t\t\t\tstruct ttm_tt *ttm,\n\t\t\t\tstruct ttm_operation_ctx *ctx)\n{\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\n\tif (i915_tt->is_shmem)\n\t\treturn i915_ttm_tt_shmem_populate(bdev, ttm, ctx);\n\n\treturn ttm_pool_alloc(&bdev->pool, ttm, ctx);\n}\n\nstatic void i915_ttm_tt_unpopulate(struct ttm_device *bdev, struct ttm_tt *ttm)\n{\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\tstruct sg_table *st = &i915_tt->cached_rsgt.table;\n\n\tif (st->sgl)\n\t\tdma_unmap_sgtable(i915_tt->dev, st, DMA_BIDIRECTIONAL, 0);\n\n\tif (i915_tt->is_shmem) {\n\t\ti915_ttm_tt_shmem_unpopulate(ttm);\n\t} else {\n\t\tsg_free_table(st);\n\t\tttm_pool_free(&bdev->pool, ttm);\n\t}\n}\n\nstatic void i915_ttm_tt_destroy(struct ttm_device *bdev, struct ttm_tt *ttm)\n{\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\n\tif (i915_tt->filp)\n\t\tfput(i915_tt->filp);\n\n\tttm_tt_fini(ttm);\n\ti915_refct_sgt_put(&i915_tt->cached_rsgt);\n}\n\nstatic bool i915_ttm_eviction_valuable(struct ttm_buffer_object *bo,\n\t\t\t\t       const struct ttm_place *place)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\n\tif (i915_ttm_is_ghost_object(bo))\n\t\treturn false;\n\n\t \n\tif (bo->ttm && bo->ttm->page_flags & TTM_TT_FLAG_EXTERNAL)\n\t\treturn false;\n\n\t \n\tif (!i915_gem_object_evictable(obj))\n\t\treturn false;\n\n\treturn ttm_bo_eviction_valuable(bo, place);\n}\n\nstatic void i915_ttm_evict_flags(struct ttm_buffer_object *bo,\n\t\t\t\t struct ttm_placement *placement)\n{\n\t*placement = i915_sys_placement;\n}\n\n \nvoid i915_ttm_free_cached_io_rsgt(struct drm_i915_gem_object *obj)\n{\n\tstruct radix_tree_iter iter;\n\tvoid __rcu **slot;\n\n\tif (!obj->ttm.cached_io_rsgt)\n\t\treturn;\n\n\trcu_read_lock();\n\tradix_tree_for_each_slot(slot, &obj->ttm.get_io_page.radix, &iter, 0)\n\t\tradix_tree_delete(&obj->ttm.get_io_page.radix, iter.index);\n\trcu_read_unlock();\n\n\ti915_refct_sgt_put(obj->ttm.cached_io_rsgt);\n\tobj->ttm.cached_io_rsgt = NULL;\n}\n\n \nint i915_ttm_purge(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tstruct i915_ttm_tt *i915_tt =\n\t\tcontainer_of(bo->ttm, typeof(*i915_tt), ttm);\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t};\n\tstruct ttm_placement place = {};\n\tint ret;\n\n\tif (obj->mm.madv == __I915_MADV_PURGED)\n\t\treturn 0;\n\n\tret = ttm_bo_validate(bo, &place, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tif (bo->ttm && i915_tt->filp) {\n\t\t \n\t\tshmem_truncate_range(file_inode(i915_tt->filp),\n\t\t\t\t     0, (loff_t)-1);\n\t\tfput(fetch_and_zero(&i915_tt->filp));\n\t}\n\n\tobj->write_domain = 0;\n\tobj->read_domains = 0;\n\ti915_ttm_adjust_gem_after_move(obj);\n\ti915_ttm_free_cached_io_rsgt(obj);\n\tobj->mm.madv = __I915_MADV_PURGED;\n\n\treturn 0;\n}\n\nstatic int i915_ttm_shrink(struct drm_i915_gem_object *obj, unsigned int flags)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tstruct i915_ttm_tt *i915_tt =\n\t\tcontainer_of(bo->ttm, typeof(*i915_tt), ttm);\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = flags & I915_GEM_OBJECT_SHRINK_NO_GPU_WAIT,\n\t};\n\tstruct ttm_placement place = {};\n\tint ret;\n\n\tif (!bo->ttm || i915_ttm_cpu_maps_iomem(bo->resource))\n\t\treturn 0;\n\n\tGEM_BUG_ON(!i915_tt->is_shmem);\n\n\tif (!i915_tt->filp)\n\t\treturn 0;\n\n\tret = ttm_bo_wait_ctx(bo, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (obj->mm.madv) {\n\tcase I915_MADV_DONTNEED:\n\t\treturn i915_ttm_purge(obj);\n\tcase __I915_MADV_PURGED:\n\t\treturn 0;\n\t}\n\n\tif (bo->ttm->page_flags & TTM_TT_FLAG_SWAPPED)\n\t\treturn 0;\n\n\tbo->ttm->page_flags |= TTM_TT_FLAG_SWAPPED;\n\tret = ttm_bo_validate(bo, &place, &ctx);\n\tif (ret) {\n\t\tbo->ttm->page_flags &= ~TTM_TT_FLAG_SWAPPED;\n\t\treturn ret;\n\t}\n\n\tif (flags & I915_GEM_OBJECT_SHRINK_WRITEBACK)\n\t\t__shmem_writeback(obj->base.size, i915_tt->filp->f_mapping);\n\n\treturn 0;\n}\n\nstatic void i915_ttm_delete_mem_notify(struct ttm_buffer_object *bo)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\n\t \n\tif ((bo->resource || bo->ttm) && !i915_ttm_is_ghost_object(bo)) {\n\t\t__i915_gem_object_pages_fini(obj);\n\t\ti915_ttm_free_cached_io_rsgt(obj);\n\t}\n}\n\nstatic struct i915_refct_sgt *i915_ttm_tt_get_st(struct ttm_tt *ttm)\n{\n\tstruct i915_ttm_tt *i915_tt = container_of(ttm, typeof(*i915_tt), ttm);\n\tstruct sg_table *st;\n\tint ret;\n\n\tif (i915_tt->cached_rsgt.table.sgl)\n\t\treturn i915_refct_sgt_get(&i915_tt->cached_rsgt);\n\n\tst = &i915_tt->cached_rsgt.table;\n\tret = sg_alloc_table_from_pages_segment(st,\n\t\t\tttm->pages, ttm->num_pages,\n\t\t\t0, (unsigned long)ttm->num_pages << PAGE_SHIFT,\n\t\t\ti915_sg_segment_size(i915_tt->dev), GFP_KERNEL);\n\tif (ret) {\n\t\tst->sgl = NULL;\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = dma_map_sgtable(i915_tt->dev, st, DMA_BIDIRECTIONAL, 0);\n\tif (ret) {\n\t\tsg_free_table(st);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn i915_refct_sgt_get(&i915_tt->cached_rsgt);\n}\n\n \nstruct i915_refct_sgt *\ni915_ttm_resource_get_st(struct drm_i915_gem_object *obj,\n\t\t\t struct ttm_resource *res)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tu32 page_alignment;\n\n\tif (!i915_ttm_gtt_binds_lmem(res))\n\t\treturn i915_ttm_tt_get_st(bo->ttm);\n\n\tpage_alignment = bo->page_alignment << PAGE_SHIFT;\n\tif (!page_alignment)\n\t\tpage_alignment = obj->mm.region->min_page_size;\n\n\t \n\tGEM_WARN_ON(!i915_ttm_cpu_maps_iomem(res));\n\tif (bo->resource == res) {\n\t\tif (!obj->ttm.cached_io_rsgt) {\n\t\t\tstruct i915_refct_sgt *rsgt;\n\n\t\t\trsgt = intel_region_ttm_resource_to_rsgt(obj->mm.region,\n\t\t\t\t\t\t\t\t res,\n\t\t\t\t\t\t\t\t page_alignment);\n\t\t\tif (IS_ERR(rsgt))\n\t\t\t\treturn rsgt;\n\n\t\t\tobj->ttm.cached_io_rsgt = rsgt;\n\t\t}\n\t\treturn i915_refct_sgt_get(obj->ttm.cached_io_rsgt);\n\t}\n\n\treturn intel_region_ttm_resource_to_rsgt(obj->mm.region, res,\n\t\t\t\t\t\t page_alignment);\n}\n\nstatic int i915_ttm_truncate(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tlong err;\n\n\tWARN_ON_ONCE(obj->mm.madv == I915_MADV_WILLNEED);\n\n\terr = dma_resv_wait_timeout(bo->base.resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t    true, 15 * HZ);\n\tif (err < 0)\n\t\treturn err;\n\tif (err == 0)\n\t\treturn -EBUSY;\n\n\terr = i915_ttm_move_notify(bo);\n\tif (err)\n\t\treturn err;\n\n\treturn i915_ttm_purge(obj);\n}\n\nstatic void i915_ttm_swap_notify(struct ttm_buffer_object *bo)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tint ret;\n\n\tif (i915_ttm_is_ghost_object(bo))\n\t\treturn;\n\n\tret = i915_ttm_move_notify(bo);\n\tGEM_WARN_ON(ret);\n\tGEM_WARN_ON(obj->ttm.cached_io_rsgt);\n\tif (!ret && obj->mm.madv != I915_MADV_WILLNEED)\n\t\ti915_ttm_purge(obj);\n}\n\n \nbool i915_ttm_resource_mappable(struct ttm_resource *res)\n{\n\tstruct i915_ttm_buddy_resource *bman_res = to_ttm_buddy_resource(res);\n\n\tif (!i915_ttm_cpu_maps_iomem(res))\n\t\treturn true;\n\n\treturn bman_res->used_visible_size == PFN_UP(bman_res->base.size);\n}\n\nstatic int i915_ttm_io_mem_reserve(struct ttm_device *bdev, struct ttm_resource *mem)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(mem->bo);\n\tbool unknown_state;\n\n\tif (i915_ttm_is_ghost_object(mem->bo))\n\t\treturn -EINVAL;\n\n\tif (!kref_get_unless_zero(&obj->base.refcount))\n\t\treturn -EINVAL;\n\n\tassert_object_held(obj);\n\n\tunknown_state = i915_gem_object_has_unknown_state(obj);\n\ti915_gem_object_put(obj);\n\tif (unknown_state)\n\t\treturn -EINVAL;\n\n\tif (!i915_ttm_cpu_maps_iomem(mem))\n\t\treturn 0;\n\n\tif (!i915_ttm_resource_mappable(mem))\n\t\treturn -EINVAL;\n\n\tmem->bus.caching = ttm_write_combined;\n\tmem->bus.is_iomem = true;\n\n\treturn 0;\n}\n\nstatic unsigned long i915_ttm_io_mem_pfn(struct ttm_buffer_object *bo,\n\t\t\t\t\t unsigned long page_offset)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tstruct scatterlist *sg;\n\tunsigned long base;\n\tunsigned int ofs;\n\n\tGEM_BUG_ON(i915_ttm_is_ghost_object(bo));\n\tGEM_WARN_ON(bo->ttm);\n\n\tbase = obj->mm.region->iomap.base - obj->mm.region->region.start;\n\tsg = i915_gem_object_page_iter_get_sg(obj, &obj->ttm.get_io_page, page_offset, &ofs);\n\n\treturn ((base + sg_dma_address(sg)) >> PAGE_SHIFT) + ofs;\n}\n\nstatic int i915_ttm_access_memory(struct ttm_buffer_object *bo,\n\t\t\t\t  unsigned long offset, void *buf,\n\t\t\t\t  int len, int write)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tresource_size_t iomap = obj->mm.region->iomap.base -\n\t\tobj->mm.region->region.start;\n\tunsigned long page = offset >> PAGE_SHIFT;\n\tunsigned long bytes_left = len;\n\n\t \n\tif (!i915_ttm_resource_mappable(bo->resource))\n\t\treturn -EIO;\n\n\toffset -= page << PAGE_SHIFT;\n\tdo {\n\t\tunsigned long bytes = min(bytes_left, PAGE_SIZE - offset);\n\t\tvoid __iomem *ptr;\n\t\tdma_addr_t daddr;\n\n\t\tdaddr = i915_gem_object_get_dma_address(obj, page);\n\t\tptr = ioremap_wc(iomap + daddr + offset, bytes);\n\t\tif (!ptr)\n\t\t\treturn -EIO;\n\n\t\tif (write)\n\t\t\tmemcpy_toio(ptr, buf, bytes);\n\t\telse\n\t\t\tmemcpy_fromio(buf, ptr, bytes);\n\t\tiounmap(ptr);\n\n\t\tpage++;\n\t\tbuf += bytes;\n\t\tbytes_left -= bytes;\n\t\toffset = 0;\n\t} while (bytes_left);\n\n\treturn len;\n}\n\n \nstatic struct ttm_device_funcs i915_ttm_bo_driver = {\n\t.ttm_tt_create = i915_ttm_tt_create,\n\t.ttm_tt_populate = i915_ttm_tt_populate,\n\t.ttm_tt_unpopulate = i915_ttm_tt_unpopulate,\n\t.ttm_tt_destroy = i915_ttm_tt_destroy,\n\t.eviction_valuable = i915_ttm_eviction_valuable,\n\t.evict_flags = i915_ttm_evict_flags,\n\t.move = i915_ttm_move,\n\t.swap_notify = i915_ttm_swap_notify,\n\t.delete_mem_notify = i915_ttm_delete_mem_notify,\n\t.io_mem_reserve = i915_ttm_io_mem_reserve,\n\t.io_mem_pfn = i915_ttm_io_mem_pfn,\n\t.access_memory = i915_ttm_access_memory,\n};\n\n \nstruct ttm_device_funcs *i915_ttm_driver(void)\n{\n\treturn &i915_ttm_bo_driver;\n}\n\nstatic int __i915_ttm_get_pages(struct drm_i915_gem_object *obj,\n\t\t\t\tstruct ttm_placement *placement)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t};\n\tint real_num_busy;\n\tint ret;\n\n\t \n\treal_num_busy = fetch_and_zero(&placement->num_busy_placement);\n\tret = ttm_bo_validate(bo, placement, &ctx);\n\tif (ret) {\n\t\tret = i915_ttm_err_to_gem(ret);\n\t\t \n\t\tif (ret == -EDEADLK || ret == -EINTR || ret == -ERESTARTSYS ||\n\t\t    ret == -EAGAIN)\n\t\t\treturn ret;\n\n\t\t \n\t\tplacement->num_busy_placement = real_num_busy;\n\t\tret = ttm_bo_validate(bo, placement, &ctx);\n\t\tif (ret)\n\t\t\treturn i915_ttm_err_to_gem(ret);\n\t}\n\n\tif (bo->ttm && !ttm_tt_is_populated(bo->ttm)) {\n\t\tret = ttm_tt_populate(bo->bdev, bo->ttm, &ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\ti915_ttm_adjust_domains_after_move(obj);\n\t\ti915_ttm_adjust_gem_after_move(obj);\n\t}\n\n\tif (!i915_gem_object_has_pages(obj)) {\n\t\tstruct i915_refct_sgt *rsgt =\n\t\t\ti915_ttm_resource_get_st(obj, bo->resource);\n\n\t\tif (IS_ERR(rsgt))\n\t\t\treturn PTR_ERR(rsgt);\n\n\t\tGEM_BUG_ON(obj->mm.rsgt);\n\t\tobj->mm.rsgt = rsgt;\n\t\t__i915_gem_object_set_pages(obj, &rsgt->table);\n\t}\n\n\tGEM_BUG_ON(bo->ttm && ((obj->base.size >> PAGE_SHIFT) < bo->ttm->num_pages));\n\ti915_ttm_adjust_lru(obj);\n\treturn ret;\n}\n\nstatic int i915_ttm_get_pages(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_place requested, busy[I915_TTM_MAX_PLACEMENTS];\n\tstruct ttm_placement placement;\n\n\t \n\tif (overflows_type(obj->base.size >> PAGE_SHIFT, unsigned int))\n\t\treturn -E2BIG;\n\n\tGEM_BUG_ON(obj->mm.n_placements > I915_TTM_MAX_PLACEMENTS);\n\n\t \n\ti915_ttm_placement_from_obj(obj, &requested, busy, &placement);\n\n\treturn __i915_ttm_get_pages(obj, &placement);\n}\n\n \nstatic int __i915_ttm_migrate(struct drm_i915_gem_object *obj,\n\t\t\t      struct intel_memory_region *mr,\n\t\t\t      unsigned int flags)\n{\n\tstruct ttm_place requested;\n\tstruct ttm_placement placement;\n\tint ret;\n\n\ti915_ttm_place_from_region(mr, &requested, obj->bo_offset,\n\t\t\t\t   obj->base.size, flags);\n\tplacement.num_placement = 1;\n\tplacement.num_busy_placement = 1;\n\tplacement.placement = &requested;\n\tplacement.busy_placement = &requested;\n\n\tret = __i915_ttm_get_pages(obj, &placement);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (obj->mm.region != mr) {\n\t\ti915_gem_object_release_memory_region(obj);\n\t\ti915_gem_object_init_memory_region(obj, mr);\n\t}\n\n\treturn 0;\n}\n\nstatic int i915_ttm_migrate(struct drm_i915_gem_object *obj,\n\t\t\t    struct intel_memory_region *mr,\n\t\t\t    unsigned int flags)\n{\n\treturn __i915_ttm_migrate(obj, mr, flags);\n}\n\nstatic void i915_ttm_put_pages(struct drm_i915_gem_object *obj,\n\t\t\t       struct sg_table *st)\n{\n\t \n\n\tif (obj->mm.rsgt)\n\t\ti915_refct_sgt_put(fetch_and_zero(&obj->mm.rsgt));\n}\n\n \nvoid i915_ttm_adjust_lru(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tstruct i915_ttm_tt *i915_tt =\n\t\tcontainer_of(bo->ttm, typeof(*i915_tt), ttm);\n\tbool shrinkable =\n\t\tbo->ttm && i915_tt->filp && ttm_tt_is_populated(bo->ttm);\n\n\t \n\tif (!kref_read(&bo->kref))\n\t\treturn;\n\n\t \n\tif (kref_get_unless_zero(&obj->base.refcount)) {\n\t\tif (shrinkable != obj->mm.ttm_shrinkable) {\n\t\t\tif (shrinkable) {\n\t\t\t\tif (obj->mm.madv == I915_MADV_WILLNEED)\n\t\t\t\t\t__i915_gem_object_make_shrinkable(obj);\n\t\t\t\telse\n\t\t\t\t\t__i915_gem_object_make_purgeable(obj);\n\t\t\t} else {\n\t\t\t\ti915_gem_object_make_unshrinkable(obj);\n\t\t\t}\n\n\t\t\tobj->mm.ttm_shrinkable = shrinkable;\n\t\t}\n\t\ti915_gem_object_put(obj);\n\t}\n\n\t \n\tspin_lock(&bo->bdev->lru_lock);\n\tif (shrinkable) {\n\t\t \n\t\tbo->priority = TTM_MAX_BO_PRIORITY - 1;\n\t} else if (obj->mm.madv != I915_MADV_WILLNEED) {\n\t\tbo->priority = I915_TTM_PRIO_PURGE;\n\t} else if (!i915_gem_object_has_pages(obj)) {\n\t\tbo->priority = I915_TTM_PRIO_NO_PAGES;\n\t} else {\n\t\tstruct ttm_resource_manager *man =\n\t\t\tttm_manager_type(bo->bdev, bo->resource->mem_type);\n\n\t\t \n\t\tif (i915_ttm_cpu_maps_iomem(bo->resource) &&\n\t\t    i915_ttm_buddy_man_visible_size(man) < man->size &&\n\t\t    !(obj->flags & I915_BO_ALLOC_GPU_ONLY))\n\t\t\tbo->priority = I915_TTM_PRIO_NEEDS_CPU_ACCESS;\n\t\telse\n\t\t\tbo->priority = I915_TTM_PRIO_HAS_PAGES;\n\t}\n\n\tttm_bo_move_to_lru_tail(bo);\n\tspin_unlock(&bo->bdev->lru_lock);\n}\n\n \nstatic void i915_ttm_delayed_free(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!obj->ttm.created);\n\n\tttm_bo_put(i915_gem_to_ttm(obj));\n}\n\nstatic vm_fault_t vm_fault_ttm(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *area = vmf->vma;\n\tstruct ttm_buffer_object *bo = area->vm_private_data;\n\tstruct drm_device *dev = bo->base.dev;\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\tintel_wakeref_t wakeref = 0;\n\tvm_fault_t ret;\n\tint idx;\n\n\t \n\tif (unlikely(i915_gem_object_is_readonly(obj) &&\n\t\t     area->vm_flags & VM_WRITE))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tif (obj->mm.madv != I915_MADV_WILLNEED) {\n\t\tdma_resv_unlock(bo->base.resv);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\t \n\tif (!bo->resource) {\n\t\tstruct ttm_operation_ctx ctx = {\n\t\t\t.interruptible = true,\n\t\t\t.no_wait_gpu = true,  \n\t\t};\n\t\tint err;\n\n\t\tGEM_BUG_ON(!bo->ttm || !(bo->ttm->page_flags & TTM_TT_FLAG_SWAPPED));\n\n\t\terr = ttm_bo_validate(bo, i915_ttm_sys_placement(), &ctx);\n\t\tif (err) {\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\t\treturn VM_FAULT_SIGBUS;\n\t\t}\n\t} else if (!i915_ttm_resource_mappable(bo->resource)) {\n\t\tint err = -ENODEV;\n\t\tint i;\n\n\t\tfor (i = 0; i < obj->mm.n_placements; i++) {\n\t\t\tstruct intel_memory_region *mr = obj->mm.placements[i];\n\t\t\tunsigned int flags;\n\n\t\t\tif (!mr->io_size && mr->type != INTEL_MEMORY_SYSTEM)\n\t\t\t\tcontinue;\n\n\t\t\tflags = obj->flags;\n\t\t\tflags &= ~I915_BO_ALLOC_GPU_ONLY;\n\t\t\terr = __i915_ttm_migrate(obj, mr, flags);\n\t\t\tif (!err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (err) {\n\t\t\tdrm_dbg(dev, \"Unable to make resource CPU accessible(err = %pe)\\n\",\n\t\t\t\tERR_PTR(err));\n\t\t\tdma_resv_unlock(bo->base.resv);\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\tgoto out_rpm;\n\t\t}\n\t}\n\n\tif (i915_ttm_cpu_maps_iomem(bo->resource))\n\t\twakeref = intel_runtime_pm_get(&to_i915(obj->base.dev)->runtime_pm);\n\n\tif (drm_dev_enter(dev, &idx)) {\n\t\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\t\tdrm_dev_exit(idx);\n\t} else {\n\t\tret = ttm_bo_vm_dummy_page(vmf, vmf->vma->vm_page_prot);\n\t}\n\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\tgoto out_rpm;\n\n\t \n\tif (ret == VM_FAULT_NOPAGE && wakeref && !obj->userfault_count) {\n\t\tobj->userfault_count = 1;\n\t\tspin_lock(&to_i915(obj->base.dev)->runtime_pm.lmem_userfault_lock);\n\t\tlist_add(&obj->userfault_link, &to_i915(obj->base.dev)->runtime_pm.lmem_userfault_list);\n\t\tspin_unlock(&to_i915(obj->base.dev)->runtime_pm.lmem_userfault_lock);\n\n\t\tGEM_WARN_ON(!i915_ttm_cpu_maps_iomem(bo->resource));\n\t}\n\n\tif (wakeref & CONFIG_DRM_I915_USERFAULT_AUTOSUSPEND)\n\t\tintel_wakeref_auto(&to_i915(obj->base.dev)->runtime_pm.userfault_wakeref,\n\t\t\t\t   msecs_to_jiffies_timeout(CONFIG_DRM_I915_USERFAULT_AUTOSUSPEND));\n\n\ti915_ttm_adjust_lru(obj);\n\n\tdma_resv_unlock(bo->base.resv);\n\nout_rpm:\n\tif (wakeref)\n\t\tintel_runtime_pm_put(&to_i915(obj->base.dev)->runtime_pm, wakeref);\n\n\treturn ret;\n}\n\nstatic int\nvm_access_ttm(struct vm_area_struct *area, unsigned long addr,\n\t      void *buf, int len, int write)\n{\n\tstruct drm_i915_gem_object *obj =\n\t\ti915_ttm_to_gem(area->vm_private_data);\n\n\tif (i915_gem_object_is_readonly(obj) && write)\n\t\treturn -EACCES;\n\n\treturn ttm_bo_vm_access(area, addr, buf, len, write);\n}\n\nstatic void ttm_vm_open(struct vm_area_struct *vma)\n{\n\tstruct drm_i915_gem_object *obj =\n\t\ti915_ttm_to_gem(vma->vm_private_data);\n\n\tGEM_BUG_ON(i915_ttm_is_ghost_object(vma->vm_private_data));\n\ti915_gem_object_get(obj);\n}\n\nstatic void ttm_vm_close(struct vm_area_struct *vma)\n{\n\tstruct drm_i915_gem_object *obj =\n\t\ti915_ttm_to_gem(vma->vm_private_data);\n\n\tGEM_BUG_ON(i915_ttm_is_ghost_object(vma->vm_private_data));\n\ti915_gem_object_put(obj);\n}\n\nstatic const struct vm_operations_struct vm_ops_ttm = {\n\t.fault = vm_fault_ttm,\n\t.access = vm_access_ttm,\n\t.open = ttm_vm_open,\n\t.close = ttm_vm_close,\n};\n\nstatic u64 i915_ttm_mmap_offset(struct drm_i915_gem_object *obj)\n{\n\t \n\tGEM_BUG_ON(!drm_mm_node_allocated(&obj->base.vma_node.vm_node));\n\n\treturn drm_vma_node_offset_addr(&obj->base.vma_node);\n}\n\nstatic void i915_ttm_unmap_virtual(struct drm_i915_gem_object *obj)\n{\n\tstruct ttm_buffer_object *bo = i915_gem_to_ttm(obj);\n\tintel_wakeref_t wakeref = 0;\n\n\tassert_object_held_shared(obj);\n\n\tif (i915_ttm_cpu_maps_iomem(bo->resource)) {\n\t\twakeref = intel_runtime_pm_get(&to_i915(obj->base.dev)->runtime_pm);\n\n\t\t \n\t\tif (obj->userfault_count) {\n\t\t\tspin_lock(&to_i915(obj->base.dev)->runtime_pm.lmem_userfault_lock);\n\t\t\tlist_del(&obj->userfault_link);\n\t\t\tspin_unlock(&to_i915(obj->base.dev)->runtime_pm.lmem_userfault_lock);\n\t\t\tobj->userfault_count = 0;\n\t\t}\n\t}\n\n\tGEM_WARN_ON(obj->userfault_count);\n\n\tttm_bo_unmap_virtual(i915_gem_to_ttm(obj));\n\n\tif (wakeref)\n\t\tintel_runtime_pm_put(&to_i915(obj->base.dev)->runtime_pm, wakeref);\n}\n\nstatic const struct drm_i915_gem_object_ops i915_gem_ttm_obj_ops = {\n\t.name = \"i915_gem_object_ttm\",\n\t.flags = I915_GEM_OBJECT_IS_SHRINKABLE |\n\t\t I915_GEM_OBJECT_SELF_MANAGED_SHRINK_LIST,\n\n\t.get_pages = i915_ttm_get_pages,\n\t.put_pages = i915_ttm_put_pages,\n\t.truncate = i915_ttm_truncate,\n\t.shrink = i915_ttm_shrink,\n\n\t.adjust_lru = i915_ttm_adjust_lru,\n\t.delayed_free = i915_ttm_delayed_free,\n\t.migrate = i915_ttm_migrate,\n\n\t.mmap_offset = i915_ttm_mmap_offset,\n\t.unmap_virtual = i915_ttm_unmap_virtual,\n\t.mmap_ops = &vm_ops_ttm,\n};\n\nvoid i915_ttm_bo_destroy(struct ttm_buffer_object *bo)\n{\n\tstruct drm_i915_gem_object *obj = i915_ttm_to_gem(bo);\n\n\ti915_gem_object_release_memory_region(obj);\n\tmutex_destroy(&obj->ttm.get_io_page.lock);\n\n\tif (obj->ttm.created) {\n\t\t \n\t\tif (obj->mm.ttm_shrinkable)\n\t\t\ti915_gem_object_make_unshrinkable(obj);\n\n\t\ti915_ttm_backup_free(obj);\n\n\t\t \n\t\t__i915_gem_free_object(obj);\n\n\t\tcall_rcu(&obj->rcu, __i915_gem_free_object_rcu);\n\t} else {\n\t\t__i915_gem_object_fini(obj);\n\t}\n}\n\n \nint __i915_gem_ttm_object_init(struct intel_memory_region *mem,\n\t\t\t       struct drm_i915_gem_object *obj,\n\t\t\t       resource_size_t offset,\n\t\t\t       resource_size_t size,\n\t\t\t       resource_size_t page_size,\n\t\t\t       unsigned int flags)\n{\n\tstatic struct lock_class_key lock_class;\n\tstruct drm_i915_private *i915 = mem->i915;\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = true,\n\t\t.no_wait_gpu = false,\n\t};\n\tenum ttm_bo_type bo_type;\n\tint ret;\n\n\tdrm_gem_private_object_init(&i915->drm, &obj->base, size);\n\ti915_gem_object_init(obj, &i915_gem_ttm_obj_ops, &lock_class, flags);\n\n\tobj->bo_offset = offset;\n\n\t \n\tobj->mm.region = mem;\n\tINIT_LIST_HEAD(&obj->mm.region_link);\n\n\tINIT_RADIX_TREE(&obj->ttm.get_io_page.radix, GFP_KERNEL | __GFP_NOWARN);\n\tmutex_init(&obj->ttm.get_io_page.lock);\n\tbo_type = (obj->flags & I915_BO_ALLOC_USER) ? ttm_bo_type_device :\n\t\tttm_bo_type_kernel;\n\n\tobj->base.vma_node.driver_private = i915_gem_to_ttm(obj);\n\n\t \n\tGEM_BUG_ON(page_size && obj->mm.n_placements);\n\n\t \n\ti915_gem_object_make_unshrinkable(obj);\n\n\t \n\tret = ttm_bo_init_reserved(&i915->bdev, i915_gem_to_ttm(obj), bo_type,\n\t\t\t\t   &i915_sys_placement, page_size >> PAGE_SHIFT,\n\t\t\t\t   &ctx, NULL, NULL, i915_ttm_bo_destroy);\n\n\t \n\tif (size >> PAGE_SHIFT > INT_MAX && ret == -ENOSPC)\n\t\tret = -E2BIG;\n\n\tif (ret)\n\t\treturn i915_ttm_err_to_gem(ret);\n\n\tobj->ttm.created = true;\n\ti915_gem_object_release_memory_region(obj);\n\ti915_gem_object_init_memory_region(obj, mem);\n\ti915_ttm_adjust_domains_after_move(obj);\n\ti915_ttm_adjust_gem_after_move(obj);\n\ti915_gem_object_unlock(obj);\n\n\treturn 0;\n}\n\nstatic const struct intel_memory_region_ops ttm_system_region_ops = {\n\t.init_object = __i915_gem_ttm_object_init,\n\t.release = intel_region_ttm_fini,\n};\n\nstruct intel_memory_region *\ni915_gem_ttm_system_setup(struct drm_i915_private *i915,\n\t\t\t  u16 type, u16 instance)\n{\n\tstruct intel_memory_region *mr;\n\n\tmr = intel_memory_region_create(i915, 0,\n\t\t\t\t\ttotalram_pages() << PAGE_SHIFT,\n\t\t\t\t\tPAGE_SIZE, 0, 0,\n\t\t\t\t\ttype, instance,\n\t\t\t\t\t&ttm_system_region_ops);\n\tif (IS_ERR(mr))\n\t\treturn mr;\n\n\tintel_memory_region_set_name(mr, \"system-ttm\");\n\treturn mr;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}