{
  "module_name": "i915_gem_shrinker.c",
  "hash_id": "689428a452d774dae92e94529f09d5746b8f495593491d268bc3a2c9e5c04d5b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c",
  "human_readable_source": " \n\n#include <linux/oom.h>\n#include <linux/sched/mm.h>\n#include <linux/shmem_fs.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/pci.h>\n#include <linux/dma-buf.h>\n#include <linux/vmalloc.h>\n\n#include \"gt/intel_gt_requests.h\"\n\n#include \"i915_trace.h\"\n\nstatic bool swap_available(void)\n{\n\treturn get_nr_swap_pages() > 0;\n}\n\nstatic bool can_release_pages(struct drm_i915_gem_object *obj)\n{\n\t \n\tif (!i915_gem_object_is_shrinkable(obj))\n\t\treturn false;\n\n\t \n\treturn swap_available() || obj->mm.madv == I915_MADV_DONTNEED;\n}\n\nstatic bool drop_pages(struct drm_i915_gem_object *obj,\n\t\t       unsigned long shrink, bool trylock_vm)\n{\n\tunsigned long flags;\n\n\tflags = 0;\n\tif (shrink & I915_SHRINK_ACTIVE)\n\t\tflags |= I915_GEM_OBJECT_UNBIND_ACTIVE;\n\tif (!(shrink & I915_SHRINK_BOUND))\n\t\tflags |= I915_GEM_OBJECT_UNBIND_TEST;\n\tif (trylock_vm)\n\t\tflags |= I915_GEM_OBJECT_UNBIND_VM_TRYLOCK;\n\n\tif (i915_gem_object_unbind(obj, flags) == 0)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int try_to_writeback(struct drm_i915_gem_object *obj, unsigned int flags)\n{\n\tif (obj->ops->shrink) {\n\t\tunsigned int shrink_flags = 0;\n\n\t\tif (!(flags & I915_SHRINK_ACTIVE))\n\t\t\tshrink_flags |= I915_GEM_OBJECT_SHRINK_NO_GPU_WAIT;\n\n\t\tif (flags & I915_SHRINK_WRITEBACK)\n\t\t\tshrink_flags |= I915_GEM_OBJECT_SHRINK_WRITEBACK;\n\n\t\treturn obj->ops->shrink(obj, shrink_flags);\n\t}\n\n\treturn 0;\n}\n\n \nunsigned long\ni915_gem_shrink(struct i915_gem_ww_ctx *ww,\n\t\tstruct drm_i915_private *i915,\n\t\tunsigned long target,\n\t\tunsigned long *nr_scanned,\n\t\tunsigned int shrink)\n{\n\tconst struct {\n\t\tstruct list_head *list;\n\t\tunsigned int bit;\n\t} phases[] = {\n\t\t{ &i915->mm.purge_list, ~0u },\n\t\t{\n\t\t\t&i915->mm.shrink_list,\n\t\t\tI915_SHRINK_BOUND | I915_SHRINK_UNBOUND\n\t\t},\n\t\t{ NULL, 0 },\n\t}, *phase;\n\tintel_wakeref_t wakeref = 0;\n\tunsigned long count = 0;\n\tunsigned long scanned = 0;\n\tint err = 0;\n\n\t \n\tbool trylock_vm = !ww && intel_vm_no_concurrent_access_wa(i915);\n\n\ttrace_i915_gem_shrink(i915, target, shrink);\n\n\t \n\tif (shrink & I915_SHRINK_BOUND) {\n\t\twakeref = intel_runtime_pm_get_if_in_use(&i915->runtime_pm);\n\t\tif (!wakeref)\n\t\t\tshrink &= ~I915_SHRINK_BOUND;\n\t}\n\n\t \n\tif (shrink & I915_SHRINK_ACTIVE)\n\t\t \n\t\tintel_gt_retire_requests(to_gt(i915));\n\n\t \n\tfor (phase = phases; phase->list; phase++) {\n\t\tstruct list_head still_in_list;\n\t\tstruct drm_i915_gem_object *obj;\n\t\tunsigned long flags;\n\n\t\tif ((shrink & phase->bit) == 0)\n\t\t\tcontinue;\n\n\t\tINIT_LIST_HEAD(&still_in_list);\n\n\t\t \n\t\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\t\twhile (count < target &&\n\t\t       (obj = list_first_entry_or_null(phase->list,\n\t\t\t\t\t\t       typeof(*obj),\n\t\t\t\t\t\t       mm.link))) {\n\t\t\tlist_move_tail(&obj->mm.link, &still_in_list);\n\n\t\t\tif (shrink & I915_SHRINK_VMAPS &&\n\t\t\t    !is_vmalloc_addr(obj->mm.mapping))\n\t\t\t\tcontinue;\n\n\t\t\tif (!(shrink & I915_SHRINK_ACTIVE) &&\n\t\t\t    i915_gem_object_is_framebuffer(obj))\n\t\t\t\tcontinue;\n\n\t\t\tif (!can_release_pages(obj))\n\t\t\t\tcontinue;\n\n\t\t\tif (!kref_get_unless_zero(&obj->base.refcount))\n\t\t\t\tcontinue;\n\n\t\t\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n\n\t\t\t \n\t\t\tif (!ww) {\n\t\t\t\tif (!i915_gem_object_trylock(obj, NULL))\n\t\t\t\t\tgoto skip;\n\t\t\t} else {\n\t\t\t\terr = i915_gem_object_lock(obj, ww);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto skip;\n\t\t\t}\n\n\t\t\tif (drop_pages(obj, shrink, trylock_vm) &&\n\t\t\t    !__i915_gem_object_put_pages(obj) &&\n\t\t\t    !try_to_writeback(obj, shrink))\n\t\t\t\tcount += obj->base.size >> PAGE_SHIFT;\n\n\t\t\tif (!ww)\n\t\t\t\ti915_gem_object_unlock(obj);\n\n\t\t\tscanned += obj->base.size >> PAGE_SHIFT;\nskip:\n\t\t\ti915_gem_object_put(obj);\n\n\t\t\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t\tlist_splice_tail(&still_in_list, phase->list);\n\t\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (shrink & I915_SHRINK_BOUND)\n\t\tintel_runtime_pm_put(&i915->runtime_pm, wakeref);\n\n\tif (err)\n\t\treturn err;\n\n\tif (nr_scanned)\n\t\t*nr_scanned += scanned;\n\treturn count;\n}\n\n \nunsigned long i915_gem_shrink_all(struct drm_i915_private *i915)\n{\n\tintel_wakeref_t wakeref;\n\tunsigned long freed = 0;\n\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref) {\n\t\tfreed = i915_gem_shrink(NULL, i915, -1UL, NULL,\n\t\t\t\t\tI915_SHRINK_BOUND |\n\t\t\t\t\tI915_SHRINK_UNBOUND);\n\t}\n\n\treturn freed;\n}\n\nstatic unsigned long\ni915_gem_shrinker_count(struct shrinker *shrinker, struct shrink_control *sc)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(shrinker, struct drm_i915_private, mm.shrinker);\n\tunsigned long num_objects;\n\tunsigned long count;\n\n\tcount = READ_ONCE(i915->mm.shrink_memory) >> PAGE_SHIFT;\n\tnum_objects = READ_ONCE(i915->mm.shrink_count);\n\n\t \n\tif (num_objects) {\n\t\tunsigned long avg = 2 * count / num_objects;\n\n\t\ti915->mm.shrinker.batch =\n\t\t\tmax((i915->mm.shrinker.batch + avg) >> 1,\n\t\t\t    128ul  );\n\t}\n\n\treturn count;\n}\n\nstatic unsigned long\ni915_gem_shrinker_scan(struct shrinker *shrinker, struct shrink_control *sc)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(shrinker, struct drm_i915_private, mm.shrinker);\n\tunsigned long freed;\n\n\tsc->nr_scanned = 0;\n\n\tfreed = i915_gem_shrink(NULL, i915,\n\t\t\t\tsc->nr_to_scan,\n\t\t\t\t&sc->nr_scanned,\n\t\t\t\tI915_SHRINK_BOUND |\n\t\t\t\tI915_SHRINK_UNBOUND);\n\tif (sc->nr_scanned < sc->nr_to_scan && current_is_kswapd()) {\n\t\tintel_wakeref_t wakeref;\n\n\t\twith_intel_runtime_pm(&i915->runtime_pm, wakeref) {\n\t\t\tfreed += i915_gem_shrink(NULL, i915,\n\t\t\t\t\t\t sc->nr_to_scan - sc->nr_scanned,\n\t\t\t\t\t\t &sc->nr_scanned,\n\t\t\t\t\t\t I915_SHRINK_ACTIVE |\n\t\t\t\t\t\t I915_SHRINK_BOUND |\n\t\t\t\t\t\t I915_SHRINK_UNBOUND |\n\t\t\t\t\t\t I915_SHRINK_WRITEBACK);\n\t\t}\n\t}\n\n\treturn sc->nr_scanned ? freed : SHRINK_STOP;\n}\n\nstatic int\ni915_gem_shrinker_oom(struct notifier_block *nb, unsigned long event, void *ptr)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(nb, struct drm_i915_private, mm.oom_notifier);\n\tstruct drm_i915_gem_object *obj;\n\tunsigned long unevictable, available, freed_pages;\n\tintel_wakeref_t wakeref;\n\tunsigned long flags;\n\n\tfreed_pages = 0;\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref)\n\t\tfreed_pages += i915_gem_shrink(NULL, i915, -1UL, NULL,\n\t\t\t\t\t       I915_SHRINK_BOUND |\n\t\t\t\t\t       I915_SHRINK_UNBOUND |\n\t\t\t\t\t       I915_SHRINK_WRITEBACK);\n\n\t \n\tavailable = unevictable = 0;\n\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\tlist_for_each_entry(obj, &i915->mm.shrink_list, mm.link) {\n\t\tif (!can_release_pages(obj))\n\t\t\tunevictable += obj->base.size >> PAGE_SHIFT;\n\t\telse\n\t\t\tavailable += obj->base.size >> PAGE_SHIFT;\n\t}\n\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n\n\tif (freed_pages || available)\n\t\tpr_info(\"Purging GPU memory, %lu pages freed, \"\n\t\t\t\"%lu pages still pinned, %lu pages left available.\\n\",\n\t\t\tfreed_pages, unevictable, available);\n\n\t*(unsigned long *)ptr += freed_pages;\n\treturn NOTIFY_DONE;\n}\n\nstatic int\ni915_gem_shrinker_vmap(struct notifier_block *nb, unsigned long event, void *ptr)\n{\n\tstruct drm_i915_private *i915 =\n\t\tcontainer_of(nb, struct drm_i915_private, mm.vmap_notifier);\n\tstruct i915_vma *vma, *next;\n\tunsigned long freed_pages = 0;\n\tintel_wakeref_t wakeref;\n\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref)\n\t\tfreed_pages += i915_gem_shrink(NULL, i915, -1UL, NULL,\n\t\t\t\t\t       I915_SHRINK_BOUND |\n\t\t\t\t\t       I915_SHRINK_UNBOUND |\n\t\t\t\t\t       I915_SHRINK_VMAPS);\n\n\t \n\tmutex_lock(&to_gt(i915)->ggtt->vm.mutex);\n\tlist_for_each_entry_safe(vma, next,\n\t\t\t\t &to_gt(i915)->ggtt->vm.bound_list, vm_link) {\n\t\tunsigned long count = i915_vma_size(vma) >> PAGE_SHIFT;\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\t\tif (!vma->iomap || i915_vma_is_active(vma))\n\t\t\tcontinue;\n\n\t\tif (!i915_gem_object_trylock(obj, NULL))\n\t\t\tcontinue;\n\n\t\tif (__i915_vma_unbind(vma) == 0)\n\t\t\tfreed_pages += count;\n\n\t\ti915_gem_object_unlock(obj);\n\t}\n\tmutex_unlock(&to_gt(i915)->ggtt->vm.mutex);\n\n\t*(unsigned long *)ptr += freed_pages;\n\treturn NOTIFY_DONE;\n}\n\nvoid i915_gem_driver_register__shrinker(struct drm_i915_private *i915)\n{\n\ti915->mm.shrinker.scan_objects = i915_gem_shrinker_scan;\n\ti915->mm.shrinker.count_objects = i915_gem_shrinker_count;\n\ti915->mm.shrinker.seeks = DEFAULT_SEEKS;\n\ti915->mm.shrinker.batch = 4096;\n\tdrm_WARN_ON(&i915->drm, register_shrinker(&i915->mm.shrinker,\n\t\t\t\t\t\t  \"drm-i915_gem\"));\n\n\ti915->mm.oom_notifier.notifier_call = i915_gem_shrinker_oom;\n\tdrm_WARN_ON(&i915->drm, register_oom_notifier(&i915->mm.oom_notifier));\n\n\ti915->mm.vmap_notifier.notifier_call = i915_gem_shrinker_vmap;\n\tdrm_WARN_ON(&i915->drm,\n\t\t    register_vmap_purge_notifier(&i915->mm.vmap_notifier));\n}\n\nvoid i915_gem_driver_unregister__shrinker(struct drm_i915_private *i915)\n{\n\tdrm_WARN_ON(&i915->drm,\n\t\t    unregister_vmap_purge_notifier(&i915->mm.vmap_notifier));\n\tdrm_WARN_ON(&i915->drm,\n\t\t    unregister_oom_notifier(&i915->mm.oom_notifier));\n\tunregister_shrinker(&i915->mm.shrinker);\n}\n\nvoid i915_gem_shrinker_taints_mutex(struct drm_i915_private *i915,\n\t\t\t\t    struct mutex *mutex)\n{\n\tif (!IS_ENABLED(CONFIG_LOCKDEP))\n\t\treturn;\n\n\tfs_reclaim_acquire(GFP_KERNEL);\n\n\tmutex_acquire(&mutex->dep_map, 0, 0, _RET_IP_);\n\tmutex_release(&mutex->dep_map, _RET_IP_);\n\n\tfs_reclaim_release(GFP_KERNEL);\n}\n\n \nvoid i915_gem_object_make_unshrinkable(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = obj_to_i915(obj);\n\tunsigned long flags;\n\n\t \n\tif (atomic_add_unless(&obj->mm.shrink_pin, 1, 0))\n\t\treturn;\n\n\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\tif (!atomic_fetch_inc(&obj->mm.shrink_pin) &&\n\t    !list_empty(&obj->mm.link)) {\n\t\tlist_del_init(&obj->mm.link);\n\t\ti915->mm.shrink_count--;\n\t\ti915->mm.shrink_memory -= obj->base.size;\n\t}\n\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n}\n\nstatic void ___i915_gem_object_make_shrinkable(struct drm_i915_gem_object *obj,\n\t\t\t\t\t       struct list_head *head)\n{\n\tstruct drm_i915_private *i915 = obj_to_i915(obj);\n\tunsigned long flags;\n\n\tif (!i915_gem_object_is_shrinkable(obj))\n\t\treturn;\n\n\tif (atomic_add_unless(&obj->mm.shrink_pin, -1, 1))\n\t\treturn;\n\n\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\tGEM_BUG_ON(!kref_read(&obj->base.refcount));\n\tif (atomic_dec_and_test(&obj->mm.shrink_pin)) {\n\t\tGEM_BUG_ON(!list_empty(&obj->mm.link));\n\n\t\tlist_add_tail(&obj->mm.link, head);\n\t\ti915->mm.shrink_count++;\n\t\ti915->mm.shrink_memory += obj->base.size;\n\n\t}\n\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n}\n\n \nvoid __i915_gem_object_make_shrinkable(struct drm_i915_gem_object *obj)\n{\n\t___i915_gem_object_make_shrinkable(obj,\n\t\t\t\t\t   &obj_to_i915(obj)->mm.shrink_list);\n}\n\n \nvoid __i915_gem_object_make_purgeable(struct drm_i915_gem_object *obj)\n{\n\t___i915_gem_object_make_shrinkable(obj,\n\t\t\t\t\t   &obj_to_i915(obj)->mm.purge_list);\n}\n\n \nvoid i915_gem_object_make_shrinkable(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(obj));\n\t__i915_gem_object_make_shrinkable(obj);\n}\n\n \nvoid i915_gem_object_make_purgeable(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(obj));\n\t__i915_gem_object_make_purgeable(obj);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}