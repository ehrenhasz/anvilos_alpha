{
  "module_name": "i915_gem_shmem.c",
  "hash_id": "934ca16a3056ab04dedd81f354f6ece6995993a2c514bd37340563e232e3be4e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_shmem.c",
  "human_readable_source": " \n\n#include <linux/pagevec.h>\n#include <linux/shmem_fs.h>\n#include <linux/swap.h>\n\n#include <drm/drm_cache.h>\n\n#include \"gem/i915_gem_region.h\"\n#include \"i915_drv.h\"\n#include \"i915_gem_object.h\"\n#include \"i915_gem_tiling.h\"\n#include \"i915_gemfs.h\"\n#include \"i915_scatterlist.h\"\n#include \"i915_trace.h\"\n\n \nstatic void check_release_folio_batch(struct folio_batch *fbatch)\n{\n\tcheck_move_unevictable_folios(fbatch);\n\t__folio_batch_release(fbatch);\n\tcond_resched();\n}\n\nvoid shmem_sg_free_table(struct sg_table *st, struct address_space *mapping,\n\t\t\t bool dirty, bool backup)\n{\n\tstruct sgt_iter sgt_iter;\n\tstruct folio_batch fbatch;\n\tstruct folio *last = NULL;\n\tstruct page *page;\n\n\tmapping_clear_unevictable(mapping);\n\n\tfolio_batch_init(&fbatch);\n\tfor_each_sgt_page(page, sgt_iter, st) {\n\t\tstruct folio *folio = page_folio(page);\n\n\t\tif (folio == last)\n\t\t\tcontinue;\n\t\tlast = folio;\n\t\tif (dirty)\n\t\t\tfolio_mark_dirty(folio);\n\t\tif (backup)\n\t\t\tfolio_mark_accessed(folio);\n\n\t\tif (!folio_batch_add(&fbatch, folio))\n\t\t\tcheck_release_folio_batch(&fbatch);\n\t}\n\tif (fbatch.nr)\n\t\tcheck_release_folio_batch(&fbatch);\n\n\tsg_free_table(st);\n}\n\nint shmem_sg_alloc_table(struct drm_i915_private *i915, struct sg_table *st,\n\t\t\t size_t size, struct intel_memory_region *mr,\n\t\t\t struct address_space *mapping,\n\t\t\t unsigned int max_segment)\n{\n\tunsigned int page_count;  \n\tunsigned long i;\n\tstruct scatterlist *sg;\n\tunsigned long next_pfn = 0;\t \n\tgfp_t noreclaim;\n\tint ret;\n\n\tif (overflows_type(size / PAGE_SIZE, page_count))\n\t\treturn -E2BIG;\n\n\tpage_count = size / PAGE_SIZE;\n\t \n\tif (size > resource_size(&mr->region))\n\t\treturn -ENOMEM;\n\n\tif (sg_alloc_table(st, page_count, GFP_KERNEL | __GFP_NOWARN))\n\t\treturn -ENOMEM;\n\n\t \n\tmapping_set_unevictable(mapping);\n\tnoreclaim = mapping_gfp_constraint(mapping, ~__GFP_RECLAIM);\n\tnoreclaim |= __GFP_NORETRY | __GFP_NOWARN;\n\n\tsg = st->sgl;\n\tst->nents = 0;\n\tfor (i = 0; i < page_count; i++) {\n\t\tstruct folio *folio;\n\t\tunsigned long nr_pages;\n\t\tconst unsigned int shrink[] = {\n\t\t\tI915_SHRINK_BOUND | I915_SHRINK_UNBOUND,\n\t\t\t0,\n\t\t}, *s = shrink;\n\t\tgfp_t gfp = noreclaim;\n\n\t\tdo {\n\t\t\tcond_resched();\n\t\t\tfolio = shmem_read_folio_gfp(mapping, i, gfp);\n\t\t\tif (!IS_ERR(folio))\n\t\t\t\tbreak;\n\n\t\t\tif (!*s) {\n\t\t\t\tret = PTR_ERR(folio);\n\t\t\t\tgoto err_sg;\n\t\t\t}\n\n\t\t\ti915_gem_shrink(NULL, i915, 2 * page_count, NULL, *s++);\n\n\t\t\t \n\t\t\tif (!*s) {\n\t\t\t\t \n\t\t\t\tgfp = mapping_gfp_mask(mapping);\n\n\t\t\t\t \n\t\t\t\tgfp |= __GFP_RETRY_MAYFAIL | __GFP_NOWARN;\n\t\t\t}\n\t\t} while (1);\n\n\t\tnr_pages = min_t(unsigned long,\n\t\t\t\tfolio_nr_pages(folio), page_count - i);\n\t\tif (!i ||\n\t\t    sg->length >= max_segment ||\n\t\t    folio_pfn(folio) != next_pfn) {\n\t\t\tif (i)\n\t\t\t\tsg = sg_next(sg);\n\n\t\t\tst->nents++;\n\t\t\tsg_set_folio(sg, folio, nr_pages * PAGE_SIZE, 0);\n\t\t} else {\n\t\t\t \n\t\t\tsg->length += nr_pages * PAGE_SIZE;\n\t\t}\n\t\tnext_pfn = folio_pfn(folio) + nr_pages;\n\t\ti += nr_pages - 1;\n\n\t\t \n\t\tGEM_BUG_ON(gfp & __GFP_DMA32 && next_pfn >= 0x00100000UL);\n\t}\n\tif (sg)  \n\t\tsg_mark_end(sg);\n\n\t \n\ti915_sg_trim(st);\n\n\treturn 0;\nerr_sg:\n\tsg_mark_end(sg);\n\tif (sg != st->sgl) {\n\t\tshmem_sg_free_table(st, mapping, false, false);\n\t} else {\n\t\tmapping_clear_unevictable(mapping);\n\t\tsg_free_table(st);\n\t}\n\n\t \n\tif (ret == -ENOSPC)\n\t\tret = -ENOMEM;\n\n\treturn ret;\n}\n\nstatic int shmem_get_pages(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct intel_memory_region *mem = obj->mm.region;\n\tstruct address_space *mapping = obj->base.filp->f_mapping;\n\tunsigned int max_segment = i915_sg_segment_size(i915->drm.dev);\n\tstruct sg_table *st;\n\tstruct sgt_iter sgt_iter;\n\tstruct page *page;\n\tint ret;\n\n\t \n\tGEM_BUG_ON(obj->read_domains & I915_GEM_GPU_DOMAINS);\n\tGEM_BUG_ON(obj->write_domain & I915_GEM_GPU_DOMAINS);\n\nrebuild_st:\n\tst = kmalloc(sizeof(*st), GFP_KERNEL | __GFP_NOWARN);\n\tif (!st)\n\t\treturn -ENOMEM;\n\n\tret = shmem_sg_alloc_table(i915, st, obj->base.size, mem, mapping,\n\t\t\t\t   max_segment);\n\tif (ret)\n\t\tgoto err_st;\n\n\tret = i915_gem_gtt_prepare_pages(obj, st);\n\tif (ret) {\n\t\t \n\t\tif (max_segment > PAGE_SIZE) {\n\t\t\tfor_each_sgt_page(page, sgt_iter, st)\n\t\t\t\tput_page(page);\n\t\t\tsg_free_table(st);\n\t\t\tkfree(st);\n\n\t\t\tmax_segment = PAGE_SIZE;\n\t\t\tgoto rebuild_st;\n\t\t} else {\n\t\t\tdev_warn(i915->drm.dev,\n\t\t\t\t \"Failed to DMA remap %zu pages\\n\",\n\t\t\t\t obj->base.size >> PAGE_SHIFT);\n\t\t\tgoto err_pages;\n\t\t}\n\t}\n\n\tif (i915_gem_object_needs_bit17_swizzle(obj))\n\t\ti915_gem_object_do_bit_17_swizzle(obj, st);\n\n\tif (i915_gem_object_can_bypass_llc(obj))\n\t\tobj->cache_dirty = true;\n\n\t__i915_gem_object_set_pages(obj, st);\n\n\treturn 0;\n\nerr_pages:\n\tshmem_sg_free_table(st, mapping, false, false);\n\t \nerr_st:\n\tif (ret == -ENOSPC)\n\t\tret = -ENOMEM;\n\n\tkfree(st);\n\n\treturn ret;\n}\n\nstatic int\nshmem_truncate(struct drm_i915_gem_object *obj)\n{\n\t \n\tshmem_truncate_range(file_inode(obj->base.filp), 0, (loff_t)-1);\n\tobj->mm.madv = __I915_MADV_PURGED;\n\tobj->mm.pages = ERR_PTR(-EFAULT);\n\n\treturn 0;\n}\n\nvoid __shmem_writeback(size_t size, struct address_space *mapping)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_NONE,\n\t\t.nr_to_write = SWAP_CLUSTER_MAX,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t\t.for_reclaim = 1,\n\t};\n\tunsigned long i;\n\n\t \n\n\t \n\tfor (i = 0; i < size >> PAGE_SHIFT; i++) {\n\t\tstruct page *page;\n\n\t\tpage = find_lock_page(mapping, i);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\tif (!page_mapped(page) && clear_page_dirty_for_io(page)) {\n\t\t\tint ret;\n\n\t\t\tSetPageReclaim(page);\n\t\t\tret = mapping->a_ops->writepage(page, &wbc);\n\t\t\tif (!PageWriteback(page))\n\t\t\t\tClearPageReclaim(page);\n\t\t\tif (!ret)\n\t\t\t\tgoto put;\n\t\t}\n\t\tunlock_page(page);\nput:\n\t\tput_page(page);\n\t}\n}\n\nstatic void\nshmem_writeback(struct drm_i915_gem_object *obj)\n{\n\t__shmem_writeback(obj->base.size, obj->base.filp->f_mapping);\n}\n\nstatic int shmem_shrink(struct drm_i915_gem_object *obj, unsigned int flags)\n{\n\tswitch (obj->mm.madv) {\n\tcase I915_MADV_DONTNEED:\n\t\treturn i915_gem_object_truncate(obj);\n\tcase __I915_MADV_PURGED:\n\t\treturn 0;\n\t}\n\n\tif (flags & I915_GEM_OBJECT_SHRINK_WRITEBACK)\n\t\tshmem_writeback(obj);\n\n\treturn 0;\n}\n\nvoid\n__i915_gem_object_release_shmem(struct drm_i915_gem_object *obj,\n\t\t\t\tstruct sg_table *pages,\n\t\t\t\tbool needs_clflush)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\n\tGEM_BUG_ON(obj->mm.madv == __I915_MADV_PURGED);\n\n\tif (obj->mm.madv == I915_MADV_DONTNEED)\n\t\tobj->mm.dirty = false;\n\n\tif (needs_clflush &&\n\t    (obj->read_domains & I915_GEM_DOMAIN_CPU) == 0 &&\n\t    !(obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_READ))\n\t\tdrm_clflush_sg(pages);\n\n\t__start_cpu_write(obj);\n\t \n\tif (!HAS_LLC(i915) && !IS_DGFX(i915))\n\t\tobj->cache_dirty = true;\n}\n\nvoid i915_gem_object_put_pages_shmem(struct drm_i915_gem_object *obj, struct sg_table *pages)\n{\n\t__i915_gem_object_release_shmem(obj, pages, true);\n\n\ti915_gem_gtt_finish_pages(obj, pages);\n\n\tif (i915_gem_object_needs_bit17_swizzle(obj))\n\t\ti915_gem_object_save_bit_17_swizzle(obj, pages);\n\n\tshmem_sg_free_table(pages, file_inode(obj->base.filp)->i_mapping,\n\t\t\t    obj->mm.dirty, obj->mm.madv == I915_MADV_WILLNEED);\n\tkfree(pages);\n\tobj->mm.dirty = false;\n}\n\nstatic void\nshmem_put_pages(struct drm_i915_gem_object *obj, struct sg_table *pages)\n{\n\tif (likely(i915_gem_object_has_struct_page(obj)))\n\t\ti915_gem_object_put_pages_shmem(obj, pages);\n\telse\n\t\ti915_gem_object_put_pages_phys(obj, pages);\n}\n\nstatic int\nshmem_pwrite(struct drm_i915_gem_object *obj,\n\t     const struct drm_i915_gem_pwrite *arg)\n{\n\tstruct address_space *mapping = obj->base.filp->f_mapping;\n\tconst struct address_space_operations *aops = mapping->a_ops;\n\tchar __user *user_data = u64_to_user_ptr(arg->data_ptr);\n\tu64 remain, offset;\n\tunsigned int pg;\n\n\t \n\tGEM_BUG_ON(!access_ok(user_data, arg->size));\n\n\tif (!i915_gem_object_has_struct_page(obj))\n\t\treturn i915_gem_object_pwrite_phys(obj, arg);\n\n\t \n\tif (i915_gem_object_has_pages(obj))\n\t\treturn -ENODEV;\n\n\tif (obj->mm.madv != I915_MADV_WILLNEED)\n\t\treturn -EFAULT;\n\n\t \n\n\tremain = arg->size;\n\toffset = arg->offset;\n\tpg = offset_in_page(offset);\n\n\tdo {\n\t\tunsigned int len, unwritten;\n\t\tstruct page *page;\n\t\tvoid *data, *vaddr;\n\t\tint err;\n\t\tchar __maybe_unused c;\n\n\t\tlen = PAGE_SIZE - pg;\n\t\tif (len > remain)\n\t\t\tlen = remain;\n\n\t\t \n\t\terr = __get_user(c, user_data);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = __get_user(c, user_data + len - 1);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\terr = aops->write_begin(obj->base.filp, mapping, offset, len,\n\t\t\t\t\t&page, &data);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\tvaddr = kmap_atomic(page);\n\t\tunwritten = __copy_from_user_inatomic(vaddr + pg,\n\t\t\t\t\t\t      user_data,\n\t\t\t\t\t\t      len);\n\t\tkunmap_atomic(vaddr);\n\n\t\terr = aops->write_end(obj->base.filp, mapping, offset, len,\n\t\t\t\t      len - unwritten, page, data);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\t \n\t\tif (unwritten)\n\t\t\treturn -ENODEV;\n\n\t\tremain -= len;\n\t\tuser_data += len;\n\t\toffset += len;\n\t\tpg = 0;\n\t} while (remain);\n\n\treturn 0;\n}\n\nstatic int\nshmem_pread(struct drm_i915_gem_object *obj,\n\t    const struct drm_i915_gem_pread *arg)\n{\n\tif (!i915_gem_object_has_struct_page(obj))\n\t\treturn i915_gem_object_pread_phys(obj, arg);\n\n\treturn -ENODEV;\n}\n\nstatic void shmem_release(struct drm_i915_gem_object *obj)\n{\n\tif (i915_gem_object_has_struct_page(obj))\n\t\ti915_gem_object_release_memory_region(obj);\n\n\tfput(obj->base.filp);\n}\n\nconst struct drm_i915_gem_object_ops i915_gem_shmem_ops = {\n\t.name = \"i915_gem_object_shmem\",\n\t.flags = I915_GEM_OBJECT_IS_SHRINKABLE,\n\n\t.get_pages = shmem_get_pages,\n\t.put_pages = shmem_put_pages,\n\t.truncate = shmem_truncate,\n\t.shrink = shmem_shrink,\n\n\t.pwrite = shmem_pwrite,\n\t.pread = shmem_pread,\n\n\t.release = shmem_release,\n};\n\nstatic int __create_shmem(struct drm_i915_private *i915,\n\t\t\t  struct drm_gem_object *obj,\n\t\t\t  resource_size_t size)\n{\n\tunsigned long flags = VM_NORESERVE;\n\tstruct file *filp;\n\n\tdrm_gem_private_object_init(&i915->drm, obj, size);\n\n\t \n\tif (BITS_PER_LONG == 64 && size > MAX_LFS_FILESIZE)\n\t\treturn -E2BIG;\n\n\tif (i915->mm.gemfs)\n\t\tfilp = shmem_file_setup_with_mnt(i915->mm.gemfs, \"i915\", size,\n\t\t\t\t\t\t flags);\n\telse\n\t\tfilp = shmem_file_setup(\"i915\", size, flags);\n\tif (IS_ERR(filp))\n\t\treturn PTR_ERR(filp);\n\n\tobj->filp = filp;\n\treturn 0;\n}\n\nstatic int shmem_object_init(struct intel_memory_region *mem,\n\t\t\t     struct drm_i915_gem_object *obj,\n\t\t\t     resource_size_t offset,\n\t\t\t     resource_size_t size,\n\t\t\t     resource_size_t page_size,\n\t\t\t     unsigned int flags)\n{\n\tstatic struct lock_class_key lock_class;\n\tstruct drm_i915_private *i915 = mem->i915;\n\tstruct address_space *mapping;\n\tunsigned int cache_level;\n\tgfp_t mask;\n\tint ret;\n\n\tret = __create_shmem(i915, &obj->base, size);\n\tif (ret)\n\t\treturn ret;\n\n\tmask = GFP_HIGHUSER | __GFP_RECLAIMABLE;\n\tif (IS_I965GM(i915) || IS_I965G(i915)) {\n\t\t \n\t\tmask &= ~__GFP_HIGHMEM;\n\t\tmask |= __GFP_DMA32;\n\t}\n\n\tmapping = obj->base.filp->f_mapping;\n\tmapping_set_gfp_mask(mapping, mask);\n\tGEM_BUG_ON(!(mapping_gfp_mask(mapping) & __GFP_RECLAIM));\n\n\ti915_gem_object_init(obj, &i915_gem_shmem_ops, &lock_class, flags);\n\tobj->mem_flags |= I915_BO_FLAG_STRUCT_PAGE;\n\tobj->write_domain = I915_GEM_DOMAIN_CPU;\n\tobj->read_domains = I915_GEM_DOMAIN_CPU;\n\n\t \n\tif (HAS_LLC(i915) || (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70)))\n\t\t \n\t\tcache_level = I915_CACHE_LLC;\n\telse\n\t\tcache_level = I915_CACHE_NONE;\n\n\ti915_gem_object_set_cache_coherency(obj, cache_level);\n\n\ti915_gem_object_init_memory_region(obj, mem);\n\n\treturn 0;\n}\n\nstruct drm_i915_gem_object *\ni915_gem_object_create_shmem(struct drm_i915_private *i915,\n\t\t\t     resource_size_t size)\n{\n\treturn i915_gem_object_create_region(i915->mm.regions[INTEL_REGION_SMEM],\n\t\t\t\t\t     size, 0, 0);\n}\n\n \nstruct drm_i915_gem_object *\ni915_gem_object_create_shmem_from_data(struct drm_i915_private *dev_priv,\n\t\t\t\t       const void *data, resource_size_t size)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct file *file;\n\tconst struct address_space_operations *aops;\n\tresource_size_t offset;\n\tint err;\n\n\tGEM_WARN_ON(IS_DGFX(dev_priv));\n\tobj = i915_gem_object_create_shmem(dev_priv, round_up(size, PAGE_SIZE));\n\tif (IS_ERR(obj))\n\t\treturn obj;\n\n\tGEM_BUG_ON(obj->write_domain != I915_GEM_DOMAIN_CPU);\n\n\tfile = obj->base.filp;\n\taops = file->f_mapping->a_ops;\n\toffset = 0;\n\tdo {\n\t\tunsigned int len = min_t(typeof(size), size, PAGE_SIZE);\n\t\tstruct page *page;\n\t\tvoid *pgdata, *vaddr;\n\n\t\terr = aops->write_begin(file, file->f_mapping, offset, len,\n\t\t\t\t\t&page, &pgdata);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\n\t\tvaddr = kmap(page);\n\t\tmemcpy(vaddr, data, len);\n\t\tkunmap(page);\n\n\t\terr = aops->write_end(file, file->f_mapping, offset, len, len,\n\t\t\t\t      page, pgdata);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\n\t\tsize -= len;\n\t\tdata += len;\n\t\toffset += len;\n\t} while (size);\n\n\treturn obj;\n\nfail:\n\ti915_gem_object_put(obj);\n\treturn ERR_PTR(err);\n}\n\nstatic int init_shmem(struct intel_memory_region *mem)\n{\n\ti915_gemfs_init(mem->i915);\n\tintel_memory_region_set_name(mem, \"system\");\n\n\treturn 0;  \n}\n\nstatic int release_shmem(struct intel_memory_region *mem)\n{\n\ti915_gemfs_fini(mem->i915);\n\treturn 0;\n}\n\nstatic const struct intel_memory_region_ops shmem_region_ops = {\n\t.init = init_shmem,\n\t.release = release_shmem,\n\t.init_object = shmem_object_init,\n};\n\nstruct intel_memory_region *i915_gem_shmem_setup(struct drm_i915_private *i915,\n\t\t\t\t\t\t u16 type, u16 instance)\n{\n\treturn intel_memory_region_create(i915, 0,\n\t\t\t\t\t  totalram_pages() << PAGE_SHIFT,\n\t\t\t\t\t  PAGE_SIZE, 0, 0,\n\t\t\t\t\t  type, instance,\n\t\t\t\t\t  &shmem_region_ops);\n}\n\nbool i915_gem_object_is_shmem(const struct drm_i915_gem_object *obj)\n{\n\treturn obj->ops == &i915_gem_shmem_ops;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}