{
  "module_name": "i915_gem_pages.c",
  "hash_id": "04d574872fa4f124e00b46723a24ef4358952cb3127d9406b600818e57ee070b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gem/i915_gem_pages.c",
  "human_readable_source": " \n\n#include <drm/drm_cache.h>\n\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_tlb.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_gem_object.h\"\n#include \"i915_scatterlist.h\"\n#include \"i915_gem_lmem.h\"\n#include \"i915_gem_mman.h\"\n\nvoid __i915_gem_object_set_pages(struct drm_i915_gem_object *obj,\n\t\t\t\t struct sg_table *pages)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tunsigned long supported = RUNTIME_INFO(i915)->page_sizes;\n\tbool shrinkable;\n\tint i;\n\n\tassert_object_held_shared(obj);\n\n\tif (i915_gem_object_is_volatile(obj))\n\t\tobj->mm.madv = I915_MADV_DONTNEED;\n\n\t \n\tif (obj->cache_dirty) {\n\t\tWARN_ON_ONCE(IS_DGFX(i915));\n\t\tobj->write_domain = 0;\n\t\tif (i915_gem_object_has_struct_page(obj))\n\t\t\tdrm_clflush_sg(pages);\n\t\tobj->cache_dirty = false;\n\t}\n\n\tobj->mm.get_page.sg_pos = pages->sgl;\n\tobj->mm.get_page.sg_idx = 0;\n\tobj->mm.get_dma_page.sg_pos = pages->sgl;\n\tobj->mm.get_dma_page.sg_idx = 0;\n\n\tobj->mm.pages = pages;\n\n\tobj->mm.page_sizes.phys = i915_sg_dma_sizes(pages->sgl);\n\tGEM_BUG_ON(!obj->mm.page_sizes.phys);\n\n\t \n\tobj->mm.page_sizes.sg = 0;\n\tfor_each_set_bit(i, &supported, ilog2(I915_GTT_MAX_PAGE_SIZE) + 1) {\n\t\tif (obj->mm.page_sizes.phys & ~0u << i)\n\t\t\tobj->mm.page_sizes.sg |= BIT(i);\n\t}\n\tGEM_BUG_ON(!HAS_PAGE_SIZES(i915, obj->mm.page_sizes.sg));\n\n\tshrinkable = i915_gem_object_is_shrinkable(obj);\n\n\tif (i915_gem_object_is_tiled(obj) &&\n\t    i915->gem_quirks & GEM_QUIRK_PIN_SWIZZLED_PAGES) {\n\t\tGEM_BUG_ON(i915_gem_object_has_tiling_quirk(obj));\n\t\ti915_gem_object_set_tiling_quirk(obj);\n\t\tGEM_BUG_ON(!list_empty(&obj->mm.link));\n\t\tatomic_inc(&obj->mm.shrink_pin);\n\t\tshrinkable = false;\n\t}\n\n\tif (shrinkable && !i915_gem_object_has_self_managed_shrink_list(obj)) {\n\t\tstruct list_head *list;\n\t\tunsigned long flags;\n\n\t\tassert_object_held(obj);\n\t\tspin_lock_irqsave(&i915->mm.obj_lock, flags);\n\n\t\ti915->mm.shrink_count++;\n\t\ti915->mm.shrink_memory += obj->base.size;\n\n\t\tif (obj->mm.madv != I915_MADV_WILLNEED)\n\t\t\tlist = &i915->mm.purge_list;\n\t\telse\n\t\t\tlist = &i915->mm.shrink_list;\n\t\tlist_add_tail(&obj->mm.link, list);\n\n\t\tatomic_set(&obj->mm.shrink_pin, 0);\n\t\tspin_unlock_irqrestore(&i915->mm.obj_lock, flags);\n\t}\n}\n\nint ____i915_gem_object_get_pages(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tint err;\n\n\tassert_object_held_shared(obj);\n\n\tif (unlikely(obj->mm.madv != I915_MADV_WILLNEED)) {\n\t\tdrm_dbg(&i915->drm,\n\t\t\t\"Attempting to obtain a purgeable object\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\terr = obj->ops->get_pages(obj);\n\tGEM_BUG_ON(!err && !i915_gem_object_has_pages(obj));\n\n\treturn err;\n}\n\n \nint __i915_gem_object_get_pages(struct drm_i915_gem_object *obj)\n{\n\tint err;\n\n\tassert_object_held(obj);\n\n\tassert_object_held_shared(obj);\n\n\tif (unlikely(!i915_gem_object_has_pages(obj))) {\n\t\tGEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));\n\n\t\terr = ____i915_gem_object_get_pages(obj);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tsmp_mb__before_atomic();\n\t}\n\tatomic_inc(&obj->mm.pages_pin_count);\n\n\treturn 0;\n}\n\nint i915_gem_object_pin_pages_unlocked(struct drm_i915_gem_object *obj)\n{\n\tstruct i915_gem_ww_ctx ww;\n\tint err;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\terr = i915_gem_object_lock(obj, &ww);\n\tif (!err)\n\t\terr = i915_gem_object_pin_pages(obj);\n\n\tif (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\treturn err;\n}\n\n \nint i915_gem_object_truncate(struct drm_i915_gem_object *obj)\n{\n\tif (obj->ops->truncate)\n\t\treturn obj->ops->truncate(obj);\n\n\treturn 0;\n}\n\nstatic void __i915_gem_object_reset_page_iter(struct drm_i915_gem_object *obj)\n{\n\tstruct radix_tree_iter iter;\n\tvoid __rcu **slot;\n\n\trcu_read_lock();\n\tradix_tree_for_each_slot(slot, &obj->mm.get_page.radix, &iter, 0)\n\t\tradix_tree_delete(&obj->mm.get_page.radix, iter.index);\n\tradix_tree_for_each_slot(slot, &obj->mm.get_dma_page.radix, &iter, 0)\n\t\tradix_tree_delete(&obj->mm.get_dma_page.radix, iter.index);\n\trcu_read_unlock();\n}\n\nstatic void unmap_object(struct drm_i915_gem_object *obj, void *ptr)\n{\n\tif (is_vmalloc_addr(ptr))\n\t\tvunmap(ptr);\n}\n\nstatic void flush_tlb_invalidate(struct drm_i915_gem_object *obj)\n{\n\tstruct drm_i915_private *i915 = to_i915(obj->base.dev);\n\tstruct intel_gt *gt;\n\tint id;\n\n\tfor_each_gt(gt, i915, id) {\n\t\tif (!obj->mm.tlb[id])\n\t\t\tcontinue;\n\n\t\tintel_gt_invalidate_tlb_full(gt, obj->mm.tlb[id]);\n\t\tobj->mm.tlb[id] = 0;\n\t}\n}\n\nstruct sg_table *\n__i915_gem_object_unset_pages(struct drm_i915_gem_object *obj)\n{\n\tstruct sg_table *pages;\n\n\tassert_object_held_shared(obj);\n\n\tpages = fetch_and_zero(&obj->mm.pages);\n\tif (IS_ERR_OR_NULL(pages))\n\t\treturn pages;\n\n\tif (i915_gem_object_is_volatile(obj))\n\t\tobj->mm.madv = I915_MADV_WILLNEED;\n\n\tif (!i915_gem_object_has_self_managed_shrink_list(obj))\n\t\ti915_gem_object_make_unshrinkable(obj);\n\n\tif (obj->mm.mapping) {\n\t\tunmap_object(obj, page_mask_bits(obj->mm.mapping));\n\t\tobj->mm.mapping = NULL;\n\t}\n\n\t__i915_gem_object_reset_page_iter(obj);\n\tobj->mm.page_sizes.phys = obj->mm.page_sizes.sg = 0;\n\n\tflush_tlb_invalidate(obj);\n\n\treturn pages;\n}\n\nint __i915_gem_object_put_pages(struct drm_i915_gem_object *obj)\n{\n\tstruct sg_table *pages;\n\n\tif (i915_gem_object_has_pinned_pages(obj))\n\t\treturn -EBUSY;\n\n\t \n\tassert_object_held_shared(obj);\n\n\ti915_gem_object_release_mmap_offset(obj);\n\n\t \n\tpages = __i915_gem_object_unset_pages(obj);\n\n\t \n\tif (!IS_ERR_OR_NULL(pages))\n\t\tobj->ops->put_pages(obj, pages);\n\n\treturn 0;\n}\n\n \nstatic void *i915_gem_object_map_page(struct drm_i915_gem_object *obj,\n\t\t\t\t      enum i915_map_type type)\n{\n\tunsigned long n_pages = obj->base.size >> PAGE_SHIFT, i;\n\tstruct page *stack[32], **pages = stack, *page;\n\tstruct sgt_iter iter;\n\tpgprot_t pgprot;\n\tvoid *vaddr;\n\n\tswitch (type) {\n\tdefault:\n\t\tMISSING_CASE(type);\n\t\tfallthrough;\t \n\tcase I915_MAP_WB:\n\t\t \n\t\tif (n_pages == 1 && !PageHighMem(sg_page(obj->mm.pages->sgl)))\n\t\t\treturn page_address(sg_page(obj->mm.pages->sgl));\n\t\tpgprot = PAGE_KERNEL;\n\t\tbreak;\n\tcase I915_MAP_WC:\n\t\tpgprot = pgprot_writecombine(PAGE_KERNEL_IO);\n\t\tbreak;\n\t}\n\n\tif (n_pages > ARRAY_SIZE(stack)) {\n\t\t \n\t\tpages = kvmalloc_array(n_pages, sizeof(*pages), GFP_KERNEL);\n\t\tif (!pages)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\ti = 0;\n\tfor_each_sgt_page(page, iter, obj->mm.pages)\n\t\tpages[i++] = page;\n\tvaddr = vmap(pages, n_pages, 0, pgprot);\n\tif (pages != stack)\n\t\tkvfree(pages);\n\n\treturn vaddr ?: ERR_PTR(-ENOMEM);\n}\n\nstatic void *i915_gem_object_map_pfn(struct drm_i915_gem_object *obj,\n\t\t\t\t     enum i915_map_type type)\n{\n\tresource_size_t iomap = obj->mm.region->iomap.base -\n\t\tobj->mm.region->region.start;\n\tunsigned long n_pfn = obj->base.size >> PAGE_SHIFT;\n\tunsigned long stack[32], *pfns = stack, i;\n\tstruct sgt_iter iter;\n\tdma_addr_t addr;\n\tvoid *vaddr;\n\n\tGEM_BUG_ON(type != I915_MAP_WC);\n\n\tif (n_pfn > ARRAY_SIZE(stack)) {\n\t\t \n\t\tpfns = kvmalloc_array(n_pfn, sizeof(*pfns), GFP_KERNEL);\n\t\tif (!pfns)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\ti = 0;\n\tfor_each_sgt_daddr(addr, iter, obj->mm.pages)\n\t\tpfns[i++] = (iomap + addr) >> PAGE_SHIFT;\n\tvaddr = vmap_pfn(pfns, n_pfn, pgprot_writecombine(PAGE_KERNEL_IO));\n\tif (pfns != stack)\n\t\tkvfree(pfns);\n\n\treturn vaddr ?: ERR_PTR(-ENOMEM);\n}\n\n \nvoid *i915_gem_object_pin_map(struct drm_i915_gem_object *obj,\n\t\t\t      enum i915_map_type type)\n{\n\tenum i915_map_type has_type;\n\tbool pinned;\n\tvoid *ptr;\n\tint err;\n\n\tif (!i915_gem_object_has_struct_page(obj) &&\n\t    !i915_gem_object_has_iomem(obj))\n\t\treturn ERR_PTR(-ENXIO);\n\n\tif (WARN_ON_ONCE(obj->flags & I915_BO_ALLOC_GPU_ONLY))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tassert_object_held(obj);\n\n\tpinned = !(type & I915_MAP_OVERRIDE);\n\ttype &= ~I915_MAP_OVERRIDE;\n\n\tif (!atomic_inc_not_zero(&obj->mm.pages_pin_count)) {\n\t\tif (unlikely(!i915_gem_object_has_pages(obj))) {\n\t\t\tGEM_BUG_ON(i915_gem_object_has_pinned_pages(obj));\n\n\t\t\terr = ____i915_gem_object_get_pages(obj);\n\t\t\tif (err)\n\t\t\t\treturn ERR_PTR(err);\n\n\t\t\tsmp_mb__before_atomic();\n\t\t}\n\t\tatomic_inc(&obj->mm.pages_pin_count);\n\t\tpinned = false;\n\t}\n\tGEM_BUG_ON(!i915_gem_object_has_pages(obj));\n\n\t \n\tif (i915_gem_object_placement_possible(obj, INTEL_MEMORY_LOCAL)) {\n\t\tif (type != I915_MAP_WC && !obj->mm.n_placements) {\n\t\t\tptr = ERR_PTR(-ENODEV);\n\t\t\tgoto err_unpin;\n\t\t}\n\n\t\ttype = I915_MAP_WC;\n\t} else if (IS_DGFX(to_i915(obj->base.dev))) {\n\t\ttype = I915_MAP_WB;\n\t}\n\n\tptr = page_unpack_bits(obj->mm.mapping, &has_type);\n\tif (ptr && has_type != type) {\n\t\tif (pinned) {\n\t\t\tptr = ERR_PTR(-EBUSY);\n\t\t\tgoto err_unpin;\n\t\t}\n\n\t\tunmap_object(obj, ptr);\n\n\t\tptr = obj->mm.mapping = NULL;\n\t}\n\n\tif (!ptr) {\n\t\terr = i915_gem_object_wait_moving_fence(obj, true);\n\t\tif (err) {\n\t\t\tptr = ERR_PTR(err);\n\t\t\tgoto err_unpin;\n\t\t}\n\n\t\tif (GEM_WARN_ON(type == I915_MAP_WC && !pat_enabled()))\n\t\t\tptr = ERR_PTR(-ENODEV);\n\t\telse if (i915_gem_object_has_struct_page(obj))\n\t\t\tptr = i915_gem_object_map_page(obj, type);\n\t\telse\n\t\t\tptr = i915_gem_object_map_pfn(obj, type);\n\t\tif (IS_ERR(ptr))\n\t\t\tgoto err_unpin;\n\n\t\tobj->mm.mapping = page_pack_bits(ptr, type);\n\t}\n\n\treturn ptr;\n\nerr_unpin:\n\tatomic_dec(&obj->mm.pages_pin_count);\n\treturn ptr;\n}\n\nvoid *i915_gem_object_pin_map_unlocked(struct drm_i915_gem_object *obj,\n\t\t\t\t       enum i915_map_type type)\n{\n\tvoid *ret;\n\n\ti915_gem_object_lock(obj, NULL);\n\tret = i915_gem_object_pin_map(obj, type);\n\ti915_gem_object_unlock(obj);\n\n\treturn ret;\n}\n\nvoid __i915_gem_object_flush_map(struct drm_i915_gem_object *obj,\n\t\t\t\t unsigned long offset,\n\t\t\t\t unsigned long size)\n{\n\tenum i915_map_type has_type;\n\tvoid *ptr;\n\n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));\n\tGEM_BUG_ON(range_overflows_t(typeof(obj->base.size),\n\t\t\t\t     offset, size, obj->base.size));\n\n\twmb();  \n\tobj->mm.dirty = true;\n\n\tif (obj->cache_coherent & I915_BO_CACHE_COHERENT_FOR_WRITE)\n\t\treturn;\n\n\tptr = page_unpack_bits(obj->mm.mapping, &has_type);\n\tif (has_type == I915_MAP_WC)\n\t\treturn;\n\n\tdrm_clflush_virt_range(ptr + offset, size);\n\tif (size == obj->base.size) {\n\t\tobj->write_domain &= ~I915_GEM_DOMAIN_CPU;\n\t\tobj->cache_dirty = false;\n\t}\n}\n\nvoid __i915_gem_object_release_map(struct drm_i915_gem_object *obj)\n{\n\tGEM_BUG_ON(!obj->mm.mapping);\n\n\t \n\tunmap_object(obj, page_mask_bits(fetch_and_zero(&obj->mm.mapping)));\n\n\ti915_gem_object_unpin_map(obj);\n}\n\nstruct scatterlist *\n__i915_gem_object_page_iter_get_sg(struct drm_i915_gem_object *obj,\n\t\t\t\t   struct i915_gem_object_page_iter *iter,\n\t\t\t\t   pgoff_t n,\n\t\t\t\t   unsigned int *offset)\n\n{\n\tconst bool dma = iter == &obj->mm.get_dma_page ||\n\t\t\t iter == &obj->ttm.get_io_page;\n\tunsigned int idx, count;\n\tstruct scatterlist *sg;\n\n\tmight_sleep();\n\tGEM_BUG_ON(n >= obj->base.size >> PAGE_SHIFT);\n\tif (!i915_gem_object_has_pinned_pages(obj))\n\t\tassert_object_held(obj);\n\n\t \n\tif (n < READ_ONCE(iter->sg_idx))\n\t\tgoto lookup;\n\n\tmutex_lock(&iter->lock);\n\n\t \n\n\tsg = iter->sg_pos;\n\tidx = iter->sg_idx;\n\tcount = dma ? __sg_dma_page_count(sg) : __sg_page_count(sg);\n\n\twhile (idx + count <= n) {\n\t\tvoid *entry;\n\t\tunsigned long i;\n\t\tint ret;\n\n\t\t \n\t\tret = radix_tree_insert(&iter->radix, idx, sg);\n\t\tif (ret && ret != -EEXIST)\n\t\t\tgoto scan;\n\n\t\tentry = xa_mk_value(idx);\n\t\tfor (i = 1; i < count; i++) {\n\t\t\tret = radix_tree_insert(&iter->radix, idx + i, entry);\n\t\t\tif (ret && ret != -EEXIST)\n\t\t\t\tgoto scan;\n\t\t}\n\n\t\tidx += count;\n\t\tsg = ____sg_next(sg);\n\t\tcount = dma ? __sg_dma_page_count(sg) : __sg_page_count(sg);\n\t}\n\nscan:\n\titer->sg_pos = sg;\n\titer->sg_idx = idx;\n\n\tmutex_unlock(&iter->lock);\n\n\tif (unlikely(n < idx))  \n\t\tgoto lookup;\n\n\t \n\twhile (idx + count <= n) {\n\t\tidx += count;\n\t\tsg = ____sg_next(sg);\n\t\tcount = dma ? __sg_dma_page_count(sg) : __sg_page_count(sg);\n\t}\n\n\t*offset = n - idx;\n\treturn sg;\n\nlookup:\n\trcu_read_lock();\n\n\tsg = radix_tree_lookup(&iter->radix, n);\n\tGEM_BUG_ON(!sg);\n\n\t \n\t*offset = 0;\n\tif (unlikely(xa_is_value(sg))) {\n\t\tunsigned long base = xa_to_value(sg);\n\n\t\tsg = radix_tree_lookup(&iter->radix, base);\n\t\tGEM_BUG_ON(!sg);\n\n\t\t*offset = n - base;\n\t}\n\n\trcu_read_unlock();\n\n\treturn sg;\n}\n\nstruct page *\n__i915_gem_object_get_page(struct drm_i915_gem_object *obj, pgoff_t n)\n{\n\tstruct scatterlist *sg;\n\tunsigned int offset;\n\n\tGEM_BUG_ON(!i915_gem_object_has_struct_page(obj));\n\n\tsg = i915_gem_object_get_sg(obj, n, &offset);\n\treturn nth_page(sg_page(sg), offset);\n}\n\n \nstruct page *\n__i915_gem_object_get_dirty_page(struct drm_i915_gem_object *obj, pgoff_t n)\n{\n\tstruct page *page;\n\n\tpage = i915_gem_object_get_page(obj, n);\n\tif (!obj->mm.dirty)\n\t\tset_page_dirty(page);\n\n\treturn page;\n}\n\ndma_addr_t\n__i915_gem_object_get_dma_address_len(struct drm_i915_gem_object *obj,\n\t\t\t\t      pgoff_t n, unsigned int *len)\n{\n\tstruct scatterlist *sg;\n\tunsigned int offset;\n\n\tsg = i915_gem_object_get_sg_dma(obj, n, &offset);\n\n\tif (len)\n\t\t*len = sg_dma_len(sg) - (offset << PAGE_SHIFT);\n\n\treturn sg_dma_address(sg) + (offset << PAGE_SHIFT);\n}\n\ndma_addr_t\n__i915_gem_object_get_dma_address(struct drm_i915_gem_object *obj, pgoff_t n)\n{\n\treturn i915_gem_object_get_dma_address_len(obj, n, NULL);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}