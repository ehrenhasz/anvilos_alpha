{
  "module_name": "intel_migrate.c",
  "hash_id": "8e530180c665e6db45e8424ef39ab5a6d104a0dacb3527d621797d25d1752b00",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_migrate.c",
  "human_readable_source": "\n \n\n#include \"i915_drv.h\"\n#include \"intel_context.h\"\n#include \"intel_gpu_commands.h\"\n#include \"intel_gt.h\"\n#include \"intel_gtt.h\"\n#include \"intel_migrate.h\"\n#include \"intel_ring.h\"\n#include \"gem/i915_gem_lmem.h\"\n\nstruct insert_pte_data {\n\tu64 offset;\n};\n\n#define CHUNK_SZ SZ_8M  \n\n#define GET_CCS_BYTES(i915, size)\t(HAS_FLAT_CCS(i915) ? \\\n\t\t\t\t\t DIV_ROUND_UP(size, NUM_BYTES_PER_CCS_BYTE) : 0)\nstatic bool engine_supports_migration(struct intel_engine_cs *engine)\n{\n\tif (!engine)\n\t\treturn false;\n\n\t \n\tGEM_BUG_ON(engine->class != COPY_ENGINE_CLASS);\n\n\treturn true;\n}\n\nstatic void xehpsdv_toggle_pdes(struct i915_address_space *vm,\n\t\t\t\tstruct i915_page_table *pt,\n\t\t\t\tvoid *data)\n{\n\tstruct insert_pte_data *d = data;\n\n\t \n\tvm->insert_page(vm, 0, d->offset,\n\t\t\ti915_gem_get_pat_index(vm->i915, I915_CACHE_NONE),\n\t\t\tPTE_LM);\n\tGEM_BUG_ON(!pt->is_compact);\n\td->offset += SZ_2M;\n}\n\nstatic void xehpsdv_insert_pte(struct i915_address_space *vm,\n\t\t\t       struct i915_page_table *pt,\n\t\t\t       void *data)\n{\n\tstruct insert_pte_data *d = data;\n\n\t \n\tvm->insert_page(vm, px_dma(pt), d->offset,\n\t\t\ti915_gem_get_pat_index(vm->i915, I915_CACHE_NONE),\n\t\t\tPTE_LM);\n\td->offset += SZ_64K;\n}\n\nstatic void insert_pte(struct i915_address_space *vm,\n\t\t       struct i915_page_table *pt,\n\t\t       void *data)\n{\n\tstruct insert_pte_data *d = data;\n\n\tvm->insert_page(vm, px_dma(pt), d->offset,\n\t\t\ti915_gem_get_pat_index(vm->i915, I915_CACHE_NONE),\n\t\t\ti915_gem_object_is_lmem(pt->base) ? PTE_LM : 0);\n\td->offset += PAGE_SIZE;\n}\n\nstatic struct i915_address_space *migrate_vm(struct intel_gt *gt)\n{\n\tstruct i915_vm_pt_stash stash = {};\n\tstruct i915_ppgtt *vm;\n\tint err;\n\tint i;\n\n\t \n\n\tvm = i915_ppgtt_create(gt, I915_BO_ALLOC_PM_EARLY);\n\tif (IS_ERR(vm))\n\t\treturn ERR_CAST(vm);\n\n\tif (!vm->vm.allocate_va_range || !vm->vm.foreach) {\n\t\terr = -ENODEV;\n\t\tgoto err_vm;\n\t}\n\n\tif (HAS_64K_PAGES(gt->i915))\n\t\tstash.pt_sz = I915_GTT_PAGE_SIZE_64K;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(gt->engine_class[COPY_ENGINE_CLASS]); i++) {\n\t\tstruct intel_engine_cs *engine;\n\t\tu64 base = (u64)i << 32;\n\t\tstruct insert_pte_data d = {};\n\t\tstruct i915_gem_ww_ctx ww;\n\t\tu64 sz;\n\n\t\tengine = gt->engine_class[COPY_ENGINE_CLASS][i];\n\t\tif (!engine_supports_migration(engine))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (HAS_64K_PAGES(gt->i915))\n\t\t\tsz = 3 * CHUNK_SZ;\n\t\telse\n\t\t\tsz = 2 * CHUNK_SZ;\n\t\td.offset = base + sz;\n\n\t\t \n\t\tif (HAS_64K_PAGES(gt->i915))\n\t\t\tsz += (sz / SZ_2M) * SZ_64K;\n\t\telse\n\t\t\tsz += (sz >> 12) * sizeof(u64);\n\n\t\terr = i915_vm_alloc_pt_stash(&vm->vm, &stash, sz);\n\t\tif (err)\n\t\t\tgoto err_vm;\n\n\t\tfor_i915_gem_ww(&ww, err, true) {\n\t\t\terr = i915_vm_lock_objects(&vm->vm, &ww);\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\t\t\terr = i915_vm_map_pt_stash(&vm->vm, &stash);\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\n\t\t\tvm->vm.allocate_va_range(&vm->vm, &stash, base, sz);\n\t\t}\n\t\ti915_vm_free_pt_stash(&vm->vm, &stash);\n\t\tif (err)\n\t\t\tgoto err_vm;\n\n\t\t \n\t\tif (HAS_64K_PAGES(gt->i915)) {\n\t\t\tvm->vm.foreach(&vm->vm, base, d.offset - base,\n\t\t\t\t       xehpsdv_insert_pte, &d);\n\t\t\td.offset = base + CHUNK_SZ;\n\t\t\tvm->vm.foreach(&vm->vm,\n\t\t\t\t       d.offset,\n\t\t\t\t       2 * CHUNK_SZ,\n\t\t\t\t       xehpsdv_toggle_pdes, &d);\n\t\t} else {\n\t\t\tvm->vm.foreach(&vm->vm, base, d.offset - base,\n\t\t\t\t       insert_pte, &d);\n\t\t}\n\t}\n\n\treturn &vm->vm;\n\nerr_vm:\n\ti915_vm_put(&vm->vm);\n\treturn ERR_PTR(err);\n}\n\nstatic struct intel_engine_cs *first_copy_engine(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(gt->engine_class[COPY_ENGINE_CLASS]); i++) {\n\t\tengine = gt->engine_class[COPY_ENGINE_CLASS][i];\n\t\tif (engine_supports_migration(engine))\n\t\t\treturn engine;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct intel_context *pinned_context(struct intel_gt *gt)\n{\n\tstatic struct lock_class_key key;\n\tstruct intel_engine_cs *engine;\n\tstruct i915_address_space *vm;\n\tstruct intel_context *ce;\n\n\tengine = first_copy_engine(gt);\n\tif (!engine)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tvm = migrate_vm(gt);\n\tif (IS_ERR(vm))\n\t\treturn ERR_CAST(vm);\n\n\tce = intel_engine_create_pinned_context(engine, vm, SZ_512K,\n\t\t\t\t\t\tI915_GEM_HWS_MIGRATE,\n\t\t\t\t\t\t&key, \"migrate\");\n\ti915_vm_put(vm);\n\treturn ce;\n}\n\nint intel_migrate_init(struct intel_migrate *m, struct intel_gt *gt)\n{\n\tstruct intel_context *ce;\n\n\tmemset(m, 0, sizeof(*m));\n\n\tce = pinned_context(gt);\n\tif (IS_ERR(ce))\n\t\treturn PTR_ERR(ce);\n\n\tm->context = ce;\n\treturn 0;\n}\n\nstatic int random_index(unsigned int max)\n{\n\treturn upper_32_bits(mul_u32_u32(get_random_u32(), max));\n}\n\nstatic struct intel_context *__migrate_engines(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engines[MAX_ENGINE_INSTANCE];\n\tstruct intel_engine_cs *engine;\n\tunsigned int count, i;\n\n\tcount = 0;\n\tfor (i = 0; i < ARRAY_SIZE(gt->engine_class[COPY_ENGINE_CLASS]); i++) {\n\t\tengine = gt->engine_class[COPY_ENGINE_CLASS][i];\n\t\tif (engine_supports_migration(engine))\n\t\t\tengines[count++] = engine;\n\t}\n\n\treturn intel_context_create(engines[random_index(count)]);\n}\n\nstruct intel_context *intel_migrate_create_context(struct intel_migrate *m)\n{\n\tstruct intel_context *ce;\n\n\t \n\tce = __migrate_engines(m->context->engine->gt);\n\tif (IS_ERR(ce))\n\t\treturn ce;\n\n\tce->ring = NULL;\n\tce->ring_size = SZ_256K;\n\n\ti915_vm_put(ce->vm);\n\tce->vm = i915_vm_get(m->context->vm);\n\n\treturn ce;\n}\n\nstatic inline struct sgt_dma sg_sgt(struct scatterlist *sg)\n{\n\tdma_addr_t addr = sg_dma_address(sg);\n\n\treturn (struct sgt_dma){ sg, addr, addr + sg_dma_len(sg) };\n}\n\nstatic int emit_no_arbitration(struct i915_request *rq)\n{\n\tu32 *cs;\n\n\tcs = intel_ring_begin(rq, 2);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\t*cs++ = MI_ARB_ON_OFF;\n\t*cs++ = MI_NOOP;\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic int max_pte_pkt_size(struct i915_request *rq, int pkt)\n{\n\tstruct intel_ring *ring = rq->ring;\n\n\tpkt = min_t(int, pkt, (ring->space - rq->reserved_space) / sizeof(u32) + 5);\n\tpkt = min_t(int, pkt, (ring->size - ring->emit) / sizeof(u32) + 5);\n\n\treturn pkt;\n}\n\n#define I915_EMIT_PTE_NUM_DWORDS 6\n\nstatic int emit_pte(struct i915_request *rq,\n\t\t    struct sgt_dma *it,\n\t\t    unsigned int pat_index,\n\t\t    bool is_lmem,\n\t\t    u64 offset,\n\t\t    int length)\n{\n\tbool has_64K_pages = HAS_64K_PAGES(rq->i915);\n\tconst u64 encode = rq->context->vm->pte_encode(0, pat_index,\n\t\t\t\t\t\t       is_lmem ? PTE_LM : 0);\n\tstruct intel_ring *ring = rq->ring;\n\tint pkt, dword_length;\n\tu32 total = 0;\n\tu32 page_size;\n\tu32 *hdr, *cs;\n\n\tGEM_BUG_ON(GRAPHICS_VER(rq->i915) < 8);\n\n\tpage_size = I915_GTT_PAGE_SIZE;\n\tdword_length = 0x400;\n\n\t \n\tif (has_64K_pages) {\n\t\tGEM_BUG_ON(!IS_ALIGNED(offset, SZ_2M));\n\n\t\toffset /= SZ_2M;\n\t\toffset *= SZ_64K;\n\t\toffset += 3 * CHUNK_SZ;\n\n\t\tif (is_lmem) {\n\t\t\tpage_size = I915_GTT_PAGE_SIZE_64K;\n\t\t\tdword_length = 0x40;\n\t\t}\n\t} else {\n\t\toffset >>= 12;\n\t\toffset *= sizeof(u64);\n\t\toffset += 2 * CHUNK_SZ;\n\t}\n\n\toffset += (u64)rq->engine->instance << 32;\n\n\tcs = intel_ring_begin(rq, I915_EMIT_PTE_NUM_DWORDS);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\tpkt = max_pte_pkt_size(rq, dword_length);\n\n\thdr = cs;\n\t*cs++ = MI_STORE_DATA_IMM | REG_BIT(21);  \n\t*cs++ = lower_32_bits(offset);\n\t*cs++ = upper_32_bits(offset);\n\n\tdo {\n\t\tif (cs - hdr >= pkt) {\n\t\t\tint dword_rem;\n\n\t\t\t*hdr += cs - hdr - 2;\n\t\t\t*cs++ = MI_NOOP;\n\n\t\t\tring->emit = (void *)cs - ring->vaddr;\n\t\t\tintel_ring_advance(rq, cs);\n\t\t\tintel_ring_update_space(ring);\n\n\t\t\tcs = intel_ring_begin(rq, I915_EMIT_PTE_NUM_DWORDS);\n\t\t\tif (IS_ERR(cs))\n\t\t\t\treturn PTR_ERR(cs);\n\n\t\t\tdword_rem = dword_length;\n\t\t\tif (has_64K_pages) {\n\t\t\t\tif (IS_ALIGNED(total, SZ_2M)) {\n\t\t\t\t\toffset = round_up(offset, SZ_64K);\n\t\t\t\t} else {\n\t\t\t\t\tdword_rem = SZ_2M - (total & (SZ_2M - 1));\n\t\t\t\t\tdword_rem /= page_size;\n\t\t\t\t\tdword_rem *= 2;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tpkt = max_pte_pkt_size(rq, dword_rem);\n\n\t\t\thdr = cs;\n\t\t\t*cs++ = MI_STORE_DATA_IMM | REG_BIT(21);\n\t\t\t*cs++ = lower_32_bits(offset);\n\t\t\t*cs++ = upper_32_bits(offset);\n\t\t}\n\n\t\tGEM_BUG_ON(!IS_ALIGNED(it->dma, page_size));\n\n\t\t*cs++ = lower_32_bits(encode | it->dma);\n\t\t*cs++ = upper_32_bits(encode | it->dma);\n\n\t\toffset += 8;\n\t\ttotal += page_size;\n\n\t\tit->dma += page_size;\n\t\tif (it->dma >= it->max) {\n\t\t\tit->sg = __sg_next(it->sg);\n\t\t\tif (!it->sg || sg_dma_len(it->sg) == 0)\n\t\t\t\tbreak;\n\n\t\t\tit->dma = sg_dma_address(it->sg);\n\t\t\tit->max = it->dma + sg_dma_len(it->sg);\n\t\t}\n\t} while (total < length);\n\n\t*hdr += cs - hdr - 2;\n\t*cs++ = MI_NOOP;\n\n\tring->emit = (void *)cs - ring->vaddr;\n\tintel_ring_advance(rq, cs);\n\tintel_ring_update_space(ring);\n\n\treturn total;\n}\n\nstatic bool wa_1209644611_applies(int ver, u32 size)\n{\n\tu32 height = size >> PAGE_SHIFT;\n\n\tif (ver != 11)\n\t\treturn false;\n\n\treturn height % 4 == 3 && height <= 8;\n}\n\n \n\nstatic inline u32 *i915_flush_dw(u32 *cmd, u32 flags)\n{\n\t*cmd++ = MI_FLUSH_DW | flags;\n\t*cmd++ = 0;\n\t*cmd++ = 0;\n\n\treturn cmd;\n}\n\nstatic int emit_copy_ccs(struct i915_request *rq,\n\t\t\t u32 dst_offset, u8 dst_access,\n\t\t\t u32 src_offset, u8 src_access, int size)\n{\n\tstruct drm_i915_private *i915 = rq->i915;\n\tint mocs = rq->engine->gt->mocs.uc_index << 1;\n\tu32 num_ccs_blks;\n\tu32 *cs;\n\n\tcs = intel_ring_begin(rq, 12);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\tnum_ccs_blks = DIV_ROUND_UP(GET_CCS_BYTES(i915, size),\n\t\t\t\t    NUM_CCS_BYTES_PER_BLOCK);\n\tGEM_BUG_ON(num_ccs_blks > NUM_CCS_BLKS_PER_XFER);\n\tcs = i915_flush_dw(cs, MI_FLUSH_DW_LLC | MI_FLUSH_DW_CCS);\n\n\t \n\t*cs++ = XY_CTRL_SURF_COPY_BLT |\n\t\tsrc_access << SRC_ACCESS_TYPE_SHIFT |\n\t\tdst_access << DST_ACCESS_TYPE_SHIFT |\n\t\t((num_ccs_blks - 1) & CCS_SIZE_MASK) << CCS_SIZE_SHIFT;\n\t*cs++ = src_offset;\n\t*cs++ = rq->engine->instance |\n\t\tFIELD_PREP(XY_CTRL_SURF_MOCS_MASK, mocs);\n\t*cs++ = dst_offset;\n\t*cs++ = rq->engine->instance |\n\t\tFIELD_PREP(XY_CTRL_SURF_MOCS_MASK, mocs);\n\n\tcs = i915_flush_dw(cs, MI_FLUSH_DW_LLC | MI_FLUSH_DW_CCS);\n\t*cs++ = MI_NOOP;\n\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic int emit_copy(struct i915_request *rq,\n\t\t     u32 dst_offset, u32 src_offset, int size)\n{\n\tconst int ver = GRAPHICS_VER(rq->i915);\n\tu32 instance = rq->engine->instance;\n\tu32 *cs;\n\n\tcs = intel_ring_begin(rq, ver >= 8 ? 10 : 6);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\tif (ver >= 9 && !wa_1209644611_applies(ver, size)) {\n\t\t*cs++ = GEN9_XY_FAST_COPY_BLT_CMD | (10 - 2);\n\t\t*cs++ = BLT_DEPTH_32 | PAGE_SIZE;\n\t\t*cs++ = 0;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE / 4;\n\t\t*cs++ = dst_offset;\n\t\t*cs++ = instance;\n\t\t*cs++ = 0;\n\t\t*cs++ = PAGE_SIZE;\n\t\t*cs++ = src_offset;\n\t\t*cs++ = instance;\n\t} else if (ver >= 8) {\n\t\t*cs++ = XY_SRC_COPY_BLT_CMD | BLT_WRITE_RGBA | (10 - 2);\n\t\t*cs++ = BLT_DEPTH_32 | BLT_ROP_SRC_COPY | PAGE_SIZE;\n\t\t*cs++ = 0;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE / 4;\n\t\t*cs++ = dst_offset;\n\t\t*cs++ = instance;\n\t\t*cs++ = 0;\n\t\t*cs++ = PAGE_SIZE;\n\t\t*cs++ = src_offset;\n\t\t*cs++ = instance;\n\t} else {\n\t\tGEM_BUG_ON(instance);\n\t\t*cs++ = SRC_COPY_BLT_CMD | BLT_WRITE_RGBA | (6 - 2);\n\t\t*cs++ = BLT_DEPTH_32 | BLT_ROP_SRC_COPY | PAGE_SIZE;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE;\n\t\t*cs++ = dst_offset;\n\t\t*cs++ = PAGE_SIZE;\n\t\t*cs++ = src_offset;\n\t}\n\n\tintel_ring_advance(rq, cs);\n\treturn 0;\n}\n\nstatic u64 scatter_list_length(struct scatterlist *sg)\n{\n\tu64 len = 0;\n\n\twhile (sg && sg_dma_len(sg)) {\n\t\tlen += sg_dma_len(sg);\n\t\tsg = sg_next(sg);\n\t}\n\n\treturn len;\n}\n\nstatic int\ncalculate_chunk_sz(struct drm_i915_private *i915, bool src_is_lmem,\n\t\t   u64 bytes_to_cpy, u64 ccs_bytes_to_cpy)\n{\n\tif (ccs_bytes_to_cpy && !src_is_lmem)\n\t\t \n\t\treturn min_t(u64, bytes_to_cpy, CHUNK_SZ);\n\telse\n\t\treturn CHUNK_SZ;\n}\n\nstatic void get_ccs_sg_sgt(struct sgt_dma *it, u64 bytes_to_cpy)\n{\n\tu64 len;\n\n\tdo {\n\t\tGEM_BUG_ON(!it->sg || !sg_dma_len(it->sg));\n\t\tlen = it->max - it->dma;\n\t\tif (len > bytes_to_cpy) {\n\t\t\tit->dma += bytes_to_cpy;\n\t\t\tbreak;\n\t\t}\n\n\t\tbytes_to_cpy -= len;\n\n\t\tit->sg = __sg_next(it->sg);\n\t\tit->dma = sg_dma_address(it->sg);\n\t\tit->max = it->dma + sg_dma_len(it->sg);\n\t} while (bytes_to_cpy);\n}\n\nint\nintel_context_migrate_copy(struct intel_context *ce,\n\t\t\t   const struct i915_deps *deps,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int src_pat_index,\n\t\t\t   bool src_is_lmem,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   unsigned int dst_pat_index,\n\t\t\t   bool dst_is_lmem,\n\t\t\t   struct i915_request **out)\n{\n\tstruct sgt_dma it_src = sg_sgt(src), it_dst = sg_sgt(dst), it_ccs;\n\tstruct drm_i915_private *i915 = ce->engine->i915;\n\tu64 ccs_bytes_to_cpy = 0, bytes_to_cpy;\n\tunsigned int ccs_pat_index;\n\tu32 src_offset, dst_offset;\n\tu8 src_access, dst_access;\n\tstruct i915_request *rq;\n\tu64 src_sz, dst_sz;\n\tbool ccs_is_src, overwrite_ccs;\n\tint err;\n\n\tGEM_BUG_ON(ce->vm != ce->engine->gt->migrate.context->vm);\n\tGEM_BUG_ON(IS_DGFX(ce->engine->i915) && (!src_is_lmem && !dst_is_lmem));\n\t*out = NULL;\n\n\tGEM_BUG_ON(ce->ring->size < SZ_64K);\n\n\tsrc_sz = scatter_list_length(src);\n\tbytes_to_cpy = src_sz;\n\n\tif (HAS_FLAT_CCS(i915) && src_is_lmem ^ dst_is_lmem) {\n\t\tsrc_access = !src_is_lmem && dst_is_lmem;\n\t\tdst_access = !src_access;\n\n\t\tdst_sz = scatter_list_length(dst);\n\t\tif (src_is_lmem) {\n\t\t\tit_ccs = it_dst;\n\t\t\tccs_pat_index = dst_pat_index;\n\t\t\tccs_is_src = false;\n\t\t} else if (dst_is_lmem) {\n\t\t\tbytes_to_cpy = dst_sz;\n\t\t\tit_ccs = it_src;\n\t\t\tccs_pat_index = src_pat_index;\n\t\t\tccs_is_src = true;\n\t\t}\n\n\t\t \n\t\tccs_bytes_to_cpy = src_sz != dst_sz ? GET_CCS_BYTES(i915, bytes_to_cpy) : 0;\n\t\tif (ccs_bytes_to_cpy)\n\t\t\tget_ccs_sg_sgt(&it_ccs, bytes_to_cpy);\n\t}\n\n\toverwrite_ccs = HAS_FLAT_CCS(i915) && !ccs_bytes_to_cpy && dst_is_lmem;\n\n\tsrc_offset = 0;\n\tdst_offset = CHUNK_SZ;\n\tif (HAS_64K_PAGES(ce->engine->i915)) {\n\t\tsrc_offset = 0;\n\t\tdst_offset = 0;\n\t\tif (src_is_lmem)\n\t\t\tsrc_offset = CHUNK_SZ;\n\t\tif (dst_is_lmem)\n\t\t\tdst_offset = 2 * CHUNK_SZ;\n\t}\n\n\tdo {\n\t\tint len;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_ce;\n\t\t}\n\n\t\tif (deps) {\n\t\t\terr = i915_request_await_deps(rq, deps);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\tif (rq->engine->emit_init_breadcrumb) {\n\t\t\t\terr = rq->engine->emit_init_breadcrumb(rq);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_rq;\n\t\t\t}\n\n\t\t\tdeps = NULL;\n\t\t}\n\n\t\t \n\t\terr = emit_no_arbitration(rq);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\tsrc_sz = calculate_chunk_sz(i915, src_is_lmem,\n\t\t\t\t\t    bytes_to_cpy, ccs_bytes_to_cpy);\n\n\t\tlen = emit_pte(rq, &it_src, src_pat_index, src_is_lmem,\n\t\t\t       src_offset, src_sz);\n\t\tif (!len) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_rq;\n\t\t}\n\t\tif (len < 0) {\n\t\t\terr = len;\n\t\t\tgoto out_rq;\n\t\t}\n\n\t\terr = emit_pte(rq, &it_dst, dst_pat_index, dst_is_lmem,\n\t\t\t       dst_offset, len);\n\t\tif (err < 0)\n\t\t\tgoto out_rq;\n\t\tif (err < len) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out_rq;\n\t\t}\n\n\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\terr = emit_copy(rq, dst_offset,\tsrc_offset, len);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\tbytes_to_cpy -= len;\n\n\t\tif (ccs_bytes_to_cpy) {\n\t\t\tint ccs_sz;\n\n\t\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\tccs_sz = GET_CCS_BYTES(i915, len);\n\t\t\terr = emit_pte(rq, &it_ccs, ccs_pat_index, false,\n\t\t\t\t       ccs_is_src ? src_offset : dst_offset,\n\t\t\t\t       ccs_sz);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_rq;\n\t\t\tif (err < ccs_sz) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_rq;\n\t\t\t}\n\n\t\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\terr = emit_copy_ccs(rq, dst_offset, dst_access,\n\t\t\t\t\t    src_offset, src_access, len);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\t\t\tccs_bytes_to_cpy -= ccs_sz;\n\t\t} else if (overwrite_ccs) {\n\t\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\tif (src_is_lmem) {\n\t\t\t\t \n\t\t\t\terr = emit_copy_ccs(rq,\n\t\t\t\t\t\t    dst_offset, INDIRECT_ACCESS,\n\t\t\t\t\t\t    src_offset, INDIRECT_ACCESS,\n\t\t\t\t\t\t    len);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\terr = emit_copy_ccs(rq,\n\t\t\t\t\t\t    dst_offset, INDIRECT_ACCESS,\n\t\t\t\t\t\t    dst_offset, DIRECT_ACCESS,\n\t\t\t\t\t\t    len);\n\t\t\t}\n\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\t\t}\n\n\t\t \nout_rq:\n\t\tif (*out)\n\t\t\ti915_request_put(*out);\n\t\t*out = i915_request_get(rq);\n\t\ti915_request_add(rq);\n\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tif (!bytes_to_cpy && !ccs_bytes_to_cpy) {\n\t\t\tif (src_is_lmem)\n\t\t\t\tWARN_ON(it_src.sg && sg_dma_len(it_src.sg));\n\t\t\telse\n\t\t\t\tWARN_ON(it_dst.sg && sg_dma_len(it_dst.sg));\n\t\t\tbreak;\n\t\t}\n\n\t\tif (WARN_ON(!it_src.sg || !sg_dma_len(it_src.sg) ||\n\t\t\t    !it_dst.sg || !sg_dma_len(it_dst.sg) ||\n\t\t\t    (ccs_bytes_to_cpy && (!it_ccs.sg ||\n\t\t\t\t\t\t  !sg_dma_len(it_ccs.sg))))) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t} while (1);\n\nout_ce:\n\treturn err;\n}\n\nstatic int emit_clear(struct i915_request *rq, u32 offset, int size,\n\t\t      u32 value, bool is_lmem)\n{\n\tstruct drm_i915_private *i915 = rq->i915;\n\tint mocs = rq->engine->gt->mocs.uc_index << 1;\n\tconst int ver = GRAPHICS_VER(i915);\n\tint ring_sz;\n\tu32 *cs;\n\n\tGEM_BUG_ON(size >> PAGE_SHIFT > S16_MAX);\n\n\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50))\n\t\tring_sz = XY_FAST_COLOR_BLT_DW;\n\telse if (ver >= 8)\n\t\tring_sz = 8;\n\telse\n\t\tring_sz = 6;\n\n\tcs = intel_ring_begin(rq, ring_sz);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50)) {\n\t\t*cs++ = XY_FAST_COLOR_BLT_CMD | XY_FAST_COLOR_BLT_DEPTH_32 |\n\t\t\t(XY_FAST_COLOR_BLT_DW - 2);\n\t\t*cs++ = FIELD_PREP(XY_FAST_COLOR_BLT_MOCS_MASK, mocs) |\n\t\t\t(PAGE_SIZE - 1);\n\t\t*cs++ = 0;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE / 4;\n\t\t*cs++ = offset;\n\t\t*cs++ = rq->engine->instance;\n\t\t*cs++ = !is_lmem << XY_FAST_COLOR_BLT_MEM_TYPE_SHIFT;\n\t\t \n\t\t*cs++ = value;\n\t\t*cs++ = 0;\n\t\t*cs++ = 0;\n\t\t*cs++ = 0;\n\t\t \n\t\t*cs++ = 0;\n\t\t*cs++ = 0;\n\t\t \n\t\t*cs++ = 0;\n\t\t*cs++ = 0;\n\t\t*cs++ = 0;\n\t} else if (ver >= 8) {\n\t\t*cs++ = XY_COLOR_BLT_CMD | BLT_WRITE_RGBA | (7 - 2);\n\t\t*cs++ = BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | PAGE_SIZE;\n\t\t*cs++ = 0;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE / 4;\n\t\t*cs++ = offset;\n\t\t*cs++ = rq->engine->instance;\n\t\t*cs++ = value;\n\t\t*cs++ = MI_NOOP;\n\t} else {\n\t\t*cs++ = XY_COLOR_BLT_CMD | BLT_WRITE_RGBA | (6 - 2);\n\t\t*cs++ = BLT_DEPTH_32 | BLT_ROP_COLOR_COPY | PAGE_SIZE;\n\t\t*cs++ = 0;\n\t\t*cs++ = size >> PAGE_SHIFT << 16 | PAGE_SIZE / 4;\n\t\t*cs++ = offset;\n\t\t*cs++ = value;\n\t}\n\n\tintel_ring_advance(rq, cs);\n\treturn 0;\n}\n\nint\nintel_context_migrate_clear(struct intel_context *ce,\n\t\t\t    const struct i915_deps *deps,\n\t\t\t    struct scatterlist *sg,\n\t\t\t    unsigned int pat_index,\n\t\t\t    bool is_lmem,\n\t\t\t    u32 value,\n\t\t\t    struct i915_request **out)\n{\n\tstruct drm_i915_private *i915 = ce->engine->i915;\n\tstruct sgt_dma it = sg_sgt(sg);\n\tstruct i915_request *rq;\n\tu32 offset;\n\tint err;\n\n\tGEM_BUG_ON(ce->vm != ce->engine->gt->migrate.context->vm);\n\t*out = NULL;\n\n\tGEM_BUG_ON(ce->ring->size < SZ_64K);\n\n\toffset = 0;\n\tif (HAS_64K_PAGES(i915) && is_lmem)\n\t\toffset = CHUNK_SZ;\n\n\tdo {\n\t\tint len;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_ce;\n\t\t}\n\n\t\tif (deps) {\n\t\t\terr = i915_request_await_deps(rq, deps);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\tif (rq->engine->emit_init_breadcrumb) {\n\t\t\t\terr = rq->engine->emit_init_breadcrumb(rq);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_rq;\n\t\t\t}\n\n\t\t\tdeps = NULL;\n\t\t}\n\n\t\t \n\t\terr = emit_no_arbitration(rq);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\tlen = emit_pte(rq, &it, pat_index, is_lmem, offset, CHUNK_SZ);\n\t\tif (len <= 0) {\n\t\t\terr = len;\n\t\t\tgoto out_rq;\n\t\t}\n\n\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\terr = emit_clear(rq, offset, len, value, is_lmem);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\tif (HAS_FLAT_CCS(i915) && is_lmem && !value) {\n\t\t\t \n\t\t\terr = emit_copy_ccs(rq, offset, INDIRECT_ACCESS, offset,\n\t\t\t\t\t    DIRECT_ACCESS, len);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\t\t}\n\n\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\n\t\t \nout_rq:\n\t\tif (*out)\n\t\t\ti915_request_put(*out);\n\t\t*out = i915_request_get(rq);\n\t\ti915_request_add(rq);\n\t\tif (err || !it.sg || !sg_dma_len(it.sg))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t} while (1);\n\nout_ce:\n\treturn err;\n}\n\nint intel_migrate_copy(struct intel_migrate *m,\n\t\t       struct i915_gem_ww_ctx *ww,\n\t\t       const struct i915_deps *deps,\n\t\t       struct scatterlist *src,\n\t\t       unsigned int src_pat_index,\n\t\t       bool src_is_lmem,\n\t\t       struct scatterlist *dst,\n\t\t       unsigned int dst_pat_index,\n\t\t       bool dst_is_lmem,\n\t\t       struct i915_request **out)\n{\n\tstruct intel_context *ce;\n\tint err;\n\n\t*out = NULL;\n\tif (!m->context)\n\t\treturn -ENODEV;\n\n\tce = intel_migrate_create_context(m);\n\tif (IS_ERR(ce))\n\t\tce = intel_context_get(m->context);\n\tGEM_BUG_ON(IS_ERR(ce));\n\n\terr = intel_context_pin_ww(ce, ww);\n\tif (err)\n\t\tgoto out;\n\n\terr = intel_context_migrate_copy(ce, deps,\n\t\t\t\t\t src, src_pat_index, src_is_lmem,\n\t\t\t\t\t dst, dst_pat_index, dst_is_lmem,\n\t\t\t\t\t out);\n\n\tintel_context_unpin(ce);\nout:\n\tintel_context_put(ce);\n\treturn err;\n}\n\nint\nintel_migrate_clear(struct intel_migrate *m,\n\t\t    struct i915_gem_ww_ctx *ww,\n\t\t    const struct i915_deps *deps,\n\t\t    struct scatterlist *sg,\n\t\t    unsigned int pat_index,\n\t\t    bool is_lmem,\n\t\t    u32 value,\n\t\t    struct i915_request **out)\n{\n\tstruct intel_context *ce;\n\tint err;\n\n\t*out = NULL;\n\tif (!m->context)\n\t\treturn -ENODEV;\n\n\tce = intel_migrate_create_context(m);\n\tif (IS_ERR(ce))\n\t\tce = intel_context_get(m->context);\n\tGEM_BUG_ON(IS_ERR(ce));\n\n\terr = intel_context_pin_ww(ce, ww);\n\tif (err)\n\t\tgoto out;\n\n\terr = intel_context_migrate_clear(ce, deps, sg, pat_index,\n\t\t\t\t\t  is_lmem, value, out);\n\n\tintel_context_unpin(ce);\nout:\n\tintel_context_put(ce);\n\treturn err;\n}\n\nvoid intel_migrate_fini(struct intel_migrate *m)\n{\n\tstruct intel_context *ce;\n\n\tce = fetch_and_zero(&m->context);\n\tif (!ce)\n\t\treturn;\n\n\tintel_engine_destroy_pinned_context(ce);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_migrate.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}