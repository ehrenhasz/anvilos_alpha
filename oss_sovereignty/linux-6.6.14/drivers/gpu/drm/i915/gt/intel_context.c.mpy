{
  "module_name": "intel_context.c",
  "hash_id": "e649859acdddc08ec06c7308fbc0201658f8f0690e18f15666d65ebd1f058aa3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_context.c",
  "human_readable_source": "\n \n\n#include \"gem/i915_gem_context.h\"\n#include \"gem/i915_gem_pm.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_trace.h\"\n\n#include \"intel_context.h\"\n#include \"intel_engine.h\"\n#include \"intel_engine_pm.h\"\n#include \"intel_ring.h\"\n\nstatic struct kmem_cache *slab_ce;\n\nstatic struct intel_context *intel_context_alloc(void)\n{\n\treturn kmem_cache_zalloc(slab_ce, GFP_KERNEL);\n}\n\nstatic void rcu_context_free(struct rcu_head *rcu)\n{\n\tstruct intel_context *ce = container_of(rcu, typeof(*ce), rcu);\n\n\ttrace_intel_context_free(ce);\n\tkmem_cache_free(slab_ce, ce);\n}\n\nvoid intel_context_free(struct intel_context *ce)\n{\n\tcall_rcu(&ce->rcu, rcu_context_free);\n}\n\nstruct intel_context *\nintel_context_create(struct intel_engine_cs *engine)\n{\n\tstruct intel_context *ce;\n\n\tce = intel_context_alloc();\n\tif (!ce)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tintel_context_init(ce, engine);\n\ttrace_intel_context_create(ce);\n\treturn ce;\n}\n\nint intel_context_alloc_state(struct intel_context *ce)\n{\n\tint err = 0;\n\n\tif (mutex_lock_interruptible(&ce->pin_mutex))\n\t\treturn -EINTR;\n\n\tif (!test_bit(CONTEXT_ALLOC_BIT, &ce->flags)) {\n\t\tif (intel_context_is_banned(ce)) {\n\t\t\terr = -EIO;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\terr = ce->ops->alloc(ce);\n\t\tif (unlikely(err))\n\t\t\tgoto unlock;\n\n\t\tset_bit(CONTEXT_ALLOC_BIT, &ce->flags);\n\t}\n\nunlock:\n\tmutex_unlock(&ce->pin_mutex);\n\treturn err;\n}\n\nstatic int intel_context_active_acquire(struct intel_context *ce)\n{\n\tint err;\n\n\t__i915_active_acquire(&ce->active);\n\n\tif (intel_context_is_barrier(ce) || intel_engine_uses_guc(ce->engine) ||\n\t    intel_context_is_parallel(ce))\n\t\treturn 0;\n\n\t \n\terr = i915_active_acquire_preallocate_barrier(&ce->active,\n\t\t\t\t\t\t      ce->engine);\n\tif (err)\n\t\ti915_active_release(&ce->active);\n\n\treturn err;\n}\n\nstatic void intel_context_active_release(struct intel_context *ce)\n{\n\t \n\ti915_active_acquire_barrier(&ce->active);\n\ti915_active_release(&ce->active);\n}\n\nstatic int __context_pin_state(struct i915_vma *vma, struct i915_gem_ww_ctx *ww)\n{\n\tunsigned int bias = i915_ggtt_pin_bias(vma) | PIN_OFFSET_BIAS;\n\tint err;\n\n\terr = i915_ggtt_pin(vma, ww, 0, bias | PIN_HIGH);\n\tif (err)\n\t\treturn err;\n\n\terr = i915_active_acquire(&vma->active);\n\tif (err)\n\t\tgoto err_unpin;\n\n\t \n\ti915_vma_make_unshrinkable(vma);\n\tvma->obj->mm.dirty = true;\n\n\treturn 0;\n\nerr_unpin:\n\ti915_vma_unpin(vma);\n\treturn err;\n}\n\nstatic void __context_unpin_state(struct i915_vma *vma)\n{\n\ti915_vma_make_shrinkable(vma);\n\ti915_active_release(&vma->active);\n\t__i915_vma_unpin(vma);\n}\n\nstatic int __ring_active(struct intel_ring *ring,\n\t\t\t struct i915_gem_ww_ctx *ww)\n{\n\tint err;\n\n\terr = intel_ring_pin(ring, ww);\n\tif (err)\n\t\treturn err;\n\n\terr = i915_active_acquire(&ring->vma->active);\n\tif (err)\n\t\tgoto err_pin;\n\n\treturn 0;\n\nerr_pin:\n\tintel_ring_unpin(ring);\n\treturn err;\n}\n\nstatic void __ring_retire(struct intel_ring *ring)\n{\n\ti915_active_release(&ring->vma->active);\n\tintel_ring_unpin(ring);\n}\n\nstatic int intel_context_pre_pin(struct intel_context *ce,\n\t\t\t\t struct i915_gem_ww_ctx *ww)\n{\n\tint err;\n\n\tCE_TRACE(ce, \"active\\n\");\n\n\terr = __ring_active(ce->ring, ww);\n\tif (err)\n\t\treturn err;\n\n\terr = intel_timeline_pin(ce->timeline, ww);\n\tif (err)\n\t\tgoto err_ring;\n\n\tif (!ce->state)\n\t\treturn 0;\n\n\terr = __context_pin_state(ce->state, ww);\n\tif (err)\n\t\tgoto err_timeline;\n\n\n\treturn 0;\n\nerr_timeline:\n\tintel_timeline_unpin(ce->timeline);\nerr_ring:\n\t__ring_retire(ce->ring);\n\treturn err;\n}\n\nstatic void intel_context_post_unpin(struct intel_context *ce)\n{\n\tif (ce->state)\n\t\t__context_unpin_state(ce->state);\n\n\tintel_timeline_unpin(ce->timeline);\n\t__ring_retire(ce->ring);\n}\n\nint __intel_context_do_pin_ww(struct intel_context *ce,\n\t\t\t      struct i915_gem_ww_ctx *ww)\n{\n\tbool handoff = false;\n\tvoid *vaddr;\n\tint err = 0;\n\n\tif (unlikely(!test_bit(CONTEXT_ALLOC_BIT, &ce->flags))) {\n\t\terr = intel_context_alloc_state(ce);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\n\terr = i915_gem_object_lock(ce->timeline->hwsp_ggtt->obj, ww);\n\tif (!err)\n\t\terr = i915_gem_object_lock(ce->ring->vma->obj, ww);\n\tif (!err && ce->state)\n\t\terr = i915_gem_object_lock(ce->state->obj, ww);\n\tif (!err)\n\t\terr = intel_context_pre_pin(ce, ww);\n\tif (err)\n\t\treturn err;\n\n\terr = ce->ops->pre_pin(ce, ww, &vaddr);\n\tif (err)\n\t\tgoto err_ctx_unpin;\n\n\terr = i915_active_acquire(&ce->active);\n\tif (err)\n\t\tgoto err_post_unpin;\n\n\terr = mutex_lock_interruptible(&ce->pin_mutex);\n\tif (err)\n\t\tgoto err_release;\n\n\tintel_engine_pm_might_get(ce->engine);\n\n\tif (unlikely(intel_context_is_closed(ce))) {\n\t\terr = -ENOENT;\n\t\tgoto err_unlock;\n\t}\n\n\tif (likely(!atomic_add_unless(&ce->pin_count, 1, 0))) {\n\t\terr = intel_context_active_acquire(ce);\n\t\tif (unlikely(err))\n\t\t\tgoto err_unlock;\n\n\t\terr = ce->ops->pin(ce, vaddr);\n\t\tif (err) {\n\t\t\tintel_context_active_release(ce);\n\t\t\tgoto err_unlock;\n\t\t}\n\n\t\tCE_TRACE(ce, \"pin ring:{start:%08x, head:%04x, tail:%04x}\\n\",\n\t\t\t i915_ggtt_offset(ce->ring->vma),\n\t\t\t ce->ring->head, ce->ring->tail);\n\n\t\thandoff = true;\n\t\tsmp_mb__before_atomic();  \n\t\tatomic_inc(&ce->pin_count);\n\t}\n\n\tGEM_BUG_ON(!intel_context_is_pinned(ce));  \n\n\ttrace_intel_context_do_pin(ce);\n\nerr_unlock:\n\tmutex_unlock(&ce->pin_mutex);\nerr_release:\n\ti915_active_release(&ce->active);\nerr_post_unpin:\n\tif (!handoff)\n\t\tce->ops->post_unpin(ce);\nerr_ctx_unpin:\n\tintel_context_post_unpin(ce);\n\n\t \n\ti915_gem_ww_unlock_single(ce->timeline->hwsp_ggtt->obj);\n\n\treturn err;\n}\n\nint __intel_context_do_pin(struct intel_context *ce)\n{\n\tstruct i915_gem_ww_ctx ww;\n\tint err;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\terr = __intel_context_do_pin_ww(ce, &ww);\n\tif (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\treturn err;\n}\n\nvoid __intel_context_do_unpin(struct intel_context *ce, int sub)\n{\n\tif (!atomic_sub_and_test(sub, &ce->pin_count))\n\t\treturn;\n\n\tCE_TRACE(ce, \"unpin\\n\");\n\tce->ops->unpin(ce);\n\tce->ops->post_unpin(ce);\n\n\t \n\tintel_context_get(ce);\n\tintel_context_active_release(ce);\n\ttrace_intel_context_do_unpin(ce);\n\tintel_context_put(ce);\n}\n\nstatic void __intel_context_retire(struct i915_active *active)\n{\n\tstruct intel_context *ce = container_of(active, typeof(*ce), active);\n\n\tCE_TRACE(ce, \"retire runtime: { total:%lluns, avg:%lluns }\\n\",\n\t\t intel_context_get_total_runtime_ns(ce),\n\t\t intel_context_get_avg_runtime_ns(ce));\n\n\tset_bit(CONTEXT_VALID_BIT, &ce->flags);\n\tintel_context_post_unpin(ce);\n\tintel_context_put(ce);\n}\n\nstatic int __intel_context_active(struct i915_active *active)\n{\n\tstruct intel_context *ce = container_of(active, typeof(*ce), active);\n\n\tintel_context_get(ce);\n\n\t \n\tGEM_WARN_ON(!i915_active_acquire_if_busy(&ce->ring->vma->active));\n\t__intel_ring_pin(ce->ring);\n\n\t__intel_timeline_pin(ce->timeline);\n\n\tif (ce->state) {\n\t\tGEM_WARN_ON(!i915_active_acquire_if_busy(&ce->state->active));\n\t\t__i915_vma_pin(ce->state);\n\t\ti915_vma_make_unshrinkable(ce->state);\n\t}\n\n\treturn 0;\n}\n\nstatic int\nsw_fence_dummy_notify(struct i915_sw_fence *sf,\n\t\t      enum i915_sw_fence_notify state)\n{\n\treturn NOTIFY_DONE;\n}\n\nvoid\nintel_context_init(struct intel_context *ce, struct intel_engine_cs *engine)\n{\n\tGEM_BUG_ON(!engine->cops);\n\tGEM_BUG_ON(!engine->gt->vm);\n\n\tkref_init(&ce->ref);\n\n\tce->engine = engine;\n\tce->ops = engine->cops;\n\tce->sseu = engine->sseu;\n\tce->ring = NULL;\n\tce->ring_size = SZ_4K;\n\n\tewma_runtime_init(&ce->stats.runtime.avg);\n\n\tce->vm = i915_vm_get(engine->gt->vm);\n\n\t \n\tspin_lock_init(&ce->signal_lock);\n\tINIT_LIST_HEAD(&ce->signals);\n\n\tmutex_init(&ce->pin_mutex);\n\n\tspin_lock_init(&ce->guc_state.lock);\n\tINIT_LIST_HEAD(&ce->guc_state.fences);\n\tINIT_LIST_HEAD(&ce->guc_state.requests);\n\n\tce->guc_id.id = GUC_INVALID_CONTEXT_ID;\n\tINIT_LIST_HEAD(&ce->guc_id.link);\n\n\tINIT_LIST_HEAD(&ce->destroyed_link);\n\n\tINIT_LIST_HEAD(&ce->parallel.child_list);\n\n\t \n\ti915_sw_fence_init(&ce->guc_state.blocked,\n\t\t\t   sw_fence_dummy_notify);\n\ti915_sw_fence_commit(&ce->guc_state.blocked);\n\n\ti915_active_init(&ce->active,\n\t\t\t __intel_context_active, __intel_context_retire, 0);\n}\n\nvoid intel_context_fini(struct intel_context *ce)\n{\n\tstruct intel_context *child, *next;\n\n\tif (ce->timeline)\n\t\tintel_timeline_put(ce->timeline);\n\ti915_vm_put(ce->vm);\n\n\t \n\tif (intel_context_is_parent(ce))\n\t\tfor_each_child_safe(ce, child, next)\n\t\t\tintel_context_put(child);\n\n\tmutex_destroy(&ce->pin_mutex);\n\ti915_active_fini(&ce->active);\n\ti915_sw_fence_fini(&ce->guc_state.blocked);\n}\n\nvoid i915_context_module_exit(void)\n{\n\tkmem_cache_destroy(slab_ce);\n}\n\nint __init i915_context_module_init(void)\n{\n\tslab_ce = KMEM_CACHE(intel_context, SLAB_HWCACHE_ALIGN);\n\tif (!slab_ce)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid intel_context_enter_engine(struct intel_context *ce)\n{\n\tintel_engine_pm_get(ce->engine);\n\tintel_timeline_enter(ce->timeline);\n}\n\nvoid intel_context_exit_engine(struct intel_context *ce)\n{\n\tintel_timeline_exit(ce->timeline);\n\tintel_engine_pm_put(ce->engine);\n}\n\nint intel_context_prepare_remote_request(struct intel_context *ce,\n\t\t\t\t\t struct i915_request *rq)\n{\n\tstruct intel_timeline *tl = ce->timeline;\n\tint err;\n\n\t \n\tGEM_BUG_ON(rq->context == ce);\n\n\tif (rcu_access_pointer(rq->timeline) != tl) {  \n\t\t \n\t\terr = i915_active_fence_set(&tl->last_request, rq);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tGEM_BUG_ON(i915_active_is_idle(&ce->active));\n\treturn i915_active_add_request(&ce->active, rq);\n}\n\nstruct i915_request *intel_context_create_request(struct intel_context *ce)\n{\n\tstruct i915_gem_ww_ctx ww;\n\tstruct i915_request *rq;\n\tint err;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\terr = intel_context_pin_ww(ce, &ww);\n\tif (!err) {\n\t\trq = i915_request_create(ce);\n\t\tintel_context_unpin(ce);\n\t} else if (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t\trq = ERR_PTR(err);\n\t} else {\n\t\trq = ERR_PTR(err);\n\t}\n\n\ti915_gem_ww_ctx_fini(&ww);\n\n\tif (IS_ERR(rq))\n\t\treturn rq;\n\n\t \n\tlockdep_unpin_lock(&ce->timeline->mutex, rq->cookie);\n\tmutex_release(&ce->timeline->mutex.dep_map, _RET_IP_);\n\tmutex_acquire(&ce->timeline->mutex.dep_map, SINGLE_DEPTH_NESTING, 0, _RET_IP_);\n\trq->cookie = lockdep_pin_lock(&ce->timeline->mutex);\n\n\treturn rq;\n}\n\nstruct i915_request *intel_context_get_active_request(struct intel_context *ce)\n{\n\tstruct intel_context *parent = intel_context_to_parent(ce);\n\tstruct i915_request *rq, *active = NULL;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(!intel_engine_uses_guc(ce->engine));\n\n\t \n\tspin_lock_irqsave(&parent->guc_state.lock, flags);\n\tlist_for_each_entry_reverse(rq, &parent->guc_state.requests,\n\t\t\t\t    sched.link) {\n\t\tif (rq->context != ce)\n\t\t\tcontinue;\n\t\tif (i915_request_completed(rq))\n\t\t\tbreak;\n\n\t\tactive = rq;\n\t}\n\tif (active)\n\t\tactive = i915_request_get_rcu(active);\n\tspin_unlock_irqrestore(&parent->guc_state.lock, flags);\n\n\treturn active;\n}\n\nvoid intel_context_bind_parent_child(struct intel_context *parent,\n\t\t\t\t     struct intel_context *child)\n{\n\t \n\tGEM_BUG_ON(intel_context_is_pinned(parent));\n\tGEM_BUG_ON(intel_context_is_child(parent));\n\tGEM_BUG_ON(intel_context_is_pinned(child));\n\tGEM_BUG_ON(intel_context_is_child(child));\n\tGEM_BUG_ON(intel_context_is_parent(child));\n\n\tparent->parallel.child_index = parent->parallel.number_children++;\n\tlist_add_tail(&child->parallel.child_link,\n\t\t      &parent->parallel.child_list);\n\tchild->parallel.parent = parent;\n}\n\nu64 intel_context_get_total_runtime_ns(struct intel_context *ce)\n{\n\tu64 total, active;\n\n\tif (ce->ops->update_stats)\n\t\tce->ops->update_stats(ce);\n\n\ttotal = ce->stats.runtime.total;\n\tif (ce->ops->flags & COPS_RUNTIME_CYCLES)\n\t\ttotal *= ce->engine->gt->clock_period_ns;\n\n\tactive = READ_ONCE(ce->stats.active);\n\tif (active)\n\t\tactive = intel_context_clock() - active;\n\n\treturn total + active;\n}\n\nu64 intel_context_get_avg_runtime_ns(struct intel_context *ce)\n{\n\tu64 avg = ewma_runtime_read(&ce->stats.runtime.avg);\n\n\tif (ce->ops->flags & COPS_RUNTIME_CYCLES)\n\t\tavg *= ce->engine->gt->clock_period_ns;\n\n\treturn avg;\n}\n\nbool intel_context_ban(struct intel_context *ce, struct i915_request *rq)\n{\n\tbool ret = intel_context_set_banned(ce);\n\n\ttrace_intel_context_ban(ce);\n\n\tif (ce->ops->revoke)\n\t\tce->ops->revoke(ce, rq,\n\t\t\t\tINTEL_CONTEXT_BANNED_PREEMPT_TIMEOUT_MS);\n\n\treturn ret;\n}\n\nbool intel_context_revoke(struct intel_context *ce)\n{\n\tbool ret = intel_context_set_exiting(ce);\n\n\tif (ce->ops->revoke)\n\t\tce->ops->revoke(ce, NULL, ce->engine->props.preempt_timeout_ms);\n\n\treturn ret;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_context.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}