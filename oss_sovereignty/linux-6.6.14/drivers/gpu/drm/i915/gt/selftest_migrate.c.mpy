{
  "module_name": "selftest_migrate.c",
  "hash_id": "5cbee36de1dcce2aa5d105a029057d320bc7e59d1ce4e71aab09158b6cc2a1fb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/selftest_migrate.c",
  "human_readable_source": "\n \n\n#include <linux/sort.h>\n\n#include \"gem/i915_gem_internal.h\"\n#include \"gem/i915_gem_lmem.h\"\n\n#include \"selftests/igt_spinner.h\"\n#include \"selftests/i915_random.h\"\n\nstatic const unsigned int sizes[] = {\n\tSZ_4K,\n\tSZ_64K,\n\tSZ_2M,\n\tCHUNK_SZ - SZ_4K,\n\tCHUNK_SZ,\n\tCHUNK_SZ + SZ_4K,\n\tSZ_64M,\n};\n\nstatic struct drm_i915_gem_object *\ncreate_lmem_or_internal(struct drm_i915_private *i915, size_t size)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tobj = i915_gem_object_create_lmem(i915, size, 0);\n\tif (!IS_ERR(obj))\n\t\treturn obj;\n\n\treturn i915_gem_object_create_internal(i915, size);\n}\n\nstatic int copy(struct intel_migrate *migrate,\n\t\tint (*fn)(struct intel_migrate *migrate,\n\t\t\t  struct i915_gem_ww_ctx *ww,\n\t\t\t  struct drm_i915_gem_object *src,\n\t\t\t  struct drm_i915_gem_object *dst,\n\t\t\t  struct i915_request **out),\n\t\tu32 sz, struct rnd_state *prng)\n{\n\tstruct drm_i915_private *i915 = migrate->context->engine->i915;\n\tstruct drm_i915_gem_object *src, *dst;\n\tstruct i915_request *rq;\n\tstruct i915_gem_ww_ctx ww;\n\tu32 *vaddr;\n\tint err = 0;\n\tint i;\n\n\tsrc = create_lmem_or_internal(i915, sz);\n\tif (IS_ERR(src))\n\t\treturn 0;\n\n\tsz = src->base.size;\n\tdst = i915_gem_object_create_internal(i915, sz);\n\tif (IS_ERR(dst))\n\t\tgoto err_free_src;\n\n\tfor_i915_gem_ww(&ww, err, true) {\n\t\terr = i915_gem_object_lock(src, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\terr = i915_gem_object_lock(dst, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tvaddr = i915_gem_object_pin_map(src, I915_MAP_WC);\n\t\tif (IS_ERR(vaddr)) {\n\t\t\terr = PTR_ERR(vaddr);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (i = 0; i < sz / sizeof(u32); i++)\n\t\t\tvaddr[i] = i;\n\t\ti915_gem_object_flush_map(src);\n\n\t\tvaddr = i915_gem_object_pin_map(dst, I915_MAP_WC);\n\t\tif (IS_ERR(vaddr)) {\n\t\t\terr = PTR_ERR(vaddr);\n\t\t\tgoto unpin_src;\n\t\t}\n\n\t\tfor (i = 0; i < sz / sizeof(u32); i++)\n\t\t\tvaddr[i] = ~i;\n\t\ti915_gem_object_flush_map(dst);\n\n\t\terr = fn(migrate, &ww, src, dst, &rq);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\tif (err != -EDEADLK && err != -EINTR && err != -ERESTARTSYS)\n\t\t\tpr_err(\"%ps failed, size: %u\\n\", fn, sz);\n\t\tif (rq) {\n\t\t\ti915_request_wait(rq, 0, HZ);\n\t\t\ti915_request_put(rq);\n\t\t}\n\t\ti915_gem_object_unpin_map(dst);\nunpin_src:\n\t\ti915_gem_object_unpin_map(src);\n\t}\n\tif (err)\n\t\tgoto err_out;\n\n\tif (rq) {\n\t\tif (i915_request_wait(rq, 0, HZ) < 0) {\n\t\t\tpr_err(\"%ps timed out, size: %u\\n\", fn, sz);\n\t\t\terr = -ETIME;\n\t\t}\n\t\ti915_request_put(rq);\n\t}\n\n\tfor (i = 0; !err && i < sz / PAGE_SIZE; i++) {\n\t\tint x = i * 1024 + i915_prandom_u32_max_state(1024, prng);\n\n\t\tif (vaddr[x] != x) {\n\t\t\tpr_err(\"%ps failed, size: %u, offset: %zu\\n\",\n\t\t\t       fn, sz, x * sizeof(u32));\n\t\t\tigt_hexdump(vaddr + i * 1024, 4096);\n\t\t\terr = -EINVAL;\n\t\t}\n\t}\n\n\ti915_gem_object_unpin_map(dst);\n\ti915_gem_object_unpin_map(src);\n\nerr_out:\n\ti915_gem_object_put(dst);\nerr_free_src:\n\ti915_gem_object_put(src);\n\n\treturn err;\n}\n\nstatic int intel_context_copy_ccs(struct intel_context *ce,\n\t\t\t\t  const struct i915_deps *deps,\n\t\t\t\t  struct scatterlist *sg,\n\t\t\t\t  unsigned int pat_index,\n\t\t\t\t  bool write_to_ccs,\n\t\t\t\t  struct i915_request **out)\n{\n\tu8 src_access = write_to_ccs ? DIRECT_ACCESS : INDIRECT_ACCESS;\n\tu8 dst_access = write_to_ccs ? INDIRECT_ACCESS : DIRECT_ACCESS;\n\tstruct sgt_dma it = sg_sgt(sg);\n\tstruct i915_request *rq;\n\tu32 offset;\n\tint err;\n\n\tGEM_BUG_ON(ce->vm != ce->engine->gt->migrate.context->vm);\n\t*out = NULL;\n\n\tGEM_BUG_ON(ce->ring->size < SZ_64K);\n\n\toffset = 0;\n\tif (HAS_64K_PAGES(ce->engine->i915))\n\t\toffset = CHUNK_SZ;\n\n\tdo {\n\t\tint len;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_ce;\n\t\t}\n\n\t\tif (deps) {\n\t\t\terr = i915_request_await_deps(rq, deps);\n\t\t\tif (err)\n\t\t\t\tgoto out_rq;\n\n\t\t\tif (rq->engine->emit_init_breadcrumb) {\n\t\t\t\terr = rq->engine->emit_init_breadcrumb(rq);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out_rq;\n\t\t\t}\n\n\t\t\tdeps = NULL;\n\t\t}\n\n\t\t \n\t\terr = emit_no_arbitration(rq);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\tlen = emit_pte(rq, &it, pat_index, true, offset, CHUNK_SZ);\n\t\tif (len <= 0) {\n\t\t\terr = len;\n\t\t\tgoto out_rq;\n\t\t}\n\n\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\terr = emit_copy_ccs(rq, offset, dst_access,\n\t\t\t\t    offset, src_access, len);\n\t\tif (err)\n\t\t\tgoto out_rq;\n\n\t\terr = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\n\t\t \nout_rq:\n\t\tif (*out)\n\t\t\ti915_request_put(*out);\n\t\t*out = i915_request_get(rq);\n\t\ti915_request_add(rq);\n\t\tif (err || !it.sg || !sg_dma_len(it.sg))\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t} while (1);\n\nout_ce:\n\treturn err;\n}\n\nstatic int\nintel_migrate_ccs_copy(struct intel_migrate *m,\n\t\t       struct i915_gem_ww_ctx *ww,\n\t\t       const struct i915_deps *deps,\n\t\t       struct scatterlist *sg,\n\t\t       unsigned int pat_index,\n\t\t       bool write_to_ccs,\n\t\t       struct i915_request **out)\n{\n\tstruct intel_context *ce;\n\tint err;\n\n\t*out = NULL;\n\tif (!m->context)\n\t\treturn -ENODEV;\n\n\tce = intel_migrate_create_context(m);\n\tif (IS_ERR(ce))\n\t\tce = intel_context_get(m->context);\n\tGEM_BUG_ON(IS_ERR(ce));\n\n\terr = intel_context_pin_ww(ce, ww);\n\tif (err)\n\t\tgoto out;\n\n\terr = intel_context_copy_ccs(ce, deps, sg, pat_index,\n\t\t\t\t     write_to_ccs, out);\n\n\tintel_context_unpin(ce);\nout:\n\tintel_context_put(ce);\n\treturn err;\n}\n\nstatic int clear(struct intel_migrate *migrate,\n\t\t int (*fn)(struct intel_migrate *migrate,\n\t\t\t   struct i915_gem_ww_ctx *ww,\n\t\t\t   struct drm_i915_gem_object *obj,\n\t\t\t   u32 value,\n\t\t\t   struct i915_request **out),\n\t\t u32 sz, struct rnd_state *prng)\n{\n\tstruct drm_i915_private *i915 = migrate->context->engine->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_request *rq;\n\tstruct i915_gem_ww_ctx ww;\n\tu32 *vaddr, val = 0;\n\tbool ccs_cap = false;\n\tint err = 0;\n\tint i;\n\n\tobj = create_lmem_or_internal(i915, sz);\n\tif (IS_ERR(obj))\n\t\treturn 0;\n\n\t \n\tsz = obj->base.size;\n\n\tif (HAS_FLAT_CCS(i915) && i915_gem_object_is_lmem(obj))\n\t\tccs_cap = true;\n\n\tfor_i915_gem_ww(&ww, err, true) {\n\t\tint ccs_bytes, ccs_bytes_per_chunk;\n\n\t\terr = i915_gem_object_lock(obj, &ww);\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tvaddr = i915_gem_object_pin_map(obj, I915_MAP_WC);\n\t\tif (IS_ERR(vaddr)) {\n\t\t\terr = PTR_ERR(vaddr);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (i = 0; i < sz / sizeof(u32); i++)\n\t\t\tvaddr[i] = ~i;\n\t\ti915_gem_object_flush_map(obj);\n\n\t\tif (ccs_cap && !val) {\n\t\t\t \n\t\t\terr = intel_migrate_ccs_copy(migrate, &ww, NULL,\n\t\t\t\t\t\t     obj->mm.pages->sgl,\n\t\t\t\t\t\t     obj->pat_index,\n\t\t\t\t\t\t     true, &rq);\n\t\t\tif (rq && !err) {\n\t\t\t\tif (i915_request_wait(rq, 0, HZ) < 0) {\n\t\t\t\t\tpr_err(\"%ps timed out, size: %u\\n\",\n\t\t\t\t\t       fn, sz);\n\t\t\t\t\terr = -ETIME;\n\t\t\t\t}\n\t\t\t\ti915_request_put(rq);\n\t\t\t\trq = NULL;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\terr = fn(migrate, &ww, obj, val, &rq);\n\t\tif (rq && !err) {\n\t\t\tif (i915_request_wait(rq, 0, HZ) < 0) {\n\t\t\t\tpr_err(\"%ps timed out, size: %u\\n\", fn, sz);\n\t\t\t\terr = -ETIME;\n\t\t\t}\n\t\t\ti915_request_put(rq);\n\t\t\trq = NULL;\n\t\t}\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\ti915_gem_object_flush_map(obj);\n\n\t\t \n\t\tfor (i = 0; !err && i < sz / PAGE_SIZE; i++) {\n\t\t\tint x = i * 1024 +\n\t\t\t\ti915_prandom_u32_max_state(1024, prng);\n\n\t\t\tif (vaddr[x] != val) {\n\t\t\t\tpr_err(\"%ps failed, (%u != %u), offset: %zu\\n\",\n\t\t\t\t       fn, vaddr[x], val,  x * sizeof(u32));\n\t\t\t\tigt_hexdump(vaddr + i * 1024, 4096);\n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\tif (err)\n\t\t\tcontinue;\n\n\t\tif (ccs_cap && !val) {\n\t\t\tfor (i = 0; i < sz / sizeof(u32); i++)\n\t\t\t\tvaddr[i] = ~i;\n\t\t\ti915_gem_object_flush_map(obj);\n\n\t\t\terr = intel_migrate_ccs_copy(migrate, &ww, NULL,\n\t\t\t\t\t\t     obj->mm.pages->sgl,\n\t\t\t\t\t\t     obj->pat_index,\n\t\t\t\t\t\t     false, &rq);\n\t\t\tif (rq && !err) {\n\t\t\t\tif (i915_request_wait(rq, 0, HZ) < 0) {\n\t\t\t\t\tpr_err(\"%ps timed out, size: %u\\n\",\n\t\t\t\t\t       fn, sz);\n\t\t\t\t\terr = -ETIME;\n\t\t\t\t}\n\t\t\t\ti915_request_put(rq);\n\t\t\t\trq = NULL;\n\t\t\t}\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\n\t\t\tccs_bytes = GET_CCS_BYTES(i915, sz);\n\t\t\tccs_bytes_per_chunk = GET_CCS_BYTES(i915, CHUNK_SZ);\n\t\t\ti915_gem_object_flush_map(obj);\n\n\t\t\tfor (i = 0; !err && i < DIV_ROUND_UP(ccs_bytes, PAGE_SIZE); i++) {\n\t\t\t\tint offset = ((i * PAGE_SIZE)  /\n\t\t\t\t\tccs_bytes_per_chunk) * CHUNK_SZ / sizeof(u32);\n\t\t\t\tint ccs_bytes_left = (ccs_bytes - i * PAGE_SIZE) / sizeof(u32);\n\t\t\t\tint x = i915_prandom_u32_max_state(min_t(int, 1024,\n\t\t\t\t\t\t\t\t\t ccs_bytes_left), prng);\n\n\t\t\t\tif (vaddr[offset + x]) {\n\t\t\t\t\tpr_err(\"%ps ccs clearing failed, offset: %ld/%d\\n\",\n\t\t\t\t\t       fn, i * PAGE_SIZE + x * sizeof(u32), ccs_bytes);\n\t\t\t\t\tigt_hexdump(vaddr + offset,\n\t\t\t\t\t\t    min_t(int, 4096,\n\t\t\t\t\t\t\t  ccs_bytes_left * sizeof(u32)));\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (err)\n\t\t\t\tcontinue;\n\t\t}\n\t\ti915_gem_object_unpin_map(obj);\n\t}\n\n\tif (err) {\n\t\tif (err != -EDEADLK && err != -EINTR && err != -ERESTARTSYS)\n\t\t\tpr_err(\"%ps failed, size: %u\\n\", fn, sz);\n\t\tif (rq && err != -EINVAL) {\n\t\t\ti915_request_wait(rq, 0, HZ);\n\t\t\ti915_request_put(rq);\n\t\t}\n\n\t\ti915_gem_object_unpin_map(obj);\n\t}\n\n\ti915_gem_object_put(obj);\n\treturn err;\n}\n\nstatic int __migrate_copy(struct intel_migrate *migrate,\n\t\t\t  struct i915_gem_ww_ctx *ww,\n\t\t\t  struct drm_i915_gem_object *src,\n\t\t\t  struct drm_i915_gem_object *dst,\n\t\t\t  struct i915_request **out)\n{\n\treturn intel_migrate_copy(migrate, ww, NULL,\n\t\t\t\t  src->mm.pages->sgl, src->pat_index,\n\t\t\t\t  i915_gem_object_is_lmem(src),\n\t\t\t\t  dst->mm.pages->sgl, dst->pat_index,\n\t\t\t\t  i915_gem_object_is_lmem(dst),\n\t\t\t\t  out);\n}\n\nstatic int __global_copy(struct intel_migrate *migrate,\n\t\t\t struct i915_gem_ww_ctx *ww,\n\t\t\t struct drm_i915_gem_object *src,\n\t\t\t struct drm_i915_gem_object *dst,\n\t\t\t struct i915_request **out)\n{\n\treturn intel_context_migrate_copy(migrate->context, NULL,\n\t\t\t\t\t  src->mm.pages->sgl, src->pat_index,\n\t\t\t\t\t  i915_gem_object_is_lmem(src),\n\t\t\t\t\t  dst->mm.pages->sgl, dst->pat_index,\n\t\t\t\t\t  i915_gem_object_is_lmem(dst),\n\t\t\t\t\t  out);\n}\n\nstatic int\nmigrate_copy(struct intel_migrate *migrate, u32 sz, struct rnd_state *prng)\n{\n\treturn copy(migrate, __migrate_copy, sz, prng);\n}\n\nstatic int\nglobal_copy(struct intel_migrate *migrate, u32 sz, struct rnd_state *prng)\n{\n\treturn copy(migrate, __global_copy, sz, prng);\n}\n\nstatic int __migrate_clear(struct intel_migrate *migrate,\n\t\t\t   struct i915_gem_ww_ctx *ww,\n\t\t\t   struct drm_i915_gem_object *obj,\n\t\t\t   u32 value,\n\t\t\t   struct i915_request **out)\n{\n\treturn intel_migrate_clear(migrate, ww, NULL,\n\t\t\t\t   obj->mm.pages->sgl,\n\t\t\t\t   obj->pat_index,\n\t\t\t\t   i915_gem_object_is_lmem(obj),\n\t\t\t\t   value, out);\n}\n\nstatic int __global_clear(struct intel_migrate *migrate,\n\t\t\t  struct i915_gem_ww_ctx *ww,\n\t\t\t  struct drm_i915_gem_object *obj,\n\t\t\t  u32 value,\n\t\t\t  struct i915_request **out)\n{\n\treturn intel_context_migrate_clear(migrate->context, NULL,\n\t\t\t\t\t   obj->mm.pages->sgl,\n\t\t\t\t\t   obj->pat_index,\n\t\t\t\t\t   i915_gem_object_is_lmem(obj),\n\t\t\t\t\t   value, out);\n}\n\nstatic int\nmigrate_clear(struct intel_migrate *migrate, u32 sz, struct rnd_state *prng)\n{\n\treturn clear(migrate, __migrate_clear, sz, prng);\n}\n\nstatic int\nglobal_clear(struct intel_migrate *migrate, u32 sz, struct rnd_state *prng)\n{\n\treturn clear(migrate, __global_clear, sz, prng);\n}\n\nstatic int live_migrate_copy(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\tstruct drm_i915_private *i915 = migrate->context->engine->i915;\n\tI915_RND_STATE(prng);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sizes); i++) {\n\t\tint err;\n\n\t\terr = migrate_copy(migrate, sizes[i], &prng);\n\t\tif (err == 0)\n\t\t\terr = global_copy(migrate, sizes[i], &prng);\n\t\ti915_gem_drain_freed_objects(i915);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int live_migrate_clear(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\tstruct drm_i915_private *i915 = migrate->context->engine->i915;\n\tI915_RND_STATE(prng);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sizes); i++) {\n\t\tint err;\n\n\t\terr = migrate_clear(migrate, sizes[i], &prng);\n\t\tif (err == 0)\n\t\t\terr = global_clear(migrate, sizes[i], &prng);\n\n\t\ti915_gem_drain_freed_objects(i915);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstruct spinner_timer {\n\tstruct timer_list timer;\n\tstruct igt_spinner spin;\n};\n\nstatic void spinner_kill(struct timer_list *timer)\n{\n\tstruct spinner_timer *st = from_timer(st, timer, timer);\n\n\tigt_spinner_end(&st->spin);\n\tpr_info(\"%s\\n\", __func__);\n}\n\nstatic int live_emit_pte_full_ring(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\tstruct drm_i915_private *i915 = migrate->context->engine->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct intel_context *ce;\n\tstruct i915_request *rq, *prev;\n\tstruct spinner_timer st;\n\tstruct sgt_dma it;\n\tint len, sz, err;\n\tu32 *cs;\n\n\t \n\n\tif (igt_spinner_init(&st.spin, to_gt(i915)))\n\t\treturn -ENOMEM;\n\n\tobj = i915_gem_object_create_internal(i915, 2 * PAGE_SIZE);\n\tif (IS_ERR(obj)) {\n\t\terr = PTR_ERR(obj);\n\t\tgoto out_spinner;\n\t}\n\n\terr = i915_gem_object_pin_pages_unlocked(obj);\n\tif (err)\n\t\tgoto out_obj;\n\n\tce = intel_migrate_create_context(migrate);\n\tif (IS_ERR(ce)) {\n\t\terr = PTR_ERR(ce);\n\t\tgoto out_obj;\n\t}\n\n\tce->ring_size = SZ_4K;  \n\n\terr = intel_context_pin(ce);\n\tif (err)\n\t\tgoto out_put;\n\n\trq = igt_spinner_create_request(&st.spin, ce, MI_ARB_CHECK);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto out_unpin;\n\t}\n\n\ti915_request_add(rq);\n\tif (!igt_wait_for_spinner(&st.spin, rq)) {\n\t\terr = -EIO;\n\t\tgoto out_unpin;\n\t}\n\n\t \n\n\tprev = NULL;\n\tdo {\n\t\tif (prev)\n\t\t\ti915_request_add(rq);\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_unpin;\n\t\t}\n\n\t\tsz = (rq->ring->space - rq->reserved_space) / sizeof(u32) -\n\t\t\tI915_EMIT_PTE_NUM_DWORDS;\n\t\tsz = min_t(u32, sz, (SZ_1K - rq->reserved_space) / sizeof(u32) -\n\t\t\t   I915_EMIT_PTE_NUM_DWORDS);\n\t\tcs = intel_ring_begin(rq, sz);\n\t\tif (IS_ERR(cs)) {\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto out_rq;\n\t\t}\n\n\t\tmemset32(cs, MI_NOOP, sz);\n\t\tcs += sz;\n\t\tintel_ring_advance(rq, cs);\n\n\t\tpr_info(\"%s emit=%u sz=%d\\n\", __func__, rq->ring->emit, sz);\n\n\t\tprev = rq;\n\t} while (rq->ring->space > (rq->reserved_space +\n\t\t\t\t    I915_EMIT_PTE_NUM_DWORDS * sizeof(u32)));\n\n\ttimer_setup_on_stack(&st.timer, spinner_kill, 0);\n\tmod_timer(&st.timer, jiffies + 2 * HZ);\n\n\t \n\tpr_info(\"%s emite_pte ring space=%u\\n\", __func__, rq->ring->space);\n\tit = sg_sgt(obj->mm.pages->sgl);\n\tlen = emit_pte(rq, &it, obj->pat_index, false, 0, CHUNK_SZ);\n\tif (!len) {\n\t\terr = -EINVAL;\n\t\tgoto out_rq;\n\t}\n\tif (len < 0) {\n\t\terr = len;\n\t\tgoto out_rq;\n\t}\n\nout_rq:\n\ti915_request_add(rq);  \n\tdel_timer_sync(&st.timer);\n\tdestroy_timer_on_stack(&st.timer);\nout_unpin:\n\tintel_context_unpin(ce);\nout_put:\n\tintel_context_put(ce);\nout_obj:\n\ti915_gem_object_put(obj);\nout_spinner:\n\tigt_spinner_fini(&st.spin);\n\treturn err;\n}\n\nstruct threaded_migrate {\n\tstruct intel_migrate *migrate;\n\tstruct task_struct *tsk;\n\tstruct rnd_state prng;\n};\n\nstatic int threaded_migrate(struct intel_migrate *migrate,\n\t\t\t    int (*fn)(void *arg),\n\t\t\t    unsigned int flags)\n{\n\tconst unsigned int n_cpus = num_online_cpus() + 1;\n\tstruct threaded_migrate *thread;\n\tI915_RND_STATE(prng);\n\tunsigned int i;\n\tint err = 0;\n\n\tthread = kcalloc(n_cpus, sizeof(*thread), GFP_KERNEL);\n\tif (!thread)\n\t\treturn 0;\n\n\tfor (i = 0; i < n_cpus; ++i) {\n\t\tstruct task_struct *tsk;\n\n\t\tthread[i].migrate = migrate;\n\t\tthread[i].prng =\n\t\t\tI915_RND_STATE_INITIALIZER(prandom_u32_state(&prng));\n\n\t\ttsk = kthread_run(fn, &thread[i], \"igt-%d\", i);\n\t\tif (IS_ERR(tsk)) {\n\t\t\terr = PTR_ERR(tsk);\n\t\t\tbreak;\n\t\t}\n\n\t\tget_task_struct(tsk);\n\t\tthread[i].tsk = tsk;\n\t}\n\n\tmsleep(10);  \n\n\tfor (i = 0; i < n_cpus; ++i) {\n\t\tstruct task_struct *tsk = thread[i].tsk;\n\t\tint status;\n\n\t\tif (IS_ERR_OR_NULL(tsk))\n\t\t\tcontinue;\n\n\t\tstatus = kthread_stop(tsk);\n\t\tif (status && !err)\n\t\t\terr = status;\n\n\t\tput_task_struct(tsk);\n\t}\n\n\tkfree(thread);\n\treturn err;\n}\n\nstatic int __thread_migrate_copy(void *arg)\n{\n\tstruct threaded_migrate *tm = arg;\n\n\treturn migrate_copy(tm->migrate, 2 * CHUNK_SZ, &tm->prng);\n}\n\nstatic int thread_migrate_copy(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\n\treturn threaded_migrate(migrate, __thread_migrate_copy, 0);\n}\n\nstatic int __thread_global_copy(void *arg)\n{\n\tstruct threaded_migrate *tm = arg;\n\n\treturn global_copy(tm->migrate, 2 * CHUNK_SZ, &tm->prng);\n}\n\nstatic int thread_global_copy(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\n\treturn threaded_migrate(migrate, __thread_global_copy, 0);\n}\n\nstatic int __thread_migrate_clear(void *arg)\n{\n\tstruct threaded_migrate *tm = arg;\n\n\treturn migrate_clear(tm->migrate, 2 * CHUNK_SZ, &tm->prng);\n}\n\nstatic int __thread_global_clear(void *arg)\n{\n\tstruct threaded_migrate *tm = arg;\n\n\treturn global_clear(tm->migrate, 2 * CHUNK_SZ, &tm->prng);\n}\n\nstatic int thread_migrate_clear(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\n\treturn threaded_migrate(migrate, __thread_migrate_clear, 0);\n}\n\nstatic int thread_global_clear(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_migrate *migrate = &gt->migrate;\n\n\treturn threaded_migrate(migrate, __thread_global_clear, 0);\n}\n\nint intel_migrate_live_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(live_migrate_copy),\n\t\tSUBTEST(live_migrate_clear),\n\t\tSUBTEST(live_emit_pte_full_ring),\n\t\tSUBTEST(thread_migrate_copy),\n\t\tSUBTEST(thread_migrate_clear),\n\t\tSUBTEST(thread_global_copy),\n\t\tSUBTEST(thread_global_clear),\n\t};\n\tstruct intel_gt *gt = to_gt(i915);\n\n\tif (!gt->migrate.context)\n\t\treturn 0;\n\n\treturn intel_gt_live_subtests(tests, gt);\n}\n\nstatic struct drm_i915_gem_object *\ncreate_init_lmem_internal(struct intel_gt *gt, size_t sz, bool try_lmem)\n{\n\tstruct drm_i915_gem_object *obj = NULL;\n\tint err;\n\n\tif (try_lmem)\n\t\tobj = i915_gem_object_create_lmem(gt->i915, sz, 0);\n\n\tif (IS_ERR_OR_NULL(obj)) {\n\t\tobj = i915_gem_object_create_internal(gt->i915, sz);\n\t\tif (IS_ERR(obj))\n\t\t\treturn obj;\n\t}\n\n\ti915_gem_object_trylock(obj, NULL);\n\terr = i915_gem_object_pin_pages(obj);\n\tif (err) {\n\t\ti915_gem_object_unlock(obj);\n\t\ti915_gem_object_put(obj);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn obj;\n}\n\nstatic int wrap_ktime_compare(const void *A, const void *B)\n{\n\tconst ktime_t *a = A, *b = B;\n\n\treturn ktime_compare(*a, *b);\n}\n\nstatic int __perf_clear_blt(struct intel_context *ce,\n\t\t\t    struct scatterlist *sg,\n\t\t\t    unsigned int pat_index,\n\t\t\t    bool is_lmem,\n\t\t\t    size_t sz)\n{\n\tktime_t t[5];\n\tint pass;\n\tint err = 0;\n\n\tfor (pass = 0; pass < ARRAY_SIZE(t); pass++) {\n\t\tstruct i915_request *rq;\n\t\tktime_t t0, t1;\n\n\t\tt0 = ktime_get();\n\n\t\terr = intel_context_migrate_clear(ce, NULL, sg, pat_index,\n\t\t\t\t\t\t  is_lmem, 0, &rq);\n\t\tif (rq) {\n\t\t\tif (i915_request_wait(rq, 0, MAX_SCHEDULE_TIMEOUT) < 0)\n\t\t\t\terr = -EIO;\n\t\t\ti915_request_put(rq);\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tt1 = ktime_get();\n\t\tt[pass] = ktime_sub(t1, t0);\n\t}\n\tif (err)\n\t\treturn err;\n\n\tsort(t, ARRAY_SIZE(t), sizeof(*t), wrap_ktime_compare, NULL);\n\tpr_info(\"%s: %zd KiB fill: %lld MiB/s\\n\",\n\t\tce->engine->name, sz >> 10,\n\t\tdiv64_u64(mul_u32_u32(4 * sz,\n\t\t\t\t      1000 * 1000 * 1000),\n\t\t\t  t[1] + 2 * t[2] + t[3]) >> 20);\n\treturn 0;\n}\n\nstatic int perf_clear_blt(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstatic const unsigned long sizes[] = {\n\t\tSZ_4K,\n\t\tSZ_64K,\n\t\tSZ_2M,\n\t\tSZ_64M\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sizes); i++) {\n\t\tstruct drm_i915_gem_object *dst;\n\t\tint err;\n\n\t\tdst = create_init_lmem_internal(gt, sizes[i], true);\n\t\tif (IS_ERR(dst))\n\t\t\treturn PTR_ERR(dst);\n\n\t\terr = __perf_clear_blt(gt->migrate.context,\n\t\t\t\t       dst->mm.pages->sgl,\n\t\t\t\t       i915_gem_get_pat_index(gt->i915,\n\t\t\t\t\t\t\t      I915_CACHE_NONE),\n\t\t\t\t       i915_gem_object_is_lmem(dst),\n\t\t\t\t       sizes[i]);\n\n\t\ti915_gem_object_unlock(dst);\n\t\ti915_gem_object_put(dst);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int __perf_copy_blt(struct intel_context *ce,\n\t\t\t   struct scatterlist *src,\n\t\t\t   unsigned int src_pat_index,\n\t\t\t   bool src_is_lmem,\n\t\t\t   struct scatterlist *dst,\n\t\t\t   unsigned int dst_pat_index,\n\t\t\t   bool dst_is_lmem,\n\t\t\t   size_t sz)\n{\n\tktime_t t[5];\n\tint pass;\n\tint err = 0;\n\n\tfor (pass = 0; pass < ARRAY_SIZE(t); pass++) {\n\t\tstruct i915_request *rq;\n\t\tktime_t t0, t1;\n\n\t\tt0 = ktime_get();\n\n\t\terr = intel_context_migrate_copy(ce, NULL,\n\t\t\t\t\t\t src, src_pat_index,\n\t\t\t\t\t\t src_is_lmem,\n\t\t\t\t\t\t dst, dst_pat_index,\n\t\t\t\t\t\t dst_is_lmem,\n\t\t\t\t\t\t &rq);\n\t\tif (rq) {\n\t\t\tif (i915_request_wait(rq, 0, MAX_SCHEDULE_TIMEOUT) < 0)\n\t\t\t\terr = -EIO;\n\t\t\ti915_request_put(rq);\n\t\t}\n\t\tif (err)\n\t\t\tbreak;\n\n\t\tt1 = ktime_get();\n\t\tt[pass] = ktime_sub(t1, t0);\n\t}\n\tif (err)\n\t\treturn err;\n\n\tsort(t, ARRAY_SIZE(t), sizeof(*t), wrap_ktime_compare, NULL);\n\tpr_info(\"%s: %zd KiB copy: %lld MiB/s\\n\",\n\t\tce->engine->name, sz >> 10,\n\t\tdiv64_u64(mul_u32_u32(4 * sz,\n\t\t\t\t      1000 * 1000 * 1000),\n\t\t\t  t[1] + 2 * t[2] + t[3]) >> 20);\n\treturn 0;\n}\n\nstatic int perf_copy_blt(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstatic const unsigned long sizes[] = {\n\t\tSZ_4K,\n\t\tSZ_64K,\n\t\tSZ_2M,\n\t\tSZ_64M\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sizes); i++) {\n\t\tstruct drm_i915_gem_object *src, *dst;\n\t\tsize_t sz;\n\t\tint err;\n\n\t\tsrc = create_init_lmem_internal(gt, sizes[i], true);\n\t\tif (IS_ERR(src))\n\t\t\treturn PTR_ERR(src);\n\n\t\tsz = src->base.size;\n\t\tdst = create_init_lmem_internal(gt, sz, false);\n\t\tif (IS_ERR(dst)) {\n\t\t\terr = PTR_ERR(dst);\n\t\t\tgoto err_src;\n\t\t}\n\n\t\terr = __perf_copy_blt(gt->migrate.context,\n\t\t\t\t      src->mm.pages->sgl,\n\t\t\t\t      i915_gem_get_pat_index(gt->i915,\n\t\t\t\t\t\t\t     I915_CACHE_NONE),\n\t\t\t\t      i915_gem_object_is_lmem(src),\n\t\t\t\t      dst->mm.pages->sgl,\n\t\t\t\t      i915_gem_get_pat_index(gt->i915,\n\t\t\t\t\t\t\t     I915_CACHE_NONE),\n\t\t\t\t      i915_gem_object_is_lmem(dst),\n\t\t\t\t      sz);\n\n\t\ti915_gem_object_unlock(dst);\n\t\ti915_gem_object_put(dst);\nerr_src:\n\t\ti915_gem_object_unlock(src);\n\t\ti915_gem_object_put(src);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nint intel_migrate_perf_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(perf_clear_blt),\n\t\tSUBTEST(perf_copy_blt),\n\t};\n\tstruct intel_gt *gt = to_gt(i915);\n\n\tif (intel_gt_is_wedged(gt))\n\t\treturn 0;\n\n\tif (!gt->migrate.context)\n\t\treturn 0;\n\n\treturn intel_gt_live_subtests(tests, gt);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}