{
  "module_name": "intel_reset.c",
  "hash_id": "574f711cb3ccd3d1b153f3d52b058b8c224f68b99987241cb3bb38daf6185602",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_reset.c",
  "human_readable_source": "\n \n\n#include <linux/sched/mm.h>\n#include <linux/stop_machine.h>\n#include <linux/string_helpers.h>\n\n#include \"display/intel_display_reset.h\"\n#include \"display/intel_overlay.h\"\n\n#include \"gem/i915_gem_context.h\"\n\n#include \"gt/intel_gt_regs.h\"\n\n#include \"gt/uc/intel_gsc_fw.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_file_private.h\"\n#include \"i915_gpu_error.h\"\n#include \"i915_irq.h\"\n#include \"i915_reg.h\"\n#include \"intel_breadcrumbs.h\"\n#include \"intel_engine_pm.h\"\n#include \"intel_engine_regs.h\"\n#include \"intel_gt.h\"\n#include \"intel_gt_pm.h\"\n#include \"intel_gt_requests.h\"\n#include \"intel_mchbar_regs.h\"\n#include \"intel_pci_config.h\"\n#include \"intel_reset.h\"\n\n#include \"uc/intel_guc.h\"\n\n#define RESET_MAX_RETRIES 3\n\nstatic void client_mark_guilty(struct i915_gem_context *ctx, bool banned)\n{\n\tstruct drm_i915_file_private *file_priv = ctx->file_priv;\n\tunsigned long prev_hang;\n\tunsigned int score;\n\n\tif (IS_ERR_OR_NULL(file_priv))\n\t\treturn;\n\n\tscore = 0;\n\tif (banned)\n\t\tscore = I915_CLIENT_SCORE_CONTEXT_BAN;\n\n\tprev_hang = xchg(&file_priv->hang_timestamp, jiffies);\n\tif (time_before(jiffies, prev_hang + I915_CLIENT_FAST_HANG_JIFFIES))\n\t\tscore += I915_CLIENT_SCORE_HANG_FAST;\n\n\tif (score) {\n\t\tatomic_add(score, &file_priv->ban_score);\n\n\t\tdrm_dbg(&ctx->i915->drm,\n\t\t\t\"client %s: gained %u ban score, now %u\\n\",\n\t\t\tctx->name, score,\n\t\t\tatomic_read(&file_priv->ban_score));\n\t}\n}\n\nstatic bool mark_guilty(struct i915_request *rq)\n{\n\tstruct i915_gem_context *ctx;\n\tunsigned long prev_hang;\n\tbool banned;\n\tint i;\n\n\tif (intel_context_is_closed(rq->context))\n\t\treturn true;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(rq->context->gem_context);\n\tif (ctx && !kref_get_unless_zero(&ctx->ref))\n\t\tctx = NULL;\n\trcu_read_unlock();\n\tif (!ctx)\n\t\treturn intel_context_is_banned(rq->context);\n\n\tatomic_inc(&ctx->guilty_count);\n\n\t \n\tif (!i915_gem_context_is_bannable(ctx)) {\n\t\tbanned = false;\n\t\tgoto out;\n\t}\n\n\tdrm_notice(&ctx->i915->drm,\n\t\t   \"%s context reset due to GPU hang\\n\",\n\t\t   ctx->name);\n\n\t \n\tprev_hang = ctx->hang_timestamp[0];\n\tfor (i = 0; i < ARRAY_SIZE(ctx->hang_timestamp) - 1; i++)\n\t\tctx->hang_timestamp[i] = ctx->hang_timestamp[i + 1];\n\tctx->hang_timestamp[i] = jiffies;\n\n\t \n\tbanned = !i915_gem_context_is_recoverable(ctx);\n\tif (time_before(jiffies, prev_hang + CONTEXT_FAST_HANG_JIFFIES))\n\t\tbanned = true;\n\tif (banned)\n\t\tdrm_dbg(&ctx->i915->drm, \"context %s: guilty %d, banned\\n\",\n\t\t\tctx->name, atomic_read(&ctx->guilty_count));\n\n\tclient_mark_guilty(ctx, banned);\n\nout:\n\ti915_gem_context_put(ctx);\n\treturn banned;\n}\n\nstatic void mark_innocent(struct i915_request *rq)\n{\n\tstruct i915_gem_context *ctx;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(rq->context->gem_context);\n\tif (ctx)\n\t\tatomic_inc(&ctx->active_count);\n\trcu_read_unlock();\n}\n\nvoid __i915_request_reset(struct i915_request *rq, bool guilty)\n{\n\tbool banned = false;\n\n\tRQ_TRACE(rq, \"guilty? %s\\n\", str_yes_no(guilty));\n\tGEM_BUG_ON(__i915_request_is_complete(rq));\n\n\trcu_read_lock();  \n\tif (guilty) {\n\t\ti915_request_set_error_once(rq, -EIO);\n\t\t__i915_request_skip(rq);\n\t\tbanned = mark_guilty(rq);\n\t} else {\n\t\ti915_request_set_error_once(rq, -EAGAIN);\n\t\tmark_innocent(rq);\n\t}\n\trcu_read_unlock();\n\n\tif (banned)\n\t\tintel_context_ban(rq->context, rq);\n}\n\nstatic bool i915_in_reset(struct pci_dev *pdev)\n{\n\tu8 gdrst;\n\n\tpci_read_config_byte(pdev, I915_GDRST, &gdrst);\n\treturn gdrst & GRDOM_RESET_STATUS;\n}\n\nstatic int i915_do_reset(struct intel_gt *gt,\n\t\t\t intel_engine_mask_t engine_mask,\n\t\t\t unsigned int retry)\n{\n\tstruct pci_dev *pdev = to_pci_dev(gt->i915->drm.dev);\n\tint err;\n\n\t \n\tpci_write_config_byte(pdev, I915_GDRST, GRDOM_RESET_ENABLE);\n\tudelay(50);\n\terr = wait_for_atomic(i915_in_reset(pdev), 50);\n\n\t \n\tpci_write_config_byte(pdev, I915_GDRST, 0);\n\tudelay(50);\n\tif (!err)\n\t\terr = wait_for_atomic(!i915_in_reset(pdev), 50);\n\n\treturn err;\n}\n\nstatic bool g4x_reset_complete(struct pci_dev *pdev)\n{\n\tu8 gdrst;\n\n\tpci_read_config_byte(pdev, I915_GDRST, &gdrst);\n\treturn (gdrst & GRDOM_RESET_ENABLE) == 0;\n}\n\nstatic int g33_do_reset(struct intel_gt *gt,\n\t\t\tintel_engine_mask_t engine_mask,\n\t\t\tunsigned int retry)\n{\n\tstruct pci_dev *pdev = to_pci_dev(gt->i915->drm.dev);\n\n\tpci_write_config_byte(pdev, I915_GDRST, GRDOM_RESET_ENABLE);\n\treturn wait_for_atomic(g4x_reset_complete(pdev), 50);\n}\n\nstatic int g4x_do_reset(struct intel_gt *gt,\n\t\t\tintel_engine_mask_t engine_mask,\n\t\t\tunsigned int retry)\n{\n\tstruct pci_dev *pdev = to_pci_dev(gt->i915->drm.dev);\n\tstruct intel_uncore *uncore = gt->uncore;\n\tint ret;\n\n\t \n\tintel_uncore_rmw_fw(uncore, VDECCLK_GATE_D, 0, VCP_UNIT_CLOCK_GATE_DISABLE);\n\tintel_uncore_posting_read_fw(uncore, VDECCLK_GATE_D);\n\n\tpci_write_config_byte(pdev, I915_GDRST,\n\t\t\t      GRDOM_MEDIA | GRDOM_RESET_ENABLE);\n\tret =  wait_for_atomic(g4x_reset_complete(pdev), 50);\n\tif (ret) {\n\t\tGT_TRACE(gt, \"Wait for media reset failed\\n\");\n\t\tgoto out;\n\t}\n\n\tpci_write_config_byte(pdev, I915_GDRST,\n\t\t\t      GRDOM_RENDER | GRDOM_RESET_ENABLE);\n\tret =  wait_for_atomic(g4x_reset_complete(pdev), 50);\n\tif (ret) {\n\t\tGT_TRACE(gt, \"Wait for render reset failed\\n\");\n\t\tgoto out;\n\t}\n\nout:\n\tpci_write_config_byte(pdev, I915_GDRST, 0);\n\n\tintel_uncore_rmw_fw(uncore, VDECCLK_GATE_D, VCP_UNIT_CLOCK_GATE_DISABLE, 0);\n\tintel_uncore_posting_read_fw(uncore, VDECCLK_GATE_D);\n\n\treturn ret;\n}\n\nstatic int ilk_do_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask,\n\t\t\tunsigned int retry)\n{\n\tstruct intel_uncore *uncore = gt->uncore;\n\tint ret;\n\n\tintel_uncore_write_fw(uncore, ILK_GDSR,\n\t\t\t      ILK_GRDOM_RENDER | ILK_GRDOM_RESET_ENABLE);\n\tret = __intel_wait_for_register_fw(uncore, ILK_GDSR,\n\t\t\t\t\t   ILK_GRDOM_RESET_ENABLE, 0,\n\t\t\t\t\t   5000, 0,\n\t\t\t\t\t   NULL);\n\tif (ret) {\n\t\tGT_TRACE(gt, \"Wait for render reset failed\\n\");\n\t\tgoto out;\n\t}\n\n\tintel_uncore_write_fw(uncore, ILK_GDSR,\n\t\t\t      ILK_GRDOM_MEDIA | ILK_GRDOM_RESET_ENABLE);\n\tret = __intel_wait_for_register_fw(uncore, ILK_GDSR,\n\t\t\t\t\t   ILK_GRDOM_RESET_ENABLE, 0,\n\t\t\t\t\t   5000, 0,\n\t\t\t\t\t   NULL);\n\tif (ret) {\n\t\tGT_TRACE(gt, \"Wait for media reset failed\\n\");\n\t\tgoto out;\n\t}\n\nout:\n\tintel_uncore_write_fw(uncore, ILK_GDSR, 0);\n\tintel_uncore_posting_read_fw(uncore, ILK_GDSR);\n\treturn ret;\n}\n\n \nstatic int gen6_hw_domain_reset(struct intel_gt *gt, u32 hw_domain_mask)\n{\n\tstruct intel_uncore *uncore = gt->uncore;\n\tint loops;\n\tint err;\n\n\t \n\tloops = GRAPHICS_VER_FULL(gt->i915) < IP_VER(12, 70) ? 2 : 1;\n\n\t \n\tdo {\n\t\tintel_uncore_write_fw(uncore, GEN6_GDRST, hw_domain_mask);\n\n\t\t \n\t\terr = __intel_wait_for_register_fw(uncore, GEN6_GDRST,\n\t\t\t\t\t\t   hw_domain_mask, 0,\n\t\t\t\t\t\t   2000, 0,\n\t\t\t\t\t\t   NULL);\n\t} while (err == 0 && --loops);\n\tif (err)\n\t\tGT_TRACE(gt,\n\t\t\t \"Wait for 0x%08x engines reset failed\\n\",\n\t\t\t hw_domain_mask);\n\n\t \n\tudelay(50);\n\n\treturn err;\n}\n\nstatic int __gen6_reset_engines(struct intel_gt *gt,\n\t\t\t\tintel_engine_mask_t engine_mask,\n\t\t\t\tunsigned int retry)\n{\n\tstruct intel_engine_cs *engine;\n\tu32 hw_mask;\n\n\tif (engine_mask == ALL_ENGINES) {\n\t\thw_mask = GEN6_GRDOM_FULL;\n\t} else {\n\t\tintel_engine_mask_t tmp;\n\n\t\thw_mask = 0;\n\t\tfor_each_engine_masked(engine, gt, engine_mask, tmp) {\n\t\t\thw_mask |= engine->reset_domain;\n\t\t}\n\t}\n\n\treturn gen6_hw_domain_reset(gt, hw_mask);\n}\n\nstatic int gen6_reset_engines(struct intel_gt *gt,\n\t\t\t      intel_engine_mask_t engine_mask,\n\t\t\t      unsigned int retry)\n{\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&gt->uncore->lock, flags);\n\tret = __gen6_reset_engines(gt, engine_mask, retry);\n\tspin_unlock_irqrestore(&gt->uncore->lock, flags);\n\n\treturn ret;\n}\n\nstatic struct intel_engine_cs *find_sfc_paired_vecs_engine(struct intel_engine_cs *engine)\n{\n\tint vecs_id;\n\n\tGEM_BUG_ON(engine->class != VIDEO_DECODE_CLASS);\n\n\tvecs_id = _VECS((engine->instance) / 2);\n\n\treturn engine->gt->engine[vecs_id];\n}\n\nstruct sfc_lock_data {\n\ti915_reg_t lock_reg;\n\ti915_reg_t ack_reg;\n\ti915_reg_t usage_reg;\n\tu32 lock_bit;\n\tu32 ack_bit;\n\tu32 usage_bit;\n\tu32 reset_bit;\n};\n\nstatic void get_sfc_forced_lock_data(struct intel_engine_cs *engine,\n\t\t\t\t     struct sfc_lock_data *sfc_lock)\n{\n\tswitch (engine->class) {\n\tdefault:\n\t\tMISSING_CASE(engine->class);\n\t\tfallthrough;\n\tcase VIDEO_DECODE_CLASS:\n\t\tsfc_lock->lock_reg = GEN11_VCS_SFC_FORCED_LOCK(engine->mmio_base);\n\t\tsfc_lock->lock_bit = GEN11_VCS_SFC_FORCED_LOCK_BIT;\n\n\t\tsfc_lock->ack_reg = GEN11_VCS_SFC_LOCK_STATUS(engine->mmio_base);\n\t\tsfc_lock->ack_bit  = GEN11_VCS_SFC_LOCK_ACK_BIT;\n\n\t\tsfc_lock->usage_reg = GEN11_VCS_SFC_LOCK_STATUS(engine->mmio_base);\n\t\tsfc_lock->usage_bit = GEN11_VCS_SFC_USAGE_BIT;\n\t\tsfc_lock->reset_bit = GEN11_VCS_SFC_RESET_BIT(engine->instance);\n\n\t\tbreak;\n\tcase VIDEO_ENHANCEMENT_CLASS:\n\t\tsfc_lock->lock_reg = GEN11_VECS_SFC_FORCED_LOCK(engine->mmio_base);\n\t\tsfc_lock->lock_bit = GEN11_VECS_SFC_FORCED_LOCK_BIT;\n\n\t\tsfc_lock->ack_reg = GEN11_VECS_SFC_LOCK_ACK(engine->mmio_base);\n\t\tsfc_lock->ack_bit  = GEN11_VECS_SFC_LOCK_ACK_BIT;\n\n\t\tsfc_lock->usage_reg = GEN11_VECS_SFC_USAGE(engine->mmio_base);\n\t\tsfc_lock->usage_bit = GEN11_VECS_SFC_USAGE_BIT;\n\t\tsfc_lock->reset_bit = GEN11_VECS_SFC_RESET_BIT(engine->instance);\n\n\t\tbreak;\n\t}\n}\n\nstatic int gen11_lock_sfc(struct intel_engine_cs *engine,\n\t\t\t  u32 *reset_mask,\n\t\t\t  u32 *unlock_mask)\n{\n\tstruct intel_uncore *uncore = engine->uncore;\n\tu8 vdbox_sfc_access = engine->gt->info.vdbox_sfc_access;\n\tstruct sfc_lock_data sfc_lock;\n\tbool lock_obtained, lock_to_other = false;\n\tint ret;\n\n\tswitch (engine->class) {\n\tcase VIDEO_DECODE_CLASS:\n\t\tif ((BIT(engine->instance) & vdbox_sfc_access) == 0)\n\t\t\treturn 0;\n\n\t\tfallthrough;\n\tcase VIDEO_ENHANCEMENT_CLASS:\n\t\tget_sfc_forced_lock_data(engine, &sfc_lock);\n\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (!(intel_uncore_read_fw(uncore, sfc_lock.usage_reg) & sfc_lock.usage_bit)) {\n\t\tstruct intel_engine_cs *paired_vecs;\n\n\t\tif (engine->class != VIDEO_DECODE_CLASS ||\n\t\t    GRAPHICS_VER(engine->i915) != 12)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!(intel_uncore_read_fw(uncore,\n\t\t\t\t\t   GEN12_HCP_SFC_LOCK_STATUS(engine->mmio_base)) &\n\t\t      GEN12_HCP_SFC_USAGE_BIT))\n\t\t\treturn 0;\n\n\t\tpaired_vecs = find_sfc_paired_vecs_engine(engine);\n\t\tget_sfc_forced_lock_data(paired_vecs, &sfc_lock);\n\t\tlock_to_other = true;\n\t\t*unlock_mask |= paired_vecs->mask;\n\t} else {\n\t\t*unlock_mask |= engine->mask;\n\t}\n\n\t \n\tintel_uncore_rmw_fw(uncore, sfc_lock.lock_reg, 0, sfc_lock.lock_bit);\n\n\tret = __intel_wait_for_register_fw(uncore,\n\t\t\t\t\t   sfc_lock.ack_reg,\n\t\t\t\t\t   sfc_lock.ack_bit,\n\t\t\t\t\t   sfc_lock.ack_bit,\n\t\t\t\t\t   1000, 0, NULL);\n\n\t \n\tlock_obtained = (intel_uncore_read_fw(uncore, sfc_lock.usage_reg) &\n\t\t\tsfc_lock.usage_bit) != 0;\n\tif (lock_obtained == lock_to_other)\n\t\treturn 0;\n\n\tif (ret) {\n\t\tENGINE_TRACE(engine, \"Wait for SFC forced lock ack failed\\n\");\n\t\treturn ret;\n\t}\n\n\t*reset_mask |= sfc_lock.reset_bit;\n\treturn 0;\n}\n\nstatic void gen11_unlock_sfc(struct intel_engine_cs *engine)\n{\n\tstruct intel_uncore *uncore = engine->uncore;\n\tu8 vdbox_sfc_access = engine->gt->info.vdbox_sfc_access;\n\tstruct sfc_lock_data sfc_lock = {};\n\n\tif (engine->class != VIDEO_DECODE_CLASS &&\n\t    engine->class != VIDEO_ENHANCEMENT_CLASS)\n\t\treturn;\n\n\tif (engine->class == VIDEO_DECODE_CLASS &&\n\t    (BIT(engine->instance) & vdbox_sfc_access) == 0)\n\t\treturn;\n\n\tget_sfc_forced_lock_data(engine, &sfc_lock);\n\n\tintel_uncore_rmw_fw(uncore, sfc_lock.lock_reg, sfc_lock.lock_bit, 0);\n}\n\nstatic int __gen11_reset_engines(struct intel_gt *gt,\n\t\t\t\t intel_engine_mask_t engine_mask,\n\t\t\t\t unsigned int retry)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t tmp;\n\tu32 reset_mask, unlock_mask = 0;\n\tint ret;\n\n\tif (engine_mask == ALL_ENGINES) {\n\t\treset_mask = GEN11_GRDOM_FULL;\n\t} else {\n\t\treset_mask = 0;\n\t\tfor_each_engine_masked(engine, gt, engine_mask, tmp) {\n\t\t\treset_mask |= engine->reset_domain;\n\t\t\tret = gen11_lock_sfc(engine, &reset_mask, &unlock_mask);\n\t\t\tif (ret)\n\t\t\t\tgoto sfc_unlock;\n\t\t}\n\t}\n\n\tret = gen6_hw_domain_reset(gt, reset_mask);\n\nsfc_unlock:\n\t \n\tfor_each_engine_masked(engine, gt, unlock_mask, tmp)\n\t\tgen11_unlock_sfc(engine);\n\n\treturn ret;\n}\n\nstatic int gen8_engine_reset_prepare(struct intel_engine_cs *engine)\n{\n\tstruct intel_uncore *uncore = engine->uncore;\n\tconst i915_reg_t reg = RING_RESET_CTL(engine->mmio_base);\n\tu32 request, mask, ack;\n\tint ret;\n\n\tif (I915_SELFTEST_ONLY(should_fail(&engine->reset_timeout, 1)))\n\t\treturn -ETIMEDOUT;\n\n\tack = intel_uncore_read_fw(uncore, reg);\n\tif (ack & RESET_CTL_CAT_ERROR) {\n\t\t \n\t\trequest = RESET_CTL_CAT_ERROR;\n\t\tmask = RESET_CTL_CAT_ERROR;\n\n\t\t \n\t\tack = 0;\n\t} else if (!(ack & RESET_CTL_READY_TO_RESET)) {\n\t\trequest = RESET_CTL_REQUEST_RESET;\n\t\tmask = RESET_CTL_READY_TO_RESET;\n\t\tack = RESET_CTL_READY_TO_RESET;\n\t} else {\n\t\treturn 0;\n\t}\n\n\tintel_uncore_write_fw(uncore, reg, _MASKED_BIT_ENABLE(request));\n\tret = __intel_wait_for_register_fw(uncore, reg, mask, ack,\n\t\t\t\t\t   700, 0, NULL);\n\tif (ret)\n\t\tdrm_err(&engine->i915->drm,\n\t\t\t\"%s reset request timed out: {request: %08x, RESET_CTL: %08x}\\n\",\n\t\t\tengine->name, request,\n\t\t\tintel_uncore_read_fw(uncore, reg));\n\n\treturn ret;\n}\n\nstatic void gen8_engine_reset_cancel(struct intel_engine_cs *engine)\n{\n\tintel_uncore_write_fw(engine->uncore,\n\t\t\t      RING_RESET_CTL(engine->mmio_base),\n\t\t\t      _MASKED_BIT_DISABLE(RESET_CTL_REQUEST_RESET));\n}\n\nstatic int gen8_reset_engines(struct intel_gt *gt,\n\t\t\t      intel_engine_mask_t engine_mask,\n\t\t\t      unsigned int retry)\n{\n\tstruct intel_engine_cs *engine;\n\tconst bool reset_non_ready = retry >= 1;\n\tintel_engine_mask_t tmp;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&gt->uncore->lock, flags);\n\n\tfor_each_engine_masked(engine, gt, engine_mask, tmp) {\n\t\tret = gen8_engine_reset_prepare(engine);\n\t\tif (ret && !reset_non_ready)\n\t\t\tgoto skip_reset;\n\n\t\t \n\t}\n\n\t \n\tif (IS_DG2(gt->i915) && engine_mask == ALL_ENGINES)\n\t\t__gen11_reset_engines(gt, gt->info.engine_mask, 0);\n\n\tif (GRAPHICS_VER(gt->i915) >= 11)\n\t\tret = __gen11_reset_engines(gt, engine_mask, retry);\n\telse\n\t\tret = __gen6_reset_engines(gt, engine_mask, retry);\n\nskip_reset:\n\tfor_each_engine_masked(engine, gt, engine_mask, tmp)\n\t\tgen8_engine_reset_cancel(engine);\n\n\tspin_unlock_irqrestore(&gt->uncore->lock, flags);\n\n\treturn ret;\n}\n\nstatic int mock_reset(struct intel_gt *gt,\n\t\t      intel_engine_mask_t mask,\n\t\t      unsigned int retry)\n{\n\treturn 0;\n}\n\ntypedef int (*reset_func)(struct intel_gt *,\n\t\t\t  intel_engine_mask_t engine_mask,\n\t\t\t  unsigned int retry);\n\nstatic reset_func intel_get_gpu_reset(const struct intel_gt *gt)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\n\tif (is_mock_gt(gt))\n\t\treturn mock_reset;\n\telse if (GRAPHICS_VER(i915) >= 8)\n\t\treturn gen8_reset_engines;\n\telse if (GRAPHICS_VER(i915) >= 6)\n\t\treturn gen6_reset_engines;\n\telse if (GRAPHICS_VER(i915) >= 5)\n\t\treturn ilk_do_reset;\n\telse if (IS_G4X(i915))\n\t\treturn g4x_do_reset;\n\telse if (IS_G33(i915) || IS_PINEVIEW(i915))\n\t\treturn g33_do_reset;\n\telse if (GRAPHICS_VER(i915) >= 3)\n\t\treturn i915_do_reset;\n\telse\n\t\treturn NULL;\n}\n\nstatic int __reset_guc(struct intel_gt *gt)\n{\n\tu32 guc_domain =\n\t\tGRAPHICS_VER(gt->i915) >= 11 ? GEN11_GRDOM_GUC : GEN9_GRDOM_GUC;\n\n\treturn gen6_hw_domain_reset(gt, guc_domain);\n}\n\nstatic bool needs_wa_14015076503(struct intel_gt *gt, intel_engine_mask_t engine_mask)\n{\n\tif (!IS_METEORLAKE(gt->i915) || !HAS_ENGINE(gt, GSC0))\n\t\treturn false;\n\n\tif (!__HAS_ENGINE(engine_mask, GSC0))\n\t\treturn false;\n\n\treturn intel_gsc_uc_fw_init_done(&gt->uc.gsc);\n}\n\nstatic intel_engine_mask_t\nwa_14015076503_start(struct intel_gt *gt, intel_engine_mask_t engine_mask, bool first)\n{\n\tif (!needs_wa_14015076503(gt, engine_mask))\n\t\treturn engine_mask;\n\n\t \n\tif (engine_mask == ALL_ENGINES && first && intel_engine_is_idle(gt->engine[GSC0])) {\n\t\t__reset_guc(gt);\n\t\tengine_mask = gt->info.engine_mask & ~BIT(GSC0);\n\t} else {\n\t\tintel_uncore_rmw(gt->uncore,\n\t\t\t\t HECI_H_GS1(MTL_GSC_HECI2_BASE),\n\t\t\t\t 0, HECI_H_GS1_ER_PREP);\n\n\t\t \n\t\tintel_uncore_rmw(gt->uncore,\n\t\t\t\t HECI_H_CSR(MTL_GSC_HECI2_BASE),\n\t\t\t\t HECI_H_CSR_RST, HECI_H_CSR_IG);\n\t\tmsleep(200);\n\t}\n\n\treturn engine_mask;\n}\n\nstatic void\nwa_14015076503_end(struct intel_gt *gt, intel_engine_mask_t engine_mask)\n{\n\tif (!needs_wa_14015076503(gt, engine_mask))\n\t\treturn;\n\n\tintel_uncore_rmw(gt->uncore,\n\t\t\t HECI_H_GS1(MTL_GSC_HECI2_BASE),\n\t\t\t HECI_H_GS1_ER_PREP, 0);\n}\n\nint __intel_gt_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask)\n{\n\tconst int retries = engine_mask == ALL_ENGINES ? RESET_MAX_RETRIES : 1;\n\treset_func reset;\n\tint ret = -ETIMEDOUT;\n\tint retry;\n\n\treset = intel_get_gpu_reset(gt);\n\tif (!reset)\n\t\treturn -ENODEV;\n\n\t \n\tintel_uncore_forcewake_get(gt->uncore, FORCEWAKE_ALL);\n\tfor (retry = 0; ret == -ETIMEDOUT && retry < retries; retry++) {\n\t\tintel_engine_mask_t reset_mask;\n\n\t\treset_mask = wa_14015076503_start(gt, engine_mask, !retry);\n\n\t\tGT_TRACE(gt, \"engine_mask=%x\\n\", reset_mask);\n\t\tpreempt_disable();\n\t\tret = reset(gt, reset_mask, retry);\n\t\tpreempt_enable();\n\n\t\twa_14015076503_end(gt, reset_mask);\n\t}\n\tintel_uncore_forcewake_put(gt->uncore, FORCEWAKE_ALL);\n\n\treturn ret;\n}\n\nbool intel_has_gpu_reset(const struct intel_gt *gt)\n{\n\tif (!gt->i915->params.reset)\n\t\treturn NULL;\n\n\treturn intel_get_gpu_reset(gt);\n}\n\nbool intel_has_reset_engine(const struct intel_gt *gt)\n{\n\tif (gt->i915->params.reset < 2)\n\t\treturn false;\n\n\treturn INTEL_INFO(gt->i915)->has_reset_engine;\n}\n\nint intel_reset_guc(struct intel_gt *gt)\n{\n\tint ret;\n\n\tGEM_BUG_ON(!HAS_GT_UC(gt->i915));\n\n\tintel_uncore_forcewake_get(gt->uncore, FORCEWAKE_ALL);\n\tret = __reset_guc(gt);\n\tintel_uncore_forcewake_put(gt->uncore, FORCEWAKE_ALL);\n\n\treturn ret;\n}\n\n \nstatic void reset_prepare_engine(struct intel_engine_cs *engine)\n{\n\t \n\tintel_uncore_forcewake_get(engine->uncore, FORCEWAKE_ALL);\n\tif (engine->reset.prepare)\n\t\tengine->reset.prepare(engine);\n}\n\nstatic void revoke_mmaps(struct intel_gt *gt)\n{\n\tint i;\n\n\tfor (i = 0; i < gt->ggtt->num_fences; i++) {\n\t\tstruct drm_vma_offset_node *node;\n\t\tstruct i915_vma *vma;\n\t\tu64 vma_offset;\n\n\t\tvma = READ_ONCE(gt->ggtt->fence_regs[i].vma);\n\t\tif (!vma)\n\t\t\tcontinue;\n\n\t\tif (!i915_vma_has_userfault(vma))\n\t\t\tcontinue;\n\n\t\tGEM_BUG_ON(vma->fence != &gt->ggtt->fence_regs[i]);\n\n\t\tif (!vma->mmo)\n\t\t\tcontinue;\n\n\t\tnode = &vma->mmo->vma_node;\n\t\tvma_offset = vma->gtt_view.partial.offset << PAGE_SHIFT;\n\n\t\tunmap_mapping_range(gt->i915->drm.anon_inode->i_mapping,\n\t\t\t\t    drm_vma_node_offset_addr(node) + vma_offset,\n\t\t\t\t    vma->size,\n\t\t\t\t    1);\n\t}\n}\n\nstatic intel_engine_mask_t reset_prepare(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t awake = 0;\n\tenum intel_engine_id id;\n\n\t \n\tintel_uc_reset_prepare(&gt->uc);\n\n\tfor_each_engine(engine, gt, id) {\n\t\tif (intel_engine_pm_get_if_awake(engine))\n\t\t\tawake |= engine->mask;\n\t\treset_prepare_engine(engine);\n\t}\n\n\treturn awake;\n}\n\nstatic void gt_revoke(struct intel_gt *gt)\n{\n\trevoke_mmaps(gt);\n}\n\nstatic int gt_reset(struct intel_gt *gt, intel_engine_mask_t stalled_mask)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tint err;\n\n\t \n\terr = i915_ggtt_enable_hw(gt->i915);\n\tif (err)\n\t\treturn err;\n\n\tlocal_bh_disable();\n\tfor_each_engine(engine, gt, id)\n\t\t__intel_engine_reset(engine, stalled_mask & engine->mask);\n\tlocal_bh_enable();\n\n\tintel_uc_reset(&gt->uc, ALL_ENGINES);\n\n\tintel_ggtt_restore_fences(gt->ggtt);\n\n\treturn err;\n}\n\nstatic void reset_finish_engine(struct intel_engine_cs *engine)\n{\n\tif (engine->reset.finish)\n\t\tengine->reset.finish(engine);\n\tintel_uncore_forcewake_put(engine->uncore, FORCEWAKE_ALL);\n\n\tintel_engine_signal_breadcrumbs(engine);\n}\n\nstatic void reset_finish(struct intel_gt *gt, intel_engine_mask_t awake)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tfor_each_engine(engine, gt, id) {\n\t\treset_finish_engine(engine);\n\t\tif (awake & engine->mask)\n\t\t\tintel_engine_pm_put(engine);\n\t}\n\n\tintel_uc_reset_finish(&gt->uc);\n}\n\nstatic void nop_submit_request(struct i915_request *request)\n{\n\tRQ_TRACE(request, \"-EIO\\n\");\n\n\trequest = i915_request_mark_eio(request);\n\tif (request) {\n\t\ti915_request_submit(request);\n\t\tintel_engine_signal_breadcrumbs(request->engine);\n\n\t\ti915_request_put(request);\n\t}\n}\n\nstatic void __intel_gt_set_wedged(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t awake;\n\tenum intel_engine_id id;\n\n\tif (test_bit(I915_WEDGED, &gt->reset.flags))\n\t\treturn;\n\n\tGT_TRACE(gt, \"start\\n\");\n\n\t \n\tawake = reset_prepare(gt);\n\n\t \n\tif (!INTEL_INFO(gt->i915)->gpu_reset_clobbers_display)\n\t\t__intel_gt_reset(gt, ALL_ENGINES);\n\n\tfor_each_engine(engine, gt, id)\n\t\tengine->submit_request = nop_submit_request;\n\n\t \n\tsynchronize_rcu_expedited();\n\tset_bit(I915_WEDGED, &gt->reset.flags);\n\n\t \n\tlocal_bh_disable();\n\tfor_each_engine(engine, gt, id)\n\t\tif (engine->reset.cancel)\n\t\t\tengine->reset.cancel(engine);\n\tintel_uc_cancel_requests(&gt->uc);\n\tlocal_bh_enable();\n\n\treset_finish(gt, awake);\n\n\tGT_TRACE(gt, \"end\\n\");\n}\n\nvoid intel_gt_set_wedged(struct intel_gt *gt)\n{\n\tintel_wakeref_t wakeref;\n\n\tif (test_bit(I915_WEDGED, &gt->reset.flags))\n\t\treturn;\n\n\twakeref = intel_runtime_pm_get(gt->uncore->rpm);\n\tmutex_lock(&gt->reset.mutex);\n\n\tif (GEM_SHOW_DEBUG()) {\n\t\tstruct drm_printer p = drm_debug_printer(__func__);\n\t\tstruct intel_engine_cs *engine;\n\t\tenum intel_engine_id id;\n\n\t\tdrm_printf(&p, \"called from %pS\\n\", (void *)_RET_IP_);\n\t\tfor_each_engine(engine, gt, id) {\n\t\t\tif (intel_engine_is_idle(engine))\n\t\t\t\tcontinue;\n\n\t\t\tintel_engine_dump(engine, &p, \"%s\\n\", engine->name);\n\t\t}\n\t}\n\n\t__intel_gt_set_wedged(gt);\n\n\tmutex_unlock(&gt->reset.mutex);\n\tintel_runtime_pm_put(gt->uncore->rpm, wakeref);\n}\n\nstatic bool __intel_gt_unset_wedged(struct intel_gt *gt)\n{\n\tstruct intel_gt_timelines *timelines = &gt->timelines;\n\tstruct intel_timeline *tl;\n\tbool ok;\n\n\tif (!test_bit(I915_WEDGED, &gt->reset.flags))\n\t\treturn true;\n\n\t \n\tif (intel_gt_has_unrecoverable_error(gt))\n\t\treturn false;\n\n\tGT_TRACE(gt, \"start\\n\");\n\n\t \n\tspin_lock(&timelines->lock);\n\tlist_for_each_entry(tl, &timelines->active_list, link) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = i915_active_fence_get(&tl->last_request);\n\t\tif (!fence)\n\t\t\tcontinue;\n\n\t\tspin_unlock(&timelines->lock);\n\n\t\t \n\t\tdma_fence_default_wait(fence, false, MAX_SCHEDULE_TIMEOUT);\n\t\tdma_fence_put(fence);\n\n\t\t \n\t\tspin_lock(&timelines->lock);\n\t\ttl = list_entry(&timelines->active_list, typeof(*tl), link);\n\t}\n\tspin_unlock(&timelines->lock);\n\n\t \n\tok = !HAS_EXECLISTS(gt->i915);  \n\tif (!INTEL_INFO(gt->i915)->gpu_reset_clobbers_display)\n\t\tok = __intel_gt_reset(gt, ALL_ENGINES) == 0;\n\tif (!ok) {\n\t\t \n\t\tadd_taint_for_CI(gt->i915, TAINT_WARN);\n\t\treturn false;\n\t}\n\n\t \n\tintel_engines_reset_default_submission(gt);\n\n\tGT_TRACE(gt, \"end\\n\");\n\n\tsmp_mb__before_atomic();  \n\tclear_bit(I915_WEDGED, &gt->reset.flags);\n\n\treturn true;\n}\n\nbool intel_gt_unset_wedged(struct intel_gt *gt)\n{\n\tbool result;\n\n\tmutex_lock(&gt->reset.mutex);\n\tresult = __intel_gt_unset_wedged(gt);\n\tmutex_unlock(&gt->reset.mutex);\n\n\treturn result;\n}\n\nstatic int do_reset(struct intel_gt *gt, intel_engine_mask_t stalled_mask)\n{\n\tint err, i;\n\n\terr = __intel_gt_reset(gt, ALL_ENGINES);\n\tfor (i = 0; err && i < RESET_MAX_RETRIES; i++) {\n\t\tmsleep(10 * (i + 1));\n\t\terr = __intel_gt_reset(gt, ALL_ENGINES);\n\t}\n\tif (err)\n\t\treturn err;\n\n\treturn gt_reset(gt, stalled_mask);\n}\n\nstatic int resume(struct intel_gt *gt)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tint ret;\n\n\tfor_each_engine(engine, gt, id) {\n\t\tret = intel_engine_resume(engine);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nvoid intel_gt_reset(struct intel_gt *gt,\n\t\t    intel_engine_mask_t stalled_mask,\n\t\t    const char *reason)\n{\n\tintel_engine_mask_t awake;\n\tint ret;\n\n\tGT_TRACE(gt, \"flags=%lx\\n\", gt->reset.flags);\n\n\tmight_sleep();\n\tGEM_BUG_ON(!test_bit(I915_RESET_BACKOFF, &gt->reset.flags));\n\n\t \n\tgt_revoke(gt);\n\n\tmutex_lock(&gt->reset.mutex);\n\n\t \n\tif (!__intel_gt_unset_wedged(gt))\n\t\tgoto unlock;\n\n\tif (reason)\n\t\tdrm_notice(&gt->i915->drm,\n\t\t\t   \"Resetting chip for %s\\n\", reason);\n\tatomic_inc(&gt->i915->gpu_error.reset_count);\n\n\tawake = reset_prepare(gt);\n\n\tif (!intel_has_gpu_reset(gt)) {\n\t\tif (gt->i915->params.reset)\n\t\t\tdrm_err(&gt->i915->drm, \"GPU reset not supported\\n\");\n\t\telse\n\t\t\tdrm_dbg(&gt->i915->drm, \"GPU reset disabled\\n\");\n\t\tgoto error;\n\t}\n\n\tif (INTEL_INFO(gt->i915)->gpu_reset_clobbers_display)\n\t\tintel_runtime_pm_disable_interrupts(gt->i915);\n\n\tif (do_reset(gt, stalled_mask)) {\n\t\tdrm_err(&gt->i915->drm, \"Failed to reset chip\\n\");\n\t\tgoto taint;\n\t}\n\n\tif (INTEL_INFO(gt->i915)->gpu_reset_clobbers_display)\n\t\tintel_runtime_pm_enable_interrupts(gt->i915);\n\n\tintel_overlay_reset(gt->i915);\n\n\t \n\tret = intel_gt_init_hw(gt);\n\tif (ret) {\n\t\tdrm_err(&gt->i915->drm,\n\t\t\t\"Failed to initialise HW following reset (%d)\\n\",\n\t\t\tret);\n\t\tgoto taint;\n\t}\n\n\tret = resume(gt);\n\tif (ret)\n\t\tgoto taint;\n\nfinish:\n\treset_finish(gt, awake);\nunlock:\n\tmutex_unlock(&gt->reset.mutex);\n\treturn;\n\ntaint:\n\t \n\tadd_taint_for_CI(gt->i915, TAINT_WARN);\nerror:\n\t__intel_gt_set_wedged(gt);\n\tgoto finish;\n}\n\nstatic int intel_gt_reset_engine(struct intel_engine_cs *engine)\n{\n\treturn __intel_gt_reset(engine->gt, engine->mask);\n}\n\nint __intel_engine_reset_bh(struct intel_engine_cs *engine, const char *msg)\n{\n\tstruct intel_gt *gt = engine->gt;\n\tint ret;\n\n\tENGINE_TRACE(engine, \"flags=%lx\\n\", gt->reset.flags);\n\tGEM_BUG_ON(!test_bit(I915_RESET_ENGINE + engine->id, &gt->reset.flags));\n\n\tif (intel_engine_uses_guc(engine))\n\t\treturn -ENODEV;\n\n\tif (!intel_engine_pm_get_if_awake(engine))\n\t\treturn 0;\n\n\treset_prepare_engine(engine);\n\n\tif (msg)\n\t\tdrm_notice(&engine->i915->drm,\n\t\t\t   \"Resetting %s for %s\\n\", engine->name, msg);\n\ti915_increase_reset_engine_count(&engine->i915->gpu_error, engine);\n\n\tret = intel_gt_reset_engine(engine);\n\tif (ret) {\n\t\t \n\t\tENGINE_TRACE(engine, \"Failed to reset %s, err: %d\\n\", engine->name, ret);\n\t\tgoto out;\n\t}\n\n\t \n\t__intel_engine_reset(engine, true);\n\n\t \n\tret = intel_engine_resume(engine);\n\nout:\n\tintel_engine_cancel_stop_cs(engine);\n\treset_finish_engine(engine);\n\tintel_engine_pm_put_async(engine);\n\treturn ret;\n}\n\n \nint intel_engine_reset(struct intel_engine_cs *engine, const char *msg)\n{\n\tint err;\n\n\tlocal_bh_disable();\n\terr = __intel_engine_reset_bh(engine, msg);\n\tlocal_bh_enable();\n\n\treturn err;\n}\n\nstatic void intel_gt_reset_global(struct intel_gt *gt,\n\t\t\t\t  u32 engine_mask,\n\t\t\t\t  const char *reason)\n{\n\tstruct kobject *kobj = &gt->i915->drm.primary->kdev->kobj;\n\tchar *error_event[] = { I915_ERROR_UEVENT \"=1\", NULL };\n\tchar *reset_event[] = { I915_RESET_UEVENT \"=1\", NULL };\n\tchar *reset_done_event[] = { I915_ERROR_UEVENT \"=0\", NULL };\n\tstruct intel_wedge_me w;\n\n\tkobject_uevent_env(kobj, KOBJ_CHANGE, error_event);\n\n\tGT_TRACE(gt, \"resetting chip, engines=%x\\n\", engine_mask);\n\tkobject_uevent_env(kobj, KOBJ_CHANGE, reset_event);\n\n\t \n\tintel_wedge_on_timeout(&w, gt, 60 * HZ) {\n\t\tintel_display_reset_prepare(gt->i915);\n\n\t\tintel_gt_reset(gt, engine_mask, reason);\n\n\t\tintel_display_reset_finish(gt->i915);\n\t}\n\n\tif (!test_bit(I915_WEDGED, &gt->reset.flags))\n\t\tkobject_uevent_env(kobj, KOBJ_CHANGE, reset_done_event);\n}\n\n \nvoid intel_gt_handle_error(struct intel_gt *gt,\n\t\t\t   intel_engine_mask_t engine_mask,\n\t\t\t   unsigned long flags,\n\t\t\t   const char *fmt, ...)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_wakeref_t wakeref;\n\tintel_engine_mask_t tmp;\n\tchar error_msg[80];\n\tchar *msg = NULL;\n\n\tif (fmt) {\n\t\tva_list args;\n\n\t\tva_start(args, fmt);\n\t\tvscnprintf(error_msg, sizeof(error_msg), fmt, args);\n\t\tva_end(args);\n\n\t\tmsg = error_msg;\n\t}\n\n\t \n\twakeref = intel_runtime_pm_get(gt->uncore->rpm);\n\n\tengine_mask &= gt->info.engine_mask;\n\n\tif (flags & I915_ERROR_CAPTURE) {\n\t\ti915_capture_error_state(gt, engine_mask, CORE_DUMP_FLAG_NONE);\n\t\tintel_gt_clear_error_registers(gt, engine_mask);\n\t}\n\n\t \n\tif (!intel_uc_uses_guc_submission(&gt->uc) &&\n\t    intel_has_reset_engine(gt) && !intel_gt_is_wedged(gt)) {\n\t\tlocal_bh_disable();\n\t\tfor_each_engine_masked(engine, gt, engine_mask, tmp) {\n\t\t\tBUILD_BUG_ON(I915_RESET_MODESET >= I915_RESET_ENGINE);\n\t\t\tif (test_and_set_bit(I915_RESET_ENGINE + engine->id,\n\t\t\t\t\t     &gt->reset.flags))\n\t\t\t\tcontinue;\n\n\t\t\tif (__intel_engine_reset_bh(engine, msg) == 0)\n\t\t\t\tengine_mask &= ~engine->mask;\n\n\t\t\tclear_and_wake_up_bit(I915_RESET_ENGINE + engine->id,\n\t\t\t\t\t      &gt->reset.flags);\n\t\t}\n\t\tlocal_bh_enable();\n\t}\n\n\tif (!engine_mask)\n\t\tgoto out;\n\n\t \n\tif (test_and_set_bit(I915_RESET_BACKOFF, &gt->reset.flags)) {\n\t\twait_event(gt->reset.queue,\n\t\t\t   !test_bit(I915_RESET_BACKOFF, &gt->reset.flags));\n\t\tgoto out;  \n\t}\n\n\t \n\tsynchronize_rcu_expedited();\n\n\t \n\tif (!intel_uc_uses_guc_submission(&gt->uc)) {\n\t\tfor_each_engine(engine, gt, tmp) {\n\t\t\twhile (test_and_set_bit(I915_RESET_ENGINE + engine->id,\n\t\t\t\t\t\t&gt->reset.flags))\n\t\t\t\twait_on_bit(&gt->reset.flags,\n\t\t\t\t\t    I915_RESET_ENGINE + engine->id,\n\t\t\t\t\t    TASK_UNINTERRUPTIBLE);\n\t\t}\n\t}\n\n\t \n\tsynchronize_srcu_expedited(&gt->reset.backoff_srcu);\n\n\tintel_gt_reset_global(gt, engine_mask, msg);\n\n\tif (!intel_uc_uses_guc_submission(&gt->uc)) {\n\t\tfor_each_engine(engine, gt, tmp)\n\t\t\tclear_bit_unlock(I915_RESET_ENGINE + engine->id,\n\t\t\t\t\t &gt->reset.flags);\n\t}\n\tclear_bit_unlock(I915_RESET_BACKOFF, &gt->reset.flags);\n\tsmp_mb__after_atomic();\n\twake_up_all(&gt->reset.queue);\n\nout:\n\tintel_runtime_pm_put(gt->uncore->rpm, wakeref);\n}\n\nstatic int _intel_gt_reset_lock(struct intel_gt *gt, int *srcu, bool retry)\n{\n\tmight_lock(&gt->reset.backoff_srcu);\n\tif (retry)\n\t\tmight_sleep();\n\n\trcu_read_lock();\n\twhile (test_bit(I915_RESET_BACKOFF, &gt->reset.flags)) {\n\t\trcu_read_unlock();\n\n\t\tif (!retry)\n\t\t\treturn -EBUSY;\n\n\t\tif (wait_event_interruptible(gt->reset.queue,\n\t\t\t\t\t     !test_bit(I915_RESET_BACKOFF,\n\t\t\t\t\t\t       &gt->reset.flags)))\n\t\t\treturn -EINTR;\n\n\t\trcu_read_lock();\n\t}\n\t*srcu = srcu_read_lock(&gt->reset.backoff_srcu);\n\trcu_read_unlock();\n\n\treturn 0;\n}\n\nint intel_gt_reset_trylock(struct intel_gt *gt, int *srcu)\n{\n\treturn _intel_gt_reset_lock(gt, srcu, false);\n}\n\nint intel_gt_reset_lock_interruptible(struct intel_gt *gt, int *srcu)\n{\n\treturn _intel_gt_reset_lock(gt, srcu, true);\n}\n\nvoid intel_gt_reset_unlock(struct intel_gt *gt, int tag)\n__releases(&gt->reset.backoff_srcu)\n{\n\tsrcu_read_unlock(&gt->reset.backoff_srcu, tag);\n}\n\nint intel_gt_terminally_wedged(struct intel_gt *gt)\n{\n\tmight_sleep();\n\n\tif (!intel_gt_is_wedged(gt))\n\t\treturn 0;\n\n\tif (intel_gt_has_unrecoverable_error(gt))\n\t\treturn -EIO;\n\n\t \n\tif (wait_event_interruptible(gt->reset.queue,\n\t\t\t\t     !test_bit(I915_RESET_BACKOFF,\n\t\t\t\t\t       &gt->reset.flags)))\n\t\treturn -EINTR;\n\n\treturn intel_gt_is_wedged(gt) ? -EIO : 0;\n}\n\nvoid intel_gt_set_wedged_on_init(struct intel_gt *gt)\n{\n\tBUILD_BUG_ON(I915_RESET_ENGINE + I915_NUM_ENGINES >\n\t\t     I915_WEDGED_ON_INIT);\n\tintel_gt_set_wedged(gt);\n\ti915_disable_error_state(gt->i915, -ENODEV);\n\tset_bit(I915_WEDGED_ON_INIT, &gt->reset.flags);\n\n\t \n\tadd_taint_for_CI(gt->i915, TAINT_WARN);\n}\n\nvoid intel_gt_set_wedged_on_fini(struct intel_gt *gt)\n{\n\tintel_gt_set_wedged(gt);\n\ti915_disable_error_state(gt->i915, -ENODEV);\n\tset_bit(I915_WEDGED_ON_FINI, &gt->reset.flags);\n\tintel_gt_retire_requests(gt);  \n}\n\nvoid intel_gt_init_reset(struct intel_gt *gt)\n{\n\tinit_waitqueue_head(&gt->reset.queue);\n\tmutex_init(&gt->reset.mutex);\n\tinit_srcu_struct(&gt->reset.backoff_srcu);\n\n\t \n\ti915_gem_shrinker_taints_mutex(gt->i915, &gt->reset.mutex);\n\n\t \n\t__set_bit(I915_WEDGED, &gt->reset.flags);\n}\n\nvoid intel_gt_fini_reset(struct intel_gt *gt)\n{\n\tcleanup_srcu_struct(&gt->reset.backoff_srcu);\n}\n\nstatic void intel_wedge_me(struct work_struct *work)\n{\n\tstruct intel_wedge_me *w = container_of(work, typeof(*w), work.work);\n\n\tdrm_err(&w->gt->i915->drm,\n\t\t\"%s timed out, cancelling all in-flight rendering.\\n\",\n\t\tw->name);\n\tintel_gt_set_wedged(w->gt);\n}\n\nvoid __intel_init_wedge(struct intel_wedge_me *w,\n\t\t\tstruct intel_gt *gt,\n\t\t\tlong timeout,\n\t\t\tconst char *name)\n{\n\tw->gt = gt;\n\tw->name = name;\n\n\tINIT_DELAYED_WORK_ONSTACK(&w->work, intel_wedge_me);\n\tqueue_delayed_work(gt->i915->unordered_wq, &w->work, timeout);\n}\n\nvoid __intel_fini_wedge(struct intel_wedge_me *w)\n{\n\tcancel_delayed_work_sync(&w->work);\n\tdestroy_delayed_work_on_stack(&w->work);\n\tw->gt = NULL;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_reset.c\"\n#include \"selftest_hangcheck.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}