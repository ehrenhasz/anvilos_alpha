{
  "module_name": "intel_guc.c",
  "hash_id": "7d784c5f44550f69c576c5bd72f69d43bbaf83721241380c66660e600aa05011",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/uc/intel_guc.c",
  "human_readable_source": "\n \n\n#include \"gem/i915_gem_lmem.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_irq.h\"\n#include \"gt/intel_gt_pm_irq.h\"\n#include \"gt/intel_gt_regs.h\"\n#include \"intel_guc.h\"\n#include \"intel_guc_ads.h\"\n#include \"intel_guc_capture.h\"\n#include \"intel_guc_print.h\"\n#include \"intel_guc_slpc.h\"\n#include \"intel_guc_submission.h\"\n#include \"i915_drv.h\"\n#include \"i915_irq.h\"\n#include \"i915_reg.h\"\n\n \n\nvoid intel_guc_notify(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\t \n\tintel_uncore_write(gt->uncore, guc->notify_reg, GUC_SEND_TRIGGER);\n}\n\nstatic inline i915_reg_t guc_send_reg(struct intel_guc *guc, u32 i)\n{\n\tGEM_BUG_ON(!guc->send_regs.base);\n\tGEM_BUG_ON(!guc->send_regs.count);\n\tGEM_BUG_ON(i >= guc->send_regs.count);\n\n\treturn _MMIO(guc->send_regs.base + 4 * i);\n}\n\nvoid intel_guc_init_send_regs(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tenum forcewake_domains fw_domains = 0;\n\tunsigned int i;\n\n\tGEM_BUG_ON(!guc->send_regs.base);\n\tGEM_BUG_ON(!guc->send_regs.count);\n\n\tfor (i = 0; i < guc->send_regs.count; i++) {\n\t\tfw_domains |= intel_uncore_forcewake_for_reg(gt->uncore,\n\t\t\t\t\tguc_send_reg(guc, i),\n\t\t\t\t\tFW_REG_READ | FW_REG_WRITE);\n\t}\n\tguc->send_regs.fw_domains = fw_domains;\n}\n\nstatic void gen9_reset_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tassert_rpm_wakelock_held(&gt->i915->runtime_pm);\n\n\tspin_lock_irq(gt->irq_lock);\n\tgen6_gt_pm_reset_iir(gt, gt->pm_guc_events);\n\tspin_unlock_irq(gt->irq_lock);\n}\n\nstatic void gen9_enable_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tassert_rpm_wakelock_held(&gt->i915->runtime_pm);\n\n\tspin_lock_irq(gt->irq_lock);\n\tguc_WARN_ON_ONCE(guc, intel_uncore_read(gt->uncore, GEN8_GT_IIR(2)) &\n\t\t\t gt->pm_guc_events);\n\tgen6_gt_pm_enable_irq(gt, gt->pm_guc_events);\n\tspin_unlock_irq(gt->irq_lock);\n\n\tguc->interrupts.enabled = true;\n}\n\nstatic void gen9_disable_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tassert_rpm_wakelock_held(&gt->i915->runtime_pm);\n\tguc->interrupts.enabled = false;\n\n\tspin_lock_irq(gt->irq_lock);\n\n\tgen6_gt_pm_disable_irq(gt, gt->pm_guc_events);\n\n\tspin_unlock_irq(gt->irq_lock);\n\tintel_synchronize_irq(gt->i915);\n\n\tgen9_reset_guc_interrupts(guc);\n}\n\nstatic bool __gen11_reset_guc_interrupts(struct intel_gt *gt)\n{\n\tu32 irq = gt->type == GT_MEDIA ? MTL_MGUC : GEN11_GUC;\n\n\tlockdep_assert_held(gt->irq_lock);\n\treturn gen11_gt_reset_one_iir(gt, 0, irq);\n}\n\nstatic void gen11_reset_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tspin_lock_irq(gt->irq_lock);\n\t__gen11_reset_guc_interrupts(gt);\n\tspin_unlock_irq(gt->irq_lock);\n}\n\nstatic void gen11_enable_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tspin_lock_irq(gt->irq_lock);\n\t__gen11_reset_guc_interrupts(gt);\n\tspin_unlock_irq(gt->irq_lock);\n\n\tguc->interrupts.enabled = true;\n}\n\nstatic void gen11_disable_guc_interrupts(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tguc->interrupts.enabled = false;\n\tintel_synchronize_irq(gt->i915);\n\n\tgen11_reset_guc_interrupts(guc);\n}\n\nvoid intel_guc_init_early(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_private *i915 = gt->i915;\n\n\tintel_uc_fw_init_early(&guc->fw, INTEL_UC_FW_TYPE_GUC, true);\n\tintel_guc_ct_init_early(&guc->ct);\n\tintel_guc_log_init_early(&guc->log);\n\tintel_guc_submission_init_early(guc);\n\tintel_guc_slpc_init_early(&guc->slpc);\n\tintel_guc_rc_init_early(guc);\n\n\tmutex_init(&guc->send_mutex);\n\tspin_lock_init(&guc->irq_lock);\n\tif (GRAPHICS_VER(i915) >= 11) {\n\t\tguc->interrupts.reset = gen11_reset_guc_interrupts;\n\t\tguc->interrupts.enable = gen11_enable_guc_interrupts;\n\t\tguc->interrupts.disable = gen11_disable_guc_interrupts;\n\t\tif (gt->type == GT_MEDIA) {\n\t\t\tguc->notify_reg = MEDIA_GUC_HOST_INTERRUPT;\n\t\t\tguc->send_regs.base = i915_mmio_reg_offset(MEDIA_SOFT_SCRATCH(0));\n\t\t} else {\n\t\t\tguc->notify_reg = GEN11_GUC_HOST_INTERRUPT;\n\t\t\tguc->send_regs.base = i915_mmio_reg_offset(GEN11_SOFT_SCRATCH(0));\n\t\t}\n\n\t\tguc->send_regs.count = GEN11_SOFT_SCRATCH_COUNT;\n\n\t} else {\n\t\tguc->notify_reg = GUC_SEND_INTERRUPT;\n\t\tguc->interrupts.reset = gen9_reset_guc_interrupts;\n\t\tguc->interrupts.enable = gen9_enable_guc_interrupts;\n\t\tguc->interrupts.disable = gen9_disable_guc_interrupts;\n\t\tguc->send_regs.base = i915_mmio_reg_offset(SOFT_SCRATCH(0));\n\t\tguc->send_regs.count = GUC_MAX_MMIO_MSG_LEN;\n\t\tBUILD_BUG_ON(GUC_MAX_MMIO_MSG_LEN > SOFT_SCRATCH_COUNT);\n\t}\n\n\tintel_guc_enable_msg(guc, INTEL_GUC_RECV_MSG_EXCEPTION |\n\t\t\t\t  INTEL_GUC_RECV_MSG_CRASH_DUMP_POSTED);\n}\n\nvoid intel_guc_init_late(struct intel_guc *guc)\n{\n\tintel_guc_ads_init_late(guc);\n}\n\nstatic u32 guc_ctl_debug_flags(struct intel_guc *guc)\n{\n\tu32 level = intel_guc_log_get_level(&guc->log);\n\tu32 flags = 0;\n\n\tif (!GUC_LOG_LEVEL_IS_VERBOSE(level))\n\t\tflags |= GUC_LOG_DISABLED;\n\telse\n\t\tflags |= GUC_LOG_LEVEL_TO_VERBOSITY(level) <<\n\t\t\t GUC_LOG_VERBOSITY_SHIFT;\n\n\treturn flags;\n}\n\nstatic u32 guc_ctl_feature_flags(struct intel_guc *guc)\n{\n\tu32 flags = 0;\n\n\tif (!intel_guc_submission_is_used(guc))\n\t\tflags |= GUC_CTL_DISABLE_SCHEDULER;\n\n\tif (intel_guc_slpc_is_used(guc))\n\t\tflags |= GUC_CTL_ENABLE_SLPC;\n\n\treturn flags;\n}\n\nstatic u32 guc_ctl_log_params_flags(struct intel_guc *guc)\n{\n\tstruct intel_guc_log *log = &guc->log;\n\tu32 offset, flags;\n\n\tGEM_BUG_ON(!log->sizes_initialised);\n\n\toffset = intel_guc_ggtt_offset(guc, log->vma) >> PAGE_SHIFT;\n\n\tflags = GUC_LOG_VALID |\n\t\tGUC_LOG_NOTIFY_ON_HALF_FULL |\n\t\tlog->sizes[GUC_LOG_SECTIONS_DEBUG].flag |\n\t\tlog->sizes[GUC_LOG_SECTIONS_CAPTURE].flag |\n\t\t(log->sizes[GUC_LOG_SECTIONS_CRASH].count << GUC_LOG_CRASH_SHIFT) |\n\t\t(log->sizes[GUC_LOG_SECTIONS_DEBUG].count << GUC_LOG_DEBUG_SHIFT) |\n\t\t(log->sizes[GUC_LOG_SECTIONS_CAPTURE].count << GUC_LOG_CAPTURE_SHIFT) |\n\t\t(offset << GUC_LOG_BUF_ADDR_SHIFT);\n\n\treturn flags;\n}\n\nstatic u32 guc_ctl_ads_flags(struct intel_guc *guc)\n{\n\tu32 ads = intel_guc_ggtt_offset(guc, guc->ads_vma) >> PAGE_SHIFT;\n\tu32 flags = ads << GUC_ADS_ADDR_SHIFT;\n\n\treturn flags;\n}\n\nstatic u32 guc_ctl_wa_flags(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tu32 flags = 0;\n\n\t \n\tif (GRAPHICS_VER(gt->i915) >= 11 &&\n\t    GRAPHICS_VER_FULL(gt->i915) < IP_VER(12, 50))\n\t\tflags |= GUC_WA_POLLCS;\n\n\t \n\tif (IS_DG2_GRAPHICS_STEP(gt->i915, G10, STEP_A0, STEP_B0))\n\t\tflags |= GUC_WA_GAM_CREDITS;\n\n\t \n\tif (IS_MTL_GRAPHICS_STEP(gt->i915, M, STEP_A0, STEP_B0) ||\n\t    IS_DG2(gt->i915))\n\t\tflags |= GUC_WA_HOLD_CCS_SWITCHOUT;\n\n\t \n\tif (IS_DG2(gt->i915))\n\t\tflags |= GUC_WA_DUAL_QUEUE;\n\n\t \n\tif (IS_MTL_GRAPHICS_STEP(gt->i915, M, STEP_A0, STEP_B0) ||\n\t    (GRAPHICS_VER(gt->i915) >= 11 &&\n\t    GRAPHICS_VER_FULL(gt->i915) < IP_VER(12, 70)))\n\t\tflags |= GUC_WA_PRE_PARSER;\n\n\t \n\tif (IS_DG2_GRAPHICS_STEP(gt->i915, G10, STEP_A0, STEP_C0) ||\n\t    IS_DG2_GRAPHICS_STEP(gt->i915, G11, STEP_A0, STEP_B0))\n\t\tflags |= GUC_WA_RCS_RESET_BEFORE_RC6;\n\n\t \n\tif (IS_DG2_GRAPHICS_STEP(gt->i915, G10, STEP_A0, STEP_C0) ||\n\t    IS_DG2_GRAPHICS_STEP(gt->i915, G11, STEP_A0, STEP_FOREVER))\n\t\tflags |= GUC_WA_CONTEXT_ISOLATION;\n\n\t \n\tif (!RCS_MASK(gt))\n\t\tflags |= GUC_WA_RCS_REGS_IN_CCS_REGS_LIST;\n\n\treturn flags;\n}\n\nstatic u32 guc_ctl_devid(struct intel_guc *guc)\n{\n\tstruct drm_i915_private *i915 = guc_to_gt(guc)->i915;\n\n\treturn (INTEL_DEVID(i915) << 16) | INTEL_REVID(i915);\n}\n\n \nstatic void guc_init_params(struct intel_guc *guc)\n{\n\tu32 *params = guc->params;\n\tint i;\n\n\tBUILD_BUG_ON(sizeof(guc->params) != GUC_CTL_MAX_DWORDS * sizeof(u32));\n\n\tparams[GUC_CTL_LOG_PARAMS] = guc_ctl_log_params_flags(guc);\n\tparams[GUC_CTL_FEATURE] = guc_ctl_feature_flags(guc);\n\tparams[GUC_CTL_DEBUG] = guc_ctl_debug_flags(guc);\n\tparams[GUC_CTL_ADS] = guc_ctl_ads_flags(guc);\n\tparams[GUC_CTL_WA] = guc_ctl_wa_flags(guc);\n\tparams[GUC_CTL_DEVID] = guc_ctl_devid(guc);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tguc_dbg(guc, \"param[%2d] = %#x\\n\", i, params[i]);\n}\n\n \nvoid intel_guc_write_params(struct intel_guc *guc)\n{\n\tstruct intel_uncore *uncore = guc_to_gt(guc)->uncore;\n\tint i;\n\n\t \n\tintel_uncore_forcewake_get(uncore, FORCEWAKE_GT);\n\n\tintel_uncore_write(uncore, SOFT_SCRATCH(0), 0);\n\n\tfor (i = 0; i < GUC_CTL_MAX_DWORDS; i++)\n\t\tintel_uncore_write(uncore, SOFT_SCRATCH(1 + i), guc->params[i]);\n\n\tintel_uncore_forcewake_put(uncore, FORCEWAKE_GT);\n}\n\nvoid intel_guc_dump_time_info(struct intel_guc *guc, struct drm_printer *p)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tintel_wakeref_t wakeref;\n\tu32 stamp = 0;\n\tu64 ktime;\n\n\twith_intel_runtime_pm(&gt->i915->runtime_pm, wakeref)\n\t\tstamp = intel_uncore_read(gt->uncore, GUCPMTIMESTAMP);\n\tktime = ktime_get_boottime_ns();\n\n\tdrm_printf(p, \"Kernel timestamp: 0x%08llX [%llu]\\n\", ktime, ktime);\n\tdrm_printf(p, \"GuC timestamp: 0x%08X [%u]\\n\", stamp, stamp);\n\tdrm_printf(p, \"CS timestamp frequency: %u Hz, %u ns\\n\",\n\t\t   gt->clock_frequency, gt->clock_period_ns);\n}\n\nint intel_guc_init(struct intel_guc *guc)\n{\n\tint ret;\n\n\tret = intel_uc_fw_init(&guc->fw);\n\tif (ret)\n\t\tgoto out;\n\n\tret = intel_guc_log_create(&guc->log);\n\tif (ret)\n\t\tgoto err_fw;\n\n\tret = intel_guc_capture_init(guc);\n\tif (ret)\n\t\tgoto err_log;\n\n\tret = intel_guc_ads_create(guc);\n\tif (ret)\n\t\tgoto err_capture;\n\n\tGEM_BUG_ON(!guc->ads_vma);\n\n\tret = intel_guc_ct_init(&guc->ct);\n\tif (ret)\n\t\tgoto err_ads;\n\n\tif (intel_guc_submission_is_used(guc)) {\n\t\t \n\t\tret = intel_guc_submission_init(guc);\n\t\tif (ret)\n\t\t\tgoto err_ct;\n\t}\n\n\tif (intel_guc_slpc_is_used(guc)) {\n\t\tret = intel_guc_slpc_init(&guc->slpc);\n\t\tif (ret)\n\t\t\tgoto err_submission;\n\t}\n\n\t \n\tguc_init_params(guc);\n\n\tintel_uc_fw_change_status(&guc->fw, INTEL_UC_FIRMWARE_LOADABLE);\n\n\treturn 0;\n\nerr_submission:\n\tintel_guc_submission_fini(guc);\nerr_ct:\n\tintel_guc_ct_fini(&guc->ct);\nerr_ads:\n\tintel_guc_ads_destroy(guc);\nerr_capture:\n\tintel_guc_capture_destroy(guc);\nerr_log:\n\tintel_guc_log_destroy(&guc->log);\nerr_fw:\n\tintel_uc_fw_fini(&guc->fw);\nout:\n\tintel_uc_fw_change_status(&guc->fw, INTEL_UC_FIRMWARE_INIT_FAIL);\n\tguc_probe_error(guc, \"failed with %pe\\n\", ERR_PTR(ret));\n\treturn ret;\n}\n\nvoid intel_guc_fini(struct intel_guc *guc)\n{\n\tif (!intel_uc_fw_is_loadable(&guc->fw))\n\t\treturn;\n\n\tif (intel_guc_slpc_is_used(guc))\n\t\tintel_guc_slpc_fini(&guc->slpc);\n\n\tif (intel_guc_submission_is_used(guc))\n\t\tintel_guc_submission_fini(guc);\n\n\tintel_guc_ct_fini(&guc->ct);\n\n\tintel_guc_ads_destroy(guc);\n\tintel_guc_capture_destroy(guc);\n\tintel_guc_log_destroy(&guc->log);\n\tintel_uc_fw_fini(&guc->fw);\n}\n\n \nint intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,\n\t\t\tu32 *response_buf, u32 response_buf_size)\n{\n\tstruct intel_uncore *uncore = guc_to_gt(guc)->uncore;\n\tu32 header;\n\tint i;\n\tint ret;\n\n\tGEM_BUG_ON(!len);\n\tGEM_BUG_ON(len > guc->send_regs.count);\n\n\tGEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, request[0]) != GUC_HXG_ORIGIN_HOST);\n\tGEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, request[0]) != GUC_HXG_TYPE_REQUEST);\n\n\tmutex_lock(&guc->send_mutex);\n\tintel_uncore_forcewake_get(uncore, guc->send_regs.fw_domains);\n\nretry:\n\tfor (i = 0; i < len; i++)\n\t\tintel_uncore_write(uncore, guc_send_reg(guc, i), request[i]);\n\n\tintel_uncore_posting_read(uncore, guc_send_reg(guc, i - 1));\n\n\tintel_guc_notify(guc);\n\n\t \n\tret = __intel_wait_for_register_fw(uncore,\n\t\t\t\t\t   guc_send_reg(guc, 0),\n\t\t\t\t\t   GUC_HXG_MSG_0_ORIGIN,\n\t\t\t\t\t   FIELD_PREP(GUC_HXG_MSG_0_ORIGIN,\n\t\t\t\t\t\t      GUC_HXG_ORIGIN_GUC),\n\t\t\t\t\t   10, 10, &header);\n\tif (unlikely(ret)) {\ntimeout:\n\t\tguc_err(guc, \"mmio request %#x: no reply %x\\n\",\n\t\t\trequest[0], header);\n\t\tgoto out;\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) == GUC_HXG_TYPE_NO_RESPONSE_BUSY) {\n#define done ({ header = intel_uncore_read(uncore, guc_send_reg(guc, 0)); \\\n\t\tFIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) != GUC_HXG_ORIGIN_GUC || \\\n\t\tFIELD_GET(GUC_HXG_MSG_0_TYPE, header) != GUC_HXG_TYPE_NO_RESPONSE_BUSY; })\n\n\t\tret = wait_for(done, 1000);\n\t\tif (unlikely(ret))\n\t\t\tgoto timeout;\n\t\tif (unlikely(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, header) !=\n\t\t\t\t       GUC_HXG_ORIGIN_GUC))\n\t\t\tgoto proto;\n#undef done\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) == GUC_HXG_TYPE_NO_RESPONSE_RETRY) {\n\t\tu32 reason = FIELD_GET(GUC_HXG_RETRY_MSG_0_REASON, header);\n\n\t\tguc_dbg(guc, \"mmio request %#x: retrying, reason %u\\n\",\n\t\t\trequest[0], reason);\n\t\tgoto retry;\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) == GUC_HXG_TYPE_RESPONSE_FAILURE) {\n\t\tu32 hint = FIELD_GET(GUC_HXG_FAILURE_MSG_0_HINT, header);\n\t\tu32 error = FIELD_GET(GUC_HXG_FAILURE_MSG_0_ERROR, header);\n\n\t\tguc_err(guc, \"mmio request %#x: failure %x/%u\\n\",\n\t\t\trequest[0], error, hint);\n\t\tret = -ENXIO;\n\t\tgoto out;\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, header) != GUC_HXG_TYPE_RESPONSE_SUCCESS) {\nproto:\n\t\tguc_err(guc, \"mmio request %#x: unexpected reply %#x\\n\",\n\t\t\trequest[0], header);\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\tif (response_buf) {\n\t\tint count = min(response_buf_size, guc->send_regs.count);\n\n\t\tGEM_BUG_ON(!count);\n\n\t\tresponse_buf[0] = header;\n\n\t\tfor (i = 1; i < count; i++)\n\t\t\tresponse_buf[i] = intel_uncore_read(uncore,\n\t\t\t\t\t\t\t    guc_send_reg(guc, i));\n\n\t\t \n\t\tret = count;\n\t} else {\n\t\t \n\t\tret = FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, header);\n\t}\n\nout:\n\tintel_uncore_forcewake_put(uncore, guc->send_regs.fw_domains);\n\tmutex_unlock(&guc->send_mutex);\n\n\treturn ret;\n}\n\nint intel_guc_to_host_process_recv_msg(struct intel_guc *guc,\n\t\t\t\t       const u32 *payload, u32 len)\n{\n\tu32 msg;\n\n\tif (unlikely(!len))\n\t\treturn -EPROTO;\n\n\t \n\tmsg = payload[0] & guc->msg_enabled_mask;\n\n\tif (msg & INTEL_GUC_RECV_MSG_CRASH_DUMP_POSTED)\n\t\tguc_err(guc, \"Received early crash dump notification!\\n\");\n\tif (msg & INTEL_GUC_RECV_MSG_EXCEPTION)\n\t\tguc_err(guc, \"Received early exception notification!\\n\");\n\n\treturn 0;\n}\n\n \nint intel_guc_auth_huc(struct intel_guc *guc, u32 rsa_offset)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_AUTHENTICATE_HUC,\n\t\trsa_offset\n\t};\n\n\treturn intel_guc_send(guc, action, ARRAY_SIZE(action));\n}\n\n \nint intel_guc_suspend(struct intel_guc *guc)\n{\n\tint ret;\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_CLIENT_SOFT_RESET,\n\t};\n\n\tif (!intel_guc_is_ready(guc))\n\t\treturn 0;\n\n\tif (intel_guc_submission_is_used(guc)) {\n\t\t \n\t\tret = intel_guc_send_mmio(guc, action, ARRAY_SIZE(action), NULL, 0);\n\t\tif (ret)\n\t\t\tguc_err(guc, \"suspend: RESET_CLIENT action failed with %pe\\n\",\n\t\t\t\tERR_PTR(ret));\n\t}\n\n\t \n\tintel_guc_sanitize(guc);\n\n\treturn 0;\n}\n\n \nint intel_guc_resume(struct intel_guc *guc)\n{\n\t \n\treturn 0;\n}\n\n \n\n \nstruct i915_vma *intel_guc_allocate_vma(struct intel_guc *guc, u32 size)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tu64 flags;\n\tint ret;\n\n\tif (HAS_LMEM(gt->i915))\n\t\tobj = i915_gem_object_create_lmem(gt->i915, size,\n\t\t\t\t\t\t  I915_BO_ALLOC_CPU_CLEAR |\n\t\t\t\t\t\t  I915_BO_ALLOC_CONTIGUOUS |\n\t\t\t\t\t\t  I915_BO_ALLOC_PM_EARLY);\n\telse\n\t\tobj = i915_gem_object_create_shmem(gt->i915, size);\n\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\t \n\tif (intel_gt_needs_wa_22016122933(gt))\n\t\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_NONE);\n\n\tvma = i915_vma_instance(obj, &gt->ggtt->vm, NULL);\n\tif (IS_ERR(vma))\n\t\tgoto err;\n\n\tflags = PIN_OFFSET_BIAS | i915_ggtt_pin_bias(vma);\n\tret = i915_ggtt_pin(vma, NULL, 0, flags);\n\tif (ret) {\n\t\tvma = ERR_PTR(ret);\n\t\tgoto err;\n\t}\n\n\treturn i915_vma_make_unshrinkable(vma);\n\nerr:\n\ti915_gem_object_put(obj);\n\treturn vma;\n}\n\n \nint intel_guc_allocate_and_map_vma(struct intel_guc *guc, u32 size,\n\t\t\t\t   struct i915_vma **out_vma, void **out_vaddr)\n{\n\tstruct i915_vma *vma;\n\tvoid *vaddr;\n\n\tvma = intel_guc_allocate_vma(guc, size);\n\tif (IS_ERR(vma))\n\t\treturn PTR_ERR(vma);\n\n\tvaddr = i915_gem_object_pin_map_unlocked(vma->obj,\n\t\t\t\t\t\t intel_gt_coherent_map_type(guc_to_gt(guc),\n\t\t\t\t\t\t\t\t\t    vma->obj, true));\n\tif (IS_ERR(vaddr)) {\n\t\ti915_vma_unpin_and_release(&vma, 0);\n\t\treturn PTR_ERR(vaddr);\n\t}\n\n\t*out_vma = vma;\n\t*out_vaddr = vaddr;\n\n\treturn 0;\n}\n\nstatic int __guc_action_self_cfg(struct intel_guc *guc, u16 key, u16 len, u64 value)\n{\n\tu32 request[HOST2GUC_SELF_CFG_REQUEST_MSG_LEN] = {\n\t\tFIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |\n\t\tFIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |\n\t\tFIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_HOST2GUC_SELF_CFG),\n\t\tFIELD_PREP(HOST2GUC_SELF_CFG_REQUEST_MSG_1_KLV_KEY, key) |\n\t\tFIELD_PREP(HOST2GUC_SELF_CFG_REQUEST_MSG_1_KLV_LEN, len),\n\t\tFIELD_PREP(HOST2GUC_SELF_CFG_REQUEST_MSG_2_VALUE32, lower_32_bits(value)),\n\t\tFIELD_PREP(HOST2GUC_SELF_CFG_REQUEST_MSG_3_VALUE64, upper_32_bits(value)),\n\t};\n\tint ret;\n\n\tGEM_BUG_ON(len > 2);\n\tGEM_BUG_ON(len == 1 && upper_32_bits(value));\n\n\t \n\tret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request), NULL, 0);\n\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\tif (unlikely(ret > 1))\n\t\treturn -EPROTO;\n\tif (unlikely(!ret))\n\t\treturn -ENOKEY;\n\n\treturn 0;\n}\n\nstatic int __guc_self_cfg(struct intel_guc *guc, u16 key, u16 len, u64 value)\n{\n\tint err = __guc_action_self_cfg(guc, key, len, value);\n\n\tif (unlikely(err))\n\t\tguc_probe_error(guc, \"Unsuccessful self-config (%pe) key %#hx value %#llx\\n\",\n\t\t\t\tERR_PTR(err), key, value);\n\treturn err;\n}\n\nint intel_guc_self_cfg32(struct intel_guc *guc, u16 key, u32 value)\n{\n\treturn __guc_self_cfg(guc, key, 1, value);\n}\n\nint intel_guc_self_cfg64(struct intel_guc *guc, u16 key, u64 value)\n{\n\treturn __guc_self_cfg(guc, key, 2, value);\n}\n\n \nvoid intel_guc_load_status(struct intel_guc *guc, struct drm_printer *p)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct intel_uncore *uncore = gt->uncore;\n\tintel_wakeref_t wakeref;\n\n\tif (!intel_guc_is_supported(guc)) {\n\t\tdrm_printf(p, \"GuC not supported\\n\");\n\t\treturn;\n\t}\n\n\tif (!intel_guc_is_wanted(guc)) {\n\t\tdrm_printf(p, \"GuC disabled\\n\");\n\t\treturn;\n\t}\n\n\tintel_uc_fw_dump(&guc->fw, p);\n\n\twith_intel_runtime_pm(uncore->rpm, wakeref) {\n\t\tu32 status = intel_uncore_read(uncore, GUC_STATUS);\n\t\tu32 i;\n\n\t\tdrm_printf(p, \"GuC status 0x%08x:\\n\", status);\n\t\tdrm_printf(p, \"\\tBootrom status = 0x%x\\n\",\n\t\t\t   (status & GS_BOOTROM_MASK) >> GS_BOOTROM_SHIFT);\n\t\tdrm_printf(p, \"\\tuKernel status = 0x%x\\n\",\n\t\t\t   (status & GS_UKERNEL_MASK) >> GS_UKERNEL_SHIFT);\n\t\tdrm_printf(p, \"\\tMIA Core status = 0x%x\\n\",\n\t\t\t   (status & GS_MIA_MASK) >> GS_MIA_SHIFT);\n\t\tdrm_puts(p, \"Scratch registers:\\n\");\n\t\tfor (i = 0; i < 16; i++) {\n\t\t\tdrm_printf(p, \"\\t%2d: \\t0x%x\\n\",\n\t\t\t\t   i, intel_uncore_read(uncore, SOFT_SCRATCH(i)));\n\t\t}\n\t}\n}\n\nvoid intel_guc_write_barrier(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\n\tif (i915_gem_object_is_lmem(guc->ct.vma->obj)) {\n\t\t \n\t\tGEM_BUG_ON(guc->send_regs.fw_domains);\n\n\t\t \n\t\tintel_uncore_write_fw(gt->uncore, GEN11_SOFT_SCRATCH(0), 0);\n\t} else {\n\t\t \n\t\twmb();\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}