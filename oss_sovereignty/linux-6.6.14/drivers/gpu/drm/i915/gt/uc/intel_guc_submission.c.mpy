{
  "module_name": "intel_guc_submission.c",
  "hash_id": "0c99d3cba175d0c460c6dae68a5a1cb4a1a6d869eee176c452dcce582e242ef4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c",
  "human_readable_source": "\n \n\n#include <linux/circ_buf.h>\n\n#include \"gem/i915_gem_context.h\"\n#include \"gem/i915_gem_lmem.h\"\n#include \"gt/gen8_engine_cs.h\"\n#include \"gt/intel_breadcrumbs.h\"\n#include \"gt/intel_context.h\"\n#include \"gt/intel_engine_heartbeat.h\"\n#include \"gt/intel_engine_pm.h\"\n#include \"gt/intel_engine_regs.h\"\n#include \"gt/intel_gpu_commands.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_clock_utils.h\"\n#include \"gt/intel_gt_irq.h\"\n#include \"gt/intel_gt_pm.h\"\n#include \"gt/intel_gt_regs.h\"\n#include \"gt/intel_gt_requests.h\"\n#include \"gt/intel_lrc.h\"\n#include \"gt/intel_lrc_reg.h\"\n#include \"gt/intel_mocs.h\"\n#include \"gt/intel_ring.h\"\n\n#include \"intel_guc_ads.h\"\n#include \"intel_guc_capture.h\"\n#include \"intel_guc_print.h\"\n#include \"intel_guc_submission.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_reg.h\"\n#include \"i915_trace.h\"\n\n \n\n \nstruct guc_virtual_engine {\n\tstruct intel_engine_cs base;\n\tstruct intel_context context;\n};\n\nstatic struct intel_context *\nguc_create_virtual(struct intel_engine_cs **siblings, unsigned int count,\n\t\t   unsigned long flags);\n\nstatic struct intel_context *\nguc_create_parallel(struct intel_engine_cs **engines,\n\t\t    unsigned int num_siblings,\n\t\t    unsigned int width);\n\n#define GUC_REQUEST_SIZE 64  \n\n \n#define NUMBER_MULTI_LRC_GUC_ID(guc)\t\\\n\t((guc)->submission_state.num_guc_ids / 16)\n\n \n#define SCHED_STATE_WAIT_FOR_DEREGISTER_TO_REGISTER\tBIT(0)\n#define SCHED_STATE_DESTROYED\t\t\t\tBIT(1)\n#define SCHED_STATE_PENDING_DISABLE\t\t\tBIT(2)\n#define SCHED_STATE_BANNED\t\t\t\tBIT(3)\n#define SCHED_STATE_ENABLED\t\t\t\tBIT(4)\n#define SCHED_STATE_PENDING_ENABLE\t\t\tBIT(5)\n#define SCHED_STATE_REGISTERED\t\t\t\tBIT(6)\n#define SCHED_STATE_POLICY_REQUIRED\t\t\tBIT(7)\n#define SCHED_STATE_CLOSED\t\t\t\tBIT(8)\n#define SCHED_STATE_BLOCKED_SHIFT\t\t\t9\n#define SCHED_STATE_BLOCKED\t\tBIT(SCHED_STATE_BLOCKED_SHIFT)\n#define SCHED_STATE_BLOCKED_MASK\t(0xfff << SCHED_STATE_BLOCKED_SHIFT)\n\nstatic inline void init_sched_state(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= SCHED_STATE_BLOCKED_MASK;\n}\n\n \n#define SCHED_STATE_VALID_INIT \\\n\t(SCHED_STATE_BLOCKED_MASK | \\\n\t SCHED_STATE_CLOSED | \\\n\t SCHED_STATE_REGISTERED)\n\n__maybe_unused\nstatic bool sched_state_is_init(struct intel_context *ce)\n{\n\treturn !(ce->guc_state.sched_state & ~SCHED_STATE_VALID_INIT);\n}\n\nstatic inline bool\ncontext_wait_for_deregister_to_register(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state &\n\t\tSCHED_STATE_WAIT_FOR_DEREGISTER_TO_REGISTER;\n}\n\nstatic inline void\nset_context_wait_for_deregister_to_register(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |=\n\t\tSCHED_STATE_WAIT_FOR_DEREGISTER_TO_REGISTER;\n}\n\nstatic inline void\nclr_context_wait_for_deregister_to_register(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &=\n\t\t~SCHED_STATE_WAIT_FOR_DEREGISTER_TO_REGISTER;\n}\n\nstatic inline bool\ncontext_destroyed(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_DESTROYED;\n}\n\nstatic inline void\nset_context_destroyed(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_DESTROYED;\n}\n\nstatic inline bool context_pending_disable(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_PENDING_DISABLE;\n}\n\nstatic inline void set_context_pending_disable(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_PENDING_DISABLE;\n}\n\nstatic inline void clr_context_pending_disable(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_PENDING_DISABLE;\n}\n\nstatic inline bool context_banned(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_BANNED;\n}\n\nstatic inline void set_context_banned(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_BANNED;\n}\n\nstatic inline void clr_context_banned(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_BANNED;\n}\n\nstatic inline bool context_enabled(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_ENABLED;\n}\n\nstatic inline void set_context_enabled(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_ENABLED;\n}\n\nstatic inline void clr_context_enabled(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_ENABLED;\n}\n\nstatic inline bool context_pending_enable(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_PENDING_ENABLE;\n}\n\nstatic inline void set_context_pending_enable(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_PENDING_ENABLE;\n}\n\nstatic inline void clr_context_pending_enable(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_PENDING_ENABLE;\n}\n\nstatic inline bool context_registered(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_REGISTERED;\n}\n\nstatic inline void set_context_registered(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_REGISTERED;\n}\n\nstatic inline void clr_context_registered(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_REGISTERED;\n}\n\nstatic inline bool context_policy_required(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_POLICY_REQUIRED;\n}\n\nstatic inline void set_context_policy_required(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_POLICY_REQUIRED;\n}\n\nstatic inline void clr_context_policy_required(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state &= ~SCHED_STATE_POLICY_REQUIRED;\n}\n\nstatic inline bool context_close_done(struct intel_context *ce)\n{\n\treturn ce->guc_state.sched_state & SCHED_STATE_CLOSED;\n}\n\nstatic inline void set_context_close_done(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tce->guc_state.sched_state |= SCHED_STATE_CLOSED;\n}\n\nstatic inline u32 context_blocked(struct intel_context *ce)\n{\n\treturn (ce->guc_state.sched_state & SCHED_STATE_BLOCKED_MASK) >>\n\t\tSCHED_STATE_BLOCKED_SHIFT;\n}\n\nstatic inline void incr_context_blocked(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tce->guc_state.sched_state += SCHED_STATE_BLOCKED;\n\n\tGEM_BUG_ON(!context_blocked(ce));\t \n}\n\nstatic inline void decr_context_blocked(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tGEM_BUG_ON(!context_blocked(ce));\t \n\n\tce->guc_state.sched_state -= SCHED_STATE_BLOCKED;\n}\n\nstatic struct intel_context *\nrequest_to_scheduling_context(struct i915_request *rq)\n{\n\treturn intel_context_to_parent(rq->context);\n}\n\nstatic inline bool context_guc_id_invalid(struct intel_context *ce)\n{\n\treturn ce->guc_id.id == GUC_INVALID_CONTEXT_ID;\n}\n\nstatic inline void set_context_guc_id_invalid(struct intel_context *ce)\n{\n\tce->guc_id.id = GUC_INVALID_CONTEXT_ID;\n}\n\nstatic inline struct intel_guc *ce_to_guc(struct intel_context *ce)\n{\n\treturn &ce->engine->gt->uc.guc;\n}\n\nstatic inline struct i915_priolist *to_priolist(struct rb_node *rb)\n{\n\treturn rb_entry(rb, struct i915_priolist, node);\n}\n\n \n#define WQ_SIZE\t\t\t(PARENT_SCRATCH_SIZE / 2)\n#define WQ_OFFSET\t\t(PARENT_SCRATCH_SIZE - WQ_SIZE)\n\nstruct sync_semaphore {\n\tu32 semaphore;\n\tu8 unused[CACHELINE_BYTES - sizeof(u32)];\n};\n\nstruct parent_scratch {\n\tunion guc_descs {\n\t\tstruct guc_sched_wq_desc wq_desc;\n\t\tstruct guc_process_desc_v69 pdesc;\n\t} descs;\n\n\tstruct sync_semaphore go;\n\tstruct sync_semaphore join[MAX_ENGINE_INSTANCE + 1];\n\n\tu8 unused[WQ_OFFSET - sizeof(union guc_descs) -\n\t\tsizeof(struct sync_semaphore) * (MAX_ENGINE_INSTANCE + 2)];\n\n\tu32 wq[WQ_SIZE / sizeof(u32)];\n};\n\nstatic u32 __get_parent_scratch_offset(struct intel_context *ce)\n{\n\tGEM_BUG_ON(!ce->parallel.guc.parent_page);\n\n\treturn ce->parallel.guc.parent_page * PAGE_SIZE;\n}\n\nstatic u32 __get_wq_offset(struct intel_context *ce)\n{\n\tBUILD_BUG_ON(offsetof(struct parent_scratch, wq) != WQ_OFFSET);\n\n\treturn __get_parent_scratch_offset(ce) + WQ_OFFSET;\n}\n\nstatic struct parent_scratch *\n__get_parent_scratch(struct intel_context *ce)\n{\n\tBUILD_BUG_ON(sizeof(struct parent_scratch) != PARENT_SCRATCH_SIZE);\n\tBUILD_BUG_ON(sizeof(struct sync_semaphore) != CACHELINE_BYTES);\n\n\t \n\treturn (struct parent_scratch *)\n\t\t(ce->lrc_reg_state +\n\t\t ((__get_parent_scratch_offset(ce) -\n\t\t   LRC_STATE_OFFSET) / sizeof(u32)));\n}\n\nstatic struct guc_process_desc_v69 *\n__get_process_desc_v69(struct intel_context *ce)\n{\n\tstruct parent_scratch *ps = __get_parent_scratch(ce);\n\n\treturn &ps->descs.pdesc;\n}\n\nstatic struct guc_sched_wq_desc *\n__get_wq_desc_v70(struct intel_context *ce)\n{\n\tstruct parent_scratch *ps = __get_parent_scratch(ce);\n\n\treturn &ps->descs.wq_desc;\n}\n\nstatic u32 *get_wq_pointer(struct intel_context *ce, u32 wqi_size)\n{\n\t \n#define AVAILABLE_SPACE\t\\\n\tCIRC_SPACE(ce->parallel.guc.wqi_tail, ce->parallel.guc.wqi_head, WQ_SIZE)\n\tif (wqi_size > AVAILABLE_SPACE) {\n\t\tce->parallel.guc.wqi_head = READ_ONCE(*ce->parallel.guc.wq_head);\n\n\t\tif (wqi_size > AVAILABLE_SPACE)\n\t\t\treturn NULL;\n\t}\n#undef AVAILABLE_SPACE\n\n\treturn &__get_parent_scratch(ce)->wq[ce->parallel.guc.wqi_tail / sizeof(u32)];\n}\n\nstatic inline struct intel_context *__get_context(struct intel_guc *guc, u32 id)\n{\n\tstruct intel_context *ce = xa_load(&guc->context_lookup, id);\n\n\tGEM_BUG_ON(id >= GUC_MAX_CONTEXT_ID);\n\n\treturn ce;\n}\n\nstatic struct guc_lrc_desc_v69 *__get_lrc_desc_v69(struct intel_guc *guc, u32 index)\n{\n\tstruct guc_lrc_desc_v69 *base = guc->lrc_desc_pool_vaddr_v69;\n\n\tif (!base)\n\t\treturn NULL;\n\n\tGEM_BUG_ON(index >= GUC_MAX_CONTEXT_ID);\n\n\treturn &base[index];\n}\n\nstatic int guc_lrc_desc_pool_create_v69(struct intel_guc *guc)\n{\n\tu32 size;\n\tint ret;\n\n\tsize = PAGE_ALIGN(sizeof(struct guc_lrc_desc_v69) *\n\t\t\t  GUC_MAX_CONTEXT_ID);\n\tret = intel_guc_allocate_and_map_vma(guc, size, &guc->lrc_desc_pool_v69,\n\t\t\t\t\t     (void **)&guc->lrc_desc_pool_vaddr_v69);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic void guc_lrc_desc_pool_destroy_v69(struct intel_guc *guc)\n{\n\tif (!guc->lrc_desc_pool_vaddr_v69)\n\t\treturn;\n\n\tguc->lrc_desc_pool_vaddr_v69 = NULL;\n\ti915_vma_unpin_and_release(&guc->lrc_desc_pool_v69, I915_VMA_RELEASE_MAP);\n}\n\nstatic inline bool guc_submission_initialized(struct intel_guc *guc)\n{\n\treturn guc->submission_initialized;\n}\n\nstatic inline void _reset_lrc_desc_v69(struct intel_guc *guc, u32 id)\n{\n\tstruct guc_lrc_desc_v69 *desc = __get_lrc_desc_v69(guc, id);\n\n\tif (desc)\n\t\tmemset(desc, 0, sizeof(*desc));\n}\n\nstatic inline bool ctx_id_mapped(struct intel_guc *guc, u32 id)\n{\n\treturn __get_context(guc, id);\n}\n\nstatic inline void set_ctx_id_mapping(struct intel_guc *guc, u32 id,\n\t\t\t\t      struct intel_context *ce)\n{\n\tunsigned long flags;\n\n\t \n\txa_lock_irqsave(&guc->context_lookup, flags);\n\t__xa_store(&guc->context_lookup, id, ce, GFP_ATOMIC);\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\nstatic inline void clr_ctx_id_mapping(struct intel_guc *guc, u32 id)\n{\n\tunsigned long flags;\n\n\tif (unlikely(!guc_submission_initialized(guc)))\n\t\treturn;\n\n\t_reset_lrc_desc_v69(guc, id);\n\n\t \n\txa_lock_irqsave(&guc->context_lookup, flags);\n\t__xa_erase(&guc->context_lookup, id);\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\nstatic void decr_outstanding_submission_g2h(struct intel_guc *guc)\n{\n\tif (atomic_dec_and_test(&guc->outstanding_submission_g2h))\n\t\twake_up_all(&guc->ct.wq);\n}\n\nstatic int guc_submission_send_busy_loop(struct intel_guc *guc,\n\t\t\t\t\t const u32 *action,\n\t\t\t\t\t u32 len,\n\t\t\t\t\t u32 g2h_len_dw,\n\t\t\t\t\t bool loop)\n{\n\t \n\tGEM_BUG_ON(g2h_len_dw && !loop);\n\n\tif (g2h_len_dw)\n\t\tatomic_inc(&guc->outstanding_submission_g2h);\n\n\treturn intel_guc_send_busy_loop(guc, action, len, g2h_len_dw, loop);\n}\n\nint intel_guc_wait_for_pending_msg(struct intel_guc *guc,\n\t\t\t\t   atomic_t *wait_var,\n\t\t\t\t   bool interruptible,\n\t\t\t\t   long timeout)\n{\n\tconst int state = interruptible ?\n\t\tTASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE;\n\tDEFINE_WAIT(wait);\n\n\tmight_sleep();\n\tGEM_BUG_ON(timeout < 0);\n\n\tif (!atomic_read(wait_var))\n\t\treturn 0;\n\n\tif (!timeout)\n\t\treturn -ETIME;\n\n\tfor (;;) {\n\t\tprepare_to_wait(&guc->ct.wq, &wait, state);\n\n\t\tif (!atomic_read(wait_var))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(state, current)) {\n\t\t\ttimeout = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!timeout) {\n\t\t\ttimeout = -ETIME;\n\t\t\tbreak;\n\t\t}\n\n\t\ttimeout = io_schedule_timeout(timeout);\n\t}\n\tfinish_wait(&guc->ct.wq, &wait);\n\n\treturn (timeout < 0) ? timeout : 0;\n}\n\nint intel_guc_wait_for_idle(struct intel_guc *guc, long timeout)\n{\n\tif (!intel_uc_uses_guc_submission(&guc_to_gt(guc)->uc))\n\t\treturn 0;\n\n\treturn intel_guc_wait_for_pending_msg(guc,\n\t\t\t\t\t      &guc->outstanding_submission_g2h,\n\t\t\t\t\t      true, timeout);\n}\n\nstatic int guc_context_policy_init_v70(struct intel_context *ce, bool loop);\nstatic int try_context_registration(struct intel_context *ce, bool loop);\n\nstatic int __guc_add_request(struct intel_guc *guc, struct i915_request *rq)\n{\n\tint err = 0;\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tu32 action[3];\n\tint len = 0;\n\tu32 g2h_len_dw = 0;\n\tbool enabled;\n\n\tlockdep_assert_held(&rq->engine->sched_engine->lock);\n\n\t \n\tif (unlikely(!intel_context_is_schedulable(ce))) {\n\t\ti915_request_put(i915_request_mark_eio(rq));\n\t\tintel_engine_signal_breadcrumbs(ce->engine);\n\t\treturn 0;\n\t}\n\n\tGEM_BUG_ON(!atomic_read(&ce->guc_id.ref));\n\tGEM_BUG_ON(context_guc_id_invalid(ce));\n\n\tif (context_policy_required(ce)) {\n\t\terr = guc_context_policy_init_v70(ce, false);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tspin_lock(&ce->guc_state.lock);\n\n\t \n\tif (unlikely(context_blocked(ce) && !intel_context_is_parent(ce)))\n\t\tgoto out;\n\n\tenabled = context_enabled(ce) || context_blocked(ce);\n\n\tif (!enabled) {\n\t\taction[len++] = INTEL_GUC_ACTION_SCHED_CONTEXT_MODE_SET;\n\t\taction[len++] = ce->guc_id.id;\n\t\taction[len++] = GUC_CONTEXT_ENABLE;\n\t\tset_context_pending_enable(ce);\n\t\tintel_context_get(ce);\n\t\tg2h_len_dw = G2H_LEN_DW_SCHED_CONTEXT_MODE_SET;\n\t} else {\n\t\taction[len++] = INTEL_GUC_ACTION_SCHED_CONTEXT;\n\t\taction[len++] = ce->guc_id.id;\n\t}\n\n\terr = intel_guc_send_nb(guc, action, len, g2h_len_dw);\n\tif (!enabled && !err) {\n\t\ttrace_intel_context_sched_enable(ce);\n\t\tatomic_inc(&guc->outstanding_submission_g2h);\n\t\tset_context_enabled(ce);\n\n\t\t \n\t\tif (intel_context_is_parent(ce)) {\n\t\t\taction[0] = INTEL_GUC_ACTION_SCHED_CONTEXT;\n\t\t\terr = intel_guc_send_nb(guc, action, len - 1, 0);\n\t\t}\n\t} else if (!enabled) {\n\t\tclr_context_pending_enable(ce);\n\t\tintel_context_put(ce);\n\t}\n\tif (likely(!err))\n\t\ttrace_i915_request_guc_submit(rq);\n\nout:\n\tspin_unlock(&ce->guc_state.lock);\n\treturn err;\n}\n\nstatic int guc_add_request(struct intel_guc *guc, struct i915_request *rq)\n{\n\tint ret = __guc_add_request(guc, rq);\n\n\tif (unlikely(ret == -EBUSY)) {\n\t\tguc->stalled_request = rq;\n\t\tguc->submission_stall_reason = STALL_ADD_REQUEST;\n\t}\n\n\treturn ret;\n}\n\nstatic inline void guc_set_lrc_tail(struct i915_request *rq)\n{\n\trq->context->lrc_reg_state[CTX_RING_TAIL] =\n\t\tintel_ring_set_tail(rq->ring, rq->tail);\n}\n\nstatic inline int rq_prio(const struct i915_request *rq)\n{\n\treturn rq->sched.attr.priority;\n}\n\nstatic bool is_multi_lrc_rq(struct i915_request *rq)\n{\n\treturn intel_context_is_parallel(rq->context);\n}\n\nstatic bool can_merge_rq(struct i915_request *rq,\n\t\t\t struct i915_request *last)\n{\n\treturn request_to_scheduling_context(rq) ==\n\t\trequest_to_scheduling_context(last);\n}\n\nstatic u32 wq_space_until_wrap(struct intel_context *ce)\n{\n\treturn (WQ_SIZE - ce->parallel.guc.wqi_tail);\n}\n\nstatic void write_wqi(struct intel_context *ce, u32 wqi_size)\n{\n\tBUILD_BUG_ON(!is_power_of_2(WQ_SIZE));\n\n\t \n\tintel_guc_write_barrier(ce_to_guc(ce));\n\n\tce->parallel.guc.wqi_tail = (ce->parallel.guc.wqi_tail + wqi_size) &\n\t\t(WQ_SIZE - 1);\n\tWRITE_ONCE(*ce->parallel.guc.wq_tail, ce->parallel.guc.wqi_tail);\n}\n\nstatic int guc_wq_noop_append(struct intel_context *ce)\n{\n\tu32 *wqi = get_wq_pointer(ce, wq_space_until_wrap(ce));\n\tu32 len_dw = wq_space_until_wrap(ce) / sizeof(u32) - 1;\n\n\tif (!wqi)\n\t\treturn -EBUSY;\n\n\tGEM_BUG_ON(!FIELD_FIT(WQ_LEN_MASK, len_dw));\n\n\t*wqi = FIELD_PREP(WQ_TYPE_MASK, WQ_TYPE_NOOP) |\n\t\tFIELD_PREP(WQ_LEN_MASK, len_dw);\n\tce->parallel.guc.wqi_tail = 0;\n\n\treturn 0;\n}\n\nstatic int __guc_wq_item_append(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tstruct intel_context *child;\n\tunsigned int wqi_size = (ce->parallel.number_children + 4) *\n\t\tsizeof(u32);\n\tu32 *wqi;\n\tu32 len_dw = (wqi_size / sizeof(u32)) - 1;\n\tint ret;\n\n\t \n\tGEM_BUG_ON(!atomic_read(&ce->guc_id.ref));\n\tGEM_BUG_ON(context_guc_id_invalid(ce));\n\tGEM_BUG_ON(context_wait_for_deregister_to_register(ce));\n\tGEM_BUG_ON(!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id));\n\n\t \n\tif (wqi_size > wq_space_until_wrap(ce)) {\n\t\tret = guc_wq_noop_append(ce);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\twqi = get_wq_pointer(ce, wqi_size);\n\tif (!wqi)\n\t\treturn -EBUSY;\n\n\tGEM_BUG_ON(!FIELD_FIT(WQ_LEN_MASK, len_dw));\n\n\t*wqi++ = FIELD_PREP(WQ_TYPE_MASK, WQ_TYPE_MULTI_LRC) |\n\t\tFIELD_PREP(WQ_LEN_MASK, len_dw);\n\t*wqi++ = ce->lrc.lrca;\n\t*wqi++ = FIELD_PREP(WQ_GUC_ID_MASK, ce->guc_id.id) |\n\t       FIELD_PREP(WQ_RING_TAIL_MASK, ce->ring->tail / sizeof(u64));\n\t*wqi++ = 0;\t \n\tfor_each_child(ce, child)\n\t\t*wqi++ = child->ring->tail / sizeof(u64);\n\n\twrite_wqi(ce, wqi_size);\n\n\treturn 0;\n}\n\nstatic int guc_wq_item_append(struct intel_guc *guc,\n\t\t\t      struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tint ret;\n\n\tif (unlikely(!intel_context_is_schedulable(ce)))\n\t\treturn 0;\n\n\tret = __guc_wq_item_append(rq);\n\tif (unlikely(ret == -EBUSY)) {\n\t\tguc->stalled_request = rq;\n\t\tguc->submission_stall_reason = STALL_MOVE_LRC_TAIL;\n\t}\n\n\treturn ret;\n}\n\nstatic bool multi_lrc_submit(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\n\tintel_ring_set_tail(rq->ring, rq->tail);\n\n\t \n\treturn test_bit(I915_FENCE_FLAG_SUBMIT_PARALLEL, &rq->fence.flags) ||\n\t       !intel_context_is_schedulable(ce);\n}\n\nstatic int guc_dequeue_one_context(struct intel_guc *guc)\n{\n\tstruct i915_sched_engine * const sched_engine = guc->sched_engine;\n\tstruct i915_request *last = NULL;\n\tbool submit = false;\n\tstruct rb_node *rb;\n\tint ret;\n\n\tlockdep_assert_held(&sched_engine->lock);\n\n\tif (guc->stalled_request) {\n\t\tsubmit = true;\n\t\tlast = guc->stalled_request;\n\n\t\tswitch (guc->submission_stall_reason) {\n\t\tcase STALL_REGISTER_CONTEXT:\n\t\t\tgoto register_context;\n\t\tcase STALL_MOVE_LRC_TAIL:\n\t\t\tgoto move_lrc_tail;\n\t\tcase STALL_ADD_REQUEST:\n\t\t\tgoto add_request;\n\t\tdefault:\n\t\t\tMISSING_CASE(guc->submission_stall_reason);\n\t\t}\n\t}\n\n\twhile ((rb = rb_first_cached(&sched_engine->queue))) {\n\t\tstruct i915_priolist *p = to_priolist(rb);\n\t\tstruct i915_request *rq, *rn;\n\n\t\tpriolist_for_each_request_consume(rq, rn, p) {\n\t\t\tif (last && !can_merge_rq(rq, last))\n\t\t\t\tgoto register_context;\n\n\t\t\tlist_del_init(&rq->sched.link);\n\n\t\t\t__i915_request_submit(rq);\n\n\t\t\ttrace_i915_request_in(rq, 0);\n\t\t\tlast = rq;\n\n\t\t\tif (is_multi_lrc_rq(rq)) {\n\t\t\t\t \n\t\t\t\tif (multi_lrc_submit(rq)) {\n\t\t\t\t\tsubmit = true;\n\t\t\t\t\tgoto register_context;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tsubmit = true;\n\t\t\t}\n\t\t}\n\n\t\trb_erase_cached(&p->node, &sched_engine->queue);\n\t\ti915_priolist_free(p);\n\t}\n\nregister_context:\n\tif (submit) {\n\t\tstruct intel_context *ce = request_to_scheduling_context(last);\n\n\t\tif (unlikely(!ctx_id_mapped(guc, ce->guc_id.id) &&\n\t\t\t     intel_context_is_schedulable(ce))) {\n\t\t\tret = try_context_registration(ce, false);\n\t\t\tif (unlikely(ret == -EPIPE)) {\n\t\t\t\tgoto deadlk;\n\t\t\t} else if (ret == -EBUSY) {\n\t\t\t\tguc->stalled_request = last;\n\t\t\t\tguc->submission_stall_reason =\n\t\t\t\t\tSTALL_REGISTER_CONTEXT;\n\t\t\t\tgoto schedule_tasklet;\n\t\t\t} else if (ret != 0) {\n\t\t\t\tGEM_WARN_ON(ret);\t \n\t\t\t\tgoto deadlk;\n\t\t\t}\n\t\t}\n\nmove_lrc_tail:\n\t\tif (is_multi_lrc_rq(last)) {\n\t\t\tret = guc_wq_item_append(guc, last);\n\t\t\tif (ret == -EBUSY) {\n\t\t\t\tgoto schedule_tasklet;\n\t\t\t} else if (ret != 0) {\n\t\t\t\tGEM_WARN_ON(ret);\t \n\t\t\t\tgoto deadlk;\n\t\t\t}\n\t\t} else {\n\t\t\tguc_set_lrc_tail(last);\n\t\t}\n\nadd_request:\n\t\tret = guc_add_request(guc, last);\n\t\tif (unlikely(ret == -EPIPE)) {\n\t\t\tgoto deadlk;\n\t\t} else if (ret == -EBUSY) {\n\t\t\tgoto schedule_tasklet;\n\t\t} else if (ret != 0) {\n\t\t\tGEM_WARN_ON(ret);\t \n\t\t\tgoto deadlk;\n\t\t}\n\t}\n\n\tguc->stalled_request = NULL;\n\tguc->submission_stall_reason = STALL_NONE;\n\treturn submit;\n\ndeadlk:\n\tsched_engine->tasklet.callback = NULL;\n\ttasklet_disable_nosync(&sched_engine->tasklet);\n\treturn false;\n\nschedule_tasklet:\n\ttasklet_schedule(&sched_engine->tasklet);\n\treturn false;\n}\n\nstatic void guc_submission_tasklet(struct tasklet_struct *t)\n{\n\tstruct i915_sched_engine *sched_engine =\n\t\tfrom_tasklet(sched_engine, t, tasklet);\n\tunsigned long flags;\n\tbool loop;\n\n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\n\tdo {\n\t\tloop = guc_dequeue_one_context(sched_engine->private_data);\n\t} while (loop);\n\n\ti915_sched_engine_reset_on_empty(sched_engine);\n\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nstatic void cs_irq_handler(struct intel_engine_cs *engine, u16 iir)\n{\n\tif (iir & GT_RENDER_USER_INTERRUPT)\n\t\tintel_engine_signal_breadcrumbs(engine);\n}\n\nstatic void __guc_context_destroy(struct intel_context *ce);\nstatic void release_guc_id(struct intel_guc *guc, struct intel_context *ce);\nstatic void guc_signal_context_fence(struct intel_context *ce);\nstatic void guc_cancel_context_requests(struct intel_context *ce);\nstatic void guc_blocked_fence_complete(struct intel_context *ce);\n\nstatic void scrub_guc_desc_for_outstanding_g2h(struct intel_guc *guc)\n{\n\tstruct intel_context *ce;\n\tunsigned long index, flags;\n\tbool pending_disable, pending_enable, deregister, destroyed, banned;\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\t \n\t\tbool do_put = kref_get_unless_zero(&ce->ref);\n\n\t\txa_unlock(&guc->context_lookup);\n\n\t\tif (test_bit(CONTEXT_GUC_INIT, &ce->flags) &&\n\t\t    (cancel_delayed_work(&ce->guc_state.sched_disable_delay_work))) {\n\t\t\t \n\t\t\tintel_context_sched_disable_unpin(ce);\n\t\t}\n\n\t\tspin_lock(&ce->guc_state.lock);\n\n\t\t \n\n\t\tdestroyed = context_destroyed(ce);\n\t\tpending_enable = context_pending_enable(ce);\n\t\tpending_disable = context_pending_disable(ce);\n\t\tderegister = context_wait_for_deregister_to_register(ce);\n\t\tbanned = context_banned(ce);\n\t\tinit_sched_state(ce);\n\n\t\tspin_unlock(&ce->guc_state.lock);\n\n\t\tif (pending_enable || destroyed || deregister) {\n\t\t\tdecr_outstanding_submission_g2h(guc);\n\t\t\tif (deregister)\n\t\t\t\tguc_signal_context_fence(ce);\n\t\t\tif (destroyed) {\n\t\t\t\tintel_gt_pm_put_async(guc_to_gt(guc));\n\t\t\t\trelease_guc_id(guc, ce);\n\t\t\t\t__guc_context_destroy(ce);\n\t\t\t}\n\t\t\tif (pending_enable || deregister)\n\t\t\t\tintel_context_put(ce);\n\t\t}\n\n\t\t \n\t\tif (pending_disable) {\n\t\t\tguc_signal_context_fence(ce);\n\t\t\tif (banned) {\n\t\t\t\tguc_cancel_context_requests(ce);\n\t\t\t\tintel_engine_signal_breadcrumbs(ce->engine);\n\t\t\t}\n\t\t\tintel_context_sched_disable_unpin(ce);\n\t\t\tdecr_outstanding_submission_g2h(guc);\n\n\t\t\tspin_lock(&ce->guc_state.lock);\n\t\t\tguc_blocked_fence_complete(ce);\n\t\t\tspin_unlock(&ce->guc_state.lock);\n\n\t\t\tintel_context_put(ce);\n\t\t}\n\n\t\tif (do_put)\n\t\t\tintel_context_put(ce);\n\t\txa_lock(&guc->context_lookup);\n\t}\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\n \n\n#define WRAP_TIME_CLKS U32_MAX\n#define POLL_TIME_CLKS (WRAP_TIME_CLKS >> 3)\n\nstatic void\n__extend_last_switch(struct intel_guc *guc, u64 *prev_start, u32 new_start)\n{\n\tu32 gt_stamp_hi = upper_32_bits(guc->timestamp.gt_stamp);\n\tu32 gt_stamp_last = lower_32_bits(guc->timestamp.gt_stamp);\n\n\tif (new_start == lower_32_bits(*prev_start))\n\t\treturn;\n\n\t \n\tif (new_start < gt_stamp_last &&\n\t    (new_start - gt_stamp_last) <= POLL_TIME_CLKS)\n\t\tgt_stamp_hi++;\n\n\tif (new_start > gt_stamp_last &&\n\t    (gt_stamp_last - new_start) <= POLL_TIME_CLKS && gt_stamp_hi)\n\t\tgt_stamp_hi--;\n\n\t*prev_start = ((u64)gt_stamp_hi << 32) | new_start;\n}\n\n#define record_read(map_, field_) \\\n\tiosys_map_rd_field(map_, 0, struct guc_engine_usage_record, field_)\n\n \nstatic void __get_engine_usage_record(struct intel_engine_cs *engine,\n\t\t\t\t      u32 *last_in, u32 *id, u32 *total)\n{\n\tstruct iosys_map rec_map = intel_guc_engine_usage_record_map(engine);\n\tint i = 0;\n\n\tdo {\n\t\t*last_in = record_read(&rec_map, last_switch_in_stamp);\n\t\t*id = record_read(&rec_map, current_context_index);\n\t\t*total = record_read(&rec_map, total_runtime);\n\n\t\tif (record_read(&rec_map, last_switch_in_stamp) == *last_in &&\n\t\t    record_read(&rec_map, current_context_index) == *id &&\n\t\t    record_read(&rec_map, total_runtime) == *total)\n\t\t\tbreak;\n\t} while (++i < 6);\n}\n\nstatic void guc_update_engine_gt_clks(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_guc_stats *stats = &engine->stats.guc;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tu32 last_switch, ctx_id, total;\n\n\tlockdep_assert_held(&guc->timestamp.lock);\n\n\t__get_engine_usage_record(engine, &last_switch, &ctx_id, &total);\n\n\tstats->running = ctx_id != ~0U && last_switch;\n\tif (stats->running)\n\t\t__extend_last_switch(guc, &stats->start_gt_clk, last_switch);\n\n\t \n\tif (total && total != ~0U) {\n\t\tstats->total_gt_clks += (u32)(total - stats->prev_total);\n\t\tstats->prev_total = total;\n\t}\n}\n\nstatic u32 gpm_timestamp_shift(struct intel_gt *gt)\n{\n\tintel_wakeref_t wakeref;\n\tu32 reg, shift;\n\n\twith_intel_runtime_pm(gt->uncore->rpm, wakeref)\n\t\treg = intel_uncore_read(gt->uncore, RPM_CONFIG0);\n\n\tshift = (reg & GEN10_RPM_CONFIG0_CTC_SHIFT_PARAMETER_MASK) >>\n\t\tGEN10_RPM_CONFIG0_CTC_SHIFT_PARAMETER_SHIFT;\n\n\treturn 3 - shift;\n}\n\nstatic void guc_update_pm_timestamp(struct intel_guc *guc, ktime_t *now)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tu32 gt_stamp_lo, gt_stamp_hi;\n\tu64 gpm_ts;\n\n\tlockdep_assert_held(&guc->timestamp.lock);\n\n\tgt_stamp_hi = upper_32_bits(guc->timestamp.gt_stamp);\n\tgpm_ts = intel_uncore_read64_2x32(gt->uncore, MISC_STATUS0,\n\t\t\t\t\t  MISC_STATUS1) >> guc->timestamp.shift;\n\tgt_stamp_lo = lower_32_bits(gpm_ts);\n\t*now = ktime_get();\n\n\tif (gt_stamp_lo < lower_32_bits(guc->timestamp.gt_stamp))\n\t\tgt_stamp_hi++;\n\n\tguc->timestamp.gt_stamp = ((u64)gt_stamp_hi << 32) | gt_stamp_lo;\n}\n\n \nstatic ktime_t guc_engine_busyness(struct intel_engine_cs *engine, ktime_t *now)\n{\n\tstruct intel_engine_guc_stats stats_saved, *stats = &engine->stats.guc;\n\tstruct i915_gpu_error *gpu_error = &engine->i915->gpu_error;\n\tstruct intel_gt *gt = engine->gt;\n\tstruct intel_guc *guc = &gt->uc.guc;\n\tu64 total, gt_stamp_saved;\n\tunsigned long flags;\n\tu32 reset_count;\n\tbool in_reset;\n\n\tspin_lock_irqsave(&guc->timestamp.lock, flags);\n\n\t \n\treset_count = i915_reset_count(gpu_error);\n\tin_reset = test_bit(I915_RESET_BACKOFF, &gt->reset.flags);\n\n\t*now = ktime_get();\n\n\t \n\tif (!in_reset && intel_gt_pm_get_if_awake(gt)) {\n\t\tstats_saved = *stats;\n\t\tgt_stamp_saved = guc->timestamp.gt_stamp;\n\t\t \n\t\tguc_update_engine_gt_clks(engine);\n\t\tguc_update_pm_timestamp(guc, now);\n\t\tintel_gt_pm_put_async(gt);\n\t\tif (i915_reset_count(gpu_error) != reset_count) {\n\t\t\t*stats = stats_saved;\n\t\t\tguc->timestamp.gt_stamp = gt_stamp_saved;\n\t\t}\n\t}\n\n\ttotal = intel_gt_clock_interval_to_ns(gt, stats->total_gt_clks);\n\tif (stats->running) {\n\t\tu64 clk = guc->timestamp.gt_stamp - stats->start_gt_clk;\n\n\t\ttotal += intel_gt_clock_interval_to_ns(gt, clk);\n\t}\n\n\tspin_unlock_irqrestore(&guc->timestamp.lock, flags);\n\n\treturn ns_to_ktime(total);\n}\n\nstatic void guc_enable_busyness_worker(struct intel_guc *guc)\n{\n\tmod_delayed_work(system_highpri_wq, &guc->timestamp.work, guc->timestamp.ping_delay);\n}\n\nstatic void guc_cancel_busyness_worker(struct intel_guc *guc)\n{\n\tcancel_delayed_work_sync(&guc->timestamp.work);\n}\n\nstatic void __reset_guc_busyness_stats(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tunsigned long flags;\n\tktime_t unused;\n\n\tguc_cancel_busyness_worker(guc);\n\n\tspin_lock_irqsave(&guc->timestamp.lock, flags);\n\n\tguc_update_pm_timestamp(guc, &unused);\n\tfor_each_engine(engine, gt, id) {\n\t\tguc_update_engine_gt_clks(engine);\n\t\tengine->stats.guc.prev_total = 0;\n\t}\n\n\tspin_unlock_irqrestore(&guc->timestamp.lock, flags);\n}\n\nstatic void __update_guc_busyness_stats(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tunsigned long flags;\n\tktime_t unused;\n\n\tguc->timestamp.last_stat_jiffies = jiffies;\n\n\tspin_lock_irqsave(&guc->timestamp.lock, flags);\n\n\tguc_update_pm_timestamp(guc, &unused);\n\tfor_each_engine(engine, gt, id)\n\t\tguc_update_engine_gt_clks(engine);\n\n\tspin_unlock_irqrestore(&guc->timestamp.lock, flags);\n}\n\nstatic void __guc_context_update_stats(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&guc->timestamp.lock, flags);\n\tlrc_update_runtime(ce);\n\tspin_unlock_irqrestore(&guc->timestamp.lock, flags);\n}\n\nstatic void guc_context_update_stats(struct intel_context *ce)\n{\n\tif (!intel_context_pin_if_active(ce))\n\t\treturn;\n\n\t__guc_context_update_stats(ce);\n\tintel_context_unpin(ce);\n}\n\nstatic void guc_timestamp_ping(struct work_struct *wrk)\n{\n\tstruct intel_guc *guc = container_of(wrk, typeof(*guc),\n\t\t\t\t\t     timestamp.work.work);\n\tstruct intel_uc *uc = container_of(guc, typeof(*uc), guc);\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct intel_context *ce;\n\tintel_wakeref_t wakeref;\n\tunsigned long index;\n\tint srcu, ret;\n\n\t \n\twakeref = intel_runtime_pm_get_if_active(&gt->i915->runtime_pm);\n\tif (!wakeref)\n\t\treturn;\n\n\t \n\tret = intel_gt_reset_trylock(gt, &srcu);\n\tif (ret)\n\t\tgoto err_trylock;\n\n\t__update_guc_busyness_stats(guc);\n\n\t \n\txa_for_each(&guc->context_lookup, index, ce)\n\t\tguc_context_update_stats(ce);\n\n\tintel_gt_reset_unlock(gt, srcu);\n\n\tguc_enable_busyness_worker(guc);\n\nerr_trylock:\n\tintel_runtime_pm_put(&gt->i915->runtime_pm, wakeref);\n}\n\nstatic int guc_action_enable_usage_stats(struct intel_guc *guc)\n{\n\tu32 offset = intel_guc_engine_usage_offset(guc);\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_SET_ENG_UTIL_BUFF,\n\t\toffset,\n\t\t0,\n\t};\n\n\treturn intel_guc_send(guc, action, ARRAY_SIZE(action));\n}\n\nstatic int guc_init_engine_stats(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tintel_wakeref_t wakeref;\n\tint ret;\n\n\twith_intel_runtime_pm(&gt->i915->runtime_pm, wakeref)\n\t\tret = guc_action_enable_usage_stats(guc);\n\n\tif (ret)\n\t\tguc_err(guc, \"Failed to enable usage stats: %pe\\n\", ERR_PTR(ret));\n\telse\n\t\tguc_enable_busyness_worker(guc);\n\n\treturn ret;\n}\n\nstatic void guc_fini_engine_stats(struct intel_guc *guc)\n{\n\tguc_cancel_busyness_worker(guc);\n}\n\nvoid intel_guc_busyness_park(struct intel_gt *gt)\n{\n\tstruct intel_guc *guc = &gt->uc.guc;\n\n\tif (!guc_submission_initialized(guc))\n\t\treturn;\n\n\t \n\tguc_cancel_busyness_worker(guc);\n\n\t \n\tif (guc->timestamp.last_stat_jiffies &&\n\t    !time_after(jiffies, guc->timestamp.last_stat_jiffies +\n\t\t\t(guc->timestamp.ping_delay / 2)))\n\t\treturn;\n\n\t__update_guc_busyness_stats(guc);\n}\n\nvoid intel_guc_busyness_unpark(struct intel_gt *gt)\n{\n\tstruct intel_guc *guc = &gt->uc.guc;\n\tunsigned long flags;\n\tktime_t unused;\n\n\tif (!guc_submission_initialized(guc))\n\t\treturn;\n\n\tspin_lock_irqsave(&guc->timestamp.lock, flags);\n\tguc_update_pm_timestamp(guc, &unused);\n\tspin_unlock_irqrestore(&guc->timestamp.lock, flags);\n\tguc_enable_busyness_worker(guc);\n}\n\nstatic inline bool\nsubmission_disabled(struct intel_guc *guc)\n{\n\tstruct i915_sched_engine * const sched_engine = guc->sched_engine;\n\n\treturn unlikely(!sched_engine ||\n\t\t\t!__tasklet_is_enabled(&sched_engine->tasklet) ||\n\t\t\tintel_gt_is_wedged(guc_to_gt(guc)));\n}\n\nstatic void disable_submission(struct intel_guc *guc)\n{\n\tstruct i915_sched_engine * const sched_engine = guc->sched_engine;\n\n\tif (__tasklet_is_enabled(&sched_engine->tasklet)) {\n\t\tGEM_BUG_ON(!guc->ct.enabled);\n\t\t__tasklet_disable_sync_once(&sched_engine->tasklet);\n\t\tsched_engine->tasklet.callback = NULL;\n\t}\n}\n\nstatic void enable_submission(struct intel_guc *guc)\n{\n\tstruct i915_sched_engine * const sched_engine = guc->sched_engine;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&guc->sched_engine->lock, flags);\n\tsched_engine->tasklet.callback = guc_submission_tasklet;\n\twmb();\t \n\tif (!__tasklet_is_enabled(&sched_engine->tasklet) &&\n\t    __tasklet_enable(&sched_engine->tasklet)) {\n\t\tGEM_BUG_ON(!guc->ct.enabled);\n\n\t\t \n\t\ttasklet_hi_schedule(&sched_engine->tasklet);\n\t}\n\tspin_unlock_irqrestore(&guc->sched_engine->lock, flags);\n}\n\nstatic void guc_flush_submissions(struct intel_guc *guc)\n{\n\tstruct i915_sched_engine * const sched_engine = guc->sched_engine;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nstatic void guc_flush_destroyed_contexts(struct intel_guc *guc);\n\nvoid intel_guc_submission_reset_prepare(struct intel_guc *guc)\n{\n\tif (unlikely(!guc_submission_initialized(guc))) {\n\t\t \n\t\treturn;\n\t}\n\n\tintel_gt_park_heartbeats(guc_to_gt(guc));\n\tdisable_submission(guc);\n\tguc->interrupts.disable(guc);\n\t__reset_guc_busyness_stats(guc);\n\n\t \n\tspin_lock_irq(guc_to_gt(guc)->irq_lock);\n\tspin_unlock_irq(guc_to_gt(guc)->irq_lock);\n\n\tguc_flush_submissions(guc);\n\tguc_flush_destroyed_contexts(guc);\n\tflush_work(&guc->ct.requests.worker);\n\n\tscrub_guc_desc_for_outstanding_g2h(guc);\n}\n\nstatic struct intel_engine_cs *\nguc_virtual_get_sibling(struct intel_engine_cs *ve, unsigned int sibling)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t tmp, mask = ve->mask;\n\tunsigned int num_siblings = 0;\n\n\tfor_each_engine_masked(engine, ve->gt, mask, tmp)\n\t\tif (num_siblings++ == sibling)\n\t\t\treturn engine;\n\n\treturn NULL;\n}\n\nstatic inline struct intel_engine_cs *\n__context_to_physical_engine(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\n\tif (intel_engine_is_virtual(engine))\n\t\tengine = guc_virtual_get_sibling(engine, 0);\n\n\treturn engine;\n}\n\nstatic void guc_reset_state(struct intel_context *ce, u32 head, bool scrub)\n{\n\tstruct intel_engine_cs *engine = __context_to_physical_engine(ce);\n\n\tif (!intel_context_is_schedulable(ce))\n\t\treturn;\n\n\tGEM_BUG_ON(!intel_context_is_pinned(ce));\n\n\t \n\tif (scrub)\n\t\tlrc_init_regs(ce, engine, true);\n\n\t \n\tlrc_update_regs(ce, engine, head);\n}\n\nstatic void guc_engine_reset_prepare(struct intel_engine_cs *engine)\n{\n\t \n\tif (IS_MTL_GRAPHICS_STEP(engine->i915, M, STEP_A0, STEP_B0) ||\n\t    (GRAPHICS_VER(engine->i915) >= 11 &&\n\t     GRAPHICS_VER_FULL(engine->i915) < IP_VER(12, 70))) {\n\t\tintel_engine_stop_cs(engine);\n\t\tintel_engine_wait_for_pending_mi_fw(engine);\n\t}\n}\n\nstatic void guc_reset_nop(struct intel_engine_cs *engine)\n{\n}\n\nstatic void guc_rewind_nop(struct intel_engine_cs *engine, bool stalled)\n{\n}\n\nstatic void\n__unwind_incomplete_requests(struct intel_context *ce)\n{\n\tstruct i915_request *rq, *rn;\n\tstruct list_head *pl;\n\tint prio = I915_PRIORITY_INVALID;\n\tstruct i915_sched_engine * const sched_engine =\n\t\tce->engine->sched_engine;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\tspin_lock(&ce->guc_state.lock);\n\tlist_for_each_entry_safe_reverse(rq, rn,\n\t\t\t\t\t &ce->guc_state.requests,\n\t\t\t\t\t sched.link) {\n\t\tif (i915_request_completed(rq))\n\t\t\tcontinue;\n\n\t\tlist_del_init(&rq->sched.link);\n\t\t__i915_request_unsubmit(rq);\n\n\t\t \n\t\tGEM_BUG_ON(rq_prio(rq) == I915_PRIORITY_INVALID);\n\t\tif (rq_prio(rq) != prio) {\n\t\t\tprio = rq_prio(rq);\n\t\t\tpl = i915_sched_lookup_priolist(sched_engine, prio);\n\t\t}\n\t\tGEM_BUG_ON(i915_sched_engine_is_empty(sched_engine));\n\n\t\tlist_add(&rq->sched.link, pl);\n\t\tset_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\t}\n\tspin_unlock(&ce->guc_state.lock);\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nstatic void __guc_reset_context(struct intel_context *ce, intel_engine_mask_t stalled)\n{\n\tbool guilty;\n\tstruct i915_request *rq;\n\tunsigned long flags;\n\tu32 head;\n\tint i, number_children = ce->parallel.number_children;\n\tstruct intel_context *parent = ce;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tintel_context_get(ce);\n\n\t \n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tclr_context_enabled(ce);\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\t \n\tfor (i = 0; i < number_children + 1; ++i) {\n\t\tif (!intel_context_is_pinned(ce))\n\t\t\tgoto next_context;\n\n\t\tguilty = false;\n\t\trq = intel_context_get_active_request(ce);\n\t\tif (!rq) {\n\t\t\thead = ce->ring->tail;\n\t\t\tgoto out_replay;\n\t\t}\n\n\t\tif (i915_request_started(rq))\n\t\t\tguilty = stalled & ce->engine->mask;\n\n\t\tGEM_BUG_ON(i915_active_is_idle(&ce->active));\n\t\thead = intel_ring_wrap(ce->ring, rq->head);\n\n\t\t__i915_request_reset(rq, guilty);\n\t\ti915_request_put(rq);\nout_replay:\n\t\tguc_reset_state(ce, head, guilty);\nnext_context:\n\t\tif (i != number_children)\n\t\t\tce = list_next_entry(ce, parallel.child_link);\n\t}\n\n\t__unwind_incomplete_requests(parent);\n\tintel_context_put(parent);\n}\n\nvoid intel_guc_submission_reset(struct intel_guc *guc, intel_engine_mask_t stalled)\n{\n\tstruct intel_context *ce;\n\tunsigned long index;\n\tunsigned long flags;\n\n\tif (unlikely(!guc_submission_initialized(guc))) {\n\t\t \n\t\treturn;\n\t}\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\tif (!kref_get_unless_zero(&ce->ref))\n\t\t\tcontinue;\n\n\t\txa_unlock(&guc->context_lookup);\n\n\t\tif (intel_context_is_pinned(ce) &&\n\t\t    !intel_context_is_child(ce))\n\t\t\t__guc_reset_context(ce, stalled);\n\n\t\tintel_context_put(ce);\n\n\t\txa_lock(&guc->context_lookup);\n\t}\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n\n\t \n\txa_destroy(&guc->context_lookup);\n}\n\nstatic void guc_cancel_context_requests(struct intel_context *ce)\n{\n\tstruct i915_sched_engine *sched_engine = ce_to_guc(ce)->sched_engine;\n\tstruct i915_request *rq;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\tspin_lock(&ce->guc_state.lock);\n\tlist_for_each_entry(rq, &ce->guc_state.requests, sched.link)\n\t\ti915_request_put(i915_request_mark_eio(rq));\n\tspin_unlock(&ce->guc_state.lock);\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nstatic void\nguc_cancel_sched_engine_requests(struct i915_sched_engine *sched_engine)\n{\n\tstruct i915_request *rq, *rn;\n\tstruct rb_node *rb;\n\tunsigned long flags;\n\n\t \n\tif (!sched_engine)\n\t\treturn;\n\n\t \n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\n\t \n\twhile ((rb = rb_first_cached(&sched_engine->queue))) {\n\t\tstruct i915_priolist *p = to_priolist(rb);\n\n\t\tpriolist_for_each_request_consume(rq, rn, p) {\n\t\t\tlist_del_init(&rq->sched.link);\n\n\t\t\t__i915_request_submit(rq);\n\n\t\t\ti915_request_put(i915_request_mark_eio(rq));\n\t\t}\n\n\t\trb_erase_cached(&p->node, &sched_engine->queue);\n\t\ti915_priolist_free(p);\n\t}\n\n\t \n\n\tsched_engine->queue_priority_hint = INT_MIN;\n\tsched_engine->queue = RB_ROOT_CACHED;\n\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nvoid intel_guc_submission_cancel_requests(struct intel_guc *guc)\n{\n\tstruct intel_context *ce;\n\tunsigned long index;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\tif (!kref_get_unless_zero(&ce->ref))\n\t\t\tcontinue;\n\n\t\txa_unlock(&guc->context_lookup);\n\n\t\tif (intel_context_is_pinned(ce) &&\n\t\t    !intel_context_is_child(ce))\n\t\t\tguc_cancel_context_requests(ce);\n\n\t\tintel_context_put(ce);\n\n\t\txa_lock(&guc->context_lookup);\n\t}\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n\n\tguc_cancel_sched_engine_requests(guc->sched_engine);\n\n\t \n\txa_destroy(&guc->context_lookup);\n}\n\nvoid intel_guc_submission_reset_finish(struct intel_guc *guc)\n{\n\t \n\tif (unlikely(!guc_submission_initialized(guc) ||\n\t\t     intel_gt_is_wedged(guc_to_gt(guc)))) {\n\t\treturn;\n\t}\n\n\t \n\tGEM_WARN_ON(atomic_read(&guc->outstanding_submission_g2h));\n\tatomic_set(&guc->outstanding_submission_g2h, 0);\n\n\tintel_guc_global_policies_update(guc);\n\tenable_submission(guc);\n\tintel_gt_unpark_heartbeats(guc_to_gt(guc));\n}\n\nstatic void destroyed_worker_func(struct work_struct *w);\nstatic void reset_fail_worker_func(struct work_struct *w);\n\n \nint intel_guc_submission_init(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tint ret;\n\n\tif (guc->submission_initialized)\n\t\treturn 0;\n\n\tif (GUC_SUBMIT_VER(guc) < MAKE_GUC_VER(1, 0, 0)) {\n\t\tret = guc_lrc_desc_pool_create_v69(guc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tguc->submission_state.guc_ids_bitmap =\n\t\tbitmap_zalloc(NUMBER_MULTI_LRC_GUC_ID(guc), GFP_KERNEL);\n\tif (!guc->submission_state.guc_ids_bitmap) {\n\t\tret = -ENOMEM;\n\t\tgoto destroy_pool;\n\t}\n\n\tguc->timestamp.ping_delay = (POLL_TIME_CLKS / gt->clock_frequency + 1) * HZ;\n\tguc->timestamp.shift = gpm_timestamp_shift(gt);\n\tguc->submission_initialized = true;\n\n\treturn 0;\n\ndestroy_pool:\n\tguc_lrc_desc_pool_destroy_v69(guc);\n\n\treturn ret;\n}\n\nvoid intel_guc_submission_fini(struct intel_guc *guc)\n{\n\tif (!guc->submission_initialized)\n\t\treturn;\n\n\tguc_flush_destroyed_contexts(guc);\n\tguc_lrc_desc_pool_destroy_v69(guc);\n\ti915_sched_engine_put(guc->sched_engine);\n\tbitmap_free(guc->submission_state.guc_ids_bitmap);\n\tguc->submission_initialized = false;\n}\n\nstatic inline void queue_request(struct i915_sched_engine *sched_engine,\n\t\t\t\t struct i915_request *rq,\n\t\t\t\t int prio)\n{\n\tGEM_BUG_ON(!list_empty(&rq->sched.link));\n\tlist_add_tail(&rq->sched.link,\n\t\t      i915_sched_lookup_priolist(sched_engine, prio));\n\tset_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\ttasklet_hi_schedule(&sched_engine->tasklet);\n}\n\nstatic int guc_bypass_tasklet_submit(struct intel_guc *guc,\n\t\t\t\t     struct i915_request *rq)\n{\n\tint ret = 0;\n\n\t__i915_request_submit(rq);\n\n\ttrace_i915_request_in(rq, 0);\n\n\tif (is_multi_lrc_rq(rq)) {\n\t\tif (multi_lrc_submit(rq)) {\n\t\t\tret = guc_wq_item_append(guc, rq);\n\t\t\tif (!ret)\n\t\t\t\tret = guc_add_request(guc, rq);\n\t\t}\n\t} else {\n\t\tguc_set_lrc_tail(rq);\n\t\tret = guc_add_request(guc, rq);\n\t}\n\n\tif (unlikely(ret == -EPIPE))\n\t\tdisable_submission(guc);\n\n\treturn ret;\n}\n\nstatic bool need_tasklet(struct intel_guc *guc, struct i915_request *rq)\n{\n\tstruct i915_sched_engine *sched_engine = rq->engine->sched_engine;\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\n\treturn submission_disabled(guc) || guc->stalled_request ||\n\t\t!i915_sched_engine_is_empty(sched_engine) ||\n\t\t!ctx_id_mapped(guc, ce->guc_id.id);\n}\n\nstatic void guc_submit_request(struct i915_request *rq)\n{\n\tstruct i915_sched_engine *sched_engine = rq->engine->sched_engine;\n\tstruct intel_guc *guc = &rq->engine->gt->uc.guc;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\n\tif (need_tasklet(guc, rq))\n\t\tqueue_request(sched_engine, rq, rq_prio(rq));\n\telse if (guc_bypass_tasklet_submit(guc, rq) == -EBUSY)\n\t\ttasklet_hi_schedule(&sched_engine->tasklet);\n\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nstatic int new_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tint ret;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tif (intel_context_is_parent(ce))\n\t\tret = bitmap_find_free_region(guc->submission_state.guc_ids_bitmap,\n\t\t\t\t\t      NUMBER_MULTI_LRC_GUC_ID(guc),\n\t\t\t\t\t      order_base_2(ce->parallel.number_children\n\t\t\t\t\t\t\t   + 1));\n\telse\n\t\tret = ida_simple_get(&guc->submission_state.guc_ids,\n\t\t\t\t     NUMBER_MULTI_LRC_GUC_ID(guc),\n\t\t\t\t     guc->submission_state.num_guc_ids,\n\t\t\t\t     GFP_KERNEL | __GFP_RETRY_MAYFAIL |\n\t\t\t\t     __GFP_NOWARN);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tif (!intel_context_is_parent(ce))\n\t\t++guc->submission_state.guc_ids_in_use;\n\n\tce->guc_id.id = ret;\n\treturn 0;\n}\n\nstatic void __release_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tif (!context_guc_id_invalid(ce)) {\n\t\tif (intel_context_is_parent(ce)) {\n\t\t\tbitmap_release_region(guc->submission_state.guc_ids_bitmap,\n\t\t\t\t\t      ce->guc_id.id,\n\t\t\t\t\t      order_base_2(ce->parallel.number_children\n\t\t\t\t\t\t\t   + 1));\n\t\t} else {\n\t\t\t--guc->submission_state.guc_ids_in_use;\n\t\t\tida_simple_remove(&guc->submission_state.guc_ids,\n\t\t\t\t\t  ce->guc_id.id);\n\t\t}\n\t\tclr_ctx_id_mapping(guc, ce->guc_id.id);\n\t\tset_context_guc_id_invalid(ce);\n\t}\n\tif (!list_empty(&ce->guc_id.link))\n\t\tlist_del_init(&ce->guc_id.link);\n}\n\nstatic void release_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\t__release_guc_id(guc, ce);\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n}\n\nstatic int steal_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tstruct intel_context *cn;\n\n\tlockdep_assert_held(&guc->submission_state.lock);\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\tGEM_BUG_ON(intel_context_is_parent(ce));\n\n\tif (!list_empty(&guc->submission_state.guc_id_list)) {\n\t\tcn = list_first_entry(&guc->submission_state.guc_id_list,\n\t\t\t\t      struct intel_context,\n\t\t\t\t      guc_id.link);\n\n\t\tGEM_BUG_ON(atomic_read(&cn->guc_id.ref));\n\t\tGEM_BUG_ON(context_guc_id_invalid(cn));\n\t\tGEM_BUG_ON(intel_context_is_child(cn));\n\t\tGEM_BUG_ON(intel_context_is_parent(cn));\n\n\t\tlist_del_init(&cn->guc_id.link);\n\t\tce->guc_id.id = cn->guc_id.id;\n\n\t\tspin_lock(&cn->guc_state.lock);\n\t\tclr_context_registered(cn);\n\t\tspin_unlock(&cn->guc_state.lock);\n\n\t\tset_context_guc_id_invalid(cn);\n\n#ifdef CONFIG_DRM_I915_SELFTEST\n\t\tguc->number_guc_id_stolen++;\n#endif\n\n\t\treturn 0;\n\t} else {\n\t\treturn -EAGAIN;\n\t}\n}\n\nstatic int assign_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tint ret;\n\n\tlockdep_assert_held(&guc->submission_state.lock);\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tret = new_guc_id(guc, ce);\n\tif (unlikely(ret < 0)) {\n\t\tif (intel_context_is_parent(ce))\n\t\t\treturn -ENOSPC;\n\n\t\tret = steal_guc_id(guc, ce);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (intel_context_is_parent(ce)) {\n\t\tstruct intel_context *child;\n\t\tint i = 1;\n\n\t\tfor_each_child(ce, child)\n\t\t\tchild->guc_id.id = ce->guc_id.id + i++;\n\t}\n\n\treturn 0;\n}\n\n#define PIN_GUC_ID_TRIES\t4\nstatic int pin_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tint ret = 0;\n\tunsigned long flags, tries = PIN_GUC_ID_TRIES;\n\n\tGEM_BUG_ON(atomic_read(&ce->guc_id.ref));\n\ntry_again:\n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\n\tmight_lock(&ce->guc_state.lock);\n\n\tif (context_guc_id_invalid(ce)) {\n\t\tret = assign_guc_id(guc, ce);\n\t\tif (ret)\n\t\t\tgoto out_unlock;\n\t\tret = 1;\t \n\t}\n\tif (!list_empty(&ce->guc_id.link))\n\t\tlist_del_init(&ce->guc_id.link);\n\tatomic_inc(&ce->guc_id.ref);\n\nout_unlock:\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\n\t \n\tif (ret == -EAGAIN && --tries) {\n\t\tif (PIN_GUC_ID_TRIES - tries > 1) {\n\t\t\tunsigned int timeslice_shifted =\n\t\t\t\tce->engine->props.timeslice_duration_ms <<\n\t\t\t\t(PIN_GUC_ID_TRIES - tries - 2);\n\t\t\tunsigned int max = min_t(unsigned int, 100,\n\t\t\t\t\t\t timeslice_shifted);\n\n\t\t\tmsleep(max_t(unsigned int, max, 1));\n\t\t}\n\t\tintel_gt_retire_requests(guc_to_gt(guc));\n\t\tgoto try_again;\n\t}\n\n\treturn ret;\n}\n\nstatic void unpin_guc_id(struct intel_guc *guc, struct intel_context *ce)\n{\n\tunsigned long flags;\n\n\tGEM_BUG_ON(atomic_read(&ce->guc_id.ref) < 0);\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tif (unlikely(context_guc_id_invalid(ce) ||\n\t\t     intel_context_is_parent(ce)))\n\t\treturn;\n\n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\tif (!context_guc_id_invalid(ce) && list_empty(&ce->guc_id.link) &&\n\t    !atomic_read(&ce->guc_id.ref))\n\t\tlist_add_tail(&ce->guc_id.link,\n\t\t\t      &guc->submission_state.guc_id_list);\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n}\n\nstatic int __guc_action_register_multi_lrc_v69(struct intel_guc *guc,\n\t\t\t\t\t       struct intel_context *ce,\n\t\t\t\t\t       u32 guc_id,\n\t\t\t\t\t       u32 offset,\n\t\t\t\t\t       bool loop)\n{\n\tstruct intel_context *child;\n\tu32 action[4 + MAX_ENGINE_INSTANCE];\n\tint len = 0;\n\n\tGEM_BUG_ON(ce->parallel.number_children > MAX_ENGINE_INSTANCE);\n\n\taction[len++] = INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC;\n\taction[len++] = guc_id;\n\taction[len++] = ce->parallel.number_children + 1;\n\taction[len++] = offset;\n\tfor_each_child(ce, child) {\n\t\toffset += sizeof(struct guc_lrc_desc_v69);\n\t\taction[len++] = offset;\n\t}\n\n\treturn guc_submission_send_busy_loop(guc, action, len, 0, loop);\n}\n\nstatic int __guc_action_register_multi_lrc_v70(struct intel_guc *guc,\n\t\t\t\t\t       struct intel_context *ce,\n\t\t\t\t\t       struct guc_ctxt_registration_info *info,\n\t\t\t\t\t       bool loop)\n{\n\tstruct intel_context *child;\n\tu32 action[13 + (MAX_ENGINE_INSTANCE * 2)];\n\tint len = 0;\n\tu32 next_id;\n\n\tGEM_BUG_ON(ce->parallel.number_children > MAX_ENGINE_INSTANCE);\n\n\taction[len++] = INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC;\n\taction[len++] = info->flags;\n\taction[len++] = info->context_idx;\n\taction[len++] = info->engine_class;\n\taction[len++] = info->engine_submit_mask;\n\taction[len++] = info->wq_desc_lo;\n\taction[len++] = info->wq_desc_hi;\n\taction[len++] = info->wq_base_lo;\n\taction[len++] = info->wq_base_hi;\n\taction[len++] = info->wq_size;\n\taction[len++] = ce->parallel.number_children + 1;\n\taction[len++] = info->hwlrca_lo;\n\taction[len++] = info->hwlrca_hi;\n\n\tnext_id = info->context_idx + 1;\n\tfor_each_child(ce, child) {\n\t\tGEM_BUG_ON(next_id++ != child->guc_id.id);\n\n\t\t \n\t\taction[len++] = lower_32_bits(child->lrc.lrca);\n\t\taction[len++] = upper_32_bits(child->lrc.lrca);\n\t}\n\n\tGEM_BUG_ON(len > ARRAY_SIZE(action));\n\n\treturn guc_submission_send_busy_loop(guc, action, len, 0, loop);\n}\n\nstatic int __guc_action_register_context_v69(struct intel_guc *guc,\n\t\t\t\t\t     u32 guc_id,\n\t\t\t\t\t     u32 offset,\n\t\t\t\t\t     bool loop)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_REGISTER_CONTEXT,\n\t\tguc_id,\n\t\toffset,\n\t};\n\n\treturn guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),\n\t\t\t\t\t     0, loop);\n}\n\nstatic int __guc_action_register_context_v70(struct intel_guc *guc,\n\t\t\t\t\t     struct guc_ctxt_registration_info *info,\n\t\t\t\t\t     bool loop)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_REGISTER_CONTEXT,\n\t\tinfo->flags,\n\t\tinfo->context_idx,\n\t\tinfo->engine_class,\n\t\tinfo->engine_submit_mask,\n\t\tinfo->wq_desc_lo,\n\t\tinfo->wq_desc_hi,\n\t\tinfo->wq_base_lo,\n\t\tinfo->wq_base_hi,\n\t\tinfo->wq_size,\n\t\tinfo->hwlrca_lo,\n\t\tinfo->hwlrca_hi,\n\t};\n\n\treturn guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),\n\t\t\t\t\t     0, loop);\n}\n\nstatic void prepare_context_registration_info_v69(struct intel_context *ce);\nstatic void prepare_context_registration_info_v70(struct intel_context *ce,\n\t\t\t\t\t\t  struct guc_ctxt_registration_info *info);\n\nstatic int\nregister_context_v69(struct intel_guc *guc, struct intel_context *ce, bool loop)\n{\n\tu32 offset = intel_guc_ggtt_offset(guc, guc->lrc_desc_pool_v69) +\n\t\tce->guc_id.id * sizeof(struct guc_lrc_desc_v69);\n\n\tprepare_context_registration_info_v69(ce);\n\n\tif (intel_context_is_parent(ce))\n\t\treturn __guc_action_register_multi_lrc_v69(guc, ce, ce->guc_id.id,\n\t\t\t\t\t\t\t   offset, loop);\n\telse\n\t\treturn __guc_action_register_context_v69(guc, ce->guc_id.id,\n\t\t\t\t\t\t\t offset, loop);\n}\n\nstatic int\nregister_context_v70(struct intel_guc *guc, struct intel_context *ce, bool loop)\n{\n\tstruct guc_ctxt_registration_info info;\n\n\tprepare_context_registration_info_v70(ce, &info);\n\n\tif (intel_context_is_parent(ce))\n\t\treturn __guc_action_register_multi_lrc_v70(guc, ce, &info, loop);\n\telse\n\t\treturn __guc_action_register_context_v70(guc, &info, loop);\n}\n\nstatic int register_context(struct intel_context *ce, bool loop)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tint ret;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\ttrace_intel_context_register(ce);\n\n\tif (GUC_SUBMIT_VER(guc) >= MAKE_GUC_VER(1, 0, 0))\n\t\tret = register_context_v70(guc, ce, loop);\n\telse\n\t\tret = register_context_v69(guc, ce, loop);\n\n\tif (likely(!ret)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\t\tset_context_registered(ce);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\t\tif (GUC_SUBMIT_VER(guc) >= MAKE_GUC_VER(1, 0, 0))\n\t\t\tguc_context_policy_init_v70(ce, loop);\n\t}\n\n\treturn ret;\n}\n\nstatic int __guc_action_deregister_context(struct intel_guc *guc,\n\t\t\t\t\t   u32 guc_id)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_DEREGISTER_CONTEXT,\n\t\tguc_id,\n\t};\n\n\treturn guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),\n\t\t\t\t\t     G2H_LEN_DW_DEREGISTER_CONTEXT,\n\t\t\t\t\t     true);\n}\n\nstatic int deregister_context(struct intel_context *ce, u32 guc_id)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\ttrace_intel_context_deregister(ce);\n\n\treturn __guc_action_deregister_context(guc, guc_id);\n}\n\nstatic inline void clear_children_join_go_memory(struct intel_context *ce)\n{\n\tstruct parent_scratch *ps = __get_parent_scratch(ce);\n\tint i;\n\n\tps->go.semaphore = 0;\n\tfor (i = 0; i < ce->parallel.number_children + 1; ++i)\n\t\tps->join[i].semaphore = 0;\n}\n\nstatic inline u32 get_children_go_value(struct intel_context *ce)\n{\n\treturn __get_parent_scratch(ce)->go.semaphore;\n}\n\nstatic inline u32 get_children_join_value(struct intel_context *ce,\n\t\t\t\t\t  u8 child_index)\n{\n\treturn __get_parent_scratch(ce)->join[child_index].semaphore;\n}\n\nstruct context_policy {\n\tu32 count;\n\tstruct guc_update_context_policy h2g;\n};\n\nstatic u32 __guc_context_policy_action_size(struct context_policy *policy)\n{\n\tsize_t bytes = sizeof(policy->h2g.header) +\n\t\t       (sizeof(policy->h2g.klv[0]) * policy->count);\n\n\treturn bytes / sizeof(u32);\n}\n\nstatic void __guc_context_policy_start_klv(struct context_policy *policy, u16 guc_id)\n{\n\tpolicy->h2g.header.action = INTEL_GUC_ACTION_HOST2GUC_UPDATE_CONTEXT_POLICIES;\n\tpolicy->h2g.header.ctx_id = guc_id;\n\tpolicy->count = 0;\n}\n\n#define MAKE_CONTEXT_POLICY_ADD(func, id) \\\nstatic void __guc_context_policy_add_##func(struct context_policy *policy, u32 data) \\\n{ \\\n\tGEM_BUG_ON(policy->count >= GUC_CONTEXT_POLICIES_KLV_NUM_IDS); \\\n\tpolicy->h2g.klv[policy->count].kl = \\\n\t\tFIELD_PREP(GUC_KLV_0_KEY, GUC_CONTEXT_POLICIES_KLV_ID_##id) | \\\n\t\tFIELD_PREP(GUC_KLV_0_LEN, 1); \\\n\tpolicy->h2g.klv[policy->count].value = data; \\\n\tpolicy->count++; \\\n}\n\nMAKE_CONTEXT_POLICY_ADD(execution_quantum, EXECUTION_QUANTUM)\nMAKE_CONTEXT_POLICY_ADD(preemption_timeout, PREEMPTION_TIMEOUT)\nMAKE_CONTEXT_POLICY_ADD(priority, SCHEDULING_PRIORITY)\nMAKE_CONTEXT_POLICY_ADD(preempt_to_idle, PREEMPT_TO_IDLE_ON_QUANTUM_EXPIRY)\n\n#undef MAKE_CONTEXT_POLICY_ADD\n\nstatic int __guc_context_set_context_policies(struct intel_guc *guc,\n\t\t\t\t\t      struct context_policy *policy,\n\t\t\t\t\t      bool loop)\n{\n\treturn guc_submission_send_busy_loop(guc, (u32 *)&policy->h2g,\n\t\t\t\t\t__guc_context_policy_action_size(policy),\n\t\t\t\t\t0, loop);\n}\n\nstatic int guc_context_policy_init_v70(struct intel_context *ce, bool loop)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tstruct context_policy policy;\n\tu32 execution_quantum;\n\tu32 preemption_timeout;\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tGEM_BUG_ON(overflows_type(engine->props.timeslice_duration_ms * 1000,\n\t\t\t\t  execution_quantum));\n\tGEM_BUG_ON(overflows_type(engine->props.preempt_timeout_ms * 1000,\n\t\t\t\t  preemption_timeout));\n\texecution_quantum = engine->props.timeslice_duration_ms * 1000;\n\tpreemption_timeout = engine->props.preempt_timeout_ms * 1000;\n\n\t__guc_context_policy_start_klv(&policy, ce->guc_id.id);\n\n\t__guc_context_policy_add_priority(&policy, ce->guc_state.prio);\n\t__guc_context_policy_add_execution_quantum(&policy, execution_quantum);\n\t__guc_context_policy_add_preemption_timeout(&policy, preemption_timeout);\n\n\tif (engine->flags & I915_ENGINE_WANT_FORCED_PREEMPTION)\n\t\t__guc_context_policy_add_preempt_to_idle(&policy, 1);\n\n\tret = __guc_context_set_context_policies(guc, &policy, loop);\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tif (ret != 0)\n\t\tset_context_policy_required(ce);\n\telse\n\t\tclr_context_policy_required(ce);\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\treturn ret;\n}\n\nstatic void guc_context_policy_init_v69(struct intel_engine_cs *engine,\n\t\t\t\t\tstruct guc_lrc_desc_v69 *desc)\n{\n\tdesc->policy_flags = 0;\n\n\tif (engine->flags & I915_ENGINE_WANT_FORCED_PREEMPTION)\n\t\tdesc->policy_flags |= CONTEXT_POLICY_FLAG_PREEMPT_TO_IDLE_V69;\n\n\t \n\tGEM_BUG_ON(overflows_type(engine->props.timeslice_duration_ms * 1000,\n\t\t\t\t  desc->execution_quantum));\n\tGEM_BUG_ON(overflows_type(engine->props.preempt_timeout_ms * 1000,\n\t\t\t\t  desc->preemption_timeout));\n\tdesc->execution_quantum = engine->props.timeslice_duration_ms * 1000;\n\tdesc->preemption_timeout = engine->props.preempt_timeout_ms * 1000;\n}\n\nstatic u32 map_guc_prio_to_lrc_desc_prio(u8 prio)\n{\n\t \n\tswitch (prio) {\n\tdefault:\n\t\tMISSING_CASE(prio);\n\t\tfallthrough;\n\tcase GUC_CLIENT_PRIORITY_KMD_NORMAL:\n\t\treturn GEN12_CTX_PRIORITY_NORMAL;\n\tcase GUC_CLIENT_PRIORITY_NORMAL:\n\t\treturn GEN12_CTX_PRIORITY_LOW;\n\tcase GUC_CLIENT_PRIORITY_HIGH:\n\tcase GUC_CLIENT_PRIORITY_KMD_HIGH:\n\t\treturn GEN12_CTX_PRIORITY_HIGH;\n\t}\n}\n\nstatic void prepare_context_registration_info_v69(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tu32 ctx_id = ce->guc_id.id;\n\tstruct guc_lrc_desc_v69 *desc;\n\tstruct intel_context *child;\n\n\tGEM_BUG_ON(!engine->mask);\n\n\t \n\tGEM_BUG_ON(i915_gem_object_is_lmem(guc->ct.vma->obj) !=\n\t\t   i915_gem_object_is_lmem(ce->ring->vma->obj));\n\n\tdesc = __get_lrc_desc_v69(guc, ctx_id);\n\tGEM_BUG_ON(!desc);\n\tdesc->engine_class = engine_class_to_guc_class(engine->class);\n\tdesc->engine_submit_mask = engine->logical_mask;\n\tdesc->hw_context_desc = ce->lrc.lrca;\n\tdesc->priority = ce->guc_state.prio;\n\tdesc->context_flags = CONTEXT_REGISTRATION_FLAG_KMD;\n\tguc_context_policy_init_v69(engine, desc);\n\n\t \n\tif (intel_context_is_parent(ce)) {\n\t\tstruct guc_process_desc_v69 *pdesc;\n\n\t\tce->parallel.guc.wqi_tail = 0;\n\t\tce->parallel.guc.wqi_head = 0;\n\n\t\tdesc->process_desc = i915_ggtt_offset(ce->state) +\n\t\t\t__get_parent_scratch_offset(ce);\n\t\tdesc->wq_addr = i915_ggtt_offset(ce->state) +\n\t\t\t__get_wq_offset(ce);\n\t\tdesc->wq_size = WQ_SIZE;\n\n\t\tpdesc = __get_process_desc_v69(ce);\n\t\tmemset(pdesc, 0, sizeof(*(pdesc)));\n\t\tpdesc->stage_id = ce->guc_id.id;\n\t\tpdesc->wq_base_addr = desc->wq_addr;\n\t\tpdesc->wq_size_bytes = desc->wq_size;\n\t\tpdesc->wq_status = WQ_STATUS_ACTIVE;\n\n\t\tce->parallel.guc.wq_head = &pdesc->head;\n\t\tce->parallel.guc.wq_tail = &pdesc->tail;\n\t\tce->parallel.guc.wq_status = &pdesc->wq_status;\n\n\t\tfor_each_child(ce, child) {\n\t\t\tdesc = __get_lrc_desc_v69(guc, child->guc_id.id);\n\n\t\t\tdesc->engine_class =\n\t\t\t\tengine_class_to_guc_class(engine->class);\n\t\t\tdesc->hw_context_desc = child->lrc.lrca;\n\t\t\tdesc->priority = ce->guc_state.prio;\n\t\t\tdesc->context_flags = CONTEXT_REGISTRATION_FLAG_KMD;\n\t\t\tguc_context_policy_init_v69(engine, desc);\n\t\t}\n\n\t\tclear_children_join_go_memory(ce);\n\t}\n}\n\nstatic void prepare_context_registration_info_v70(struct intel_context *ce,\n\t\t\t\t\t\t  struct guc_ctxt_registration_info *info)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tu32 ctx_id = ce->guc_id.id;\n\n\tGEM_BUG_ON(!engine->mask);\n\n\t \n\tGEM_BUG_ON(i915_gem_object_is_lmem(guc->ct.vma->obj) !=\n\t\t   i915_gem_object_is_lmem(ce->ring->vma->obj));\n\n\tmemset(info, 0, sizeof(*info));\n\tinfo->context_idx = ctx_id;\n\tinfo->engine_class = engine_class_to_guc_class(engine->class);\n\tinfo->engine_submit_mask = engine->logical_mask;\n\t \n\tinfo->hwlrca_lo = lower_32_bits(ce->lrc.lrca);\n\tinfo->hwlrca_hi = upper_32_bits(ce->lrc.lrca);\n\tif (engine->flags & I915_ENGINE_HAS_EU_PRIORITY)\n\t\tinfo->hwlrca_lo |= map_guc_prio_to_lrc_desc_prio(ce->guc_state.prio);\n\tinfo->flags = CONTEXT_REGISTRATION_FLAG_KMD;\n\n\t \n\tif (intel_context_is_parent(ce)) {\n\t\tstruct guc_sched_wq_desc *wq_desc;\n\t\tu64 wq_desc_offset, wq_base_offset;\n\n\t\tce->parallel.guc.wqi_tail = 0;\n\t\tce->parallel.guc.wqi_head = 0;\n\n\t\twq_desc_offset = i915_ggtt_offset(ce->state) +\n\t\t\t\t __get_parent_scratch_offset(ce);\n\t\twq_base_offset = i915_ggtt_offset(ce->state) +\n\t\t\t\t __get_wq_offset(ce);\n\t\tinfo->wq_desc_lo = lower_32_bits(wq_desc_offset);\n\t\tinfo->wq_desc_hi = upper_32_bits(wq_desc_offset);\n\t\tinfo->wq_base_lo = lower_32_bits(wq_base_offset);\n\t\tinfo->wq_base_hi = upper_32_bits(wq_base_offset);\n\t\tinfo->wq_size = WQ_SIZE;\n\n\t\twq_desc = __get_wq_desc_v70(ce);\n\t\tmemset(wq_desc, 0, sizeof(*wq_desc));\n\t\twq_desc->wq_status = WQ_STATUS_ACTIVE;\n\n\t\tce->parallel.guc.wq_head = &wq_desc->head;\n\t\tce->parallel.guc.wq_tail = &wq_desc->tail;\n\t\tce->parallel.guc.wq_status = &wq_desc->wq_status;\n\n\t\tclear_children_join_go_memory(ce);\n\t}\n}\n\nstatic int try_context_registration(struct intel_context *ce, bool loop)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct intel_runtime_pm *runtime_pm = engine->uncore->rpm;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tintel_wakeref_t wakeref;\n\tu32 ctx_id = ce->guc_id.id;\n\tbool context_registered;\n\tint ret = 0;\n\n\tGEM_BUG_ON(!sched_state_is_init(ce));\n\n\tcontext_registered = ctx_id_mapped(guc, ctx_id);\n\n\tclr_ctx_id_mapping(guc, ctx_id);\n\tset_ctx_id_mapping(guc, ctx_id, ce);\n\n\t \n\tif (context_registered) {\n\t\tbool disabled;\n\t\tunsigned long flags;\n\n\t\ttrace_intel_context_steal_guc_id(ce);\n\t\tGEM_BUG_ON(!loop);\n\n\t\t \n\t\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\t\tdisabled = submission_disabled(guc);\n\t\tif (likely(!disabled)) {\n\t\t\tset_context_wait_for_deregister_to_register(ce);\n\t\t\tintel_context_get(ce);\n\t\t}\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t\tif (unlikely(disabled)) {\n\t\t\tclr_ctx_id_mapping(guc, ctx_id);\n\t\t\treturn 0;\t \n\t\t}\n\n\t\t \n\t\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t\tret = deregister_context(ce, ce->guc_id.id);\n\t\tif (unlikely(ret == -ENODEV))\n\t\t\tret = 0;\t \n\t} else {\n\t\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t\tret = register_context(ce, loop);\n\t\tif (unlikely(ret == -EBUSY)) {\n\t\t\tclr_ctx_id_mapping(guc, ctx_id);\n\t\t} else if (unlikely(ret == -ENODEV)) {\n\t\t\tclr_ctx_id_mapping(guc, ctx_id);\n\t\t\tret = 0;\t \n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int __guc_context_pre_pin(struct intel_context *ce,\n\t\t\t\t struct intel_engine_cs *engine,\n\t\t\t\t struct i915_gem_ww_ctx *ww,\n\t\t\t\t void **vaddr)\n{\n\treturn lrc_pre_pin(ce, engine, ww, vaddr);\n}\n\nstatic int __guc_context_pin(struct intel_context *ce,\n\t\t\t     struct intel_engine_cs *engine,\n\t\t\t     void *vaddr)\n{\n\tif (i915_ggtt_offset(ce->state) !=\n\t    (ce->lrc.lrca & CTX_GTT_ADDRESS_MASK))\n\t\tset_bit(CONTEXT_LRCA_DIRTY, &ce->flags);\n\n\t \n\n\treturn lrc_pin(ce, engine, vaddr);\n}\n\nstatic int guc_context_pre_pin(struct intel_context *ce,\n\t\t\t       struct i915_gem_ww_ctx *ww,\n\t\t\t       void **vaddr)\n{\n\treturn __guc_context_pre_pin(ce, ce->engine, ww, vaddr);\n}\n\nstatic int guc_context_pin(struct intel_context *ce, void *vaddr)\n{\n\tint ret = __guc_context_pin(ce, ce->engine, vaddr);\n\n\tif (likely(!ret && !intel_context_is_barrier(ce)))\n\t\tintel_engine_pm_get(ce->engine);\n\n\treturn ret;\n}\n\nstatic void guc_context_unpin(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\n\t__guc_context_update_stats(ce);\n\tunpin_guc_id(guc, ce);\n\tlrc_unpin(ce);\n\n\tif (likely(!intel_context_is_barrier(ce)))\n\t\tintel_engine_pm_put_async(ce->engine);\n}\n\nstatic void guc_context_post_unpin(struct intel_context *ce)\n{\n\tlrc_post_unpin(ce);\n}\n\nstatic void __guc_context_sched_enable(struct intel_guc *guc,\n\t\t\t\t       struct intel_context *ce)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_SCHED_CONTEXT_MODE_SET,\n\t\tce->guc_id.id,\n\t\tGUC_CONTEXT_ENABLE\n\t};\n\n\ttrace_intel_context_sched_enable(ce);\n\n\tguc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),\n\t\t\t\t      G2H_LEN_DW_SCHED_CONTEXT_MODE_SET, true);\n}\n\nstatic void __guc_context_sched_disable(struct intel_guc *guc,\n\t\t\t\t\tstruct intel_context *ce,\n\t\t\t\t\tu16 guc_id)\n{\n\tu32 action[] = {\n\t\tINTEL_GUC_ACTION_SCHED_CONTEXT_MODE_SET,\n\t\tguc_id,\t \n\t\tGUC_CONTEXT_DISABLE\n\t};\n\n\tGEM_BUG_ON(guc_id == GUC_INVALID_CONTEXT_ID);\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\ttrace_intel_context_sched_disable(ce);\n\n\tguc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),\n\t\t\t\t      G2H_LEN_DW_SCHED_CONTEXT_MODE_SET, true);\n}\n\nstatic void guc_blocked_fence_complete(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tif (!i915_sw_fence_done(&ce->guc_state.blocked))\n\t\ti915_sw_fence_complete(&ce->guc_state.blocked);\n}\n\nstatic void guc_blocked_fence_reinit(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tGEM_BUG_ON(!i915_sw_fence_done(&ce->guc_state.blocked));\n\n\t \n\ti915_sw_fence_fini(&ce->guc_state.blocked);\n\ti915_sw_fence_reinit(&ce->guc_state.blocked);\n\ti915_sw_fence_await(&ce->guc_state.blocked);\n\ti915_sw_fence_commit(&ce->guc_state.blocked);\n}\n\nstatic u16 prep_context_pending_disable(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tset_context_pending_disable(ce);\n\tclr_context_enabled(ce);\n\tguc_blocked_fence_reinit(ce);\n\tintel_context_get(ce);\n\n\treturn ce->guc_id.id;\n}\n\nstatic struct i915_sw_fence *guc_context_block(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\tstruct intel_runtime_pm *runtime_pm = ce->engine->uncore->rpm;\n\tintel_wakeref_t wakeref;\n\tu16 guc_id;\n\tbool enabled;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\n\tincr_context_blocked(ce);\n\n\tenabled = context_enabled(ce);\n\tif (unlikely(!enabled || submission_disabled(guc))) {\n\t\tif (enabled)\n\t\t\tclr_context_enabled(ce);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t\treturn &ce->guc_state.blocked;\n\t}\n\n\t \n\tatomic_add(2, &ce->pin_count);\n\n\tguc_id = prep_context_pending_disable(ce);\n\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t__guc_context_sched_disable(guc, ce, guc_id);\n\n\treturn &ce->guc_state.blocked;\n}\n\n#define SCHED_STATE_MULTI_BLOCKED_MASK \\\n\t(SCHED_STATE_BLOCKED_MASK & ~SCHED_STATE_BLOCKED)\n#define SCHED_STATE_NO_UNBLOCK \\\n\t(SCHED_STATE_MULTI_BLOCKED_MASK | \\\n\t SCHED_STATE_PENDING_DISABLE | \\\n\t SCHED_STATE_BANNED)\n\nstatic bool context_cant_unblock(struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\treturn (ce->guc_state.sched_state & SCHED_STATE_NO_UNBLOCK) ||\n\t\tcontext_guc_id_invalid(ce) ||\n\t\t!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id) ||\n\t\t!intel_context_is_pinned(ce);\n}\n\nstatic void guc_context_unblock(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\tstruct intel_runtime_pm *runtime_pm = ce->engine->uncore->rpm;\n\tintel_wakeref_t wakeref;\n\tbool enable;\n\n\tGEM_BUG_ON(context_enabled(ce));\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\n\tif (unlikely(submission_disabled(guc) ||\n\t\t     context_cant_unblock(ce))) {\n\t\tenable = false;\n\t} else {\n\t\tenable = true;\n\t\tset_context_pending_enable(ce);\n\t\tset_context_enabled(ce);\n\t\tintel_context_get(ce);\n\t}\n\n\tdecr_context_blocked(ce);\n\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\tif (enable) {\n\t\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t\t__guc_context_sched_enable(guc, ce);\n\t}\n}\n\nstatic void guc_context_cancel_request(struct intel_context *ce,\n\t\t\t\t       struct i915_request *rq)\n{\n\tstruct intel_context *block_context =\n\t\trequest_to_scheduling_context(rq);\n\n\tif (i915_sw_fence_signaled(&rq->submit)) {\n\t\tstruct i915_sw_fence *fence;\n\n\t\tintel_context_get(ce);\n\t\tfence = guc_context_block(block_context);\n\t\ti915_sw_fence_wait(fence);\n\t\tif (!i915_request_completed(rq)) {\n\t\t\t__i915_request_skip(rq);\n\t\t\tguc_reset_state(ce, intel_ring_wrap(ce->ring, rq->head),\n\t\t\t\t\ttrue);\n\t\t}\n\n\t\tguc_context_unblock(block_context);\n\t\tintel_context_put(ce);\n\t}\n}\n\nstatic void __guc_context_set_preemption_timeout(struct intel_guc *guc,\n\t\t\t\t\t\t u16 guc_id,\n\t\t\t\t\t\t u32 preemption_timeout)\n{\n\tif (GUC_SUBMIT_VER(guc) >= MAKE_GUC_VER(1, 0, 0)) {\n\t\tstruct context_policy policy;\n\n\t\t__guc_context_policy_start_klv(&policy, guc_id);\n\t\t__guc_context_policy_add_preemption_timeout(&policy, preemption_timeout);\n\t\t__guc_context_set_context_policies(guc, &policy, true);\n\t} else {\n\t\tu32 action[] = {\n\t\t\tINTEL_GUC_ACTION_V69_SET_CONTEXT_PREEMPTION_TIMEOUT,\n\t\t\tguc_id,\n\t\t\tpreemption_timeout\n\t\t};\n\n\t\tintel_guc_send_busy_loop(guc, action, ARRAY_SIZE(action), 0, true);\n\t}\n}\n\nstatic void\nguc_context_revoke(struct intel_context *ce, struct i915_request *rq,\n\t\t   unsigned int preempt_timeout_ms)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tstruct intel_runtime_pm *runtime_pm =\n\t\t&ce->engine->gt->i915->runtime_pm;\n\tintel_wakeref_t wakeref;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tguc_flush_submissions(guc);\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tset_context_banned(ce);\n\n\tif (submission_disabled(guc) ||\n\t    (!context_enabled(ce) && !context_pending_disable(ce))) {\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\t\tguc_cancel_context_requests(ce);\n\t\tintel_engine_signal_breadcrumbs(ce->engine);\n\t} else if (!context_pending_disable(ce)) {\n\t\tu16 guc_id;\n\n\t\t \n\t\tatomic_add(2, &ce->pin_count);\n\n\t\tguc_id = prep_context_pending_disable(ce);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\t\t \n\t\twith_intel_runtime_pm(runtime_pm, wakeref) {\n\t\t\t__guc_context_set_preemption_timeout(guc, guc_id,\n\t\t\t\t\t\t\t     preempt_timeout_ms);\n\t\t\t__guc_context_sched_disable(guc, ce, guc_id);\n\t\t}\n\t} else {\n\t\tif (!context_guc_id_invalid(ce))\n\t\t\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t\t\t__guc_context_set_preemption_timeout(guc,\n\t\t\t\t\t\t\t\t     ce->guc_id.id,\n\t\t\t\t\t\t\t\t     preempt_timeout_ms);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t}\n}\n\nstatic void do_sched_disable(struct intel_guc *guc, struct intel_context *ce,\n\t\t\t     unsigned long flags)\n\t__releases(ce->guc_state.lock)\n{\n\tstruct intel_runtime_pm *runtime_pm = &ce->engine->gt->i915->runtime_pm;\n\tintel_wakeref_t wakeref;\n\tu16 guc_id;\n\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tguc_id = prep_context_pending_disable(ce);\n\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t__guc_context_sched_disable(guc, ce, guc_id);\n}\n\nstatic bool bypass_sched_disable(struct intel_guc *guc,\n\t\t\t\t struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tif (submission_disabled(guc) || context_guc_id_invalid(ce) ||\n\t    !ctx_id_mapped(guc, ce->guc_id.id)) {\n\t\tclr_context_enabled(ce);\n\t\treturn true;\n\t}\n\n\treturn !context_enabled(ce);\n}\n\nstatic void __delay_sched_disable(struct work_struct *wrk)\n{\n\tstruct intel_context *ce =\n\t\tcontainer_of(wrk, typeof(*ce), guc_state.sched_disable_delay_work.work);\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\n\tif (bypass_sched_disable(guc, ce)) {\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t\tintel_context_sched_disable_unpin(ce);\n\t} else {\n\t\tdo_sched_disable(guc, ce, flags);\n\t}\n}\n\nstatic bool guc_id_pressure(struct intel_guc *guc, struct intel_context *ce)\n{\n\t \n\tif (intel_context_is_parent(ce))\n\t\treturn true;\n\n\t \n\treturn guc->submission_state.guc_ids_in_use >\n\t\tguc->submission_state.sched_disable_gucid_threshold;\n}\n\nstatic void guc_context_sched_disable(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tu64 delay = guc->submission_state.sched_disable_delay_ms;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\n\tif (bypass_sched_disable(guc, ce)) {\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t\tintel_context_sched_disable_unpin(ce);\n\t} else if (!intel_context_is_closed(ce) && !guc_id_pressure(guc, ce) &&\n\t\t   delay) {\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t\tmod_delayed_work(system_unbound_wq,\n\t\t\t\t &ce->guc_state.sched_disable_delay_work,\n\t\t\t\t msecs_to_jiffies(delay));\n\t} else {\n\t\tdo_sched_disable(guc, ce, flags);\n\t}\n}\n\nstatic void guc_context_close(struct intel_context *ce)\n{\n\tunsigned long flags;\n\n\tif (test_bit(CONTEXT_GUC_INIT, &ce->flags) &&\n\t    cancel_delayed_work(&ce->guc_state.sched_disable_delay_work))\n\t\t__delay_sched_disable(&ce->guc_state.sched_disable_delay_work.work);\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tset_context_close_done(ce);\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n}\n\nstatic inline void guc_lrc_desc_unpin(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tunsigned long flags;\n\tbool disabled;\n\n\tGEM_BUG_ON(!intel_gt_pm_is_awake(gt));\n\tGEM_BUG_ON(!ctx_id_mapped(guc, ce->guc_id.id));\n\tGEM_BUG_ON(ce != __get_context(guc, ce->guc_id.id));\n\tGEM_BUG_ON(context_enabled(ce));\n\n\t \n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tdisabled = submission_disabled(guc);\n\tif (likely(!disabled)) {\n\t\t__intel_gt_pm_get(gt);\n\t\tset_context_destroyed(ce);\n\t\tclr_context_registered(ce);\n\t}\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\tif (unlikely(disabled)) {\n\t\trelease_guc_id(guc, ce);\n\t\t__guc_context_destroy(ce);\n\t\treturn;\n\t}\n\n\tderegister_context(ce, ce->guc_id.id);\n}\n\nstatic void __guc_context_destroy(struct intel_context *ce)\n{\n\tGEM_BUG_ON(ce->guc_state.prio_count[GUC_CLIENT_PRIORITY_KMD_HIGH] ||\n\t\t   ce->guc_state.prio_count[GUC_CLIENT_PRIORITY_HIGH] ||\n\t\t   ce->guc_state.prio_count[GUC_CLIENT_PRIORITY_KMD_NORMAL] ||\n\t\t   ce->guc_state.prio_count[GUC_CLIENT_PRIORITY_NORMAL]);\n\n\tlrc_fini(ce);\n\tintel_context_fini(ce);\n\n\tif (intel_engine_is_virtual(ce->engine)) {\n\t\tstruct guc_virtual_engine *ve =\n\t\t\tcontainer_of(ce, typeof(*ve), context);\n\n\t\tif (ve->base.breadcrumbs)\n\t\t\tintel_breadcrumbs_put(ve->base.breadcrumbs);\n\n\t\tkfree(ve);\n\t} else {\n\t\tintel_context_free(ce);\n\t}\n}\n\nstatic void guc_flush_destroyed_contexts(struct intel_guc *guc)\n{\n\tstruct intel_context *ce;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(!submission_disabled(guc) &&\n\t\t   guc_submission_initialized(guc));\n\n\twhile (!list_empty(&guc->submission_state.destroyed_contexts)) {\n\t\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\t\tce = list_first_entry_or_null(&guc->submission_state.destroyed_contexts,\n\t\t\t\t\t      struct intel_context,\n\t\t\t\t\t      destroyed_link);\n\t\tif (ce)\n\t\t\tlist_del_init(&ce->destroyed_link);\n\t\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\n\t\tif (!ce)\n\t\t\tbreak;\n\n\t\trelease_guc_id(guc, ce);\n\t\t__guc_context_destroy(ce);\n\t}\n}\n\nstatic void deregister_destroyed_contexts(struct intel_guc *guc)\n{\n\tstruct intel_context *ce;\n\tunsigned long flags;\n\n\twhile (!list_empty(&guc->submission_state.destroyed_contexts)) {\n\t\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\t\tce = list_first_entry_or_null(&guc->submission_state.destroyed_contexts,\n\t\t\t\t\t      struct intel_context,\n\t\t\t\t\t      destroyed_link);\n\t\tif (ce)\n\t\t\tlist_del_init(&ce->destroyed_link);\n\t\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\n\t\tif (!ce)\n\t\t\tbreak;\n\n\t\tguc_lrc_desc_unpin(ce);\n\t}\n}\n\nstatic void destroyed_worker_func(struct work_struct *w)\n{\n\tstruct intel_guc *guc = container_of(w, struct intel_guc,\n\t\t\t\t\t     submission_state.destroyed_worker);\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tint tmp;\n\n\twith_intel_gt_pm(gt, tmp)\n\t\tderegister_destroyed_contexts(guc);\n}\n\nstatic void guc_context_destroy(struct kref *kref)\n{\n\tstruct intel_context *ce = container_of(kref, typeof(*ce), ref);\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\tbool destroy;\n\n\t \n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\tdestroy = submission_disabled(guc) || context_guc_id_invalid(ce) ||\n\t\t!ctx_id_mapped(guc, ce->guc_id.id);\n\tif (likely(!destroy)) {\n\t\tif (!list_empty(&ce->guc_id.link))\n\t\t\tlist_del_init(&ce->guc_id.link);\n\t\tlist_add_tail(&ce->destroyed_link,\n\t\t\t      &guc->submission_state.destroyed_contexts);\n\t} else {\n\t\t__release_guc_id(guc, ce);\n\t}\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\tif (unlikely(destroy)) {\n\t\t__guc_context_destroy(ce);\n\t\treturn;\n\t}\n\n\t \n\tqueue_work(system_unbound_wq, &guc->submission_state.destroyed_worker);\n}\n\nstatic int guc_context_alloc(struct intel_context *ce)\n{\n\treturn lrc_alloc(ce, ce->engine);\n}\n\nstatic void __guc_context_set_prio(struct intel_guc *guc,\n\t\t\t\t   struct intel_context *ce)\n{\n\tif (GUC_SUBMIT_VER(guc) >= MAKE_GUC_VER(1, 0, 0)) {\n\t\tstruct context_policy policy;\n\n\t\t__guc_context_policy_start_klv(&policy, ce->guc_id.id);\n\t\t__guc_context_policy_add_priority(&policy, ce->guc_state.prio);\n\t\t__guc_context_set_context_policies(guc, &policy, true);\n\t} else {\n\t\tu32 action[] = {\n\t\t\tINTEL_GUC_ACTION_V69_SET_CONTEXT_PRIORITY,\n\t\t\tce->guc_id.id,\n\t\t\tce->guc_state.prio,\n\t\t};\n\n\t\tguc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action), 0, true);\n\t}\n}\n\nstatic void guc_context_set_prio(struct intel_guc *guc,\n\t\t\t\t struct intel_context *ce,\n\t\t\t\t u8 prio)\n{\n\tGEM_BUG_ON(prio < GUC_CLIENT_PRIORITY_KMD_HIGH ||\n\t\t   prio > GUC_CLIENT_PRIORITY_NORMAL);\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tif (ce->guc_state.prio == prio || submission_disabled(guc) ||\n\t    !context_registered(ce)) {\n\t\tce->guc_state.prio = prio;\n\t\treturn;\n\t}\n\n\tce->guc_state.prio = prio;\n\t__guc_context_set_prio(guc, ce);\n\n\ttrace_intel_context_set_prio(ce);\n}\n\nstatic inline u8 map_i915_prio_to_guc_prio(int prio)\n{\n\tif (prio == I915_PRIORITY_NORMAL)\n\t\treturn GUC_CLIENT_PRIORITY_KMD_NORMAL;\n\telse if (prio < I915_PRIORITY_NORMAL)\n\t\treturn GUC_CLIENT_PRIORITY_NORMAL;\n\telse if (prio < I915_PRIORITY_DISPLAY)\n\t\treturn GUC_CLIENT_PRIORITY_HIGH;\n\telse\n\t\treturn GUC_CLIENT_PRIORITY_KMD_HIGH;\n}\n\nstatic inline void add_context_inflight_prio(struct intel_context *ce,\n\t\t\t\t\t     u8 guc_prio)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tGEM_BUG_ON(guc_prio >= ARRAY_SIZE(ce->guc_state.prio_count));\n\n\t++ce->guc_state.prio_count[guc_prio];\n\n\t \n\tGEM_WARN_ON(!ce->guc_state.prio_count[guc_prio]);\n}\n\nstatic inline void sub_context_inflight_prio(struct intel_context *ce,\n\t\t\t\t\t     u8 guc_prio)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\tGEM_BUG_ON(guc_prio >= ARRAY_SIZE(ce->guc_state.prio_count));\n\n\t \n\tGEM_WARN_ON(!ce->guc_state.prio_count[guc_prio]);\n\n\t--ce->guc_state.prio_count[guc_prio];\n}\n\nstatic inline void update_context_prio(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = &ce->engine->gt->uc.guc;\n\tint i;\n\n\tBUILD_BUG_ON(GUC_CLIENT_PRIORITY_KMD_HIGH != 0);\n\tBUILD_BUG_ON(GUC_CLIENT_PRIORITY_KMD_HIGH > GUC_CLIENT_PRIORITY_NORMAL);\n\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tfor (i = 0; i < ARRAY_SIZE(ce->guc_state.prio_count); ++i) {\n\t\tif (ce->guc_state.prio_count[i]) {\n\t\t\tguc_context_set_prio(guc, ce, i);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic inline bool new_guc_prio_higher(u8 old_guc_prio, u8 new_guc_prio)\n{\n\t \n\treturn new_guc_prio < old_guc_prio;\n}\n\nstatic void add_to_context(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tu8 new_guc_prio = map_i915_prio_to_guc_prio(rq_prio(rq));\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\tGEM_BUG_ON(rq->guc_prio == GUC_PRIO_FINI);\n\n\tspin_lock(&ce->guc_state.lock);\n\tlist_move_tail(&rq->sched.link, &ce->guc_state.requests);\n\n\tif (rq->guc_prio == GUC_PRIO_INIT) {\n\t\trq->guc_prio = new_guc_prio;\n\t\tadd_context_inflight_prio(ce, rq->guc_prio);\n\t} else if (new_guc_prio_higher(rq->guc_prio, new_guc_prio)) {\n\t\tsub_context_inflight_prio(ce, rq->guc_prio);\n\t\trq->guc_prio = new_guc_prio;\n\t\tadd_context_inflight_prio(ce, rq->guc_prio);\n\t}\n\tupdate_context_prio(ce);\n\n\tspin_unlock(&ce->guc_state.lock);\n}\n\nstatic void guc_prio_fini(struct i915_request *rq, struct intel_context *ce)\n{\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tif (rq->guc_prio != GUC_PRIO_INIT &&\n\t    rq->guc_prio != GUC_PRIO_FINI) {\n\t\tsub_context_inflight_prio(ce, rq->guc_prio);\n\t\tupdate_context_prio(ce);\n\t}\n\trq->guc_prio = GUC_PRIO_FINI;\n}\n\nstatic void remove_from_context(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tspin_lock_irq(&ce->guc_state.lock);\n\n\tlist_del_init(&rq->sched.link);\n\tclear_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\n\t \n\tset_bit(I915_FENCE_FLAG_ACTIVE, &rq->fence.flags);\n\n\tguc_prio_fini(rq, ce);\n\n\tspin_unlock_irq(&ce->guc_state.lock);\n\n\tatomic_dec(&ce->guc_id.ref);\n\ti915_request_notify_execute_cb_imm(rq);\n}\n\nstatic const struct intel_context_ops guc_context_ops = {\n\t.flags = COPS_RUNTIME_CYCLES,\n\t.alloc = guc_context_alloc,\n\n\t.close = guc_context_close,\n\n\t.pre_pin = guc_context_pre_pin,\n\t.pin = guc_context_pin,\n\t.unpin = guc_context_unpin,\n\t.post_unpin = guc_context_post_unpin,\n\n\t.revoke = guc_context_revoke,\n\n\t.cancel_request = guc_context_cancel_request,\n\n\t.enter = intel_context_enter_engine,\n\t.exit = intel_context_exit_engine,\n\n\t.sched_disable = guc_context_sched_disable,\n\n\t.update_stats = guc_context_update_stats,\n\n\t.reset = lrc_reset,\n\t.destroy = guc_context_destroy,\n\n\t.create_virtual = guc_create_virtual,\n\t.create_parallel = guc_create_parallel,\n};\n\nstatic void submit_work_cb(struct irq_work *wrk)\n{\n\tstruct i915_request *rq = container_of(wrk, typeof(*rq), submit_work);\n\n\tmight_lock(&rq->engine->sched_engine->lock);\n\ti915_sw_fence_complete(&rq->submit);\n}\n\nstatic void __guc_signal_context_fence(struct intel_context *ce)\n{\n\tstruct i915_request *rq, *rn;\n\n\tlockdep_assert_held(&ce->guc_state.lock);\n\n\tif (!list_empty(&ce->guc_state.fences))\n\t\ttrace_intel_context_fence_release(ce);\n\n\t \n\tlist_for_each_entry_safe(rq, rn, &ce->guc_state.fences,\n\t\t\t\t guc_fence_link) {\n\t\tlist_del(&rq->guc_fence_link);\n\t\tirq_work_queue(&rq->submit_work);\n\t}\n\n\tINIT_LIST_HEAD(&ce->guc_state.fences);\n}\n\nstatic void guc_signal_context_fence(struct intel_context *ce)\n{\n\tunsigned long flags;\n\n\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tclr_context_wait_for_deregister_to_register(ce);\n\t__guc_signal_context_fence(ce);\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n}\n\nstatic bool context_needs_register(struct intel_context *ce, bool new_guc_id)\n{\n\treturn (new_guc_id || test_bit(CONTEXT_LRCA_DIRTY, &ce->flags) ||\n\t\t!ctx_id_mapped(ce_to_guc(ce), ce->guc_id.id)) &&\n\t\t!submission_disabled(ce_to_guc(ce));\n}\n\nstatic void guc_context_init(struct intel_context *ce)\n{\n\tconst struct i915_gem_context *ctx;\n\tint prio = I915_CONTEXT_DEFAULT_PRIORITY;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(ce->gem_context);\n\tif (ctx)\n\t\tprio = ctx->sched.priority;\n\trcu_read_unlock();\n\n\tce->guc_state.prio = map_i915_prio_to_guc_prio(prio);\n\n\tINIT_DELAYED_WORK(&ce->guc_state.sched_disable_delay_work,\n\t\t\t  __delay_sched_disable);\n\n\tset_bit(CONTEXT_GUC_INIT, &ce->flags);\n}\n\nstatic int guc_request_alloc(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tunsigned long flags;\n\tint ret;\n\n\tGEM_BUG_ON(!intel_context_is_pinned(rq->context));\n\n\t \n\trq->reserved_space += GUC_REQUEST_SIZE;\n\n\t \n\n\t \n\tret = rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n\tif (ret)\n\t\treturn ret;\n\n\trq->reserved_space -= GUC_REQUEST_SIZE;\n\n\tif (unlikely(!test_bit(CONTEXT_GUC_INIT, &ce->flags)))\n\t\tguc_context_init(ce);\n\n\t \n\tif (cancel_delayed_work_sync(&ce->guc_state.sched_disable_delay_work))\n\t\tintel_context_sched_disable_unpin(ce);\n\telse if (intel_context_is_closed(ce))\n\t\tif (wait_for(context_close_done(ce), 1500))\n\t\t\tguc_warn(guc, \"timed out waiting on context sched close before realloc\\n\");\n\t \n\tif (atomic_add_unless(&ce->guc_id.ref, 1, 0))\n\t\tgoto out;\n\n\tret = pin_guc_id(guc, ce);\t \n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\tif (context_needs_register(ce, !!ret)) {\n\t\tret = try_context_registration(ce, true);\n\t\tif (unlikely(ret)) {\t \n\t\t\tif (ret == -EPIPE) {\n\t\t\t\tdisable_submission(guc);\n\t\t\t\tgoto out;\t \n\t\t\t}\n\t\t\tatomic_dec(&ce->guc_id.ref);\n\t\t\tunpin_guc_id(guc, ce);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tclear_bit(CONTEXT_LRCA_DIRTY, &ce->flags);\n\nout:\n\t \n\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\tif (context_wait_for_deregister_to_register(ce) ||\n\t    context_pending_disable(ce)) {\n\t\tinit_irq_work(&rq->submit_work, submit_work_cb);\n\t\ti915_sw_fence_await(&rq->submit);\n\n\t\tlist_add_tail(&rq->guc_fence_link, &ce->guc_state.fences);\n\t}\n\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\treturn 0;\n}\n\nstatic int guc_virtual_context_pre_pin(struct intel_context *ce,\n\t\t\t\t       struct i915_gem_ww_ctx *ww,\n\t\t\t\t       void **vaddr)\n{\n\tstruct intel_engine_cs *engine = guc_virtual_get_sibling(ce->engine, 0);\n\n\treturn __guc_context_pre_pin(ce, engine, ww, vaddr);\n}\n\nstatic int guc_virtual_context_pin(struct intel_context *ce, void *vaddr)\n{\n\tstruct intel_engine_cs *engine = guc_virtual_get_sibling(ce->engine, 0);\n\tint ret = __guc_context_pin(ce, engine, vaddr);\n\tintel_engine_mask_t tmp, mask = ce->engine->mask;\n\n\tif (likely(!ret))\n\t\tfor_each_engine_masked(engine, ce->engine->gt, mask, tmp)\n\t\t\tintel_engine_pm_get(engine);\n\n\treturn ret;\n}\n\nstatic void guc_virtual_context_unpin(struct intel_context *ce)\n{\n\tintel_engine_mask_t tmp, mask = ce->engine->mask;\n\tstruct intel_engine_cs *engine;\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\n\tGEM_BUG_ON(context_enabled(ce));\n\tGEM_BUG_ON(intel_context_is_barrier(ce));\n\n\tunpin_guc_id(guc, ce);\n\tlrc_unpin(ce);\n\n\tfor_each_engine_masked(engine, ce->engine->gt, mask, tmp)\n\t\tintel_engine_pm_put_async(engine);\n}\n\nstatic void guc_virtual_context_enter(struct intel_context *ce)\n{\n\tintel_engine_mask_t tmp, mask = ce->engine->mask;\n\tstruct intel_engine_cs *engine;\n\n\tfor_each_engine_masked(engine, ce->engine->gt, mask, tmp)\n\t\tintel_engine_pm_get(engine);\n\n\tintel_timeline_enter(ce->timeline);\n}\n\nstatic void guc_virtual_context_exit(struct intel_context *ce)\n{\n\tintel_engine_mask_t tmp, mask = ce->engine->mask;\n\tstruct intel_engine_cs *engine;\n\n\tfor_each_engine_masked(engine, ce->engine->gt, mask, tmp)\n\t\tintel_engine_pm_put(engine);\n\n\tintel_timeline_exit(ce->timeline);\n}\n\nstatic int guc_virtual_context_alloc(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = guc_virtual_get_sibling(ce->engine, 0);\n\n\treturn lrc_alloc(ce, engine);\n}\n\nstatic const struct intel_context_ops virtual_guc_context_ops = {\n\t.flags = COPS_RUNTIME_CYCLES,\n\t.alloc = guc_virtual_context_alloc,\n\n\t.close = guc_context_close,\n\n\t.pre_pin = guc_virtual_context_pre_pin,\n\t.pin = guc_virtual_context_pin,\n\t.unpin = guc_virtual_context_unpin,\n\t.post_unpin = guc_context_post_unpin,\n\n\t.revoke = guc_context_revoke,\n\n\t.cancel_request = guc_context_cancel_request,\n\n\t.enter = guc_virtual_context_enter,\n\t.exit = guc_virtual_context_exit,\n\n\t.sched_disable = guc_context_sched_disable,\n\t.update_stats = guc_context_update_stats,\n\n\t.destroy = guc_context_destroy,\n\n\t.get_sibling = guc_virtual_get_sibling,\n};\n\nstatic int guc_parent_context_pin(struct intel_context *ce, void *vaddr)\n{\n\tstruct intel_engine_cs *engine = guc_virtual_get_sibling(ce->engine, 0);\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\tint ret;\n\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\tGEM_BUG_ON(!intel_engine_is_virtual(ce->engine));\n\n\tret = pin_guc_id(guc, ce);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\treturn __guc_context_pin(ce, engine, vaddr);\n}\n\nstatic int guc_child_context_pin(struct intel_context *ce, void *vaddr)\n{\n\tstruct intel_engine_cs *engine = guc_virtual_get_sibling(ce->engine, 0);\n\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\tGEM_BUG_ON(!intel_engine_is_virtual(ce->engine));\n\n\t__intel_context_pin(ce->parallel.parent);\n\treturn __guc_context_pin(ce, engine, vaddr);\n}\n\nstatic void guc_parent_context_unpin(struct intel_context *ce)\n{\n\tstruct intel_guc *guc = ce_to_guc(ce);\n\n\tGEM_BUG_ON(context_enabled(ce));\n\tGEM_BUG_ON(intel_context_is_barrier(ce));\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\tGEM_BUG_ON(!intel_engine_is_virtual(ce->engine));\n\n\tunpin_guc_id(guc, ce);\n\tlrc_unpin(ce);\n}\n\nstatic void guc_child_context_unpin(struct intel_context *ce)\n{\n\tGEM_BUG_ON(context_enabled(ce));\n\tGEM_BUG_ON(intel_context_is_barrier(ce));\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\tGEM_BUG_ON(!intel_engine_is_virtual(ce->engine));\n\n\tlrc_unpin(ce);\n}\n\nstatic void guc_child_context_post_unpin(struct intel_context *ce)\n{\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\tGEM_BUG_ON(!intel_context_is_pinned(ce->parallel.parent));\n\tGEM_BUG_ON(!intel_engine_is_virtual(ce->engine));\n\n\tlrc_post_unpin(ce);\n\tintel_context_unpin(ce->parallel.parent);\n}\n\nstatic void guc_child_context_destroy(struct kref *kref)\n{\n\tstruct intel_context *ce = container_of(kref, typeof(*ce), ref);\n\n\t__guc_context_destroy(ce);\n}\n\nstatic const struct intel_context_ops virtual_parent_context_ops = {\n\t.alloc = guc_virtual_context_alloc,\n\n\t.close = guc_context_close,\n\n\t.pre_pin = guc_context_pre_pin,\n\t.pin = guc_parent_context_pin,\n\t.unpin = guc_parent_context_unpin,\n\t.post_unpin = guc_context_post_unpin,\n\n\t.revoke = guc_context_revoke,\n\n\t.cancel_request = guc_context_cancel_request,\n\n\t.enter = guc_virtual_context_enter,\n\t.exit = guc_virtual_context_exit,\n\n\t.sched_disable = guc_context_sched_disable,\n\n\t.destroy = guc_context_destroy,\n\n\t.get_sibling = guc_virtual_get_sibling,\n};\n\nstatic const struct intel_context_ops virtual_child_context_ops = {\n\t.alloc = guc_virtual_context_alloc,\n\n\t.pre_pin = guc_context_pre_pin,\n\t.pin = guc_child_context_pin,\n\t.unpin = guc_child_context_unpin,\n\t.post_unpin = guc_child_context_post_unpin,\n\n\t.cancel_request = guc_context_cancel_request,\n\n\t.enter = guc_virtual_context_enter,\n\t.exit = guc_virtual_context_exit,\n\n\t.destroy = guc_child_context_destroy,\n\n\t.get_sibling = guc_virtual_get_sibling,\n};\n\n \nstatic int emit_bb_start_parent_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t     u64 offset, u32 len,\n\t\t\t\t\t\t     const unsigned int flags);\nstatic int emit_bb_start_child_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t    u64 offset, u32 len,\n\t\t\t\t\t\t    const unsigned int flags);\nstatic u32 *\nemit_fini_breadcrumb_parent_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t u32 *cs);\nstatic u32 *\nemit_fini_breadcrumb_child_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\tu32 *cs);\n\nstatic struct intel_context *\nguc_create_parallel(struct intel_engine_cs **engines,\n\t\t    unsigned int num_siblings,\n\t\t    unsigned int width)\n{\n\tstruct intel_engine_cs **siblings = NULL;\n\tstruct intel_context *parent = NULL, *ce, *err;\n\tint i, j;\n\n\tsiblings = kmalloc_array(num_siblings,\n\t\t\t\t sizeof(*siblings),\n\t\t\t\t GFP_KERNEL);\n\tif (!siblings)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tfor (i = 0; i < width; ++i) {\n\t\tfor (j = 0; j < num_siblings; ++j)\n\t\t\tsiblings[j] = engines[i * num_siblings + j];\n\n\t\tce = intel_engine_create_virtual(siblings, num_siblings,\n\t\t\t\t\t\t FORCE_VIRTUAL);\n\t\tif (IS_ERR(ce)) {\n\t\t\terr = ERR_CAST(ce);\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tif (i == 0) {\n\t\t\tparent = ce;\n\t\t\tparent->ops = &virtual_parent_context_ops;\n\t\t} else {\n\t\t\tce->ops = &virtual_child_context_ops;\n\t\t\tintel_context_bind_parent_child(parent, ce);\n\t\t}\n\t}\n\n\tparent->parallel.fence_context = dma_fence_context_alloc(1);\n\n\tparent->engine->emit_bb_start =\n\t\temit_bb_start_parent_no_preempt_mid_batch;\n\tparent->engine->emit_fini_breadcrumb =\n\t\temit_fini_breadcrumb_parent_no_preempt_mid_batch;\n\tparent->engine->emit_fini_breadcrumb_dw =\n\t\t12 + 4 * parent->parallel.number_children;\n\tfor_each_child(parent, ce) {\n\t\tce->engine->emit_bb_start =\n\t\t\temit_bb_start_child_no_preempt_mid_batch;\n\t\tce->engine->emit_fini_breadcrumb =\n\t\t\temit_fini_breadcrumb_child_no_preempt_mid_batch;\n\t\tce->engine->emit_fini_breadcrumb_dw = 16;\n\t}\n\n\tkfree(siblings);\n\treturn parent;\n\nunwind:\n\tif (parent)\n\t\tintel_context_put(parent);\n\tkfree(siblings);\n\treturn err;\n}\n\nstatic bool\nguc_irq_enable_breadcrumbs(struct intel_breadcrumbs *b)\n{\n\tstruct intel_engine_cs *sibling;\n\tintel_engine_mask_t tmp, mask = b->engine_mask;\n\tbool result = false;\n\n\tfor_each_engine_masked(sibling, b->irq_engine->gt, mask, tmp)\n\t\tresult |= intel_engine_irq_enable(sibling);\n\n\treturn result;\n}\n\nstatic void\nguc_irq_disable_breadcrumbs(struct intel_breadcrumbs *b)\n{\n\tstruct intel_engine_cs *sibling;\n\tintel_engine_mask_t tmp, mask = b->engine_mask;\n\n\tfor_each_engine_masked(sibling, b->irq_engine->gt, mask, tmp)\n\t\tintel_engine_irq_disable(sibling);\n}\n\nstatic void guc_init_breadcrumbs(struct intel_engine_cs *engine)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < MAX_ENGINE_INSTANCE; ++i) {\n\t\tstruct intel_engine_cs *sibling =\n\t\t\tengine->gt->engine_class[engine->class][i];\n\n\t\tif (sibling) {\n\t\t\tif (engine->breadcrumbs != sibling->breadcrumbs) {\n\t\t\t\tintel_breadcrumbs_put(engine->breadcrumbs);\n\t\t\t\tengine->breadcrumbs =\n\t\t\t\t\tintel_breadcrumbs_get(sibling->breadcrumbs);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (engine->breadcrumbs) {\n\t\tengine->breadcrumbs->engine_mask |= engine->mask;\n\t\tengine->breadcrumbs->irq_enable = guc_irq_enable_breadcrumbs;\n\t\tengine->breadcrumbs->irq_disable = guc_irq_disable_breadcrumbs;\n\t}\n}\n\nstatic void guc_bump_inflight_request_prio(struct i915_request *rq,\n\t\t\t\t\t   int prio)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\tu8 new_guc_prio = map_i915_prio_to_guc_prio(prio);\n\n\t \n\tif (prio < I915_PRIORITY_NORMAL ||\n\t    rq->guc_prio == GUC_PRIO_FINI ||\n\t    (rq->guc_prio != GUC_PRIO_INIT &&\n\t     !new_guc_prio_higher(rq->guc_prio, new_guc_prio)))\n\t\treturn;\n\n\tspin_lock(&ce->guc_state.lock);\n\tif (rq->guc_prio != GUC_PRIO_FINI) {\n\t\tif (rq->guc_prio != GUC_PRIO_INIT)\n\t\t\tsub_context_inflight_prio(ce, rq->guc_prio);\n\t\trq->guc_prio = new_guc_prio;\n\t\tadd_context_inflight_prio(ce, rq->guc_prio);\n\t\tupdate_context_prio(ce);\n\t}\n\tspin_unlock(&ce->guc_state.lock);\n}\n\nstatic void guc_retire_inflight_request_prio(struct i915_request *rq)\n{\n\tstruct intel_context *ce = request_to_scheduling_context(rq);\n\n\tspin_lock(&ce->guc_state.lock);\n\tguc_prio_fini(rq, ce);\n\tspin_unlock(&ce->guc_state.lock);\n}\n\nstatic void sanitize_hwsp(struct intel_engine_cs *engine)\n{\n\tstruct intel_timeline *tl;\n\n\tlist_for_each_entry(tl, &engine->status_page.timelines, engine_link)\n\t\tintel_timeline_reset_seqno(tl);\n}\n\nstatic void guc_sanitize(struct intel_engine_cs *engine)\n{\n\t \n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tmemset(engine->status_page.addr, POISON_INUSE, PAGE_SIZE);\n\n\t \n\tsanitize_hwsp(engine);\n\n\t \n\tdrm_clflush_virt_range(engine->status_page.addr, PAGE_SIZE);\n\n\tintel_engine_reset_pinned_contexts(engine);\n}\n\nstatic void setup_hwsp(struct intel_engine_cs *engine)\n{\n\tintel_engine_set_hwsp_writemask(engine, ~0u);  \n\n\tENGINE_WRITE_FW(engine,\n\t\t\tRING_HWS_PGA,\n\t\t\ti915_ggtt_offset(engine->status_page.vma));\n}\n\nstatic void start_engine(struct intel_engine_cs *engine)\n{\n\tENGINE_WRITE_FW(engine,\n\t\t\tRING_MODE_GEN7,\n\t\t\t_MASKED_BIT_ENABLE(GEN11_GFX_DISABLE_LEGACY_MODE));\n\n\tENGINE_WRITE_FW(engine, RING_MI_MODE, _MASKED_BIT_DISABLE(STOP_RING));\n\tENGINE_POSTING_READ(engine, RING_MI_MODE);\n}\n\nstatic int guc_resume(struct intel_engine_cs *engine)\n{\n\tassert_forcewakes_active(engine->uncore, FORCEWAKE_ALL);\n\n\tintel_mocs_init_engine(engine);\n\n\tintel_breadcrumbs_reset(engine->breadcrumbs);\n\n\tsetup_hwsp(engine);\n\tstart_engine(engine);\n\n\tif (engine->flags & I915_ENGINE_FIRST_RENDER_COMPUTE)\n\t\txehp_enable_ccs_engines(engine);\n\n\treturn 0;\n}\n\nstatic bool guc_sched_engine_disabled(struct i915_sched_engine *sched_engine)\n{\n\treturn !sched_engine->tasklet.callback;\n}\n\nstatic void guc_set_default_submission(struct intel_engine_cs *engine)\n{\n\tengine->submit_request = guc_submit_request;\n}\n\nstatic inline int guc_kernel_context_pin(struct intel_guc *guc,\n\t\t\t\t\t struct intel_context *ce)\n{\n\tint ret;\n\n\t \n\n\tif (context_guc_id_invalid(ce)) {\n\t\tret = pin_guc_id(guc, ce);\n\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tif (!test_bit(CONTEXT_GUC_INIT, &ce->flags))\n\t\tguc_context_init(ce);\n\n\tret = try_context_registration(ce, true);\n\tif (ret)\n\t\tunpin_guc_id(guc, ce);\n\n\treturn ret;\n}\n\nstatic inline int guc_init_submission(struct intel_guc *guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\t \n\txa_destroy(&guc->context_lookup);\n\n\t \n\tguc->stalled_request = NULL;\n\tguc->submission_stall_reason = STALL_NONE;\n\n\t \n\tfor_each_engine(engine, gt, id) {\n\t\tstruct intel_context *ce;\n\n\t\tlist_for_each_entry(ce, &engine->pinned_contexts_list,\n\t\t\t\t    pinned_contexts_link) {\n\t\t\tint ret = guc_kernel_context_pin(guc, ce);\n\n\t\t\tif (ret) {\n\t\t\t\t \n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void guc_release(struct intel_engine_cs *engine)\n{\n\tengine->sanitize = NULL;  \n\n\tintel_engine_cleanup_common(engine);\n\tlrc_fini_wa_ctx(engine);\n}\n\nstatic void virtual_guc_bump_serial(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_cs *e;\n\tintel_engine_mask_t tmp, mask = engine->mask;\n\n\tfor_each_engine_masked(e, engine->gt, mask, tmp)\n\t\te->serial++;\n}\n\nstatic void guc_default_vfuncs(struct intel_engine_cs *engine)\n{\n\t \n\n\tengine->resume = guc_resume;\n\n\tengine->cops = &guc_context_ops;\n\tengine->request_alloc = guc_request_alloc;\n\tengine->add_active_request = add_to_context;\n\tengine->remove_active_request = remove_from_context;\n\n\tengine->sched_engine->schedule = i915_schedule;\n\n\tengine->reset.prepare = guc_engine_reset_prepare;\n\tengine->reset.rewind = guc_rewind_nop;\n\tengine->reset.cancel = guc_reset_nop;\n\tengine->reset.finish = guc_reset_nop;\n\n\tengine->emit_flush = gen8_emit_flush_xcs;\n\tengine->emit_init_breadcrumb = gen8_emit_init_breadcrumb;\n\tengine->emit_fini_breadcrumb = gen8_emit_fini_breadcrumb_xcs;\n\tif (GRAPHICS_VER(engine->i915) >= 12) {\n\t\tengine->emit_fini_breadcrumb = gen12_emit_fini_breadcrumb_xcs;\n\t\tengine->emit_flush = gen12_emit_flush_xcs;\n\t}\n\tengine->set_default_submission = guc_set_default_submission;\n\tengine->busyness = guc_engine_busyness;\n\n\tengine->flags |= I915_ENGINE_SUPPORTS_STATS;\n\tengine->flags |= I915_ENGINE_HAS_PREEMPTION;\n\tengine->flags |= I915_ENGINE_HAS_TIMESLICES;\n\n\t \n\tif (engine->class == COMPUTE_CLASS)\n\t\tif (IS_MTL_GRAPHICS_STEP(engine->i915, M, STEP_A0, STEP_B0) ||\n\t\t    IS_DG2(engine->i915))\n\t\t\tengine->flags |= I915_ENGINE_USES_WA_HOLD_CCS_SWITCHOUT;\n\n\t \n\n\tengine->emit_bb_start = gen8_emit_bb_start;\n\tif (GRAPHICS_VER_FULL(engine->i915) >= IP_VER(12, 50))\n\t\tengine->emit_bb_start = xehp_emit_bb_start;\n}\n\nstatic void rcs_submission_override(struct intel_engine_cs *engine)\n{\n\tswitch (GRAPHICS_VER(engine->i915)) {\n\tcase 12:\n\t\tengine->emit_flush = gen12_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen12_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\tcase 11:\n\t\tengine->emit_flush = gen11_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen11_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\tdefault:\n\t\tengine->emit_flush = gen8_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen8_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\t}\n}\n\nstatic inline void guc_default_irqs(struct intel_engine_cs *engine)\n{\n\tengine->irq_keep_mask = GT_RENDER_USER_INTERRUPT;\n\tintel_engine_set_irq_handler(engine, cs_irq_handler);\n}\n\nstatic void guc_sched_engine_destroy(struct kref *kref)\n{\n\tstruct i915_sched_engine *sched_engine =\n\t\tcontainer_of(kref, typeof(*sched_engine), ref);\n\tstruct intel_guc *guc = sched_engine->private_data;\n\n\tguc->sched_engine = NULL;\n\ttasklet_kill(&sched_engine->tasklet);  \n\tkfree(sched_engine);\n}\n\nint intel_guc_submission_setup(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\n\t \n\tGEM_BUG_ON(GRAPHICS_VER(i915) < 11);\n\n\tif (!guc->sched_engine) {\n\t\tguc->sched_engine = i915_sched_engine_create(ENGINE_VIRTUAL);\n\t\tif (!guc->sched_engine)\n\t\t\treturn -ENOMEM;\n\n\t\tguc->sched_engine->schedule = i915_schedule;\n\t\tguc->sched_engine->disabled = guc_sched_engine_disabled;\n\t\tguc->sched_engine->private_data = guc;\n\t\tguc->sched_engine->destroy = guc_sched_engine_destroy;\n\t\tguc->sched_engine->bump_inflight_request_prio =\n\t\t\tguc_bump_inflight_request_prio;\n\t\tguc->sched_engine->retire_inflight_request_prio =\n\t\t\tguc_retire_inflight_request_prio;\n\t\ttasklet_setup(&guc->sched_engine->tasklet,\n\t\t\t      guc_submission_tasklet);\n\t}\n\ti915_sched_engine_put(engine->sched_engine);\n\tengine->sched_engine = i915_sched_engine_get(guc->sched_engine);\n\n\tguc_default_vfuncs(engine);\n\tguc_default_irqs(engine);\n\tguc_init_breadcrumbs(engine);\n\n\tif (engine->flags & I915_ENGINE_HAS_RCS_REG_STATE)\n\t\trcs_submission_override(engine);\n\n\tlrc_init_wa_ctx(engine);\n\n\t \n\tengine->sanitize = guc_sanitize;\n\tengine->release = guc_release;\n\n\treturn 0;\n}\n\nstruct scheduling_policy {\n\t \n\tu32 max_words, num_words;\n\tu32 count;\n\t \n\tstruct guc_update_scheduling_policy h2g;\n};\n\nstatic u32 __guc_scheduling_policy_action_size(struct scheduling_policy *policy)\n{\n\tu32 *start = (void *)&policy->h2g;\n\tu32 *end = policy->h2g.data + policy->num_words;\n\tsize_t delta = end - start;\n\n\treturn delta;\n}\n\nstatic struct scheduling_policy *__guc_scheduling_policy_start_klv(struct scheduling_policy *policy)\n{\n\tpolicy->h2g.header.action = INTEL_GUC_ACTION_UPDATE_SCHEDULING_POLICIES_KLV;\n\tpolicy->max_words = ARRAY_SIZE(policy->h2g.data);\n\tpolicy->num_words = 0;\n\tpolicy->count = 0;\n\n\treturn policy;\n}\n\nstatic void __guc_scheduling_policy_add_klv(struct scheduling_policy *policy,\n\t\t\t\t\t    u32 action, u32 *data, u32 len)\n{\n\tu32 *klv_ptr = policy->h2g.data + policy->num_words;\n\n\tGEM_BUG_ON((policy->num_words + 1 + len) > policy->max_words);\n\t*(klv_ptr++) = FIELD_PREP(GUC_KLV_0_KEY, action) |\n\t\t       FIELD_PREP(GUC_KLV_0_LEN, len);\n\tmemcpy(klv_ptr, data, sizeof(u32) * len);\n\tpolicy->num_words += 1 + len;\n\tpolicy->count++;\n}\n\nstatic int __guc_action_set_scheduling_policies(struct intel_guc *guc,\n\t\t\t\t\t\tstruct scheduling_policy *policy)\n{\n\tint ret;\n\n\tret = intel_guc_send(guc, (u32 *)&policy->h2g,\n\t\t\t     __guc_scheduling_policy_action_size(policy));\n\tif (ret < 0) {\n\t\tguc_probe_error(guc, \"Failed to configure global scheduling policies: %pe!\\n\",\n\t\t\t\tERR_PTR(ret));\n\t\treturn ret;\n\t}\n\n\tif (ret != policy->count) {\n\t\tguc_warn(guc, \"global scheduler policy processed %d of %d KLVs!\",\n\t\t\t ret, policy->count);\n\t\tif (ret > policy->count)\n\t\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic int guc_init_global_schedule_policy(struct intel_guc *guc)\n{\n\tstruct scheduling_policy policy;\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tintel_wakeref_t wakeref;\n\tint ret;\n\n\tif (GUC_SUBMIT_VER(guc) < MAKE_GUC_VER(1, 1, 0))\n\t\treturn 0;\n\n\t__guc_scheduling_policy_start_klv(&policy);\n\n\twith_intel_runtime_pm(&gt->i915->runtime_pm, wakeref) {\n\t\tu32 yield[] = {\n\t\t\tGLOBAL_SCHEDULE_POLICY_RC_YIELD_DURATION,\n\t\t\tGLOBAL_SCHEDULE_POLICY_RC_YIELD_RATIO,\n\t\t};\n\n\t\t__guc_scheduling_policy_add_klv(&policy,\n\t\t\t\t\t\tGUC_SCHEDULING_POLICIES_KLV_ID_RENDER_COMPUTE_YIELD,\n\t\t\t\t\t\tyield, ARRAY_SIZE(yield));\n\n\t\tret = __guc_action_set_scheduling_policies(guc, &policy);\n\t}\n\n\treturn ret;\n}\n\nstatic void guc_route_semaphores(struct intel_guc *guc, bool to_guc)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tu32 val;\n\n\tif (GRAPHICS_VER(gt->i915) < 12)\n\t\treturn;\n\n\tif (to_guc)\n\t\tval = GUC_SEM_INTR_ROUTE_TO_GUC | GUC_SEM_INTR_ENABLE_ALL;\n\telse\n\t\tval = 0;\n\n\tintel_uncore_write(gt->uncore, GEN12_GUC_SEM_INTR_ENABLES, val);\n}\n\nint intel_guc_submission_enable(struct intel_guc *guc)\n{\n\tint ret;\n\n\t \n\tguc_route_semaphores(guc, true);\n\n\tret = guc_init_submission(guc);\n\tif (ret)\n\t\tgoto fail_sem;\n\n\tret = guc_init_engine_stats(guc);\n\tif (ret)\n\t\tgoto fail_sem;\n\n\tret = guc_init_global_schedule_policy(guc);\n\tif (ret)\n\t\tgoto fail_stats;\n\n\treturn 0;\n\nfail_stats:\n\tguc_fini_engine_stats(guc);\nfail_sem:\n\tguc_route_semaphores(guc, false);\n\treturn ret;\n}\n\n \nvoid intel_guc_submission_disable(struct intel_guc *guc)\n{\n\tguc_cancel_busyness_worker(guc);\n\n\t \n\tguc_route_semaphores(guc, false);\n}\n\nstatic bool __guc_submission_supported(struct intel_guc *guc)\n{\n\t \n\treturn intel_guc_is_supported(guc) &&\n\t       GRAPHICS_VER(guc_to_gt(guc)->i915) >= 11;\n}\n\nstatic bool __guc_submission_selected(struct intel_guc *guc)\n{\n\tstruct drm_i915_private *i915 = guc_to_gt(guc)->i915;\n\n\tif (!intel_guc_submission_is_supported(guc))\n\t\treturn false;\n\n\treturn i915->params.enable_guc & ENABLE_GUC_SUBMISSION;\n}\n\nint intel_guc_sched_disable_gucid_threshold_max(struct intel_guc *guc)\n{\n\treturn guc->submission_state.num_guc_ids - NUMBER_MULTI_LRC_GUC_ID(guc);\n}\n\n \n#define SCHED_DISABLE_DELAY_MS\t34\n\n \n#define NUM_SCHED_DISABLE_GUCIDS_DEFAULT_THRESHOLD(__guc) \\\n\t(((intel_guc_sched_disable_gucid_threshold_max(guc)) * 3) / 4)\n\nvoid intel_guc_submission_init_early(struct intel_guc *guc)\n{\n\txa_init_flags(&guc->context_lookup, XA_FLAGS_LOCK_IRQ);\n\n\tspin_lock_init(&guc->submission_state.lock);\n\tINIT_LIST_HEAD(&guc->submission_state.guc_id_list);\n\tida_init(&guc->submission_state.guc_ids);\n\tINIT_LIST_HEAD(&guc->submission_state.destroyed_contexts);\n\tINIT_WORK(&guc->submission_state.destroyed_worker,\n\t\t  destroyed_worker_func);\n\tINIT_WORK(&guc->submission_state.reset_fail_worker,\n\t\t  reset_fail_worker_func);\n\n\tspin_lock_init(&guc->timestamp.lock);\n\tINIT_DELAYED_WORK(&guc->timestamp.work, guc_timestamp_ping);\n\n\tguc->submission_state.sched_disable_delay_ms = SCHED_DISABLE_DELAY_MS;\n\tguc->submission_state.num_guc_ids = GUC_MAX_CONTEXT_ID;\n\tguc->submission_state.sched_disable_gucid_threshold =\n\t\tNUM_SCHED_DISABLE_GUCIDS_DEFAULT_THRESHOLD(guc);\n\tguc->submission_supported = __guc_submission_supported(guc);\n\tguc->submission_selected = __guc_submission_selected(guc);\n}\n\nstatic inline struct intel_context *\ng2h_context_lookup(struct intel_guc *guc, u32 ctx_id)\n{\n\tstruct intel_context *ce;\n\n\tif (unlikely(ctx_id >= GUC_MAX_CONTEXT_ID)) {\n\t\tguc_err(guc, \"Invalid ctx_id %u\\n\", ctx_id);\n\t\treturn NULL;\n\t}\n\n\tce = __get_context(guc, ctx_id);\n\tif (unlikely(!ce)) {\n\t\tguc_err(guc, \"Context is NULL, ctx_id %u\\n\", ctx_id);\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(intel_context_is_child(ce))) {\n\t\tguc_err(guc, \"Context is child, ctx_id %u\\n\", ctx_id);\n\t\treturn NULL;\n\t}\n\n\treturn ce;\n}\n\nint intel_guc_deregister_done_process_msg(struct intel_guc *guc,\n\t\t\t\t\t  const u32 *msg,\n\t\t\t\t\t  u32 len)\n{\n\tstruct intel_context *ce;\n\tu32 ctx_id;\n\n\tif (unlikely(len < 1)) {\n\t\tguc_err(guc, \"Invalid length %u\\n\", len);\n\t\treturn -EPROTO;\n\t}\n\tctx_id = msg[0];\n\n\tce = g2h_context_lookup(guc, ctx_id);\n\tif (unlikely(!ce))\n\t\treturn -EPROTO;\n\n\ttrace_intel_context_deregister_done(ce);\n\n#ifdef CONFIG_DRM_I915_SELFTEST\n\tif (unlikely(ce->drop_deregister)) {\n\t\tce->drop_deregister = false;\n\t\treturn 0;\n\t}\n#endif\n\n\tif (context_wait_for_deregister_to_register(ce)) {\n\t\tstruct intel_runtime_pm *runtime_pm =\n\t\t\t&ce->engine->gt->i915->runtime_pm;\n\t\tintel_wakeref_t wakeref;\n\n\t\t \n\t\twith_intel_runtime_pm(runtime_pm, wakeref)\n\t\t\tregister_context(ce, true);\n\t\tguc_signal_context_fence(ce);\n\t\tintel_context_put(ce);\n\t} else if (context_destroyed(ce)) {\n\t\t \n\t\tintel_gt_pm_put_async(guc_to_gt(guc));\n\t\trelease_guc_id(guc, ce);\n\t\t__guc_context_destroy(ce);\n\t}\n\n\tdecr_outstanding_submission_g2h(guc);\n\n\treturn 0;\n}\n\nint intel_guc_sched_done_process_msg(struct intel_guc *guc,\n\t\t\t\t     const u32 *msg,\n\t\t\t\t     u32 len)\n{\n\tstruct intel_context *ce;\n\tunsigned long flags;\n\tu32 ctx_id;\n\n\tif (unlikely(len < 2)) {\n\t\tguc_err(guc, \"Invalid length %u\\n\", len);\n\t\treturn -EPROTO;\n\t}\n\tctx_id = msg[0];\n\n\tce = g2h_context_lookup(guc, ctx_id);\n\tif (unlikely(!ce))\n\t\treturn -EPROTO;\n\n\tif (unlikely(context_destroyed(ce) ||\n\t\t     (!context_pending_enable(ce) &&\n\t\t     !context_pending_disable(ce)))) {\n\t\tguc_err(guc, \"Bad context sched_state 0x%x, ctx_id %u\\n\",\n\t\t\tce->guc_state.sched_state, ctx_id);\n\t\treturn -EPROTO;\n\t}\n\n\ttrace_intel_context_sched_done(ce);\n\n\tif (context_pending_enable(ce)) {\n#ifdef CONFIG_DRM_I915_SELFTEST\n\t\tif (unlikely(ce->drop_schedule_enable)) {\n\t\t\tce->drop_schedule_enable = false;\n\t\t\treturn 0;\n\t\t}\n#endif\n\n\t\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\t\tclr_context_pending_enable(ce);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\t} else if (context_pending_disable(ce)) {\n\t\tbool banned;\n\n#ifdef CONFIG_DRM_I915_SELFTEST\n\t\tif (unlikely(ce->drop_schedule_disable)) {\n\t\t\tce->drop_schedule_disable = false;\n\t\t\treturn 0;\n\t\t}\n#endif\n\n\t\t \n\t\tintel_context_sched_disable_unpin(ce);\n\n\t\tspin_lock_irqsave(&ce->guc_state.lock, flags);\n\t\tbanned = context_banned(ce);\n\t\tclr_context_banned(ce);\n\t\tclr_context_pending_disable(ce);\n\t\t__guc_signal_context_fence(ce);\n\t\tguc_blocked_fence_complete(ce);\n\t\tspin_unlock_irqrestore(&ce->guc_state.lock, flags);\n\n\t\tif (banned) {\n\t\t\tguc_cancel_context_requests(ce);\n\t\t\tintel_engine_signal_breadcrumbs(ce->engine);\n\t\t}\n\t}\n\n\tdecr_outstanding_submission_g2h(guc);\n\tintel_context_put(ce);\n\n\treturn 0;\n}\n\nstatic void capture_error_state(struct intel_guc *guc,\n\t\t\t\tstruct intel_context *ce)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tstruct drm_i915_private *i915 = gt->i915;\n\tintel_wakeref_t wakeref;\n\tintel_engine_mask_t engine_mask;\n\n\tif (intel_engine_is_virtual(ce->engine)) {\n\t\tstruct intel_engine_cs *e;\n\t\tintel_engine_mask_t tmp, virtual_mask = ce->engine->mask;\n\n\t\tengine_mask = 0;\n\t\tfor_each_engine_masked(e, ce->engine->gt, virtual_mask, tmp) {\n\t\t\tbool match = intel_guc_capture_is_matching_engine(gt, ce, e);\n\n\t\t\tif (match) {\n\t\t\t\tintel_engine_set_hung_context(e, ce);\n\t\t\t\tengine_mask |= e->mask;\n\t\t\t\ti915_increase_reset_engine_count(&i915->gpu_error,\n\t\t\t\t\t\t\t\t e);\n\t\t\t}\n\t\t}\n\n\t\tif (!engine_mask) {\n\t\t\tguc_warn(guc, \"No matching physical engine capture for virtual engine context 0x%04X / %s\",\n\t\t\t\t ce->guc_id.id, ce->engine->name);\n\t\t\tengine_mask = ~0U;\n\t\t}\n\t} else {\n\t\tintel_engine_set_hung_context(ce->engine, ce);\n\t\tengine_mask = ce->engine->mask;\n\t\ti915_increase_reset_engine_count(&i915->gpu_error, ce->engine);\n\t}\n\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref)\n\t\ti915_capture_error_state(gt, engine_mask, CORE_DUMP_FLAG_IS_GUC_CAPTURE);\n}\n\nstatic void guc_context_replay(struct intel_context *ce)\n{\n\tstruct i915_sched_engine *sched_engine = ce->engine->sched_engine;\n\n\t__guc_reset_context(ce, ce->engine->mask);\n\ttasklet_hi_schedule(&sched_engine->tasklet);\n}\n\nstatic void guc_handle_context_reset(struct intel_guc *guc,\n\t\t\t\t     struct intel_context *ce)\n{\n\ttrace_intel_context_reset(ce);\n\n\tguc_dbg(guc, \"Got context reset notification: 0x%04X on %s, exiting = %s, banned = %s\\n\",\n\t\tce->guc_id.id, ce->engine->name,\n\t\tstr_yes_no(intel_context_is_exiting(ce)),\n\t\tstr_yes_no(intel_context_is_banned(ce)));\n\n\tif (likely(intel_context_is_schedulable(ce))) {\n\t\tcapture_error_state(guc, ce);\n\t\tguc_context_replay(ce);\n\t} else {\n\t\tguc_info(guc, \"Ignoring context reset notification of exiting context 0x%04X on %s\",\n\t\t\t ce->guc_id.id, ce->engine->name);\n\t}\n}\n\nint intel_guc_context_reset_process_msg(struct intel_guc *guc,\n\t\t\t\t\tconst u32 *msg, u32 len)\n{\n\tstruct intel_context *ce;\n\tunsigned long flags;\n\tint ctx_id;\n\n\tif (unlikely(len != 1)) {\n\t\tguc_err(guc, \"Invalid length %u\", len);\n\t\treturn -EPROTO;\n\t}\n\n\tctx_id = msg[0];\n\n\t \n\txa_lock_irqsave(&guc->context_lookup, flags);\n\tce = g2h_context_lookup(guc, ctx_id);\n\tif (ce)\n\t\tintel_context_get(ce);\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n\n\tif (unlikely(!ce))\n\t\treturn -EPROTO;\n\n\tguc_handle_context_reset(guc, ce);\n\tintel_context_put(ce);\n\n\treturn 0;\n}\n\nint intel_guc_error_capture_process_msg(struct intel_guc *guc,\n\t\t\t\t\tconst u32 *msg, u32 len)\n{\n\tu32 status;\n\n\tif (unlikely(len != 1)) {\n\t\tguc_dbg(guc, \"Invalid length %u\", len);\n\t\treturn -EPROTO;\n\t}\n\n\tstatus = msg[0] & INTEL_GUC_STATE_CAPTURE_EVENT_STATUS_MASK;\n\tif (status == INTEL_GUC_STATE_CAPTURE_EVENT_STATUS_NOSPACE)\n\t\tguc_warn(guc, \"No space for error capture\");\n\n\tintel_guc_capture_process(guc);\n\n\treturn 0;\n}\n\nstruct intel_engine_cs *\nintel_guc_lookup_engine(struct intel_guc *guc, u8 guc_class, u8 instance)\n{\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tu8 engine_class = guc_class_to_engine_class(guc_class);\n\n\t \n\tGEM_BUG_ON(instance > MAX_ENGINE_INSTANCE);\n\n\treturn gt->engine_class[engine_class][instance];\n}\n\nstatic void reset_fail_worker_func(struct work_struct *w)\n{\n\tstruct intel_guc *guc = container_of(w, struct intel_guc,\n\t\t\t\t\t     submission_state.reset_fail_worker);\n\tstruct intel_gt *gt = guc_to_gt(guc);\n\tintel_engine_mask_t reset_fail_mask;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\treset_fail_mask = guc->submission_state.reset_fail_mask;\n\tguc->submission_state.reset_fail_mask = 0;\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\n\tif (likely(reset_fail_mask)) {\n\t\tstruct intel_engine_cs *engine;\n\t\tenum intel_engine_id id;\n\n\t\t \n\t\tfor_each_engine_masked(engine, gt, reset_fail_mask, id)\n\t\t\tintel_guc_find_hung_context(engine);\n\n\t\tintel_gt_handle_error(gt, reset_fail_mask,\n\t\t\t\t      I915_ERROR_CAPTURE,\n\t\t\t\t      \"GuC failed to reset engine mask=0x%x\",\n\t\t\t\t      reset_fail_mask);\n\t}\n}\n\nint intel_guc_engine_failure_process_msg(struct intel_guc *guc,\n\t\t\t\t\t const u32 *msg, u32 len)\n{\n\tstruct intel_engine_cs *engine;\n\tu8 guc_class, instance;\n\tu32 reason;\n\tunsigned long flags;\n\n\tif (unlikely(len != 3)) {\n\t\tguc_err(guc, \"Invalid length %u\", len);\n\t\treturn -EPROTO;\n\t}\n\n\tguc_class = msg[0];\n\tinstance = msg[1];\n\treason = msg[2];\n\n\tengine = intel_guc_lookup_engine(guc, guc_class, instance);\n\tif (unlikely(!engine)) {\n\t\tguc_err(guc, \"Invalid engine %d:%d\", guc_class, instance);\n\t\treturn -EPROTO;\n\t}\n\n\t \n\tguc_err(guc, \"Engine reset failed on %d:%d (%s) because 0x%08X\",\n\t\tguc_class, instance, engine->name, reason);\n\n\tspin_lock_irqsave(&guc->submission_state.lock, flags);\n\tguc->submission_state.reset_fail_mask |= engine->mask;\n\tspin_unlock_irqrestore(&guc->submission_state.lock, flags);\n\n\t \n\tqueue_work(system_unbound_wq, &guc->submission_state.reset_fail_worker);\n\n\treturn 0;\n}\n\nvoid intel_guc_find_hung_context(struct intel_engine_cs *engine)\n{\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tstruct intel_context *ce;\n\tstruct i915_request *rq;\n\tunsigned long index;\n\tunsigned long flags;\n\n\t \n\tif (unlikely(!guc_submission_initialized(guc)))\n\t\treturn;\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\tbool found;\n\n\t\tif (!kref_get_unless_zero(&ce->ref))\n\t\t\tcontinue;\n\n\t\txa_unlock(&guc->context_lookup);\n\n\t\tif (!intel_context_is_pinned(ce))\n\t\t\tgoto next;\n\n\t\tif (intel_engine_is_virtual(ce->engine)) {\n\t\t\tif (!(ce->engine->mask & engine->mask))\n\t\t\t\tgoto next;\n\t\t} else {\n\t\t\tif (ce->engine != engine)\n\t\t\t\tgoto next;\n\t\t}\n\n\t\tfound = false;\n\t\tspin_lock(&ce->guc_state.lock);\n\t\tlist_for_each_entry(rq, &ce->guc_state.requests, sched.link) {\n\t\t\tif (i915_test_request_state(rq) != I915_REQUEST_ACTIVE)\n\t\t\t\tcontinue;\n\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ce->guc_state.lock);\n\n\t\tif (found) {\n\t\t\tintel_engine_set_hung_context(engine, ce);\n\n\t\t\t \n\t\t\tintel_context_put(ce);\n\t\t\txa_lock(&guc->context_lookup);\n\t\t\tgoto done;\n\t\t}\n\nnext:\n\t\tintel_context_put(ce);\n\t\txa_lock(&guc->context_lookup);\n\t}\ndone:\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\nvoid intel_guc_dump_active_requests(struct intel_engine_cs *engine,\n\t\t\t\t    struct i915_request *hung_rq,\n\t\t\t\t    struct drm_printer *m)\n{\n\tstruct intel_guc *guc = &engine->gt->uc.guc;\n\tstruct intel_context *ce;\n\tunsigned long index;\n\tunsigned long flags;\n\n\t \n\tif (unlikely(!guc_submission_initialized(guc)))\n\t\treturn;\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\tif (!kref_get_unless_zero(&ce->ref))\n\t\t\tcontinue;\n\n\t\txa_unlock(&guc->context_lookup);\n\n\t\tif (!intel_context_is_pinned(ce))\n\t\t\tgoto next;\n\n\t\tif (intel_engine_is_virtual(ce->engine)) {\n\t\t\tif (!(ce->engine->mask & engine->mask))\n\t\t\t\tgoto next;\n\t\t} else {\n\t\t\tif (ce->engine != engine)\n\t\t\t\tgoto next;\n\t\t}\n\n\t\tspin_lock(&ce->guc_state.lock);\n\t\tintel_engine_dump_active_requests(&ce->guc_state.requests,\n\t\t\t\t\t\t  hung_rq, m);\n\t\tspin_unlock(&ce->guc_state.lock);\n\nnext:\n\t\tintel_context_put(ce);\n\t\txa_lock(&guc->context_lookup);\n\t}\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\nvoid intel_guc_submission_print_info(struct intel_guc *guc,\n\t\t\t\t     struct drm_printer *p)\n{\n\tstruct i915_sched_engine *sched_engine = guc->sched_engine;\n\tstruct rb_node *rb;\n\tunsigned long flags;\n\n\tif (!sched_engine)\n\t\treturn;\n\n\tdrm_printf(p, \"GuC Submission API Version: %d.%d.%d\\n\",\n\t\t   guc->submission_version.major, guc->submission_version.minor,\n\t\t   guc->submission_version.patch);\n\tdrm_printf(p, \"GuC Number Outstanding Submission G2H: %u\\n\",\n\t\t   atomic_read(&guc->outstanding_submission_g2h));\n\tdrm_printf(p, \"GuC tasklet count: %u\\n\",\n\t\t   atomic_read(&sched_engine->tasklet.count));\n\n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\tdrm_printf(p, \"Requests in GuC submit tasklet:\\n\");\n\tfor (rb = rb_first_cached(&sched_engine->queue); rb; rb = rb_next(rb)) {\n\t\tstruct i915_priolist *pl = to_priolist(rb);\n\t\tstruct i915_request *rq;\n\n\t\tpriolist_for_each_request(rq, pl)\n\t\t\tdrm_printf(p, \"guc_id=%u, seqno=%llu\\n\",\n\t\t\t\t   rq->context->guc_id.id,\n\t\t\t\t   rq->fence.seqno);\n\t}\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n\tdrm_printf(p, \"\\n\");\n}\n\nstatic inline void guc_log_context_priority(struct drm_printer *p,\n\t\t\t\t\t    struct intel_context *ce)\n{\n\tint i;\n\n\tdrm_printf(p, \"\\t\\tPriority: %d\\n\", ce->guc_state.prio);\n\tdrm_printf(p, \"\\t\\tNumber Requests (lower index == higher priority)\\n\");\n\tfor (i = GUC_CLIENT_PRIORITY_KMD_HIGH;\n\t     i < GUC_CLIENT_PRIORITY_NUM; ++i) {\n\t\tdrm_printf(p, \"\\t\\tNumber requests in priority band[%d]: %d\\n\",\n\t\t\t   i, ce->guc_state.prio_count[i]);\n\t}\n\tdrm_printf(p, \"\\n\");\n}\n\nstatic inline void guc_log_context(struct drm_printer *p,\n\t\t\t\t   struct intel_context *ce)\n{\n\tdrm_printf(p, \"GuC lrc descriptor %u:\\n\", ce->guc_id.id);\n\tdrm_printf(p, \"\\tHW Context Desc: 0x%08x\\n\", ce->lrc.lrca);\n\tdrm_printf(p, \"\\t\\tLRC Head: Internal %u, Memory %u\\n\",\n\t\t   ce->ring->head,\n\t\t   ce->lrc_reg_state[CTX_RING_HEAD]);\n\tdrm_printf(p, \"\\t\\tLRC Tail: Internal %u, Memory %u\\n\",\n\t\t   ce->ring->tail,\n\t\t   ce->lrc_reg_state[CTX_RING_TAIL]);\n\tdrm_printf(p, \"\\t\\tContext Pin Count: %u\\n\",\n\t\t   atomic_read(&ce->pin_count));\n\tdrm_printf(p, \"\\t\\tGuC ID Ref Count: %u\\n\",\n\t\t   atomic_read(&ce->guc_id.ref));\n\tdrm_printf(p, \"\\t\\tSchedule State: 0x%x\\n\",\n\t\t   ce->guc_state.sched_state);\n}\n\nvoid intel_guc_submission_print_context_info(struct intel_guc *guc,\n\t\t\t\t\t     struct drm_printer *p)\n{\n\tstruct intel_context *ce;\n\tunsigned long index;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&guc->context_lookup, flags);\n\txa_for_each(&guc->context_lookup, index, ce) {\n\t\tGEM_BUG_ON(intel_context_is_child(ce));\n\n\t\tguc_log_context(p, ce);\n\t\tguc_log_context_priority(p, ce);\n\n\t\tif (intel_context_is_parent(ce)) {\n\t\t\tstruct intel_context *child;\n\n\t\t\tdrm_printf(p, \"\\t\\tNumber children: %u\\n\",\n\t\t\t\t   ce->parallel.number_children);\n\n\t\t\tif (ce->parallel.guc.wq_status) {\n\t\t\t\tdrm_printf(p, \"\\t\\tWQI Head: %u\\n\",\n\t\t\t\t\t   READ_ONCE(*ce->parallel.guc.wq_head));\n\t\t\t\tdrm_printf(p, \"\\t\\tWQI Tail: %u\\n\",\n\t\t\t\t\t   READ_ONCE(*ce->parallel.guc.wq_tail));\n\t\t\t\tdrm_printf(p, \"\\t\\tWQI Status: %u\\n\",\n\t\t\t\t\t   READ_ONCE(*ce->parallel.guc.wq_status));\n\t\t\t}\n\n\t\t\tif (ce->engine->emit_bb_start ==\n\t\t\t    emit_bb_start_parent_no_preempt_mid_batch) {\n\t\t\t\tu8 i;\n\n\t\t\t\tdrm_printf(p, \"\\t\\tChildren Go: %u\\n\",\n\t\t\t\t\t   get_children_go_value(ce));\n\t\t\t\tfor (i = 0; i < ce->parallel.number_children; ++i)\n\t\t\t\t\tdrm_printf(p, \"\\t\\tChildren Join: %u\\n\",\n\t\t\t\t\t\t   get_children_join_value(ce, i));\n\t\t\t}\n\n\t\t\tfor_each_child(ce, child)\n\t\t\t\tguc_log_context(p, child);\n\t\t}\n\t}\n\txa_unlock_irqrestore(&guc->context_lookup, flags);\n}\n\nstatic inline u32 get_children_go_addr(struct intel_context *ce)\n{\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\n\treturn i915_ggtt_offset(ce->state) +\n\t\t__get_parent_scratch_offset(ce) +\n\t\toffsetof(struct parent_scratch, go.semaphore);\n}\n\nstatic inline u32 get_children_join_addr(struct intel_context *ce,\n\t\t\t\t\t u8 child_index)\n{\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\n\treturn i915_ggtt_offset(ce->state) +\n\t\t__get_parent_scratch_offset(ce) +\n\t\toffsetof(struct parent_scratch, join[child_index].semaphore);\n}\n\n#define PARENT_GO_BB\t\t\t1\n#define PARENT_GO_FINI_BREADCRUMB\t0\n#define CHILD_GO_BB\t\t\t1\n#define CHILD_GO_FINI_BREADCRUMB\t0\nstatic int emit_bb_start_parent_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t     u64 offset, u32 len,\n\t\t\t\t\t\t     const unsigned int flags)\n{\n\tstruct intel_context *ce = rq->context;\n\tu32 *cs;\n\tu8 i;\n\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\n\tcs = intel_ring_begin(rq, 10 + 4 * ce->parallel.number_children);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\tfor (i = 0; i < ce->parallel.number_children; ++i) {\n\t\t*cs++ = (MI_SEMAPHORE_WAIT |\n\t\t\t MI_SEMAPHORE_GLOBAL_GTT |\n\t\t\t MI_SEMAPHORE_POLL |\n\t\t\t MI_SEMAPHORE_SAD_EQ_SDD);\n\t\t*cs++ = PARENT_GO_BB;\n\t\t*cs++ = get_children_join_addr(ce, i);\n\t\t*cs++ = 0;\n\t}\n\n\t \n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\t*cs++ = MI_NOOP;\n\n\t \n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  CHILD_GO_BB,\n\t\t\t\t  get_children_go_addr(ce),\n\t\t\t\t  0);\n\n\t \n\t*cs++ = MI_BATCH_BUFFER_START_GEN8 |\n\t\t(flags & I915_DISPATCH_SECURE ? 0 : BIT(8));\n\t*cs++ = lower_32_bits(offset);\n\t*cs++ = upper_32_bits(offset);\n\t*cs++ = MI_NOOP;\n\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic int emit_bb_start_child_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t    u64 offset, u32 len,\n\t\t\t\t\t\t    const unsigned int flags)\n{\n\tstruct intel_context *ce = rq->context;\n\tstruct intel_context *parent = intel_context_to_parent(ce);\n\tu32 *cs;\n\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\n\tcs = intel_ring_begin(rq, 12);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  PARENT_GO_BB,\n\t\t\t\t  get_children_join_addr(parent,\n\t\t\t\t\t\t\t ce->parallel.child_index),\n\t\t\t\t  0);\n\n\t \n\t*cs++ = (MI_SEMAPHORE_WAIT |\n\t\t MI_SEMAPHORE_GLOBAL_GTT |\n\t\t MI_SEMAPHORE_POLL |\n\t\t MI_SEMAPHORE_SAD_EQ_SDD);\n\t*cs++ = CHILD_GO_BB;\n\t*cs++ = get_children_go_addr(parent);\n\t*cs++ = 0;\n\n\t \n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\n\t \n\t*cs++ = MI_BATCH_BUFFER_START_GEN8 |\n\t\t(flags & I915_DISPATCH_SECURE ? 0 : BIT(8));\n\t*cs++ = lower_32_bits(offset);\n\t*cs++ = upper_32_bits(offset);\n\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic u32 *\n__emit_fini_breadcrumb_parent_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t   u32 *cs)\n{\n\tstruct intel_context *ce = rq->context;\n\tu8 i;\n\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\n\t \n\tfor (i = 0; i < ce->parallel.number_children; ++i) {\n\t\t*cs++ = (MI_SEMAPHORE_WAIT |\n\t\t\t MI_SEMAPHORE_GLOBAL_GTT |\n\t\t\t MI_SEMAPHORE_POLL |\n\t\t\t MI_SEMAPHORE_SAD_EQ_SDD);\n\t\t*cs++ = PARENT_GO_FINI_BREADCRUMB;\n\t\t*cs++ = get_children_join_addr(ce, i);\n\t\t*cs++ = 0;\n\t}\n\n\t \n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_ENABLE;\n\t*cs++ = MI_NOOP;\n\n\t \n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  CHILD_GO_FINI_BREADCRUMB,\n\t\t\t\t  get_children_go_addr(ce),\n\t\t\t\t  0);\n\n\treturn cs;\n}\n\n \nstatic inline bool skip_handshake(struct i915_request *rq)\n{\n\treturn test_bit(I915_FENCE_FLAG_SKIP_PARALLEL, &rq->fence.flags);\n}\n\n#define NON_SKIP_LEN\t6\nstatic u32 *\nemit_fini_breadcrumb_parent_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t u32 *cs)\n{\n\tstruct intel_context *ce = rq->context;\n\t__maybe_unused u32 *before_fini_breadcrumb_user_interrupt_cs;\n\t__maybe_unused u32 *start_fini_breadcrumb_cs = cs;\n\n\tGEM_BUG_ON(!intel_context_is_parent(ce));\n\n\tif (unlikely(skip_handshake(rq))) {\n\t\t \n\t\tmemset(cs, 0, sizeof(u32) *\n\t\t       (ce->engine->emit_fini_breadcrumb_dw - NON_SKIP_LEN));\n\t\tcs += ce->engine->emit_fini_breadcrumb_dw - NON_SKIP_LEN;\n\t} else {\n\t\tcs = __emit_fini_breadcrumb_parent_no_preempt_mid_batch(rq, cs);\n\t}\n\n\t \n\tbefore_fini_breadcrumb_user_interrupt_cs = cs;\n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  rq->fence.seqno,\n\t\t\t\t  i915_request_active_timeline(rq)->hwsp_offset,\n\t\t\t\t  0);\n\n\t \n\t*cs++ = MI_USER_INTERRUPT;\n\t*cs++ = MI_NOOP;\n\n\t \n\tGEM_BUG_ON(before_fini_breadcrumb_user_interrupt_cs + NON_SKIP_LEN !=\n\t\t   cs);\n\tGEM_BUG_ON(start_fini_breadcrumb_cs +\n\t\t   ce->engine->emit_fini_breadcrumb_dw != cs);\n\n\trq->tail = intel_ring_offset(rq, cs);\n\n\treturn cs;\n}\n\nstatic u32 *\n__emit_fini_breadcrumb_child_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\t  u32 *cs)\n{\n\tstruct intel_context *ce = rq->context;\n\tstruct intel_context *parent = intel_context_to_parent(ce);\n\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\n\t \n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_ENABLE;\n\t*cs++ = MI_NOOP;\n\n\t \n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  PARENT_GO_FINI_BREADCRUMB,\n\t\t\t\t  get_children_join_addr(parent,\n\t\t\t\t\t\t\t ce->parallel.child_index),\n\t\t\t\t  0);\n\n\t \n\t*cs++ = (MI_SEMAPHORE_WAIT |\n\t\t MI_SEMAPHORE_GLOBAL_GTT |\n\t\t MI_SEMAPHORE_POLL |\n\t\t MI_SEMAPHORE_SAD_EQ_SDD);\n\t*cs++ = CHILD_GO_FINI_BREADCRUMB;\n\t*cs++ = get_children_go_addr(parent);\n\t*cs++ = 0;\n\n\treturn cs;\n}\n\nstatic u32 *\nemit_fini_breadcrumb_child_no_preempt_mid_batch(struct i915_request *rq,\n\t\t\t\t\t\tu32 *cs)\n{\n\tstruct intel_context *ce = rq->context;\n\t__maybe_unused u32 *before_fini_breadcrumb_user_interrupt_cs;\n\t__maybe_unused u32 *start_fini_breadcrumb_cs = cs;\n\n\tGEM_BUG_ON(!intel_context_is_child(ce));\n\n\tif (unlikely(skip_handshake(rq))) {\n\t\t \n\t\tmemset(cs, 0, sizeof(u32) *\n\t\t       (ce->engine->emit_fini_breadcrumb_dw - NON_SKIP_LEN));\n\t\tcs += ce->engine->emit_fini_breadcrumb_dw - NON_SKIP_LEN;\n\t} else {\n\t\tcs = __emit_fini_breadcrumb_child_no_preempt_mid_batch(rq, cs);\n\t}\n\n\t \n\tbefore_fini_breadcrumb_user_interrupt_cs = cs;\n\tcs = gen8_emit_ggtt_write(cs,\n\t\t\t\t  rq->fence.seqno,\n\t\t\t\t  i915_request_active_timeline(rq)->hwsp_offset,\n\t\t\t\t  0);\n\n\t \n\t*cs++ = MI_USER_INTERRUPT;\n\t*cs++ = MI_NOOP;\n\n\t \n\tGEM_BUG_ON(before_fini_breadcrumb_user_interrupt_cs + NON_SKIP_LEN !=\n\t\t   cs);\n\tGEM_BUG_ON(start_fini_breadcrumb_cs +\n\t\t   ce->engine->emit_fini_breadcrumb_dw != cs);\n\n\trq->tail = intel_ring_offset(rq, cs);\n\n\treturn cs;\n}\n\n#undef NON_SKIP_LEN\n\nstatic struct intel_context *\nguc_create_virtual(struct intel_engine_cs **siblings, unsigned int count,\n\t\t   unsigned long flags)\n{\n\tstruct guc_virtual_engine *ve;\n\tstruct intel_guc *guc;\n\tunsigned int n;\n\tint err;\n\n\tve = kzalloc(sizeof(*ve), GFP_KERNEL);\n\tif (!ve)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tguc = &siblings[0]->gt->uc.guc;\n\n\tve->base.i915 = siblings[0]->i915;\n\tve->base.gt = siblings[0]->gt;\n\tve->base.uncore = siblings[0]->uncore;\n\tve->base.id = -1;\n\n\tve->base.uabi_class = I915_ENGINE_CLASS_INVALID;\n\tve->base.instance = I915_ENGINE_CLASS_INVALID_VIRTUAL;\n\tve->base.uabi_instance = I915_ENGINE_CLASS_INVALID_VIRTUAL;\n\tve->base.saturated = ALL_ENGINES;\n\n\tsnprintf(ve->base.name, sizeof(ve->base.name), \"virtual\");\n\n\tve->base.sched_engine = i915_sched_engine_get(guc->sched_engine);\n\n\tve->base.cops = &virtual_guc_context_ops;\n\tve->base.request_alloc = guc_request_alloc;\n\tve->base.bump_serial = virtual_guc_bump_serial;\n\n\tve->base.submit_request = guc_submit_request;\n\n\tve->base.flags = I915_ENGINE_IS_VIRTUAL;\n\n\tBUILD_BUG_ON(ilog2(VIRTUAL_ENGINES) < I915_NUM_ENGINES);\n\tve->base.mask = VIRTUAL_ENGINES;\n\n\tintel_context_init(&ve->context, &ve->base);\n\n\tfor (n = 0; n < count; n++) {\n\t\tstruct intel_engine_cs *sibling = siblings[n];\n\n\t\tGEM_BUG_ON(!is_power_of_2(sibling->mask));\n\t\tif (sibling->mask & ve->base.mask) {\n\t\t\tguc_dbg(guc, \"duplicate %s entry in load balancer\\n\",\n\t\t\t\tsibling->name);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put;\n\t\t}\n\n\t\tve->base.mask |= sibling->mask;\n\t\tve->base.logical_mask |= sibling->logical_mask;\n\n\t\tif (n != 0 && ve->base.class != sibling->class) {\n\t\t\tguc_dbg(guc, \"invalid mixing of engine class, sibling %d, already %d\\n\",\n\t\t\t\tsibling->class, ve->base.class);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put;\n\t\t} else if (n == 0) {\n\t\t\tve->base.class = sibling->class;\n\t\t\tve->base.uabi_class = sibling->uabi_class;\n\t\t\tsnprintf(ve->base.name, sizeof(ve->base.name),\n\t\t\t\t \"v%dx%d\", ve->base.class, count);\n\t\t\tve->base.context_size = sibling->context_size;\n\n\t\t\tve->base.add_active_request =\n\t\t\t\tsibling->add_active_request;\n\t\t\tve->base.remove_active_request =\n\t\t\t\tsibling->remove_active_request;\n\t\t\tve->base.emit_bb_start = sibling->emit_bb_start;\n\t\t\tve->base.emit_flush = sibling->emit_flush;\n\t\t\tve->base.emit_init_breadcrumb =\n\t\t\t\tsibling->emit_init_breadcrumb;\n\t\t\tve->base.emit_fini_breadcrumb =\n\t\t\t\tsibling->emit_fini_breadcrumb;\n\t\t\tve->base.emit_fini_breadcrumb_dw =\n\t\t\t\tsibling->emit_fini_breadcrumb_dw;\n\t\t\tve->base.breadcrumbs =\n\t\t\t\tintel_breadcrumbs_get(sibling->breadcrumbs);\n\n\t\t\tve->base.flags |= sibling->flags;\n\n\t\t\tve->base.props.timeslice_duration_ms =\n\t\t\t\tsibling->props.timeslice_duration_ms;\n\t\t\tve->base.props.preempt_timeout_ms =\n\t\t\t\tsibling->props.preempt_timeout_ms;\n\t\t}\n\t}\n\n\treturn &ve->context;\n\nerr_put:\n\tintel_context_put(&ve->context);\n\treturn ERR_PTR(err);\n}\n\nbool intel_guc_virtual_engine_has_heartbeat(const struct intel_engine_cs *ve)\n{\n\tstruct intel_engine_cs *engine;\n\tintel_engine_mask_t tmp, mask = ve->mask;\n\n\tfor_each_engine_masked(engine, ve->gt, mask, tmp)\n\t\tif (READ_ONCE(engine->props.heartbeat_interval_ms))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_guc.c\"\n#include \"selftest_guc_multi_lrc.c\"\n#include \"selftest_guc_hangcheck.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}