{
  "module_name": "intel_guc_ct.c",
  "hash_id": "9ecaf6ff705f07ddb97ab66812edb93232086cad35636491c8fdf7d3fea779ef",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c",
  "human_readable_source": "\n \n\n#include <linux/circ_buf.h>\n#include <linux/ktime.h>\n#include <linux/time64.h>\n#include <linux/string_helpers.h>\n#include <linux/timekeeping.h>\n\n#include \"i915_drv.h\"\n#include \"intel_guc_ct.h\"\n#include \"intel_guc_print.h\"\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\nenum {\n\tCT_DEAD_ALIVE = 0,\n\tCT_DEAD_SETUP,\n\tCT_DEAD_WRITE,\n\tCT_DEAD_DEADLOCK,\n\tCT_DEAD_H2G_HAS_ROOM,\n\tCT_DEAD_READ,\n\tCT_DEAD_PROCESS_FAILED,\n};\n\nstatic void ct_dead_ct_worker_func(struct work_struct *w);\n\n#define CT_DEAD(ct, reason)\t\\\n\tdo { \\\n\t\tif (!(ct)->dead_ct_reported) { \\\n\t\t\t(ct)->dead_ct_reason |= 1 << CT_DEAD_##reason; \\\n\t\t\tqueue_work(system_unbound_wq, &(ct)->dead_ct_worker); \\\n\t\t} \\\n\t} while (0)\n#else\n#define CT_DEAD(ct, reason)\tdo { } while (0)\n#endif\n\nstatic inline struct intel_guc *ct_to_guc(struct intel_guc_ct *ct)\n{\n\treturn container_of(ct, struct intel_guc, ct);\n}\n\n#define CT_ERROR(_ct, _fmt, ...) \\\n\tguc_err(ct_to_guc(_ct), \"CT: \" _fmt, ##__VA_ARGS__)\n#ifdef CONFIG_DRM_I915_DEBUG_GUC\n#define CT_DEBUG(_ct, _fmt, ...) \\\n\tguc_dbg(ct_to_guc(_ct), \"CT: \" _fmt, ##__VA_ARGS__)\n#else\n#define CT_DEBUG(...)\tdo { } while (0)\n#endif\n#define CT_PROBE_ERROR(_ct, _fmt, ...) \\\n\tguc_probe_error(ct_to_guc(ct), \"CT: \" _fmt, ##__VA_ARGS__)\n\n \n#define CTB_DESC_SIZE\t\tALIGN(sizeof(struct guc_ct_buffer_desc), SZ_2K)\n#define CTB_H2G_BUFFER_SIZE\t(SZ_4K)\n#define CTB_G2H_BUFFER_SIZE\t(4 * CTB_H2G_BUFFER_SIZE)\n#define G2H_ROOM_BUFFER_SIZE\t(CTB_G2H_BUFFER_SIZE / 4)\n\nstruct ct_request {\n\tstruct list_head link;\n\tu32 fence;\n\tu32 status;\n\tu32 response_len;\n\tu32 *response_buf;\n};\n\nstruct ct_incoming_msg {\n\tstruct list_head link;\n\tu32 size;\n\tu32 msg[];\n};\n\nenum { CTB_SEND = 0, CTB_RECV = 1 };\n\nenum { CTB_OWNER_HOST = 0 };\n\nstatic void ct_receive_tasklet_func(struct tasklet_struct *t);\nstatic void ct_incoming_request_worker_func(struct work_struct *w);\n\n \nvoid intel_guc_ct_init_early(struct intel_guc_ct *ct)\n{\n\tspin_lock_init(&ct->ctbs.send.lock);\n\tspin_lock_init(&ct->ctbs.recv.lock);\n\tspin_lock_init(&ct->requests.lock);\n\tINIT_LIST_HEAD(&ct->requests.pending);\n\tINIT_LIST_HEAD(&ct->requests.incoming);\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\n\tINIT_WORK(&ct->dead_ct_worker, ct_dead_ct_worker_func);\n#endif\n\tINIT_WORK(&ct->requests.worker, ct_incoming_request_worker_func);\n\ttasklet_setup(&ct->receive_tasklet, ct_receive_tasklet_func);\n\tinit_waitqueue_head(&ct->wq);\n}\n\nstatic void guc_ct_buffer_desc_init(struct guc_ct_buffer_desc *desc)\n{\n\tmemset(desc, 0, sizeof(*desc));\n}\n\nstatic void guc_ct_buffer_reset(struct intel_guc_ct_buffer *ctb)\n{\n\tu32 space;\n\n\tctb->broken = false;\n\tctb->tail = 0;\n\tctb->head = 0;\n\tspace = CIRC_SPACE(ctb->tail, ctb->head, ctb->size) - ctb->resv_space;\n\tatomic_set(&ctb->space, space);\n\n\tguc_ct_buffer_desc_init(ctb->desc);\n}\n\nstatic void guc_ct_buffer_init(struct intel_guc_ct_buffer *ctb,\n\t\t\t       struct guc_ct_buffer_desc *desc,\n\t\t\t       u32 *cmds, u32 size_in_bytes, u32 resv_space)\n{\n\tGEM_BUG_ON(size_in_bytes % 4);\n\n\tctb->desc = desc;\n\tctb->cmds = cmds;\n\tctb->size = size_in_bytes / 4;\n\tctb->resv_space = resv_space / 4;\n\n\tguc_ct_buffer_reset(ctb);\n}\n\nstatic int guc_action_control_ctb(struct intel_guc *guc, u32 control)\n{\n\tu32 request[HOST2GUC_CONTROL_CTB_REQUEST_MSG_LEN] = {\n\t\tFIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |\n\t\tFIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |\n\t\tFIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_HOST2GUC_CONTROL_CTB),\n\t\tFIELD_PREP(HOST2GUC_CONTROL_CTB_REQUEST_MSG_1_CONTROL, control),\n\t};\n\tint ret;\n\n\tGEM_BUG_ON(control != GUC_CTB_CONTROL_DISABLE && control != GUC_CTB_CONTROL_ENABLE);\n\n\t \n\tret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request), NULL, 0);\n\n\treturn ret > 0 ? -EPROTO : ret;\n}\n\nstatic int ct_control_enable(struct intel_guc_ct *ct, bool enable)\n{\n\tint err;\n\n\terr = guc_action_control_ctb(ct_to_guc(ct), enable ?\n\t\t\t\t     GUC_CTB_CONTROL_ENABLE : GUC_CTB_CONTROL_DISABLE);\n\tif (unlikely(err))\n\t\tCT_PROBE_ERROR(ct, \"Failed to control/%s CTB (%pe)\\n\",\n\t\t\t       str_enable_disable(enable), ERR_PTR(err));\n\n\treturn err;\n}\n\nstatic int ct_register_buffer(struct intel_guc_ct *ct, bool send,\n\t\t\t      u32 desc_addr, u32 buff_addr, u32 size)\n{\n\tint err;\n\n\terr = intel_guc_self_cfg64(ct_to_guc(ct), send ?\n\t\t\t\t   GUC_KLV_SELF_CFG_H2G_CTB_DESCRIPTOR_ADDR_KEY :\n\t\t\t\t   GUC_KLV_SELF_CFG_G2H_CTB_DESCRIPTOR_ADDR_KEY,\n\t\t\t\t   desc_addr);\n\tif (unlikely(err))\n\t\tgoto failed;\n\n\terr = intel_guc_self_cfg64(ct_to_guc(ct), send ?\n\t\t\t\t   GUC_KLV_SELF_CFG_H2G_CTB_ADDR_KEY :\n\t\t\t\t   GUC_KLV_SELF_CFG_G2H_CTB_ADDR_KEY,\n\t\t\t\t   buff_addr);\n\tif (unlikely(err))\n\t\tgoto failed;\n\n\terr = intel_guc_self_cfg32(ct_to_guc(ct), send ?\n\t\t\t\t   GUC_KLV_SELF_CFG_H2G_CTB_SIZE_KEY :\n\t\t\t\t   GUC_KLV_SELF_CFG_G2H_CTB_SIZE_KEY,\n\t\t\t\t   size);\n\tif (unlikely(err))\nfailed:\n\t\tCT_PROBE_ERROR(ct, \"Failed to register %s buffer (%pe)\\n\",\n\t\t\t       send ? \"SEND\" : \"RECV\", ERR_PTR(err));\n\n\treturn err;\n}\n\n \nint intel_guc_ct_init(struct intel_guc_ct *ct)\n{\n\tstruct intel_guc *guc = ct_to_guc(ct);\n\tstruct guc_ct_buffer_desc *desc;\n\tu32 blob_size;\n\tu32 cmds_size;\n\tu32 resv_space;\n\tvoid *blob;\n\tu32 *cmds;\n\tint err;\n\n\terr = i915_inject_probe_error(guc_to_gt(guc)->i915, -ENXIO);\n\tif (err)\n\t\treturn err;\n\n\tGEM_BUG_ON(ct->vma);\n\n\tblob_size = 2 * CTB_DESC_SIZE + CTB_H2G_BUFFER_SIZE + CTB_G2H_BUFFER_SIZE;\n\terr = intel_guc_allocate_and_map_vma(guc, blob_size, &ct->vma, &blob);\n\tif (unlikely(err)) {\n\t\tCT_PROBE_ERROR(ct, \"Failed to allocate %u for CTB data (%pe)\\n\",\n\t\t\t       blob_size, ERR_PTR(err));\n\t\treturn err;\n\t}\n\n\tCT_DEBUG(ct, \"base=%#x size=%u\\n\", intel_guc_ggtt_offset(guc, ct->vma), blob_size);\n\n\t \n\tdesc = blob;\n\tcmds = blob + 2 * CTB_DESC_SIZE;\n\tcmds_size = CTB_H2G_BUFFER_SIZE;\n\tresv_space = 0;\n\tCT_DEBUG(ct, \"%s desc %#tx cmds %#tx size %u/%u\\n\", \"send\",\n\t\t ptrdiff(desc, blob), ptrdiff(cmds, blob), cmds_size,\n\t\t resv_space);\n\n\tguc_ct_buffer_init(&ct->ctbs.send, desc, cmds, cmds_size, resv_space);\n\n\t \n\tdesc = blob + CTB_DESC_SIZE;\n\tcmds = blob + 2 * CTB_DESC_SIZE + CTB_H2G_BUFFER_SIZE;\n\tcmds_size = CTB_G2H_BUFFER_SIZE;\n\tresv_space = G2H_ROOM_BUFFER_SIZE;\n\tCT_DEBUG(ct, \"%s desc %#tx cmds %#tx size %u/%u\\n\", \"recv\",\n\t\t ptrdiff(desc, blob), ptrdiff(cmds, blob), cmds_size,\n\t\t resv_space);\n\n\tguc_ct_buffer_init(&ct->ctbs.recv, desc, cmds, cmds_size, resv_space);\n\n\treturn 0;\n}\n\n \nvoid intel_guc_ct_fini(struct intel_guc_ct *ct)\n{\n\tGEM_BUG_ON(ct->enabled);\n\n\ttasklet_kill(&ct->receive_tasklet);\n\ti915_vma_unpin_and_release(&ct->vma, I915_VMA_RELEASE_MAP);\n\tmemset(ct, 0, sizeof(*ct));\n}\n\n \nint intel_guc_ct_enable(struct intel_guc_ct *ct)\n{\n\tstruct intel_guc *guc = ct_to_guc(ct);\n\tu32 base, desc, cmds, size;\n\tvoid *blob;\n\tint err;\n\n\tGEM_BUG_ON(ct->enabled);\n\n\t \n\tGEM_BUG_ON(!ct->vma);\n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(ct->vma->obj));\n\tbase = intel_guc_ggtt_offset(guc, ct->vma);\n\n\t \n\tblob = __px_vaddr(ct->vma->obj);\n\tGEM_BUG_ON(blob != ct->ctbs.send.desc);\n\n\t \n\tguc_ct_buffer_reset(&ct->ctbs.send);\n\tguc_ct_buffer_reset(&ct->ctbs.recv);\n\n\t \n\tdesc = base + ptrdiff(ct->ctbs.recv.desc, blob);\n\tcmds = base + ptrdiff(ct->ctbs.recv.cmds, blob);\n\tsize = ct->ctbs.recv.size * 4;\n\terr = ct_register_buffer(ct, false, desc, cmds, size);\n\tif (unlikely(err))\n\t\tgoto err_out;\n\n\tdesc = base + ptrdiff(ct->ctbs.send.desc, blob);\n\tcmds = base + ptrdiff(ct->ctbs.send.cmds, blob);\n\tsize = ct->ctbs.send.size * 4;\n\terr = ct_register_buffer(ct, true, desc, cmds, size);\n\tif (unlikely(err))\n\t\tgoto err_out;\n\n\terr = ct_control_enable(ct, true);\n\tif (unlikely(err))\n\t\tgoto err_out;\n\n\tct->enabled = true;\n\tct->stall_time = KTIME_MAX;\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\n\tct->dead_ct_reported = false;\n\tct->dead_ct_reason = CT_DEAD_ALIVE;\n#endif\n\n\treturn 0;\n\nerr_out:\n\tCT_PROBE_ERROR(ct, \"Failed to enable CTB (%pe)\\n\", ERR_PTR(err));\n\tCT_DEAD(ct, SETUP);\n\treturn err;\n}\n\n \nvoid intel_guc_ct_disable(struct intel_guc_ct *ct)\n{\n\tstruct intel_guc *guc = ct_to_guc(ct);\n\n\tGEM_BUG_ON(!ct->enabled);\n\n\tct->enabled = false;\n\n\tif (intel_guc_is_fw_running(guc)) {\n\t\tct_control_enable(ct, false);\n\t}\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\nstatic void ct_track_lost_and_found(struct intel_guc_ct *ct, u32 fence, u32 action)\n{\n\tunsigned int lost = fence % ARRAY_SIZE(ct->requests.lost_and_found);\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\n\tunsigned long entries[SZ_32];\n\tunsigned int n;\n\n\tn = stack_trace_save(entries, ARRAY_SIZE(entries), 1);\n\n\t \n\tct->requests.lost_and_found[lost].stack = stack_depot_save(entries, n, GFP_NOWAIT);\n#endif\n\tct->requests.lost_and_found[lost].fence = fence;\n\tct->requests.lost_and_found[lost].action = action;\n}\n#endif\n\nstatic u32 ct_get_next_fence(struct intel_guc_ct *ct)\n{\n\t \n\treturn ++ct->requests.last_fence;\n}\n\nstatic int ct_write(struct intel_guc_ct *ct,\n\t\t    const u32 *action,\n\t\t    u32 len  ,\n\t\t    u32 fence, u32 flags)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.send;\n\tstruct guc_ct_buffer_desc *desc = ctb->desc;\n\tu32 tail = ctb->tail;\n\tu32 size = ctb->size;\n\tu32 header;\n\tu32 hxg;\n\tu32 type;\n\tu32 *cmds = ctb->cmds;\n\tunsigned int i;\n\n\tif (unlikely(desc->status))\n\t\tgoto corrupted;\n\n\tGEM_BUG_ON(tail > size);\n\n#ifdef CONFIG_DRM_I915_DEBUG_GUC\n\tif (unlikely(tail != READ_ONCE(desc->tail))) {\n\t\tCT_ERROR(ct, \"Tail was modified %u != %u\\n\",\n\t\t\t desc->tail, tail);\n\t\tdesc->status |= GUC_CTB_STATUS_MISMATCH;\n\t\tgoto corrupted;\n\t}\n\tif (unlikely(READ_ONCE(desc->head) >= size)) {\n\t\tCT_ERROR(ct, \"Invalid head offset %u >= %u)\\n\",\n\t\t\t desc->head, size);\n\t\tdesc->status |= GUC_CTB_STATUS_OVERFLOW;\n\t\tgoto corrupted;\n\t}\n#endif\n\n\t \n\theader = FIELD_PREP(GUC_CTB_MSG_0_FORMAT, GUC_CTB_FORMAT_HXG) |\n\t\t FIELD_PREP(GUC_CTB_MSG_0_NUM_DWORDS, len) |\n\t\t FIELD_PREP(GUC_CTB_MSG_0_FENCE, fence);\n\n\ttype = (flags & INTEL_GUC_CT_SEND_NB) ? GUC_HXG_TYPE_FAST_REQUEST :\n\t\tGUC_HXG_TYPE_REQUEST;\n\thxg = FIELD_PREP(GUC_HXG_MSG_0_TYPE, type) |\n\t\tFIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION |\n\t\t\t   GUC_HXG_REQUEST_MSG_0_DATA0, action[0]);\n\n\tCT_DEBUG(ct, \"writing (tail %u) %*ph %*ph %*ph\\n\",\n\t\t tail, 4, &header, 4, &hxg, 4 * (len - 1), &action[1]);\n\n\tcmds[tail] = header;\n\ttail = (tail + 1) % size;\n\n\tcmds[tail] = hxg;\n\ttail = (tail + 1) % size;\n\n\tfor (i = 1; i < len; i++) {\n\t\tcmds[tail] = action[i];\n\t\ttail = (tail + 1) % size;\n\t}\n\tGEM_BUG_ON(tail > size);\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\n\tct_track_lost_and_found(ct, fence,\n\t\t\t\tFIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, action[0]));\n#endif\n\n\t \n\tintel_guc_write_barrier(ct_to_guc(ct));\n\n\t \n\tctb->tail = tail;\n\tGEM_BUG_ON(atomic_read(&ctb->space) < len + GUC_CTB_HDR_LEN);\n\tatomic_sub(len + GUC_CTB_HDR_LEN, &ctb->space);\n\n\t \n\tWRITE_ONCE(desc->tail, tail);\n\n\treturn 0;\n\ncorrupted:\n\tCT_ERROR(ct, \"Corrupted descriptor head=%u tail=%u status=%#x\\n\",\n\t\t desc->head, desc->tail, desc->status);\n\tCT_DEAD(ct, WRITE);\n\tctb->broken = true;\n\treturn -EPIPE;\n}\n\n \nstatic int wait_for_ct_request_update(struct intel_guc_ct *ct, struct ct_request *req, u32 *status)\n{\n\tint err;\n\tbool ct_enabled;\n\n\t \n#define GUC_CTB_RESPONSE_TIMEOUT_SHORT_MS 10\n#define GUC_CTB_RESPONSE_TIMEOUT_LONG_MS 1000\n#define done \\\n\t(!(ct_enabled = intel_guc_ct_enabled(ct)) || \\\n\t FIELD_GET(GUC_HXG_MSG_0_ORIGIN, READ_ONCE(req->status)) == \\\n\t GUC_HXG_ORIGIN_GUC)\n\terr = wait_for_us(done, GUC_CTB_RESPONSE_TIMEOUT_SHORT_MS);\n\tif (err)\n\t\terr = wait_for(done, GUC_CTB_RESPONSE_TIMEOUT_LONG_MS);\n#undef done\n\tif (!ct_enabled)\n\t\terr = -ENODEV;\n\n\t*status = req->status;\n\treturn err;\n}\n\n#define GUC_CTB_TIMEOUT_MS\t1500\nstatic inline bool ct_deadlocked(struct intel_guc_ct *ct)\n{\n\tlong timeout = GUC_CTB_TIMEOUT_MS;\n\tbool ret = ktime_ms_delta(ktime_get(), ct->stall_time) > timeout;\n\n\tif (unlikely(ret)) {\n\t\tstruct guc_ct_buffer_desc *send = ct->ctbs.send.desc;\n\t\tstruct guc_ct_buffer_desc *recv = ct->ctbs.send.desc;\n\n\t\tCT_ERROR(ct, \"Communication stalled for %lld ms, desc status=%#x,%#x\\n\",\n\t\t\t ktime_ms_delta(ktime_get(), ct->stall_time),\n\t\t\t send->status, recv->status);\n\t\tCT_ERROR(ct, \"H2G Space: %u (Bytes)\\n\",\n\t\t\t atomic_read(&ct->ctbs.send.space) * 4);\n\t\tCT_ERROR(ct, \"Head: %u (Dwords)\\n\", ct->ctbs.send.desc->head);\n\t\tCT_ERROR(ct, \"Tail: %u (Dwords)\\n\", ct->ctbs.send.desc->tail);\n\t\tCT_ERROR(ct, \"G2H Space: %u (Bytes)\\n\",\n\t\t\t atomic_read(&ct->ctbs.recv.space) * 4);\n\t\tCT_ERROR(ct, \"Head: %u\\n (Dwords)\", ct->ctbs.recv.desc->head);\n\t\tCT_ERROR(ct, \"Tail: %u\\n (Dwords)\", ct->ctbs.recv.desc->tail);\n\n\t\tCT_DEAD(ct, DEADLOCK);\n\t\tct->ctbs.send.broken = true;\n\t}\n\n\treturn ret;\n}\n\nstatic inline bool g2h_has_room(struct intel_guc_ct *ct, u32 g2h_len_dw)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.recv;\n\n\t \n\treturn !g2h_len_dw || atomic_read(&ctb->space) >= g2h_len_dw;\n}\n\nstatic inline void g2h_reserve_space(struct intel_guc_ct *ct, u32 g2h_len_dw)\n{\n\tlockdep_assert_held(&ct->ctbs.send.lock);\n\n\tGEM_BUG_ON(!g2h_has_room(ct, g2h_len_dw));\n\n\tif (g2h_len_dw)\n\t\tatomic_sub(g2h_len_dw, &ct->ctbs.recv.space);\n}\n\nstatic inline void g2h_release_space(struct intel_guc_ct *ct, u32 g2h_len_dw)\n{\n\tatomic_add(g2h_len_dw, &ct->ctbs.recv.space);\n}\n\nstatic inline bool h2g_has_room(struct intel_guc_ct *ct, u32 len_dw)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.send;\n\tstruct guc_ct_buffer_desc *desc = ctb->desc;\n\tu32 head;\n\tu32 space;\n\n\tif (atomic_read(&ctb->space) >= len_dw)\n\t\treturn true;\n\n\thead = READ_ONCE(desc->head);\n\tif (unlikely(head > ctb->size)) {\n\t\tCT_ERROR(ct, \"Invalid head offset %u >= %u)\\n\",\n\t\t\t head, ctb->size);\n\t\tdesc->status |= GUC_CTB_STATUS_OVERFLOW;\n\t\tctb->broken = true;\n\t\tCT_DEAD(ct, H2G_HAS_ROOM);\n\t\treturn false;\n\t}\n\n\tspace = CIRC_SPACE(ctb->tail, head, ctb->size);\n\tatomic_set(&ctb->space, space);\n\n\treturn space >= len_dw;\n}\n\nstatic int has_room_nb(struct intel_guc_ct *ct, u32 h2g_dw, u32 g2h_dw)\n{\n\tbool h2g = h2g_has_room(ct, h2g_dw);\n\tbool g2h = g2h_has_room(ct, g2h_dw);\n\n\tlockdep_assert_held(&ct->ctbs.send.lock);\n\n\tif (unlikely(!h2g || !g2h)) {\n\t\tif (ct->stall_time == KTIME_MAX)\n\t\t\tct->stall_time = ktime_get();\n\n\t\t \n\t\tif (!g2h)\n\t\t\ttasklet_hi_schedule(&ct->receive_tasklet);\n\n\t\tif (unlikely(ct_deadlocked(ct)))\n\t\t\treturn -EPIPE;\n\t\telse\n\t\t\treturn -EBUSY;\n\t}\n\n\tct->stall_time = KTIME_MAX;\n\treturn 0;\n}\n\n#define G2H_LEN_DW(f) ({ \\\n\ttypeof(f) f_ = (f); \\\n\tFIELD_GET(INTEL_GUC_CT_SEND_G2H_DW_MASK, f_) ? \\\n\tFIELD_GET(INTEL_GUC_CT_SEND_G2H_DW_MASK, f_) + \\\n\tGUC_CTB_HXG_MSG_MIN_LEN : 0; \\\n})\nstatic int ct_send_nb(struct intel_guc_ct *ct,\n\t\t      const u32 *action,\n\t\t      u32 len,\n\t\t      u32 flags)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.send;\n\tunsigned long spin_flags;\n\tu32 g2h_len_dw = G2H_LEN_DW(flags);\n\tu32 fence;\n\tint ret;\n\n\tspin_lock_irqsave(&ctb->lock, spin_flags);\n\n\tret = has_room_nb(ct, len + GUC_CTB_HDR_LEN, g2h_len_dw);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tfence = ct_get_next_fence(ct);\n\tret = ct_write(ct, action, len, fence, flags);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tg2h_reserve_space(ct, g2h_len_dw);\n\tintel_guc_notify(ct_to_guc(ct));\n\nout:\n\tspin_unlock_irqrestore(&ctb->lock, spin_flags);\n\n\treturn ret;\n}\n\nstatic int ct_send(struct intel_guc_ct *ct,\n\t\t   const u32 *action,\n\t\t   u32 len,\n\t\t   u32 *response_buf,\n\t\t   u32 response_buf_size,\n\t\t   u32 *status)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.send;\n\tstruct ct_request request;\n\tunsigned long flags;\n\tunsigned int sleep_period_ms = 1;\n\tbool send_again;\n\tu32 fence;\n\tint err;\n\n\tGEM_BUG_ON(!ct->enabled);\n\tGEM_BUG_ON(!len);\n\tGEM_BUG_ON(len > GUC_CTB_HXG_MSG_MAX_LEN - GUC_CTB_HDR_LEN);\n\tGEM_BUG_ON(!response_buf && response_buf_size);\n\tmight_sleep();\n\nresend:\n\tsend_again = false;\n\n\t \nretry:\n\tspin_lock_irqsave(&ctb->lock, flags);\n\tif (unlikely(!h2g_has_room(ct, len + GUC_CTB_HDR_LEN) ||\n\t\t     !g2h_has_room(ct, GUC_CTB_HXG_MSG_MAX_LEN))) {\n\t\tif (ct->stall_time == KTIME_MAX)\n\t\t\tct->stall_time = ktime_get();\n\t\tspin_unlock_irqrestore(&ctb->lock, flags);\n\n\t\tif (unlikely(ct_deadlocked(ct)))\n\t\t\treturn -EPIPE;\n\n\t\tif (msleep_interruptible(sleep_period_ms))\n\t\t\treturn -EINTR;\n\t\tsleep_period_ms = sleep_period_ms << 1;\n\n\t\tgoto retry;\n\t}\n\n\tct->stall_time = KTIME_MAX;\n\n\tfence = ct_get_next_fence(ct);\n\trequest.fence = fence;\n\trequest.status = 0;\n\trequest.response_len = response_buf_size;\n\trequest.response_buf = response_buf;\n\n\tspin_lock(&ct->requests.lock);\n\tlist_add_tail(&request.link, &ct->requests.pending);\n\tspin_unlock(&ct->requests.lock);\n\n\terr = ct_write(ct, action, len, fence, 0);\n\tg2h_reserve_space(ct, GUC_CTB_HXG_MSG_MAX_LEN);\n\n\tspin_unlock_irqrestore(&ctb->lock, flags);\n\n\tif (unlikely(err))\n\t\tgoto unlink;\n\n\tintel_guc_notify(ct_to_guc(ct));\n\n\terr = wait_for_ct_request_update(ct, &request, status);\n\tg2h_release_space(ct, GUC_CTB_HXG_MSG_MAX_LEN);\n\tif (unlikely(err)) {\n\t\tif (err == -ENODEV)\n\t\t\t \n\t\t\tCT_DEBUG(ct, \"Request %#x (fence %u) cancelled as CTB is disabled\\n\",\n\t\t\t\t action[0], request.fence);\n\t\telse\n\t\t\tCT_ERROR(ct, \"No response for request %#x (fence %u)\\n\",\n\t\t\t\t action[0], request.fence);\n\t\tgoto unlink;\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, *status) == GUC_HXG_TYPE_NO_RESPONSE_RETRY) {\n\t\tCT_DEBUG(ct, \"retrying request %#x (%u)\\n\", *action,\n\t\t\t FIELD_GET(GUC_HXG_RETRY_MSG_0_REASON, *status));\n\t\tsend_again = true;\n\t\tgoto unlink;\n\t}\n\n\tif (FIELD_GET(GUC_HXG_MSG_0_TYPE, *status) != GUC_HXG_TYPE_RESPONSE_SUCCESS) {\n\t\terr = -EIO;\n\t\tgoto unlink;\n\t}\n\n\tif (response_buf) {\n\t\t \n\t\tWARN_ON(FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, request.status));\n\t\t \n\t\terr = request.response_len;\n\t} else {\n\t\t \n\t\tWARN_ON(request.response_len);\n\t\t \n\t\terr = FIELD_GET(GUC_HXG_RESPONSE_MSG_0_DATA0, *status);\n\t}\n\nunlink:\n\tspin_lock_irqsave(&ct->requests.lock, flags);\n\tlist_del(&request.link);\n\tspin_unlock_irqrestore(&ct->requests.lock, flags);\n\n\tif (unlikely(send_again))\n\t\tgoto resend;\n\n\treturn err;\n}\n\n \nint intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,\n\t\t      u32 *response_buf, u32 response_buf_size, u32 flags)\n{\n\tu32 status = ~0;  \n\tint ret;\n\n\tif (unlikely(!ct->enabled)) {\n\t\tstruct intel_guc *guc = ct_to_guc(ct);\n\t\tstruct intel_uc *uc = container_of(guc, struct intel_uc, guc);\n\n\t\tWARN(!uc->reset_in_progress, \"Unexpected send: action=%#x\\n\", *action);\n\t\treturn -ENODEV;\n\t}\n\n\tif (unlikely(ct->ctbs.send.broken))\n\t\treturn -EPIPE;\n\n\tif (flags & INTEL_GUC_CT_SEND_NB)\n\t\treturn ct_send_nb(ct, action, len, flags);\n\n\tret = ct_send(ct, action, len, response_buf, response_buf_size, &status);\n\tif (unlikely(ret < 0)) {\n\t\tif (ret != -ENODEV)\n\t\t\tCT_ERROR(ct, \"Sending action %#x failed (%pe) status=%#X\\n\",\n\t\t\t\t action[0], ERR_PTR(ret), status);\n\t} else if (unlikely(ret)) {\n\t\tCT_DEBUG(ct, \"send action %#x returned %d (%#x)\\n\",\n\t\t\t action[0], ret, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic struct ct_incoming_msg *ct_alloc_msg(u32 num_dwords)\n{\n\tstruct ct_incoming_msg *msg;\n\n\tmsg = kmalloc(struct_size(msg, msg, num_dwords), GFP_ATOMIC);\n\tif (msg)\n\t\tmsg->size = num_dwords;\n\treturn msg;\n}\n\nstatic void ct_free_msg(struct ct_incoming_msg *msg)\n{\n\tkfree(msg);\n}\n\n \nstatic int ct_read(struct intel_guc_ct *ct, struct ct_incoming_msg **msg)\n{\n\tstruct intel_guc_ct_buffer *ctb = &ct->ctbs.recv;\n\tstruct guc_ct_buffer_desc *desc = ctb->desc;\n\tu32 head = ctb->head;\n\tu32 tail = READ_ONCE(desc->tail);\n\tu32 size = ctb->size;\n\tu32 *cmds = ctb->cmds;\n\ts32 available;\n\tunsigned int len;\n\tunsigned int i;\n\tu32 header;\n\n\tif (unlikely(ctb->broken))\n\t\treturn -EPIPE;\n\n\tif (unlikely(desc->status)) {\n\t\tu32 status = desc->status;\n\n\t\tif (status & GUC_CTB_STATUS_UNUSED) {\n\t\t\t \n\t\t\tCT_ERROR(ct, \"Unexpected G2H after GuC has stopped!\\n\");\n\t\t\tstatus &= ~GUC_CTB_STATUS_UNUSED;\n\t\t}\n\n\t\tif (status)\n\t\t\tgoto corrupted;\n\t}\n\n\tGEM_BUG_ON(head > size);\n\n#ifdef CONFIG_DRM_I915_DEBUG_GUC\n\tif (unlikely(head != READ_ONCE(desc->head))) {\n\t\tCT_ERROR(ct, \"Head was modified %u != %u\\n\",\n\t\t\t desc->head, head);\n\t\tdesc->status |= GUC_CTB_STATUS_MISMATCH;\n\t\tgoto corrupted;\n\t}\n#endif\n\tif (unlikely(tail >= size)) {\n\t\tCT_ERROR(ct, \"Invalid tail offset %u >= %u)\\n\",\n\t\t\t tail, size);\n\t\tdesc->status |= GUC_CTB_STATUS_OVERFLOW;\n\t\tgoto corrupted;\n\t}\n\n\t \n\tavailable = tail - head;\n\tif (unlikely(available == 0)) {\n\t\t*msg = NULL;\n\t\treturn 0;\n\t}\n\n\t \n\tif (unlikely(available < 0))\n\t\tavailable += size;\n\tCT_DEBUG(ct, \"available %d (%u:%u:%u)\\n\", available, head, tail, size);\n\tGEM_BUG_ON(available < 0);\n\n\theader = cmds[head];\n\thead = (head + 1) % size;\n\n\t \n\tlen = FIELD_GET(GUC_CTB_MSG_0_NUM_DWORDS, header) + GUC_CTB_MSG_MIN_LEN;\n\tif (unlikely(len > (u32)available)) {\n\t\tCT_ERROR(ct, \"Incomplete message %*ph %*ph %*ph\\n\",\n\t\t\t 4, &header,\n\t\t\t 4 * (head + available - 1 > size ?\n\t\t\t      size - head : available - 1), &cmds[head],\n\t\t\t 4 * (head + available - 1 > size ?\n\t\t\t      available - 1 - size + head : 0), &cmds[0]);\n\t\tdesc->status |= GUC_CTB_STATUS_UNDERFLOW;\n\t\tgoto corrupted;\n\t}\n\n\t*msg = ct_alloc_msg(len);\n\tif (!*msg) {\n\t\tCT_ERROR(ct, \"No memory for message %*ph %*ph %*ph\\n\",\n\t\t\t 4, &header,\n\t\t\t 4 * (head + available - 1 > size ?\n\t\t\t      size - head : available - 1), &cmds[head],\n\t\t\t 4 * (head + available - 1 > size ?\n\t\t\t      available - 1 - size + head : 0), &cmds[0]);\n\t\treturn available;\n\t}\n\n\t(*msg)->msg[0] = header;\n\n\tfor (i = 1; i < len; i++) {\n\t\t(*msg)->msg[i] = cmds[head];\n\t\thead = (head + 1) % size;\n\t}\n\tCT_DEBUG(ct, \"received %*ph\\n\", 4 * len, (*msg)->msg);\n\n\t \n\tctb->head = head;\n\n\t \n\tWRITE_ONCE(desc->head, head);\n\n\tintel_guc_write_barrier(ct_to_guc(ct));\n\n\treturn available - len;\n\ncorrupted:\n\tCT_ERROR(ct, \"Corrupted descriptor head=%u tail=%u status=%#x\\n\",\n\t\t desc->head, desc->tail, desc->status);\n\tctb->broken = true;\n\tCT_DEAD(ct, READ);\n\treturn -EPIPE;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\nstatic bool ct_check_lost_and_found(struct intel_guc_ct *ct, u32 fence)\n{\n\tunsigned int n;\n\tchar *buf = NULL;\n\tbool found = false;\n\n\tlockdep_assert_held(&ct->requests.lock);\n\n\tfor (n = 0; n < ARRAY_SIZE(ct->requests.lost_and_found); n++) {\n\t\tif (ct->requests.lost_and_found[n].fence != fence)\n\t\t\tcontinue;\n\t\tfound = true;\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\n\t\tbuf = kmalloc(SZ_4K, GFP_NOWAIT);\n\t\tif (buf && stack_depot_snprint(ct->requests.lost_and_found[n].stack,\n\t\t\t\t\t       buf, SZ_4K, 0)) {\n\t\t\tCT_ERROR(ct, \"Fence %u was used by action %#04x sent at\\n%s\",\n\t\t\t\t fence, ct->requests.lost_and_found[n].action, buf);\n\t\t\tbreak;\n\t\t}\n#endif\n\t\tCT_ERROR(ct, \"Fence %u was used by action %#04x\\n\",\n\t\t\t fence, ct->requests.lost_and_found[n].action);\n\t\tbreak;\n\t}\n\tkfree(buf);\n\treturn found;\n}\n#else\nstatic bool ct_check_lost_and_found(struct intel_guc_ct *ct, u32 fence)\n{\n\treturn false;\n}\n#endif\n\nstatic int ct_handle_response(struct intel_guc_ct *ct, struct ct_incoming_msg *response)\n{\n\tu32 len = FIELD_GET(GUC_CTB_MSG_0_NUM_DWORDS, response->msg[0]);\n\tu32 fence = FIELD_GET(GUC_CTB_MSG_0_FENCE, response->msg[0]);\n\tconst u32 *hxg = &response->msg[GUC_CTB_MSG_MIN_LEN];\n\tconst u32 *data = &hxg[GUC_HXG_MSG_MIN_LEN];\n\tu32 datalen = len - GUC_HXG_MSG_MIN_LEN;\n\tstruct ct_request *req;\n\tunsigned long flags;\n\tbool found = false;\n\tint err = 0;\n\n\tGEM_BUG_ON(len < GUC_HXG_MSG_MIN_LEN);\n\tGEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_ORIGIN, hxg[0]) != GUC_HXG_ORIGIN_GUC);\n\tGEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, hxg[0]) != GUC_HXG_TYPE_RESPONSE_SUCCESS &&\n\t\t   FIELD_GET(GUC_HXG_MSG_0_TYPE, hxg[0]) != GUC_HXG_TYPE_NO_RESPONSE_RETRY &&\n\t\t   FIELD_GET(GUC_HXG_MSG_0_TYPE, hxg[0]) != GUC_HXG_TYPE_RESPONSE_FAILURE);\n\n\tCT_DEBUG(ct, \"response fence %u status %#x\\n\", fence, hxg[0]);\n\n\tspin_lock_irqsave(&ct->requests.lock, flags);\n\tlist_for_each_entry(req, &ct->requests.pending, link) {\n\t\tif (unlikely(fence != req->fence)) {\n\t\t\tCT_DEBUG(ct, \"request %u awaits response\\n\",\n\t\t\t\t req->fence);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(datalen > req->response_len)) {\n\t\t\tCT_ERROR(ct, \"Response %u too long (datalen %u > %u)\\n\",\n\t\t\t\t req->fence, datalen, req->response_len);\n\t\t\tdatalen = min(datalen, req->response_len);\n\t\t\terr = -EMSGSIZE;\n\t\t}\n\t\tif (datalen)\n\t\t\tmemcpy(req->response_buf, data, 4 * datalen);\n\t\treq->response_len = datalen;\n\t\tWRITE_ONCE(req->status, hxg[0]);\n\t\tfound = true;\n\t\tbreak;\n\t}\n\tif (!found) {\n\t\tCT_ERROR(ct, \"Unsolicited response message: len %u, data %#x (fence %u, last %u)\\n\",\n\t\t\t len, hxg[0], fence, ct->requests.last_fence);\n\t\tif (!ct_check_lost_and_found(ct, fence)) {\n\t\t\tlist_for_each_entry(req, &ct->requests.pending, link)\n\t\t\t\tCT_ERROR(ct, \"request %u awaits response\\n\",\n\t\t\t\t\t req->fence);\n\t\t}\n\t\terr = -ENOKEY;\n\t}\n\tspin_unlock_irqrestore(&ct->requests.lock, flags);\n\n\tif (unlikely(err))\n\t\treturn err;\n\n\tct_free_msg(response);\n\treturn 0;\n}\n\nstatic int ct_process_request(struct intel_guc_ct *ct, struct ct_incoming_msg *request)\n{\n\tstruct intel_guc *guc = ct_to_guc(ct);\n\tconst u32 *hxg;\n\tconst u32 *payload;\n\tu32 hxg_len, action, len;\n\tint ret;\n\n\thxg = &request->msg[GUC_CTB_MSG_MIN_LEN];\n\thxg_len = request->size - GUC_CTB_MSG_MIN_LEN;\n\tpayload = &hxg[GUC_HXG_MSG_MIN_LEN];\n\taction = FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, hxg[0]);\n\tlen = hxg_len - GUC_HXG_MSG_MIN_LEN;\n\n\tCT_DEBUG(ct, \"request %x %*ph\\n\", action, 4 * len, payload);\n\n\tswitch (action) {\n\tcase INTEL_GUC_ACTION_DEFAULT:\n\t\tret = intel_guc_to_host_process_recv_msg(guc, payload, len);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_DEREGISTER_CONTEXT_DONE:\n\t\tret = intel_guc_deregister_done_process_msg(guc, payload,\n\t\t\t\t\t\t\t    len);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_SCHED_CONTEXT_MODE_DONE:\n\t\tret = intel_guc_sched_done_process_msg(guc, payload, len);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_CONTEXT_RESET_NOTIFICATION:\n\t\tret = intel_guc_context_reset_process_msg(guc, payload, len);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_STATE_CAPTURE_NOTIFICATION:\n\t\tret = intel_guc_error_capture_process_msg(guc, payload, len);\n\t\tif (unlikely(ret))\n\t\t\tCT_ERROR(ct, \"error capture notification failed %x %*ph\\n\",\n\t\t\t\t action, 4 * len, payload);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_ENGINE_FAILURE_NOTIFICATION:\n\t\tret = intel_guc_engine_failure_process_msg(guc, payload, len);\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_NOTIFY_FLUSH_LOG_BUFFER_TO_FILE:\n\t\tintel_guc_log_handle_flush_event(&guc->log);\n\t\tret = 0;\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_NOTIFY_CRASH_DUMP_POSTED:\n\t\tCT_ERROR(ct, \"Received GuC crash dump notification!\\n\");\n\t\tret = 0;\n\t\tbreak;\n\tcase INTEL_GUC_ACTION_NOTIFY_EXCEPTION:\n\t\tCT_ERROR(ct, \"Received GuC exception notification!\\n\");\n\t\tret = 0;\n\t\tbreak;\n\tdefault:\n\t\tret = -EOPNOTSUPP;\n\t\tbreak;\n\t}\n\n\tif (unlikely(ret)) {\n\t\tCT_ERROR(ct, \"Failed to process request %04x (%pe)\\n\",\n\t\t\t action, ERR_PTR(ret));\n\t\treturn ret;\n\t}\n\n\tct_free_msg(request);\n\treturn 0;\n}\n\nstatic bool ct_process_incoming_requests(struct intel_guc_ct *ct)\n{\n\tunsigned long flags;\n\tstruct ct_incoming_msg *request;\n\tbool done;\n\tint err;\n\n\tspin_lock_irqsave(&ct->requests.lock, flags);\n\trequest = list_first_entry_or_null(&ct->requests.incoming,\n\t\t\t\t\t   struct ct_incoming_msg, link);\n\tif (request)\n\t\tlist_del(&request->link);\n\tdone = !!list_empty(&ct->requests.incoming);\n\tspin_unlock_irqrestore(&ct->requests.lock, flags);\n\n\tif (!request)\n\t\treturn true;\n\n\terr = ct_process_request(ct, request);\n\tif (unlikely(err)) {\n\t\tCT_ERROR(ct, \"Failed to process CT message (%pe) %*ph\\n\",\n\t\t\t ERR_PTR(err), 4 * request->size, request->msg);\n\t\tCT_DEAD(ct, PROCESS_FAILED);\n\t\tct_free_msg(request);\n\t}\n\n\treturn done;\n}\n\nstatic void ct_incoming_request_worker_func(struct work_struct *w)\n{\n\tstruct intel_guc_ct *ct =\n\t\tcontainer_of(w, struct intel_guc_ct, requests.worker);\n\tbool done;\n\n\tdo {\n\t\tdone = ct_process_incoming_requests(ct);\n\t} while (!done);\n}\n\nstatic int ct_handle_event(struct intel_guc_ct *ct, struct ct_incoming_msg *request)\n{\n\tconst u32 *hxg = &request->msg[GUC_CTB_MSG_MIN_LEN];\n\tu32 action = FIELD_GET(GUC_HXG_EVENT_MSG_0_ACTION, hxg[0]);\n\tunsigned long flags;\n\n\tGEM_BUG_ON(FIELD_GET(GUC_HXG_MSG_0_TYPE, hxg[0]) != GUC_HXG_TYPE_EVENT);\n\n\t \n\tswitch (action) {\n\tcase INTEL_GUC_ACTION_SCHED_CONTEXT_MODE_DONE:\n\tcase INTEL_GUC_ACTION_DEREGISTER_CONTEXT_DONE:\n\t\tg2h_release_space(ct, request->size);\n\t}\n\n\tspin_lock_irqsave(&ct->requests.lock, flags);\n\tlist_add_tail(&request->link, &ct->requests.incoming);\n\tspin_unlock_irqrestore(&ct->requests.lock, flags);\n\n\tqueue_work(system_unbound_wq, &ct->requests.worker);\n\treturn 0;\n}\n\nstatic int ct_handle_hxg(struct intel_guc_ct *ct, struct ct_incoming_msg *msg)\n{\n\tu32 origin, type;\n\tu32 *hxg;\n\tint err;\n\n\tif (unlikely(msg->size < GUC_CTB_HXG_MSG_MIN_LEN))\n\t\treturn -EBADMSG;\n\n\thxg = &msg->msg[GUC_CTB_MSG_MIN_LEN];\n\n\torigin = FIELD_GET(GUC_HXG_MSG_0_ORIGIN, hxg[0]);\n\tif (unlikely(origin != GUC_HXG_ORIGIN_GUC)) {\n\t\terr = -EPROTO;\n\t\tgoto failed;\n\t}\n\n\ttype = FIELD_GET(GUC_HXG_MSG_0_TYPE, hxg[0]);\n\tswitch (type) {\n\tcase GUC_HXG_TYPE_EVENT:\n\t\terr = ct_handle_event(ct, msg);\n\t\tbreak;\n\tcase GUC_HXG_TYPE_RESPONSE_SUCCESS:\n\tcase GUC_HXG_TYPE_RESPONSE_FAILURE:\n\tcase GUC_HXG_TYPE_NO_RESPONSE_RETRY:\n\t\terr = ct_handle_response(ct, msg);\n\t\tbreak;\n\tdefault:\n\t\terr = -EOPNOTSUPP;\n\t}\n\n\tif (unlikely(err)) {\nfailed:\n\t\tCT_ERROR(ct, \"Failed to handle HXG message (%pe) %*ph\\n\",\n\t\t\t ERR_PTR(err), 4 * GUC_HXG_MSG_MIN_LEN, hxg);\n\t}\n\treturn err;\n}\n\nstatic void ct_handle_msg(struct intel_guc_ct *ct, struct ct_incoming_msg *msg)\n{\n\tu32 format = FIELD_GET(GUC_CTB_MSG_0_FORMAT, msg->msg[0]);\n\tint err;\n\n\tif (format == GUC_CTB_FORMAT_HXG)\n\t\terr = ct_handle_hxg(ct, msg);\n\telse\n\t\terr = -EOPNOTSUPP;\n\n\tif (unlikely(err)) {\n\t\tCT_ERROR(ct, \"Failed to process CT message (%pe) %*ph\\n\",\n\t\t\t ERR_PTR(err), 4 * msg->size, msg->msg);\n\t\tct_free_msg(msg);\n\t}\n}\n\n \nstatic int ct_receive(struct intel_guc_ct *ct)\n{\n\tstruct ct_incoming_msg *msg = NULL;\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&ct->ctbs.recv.lock, flags);\n\tret = ct_read(ct, &msg);\n\tspin_unlock_irqrestore(&ct->ctbs.recv.lock, flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (msg)\n\t\tct_handle_msg(ct, msg);\n\n\treturn ret;\n}\n\nstatic void ct_try_receive_message(struct intel_guc_ct *ct)\n{\n\tint ret;\n\n\tif (GEM_WARN_ON(!ct->enabled))\n\t\treturn;\n\n\tret = ct_receive(ct);\n\tif (ret > 0)\n\t\ttasklet_hi_schedule(&ct->receive_tasklet);\n}\n\nstatic void ct_receive_tasklet_func(struct tasklet_struct *t)\n{\n\tstruct intel_guc_ct *ct = from_tasklet(ct, t, receive_tasklet);\n\n\tct_try_receive_message(ct);\n}\n\n \nvoid intel_guc_ct_event_handler(struct intel_guc_ct *ct)\n{\n\tif (unlikely(!ct->enabled)) {\n\t\tWARN(1, \"Unexpected GuC event received while CT disabled!\\n\");\n\t\treturn;\n\t}\n\n\tct_try_receive_message(ct);\n}\n\nvoid intel_guc_ct_print_info(struct intel_guc_ct *ct,\n\t\t\t     struct drm_printer *p)\n{\n\tdrm_printf(p, \"CT %s\\n\", str_enabled_disabled(ct->enabled));\n\n\tif (!ct->enabled)\n\t\treturn;\n\n\tdrm_printf(p, \"H2G Space: %u\\n\",\n\t\t   atomic_read(&ct->ctbs.send.space) * 4);\n\tdrm_printf(p, \"Head: %u\\n\",\n\t\t   ct->ctbs.send.desc->head);\n\tdrm_printf(p, \"Tail: %u\\n\",\n\t\t   ct->ctbs.send.desc->tail);\n\tdrm_printf(p, \"G2H Space: %u\\n\",\n\t\t   atomic_read(&ct->ctbs.recv.space) * 4);\n\tdrm_printf(p, \"Head: %u\\n\",\n\t\t   ct->ctbs.recv.desc->head);\n\tdrm_printf(p, \"Tail: %u\\n\",\n\t\t   ct->ctbs.recv.desc->tail);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GUC)\nstatic void ct_dead_ct_worker_func(struct work_struct *w)\n{\n\tstruct intel_guc_ct *ct = container_of(w, struct intel_guc_ct, dead_ct_worker);\n\tstruct intel_guc *guc = ct_to_guc(ct);\n\n\tif (ct->dead_ct_reported)\n\t\treturn;\n\n\tct->dead_ct_reported = true;\n\n\tguc_info(guc, \"CTB is dead - reason=0x%X\\n\", ct->dead_ct_reason);\n\tintel_klog_error_capture(guc_to_gt(guc), (intel_engine_mask_t)~0U);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}