{
  "module_name": "intel_execlists_submission.c",
  "hash_id": "f8c8afb7f7d78aea551c187433ef2f17990fcf77729e946445562b11259dde75",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_execlists_submission.c",
  "human_readable_source": "\n \n\n \n#include <linux/interrupt.h>\n#include <linux/string_helpers.h>\n\n#include \"i915_drv.h\"\n#include \"i915_reg.h\"\n#include \"i915_trace.h\"\n#include \"i915_vgpu.h\"\n#include \"gen8_engine_cs.h\"\n#include \"intel_breadcrumbs.h\"\n#include \"intel_context.h\"\n#include \"intel_engine_heartbeat.h\"\n#include \"intel_engine_pm.h\"\n#include \"intel_engine_regs.h\"\n#include \"intel_engine_stats.h\"\n#include \"intel_execlists_submission.h\"\n#include \"intel_gt.h\"\n#include \"intel_gt_irq.h\"\n#include \"intel_gt_pm.h\"\n#include \"intel_gt_regs.h\"\n#include \"intel_gt_requests.h\"\n#include \"intel_lrc.h\"\n#include \"intel_lrc_reg.h\"\n#include \"intel_mocs.h\"\n#include \"intel_reset.h\"\n#include \"intel_ring.h\"\n#include \"intel_workarounds.h\"\n#include \"shmem_utils.h\"\n\n#define RING_EXECLIST_QFULL\t\t(1 << 0x2)\n#define RING_EXECLIST1_VALID\t\t(1 << 0x3)\n#define RING_EXECLIST0_VALID\t\t(1 << 0x4)\n#define RING_EXECLIST_ACTIVE_STATUS\t(3 << 0xE)\n#define RING_EXECLIST1_ACTIVE\t\t(1 << 0x11)\n#define RING_EXECLIST0_ACTIVE\t\t(1 << 0x12)\n\n#define GEN8_CTX_STATUS_IDLE_ACTIVE\t(1 << 0)\n#define GEN8_CTX_STATUS_PREEMPTED\t(1 << 1)\n#define GEN8_CTX_STATUS_ELEMENT_SWITCH\t(1 << 2)\n#define GEN8_CTX_STATUS_ACTIVE_IDLE\t(1 << 3)\n#define GEN8_CTX_STATUS_COMPLETE\t(1 << 4)\n#define GEN8_CTX_STATUS_LITE_RESTORE\t(1 << 15)\n\n#define GEN8_CTX_STATUS_COMPLETED_MASK \\\n\t (GEN8_CTX_STATUS_COMPLETE | GEN8_CTX_STATUS_PREEMPTED)\n\n#define GEN12_CTX_STATUS_SWITCHED_TO_NEW_QUEUE\t(0x1)  \n#define GEN12_CTX_SWITCH_DETAIL(csb_dw)\t((csb_dw) & 0xF)  \n#define GEN12_CSB_SW_CTX_ID_MASK\t\tGENMASK(25, 15)\n#define GEN12_IDLE_CTX_ID\t\t0x7FF\n#define GEN12_CSB_CTX_VALID(csb_dw) \\\n\t(FIELD_GET(GEN12_CSB_SW_CTX_ID_MASK, csb_dw) != GEN12_IDLE_CTX_ID)\n\n#define XEHP_CTX_STATUS_SWITCHED_TO_NEW_QUEUE\tBIT(1)  \n#define XEHP_CSB_SW_CTX_ID_MASK\t\t\tGENMASK(31, 10)\n#define XEHP_IDLE_CTX_ID\t\t\t0xFFFF\n#define XEHP_CSB_CTX_VALID(csb_dw) \\\n\t(FIELD_GET(XEHP_CSB_SW_CTX_ID_MASK, csb_dw) != XEHP_IDLE_CTX_ID)\n\n \n#define EXECLISTS_REQUEST_SIZE 64  \n\nstruct virtual_engine {\n\tstruct intel_engine_cs base;\n\tstruct intel_context context;\n\tstruct rcu_work rcu;\n\n\t \n\tstruct i915_request *request;\n\n\t \n\tstruct ve_node {\n\t\tstruct rb_node rb;\n\t\tint prio;\n\t} nodes[I915_NUM_ENGINES];\n\n\t \n\tunsigned int num_siblings;\n\tstruct intel_engine_cs *siblings[];\n};\n\nstatic struct virtual_engine *to_virtual_engine(struct intel_engine_cs *engine)\n{\n\tGEM_BUG_ON(!intel_engine_is_virtual(engine));\n\treturn container_of(engine, struct virtual_engine, base);\n}\n\nstatic struct intel_context *\nexeclists_create_virtual(struct intel_engine_cs **siblings, unsigned int count,\n\t\t\t unsigned long flags);\n\nstatic struct i915_request *\n__active_request(const struct intel_timeline * const tl,\n\t\t struct i915_request *rq,\n\t\t int error)\n{\n\tstruct i915_request *active = rq;\n\n\tlist_for_each_entry_from_reverse(rq, &tl->requests, link) {\n\t\tif (__i915_request_is_complete(rq))\n\t\t\tbreak;\n\n\t\tif (error) {\n\t\t\ti915_request_set_error_once(rq, error);\n\t\t\t__i915_request_skip(rq);\n\t\t}\n\t\tactive = rq;\n\t}\n\n\treturn active;\n}\n\nstatic struct i915_request *\nactive_request(const struct intel_timeline * const tl, struct i915_request *rq)\n{\n\treturn __active_request(tl, rq, 0);\n}\n\nstatic void ring_set_paused(const struct intel_engine_cs *engine, int state)\n{\n\t \n\tengine->status_page.addr[I915_GEM_HWS_PREEMPT] = state;\n\tif (state)\n\t\twmb();\n}\n\nstatic struct i915_priolist *to_priolist(struct rb_node *rb)\n{\n\treturn rb_entry(rb, struct i915_priolist, node);\n}\n\nstatic int rq_prio(const struct i915_request *rq)\n{\n\treturn READ_ONCE(rq->sched.attr.priority);\n}\n\nstatic int effective_prio(const struct i915_request *rq)\n{\n\tint prio = rq_prio(rq);\n\n\t \n\tif (i915_request_has_nopreempt(rq))\n\t\tprio = I915_PRIORITY_UNPREEMPTABLE;\n\n\treturn prio;\n}\n\nstatic int queue_prio(const struct i915_sched_engine *sched_engine)\n{\n\tstruct rb_node *rb;\n\n\trb = rb_first_cached(&sched_engine->queue);\n\tif (!rb)\n\t\treturn INT_MIN;\n\n\treturn to_priolist(rb)->priority;\n}\n\nstatic int virtual_prio(const struct intel_engine_execlists *el)\n{\n\tstruct rb_node *rb = rb_first_cached(&el->virtual);\n\n\treturn rb ? rb_entry(rb, struct ve_node, rb)->prio : INT_MIN;\n}\n\nstatic bool need_preempt(const struct intel_engine_cs *engine,\n\t\t\t const struct i915_request *rq)\n{\n\tint last_prio;\n\n\tif (!intel_engine_has_semaphores(engine))\n\t\treturn false;\n\n\t \n\tlast_prio = max(effective_prio(rq), I915_PRIORITY_NORMAL - 1);\n\tif (engine->sched_engine->queue_priority_hint <= last_prio)\n\t\treturn false;\n\n\t \n\tif (!list_is_last(&rq->sched.link, &engine->sched_engine->requests) &&\n\t    rq_prio(list_next_entry(rq, sched.link)) > last_prio)\n\t\treturn true;\n\n\t \n\treturn max(virtual_prio(&engine->execlists),\n\t\t   queue_prio(engine->sched_engine)) > last_prio;\n}\n\n__maybe_unused static bool\nassert_priority_queue(const struct i915_request *prev,\n\t\t      const struct i915_request *next)\n{\n\t \n\tif (i915_request_is_active(prev))\n\t\treturn true;\n\n\treturn rq_prio(prev) >= rq_prio(next);\n}\n\nstatic struct i915_request *\n__unwind_incomplete_requests(struct intel_engine_cs *engine)\n{\n\tstruct i915_request *rq, *rn, *active = NULL;\n\tstruct list_head *pl;\n\tint prio = I915_PRIORITY_INVALID;\n\n\tlockdep_assert_held(&engine->sched_engine->lock);\n\n\tlist_for_each_entry_safe_reverse(rq, rn,\n\t\t\t\t\t &engine->sched_engine->requests,\n\t\t\t\t\t sched.link) {\n\t\tif (__i915_request_is_complete(rq)) {\n\t\t\tlist_del_init(&rq->sched.link);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__i915_request_unsubmit(rq);\n\n\t\tGEM_BUG_ON(rq_prio(rq) == I915_PRIORITY_INVALID);\n\t\tif (rq_prio(rq) != prio) {\n\t\t\tprio = rq_prio(rq);\n\t\t\tpl = i915_sched_lookup_priolist(engine->sched_engine,\n\t\t\t\t\t\t\tprio);\n\t\t}\n\t\tGEM_BUG_ON(i915_sched_engine_is_empty(engine->sched_engine));\n\n\t\tlist_move(&rq->sched.link, pl);\n\t\tset_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\n\t\t \n\t\tif (intel_ring_direction(rq->ring,\n\t\t\t\t\t rq->tail,\n\t\t\t\t\t rq->ring->tail + 8) > 0)\n\t\t\trq->context->lrc.desc |= CTX_DESC_FORCE_RESTORE;\n\n\t\tactive = rq;\n\t}\n\n\treturn active;\n}\n\nstruct i915_request *\nexeclists_unwind_incomplete_requests(struct intel_engine_execlists *execlists)\n{\n\tstruct intel_engine_cs *engine =\n\t\tcontainer_of(execlists, typeof(*engine), execlists);\n\n\treturn __unwind_incomplete_requests(engine);\n}\n\nstatic void\nexeclists_context_status_change(struct i915_request *rq, unsigned long status)\n{\n\t \n\tif (!IS_ENABLED(CONFIG_DRM_I915_GVT))\n\t\treturn;\n\n\tatomic_notifier_call_chain(&rq->engine->context_status_notifier,\n\t\t\t\t   status, rq);\n}\n\nstatic void reset_active(struct i915_request *rq,\n\t\t\t struct intel_engine_cs *engine)\n{\n\tstruct intel_context * const ce = rq->context;\n\tu32 head;\n\n\t \n\tENGINE_TRACE(engine, \"{ reset rq=%llx:%lld }\\n\",\n\t\t     rq->fence.context, rq->fence.seqno);\n\n\t \n\tif (__i915_request_is_complete(rq))\n\t\thead = rq->tail;\n\telse\n\t\thead = __active_request(ce->timeline, rq, -EIO)->head;\n\thead = intel_ring_wrap(ce->ring, head);\n\n\t \n\tlrc_init_regs(ce, engine, true);\n\n\t \n\tce->lrc.lrca = lrc_update_regs(ce, engine, head);\n}\n\nstatic bool bad_request(const struct i915_request *rq)\n{\n\treturn rq->fence.error && i915_request_started(rq);\n}\n\nstatic struct intel_engine_cs *\n__execlists_schedule_in(struct i915_request *rq)\n{\n\tstruct intel_engine_cs * const engine = rq->engine;\n\tstruct intel_context * const ce = rq->context;\n\n\tintel_context_get(ce);\n\n\tif (unlikely(intel_context_is_closed(ce) &&\n\t\t     !intel_engine_has_heartbeat(engine)))\n\t\tintel_context_set_exiting(ce);\n\n\tif (unlikely(!intel_context_is_schedulable(ce) || bad_request(rq)))\n\t\treset_active(rq, engine);\n\n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tlrc_check_regs(ce, engine, \"before\");\n\n\tif (ce->tag) {\n\t\t \n\t\tGEM_BUG_ON(ce->tag <= BITS_PER_LONG);\n\t\tce->lrc.ccid = ce->tag;\n\t} else if (GRAPHICS_VER_FULL(engine->i915) >= IP_VER(12, 50)) {\n\t\t \n\t\tunsigned int tag = ffs(READ_ONCE(engine->context_tag));\n\n\t\tGEM_BUG_ON(tag == 0 || tag >= BITS_PER_LONG);\n\t\tclear_bit(tag - 1, &engine->context_tag);\n\t\tce->lrc.ccid = tag << (XEHP_SW_CTX_ID_SHIFT - 32);\n\n\t\tBUILD_BUG_ON(BITS_PER_LONG > GEN12_MAX_CONTEXT_HW_ID);\n\n\t} else {\n\t\t \n\t\tunsigned int tag = __ffs(engine->context_tag);\n\n\t\tGEM_BUG_ON(tag >= BITS_PER_LONG);\n\t\t__clear_bit(tag, &engine->context_tag);\n\t\tce->lrc.ccid = (1 + tag) << (GEN11_SW_CTX_ID_SHIFT - 32);\n\n\t\tBUILD_BUG_ON(BITS_PER_LONG > GEN12_MAX_CONTEXT_HW_ID);\n\t}\n\n\tce->lrc.ccid |= engine->execlists.ccid;\n\n\t__intel_gt_pm_get(engine->gt);\n\tif (engine->fw_domain && !engine->fw_active++)\n\t\tintel_uncore_forcewake_get(engine->uncore, engine->fw_domain);\n\texeclists_context_status_change(rq, INTEL_CONTEXT_SCHEDULE_IN);\n\tintel_engine_context_in(engine);\n\n\tCE_TRACE(ce, \"schedule-in, ccid:%x\\n\", ce->lrc.ccid);\n\n\treturn engine;\n}\n\nstatic void execlists_schedule_in(struct i915_request *rq, int idx)\n{\n\tstruct intel_context * const ce = rq->context;\n\tstruct intel_engine_cs *old;\n\n\tGEM_BUG_ON(!intel_engine_pm_is_awake(rq->engine));\n\ttrace_i915_request_in(rq, idx);\n\n\told = ce->inflight;\n\tif (!old)\n\t\told = __execlists_schedule_in(rq);\n\tWRITE_ONCE(ce->inflight, ptr_inc(old));\n\n\tGEM_BUG_ON(intel_context_inflight(ce) != rq->engine);\n}\n\nstatic void\nresubmit_virtual_request(struct i915_request *rq, struct virtual_engine *ve)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\n\tspin_lock_irq(&engine->sched_engine->lock);\n\n\tclear_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\tWRITE_ONCE(rq->engine, &ve->base);\n\tve->base.submit_request(rq);\n\n\tspin_unlock_irq(&engine->sched_engine->lock);\n}\n\nstatic void kick_siblings(struct i915_request *rq, struct intel_context *ce)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\tstruct intel_engine_cs *engine = rq->engine;\n\n\t \n\tif (!list_empty(&ce->signals))\n\t\tintel_context_remove_breadcrumbs(ce, engine->breadcrumbs);\n\n\t \n\tif (i915_request_in_priority_queue(rq) &&\n\t    rq->execution_mask != engine->mask)\n\t\tresubmit_virtual_request(rq, ve);\n\n\tif (READ_ONCE(ve->request))\n\t\ttasklet_hi_schedule(&ve->base.sched_engine->tasklet);\n}\n\nstatic void __execlists_schedule_out(struct i915_request * const rq,\n\t\t\t\t     struct intel_context * const ce)\n{\n\tstruct intel_engine_cs * const engine = rq->engine;\n\tunsigned int ccid;\n\n\t \n\n\tCE_TRACE(ce, \"schedule-out, ccid:%x\\n\", ce->lrc.ccid);\n\tGEM_BUG_ON(ce->inflight != engine);\n\n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tlrc_check_regs(ce, engine, \"after\");\n\n\t \n\tif (intel_timeline_is_last(ce->timeline, rq) &&\n\t    __i915_request_is_complete(rq))\n\t\tintel_engine_add_retire(engine, ce->timeline);\n\n\tccid = ce->lrc.ccid;\n\tif (GRAPHICS_VER_FULL(engine->i915) >= IP_VER(12, 50)) {\n\t\tccid >>= XEHP_SW_CTX_ID_SHIFT - 32;\n\t\tccid &= XEHP_MAX_CONTEXT_HW_ID;\n\t} else {\n\t\tccid >>= GEN11_SW_CTX_ID_SHIFT - 32;\n\t\tccid &= GEN12_MAX_CONTEXT_HW_ID;\n\t}\n\n\tif (ccid < BITS_PER_LONG) {\n\t\tGEM_BUG_ON(ccid == 0);\n\t\tGEM_BUG_ON(test_bit(ccid - 1, &engine->context_tag));\n\t\t__set_bit(ccid - 1, &engine->context_tag);\n\t}\n\tintel_engine_context_out(engine);\n\texeclists_context_status_change(rq, INTEL_CONTEXT_SCHEDULE_OUT);\n\tif (engine->fw_domain && !--engine->fw_active)\n\t\tintel_uncore_forcewake_put(engine->uncore, engine->fw_domain);\n\tintel_gt_pm_put_async(engine->gt);\n\n\t \n\tif (ce->engine != engine)\n\t\tkick_siblings(rq, ce);\n\n\tWRITE_ONCE(ce->inflight, NULL);\n\tintel_context_put(ce);\n}\n\nstatic inline void execlists_schedule_out(struct i915_request *rq)\n{\n\tstruct intel_context * const ce = rq->context;\n\n\ttrace_i915_request_out(rq);\n\n\tGEM_BUG_ON(!ce->inflight);\n\tce->inflight = ptr_dec(ce->inflight);\n\tif (!__intel_context_inflight_count(ce->inflight))\n\t\t__execlists_schedule_out(rq, ce);\n\n\ti915_request_put(rq);\n}\n\nstatic u32 map_i915_prio_to_lrc_desc_prio(int prio)\n{\n\tif (prio > I915_PRIORITY_NORMAL)\n\t\treturn GEN12_CTX_PRIORITY_HIGH;\n\telse if (prio < I915_PRIORITY_NORMAL)\n\t\treturn GEN12_CTX_PRIORITY_LOW;\n\telse\n\t\treturn GEN12_CTX_PRIORITY_NORMAL;\n}\n\nstatic u64 execlists_update_context(struct i915_request *rq)\n{\n\tstruct intel_context *ce = rq->context;\n\tu64 desc;\n\tu32 tail, prev;\n\n\tdesc = ce->lrc.desc;\n\tif (rq->engine->flags & I915_ENGINE_HAS_EU_PRIORITY)\n\t\tdesc |= map_i915_prio_to_lrc_desc_prio(rq_prio(rq));\n\n\t \n\tGEM_BUG_ON(ce->lrc_reg_state[CTX_RING_TAIL] != rq->ring->tail);\n\tprev = rq->ring->tail;\n\ttail = intel_ring_set_tail(rq->ring, rq->tail);\n\tif (unlikely(intel_ring_direction(rq->ring, tail, prev) <= 0))\n\t\tdesc |= CTX_DESC_FORCE_RESTORE;\n\tce->lrc_reg_state[CTX_RING_TAIL] = tail;\n\trq->tail = rq->wa_tail;\n\n\t \n\twmb();\n\n\tce->lrc.desc &= ~CTX_DESC_FORCE_RESTORE;\n\treturn desc;\n}\n\nstatic void write_desc(struct intel_engine_execlists *execlists, u64 desc, u32 port)\n{\n\tif (execlists->ctrl_reg) {\n\t\twritel(lower_32_bits(desc), execlists->submit_reg + port * 2);\n\t\twritel(upper_32_bits(desc), execlists->submit_reg + port * 2 + 1);\n\t} else {\n\t\twritel(upper_32_bits(desc), execlists->submit_reg);\n\t\twritel(lower_32_bits(desc), execlists->submit_reg);\n\t}\n}\n\nstatic __maybe_unused char *\ndump_port(char *buf, int buflen, const char *prefix, struct i915_request *rq)\n{\n\tif (!rq)\n\t\treturn \"\";\n\n\tsnprintf(buf, buflen, \"%sccid:%x %llx:%lld%s prio %d\",\n\t\t prefix,\n\t\t rq->context->lrc.ccid,\n\t\t rq->fence.context, rq->fence.seqno,\n\t\t __i915_request_is_complete(rq) ? \"!\" :\n\t\t __i915_request_has_started(rq) ? \"*\" :\n\t\t \"\",\n\t\t rq_prio(rq));\n\n\treturn buf;\n}\n\nstatic __maybe_unused noinline void\ntrace_ports(const struct intel_engine_execlists *execlists,\n\t    const char *msg,\n\t    struct i915_request * const *ports)\n{\n\tconst struct intel_engine_cs *engine =\n\t\tcontainer_of(execlists, typeof(*engine), execlists);\n\tchar __maybe_unused p0[40], p1[40];\n\n\tif (!ports[0])\n\t\treturn;\n\n\tENGINE_TRACE(engine, \"%s { %s%s }\\n\", msg,\n\t\t     dump_port(p0, sizeof(p0), \"\", ports[0]),\n\t\t     dump_port(p1, sizeof(p1), \", \", ports[1]));\n}\n\nstatic bool\nreset_in_progress(const struct intel_engine_cs *engine)\n{\n\treturn unlikely(!__tasklet_is_enabled(&engine->sched_engine->tasklet));\n}\n\nstatic __maybe_unused noinline bool\nassert_pending_valid(const struct intel_engine_execlists *execlists,\n\t\t     const char *msg)\n{\n\tstruct intel_engine_cs *engine =\n\t\tcontainer_of(execlists, typeof(*engine), execlists);\n\tstruct i915_request * const *port, *rq, *prev = NULL;\n\tstruct intel_context *ce = NULL;\n\tu32 ccid = -1;\n\n\ttrace_ports(execlists, msg, execlists->pending);\n\n\t \n\tif (reset_in_progress(engine))\n\t\treturn true;\n\n\tif (!execlists->pending[0]) {\n\t\tGEM_TRACE_ERR(\"%s: Nothing pending for promotion!\\n\",\n\t\t\t      engine->name);\n\t\treturn false;\n\t}\n\n\tif (execlists->pending[execlists_num_ports(execlists)]) {\n\t\tGEM_TRACE_ERR(\"%s: Excess pending[%d] for promotion!\\n\",\n\t\t\t      engine->name, execlists_num_ports(execlists));\n\t\treturn false;\n\t}\n\n\tfor (port = execlists->pending; (rq = *port); port++) {\n\t\tunsigned long flags;\n\t\tbool ok = true;\n\n\t\tGEM_BUG_ON(!kref_read(&rq->fence.refcount));\n\t\tGEM_BUG_ON(!i915_request_is_active(rq));\n\n\t\tif (ce == rq->context) {\n\t\t\tGEM_TRACE_ERR(\"%s: Dup context:%llx in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\treturn false;\n\t\t}\n\t\tce = rq->context;\n\n\t\tif (ccid == ce->lrc.ccid) {\n\t\t\tGEM_TRACE_ERR(\"%s: Dup ccid:%x context:%llx in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ccid, ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\treturn false;\n\t\t}\n\t\tccid = ce->lrc.ccid;\n\n\t\t \n\t\tif (prev && i915_request_has_sentinel(prev) &&\n\t\t    !READ_ONCE(prev->fence.error)) {\n\t\t\tGEM_TRACE_ERR(\"%s: context:%llx after sentinel in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\treturn false;\n\t\t}\n\t\tprev = rq;\n\n\t\t \n\t\tif (rq->execution_mask != engine->mask &&\n\t\t    port != execlists->pending) {\n\t\t\tGEM_TRACE_ERR(\"%s: virtual engine:%llx not in prime position[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\treturn false;\n\t\t}\n\n\t\t \n\t\tif (!spin_trylock_irqsave(&rq->lock, flags))\n\t\t\tcontinue;\n\n\t\tif (__i915_request_is_complete(rq))\n\t\t\tgoto unlock;\n\n\t\tif (i915_active_is_idle(&ce->active) &&\n\t\t    !intel_context_is_barrier(ce)) {\n\t\t\tGEM_TRACE_ERR(\"%s: Inactive context:%llx in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\tok = false;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (!i915_vma_is_pinned(ce->state)) {\n\t\t\tGEM_TRACE_ERR(\"%s: Unpinned context:%llx in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\tok = false;\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (!i915_vma_is_pinned(ce->ring->vma)) {\n\t\t\tGEM_TRACE_ERR(\"%s: Unpinned ring:%llx in pending[%zd]\\n\",\n\t\t\t\t      engine->name,\n\t\t\t\t      ce->timeline->fence_context,\n\t\t\t\t      port - execlists->pending);\n\t\t\tok = false;\n\t\t\tgoto unlock;\n\t\t}\n\nunlock:\n\t\tspin_unlock_irqrestore(&rq->lock, flags);\n\t\tif (!ok)\n\t\t\treturn false;\n\t}\n\n\treturn ce;\n}\n\nstatic void execlists_submit_ports(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists *execlists = &engine->execlists;\n\tunsigned int n;\n\n\tGEM_BUG_ON(!assert_pending_valid(execlists, \"submit\"));\n\n\t \n\tGEM_BUG_ON(!intel_engine_pm_is_awake(engine));\n\n\t \n\tfor (n = execlists_num_ports(execlists); n--; ) {\n\t\tstruct i915_request *rq = execlists->pending[n];\n\n\t\twrite_desc(execlists,\n\t\t\t   rq ? execlists_update_context(rq) : 0,\n\t\t\t   n);\n\t}\n\n\t \n\tif (execlists->ctrl_reg)\n\t\twritel(EL_CTRL_LOAD, execlists->ctrl_reg);\n}\n\nstatic bool ctx_single_port_submission(const struct intel_context *ce)\n{\n\treturn (IS_ENABLED(CONFIG_DRM_I915_GVT) &&\n\t\tintel_context_force_single_submission(ce));\n}\n\nstatic bool can_merge_ctx(const struct intel_context *prev,\n\t\t\t  const struct intel_context *next)\n{\n\tif (prev != next)\n\t\treturn false;\n\n\tif (ctx_single_port_submission(prev))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic unsigned long i915_request_flags(const struct i915_request *rq)\n{\n\treturn READ_ONCE(rq->fence.flags);\n}\n\nstatic bool can_merge_rq(const struct i915_request *prev,\n\t\t\t const struct i915_request *next)\n{\n\tGEM_BUG_ON(prev == next);\n\tGEM_BUG_ON(!assert_priority_queue(prev, next));\n\n\t \n\tif (__i915_request_is_complete(next))\n\t\treturn true;\n\n\tif (unlikely((i915_request_flags(prev) | i915_request_flags(next)) &\n\t\t     (BIT(I915_FENCE_FLAG_NOPREEMPT) |\n\t\t      BIT(I915_FENCE_FLAG_SENTINEL))))\n\t\treturn false;\n\n\tif (!can_merge_ctx(prev->context, next->context))\n\t\treturn false;\n\n\tGEM_BUG_ON(i915_seqno_passed(prev->fence.seqno, next->fence.seqno));\n\treturn true;\n}\n\nstatic bool virtual_matches(const struct virtual_engine *ve,\n\t\t\t    const struct i915_request *rq,\n\t\t\t    const struct intel_engine_cs *engine)\n{\n\tconst struct intel_engine_cs *inflight;\n\n\tif (!rq)\n\t\treturn false;\n\n\tif (!(rq->execution_mask & engine->mask))  \n\t\treturn false;\n\n\t \n\tinflight = intel_context_inflight(&ve->context);\n\tif (inflight && inflight != engine)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic struct virtual_engine *\nfirst_virtual_engine(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists *el = &engine->execlists;\n\tstruct rb_node *rb = rb_first_cached(&el->virtual);\n\n\twhile (rb) {\n\t\tstruct virtual_engine *ve =\n\t\t\trb_entry(rb, typeof(*ve), nodes[engine->id].rb);\n\t\tstruct i915_request *rq = READ_ONCE(ve->request);\n\n\t\t \n\t\tif (!rq || !virtual_matches(ve, rq, engine)) {\n\t\t\trb_erase_cached(rb, &el->virtual);\n\t\t\tRB_CLEAR_NODE(rb);\n\t\t\trb = rb_first_cached(&el->virtual);\n\t\t\tcontinue;\n\t\t}\n\n\t\treturn ve;\n\t}\n\n\treturn NULL;\n}\n\nstatic void virtual_xfer_context(struct virtual_engine *ve,\n\t\t\t\t struct intel_engine_cs *engine)\n{\n\tunsigned int n;\n\n\tif (likely(engine == ve->siblings[0]))\n\t\treturn;\n\n\tGEM_BUG_ON(READ_ONCE(ve->context.inflight));\n\tif (!intel_engine_has_relative_mmio(engine))\n\t\tlrc_update_offsets(&ve->context, engine);\n\n\t \n\tfor (n = 1; n < ve->num_siblings; n++) {\n\t\tif (ve->siblings[n] == engine) {\n\t\t\tswap(ve->siblings[n], ve->siblings[0]);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void defer_request(struct i915_request *rq, struct list_head * const pl)\n{\n\tLIST_HEAD(list);\n\n\t \n\tdo {\n\t\tstruct i915_dependency *p;\n\n\t\tGEM_BUG_ON(i915_request_is_active(rq));\n\t\tlist_move_tail(&rq->sched.link, pl);\n\n\t\tfor_each_waiter(p, rq) {\n\t\t\tstruct i915_request *w =\n\t\t\t\tcontainer_of(p->waiter, typeof(*w), sched);\n\n\t\t\tif (p->flags & I915_DEPENDENCY_WEAK)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (w->engine != rq->engine)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tGEM_BUG_ON(i915_request_has_initial_breadcrumb(w) &&\n\t\t\t\t   __i915_request_has_started(w) &&\n\t\t\t\t   !__i915_request_is_complete(rq));\n\n\t\t\tif (!i915_request_is_ready(w))\n\t\t\t\tcontinue;\n\n\t\t\tif (rq_prio(w) < rq_prio(rq))\n\t\t\t\tcontinue;\n\n\t\t\tGEM_BUG_ON(rq_prio(w) > rq_prio(rq));\n\t\t\tGEM_BUG_ON(i915_request_is_active(w));\n\t\t\tlist_move_tail(&w->sched.link, &list);\n\t\t}\n\n\t\trq = list_first_entry_or_null(&list, typeof(*rq), sched.link);\n\t} while (rq);\n}\n\nstatic void defer_active(struct intel_engine_cs *engine)\n{\n\tstruct i915_request *rq;\n\n\trq = __unwind_incomplete_requests(engine);\n\tif (!rq)\n\t\treturn;\n\n\tdefer_request(rq, i915_sched_lookup_priolist(engine->sched_engine,\n\t\t\t\t\t\t     rq_prio(rq)));\n}\n\nstatic bool\ntimeslice_yield(const struct intel_engine_execlists *el,\n\t\tconst struct i915_request *rq)\n{\n\t \n\treturn rq->context->lrc.ccid == READ_ONCE(el->yield);\n}\n\nstatic bool needs_timeslice(const struct intel_engine_cs *engine,\n\t\t\t    const struct i915_request *rq)\n{\n\tif (!intel_engine_has_timeslices(engine))\n\t\treturn false;\n\n\t \n\tif (!rq || __i915_request_is_complete(rq))\n\t\treturn false;\n\n\t \n\tif (READ_ONCE(engine->execlists.pending[0]))\n\t\treturn false;\n\n\t \n\tif (!list_is_last_rcu(&rq->sched.link,\n\t\t\t      &engine->sched_engine->requests)) {\n\t\tENGINE_TRACE(engine, \"timeslice required for second inflight context\\n\");\n\t\treturn true;\n\t}\n\n\t \n\tif (!i915_sched_engine_is_empty(engine->sched_engine)) {\n\t\tENGINE_TRACE(engine, \"timeslice required for queue\\n\");\n\t\treturn true;\n\t}\n\n\tif (!RB_EMPTY_ROOT(&engine->execlists.virtual.rb_root)) {\n\t\tENGINE_TRACE(engine, \"timeslice required for virtual\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool\ntimeslice_expired(struct intel_engine_cs *engine, const struct i915_request *rq)\n{\n\tconst struct intel_engine_execlists *el = &engine->execlists;\n\n\tif (i915_request_has_nopreempt(rq) && __i915_request_has_started(rq))\n\t\treturn false;\n\n\tif (!needs_timeslice(engine, rq))\n\t\treturn false;\n\n\treturn timer_expired(&el->timer) || timeslice_yield(el, rq);\n}\n\nstatic unsigned long timeslice(const struct intel_engine_cs *engine)\n{\n\treturn READ_ONCE(engine->props.timeslice_duration_ms);\n}\n\nstatic void start_timeslice(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists *el = &engine->execlists;\n\tunsigned long duration;\n\n\t \n\tduration = 0;\n\tif (needs_timeslice(engine, *el->active)) {\n\t\t \n\t\tif (timer_active(&el->timer)) {\n\t\t\t \n\t\t\tif (!timer_pending(&el->timer))\n\t\t\t\ttasklet_hi_schedule(&engine->sched_engine->tasklet);\n\t\t\treturn;\n\t\t}\n\n\t\tduration = timeslice(engine);\n\t}\n\n\tset_timer_ms(&el->timer, duration);\n}\n\nstatic void record_preemption(struct intel_engine_execlists *execlists)\n{\n\t(void)I915_SELFTEST_ONLY(execlists->preempt_hang.count++);\n}\n\nstatic unsigned long active_preempt_timeout(struct intel_engine_cs *engine,\n\t\t\t\t\t    const struct i915_request *rq)\n{\n\tif (!rq)\n\t\treturn 0;\n\n\t \n\tengine->execlists.preempt_target = rq;\n\n\t \n\tif (unlikely(intel_context_is_banned(rq->context) || bad_request(rq)))\n\t\treturn INTEL_CONTEXT_BANNED_PREEMPT_TIMEOUT_MS;\n\n\treturn READ_ONCE(engine->props.preempt_timeout_ms);\n}\n\nstatic void set_preempt_timeout(struct intel_engine_cs *engine,\n\t\t\t\tconst struct i915_request *rq)\n{\n\tif (!intel_engine_has_preempt_reset(engine))\n\t\treturn;\n\n\tset_timer_ms(&engine->execlists.preempt,\n\t\t     active_preempt_timeout(engine, rq));\n}\n\nstatic bool completed(const struct i915_request *rq)\n{\n\tif (i915_request_has_sentinel(rq))\n\t\treturn false;\n\n\treturn __i915_request_is_complete(rq);\n}\n\nstatic void execlists_dequeue(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tstruct i915_sched_engine * const sched_engine = engine->sched_engine;\n\tstruct i915_request **port = execlists->pending;\n\tstruct i915_request ** const last_port = port + execlists->port_mask;\n\tstruct i915_request *last, * const *active;\n\tstruct virtual_engine *ve;\n\tstruct rb_node *rb;\n\tbool submit = false;\n\n\t \n\n\tspin_lock(&sched_engine->lock);\n\n\t \n\tactive = execlists->active;\n\twhile ((last = *active) && completed(last))\n\t\tactive++;\n\n\tif (last) {\n\t\tif (need_preempt(engine, last)) {\n\t\t\tENGINE_TRACE(engine,\n\t\t\t\t     \"preempting last=%llx:%lld, prio=%d, hint=%d\\n\",\n\t\t\t\t     last->fence.context,\n\t\t\t\t     last->fence.seqno,\n\t\t\t\t     last->sched.attr.priority,\n\t\t\t\t     sched_engine->queue_priority_hint);\n\t\t\trecord_preemption(execlists);\n\n\t\t\t \n\t\t\tring_set_paused(engine, 1);\n\n\t\t\t \n\t\t\t__unwind_incomplete_requests(engine);\n\n\t\t\tlast = NULL;\n\t\t} else if (timeslice_expired(engine, last)) {\n\t\t\tENGINE_TRACE(engine,\n\t\t\t\t     \"expired:%s last=%llx:%lld, prio=%d, hint=%d, yield?=%s\\n\",\n\t\t\t\t     str_yes_no(timer_expired(&execlists->timer)),\n\t\t\t\t     last->fence.context, last->fence.seqno,\n\t\t\t\t     rq_prio(last),\n\t\t\t\t     sched_engine->queue_priority_hint,\n\t\t\t\t     str_yes_no(timeslice_yield(execlists, last)));\n\n\t\t\t \n\t\t\tcancel_timer(&execlists->timer);\n\t\t\tring_set_paused(engine, 1);\n\t\t\tdefer_active(engine);\n\n\t\t\t \n\t\t\tlast = NULL;\n\t\t} else {\n\t\t\t \n\t\t\tif (active[1]) {\n\t\t\t\t \n\t\t\t\tspin_unlock(&sched_engine->lock);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\twhile ((ve = first_virtual_engine(engine))) {\n\t\tstruct i915_request *rq;\n\n\t\tspin_lock(&ve->base.sched_engine->lock);\n\n\t\trq = ve->request;\n\t\tif (unlikely(!virtual_matches(ve, rq, engine)))\n\t\t\tgoto unlock;  \n\n\t\tGEM_BUG_ON(rq->engine != &ve->base);\n\t\tGEM_BUG_ON(rq->context != &ve->context);\n\n\t\tif (unlikely(rq_prio(rq) < queue_prio(sched_engine))) {\n\t\t\tspin_unlock(&ve->base.sched_engine->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (last && !can_merge_rq(last, rq)) {\n\t\t\tspin_unlock(&ve->base.sched_engine->lock);\n\t\t\tspin_unlock(&engine->sched_engine->lock);\n\t\t\treturn;  \n\t\t}\n\n\t\tENGINE_TRACE(engine,\n\t\t\t     \"virtual rq=%llx:%lld%s, new engine? %s\\n\",\n\t\t\t     rq->fence.context,\n\t\t\t     rq->fence.seqno,\n\t\t\t     __i915_request_is_complete(rq) ? \"!\" :\n\t\t\t     __i915_request_has_started(rq) ? \"*\" :\n\t\t\t     \"\",\n\t\t\t     str_yes_no(engine != ve->siblings[0]));\n\n\t\tWRITE_ONCE(ve->request, NULL);\n\t\tWRITE_ONCE(ve->base.sched_engine->queue_priority_hint, INT_MIN);\n\n\t\trb = &ve->nodes[engine->id].rb;\n\t\trb_erase_cached(rb, &execlists->virtual);\n\t\tRB_CLEAR_NODE(rb);\n\n\t\tGEM_BUG_ON(!(rq->execution_mask & engine->mask));\n\t\tWRITE_ONCE(rq->engine, engine);\n\n\t\tif (__i915_request_submit(rq)) {\n\t\t\t \n\t\t\tvirtual_xfer_context(ve, engine);\n\t\t\tGEM_BUG_ON(ve->siblings[0] != engine);\n\n\t\t\tsubmit = true;\n\t\t\tlast = rq;\n\t\t}\n\n\t\ti915_request_put(rq);\nunlock:\n\t\tspin_unlock(&ve->base.sched_engine->lock);\n\n\t\t \n\t\tif (submit)\n\t\t\tbreak;\n\t}\n\n\twhile ((rb = rb_first_cached(&sched_engine->queue))) {\n\t\tstruct i915_priolist *p = to_priolist(rb);\n\t\tstruct i915_request *rq, *rn;\n\n\t\tpriolist_for_each_request_consume(rq, rn, p) {\n\t\t\tbool merge = true;\n\n\t\t\t \n\t\t\tif (last && !can_merge_rq(last, rq)) {\n\t\t\t\t \n\t\t\t\tif (port == last_port)\n\t\t\t\t\tgoto done;\n\n\t\t\t\t \n\t\t\t\tif (last->context == rq->context)\n\t\t\t\t\tgoto done;\n\n\t\t\t\tif (i915_request_has_sentinel(last))\n\t\t\t\t\tgoto done;\n\n\t\t\t\t \n\t\t\t\tif (rq->execution_mask != engine->mask)\n\t\t\t\t\tgoto done;\n\n\t\t\t\t \n\t\t\t\tif (ctx_single_port_submission(last->context) ||\n\t\t\t\t    ctx_single_port_submission(rq->context))\n\t\t\t\t\tgoto done;\n\n\t\t\t\tmerge = false;\n\t\t\t}\n\n\t\t\tif (__i915_request_submit(rq)) {\n\t\t\t\tif (!merge) {\n\t\t\t\t\t*port++ = i915_request_get(last);\n\t\t\t\t\tlast = NULL;\n\t\t\t\t}\n\n\t\t\t\tGEM_BUG_ON(last &&\n\t\t\t\t\t   !can_merge_ctx(last->context,\n\t\t\t\t\t\t\t  rq->context));\n\t\t\t\tGEM_BUG_ON(last &&\n\t\t\t\t\t   i915_seqno_passed(last->fence.seqno,\n\t\t\t\t\t\t\t     rq->fence.seqno));\n\n\t\t\t\tsubmit = true;\n\t\t\t\tlast = rq;\n\t\t\t}\n\t\t}\n\n\t\trb_erase_cached(&p->node, &sched_engine->queue);\n\t\ti915_priolist_free(p);\n\t}\ndone:\n\t*port++ = i915_request_get(last);\n\n\t \n\tsched_engine->queue_priority_hint = queue_prio(sched_engine);\n\ti915_sched_engine_reset_on_empty(sched_engine);\n\tspin_unlock(&sched_engine->lock);\n\n\t \n\tif (submit &&\n\t    memcmp(active,\n\t\t   execlists->pending,\n\t\t   (port - execlists->pending) * sizeof(*port))) {\n\t\t*port = NULL;\n\t\twhile (port-- != execlists->pending)\n\t\t\texeclists_schedule_in(*port, port - execlists->pending);\n\n\t\tWRITE_ONCE(execlists->yield, -1);\n\t\tset_preempt_timeout(engine, *active);\n\t\texeclists_submit_ports(engine);\n\t} else {\n\t\tring_set_paused(engine, 0);\n\t\twhile (port-- != execlists->pending)\n\t\t\ti915_request_put(*port);\n\t\t*execlists->pending = NULL;\n\t}\n}\n\nstatic void execlists_dequeue_irq(struct intel_engine_cs *engine)\n{\n\tlocal_irq_disable();  \n\texeclists_dequeue(engine);\n\tlocal_irq_enable();  \n}\n\nstatic void clear_ports(struct i915_request **ports, int count)\n{\n\tmemset_p((void **)ports, NULL, count);\n}\n\nstatic void\ncopy_ports(struct i915_request **dst, struct i915_request **src, int count)\n{\n\t \n\twhile (count--)\n\t\tWRITE_ONCE(*dst++, *src++);  \n}\n\nstatic struct i915_request **\ncancel_port_requests(struct intel_engine_execlists * const execlists,\n\t\t     struct i915_request **inactive)\n{\n\tstruct i915_request * const *port;\n\n\tfor (port = execlists->pending; *port; port++)\n\t\t*inactive++ = *port;\n\tclear_ports(execlists->pending, ARRAY_SIZE(execlists->pending));\n\n\t \n\tfor (port = xchg(&execlists->active, execlists->pending); *port; port++)\n\t\t*inactive++ = *port;\n\tclear_ports(execlists->inflight, ARRAY_SIZE(execlists->inflight));\n\n\tsmp_wmb();  \n\tWRITE_ONCE(execlists->active, execlists->inflight);\n\n\t \n\tGEM_BUG_ON(execlists->pending[0]);\n\tcancel_timer(&execlists->timer);\n\tcancel_timer(&execlists->preempt);\n\n\treturn inactive;\n}\n\n \nstatic inline bool\n__gen12_csb_parse(bool ctx_to_valid, bool ctx_away_valid, bool new_queue,\n\t\t  u8 switch_detail)\n{\n\t \n\tif (!ctx_away_valid || new_queue) {\n\t\tGEM_BUG_ON(!ctx_to_valid);\n\t\treturn true;\n\t}\n\n\t \n\tGEM_BUG_ON(switch_detail);\n\treturn false;\n}\n\nstatic bool xehp_csb_parse(const u64 csb)\n{\n\treturn __gen12_csb_parse(XEHP_CSB_CTX_VALID(lower_32_bits(csb)),  \n\t\t\t\t XEHP_CSB_CTX_VALID(upper_32_bits(csb)),  \n\t\t\t\t upper_32_bits(csb) & XEHP_CTX_STATUS_SWITCHED_TO_NEW_QUEUE,\n\t\t\t\t GEN12_CTX_SWITCH_DETAIL(lower_32_bits(csb)));\n}\n\nstatic bool gen12_csb_parse(const u64 csb)\n{\n\treturn __gen12_csb_parse(GEN12_CSB_CTX_VALID(lower_32_bits(csb)),  \n\t\t\t\t GEN12_CSB_CTX_VALID(upper_32_bits(csb)),  \n\t\t\t\t lower_32_bits(csb) & GEN12_CTX_STATUS_SWITCHED_TO_NEW_QUEUE,\n\t\t\t\t GEN12_CTX_SWITCH_DETAIL(upper_32_bits(csb)));\n}\n\nstatic bool gen8_csb_parse(const u64 csb)\n{\n\treturn csb & (GEN8_CTX_STATUS_IDLE_ACTIVE | GEN8_CTX_STATUS_PREEMPTED);\n}\n\nstatic noinline u64\nwa_csb_read(const struct intel_engine_cs *engine, u64 * const csb)\n{\n\tu64 entry;\n\n\t \n\tpreempt_disable();\n\tif (wait_for_atomic_us((entry = READ_ONCE(*csb)) != -1, 10)) {\n\t\tint idx = csb - engine->execlists.csb_status;\n\t\tint status;\n\n\t\tstatus = GEN8_EXECLISTS_STATUS_BUF;\n\t\tif (idx >= 6) {\n\t\t\tstatus = GEN11_EXECLISTS_STATUS_BUF2;\n\t\t\tidx -= 6;\n\t\t}\n\t\tstatus += sizeof(u64) * idx;\n\n\t\tentry = intel_uncore_read64(engine->uncore,\n\t\t\t\t\t    _MMIO(engine->mmio_base + status));\n\t}\n\tpreempt_enable();\n\n\treturn entry;\n}\n\nstatic u64 csb_read(const struct intel_engine_cs *engine, u64 * const csb)\n{\n\tu64 entry = READ_ONCE(*csb);\n\n\t \n\tif (unlikely(entry == -1))\n\t\tentry = wa_csb_read(engine, csb);\n\n\t \n\tWRITE_ONCE(*csb, -1);\n\n\t \n\treturn entry;\n}\n\nstatic void new_timeslice(struct intel_engine_execlists *el)\n{\n\t \n\tcancel_timer(&el->timer);\n}\n\nstatic struct i915_request **\nprocess_csb(struct intel_engine_cs *engine, struct i915_request **inactive)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tu64 * const buf = execlists->csb_status;\n\tconst u8 num_entries = execlists->csb_size;\n\tstruct i915_request **prev;\n\tu8 head, tail;\n\n\t \n\tGEM_BUG_ON(!tasklet_is_locked(&engine->sched_engine->tasklet) &&\n\t\t   !reset_in_progress(engine));\n\n\t \n\thead = execlists->csb_head;\n\ttail = READ_ONCE(*execlists->csb_write);\n\tif (unlikely(head == tail))\n\t\treturn inactive;\n\n\t \n\texeclists->csb_head = tail;\n\tENGINE_TRACE(engine, \"cs-irq head=%d, tail=%d\\n\", head, tail);\n\n\t \n\trmb();\n\n\t \n\tprev = inactive;\n\t*prev = NULL;\n\n\tdo {\n\t\tbool promote;\n\t\tu64 csb;\n\n\t\tif (++head == num_entries)\n\t\t\thead = 0;\n\n\t\t \n\n\t\tcsb = csb_read(engine, buf + head);\n\t\tENGINE_TRACE(engine, \"csb[%d]: status=0x%08x:0x%08x\\n\",\n\t\t\t     head, upper_32_bits(csb), lower_32_bits(csb));\n\n\t\tif (GRAPHICS_VER_FULL(engine->i915) >= IP_VER(12, 50))\n\t\t\tpromote = xehp_csb_parse(csb);\n\t\telse if (GRAPHICS_VER(engine->i915) >= 12)\n\t\t\tpromote = gen12_csb_parse(csb);\n\t\telse\n\t\t\tpromote = gen8_csb_parse(csb);\n\t\tif (promote) {\n\t\t\tstruct i915_request * const *old = execlists->active;\n\n\t\t\tif (GEM_WARN_ON(!*execlists->pending)) {\n\t\t\t\texeclists->error_interrupt |= ERROR_CSB;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tring_set_paused(engine, 0);\n\n\t\t\t \n\t\t\tWRITE_ONCE(execlists->active, execlists->pending);\n\t\t\tsmp_wmb();  \n\n\t\t\t \n\t\t\ttrace_ports(execlists, \"preempted\", old);\n\t\t\twhile (*old)\n\t\t\t\t*inactive++ = *old++;\n\n\t\t\t \n\t\t\tGEM_BUG_ON(!assert_pending_valid(execlists, \"promote\"));\n\t\t\tcopy_ports(execlists->inflight,\n\t\t\t\t   execlists->pending,\n\t\t\t\t   execlists_num_ports(execlists));\n\t\t\tsmp_wmb();  \n\t\t\tWRITE_ONCE(execlists->active, execlists->inflight);\n\n\t\t\t \n\t\t\tENGINE_POSTING_READ(engine, RING_CONTEXT_STATUS_PTR);\n\n\t\t\tWRITE_ONCE(execlists->pending[0], NULL);\n\t\t} else {\n\t\t\tif (GEM_WARN_ON(!*execlists->active)) {\n\t\t\t\texeclists->error_interrupt |= ERROR_CSB;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\ttrace_ports(execlists, \"completed\", execlists->active);\n\n\t\t\t \n\t\t\tif (GEM_SHOW_DEBUG() &&\n\t\t\t    !__i915_request_is_complete(*execlists->active)) {\n\t\t\t\tstruct i915_request *rq = *execlists->active;\n\t\t\t\tconst u32 *regs __maybe_unused =\n\t\t\t\t\trq->context->lrc_reg_state;\n\n\t\t\t\tENGINE_TRACE(engine,\n\t\t\t\t\t     \"context completed before request!\\n\");\n\t\t\t\tENGINE_TRACE(engine,\n\t\t\t\t\t     \"ring:{start:0x%08x, head:%04x, tail:%04x, ctl:%08x, mode:%08x}\\n\",\n\t\t\t\t\t     ENGINE_READ(engine, RING_START),\n\t\t\t\t\t     ENGINE_READ(engine, RING_HEAD) & HEAD_ADDR,\n\t\t\t\t\t     ENGINE_READ(engine, RING_TAIL) & TAIL_ADDR,\n\t\t\t\t\t     ENGINE_READ(engine, RING_CTL),\n\t\t\t\t\t     ENGINE_READ(engine, RING_MI_MODE));\n\t\t\t\tENGINE_TRACE(engine,\n\t\t\t\t\t     \"rq:{start:%08x, head:%04x, tail:%04x, seqno:%llx:%d, hwsp:%d}, \",\n\t\t\t\t\t     i915_ggtt_offset(rq->ring->vma),\n\t\t\t\t\t     rq->head, rq->tail,\n\t\t\t\t\t     rq->fence.context,\n\t\t\t\t\t     lower_32_bits(rq->fence.seqno),\n\t\t\t\t\t     hwsp_seqno(rq));\n\t\t\t\tENGINE_TRACE(engine,\n\t\t\t\t\t     \"ctx:{start:%08x, head:%04x, tail:%04x}, \",\n\t\t\t\t\t     regs[CTX_RING_START],\n\t\t\t\t\t     regs[CTX_RING_HEAD],\n\t\t\t\t\t     regs[CTX_RING_TAIL]);\n\t\t\t}\n\n\t\t\t*inactive++ = *execlists->active++;\n\n\t\t\tGEM_BUG_ON(execlists->active - execlists->inflight >\n\t\t\t\t   execlists_num_ports(execlists));\n\t\t}\n\t} while (head != tail);\n\n\t \n\tdrm_clflush_virt_range(&buf[0], num_entries * sizeof(buf[0]));\n\n\t \n\tif (*prev != *execlists->active) {  \n\t\tstruct intel_context *prev_ce = NULL, *active_ce = NULL;\n\n\t\t \n\t\tif (*prev)\n\t\t\tprev_ce = (*prev)->context;\n\t\tif (*execlists->active)\n\t\t\tactive_ce = (*execlists->active)->context;\n\t\tif (prev_ce != active_ce) {\n\t\t\tif (prev_ce)\n\t\t\t\tlrc_runtime_stop(prev_ce);\n\t\t\tif (active_ce)\n\t\t\t\tlrc_runtime_start(active_ce);\n\t\t}\n\t\tnew_timeslice(execlists);\n\t}\n\n\treturn inactive;\n}\n\nstatic void post_process_csb(struct i915_request **port,\n\t\t\t     struct i915_request **last)\n{\n\twhile (port != last)\n\t\texeclists_schedule_out(*port++);\n}\n\nstatic void __execlists_hold(struct i915_request *rq)\n{\n\tLIST_HEAD(list);\n\n\tdo {\n\t\tstruct i915_dependency *p;\n\n\t\tif (i915_request_is_active(rq))\n\t\t\t__i915_request_unsubmit(rq);\n\n\t\tclear_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\t\tlist_move_tail(&rq->sched.link,\n\t\t\t       &rq->engine->sched_engine->hold);\n\t\ti915_request_set_hold(rq);\n\t\tRQ_TRACE(rq, \"on hold\\n\");\n\n\t\tfor_each_waiter(p, rq) {\n\t\t\tstruct i915_request *w =\n\t\t\t\tcontainer_of(p->waiter, typeof(*w), sched);\n\n\t\t\tif (p->flags & I915_DEPENDENCY_WEAK)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (w->engine != rq->engine)\n\t\t\t\tcontinue;\n\n\t\t\tif (!i915_request_is_ready(w))\n\t\t\t\tcontinue;\n\n\t\t\tif (__i915_request_is_complete(w))\n\t\t\t\tcontinue;\n\n\t\t\tif (i915_request_on_hold(w))\n\t\t\t\tcontinue;\n\n\t\t\tlist_move_tail(&w->sched.link, &list);\n\t\t}\n\n\t\trq = list_first_entry_or_null(&list, typeof(*rq), sched.link);\n\t} while (rq);\n}\n\nstatic bool execlists_hold(struct intel_engine_cs *engine,\n\t\t\t   struct i915_request *rq)\n{\n\tif (i915_request_on_hold(rq))\n\t\treturn false;\n\n\tspin_lock_irq(&engine->sched_engine->lock);\n\n\tif (__i915_request_is_complete(rq)) {  \n\t\trq = NULL;\n\t\tgoto unlock;\n\t}\n\n\t \n\tGEM_BUG_ON(i915_request_on_hold(rq));\n\tGEM_BUG_ON(rq->engine != engine);\n\t__execlists_hold(rq);\n\tGEM_BUG_ON(list_empty(&engine->sched_engine->hold));\n\nunlock:\n\tspin_unlock_irq(&engine->sched_engine->lock);\n\treturn rq;\n}\n\nstatic bool hold_request(const struct i915_request *rq)\n{\n\tstruct i915_dependency *p;\n\tbool result = false;\n\n\t \n\trcu_read_lock();\n\tfor_each_signaler(p, rq) {\n\t\tconst struct i915_request *s =\n\t\t\tcontainer_of(p->signaler, typeof(*s), sched);\n\n\t\tif (s->engine != rq->engine)\n\t\t\tcontinue;\n\n\t\tresult = i915_request_on_hold(s);\n\t\tif (result)\n\t\t\tbreak;\n\t}\n\trcu_read_unlock();\n\n\treturn result;\n}\n\nstatic void __execlists_unhold(struct i915_request *rq)\n{\n\tLIST_HEAD(list);\n\n\tdo {\n\t\tstruct i915_dependency *p;\n\n\t\tRQ_TRACE(rq, \"hold release\\n\");\n\n\t\tGEM_BUG_ON(!i915_request_on_hold(rq));\n\t\tGEM_BUG_ON(!i915_sw_fence_signaled(&rq->submit));\n\n\t\ti915_request_clear_hold(rq);\n\t\tlist_move_tail(&rq->sched.link,\n\t\t\t       i915_sched_lookup_priolist(rq->engine->sched_engine,\n\t\t\t\t\t\t\t  rq_prio(rq)));\n\t\tset_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\n\t\t \n\t\tfor_each_waiter(p, rq) {\n\t\t\tstruct i915_request *w =\n\t\t\t\tcontainer_of(p->waiter, typeof(*w), sched);\n\n\t\t\tif (p->flags & I915_DEPENDENCY_WEAK)\n\t\t\t\tcontinue;\n\n\t\t\tif (w->engine != rq->engine)\n\t\t\t\tcontinue;\n\n\t\t\tif (!i915_request_on_hold(w))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (hold_request(w))\n\t\t\t\tcontinue;\n\n\t\t\tlist_move_tail(&w->sched.link, &list);\n\t\t}\n\n\t\trq = list_first_entry_or_null(&list, typeof(*rq), sched.link);\n\t} while (rq);\n}\n\nstatic void execlists_unhold(struct intel_engine_cs *engine,\n\t\t\t     struct i915_request *rq)\n{\n\tspin_lock_irq(&engine->sched_engine->lock);\n\n\t \n\t__execlists_unhold(rq);\n\n\tif (rq_prio(rq) > engine->sched_engine->queue_priority_hint) {\n\t\tengine->sched_engine->queue_priority_hint = rq_prio(rq);\n\t\ttasklet_hi_schedule(&engine->sched_engine->tasklet);\n\t}\n\n\tspin_unlock_irq(&engine->sched_engine->lock);\n}\n\nstruct execlists_capture {\n\tstruct work_struct work;\n\tstruct i915_request *rq;\n\tstruct i915_gpu_coredump *error;\n};\n\nstatic void execlists_capture_work(struct work_struct *work)\n{\n\tstruct execlists_capture *cap = container_of(work, typeof(*cap), work);\n\tconst gfp_t gfp = __GFP_KSWAPD_RECLAIM | __GFP_RETRY_MAYFAIL |\n\t\t__GFP_NOWARN;\n\tstruct intel_engine_cs *engine = cap->rq->engine;\n\tstruct intel_gt_coredump *gt = cap->error->gt;\n\tstruct intel_engine_capture_vma *vma;\n\n\t \n\tvma = intel_engine_coredump_add_request(gt->engine, cap->rq, gfp);\n\tif (vma) {\n\t\tstruct i915_vma_compress *compress =\n\t\t\ti915_vma_capture_prepare(gt);\n\n\t\tintel_engine_coredump_add_vma(gt->engine, vma, compress);\n\t\ti915_vma_capture_finish(gt, compress);\n\t}\n\n\tgt->simulated = gt->engine->simulated;\n\tcap->error->simulated = gt->simulated;\n\n\t \n\ti915_error_state_store(cap->error);\n\ti915_gpu_coredump_put(cap->error);\n\n\t \n\texeclists_unhold(engine, cap->rq);\n\ti915_request_put(cap->rq);\n\n\tkfree(cap);\n}\n\nstatic struct execlists_capture *capture_regs(struct intel_engine_cs *engine)\n{\n\tconst gfp_t gfp = GFP_ATOMIC | __GFP_NOWARN;\n\tstruct execlists_capture *cap;\n\n\tcap = kmalloc(sizeof(*cap), gfp);\n\tif (!cap)\n\t\treturn NULL;\n\n\tcap->error = i915_gpu_coredump_alloc(engine->i915, gfp);\n\tif (!cap->error)\n\t\tgoto err_cap;\n\n\tcap->error->gt = intel_gt_coredump_alloc(engine->gt, gfp, CORE_DUMP_FLAG_NONE);\n\tif (!cap->error->gt)\n\t\tgoto err_gpu;\n\n\tcap->error->gt->engine = intel_engine_coredump_alloc(engine, gfp, CORE_DUMP_FLAG_NONE);\n\tif (!cap->error->gt->engine)\n\t\tgoto err_gt;\n\n\tcap->error->gt->engine->hung = true;\n\n\treturn cap;\n\nerr_gt:\n\tkfree(cap->error->gt);\nerr_gpu:\n\tkfree(cap->error);\nerr_cap:\n\tkfree(cap);\n\treturn NULL;\n}\n\nstatic struct i915_request *\nactive_context(struct intel_engine_cs *engine, u32 ccid)\n{\n\tconst struct intel_engine_execlists * const el = &engine->execlists;\n\tstruct i915_request * const *port, *rq;\n\n\t \n\n\tfor (port = el->active; (rq = *port); port++) {\n\t\tif (rq->context->lrc.ccid == ccid) {\n\t\t\tENGINE_TRACE(engine,\n\t\t\t\t     \"ccid:%x found at active:%zd\\n\",\n\t\t\t\t     ccid, port - el->active);\n\t\t\treturn rq;\n\t\t}\n\t}\n\n\tfor (port = el->pending; (rq = *port); port++) {\n\t\tif (rq->context->lrc.ccid == ccid) {\n\t\t\tENGINE_TRACE(engine,\n\t\t\t\t     \"ccid:%x found at pending:%zd\\n\",\n\t\t\t\t     ccid, port - el->pending);\n\t\t\treturn rq;\n\t\t}\n\t}\n\n\tENGINE_TRACE(engine, \"ccid:%x not found\\n\", ccid);\n\treturn NULL;\n}\n\nstatic u32 active_ccid(struct intel_engine_cs *engine)\n{\n\treturn ENGINE_READ_FW(engine, RING_EXECLIST_STATUS_HI);\n}\n\nstatic void execlists_capture(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\tstruct execlists_capture *cap;\n\n\tif (!IS_ENABLED(CONFIG_DRM_I915_CAPTURE_ERROR))\n\t\treturn;\n\n\t \n\tcap = capture_regs(engine);\n\tif (!cap)\n\t\treturn;\n\n\tspin_lock_irq(&engine->sched_engine->lock);\n\tcap->rq = active_context(engine, active_ccid(engine));\n\tif (cap->rq) {\n\t\tcap->rq = active_request(cap->rq->context->timeline, cap->rq);\n\t\tcap->rq = i915_request_get_rcu(cap->rq);\n\t}\n\tspin_unlock_irq(&engine->sched_engine->lock);\n\tif (!cap->rq)\n\t\tgoto err_free;\n\n\t \n\tif (!execlists_hold(engine, cap->rq))\n\t\tgoto err_rq;\n\n\tINIT_WORK(&cap->work, execlists_capture_work);\n\tqueue_work(i915->unordered_wq, &cap->work);\n\treturn;\n\nerr_rq:\n\ti915_request_put(cap->rq);\nerr_free:\n\ti915_gpu_coredump_put(cap->error);\n\tkfree(cap);\n}\n\nstatic void execlists_reset(struct intel_engine_cs *engine, const char *msg)\n{\n\tconst unsigned int bit = I915_RESET_ENGINE + engine->id;\n\tunsigned long *lock = &engine->gt->reset.flags;\n\n\tif (!intel_has_reset_engine(engine->gt))\n\t\treturn;\n\n\tif (test_and_set_bit(bit, lock))\n\t\treturn;\n\n\tENGINE_TRACE(engine, \"reset for %s\\n\", msg);\n\n\t \n\ttasklet_disable_nosync(&engine->sched_engine->tasklet);\n\n\tring_set_paused(engine, 1);  \n\texeclists_capture(engine);\n\tintel_engine_reset(engine, msg);\n\n\ttasklet_enable(&engine->sched_engine->tasklet);\n\tclear_and_wake_up_bit(bit, lock);\n}\n\nstatic bool preempt_timeout(const struct intel_engine_cs *const engine)\n{\n\tconst struct timer_list *t = &engine->execlists.preempt;\n\n\tif (!CONFIG_DRM_I915_PREEMPT_TIMEOUT)\n\t\treturn false;\n\n\tif (!timer_expired(t))\n\t\treturn false;\n\n\treturn engine->execlists.pending[0];\n}\n\n \nstatic void execlists_submission_tasklet(struct tasklet_struct *t)\n{\n\tstruct i915_sched_engine *sched_engine =\n\t\tfrom_tasklet(sched_engine, t, tasklet);\n\tstruct intel_engine_cs * const engine = sched_engine->private_data;\n\tstruct i915_request *post[2 * EXECLIST_MAX_PORTS];\n\tstruct i915_request **inactive;\n\n\trcu_read_lock();\n\tinactive = process_csb(engine, post);\n\tGEM_BUG_ON(inactive - post > ARRAY_SIZE(post));\n\n\tif (unlikely(preempt_timeout(engine))) {\n\t\tconst struct i915_request *rq = *engine->execlists.active;\n\n\t\t \n\t\tcancel_timer(&engine->execlists.preempt);\n\t\tif (rq == engine->execlists.preempt_target)\n\t\t\tengine->execlists.error_interrupt |= ERROR_PREEMPT;\n\t\telse\n\t\t\tset_timer_ms(&engine->execlists.preempt,\n\t\t\t\t     active_preempt_timeout(engine, rq));\n\t}\n\n\tif (unlikely(READ_ONCE(engine->execlists.error_interrupt))) {\n\t\tconst char *msg;\n\n\t\t \n\t\tif (engine->execlists.error_interrupt & GENMASK(15, 0))\n\t\t\tmsg = \"CS error\";  \n\t\telse if (engine->execlists.error_interrupt & ERROR_CSB)\n\t\t\tmsg = \"invalid CSB event\";\n\t\telse if (engine->execlists.error_interrupt & ERROR_PREEMPT)\n\t\t\tmsg = \"preemption time out\";\n\t\telse\n\t\t\tmsg = \"internal error\";\n\n\t\tengine->execlists.error_interrupt = 0;\n\t\texeclists_reset(engine, msg);\n\t}\n\n\tif (!engine->execlists.pending[0]) {\n\t\texeclists_dequeue_irq(engine);\n\t\tstart_timeslice(engine);\n\t}\n\n\tpost_process_csb(post, inactive);\n\trcu_read_unlock();\n}\n\nstatic void execlists_irq_handler(struct intel_engine_cs *engine, u16 iir)\n{\n\tbool tasklet = false;\n\n\tif (unlikely(iir & GT_CS_MASTER_ERROR_INTERRUPT)) {\n\t\tu32 eir;\n\n\t\t \n\t\teir = ENGINE_READ(engine, RING_EIR) & GENMASK(15, 0);\n\t\tENGINE_TRACE(engine, \"CS error: %x\\n\", eir);\n\n\t\t \n\t\tif (likely(eir)) {\n\t\t\tENGINE_WRITE(engine, RING_EMR, ~0u);\n\t\t\tENGINE_WRITE(engine, RING_EIR, eir);\n\t\t\tWRITE_ONCE(engine->execlists.error_interrupt, eir);\n\t\t\ttasklet = true;\n\t\t}\n\t}\n\n\tif (iir & GT_WAIT_SEMAPHORE_INTERRUPT) {\n\t\tWRITE_ONCE(engine->execlists.yield,\n\t\t\t   ENGINE_READ_FW(engine, RING_EXECLIST_STATUS_HI));\n\t\tENGINE_TRACE(engine, \"semaphore yield: %08x\\n\",\n\t\t\t     engine->execlists.yield);\n\t\tif (del_timer(&engine->execlists.timer))\n\t\t\ttasklet = true;\n\t}\n\n\tif (iir & GT_CONTEXT_SWITCH_INTERRUPT)\n\t\ttasklet = true;\n\n\tif (iir & GT_RENDER_USER_INTERRUPT)\n\t\tintel_engine_signal_breadcrumbs(engine);\n\n\tif (tasklet)\n\t\ttasklet_hi_schedule(&engine->sched_engine->tasklet);\n}\n\nstatic void __execlists_kick(struct intel_engine_execlists *execlists)\n{\n\tstruct intel_engine_cs *engine =\n\t\tcontainer_of(execlists, typeof(*engine), execlists);\n\n\t \n\ttasklet_hi_schedule(&engine->sched_engine->tasklet);\n}\n\n#define execlists_kick(t, member) \\\n\t__execlists_kick(container_of(t, struct intel_engine_execlists, member))\n\nstatic void execlists_timeslice(struct timer_list *timer)\n{\n\texeclists_kick(timer, timer);\n}\n\nstatic void execlists_preempt(struct timer_list *timer)\n{\n\texeclists_kick(timer, preempt);\n}\n\nstatic void queue_request(struct intel_engine_cs *engine,\n\t\t\t  struct i915_request *rq)\n{\n\tGEM_BUG_ON(!list_empty(&rq->sched.link));\n\tlist_add_tail(&rq->sched.link,\n\t\t      i915_sched_lookup_priolist(engine->sched_engine,\n\t\t\t\t\t\t rq_prio(rq)));\n\tset_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n}\n\nstatic bool submit_queue(struct intel_engine_cs *engine,\n\t\t\t const struct i915_request *rq)\n{\n\tstruct i915_sched_engine *sched_engine = engine->sched_engine;\n\n\tif (rq_prio(rq) <= sched_engine->queue_priority_hint)\n\t\treturn false;\n\n\tsched_engine->queue_priority_hint = rq_prio(rq);\n\treturn true;\n}\n\nstatic bool ancestor_on_hold(const struct intel_engine_cs *engine,\n\t\t\t     const struct i915_request *rq)\n{\n\tGEM_BUG_ON(i915_request_on_hold(rq));\n\treturn !list_empty(&engine->sched_engine->hold) && hold_request(rq);\n}\n\nstatic void execlists_submit_request(struct i915_request *request)\n{\n\tstruct intel_engine_cs *engine = request->engine;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\tif (unlikely(ancestor_on_hold(engine, request))) {\n\t\tRQ_TRACE(request, \"ancestor on hold\\n\");\n\t\tlist_add_tail(&request->sched.link,\n\t\t\t      &engine->sched_engine->hold);\n\t\ti915_request_set_hold(request);\n\t} else {\n\t\tqueue_request(engine, request);\n\n\t\tGEM_BUG_ON(i915_sched_engine_is_empty(engine->sched_engine));\n\t\tGEM_BUG_ON(list_empty(&request->sched.link));\n\n\t\tif (submit_queue(engine, request))\n\t\t\t__execlists_kick(&engine->execlists);\n\t}\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\nstatic int\n__execlists_context_pre_pin(struct intel_context *ce,\n\t\t\t    struct intel_engine_cs *engine,\n\t\t\t    struct i915_gem_ww_ctx *ww, void **vaddr)\n{\n\tint err;\n\n\terr = lrc_pre_pin(ce, engine, ww, vaddr);\n\tif (err)\n\t\treturn err;\n\n\tif (!__test_and_set_bit(CONTEXT_INIT_BIT, &ce->flags)) {\n\t\tlrc_init_state(ce, engine, *vaddr);\n\n\t\t__i915_gem_object_flush_map(ce->state->obj, 0, engine->context_size);\n\t}\n\n\treturn 0;\n}\n\nstatic int execlists_context_pre_pin(struct intel_context *ce,\n\t\t\t\t     struct i915_gem_ww_ctx *ww,\n\t\t\t\t     void **vaddr)\n{\n\treturn __execlists_context_pre_pin(ce, ce->engine, ww, vaddr);\n}\n\nstatic int execlists_context_pin(struct intel_context *ce, void *vaddr)\n{\n\treturn lrc_pin(ce, ce->engine, vaddr);\n}\n\nstatic int execlists_context_alloc(struct intel_context *ce)\n{\n\treturn lrc_alloc(ce, ce->engine);\n}\n\nstatic void execlists_context_cancel_request(struct intel_context *ce,\n\t\t\t\t\t     struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = NULL;\n\n\ti915_request_active_engine(rq, &engine);\n\n\tif (engine && intel_engine_pulse(engine))\n\t\tintel_gt_handle_error(engine->gt, engine->mask, 0,\n\t\t\t\t      \"request cancellation by %s\",\n\t\t\t\t      current->comm);\n}\n\nstatic struct intel_context *\nexeclists_create_parallel(struct intel_engine_cs **engines,\n\t\t\t  unsigned int num_siblings,\n\t\t\t  unsigned int width)\n{\n\tstruct intel_context *parent = NULL, *ce, *err;\n\tint i;\n\n\tGEM_BUG_ON(num_siblings != 1);\n\n\tfor (i = 0; i < width; ++i) {\n\t\tce = intel_context_create(engines[i]);\n\t\tif (IS_ERR(ce)) {\n\t\t\terr = ce;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tif (i == 0)\n\t\t\tparent = ce;\n\t\telse\n\t\t\tintel_context_bind_parent_child(parent, ce);\n\t}\n\n\tparent->parallel.fence_context = dma_fence_context_alloc(1);\n\n\tintel_context_set_nopreempt(parent);\n\tfor_each_child(parent, ce)\n\t\tintel_context_set_nopreempt(ce);\n\n\treturn parent;\n\nunwind:\n\tif (parent)\n\t\tintel_context_put(parent);\n\treturn err;\n}\n\nstatic const struct intel_context_ops execlists_context_ops = {\n\t.flags = COPS_HAS_INFLIGHT | COPS_RUNTIME_CYCLES,\n\n\t.alloc = execlists_context_alloc,\n\n\t.cancel_request = execlists_context_cancel_request,\n\n\t.pre_pin = execlists_context_pre_pin,\n\t.pin = execlists_context_pin,\n\t.unpin = lrc_unpin,\n\t.post_unpin = lrc_post_unpin,\n\n\t.enter = intel_context_enter_engine,\n\t.exit = intel_context_exit_engine,\n\n\t.reset = lrc_reset,\n\t.destroy = lrc_destroy,\n\n\t.create_parallel = execlists_create_parallel,\n\t.create_virtual = execlists_create_virtual,\n};\n\nstatic int emit_pdps(struct i915_request *rq)\n{\n\tconst struct intel_engine_cs * const engine = rq->engine;\n\tstruct i915_ppgtt * const ppgtt = i915_vm_to_ppgtt(rq->context->vm);\n\tint err, i;\n\tu32 *cs;\n\n\tGEM_BUG_ON(intel_vgpu_active(rq->i915));\n\n\t \n\n\tcs = intel_ring_begin(rq, 2);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\t*cs++ = MI_NOOP;\n\tintel_ring_advance(rq, cs);\n\n\t \n\terr = engine->emit_flush(rq, EMIT_FLUSH);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = engine->emit_flush(rq, EMIT_INVALIDATE);\n\tif (err)\n\t\treturn err;\n\n\tcs = intel_ring_begin(rq, 4 * GEN8_3LVL_PDPES + 2);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\t*cs++ = MI_LOAD_REGISTER_IMM(2 * GEN8_3LVL_PDPES) | MI_LRI_FORCE_POSTED;\n\tfor (i = GEN8_3LVL_PDPES; i--; ) {\n\t\tconst dma_addr_t pd_daddr = i915_page_dir_dma_addr(ppgtt, i);\n\t\tu32 base = engine->mmio_base;\n\n\t\t*cs++ = i915_mmio_reg_offset(GEN8_RING_PDP_UDW(base, i));\n\t\t*cs++ = upper_32_bits(pd_daddr);\n\t\t*cs++ = i915_mmio_reg_offset(GEN8_RING_PDP_LDW(base, i));\n\t\t*cs++ = lower_32_bits(pd_daddr);\n\t}\n\t*cs++ = MI_ARB_ON_OFF | MI_ARB_ENABLE;\n\tintel_ring_advance(rq, cs);\n\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic int execlists_request_alloc(struct i915_request *request)\n{\n\tint ret;\n\n\tGEM_BUG_ON(!intel_context_is_pinned(request->context));\n\n\t \n\trequest->reserved_space += EXECLISTS_REQUEST_SIZE;\n\n\t \n\n\tif (!i915_vm_is_4lvl(request->context->vm)) {\n\t\tret = emit_pdps(request);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tret = request->engine->emit_flush(request, EMIT_INVALIDATE);\n\tif (ret)\n\t\treturn ret;\n\n\trequest->reserved_space -= EXECLISTS_REQUEST_SIZE;\n\treturn 0;\n}\n\nstatic void reset_csb_pointers(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tconst unsigned int reset_value = execlists->csb_size - 1;\n\n\tring_set_paused(engine, 0);\n\n\t \n\tENGINE_WRITE(engine, RING_CONTEXT_STATUS_PTR,\n\t\t     0xffff << 16 | reset_value << 8 | reset_value);\n\tENGINE_POSTING_READ(engine, RING_CONTEXT_STATUS_PTR);\n\n\t \n\texeclists->csb_head = reset_value;\n\tWRITE_ONCE(*execlists->csb_write, reset_value);\n\twmb();  \n\n\t \n\tmemset(execlists->csb_status, -1, (reset_value + 1) * sizeof(u64));\n\tdrm_clflush_virt_range(execlists->csb_status,\n\t\t\t       execlists->csb_size *\n\t\t\t       sizeof(execlists->csb_status));\n\n\t \n\tENGINE_WRITE(engine, RING_CONTEXT_STATUS_PTR,\n\t\t     0xffff << 16 | reset_value << 8 | reset_value);\n\tENGINE_POSTING_READ(engine, RING_CONTEXT_STATUS_PTR);\n\n\tGEM_BUG_ON(READ_ONCE(*execlists->csb_write) != reset_value);\n}\n\nstatic void sanitize_hwsp(struct intel_engine_cs *engine)\n{\n\tstruct intel_timeline *tl;\n\n\tlist_for_each_entry(tl, &engine->status_page.timelines, engine_link)\n\t\tintel_timeline_reset_seqno(tl);\n}\n\nstatic void execlists_sanitize(struct intel_engine_cs *engine)\n{\n\tGEM_BUG_ON(execlists_active(&engine->execlists));\n\n\t \n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tmemset(engine->status_page.addr, POISON_INUSE, PAGE_SIZE);\n\n\treset_csb_pointers(engine);\n\n\t \n\tsanitize_hwsp(engine);\n\n\t \n\tdrm_clflush_virt_range(engine->status_page.addr, PAGE_SIZE);\n\n\tintel_engine_reset_pinned_contexts(engine);\n}\n\nstatic void enable_error_interrupt(struct intel_engine_cs *engine)\n{\n\tu32 status;\n\n\tengine->execlists.error_interrupt = 0;\n\tENGINE_WRITE(engine, RING_EMR, ~0u);\n\tENGINE_WRITE(engine, RING_EIR, ~0u);  \n\n\tstatus = ENGINE_READ(engine, RING_ESR);\n\tif (unlikely(status)) {\n\t\tdrm_err(&engine->i915->drm,\n\t\t\t\"engine '%s' resumed still in error: %08x\\n\",\n\t\t\tengine->name, status);\n\t\t__intel_gt_reset(engine->gt, engine->mask);\n\t}\n\n\t \n\tENGINE_WRITE(engine, RING_EMR, ~I915_ERROR_INSTRUCTION);\n}\n\nstatic void enable_execlists(struct intel_engine_cs *engine)\n{\n\tu32 mode;\n\n\tassert_forcewakes_active(engine->uncore, FORCEWAKE_ALL);\n\n\tintel_engine_set_hwsp_writemask(engine, ~0u);  \n\n\tif (GRAPHICS_VER(engine->i915) >= 11)\n\t\tmode = _MASKED_BIT_ENABLE(GEN11_GFX_DISABLE_LEGACY_MODE);\n\telse\n\t\tmode = _MASKED_BIT_ENABLE(GFX_RUN_LIST_ENABLE);\n\tENGINE_WRITE_FW(engine, RING_MODE_GEN7, mode);\n\n\tENGINE_WRITE_FW(engine, RING_MI_MODE, _MASKED_BIT_DISABLE(STOP_RING));\n\n\tENGINE_WRITE_FW(engine,\n\t\t\tRING_HWS_PGA,\n\t\t\ti915_ggtt_offset(engine->status_page.vma));\n\tENGINE_POSTING_READ(engine, RING_HWS_PGA);\n\n\tenable_error_interrupt(engine);\n}\n\nstatic int execlists_resume(struct intel_engine_cs *engine)\n{\n\tintel_mocs_init_engine(engine);\n\tintel_breadcrumbs_reset(engine->breadcrumbs);\n\n\tenable_execlists(engine);\n\n\tif (engine->flags & I915_ENGINE_FIRST_RENDER_COMPUTE)\n\t\txehp_enable_ccs_engines(engine);\n\n\treturn 0;\n}\n\nstatic void execlists_reset_prepare(struct intel_engine_cs *engine)\n{\n\tENGINE_TRACE(engine, \"depth<-%d\\n\",\n\t\t     atomic_read(&engine->sched_engine->tasklet.count));\n\n\t \n\t__tasklet_disable_sync_once(&engine->sched_engine->tasklet);\n\tGEM_BUG_ON(!reset_in_progress(engine));\n\n\t \n\tring_set_paused(engine, 1);\n\tintel_engine_stop_cs(engine);\n\n\t \n\tif (IS_MTL_GRAPHICS_STEP(engine->i915, M, STEP_A0, STEP_B0) ||\n\t    (GRAPHICS_VER(engine->i915) >= 11 &&\n\t    GRAPHICS_VER_FULL(engine->i915) < IP_VER(12, 70)))\n\t\tintel_engine_wait_for_pending_mi_fw(engine);\n\n\tengine->execlists.reset_ccid = active_ccid(engine);\n}\n\nstatic struct i915_request **\nreset_csb(struct intel_engine_cs *engine, struct i915_request **inactive)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\n\tdrm_clflush_virt_range(execlists->csb_write,\n\t\t\t       sizeof(execlists->csb_write[0]));\n\n\tinactive = process_csb(engine, inactive);  \n\n\t \n\treset_csb_pointers(engine);\n\n\treturn inactive;\n}\n\nstatic void\nexeclists_reset_active(struct intel_engine_cs *engine, bool stalled)\n{\n\tstruct intel_context *ce;\n\tstruct i915_request *rq;\n\tu32 head;\n\n\t \n\trq = active_context(engine, engine->execlists.reset_ccid);\n\tif (!rq)\n\t\treturn;\n\n\tce = rq->context;\n\tGEM_BUG_ON(!i915_vma_is_pinned(ce->state));\n\n\tif (__i915_request_is_complete(rq)) {\n\t\t \n\t\thead = intel_ring_wrap(ce->ring, rq->tail);\n\t\tgoto out_replay;\n\t}\n\n\t \n\tGEM_BUG_ON(!intel_engine_pm_is_awake(engine));\n\n\t \n\tGEM_BUG_ON(i915_active_is_idle(&ce->active));\n\n\trq = active_request(ce->timeline, rq);\n\thead = intel_ring_wrap(ce->ring, rq->head);\n\tGEM_BUG_ON(head == ce->ring->tail);\n\n\t \n\tif (!__i915_request_has_started(rq))\n\t\tgoto out_replay;\n\n\t \n\t__i915_request_reset(rq, stalled);\n\n\t \nout_replay:\n\tENGINE_TRACE(engine, \"replay {head:%04x, tail:%04x}\\n\",\n\t\t     head, ce->ring->tail);\n\tlrc_reset_regs(ce, engine);\n\tce->lrc.lrca = lrc_update_regs(ce, engine, head);\n}\n\nstatic void execlists_reset_csb(struct intel_engine_cs *engine, bool stalled)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tstruct i915_request *post[2 * EXECLIST_MAX_PORTS];\n\tstruct i915_request **inactive;\n\n\trcu_read_lock();\n\tinactive = reset_csb(engine, post);\n\n\texeclists_reset_active(engine, true);\n\n\tinactive = cancel_port_requests(execlists, inactive);\n\tpost_process_csb(post, inactive);\n\trcu_read_unlock();\n}\n\nstatic void execlists_reset_rewind(struct intel_engine_cs *engine, bool stalled)\n{\n\tunsigned long flags;\n\n\tENGINE_TRACE(engine, \"\\n\");\n\n\t \n\texeclists_reset_csb(engine, stalled);\n\n\t \n\trcu_read_lock();\n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\t__unwind_incomplete_requests(engine);\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n\trcu_read_unlock();\n}\n\nstatic void nop_submission_tasklet(struct tasklet_struct *t)\n{\n\tstruct i915_sched_engine *sched_engine =\n\t\tfrom_tasklet(sched_engine, t, tasklet);\n\tstruct intel_engine_cs * const engine = sched_engine->private_data;\n\n\t \n\tWRITE_ONCE(engine->sched_engine->queue_priority_hint, INT_MIN);\n}\n\nstatic void execlists_reset_cancel(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tstruct i915_sched_engine * const sched_engine = engine->sched_engine;\n\tstruct i915_request *rq, *rn;\n\tstruct rb_node *rb;\n\tunsigned long flags;\n\n\tENGINE_TRACE(engine, \"\\n\");\n\n\t \n\texeclists_reset_csb(engine, true);\n\n\trcu_read_lock();\n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\t \n\tlist_for_each_entry(rq, &engine->sched_engine->requests, sched.link)\n\t\ti915_request_put(i915_request_mark_eio(rq));\n\tintel_engine_signal_breadcrumbs(engine);\n\n\t \n\twhile ((rb = rb_first_cached(&sched_engine->queue))) {\n\t\tstruct i915_priolist *p = to_priolist(rb);\n\n\t\tpriolist_for_each_request_consume(rq, rn, p) {\n\t\t\tif (i915_request_mark_eio(rq)) {\n\t\t\t\t__i915_request_submit(rq);\n\t\t\t\ti915_request_put(rq);\n\t\t\t}\n\t\t}\n\n\t\trb_erase_cached(&p->node, &sched_engine->queue);\n\t\ti915_priolist_free(p);\n\t}\n\n\t \n\tlist_for_each_entry(rq, &sched_engine->hold, sched.link)\n\t\ti915_request_put(i915_request_mark_eio(rq));\n\n\t \n\twhile ((rb = rb_first_cached(&execlists->virtual))) {\n\t\tstruct virtual_engine *ve =\n\t\t\trb_entry(rb, typeof(*ve), nodes[engine->id].rb);\n\n\t\trb_erase_cached(rb, &execlists->virtual);\n\t\tRB_CLEAR_NODE(rb);\n\n\t\tspin_lock(&ve->base.sched_engine->lock);\n\t\trq = fetch_and_zero(&ve->request);\n\t\tif (rq) {\n\t\t\tif (i915_request_mark_eio(rq)) {\n\t\t\t\trq->engine = engine;\n\t\t\t\t__i915_request_submit(rq);\n\t\t\t\ti915_request_put(rq);\n\t\t\t}\n\t\t\ti915_request_put(rq);\n\n\t\t\tve->base.sched_engine->queue_priority_hint = INT_MIN;\n\t\t}\n\t\tspin_unlock(&ve->base.sched_engine->lock);\n\t}\n\n\t \n\n\tsched_engine->queue_priority_hint = INT_MIN;\n\tsched_engine->queue = RB_ROOT_CACHED;\n\n\tGEM_BUG_ON(__tasklet_is_enabled(&engine->sched_engine->tasklet));\n\tengine->sched_engine->tasklet.callback = nop_submission_tasklet;\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n\trcu_read_unlock();\n}\n\nstatic void execlists_reset_finish(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\n\t \n\tGEM_BUG_ON(!reset_in_progress(engine));\n\n\t \n\tif (__tasklet_enable(&engine->sched_engine->tasklet))\n\t\t__execlists_kick(execlists);\n\n\tENGINE_TRACE(engine, \"depth->%d\\n\",\n\t\t     atomic_read(&engine->sched_engine->tasklet.count));\n}\n\nstatic void gen8_logical_ring_enable_irq(struct intel_engine_cs *engine)\n{\n\tENGINE_WRITE(engine, RING_IMR,\n\t\t     ~(engine->irq_enable_mask | engine->irq_keep_mask));\n\tENGINE_POSTING_READ(engine, RING_IMR);\n}\n\nstatic void gen8_logical_ring_disable_irq(struct intel_engine_cs *engine)\n{\n\tENGINE_WRITE(engine, RING_IMR, ~engine->irq_keep_mask);\n}\n\nstatic void execlists_park(struct intel_engine_cs *engine)\n{\n\tcancel_timer(&engine->execlists.timer);\n\tcancel_timer(&engine->execlists.preempt);\n}\n\nstatic void add_to_engine(struct i915_request *rq)\n{\n\tlockdep_assert_held(&rq->engine->sched_engine->lock);\n\tlist_move_tail(&rq->sched.link, &rq->engine->sched_engine->requests);\n}\n\nstatic void remove_from_engine(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine, *locked;\n\n\t \n\tlocked = READ_ONCE(rq->engine);\n\tspin_lock_irq(&locked->sched_engine->lock);\n\twhile (unlikely(locked != (engine = READ_ONCE(rq->engine)))) {\n\t\tspin_unlock(&locked->sched_engine->lock);\n\t\tspin_lock(&engine->sched_engine->lock);\n\t\tlocked = engine;\n\t}\n\tlist_del_init(&rq->sched.link);\n\n\tclear_bit(I915_FENCE_FLAG_PQUEUE, &rq->fence.flags);\n\tclear_bit(I915_FENCE_FLAG_HOLD, &rq->fence.flags);\n\n\t \n\tset_bit(I915_FENCE_FLAG_ACTIVE, &rq->fence.flags);\n\n\tspin_unlock_irq(&locked->sched_engine->lock);\n\n\ti915_request_notify_execute_cb_imm(rq);\n}\n\nstatic bool can_preempt(struct intel_engine_cs *engine)\n{\n\tif (GRAPHICS_VER(engine->i915) > 8)\n\t\treturn true;\n\n\t \n\treturn engine->class != RENDER_CLASS;\n}\n\nstatic void kick_execlists(const struct i915_request *rq, int prio)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tstruct i915_sched_engine *sched_engine = engine->sched_engine;\n\tconst struct i915_request *inflight;\n\n\t \n\tif (prio <= sched_engine->queue_priority_hint)\n\t\treturn;\n\n\trcu_read_lock();\n\n\t \n\tinflight = execlists_active(&engine->execlists);\n\tif (!inflight)\n\t\tgoto unlock;\n\n\t \n\tif (inflight->context == rq->context)\n\t\tgoto unlock;\n\n\tENGINE_TRACE(engine,\n\t\t     \"bumping queue-priority-hint:%d for rq:%llx:%lld, inflight:%llx:%lld prio %d\\n\",\n\t\t     prio,\n\t\t     rq->fence.context, rq->fence.seqno,\n\t\t     inflight->fence.context, inflight->fence.seqno,\n\t\t     inflight->sched.attr.priority);\n\n\tsched_engine->queue_priority_hint = prio;\n\n\t \n\tif (prio >= max(I915_PRIORITY_NORMAL, rq_prio(inflight)))\n\t\ttasklet_hi_schedule(&sched_engine->tasklet);\n\nunlock:\n\trcu_read_unlock();\n}\n\nstatic void execlists_set_default_submission(struct intel_engine_cs *engine)\n{\n\tengine->submit_request = execlists_submit_request;\n\tengine->sched_engine->schedule = i915_schedule;\n\tengine->sched_engine->kick_backend = kick_execlists;\n\tengine->sched_engine->tasklet.callback = execlists_submission_tasklet;\n}\n\nstatic void execlists_shutdown(struct intel_engine_cs *engine)\n{\n\t \n\tdel_timer_sync(&engine->execlists.timer);\n\tdel_timer_sync(&engine->execlists.preempt);\n\ttasklet_kill(&engine->sched_engine->tasklet);\n}\n\nstatic void execlists_release(struct intel_engine_cs *engine)\n{\n\tengine->sanitize = NULL;  \n\n\texeclists_shutdown(engine);\n\n\tintel_engine_cleanup_common(engine);\n\tlrc_fini_wa_ctx(engine);\n}\n\nstatic ktime_t __execlists_engine_busyness(struct intel_engine_cs *engine,\n\t\t\t\t\t   ktime_t *now)\n{\n\tstruct intel_engine_execlists_stats *stats = &engine->stats.execlists;\n\tktime_t total = stats->total;\n\n\t \n\t*now = ktime_get();\n\tif (READ_ONCE(stats->active))\n\t\ttotal = ktime_add(total, ktime_sub(*now, stats->start));\n\n\treturn total;\n}\n\nstatic ktime_t execlists_engine_busyness(struct intel_engine_cs *engine,\n\t\t\t\t\t ktime_t *now)\n{\n\tstruct intel_engine_execlists_stats *stats = &engine->stats.execlists;\n\tunsigned int seq;\n\tktime_t total;\n\n\tdo {\n\t\tseq = read_seqcount_begin(&stats->lock);\n\t\ttotal = __execlists_engine_busyness(engine, now);\n\t} while (read_seqcount_retry(&stats->lock, seq));\n\n\treturn total;\n}\n\nstatic void\nlogical_ring_default_vfuncs(struct intel_engine_cs *engine)\n{\n\t \n\n\tengine->resume = execlists_resume;\n\n\tengine->cops = &execlists_context_ops;\n\tengine->request_alloc = execlists_request_alloc;\n\tengine->add_active_request = add_to_engine;\n\tengine->remove_active_request = remove_from_engine;\n\n\tengine->reset.prepare = execlists_reset_prepare;\n\tengine->reset.rewind = execlists_reset_rewind;\n\tengine->reset.cancel = execlists_reset_cancel;\n\tengine->reset.finish = execlists_reset_finish;\n\n\tengine->park = execlists_park;\n\tengine->unpark = NULL;\n\n\tengine->emit_flush = gen8_emit_flush_xcs;\n\tengine->emit_init_breadcrumb = gen8_emit_init_breadcrumb;\n\tengine->emit_fini_breadcrumb = gen8_emit_fini_breadcrumb_xcs;\n\tif (GRAPHICS_VER(engine->i915) >= 12) {\n\t\tengine->emit_fini_breadcrumb = gen12_emit_fini_breadcrumb_xcs;\n\t\tengine->emit_flush = gen12_emit_flush_xcs;\n\t}\n\tengine->set_default_submission = execlists_set_default_submission;\n\n\tif (GRAPHICS_VER(engine->i915) < 11) {\n\t\tengine->irq_enable = gen8_logical_ring_enable_irq;\n\t\tengine->irq_disable = gen8_logical_ring_disable_irq;\n\t} else {\n\t\t \n\t}\n\tintel_engine_set_irq_handler(engine, execlists_irq_handler);\n\n\tengine->flags |= I915_ENGINE_SUPPORTS_STATS;\n\tif (!intel_vgpu_active(engine->i915)) {\n\t\tengine->flags |= I915_ENGINE_HAS_SEMAPHORES;\n\t\tif (can_preempt(engine)) {\n\t\t\tengine->flags |= I915_ENGINE_HAS_PREEMPTION;\n\t\t\tif (CONFIG_DRM_I915_TIMESLICE_DURATION)\n\t\t\t\tengine->flags |= I915_ENGINE_HAS_TIMESLICES;\n\t\t}\n\t}\n\n\tif (GRAPHICS_VER_FULL(engine->i915) >= IP_VER(12, 50)) {\n\t\tif (intel_engine_has_preemption(engine))\n\t\t\tengine->emit_bb_start = xehp_emit_bb_start;\n\t\telse\n\t\t\tengine->emit_bb_start = xehp_emit_bb_start_noarb;\n\t} else {\n\t\tif (intel_engine_has_preemption(engine))\n\t\t\tengine->emit_bb_start = gen8_emit_bb_start;\n\t\telse\n\t\t\tengine->emit_bb_start = gen8_emit_bb_start_noarb;\n\t}\n\n\tengine->busyness = execlists_engine_busyness;\n}\n\nstatic void logical_ring_default_irqs(struct intel_engine_cs *engine)\n{\n\tunsigned int shift = 0;\n\n\tif (GRAPHICS_VER(engine->i915) < 11) {\n\t\tconst u8 irq_shifts[] = {\n\t\t\t[RCS0]  = GEN8_RCS_IRQ_SHIFT,\n\t\t\t[BCS0]  = GEN8_BCS_IRQ_SHIFT,\n\t\t\t[VCS0]  = GEN8_VCS0_IRQ_SHIFT,\n\t\t\t[VCS1]  = GEN8_VCS1_IRQ_SHIFT,\n\t\t\t[VECS0] = GEN8_VECS_IRQ_SHIFT,\n\t\t};\n\n\t\tshift = irq_shifts[engine->id];\n\t}\n\n\tengine->irq_enable_mask = GT_RENDER_USER_INTERRUPT << shift;\n\tengine->irq_keep_mask = GT_CONTEXT_SWITCH_INTERRUPT << shift;\n\tengine->irq_keep_mask |= GT_CS_MASTER_ERROR_INTERRUPT << shift;\n\tengine->irq_keep_mask |= GT_WAIT_SEMAPHORE_INTERRUPT << shift;\n}\n\nstatic void rcs_submission_override(struct intel_engine_cs *engine)\n{\n\tswitch (GRAPHICS_VER(engine->i915)) {\n\tcase 12:\n\t\tengine->emit_flush = gen12_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen12_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\tcase 11:\n\t\tengine->emit_flush = gen11_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen11_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\tdefault:\n\t\tengine->emit_flush = gen8_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen8_emit_fini_breadcrumb_rcs;\n\t\tbreak;\n\t}\n}\n\nint intel_execlists_submission_setup(struct intel_engine_cs *engine)\n{\n\tstruct intel_engine_execlists * const execlists = &engine->execlists;\n\tstruct drm_i915_private *i915 = engine->i915;\n\tstruct intel_uncore *uncore = engine->uncore;\n\tu32 base = engine->mmio_base;\n\n\ttasklet_setup(&engine->sched_engine->tasklet, execlists_submission_tasklet);\n\ttimer_setup(&engine->execlists.timer, execlists_timeslice, 0);\n\ttimer_setup(&engine->execlists.preempt, execlists_preempt, 0);\n\n\tlogical_ring_default_vfuncs(engine);\n\tlogical_ring_default_irqs(engine);\n\n\tseqcount_init(&engine->stats.execlists.lock);\n\n\tif (engine->flags & I915_ENGINE_HAS_RCS_REG_STATE)\n\t\trcs_submission_override(engine);\n\n\tlrc_init_wa_ctx(engine);\n\n\tif (HAS_LOGICAL_RING_ELSQ(i915)) {\n\t\texeclists->submit_reg = intel_uncore_regs(uncore) +\n\t\t\ti915_mmio_reg_offset(RING_EXECLIST_SQ_CONTENTS(base));\n\t\texeclists->ctrl_reg = intel_uncore_regs(uncore) +\n\t\t\ti915_mmio_reg_offset(RING_EXECLIST_CONTROL(base));\n\n\t\tengine->fw_domain = intel_uncore_forcewake_for_reg(engine->uncore,\n\t\t\t\t    RING_EXECLIST_CONTROL(engine->mmio_base),\n\t\t\t\t    FW_REG_WRITE);\n\t} else {\n\t\texeclists->submit_reg = intel_uncore_regs(uncore) +\n\t\t\ti915_mmio_reg_offset(RING_ELSP(base));\n\t}\n\n\texeclists->csb_status =\n\t\t(u64 *)&engine->status_page.addr[I915_HWS_CSB_BUF0_INDEX];\n\n\texeclists->csb_write =\n\t\t&engine->status_page.addr[INTEL_HWS_CSB_WRITE_INDEX(i915)];\n\n\tif (GRAPHICS_VER(i915) < 11)\n\t\texeclists->csb_size = GEN8_CSB_ENTRIES;\n\telse\n\t\texeclists->csb_size = GEN11_CSB_ENTRIES;\n\n\tengine->context_tag = GENMASK(BITS_PER_LONG - 2, 0);\n\tif (GRAPHICS_VER(engine->i915) >= 11 &&\n\t    GRAPHICS_VER_FULL(engine->i915) < IP_VER(12, 50)) {\n\t\texeclists->ccid |= engine->instance << (GEN11_ENGINE_INSTANCE_SHIFT - 32);\n\t\texeclists->ccid |= engine->class << (GEN11_ENGINE_CLASS_SHIFT - 32);\n\t}\n\n\t \n\tengine->sanitize = execlists_sanitize;\n\tengine->release = execlists_release;\n\n\treturn 0;\n}\n\nstatic struct list_head *virtual_queue(struct virtual_engine *ve)\n{\n\treturn &ve->base.sched_engine->default_priolist.requests;\n}\n\nstatic void rcu_virtual_context_destroy(struct work_struct *wrk)\n{\n\tstruct virtual_engine *ve =\n\t\tcontainer_of(wrk, typeof(*ve), rcu.work);\n\tunsigned int n;\n\n\tGEM_BUG_ON(ve->context.inflight);\n\n\t \n\tif (unlikely(ve->request)) {\n\t\tstruct i915_request *old;\n\n\t\tspin_lock_irq(&ve->base.sched_engine->lock);\n\n\t\told = fetch_and_zero(&ve->request);\n\t\tif (old) {\n\t\t\tGEM_BUG_ON(!__i915_request_is_complete(old));\n\t\t\t__i915_request_submit(old);\n\t\t\ti915_request_put(old);\n\t\t}\n\n\t\tspin_unlock_irq(&ve->base.sched_engine->lock);\n\t}\n\n\t \n\ttasklet_kill(&ve->base.sched_engine->tasklet);\n\n\t \n\tfor (n = 0; n < ve->num_siblings; n++) {\n\t\tstruct intel_engine_cs *sibling = ve->siblings[n];\n\t\tstruct rb_node *node = &ve->nodes[sibling->id].rb;\n\n\t\tif (RB_EMPTY_NODE(node))\n\t\t\tcontinue;\n\n\t\tspin_lock_irq(&sibling->sched_engine->lock);\n\n\t\t \n\t\tif (!RB_EMPTY_NODE(node))\n\t\t\trb_erase_cached(node, &sibling->execlists.virtual);\n\n\t\tspin_unlock_irq(&sibling->sched_engine->lock);\n\t}\n\tGEM_BUG_ON(__tasklet_is_scheduled(&ve->base.sched_engine->tasklet));\n\tGEM_BUG_ON(!list_empty(virtual_queue(ve)));\n\n\tlrc_fini(&ve->context);\n\tintel_context_fini(&ve->context);\n\n\tif (ve->base.breadcrumbs)\n\t\tintel_breadcrumbs_put(ve->base.breadcrumbs);\n\tif (ve->base.sched_engine)\n\t\ti915_sched_engine_put(ve->base.sched_engine);\n\tintel_engine_free_request_pool(&ve->base);\n\n\tkfree(ve);\n}\n\nstatic void virtual_context_destroy(struct kref *kref)\n{\n\tstruct virtual_engine *ve =\n\t\tcontainer_of(kref, typeof(*ve), context.ref);\n\n\tGEM_BUG_ON(!list_empty(&ve->context.signals));\n\n\t \n\tINIT_RCU_WORK(&ve->rcu, rcu_virtual_context_destroy);\n\tqueue_rcu_work(ve->context.engine->i915->unordered_wq, &ve->rcu);\n}\n\nstatic void virtual_engine_initial_hint(struct virtual_engine *ve)\n{\n\tint swp;\n\n\t \n\tswp = get_random_u32_below(ve->num_siblings);\n\tif (swp)\n\t\tswap(ve->siblings[swp], ve->siblings[0]);\n}\n\nstatic int virtual_context_alloc(struct intel_context *ce)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\n\treturn lrc_alloc(ce, ve->siblings[0]);\n}\n\nstatic int virtual_context_pre_pin(struct intel_context *ce,\n\t\t\t\t   struct i915_gem_ww_ctx *ww,\n\t\t\t\t   void **vaddr)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\n\t  \n\treturn __execlists_context_pre_pin(ce, ve->siblings[0], ww, vaddr);\n}\n\nstatic int virtual_context_pin(struct intel_context *ce, void *vaddr)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\n\treturn lrc_pin(ce, ve->siblings[0], vaddr);\n}\n\nstatic void virtual_context_enter(struct intel_context *ce)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\tunsigned int n;\n\n\tfor (n = 0; n < ve->num_siblings; n++)\n\t\tintel_engine_pm_get(ve->siblings[n]);\n\n\tintel_timeline_enter(ce->timeline);\n}\n\nstatic void virtual_context_exit(struct intel_context *ce)\n{\n\tstruct virtual_engine *ve = container_of(ce, typeof(*ve), context);\n\tunsigned int n;\n\n\tintel_timeline_exit(ce->timeline);\n\n\tfor (n = 0; n < ve->num_siblings; n++)\n\t\tintel_engine_pm_put(ve->siblings[n]);\n}\n\nstatic struct intel_engine_cs *\nvirtual_get_sibling(struct intel_engine_cs *engine, unsigned int sibling)\n{\n\tstruct virtual_engine *ve = to_virtual_engine(engine);\n\n\tif (sibling >= ve->num_siblings)\n\t\treturn NULL;\n\n\treturn ve->siblings[sibling];\n}\n\nstatic const struct intel_context_ops virtual_context_ops = {\n\t.flags = COPS_HAS_INFLIGHT | COPS_RUNTIME_CYCLES,\n\n\t.alloc = virtual_context_alloc,\n\n\t.cancel_request = execlists_context_cancel_request,\n\n\t.pre_pin = virtual_context_pre_pin,\n\t.pin = virtual_context_pin,\n\t.unpin = lrc_unpin,\n\t.post_unpin = lrc_post_unpin,\n\n\t.enter = virtual_context_enter,\n\t.exit = virtual_context_exit,\n\n\t.destroy = virtual_context_destroy,\n\n\t.get_sibling = virtual_get_sibling,\n};\n\nstatic intel_engine_mask_t virtual_submission_mask(struct virtual_engine *ve)\n{\n\tstruct i915_request *rq;\n\tintel_engine_mask_t mask;\n\n\trq = READ_ONCE(ve->request);\n\tif (!rq)\n\t\treturn 0;\n\n\t \n\tmask = rq->execution_mask;\n\tif (unlikely(!mask)) {\n\t\t \n\t\ti915_request_set_error_once(rq, -ENODEV);\n\t\tmask = ve->siblings[0]->mask;\n\t}\n\n\tENGINE_TRACE(&ve->base, \"rq=%llx:%lld, mask=%x, prio=%d\\n\",\n\t\t     rq->fence.context, rq->fence.seqno,\n\t\t     mask, ve->base.sched_engine->queue_priority_hint);\n\n\treturn mask;\n}\n\nstatic void virtual_submission_tasklet(struct tasklet_struct *t)\n{\n\tstruct i915_sched_engine *sched_engine =\n\t\tfrom_tasklet(sched_engine, t, tasklet);\n\tstruct virtual_engine * const ve =\n\t\t(struct virtual_engine *)sched_engine->private_data;\n\tconst int prio = READ_ONCE(sched_engine->queue_priority_hint);\n\tintel_engine_mask_t mask;\n\tunsigned int n;\n\n\trcu_read_lock();\n\tmask = virtual_submission_mask(ve);\n\trcu_read_unlock();\n\tif (unlikely(!mask))\n\t\treturn;\n\n\tfor (n = 0; n < ve->num_siblings; n++) {\n\t\tstruct intel_engine_cs *sibling = READ_ONCE(ve->siblings[n]);\n\t\tstruct ve_node * const node = &ve->nodes[sibling->id];\n\t\tstruct rb_node **parent, *rb;\n\t\tbool first;\n\n\t\tif (!READ_ONCE(ve->request))\n\t\t\tbreak;  \n\n\t\tspin_lock_irq(&sibling->sched_engine->lock);\n\n\t\tif (unlikely(!(mask & sibling->mask))) {\n\t\t\tif (!RB_EMPTY_NODE(&node->rb)) {\n\t\t\t\trb_erase_cached(&node->rb,\n\t\t\t\t\t\t&sibling->execlists.virtual);\n\t\t\t\tRB_CLEAR_NODE(&node->rb);\n\t\t\t}\n\n\t\t\tgoto unlock_engine;\n\t\t}\n\n\t\tif (unlikely(!RB_EMPTY_NODE(&node->rb))) {\n\t\t\t \n\t\t\tfirst = rb_first_cached(&sibling->execlists.virtual) ==\n\t\t\t\t&node->rb;\n\t\t\tif (prio == node->prio || (prio > node->prio && first))\n\t\t\t\tgoto submit_engine;\n\n\t\t\trb_erase_cached(&node->rb, &sibling->execlists.virtual);\n\t\t}\n\n\t\trb = NULL;\n\t\tfirst = true;\n\t\tparent = &sibling->execlists.virtual.rb_root.rb_node;\n\t\twhile (*parent) {\n\t\t\tstruct ve_node *other;\n\n\t\t\trb = *parent;\n\t\t\tother = rb_entry(rb, typeof(*other), rb);\n\t\t\tif (prio > other->prio) {\n\t\t\t\tparent = &rb->rb_left;\n\t\t\t} else {\n\t\t\t\tparent = &rb->rb_right;\n\t\t\t\tfirst = false;\n\t\t\t}\n\t\t}\n\n\t\trb_link_node(&node->rb, rb, parent);\n\t\trb_insert_color_cached(&node->rb,\n\t\t\t\t       &sibling->execlists.virtual,\n\t\t\t\t       first);\n\nsubmit_engine:\n\t\tGEM_BUG_ON(RB_EMPTY_NODE(&node->rb));\n\t\tnode->prio = prio;\n\t\tif (first && prio > sibling->sched_engine->queue_priority_hint)\n\t\t\ttasklet_hi_schedule(&sibling->sched_engine->tasklet);\n\nunlock_engine:\n\t\tspin_unlock_irq(&sibling->sched_engine->lock);\n\n\t\tif (intel_context_inflight(&ve->context))\n\t\t\tbreak;\n\t}\n}\n\nstatic void virtual_submit_request(struct i915_request *rq)\n{\n\tstruct virtual_engine *ve = to_virtual_engine(rq->engine);\n\tunsigned long flags;\n\n\tENGINE_TRACE(&ve->base, \"rq=%llx:%lld\\n\",\n\t\t     rq->fence.context,\n\t\t     rq->fence.seqno);\n\n\tGEM_BUG_ON(ve->base.submit_request != virtual_submit_request);\n\n\tspin_lock_irqsave(&ve->base.sched_engine->lock, flags);\n\n\t \n\tif (__i915_request_is_complete(rq)) {\n\t\t__i915_request_submit(rq);\n\t\tgoto unlock;\n\t}\n\n\tif (ve->request) {  \n\t\tGEM_BUG_ON(!__i915_request_is_complete(ve->request));\n\t\t__i915_request_submit(ve->request);\n\t\ti915_request_put(ve->request);\n\t}\n\n\tve->base.sched_engine->queue_priority_hint = rq_prio(rq);\n\tve->request = i915_request_get(rq);\n\n\tGEM_BUG_ON(!list_empty(virtual_queue(ve)));\n\tlist_move_tail(&rq->sched.link, virtual_queue(ve));\n\n\ttasklet_hi_schedule(&ve->base.sched_engine->tasklet);\n\nunlock:\n\tspin_unlock_irqrestore(&ve->base.sched_engine->lock, flags);\n}\n\nstatic struct intel_context *\nexeclists_create_virtual(struct intel_engine_cs **siblings, unsigned int count,\n\t\t\t unsigned long flags)\n{\n\tstruct drm_i915_private *i915 = siblings[0]->i915;\n\tstruct virtual_engine *ve;\n\tunsigned int n;\n\tint err;\n\n\tve = kzalloc(struct_size(ve, siblings, count), GFP_KERNEL);\n\tif (!ve)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tve->base.i915 = i915;\n\tve->base.gt = siblings[0]->gt;\n\tve->base.uncore = siblings[0]->uncore;\n\tve->base.id = -1;\n\n\tve->base.class = OTHER_CLASS;\n\tve->base.uabi_class = I915_ENGINE_CLASS_INVALID;\n\tve->base.instance = I915_ENGINE_CLASS_INVALID_VIRTUAL;\n\tve->base.uabi_instance = I915_ENGINE_CLASS_INVALID_VIRTUAL;\n\n\t \n\tve->base.saturated = ALL_ENGINES;\n\n\tsnprintf(ve->base.name, sizeof(ve->base.name), \"virtual\");\n\n\tintel_engine_init_execlists(&ve->base);\n\n\tve->base.sched_engine = i915_sched_engine_create(ENGINE_VIRTUAL);\n\tif (!ve->base.sched_engine) {\n\t\terr = -ENOMEM;\n\t\tgoto err_put;\n\t}\n\tve->base.sched_engine->private_data = &ve->base;\n\n\tve->base.cops = &virtual_context_ops;\n\tve->base.request_alloc = execlists_request_alloc;\n\n\tve->base.sched_engine->schedule = i915_schedule;\n\tve->base.sched_engine->kick_backend = kick_execlists;\n\tve->base.submit_request = virtual_submit_request;\n\n\tINIT_LIST_HEAD(virtual_queue(ve));\n\ttasklet_setup(&ve->base.sched_engine->tasklet, virtual_submission_tasklet);\n\n\tintel_context_init(&ve->context, &ve->base);\n\n\tve->base.breadcrumbs = intel_breadcrumbs_create(NULL);\n\tif (!ve->base.breadcrumbs) {\n\t\terr = -ENOMEM;\n\t\tgoto err_put;\n\t}\n\n\tfor (n = 0; n < count; n++) {\n\t\tstruct intel_engine_cs *sibling = siblings[n];\n\n\t\tGEM_BUG_ON(!is_power_of_2(sibling->mask));\n\t\tif (sibling->mask & ve->base.mask) {\n\t\t\tdrm_dbg(&i915->drm,\n\t\t\t\t\"duplicate %s entry in load balancer\\n\",\n\t\t\t\tsibling->name);\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_put;\n\t\t}\n\n\t\t \n\t\tif (sibling->sched_engine->tasklet.callback !=\n\t\t    execlists_submission_tasklet) {\n\t\t\terr = -ENODEV;\n\t\t\tgoto err_put;\n\t\t}\n\n\t\tGEM_BUG_ON(RB_EMPTY_NODE(&ve->nodes[sibling->id].rb));\n\t\tRB_CLEAR_NODE(&ve->nodes[sibling->id].rb);\n\n\t\tve->siblings[ve->num_siblings++] = sibling;\n\t\tve->base.mask |= sibling->mask;\n\t\tve->base.logical_mask |= sibling->logical_mask;\n\n\t\t \n\t\tif (ve->base.class != OTHER_CLASS) {\n\t\t\tif (ve->base.class != sibling->class) {\n\t\t\t\tdrm_dbg(&i915->drm,\n\t\t\t\t\t\"invalid mixing of engine class, sibling %d, already %d\\n\",\n\t\t\t\t\tsibling->class, ve->base.class);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_put;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tve->base.class = sibling->class;\n\t\tve->base.uabi_class = sibling->uabi_class;\n\t\tsnprintf(ve->base.name, sizeof(ve->base.name),\n\t\t\t \"v%dx%d\", ve->base.class, count);\n\t\tve->base.context_size = sibling->context_size;\n\n\t\tve->base.add_active_request = sibling->add_active_request;\n\t\tve->base.remove_active_request = sibling->remove_active_request;\n\t\tve->base.emit_bb_start = sibling->emit_bb_start;\n\t\tve->base.emit_flush = sibling->emit_flush;\n\t\tve->base.emit_init_breadcrumb = sibling->emit_init_breadcrumb;\n\t\tve->base.emit_fini_breadcrumb = sibling->emit_fini_breadcrumb;\n\t\tve->base.emit_fini_breadcrumb_dw =\n\t\t\tsibling->emit_fini_breadcrumb_dw;\n\n\t\tve->base.flags = sibling->flags;\n\t}\n\n\tve->base.flags |= I915_ENGINE_IS_VIRTUAL;\n\n\tvirtual_engine_initial_hint(ve);\n\treturn &ve->context;\n\nerr_put:\n\tintel_context_put(&ve->context);\n\treturn ERR_PTR(err);\n}\n\nvoid intel_execlists_show_requests(struct intel_engine_cs *engine,\n\t\t\t\t   struct drm_printer *m,\n\t\t\t\t   void (*show_request)(struct drm_printer *m,\n\t\t\t\t\t\t\tconst struct i915_request *rq,\n\t\t\t\t\t\t\tconst char *prefix,\n\t\t\t\t\t\t\tint indent),\n\t\t\t\t   unsigned int max)\n{\n\tconst struct intel_engine_execlists *execlists = &engine->execlists;\n\tstruct i915_sched_engine *sched_engine = engine->sched_engine;\n\tstruct i915_request *rq, *last;\n\tunsigned long flags;\n\tunsigned int count;\n\tstruct rb_node *rb;\n\n\tspin_lock_irqsave(&sched_engine->lock, flags);\n\n\tlast = NULL;\n\tcount = 0;\n\tlist_for_each_entry(rq, &sched_engine->requests, sched.link) {\n\t\tif (count++ < max - 1)\n\t\t\tshow_request(m, rq, \"\\t\\t\", 0);\n\t\telse\n\t\t\tlast = rq;\n\t}\n\tif (last) {\n\t\tif (count > max) {\n\t\t\tdrm_printf(m,\n\t\t\t\t   \"\\t\\t...skipping %d executing requests...\\n\",\n\t\t\t\t   count - max);\n\t\t}\n\t\tshow_request(m, last, \"\\t\\t\", 0);\n\t}\n\n\tif (sched_engine->queue_priority_hint != INT_MIN)\n\t\tdrm_printf(m, \"\\t\\tQueue priority hint: %d\\n\",\n\t\t\t   READ_ONCE(sched_engine->queue_priority_hint));\n\n\tlast = NULL;\n\tcount = 0;\n\tfor (rb = rb_first_cached(&sched_engine->queue); rb; rb = rb_next(rb)) {\n\t\tstruct i915_priolist *p = rb_entry(rb, typeof(*p), node);\n\n\t\tpriolist_for_each_request(rq, p) {\n\t\t\tif (count++ < max - 1)\n\t\t\t\tshow_request(m, rq, \"\\t\\t\", 0);\n\t\t\telse\n\t\t\t\tlast = rq;\n\t\t}\n\t}\n\tif (last) {\n\t\tif (count > max) {\n\t\t\tdrm_printf(m,\n\t\t\t\t   \"\\t\\t...skipping %d queued requests...\\n\",\n\t\t\t\t   count - max);\n\t\t}\n\t\tshow_request(m, last, \"\\t\\t\", 0);\n\t}\n\n\tlast = NULL;\n\tcount = 0;\n\tfor (rb = rb_first_cached(&execlists->virtual); rb; rb = rb_next(rb)) {\n\t\tstruct virtual_engine *ve =\n\t\t\trb_entry(rb, typeof(*ve), nodes[engine->id].rb);\n\t\tstruct i915_request *rq = READ_ONCE(ve->request);\n\n\t\tif (rq) {\n\t\t\tif (count++ < max - 1)\n\t\t\t\tshow_request(m, rq, \"\\t\\t\", 0);\n\t\t\telse\n\t\t\t\tlast = rq;\n\t\t}\n\t}\n\tif (last) {\n\t\tif (count > max) {\n\t\t\tdrm_printf(m,\n\t\t\t\t   \"\\t\\t...skipping %d virtual requests...\\n\",\n\t\t\t\t   count - max);\n\t\t}\n\t\tshow_request(m, last, \"\\t\\t\", 0);\n\t}\n\n\tspin_unlock_irqrestore(&sched_engine->lock, flags);\n}\n\nvoid intel_execlists_dump_active_requests(struct intel_engine_cs *engine,\n\t\t\t\t\t  struct i915_request *hung_rq,\n\t\t\t\t\t  struct drm_printer *m)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\tintel_engine_dump_active_requests(&engine->sched_engine->requests, hung_rq, m);\n\n\tdrm_printf(m, \"\\tOn hold?: %zu\\n\",\n\t\t   list_count_nodes(&engine->sched_engine->hold));\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_execlists.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}