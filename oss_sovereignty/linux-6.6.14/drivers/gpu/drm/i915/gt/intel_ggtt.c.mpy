{
  "module_name": "intel_ggtt.c",
  "hash_id": "a01fb5d8f1c329c619701003089f02023e03c5f722b7b089dbc14ecd4832b03f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_ggtt.c",
  "human_readable_source": "\n \n\n#include <asm/set_memory.h>\n#include <asm/smp.h>\n#include <linux/types.h>\n#include <linux/stop_machine.h>\n\n#include <drm/drm_managed.h>\n#include <drm/i915_drm.h>\n#include <drm/intel-gtt.h>\n\n#include \"display/intel_display.h\"\n#include \"gem/i915_gem_lmem.h\"\n\n#include \"intel_ggtt_gmch.h\"\n#include \"intel_gt.h\"\n#include \"intel_gt_regs.h\"\n#include \"intel_pci_config.h\"\n#include \"i915_drv.h\"\n#include \"i915_pci.h\"\n#include \"i915_scatterlist.h\"\n#include \"i915_utils.h\"\n#include \"i915_vgpu.h\"\n\n#include \"intel_gtt.h\"\n#include \"gen8_ppgtt.h\"\n\nstatic void i915_ggtt_color_adjust(const struct drm_mm_node *node,\n\t\t\t\t   unsigned long color,\n\t\t\t\t   u64 *start,\n\t\t\t\t   u64 *end)\n{\n\tif (i915_node_color_differs(node, color))\n\t\t*start += I915_GTT_PAGE_SIZE;\n\n\t \n\tnode = list_next_entry(node, node_list);\n\tif (node->color != color)\n\t\t*end -= I915_GTT_PAGE_SIZE;\n}\n\nstatic int ggtt_init_hw(struct i915_ggtt *ggtt)\n{\n\tstruct drm_i915_private *i915 = ggtt->vm.i915;\n\n\ti915_address_space_init(&ggtt->vm, VM_CLASS_GGTT);\n\n\tggtt->vm.is_ggtt = true;\n\n\t \n\tggtt->vm.has_read_only = IS_VALLEYVIEW(i915);\n\n\tif (!HAS_LLC(i915) && !HAS_PPGTT(i915))\n\t\tggtt->vm.mm.color_adjust = i915_ggtt_color_adjust;\n\n\tif (ggtt->mappable_end) {\n\t\tif (!io_mapping_init_wc(&ggtt->iomap,\n\t\t\t\t\tggtt->gmadr.start,\n\t\t\t\t\tggtt->mappable_end)) {\n\t\t\tggtt->vm.cleanup(&ggtt->vm);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tggtt->mtrr = arch_phys_wc_add(ggtt->gmadr.start,\n\t\t\t\t\t      ggtt->mappable_end);\n\t}\n\n\tintel_ggtt_init_fences(ggtt);\n\n\treturn 0;\n}\n\n \nint i915_ggtt_init_hw(struct drm_i915_private *i915)\n{\n\tint ret;\n\n\t \n\tret = ggtt_init_hw(to_gt(i915)->ggtt);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n \nvoid i915_ggtt_suspend_vm(struct i915_address_space *vm)\n{\n\tstruct i915_vma *vma, *vn;\n\tint save_skip_rewrite;\n\n\tdrm_WARN_ON(&vm->i915->drm, !vm->is_ggtt && !vm->is_dpt);\n\nretry:\n\ti915_gem_drain_freed_objects(vm->i915);\n\n\tmutex_lock(&vm->mutex);\n\n\t \n\tsave_skip_rewrite = vm->skip_pte_rewrite;\n\tvm->skip_pte_rewrite = true;\n\n\tlist_for_each_entry_safe(vma, vn, &vm->bound_list, vm_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\t\tGEM_BUG_ON(!drm_mm_node_allocated(&vma->node));\n\n\t\tif (i915_vma_is_pinned(vma) || !i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (WARN_ON(!i915_gem_object_trylock(obj, NULL))) {\n\t\t\t \n\t\t\ti915_gem_object_get(obj);\n\n\t\t\tmutex_unlock(&vm->mutex);\n\n\t\t\ti915_gem_object_lock(obj, NULL);\n\t\t\tGEM_WARN_ON(i915_vma_unbind(vma));\n\t\t\ti915_gem_object_unlock(obj);\n\t\t\ti915_gem_object_put(obj);\n\n\t\t\tvm->skip_pte_rewrite = save_skip_rewrite;\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (!i915_vma_is_bound(vma, I915_VMA_GLOBAL_BIND)) {\n\t\t\ti915_vma_wait_for_bind(vma);\n\n\t\t\t__i915_vma_evict(vma, false);\n\t\t\tdrm_mm_remove_node(&vma->node);\n\t\t}\n\n\t\ti915_gem_object_unlock(obj);\n\t}\n\n\tvm->clear_range(vm, 0, vm->total);\n\n\tvm->skip_pte_rewrite = save_skip_rewrite;\n\n\tmutex_unlock(&vm->mutex);\n}\n\nvoid i915_ggtt_suspend(struct i915_ggtt *ggtt)\n{\n\tstruct intel_gt *gt;\n\n\ti915_ggtt_suspend_vm(&ggtt->vm);\n\tggtt->invalidate(ggtt);\n\n\tlist_for_each_entry(gt, &ggtt->gt_list, ggtt_link)\n\t\tintel_gt_check_and_clear_faults(gt);\n}\n\nvoid gen6_ggtt_invalidate(struct i915_ggtt *ggtt)\n{\n\tstruct intel_uncore *uncore = ggtt->vm.gt->uncore;\n\n\tspin_lock_irq(&uncore->lock);\n\tintel_uncore_write_fw(uncore, GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);\n\tintel_uncore_read_fw(uncore, GFX_FLSH_CNTL_GEN6);\n\tspin_unlock_irq(&uncore->lock);\n}\n\nstatic bool needs_wc_ggtt_mapping(struct drm_i915_private *i915)\n{\n\t \n\tif (!IS_GEN9_LP(i915) && GRAPHICS_VER(i915) < 11)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void gen8_ggtt_invalidate(struct i915_ggtt *ggtt)\n{\n\tstruct intel_uncore *uncore = ggtt->vm.gt->uncore;\n\n\t \n\tif (needs_wc_ggtt_mapping(ggtt->vm.i915))\n\t\tintel_uncore_write_fw(uncore, GFX_FLSH_CNTL_GEN6,\n\t\t\t\t      GFX_FLSH_CNTL_EN);\n}\n\nstatic void guc_ggtt_invalidate(struct i915_ggtt *ggtt)\n{\n\tstruct drm_i915_private *i915 = ggtt->vm.i915;\n\n\tgen8_ggtt_invalidate(ggtt);\n\n\tif (GRAPHICS_VER(i915) >= 12) {\n\t\tstruct intel_gt *gt;\n\n\t\tlist_for_each_entry(gt, &ggtt->gt_list, ggtt_link)\n\t\t\tintel_uncore_write_fw(gt->uncore,\n\t\t\t\t\t      GEN12_GUC_TLB_INV_CR,\n\t\t\t\t\t      GEN12_GUC_TLB_INV_CR_INVALIDATE);\n\t} else {\n\t\tintel_uncore_write_fw(ggtt->vm.gt->uncore,\n\t\t\t\t      GEN8_GTCR, GEN8_GTCR_INVALIDATE);\n\t}\n}\n\nstatic u64 mtl_ggtt_pte_encode(dma_addr_t addr,\n\t\t\t       unsigned int pat_index,\n\t\t\t       u32 flags)\n{\n\tgen8_pte_t pte = addr | GEN8_PAGE_PRESENT;\n\n\tWARN_ON_ONCE(addr & ~GEN12_GGTT_PTE_ADDR_MASK);\n\n\tif (flags & PTE_LM)\n\t\tpte |= GEN12_GGTT_PTE_LM;\n\n\tif (pat_index & BIT(0))\n\t\tpte |= MTL_GGTT_PTE_PAT0;\n\n\tif (pat_index & BIT(1))\n\t\tpte |= MTL_GGTT_PTE_PAT1;\n\n\treturn pte;\n}\n\nu64 gen8_ggtt_pte_encode(dma_addr_t addr,\n\t\t\t unsigned int pat_index,\n\t\t\t u32 flags)\n{\n\tgen8_pte_t pte = addr | GEN8_PAGE_PRESENT;\n\n\tif (flags & PTE_LM)\n\t\tpte |= GEN12_GGTT_PTE_LM;\n\n\treturn pte;\n}\n\nstatic void gen8_set_pte(void __iomem *addr, gen8_pte_t pte)\n{\n\twriteq(pte, addr);\n}\n\nstatic void gen8_ggtt_insert_page(struct i915_address_space *vm,\n\t\t\t\t  dma_addr_t addr,\n\t\t\t\t  u64 offset,\n\t\t\t\t  unsigned int pat_index,\n\t\t\t\t  u32 flags)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tgen8_pte_t __iomem *pte =\n\t\t(gen8_pte_t __iomem *)ggtt->gsm + offset / I915_GTT_PAGE_SIZE;\n\n\tgen8_set_pte(pte, ggtt->vm.pte_encode(addr, pat_index, flags));\n\n\tggtt->invalidate(ggtt);\n}\n\nstatic void gen8_ggtt_insert_entries(struct i915_address_space *vm,\n\t\t\t\t     struct i915_vma_resource *vma_res,\n\t\t\t\t     unsigned int pat_index,\n\t\t\t\t     u32 flags)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tconst gen8_pte_t pte_encode = ggtt->vm.pte_encode(0, pat_index, flags);\n\tgen8_pte_t __iomem *gte;\n\tgen8_pte_t __iomem *end;\n\tstruct sgt_iter iter;\n\tdma_addr_t addr;\n\n\t \n\n\tgte = (gen8_pte_t __iomem *)ggtt->gsm;\n\tgte += (vma_res->start - vma_res->guard) / I915_GTT_PAGE_SIZE;\n\tend = gte + vma_res->guard / I915_GTT_PAGE_SIZE;\n\twhile (gte < end)\n\t\tgen8_set_pte(gte++, vm->scratch[0]->encode);\n\tend += (vma_res->node_size + vma_res->guard) / I915_GTT_PAGE_SIZE;\n\n\tfor_each_sgt_daddr(addr, iter, vma_res->bi.pages)\n\t\tgen8_set_pte(gte++, pte_encode | addr);\n\tGEM_BUG_ON(gte > end);\n\n\t \n\twhile (gte < end)\n\t\tgen8_set_pte(gte++, vm->scratch[0]->encode);\n\n\t \n\tggtt->invalidate(ggtt);\n}\n\nstatic void gen8_ggtt_clear_range(struct i915_address_space *vm,\n\t\t\t\t  u64 start, u64 length)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tunsigned int first_entry = start / I915_GTT_PAGE_SIZE;\n\tunsigned int num_entries = length / I915_GTT_PAGE_SIZE;\n\tconst gen8_pte_t scratch_pte = vm->scratch[0]->encode;\n\tgen8_pte_t __iomem *gtt_base =\n\t\t(gen8_pte_t __iomem *)ggtt->gsm + first_entry;\n\tconst int max_entries = ggtt_total_entries(ggtt) - first_entry;\n\tint i;\n\n\tif (WARN(num_entries > max_entries,\n\t\t \"First entry = %d; Num entries = %d (max=%d)\\n\",\n\t\t first_entry, num_entries, max_entries))\n\t\tnum_entries = max_entries;\n\n\tfor (i = 0; i < num_entries; i++)\n\t\tgen8_set_pte(&gtt_base[i], scratch_pte);\n}\n\nstatic void gen6_ggtt_insert_page(struct i915_address_space *vm,\n\t\t\t\t  dma_addr_t addr,\n\t\t\t\t  u64 offset,\n\t\t\t\t  unsigned int pat_index,\n\t\t\t\t  u32 flags)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tgen6_pte_t __iomem *pte =\n\t\t(gen6_pte_t __iomem *)ggtt->gsm + offset / I915_GTT_PAGE_SIZE;\n\n\tiowrite32(vm->pte_encode(addr, pat_index, flags), pte);\n\n\tggtt->invalidate(ggtt);\n}\n\n \nstatic void gen6_ggtt_insert_entries(struct i915_address_space *vm,\n\t\t\t\t     struct i915_vma_resource *vma_res,\n\t\t\t\t     unsigned int pat_index,\n\t\t\t\t     u32 flags)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tgen6_pte_t __iomem *gte;\n\tgen6_pte_t __iomem *end;\n\tstruct sgt_iter iter;\n\tdma_addr_t addr;\n\n\tgte = (gen6_pte_t __iomem *)ggtt->gsm;\n\tgte += (vma_res->start - vma_res->guard) / I915_GTT_PAGE_SIZE;\n\n\tend = gte + vma_res->guard / I915_GTT_PAGE_SIZE;\n\twhile (gte < end)\n\t\tiowrite32(vm->scratch[0]->encode, gte++);\n\tend += (vma_res->node_size + vma_res->guard) / I915_GTT_PAGE_SIZE;\n\tfor_each_sgt_daddr(addr, iter, vma_res->bi.pages)\n\t\tiowrite32(vm->pte_encode(addr, pat_index, flags), gte++);\n\tGEM_BUG_ON(gte > end);\n\n\t \n\twhile (gte < end)\n\t\tiowrite32(vm->scratch[0]->encode, gte++);\n\n\t \n\tggtt->invalidate(ggtt);\n}\n\nstatic void nop_clear_range(struct i915_address_space *vm,\n\t\t\t    u64 start, u64 length)\n{\n}\n\nstatic void bxt_vtd_ggtt_wa(struct i915_address_space *vm)\n{\n\t \n\tintel_uncore_posting_read_fw(vm->gt->uncore, GFX_FLSH_CNTL_GEN6);\n}\n\nstruct insert_page {\n\tstruct i915_address_space *vm;\n\tdma_addr_t addr;\n\tu64 offset;\n\tunsigned int pat_index;\n};\n\nstatic int bxt_vtd_ggtt_insert_page__cb(void *_arg)\n{\n\tstruct insert_page *arg = _arg;\n\n\tgen8_ggtt_insert_page(arg->vm, arg->addr, arg->offset,\n\t\t\t      arg->pat_index, 0);\n\tbxt_vtd_ggtt_wa(arg->vm);\n\n\treturn 0;\n}\n\nstatic void bxt_vtd_ggtt_insert_page__BKL(struct i915_address_space *vm,\n\t\t\t\t\t  dma_addr_t addr,\n\t\t\t\t\t  u64 offset,\n\t\t\t\t\t  unsigned int pat_index,\n\t\t\t\t\t  u32 unused)\n{\n\tstruct insert_page arg = { vm, addr, offset, pat_index };\n\n\tstop_machine(bxt_vtd_ggtt_insert_page__cb, &arg, NULL);\n}\n\nstruct insert_entries {\n\tstruct i915_address_space *vm;\n\tstruct i915_vma_resource *vma_res;\n\tunsigned int pat_index;\n\tu32 flags;\n};\n\nstatic int bxt_vtd_ggtt_insert_entries__cb(void *_arg)\n{\n\tstruct insert_entries *arg = _arg;\n\n\tgen8_ggtt_insert_entries(arg->vm, arg->vma_res,\n\t\t\t\t arg->pat_index, arg->flags);\n\tbxt_vtd_ggtt_wa(arg->vm);\n\n\treturn 0;\n}\n\nstatic void bxt_vtd_ggtt_insert_entries__BKL(struct i915_address_space *vm,\n\t\t\t\t\t     struct i915_vma_resource *vma_res,\n\t\t\t\t\t     unsigned int pat_index,\n\t\t\t\t\t     u32 flags)\n{\n\tstruct insert_entries arg = { vm, vma_res, pat_index, flags };\n\n\tstop_machine(bxt_vtd_ggtt_insert_entries__cb, &arg, NULL);\n}\n\nstatic void gen6_ggtt_clear_range(struct i915_address_space *vm,\n\t\t\t\t  u64 start, u64 length)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tunsigned int first_entry = start / I915_GTT_PAGE_SIZE;\n\tunsigned int num_entries = length / I915_GTT_PAGE_SIZE;\n\tgen6_pte_t scratch_pte, __iomem *gtt_base =\n\t\t(gen6_pte_t __iomem *)ggtt->gsm + first_entry;\n\tconst int max_entries = ggtt_total_entries(ggtt) - first_entry;\n\tint i;\n\n\tif (WARN(num_entries > max_entries,\n\t\t \"First entry = %d; Num entries = %d (max=%d)\\n\",\n\t\t first_entry, num_entries, max_entries))\n\t\tnum_entries = max_entries;\n\n\tscratch_pte = vm->scratch[0]->encode;\n\tfor (i = 0; i < num_entries; i++)\n\t\tiowrite32(scratch_pte, &gtt_base[i]);\n}\n\nvoid intel_ggtt_bind_vma(struct i915_address_space *vm,\n\t\t\t struct i915_vm_pt_stash *stash,\n\t\t\t struct i915_vma_resource *vma_res,\n\t\t\t unsigned int pat_index,\n\t\t\t u32 flags)\n{\n\tu32 pte_flags;\n\n\tif (vma_res->bound_flags & (~flags & I915_VMA_BIND_MASK))\n\t\treturn;\n\n\tvma_res->bound_flags |= flags;\n\n\t \n\tpte_flags = 0;\n\tif (vma_res->bi.readonly)\n\t\tpte_flags |= PTE_READ_ONLY;\n\tif (vma_res->bi.lmem)\n\t\tpte_flags |= PTE_LM;\n\n\tvm->insert_entries(vm, vma_res, pat_index, pte_flags);\n\tvma_res->page_sizes_gtt = I915_GTT_PAGE_SIZE;\n}\n\nvoid intel_ggtt_unbind_vma(struct i915_address_space *vm,\n\t\t\t   struct i915_vma_resource *vma_res)\n{\n\tvm->clear_range(vm, vma_res->start, vma_res->vma_size);\n}\n\n \n#define GUC_TOP_RESERVE_SIZE (SZ_4G - GUC_GGTT_TOP)\n\nstatic int ggtt_reserve_guc_top(struct i915_ggtt *ggtt)\n{\n\tu64 offset;\n\tint ret;\n\n\tif (!intel_uc_uses_guc(&ggtt->vm.gt->uc))\n\t\treturn 0;\n\n\tGEM_BUG_ON(ggtt->vm.total <= GUC_TOP_RESERVE_SIZE);\n\toffset = ggtt->vm.total - GUC_TOP_RESERVE_SIZE;\n\n\tret = i915_gem_gtt_reserve(&ggtt->vm, NULL, &ggtt->uc_fw,\n\t\t\t\t   GUC_TOP_RESERVE_SIZE, offset,\n\t\t\t\t   I915_COLOR_UNEVICTABLE, PIN_NOEVICT);\n\tif (ret)\n\t\tdrm_dbg(&ggtt->vm.i915->drm,\n\t\t\t\"Failed to reserve top of GGTT for GuC\\n\");\n\n\treturn ret;\n}\n\nstatic void ggtt_release_guc_top(struct i915_ggtt *ggtt)\n{\n\tif (drm_mm_node_allocated(&ggtt->uc_fw))\n\t\tdrm_mm_remove_node(&ggtt->uc_fw);\n}\n\nstatic void cleanup_init_ggtt(struct i915_ggtt *ggtt)\n{\n\tggtt_release_guc_top(ggtt);\n\tif (drm_mm_node_allocated(&ggtt->error_capture))\n\t\tdrm_mm_remove_node(&ggtt->error_capture);\n\tmutex_destroy(&ggtt->error_mutex);\n}\n\nstatic int init_ggtt(struct i915_ggtt *ggtt)\n{\n\t \n\tunsigned long hole_start, hole_end;\n\tstruct drm_mm_node *entry;\n\tint ret;\n\n\t \n\tggtt->pin_bias = max_t(u32, I915_GTT_PAGE_SIZE,\n\t\t\t       intel_wopcm_guc_size(&ggtt->vm.gt->wopcm));\n\n\tret = intel_vgt_balloon(ggtt);\n\tif (ret)\n\t\treturn ret;\n\n\tmutex_init(&ggtt->error_mutex);\n\tif (ggtt->mappable_end) {\n\t\t \n\t\tggtt->error_capture.size = 2 * I915_GTT_PAGE_SIZE;\n\t\tggtt->error_capture.color = I915_COLOR_UNEVICTABLE;\n\t\tif (drm_mm_reserve_node(&ggtt->vm.mm, &ggtt->error_capture))\n\t\t\tdrm_mm_insert_node_in_range(&ggtt->vm.mm,\n\t\t\t\t\t\t    &ggtt->error_capture,\n\t\t\t\t\t\t    ggtt->error_capture.size, 0,\n\t\t\t\t\t\t    ggtt->error_capture.color,\n\t\t\t\t\t\t    0, ggtt->mappable_end,\n\t\t\t\t\t\t    DRM_MM_INSERT_LOW);\n\t}\n\tif (drm_mm_node_allocated(&ggtt->error_capture)) {\n\t\tu64 start = ggtt->error_capture.start;\n\t\tu64 size = ggtt->error_capture.size;\n\n\t\tggtt->vm.scratch_range(&ggtt->vm, start, size);\n\t\tdrm_dbg(&ggtt->vm.i915->drm,\n\t\t\t\"Reserved GGTT:[%llx, %llx] for use by error capture\\n\",\n\t\t\tstart, start + size);\n\t}\n\n\t \n\tret = ggtt_reserve_guc_top(ggtt);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tdrm_mm_for_each_hole(entry, &ggtt->vm.mm, hole_start, hole_end) {\n\t\tdrm_dbg(&ggtt->vm.i915->drm,\n\t\t\t\"clearing unused GTT space: [%lx, %lx]\\n\",\n\t\t\thole_start, hole_end);\n\t\tggtt->vm.clear_range(&ggtt->vm, hole_start,\n\t\t\t\t     hole_end - hole_start);\n\t}\n\n\t \n\tggtt->vm.clear_range(&ggtt->vm, ggtt->vm.total - PAGE_SIZE, PAGE_SIZE);\n\n\treturn 0;\n\nerr:\n\tcleanup_init_ggtt(ggtt);\n\treturn ret;\n}\n\nstatic void aliasing_gtt_bind_vma(struct i915_address_space *vm,\n\t\t\t\t  struct i915_vm_pt_stash *stash,\n\t\t\t\t  struct i915_vma_resource *vma_res,\n\t\t\t\t  unsigned int pat_index,\n\t\t\t\t  u32 flags)\n{\n\tu32 pte_flags;\n\n\t \n\tpte_flags = 0;\n\tif (vma_res->bi.readonly)\n\t\tpte_flags |= PTE_READ_ONLY;\n\n\tif (flags & I915_VMA_LOCAL_BIND)\n\t\tppgtt_bind_vma(&i915_vm_to_ggtt(vm)->alias->vm,\n\t\t\t       stash, vma_res, pat_index, flags);\n\n\tif (flags & I915_VMA_GLOBAL_BIND)\n\t\tvm->insert_entries(vm, vma_res, pat_index, pte_flags);\n\n\tvma_res->bound_flags |= flags;\n}\n\nstatic void aliasing_gtt_unbind_vma(struct i915_address_space *vm,\n\t\t\t\t    struct i915_vma_resource *vma_res)\n{\n\tif (vma_res->bound_flags & I915_VMA_GLOBAL_BIND)\n\t\tvm->clear_range(vm, vma_res->start, vma_res->vma_size);\n\n\tif (vma_res->bound_flags & I915_VMA_LOCAL_BIND)\n\t\tppgtt_unbind_vma(&i915_vm_to_ggtt(vm)->alias->vm, vma_res);\n}\n\nstatic int init_aliasing_ppgtt(struct i915_ggtt *ggtt)\n{\n\tstruct i915_vm_pt_stash stash = {};\n\tstruct i915_ppgtt *ppgtt;\n\tint err;\n\n\tppgtt = i915_ppgtt_create(ggtt->vm.gt, 0);\n\tif (IS_ERR(ppgtt))\n\t\treturn PTR_ERR(ppgtt);\n\n\tif (GEM_WARN_ON(ppgtt->vm.total < ggtt->vm.total)) {\n\t\terr = -ENODEV;\n\t\tgoto err_ppgtt;\n\t}\n\n\terr = i915_vm_alloc_pt_stash(&ppgtt->vm, &stash, ggtt->vm.total);\n\tif (err)\n\t\tgoto err_ppgtt;\n\n\ti915_gem_object_lock(ppgtt->vm.scratch[0], NULL);\n\terr = i915_vm_map_pt_stash(&ppgtt->vm, &stash);\n\ti915_gem_object_unlock(ppgtt->vm.scratch[0]);\n\tif (err)\n\t\tgoto err_stash;\n\n\t \n\tppgtt->vm.allocate_va_range(&ppgtt->vm, &stash, 0, ggtt->vm.total);\n\n\tggtt->alias = ppgtt;\n\tggtt->vm.bind_async_flags |= ppgtt->vm.bind_async_flags;\n\n\tGEM_BUG_ON(ggtt->vm.vma_ops.bind_vma != intel_ggtt_bind_vma);\n\tggtt->vm.vma_ops.bind_vma = aliasing_gtt_bind_vma;\n\n\tGEM_BUG_ON(ggtt->vm.vma_ops.unbind_vma != intel_ggtt_unbind_vma);\n\tggtt->vm.vma_ops.unbind_vma = aliasing_gtt_unbind_vma;\n\n\ti915_vm_free_pt_stash(&ppgtt->vm, &stash);\n\treturn 0;\n\nerr_stash:\n\ti915_vm_free_pt_stash(&ppgtt->vm, &stash);\nerr_ppgtt:\n\ti915_vm_put(&ppgtt->vm);\n\treturn err;\n}\n\nstatic void fini_aliasing_ppgtt(struct i915_ggtt *ggtt)\n{\n\tstruct i915_ppgtt *ppgtt;\n\n\tppgtt = fetch_and_zero(&ggtt->alias);\n\tif (!ppgtt)\n\t\treturn;\n\n\ti915_vm_put(&ppgtt->vm);\n\n\tggtt->vm.vma_ops.bind_vma   = intel_ggtt_bind_vma;\n\tggtt->vm.vma_ops.unbind_vma = intel_ggtt_unbind_vma;\n}\n\nint i915_init_ggtt(struct drm_i915_private *i915)\n{\n\tint ret;\n\n\tret = init_ggtt(to_gt(i915)->ggtt);\n\tif (ret)\n\t\treturn ret;\n\n\tif (INTEL_PPGTT(i915) == INTEL_PPGTT_ALIASING) {\n\t\tret = init_aliasing_ppgtt(to_gt(i915)->ggtt);\n\t\tif (ret)\n\t\t\tcleanup_init_ggtt(to_gt(i915)->ggtt);\n\t}\n\n\treturn 0;\n}\n\nstatic void ggtt_cleanup_hw(struct i915_ggtt *ggtt)\n{\n\tstruct i915_vma *vma, *vn;\n\n\tflush_workqueue(ggtt->vm.i915->wq);\n\ti915_gem_drain_freed_objects(ggtt->vm.i915);\n\n\tmutex_lock(&ggtt->vm.mutex);\n\n\tggtt->vm.skip_pte_rewrite = true;\n\n\tlist_for_each_entry_safe(vma, vn, &ggtt->vm.bound_list, vm_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\t\tbool trylock;\n\n\t\ttrylock = i915_gem_object_trylock(obj, NULL);\n\t\tWARN_ON(!trylock);\n\n\t\tWARN_ON(__i915_vma_unbind(vma));\n\t\tif (trylock)\n\t\t\ti915_gem_object_unlock(obj);\n\t}\n\n\tif (drm_mm_node_allocated(&ggtt->error_capture))\n\t\tdrm_mm_remove_node(&ggtt->error_capture);\n\tmutex_destroy(&ggtt->error_mutex);\n\n\tggtt_release_guc_top(ggtt);\n\tintel_vgt_deballoon(ggtt);\n\n\tggtt->vm.cleanup(&ggtt->vm);\n\n\tmutex_unlock(&ggtt->vm.mutex);\n\ti915_address_space_fini(&ggtt->vm);\n\n\tarch_phys_wc_del(ggtt->mtrr);\n\n\tif (ggtt->iomap.size)\n\t\tio_mapping_fini(&ggtt->iomap);\n}\n\n \nvoid i915_ggtt_driver_release(struct drm_i915_private *i915)\n{\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\n\tfini_aliasing_ppgtt(ggtt);\n\n\tintel_ggtt_fini_fences(ggtt);\n\tggtt_cleanup_hw(ggtt);\n}\n\n \nvoid i915_ggtt_driver_late_release(struct drm_i915_private *i915)\n{\n\tstruct i915_ggtt *ggtt = to_gt(i915)->ggtt;\n\n\tGEM_WARN_ON(kref_read(&ggtt->vm.resv_ref) != 1);\n\tdma_resv_fini(&ggtt->vm._resv);\n}\n\nstatic unsigned int gen6_get_total_gtt_size(u16 snb_gmch_ctl)\n{\n\tsnb_gmch_ctl >>= SNB_GMCH_GGMS_SHIFT;\n\tsnb_gmch_ctl &= SNB_GMCH_GGMS_MASK;\n\treturn snb_gmch_ctl << 20;\n}\n\nstatic unsigned int gen8_get_total_gtt_size(u16 bdw_gmch_ctl)\n{\n\tbdw_gmch_ctl >>= BDW_GMCH_GGMS_SHIFT;\n\tbdw_gmch_ctl &= BDW_GMCH_GGMS_MASK;\n\tif (bdw_gmch_ctl)\n\t\tbdw_gmch_ctl = 1 << bdw_gmch_ctl;\n\n#ifdef CONFIG_X86_32\n\t \n\tif (bdw_gmch_ctl > 4)\n\t\tbdw_gmch_ctl = 4;\n#endif\n\n\treturn bdw_gmch_ctl << 20;\n}\n\nstatic unsigned int chv_get_total_gtt_size(u16 gmch_ctrl)\n{\n\tgmch_ctrl >>= SNB_GMCH_GGMS_SHIFT;\n\tgmch_ctrl &= SNB_GMCH_GGMS_MASK;\n\n\tif (gmch_ctrl)\n\t\treturn 1 << (20 + gmch_ctrl);\n\n\treturn 0;\n}\n\nstatic unsigned int gen6_gttmmadr_size(struct drm_i915_private *i915)\n{\n\t \n\tGEM_BUG_ON(GRAPHICS_VER(i915) < 6);\n\treturn (GRAPHICS_VER(i915) < 8) ? SZ_4M : SZ_16M;\n}\n\nstatic unsigned int gen6_gttadr_offset(struct drm_i915_private *i915)\n{\n\treturn gen6_gttmmadr_size(i915) / 2;\n}\n\nstatic int ggtt_probe_common(struct i915_ggtt *ggtt, u64 size)\n{\n\tstruct drm_i915_private *i915 = ggtt->vm.i915;\n\tstruct pci_dev *pdev = to_pci_dev(i915->drm.dev);\n\tphys_addr_t phys_addr;\n\tu32 pte_flags;\n\tint ret;\n\n\tGEM_WARN_ON(pci_resource_len(pdev, GEN4_GTTMMADR_BAR) != gen6_gttmmadr_size(i915));\n\tphys_addr = pci_resource_start(pdev, GEN4_GTTMMADR_BAR) + gen6_gttadr_offset(i915);\n\n\tif (needs_wc_ggtt_mapping(i915))\n\t\tggtt->gsm = ioremap_wc(phys_addr, size);\n\telse\n\t\tggtt->gsm = ioremap(phys_addr, size);\n\n\tif (!ggtt->gsm) {\n\t\tdrm_err(&i915->drm, \"Failed to map the ggtt page table\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tkref_init(&ggtt->vm.resv_ref);\n\tret = setup_scratch_page(&ggtt->vm);\n\tif (ret) {\n\t\tdrm_err(&i915->drm, \"Scratch setup failed\\n\");\n\t\t \n\t\tiounmap(ggtt->gsm);\n\t\treturn ret;\n\t}\n\n\tpte_flags = 0;\n\tif (i915_gem_object_is_lmem(ggtt->vm.scratch[0]))\n\t\tpte_flags |= PTE_LM;\n\n\tggtt->vm.scratch[0]->encode =\n\t\tggtt->vm.pte_encode(px_dma(ggtt->vm.scratch[0]),\n\t\t\t\t    i915_gem_get_pat_index(i915,\n\t\t\t\t\t\t\t   I915_CACHE_NONE),\n\t\t\t\t    pte_flags);\n\n\treturn 0;\n}\n\nstatic void gen6_gmch_remove(struct i915_address_space *vm)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\n\tiounmap(ggtt->gsm);\n\tfree_scratch(vm);\n}\n\nstatic struct resource pci_resource(struct pci_dev *pdev, int bar)\n{\n\treturn DEFINE_RES_MEM(pci_resource_start(pdev, bar),\n\t\t\t      pci_resource_len(pdev, bar));\n}\n\nstatic int gen8_gmch_probe(struct i915_ggtt *ggtt)\n{\n\tstruct drm_i915_private *i915 = ggtt->vm.i915;\n\tstruct pci_dev *pdev = to_pci_dev(i915->drm.dev);\n\tunsigned int size;\n\tu16 snb_gmch_ctl;\n\n\tif (!HAS_LMEM(i915) && !HAS_LMEMBAR_SMEM_STOLEN(i915)) {\n\t\tif (!i915_pci_resource_valid(pdev, GEN4_GMADR_BAR))\n\t\t\treturn -ENXIO;\n\n\t\tggtt->gmadr = pci_resource(pdev, GEN4_GMADR_BAR);\n\t\tggtt->mappable_end = resource_size(&ggtt->gmadr);\n\t}\n\n\tpci_read_config_word(pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);\n\tif (IS_CHERRYVIEW(i915))\n\t\tsize = chv_get_total_gtt_size(snb_gmch_ctl);\n\telse\n\t\tsize = gen8_get_total_gtt_size(snb_gmch_ctl);\n\n\tggtt->vm.alloc_pt_dma = alloc_pt_dma;\n\tggtt->vm.alloc_scratch_dma = alloc_pt_dma;\n\tggtt->vm.lmem_pt_obj_flags = I915_BO_ALLOC_PM_EARLY;\n\n\tggtt->vm.total = (size / sizeof(gen8_pte_t)) * I915_GTT_PAGE_SIZE;\n\tggtt->vm.cleanup = gen6_gmch_remove;\n\tggtt->vm.insert_page = gen8_ggtt_insert_page;\n\tggtt->vm.clear_range = nop_clear_range;\n\tggtt->vm.scratch_range = gen8_ggtt_clear_range;\n\n\tggtt->vm.insert_entries = gen8_ggtt_insert_entries;\n\n\t \n\tif (intel_vm_no_concurrent_access_wa(i915)) {\n\t\tggtt->vm.insert_entries = bxt_vtd_ggtt_insert_entries__BKL;\n\t\tggtt->vm.insert_page    = bxt_vtd_ggtt_insert_page__BKL;\n\n\t\t \n\t\tggtt->vm.raw_insert_page = gen8_ggtt_insert_page;\n\t\tggtt->vm.raw_insert_entries = gen8_ggtt_insert_entries;\n\n\t\tggtt->vm.bind_async_flags =\n\t\t\tI915_VMA_GLOBAL_BIND | I915_VMA_LOCAL_BIND;\n\t}\n\n\tif (intel_uc_wants_guc(&ggtt->vm.gt->uc))\n\t\tggtt->invalidate = guc_ggtt_invalidate;\n\telse\n\t\tggtt->invalidate = gen8_ggtt_invalidate;\n\n\tggtt->vm.vma_ops.bind_vma    = intel_ggtt_bind_vma;\n\tggtt->vm.vma_ops.unbind_vma  = intel_ggtt_unbind_vma;\n\n\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70))\n\t\tggtt->vm.pte_encode = mtl_ggtt_pte_encode;\n\telse\n\t\tggtt->vm.pte_encode = gen8_ggtt_pte_encode;\n\n\treturn ggtt_probe_common(ggtt, size);\n}\n\n \nstatic u64 snb_pte_encode(dma_addr_t addr,\n\t\t\t  unsigned int pat_index,\n\t\t\t  u32 flags)\n{\n\tgen6_pte_t pte = GEN6_PTE_ADDR_ENCODE(addr) | GEN6_PTE_VALID;\n\n\tswitch (pat_index) {\n\tcase I915_CACHE_L3_LLC:\n\tcase I915_CACHE_LLC:\n\t\tpte |= GEN6_PTE_CACHE_LLC;\n\t\tbreak;\n\tcase I915_CACHE_NONE:\n\t\tpte |= GEN6_PTE_UNCACHED;\n\t\tbreak;\n\tdefault:\n\t\tMISSING_CASE(pat_index);\n\t}\n\n\treturn pte;\n}\n\nstatic u64 ivb_pte_encode(dma_addr_t addr,\n\t\t\t  unsigned int pat_index,\n\t\t\t  u32 flags)\n{\n\tgen6_pte_t pte = GEN6_PTE_ADDR_ENCODE(addr) | GEN6_PTE_VALID;\n\n\tswitch (pat_index) {\n\tcase I915_CACHE_L3_LLC:\n\t\tpte |= GEN7_PTE_CACHE_L3_LLC;\n\t\tbreak;\n\tcase I915_CACHE_LLC:\n\t\tpte |= GEN6_PTE_CACHE_LLC;\n\t\tbreak;\n\tcase I915_CACHE_NONE:\n\t\tpte |= GEN6_PTE_UNCACHED;\n\t\tbreak;\n\tdefault:\n\t\tMISSING_CASE(pat_index);\n\t}\n\n\treturn pte;\n}\n\nstatic u64 byt_pte_encode(dma_addr_t addr,\n\t\t\t  unsigned int pat_index,\n\t\t\t  u32 flags)\n{\n\tgen6_pte_t pte = GEN6_PTE_ADDR_ENCODE(addr) | GEN6_PTE_VALID;\n\n\tif (!(flags & PTE_READ_ONLY))\n\t\tpte |= BYT_PTE_WRITEABLE;\n\n\tif (pat_index != I915_CACHE_NONE)\n\t\tpte |= BYT_PTE_SNOOPED_BY_CPU_CACHES;\n\n\treturn pte;\n}\n\nstatic u64 hsw_pte_encode(dma_addr_t addr,\n\t\t\t  unsigned int pat_index,\n\t\t\t  u32 flags)\n{\n\tgen6_pte_t pte = HSW_PTE_ADDR_ENCODE(addr) | GEN6_PTE_VALID;\n\n\tif (pat_index != I915_CACHE_NONE)\n\t\tpte |= HSW_WB_LLC_AGE3;\n\n\treturn pte;\n}\n\nstatic u64 iris_pte_encode(dma_addr_t addr,\n\t\t\t   unsigned int pat_index,\n\t\t\t   u32 flags)\n{\n\tgen6_pte_t pte = HSW_PTE_ADDR_ENCODE(addr) | GEN6_PTE_VALID;\n\n\tswitch (pat_index) {\n\tcase I915_CACHE_NONE:\n\t\tbreak;\n\tcase I915_CACHE_WT:\n\t\tpte |= HSW_WT_ELLC_LLC_AGE3;\n\t\tbreak;\n\tdefault:\n\t\tpte |= HSW_WB_ELLC_LLC_AGE3;\n\t\tbreak;\n\t}\n\n\treturn pte;\n}\n\nstatic int gen6_gmch_probe(struct i915_ggtt *ggtt)\n{\n\tstruct drm_i915_private *i915 = ggtt->vm.i915;\n\tstruct pci_dev *pdev = to_pci_dev(i915->drm.dev);\n\tunsigned int size;\n\tu16 snb_gmch_ctl;\n\n\tif (!i915_pci_resource_valid(pdev, GEN4_GMADR_BAR))\n\t\treturn -ENXIO;\n\n\tggtt->gmadr = pci_resource(pdev, GEN4_GMADR_BAR);\n\tggtt->mappable_end = resource_size(&ggtt->gmadr);\n\n\t \n\tif (ggtt->mappable_end < (64 << 20) ||\n\t    ggtt->mappable_end > (512 << 20)) {\n\t\tdrm_err(&i915->drm, \"Unknown GMADR size (%pa)\\n\",\n\t\t\t&ggtt->mappable_end);\n\t\treturn -ENXIO;\n\t}\n\n\tpci_read_config_word(pdev, SNB_GMCH_CTRL, &snb_gmch_ctl);\n\n\tsize = gen6_get_total_gtt_size(snb_gmch_ctl);\n\tggtt->vm.total = (size / sizeof(gen6_pte_t)) * I915_GTT_PAGE_SIZE;\n\n\tggtt->vm.alloc_pt_dma = alloc_pt_dma;\n\tggtt->vm.alloc_scratch_dma = alloc_pt_dma;\n\n\tggtt->vm.clear_range = nop_clear_range;\n\tif (!HAS_FULL_PPGTT(i915))\n\t\tggtt->vm.clear_range = gen6_ggtt_clear_range;\n\tggtt->vm.scratch_range = gen6_ggtt_clear_range;\n\tggtt->vm.insert_page = gen6_ggtt_insert_page;\n\tggtt->vm.insert_entries = gen6_ggtt_insert_entries;\n\tggtt->vm.cleanup = gen6_gmch_remove;\n\n\tggtt->invalidate = gen6_ggtt_invalidate;\n\n\tif (HAS_EDRAM(i915))\n\t\tggtt->vm.pte_encode = iris_pte_encode;\n\telse if (IS_HASWELL(i915))\n\t\tggtt->vm.pte_encode = hsw_pte_encode;\n\telse if (IS_VALLEYVIEW(i915))\n\t\tggtt->vm.pte_encode = byt_pte_encode;\n\telse if (GRAPHICS_VER(i915) >= 7)\n\t\tggtt->vm.pte_encode = ivb_pte_encode;\n\telse\n\t\tggtt->vm.pte_encode = snb_pte_encode;\n\n\tggtt->vm.vma_ops.bind_vma    = intel_ggtt_bind_vma;\n\tggtt->vm.vma_ops.unbind_vma  = intel_ggtt_unbind_vma;\n\n\treturn ggtt_probe_common(ggtt, size);\n}\n\nstatic int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tint ret;\n\n\tggtt->vm.gt = gt;\n\tggtt->vm.i915 = i915;\n\tggtt->vm.dma = i915->drm.dev;\n\tdma_resv_init(&ggtt->vm._resv);\n\n\tif (GRAPHICS_VER(i915) >= 8)\n\t\tret = gen8_gmch_probe(ggtt);\n\telse if (GRAPHICS_VER(i915) >= 6)\n\t\tret = gen6_gmch_probe(ggtt);\n\telse\n\t\tret = intel_ggtt_gmch_probe(ggtt);\n\n\tif (ret) {\n\t\tdma_resv_fini(&ggtt->vm._resv);\n\t\treturn ret;\n\t}\n\n\tif ((ggtt->vm.total - 1) >> 32) {\n\t\tdrm_err(&i915->drm,\n\t\t\t\"We never expected a Global GTT with more than 32bits\"\n\t\t\t\" of address space! Found %lldM!\\n\",\n\t\t\tggtt->vm.total >> 20);\n\t\tggtt->vm.total = 1ULL << 32;\n\t\tggtt->mappable_end =\n\t\t\tmin_t(u64, ggtt->mappable_end, ggtt->vm.total);\n\t}\n\n\tif (ggtt->mappable_end > ggtt->vm.total) {\n\t\tdrm_err(&i915->drm,\n\t\t\t\"mappable aperture extends past end of GGTT,\"\n\t\t\t\" aperture=%pa, total=%llx\\n\",\n\t\t\t&ggtt->mappable_end, ggtt->vm.total);\n\t\tggtt->mappable_end = ggtt->vm.total;\n\t}\n\n\t \n\tdrm_dbg(&i915->drm, \"GGTT size = %lluM\\n\", ggtt->vm.total >> 20);\n\tdrm_dbg(&i915->drm, \"GMADR size = %lluM\\n\",\n\t\t(u64)ggtt->mappable_end >> 20);\n\tdrm_dbg(&i915->drm, \"DSM size = %lluM\\n\",\n\t\t(u64)resource_size(&intel_graphics_stolen_res) >> 20);\n\n\treturn 0;\n}\n\n \nint i915_ggtt_probe_hw(struct drm_i915_private *i915)\n{\n\tstruct intel_gt *gt;\n\tint ret, i;\n\n\tfor_each_gt(gt, i915, i) {\n\t\tret = intel_gt_assign_ggtt(gt);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = ggtt_probe_hw(to_gt(i915)->ggtt, to_gt(i915));\n\tif (ret)\n\t\treturn ret;\n\n\tif (i915_vtd_active(i915))\n\t\tdrm_info(&i915->drm, \"VT-d active for gfx access\\n\");\n\n\treturn 0;\n}\n\nstruct i915_ggtt *i915_ggtt_create(struct drm_i915_private *i915)\n{\n\tstruct i915_ggtt *ggtt;\n\n\tggtt = drmm_kzalloc(&i915->drm, sizeof(*ggtt), GFP_KERNEL);\n\tif (!ggtt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ggtt->gt_list);\n\n\treturn ggtt;\n}\n\nint i915_ggtt_enable_hw(struct drm_i915_private *i915)\n{\n\tif (GRAPHICS_VER(i915) < 6)\n\t\treturn intel_ggtt_gmch_enable_hw(i915);\n\n\treturn 0;\n}\n\n \nbool i915_ggtt_resume_vm(struct i915_address_space *vm)\n{\n\tstruct i915_vma *vma;\n\tbool write_domain_objs = false;\n\n\tdrm_WARN_ON(&vm->i915->drm, !vm->is_ggtt && !vm->is_dpt);\n\n\t \n\tvm->clear_range(vm, 0, vm->total);\n\n\t \n\tlist_for_each_entry(vma, &vm->bound_list, vm_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\t\tunsigned int was_bound =\n\t\t\tatomic_read(&vma->flags) & I915_VMA_BIND_MASK;\n\n\t\tGEM_BUG_ON(!was_bound);\n\n\t\t \n\t\tvma->resource->bound_flags = 0;\n\t\tvma->ops->bind_vma(vm, NULL, vma->resource,\n\t\t\t\t   obj ? obj->pat_index :\n\t\t\t\t\t i915_gem_get_pat_index(vm->i915,\n\t\t\t\t\t\t\t\tI915_CACHE_NONE),\n\t\t\t\t   was_bound);\n\n\t\tif (obj) {  \n\t\t\twrite_domain_objs |= fetch_and_zero(&obj->write_domain);\n\t\t\tobj->read_domains |= I915_GEM_DOMAIN_GTT;\n\t\t}\n\t}\n\n\treturn write_domain_objs;\n}\n\nvoid i915_ggtt_resume(struct i915_ggtt *ggtt)\n{\n\tstruct intel_gt *gt;\n\tbool flush;\n\n\tlist_for_each_entry(gt, &ggtt->gt_list, ggtt_link)\n\t\tintel_gt_check_and_clear_faults(gt);\n\n\tflush = i915_ggtt_resume_vm(&ggtt->vm);\n\n\tif (drm_mm_node_allocated(&ggtt->error_capture))\n\t\tggtt->vm.scratch_range(&ggtt->vm, ggtt->error_capture.start,\n\t\t\t\t       ggtt->error_capture.size);\n\n\tlist_for_each_entry(gt, &ggtt->gt_list, ggtt_link)\n\t\tintel_uc_resume_mappings(&gt->uc);\n\n\tggtt->invalidate(ggtt);\n\n\tif (flush)\n\t\twbinvd_on_all_cpus();\n\n\tintel_ggtt_restore_fences(ggtt);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}