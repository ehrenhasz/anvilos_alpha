{
  "module_name": "gen6_ppgtt.c",
  "hash_id": "a722be6fafd74322746cce105e9731357083a1d8505704e98476c554f8b46742",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/gen6_ppgtt.c",
  "human_readable_source": "\n \n\n#include <linux/log2.h>\n\n#include \"gem/i915_gem_internal.h\"\n\n#include \"gen6_ppgtt.h\"\n#include \"i915_scatterlist.h\"\n#include \"i915_trace.h\"\n#include \"i915_vgpu.h\"\n#include \"intel_gt_regs.h\"\n#include \"intel_engine_regs.h\"\n#include \"intel_gt.h\"\n\n \nstatic void gen6_write_pde(const struct gen6_ppgtt *ppgtt,\n\t\t\t   const unsigned int pde,\n\t\t\t   const struct i915_page_table *pt)\n{\n\tdma_addr_t addr = pt ? px_dma(pt) : px_dma(ppgtt->base.vm.scratch[1]);\n\n\t \n\tiowrite32(GEN6_PDE_ADDR_ENCODE(addr) | GEN6_PDE_VALID,\n\t\t  ppgtt->pd_addr + pde);\n}\n\nvoid gen7_ppgtt_enable(struct intel_gt *gt)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_uncore *uncore = gt->uncore;\n\tu32 ecochk;\n\n\tintel_uncore_rmw(uncore, GAC_ECO_BITS, 0, ECOBITS_PPGTT_CACHE64B);\n\n\tecochk = intel_uncore_read(uncore, GAM_ECOCHK);\n\tif (IS_HASWELL(i915)) {\n\t\tecochk |= ECOCHK_PPGTT_WB_HSW;\n\t} else {\n\t\tecochk |= ECOCHK_PPGTT_LLC_IVB;\n\t\tecochk &= ~ECOCHK_PPGTT_GFDT_IVB;\n\t}\n\tintel_uncore_write(uncore, GAM_ECOCHK, ecochk);\n}\n\nvoid gen6_ppgtt_enable(struct intel_gt *gt)\n{\n\tstruct intel_uncore *uncore = gt->uncore;\n\n\tintel_uncore_rmw(uncore,\n\t\t\t GAC_ECO_BITS,\n\t\t\t 0,\n\t\t\t ECOBITS_SNB_BIT | ECOBITS_PPGTT_CACHE64B);\n\n\tintel_uncore_rmw(uncore,\n\t\t\t GAB_CTL,\n\t\t\t 0,\n\t\t\t GAB_CTL_CONT_AFTER_PAGEFAULT);\n\n\tintel_uncore_rmw(uncore,\n\t\t\t GAM_ECOCHK,\n\t\t\t 0,\n\t\t\t ECOCHK_SNB_BIT | ECOCHK_PPGTT_CACHE64B);\n\n\tif (HAS_PPGTT(uncore->i915))  \n\t\tintel_uncore_write(uncore,\n\t\t\t\t   GFX_MODE,\n\t\t\t\t   _MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));\n}\n\n \nstatic void gen6_ppgtt_clear_range(struct i915_address_space *vm,\n\t\t\t\t   u64 start, u64 length)\n{\n\tstruct gen6_ppgtt * const ppgtt = to_gen6_ppgtt(i915_vm_to_ppgtt(vm));\n\tconst unsigned int first_entry = start / I915_GTT_PAGE_SIZE;\n\tconst gen6_pte_t scratch_pte = vm->scratch[0]->encode;\n\tunsigned int pde = first_entry / GEN6_PTES;\n\tunsigned int pte = first_entry % GEN6_PTES;\n\tunsigned int num_entries = length / I915_GTT_PAGE_SIZE;\n\n\twhile (num_entries) {\n\t\tstruct i915_page_table * const pt =\n\t\t\ti915_pt_entry(ppgtt->base.pd, pde++);\n\t\tconst unsigned int count = min(num_entries, GEN6_PTES - pte);\n\t\tgen6_pte_t *vaddr;\n\n\t\tnum_entries -= count;\n\n\t\tGEM_BUG_ON(count > atomic_read(&pt->used));\n\t\tif (!atomic_sub_return(count, &pt->used))\n\t\t\tppgtt->scan_for_unused_pt = true;\n\n\t\t \n\n\t\tvaddr = px_vaddr(pt);\n\t\tmemset32(vaddr + pte, scratch_pte, count);\n\n\t\tpte = 0;\n\t}\n}\n\nstatic void gen6_ppgtt_insert_entries(struct i915_address_space *vm,\n\t\t\t\t      struct i915_vma_resource *vma_res,\n\t\t\t\t      unsigned int pat_index,\n\t\t\t\t      u32 flags)\n{\n\tstruct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);\n\tstruct i915_page_directory * const pd = ppgtt->pd;\n\tunsigned int first_entry = vma_res->start / I915_GTT_PAGE_SIZE;\n\tunsigned int act_pt = first_entry / GEN6_PTES;\n\tunsigned int act_pte = first_entry % GEN6_PTES;\n\tconst u32 pte_encode = vm->pte_encode(0, pat_index, flags);\n\tstruct sgt_dma iter = sgt_dma(vma_res);\n\tgen6_pte_t *vaddr;\n\n\tGEM_BUG_ON(!pd->entry[act_pt]);\n\n\tvaddr = px_vaddr(i915_pt_entry(pd, act_pt));\n\tdo {\n\t\tGEM_BUG_ON(sg_dma_len(iter.sg) < I915_GTT_PAGE_SIZE);\n\t\tvaddr[act_pte] = pte_encode | GEN6_PTE_ADDR_ENCODE(iter.dma);\n\n\t\titer.dma += I915_GTT_PAGE_SIZE;\n\t\tif (iter.dma == iter.max) {\n\t\t\titer.sg = __sg_next(iter.sg);\n\t\t\tif (!iter.sg || sg_dma_len(iter.sg) == 0)\n\t\t\t\tbreak;\n\n\t\t\titer.dma = sg_dma_address(iter.sg);\n\t\t\titer.max = iter.dma + sg_dma_len(iter.sg);\n\t\t}\n\n\t\tif (++act_pte == GEN6_PTES) {\n\t\t\tvaddr = px_vaddr(i915_pt_entry(pd, ++act_pt));\n\t\t\tact_pte = 0;\n\t\t}\n\t} while (1);\n\n\tvma_res->page_sizes_gtt = I915_GTT_PAGE_SIZE;\n}\n\nstatic void gen6_flush_pd(struct gen6_ppgtt *ppgtt, u64 start, u64 end)\n{\n\tstruct i915_page_directory * const pd = ppgtt->base.pd;\n\tstruct i915_page_table *pt;\n\tunsigned int pde;\n\n\tstart = round_down(start, SZ_64K);\n\tend = round_up(end, SZ_64K) - start;\n\n\tmutex_lock(&ppgtt->flush);\n\n\tgen6_for_each_pde(pt, pd, start, end, pde)\n\t\tgen6_write_pde(ppgtt, pde, pt);\n\n\tmb();\n\tioread32(ppgtt->pd_addr + pde - 1);\n\tgen6_ggtt_invalidate(ppgtt->base.vm.gt->ggtt);\n\tmb();\n\n\tmutex_unlock(&ppgtt->flush);\n}\n\nstatic void gen6_alloc_va_range(struct i915_address_space *vm,\n\t\t\t\tstruct i915_vm_pt_stash *stash,\n\t\t\t\tu64 start, u64 length)\n{\n\tstruct gen6_ppgtt *ppgtt = to_gen6_ppgtt(i915_vm_to_ppgtt(vm));\n\tstruct i915_page_directory * const pd = ppgtt->base.pd;\n\tstruct i915_page_table *pt;\n\tbool flush = false;\n\tu64 from = start;\n\tunsigned int pde;\n\n\tspin_lock(&pd->lock);\n\tgen6_for_each_pde(pt, pd, start, length, pde) {\n\t\tconst unsigned int count = gen6_pte_count(start, length);\n\n\t\tif (!pt) {\n\t\t\tspin_unlock(&pd->lock);\n\n\t\t\tpt = stash->pt[0];\n\t\t\t__i915_gem_object_pin_pages(pt->base);\n\n\t\t\tfill32_px(pt, vm->scratch[0]->encode);\n\n\t\t\tspin_lock(&pd->lock);\n\t\t\tif (!pd->entry[pde]) {\n\t\t\t\tstash->pt[0] = pt->stash;\n\t\t\t\tatomic_set(&pt->used, 0);\n\t\t\t\tpd->entry[pde] = pt;\n\t\t\t} else {\n\t\t\t\tpt = pd->entry[pde];\n\t\t\t}\n\n\t\t\tflush = true;\n\t\t}\n\n\t\tatomic_add(count, &pt->used);\n\t}\n\tspin_unlock(&pd->lock);\n\n\tif (flush && i915_vma_is_bound(ppgtt->vma, I915_VMA_GLOBAL_BIND)) {\n\t\tintel_wakeref_t wakeref;\n\n\t\twith_intel_runtime_pm(&vm->i915->runtime_pm, wakeref)\n\t\t\tgen6_flush_pd(ppgtt, from, start);\n\t}\n}\n\nstatic int gen6_ppgtt_init_scratch(struct gen6_ppgtt *ppgtt)\n{\n\tstruct i915_address_space * const vm = &ppgtt->base.vm;\n\tint ret;\n\n\tret = setup_scratch_page(vm);\n\tif (ret)\n\t\treturn ret;\n\n\tvm->scratch[0]->encode =\n\t\tvm->pte_encode(px_dma(vm->scratch[0]),\n\t\t\t       i915_gem_get_pat_index(vm->i915,\n\t\t\t\t\t\t      I915_CACHE_NONE),\n\t\t\t       PTE_READ_ONLY);\n\n\tvm->scratch[1] = vm->alloc_pt_dma(vm, I915_GTT_PAGE_SIZE_4K);\n\tif (IS_ERR(vm->scratch[1])) {\n\t\tret = PTR_ERR(vm->scratch[1]);\n\t\tgoto err_scratch0;\n\t}\n\n\tret = map_pt_dma(vm, vm->scratch[1]);\n\tif (ret)\n\t\tgoto err_scratch1;\n\n\tfill32_px(vm->scratch[1], vm->scratch[0]->encode);\n\n\treturn 0;\n\nerr_scratch1:\n\ti915_gem_object_put(vm->scratch[1]);\nerr_scratch0:\n\ti915_gem_object_put(vm->scratch[0]);\n\tvm->scratch[0] = NULL;\n\treturn ret;\n}\n\nstatic void gen6_ppgtt_free_pd(struct gen6_ppgtt *ppgtt)\n{\n\tstruct i915_page_directory * const pd = ppgtt->base.pd;\n\tstruct i915_page_table *pt;\n\tu32 pde;\n\n\tgen6_for_all_pdes(pt, pd, pde)\n\t\tif (pt)\n\t\t\tfree_pt(&ppgtt->base.vm, pt);\n}\n\nstatic void gen6_ppgtt_cleanup(struct i915_address_space *vm)\n{\n\tstruct gen6_ppgtt *ppgtt = to_gen6_ppgtt(i915_vm_to_ppgtt(vm));\n\n\tgen6_ppgtt_free_pd(ppgtt);\n\tfree_scratch(vm);\n\n\tif (ppgtt->base.pd)\n\t\tfree_pd(&ppgtt->base.vm, ppgtt->base.pd);\n\n\tmutex_destroy(&ppgtt->flush);\n}\n\nstatic void pd_vma_bind(struct i915_address_space *vm,\n\t\t\tstruct i915_vm_pt_stash *stash,\n\t\t\tstruct i915_vma_resource *vma_res,\n\t\t\tunsigned int pat_index,\n\t\t\tu32 unused)\n{\n\tstruct i915_ggtt *ggtt = i915_vm_to_ggtt(vm);\n\tstruct gen6_ppgtt *ppgtt = vma_res->private;\n\tu32 ggtt_offset = vma_res->start / I915_GTT_PAGE_SIZE;\n\n\tppgtt->pp_dir = ggtt_offset * sizeof(gen6_pte_t) << 10;\n\tppgtt->pd_addr = (gen6_pte_t __iomem *)ggtt->gsm + ggtt_offset;\n\n\tgen6_flush_pd(ppgtt, 0, ppgtt->base.vm.total);\n}\n\nstatic void pd_vma_unbind(struct i915_address_space *vm,\n\t\t\t  struct i915_vma_resource *vma_res)\n{\n\tstruct gen6_ppgtt *ppgtt = vma_res->private;\n\tstruct i915_page_directory * const pd = ppgtt->base.pd;\n\tstruct i915_page_table *pt;\n\tunsigned int pde;\n\n\tif (!ppgtt->scan_for_unused_pt)\n\t\treturn;\n\n\t \n\tgen6_for_all_pdes(pt, ppgtt->base.pd, pde) {\n\t\tif (!pt || atomic_read(&pt->used))\n\t\t\tcontinue;\n\n\t\tfree_pt(&ppgtt->base.vm, pt);\n\t\tpd->entry[pde] = NULL;\n\t}\n\n\tppgtt->scan_for_unused_pt = false;\n}\n\nstatic const struct i915_vma_ops pd_vma_ops = {\n\t.bind_vma = pd_vma_bind,\n\t.unbind_vma = pd_vma_unbind,\n};\n\nint gen6_ppgtt_pin(struct i915_ppgtt *base, struct i915_gem_ww_ctx *ww)\n{\n\tstruct gen6_ppgtt *ppgtt = to_gen6_ppgtt(base);\n\tint err;\n\n\tGEM_BUG_ON(!kref_read(&ppgtt->base.vm.ref));\n\n\t \n\tif (atomic_add_unless(&ppgtt->pin_count, 1, 0))\n\t\treturn 0;\n\n\t \n\terr = i915_vm_lock_objects(&ppgtt->base.vm, ww);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (!atomic_read(&ppgtt->pin_count)) {\n\t\terr = i915_ggtt_pin(ppgtt->vma, ww, GEN6_PD_ALIGN, PIN_HIGH);\n\n\t\tGEM_BUG_ON(ppgtt->vma->fence);\n\t\tclear_bit(I915_VMA_CAN_FENCE_BIT, __i915_vma_flags(ppgtt->vma));\n\t}\n\tif (!err)\n\t\tatomic_inc(&ppgtt->pin_count);\n\n\treturn err;\n}\n\nstatic int pd_dummy_obj_get_pages(struct drm_i915_gem_object *obj)\n{\n\tobj->mm.pages = ZERO_SIZE_PTR;\n\treturn 0;\n}\n\nstatic void pd_dummy_obj_put_pages(struct drm_i915_gem_object *obj,\n\t\t\t\t   struct sg_table *pages)\n{\n}\n\nstatic const struct drm_i915_gem_object_ops pd_dummy_obj_ops = {\n\t.name = \"pd_dummy_obj\",\n\t.get_pages = pd_dummy_obj_get_pages,\n\t.put_pages = pd_dummy_obj_put_pages,\n};\n\nstatic struct i915_page_directory *\ngen6_alloc_top_pd(struct gen6_ppgtt *ppgtt)\n{\n\tstruct i915_ggtt * const ggtt = ppgtt->base.vm.gt->ggtt;\n\tstruct i915_page_directory *pd;\n\tint err;\n\n\tpd = __alloc_pd(I915_PDES);\n\tif (unlikely(!pd))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpd->pt.base = __i915_gem_object_create_internal(ppgtt->base.vm.gt->i915,\n\t\t\t\t\t\t\t&pd_dummy_obj_ops,\n\t\t\t\t\t\t\tI915_PDES * SZ_4K);\n\tif (IS_ERR(pd->pt.base)) {\n\t\terr = PTR_ERR(pd->pt.base);\n\t\tpd->pt.base = NULL;\n\t\tgoto err_pd;\n\t}\n\n\tpd->pt.base->base.resv = i915_vm_resv_get(&ppgtt->base.vm);\n\tpd->pt.base->shares_resv_from = &ppgtt->base.vm;\n\n\tppgtt->vma = i915_vma_instance(pd->pt.base, &ggtt->vm, NULL);\n\tif (IS_ERR(ppgtt->vma)) {\n\t\terr = PTR_ERR(ppgtt->vma);\n\t\tppgtt->vma = NULL;\n\t\tgoto err_pd;\n\t}\n\n\t \n\tppgtt->vma->ops = &pd_vma_ops;\n\tppgtt->vma->private = ppgtt;\n\treturn pd;\n\nerr_pd:\n\tfree_pd(&ppgtt->base.vm, pd);\n\treturn ERR_PTR(err);\n}\n\nvoid gen6_ppgtt_unpin(struct i915_ppgtt *base)\n{\n\tstruct gen6_ppgtt *ppgtt = to_gen6_ppgtt(base);\n\n\tGEM_BUG_ON(!atomic_read(&ppgtt->pin_count));\n\tif (atomic_dec_and_test(&ppgtt->pin_count))\n\t\ti915_vma_unpin(ppgtt->vma);\n}\n\nstruct i915_ppgtt *gen6_ppgtt_create(struct intel_gt *gt)\n{\n\tstruct i915_ggtt * const ggtt = gt->ggtt;\n\tstruct gen6_ppgtt *ppgtt;\n\tint err;\n\n\tppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);\n\tif (!ppgtt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&ppgtt->flush);\n\n\tppgtt_init(&ppgtt->base, gt, 0);\n\tppgtt->base.vm.pd_shift = ilog2(SZ_4K * SZ_4K / sizeof(gen6_pte_t));\n\tppgtt->base.vm.top = 1;\n\n\tppgtt->base.vm.bind_async_flags = I915_VMA_LOCAL_BIND;\n\tppgtt->base.vm.allocate_va_range = gen6_alloc_va_range;\n\tppgtt->base.vm.clear_range = gen6_ppgtt_clear_range;\n\tppgtt->base.vm.insert_entries = gen6_ppgtt_insert_entries;\n\tppgtt->base.vm.cleanup = gen6_ppgtt_cleanup;\n\n\tppgtt->base.vm.alloc_pt_dma = alloc_pt_dma;\n\tppgtt->base.vm.alloc_scratch_dma = alloc_pt_dma;\n\tppgtt->base.vm.pte_encode = ggtt->vm.pte_encode;\n\n\terr = gen6_ppgtt_init_scratch(ppgtt);\n\tif (err)\n\t\tgoto err_put;\n\n\tppgtt->base.pd = gen6_alloc_top_pd(ppgtt);\n\tif (IS_ERR(ppgtt->base.pd)) {\n\t\terr = PTR_ERR(ppgtt->base.pd);\n\t\tgoto err_put;\n\t}\n\n\treturn &ppgtt->base;\n\nerr_put:\n\ti915_vm_put(&ppgtt->base.vm);\n\treturn ERR_PTR(err);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}