{
  "module_name": "selftest_workarounds.c",
  "hash_id": "0f197b063616d699e834893ddf159aa2dea38a642144e0528160e40aaaa83414",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/selftest_workarounds.c",
  "human_readable_source": "\n \n\n#include \"gem/i915_gem_internal.h\"\n#include \"gem/i915_gem_pm.h\"\n#include \"gt/intel_engine_user.h\"\n#include \"gt/intel_gt.h\"\n#include \"i915_selftest.h\"\n#include \"intel_reset.h\"\n\n#include \"selftests/igt_flush_test.h\"\n#include \"selftests/igt_reset.h\"\n#include \"selftests/igt_spinner.h\"\n#include \"selftests/intel_scheduler_helpers.h\"\n#include \"selftests/mock_drm.h\"\n\n#include \"gem/selftests/igt_gem_utils.h\"\n#include \"gem/selftests/mock_context.h\"\n\nstatic const struct wo_register {\n\tenum intel_platform platform;\n\tu32 reg;\n} wo_registers[] = {\n\t{ INTEL_GEMINILAKE, 0x731c }\n};\n\nstruct wa_lists {\n\tstruct i915_wa_list gt_wa_list;\n\tstruct {\n\t\tstruct i915_wa_list wa_list;\n\t\tstruct i915_wa_list ctx_wa_list;\n\t} engine[I915_NUM_ENGINES];\n};\n\nstatic int request_add_sync(struct i915_request *rq, int err)\n{\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\tif (i915_request_wait(rq, 0, HZ / 5) < 0)\n\t\terr = -EIO;\n\ti915_request_put(rq);\n\n\treturn err;\n}\n\nstatic int request_add_spin(struct i915_request *rq, struct igt_spinner *spin)\n{\n\tint err = 0;\n\n\ti915_request_get(rq);\n\ti915_request_add(rq);\n\tif (spin && !igt_wait_for_spinner(spin, rq))\n\t\terr = -ETIMEDOUT;\n\ti915_request_put(rq);\n\n\treturn err;\n}\n\nstatic void\nreference_lists_init(struct intel_gt *gt, struct wa_lists *lists)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tmemset(lists, 0, sizeof(*lists));\n\n\twa_init_start(&lists->gt_wa_list, gt, \"GT_REF\", \"global\");\n\tgt_init_workarounds(gt, &lists->gt_wa_list);\n\twa_init_finish(&lists->gt_wa_list);\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct i915_wa_list *wal = &lists->engine[id].wa_list;\n\n\t\twa_init_start(wal, gt, \"REF\", engine->name);\n\t\tengine_init_workarounds(engine, wal);\n\t\twa_init_finish(wal);\n\n\t\t__intel_engine_init_ctx_wa(engine,\n\t\t\t\t\t   &lists->engine[id].ctx_wa_list,\n\t\t\t\t\t   \"CTX_REF\");\n\t}\n}\n\nstatic void\nreference_lists_fini(struct intel_gt *gt, struct wa_lists *lists)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tfor_each_engine(engine, gt, id)\n\t\tintel_wa_list_free(&lists->engine[id].wa_list);\n\n\tintel_wa_list_free(&lists->gt_wa_list);\n}\n\nstatic struct drm_i915_gem_object *\nread_nonprivs(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tconst u32 base = engine->mmio_base;\n\tstruct drm_i915_gem_object *result;\n\tstruct i915_request *rq;\n\tstruct i915_vma *vma;\n\tu32 srm, *cs;\n\tint err;\n\tint i;\n\n\tresult = i915_gem_object_create_internal(engine->i915, PAGE_SIZE);\n\tif (IS_ERR(result))\n\t\treturn result;\n\n\ti915_gem_object_set_cache_coherency(result, I915_CACHE_LLC);\n\n\tcs = i915_gem_object_pin_map_unlocked(result, I915_MAP_WB);\n\tif (IS_ERR(cs)) {\n\t\terr = PTR_ERR(cs);\n\t\tgoto err_obj;\n\t}\n\tmemset(cs, 0xc5, PAGE_SIZE);\n\ti915_gem_object_flush_map(result);\n\ti915_gem_object_unpin_map(result);\n\n\tvma = i915_vma_instance(result, &engine->gt->ggtt->vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\terr = PTR_ERR(vma);\n\t\tgoto err_obj;\n\t}\n\n\terr = i915_vma_pin(vma, 0, 0, PIN_GLOBAL);\n\tif (err)\n\t\tgoto err_obj;\n\n\trq = intel_context_create_request(ce);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto err_pin;\n\t}\n\n\terr = igt_vma_move_to_active_unlocked(vma, rq, EXEC_OBJECT_WRITE);\n\tif (err)\n\t\tgoto err_req;\n\n\tsrm = MI_STORE_REGISTER_MEM | MI_SRM_LRM_GLOBAL_GTT;\n\tif (GRAPHICS_VER(engine->i915) >= 8)\n\t\tsrm++;\n\n\tcs = intel_ring_begin(rq, 4 * RING_MAX_NONPRIV_SLOTS);\n\tif (IS_ERR(cs)) {\n\t\terr = PTR_ERR(cs);\n\t\tgoto err_req;\n\t}\n\n\tfor (i = 0; i < RING_MAX_NONPRIV_SLOTS; i++) {\n\t\t*cs++ = srm;\n\t\t*cs++ = i915_mmio_reg_offset(RING_FORCE_TO_NONPRIV(base, i));\n\t\t*cs++ = i915_ggtt_offset(vma) + sizeof(u32) * i;\n\t\t*cs++ = 0;\n\t}\n\tintel_ring_advance(rq, cs);\n\n\ti915_request_add(rq);\n\ti915_vma_unpin(vma);\n\n\treturn result;\n\nerr_req:\n\ti915_request_add(rq);\nerr_pin:\n\ti915_vma_unpin(vma);\nerr_obj:\n\ti915_gem_object_put(result);\n\treturn ERR_PTR(err);\n}\n\nstatic u32\nget_whitelist_reg(const struct intel_engine_cs *engine, unsigned int i)\n{\n\ti915_reg_t reg = i < engine->whitelist.count ?\n\t\t\t engine->whitelist.list[i].reg :\n\t\t\t RING_NOPID(engine->mmio_base);\n\n\treturn i915_mmio_reg_offset(reg);\n}\n\nstatic void\nprint_results(const struct intel_engine_cs *engine, const u32 *results)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < RING_MAX_NONPRIV_SLOTS; i++) {\n\t\tu32 expected = get_whitelist_reg(engine, i);\n\t\tu32 actual = results[i];\n\n\t\tpr_info(\"RING_NONPRIV[%d]: expected 0x%08x, found 0x%08x\\n\",\n\t\t\ti, expected, actual);\n\t}\n}\n\nstatic int check_whitelist(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct drm_i915_gem_object *results;\n\tstruct intel_wedge_me wedge;\n\tu32 *vaddr;\n\tint err;\n\tint i;\n\n\tresults = read_nonprivs(ce);\n\tif (IS_ERR(results))\n\t\treturn PTR_ERR(results);\n\n\terr = 0;\n\ti915_gem_object_lock(results, NULL);\n\tintel_wedge_on_timeout(&wedge, engine->gt, HZ / 5)  \n\t\terr = i915_gem_object_set_to_cpu_domain(results, false);\n\n\tif (intel_gt_is_wedged(engine->gt))\n\t\terr = -EIO;\n\tif (err)\n\t\tgoto out_put;\n\n\tvaddr = i915_gem_object_pin_map(results, I915_MAP_WB);\n\tif (IS_ERR(vaddr)) {\n\t\terr = PTR_ERR(vaddr);\n\t\tgoto out_put;\n\t}\n\n\tfor (i = 0; i < RING_MAX_NONPRIV_SLOTS; i++) {\n\t\tu32 expected = get_whitelist_reg(engine, i);\n\t\tu32 actual = vaddr[i];\n\n\t\tif (expected != actual) {\n\t\t\tprint_results(engine, vaddr);\n\t\t\tpr_err(\"Invalid RING_NONPRIV[%d], expected 0x%08x, found 0x%08x\\n\",\n\t\t\t       i, expected, actual);\n\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ti915_gem_object_unpin_map(results);\nout_put:\n\ti915_gem_object_unlock(results);\n\ti915_gem_object_put(results);\n\treturn err;\n}\n\nstatic int do_device_reset(struct intel_engine_cs *engine)\n{\n\tintel_gt_reset(engine->gt, engine->mask, \"live_workarounds\");\n\treturn 0;\n}\n\nstatic int do_engine_reset(struct intel_engine_cs *engine)\n{\n\treturn intel_engine_reset(engine, \"live_workarounds\");\n}\n\nstatic int do_guc_reset(struct intel_engine_cs *engine)\n{\n\t \n\treturn 0;\n}\n\nstatic int\nswitch_to_scratch_context(struct intel_engine_cs *engine,\n\t\t\t  struct igt_spinner *spin,\n\t\t\t  struct i915_request **rq)\n{\n\tstruct intel_context *ce;\n\tint err = 0;\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce))\n\t\treturn PTR_ERR(ce);\n\n\t*rq = igt_spinner_create_request(spin, ce, MI_NOOP);\n\tintel_context_put(ce);\n\n\tif (IS_ERR(*rq)) {\n\t\tspin = NULL;\n\t\terr = PTR_ERR(*rq);\n\t\tgoto err;\n\t}\n\n\terr = request_add_spin(*rq, spin);\nerr:\n\tif (err && spin)\n\t\tigt_spinner_end(spin);\n\n\treturn err;\n}\n\nstatic int check_whitelist_across_reset(struct intel_engine_cs *engine,\n\t\t\t\t\tint (*reset)(struct intel_engine_cs *),\n\t\t\t\t\tconst char *name)\n{\n\tstruct intel_context *ce, *tmp;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tintel_wakeref_t wakeref;\n\tint err;\n\n\tpr_info(\"Checking %d whitelisted registers on %s (RING_NONPRIV) [%s]\\n\",\n\t\tengine->whitelist.count, engine->name, name);\n\n\tce = intel_context_create(engine);\n\tif (IS_ERR(ce))\n\t\treturn PTR_ERR(ce);\n\n\terr = igt_spinner_init(&spin, engine->gt);\n\tif (err)\n\t\tgoto out_ctx;\n\n\terr = check_whitelist(ce);\n\tif (err) {\n\t\tpr_err(\"Invalid whitelist *before* %s reset!\\n\", name);\n\t\tgoto out_spin;\n\t}\n\n\terr = switch_to_scratch_context(engine, &spin, &rq);\n\tif (err)\n\t\tgoto out_spin;\n\n\t \n\tif (i915_request_completed(rq)) {\n\t\tpr_err(\"%s spinner failed to start\\n\", name);\n\t\terr = -ETIMEDOUT;\n\t\tgoto out_spin;\n\t}\n\n\twith_intel_runtime_pm(engine->uncore->rpm, wakeref)\n\t\terr = reset(engine);\n\n\t \n\tif (err == 0)\n\t\terr = intel_selftest_wait_for_rq(rq);\n\n\tigt_spinner_end(&spin);\n\n\tif (err) {\n\t\tpr_err(\"%s reset failed\\n\", name);\n\t\tgoto out_spin;\n\t}\n\n\terr = check_whitelist(ce);\n\tif (err) {\n\t\tpr_err(\"Whitelist not preserved in context across %s reset!\\n\",\n\t\t       name);\n\t\tgoto out_spin;\n\t}\n\n\ttmp = intel_context_create(engine);\n\tif (IS_ERR(tmp)) {\n\t\terr = PTR_ERR(tmp);\n\t\tgoto out_spin;\n\t}\n\tintel_context_put(ce);\n\tce = tmp;\n\n\terr = check_whitelist(ce);\n\tif (err) {\n\t\tpr_err(\"Invalid whitelist *after* %s reset in fresh context!\\n\",\n\t\t       name);\n\t\tgoto out_spin;\n\t}\n\nout_spin:\n\tigt_spinner_fini(&spin);\nout_ctx:\n\tintel_context_put(ce);\n\treturn err;\n}\n\nstatic struct i915_vma *create_batch(struct i915_address_space *vm)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tint err;\n\n\tobj = i915_gem_object_create_internal(vm->i915, 16 * PAGE_SIZE);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tvma = i915_vma_instance(obj, vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\terr = PTR_ERR(vma);\n\t\tgoto err_obj;\n\t}\n\n\terr = i915_vma_pin(vma, 0, 0, PIN_USER);\n\tif (err)\n\t\tgoto err_obj;\n\n\treturn vma;\n\nerr_obj:\n\ti915_gem_object_put(obj);\n\treturn ERR_PTR(err);\n}\n\nstatic u32 reg_write(u32 old, u32 new, u32 rsvd)\n{\n\tif (rsvd == 0x0000ffff) {\n\t\told &= ~(new >> 16);\n\t\told |= new & (new >> 16);\n\t} else {\n\t\told &= ~rsvd;\n\t\told |= new & rsvd;\n\t}\n\n\treturn old;\n}\n\nstatic bool wo_register(struct intel_engine_cs *engine, u32 reg)\n{\n\tenum intel_platform platform = INTEL_INFO(engine->i915)->platform;\n\tint i;\n\n\tif ((reg & RING_FORCE_TO_NONPRIV_ACCESS_MASK) ==\n\t     RING_FORCE_TO_NONPRIV_ACCESS_WR)\n\t\treturn true;\n\n\tfor (i = 0; i < ARRAY_SIZE(wo_registers); i++) {\n\t\tif (wo_registers[i].platform == platform &&\n\t\t    wo_registers[i].reg == reg)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool timestamp(const struct intel_engine_cs *engine, u32 reg)\n{\n\treg = (reg - engine->mmio_base) & ~RING_FORCE_TO_NONPRIV_ACCESS_MASK;\n\tswitch (reg) {\n\tcase 0x358:\n\tcase 0x35c:\n\tcase 0x3a8:\n\t\treturn true;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool ro_register(u32 reg)\n{\n\tif ((reg & RING_FORCE_TO_NONPRIV_ACCESS_MASK) ==\n\t     RING_FORCE_TO_NONPRIV_ACCESS_RD)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int whitelist_writable_count(struct intel_engine_cs *engine)\n{\n\tint count = engine->whitelist.count;\n\tint i;\n\n\tfor (i = 0; i < engine->whitelist.count; i++) {\n\t\tu32 reg = i915_mmio_reg_offset(engine->whitelist.list[i].reg);\n\n\t\tif (ro_register(reg))\n\t\t\tcount--;\n\t}\n\n\treturn count;\n}\n\nstatic int check_dirty_whitelist(struct intel_context *ce)\n{\n\tconst u32 values[] = {\n\t\t0x00000000,\n\t\t0x01010101,\n\t\t0x10100101,\n\t\t0x03030303,\n\t\t0x30300303,\n\t\t0x05050505,\n\t\t0x50500505,\n\t\t0x0f0f0f0f,\n\t\t0xf00ff00f,\n\t\t0x10101010,\n\t\t0xf0f01010,\n\t\t0x30303030,\n\t\t0xa0a03030,\n\t\t0x50505050,\n\t\t0xc0c05050,\n\t\t0xf0f0f0f0,\n\t\t0x11111111,\n\t\t0x33333333,\n\t\t0x55555555,\n\t\t0x0000ffff,\n\t\t0x00ff00ff,\n\t\t0xff0000ff,\n\t\t0xffff00ff,\n\t\t0xffffffff,\n\t};\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct i915_vma *scratch;\n\tstruct i915_vma *batch;\n\tint err = 0, i, v, sz;\n\tu32 *cs, *results;\n\n\tsz = (2 * ARRAY_SIZE(values) + 1) * sizeof(u32);\n\tscratch = __vm_create_scratch_for_read_pinned(ce->vm, sz);\n\tif (IS_ERR(scratch))\n\t\treturn PTR_ERR(scratch);\n\n\tbatch = create_batch(ce->vm);\n\tif (IS_ERR(batch)) {\n\t\terr = PTR_ERR(batch);\n\t\tgoto out_scratch;\n\t}\n\n\tfor (i = 0; i < engine->whitelist.count; i++) {\n\t\tu32 reg = i915_mmio_reg_offset(engine->whitelist.list[i].reg);\n\t\tstruct i915_gem_ww_ctx ww;\n\t\tu64 addr = i915_vma_offset(scratch);\n\t\tstruct i915_request *rq;\n\t\tu32 srm, lrm, rsvd;\n\t\tu32 expect;\n\t\tint idx;\n\t\tbool ro_reg;\n\n\t\tif (wo_register(engine, reg))\n\t\t\tcontinue;\n\n\t\tif (timestamp(engine, reg))\n\t\t\tcontinue;  \n\n\t\tro_reg = ro_register(reg);\n\n\t\ti915_gem_ww_ctx_init(&ww, false);\nretry:\n\t\tcs = NULL;\n\t\terr = i915_gem_object_lock(scratch->obj, &ww);\n\t\tif (!err)\n\t\t\terr = i915_gem_object_lock(batch->obj, &ww);\n\t\tif (!err)\n\t\t\terr = intel_context_pin_ww(ce, &ww);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tcs = i915_gem_object_pin_map(batch->obj, I915_MAP_WC);\n\t\tif (IS_ERR(cs)) {\n\t\t\terr = PTR_ERR(cs);\n\t\t\tgoto out_ctx;\n\t\t}\n\n\t\tresults = i915_gem_object_pin_map(scratch->obj, I915_MAP_WB);\n\t\tif (IS_ERR(results)) {\n\t\t\terr = PTR_ERR(results);\n\t\t\tgoto out_unmap_batch;\n\t\t}\n\n\t\t \n\t\treg &= RING_FORCE_TO_NONPRIV_ADDRESS_MASK;\n\n\t\tsrm = MI_STORE_REGISTER_MEM;\n\t\tlrm = MI_LOAD_REGISTER_MEM;\n\t\tif (GRAPHICS_VER(engine->i915) >= 8)\n\t\t\tlrm++, srm++;\n\n\t\tpr_debug(\"%s: Writing garbage to %x\\n\",\n\t\t\t engine->name, reg);\n\n\t\t \n\t\t*cs++ = srm;\n\t\t*cs++ = reg;\n\t\t*cs++ = lower_32_bits(addr);\n\t\t*cs++ = upper_32_bits(addr);\n\n\t\tidx = 1;\n\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\t \n\t\t\t*cs++ = MI_LOAD_REGISTER_IMM(1);\n\t\t\t*cs++ = reg;\n\t\t\t*cs++ = values[v];\n\n\t\t\t \n\t\t\t*cs++ = srm;\n\t\t\t*cs++ = reg;\n\t\t\t*cs++ = lower_32_bits(addr + sizeof(u32) * idx);\n\t\t\t*cs++ = upper_32_bits(addr + sizeof(u32) * idx);\n\t\t\tidx++;\n\t\t}\n\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\t \n\t\t\t*cs++ = MI_LOAD_REGISTER_IMM(1);\n\t\t\t*cs++ = reg;\n\t\t\t*cs++ = ~values[v];\n\n\t\t\t \n\t\t\t*cs++ = srm;\n\t\t\t*cs++ = reg;\n\t\t\t*cs++ = lower_32_bits(addr + sizeof(u32) * idx);\n\t\t\t*cs++ = upper_32_bits(addr + sizeof(u32) * idx);\n\t\t\tidx++;\n\t\t}\n\t\tGEM_BUG_ON(idx * sizeof(u32) > scratch->size);\n\n\t\t \n\t\t*cs++ = lrm;\n\t\t*cs++ = reg;\n\t\t*cs++ = lower_32_bits(addr);\n\t\t*cs++ = upper_32_bits(addr);\n\n\t\t*cs++ = MI_BATCH_BUFFER_END;\n\n\t\ti915_gem_object_flush_map(batch->obj);\n\t\ti915_gem_object_unpin_map(batch->obj);\n\t\tintel_gt_chipset_flush(engine->gt);\n\t\tcs = NULL;\n\n\t\trq = i915_request_create(ce);\n\t\tif (IS_ERR(rq)) {\n\t\t\terr = PTR_ERR(rq);\n\t\t\tgoto out_unmap_scratch;\n\t\t}\n\n\t\tif (engine->emit_init_breadcrumb) {  \n\t\t\terr = engine->emit_init_breadcrumb(rq);\n\t\t\tif (err)\n\t\t\t\tgoto err_request;\n\t\t}\n\n\t\terr = i915_vma_move_to_active(batch, rq, 0);\n\t\tif (err)\n\t\t\tgoto err_request;\n\n\t\terr = i915_vma_move_to_active(scratch, rq,\n\t\t\t\t\t      EXEC_OBJECT_WRITE);\n\t\tif (err)\n\t\t\tgoto err_request;\n\n\t\terr = engine->emit_bb_start(rq,\n\t\t\t\t\t    i915_vma_offset(batch), PAGE_SIZE,\n\t\t\t\t\t    0);\n\t\tif (err)\n\t\t\tgoto err_request;\n\nerr_request:\n\t\terr = request_add_sync(rq, err);\n\t\tif (err) {\n\t\t\tpr_err(\"%s: Futzing %x timedout; cancelling test\\n\",\n\t\t\t       engine->name, reg);\n\t\t\tintel_gt_set_wedged(engine->gt);\n\t\t\tgoto out_unmap_scratch;\n\t\t}\n\n\t\tGEM_BUG_ON(values[ARRAY_SIZE(values) - 1] != 0xffffffff);\n\t\tif (!ro_reg) {\n\t\t\t \n\t\t\trsvd = results[ARRAY_SIZE(values)];\n\t\t\tif (!rsvd) {\n\t\t\t\tpr_err(\"%s: Unable to write to whitelisted register %x\\n\",\n\t\t\t\t       engine->name, reg);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out_unmap_scratch;\n\t\t\t}\n\t\t} else {\n\t\t\trsvd = 0;\n\t\t}\n\n\t\texpect = results[0];\n\t\tidx = 1;\n\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\tif (ro_reg)\n\t\t\t\texpect = results[0];\n\t\t\telse\n\t\t\t\texpect = reg_write(expect, values[v], rsvd);\n\n\t\t\tif (results[idx] != expect)\n\t\t\t\terr++;\n\t\t\tidx++;\n\t\t}\n\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\tif (ro_reg)\n\t\t\t\texpect = results[0];\n\t\t\telse\n\t\t\t\texpect = reg_write(expect, ~values[v], rsvd);\n\n\t\t\tif (results[idx] != expect)\n\t\t\t\terr++;\n\t\t\tidx++;\n\t\t}\n\t\tif (err) {\n\t\t\tpr_err(\"%s: %d mismatch between values written to whitelisted register [%x], and values read back!\\n\",\n\t\t\t       engine->name, err, reg);\n\n\t\t\tif (ro_reg)\n\t\t\t\tpr_info(\"%s: Whitelisted read-only register: %x, original value %08x\\n\",\n\t\t\t\t\tengine->name, reg, results[0]);\n\t\t\telse\n\t\t\t\tpr_info(\"%s: Whitelisted register: %x, original value %08x, rsvd %08x\\n\",\n\t\t\t\t\tengine->name, reg, results[0], rsvd);\n\n\t\t\texpect = results[0];\n\t\t\tidx = 1;\n\t\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\t\tu32 w = values[v];\n\n\t\t\t\tif (ro_reg)\n\t\t\t\t\texpect = results[0];\n\t\t\t\telse\n\t\t\t\t\texpect = reg_write(expect, w, rsvd);\n\t\t\t\tpr_info(\"Wrote %08x, read %08x, expect %08x\\n\",\n\t\t\t\t\tw, results[idx], expect);\n\t\t\t\tidx++;\n\t\t\t}\n\t\t\tfor (v = 0; v < ARRAY_SIZE(values); v++) {\n\t\t\t\tu32 w = ~values[v];\n\n\t\t\t\tif (ro_reg)\n\t\t\t\t\texpect = results[0];\n\t\t\t\telse\n\t\t\t\t\texpect = reg_write(expect, w, rsvd);\n\t\t\t\tpr_info(\"Wrote %08x, read %08x, expect %08x\\n\",\n\t\t\t\t\tw, results[idx], expect);\n\t\t\t\tidx++;\n\t\t\t}\n\n\t\t\terr = -EINVAL;\n\t\t}\nout_unmap_scratch:\n\t\ti915_gem_object_unpin_map(scratch->obj);\nout_unmap_batch:\n\t\tif (cs)\n\t\t\ti915_gem_object_unpin_map(batch->obj);\nout_ctx:\n\t\tintel_context_unpin(ce);\nout:\n\t\tif (err == -EDEADLK) {\n\t\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\t\tif (!err)\n\t\t\t\tgoto retry;\n\t\t}\n\t\ti915_gem_ww_ctx_fini(&ww);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tif (igt_flush_test(engine->i915))\n\t\terr = -EIO;\n\n\ti915_vma_unpin_and_release(&batch, 0);\nout_scratch:\n\ti915_vma_unpin_and_release(&scratch, 0);\n\treturn err;\n}\n\nstatic int live_dirty_whitelist(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\t \n\n\tif (GRAPHICS_VER(gt->i915) < 7)  \n\t\treturn 0;\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct intel_context *ce;\n\t\tint err;\n\n\t\tif (engine->whitelist.count == 0)\n\t\t\tcontinue;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce))\n\t\t\treturn PTR_ERR(ce);\n\n\t\terr = check_dirty_whitelist(ce);\n\t\tintel_context_put(ce);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int live_reset_whitelist(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tint err = 0;\n\n\t \n\tigt_global_reset_lock(gt);\n\n\tfor_each_engine(engine, gt, id) {\n\t\tif (engine->whitelist.count == 0)\n\t\t\tcontinue;\n\n\t\tif (intel_has_reset_engine(gt)) {\n\t\t\tif (intel_engine_uses_guc(engine)) {\n\t\t\t\tstruct intel_selftest_saved_policy saved;\n\t\t\t\tint err2;\n\n\t\t\t\terr = intel_selftest_modify_policy(engine, &saved,\n\t\t\t\t\t\t\t\t   SELFTEST_SCHEDULER_MODIFY_FAST_RESET);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\n\t\t\t\terr = check_whitelist_across_reset(engine,\n\t\t\t\t\t\t\t\t   do_guc_reset,\n\t\t\t\t\t\t\t\t   \"guc\");\n\n\t\t\t\terr2 = intel_selftest_restore_policy(engine, &saved);\n\t\t\t\tif (err == 0)\n\t\t\t\t\terr = err2;\n\t\t\t} else {\n\t\t\t\terr = check_whitelist_across_reset(engine,\n\t\t\t\t\t\t\t\t   do_engine_reset,\n\t\t\t\t\t\t\t\t   \"engine\");\n\t\t\t}\n\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (intel_has_gpu_reset(gt)) {\n\t\t\terr = check_whitelist_across_reset(engine,\n\t\t\t\t\t\t\t   do_device_reset,\n\t\t\t\t\t\t\t   \"device\");\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tigt_global_reset_unlock(gt);\n\treturn err;\n}\n\nstatic int read_whitelisted_registers(struct intel_context *ce,\n\t\t\t\t      struct i915_vma *results)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct i915_request *rq;\n\tint i, err = 0;\n\tu32 srm, *cs;\n\n\trq = intel_context_create_request(ce);\n\tif (IS_ERR(rq))\n\t\treturn PTR_ERR(rq);\n\n\terr = igt_vma_move_to_active_unlocked(results, rq, EXEC_OBJECT_WRITE);\n\tif (err)\n\t\tgoto err_req;\n\n\tsrm = MI_STORE_REGISTER_MEM;\n\tif (GRAPHICS_VER(engine->i915) >= 8)\n\t\tsrm++;\n\n\tcs = intel_ring_begin(rq, 4 * engine->whitelist.count);\n\tif (IS_ERR(cs)) {\n\t\terr = PTR_ERR(cs);\n\t\tgoto err_req;\n\t}\n\n\tfor (i = 0; i < engine->whitelist.count; i++) {\n\t\tu64 offset = i915_vma_offset(results) + sizeof(u32) * i;\n\t\tu32 reg = i915_mmio_reg_offset(engine->whitelist.list[i].reg);\n\n\t\t \n\t\treg &= RING_FORCE_TO_NONPRIV_ADDRESS_MASK;\n\n\t\t*cs++ = srm;\n\t\t*cs++ = reg;\n\t\t*cs++ = lower_32_bits(offset);\n\t\t*cs++ = upper_32_bits(offset);\n\t}\n\tintel_ring_advance(rq, cs);\n\nerr_req:\n\treturn request_add_sync(rq, err);\n}\n\nstatic int scrub_whitelisted_registers(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\tstruct i915_request *rq;\n\tstruct i915_vma *batch;\n\tint i, err = 0;\n\tu32 *cs;\n\n\tbatch = create_batch(ce->vm);\n\tif (IS_ERR(batch))\n\t\treturn PTR_ERR(batch);\n\n\tcs = i915_gem_object_pin_map_unlocked(batch->obj, I915_MAP_WC);\n\tif (IS_ERR(cs)) {\n\t\terr = PTR_ERR(cs);\n\t\tgoto err_batch;\n\t}\n\n\t*cs++ = MI_LOAD_REGISTER_IMM(whitelist_writable_count(engine));\n\tfor (i = 0; i < engine->whitelist.count; i++) {\n\t\tu32 reg = i915_mmio_reg_offset(engine->whitelist.list[i].reg);\n\n\t\tif (ro_register(reg))\n\t\t\tcontinue;\n\n\t\t \n\t\treg &= RING_FORCE_TO_NONPRIV_ADDRESS_MASK;\n\n\t\t*cs++ = reg;\n\t\t*cs++ = 0xffffffff;\n\t}\n\t*cs++ = MI_BATCH_BUFFER_END;\n\n\ti915_gem_object_flush_map(batch->obj);\n\tintel_gt_chipset_flush(engine->gt);\n\n\trq = intel_context_create_request(ce);\n\tif (IS_ERR(rq)) {\n\t\terr = PTR_ERR(rq);\n\t\tgoto err_unpin;\n\t}\n\n\tif (engine->emit_init_breadcrumb) {  \n\t\terr = engine->emit_init_breadcrumb(rq);\n\t\tif (err)\n\t\t\tgoto err_request;\n\t}\n\n\terr = igt_vma_move_to_active_unlocked(batch, rq, 0);\n\tif (err)\n\t\tgoto err_request;\n\n\t \n\terr = engine->emit_bb_start(rq, i915_vma_offset(batch), 0, 0);\n\nerr_request:\n\terr = request_add_sync(rq, err);\n\nerr_unpin:\n\ti915_gem_object_unpin_map(batch->obj);\nerr_batch:\n\ti915_vma_unpin_and_release(&batch, 0);\n\treturn err;\n}\n\nstruct regmask {\n\ti915_reg_t reg;\n\tu8 graphics_ver;\n};\n\nstatic bool find_reg(struct drm_i915_private *i915,\n\t\t     i915_reg_t reg,\n\t\t     const struct regmask *tbl,\n\t\t     unsigned long count)\n{\n\tu32 offset = i915_mmio_reg_offset(reg);\n\n\twhile (count--) {\n\t\tif (GRAPHICS_VER(i915) == tbl->graphics_ver &&\n\t\t    i915_mmio_reg_offset(tbl->reg) == offset)\n\t\t\treturn true;\n\t\ttbl++;\n\t}\n\n\treturn false;\n}\n\nstatic bool pardon_reg(struct drm_i915_private *i915, i915_reg_t reg)\n{\n\t \n\tstatic const struct regmask pardon[] = {\n\t\t{ GEN9_CTX_PREEMPT_REG, 9 },\n\t\t{ _MMIO(0xb118), 9 },  \n\t};\n\n\treturn find_reg(i915, reg, pardon, ARRAY_SIZE(pardon));\n}\n\nstatic bool result_eq(struct intel_engine_cs *engine,\n\t\t      u32 a, u32 b, i915_reg_t reg)\n{\n\tif (a != b && !pardon_reg(engine->i915, reg)) {\n\t\tpr_err(\"Whitelisted register 0x%4x not context saved: A=%08x, B=%08x\\n\",\n\t\t       i915_mmio_reg_offset(reg), a, b);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool writeonly_reg(struct drm_i915_private *i915, i915_reg_t reg)\n{\n\t \n\tstatic const struct regmask wo[] = {\n\t\t{ GEN9_SLICE_COMMON_ECO_CHICKEN1, 9 },\n\t};\n\n\treturn find_reg(i915, reg, wo, ARRAY_SIZE(wo));\n}\n\nstatic bool result_neq(struct intel_engine_cs *engine,\n\t\t       u32 a, u32 b, i915_reg_t reg)\n{\n\tif (a == b && !writeonly_reg(engine->i915, reg)) {\n\t\tpr_err(\"Whitelist register 0x%4x:%08x was unwritable\\n\",\n\t\t       i915_mmio_reg_offset(reg), a);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int\ncheck_whitelisted_registers(struct intel_engine_cs *engine,\n\t\t\t    struct i915_vma *A,\n\t\t\t    struct i915_vma *B,\n\t\t\t    bool (*fn)(struct intel_engine_cs *engine,\n\t\t\t\t       u32 a, u32 b,\n\t\t\t\t       i915_reg_t reg))\n{\n\tu32 *a, *b;\n\tint i, err;\n\n\ta = i915_gem_object_pin_map_unlocked(A->obj, I915_MAP_WB);\n\tif (IS_ERR(a))\n\t\treturn PTR_ERR(a);\n\n\tb = i915_gem_object_pin_map_unlocked(B->obj, I915_MAP_WB);\n\tif (IS_ERR(b)) {\n\t\terr = PTR_ERR(b);\n\t\tgoto err_a;\n\t}\n\n\terr = 0;\n\tfor (i = 0; i < engine->whitelist.count; i++) {\n\t\tconst struct i915_wa *wa = &engine->whitelist.list[i];\n\n\t\tif (i915_mmio_reg_offset(wa->reg) &\n\t\t    RING_FORCE_TO_NONPRIV_ACCESS_RD)\n\t\t\tcontinue;\n\n\t\tif (!fn(engine, a[i], b[i], wa->reg))\n\t\t\terr = -EINVAL;\n\t}\n\n\ti915_gem_object_unpin_map(B->obj);\nerr_a:\n\ti915_gem_object_unpin_map(A->obj);\n\treturn err;\n}\n\nstatic int live_isolated_whitelist(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct {\n\t\tstruct i915_vma *scratch[2];\n\t} client[2] = {};\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tint i, err = 0;\n\n\t \n\n\tif (!intel_engines_has_context_isolation(gt->i915))\n\t\treturn 0;\n\n\tfor (i = 0; i < ARRAY_SIZE(client); i++) {\n\t\tclient[i].scratch[0] =\n\t\t\t__vm_create_scratch_for_read_pinned(gt->vm, 4096);\n\t\tif (IS_ERR(client[i].scratch[0])) {\n\t\t\terr = PTR_ERR(client[i].scratch[0]);\n\t\t\tgoto err;\n\t\t}\n\n\t\tclient[i].scratch[1] =\n\t\t\t__vm_create_scratch_for_read_pinned(gt->vm, 4096);\n\t\tif (IS_ERR(client[i].scratch[1])) {\n\t\t\terr = PTR_ERR(client[i].scratch[1]);\n\t\t\ti915_vma_unpin_and_release(&client[i].scratch[0], 0);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct intel_context *ce[2];\n\n\t\tif (!engine->kernel_context->vm)\n\t\t\tcontinue;\n\n\t\tif (!whitelist_writable_count(engine))\n\t\t\tcontinue;\n\n\t\tce[0] = intel_context_create(engine);\n\t\tif (IS_ERR(ce[0])) {\n\t\t\terr = PTR_ERR(ce[0]);\n\t\t\tbreak;\n\t\t}\n\t\tce[1] = intel_context_create(engine);\n\t\tif (IS_ERR(ce[1])) {\n\t\t\terr = PTR_ERR(ce[1]);\n\t\t\tintel_context_put(ce[0]);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\terr = read_whitelisted_registers(ce[0], client[0].scratch[0]);\n\t\tif (err)\n\t\t\tgoto err_ce;\n\n\t\t \n\t\terr = scrub_whitelisted_registers(ce[0]);\n\t\tif (err)\n\t\t\tgoto err_ce;\n\n\t\t \n\t\terr = read_whitelisted_registers(ce[1], client[1].scratch[0]);\n\t\tif (err)\n\t\t\tgoto err_ce;\n\n\t\t \n\t\terr = check_whitelisted_registers(engine,\n\t\t\t\t\t\t  client[0].scratch[0],\n\t\t\t\t\t\t  client[1].scratch[0],\n\t\t\t\t\t\t  result_eq);\n\t\tif (err)\n\t\t\tgoto err_ce;\n\n\t\t \n\t\terr = read_whitelisted_registers(ce[0], client[0].scratch[1]);\n\t\tif (err)\n\t\t\tgoto err_ce;\n\n\t\t \n\t\terr = check_whitelisted_registers(engine,\n\t\t\t\t\t\t  client[0].scratch[0],\n\t\t\t\t\t\t  client[0].scratch[1],\n\t\t\t\t\t\t  result_neq);\nerr_ce:\n\t\tintel_context_put(ce[1]);\n\t\tintel_context_put(ce[0]);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\nerr:\n\tfor (i = 0; i < ARRAY_SIZE(client); i++) {\n\t\ti915_vma_unpin_and_release(&client[i].scratch[1], 0);\n\t\ti915_vma_unpin_and_release(&client[i].scratch[0], 0);\n\t}\n\n\tif (igt_flush_test(gt->i915))\n\t\terr = -EIO;\n\n\treturn err;\n}\n\nstatic bool\nverify_wa_lists(struct intel_gt *gt, struct wa_lists *lists,\n\t\tconst char *str)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tbool ok = true;\n\n\tok &= wa_list_verify(gt, &lists->gt_wa_list, str);\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct intel_context *ce;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce))\n\t\t\treturn false;\n\n\t\tok &= engine_wa_list_verify(ce,\n\t\t\t\t\t    &lists->engine[id].wa_list,\n\t\t\t\t\t    str) == 0;\n\n\t\tok &= engine_wa_list_verify(ce,\n\t\t\t\t\t    &lists->engine[id].ctx_wa_list,\n\t\t\t\t\t    str) == 0;\n\n\t\tintel_context_put(ce);\n\t}\n\n\treturn ok;\n}\n\nstatic int\nlive_gpu_reset_workarounds(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tintel_wakeref_t wakeref;\n\tstruct wa_lists *lists;\n\tbool ok;\n\n\tif (!intel_has_gpu_reset(gt))\n\t\treturn 0;\n\n\tlists = kzalloc(sizeof(*lists), GFP_KERNEL);\n\tif (!lists)\n\t\treturn -ENOMEM;\n\n\tpr_info(\"Verifying after GPU reset...\\n\");\n\n\tigt_global_reset_lock(gt);\n\twakeref = intel_runtime_pm_get(gt->uncore->rpm);\n\n\treference_lists_init(gt, lists);\n\n\tok = verify_wa_lists(gt, lists, \"before reset\");\n\tif (!ok)\n\t\tgoto out;\n\n\tintel_gt_reset(gt, ALL_ENGINES, \"live_workarounds\");\n\n\tok = verify_wa_lists(gt, lists, \"after reset\");\n\nout:\n\treference_lists_fini(gt, lists);\n\tintel_runtime_pm_put(gt->uncore->rpm, wakeref);\n\tigt_global_reset_unlock(gt);\n\tkfree(lists);\n\n\treturn ok ? 0 : -ESRCH;\n}\n\nstatic int\nlive_engine_reset_workarounds(void *arg)\n{\n\tstruct intel_gt *gt = arg;\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\tstruct intel_context *ce;\n\tstruct igt_spinner spin;\n\tstruct i915_request *rq;\n\tintel_wakeref_t wakeref;\n\tstruct wa_lists *lists;\n\tint ret = 0;\n\n\tif (!intel_has_reset_engine(gt))\n\t\treturn 0;\n\n\tlists = kzalloc(sizeof(*lists), GFP_KERNEL);\n\tif (!lists)\n\t\treturn -ENOMEM;\n\n\tigt_global_reset_lock(gt);\n\twakeref = intel_runtime_pm_get(gt->uncore->rpm);\n\n\treference_lists_init(gt, lists);\n\n\tfor_each_engine(engine, gt, id) {\n\t\tstruct intel_selftest_saved_policy saved;\n\t\tbool using_guc = intel_engine_uses_guc(engine);\n\t\tbool ok;\n\t\tint ret2;\n\n\t\tpr_info(\"Verifying after %s reset...\\n\", engine->name);\n\t\tret = intel_selftest_modify_policy(engine, &saved,\n\t\t\t\t\t\t   SELFTEST_SCHEDULER_MODIFY_FAST_RESET);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tce = intel_context_create(engine);\n\t\tif (IS_ERR(ce)) {\n\t\t\tret = PTR_ERR(ce);\n\t\t\tgoto restore;\n\t\t}\n\n\t\tif (!using_guc) {\n\t\t\tok = verify_wa_lists(gt, lists, \"before reset\");\n\t\t\tif (!ok) {\n\t\t\t\tret = -ESRCH;\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tret = intel_engine_reset(engine, \"live_workarounds:idle\");\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Reset failed while idle\\n\", engine->name);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tok = verify_wa_lists(gt, lists, \"after idle reset\");\n\t\t\tif (!ok) {\n\t\t\t\tret = -ESRCH;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tret = igt_spinner_init(&spin, engine->gt);\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\trq = igt_spinner_create_request(&spin, ce, MI_NOOP);\n\t\tif (IS_ERR(rq)) {\n\t\t\tret = PTR_ERR(rq);\n\t\t\tigt_spinner_fini(&spin);\n\t\t\tgoto err;\n\t\t}\n\n\t\tret = request_add_spin(rq, &spin);\n\t\tif (ret) {\n\t\t\tpr_err(\"%s: Spinner failed to start\\n\", engine->name);\n\t\t\tigt_spinner_fini(&spin);\n\t\t\tgoto err;\n\t\t}\n\n\t\t \n\t\tif (i915_request_completed(rq)) {\n\t\t\tret = -ETIMEDOUT;\n\t\t\tgoto skip;\n\t\t}\n\n\t\tif (!using_guc) {\n\t\t\tret = intel_engine_reset(engine, \"live_workarounds:active\");\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Reset failed on an active spinner\\n\",\n\t\t\t\t       engine->name);\n\t\t\t\tigt_spinner_fini(&spin);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (ret == 0)\n\t\t\tret = intel_selftest_wait_for_rq(rq);\n\nskip:\n\t\tigt_spinner_end(&spin);\n\t\tigt_spinner_fini(&spin);\n\n\t\tok = verify_wa_lists(gt, lists, \"after busy reset\");\n\t\tif (!ok)\n\t\t\tret = -ESRCH;\n\nerr:\n\t\tintel_context_put(ce);\n\nrestore:\n\t\tret2 = intel_selftest_restore_policy(engine, &saved);\n\t\tif (ret == 0)\n\t\t\tret = ret2;\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treference_lists_fini(gt, lists);\n\tintel_runtime_pm_put(gt->uncore->rpm, wakeref);\n\tigt_global_reset_unlock(gt);\n\tkfree(lists);\n\n\tigt_flush_test(gt->i915);\n\n\treturn ret;\n}\n\nint intel_workarounds_live_selftests(struct drm_i915_private *i915)\n{\n\tstatic const struct i915_subtest tests[] = {\n\t\tSUBTEST(live_dirty_whitelist),\n\t\tSUBTEST(live_reset_whitelist),\n\t\tSUBTEST(live_isolated_whitelist),\n\t\tSUBTEST(live_gpu_reset_workarounds),\n\t\tSUBTEST(live_engine_reset_workarounds),\n\t};\n\n\tif (intel_gt_is_wedged(to_gt(i915)))\n\t\treturn 0;\n\n\treturn intel_gt_live_subtests(tests, to_gt(i915));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}