{
  "module_name": "intel_timeline.c",
  "hash_id": "8307905e650e2ccc343aa58a3a5db2126c13d6d084dbdf0d00c69ae3bbd3fe4a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_timeline.c",
  "human_readable_source": "\n \n\n#include <drm/drm_cache.h>\n\n#include \"gem/i915_gem_internal.h\"\n\n#include \"i915_active.h\"\n#include \"i915_drv.h\"\n#include \"i915_syncmap.h\"\n#include \"intel_gt.h\"\n#include \"intel_ring.h\"\n#include \"intel_timeline.h\"\n\n#define TIMELINE_SEQNO_BYTES 8\n\nstatic struct i915_vma *hwsp_alloc(struct intel_gt *gt)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\n\tobj = i915_gem_object_create_internal(i915, PAGE_SIZE);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);\n\n\tvma = i915_vma_instance(obj, &gt->ggtt->vm, NULL);\n\tif (IS_ERR(vma))\n\t\ti915_gem_object_put(obj);\n\n\treturn vma;\n}\n\nstatic void __timeline_retire(struct i915_active *active)\n{\n\tstruct intel_timeline *tl =\n\t\tcontainer_of(active, typeof(*tl), active);\n\n\ti915_vma_unpin(tl->hwsp_ggtt);\n\tintel_timeline_put(tl);\n}\n\nstatic int __timeline_active(struct i915_active *active)\n{\n\tstruct intel_timeline *tl =\n\t\tcontainer_of(active, typeof(*tl), active);\n\n\t__i915_vma_pin(tl->hwsp_ggtt);\n\tintel_timeline_get(tl);\n\treturn 0;\n}\n\nI915_SELFTEST_EXPORT int\nintel_timeline_pin_map(struct intel_timeline *timeline)\n{\n\tstruct drm_i915_gem_object *obj = timeline->hwsp_ggtt->obj;\n\tu32 ofs = offset_in_page(timeline->hwsp_offset);\n\tvoid *vaddr;\n\n\tvaddr = i915_gem_object_pin_map(obj, I915_MAP_WB);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\ttimeline->hwsp_map = vaddr;\n\ttimeline->hwsp_seqno = memset(vaddr + ofs, 0, TIMELINE_SEQNO_BYTES);\n\tdrm_clflush_virt_range(vaddr + ofs, TIMELINE_SEQNO_BYTES);\n\n\treturn 0;\n}\n\nstatic int intel_timeline_init(struct intel_timeline *timeline,\n\t\t\t       struct intel_gt *gt,\n\t\t\t       struct i915_vma *hwsp,\n\t\t\t       unsigned int offset)\n{\n\tkref_init(&timeline->kref);\n\tatomic_set(&timeline->pin_count, 0);\n\n\ttimeline->gt = gt;\n\n\tif (hwsp) {\n\t\ttimeline->hwsp_offset = offset;\n\t\ttimeline->hwsp_ggtt = i915_vma_get(hwsp);\n\t} else {\n\t\ttimeline->has_initial_breadcrumb = true;\n\t\thwsp = hwsp_alloc(gt);\n\t\tif (IS_ERR(hwsp))\n\t\t\treturn PTR_ERR(hwsp);\n\t\ttimeline->hwsp_ggtt = hwsp;\n\t}\n\n\ttimeline->hwsp_map = NULL;\n\ttimeline->hwsp_seqno = (void *)(long)timeline->hwsp_offset;\n\n\tGEM_BUG_ON(timeline->hwsp_offset >= hwsp->size);\n\n\ttimeline->fence_context = dma_fence_context_alloc(1);\n\n\tmutex_init(&timeline->mutex);\n\n\tINIT_ACTIVE_FENCE(&timeline->last_request);\n\tINIT_LIST_HEAD(&timeline->requests);\n\n\ti915_syncmap_init(&timeline->sync);\n\ti915_active_init(&timeline->active, __timeline_active,\n\t\t\t __timeline_retire, 0);\n\n\treturn 0;\n}\n\nvoid intel_gt_init_timelines(struct intel_gt *gt)\n{\n\tstruct intel_gt_timelines *timelines = &gt->timelines;\n\n\tspin_lock_init(&timelines->lock);\n\tINIT_LIST_HEAD(&timelines->active_list);\n}\n\nstatic void intel_timeline_fini(struct rcu_head *rcu)\n{\n\tstruct intel_timeline *timeline =\n\t\tcontainer_of(rcu, struct intel_timeline, rcu);\n\n\tif (timeline->hwsp_map)\n\t\ti915_gem_object_unpin_map(timeline->hwsp_ggtt->obj);\n\n\ti915_vma_put(timeline->hwsp_ggtt);\n\ti915_active_fini(&timeline->active);\n\n\t \n\ti915_syncmap_free(&timeline->sync);\n\n\tkfree(timeline);\n}\n\nstruct intel_timeline *\n__intel_timeline_create(struct intel_gt *gt,\n\t\t\tstruct i915_vma *global_hwsp,\n\t\t\tunsigned int offset)\n{\n\tstruct intel_timeline *timeline;\n\tint err;\n\n\ttimeline = kzalloc(sizeof(*timeline), GFP_KERNEL);\n\tif (!timeline)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = intel_timeline_init(timeline, gt, global_hwsp, offset);\n\tif (err) {\n\t\tkfree(timeline);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn timeline;\n}\n\nstruct intel_timeline *\nintel_timeline_create_from_engine(struct intel_engine_cs *engine,\n\t\t\t\t  unsigned int offset)\n{\n\tstruct i915_vma *hwsp = engine->status_page.vma;\n\tstruct intel_timeline *tl;\n\n\ttl = __intel_timeline_create(engine->gt, hwsp, offset);\n\tif (IS_ERR(tl))\n\t\treturn tl;\n\n\t \n\tmutex_lock(&hwsp->vm->mutex);\n\tlist_add_tail(&tl->engine_link, &engine->status_page.timelines);\n\tmutex_unlock(&hwsp->vm->mutex);\n\n\treturn tl;\n}\n\nvoid __intel_timeline_pin(struct intel_timeline *tl)\n{\n\tGEM_BUG_ON(!atomic_read(&tl->pin_count));\n\tatomic_inc(&tl->pin_count);\n}\n\nint intel_timeline_pin(struct intel_timeline *tl, struct i915_gem_ww_ctx *ww)\n{\n\tint err;\n\n\tif (atomic_add_unless(&tl->pin_count, 1, 0))\n\t\treturn 0;\n\n\tif (!tl->hwsp_map) {\n\t\terr = intel_timeline_pin_map(tl);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = i915_ggtt_pin(tl->hwsp_ggtt, ww, 0, PIN_HIGH);\n\tif (err)\n\t\treturn err;\n\n\ttl->hwsp_offset =\n\t\ti915_ggtt_offset(tl->hwsp_ggtt) +\n\t\toffset_in_page(tl->hwsp_offset);\n\tGT_TRACE(tl->gt, \"timeline:%llx using HWSP offset:%x\\n\",\n\t\t tl->fence_context, tl->hwsp_offset);\n\n\ti915_active_acquire(&tl->active);\n\tif (atomic_fetch_inc(&tl->pin_count)) {\n\t\ti915_active_release(&tl->active);\n\t\t__i915_vma_unpin(tl->hwsp_ggtt);\n\t}\n\n\treturn 0;\n}\n\nvoid intel_timeline_reset_seqno(const struct intel_timeline *tl)\n{\n\tu32 *hwsp_seqno = (u32 *)tl->hwsp_seqno;\n\t \n\tGEM_BUG_ON(!atomic_read(&tl->pin_count));\n\n\tmemset(hwsp_seqno + 1, 0, TIMELINE_SEQNO_BYTES - sizeof(*hwsp_seqno));\n\tWRITE_ONCE(*hwsp_seqno, tl->seqno);\n\tdrm_clflush_virt_range(hwsp_seqno, TIMELINE_SEQNO_BYTES);\n}\n\nvoid intel_timeline_enter(struct intel_timeline *tl)\n{\n\tstruct intel_gt_timelines *timelines = &tl->gt->timelines;\n\n\t \n\tlockdep_assert_held(&tl->mutex);\n\n\tif (atomic_add_unless(&tl->active_count, 1, 0))\n\t\treturn;\n\n\tspin_lock(&timelines->lock);\n\tif (!atomic_fetch_inc(&tl->active_count)) {\n\t\t \n\t\tintel_timeline_reset_seqno(tl);\n\t\tlist_add_tail(&tl->link, &timelines->active_list);\n\t}\n\tspin_unlock(&timelines->lock);\n}\n\nvoid intel_timeline_exit(struct intel_timeline *tl)\n{\n\tstruct intel_gt_timelines *timelines = &tl->gt->timelines;\n\n\t \n\tlockdep_assert_held(&tl->mutex);\n\n\tGEM_BUG_ON(!atomic_read(&tl->active_count));\n\tif (atomic_add_unless(&tl->active_count, -1, 1))\n\t\treturn;\n\n\tspin_lock(&timelines->lock);\n\tif (atomic_dec_and_test(&tl->active_count))\n\t\tlist_del(&tl->link);\n\tspin_unlock(&timelines->lock);\n\n\t \n\ti915_syncmap_free(&tl->sync);\n}\n\nstatic u32 timeline_advance(struct intel_timeline *tl)\n{\n\tGEM_BUG_ON(!atomic_read(&tl->pin_count));\n\tGEM_BUG_ON(tl->seqno & tl->has_initial_breadcrumb);\n\n\treturn tl->seqno += 1 + tl->has_initial_breadcrumb;\n}\n\nstatic noinline int\n__intel_timeline_get_seqno(struct intel_timeline *tl,\n\t\t\t   u32 *seqno)\n{\n\tu32 next_ofs = offset_in_page(tl->hwsp_offset + TIMELINE_SEQNO_BYTES);\n\n\t \n\tif (TIMELINE_SEQNO_BYTES <= BIT(5) && (next_ofs & BIT(5)))\n\t\tnext_ofs = offset_in_page(next_ofs + BIT(5));\n\n\ttl->hwsp_offset = i915_ggtt_offset(tl->hwsp_ggtt) + next_ofs;\n\ttl->hwsp_seqno = tl->hwsp_map + next_ofs;\n\tintel_timeline_reset_seqno(tl);\n\n\t*seqno = timeline_advance(tl);\n\tGEM_BUG_ON(i915_seqno_passed(*tl->hwsp_seqno, *seqno));\n\treturn 0;\n}\n\nint intel_timeline_get_seqno(struct intel_timeline *tl,\n\t\t\t     struct i915_request *rq,\n\t\t\t     u32 *seqno)\n{\n\t*seqno = timeline_advance(tl);\n\n\t \n\tif (unlikely(!*seqno && tl->has_initial_breadcrumb))\n\t\treturn __intel_timeline_get_seqno(tl, seqno);\n\n\treturn 0;\n}\n\nint intel_timeline_read_hwsp(struct i915_request *from,\n\t\t\t     struct i915_request *to,\n\t\t\t     u32 *hwsp)\n{\n\tstruct intel_timeline *tl;\n\tint err;\n\n\trcu_read_lock();\n\ttl = rcu_dereference(from->timeline);\n\tif (i915_request_signaled(from) ||\n\t    !i915_active_acquire_if_busy(&tl->active))\n\t\ttl = NULL;\n\n\tif (tl) {\n\t\t \n\t\t*hwsp = i915_ggtt_offset(tl->hwsp_ggtt) +\n\t\t\toffset_in_page(from->hwsp_seqno);\n\t}\n\n\t \n\tif (tl && __i915_request_is_complete(from)) {\n\t\ti915_active_release(&tl->active);\n\t\ttl = NULL;\n\t}\n\trcu_read_unlock();\n\n\tif (!tl)\n\t\treturn 1;\n\n\t \n\tif (!tl->has_initial_breadcrumb) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\terr = i915_active_add_request(&tl->active, to);\n\nout:\n\ti915_active_release(&tl->active);\n\treturn err;\n}\n\nvoid intel_timeline_unpin(struct intel_timeline *tl)\n{\n\tGEM_BUG_ON(!atomic_read(&tl->pin_count));\n\tif (!atomic_dec_and_test(&tl->pin_count))\n\t\treturn;\n\n\ti915_active_release(&tl->active);\n\t__i915_vma_unpin(tl->hwsp_ggtt);\n}\n\nvoid __intel_timeline_free(struct kref *kref)\n{\n\tstruct intel_timeline *timeline =\n\t\tcontainer_of(kref, typeof(*timeline), kref);\n\n\tGEM_BUG_ON(atomic_read(&timeline->pin_count));\n\tGEM_BUG_ON(!list_empty(&timeline->requests));\n\tGEM_BUG_ON(timeline->retire);\n\n\tcall_rcu(&timeline->rcu, intel_timeline_fini);\n}\n\nvoid intel_gt_fini_timelines(struct intel_gt *gt)\n{\n\tstruct intel_gt_timelines *timelines = &gt->timelines;\n\n\tGEM_BUG_ON(!list_empty(&timelines->active_list));\n}\n\nvoid intel_gt_show_timelines(struct intel_gt *gt,\n\t\t\t     struct drm_printer *m,\n\t\t\t     void (*show_request)(struct drm_printer *m,\n\t\t\t\t\t\t  const struct i915_request *rq,\n\t\t\t\t\t\t  const char *prefix,\n\t\t\t\t\t\t  int indent))\n{\n\tstruct intel_gt_timelines *timelines = &gt->timelines;\n\tstruct intel_timeline *tl, *tn;\n\tLIST_HEAD(free);\n\n\tspin_lock(&timelines->lock);\n\tlist_for_each_entry_safe(tl, tn, &timelines->active_list, link) {\n\t\tunsigned long count, ready, inflight;\n\t\tstruct i915_request *rq, *rn;\n\t\tstruct dma_fence *fence;\n\n\t\tif (!mutex_trylock(&tl->mutex)) {\n\t\t\tdrm_printf(m, \"Timeline %llx: busy; skipping\\n\",\n\t\t\t\t   tl->fence_context);\n\t\t\tcontinue;\n\t\t}\n\n\t\tintel_timeline_get(tl);\n\t\tGEM_BUG_ON(!atomic_read(&tl->active_count));\n\t\tatomic_inc(&tl->active_count);  \n\t\tspin_unlock(&timelines->lock);\n\n\t\tcount = 0;\n\t\tready = 0;\n\t\tinflight = 0;\n\t\tlist_for_each_entry_safe(rq, rn, &tl->requests, link) {\n\t\t\tif (i915_request_completed(rq))\n\t\t\t\tcontinue;\n\n\t\t\tcount++;\n\t\t\tif (i915_request_is_ready(rq))\n\t\t\t\tready++;\n\t\t\tif (i915_request_is_active(rq))\n\t\t\t\tinflight++;\n\t\t}\n\n\t\tdrm_printf(m, \"Timeline %llx: { \", tl->fence_context);\n\t\tdrm_printf(m, \"count: %lu, ready: %lu, inflight: %lu\",\n\t\t\t   count, ready, inflight);\n\t\tdrm_printf(m, \", seqno: { current: %d, last: %d }\",\n\t\t\t   *tl->hwsp_seqno, tl->seqno);\n\t\tfence = i915_active_fence_get(&tl->last_request);\n\t\tif (fence) {\n\t\t\tdrm_printf(m, \", engine: %s\",\n\t\t\t\t   to_request(fence)->engine->name);\n\t\t\tdma_fence_put(fence);\n\t\t}\n\t\tdrm_printf(m, \" }\\n\");\n\n\t\tif (show_request) {\n\t\t\tlist_for_each_entry_safe(rq, rn, &tl->requests, link)\n\t\t\t\tshow_request(m, rq, \"\", 2);\n\t\t}\n\n\t\tmutex_unlock(&tl->mutex);\n\t\tspin_lock(&timelines->lock);\n\n\t\t \n\t\tlist_safe_reset_next(tl, tn, link);\n\t\tif (atomic_dec_and_test(&tl->active_count))\n\t\t\tlist_del(&tl->link);\n\n\t\t \n\t\tif (refcount_dec_and_test(&tl->kref.refcount)) {\n\t\t\tGEM_BUG_ON(atomic_read(&tl->active_count));\n\t\t\tlist_add(&tl->link, &free);\n\t\t}\n\t}\n\tspin_unlock(&timelines->lock);\n\n\tlist_for_each_entry_safe(tl, tn, &free, link)\n\t\t__intel_timeline_free(&tl->kref);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"gt/selftests/mock_timeline.c\"\n#include \"gt/selftest_timeline.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}