{
  "module_name": "intel_ring_submission.c",
  "hash_id": "22e5404f4aa4edc9638cfe35cf8d97d592009b76f72c23f5743b7fbf7aa26d88",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_ring_submission.c",
  "human_readable_source": "\n \n\n#include <drm/drm_cache.h>\n\n#include \"gem/i915_gem_internal.h\"\n\n#include \"gen2_engine_cs.h\"\n#include \"gen6_engine_cs.h\"\n#include \"gen6_ppgtt.h\"\n#include \"gen7_renderclear.h\"\n#include \"i915_drv.h\"\n#include \"i915_irq.h\"\n#include \"i915_mitigations.h\"\n#include \"i915_reg.h\"\n#include \"intel_breadcrumbs.h\"\n#include \"intel_context.h\"\n#include \"intel_engine_regs.h\"\n#include \"intel_gt.h\"\n#include \"intel_gt_irq.h\"\n#include \"intel_gt_regs.h\"\n#include \"intel_reset.h\"\n#include \"intel_ring.h\"\n#include \"shmem_utils.h\"\n#include \"intel_engine_heartbeat.h\"\n#include \"intel_engine_pm.h\"\n\n \n#define LEGACY_REQUEST_SIZE 200\n\nstatic void set_hwstam(struct intel_engine_cs *engine, u32 mask)\n{\n\t \n\tif (engine->class == RENDER_CLASS) {\n\t\tif (GRAPHICS_VER(engine->i915) >= 6)\n\t\t\tmask &= ~BIT(0);\n\t\telse\n\t\t\tmask &= ~I915_USER_INTERRUPT;\n\t}\n\n\tintel_engine_set_hwsp_writemask(engine, mask);\n}\n\nstatic void set_hws_pga(struct intel_engine_cs *engine, phys_addr_t phys)\n{\n\tu32 addr;\n\n\taddr = lower_32_bits(phys);\n\tif (GRAPHICS_VER(engine->i915) >= 4)\n\t\taddr |= (phys >> 28) & 0xf0;\n\n\tintel_uncore_write(engine->uncore, HWS_PGA, addr);\n}\n\nstatic struct page *status_page(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_gem_object *obj = engine->status_page.vma->obj;\n\n\tGEM_BUG_ON(!i915_gem_object_has_pinned_pages(obj));\n\treturn sg_page(obj->mm.pages->sgl);\n}\n\nstatic void ring_setup_phys_status_page(struct intel_engine_cs *engine)\n{\n\tset_hws_pga(engine, PFN_PHYS(page_to_pfn(status_page(engine))));\n\tset_hwstam(engine, ~0u);\n}\n\nstatic void set_hwsp(struct intel_engine_cs *engine, u32 offset)\n{\n\ti915_reg_t hwsp;\n\n\t \n\tif (GRAPHICS_VER(engine->i915) == 7) {\n\t\tswitch (engine->id) {\n\t\t \n\t\tdefault:\n\t\t\tGEM_BUG_ON(engine->id);\n\t\t\tfallthrough;\n\t\tcase RCS0:\n\t\t\thwsp = RENDER_HWS_PGA_GEN7;\n\t\t\tbreak;\n\t\tcase BCS0:\n\t\t\thwsp = BLT_HWS_PGA_GEN7;\n\t\t\tbreak;\n\t\tcase VCS0:\n\t\t\thwsp = BSD_HWS_PGA_GEN7;\n\t\t\tbreak;\n\t\tcase VECS0:\n\t\t\thwsp = VEBOX_HWS_PGA_GEN7;\n\t\t\tbreak;\n\t\t}\n\t} else if (GRAPHICS_VER(engine->i915) == 6) {\n\t\thwsp = RING_HWS_PGA_GEN6(engine->mmio_base);\n\t} else {\n\t\thwsp = RING_HWS_PGA(engine->mmio_base);\n\t}\n\n\tintel_uncore_write_fw(engine->uncore, hwsp, offset);\n\tintel_uncore_posting_read_fw(engine->uncore, hwsp);\n}\n\nstatic void flush_cs_tlb(struct intel_engine_cs *engine)\n{\n\tif (!IS_GRAPHICS_VER(engine->i915, 6, 7))\n\t\treturn;\n\n\t \n\tif ((ENGINE_READ(engine, RING_MI_MODE) & MODE_IDLE) == 0)\n\t\tdrm_warn(&engine->i915->drm, \"%s not idle before sync flush!\\n\",\n\t\t\t engine->name);\n\n\tENGINE_WRITE_FW(engine, RING_INSTPM,\n\t\t\t_MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE |\n\t\t\t\t\t   INSTPM_SYNC_FLUSH));\n\tif (__intel_wait_for_register_fw(engine->uncore,\n\t\t\t\t\t RING_INSTPM(engine->mmio_base),\n\t\t\t\t\t INSTPM_SYNC_FLUSH, 0,\n\t\t\t\t\t 2000, 0, NULL))\n\t\tENGINE_TRACE(engine,\n\t\t\t     \"wait for SyncFlush to complete for TLB invalidation timed out\\n\");\n}\n\nstatic void ring_setup_status_page(struct intel_engine_cs *engine)\n{\n\tset_hwsp(engine, i915_ggtt_offset(engine->status_page.vma));\n\tset_hwstam(engine, ~0u);\n\n\tflush_cs_tlb(engine);\n}\n\nstatic struct i915_address_space *vm_alias(struct i915_address_space *vm)\n{\n\tif (i915_is_ggtt(vm))\n\t\tvm = &i915_vm_to_ggtt(vm)->alias->vm;\n\n\treturn vm;\n}\n\nstatic u32 pp_dir(struct i915_address_space *vm)\n{\n\treturn to_gen6_ppgtt(i915_vm_to_ppgtt(vm))->pp_dir;\n}\n\nstatic void set_pp_dir(struct intel_engine_cs *engine)\n{\n\tstruct i915_address_space *vm = vm_alias(engine->gt->vm);\n\n\tif (!vm)\n\t\treturn;\n\n\tENGINE_WRITE_FW(engine, RING_PP_DIR_DCLV, PP_DIR_DCLV_2G);\n\tENGINE_WRITE_FW(engine, RING_PP_DIR_BASE, pp_dir(vm));\n\n\tif (GRAPHICS_VER(engine->i915) >= 7) {\n\t\tENGINE_WRITE_FW(engine,\n\t\t\t\tRING_MODE_GEN7,\n\t\t\t\t_MASKED_BIT_ENABLE(GFX_PPGTT_ENABLE));\n\t}\n}\n\nstatic bool stop_ring(struct intel_engine_cs *engine)\n{\n\t \n\tENGINE_WRITE_FW(engine, RING_HEAD, ENGINE_READ_FW(engine, RING_TAIL));\n\tENGINE_POSTING_READ(engine, RING_HEAD);\n\n\t \n\tENGINE_WRITE_FW(engine, RING_CTL, 0);\n\tENGINE_POSTING_READ(engine, RING_CTL);\n\n\t \n\tENGINE_WRITE_FW(engine, RING_HEAD, 0);\n\tENGINE_WRITE_FW(engine, RING_TAIL, 0);\n\n\treturn (ENGINE_READ_FW(engine, RING_HEAD) & HEAD_ADDR) == 0;\n}\n\nstatic int xcs_resume(struct intel_engine_cs *engine)\n{\n\tstruct intel_ring *ring = engine->legacy.ring;\n\n\tENGINE_TRACE(engine, \"ring:{HEAD:%04x, TAIL:%04x}\\n\",\n\t\t     ring->head, ring->tail);\n\n\t \n\tintel_synchronize_hardirq(engine->i915);\n\tif (!stop_ring(engine))\n\t\tgoto err;\n\n\tif (HWS_NEEDS_PHYSICAL(engine->i915))\n\t\tring_setup_phys_status_page(engine);\n\telse\n\t\tring_setup_status_page(engine);\n\n\tintel_breadcrumbs_reset(engine->breadcrumbs);\n\n\t \n\tENGINE_POSTING_READ(engine, RING_HEAD);\n\n\t \n\tENGINE_WRITE_FW(engine, RING_START, i915_ggtt_offset(ring->vma));\n\n\t \n\tGEM_BUG_ON(!intel_ring_offset_valid(ring, ring->head));\n\tGEM_BUG_ON(!intel_ring_offset_valid(ring, ring->tail));\n\tintel_ring_update_space(ring);\n\n\tset_pp_dir(engine);\n\n\t \n\tENGINE_WRITE_FW(engine, RING_HEAD, ring->head);\n\tENGINE_WRITE_FW(engine, RING_TAIL, ring->head);\n\tENGINE_POSTING_READ(engine, RING_TAIL);\n\n\tENGINE_WRITE_FW(engine, RING_CTL,\n\t\t\tRING_CTL_SIZE(ring->size) | RING_VALID);\n\n\t \n\tif (__intel_wait_for_register_fw(engine->uncore,\n\t\t\t\t\t RING_CTL(engine->mmio_base),\n\t\t\t\t\t RING_VALID, RING_VALID,\n\t\t\t\t\t 5000, 0, NULL))\n\t\tgoto err;\n\n\tif (GRAPHICS_VER(engine->i915) > 2)\n\t\tENGINE_WRITE_FW(engine,\n\t\t\t\tRING_MI_MODE, _MASKED_BIT_DISABLE(STOP_RING));\n\n\t \n\tif (ring->tail != ring->head) {\n\t\tENGINE_WRITE_FW(engine, RING_TAIL, ring->tail);\n\t\tENGINE_POSTING_READ(engine, RING_TAIL);\n\t}\n\n\t \n\tintel_engine_signal_breadcrumbs(engine);\n\treturn 0;\n\nerr:\n\tdrm_err(&engine->i915->drm,\n\t\t\"%s initialization failed; \"\n\t\t\"ctl %08x (valid? %d) head %08x [%08x] tail %08x [%08x] start %08x [expected %08x]\\n\",\n\t\tengine->name,\n\t\tENGINE_READ(engine, RING_CTL),\n\t\tENGINE_READ(engine, RING_CTL) & RING_VALID,\n\t\tENGINE_READ(engine, RING_HEAD), ring->head,\n\t\tENGINE_READ(engine, RING_TAIL), ring->tail,\n\t\tENGINE_READ(engine, RING_START),\n\t\ti915_ggtt_offset(ring->vma));\n\treturn -EIO;\n}\n\nstatic void sanitize_hwsp(struct intel_engine_cs *engine)\n{\n\tstruct intel_timeline *tl;\n\n\tlist_for_each_entry(tl, &engine->status_page.timelines, engine_link)\n\t\tintel_timeline_reset_seqno(tl);\n}\n\nstatic void xcs_sanitize(struct intel_engine_cs *engine)\n{\n\t \n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tmemset(engine->status_page.addr, POISON_INUSE, PAGE_SIZE);\n\n\t \n\tsanitize_hwsp(engine);\n\n\t \n\tdrm_clflush_virt_range(engine->status_page.addr, PAGE_SIZE);\n\n\tintel_engine_reset_pinned_contexts(engine);\n}\n\nstatic void reset_prepare(struct intel_engine_cs *engine)\n{\n\t \n\tENGINE_TRACE(engine, \"\\n\");\n\tintel_engine_stop_cs(engine);\n\n\tif (!stop_ring(engine)) {\n\t\t \n\t\tENGINE_TRACE(engine,\n\t\t\t     \"HEAD not reset to zero, \"\n\t\t\t     \"{ CTL:%08x, HEAD:%08x, TAIL:%08x, START:%08x }\\n\",\n\t\t\t     ENGINE_READ_FW(engine, RING_CTL),\n\t\t\t     ENGINE_READ_FW(engine, RING_HEAD),\n\t\t\t     ENGINE_READ_FW(engine, RING_TAIL),\n\t\t\t     ENGINE_READ_FW(engine, RING_START));\n\t\tif (!stop_ring(engine)) {\n\t\t\tdrm_err(&engine->i915->drm,\n\t\t\t\t\"failed to set %s head to zero \"\n\t\t\t\t\"ctl %08x head %08x tail %08x start %08x\\n\",\n\t\t\t\tengine->name,\n\t\t\t\tENGINE_READ_FW(engine, RING_CTL),\n\t\t\t\tENGINE_READ_FW(engine, RING_HEAD),\n\t\t\t\tENGINE_READ_FW(engine, RING_TAIL),\n\t\t\t\tENGINE_READ_FW(engine, RING_START));\n\t\t}\n\t}\n}\n\nstatic void reset_rewind(struct intel_engine_cs *engine, bool stalled)\n{\n\tstruct i915_request *pos, *rq;\n\tunsigned long flags;\n\tu32 head;\n\n\trq = NULL;\n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\trcu_read_lock();\n\tlist_for_each_entry(pos, &engine->sched_engine->requests, sched.link) {\n\t\tif (!__i915_request_is_complete(pos)) {\n\t\t\trq = pos;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\t \n\n\tif (rq) {\n\t\t \n\t\t__i915_request_reset(rq, stalled);\n\n\t\tGEM_BUG_ON(rq->ring != engine->legacy.ring);\n\t\thead = rq->head;\n\t} else {\n\t\thead = engine->legacy.ring->tail;\n\t}\n\tengine->legacy.ring->head = intel_ring_wrap(engine->legacy.ring, head);\n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\nstatic void reset_finish(struct intel_engine_cs *engine)\n{\n}\n\nstatic void reset_cancel(struct intel_engine_cs *engine)\n{\n\tstruct i915_request *request;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&engine->sched_engine->lock, flags);\n\n\t \n\tlist_for_each_entry(request, &engine->sched_engine->requests, sched.link)\n\t\ti915_request_put(i915_request_mark_eio(request));\n\tintel_engine_signal_breadcrumbs(engine);\n\n\t \n\n\tspin_unlock_irqrestore(&engine->sched_engine->lock, flags);\n}\n\nstatic void i9xx_submit_request(struct i915_request *request)\n{\n\ti915_request_submit(request);\n\twmb();  \n\n\tENGINE_WRITE(request->engine, RING_TAIL,\n\t\t     intel_ring_set_tail(request->ring, request->tail));\n}\n\nstatic void __ring_context_fini(struct intel_context *ce)\n{\n\ti915_vma_put(ce->state);\n}\n\nstatic void ring_context_destroy(struct kref *ref)\n{\n\tstruct intel_context *ce = container_of(ref, typeof(*ce), ref);\n\n\tGEM_BUG_ON(intel_context_is_pinned(ce));\n\n\tif (ce->state)\n\t\t__ring_context_fini(ce);\n\n\tintel_context_fini(ce);\n\tintel_context_free(ce);\n}\n\nstatic int ring_context_init_default_state(struct intel_context *ce,\n\t\t\t\t\t   struct i915_gem_ww_ctx *ww)\n{\n\tstruct drm_i915_gem_object *obj = ce->state->obj;\n\tvoid *vaddr;\n\n\tvaddr = i915_gem_object_pin_map(obj, I915_MAP_WB);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\tshmem_read(ce->engine->default_state, 0,\n\t\t   vaddr, ce->engine->context_size);\n\n\ti915_gem_object_flush_map(obj);\n\t__i915_gem_object_release_map(obj);\n\n\t__set_bit(CONTEXT_VALID_BIT, &ce->flags);\n\treturn 0;\n}\n\nstatic int ring_context_pre_pin(struct intel_context *ce,\n\t\t\t\tstruct i915_gem_ww_ctx *ww,\n\t\t\t\tvoid **unused)\n{\n\tstruct i915_address_space *vm;\n\tint err = 0;\n\n\tif (ce->engine->default_state &&\n\t    !test_bit(CONTEXT_VALID_BIT, &ce->flags)) {\n\t\terr = ring_context_init_default_state(ce, ww);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tvm = vm_alias(ce->vm);\n\tif (vm)\n\t\terr = gen6_ppgtt_pin(i915_vm_to_ppgtt((vm)), ww);\n\n\treturn err;\n}\n\nstatic void __context_unpin_ppgtt(struct intel_context *ce)\n{\n\tstruct i915_address_space *vm;\n\n\tvm = vm_alias(ce->vm);\n\tif (vm)\n\t\tgen6_ppgtt_unpin(i915_vm_to_ppgtt(vm));\n}\n\nstatic void ring_context_unpin(struct intel_context *ce)\n{\n}\n\nstatic void ring_context_post_unpin(struct intel_context *ce)\n{\n\t__context_unpin_ppgtt(ce);\n}\n\nstatic struct i915_vma *\nalloc_context_vma(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tint err;\n\n\tobj = i915_gem_object_create_shmem(i915, engine->context_size);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\t \n\tif (IS_IVYBRIDGE(i915))\n\t\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_L3_LLC);\n\n\tvma = i915_vma_instance(obj, &engine->gt->ggtt->vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\terr = PTR_ERR(vma);\n\t\tgoto err_obj;\n\t}\n\n\treturn vma;\n\nerr_obj:\n\ti915_gem_object_put(obj);\n\treturn ERR_PTR(err);\n}\n\nstatic int ring_context_alloc(struct intel_context *ce)\n{\n\tstruct intel_engine_cs *engine = ce->engine;\n\n\t \n\tGEM_BUG_ON(!engine->legacy.ring);\n\tce->ring = engine->legacy.ring;\n\tce->timeline = intel_timeline_get(engine->legacy.timeline);\n\n\tGEM_BUG_ON(ce->state);\n\tif (engine->context_size) {\n\t\tstruct i915_vma *vma;\n\n\t\tvma = alloc_context_vma(engine);\n\t\tif (IS_ERR(vma))\n\t\t\treturn PTR_ERR(vma);\n\n\t\tce->state = vma;\n\t}\n\n\treturn 0;\n}\n\nstatic int ring_context_pin(struct intel_context *ce, void *unused)\n{\n\treturn 0;\n}\n\nstatic void ring_context_reset(struct intel_context *ce)\n{\n\tintel_ring_reset(ce->ring, ce->ring->emit);\n\tclear_bit(CONTEXT_VALID_BIT, &ce->flags);\n}\n\nstatic void ring_context_revoke(struct intel_context *ce,\n\t\t\t\tstruct i915_request *rq,\n\t\t\t\tunsigned int preempt_timeout_ms)\n{\n\tstruct intel_engine_cs *engine;\n\n\tif (!rq || !i915_request_is_active(rq))\n\t\treturn;\n\n\tengine = rq->engine;\n\tlockdep_assert_held(&engine->sched_engine->lock);\n\tlist_for_each_entry_continue(rq, &engine->sched_engine->requests,\n\t\t\t\t     sched.link)\n\t\tif (rq->context == ce) {\n\t\t\ti915_request_set_error_once(rq, -EIO);\n\t\t\t__i915_request_skip(rq);\n\t\t}\n}\n\nstatic void ring_context_cancel_request(struct intel_context *ce,\n\t\t\t\t\tstruct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = NULL;\n\n\ti915_request_active_engine(rq, &engine);\n\n\tif (engine && intel_engine_pulse(engine))\n\t\tintel_gt_handle_error(engine->gt, engine->mask, 0,\n\t\t\t\t      \"request cancellation by %s\",\n\t\t\t\t      current->comm);\n}\n\nstatic const struct intel_context_ops ring_context_ops = {\n\t.alloc = ring_context_alloc,\n\n\t.cancel_request = ring_context_cancel_request,\n\n\t.revoke = ring_context_revoke,\n\n\t.pre_pin = ring_context_pre_pin,\n\t.pin = ring_context_pin,\n\t.unpin = ring_context_unpin,\n\t.post_unpin = ring_context_post_unpin,\n\n\t.enter = intel_context_enter_engine,\n\t.exit = intel_context_exit_engine,\n\n\t.reset = ring_context_reset,\n\t.destroy = ring_context_destroy,\n};\n\nstatic int load_pd_dir(struct i915_request *rq,\n\t\t       struct i915_address_space *vm,\n\t\t       u32 valid)\n{\n\tconst struct intel_engine_cs * const engine = rq->engine;\n\tu32 *cs;\n\n\tcs = intel_ring_begin(rq, 12);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t*cs++ = MI_LOAD_REGISTER_IMM(1);\n\t*cs++ = i915_mmio_reg_offset(RING_PP_DIR_DCLV(engine->mmio_base));\n\t*cs++ = valid;\n\n\t*cs++ = MI_LOAD_REGISTER_IMM(1);\n\t*cs++ = i915_mmio_reg_offset(RING_PP_DIR_BASE(engine->mmio_base));\n\t*cs++ = pp_dir(vm);\n\n\t \n\t*cs++ = MI_STORE_REGISTER_MEM | MI_SRM_LRM_GLOBAL_GTT;\n\t*cs++ = i915_mmio_reg_offset(RING_PP_DIR_BASE(engine->mmio_base));\n\t*cs++ = intel_gt_scratch_offset(engine->gt,\n\t\t\t\t\tINTEL_GT_SCRATCH_FIELD_DEFAULT);\n\n\t*cs++ = MI_LOAD_REGISTER_IMM(1);\n\t*cs++ = i915_mmio_reg_offset(RING_INSTPM(engine->mmio_base));\n\t*cs++ = _MASKED_BIT_ENABLE(INSTPM_TLB_INVALIDATE);\n\n\tintel_ring_advance(rq, cs);\n\n\treturn rq->engine->emit_flush(rq, EMIT_FLUSH);\n}\n\nstatic int mi_set_context(struct i915_request *rq,\n\t\t\t  struct intel_context *ce,\n\t\t\t  u32 flags)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tstruct drm_i915_private *i915 = engine->i915;\n\tenum intel_engine_id id;\n\tconst int num_engines =\n\t\tIS_HASWELL(i915) ? engine->gt->info.num_engines - 1 : 0;\n\tbool force_restore = false;\n\tint len;\n\tu32 *cs;\n\n\tlen = 4;\n\tif (GRAPHICS_VER(i915) == 7)\n\t\tlen += 2 + (num_engines ? 4 * num_engines + 6 : 0);\n\telse if (GRAPHICS_VER(i915) == 5)\n\t\tlen += 2;\n\tif (flags & MI_FORCE_RESTORE) {\n\t\tGEM_BUG_ON(flags & MI_RESTORE_INHIBIT);\n\t\tflags &= ~MI_FORCE_RESTORE;\n\t\tforce_restore = true;\n\t\tlen += 2;\n\t}\n\n\tcs = intel_ring_begin(rq, len);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\tif (GRAPHICS_VER(i915) == 7) {\n\t\t*cs++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\t\tif (num_engines) {\n\t\t\tstruct intel_engine_cs *signaller;\n\n\t\t\t*cs++ = MI_LOAD_REGISTER_IMM(num_engines);\n\t\t\tfor_each_engine(signaller, engine->gt, id) {\n\t\t\t\tif (signaller == engine)\n\t\t\t\t\tcontinue;\n\n\t\t\t\t*cs++ = i915_mmio_reg_offset(\n\t\t\t\t\t   RING_PSMI_CTL(signaller->mmio_base));\n\t\t\t\t*cs++ = _MASKED_BIT_ENABLE(\n\t\t\t\t\t\tGEN6_PSMI_SLEEP_MSG_DISABLE);\n\t\t\t}\n\t\t}\n\t} else if (GRAPHICS_VER(i915) == 5) {\n\t\t \n\t\t*cs++ = MI_SUSPEND_FLUSH | MI_SUSPEND_FLUSH_EN;\n\t}\n\n\tif (force_restore) {\n\t\t \n\t\t*cs++ = MI_SET_CONTEXT;\n\t\t*cs++ = i915_ggtt_offset(engine->kernel_context->state) |\n\t\t\tMI_MM_SPACE_GTT |\n\t\t\tMI_RESTORE_INHIBIT;\n\t}\n\n\t*cs++ = MI_NOOP;\n\t*cs++ = MI_SET_CONTEXT;\n\t*cs++ = i915_ggtt_offset(ce->state) | flags;\n\t \n\t*cs++ = MI_NOOP;\n\n\tif (GRAPHICS_VER(i915) == 7) {\n\t\tif (num_engines) {\n\t\t\tstruct intel_engine_cs *signaller;\n\t\t\ti915_reg_t last_reg = INVALID_MMIO_REG;  \n\n\t\t\t*cs++ = MI_LOAD_REGISTER_IMM(num_engines);\n\t\t\tfor_each_engine(signaller, engine->gt, id) {\n\t\t\t\tif (signaller == engine)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tlast_reg = RING_PSMI_CTL(signaller->mmio_base);\n\t\t\t\t*cs++ = i915_mmio_reg_offset(last_reg);\n\t\t\t\t*cs++ = _MASKED_BIT_DISABLE(\n\t\t\t\t\t\tGEN6_PSMI_SLEEP_MSG_DISABLE);\n\t\t\t}\n\n\t\t\t \n\t\t\t*cs++ = MI_STORE_REGISTER_MEM | MI_SRM_LRM_GLOBAL_GTT;\n\t\t\t*cs++ = i915_mmio_reg_offset(last_reg);\n\t\t\t*cs++ = intel_gt_scratch_offset(engine->gt,\n\t\t\t\t\t\t\tINTEL_GT_SCRATCH_FIELD_DEFAULT);\n\t\t\t*cs++ = MI_NOOP;\n\t\t}\n\t\t*cs++ = MI_ARB_ON_OFF | MI_ARB_ENABLE;\n\t} else if (GRAPHICS_VER(i915) == 5) {\n\t\t*cs++ = MI_SUSPEND_FLUSH;\n\t}\n\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n}\n\nstatic int remap_l3_slice(struct i915_request *rq, int slice)\n{\n#define L3LOG_DW (GEN7_L3LOG_SIZE / sizeof(u32))\n\tu32 *cs, *remap_info = rq->i915->l3_parity.remap_info[slice];\n\tint i;\n\n\tif (!remap_info)\n\t\treturn 0;\n\n\tcs = intel_ring_begin(rq, L3LOG_DW * 2 + 2);\n\tif (IS_ERR(cs))\n\t\treturn PTR_ERR(cs);\n\n\t \n\t*cs++ = MI_LOAD_REGISTER_IMM(L3LOG_DW);\n\tfor (i = 0; i < L3LOG_DW; i++) {\n\t\t*cs++ = i915_mmio_reg_offset(GEN7_L3LOG(slice, i));\n\t\t*cs++ = remap_info[i];\n\t}\n\t*cs++ = MI_NOOP;\n\tintel_ring_advance(rq, cs);\n\n\treturn 0;\n#undef L3LOG_DW\n}\n\nstatic int remap_l3(struct i915_request *rq)\n{\n\tstruct i915_gem_context *ctx = i915_request_gem_context(rq);\n\tint i, err;\n\n\tif (!ctx || !ctx->remap_slice)\n\t\treturn 0;\n\n\tfor (i = 0; i < MAX_L3_SLICES; i++) {\n\t\tif (!(ctx->remap_slice & BIT(i)))\n\t\t\tcontinue;\n\n\t\terr = remap_l3_slice(rq, i);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tctx->remap_slice = 0;\n\treturn 0;\n}\n\nstatic int switch_mm(struct i915_request *rq, struct i915_address_space *vm)\n{\n\tint ret;\n\n\tif (!vm)\n\t\treturn 0;\n\n\tret = rq->engine->emit_flush(rq, EMIT_FLUSH);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = load_pd_dir(rq, vm, PP_DIR_DCLV_2G);\n\tif (ret)\n\t\treturn ret;\n\n\treturn rq->engine->emit_flush(rq, EMIT_INVALIDATE);\n}\n\nstatic int clear_residuals(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tint ret;\n\n\tret = switch_mm(rq, vm_alias(engine->kernel_context->vm));\n\tif (ret)\n\t\treturn ret;\n\n\tif (engine->kernel_context->state) {\n\t\tret = mi_set_context(rq,\n\t\t\t\t     engine->kernel_context,\n\t\t\t\t     MI_MM_SPACE_GTT | MI_RESTORE_INHIBIT);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = engine->emit_bb_start(rq,\n\t\t\t\t    i915_vma_offset(engine->wa_ctx.vma), 0,\n\t\t\t\t    0);\n\tif (ret)\n\t\treturn ret;\n\n\tret = engine->emit_flush(rq, EMIT_FLUSH);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\treturn engine->emit_flush(rq, EMIT_INVALIDATE);\n}\n\nstatic int switch_context(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tstruct intel_context *ce = rq->context;\n\tvoid **residuals = NULL;\n\tint ret;\n\n\tGEM_BUG_ON(HAS_EXECLISTS(engine->i915));\n\n\tif (engine->wa_ctx.vma && ce != engine->kernel_context) {\n\t\tif (engine->wa_ctx.vma->private != ce &&\n\t\t    i915_mitigate_clear_residuals()) {\n\t\t\tret = clear_residuals(rq);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tresiduals = &engine->wa_ctx.vma->private;\n\t\t}\n\t}\n\n\tret = switch_mm(rq, vm_alias(ce->vm));\n\tif (ret)\n\t\treturn ret;\n\n\tif (ce->state) {\n\t\tu32 flags;\n\n\t\tGEM_BUG_ON(engine->id != RCS0);\n\n\t\t \n\t\tBUILD_BUG_ON(HSW_MI_RS_SAVE_STATE_EN != MI_SAVE_EXT_STATE_EN);\n\t\tBUILD_BUG_ON(HSW_MI_RS_RESTORE_STATE_EN != MI_RESTORE_EXT_STATE_EN);\n\n\t\tflags = MI_SAVE_EXT_STATE_EN | MI_MM_SPACE_GTT;\n\t\tif (test_bit(CONTEXT_VALID_BIT, &ce->flags))\n\t\t\tflags |= MI_RESTORE_EXT_STATE_EN;\n\t\telse\n\t\t\tflags |= MI_RESTORE_INHIBIT;\n\n\t\tret = mi_set_context(rq, ce, flags);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = remap_l3(rq);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (residuals) {\n\t\tintel_context_put(*residuals);\n\t\t*residuals = intel_context_get(ce);\n\t}\n\n\treturn 0;\n}\n\nstatic int ring_request_alloc(struct i915_request *request)\n{\n\tint ret;\n\n\tGEM_BUG_ON(!intel_context_is_pinned(request->context));\n\tGEM_BUG_ON(i915_request_timeline(request)->has_initial_breadcrumb);\n\n\t \n\trequest->reserved_space += LEGACY_REQUEST_SIZE;\n\n\t \n\tret = request->engine->emit_flush(request, EMIT_INVALIDATE);\n\tif (ret)\n\t\treturn ret;\n\n\tret = switch_context(request);\n\tif (ret)\n\t\treturn ret;\n\n\trequest->reserved_space -= LEGACY_REQUEST_SIZE;\n\treturn 0;\n}\n\nstatic void gen6_bsd_submit_request(struct i915_request *request)\n{\n\tstruct intel_uncore *uncore = request->engine->uncore;\n\n\tintel_uncore_forcewake_get(uncore, FORCEWAKE_ALL);\n\n        \n\n\t \n\tintel_uncore_write_fw(uncore, RING_PSMI_CTL(GEN6_BSD_RING_BASE),\n\t\t\t      _MASKED_BIT_ENABLE(GEN6_PSMI_SLEEP_MSG_DISABLE));\n\n\t \n\tintel_uncore_write64_fw(uncore, GEN6_BSD_RNCID, 0x0);\n\n\t \n\tif (__intel_wait_for_register_fw(uncore,\n\t\t\t\t\t RING_PSMI_CTL(GEN6_BSD_RING_BASE),\n\t\t\t\t\t GEN6_BSD_SLEEP_INDICATOR,\n\t\t\t\t\t 0,\n\t\t\t\t\t 1000, 0, NULL))\n\t\tdrm_err(&uncore->i915->drm,\n\t\t\t\"timed out waiting for the BSD ring to wake up\\n\");\n\n\t \n\ti9xx_submit_request(request);\n\n\t \n\tintel_uncore_write_fw(uncore, RING_PSMI_CTL(GEN6_BSD_RING_BASE),\n\t\t\t      _MASKED_BIT_DISABLE(GEN6_PSMI_SLEEP_MSG_DISABLE));\n\n\tintel_uncore_forcewake_put(uncore, FORCEWAKE_ALL);\n}\n\nstatic void i9xx_set_default_submission(struct intel_engine_cs *engine)\n{\n\tengine->submit_request = i9xx_submit_request;\n}\n\nstatic void gen6_bsd_set_default_submission(struct intel_engine_cs *engine)\n{\n\tengine->submit_request = gen6_bsd_submit_request;\n}\n\nstatic void ring_release(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tdrm_WARN_ON(&i915->drm, GRAPHICS_VER(i915) > 2 &&\n\t\t    (ENGINE_READ(engine, RING_MI_MODE) & MODE_IDLE) == 0);\n\n\tintel_engine_cleanup_common(engine);\n\n\tif (engine->wa_ctx.vma) {\n\t\tintel_context_put(engine->wa_ctx.vma->private);\n\t\ti915_vma_unpin_and_release(&engine->wa_ctx.vma, 0);\n\t}\n\n\tintel_ring_unpin(engine->legacy.ring);\n\tintel_ring_put(engine->legacy.ring);\n\n\tintel_timeline_unpin(engine->legacy.timeline);\n\tintel_timeline_put(engine->legacy.timeline);\n}\n\nstatic void irq_handler(struct intel_engine_cs *engine, u16 iir)\n{\n\tintel_engine_signal_breadcrumbs(engine);\n}\n\nstatic void setup_irq(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tintel_engine_set_irq_handler(engine, irq_handler);\n\n\tif (GRAPHICS_VER(i915) >= 6) {\n\t\tengine->irq_enable = gen6_irq_enable;\n\t\tengine->irq_disable = gen6_irq_disable;\n\t} else if (GRAPHICS_VER(i915) >= 5) {\n\t\tengine->irq_enable = gen5_irq_enable;\n\t\tengine->irq_disable = gen5_irq_disable;\n\t} else if (GRAPHICS_VER(i915) >= 3) {\n\t\tengine->irq_enable = gen3_irq_enable;\n\t\tengine->irq_disable = gen3_irq_disable;\n\t} else {\n\t\tengine->irq_enable = gen2_irq_enable;\n\t\tengine->irq_disable = gen2_irq_disable;\n\t}\n}\n\nstatic void add_to_engine(struct i915_request *rq)\n{\n\tlockdep_assert_held(&rq->engine->sched_engine->lock);\n\tlist_move_tail(&rq->sched.link, &rq->engine->sched_engine->requests);\n}\n\nstatic void remove_from_engine(struct i915_request *rq)\n{\n\tspin_lock_irq(&rq->engine->sched_engine->lock);\n\tlist_del_init(&rq->sched.link);\n\n\t \n\tset_bit(I915_FENCE_FLAG_ACTIVE, &rq->fence.flags);\n\n\tspin_unlock_irq(&rq->engine->sched_engine->lock);\n\n\ti915_request_notify_execute_cb_imm(rq);\n}\n\nstatic void setup_common(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\t \n\tGEM_BUG_ON(GRAPHICS_VER(i915) >= 8);\n\n\tsetup_irq(engine);\n\n\tengine->resume = xcs_resume;\n\tengine->sanitize = xcs_sanitize;\n\n\tengine->reset.prepare = reset_prepare;\n\tengine->reset.rewind = reset_rewind;\n\tengine->reset.cancel = reset_cancel;\n\tengine->reset.finish = reset_finish;\n\n\tengine->add_active_request = add_to_engine;\n\tengine->remove_active_request = remove_from_engine;\n\n\tengine->cops = &ring_context_ops;\n\tengine->request_alloc = ring_request_alloc;\n\n\t \n\tengine->emit_fini_breadcrumb = gen3_emit_breadcrumb;\n\tif (GRAPHICS_VER(i915) == 5)\n\t\tengine->emit_fini_breadcrumb = gen5_emit_breadcrumb;\n\n\tengine->set_default_submission = i9xx_set_default_submission;\n\n\tif (GRAPHICS_VER(i915) >= 6)\n\t\tengine->emit_bb_start = gen6_emit_bb_start;\n\telse if (GRAPHICS_VER(i915) >= 4)\n\t\tengine->emit_bb_start = gen4_emit_bb_start;\n\telse if (IS_I830(i915) || IS_I845G(i915))\n\t\tengine->emit_bb_start = i830_emit_bb_start;\n\telse\n\t\tengine->emit_bb_start = gen3_emit_bb_start;\n}\n\nstatic void setup_rcs(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tif (HAS_L3_DPF(i915))\n\t\tengine->irq_keep_mask = GT_RENDER_L3_PARITY_ERROR_INTERRUPT;\n\n\tengine->irq_enable_mask = GT_RENDER_USER_INTERRUPT;\n\n\tif (GRAPHICS_VER(i915) >= 7) {\n\t\tengine->emit_flush = gen7_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen7_emit_breadcrumb_rcs;\n\t} else if (GRAPHICS_VER(i915) == 6) {\n\t\tengine->emit_flush = gen6_emit_flush_rcs;\n\t\tengine->emit_fini_breadcrumb = gen6_emit_breadcrumb_rcs;\n\t} else if (GRAPHICS_VER(i915) == 5) {\n\t\tengine->emit_flush = gen4_emit_flush_rcs;\n\t} else {\n\t\tif (GRAPHICS_VER(i915) < 4)\n\t\t\tengine->emit_flush = gen2_emit_flush;\n\t\telse\n\t\t\tengine->emit_flush = gen4_emit_flush_rcs;\n\t\tengine->irq_enable_mask = I915_USER_INTERRUPT;\n\t}\n\n\tif (IS_HASWELL(i915))\n\t\tengine->emit_bb_start = hsw_emit_bb_start;\n}\n\nstatic void setup_vcs(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tif (GRAPHICS_VER(i915) >= 6) {\n\t\t \n\t\tif (GRAPHICS_VER(i915) == 6)\n\t\t\tengine->set_default_submission = gen6_bsd_set_default_submission;\n\t\tengine->emit_flush = gen6_emit_flush_vcs;\n\t\tengine->irq_enable_mask = GT_BSD_USER_INTERRUPT;\n\n\t\tif (GRAPHICS_VER(i915) == 6)\n\t\t\tengine->emit_fini_breadcrumb = gen6_emit_breadcrumb_xcs;\n\t\telse\n\t\t\tengine->emit_fini_breadcrumb = gen7_emit_breadcrumb_xcs;\n\t} else {\n\t\tengine->emit_flush = gen4_emit_flush_vcs;\n\t\tif (GRAPHICS_VER(i915) == 5)\n\t\t\tengine->irq_enable_mask = ILK_BSD_USER_INTERRUPT;\n\t\telse\n\t\t\tengine->irq_enable_mask = I915_BSD_USER_INTERRUPT;\n\t}\n}\n\nstatic void setup_bcs(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tengine->emit_flush = gen6_emit_flush_xcs;\n\tengine->irq_enable_mask = GT_BLT_USER_INTERRUPT;\n\n\tif (GRAPHICS_VER(i915) == 6)\n\t\tengine->emit_fini_breadcrumb = gen6_emit_breadcrumb_xcs;\n\telse\n\t\tengine->emit_fini_breadcrumb = gen7_emit_breadcrumb_xcs;\n}\n\nstatic void setup_vecs(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tGEM_BUG_ON(GRAPHICS_VER(i915) < 7);\n\n\tengine->emit_flush = gen6_emit_flush_xcs;\n\tengine->irq_enable_mask = PM_VEBOX_USER_INTERRUPT;\n\tengine->irq_enable = hsw_irq_enable_vecs;\n\tengine->irq_disable = hsw_irq_disable_vecs;\n\n\tengine->emit_fini_breadcrumb = gen7_emit_breadcrumb_xcs;\n}\n\nstatic int gen7_ctx_switch_bb_setup(struct intel_engine_cs * const engine,\n\t\t\t\t    struct i915_vma * const vma)\n{\n\treturn gen7_setup_clear_gpr_bb(engine, vma);\n}\n\nstatic int gen7_ctx_switch_bb_init(struct intel_engine_cs *engine,\n\t\t\t\t   struct i915_gem_ww_ctx *ww,\n\t\t\t\t   struct i915_vma *vma)\n{\n\tint err;\n\n\terr = i915_vma_pin_ww(vma, ww, 0, 0, PIN_USER | PIN_HIGH);\n\tif (err)\n\t\treturn err;\n\n\terr = i915_vma_sync(vma);\n\tif (err)\n\t\tgoto err_unpin;\n\n\terr = gen7_ctx_switch_bb_setup(engine, vma);\n\tif (err)\n\t\tgoto err_unpin;\n\n\tengine->wa_ctx.vma = vma;\n\treturn 0;\n\nerr_unpin:\n\ti915_vma_unpin(vma);\n\treturn err;\n}\n\nstatic struct i915_vma *gen7_ctx_vma(struct intel_engine_cs *engine)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\tint size, err;\n\n\tif (GRAPHICS_VER(engine->i915) != 7 || engine->class != RENDER_CLASS)\n\t\treturn NULL;\n\n\terr = gen7_ctx_switch_bb_setup(engine, NULL  );\n\tif (err < 0)\n\t\treturn ERR_PTR(err);\n\tif (!err)\n\t\treturn NULL;\n\n\tsize = ALIGN(err, PAGE_SIZE);\n\n\tobj = i915_gem_object_create_internal(engine->i915, size);\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tvma = i915_vma_instance(obj, engine->gt->vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\ti915_gem_object_put(obj);\n\t\treturn ERR_CAST(vma);\n\t}\n\n\tvma->private = intel_context_create(engine);  \n\tif (IS_ERR(vma->private)) {\n\t\terr = PTR_ERR(vma->private);\n\t\tvma->private = NULL;\n\t\ti915_gem_object_put(obj);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn vma;\n}\n\nint intel_ring_submission_setup(struct intel_engine_cs *engine)\n{\n\tstruct i915_gem_ww_ctx ww;\n\tstruct intel_timeline *timeline;\n\tstruct intel_ring *ring;\n\tstruct i915_vma *gen7_wa_vma;\n\tint err;\n\n\tsetup_common(engine);\n\n\tswitch (engine->class) {\n\tcase RENDER_CLASS:\n\t\tsetup_rcs(engine);\n\t\tbreak;\n\tcase VIDEO_DECODE_CLASS:\n\t\tsetup_vcs(engine);\n\t\tbreak;\n\tcase COPY_ENGINE_CLASS:\n\t\tsetup_bcs(engine);\n\t\tbreak;\n\tcase VIDEO_ENHANCEMENT_CLASS:\n\t\tsetup_vecs(engine);\n\t\tbreak;\n\tdefault:\n\t\tMISSING_CASE(engine->class);\n\t\treturn -ENODEV;\n\t}\n\n\ttimeline = intel_timeline_create_from_engine(engine,\n\t\t\t\t\t\t     I915_GEM_HWS_SEQNO_ADDR);\n\tif (IS_ERR(timeline)) {\n\t\terr = PTR_ERR(timeline);\n\t\tgoto err;\n\t}\n\tGEM_BUG_ON(timeline->has_initial_breadcrumb);\n\n\tring = intel_engine_create_ring(engine, SZ_16K);\n\tif (IS_ERR(ring)) {\n\t\terr = PTR_ERR(ring);\n\t\tgoto err_timeline;\n\t}\n\n\tGEM_BUG_ON(engine->legacy.ring);\n\tengine->legacy.ring = ring;\n\tengine->legacy.timeline = timeline;\n\n\tgen7_wa_vma = gen7_ctx_vma(engine);\n\tif (IS_ERR(gen7_wa_vma)) {\n\t\terr = PTR_ERR(gen7_wa_vma);\n\t\tgoto err_ring;\n\t}\n\n\ti915_gem_ww_ctx_init(&ww, false);\n\nretry:\n\terr = i915_gem_object_lock(timeline->hwsp_ggtt->obj, &ww);\n\tif (!err && gen7_wa_vma)\n\t\terr = i915_gem_object_lock(gen7_wa_vma->obj, &ww);\n\tif (!err)\n\t\terr = i915_gem_object_lock(engine->legacy.ring->vma->obj, &ww);\n\tif (!err)\n\t\terr = intel_timeline_pin(timeline, &ww);\n\tif (!err) {\n\t\terr = intel_ring_pin(ring, &ww);\n\t\tif (err)\n\t\t\tintel_timeline_unpin(timeline);\n\t}\n\tif (err)\n\t\tgoto out;\n\n\tGEM_BUG_ON(timeline->hwsp_ggtt != engine->status_page.vma);\n\n\tif (gen7_wa_vma) {\n\t\terr = gen7_ctx_switch_bb_init(engine, &ww, gen7_wa_vma);\n\t\tif (err) {\n\t\t\tintel_ring_unpin(ring);\n\t\t\tintel_timeline_unpin(timeline);\n\t\t}\n\t}\n\nout:\n\tif (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\tif (err)\n\t\tgoto err_gen7_put;\n\n\t \n\tengine->release = ring_release;\n\n\treturn 0;\n\nerr_gen7_put:\n\tif (gen7_wa_vma) {\n\t\tintel_context_put(gen7_wa_vma->private);\n\t\ti915_gem_object_put(gen7_wa_vma->obj);\n\t}\nerr_ring:\n\tintel_ring_put(ring);\nerr_timeline:\n\tintel_timeline_put(timeline);\nerr:\n\tintel_engine_cleanup_common(engine);\n\treturn err;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftest_ring_submission.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}