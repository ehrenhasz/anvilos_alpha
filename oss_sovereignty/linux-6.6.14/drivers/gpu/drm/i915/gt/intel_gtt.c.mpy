{
  "module_name": "intel_gtt.c",
  "hash_id": "63533f63317876b853e4db05d3551837388ac3dc14aef31637d442aad1a97b7c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/gt/intel_gtt.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>  \n\n#include <linux/fault-inject.h>\n#include <linux/sched/mm.h>\n\n#include <drm/drm_cache.h>\n\n#include \"gem/i915_gem_internal.h\"\n#include \"gem/i915_gem_lmem.h\"\n#include \"i915_reg.h\"\n#include \"i915_trace.h\"\n#include \"i915_utils.h\"\n#include \"intel_gt.h\"\n#include \"intel_gt_mcr.h\"\n#include \"intel_gt_print.h\"\n#include \"intel_gt_regs.h\"\n#include \"intel_gtt.h\"\n\n\nstatic bool intel_ggtt_update_needs_vtd_wa(struct drm_i915_private *i915)\n{\n\treturn IS_BROXTON(i915) && i915_vtd_active(i915);\n}\n\nbool intel_vm_no_concurrent_access_wa(struct drm_i915_private *i915)\n{\n\treturn IS_CHERRYVIEW(i915) || intel_ggtt_update_needs_vtd_wa(i915);\n}\n\nstruct drm_i915_gem_object *alloc_pt_lmem(struct i915_address_space *vm, int sz)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\t \n\tobj = __i915_gem_object_create_lmem_with_ps(vm->i915, sz, sz,\n\t\t\t\t\t\t    vm->lmem_pt_obj_flags);\n\t \n\tif (!IS_ERR(obj)) {\n\t\tobj->base.resv = i915_vm_resv_get(vm);\n\t\tobj->shares_resv_from = vm;\n\t}\n\n\treturn obj;\n}\n\nstruct drm_i915_gem_object *alloc_pt_dma(struct i915_address_space *vm, int sz)\n{\n\tstruct drm_i915_gem_object *obj;\n\n\tif (I915_SELFTEST_ONLY(should_fail(&vm->fault_attr, 1)))\n\t\ti915_gem_shrink_all(vm->i915);\n\n\tobj = i915_gem_object_create_internal(vm->i915, sz);\n\t \n\tif (!IS_ERR(obj)) {\n\t\tobj->base.resv = i915_vm_resv_get(vm);\n\t\tobj->shares_resv_from = vm;\n\t}\n\n\treturn obj;\n}\n\nint map_pt_dma(struct i915_address_space *vm, struct drm_i915_gem_object *obj)\n{\n\tenum i915_map_type type;\n\tvoid *vaddr;\n\n\ttype = intel_gt_coherent_map_type(vm->gt, obj, true);\n\tvaddr = i915_gem_object_pin_map_unlocked(obj, type);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\ti915_gem_object_make_unshrinkable(obj);\n\treturn 0;\n}\n\nint map_pt_dma_locked(struct i915_address_space *vm, struct drm_i915_gem_object *obj)\n{\n\tenum i915_map_type type;\n\tvoid *vaddr;\n\n\ttype = intel_gt_coherent_map_type(vm->gt, obj, true);\n\tvaddr = i915_gem_object_pin_map(obj, type);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\ti915_gem_object_make_unshrinkable(obj);\n\treturn 0;\n}\n\nstatic void clear_vm_list(struct list_head *list)\n{\n\tstruct i915_vma *vma, *vn;\n\n\tlist_for_each_entry_safe(vma, vn, list, vm_link) {\n\t\tstruct drm_i915_gem_object *obj = vma->obj;\n\n\t\tif (!i915_gem_object_get_rcu(obj)) {\n\t\t\t \n\t\t\tatomic_and(~I915_VMA_PIN_MASK, &vma->flags);\n\t\t\tWARN_ON(__i915_vma_unbind(vma));\n\n\t\t\t \n\t\t\tlist_del_init(&vma->vm_link);\n\n\t\t\t \n\t\t\ti915_vm_resv_get(vma->vm);\n\t\t\tvma->vm_ddestroy = true;\n\t\t} else {\n\t\t\ti915_vma_destroy_locked(vma);\n\t\t\ti915_gem_object_put(obj);\n\t\t}\n\n\t}\n}\n\nstatic void __i915_vm_close(struct i915_address_space *vm)\n{\n\tmutex_lock(&vm->mutex);\n\n\tclear_vm_list(&vm->bound_list);\n\tclear_vm_list(&vm->unbound_list);\n\n\t \n\tGEM_BUG_ON(!list_empty(&vm->bound_list));\n\tGEM_BUG_ON(!list_empty(&vm->unbound_list));\n\n\tmutex_unlock(&vm->mutex);\n}\n\n \nint i915_vm_lock_objects(struct i915_address_space *vm,\n\t\t\t struct i915_gem_ww_ctx *ww)\n{\n\tif (vm->scratch[0]->base.resv == &vm->_resv) {\n\t\treturn i915_gem_object_lock(vm->scratch[0], ww);\n\t} else {\n\t\tstruct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);\n\n\t\t \n\t\treturn i915_gem_object_lock(ppgtt->pd->pt.base, ww);\n\t}\n}\n\nvoid i915_address_space_fini(struct i915_address_space *vm)\n{\n\tdrm_mm_takedown(&vm->mm);\n}\n\n \nvoid i915_vm_resv_release(struct kref *kref)\n{\n\tstruct i915_address_space *vm =\n\t\tcontainer_of(kref, typeof(*vm), resv_ref);\n\n\tdma_resv_fini(&vm->_resv);\n\tmutex_destroy(&vm->mutex);\n\n\tkfree(vm);\n}\n\nstatic void __i915_vm_release(struct work_struct *work)\n{\n\tstruct i915_address_space *vm =\n\t\tcontainer_of(work, struct i915_address_space, release_work);\n\n\t__i915_vm_close(vm);\n\n\t \n\ti915_vma_resource_bind_dep_sync_all(vm);\n\n\tvm->cleanup(vm);\n\ti915_address_space_fini(vm);\n\n\ti915_vm_resv_put(vm);\n}\n\nvoid i915_vm_release(struct kref *kref)\n{\n\tstruct i915_address_space *vm =\n\t\tcontainer_of(kref, struct i915_address_space, ref);\n\n\tGEM_BUG_ON(i915_is_ggtt(vm));\n\ttrace_i915_ppgtt_release(vm);\n\n\tqueue_work(vm->i915->wq, &vm->release_work);\n}\n\nvoid i915_address_space_init(struct i915_address_space *vm, int subclass)\n{\n\tkref_init(&vm->ref);\n\n\t \n\tif (!kref_read(&vm->resv_ref))\n\t\tkref_init(&vm->resv_ref);\n\n\tvm->pending_unbind = RB_ROOT_CACHED;\n\tINIT_WORK(&vm->release_work, __i915_vm_release);\n\n\t \n\tmutex_init(&vm->mutex);\n\tlockdep_set_subclass(&vm->mutex, subclass);\n\n\tif (!intel_vm_no_concurrent_access_wa(vm->i915)) {\n\t\ti915_gem_shrinker_taints_mutex(vm->i915, &vm->mutex);\n\t} else {\n\t\t \n\t\tmutex_acquire(&vm->mutex.dep_map, 0, 0, _THIS_IP_);\n\t\tmight_alloc(GFP_KERNEL);\n\t\tmutex_release(&vm->mutex.dep_map, _THIS_IP_);\n\t}\n\tdma_resv_init(&vm->_resv);\n\n\tGEM_BUG_ON(!vm->total);\n\tdrm_mm_init(&vm->mm, 0, vm->total);\n\n\tmemset64(vm->min_alignment, I915_GTT_MIN_ALIGNMENT,\n\t\t ARRAY_SIZE(vm->min_alignment));\n\n\tif (HAS_64K_PAGES(vm->i915)) {\n\t\tvm->min_alignment[INTEL_MEMORY_LOCAL] = I915_GTT_PAGE_SIZE_64K;\n\t\tvm->min_alignment[INTEL_MEMORY_STOLEN_LOCAL] = I915_GTT_PAGE_SIZE_64K;\n\t}\n\n\tvm->mm.head_node.color = I915_COLOR_UNEVICTABLE;\n\n\tINIT_LIST_HEAD(&vm->bound_list);\n\tINIT_LIST_HEAD(&vm->unbound_list);\n}\n\nvoid *__px_vaddr(struct drm_i915_gem_object *p)\n{\n\tenum i915_map_type type;\n\n\tGEM_BUG_ON(!i915_gem_object_has_pages(p));\n\treturn page_unpack_bits(p->mm.mapping, &type);\n}\n\ndma_addr_t __px_dma(struct drm_i915_gem_object *p)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(p));\n\treturn sg_dma_address(p->mm.pages->sgl);\n}\n\nstruct page *__px_page(struct drm_i915_gem_object *p)\n{\n\tGEM_BUG_ON(!i915_gem_object_has_pages(p));\n\treturn sg_page(p->mm.pages->sgl);\n}\n\nvoid\nfill_page_dma(struct drm_i915_gem_object *p, const u64 val, unsigned int count)\n{\n\tvoid *vaddr = __px_vaddr(p);\n\n\tmemset64(vaddr, val, count);\n\tdrm_clflush_virt_range(vaddr, PAGE_SIZE);\n}\n\nstatic void poison_scratch_page(struct drm_i915_gem_object *scratch)\n{\n\tvoid *vaddr = __px_vaddr(scratch);\n\tu8 val;\n\n\tval = 0;\n\tif (IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM))\n\t\tval = POISON_FREE;\n\n\tmemset(vaddr, val, scratch->base.size);\n\tdrm_clflush_virt_range(vaddr, scratch->base.size);\n}\n\nint setup_scratch_page(struct i915_address_space *vm)\n{\n\tunsigned long size;\n\n\t \n\tsize = I915_GTT_PAGE_SIZE_4K;\n\tif (i915_vm_is_4lvl(vm) &&\n\t    HAS_PAGE_SIZES(vm->i915, I915_GTT_PAGE_SIZE_64K) &&\n\t    !HAS_64K_PAGES(vm->i915))\n\t\tsize = I915_GTT_PAGE_SIZE_64K;\n\n\tdo {\n\t\tstruct drm_i915_gem_object *obj;\n\n\t\tobj = vm->alloc_scratch_dma(vm, size);\n\t\tif (IS_ERR(obj))\n\t\t\tgoto skip;\n\n\t\tif (map_pt_dma(vm, obj))\n\t\t\tgoto skip_obj;\n\n\t\t \n\t\tif (obj->mm.page_sizes.sg < size)\n\t\t\tgoto skip_obj;\n\n\t\t \n\t\tif (__px_dma(obj) & (size - 1))\n\t\t\tgoto skip_obj;\n\n\t\t \n\t\tpoison_scratch_page(obj);\n\n\t\tvm->scratch[0] = obj;\n\t\tvm->scratch_order = get_order(size);\n\t\treturn 0;\n\nskip_obj:\n\t\ti915_gem_object_put(obj);\nskip:\n\t\tif (size == I915_GTT_PAGE_SIZE_4K)\n\t\t\treturn -ENOMEM;\n\n\t\tsize = I915_GTT_PAGE_SIZE_4K;\n\t} while (1);\n}\n\nvoid free_scratch(struct i915_address_space *vm)\n{\n\tint i;\n\n\tif (!vm->scratch[0])\n\t\treturn;\n\n\tfor (i = 0; i <= vm->top; i++)\n\t\ti915_gem_object_put(vm->scratch[i]);\n}\n\nvoid gtt_write_workarounds(struct intel_gt *gt)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct intel_uncore *uncore = gt->uncore;\n\n\t \n\t \n\tif (IS_BROADWELL(i915))\n\t\tintel_uncore_write(uncore,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU_DEFAULT_VALUE_BDW);\n\telse if (IS_CHERRYVIEW(i915))\n\t\tintel_uncore_write(uncore,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU_DEFAULT_VALUE_CHV);\n\telse if (IS_GEN9_LP(i915))\n\t\tintel_uncore_write(uncore,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU,\n\t\t\t\t   GEN9_L3_LRA_1_GPGPU_DEFAULT_VALUE_BXT);\n\telse if (GRAPHICS_VER(i915) >= 9 && GRAPHICS_VER(i915) <= 11)\n\t\tintel_uncore_write(uncore,\n\t\t\t\t   GEN8_L3_LRA_1_GPGPU,\n\t\t\t\t   GEN9_L3_LRA_1_GPGPU_DEFAULT_VALUE_SKL);\n\n\t \n\tif (HAS_PAGE_SIZES(i915, I915_GTT_PAGE_SIZE_64K) &&\n\t    GRAPHICS_VER(i915) <= 10)\n\t\tintel_uncore_rmw(uncore,\n\t\t\t\t GEN8_GAMW_ECO_DEV_RW_IA,\n\t\t\t\t 0,\n\t\t\t\t GAMW_ECO_ENABLE_64K_IPS_FIELD);\n\n\tif (IS_GRAPHICS_VER(i915, 8, 11)) {\n\t\tbool can_use_gtt_cache = true;\n\n\t\t \n\t\tif (HAS_PAGE_SIZES(i915, I915_GTT_PAGE_SIZE_2M))\n\t\t\tcan_use_gtt_cache = false;\n\n\t\t \n\t\tintel_uncore_write(uncore,\n\t\t\t\t   HSW_GTT_CACHE_EN,\n\t\t\t\t   can_use_gtt_cache ? GTT_CACHE_EN_ALL : 0);\n\t\tgt_WARN_ON_ONCE(gt, can_use_gtt_cache &&\n\t\t\t\tintel_uncore_read(uncore,\n\t\t\t\t\t\t  HSW_GTT_CACHE_EN) == 0);\n\t}\n}\n\nstatic void xelpmp_setup_private_ppat(struct intel_uncore *uncore)\n{\n\tintel_uncore_write(uncore, XELPMP_PAT_INDEX(0),\n\t\t\t   MTL_PPAT_L4_0_WB);\n\tintel_uncore_write(uncore, XELPMP_PAT_INDEX(1),\n\t\t\t   MTL_PPAT_L4_1_WT);\n\tintel_uncore_write(uncore, XELPMP_PAT_INDEX(2),\n\t\t\t   MTL_PPAT_L4_3_UC);\n\tintel_uncore_write(uncore, XELPMP_PAT_INDEX(3),\n\t\t\t   MTL_PPAT_L4_0_WB | MTL_2_COH_1W);\n\tintel_uncore_write(uncore, XELPMP_PAT_INDEX(4),\n\t\t\t   MTL_PPAT_L4_0_WB | MTL_3_COH_2W);\n\n\t \n}\n\nstatic void xelpg_setup_private_ppat(struct intel_gt *gt)\n{\n\tintel_gt_mcr_multicast_write(gt, XEHP_PAT_INDEX(0),\n\t\t\t\t     MTL_PPAT_L4_0_WB);\n\tintel_gt_mcr_multicast_write(gt, XEHP_PAT_INDEX(1),\n\t\t\t\t     MTL_PPAT_L4_1_WT);\n\tintel_gt_mcr_multicast_write(gt, XEHP_PAT_INDEX(2),\n\t\t\t\t     MTL_PPAT_L4_3_UC);\n\tintel_gt_mcr_multicast_write(gt, XEHP_PAT_INDEX(3),\n\t\t\t\t     MTL_PPAT_L4_0_WB | MTL_2_COH_1W);\n\tintel_gt_mcr_multicast_write(gt, XEHP_PAT_INDEX(4),\n\t\t\t\t     MTL_PPAT_L4_0_WB | MTL_3_COH_2W);\n\n\t \n}\n\nstatic void tgl_setup_private_ppat(struct intel_uncore *uncore)\n{\n\t \n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(0), GEN8_PPAT_WB);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(1), GEN8_PPAT_WC);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(2), GEN8_PPAT_WT);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(3), GEN8_PPAT_UC);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(4), GEN8_PPAT_WB);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(5), GEN8_PPAT_WB);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(6), GEN8_PPAT_WB);\n\tintel_uncore_write(uncore, GEN12_PAT_INDEX(7), GEN8_PPAT_WB);\n}\n\nstatic void xehp_setup_private_ppat(struct intel_gt *gt)\n{\n\tenum forcewake_domains fw;\n\tunsigned long flags;\n\n\tfw = intel_uncore_forcewake_for_reg(gt->uncore, _MMIO(XEHP_PAT_INDEX(0).reg),\n\t\t\t\t\t    FW_REG_WRITE);\n\tintel_uncore_forcewake_get(gt->uncore, fw);\n\n\tintel_gt_mcr_lock(gt, &flags);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(0), GEN8_PPAT_WB);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(1), GEN8_PPAT_WC);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(2), GEN8_PPAT_WT);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(3), GEN8_PPAT_UC);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(4), GEN8_PPAT_WB);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(5), GEN8_PPAT_WB);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(6), GEN8_PPAT_WB);\n\tintel_gt_mcr_multicast_write_fw(gt, XEHP_PAT_INDEX(7), GEN8_PPAT_WB);\n\tintel_gt_mcr_unlock(gt, flags);\n\n\tintel_uncore_forcewake_put(gt->uncore, fw);\n}\n\nstatic void icl_setup_private_ppat(struct intel_uncore *uncore)\n{\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(0),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_LLC);\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(1),\n\t\t\t   GEN8_PPAT_WC | GEN8_PPAT_LLCELLC);\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(2),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_ELLC_OVERRIDE);\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(3),\n\t\t\t   GEN8_PPAT_UC);\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(4),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(0));\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(5),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(1));\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(6),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(2));\n\tintel_uncore_write(uncore,\n\t\t\t   GEN10_PAT_INDEX(7),\n\t\t\t   GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(3));\n}\n\n \nstatic void bdw_setup_private_ppat(struct intel_uncore *uncore)\n{\n\tstruct drm_i915_private *i915 = uncore->i915;\n\tu64 pat;\n\n\tpat = GEN8_PPAT(0, GEN8_PPAT_WB | GEN8_PPAT_LLC) |\t \n\t      GEN8_PPAT(1, GEN8_PPAT_WC | GEN8_PPAT_LLCELLC) |\t \n\t      GEN8_PPAT(3, GEN8_PPAT_UC) |\t\t\t \n\t      GEN8_PPAT(4, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(0)) |\n\t      GEN8_PPAT(5, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(1)) |\n\t      GEN8_PPAT(6, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(2)) |\n\t      GEN8_PPAT(7, GEN8_PPAT_WB | GEN8_PPAT_LLCELLC | GEN8_PPAT_AGE(3));\n\n\t \n\tif (GRAPHICS_VER(i915) >= 9)\n\t\tpat |= GEN8_PPAT(2, GEN8_PPAT_WB | GEN8_PPAT_ELLC_OVERRIDE);\n\telse\n\t\tpat |= GEN8_PPAT(2, GEN8_PPAT_WT | GEN8_PPAT_LLCELLC);\n\n\tintel_uncore_write(uncore, GEN8_PRIVATE_PAT_LO, lower_32_bits(pat));\n\tintel_uncore_write(uncore, GEN8_PRIVATE_PAT_HI, upper_32_bits(pat));\n}\n\nstatic void chv_setup_private_ppat(struct intel_uncore *uncore)\n{\n\tu64 pat;\n\n\t \n\n\tpat = GEN8_PPAT(0, CHV_PPAT_SNOOP) |\n\t      GEN8_PPAT(1, 0) |\n\t      GEN8_PPAT(2, 0) |\n\t      GEN8_PPAT(3, 0) |\n\t      GEN8_PPAT(4, CHV_PPAT_SNOOP) |\n\t      GEN8_PPAT(5, CHV_PPAT_SNOOP) |\n\t      GEN8_PPAT(6, CHV_PPAT_SNOOP) |\n\t      GEN8_PPAT(7, CHV_PPAT_SNOOP);\n\n\tintel_uncore_write(uncore, GEN8_PRIVATE_PAT_LO, lower_32_bits(pat));\n\tintel_uncore_write(uncore, GEN8_PRIVATE_PAT_HI, upper_32_bits(pat));\n}\n\nvoid setup_private_pat(struct intel_gt *gt)\n{\n\tstruct intel_uncore *uncore = gt->uncore;\n\tstruct drm_i915_private *i915 = gt->i915;\n\n\tGEM_BUG_ON(GRAPHICS_VER(i915) < 8);\n\n\tif (gt->type == GT_MEDIA) {\n\t\txelpmp_setup_private_ppat(gt->uncore);\n\t\treturn;\n\t}\n\n\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 70))\n\t\txelpg_setup_private_ppat(gt);\n\telse if (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50))\n\t\txehp_setup_private_ppat(gt);\n\telse if (GRAPHICS_VER(i915) >= 12)\n\t\ttgl_setup_private_ppat(uncore);\n\telse if (GRAPHICS_VER(i915) >= 11)\n\t\ticl_setup_private_ppat(uncore);\n\telse if (IS_CHERRYVIEW(i915) || IS_GEN9_LP(i915))\n\t\tchv_setup_private_ppat(uncore);\n\telse\n\t\tbdw_setup_private_ppat(uncore);\n}\n\nstruct i915_vma *\n__vm_create_scratch_for_read(struct i915_address_space *vm, unsigned long size)\n{\n\tstruct drm_i915_gem_object *obj;\n\tstruct i915_vma *vma;\n\n\tobj = i915_gem_object_create_internal(vm->i915, PAGE_ALIGN(size));\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\ti915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);\n\n\tvma = i915_vma_instance(obj, vm, NULL);\n\tif (IS_ERR(vma)) {\n\t\ti915_gem_object_put(obj);\n\t\treturn vma;\n\t}\n\n\treturn vma;\n}\n\nstruct i915_vma *\n__vm_create_scratch_for_read_pinned(struct i915_address_space *vm, unsigned long size)\n{\n\tstruct i915_vma *vma;\n\tint err;\n\n\tvma = __vm_create_scratch_for_read(vm, size);\n\tif (IS_ERR(vma))\n\t\treturn vma;\n\n\terr = i915_vma_pin(vma, 0, 0,\n\t\t\t   i915_vma_is_ggtt(vma) ? PIN_GLOBAL : PIN_USER);\n\tif (err) {\n\t\ti915_vma_put(vma);\n\t\treturn ERR_PTR(err);\n\t}\n\n\treturn vma;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/mock_gtt.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}