{
  "module_name": "i915_gpu_error.c",
  "hash_id": "40df056e0b851e9721c53f07ddcb3019d2d0a453c5a3a5e52a7cb66dce4885b9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/i915_gpu_error.c",
  "human_readable_source": " \n\n#include <linux/ascii85.h>\n#include <linux/highmem.h>\n#include <linux/nmi.h>\n#include <linux/pagevec.h>\n#include <linux/scatterlist.h>\n#include <linux/string_helpers.h>\n#include <linux/utsname.h>\n#include <linux/zlib.h>\n\n#include <drm/drm_cache.h>\n#include <drm/drm_print.h>\n\n#include \"display/intel_dmc.h\"\n#include \"display/intel_overlay.h\"\n\n#include \"gem/i915_gem_context.h\"\n#include \"gem/i915_gem_lmem.h\"\n#include \"gt/intel_engine_regs.h\"\n#include \"gt/intel_gt.h\"\n#include \"gt/intel_gt_mcr.h\"\n#include \"gt/intel_gt_pm.h\"\n#include \"gt/intel_gt_regs.h\"\n#include \"gt/uc/intel_guc_capture.h\"\n\n#include \"i915_driver.h\"\n#include \"i915_drv.h\"\n#include \"i915_gpu_error.h\"\n#include \"i915_memcpy.h\"\n#include \"i915_reg.h\"\n#include \"i915_scatterlist.h\"\n#include \"i915_utils.h\"\n\n#define ALLOW_FAIL (__GFP_KSWAPD_RECLAIM | __GFP_RETRY_MAYFAIL | __GFP_NOWARN)\n#define ATOMIC_MAYFAIL (GFP_ATOMIC | __GFP_NOWARN)\n\nstatic void __sg_set_buf(struct scatterlist *sg,\n\t\t\t void *addr, unsigned int len, loff_t it)\n{\n\tsg->page_link = (unsigned long)virt_to_page(addr);\n\tsg->offset = offset_in_page(addr);\n\tsg->length = len;\n\tsg->dma_address = it;\n}\n\nstatic bool __i915_error_grow(struct drm_i915_error_state_buf *e, size_t len)\n{\n\tif (!len)\n\t\treturn false;\n\n\tif (e->bytes + len + 1 <= e->size)\n\t\treturn true;\n\n\tif (e->bytes) {\n\t\t__sg_set_buf(e->cur++, e->buf, e->bytes, e->iter);\n\t\te->iter += e->bytes;\n\t\te->buf = NULL;\n\t\te->bytes = 0;\n\t}\n\n\tif (e->cur == e->end) {\n\t\tstruct scatterlist *sgl;\n\n\t\tsgl = (typeof(sgl))__get_free_page(ALLOW_FAIL);\n\t\tif (!sgl) {\n\t\t\te->err = -ENOMEM;\n\t\t\treturn false;\n\t\t}\n\n\t\tif (e->cur) {\n\t\t\te->cur->offset = 0;\n\t\t\te->cur->length = 0;\n\t\t\te->cur->page_link =\n\t\t\t\t(unsigned long)sgl | SG_CHAIN;\n\t\t} else {\n\t\t\te->sgl = sgl;\n\t\t}\n\n\t\te->cur = sgl;\n\t\te->end = sgl + SG_MAX_SINGLE_ALLOC - 1;\n\t}\n\n\te->size = ALIGN(len + 1, SZ_64K);\n\te->buf = kmalloc(e->size, ALLOW_FAIL);\n\tif (!e->buf) {\n\t\te->size = PAGE_ALIGN(len + 1);\n\t\te->buf = kmalloc(e->size, GFP_KERNEL);\n\t}\n\tif (!e->buf) {\n\t\te->err = -ENOMEM;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n__printf(2, 0)\nstatic void i915_error_vprintf(struct drm_i915_error_state_buf *e,\n\t\t\t       const char *fmt, va_list args)\n{\n\tva_list ap;\n\tint len;\n\n\tif (e->err)\n\t\treturn;\n\n\tva_copy(ap, args);\n\tlen = vsnprintf(NULL, 0, fmt, ap);\n\tva_end(ap);\n\tif (len <= 0) {\n\t\te->err = len;\n\t\treturn;\n\t}\n\n\tif (!__i915_error_grow(e, len))\n\t\treturn;\n\n\tGEM_BUG_ON(e->bytes >= e->size);\n\tlen = vscnprintf(e->buf + e->bytes, e->size - e->bytes, fmt, args);\n\tif (len < 0) {\n\t\te->err = len;\n\t\treturn;\n\t}\n\te->bytes += len;\n}\n\nstatic void i915_error_puts(struct drm_i915_error_state_buf *e, const char *str)\n{\n\tunsigned len;\n\n\tif (e->err || !str)\n\t\treturn;\n\n\tlen = strlen(str);\n\tif (!__i915_error_grow(e, len))\n\t\treturn;\n\n\tGEM_BUG_ON(e->bytes + len > e->size);\n\tmemcpy(e->buf + e->bytes, str, len);\n\te->bytes += len;\n}\n\n#define err_printf(e, ...) i915_error_printf(e, __VA_ARGS__)\n#define err_puts(e, s) i915_error_puts(e, s)\n\nstatic void __i915_printfn_error(struct drm_printer *p, struct va_format *vaf)\n{\n\ti915_error_vprintf(p->arg, vaf->fmt, *vaf->va);\n}\n\nstatic inline struct drm_printer\ni915_error_printer(struct drm_i915_error_state_buf *e)\n{\n\tstruct drm_printer p = {\n\t\t.printfn = __i915_printfn_error,\n\t\t.arg = e,\n\t};\n\treturn p;\n}\n\n \nstatic void pool_fini(struct folio_batch *fbatch)\n{\n\tfolio_batch_release(fbatch);\n}\n\nstatic int pool_refill(struct folio_batch *fbatch, gfp_t gfp)\n{\n\twhile (folio_batch_space(fbatch)) {\n\t\tstruct folio *folio;\n\n\t\tfolio = folio_alloc(gfp, 0);\n\t\tif (!folio)\n\t\t\treturn -ENOMEM;\n\n\t\tfolio_batch_add(fbatch, folio);\n\t}\n\n\treturn 0;\n}\n\nstatic int pool_init(struct folio_batch *fbatch, gfp_t gfp)\n{\n\tint err;\n\n\tfolio_batch_init(fbatch);\n\n\terr = pool_refill(fbatch, gfp);\n\tif (err)\n\t\tpool_fini(fbatch);\n\n\treturn err;\n}\n\nstatic void *pool_alloc(struct folio_batch *fbatch, gfp_t gfp)\n{\n\tstruct folio *folio;\n\n\tfolio = folio_alloc(gfp, 0);\n\tif (!folio && folio_batch_count(fbatch))\n\t\tfolio = fbatch->folios[--fbatch->nr];\n\n\treturn folio ? folio_address(folio) : NULL;\n}\n\nstatic void pool_free(struct folio_batch *fbatch, void *addr)\n{\n\tstruct folio *folio = virt_to_folio(addr);\n\n\tif (folio_batch_space(fbatch))\n\t\tfolio_batch_add(fbatch, folio);\n\telse\n\t\tfolio_put(folio);\n}\n\n#ifdef CONFIG_DRM_I915_COMPRESS_ERROR\n\nstruct i915_vma_compress {\n\tstruct folio_batch pool;\n\tstruct z_stream_s zstream;\n\tvoid *tmp;\n};\n\nstatic bool compress_init(struct i915_vma_compress *c)\n{\n\tstruct z_stream_s *zstream = &c->zstream;\n\n\tif (pool_init(&c->pool, ALLOW_FAIL))\n\t\treturn false;\n\n\tzstream->workspace =\n\t\tkmalloc(zlib_deflate_workspacesize(MAX_WBITS, MAX_MEM_LEVEL),\n\t\t\tALLOW_FAIL);\n\tif (!zstream->workspace) {\n\t\tpool_fini(&c->pool);\n\t\treturn false;\n\t}\n\n\tc->tmp = NULL;\n\tif (i915_has_memcpy_from_wc())\n\t\tc->tmp = pool_alloc(&c->pool, ALLOW_FAIL);\n\n\treturn true;\n}\n\nstatic bool compress_start(struct i915_vma_compress *c)\n{\n\tstruct z_stream_s *zstream = &c->zstream;\n\tvoid *workspace = zstream->workspace;\n\n\tmemset(zstream, 0, sizeof(*zstream));\n\tzstream->workspace = workspace;\n\n\treturn zlib_deflateInit(zstream, Z_DEFAULT_COMPRESSION) == Z_OK;\n}\n\nstatic void *compress_next_page(struct i915_vma_compress *c,\n\t\t\t\tstruct i915_vma_coredump *dst)\n{\n\tvoid *page_addr;\n\tstruct page *page;\n\n\tpage_addr = pool_alloc(&c->pool, ALLOW_FAIL);\n\tif (!page_addr)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpage = virt_to_page(page_addr);\n\tlist_add_tail(&page->lru, &dst->page_list);\n\treturn page_addr;\n}\n\nstatic int compress_page(struct i915_vma_compress *c,\n\t\t\t void *src,\n\t\t\t struct i915_vma_coredump *dst,\n\t\t\t bool wc)\n{\n\tstruct z_stream_s *zstream = &c->zstream;\n\n\tzstream->next_in = src;\n\tif (wc && c->tmp && i915_memcpy_from_wc(c->tmp, src, PAGE_SIZE))\n\t\tzstream->next_in = c->tmp;\n\tzstream->avail_in = PAGE_SIZE;\n\n\tdo {\n\t\tif (zstream->avail_out == 0) {\n\t\t\tzstream->next_out = compress_next_page(c, dst);\n\t\t\tif (IS_ERR(zstream->next_out))\n\t\t\t\treturn PTR_ERR(zstream->next_out);\n\n\t\t\tzstream->avail_out = PAGE_SIZE;\n\t\t}\n\n\t\tif (zlib_deflate(zstream, Z_NO_FLUSH) != Z_OK)\n\t\t\treturn -EIO;\n\n\t\tcond_resched();\n\t} while (zstream->avail_in);\n\n\t \n\tif (0 && zstream->total_out > zstream->total_in)\n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nstatic int compress_flush(struct i915_vma_compress *c,\n\t\t\t  struct i915_vma_coredump *dst)\n{\n\tstruct z_stream_s *zstream = &c->zstream;\n\n\tdo {\n\t\tswitch (zlib_deflate(zstream, Z_FINISH)) {\n\t\tcase Z_OK:  \n\t\t\tzstream->next_out = compress_next_page(c, dst);\n\t\t\tif (IS_ERR(zstream->next_out))\n\t\t\t\treturn PTR_ERR(zstream->next_out);\n\n\t\t\tzstream->avail_out = PAGE_SIZE;\n\t\t\tbreak;\n\n\t\tcase Z_STREAM_END:\n\t\t\tgoto end;\n\n\t\tdefault:  \n\t\t\treturn -EIO;\n\t\t}\n\t} while (1);\n\nend:\n\tmemset(zstream->next_out, 0, zstream->avail_out);\n\tdst->unused = zstream->avail_out;\n\treturn 0;\n}\n\nstatic void compress_finish(struct i915_vma_compress *c)\n{\n\tzlib_deflateEnd(&c->zstream);\n}\n\nstatic void compress_fini(struct i915_vma_compress *c)\n{\n\tkfree(c->zstream.workspace);\n\tif (c->tmp)\n\t\tpool_free(&c->pool, c->tmp);\n\tpool_fini(&c->pool);\n}\n\nstatic void err_compression_marker(struct drm_i915_error_state_buf *m)\n{\n\terr_puts(m, \":\");\n}\n\n#else\n\nstruct i915_vma_compress {\n\tstruct folio_batch pool;\n};\n\nstatic bool compress_init(struct i915_vma_compress *c)\n{\n\treturn pool_init(&c->pool, ALLOW_FAIL) == 0;\n}\n\nstatic bool compress_start(struct i915_vma_compress *c)\n{\n\treturn true;\n}\n\nstatic int compress_page(struct i915_vma_compress *c,\n\t\t\t void *src,\n\t\t\t struct i915_vma_coredump *dst,\n\t\t\t bool wc)\n{\n\tvoid *ptr;\n\n\tptr = pool_alloc(&c->pool, ALLOW_FAIL);\n\tif (!ptr)\n\t\treturn -ENOMEM;\n\n\tif (!(wc && i915_memcpy_from_wc(ptr, src, PAGE_SIZE)))\n\t\tmemcpy(ptr, src, PAGE_SIZE);\n\tlist_add_tail(&virt_to_page(ptr)->lru, &dst->page_list);\n\tcond_resched();\n\n\treturn 0;\n}\n\nstatic int compress_flush(struct i915_vma_compress *c,\n\t\t\t  struct i915_vma_coredump *dst)\n{\n\treturn 0;\n}\n\nstatic void compress_finish(struct i915_vma_compress *c)\n{\n}\n\nstatic void compress_fini(struct i915_vma_compress *c)\n{\n\tpool_fini(&c->pool);\n}\n\nstatic void err_compression_marker(struct drm_i915_error_state_buf *m)\n{\n\terr_puts(m, \"~\");\n}\n\n#endif\n\nstatic void error_print_instdone(struct drm_i915_error_state_buf *m,\n\t\t\t\t const struct intel_engine_coredump *ee)\n{\n\tint slice;\n\tint subslice;\n\tint iter;\n\n\terr_printf(m, \"  INSTDONE: 0x%08x\\n\",\n\t\t   ee->instdone.instdone);\n\n\tif (ee->engine->class != RENDER_CLASS || GRAPHICS_VER(m->i915) <= 3)\n\t\treturn;\n\n\terr_printf(m, \"  SC_INSTDONE: 0x%08x\\n\",\n\t\t   ee->instdone.slice_common);\n\n\tif (GRAPHICS_VER(m->i915) <= 6)\n\t\treturn;\n\n\tfor_each_ss_steering(iter, ee->engine->gt, slice, subslice)\n\t\terr_printf(m, \"  SAMPLER_INSTDONE[%d][%d]: 0x%08x\\n\",\n\t\t\t   slice, subslice,\n\t\t\t   ee->instdone.sampler[slice][subslice]);\n\n\tfor_each_ss_steering(iter, ee->engine->gt, slice, subslice)\n\t\terr_printf(m, \"  ROW_INSTDONE[%d][%d]: 0x%08x\\n\",\n\t\t\t   slice, subslice,\n\t\t\t   ee->instdone.row[slice][subslice]);\n\n\tif (GRAPHICS_VER(m->i915) < 12)\n\t\treturn;\n\n\tif (GRAPHICS_VER_FULL(m->i915) >= IP_VER(12, 55)) {\n\t\tfor_each_ss_steering(iter, ee->engine->gt, slice, subslice)\n\t\t\terr_printf(m, \"  GEOM_SVGUNIT_INSTDONE[%d][%d]: 0x%08x\\n\",\n\t\t\t\t   slice, subslice,\n\t\t\t\t   ee->instdone.geom_svg[slice][subslice]);\n\t}\n\n\terr_printf(m, \"  SC_INSTDONE_EXTRA: 0x%08x\\n\",\n\t\t   ee->instdone.slice_common_extra[0]);\n\terr_printf(m, \"  SC_INSTDONE_EXTRA2: 0x%08x\\n\",\n\t\t   ee->instdone.slice_common_extra[1]);\n}\n\nstatic void error_print_request(struct drm_i915_error_state_buf *m,\n\t\t\t\tconst char *prefix,\n\t\t\t\tconst struct i915_request_coredump *erq)\n{\n\tif (!erq->seqno)\n\t\treturn;\n\n\terr_printf(m, \"%s pid %d, seqno %8x:%08x%s%s, prio %d, head %08x, tail %08x\\n\",\n\t\t   prefix, erq->pid, erq->context, erq->seqno,\n\t\t   test_bit(DMA_FENCE_FLAG_SIGNALED_BIT,\n\t\t\t    &erq->flags) ? \"!\" : \"\",\n\t\t   test_bit(DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT,\n\t\t\t    &erq->flags) ? \"+\" : \"\",\n\t\t   erq->sched_attr.priority,\n\t\t   erq->head, erq->tail);\n}\n\nstatic void error_print_context(struct drm_i915_error_state_buf *m,\n\t\t\t\tconst char *header,\n\t\t\t\tconst struct i915_gem_context_coredump *ctx)\n{\n\terr_printf(m, \"%s%s[%d] prio %d, guilty %d active %d, runtime total %lluns, avg %lluns\\n\",\n\t\t   header, ctx->comm, ctx->pid, ctx->sched_attr.priority,\n\t\t   ctx->guilty, ctx->active,\n\t\t   ctx->total_runtime, ctx->avg_runtime);\n\terr_printf(m, \"  context timeline seqno %u\\n\", ctx->hwsp_seqno);\n}\n\nstatic struct i915_vma_coredump *\n__find_vma(struct i915_vma_coredump *vma, const char *name)\n{\n\twhile (vma) {\n\t\tif (strcmp(vma->name, name) == 0)\n\t\t\treturn vma;\n\t\tvma = vma->next;\n\t}\n\n\treturn NULL;\n}\n\nstruct i915_vma_coredump *\nintel_gpu_error_find_batch(const struct intel_engine_coredump *ee)\n{\n\treturn __find_vma(ee->vma, \"batch\");\n}\n\nstatic void error_print_engine(struct drm_i915_error_state_buf *m,\n\t\t\t       const struct intel_engine_coredump *ee)\n{\n\tstruct i915_vma_coredump *batch;\n\tint n;\n\n\terr_printf(m, \"%s command stream:\\n\", ee->engine->name);\n\terr_printf(m, \"  CCID:  0x%08x\\n\", ee->ccid);\n\terr_printf(m, \"  START: 0x%08x\\n\", ee->start);\n\terr_printf(m, \"  HEAD:  0x%08x [0x%08x]\\n\", ee->head, ee->rq_head);\n\terr_printf(m, \"  TAIL:  0x%08x [0x%08x, 0x%08x]\\n\",\n\t\t   ee->tail, ee->rq_post, ee->rq_tail);\n\terr_printf(m, \"  CTL:   0x%08x\\n\", ee->ctl);\n\terr_printf(m, \"  MODE:  0x%08x\\n\", ee->mode);\n\terr_printf(m, \"  HWS:   0x%08x\\n\", ee->hws);\n\terr_printf(m, \"  ACTHD: 0x%08x %08x\\n\",\n\t\t   (u32)(ee->acthd>>32), (u32)ee->acthd);\n\terr_printf(m, \"  IPEIR: 0x%08x\\n\", ee->ipeir);\n\terr_printf(m, \"  IPEHR: 0x%08x\\n\", ee->ipehr);\n\terr_printf(m, \"  ESR:   0x%08x\\n\", ee->esr);\n\n\terror_print_instdone(m, ee);\n\n\tbatch = intel_gpu_error_find_batch(ee);\n\tif (batch) {\n\t\tu64 start = batch->gtt_offset;\n\t\tu64 end = start + batch->gtt_size;\n\n\t\terr_printf(m, \"  batch: [0x%08x_%08x, 0x%08x_%08x]\\n\",\n\t\t\t   upper_32_bits(start), lower_32_bits(start),\n\t\t\t   upper_32_bits(end), lower_32_bits(end));\n\t}\n\tif (GRAPHICS_VER(m->i915) >= 4) {\n\t\terr_printf(m, \"  BBADDR: 0x%08x_%08x\\n\",\n\t\t\t   (u32)(ee->bbaddr>>32), (u32)ee->bbaddr);\n\t\terr_printf(m, \"  BB_STATE: 0x%08x\\n\", ee->bbstate);\n\t\terr_printf(m, \"  INSTPS: 0x%08x\\n\", ee->instps);\n\t}\n\terr_printf(m, \"  INSTPM: 0x%08x\\n\", ee->instpm);\n\terr_printf(m, \"  FADDR: 0x%08x %08x\\n\", upper_32_bits(ee->faddr),\n\t\t   lower_32_bits(ee->faddr));\n\tif (GRAPHICS_VER(m->i915) >= 6) {\n\t\terr_printf(m, \"  RC PSMI: 0x%08x\\n\", ee->rc_psmi);\n\t\terr_printf(m, \"  FAULT_REG: 0x%08x\\n\", ee->fault_reg);\n\t}\n\tif (GRAPHICS_VER(m->i915) >= 11) {\n\t\terr_printf(m, \"  NOPID: 0x%08x\\n\", ee->nopid);\n\t\terr_printf(m, \"  EXCC: 0x%08x\\n\", ee->excc);\n\t\terr_printf(m, \"  CMD_CCTL: 0x%08x\\n\", ee->cmd_cctl);\n\t\terr_printf(m, \"  CSCMDOP: 0x%08x\\n\", ee->cscmdop);\n\t\terr_printf(m, \"  CTX_SR_CTL: 0x%08x\\n\", ee->ctx_sr_ctl);\n\t\terr_printf(m, \"  DMA_FADDR_HI: 0x%08x\\n\", ee->dma_faddr_hi);\n\t\terr_printf(m, \"  DMA_FADDR_LO: 0x%08x\\n\", ee->dma_faddr_lo);\n\t}\n\tif (HAS_PPGTT(m->i915)) {\n\t\terr_printf(m, \"  GFX_MODE: 0x%08x\\n\", ee->vm_info.gfx_mode);\n\n\t\tif (GRAPHICS_VER(m->i915) >= 8) {\n\t\t\tint i;\n\t\t\tfor (i = 0; i < 4; i++)\n\t\t\t\terr_printf(m, \"  PDP%d: 0x%016llx\\n\",\n\t\t\t\t\t   i, ee->vm_info.pdp[i]);\n\t\t} else {\n\t\t\terr_printf(m, \"  PP_DIR_BASE: 0x%08x\\n\",\n\t\t\t\t   ee->vm_info.pp_dir_base);\n\t\t}\n\t}\n\n\tfor (n = 0; n < ee->num_ports; n++) {\n\t\terr_printf(m, \"  ELSP[%d]:\", n);\n\t\terror_print_request(m, \" \", &ee->execlist[n]);\n\t}\n}\n\nvoid i915_error_printf(struct drm_i915_error_state_buf *e, const char *f, ...)\n{\n\tva_list args;\n\n\tva_start(args, f);\n\ti915_error_vprintf(e, f, args);\n\tva_end(args);\n}\n\nvoid intel_gpu_error_print_vma(struct drm_i915_error_state_buf *m,\n\t\t\t       const struct intel_engine_cs *engine,\n\t\t\t       const struct i915_vma_coredump *vma)\n{\n\tchar out[ASCII85_BUFSZ];\n\tstruct page *page;\n\n\tif (!vma)\n\t\treturn;\n\n\terr_printf(m, \"%s --- %s = 0x%08x %08x\\n\",\n\t\t   engine ? engine->name : \"global\", vma->name,\n\t\t   upper_32_bits(vma->gtt_offset),\n\t\t   lower_32_bits(vma->gtt_offset));\n\n\tif (vma->gtt_page_sizes > I915_GTT_PAGE_SIZE_4K)\n\t\terr_printf(m, \"gtt_page_sizes = 0x%08x\\n\", vma->gtt_page_sizes);\n\n\terr_compression_marker(m);\n\tlist_for_each_entry(page, &vma->page_list, lru) {\n\t\tint i, len;\n\t\tconst u32 *addr = page_address(page);\n\n\t\tlen = PAGE_SIZE;\n\t\tif (page == list_last_entry(&vma->page_list, typeof(*page), lru))\n\t\t\tlen -= vma->unused;\n\t\tlen = ascii85_encode_len(len);\n\n\t\tfor (i = 0; i < len; i++)\n\t\t\terr_puts(m, ascii85_encode(addr[i], out));\n\t}\n\terr_puts(m, \"\\n\");\n}\n\nstatic void err_print_capabilities(struct drm_i915_error_state_buf *m,\n\t\t\t\t   struct i915_gpu_coredump *error)\n{\n\tstruct drm_printer p = i915_error_printer(m);\n\n\tintel_device_info_print(&error->device_info, &error->runtime_info, &p);\n\tintel_display_device_info_print(&error->display_device_info,\n\t\t\t\t\t&error->display_runtime_info, &p);\n\tintel_driver_caps_print(&error->driver_caps, &p);\n}\n\nstatic void err_print_params(struct drm_i915_error_state_buf *m,\n\t\t\t     const struct i915_params *params)\n{\n\tstruct drm_printer p = i915_error_printer(m);\n\n\ti915_params_dump(params, &p);\n}\n\nstatic void err_print_pciid(struct drm_i915_error_state_buf *m,\n\t\t\t    struct drm_i915_private *i915)\n{\n\tstruct pci_dev *pdev = to_pci_dev(i915->drm.dev);\n\n\terr_printf(m, \"PCI ID: 0x%04x\\n\", pdev->device);\n\terr_printf(m, \"PCI Revision: 0x%02x\\n\", pdev->revision);\n\terr_printf(m, \"PCI Subsystem: %04x:%04x\\n\",\n\t\t   pdev->subsystem_vendor,\n\t\t   pdev->subsystem_device);\n}\n\nstatic void err_print_guc_ctb(struct drm_i915_error_state_buf *m,\n\t\t\t      const char *name,\n\t\t\t      const struct intel_ctb_coredump *ctb)\n{\n\tif (!ctb->size)\n\t\treturn;\n\n\terr_printf(m, \"GuC %s CTB: raw: 0x%08X, 0x%08X/%08X, cached: 0x%08X/%08X, desc = 0x%08X, buf = 0x%08X x 0x%08X\\n\",\n\t\t   name, ctb->raw_status, ctb->raw_head, ctb->raw_tail,\n\t\t   ctb->head, ctb->tail, ctb->desc_offset, ctb->cmds_offset, ctb->size);\n}\n\nstatic void err_print_uc(struct drm_i915_error_state_buf *m,\n\t\t\t const struct intel_uc_coredump *error_uc)\n{\n\tstruct drm_printer p = i915_error_printer(m);\n\n\tintel_uc_fw_dump(&error_uc->guc_fw, &p);\n\tintel_uc_fw_dump(&error_uc->huc_fw, &p);\n\terr_printf(m, \"GuC timestamp: 0x%08x\\n\", error_uc->guc.timestamp);\n\tintel_gpu_error_print_vma(m, NULL, error_uc->guc.vma_log);\n\terr_printf(m, \"GuC CTB fence: %d\\n\", error_uc->guc.last_fence);\n\terr_print_guc_ctb(m, \"Send\", error_uc->guc.ctb + 0);\n\terr_print_guc_ctb(m, \"Recv\", error_uc->guc.ctb + 1);\n\tintel_gpu_error_print_vma(m, NULL, error_uc->guc.vma_ctb);\n}\n\nstatic void err_free_sgl(struct scatterlist *sgl)\n{\n\twhile (sgl) {\n\t\tstruct scatterlist *sg;\n\n\t\tfor (sg = sgl; !sg_is_chain(sg); sg++) {\n\t\t\tkfree(sg_virt(sg));\n\t\t\tif (sg_is_last(sg))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tsg = sg_is_last(sg) ? NULL : sg_chain_ptr(sg);\n\t\tfree_page((unsigned long)sgl);\n\t\tsgl = sg;\n\t}\n}\n\nstatic void err_print_gt_info(struct drm_i915_error_state_buf *m,\n\t\t\t      struct intel_gt_coredump *gt)\n{\n\tstruct drm_printer p = i915_error_printer(m);\n\n\tintel_gt_info_print(&gt->info, &p);\n\tintel_sseu_print_topology(gt->_gt->i915, &gt->info.sseu, &p);\n}\n\nstatic void err_print_gt_display(struct drm_i915_error_state_buf *m,\n\t\t\t\t struct intel_gt_coredump *gt)\n{\n\terr_printf(m, \"IER: 0x%08x\\n\", gt->ier);\n\terr_printf(m, \"DERRMR: 0x%08x\\n\", gt->derrmr);\n}\n\nstatic void err_print_gt_global_nonguc(struct drm_i915_error_state_buf *m,\n\t\t\t\t       struct intel_gt_coredump *gt)\n{\n\tint i;\n\n\terr_printf(m, \"GT awake: %s\\n\", str_yes_no(gt->awake));\n\terr_printf(m, \"CS timestamp frequency: %u Hz, %d ns\\n\",\n\t\t   gt->clock_frequency, gt->clock_period_ns);\n\terr_printf(m, \"EIR: 0x%08x\\n\", gt->eir);\n\terr_printf(m, \"PGTBL_ER: 0x%08x\\n\", gt->pgtbl_er);\n\n\tfor (i = 0; i < gt->ngtier; i++)\n\t\terr_printf(m, \"GTIER[%d]: 0x%08x\\n\", i, gt->gtier[i]);\n}\n\nstatic void err_print_gt_global(struct drm_i915_error_state_buf *m,\n\t\t\t\tstruct intel_gt_coredump *gt)\n{\n\terr_printf(m, \"FORCEWAKE: 0x%08x\\n\", gt->forcewake);\n\n\tif (IS_GRAPHICS_VER(m->i915, 6, 11)) {\n\t\terr_printf(m, \"ERROR: 0x%08x\\n\", gt->error);\n\t\terr_printf(m, \"DONE_REG: 0x%08x\\n\", gt->done_reg);\n\t}\n\n\tif (GRAPHICS_VER(m->i915) >= 8)\n\t\terr_printf(m, \"FAULT_TLB_DATA: 0x%08x 0x%08x\\n\",\n\t\t\t   gt->fault_data1, gt->fault_data0);\n\n\tif (GRAPHICS_VER(m->i915) == 7)\n\t\terr_printf(m, \"ERR_INT: 0x%08x\\n\", gt->err_int);\n\n\tif (IS_GRAPHICS_VER(m->i915, 8, 11))\n\t\terr_printf(m, \"GTT_CACHE_EN: 0x%08x\\n\", gt->gtt_cache);\n\n\tif (GRAPHICS_VER(m->i915) == 12)\n\t\terr_printf(m, \"AUX_ERR_DBG: 0x%08x\\n\", gt->aux_err);\n\n\tif (GRAPHICS_VER(m->i915) >= 12) {\n\t\tint i;\n\n\t\tfor (i = 0; i < I915_MAX_SFC; i++) {\n\t\t\t \n\t\t\tif ((gt->_gt->info.sfc_mask & BIT(i)) == 0 ||\n\t\t\t    !HAS_ENGINE(gt->_gt, _VCS(i * 2)))\n\t\t\t\tcontinue;\n\n\t\t\terr_printf(m, \"  SFC_DONE[%d]: 0x%08x\\n\", i,\n\t\t\t\t   gt->sfc_done[i]);\n\t\t}\n\n\t\terr_printf(m, \"  GAM_DONE: 0x%08x\\n\", gt->gam_done);\n\t}\n}\n\nstatic void err_print_gt_fences(struct drm_i915_error_state_buf *m,\n\t\t\t\tstruct intel_gt_coredump *gt)\n{\n\tint i;\n\n\tfor (i = 0; i < gt->nfence; i++)\n\t\terr_printf(m, \"  fence[%d] = %08llx\\n\", i, gt->fence[i]);\n}\n\nstatic void err_print_gt_engines(struct drm_i915_error_state_buf *m,\n\t\t\t\t struct intel_gt_coredump *gt)\n{\n\tconst struct intel_engine_coredump *ee;\n\n\tfor (ee = gt->engine; ee; ee = ee->next) {\n\t\tconst struct i915_vma_coredump *vma;\n\n\t\tif (gt->uc && gt->uc->guc.is_guc_capture) {\n\t\t\tif (ee->guc_capture_node)\n\t\t\t\tintel_guc_capture_print_engine_node(m, ee);\n\t\t\telse\n\t\t\t\terr_printf(m, \"  Missing GuC capture node for %s\\n\",\n\t\t\t\t\t   ee->engine->name);\n\t\t} else {\n\t\t\terror_print_engine(m, ee);\n\t\t}\n\n\t\terr_printf(m, \"  hung: %u\\n\", ee->hung);\n\t\terr_printf(m, \"  engine reset count: %u\\n\", ee->reset_count);\n\t\terror_print_context(m, \"  Active context: \", &ee->context);\n\n\t\tfor (vma = ee->vma; vma; vma = vma->next)\n\t\t\tintel_gpu_error_print_vma(m, ee->engine, vma);\n\t}\n\n}\n\nstatic void __err_print_to_sgl(struct drm_i915_error_state_buf *m,\n\t\t\t       struct i915_gpu_coredump *error)\n{\n\tconst struct intel_engine_coredump *ee;\n\tstruct timespec64 ts;\n\n\tif (*error->error_msg)\n\t\terr_printf(m, \"%s\\n\", error->error_msg);\n\terr_printf(m, \"Kernel: %s %s\\n\",\n\t\t   init_utsname()->release,\n\t\t   init_utsname()->machine);\n\terr_printf(m, \"Driver: %s\\n\", DRIVER_DATE);\n\tts = ktime_to_timespec64(error->time);\n\terr_printf(m, \"Time: %lld s %ld us\\n\",\n\t\t   (s64)ts.tv_sec, ts.tv_nsec / NSEC_PER_USEC);\n\tts = ktime_to_timespec64(error->boottime);\n\terr_printf(m, \"Boottime: %lld s %ld us\\n\",\n\t\t   (s64)ts.tv_sec, ts.tv_nsec / NSEC_PER_USEC);\n\tts = ktime_to_timespec64(error->uptime);\n\terr_printf(m, \"Uptime: %lld s %ld us\\n\",\n\t\t   (s64)ts.tv_sec, ts.tv_nsec / NSEC_PER_USEC);\n\terr_printf(m, \"Capture: %lu jiffies; %d ms ago\\n\",\n\t\t   error->capture, jiffies_to_msecs(jiffies - error->capture));\n\n\tfor (ee = error->gt ? error->gt->engine : NULL; ee; ee = ee->next)\n\t\terr_printf(m, \"Active process (on ring %s): %s [%d]\\n\",\n\t\t\t   ee->engine->name,\n\t\t\t   ee->context.comm,\n\t\t\t   ee->context.pid);\n\n\terr_printf(m, \"Reset count: %u\\n\", error->reset_count);\n\terr_printf(m, \"Suspend count: %u\\n\", error->suspend_count);\n\terr_printf(m, \"Platform: %s\\n\", intel_platform_name(error->device_info.platform));\n\terr_printf(m, \"Subplatform: 0x%x\\n\",\n\t\t   intel_subplatform(&error->runtime_info,\n\t\t\t\t     error->device_info.platform));\n\terr_print_pciid(m, m->i915);\n\n\terr_printf(m, \"IOMMU enabled?: %d\\n\", error->iommu);\n\n\tintel_dmc_print_error_state(m, m->i915);\n\n\terr_printf(m, \"RPM wakelock: %s\\n\", str_yes_no(error->wakelock));\n\terr_printf(m, \"PM suspended: %s\\n\", str_yes_no(error->suspended));\n\n\tif (error->gt) {\n\t\tbool print_guc_capture = false;\n\n\t\tif (error->gt->uc && error->gt->uc->guc.is_guc_capture)\n\t\t\tprint_guc_capture = true;\n\n\t\terr_print_gt_display(m, error->gt);\n\t\terr_print_gt_global_nonguc(m, error->gt);\n\t\terr_print_gt_fences(m, error->gt);\n\n\t\t \n\t\tif (!print_guc_capture)\n\t\t\terr_print_gt_global(m, error->gt);\n\n\t\terr_print_gt_engines(m, error->gt);\n\n\t\tif (error->gt->uc)\n\t\t\terr_print_uc(m, error->gt->uc);\n\n\t\terr_print_gt_info(m, error->gt);\n\t}\n\n\tif (error->overlay)\n\t\tintel_overlay_print_error_state(m, error->overlay);\n\n\terr_print_capabilities(m, error);\n\terr_print_params(m, &error->params);\n}\n\nstatic int err_print_to_sgl(struct i915_gpu_coredump *error)\n{\n\tstruct drm_i915_error_state_buf m;\n\n\tif (IS_ERR(error))\n\t\treturn PTR_ERR(error);\n\n\tif (READ_ONCE(error->sgl))\n\t\treturn 0;\n\n\tmemset(&m, 0, sizeof(m));\n\tm.i915 = error->i915;\n\n\t__err_print_to_sgl(&m, error);\n\n\tif (m.buf) {\n\t\t__sg_set_buf(m.cur++, m.buf, m.bytes, m.iter);\n\t\tm.bytes = 0;\n\t\tm.buf = NULL;\n\t}\n\tif (m.cur) {\n\t\tGEM_BUG_ON(m.end < m.cur);\n\t\tsg_mark_end(m.cur - 1);\n\t}\n\tGEM_BUG_ON(m.sgl && !m.cur);\n\n\tif (m.err) {\n\t\terr_free_sgl(m.sgl);\n\t\treturn m.err;\n\t}\n\n\tif (cmpxchg(&error->sgl, NULL, m.sgl))\n\t\terr_free_sgl(m.sgl);\n\n\treturn 0;\n}\n\nssize_t i915_gpu_coredump_copy_to_buffer(struct i915_gpu_coredump *error,\n\t\t\t\t\t char *buf, loff_t off, size_t rem)\n{\n\tstruct scatterlist *sg;\n\tsize_t count;\n\tloff_t pos;\n\tint err;\n\n\tif (!error || !rem)\n\t\treturn 0;\n\n\terr = err_print_to_sgl(error);\n\tif (err)\n\t\treturn err;\n\n\tsg = READ_ONCE(error->fit);\n\tif (!sg || off < sg->dma_address)\n\t\tsg = error->sgl;\n\tif (!sg)\n\t\treturn 0;\n\n\tpos = sg->dma_address;\n\tcount = 0;\n\tdo {\n\t\tsize_t len, start;\n\n\t\tif (sg_is_chain(sg)) {\n\t\t\tsg = sg_chain_ptr(sg);\n\t\t\tGEM_BUG_ON(sg_is_chain(sg));\n\t\t}\n\n\t\tlen = sg->length;\n\t\tif (pos + len <= off) {\n\t\t\tpos += len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tstart = sg->offset;\n\t\tif (pos < off) {\n\t\t\tGEM_BUG_ON(off - pos > len);\n\t\t\tlen -= off - pos;\n\t\t\tstart += off - pos;\n\t\t\tpos = off;\n\t\t}\n\n\t\tlen = min(len, rem);\n\t\tGEM_BUG_ON(!len || len > sg->length);\n\n\t\tmemcpy(buf, page_address(sg_page(sg)) + start, len);\n\n\t\tcount += len;\n\t\tpos += len;\n\n\t\tbuf += len;\n\t\trem -= len;\n\t\tif (!rem) {\n\t\t\tWRITE_ONCE(error->fit, sg);\n\t\t\tbreak;\n\t\t}\n\t} while (!sg_is_last(sg++));\n\n\treturn count;\n}\n\nstatic void i915_vma_coredump_free(struct i915_vma_coredump *vma)\n{\n\twhile (vma) {\n\t\tstruct i915_vma_coredump *next = vma->next;\n\t\tstruct page *page, *n;\n\n\t\tlist_for_each_entry_safe(page, n, &vma->page_list, lru) {\n\t\t\tlist_del_init(&page->lru);\n\t\t\t__free_page(page);\n\t\t}\n\n\t\tkfree(vma);\n\t\tvma = next;\n\t}\n}\n\nstatic void cleanup_params(struct i915_gpu_coredump *error)\n{\n\ti915_params_free(&error->params);\n}\n\nstatic void cleanup_uc(struct intel_uc_coredump *uc)\n{\n\tkfree(uc->guc_fw.file_selected.path);\n\tkfree(uc->huc_fw.file_selected.path);\n\tkfree(uc->guc_fw.file_wanted.path);\n\tkfree(uc->huc_fw.file_wanted.path);\n\ti915_vma_coredump_free(uc->guc.vma_log);\n\ti915_vma_coredump_free(uc->guc.vma_ctb);\n\n\tkfree(uc);\n}\n\nstatic void cleanup_gt(struct intel_gt_coredump *gt)\n{\n\twhile (gt->engine) {\n\t\tstruct intel_engine_coredump *ee = gt->engine;\n\n\t\tgt->engine = ee->next;\n\n\t\ti915_vma_coredump_free(ee->vma);\n\t\tintel_guc_capture_free_node(ee);\n\t\tkfree(ee);\n\t}\n\n\tif (gt->uc)\n\t\tcleanup_uc(gt->uc);\n\n\tkfree(gt);\n}\n\nvoid __i915_gpu_coredump_free(struct kref *error_ref)\n{\n\tstruct i915_gpu_coredump *error =\n\t\tcontainer_of(error_ref, typeof(*error), ref);\n\n\twhile (error->gt) {\n\t\tstruct intel_gt_coredump *gt = error->gt;\n\n\t\terror->gt = gt->next;\n\t\tcleanup_gt(gt);\n\t}\n\n\tkfree(error->overlay);\n\n\tcleanup_params(error);\n\n\terr_free_sgl(error->sgl);\n\tkfree(error);\n}\n\nstatic struct i915_vma_coredump *\ni915_vma_coredump_create(const struct intel_gt *gt,\n\t\t\t const struct i915_vma_resource *vma_res,\n\t\t\t struct i915_vma_compress *compress,\n\t\t\t const char *name)\n\n{\n\tstruct i915_ggtt *ggtt = gt->ggtt;\n\tconst u64 slot = ggtt->error_capture.start;\n\tstruct i915_vma_coredump *dst;\n\tstruct sgt_iter iter;\n\tint ret;\n\n\tmight_sleep();\n\n\tif (!vma_res || !vma_res->bi.pages || !compress)\n\t\treturn NULL;\n\n\tdst = kmalloc(sizeof(*dst), ALLOW_FAIL);\n\tif (!dst)\n\t\treturn NULL;\n\n\tif (!compress_start(compress)) {\n\t\tkfree(dst);\n\t\treturn NULL;\n\t}\n\n\tINIT_LIST_HEAD(&dst->page_list);\n\tstrcpy(dst->name, name);\n\tdst->next = NULL;\n\n\tdst->gtt_offset = vma_res->start;\n\tdst->gtt_size = vma_res->node_size;\n\tdst->gtt_page_sizes = vma_res->page_sizes_gtt;\n\tdst->unused = 0;\n\n\tret = -EINVAL;\n\tif (drm_mm_node_allocated(&ggtt->error_capture)) {\n\t\tvoid __iomem *s;\n\t\tdma_addr_t dma;\n\n\t\tfor_each_sgt_daddr(dma, iter, vma_res->bi.pages) {\n\t\t\tmutex_lock(&ggtt->error_mutex);\n\t\t\tif (ggtt->vm.raw_insert_page)\n\t\t\t\tggtt->vm.raw_insert_page(&ggtt->vm, dma, slot,\n\t\t\t\t\t\t\t i915_gem_get_pat_index(gt->i915,\n\t\t\t\t\t\t\t\t\t\tI915_CACHE_NONE),\n\t\t\t\t\t\t\t 0);\n\t\t\telse\n\t\t\t\tggtt->vm.insert_page(&ggtt->vm, dma, slot,\n\t\t\t\t\t\t     i915_gem_get_pat_index(gt->i915,\n\t\t\t\t\t\t\t\t\t    I915_CACHE_NONE),\n\t\t\t\t\t\t     0);\n\t\t\tmb();\n\n\t\t\ts = io_mapping_map_wc(&ggtt->iomap, slot, PAGE_SIZE);\n\t\t\tret = compress_page(compress,\n\t\t\t\t\t    (void  __force *)s, dst,\n\t\t\t\t\t    true);\n\t\t\tio_mapping_unmap(s);\n\n\t\t\tmb();\n\t\t\tggtt->vm.clear_range(&ggtt->vm, slot, PAGE_SIZE);\n\t\t\tmutex_unlock(&ggtt->error_mutex);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t} else if (vma_res->bi.lmem) {\n\t\tstruct intel_memory_region *mem = vma_res->mr;\n\t\tdma_addr_t dma;\n\n\t\tfor_each_sgt_daddr(dma, iter, vma_res->bi.pages) {\n\t\t\tdma_addr_t offset = dma - mem->region.start;\n\t\t\tvoid __iomem *s;\n\n\t\t\tif (offset + PAGE_SIZE > mem->io_size) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\ts = io_mapping_map_wc(&mem->iomap, offset, PAGE_SIZE);\n\t\t\tret = compress_page(compress,\n\t\t\t\t\t    (void __force *)s, dst,\n\t\t\t\t\t    true);\n\t\t\tio_mapping_unmap(s);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tstruct page *page;\n\n\t\tfor_each_sgt_page(page, iter, vma_res->bi.pages) {\n\t\t\tvoid *s;\n\n\t\t\tdrm_clflush_pages(&page, 1);\n\n\t\t\ts = kmap_local_page(page);\n\t\t\tret = compress_page(compress, s, dst, false);\n\t\t\tkunmap_local(s);\n\n\t\t\tdrm_clflush_pages(&page, 1);\n\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (ret || compress_flush(compress, dst)) {\n\t\tstruct page *page, *n;\n\n\t\tlist_for_each_entry_safe_reverse(page, n, &dst->page_list, lru) {\n\t\t\tlist_del_init(&page->lru);\n\t\t\tpool_free(&compress->pool, page_address(page));\n\t\t}\n\n\t\tkfree(dst);\n\t\tdst = NULL;\n\t}\n\tcompress_finish(compress);\n\n\treturn dst;\n}\n\nstatic void gt_record_fences(struct intel_gt_coredump *gt)\n{\n\tstruct i915_ggtt *ggtt = gt->_gt->ggtt;\n\tstruct intel_uncore *uncore = gt->_gt->uncore;\n\tint i;\n\n\tif (GRAPHICS_VER(uncore->i915) >= 6) {\n\t\tfor (i = 0; i < ggtt->num_fences; i++)\n\t\t\tgt->fence[i] =\n\t\t\t\tintel_uncore_read64(uncore,\n\t\t\t\t\t\t    FENCE_REG_GEN6_LO(i));\n\t} else if (GRAPHICS_VER(uncore->i915) >= 4) {\n\t\tfor (i = 0; i < ggtt->num_fences; i++)\n\t\t\tgt->fence[i] =\n\t\t\t\tintel_uncore_read64(uncore,\n\t\t\t\t\t\t    FENCE_REG_965_LO(i));\n\t} else {\n\t\tfor (i = 0; i < ggtt->num_fences; i++)\n\t\t\tgt->fence[i] =\n\t\t\t\tintel_uncore_read(uncore, FENCE_REG(i));\n\t}\n\tgt->nfence = i;\n}\n\nstatic void engine_record_registers(struct intel_engine_coredump *ee)\n{\n\tconst struct intel_engine_cs *engine = ee->engine;\n\tstruct drm_i915_private *i915 = engine->i915;\n\n\tif (GRAPHICS_VER(i915) >= 6) {\n\t\tee->rc_psmi = ENGINE_READ(engine, RING_PSMI_CTL);\n\n\t\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50))\n\t\t\tee->fault_reg = intel_gt_mcr_read_any(engine->gt,\n\t\t\t\t\t\t\t      XEHP_RING_FAULT_REG);\n\t\telse if (GRAPHICS_VER(i915) >= 12)\n\t\t\tee->fault_reg = intel_uncore_read(engine->uncore,\n\t\t\t\t\t\t\t  GEN12_RING_FAULT_REG);\n\t\telse if (GRAPHICS_VER(i915) >= 8)\n\t\t\tee->fault_reg = intel_uncore_read(engine->uncore,\n\t\t\t\t\t\t\t  GEN8_RING_FAULT_REG);\n\t\telse\n\t\t\tee->fault_reg = GEN6_RING_FAULT_REG_READ(engine);\n\t}\n\n\tif (GRAPHICS_VER(i915) >= 4) {\n\t\tee->esr = ENGINE_READ(engine, RING_ESR);\n\t\tee->faddr = ENGINE_READ(engine, RING_DMA_FADD);\n\t\tee->ipeir = ENGINE_READ(engine, RING_IPEIR);\n\t\tee->ipehr = ENGINE_READ(engine, RING_IPEHR);\n\t\tee->instps = ENGINE_READ(engine, RING_INSTPS);\n\t\tee->bbaddr = ENGINE_READ(engine, RING_BBADDR);\n\t\tee->ccid = ENGINE_READ(engine, CCID);\n\t\tif (GRAPHICS_VER(i915) >= 8) {\n\t\t\tee->faddr |= (u64)ENGINE_READ(engine, RING_DMA_FADD_UDW) << 32;\n\t\t\tee->bbaddr |= (u64)ENGINE_READ(engine, RING_BBADDR_UDW) << 32;\n\t\t}\n\t\tee->bbstate = ENGINE_READ(engine, RING_BBSTATE);\n\t} else {\n\t\tee->faddr = ENGINE_READ(engine, DMA_FADD_I8XX);\n\t\tee->ipeir = ENGINE_READ(engine, IPEIR);\n\t\tee->ipehr = ENGINE_READ(engine, IPEHR);\n\t}\n\n\tif (GRAPHICS_VER(i915) >= 11) {\n\t\tee->cmd_cctl = ENGINE_READ(engine, RING_CMD_CCTL);\n\t\tee->cscmdop = ENGINE_READ(engine, RING_CSCMDOP);\n\t\tee->ctx_sr_ctl = ENGINE_READ(engine, RING_CTX_SR_CTL);\n\t\tee->dma_faddr_hi = ENGINE_READ(engine, RING_DMA_FADD_UDW);\n\t\tee->dma_faddr_lo = ENGINE_READ(engine, RING_DMA_FADD);\n\t\tee->nopid = ENGINE_READ(engine, RING_NOPID);\n\t\tee->excc = ENGINE_READ(engine, RING_EXCC);\n\t}\n\n\tintel_engine_get_instdone(engine, &ee->instdone);\n\n\tee->instpm = ENGINE_READ(engine, RING_INSTPM);\n\tee->acthd = intel_engine_get_active_head(engine);\n\tee->start = ENGINE_READ(engine, RING_START);\n\tee->head = ENGINE_READ(engine, RING_HEAD);\n\tee->tail = ENGINE_READ(engine, RING_TAIL);\n\tee->ctl = ENGINE_READ(engine, RING_CTL);\n\tif (GRAPHICS_VER(i915) > 2)\n\t\tee->mode = ENGINE_READ(engine, RING_MI_MODE);\n\n\tif (!HWS_NEEDS_PHYSICAL(i915)) {\n\t\ti915_reg_t mmio;\n\n\t\tif (GRAPHICS_VER(i915) == 7) {\n\t\t\tswitch (engine->id) {\n\t\t\tdefault:\n\t\t\t\tMISSING_CASE(engine->id);\n\t\t\t\tfallthrough;\n\t\t\tcase RCS0:\n\t\t\t\tmmio = RENDER_HWS_PGA_GEN7;\n\t\t\t\tbreak;\n\t\t\tcase BCS0:\n\t\t\t\tmmio = BLT_HWS_PGA_GEN7;\n\t\t\t\tbreak;\n\t\t\tcase VCS0:\n\t\t\t\tmmio = BSD_HWS_PGA_GEN7;\n\t\t\t\tbreak;\n\t\t\tcase VECS0:\n\t\t\t\tmmio = VEBOX_HWS_PGA_GEN7;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (GRAPHICS_VER(engine->i915) == 6) {\n\t\t\tmmio = RING_HWS_PGA_GEN6(engine->mmio_base);\n\t\t} else {\n\t\t\t \n\t\t\tmmio = RING_HWS_PGA(engine->mmio_base);\n\t\t}\n\n\t\tee->hws = intel_uncore_read(engine->uncore, mmio);\n\t}\n\n\tee->reset_count = i915_reset_engine_count(&i915->gpu_error, engine);\n\n\tif (HAS_PPGTT(i915)) {\n\t\tint i;\n\n\t\tee->vm_info.gfx_mode = ENGINE_READ(engine, RING_MODE_GEN7);\n\n\t\tif (GRAPHICS_VER(i915) == 6) {\n\t\t\tee->vm_info.pp_dir_base =\n\t\t\t\tENGINE_READ(engine, RING_PP_DIR_BASE_READ);\n\t\t} else if (GRAPHICS_VER(i915) == 7) {\n\t\t\tee->vm_info.pp_dir_base =\n\t\t\t\tENGINE_READ(engine, RING_PP_DIR_BASE);\n\t\t} else if (GRAPHICS_VER(i915) >= 8) {\n\t\t\tu32 base = engine->mmio_base;\n\n\t\t\tfor (i = 0; i < 4; i++) {\n\t\t\t\tee->vm_info.pdp[i] =\n\t\t\t\t\tintel_uncore_read(engine->uncore,\n\t\t\t\t\t\t\t  GEN8_RING_PDP_UDW(base, i));\n\t\t\t\tee->vm_info.pdp[i] <<= 32;\n\t\t\t\tee->vm_info.pdp[i] |=\n\t\t\t\t\tintel_uncore_read(engine->uncore,\n\t\t\t\t\t\t\t  GEN8_RING_PDP_LDW(base, i));\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void record_request(const struct i915_request *request,\n\t\t\t   struct i915_request_coredump *erq)\n{\n\terq->flags = request->fence.flags;\n\terq->context = request->fence.context;\n\terq->seqno = request->fence.seqno;\n\terq->sched_attr = request->sched.attr;\n\terq->head = request->head;\n\terq->tail = request->tail;\n\n\terq->pid = 0;\n\trcu_read_lock();\n\tif (!intel_context_is_closed(request->context)) {\n\t\tconst struct i915_gem_context *ctx;\n\n\t\tctx = rcu_dereference(request->context->gem_context);\n\t\tif (ctx)\n\t\t\terq->pid = pid_nr(ctx->pid);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void engine_record_execlists(struct intel_engine_coredump *ee)\n{\n\tconst struct intel_engine_execlists * const el = &ee->engine->execlists;\n\tstruct i915_request * const *port = el->active;\n\tunsigned int n = 0;\n\n\twhile (*port)\n\t\trecord_request(*port++, &ee->execlist[n++]);\n\n\tee->num_ports = n;\n}\n\nstatic bool record_context(struct i915_gem_context_coredump *e,\n\t\t\t   struct intel_context *ce)\n{\n\tstruct i915_gem_context *ctx;\n\tstruct task_struct *task;\n\tbool simulated;\n\n\trcu_read_lock();\n\tctx = rcu_dereference(ce->gem_context);\n\tif (ctx && !kref_get_unless_zero(&ctx->ref))\n\t\tctx = NULL;\n\trcu_read_unlock();\n\tif (!ctx)\n\t\treturn true;\n\n\trcu_read_lock();\n\ttask = pid_task(ctx->pid, PIDTYPE_PID);\n\tif (task) {\n\t\tstrcpy(e->comm, task->comm);\n\t\te->pid = task->pid;\n\t}\n\trcu_read_unlock();\n\n\te->sched_attr = ctx->sched;\n\te->guilty = atomic_read(&ctx->guilty_count);\n\te->active = atomic_read(&ctx->active_count);\n\te->hwsp_seqno = (ce->timeline && ce->timeline->hwsp_seqno) ?\n\t\t\t\t*ce->timeline->hwsp_seqno : ~0U;\n\n\te->total_runtime = intel_context_get_total_runtime_ns(ce);\n\te->avg_runtime = intel_context_get_avg_runtime_ns(ce);\n\n\tsimulated = i915_gem_context_no_error_capture(ctx);\n\n\ti915_gem_context_put(ctx);\n\treturn simulated;\n}\n\nstruct intel_engine_capture_vma {\n\tstruct intel_engine_capture_vma *next;\n\tstruct i915_vma_resource *vma_res;\n\tchar name[16];\n\tbool lockdep_cookie;\n};\n\nstatic struct intel_engine_capture_vma *\ncapture_vma_snapshot(struct intel_engine_capture_vma *next,\n\t\t     struct i915_vma_resource *vma_res,\n\t\t     gfp_t gfp, const char *name)\n{\n\tstruct intel_engine_capture_vma *c;\n\n\tif (!vma_res)\n\t\treturn next;\n\n\tc = kmalloc(sizeof(*c), gfp);\n\tif (!c)\n\t\treturn next;\n\n\tif (!i915_vma_resource_hold(vma_res, &c->lockdep_cookie)) {\n\t\tkfree(c);\n\t\treturn next;\n\t}\n\n\tstrcpy(c->name, name);\n\tc->vma_res = i915_vma_resource_get(vma_res);\n\n\tc->next = next;\n\treturn c;\n}\n\nstatic struct intel_engine_capture_vma *\ncapture_vma(struct intel_engine_capture_vma *next,\n\t    struct i915_vma *vma,\n\t    const char *name,\n\t    gfp_t gfp)\n{\n\tif (!vma)\n\t\treturn next;\n\n\t \n\tif (GEM_WARN_ON(!i915_vma_is_pinned(vma)))\n\t\treturn next;\n\n\tnext = capture_vma_snapshot(next, vma->resource, gfp, name);\n\n\treturn next;\n}\n\nstatic struct intel_engine_capture_vma *\ncapture_user(struct intel_engine_capture_vma *capture,\n\t     const struct i915_request *rq,\n\t     gfp_t gfp)\n{\n\tstruct i915_capture_list *c;\n\n\tfor (c = rq->capture_list; c; c = c->next)\n\t\tcapture = capture_vma_snapshot(capture, c->vma_res, gfp,\n\t\t\t\t\t       \"user\");\n\n\treturn capture;\n}\n\nstatic void add_vma(struct intel_engine_coredump *ee,\n\t\t    struct i915_vma_coredump *vma)\n{\n\tif (vma) {\n\t\tvma->next = ee->vma;\n\t\tee->vma = vma;\n\t}\n}\n\nstatic struct i915_vma_coredump *\ncreate_vma_coredump(const struct intel_gt *gt, struct i915_vma *vma,\n\t\t    const char *name, struct i915_vma_compress *compress)\n{\n\tstruct i915_vma_coredump *ret = NULL;\n\tstruct i915_vma_resource *vma_res;\n\tbool lockdep_cookie;\n\n\tif (!vma)\n\t\treturn NULL;\n\n\tvma_res = vma->resource;\n\n\tif (i915_vma_resource_hold(vma_res, &lockdep_cookie)) {\n\t\tret = i915_vma_coredump_create(gt, vma_res, compress, name);\n\t\ti915_vma_resource_unhold(vma_res, lockdep_cookie);\n\t}\n\n\treturn ret;\n}\n\nstatic void add_vma_coredump(struct intel_engine_coredump *ee,\n\t\t\t     const struct intel_gt *gt,\n\t\t\t     struct i915_vma *vma,\n\t\t\t     const char *name,\n\t\t\t     struct i915_vma_compress *compress)\n{\n\tadd_vma(ee, create_vma_coredump(gt, vma, name, compress));\n}\n\nstruct intel_engine_coredump *\nintel_engine_coredump_alloc(struct intel_engine_cs *engine, gfp_t gfp, u32 dump_flags)\n{\n\tstruct intel_engine_coredump *ee;\n\n\tee = kzalloc(sizeof(*ee), gfp);\n\tif (!ee)\n\t\treturn NULL;\n\n\tee->engine = engine;\n\n\tif (!(dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE)) {\n\t\tengine_record_registers(ee);\n\t\tengine_record_execlists(ee);\n\t}\n\n\treturn ee;\n}\n\nstatic struct intel_engine_capture_vma *\nengine_coredump_add_context(struct intel_engine_coredump *ee,\n\t\t\t    struct intel_context *ce,\n\t\t\t    gfp_t gfp)\n{\n\tstruct intel_engine_capture_vma *vma = NULL;\n\n\tee->simulated |= record_context(&ee->context, ce);\n\tif (ee->simulated)\n\t\treturn NULL;\n\n\t \n\tvma = capture_vma(vma, ce->ring->vma, \"ring\", gfp);\n\tvma = capture_vma(vma, ce->state, \"HW context\", gfp);\n\n\treturn vma;\n}\n\nstruct intel_engine_capture_vma *\nintel_engine_coredump_add_request(struct intel_engine_coredump *ee,\n\t\t\t\t  struct i915_request *rq,\n\t\t\t\t  gfp_t gfp)\n{\n\tstruct intel_engine_capture_vma *vma;\n\n\tvma = engine_coredump_add_context(ee, rq->context, gfp);\n\tif (!vma)\n\t\treturn NULL;\n\n\t \n\tvma = capture_vma_snapshot(vma, rq->batch_res, gfp, \"batch\");\n\tvma = capture_user(vma, rq, gfp);\n\n\tee->rq_head = rq->head;\n\tee->rq_post = rq->postfix;\n\tee->rq_tail = rq->tail;\n\n\treturn vma;\n}\n\nvoid\nintel_engine_coredump_add_vma(struct intel_engine_coredump *ee,\n\t\t\t      struct intel_engine_capture_vma *capture,\n\t\t\t      struct i915_vma_compress *compress)\n{\n\tconst struct intel_engine_cs *engine = ee->engine;\n\n\twhile (capture) {\n\t\tstruct intel_engine_capture_vma *this = capture;\n\t\tstruct i915_vma_resource *vma_res = this->vma_res;\n\n\t\tadd_vma(ee,\n\t\t\ti915_vma_coredump_create(engine->gt, vma_res,\n\t\t\t\t\t\t compress, this->name));\n\n\t\ti915_vma_resource_unhold(vma_res, this->lockdep_cookie);\n\t\ti915_vma_resource_put(vma_res);\n\n\t\tcapture = this->next;\n\t\tkfree(this);\n\t}\n\n\tadd_vma_coredump(ee, engine->gt, engine->status_page.vma,\n\t\t\t \"HW Status\", compress);\n\n\tadd_vma_coredump(ee, engine->gt, engine->wa_ctx.vma,\n\t\t\t \"WA context\", compress);\n}\n\nstatic struct intel_engine_coredump *\ncapture_engine(struct intel_engine_cs *engine,\n\t       struct i915_vma_compress *compress,\n\t       u32 dump_flags)\n{\n\tstruct intel_engine_capture_vma *capture = NULL;\n\tstruct intel_engine_coredump *ee;\n\tstruct intel_context *ce = NULL;\n\tstruct i915_request *rq = NULL;\n\n\tee = intel_engine_coredump_alloc(engine, ALLOW_FAIL, dump_flags);\n\tif (!ee)\n\t\treturn NULL;\n\n\tintel_engine_get_hung_entity(engine, &ce, &rq);\n\tif (rq && !i915_request_started(rq))\n\t\tdrm_info(&engine->gt->i915->drm, \"Got hung context on %s with active request %lld:%lld [0x%04X] not yet started\\n\",\n\t\t\t engine->name, rq->fence.context, rq->fence.seqno, ce->guc_id.id);\n\n\tif (rq) {\n\t\tcapture = intel_engine_coredump_add_request(ee, rq, ATOMIC_MAYFAIL);\n\t\ti915_request_put(rq);\n\t} else if (ce) {\n\t\tcapture = engine_coredump_add_context(ee, ce, ATOMIC_MAYFAIL);\n\t}\n\n\tif (capture) {\n\t\tintel_engine_coredump_add_vma(ee, capture, compress);\n\n\t\tif (dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE)\n\t\t\tintel_guc_capture_get_matching_node(engine->gt, ee, ce);\n\t} else {\n\t\tkfree(ee);\n\t\tee = NULL;\n\t}\n\n\treturn ee;\n}\n\nstatic void\ngt_record_engines(struct intel_gt_coredump *gt,\n\t\t  intel_engine_mask_t engine_mask,\n\t\t  struct i915_vma_compress *compress,\n\t\t  u32 dump_flags)\n{\n\tstruct intel_engine_cs *engine;\n\tenum intel_engine_id id;\n\n\tfor_each_engine(engine, gt->_gt, id) {\n\t\tstruct intel_engine_coredump *ee;\n\n\t\t \n\t\tpool_refill(&compress->pool, ALLOW_FAIL);\n\n\t\tee = capture_engine(engine, compress, dump_flags);\n\t\tif (!ee)\n\t\t\tcontinue;\n\n\t\tee->hung = engine->mask & engine_mask;\n\n\t\tgt->simulated |= ee->simulated;\n\t\tif (ee->simulated) {\n\t\t\tif (dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE)\n\t\t\t\tintel_guc_capture_free_node(ee);\n\t\t\tkfree(ee);\n\t\t\tcontinue;\n\t\t}\n\n\t\tee->next = gt->engine;\n\t\tgt->engine = ee;\n\t}\n}\n\nstatic void gt_record_guc_ctb(struct intel_ctb_coredump *saved,\n\t\t\t      const struct intel_guc_ct_buffer *ctb,\n\t\t\t      const void *blob_ptr, struct intel_guc *guc)\n{\n\tif (!ctb || !ctb->desc)\n\t\treturn;\n\n\tsaved->raw_status = ctb->desc->status;\n\tsaved->raw_head = ctb->desc->head;\n\tsaved->raw_tail = ctb->desc->tail;\n\tsaved->head = ctb->head;\n\tsaved->tail = ctb->tail;\n\tsaved->size = ctb->size;\n\tsaved->desc_offset = ((void *)ctb->desc) - blob_ptr;\n\tsaved->cmds_offset = ((void *)ctb->cmds) - blob_ptr;\n}\n\nstatic struct intel_uc_coredump *\ngt_record_uc(struct intel_gt_coredump *gt,\n\t     struct i915_vma_compress *compress)\n{\n\tconst struct intel_uc *uc = &gt->_gt->uc;\n\tstruct intel_uc_coredump *error_uc;\n\n\terror_uc = kzalloc(sizeof(*error_uc), ALLOW_FAIL);\n\tif (!error_uc)\n\t\treturn NULL;\n\n\tmemcpy(&error_uc->guc_fw, &uc->guc.fw, sizeof(uc->guc.fw));\n\tmemcpy(&error_uc->huc_fw, &uc->huc.fw, sizeof(uc->huc.fw));\n\n\terror_uc->guc_fw.file_selected.path = kstrdup(uc->guc.fw.file_selected.path, ALLOW_FAIL);\n\terror_uc->huc_fw.file_selected.path = kstrdup(uc->huc.fw.file_selected.path, ALLOW_FAIL);\n\terror_uc->guc_fw.file_wanted.path = kstrdup(uc->guc.fw.file_wanted.path, ALLOW_FAIL);\n\terror_uc->huc_fw.file_wanted.path = kstrdup(uc->huc.fw.file_wanted.path, ALLOW_FAIL);\n\n\t \n\terror_uc->guc.timestamp = intel_uncore_read(gt->_gt->uncore, GUCPMTIMESTAMP);\n\terror_uc->guc.vma_log = create_vma_coredump(gt->_gt, uc->guc.log.vma,\n\t\t\t\t\t\t    \"GuC log buffer\", compress);\n\terror_uc->guc.vma_ctb = create_vma_coredump(gt->_gt, uc->guc.ct.vma,\n\t\t\t\t\t\t    \"GuC CT buffer\", compress);\n\terror_uc->guc.last_fence = uc->guc.ct.requests.last_fence;\n\tgt_record_guc_ctb(error_uc->guc.ctb + 0, &uc->guc.ct.ctbs.send,\n\t\t\t  uc->guc.ct.ctbs.send.desc, (struct intel_guc *)&uc->guc);\n\tgt_record_guc_ctb(error_uc->guc.ctb + 1, &uc->guc.ct.ctbs.recv,\n\t\t\t  uc->guc.ct.ctbs.send.desc, (struct intel_guc *)&uc->guc);\n\n\treturn error_uc;\n}\n\n \nstatic void gt_record_display_regs(struct intel_gt_coredump *gt)\n{\n\tstruct intel_uncore *uncore = gt->_gt->uncore;\n\tstruct drm_i915_private *i915 = uncore->i915;\n\n\tif (GRAPHICS_VER(i915) >= 6)\n\t\tgt->derrmr = intel_uncore_read(uncore, DERRMR);\n\n\tif (GRAPHICS_VER(i915) >= 8)\n\t\tgt->ier = intel_uncore_read(uncore, GEN8_DE_MISC_IER);\n\telse if (IS_VALLEYVIEW(i915))\n\t\tgt->ier = intel_uncore_read(uncore, VLV_IER);\n\telse if (HAS_PCH_SPLIT(i915))\n\t\tgt->ier = intel_uncore_read(uncore, DEIER);\n\telse if (GRAPHICS_VER(i915) == 2)\n\t\tgt->ier = intel_uncore_read16(uncore, GEN2_IER);\n\telse\n\t\tgt->ier = intel_uncore_read(uncore, GEN2_IER);\n}\n\n \nstatic void gt_record_global_nonguc_regs(struct intel_gt_coredump *gt)\n{\n\tstruct intel_uncore *uncore = gt->_gt->uncore;\n\tstruct drm_i915_private *i915 = uncore->i915;\n\tint i;\n\n\tif (IS_VALLEYVIEW(i915)) {\n\t\tgt->gtier[0] = intel_uncore_read(uncore, GTIER);\n\t\tgt->ngtier = 1;\n\t} else if (GRAPHICS_VER(i915) >= 11) {\n\t\tgt->gtier[0] =\n\t\t\tintel_uncore_read(uncore,\n\t\t\t\t\t  GEN11_RENDER_COPY_INTR_ENABLE);\n\t\tgt->gtier[1] =\n\t\t\tintel_uncore_read(uncore, GEN11_VCS_VECS_INTR_ENABLE);\n\t\tgt->gtier[2] =\n\t\t\tintel_uncore_read(uncore, GEN11_GUC_SG_INTR_ENABLE);\n\t\tgt->gtier[3] =\n\t\t\tintel_uncore_read(uncore,\n\t\t\t\t\t  GEN11_GPM_WGBOXPERF_INTR_ENABLE);\n\t\tgt->gtier[4] =\n\t\t\tintel_uncore_read(uncore,\n\t\t\t\t\t  GEN11_CRYPTO_RSVD_INTR_ENABLE);\n\t\tgt->gtier[5] =\n\t\t\tintel_uncore_read(uncore,\n\t\t\t\t\t  GEN11_GUNIT_CSME_INTR_ENABLE);\n\t\tgt->ngtier = 6;\n\t} else if (GRAPHICS_VER(i915) >= 8) {\n\t\tfor (i = 0; i < 4; i++)\n\t\t\tgt->gtier[i] =\n\t\t\t\tintel_uncore_read(uncore, GEN8_GT_IER(i));\n\t\tgt->ngtier = 4;\n\t} else if (HAS_PCH_SPLIT(i915)) {\n\t\tgt->gtier[0] = intel_uncore_read(uncore, GTIER);\n\t\tgt->ngtier = 1;\n\t}\n\n\tgt->eir = intel_uncore_read(uncore, EIR);\n\tgt->pgtbl_er = intel_uncore_read(uncore, PGTBL_ER);\n}\n\n \nstatic void gt_record_global_regs(struct intel_gt_coredump *gt)\n{\n\tstruct intel_uncore *uncore = gt->_gt->uncore;\n\tstruct drm_i915_private *i915 = uncore->i915;\n\tint i;\n\n\t \n\n\t \n\tif (IS_VALLEYVIEW(i915))\n\t\tgt->forcewake = intel_uncore_read_fw(uncore, FORCEWAKE_VLV);\n\n\tif (GRAPHICS_VER(i915) == 7)\n\t\tgt->err_int = intel_uncore_read(uncore, GEN7_ERR_INT);\n\n\tif (GRAPHICS_VER_FULL(i915) >= IP_VER(12, 50)) {\n\t\tgt->fault_data0 = intel_gt_mcr_read_any((struct intel_gt *)gt->_gt,\n\t\t\t\t\t\t\tXEHP_FAULT_TLB_DATA0);\n\t\tgt->fault_data1 = intel_gt_mcr_read_any((struct intel_gt *)gt->_gt,\n\t\t\t\t\t\t\tXEHP_FAULT_TLB_DATA1);\n\t} else if (GRAPHICS_VER(i915) >= 12) {\n\t\tgt->fault_data0 = intel_uncore_read(uncore,\n\t\t\t\t\t\t    GEN12_FAULT_TLB_DATA0);\n\t\tgt->fault_data1 = intel_uncore_read(uncore,\n\t\t\t\t\t\t    GEN12_FAULT_TLB_DATA1);\n\t} else if (GRAPHICS_VER(i915) >= 8) {\n\t\tgt->fault_data0 = intel_uncore_read(uncore,\n\t\t\t\t\t\t    GEN8_FAULT_TLB_DATA0);\n\t\tgt->fault_data1 = intel_uncore_read(uncore,\n\t\t\t\t\t\t    GEN8_FAULT_TLB_DATA1);\n\t}\n\n\tif (GRAPHICS_VER(i915) == 6) {\n\t\tgt->forcewake = intel_uncore_read_fw(uncore, FORCEWAKE);\n\t\tgt->gab_ctl = intel_uncore_read(uncore, GAB_CTL);\n\t\tgt->gfx_mode = intel_uncore_read(uncore, GFX_MODE);\n\t}\n\n\t \n\tif (GRAPHICS_VER(i915) >= 7)\n\t\tgt->forcewake = intel_uncore_read_fw(uncore, FORCEWAKE_MT);\n\n\tif (GRAPHICS_VER(i915) >= 6) {\n\t\tif (GRAPHICS_VER(i915) < 12) {\n\t\t\tgt->error = intel_uncore_read(uncore, ERROR_GEN6);\n\t\t\tgt->done_reg = intel_uncore_read(uncore, DONE_REG);\n\t\t}\n\t}\n\n\t \n\tif (IS_GRAPHICS_VER(i915, 6, 7)) {\n\t\tgt->gam_ecochk = intel_uncore_read(uncore, GAM_ECOCHK);\n\t\tgt->gac_eco = intel_uncore_read(uncore, GAC_ECO_BITS);\n\t}\n\n\tif (IS_GRAPHICS_VER(i915, 8, 11))\n\t\tgt->gtt_cache = intel_uncore_read(uncore, HSW_GTT_CACHE_EN);\n\n\tif (GRAPHICS_VER(i915) == 12)\n\t\tgt->aux_err = intel_uncore_read(uncore, GEN12_AUX_ERR_DBG);\n\n\tif (GRAPHICS_VER(i915) >= 12) {\n\t\tfor (i = 0; i < I915_MAX_SFC; i++) {\n\t\t\t \n\t\t\tif ((gt->_gt->info.sfc_mask & BIT(i)) == 0 ||\n\t\t\t    !HAS_ENGINE(gt->_gt, _VCS(i * 2)))\n\t\t\t\tcontinue;\n\n\t\t\tgt->sfc_done[i] =\n\t\t\t\tintel_uncore_read(uncore, GEN12_SFC_DONE(i));\n\t\t}\n\n\t\tgt->gam_done = intel_uncore_read(uncore, GEN12_GAM_DONE);\n\t}\n}\n\nstatic void gt_record_info(struct intel_gt_coredump *gt)\n{\n\tmemcpy(&gt->info, &gt->_gt->info, sizeof(struct intel_gt_info));\n\tgt->clock_frequency = gt->_gt->clock_frequency;\n\tgt->clock_period_ns = gt->_gt->clock_period_ns;\n}\n\n \nstatic u32 generate_ecode(const struct intel_engine_coredump *ee)\n{\n\t \n\treturn ee ? ee->ipehr ^ ee->instdone.instdone : 0;\n}\n\nstatic const char *error_msg(struct i915_gpu_coredump *error)\n{\n\tstruct intel_engine_coredump *first = NULL;\n\tunsigned int hung_classes = 0;\n\tstruct intel_gt_coredump *gt;\n\tint len;\n\n\tfor (gt = error->gt; gt; gt = gt->next) {\n\t\tstruct intel_engine_coredump *cs;\n\n\t\tfor (cs = gt->engine; cs; cs = cs->next) {\n\t\t\tif (cs->hung) {\n\t\t\t\thung_classes |= BIT(cs->engine->uabi_class);\n\t\t\t\tif (!first)\n\t\t\t\t\tfirst = cs;\n\t\t\t}\n\t\t}\n\t}\n\n\tlen = scnprintf(error->error_msg, sizeof(error->error_msg),\n\t\t\t\"GPU HANG: ecode %d:%x:%08x\",\n\t\t\tGRAPHICS_VER(error->i915), hung_classes,\n\t\t\tgenerate_ecode(first));\n\tif (first && first->context.pid) {\n\t\t \n\t\tlen += scnprintf(error->error_msg + len,\n\t\t\t\t sizeof(error->error_msg) - len,\n\t\t\t\t \", in %s [%d]\",\n\t\t\t\t first->context.comm, first->context.pid);\n\t}\n\n\treturn error->error_msg;\n}\n\nstatic void capture_gen(struct i915_gpu_coredump *error)\n{\n\tstruct drm_i915_private *i915 = error->i915;\n\n\terror->wakelock = atomic_read(&i915->runtime_pm.wakeref_count);\n\terror->suspended = i915->runtime_pm.suspended;\n\n\terror->iommu = i915_vtd_active(i915);\n\terror->reset_count = i915_reset_count(&i915->gpu_error);\n\terror->suspend_count = i915->suspend_count;\n\n\ti915_params_copy(&error->params, &i915->params);\n\tmemcpy(&error->device_info,\n\t       INTEL_INFO(i915),\n\t       sizeof(error->device_info));\n\tmemcpy(&error->runtime_info,\n\t       RUNTIME_INFO(i915),\n\t       sizeof(error->runtime_info));\n\tmemcpy(&error->display_device_info, DISPLAY_INFO(i915),\n\t       sizeof(error->display_device_info));\n\tmemcpy(&error->display_runtime_info, DISPLAY_RUNTIME_INFO(i915),\n\t       sizeof(error->display_runtime_info));\n\terror->driver_caps = i915->caps;\n}\n\nstruct i915_gpu_coredump *\ni915_gpu_coredump_alloc(struct drm_i915_private *i915, gfp_t gfp)\n{\n\tstruct i915_gpu_coredump *error;\n\n\tif (!i915->params.error_capture)\n\t\treturn NULL;\n\n\terror = kzalloc(sizeof(*error), gfp);\n\tif (!error)\n\t\treturn NULL;\n\n\tkref_init(&error->ref);\n\terror->i915 = i915;\n\n\terror->time = ktime_get_real();\n\terror->boottime = ktime_get_boottime();\n\terror->uptime = ktime_sub(ktime_get(), to_gt(i915)->last_init_time);\n\terror->capture = jiffies;\n\n\tcapture_gen(error);\n\n\treturn error;\n}\n\n#define DAY_AS_SECONDS(x) (24 * 60 * 60 * (x))\n\nstruct intel_gt_coredump *\nintel_gt_coredump_alloc(struct intel_gt *gt, gfp_t gfp, u32 dump_flags)\n{\n\tstruct intel_gt_coredump *gc;\n\n\tgc = kzalloc(sizeof(*gc), gfp);\n\tif (!gc)\n\t\treturn NULL;\n\n\tgc->_gt = gt;\n\tgc->awake = intel_gt_pm_is_awake(gt);\n\n\tgt_record_display_regs(gc);\n\tgt_record_global_nonguc_regs(gc);\n\n\t \n\tif (!(dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE))\n\t\tgt_record_global_regs(gc);\n\n\tgt_record_fences(gc);\n\n\treturn gc;\n}\n\nstruct i915_vma_compress *\ni915_vma_capture_prepare(struct intel_gt_coredump *gt)\n{\n\tstruct i915_vma_compress *compress;\n\n\tcompress = kmalloc(sizeof(*compress), ALLOW_FAIL);\n\tif (!compress)\n\t\treturn NULL;\n\n\tif (!compress_init(compress)) {\n\t\tkfree(compress);\n\t\treturn NULL;\n\t}\n\n\treturn compress;\n}\n\nvoid i915_vma_capture_finish(struct intel_gt_coredump *gt,\n\t\t\t     struct i915_vma_compress *compress)\n{\n\tif (!compress)\n\t\treturn;\n\n\tcompress_fini(compress);\n\tkfree(compress);\n}\n\nstatic struct i915_gpu_coredump *\n__i915_gpu_coredump(struct intel_gt *gt, intel_engine_mask_t engine_mask, u32 dump_flags)\n{\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct i915_gpu_coredump *error;\n\n\t \n\terror = READ_ONCE(i915->gpu_error.first_error);\n\tif (IS_ERR(error))\n\t\treturn error;\n\n\terror = i915_gpu_coredump_alloc(i915, ALLOW_FAIL);\n\tif (!error)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terror->gt = intel_gt_coredump_alloc(gt, ALLOW_FAIL, dump_flags);\n\tif (error->gt) {\n\t\tstruct i915_vma_compress *compress;\n\n\t\tcompress = i915_vma_capture_prepare(error->gt);\n\t\tif (!compress) {\n\t\t\tkfree(error->gt);\n\t\t\tkfree(error);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\tif (INTEL_INFO(i915)->has_gt_uc) {\n\t\t\terror->gt->uc = gt_record_uc(error->gt, compress);\n\t\t\tif (error->gt->uc) {\n\t\t\t\tif (dump_flags & CORE_DUMP_FLAG_IS_GUC_CAPTURE)\n\t\t\t\t\terror->gt->uc->guc.is_guc_capture = true;\n\t\t\t\telse\n\t\t\t\t\tGEM_BUG_ON(error->gt->uc->guc.is_guc_capture);\n\t\t\t}\n\t\t}\n\n\t\tgt_record_info(error->gt);\n\t\tgt_record_engines(error->gt, engine_mask, compress, dump_flags);\n\n\n\t\ti915_vma_capture_finish(error->gt, compress);\n\n\t\terror->simulated |= error->gt->simulated;\n\t}\n\n\terror->overlay = intel_overlay_capture_error_state(i915);\n\n\treturn error;\n}\n\nstruct i915_gpu_coredump *\ni915_gpu_coredump(struct intel_gt *gt, intel_engine_mask_t engine_mask, u32 dump_flags)\n{\n\tstatic DEFINE_MUTEX(capture_mutex);\n\tint ret = mutex_lock_interruptible(&capture_mutex);\n\tstruct i915_gpu_coredump *dump;\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdump = __i915_gpu_coredump(gt, engine_mask, dump_flags);\n\tmutex_unlock(&capture_mutex);\n\n\treturn dump;\n}\n\nvoid i915_error_state_store(struct i915_gpu_coredump *error)\n{\n\tstruct drm_i915_private *i915;\n\tstatic bool warned;\n\n\tif (IS_ERR_OR_NULL(error))\n\t\treturn;\n\n\ti915 = error->i915;\n\tdrm_info(&i915->drm, \"%s\\n\", error_msg(error));\n\n\tif (error->simulated ||\n\t    cmpxchg(&i915->gpu_error.first_error, NULL, error))\n\t\treturn;\n\n\ti915_gpu_coredump_get(error);\n\n\tif (!xchg(&warned, true) &&\n\t    ktime_get_real_seconds() - DRIVER_TIMESTAMP < DAY_AS_SECONDS(180)) {\n\t\tpr_info(\"GPU hangs can indicate a bug anywhere in the entire gfx stack, including userspace.\\n\");\n\t\tpr_info(\"Please file a _new_ bug report at https://gitlab.freedesktop.org/drm/intel/issues/new.\\n\");\n\t\tpr_info(\"Please see https://gitlab.freedesktop.org/drm/intel/-/wikis/How-to-file-i915-bugs for details.\\n\");\n\t\tpr_info(\"drm/i915 developers can then reassign to the right component if it's not a kernel issue.\\n\");\n\t\tpr_info(\"The GPU crash dump is required to analyze GPU hangs, so please always attach it.\\n\");\n\t\tpr_info(\"GPU crash dump saved to /sys/class/drm/card%d/error\\n\",\n\t\t\ti915->drm.primary->index);\n\t}\n}\n\n \nvoid i915_capture_error_state(struct intel_gt *gt,\n\t\t\t      intel_engine_mask_t engine_mask, u32 dump_flags)\n{\n\tstruct i915_gpu_coredump *error;\n\n\terror = i915_gpu_coredump(gt, engine_mask, dump_flags);\n\tif (IS_ERR(error)) {\n\t\tcmpxchg(&gt->i915->gpu_error.first_error, NULL, error);\n\t\treturn;\n\t}\n\n\ti915_error_state_store(error);\n\ti915_gpu_coredump_put(error);\n}\n\nstruct i915_gpu_coredump *\ni915_first_error_state(struct drm_i915_private *i915)\n{\n\tstruct i915_gpu_coredump *error;\n\n\tspin_lock_irq(&i915->gpu_error.lock);\n\terror = i915->gpu_error.first_error;\n\tif (!IS_ERR_OR_NULL(error))\n\t\ti915_gpu_coredump_get(error);\n\tspin_unlock_irq(&i915->gpu_error.lock);\n\n\treturn error;\n}\n\nvoid i915_reset_error_state(struct drm_i915_private *i915)\n{\n\tstruct i915_gpu_coredump *error;\n\n\tspin_lock_irq(&i915->gpu_error.lock);\n\terror = i915->gpu_error.first_error;\n\tif (error != ERR_PTR(-ENODEV))  \n\t\ti915->gpu_error.first_error = NULL;\n\tspin_unlock_irq(&i915->gpu_error.lock);\n\n\tif (!IS_ERR_OR_NULL(error))\n\t\ti915_gpu_coredump_put(error);\n}\n\nvoid i915_disable_error_state(struct drm_i915_private *i915, int err)\n{\n\tspin_lock_irq(&i915->gpu_error.lock);\n\tif (!i915->gpu_error.first_error)\n\t\ti915->gpu_error.first_error = ERR_PTR(err);\n\tspin_unlock_irq(&i915->gpu_error.lock);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM)\nvoid intel_klog_error_capture(struct intel_gt *gt,\n\t\t\t      intel_engine_mask_t engine_mask)\n{\n\tstatic int g_count;\n\tstruct drm_i915_private *i915 = gt->i915;\n\tstruct i915_gpu_coredump *error;\n\tintel_wakeref_t wakeref;\n\tsize_t buf_size = PAGE_SIZE * 128;\n\tsize_t pos_err;\n\tchar *buf, *ptr, *next;\n\tint l_count = g_count++;\n\tint line = 0;\n\n\t \n\tif (test_bit(I915_RESET_BACKOFF, &gt->reset.flags)) {\n\t\tdrm_err(&gt->i915->drm, \"[Capture/%d.%d] Inside GT reset, skipping error capture :(\\n\",\n\t\t\tl_count, line++);\n\t\treturn;\n\t}\n\n\terror = READ_ONCE(i915->gpu_error.first_error);\n\tif (error) {\n\t\tdrm_err(&i915->drm, \"[Capture/%d.%d] Clearing existing error capture first...\\n\",\n\t\t\tl_count, line++);\n\t\ti915_reset_error_state(i915);\n\t}\n\n\twith_intel_runtime_pm(&i915->runtime_pm, wakeref)\n\t\terror = i915_gpu_coredump(gt, engine_mask, CORE_DUMP_FLAG_NONE);\n\n\tif (IS_ERR(error)) {\n\t\tdrm_err(&i915->drm, \"[Capture/%d.%d] Failed to capture error capture: %ld!\\n\",\n\t\t\tl_count, line++, PTR_ERR(error));\n\t\treturn;\n\t}\n\n\tbuf = kvmalloc(buf_size, GFP_KERNEL);\n\tif (!buf) {\n\t\tdrm_err(&i915->drm, \"[Capture/%d.%d] Failed to allocate buffer for error capture!\\n\",\n\t\t\tl_count, line++);\n\t\ti915_gpu_coredump_put(error);\n\t\treturn;\n\t}\n\n\tdrm_info(&i915->drm, \"[Capture/%d.%d] Dumping i915 error capture for %ps...\\n\",\n\t\t l_count, line++, __builtin_return_address(0));\n\n\t \n#\tdefine MAX_CHUNK\t800\n\n\tpos_err = 0;\n\twhile (1) {\n\t\tssize_t got = i915_gpu_coredump_copy_to_buffer(error, buf, pos_err, buf_size - 1);\n\n\t\tif (got <= 0)\n\t\t\tbreak;\n\n\t\tbuf[got] = 0;\n\t\tpos_err += got;\n\n\t\tptr = buf;\n\t\twhile (got > 0) {\n\t\t\tsize_t count;\n\t\t\tchar tag[2];\n\n\t\t\tnext = strnchr(ptr, got, '\\n');\n\t\t\tif (next) {\n\t\t\t\tcount = next - ptr;\n\t\t\t\t*next = 0;\n\t\t\t\ttag[0] = '>';\n\t\t\t\ttag[1] = '<';\n\t\t\t} else {\n\t\t\t\tcount = got;\n\t\t\t\ttag[0] = '}';\n\t\t\t\ttag[1] = '{';\n\t\t\t}\n\n\t\t\tif (count > MAX_CHUNK) {\n\t\t\t\tsize_t pos;\n\t\t\t\tchar *ptr2 = ptr;\n\n\t\t\t\tfor (pos = MAX_CHUNK; pos < count; pos += MAX_CHUNK) {\n\t\t\t\t\tchar chr = ptr[pos];\n\n\t\t\t\t\tptr[pos] = 0;\n\t\t\t\t\tdrm_info(&i915->drm, \"[Capture/%d.%d] }%s{\\n\",\n\t\t\t\t\t\t l_count, line++, ptr2);\n\t\t\t\t\tptr[pos] = chr;\n\t\t\t\t\tptr2 = ptr + pos;\n\n\t\t\t\t\t \n\t\t\t\t\tcond_resched();\n\t\t\t\t}\n\n\t\t\t\tif (ptr2 < (ptr + count))\n\t\t\t\t\tdrm_info(&i915->drm, \"[Capture/%d.%d] %c%s%c\\n\",\n\t\t\t\t\t\t l_count, line++, tag[0], ptr2, tag[1]);\n\t\t\t\telse if (tag[0] == '>')\n\t\t\t\t\tdrm_info(&i915->drm, \"[Capture/%d.%d] ><\\n\",\n\t\t\t\t\t\t l_count, line++);\n\t\t\t} else {\n\t\t\t\tdrm_info(&i915->drm, \"[Capture/%d.%d] %c%s%c\\n\",\n\t\t\t\t\t l_count, line++, tag[0], ptr, tag[1]);\n\t\t\t}\n\n\t\t\tptr = next;\n\t\t\tgot -= count;\n\t\t\tif (next) {\n\t\t\t\tptr++;\n\t\t\t\tgot--;\n\t\t\t}\n\n\t\t\t \n\t\t\tcond_resched();\n\t\t}\n\n\t\tif (got)\n\t\t\tdrm_info(&i915->drm, \"[Capture/%d.%d] Got %zd bytes remaining!\\n\",\n\t\t\t\t l_count, line++, got);\n\t}\n\n\tkvfree(buf);\n\n\tdrm_info(&i915->drm, \"[Capture/%d.%d] Dumped %zd bytes\\n\", l_count, line++, pos_err);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}