{
  "module_name": "i915_active.c",
  "hash_id": "d4ea2ccab5dc1f237fe3e76ec2bb26e4cc5fc507205c8789d06a33213de6d0a4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/i915/i915_active.c",
  "human_readable_source": " \n\n#include <linux/debugobjects.h>\n\n#include \"gt/intel_context.h\"\n#include \"gt/intel_engine_heartbeat.h\"\n#include \"gt/intel_engine_pm.h\"\n#include \"gt/intel_ring.h\"\n\n#include \"i915_drv.h\"\n#include \"i915_active.h\"\n\n \nstatic struct kmem_cache *slab_cache;\n\nstruct active_node {\n\tstruct rb_node node;\n\tstruct i915_active_fence base;\n\tstruct i915_active *ref;\n\tu64 timeline;\n};\n\n#define fetch_node(x) rb_entry(READ_ONCE(x), typeof(struct active_node), node)\n\nstatic inline struct active_node *\nnode_from_active(struct i915_active_fence *active)\n{\n\treturn container_of(active, struct active_node, base);\n}\n\n#define take_preallocated_barriers(x) llist_del_all(&(x)->preallocated_barriers)\n\nstatic inline bool is_barrier(const struct i915_active_fence *active)\n{\n\treturn IS_ERR(rcu_access_pointer(active->fence));\n}\n\nstatic inline struct llist_node *barrier_to_ll(struct active_node *node)\n{\n\tGEM_BUG_ON(!is_barrier(&node->base));\n\treturn (struct llist_node *)&node->base.cb.node;\n}\n\nstatic inline struct intel_engine_cs *\n__barrier_to_engine(struct active_node *node)\n{\n\treturn (struct intel_engine_cs *)READ_ONCE(node->base.cb.node.prev);\n}\n\nstatic inline struct intel_engine_cs *\nbarrier_to_engine(struct active_node *node)\n{\n\tGEM_BUG_ON(!is_barrier(&node->base));\n\treturn __barrier_to_engine(node);\n}\n\nstatic inline struct active_node *barrier_from_ll(struct llist_node *x)\n{\n\treturn container_of((struct list_head *)x,\n\t\t\t    struct active_node, base.cb.node);\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_GEM) && IS_ENABLED(CONFIG_DEBUG_OBJECTS)\n\nstatic void *active_debug_hint(void *addr)\n{\n\tstruct i915_active *ref = addr;\n\n\treturn (void *)ref->active ?: (void *)ref->retire ?: (void *)ref;\n}\n\nstatic const struct debug_obj_descr active_debug_desc = {\n\t.name = \"i915_active\",\n\t.debug_hint = active_debug_hint,\n};\n\nstatic void debug_active_init(struct i915_active *ref)\n{\n\tdebug_object_init(ref, &active_debug_desc);\n}\n\nstatic void debug_active_activate(struct i915_active *ref)\n{\n\tlockdep_assert_held(&ref->tree_lock);\n\tdebug_object_activate(ref, &active_debug_desc);\n}\n\nstatic void debug_active_deactivate(struct i915_active *ref)\n{\n\tlockdep_assert_held(&ref->tree_lock);\n\tif (!atomic_read(&ref->count))  \n\t\tdebug_object_deactivate(ref, &active_debug_desc);\n}\n\nstatic void debug_active_fini(struct i915_active *ref)\n{\n\tdebug_object_free(ref, &active_debug_desc);\n}\n\nstatic void debug_active_assert(struct i915_active *ref)\n{\n\tdebug_object_assert_init(ref, &active_debug_desc);\n}\n\n#else\n\nstatic inline void debug_active_init(struct i915_active *ref) { }\nstatic inline void debug_active_activate(struct i915_active *ref) { }\nstatic inline void debug_active_deactivate(struct i915_active *ref) { }\nstatic inline void debug_active_fini(struct i915_active *ref) { }\nstatic inline void debug_active_assert(struct i915_active *ref) { }\n\n#endif\n\nstatic void\n__active_retire(struct i915_active *ref)\n{\n\tstruct rb_root root = RB_ROOT;\n\tstruct active_node *it, *n;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\t \n\tif (!atomic_dec_and_lock_irqsave(&ref->count, &ref->tree_lock, flags))\n\t\treturn;\n\n\tGEM_BUG_ON(rcu_access_pointer(ref->excl.fence));\n\tdebug_active_deactivate(ref);\n\n\t \n\tif (!ref->cache)\n\t\tref->cache = fetch_node(ref->tree.rb_node);\n\n\t \n\tif (ref->cache) {\n\t\t \n\t\trb_erase(&ref->cache->node, &ref->tree);\n\t\troot = ref->tree;\n\n\t\t \n\t\trb_link_node(&ref->cache->node, NULL, &ref->tree.rb_node);\n\t\trb_insert_color(&ref->cache->node, &ref->tree);\n\t\tGEM_BUG_ON(ref->tree.rb_node != &ref->cache->node);\n\n\t\t \n\t\tref->cache->timeline = 0;  \n\t}\n\n\tspin_unlock_irqrestore(&ref->tree_lock, flags);\n\n\t \n\tif (ref->retire)\n\t\tref->retire(ref);\n\n\t \n\twake_up_var(ref);\n\n\t \n\trbtree_postorder_for_each_entry_safe(it, n, &root, node) {\n\t\tGEM_BUG_ON(i915_active_fence_isset(&it->base));\n\t\tkmem_cache_free(slab_cache, it);\n\t}\n}\n\nstatic void\nactive_work(struct work_struct *wrk)\n{\n\tstruct i915_active *ref = container_of(wrk, typeof(*ref), work);\n\n\tGEM_BUG_ON(!atomic_read(&ref->count));\n\tif (atomic_add_unless(&ref->count, -1, 1))\n\t\treturn;\n\n\t__active_retire(ref);\n}\n\nstatic void\nactive_retire(struct i915_active *ref)\n{\n\tGEM_BUG_ON(!atomic_read(&ref->count));\n\tif (atomic_add_unless(&ref->count, -1, 1))\n\t\treturn;\n\n\tif (ref->flags & I915_ACTIVE_RETIRE_SLEEPS) {\n\t\tqueue_work(system_unbound_wq, &ref->work);\n\t\treturn;\n\t}\n\n\t__active_retire(ref);\n}\n\nstatic inline struct dma_fence **\n__active_fence_slot(struct i915_active_fence *active)\n{\n\treturn (struct dma_fence ** __force)&active->fence;\n}\n\nstatic inline bool\nactive_fence_cb(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tstruct i915_active_fence *active =\n\t\tcontainer_of(cb, typeof(*active), cb);\n\n\treturn cmpxchg(__active_fence_slot(active), fence, NULL) == fence;\n}\n\nstatic void\nnode_retire(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tif (active_fence_cb(fence, cb))\n\t\tactive_retire(container_of(cb, struct active_node, base.cb)->ref);\n}\n\nstatic void\nexcl_retire(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tif (active_fence_cb(fence, cb))\n\t\tactive_retire(container_of(cb, struct i915_active, excl.cb));\n}\n\nstatic struct active_node *__active_lookup(struct i915_active *ref, u64 idx)\n{\n\tstruct active_node *it;\n\n\tGEM_BUG_ON(idx == 0);  \n\n\t \n\tit = READ_ONCE(ref->cache);\n\tif (it) {\n\t\tu64 cached = READ_ONCE(it->timeline);\n\n\t\t \n\t\tif (cached == idx)\n\t\t\treturn it;\n\n\t\t \n\t\tif (!cached && !cmpxchg64(&it->timeline, 0, idx))\n\t\t\treturn it;\n\t}\n\n\tBUILD_BUG_ON(offsetof(typeof(*it), node));\n\n\t \n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\tit = fetch_node(ref->tree.rb_node);\n\twhile (it) {\n\t\tif (it->timeline < idx) {\n\t\t\tit = fetch_node(it->node.rb_right);\n\t\t} else if (it->timeline > idx) {\n\t\t\tit = fetch_node(it->node.rb_left);\n\t\t} else {\n\t\t\tWRITE_ONCE(ref->cache, it);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\treturn it;\n}\n\nstatic struct i915_active_fence *\nactive_instance(struct i915_active *ref, u64 idx)\n{\n\tstruct active_node *node;\n\tstruct rb_node **p, *parent;\n\n\tnode = __active_lookup(ref, idx);\n\tif (likely(node))\n\t\treturn &node->base;\n\n\tspin_lock_irq(&ref->tree_lock);\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\tparent = NULL;\n\tp = &ref->tree.rb_node;\n\twhile (*p) {\n\t\tparent = *p;\n\n\t\tnode = rb_entry(parent, struct active_node, node);\n\t\tif (node->timeline == idx)\n\t\t\tgoto out;\n\n\t\tif (node->timeline < idx)\n\t\t\tp = &parent->rb_right;\n\t\telse\n\t\t\tp = &parent->rb_left;\n\t}\n\n\t \n\tnode = kmem_cache_alloc(slab_cache, GFP_ATOMIC);\n\tif (!node)\n\t\tgoto out;\n\n\t__i915_active_fence_init(&node->base, NULL, node_retire);\n\tnode->ref = ref;\n\tnode->timeline = idx;\n\n\trb_link_node(&node->node, parent, p);\n\trb_insert_color(&node->node, &ref->tree);\n\nout:\n\tWRITE_ONCE(ref->cache, node);\n\tspin_unlock_irq(&ref->tree_lock);\n\n\treturn &node->base;\n}\n\nvoid __i915_active_init(struct i915_active *ref,\n\t\t\tint (*active)(struct i915_active *ref),\n\t\t\tvoid (*retire)(struct i915_active *ref),\n\t\t\tunsigned long flags,\n\t\t\tstruct lock_class_key *mkey,\n\t\t\tstruct lock_class_key *wkey)\n{\n\tdebug_active_init(ref);\n\n\tref->flags = flags;\n\tref->active = active;\n\tref->retire = retire;\n\n\tspin_lock_init(&ref->tree_lock);\n\tref->tree = RB_ROOT;\n\tref->cache = NULL;\n\n\tinit_llist_head(&ref->preallocated_barriers);\n\tatomic_set(&ref->count, 0);\n\t__mutex_init(&ref->mutex, \"i915_active\", mkey);\n\t__i915_active_fence_init(&ref->excl, NULL, excl_retire);\n\tINIT_WORK(&ref->work, active_work);\n#if IS_ENABLED(CONFIG_LOCKDEP)\n\tlockdep_init_map(&ref->work.lockdep_map, \"i915_active.work\", wkey, 0);\n#endif\n}\n\nstatic bool ____active_del_barrier(struct i915_active *ref,\n\t\t\t\t   struct active_node *node,\n\t\t\t\t   struct intel_engine_cs *engine)\n\n{\n\tstruct llist_node *head = NULL, *tail = NULL;\n\tstruct llist_node *pos, *next;\n\n\tGEM_BUG_ON(node->timeline != engine->kernel_context->timeline->fence_context);\n\n\t \n\tllist_for_each_safe(pos, next, llist_del_all(&engine->barrier_tasks)) {\n\t\tif (node == barrier_from_ll(pos)) {\n\t\t\tnode = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpos->next = head;\n\t\thead = pos;\n\t\tif (!tail)\n\t\t\ttail = pos;\n\t}\n\tif (head)\n\t\tllist_add_batch(head, tail, &engine->barrier_tasks);\n\n\treturn !node;\n}\n\nstatic bool\n__active_del_barrier(struct i915_active *ref, struct active_node *node)\n{\n\treturn ____active_del_barrier(ref, node, barrier_to_engine(node));\n}\n\nstatic bool\nreplace_barrier(struct i915_active *ref, struct i915_active_fence *active)\n{\n\tif (!is_barrier(active))  \n\t\treturn false;\n\n\t \n\treturn __active_del_barrier(ref, node_from_active(active));\n}\n\nint i915_active_add_request(struct i915_active *ref, struct i915_request *rq)\n{\n\tu64 idx = i915_request_timeline(rq)->fence_context;\n\tstruct dma_fence *fence = &rq->fence;\n\tstruct i915_active_fence *active;\n\tint err;\n\n\t \n\terr = i915_active_acquire(ref);\n\tif (err)\n\t\treturn err;\n\n\tdo {\n\t\tactive = active_instance(ref, idx);\n\t\tif (!active) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (replace_barrier(ref, active)) {\n\t\t\tRCU_INIT_POINTER(active->fence, NULL);\n\t\t\tatomic_dec(&ref->count);\n\t\t}\n\t} while (unlikely(is_barrier(active)));\n\n\tfence = __i915_active_fence_set(active, fence);\n\tif (!fence)\n\t\t__i915_active_acquire(ref);\n\telse\n\t\tdma_fence_put(fence);\n\nout:\n\ti915_active_release(ref);\n\treturn err;\n}\n\nstatic struct dma_fence *\n__i915_active_set_fence(struct i915_active *ref,\n\t\t\tstruct i915_active_fence *active,\n\t\t\tstruct dma_fence *fence)\n{\n\tstruct dma_fence *prev;\n\n\tif (replace_barrier(ref, active)) {\n\t\tRCU_INIT_POINTER(active->fence, fence);\n\t\treturn NULL;\n\t}\n\n\tprev = __i915_active_fence_set(active, fence);\n\tif (!prev)\n\t\t__i915_active_acquire(ref);\n\n\treturn prev;\n}\n\nstruct dma_fence *\ni915_active_set_exclusive(struct i915_active *ref, struct dma_fence *f)\n{\n\t \n\treturn __i915_active_set_fence(ref, &ref->excl, f);\n}\n\nbool i915_active_acquire_if_busy(struct i915_active *ref)\n{\n\tdebug_active_assert(ref);\n\treturn atomic_add_unless(&ref->count, 1, 0);\n}\n\nstatic void __i915_active_activate(struct i915_active *ref)\n{\n\tspin_lock_irq(&ref->tree_lock);  \n\tif (!atomic_fetch_inc(&ref->count))\n\t\tdebug_active_activate(ref);\n\tspin_unlock_irq(&ref->tree_lock);\n}\n\nint i915_active_acquire(struct i915_active *ref)\n{\n\tint err;\n\n\tif (i915_active_acquire_if_busy(ref))\n\t\treturn 0;\n\n\tif (!ref->active) {\n\t\t__i915_active_activate(ref);\n\t\treturn 0;\n\t}\n\n\terr = mutex_lock_interruptible(&ref->mutex);\n\tif (err)\n\t\treturn err;\n\n\tif (likely(!i915_active_acquire_if_busy(ref))) {\n\t\terr = ref->active(ref);\n\t\tif (!err)\n\t\t\t__i915_active_activate(ref);\n\t}\n\n\tmutex_unlock(&ref->mutex);\n\n\treturn err;\n}\n\nint i915_active_acquire_for_context(struct i915_active *ref, u64 idx)\n{\n\tstruct i915_active_fence *active;\n\tint err;\n\n\terr = i915_active_acquire(ref);\n\tif (err)\n\t\treturn err;\n\n\tactive = active_instance(ref, idx);\n\tif (!active) {\n\t\ti915_active_release(ref);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;  \n}\n\nvoid i915_active_release(struct i915_active *ref)\n{\n\tdebug_active_assert(ref);\n\tactive_retire(ref);\n}\n\nstatic void enable_signaling(struct i915_active_fence *active)\n{\n\tstruct dma_fence *fence;\n\n\tif (unlikely(is_barrier(active)))\n\t\treturn;\n\n\tfence = i915_active_fence_get(active);\n\tif (!fence)\n\t\treturn;\n\n\tdma_fence_enable_sw_signaling(fence);\n\tdma_fence_put(fence);\n}\n\nstatic int flush_barrier(struct active_node *it)\n{\n\tstruct intel_engine_cs *engine;\n\n\tif (likely(!is_barrier(&it->base)))\n\t\treturn 0;\n\n\tengine = __barrier_to_engine(it);\n\tsmp_rmb();  \n\tif (!is_barrier(&it->base))\n\t\treturn 0;\n\n\treturn intel_engine_flush_barriers(engine);\n}\n\nstatic int flush_lazy_signals(struct i915_active *ref)\n{\n\tstruct active_node *it, *n;\n\tint err = 0;\n\n\tenable_signaling(&ref->excl);\n\trbtree_postorder_for_each_entry_safe(it, n, &ref->tree, node) {\n\t\terr = flush_barrier(it);  \n\t\tif (err)\n\t\t\tbreak;\n\n\t\tenable_signaling(&it->base);\n\t}\n\n\treturn err;\n}\n\nint __i915_active_wait(struct i915_active *ref, int state)\n{\n\tmight_sleep();\n\n\t \n\tif (i915_active_acquire_if_busy(ref)) {\n\t\tint err;\n\n\t\terr = flush_lazy_signals(ref);\n\t\ti915_active_release(ref);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (___wait_var_event(ref, i915_active_is_idle(ref),\n\t\t\t\t      state, 0, 0, schedule()))\n\t\t\treturn -EINTR;\n\t}\n\n\t \n\tflush_work(&ref->work);\n\treturn 0;\n}\n\nstatic int __await_active(struct i915_active_fence *active,\n\t\t\t  int (*fn)(void *arg, struct dma_fence *fence),\n\t\t\t  void *arg)\n{\n\tstruct dma_fence *fence;\n\n\tif (is_barrier(active))  \n\t\treturn 0;\n\n\tfence = i915_active_fence_get(active);\n\tif (fence) {\n\t\tint err;\n\n\t\terr = fn(arg, fence);\n\t\tdma_fence_put(fence);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstruct wait_barrier {\n\tstruct wait_queue_entry base;\n\tstruct i915_active *ref;\n};\n\nstatic int\nbarrier_wake(wait_queue_entry_t *wq, unsigned int mode, int flags, void *key)\n{\n\tstruct wait_barrier *wb = container_of(wq, typeof(*wb), base);\n\n\tif (i915_active_is_idle(wb->ref)) {\n\t\tlist_del(&wq->entry);\n\t\ti915_sw_fence_complete(wq->private);\n\t\tkfree(wq);\n\t}\n\n\treturn 0;\n}\n\nstatic int __await_barrier(struct i915_active *ref, struct i915_sw_fence *fence)\n{\n\tstruct wait_barrier *wb;\n\n\twb = kmalloc(sizeof(*wb), GFP_KERNEL);\n\tif (unlikely(!wb))\n\t\treturn -ENOMEM;\n\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\tif (!i915_sw_fence_await(fence)) {\n\t\tkfree(wb);\n\t\treturn -EINVAL;\n\t}\n\n\twb->base.flags = 0;\n\twb->base.func = barrier_wake;\n\twb->base.private = fence;\n\twb->ref = ref;\n\n\tadd_wait_queue(__var_waitqueue(ref), &wb->base);\n\treturn 0;\n}\n\nstatic int await_active(struct i915_active *ref,\n\t\t\tunsigned int flags,\n\t\t\tint (*fn)(void *arg, struct dma_fence *fence),\n\t\t\tvoid *arg, struct i915_sw_fence *barrier)\n{\n\tint err = 0;\n\n\tif (!i915_active_acquire_if_busy(ref))\n\t\treturn 0;\n\n\tif (flags & I915_ACTIVE_AWAIT_EXCL &&\n\t    rcu_access_pointer(ref->excl.fence)) {\n\t\terr = __await_active(&ref->excl, fn, arg);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (flags & I915_ACTIVE_AWAIT_ACTIVE) {\n\t\tstruct active_node *it, *n;\n\n\t\trbtree_postorder_for_each_entry_safe(it, n, &ref->tree, node) {\n\t\t\terr = __await_active(&it->base, fn, arg);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (flags & I915_ACTIVE_AWAIT_BARRIER) {\n\t\terr = flush_lazy_signals(ref);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\terr = __await_barrier(ref, barrier);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\nout:\n\ti915_active_release(ref);\n\treturn err;\n}\n\nstatic int rq_await_fence(void *arg, struct dma_fence *fence)\n{\n\treturn i915_request_await_dma_fence(arg, fence);\n}\n\nint i915_request_await_active(struct i915_request *rq,\n\t\t\t      struct i915_active *ref,\n\t\t\t      unsigned int flags)\n{\n\treturn await_active(ref, flags, rq_await_fence, rq, &rq->submit);\n}\n\nstatic int sw_await_fence(void *arg, struct dma_fence *fence)\n{\n\treturn i915_sw_fence_await_dma_fence(arg, fence, 0,\n\t\t\t\t\t     GFP_NOWAIT | __GFP_NOWARN);\n}\n\nint i915_sw_fence_await_active(struct i915_sw_fence *fence,\n\t\t\t       struct i915_active *ref,\n\t\t\t       unsigned int flags)\n{\n\treturn await_active(ref, flags, sw_await_fence, fence, fence);\n}\n\nvoid i915_active_fini(struct i915_active *ref)\n{\n\tdebug_active_fini(ref);\n\tGEM_BUG_ON(atomic_read(&ref->count));\n\tGEM_BUG_ON(work_pending(&ref->work));\n\tmutex_destroy(&ref->mutex);\n\n\tif (ref->cache)\n\t\tkmem_cache_free(slab_cache, ref->cache);\n}\n\nstatic inline bool is_idle_barrier(struct active_node *node, u64 idx)\n{\n\treturn node->timeline == idx && !i915_active_fence_isset(&node->base);\n}\n\nstatic struct active_node *reuse_idle_barrier(struct i915_active *ref, u64 idx)\n{\n\tstruct rb_node *prev, *p;\n\n\tif (RB_EMPTY_ROOT(&ref->tree))\n\t\treturn NULL;\n\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\t \n\tif (ref->cache && is_idle_barrier(ref->cache, idx)) {\n\t\tp = &ref->cache->node;\n\t\tgoto match;\n\t}\n\n\tprev = NULL;\n\tp = ref->tree.rb_node;\n\twhile (p) {\n\t\tstruct active_node *node =\n\t\t\trb_entry(p, struct active_node, node);\n\n\t\tif (is_idle_barrier(node, idx))\n\t\t\tgoto match;\n\n\t\tprev = p;\n\t\tif (node->timeline < idx)\n\t\t\tp = READ_ONCE(p->rb_right);\n\t\telse\n\t\t\tp = READ_ONCE(p->rb_left);\n\t}\n\n\t \n\tfor (p = prev; p; p = rb_next(p)) {\n\t\tstruct active_node *node =\n\t\t\trb_entry(p, struct active_node, node);\n\t\tstruct intel_engine_cs *engine;\n\n\t\tif (node->timeline > idx)\n\t\t\tbreak;\n\n\t\tif (node->timeline < idx)\n\t\t\tcontinue;\n\n\t\tif (is_idle_barrier(node, idx))\n\t\t\tgoto match;\n\n\t\t \n\t\tengine = __barrier_to_engine(node);\n\t\tsmp_rmb();  \n\t\tif (is_barrier(&node->base) &&\n\t\t    ____active_del_barrier(ref, node, engine))\n\t\t\tgoto match;\n\t}\n\n\treturn NULL;\n\nmatch:\n\tspin_lock_irq(&ref->tree_lock);\n\trb_erase(p, &ref->tree);  \n\tif (p == &ref->cache->node)\n\t\tWRITE_ONCE(ref->cache, NULL);\n\tspin_unlock_irq(&ref->tree_lock);\n\n\treturn rb_entry(p, struct active_node, node);\n}\n\nint i915_active_acquire_preallocate_barrier(struct i915_active *ref,\n\t\t\t\t\t    struct intel_engine_cs *engine)\n{\n\tintel_engine_mask_t tmp, mask = engine->mask;\n\tstruct llist_node *first = NULL, *last = NULL;\n\tstruct intel_gt *gt = engine->gt;\n\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\t \n\twhile (!llist_empty(&ref->preallocated_barriers))\n\t\tcond_resched();\n\n\t \n\tGEM_BUG_ON(!mask);\n\tfor_each_engine_masked(engine, gt, mask, tmp) {\n\t\tu64 idx = engine->kernel_context->timeline->fence_context;\n\t\tstruct llist_node *prev = first;\n\t\tstruct active_node *node;\n\n\t\trcu_read_lock();\n\t\tnode = reuse_idle_barrier(ref, idx);\n\t\trcu_read_unlock();\n\t\tif (!node) {\n\t\t\tnode = kmem_cache_alloc(slab_cache, GFP_KERNEL);\n\t\t\tif (!node)\n\t\t\t\tgoto unwind;\n\n\t\t\tRCU_INIT_POINTER(node->base.fence, NULL);\n\t\t\tnode->base.cb.func = node_retire;\n\t\t\tnode->timeline = idx;\n\t\t\tnode->ref = ref;\n\t\t}\n\n\t\tif (!i915_active_fence_isset(&node->base)) {\n\t\t\t \n\t\t\tRCU_INIT_POINTER(node->base.fence, ERR_PTR(-EAGAIN));\n\t\t\tnode->base.cb.node.prev = (void *)engine;\n\t\t\t__i915_active_acquire(ref);\n\t\t}\n\t\tGEM_BUG_ON(rcu_access_pointer(node->base.fence) != ERR_PTR(-EAGAIN));\n\n\t\tGEM_BUG_ON(barrier_to_engine(node) != engine);\n\t\tfirst = barrier_to_ll(node);\n\t\tfirst->next = prev;\n\t\tif (!last)\n\t\t\tlast = first;\n\t\tintel_engine_pm_get(engine);\n\t}\n\n\tGEM_BUG_ON(!llist_empty(&ref->preallocated_barriers));\n\tllist_add_batch(first, last, &ref->preallocated_barriers);\n\n\treturn 0;\n\nunwind:\n\twhile (first) {\n\t\tstruct active_node *node = barrier_from_ll(first);\n\n\t\tfirst = first->next;\n\n\t\tatomic_dec(&ref->count);\n\t\tintel_engine_pm_put(barrier_to_engine(node));\n\n\t\tkmem_cache_free(slab_cache, node);\n\t}\n\treturn -ENOMEM;\n}\n\nvoid i915_active_acquire_barrier(struct i915_active *ref)\n{\n\tstruct llist_node *pos, *next;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(i915_active_is_idle(ref));\n\n\t \n\tllist_for_each_safe(pos, next, take_preallocated_barriers(ref)) {\n\t\tstruct active_node *node = barrier_from_ll(pos);\n\t\tstruct intel_engine_cs *engine = barrier_to_engine(node);\n\t\tstruct rb_node **p, *parent;\n\n\t\tspin_lock_irqsave_nested(&ref->tree_lock, flags,\n\t\t\t\t\t SINGLE_DEPTH_NESTING);\n\t\tparent = NULL;\n\t\tp = &ref->tree.rb_node;\n\t\twhile (*p) {\n\t\t\tstruct active_node *it;\n\n\t\t\tparent = *p;\n\n\t\t\tit = rb_entry(parent, struct active_node, node);\n\t\t\tif (it->timeline < node->timeline)\n\t\t\t\tp = &parent->rb_right;\n\t\t\telse\n\t\t\t\tp = &parent->rb_left;\n\t\t}\n\t\trb_link_node(&node->node, parent, p);\n\t\trb_insert_color(&node->node, &ref->tree);\n\t\tspin_unlock_irqrestore(&ref->tree_lock, flags);\n\n\t\tGEM_BUG_ON(!intel_engine_pm_is_awake(engine));\n\t\tllist_add(barrier_to_ll(node), &engine->barrier_tasks);\n\t\tintel_engine_pm_put_delay(engine, 2);\n\t}\n}\n\nstatic struct dma_fence **ll_to_fence_slot(struct llist_node *node)\n{\n\treturn __active_fence_slot(&barrier_from_ll(node)->base);\n}\n\nvoid i915_request_add_active_barriers(struct i915_request *rq)\n{\n\tstruct intel_engine_cs *engine = rq->engine;\n\tstruct llist_node *node, *next;\n\tunsigned long flags;\n\n\tGEM_BUG_ON(!intel_context_is_barrier(rq->context));\n\tGEM_BUG_ON(intel_engine_is_virtual(engine));\n\tGEM_BUG_ON(i915_request_timeline(rq) != engine->kernel_context->timeline);\n\n\tnode = llist_del_all(&engine->barrier_tasks);\n\tif (!node)\n\t\treturn;\n\t \n\tspin_lock_irqsave(&rq->lock, flags);\n\tllist_for_each_safe(node, next, node) {\n\t\t \n\t\tsmp_store_mb(*ll_to_fence_slot(node), &rq->fence);\n\t\tlist_add_tail((struct list_head *)node, &rq->fence.cb_list);\n\t}\n\tspin_unlock_irqrestore(&rq->lock, flags);\n}\n\n \nstruct dma_fence *\n__i915_active_fence_set(struct i915_active_fence *active,\n\t\t\tstruct dma_fence *fence)\n{\n\tstruct dma_fence *prev;\n\tunsigned long flags;\n\n\t \n\tprev = i915_active_fence_get(active);\n\tif (fence == prev)\n\t\treturn fence;\n\n\tGEM_BUG_ON(test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags));\n\n\t \n\tspin_lock_irqsave(fence->lock, flags);\n\tif (prev)\n\t\tspin_lock_nested(prev->lock, SINGLE_DEPTH_NESTING);\n\n\t \n\twhile (cmpxchg(__active_fence_slot(active), prev, fence) != prev) {\n\t\tif (prev) {\n\t\t\tspin_unlock(prev->lock);\n\t\t\tdma_fence_put(prev);\n\t\t}\n\t\tspin_unlock_irqrestore(fence->lock, flags);\n\n\t\tprev = i915_active_fence_get(active);\n\t\tGEM_BUG_ON(prev == fence);\n\n\t\tspin_lock_irqsave(fence->lock, flags);\n\t\tif (prev)\n\t\t\tspin_lock_nested(prev->lock, SINGLE_DEPTH_NESTING);\n\t}\n\n\t \n\tif (prev) {\n\t\t__list_del_entry(&active->cb.node);\n\t\tspin_unlock(prev->lock);  \n\t}\n\tlist_add_tail(&active->cb.node, &fence->cb_list);\n\tspin_unlock_irqrestore(fence->lock, flags);\n\n\treturn prev;\n}\n\nint i915_active_fence_set(struct i915_active_fence *active,\n\t\t\t  struct i915_request *rq)\n{\n\tstruct dma_fence *fence;\n\tint err = 0;\n\n\t \n\tfence = __i915_active_fence_set(active, &rq->fence);\n\tif (fence) {\n\t\terr = i915_request_await_dma_fence(rq, fence);\n\t\tdma_fence_put(fence);\n\t}\n\n\treturn err;\n}\n\nvoid i915_active_noop(struct dma_fence *fence, struct dma_fence_cb *cb)\n{\n\tactive_fence_cb(fence, cb);\n}\n\nstruct auto_active {\n\tstruct i915_active base;\n\tstruct kref ref;\n};\n\nstruct i915_active *i915_active_get(struct i915_active *ref)\n{\n\tstruct auto_active *aa = container_of(ref, typeof(*aa), base);\n\n\tkref_get(&aa->ref);\n\treturn &aa->base;\n}\n\nstatic void auto_release(struct kref *ref)\n{\n\tstruct auto_active *aa = container_of(ref, typeof(*aa), ref);\n\n\ti915_active_fini(&aa->base);\n\tkfree(aa);\n}\n\nvoid i915_active_put(struct i915_active *ref)\n{\n\tstruct auto_active *aa = container_of(ref, typeof(*aa), base);\n\n\tkref_put(&aa->ref, auto_release);\n}\n\nstatic int auto_active(struct i915_active *ref)\n{\n\ti915_active_get(ref);\n\treturn 0;\n}\n\nstatic void auto_retire(struct i915_active *ref)\n{\n\ti915_active_put(ref);\n}\n\nstruct i915_active *i915_active_create(void)\n{\n\tstruct auto_active *aa;\n\n\taa = kmalloc(sizeof(*aa), GFP_KERNEL);\n\tif (!aa)\n\t\treturn NULL;\n\n\tkref_init(&aa->ref);\n\ti915_active_init(&aa->base, auto_active, auto_retire, 0);\n\n\treturn &aa->base;\n}\n\n#if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)\n#include \"selftests/i915_active.c\"\n#endif\n\nvoid i915_active_module_exit(void)\n{\n\tkmem_cache_destroy(slab_cache);\n}\n\nint __init i915_active_module_init(void)\n{\n\tslab_cache = KMEM_CACHE(active_node, SLAB_HWCACHE_ALIGN);\n\tif (!slab_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}