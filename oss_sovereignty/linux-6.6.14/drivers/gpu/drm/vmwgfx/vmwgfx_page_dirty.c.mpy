{
  "module_name": "vmwgfx_page_dirty.c",
  "hash_id": "e0c4bc7999a0667681ecbdc8c395d1dd92d0684762ba7d4eea2b9a97335c8f7f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/vmwgfx/vmwgfx_page_dirty.c",
  "human_readable_source": "\n \n#include \"vmwgfx_bo.h\"\n#include \"vmwgfx_drv.h\"\n\n \nenum vmw_bo_dirty_method {\n\tVMW_BO_DIRTY_PAGETABLE,\n\tVMW_BO_DIRTY_MKWRITE,\n};\n\n \n#define VMW_DIRTY_NUM_CHANGE_TRIGGERS 2\n\n \n#define VMW_DIRTY_PERCENTAGE 10\n\n \nstruct vmw_bo_dirty {\n\tunsigned long start;\n\tunsigned long end;\n\tenum vmw_bo_dirty_method method;\n\tunsigned int change_count;\n\tunsigned int ref_count;\n\tunsigned long bitmap_size;\n\tunsigned long bitmap[];\n};\n\n \nstatic void vmw_bo_dirty_scan_pagetable(struct vmw_bo *vbo)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\tpgoff_t offset = drm_vma_node_start(&vbo->tbo.base.vma_node);\n\tstruct address_space *mapping = vbo->tbo.bdev->dev_mapping;\n\tpgoff_t num_marked;\n\n\tnum_marked = clean_record_shared_mapping_range\n\t\t(mapping,\n\t\t offset, dirty->bitmap_size,\n\t\t offset, &dirty->bitmap[0],\n\t\t &dirty->start, &dirty->end);\n\tif (num_marked == 0)\n\t\tdirty->change_count++;\n\telse\n\t\tdirty->change_count = 0;\n\n\tif (dirty->change_count > VMW_DIRTY_NUM_CHANGE_TRIGGERS) {\n\t\tdirty->change_count = 0;\n\t\tdirty->method = VMW_BO_DIRTY_MKWRITE;\n\t\twp_shared_mapping_range(mapping,\n\t\t\t\t\toffset, dirty->bitmap_size);\n\t\tclean_record_shared_mapping_range(mapping,\n\t\t\t\t\t\t  offset, dirty->bitmap_size,\n\t\t\t\t\t\t  offset, &dirty->bitmap[0],\n\t\t\t\t\t\t  &dirty->start, &dirty->end);\n\t}\n}\n\n \nstatic void vmw_bo_dirty_scan_mkwrite(struct vmw_bo *vbo)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\tunsigned long offset = drm_vma_node_start(&vbo->tbo.base.vma_node);\n\tstruct address_space *mapping = vbo->tbo.bdev->dev_mapping;\n\tpgoff_t num_marked;\n\n\tif (dirty->end <= dirty->start)\n\t\treturn;\n\n\tnum_marked = wp_shared_mapping_range(vbo->tbo.bdev->dev_mapping,\n\t\t\t\t\t     dirty->start + offset,\n\t\t\t\t\t     dirty->end - dirty->start);\n\n\tif (100UL * num_marked / dirty->bitmap_size >\n\t    VMW_DIRTY_PERCENTAGE)\n\t\tdirty->change_count++;\n\telse\n\t\tdirty->change_count = 0;\n\n\tif (dirty->change_count > VMW_DIRTY_NUM_CHANGE_TRIGGERS) {\n\t\tpgoff_t start = 0;\n\t\tpgoff_t end = dirty->bitmap_size;\n\n\t\tdirty->method = VMW_BO_DIRTY_PAGETABLE;\n\t\tclean_record_shared_mapping_range(mapping, offset, end, offset,\n\t\t\t\t\t\t  &dirty->bitmap[0],\n\t\t\t\t\t\t  &start, &end);\n\t\tbitmap_clear(&dirty->bitmap[0], 0, dirty->bitmap_size);\n\t\tif (dirty->start < dirty->end)\n\t\t\tbitmap_set(&dirty->bitmap[0], dirty->start,\n\t\t\t\t   dirty->end - dirty->start);\n\t\tdirty->change_count = 0;\n\t}\n}\n\n \nvoid vmw_bo_dirty_scan(struct vmw_bo *vbo)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\n\tif (dirty->method == VMW_BO_DIRTY_PAGETABLE)\n\t\tvmw_bo_dirty_scan_pagetable(vbo);\n\telse\n\t\tvmw_bo_dirty_scan_mkwrite(vbo);\n}\n\n \nstatic void vmw_bo_dirty_pre_unmap(struct vmw_bo *vbo,\n\t\t\t\t   pgoff_t start, pgoff_t end)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\tunsigned long offset = drm_vma_node_start(&vbo->tbo.base.vma_node);\n\tstruct address_space *mapping = vbo->tbo.bdev->dev_mapping;\n\n\tif (dirty->method != VMW_BO_DIRTY_PAGETABLE || start >= end)\n\t\treturn;\n\n\twp_shared_mapping_range(mapping, start + offset, end - start);\n\tclean_record_shared_mapping_range(mapping, start + offset,\n\t\t\t\t\t  end - start, offset,\n\t\t\t\t\t  &dirty->bitmap[0], &dirty->start,\n\t\t\t\t\t  &dirty->end);\n}\n\n \nvoid vmw_bo_dirty_unmap(struct vmw_bo *vbo,\n\t\t\tpgoff_t start, pgoff_t end)\n{\n\tunsigned long offset = drm_vma_node_start(&vbo->tbo.base.vma_node);\n\tstruct address_space *mapping = vbo->tbo.bdev->dev_mapping;\n\n\tvmw_bo_dirty_pre_unmap(vbo, start, end);\n\tunmap_shared_mapping_range(mapping, (offset + start) << PAGE_SHIFT,\n\t\t\t\t   (loff_t) (end - start) << PAGE_SHIFT);\n}\n\n \nint vmw_bo_dirty_add(struct vmw_bo *vbo)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\tpgoff_t num_pages = PFN_UP(vbo->tbo.resource->size);\n\tsize_t size;\n\tint ret;\n\n\tif (dirty) {\n\t\tdirty->ref_count++;\n\t\treturn 0;\n\t}\n\n\tsize = sizeof(*dirty) + BITS_TO_LONGS(num_pages) * sizeof(long);\n\tdirty = kvzalloc(size, GFP_KERNEL);\n\tif (!dirty) {\n\t\tret = -ENOMEM;\n\t\tgoto out_no_dirty;\n\t}\n\n\tdirty->bitmap_size = num_pages;\n\tdirty->start = dirty->bitmap_size;\n\tdirty->end = 0;\n\tdirty->ref_count = 1;\n\tif (num_pages < PAGE_SIZE / sizeof(pte_t)) {\n\t\tdirty->method = VMW_BO_DIRTY_PAGETABLE;\n\t} else {\n\t\tstruct address_space *mapping = vbo->tbo.bdev->dev_mapping;\n\t\tpgoff_t offset = drm_vma_node_start(&vbo->tbo.base.vma_node);\n\n\t\tdirty->method = VMW_BO_DIRTY_MKWRITE;\n\n\t\t \n\t\twp_shared_mapping_range(mapping, offset, num_pages);\n\t\tclean_record_shared_mapping_range(mapping, offset, num_pages,\n\t\t\t\t\t\t  offset,\n\t\t\t\t\t\t  &dirty->bitmap[0],\n\t\t\t\t\t\t  &dirty->start, &dirty->end);\n\t}\n\n\tvbo->dirty = dirty;\n\n\treturn 0;\n\nout_no_dirty:\n\treturn ret;\n}\n\n \nvoid vmw_bo_dirty_release(struct vmw_bo *vbo)\n{\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\n\tif (dirty && --dirty->ref_count == 0) {\n\t\tkvfree(dirty);\n\t\tvbo->dirty = NULL;\n\t}\n}\n\n \nvoid vmw_bo_dirty_transfer_to_res(struct vmw_resource *res)\n{\n\tstruct vmw_bo *vbo = res->guest_memory_bo;\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\tpgoff_t start, cur, end;\n\tunsigned long res_start = res->guest_memory_offset;\n\tunsigned long res_end = res->guest_memory_offset + res->guest_memory_size;\n\n\tWARN_ON_ONCE(res_start & ~PAGE_MASK);\n\tres_start >>= PAGE_SHIFT;\n\tres_end = DIV_ROUND_UP(res_end, PAGE_SIZE);\n\n\tif (res_start >= dirty->end || res_end <= dirty->start)\n\t\treturn;\n\n\tcur = max(res_start, dirty->start);\n\tres_end = max(res_end, dirty->end);\n\twhile (cur < res_end) {\n\t\tunsigned long num;\n\n\t\tstart = find_next_bit(&dirty->bitmap[0], res_end, cur);\n\t\tif (start >= res_end)\n\t\t\tbreak;\n\n\t\tend = find_next_zero_bit(&dirty->bitmap[0], res_end, start + 1);\n\t\tcur = end + 1;\n\t\tnum = end - start;\n\t\tbitmap_clear(&dirty->bitmap[0], start, num);\n\t\tvmw_resource_dirty_update(res, start, end);\n\t}\n\n\tif (res_start <= dirty->start && res_end > dirty->start)\n\t\tdirty->start = res_end;\n\tif (res_start < dirty->end && res_end >= dirty->end)\n\t\tdirty->end = res_start;\n}\n\n \nvoid vmw_bo_dirty_clear_res(struct vmw_resource *res)\n{\n\tunsigned long res_start = res->guest_memory_offset;\n\tunsigned long res_end = res->guest_memory_offset + res->guest_memory_size;\n\tstruct vmw_bo *vbo = res->guest_memory_bo;\n\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\n\tres_start >>= PAGE_SHIFT;\n\tres_end = DIV_ROUND_UP(res_end, PAGE_SIZE);\n\n\tif (res_start >= dirty->end || res_end <= dirty->start)\n\t\treturn;\n\n\tres_start = max(res_start, dirty->start);\n\tres_end = min(res_end, dirty->end);\n\tbitmap_clear(&dirty->bitmap[0], res_start, res_end - res_start);\n\n\tif (res_start <= dirty->start && res_end > dirty->start)\n\t\tdirty->start = res_end;\n\tif (res_start < dirty->end && res_end >= dirty->end)\n\t\tdirty->end = res_start;\n}\n\nvm_fault_t vmw_bo_vm_mkwrite(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct ttm_buffer_object *bo = (struct ttm_buffer_object *)\n\t    vma->vm_private_data;\n\tvm_fault_t ret;\n\tunsigned long page_offset;\n\tunsigned int save_flags;\n\tstruct vmw_bo *vbo = to_vmw_bo(&bo->base);\n\n\t \n\tsave_flags = vmf->flags;\n\tvmf->flags &= ~FAULT_FLAG_ALLOW_RETRY;\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tvmf->flags = save_flags;\n\tif (ret)\n\t\treturn ret;\n\n\tpage_offset = vmf->pgoff - drm_vma_node_start(&bo->base.vma_node);\n\tif (unlikely(page_offset >= PFN_UP(bo->resource->size))) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out_unlock;\n\t}\n\n\tif (vbo->dirty && vbo->dirty->method == VMW_BO_DIRTY_MKWRITE &&\n\t    !test_bit(page_offset, &vbo->dirty->bitmap[0])) {\n\t\tstruct vmw_bo_dirty *dirty = vbo->dirty;\n\n\t\t__set_bit(page_offset, &dirty->bitmap[0]);\n\t\tdirty->start = min(dirty->start, page_offset);\n\t\tdirty->end = max(dirty->end, page_offset + 1);\n\t}\n\nout_unlock:\n\tdma_resv_unlock(bo->base.resv);\n\treturn ret;\n}\n\nvm_fault_t vmw_bo_vm_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct ttm_buffer_object *bo = (struct ttm_buffer_object *)\n\t    vma->vm_private_data;\n\tstruct vmw_bo *vbo = to_vmw_bo(&bo->base);\n\tpgoff_t num_prefault;\n\tpgprot_t prot;\n\tvm_fault_t ret;\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\treturn ret;\n\n\tnum_prefault = (vma->vm_flags & VM_RAND_READ) ? 1 :\n\t\tTTM_BO_VM_NUM_PREFAULT;\n\n\tif (vbo->dirty) {\n\t\tpgoff_t allowed_prefault;\n\t\tunsigned long page_offset;\n\n\t\tpage_offset = vmf->pgoff -\n\t\t\tdrm_vma_node_start(&bo->base.vma_node);\n\t\tif (page_offset >= PFN_UP(bo->resource->size) ||\n\t\t    vmw_resources_clean(vbo, page_offset,\n\t\t\t\t\tpage_offset + PAGE_SIZE,\n\t\t\t\t\t&allowed_prefault)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tnum_prefault = min(num_prefault, allowed_prefault);\n\t}\n\n\t \n\tif (vbo->dirty && vbo->dirty->method == VMW_BO_DIRTY_MKWRITE)\n\t\tprot = vm_get_page_prot(vma->vm_flags & ~VM_SHARED);\n\telse\n\t\tprot = vm_get_page_prot(vma->vm_flags);\n\n\tret = ttm_bo_vm_fault_reserved(vmf, prot, num_prefault);\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\treturn ret;\n\nout_unlock:\n\tdma_resv_unlock(bo->base.resv);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}