{
  "module_name": "vmwgfx_ttm_buffer.c",
  "hash_id": "5109e9815035348c87bda6ae317f3a23c7511cb75777f424e30e82ff800f6894",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/vmwgfx/vmwgfx_ttm_buffer.c",
  "human_readable_source": "\n \n\n#include \"vmwgfx_bo.h\"\n#include \"vmwgfx_drv.h\"\n#include <drm/ttm/ttm_placement.h>\n\nstatic const struct ttm_place vram_placement_flags = {\n\t.fpfn = 0,\n\t.lpfn = 0,\n\t.mem_type = TTM_PL_VRAM,\n\t.flags = 0\n};\n\nstatic const struct ttm_place sys_placement_flags = {\n\t.fpfn = 0,\n\t.lpfn = 0,\n\t.mem_type = TTM_PL_SYSTEM,\n\t.flags = 0\n};\n\nstatic const struct ttm_place gmr_placement_flags = {\n\t.fpfn = 0,\n\t.lpfn = 0,\n\t.mem_type = VMW_PL_GMR,\n\t.flags = 0\n};\n\nstruct ttm_placement vmw_vram_placement = {\n\t.num_placement = 1,\n\t.placement = &vram_placement_flags,\n\t.num_busy_placement = 1,\n\t.busy_placement = &vram_placement_flags\n};\n\nstatic const struct ttm_place vram_gmr_placement_flags[] = {\n\t{\n\t\t.fpfn = 0,\n\t\t.lpfn = 0,\n\t\t.mem_type = TTM_PL_VRAM,\n\t\t.flags = 0\n\t}, {\n\t\t.fpfn = 0,\n\t\t.lpfn = 0,\n\t\t.mem_type = VMW_PL_GMR,\n\t\t.flags = 0\n\t}\n};\n\nstruct ttm_placement vmw_vram_gmr_placement = {\n\t.num_placement = 2,\n\t.placement = vram_gmr_placement_flags,\n\t.num_busy_placement = 1,\n\t.busy_placement = &gmr_placement_flags\n};\n\nstruct ttm_placement vmw_sys_placement = {\n\t.num_placement = 1,\n\t.placement = &sys_placement_flags,\n\t.num_busy_placement = 1,\n\t.busy_placement = &sys_placement_flags\n};\n\nconst size_t vmw_tt_size = sizeof(struct vmw_ttm_tt);\n\n \nstatic bool __vmw_piter_non_sg_next(struct vmw_piter *viter)\n{\n\treturn ++(viter->i) < viter->num_pages;\n}\n\nstatic bool __vmw_piter_sg_next(struct vmw_piter *viter)\n{\n\tbool ret = __vmw_piter_non_sg_next(viter);\n\n\treturn __sg_page_iter_dma_next(&viter->iter) && ret;\n}\n\n\nstatic dma_addr_t __vmw_piter_dma_addr(struct vmw_piter *viter)\n{\n\treturn viter->addrs[viter->i];\n}\n\nstatic dma_addr_t __vmw_piter_sg_addr(struct vmw_piter *viter)\n{\n\treturn sg_page_iter_dma_address(&viter->iter);\n}\n\n\n \nvoid vmw_piter_start(struct vmw_piter *viter, const struct vmw_sg_table *vsgt,\n\t\t     unsigned long p_offset)\n{\n\tviter->i = p_offset - 1;\n\tviter->num_pages = vsgt->num_pages;\n\tviter->pages = vsgt->pages;\n\tswitch (vsgt->mode) {\n\tcase vmw_dma_alloc_coherent:\n\t\tviter->next = &__vmw_piter_non_sg_next;\n\t\tviter->dma_address = &__vmw_piter_dma_addr;\n\t\tviter->addrs = vsgt->addrs;\n\t\tbreak;\n\tcase vmw_dma_map_populate:\n\tcase vmw_dma_map_bind:\n\t\tviter->next = &__vmw_piter_sg_next;\n\t\tviter->dma_address = &__vmw_piter_sg_addr;\n\t\t__sg_page_iter_start(&viter->iter.base, vsgt->sgt->sgl,\n\t\t\t\t     vsgt->sgt->orig_nents, p_offset);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\n \nstatic void vmw_ttm_unmap_from_dma(struct vmw_ttm_tt *vmw_tt)\n{\n\tstruct device *dev = vmw_tt->dev_priv->drm.dev;\n\n\tdma_unmap_sgtable(dev, &vmw_tt->sgt, DMA_BIDIRECTIONAL, 0);\n\tvmw_tt->sgt.nents = vmw_tt->sgt.orig_nents;\n}\n\n \nstatic int vmw_ttm_map_for_dma(struct vmw_ttm_tt *vmw_tt)\n{\n\tstruct device *dev = vmw_tt->dev_priv->drm.dev;\n\n\treturn dma_map_sgtable(dev, &vmw_tt->sgt, DMA_BIDIRECTIONAL, 0);\n}\n\n \nstatic int vmw_ttm_map_dma(struct vmw_ttm_tt *vmw_tt)\n{\n\tstruct vmw_private *dev_priv = vmw_tt->dev_priv;\n\tstruct vmw_sg_table *vsgt = &vmw_tt->vsgt;\n\tint ret = 0;\n\n\tif (vmw_tt->mapped)\n\t\treturn 0;\n\n\tvsgt->mode = dev_priv->map_mode;\n\tvsgt->pages = vmw_tt->dma_ttm.pages;\n\tvsgt->num_pages = vmw_tt->dma_ttm.num_pages;\n\tvsgt->addrs = vmw_tt->dma_ttm.dma_address;\n\tvsgt->sgt = NULL;\n\n\tswitch (dev_priv->map_mode) {\n\tcase vmw_dma_map_bind:\n\tcase vmw_dma_map_populate:\n\t\tvsgt->sgt = &vmw_tt->sgt;\n\t\tret = sg_alloc_table_from_pages_segment(\n\t\t\t&vmw_tt->sgt, vsgt->pages, vsgt->num_pages, 0,\n\t\t\t(unsigned long)vsgt->num_pages << PAGE_SHIFT,\n\t\t\tdma_get_max_seg_size(dev_priv->drm.dev), GFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto out_sg_alloc_fail;\n\n\t\tret = vmw_ttm_map_for_dma(vmw_tt);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_map_fail;\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tvmw_tt->mapped = true;\n\treturn 0;\n\nout_map_fail:\n\tsg_free_table(vmw_tt->vsgt.sgt);\n\tvmw_tt->vsgt.sgt = NULL;\nout_sg_alloc_fail:\n\treturn ret;\n}\n\n \nstatic void vmw_ttm_unmap_dma(struct vmw_ttm_tt *vmw_tt)\n{\n\tstruct vmw_private *dev_priv = vmw_tt->dev_priv;\n\n\tif (!vmw_tt->vsgt.sgt)\n\t\treturn;\n\n\tswitch (dev_priv->map_mode) {\n\tcase vmw_dma_map_bind:\n\tcase vmw_dma_map_populate:\n\t\tvmw_ttm_unmap_from_dma(vmw_tt);\n\t\tsg_free_table(vmw_tt->vsgt.sgt);\n\t\tvmw_tt->vsgt.sgt = NULL;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tvmw_tt->mapped = false;\n}\n\n \nconst struct vmw_sg_table *vmw_bo_sg_table(struct ttm_buffer_object *bo)\n{\n\tstruct vmw_ttm_tt *vmw_tt =\n\t\tcontainer_of(bo->ttm, struct vmw_ttm_tt, dma_ttm);\n\n\treturn &vmw_tt->vsgt;\n}\n\n\nstatic int vmw_ttm_bind(struct ttm_device *bdev,\n\t\t\tstruct ttm_tt *ttm, struct ttm_resource *bo_mem)\n{\n\tstruct vmw_ttm_tt *vmw_be =\n\t\tcontainer_of(ttm, struct vmw_ttm_tt, dma_ttm);\n\tint ret = 0;\n\n\tif (!bo_mem)\n\t\treturn -EINVAL;\n\n\tif (vmw_be->bound)\n\t\treturn 0;\n\n\tret = vmw_ttm_map_dma(vmw_be);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tvmw_be->gmr_id = bo_mem->start;\n\tvmw_be->mem_type = bo_mem->mem_type;\n\n\tswitch (bo_mem->mem_type) {\n\tcase VMW_PL_GMR:\n\t\tret = vmw_gmr_bind(vmw_be->dev_priv, &vmw_be->vsgt,\n\t\t\t\t    ttm->num_pages, vmw_be->gmr_id);\n\t\tbreak;\n\tcase VMW_PL_MOB:\n\t\tif (unlikely(vmw_be->mob == NULL)) {\n\t\t\tvmw_be->mob =\n\t\t\t\tvmw_mob_create(ttm->num_pages);\n\t\t\tif (unlikely(vmw_be->mob == NULL))\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tret = vmw_mob_bind(vmw_be->dev_priv, vmw_be->mob,\n\t\t\t\t    &vmw_be->vsgt, ttm->num_pages,\n\t\t\t\t    vmw_be->gmr_id);\n\t\tbreak;\n\tcase VMW_PL_SYSTEM:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\tvmw_be->bound = true;\n\treturn ret;\n}\n\nstatic void vmw_ttm_unbind(struct ttm_device *bdev,\n\t\t\t   struct ttm_tt *ttm)\n{\n\tstruct vmw_ttm_tt *vmw_be =\n\t\tcontainer_of(ttm, struct vmw_ttm_tt, dma_ttm);\n\n\tif (!vmw_be->bound)\n\t\treturn;\n\n\tswitch (vmw_be->mem_type) {\n\tcase VMW_PL_GMR:\n\t\tvmw_gmr_unbind(vmw_be->dev_priv, vmw_be->gmr_id);\n\t\tbreak;\n\tcase VMW_PL_MOB:\n\t\tvmw_mob_unbind(vmw_be->dev_priv, vmw_be->mob);\n\t\tbreak;\n\tcase VMW_PL_SYSTEM:\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tif (vmw_be->dev_priv->map_mode == vmw_dma_map_bind)\n\t\tvmw_ttm_unmap_dma(vmw_be);\n\tvmw_be->bound = false;\n}\n\n\nstatic void vmw_ttm_destroy(struct ttm_device *bdev, struct ttm_tt *ttm)\n{\n\tstruct vmw_ttm_tt *vmw_be =\n\t\tcontainer_of(ttm, struct vmw_ttm_tt, dma_ttm);\n\n\tvmw_ttm_unmap_dma(vmw_be);\n\tttm_tt_fini(ttm);\n\tif (vmw_be->mob)\n\t\tvmw_mob_destroy(vmw_be->mob);\n\n\tkfree(vmw_be);\n}\n\n\nstatic int vmw_ttm_populate(struct ttm_device *bdev,\n\t\t\t    struct ttm_tt *ttm, struct ttm_operation_ctx *ctx)\n{\n\tint ret;\n\n\t \n\tif (ttm_tt_is_populated(ttm))\n\t\treturn 0;\n\n\tret = ttm_pool_alloc(&bdev->pool, ttm, ctx);\n\n\treturn ret;\n}\n\nstatic void vmw_ttm_unpopulate(struct ttm_device *bdev,\n\t\t\t       struct ttm_tt *ttm)\n{\n\tstruct vmw_ttm_tt *vmw_tt = container_of(ttm, struct vmw_ttm_tt,\n\t\t\t\t\t\t dma_ttm);\n\n\tvmw_ttm_unbind(bdev, ttm);\n\n\tif (vmw_tt->mob) {\n\t\tvmw_mob_destroy(vmw_tt->mob);\n\t\tvmw_tt->mob = NULL;\n\t}\n\n\tvmw_ttm_unmap_dma(vmw_tt);\n\n\tttm_pool_free(&bdev->pool, ttm);\n}\n\nstatic struct ttm_tt *vmw_ttm_tt_create(struct ttm_buffer_object *bo,\n\t\t\t\t\tuint32_t page_flags)\n{\n\tstruct vmw_ttm_tt *vmw_be;\n\tint ret;\n\n\tvmw_be = kzalloc(sizeof(*vmw_be), GFP_KERNEL);\n\tif (!vmw_be)\n\t\treturn NULL;\n\n\tvmw_be->dev_priv = vmw_priv_from_ttm(bo->bdev);\n\tvmw_be->mob = NULL;\n\n\tif (vmw_be->dev_priv->map_mode == vmw_dma_alloc_coherent)\n\t\tret = ttm_sg_tt_init(&vmw_be->dma_ttm, bo, page_flags,\n\t\t\t\t     ttm_cached);\n\telse\n\t\tret = ttm_tt_init(&vmw_be->dma_ttm, bo, page_flags,\n\t\t\t\t  ttm_cached, 0);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_init;\n\n\treturn &vmw_be->dma_ttm;\nout_no_init:\n\tkfree(vmw_be);\n\treturn NULL;\n}\n\nstatic void vmw_evict_flags(struct ttm_buffer_object *bo,\n\t\t     struct ttm_placement *placement)\n{\n\t*placement = vmw_sys_placement;\n}\n\nstatic int vmw_ttm_io_mem_reserve(struct ttm_device *bdev, struct ttm_resource *mem)\n{\n\tstruct vmw_private *dev_priv = vmw_priv_from_ttm(bdev);\n\n\tswitch (mem->mem_type) {\n\tcase TTM_PL_SYSTEM:\n\tcase VMW_PL_SYSTEM:\n\tcase VMW_PL_GMR:\n\tcase VMW_PL_MOB:\n\t\treturn 0;\n\tcase TTM_PL_VRAM:\n\t\tmem->bus.offset = (mem->start << PAGE_SHIFT) +\n\t\t\tdev_priv->vram_start;\n\t\tmem->bus.is_iomem = true;\n\t\tmem->bus.caching = ttm_cached;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic void vmw_move_notify(struct ttm_buffer_object *bo,\n\t\t\t    struct ttm_resource *old_mem,\n\t\t\t    struct ttm_resource *new_mem)\n{\n\tvmw_bo_move_notify(bo, new_mem);\n\tvmw_query_move_notify(bo, old_mem, new_mem);\n}\n\n\n \nstatic void vmw_swap_notify(struct ttm_buffer_object *bo)\n{\n\tvmw_bo_swap_notify(bo);\n\t(void) ttm_bo_wait(bo, false, false);\n}\n\nstatic bool vmw_memtype_is_system(uint32_t mem_type)\n{\n\treturn mem_type == TTM_PL_SYSTEM || mem_type == VMW_PL_SYSTEM;\n}\n\nstatic int vmw_move(struct ttm_buffer_object *bo,\n\t\t    bool evict,\n\t\t    struct ttm_operation_ctx *ctx,\n\t\t    struct ttm_resource *new_mem,\n\t\t    struct ttm_place *hop)\n{\n\tstruct ttm_resource_manager *new_man;\n\tstruct ttm_resource_manager *old_man = NULL;\n\tint ret = 0;\n\n\tnew_man = ttm_manager_type(bo->bdev, new_mem->mem_type);\n\tif (bo->resource)\n\t\told_man = ttm_manager_type(bo->bdev, bo->resource->mem_type);\n\n\tif (new_man->use_tt && !vmw_memtype_is_system(new_mem->mem_type)) {\n\t\tret = vmw_ttm_bind(bo->bdev, bo->ttm, new_mem);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (!bo->resource || (bo->resource->mem_type == TTM_PL_SYSTEM &&\n\t\t\t      bo->ttm == NULL)) {\n\t\tttm_bo_move_null(bo, new_mem);\n\t\treturn 0;\n\t}\n\n\tvmw_move_notify(bo, bo->resource, new_mem);\n\n\tif (old_man && old_man->use_tt && new_man->use_tt) {\n\t\tif (vmw_memtype_is_system(bo->resource->mem_type)) {\n\t\t\tttm_bo_move_null(bo, new_mem);\n\t\t\treturn 0;\n\t\t}\n\t\tret = ttm_bo_wait_ctx(bo, ctx);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t\tvmw_ttm_unbind(bo->bdev, bo->ttm);\n\t\tttm_resource_free(bo, &bo->resource);\n\t\tttm_bo_assign_mem(bo, new_mem);\n\t\treturn 0;\n\t} else {\n\t\tret = ttm_bo_move_memcpy(bo, ctx, new_mem);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\treturn 0;\nfail:\n\tvmw_move_notify(bo, new_mem, bo->resource);\n\treturn ret;\n}\n\nstruct ttm_device_funcs vmw_bo_driver = {\n\t.ttm_tt_create = &vmw_ttm_tt_create,\n\t.ttm_tt_populate = &vmw_ttm_populate,\n\t.ttm_tt_unpopulate = &vmw_ttm_unpopulate,\n\t.ttm_tt_destroy = &vmw_ttm_destroy,\n\t.eviction_valuable = ttm_bo_eviction_valuable,\n\t.evict_flags = vmw_evict_flags,\n\t.move = vmw_move,\n\t.swap_notify = vmw_swap_notify,\n\t.io_mem_reserve = &vmw_ttm_io_mem_reserve,\n};\n\nint vmw_bo_create_and_populate(struct vmw_private *dev_priv,\n\t\t\t       size_t bo_size, u32 domain,\n\t\t\t       struct vmw_bo **bo_p)\n{\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = false,\n\t\t.no_wait_gpu = false\n\t};\n\tstruct vmw_bo *vbo;\n\tint ret;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = domain,\n\t\t.busy_domain = domain,\n\t\t.bo_type = ttm_bo_type_kernel,\n\t\t.size = bo_size,\n\t\t.pin = true\n\t};\n\n\tret = vmw_bo_create(dev_priv, &bo_params, &vbo);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tret = ttm_bo_reserve(&vbo->tbo, false, true, NULL);\n\tBUG_ON(ret != 0);\n\tret = vmw_ttm_populate(vbo->tbo.bdev, vbo->tbo.ttm, &ctx);\n\tif (likely(ret == 0)) {\n\t\tstruct vmw_ttm_tt *vmw_tt =\n\t\t\tcontainer_of(vbo->tbo.ttm, struct vmw_ttm_tt, dma_ttm);\n\t\tret = vmw_ttm_map_dma(vmw_tt);\n\t}\n\n\tttm_bo_unreserve(&vbo->tbo);\n\n\tif (likely(ret == 0))\n\t\t*bo_p = vbo;\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}