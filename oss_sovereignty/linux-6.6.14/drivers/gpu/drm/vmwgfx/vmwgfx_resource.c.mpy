{
  "module_name": "vmwgfx_resource.c",
  "hash_id": "4ec8f2d6d14bf3f53ce98537396e72acfcb22bd16ebe37b26a64f2403680237e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/vmwgfx/vmwgfx_resource.c",
  "human_readable_source": "\n \n\n#include <drm/ttm/ttm_placement.h>\n\n#include \"vmwgfx_binding.h\"\n#include \"vmwgfx_bo.h\"\n#include \"vmwgfx_drv.h\"\n#include \"vmwgfx_resource_priv.h\"\n\n#define VMW_RES_EVICT_ERR_COUNT 10\n\n \nvoid vmw_resource_mob_attach(struct vmw_resource *res)\n{\n\tstruct vmw_bo *gbo = res->guest_memory_bo;\n\tstruct rb_node **new = &gbo->res_tree.rb_node, *parent = NULL;\n\n\tdma_resv_assert_held(gbo->tbo.base.resv);\n\tres->used_prio = (res->res_dirty) ? res->func->dirty_prio :\n\t\tres->func->prio;\n\n\twhile (*new) {\n\t\tstruct vmw_resource *this =\n\t\t\tcontainer_of(*new, struct vmw_resource, mob_node);\n\n\t\tparent = *new;\n\t\tnew = (res->guest_memory_offset < this->guest_memory_offset) ?\n\t\t\t&((*new)->rb_left) : &((*new)->rb_right);\n\t}\n\n\trb_link_node(&res->mob_node, parent, new);\n\trb_insert_color(&res->mob_node, &gbo->res_tree);\n\n\tvmw_bo_prio_add(gbo, res->used_prio);\n}\n\n \nvoid vmw_resource_mob_detach(struct vmw_resource *res)\n{\n\tstruct vmw_bo *gbo = res->guest_memory_bo;\n\n\tdma_resv_assert_held(gbo->tbo.base.resv);\n\tif (vmw_resource_mob_attached(res)) {\n\t\trb_erase(&res->mob_node, &gbo->res_tree);\n\t\tRB_CLEAR_NODE(&res->mob_node);\n\t\tvmw_bo_prio_del(gbo, res->used_prio);\n\t}\n}\n\nstruct vmw_resource *vmw_resource_reference(struct vmw_resource *res)\n{\n\tkref_get(&res->kref);\n\treturn res;\n}\n\nstruct vmw_resource *\nvmw_resource_reference_unless_doomed(struct vmw_resource *res)\n{\n\treturn kref_get_unless_zero(&res->kref) ? res : NULL;\n}\n\n \nvoid vmw_resource_release_id(struct vmw_resource *res)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\n\n\tspin_lock(&dev_priv->resource_lock);\n\tif (res->id != -1)\n\t\tidr_remove(idr, res->id);\n\tres->id = -1;\n\tspin_unlock(&dev_priv->resource_lock);\n}\n\nstatic void vmw_resource_release(struct kref *kref)\n{\n\tstruct vmw_resource *res =\n\t    container_of(kref, struct vmw_resource, kref);\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint id;\n\tint ret;\n\tstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_del_init(&res->lru_head);\n\tspin_unlock(&dev_priv->resource_lock);\n\tif (res->guest_memory_bo) {\n\t\tstruct ttm_buffer_object *bo = &res->guest_memory_bo->tbo;\n\n\t\tret = ttm_bo_reserve(bo, false, false, NULL);\n\t\tBUG_ON(ret);\n\t\tif (vmw_resource_mob_attached(res) &&\n\t\t    res->func->unbind != NULL) {\n\t\t\tstruct ttm_validate_buffer val_buf;\n\n\t\t\tval_buf.bo = bo;\n\t\t\tval_buf.num_shared = 0;\n\t\t\tres->func->unbind(res, false, &val_buf);\n\t\t}\n\t\tres->guest_memory_size = false;\n\t\tvmw_resource_mob_detach(res);\n\t\tif (res->dirty)\n\t\t\tres->func->dirty_free(res);\n\t\tif (res->coherent)\n\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\tttm_bo_unreserve(bo);\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t}\n\n\tif (likely(res->hw_destroy != NULL)) {\n\t\tmutex_lock(&dev_priv->binding_mutex);\n\t\tvmw_binding_res_list_kill(&res->binding_head);\n\t\tmutex_unlock(&dev_priv->binding_mutex);\n\t\tres->hw_destroy(res);\n\t}\n\n\tid = res->id;\n\tif (res->res_free != NULL)\n\t\tres->res_free(res);\n\telse\n\t\tkfree(res);\n\n\tspin_lock(&dev_priv->resource_lock);\n\tif (id != -1)\n\t\tidr_remove(idr, id);\n\tspin_unlock(&dev_priv->resource_lock);\n}\n\nvoid vmw_resource_unreference(struct vmw_resource **p_res)\n{\n\tstruct vmw_resource *res = *p_res;\n\n\t*p_res = NULL;\n\tkref_put(&res->kref, vmw_resource_release);\n}\n\n\n \nint vmw_resource_alloc_id(struct vmw_resource *res)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint ret;\n\tstruct idr *idr = &dev_priv->res_idr[res->func->res_type];\n\n\tBUG_ON(res->id != -1);\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&dev_priv->resource_lock);\n\n\tret = idr_alloc(idr, res, 1, 0, GFP_NOWAIT);\n\tif (ret >= 0)\n\t\tres->id = ret;\n\n\tspin_unlock(&dev_priv->resource_lock);\n\tidr_preload_end();\n\treturn ret < 0 ? ret : 0;\n}\n\n \nint vmw_resource_init(struct vmw_private *dev_priv, struct vmw_resource *res,\n\t\t      bool delay_id,\n\t\t      void (*res_free) (struct vmw_resource *res),\n\t\t      const struct vmw_res_func *func)\n{\n\tkref_init(&res->kref);\n\tres->hw_destroy = NULL;\n\tres->res_free = res_free;\n\tres->dev_priv = dev_priv;\n\tres->func = func;\n\tRB_CLEAR_NODE(&res->mob_node);\n\tINIT_LIST_HEAD(&res->lru_head);\n\tINIT_LIST_HEAD(&res->binding_head);\n\tres->id = -1;\n\tres->guest_memory_bo = NULL;\n\tres->guest_memory_offset = 0;\n\tres->guest_memory_dirty = false;\n\tres->res_dirty = false;\n\tres->coherent = false;\n\tres->used_prio = 3;\n\tres->dirty = NULL;\n\tif (delay_id)\n\t\treturn 0;\n\telse\n\t\treturn vmw_resource_alloc_id(res);\n}\n\n\n \nint vmw_user_resource_lookup_handle(struct vmw_private *dev_priv,\n\t\t\t\t    struct ttm_object_file *tfile,\n\t\t\t\t    uint32_t handle,\n\t\t\t\t    const struct vmw_user_resource_conv\n\t\t\t\t    *converter,\n\t\t\t\t    struct vmw_resource **p_res)\n{\n\tstruct ttm_base_object *base;\n\tstruct vmw_resource *res;\n\tint ret = -EINVAL;\n\n\tbase = ttm_base_object_lookup(tfile, handle);\n\tif (unlikely(!base))\n\t\treturn -EINVAL;\n\n\tif (unlikely(ttm_base_object_type(base) != converter->object_type))\n\t\tgoto out_bad_resource;\n\n\tres = converter->base_obj_to_res(base);\n\tkref_get(&res->kref);\n\n\t*p_res = res;\n\tret = 0;\n\nout_bad_resource:\n\tttm_base_object_unref(&base);\n\n\treturn ret;\n}\n\n \nint vmw_user_lookup_handle(struct vmw_private *dev_priv,\n\t\t\t   struct drm_file *filp,\n\t\t\t   uint32_t handle,\n\t\t\t   struct vmw_surface **out_surf,\n\t\t\t   struct vmw_bo **out_buf)\n{\n\tstruct ttm_object_file *tfile = vmw_fpriv(filp)->tfile;\n\tstruct vmw_resource *res;\n\tint ret;\n\n\tBUG_ON(*out_surf || *out_buf);\n\n\tret = vmw_user_resource_lookup_handle(dev_priv, tfile, handle,\n\t\t\t\t\t      user_surface_converter,\n\t\t\t\t\t      &res);\n\tif (!ret) {\n\t\t*out_surf = vmw_res_to_srf(res);\n\t\treturn 0;\n\t}\n\n\t*out_surf = NULL;\n\tret = vmw_user_bo_lookup(filp, handle, out_buf);\n\treturn ret;\n}\n\n \nstatic int vmw_resource_buf_alloc(struct vmw_resource *res,\n\t\t\t\t  bool interruptible)\n{\n\tunsigned long size = PFN_ALIGN(res->guest_memory_size);\n\tstruct vmw_bo *gbo;\n\tstruct vmw_bo_params bo_params = {\n\t\t.domain = res->func->domain,\n\t\t.busy_domain = res->func->busy_domain,\n\t\t.bo_type = ttm_bo_type_device,\n\t\t.size = res->guest_memory_size,\n\t\t.pin = false\n\t};\n\tint ret;\n\n\tif (likely(res->guest_memory_bo)) {\n\t\tBUG_ON(res->guest_memory_bo->tbo.base.size < size);\n\t\treturn 0;\n\t}\n\n\tret = vmw_gem_object_create(res->dev_priv, &bo_params, &gbo);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_bo;\n\n\tres->guest_memory_bo = gbo;\n\nout_no_bo:\n\treturn ret;\n}\n\n \nstatic int vmw_resource_do_validate(struct vmw_resource *res,\n\t\t\t\t    struct ttm_validate_buffer *val_buf,\n\t\t\t\t    bool dirtying)\n{\n\tint ret = 0;\n\tconst struct vmw_res_func *func = res->func;\n\n\tif (unlikely(res->id == -1)) {\n\t\tret = func->create(res);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tif (func->bind &&\n\t    ((func->needs_guest_memory && !vmw_resource_mob_attached(res) &&\n\t      val_buf->bo) ||\n\t     (!func->needs_guest_memory && val_buf->bo))) {\n\t\tret = func->bind(res, val_buf);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_bind_failed;\n\t\tif (func->needs_guest_memory)\n\t\t\tvmw_resource_mob_attach(res);\n\t}\n\n\t \n\tif (func->dirty_alloc && vmw_resource_mob_attached(res) &&\n\t    !res->coherent) {\n\t\tif (res->guest_memory_bo->dirty && !res->dirty) {\n\t\t\tret = func->dirty_alloc(res);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else if (!res->guest_memory_bo->dirty && res->dirty) {\n\t\t\tfunc->dirty_free(res);\n\t\t}\n\t}\n\n\t \n\tif (res->dirty) {\n\t\tif (dirtying && !res->res_dirty) {\n\t\t\tpgoff_t start = res->guest_memory_offset >> PAGE_SHIFT;\n\t\t\tpgoff_t end = __KERNEL_DIV_ROUND_UP\n\t\t\t\t(res->guest_memory_offset + res->guest_memory_size,\n\t\t\t\t PAGE_SIZE);\n\n\t\t\tvmw_bo_dirty_unmap(res->guest_memory_bo, start, end);\n\t\t}\n\n\t\tvmw_bo_dirty_transfer_to_res(res);\n\t\treturn func->dirty_sync(res);\n\t}\n\n\treturn 0;\n\nout_bind_failed:\n\tfunc->destroy(res);\n\n\treturn ret;\n}\n\n \nvoid vmw_resource_unreserve(struct vmw_resource *res,\n\t\t\t    bool dirty_set,\n\t\t\t    bool dirty,\n\t\t\t    bool switch_guest_memory,\n\t\t\t    struct vmw_bo *new_guest_memory_bo,\n\t\t\t    unsigned long new_guest_memory_offset)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\n\tif (!list_empty(&res->lru_head))\n\t\treturn;\n\n\tif (switch_guest_memory && new_guest_memory_bo != res->guest_memory_bo) {\n\t\tif (res->guest_memory_bo) {\n\t\t\tvmw_resource_mob_detach(res);\n\t\t\tif (res->coherent)\n\t\t\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t\t}\n\n\t\tif (new_guest_memory_bo) {\n\t\t\tres->guest_memory_bo = vmw_user_bo_ref(new_guest_memory_bo);\n\n\t\t\t \n\t\t\tWARN_ON(res->coherent && !new_guest_memory_bo->dirty);\n\n\t\t\tvmw_resource_mob_attach(res);\n\t\t} else {\n\t\t\tres->guest_memory_bo = NULL;\n\t\t}\n\t} else if (switch_guest_memory && res->coherent) {\n\t\tvmw_bo_dirty_release(res->guest_memory_bo);\n\t}\n\n\tif (switch_guest_memory)\n\t\tres->guest_memory_offset = new_guest_memory_offset;\n\n\tif (dirty_set)\n\t\tres->res_dirty = dirty;\n\n\tif (!res->func->may_evict || res->id == -1 || res->pin_count)\n\t\treturn;\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_add_tail(&res->lru_head,\n\t\t      &res->dev_priv->res_lru[res->func->res_type]);\n\tspin_unlock(&dev_priv->resource_lock);\n}\n\n \nstatic int\nvmw_resource_check_buffer(struct ww_acquire_ctx *ticket,\n\t\t\t  struct vmw_resource *res,\n\t\t\t  bool interruptible,\n\t\t\t  struct ttm_validate_buffer *val_buf)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct list_head val_list;\n\tbool guest_memory_dirty = false;\n\tint ret;\n\n\tif (unlikely(!res->guest_memory_bo)) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0))\n\t\t\treturn ret;\n\t}\n\n\tINIT_LIST_HEAD(&val_list);\n\tttm_bo_get(&res->guest_memory_bo->tbo);\n\tval_buf->bo = &res->guest_memory_bo->tbo;\n\tval_buf->num_shared = 0;\n\tlist_add_tail(&val_buf->head, &val_list);\n\tret = ttm_eu_reserve_buffers(ticket, &val_list, interruptible, NULL);\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_reserve;\n\n\tif (res->func->needs_guest_memory && !vmw_resource_mob_attached(res))\n\t\treturn 0;\n\n\tguest_memory_dirty = res->guest_memory_dirty;\n\tvmw_bo_placement_set(res->guest_memory_bo, res->func->domain,\n\t\t\t     res->func->busy_domain);\n\tret = ttm_bo_validate(&res->guest_memory_bo->tbo,\n\t\t\t      &res->guest_memory_bo->placement,\n\t\t\t      &ctx);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\n\treturn 0;\n\nout_no_validate:\n\tttm_eu_backoff_reservation(ticket, &val_list);\nout_no_reserve:\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n\tif (guest_memory_dirty)\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\n\treturn ret;\n}\n\n \nint vmw_resource_reserve(struct vmw_resource *res, bool interruptible,\n\t\t\t bool no_guest_memory)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint ret;\n\n\tspin_lock(&dev_priv->resource_lock);\n\tlist_del_init(&res->lru_head);\n\tspin_unlock(&dev_priv->resource_lock);\n\n\tif (res->func->needs_guest_memory && !res->guest_memory_bo &&\n\t    !no_guest_memory) {\n\t\tret = vmw_resource_buf_alloc(res, interruptible);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tDRM_ERROR(\"Failed to allocate a guest memory buffer \"\n\t\t\t\t  \"of size %lu. bytes\\n\",\n\t\t\t\t  (unsigned long) res->guest_memory_size);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void\nvmw_resource_backoff_reservation(struct ww_acquire_ctx *ticket,\n\t\t\t\t struct ttm_validate_buffer *val_buf)\n{\n\tstruct list_head val_list;\n\n\tif (likely(val_buf->bo == NULL))\n\t\treturn;\n\n\tINIT_LIST_HEAD(&val_list);\n\tlist_add_tail(&val_buf->head, &val_list);\n\tttm_eu_backoff_reservation(ticket, &val_list);\n\tttm_bo_put(val_buf->bo);\n\tval_buf->bo = NULL;\n}\n\n \nstatic int vmw_resource_do_evict(struct ww_acquire_ctx *ticket,\n\t\t\t\t struct vmw_resource *res, bool interruptible)\n{\n\tstruct ttm_validate_buffer val_buf;\n\tconst struct vmw_res_func *func = res->func;\n\tint ret;\n\n\tBUG_ON(!func->may_evict);\n\n\tval_buf.bo = NULL;\n\tval_buf.num_shared = 0;\n\tret = vmw_resource_check_buffer(ticket, res, interruptible, &val_buf);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tif (unlikely(func->unbind != NULL &&\n\t\t     (!func->needs_guest_memory || vmw_resource_mob_attached(res)))) {\n\t\tret = func->unbind(res, res->res_dirty, &val_buf);\n\t\tif (unlikely(ret != 0))\n\t\t\tgoto out_no_unbind;\n\t\tvmw_resource_mob_detach(res);\n\t}\n\tret = func->destroy(res);\n\tres->guest_memory_dirty = true;\n\tres->res_dirty = false;\nout_no_unbind:\n\tvmw_resource_backoff_reservation(ticket, &val_buf);\n\n\treturn ret;\n}\n\n\n \nint vmw_resource_validate(struct vmw_resource *res, bool intr,\n\t\t\t  bool dirtying)\n{\n\tint ret;\n\tstruct vmw_resource *evict_res;\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tstruct list_head *lru_list = &dev_priv->res_lru[res->func->res_type];\n\tstruct ttm_validate_buffer val_buf;\n\tunsigned err_count = 0;\n\n\tif (!res->func->create)\n\t\treturn 0;\n\n\tval_buf.bo = NULL;\n\tval_buf.num_shared = 0;\n\tif (res->guest_memory_bo)\n\t\tval_buf.bo = &res->guest_memory_bo->tbo;\n\tdo {\n\t\tret = vmw_resource_do_validate(res, &val_buf, dirtying);\n\t\tif (likely(ret != -EBUSY))\n\t\t\tbreak;\n\n\t\tspin_lock(&dev_priv->resource_lock);\n\t\tif (list_empty(lru_list) || !res->func->may_evict) {\n\t\t\tDRM_ERROR(\"Out of device device resources \"\n\t\t\t\t  \"for %s.\\n\", res->func->type_name);\n\t\t\tret = -EBUSY;\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tevict_res = vmw_resource_reference\n\t\t\t(list_first_entry(lru_list, struct vmw_resource,\n\t\t\t\t\t  lru_head));\n\t\tlist_del_init(&evict_res->lru_head);\n\n\t\tspin_unlock(&dev_priv->resource_lock);\n\n\t\t \n\t\tret = vmw_resource_do_evict(NULL, evict_res, intr);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tspin_lock(&dev_priv->resource_lock);\n\t\t\tlist_add_tail(&evict_res->lru_head, lru_list);\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tif (ret == -ERESTARTSYS ||\n\t\t\t    ++err_count > VMW_RES_EVICT_ERR_COUNT) {\n\t\t\t\tvmw_resource_unreference(&evict_res);\n\t\t\t\tgoto out_no_validate;\n\t\t\t}\n\t\t}\n\n\t\tvmw_resource_unreference(&evict_res);\n\t} while (1);\n\n\tif (unlikely(ret != 0))\n\t\tgoto out_no_validate;\n\telse if (!res->func->needs_guest_memory && res->guest_memory_bo) {\n\t\tWARN_ON_ONCE(vmw_resource_mob_attached(res));\n\t\tvmw_user_bo_unref(&res->guest_memory_bo);\n\t}\n\n\treturn 0;\n\nout_no_validate:\n\treturn ret;\n}\n\n\n \nvoid vmw_resource_unbind_list(struct vmw_bo *vbo)\n{\n\tstruct ttm_validate_buffer val_buf = {\n\t\t.bo = &vbo->tbo,\n\t\t.num_shared = 0\n\t};\n\n\tdma_resv_assert_held(vbo->tbo.base.resv);\n\twhile (!RB_EMPTY_ROOT(&vbo->res_tree)) {\n\t\tstruct rb_node *node = vbo->res_tree.rb_node;\n\t\tstruct vmw_resource *res =\n\t\t\tcontainer_of(node, struct vmw_resource, mob_node);\n\n\t\tif (!WARN_ON_ONCE(!res->func->unbind))\n\t\t\t(void) res->func->unbind(res, res->res_dirty, &val_buf);\n\n\t\tres->guest_memory_size = true;\n\t\tres->res_dirty = false;\n\t\tvmw_resource_mob_detach(res);\n\t}\n\n\t(void) ttm_bo_wait(&vbo->tbo, false, false);\n}\n\n\n \nint vmw_query_readback_all(struct vmw_bo *dx_query_mob)\n{\n\tstruct vmw_resource *dx_query_ctx;\n\tstruct vmw_private *dev_priv;\n\tstruct {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdDXReadbackAllQuery body;\n\t} *cmd;\n\n\n\t \n\tif (!dx_query_mob || !dx_query_mob->dx_query_ctx)\n\t\treturn 0;\n\n\tdx_query_ctx = dx_query_mob->dx_query_ctx;\n\tdev_priv     = dx_query_ctx->dev_priv;\n\n\tcmd = VMW_CMD_CTX_RESERVE(dev_priv, sizeof(*cmd), dx_query_ctx->id);\n\tif (unlikely(cmd == NULL))\n\t\treturn -ENOMEM;\n\n\tcmd->header.id   = SVGA_3D_CMD_DX_READBACK_ALL_QUERY;\n\tcmd->header.size = sizeof(cmd->body);\n\tcmd->body.cid    = dx_query_ctx->id;\n\n\tvmw_cmd_commit(dev_priv, sizeof(*cmd));\n\n\t \n\tdx_query_mob->dx_query_ctx = NULL;\n\n\treturn 0;\n}\n\n\n\n \nvoid vmw_query_move_notify(struct ttm_buffer_object *bo,\n\t\t\t   struct ttm_resource *old_mem,\n\t\t\t   struct ttm_resource *new_mem)\n{\n\tstruct vmw_bo *dx_query_mob;\n\tstruct ttm_device *bdev = bo->bdev;\n\tstruct vmw_private *dev_priv = vmw_priv_from_ttm(bdev);\n\n\tmutex_lock(&dev_priv->binding_mutex);\n\n\t \n\tif (old_mem &&\n\t    new_mem->mem_type == TTM_PL_SYSTEM &&\n\t    old_mem->mem_type == VMW_PL_MOB) {\n\t\tstruct vmw_fence_obj *fence;\n\n\t\tdx_query_mob = to_vmw_bo(&bo->base);\n\t\tif (!dx_query_mob || !dx_query_mob->dx_query_ctx) {\n\t\t\tmutex_unlock(&dev_priv->binding_mutex);\n\t\t\treturn;\n\t\t}\n\n\t\t(void) vmw_query_readback_all(dx_query_mob);\n\t\tmutex_unlock(&dev_priv->binding_mutex);\n\n\t\t \n\t\t(void) vmw_execbuf_fence_commands(NULL, dev_priv, &fence, NULL);\n\t\tvmw_bo_fence_single(bo, fence);\n\n\t\tif (fence != NULL)\n\t\t\tvmw_fence_obj_unreference(&fence);\n\n\t\t(void) ttm_bo_wait(bo, false, false);\n\t} else\n\t\tmutex_unlock(&dev_priv->binding_mutex);\n}\n\n \nbool vmw_resource_needs_backup(const struct vmw_resource *res)\n{\n\treturn res->func->needs_guest_memory;\n}\n\n \nstatic void vmw_resource_evict_type(struct vmw_private *dev_priv,\n\t\t\t\t    enum vmw_res_type type)\n{\n\tstruct list_head *lru_list = &dev_priv->res_lru[type];\n\tstruct vmw_resource *evict_res;\n\tunsigned err_count = 0;\n\tint ret;\n\tstruct ww_acquire_ctx ticket;\n\n\tdo {\n\t\tspin_lock(&dev_priv->resource_lock);\n\n\t\tif (list_empty(lru_list))\n\t\t\tgoto out_unlock;\n\n\t\tevict_res = vmw_resource_reference(\n\t\t\tlist_first_entry(lru_list, struct vmw_resource,\n\t\t\t\t\t lru_head));\n\t\tlist_del_init(&evict_res->lru_head);\n\t\tspin_unlock(&dev_priv->resource_lock);\n\n\t\t \n\t\tret = vmw_resource_do_evict(&ticket, evict_res, false);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tspin_lock(&dev_priv->resource_lock);\n\t\t\tlist_add_tail(&evict_res->lru_head, lru_list);\n\t\t\tspin_unlock(&dev_priv->resource_lock);\n\t\t\tif (++err_count > VMW_RES_EVICT_ERR_COUNT) {\n\t\t\t\tvmw_resource_unreference(&evict_res);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\tvmw_resource_unreference(&evict_res);\n\t} while (1);\n\nout_unlock:\n\tspin_unlock(&dev_priv->resource_lock);\n}\n\n \nvoid vmw_resource_evict_all(struct vmw_private *dev_priv)\n{\n\tenum vmw_res_type type;\n\n\tmutex_lock(&dev_priv->cmdbuf_mutex);\n\n\tfor (type = 0; type < vmw_res_max; ++type)\n\t\tvmw_resource_evict_type(dev_priv, type);\n\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n}\n\n \nint vmw_resource_pin(struct vmw_resource *res, bool interruptible)\n{\n\tstruct ttm_operation_ctx ctx = { interruptible, false };\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint ret;\n\n\tmutex_lock(&dev_priv->cmdbuf_mutex);\n\tret = vmw_resource_reserve(res, interruptible, false);\n\tif (ret)\n\t\tgoto out_no_reserve;\n\n\tif (res->pin_count == 0) {\n\t\tstruct vmw_bo *vbo = NULL;\n\n\t\tif (res->guest_memory_bo) {\n\t\t\tvbo = res->guest_memory_bo;\n\n\t\t\tret = ttm_bo_reserve(&vbo->tbo, interruptible, false, NULL);\n\t\t\tif (ret)\n\t\t\t\tgoto out_no_validate;\n\t\t\tif (!vbo->tbo.pin_count) {\n\t\t\t\tvmw_bo_placement_set(vbo,\n\t\t\t\t\t\t     res->func->domain,\n\t\t\t\t\t\t     res->func->busy_domain);\n\t\t\t\tret = ttm_bo_validate\n\t\t\t\t\t(&vbo->tbo,\n\t\t\t\t\t &vbo->placement,\n\t\t\t\t\t &ctx);\n\t\t\t\tif (ret) {\n\t\t\t\t\tttm_bo_unreserve(&vbo->tbo);\n\t\t\t\t\tgoto out_no_validate;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tvmw_bo_pin_reserved(vbo, true);\n\t\t}\n\t\tret = vmw_resource_validate(res, interruptible, true);\n\t\tif (vbo)\n\t\t\tttm_bo_unreserve(&vbo->tbo);\n\t\tif (ret)\n\t\t\tgoto out_no_validate;\n\t}\n\tres->pin_count++;\n\nout_no_validate:\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0UL);\nout_no_reserve:\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n\n\treturn ret;\n}\n\n \nvoid vmw_resource_unpin(struct vmw_resource *res)\n{\n\tstruct vmw_private *dev_priv = res->dev_priv;\n\tint ret;\n\n\tmutex_lock(&dev_priv->cmdbuf_mutex);\n\n\tret = vmw_resource_reserve(res, false, true);\n\tWARN_ON(ret);\n\n\tWARN_ON(res->pin_count == 0);\n\tif (--res->pin_count == 0 && res->guest_memory_bo) {\n\t\tstruct vmw_bo *vbo = res->guest_memory_bo;\n\n\t\t(void) ttm_bo_reserve(&vbo->tbo, false, false, NULL);\n\t\tvmw_bo_pin_reserved(vbo, false);\n\t\tttm_bo_unreserve(&vbo->tbo);\n\t}\n\n\tvmw_resource_unreserve(res, false, false, false, NULL, 0UL);\n\n\tmutex_unlock(&dev_priv->cmdbuf_mutex);\n}\n\n \nenum vmw_res_type vmw_res_type(const struct vmw_resource *res)\n{\n\treturn res->func->res_type;\n}\n\n \nvoid vmw_resource_dirty_update(struct vmw_resource *res, pgoff_t start,\n\t\t\t       pgoff_t end)\n{\n\tif (res->dirty)\n\t\tres->func->dirty_range_add(res, start << PAGE_SHIFT,\n\t\t\t\t\t   end << PAGE_SHIFT);\n}\n\n \nint vmw_resources_clean(struct vmw_bo *vbo, pgoff_t start,\n\t\t\tpgoff_t end, pgoff_t *num_prefault)\n{\n\tstruct rb_node *cur = vbo->res_tree.rb_node;\n\tstruct vmw_resource *found = NULL;\n\tunsigned long res_start = start << PAGE_SHIFT;\n\tunsigned long res_end = end << PAGE_SHIFT;\n\tunsigned long last_cleaned = 0;\n\n\t \n\twhile (cur) {\n\t\tstruct vmw_resource *cur_res =\n\t\t\tcontainer_of(cur, struct vmw_resource, mob_node);\n\n\t\tif (cur_res->guest_memory_offset >= res_end) {\n\t\t\tcur = cur->rb_left;\n\t\t} else if (cur_res->guest_memory_offset + cur_res->guest_memory_size <=\n\t\t\t   res_start) {\n\t\t\tcur = cur->rb_right;\n\t\t} else {\n\t\t\tfound = cur_res;\n\t\t\tcur = cur->rb_left;\n\t\t\t \n\t\t}\n\t}\n\n\t \n\twhile (found) {\n\t\tif (found->res_dirty) {\n\t\t\tint ret;\n\n\t\t\tif (!found->func->clean)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tret = found->func->clean(found);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tfound->res_dirty = false;\n\t\t}\n\t\tlast_cleaned = found->guest_memory_offset + found->guest_memory_size;\n\t\tcur = rb_next(&found->mob_node);\n\t\tif (!cur)\n\t\t\tbreak;\n\n\t\tfound = container_of(cur, struct vmw_resource, mob_node);\n\t\tif (found->guest_memory_offset >= res_end)\n\t\t\tbreak;\n\t}\n\n\t \n\t*num_prefault = 1;\n\tif (last_cleaned > res_start) {\n\t\tstruct ttm_buffer_object *bo = &vbo->tbo;\n\n\t\t*num_prefault = __KERNEL_DIV_ROUND_UP(last_cleaned - res_start,\n\t\t\t\t\t\t      PAGE_SIZE);\n\t\tvmw_bo_fence_single(bo, NULL);\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}