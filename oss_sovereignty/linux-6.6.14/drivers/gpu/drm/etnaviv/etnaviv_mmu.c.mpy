{
  "module_name": "etnaviv_mmu.c",
  "hash_id": "a0d0b0561e071f689b0daaff68a90a327199b5c828e6f9add0eda96ee55cb506",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/etnaviv/etnaviv_mmu.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include <linux/scatterlist.h>\n\n#include \"common.xml.h\"\n#include \"etnaviv_cmdbuf.h\"\n#include \"etnaviv_drv.h\"\n#include \"etnaviv_gem.h\"\n#include \"etnaviv_gpu.h\"\n#include \"etnaviv_mmu.h\"\n\nstatic void etnaviv_context_unmap(struct etnaviv_iommu_context *context,\n\t\t\t\t unsigned long iova, size_t size)\n{\n\tsize_t unmapped_page, unmapped = 0;\n\tsize_t pgsize = SZ_4K;\n\n\tif (!IS_ALIGNED(iova | size, pgsize)) {\n\t\tpr_err(\"unaligned: iova 0x%lx size 0x%zx min_pagesz 0x%zx\\n\",\n\t\t       iova, size, pgsize);\n\t\treturn;\n\t}\n\n\twhile (unmapped < size) {\n\t\tunmapped_page = context->global->ops->unmap(context, iova,\n\t\t\t\t\t\t\t    pgsize);\n\t\tif (!unmapped_page)\n\t\t\tbreak;\n\n\t\tiova += unmapped_page;\n\t\tunmapped += unmapped_page;\n\t}\n}\n\nstatic int etnaviv_context_map(struct etnaviv_iommu_context *context,\n\t\t\t      unsigned long iova, phys_addr_t paddr,\n\t\t\t      size_t size, int prot)\n{\n\tunsigned long orig_iova = iova;\n\tsize_t pgsize = SZ_4K;\n\tsize_t orig_size = size;\n\tint ret = 0;\n\n\tif (!IS_ALIGNED(iova | paddr | size, pgsize)) {\n\t\tpr_err(\"unaligned: iova 0x%lx pa %pa size 0x%zx min_pagesz 0x%zx\\n\",\n\t\t       iova, &paddr, size, pgsize);\n\t\treturn -EINVAL;\n\t}\n\n\twhile (size) {\n\t\tret = context->global->ops->map(context, iova, paddr, pgsize,\n\t\t\t\t\t\tprot);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tiova += pgsize;\n\t\tpaddr += pgsize;\n\t\tsize -= pgsize;\n\t}\n\n\t \n\tif (ret)\n\t\tetnaviv_context_unmap(context, orig_iova, orig_size - size);\n\n\treturn ret;\n}\n\nstatic int etnaviv_iommu_map(struct etnaviv_iommu_context *context, u32 iova,\n\t\t\t     struct sg_table *sgt, unsigned len, int prot)\n{\tstruct scatterlist *sg;\n\tunsigned int da = iova;\n\tunsigned int i;\n\tint ret;\n\n\tif (!context || !sgt)\n\t\treturn -EINVAL;\n\n\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\tphys_addr_t pa = sg_dma_address(sg) - sg->offset;\n\t\tsize_t bytes = sg_dma_len(sg) + sg->offset;\n\n\t\tVERB(\"map[%d]: %08x %pap(%zx)\", i, iova, &pa, bytes);\n\n\t\tret = etnaviv_context_map(context, da, pa, bytes, prot);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t\tda += bytes;\n\t}\n\n\tcontext->flush_seq++;\n\n\treturn 0;\n\nfail:\n\tetnaviv_context_unmap(context, iova, da - iova);\n\treturn ret;\n}\n\nstatic void etnaviv_iommu_unmap(struct etnaviv_iommu_context *context, u32 iova,\n\t\t\t\tstruct sg_table *sgt, unsigned len)\n{\n\tstruct scatterlist *sg;\n\tunsigned int da = iova;\n\tint i;\n\n\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\tsize_t bytes = sg_dma_len(sg) + sg->offset;\n\n\t\tetnaviv_context_unmap(context, da, bytes);\n\n\t\tVERB(\"unmap[%d]: %08x(%zx)\", i, iova, bytes);\n\n\t\tBUG_ON(!PAGE_ALIGNED(bytes));\n\n\t\tda += bytes;\n\t}\n\n\tcontext->flush_seq++;\n}\n\nstatic void etnaviv_iommu_remove_mapping(struct etnaviv_iommu_context *context,\n\tstruct etnaviv_vram_mapping *mapping)\n{\n\tstruct etnaviv_gem_object *etnaviv_obj = mapping->object;\n\n\tlockdep_assert_held(&context->lock);\n\n\tetnaviv_iommu_unmap(context, mapping->vram_node.start,\n\t\t\t    etnaviv_obj->sgt, etnaviv_obj->base.size);\n\tdrm_mm_remove_node(&mapping->vram_node);\n}\n\nvoid etnaviv_iommu_reap_mapping(struct etnaviv_vram_mapping *mapping)\n{\n\tstruct etnaviv_iommu_context *context = mapping->context;\n\n\tlockdep_assert_held(&context->lock);\n\tWARN_ON(mapping->use);\n\n\tetnaviv_iommu_remove_mapping(context, mapping);\n\tetnaviv_iommu_context_put(mapping->context);\n\tmapping->context = NULL;\n\tlist_del_init(&mapping->mmu_node);\n}\n\nstatic int etnaviv_iommu_find_iova(struct etnaviv_iommu_context *context,\n\t\t\t\t   struct drm_mm_node *node, size_t size)\n{\n\tstruct etnaviv_vram_mapping *free = NULL;\n\tenum drm_mm_insert_mode mode = DRM_MM_INSERT_LOW;\n\tint ret;\n\n\tlockdep_assert_held(&context->lock);\n\n\twhile (1) {\n\t\tstruct etnaviv_vram_mapping *m, *n;\n\t\tstruct drm_mm_scan scan;\n\t\tstruct list_head list;\n\t\tbool found;\n\n\t\tret = drm_mm_insert_node_in_range(&context->mm, node,\n\t\t\t\t\t\t  size, 0, 0, 0, U64_MAX, mode);\n\t\tif (ret != -ENOSPC)\n\t\t\tbreak;\n\n\t\t \n\t\tdrm_mm_scan_init(&scan, &context->mm, size, 0, 0, mode);\n\n\t\tfound = 0;\n\t\tINIT_LIST_HEAD(&list);\n\t\tlist_for_each_entry(free, &context->mappings, mmu_node) {\n\t\t\t \n\t\t\tif (!free->vram_node.mm)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (free->use)\n\t\t\t\tcontinue;\n\n\t\t\tlist_add(&free->scan_node, &list);\n\t\t\tif (drm_mm_scan_add_block(&scan, &free->vram_node)) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\t \n\t\t\tlist_for_each_entry_safe(m, n, &list, scan_node)\n\t\t\t\tBUG_ON(drm_mm_scan_remove_block(&scan, &m->vram_node));\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry_safe(m, n, &list, scan_node)\n\t\t\tif (!drm_mm_scan_remove_block(&scan, &m->vram_node))\n\t\t\t\tlist_del_init(&m->scan_node);\n\n\t\t \n\t\tlist_for_each_entry_safe(m, n, &list, scan_node) {\n\t\t\tetnaviv_iommu_reap_mapping(m);\n\t\t\tlist_del_init(&m->scan_node);\n\t\t}\n\n\t\tmode = DRM_MM_INSERT_EVICT;\n\n\t\t \n\t}\n\n\treturn ret;\n}\n\nstatic int etnaviv_iommu_insert_exact(struct etnaviv_iommu_context *context,\n\t\t   struct drm_mm_node *node, size_t size, u64 va)\n{\n\tstruct etnaviv_vram_mapping *m, *n;\n\tstruct drm_mm_node *scan_node;\n\tLIST_HEAD(scan_list);\n\tint ret;\n\n\tlockdep_assert_held(&context->lock);\n\n\tret = drm_mm_insert_node_in_range(&context->mm, node, size, 0, 0, va,\n\t\t\t\t\t  va + size, DRM_MM_INSERT_LOWEST);\n\tif (ret != -ENOSPC)\n\t\treturn ret;\n\n\t \n\n\tdrm_mm_for_each_node_in_range(scan_node, &context->mm, va, va + size) {\n\t\tm = container_of(scan_node, struct etnaviv_vram_mapping,\n\t\t\t\t vram_node);\n\n\t\tif (m->use)\n\t\t\treturn -ENOSPC;\n\n\t\tlist_add(&m->scan_node, &scan_list);\n\t}\n\n\tlist_for_each_entry_safe(m, n, &scan_list, scan_node) {\n\t\tetnaviv_iommu_reap_mapping(m);\n\t\tlist_del_init(&m->scan_node);\n\t}\n\n\treturn drm_mm_insert_node_in_range(&context->mm, node, size, 0, 0, va,\n\t\t\t\t\t   va + size, DRM_MM_INSERT_LOWEST);\n}\n\nint etnaviv_iommu_map_gem(struct etnaviv_iommu_context *context,\n\tstruct etnaviv_gem_object *etnaviv_obj, u32 memory_base,\n\tstruct etnaviv_vram_mapping *mapping, u64 va)\n{\n\tstruct sg_table *sgt = etnaviv_obj->sgt;\n\tstruct drm_mm_node *node;\n\tint ret;\n\n\tlockdep_assert_held(&etnaviv_obj->lock);\n\n\tmutex_lock(&context->lock);\n\n\t \n\tif (context->global->version == ETNAVIV_IOMMU_V1 &&\n\t    sgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {\n\t\tu32 iova;\n\n\t\tiova = sg_dma_address(sgt->sgl) - memory_base;\n\t\tif (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {\n\t\t\tmapping->iova = iova;\n\t\t\tmapping->context = etnaviv_iommu_context_get(context);\n\t\t\tlist_add_tail(&mapping->mmu_node, &context->mappings);\n\t\t\tret = 0;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\tnode = &mapping->vram_node;\n\n\tif (va)\n\t\tret = etnaviv_iommu_insert_exact(context, node,\n\t\t\t\t\t\t etnaviv_obj->base.size, va);\n\telse\n\t\tret = etnaviv_iommu_find_iova(context, node,\n\t\t\t\t\t      etnaviv_obj->base.size);\n\tif (ret < 0)\n\t\tgoto unlock;\n\n\tmapping->iova = node->start;\n\tret = etnaviv_iommu_map(context, node->start, sgt, etnaviv_obj->base.size,\n\t\t\t\tETNAVIV_PROT_READ | ETNAVIV_PROT_WRITE);\n\n\tif (ret < 0) {\n\t\tdrm_mm_remove_node(node);\n\t\tgoto unlock;\n\t}\n\n\tmapping->context = etnaviv_iommu_context_get(context);\n\tlist_add_tail(&mapping->mmu_node, &context->mappings);\nunlock:\n\tmutex_unlock(&context->lock);\n\n\treturn ret;\n}\n\nvoid etnaviv_iommu_unmap_gem(struct etnaviv_iommu_context *context,\n\tstruct etnaviv_vram_mapping *mapping)\n{\n\tWARN_ON(mapping->use);\n\n\tmutex_lock(&context->lock);\n\n\t \n\tif (!mapping->context) {\n\t\tmutex_unlock(&context->lock);\n\t\treturn;\n\t}\n\n\t \n\tif (mapping->vram_node.mm == &context->mm)\n\t\tetnaviv_iommu_remove_mapping(context, mapping);\n\n\tlist_del(&mapping->mmu_node);\n\tmutex_unlock(&context->lock);\n\tetnaviv_iommu_context_put(context);\n}\n\nstatic void etnaviv_iommu_context_free(struct kref *kref)\n{\n\tstruct etnaviv_iommu_context *context =\n\t\tcontainer_of(kref, struct etnaviv_iommu_context, refcount);\n\n\tetnaviv_cmdbuf_suballoc_unmap(context, &context->cmdbuf_mapping);\n\n\tcontext->global->ops->free(context);\n}\nvoid etnaviv_iommu_context_put(struct etnaviv_iommu_context *context)\n{\n\tkref_put(&context->refcount, etnaviv_iommu_context_free);\n}\n\nstruct etnaviv_iommu_context *\netnaviv_iommu_context_init(struct etnaviv_iommu_global *global,\n\t\t\t   struct etnaviv_cmdbuf_suballoc *suballoc)\n{\n\tstruct etnaviv_iommu_context *ctx;\n\tint ret;\n\n\tif (global->version == ETNAVIV_IOMMU_V1)\n\t\tctx = etnaviv_iommuv1_context_alloc(global);\n\telse\n\t\tctx = etnaviv_iommuv2_context_alloc(global);\n\n\tif (!ctx)\n\t\treturn NULL;\n\n\tret = etnaviv_cmdbuf_suballoc_map(suballoc, ctx, &ctx->cmdbuf_mapping,\n\t\t\t\t\t  global->memory_base);\n\tif (ret)\n\t\tgoto out_free;\n\n\tif (global->version == ETNAVIV_IOMMU_V1 &&\n\t    ctx->cmdbuf_mapping.iova > 0x80000000) {\n\t\tdev_err(global->dev,\n\t\t        \"command buffer outside valid memory window\\n\");\n\t\tgoto out_unmap;\n\t}\n\n\treturn ctx;\n\nout_unmap:\n\tetnaviv_cmdbuf_suballoc_unmap(ctx, &ctx->cmdbuf_mapping);\nout_free:\n\tglobal->ops->free(ctx);\n\treturn NULL;\n}\n\nvoid etnaviv_iommu_restore(struct etnaviv_gpu *gpu,\n\t\t\t   struct etnaviv_iommu_context *context)\n{\n\tcontext->global->ops->restore(gpu, context);\n}\n\nint etnaviv_iommu_get_suballoc_va(struct etnaviv_iommu_context *context,\n\t\t\t\t  struct etnaviv_vram_mapping *mapping,\n\t\t\t\t  u32 memory_base, dma_addr_t paddr,\n\t\t\t\t  size_t size)\n{\n\tmutex_lock(&context->lock);\n\n\tif (mapping->use > 0) {\n\t\tmapping->use++;\n\t\tmutex_unlock(&context->lock);\n\t\treturn 0;\n\t}\n\n\t \n\tif (context->global->version == ETNAVIV_IOMMU_V1) {\n\t\tmapping->iova = paddr - memory_base;\n\t} else {\n\t\tstruct drm_mm_node *node = &mapping->vram_node;\n\t\tint ret;\n\n\t\tret = etnaviv_iommu_find_iova(context, node, size);\n\t\tif (ret < 0) {\n\t\t\tmutex_unlock(&context->lock);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmapping->iova = node->start;\n\t\tret = etnaviv_context_map(context, node->start, paddr, size,\n\t\t\t\t\t  ETNAVIV_PROT_READ);\n\t\tif (ret < 0) {\n\t\t\tdrm_mm_remove_node(node);\n\t\t\tmutex_unlock(&context->lock);\n\t\t\treturn ret;\n\t\t}\n\n\t\tcontext->flush_seq++;\n\t}\n\n\tlist_add_tail(&mapping->mmu_node, &context->mappings);\n\tmapping->use = 1;\n\n\tmutex_unlock(&context->lock);\n\n\treturn 0;\n}\n\nvoid etnaviv_iommu_put_suballoc_va(struct etnaviv_iommu_context *context,\n\t\t  struct etnaviv_vram_mapping *mapping)\n{\n\tstruct drm_mm_node *node = &mapping->vram_node;\n\n\tmutex_lock(&context->lock);\n\tmapping->use--;\n\n\tif (mapping->use > 0 || context->global->version == ETNAVIV_IOMMU_V1) {\n\t\tmutex_unlock(&context->lock);\n\t\treturn;\n\t}\n\n\tetnaviv_context_unmap(context, node->start, node->size);\n\tdrm_mm_remove_node(node);\n\tmutex_unlock(&context->lock);\n}\n\nsize_t etnaviv_iommu_dump_size(struct etnaviv_iommu_context *context)\n{\n\treturn context->global->ops->dump_size(context);\n}\n\nvoid etnaviv_iommu_dump(struct etnaviv_iommu_context *context, void *buf)\n{\n\tcontext->global->ops->dump(context, buf);\n}\n\nint etnaviv_iommu_global_init(struct etnaviv_gpu *gpu)\n{\n\tenum etnaviv_iommu_version version = ETNAVIV_IOMMU_V1;\n\tstruct etnaviv_drm_private *priv = gpu->drm->dev_private;\n\tstruct etnaviv_iommu_global *global;\n\tstruct device *dev = gpu->drm->dev;\n\n\tif (gpu->identity.minor_features1 & chipMinorFeatures1_MMU_VERSION)\n\t\tversion = ETNAVIV_IOMMU_V2;\n\n\tif (priv->mmu_global) {\n\t\tif (priv->mmu_global->version != version) {\n\t\t\tdev_err(gpu->dev,\n\t\t\t\t\"MMU version doesn't match global version\\n\");\n\t\t\treturn -ENXIO;\n\t\t}\n\n\t\tpriv->mmu_global->use++;\n\t\treturn 0;\n\t}\n\n\tglobal = kzalloc(sizeof(*global), GFP_KERNEL);\n\tif (!global)\n\t\treturn -ENOMEM;\n\n\tglobal->bad_page_cpu = dma_alloc_wc(dev, SZ_4K, &global->bad_page_dma,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!global->bad_page_cpu)\n\t\tgoto free_global;\n\n\tmemset32(global->bad_page_cpu, 0xdead55aa, SZ_4K / sizeof(u32));\n\n\tif (version == ETNAVIV_IOMMU_V2) {\n\t\tglobal->v2.pta_cpu = dma_alloc_wc(dev, ETNAVIV_PTA_SIZE,\n\t\t\t\t\t       &global->v2.pta_dma, GFP_KERNEL);\n\t\tif (!global->v2.pta_cpu)\n\t\t\tgoto free_bad_page;\n\t}\n\n\tglobal->dev = dev;\n\tglobal->version = version;\n\tglobal->use = 1;\n\tmutex_init(&global->lock);\n\n\tif (version == ETNAVIV_IOMMU_V1)\n\t\tglobal->ops = &etnaviv_iommuv1_ops;\n\telse\n\t\tglobal->ops = &etnaviv_iommuv2_ops;\n\n\tpriv->mmu_global = global;\n\n\treturn 0;\n\nfree_bad_page:\n\tdma_free_wc(dev, SZ_4K, global->bad_page_cpu, global->bad_page_dma);\nfree_global:\n\tkfree(global);\n\n\treturn -ENOMEM;\n}\n\nvoid etnaviv_iommu_global_fini(struct etnaviv_gpu *gpu)\n{\n\tstruct etnaviv_drm_private *priv = gpu->drm->dev_private;\n\tstruct etnaviv_iommu_global *global = priv->mmu_global;\n\n\tif (!global)\n\t\treturn;\n\n\tif (--global->use > 0)\n\t\treturn;\n\n\tif (global->v2.pta_cpu)\n\t\tdma_free_wc(global->dev, ETNAVIV_PTA_SIZE,\n\t\t\t    global->v2.pta_cpu, global->v2.pta_dma);\n\n\tif (global->bad_page_cpu)\n\t\tdma_free_wc(global->dev, SZ_4K,\n\t\t\t    global->bad_page_cpu, global->bad_page_dma);\n\n\tmutex_destroy(&global->lock);\n\tkfree(global);\n\n\tpriv->mmu_global = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}