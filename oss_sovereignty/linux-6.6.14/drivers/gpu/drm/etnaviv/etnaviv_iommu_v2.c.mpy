{
  "module_name": "etnaviv_iommu_v2.c",
  "hash_id": "5d1f192399b4434da19ff39147e956a53dbf29d7b8f3394bee5836b0234f533d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/etnaviv/etnaviv_iommu_v2.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/dma-mapping.h>\n#include <linux/platform_device.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n#include \"etnaviv_cmdbuf.h\"\n#include \"etnaviv_gpu.h\"\n#include \"etnaviv_mmu.h\"\n#include \"state.xml.h\"\n#include \"state_hi.xml.h\"\n\n#define MMUv2_PTE_PRESENT\t\tBIT(0)\n#define MMUv2_PTE_EXCEPTION\t\tBIT(1)\n#define MMUv2_PTE_WRITEABLE\t\tBIT(2)\n\n#define MMUv2_MTLB_MASK\t\t\t0xffc00000\n#define MMUv2_MTLB_SHIFT\t\t22\n#define MMUv2_STLB_MASK\t\t\t0x003ff000\n#define MMUv2_STLB_SHIFT\t\t12\n\n#define MMUv2_MAX_STLB_ENTRIES\t\t1024\n\nstruct etnaviv_iommuv2_context {\n\tstruct etnaviv_iommu_context base;\n\tunsigned short id;\n\t \n\tu32 *mtlb_cpu;\n\tdma_addr_t mtlb_dma;\n\t \n\tu32 *stlb_cpu[MMUv2_MAX_STLB_ENTRIES];\n\tdma_addr_t stlb_dma[MMUv2_MAX_STLB_ENTRIES];\n};\n\nstatic struct etnaviv_iommuv2_context *\nto_v2_context(struct etnaviv_iommu_context *context)\n{\n\treturn container_of(context, struct etnaviv_iommuv2_context, base);\n}\n\nstatic void etnaviv_iommuv2_free(struct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tint i;\n\n\tdrm_mm_takedown(&context->mm);\n\n\tfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++) {\n\t\tif (v2_context->stlb_cpu[i])\n\t\t\tdma_free_wc(context->global->dev, SZ_4K,\n\t\t\t\t    v2_context->stlb_cpu[i],\n\t\t\t\t    v2_context->stlb_dma[i]);\n\t}\n\n\tdma_free_wc(context->global->dev, SZ_4K, v2_context->mtlb_cpu,\n\t\t    v2_context->mtlb_dma);\n\n\tclear_bit(v2_context->id, context->global->v2.pta_alloc);\n\n\tvfree(v2_context);\n}\nstatic int\netnaviv_iommuv2_ensure_stlb(struct etnaviv_iommuv2_context *v2_context,\n\t\t\t    int stlb)\n{\n\tif (v2_context->stlb_cpu[stlb])\n\t\treturn 0;\n\n\tv2_context->stlb_cpu[stlb] =\n\t\t\tdma_alloc_wc(v2_context->base.global->dev, SZ_4K,\n\t\t\t\t     &v2_context->stlb_dma[stlb],\n\t\t\t\t     GFP_KERNEL);\n\n\tif (!v2_context->stlb_cpu[stlb])\n\t\treturn -ENOMEM;\n\n\tmemset32(v2_context->stlb_cpu[stlb], MMUv2_PTE_EXCEPTION,\n\t\t SZ_4K / sizeof(u32));\n\n\tv2_context->mtlb_cpu[stlb] =\n\t\t\tv2_context->stlb_dma[stlb] | MMUv2_PTE_PRESENT;\n\n\treturn 0;\n}\n\nstatic int etnaviv_iommuv2_map(struct etnaviv_iommu_context *context,\n\t\t\t       unsigned long iova, phys_addr_t paddr,\n\t\t\t       size_t size, int prot)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tint mtlb_entry, stlb_entry, ret;\n\tu32 entry = lower_32_bits(paddr) | MMUv2_PTE_PRESENT;\n\n\tif (size != SZ_4K)\n\t\treturn -EINVAL;\n\n\tif (IS_ENABLED(CONFIG_PHYS_ADDR_T_64BIT))\n\t\tentry |= (upper_32_bits(paddr) & 0xff) << 4;\n\n\tif (prot & ETNAVIV_PROT_WRITE)\n\t\tentry |= MMUv2_PTE_WRITEABLE;\n\n\tmtlb_entry = (iova & MMUv2_MTLB_MASK) >> MMUv2_MTLB_SHIFT;\n\tstlb_entry = (iova & MMUv2_STLB_MASK) >> MMUv2_STLB_SHIFT;\n\n\tret = etnaviv_iommuv2_ensure_stlb(v2_context, mtlb_entry);\n\tif (ret)\n\t\treturn ret;\n\n\tv2_context->stlb_cpu[mtlb_entry][stlb_entry] = entry;\n\n\treturn 0;\n}\n\nstatic size_t etnaviv_iommuv2_unmap(struct etnaviv_iommu_context *context,\n\t\t\t\t    unsigned long iova, size_t size)\n{\n\tstruct etnaviv_iommuv2_context *etnaviv_domain = to_v2_context(context);\n\tint mtlb_entry, stlb_entry;\n\n\tif (size != SZ_4K)\n\t\treturn -EINVAL;\n\n\tmtlb_entry = (iova & MMUv2_MTLB_MASK) >> MMUv2_MTLB_SHIFT;\n\tstlb_entry = (iova & MMUv2_STLB_MASK) >> MMUv2_STLB_SHIFT;\n\n\tetnaviv_domain->stlb_cpu[mtlb_entry][stlb_entry] = MMUv2_PTE_EXCEPTION;\n\n\treturn SZ_4K;\n}\n\nstatic size_t etnaviv_iommuv2_dump_size(struct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tsize_t dump_size = SZ_4K;\n\tint i;\n\n\tfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++)\n\t\tif (v2_context->mtlb_cpu[i] & MMUv2_PTE_PRESENT)\n\t\t\tdump_size += SZ_4K;\n\n\treturn dump_size;\n}\n\nstatic void etnaviv_iommuv2_dump(struct etnaviv_iommu_context *context, void *buf)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tint i;\n\n\tmemcpy(buf, v2_context->mtlb_cpu, SZ_4K);\n\tbuf += SZ_4K;\n\tfor (i = 0; i < MMUv2_MAX_STLB_ENTRIES; i++)\n\t\tif (v2_context->mtlb_cpu[i] & MMUv2_PTE_PRESENT) {\n\t\t\tmemcpy(buf, v2_context->stlb_cpu[i], SZ_4K);\n\t\t\tbuf += SZ_4K;\n\t\t}\n}\n\nstatic void etnaviv_iommuv2_restore_nonsec(struct etnaviv_gpu *gpu,\n\tstruct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tu16 prefetch;\n\n\t \n\tif (gpu_read(gpu, VIVS_MMUv2_CONTROL) & VIVS_MMUv2_CONTROL_ENABLE)\n\t\treturn;\n\n\tif (gpu->mmu_context)\n\t\tetnaviv_iommu_context_put(gpu->mmu_context);\n\tgpu->mmu_context = etnaviv_iommu_context_get(context);\n\n\tprefetch = etnaviv_buffer_config_mmuv2(gpu,\n\t\t\t\t(u32)v2_context->mtlb_dma,\n\t\t\t\t(u32)context->global->bad_page_dma);\n\tetnaviv_gpu_start_fe(gpu, (u32)etnaviv_cmdbuf_get_pa(&gpu->buffer),\n\t\t\t     prefetch);\n\tetnaviv_gpu_wait_idle(gpu, 100);\n\n\tgpu_write(gpu, VIVS_MMUv2_CONTROL, VIVS_MMUv2_CONTROL_ENABLE);\n}\n\nstatic void etnaviv_iommuv2_restore_sec(struct etnaviv_gpu *gpu,\n\tstruct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\tu16 prefetch;\n\n\t \n\tif (gpu_read(gpu, VIVS_MMUv2_SEC_CONTROL) & VIVS_MMUv2_SEC_CONTROL_ENABLE)\n\t\treturn;\n\n\tif (gpu->mmu_context)\n\t\tetnaviv_iommu_context_put(gpu->mmu_context);\n\tgpu->mmu_context = etnaviv_iommu_context_get(context);\n\n\tgpu_write(gpu, VIVS_MMUv2_PTA_ADDRESS_LOW,\n\t\t  lower_32_bits(context->global->v2.pta_dma));\n\tgpu_write(gpu, VIVS_MMUv2_PTA_ADDRESS_HIGH,\n\t\t  upper_32_bits(context->global->v2.pta_dma));\n\tgpu_write(gpu, VIVS_MMUv2_PTA_CONTROL, VIVS_MMUv2_PTA_CONTROL_ENABLE);\n\n\tgpu_write(gpu, VIVS_MMUv2_NONSEC_SAFE_ADDR_LOW,\n\t\t  lower_32_bits(context->global->bad_page_dma));\n\tgpu_write(gpu, VIVS_MMUv2_SEC_SAFE_ADDR_LOW,\n\t\t  lower_32_bits(context->global->bad_page_dma));\n\tgpu_write(gpu, VIVS_MMUv2_SAFE_ADDRESS_CONFIG,\n\t\t  VIVS_MMUv2_SAFE_ADDRESS_CONFIG_NON_SEC_SAFE_ADDR_HIGH(\n\t\t  upper_32_bits(context->global->bad_page_dma)) |\n\t\t  VIVS_MMUv2_SAFE_ADDRESS_CONFIG_SEC_SAFE_ADDR_HIGH(\n\t\t  upper_32_bits(context->global->bad_page_dma)));\n\n\tcontext->global->v2.pta_cpu[v2_context->id] = v2_context->mtlb_dma |\n\t\t\t\t \t VIVS_MMUv2_CONFIGURATION_MODE_MODE4_K;\n\n\t \n\tprefetch = etnaviv_buffer_config_pta(gpu, v2_context->id);\n\tetnaviv_gpu_start_fe(gpu, (u32)etnaviv_cmdbuf_get_pa(&gpu->buffer),\n\t\t\t     prefetch);\n\tetnaviv_gpu_wait_idle(gpu, 100);\n\n\tgpu_write(gpu, VIVS_MMUv2_SEC_CONTROL, VIVS_MMUv2_SEC_CONTROL_ENABLE);\n}\n\nu32 etnaviv_iommuv2_get_mtlb_addr(struct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\n\treturn v2_context->mtlb_dma;\n}\n\nunsigned short etnaviv_iommuv2_get_pta_id(struct etnaviv_iommu_context *context)\n{\n\tstruct etnaviv_iommuv2_context *v2_context = to_v2_context(context);\n\n\treturn v2_context->id;\n}\nstatic void etnaviv_iommuv2_restore(struct etnaviv_gpu *gpu,\n\t\t\t\t    struct etnaviv_iommu_context *context)\n{\n\tswitch (gpu->sec_mode) {\n\tcase ETNA_SEC_NONE:\n\t\tetnaviv_iommuv2_restore_nonsec(gpu, context);\n\t\tbreak;\n\tcase ETNA_SEC_KERNEL:\n\t\tetnaviv_iommuv2_restore_sec(gpu, context);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unhandled GPU security mode\\n\");\n\t\tbreak;\n\t}\n}\n\nconst struct etnaviv_iommu_ops etnaviv_iommuv2_ops = {\n\t.free = etnaviv_iommuv2_free,\n\t.map = etnaviv_iommuv2_map,\n\t.unmap = etnaviv_iommuv2_unmap,\n\t.dump_size = etnaviv_iommuv2_dump_size,\n\t.dump = etnaviv_iommuv2_dump,\n\t.restore = etnaviv_iommuv2_restore,\n};\n\nstruct etnaviv_iommu_context *\netnaviv_iommuv2_context_alloc(struct etnaviv_iommu_global *global)\n{\n\tstruct etnaviv_iommuv2_context *v2_context;\n\tstruct etnaviv_iommu_context *context;\n\n\tv2_context = vzalloc(sizeof(*v2_context));\n\tif (!v2_context)\n\t\treturn NULL;\n\n\tmutex_lock(&global->lock);\n\tv2_context->id = find_first_zero_bit(global->v2.pta_alloc,\n\t\t\t\t\t     ETNAVIV_PTA_ENTRIES);\n\tif (v2_context->id < ETNAVIV_PTA_ENTRIES) {\n\t\tset_bit(v2_context->id, global->v2.pta_alloc);\n\t} else {\n\t\tmutex_unlock(&global->lock);\n\t\tgoto out_free;\n\t}\n\tmutex_unlock(&global->lock);\n\n\tv2_context->mtlb_cpu = dma_alloc_wc(global->dev, SZ_4K,\n\t\t\t\t\t    &v2_context->mtlb_dma, GFP_KERNEL);\n\tif (!v2_context->mtlb_cpu)\n\t\tgoto out_free_id;\n\n\tmemset32(v2_context->mtlb_cpu, MMUv2_PTE_EXCEPTION,\n\t\t MMUv2_MAX_STLB_ENTRIES);\n\n\tglobal->v2.pta_cpu[v2_context->id] = v2_context->mtlb_dma;\n\n\tcontext = &v2_context->base;\n\tcontext->global = global;\n\tkref_init(&context->refcount);\n\tmutex_init(&context->lock);\n\tINIT_LIST_HEAD(&context->mappings);\n\tdrm_mm_init(&context->mm, SZ_4K, (u64)SZ_1G * 4 - SZ_4K);\n\n\treturn context;\n\nout_free_id:\n\tclear_bit(v2_context->id, global->v2.pta_alloc);\nout_free:\n\tvfree(v2_context);\n\treturn NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}