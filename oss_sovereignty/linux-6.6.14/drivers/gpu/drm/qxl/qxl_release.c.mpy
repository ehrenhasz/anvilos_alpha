{
  "module_name": "qxl_release.c",
  "hash_id": "433b930d591ab34f4f37d5878c9e13a311fcbf9bdb01ab52dc3d1ea005675f5d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/qxl/qxl_release.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n\n#include <trace/events/dma_fence.h>\n\n#include \"qxl_drv.h\"\n#include \"qxl_object.h\"\n\n \n \n \n#define RELEASE_SIZE 256\n#define RELEASES_PER_BO (PAGE_SIZE / RELEASE_SIZE)\n \n#define SURFACE_RELEASE_SIZE 128\n#define SURFACE_RELEASES_PER_BO (PAGE_SIZE / SURFACE_RELEASE_SIZE)\n\nstatic const int release_size_per_bo[] = { RELEASE_SIZE, SURFACE_RELEASE_SIZE, RELEASE_SIZE };\nstatic const int releases_per_bo[] = { RELEASES_PER_BO, SURFACE_RELEASES_PER_BO, RELEASES_PER_BO };\n\nstatic const char *qxl_get_driver_name(struct dma_fence *fence)\n{\n\treturn \"qxl\";\n}\n\nstatic const char *qxl_get_timeline_name(struct dma_fence *fence)\n{\n\treturn \"release\";\n}\n\nstatic long qxl_fence_wait(struct dma_fence *fence, bool intr,\n\t\t\t   signed long timeout)\n{\n\tstruct qxl_device *qdev;\n\tunsigned long cur, end = jiffies + timeout;\n\n\tqdev = container_of(fence->lock, struct qxl_device, release_lock);\n\n\tif (!wait_event_timeout(qdev->release_event,\n\t\t\t\t(dma_fence_is_signaled(fence) ||\n\t\t\t\t (qxl_io_notify_oom(qdev), 0)),\n\t\t\t\ttimeout))\n\t\treturn 0;\n\n\tcur = jiffies;\n\tif (time_after(cur, end))\n\t\treturn 0;\n\treturn end - cur;\n}\n\nstatic const struct dma_fence_ops qxl_fence_ops = {\n\t.get_driver_name = qxl_get_driver_name,\n\t.get_timeline_name = qxl_get_timeline_name,\n\t.wait = qxl_fence_wait,\n};\n\nstatic int\nqxl_release_alloc(struct qxl_device *qdev, int type,\n\t\t  struct qxl_release **ret)\n{\n\tstruct qxl_release *release;\n\tint handle;\n\tsize_t size = sizeof(*release);\n\n\trelease = kmalloc(size, GFP_KERNEL);\n\tif (!release) {\n\t\tDRM_ERROR(\"Out of memory\\n\");\n\t\treturn -ENOMEM;\n\t}\n\trelease->base.ops = NULL;\n\trelease->type = type;\n\trelease->release_offset = 0;\n\trelease->surface_release_id = 0;\n\tINIT_LIST_HEAD(&release->bos);\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&qdev->release_idr_lock);\n\thandle = idr_alloc(&qdev->release_idr, release, 1, 0, GFP_NOWAIT);\n\trelease->base.seqno = ++qdev->release_seqno;\n\tspin_unlock(&qdev->release_idr_lock);\n\tidr_preload_end();\n\tif (handle < 0) {\n\t\tkfree(release);\n\t\t*ret = NULL;\n\t\treturn handle;\n\t}\n\t*ret = release;\n\tDRM_DEBUG_DRIVER(\"allocated release %d\\n\", handle);\n\trelease->id = handle;\n\treturn handle;\n}\n\nstatic void\nqxl_release_free_list(struct qxl_release *release)\n{\n\twhile (!list_empty(&release->bos)) {\n\t\tstruct qxl_bo_list *entry;\n\t\tstruct qxl_bo *bo;\n\n\t\tentry = container_of(release->bos.next,\n\t\t\t\t     struct qxl_bo_list, tv.head);\n\t\tbo = to_qxl_bo(entry->tv.bo);\n\t\tqxl_bo_unref(&bo);\n\t\tlist_del(&entry->tv.head);\n\t\tkfree(entry);\n\t}\n\trelease->release_bo = NULL;\n}\n\nvoid\nqxl_release_free(struct qxl_device *qdev,\n\t\t struct qxl_release *release)\n{\n\tDRM_DEBUG_DRIVER(\"release %d, type %d\\n\", release->id, release->type);\n\n\tif (release->surface_release_id)\n\t\tqxl_surface_id_dealloc(qdev, release->surface_release_id);\n\n\tspin_lock(&qdev->release_idr_lock);\n\tidr_remove(&qdev->release_idr, release->id);\n\tspin_unlock(&qdev->release_idr_lock);\n\n\tif (release->base.ops) {\n\t\tWARN_ON(list_empty(&release->bos));\n\t\tqxl_release_free_list(release);\n\n\t\tdma_fence_signal(&release->base);\n\t\tdma_fence_put(&release->base);\n\t} else {\n\t\tqxl_release_free_list(release);\n\t\tkfree(release);\n\t}\n\tatomic_dec(&qdev->release_count);\n}\n\nstatic int qxl_release_bo_alloc(struct qxl_device *qdev,\n\t\t\t\tstruct qxl_bo **bo,\n\t\t\t\tu32 priority)\n{\n\t \n\treturn qxl_bo_create(qdev, PAGE_SIZE, false, true,\n\t\t\t     QXL_GEM_DOMAIN_VRAM, priority, NULL, bo);\n}\n\nint qxl_release_list_add(struct qxl_release *release, struct qxl_bo *bo)\n{\n\tstruct qxl_bo_list *entry;\n\n\tlist_for_each_entry(entry, &release->bos, tv.head) {\n\t\tif (entry->tv.bo == &bo->tbo)\n\t\t\treturn 0;\n\t}\n\n\tentry = kmalloc(sizeof(struct qxl_bo_list), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tqxl_bo_ref(bo);\n\tentry->tv.bo = &bo->tbo;\n\tentry->tv.num_shared = 0;\n\tlist_add_tail(&entry->tv.head, &release->bos);\n\treturn 0;\n}\n\nstatic int qxl_release_validate_bo(struct qxl_bo *bo)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tint ret;\n\n\tif (!bo->tbo.pin_count) {\n\t\tqxl_ttm_placement_from_domain(bo, bo->type);\n\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = qxl_bo_check_id(to_qxl(bo->tbo.base.dev), bo);\n\tif (ret)\n\t\treturn ret;\n\treturn 0;\n}\n\nint qxl_release_reserve_list(struct qxl_release *release, bool no_intr)\n{\n\tint ret;\n\tstruct qxl_bo_list *entry;\n\n\t \n\tif (list_is_singular(&release->bos))\n\t\treturn 0;\n\n\tret = ttm_eu_reserve_buffers(&release->ticket, &release->bos,\n\t\t\t\t     !no_intr, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tlist_for_each_entry(entry, &release->bos, tv.head) {\n\t\tstruct qxl_bo *bo = to_qxl_bo(entry->tv.bo);\n\n\t\tret = qxl_release_validate_bo(bo);\n\t\tif (ret) {\n\t\t\tttm_eu_backoff_reservation(&release->ticket, &release->bos);\n\t\t\treturn ret;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid qxl_release_backoff_reserve_list(struct qxl_release *release)\n{\n\t \n\tif (list_is_singular(&release->bos))\n\t\treturn;\n\n\tttm_eu_backoff_reservation(&release->ticket, &release->bos);\n}\n\nint qxl_alloc_surface_release_reserved(struct qxl_device *qdev,\n\t\t\t\t       enum qxl_surface_cmd_type surface_cmd_type,\n\t\t\t\t       struct qxl_release *create_rel,\n\t\t\t\t       struct qxl_release **release)\n{\n\tif (surface_cmd_type == QXL_SURFACE_CMD_DESTROY && create_rel) {\n\t\tint idr_ret;\n\t\tstruct qxl_bo *bo;\n\t\tunion qxl_release_info *info;\n\n\t\t \n\t\tidr_ret = qxl_release_alloc(qdev, QXL_RELEASE_SURFACE_CMD, release);\n\t\tif (idr_ret < 0)\n\t\t\treturn idr_ret;\n\t\tbo = create_rel->release_bo;\n\n\t\t(*release)->release_bo = bo;\n\t\t(*release)->release_offset = create_rel->release_offset + 64;\n\n\t\tqxl_release_list_add(*release, bo);\n\n\t\tinfo = qxl_release_map(qdev, *release);\n\t\tinfo->id = idr_ret;\n\t\tqxl_release_unmap(qdev, *release, info);\n\t\treturn 0;\n\t}\n\n\treturn qxl_alloc_release_reserved(qdev, sizeof(struct qxl_surface_cmd),\n\t\t\t\t\t QXL_RELEASE_SURFACE_CMD, release, NULL);\n}\n\nint qxl_alloc_release_reserved(struct qxl_device *qdev, unsigned long size,\n\t\t\t\t       int type, struct qxl_release **release,\n\t\t\t\t       struct qxl_bo **rbo)\n{\n\tstruct qxl_bo *bo, *free_bo = NULL;\n\tint idr_ret;\n\tint ret = 0;\n\tunion qxl_release_info *info;\n\tint cur_idx;\n\tu32 priority;\n\n\tif (type == QXL_RELEASE_DRAWABLE) {\n\t\tcur_idx = 0;\n\t\tpriority = 0;\n\t} else if (type == QXL_RELEASE_SURFACE_CMD) {\n\t\tcur_idx = 1;\n\t\tpriority = 1;\n\t} else if (type == QXL_RELEASE_CURSOR_CMD) {\n\t\tcur_idx = 2;\n\t\tpriority = 1;\n\t}\n\telse {\n\t\tDRM_ERROR(\"got illegal type: %d\\n\", type);\n\t\treturn -EINVAL;\n\t}\n\n\tidr_ret = qxl_release_alloc(qdev, type, release);\n\tif (idr_ret < 0) {\n\t\tif (rbo)\n\t\t\t*rbo = NULL;\n\t\treturn idr_ret;\n\t}\n\tatomic_inc(&qdev->release_count);\n\n\tmutex_lock(&qdev->release_mutex);\n\tif (qdev->current_release_bo_offset[cur_idx] + 1 >= releases_per_bo[cur_idx]) {\n\t\tfree_bo = qdev->current_release_bo[cur_idx];\n\t\tqdev->current_release_bo_offset[cur_idx] = 0;\n\t\tqdev->current_release_bo[cur_idx] = NULL;\n\t}\n\tif (!qdev->current_release_bo[cur_idx]) {\n\t\tret = qxl_release_bo_alloc(qdev, &qdev->current_release_bo[cur_idx], priority);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&qdev->release_mutex);\n\t\t\tif (free_bo) {\n\t\t\t\tqxl_bo_unpin(free_bo);\n\t\t\t\tqxl_bo_unref(&free_bo);\n\t\t\t}\n\t\t\tqxl_release_free(qdev, *release);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tbo = qxl_bo_ref(qdev->current_release_bo[cur_idx]);\n\n\t(*release)->release_bo = bo;\n\t(*release)->release_offset = qdev->current_release_bo_offset[cur_idx] * release_size_per_bo[cur_idx];\n\tqdev->current_release_bo_offset[cur_idx]++;\n\n\tif (rbo)\n\t\t*rbo = bo;\n\n\tmutex_unlock(&qdev->release_mutex);\n\tif (free_bo) {\n\t\tqxl_bo_unpin(free_bo);\n\t\tqxl_bo_unref(&free_bo);\n\t}\n\n\tret = qxl_release_list_add(*release, bo);\n\tqxl_bo_unref(&bo);\n\tif (ret) {\n\t\tqxl_release_free(qdev, *release);\n\t\treturn ret;\n\t}\n\n\tinfo = qxl_release_map(qdev, *release);\n\tinfo->id = idr_ret;\n\tqxl_release_unmap(qdev, *release, info);\n\n\treturn ret;\n}\n\nstruct qxl_release *qxl_release_from_id_locked(struct qxl_device *qdev,\n\t\t\t\t\t\t   uint64_t id)\n{\n\tstruct qxl_release *release;\n\n\tspin_lock(&qdev->release_idr_lock);\n\trelease = idr_find(&qdev->release_idr, id);\n\tspin_unlock(&qdev->release_idr_lock);\n\tif (!release) {\n\t\tDRM_ERROR(\"failed to find id in release_idr\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn release;\n}\n\nunion qxl_release_info *qxl_release_map(struct qxl_device *qdev,\n\t\t\t\t\tstruct qxl_release *release)\n{\n\tvoid *ptr;\n\tunion qxl_release_info *info;\n\tstruct qxl_bo *bo = release->release_bo;\n\n\tptr = qxl_bo_kmap_atomic_page(qdev, bo, release->release_offset & PAGE_MASK);\n\tif (!ptr)\n\t\treturn NULL;\n\tinfo = ptr + (release->release_offset & ~PAGE_MASK);\n\treturn info;\n}\n\nvoid qxl_release_unmap(struct qxl_device *qdev,\n\t\t       struct qxl_release *release,\n\t\t       union qxl_release_info *info)\n{\n\tstruct qxl_bo *bo = release->release_bo;\n\tvoid *ptr;\n\n\tptr = ((void *)info) - (release->release_offset & ~PAGE_MASK);\n\tqxl_bo_kunmap_atomic_page(qdev, bo, ptr);\n}\n\nvoid qxl_release_fence_buffer_objects(struct qxl_release *release)\n{\n\tstruct ttm_buffer_object *bo;\n\tstruct ttm_device *bdev;\n\tstruct ttm_validate_buffer *entry;\n\tstruct qxl_device *qdev;\n\n\t \n\tif (list_is_singular(&release->bos) || list_empty(&release->bos))\n\t\treturn;\n\n\tbo = list_first_entry(&release->bos, struct ttm_validate_buffer, head)->bo;\n\tbdev = bo->bdev;\n\tqdev = container_of(bdev, struct qxl_device, mman.bdev);\n\n\t \n\tdma_fence_init(&release->base, &qxl_fence_ops, &qdev->release_lock,\n\t\t       release->id | 0xf0000000, release->base.seqno);\n\ttrace_dma_fence_emit(&release->base);\n\n\tlist_for_each_entry(entry, &release->bos, head) {\n\t\tbo = entry->bo;\n\n\t\tdma_resv_add_fence(bo->base.resv, &release->base,\n\t\t\t\t   DMA_RESV_USAGE_READ);\n\t\tttm_bo_move_to_lru_tail_unlocked(bo);\n\t\tdma_resv_unlock(bo->base.resv);\n\t}\n\tww_acquire_fini(&release->ticket);\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}