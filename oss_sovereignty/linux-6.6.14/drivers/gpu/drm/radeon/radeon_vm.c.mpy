{
  "module_name": "radeon_vm.c",
  "hash_id": "cfd146d9d0650fa413e0bab22bfef783ae208aaaf7658eef9f69fd2077f7b1c3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/radeon_vm.c",
  "human_readable_source": " \n\n#include <drm/radeon_drm.h>\n#include \"radeon.h\"\n#include \"radeon_trace.h\"\n\n \n\n \nstatic unsigned radeon_vm_num_pdes(struct radeon_device *rdev)\n{\n\treturn rdev->vm_manager.max_pfn >> radeon_vm_block_size;\n}\n\n \nstatic unsigned radeon_vm_directory_size(struct radeon_device *rdev)\n{\n\treturn RADEON_GPU_PAGE_ALIGN(radeon_vm_num_pdes(rdev) * 8);\n}\n\n \nint radeon_vm_manager_init(struct radeon_device *rdev)\n{\n\tint r;\n\n\tif (!rdev->vm_manager.enabled) {\n\t\tr = radeon_asic_vm_init(rdev);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\trdev->vm_manager.enabled = true;\n\t}\n\treturn 0;\n}\n\n \nvoid radeon_vm_manager_fini(struct radeon_device *rdev)\n{\n\tint i;\n\n\tif (!rdev->vm_manager.enabled)\n\t\treturn;\n\n\tfor (i = 0; i < RADEON_NUM_VM; ++i)\n\t\tradeon_fence_unref(&rdev->vm_manager.active[i]);\n\tradeon_asic_vm_fini(rdev);\n\trdev->vm_manager.enabled = false;\n}\n\n \nstruct radeon_bo_list *radeon_vm_get_bos(struct radeon_device *rdev,\n\t\t\t\t\t  struct radeon_vm *vm,\n\t\t\t\t\t  struct list_head *head)\n{\n\tstruct radeon_bo_list *list;\n\tunsigned i, idx;\n\n\tlist = kvmalloc_array(vm->max_pde_used + 2,\n\t\t\t     sizeof(struct radeon_bo_list), GFP_KERNEL);\n\tif (!list)\n\t\treturn NULL;\n\n\t \n\tlist[0].robj = vm->page_directory;\n\tlist[0].preferred_domains = RADEON_GEM_DOMAIN_VRAM;\n\tlist[0].allowed_domains = RADEON_GEM_DOMAIN_VRAM;\n\tlist[0].tv.bo = &vm->page_directory->tbo;\n\tlist[0].tv.num_shared = 1;\n\tlist[0].tiling_flags = 0;\n\tlist_add(&list[0].tv.head, head);\n\n\tfor (i = 0, idx = 1; i <= vm->max_pde_used; i++) {\n\t\tif (!vm->page_tables[i].bo)\n\t\t\tcontinue;\n\n\t\tlist[idx].robj = vm->page_tables[i].bo;\n\t\tlist[idx].preferred_domains = RADEON_GEM_DOMAIN_VRAM;\n\t\tlist[idx].allowed_domains = RADEON_GEM_DOMAIN_VRAM;\n\t\tlist[idx].tv.bo = &list[idx].robj->tbo;\n\t\tlist[idx].tv.num_shared = 1;\n\t\tlist[idx].tiling_flags = 0;\n\t\tlist_add(&list[idx++].tv.head, head);\n\t}\n\n\treturn list;\n}\n\n \nstruct radeon_fence *radeon_vm_grab_id(struct radeon_device *rdev,\n\t\t\t\t       struct radeon_vm *vm, int ring)\n{\n\tstruct radeon_fence *best[RADEON_NUM_RINGS] = {};\n\tstruct radeon_vm_id *vm_id = &vm->ids[ring];\n\n\tunsigned choices[2] = {};\n\tunsigned i;\n\n\t \n\tif (vm_id->id && vm_id->last_id_use &&\n\t    vm_id->last_id_use == rdev->vm_manager.active[vm_id->id])\n\t\treturn NULL;\n\n\t \n\tvm_id->pd_gpu_addr = ~0ll;\n\n\t \n\tfor (i = 1; i < rdev->vm_manager.nvm; ++i) {\n\t\tstruct radeon_fence *fence = rdev->vm_manager.active[i];\n\n\t\tif (fence == NULL) {\n\t\t\t \n\t\t\tvm_id->id = i;\n\t\t\ttrace_radeon_vm_grab_id(i, ring);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (radeon_fence_is_earlier(fence, best[fence->ring])) {\n\t\t\tbest[fence->ring] = fence;\n\t\t\tchoices[fence->ring == ring ? 0 : 1] = i;\n\t\t}\n\t}\n\n\tfor (i = 0; i < 2; ++i) {\n\t\tif (choices[i]) {\n\t\t\tvm_id->id = choices[i];\n\t\t\ttrace_radeon_vm_grab_id(choices[i], ring);\n\t\t\treturn rdev->vm_manager.active[choices[i]];\n\t\t}\n\t}\n\n\t \n\tBUG();\n\treturn NULL;\n}\n\n \nvoid radeon_vm_flush(struct radeon_device *rdev,\n\t\t     struct radeon_vm *vm,\n\t\t     int ring, struct radeon_fence *updates)\n{\n\tuint64_t pd_addr = radeon_bo_gpu_offset(vm->page_directory);\n\tstruct radeon_vm_id *vm_id = &vm->ids[ring];\n\n\tif (pd_addr != vm_id->pd_gpu_addr || !vm_id->flushed_updates ||\n\t    radeon_fence_is_earlier(vm_id->flushed_updates, updates)) {\n\n\t\ttrace_radeon_vm_flush(pd_addr, ring, vm->ids[ring].id);\n\t\tradeon_fence_unref(&vm_id->flushed_updates);\n\t\tvm_id->flushed_updates = radeon_fence_ref(updates);\n\t\tvm_id->pd_gpu_addr = pd_addr;\n\t\tradeon_ring_vm_flush(rdev, &rdev->ring[ring],\n\t\t\t\t     vm_id->id, vm_id->pd_gpu_addr);\n\n\t}\n}\n\n \nvoid radeon_vm_fence(struct radeon_device *rdev,\n\t\t     struct radeon_vm *vm,\n\t\t     struct radeon_fence *fence)\n{\n\tunsigned vm_id = vm->ids[fence->ring].id;\n\n\tradeon_fence_unref(&rdev->vm_manager.active[vm_id]);\n\trdev->vm_manager.active[vm_id] = radeon_fence_ref(fence);\n\n\tradeon_fence_unref(&vm->ids[fence->ring].last_id_use);\n\tvm->ids[fence->ring].last_id_use = radeon_fence_ref(fence);\n}\n\n \nstruct radeon_bo_va *radeon_vm_bo_find(struct radeon_vm *vm,\n\t\t\t\t       struct radeon_bo *bo)\n{\n\tstruct radeon_bo_va *bo_va;\n\n\tlist_for_each_entry(bo_va, &bo->va, bo_list) {\n\t\tif (bo_va->vm == vm)\n\t\t\treturn bo_va;\n\n\t}\n\treturn NULL;\n}\n\n \nstruct radeon_bo_va *radeon_vm_bo_add(struct radeon_device *rdev,\n\t\t\t\t      struct radeon_vm *vm,\n\t\t\t\t      struct radeon_bo *bo)\n{\n\tstruct radeon_bo_va *bo_va;\n\n\tbo_va = kzalloc(sizeof(struct radeon_bo_va), GFP_KERNEL);\n\tif (bo_va == NULL)\n\t\treturn NULL;\n\n\tbo_va->vm = vm;\n\tbo_va->bo = bo;\n\tbo_va->it.start = 0;\n\tbo_va->it.last = 0;\n\tbo_va->flags = 0;\n\tbo_va->ref_count = 1;\n\tINIT_LIST_HEAD(&bo_va->bo_list);\n\tINIT_LIST_HEAD(&bo_va->vm_status);\n\n\tmutex_lock(&vm->mutex);\n\tlist_add_tail(&bo_va->bo_list, &bo->va);\n\tmutex_unlock(&vm->mutex);\n\n\treturn bo_va;\n}\n\n \nstatic void radeon_vm_set_pages(struct radeon_device *rdev,\n\t\t\t\tstruct radeon_ib *ib,\n\t\t\t\tuint64_t pe,\n\t\t\t\tuint64_t addr, unsigned count,\n\t\t\t\tuint32_t incr, uint32_t flags)\n{\n\ttrace_radeon_vm_set_page(pe, addr, count, incr, flags);\n\n\tif ((flags & R600_PTE_GART_MASK) == R600_PTE_GART_MASK) {\n\t\tuint64_t src = rdev->gart.table_addr + (addr >> 12) * 8;\n\t\tradeon_asic_vm_copy_pages(rdev, ib, pe, src, count);\n\n\t} else if ((flags & R600_PTE_SYSTEM) || (count < 3)) {\n\t\tradeon_asic_vm_write_pages(rdev, ib, pe, addr,\n\t\t\t\t\t   count, incr, flags);\n\n\t} else {\n\t\tradeon_asic_vm_set_pages(rdev, ib, pe, addr,\n\t\t\t\t\t count, incr, flags);\n\t}\n}\n\n \nstatic int radeon_vm_clear_bo(struct radeon_device *rdev,\n\t\t\t      struct radeon_bo *bo)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct radeon_ib ib;\n\tunsigned entries;\n\tuint64_t addr;\n\tint r;\n\n\tr = radeon_bo_reserve(bo, false);\n\tif (r)\n\t\treturn r;\n\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (r)\n\t\tgoto error_unreserve;\n\n\taddr = radeon_bo_gpu_offset(bo);\n\tentries = radeon_bo_size(bo) / 8;\n\n\tr = radeon_ib_get(rdev, R600_RING_TYPE_DMA_INDEX, &ib, NULL, 256);\n\tif (r)\n\t\tgoto error_unreserve;\n\n\tib.length_dw = 0;\n\n\tradeon_vm_set_pages(rdev, &ib, addr, 0, entries, 0, 0);\n\tradeon_asic_vm_pad_ib(rdev, &ib);\n\tWARN_ON(ib.length_dw > 64);\n\n\tr = radeon_ib_schedule(rdev, &ib, NULL, false);\n\tif (r)\n\t\tgoto error_free;\n\n\tib.fence->is_vm_update = true;\n\tradeon_bo_fence(bo, ib.fence, false);\n\nerror_free:\n\tradeon_ib_free(rdev, &ib);\n\nerror_unreserve:\n\tradeon_bo_unreserve(bo);\n\treturn r;\n}\n\n \nint radeon_vm_bo_set_addr(struct radeon_device *rdev,\n\t\t\t  struct radeon_bo_va *bo_va,\n\t\t\t  uint64_t soffset,\n\t\t\t  uint32_t flags)\n{\n\tuint64_t size = radeon_bo_size(bo_va->bo);\n\tstruct radeon_vm *vm = bo_va->vm;\n\tunsigned last_pfn, pt_idx;\n\tuint64_t eoffset;\n\tint r;\n\n\tif (soffset) {\n\t\t \n\t\teoffset = soffset + size - 1;\n\t\tif (soffset >= eoffset) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto error_unreserve;\n\t\t}\n\n\t\tlast_pfn = eoffset / RADEON_GPU_PAGE_SIZE;\n\t\tif (last_pfn >= rdev->vm_manager.max_pfn) {\n\t\t\tdev_err(rdev->dev, \"va above limit (0x%08X >= 0x%08X)\\n\",\n\t\t\t\tlast_pfn, rdev->vm_manager.max_pfn);\n\t\t\tr = -EINVAL;\n\t\t\tgoto error_unreserve;\n\t\t}\n\n\t} else {\n\t\teoffset = last_pfn = 0;\n\t}\n\n\tmutex_lock(&vm->mutex);\n\tsoffset /= RADEON_GPU_PAGE_SIZE;\n\teoffset /= RADEON_GPU_PAGE_SIZE;\n\tif (soffset || eoffset) {\n\t\tstruct interval_tree_node *it;\n\t\tit = interval_tree_iter_first(&vm->va, soffset, eoffset);\n\t\tif (it && it != &bo_va->it) {\n\t\t\tstruct radeon_bo_va *tmp;\n\t\t\ttmp = container_of(it, struct radeon_bo_va, it);\n\t\t\t \n\t\t\tdev_err(rdev->dev, \"bo %p va 0x%010Lx conflict with \"\n\t\t\t\t\"(bo %p 0x%010lx 0x%010lx)\\n\", bo_va->bo,\n\t\t\t\tsoffset, tmp->bo, tmp->it.start, tmp->it.last);\n\t\t\tmutex_unlock(&vm->mutex);\n\t\t\tr = -EINVAL;\n\t\t\tgoto error_unreserve;\n\t\t}\n\t}\n\n\tif (bo_va->it.start || bo_va->it.last) {\n\t\t \n\t\tstruct radeon_bo_va *tmp;\n\t\ttmp = kzalloc(sizeof(struct radeon_bo_va), GFP_KERNEL);\n\t\tif (!tmp) {\n\t\t\tmutex_unlock(&vm->mutex);\n\t\t\tr = -ENOMEM;\n\t\t\tgoto error_unreserve;\n\t\t}\n\t\ttmp->it.start = bo_va->it.start;\n\t\ttmp->it.last = bo_va->it.last;\n\t\ttmp->vm = vm;\n\t\ttmp->bo = radeon_bo_ref(bo_va->bo);\n\n\t\tinterval_tree_remove(&bo_va->it, &vm->va);\n\t\tspin_lock(&vm->status_lock);\n\t\tbo_va->it.start = 0;\n\t\tbo_va->it.last = 0;\n\t\tlist_del_init(&bo_va->vm_status);\n\t\tlist_add(&tmp->vm_status, &vm->freed);\n\t\tspin_unlock(&vm->status_lock);\n\t}\n\n\tif (soffset || eoffset) {\n\t\tspin_lock(&vm->status_lock);\n\t\tbo_va->it.start = soffset;\n\t\tbo_va->it.last = eoffset;\n\t\tlist_add(&bo_va->vm_status, &vm->cleared);\n\t\tspin_unlock(&vm->status_lock);\n\t\tinterval_tree_insert(&bo_va->it, &vm->va);\n\t}\n\n\tbo_va->flags = flags;\n\n\tsoffset >>= radeon_vm_block_size;\n\teoffset >>= radeon_vm_block_size;\n\n\tBUG_ON(eoffset >= radeon_vm_num_pdes(rdev));\n\n\tif (eoffset > vm->max_pde_used)\n\t\tvm->max_pde_used = eoffset;\n\n\tradeon_bo_unreserve(bo_va->bo);\n\n\t \n\tfor (pt_idx = soffset; pt_idx <= eoffset; ++pt_idx) {\n\t\tstruct radeon_bo *pt;\n\n\t\tif (vm->page_tables[pt_idx].bo)\n\t\t\tcontinue;\n\n\t\t \n\t\tmutex_unlock(&vm->mutex);\n\n\t\tr = radeon_bo_create(rdev, RADEON_VM_PTE_COUNT * 8,\n\t\t\t\t     RADEON_GPU_PAGE_SIZE, true,\n\t\t\t\t     RADEON_GEM_DOMAIN_VRAM, 0,\n\t\t\t\t     NULL, NULL, &pt);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = radeon_vm_clear_bo(rdev, pt);\n\t\tif (r) {\n\t\t\tradeon_bo_unref(&pt);\n\t\t\treturn r;\n\t\t}\n\n\t\t \n\t\tmutex_lock(&vm->mutex);\n\t\tif (vm->page_tables[pt_idx].bo) {\n\t\t\t \n\t\t\tmutex_unlock(&vm->mutex);\n\t\t\tradeon_bo_unref(&pt);\n\t\t\tmutex_lock(&vm->mutex);\n\t\t\tcontinue;\n\t\t}\n\n\t\tvm->page_tables[pt_idx].addr = 0;\n\t\tvm->page_tables[pt_idx].bo = pt;\n\t}\n\n\tmutex_unlock(&vm->mutex);\n\treturn 0;\n\nerror_unreserve:\n\tradeon_bo_unreserve(bo_va->bo);\n\treturn r;\n}\n\n \nuint64_t radeon_vm_map_gart(struct radeon_device *rdev, uint64_t addr)\n{\n\tuint64_t result;\n\n\t \n\tresult = rdev->gart.pages_entry[addr >> RADEON_GPU_PAGE_SHIFT];\n\tresult &= ~RADEON_GPU_PAGE_MASK;\n\n\treturn result;\n}\n\n \nstatic uint32_t radeon_vm_page_flags(uint32_t flags)\n{\n\tuint32_t hw_flags = 0;\n\n\thw_flags |= (flags & RADEON_VM_PAGE_VALID) ? R600_PTE_VALID : 0;\n\thw_flags |= (flags & RADEON_VM_PAGE_READABLE) ? R600_PTE_READABLE : 0;\n\thw_flags |= (flags & RADEON_VM_PAGE_WRITEABLE) ? R600_PTE_WRITEABLE : 0;\n\tif (flags & RADEON_VM_PAGE_SYSTEM) {\n\t\thw_flags |= R600_PTE_SYSTEM;\n\t\thw_flags |= (flags & RADEON_VM_PAGE_SNOOPED) ? R600_PTE_SNOOPED : 0;\n\t}\n\treturn hw_flags;\n}\n\n \nint radeon_vm_update_page_directory(struct radeon_device *rdev,\n\t\t\t\t    struct radeon_vm *vm)\n{\n\tstruct radeon_bo *pd = vm->page_directory;\n\tuint64_t pd_addr = radeon_bo_gpu_offset(pd);\n\tuint32_t incr = RADEON_VM_PTE_COUNT * 8;\n\tuint64_t last_pde = ~0, last_pt = ~0;\n\tunsigned count = 0, pt_idx, ndw;\n\tstruct radeon_ib ib;\n\tint r;\n\n\t \n\tndw = 64;\n\n\t \n\tndw += vm->max_pde_used * 6;\n\n\t \n\tif (ndw > 0xfffff)\n\t\treturn -ENOMEM;\n\n\tr = radeon_ib_get(rdev, R600_RING_TYPE_DMA_INDEX, &ib, NULL, ndw * 4);\n\tif (r)\n\t\treturn r;\n\tib.length_dw = 0;\n\n\t \n\tfor (pt_idx = 0; pt_idx <= vm->max_pde_used; ++pt_idx) {\n\t\tstruct radeon_bo *bo = vm->page_tables[pt_idx].bo;\n\t\tuint64_t pde, pt;\n\n\t\tif (bo == NULL)\n\t\t\tcontinue;\n\n\t\tpt = radeon_bo_gpu_offset(bo);\n\t\tif (vm->page_tables[pt_idx].addr == pt)\n\t\t\tcontinue;\n\t\tvm->page_tables[pt_idx].addr = pt;\n\n\t\tpde = pd_addr + pt_idx * 8;\n\t\tif (((last_pde + 8 * count) != pde) ||\n\t\t    ((last_pt + incr * count) != pt)) {\n\n\t\t\tif (count) {\n\t\t\t\tradeon_vm_set_pages(rdev, &ib, last_pde,\n\t\t\t\t\t\t    last_pt, count, incr,\n\t\t\t\t\t\t    R600_PTE_VALID);\n\t\t\t}\n\n\t\t\tcount = 1;\n\t\t\tlast_pde = pde;\n\t\t\tlast_pt = pt;\n\t\t} else {\n\t\t\t++count;\n\t\t}\n\t}\n\n\tif (count)\n\t\tradeon_vm_set_pages(rdev, &ib, last_pde, last_pt, count,\n\t\t\t\t    incr, R600_PTE_VALID);\n\n\tif (ib.length_dw != 0) {\n\t\tradeon_asic_vm_pad_ib(rdev, &ib);\n\n\t\tradeon_sync_resv(rdev, &ib.sync, pd->tbo.base.resv, true);\n\t\tWARN_ON(ib.length_dw > ndw);\n\t\tr = radeon_ib_schedule(rdev, &ib, NULL, false);\n\t\tif (r) {\n\t\t\tradeon_ib_free(rdev, &ib);\n\t\t\treturn r;\n\t\t}\n\t\tib.fence->is_vm_update = true;\n\t\tradeon_bo_fence(pd, ib.fence, false);\n\t}\n\tradeon_ib_free(rdev, &ib);\n\n\treturn 0;\n}\n\n \nstatic void radeon_vm_frag_ptes(struct radeon_device *rdev,\n\t\t\t\tstruct radeon_ib *ib,\n\t\t\t\tuint64_t pe_start, uint64_t pe_end,\n\t\t\t\tuint64_t addr, uint32_t flags)\n{\n\t \n\n\t \n\tuint64_t frag_flags = ((rdev->family == CHIP_CAYMAN) ||\n\t\t\t       (rdev->family == CHIP_ARUBA)) ?\n\t\t\tR600_PTE_FRAG_256KB : R600_PTE_FRAG_64KB;\n\tuint64_t frag_align = ((rdev->family == CHIP_CAYMAN) ||\n\t\t\t       (rdev->family == CHIP_ARUBA)) ? 0x200 : 0x80;\n\n\tuint64_t frag_start = ALIGN(pe_start, frag_align);\n\tuint64_t frag_end = pe_end & ~(frag_align - 1);\n\n\tunsigned count;\n\n\t \n\tif ((flags & R600_PTE_SYSTEM) || !(flags & R600_PTE_VALID) ||\n\t    (frag_start >= frag_end)) {\n\n\t\tcount = (pe_end - pe_start) / 8;\n\t\tradeon_vm_set_pages(rdev, ib, pe_start, addr, count,\n\t\t\t\t    RADEON_GPU_PAGE_SIZE, flags);\n\t\treturn;\n\t}\n\n\t \n\tif (pe_start != frag_start) {\n\t\tcount = (frag_start - pe_start) / 8;\n\t\tradeon_vm_set_pages(rdev, ib, pe_start, addr, count,\n\t\t\t\t    RADEON_GPU_PAGE_SIZE, flags);\n\t\taddr += RADEON_GPU_PAGE_SIZE * count;\n\t}\n\n\t \n\tcount = (frag_end - frag_start) / 8;\n\tradeon_vm_set_pages(rdev, ib, frag_start, addr, count,\n\t\t\t    RADEON_GPU_PAGE_SIZE, flags | frag_flags);\n\n\t \n\tif (frag_end != pe_end) {\n\t\taddr += RADEON_GPU_PAGE_SIZE * count;\n\t\tcount = (pe_end - frag_end) / 8;\n\t\tradeon_vm_set_pages(rdev, ib, frag_end, addr, count,\n\t\t\t\t    RADEON_GPU_PAGE_SIZE, flags);\n\t}\n}\n\n \nstatic int radeon_vm_update_ptes(struct radeon_device *rdev,\n\t\t\t\t struct radeon_vm *vm,\n\t\t\t\t struct radeon_ib *ib,\n\t\t\t\t uint64_t start, uint64_t end,\n\t\t\t\t uint64_t dst, uint32_t flags)\n{\n\tuint64_t mask = RADEON_VM_PTE_COUNT - 1;\n\tuint64_t last_pte = ~0, last_dst = ~0;\n\tunsigned count = 0;\n\tuint64_t addr;\n\n\t \n\tfor (addr = start; addr < end; ) {\n\t\tuint64_t pt_idx = addr >> radeon_vm_block_size;\n\t\tstruct radeon_bo *pt = vm->page_tables[pt_idx].bo;\n\t\tunsigned nptes;\n\t\tuint64_t pte;\n\t\tint r;\n\n\t\tradeon_sync_resv(rdev, &ib->sync, pt->tbo.base.resv, true);\n\t\tr = dma_resv_reserve_fences(pt->tbo.base.resv, 1);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif ((addr & ~mask) == (end & ~mask))\n\t\t\tnptes = end - addr;\n\t\telse\n\t\t\tnptes = RADEON_VM_PTE_COUNT - (addr & mask);\n\n\t\tpte = radeon_bo_gpu_offset(pt);\n\t\tpte += (addr & mask) * 8;\n\n\t\tif ((last_pte + 8 * count) != pte) {\n\n\t\t\tif (count) {\n\t\t\t\tradeon_vm_frag_ptes(rdev, ib, last_pte,\n\t\t\t\t\t\t    last_pte + 8 * count,\n\t\t\t\t\t\t    last_dst, flags);\n\t\t\t}\n\n\t\t\tcount = nptes;\n\t\t\tlast_pte = pte;\n\t\t\tlast_dst = dst;\n\t\t} else {\n\t\t\tcount += nptes;\n\t\t}\n\n\t\taddr += nptes;\n\t\tdst += nptes * RADEON_GPU_PAGE_SIZE;\n\t}\n\n\tif (count) {\n\t\tradeon_vm_frag_ptes(rdev, ib, last_pte,\n\t\t\t\t    last_pte + 8 * count,\n\t\t\t\t    last_dst, flags);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void radeon_vm_fence_pts(struct radeon_vm *vm,\n\t\t\t\tuint64_t start, uint64_t end,\n\t\t\t\tstruct radeon_fence *fence)\n{\n\tunsigned i;\n\n\tstart >>= radeon_vm_block_size;\n\tend = (end - 1) >> radeon_vm_block_size;\n\n\tfor (i = start; i <= end; ++i)\n\t\tradeon_bo_fence(vm->page_tables[i].bo, fence, true);\n}\n\n \nint radeon_vm_bo_update(struct radeon_device *rdev,\n\t\t\tstruct radeon_bo_va *bo_va,\n\t\t\tstruct ttm_resource *mem)\n{\n\tstruct radeon_vm *vm = bo_va->vm;\n\tstruct radeon_ib ib;\n\tunsigned nptes, ncmds, ndw;\n\tuint64_t addr;\n\tuint32_t flags;\n\tint r;\n\n\tif (!bo_va->it.start) {\n\t\tdev_err(rdev->dev, \"bo %p don't has a mapping in vm %p\\n\",\n\t\t\tbo_va->bo, vm);\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock(&vm->status_lock);\n\tif (mem) {\n\t\tif (list_empty(&bo_va->vm_status)) {\n\t\t\tspin_unlock(&vm->status_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tlist_del_init(&bo_va->vm_status);\n\t} else {\n\t\tlist_del(&bo_va->vm_status);\n\t\tlist_add(&bo_va->vm_status, &vm->cleared);\n\t}\n\tspin_unlock(&vm->status_lock);\n\n\tbo_va->flags &= ~RADEON_VM_PAGE_VALID;\n\tbo_va->flags &= ~RADEON_VM_PAGE_SYSTEM;\n\tbo_va->flags &= ~RADEON_VM_PAGE_SNOOPED;\n\tif (bo_va->bo && radeon_ttm_tt_is_readonly(rdev, bo_va->bo->tbo.ttm))\n\t\tbo_va->flags &= ~RADEON_VM_PAGE_WRITEABLE;\n\n\tif (mem) {\n\t\taddr = (u64)mem->start << PAGE_SHIFT;\n\t\tif (mem->mem_type != TTM_PL_SYSTEM)\n\t\t\tbo_va->flags |= RADEON_VM_PAGE_VALID;\n\n\t\tif (mem->mem_type == TTM_PL_TT) {\n\t\t\tbo_va->flags |= RADEON_VM_PAGE_SYSTEM;\n\t\t\tif (!(bo_va->bo->flags & (RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC)))\n\t\t\t\tbo_va->flags |= RADEON_VM_PAGE_SNOOPED;\n\n\t\t} else {\n\t\t\taddr += rdev->vm_manager.vram_base_offset;\n\t\t}\n\t} else {\n\t\taddr = 0;\n\t}\n\n\ttrace_radeon_vm_bo_update(bo_va);\n\n\tnptes = bo_va->it.last - bo_va->it.start + 1;\n\n\t \n\tncmds = (nptes >> min(radeon_vm_block_size, 11)) + 1;\n\n\t \n\tndw = 64;\n\n\tflags = radeon_vm_page_flags(bo_va->flags);\n\tif ((flags & R600_PTE_GART_MASK) == R600_PTE_GART_MASK) {\n\t\t \n\t\tndw += ncmds * 7;\n\n\t} else if (flags & R600_PTE_SYSTEM) {\n\t\t \n\t\tndw += ncmds * 4;\n\n\t\t \n\t\tndw += nptes * 2;\n\n\t} else {\n\t\t \n\t\tndw += ncmds * 10;\n\n\t\t \n\t\tndw += 2 * 10;\n\t}\n\n\t \n\tif (ndw > 0xfffff)\n\t\treturn -ENOMEM;\n\n\tr = radeon_ib_get(rdev, R600_RING_TYPE_DMA_INDEX, &ib, NULL, ndw * 4);\n\tif (r)\n\t\treturn r;\n\tib.length_dw = 0;\n\n\tif (!(bo_va->flags & RADEON_VM_PAGE_VALID)) {\n\t\tunsigned i;\n\n\t\tfor (i = 0; i < RADEON_NUM_RINGS; ++i)\n\t\t\tradeon_sync_fence(&ib.sync, vm->ids[i].last_id_use);\n\t}\n\n\tr = radeon_vm_update_ptes(rdev, vm, &ib, bo_va->it.start,\n\t\t\t\t  bo_va->it.last + 1, addr,\n\t\t\t\t  radeon_vm_page_flags(bo_va->flags));\n\tif (r) {\n\t\tradeon_ib_free(rdev, &ib);\n\t\treturn r;\n\t}\n\n\tradeon_asic_vm_pad_ib(rdev, &ib);\n\tWARN_ON(ib.length_dw > ndw);\n\n\tr = radeon_ib_schedule(rdev, &ib, NULL, false);\n\tif (r) {\n\t\tradeon_ib_free(rdev, &ib);\n\t\treturn r;\n\t}\n\tib.fence->is_vm_update = true;\n\tradeon_vm_fence_pts(vm, bo_va->it.start, bo_va->it.last + 1, ib.fence);\n\tradeon_fence_unref(&bo_va->last_pt_update);\n\tbo_va->last_pt_update = radeon_fence_ref(ib.fence);\n\tradeon_ib_free(rdev, &ib);\n\n\treturn 0;\n}\n\n \nint radeon_vm_clear_freed(struct radeon_device *rdev,\n\t\t\t  struct radeon_vm *vm)\n{\n\tstruct radeon_bo_va *bo_va;\n\tint r = 0;\n\n\tspin_lock(&vm->status_lock);\n\twhile (!list_empty(&vm->freed)) {\n\t\tbo_va = list_first_entry(&vm->freed,\n\t\t\tstruct radeon_bo_va, vm_status);\n\t\tspin_unlock(&vm->status_lock);\n\n\t\tr = radeon_vm_bo_update(rdev, bo_va, NULL);\n\t\tradeon_bo_unref(&bo_va->bo);\n\t\tradeon_fence_unref(&bo_va->last_pt_update);\n\t\tspin_lock(&vm->status_lock);\n\t\tlist_del(&bo_va->vm_status);\n\t\tkfree(bo_va);\n\t\tif (r)\n\t\t\tbreak;\n\n\t}\n\tspin_unlock(&vm->status_lock);\n\treturn r;\n\n}\n\n \nint radeon_vm_clear_invalids(struct radeon_device *rdev,\n\t\t\t     struct radeon_vm *vm)\n{\n\tstruct radeon_bo_va *bo_va;\n\tint r;\n\n\tspin_lock(&vm->status_lock);\n\twhile (!list_empty(&vm->invalidated)) {\n\t\tbo_va = list_first_entry(&vm->invalidated,\n\t\t\tstruct radeon_bo_va, vm_status);\n\t\tspin_unlock(&vm->status_lock);\n\n\t\tr = radeon_vm_bo_update(rdev, bo_va, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tspin_lock(&vm->status_lock);\n\t}\n\tspin_unlock(&vm->status_lock);\n\n\treturn 0;\n}\n\n \nvoid radeon_vm_bo_rmv(struct radeon_device *rdev,\n\t\t      struct radeon_bo_va *bo_va)\n{\n\tstruct radeon_vm *vm = bo_va->vm;\n\n\tlist_del(&bo_va->bo_list);\n\n\tmutex_lock(&vm->mutex);\n\tif (bo_va->it.start || bo_va->it.last)\n\t\tinterval_tree_remove(&bo_va->it, &vm->va);\n\n\tspin_lock(&vm->status_lock);\n\tlist_del(&bo_va->vm_status);\n\tif (bo_va->it.start || bo_va->it.last) {\n\t\tbo_va->bo = radeon_bo_ref(bo_va->bo);\n\t\tlist_add(&bo_va->vm_status, &vm->freed);\n\t} else {\n\t\tradeon_fence_unref(&bo_va->last_pt_update);\n\t\tkfree(bo_va);\n\t}\n\tspin_unlock(&vm->status_lock);\n\n\tmutex_unlock(&vm->mutex);\n}\n\n \nvoid radeon_vm_bo_invalidate(struct radeon_device *rdev,\n\t\t\t     struct radeon_bo *bo)\n{\n\tstruct radeon_bo_va *bo_va;\n\n\tlist_for_each_entry(bo_va, &bo->va, bo_list) {\n\t\tspin_lock(&bo_va->vm->status_lock);\n\t\tif (list_empty(&bo_va->vm_status) &&\n\t\t    (bo_va->it.start || bo_va->it.last))\n\t\t\tlist_add(&bo_va->vm_status, &bo_va->vm->invalidated);\n\t\tspin_unlock(&bo_va->vm->status_lock);\n\t}\n}\n\n \nint radeon_vm_init(struct radeon_device *rdev, struct radeon_vm *vm)\n{\n\tconst unsigned align = min(RADEON_VM_PTB_ALIGN_SIZE,\n\t\tRADEON_VM_PTE_COUNT * 8);\n\tunsigned pd_size, pd_entries, pts_size;\n\tint i, r;\n\n\tvm->ib_bo_va = NULL;\n\tfor (i = 0; i < RADEON_NUM_RINGS; ++i) {\n\t\tvm->ids[i].id = 0;\n\t\tvm->ids[i].flushed_updates = NULL;\n\t\tvm->ids[i].last_id_use = NULL;\n\t}\n\tmutex_init(&vm->mutex);\n\tvm->va = RB_ROOT_CACHED;\n\tspin_lock_init(&vm->status_lock);\n\tINIT_LIST_HEAD(&vm->invalidated);\n\tINIT_LIST_HEAD(&vm->freed);\n\tINIT_LIST_HEAD(&vm->cleared);\n\n\tpd_size = radeon_vm_directory_size(rdev);\n\tpd_entries = radeon_vm_num_pdes(rdev);\n\n\t \n\tpts_size = pd_entries * sizeof(struct radeon_vm_pt);\n\tvm->page_tables = kzalloc(pts_size, GFP_KERNEL);\n\tif (vm->page_tables == NULL) {\n\t\tDRM_ERROR(\"Cannot allocate memory for page table array\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tr = radeon_bo_create(rdev, pd_size, align, true,\n\t\t\t     RADEON_GEM_DOMAIN_VRAM, 0, NULL,\n\t\t\t     NULL, &vm->page_directory);\n\tif (r) {\n\t\tkfree(vm->page_tables);\n\t\tvm->page_tables = NULL;\n\t\treturn r;\n\t}\n\tr = radeon_vm_clear_bo(rdev, vm->page_directory);\n\tif (r) {\n\t\tradeon_bo_unref(&vm->page_directory);\n\t\tvm->page_directory = NULL;\n\t\tkfree(vm->page_tables);\n\t\tvm->page_tables = NULL;\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n \nvoid radeon_vm_fini(struct radeon_device *rdev, struct radeon_vm *vm)\n{\n\tstruct radeon_bo_va *bo_va, *tmp;\n\tint i, r;\n\n\tif (!RB_EMPTY_ROOT(&vm->va.rb_root))\n\t\tdev_err(rdev->dev, \"still active bo inside vm\\n\");\n\n\trbtree_postorder_for_each_entry_safe(bo_va, tmp,\n\t\t\t\t\t     &vm->va.rb_root, it.rb) {\n\t\tinterval_tree_remove(&bo_va->it, &vm->va);\n\t\tr = radeon_bo_reserve(bo_va->bo, false);\n\t\tif (!r) {\n\t\t\tlist_del_init(&bo_va->bo_list);\n\t\t\tradeon_bo_unreserve(bo_va->bo);\n\t\t\tradeon_fence_unref(&bo_va->last_pt_update);\n\t\t\tkfree(bo_va);\n\t\t}\n\t}\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->freed, vm_status) {\n\t\tradeon_bo_unref(&bo_va->bo);\n\t\tradeon_fence_unref(&bo_va->last_pt_update);\n\t\tkfree(bo_va);\n\t}\n\n\tfor (i = 0; i < radeon_vm_num_pdes(rdev); i++)\n\t\tradeon_bo_unref(&vm->page_tables[i].bo);\n\tkfree(vm->page_tables);\n\n\tradeon_bo_unref(&vm->page_directory);\n\n\tfor (i = 0; i < RADEON_NUM_RINGS; ++i) {\n\t\tradeon_fence_unref(&vm->ids[i].flushed_updates);\n\t\tradeon_fence_unref(&vm->ids[i].last_id_use);\n\t}\n\n\tmutex_destroy(&vm->mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}