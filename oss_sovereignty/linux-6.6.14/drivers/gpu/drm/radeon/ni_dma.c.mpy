{
  "module_name": "ni_dma.c",
  "hash_id": "5b379e5307872ae75529a426d50835f71095168c9bcc39ad5e06fe105ff36670",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/ni_dma.c",
  "human_readable_source": " \n\n#include \"radeon.h\"\n#include \"radeon_asic.h\"\n#include \"radeon_trace.h\"\n#include \"ni.h\"\n#include \"nid.h\"\n\n \n\n \nuint32_t cayman_dma_get_rptr(struct radeon_device *rdev,\n\t\t\t     struct radeon_ring *ring)\n{\n\tu32 rptr, reg;\n\n\tif (rdev->wb.enabled) {\n\t\trptr = rdev->wb.wb[ring->rptr_offs/4];\n\t} else {\n\t\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\t\treg = DMA_RB_RPTR + DMA0_REGISTER_OFFSET;\n\t\telse\n\t\t\treg = DMA_RB_RPTR + DMA1_REGISTER_OFFSET;\n\n\t\trptr = RREG32(reg);\n\t}\n\n\treturn (rptr & 0x3fffc) >> 2;\n}\n\n \nuint32_t cayman_dma_get_wptr(struct radeon_device *rdev,\n\t\t\t   struct radeon_ring *ring)\n{\n\tu32 reg;\n\n\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\treg = DMA_RB_WPTR + DMA0_REGISTER_OFFSET;\n\telse\n\t\treg = DMA_RB_WPTR + DMA1_REGISTER_OFFSET;\n\n\treturn (RREG32(reg) & 0x3fffc) >> 2;\n}\n\n \nvoid cayman_dma_set_wptr(struct radeon_device *rdev,\n\t\t\t struct radeon_ring *ring)\n{\n\tu32 reg;\n\n\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\treg = DMA_RB_WPTR + DMA0_REGISTER_OFFSET;\n\telse\n\t\treg = DMA_RB_WPTR + DMA1_REGISTER_OFFSET;\n\n\tWREG32(reg, (ring->wptr << 2) & 0x3fffc);\n}\n\n \nvoid cayman_dma_ring_ib_execute(struct radeon_device *rdev,\n\t\t\t\tstruct radeon_ib *ib)\n{\n\tstruct radeon_ring *ring = &rdev->ring[ib->ring];\n\tunsigned vm_id = ib->vm ? ib->vm->ids[ib->ring].id : 0;\n\n\tif (rdev->wb.enabled) {\n\t\tu32 next_rptr = ring->wptr + 4;\n\t\twhile ((next_rptr & 7) != 5)\n\t\t\tnext_rptr++;\n\t\tnext_rptr += 3;\n\t\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 1));\n\t\tradeon_ring_write(ring, ring->next_rptr_gpu_addr & 0xfffffffc);\n\t\tradeon_ring_write(ring, upper_32_bits(ring->next_rptr_gpu_addr) & 0xff);\n\t\tradeon_ring_write(ring, next_rptr);\n\t}\n\n\t \n\twhile ((ring->wptr & 7) != 5)\n\t\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0));\n\tradeon_ring_write(ring, DMA_IB_PACKET(DMA_PACKET_INDIRECT_BUFFER, vm_id, 0));\n\tradeon_ring_write(ring, (ib->gpu_addr & 0xFFFFFFE0));\n\tradeon_ring_write(ring, (ib->length_dw << 12) | (upper_32_bits(ib->gpu_addr) & 0xFF));\n\n}\n\n \nvoid cayman_dma_stop(struct radeon_device *rdev)\n{\n\tu32 rb_cntl;\n\n\tif ((rdev->asic->copy.copy_ring_index == R600_RING_TYPE_DMA_INDEX) ||\n\t    (rdev->asic->copy.copy_ring_index == CAYMAN_RING_TYPE_DMA1_INDEX))\n\t\tradeon_ttm_set_active_vram_size(rdev, rdev->mc.visible_vram_size);\n\n\t \n\trb_cntl = RREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET);\n\trb_cntl &= ~DMA_RB_ENABLE;\n\tWREG32(DMA_RB_CNTL + DMA0_REGISTER_OFFSET, rb_cntl);\n\n\t \n\trb_cntl = RREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET);\n\trb_cntl &= ~DMA_RB_ENABLE;\n\tWREG32(DMA_RB_CNTL + DMA1_REGISTER_OFFSET, rb_cntl);\n\n\trdev->ring[R600_RING_TYPE_DMA_INDEX].ready = false;\n\trdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX].ready = false;\n}\n\n \nint cayman_dma_resume(struct radeon_device *rdev)\n{\n\tstruct radeon_ring *ring;\n\tu32 rb_cntl, dma_cntl, ib_cntl;\n\tu32 rb_bufsz;\n\tu32 reg_offset, wb_offset;\n\tint i, r;\n\n\tfor (i = 0; i < 2; i++) {\n\t\tif (i == 0) {\n\t\t\tring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\n\t\t\treg_offset = DMA0_REGISTER_OFFSET;\n\t\t\twb_offset = R600_WB_DMA_RPTR_OFFSET;\n\t\t} else {\n\t\t\tring = &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX];\n\t\t\treg_offset = DMA1_REGISTER_OFFSET;\n\t\t\twb_offset = CAYMAN_WB_DMA1_RPTR_OFFSET;\n\t\t}\n\n\t\tWREG32(DMA_SEM_INCOMPLETE_TIMER_CNTL + reg_offset, 0);\n\t\tWREG32(DMA_SEM_WAIT_FAIL_TIMER_CNTL + reg_offset, 0);\n\n\t\t \n\t\trb_bufsz = order_base_2(ring->ring_size / 4);\n\t\trb_cntl = rb_bufsz << 1;\n#ifdef __BIG_ENDIAN\n\t\trb_cntl |= DMA_RB_SWAP_ENABLE | DMA_RPTR_WRITEBACK_SWAP_ENABLE;\n#endif\n\t\tWREG32(DMA_RB_CNTL + reg_offset, rb_cntl);\n\n\t\t \n\t\tWREG32(DMA_RB_RPTR + reg_offset, 0);\n\t\tWREG32(DMA_RB_WPTR + reg_offset, 0);\n\n\t\t \n\t\tWREG32(DMA_RB_RPTR_ADDR_HI + reg_offset,\n\t\t       upper_32_bits(rdev->wb.gpu_addr + wb_offset) & 0xFF);\n\t\tWREG32(DMA_RB_RPTR_ADDR_LO + reg_offset,\n\t\t       ((rdev->wb.gpu_addr + wb_offset) & 0xFFFFFFFC));\n\n\t\tif (rdev->wb.enabled)\n\t\t\trb_cntl |= DMA_RPTR_WRITEBACK_ENABLE;\n\n\t\tWREG32(DMA_RB_BASE + reg_offset, ring->gpu_addr >> 8);\n\n\t\t \n\t\tib_cntl = DMA_IB_ENABLE | CMD_VMID_FORCE;\n#ifdef __BIG_ENDIAN\n\t\tib_cntl |= DMA_IB_SWAP_ENABLE;\n#endif\n\t\tWREG32(DMA_IB_CNTL + reg_offset, ib_cntl);\n\n\t\tdma_cntl = RREG32(DMA_CNTL + reg_offset);\n\t\tdma_cntl &= ~CTXEMPTY_INT_ENABLE;\n\t\tWREG32(DMA_CNTL + reg_offset, dma_cntl);\n\n\t\tring->wptr = 0;\n\t\tWREG32(DMA_RB_WPTR + reg_offset, ring->wptr << 2);\n\n\t\tWREG32(DMA_RB_CNTL + reg_offset, rb_cntl | DMA_RB_ENABLE);\n\n\t\tring->ready = true;\n\n\t\tr = radeon_ring_test(rdev, ring->idx, ring);\n\t\tif (r) {\n\t\t\tring->ready = false;\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif ((rdev->asic->copy.copy_ring_index == R600_RING_TYPE_DMA_INDEX) ||\n\t    (rdev->asic->copy.copy_ring_index == CAYMAN_RING_TYPE_DMA1_INDEX))\n\t\tradeon_ttm_set_active_vram_size(rdev, rdev->mc.real_vram_size);\n\n\treturn 0;\n}\n\n \nvoid cayman_dma_fini(struct radeon_device *rdev)\n{\n\tcayman_dma_stop(rdev);\n\tradeon_ring_fini(rdev, &rdev->ring[R600_RING_TYPE_DMA_INDEX]);\n\tradeon_ring_fini(rdev, &rdev->ring[CAYMAN_RING_TYPE_DMA1_INDEX]);\n}\n\n \nbool cayman_dma_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\n{\n\tu32 reset_mask = cayman_gpu_check_soft_reset(rdev);\n\tu32 mask;\n\n\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\tmask = RADEON_RESET_DMA;\n\telse\n\t\tmask = RADEON_RESET_DMA1;\n\n\tif (!(reset_mask & mask)) {\n\t\tradeon_ring_lockup_update(rdev, ring);\n\t\treturn false;\n\t}\n\treturn radeon_ring_test_lockup(rdev, ring);\n}\n\n \nvoid cayman_dma_vm_copy_pages(struct radeon_device *rdev,\n\t\t\t      struct radeon_ib *ib,\n\t\t\t      uint64_t pe, uint64_t src,\n\t\t\t      unsigned count)\n{\n\tunsigned ndw;\n\n\twhile (count) {\n\t\tndw = count * 2;\n\t\tif (ndw > 0xFFFFE)\n\t\t\tndw = 0xFFFFE;\n\n\t\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_COPY,\n\t\t\t\t\t\t      0, 0, ndw);\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(pe) & 0xff;\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(src) & 0xff;\n\n\t\tpe += ndw * 4;\n\t\tsrc += ndw * 4;\n\t\tcount -= ndw / 2;\n\t}\n}\n\n \nvoid cayman_dma_vm_write_pages(struct radeon_device *rdev,\n\t\t\t       struct radeon_ib *ib,\n\t\t\t       uint64_t pe,\n\t\t\t       uint64_t addr, unsigned count,\n\t\t\t       uint32_t incr, uint32_t flags)\n{\n\tuint64_t value;\n\tunsigned ndw;\n\n\twhile (count) {\n\t\tndw = count * 2;\n\t\tif (ndw > 0xFFFFE)\n\t\t\tndw = 0xFFFFE;\n\n\t\t \n\t\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_WRITE,\n\t\t\t\t\t\t      0, 0, ndw);\n\t\tib->ptr[ib->length_dw++] = pe;\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(pe) & 0xff;\n\t\tfor (; ndw > 0; ndw -= 2, --count, pe += 8) {\n\t\t\tif (flags & R600_PTE_SYSTEM) {\n\t\t\t\tvalue = radeon_vm_map_gart(rdev, addr);\n\t\t\t} else if (flags & R600_PTE_VALID) {\n\t\t\t\tvalue = addr;\n\t\t\t} else {\n\t\t\t\tvalue = 0;\n\t\t\t}\n\t\t\taddr += incr;\n\t\t\tvalue |= flags;\n\t\t\tib->ptr[ib->length_dw++] = value;\n\t\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\t}\n\t}\n}\n\n \nvoid cayman_dma_vm_set_pages(struct radeon_device *rdev,\n\t\t\t     struct radeon_ib *ib,\n\t\t\t     uint64_t pe,\n\t\t\t     uint64_t addr, unsigned count,\n\t\t\t     uint32_t incr, uint32_t flags)\n{\n\tuint64_t value;\n\tunsigned ndw;\n\n\twhile (count) {\n\t\tndw = count * 2;\n\t\tif (ndw > 0xFFFFE)\n\t\t\tndw = 0xFFFFE;\n\n\t\tif (flags & R600_PTE_VALID)\n\t\t\tvalue = addr;\n\t\telse\n\t\t\tvalue = 0;\n\n\t\t \n\t\tib->ptr[ib->length_dw++] = DMA_PTE_PDE_PACKET(ndw);\n\t\tib->ptr[ib->length_dw++] = pe;  \n\t\tib->ptr[ib->length_dw++] = upper_32_bits(pe) & 0xff;\n\t\tib->ptr[ib->length_dw++] = flags;  \n\t\tib->ptr[ib->length_dw++] = 0;\n\t\tib->ptr[ib->length_dw++] = value;  \n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = incr;  \n\t\tib->ptr[ib->length_dw++] = 0;\n\n\t\tpe += ndw * 4;\n\t\taddr += (ndw / 2) * incr;\n\t\tcount -= ndw / 2;\n\t}\n}\n\n \nvoid cayman_dma_vm_pad_ib(struct radeon_ib *ib)\n{\n\twhile (ib->length_dw & 0x7)\n\t\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0);\n}\n\nvoid cayman_dma_vm_flush(struct radeon_device *rdev, struct radeon_ring *ring,\n\t\t\t unsigned vm_id, uint64_t pd_addr)\n{\n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_SRBM_WRITE, 0, 0, 0));\n\tradeon_ring_write(ring, (0xf << 16) | ((VM_CONTEXT0_PAGE_TABLE_BASE_ADDR + (vm_id << 2)) >> 2));\n\tradeon_ring_write(ring, pd_addr >> 12);\n\n\t \n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_SRBM_WRITE, 0, 0, 0));\n\tradeon_ring_write(ring, (0xf << 16) | (HDP_MEM_COHERENCY_FLUSH_CNTL >> 2));\n\tradeon_ring_write(ring, 1);\n\n\t \n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_SRBM_WRITE, 0, 0, 0));\n\tradeon_ring_write(ring, (0xf << 16) | (VM_INVALIDATE_REQUEST >> 2));\n\tradeon_ring_write(ring, 1 << vm_id);\n\n\t \n\tradeon_ring_write(ring, DMA_SRBM_READ_PACKET);\n\tradeon_ring_write(ring, (0xff << 20) | (VM_INVALIDATE_REQUEST >> 2));\n\tradeon_ring_write(ring, 0);  \n\tradeon_ring_write(ring, 0);  \n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}