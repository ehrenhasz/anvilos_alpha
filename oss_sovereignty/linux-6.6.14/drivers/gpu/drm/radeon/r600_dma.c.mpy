{
  "module_name": "r600_dma.c",
  "hash_id": "340017980008f122721ded3024a31913b9d8efcd862114ab485b45b3d43c954b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/r600_dma.c",
  "human_readable_source": " \n\n#include \"radeon.h\"\n#include \"radeon_asic.h\"\n#include \"r600.h\"\n#include \"r600d.h\"\n\n \n\n \nuint32_t r600_dma_get_rptr(struct radeon_device *rdev,\n\t\t\t   struct radeon_ring *ring)\n{\n\tu32 rptr;\n\n\tif (rdev->wb.enabled)\n\t\trptr = rdev->wb.wb[ring->rptr_offs/4];\n\telse\n\t\trptr = RREG32(DMA_RB_RPTR);\n\n\treturn (rptr & 0x3fffc) >> 2;\n}\n\n \nuint32_t r600_dma_get_wptr(struct radeon_device *rdev,\n\t\t\t   struct radeon_ring *ring)\n{\n\treturn (RREG32(DMA_RB_WPTR) & 0x3fffc) >> 2;\n}\n\n \nvoid r600_dma_set_wptr(struct radeon_device *rdev,\n\t\t       struct radeon_ring *ring)\n{\n\tWREG32(DMA_RB_WPTR, (ring->wptr << 2) & 0x3fffc);\n}\n\n \nvoid r600_dma_stop(struct radeon_device *rdev)\n{\n\tu32 rb_cntl = RREG32(DMA_RB_CNTL);\n\n\tif (rdev->asic->copy.copy_ring_index == R600_RING_TYPE_DMA_INDEX)\n\t\tradeon_ttm_set_active_vram_size(rdev, rdev->mc.visible_vram_size);\n\n\trb_cntl &= ~DMA_RB_ENABLE;\n\tWREG32(DMA_RB_CNTL, rb_cntl);\n\n\trdev->ring[R600_RING_TYPE_DMA_INDEX].ready = false;\n}\n\n \nint r600_dma_resume(struct radeon_device *rdev)\n{\n\tstruct radeon_ring *ring = &rdev->ring[R600_RING_TYPE_DMA_INDEX];\n\tu32 rb_cntl, dma_cntl, ib_cntl;\n\tu32 rb_bufsz;\n\tint r;\n\n\tWREG32(DMA_SEM_INCOMPLETE_TIMER_CNTL, 0);\n\tWREG32(DMA_SEM_WAIT_FAIL_TIMER_CNTL, 0);\n\n\t \n\trb_bufsz = order_base_2(ring->ring_size / 4);\n\trb_cntl = rb_bufsz << 1;\n#ifdef __BIG_ENDIAN\n\trb_cntl |= DMA_RB_SWAP_ENABLE | DMA_RPTR_WRITEBACK_SWAP_ENABLE;\n#endif\n\tWREG32(DMA_RB_CNTL, rb_cntl);\n\n\t \n\tWREG32(DMA_RB_RPTR, 0);\n\tWREG32(DMA_RB_WPTR, 0);\n\n\t \n\tWREG32(DMA_RB_RPTR_ADDR_HI,\n\t       upper_32_bits(rdev->wb.gpu_addr + R600_WB_DMA_RPTR_OFFSET) & 0xFF);\n\tWREG32(DMA_RB_RPTR_ADDR_LO,\n\t       ((rdev->wb.gpu_addr + R600_WB_DMA_RPTR_OFFSET) & 0xFFFFFFFC));\n\n\tif (rdev->wb.enabled)\n\t\trb_cntl |= DMA_RPTR_WRITEBACK_ENABLE;\n\n\tWREG32(DMA_RB_BASE, ring->gpu_addr >> 8);\n\n\t \n\tib_cntl = DMA_IB_ENABLE;\n#ifdef __BIG_ENDIAN\n\tib_cntl |= DMA_IB_SWAP_ENABLE;\n#endif\n\tWREG32(DMA_IB_CNTL, ib_cntl);\n\n\tdma_cntl = RREG32(DMA_CNTL);\n\tdma_cntl &= ~CTXEMPTY_INT_ENABLE;\n\tWREG32(DMA_CNTL, dma_cntl);\n\n\tif (rdev->family >= CHIP_RV770)\n\t\tWREG32(DMA_MODE, 1);\n\n\tring->wptr = 0;\n\tWREG32(DMA_RB_WPTR, ring->wptr << 2);\n\n\tWREG32(DMA_RB_CNTL, rb_cntl | DMA_RB_ENABLE);\n\n\tring->ready = true;\n\n\tr = radeon_ring_test(rdev, R600_RING_TYPE_DMA_INDEX, ring);\n\tif (r) {\n\t\tring->ready = false;\n\t\treturn r;\n\t}\n\n\tif (rdev->asic->copy.copy_ring_index == R600_RING_TYPE_DMA_INDEX)\n\t\tradeon_ttm_set_active_vram_size(rdev, rdev->mc.real_vram_size);\n\n\treturn 0;\n}\n\n \nvoid r600_dma_fini(struct radeon_device *rdev)\n{\n\tr600_dma_stop(rdev);\n\tradeon_ring_fini(rdev, &rdev->ring[R600_RING_TYPE_DMA_INDEX]);\n}\n\n \nbool r600_dma_is_lockup(struct radeon_device *rdev, struct radeon_ring *ring)\n{\n\tu32 reset_mask = r600_gpu_check_soft_reset(rdev);\n\n\tif (!(reset_mask & RADEON_RESET_DMA)) {\n\t\tradeon_ring_lockup_update(rdev, ring);\n\t\treturn false;\n\t}\n\treturn radeon_ring_test_lockup(rdev, ring);\n}\n\n\n \nint r600_dma_ring_test(struct radeon_device *rdev,\n\t\t       struct radeon_ring *ring)\n{\n\tunsigned i;\n\tint r;\n\tunsigned index;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\tindex = R600_WB_DMA_RING_TEST_OFFSET;\n\telse\n\t\tindex = CAYMAN_WB_DMA1_RING_TEST_OFFSET;\n\n\tgpu_addr = rdev->wb.gpu_addr + index;\n\n\ttmp = 0xCAFEDEAD;\n\trdev->wb.wb[index/4] = cpu_to_le32(tmp);\n\n\tr = radeon_ring_lock(rdev, ring, 4);\n\tif (r) {\n\t\tDRM_ERROR(\"radeon: dma failed to lock ring %d (%d).\\n\", ring->idx, r);\n\t\treturn r;\n\t}\n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 1));\n\tradeon_ring_write(ring, lower_32_bits(gpu_addr));\n\tradeon_ring_write(ring, upper_32_bits(gpu_addr) & 0xff);\n\tradeon_ring_write(ring, 0xDEADBEEF);\n\tradeon_ring_unlock_commit(rdev, ring, false);\n\n\tfor (i = 0; i < rdev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(rdev->wb.wb[index/4]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i < rdev->usec_timeout) {\n\t\tDRM_INFO(\"ring test on %d succeeded in %d usecs\\n\", ring->idx, i);\n\t} else {\n\t\tDRM_ERROR(\"radeon: ring %d test failed (0x%08X)\\n\",\n\t\t\t  ring->idx, tmp);\n\t\tr = -EINVAL;\n\t}\n\treturn r;\n}\n\n \nvoid r600_dma_fence_ring_emit(struct radeon_device *rdev,\n\t\t\t      struct radeon_fence *fence)\n{\n\tstruct radeon_ring *ring = &rdev->ring[fence->ring];\n\tu64 addr = rdev->fence_drv[fence->ring].gpu_addr;\n\n\t \n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_FENCE, 0, 0, 0));\n\tradeon_ring_write(ring, addr & 0xfffffffc);\n\tradeon_ring_write(ring, (upper_32_bits(addr) & 0xff));\n\tradeon_ring_write(ring, lower_32_bits(fence->seq));\n\t \n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_TRAP, 0, 0, 0));\n}\n\n \nbool r600_dma_semaphore_ring_emit(struct radeon_device *rdev,\n\t\t\t\t  struct radeon_ring *ring,\n\t\t\t\t  struct radeon_semaphore *semaphore,\n\t\t\t\t  bool emit_wait)\n{\n\tu64 addr = semaphore->gpu_addr;\n\tu32 s = emit_wait ? 0 : 1;\n\n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_SEMAPHORE, 0, s, 0));\n\tradeon_ring_write(ring, addr & 0xfffffffc);\n\tradeon_ring_write(ring, upper_32_bits(addr) & 0xff);\n\n\treturn true;\n}\n\n \nint r600_dma_ib_test(struct radeon_device *rdev, struct radeon_ring *ring)\n{\n\tstruct radeon_ib ib;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\n\tif (ring->idx == R600_RING_TYPE_DMA_INDEX)\n\t\tindex = R600_WB_DMA_RING_TEST_OFFSET;\n\telse\n\t\tindex = CAYMAN_WB_DMA1_RING_TEST_OFFSET;\n\n\tgpu_addr = rdev->wb.gpu_addr + index;\n\n\tr = radeon_ib_get(rdev, ring->idx, &ib, NULL, 256);\n\tif (r) {\n\t\tDRM_ERROR(\"radeon: failed to get ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\tib.ptr[0] = DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 1);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr) & 0xff;\n\tib.ptr[3] = 0xDEADBEEF;\n\tib.length_dw = 4;\n\n\tr = radeon_ib_schedule(rdev, &ib, NULL, false);\n\tif (r) {\n\t\tradeon_ib_free(rdev, &ib);\n\t\tDRM_ERROR(\"radeon: failed to schedule ib (%d).\\n\", r);\n\t\treturn r;\n\t}\n\tr = radeon_fence_wait_timeout(ib.fence, false, usecs_to_jiffies(\n\t\tRADEON_USEC_IB_TEST_TIMEOUT));\n\tif (r < 0) {\n\t\tDRM_ERROR(\"radeon: fence wait failed (%d).\\n\", r);\n\t\treturn r;\n\t} else if (r == 0) {\n\t\tDRM_ERROR(\"radeon: fence wait timed out.\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\tr = 0;\n\tfor (i = 0; i < rdev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(rdev->wb.wb[index/4]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\tif (i < rdev->usec_timeout) {\n\t\tDRM_INFO(\"ib test on ring %d succeeded in %u usecs\\n\", ib.fence->ring, i);\n\t} else {\n\t\tDRM_ERROR(\"radeon: ib test failed (0x%08X)\\n\", tmp);\n\t\tr = -EINVAL;\n\t}\n\tradeon_ib_free(rdev, &ib);\n\treturn r;\n}\n\n \nvoid r600_dma_ring_ib_execute(struct radeon_device *rdev, struct radeon_ib *ib)\n{\n\tstruct radeon_ring *ring = &rdev->ring[ib->ring];\n\n\tif (rdev->wb.enabled) {\n\t\tu32 next_rptr = ring->wptr + 4;\n\t\twhile ((next_rptr & 7) != 5)\n\t\t\tnext_rptr++;\n\t\tnext_rptr += 3;\n\t\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 1));\n\t\tradeon_ring_write(ring, ring->next_rptr_gpu_addr & 0xfffffffc);\n\t\tradeon_ring_write(ring, upper_32_bits(ring->next_rptr_gpu_addr) & 0xff);\n\t\tradeon_ring_write(ring, next_rptr);\n\t}\n\n\t \n\twhile ((ring->wptr & 7) != 5)\n\t\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0));\n\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_INDIRECT_BUFFER, 0, 0, 0));\n\tradeon_ring_write(ring, (ib->gpu_addr & 0xFFFFFFE0));\n\tradeon_ring_write(ring, (ib->length_dw << 16) | (upper_32_bits(ib->gpu_addr) & 0xFF));\n\n}\n\n \nstruct radeon_fence *r600_copy_dma(struct radeon_device *rdev,\n\t\t\t\t   uint64_t src_offset, uint64_t dst_offset,\n\t\t\t\t   unsigned num_gpu_pages,\n\t\t\t\t   struct dma_resv *resv)\n{\n\tstruct radeon_fence *fence;\n\tstruct radeon_sync sync;\n\tint ring_index = rdev->asic->copy.dma_ring_index;\n\tstruct radeon_ring *ring = &rdev->ring[ring_index];\n\tu32 size_in_dw, cur_size_in_dw;\n\tint i, num_loops;\n\tint r = 0;\n\n\tradeon_sync_create(&sync);\n\n\tsize_in_dw = (num_gpu_pages << RADEON_GPU_PAGE_SHIFT) / 4;\n\tnum_loops = DIV_ROUND_UP(size_in_dw, 0xFFFE);\n\tr = radeon_ring_lock(rdev, ring, num_loops * 4 + 8);\n\tif (r) {\n\t\tDRM_ERROR(\"radeon: moving bo (%d).\\n\", r);\n\t\tradeon_sync_free(rdev, &sync, NULL);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tradeon_sync_resv(rdev, &sync, resv, false);\n\tradeon_sync_rings(rdev, &sync, ring->idx);\n\n\tfor (i = 0; i < num_loops; i++) {\n\t\tcur_size_in_dw = size_in_dw;\n\t\tif (cur_size_in_dw > 0xFFFE)\n\t\t\tcur_size_in_dw = 0xFFFE;\n\t\tsize_in_dw -= cur_size_in_dw;\n\t\tradeon_ring_write(ring, DMA_PACKET(DMA_PACKET_COPY, 0, 0, cur_size_in_dw));\n\t\tradeon_ring_write(ring, dst_offset & 0xfffffffc);\n\t\tradeon_ring_write(ring, src_offset & 0xfffffffc);\n\t\tradeon_ring_write(ring, (((upper_32_bits(dst_offset) & 0xff) << 16) |\n\t\t\t\t\t (upper_32_bits(src_offset) & 0xff)));\n\t\tsrc_offset += cur_size_in_dw * 4;\n\t\tdst_offset += cur_size_in_dw * 4;\n\t}\n\n\tr = radeon_fence_emit(rdev, &fence, ring->idx);\n\tif (r) {\n\t\tradeon_ring_unlock_undo(rdev, ring);\n\t\tradeon_sync_free(rdev, &sync, NULL);\n\t\treturn ERR_PTR(r);\n\t}\n\n\tradeon_ring_unlock_commit(rdev, ring, false);\n\tradeon_sync_free(rdev, &sync, fence);\n\n\treturn fence;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}