{
  "module_name": "radeon_uvd.c",
  "hash_id": "f45cb706369ad0ab86b4ac058da34c555d86e809868b5228f099776de7af9607",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/radeon_uvd.c",
  "human_readable_source": " \n \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n\n#include <drm/drm.h>\n\n#include \"radeon.h\"\n#include \"radeon_ucode.h\"\n#include \"r600d.h\"\n\n \n#define UVD_IDLE_TIMEOUT_MS\t1000\n\n \n#define FIRMWARE_R600\t\t\"radeon/R600_uvd.bin\"\n#define FIRMWARE_RS780\t\t\"radeon/RS780_uvd.bin\"\n#define FIRMWARE_RV770\t\t\"radeon/RV770_uvd.bin\"\n#define FIRMWARE_RV710\t\t\"radeon/RV710_uvd.bin\"\n#define FIRMWARE_CYPRESS\t\"radeon/CYPRESS_uvd.bin\"\n#define FIRMWARE_SUMO\t\t\"radeon/SUMO_uvd.bin\"\n#define FIRMWARE_TAHITI\t\t\"radeon/TAHITI_uvd.bin\"\n#define FIRMWARE_BONAIRE_LEGACY\t\"radeon/BONAIRE_uvd.bin\"\n#define FIRMWARE_BONAIRE\t\"radeon/bonaire_uvd.bin\"\n\nMODULE_FIRMWARE(FIRMWARE_R600);\nMODULE_FIRMWARE(FIRMWARE_RS780);\nMODULE_FIRMWARE(FIRMWARE_RV770);\nMODULE_FIRMWARE(FIRMWARE_RV710);\nMODULE_FIRMWARE(FIRMWARE_CYPRESS);\nMODULE_FIRMWARE(FIRMWARE_SUMO);\nMODULE_FIRMWARE(FIRMWARE_TAHITI);\nMODULE_FIRMWARE(FIRMWARE_BONAIRE_LEGACY);\nMODULE_FIRMWARE(FIRMWARE_BONAIRE);\n\nstatic void radeon_uvd_idle_work_handler(struct work_struct *work);\n\nint radeon_uvd_init(struct radeon_device *rdev)\n{\n\tunsigned long bo_size;\n\tconst char *fw_name = NULL, *legacy_fw_name = NULL;\n\tint i, r;\n\n\tINIT_DELAYED_WORK(&rdev->uvd.idle_work, radeon_uvd_idle_work_handler);\n\n\tswitch (rdev->family) {\n\tcase CHIP_RV610:\n\tcase CHIP_RV630:\n\tcase CHIP_RV670:\n\tcase CHIP_RV620:\n\tcase CHIP_RV635:\n\t\tlegacy_fw_name = FIRMWARE_R600;\n\t\tbreak;\n\n\tcase CHIP_RS780:\n\tcase CHIP_RS880:\n\t\tlegacy_fw_name = FIRMWARE_RS780;\n\t\tbreak;\n\n\tcase CHIP_RV770:\n\t\tlegacy_fw_name = FIRMWARE_RV770;\n\t\tbreak;\n\n\tcase CHIP_RV710:\n\tcase CHIP_RV730:\n\tcase CHIP_RV740:\n\t\tlegacy_fw_name = FIRMWARE_RV710;\n\t\tbreak;\n\n\tcase CHIP_CYPRESS:\n\tcase CHIP_HEMLOCK:\n\tcase CHIP_JUNIPER:\n\tcase CHIP_REDWOOD:\n\tcase CHIP_CEDAR:\n\t\tlegacy_fw_name = FIRMWARE_CYPRESS;\n\t\tbreak;\n\n\tcase CHIP_SUMO:\n\tcase CHIP_SUMO2:\n\tcase CHIP_PALM:\n\tcase CHIP_CAYMAN:\n\tcase CHIP_BARTS:\n\tcase CHIP_TURKS:\n\tcase CHIP_CAICOS:\n\t\tlegacy_fw_name = FIRMWARE_SUMO;\n\t\tbreak;\n\n\tcase CHIP_TAHITI:\n\tcase CHIP_VERDE:\n\tcase CHIP_PITCAIRN:\n\tcase CHIP_ARUBA:\n\tcase CHIP_OLAND:\n\t\tlegacy_fw_name = FIRMWARE_TAHITI;\n\t\tbreak;\n\n\tcase CHIP_BONAIRE:\n\tcase CHIP_KABINI:\n\tcase CHIP_KAVERI:\n\tcase CHIP_HAWAII:\n\tcase CHIP_MULLINS:\n\t\tlegacy_fw_name = FIRMWARE_BONAIRE_LEGACY;\n\t\tfw_name = FIRMWARE_BONAIRE;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\trdev->uvd.fw_header_present = false;\n\trdev->uvd.max_handles = RADEON_DEFAULT_UVD_HANDLES;\n\tif (fw_name) {\n\t\t \n\t\tr = request_firmware(&rdev->uvd_fw, fw_name, rdev->dev);\n\t\tif (r) {\n\t\t\tdev_err(rdev->dev, \"radeon_uvd: Can't load firmware \\\"%s\\\"\\n\",\n\t\t\t\tfw_name);\n\t\t} else {\n\t\t\tstruct common_firmware_header *hdr = (void *)rdev->uvd_fw->data;\n\t\t\tunsigned version_major, version_minor, family_id;\n\n\t\t\tr = radeon_ucode_validate(rdev->uvd_fw);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\trdev->uvd.fw_header_present = true;\n\n\t\t\tfamily_id = (__force u32)(hdr->ucode_version) & 0xff;\n\t\t\tversion_major = (le32_to_cpu((__force __le32)(hdr->ucode_version))\n\t\t\t\t\t\t\t >> 24) & 0xff;\n\t\t\tversion_minor = (le32_to_cpu((__force __le32)(hdr->ucode_version))\n\t\t\t\t\t\t\t >> 8) & 0xff;\n\t\t\tDRM_INFO(\"Found UVD firmware Version: %u.%u Family ID: %u\\n\",\n\t\t\t\t version_major, version_minor, family_id);\n\n\t\t\t \n\t\t\tif ((version_major >= 0x01) && (version_minor >= 0x37))\n\t\t\t\trdev->uvd.max_handles = RADEON_MAX_UVD_HANDLES;\n\t\t}\n\t}\n\n\t \n\tif (!fw_name || r) {\n\t\tr = request_firmware(&rdev->uvd_fw, legacy_fw_name, rdev->dev);\n\t\tif (r) {\n\t\t\tdev_err(rdev->dev, \"radeon_uvd: Can't load firmware \\\"%s\\\"\\n\",\n\t\t\t\tlegacy_fw_name);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tbo_size = RADEON_GPU_PAGE_ALIGN(rdev->uvd_fw->size + 8) +\n\t\t  RADEON_UVD_STACK_SIZE + RADEON_UVD_HEAP_SIZE +\n\t\t  RADEON_UVD_SESSION_SIZE * rdev->uvd.max_handles;\n\tr = radeon_bo_create(rdev, bo_size, PAGE_SIZE, true,\n\t\t\t     RADEON_GEM_DOMAIN_VRAM, 0, NULL,\n\t\t\t     NULL, &rdev->uvd.vcpu_bo);\n\tif (r) {\n\t\tdev_err(rdev->dev, \"(%d) failed to allocate UVD bo\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = radeon_bo_reserve(rdev->uvd.vcpu_bo, false);\n\tif (r) {\n\t\tradeon_bo_unref(&rdev->uvd.vcpu_bo);\n\t\tdev_err(rdev->dev, \"(%d) failed to reserve UVD bo\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = radeon_bo_pin(rdev->uvd.vcpu_bo, RADEON_GEM_DOMAIN_VRAM,\n\t\t\t  &rdev->uvd.gpu_addr);\n\tif (r) {\n\t\tradeon_bo_unreserve(rdev->uvd.vcpu_bo);\n\t\tradeon_bo_unref(&rdev->uvd.vcpu_bo);\n\t\tdev_err(rdev->dev, \"(%d) UVD bo pin failed\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = radeon_bo_kmap(rdev->uvd.vcpu_bo, &rdev->uvd.cpu_addr);\n\tif (r) {\n\t\tdev_err(rdev->dev, \"(%d) UVD map failed\\n\", r);\n\t\treturn r;\n\t}\n\n\tradeon_bo_unreserve(rdev->uvd.vcpu_bo);\n\n\tfor (i = 0; i < rdev->uvd.max_handles; ++i) {\n\t\tatomic_set(&rdev->uvd.handles[i], 0);\n\t\trdev->uvd.filp[i] = NULL;\n\t\trdev->uvd.img_size[i] = 0;\n\t}\n\n\treturn 0;\n}\n\nvoid radeon_uvd_fini(struct radeon_device *rdev)\n{\n\tint r;\n\n\tif (rdev->uvd.vcpu_bo == NULL)\n\t\treturn;\n\n\tr = radeon_bo_reserve(rdev->uvd.vcpu_bo, false);\n\tif (!r) {\n\t\tradeon_bo_kunmap(rdev->uvd.vcpu_bo);\n\t\tradeon_bo_unpin(rdev->uvd.vcpu_bo);\n\t\tradeon_bo_unreserve(rdev->uvd.vcpu_bo);\n\t}\n\n\tradeon_bo_unref(&rdev->uvd.vcpu_bo);\n\n\tradeon_ring_fini(rdev, &rdev->ring[R600_RING_TYPE_UVD_INDEX]);\n\n\trelease_firmware(rdev->uvd_fw);\n}\n\nint radeon_uvd_suspend(struct radeon_device *rdev)\n{\n\tint i, r;\n\n\tif (rdev->uvd.vcpu_bo == NULL)\n\t\treturn 0;\n\n\tfor (i = 0; i < rdev->uvd.max_handles; ++i) {\n\t\tuint32_t handle = atomic_read(&rdev->uvd.handles[i]);\n\t\tif (handle != 0) {\n\t\t\tstruct radeon_fence *fence;\n\n\t\t\tradeon_uvd_note_usage(rdev);\n\n\t\t\tr = radeon_uvd_get_destroy_msg(rdev,\n\t\t\t\tR600_RING_TYPE_UVD_INDEX, handle, &fence);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"Error destroying UVD (%d)!\\n\", r);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tradeon_fence_wait(fence, false);\n\t\t\tradeon_fence_unref(&fence);\n\n\t\t\trdev->uvd.filp[i] = NULL;\n\t\t\tatomic_set(&rdev->uvd.handles[i], 0);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint radeon_uvd_resume(struct radeon_device *rdev)\n{\n\tunsigned size;\n\tvoid *ptr;\n\n\tif (rdev->uvd.vcpu_bo == NULL)\n\t\treturn -EINVAL;\n\n\tmemcpy_toio((void __iomem *)rdev->uvd.cpu_addr, rdev->uvd_fw->data, rdev->uvd_fw->size);\n\n\tsize = radeon_bo_size(rdev->uvd.vcpu_bo);\n\tsize -= rdev->uvd_fw->size;\n\n\tptr = rdev->uvd.cpu_addr;\n\tptr += rdev->uvd_fw->size;\n\n\tmemset_io((void __iomem *)ptr, 0, size);\n\n\treturn 0;\n}\n\nvoid radeon_uvd_force_into_uvd_segment(struct radeon_bo *rbo,\n\t\t\t\t       uint32_t allowed_domains)\n{\n\tint i;\n\n\tfor (i = 0; i < rbo->placement.num_placement; ++i) {\n\t\trbo->placements[i].fpfn = 0 >> PAGE_SHIFT;\n\t\trbo->placements[i].lpfn = (256 * 1024 * 1024) >> PAGE_SHIFT;\n\t}\n\n\t \n\tif (allowed_domains == RADEON_GEM_DOMAIN_VRAM)\n\t\treturn;\n\n\t \n\tif (rbo->placement.num_placement > 1)\n\t\treturn;\n\n\t \n\trbo->placements[1] = rbo->placements[0];\n\trbo->placements[1].fpfn += (256 * 1024 * 1024) >> PAGE_SHIFT;\n\trbo->placements[1].lpfn += (256 * 1024 * 1024) >> PAGE_SHIFT;\n\trbo->placement.num_placement++;\n\trbo->placement.num_busy_placement++;\n}\n\nvoid radeon_uvd_free_handles(struct radeon_device *rdev, struct drm_file *filp)\n{\n\tint i, r;\n\tfor (i = 0; i < rdev->uvd.max_handles; ++i) {\n\t\tuint32_t handle = atomic_read(&rdev->uvd.handles[i]);\n\t\tif (handle != 0 && rdev->uvd.filp[i] == filp) {\n\t\t\tstruct radeon_fence *fence;\n\n\t\t\tradeon_uvd_note_usage(rdev);\n\n\t\t\tr = radeon_uvd_get_destroy_msg(rdev,\n\t\t\t\tR600_RING_TYPE_UVD_INDEX, handle, &fence);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"Error destroying UVD (%d)!\\n\", r);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tradeon_fence_wait(fence, false);\n\t\t\tradeon_fence_unref(&fence);\n\n\t\t\trdev->uvd.filp[i] = NULL;\n\t\t\tatomic_set(&rdev->uvd.handles[i], 0);\n\t\t}\n\t}\n}\n\nstatic int radeon_uvd_cs_msg_decode(uint32_t *msg, unsigned buf_sizes[])\n{\n\tunsigned stream_type = msg[4];\n\tunsigned width = msg[6];\n\tunsigned height = msg[7];\n\tunsigned dpb_size = msg[9];\n\tunsigned pitch = msg[28];\n\n\tunsigned width_in_mb = width / 16;\n\tunsigned height_in_mb = ALIGN(height / 16, 2);\n\n\tunsigned image_size, tmp, min_dpb_size;\n\n\timage_size = width * height;\n\timage_size += image_size / 2;\n\timage_size = ALIGN(image_size, 1024);\n\n\tswitch (stream_type) {\n\tcase 0:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 17;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 17 * 192;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 32;\n\t\tbreak;\n\n\tcase 1:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 128;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * 64;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * 128;\n\n\t\t \n\t\ttmp = max(width_in_mb, height_in_mb);\n\t\tmin_dpb_size += ALIGN(tmp * 7 * 16, 64);\n\t\tbreak;\n\n\tcase 3:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\t\tbreak;\n\n\tcase 4:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 64;\n\n\t\t \n\t\tmin_dpb_size += ALIGN(width_in_mb * height_in_mb * 32, 64);\n\t\tbreak;\n\n\tdefault:\n\t\tDRM_ERROR(\"UVD codec not handled %d!\\n\", stream_type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (width > pitch) {\n\t\tDRM_ERROR(\"Invalid UVD decoding target pitch!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dpb_size < min_dpb_size) {\n\t\tDRM_ERROR(\"Invalid dpb_size in UVD message (%d / %d)!\\n\",\n\t\t\t  dpb_size, min_dpb_size);\n\t\treturn -EINVAL;\n\t}\n\n\tbuf_sizes[0x1] = dpb_size;\n\tbuf_sizes[0x2] = image_size;\n\treturn 0;\n}\n\nstatic int radeon_uvd_validate_codec(struct radeon_cs_parser *p,\n\t\t\t\t     unsigned stream_type)\n{\n\tswitch (stream_type) {\n\tcase 0:  \n\tcase 1:  \n\t\t \n\t\treturn 0;\n\n\tcase 3:  \n\tcase 4:  \n\t\t \n\t\tif (p->rdev->family >= CHIP_PALM)\n\t\t\treturn 0;\n\n\t\tfallthrough;\n\tdefault:\n\t\tDRM_ERROR(\"UVD codec not supported by hardware %d!\\n\",\n\t\t\t  stream_type);\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int radeon_uvd_cs_msg(struct radeon_cs_parser *p, struct radeon_bo *bo,\n\t\t\t     unsigned offset, unsigned buf_sizes[])\n{\n\tint32_t *msg, msg_type, handle;\n\tunsigned img_size = 0;\n\tvoid *ptr;\n\tint i, r;\n\n\tif (offset & 0x3F) {\n\t\tDRM_ERROR(\"UVD messages must be 64 byte aligned!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tr = radeon_bo_kmap(bo, &ptr);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed mapping the UVD message (%d)!\\n\", r);\n\t\treturn r;\n\t}\n\n\tmsg = ptr + offset;\n\n\tmsg_type = msg[1];\n\thandle = msg[2];\n\n\tif (handle == 0) {\n\t\tradeon_bo_kunmap(bo);\n\t\tDRM_ERROR(\"Invalid UVD handle!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (msg_type) {\n\tcase 0:\n\t\t \n\t\timg_size = msg[7] * msg[8];\n\n\t\tr = radeon_uvd_validate_codec(p, msg[4]);\n\t\tradeon_bo_kunmap(bo);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tfor (i = 0; i < p->rdev->uvd.max_handles; ++i) {\n\t\t\tif (atomic_read(&p->rdev->uvd.handles[i]) == handle) {\n\t\t\t\tDRM_ERROR(\"Handle 0x%x already in use!\\n\", handle);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (!atomic_cmpxchg(&p->rdev->uvd.handles[i], 0, handle)) {\n\t\t\t\tp->rdev->uvd.filp[i] = p->filp;\n\t\t\t\tp->rdev->uvd.img_size[i] = img_size;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tDRM_ERROR(\"No more free UVD handles!\\n\");\n\t\treturn -EINVAL;\n\n\tcase 1:\n\t\t \n\t\tr = radeon_uvd_validate_codec(p, msg[4]);\n\t\tif (!r)\n\t\t\tr = radeon_uvd_cs_msg_decode(msg, buf_sizes);\n\t\tradeon_bo_kunmap(bo);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tfor (i = 0; i < p->rdev->uvd.max_handles; ++i) {\n\t\t\tif (atomic_read(&p->rdev->uvd.handles[i]) == handle) {\n\t\t\t\tif (p->rdev->uvd.filp[i] != p->filp) {\n\t\t\t\t\tDRM_ERROR(\"UVD handle collision detected!\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tDRM_ERROR(\"Invalid UVD handle 0x%x!\\n\", handle);\n\t\treturn -ENOENT;\n\n\tcase 2:\n\t\t \n\t\tfor (i = 0; i < p->rdev->uvd.max_handles; ++i)\n\t\t\tatomic_cmpxchg(&p->rdev->uvd.handles[i], handle, 0);\n\t\tradeon_bo_kunmap(bo);\n\t\treturn 0;\n\n\tdefault:\n\t\tDRM_ERROR(\"Illegal UVD message type (%d)!\\n\", msg_type);\n\t}\n\n\tradeon_bo_kunmap(bo);\n\treturn -EINVAL;\n}\n\nstatic int radeon_uvd_cs_reloc(struct radeon_cs_parser *p,\n\t\t\t       int data0, int data1,\n\t\t\t       unsigned buf_sizes[], bool *has_msg_cmd)\n{\n\tstruct radeon_cs_chunk *relocs_chunk;\n\tstruct radeon_bo_list *reloc;\n\tunsigned idx, cmd, offset;\n\tuint64_t start, end;\n\tint r;\n\n\trelocs_chunk = p->chunk_relocs;\n\toffset = radeon_get_ib_value(p, data0);\n\tidx = radeon_get_ib_value(p, data1);\n\tif (idx >= relocs_chunk->length_dw) {\n\t\tDRM_ERROR(\"Relocs at %d after relocations chunk end %d !\\n\",\n\t\t\t  idx, relocs_chunk->length_dw);\n\t\treturn -EINVAL;\n\t}\n\n\treloc = &p->relocs[(idx / 4)];\n\tstart = reloc->gpu_offset;\n\tend = start + radeon_bo_size(reloc->robj);\n\tstart += offset;\n\n\tp->ib.ptr[data0] = start & 0xFFFFFFFF;\n\tp->ib.ptr[data1] = start >> 32;\n\n\tcmd = radeon_get_ib_value(p, p->idx) >> 1;\n\n\tif (cmd < 0x4) {\n\t\tif (end <= start) {\n\t\t\tDRM_ERROR(\"invalid reloc offset %X!\\n\", offset);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif ((end - start) < buf_sizes[cmd]) {\n\t\t\tDRM_ERROR(\"buffer (%d) to small (%d / %d)!\\n\", cmd,\n\t\t\t\t  (unsigned)(end - start), buf_sizes[cmd]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t} else if (cmd != 0x100) {\n\t\tDRM_ERROR(\"invalid UVD command %X!\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((start >> 28) != ((end - 1) >> 28)) {\n\t\tDRM_ERROR(\"reloc %LX-%LX crossing 256MB boundary!\\n\",\n\t\t\t  start, end);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((cmd == 0 || cmd == 0x3) &&\n\t    (start >> 28) != (p->rdev->uvd.gpu_addr >> 28)) {\n\t\tDRM_ERROR(\"msg/fb buffer %LX-%LX out of 256MB segment!\\n\",\n\t\t\t  start, end);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cmd == 0) {\n\t\tif (*has_msg_cmd) {\n\t\t\tDRM_ERROR(\"More than one message in a UVD-IB!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*has_msg_cmd = true;\n\t\tr = radeon_uvd_cs_msg(p, reloc->robj, offset, buf_sizes);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!*has_msg_cmd) {\n\t\tDRM_ERROR(\"Message needed before other commands are send!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int radeon_uvd_cs_reg(struct radeon_cs_parser *p,\n\t\t\t     struct radeon_cs_packet *pkt,\n\t\t\t     int *data0, int *data1,\n\t\t\t     unsigned buf_sizes[],\n\t\t\t     bool *has_msg_cmd)\n{\n\tint i, r;\n\n\tp->idx++;\n\tfor (i = 0; i <= pkt->count; ++i) {\n\t\tswitch (pkt->reg + i*4) {\n\t\tcase UVD_GPCOM_VCPU_DATA0:\n\t\t\t*data0 = p->idx;\n\t\t\tbreak;\n\t\tcase UVD_GPCOM_VCPU_DATA1:\n\t\t\t*data1 = p->idx;\n\t\t\tbreak;\n\t\tcase UVD_GPCOM_VCPU_CMD:\n\t\t\tr = radeon_uvd_cs_reloc(p, *data0, *data1,\n\t\t\t\t\t\tbuf_sizes, has_msg_cmd);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase UVD_ENGINE_CNTL:\n\t\tcase UVD_NO_OP:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Invalid reg 0x%X!\\n\",\n\t\t\t\t  pkt->reg + i*4);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tp->idx++;\n\t}\n\treturn 0;\n}\n\nint radeon_uvd_cs_parse(struct radeon_cs_parser *p)\n{\n\tstruct radeon_cs_packet pkt;\n\tint r, data0 = 0, data1 = 0;\n\n\t \n\tbool has_msg_cmd = false;\n\n\t \n\tunsigned buf_sizes[] = {\n\t\t[0x00000000]\t=\t2048,\n\t\t[0x00000001]\t=\t32 * 1024 * 1024,\n\t\t[0x00000002]\t=\t2048 * 1152 * 3,\n\t\t[0x00000003]\t=\t2048,\n\t};\n\n\tif (p->chunk_ib->length_dw % 16) {\n\t\tDRM_ERROR(\"UVD IB length (%d) not 16 dwords aligned!\\n\",\n\t\t\t  p->chunk_ib->length_dw);\n\t\treturn -EINVAL;\n\t}\n\n\tif (p->chunk_relocs == NULL) {\n\t\tDRM_ERROR(\"No relocation chunk !\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\n\tdo {\n\t\tr = radeon_cs_packet_parse(p, &pkt, p->idx);\n\t\tif (r)\n\t\t\treturn r;\n\t\tswitch (pkt.type) {\n\t\tcase RADEON_PACKET_TYPE0:\n\t\t\tr = radeon_uvd_cs_reg(p, &pkt, &data0, &data1,\n\t\t\t\t\t      buf_sizes, &has_msg_cmd);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase RADEON_PACKET_TYPE2:\n\t\t\tp->idx += pkt.count + 2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown packet type %d !\\n\", pkt.type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} while (p->idx < p->chunk_ib->length_dw);\n\n\tif (!has_msg_cmd) {\n\t\tDRM_ERROR(\"UVD-IBs need a msg command!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int radeon_uvd_send_msg(struct radeon_device *rdev,\n\t\t\t       int ring, uint64_t addr,\n\t\t\t       struct radeon_fence **fence)\n{\n\tstruct radeon_ib ib;\n\tint i, r;\n\n\tr = radeon_ib_get(rdev, ring, &ib, NULL, 64);\n\tif (r)\n\t\treturn r;\n\n\tib.ptr[0] = PACKET0(UVD_GPCOM_VCPU_DATA0, 0);\n\tib.ptr[1] = addr;\n\tib.ptr[2] = PACKET0(UVD_GPCOM_VCPU_DATA1, 0);\n\tib.ptr[3] = addr >> 32;\n\tib.ptr[4] = PACKET0(UVD_GPCOM_VCPU_CMD, 0);\n\tib.ptr[5] = 0;\n\tfor (i = 6; i < 16; i += 2) {\n\t\tib.ptr[i] = PACKET0(UVD_NO_OP, 0);\n\t\tib.ptr[i+1] = 0;\n\t}\n\tib.length_dw = 16;\n\n\tr = radeon_ib_schedule(rdev, &ib, NULL, false);\n\n\tif (fence)\n\t\t*fence = radeon_fence_ref(ib.fence);\n\n\tradeon_ib_free(rdev, &ib);\n\treturn r;\n}\n\n \nint radeon_uvd_get_create_msg(struct radeon_device *rdev, int ring,\n\t\t\t      uint32_t handle, struct radeon_fence **fence)\n{\n\t \n\tuint64_t offs = radeon_bo_size(rdev->uvd.vcpu_bo) -\n\t\tRADEON_GPU_PAGE_SIZE;\n\n\tuint32_t __iomem *msg = (void __iomem *)(rdev->uvd.cpu_addr + offs);\n\tuint64_t addr = rdev->uvd.gpu_addr + offs;\n\n\tint r, i;\n\n\tr = radeon_bo_reserve(rdev->uvd.vcpu_bo, true);\n\tif (r)\n\t\treturn r;\n\n\t \n\twritel((__force u32)cpu_to_le32(0x00000de4), &msg[0]);\n\twritel(0x0, (void __iomem *)&msg[1]);\n\twritel((__force u32)cpu_to_le32(handle), &msg[2]);\n\twritel(0x0, &msg[3]);\n\twritel(0x0, &msg[4]);\n\twritel(0x0, &msg[5]);\n\twritel(0x0, &msg[6]);\n\twritel((__force u32)cpu_to_le32(0x00000780), &msg[7]);\n\twritel((__force u32)cpu_to_le32(0x00000440), &msg[8]);\n\twritel(0x0, &msg[9]);\n\twritel((__force u32)cpu_to_le32(0x01b37000), &msg[10]);\n\tfor (i = 11; i < 1024; ++i)\n\t\twritel(0x0, &msg[i]);\n\n\tr = radeon_uvd_send_msg(rdev, ring, addr, fence);\n\tradeon_bo_unreserve(rdev->uvd.vcpu_bo);\n\treturn r;\n}\n\nint radeon_uvd_get_destroy_msg(struct radeon_device *rdev, int ring,\n\t\t\t       uint32_t handle, struct radeon_fence **fence)\n{\n\t \n\tuint64_t offs = radeon_bo_size(rdev->uvd.vcpu_bo) -\n\t\tRADEON_GPU_PAGE_SIZE;\n\n\tuint32_t __iomem *msg = (void __iomem *)(rdev->uvd.cpu_addr + offs);\n\tuint64_t addr = rdev->uvd.gpu_addr + offs;\n\n\tint r, i;\n\n\tr = radeon_bo_reserve(rdev->uvd.vcpu_bo, true);\n\tif (r)\n\t\treturn r;\n\n\t \n\twritel((__force u32)cpu_to_le32(0x00000de4), &msg[0]);\n\twritel((__force u32)cpu_to_le32(0x00000002), &msg[1]);\n\twritel((__force u32)cpu_to_le32(handle), &msg[2]);\n\twritel(0x0, &msg[3]);\n\tfor (i = 4; i < 1024; ++i)\n\t\twritel(0x0, &msg[i]);\n\n\tr = radeon_uvd_send_msg(rdev, ring, addr, fence);\n\tradeon_bo_unreserve(rdev->uvd.vcpu_bo);\n\treturn r;\n}\n\n \nstatic void radeon_uvd_count_handles(struct radeon_device *rdev,\n\t\t\t\t     unsigned *sd, unsigned *hd)\n{\n\tunsigned i;\n\n\t*sd = 0;\n\t*hd = 0;\n\n\tfor (i = 0; i < rdev->uvd.max_handles; ++i) {\n\t\tif (!atomic_read(&rdev->uvd.handles[i]))\n\t\t\tcontinue;\n\n\t\tif (rdev->uvd.img_size[i] >= 720*576)\n\t\t\t++(*hd);\n\t\telse\n\t\t\t++(*sd);\n\t}\n}\n\nstatic void radeon_uvd_idle_work_handler(struct work_struct *work)\n{\n\tstruct radeon_device *rdev =\n\t\tcontainer_of(work, struct radeon_device, uvd.idle_work.work);\n\n\tif (radeon_fence_count_emitted(rdev, R600_RING_TYPE_UVD_INDEX) == 0) {\n\t\tif ((rdev->pm.pm_method == PM_METHOD_DPM) && rdev->pm.dpm_enabled) {\n\t\t\tradeon_uvd_count_handles(rdev, &rdev->pm.dpm.sd,\n\t\t\t\t\t\t &rdev->pm.dpm.hd);\n\t\t\tradeon_dpm_enable_uvd(rdev, false);\n\t\t} else {\n\t\t\tradeon_set_uvd_clocks(rdev, 0, 0);\n\t\t}\n\t} else {\n\t\tschedule_delayed_work(&rdev->uvd.idle_work,\n\t\t\t\t      msecs_to_jiffies(UVD_IDLE_TIMEOUT_MS));\n\t}\n}\n\nvoid radeon_uvd_note_usage(struct radeon_device *rdev)\n{\n\tbool streams_changed = false;\n\tbool set_clocks = !cancel_delayed_work_sync(&rdev->uvd.idle_work);\n\tset_clocks &= schedule_delayed_work(&rdev->uvd.idle_work,\n\t\t\t\t\t    msecs_to_jiffies(UVD_IDLE_TIMEOUT_MS));\n\n\tif ((rdev->pm.pm_method == PM_METHOD_DPM) && rdev->pm.dpm_enabled) {\n\t\tunsigned hd = 0, sd = 0;\n\t\tradeon_uvd_count_handles(rdev, &sd, &hd);\n\t\tif ((rdev->pm.dpm.sd != sd) ||\n\t\t    (rdev->pm.dpm.hd != hd)) {\n\t\t\trdev->pm.dpm.sd = sd;\n\t\t\trdev->pm.dpm.hd = hd;\n\t\t\t \n\t\t\t \n\t\t}\n\t}\n\n\tif (set_clocks || streams_changed) {\n\t\tif ((rdev->pm.pm_method == PM_METHOD_DPM) && rdev->pm.dpm_enabled) {\n\t\t\tradeon_dpm_enable_uvd(rdev, true);\n\t\t} else {\n\t\t\tradeon_set_uvd_clocks(rdev, 53300, 40000);\n\t\t}\n\t}\n}\n\nstatic unsigned radeon_uvd_calc_upll_post_div(unsigned vco_freq,\n\t\t\t\t\t      unsigned target_freq,\n\t\t\t\t\t      unsigned pd_min,\n\t\t\t\t\t      unsigned pd_even)\n{\n\tunsigned post_div = vco_freq / target_freq;\n\n\t \n\tif (post_div < pd_min)\n\t\tpost_div = pd_min;\n\n\t \n\tif ((vco_freq / post_div) > target_freq)\n\t\tpost_div += 1;\n\n\t \n\tif (post_div > pd_even && post_div % 2)\n\t\tpost_div += 1;\n\n\treturn post_div;\n}\n\n \nint radeon_uvd_calc_upll_dividers(struct radeon_device *rdev,\n\t\t\t\t  unsigned vclk, unsigned dclk,\n\t\t\t\t  unsigned vco_min, unsigned vco_max,\n\t\t\t\t  unsigned fb_factor, unsigned fb_mask,\n\t\t\t\t  unsigned pd_min, unsigned pd_max,\n\t\t\t\t  unsigned pd_even,\n\t\t\t\t  unsigned *optimal_fb_div,\n\t\t\t\t  unsigned *optimal_vclk_div,\n\t\t\t\t  unsigned *optimal_dclk_div)\n{\n\tunsigned vco_freq, ref_freq = rdev->clock.spll.reference_freq;\n\n\t \n\tunsigned optimal_score = ~0;\n\n\t \n\tvco_min = max(max(vco_min, vclk), dclk);\n\tfor (vco_freq = vco_min; vco_freq <= vco_max; vco_freq += 100) {\n\n\t\tuint64_t fb_div = (uint64_t)vco_freq * fb_factor;\n\t\tunsigned vclk_div, dclk_div, score;\n\n\t\tdo_div(fb_div, ref_freq);\n\n\t\t \n\t\tif (fb_div > fb_mask)\n\t\t\tbreak;  \n\n\t\tfb_div &= fb_mask;\n\n\t\t \n\t\tvclk_div = radeon_uvd_calc_upll_post_div(vco_freq, vclk,\n\t\t\t\t\t\t\t pd_min, pd_even);\n\t\tif (vclk_div > pd_max)\n\t\t\tbreak;  \n\n\t\t \n\t\tdclk_div = radeon_uvd_calc_upll_post_div(vco_freq, dclk,\n\t\t\t\t\t\t\t pd_min, pd_even);\n\t\tif (dclk_div > pd_max)\n\t\t\tbreak;  \n\n\t\t \n\t\tscore = vclk - (vco_freq / vclk_div) + dclk - (vco_freq / dclk_div);\n\n\t\t \n\t\tif (score < optimal_score) {\n\t\t\t*optimal_fb_div = fb_div;\n\t\t\t*optimal_vclk_div = vclk_div;\n\t\t\t*optimal_dclk_div = dclk_div;\n\t\t\toptimal_score = score;\n\t\t\tif (optimal_score == 0)\n\t\t\t\tbreak;  \n\t\t}\n\t}\n\n\t \n\tif (optimal_score == ~0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nint radeon_uvd_send_upll_ctlreq(struct radeon_device *rdev,\n\t\t\t\tunsigned cg_upll_func_cntl)\n{\n\tunsigned i;\n\n\t \n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tmdelay(10);\n\n\t \n\tWREG32_P(cg_upll_func_cntl, UPLL_CTLREQ_MASK, ~UPLL_CTLREQ_MASK);\n\n\t \n\tfor (i = 0; i < 100; ++i) {\n\t\tuint32_t mask = UPLL_CTLACK_MASK | UPLL_CTLACK2_MASK;\n\t\tif ((RREG32(cg_upll_func_cntl) & mask) == mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\t \n\tWREG32_P(cg_upll_func_cntl, 0, ~UPLL_CTLREQ_MASK);\n\n\tif (i == 100) {\n\t\tDRM_ERROR(\"Timeout setting UVD clocks!\\n\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}