{
  "module_name": "radeon_gem.c",
  "hash_id": "3be48ab1404b7533703a001dbba9ed51f8b2a3c987dfe63770d9d5c93e50d6ba",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/radeon_gem.c",
  "human_readable_source": " \n\n#include <linux/iosys-map.h>\n#include <linux/pci.h>\n\n#include <drm/drm_device.h>\n#include <drm/drm_file.h>\n#include <drm/drm_gem_ttm_helper.h>\n#include <drm/radeon_drm.h>\n\n#include \"radeon.h\"\n#include \"radeon_prime.h\"\n\nstruct dma_buf *radeon_gem_prime_export(struct drm_gem_object *gobj,\n\t\t\t\t\tint flags);\nstruct sg_table *radeon_gem_prime_get_sg_table(struct drm_gem_object *obj);\nint radeon_gem_prime_pin(struct drm_gem_object *obj);\nvoid radeon_gem_prime_unpin(struct drm_gem_object *obj);\n\nconst struct drm_gem_object_funcs radeon_gem_object_funcs;\n\nstatic vm_fault_t radeon_gem_fault(struct vm_fault *vmf)\n{\n\tstruct ttm_buffer_object *bo = vmf->vma->vm_private_data;\n\tstruct radeon_device *rdev = radeon_get_rdev(bo->bdev);\n\tvm_fault_t ret;\n\n\tdown_read(&rdev->pm.mclk_lock);\n\n\tret = ttm_bo_vm_reserve(bo, vmf);\n\tif (ret)\n\t\tgoto unlock_mclk;\n\n\tret = radeon_bo_fault_reserve_notify(bo);\n\tif (ret)\n\t\tgoto unlock_resv;\n\n\tret = ttm_bo_vm_fault_reserved(vmf, vmf->vma->vm_page_prot,\n\t\t\t\t       TTM_BO_VM_NUM_PREFAULT);\n\tif (ret == VM_FAULT_RETRY && !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT))\n\t\tgoto unlock_mclk;\n\nunlock_resv:\n\tdma_resv_unlock(bo->base.resv);\n\nunlock_mclk:\n\tup_read(&rdev->pm.mclk_lock);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct radeon_gem_vm_ops = {\n\t.fault = radeon_gem_fault,\n\t.open = ttm_bo_vm_open,\n\t.close = ttm_bo_vm_close,\n\t.access = ttm_bo_vm_access\n};\n\nstatic void radeon_gem_object_free(struct drm_gem_object *gobj)\n{\n\tstruct radeon_bo *robj = gem_to_radeon_bo(gobj);\n\n\tif (robj) {\n\t\tradeon_mn_unregister(robj);\n\t\tradeon_bo_unref(&robj);\n\t}\n}\n\nint radeon_gem_object_create(struct radeon_device *rdev, unsigned long size,\n\t\t\t\tint alignment, int initial_domain,\n\t\t\t\tu32 flags, bool kernel,\n\t\t\t\tstruct drm_gem_object **obj)\n{\n\tstruct radeon_bo *robj;\n\tunsigned long max_size;\n\tint r;\n\n\t*obj = NULL;\n\t \n\tif (alignment < PAGE_SIZE) {\n\t\talignment = PAGE_SIZE;\n\t}\n\n\t \n\tmax_size = rdev->mc.gtt_size - rdev->gart_pin_size;\n\tif (size > max_size) {\n\t\tDRM_DEBUG(\"Allocation size %ldMb bigger than %ldMb limit\\n\",\n\t\t\t  size >> 20, max_size >> 20);\n\t\treturn -ENOMEM;\n\t}\n\nretry:\n\tr = radeon_bo_create(rdev, size, alignment, kernel, initial_domain,\n\t\t\t     flags, NULL, NULL, &robj);\n\tif (r) {\n\t\tif (r != -ERESTARTSYS) {\n\t\t\tif (initial_domain == RADEON_GEM_DOMAIN_VRAM) {\n\t\t\t\tinitial_domain |= RADEON_GEM_DOMAIN_GTT;\n\t\t\t\tgoto retry;\n\t\t\t}\n\t\t\tDRM_ERROR(\"Failed to allocate GEM object (%ld, %d, %u, %d)\\n\",\n\t\t\t\t  size, initial_domain, alignment, r);\n\t\t}\n\t\treturn r;\n\t}\n\t*obj = &robj->tbo.base;\n\t(*obj)->funcs = &radeon_gem_object_funcs;\n\trobj->pid = task_pid_nr(current);\n\n\tmutex_lock(&rdev->gem.mutex);\n\tlist_add_tail(&robj->list, &rdev->gem.objects);\n\tmutex_unlock(&rdev->gem.mutex);\n\n\treturn 0;\n}\n\nstatic int radeon_gem_set_domain(struct drm_gem_object *gobj,\n\t\t\t  uint32_t rdomain, uint32_t wdomain)\n{\n\tstruct radeon_bo *robj;\n\tuint32_t domain;\n\tlong r;\n\n\t \n\trobj = gem_to_radeon_bo(gobj);\n\t \n\tdomain = wdomain;\n\tif (!domain) {\n\t\tdomain = rdomain;\n\t}\n\tif (!domain) {\n\t\t \n\t\tpr_warn(\"Set domain without domain !\\n\");\n\t\treturn 0;\n\t}\n\tif (domain == RADEON_GEM_DOMAIN_CPU) {\n\t\t \n\t\tr = dma_resv_wait_timeout(robj->tbo.base.resv,\n\t\t\t\t\t  DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t\t  true, 30 * HZ);\n\t\tif (!r)\n\t\t\tr = -EBUSY;\n\n\t\tif (r < 0 && r != -EINTR) {\n\t\t\tpr_err(\"Failed to wait for object: %li\\n\", r);\n\t\t\treturn r;\n\t\t}\n\t}\n\tif (domain == RADEON_GEM_DOMAIN_VRAM && robj->prime_shared_count) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nint radeon_gem_init(struct radeon_device *rdev)\n{\n\tINIT_LIST_HEAD(&rdev->gem.objects);\n\treturn 0;\n}\n\nvoid radeon_gem_fini(struct radeon_device *rdev)\n{\n\tradeon_bo_force_delete(rdev);\n}\n\n \nstatic int radeon_gem_object_open(struct drm_gem_object *obj, struct drm_file *file_priv)\n{\n\tstruct radeon_bo *rbo = gem_to_radeon_bo(obj);\n\tstruct radeon_device *rdev = rbo->rdev;\n\tstruct radeon_fpriv *fpriv = file_priv->driver_priv;\n\tstruct radeon_vm *vm = &fpriv->vm;\n\tstruct radeon_bo_va *bo_va;\n\tint r;\n\n\tif ((rdev->family < CHIP_CAYMAN) ||\n\t    (!rdev->accel_working)) {\n\t\treturn 0;\n\t}\n\n\tr = radeon_bo_reserve(rbo, false);\n\tif (r) {\n\t\treturn r;\n\t}\n\n\tbo_va = radeon_vm_bo_find(vm, rbo);\n\tif (!bo_va) {\n\t\tbo_va = radeon_vm_bo_add(rdev, vm, rbo);\n\t} else {\n\t\t++bo_va->ref_count;\n\t}\n\tradeon_bo_unreserve(rbo);\n\n\treturn 0;\n}\n\nstatic void radeon_gem_object_close(struct drm_gem_object *obj,\n\t\t\t\t    struct drm_file *file_priv)\n{\n\tstruct radeon_bo *rbo = gem_to_radeon_bo(obj);\n\tstruct radeon_device *rdev = rbo->rdev;\n\tstruct radeon_fpriv *fpriv = file_priv->driver_priv;\n\tstruct radeon_vm *vm = &fpriv->vm;\n\tstruct radeon_bo_va *bo_va;\n\tint r;\n\n\tif ((rdev->family < CHIP_CAYMAN) ||\n\t    (!rdev->accel_working)) {\n\t\treturn;\n\t}\n\n\tr = radeon_bo_reserve(rbo, true);\n\tif (r) {\n\t\tdev_err(rdev->dev, \"leaking bo va because \"\n\t\t\t\"we fail to reserve bo (%d)\\n\", r);\n\t\treturn;\n\t}\n\tbo_va = radeon_vm_bo_find(vm, rbo);\n\tif (bo_va) {\n\t\tif (--bo_va->ref_count == 0) {\n\t\t\tradeon_vm_bo_rmv(rdev, bo_va);\n\t\t}\n\t}\n\tradeon_bo_unreserve(rbo);\n}\n\nstatic int radeon_gem_handle_lockup(struct radeon_device *rdev, int r)\n{\n\tif (r == -EDEADLK) {\n\t\tr = radeon_gpu_reset(rdev);\n\t\tif (!r)\n\t\t\tr = -EAGAIN;\n\t}\n\treturn r;\n}\n\nstatic int radeon_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct radeon_bo *bo = gem_to_radeon_bo(obj);\n\tstruct radeon_device *rdev = radeon_get_rdev(bo->tbo.bdev);\n\n\tif (radeon_ttm_tt_has_userptr(rdev, bo->tbo.ttm))\n\t\treturn -EPERM;\n\n\treturn drm_gem_ttm_mmap(obj, vma);\n}\n\nconst struct drm_gem_object_funcs radeon_gem_object_funcs = {\n\t.free = radeon_gem_object_free,\n\t.open = radeon_gem_object_open,\n\t.close = radeon_gem_object_close,\n\t.export = radeon_gem_prime_export,\n\t.pin = radeon_gem_prime_pin,\n\t.unpin = radeon_gem_prime_unpin,\n\t.get_sg_table = radeon_gem_prime_get_sg_table,\n\t.vmap = drm_gem_ttm_vmap,\n\t.vunmap = drm_gem_ttm_vunmap,\n\t.mmap = radeon_gem_object_mmap,\n\t.vm_ops = &radeon_gem_vm_ops,\n};\n\n \nint radeon_gem_info_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_radeon_gem_info *args = data;\n\tstruct ttm_resource_manager *man;\n\n\tman = ttm_manager_type(&rdev->mman.bdev, TTM_PL_VRAM);\n\n\targs->vram_size = (u64)man->size << PAGE_SHIFT;\n\targs->vram_visible = rdev->mc.visible_vram_size;\n\targs->vram_visible -= rdev->vram_pin_size;\n\targs->gart_size = rdev->mc.gtt_size;\n\targs->gart_size -= rdev->gart_pin_size;\n\n\treturn 0;\n}\n\nint radeon_gem_create_ioctl(struct drm_device *dev, void *data,\n\t\t\t    struct drm_file *filp)\n{\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_radeon_gem_create *args = data;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tint r;\n\n\tdown_read(&rdev->exclusive_lock);\n\t \n\targs->size = roundup(args->size, PAGE_SIZE);\n\tr = radeon_gem_object_create(rdev, args->size, args->alignment,\n\t\t\t\t     args->initial_domain, args->flags,\n\t\t\t\t     false, &gobj);\n\tif (r) {\n\t\tup_read(&rdev->exclusive_lock);\n\t\tr = radeon_gem_handle_lockup(rdev, r);\n\t\treturn r;\n\t}\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t \n\tdrm_gem_object_put(gobj);\n\tif (r) {\n\t\tup_read(&rdev->exclusive_lock);\n\t\tr = radeon_gem_handle_lockup(rdev, r);\n\t\treturn r;\n\t}\n\targs->handle = handle;\n\tup_read(&rdev->exclusive_lock);\n\treturn 0;\n}\n\nint radeon_gem_userptr_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *filp)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_radeon_gem_userptr *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *bo;\n\tuint32_t handle;\n\tint r;\n\n\targs->addr = untagged_addr(args->addr);\n\n\tif (offset_in_page(args->addr | args->size))\n\t\treturn -EINVAL;\n\n\t \n\tif (args->flags & ~(RADEON_GEM_USERPTR_READONLY |\n\t    RADEON_GEM_USERPTR_ANONONLY | RADEON_GEM_USERPTR_VALIDATE |\n\t    RADEON_GEM_USERPTR_REGISTER))\n\t\treturn -EINVAL;\n\n\tif (args->flags & RADEON_GEM_USERPTR_READONLY) {\n\t\t \n\t\tif (rdev->family < CHIP_R600)\n\t\t\treturn -EINVAL;\n\n\t} else if (!(args->flags & RADEON_GEM_USERPTR_ANONONLY) ||\n\t\t   !(args->flags & RADEON_GEM_USERPTR_REGISTER)) {\n\n\t\t \n\t\treturn -EACCES;\n\t}\n\n\tdown_read(&rdev->exclusive_lock);\n\n\t \n\tr = radeon_gem_object_create(rdev, args->size, 0,\n\t\t\t\t     RADEON_GEM_DOMAIN_CPU, 0,\n\t\t\t\t     false, &gobj);\n\tif (r)\n\t\tgoto handle_lockup;\n\n\tbo = gem_to_radeon_bo(gobj);\n\tr = radeon_ttm_tt_set_userptr(rdev, bo->tbo.ttm, args->addr, args->flags);\n\tif (r)\n\t\tgoto release_object;\n\n\tif (args->flags & RADEON_GEM_USERPTR_REGISTER) {\n\t\tr = radeon_mn_register(bo, args->addr);\n\t\tif (r)\n\t\t\tgoto release_object;\n\t}\n\n\tif (args->flags & RADEON_GEM_USERPTR_VALIDATE) {\n\t\tmmap_read_lock(current->mm);\n\t\tr = radeon_bo_reserve(bo, true);\n\t\tif (r) {\n\t\t\tmmap_read_unlock(current->mm);\n\t\t\tgoto release_object;\n\t\t}\n\n\t\tradeon_ttm_placement_from_domain(bo, RADEON_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\tradeon_bo_unreserve(bo);\n\t\tmmap_read_unlock(current->mm);\n\t\tif (r)\n\t\t\tgoto release_object;\n\t}\n\n\tr = drm_gem_handle_create(filp, gobj, &handle);\n\t \n\tdrm_gem_object_put(gobj);\n\tif (r)\n\t\tgoto handle_lockup;\n\n\targs->handle = handle;\n\tup_read(&rdev->exclusive_lock);\n\treturn 0;\n\nrelease_object:\n\tdrm_gem_object_put(gobj);\n\nhandle_lockup:\n\tup_read(&rdev->exclusive_lock);\n\tr = radeon_gem_handle_lockup(rdev, r);\n\n\treturn r;\n}\n\nint radeon_gem_set_domain_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\t \n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_radeon_gem_set_domain *args = data;\n\tstruct drm_gem_object *gobj;\n\tint r;\n\n\t \n\tdown_read(&rdev->exclusive_lock);\n\n\t \n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL) {\n\t\tup_read(&rdev->exclusive_lock);\n\t\treturn -ENOENT;\n\t}\n\n\tr = radeon_gem_set_domain(gobj, args->read_domains, args->write_domain);\n\n\tdrm_gem_object_put(gobj);\n\tup_read(&rdev->exclusive_lock);\n\tr = radeon_gem_handle_lockup(rdev, r);\n\treturn r;\n}\n\nint radeon_mode_dumb_mmap(struct drm_file *filp,\n\t\t\t  struct drm_device *dev,\n\t\t\t  uint32_t handle, uint64_t *offset_p)\n{\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *robj;\n\n\tgobj = drm_gem_object_lookup(filp, handle);\n\tif (gobj == NULL) {\n\t\treturn -ENOENT;\n\t}\n\trobj = gem_to_radeon_bo(gobj);\n\tif (radeon_ttm_tt_has_userptr(robj->rdev, robj->tbo.ttm)) {\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -EPERM;\n\t}\n\t*offset_p = radeon_bo_mmap_offset(robj);\n\tdrm_gem_object_put(gobj);\n\treturn 0;\n}\n\nint radeon_gem_mmap_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tstruct drm_radeon_gem_mmap *args = data;\n\n\treturn radeon_mode_dumb_mmap(filp, dev, args->handle, &args->addr_ptr);\n}\n\nint radeon_gem_busy_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tstruct drm_radeon_gem_busy *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *robj;\n\tint r;\n\tuint32_t cur_placement = 0;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL) {\n\t\treturn -ENOENT;\n\t}\n\trobj = gem_to_radeon_bo(gobj);\n\n\tr = dma_resv_test_signaled(robj->tbo.base.resv, DMA_RESV_USAGE_READ);\n\tif (r == 0)\n\t\tr = -EBUSY;\n\telse\n\t\tr = 0;\n\n\tcur_placement = READ_ONCE(robj->tbo.resource->mem_type);\n\targs->domain = radeon_mem_type_to_domain(cur_placement);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint radeon_gem_wait_idle_ioctl(struct drm_device *dev, void *data,\n\t\t\t      struct drm_file *filp)\n{\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_radeon_gem_wait_idle *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *robj;\n\tint r = 0;\n\tuint32_t cur_placement = 0;\n\tlong ret;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL) {\n\t\treturn -ENOENT;\n\t}\n\trobj = gem_to_radeon_bo(gobj);\n\n\tret = dma_resv_wait_timeout(robj->tbo.base.resv, DMA_RESV_USAGE_READ,\n\t\t\t\t    true, 30 * HZ);\n\tif (ret == 0)\n\t\tr = -EBUSY;\n\telse if (ret < 0)\n\t\tr = ret;\n\n\t \n\tcur_placement = READ_ONCE(robj->tbo.resource->mem_type);\n\tif (rdev->asic->mmio_hdp_flush &&\n\t    radeon_mem_type_to_domain(cur_placement) == RADEON_GEM_DOMAIN_VRAM)\n\t\trobj->rdev->asic->mmio_hdp_flush(rdev);\n\tdrm_gem_object_put(gobj);\n\tr = radeon_gem_handle_lockup(rdev, r);\n\treturn r;\n}\n\nint radeon_gem_set_tiling_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_radeon_gem_set_tiling *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *robj;\n\tint r = 0;\n\n\tDRM_DEBUG(\"%d \\n\", args->handle);\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trobj = gem_to_radeon_bo(gobj);\n\tr = radeon_bo_set_tiling_flags(robj, args->tiling_flags, args->pitch);\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint radeon_gem_get_tiling_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_radeon_gem_get_tiling *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *rbo;\n\tint r = 0;\n\n\tDRM_DEBUG(\"\\n\");\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL)\n\t\treturn -ENOENT;\n\trbo = gem_to_radeon_bo(gobj);\n\tr = radeon_bo_reserve(rbo, false);\n\tif (unlikely(r != 0))\n\t\tgoto out;\n\tradeon_bo_get_tiling_flags(rbo, &args->tiling_flags, &args->pitch);\n\tradeon_bo_unreserve(rbo);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\n \nstatic void radeon_gem_va_update_vm(struct radeon_device *rdev,\n\t\t\t\t    struct radeon_bo_va *bo_va)\n{\n\tstruct ttm_validate_buffer tv, *entry;\n\tstruct radeon_bo_list *vm_bos;\n\tstruct ww_acquire_ctx ticket;\n\tstruct list_head list;\n\tunsigned domain;\n\tint r;\n\n\tINIT_LIST_HEAD(&list);\n\n\ttv.bo = &bo_va->bo->tbo;\n\ttv.num_shared = 1;\n\tlist_add(&tv.head, &list);\n\n\tvm_bos = radeon_vm_get_bos(rdev, bo_va->vm, &list);\n\tif (!vm_bos)\n\t\treturn;\n\n\tr = ttm_eu_reserve_buffers(&ticket, &list, true, NULL);\n\tif (r)\n\t\tgoto error_free;\n\n\tlist_for_each_entry(entry, &list, head) {\n\t\tdomain = radeon_mem_type_to_domain(entry->bo->resource->mem_type);\n\t\t \n\t\tif (domain == RADEON_GEM_DOMAIN_CPU)\n\t\t\tgoto error_unreserve;\n\t}\n\n\tmutex_lock(&bo_va->vm->mutex);\n\tr = radeon_vm_clear_freed(rdev, bo_va->vm);\n\tif (r)\n\t\tgoto error_unlock;\n\n\tif (bo_va->it.start)\n\t\tr = radeon_vm_bo_update(rdev, bo_va, bo_va->bo->tbo.resource);\n\nerror_unlock:\n\tmutex_unlock(&bo_va->vm->mutex);\n\nerror_unreserve:\n\tttm_eu_backoff_reservation(&ticket, &list);\n\nerror_free:\n\tkvfree(vm_bos);\n\n\tif (r && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Couldn't update BO_VA (%d)\\n\", r);\n}\n\nint radeon_gem_va_ioctl(struct drm_device *dev, void *data,\n\t\t\t  struct drm_file *filp)\n{\n\tstruct drm_radeon_gem_va *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct radeon_fpriv *fpriv = filp->driver_priv;\n\tstruct radeon_bo *rbo;\n\tstruct radeon_bo_va *bo_va;\n\tu32 invalid_flags;\n\tint r = 0;\n\n\tif (!rdev->vm_manager.enabled) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -ENOTTY;\n\t}\n\n\t \n\tif (args->vm_id) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->offset < RADEON_VA_RESERVED_SIZE) {\n\t\tdev_err(dev->dev,\n\t\t\t\"offset 0x%lX is in reserved area 0x%X\\n\",\n\t\t\t(unsigned long)args->offset,\n\t\t\tRADEON_VA_RESERVED_SIZE);\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tinvalid_flags = RADEON_VM_PAGE_VALID | RADEON_VM_PAGE_SYSTEM;\n\tif ((args->flags & invalid_flags)) {\n\t\tdev_err(dev->dev, \"invalid flags 0x%08X vs 0x%08X\\n\",\n\t\t\targs->flags, invalid_flags);\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (args->operation) {\n\tcase RADEON_VA_MAP:\n\tcase RADEON_VA_UNMAP:\n\t\tbreak;\n\tdefault:\n\t\tdev_err(dev->dev, \"unsupported operation %d\\n\",\n\t\t\targs->operation);\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -EINVAL;\n\t}\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\treturn -ENOENT;\n\t}\n\trbo = gem_to_radeon_bo(gobj);\n\tr = radeon_bo_reserve(rbo, false);\n\tif (r) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\tdrm_gem_object_put(gobj);\n\t\treturn r;\n\t}\n\tbo_va = radeon_vm_bo_find(&fpriv->vm, rbo);\n\tif (!bo_va) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t\tradeon_bo_unreserve(rbo);\n\t\tdrm_gem_object_put(gobj);\n\t\treturn -ENOENT;\n\t}\n\n\tswitch (args->operation) {\n\tcase RADEON_VA_MAP:\n\t\tif (bo_va->it.start) {\n\t\t\targs->operation = RADEON_VA_RESULT_VA_EXIST;\n\t\t\targs->offset = bo_va->it.start * RADEON_GPU_PAGE_SIZE;\n\t\t\tradeon_bo_unreserve(rbo);\n\t\t\tgoto out;\n\t\t}\n\t\tr = radeon_vm_bo_set_addr(rdev, bo_va, args->offset, args->flags);\n\t\tbreak;\n\tcase RADEON_VA_UNMAP:\n\t\tr = radeon_vm_bo_set_addr(rdev, bo_va, 0, 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tif (!r)\n\t\tradeon_gem_va_update_vm(rdev, bo_va);\n\targs->operation = RADEON_VA_RESULT_OK;\n\tif (r) {\n\t\targs->operation = RADEON_VA_RESULT_ERROR;\n\t}\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint radeon_gem_op_ioctl(struct drm_device *dev, void *data,\n\t\t\tstruct drm_file *filp)\n{\n\tstruct drm_radeon_gem_op *args = data;\n\tstruct drm_gem_object *gobj;\n\tstruct radeon_bo *robj;\n\tint r;\n\n\tgobj = drm_gem_object_lookup(filp, args->handle);\n\tif (gobj == NULL) {\n\t\treturn -ENOENT;\n\t}\n\trobj = gem_to_radeon_bo(gobj);\n\n\tr = -EPERM;\n\tif (radeon_ttm_tt_has_userptr(robj->rdev, robj->tbo.ttm))\n\t\tgoto out;\n\n\tr = radeon_bo_reserve(robj, false);\n\tif (unlikely(r))\n\t\tgoto out;\n\n\tswitch (args->op) {\n\tcase RADEON_GEM_OP_GET_INITIAL_DOMAIN:\n\t\targs->value = robj->initial_domain;\n\t\tbreak;\n\tcase RADEON_GEM_OP_SET_INITIAL_DOMAIN:\n\t\trobj->initial_domain = args->value & (RADEON_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t      RADEON_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t      RADEON_GEM_DOMAIN_CPU);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t}\n\n\tradeon_bo_unreserve(robj);\nout:\n\tdrm_gem_object_put(gobj);\n\treturn r;\n}\n\nint radeon_align_pitch(struct radeon_device *rdev, int width, int cpp, bool tiled)\n{\n\tint aligned = width;\n\tint align_large = (ASIC_IS_AVIVO(rdev)) || tiled;\n\tint pitch_mask = 0;\n\n\tswitch (cpp) {\n\tcase 1:\n\t\tpitch_mask = align_large ? 255 : 127;\n\t\tbreak;\n\tcase 2:\n\t\tpitch_mask = align_large ? 127 : 31;\n\t\tbreak;\n\tcase 3:\n\tcase 4:\n\t\tpitch_mask = align_large ? 63 : 15;\n\t\tbreak;\n\t}\n\n\taligned += pitch_mask;\n\taligned &= ~pitch_mask;\n\treturn aligned * cpp;\n}\n\nint radeon_mode_dumb_create(struct drm_file *file_priv,\n\t\t\t    struct drm_device *dev,\n\t\t\t    struct drm_mode_create_dumb *args)\n{\n\tstruct radeon_device *rdev = dev->dev_private;\n\tstruct drm_gem_object *gobj;\n\tuint32_t handle;\n\tint r;\n\n\targs->pitch = radeon_align_pitch(rdev, args->width,\n\t\t\t\t\t DIV_ROUND_UP(args->bpp, 8), 0);\n\targs->size = (u64)args->pitch * args->height;\n\targs->size = ALIGN(args->size, PAGE_SIZE);\n\n\tr = radeon_gem_object_create(rdev, args->size, 0,\n\t\t\t\t     RADEON_GEM_DOMAIN_VRAM, 0,\n\t\t\t\t     false, &gobj);\n\tif (r)\n\t\treturn -ENOMEM;\n\n\tr = drm_gem_handle_create(file_priv, gobj, &handle);\n\t \n\tdrm_gem_object_put(gobj);\n\tif (r) {\n\t\treturn r;\n\t}\n\targs->handle = handle;\n\treturn 0;\n}\n\n#if defined(CONFIG_DEBUG_FS)\nstatic int radeon_debugfs_gem_info_show(struct seq_file *m, void *unused)\n{\n\tstruct radeon_device *rdev = m->private;\n\tstruct radeon_bo *rbo;\n\tunsigned i = 0;\n\n\tmutex_lock(&rdev->gem.mutex);\n\tlist_for_each_entry(rbo, &rdev->gem.objects, list) {\n\t\tunsigned domain;\n\t\tconst char *placement;\n\n\t\tdomain = radeon_mem_type_to_domain(rbo->tbo.resource->mem_type);\n\t\tswitch (domain) {\n\t\tcase RADEON_GEM_DOMAIN_VRAM:\n\t\t\tplacement = \"VRAM\";\n\t\t\tbreak;\n\t\tcase RADEON_GEM_DOMAIN_GTT:\n\t\t\tplacement = \" GTT\";\n\t\t\tbreak;\n\t\tcase RADEON_GEM_DOMAIN_CPU:\n\t\tdefault:\n\t\t\tplacement = \" CPU\";\n\t\t\tbreak;\n\t\t}\n\t\tseq_printf(m, \"bo[0x%08x] %8ldkB %8ldMB %s pid %8ld\\n\",\n\t\t\t   i, radeon_bo_size(rbo) >> 10, radeon_bo_size(rbo) >> 20,\n\t\t\t   placement, (unsigned long)rbo->pid);\n\t\ti++;\n\t}\n\tmutex_unlock(&rdev->gem.mutex);\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(radeon_debugfs_gem_info);\n#endif\n\nvoid radeon_gem_debugfs_init(struct radeon_device *rdev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct dentry *root = rdev->ddev->primary->debugfs_root;\n\n\tdebugfs_create_file(\"radeon_gem_info\", 0444, root, rdev,\n\t\t\t    &radeon_debugfs_gem_info_fops);\n\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}