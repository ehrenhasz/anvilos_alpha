{
  "module_name": "radeon_object.c",
  "hash_id": "d42ea4137833601bd5dd19d470fe7c53bcf9acf15ecb0c63090b1387cc09157a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/radeon/radeon_object.c",
  "human_readable_source": " \n \n\n#include <linux/io.h>\n#include <linux/list.h>\n#include <linux/slab.h>\n\n#include <drm/drm_cache.h>\n#include <drm/drm_prime.h>\n#include <drm/radeon_drm.h>\n\n#include \"radeon.h\"\n#include \"radeon_trace.h\"\n#include \"radeon_ttm.h\"\n\nstatic void radeon_bo_clear_surface_reg(struct radeon_bo *bo);\n\n \n\nstatic void radeon_ttm_bo_destroy(struct ttm_buffer_object *tbo)\n{\n\tstruct radeon_bo *bo;\n\n\tbo = container_of(tbo, struct radeon_bo, tbo);\n\n\tmutex_lock(&bo->rdev->gem.mutex);\n\tlist_del_init(&bo->list);\n\tmutex_unlock(&bo->rdev->gem.mutex);\n\tradeon_bo_clear_surface_reg(bo);\n\tWARN_ON_ONCE(!list_empty(&bo->va));\n\tif (bo->tbo.base.import_attach)\n\t\tdrm_prime_gem_destroy(&bo->tbo.base, bo->tbo.sg);\n\tdrm_gem_object_release(&bo->tbo.base);\n\tkfree(bo);\n}\n\nbool radeon_ttm_bo_is_radeon_bo(struct ttm_buffer_object *bo)\n{\n\tif (bo->destroy == &radeon_ttm_bo_destroy)\n\t\treturn true;\n\treturn false;\n}\n\nvoid radeon_ttm_placement_from_domain(struct radeon_bo *rbo, u32 domain)\n{\n\tu32 c = 0, i;\n\n\trbo->placement.placement = rbo->placements;\n\trbo->placement.busy_placement = rbo->placements;\n\tif (domain & RADEON_GEM_DOMAIN_VRAM) {\n\t\t \n\t\tif ((rbo->flags & RADEON_GEM_NO_CPU_ACCESS) &&\n\t\t    rbo->rdev->mc.visible_vram_size < rbo->rdev->mc.real_vram_size) {\n\t\t\trbo->placements[c].fpfn =\n\t\t\t\trbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\n\t\t\trbo->placements[c].mem_type = TTM_PL_VRAM;\n\t\t\trbo->placements[c++].flags = 0;\n\t\t}\n\n\t\trbo->placements[c].fpfn = 0;\n\t\trbo->placements[c].mem_type = TTM_PL_VRAM;\n\t\trbo->placements[c++].flags = 0;\n\t}\n\n\tif (domain & RADEON_GEM_DOMAIN_GTT) {\n\t\trbo->placements[c].fpfn = 0;\n\t\trbo->placements[c].mem_type = TTM_PL_TT;\n\t\trbo->placements[c++].flags = 0;\n\t}\n\n\tif (domain & RADEON_GEM_DOMAIN_CPU) {\n\t\trbo->placements[c].fpfn = 0;\n\t\trbo->placements[c].mem_type = TTM_PL_SYSTEM;\n\t\trbo->placements[c++].flags = 0;\n\t}\n\tif (!c) {\n\t\trbo->placements[c].fpfn = 0;\n\t\trbo->placements[c].mem_type = TTM_PL_SYSTEM;\n\t\trbo->placements[c++].flags = 0;\n\t}\n\n\trbo->placement.num_placement = c;\n\trbo->placement.num_busy_placement = c;\n\n\tfor (i = 0; i < c; ++i) {\n\t\tif ((rbo->flags & RADEON_GEM_CPU_ACCESS) &&\n\t\t    (rbo->placements[i].mem_type == TTM_PL_VRAM) &&\n\t\t    !rbo->placements[i].fpfn)\n\t\t\trbo->placements[i].lpfn =\n\t\t\t\trbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\n\t\telse\n\t\t\trbo->placements[i].lpfn = 0;\n\t}\n}\n\nint radeon_bo_create(struct radeon_device *rdev,\n\t\t     unsigned long size, int byte_align, bool kernel,\n\t\t     u32 domain, u32 flags, struct sg_table *sg,\n\t\t     struct dma_resv *resv,\n\t\t     struct radeon_bo **bo_ptr)\n{\n\tstruct radeon_bo *bo;\n\tenum ttm_bo_type type;\n\tunsigned long page_align = roundup(byte_align, PAGE_SIZE) >> PAGE_SHIFT;\n\tint r;\n\n\tsize = ALIGN(size, PAGE_SIZE);\n\n\tif (kernel) {\n\t\ttype = ttm_bo_type_kernel;\n\t} else if (sg) {\n\t\ttype = ttm_bo_type_sg;\n\t} else {\n\t\ttype = ttm_bo_type_device;\n\t}\n\t*bo_ptr = NULL;\n\n\tbo = kzalloc(sizeof(struct radeon_bo), GFP_KERNEL);\n\tif (bo == NULL)\n\t\treturn -ENOMEM;\n\tdrm_gem_private_object_init(rdev->ddev, &bo->tbo.base, size);\n\tbo->rdev = rdev;\n\tbo->surface_reg = -1;\n\tINIT_LIST_HEAD(&bo->list);\n\tINIT_LIST_HEAD(&bo->va);\n\tbo->initial_domain = domain & (RADEON_GEM_DOMAIN_VRAM |\n\t\t\t\t       RADEON_GEM_DOMAIN_GTT |\n\t\t\t\t       RADEON_GEM_DOMAIN_CPU);\n\n\tbo->flags = flags;\n\t \n\tif (!(rdev->flags & RADEON_IS_PCIE))\n\t\tbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\n\n\t \n\tif (rdev->family >= CHIP_RV610 && rdev->family <= CHIP_RV635)\n\t\tbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\n\n#ifdef CONFIG_X86_32\n\t \n\tbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\n#elif defined(CONFIG_X86) && !defined(CONFIG_X86_PAT)\n\t \n#ifndef CONFIG_COMPILE_TEST\n#warning Please enable CONFIG_MTRR and CONFIG_X86_PAT for better performance \\\n\t thanks to write-combining\n#endif\n\n\tif (bo->flags & RADEON_GEM_GTT_WC)\n\t\tDRM_INFO_ONCE(\"Please enable CONFIG_MTRR and CONFIG_X86_PAT for \"\n\t\t\t      \"better performance thanks to write-combining\\n\");\n\tbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\n#else\n\t \n\tif (!drm_arch_can_wc_memory())\n\t\tbo->flags &= ~RADEON_GEM_GTT_WC;\n#endif\n\n\tradeon_ttm_placement_from_domain(bo, domain);\n\t \n\tdown_read(&rdev->pm.mclk_lock);\n\tr = ttm_bo_init_validate(&rdev->mman.bdev, &bo->tbo, type,\n\t\t\t\t &bo->placement, page_align, !kernel, sg, resv,\n\t\t\t\t &radeon_ttm_bo_destroy);\n\tup_read(&rdev->pm.mclk_lock);\n\tif (unlikely(r != 0)) {\n\t\treturn r;\n\t}\n\t*bo_ptr = bo;\n\n\ttrace_radeon_bo_create(bo);\n\n\treturn 0;\n}\n\nint radeon_bo_kmap(struct radeon_bo *bo, void **ptr)\n{\n\tbool is_iomem;\n\tlong r;\n\n\tr = dma_resv_wait_timeout(bo->tbo.base.resv, DMA_RESV_USAGE_KERNEL,\n\t\t\t\t  false, MAX_SCHEDULE_TIMEOUT);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (bo->kptr) {\n\t\tif (ptr) {\n\t\t\t*ptr = bo->kptr;\n\t\t}\n\t\treturn 0;\n\t}\n\tr = ttm_bo_kmap(&bo->tbo, 0, PFN_UP(bo->tbo.base.size), &bo->kmap);\n\tif (r) {\n\t\treturn r;\n\t}\n\tbo->kptr = ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\n\tif (ptr) {\n\t\t*ptr = bo->kptr;\n\t}\n\tradeon_bo_check_tiling(bo, 0, 0);\n\treturn 0;\n}\n\nvoid radeon_bo_kunmap(struct radeon_bo *bo)\n{\n\tif (bo->kptr == NULL)\n\t\treturn;\n\tbo->kptr = NULL;\n\tradeon_bo_check_tiling(bo, 0, 0);\n\tttm_bo_kunmap(&bo->kmap);\n}\n\nstruct radeon_bo *radeon_bo_ref(struct radeon_bo *bo)\n{\n\tif (bo == NULL)\n\t\treturn NULL;\n\n\tttm_bo_get(&bo->tbo);\n\treturn bo;\n}\n\nvoid radeon_bo_unref(struct radeon_bo **bo)\n{\n\tstruct ttm_buffer_object *tbo;\n\n\tif ((*bo) == NULL)\n\t\treturn;\n\ttbo = &((*bo)->tbo);\n\tttm_bo_put(tbo);\n\t*bo = NULL;\n}\n\nint radeon_bo_pin_restricted(struct radeon_bo *bo, u32 domain, u64 max_offset,\n\t\t\t     u64 *gpu_addr)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint r, i;\n\n\tif (radeon_ttm_tt_has_userptr(bo->rdev, bo->tbo.ttm))\n\t\treturn -EPERM;\n\n\tif (bo->tbo.pin_count) {\n\t\tttm_bo_pin(&bo->tbo);\n\t\tif (gpu_addr)\n\t\t\t*gpu_addr = radeon_bo_gpu_offset(bo);\n\n\t\tif (max_offset != 0) {\n\t\t\tu64 domain_start;\n\n\t\t\tif (domain == RADEON_GEM_DOMAIN_VRAM)\n\t\t\t\tdomain_start = bo->rdev->mc.vram_start;\n\t\t\telse\n\t\t\t\tdomain_start = bo->rdev->mc.gtt_start;\n\t\t\tWARN_ON_ONCE(max_offset <\n\t\t\t\t     (radeon_bo_gpu_offset(bo) - domain_start));\n\t\t}\n\n\t\treturn 0;\n\t}\n\tif (bo->prime_shared_count && domain == RADEON_GEM_DOMAIN_VRAM) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tradeon_ttm_placement_from_domain(bo, domain);\n\tfor (i = 0; i < bo->placement.num_placement; i++) {\n\t\t \n\t\tif ((bo->placements[i].mem_type == TTM_PL_VRAM) &&\n\t\t    !(bo->flags & RADEON_GEM_NO_CPU_ACCESS) &&\n\t\t    (!max_offset || max_offset > bo->rdev->mc.visible_vram_size))\n\t\t\tbo->placements[i].lpfn =\n\t\t\t\tbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\n\t\telse\n\t\t\tbo->placements[i].lpfn = max_offset >> PAGE_SHIFT;\n\t}\n\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (likely(r == 0)) {\n\t\tttm_bo_pin(&bo->tbo);\n\t\tif (gpu_addr != NULL)\n\t\t\t*gpu_addr = radeon_bo_gpu_offset(bo);\n\t\tif (domain == RADEON_GEM_DOMAIN_VRAM)\n\t\t\tbo->rdev->vram_pin_size += radeon_bo_size(bo);\n\t\telse\n\t\t\tbo->rdev->gart_pin_size += radeon_bo_size(bo);\n\t} else {\n\t\tdev_err(bo->rdev->dev, \"%p pin failed\\n\", bo);\n\t}\n\treturn r;\n}\n\nint radeon_bo_pin(struct radeon_bo *bo, u32 domain, u64 *gpu_addr)\n{\n\treturn radeon_bo_pin_restricted(bo, domain, 0, gpu_addr);\n}\n\nvoid radeon_bo_unpin(struct radeon_bo *bo)\n{\n\tttm_bo_unpin(&bo->tbo);\n\tif (!bo->tbo.pin_count) {\n\t\tif (bo->tbo.resource->mem_type == TTM_PL_VRAM)\n\t\t\tbo->rdev->vram_pin_size -= radeon_bo_size(bo);\n\t\telse\n\t\t\tbo->rdev->gart_pin_size -= radeon_bo_size(bo);\n\t}\n}\n\nint radeon_bo_evict_vram(struct radeon_device *rdev)\n{\n\tstruct ttm_device *bdev = &rdev->mman.bdev;\n\tstruct ttm_resource_manager *man;\n\n\t \n#ifndef CONFIG_HIBERNATION\n\tif (rdev->flags & RADEON_IS_IGP) {\n\t\tif (rdev->mc.igp_sideport_enabled == false)\n\t\t\t \n\t\t\treturn 0;\n\t}\n#endif\n\tman = ttm_manager_type(bdev, TTM_PL_VRAM);\n\tif (!man)\n\t\treturn 0;\n\treturn ttm_resource_manager_evict_all(bdev, man);\n}\n\nvoid radeon_bo_force_delete(struct radeon_device *rdev)\n{\n\tstruct radeon_bo *bo, *n;\n\n\tif (list_empty(&rdev->gem.objects)) {\n\t\treturn;\n\t}\n\tdev_err(rdev->dev, \"Userspace still has active objects !\\n\");\n\tlist_for_each_entry_safe(bo, n, &rdev->gem.objects, list) {\n\t\tdev_err(rdev->dev, \"%p %p %lu %lu force free\\n\",\n\t\t\t&bo->tbo.base, bo, (unsigned long)bo->tbo.base.size,\n\t\t\t*((unsigned long *)&bo->tbo.base.refcount));\n\t\tmutex_lock(&bo->rdev->gem.mutex);\n\t\tlist_del_init(&bo->list);\n\t\tmutex_unlock(&bo->rdev->gem.mutex);\n\t\t \n\t\tdrm_gem_object_put(&bo->tbo.base);\n\t}\n}\n\nint radeon_bo_init(struct radeon_device *rdev)\n{\n\t \n\tarch_io_reserve_memtype_wc(rdev->mc.aper_base,\n\t\t\t\t   rdev->mc.aper_size);\n\n\t \n\tif (!rdev->fastfb_working) {\n\t\trdev->mc.vram_mtrr = arch_phys_wc_add(rdev->mc.aper_base,\n\t\t\t\t\t\t      rdev->mc.aper_size);\n\t}\n\tDRM_INFO(\"Detected VRAM RAM=%lluM, BAR=%lluM\\n\",\n\t\trdev->mc.mc_vram_size >> 20,\n\t\t(unsigned long long)rdev->mc.aper_size >> 20);\n\tDRM_INFO(\"RAM width %dbits %cDR\\n\",\n\t\t\trdev->mc.vram_width, rdev->mc.vram_is_ddr ? 'D' : 'S');\n\treturn radeon_ttm_init(rdev);\n}\n\nvoid radeon_bo_fini(struct radeon_device *rdev)\n{\n\tradeon_ttm_fini(rdev);\n\tarch_phys_wc_del(rdev->mc.vram_mtrr);\n\tarch_io_free_memtype_wc(rdev->mc.aper_base, rdev->mc.aper_size);\n}\n\n \nstatic u64 radeon_bo_get_threshold_for_moves(struct radeon_device *rdev)\n{\n\tu64 real_vram_size = rdev->mc.real_vram_size;\n\tstruct ttm_resource_manager *man =\n\t\tttm_manager_type(&rdev->mman.bdev, TTM_PL_VRAM);\n\tu64 vram_usage = ttm_resource_manager_usage(man);\n\n\t \n\n\tu64 half_vram = real_vram_size >> 1;\n\tu64 half_free_vram = vram_usage >= half_vram ? 0 : half_vram - vram_usage;\n\tu64 bytes_moved_threshold = half_free_vram >> 1;\n\treturn max(bytes_moved_threshold, 1024*1024ull);\n}\n\nint radeon_bo_list_validate(struct radeon_device *rdev,\n\t\t\t    struct ww_acquire_ctx *ticket,\n\t\t\t    struct list_head *head, int ring)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct radeon_bo_list *lobj;\n\tstruct list_head duplicates;\n\tint r;\n\tu64 bytes_moved = 0, initial_bytes_moved;\n\tu64 bytes_moved_threshold = radeon_bo_get_threshold_for_moves(rdev);\n\n\tINIT_LIST_HEAD(&duplicates);\n\tr = ttm_eu_reserve_buffers(ticket, head, true, &duplicates);\n\tif (unlikely(r != 0)) {\n\t\treturn r;\n\t}\n\n\tlist_for_each_entry(lobj, head, tv.head) {\n\t\tstruct radeon_bo *bo = lobj->robj;\n\t\tif (!bo->tbo.pin_count) {\n\t\t\tu32 domain = lobj->preferred_domains;\n\t\t\tu32 allowed = lobj->allowed_domains;\n\t\t\tu32 current_domain =\n\t\t\t\tradeon_mem_type_to_domain(bo->tbo.resource->mem_type);\n\n\t\t\t \n\t\t\tif ((allowed & current_domain) != 0 &&\n\t\t\t    (domain & current_domain) == 0 &&  \n\t\t\t    bytes_moved > bytes_moved_threshold) {\n\t\t\t\t \n\t\t\t\tdomain = current_domain;\n\t\t\t}\n\n\t\tretry:\n\t\t\tradeon_ttm_placement_from_domain(bo, domain);\n\t\t\tif (ring == R600_RING_TYPE_UVD_INDEX)\n\t\t\t\tradeon_uvd_force_into_uvd_segment(bo, allowed);\n\n\t\t\tinitial_bytes_moved = atomic64_read(&rdev->num_bytes_moved);\n\t\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tbytes_moved += atomic64_read(&rdev->num_bytes_moved) -\n\t\t\t\t       initial_bytes_moved;\n\n\t\t\tif (unlikely(r)) {\n\t\t\t\tif (r != -ERESTARTSYS &&\n\t\t\t\t    domain != lobj->allowed_domains) {\n\t\t\t\t\tdomain = lobj->allowed_domains;\n\t\t\t\t\tgoto retry;\n\t\t\t\t}\n\t\t\t\tttm_eu_backoff_reservation(ticket, head);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tlobj->gpu_offset = radeon_bo_gpu_offset(bo);\n\t\tlobj->tiling_flags = bo->tiling_flags;\n\t}\n\n\tlist_for_each_entry(lobj, &duplicates, tv.head) {\n\t\tlobj->gpu_offset = radeon_bo_gpu_offset(lobj->robj);\n\t\tlobj->tiling_flags = lobj->robj->tiling_flags;\n\t}\n\n\treturn 0;\n}\n\nint radeon_bo_get_surface_reg(struct radeon_bo *bo)\n{\n\tstruct radeon_device *rdev = bo->rdev;\n\tstruct radeon_surface_reg *reg;\n\tstruct radeon_bo *old_object;\n\tint steal;\n\tint i;\n\n\tdma_resv_assert_held(bo->tbo.base.resv);\n\n\tif (!bo->tiling_flags)\n\t\treturn 0;\n\n\tif (bo->surface_reg >= 0) {\n\t\ti = bo->surface_reg;\n\t\tgoto out;\n\t}\n\n\tsteal = -1;\n\tfor (i = 0; i < RADEON_GEM_MAX_SURFACES; i++) {\n\n\t\treg = &rdev->surface_regs[i];\n\t\tif (!reg->bo)\n\t\t\tbreak;\n\n\t\told_object = reg->bo;\n\t\tif (old_object->tbo.pin_count == 0)\n\t\t\tsteal = i;\n\t}\n\n\t \n\tif (i == RADEON_GEM_MAX_SURFACES) {\n\t\tif (steal == -1)\n\t\t\treturn -ENOMEM;\n\t\t \n\t\treg = &rdev->surface_regs[steal];\n\t\told_object = reg->bo;\n\t\t \n\t\tDRM_DEBUG(\"stealing surface reg %d from %p\\n\", steal, old_object);\n\t\tttm_bo_unmap_virtual(&old_object->tbo);\n\t\told_object->surface_reg = -1;\n\t\ti = steal;\n\t}\n\n\tbo->surface_reg = i;\n\treg->bo = bo;\n\nout:\n\tradeon_set_surface_reg(rdev, i, bo->tiling_flags, bo->pitch,\n\t\t\t       bo->tbo.resource->start << PAGE_SHIFT,\n\t\t\t       bo->tbo.base.size);\n\treturn 0;\n}\n\nstatic void radeon_bo_clear_surface_reg(struct radeon_bo *bo)\n{\n\tstruct radeon_device *rdev = bo->rdev;\n\tstruct radeon_surface_reg *reg;\n\n\tif (bo->surface_reg == -1)\n\t\treturn;\n\n\treg = &rdev->surface_regs[bo->surface_reg];\n\tradeon_clear_surface_reg(rdev, bo->surface_reg);\n\n\treg->bo = NULL;\n\tbo->surface_reg = -1;\n}\n\nint radeon_bo_set_tiling_flags(struct radeon_bo *bo,\n\t\t\t\tuint32_t tiling_flags, uint32_t pitch)\n{\n\tstruct radeon_device *rdev = bo->rdev;\n\tint r;\n\n\tif (rdev->family >= CHIP_CEDAR) {\n\t\tunsigned bankw, bankh, mtaspect, tilesplit, stilesplit;\n\n\t\tbankw = (tiling_flags >> RADEON_TILING_EG_BANKW_SHIFT) & RADEON_TILING_EG_BANKW_MASK;\n\t\tbankh = (tiling_flags >> RADEON_TILING_EG_BANKH_SHIFT) & RADEON_TILING_EG_BANKH_MASK;\n\t\tmtaspect = (tiling_flags >> RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT) & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK;\n\t\ttilesplit = (tiling_flags >> RADEON_TILING_EG_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_TILE_SPLIT_MASK;\n\t\tstilesplit = (tiling_flags >> RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK;\n\t\tswitch (bankw) {\n\t\tcase 0:\n\t\tcase 1:\n\t\tcase 2:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tswitch (bankh) {\n\t\tcase 0:\n\t\tcase 1:\n\t\tcase 2:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tswitch (mtaspect) {\n\t\tcase 0:\n\t\tcase 1:\n\t\tcase 2:\n\t\tcase 4:\n\t\tcase 8:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (tilesplit > 6) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (stilesplit > 6) {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tr = radeon_bo_reserve(bo, false);\n\tif (unlikely(r != 0))\n\t\treturn r;\n\tbo->tiling_flags = tiling_flags;\n\tbo->pitch = pitch;\n\tradeon_bo_unreserve(bo);\n\treturn 0;\n}\n\nvoid radeon_bo_get_tiling_flags(struct radeon_bo *bo,\n\t\t\t\tuint32_t *tiling_flags,\n\t\t\t\tuint32_t *pitch)\n{\n\tdma_resv_assert_held(bo->tbo.base.resv);\n\n\tif (tiling_flags)\n\t\t*tiling_flags = bo->tiling_flags;\n\tif (pitch)\n\t\t*pitch = bo->pitch;\n}\n\nint radeon_bo_check_tiling(struct radeon_bo *bo, bool has_moved,\n\t\t\t\tbool force_drop)\n{\n\tif (!force_drop)\n\t\tdma_resv_assert_held(bo->tbo.base.resv);\n\n\tif (!(bo->tiling_flags & RADEON_TILING_SURFACE))\n\t\treturn 0;\n\n\tif (force_drop) {\n\t\tradeon_bo_clear_surface_reg(bo);\n\t\treturn 0;\n\t}\n\n\tif (bo->tbo.resource->mem_type != TTM_PL_VRAM) {\n\t\tif (!has_moved)\n\t\t\treturn 0;\n\n\t\tif (bo->surface_reg >= 0)\n\t\t\tradeon_bo_clear_surface_reg(bo);\n\t\treturn 0;\n\t}\n\n\tif ((bo->surface_reg >= 0) && !has_moved)\n\t\treturn 0;\n\n\treturn radeon_bo_get_surface_reg(bo);\n}\n\nvoid radeon_bo_move_notify(struct ttm_buffer_object *bo)\n{\n\tstruct radeon_bo *rbo;\n\n\tif (!radeon_ttm_bo_is_radeon_bo(bo))\n\t\treturn;\n\n\trbo = container_of(bo, struct radeon_bo, tbo);\n\tradeon_bo_check_tiling(rbo, 0, 1);\n\tradeon_vm_bo_invalidate(rbo->rdev, rbo);\n}\n\nvm_fault_t radeon_bo_fault_reserve_notify(struct ttm_buffer_object *bo)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct radeon_device *rdev;\n\tstruct radeon_bo *rbo;\n\tunsigned long offset, size, lpfn;\n\tint i, r;\n\n\tif (!radeon_ttm_bo_is_radeon_bo(bo))\n\t\treturn 0;\n\trbo = container_of(bo, struct radeon_bo, tbo);\n\tradeon_bo_check_tiling(rbo, 0, 0);\n\trdev = rbo->rdev;\n\tif (bo->resource->mem_type != TTM_PL_VRAM)\n\t\treturn 0;\n\n\tsize = bo->resource->size;\n\toffset = bo->resource->start << PAGE_SHIFT;\n\tif ((offset + size) <= rdev->mc.visible_vram_size)\n\t\treturn 0;\n\n\t \n\tif (rbo->tbo.pin_count > 0)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tradeon_ttm_placement_from_domain(rbo, RADEON_GEM_DOMAIN_VRAM);\n\tlpfn =\trdev->mc.visible_vram_size >> PAGE_SHIFT;\n\tfor (i = 0; i < rbo->placement.num_placement; i++) {\n\t\t \n\t\tif ((rbo->placements[i].mem_type == TTM_PL_VRAM) &&\n\t\t    (!rbo->placements[i].lpfn || rbo->placements[i].lpfn > lpfn))\n\t\t\trbo->placements[i].lpfn = lpfn;\n\t}\n\tr = ttm_bo_validate(bo, &rbo->placement, &ctx);\n\tif (unlikely(r == -ENOMEM)) {\n\t\tradeon_ttm_placement_from_domain(rbo, RADEON_GEM_DOMAIN_GTT);\n\t\tr = ttm_bo_validate(bo, &rbo->placement, &ctx);\n\t} else if (likely(!r)) {\n\t\toffset = bo->resource->start << PAGE_SHIFT;\n\t\t \n\t\tif ((offset + size) > rdev->mc.visible_vram_size)\n\t\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tif (unlikely(r == -EBUSY || r == -ERESTARTSYS))\n\t\treturn VM_FAULT_NOPAGE;\n\telse if (unlikely(r))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tttm_bo_move_to_lru_tail_unlocked(bo);\n\treturn 0;\n}\n\n \nvoid radeon_bo_fence(struct radeon_bo *bo, struct radeon_fence *fence,\n\t\t     bool shared)\n{\n\tstruct dma_resv *resv = bo->tbo.base.resv;\n\tint r;\n\n\tr = dma_resv_reserve_fences(resv, 1);\n\tif (r) {\n\t\t \n\t\tdma_fence_wait(&fence->base, false);\n\t\treturn;\n\t}\n\n\tdma_resv_add_fence(resv, &fence->base, shared ?\n\t\t\t   DMA_RESV_USAGE_READ : DMA_RESV_USAGE_WRITE);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}