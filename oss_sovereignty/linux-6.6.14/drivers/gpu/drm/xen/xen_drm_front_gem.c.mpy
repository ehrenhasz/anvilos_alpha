{
  "module_name": "xen_drm_front_gem.c",
  "hash_id": "7b5b07134b043913e1a925ea232a50289fc9072955e045009f057b3ef0f70199",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/xen/xen_drm_front_gem.c",
  "human_readable_source": "\n\n \n\n#include <linux/dma-buf.h>\n#include <linux/scatterlist.h>\n#include <linux/shmem_fs.h>\n\n#include <drm/drm_gem.h>\n#include <drm/drm_prime.h>\n#include <drm/drm_probe_helper.h>\n\n#include <xen/balloon.h>\n#include <xen/xen.h>\n\n#include \"xen_drm_front.h\"\n#include \"xen_drm_front_gem.h\"\n\nstruct xen_gem_object {\n\tstruct drm_gem_object base;\n\n\tsize_t num_pages;\n\tstruct page **pages;\n\n\t \n\tbool be_alloc;\n\n\t \n\tstruct sg_table *sgt_imported;\n};\n\nstatic inline struct xen_gem_object *\nto_xen_gem_obj(struct drm_gem_object *gem_obj)\n{\n\treturn container_of(gem_obj, struct xen_gem_object, base);\n}\n\nstatic int gem_alloc_pages_array(struct xen_gem_object *xen_obj,\n\t\t\t\t size_t buf_size)\n{\n\txen_obj->num_pages = DIV_ROUND_UP(buf_size, PAGE_SIZE);\n\txen_obj->pages = kvmalloc_array(xen_obj->num_pages,\n\t\t\t\t\tsizeof(struct page *), GFP_KERNEL);\n\treturn !xen_obj->pages ? -ENOMEM : 0;\n}\n\nstatic void gem_free_pages_array(struct xen_gem_object *xen_obj)\n{\n\tkvfree(xen_obj->pages);\n\txen_obj->pages = NULL;\n}\n\nstatic int xen_drm_front_gem_object_mmap(struct drm_gem_object *gem_obj,\n\t\t\t\t\t struct vm_area_struct *vma)\n{\n\tstruct xen_gem_object *xen_obj = to_xen_gem_obj(gem_obj);\n\tint ret;\n\n\tvma->vm_ops = gem_obj->funcs->vm_ops;\n\n\t \n\tvm_flags_mod(vma, VM_MIXEDMAP | VM_DONTEXPAND, VM_PFNMAP);\n\tvma->vm_pgoff = 0;\n\n\t \n\tvma->vm_page_prot = vm_get_page_prot(vma->vm_flags);\n\n\t \n\tret = vm_map_pages(vma, xen_obj->pages, xen_obj->num_pages);\n\tif (ret < 0)\n\t\tDRM_ERROR(\"Failed to map pages into vma: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct xen_drm_drv_vm_ops = {\n\t.open           = drm_gem_vm_open,\n\t.close          = drm_gem_vm_close,\n};\n\nstatic const struct drm_gem_object_funcs xen_drm_front_gem_object_funcs = {\n\t.free = xen_drm_front_gem_object_free,\n\t.get_sg_table = xen_drm_front_gem_get_sg_table,\n\t.vmap = xen_drm_front_gem_prime_vmap,\n\t.vunmap = xen_drm_front_gem_prime_vunmap,\n\t.mmap = xen_drm_front_gem_object_mmap,\n\t.vm_ops = &xen_drm_drv_vm_ops,\n};\n\nstatic struct xen_gem_object *gem_create_obj(struct drm_device *dev,\n\t\t\t\t\t     size_t size)\n{\n\tstruct xen_gem_object *xen_obj;\n\tint ret;\n\n\txen_obj = kzalloc(sizeof(*xen_obj), GFP_KERNEL);\n\tif (!xen_obj)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\txen_obj->base.funcs = &xen_drm_front_gem_object_funcs;\n\n\tret = drm_gem_object_init(dev, &xen_obj->base, size);\n\tif (ret < 0) {\n\t\tkfree(xen_obj);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn xen_obj;\n}\n\nstatic struct xen_gem_object *gem_create(struct drm_device *dev, size_t size)\n{\n\tstruct xen_drm_front_drm_info *drm_info = dev->dev_private;\n\tstruct xen_gem_object *xen_obj;\n\tint ret;\n\n\tsize = round_up(size, PAGE_SIZE);\n\txen_obj = gem_create_obj(dev, size);\n\tif (IS_ERR(xen_obj))\n\t\treturn xen_obj;\n\n\tif (drm_info->front_info->cfg.be_alloc) {\n\t\t \n\t\tret = gem_alloc_pages_array(xen_obj, size);\n\t\tif (ret < 0)\n\t\t\tgoto fail;\n\n\t\t \n\t\tret = xen_alloc_unpopulated_pages(xen_obj->num_pages,\n\t\t\t\t\t          xen_obj->pages);\n\t\tif (ret < 0) {\n\t\t\tDRM_ERROR(\"Cannot allocate %zu ballooned pages: %d\\n\",\n\t\t\t\t  xen_obj->num_pages, ret);\n\t\t\tgem_free_pages_array(xen_obj);\n\t\t\tgoto fail;\n\t\t}\n\n\t\txen_obj->be_alloc = true;\n\t\treturn xen_obj;\n\t}\n\t \n\txen_obj->num_pages = DIV_ROUND_UP(size, PAGE_SIZE);\n\txen_obj->pages = drm_gem_get_pages(&xen_obj->base);\n\tif (IS_ERR(xen_obj->pages)) {\n\t\tret = PTR_ERR(xen_obj->pages);\n\t\txen_obj->pages = NULL;\n\t\tgoto fail;\n\t}\n\n\treturn xen_obj;\n\nfail:\n\tDRM_ERROR(\"Failed to allocate buffer with size %zu\\n\", size);\n\treturn ERR_PTR(ret);\n}\n\nstruct drm_gem_object *xen_drm_front_gem_create(struct drm_device *dev,\n\t\t\t\t\t\tsize_t size)\n{\n\tstruct xen_gem_object *xen_obj;\n\n\txen_obj = gem_create(dev, size);\n\tif (IS_ERR(xen_obj))\n\t\treturn ERR_CAST(xen_obj);\n\n\treturn &xen_obj->base;\n}\n\nvoid xen_drm_front_gem_free_object_unlocked(struct drm_gem_object *gem_obj)\n{\n\tstruct xen_gem_object *xen_obj = to_xen_gem_obj(gem_obj);\n\n\tif (xen_obj->base.import_attach) {\n\t\tdrm_prime_gem_destroy(&xen_obj->base, xen_obj->sgt_imported);\n\t\tgem_free_pages_array(xen_obj);\n\t} else {\n\t\tif (xen_obj->pages) {\n\t\t\tif (xen_obj->be_alloc) {\n\t\t\t\txen_free_unpopulated_pages(xen_obj->num_pages,\n\t\t\t\t\t\t\t   xen_obj->pages);\n\t\t\t\tgem_free_pages_array(xen_obj);\n\t\t\t} else {\n\t\t\t\tdrm_gem_put_pages(&xen_obj->base,\n\t\t\t\t\t\t  xen_obj->pages, true, false);\n\t\t\t}\n\t\t}\n\t}\n\tdrm_gem_object_release(gem_obj);\n\tkfree(xen_obj);\n}\n\nstruct page **xen_drm_front_gem_get_pages(struct drm_gem_object *gem_obj)\n{\n\tstruct xen_gem_object *xen_obj = to_xen_gem_obj(gem_obj);\n\n\treturn xen_obj->pages;\n}\n\nstruct sg_table *xen_drm_front_gem_get_sg_table(struct drm_gem_object *gem_obj)\n{\n\tstruct xen_gem_object *xen_obj = to_xen_gem_obj(gem_obj);\n\n\tif (!xen_obj->pages)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn drm_prime_pages_to_sg(gem_obj->dev,\n\t\t\t\t     xen_obj->pages, xen_obj->num_pages);\n}\n\nstruct drm_gem_object *\nxen_drm_front_gem_import_sg_table(struct drm_device *dev,\n\t\t\t\t  struct dma_buf_attachment *attach,\n\t\t\t\t  struct sg_table *sgt)\n{\n\tstruct xen_drm_front_drm_info *drm_info = dev->dev_private;\n\tstruct xen_gem_object *xen_obj;\n\tsize_t size;\n\tint ret;\n\n\tsize = attach->dmabuf->size;\n\txen_obj = gem_create_obj(dev, size);\n\tif (IS_ERR(xen_obj))\n\t\treturn ERR_CAST(xen_obj);\n\n\tret = gem_alloc_pages_array(xen_obj, size);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\txen_obj->sgt_imported = sgt;\n\n\tret = drm_prime_sg_to_page_array(sgt, xen_obj->pages,\n\t\t\t\t\t xen_obj->num_pages);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tret = xen_drm_front_dbuf_create(drm_info->front_info,\n\t\t\t\t\txen_drm_front_dbuf_to_cookie(&xen_obj->base),\n\t\t\t\t\t0, 0, 0, size, sgt->sgl->offset,\n\t\t\t\t\txen_obj->pages);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tDRM_DEBUG(\"Imported buffer of size %zu with nents %u\\n\",\n\t\t  size, sgt->orig_nents);\n\n\treturn &xen_obj->base;\n}\n\nint xen_drm_front_gem_prime_vmap(struct drm_gem_object *gem_obj,\n\t\t\t\t struct iosys_map *map)\n{\n\tstruct xen_gem_object *xen_obj = to_xen_gem_obj(gem_obj);\n\tvoid *vaddr;\n\n\tif (!xen_obj->pages)\n\t\treturn -ENOMEM;\n\n\t \n\tvaddr = vmap(xen_obj->pages, xen_obj->num_pages,\n\t\t     VM_MAP, PAGE_KERNEL);\n\tif (!vaddr)\n\t\treturn -ENOMEM;\n\tiosys_map_set_vaddr(map, vaddr);\n\n\treturn 0;\n}\n\nvoid xen_drm_front_gem_prime_vunmap(struct drm_gem_object *gem_obj,\n\t\t\t\t    struct iosys_map *map)\n{\n\tvunmap(map->vaddr);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}