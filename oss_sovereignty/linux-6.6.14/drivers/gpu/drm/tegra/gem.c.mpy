{
  "module_name": "gem.c",
  "hash_id": "225aa72a4b3b5c5d609023dba93bf30e62d0620be3ecf34d0503aea35dfb3f0f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/tegra/gem.c",
  "human_readable_source": "\n \n\n#include <linux/dma-buf.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n\n#include <drm/drm_drv.h>\n#include <drm/drm_prime.h>\n#include <drm/tegra_drm.h>\n\n#include \"drm.h\"\n#include \"gem.h\"\n\nMODULE_IMPORT_NS(DMA_BUF);\n\nstatic unsigned int sg_dma_count_chunks(struct scatterlist *sgl, unsigned int nents)\n{\n\tdma_addr_t next = ~(dma_addr_t)0;\n\tunsigned int count = 0, i;\n\tstruct scatterlist *s;\n\n\tfor_each_sg(sgl, s, nents, i) {\n\t\t \n\t\tif (!sg_dma_len(s))\n\t\t\tcontinue;\n\n\t\tif (sg_dma_address(s) != next) {\n\t\t\tnext = sg_dma_address(s) + sg_dma_len(s);\n\t\t\tcount++;\n\t\t}\n\t}\n\n\treturn count;\n}\n\nstatic inline unsigned int sgt_dma_count_chunks(struct sg_table *sgt)\n{\n\treturn sg_dma_count_chunks(sgt->sgl, sgt->nents);\n}\n\nstatic void tegra_bo_put(struct host1x_bo *bo)\n{\n\tstruct tegra_bo *obj = host1x_to_tegra_bo(bo);\n\n\tdrm_gem_object_put(&obj->gem);\n}\n\nstatic struct host1x_bo_mapping *tegra_bo_pin(struct device *dev, struct host1x_bo *bo,\n\t\t\t\t\t      enum dma_data_direction direction)\n{\n\tstruct tegra_bo *obj = host1x_to_tegra_bo(bo);\n\tstruct drm_gem_object *gem = &obj->gem;\n\tstruct host1x_bo_mapping *map;\n\tint err;\n\n\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\tif (!map)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tkref_init(&map->ref);\n\tmap->bo = host1x_bo_get(bo);\n\tmap->direction = direction;\n\tmap->dev = dev;\n\n\t \n\tif (gem->import_attach) {\n\t\tstruct dma_buf *buf = gem->import_attach->dmabuf;\n\n\t\tmap->attach = dma_buf_attach(buf, dev);\n\t\tif (IS_ERR(map->attach)) {\n\t\t\terr = PTR_ERR(map->attach);\n\t\t\tgoto free;\n\t\t}\n\n\t\tmap->sgt = dma_buf_map_attachment_unlocked(map->attach, direction);\n\t\tif (IS_ERR(map->sgt)) {\n\t\t\tdma_buf_detach(buf, map->attach);\n\t\t\terr = PTR_ERR(map->sgt);\n\t\t\tmap->sgt = NULL;\n\t\t\tgoto free;\n\t\t}\n\n\t\terr = sgt_dma_count_chunks(map->sgt);\n\t\tmap->size = gem->size;\n\n\t\tgoto out;\n\t}\n\n\t \n\tmap->sgt = kzalloc(sizeof(*map->sgt), GFP_KERNEL);\n\tif (!map->sgt) {\n\t\terr = -ENOMEM;\n\t\tgoto free;\n\t}\n\n\tif (obj->pages) {\n\t\t \n\t\terr = sg_alloc_table_from_pages(map->sgt, obj->pages, obj->num_pages, 0, gem->size,\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (err < 0)\n\t\t\tgoto free;\n\t} else {\n\t\t \n\t\terr = dma_get_sgtable(dev, map->sgt, obj->vaddr, obj->iova, gem->size);\n\t\tif (err < 0)\n\t\t\tgoto free;\n\t}\n\n\terr = dma_map_sgtable(dev, map->sgt, direction, 0);\n\tif (err)\n\t\tgoto free_sgt;\n\nout:\n\t \n\tif (!obj->mm) {\n\t\tmap->phys = sg_dma_address(map->sgt->sgl);\n\t\tmap->chunks = err;\n\t} else {\n\t\tmap->phys = obj->iova;\n\t\tmap->chunks = 1;\n\t}\n\n\tmap->size = gem->size;\n\n\treturn map;\n\nfree_sgt:\n\tsg_free_table(map->sgt);\nfree:\n\tkfree(map->sgt);\n\tkfree(map);\n\treturn ERR_PTR(err);\n}\n\nstatic void tegra_bo_unpin(struct host1x_bo_mapping *map)\n{\n\tif (map->attach) {\n\t\tdma_buf_unmap_attachment_unlocked(map->attach, map->sgt,\n\t\t\t\t\t\t  map->direction);\n\t\tdma_buf_detach(map->attach->dmabuf, map->attach);\n\t} else {\n\t\tdma_unmap_sgtable(map->dev, map->sgt, map->direction, 0);\n\t\tsg_free_table(map->sgt);\n\t\tkfree(map->sgt);\n\t}\n\n\thost1x_bo_put(map->bo);\n\tkfree(map);\n}\n\nstatic void *tegra_bo_mmap(struct host1x_bo *bo)\n{\n\tstruct tegra_bo *obj = host1x_to_tegra_bo(bo);\n\tstruct iosys_map map;\n\tint ret;\n\n\tif (obj->vaddr) {\n\t\treturn obj->vaddr;\n\t} else if (obj->gem.import_attach) {\n\t\tret = dma_buf_vmap_unlocked(obj->gem.import_attach->dmabuf, &map);\n\t\treturn ret ? NULL : map.vaddr;\n\t} else {\n\t\treturn vmap(obj->pages, obj->num_pages, VM_MAP,\n\t\t\t    pgprot_writecombine(PAGE_KERNEL));\n\t}\n}\n\nstatic void tegra_bo_munmap(struct host1x_bo *bo, void *addr)\n{\n\tstruct tegra_bo *obj = host1x_to_tegra_bo(bo);\n\tstruct iosys_map map = IOSYS_MAP_INIT_VADDR(addr);\n\n\tif (obj->vaddr)\n\t\treturn;\n\telse if (obj->gem.import_attach)\n\t\tdma_buf_vunmap_unlocked(obj->gem.import_attach->dmabuf, &map);\n\telse\n\t\tvunmap(addr);\n}\n\nstatic struct host1x_bo *tegra_bo_get(struct host1x_bo *bo)\n{\n\tstruct tegra_bo *obj = host1x_to_tegra_bo(bo);\n\n\tdrm_gem_object_get(&obj->gem);\n\n\treturn bo;\n}\n\nstatic const struct host1x_bo_ops tegra_bo_ops = {\n\t.get = tegra_bo_get,\n\t.put = tegra_bo_put,\n\t.pin = tegra_bo_pin,\n\t.unpin = tegra_bo_unpin,\n\t.mmap = tegra_bo_mmap,\n\t.munmap = tegra_bo_munmap,\n};\n\nstatic int tegra_bo_iommu_map(struct tegra_drm *tegra, struct tegra_bo *bo)\n{\n\tint prot = IOMMU_READ | IOMMU_WRITE;\n\tint err;\n\n\tif (bo->mm)\n\t\treturn -EBUSY;\n\n\tbo->mm = kzalloc(sizeof(*bo->mm), GFP_KERNEL);\n\tif (!bo->mm)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&tegra->mm_lock);\n\n\terr = drm_mm_insert_node_generic(&tegra->mm,\n\t\t\t\t\t bo->mm, bo->gem.size, PAGE_SIZE, 0, 0);\n\tif (err < 0) {\n\t\tdev_err(tegra->drm->dev, \"out of I/O virtual memory: %d\\n\",\n\t\t\terr);\n\t\tgoto unlock;\n\t}\n\n\tbo->iova = bo->mm->start;\n\n\tbo->size = iommu_map_sgtable(tegra->domain, bo->iova, bo->sgt, prot);\n\tif (!bo->size) {\n\t\tdev_err(tegra->drm->dev, \"failed to map buffer\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto remove;\n\t}\n\n\tmutex_unlock(&tegra->mm_lock);\n\n\treturn 0;\n\nremove:\n\tdrm_mm_remove_node(bo->mm);\nunlock:\n\tmutex_unlock(&tegra->mm_lock);\n\tkfree(bo->mm);\n\treturn err;\n}\n\nstatic int tegra_bo_iommu_unmap(struct tegra_drm *tegra, struct tegra_bo *bo)\n{\n\tif (!bo->mm)\n\t\treturn 0;\n\n\tmutex_lock(&tegra->mm_lock);\n\tiommu_unmap(tegra->domain, bo->iova, bo->size);\n\tdrm_mm_remove_node(bo->mm);\n\tmutex_unlock(&tegra->mm_lock);\n\n\tkfree(bo->mm);\n\n\treturn 0;\n}\n\nstatic const struct drm_gem_object_funcs tegra_gem_object_funcs = {\n\t.free = tegra_bo_free_object,\n\t.export = tegra_gem_prime_export,\n\t.vm_ops = &tegra_bo_vm_ops,\n};\n\nstatic struct tegra_bo *tegra_bo_alloc_object(struct drm_device *drm,\n\t\t\t\t\t      size_t size)\n{\n\tstruct tegra_bo *bo;\n\tint err;\n\n\tbo = kzalloc(sizeof(*bo), GFP_KERNEL);\n\tif (!bo)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbo->gem.funcs = &tegra_gem_object_funcs;\n\n\thost1x_bo_init(&bo->base, &tegra_bo_ops);\n\tsize = round_up(size, PAGE_SIZE);\n\n\terr = drm_gem_object_init(drm, &bo->gem, size);\n\tif (err < 0)\n\t\tgoto free;\n\n\terr = drm_gem_create_mmap_offset(&bo->gem);\n\tif (err < 0)\n\t\tgoto release;\n\n\treturn bo;\n\nrelease:\n\tdrm_gem_object_release(&bo->gem);\nfree:\n\tkfree(bo);\n\treturn ERR_PTR(err);\n}\n\nstatic void tegra_bo_free(struct drm_device *drm, struct tegra_bo *bo)\n{\n\tif (bo->pages) {\n\t\tdma_unmap_sgtable(drm->dev, bo->sgt, DMA_FROM_DEVICE, 0);\n\t\tdrm_gem_put_pages(&bo->gem, bo->pages, true, true);\n\t\tsg_free_table(bo->sgt);\n\t\tkfree(bo->sgt);\n\t} else if (bo->vaddr) {\n\t\tdma_free_wc(drm->dev, bo->gem.size, bo->vaddr, bo->iova);\n\t}\n}\n\nstatic int tegra_bo_get_pages(struct drm_device *drm, struct tegra_bo *bo)\n{\n\tint err;\n\n\tbo->pages = drm_gem_get_pages(&bo->gem);\n\tif (IS_ERR(bo->pages))\n\t\treturn PTR_ERR(bo->pages);\n\n\tbo->num_pages = bo->gem.size >> PAGE_SHIFT;\n\n\tbo->sgt = drm_prime_pages_to_sg(bo->gem.dev, bo->pages, bo->num_pages);\n\tif (IS_ERR(bo->sgt)) {\n\t\terr = PTR_ERR(bo->sgt);\n\t\tgoto put_pages;\n\t}\n\n\terr = dma_map_sgtable(drm->dev, bo->sgt, DMA_FROM_DEVICE, 0);\n\tif (err)\n\t\tgoto free_sgt;\n\n\treturn 0;\n\nfree_sgt:\n\tsg_free_table(bo->sgt);\n\tkfree(bo->sgt);\nput_pages:\n\tdrm_gem_put_pages(&bo->gem, bo->pages, false, false);\n\treturn err;\n}\n\nstatic int tegra_bo_alloc(struct drm_device *drm, struct tegra_bo *bo)\n{\n\tstruct tegra_drm *tegra = drm->dev_private;\n\tint err;\n\n\tif (tegra->domain) {\n\t\terr = tegra_bo_get_pages(drm, bo);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = tegra_bo_iommu_map(tegra, bo);\n\t\tif (err < 0) {\n\t\t\ttegra_bo_free(drm, bo);\n\t\t\treturn err;\n\t\t}\n\t} else {\n\t\tsize_t size = bo->gem.size;\n\n\t\tbo->vaddr = dma_alloc_wc(drm->dev, size, &bo->iova,\n\t\t\t\t\t GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!bo->vaddr) {\n\t\t\tdev_err(drm->dev,\n\t\t\t\t\"failed to allocate buffer of size %zu\\n\",\n\t\t\t\tsize);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstruct tegra_bo *tegra_bo_create(struct drm_device *drm, size_t size,\n\t\t\t\t unsigned long flags)\n{\n\tstruct tegra_bo *bo;\n\tint err;\n\n\tbo = tegra_bo_alloc_object(drm, size);\n\tif (IS_ERR(bo))\n\t\treturn bo;\n\n\terr = tegra_bo_alloc(drm, bo);\n\tif (err < 0)\n\t\tgoto release;\n\n\tif (flags & DRM_TEGRA_GEM_CREATE_TILED)\n\t\tbo->tiling.mode = TEGRA_BO_TILING_MODE_TILED;\n\n\tif (flags & DRM_TEGRA_GEM_CREATE_BOTTOM_UP)\n\t\tbo->flags |= TEGRA_BO_BOTTOM_UP;\n\n\treturn bo;\n\nrelease:\n\tdrm_gem_object_release(&bo->gem);\n\tkfree(bo);\n\treturn ERR_PTR(err);\n}\n\nstruct tegra_bo *tegra_bo_create_with_handle(struct drm_file *file,\n\t\t\t\t\t     struct drm_device *drm,\n\t\t\t\t\t     size_t size,\n\t\t\t\t\t     unsigned long flags,\n\t\t\t\t\t     u32 *handle)\n{\n\tstruct tegra_bo *bo;\n\tint err;\n\n\tbo = tegra_bo_create(drm, size, flags);\n\tif (IS_ERR(bo))\n\t\treturn bo;\n\n\terr = drm_gem_handle_create(file, &bo->gem, handle);\n\tif (err) {\n\t\ttegra_bo_free_object(&bo->gem);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tdrm_gem_object_put(&bo->gem);\n\n\treturn bo;\n}\n\nstatic struct tegra_bo *tegra_bo_import(struct drm_device *drm,\n\t\t\t\t\tstruct dma_buf *buf)\n{\n\tstruct tegra_drm *tegra = drm->dev_private;\n\tstruct dma_buf_attachment *attach;\n\tstruct tegra_bo *bo;\n\tint err;\n\n\tbo = tegra_bo_alloc_object(drm, buf->size);\n\tif (IS_ERR(bo))\n\t\treturn bo;\n\n\tattach = dma_buf_attach(buf, drm->dev);\n\tif (IS_ERR(attach)) {\n\t\terr = PTR_ERR(attach);\n\t\tgoto free;\n\t}\n\n\tget_dma_buf(buf);\n\n\tbo->sgt = dma_buf_map_attachment_unlocked(attach, DMA_TO_DEVICE);\n\tif (IS_ERR(bo->sgt)) {\n\t\terr = PTR_ERR(bo->sgt);\n\t\tgoto detach;\n\t}\n\n\tif (tegra->domain) {\n\t\terr = tegra_bo_iommu_map(tegra, bo);\n\t\tif (err < 0)\n\t\t\tgoto detach;\n\t}\n\n\tbo->gem.import_attach = attach;\n\n\treturn bo;\n\ndetach:\n\tif (!IS_ERR_OR_NULL(bo->sgt))\n\t\tdma_buf_unmap_attachment_unlocked(attach, bo->sgt, DMA_TO_DEVICE);\n\n\tdma_buf_detach(buf, attach);\n\tdma_buf_put(buf);\nfree:\n\tdrm_gem_object_release(&bo->gem);\n\tkfree(bo);\n\treturn ERR_PTR(err);\n}\n\nvoid tegra_bo_free_object(struct drm_gem_object *gem)\n{\n\tstruct tegra_drm *tegra = gem->dev->dev_private;\n\tstruct host1x_bo_mapping *mapping, *tmp;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\n\t \n\tlist_for_each_entry_safe(mapping, tmp, &bo->base.mappings, list) {\n\t\tif (mapping->cache)\n\t\t\thost1x_bo_unpin(mapping);\n\t\telse\n\t\t\tdev_err(gem->dev->dev, \"mapping %p stale for device %s\\n\", mapping,\n\t\t\t\tdev_name(mapping->dev));\n\t}\n\n\tif (tegra->domain)\n\t\ttegra_bo_iommu_unmap(tegra, bo);\n\n\tif (gem->import_attach) {\n\t\tdma_buf_unmap_attachment_unlocked(gem->import_attach, bo->sgt,\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\tdrm_prime_gem_destroy(gem, NULL);\n\t} else {\n\t\ttegra_bo_free(gem->dev, bo);\n\t}\n\n\tdrm_gem_object_release(gem);\n\tkfree(bo);\n}\n\nint tegra_bo_dumb_create(struct drm_file *file, struct drm_device *drm,\n\t\t\t struct drm_mode_create_dumb *args)\n{\n\tunsigned int min_pitch = DIV_ROUND_UP(args->width * args->bpp, 8);\n\tstruct tegra_drm *tegra = drm->dev_private;\n\tstruct tegra_bo *bo;\n\n\targs->pitch = round_up(min_pitch, tegra->pitch_align);\n\targs->size = args->pitch * args->height;\n\n\tbo = tegra_bo_create_with_handle(file, drm, args->size, 0,\n\t\t\t\t\t &args->handle);\n\tif (IS_ERR(bo))\n\t\treturn PTR_ERR(bo);\n\n\treturn 0;\n}\n\nstatic vm_fault_t tegra_bo_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct drm_gem_object *gem = vma->vm_private_data;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\tstruct page *page;\n\tpgoff_t offset;\n\n\tif (!bo->pages)\n\t\treturn VM_FAULT_SIGBUS;\n\n\toffset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\n\tpage = bo->pages[offset];\n\n\treturn vmf_insert_page(vma, vmf->address, page);\n}\n\nconst struct vm_operations_struct tegra_bo_vm_ops = {\n\t.fault = tegra_bo_fault,\n\t.open = drm_gem_vm_open,\n\t.close = drm_gem_vm_close,\n};\n\nint __tegra_gem_mmap(struct drm_gem_object *gem, struct vm_area_struct *vma)\n{\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\n\tif (!bo->pages) {\n\t\tunsigned long vm_pgoff = vma->vm_pgoff;\n\t\tint err;\n\n\t\t \n\t\tvm_flags_clear(vma, VM_PFNMAP);\n\t\tvma->vm_pgoff = 0;\n\n\t\terr = dma_mmap_wc(gem->dev->dev, vma, bo->vaddr, bo->iova,\n\t\t\t\t  gem->size);\n\t\tif (err < 0) {\n\t\t\tdrm_gem_vm_close(vma);\n\t\t\treturn err;\n\t\t}\n\n\t\tvma->vm_pgoff = vm_pgoff;\n\t} else {\n\t\tpgprot_t prot = vm_get_page_prot(vma->vm_flags);\n\n\t\tvm_flags_mod(vma, VM_MIXEDMAP, VM_PFNMAP);\n\n\t\tvma->vm_page_prot = pgprot_writecombine(prot);\n\t}\n\n\treturn 0;\n}\n\nint tegra_drm_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct drm_gem_object *gem;\n\tint err;\n\n\terr = drm_gem_mmap(file, vma);\n\tif (err < 0)\n\t\treturn err;\n\n\tgem = vma->vm_private_data;\n\n\treturn __tegra_gem_mmap(gem, vma);\n}\n\nstatic struct sg_table *\ntegra_gem_prime_map_dma_buf(struct dma_buf_attachment *attach,\n\t\t\t    enum dma_data_direction dir)\n{\n\tstruct drm_gem_object *gem = attach->dmabuf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\tstruct sg_table *sgt;\n\n\tsgt = kmalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt)\n\t\treturn NULL;\n\n\tif (bo->pages) {\n\t\tif (sg_alloc_table_from_pages(sgt, bo->pages, bo->num_pages,\n\t\t\t\t\t      0, gem->size, GFP_KERNEL) < 0)\n\t\t\tgoto free;\n\t} else {\n\t\tif (dma_get_sgtable(attach->dev, sgt, bo->vaddr, bo->iova,\n\t\t\t\t    gem->size) < 0)\n\t\t\tgoto free;\n\t}\n\n\tif (dma_map_sgtable(attach->dev, sgt, dir, 0))\n\t\tgoto free;\n\n\treturn sgt;\n\nfree:\n\tsg_free_table(sgt);\n\tkfree(sgt);\n\treturn NULL;\n}\n\nstatic void tegra_gem_prime_unmap_dma_buf(struct dma_buf_attachment *attach,\n\t\t\t\t\t  struct sg_table *sgt,\n\t\t\t\t\t  enum dma_data_direction dir)\n{\n\tstruct drm_gem_object *gem = attach->dmabuf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\n\tif (bo->pages)\n\t\tdma_unmap_sgtable(attach->dev, sgt, dir, 0);\n\n\tsg_free_table(sgt);\n\tkfree(sgt);\n}\n\nstatic void tegra_gem_prime_release(struct dma_buf *buf)\n{\n\tdrm_gem_dmabuf_release(buf);\n}\n\nstatic int tegra_gem_prime_begin_cpu_access(struct dma_buf *buf,\n\t\t\t\t\t    enum dma_data_direction direction)\n{\n\tstruct drm_gem_object *gem = buf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\tstruct drm_device *drm = gem->dev;\n\n\tif (bo->pages)\n\t\tdma_sync_sgtable_for_cpu(drm->dev, bo->sgt, DMA_FROM_DEVICE);\n\n\treturn 0;\n}\n\nstatic int tegra_gem_prime_end_cpu_access(struct dma_buf *buf,\n\t\t\t\t\t  enum dma_data_direction direction)\n{\n\tstruct drm_gem_object *gem = buf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\tstruct drm_device *drm = gem->dev;\n\n\tif (bo->pages)\n\t\tdma_sync_sgtable_for_device(drm->dev, bo->sgt, DMA_TO_DEVICE);\n\n\treturn 0;\n}\n\nstatic int tegra_gem_prime_mmap(struct dma_buf *buf, struct vm_area_struct *vma)\n{\n\tstruct drm_gem_object *gem = buf->priv;\n\tint err;\n\n\terr = drm_gem_mmap_obj(gem, gem->size, vma);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn __tegra_gem_mmap(gem, vma);\n}\n\nstatic int tegra_gem_prime_vmap(struct dma_buf *buf, struct iosys_map *map)\n{\n\tstruct drm_gem_object *gem = buf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\tvoid *vaddr;\n\n\tvaddr = tegra_bo_mmap(&bo->base);\n\tif (IS_ERR(vaddr))\n\t\treturn PTR_ERR(vaddr);\n\n\tiosys_map_set_vaddr(map, vaddr);\n\n\treturn 0;\n}\n\nstatic void tegra_gem_prime_vunmap(struct dma_buf *buf, struct iosys_map *map)\n{\n\tstruct drm_gem_object *gem = buf->priv;\n\tstruct tegra_bo *bo = to_tegra_bo(gem);\n\n\ttegra_bo_munmap(&bo->base, map->vaddr);\n}\n\nstatic const struct dma_buf_ops tegra_gem_prime_dmabuf_ops = {\n\t.map_dma_buf = tegra_gem_prime_map_dma_buf,\n\t.unmap_dma_buf = tegra_gem_prime_unmap_dma_buf,\n\t.release = tegra_gem_prime_release,\n\t.begin_cpu_access = tegra_gem_prime_begin_cpu_access,\n\t.end_cpu_access = tegra_gem_prime_end_cpu_access,\n\t.mmap = tegra_gem_prime_mmap,\n\t.vmap = tegra_gem_prime_vmap,\n\t.vunmap = tegra_gem_prime_vunmap,\n};\n\nstruct dma_buf *tegra_gem_prime_export(struct drm_gem_object *gem,\n\t\t\t\t       int flags)\n{\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\texp_info.exp_name = KBUILD_MODNAME;\n\texp_info.owner = gem->dev->driver->fops->owner;\n\texp_info.ops = &tegra_gem_prime_dmabuf_ops;\n\texp_info.size = gem->size;\n\texp_info.flags = flags;\n\texp_info.priv = gem;\n\n\treturn drm_gem_dmabuf_export(gem->dev, &exp_info);\n}\n\nstruct drm_gem_object *tegra_gem_prime_import(struct drm_device *drm,\n\t\t\t\t\t      struct dma_buf *buf)\n{\n\tstruct tegra_bo *bo;\n\n\tif (buf->ops == &tegra_gem_prime_dmabuf_ops) {\n\t\tstruct drm_gem_object *gem = buf->priv;\n\n\t\tif (gem->dev == drm) {\n\t\t\tdrm_gem_object_get(gem);\n\t\t\treturn gem;\n\t\t}\n\t}\n\n\tbo = tegra_bo_import(drm, buf);\n\tif (IS_ERR(bo))\n\t\treturn ERR_CAST(bo);\n\n\treturn &bo->gem;\n}\n\nstruct host1x_bo *tegra_gem_lookup(struct drm_file *file, u32 handle)\n{\n\tstruct drm_gem_object *gem;\n\tstruct tegra_bo *bo;\n\n\tgem = drm_gem_object_lookup(file, handle);\n\tif (!gem)\n\t\treturn NULL;\n\n\tbo = to_tegra_bo(gem);\n\treturn &bo->base;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}