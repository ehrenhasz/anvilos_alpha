{
  "module_name": "omap_dmm_tiler.c",
  "hash_id": "4a65b3a52703080fb2a278611fd07925039d0346ac4c195f3bf60189834ef39c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/omapdrm/omap_dmm_tiler.c",
  "human_readable_source": "\n \n\n#include <linux/completion.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmaengine.h>\n#include <linux/errno.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>  \n#include <linux/sched.h>\n#include <linux/seq_file.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/vmalloc.h>\n#include <linux/wait.h>\n\n#include \"omap_dmm_tiler.h\"\n#include \"omap_dmm_priv.h\"\n\n#define DMM_DRIVER_NAME \"dmm\"\n\n \nstatic struct tcm *containers[TILFMT_NFORMATS];\nstatic struct dmm *omap_dmm;\n\n#if defined(CONFIG_OF)\nstatic const struct of_device_id dmm_of_match[];\n#endif\n\n \nstatic DEFINE_SPINLOCK(list_lock);\n\n \n#define GEOM(xshift, yshift, bytes_per_pixel) { \\\n\t\t.x_shft = (xshift), \\\n\t\t.y_shft = (yshift), \\\n\t\t.cpp    = (bytes_per_pixel), \\\n\t\t.slot_w = 1 << (SLOT_WIDTH_BITS - (xshift)), \\\n\t\t.slot_h = 1 << (SLOT_HEIGHT_BITS - (yshift)), \\\n\t}\n\nstatic const struct {\n\tu32 x_shft;\t \n\tu32 y_shft;\t \n\tu32 cpp;\t\t \n\tu32 slot_w;\t \n\tu32 slot_h;\t \n} geom[TILFMT_NFORMATS] = {\n\t[TILFMT_8BIT]  = GEOM(0, 0, 1),\n\t[TILFMT_16BIT] = GEOM(0, 1, 2),\n\t[TILFMT_32BIT] = GEOM(1, 1, 4),\n\t[TILFMT_PAGE]  = GEOM(SLOT_WIDTH_BITS, SLOT_HEIGHT_BITS, 1),\n};\n\n\n \nstatic const u32 reg[][4] = {\n\t[PAT_STATUS] = {DMM_PAT_STATUS__0, DMM_PAT_STATUS__1,\n\t\t\tDMM_PAT_STATUS__2, DMM_PAT_STATUS__3},\n\t[PAT_DESCR]  = {DMM_PAT_DESCR__0, DMM_PAT_DESCR__1,\n\t\t\tDMM_PAT_DESCR__2, DMM_PAT_DESCR__3},\n};\n\nstatic int dmm_dma_copy(struct dmm *dmm, dma_addr_t src, dma_addr_t dst)\n{\n\tstruct dma_async_tx_descriptor *tx;\n\tenum dma_status status;\n\tdma_cookie_t cookie;\n\n\ttx = dmaengine_prep_dma_memcpy(dmm->wa_dma_chan, dst, src, 4, 0);\n\tif (!tx) {\n\t\tdev_err(dmm->dev, \"Failed to prepare DMA memcpy\\n\");\n\t\treturn -EIO;\n\t}\n\n\tcookie = tx->tx_submit(tx);\n\tif (dma_submit_error(cookie)) {\n\t\tdev_err(dmm->dev, \"Failed to do DMA tx_submit\\n\");\n\t\treturn -EIO;\n\t}\n\n\tstatus = dma_sync_wait(dmm->wa_dma_chan, cookie);\n\tif (status != DMA_COMPLETE)\n\t\tdev_err(dmm->dev, \"i878 wa DMA copy failure\\n\");\n\n\tdmaengine_terminate_all(dmm->wa_dma_chan);\n\treturn 0;\n}\n\nstatic u32 dmm_read_wa(struct dmm *dmm, u32 reg)\n{\n\tdma_addr_t src, dst;\n\tint r;\n\n\tsrc = dmm->phys_base + reg;\n\tdst = dmm->wa_dma_handle;\n\n\tr = dmm_dma_copy(dmm, src, dst);\n\tif (r) {\n\t\tdev_err(dmm->dev, \"sDMA read transfer timeout\\n\");\n\t\treturn readl(dmm->base + reg);\n\t}\n\n\t \n\trmb();\n\treturn readl(dmm->wa_dma_data);\n}\n\nstatic void dmm_write_wa(struct dmm *dmm, u32 val, u32 reg)\n{\n\tdma_addr_t src, dst;\n\tint r;\n\n\twritel(val, dmm->wa_dma_data);\n\t \n\twmb();\n\n\tsrc = dmm->wa_dma_handle;\n\tdst = dmm->phys_base + reg;\n\n\tr = dmm_dma_copy(dmm, src, dst);\n\tif (r) {\n\t\tdev_err(dmm->dev, \"sDMA write transfer timeout\\n\");\n\t\twritel(val, dmm->base + reg);\n\t}\n}\n\nstatic u32 dmm_read(struct dmm *dmm, u32 reg)\n{\n\tif (dmm->dmm_workaround) {\n\t\tu32 v;\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&dmm->wa_lock, flags);\n\t\tv = dmm_read_wa(dmm, reg);\n\t\tspin_unlock_irqrestore(&dmm->wa_lock, flags);\n\n\t\treturn v;\n\t} else {\n\t\treturn readl(dmm->base + reg);\n\t}\n}\n\nstatic void dmm_write(struct dmm *dmm, u32 val, u32 reg)\n{\n\tif (dmm->dmm_workaround) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&dmm->wa_lock, flags);\n\t\tdmm_write_wa(dmm, val, reg);\n\t\tspin_unlock_irqrestore(&dmm->wa_lock, flags);\n\t} else {\n\t\twritel(val, dmm->base + reg);\n\t}\n}\n\nstatic int dmm_workaround_init(struct dmm *dmm)\n{\n\tdma_cap_mask_t mask;\n\n\tspin_lock_init(&dmm->wa_lock);\n\n\tdmm->wa_dma_data = dma_alloc_coherent(dmm->dev,  sizeof(u32),\n\t\t\t\t\t      &dmm->wa_dma_handle, GFP_KERNEL);\n\tif (!dmm->wa_dma_data)\n\t\treturn -ENOMEM;\n\n\tdma_cap_zero(mask);\n\tdma_cap_set(DMA_MEMCPY, mask);\n\n\tdmm->wa_dma_chan = dma_request_channel(mask, NULL, NULL);\n\tif (!dmm->wa_dma_chan) {\n\t\tdma_free_coherent(dmm->dev, 4, dmm->wa_dma_data, dmm->wa_dma_handle);\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\nstatic void dmm_workaround_uninit(struct dmm *dmm)\n{\n\tdma_release_channel(dmm->wa_dma_chan);\n\n\tdma_free_coherent(dmm->dev, 4, dmm->wa_dma_data, dmm->wa_dma_handle);\n}\n\n \nstatic void *alloc_dma(struct dmm_txn *txn, size_t sz, dma_addr_t *pa)\n{\n\tvoid *ptr;\n\tstruct refill_engine *engine = txn->engine_handle;\n\n\t \n\ttxn->current_pa = round_up(txn->current_pa, 16);\n\ttxn->current_va = (void *)round_up((long)txn->current_va, 16);\n\n\tptr = txn->current_va;\n\t*pa = txn->current_pa;\n\n\ttxn->current_pa += sz;\n\ttxn->current_va += sz;\n\n\tBUG_ON((txn->current_va - engine->refill_va) > REFILL_BUFFER_SIZE);\n\n\treturn ptr;\n}\n\n \nstatic int wait_status(struct refill_engine *engine, u32 wait_mask)\n{\n\tstruct dmm *dmm = engine->dmm;\n\tu32 r = 0, err, i;\n\n\ti = DMM_FIXED_RETRY_COUNT;\n\twhile (true) {\n\t\tr = dmm_read(dmm, reg[PAT_STATUS][engine->id]);\n\t\terr = r & DMM_PATSTATUS_ERR;\n\t\tif (err) {\n\t\t\tdev_err(dmm->dev,\n\t\t\t\t\"%s: error (engine%d). PAT_STATUS: 0x%08x\\n\",\n\t\t\t\t__func__, engine->id, r);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif ((r & wait_mask) == wait_mask)\n\t\t\tbreak;\n\n\t\tif (--i == 0) {\n\t\t\tdev_err(dmm->dev,\n\t\t\t\t\"%s: timeout (engine%d). PAT_STATUS: 0x%08x\\n\",\n\t\t\t\t__func__, engine->id, r);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\n\t\tudelay(1);\n\t}\n\n\treturn 0;\n}\n\nstatic void release_engine(struct refill_engine *engine)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&list_lock, flags);\n\tlist_add(&engine->idle_node, &omap_dmm->idle_head);\n\tspin_unlock_irqrestore(&list_lock, flags);\n\n\tatomic_inc(&omap_dmm->engine_counter);\n\twake_up_interruptible(&omap_dmm->engine_queue);\n}\n\nstatic irqreturn_t omap_dmm_irq_handler(int irq, void *arg)\n{\n\tstruct dmm *dmm = arg;\n\tu32 status = dmm_read(dmm, DMM_PAT_IRQSTATUS);\n\tint i;\n\n\t \n\tdmm_write(dmm, status, DMM_PAT_IRQSTATUS);\n\n\tfor (i = 0; i < dmm->num_engines; i++) {\n\t\tif (status & DMM_IRQSTAT_ERR_MASK)\n\t\t\tdev_err(dmm->dev,\n\t\t\t\t\"irq error(engine%d): IRQSTAT 0x%02x\\n\",\n\t\t\t\ti, status & 0xff);\n\n\t\tif (status & DMM_IRQSTAT_LST) {\n\t\t\tif (dmm->engines[i].async)\n\t\t\t\trelease_engine(&dmm->engines[i]);\n\n\t\t\tcomplete(&dmm->engines[i].compl);\n\t\t}\n\n\t\tstatus >>= 8;\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic struct dmm_txn *dmm_txn_init(struct dmm *dmm, struct tcm *tcm)\n{\n\tstruct dmm_txn *txn = NULL;\n\tstruct refill_engine *engine = NULL;\n\tint ret;\n\tunsigned long flags;\n\n\n\t \n\tret = wait_event_interruptible(omap_dmm->engine_queue,\n\t\tatomic_add_unless(&omap_dmm->engine_counter, -1, 0));\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t \n\tspin_lock_irqsave(&list_lock, flags);\n\tif (!list_empty(&dmm->idle_head)) {\n\t\tengine = list_entry(dmm->idle_head.next, struct refill_engine,\n\t\t\t\t\tidle_node);\n\t\tlist_del(&engine->idle_node);\n\t}\n\tspin_unlock_irqrestore(&list_lock, flags);\n\n\tBUG_ON(!engine);\n\n\ttxn = &engine->txn;\n\tengine->tcm = tcm;\n\ttxn->engine_handle = engine;\n\ttxn->last_pat = NULL;\n\ttxn->current_va = engine->refill_va;\n\ttxn->current_pa = engine->refill_pa;\n\n\treturn txn;\n}\n\n \nstatic void dmm_txn_append(struct dmm_txn *txn, struct pat_area *area,\n\t\tstruct page **pages, u32 npages, u32 roll)\n{\n\tdma_addr_t pat_pa = 0, data_pa = 0;\n\tu32 *data;\n\tstruct pat *pat;\n\tstruct refill_engine *engine = txn->engine_handle;\n\tint columns = (1 + area->x1 - area->x0);\n\tint rows = (1 + area->y1 - area->y0);\n\tint i = columns*rows;\n\n\tpat = alloc_dma(txn, sizeof(*pat), &pat_pa);\n\n\tif (txn->last_pat)\n\t\ttxn->last_pat->next_pa = (u32)pat_pa;\n\n\tpat->area = *area;\n\n\t \n\tpat->area.y0 += engine->tcm->y_offset;\n\tpat->area.y1 += engine->tcm->y_offset;\n\n\tpat->ctrl = (struct pat_ctrl){\n\t\t\t.start = 1,\n\t\t\t.lut_id = engine->tcm->lut_id,\n\t\t};\n\n\tdata = alloc_dma(txn, 4*i, &data_pa);\n\t \n\tpat->data_pa = data_pa;\n\n\twhile (i--) {\n\t\tint n = i + roll;\n\t\tif (n >= npages)\n\t\t\tn -= npages;\n\t\tdata[i] = (pages && pages[n]) ?\n\t\t\tpage_to_phys(pages[n]) : engine->dmm->dummy_pa;\n\t}\n\n\ttxn->last_pat = pat;\n\n\treturn;\n}\n\n \nstatic int dmm_txn_commit(struct dmm_txn *txn, bool wait)\n{\n\tint ret = 0;\n\tstruct refill_engine *engine = txn->engine_handle;\n\tstruct dmm *dmm = engine->dmm;\n\n\tif (!txn->last_pat) {\n\t\tdev_err(engine->dmm->dev, \"need at least one txn\\n\");\n\t\tret = -EINVAL;\n\t\tgoto cleanup;\n\t}\n\n\ttxn->last_pat->next_pa = 0;\n\t \n\twmb();\n\n\t \n\n\t \n\treadl(&txn->last_pat->next_pa);\n\n\t \n\tdmm_write(dmm, 0x0, reg[PAT_DESCR][engine->id]);\n\n\t \n\tret = wait_status(engine, DMM_PATSTATUS_READY);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto cleanup;\n\t}\n\n\t \n\tengine->async = wait ? false : true;\n\treinit_completion(&engine->compl);\n\t \n\tsmp_mb();\n\n\t \n\tdmm_write(dmm, engine->refill_pa, reg[PAT_DESCR][engine->id]);\n\n\tif (wait) {\n\t\tif (!wait_for_completion_timeout(&engine->compl,\n\t\t\t\tmsecs_to_jiffies(100))) {\n\t\t\tdev_err(dmm->dev, \"timed out waiting for done\\n\");\n\t\t\tret = -ETIMEDOUT;\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t \n\t\tret = wait_status(engine, DMM_PATSTATUS_READY |\n\t\t\t\t  DMM_PATSTATUS_VALID | DMM_PATSTATUS_DONE);\n\t}\n\ncleanup:\n\t \n\tif (ret || wait)\n\t\trelease_engine(engine);\n\n\treturn ret;\n}\n\n \nstatic int fill(struct tcm_area *area, struct page **pages,\n\t\tu32 npages, u32 roll, bool wait)\n{\n\tint ret = 0;\n\tstruct tcm_area slice, area_s;\n\tstruct dmm_txn *txn;\n\n\t \n\n\twait = true;\n\n\ttxn = dmm_txn_init(omap_dmm, area->tcm);\n\tif (IS_ERR_OR_NULL(txn))\n\t\treturn -ENOMEM;\n\n\ttcm_for_each_slice(slice, *area, area_s) {\n\t\tstruct pat_area p_area = {\n\t\t\t\t.x0 = slice.p0.x,  .y0 = slice.p0.y,\n\t\t\t\t.x1 = slice.p1.x,  .y1 = slice.p1.y,\n\t\t};\n\n\t\tdmm_txn_append(txn, &p_area, pages, npages, roll);\n\n\t\troll += tcm_sizeof(slice);\n\t}\n\n\tret = dmm_txn_commit(txn, wait);\n\n\treturn ret;\n}\n\n \n\n \nint tiler_pin(struct tiler_block *block, struct page **pages,\n\t\tu32 npages, u32 roll, bool wait)\n{\n\tint ret;\n\n\tret = fill(&block->area, pages, npages, roll, wait);\n\n\tif (ret)\n\t\ttiler_unpin(block);\n\n\treturn ret;\n}\n\nint tiler_unpin(struct tiler_block *block)\n{\n\treturn fill(&block->area, NULL, 0, 0, false);\n}\n\n \nstruct tiler_block *tiler_reserve_2d(enum tiler_fmt fmt, u16 w,\n\t\tu16 h, u16 align)\n{\n\tstruct tiler_block *block;\n\tu32 min_align = 128;\n\tint ret;\n\tunsigned long flags;\n\tu32 slot_bytes;\n\n\tblock = kzalloc(sizeof(*block), GFP_KERNEL);\n\tif (!block)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tBUG_ON(!validfmt(fmt));\n\n\t \n\tw = DIV_ROUND_UP(w, geom[fmt].slot_w);\n\th = DIV_ROUND_UP(h, geom[fmt].slot_h);\n\n\t \n\tslot_bytes = geom[fmt].slot_w * geom[fmt].cpp;\n\tmin_align = max(min_align, slot_bytes);\n\talign = (align > min_align) ? ALIGN(align, min_align) : min_align;\n\talign /= slot_bytes;\n\n\tblock->fmt = fmt;\n\n\tret = tcm_reserve_2d(containers[fmt], w, h, align, -1, slot_bytes,\n\t\t\t&block->area);\n\tif (ret) {\n\t\tkfree(block);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\tspin_lock_irqsave(&list_lock, flags);\n\tlist_add(&block->alloc_node, &omap_dmm->alloc_head);\n\tspin_unlock_irqrestore(&list_lock, flags);\n\n\treturn block;\n}\n\nstruct tiler_block *tiler_reserve_1d(size_t size)\n{\n\tstruct tiler_block *block = kzalloc(sizeof(*block), GFP_KERNEL);\n\tint num_pages = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tunsigned long flags;\n\n\tif (!block)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tblock->fmt = TILFMT_PAGE;\n\n\tif (tcm_reserve_1d(containers[TILFMT_PAGE], num_pages,\n\t\t\t\t&block->area)) {\n\t\tkfree(block);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tspin_lock_irqsave(&list_lock, flags);\n\tlist_add(&block->alloc_node, &omap_dmm->alloc_head);\n\tspin_unlock_irqrestore(&list_lock, flags);\n\n\treturn block;\n}\n\n \nint tiler_release(struct tiler_block *block)\n{\n\tint ret = tcm_free(&block->area);\n\tunsigned long flags;\n\n\tif (block->area.tcm)\n\t\tdev_err(omap_dmm->dev, \"failed to release block\\n\");\n\n\tspin_lock_irqsave(&list_lock, flags);\n\tlist_del(&block->alloc_node);\n\tspin_unlock_irqrestore(&list_lock, flags);\n\n\tkfree(block);\n\treturn ret;\n}\n\n \n\n \nstatic u32 tiler_get_address(enum tiler_fmt fmt, u32 orient, u32 x, u32 y)\n{\n\tu32 x_bits, y_bits, tmp, x_mask, y_mask, alignment;\n\n\tx_bits = CONT_WIDTH_BITS - geom[fmt].x_shft;\n\ty_bits = CONT_HEIGHT_BITS - geom[fmt].y_shft;\n\talignment = geom[fmt].x_shft + geom[fmt].y_shft;\n\n\t \n\tx_mask = MASK(x_bits);\n\ty_mask = MASK(y_bits);\n\n\tif (x < 0 || x > x_mask || y < 0 || y > y_mask) {\n\t\tDBG(\"invalid coords: %u < 0 || %u > %u || %u < 0 || %u > %u\",\n\t\t\t\tx, x, x_mask, y, y, y_mask);\n\t\treturn 0;\n\t}\n\n\t \n\tif (orient & MASK_X_INVERT)\n\t\tx ^= x_mask;\n\tif (orient & MASK_Y_INVERT)\n\t\ty ^= y_mask;\n\n\t \n\tif (orient & MASK_XY_FLIP)\n\t\ttmp = ((x << y_bits) + y);\n\telse\n\t\ttmp = ((y << x_bits) + x);\n\n\treturn TIL_ADDR((tmp << alignment), orient, fmt);\n}\n\ndma_addr_t tiler_ssptr(struct tiler_block *block)\n{\n\tBUG_ON(!validfmt(block->fmt));\n\n\treturn TILVIEW_8BIT + tiler_get_address(block->fmt, 0,\n\t\t\tblock->area.p0.x * geom[block->fmt].slot_w,\n\t\t\tblock->area.p0.y * geom[block->fmt].slot_h);\n}\n\ndma_addr_t tiler_tsptr(struct tiler_block *block, u32 orient,\n\t\tu32 x, u32 y)\n{\n\tstruct tcm_pt *p = &block->area.p0;\n\tBUG_ON(!validfmt(block->fmt));\n\n\treturn tiler_get_address(block->fmt, orient,\n\t\t\t(p->x * geom[block->fmt].slot_w) + x,\n\t\t\t(p->y * geom[block->fmt].slot_h) + y);\n}\n\nvoid tiler_align(enum tiler_fmt fmt, u16 *w, u16 *h)\n{\n\tBUG_ON(!validfmt(fmt));\n\t*w = round_up(*w, geom[fmt].slot_w);\n\t*h = round_up(*h, geom[fmt].slot_h);\n}\n\nu32 tiler_stride(enum tiler_fmt fmt, u32 orient)\n{\n\tBUG_ON(!validfmt(fmt));\n\n\tif (orient & MASK_XY_FLIP)\n\t\treturn 1 << (CONT_HEIGHT_BITS + geom[fmt].x_shft);\n\telse\n\t\treturn 1 << (CONT_WIDTH_BITS + geom[fmt].y_shft);\n}\n\nsize_t tiler_size(enum tiler_fmt fmt, u16 w, u16 h)\n{\n\ttiler_align(fmt, &w, &h);\n\treturn geom[fmt].cpp * w * h;\n}\n\nsize_t tiler_vsize(enum tiler_fmt fmt, u16 w, u16 h)\n{\n\tBUG_ON(!validfmt(fmt));\n\treturn round_up(geom[fmt].cpp * w, PAGE_SIZE) * h;\n}\n\nu32 tiler_get_cpu_cache_flags(void)\n{\n\treturn omap_dmm->plat_data->cpu_cache_flags;\n}\n\nbool dmm_is_available(void)\n{\n\treturn omap_dmm ? true : false;\n}\n\nstatic void omap_dmm_remove(struct platform_device *dev)\n{\n\tstruct tiler_block *block, *_block;\n\tint i;\n\tunsigned long flags;\n\n\tif (omap_dmm) {\n\t\t \n\t\tdmm_write(omap_dmm, 0x7e7e7e7e, DMM_PAT_IRQENABLE_CLR);\n\t\tfree_irq(omap_dmm->irq, omap_dmm);\n\n\t\t \n\t\tspin_lock_irqsave(&list_lock, flags);\n\t\tlist_for_each_entry_safe(block, _block, &omap_dmm->alloc_head,\n\t\t\t\t\talloc_node) {\n\t\t\tlist_del(&block->alloc_node);\n\t\t\tkfree(block);\n\t\t}\n\t\tspin_unlock_irqrestore(&list_lock, flags);\n\n\t\tfor (i = 0; i < omap_dmm->num_lut; i++)\n\t\t\tif (omap_dmm->tcm && omap_dmm->tcm[i])\n\t\t\t\tomap_dmm->tcm[i]->deinit(omap_dmm->tcm[i]);\n\t\tkfree(omap_dmm->tcm);\n\n\t\tkfree(omap_dmm->engines);\n\t\tif (omap_dmm->refill_va)\n\t\t\tdma_free_wc(omap_dmm->dev,\n\t\t\t\t    REFILL_BUFFER_SIZE * omap_dmm->num_engines,\n\t\t\t\t    omap_dmm->refill_va, omap_dmm->refill_pa);\n\t\tif (omap_dmm->dummy_page)\n\t\t\t__free_page(omap_dmm->dummy_page);\n\n\t\tif (omap_dmm->dmm_workaround)\n\t\t\tdmm_workaround_uninit(omap_dmm);\n\n\t\tiounmap(omap_dmm->base);\n\t\tkfree(omap_dmm);\n\t\tomap_dmm = NULL;\n\t}\n}\n\nstatic int omap_dmm_probe(struct platform_device *dev)\n{\n\tint ret = -EFAULT, i;\n\tstruct tcm_area area = {0};\n\tu32 hwinfo, pat_geom;\n\tstruct resource *mem;\n\n\tomap_dmm = kzalloc(sizeof(*omap_dmm), GFP_KERNEL);\n\tif (!omap_dmm)\n\t\tgoto fail;\n\n\t \n\tINIT_LIST_HEAD(&omap_dmm->alloc_head);\n\tINIT_LIST_HEAD(&omap_dmm->idle_head);\n\n\tinit_waitqueue_head(&omap_dmm->engine_queue);\n\n\tif (dev->dev.of_node) {\n\t\tconst struct of_device_id *match;\n\n\t\tmatch = of_match_node(dmm_of_match, dev->dev.of_node);\n\t\tif (!match) {\n\t\t\tdev_err(&dev->dev, \"failed to find matching device node\\n\");\n\t\t\tret = -ENODEV;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tomap_dmm->plat_data = match->data;\n\t}\n\n\t \n\tmem = platform_get_resource(dev, IORESOURCE_MEM, 0);\n\tif (!mem) {\n\t\tdev_err(&dev->dev, \"failed to get base address resource\\n\");\n\t\tgoto fail;\n\t}\n\n\tomap_dmm->phys_base = mem->start;\n\tomap_dmm->base = ioremap(mem->start, SZ_2K);\n\n\tif (!omap_dmm->base) {\n\t\tdev_err(&dev->dev, \"failed to get dmm base address\\n\");\n\t\tgoto fail;\n\t}\n\n\tomap_dmm->irq = platform_get_irq(dev, 0);\n\tif (omap_dmm->irq < 0)\n\t\tgoto fail;\n\n\tomap_dmm->dev = &dev->dev;\n\n\tif (of_machine_is_compatible(\"ti,dra7\")) {\n\t\t \n\t\tif (!dmm_workaround_init(omap_dmm)) {\n\t\t\tomap_dmm->dmm_workaround = true;\n\t\t\tdev_info(&dev->dev,\n\t\t\t\t\"workaround for errata i878 in use\\n\");\n\t\t} else {\n\t\t\tdev_warn(&dev->dev,\n\t\t\t\t \"failed to initialize work-around for i878\\n\");\n\t\t}\n\t}\n\n\thwinfo = dmm_read(omap_dmm, DMM_PAT_HWINFO);\n\tomap_dmm->num_engines = (hwinfo >> 24) & 0x1F;\n\tomap_dmm->num_lut = (hwinfo >> 16) & 0x1F;\n\tomap_dmm->container_width = 256;\n\tomap_dmm->container_height = 128;\n\n\tatomic_set(&omap_dmm->engine_counter, omap_dmm->num_engines);\n\n\t \n\tpat_geom = dmm_read(omap_dmm, DMM_PAT_GEOMETRY);\n\tomap_dmm->lut_width = ((pat_geom >> 16) & 0xF) << 5;\n\tomap_dmm->lut_height = ((pat_geom >> 24) & 0xF) << 5;\n\n\t \n\t \n\tif (omap_dmm->lut_height != omap_dmm->container_height)\n\t\tomap_dmm->num_lut++;\n\n\t \n\tdmm_write(omap_dmm, 0x88888888, DMM_PAT_VIEW__0);\n\tdmm_write(omap_dmm, 0x88888888, DMM_PAT_VIEW__1);\n\tdmm_write(omap_dmm, 0x80808080, DMM_PAT_VIEW_MAP__0);\n\tdmm_write(omap_dmm, 0x80000000, DMM_PAT_VIEW_MAP_BASE);\n\tdmm_write(omap_dmm, 0x88888888, DMM_TILER_OR__0);\n\tdmm_write(omap_dmm, 0x88888888, DMM_TILER_OR__1);\n\n\tomap_dmm->dummy_page = alloc_page(GFP_KERNEL | __GFP_DMA32);\n\tif (!omap_dmm->dummy_page) {\n\t\tdev_err(&dev->dev, \"could not allocate dummy page\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tret = dma_set_coherent_mask(&dev->dev, DMA_BIT_MASK(32));\n\tif (ret)\n\t\tgoto fail;\n\n\tomap_dmm->dummy_pa = page_to_phys(omap_dmm->dummy_page);\n\n\t \n\tomap_dmm->refill_va = dma_alloc_wc(&dev->dev,\n\t\t\t\t\t   REFILL_BUFFER_SIZE * omap_dmm->num_engines,\n\t\t\t\t\t   &omap_dmm->refill_pa, GFP_KERNEL);\n\tif (!omap_dmm->refill_va) {\n\t\tdev_err(&dev->dev, \"could not allocate refill memory\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tomap_dmm->engines = kcalloc(omap_dmm->num_engines,\n\t\t\t\t    sizeof(*omap_dmm->engines), GFP_KERNEL);\n\tif (!omap_dmm->engines) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tfor (i = 0; i < omap_dmm->num_engines; i++) {\n\t\tomap_dmm->engines[i].id = i;\n\t\tomap_dmm->engines[i].dmm = omap_dmm;\n\t\tomap_dmm->engines[i].refill_va = omap_dmm->refill_va +\n\t\t\t\t\t\t(REFILL_BUFFER_SIZE * i);\n\t\tomap_dmm->engines[i].refill_pa = omap_dmm->refill_pa +\n\t\t\t\t\t\t(REFILL_BUFFER_SIZE * i);\n\t\tinit_completion(&omap_dmm->engines[i].compl);\n\n\t\tlist_add(&omap_dmm->engines[i].idle_node, &omap_dmm->idle_head);\n\t}\n\n\tomap_dmm->tcm = kcalloc(omap_dmm->num_lut, sizeof(*omap_dmm->tcm),\n\t\t\t\tGFP_KERNEL);\n\tif (!omap_dmm->tcm) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\t \n\tfor (i = 0; i < omap_dmm->num_lut; i++) {\n\t\tomap_dmm->tcm[i] = sita_init(omap_dmm->container_width,\n\t\t\t\t\t\tomap_dmm->container_height);\n\n\t\tif (!omap_dmm->tcm[i]) {\n\t\t\tdev_err(&dev->dev, \"failed to allocate container\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tomap_dmm->tcm[i]->lut_id = i;\n\t}\n\n\t \n\t \n\t \n\tcontainers[TILFMT_8BIT] = omap_dmm->tcm[0];\n\tcontainers[TILFMT_16BIT] = omap_dmm->tcm[0];\n\tcontainers[TILFMT_32BIT] = omap_dmm->tcm[0];\n\n\tif (omap_dmm->container_height != omap_dmm->lut_height) {\n\t\t \n\t\tcontainers[TILFMT_PAGE] = omap_dmm->tcm[1];\n\t\tomap_dmm->tcm[1]->y_offset = OMAP5_LUT_OFFSET;\n\t\tomap_dmm->tcm[1]->lut_id = 0;\n\t} else {\n\t\tcontainers[TILFMT_PAGE] = omap_dmm->tcm[0];\n\t}\n\n\tarea = (struct tcm_area) {\n\t\t.tcm = NULL,\n\t\t.p1.x = omap_dmm->container_width - 1,\n\t\t.p1.y = omap_dmm->container_height - 1,\n\t};\n\n\tret = request_irq(omap_dmm->irq, omap_dmm_irq_handler, IRQF_SHARED,\n\t\t\t\t\"omap_dmm_irq_handler\", omap_dmm);\n\n\tif (ret) {\n\t\tdev_err(&dev->dev, \"couldn't register IRQ %d, error %d\\n\",\n\t\t\tomap_dmm->irq, ret);\n\t\tomap_dmm->irq = -1;\n\t\tgoto fail;\n\t}\n\n\t \n\tdmm_write(omap_dmm, 0x7e7e7e7e, DMM_PAT_IRQENABLE_SET);\n\n\t \n\tfor (i = 0; i < omap_dmm->num_lut; i++) {\n\t\tarea.tcm = omap_dmm->tcm[i];\n\t\tif (fill(&area, NULL, 0, 0, true))\n\t\t\tdev_err(omap_dmm->dev, \"refill failed\");\n\t}\n\n\tdev_info(omap_dmm->dev, \"initialized all PAT entries\\n\");\n\n\treturn 0;\n\nfail:\n\tomap_dmm_remove(dev);\n\treturn ret;\n}\n\n \n\n#ifdef CONFIG_DEBUG_FS\n\nstatic const char *alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n\t\t\t\t\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\";\nstatic const char *special = \".,:;'\\\"`~!^-+\";\n\nstatic void fill_map(char **map, int xdiv, int ydiv, struct tcm_area *a,\n\t\t\t\t\t\t\tchar c, bool ovw)\n{\n\tint x, y;\n\tfor (y = a->p0.y / ydiv; y <= a->p1.y / ydiv; y++)\n\t\tfor (x = a->p0.x / xdiv; x <= a->p1.x / xdiv; x++)\n\t\t\tif (map[y][x] == ' ' || ovw)\n\t\t\t\tmap[y][x] = c;\n}\n\nstatic void fill_map_pt(char **map, int xdiv, int ydiv, struct tcm_pt *p,\n\t\t\t\t\t\t\t\t\tchar c)\n{\n\tmap[p->y / ydiv][p->x / xdiv] = c;\n}\n\nstatic char read_map_pt(char **map, int xdiv, int ydiv, struct tcm_pt *p)\n{\n\treturn map[p->y / ydiv][p->x / xdiv];\n}\n\nstatic int map_width(int xdiv, int x0, int x1)\n{\n\treturn (x1 / xdiv) - (x0 / xdiv) + 1;\n}\n\nstatic void text_map(char **map, int xdiv, char *nice, int yd, int x0, int x1)\n{\n\tchar *p = map[yd] + (x0 / xdiv);\n\tint w = (map_width(xdiv, x0, x1) - strlen(nice)) / 2;\n\tif (w >= 0) {\n\t\tp += w;\n\t\twhile (*nice)\n\t\t\t*p++ = *nice++;\n\t}\n}\n\nstatic void map_1d_info(char **map, int xdiv, int ydiv, char *nice,\n\t\t\t\t\t\t\tstruct tcm_area *a)\n{\n\tsprintf(nice, \"%dK\", tcm_sizeof(*a) * 4);\n\tif (a->p0.y + 1 < a->p1.y) {\n\t\ttext_map(map, xdiv, nice, (a->p0.y + a->p1.y) / 2 / ydiv, 0,\n\t\t\t\t\t\t\t256 - 1);\n\t} else if (a->p0.y < a->p1.y) {\n\t\tif (strlen(nice) < map_width(xdiv, a->p0.x, 256 - 1))\n\t\t\ttext_map(map, xdiv, nice, a->p0.y / ydiv,\n\t\t\t\t\ta->p0.x + xdiv,\t256 - 1);\n\t\telse if (strlen(nice) < map_width(xdiv, 0, a->p1.x))\n\t\t\ttext_map(map, xdiv, nice, a->p1.y / ydiv,\n\t\t\t\t\t0, a->p1.y - xdiv);\n\t} else if (strlen(nice) + 1 < map_width(xdiv, a->p0.x, a->p1.x)) {\n\t\ttext_map(map, xdiv, nice, a->p0.y / ydiv, a->p0.x, a->p1.x);\n\t}\n}\n\nstatic void map_2d_info(char **map, int xdiv, int ydiv, char *nice,\n\t\t\t\t\t\t\tstruct tcm_area *a)\n{\n\tsprintf(nice, \"(%d*%d)\", tcm_awidth(*a), tcm_aheight(*a));\n\tif (strlen(nice) + 1 < map_width(xdiv, a->p0.x, a->p1.x))\n\t\ttext_map(map, xdiv, nice, (a->p0.y + a->p1.y) / 2 / ydiv,\n\t\t\t\t\t\t\ta->p0.x, a->p1.x);\n}\n\nint tiler_map_show(struct seq_file *s, void *arg)\n{\n\tint xdiv = 2, ydiv = 1;\n\tchar **map = NULL, *global_map;\n\tstruct tiler_block *block;\n\tstruct tcm_area a, p;\n\tint i;\n\tconst char *m2d = alphabet;\n\tconst char *a2d = special;\n\tconst char *m2dp = m2d, *a2dp = a2d;\n\tchar nice[128];\n\tint h_adj;\n\tint w_adj;\n\tunsigned long flags;\n\tint lut_idx;\n\n\n\tif (!omap_dmm) {\n\t\t \n\t\treturn 0;\n\t}\n\n\th_adj = omap_dmm->container_height / ydiv;\n\tw_adj = omap_dmm->container_width / xdiv;\n\n\tmap = kmalloc_array(h_adj, sizeof(*map), GFP_KERNEL);\n\tglobal_map = kmalloc_array(w_adj + 1, h_adj, GFP_KERNEL);\n\n\tif (!map || !global_map)\n\t\tgoto error;\n\n\tfor (lut_idx = 0; lut_idx < omap_dmm->num_lut; lut_idx++) {\n\t\tmemset(map, 0, h_adj * sizeof(*map));\n\t\tmemset(global_map, ' ', (w_adj + 1) * h_adj);\n\n\t\tfor (i = 0; i < omap_dmm->container_height; i++) {\n\t\t\tmap[i] = global_map + i * (w_adj + 1);\n\t\t\tmap[i][w_adj] = 0;\n\t\t}\n\n\t\tspin_lock_irqsave(&list_lock, flags);\n\n\t\tlist_for_each_entry(block, &omap_dmm->alloc_head, alloc_node) {\n\t\t\tif (block->area.tcm == omap_dmm->tcm[lut_idx]) {\n\t\t\t\tif (block->fmt != TILFMT_PAGE) {\n\t\t\t\t\tfill_map(map, xdiv, ydiv, &block->area,\n\t\t\t\t\t\t*m2dp, true);\n\t\t\t\t\tif (!*++a2dp)\n\t\t\t\t\t\ta2dp = a2d;\n\t\t\t\t\tif (!*++m2dp)\n\t\t\t\t\t\tm2dp = m2d;\n\t\t\t\t\tmap_2d_info(map, xdiv, ydiv, nice,\n\t\t\t\t\t\t\t&block->area);\n\t\t\t\t} else {\n\t\t\t\t\tbool start = read_map_pt(map, xdiv,\n\t\t\t\t\t\tydiv, &block->area.p0) == ' ';\n\t\t\t\t\tbool end = read_map_pt(map, xdiv, ydiv,\n\t\t\t\t\t\t\t&block->area.p1) == ' ';\n\n\t\t\t\t\ttcm_for_each_slice(a, block->area, p)\n\t\t\t\t\t\tfill_map(map, xdiv, ydiv, &a,\n\t\t\t\t\t\t\t'=', true);\n\t\t\t\t\tfill_map_pt(map, xdiv, ydiv,\n\t\t\t\t\t\t\t&block->area.p0,\n\t\t\t\t\t\t\tstart ? '<' : 'X');\n\t\t\t\t\tfill_map_pt(map, xdiv, ydiv,\n\t\t\t\t\t\t\t&block->area.p1,\n\t\t\t\t\t\t\tend ? '>' : 'X');\n\t\t\t\t\tmap_1d_info(map, xdiv, ydiv, nice,\n\t\t\t\t\t\t\t&block->area);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tspin_unlock_irqrestore(&list_lock, flags);\n\n\t\tif (s) {\n\t\t\tseq_printf(s, \"CONTAINER %d DUMP BEGIN\\n\", lut_idx);\n\t\t\tfor (i = 0; i < 128; i++)\n\t\t\t\tseq_printf(s, \"%03d:%s\\n\", i, map[i]);\n\t\t\tseq_printf(s, \"CONTAINER %d DUMP END\\n\", lut_idx);\n\t\t} else {\n\t\t\tdev_dbg(omap_dmm->dev, \"CONTAINER %d DUMP BEGIN\\n\",\n\t\t\t\tlut_idx);\n\t\t\tfor (i = 0; i < 128; i++)\n\t\t\t\tdev_dbg(omap_dmm->dev, \"%03d:%s\\n\", i, map[i]);\n\t\t\tdev_dbg(omap_dmm->dev, \"CONTAINER %d DUMP END\\n\",\n\t\t\t\tlut_idx);\n\t\t}\n\t}\n\nerror:\n\tkfree(map);\n\tkfree(global_map);\n\n\treturn 0;\n}\n#endif\n\n#ifdef CONFIG_PM_SLEEP\nstatic int omap_dmm_resume(struct device *dev)\n{\n\tstruct tcm_area area;\n\tint i;\n\n\tif (!omap_dmm)\n\t\treturn -ENODEV;\n\n\tarea = (struct tcm_area) {\n\t\t.tcm = NULL,\n\t\t.p1.x = omap_dmm->container_width - 1,\n\t\t.p1.y = omap_dmm->container_height - 1,\n\t};\n\n\t \n\tfor (i = 0; i < omap_dmm->num_lut; i++) {\n\t\tarea.tcm = omap_dmm->tcm[i];\n\t\tif (fill(&area, NULL, 0, 0, true))\n\t\t\tdev_err(dev, \"refill failed\");\n\t}\n\n\treturn 0;\n}\n#endif\n\nstatic SIMPLE_DEV_PM_OPS(omap_dmm_pm_ops, NULL, omap_dmm_resume);\n\n#if defined(CONFIG_OF)\nstatic const struct dmm_platform_data dmm_omap4_platform_data = {\n\t.cpu_cache_flags = OMAP_BO_WC,\n};\n\nstatic const struct dmm_platform_data dmm_omap5_platform_data = {\n\t.cpu_cache_flags = OMAP_BO_UNCACHED,\n};\n\nstatic const struct of_device_id dmm_of_match[] = {\n\t{\n\t\t.compatible = \"ti,omap4-dmm\",\n\t\t.data = &dmm_omap4_platform_data,\n\t},\n\t{\n\t\t.compatible = \"ti,omap5-dmm\",\n\t\t.data = &dmm_omap5_platform_data,\n\t},\n\t{},\n};\n#endif\n\nstruct platform_driver omap_dmm_driver = {\n\t.probe = omap_dmm_probe,\n\t.remove_new = omap_dmm_remove,\n\t.driver = {\n\t\t.owner = THIS_MODULE,\n\t\t.name = DMM_DRIVER_NAME,\n\t\t.of_match_table = of_match_ptr(dmm_of_match),\n\t\t.pm = &omap_dmm_pm_ops,\n\t},\n};\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Andy Gross <andy.gross@ti.com>\");\nMODULE_DESCRIPTION(\"OMAP DMM/Tiler Driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}