{
  "module_name": "kfd_packet_manager.c",
  "hash_id": "0425c6e2393b3f243783bfdd1b043111c9975af26139ba006a0acc6b01c36bd3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_packet_manager.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/mutex.h>\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_kernel_queue.h\"\n#include \"kfd_priv.h\"\n\nstatic inline void inc_wptr(unsigned int *wptr, unsigned int increment_bytes,\n\t\t\t\tunsigned int buffer_size_bytes)\n{\n\tunsigned int temp = *wptr + increment_bytes / sizeof(uint32_t);\n\n\tWARN((temp * sizeof(uint32_t)) > buffer_size_bytes,\n\t     \"Runlist IB overflow\");\n\t*wptr = temp;\n}\n\nstatic void pm_calc_rlib_size(struct packet_manager *pm,\n\t\t\t\tunsigned int *rlib_size,\n\t\t\t\tbool *over_subscription)\n{\n\tunsigned int process_count, queue_count, compute_queue_count, gws_queue_count;\n\tunsigned int map_queue_size;\n\tunsigned int max_proc_per_quantum = 1;\n\tstruct kfd_node *dev = pm->dqm->dev;\n\n\tprocess_count = pm->dqm->processes_count;\n\tqueue_count = pm->dqm->active_queue_count;\n\tcompute_queue_count = pm->dqm->active_cp_queue_count;\n\tgws_queue_count = pm->dqm->gws_queue_count;\n\n\t \n\t*over_subscription = false;\n\n\tif (dev->max_proc_per_quantum > 1)\n\t\tmax_proc_per_quantum = dev->max_proc_per_quantum;\n\n\tif ((process_count > max_proc_per_quantum) ||\n\t    compute_queue_count > get_cp_queues_num(pm->dqm) ||\n\t    gws_queue_count > 1) {\n\t\t*over_subscription = true;\n\t\tpr_debug(\"Over subscribed runlist\\n\");\n\t}\n\n\tmap_queue_size = pm->pmf->map_queues_size;\n\t \n\t*rlib_size = process_count * pm->pmf->map_process_size +\n\t\t     queue_count * map_queue_size;\n\n\t \n\tif (*over_subscription)\n\t\t*rlib_size += pm->pmf->runlist_size;\n\n\tpr_debug(\"runlist ib size %d\\n\", *rlib_size);\n}\n\nstatic int pm_allocate_runlist_ib(struct packet_manager *pm,\n\t\t\t\tunsigned int **rl_buffer,\n\t\t\t\tuint64_t *rl_gpu_buffer,\n\t\t\t\tunsigned int *rl_buffer_size,\n\t\t\t\tbool *is_over_subscription)\n{\n\tint retval;\n\n\tif (WARN_ON(pm->allocated))\n\t\treturn -EINVAL;\n\n\tpm_calc_rlib_size(pm, rl_buffer_size, is_over_subscription);\n\n\tmutex_lock(&pm->lock);\n\n\tretval = kfd_gtt_sa_allocate(pm->dqm->dev, *rl_buffer_size,\n\t\t\t\t\t&pm->ib_buffer_obj);\n\n\tif (retval) {\n\t\tpr_err(\"Failed to allocate runlist IB\\n\");\n\t\tgoto out;\n\t}\n\n\t*(void **)rl_buffer = pm->ib_buffer_obj->cpu_ptr;\n\t*rl_gpu_buffer = pm->ib_buffer_obj->gpu_addr;\n\n\tmemset(*rl_buffer, 0, *rl_buffer_size);\n\tpm->allocated = true;\n\nout:\n\tmutex_unlock(&pm->lock);\n\treturn retval;\n}\n\nstatic int pm_create_runlist_ib(struct packet_manager *pm,\n\t\t\t\tstruct list_head *queues,\n\t\t\t\tuint64_t *rl_gpu_addr,\n\t\t\t\tsize_t *rl_size_bytes)\n{\n\tunsigned int alloc_size_bytes;\n\tunsigned int *rl_buffer, rl_wptr, i;\n\tint retval, processes_mapped;\n\tstruct device_process_node *cur;\n\tstruct qcm_process_device *qpd;\n\tstruct queue *q;\n\tstruct kernel_queue *kq;\n\tbool is_over_subscription;\n\n\trl_wptr = retval = processes_mapped = 0;\n\n\tretval = pm_allocate_runlist_ib(pm, &rl_buffer, rl_gpu_addr,\n\t\t\t\t&alloc_size_bytes, &is_over_subscription);\n\tif (retval)\n\t\treturn retval;\n\n\t*rl_size_bytes = alloc_size_bytes;\n\tpm->ib_size_bytes = alloc_size_bytes;\n\n\tpr_debug(\"Building runlist ib process count: %d queues count %d\\n\",\n\t\tpm->dqm->processes_count, pm->dqm->active_queue_count);\n\n\t \n\tlist_for_each_entry(cur, queues, list) {\n\t\tqpd = cur->qpd;\n\t\t \n\t\tif (processes_mapped >= pm->dqm->processes_count) {\n\t\t\tpr_debug(\"Not enough space left in runlist IB\\n\");\n\t\t\tpm_release_ib(pm);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tretval = pm->pmf->map_process(pm, &rl_buffer[rl_wptr], qpd);\n\t\tif (retval)\n\t\t\treturn retval;\n\n\t\tprocesses_mapped++;\n\t\tinc_wptr(&rl_wptr, pm->pmf->map_process_size,\n\t\t\t\talloc_size_bytes);\n\n\t\tlist_for_each_entry(kq, &qpd->priv_queue_list, list) {\n\t\t\tif (!kq->queue->properties.is_active)\n\t\t\t\tcontinue;\n\n\t\t\tpr_debug(\"static_queue, mapping kernel q %d, is debug status %d\\n\",\n\t\t\t\tkq->queue->queue, qpd->is_debug);\n\n\t\t\tretval = pm->pmf->map_queues(pm,\n\t\t\t\t\t\t&rl_buffer[rl_wptr],\n\t\t\t\t\t\tkq->queue,\n\t\t\t\t\t\tqpd->is_debug);\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\n\t\t\tinc_wptr(&rl_wptr,\n\t\t\t\tpm->pmf->map_queues_size,\n\t\t\t\talloc_size_bytes);\n\t\t}\n\n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tif (!q->properties.is_active)\n\t\t\t\tcontinue;\n\n\t\t\tpr_debug(\"static_queue, mapping user queue %d, is debug status %d\\n\",\n\t\t\t\tq->queue, qpd->is_debug);\n\n\t\t\tretval = pm->pmf->map_queues(pm,\n\t\t\t\t\t\t&rl_buffer[rl_wptr],\n\t\t\t\t\t\tq,\n\t\t\t\t\t\tqpd->is_debug);\n\n\t\t\tif (retval)\n\t\t\t\treturn retval;\n\n\t\t\tinc_wptr(&rl_wptr,\n\t\t\t\tpm->pmf->map_queues_size,\n\t\t\t\talloc_size_bytes);\n\t\t}\n\t}\n\n\tpr_debug(\"Finished map process and queues to runlist\\n\");\n\n\tif (is_over_subscription) {\n\t\tif (!pm->is_over_subscription)\n\t\t\tpr_warn(\"Runlist is getting oversubscribed. Expect reduced ROCm performance.\\n\");\n\t\tretval = pm->pmf->runlist(pm, &rl_buffer[rl_wptr],\n\t\t\t\t\t*rl_gpu_addr,\n\t\t\t\t\talloc_size_bytes / sizeof(uint32_t),\n\t\t\t\t\ttrue);\n\t}\n\tpm->is_over_subscription = is_over_subscription;\n\n\tfor (i = 0; i < alloc_size_bytes / sizeof(uint32_t); i++)\n\t\tpr_debug(\"0x%2X \", rl_buffer[i]);\n\tpr_debug(\"\\n\");\n\n\treturn retval;\n}\n\nint pm_init(struct packet_manager *pm, struct device_queue_manager *dqm)\n{\n\tswitch (dqm->dev->adev->asic_type) {\n\tcase CHIP_KAVERI:\n\tcase CHIP_HAWAII:\n\t\t \n\tcase CHIP_CARRIZO:\n\tcase CHIP_TONGA:\n\tcase CHIP_FIJI:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tpm->pmf = &kfd_vi_pm_funcs;\n\t\tbreak;\n\tdefault:\n\t\tif (KFD_GC_VERSION(dqm->dev) == IP_VERSION(9, 4, 2) ||\n\t\t    KFD_GC_VERSION(dqm->dev) == IP_VERSION(9, 4, 3))\n\t\t\tpm->pmf = &kfd_aldebaran_pm_funcs;\n\t\telse if (KFD_GC_VERSION(dqm->dev) >= IP_VERSION(9, 0, 1))\n\t\t\tpm->pmf = &kfd_v9_pm_funcs;\n\t\telse {\n\t\t\tWARN(1, \"Unexpected ASIC family %u\",\n\t\t\t     dqm->dev->adev->asic_type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tpm->dqm = dqm;\n\tmutex_init(&pm->lock);\n\tpm->priv_queue = kernel_queue_init(dqm->dev, KFD_QUEUE_TYPE_HIQ);\n\tif (!pm->priv_queue) {\n\t\tmutex_destroy(&pm->lock);\n\t\treturn -ENOMEM;\n\t}\n\tpm->allocated = false;\n\n\treturn 0;\n}\n\nvoid pm_uninit(struct packet_manager *pm, bool hanging)\n{\n\tmutex_destroy(&pm->lock);\n\tkernel_queue_uninit(pm->priv_queue, hanging);\n\tpm->priv_queue = NULL;\n}\n\nint pm_send_set_resources(struct packet_manager *pm,\n\t\t\t\tstruct scheduling_resources *res)\n{\n\tuint32_t *buffer, size;\n\tint retval = 0;\n\n\tsize = pm->pmf->set_resources_size;\n\tmutex_lock(&pm->lock);\n\tkq_acquire_packet_buffer(pm->priv_queue,\n\t\t\t\t\tsize / sizeof(uint32_t),\n\t\t\t\t\t(unsigned int **)&buffer);\n\tif (!buffer) {\n\t\tpr_err(\"Failed to allocate buffer on kernel queue\\n\");\n\t\tretval = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tretval = pm->pmf->set_resources(pm, buffer, res);\n\tif (!retval)\n\t\tkq_submit_packet(pm->priv_queue);\n\telse\n\t\tkq_rollback_packet(pm->priv_queue);\n\nout:\n\tmutex_unlock(&pm->lock);\n\n\treturn retval;\n}\n\nint pm_send_runlist(struct packet_manager *pm, struct list_head *dqm_queues)\n{\n\tuint64_t rl_gpu_ib_addr;\n\tuint32_t *rl_buffer;\n\tsize_t rl_ib_size, packet_size_dwords;\n\tint retval;\n\n\tretval = pm_create_runlist_ib(pm, dqm_queues, &rl_gpu_ib_addr,\n\t\t\t\t\t&rl_ib_size);\n\tif (retval)\n\t\tgoto fail_create_runlist_ib;\n\n\tpr_debug(\"runlist IB address: 0x%llX\\n\", rl_gpu_ib_addr);\n\n\tpacket_size_dwords = pm->pmf->runlist_size / sizeof(uint32_t);\n\tmutex_lock(&pm->lock);\n\n\tretval = kq_acquire_packet_buffer(pm->priv_queue,\n\t\t\t\t\tpacket_size_dwords, &rl_buffer);\n\tif (retval)\n\t\tgoto fail_acquire_packet_buffer;\n\n\tretval = pm->pmf->runlist(pm, rl_buffer, rl_gpu_ib_addr,\n\t\t\t\t\trl_ib_size / sizeof(uint32_t), false);\n\tif (retval)\n\t\tgoto fail_create_runlist;\n\n\tkq_submit_packet(pm->priv_queue);\n\n\tmutex_unlock(&pm->lock);\n\n\treturn retval;\n\nfail_create_runlist:\n\tkq_rollback_packet(pm->priv_queue);\nfail_acquire_packet_buffer:\n\tmutex_unlock(&pm->lock);\nfail_create_runlist_ib:\n\tpm_release_ib(pm);\n\treturn retval;\n}\n\nint pm_send_query_status(struct packet_manager *pm, uint64_t fence_address,\n\t\t\tuint64_t fence_value)\n{\n\tuint32_t *buffer, size;\n\tint retval = 0;\n\n\tif (WARN_ON(!fence_address))\n\t\treturn -EFAULT;\n\n\tsize = pm->pmf->query_status_size;\n\tmutex_lock(&pm->lock);\n\tkq_acquire_packet_buffer(pm->priv_queue,\n\t\t\tsize / sizeof(uint32_t), (unsigned int **)&buffer);\n\tif (!buffer) {\n\t\tpr_err(\"Failed to allocate buffer on kernel queue\\n\");\n\t\tretval = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tretval = pm->pmf->query_status(pm, buffer, fence_address, fence_value);\n\tif (!retval)\n\t\tkq_submit_packet(pm->priv_queue);\n\telse\n\t\tkq_rollback_packet(pm->priv_queue);\n\nout:\n\tmutex_unlock(&pm->lock);\n\treturn retval;\n}\n\nint pm_update_grace_period(struct packet_manager *pm, uint32_t grace_period)\n{\n\tint retval = 0;\n\tuint32_t *buffer, size;\n\n\tsize = pm->pmf->set_grace_period_size;\n\n\tmutex_lock(&pm->lock);\n\n\tif (size) {\n\t\tkq_acquire_packet_buffer(pm->priv_queue,\n\t\t\tsize / sizeof(uint32_t),\n\t\t\t(unsigned int **)&buffer);\n\n\t\tif (!buffer) {\n\t\t\tpr_err(\"Failed to allocate buffer on kernel queue\\n\");\n\t\t\tretval = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tretval = pm->pmf->set_grace_period(pm, buffer, grace_period);\n\t\tif (!retval)\n\t\t\tkq_submit_packet(pm->priv_queue);\n\t\telse\n\t\t\tkq_rollback_packet(pm->priv_queue);\n\t}\n\nout:\n\tmutex_unlock(&pm->lock);\n\treturn retval;\n}\n\nint pm_send_unmap_queue(struct packet_manager *pm,\n\t\t\tenum kfd_unmap_queues_filter filter,\n\t\t\tuint32_t filter_param, bool reset)\n{\n\tuint32_t *buffer, size;\n\tint retval = 0;\n\n\tsize = pm->pmf->unmap_queues_size;\n\tmutex_lock(&pm->lock);\n\tkq_acquire_packet_buffer(pm->priv_queue,\n\t\t\tsize / sizeof(uint32_t), (unsigned int **)&buffer);\n\tif (!buffer) {\n\t\tpr_err(\"Failed to allocate buffer on kernel queue\\n\");\n\t\tretval = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tretval = pm->pmf->unmap_queues(pm, buffer, filter, filter_param, reset);\n\tif (!retval)\n\t\tkq_submit_packet(pm->priv_queue);\n\telse\n\t\tkq_rollback_packet(pm->priv_queue);\n\nout:\n\tmutex_unlock(&pm->lock);\n\treturn retval;\n}\n\nvoid pm_release_ib(struct packet_manager *pm)\n{\n\tmutex_lock(&pm->lock);\n\tif (pm->allocated) {\n\t\tkfd_gtt_sa_free(pm->dqm->dev, pm->ib_buffer_obj);\n\t\tpm->allocated = false;\n\t}\n\tmutex_unlock(&pm->lock);\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint pm_debugfs_runlist(struct seq_file *m, void *data)\n{\n\tstruct packet_manager *pm = data;\n\n\tmutex_lock(&pm->lock);\n\n\tif (!pm->allocated) {\n\t\tseq_puts(m, \"  No active runlist\\n\");\n\t\tgoto out;\n\t}\n\n\tseq_hex_dump(m, \"  \", DUMP_PREFIX_OFFSET, 32, 4,\n\t\t     pm->ib_buffer_obj->cpu_ptr, pm->ib_size_bytes, false);\n\nout:\n\tmutex_unlock(&pm->lock);\n\treturn 0;\n}\n\nint pm_debugfs_hang_hws(struct packet_manager *pm)\n{\n\tuint32_t *buffer, size;\n\tint r = 0;\n\n\tif (!pm->priv_queue)\n\t\treturn -EAGAIN;\n\n\tsize = pm->pmf->query_status_size;\n\tmutex_lock(&pm->lock);\n\tkq_acquire_packet_buffer(pm->priv_queue,\n\t\t\tsize / sizeof(uint32_t), (unsigned int **)&buffer);\n\tif (!buffer) {\n\t\tpr_err(\"Failed to allocate buffer on kernel queue\\n\");\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\tmemset(buffer, 0x55, size);\n\tkq_submit_packet(pm->priv_queue);\n\n\tpr_info(\"Submitting %x %x %x %x %x %x %x to HIQ to hang the HWS.\",\n\t\tbuffer[0], buffer[1], buffer[2], buffer[3],\n\t\tbuffer[4], buffer[5], buffer[6]);\nout:\n\tmutex_unlock(&pm->lock);\n\treturn r;\n}\n\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}