{
  "module_name": "kfd_process.c",
  "hash_id": "192303c9d7e372edf72168a16dd1604defbe231328bc4c9ff66b183acdb88bbe",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_process.c",
  "human_readable_source": "\n \n\n#include <linux/mutex.h>\n#include <linux/log2.h>\n#include <linux/sched.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <linux/mmu_context.h>\n#include <linux/slab.h>\n#include <linux/notifier.h>\n#include <linux/compat.h>\n#include <linux/mman.h>\n#include <linux/file.h>\n#include <linux/pm_runtime.h>\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu.h\"\n\nstruct mm_struct;\n\n#include \"kfd_priv.h\"\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_svm.h\"\n#include \"kfd_smi_events.h\"\n#include \"kfd_debug.h\"\n\n \nDEFINE_HASHTABLE(kfd_processes_table, KFD_PROCESS_TABLE_SIZE);\nDEFINE_MUTEX(kfd_processes_mutex);\n\nDEFINE_SRCU(kfd_processes_srcu);\n\n \nstatic struct workqueue_struct *kfd_process_wq;\n\n \nstatic struct workqueue_struct *kfd_restore_wq;\n\nstatic struct kfd_process *find_process(const struct task_struct *thread,\n\t\t\t\t\tbool ref);\nstatic void kfd_process_ref_release(struct kref *ref);\nstatic struct kfd_process *create_process(const struct task_struct *thread);\n\nstatic void evict_process_worker(struct work_struct *work);\nstatic void restore_process_worker(struct work_struct *work);\n\nstatic void kfd_process_device_destroy_cwsr_dgpu(struct kfd_process_device *pdd);\n\nstruct kfd_procfs_tree {\n\tstruct kobject *kobj;\n};\n\nstatic struct kfd_procfs_tree procfs;\n\n \nstruct kfd_sdma_activity_handler_workarea {\n\tstruct work_struct sdma_activity_work;\n\tstruct kfd_process_device *pdd;\n\tuint64_t sdma_activity_counter;\n};\n\nstruct temp_sdma_queue_list {\n\tuint64_t __user *rptr;\n\tuint64_t sdma_val;\n\tunsigned int queue_id;\n\tstruct list_head list;\n};\n\nstatic void kfd_sdma_activity_worker(struct work_struct *work)\n{\n\tstruct kfd_sdma_activity_handler_workarea *workarea;\n\tstruct kfd_process_device *pdd;\n\tuint64_t val;\n\tstruct mm_struct *mm;\n\tstruct queue *q;\n\tstruct qcm_process_device *qpd;\n\tstruct device_queue_manager *dqm;\n\tint ret = 0;\n\tstruct temp_sdma_queue_list sdma_q_list;\n\tstruct temp_sdma_queue_list *sdma_q, *next;\n\n\tworkarea = container_of(work, struct kfd_sdma_activity_handler_workarea,\n\t\t\t\tsdma_activity_work);\n\n\tpdd = workarea->pdd;\n\tif (!pdd)\n\t\treturn;\n\tdqm = pdd->dev->dqm;\n\tqpd = &pdd->qpd;\n\tif (!dqm || !qpd)\n\t\treturn;\n\t \n\tINIT_LIST_HEAD(&sdma_q_list.list);\n\n\t \n\tdqm_lock(dqm);\n\n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tif ((q->properties.type != KFD_QUEUE_TYPE_SDMA) &&\n\t\t    (q->properties.type != KFD_QUEUE_TYPE_SDMA_XGMI))\n\t\t\tcontinue;\n\n\t\tsdma_q = kzalloc(sizeof(struct temp_sdma_queue_list), GFP_KERNEL);\n\t\tif (!sdma_q) {\n\t\t\tdqm_unlock(dqm);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&sdma_q->list);\n\t\tsdma_q->rptr = (uint64_t __user *)q->properties.read_ptr;\n\t\tsdma_q->queue_id = q->properties.queue_id;\n\t\tlist_add_tail(&sdma_q->list, &sdma_q_list.list);\n\t}\n\n\t \n\tif (list_empty(&sdma_q_list.list)) {\n\t\tworkarea->sdma_activity_counter = pdd->sdma_past_activity_counter;\n\t\tdqm_unlock(dqm);\n\t\treturn;\n\t}\n\n\tdqm_unlock(dqm);\n\n\t \n\tmm = get_task_mm(pdd->process->lead_thread);\n\tif (!mm)\n\t\tgoto cleanup;\n\n\tkthread_use_mm(mm);\n\n\tlist_for_each_entry(sdma_q, &sdma_q_list.list, list) {\n\t\tval = 0;\n\t\tret = read_sdma_queue_counter(sdma_q->rptr, &val);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed to read SDMA queue active counter for queue id: %d\",\n\t\t\t\t sdma_q->queue_id);\n\t\t} else {\n\t\t\tsdma_q->sdma_val = val;\n\t\t\tworkarea->sdma_activity_counter += val;\n\t\t}\n\t}\n\n\tkthread_unuse_mm(mm);\n\tmmput(mm);\n\n\t \n\tdqm_lock(dqm);\n\n\tworkarea->sdma_activity_counter += pdd->sdma_past_activity_counter;\n\n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tif (list_empty(&sdma_q_list.list))\n\t\t\tbreak;\n\n\t\tif ((q->properties.type != KFD_QUEUE_TYPE_SDMA) &&\n\t\t    (q->properties.type != KFD_QUEUE_TYPE_SDMA_XGMI))\n\t\t\tcontinue;\n\n\t\tlist_for_each_entry_safe(sdma_q, next, &sdma_q_list.list, list) {\n\t\t\tif (((uint64_t __user *)q->properties.read_ptr == sdma_q->rptr) &&\n\t\t\t     (sdma_q->queue_id == q->properties.queue_id)) {\n\t\t\t\tlist_del(&sdma_q->list);\n\t\t\t\tkfree(sdma_q);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tdqm_unlock(dqm);\n\n\t \n\tlist_for_each_entry_safe(sdma_q, next, &sdma_q_list.list, list) {\n\t\tworkarea->sdma_activity_counter -= sdma_q->sdma_val;\n\t\tlist_del(&sdma_q->list);\n\t\tkfree(sdma_q);\n\t}\n\n\treturn;\n\ncleanup:\n\tlist_for_each_entry_safe(sdma_q, next, &sdma_q_list.list, list) {\n\t\tlist_del(&sdma_q->list);\n\t\tkfree(sdma_q);\n\t}\n}\n\n \nstatic int kfd_get_cu_occupancy(struct attribute *attr, char *buffer)\n{\n\tint cu_cnt;\n\tint wave_cnt;\n\tint max_waves_per_cu;\n\tstruct kfd_node *dev = NULL;\n\tstruct kfd_process *proc = NULL;\n\tstruct kfd_process_device *pdd = NULL;\n\n\tpdd = container_of(attr, struct kfd_process_device, attr_cu_occupancy);\n\tdev = pdd->dev;\n\tif (dev->kfd2kgd->get_cu_occupancy == NULL)\n\t\treturn -EINVAL;\n\n\tcu_cnt = 0;\n\tproc = pdd->process;\n\tif (pdd->qpd.queue_count == 0) {\n\t\tpr_debug(\"Gpu-Id: %d has no active queues for process %d\\n\",\n\t\t\t dev->id, proc->pasid);\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%d\\n\", cu_cnt);\n\t}\n\n\t \n\twave_cnt = 0;\n\tmax_waves_per_cu = 0;\n\tdev->kfd2kgd->get_cu_occupancy(dev->adev, proc->pasid, &wave_cnt,\n\t\t\t&max_waves_per_cu, 0);\n\n\t \n\tcu_cnt = (wave_cnt + (max_waves_per_cu - 1)) / max_waves_per_cu;\n\treturn snprintf(buffer, PAGE_SIZE, \"%d\\n\", cu_cnt);\n}\n\nstatic ssize_t kfd_procfs_show(struct kobject *kobj, struct attribute *attr,\n\t\t\t       char *buffer)\n{\n\tif (strcmp(attr->name, \"pasid\") == 0) {\n\t\tstruct kfd_process *p = container_of(attr, struct kfd_process,\n\t\t\t\t\t\t     attr_pasid);\n\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%d\\n\", p->pasid);\n\t} else if (strncmp(attr->name, \"vram_\", 5) == 0) {\n\t\tstruct kfd_process_device *pdd = container_of(attr, struct kfd_process_device,\n\t\t\t\t\t\t\t      attr_vram);\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%llu\\n\", READ_ONCE(pdd->vram_usage));\n\t} else if (strncmp(attr->name, \"sdma_\", 5) == 0) {\n\t\tstruct kfd_process_device *pdd = container_of(attr, struct kfd_process_device,\n\t\t\t\t\t\t\t      attr_sdma);\n\t\tstruct kfd_sdma_activity_handler_workarea sdma_activity_work_handler;\n\n\t\tINIT_WORK(&sdma_activity_work_handler.sdma_activity_work,\n\t\t\t\t\tkfd_sdma_activity_worker);\n\n\t\tsdma_activity_work_handler.pdd = pdd;\n\t\tsdma_activity_work_handler.sdma_activity_counter = 0;\n\n\t\tschedule_work(&sdma_activity_work_handler.sdma_activity_work);\n\n\t\tflush_work(&sdma_activity_work_handler.sdma_activity_work);\n\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%llu\\n\",\n\t\t\t\t(sdma_activity_work_handler.sdma_activity_counter)/\n\t\t\t\t SDMA_ACTIVITY_DIVISOR);\n\t} else {\n\t\tpr_err(\"Invalid attribute\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void kfd_procfs_kobj_release(struct kobject *kobj)\n{\n\tkfree(kobj);\n}\n\nstatic const struct sysfs_ops kfd_procfs_ops = {\n\t.show = kfd_procfs_show,\n};\n\nstatic const struct kobj_type procfs_type = {\n\t.release = kfd_procfs_kobj_release,\n\t.sysfs_ops = &kfd_procfs_ops,\n};\n\nvoid kfd_procfs_init(void)\n{\n\tint ret = 0;\n\n\tprocfs.kobj = kfd_alloc_struct(procfs.kobj);\n\tif (!procfs.kobj)\n\t\treturn;\n\n\tret = kobject_init_and_add(procfs.kobj, &procfs_type,\n\t\t\t\t   &kfd_device->kobj, \"proc\");\n\tif (ret) {\n\t\tpr_warn(\"Could not create procfs proc folder\");\n\t\t \n\t\tkfd_procfs_shutdown();\n\t}\n}\n\nvoid kfd_procfs_shutdown(void)\n{\n\tif (procfs.kobj) {\n\t\tkobject_del(procfs.kobj);\n\t\tkobject_put(procfs.kobj);\n\t\tprocfs.kobj = NULL;\n\t}\n}\n\nstatic ssize_t kfd_procfs_queue_show(struct kobject *kobj,\n\t\t\t\t     struct attribute *attr, char *buffer)\n{\n\tstruct queue *q = container_of(kobj, struct queue, kobj);\n\n\tif (!strcmp(attr->name, \"size\"))\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%llu\",\n\t\t\t\tq->properties.queue_size);\n\telse if (!strcmp(attr->name, \"type\"))\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%d\", q->properties.type);\n\telse if (!strcmp(attr->name, \"gpuid\"))\n\t\treturn snprintf(buffer, PAGE_SIZE, \"%u\", q->device->id);\n\telse\n\t\tpr_err(\"Invalid attribute\");\n\n\treturn 0;\n}\n\nstatic ssize_t kfd_procfs_stats_show(struct kobject *kobj,\n\t\t\t\t     struct attribute *attr, char *buffer)\n{\n\tif (strcmp(attr->name, \"evicted_ms\") == 0) {\n\t\tstruct kfd_process_device *pdd = container_of(attr,\n\t\t\t\tstruct kfd_process_device,\n\t\t\t\tattr_evict);\n\t\tuint64_t evict_jiffies;\n\n\t\tevict_jiffies = atomic64_read(&pdd->evict_duration_counter);\n\n\t\treturn snprintf(buffer,\n\t\t\t\tPAGE_SIZE,\n\t\t\t\t\"%llu\\n\",\n\t\t\t\tjiffies64_to_msecs(evict_jiffies));\n\n\t \n\t} else if (strcmp(attr->name, \"cu_occupancy\") == 0) {\n\t\treturn kfd_get_cu_occupancy(attr, buffer);\n\t} else {\n\t\tpr_err(\"Invalid attribute\");\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t kfd_sysfs_counters_show(struct kobject *kobj,\n\t\t\t\t       struct attribute *attr, char *buf)\n{\n\tstruct kfd_process_device *pdd;\n\n\tif (!strcmp(attr->name, \"faults\")) {\n\t\tpdd = container_of(attr, struct kfd_process_device,\n\t\t\t\t   attr_faults);\n\t\treturn sysfs_emit(buf, \"%llu\\n\", READ_ONCE(pdd->faults));\n\t}\n\tif (!strcmp(attr->name, \"page_in\")) {\n\t\tpdd = container_of(attr, struct kfd_process_device,\n\t\t\t\t   attr_page_in);\n\t\treturn sysfs_emit(buf, \"%llu\\n\", READ_ONCE(pdd->page_in));\n\t}\n\tif (!strcmp(attr->name, \"page_out\")) {\n\t\tpdd = container_of(attr, struct kfd_process_device,\n\t\t\t\t   attr_page_out);\n\t\treturn sysfs_emit(buf, \"%llu\\n\", READ_ONCE(pdd->page_out));\n\t}\n\treturn 0;\n}\n\nstatic struct attribute attr_queue_size = {\n\t.name = \"size\",\n\t.mode = KFD_SYSFS_FILE_MODE\n};\n\nstatic struct attribute attr_queue_type = {\n\t.name = \"type\",\n\t.mode = KFD_SYSFS_FILE_MODE\n};\n\nstatic struct attribute attr_queue_gpuid = {\n\t.name = \"gpuid\",\n\t.mode = KFD_SYSFS_FILE_MODE\n};\n\nstatic struct attribute *procfs_queue_attrs[] = {\n\t&attr_queue_size,\n\t&attr_queue_type,\n\t&attr_queue_gpuid,\n\tNULL\n};\nATTRIBUTE_GROUPS(procfs_queue);\n\nstatic const struct sysfs_ops procfs_queue_ops = {\n\t.show = kfd_procfs_queue_show,\n};\n\nstatic const struct kobj_type procfs_queue_type = {\n\t.sysfs_ops = &procfs_queue_ops,\n\t.default_groups = procfs_queue_groups,\n};\n\nstatic const struct sysfs_ops procfs_stats_ops = {\n\t.show = kfd_procfs_stats_show,\n};\n\nstatic const struct kobj_type procfs_stats_type = {\n\t.sysfs_ops = &procfs_stats_ops,\n\t.release = kfd_procfs_kobj_release,\n};\n\nstatic const struct sysfs_ops sysfs_counters_ops = {\n\t.show = kfd_sysfs_counters_show,\n};\n\nstatic const struct kobj_type sysfs_counters_type = {\n\t.sysfs_ops = &sysfs_counters_ops,\n\t.release = kfd_procfs_kobj_release,\n};\n\nint kfd_procfs_add_queue(struct queue *q)\n{\n\tstruct kfd_process *proc;\n\tint ret;\n\n\tif (!q || !q->process)\n\t\treturn -EINVAL;\n\tproc = q->process;\n\n\t \n\tif (!proc->kobj_queues)\n\t\treturn -EFAULT;\n\tret = kobject_init_and_add(&q->kobj, &procfs_queue_type,\n\t\t\tproc->kobj_queues, \"%u\", q->properties.queue_id);\n\tif (ret < 0) {\n\t\tpr_warn(\"Creating proc/<pid>/queues/%u failed\",\n\t\t\tq->properties.queue_id);\n\t\tkobject_put(&q->kobj);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void kfd_sysfs_create_file(struct kobject *kobj, struct attribute *attr,\n\t\t\t\t char *name)\n{\n\tint ret;\n\n\tif (!kobj || !attr || !name)\n\t\treturn;\n\n\tattr->name = name;\n\tattr->mode = KFD_SYSFS_FILE_MODE;\n\tsysfs_attr_init(attr);\n\n\tret = sysfs_create_file(kobj, attr);\n\tif (ret)\n\t\tpr_warn(\"Create sysfs %s/%s failed %d\", kobj->name, name, ret);\n}\n\nstatic void kfd_procfs_add_sysfs_stats(struct kfd_process *p)\n{\n\tint ret;\n\tint i;\n\tchar stats_dir_filename[MAX_SYSFS_FILENAME_LEN];\n\n\tif (!p || !p->kobj)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tsnprintf(stats_dir_filename, MAX_SYSFS_FILENAME_LEN,\n\t\t\t\t\"stats_%u\", pdd->dev->id);\n\t\tpdd->kobj_stats = kfd_alloc_struct(pdd->kobj_stats);\n\t\tif (!pdd->kobj_stats)\n\t\t\treturn;\n\n\t\tret = kobject_init_and_add(pdd->kobj_stats,\n\t\t\t\t\t   &procfs_stats_type,\n\t\t\t\t\t   p->kobj,\n\t\t\t\t\t   stats_dir_filename);\n\n\t\tif (ret) {\n\t\t\tpr_warn(\"Creating KFD proc/stats_%s folder failed\",\n\t\t\t\tstats_dir_filename);\n\t\t\tkobject_put(pdd->kobj_stats);\n\t\t\tpdd->kobj_stats = NULL;\n\t\t\treturn;\n\t\t}\n\n\t\tkfd_sysfs_create_file(pdd->kobj_stats, &pdd->attr_evict,\n\t\t\t\t      \"evicted_ms\");\n\t\t \n\t\tif (pdd->dev->kfd2kgd->get_cu_occupancy)\n\t\t\tkfd_sysfs_create_file(pdd->kobj_stats,\n\t\t\t\t\t      &pdd->attr_cu_occupancy,\n\t\t\t\t\t      \"cu_occupancy\");\n\t}\n}\n\nstatic void kfd_procfs_add_sysfs_counters(struct kfd_process *p)\n{\n\tint ret = 0;\n\tint i;\n\tchar counters_dir_filename[MAX_SYSFS_FILENAME_LEN];\n\n\tif (!p || !p->kobj)\n\t\treturn;\n\n\t \n\tfor_each_set_bit(i, p->svms.bitmap_supported, p->n_pdds) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tstruct kobject *kobj_counters;\n\n\t\tsnprintf(counters_dir_filename, MAX_SYSFS_FILENAME_LEN,\n\t\t\t\"counters_%u\", pdd->dev->id);\n\t\tkobj_counters = kfd_alloc_struct(kobj_counters);\n\t\tif (!kobj_counters)\n\t\t\treturn;\n\n\t\tret = kobject_init_and_add(kobj_counters, &sysfs_counters_type,\n\t\t\t\t\t   p->kobj, counters_dir_filename);\n\t\tif (ret) {\n\t\t\tpr_warn(\"Creating KFD proc/%s folder failed\",\n\t\t\t\tcounters_dir_filename);\n\t\t\tkobject_put(kobj_counters);\n\t\t\treturn;\n\t\t}\n\n\t\tpdd->kobj_counters = kobj_counters;\n\t\tkfd_sysfs_create_file(kobj_counters, &pdd->attr_faults,\n\t\t\t\t      \"faults\");\n\t\tkfd_sysfs_create_file(kobj_counters, &pdd->attr_page_in,\n\t\t\t\t      \"page_in\");\n\t\tkfd_sysfs_create_file(kobj_counters, &pdd->attr_page_out,\n\t\t\t\t      \"page_out\");\n\t}\n}\n\nstatic void kfd_procfs_add_sysfs_files(struct kfd_process *p)\n{\n\tint i;\n\n\tif (!p || !p->kobj)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tsnprintf(pdd->vram_filename, MAX_SYSFS_FILENAME_LEN, \"vram_%u\",\n\t\t\t pdd->dev->id);\n\t\tkfd_sysfs_create_file(p->kobj, &pdd->attr_vram,\n\t\t\t\t      pdd->vram_filename);\n\n\t\tsnprintf(pdd->sdma_filename, MAX_SYSFS_FILENAME_LEN, \"sdma_%u\",\n\t\t\t pdd->dev->id);\n\t\tkfd_sysfs_create_file(p->kobj, &pdd->attr_sdma,\n\t\t\t\t\t    pdd->sdma_filename);\n\t}\n}\n\nvoid kfd_procfs_del_queue(struct queue *q)\n{\n\tif (!q)\n\t\treturn;\n\n\tkobject_del(&q->kobj);\n\tkobject_put(&q->kobj);\n}\n\nint kfd_process_create_wq(void)\n{\n\tif (!kfd_process_wq)\n\t\tkfd_process_wq = alloc_workqueue(\"kfd_process_wq\", 0, 0);\n\tif (!kfd_restore_wq)\n\t\tkfd_restore_wq = alloc_ordered_workqueue(\"kfd_restore_wq\", 0);\n\n\tif (!kfd_process_wq || !kfd_restore_wq) {\n\t\tkfd_process_destroy_wq();\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid kfd_process_destroy_wq(void)\n{\n\tif (kfd_process_wq) {\n\t\tdestroy_workqueue(kfd_process_wq);\n\t\tkfd_process_wq = NULL;\n\t}\n\tif (kfd_restore_wq) {\n\t\tdestroy_workqueue(kfd_restore_wq);\n\t\tkfd_restore_wq = NULL;\n\t}\n}\n\nstatic void kfd_process_free_gpuvm(struct kgd_mem *mem,\n\t\t\tstruct kfd_process_device *pdd, void **kptr)\n{\n\tstruct kfd_node *dev = pdd->dev;\n\n\tif (kptr && *kptr) {\n\t\tamdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(mem);\n\t\t*kptr = NULL;\n\t}\n\n\tamdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(dev->adev, mem, pdd->drm_priv);\n\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->adev, mem, pdd->drm_priv,\n\t\t\t\t\t       NULL);\n}\n\n \nstatic int kfd_process_alloc_gpuvm(struct kfd_process_device *pdd,\n\t\t\t\t   uint64_t gpu_va, uint32_t size,\n\t\t\t\t   uint32_t flags, struct kgd_mem **mem, void **kptr)\n{\n\tstruct kfd_node *kdev = pdd->dev;\n\tint err;\n\n\terr = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(kdev->adev, gpu_va, size,\n\t\t\t\t\t\t pdd->drm_priv, mem, NULL,\n\t\t\t\t\t\t flags, false);\n\tif (err)\n\t\tgoto err_alloc_mem;\n\n\terr = amdgpu_amdkfd_gpuvm_map_memory_to_gpu(kdev->adev, *mem,\n\t\t\tpdd->drm_priv);\n\tif (err)\n\t\tgoto err_map_mem;\n\n\terr = amdgpu_amdkfd_gpuvm_sync_memory(kdev->adev, *mem, true);\n\tif (err) {\n\t\tpr_debug(\"Sync memory failed, wait interrupted by user signal\\n\");\n\t\tgoto sync_memory_failed;\n\t}\n\n\tif (kptr) {\n\t\terr = amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(\n\t\t\t\t(struct kgd_mem *)*mem, kptr, NULL);\n\t\tif (err) {\n\t\t\tpr_debug(\"Map GTT BO to kernel failed\\n\");\n\t\t\tgoto sync_memory_failed;\n\t\t}\n\t}\n\n\treturn err;\n\nsync_memory_failed:\n\tamdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(kdev->adev, *mem, pdd->drm_priv);\n\nerr_map_mem:\n\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(kdev->adev, *mem, pdd->drm_priv,\n\t\t\t\t\t       NULL);\nerr_alloc_mem:\n\t*mem = NULL;\n\t*kptr = NULL;\n\treturn err;\n}\n\n \nstatic int kfd_process_device_reserve_ib_mem(struct kfd_process_device *pdd)\n{\n\tstruct qcm_process_device *qpd = &pdd->qpd;\n\tuint32_t flags = KFD_IOC_ALLOC_MEM_FLAGS_GTT |\n\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_NO_SUBSTITUTE |\n\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_WRITABLE |\n\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\tstruct kgd_mem *mem;\n\tvoid *kaddr;\n\tint ret;\n\n\tif (qpd->ib_kaddr || !qpd->ib_base)\n\t\treturn 0;\n\n\t \n\tret = kfd_process_alloc_gpuvm(pdd, qpd->ib_base, PAGE_SIZE, flags,\n\t\t\t\t      &mem, &kaddr);\n\tif (ret)\n\t\treturn ret;\n\n\tqpd->ib_mem = mem;\n\tqpd->ib_kaddr = kaddr;\n\n\treturn 0;\n}\n\nstatic void kfd_process_device_destroy_ib_mem(struct kfd_process_device *pdd)\n{\n\tstruct qcm_process_device *qpd = &pdd->qpd;\n\n\tif (!qpd->ib_kaddr || !qpd->ib_base)\n\t\treturn;\n\n\tkfd_process_free_gpuvm(qpd->ib_mem, pdd, &qpd->ib_kaddr);\n}\n\nstruct kfd_process *kfd_create_process(struct task_struct *thread)\n{\n\tstruct kfd_process *process;\n\tint ret;\n\n\tif (!(thread->mm && mmget_not_zero(thread->mm)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (thread->group_leader->mm != thread->mm) {\n\t\tmmput(thread->mm);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tmutex_lock(&kfd_processes_mutex);\n\n\tif (kfd_is_locked()) {\n\t\tmutex_unlock(&kfd_processes_mutex);\n\t\tpr_debug(\"KFD is locked! Cannot create process\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\tprocess = find_process(thread, false);\n\tif (process) {\n\t\tpr_debug(\"Process already found\\n\");\n\t} else {\n\t\tprocess = create_process(thread);\n\t\tif (IS_ERR(process))\n\t\t\tgoto out;\n\n\t\tif (!procfs.kobj)\n\t\t\tgoto out;\n\n\t\tprocess->kobj = kfd_alloc_struct(process->kobj);\n\t\tif (!process->kobj) {\n\t\t\tpr_warn(\"Creating procfs kobject failed\");\n\t\t\tgoto out;\n\t\t}\n\t\tret = kobject_init_and_add(process->kobj, &procfs_type,\n\t\t\t\t\t   procfs.kobj, \"%d\",\n\t\t\t\t\t   (int)process->lead_thread->pid);\n\t\tif (ret) {\n\t\t\tpr_warn(\"Creating procfs pid directory failed\");\n\t\t\tkobject_put(process->kobj);\n\t\t\tgoto out;\n\t\t}\n\n\t\tkfd_sysfs_create_file(process->kobj, &process->attr_pasid,\n\t\t\t\t      \"pasid\");\n\n\t\tprocess->kobj_queues = kobject_create_and_add(\"queues\",\n\t\t\t\t\t\t\tprocess->kobj);\n\t\tif (!process->kobj_queues)\n\t\t\tpr_warn(\"Creating KFD proc/queues folder failed\");\n\n\t\tkfd_procfs_add_sysfs_stats(process);\n\t\tkfd_procfs_add_sysfs_files(process);\n\t\tkfd_procfs_add_sysfs_counters(process);\n\n\t\tinit_waitqueue_head(&process->wait_irq_drain);\n\t}\nout:\n\tif (!IS_ERR(process))\n\t\tkref_get(&process->ref);\n\tmutex_unlock(&kfd_processes_mutex);\n\tmmput(thread->mm);\n\n\treturn process;\n}\n\nstruct kfd_process *kfd_get_process(const struct task_struct *thread)\n{\n\tstruct kfd_process *process;\n\n\tif (!thread->mm)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (thread->group_leader->mm != thread->mm)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tprocess = find_process(thread, false);\n\tif (!process)\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn process;\n}\n\nstatic struct kfd_process *find_process_by_mm(const struct mm_struct *mm)\n{\n\tstruct kfd_process *process;\n\n\thash_for_each_possible_rcu(kfd_processes_table, process,\n\t\t\t\t\tkfd_processes, (uintptr_t)mm)\n\t\tif (process->mm == mm)\n\t\t\treturn process;\n\n\treturn NULL;\n}\n\nstatic struct kfd_process *find_process(const struct task_struct *thread,\n\t\t\t\t\tbool ref)\n{\n\tstruct kfd_process *p;\n\tint idx;\n\n\tidx = srcu_read_lock(&kfd_processes_srcu);\n\tp = find_process_by_mm(thread->mm);\n\tif (p && ref)\n\t\tkref_get(&p->ref);\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\n\treturn p;\n}\n\nvoid kfd_unref_process(struct kfd_process *p)\n{\n\tkref_put(&p->ref, kfd_process_ref_release);\n}\n\n \nstruct kfd_process *kfd_lookup_process_by_pid(struct pid *pid)\n{\n\tstruct task_struct *task = NULL;\n\tstruct kfd_process *p    = NULL;\n\n\tif (!pid) {\n\t\ttask = current;\n\t\tget_task_struct(task);\n\t} else {\n\t\ttask = get_pid_task(pid, PIDTYPE_PID);\n\t}\n\n\tif (task) {\n\t\tp = find_process(task, true);\n\t\tput_task_struct(task);\n\t}\n\n\treturn p;\n}\n\nstatic void kfd_process_device_free_bos(struct kfd_process_device *pdd)\n{\n\tstruct kfd_process *p = pdd->process;\n\tvoid *mem;\n\tint id;\n\tint i;\n\n\t \n\tidr_for_each_entry(&pdd->alloc_idr, mem, id) {\n\n\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *peer_pdd = p->pdds[i];\n\n\t\t\tif (!peer_pdd->drm_priv)\n\t\t\t\tcontinue;\n\t\t\tamdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\t\t\tpeer_pdd->dev->adev, mem, peer_pdd->drm_priv);\n\t\t}\n\n\t\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->adev, mem,\n\t\t\t\t\t\t       pdd->drm_priv, NULL);\n\t\tkfd_process_device_remove_obj_handle(pdd, id);\n\t}\n}\n\n \nstatic void kfd_process_kunmap_signal_bo(struct kfd_process *p)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct kfd_node *kdev;\n\tvoid *mem;\n\n\tkdev = kfd_device_by_id(GET_GPU_ID(p->signal_handle));\n\tif (!kdev)\n\t\treturn;\n\n\tmutex_lock(&p->mutex);\n\n\tpdd = kfd_get_process_device_data(kdev, p);\n\tif (!pdd)\n\t\tgoto out;\n\n\tmem = kfd_process_device_translate_handle(\n\t\tpdd, GET_IDR_HANDLE(p->signal_handle));\n\tif (!mem)\n\t\tgoto out;\n\n\tamdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(mem);\n\nout:\n\tmutex_unlock(&p->mutex);\n}\n\nstatic void kfd_process_free_outstanding_kfd_bos(struct kfd_process *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++)\n\t\tkfd_process_device_free_bos(p->pdds[i]);\n}\n\nstatic void kfd_process_destroy_pdds(struct kfd_process *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tpr_debug(\"Releasing pdd (topology id %d) for process (pasid 0x%x)\\n\",\n\t\t\t\tpdd->dev->id, p->pasid);\n\n\t\tkfd_process_device_destroy_cwsr_dgpu(pdd);\n\t\tkfd_process_device_destroy_ib_mem(pdd);\n\n\t\tif (pdd->drm_file) {\n\t\t\tamdgpu_amdkfd_gpuvm_release_process_vm(\n\t\t\t\t\tpdd->dev->adev, pdd->drm_priv);\n\t\t\tfput(pdd->drm_file);\n\t\t}\n\n\t\tif (pdd->qpd.cwsr_kaddr && !pdd->qpd.cwsr_base)\n\t\t\tfree_pages((unsigned long)pdd->qpd.cwsr_kaddr,\n\t\t\t\tget_order(KFD_CWSR_TBA_TMA_SIZE));\n\n\t\tidr_destroy(&pdd->alloc_idr);\n\n\t\tkfd_free_process_doorbells(pdd->dev->kfd, pdd);\n\n\t\tif (pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tamdgpu_amdkfd_free_gtt_mem(pdd->dev->adev,\n\t\t\t\t\t\t   pdd->proc_ctx_bo);\n\t\t \n\t\tif (pdd->runtime_inuse) {\n\t\t\tpm_runtime_mark_last_busy(adev_to_drm(pdd->dev->adev)->dev);\n\t\t\tpm_runtime_put_autosuspend(adev_to_drm(pdd->dev->adev)->dev);\n\t\t\tpdd->runtime_inuse = false;\n\t\t}\n\n\t\tkfree(pdd);\n\t\tp->pdds[i] = NULL;\n\t}\n\tp->n_pdds = 0;\n}\n\nstatic void kfd_process_remove_sysfs(struct kfd_process *p)\n{\n\tstruct kfd_process_device *pdd;\n\tint i;\n\n\tif (!p->kobj)\n\t\treturn;\n\n\tsysfs_remove_file(p->kobj, &p->attr_pasid);\n\tkobject_del(p->kobj_queues);\n\tkobject_put(p->kobj_queues);\n\tp->kobj_queues = NULL;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tpdd = p->pdds[i];\n\n\t\tsysfs_remove_file(p->kobj, &pdd->attr_vram);\n\t\tsysfs_remove_file(p->kobj, &pdd->attr_sdma);\n\n\t\tsysfs_remove_file(pdd->kobj_stats, &pdd->attr_evict);\n\t\tif (pdd->dev->kfd2kgd->get_cu_occupancy)\n\t\t\tsysfs_remove_file(pdd->kobj_stats,\n\t\t\t\t\t  &pdd->attr_cu_occupancy);\n\t\tkobject_del(pdd->kobj_stats);\n\t\tkobject_put(pdd->kobj_stats);\n\t\tpdd->kobj_stats = NULL;\n\t}\n\n\tfor_each_set_bit(i, p->svms.bitmap_supported, p->n_pdds) {\n\t\tpdd = p->pdds[i];\n\n\t\tsysfs_remove_file(pdd->kobj_counters, &pdd->attr_faults);\n\t\tsysfs_remove_file(pdd->kobj_counters, &pdd->attr_page_in);\n\t\tsysfs_remove_file(pdd->kobj_counters, &pdd->attr_page_out);\n\t\tkobject_del(pdd->kobj_counters);\n\t\tkobject_put(pdd->kobj_counters);\n\t\tpdd->kobj_counters = NULL;\n\t}\n\n\tkobject_del(p->kobj);\n\tkobject_put(p->kobj);\n\tp->kobj = NULL;\n}\n\n \nstatic void kfd_process_wq_release(struct work_struct *work)\n{\n\tstruct kfd_process *p = container_of(work, struct kfd_process,\n\t\t\t\t\t     release_work);\n\n\tkfd_process_dequeue_from_all_devices(p);\n\tpqm_uninit(&p->pqm);\n\n\t \n\tdma_fence_signal(p->ef);\n\n\tkfd_process_remove_sysfs(p);\n\n\tkfd_process_kunmap_signal_bo(p);\n\tkfd_process_free_outstanding_kfd_bos(p);\n\tsvm_range_list_fini(p);\n\n\tkfd_process_destroy_pdds(p);\n\tdma_fence_put(p->ef);\n\n\tkfd_event_free_process(p);\n\n\tkfd_pasid_free(p->pasid);\n\tmutex_destroy(&p->mutex);\n\n\tput_task_struct(p->lead_thread);\n\n\tkfree(p);\n}\n\nstatic void kfd_process_ref_release(struct kref *ref)\n{\n\tstruct kfd_process *p = container_of(ref, struct kfd_process, ref);\n\n\tINIT_WORK(&p->release_work, kfd_process_wq_release);\n\tqueue_work(kfd_process_wq, &p->release_work);\n}\n\nstatic struct mmu_notifier *kfd_process_alloc_notifier(struct mm_struct *mm)\n{\n\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\tstruct kfd_process *p = find_process_by_mm(mm);\n\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\n\treturn p ? &p->mmu_notifier : ERR_PTR(-ESRCH);\n}\n\nstatic void kfd_process_free_notifier(struct mmu_notifier *mn)\n{\n\tkfd_unref_process(container_of(mn, struct kfd_process, mmu_notifier));\n}\n\nstatic void kfd_process_notifier_release_internal(struct kfd_process *p)\n{\n\tint i;\n\n\tcancel_delayed_work_sync(&p->eviction_work);\n\tcancel_delayed_work_sync(&p->restore_work);\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\t \n\t\tif (!kfd_dbg_is_rlc_restore_supported(pdd->dev) && p->runtime_info.ttmp_setup)\n\t\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\t}\n\n\t \n\tp->mm = NULL;\n\tkfd_dbg_trap_disable(p);\n\n\tif (atomic_read(&p->debugged_process_count) > 0) {\n\t\tstruct kfd_process *target;\n\t\tunsigned int temp;\n\t\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\n\t\thash_for_each_rcu(kfd_processes_table, temp, target, kfd_processes) {\n\t\t\tif (target->debugger_process && target->debugger_process == p) {\n\t\t\t\tmutex_lock_nested(&target->mutex, 1);\n\t\t\t\tkfd_dbg_trap_disable(target);\n\t\t\t\tmutex_unlock(&target->mutex);\n\t\t\t\tif (atomic_read(&p->debugged_process_count) == 0)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\t}\n\n\tmmu_notifier_put(&p->mmu_notifier);\n}\n\nstatic void kfd_process_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm)\n{\n\tstruct kfd_process *p;\n\n\t \n\tp = container_of(mn, struct kfd_process, mmu_notifier);\n\tif (WARN_ON(p->mm != mm))\n\t\treturn;\n\n\tmutex_lock(&kfd_processes_mutex);\n\t \n\tif (hash_empty(kfd_processes_table)) {\n\t\tmutex_unlock(&kfd_processes_mutex);\n\t\treturn;\n\t}\n\thash_del_rcu(&p->kfd_processes);\n\tmutex_unlock(&kfd_processes_mutex);\n\tsynchronize_srcu(&kfd_processes_srcu);\n\n\tkfd_process_notifier_release_internal(p);\n}\n\nstatic const struct mmu_notifier_ops kfd_process_mmu_notifier_ops = {\n\t.release = kfd_process_notifier_release,\n\t.alloc_notifier = kfd_process_alloc_notifier,\n\t.free_notifier = kfd_process_free_notifier,\n};\n\n \nvoid kfd_cleanup_processes(void)\n{\n\tstruct kfd_process *p;\n\tstruct hlist_node *p_temp;\n\tunsigned int temp;\n\tHLIST_HEAD(cleanup_list);\n\n\t \n\tmutex_lock(&kfd_processes_mutex);\n\thash_for_each_safe(kfd_processes_table, temp, p_temp, p, kfd_processes) {\n\t\thash_del_rcu(&p->kfd_processes);\n\t\tsynchronize_srcu(&kfd_processes_srcu);\n\t\thlist_add_head(&p->kfd_processes, &cleanup_list);\n\t}\n\tmutex_unlock(&kfd_processes_mutex);\n\n\thlist_for_each_entry_safe(p, p_temp, &cleanup_list, kfd_processes)\n\t\tkfd_process_notifier_release_internal(p);\n\n\t \n\tmmu_notifier_synchronize();\n}\n\nint kfd_process_init_cwsr_apu(struct kfd_process *p, struct file *filep)\n{\n\tunsigned long  offset;\n\tint i;\n\n\tif (p->has_cwsr)\n\t\treturn 0;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_node *dev = p->pdds[i]->dev;\n\t\tstruct qcm_process_device *qpd = &p->pdds[i]->qpd;\n\n\t\tif (!dev->kfd->cwsr_enabled || qpd->cwsr_kaddr || qpd->cwsr_base)\n\t\t\tcontinue;\n\n\t\toffset = KFD_MMAP_TYPE_RESERVED_MEM | KFD_MMAP_GPU_ID(dev->id);\n\t\tqpd->tba_addr = (int64_t)vm_mmap(filep, 0,\n\t\t\tKFD_CWSR_TBA_TMA_SIZE, PROT_READ | PROT_EXEC,\n\t\t\tMAP_SHARED, offset);\n\n\t\tif (IS_ERR_VALUE(qpd->tba_addr)) {\n\t\t\tint err = qpd->tba_addr;\n\n\t\t\tpr_err(\"Failure to set tba address. error %d.\\n\", err);\n\t\t\tqpd->tba_addr = 0;\n\t\t\tqpd->cwsr_kaddr = NULL;\n\t\t\treturn err;\n\t\t}\n\n\t\tmemcpy(qpd->cwsr_kaddr, dev->kfd->cwsr_isa, dev->kfd->cwsr_isa_size);\n\n\t\tkfd_process_set_trap_debug_flag(qpd, p->debug_trap_enabled);\n\n\t\tqpd->tma_addr = qpd->tba_addr + KFD_CWSR_TMA_OFFSET;\n\t\tpr_debug(\"set tba :0x%llx, tma:0x%llx, cwsr_kaddr:%p for pqm.\\n\",\n\t\t\tqpd->tba_addr, qpd->tma_addr, qpd->cwsr_kaddr);\n\t}\n\n\tp->has_cwsr = true;\n\n\treturn 0;\n}\n\nstatic int kfd_process_device_init_cwsr_dgpu(struct kfd_process_device *pdd)\n{\n\tstruct kfd_node *dev = pdd->dev;\n\tstruct qcm_process_device *qpd = &pdd->qpd;\n\tuint32_t flags = KFD_IOC_ALLOC_MEM_FLAGS_GTT\n\t\t\t| KFD_IOC_ALLOC_MEM_FLAGS_NO_SUBSTITUTE\n\t\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\tstruct kgd_mem *mem;\n\tvoid *kaddr;\n\tint ret;\n\n\tif (!dev->kfd->cwsr_enabled || qpd->cwsr_kaddr || !qpd->cwsr_base)\n\t\treturn 0;\n\n\t \n\tret = kfd_process_alloc_gpuvm(pdd, qpd->cwsr_base,\n\t\t\t\t      KFD_CWSR_TBA_TMA_SIZE, flags, &mem, &kaddr);\n\tif (ret)\n\t\treturn ret;\n\n\tqpd->cwsr_mem = mem;\n\tqpd->cwsr_kaddr = kaddr;\n\tqpd->tba_addr = qpd->cwsr_base;\n\n\tmemcpy(qpd->cwsr_kaddr, dev->kfd->cwsr_isa, dev->kfd->cwsr_isa_size);\n\n\tkfd_process_set_trap_debug_flag(&pdd->qpd,\n\t\t\t\t\tpdd->process->debug_trap_enabled);\n\n\tqpd->tma_addr = qpd->tba_addr + KFD_CWSR_TMA_OFFSET;\n\tpr_debug(\"set tba :0x%llx, tma:0x%llx, cwsr_kaddr:%p for pqm.\\n\",\n\t\t qpd->tba_addr, qpd->tma_addr, qpd->cwsr_kaddr);\n\n\treturn 0;\n}\n\nstatic void kfd_process_device_destroy_cwsr_dgpu(struct kfd_process_device *pdd)\n{\n\tstruct kfd_node *dev = pdd->dev;\n\tstruct qcm_process_device *qpd = &pdd->qpd;\n\n\tif (!dev->kfd->cwsr_enabled || !qpd->cwsr_kaddr || !qpd->cwsr_base)\n\t\treturn;\n\n\tkfd_process_free_gpuvm(qpd->cwsr_mem, pdd, &qpd->cwsr_kaddr);\n}\n\nvoid kfd_process_set_trap_handler(struct qcm_process_device *qpd,\n\t\t\t\t  uint64_t tba_addr,\n\t\t\t\t  uint64_t tma_addr)\n{\n\tif (qpd->cwsr_kaddr) {\n\t\t \n\t\tuint64_t *tma =\n\t\t\t(uint64_t *)(qpd->cwsr_kaddr + KFD_CWSR_TMA_OFFSET);\n\t\ttma[0] = tba_addr;\n\t\ttma[1] = tma_addr;\n\t} else {\n\t\t \n\t\tqpd->tba_addr = tba_addr;\n\t\tqpd->tma_addr = tma_addr;\n\t}\n}\n\nbool kfd_process_xnack_mode(struct kfd_process *p, bool supported)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_node *dev = p->pdds[i]->dev;\n\n\t\t \n\t\tif (!KFD_IS_SOC15(dev))\n\t\t\tcontinue;\n\t\t \n\t\tif (supported && KFD_SUPPORT_XNACK_PER_PROCESS(dev))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (KFD_GC_VERSION(dev) >= IP_VERSION(10, 1, 1))\n\t\t\treturn false;\n\n\t\tif (dev->kfd->noretry)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid kfd_process_set_trap_debug_flag(struct qcm_process_device *qpd,\n\t\t\t\t     bool enabled)\n{\n\tif (qpd->cwsr_kaddr) {\n\t\tuint64_t *tma =\n\t\t\t(uint64_t *)(qpd->cwsr_kaddr + KFD_CWSR_TMA_OFFSET);\n\t\ttma[2] = enabled;\n\t}\n}\n\n \nstatic struct kfd_process *create_process(const struct task_struct *thread)\n{\n\tstruct kfd_process *process;\n\tstruct mmu_notifier *mn;\n\tint err = -ENOMEM;\n\n\tprocess = kzalloc(sizeof(*process), GFP_KERNEL);\n\tif (!process)\n\t\tgoto err_alloc_process;\n\n\tkref_init(&process->ref);\n\tmutex_init(&process->mutex);\n\tprocess->mm = thread->mm;\n\tprocess->lead_thread = thread->group_leader;\n\tprocess->n_pdds = 0;\n\tprocess->queues_paused = false;\n\tINIT_DELAYED_WORK(&process->eviction_work, evict_process_worker);\n\tINIT_DELAYED_WORK(&process->restore_work, restore_process_worker);\n\tprocess->last_restore_timestamp = get_jiffies_64();\n\terr = kfd_event_init_process(process);\n\tif (err)\n\t\tgoto err_event_init;\n\tprocess->is_32bit_user_mode = in_compat_syscall();\n\tprocess->debug_trap_enabled = false;\n\tprocess->debugger_process = NULL;\n\tprocess->exception_enable_mask = 0;\n\tatomic_set(&process->debugged_process_count, 0);\n\tsema_init(&process->runtime_enable_sema, 0);\n\n\tprocess->pasid = kfd_pasid_alloc();\n\tif (process->pasid == 0) {\n\t\terr = -ENOSPC;\n\t\tgoto err_alloc_pasid;\n\t}\n\n\terr = pqm_init(&process->pqm, process);\n\tif (err != 0)\n\t\tgoto err_process_pqm_init;\n\n\t \n\terr = kfd_init_apertures(process);\n\tif (err != 0)\n\t\tgoto err_init_apertures;\n\n\t \n\tprocess->xnack_enabled = kfd_process_xnack_mode(process, false);\n\n\terr = svm_range_list_init(process);\n\tif (err)\n\t\tgoto err_init_svm_range_list;\n\n\t \n\thash_add_rcu(kfd_processes_table, &process->kfd_processes,\n\t\t\t(uintptr_t)process->mm);\n\n\t \n\tkref_get(&process->ref);\n\n\t \n\tmn = mmu_notifier_get(&kfd_process_mmu_notifier_ops, process->mm);\n\tif (IS_ERR(mn)) {\n\t\terr = PTR_ERR(mn);\n\t\tgoto err_register_notifier;\n\t}\n\tBUG_ON(mn != &process->mmu_notifier);\n\n\tkfd_unref_process(process);\n\tget_task_struct(process->lead_thread);\n\n\tINIT_WORK(&process->debug_event_workarea, debug_event_write_work_handler);\n\n\treturn process;\n\nerr_register_notifier:\n\thash_del_rcu(&process->kfd_processes);\n\tsvm_range_list_fini(process);\nerr_init_svm_range_list:\n\tkfd_process_free_outstanding_kfd_bos(process);\n\tkfd_process_destroy_pdds(process);\nerr_init_apertures:\n\tpqm_uninit(&process->pqm);\nerr_process_pqm_init:\n\tkfd_pasid_free(process->pasid);\nerr_alloc_pasid:\n\tkfd_event_free_process(process);\nerr_event_init:\n\tmutex_destroy(&process->mutex);\n\tkfree(process);\nerr_alloc_process:\n\treturn ERR_PTR(err);\n}\n\nstruct kfd_process_device *kfd_get_process_device_data(struct kfd_node *dev,\n\t\t\t\t\t\t\tstruct kfd_process *p)\n{\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++)\n\t\tif (p->pdds[i]->dev == dev)\n\t\t\treturn p->pdds[i];\n\n\treturn NULL;\n}\n\nstruct kfd_process_device *kfd_create_process_device_data(struct kfd_node *dev,\n\t\t\t\t\t\t\tstruct kfd_process *p)\n{\n\tstruct kfd_process_device *pdd = NULL;\n\tint retval = 0;\n\n\tif (WARN_ON_ONCE(p->n_pdds >= MAX_GPU_INSTANCE))\n\t\treturn NULL;\n\tpdd = kzalloc(sizeof(*pdd), GFP_KERNEL);\n\tif (!pdd)\n\t\treturn NULL;\n\n\tpdd->dev = dev;\n\tINIT_LIST_HEAD(&pdd->qpd.queues_list);\n\tINIT_LIST_HEAD(&pdd->qpd.priv_queue_list);\n\tpdd->qpd.dqm = dev->dqm;\n\tpdd->qpd.pqm = &p->pqm;\n\tpdd->qpd.evicted = 0;\n\tpdd->qpd.mapped_gws_queue = false;\n\tpdd->process = p;\n\tpdd->bound = PDD_UNBOUND;\n\tpdd->already_dequeued = false;\n\tpdd->runtime_inuse = false;\n\tpdd->vram_usage = 0;\n\tpdd->sdma_past_activity_counter = 0;\n\tpdd->user_gpu_id = dev->id;\n\tatomic64_set(&pdd->evict_duration_counter, 0);\n\n\tif (dev->kfd->shared_resources.enable_mes) {\n\t\tretval = amdgpu_amdkfd_alloc_gtt_mem(dev->adev,\n\t\t\t\t\t\tAMDGPU_MES_PROC_CTX_SIZE,\n\t\t\t\t\t\t&pdd->proc_ctx_bo,\n\t\t\t\t\t\t&pdd->proc_ctx_gpu_addr,\n\t\t\t\t\t\t&pdd->proc_ctx_cpu_ptr,\n\t\t\t\t\t\tfalse);\n\t\tif (retval) {\n\t\t\tpr_err(\"failed to allocate process context bo\\n\");\n\t\t\tgoto err_free_pdd;\n\t\t}\n\t\tmemset(pdd->proc_ctx_cpu_ptr, 0, AMDGPU_MES_PROC_CTX_SIZE);\n\t}\n\n\tp->pdds[p->n_pdds++] = pdd;\n\tif (kfd_dbg_is_per_vmid_supported(pdd->dev))\n\t\tpdd->spi_dbg_override = pdd->dev->kfd2kgd->disable_debug_trap(\n\t\t\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\t\t\tfalse,\n\t\t\t\t\t\t\t0);\n\n\t \n\tidr_init(&pdd->alloc_idr);\n\n\treturn pdd;\n\nerr_free_pdd:\n\tkfree(pdd);\n\treturn NULL;\n}\n\n \nint kfd_process_device_init_vm(struct kfd_process_device *pdd,\n\t\t\t       struct file *drm_file)\n{\n\tstruct amdgpu_fpriv *drv_priv;\n\tstruct amdgpu_vm *avm;\n\tstruct kfd_process *p;\n\tstruct kfd_node *dev;\n\tint ret;\n\n\tif (!drm_file)\n\t\treturn -EINVAL;\n\n\tif (pdd->drm_priv)\n\t\treturn -EBUSY;\n\n\tret = amdgpu_file_to_fpriv(drm_file, &drv_priv);\n\tif (ret)\n\t\treturn ret;\n\tavm = &drv_priv->vm;\n\n\tp = pdd->process;\n\tdev = pdd->dev;\n\n\tret = amdgpu_amdkfd_gpuvm_acquire_process_vm(dev->adev, avm,\n\t\t\t\t\t\t     &p->kgd_process_info,\n\t\t\t\t\t\t     &p->ef);\n\tif (ret) {\n\t\tpr_err(\"Failed to create process VM object\\n\");\n\t\treturn ret;\n\t}\n\tpdd->drm_priv = drm_file->private_data;\n\tatomic64_set(&pdd->tlb_seq, 0);\n\n\tret = kfd_process_device_reserve_ib_mem(pdd);\n\tif (ret)\n\t\tgoto err_reserve_ib_mem;\n\tret = kfd_process_device_init_cwsr_dgpu(pdd);\n\tif (ret)\n\t\tgoto err_init_cwsr;\n\n\tret = amdgpu_amdkfd_gpuvm_set_vm_pasid(dev->adev, avm, p->pasid);\n\tif (ret)\n\t\tgoto err_set_pasid;\n\n\tpdd->drm_file = drm_file;\n\n\treturn 0;\n\nerr_set_pasid:\n\tkfd_process_device_destroy_cwsr_dgpu(pdd);\nerr_init_cwsr:\n\tkfd_process_device_destroy_ib_mem(pdd);\nerr_reserve_ib_mem:\n\tpdd->drm_priv = NULL;\n\tamdgpu_amdkfd_gpuvm_destroy_cb(dev->adev, avm);\n\n\treturn ret;\n}\n\n \nstruct kfd_process_device *kfd_bind_process_to_device(struct kfd_node *dev,\n\t\t\t\t\t\t\tstruct kfd_process *p)\n{\n\tstruct kfd_process_device *pdd;\n\tint err;\n\n\tpdd = kfd_get_process_device_data(dev, p);\n\tif (!pdd) {\n\t\tpr_err(\"Process device data doesn't exist\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (!pdd->drm_priv)\n\t\treturn ERR_PTR(-ENODEV);\n\n\t \n\tif (!pdd->runtime_inuse) {\n\t\terr = pm_runtime_get_sync(adev_to_drm(dev->adev)->dev);\n\t\tif (err < 0) {\n\t\t\tpm_runtime_put_autosuspend(adev_to_drm(dev->adev)->dev);\n\t\t\treturn ERR_PTR(err);\n\t\t}\n\t}\n\n\t \n\tpdd->runtime_inuse = true;\n\n\treturn pdd;\n}\n\n \nint kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,\n\t\t\t\t\tvoid *mem)\n{\n\treturn idr_alloc(&pdd->alloc_idr, mem, 0, 0, GFP_KERNEL);\n}\n\n \nvoid *kfd_process_device_translate_handle(struct kfd_process_device *pdd,\n\t\t\t\t\tint handle)\n{\n\tif (handle < 0)\n\t\treturn NULL;\n\n\treturn idr_find(&pdd->alloc_idr, handle);\n}\n\n \nvoid kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,\n\t\t\t\t\tint handle)\n{\n\tif (handle >= 0)\n\t\tidr_remove(&pdd->alloc_idr, handle);\n}\n\n \nstruct kfd_process *kfd_lookup_process_by_pasid(u32 pasid)\n{\n\tstruct kfd_process *p, *ret_p = NULL;\n\tunsigned int temp;\n\n\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\n\thash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {\n\t\tif (p->pasid == pasid) {\n\t\t\tkref_get(&p->ref);\n\t\t\tret_p = p;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\n\treturn ret_p;\n}\n\n \nstruct kfd_process *kfd_lookup_process_by_mm(const struct mm_struct *mm)\n{\n\tstruct kfd_process *p;\n\n\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\n\tp = find_process_by_mm(mm);\n\tif (p)\n\t\tkref_get(&p->ref);\n\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\n\treturn p;\n}\n\n \nint kfd_process_evict_queues(struct kfd_process *p, uint32_t trigger)\n{\n\tint r = 0;\n\tint i;\n\tunsigned int n_evicted = 0;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tkfd_smi_event_queue_eviction(pdd->dev, p->lead_thread->pid,\n\t\t\t\t\t     trigger);\n\n\t\tr = pdd->dev->dqm->ops.evict_process_queues(pdd->dev->dqm,\n\t\t\t\t\t\t\t    &pdd->qpd);\n\t\t \n\t\tif (r && r != -EIO) {\n\t\t\tpr_err(\"Failed to evict process queues\\n\");\n\t\t\tgoto fail;\n\t\t}\n\t\tn_evicted++;\n\t}\n\n\treturn r;\n\nfail:\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tif (n_evicted == 0)\n\t\t\tbreak;\n\n\t\tkfd_smi_event_queue_restore(pdd->dev, p->lead_thread->pid);\n\n\t\tif (pdd->dev->dqm->ops.restore_process_queues(pdd->dev->dqm,\n\t\t\t\t\t\t\t      &pdd->qpd))\n\t\t\tpr_err(\"Failed to restore queues\\n\");\n\n\t\tn_evicted--;\n\t}\n\n\treturn r;\n}\n\n \nint kfd_process_restore_queues(struct kfd_process *p)\n{\n\tint r, ret = 0;\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tkfd_smi_event_queue_restore(pdd->dev, p->lead_thread->pid);\n\n\t\tr = pdd->dev->dqm->ops.restore_process_queues(pdd->dev->dqm,\n\t\t\t\t\t\t\t      &pdd->qpd);\n\t\tif (r) {\n\t\t\tpr_err(\"Failed to restore process queues\\n\");\n\t\t\tif (!ret)\n\t\t\t\tret = r;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nint kfd_process_gpuidx_from_gpuid(struct kfd_process *p, uint32_t gpu_id)\n{\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++)\n\t\tif (p->pdds[i] && gpu_id == p->pdds[i]->user_gpu_id)\n\t\t\treturn i;\n\treturn -EINVAL;\n}\n\nint\nkfd_process_gpuid_from_node(struct kfd_process *p, struct kfd_node *node,\n\t\t\t    uint32_t *gpuid, uint32_t *gpuidx)\n{\n\tint i;\n\n\tfor (i = 0; i < p->n_pdds; i++)\n\t\tif (p->pdds[i] && p->pdds[i]->dev == node) {\n\t\t\t*gpuid = p->pdds[i]->user_gpu_id;\n\t\t\t*gpuidx = i;\n\t\t\treturn 0;\n\t\t}\n\treturn -EINVAL;\n}\n\nstatic void evict_process_worker(struct work_struct *work)\n{\n\tint ret;\n\tstruct kfd_process *p;\n\tstruct delayed_work *dwork;\n\n\tdwork = to_delayed_work(work);\n\n\t \n\tp = container_of(dwork, struct kfd_process, eviction_work);\n\tWARN_ONCE(p->last_eviction_seqno != p->ef->seqno,\n\t\t  \"Eviction fence mismatch\\n\");\n\n\t \n\tflush_delayed_work(&p->restore_work);\n\n\tpr_debug(\"Started evicting pasid 0x%x\\n\", p->pasid);\n\tret = kfd_process_evict_queues(p, KFD_QUEUE_EVICTION_TRIGGER_TTM);\n\tif (!ret) {\n\t\tdma_fence_signal(p->ef);\n\t\tdma_fence_put(p->ef);\n\t\tp->ef = NULL;\n\t\tqueue_delayed_work(kfd_restore_wq, &p->restore_work,\n\t\t\t\tmsecs_to_jiffies(PROCESS_RESTORE_TIME_MS));\n\n\t\tpr_debug(\"Finished evicting pasid 0x%x\\n\", p->pasid);\n\t} else\n\t\tpr_err(\"Failed to evict queues of pasid 0x%x\\n\", p->pasid);\n}\n\nstatic void restore_process_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork;\n\tstruct kfd_process *p;\n\tint ret = 0;\n\n\tdwork = to_delayed_work(work);\n\n\t \n\tp = container_of(dwork, struct kfd_process, restore_work);\n\tpr_debug(\"Started restoring pasid 0x%x\\n\", p->pasid);\n\n\t \n\n\tp->last_restore_timestamp = get_jiffies_64();\n\t \n\tif (p->kgd_process_info)\n\t\tret = amdgpu_amdkfd_gpuvm_restore_process_bos(p->kgd_process_info,\n\t\t\t\t\t\t\t     &p->ef);\n\tif (ret) {\n\t\tpr_debug(\"Failed to restore BOs of pasid 0x%x, retry after %d ms\\n\",\n\t\t\t p->pasid, PROCESS_BACK_OFF_TIME_MS);\n\t\tret = queue_delayed_work(kfd_restore_wq, &p->restore_work,\n\t\t\t\tmsecs_to_jiffies(PROCESS_BACK_OFF_TIME_MS));\n\t\tWARN(!ret, \"reschedule restore work failed\\n\");\n\t\treturn;\n\t}\n\n\tret = kfd_process_restore_queues(p);\n\tif (!ret)\n\t\tpr_debug(\"Finished restoring pasid 0x%x\\n\", p->pasid);\n\telse\n\t\tpr_err(\"Failed to restore queues of pasid 0x%x\\n\", p->pasid);\n}\n\nvoid kfd_suspend_all_processes(void)\n{\n\tstruct kfd_process *p;\n\tunsigned int temp;\n\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\n\tWARN(debug_evictions, \"Evicting all processes\");\n\thash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {\n\t\tcancel_delayed_work_sync(&p->eviction_work);\n\t\tflush_delayed_work(&p->restore_work);\n\n\t\tif (kfd_process_evict_queues(p, KFD_QUEUE_EVICTION_TRIGGER_SUSPEND))\n\t\t\tpr_err(\"Failed to suspend process 0x%x\\n\", p->pasid);\n\t\tdma_fence_signal(p->ef);\n\t\tdma_fence_put(p->ef);\n\t\tp->ef = NULL;\n\t}\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n}\n\nint kfd_resume_all_processes(void)\n{\n\tstruct kfd_process *p;\n\tunsigned int temp;\n\tint ret = 0, idx = srcu_read_lock(&kfd_processes_srcu);\n\n\thash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {\n\t\tif (!queue_delayed_work(kfd_restore_wq, &p->restore_work, 0)) {\n\t\t\tpr_err(\"Restore process %d failed during resume\\n\",\n\t\t\t       p->pasid);\n\t\t\tret = -EFAULT;\n\t\t}\n\t}\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\treturn ret;\n}\n\nint kfd_reserved_mem_mmap(struct kfd_node *dev, struct kfd_process *process,\n\t\t\t  struct vm_area_struct *vma)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct qcm_process_device *qpd;\n\n\tif ((vma->vm_end - vma->vm_start) != KFD_CWSR_TBA_TMA_SIZE) {\n\t\tpr_err(\"Incorrect CWSR mapping size.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tpdd = kfd_get_process_device_data(dev, process);\n\tif (!pdd)\n\t\treturn -EINVAL;\n\tqpd = &pdd->qpd;\n\n\tqpd->cwsr_kaddr = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\tget_order(KFD_CWSR_TBA_TMA_SIZE));\n\tif (!qpd->cwsr_kaddr) {\n\t\tpr_err(\"Error allocating per process CWSR buffer.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tvm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND\n\t\t| VM_NORESERVE | VM_DONTDUMP | VM_PFNMAP);\n\t \n\treturn remap_pfn_range(vma, vma->vm_start,\n\t\t\t       PFN_DOWN(__pa(qpd->cwsr_kaddr)),\n\t\t\t       KFD_CWSR_TBA_TMA_SIZE, vma->vm_page_prot);\n}\n\nvoid kfd_flush_tlb(struct kfd_process_device *pdd, enum TLB_FLUSH_TYPE type)\n{\n\tstruct amdgpu_vm *vm = drm_priv_to_vm(pdd->drm_priv);\n\tuint64_t tlb_seq = amdgpu_vm_tlb_seq(vm);\n\tstruct kfd_node *dev = pdd->dev;\n\tuint32_t xcc_mask = dev->xcc_mask;\n\tint xcc = 0;\n\n\t \n\tif (atomic64_xchg(&pdd->tlb_seq, tlb_seq) == tlb_seq)\n\t\treturn;\n\n\tif (dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\t \n\t\tif (pdd->qpd.vmid)\n\t\t\tamdgpu_amdkfd_flush_gpu_tlb_vmid(dev->adev,\n\t\t\t\t\t\t\tpdd->qpd.vmid);\n\t} else {\n\t\tfor_each_inst(xcc, xcc_mask)\n\t\t\tamdgpu_amdkfd_flush_gpu_tlb_pasid(\n\t\t\t\tdev->adev, pdd->process->pasid, type, xcc);\n\t}\n}\n\n \nint kfd_process_drain_interrupts(struct kfd_process_device *pdd)\n{\n\tuint32_t irq_drain_fence[8];\n\tuint8_t node_id = 0;\n\tint r = 0;\n\n\tif (!KFD_IS_SOC15(pdd->dev))\n\t\treturn 0;\n\n\tpdd->process->irq_drain_is_open = true;\n\n\tmemset(irq_drain_fence, 0, sizeof(irq_drain_fence));\n\tirq_drain_fence[0] = (KFD_IRQ_FENCE_SOURCEID << 8) |\n\t\t\t\t\t\t\tKFD_IRQ_FENCE_CLIENTID;\n\tirq_drain_fence[3] = pdd->process->pasid;\n\n\t \n\tif (KFD_GC_VERSION(pdd->dev->kfd) == IP_VERSION(9, 4, 3)) {\n\t\tnode_id = ffs(pdd->dev->interrupt_bitmap) - 1;\n\t\tirq_drain_fence[3] |= node_id << 16;\n\t}\n\n\t \n\tif (amdgpu_amdkfd_send_close_event_drain_irq(pdd->dev->adev,\n\t\t\t\t\t\t     irq_drain_fence)) {\n\t\tpdd->process->irq_drain_is_open = false;\n\t\treturn 0;\n\t}\n\n\tr = wait_event_interruptible(pdd->process->wait_irq_drain,\n\t\t\t\t     !READ_ONCE(pdd->process->irq_drain_is_open));\n\tif (r)\n\t\tpdd->process->irq_drain_is_open = false;\n\n\treturn r;\n}\n\nvoid kfd_process_close_interrupt_drain(unsigned int pasid)\n{\n\tstruct kfd_process *p;\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\n\tif (!p)\n\t\treturn;\n\n\tWRITE_ONCE(p->irq_drain_is_open, false);\n\twake_up_all(&p->wait_irq_drain);\n\tkfd_unref_process(p);\n}\n\nstruct send_exception_work_handler_workarea {\n\tstruct work_struct work;\n\tstruct kfd_process *p;\n\tunsigned int queue_id;\n\tuint64_t error_reason;\n};\n\nstatic void send_exception_work_handler(struct work_struct *work)\n{\n\tstruct send_exception_work_handler_workarea *workarea;\n\tstruct kfd_process *p;\n\tstruct queue *q;\n\tstruct mm_struct *mm;\n\tstruct kfd_context_save_area_header __user *csa_header;\n\tuint64_t __user *err_payload_ptr;\n\tuint64_t cur_err;\n\tuint32_t ev_id;\n\n\tworkarea = container_of(work,\n\t\t\t\tstruct send_exception_work_handler_workarea,\n\t\t\t\twork);\n\tp = workarea->p;\n\n\tmm = get_task_mm(p->lead_thread);\n\n\tif (!mm)\n\t\treturn;\n\n\tkthread_use_mm(mm);\n\n\tq = pqm_get_user_queue(&p->pqm, workarea->queue_id);\n\n\tif (!q)\n\t\tgoto out;\n\n\tcsa_header = (void __user *)q->properties.ctx_save_restore_area_address;\n\n\tget_user(err_payload_ptr, (uint64_t __user **)&csa_header->err_payload_addr);\n\tget_user(cur_err, err_payload_ptr);\n\tcur_err |= workarea->error_reason;\n\tput_user(cur_err, err_payload_ptr);\n\tget_user(ev_id, &csa_header->err_event_id);\n\n\tkfd_set_event(p, ev_id);\n\nout:\n\tkthread_unuse_mm(mm);\n\tmmput(mm);\n}\n\nint kfd_send_exception_to_runtime(struct kfd_process *p,\n\t\t\tunsigned int queue_id,\n\t\t\tuint64_t error_reason)\n{\n\tstruct send_exception_work_handler_workarea worker;\n\n\tINIT_WORK_ONSTACK(&worker.work, send_exception_work_handler);\n\n\tworker.p = p;\n\tworker.queue_id = queue_id;\n\tworker.error_reason = error_reason;\n\n\tschedule_work(&worker.work);\n\tflush_work(&worker.work);\n\tdestroy_work_on_stack(&worker.work);\n\n\treturn 0;\n}\n\nstruct kfd_process_device *kfd_process_device_data_by_id(struct kfd_process *p, uint32_t gpu_id)\n{\n\tint i;\n\n\tif (gpu_id) {\n\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\t\tif (pdd->user_gpu_id == gpu_id)\n\t\t\t\treturn pdd;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nint kfd_process_get_user_gpu_id(struct kfd_process *p, uint32_t actual_gpu_id)\n{\n\tint i;\n\n\tif (!actual_gpu_id)\n\t\treturn 0;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tif (pdd->dev->id == actual_gpu_id)\n\t\t\treturn pdd->user_gpu_id;\n\t}\n\treturn -EINVAL;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_mqds_by_process(struct seq_file *m, void *data)\n{\n\tstruct kfd_process *p;\n\tunsigned int temp;\n\tint r = 0;\n\n\tint idx = srcu_read_lock(&kfd_processes_srcu);\n\n\thash_for_each_rcu(kfd_processes_table, temp, p, kfd_processes) {\n\t\tseq_printf(m, \"Process %d PASID 0x%x:\\n\",\n\t\t\t   p->lead_thread->tgid, p->pasid);\n\n\t\tmutex_lock(&p->mutex);\n\t\tr = pqm_debugfs_mqds(m, &p->pqm);\n\t\tmutex_unlock(&p->mutex);\n\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\tsrcu_read_unlock(&kfd_processes_srcu, idx);\n\n\treturn r;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}