{
  "module_name": "kfd_svm.h",
  "hash_id": "0fa232943224b5e94e0b56d6459d55df5a23359df39f2762ef0149758fc995a7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_svm.h",
  "human_readable_source": " \n \n\n#ifndef KFD_SVM_H_\n#define KFD_SVM_H_\n\n#if IS_ENABLED(CONFIG_HSA_AMD_SVM)\n\n#include <linux/rwsem.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/sched/mm.h>\n#include <linux/hmm.h>\n#include \"amdgpu.h\"\n#include \"kfd_priv.h\"\n\n#define SVM_RANGE_VRAM_DOMAIN (1UL << 0)\n#define SVM_ADEV_PGMAP_OWNER(adev)\\\n\t\t\t((adev)->hive ? (void *)(adev)->hive : (void *)(adev))\n\nstruct svm_range_bo {\n\tstruct amdgpu_bo\t\t*bo;\n\tstruct kref\t\t\tkref;\n\tstruct list_head\t\trange_list;  \n\tspinlock_t\t\t\tlist_lock;\n\tstruct amdgpu_amdkfd_fence\t*eviction_fence;\n\tstruct work_struct\t\teviction_work;\n\tuint32_t\t\t\tevicting;\n\tstruct work_struct\t\trelease_work;\n\tstruct kfd_node\t\t\t*node;\n};\n\nenum svm_work_list_ops {\n\tSVM_OP_NULL,\n\tSVM_OP_UNMAP_RANGE,\n\tSVM_OP_UPDATE_RANGE_NOTIFIER,\n\tSVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP,\n\tSVM_OP_ADD_RANGE,\n\tSVM_OP_ADD_RANGE_AND_MAP\n};\n\nstruct svm_work_list_item {\n\tenum svm_work_list_ops op;\n\tstruct mm_struct *mm;\n};\n\n \nstruct svm_range {\n\tstruct svm_range_list\t\t*svms;\n\tstruct mutex\t\t\tmigrate_mutex;\n\tunsigned long\t\t\tstart;\n\tunsigned long\t\t\tlast;\n\tstruct interval_tree_node\tit_node;\n\tstruct list_head\t\tlist;\n\tstruct list_head\t\tupdate_list;\n\tuint64_t\t\t\tnpages;\n\tdma_addr_t\t\t\t*dma_addr[MAX_GPU_INSTANCE];\n\tstruct ttm_resource\t\t*ttm_res;\n\tuint64_t\t\t\toffset;\n\tstruct svm_range_bo\t\t*svm_bo;\n\tstruct list_head\t\tsvm_bo_list;\n\tstruct mutex                    lock;\n\tunsigned int                    saved_flags;\n\tuint32_t\t\t\tflags;\n\tuint32_t\t\t\tpreferred_loc;\n\tuint32_t\t\t\tprefetch_loc;\n\tuint32_t\t\t\tactual_loc;\n\tuint8_t\t\t\t\tgranularity;\n\tatomic_t\t\t\tinvalid;\n\tktime_t\t\t\t\tvalidate_timestamp;\n\tstruct mmu_interval_notifier\tnotifier;\n\tstruct svm_work_list_item\twork_item;\n\tstruct list_head\t\tdeferred_list;\n\tstruct list_head\t\tchild_list;\n\tDECLARE_BITMAP(bitmap_access, MAX_GPU_INSTANCE);\n\tDECLARE_BITMAP(bitmap_aip, MAX_GPU_INSTANCE);\n\tbool\t\t\t\tmapped_to_gpu;\n};\n\nstatic inline void svm_range_lock(struct svm_range *prange)\n{\n\tmutex_lock(&prange->lock);\n\tprange->saved_flags = memalloc_noreclaim_save();\n\n}\nstatic inline void svm_range_unlock(struct svm_range *prange)\n{\n\tmemalloc_noreclaim_restore(prange->saved_flags);\n\tmutex_unlock(&prange->lock);\n}\n\nstatic inline struct svm_range_bo *svm_range_bo_ref(struct svm_range_bo *svm_bo)\n{\n\tif (svm_bo)\n\t\tkref_get(&svm_bo->kref);\n\n\treturn svm_bo;\n}\n\nint svm_range_list_init(struct kfd_process *p);\nvoid svm_range_list_fini(struct kfd_process *p);\nint svm_ioctl(struct kfd_process *p, enum kfd_ioctl_svm_op op, uint64_t start,\n\t      uint64_t size, uint32_t nattrs,\n\t      struct kfd_ioctl_svm_attribute *attrs);\nstruct svm_range *svm_range_from_addr(struct svm_range_list *svms,\n\t\t\t\t      unsigned long addr,\n\t\t\t\t      struct svm_range **parent);\nstruct kfd_node *svm_range_get_node_by_id(struct svm_range *prange,\n\t\t\t\t\t  uint32_t gpu_id);\nint svm_range_vram_node_new(struct kfd_node *node, struct svm_range *prange,\n\t\t\t    bool clear);\nvoid svm_range_vram_node_free(struct svm_range *prange);\nint svm_range_split_by_granularity(struct kfd_process *p, struct mm_struct *mm,\n\t\t\t       unsigned long addr, struct svm_range *parent,\n\t\t\t       struct svm_range *prange);\nint svm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\t    uint32_t vmid, uint32_t node_id, uint64_t addr,\n\t\t\t    bool write_fault);\nint svm_range_schedule_evict_svm_bo(struct amdgpu_amdkfd_fence *fence);\nvoid svm_range_add_list_work(struct svm_range_list *svms,\n\t\t\t     struct svm_range *prange, struct mm_struct *mm,\n\t\t\t     enum svm_work_list_ops op);\nvoid schedule_deferred_list_work(struct svm_range_list *svms);\nvoid svm_range_dma_unmap(struct device *dev, dma_addr_t *dma_addr,\n\t\t\t unsigned long offset, unsigned long npages);\nvoid svm_range_free_dma_mappings(struct svm_range *prange, bool unmap_dma);\nint svm_range_get_info(struct kfd_process *p, uint32_t *num_svm_ranges,\n\t\t       uint64_t *svm_priv_data_size);\nint kfd_criu_checkpoint_svm(struct kfd_process *p,\n\t\t\t    uint8_t __user *user_priv_data,\n\t\t\t    uint64_t *priv_offset);\nint kfd_criu_restore_svm(struct kfd_process *p,\n\t\t\t uint8_t __user *user_priv_ptr,\n\t\t\t uint64_t *priv_data_offset,\n\t\t\t uint64_t max_priv_data_size);\nint kfd_criu_resume_svm(struct kfd_process *p);\nstruct kfd_process_device *\nsvm_range_get_pdd_by_node(struct svm_range *prange, struct kfd_node *node);\nvoid svm_range_list_lock_and_flush_work(struct svm_range_list *svms, struct mm_struct *mm);\n\n \n#define KFD_IS_SVM_API_SUPPORTED(adev) ((adev)->kfd.pgmap.type != 0 ||\\\n\t\t\t\t\t(adev)->gmc.is_app_apu)\n\nvoid svm_range_bo_unref_async(struct svm_range_bo *svm_bo);\n\nvoid svm_range_set_max_pages(struct amdgpu_device *adev);\nint svm_range_switch_xnack_reserve_mem(struct kfd_process *p, bool xnack_enabled);\n\n#else\n\nstruct kfd_process;\n\nstatic inline int svm_range_list_init(struct kfd_process *p)\n{\n\treturn 0;\n}\nstatic inline void svm_range_list_fini(struct kfd_process *p)\n{\n\t \n}\n\nstatic inline int svm_range_restore_pages(struct amdgpu_device *adev,\n\t\t\t\t\t  unsigned int pasid,\n\t\t\t\t\t  uint32_t client_id, uint32_t node_id,\n\t\t\t\t\t  uint64_t addr, bool write_fault)\n{\n\treturn -EFAULT;\n}\n\nstatic inline int svm_range_schedule_evict_svm_bo(\n\t\tstruct amdgpu_amdkfd_fence *fence)\n{\n\tWARN_ONCE(1, \"SVM eviction fence triggered, but SVM is disabled\");\n\treturn -EINVAL;\n}\n\nstatic inline int svm_range_get_info(struct kfd_process *p,\n\t\t\t\t     uint32_t *num_svm_ranges,\n\t\t\t\t     uint64_t *svm_priv_data_size)\n{\n\t*num_svm_ranges = 0;\n\t*svm_priv_data_size = 0;\n\treturn 0;\n}\n\nstatic inline int kfd_criu_checkpoint_svm(struct kfd_process *p,\n\t\t\t\t\t  uint8_t __user *user_priv_data,\n\t\t\t\t\t  uint64_t *priv_offset)\n{\n\treturn 0;\n}\n\nstatic inline int kfd_criu_restore_svm(struct kfd_process *p,\n\t\t\t\t       uint8_t __user *user_priv_ptr,\n\t\t\t\t       uint64_t *priv_data_offset,\n\t\t\t\t       uint64_t max_priv_data_size)\n{\n\treturn -EINVAL;\n}\n\nstatic inline int kfd_criu_resume_svm(struct kfd_process *p)\n{\n\treturn 0;\n}\n\nstatic inline void svm_range_set_max_pages(struct amdgpu_device *adev)\n{\n}\n\n#define KFD_IS_SVM_API_SUPPORTED(dev) false\n\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}