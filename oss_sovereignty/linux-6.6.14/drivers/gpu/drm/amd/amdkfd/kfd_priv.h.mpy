{
  "module_name": "kfd_priv.h",
  "hash_id": "fea80b11d42ac3b3c4c5079c4b1db2b3183ec8bc9b4ffb2008492ea0b7b989cf",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_priv.h",
  "human_readable_source": " \n \n\n#ifndef KFD_PRIV_H_INCLUDED\n#define KFD_PRIV_H_INCLUDED\n\n#include <linux/hashtable.h>\n#include <linux/mmu_notifier.h>\n#include <linux/memremap.h>\n#include <linux/mutex.h>\n#include <linux/types.h>\n#include <linux/atomic.h>\n#include <linux/workqueue.h>\n#include <linux/spinlock.h>\n#include <linux/kfd_ioctl.h>\n#include <linux/idr.h>\n#include <linux/kfifo.h>\n#include <linux/seq_file.h>\n#include <linux/kref.h>\n#include <linux/sysfs.h>\n#include <linux/device_cgroup.h>\n#include <drm/drm_file.h>\n#include <drm/drm_drv.h>\n#include <drm/drm_device.h>\n#include <drm/drm_ioctl.h>\n#include <kgd_kfd_interface.h>\n#include <linux/swap.h>\n\n#include \"amd_shared.h\"\n#include \"amdgpu.h\"\n\n#define KFD_MAX_RING_ENTRY_SIZE\t8\n\n#define KFD_SYSFS_FILE_MODE 0444\n\n \n#define KFD_GPU_ID_HASH_WIDTH 16\n\n \n#define KFD_MMAP_TYPE_SHIFT\t62\n#define KFD_MMAP_TYPE_MASK\t(0x3ULL << KFD_MMAP_TYPE_SHIFT)\n#define KFD_MMAP_TYPE_DOORBELL\t(0x3ULL << KFD_MMAP_TYPE_SHIFT)\n#define KFD_MMAP_TYPE_EVENTS\t(0x2ULL << KFD_MMAP_TYPE_SHIFT)\n#define KFD_MMAP_TYPE_RESERVED_MEM\t(0x1ULL << KFD_MMAP_TYPE_SHIFT)\n#define KFD_MMAP_TYPE_MMIO\t(0x0ULL << KFD_MMAP_TYPE_SHIFT)\n\n#define KFD_MMAP_GPU_ID_SHIFT 46\n#define KFD_MMAP_GPU_ID_MASK (((1ULL << KFD_GPU_ID_HASH_WIDTH) - 1) \\\n\t\t\t\t<< KFD_MMAP_GPU_ID_SHIFT)\n#define KFD_MMAP_GPU_ID(gpu_id) ((((uint64_t)gpu_id) << KFD_MMAP_GPU_ID_SHIFT)\\\n\t\t\t\t& KFD_MMAP_GPU_ID_MASK)\n#define KFD_MMAP_GET_GPU_ID(offset)    ((offset & KFD_MMAP_GPU_ID_MASK) \\\n\t\t\t\t>> KFD_MMAP_GPU_ID_SHIFT)\n\n \n#define KFD_CIK_HIQ_PIPE 4\n#define KFD_CIK_HIQ_QUEUE 0\n\n \n#define kfd_alloc_struct(ptr_to_struct)\t\\\n\t((typeof(ptr_to_struct)) kzalloc(sizeof(*ptr_to_struct), GFP_KERNEL))\n\n#define KFD_MAX_NUM_OF_PROCESSES 512\n#define KFD_MAX_NUM_OF_QUEUES_PER_PROCESS 1024\n\n \n#define KFD_CWSR_TBA_TMA_SIZE (PAGE_SIZE * 2)\n#define KFD_CWSR_TMA_OFFSET PAGE_SIZE\n\n#define KFD_MAX_NUM_OF_QUEUES_PER_DEVICE\t\t\\\n\t(KFD_MAX_NUM_OF_PROCESSES *\t\t\t\\\n\t\t\tKFD_MAX_NUM_OF_QUEUES_PER_PROCESS)\n\n#define KFD_KERNEL_QUEUE_SIZE 2048\n\n#define KFD_UNMAP_LATENCY_MS\t(4000)\n\n#define KFD_MAX_SDMA_QUEUES\t128\n\n \n#define KFD_QUEUE_DOORBELL_MIRROR_OFFSET 512\n\n \nenum kfd_ioctl_flags {\n\t \n\tKFD_IOC_FLAG_CHECKPOINT_RESTORE = BIT(0),\n};\n \nextern int max_num_of_queues_per_device;\n\n\n \nextern int sched_policy;\n\n \nextern int hws_max_conc_proc;\n\nextern int cwsr_enable;\n\n \nextern int send_sigterm;\n\n \nextern int debug_largebar;\n\n \nextern int amdgpu_noretry;\n\n \nextern int halt_if_hws_hang;\n\n \nextern bool hws_gws_support;\n\n \nextern int queue_preemption_timeout_ms;\n\n \nextern int amdgpu_no_queue_eviction_on_vm_fault;\n\n \nextern bool debug_evictions;\n\nextern struct mutex kfd_processes_mutex;\n\nenum cache_policy {\n\tcache_policy_coherent,\n\tcache_policy_noncoherent\n};\n\n#define KFD_GC_VERSION(dev) ((dev)->adev->ip_versions[GC_HWIP][0])\n#define KFD_IS_SOC15(dev)   ((KFD_GC_VERSION(dev)) >= (IP_VERSION(9, 0, 1)))\n#define KFD_SUPPORT_XNACK_PER_PROCESS(dev)\\\n\t((KFD_GC_VERSION(dev) == IP_VERSION(9, 4, 2)) ||\t\\\n\t (KFD_GC_VERSION(dev) == IP_VERSION(9, 4, 3)))\n\nstruct kfd_node;\n\nstruct kfd_event_interrupt_class {\n\tbool (*interrupt_isr)(struct kfd_node *dev,\n\t\t\tconst uint32_t *ih_ring_entry, uint32_t *patched_ihre,\n\t\t\tbool *patched_flag);\n\tvoid (*interrupt_wq)(struct kfd_node *dev,\n\t\t\tconst uint32_t *ih_ring_entry);\n};\n\nstruct kfd_device_info {\n\tuint32_t gfx_target_version;\n\tconst struct kfd_event_interrupt_class *event_interrupt_class;\n\tunsigned int max_pasid_bits;\n\tunsigned int max_no_of_hqd;\n\tunsigned int doorbell_size;\n\tsize_t ih_ring_entry_size;\n\tuint8_t num_of_watch_points;\n\tuint16_t mqd_size_aligned;\n\tbool supports_cwsr;\n\tbool needs_pci_atomics;\n\tuint32_t no_atomic_fw_version;\n\tunsigned int num_sdma_queues_per_engine;\n\tunsigned int num_reserved_sdma_queues_per_engine;\n\tDECLARE_BITMAP(reserved_sdma_queues_bitmap, KFD_MAX_SDMA_QUEUES);\n};\n\nunsigned int kfd_get_num_sdma_engines(struct kfd_node *kdev);\nunsigned int kfd_get_num_xgmi_sdma_engines(struct kfd_node *kdev);\n\nstruct kfd_mem_obj {\n\tuint32_t range_start;\n\tuint32_t range_end;\n\tuint64_t gpu_addr;\n\tuint32_t *cpu_ptr;\n\tvoid *gtt_mem;\n};\n\nstruct kfd_vmid_info {\n\tuint32_t first_vmid_kfd;\n\tuint32_t last_vmid_kfd;\n\tuint32_t vmid_num_kfd;\n};\n\n#define MAX_KFD_NODES\t8\n\nstruct kfd_dev;\n\nstruct kfd_node {\n\tunsigned int node_id;\n\tstruct amdgpu_device *adev;      \n\tconst struct kfd2kgd_calls *kfd2kgd;  \n\tstruct kfd_vmid_info vm_info;\n\tunsigned int id;                 \n\tuint32_t xcc_mask;  \n\tstruct amdgpu_xcp *xcp;\n\n\t \n\tstruct kfifo ih_fifo;\n\tstruct workqueue_struct *ih_wq;\n\tstruct work_struct interrupt_work;\n\tspinlock_t interrupt_lock;\n\n\t \n\tbool interrupts_active;\n\tuint32_t interrupt_bitmap;  \n\n\t \n\tstruct device_queue_manager *dqm;\n\n\t \n\tvoid *gws;\n\tbool gws_debug_workaround;\n\n\t \n\tstruct list_head smi_clients;\n\tspinlock_t smi_lock;\n\tuint32_t reset_seq_num;\n\n\t \n\tatomic_t sram_ecc_flag;\n\n\t \n\tunsigned int spm_pasid;\n\n\t \n\tunsigned int max_proc_per_quantum;\n\n\tunsigned int compute_vmid_bitmap;\n\n\tstruct kfd_local_mem_info local_mem_info;\n\n\tstruct kfd_dev *kfd;\n};\n\nstruct kfd_dev {\n\tstruct amdgpu_device *adev;\n\n\tstruct kfd_device_info device_info;\n\n\tu32 __iomem *doorbell_kernel_ptr;  \n\n\tstruct kgd2kfd_shared_resources shared_resources;\n\n\tconst struct kfd2kgd_calls *kfd2kgd;\n\tstruct mutex doorbell_mutex;\n\n\tvoid *gtt_mem;\n\tuint64_t gtt_start_gpu_addr;\n\tvoid *gtt_start_cpu_ptr;\n\tvoid *gtt_sa_bitmap;\n\tstruct mutex gtt_sa_lock;\n\tunsigned int gtt_sa_chunk_size;\n\tunsigned int gtt_sa_num_of_chunks;\n\n\tbool init_complete;\n\n\t \n\tuint16_t mec_fw_version;\n\tuint16_t mec2_fw_version;\n\tuint16_t sdma_fw_version;\n\n\t \n\tbool cwsr_enabled;\n\tconst void *cwsr_isa;\n\tunsigned int cwsr_isa_size;\n\n\t \n\tuint64_t hive_id;\n\n\tbool pci_atomic_requested;\n\n\t \n\tatomic_t compute_profile;\n\n\tstruct ida doorbell_ida;\n\tunsigned int max_doorbell_slices;\n\n\tint noretry;\n\n\tstruct kfd_node *nodes[MAX_KFD_NODES];\n\tunsigned int num_nodes;\n\n\t \n\tuint32_t alloc_watch_ids;\n\tspinlock_t watch_points_lock;\n\n\t \n\tstruct amdgpu_bo *doorbells;\n\n\t \n\tunsigned long *doorbell_bitmap;\n};\n\nenum kfd_mempool {\n\tKFD_MEMPOOL_SYSTEM_CACHEABLE = 1,\n\tKFD_MEMPOOL_SYSTEM_WRITECOMBINE = 2,\n\tKFD_MEMPOOL_FRAMEBUFFER = 3,\n};\n\n \nint kfd_chardev_init(void);\nvoid kfd_chardev_exit(void);\n\n \nenum kfd_unmap_queues_filter {\n\tKFD_UNMAP_QUEUES_FILTER_ALL_QUEUES = 1,\n\tKFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES = 2,\n\tKFD_UNMAP_QUEUES_FILTER_BY_PASID = 3\n};\n\n \nenum kfd_queue_type  {\n\tKFD_QUEUE_TYPE_COMPUTE,\n\tKFD_QUEUE_TYPE_SDMA,\n\tKFD_QUEUE_TYPE_HIQ,\n\tKFD_QUEUE_TYPE_DIQ,\n\tKFD_QUEUE_TYPE_SDMA_XGMI\n};\n\nenum kfd_queue_format {\n\tKFD_QUEUE_FORMAT_PM4,\n\tKFD_QUEUE_FORMAT_AQL\n};\n\nenum KFD_QUEUE_PRIORITY {\n\tKFD_QUEUE_PRIORITY_MINIMUM = 0,\n\tKFD_QUEUE_PRIORITY_MAXIMUM = 15\n};\n\n \n\nstruct queue_properties {\n\tenum kfd_queue_type type;\n\tenum kfd_queue_format format;\n\tunsigned int queue_id;\n\tuint64_t queue_address;\n\tuint64_t  queue_size;\n\tuint32_t priority;\n\tuint32_t queue_percent;\n\tuint32_t *read_ptr;\n\tuint32_t *write_ptr;\n\tvoid __iomem *doorbell_ptr;\n\tuint32_t doorbell_off;\n\tbool is_interop;\n\tbool is_evicted;\n\tbool is_suspended;\n\tbool is_being_destroyed;\n\tbool is_active;\n\tbool is_gws;\n\tuint32_t pm4_target_xcc;\n\tbool is_dbg_wa;\n\tbool is_user_cu_masked;\n\t \n\tunsigned int vmid;\n\t \n\tuint32_t sdma_engine_id;\n\tuint32_t sdma_queue_id;\n\tuint32_t sdma_vm_addr;\n\t \n\tuint64_t eop_ring_buffer_address;\n\tuint32_t eop_ring_buffer_size;\n\tuint64_t ctx_save_restore_area_address;\n\tuint32_t ctx_save_restore_area_size;\n\tuint32_t ctl_stack_size;\n\tuint64_t tba_addr;\n\tuint64_t tma_addr;\n\tuint64_t exception_status;\n};\n\n#define QUEUE_IS_ACTIVE(q) ((q).queue_size > 0 &&\t\\\n\t\t\t    (q).queue_address != 0 &&\t\\\n\t\t\t    (q).queue_percent > 0 &&\t\\\n\t\t\t    !(q).is_evicted &&\t\t\\\n\t\t\t    !(q).is_suspended)\n\nenum mqd_update_flag {\n\tUPDATE_FLAG_DBG_WA_ENABLE = 1,\n\tUPDATE_FLAG_DBG_WA_DISABLE = 2,\n};\n\nstruct mqd_update_info {\n\tunion {\n\t\tstruct {\n\t\t\tuint32_t count;  \n\t\t\tuint32_t *ptr;\n\t\t} cu_mask;\n\t};\n\tenum mqd_update_flag update_flag;\n};\n\n \n\nstruct queue {\n\tstruct list_head list;\n\tvoid *mqd;\n\tstruct kfd_mem_obj *mqd_mem_obj;\n\tuint64_t gart_mqd_addr;\n\tstruct queue_properties properties;\n\n\tuint32_t mec;\n\tuint32_t pipe;\n\tuint32_t queue;\n\n\tunsigned int sdma_id;\n\tunsigned int doorbell_id;\n\n\tstruct kfd_process\t*process;\n\tstruct kfd_node\t\t*device;\n\tvoid *gws;\n\n\t \n\tstruct kobject kobj;\n\n\tvoid *gang_ctx_bo;\n\tuint64_t gang_ctx_gpu_addr;\n\tvoid *gang_ctx_cpu_ptr;\n\n\tstruct amdgpu_bo *wptr_bo;\n};\n\nenum KFD_MQD_TYPE {\n\tKFD_MQD_TYPE_HIQ = 0,\t\t \n\tKFD_MQD_TYPE_CP,\t\t \n\tKFD_MQD_TYPE_SDMA,\t\t \n\tKFD_MQD_TYPE_DIQ,\t\t \n\tKFD_MQD_TYPE_MAX\n};\n\nenum KFD_PIPE_PRIORITY {\n\tKFD_PIPE_PRIORITY_CS_LOW = 0,\n\tKFD_PIPE_PRIORITY_CS_MEDIUM,\n\tKFD_PIPE_PRIORITY_CS_HIGH\n};\n\nstruct scheduling_resources {\n\tunsigned int vmid_mask;\n\tenum kfd_queue_type type;\n\tuint64_t queue_mask;\n\tuint64_t gws_mask;\n\tuint32_t oac_mask;\n\tuint32_t gds_heap_base;\n\tuint32_t gds_heap_size;\n};\n\nstruct process_queue_manager {\n\t \n\tstruct kfd_process\t*process;\n\tstruct list_head\tqueues;\n\tunsigned long\t\t*queue_slot_bitmap;\n};\n\nstruct qcm_process_device {\n\t \n\tstruct device_queue_manager *dqm;\n\tstruct process_queue_manager *pqm;\n\t \n\tstruct list_head queues_list;\n\tstruct list_head priv_queue_list;\n\n\tunsigned int queue_count;\n\tunsigned int vmid;\n\tbool is_debug;\n\tunsigned int evicted;  \n\n\t \n\tbool reset_wavefronts;\n\n\t \n\tbool mapped_gws_queue;\n\n\t \n\tuint64_t gds_context_area;\n\t \n\tuint64_t page_table_base;\n\tuint32_t sh_mem_config;\n\tuint32_t sh_mem_bases;\n\tuint32_t sh_mem_ape1_base;\n\tuint32_t sh_mem_ape1_limit;\n\tuint32_t gds_size;\n\tuint32_t num_gws;\n\tuint32_t num_oac;\n\tuint32_t sh_hidden_private_base;\n\n\t \n\tstruct kgd_mem *cwsr_mem;\n\tvoid *cwsr_kaddr;\n\tuint64_t cwsr_base;\n\tuint64_t tba_addr;\n\tuint64_t tma_addr;\n\n\t \n\tstruct kgd_mem *ib_mem;\n\tuint64_t ib_base;\n\tvoid *ib_kaddr;\n\n\t \n\tstruct amdgpu_bo *proc_doorbells;\n\n\t \n\tunsigned long *doorbell_bitmap;\n};\n\n \n\n \n#define PROCESS_RESTORE_TIME_MS 100\n \n#define PROCESS_BACK_OFF_TIME_MS 100\n \n#define PROCESS_ACTIVE_TIME_MS 10\n\n \n#define MAKE_HANDLE(gpu_id, idr_handle) \\\n\t(((uint64_t)(gpu_id) << 32) + idr_handle)\n#define GET_GPU_ID(handle) (handle >> 32)\n#define GET_IDR_HANDLE(handle) (handle & 0xFFFFFFFF)\n\nenum kfd_pdd_bound {\n\tPDD_UNBOUND = 0,\n\tPDD_BOUND,\n\tPDD_BOUND_SUSPENDED,\n};\n\n#define MAX_SYSFS_FILENAME_LEN 15\n\n \n#define SDMA_ACTIVITY_DIVISOR  100\n\n \nstruct kfd_process_device {\n\t \n\tstruct kfd_node *dev;\n\n\t \n\tstruct kfd_process *process;\n\n\t \n\tstruct qcm_process_device qpd;\n\n\t \n\tuint64_t lds_base;\n\tuint64_t lds_limit;\n\tuint64_t gpuvm_base;\n\tuint64_t gpuvm_limit;\n\tuint64_t scratch_base;\n\tuint64_t scratch_limit;\n\n\t \n\tstruct file *drm_file;\n\tvoid *drm_priv;\n\tatomic64_t tlb_seq;\n\n\t \n\tstruct idr alloc_idr;\n\n\t \n\tbool already_dequeued;\n\tbool runtime_inuse;\n\n\t \n\tenum kfd_pdd_bound bound;\n\n\t \n\tuint64_t vram_usage;\n\tstruct attribute attr_vram;\n\tchar vram_filename[MAX_SYSFS_FILENAME_LEN];\n\n\t \n\tuint64_t sdma_past_activity_counter;\n\tstruct attribute attr_sdma;\n\tchar sdma_filename[MAX_SYSFS_FILENAME_LEN];\n\n\t \n\tuint64_t last_evict_timestamp;\n\tatomic64_t evict_duration_counter;\n\tstruct attribute attr_evict;\n\n\tstruct kobject *kobj_stats;\n\n\t \n\tstruct attribute attr_cu_occupancy;\n\n\t \n\tstruct kobject *kobj_counters;\n\tstruct attribute attr_faults;\n\tstruct attribute attr_page_in;\n\tstruct attribute attr_page_out;\n\tuint64_t faults;\n\tuint64_t page_in;\n\tuint64_t page_out;\n\n\t \n\tuint64_t exception_status;\n\tvoid *vm_fault_exc_data;\n\tsize_t vm_fault_exc_data_size;\n\n\t \n\tuint32_t spi_dbg_override;\n\tuint32_t spi_dbg_launch_mode;\n\tuint32_t watch_points[4];\n\tuint32_t alloc_watch_ids;\n\n\t \n\tuint32_t user_gpu_id;\n\n\tvoid *proc_ctx_bo;\n\tuint64_t proc_ctx_gpu_addr;\n\tvoid *proc_ctx_cpu_ptr;\n};\n\n#define qpd_to_pdd(x) container_of(x, struct kfd_process_device, qpd)\n\nstruct svm_range_list {\n\tstruct mutex\t\t\tlock;\n\tstruct rb_root_cached\t\tobjects;\n\tstruct list_head\t\tlist;\n\tstruct work_struct\t\tdeferred_list_work;\n\tstruct list_head\t\tdeferred_range_list;\n\tstruct list_head                criu_svm_metadata_list;\n\tspinlock_t\t\t\tdeferred_list_lock;\n\tatomic_t\t\t\tevicted_ranges;\n\tatomic_t\t\t\tdrain_pagefaults;\n\tstruct delayed_work\t\trestore_work;\n\tDECLARE_BITMAP(bitmap_supported, MAX_GPU_INSTANCE);\n\tstruct task_struct\t\t*faulting_task;\n};\n\n \nstruct kfd_process {\n\t \n\tstruct hlist_node kfd_processes;\n\n\t \n\tvoid *mm;\n\n\tstruct kref ref;\n\tstruct work_struct release_work;\n\n\tstruct mutex mutex;\n\n\t \n\tstruct task_struct *lead_thread;\n\n\t \n\tstruct mmu_notifier mmu_notifier;\n\n\tu32 pasid;\n\n\t \n\tstruct kfd_process_device *pdds[MAX_GPU_INSTANCE];\n\tuint32_t n_pdds;\n\n\tstruct process_queue_manager pqm;\n\n\t \n\tbool is_32bit_user_mode;\n\n\t \n\tstruct mutex event_mutex;\n\t \n\tstruct idr event_idr;\n\t \n\tu64 signal_handle;\n\tstruct kfd_signal_page *signal_page;\n\tsize_t signal_mapped_size;\n\tsize_t signal_event_count;\n\tbool signal_event_limit_reached;\n\n\t \n\tvoid *kgd_process_info;\n\t \n\tstruct dma_fence *ef;\n\n\t \n\tstruct delayed_work eviction_work;\n\tstruct delayed_work restore_work;\n\t \n\tunsigned int last_eviction_seqno;\n\t \n\tunsigned long last_restore_timestamp;\n\n\t \n\tbool debug_trap_enabled;\n\n\t \n\tstruct file *dbg_ev_file;\n\n\t \n\tatomic_t debugged_process_count;\n\n\t \n\tstruct kfd_process *debugger_process;\n\n\t \n\tstruct kobject *kobj;\n\tstruct kobject *kobj_queues;\n\tstruct attribute attr_pasid;\n\n\t \n\tbool has_cwsr;\n\n\t \n\tuint64_t exception_enable_mask;\n\tuint64_t exception_status;\n\n\t \n\twait_queue_head_t wait_irq_drain;\n\tbool irq_drain_is_open;\n\n\t \n\tstruct svm_range_list svms;\n\n\tbool xnack_enabled;\n\n\t \n\tstruct work_struct debug_event_workarea;\n\n\t \n\tu32 dbg_flags;\n\n\tatomic_t poison;\n\t \n\tbool queues_paused;\n\n\t \n\tstruct semaphore runtime_enable_sema;\n\tbool is_runtime_retry;\n\tstruct kfd_runtime_info runtime_info;\n};\n\n#define KFD_PROCESS_TABLE_SIZE 5  \nextern DECLARE_HASHTABLE(kfd_processes_table, KFD_PROCESS_TABLE_SIZE);\nextern struct srcu_struct kfd_processes_srcu;\n\n \ntypedef int amdkfd_ioctl_t(struct file *filep, struct kfd_process *p,\n\t\t\t\tvoid *data);\n\nstruct amdkfd_ioctl_desc {\n\tunsigned int cmd;\n\tint flags;\n\tamdkfd_ioctl_t *func;\n\tunsigned int cmd_drv;\n\tconst char *name;\n};\nbool kfd_dev_is_large_bar(struct kfd_node *dev);\n\nint kfd_process_create_wq(void);\nvoid kfd_process_destroy_wq(void);\nvoid kfd_cleanup_processes(void);\nstruct kfd_process *kfd_create_process(struct task_struct *thread);\nstruct kfd_process *kfd_get_process(const struct task_struct *task);\nstruct kfd_process *kfd_lookup_process_by_pasid(u32 pasid);\nstruct kfd_process *kfd_lookup_process_by_mm(const struct mm_struct *mm);\n\nint kfd_process_gpuidx_from_gpuid(struct kfd_process *p, uint32_t gpu_id);\nint kfd_process_gpuid_from_node(struct kfd_process *p, struct kfd_node *node,\n\t\t\t\tuint32_t *gpuid, uint32_t *gpuidx);\nstatic inline int kfd_process_gpuid_from_gpuidx(struct kfd_process *p,\n\t\t\t\tuint32_t gpuidx, uint32_t *gpuid) {\n\treturn gpuidx < p->n_pdds ? p->pdds[gpuidx]->dev->id : -EINVAL;\n}\nstatic inline struct kfd_process_device *kfd_process_device_from_gpuidx(\n\t\t\t\tstruct kfd_process *p, uint32_t gpuidx) {\n\treturn gpuidx < p->n_pdds ? p->pdds[gpuidx] : NULL;\n}\n\nvoid kfd_unref_process(struct kfd_process *p);\nint kfd_process_evict_queues(struct kfd_process *p, uint32_t trigger);\nint kfd_process_restore_queues(struct kfd_process *p);\nvoid kfd_suspend_all_processes(void);\nint kfd_resume_all_processes(void);\n\nstruct kfd_process_device *kfd_process_device_data_by_id(struct kfd_process *process,\n\t\t\t\t\t\t\t uint32_t gpu_id);\n\nint kfd_process_get_user_gpu_id(struct kfd_process *p, uint32_t actual_gpu_id);\n\nint kfd_process_device_init_vm(struct kfd_process_device *pdd,\n\t\t\t       struct file *drm_file);\nstruct kfd_process_device *kfd_bind_process_to_device(struct kfd_node *dev,\n\t\t\t\t\t\tstruct kfd_process *p);\nstruct kfd_process_device *kfd_get_process_device_data(struct kfd_node *dev,\n\t\t\t\t\t\t\tstruct kfd_process *p);\nstruct kfd_process_device *kfd_create_process_device_data(struct kfd_node *dev,\n\t\t\t\t\t\t\tstruct kfd_process *p);\n\nbool kfd_process_xnack_mode(struct kfd_process *p, bool supported);\n\nint kfd_reserved_mem_mmap(struct kfd_node *dev, struct kfd_process *process,\n\t\t\t  struct vm_area_struct *vma);\n\n \nint kfd_process_device_create_obj_handle(struct kfd_process_device *pdd,\n\t\t\t\t\tvoid *mem);\nvoid *kfd_process_device_translate_handle(struct kfd_process_device *p,\n\t\t\t\t\tint handle);\nvoid kfd_process_device_remove_obj_handle(struct kfd_process_device *pdd,\n\t\t\t\t\tint handle);\nstruct kfd_process *kfd_lookup_process_by_pid(struct pid *pid);\n\n \nint kfd_pasid_init(void);\nvoid kfd_pasid_exit(void);\nbool kfd_set_pasid_limit(unsigned int new_limit);\nunsigned int kfd_get_pasid_limit(void);\nu32 kfd_pasid_alloc(void);\nvoid kfd_pasid_free(u32 pasid);\n\n \nsize_t kfd_doorbell_process_slice(struct kfd_dev *kfd);\nint kfd_doorbell_init(struct kfd_dev *kfd);\nvoid kfd_doorbell_fini(struct kfd_dev *kfd);\nint kfd_doorbell_mmap(struct kfd_node *dev, struct kfd_process *process,\n\t\t      struct vm_area_struct *vma);\nvoid __iomem *kfd_get_kernel_doorbell(struct kfd_dev *kfd,\n\t\t\t\t\tunsigned int *doorbell_off);\nvoid kfd_release_kernel_doorbell(struct kfd_dev *kfd, u32 __iomem *db_addr);\nu32 read_kernel_doorbell(u32 __iomem *db);\nvoid write_kernel_doorbell(void __iomem *db, u32 value);\nvoid write_kernel_doorbell64(void __iomem *db, u64 value);\nunsigned int kfd_get_doorbell_dw_offset_in_bar(struct kfd_dev *kfd,\n\t\t\t\t\tstruct kfd_process_device *pdd,\n\t\t\t\t\tunsigned int doorbell_id);\nphys_addr_t kfd_get_process_doorbells(struct kfd_process_device *pdd);\nint kfd_alloc_process_doorbells(struct kfd_dev *kfd,\n\t\t\t\tstruct kfd_process_device *pdd);\nvoid kfd_free_process_doorbells(struct kfd_dev *kfd,\n\t\t\t\tstruct kfd_process_device *pdd);\n \n\nint kfd_gtt_sa_allocate(struct kfd_node *node, unsigned int size,\n\t\t\tstruct kfd_mem_obj **mem_obj);\n\nint kfd_gtt_sa_free(struct kfd_node *node, struct kfd_mem_obj *mem_obj);\n\nextern struct device *kfd_device;\n\n \nvoid kfd_procfs_init(void);\nvoid kfd_procfs_shutdown(void);\nint kfd_procfs_add_queue(struct queue *q);\nvoid kfd_procfs_del_queue(struct queue *q);\n\n \nint kfd_topology_init(void);\nvoid kfd_topology_shutdown(void);\nint kfd_topology_add_device(struct kfd_node *gpu);\nint kfd_topology_remove_device(struct kfd_node *gpu);\nstruct kfd_topology_device *kfd_topology_device_by_proximity_domain(\n\t\t\t\t\t\tuint32_t proximity_domain);\nstruct kfd_topology_device *kfd_topology_device_by_proximity_domain_no_lock(\n\t\t\t\t\t\tuint32_t proximity_domain);\nstruct kfd_topology_device *kfd_topology_device_by_id(uint32_t gpu_id);\nstruct kfd_node *kfd_device_by_id(uint32_t gpu_id);\nstruct kfd_node *kfd_device_by_pci_dev(const struct pci_dev *pdev);\nstatic inline bool kfd_irq_is_from_node(struct kfd_node *node, uint32_t node_id,\n\t\t\t\t\tuint32_t vmid)\n{\n\treturn (node->interrupt_bitmap & (1 << node_id)) != 0 &&\n\t       (node->compute_vmid_bitmap & (1 << vmid)) != 0;\n}\nstatic inline struct kfd_node *kfd_node_by_irq_ids(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_id, uint32_t vmid) {\n\tstruct kfd_dev *dev = adev->kfd.dev;\n\tuint32_t i;\n\n\tif (KFD_GC_VERSION(dev) != IP_VERSION(9, 4, 3))\n\t\treturn dev->nodes[0];\n\n\tfor (i = 0; i < dev->num_nodes; i++)\n\t\tif (kfd_irq_is_from_node(dev->nodes[i], node_id, vmid))\n\t\t\treturn dev->nodes[i];\n\n\treturn NULL;\n}\nint kfd_topology_enum_kfd_devices(uint8_t idx, struct kfd_node **kdev);\nint kfd_numa_node_to_apic_id(int numa_node_id);\n\n \n#define\tKFD_IRQ_FENCE_CLIENTID\t0xff\n#define\tKFD_IRQ_FENCE_SOURCEID\t0xff\n#define\tKFD_IRQ_IS_FENCE(client, source)\t\t\t\t\\\n\t\t\t\t((client) == KFD_IRQ_FENCE_CLIENTID &&\t\\\n\t\t\t\t(source) == KFD_IRQ_FENCE_SOURCEID)\nint kfd_interrupt_init(struct kfd_node *dev);\nvoid kfd_interrupt_exit(struct kfd_node *dev);\nbool enqueue_ih_ring_entry(struct kfd_node *kfd, const void *ih_ring_entry);\nbool interrupt_is_wanted(struct kfd_node *dev,\n\t\t\t\tconst uint32_t *ih_ring_entry,\n\t\t\t\tuint32_t *patched_ihre, bool *flag);\nint kfd_process_drain_interrupts(struct kfd_process_device *pdd);\nvoid kfd_process_close_interrupt_drain(unsigned int pasid);\n\n \nint kfd_init_apertures(struct kfd_process *process);\n\nvoid kfd_process_set_trap_handler(struct qcm_process_device *qpd,\n\t\t\t\t  uint64_t tba_addr,\n\t\t\t\t  uint64_t tma_addr);\nvoid kfd_process_set_trap_debug_flag(struct qcm_process_device *qpd,\n\t\t\t\t     bool enabled);\n\n \nint kfd_process_init_cwsr_apu(struct kfd_process *process, struct file *filep);\n\n \n \n\n#define KFD_CRIU_PRIV_VERSION 1\n\nstruct kfd_criu_process_priv_data {\n\tuint32_t version;\n\tuint32_t xnack_mode;\n};\n\nstruct kfd_criu_device_priv_data {\n\t \n\tuint64_t reserved;\n};\n\nstruct kfd_criu_bo_priv_data {\n\tuint64_t user_addr;\n\tuint32_t idr_handle;\n\tuint32_t mapped_gpuids[MAX_GPU_INSTANCE];\n};\n\n \nenum kfd_criu_object_type {\n\tKFD_CRIU_OBJECT_TYPE_QUEUE,\n\tKFD_CRIU_OBJECT_TYPE_EVENT,\n\tKFD_CRIU_OBJECT_TYPE_SVM_RANGE,\n};\n\nstruct kfd_criu_svm_range_priv_data {\n\tuint32_t object_type;\n\tuint64_t start_addr;\n\tuint64_t size;\n\t \n\tstruct kfd_ioctl_svm_attribute attrs[];\n};\n\nstruct kfd_criu_queue_priv_data {\n\tuint32_t object_type;\n\tuint64_t q_address;\n\tuint64_t q_size;\n\tuint64_t read_ptr_addr;\n\tuint64_t write_ptr_addr;\n\tuint64_t doorbell_off;\n\tuint64_t eop_ring_buffer_address;\n\tuint64_t ctx_save_restore_area_address;\n\tuint32_t gpu_id;\n\tuint32_t type;\n\tuint32_t format;\n\tuint32_t q_id;\n\tuint32_t priority;\n\tuint32_t q_percent;\n\tuint32_t doorbell_id;\n\tuint32_t gws;\n\tuint32_t sdma_id;\n\tuint32_t eop_ring_buffer_size;\n\tuint32_t ctx_save_restore_area_size;\n\tuint32_t ctl_stack_size;\n\tuint32_t mqd_size;\n};\n\nstruct kfd_criu_event_priv_data {\n\tuint32_t object_type;\n\tuint64_t user_handle;\n\tuint32_t event_id;\n\tuint32_t auto_reset;\n\tuint32_t type;\n\tuint32_t signaled;\n\n\tunion {\n\t\tstruct kfd_hsa_memory_exception_data memory_exception_data;\n\t\tstruct kfd_hsa_hw_exception_data hw_exception_data;\n\t};\n};\n\nint kfd_process_get_queue_info(struct kfd_process *p,\n\t\t\t       uint32_t *num_queues,\n\t\t\t       uint64_t *priv_data_sizes);\n\nint kfd_criu_checkpoint_queues(struct kfd_process *p,\n\t\t\t uint8_t __user *user_priv_data,\n\t\t\t uint64_t *priv_data_offset);\n\nint kfd_criu_restore_queue(struct kfd_process *p,\n\t\t\t   uint8_t __user *user_priv_data,\n\t\t\t   uint64_t *priv_data_offset,\n\t\t\t   uint64_t max_priv_data_size);\n\nint kfd_criu_checkpoint_events(struct kfd_process *p,\n\t\t\t uint8_t __user *user_priv_data,\n\t\t\t uint64_t *priv_data_offset);\n\nint kfd_criu_restore_event(struct file *devkfd,\n\t\t\t   struct kfd_process *p,\n\t\t\t   uint8_t __user *user_priv_data,\n\t\t\t   uint64_t *priv_data_offset,\n\t\t\t   uint64_t max_priv_data_size);\n \n\n \nint init_queue(struct queue **q, const struct queue_properties *properties);\nvoid uninit_queue(struct queue *q);\nvoid print_queue_properties(struct queue_properties *q);\nvoid print_queue(struct queue *q);\n\nstruct mqd_manager *mqd_manager_init_cik(enum KFD_MQD_TYPE type,\n\t\tstruct kfd_node *dev);\nstruct mqd_manager *mqd_manager_init_vi(enum KFD_MQD_TYPE type,\n\t\tstruct kfd_node *dev);\nstruct mqd_manager *mqd_manager_init_v9(enum KFD_MQD_TYPE type,\n\t\tstruct kfd_node *dev);\nstruct mqd_manager *mqd_manager_init_v10(enum KFD_MQD_TYPE type,\n\t\tstruct kfd_node *dev);\nstruct mqd_manager *mqd_manager_init_v11(enum KFD_MQD_TYPE type,\n\t\tstruct kfd_node *dev);\nstruct device_queue_manager *device_queue_manager_init(struct kfd_node *dev);\nvoid device_queue_manager_uninit(struct device_queue_manager *dqm);\nstruct kernel_queue *kernel_queue_init(struct kfd_node *dev,\n\t\t\t\t\tenum kfd_queue_type type);\nvoid kernel_queue_uninit(struct kernel_queue *kq, bool hanging);\nint kfd_dqm_evict_pasid(struct device_queue_manager *dqm, u32 pasid);\n\n \nstruct process_queue_node {\n\tstruct queue *q;\n\tstruct kernel_queue *kq;\n\tstruct list_head process_queue_list;\n};\n\nvoid kfd_process_dequeue_from_device(struct kfd_process_device *pdd);\nvoid kfd_process_dequeue_from_all_devices(struct kfd_process *p);\nint pqm_init(struct process_queue_manager *pqm, struct kfd_process *p);\nvoid pqm_uninit(struct process_queue_manager *pqm);\nint pqm_create_queue(struct process_queue_manager *pqm,\n\t\t\t    struct kfd_node *dev,\n\t\t\t    struct file *f,\n\t\t\t    struct queue_properties *properties,\n\t\t\t    unsigned int *qid,\n\t\t\t    struct amdgpu_bo *wptr_bo,\n\t\t\t    const struct kfd_criu_queue_priv_data *q_data,\n\t\t\t    const void *restore_mqd,\n\t\t\t    const void *restore_ctl_stack,\n\t\t\t    uint32_t *p_doorbell_offset_in_process);\nint pqm_destroy_queue(struct process_queue_manager *pqm, unsigned int qid);\nint pqm_update_queue_properties(struct process_queue_manager *pqm, unsigned int qid,\n\t\t\tstruct queue_properties *p);\nint pqm_update_mqd(struct process_queue_manager *pqm, unsigned int qid,\n\t\t\tstruct mqd_update_info *minfo);\nint pqm_set_gws(struct process_queue_manager *pqm, unsigned int qid,\n\t\t\tvoid *gws);\nstruct kernel_queue *pqm_get_kernel_queue(struct process_queue_manager *pqm,\n\t\t\t\t\t\tunsigned int qid);\nstruct queue *pqm_get_user_queue(struct process_queue_manager *pqm,\n\t\t\t\t\t\tunsigned int qid);\nint pqm_get_wave_state(struct process_queue_manager *pqm,\n\t\t       unsigned int qid,\n\t\t       void __user *ctl_stack,\n\t\t       u32 *ctl_stack_used_size,\n\t\t       u32 *save_area_used_size);\nint pqm_get_queue_snapshot(struct process_queue_manager *pqm,\n\t\t\t   uint64_t exception_clear_mask,\n\t\t\t   void __user *buf,\n\t\t\t   int *num_qss_entries,\n\t\t\t   uint32_t *entry_size);\n\nint amdkfd_fence_wait_timeout(uint64_t *fence_addr,\n\t\t\t      uint64_t fence_value,\n\t\t\t      unsigned int timeout_ms);\n\nint pqm_get_queue_checkpoint_info(struct process_queue_manager *pqm,\n\t\t\t\t  unsigned int qid,\n\t\t\t\t  u32 *mqd_size,\n\t\t\t\t  u32 *ctl_stack_size);\n \n\n#define KFD_FENCE_COMPLETED (100)\n#define KFD_FENCE_INIT   (10)\n\nstruct packet_manager {\n\tstruct device_queue_manager *dqm;\n\tstruct kernel_queue *priv_queue;\n\tstruct mutex lock;\n\tbool allocated;\n\tstruct kfd_mem_obj *ib_buffer_obj;\n\tunsigned int ib_size_bytes;\n\tbool is_over_subscription;\n\n\tconst struct packet_manager_funcs *pmf;\n};\n\nstruct packet_manager_funcs {\n\t \n\tint (*map_process)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tstruct qcm_process_device *qpd);\n\tint (*runlist)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tuint64_t ib, size_t ib_size_in_dwords, bool chain);\n\tint (*set_resources)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tstruct scheduling_resources *res);\n\tint (*map_queues)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tstruct queue *q, bool is_static);\n\tint (*unmap_queues)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tenum kfd_unmap_queues_filter mode,\n\t\t\tuint32_t filter_param, bool reset);\n\tint (*set_grace_period)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tuint32_t grace_period);\n\tint (*query_status)(struct packet_manager *pm, uint32_t *buffer,\n\t\t\tuint64_t fence_address,\tuint64_t fence_value);\n\tint (*release_mem)(uint64_t gpu_addr, uint32_t *buffer);\n\n\t \n\tint map_process_size;\n\tint runlist_size;\n\tint set_resources_size;\n\tint map_queues_size;\n\tint unmap_queues_size;\n\tint set_grace_period_size;\n\tint query_status_size;\n\tint release_mem_size;\n};\n\nextern const struct packet_manager_funcs kfd_vi_pm_funcs;\nextern const struct packet_manager_funcs kfd_v9_pm_funcs;\nextern const struct packet_manager_funcs kfd_aldebaran_pm_funcs;\n\nint pm_init(struct packet_manager *pm, struct device_queue_manager *dqm);\nvoid pm_uninit(struct packet_manager *pm, bool hanging);\nint pm_send_set_resources(struct packet_manager *pm,\n\t\t\t\tstruct scheduling_resources *res);\nint pm_send_runlist(struct packet_manager *pm, struct list_head *dqm_queues);\nint pm_send_query_status(struct packet_manager *pm, uint64_t fence_address,\n\t\t\t\tuint64_t fence_value);\n\nint pm_send_unmap_queue(struct packet_manager *pm,\n\t\t\tenum kfd_unmap_queues_filter mode,\n\t\t\tuint32_t filter_param, bool reset);\n\nvoid pm_release_ib(struct packet_manager *pm);\n\nint pm_update_grace_period(struct packet_manager *pm, uint32_t grace_period);\n\n \nunsigned int pm_build_pm4_header(unsigned int opcode, size_t packet_size);\n\nuint64_t kfd_get_number_elems(struct kfd_dev *kfd);\n\n \nextern const struct kfd_event_interrupt_class event_interrupt_class_cik;\nextern const struct kfd_event_interrupt_class event_interrupt_class_v9;\nextern const struct kfd_event_interrupt_class event_interrupt_class_v9_4_3;\nextern const struct kfd_event_interrupt_class event_interrupt_class_v10;\nextern const struct kfd_event_interrupt_class event_interrupt_class_v11;\n\nextern const struct kfd_device_global_init_class device_global_init_class_cik;\n\nint kfd_event_init_process(struct kfd_process *p);\nvoid kfd_event_free_process(struct kfd_process *p);\nint kfd_event_mmap(struct kfd_process *process, struct vm_area_struct *vma);\nint kfd_wait_on_events(struct kfd_process *p,\n\t\t       uint32_t num_events, void __user *data,\n\t\t       bool all, uint32_t *user_timeout_ms,\n\t\t       uint32_t *wait_result);\nvoid kfd_signal_event_interrupt(u32 pasid, uint32_t partial_id,\n\t\t\t\tuint32_t valid_id_bits);\nvoid kfd_signal_hw_exception_event(u32 pasid);\nint kfd_set_event(struct kfd_process *p, uint32_t event_id);\nint kfd_reset_event(struct kfd_process *p, uint32_t event_id);\nint kfd_kmap_event_page(struct kfd_process *p, uint64_t event_page_offset);\n\nint kfd_event_create(struct file *devkfd, struct kfd_process *p,\n\t\t     uint32_t event_type, bool auto_reset, uint32_t node_id,\n\t\t     uint32_t *event_id, uint32_t *event_trigger_data,\n\t\t     uint64_t *event_page_offset, uint32_t *event_slot_index);\n\nint kfd_get_num_events(struct kfd_process *p);\nint kfd_event_destroy(struct kfd_process *p, uint32_t event_id);\n\nvoid kfd_signal_vm_fault_event(struct kfd_node *dev, u32 pasid,\n\t\t\t\tstruct kfd_vm_fault_info *info,\n\t\t\t\tstruct kfd_hsa_memory_exception_data *data);\n\nvoid kfd_signal_reset_event(struct kfd_node *dev);\n\nvoid kfd_signal_poison_consumed_event(struct kfd_node *dev, u32 pasid);\n\nvoid kfd_flush_tlb(struct kfd_process_device *pdd, enum TLB_FLUSH_TYPE type);\n\nstatic inline bool kfd_flush_tlb_after_unmap(struct kfd_dev *dev)\n{\n\treturn KFD_GC_VERSION(dev) > IP_VERSION(9, 4, 2) ||\n\t       (KFD_GC_VERSION(dev) == IP_VERSION(9, 4, 1) && dev->sdma_fw_version >= 18) ||\n\t       KFD_GC_VERSION(dev) == IP_VERSION(9, 4, 0);\n}\n\nint kfd_send_exception_to_runtime(struct kfd_process *p,\n\t\t\t\tunsigned int queue_id,\n\t\t\t\tuint64_t error_reason);\nbool kfd_is_locked(void);\n\n \nvoid kfd_inc_compute_active(struct kfd_node *dev);\nvoid kfd_dec_compute_active(struct kfd_node *dev);\n\n \n \nstatic inline int kfd_devcgroup_check_permission(struct kfd_node *kfd)\n{\n#if defined(CONFIG_CGROUP_DEVICE) || defined(CONFIG_CGROUP_BPF)\n\tstruct drm_device *ddev = adev_to_drm(kfd->adev);\n\n\treturn devcgroup_check_permission(DEVCG_DEV_CHAR, DRM_MAJOR,\n\t\t\t\t\t  ddev->render->index,\n\t\t\t\t\t  DEVCG_ACC_WRITE | DEVCG_ACC_READ);\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool kfd_is_first_node(struct kfd_node *node)\n{\n\treturn (node == node->kfd->nodes[0]);\n}\n\n \n#if defined(CONFIG_DEBUG_FS)\n\nvoid kfd_debugfs_init(void);\nvoid kfd_debugfs_fini(void);\nint kfd_debugfs_mqds_by_process(struct seq_file *m, void *data);\nint pqm_debugfs_mqds(struct seq_file *m, void *data);\nint kfd_debugfs_hqds_by_device(struct seq_file *m, void *data);\nint dqm_debugfs_hqds(struct seq_file *m, void *data);\nint kfd_debugfs_rls_by_device(struct seq_file *m, void *data);\nint pm_debugfs_runlist(struct seq_file *m, void *data);\n\nint kfd_debugfs_hang_hws(struct kfd_node *dev);\nint pm_debugfs_hang_hws(struct packet_manager *pm);\nint dqm_debugfs_hang_hws(struct device_queue_manager *dqm);\n\n#else\n\nstatic inline void kfd_debugfs_init(void) {}\nstatic inline void kfd_debugfs_fini(void) {}\n\n#endif\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}