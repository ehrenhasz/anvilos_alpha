{
  "module_name": "kfd_device_queue_manager.c",
  "hash_id": "8a55366d28dce8c8c48e6e8521b0eadc57bea214d481d63b0ebdbac06cafcb72",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_device_queue_manager.c",
  "human_readable_source": "\n \n\n#include <linux/ratelimit.h>\n#include <linux/printk.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/types.h>\n#include <linux/bitops.h>\n#include <linux/sched.h>\n#include \"kfd_priv.h\"\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_mqd_manager.h\"\n#include \"cik_regs.h\"\n#include \"kfd_kernel_queue.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"mes_api_def.h\"\n#include \"kfd_debug.h\"\n\n \n#define CIK_HPD_EOP_BYTES_LOG2 11\n#define CIK_HPD_EOP_BYTES (1U << CIK_HPD_EOP_BYTES_LOG2)\n\nstatic int set_pasid_vmid_mapping(struct device_queue_manager *dqm,\n\t\t\t\t  u32 pasid, unsigned int vmid);\n\nstatic int execute_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\tenum kfd_unmap_queues_filter filter,\n\t\t\t\tuint32_t filter_param,\n\t\t\t\tuint32_t grace_period);\nstatic int unmap_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\tenum kfd_unmap_queues_filter filter,\n\t\t\t\tuint32_t filter_param,\n\t\t\t\tuint32_t grace_period,\n\t\t\t\tbool reset);\n\nstatic int map_queues_cpsch(struct device_queue_manager *dqm);\n\nstatic void deallocate_sdma_queue(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q);\n\nstatic inline void deallocate_hqd(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q);\nstatic int allocate_hqd(struct device_queue_manager *dqm, struct queue *q);\nstatic int allocate_sdma_queue(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q, const uint32_t *restore_sdma_id);\nstatic void kfd_process_hw_exception(struct work_struct *work);\n\nstatic inline\nenum KFD_MQD_TYPE get_mqd_type_from_queue_type(enum kfd_queue_type type)\n{\n\tif (type == KFD_QUEUE_TYPE_SDMA || type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\treturn KFD_MQD_TYPE_SDMA;\n\treturn KFD_MQD_TYPE_CP;\n}\n\nstatic bool is_pipe_enabled(struct device_queue_manager *dqm, int mec, int pipe)\n{\n\tint i;\n\tint pipe_offset = (mec * dqm->dev->kfd->shared_resources.num_pipe_per_mec\n\t\t+ pipe) * dqm->dev->kfd->shared_resources.num_queue_per_pipe;\n\n\t \n\tfor (i = 0; i <  dqm->dev->kfd->shared_resources.num_queue_per_pipe; ++i)\n\t\tif (test_bit(pipe_offset + i,\n\t\t\t      dqm->dev->kfd->shared_resources.cp_queue_bitmap))\n\t\t\treturn true;\n\treturn false;\n}\n\nunsigned int get_cp_queues_num(struct device_queue_manager *dqm)\n{\n\treturn bitmap_weight(dqm->dev->kfd->shared_resources.cp_queue_bitmap,\n\t\t\t\tKGD_MAX_QUEUES);\n}\n\nunsigned int get_queues_per_pipe(struct device_queue_manager *dqm)\n{\n\treturn dqm->dev->kfd->shared_resources.num_queue_per_pipe;\n}\n\nunsigned int get_pipes_per_mec(struct device_queue_manager *dqm)\n{\n\treturn dqm->dev->kfd->shared_resources.num_pipe_per_mec;\n}\n\nstatic unsigned int get_num_all_sdma_engines(struct device_queue_manager *dqm)\n{\n\treturn kfd_get_num_sdma_engines(dqm->dev) +\n\t\tkfd_get_num_xgmi_sdma_engines(dqm->dev);\n}\n\nunsigned int get_num_sdma_queues(struct device_queue_manager *dqm)\n{\n\treturn kfd_get_num_sdma_engines(dqm->dev) *\n\t\tdqm->dev->kfd->device_info.num_sdma_queues_per_engine;\n}\n\nunsigned int get_num_xgmi_sdma_queues(struct device_queue_manager *dqm)\n{\n\treturn kfd_get_num_xgmi_sdma_engines(dqm->dev) *\n\t\tdqm->dev->kfd->device_info.num_sdma_queues_per_engine;\n}\n\nstatic void init_sdma_bitmaps(struct device_queue_manager *dqm)\n{\n\tbitmap_zero(dqm->sdma_bitmap, KFD_MAX_SDMA_QUEUES);\n\tbitmap_set(dqm->sdma_bitmap, 0, get_num_sdma_queues(dqm));\n\n\tbitmap_zero(dqm->xgmi_sdma_bitmap, KFD_MAX_SDMA_QUEUES);\n\tbitmap_set(dqm->xgmi_sdma_bitmap, 0, get_num_xgmi_sdma_queues(dqm));\n\n\t \n\tbitmap_andnot(dqm->sdma_bitmap, dqm->sdma_bitmap,\n\t\t      dqm->dev->kfd->device_info.reserved_sdma_queues_bitmap,\n\t\t      KFD_MAX_SDMA_QUEUES);\n}\n\nvoid program_sh_mem_settings(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tuint32_t xcc_mask = dqm->dev->xcc_mask;\n\tint xcc_id;\n\n\tfor_each_inst(xcc_id, xcc_mask)\n\t\tdqm->dev->kfd2kgd->program_sh_mem_settings(\n\t\t\tdqm->dev->adev, qpd->vmid, qpd->sh_mem_config,\n\t\t\tqpd->sh_mem_ape1_base, qpd->sh_mem_ape1_limit,\n\t\t\tqpd->sh_mem_bases, xcc_id);\n}\n\nstatic void kfd_hws_hang(struct device_queue_manager *dqm)\n{\n\t \n\tdqm->is_hws_hang = true;\n\n\t \n\tif (!dqm->is_resetting)\n\t\tschedule_work(&dqm->hw_exception_work);\n}\n\nstatic int convert_to_mes_queue_type(int queue_type)\n{\n\tint mes_queue_type;\n\n\tswitch (queue_type) {\n\tcase KFD_QUEUE_TYPE_COMPUTE:\n\t\tmes_queue_type = MES_QUEUE_TYPE_COMPUTE;\n\t\tbreak;\n\tcase KFD_QUEUE_TYPE_SDMA:\n\t\tmes_queue_type = MES_QUEUE_TYPE_SDMA;\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"Invalid queue type %d\", queue_type);\n\t\tmes_queue_type = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn mes_queue_type;\n}\n\nstatic int add_queue_mes(struct device_queue_manager *dqm, struct queue *q,\n\t\t\t struct qcm_process_device *qpd)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)dqm->dev->adev;\n\tstruct kfd_process_device *pdd = qpd_to_pdd(qpd);\n\tstruct mes_add_queue_input queue_input;\n\tint r, queue_type;\n\tuint64_t wptr_addr_off;\n\n\tif (dqm->is_hws_hang)\n\t\treturn -EIO;\n\n\tmemset(&queue_input, 0x0, sizeof(struct mes_add_queue_input));\n\tqueue_input.process_id = qpd->pqm->process->pasid;\n\tqueue_input.page_table_base_addr =  qpd->page_table_base;\n\tqueue_input.process_va_start = 0;\n\tqueue_input.process_va_end = adev->vm_manager.max_pfn - 1;\n\t \n\tqueue_input.process_quantum = KFD_MES_PROCESS_QUANTUM;   \n\tqueue_input.process_context_addr = pdd->proc_ctx_gpu_addr;\n\tqueue_input.gang_quantum = KFD_MES_GANG_QUANTUM;  \n\tqueue_input.gang_context_addr = q->gang_ctx_gpu_addr;\n\tqueue_input.inprocess_gang_priority = q->properties.priority;\n\tqueue_input.gang_global_priority_level =\n\t\t\t\t\tAMDGPU_MES_PRIORITY_LEVEL_NORMAL;\n\tqueue_input.doorbell_offset = q->properties.doorbell_off;\n\tqueue_input.mqd_addr = q->gart_mqd_addr;\n\tqueue_input.wptr_addr = (uint64_t)q->properties.write_ptr;\n\n\tif (q->wptr_bo) {\n\t\twptr_addr_off = (uint64_t)q->properties.write_ptr & (PAGE_SIZE - 1);\n\t\tqueue_input.wptr_mc_addr = amdgpu_bo_gpu_offset(q->wptr_bo) + wptr_addr_off;\n\t}\n\n\tqueue_input.is_kfd_process = 1;\n\tqueue_input.is_aql_queue = (q->properties.format == KFD_QUEUE_FORMAT_AQL);\n\tqueue_input.queue_size = q->properties.queue_size >> 2;\n\n\tqueue_input.paging = false;\n\tqueue_input.tba_addr = qpd->tba_addr;\n\tqueue_input.tma_addr = qpd->tma_addr;\n\tqueue_input.trap_en = !kfd_dbg_has_cwsr_workaround(q->device);\n\tqueue_input.skip_process_ctx_clear = qpd->pqm->process->debug_trap_enabled ||\n\t\t\t\t\t     kfd_dbg_has_ttmps_always_setup(q->device);\n\n\tqueue_type = convert_to_mes_queue_type(q->properties.type);\n\tif (queue_type < 0) {\n\t\tpr_err(\"Queue type not supported with MES, queue:%d\\n\",\n\t\t\t\tq->properties.type);\n\t\treturn -EINVAL;\n\t}\n\tqueue_input.queue_type = (uint32_t)queue_type;\n\n\tqueue_input.exclusively_scheduled = q->properties.is_gws;\n\n\tamdgpu_mes_lock(&adev->mes);\n\tr = adev->mes.funcs->add_hw_queue(&adev->mes, &queue_input);\n\tamdgpu_mes_unlock(&adev->mes);\n\tif (r) {\n\t\tpr_err(\"failed to add hardware queue to MES, doorbell=0x%x\\n\",\n\t\t\tq->properties.doorbell_off);\n\t\tpr_err(\"MES might be in unrecoverable state, issue a GPU reset\\n\");\n\t\tkfd_hws_hang(dqm);\n\t}\n\n\treturn r;\n}\n\nstatic int remove_queue_mes(struct device_queue_manager *dqm, struct queue *q,\n\t\t\tstruct qcm_process_device *qpd)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)dqm->dev->adev;\n\tint r;\n\tstruct mes_remove_queue_input queue_input;\n\n\tif (dqm->is_hws_hang)\n\t\treturn -EIO;\n\n\tmemset(&queue_input, 0x0, sizeof(struct mes_remove_queue_input));\n\tqueue_input.doorbell_offset = q->properties.doorbell_off;\n\tqueue_input.gang_context_addr = q->gang_ctx_gpu_addr;\n\n\tamdgpu_mes_lock(&adev->mes);\n\tr = adev->mes.funcs->remove_hw_queue(&adev->mes, &queue_input);\n\tamdgpu_mes_unlock(&adev->mes);\n\n\tif (r) {\n\t\tpr_err(\"failed to remove hardware queue from MES, doorbell=0x%x\\n\",\n\t\t\tq->properties.doorbell_off);\n\t\tpr_err(\"MES might be in unrecoverable state, issue a GPU reset\\n\");\n\t\tkfd_hws_hang(dqm);\n\t}\n\n\treturn r;\n}\n\nstatic int remove_all_queues_mes(struct device_queue_manager *dqm)\n{\n\tstruct device_process_node *cur;\n\tstruct qcm_process_device *qpd;\n\tstruct queue *q;\n\tint retval = 0;\n\n\tlist_for_each_entry(cur, &dqm->queues, list) {\n\t\tqpd = cur->qpd;\n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tif (q->properties.is_active) {\n\t\t\t\tretval = remove_queue_mes(dqm, q, qpd);\n\t\t\t\tif (retval) {\n\t\t\t\t\tpr_err(\"%s: Failed to remove queue %d for dev %d\",\n\t\t\t\t\t\t__func__,\n\t\t\t\t\t\tq->properties.queue_id,\n\t\t\t\t\t\tdqm->dev->id);\n\t\t\t\t\treturn retval;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn retval;\n}\n\nstatic void increment_queue_count(struct device_queue_manager *dqm,\n\t\t\t\t  struct qcm_process_device *qpd,\n\t\t\t\t  struct queue *q)\n{\n\tdqm->active_queue_count++;\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||\n\t    q->properties.type == KFD_QUEUE_TYPE_DIQ)\n\t\tdqm->active_cp_queue_count++;\n\n\tif (q->properties.is_gws) {\n\t\tdqm->gws_queue_count++;\n\t\tqpd->mapped_gws_queue = true;\n\t}\n}\n\nstatic void decrement_queue_count(struct device_queue_manager *dqm,\n\t\t\t\t  struct qcm_process_device *qpd,\n\t\t\t\t  struct queue *q)\n{\n\tdqm->active_queue_count--;\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||\n\t    q->properties.type == KFD_QUEUE_TYPE_DIQ)\n\t\tdqm->active_cp_queue_count--;\n\n\tif (q->properties.is_gws) {\n\t\tdqm->gws_queue_count--;\n\t\tqpd->mapped_gws_queue = false;\n\t}\n}\n\n \nstatic int allocate_doorbell(struct qcm_process_device *qpd,\n\t\t\t     struct queue *q,\n\t\t\t     uint32_t const *restore_id)\n{\n\tstruct kfd_node *dev = qpd->dqm->dev;\n\n\tif (!KFD_IS_SOC15(dev)) {\n\t\t \n\n\t\tif (restore_id && *restore_id != q->properties.queue_id)\n\t\t\treturn -EINVAL;\n\n\t\tq->doorbell_id = q->properties.queue_id;\n\t} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\t \n\n\t\tuint32_t *idx_offset = dev->kfd->shared_resources.sdma_doorbell_idx;\n\n\t\t \n\t\tuint32_t valid_id = idx_offset[qpd->dqm->dev->node_id *\n\t\t\t\t\t       get_num_all_sdma_engines(qpd->dqm) +\n\t\t\t\t\t       q->properties.sdma_engine_id]\n\t\t\t\t\t\t+ (q->properties.sdma_queue_id & 1)\n\t\t\t\t\t\t* KFD_QUEUE_DOORBELL_MIRROR_OFFSET\n\t\t\t\t\t\t+ (q->properties.sdma_queue_id >> 1);\n\n\t\tif (restore_id && *restore_id != valid_id)\n\t\t\treturn -EINVAL;\n\t\tq->doorbell_id = valid_id;\n\t} else {\n\t\t \n\t\tif (restore_id) {\n\t\t\t \n\t\t\tif (__test_and_set_bit(*restore_id, qpd->doorbell_bitmap))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tq->doorbell_id = *restore_id;\n\t\t} else {\n\t\t\t \n\t\t\tunsigned int found;\n\n\t\t\tfound = find_first_zero_bit(qpd->doorbell_bitmap,\n\t\t\t\t\t\t    KFD_MAX_NUM_OF_QUEUES_PER_PROCESS);\n\t\t\tif (found >= KFD_MAX_NUM_OF_QUEUES_PER_PROCESS) {\n\t\t\t\tpr_debug(\"No doorbells available\");\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tset_bit(found, qpd->doorbell_bitmap);\n\t\t\tq->doorbell_id = found;\n\t\t}\n\t}\n\n\tq->properties.doorbell_off = amdgpu_doorbell_index_on_bar(dev->adev,\n\t\t\t\t\t\t\t\t  qpd->proc_doorbells,\n\t\t\t\t\t\t\t\t  q->doorbell_id,\n\t\t\t\t\t\t\t\t  dev->kfd->device_info.doorbell_size);\n\treturn 0;\n}\n\nstatic void deallocate_doorbell(struct qcm_process_device *qpd,\n\t\t\t\tstruct queue *q)\n{\n\tunsigned int old;\n\tstruct kfd_node *dev = qpd->dqm->dev;\n\n\tif (!KFD_IS_SOC15(dev) ||\n\t    q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t    q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\treturn;\n\n\told = test_and_clear_bit(q->doorbell_id, qpd->doorbell_bitmap);\n\tWARN_ON(!old);\n}\n\nstatic void program_trap_handler_settings(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tuint32_t xcc_mask = dqm->dev->xcc_mask;\n\tint xcc_id;\n\n\tif (dqm->dev->kfd2kgd->program_trap_handler_settings)\n\t\tfor_each_inst(xcc_id, xcc_mask)\n\t\t\tdqm->dev->kfd2kgd->program_trap_handler_settings(\n\t\t\t\tdqm->dev->adev, qpd->vmid, qpd->tba_addr,\n\t\t\t\tqpd->tma_addr, xcc_id);\n}\n\nstatic int allocate_vmid(struct device_queue_manager *dqm,\n\t\t\tstruct qcm_process_device *qpd,\n\t\t\tstruct queue *q)\n{\n\tint allocated_vmid = -1, i;\n\n\tfor (i = dqm->dev->vm_info.first_vmid_kfd;\n\t\t\ti <= dqm->dev->vm_info.last_vmid_kfd; i++) {\n\t\tif (!dqm->vmid_pasid[i]) {\n\t\t\tallocated_vmid = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (allocated_vmid < 0) {\n\t\tpr_err(\"no more vmid to allocate\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tpr_debug(\"vmid allocated: %d\\n\", allocated_vmid);\n\n\tdqm->vmid_pasid[allocated_vmid] = q->process->pasid;\n\n\tset_pasid_vmid_mapping(dqm, q->process->pasid, allocated_vmid);\n\n\tqpd->vmid = allocated_vmid;\n\tq->properties.vmid = allocated_vmid;\n\n\tprogram_sh_mem_settings(dqm, qpd);\n\n\tif (KFD_IS_SOC15(dqm->dev) && dqm->dev->kfd->cwsr_enabled)\n\t\tprogram_trap_handler_settings(dqm, qpd);\n\n\t \n\tdqm->dev->kfd2kgd->set_vm_context_page_table_base(dqm->dev->adev,\n\t\t\tqpd->vmid,\n\t\t\tqpd->page_table_base);\n\t \n\tkfd_flush_tlb(qpd_to_pdd(qpd), TLB_FLUSH_LEGACY);\n\n\tif (dqm->dev->kfd2kgd->set_scratch_backing_va)\n\t\tdqm->dev->kfd2kgd->set_scratch_backing_va(dqm->dev->adev,\n\t\t\t\tqpd->sh_hidden_private_base, qpd->vmid);\n\n\treturn 0;\n}\n\nstatic int flush_texture_cache_nocpsch(struct kfd_node *kdev,\n\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tconst struct packet_manager_funcs *pmf = qpd->dqm->packet_mgr.pmf;\n\tint ret;\n\n\tif (!qpd->ib_kaddr)\n\t\treturn -ENOMEM;\n\n\tret = pmf->release_mem(qpd->ib_base, (uint32_t *)qpd->ib_kaddr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_amdkfd_submit_ib(kdev->adev, KGD_ENGINE_MEC1, qpd->vmid,\n\t\t\t\tqpd->ib_base, (uint32_t *)qpd->ib_kaddr,\n\t\t\t\tpmf->release_mem_size / sizeof(uint32_t));\n}\n\nstatic void deallocate_vmid(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd,\n\t\t\t\tstruct queue *q)\n{\n\t \n\tif (q->device->adev->asic_type == CHIP_HAWAII)\n\t\tif (flush_texture_cache_nocpsch(q->device, qpd))\n\t\t\tpr_err(\"Failed to flush TC\\n\");\n\n\tkfd_flush_tlb(qpd_to_pdd(qpd), TLB_FLUSH_LEGACY);\n\n\t \n\tset_pasid_vmid_mapping(dqm, 0, qpd->vmid);\n\tdqm->vmid_pasid[qpd->vmid] = 0;\n\n\tqpd->vmid = 0;\n\tq->properties.vmid = 0;\n}\n\nstatic int create_queue_nocpsch(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q,\n\t\t\t\tstruct qcm_process_device *qpd,\n\t\t\t\tconst struct kfd_criu_queue_priv_data *qd,\n\t\t\t\tconst void *restore_mqd, const void *restore_ctl_stack)\n{\n\tstruct mqd_manager *mqd_mgr;\n\tint retval;\n\n\tdqm_lock(dqm);\n\n\tif (dqm->total_queue_count >= max_num_of_queues_per_device) {\n\t\tpr_warn(\"Can't create new usermode queue because %d queues were already created\\n\",\n\t\t\t\tdqm->total_queue_count);\n\t\tretval = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\tif (list_empty(&qpd->queues_list)) {\n\t\tretval = allocate_vmid(dqm, qpd, q);\n\t\tif (retval)\n\t\t\tgoto out_unlock;\n\t}\n\tq->properties.vmid = qpd->vmid;\n\t \n\tq->properties.is_evicted = !!qpd->evicted;\n\n\tq->properties.tba_addr = qpd->tba_addr;\n\tq->properties.tma_addr = qpd->tma_addr;\n\n\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\tq->properties.type)];\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE) {\n\t\tretval = allocate_hqd(dqm, q);\n\t\tif (retval)\n\t\t\tgoto deallocate_vmid;\n\t\tpr_debug(\"Loading mqd to hqd on pipe %d, queue %d\\n\",\n\t\t\tq->pipe, q->queue);\n\t} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\tretval = allocate_sdma_queue(dqm, q, qd ? &qd->sdma_id : NULL);\n\t\tif (retval)\n\t\t\tgoto deallocate_vmid;\n\t\tdqm->asic_ops.init_sdma_vm(dqm, q, qpd);\n\t}\n\n\tretval = allocate_doorbell(qpd, q, qd ? &qd->doorbell_id : NULL);\n\tif (retval)\n\t\tgoto out_deallocate_hqd;\n\n\t \n\tdqm_unlock(dqm);\n\tq->mqd_mem_obj = mqd_mgr->allocate_mqd(mqd_mgr->dev, &q->properties);\n\tdqm_lock(dqm);\n\n\tif (!q->mqd_mem_obj) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_deallocate_doorbell;\n\t}\n\n\tif (qd)\n\t\tmqd_mgr->restore_mqd(mqd_mgr, &q->mqd, q->mqd_mem_obj, &q->gart_mqd_addr,\n\t\t\t\t     &q->properties, restore_mqd, restore_ctl_stack,\n\t\t\t\t     qd->ctl_stack_size);\n\telse\n\t\tmqd_mgr->init_mqd(mqd_mgr, &q->mqd, q->mqd_mem_obj,\n\t\t\t\t\t&q->gart_mqd_addr, &q->properties);\n\n\tif (q->properties.is_active) {\n\t\tif (!dqm->sched_running) {\n\t\t\tWARN_ONCE(1, \"Load non-HWS mqd while stopped\\n\");\n\t\t\tgoto add_queue_to_list;\n\t\t}\n\n\t\tif (WARN(q->process->mm != current->mm,\n\t\t\t\t\t\"should only run in user thread\"))\n\t\t\tretval = -EFAULT;\n\t\telse\n\t\t\tretval = mqd_mgr->load_mqd(mqd_mgr, q->mqd, q->pipe,\n\t\t\t\t\tq->queue, &q->properties, current->mm);\n\t\tif (retval)\n\t\t\tgoto out_free_mqd;\n\t}\n\nadd_queue_to_list:\n\tlist_add(&q->list, &qpd->queues_list);\n\tqpd->queue_count++;\n\tif (q->properties.is_active)\n\t\tincrement_queue_count(dqm, qpd, q);\n\n\t \n\tdqm->total_queue_count++;\n\tpr_debug(\"Total of %d queues are accountable so far\\n\",\n\t\t\tdqm->total_queue_count);\n\tgoto out_unlock;\n\nout_free_mqd:\n\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\nout_deallocate_doorbell:\n\tdeallocate_doorbell(qpd, q);\nout_deallocate_hqd:\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE)\n\t\tdeallocate_hqd(dqm, q);\n\telse if (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\tdeallocate_sdma_queue(dqm, q);\ndeallocate_vmid:\n\tif (list_empty(&qpd->queues_list))\n\t\tdeallocate_vmid(dqm, qpd, q);\nout_unlock:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\nstatic int allocate_hqd(struct device_queue_manager *dqm, struct queue *q)\n{\n\tbool set;\n\tint pipe, bit, i;\n\n\tset = false;\n\n\tfor (pipe = dqm->next_pipe_to_allocate, i = 0;\n\t\t\ti < get_pipes_per_mec(dqm);\n\t\t\tpipe = ((pipe + 1) % get_pipes_per_mec(dqm)), ++i) {\n\n\t\tif (!is_pipe_enabled(dqm, 0, pipe))\n\t\t\tcontinue;\n\n\t\tif (dqm->allocated_queues[pipe] != 0) {\n\t\t\tbit = ffs(dqm->allocated_queues[pipe]) - 1;\n\t\t\tdqm->allocated_queues[pipe] &= ~(1 << bit);\n\t\t\tq->pipe = pipe;\n\t\t\tq->queue = bit;\n\t\t\tset = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!set)\n\t\treturn -EBUSY;\n\n\tpr_debug(\"hqd slot - pipe %d, queue %d\\n\", q->pipe, q->queue);\n\t \n\tdqm->next_pipe_to_allocate = (pipe + 1) % get_pipes_per_mec(dqm);\n\n\treturn 0;\n}\n\nstatic inline void deallocate_hqd(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q)\n{\n\tdqm->allocated_queues[q->pipe] |= (1 << q->queue);\n}\n\n#define SQ_IND_CMD_CMD_KILL\t\t0x00000003\n#define SQ_IND_CMD_MODE_BROADCAST\t0x00000001\n\nstatic int dbgdev_wave_reset_wavefronts(struct kfd_node *dev, struct kfd_process *p)\n{\n\tint status = 0;\n\tunsigned int vmid;\n\tuint16_t queried_pasid;\n\tunion SQ_CMD_BITS reg_sq_cmd;\n\tunion GRBM_GFX_INDEX_BITS reg_gfx_index;\n\tstruct kfd_process_device *pdd;\n\tint first_vmid_to_scan = dev->vm_info.first_vmid_kfd;\n\tint last_vmid_to_scan = dev->vm_info.last_vmid_kfd;\n\tuint32_t xcc_mask = dev->xcc_mask;\n\tint xcc_id;\n\n\treg_sq_cmd.u32All = 0;\n\treg_gfx_index.u32All = 0;\n\n\tpr_debug(\"Killing all process wavefronts\\n\");\n\n\tif (!dev->kfd2kgd->get_atc_vmid_pasid_mapping_info) {\n\t\tpr_err(\"no vmid pasid mapping supported \\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\n\tfor (vmid = first_vmid_to_scan; vmid <= last_vmid_to_scan; vmid++) {\n\t\tstatus = dev->kfd2kgd->get_atc_vmid_pasid_mapping_info\n\t\t\t\t(dev->adev, vmid, &queried_pasid);\n\n\t\tif (status && queried_pasid == p->pasid) {\n\t\t\tpr_debug(\"Killing wave fronts of vmid %d and pasid 0x%x\\n\",\n\t\t\t\t\tvmid, p->pasid);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (vmid > last_vmid_to_scan) {\n\t\tpr_err(\"Didn't find vmid for pasid 0x%x\\n\", p->pasid);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tpdd = kfd_get_process_device_data(dev, p);\n\tif (!pdd)\n\t\treturn -EFAULT;\n\n\treg_gfx_index.bits.sh_broadcast_writes = 1;\n\treg_gfx_index.bits.se_broadcast_writes = 1;\n\treg_gfx_index.bits.instance_broadcast_writes = 1;\n\treg_sq_cmd.bits.mode = SQ_IND_CMD_MODE_BROADCAST;\n\treg_sq_cmd.bits.cmd = SQ_IND_CMD_CMD_KILL;\n\treg_sq_cmd.bits.vm_id = vmid;\n\n\tfor_each_inst(xcc_id, xcc_mask)\n\t\tdev->kfd2kgd->wave_control_execute(\n\t\t\tdev->adev, reg_gfx_index.u32All,\n\t\t\treg_sq_cmd.u32All, xcc_id);\n\n\treturn 0;\n}\n\n \nstatic int destroy_queue_nocpsch_locked(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd,\n\t\t\t\tstruct queue *q)\n{\n\tint retval;\n\tstruct mqd_manager *mqd_mgr;\n\n\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\tq->properties.type)];\n\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE)\n\t\tdeallocate_hqd(dqm, q);\n\telse if (q->properties.type == KFD_QUEUE_TYPE_SDMA)\n\t\tdeallocate_sdma_queue(dqm, q);\n\telse if (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\tdeallocate_sdma_queue(dqm, q);\n\telse {\n\t\tpr_debug(\"q->properties.type %d is invalid\\n\",\n\t\t\t\tq->properties.type);\n\t\treturn -EINVAL;\n\t}\n\tdqm->total_queue_count--;\n\n\tdeallocate_doorbell(qpd, q);\n\n\tif (!dqm->sched_running) {\n\t\tWARN_ONCE(1, \"Destroy non-HWS queue while stopped\\n\");\n\t\treturn 0;\n\t}\n\n\tretval = mqd_mgr->destroy_mqd(mqd_mgr, q->mqd,\n\t\t\t\tKFD_PREEMPT_TYPE_WAVEFRONT_RESET,\n\t\t\t\tKFD_UNMAP_LATENCY_MS,\n\t\t\t\tq->pipe, q->queue);\n\tif (retval == -ETIME)\n\t\tqpd->reset_wavefronts = true;\n\n\tlist_del(&q->list);\n\tif (list_empty(&qpd->queues_list)) {\n\t\tif (qpd->reset_wavefronts) {\n\t\t\tpr_warn(\"Resetting wave fronts (nocpsch) on dev %p\\n\",\n\t\t\t\t\tdqm->dev);\n\t\t\t \n\t\t\tdbgdev_wave_reset_wavefronts(dqm->dev,\n\t\t\t\t\tqpd->pqm->process);\n\t\t\tqpd->reset_wavefronts = false;\n\t\t}\n\n\t\tdeallocate_vmid(dqm, qpd, q);\n\t}\n\tqpd->queue_count--;\n\tif (q->properties.is_active)\n\t\tdecrement_queue_count(dqm, qpd, q);\n\n\treturn retval;\n}\n\nstatic int destroy_queue_nocpsch(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd,\n\t\t\t\tstruct queue *q)\n{\n\tint retval;\n\tuint64_t sdma_val = 0;\n\tstruct kfd_process_device *pdd = qpd_to_pdd(qpd);\n\tstruct mqd_manager *mqd_mgr =\n\t\tdqm->mqd_mgrs[get_mqd_type_from_queue_type(q->properties.type)];\n\n\t \n\tif ((q->properties.type == KFD_QUEUE_TYPE_SDMA) ||\n\t    (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)) {\n\t\tretval = read_sdma_queue_counter((uint64_t __user *)q->properties.read_ptr,\n\t\t\t\t\t\t\t&sdma_val);\n\t\tif (retval)\n\t\t\tpr_err(\"Failed to read SDMA queue counter for queue: %d\\n\",\n\t\t\t\tq->properties.queue_id);\n\t}\n\n\tdqm_lock(dqm);\n\tretval = destroy_queue_nocpsch_locked(dqm, qpd, q);\n\tif (!retval)\n\t\tpdd->sdma_past_activity_counter += sdma_val;\n\tdqm_unlock(dqm);\n\n\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\n\n\treturn retval;\n}\n\nstatic int update_queue(struct device_queue_manager *dqm, struct queue *q,\n\t\t\tstruct mqd_update_info *minfo)\n{\n\tint retval = 0;\n\tstruct mqd_manager *mqd_mgr;\n\tstruct kfd_process_device *pdd;\n\tbool prev_active = false;\n\n\tdqm_lock(dqm);\n\tpdd = kfd_get_process_device_data(q->device, q->process);\n\tif (!pdd) {\n\t\tretval = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\tq->properties.type)];\n\n\t \n\tprev_active = q->properties.is_active;\n\n\t \n\tif (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS) {\n\t\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\t\tretval = unmap_queues_cpsch(dqm,\n\t\t\t\t\t\t    KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0, USE_DEFAULT_GRACE_PERIOD, false);\n\t\telse if (prev_active)\n\t\t\tretval = remove_queue_mes(dqm, q, &pdd->qpd);\n\n\t\tif (retval) {\n\t\t\tpr_err(\"unmap queue failed\\n\");\n\t\t\tgoto out_unlock;\n\t\t}\n\t} else if (prev_active &&\n\t\t   (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||\n\t\t    q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\t    q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)) {\n\n\t\tif (!dqm->sched_running) {\n\t\t\tWARN_ONCE(1, \"Update non-HWS queue while stopped\\n\");\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tretval = mqd_mgr->destroy_mqd(mqd_mgr, q->mqd,\n\t\t\t\t(dqm->dev->kfd->cwsr_enabled ?\n\t\t\t\t KFD_PREEMPT_TYPE_WAVEFRONT_SAVE :\n\t\t\t\t KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN),\n\t\t\t\tKFD_UNMAP_LATENCY_MS, q->pipe, q->queue);\n\t\tif (retval) {\n\t\t\tpr_err(\"destroy mqd failed\\n\");\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tmqd_mgr->update_mqd(mqd_mgr, q->mqd, &q->properties, minfo);\n\n\t \n\tif (q->properties.is_active && !prev_active) {\n\t\tincrement_queue_count(dqm, &pdd->qpd, q);\n\t} else if (!q->properties.is_active && prev_active) {\n\t\tdecrement_queue_count(dqm, &pdd->qpd, q);\n\t} else if (q->gws && !q->properties.is_gws) {\n\t\tif (q->properties.is_active) {\n\t\t\tdqm->gws_queue_count++;\n\t\t\tpdd->qpd.mapped_gws_queue = true;\n\t\t}\n\t\tq->properties.is_gws = true;\n\t} else if (!q->gws && q->properties.is_gws) {\n\t\tif (q->properties.is_active) {\n\t\t\tdqm->gws_queue_count--;\n\t\t\tpdd->qpd.mapped_gws_queue = false;\n\t\t}\n\t\tq->properties.is_gws = false;\n\t}\n\n\tif (dqm->sched_policy != KFD_SCHED_POLICY_NO_HWS) {\n\t\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\t\tretval = map_queues_cpsch(dqm);\n\t\telse if (q->properties.is_active)\n\t\t\tretval = add_queue_mes(dqm, q, &pdd->qpd);\n\t} else if (q->properties.is_active &&\n\t\t (q->properties.type == KFD_QUEUE_TYPE_COMPUTE ||\n\t\t  q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\t  q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)) {\n\t\tif (WARN(q->process->mm != current->mm,\n\t\t\t \"should only run in user thread\"))\n\t\t\tretval = -EFAULT;\n\t\telse\n\t\t\tretval = mqd_mgr->load_mqd(mqd_mgr, q->mqd,\n\t\t\t\t\t\t   q->pipe, q->queue,\n\t\t\t\t\t\t   &q->properties, current->mm);\n\t}\n\nout_unlock:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\n \nstatic int suspend_single_queue(struct device_queue_manager *dqm,\n\t\t\t\t      struct kfd_process_device *pdd,\n\t\t\t\t      struct queue *q)\n{\n\tbool is_new;\n\n\tif (q->properties.is_suspended)\n\t\treturn 0;\n\n\tpr_debug(\"Suspending PASID %u queue [%i]\\n\",\n\t\t\tpdd->process->pasid,\n\t\t\tq->properties.queue_id);\n\n\tis_new = q->properties.exception_status & KFD_EC_MASK(EC_QUEUE_NEW);\n\n\tif (is_new || q->properties.is_being_destroyed) {\n\t\tpr_debug(\"Suspend: skip %s queue id %i\\n\",\n\t\t\t\tis_new ? \"new\" : \"destroyed\",\n\t\t\t\tq->properties.queue_id);\n\t\treturn -EBUSY;\n\t}\n\n\tq->properties.is_suspended = true;\n\tif (q->properties.is_active) {\n\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\tint r = remove_queue_mes(dqm, q, &pdd->qpd);\n\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\tdecrement_queue_count(dqm, &pdd->qpd, q);\n\t\tq->properties.is_active = false;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int resume_single_queue(struct device_queue_manager *dqm,\n\t\t\t\t      struct qcm_process_device *qpd,\n\t\t\t\t      struct queue *q)\n{\n\tstruct kfd_process_device *pdd;\n\n\tif (!q->properties.is_suspended)\n\t\treturn 0;\n\n\tpdd = qpd_to_pdd(qpd);\n\n\tpr_debug(\"Restoring from suspend PASID %u queue [%i]\\n\",\n\t\t\t    pdd->process->pasid,\n\t\t\t    q->properties.queue_id);\n\n\tq->properties.is_suspended = false;\n\n\tif (QUEUE_IS_ACTIVE(q->properties)) {\n\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\tint r = add_queue_mes(dqm, q, &pdd->qpd);\n\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\tq->properties.is_active = true;\n\t\tincrement_queue_count(dqm, qpd, q);\n\t}\n\n\treturn 0;\n}\n\nstatic int evict_process_queues_nocpsch(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tstruct queue *q;\n\tstruct mqd_manager *mqd_mgr;\n\tstruct kfd_process_device *pdd;\n\tint retval, ret = 0;\n\n\tdqm_lock(dqm);\n\tif (qpd->evicted++ > 0)  \n\t\tgoto out;\n\n\tpdd = qpd_to_pdd(qpd);\n\tpr_debug_ratelimited(\"Evicting PASID 0x%x queues\\n\",\n\t\t\t    pdd->process->pasid);\n\n\tpdd->last_evict_timestamp = get_jiffies_64();\n\t \n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tq->properties.is_evicted = true;\n\t\tif (!q->properties.is_active)\n\t\t\tcontinue;\n\n\t\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\t\tq->properties.type)];\n\t\tq->properties.is_active = false;\n\t\tdecrement_queue_count(dqm, qpd, q);\n\n\t\tif (WARN_ONCE(!dqm->sched_running, \"Evict when stopped\\n\"))\n\t\t\tcontinue;\n\n\t\tretval = mqd_mgr->destroy_mqd(mqd_mgr, q->mqd,\n\t\t\t\t(dqm->dev->kfd->cwsr_enabled ?\n\t\t\t\t KFD_PREEMPT_TYPE_WAVEFRONT_SAVE :\n\t\t\t\t KFD_PREEMPT_TYPE_WAVEFRONT_DRAIN),\n\t\t\t\tKFD_UNMAP_LATENCY_MS, q->pipe, q->queue);\n\t\tif (retval && !ret)\n\t\t\t \n\t\t\tret = retval;\n\t}\n\nout:\n\tdqm_unlock(dqm);\n\treturn ret;\n}\n\nstatic int evict_process_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\t      struct qcm_process_device *qpd)\n{\n\tstruct queue *q;\n\tstruct kfd_process_device *pdd;\n\tint retval = 0;\n\n\tdqm_lock(dqm);\n\tif (qpd->evicted++ > 0)  \n\t\tgoto out;\n\n\tpdd = qpd_to_pdd(qpd);\n\n\t \n\tif (!pdd->drm_priv)\n\t\tgoto out;\n\n\tpr_debug_ratelimited(\"Evicting PASID 0x%x queues\\n\",\n\t\t\t    pdd->process->pasid);\n\n\t \n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tq->properties.is_evicted = true;\n\t\tif (!q->properties.is_active)\n\t\t\tcontinue;\n\n\t\tq->properties.is_active = false;\n\t\tdecrement_queue_count(dqm, qpd, q);\n\n\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\tretval = remove_queue_mes(dqm, q, qpd);\n\t\t\tif (retval) {\n\t\t\t\tpr_err(\"Failed to evict queue %d\\n\",\n\t\t\t\t\tq->properties.queue_id);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tpdd->last_evict_timestamp = get_jiffies_64();\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tretval = execute_queues_cpsch(dqm,\n\t\t\t\t\t      qpd->is_debug ?\n\t\t\t\t\t      KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES :\n\t\t\t\t\t      KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,\n\t\t\t\t\t      USE_DEFAULT_GRACE_PERIOD);\n\nout:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\nstatic int restore_process_queues_nocpsch(struct device_queue_manager *dqm,\n\t\t\t\t\t  struct qcm_process_device *qpd)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct queue *q;\n\tstruct mqd_manager *mqd_mgr;\n\tstruct kfd_process_device *pdd;\n\tuint64_t pd_base;\n\tuint64_t eviction_duration;\n\tint retval, ret = 0;\n\n\tpdd = qpd_to_pdd(qpd);\n\t \n\tpd_base = amdgpu_amdkfd_gpuvm_get_process_page_dir(pdd->drm_priv);\n\n\tdqm_lock(dqm);\n\tif (WARN_ON_ONCE(!qpd->evicted))  \n\t\tgoto out;\n\tif (qpd->evicted > 1) {  \n\t\tqpd->evicted--;\n\t\tgoto out;\n\t}\n\n\tpr_debug_ratelimited(\"Restoring PASID 0x%x queues\\n\",\n\t\t\t    pdd->process->pasid);\n\n\t \n\tqpd->page_table_base = pd_base;\n\tpr_debug(\"Updated PD address to 0x%llx\\n\", pd_base);\n\n\tif (!list_empty(&qpd->queues_list)) {\n\t\tdqm->dev->kfd2kgd->set_vm_context_page_table_base(\n\t\t\t\tdqm->dev->adev,\n\t\t\t\tqpd->vmid,\n\t\t\t\tqpd->page_table_base);\n\t\tkfd_flush_tlb(pdd, TLB_FLUSH_LEGACY);\n\t}\n\n\t \n\tmm = get_task_mm(pdd->process->lead_thread);\n\tif (!mm) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tq->properties.is_evicted = false;\n\t\tif (!QUEUE_IS_ACTIVE(q->properties))\n\t\t\tcontinue;\n\n\t\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\t\tq->properties.type)];\n\t\tq->properties.is_active = true;\n\t\tincrement_queue_count(dqm, qpd, q);\n\n\t\tif (WARN_ONCE(!dqm->sched_running, \"Restore when stopped\\n\"))\n\t\t\tcontinue;\n\n\t\tretval = mqd_mgr->load_mqd(mqd_mgr, q->mqd, q->pipe,\n\t\t\t\t       q->queue, &q->properties, mm);\n\t\tif (retval && !ret)\n\t\t\t \n\t\t\tret = retval;\n\t}\n\tqpd->evicted = 0;\n\teviction_duration = get_jiffies_64() - pdd->last_evict_timestamp;\n\tatomic64_add(eviction_duration, &pdd->evict_duration_counter);\nout:\n\tif (mm)\n\t\tmmput(mm);\n\tdqm_unlock(dqm);\n\treturn ret;\n}\n\nstatic int restore_process_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tstruct queue *q;\n\tstruct kfd_process_device *pdd;\n\tuint64_t eviction_duration;\n\tint retval = 0;\n\n\tpdd = qpd_to_pdd(qpd);\n\n\tdqm_lock(dqm);\n\tif (WARN_ON_ONCE(!qpd->evicted))  \n\t\tgoto out;\n\tif (qpd->evicted > 1) {  \n\t\tqpd->evicted--;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!pdd->drm_priv)\n\t\tgoto vm_not_acquired;\n\n\tpr_debug_ratelimited(\"Restoring PASID 0x%x queues\\n\",\n\t\t\t    pdd->process->pasid);\n\n\t \n\tqpd->page_table_base = amdgpu_amdkfd_gpuvm_get_process_page_dir(pdd->drm_priv);\n\tpr_debug(\"Updated PD address to 0x%llx\\n\", qpd->page_table_base);\n\n\t \n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tq->properties.is_evicted = false;\n\t\tif (!QUEUE_IS_ACTIVE(q->properties))\n\t\t\tcontinue;\n\n\t\tq->properties.is_active = true;\n\t\tincrement_queue_count(dqm, &pdd->qpd, q);\n\n\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\tretval = add_queue_mes(dqm, q, qpd);\n\t\t\tif (retval) {\n\t\t\t\tpr_err(\"Failed to restore queue %d\\n\",\n\t\t\t\t\tq->properties.queue_id);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tretval = execute_queues_cpsch(dqm,\n\t\t\t\t\t      KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0, USE_DEFAULT_GRACE_PERIOD);\n\teviction_duration = get_jiffies_64() - pdd->last_evict_timestamp;\n\tatomic64_add(eviction_duration, &pdd->evict_duration_counter);\nvm_not_acquired:\n\tqpd->evicted = 0;\nout:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\nstatic int register_process(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tstruct device_process_node *n;\n\tstruct kfd_process_device *pdd;\n\tuint64_t pd_base;\n\tint retval;\n\n\tn = kzalloc(sizeof(*n), GFP_KERNEL);\n\tif (!n)\n\t\treturn -ENOMEM;\n\n\tn->qpd = qpd;\n\n\tpdd = qpd_to_pdd(qpd);\n\t \n\tpd_base = amdgpu_amdkfd_gpuvm_get_process_page_dir(pdd->drm_priv);\n\n\tdqm_lock(dqm);\n\tlist_add(&n->list, &dqm->queues);\n\n\t \n\tqpd->page_table_base = pd_base;\n\tpr_debug(\"Updated PD address to 0x%llx\\n\", pd_base);\n\n\tretval = dqm->asic_ops.update_qpd(dqm, qpd);\n\n\tdqm->processes_count++;\n\n\tdqm_unlock(dqm);\n\n\t \n\tkfd_inc_compute_active(dqm->dev);\n\n\treturn retval;\n}\n\nstatic int unregister_process(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tint retval;\n\tstruct device_process_node *cur, *next;\n\n\tpr_debug(\"qpd->queues_list is %s\\n\",\n\t\t\tlist_empty(&qpd->queues_list) ? \"empty\" : \"not empty\");\n\n\tretval = 0;\n\tdqm_lock(dqm);\n\n\tlist_for_each_entry_safe(cur, next, &dqm->queues, list) {\n\t\tif (qpd == cur->qpd) {\n\t\t\tlist_del(&cur->list);\n\t\t\tkfree(cur);\n\t\t\tdqm->processes_count--;\n\t\t\tgoto out;\n\t\t}\n\t}\n\t \n\tretval = 1;\nout:\n\tdqm_unlock(dqm);\n\n\t \n\tif (!retval)\n\t\tkfd_dec_compute_active(dqm->dev);\n\n\treturn retval;\n}\n\nstatic int\nset_pasid_vmid_mapping(struct device_queue_manager *dqm, u32 pasid,\n\t\t\tunsigned int vmid)\n{\n\tuint32_t xcc_mask = dqm->dev->xcc_mask;\n\tint xcc_id, ret;\n\n\tfor_each_inst(xcc_id, xcc_mask) {\n\t\tret = dqm->dev->kfd2kgd->set_pasid_vmid_mapping(\n\t\t\tdqm->dev->adev, pasid, vmid, xcc_id);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void init_interrupts(struct device_queue_manager *dqm)\n{\n\tuint32_t xcc_mask = dqm->dev->xcc_mask;\n\tunsigned int i, xcc_id;\n\n\tfor_each_inst(xcc_id, xcc_mask) {\n\t\tfor (i = 0 ; i < get_pipes_per_mec(dqm) ; i++) {\n\t\t\tif (is_pipe_enabled(dqm, 0, i)) {\n\t\t\t\tdqm->dev->kfd2kgd->init_interrupts(\n\t\t\t\t\tdqm->dev->adev, i, xcc_id);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int initialize_nocpsch(struct device_queue_manager *dqm)\n{\n\tint pipe, queue;\n\n\tpr_debug(\"num of pipes: %d\\n\", get_pipes_per_mec(dqm));\n\n\tdqm->allocated_queues = kcalloc(get_pipes_per_mec(dqm),\n\t\t\t\t\tsizeof(unsigned int), GFP_KERNEL);\n\tif (!dqm->allocated_queues)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&dqm->lock_hidden);\n\tINIT_LIST_HEAD(&dqm->queues);\n\tdqm->active_queue_count = dqm->next_pipe_to_allocate = 0;\n\tdqm->active_cp_queue_count = 0;\n\tdqm->gws_queue_count = 0;\n\n\tfor (pipe = 0; pipe < get_pipes_per_mec(dqm); pipe++) {\n\t\tint pipe_offset = pipe * get_queues_per_pipe(dqm);\n\n\t\tfor (queue = 0; queue < get_queues_per_pipe(dqm); queue++)\n\t\t\tif (test_bit(pipe_offset + queue,\n\t\t\t\t     dqm->dev->kfd->shared_resources.cp_queue_bitmap))\n\t\t\t\tdqm->allocated_queues[pipe] |= 1 << queue;\n\t}\n\n\tmemset(dqm->vmid_pasid, 0, sizeof(dqm->vmid_pasid));\n\n\tinit_sdma_bitmaps(dqm);\n\n\treturn 0;\n}\n\nstatic void uninitialize(struct device_queue_manager *dqm)\n{\n\tint i;\n\n\tWARN_ON(dqm->active_queue_count > 0 || dqm->processes_count > 0);\n\n\tkfree(dqm->allocated_queues);\n\tfor (i = 0 ; i < KFD_MQD_TYPE_MAX ; i++)\n\t\tkfree(dqm->mqd_mgrs[i]);\n\tmutex_destroy(&dqm->lock_hidden);\n}\n\nstatic int start_nocpsch(struct device_queue_manager *dqm)\n{\n\tint r = 0;\n\n\tpr_info(\"SW scheduler is used\");\n\tinit_interrupts(dqm);\n\n\tif (dqm->dev->adev->asic_type == CHIP_HAWAII)\n\t\tr = pm_init(&dqm->packet_mgr, dqm);\n\tif (!r)\n\t\tdqm->sched_running = true;\n\n\treturn r;\n}\n\nstatic int stop_nocpsch(struct device_queue_manager *dqm)\n{\n\tdqm_lock(dqm);\n\tif (!dqm->sched_running) {\n\t\tdqm_unlock(dqm);\n\t\treturn 0;\n\t}\n\n\tif (dqm->dev->adev->asic_type == CHIP_HAWAII)\n\t\tpm_uninit(&dqm->packet_mgr, false);\n\tdqm->sched_running = false;\n\tdqm_unlock(dqm);\n\n\treturn 0;\n}\n\nstatic void pre_reset(struct device_queue_manager *dqm)\n{\n\tdqm_lock(dqm);\n\tdqm->is_resetting = true;\n\tdqm_unlock(dqm);\n}\n\nstatic int allocate_sdma_queue(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q, const uint32_t *restore_sdma_id)\n{\n\tint bit;\n\n\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA) {\n\t\tif (bitmap_empty(dqm->sdma_bitmap, KFD_MAX_SDMA_QUEUES)) {\n\t\t\tpr_err(\"No more SDMA queue to allocate\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (restore_sdma_id) {\n\t\t\t \n\t\t\tif (!test_bit(*restore_sdma_id, dqm->sdma_bitmap)) {\n\t\t\t\tpr_err(\"SDMA queue already in use\\n\");\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tclear_bit(*restore_sdma_id, dqm->sdma_bitmap);\n\t\t\tq->sdma_id = *restore_sdma_id;\n\t\t} else {\n\t\t\t \n\t\t\tbit = find_first_bit(dqm->sdma_bitmap,\n\t\t\t\t\t     get_num_sdma_queues(dqm));\n\t\t\tclear_bit(bit, dqm->sdma_bitmap);\n\t\t\tq->sdma_id = bit;\n\t\t}\n\n\t\tq->properties.sdma_engine_id =\n\t\t\tq->sdma_id % kfd_get_num_sdma_engines(dqm->dev);\n\t\tq->properties.sdma_queue_id = q->sdma_id /\n\t\t\t\tkfd_get_num_sdma_engines(dqm->dev);\n\t} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\tif (bitmap_empty(dqm->xgmi_sdma_bitmap, KFD_MAX_SDMA_QUEUES)) {\n\t\t\tpr_err(\"No more XGMI SDMA queue to allocate\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tif (restore_sdma_id) {\n\t\t\t \n\t\t\tif (!test_bit(*restore_sdma_id, dqm->xgmi_sdma_bitmap)) {\n\t\t\t\tpr_err(\"SDMA queue already in use\\n\");\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tclear_bit(*restore_sdma_id, dqm->xgmi_sdma_bitmap);\n\t\t\tq->sdma_id = *restore_sdma_id;\n\t\t} else {\n\t\t\tbit = find_first_bit(dqm->xgmi_sdma_bitmap,\n\t\t\t\t\t     get_num_xgmi_sdma_queues(dqm));\n\t\t\tclear_bit(bit, dqm->xgmi_sdma_bitmap);\n\t\t\tq->sdma_id = bit;\n\t\t}\n\t\t \n\t\tq->properties.sdma_engine_id =\n\t\t\tkfd_get_num_sdma_engines(dqm->dev) +\n\t\t\tq->sdma_id % kfd_get_num_xgmi_sdma_engines(dqm->dev);\n\t\tq->properties.sdma_queue_id = q->sdma_id /\n\t\t\tkfd_get_num_xgmi_sdma_engines(dqm->dev);\n\t}\n\n\tpr_debug(\"SDMA engine id: %d\\n\", q->properties.sdma_engine_id);\n\tpr_debug(\"SDMA queue id: %d\\n\", q->properties.sdma_queue_id);\n\n\treturn 0;\n}\n\nstatic void deallocate_sdma_queue(struct device_queue_manager *dqm,\n\t\t\t\tstruct queue *q)\n{\n\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA) {\n\t\tif (q->sdma_id >= get_num_sdma_queues(dqm))\n\t\t\treturn;\n\t\tset_bit(q->sdma_id, dqm->sdma_bitmap);\n\t} else if (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\tif (q->sdma_id >= get_num_xgmi_sdma_queues(dqm))\n\t\t\treturn;\n\t\tset_bit(q->sdma_id, dqm->xgmi_sdma_bitmap);\n\t}\n}\n\n \n\nstatic int set_sched_resources(struct device_queue_manager *dqm)\n{\n\tint i, mec;\n\tstruct scheduling_resources res;\n\n\tres.vmid_mask = dqm->dev->compute_vmid_bitmap;\n\n\tres.queue_mask = 0;\n\tfor (i = 0; i < KGD_MAX_QUEUES; ++i) {\n\t\tmec = (i / dqm->dev->kfd->shared_resources.num_queue_per_pipe)\n\t\t\t/ dqm->dev->kfd->shared_resources.num_pipe_per_mec;\n\n\t\tif (!test_bit(i, dqm->dev->kfd->shared_resources.cp_queue_bitmap))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (mec > 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (WARN_ON(i >= (sizeof(res.queue_mask)*8))) {\n\t\t\tpr_err(\"Invalid queue enabled by amdgpu: %d\\n\", i);\n\t\t\tbreak;\n\t\t}\n\n\t\tres.queue_mask |= 1ull\n\t\t\t<< amdgpu_queue_mask_bit_to_set_resource_bit(\n\t\t\t\tdqm->dev->adev, i);\n\t}\n\tres.gws_mask = ~0ull;\n\tres.oac_mask = res.gds_heap_base = res.gds_heap_size = 0;\n\n\tpr_debug(\"Scheduling resources:\\n\"\n\t\t\t\"vmid mask: 0x%8X\\n\"\n\t\t\t\"queue mask: 0x%8llX\\n\",\n\t\t\tres.vmid_mask, res.queue_mask);\n\n\treturn pm_send_set_resources(&dqm->packet_mgr, &res);\n}\n\nstatic int initialize_cpsch(struct device_queue_manager *dqm)\n{\n\tpr_debug(\"num of pipes: %d\\n\", get_pipes_per_mec(dqm));\n\n\tmutex_init(&dqm->lock_hidden);\n\tINIT_LIST_HEAD(&dqm->queues);\n\tdqm->active_queue_count = dqm->processes_count = 0;\n\tdqm->active_cp_queue_count = 0;\n\tdqm->gws_queue_count = 0;\n\tdqm->active_runlist = false;\n\tINIT_WORK(&dqm->hw_exception_work, kfd_process_hw_exception);\n\tdqm->trap_debug_vmid = 0;\n\n\tinit_sdma_bitmaps(dqm);\n\n\tif (dqm->dev->kfd2kgd->get_iq_wait_times)\n\t\tdqm->dev->kfd2kgd->get_iq_wait_times(dqm->dev->adev,\n\t\t\t\t\t&dqm->wait_times,\n\t\t\t\t\tffs(dqm->dev->xcc_mask) - 1);\n\treturn 0;\n}\n\nstatic int start_cpsch(struct device_queue_manager *dqm)\n{\n\tint retval;\n\n\tretval = 0;\n\n\tdqm_lock(dqm);\n\n\tif (!dqm->dev->kfd->shared_resources.enable_mes) {\n\t\tretval = pm_init(&dqm->packet_mgr, dqm);\n\t\tif (retval)\n\t\t\tgoto fail_packet_manager_init;\n\n\t\tretval = set_sched_resources(dqm);\n\t\tif (retval)\n\t\t\tgoto fail_set_sched_resources;\n\t}\n\tpr_debug(\"Allocating fence memory\\n\");\n\n\t \n\tretval = kfd_gtt_sa_allocate(dqm->dev, sizeof(*dqm->fence_addr),\n\t\t\t\t\t&dqm->fence_mem);\n\n\tif (retval)\n\t\tgoto fail_allocate_vidmem;\n\n\tdqm->fence_addr = (uint64_t *)dqm->fence_mem->cpu_ptr;\n\tdqm->fence_gpu_addr = dqm->fence_mem->gpu_addr;\n\n\tinit_interrupts(dqm);\n\n\t \n\tdqm->is_hws_hang = false;\n\tdqm->is_resetting = false;\n\tdqm->sched_running = true;\n\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\texecute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0, USE_DEFAULT_GRACE_PERIOD);\n\n\t \n\tif (amdgpu_emu_mode == 0 && dqm->dev->adev->gmc.is_app_apu &&\n\t    (KFD_GC_VERSION(dqm->dev) == IP_VERSION(9, 4, 3))) {\n\t\tuint32_t reg_offset = 0;\n\t\tuint32_t grace_period = 1;\n\n\t\tretval = pm_update_grace_period(&dqm->packet_mgr,\n\t\t\t\t\t\tgrace_period);\n\t\tif (retval)\n\t\t\tpr_err(\"Setting grace timeout failed\\n\");\n\t\telse if (dqm->dev->kfd2kgd->build_grace_period_packet_info)\n\t\t\t \n\t\t\tdqm->dev->kfd2kgd->build_grace_period_packet_info(\n\t\t\t\t\tdqm->dev->adev,\tdqm->wait_times,\n\t\t\t\t\tgrace_period, &reg_offset,\n\t\t\t\t\t&dqm->wait_times);\n\t}\n\n\tdqm_unlock(dqm);\n\n\treturn 0;\nfail_allocate_vidmem:\nfail_set_sched_resources:\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tpm_uninit(&dqm->packet_mgr, false);\nfail_packet_manager_init:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\nstatic int stop_cpsch(struct device_queue_manager *dqm)\n{\n\tbool hanging;\n\n\tdqm_lock(dqm);\n\tif (!dqm->sched_running) {\n\t\tdqm_unlock(dqm);\n\t\treturn 0;\n\t}\n\n\tif (!dqm->is_hws_hang) {\n\t\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\t\tunmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0, USE_DEFAULT_GRACE_PERIOD, false);\n\t\telse\n\t\t\tremove_all_queues_mes(dqm);\n\t}\n\n\thanging = dqm->is_hws_hang || dqm->is_resetting;\n\tdqm->sched_running = false;\n\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tpm_release_ib(&dqm->packet_mgr);\n\n\tkfd_gtt_sa_free(dqm->dev, dqm->fence_mem);\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tpm_uninit(&dqm->packet_mgr, hanging);\n\tdqm_unlock(dqm);\n\n\treturn 0;\n}\n\nstatic int create_kernel_queue_cpsch(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct kernel_queue *kq,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tdqm_lock(dqm);\n\tif (dqm->total_queue_count >= max_num_of_queues_per_device) {\n\t\tpr_warn(\"Can't create new kernel queue because %d queues were already created\\n\",\n\t\t\t\tdqm->total_queue_count);\n\t\tdqm_unlock(dqm);\n\t\treturn -EPERM;\n\t}\n\n\t \n\tdqm->total_queue_count++;\n\tpr_debug(\"Total of %d queues are accountable so far\\n\",\n\t\t\tdqm->total_queue_count);\n\n\tlist_add(&kq->list, &qpd->priv_queue_list);\n\tincrement_queue_count(dqm, qpd, kq->queue);\n\tqpd->is_debug = true;\n\texecute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,\n\t\t\tUSE_DEFAULT_GRACE_PERIOD);\n\tdqm_unlock(dqm);\n\n\treturn 0;\n}\n\nstatic void destroy_kernel_queue_cpsch(struct device_queue_manager *dqm,\n\t\t\t\t\tstruct kernel_queue *kq,\n\t\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tdqm_lock(dqm);\n\tlist_del(&kq->list);\n\tdecrement_queue_count(dqm, qpd, kq->queue);\n\tqpd->is_debug = false;\n\texecute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,\n\t\t\tUSE_DEFAULT_GRACE_PERIOD);\n\t \n\tdqm->total_queue_count--;\n\tpr_debug(\"Total of %d queues are accountable so far\\n\",\n\t\t\tdqm->total_queue_count);\n\tdqm_unlock(dqm);\n}\n\nstatic int create_queue_cpsch(struct device_queue_manager *dqm, struct queue *q,\n\t\t\tstruct qcm_process_device *qpd,\n\t\t\tconst struct kfd_criu_queue_priv_data *qd,\n\t\t\tconst void *restore_mqd, const void *restore_ctl_stack)\n{\n\tint retval;\n\tstruct mqd_manager *mqd_mgr;\n\n\tif (dqm->total_queue_count >= max_num_of_queues_per_device) {\n\t\tpr_warn(\"Can't create new usermode queue because %d queues were already created\\n\",\n\t\t\t\tdqm->total_queue_count);\n\t\tretval = -EPERM;\n\t\tgoto out;\n\t}\n\n\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\tdqm_lock(dqm);\n\t\tretval = allocate_sdma_queue(dqm, q, qd ? &qd->sdma_id : NULL);\n\t\tdqm_unlock(dqm);\n\t\tif (retval)\n\t\t\tgoto out;\n\t}\n\n\tretval = allocate_doorbell(qpd, q, qd ? &qd->doorbell_id : NULL);\n\tif (retval)\n\t\tgoto out_deallocate_sdma_queue;\n\n\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\tq->properties.type)];\n\n\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\tdqm->asic_ops.init_sdma_vm(dqm, q, qpd);\n\tq->properties.tba_addr = qpd->tba_addr;\n\tq->properties.tma_addr = qpd->tma_addr;\n\tq->mqd_mem_obj = mqd_mgr->allocate_mqd(mqd_mgr->dev, &q->properties);\n\tif (!q->mqd_mem_obj) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_deallocate_doorbell;\n\t}\n\n\tdqm_lock(dqm);\n\t \n\tq->properties.is_evicted = !!qpd->evicted;\n\tq->properties.is_dbg_wa = qpd->pqm->process->debug_trap_enabled &&\n\t\t\t\t  kfd_dbg_has_cwsr_workaround(q->device);\n\n\tif (qd)\n\t\tmqd_mgr->restore_mqd(mqd_mgr, &q->mqd, q->mqd_mem_obj, &q->gart_mqd_addr,\n\t\t\t\t     &q->properties, restore_mqd, restore_ctl_stack,\n\t\t\t\t     qd->ctl_stack_size);\n\telse\n\t\tmqd_mgr->init_mqd(mqd_mgr, &q->mqd, q->mqd_mem_obj,\n\t\t\t\t\t&q->gart_mqd_addr, &q->properties);\n\n\tlist_add(&q->list, &qpd->queues_list);\n\tqpd->queue_count++;\n\n\tif (q->properties.is_active) {\n\t\tincrement_queue_count(dqm, qpd, q);\n\n\t\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\t\tretval = execute_queues_cpsch(dqm,\n\t\t\t\t\tKFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0, USE_DEFAULT_GRACE_PERIOD);\n\t\telse\n\t\t\tretval = add_queue_mes(dqm, q, qpd);\n\t\tif (retval)\n\t\t\tgoto cleanup_queue;\n\t}\n\n\t \n\tdqm->total_queue_count++;\n\n\tpr_debug(\"Total of %d queues are accountable so far\\n\",\n\t\t\tdqm->total_queue_count);\n\n\tdqm_unlock(dqm);\n\treturn retval;\n\ncleanup_queue:\n\tqpd->queue_count--;\n\tlist_del(&q->list);\n\tif (q->properties.is_active)\n\t\tdecrement_queue_count(dqm, qpd, q);\n\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\n\tdqm_unlock(dqm);\nout_deallocate_doorbell:\n\tdeallocate_doorbell(qpd, q);\nout_deallocate_sdma_queue:\n\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA ||\n\t\tq->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI) {\n\t\tdqm_lock(dqm);\n\t\tdeallocate_sdma_queue(dqm, q);\n\t\tdqm_unlock(dqm);\n\t}\nout:\n\treturn retval;\n}\n\nint amdkfd_fence_wait_timeout(uint64_t *fence_addr,\n\t\t\t\tuint64_t fence_value,\n\t\t\t\tunsigned int timeout_ms)\n{\n\tunsigned long end_jiffies = msecs_to_jiffies(timeout_ms) + jiffies;\n\n\twhile (*fence_addr != fence_value) {\n\t\tif (time_after(jiffies, end_jiffies)) {\n\t\t\tpr_err(\"qcm fence wait loop timeout expired\\n\");\n\t\t\t \n\t\t\twhile (halt_if_hws_hang)\n\t\t\t\tschedule();\n\n\t\t\treturn -ETIME;\n\t\t}\n\t\tschedule();\n\t}\n\n\treturn 0;\n}\n\n \nstatic int map_queues_cpsch(struct device_queue_manager *dqm)\n{\n\tint retval;\n\n\tif (!dqm->sched_running)\n\t\treturn 0;\n\tif (dqm->active_queue_count <= 0 || dqm->processes_count <= 0)\n\t\treturn 0;\n\tif (dqm->active_runlist)\n\t\treturn 0;\n\n\tretval = pm_send_runlist(&dqm->packet_mgr, &dqm->queues);\n\tpr_debug(\"%s sent runlist\\n\", __func__);\n\tif (retval) {\n\t\tpr_err(\"failed to execute runlist\\n\");\n\t\treturn retval;\n\t}\n\tdqm->active_runlist = true;\n\n\treturn retval;\n}\n\n \nstatic int unmap_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\tenum kfd_unmap_queues_filter filter,\n\t\t\t\tuint32_t filter_param,\n\t\t\t\tuint32_t grace_period,\n\t\t\t\tbool reset)\n{\n\tint retval = 0;\n\tstruct mqd_manager *mqd_mgr;\n\n\tif (!dqm->sched_running)\n\t\treturn 0;\n\tif (dqm->is_hws_hang || dqm->is_resetting)\n\t\treturn -EIO;\n\tif (!dqm->active_runlist)\n\t\treturn retval;\n\n\tif (grace_period != USE_DEFAULT_GRACE_PERIOD) {\n\t\tretval = pm_update_grace_period(&dqm->packet_mgr, grace_period);\n\t\tif (retval)\n\t\t\treturn retval;\n\t}\n\n\tretval = pm_send_unmap_queue(&dqm->packet_mgr, filter, filter_param, reset);\n\tif (retval)\n\t\treturn retval;\n\n\t*dqm->fence_addr = KFD_FENCE_INIT;\n\tpm_send_query_status(&dqm->packet_mgr, dqm->fence_gpu_addr,\n\t\t\t\tKFD_FENCE_COMPLETED);\n\t \n\tretval = amdkfd_fence_wait_timeout(dqm->fence_addr, KFD_FENCE_COMPLETED,\n\t\t\t\tqueue_preemption_timeout_ms);\n\tif (retval) {\n\t\tpr_err(\"The cp might be in an unrecoverable state due to an unsuccessful queues preemption\\n\");\n\t\tkfd_hws_hang(dqm);\n\t\treturn retval;\n\t}\n\n\t \n\tmqd_mgr = dqm->mqd_mgrs[KFD_MQD_TYPE_HIQ];\n\tif (mqd_mgr->read_doorbell_id(dqm->packet_mgr.priv_queue->queue->mqd)) {\n\t\tpr_err(\"HIQ MQD's queue_doorbell_id0 is not 0, Queue preemption time out\\n\");\n\t\twhile (halt_if_hws_hang)\n\t\t\tschedule();\n\t\treturn -ETIME;\n\t}\n\n\t \n\tif (grace_period != USE_DEFAULT_GRACE_PERIOD) {\n\t\tif (pm_update_grace_period(&dqm->packet_mgr,\n\t\t\t\t\tUSE_DEFAULT_GRACE_PERIOD))\n\t\t\tpr_err(\"Failed to reset grace period\\n\");\n\t}\n\n\tpm_release_ib(&dqm->packet_mgr);\n\tdqm->active_runlist = false;\n\n\treturn retval;\n}\n\n \nstatic int reset_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\tuint16_t pasid)\n{\n\tint retval;\n\n\tdqm_lock(dqm);\n\n\tretval = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_BY_PASID,\n\t\t\tpasid, USE_DEFAULT_GRACE_PERIOD, true);\n\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\n \nstatic int execute_queues_cpsch(struct device_queue_manager *dqm,\n\t\t\t\tenum kfd_unmap_queues_filter filter,\n\t\t\t\tuint32_t filter_param,\n\t\t\t\tuint32_t grace_period)\n{\n\tint retval;\n\n\tif (dqm->is_hws_hang)\n\t\treturn -EIO;\n\tretval = unmap_queues_cpsch(dqm, filter, filter_param, grace_period, false);\n\tif (retval)\n\t\treturn retval;\n\n\treturn map_queues_cpsch(dqm);\n}\n\nstatic int wait_on_destroy_queue(struct device_queue_manager *dqm,\n\t\t\t\t struct queue *q)\n{\n\tstruct kfd_process_device *pdd = kfd_get_process_device_data(q->device,\n\t\t\t\t\t\t\t\tq->process);\n\tint ret = 0;\n\n\tif (pdd->qpd.is_debug)\n\t\treturn ret;\n\n\tq->properties.is_being_destroyed = true;\n\n\tif (pdd->process->debug_trap_enabled && q->properties.is_suspended) {\n\t\tdqm_unlock(dqm);\n\t\tmutex_unlock(&q->process->mutex);\n\t\tret = wait_event_interruptible(dqm->destroy_wait,\n\t\t\t\t\t\t!q->properties.is_suspended);\n\n\t\tmutex_lock(&q->process->mutex);\n\t\tdqm_lock(dqm);\n\t}\n\n\treturn ret;\n}\n\nstatic int destroy_queue_cpsch(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd,\n\t\t\t\tstruct queue *q)\n{\n\tint retval;\n\tstruct mqd_manager *mqd_mgr;\n\tuint64_t sdma_val = 0;\n\tstruct kfd_process_device *pdd = qpd_to_pdd(qpd);\n\n\t \n\tif ((q->properties.type == KFD_QUEUE_TYPE_SDMA) ||\n\t    (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)) {\n\t\tretval = read_sdma_queue_counter((uint64_t __user *)q->properties.read_ptr,\n\t\t\t\t\t\t\t&sdma_val);\n\t\tif (retval)\n\t\t\tpr_err(\"Failed to read SDMA queue counter for queue: %d\\n\",\n\t\t\t\tq->properties.queue_id);\n\t}\n\n\t \n\tdqm_lock(dqm);\n\n\tretval = wait_on_destroy_queue(dqm, q);\n\n\tif (retval) {\n\t\tdqm_unlock(dqm);\n\t\treturn retval;\n\t}\n\n\tif (qpd->is_debug) {\n\t\t \n\t\tretval = -EBUSY;\n\t\tgoto failed_try_destroy_debugged_queue;\n\n\t}\n\n\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\tq->properties.type)];\n\n\tdeallocate_doorbell(qpd, q);\n\n\tif ((q->properties.type == KFD_QUEUE_TYPE_SDMA) ||\n\t    (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)) {\n\t\tdeallocate_sdma_queue(dqm, q);\n\t\tpdd->sdma_past_activity_counter += sdma_val;\n\t}\n\n\tlist_del(&q->list);\n\tqpd->queue_count--;\n\tif (q->properties.is_active) {\n\t\tdecrement_queue_count(dqm, qpd, q);\n\t\tif (!dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\tretval = execute_queues_cpsch(dqm,\n\t\t\t\t\t\t      KFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,\n\t\t\t\t\t\t      USE_DEFAULT_GRACE_PERIOD);\n\t\t\tif (retval == -ETIME)\n\t\t\t\tqpd->reset_wavefronts = true;\n\t\t} else {\n\t\t\tretval = remove_queue_mes(dqm, q, qpd);\n\t\t}\n\t}\n\n\t \n\tdqm->total_queue_count--;\n\tpr_debug(\"Total of %d queues are accountable so far\\n\",\n\t\t\tdqm->total_queue_count);\n\n\tdqm_unlock(dqm);\n\n\t \n\tkfd_dbg_ev_raise(KFD_EC_MASK(EC_DEVICE_QUEUE_DELETE),\n\t\t\t\tqpd->pqm->process, q->device,\n\t\t\t\t-1, false, NULL, 0);\n\n\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\n\n\treturn retval;\n\nfailed_try_destroy_debugged_queue:\n\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\n \n#define APE1_FIXED_BITS_MASK 0xFFFF80000000FFFFULL\n \n#define APE1_LIMIT_ALIGNMENT 0xFFFF\n\nstatic bool set_cache_memory_policy(struct device_queue_manager *dqm,\n\t\t\t\t   struct qcm_process_device *qpd,\n\t\t\t\t   enum cache_policy default_policy,\n\t\t\t\t   enum cache_policy alternate_policy,\n\t\t\t\t   void __user *alternate_aperture_base,\n\t\t\t\t   uint64_t alternate_aperture_size)\n{\n\tbool retval = true;\n\n\tif (!dqm->asic_ops.set_cache_memory_policy)\n\t\treturn retval;\n\n\tdqm_lock(dqm);\n\n\tif (alternate_aperture_size == 0) {\n\t\t \n\t\tqpd->sh_mem_ape1_base = 1;\n\t\tqpd->sh_mem_ape1_limit = 0;\n\t} else {\n\t\t \n\n\t\tuint64_t base = (uintptr_t)alternate_aperture_base;\n\t\tuint64_t limit = base + alternate_aperture_size - 1;\n\n\t\tif (limit <= base || (base & APE1_FIXED_BITS_MASK) != 0 ||\n\t\t   (limit & APE1_FIXED_BITS_MASK) != APE1_LIMIT_ALIGNMENT) {\n\t\t\tretval = false;\n\t\t\tgoto out;\n\t\t}\n\n\t\tqpd->sh_mem_ape1_base = base >> 16;\n\t\tqpd->sh_mem_ape1_limit = limit >> 16;\n\t}\n\n\tretval = dqm->asic_ops.set_cache_memory_policy(\n\t\t\tdqm,\n\t\t\tqpd,\n\t\t\tdefault_policy,\n\t\t\talternate_policy,\n\t\t\talternate_aperture_base,\n\t\t\talternate_aperture_size);\n\n\tif ((dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) && (qpd->vmid != 0))\n\t\tprogram_sh_mem_settings(dqm, qpd);\n\n\tpr_debug(\"sh_mem_config: 0x%x, ape1_base: 0x%x, ape1_limit: 0x%x\\n\",\n\t\tqpd->sh_mem_config, qpd->sh_mem_ape1_base,\n\t\tqpd->sh_mem_ape1_limit);\n\nout:\n\tdqm_unlock(dqm);\n\treturn retval;\n}\n\nstatic int process_termination_nocpsch(struct device_queue_manager *dqm,\n\t\tstruct qcm_process_device *qpd)\n{\n\tstruct queue *q;\n\tstruct device_process_node *cur, *next_dpn;\n\tint retval = 0;\n\tbool found = false;\n\n\tdqm_lock(dqm);\n\n\t \n\twhile (!list_empty(&qpd->queues_list)) {\n\t\tstruct mqd_manager *mqd_mgr;\n\t\tint ret;\n\n\t\tq = list_first_entry(&qpd->queues_list, struct queue, list);\n\t\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\t\tq->properties.type)];\n\t\tret = destroy_queue_nocpsch_locked(dqm, qpd, q);\n\t\tif (ret)\n\t\t\tretval = ret;\n\t\tdqm_unlock(dqm);\n\t\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\n\t\tdqm_lock(dqm);\n\t}\n\n\t \n\tlist_for_each_entry_safe(cur, next_dpn, &dqm->queues, list) {\n\t\tif (qpd == cur->qpd) {\n\t\t\tlist_del(&cur->list);\n\t\t\tkfree(cur);\n\t\t\tdqm->processes_count--;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdqm_unlock(dqm);\n\n\t \n\tif (found)\n\t\tkfd_dec_compute_active(dqm->dev);\n\n\treturn retval;\n}\n\nstatic int get_wave_state(struct device_queue_manager *dqm,\n\t\t\t  struct queue *q,\n\t\t\t  void __user *ctl_stack,\n\t\t\t  u32 *ctl_stack_used_size,\n\t\t\t  u32 *save_area_used_size)\n{\n\tstruct mqd_manager *mqd_mgr;\n\n\tdqm_lock(dqm);\n\n\tmqd_mgr = dqm->mqd_mgrs[KFD_MQD_TYPE_CP];\n\n\tif (q->properties.type != KFD_QUEUE_TYPE_COMPUTE ||\n\t    q->properties.is_active || !q->device->kfd->cwsr_enabled ||\n\t    !mqd_mgr->get_wave_state) {\n\t\tdqm_unlock(dqm);\n\t\treturn -EINVAL;\n\t}\n\n\tdqm_unlock(dqm);\n\n\t \n\treturn mqd_mgr->get_wave_state(mqd_mgr, q->mqd, &q->properties,\n\t\t\tctl_stack, ctl_stack_used_size, save_area_used_size);\n}\n\nstatic void get_queue_checkpoint_info(struct device_queue_manager *dqm,\n\t\t\tconst struct queue *q,\n\t\t\tu32 *mqd_size,\n\t\t\tu32 *ctl_stack_size)\n{\n\tstruct mqd_manager *mqd_mgr;\n\tenum KFD_MQD_TYPE mqd_type =\n\t\t\tget_mqd_type_from_queue_type(q->properties.type);\n\n\tdqm_lock(dqm);\n\tmqd_mgr = dqm->mqd_mgrs[mqd_type];\n\t*mqd_size = mqd_mgr->mqd_size;\n\t*ctl_stack_size = 0;\n\n\tif (q->properties.type == KFD_QUEUE_TYPE_COMPUTE && mqd_mgr->get_checkpoint_info)\n\t\tmqd_mgr->get_checkpoint_info(mqd_mgr, q->mqd, ctl_stack_size);\n\n\tdqm_unlock(dqm);\n}\n\nstatic int checkpoint_mqd(struct device_queue_manager *dqm,\n\t\t\t  const struct queue *q,\n\t\t\t  void *mqd,\n\t\t\t  void *ctl_stack)\n{\n\tstruct mqd_manager *mqd_mgr;\n\tint r = 0;\n\tenum KFD_MQD_TYPE mqd_type =\n\t\t\tget_mqd_type_from_queue_type(q->properties.type);\n\n\tdqm_lock(dqm);\n\n\tif (q->properties.is_active || !q->device->kfd->cwsr_enabled) {\n\t\tr = -EINVAL;\n\t\tgoto dqm_unlock;\n\t}\n\n\tmqd_mgr = dqm->mqd_mgrs[mqd_type];\n\tif (!mqd_mgr->checkpoint_mqd) {\n\t\tr = -EOPNOTSUPP;\n\t\tgoto dqm_unlock;\n\t}\n\n\tmqd_mgr->checkpoint_mqd(mqd_mgr, q->mqd, mqd, ctl_stack);\n\ndqm_unlock:\n\tdqm_unlock(dqm);\n\treturn r;\n}\n\nstatic int process_termination_cpsch(struct device_queue_manager *dqm,\n\t\tstruct qcm_process_device *qpd)\n{\n\tint retval;\n\tstruct queue *q;\n\tstruct kernel_queue *kq, *kq_next;\n\tstruct mqd_manager *mqd_mgr;\n\tstruct device_process_node *cur, *next_dpn;\n\tenum kfd_unmap_queues_filter filter =\n\t\tKFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES;\n\tbool found = false;\n\n\tretval = 0;\n\n\tdqm_lock(dqm);\n\n\t \n\tlist_for_each_entry_safe(kq, kq_next, &qpd->priv_queue_list, list) {\n\t\tlist_del(&kq->list);\n\t\tdecrement_queue_count(dqm, qpd, kq->queue);\n\t\tqpd->is_debug = false;\n\t\tdqm->total_queue_count--;\n\t\tfilter = KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES;\n\t}\n\n\t \n\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\tif (q->properties.type == KFD_QUEUE_TYPE_SDMA)\n\t\t\tdeallocate_sdma_queue(dqm, q);\n\t\telse if (q->properties.type == KFD_QUEUE_TYPE_SDMA_XGMI)\n\t\t\tdeallocate_sdma_queue(dqm, q);\n\n\t\tif (q->properties.is_active) {\n\t\t\tdecrement_queue_count(dqm, qpd, q);\n\n\t\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\t\tretval = remove_queue_mes(dqm, q, qpd);\n\t\t\t\tif (retval)\n\t\t\t\t\tpr_err(\"Failed to remove queue %d\\n\",\n\t\t\t\t\t\tq->properties.queue_id);\n\t\t\t}\n\t\t}\n\n\t\tdqm->total_queue_count--;\n\t}\n\n\t \n\tlist_for_each_entry_safe(cur, next_dpn, &dqm->queues, list) {\n\t\tif (qpd == cur->qpd) {\n\t\t\tlist_del(&cur->list);\n\t\t\tkfree(cur);\n\t\t\tdqm->processes_count--;\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tretval = execute_queues_cpsch(dqm, filter, 0, USE_DEFAULT_GRACE_PERIOD);\n\n\tif ((!dqm->is_hws_hang) && (retval || qpd->reset_wavefronts)) {\n\t\tpr_warn(\"Resetting wave fronts (cpsch) on dev %p\\n\", dqm->dev);\n\t\tdbgdev_wave_reset_wavefronts(dqm->dev, qpd->pqm->process);\n\t\tqpd->reset_wavefronts = false;\n\t}\n\n\t \n\twhile (!list_empty(&qpd->queues_list)) {\n\t\tq = list_first_entry(&qpd->queues_list, struct queue, list);\n\t\tmqd_mgr = dqm->mqd_mgrs[get_mqd_type_from_queue_type(\n\t\t\t\tq->properties.type)];\n\t\tlist_del(&q->list);\n\t\tqpd->queue_count--;\n\t\tdqm_unlock(dqm);\n\t\tmqd_mgr->free_mqd(mqd_mgr, q->mqd, q->mqd_mem_obj);\n\t\tdqm_lock(dqm);\n\t}\n\tdqm_unlock(dqm);\n\n\t \n\tif (found)\n\t\tkfd_dec_compute_active(dqm->dev);\n\n\treturn retval;\n}\n\nstatic int init_mqd_managers(struct device_queue_manager *dqm)\n{\n\tint i, j;\n\tstruct mqd_manager *mqd_mgr;\n\n\tfor (i = 0; i < KFD_MQD_TYPE_MAX; i++) {\n\t\tmqd_mgr = dqm->asic_ops.mqd_manager_init(i, dqm->dev);\n\t\tif (!mqd_mgr) {\n\t\t\tpr_err(\"mqd manager [%d] initialization failed\\n\", i);\n\t\t\tgoto out_free;\n\t\t}\n\t\tdqm->mqd_mgrs[i] = mqd_mgr;\n\t}\n\n\treturn 0;\n\nout_free:\n\tfor (j = 0; j < i; j++) {\n\t\tkfree(dqm->mqd_mgrs[j]);\n\t\tdqm->mqd_mgrs[j] = NULL;\n\t}\n\n\treturn -ENOMEM;\n}\n\n \nstatic int allocate_hiq_sdma_mqd(struct device_queue_manager *dqm)\n{\n\tint retval;\n\tstruct kfd_node *dev = dqm->dev;\n\tstruct kfd_mem_obj *mem_obj = &dqm->hiq_sdma_mqd;\n\tuint32_t size = dqm->mqd_mgrs[KFD_MQD_TYPE_SDMA]->mqd_size *\n\t\tget_num_all_sdma_engines(dqm) *\n\t\tdev->kfd->device_info.num_sdma_queues_per_engine +\n\t\t(dqm->mqd_mgrs[KFD_MQD_TYPE_HIQ]->mqd_size *\n\t\tNUM_XCC(dqm->dev->xcc_mask));\n\n\tretval = amdgpu_amdkfd_alloc_gtt_mem(dev->adev, size,\n\t\t&(mem_obj->gtt_mem), &(mem_obj->gpu_addr),\n\t\t(void *)&(mem_obj->cpu_ptr), false);\n\n\treturn retval;\n}\n\nstruct device_queue_manager *device_queue_manager_init(struct kfd_node *dev)\n{\n\tstruct device_queue_manager *dqm;\n\n\tpr_debug(\"Loading device queue manager\\n\");\n\n\tdqm = kzalloc(sizeof(*dqm), GFP_KERNEL);\n\tif (!dqm)\n\t\treturn NULL;\n\n\tswitch (dev->adev->asic_type) {\n\t \n\tcase CHIP_HAWAII:\n\t \n\tcase CHIP_TONGA:\n\t\tdqm->sched_policy = KFD_SCHED_POLICY_NO_HWS;\n\t\tbreak;\n\tdefault:\n\t\tdqm->sched_policy = sched_policy;\n\t\tbreak;\n\t}\n\n\tdqm->dev = dev;\n\tswitch (dqm->sched_policy) {\n\tcase KFD_SCHED_POLICY_HWS:\n\tcase KFD_SCHED_POLICY_HWS_NO_OVERSUBSCRIPTION:\n\t\t \n\t\tdqm->ops.create_queue = create_queue_cpsch;\n\t\tdqm->ops.initialize = initialize_cpsch;\n\t\tdqm->ops.start = start_cpsch;\n\t\tdqm->ops.stop = stop_cpsch;\n\t\tdqm->ops.pre_reset = pre_reset;\n\t\tdqm->ops.destroy_queue = destroy_queue_cpsch;\n\t\tdqm->ops.update_queue = update_queue;\n\t\tdqm->ops.register_process = register_process;\n\t\tdqm->ops.unregister_process = unregister_process;\n\t\tdqm->ops.uninitialize = uninitialize;\n\t\tdqm->ops.create_kernel_queue = create_kernel_queue_cpsch;\n\t\tdqm->ops.destroy_kernel_queue = destroy_kernel_queue_cpsch;\n\t\tdqm->ops.set_cache_memory_policy = set_cache_memory_policy;\n\t\tdqm->ops.process_termination = process_termination_cpsch;\n\t\tdqm->ops.evict_process_queues = evict_process_queues_cpsch;\n\t\tdqm->ops.restore_process_queues = restore_process_queues_cpsch;\n\t\tdqm->ops.get_wave_state = get_wave_state;\n\t\tdqm->ops.reset_queues = reset_queues_cpsch;\n\t\tdqm->ops.get_queue_checkpoint_info = get_queue_checkpoint_info;\n\t\tdqm->ops.checkpoint_mqd = checkpoint_mqd;\n\t\tbreak;\n\tcase KFD_SCHED_POLICY_NO_HWS:\n\t\t \n\t\tdqm->ops.start = start_nocpsch;\n\t\tdqm->ops.stop = stop_nocpsch;\n\t\tdqm->ops.pre_reset = pre_reset;\n\t\tdqm->ops.create_queue = create_queue_nocpsch;\n\t\tdqm->ops.destroy_queue = destroy_queue_nocpsch;\n\t\tdqm->ops.update_queue = update_queue;\n\t\tdqm->ops.register_process = register_process;\n\t\tdqm->ops.unregister_process = unregister_process;\n\t\tdqm->ops.initialize = initialize_nocpsch;\n\t\tdqm->ops.uninitialize = uninitialize;\n\t\tdqm->ops.set_cache_memory_policy = set_cache_memory_policy;\n\t\tdqm->ops.process_termination = process_termination_nocpsch;\n\t\tdqm->ops.evict_process_queues = evict_process_queues_nocpsch;\n\t\tdqm->ops.restore_process_queues =\n\t\t\trestore_process_queues_nocpsch;\n\t\tdqm->ops.get_wave_state = get_wave_state;\n\t\tdqm->ops.get_queue_checkpoint_info = get_queue_checkpoint_info;\n\t\tdqm->ops.checkpoint_mqd = checkpoint_mqd;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Invalid scheduling policy %d\\n\", dqm->sched_policy);\n\t\tgoto out_free;\n\t}\n\n\tswitch (dev->adev->asic_type) {\n\tcase CHIP_KAVERI:\n\tcase CHIP_HAWAII:\n\t\tdevice_queue_manager_init_cik(&dqm->asic_ops);\n\t\tbreak;\n\n\tcase CHIP_CARRIZO:\n\tcase CHIP_TONGA:\n\tcase CHIP_FIJI:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tdevice_queue_manager_init_vi(&dqm->asic_ops);\n\t\tbreak;\n\n\tdefault:\n\t\tif (KFD_GC_VERSION(dev) >= IP_VERSION(11, 0, 0))\n\t\t\tdevice_queue_manager_init_v11(&dqm->asic_ops);\n\t\telse if (KFD_GC_VERSION(dev) >= IP_VERSION(10, 1, 1))\n\t\t\tdevice_queue_manager_init_v10(&dqm->asic_ops);\n\t\telse if (KFD_GC_VERSION(dev) >= IP_VERSION(9, 0, 1))\n\t\t\tdevice_queue_manager_init_v9(&dqm->asic_ops);\n\t\telse {\n\t\t\tWARN(1, \"Unexpected ASIC family %u\",\n\t\t\t     dev->adev->asic_type);\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\tif (init_mqd_managers(dqm))\n\t\tgoto out_free;\n\n\tif (!dev->kfd->shared_resources.enable_mes && allocate_hiq_sdma_mqd(dqm)) {\n\t\tpr_err(\"Failed to allocate hiq sdma mqd trunk buffer\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (!dqm->ops.initialize(dqm)) {\n\t\tinit_waitqueue_head(&dqm->destroy_wait);\n\t\treturn dqm;\n\t}\n\nout_free:\n\tkfree(dqm);\n\treturn NULL;\n}\n\nstatic void deallocate_hiq_sdma_mqd(struct kfd_node *dev,\n\t\t\t\t    struct kfd_mem_obj *mqd)\n{\n\tWARN(!mqd, \"No hiq sdma mqd trunk to free\");\n\n\tamdgpu_amdkfd_free_gtt_mem(dev->adev, mqd->gtt_mem);\n}\n\nvoid device_queue_manager_uninit(struct device_queue_manager *dqm)\n{\n\tdqm->ops.stop(dqm);\n\tdqm->ops.uninitialize(dqm);\n\tif (!dqm->dev->kfd->shared_resources.enable_mes)\n\t\tdeallocate_hiq_sdma_mqd(dqm->dev, &dqm->hiq_sdma_mqd);\n\tkfree(dqm);\n}\n\nint kfd_dqm_evict_pasid(struct device_queue_manager *dqm, u32 pasid)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct kfd_process *p = kfd_lookup_process_by_pasid(pasid);\n\tint ret = 0;\n\n\tif (!p)\n\t\treturn -EINVAL;\n\tWARN(debug_evictions, \"Evicting pid %d\", p->lead_thread->pid);\n\tpdd = kfd_get_process_device_data(dqm->dev, p);\n\tif (pdd)\n\t\tret = dqm->ops.evict_process_queues(dqm, &pdd->qpd);\n\tkfd_unref_process(p);\n\n\treturn ret;\n}\n\nstatic void kfd_process_hw_exception(struct work_struct *work)\n{\n\tstruct device_queue_manager *dqm = container_of(work,\n\t\t\tstruct device_queue_manager, hw_exception_work);\n\tamdgpu_amdkfd_gpu_reset(dqm->dev->adev);\n}\n\nint reserve_debug_trap_vmid(struct device_queue_manager *dqm,\n\t\t\t\tstruct qcm_process_device *qpd)\n{\n\tint r;\n\tint updated_vmid_mask;\n\n\tif (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tpr_err(\"Unsupported on sched_policy: %i\\n\", dqm->sched_policy);\n\t\treturn -EINVAL;\n\t}\n\n\tdqm_lock(dqm);\n\n\tif (dqm->trap_debug_vmid != 0) {\n\t\tpr_err(\"Trap debug id already reserved\\n\");\n\t\tr = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tr = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,\n\t\t\tUSE_DEFAULT_GRACE_PERIOD, false);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tupdated_vmid_mask = dqm->dev->kfd->shared_resources.compute_vmid_bitmap;\n\tupdated_vmid_mask &= ~(1 << dqm->dev->vm_info.last_vmid_kfd);\n\n\tdqm->dev->kfd->shared_resources.compute_vmid_bitmap = updated_vmid_mask;\n\tdqm->trap_debug_vmid = dqm->dev->vm_info.last_vmid_kfd;\n\tr = set_sched_resources(dqm);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tr = map_queues_cpsch(dqm);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tpr_debug(\"Reserved VMID for trap debug: %i\\n\", dqm->trap_debug_vmid);\n\nout_unlock:\n\tdqm_unlock(dqm);\n\treturn r;\n}\n\n \nint release_debug_trap_vmid(struct device_queue_manager *dqm,\n\t\t\tstruct qcm_process_device *qpd)\n{\n\tint r;\n\tint updated_vmid_mask;\n\tuint32_t trap_debug_vmid;\n\n\tif (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tpr_err(\"Unsupported on sched_policy: %i\\n\", dqm->sched_policy);\n\t\treturn -EINVAL;\n\t}\n\n\tdqm_lock(dqm);\n\ttrap_debug_vmid = dqm->trap_debug_vmid;\n\tif (dqm->trap_debug_vmid == 0) {\n\t\tpr_err(\"Trap debug id is not reserved\\n\");\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tr = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0,\n\t\t\tUSE_DEFAULT_GRACE_PERIOD, false);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tupdated_vmid_mask = dqm->dev->kfd->shared_resources.compute_vmid_bitmap;\n\tupdated_vmid_mask |= (1 << dqm->dev->vm_info.last_vmid_kfd);\n\n\tdqm->dev->kfd->shared_resources.compute_vmid_bitmap = updated_vmid_mask;\n\tdqm->trap_debug_vmid = 0;\n\tr = set_sched_resources(dqm);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tr = map_queues_cpsch(dqm);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tpr_debug(\"Released VMID for trap debug: %i\\n\", trap_debug_vmid);\n\nout_unlock:\n\tdqm_unlock(dqm);\n\treturn r;\n}\n\n#define QUEUE_NOT_FOUND\t\t-1\n \nstatic void q_array_invalidate(uint32_t num_queues, uint32_t *queue_ids)\n{\n\tint i;\n\n\tfor (i = 0; i < num_queues; i++)\n\t\tqueue_ids[i] |= KFD_DBG_QUEUE_INVALID_MASK;\n}\n\n \nstatic int q_array_get_index(unsigned int queue_id,\n\t\tuint32_t num_queues,\n\t\tuint32_t *queue_ids)\n{\n\tint i;\n\n\tfor (i = 0; i < num_queues; i++)\n\t\tif (queue_id == (queue_ids[i] & ~KFD_DBG_QUEUE_INVALID_MASK))\n\t\t\treturn i;\n\n\treturn QUEUE_NOT_FOUND;\n}\n\nstruct copy_context_work_handler_workarea {\n\tstruct work_struct copy_context_work;\n\tstruct kfd_process *p;\n};\n\nstatic void copy_context_work_handler (struct work_struct *work)\n{\n\tstruct copy_context_work_handler_workarea *workarea;\n\tstruct mqd_manager *mqd_mgr;\n\tstruct queue *q;\n\tstruct mm_struct *mm;\n\tstruct kfd_process *p;\n\tuint32_t tmp_ctl_stack_used_size, tmp_save_area_used_size;\n\tint i;\n\n\tworkarea = container_of(work,\n\t\t\tstruct copy_context_work_handler_workarea,\n\t\t\tcopy_context_work);\n\n\tp = workarea->p;\n\tmm = get_task_mm(p->lead_thread);\n\n\tif (!mm)\n\t\treturn;\n\n\tkthread_use_mm(mm);\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tstruct device_queue_manager *dqm = pdd->dev->dqm;\n\t\tstruct qcm_process_device *qpd = &pdd->qpd;\n\n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tmqd_mgr = dqm->mqd_mgrs[KFD_MQD_TYPE_CP];\n\n\t\t\t \n\t\t\tmqd_mgr->get_wave_state(mqd_mgr,\n\t\t\t\t\tq->mqd,\n\t\t\t\t\t&q->properties,\n\t\t\t\t\t(void __user *)\tq->properties.ctx_save_restore_area_address,\n\t\t\t\t\t&tmp_ctl_stack_used_size,\n\t\t\t\t\t&tmp_save_area_used_size);\n\t\t}\n\t}\n\tkthread_unuse_mm(mm);\n\tmmput(mm);\n}\n\nstatic uint32_t *get_queue_ids(uint32_t num_queues, uint32_t *usr_queue_id_array)\n{\n\tsize_t array_size = num_queues * sizeof(uint32_t);\n\n\tif (!usr_queue_id_array)\n\t\treturn NULL;\n\n\treturn memdup_user(usr_queue_id_array, array_size);\n}\n\nint resume_queues(struct kfd_process *p,\n\t\tuint32_t num_queues,\n\t\tuint32_t *usr_queue_id_array)\n{\n\tuint32_t *queue_ids = NULL;\n\tint total_resumed = 0;\n\tint i;\n\n\tif (usr_queue_id_array) {\n\t\tqueue_ids = get_queue_ids(num_queues, usr_queue_id_array);\n\n\t\tif (IS_ERR(queue_ids))\n\t\t\treturn PTR_ERR(queue_ids);\n\n\t\t \n\t\tq_array_invalidate(num_queues, queue_ids);\n\t}\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tstruct device_queue_manager *dqm = pdd->dev->dqm;\n\t\tstruct qcm_process_device *qpd = &pdd->qpd;\n\t\tstruct queue *q;\n\t\tint r, per_device_resumed = 0;\n\n\t\tdqm_lock(dqm);\n\n\t\t \n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tint q_idx = QUEUE_NOT_FOUND;\n\n\t\t\tif (queue_ids)\n\t\t\t\tq_idx = q_array_get_index(\n\t\t\t\t\t\tq->properties.queue_id,\n\t\t\t\t\t\tnum_queues,\n\t\t\t\t\t\tqueue_ids);\n\n\t\t\tif (!queue_ids || q_idx != QUEUE_NOT_FOUND) {\n\t\t\t\tint err = resume_single_queue(dqm, &pdd->qpd, q);\n\n\t\t\t\tif (queue_ids) {\n\t\t\t\t\tif (!err) {\n\t\t\t\t\t\tqueue_ids[q_idx] &=\n\t\t\t\t\t\t\t~KFD_DBG_QUEUE_INVALID_MASK;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tqueue_ids[q_idx] |=\n\t\t\t\t\t\t\tKFD_DBG_QUEUE_ERROR_MASK;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (dqm->dev->kfd->shared_resources.enable_mes) {\n\t\t\t\t\twake_up_all(&dqm->destroy_wait);\n\t\t\t\t\tif (!err)\n\t\t\t\t\t\ttotal_resumed++;\n\t\t\t\t} else {\n\t\t\t\t\tper_device_resumed++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (!per_device_resumed) {\n\t\t\tdqm_unlock(dqm);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = execute_queues_cpsch(dqm,\n\t\t\t\t\tKFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES,\n\t\t\t\t\t0,\n\t\t\t\t\tUSE_DEFAULT_GRACE_PERIOD);\n\t\tif (r) {\n\t\t\tpr_err(\"Failed to resume process queues\\n\");\n\t\t\tif (queue_ids) {\n\t\t\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\t\t\tint q_idx = q_array_get_index(\n\t\t\t\t\t\t\tq->properties.queue_id,\n\t\t\t\t\t\t\tnum_queues,\n\t\t\t\t\t\t\tqueue_ids);\n\n\t\t\t\t\t \n\t\t\t\t\tif (q_idx != QUEUE_NOT_FOUND)\n\t\t\t\t\t\tqueue_ids[q_idx] |=\n\t\t\t\t\t\t\tKFD_DBG_QUEUE_ERROR_MASK;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\twake_up_all(&dqm->destroy_wait);\n\t\t\ttotal_resumed += per_device_resumed;\n\t\t}\n\n\t\tdqm_unlock(dqm);\n\t}\n\n\tif (queue_ids) {\n\t\tif (copy_to_user((void __user *)usr_queue_id_array, queue_ids,\n\t\t\t\tnum_queues * sizeof(uint32_t)))\n\t\t\tpr_err(\"copy_to_user failed on queue resume\\n\");\n\n\t\tkfree(queue_ids);\n\t}\n\n\treturn total_resumed;\n}\n\nint suspend_queues(struct kfd_process *p,\n\t\t\tuint32_t num_queues,\n\t\t\tuint32_t grace_period,\n\t\t\tuint64_t exception_clear_mask,\n\t\t\tuint32_t *usr_queue_id_array)\n{\n\tuint32_t *queue_ids = get_queue_ids(num_queues, usr_queue_id_array);\n\tint total_suspended = 0;\n\tint i;\n\n\tif (IS_ERR(queue_ids))\n\t\treturn PTR_ERR(queue_ids);\n\n\t \n\tq_array_invalidate(num_queues, queue_ids);\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tstruct device_queue_manager *dqm = pdd->dev->dqm;\n\t\tstruct qcm_process_device *qpd = &pdd->qpd;\n\t\tstruct queue *q;\n\t\tint r, per_device_suspended = 0;\n\n\t\tmutex_lock(&p->event_mutex);\n\t\tdqm_lock(dqm);\n\n\t\t \n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tint q_idx = q_array_get_index(q->properties.queue_id,\n\t\t\t\t\t\t\tnum_queues,\n\t\t\t\t\t\t\tqueue_ids);\n\n\t\t\tif (q_idx != QUEUE_NOT_FOUND) {\n\t\t\t\tint err = suspend_single_queue(dqm, pdd, q);\n\t\t\t\tbool is_mes = dqm->dev->kfd->shared_resources.enable_mes;\n\n\t\t\t\tif (!err) {\n\t\t\t\t\tqueue_ids[q_idx] &= ~KFD_DBG_QUEUE_INVALID_MASK;\n\t\t\t\t\tif (exception_clear_mask && is_mes)\n\t\t\t\t\t\tq->properties.exception_status &=\n\t\t\t\t\t\t\t~exception_clear_mask;\n\n\t\t\t\t\tif (is_mes)\n\t\t\t\t\t\ttotal_suspended++;\n\t\t\t\t\telse\n\t\t\t\t\t\tper_device_suspended++;\n\t\t\t\t} else if (err != -EBUSY) {\n\t\t\t\t\tr = err;\n\t\t\t\t\tqueue_ids[q_idx] |= KFD_DBG_QUEUE_ERROR_MASK;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (!per_device_suspended) {\n\t\t\tdqm_unlock(dqm);\n\t\t\tmutex_unlock(&p->event_mutex);\n\t\t\tif (total_suspended)\n\t\t\t\tamdgpu_amdkfd_debug_mem_fence(dqm->dev->adev);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = execute_queues_cpsch(dqm,\n\t\t\tKFD_UNMAP_QUEUES_FILTER_DYNAMIC_QUEUES, 0,\n\t\t\tgrace_period);\n\n\t\tif (r)\n\t\t\tpr_err(\"Failed to suspend process queues.\\n\");\n\t\telse\n\t\t\ttotal_suspended += per_device_suspended;\n\n\t\tlist_for_each_entry(q, &qpd->queues_list, list) {\n\t\t\tint q_idx = q_array_get_index(q->properties.queue_id,\n\t\t\t\t\t\tnum_queues, queue_ids);\n\n\t\t\tif (q_idx == QUEUE_NOT_FOUND)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (r)\n\t\t\t\tqueue_ids[q_idx] |= KFD_DBG_QUEUE_ERROR_MASK;\n\t\t\telse if (exception_clear_mask)\n\t\t\t\tq->properties.exception_status &=\n\t\t\t\t\t\t\t~exception_clear_mask;\n\t\t}\n\n\t\tdqm_unlock(dqm);\n\t\tmutex_unlock(&p->event_mutex);\n\t\tamdgpu_device_flush_hdp(dqm->dev->adev, NULL);\n\t}\n\n\tif (total_suspended) {\n\t\tstruct copy_context_work_handler_workarea copy_context_worker;\n\n\t\tINIT_WORK_ONSTACK(\n\t\t\t\t&copy_context_worker.copy_context_work,\n\t\t\t\tcopy_context_work_handler);\n\n\t\tcopy_context_worker.p = p;\n\n\t\tschedule_work(&copy_context_worker.copy_context_work);\n\n\n\t\tflush_work(&copy_context_worker.copy_context_work);\n\t\tdestroy_work_on_stack(&copy_context_worker.copy_context_work);\n\t}\n\n\tif (copy_to_user((void __user *)usr_queue_id_array, queue_ids,\n\t\t\tnum_queues * sizeof(uint32_t)))\n\t\tpr_err(\"copy_to_user failed on queue suspend\\n\");\n\n\tkfree(queue_ids);\n\n\treturn total_suspended;\n}\n\nstatic uint32_t set_queue_type_for_user(struct queue_properties *q_props)\n{\n\tswitch (q_props->type) {\n\tcase KFD_QUEUE_TYPE_COMPUTE:\n\t\treturn q_props->format == KFD_QUEUE_FORMAT_PM4\n\t\t\t\t\t? KFD_IOC_QUEUE_TYPE_COMPUTE\n\t\t\t\t\t: KFD_IOC_QUEUE_TYPE_COMPUTE_AQL;\n\tcase KFD_QUEUE_TYPE_SDMA:\n\t\treturn KFD_IOC_QUEUE_TYPE_SDMA;\n\tcase KFD_QUEUE_TYPE_SDMA_XGMI:\n\t\treturn KFD_IOC_QUEUE_TYPE_SDMA_XGMI;\n\tdefault:\n\t\tWARN_ONCE(true, \"queue type not recognized!\");\n\t\treturn 0xffffffff;\n\t};\n}\n\nvoid set_queue_snapshot_entry(struct queue *q,\n\t\t\t      uint64_t exception_clear_mask,\n\t\t\t      struct kfd_queue_snapshot_entry *qss_entry)\n{\n\tqss_entry->ring_base_address = q->properties.queue_address;\n\tqss_entry->write_pointer_address = (uint64_t)q->properties.write_ptr;\n\tqss_entry->read_pointer_address = (uint64_t)q->properties.read_ptr;\n\tqss_entry->ctx_save_restore_address =\n\t\t\t\tq->properties.ctx_save_restore_area_address;\n\tqss_entry->ctx_save_restore_area_size =\n\t\t\t\tq->properties.ctx_save_restore_area_size;\n\tqss_entry->exception_status = q->properties.exception_status;\n\tqss_entry->queue_id = q->properties.queue_id;\n\tqss_entry->gpu_id = q->device->id;\n\tqss_entry->ring_size = (uint32_t)q->properties.queue_size;\n\tqss_entry->queue_type = set_queue_type_for_user(&q->properties);\n\tq->properties.exception_status &= ~exception_clear_mask;\n}\n\nint debug_lock_and_unmap(struct device_queue_manager *dqm)\n{\n\tint r;\n\n\tif (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tpr_err(\"Unsupported on sched_policy: %i\\n\", dqm->sched_policy);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!kfd_dbg_is_per_vmid_supported(dqm->dev))\n\t\treturn 0;\n\n\tdqm_lock(dqm);\n\n\tr = unmap_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES, 0, 0, false);\n\tif (r)\n\t\tdqm_unlock(dqm);\n\n\treturn r;\n}\n\nint debug_map_and_unlock(struct device_queue_manager *dqm)\n{\n\tint r;\n\n\tif (dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tpr_err(\"Unsupported on sched_policy: %i\\n\", dqm->sched_policy);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!kfd_dbg_is_per_vmid_supported(dqm->dev))\n\t\treturn 0;\n\n\tr = map_queues_cpsch(dqm);\n\n\tdqm_unlock(dqm);\n\n\treturn r;\n}\n\nint debug_refresh_runlist(struct device_queue_manager *dqm)\n{\n\tint r = debug_lock_and_unmap(dqm);\n\n\tif (r)\n\t\treturn r;\n\n\treturn debug_map_and_unlock(dqm);\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nstatic void seq_reg_dump(struct seq_file *m,\n\t\t\t uint32_t (*dump)[2], uint32_t n_regs)\n{\n\tuint32_t i, count;\n\n\tfor (i = 0, count = 0; i < n_regs; i++) {\n\t\tif (count == 0 ||\n\t\t    dump[i-1][0] + sizeof(uint32_t) != dump[i][0]) {\n\t\t\tseq_printf(m, \"%s    %08x: %08x\",\n\t\t\t\t   i ? \"\\n\" : \"\",\n\t\t\t\t   dump[i][0], dump[i][1]);\n\t\t\tcount = 7;\n\t\t} else {\n\t\t\tseq_printf(m, \" %08x\", dump[i][1]);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tseq_puts(m, \"\\n\");\n}\n\nint dqm_debugfs_hqds(struct seq_file *m, void *data)\n{\n\tstruct device_queue_manager *dqm = data;\n\tuint32_t xcc_mask = dqm->dev->xcc_mask;\n\tuint32_t (*dump)[2], n_regs;\n\tint pipe, queue;\n\tint r = 0, xcc_id;\n\tuint32_t sdma_engine_start;\n\n\tif (!dqm->sched_running) {\n\t\tseq_puts(m, \" Device is stopped\\n\");\n\t\treturn 0;\n\t}\n\n\tfor_each_inst(xcc_id, xcc_mask) {\n\t\tr = dqm->dev->kfd2kgd->hqd_dump(dqm->dev->adev,\n\t\t\t\t\t\tKFD_CIK_HIQ_PIPE,\n\t\t\t\t\t\tKFD_CIK_HIQ_QUEUE, &dump,\n\t\t\t\t\t\t&n_regs, xcc_id);\n\t\tif (!r) {\n\t\t\tseq_printf(\n\t\t\t\tm,\n\t\t\t\t\"   Inst %d, HIQ on MEC %d Pipe %d Queue %d\\n\",\n\t\t\t\txcc_id,\n\t\t\t\tKFD_CIK_HIQ_PIPE / get_pipes_per_mec(dqm) + 1,\n\t\t\t\tKFD_CIK_HIQ_PIPE % get_pipes_per_mec(dqm),\n\t\t\t\tKFD_CIK_HIQ_QUEUE);\n\t\t\tseq_reg_dump(m, dump, n_regs);\n\n\t\t\tkfree(dump);\n\t\t}\n\n\t\tfor (pipe = 0; pipe < get_pipes_per_mec(dqm); pipe++) {\n\t\t\tint pipe_offset = pipe * get_queues_per_pipe(dqm);\n\n\t\t\tfor (queue = 0; queue < get_queues_per_pipe(dqm); queue++) {\n\t\t\t\tif (!test_bit(pipe_offset + queue,\n\t\t\t\t      dqm->dev->kfd->shared_resources.cp_queue_bitmap))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tr = dqm->dev->kfd2kgd->hqd_dump(dqm->dev->adev,\n\t\t\t\t\t\t\t\tpipe, queue,\n\t\t\t\t\t\t\t\t&dump, &n_regs,\n\t\t\t\t\t\t\t\txcc_id);\n\t\t\t\tif (r)\n\t\t\t\t\tbreak;\n\n\t\t\t\tseq_printf(m,\n\t\t\t\t\t   \" Inst %d,  CP Pipe %d, Queue %d\\n\",\n\t\t\t\t\t   xcc_id, pipe, queue);\n\t\t\t\tseq_reg_dump(m, dump, n_regs);\n\n\t\t\t\tkfree(dump);\n\t\t\t}\n\t\t}\n\t}\n\n\tsdma_engine_start = dqm->dev->node_id * get_num_all_sdma_engines(dqm);\n\tfor (pipe = sdma_engine_start;\n\t     pipe < (sdma_engine_start + get_num_all_sdma_engines(dqm));\n\t     pipe++) {\n\t\tfor (queue = 0;\n\t\t     queue < dqm->dev->kfd->device_info.num_sdma_queues_per_engine;\n\t\t     queue++) {\n\t\t\tr = dqm->dev->kfd2kgd->hqd_sdma_dump(\n\t\t\t\tdqm->dev->adev, pipe, queue, &dump, &n_regs);\n\t\t\tif (r)\n\t\t\t\tbreak;\n\n\t\t\tseq_printf(m, \"  SDMA Engine %d, RLC %d\\n\",\n\t\t\t\t  pipe, queue);\n\t\t\tseq_reg_dump(m, dump, n_regs);\n\n\t\t\tkfree(dump);\n\t\t}\n\t}\n\n\treturn r;\n}\n\nint dqm_debugfs_hang_hws(struct device_queue_manager *dqm)\n{\n\tint r = 0;\n\n\tdqm_lock(dqm);\n\tr = pm_debugfs_hang_hws(&dqm->packet_mgr);\n\tif (r) {\n\t\tdqm_unlock(dqm);\n\t\treturn r;\n\t}\n\tdqm->active_runlist = true;\n\tr = execute_queues_cpsch(dqm, KFD_UNMAP_QUEUES_FILTER_ALL_QUEUES,\n\t\t\t\t0, USE_DEFAULT_GRACE_PERIOD);\n\tdqm_unlock(dqm);\n\n\treturn r;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}