{
  "module_name": "kfd_device.c",
  "hash_id": "129f34c9f4007b962d7c877a5d808fcb1228b2bdabc8d13e4dc375c9611f155e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_device.c",
  "human_readable_source": "\n \n\n#include <linux/bsearch.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include \"kfd_priv.h\"\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_pm4_headers_vi.h\"\n#include \"kfd_pm4_headers_aldebaran.h\"\n#include \"cwsr_trap_handler.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"kfd_smi_events.h\"\n#include \"kfd_svm.h\"\n#include \"kfd_migrate.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_xcp.h\"\n\n#define MQD_SIZE_ALIGNED 768\n\n \nstatic int kfd_locked;\n\n#ifdef CONFIG_DRM_AMDGPU_CIK\nextern const struct kfd2kgd_calls gfx_v7_kfd2kgd;\n#endif\nextern const struct kfd2kgd_calls gfx_v8_kfd2kgd;\nextern const struct kfd2kgd_calls gfx_v9_kfd2kgd;\nextern const struct kfd2kgd_calls arcturus_kfd2kgd;\nextern const struct kfd2kgd_calls aldebaran_kfd2kgd;\nextern const struct kfd2kgd_calls gc_9_4_3_kfd2kgd;\nextern const struct kfd2kgd_calls gfx_v10_kfd2kgd;\nextern const struct kfd2kgd_calls gfx_v10_3_kfd2kgd;\nextern const struct kfd2kgd_calls gfx_v11_kfd2kgd;\n\nstatic int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,\n\t\t\t\tunsigned int chunk_size);\nstatic void kfd_gtt_sa_fini(struct kfd_dev *kfd);\n\nstatic int kfd_resume(struct kfd_node *kfd);\n\nstatic void kfd_device_info_set_sdma_info(struct kfd_dev *kfd)\n{\n\tuint32_t sdma_version = kfd->adev->ip_versions[SDMA0_HWIP][0];\n\n\tswitch (sdma_version) {\n\tcase IP_VERSION(4, 0, 0): \n\tcase IP_VERSION(4, 0, 1): \n\tcase IP_VERSION(4, 1, 0): \n\tcase IP_VERSION(4, 1, 1): \n\tcase IP_VERSION(4, 1, 2): \n\tcase IP_VERSION(5, 2, 1): \n\tcase IP_VERSION(5, 2, 3): \n\tcase IP_VERSION(5, 2, 6): \n\tcase IP_VERSION(5, 2, 7): \n\t\tkfd->device_info.num_sdma_queues_per_engine = 2;\n\t\tbreak;\n\tcase IP_VERSION(4, 2, 0): \n\tcase IP_VERSION(4, 2, 2): \n\tcase IP_VERSION(4, 4, 0): \n\tcase IP_VERSION(4, 4, 2):\n\tcase IP_VERSION(5, 0, 0): \n\tcase IP_VERSION(5, 0, 1): \n\tcase IP_VERSION(5, 0, 2): \n\tcase IP_VERSION(5, 0, 5): \n\tcase IP_VERSION(5, 2, 0): \n\tcase IP_VERSION(5, 2, 2): \n\tcase IP_VERSION(5, 2, 4): \n\tcase IP_VERSION(5, 2, 5): \n\tcase IP_VERSION(6, 0, 0):\n\tcase IP_VERSION(6, 0, 1):\n\tcase IP_VERSION(6, 0, 2):\n\tcase IP_VERSION(6, 0, 3):\n\t\tkfd->device_info.num_sdma_queues_per_engine = 8;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(kfd_device,\n\t\t\t\"Default sdma queue per engine(8) is set due to mismatch of sdma ip block(SDMA_HWIP:0x%x).\\n\",\n\t\t\tsdma_version);\n\t\tkfd->device_info.num_sdma_queues_per_engine = 8;\n\t}\n\n\tbitmap_zero(kfd->device_info.reserved_sdma_queues_bitmap, KFD_MAX_SDMA_QUEUES);\n\n\tswitch (sdma_version) {\n\tcase IP_VERSION(6, 0, 0):\n\tcase IP_VERSION(6, 0, 1):\n\tcase IP_VERSION(6, 0, 2):\n\tcase IP_VERSION(6, 0, 3):\n\t\t \n\t\tkfd->device_info.num_reserved_sdma_queues_per_engine = 2;\n\t\t \n\t\tbitmap_set(kfd->device_info.reserved_sdma_queues_bitmap, 0,\n\t\t\t   kfd->adev->sdma.num_instances *\n\t\t\t   kfd->device_info.num_reserved_sdma_queues_per_engine);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void kfd_device_info_set_event_interrupt_class(struct kfd_dev *kfd)\n{\n\tuint32_t gc_version = KFD_GC_VERSION(kfd);\n\n\tswitch (gc_version) {\n\tcase IP_VERSION(9, 0, 1):  \n\tcase IP_VERSION(9, 1, 0):  \n\tcase IP_VERSION(9, 2, 1):  \n\tcase IP_VERSION(9, 2, 2):  \n\tcase IP_VERSION(9, 3, 0):  \n\tcase IP_VERSION(9, 4, 0):  \n\tcase IP_VERSION(9, 4, 1):  \n\tcase IP_VERSION(9, 4, 2):  \n\t\tkfd->device_info.event_interrupt_class = &event_interrupt_class_v9;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 3):  \n\t\tkfd->device_info.event_interrupt_class =\n\t\t\t\t\t\t&event_interrupt_class_v9_4_3;\n\t\tbreak;\n\tcase IP_VERSION(10, 3, 1):  \n\tcase IP_VERSION(10, 3, 3):  \n\tcase IP_VERSION(10, 3, 6):  \n\tcase IP_VERSION(10, 3, 7):  \n\tcase IP_VERSION(10, 1, 3):  \n\tcase IP_VERSION(10, 1, 4):\n\tcase IP_VERSION(10, 1, 10):  \n\tcase IP_VERSION(10, 1, 2):  \n\tcase IP_VERSION(10, 1, 1):  \n\tcase IP_VERSION(10, 3, 0):  \n\tcase IP_VERSION(10, 3, 2):  \n\tcase IP_VERSION(10, 3, 4):  \n\tcase IP_VERSION(10, 3, 5):  \n\t\tkfd->device_info.event_interrupt_class = &event_interrupt_class_v10;\n\t\tbreak;\n\tcase IP_VERSION(11, 0, 0):\n\tcase IP_VERSION(11, 0, 1):\n\tcase IP_VERSION(11, 0, 2):\n\tcase IP_VERSION(11, 0, 3):\n\tcase IP_VERSION(11, 0, 4):\n\t\tkfd->device_info.event_interrupt_class = &event_interrupt_class_v11;\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(kfd_device, \"v9 event interrupt handler is set due to \"\n\t\t\t\"mismatch of gc ip block(GC_HWIP:0x%x).\\n\", gc_version);\n\t\tkfd->device_info.event_interrupt_class = &event_interrupt_class_v9;\n\t}\n}\n\nstatic void kfd_device_info_init(struct kfd_dev *kfd,\n\t\t\t\t bool vf, uint32_t gfx_target_version)\n{\n\tuint32_t gc_version = KFD_GC_VERSION(kfd);\n\tuint32_t asic_type = kfd->adev->asic_type;\n\n\tkfd->device_info.max_pasid_bits = 16;\n\tkfd->device_info.max_no_of_hqd = 24;\n\tkfd->device_info.num_of_watch_points = 4;\n\tkfd->device_info.mqd_size_aligned = MQD_SIZE_ALIGNED;\n\tkfd->device_info.gfx_target_version = gfx_target_version;\n\n\tif (KFD_IS_SOC15(kfd)) {\n\t\tkfd->device_info.doorbell_size = 8;\n\t\tkfd->device_info.ih_ring_entry_size = 8 * sizeof(uint32_t);\n\t\tkfd->device_info.supports_cwsr = true;\n\n\t\tkfd_device_info_set_sdma_info(kfd);\n\n\t\tkfd_device_info_set_event_interrupt_class(kfd);\n\n\t\tif (gc_version < IP_VERSION(11, 0, 0)) {\n\t\t\t \n\t\t\tif (gc_version == IP_VERSION(10, 3, 6))\n\t\t\t\tkfd->device_info.no_atomic_fw_version = 14;\n\t\t\telse if (gc_version == IP_VERSION(10, 3, 7))\n\t\t\t\tkfd->device_info.no_atomic_fw_version = 3;\n\t\t\telse if (gc_version >= IP_VERSION(10, 3, 0))\n\t\t\t\tkfd->device_info.no_atomic_fw_version = 92;\n\t\t\telse if (gc_version >= IP_VERSION(10, 1, 1))\n\t\t\t\tkfd->device_info.no_atomic_fw_version = 145;\n\n\t\t\t \n\t\t\tif (gc_version >= IP_VERSION(10, 1, 1))\n\t\t\t\tkfd->device_info.needs_pci_atomics = true;\n\t\t} else if (gc_version < IP_VERSION(12, 0, 0)) {\n\t\t\t \n\t\t\tkfd->device_info.needs_pci_atomics = true;\n\t\t\tkfd->device_info.no_atomic_fw_version = kfd->adev->gfx.rs64_enable ? 509 : 0;\n\t\t}\n\t} else {\n\t\tkfd->device_info.doorbell_size = 4;\n\t\tkfd->device_info.ih_ring_entry_size = 4 * sizeof(uint32_t);\n\t\tkfd->device_info.event_interrupt_class = &event_interrupt_class_cik;\n\t\tkfd->device_info.num_sdma_queues_per_engine = 2;\n\n\t\tif (asic_type != CHIP_KAVERI &&\n\t\t    asic_type != CHIP_HAWAII &&\n\t\t    asic_type != CHIP_TONGA)\n\t\t\tkfd->device_info.supports_cwsr = true;\n\n\t\tif (asic_type != CHIP_HAWAII && !vf)\n\t\t\tkfd->device_info.needs_pci_atomics = true;\n\t}\n}\n\nstruct kfd_dev *kgd2kfd_probe(struct amdgpu_device *adev, bool vf)\n{\n\tstruct kfd_dev *kfd = NULL;\n\tconst struct kfd2kgd_calls *f2g = NULL;\n\tuint32_t gfx_target_version = 0;\n\n\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_KAVERI:\n\t\tgfx_target_version = 70000;\n\t\tif (!vf)\n\t\t\tf2g = &gfx_v7_kfd2kgd;\n\t\tbreak;\n#endif\n\tcase CHIP_CARRIZO:\n\t\tgfx_target_version = 80001;\n\t\tif (!vf)\n\t\t\tf2g = &gfx_v8_kfd2kgd;\n\t\tbreak;\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_HAWAII:\n\t\tgfx_target_version = 70001;\n\t\tif (!amdgpu_exp_hw_support)\n\t\t\tpr_info(\n\t\"KFD support on Hawaii is experimental. See modparam exp_hw_support\\n\"\n\t\t\t\t);\n\t\telse if (!vf)\n\t\t\tf2g = &gfx_v7_kfd2kgd;\n\t\tbreak;\n#endif\n\tcase CHIP_TONGA:\n\t\tgfx_target_version = 80002;\n\t\tif (!vf)\n\t\t\tf2g = &gfx_v8_kfd2kgd;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\tcase CHIP_POLARIS10:\n\t\tgfx_target_version = 80003;\n\t\tf2g = &gfx_v8_kfd2kgd;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tgfx_target_version = 80003;\n\t\tif (!vf)\n\t\t\tf2g = &gfx_v8_kfd2kgd;\n\t\tbreak;\n\tdefault:\n\t\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\t\t \n\t\tcase IP_VERSION(9, 0, 1):\n\t\t\tgfx_target_version = 90000;\n\t\t\tf2g = &gfx_v9_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 1, 0):\n\t\tcase IP_VERSION(9, 2, 2):\n\t\t\tgfx_target_version = 90002;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v9_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 2, 1):\n\t\t\tgfx_target_version = 90004;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v9_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 3, 0):\n\t\t\tgfx_target_version = 90012;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v9_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 4, 0):\n\t\t\tgfx_target_version = 90006;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v9_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 4, 1):\n\t\t\tgfx_target_version = 90008;\n\t\t\tf2g = &arcturus_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(9, 4, 2):\n\t\t\tgfx_target_version = 90010;\n\t\t\tf2g = &aldebaran_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 4, 3):\n\t\t\tgfx_target_version = adev->rev_id >= 1 ? 90402\n\t\t\t\t\t   : adev->flags & AMD_IS_APU ? 90400\n\t\t\t\t\t   : 90401;\n\t\t\tf2g = &gc_9_4_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 1, 10):\n\t\t\tgfx_target_version = 100100;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 1, 2):\n\t\t\tgfx_target_version = 100101;\n\t\t\tf2g = &gfx_v10_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 1, 1):\n\t\t\tgfx_target_version = 100102;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 1, 3):\n\t\tcase IP_VERSION(10, 1, 4):\n\t\t\tgfx_target_version = 100103;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 0):\n\t\t\tgfx_target_version = 100300;\n\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 2):\n\t\t\tgfx_target_version = 100301;\n\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 1):\n\t\t\tgfx_target_version = 100303;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 4):\n\t\t\tgfx_target_version = 100302;\n\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 5):\n\t\t\tgfx_target_version = 100304;\n\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\t \n\t\tcase IP_VERSION(10, 3, 3):\n\t\t\tgfx_target_version = 100305;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(10, 3, 6):\n\t\tcase IP_VERSION(10, 3, 7):\n\t\t\tgfx_target_version = 100306;\n\t\t\tif (!vf)\n\t\t\t\tf2g = &gfx_v10_3_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(11, 0, 0):\n\t\t\tgfx_target_version = 110000;\n\t\t\tf2g = &gfx_v11_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(11, 0, 1):\n\t\tcase IP_VERSION(11, 0, 4):\n\t\t\tgfx_target_version = 110003;\n\t\t\tf2g = &gfx_v11_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(11, 0, 2):\n\t\t\tgfx_target_version = 110002;\n\t\t\tf2g = &gfx_v11_kfd2kgd;\n\t\t\tbreak;\n\t\tcase IP_VERSION(11, 0, 3):\n\t\t\tif ((adev->pdev->device == 0x7460 &&\n\t\t\t     adev->pdev->revision == 0x00) ||\n\t\t\t    (adev->pdev->device == 0x7461 &&\n\t\t\t     adev->pdev->revision == 0x00))\n\t\t\t\t \n\t\t\t\tgfx_target_version = 110005;\n\t\t\telse\n\t\t\t\t \n\t\t\t\tgfx_target_version = 110001;\n\t\t\tf2g = &gfx_v11_kfd2kgd;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (!f2g) {\n\t\tif (adev->ip_versions[GC_HWIP][0])\n\t\t\tdev_err(kfd_device, \"GC IP %06x %s not supported in kfd\\n\",\n\t\t\t\tadev->ip_versions[GC_HWIP][0], vf ? \"VF\" : \"\");\n\t\telse\n\t\t\tdev_err(kfd_device, \"%s %s not supported in kfd\\n\",\n\t\t\t\tamdgpu_asic_name[adev->asic_type], vf ? \"VF\" : \"\");\n\t\treturn NULL;\n\t}\n\n\tkfd = kzalloc(sizeof(*kfd), GFP_KERNEL);\n\tif (!kfd)\n\t\treturn NULL;\n\n\tkfd->adev = adev;\n\tkfd_device_info_init(kfd, vf, gfx_target_version);\n\tkfd->init_complete = false;\n\tkfd->kfd2kgd = f2g;\n\tatomic_set(&kfd->compute_profile, 0);\n\n\tmutex_init(&kfd->doorbell_mutex);\n\n\tida_init(&kfd->doorbell_ida);\n\n\treturn kfd;\n}\n\nstatic void kfd_cwsr_init(struct kfd_dev *kfd)\n{\n\tif (cwsr_enable && kfd->device_info.supports_cwsr) {\n\t\tif (KFD_GC_VERSION(kfd) < IP_VERSION(9, 0, 1)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_gfx8_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_gfx8_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_gfx8_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 1)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_arcturus_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_arcturus_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_arcturus_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 2)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_aldebaran_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_aldebaran_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_aldebaran_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 3)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_gfx9_4_3_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_gfx9_4_3_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_gfx9_4_3_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) < IP_VERSION(10, 1, 1)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_gfx9_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_gfx9_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_gfx9_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) < IP_VERSION(10, 3, 0)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_nv1x_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_nv1x_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_nv1x_hex);\n\t\t} else if (KFD_GC_VERSION(kfd) < IP_VERSION(11, 0, 0)) {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_gfx10_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_gfx10_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_gfx10_hex);\n\t\t} else {\n\t\t\tBUILD_BUG_ON(sizeof(cwsr_trap_gfx11_hex) > PAGE_SIZE);\n\t\t\tkfd->cwsr_isa = cwsr_trap_gfx11_hex;\n\t\t\tkfd->cwsr_isa_size = sizeof(cwsr_trap_gfx11_hex);\n\t\t}\n\n\t\tkfd->cwsr_enabled = true;\n\t}\n}\n\nstatic int kfd_gws_init(struct kfd_node *node)\n{\n\tint ret = 0;\n\tstruct kfd_dev *kfd = node->kfd;\n\tuint32_t mes_rev = node->adev->mes.sched_version & AMDGPU_MES_VERSION_MASK;\n\n\tif (node->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS)\n\t\treturn 0;\n\n\tif (hws_gws_support || (KFD_IS_SOC15(node) &&\n\t\t((KFD_GC_VERSION(node) == IP_VERSION(9, 0, 1)\n\t\t\t&& kfd->mec2_fw_version >= 0x81b3) ||\n\t\t(KFD_GC_VERSION(node) <= IP_VERSION(9, 4, 0)\n\t\t\t&& kfd->mec2_fw_version >= 0x1b3)  ||\n\t\t(KFD_GC_VERSION(node) == IP_VERSION(9, 4, 1)\n\t\t\t&& kfd->mec2_fw_version >= 0x30)   ||\n\t\t(KFD_GC_VERSION(node) == IP_VERSION(9, 4, 2)\n\t\t\t&& kfd->mec2_fw_version >= 0x28) ||\n\t\t(KFD_GC_VERSION(node) == IP_VERSION(9, 4, 3)) ||\n\t\t(KFD_GC_VERSION(node) >= IP_VERSION(10, 3, 0)\n\t\t\t&& KFD_GC_VERSION(node) < IP_VERSION(11, 0, 0)\n\t\t\t&& kfd->mec2_fw_version >= 0x6b) ||\n\t\t(KFD_GC_VERSION(node) >= IP_VERSION(11, 0, 0)\n\t\t\t&& KFD_GC_VERSION(node) < IP_VERSION(12, 0, 0)\n\t\t\t&& mes_rev >= 68))))\n\t\tret = amdgpu_amdkfd_alloc_gws(node->adev,\n\t\t\t\tnode->adev->gds.gws_size, &node->gws);\n\n\treturn ret;\n}\n\nstatic void kfd_smi_init(struct kfd_node *dev)\n{\n\tINIT_LIST_HEAD(&dev->smi_clients);\n\tspin_lock_init(&dev->smi_lock);\n}\n\nstatic int kfd_init_node(struct kfd_node *node)\n{\n\tint err = -1;\n\n\tif (kfd_interrupt_init(node)) {\n\t\tdev_err(kfd_device, \"Error initializing interrupts\\n\");\n\t\tgoto kfd_interrupt_error;\n\t}\n\n\tnode->dqm = device_queue_manager_init(node);\n\tif (!node->dqm) {\n\t\tdev_err(kfd_device, \"Error initializing queue manager\\n\");\n\t\tgoto device_queue_manager_error;\n\t}\n\n\tif (kfd_gws_init(node)) {\n\t\tdev_err(kfd_device, \"Could not allocate %d gws\\n\",\n\t\t\tnode->adev->gds.gws_size);\n\t\tgoto gws_error;\n\t}\n\n\tif (kfd_resume(node))\n\t\tgoto kfd_resume_error;\n\n\tif (kfd_topology_add_device(node)) {\n\t\tdev_err(kfd_device, \"Error adding device to topology\\n\");\n\t\tgoto kfd_topology_add_device_error;\n\t}\n\n\tkfd_smi_init(node);\n\n\treturn 0;\n\nkfd_topology_add_device_error:\nkfd_resume_error:\ngws_error:\n\tdevice_queue_manager_uninit(node->dqm);\ndevice_queue_manager_error:\n\tkfd_interrupt_exit(node);\nkfd_interrupt_error:\n\tif (node->gws)\n\t\tamdgpu_amdkfd_free_gws(node->adev, node->gws);\n\n\t \n\tkfree(node);\n\treturn err;\n}\n\nstatic void kfd_cleanup_nodes(struct kfd_dev *kfd, unsigned int num_nodes)\n{\n\tstruct kfd_node *knode;\n\tunsigned int i;\n\n\tfor (i = 0; i < num_nodes; i++) {\n\t\tknode = kfd->nodes[i];\n\t\tdevice_queue_manager_uninit(knode->dqm);\n\t\tkfd_interrupt_exit(knode);\n\t\tkfd_topology_remove_device(knode);\n\t\tif (knode->gws)\n\t\t\tamdgpu_amdkfd_free_gws(knode->adev, knode->gws);\n\t\tkfree(knode);\n\t\tkfd->nodes[i] = NULL;\n\t}\n}\n\nstatic void kfd_setup_interrupt_bitmap(struct kfd_node *node,\n\t\t\t\t       unsigned int kfd_node_idx)\n{\n\tstruct amdgpu_device *adev = node->adev;\n\tuint32_t xcc_mask = node->xcc_mask;\n\tuint32_t xcc, mapped_xcc;\n\t \n\tfor_each_inst(xcc, xcc_mask) {\n\t\tmapped_xcc = GET_INST(GC, xcc);\n\t\tnode->interrupt_bitmap |= (mapped_xcc % 2 ? 5 : 3) << (4 * (mapped_xcc / 2));\n\t}\n\tdev_info(kfd_device, \"Node: %d, interrupt_bitmap: %x\\n\", kfd_node_idx,\n\t\t\t\t\t\t\tnode->interrupt_bitmap);\n}\n\nbool kgd2kfd_device_init(struct kfd_dev *kfd,\n\t\t\t const struct kgd2kfd_shared_resources *gpu_resources)\n{\n\tunsigned int size, map_process_packet_size, i;\n\tstruct kfd_node *node;\n\tuint32_t first_vmid_kfd, last_vmid_kfd, vmid_num_kfd;\n\tunsigned int max_proc_per_quantum;\n\tint partition_mode;\n\tint xcp_idx;\n\n\tkfd->mec_fw_version = amdgpu_amdkfd_get_fw_version(kfd->adev,\n\t\t\tKGD_ENGINE_MEC1);\n\tkfd->mec2_fw_version = amdgpu_amdkfd_get_fw_version(kfd->adev,\n\t\t\tKGD_ENGINE_MEC2);\n\tkfd->sdma_fw_version = amdgpu_amdkfd_get_fw_version(kfd->adev,\n\t\t\tKGD_ENGINE_SDMA1);\n\tkfd->shared_resources = *gpu_resources;\n\n\tkfd->num_nodes = amdgpu_xcp_get_num_xcp(kfd->adev->xcp_mgr);\n\n\tif (kfd->num_nodes == 0) {\n\t\tdev_err(kfd_device,\n\t\t\t\"KFD num nodes cannot be 0, num_xcc_in_node: %d\\n\",\n\t\t\tkfd->adev->gfx.num_xcc_per_xcp);\n\t\tgoto out;\n\t}\n\n\t \n\tkfd->pci_atomic_requested = amdgpu_amdkfd_have_atomics_support(kfd->adev);\n\tif (!kfd->pci_atomic_requested &&\n\t    kfd->device_info.needs_pci_atomics &&\n\t    (!kfd->device_info.no_atomic_fw_version ||\n\t     kfd->mec_fw_version < kfd->device_info.no_atomic_fw_version)) {\n\t\tdev_info(kfd_device,\n\t\t\t \"skipped device %x:%x, PCI rejects atomics %d<%d\\n\",\n\t\t\t kfd->adev->pdev->vendor, kfd->adev->pdev->device,\n\t\t\t kfd->mec_fw_version,\n\t\t\t kfd->device_info.no_atomic_fw_version);\n\t\treturn false;\n\t}\n\n\tfirst_vmid_kfd = ffs(gpu_resources->compute_vmid_bitmap)-1;\n\tlast_vmid_kfd = fls(gpu_resources->compute_vmid_bitmap)-1;\n\tvmid_num_kfd = last_vmid_kfd - first_vmid_kfd + 1;\n\n\t \n\tif (kfd->adev->xcp_mgr) {\n\t\tpartition_mode = amdgpu_xcp_query_partition_mode(kfd->adev->xcp_mgr,\n\t\t\t\t\t\t\t\t AMDGPU_XCP_FL_LOCKED);\n\t\tif (partition_mode == AMDGPU_CPX_PARTITION_MODE &&\n\t\t    kfd->num_nodes != 1) {\n\t\t\tvmid_num_kfd /= 2;\n\t\t\tfirst_vmid_kfd = last_vmid_kfd + 1 - vmid_num_kfd*2;\n\t\t}\n\t}\n\n\t \n\tif (hws_max_conc_proc >= 0)\n\t\tmax_proc_per_quantum = min((u32)hws_max_conc_proc, vmid_num_kfd);\n\telse\n\t\tmax_proc_per_quantum = vmid_num_kfd;\n\n\t \n\tsize = max_num_of_queues_per_device *\n\t\t\tkfd->device_info.mqd_size_aligned;\n\n\t \n\tmap_process_packet_size = KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 2) ?\n\t\t\t\tsizeof(struct pm4_mes_map_process_aldebaran) :\n\t\t\t\tsizeof(struct pm4_mes_map_process);\n\tsize += (KFD_MAX_NUM_OF_PROCESSES * map_process_packet_size +\n\t\tmax_num_of_queues_per_device * sizeof(struct pm4_mes_map_queues)\n\t\t+ sizeof(struct pm4_mes_runlist)) * 2;\n\n\t \n\tsize += KFD_KERNEL_QUEUE_SIZE * 2;\n\n\t \n\tsize += 512 * 1024;\n\n\tif (amdgpu_amdkfd_alloc_gtt_mem(\n\t\t\tkfd->adev, size, &kfd->gtt_mem,\n\t\t\t&kfd->gtt_start_gpu_addr, &kfd->gtt_start_cpu_ptr,\n\t\t\tfalse)) {\n\t\tdev_err(kfd_device, \"Could not allocate %d bytes\\n\", size);\n\t\tgoto alloc_gtt_mem_failure;\n\t}\n\n\tdev_info(kfd_device, \"Allocated %d bytes on gart\\n\", size);\n\n\t \n\tif (kfd_gtt_sa_init(kfd, size, 512) != 0) {\n\t\tdev_err(kfd_device, \"Error initializing gtt sub-allocator\\n\");\n\t\tgoto kfd_gtt_sa_init_error;\n\t}\n\n\tif (kfd_doorbell_init(kfd)) {\n\t\tdev_err(kfd_device,\n\t\t\t\"Error initializing doorbell aperture\\n\");\n\t\tgoto kfd_doorbell_error;\n\t}\n\n\tif (amdgpu_use_xgmi_p2p)\n\t\tkfd->hive_id = kfd->adev->gmc.xgmi.hive_id;\n\n\t \n\tif (!kfd->hive_id && (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 3)) && kfd->num_nodes > 1)\n\t\tkfd->hive_id = pci_dev_id(kfd->adev->pdev);\n\n\tkfd->noretry = kfd->adev->gmc.noretry;\n\n\tkfd_cwsr_init(kfd);\n\n\tdev_info(kfd_device, \"Total number of KFD nodes to be created: %d\\n\",\n\t\t\t\tkfd->num_nodes);\n\n\t \n\tfor (i = 0, xcp_idx = 0; i < kfd->num_nodes; i++) {\n\t\tnode = kzalloc(sizeof(struct kfd_node), GFP_KERNEL);\n\t\tif (!node)\n\t\t\tgoto node_alloc_error;\n\n\t\tnode->node_id = i;\n\t\tnode->adev = kfd->adev;\n\t\tnode->kfd = kfd;\n\t\tnode->kfd2kgd = kfd->kfd2kgd;\n\t\tnode->vm_info.vmid_num_kfd = vmid_num_kfd;\n\t\tnode->xcp = amdgpu_get_next_xcp(kfd->adev->xcp_mgr, &xcp_idx);\n\t\t \n\t\tif (node->xcp) {\n\t\t\tamdgpu_xcp_get_inst_details(node->xcp, AMDGPU_XCP_GFX,\n\t\t\t\t\t\t    &node->xcc_mask);\n\t\t\t++xcp_idx;\n\t\t} else {\n\t\t\tnode->xcc_mask =\n\t\t\t\t(1U << NUM_XCC(kfd->adev->gfx.xcc_mask)) - 1;\n\t\t}\n\n\t\tif (node->xcp) {\n\t\t\tdev_info(kfd_device, \"KFD node %d partition %d size %lldM\\n\",\n\t\t\t\tnode->node_id, node->xcp->mem_id,\n\t\t\t\tKFD_XCP_MEMORY_SIZE(node->adev, node->node_id) >> 20);\n\t\t}\n\n\t\tif (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 3) &&\n\t\t    partition_mode == AMDGPU_CPX_PARTITION_MODE &&\n\t\t    kfd->num_nodes != 1) {\n\t\t\t \n\n\t\t\tnode->vm_info.first_vmid_kfd = (i%2 == 0) ?\n\t\t\t\t\t\tfirst_vmid_kfd :\n\t\t\t\t\t\tfirst_vmid_kfd+vmid_num_kfd;\n\t\t\tnode->vm_info.last_vmid_kfd = (i%2 == 0) ?\n\t\t\t\t\t\tlast_vmid_kfd-vmid_num_kfd :\n\t\t\t\t\t\tlast_vmid_kfd;\n\t\t\tnode->compute_vmid_bitmap =\n\t\t\t\t((0x1 << (node->vm_info.last_vmid_kfd + 1)) - 1) -\n\t\t\t\t((0x1 << (node->vm_info.first_vmid_kfd)) - 1);\n\t\t} else {\n\t\t\tnode->vm_info.first_vmid_kfd = first_vmid_kfd;\n\t\t\tnode->vm_info.last_vmid_kfd = last_vmid_kfd;\n\t\t\tnode->compute_vmid_bitmap =\n\t\t\t\tgpu_resources->compute_vmid_bitmap;\n\t\t}\n\t\tnode->max_proc_per_quantum = max_proc_per_quantum;\n\t\tatomic_set(&node->sram_ecc_flag, 0);\n\n\t\tamdgpu_amdkfd_get_local_mem_info(kfd->adev,\n\t\t\t\t\t&node->local_mem_info, node->xcp);\n\n\t\tif (KFD_GC_VERSION(kfd) == IP_VERSION(9, 4, 3))\n\t\t\tkfd_setup_interrupt_bitmap(node, i);\n\n\t\t \n\t\tif (kfd_init_node(node)) {\n\t\t\tdev_err(kfd_device, \"Error initializing KFD node\\n\");\n\t\t\tgoto node_init_error;\n\t\t}\n\t\tkfd->nodes[i] = node;\n\t}\n\n\tsvm_range_set_max_pages(kfd->adev);\n\n\tspin_lock_init(&kfd->watch_points_lock);\n\n\tkfd->init_complete = true;\n\tdev_info(kfd_device, \"added device %x:%x\\n\", kfd->adev->pdev->vendor,\n\t\t kfd->adev->pdev->device);\n\n\tpr_debug(\"Starting kfd with the following scheduling policy %d\\n\",\n\t\tnode->dqm->sched_policy);\n\n\tgoto out;\n\nnode_init_error:\nnode_alloc_error:\n\tkfd_cleanup_nodes(kfd, i);\n\tkfd_doorbell_fini(kfd);\nkfd_doorbell_error:\n\tkfd_gtt_sa_fini(kfd);\nkfd_gtt_sa_init_error:\n\tamdgpu_amdkfd_free_gtt_mem(kfd->adev, kfd->gtt_mem);\nalloc_gtt_mem_failure:\n\tdev_err(kfd_device,\n\t\t\"device %x:%x NOT added due to errors\\n\",\n\t\tkfd->adev->pdev->vendor, kfd->adev->pdev->device);\nout:\n\treturn kfd->init_complete;\n}\n\nvoid kgd2kfd_device_exit(struct kfd_dev *kfd)\n{\n\tif (kfd->init_complete) {\n\t\t \n\t\tkfd_cleanup_nodes(kfd, kfd->num_nodes);\n\t\t \n\t\tkfd_doorbell_fini(kfd);\n\t\tida_destroy(&kfd->doorbell_ida);\n\t\tkfd_gtt_sa_fini(kfd);\n\t\tamdgpu_amdkfd_free_gtt_mem(kfd->adev, kfd->gtt_mem);\n\t}\n\n\tkfree(kfd);\n}\n\nint kgd2kfd_pre_reset(struct kfd_dev *kfd)\n{\n\tstruct kfd_node *node;\n\tint i;\n\n\tif (!kfd->init_complete)\n\t\treturn 0;\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tnode = kfd->nodes[i];\n\t\tkfd_smi_event_update_gpu_reset(node, false);\n\t\tnode->dqm->ops.pre_reset(node->dqm);\n\t}\n\n\tkgd2kfd_suspend(kfd, false);\n\n\tfor (i = 0; i < kfd->num_nodes; i++)\n\t\tkfd_signal_reset_event(kfd->nodes[i]);\n\n\treturn 0;\n}\n\n \n\nint kgd2kfd_post_reset(struct kfd_dev *kfd)\n{\n\tint ret;\n\tstruct kfd_node *node;\n\tint i;\n\n\tif (!kfd->init_complete)\n\t\treturn 0;\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tret = kfd_resume(kfd->nodes[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmutex_lock(&kfd_processes_mutex);\n\t--kfd_locked;\n\tmutex_unlock(&kfd_processes_mutex);\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tnode = kfd->nodes[i];\n\t\tatomic_set(&node->sram_ecc_flag, 0);\n\t\tkfd_smi_event_update_gpu_reset(node, true);\n\t}\n\n\treturn 0;\n}\n\nbool kfd_is_locked(void)\n{\n\tlockdep_assert_held(&kfd_processes_mutex);\n\treturn  (kfd_locked > 0);\n}\n\nvoid kgd2kfd_suspend(struct kfd_dev *kfd, bool run_pm)\n{\n\tstruct kfd_node *node;\n\tint i;\n\tint count;\n\n\tif (!kfd->init_complete)\n\t\treturn;\n\n\t \n\tif (!run_pm) {\n\t\tmutex_lock(&kfd_processes_mutex);\n\t\tcount = ++kfd_locked;\n\t\tmutex_unlock(&kfd_processes_mutex);\n\n\t\t \n\t\tif (count == 1)\n\t\t\tkfd_suspend_all_processes();\n\t}\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tnode = kfd->nodes[i];\n\t\tnode->dqm->ops.stop(node->dqm);\n\t}\n}\n\nint kgd2kfd_resume(struct kfd_dev *kfd, bool run_pm)\n{\n\tint ret, count, i;\n\n\tif (!kfd->init_complete)\n\t\treturn 0;\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tret = kfd_resume(kfd->nodes[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\t \n\tif (!run_pm) {\n\t\tmutex_lock(&kfd_processes_mutex);\n\t\tcount = --kfd_locked;\n\t\tmutex_unlock(&kfd_processes_mutex);\n\n\t\tWARN_ONCE(count < 0, \"KFD suspend / resume ref. error\");\n\t\tif (count == 0)\n\t\t\tret = kfd_resume_all_processes();\n\t}\n\n\treturn ret;\n}\n\nstatic int kfd_resume(struct kfd_node *node)\n{\n\tint err = 0;\n\n\terr = node->dqm->ops.start(node->dqm);\n\tif (err)\n\t\tdev_err(kfd_device,\n\t\t\t\"Error starting queue manager for device %x:%x\\n\",\n\t\t\tnode->adev->pdev->vendor, node->adev->pdev->device);\n\n\treturn err;\n}\n\nstatic inline void kfd_queue_work(struct workqueue_struct *wq,\n\t\t\t\t  struct work_struct *work)\n{\n\tint cpu, new_cpu;\n\n\tcpu = new_cpu = smp_processor_id();\n\tdo {\n\t\tnew_cpu = cpumask_next(new_cpu, cpu_online_mask) % nr_cpu_ids;\n\t\tif (cpu_to_node(new_cpu) == numa_node_id())\n\t\t\tbreak;\n\t} while (cpu != new_cpu);\n\n\tqueue_work_on(new_cpu, wq, work);\n}\n\n \nvoid kgd2kfd_interrupt(struct kfd_dev *kfd, const void *ih_ring_entry)\n{\n\tuint32_t patched_ihre[KFD_MAX_RING_ENTRY_SIZE], i;\n\tbool is_patched = false;\n\tunsigned long flags;\n\tstruct kfd_node *node;\n\n\tif (!kfd->init_complete)\n\t\treturn;\n\n\tif (kfd->device_info.ih_ring_entry_size > sizeof(patched_ihre)) {\n\t\tdev_err_once(kfd_device, \"Ring entry too small\\n\");\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < kfd->num_nodes; i++) {\n\t\tnode = kfd->nodes[i];\n\t\tspin_lock_irqsave(&node->interrupt_lock, flags);\n\n\t\tif (node->interrupts_active\n\t\t    && interrupt_is_wanted(node, ih_ring_entry,\n\t\t\t    \tpatched_ihre, &is_patched)\n\t\t    && enqueue_ih_ring_entry(node,\n\t\t\t    \tis_patched ? patched_ihre : ih_ring_entry)) {\n\t\t\tkfd_queue_work(node->ih_wq, &node->interrupt_work);\n\t\t\tspin_unlock_irqrestore(&node->interrupt_lock, flags);\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock_irqrestore(&node->interrupt_lock, flags);\n\t}\n\n}\n\nint kgd2kfd_quiesce_mm(struct mm_struct *mm, uint32_t trigger)\n{\n\tstruct kfd_process *p;\n\tint r;\n\n\t \n\tp = kfd_lookup_process_by_mm(mm);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tWARN(debug_evictions, \"Evicting pid %d\", p->lead_thread->pid);\n\tr = kfd_process_evict_queues(p, trigger);\n\n\tkfd_unref_process(p);\n\treturn r;\n}\n\nint kgd2kfd_resume_mm(struct mm_struct *mm)\n{\n\tstruct kfd_process *p;\n\tint r;\n\n\t \n\tp = kfd_lookup_process_by_mm(mm);\n\tif (!p)\n\t\treturn -ESRCH;\n\n\tr = kfd_process_restore_queues(p);\n\n\tkfd_unref_process(p);\n\treturn r;\n}\n\n \nint kgd2kfd_schedule_evict_and_restore_process(struct mm_struct *mm,\n\t\t\t\t\t       struct dma_fence *fence)\n{\n\tstruct kfd_process *p;\n\tunsigned long active_time;\n\tunsigned long delay_jiffies = msecs_to_jiffies(PROCESS_ACTIVE_TIME_MS);\n\n\tif (!fence)\n\t\treturn -EINVAL;\n\n\tif (dma_fence_is_signaled(fence))\n\t\treturn 0;\n\n\tp = kfd_lookup_process_by_mm(mm);\n\tif (!p)\n\t\treturn -ENODEV;\n\n\tif (fence->seqno == p->last_eviction_seqno)\n\t\tgoto out;\n\n\tp->last_eviction_seqno = fence->seqno;\n\n\t \n\tactive_time = get_jiffies_64() - p->last_restore_timestamp;\n\tif (delay_jiffies > active_time)\n\t\tdelay_jiffies -= active_time;\n\telse\n\t\tdelay_jiffies = 0;\n\n\t \n\tWARN(debug_evictions, \"Scheduling eviction of pid %d in %ld jiffies\",\n\t     p->lead_thread->pid, delay_jiffies);\n\tschedule_delayed_work(&p->eviction_work, delay_jiffies);\nout:\n\tkfd_unref_process(p);\n\treturn 0;\n}\n\nstatic int kfd_gtt_sa_init(struct kfd_dev *kfd, unsigned int buf_size,\n\t\t\t\tunsigned int chunk_size)\n{\n\tif (WARN_ON(buf_size < chunk_size))\n\t\treturn -EINVAL;\n\tif (WARN_ON(buf_size == 0))\n\t\treturn -EINVAL;\n\tif (WARN_ON(chunk_size == 0))\n\t\treturn -EINVAL;\n\n\tkfd->gtt_sa_chunk_size = chunk_size;\n\tkfd->gtt_sa_num_of_chunks = buf_size / chunk_size;\n\n\tkfd->gtt_sa_bitmap = bitmap_zalloc(kfd->gtt_sa_num_of_chunks,\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!kfd->gtt_sa_bitmap)\n\t\treturn -ENOMEM;\n\n\tpr_debug(\"gtt_sa_num_of_chunks = %d, gtt_sa_bitmap = %p\\n\",\n\t\t\tkfd->gtt_sa_num_of_chunks, kfd->gtt_sa_bitmap);\n\n\tmutex_init(&kfd->gtt_sa_lock);\n\n\treturn 0;\n}\n\nstatic void kfd_gtt_sa_fini(struct kfd_dev *kfd)\n{\n\tmutex_destroy(&kfd->gtt_sa_lock);\n\tbitmap_free(kfd->gtt_sa_bitmap);\n}\n\nstatic inline uint64_t kfd_gtt_sa_calc_gpu_addr(uint64_t start_addr,\n\t\t\t\t\t\tunsigned int bit_num,\n\t\t\t\t\t\tunsigned int chunk_size)\n{\n\treturn start_addr + bit_num * chunk_size;\n}\n\nstatic inline uint32_t *kfd_gtt_sa_calc_cpu_addr(void *start_addr,\n\t\t\t\t\t\tunsigned int bit_num,\n\t\t\t\t\t\tunsigned int chunk_size)\n{\n\treturn (uint32_t *) ((uint64_t) start_addr + bit_num * chunk_size);\n}\n\nint kfd_gtt_sa_allocate(struct kfd_node *node, unsigned int size,\n\t\t\tstruct kfd_mem_obj **mem_obj)\n{\n\tunsigned int found, start_search, cur_size;\n\tstruct kfd_dev *kfd = node->kfd;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tif (size > kfd->gtt_sa_num_of_chunks * kfd->gtt_sa_chunk_size)\n\t\treturn -ENOMEM;\n\n\t*mem_obj = kzalloc(sizeof(struct kfd_mem_obj), GFP_KERNEL);\n\tif (!(*mem_obj))\n\t\treturn -ENOMEM;\n\n\tpr_debug(\"Allocated mem_obj = %p for size = %d\\n\", *mem_obj, size);\n\n\tstart_search = 0;\n\n\tmutex_lock(&kfd->gtt_sa_lock);\n\nkfd_gtt_restart_search:\n\t \n\tfound = find_next_zero_bit(kfd->gtt_sa_bitmap,\n\t\t\t\t\tkfd->gtt_sa_num_of_chunks,\n\t\t\t\t\tstart_search);\n\n\tpr_debug(\"Found = %d\\n\", found);\n\n\t \n\tif (found == kfd->gtt_sa_num_of_chunks)\n\t\tgoto kfd_gtt_no_free_chunk;\n\n\t \n\t(*mem_obj)->range_start = found;\n\t(*mem_obj)->range_end = found;\n\t(*mem_obj)->gpu_addr = kfd_gtt_sa_calc_gpu_addr(\n\t\t\t\t\tkfd->gtt_start_gpu_addr,\n\t\t\t\t\tfound,\n\t\t\t\t\tkfd->gtt_sa_chunk_size);\n\t(*mem_obj)->cpu_ptr = kfd_gtt_sa_calc_cpu_addr(\n\t\t\t\t\tkfd->gtt_start_cpu_ptr,\n\t\t\t\t\tfound,\n\t\t\t\t\tkfd->gtt_sa_chunk_size);\n\n\tpr_debug(\"gpu_addr = %p, cpu_addr = %p\\n\",\n\t\t\t(uint64_t *) (*mem_obj)->gpu_addr, (*mem_obj)->cpu_ptr);\n\n\t \n\tif (size <= kfd->gtt_sa_chunk_size) {\n\t\tpr_debug(\"Single bit\\n\");\n\t\t__set_bit(found, kfd->gtt_sa_bitmap);\n\t\tgoto kfd_gtt_out;\n\t}\n\n\t \n\tcur_size = size - kfd->gtt_sa_chunk_size;\n\tdo {\n\t\t(*mem_obj)->range_end =\n\t\t\tfind_next_zero_bit(kfd->gtt_sa_bitmap,\n\t\t\t\t\tkfd->gtt_sa_num_of_chunks, ++found);\n\t\t \n\t\tif ((*mem_obj)->range_end != found) {\n\t\t\tstart_search = found;\n\t\t\tgoto kfd_gtt_restart_search;\n\t\t}\n\n\t\t \n\t\tif (found == kfd->gtt_sa_num_of_chunks)\n\t\t\tgoto kfd_gtt_no_free_chunk;\n\n\t\t \n\t\tif (cur_size <= kfd->gtt_sa_chunk_size)\n\t\t\tcur_size = 0;\n\t\telse\n\t\t\tcur_size -= kfd->gtt_sa_chunk_size;\n\n\t} while (cur_size > 0);\n\n\tpr_debug(\"range_start = %d, range_end = %d\\n\",\n\t\t(*mem_obj)->range_start, (*mem_obj)->range_end);\n\n\t \n\tbitmap_set(kfd->gtt_sa_bitmap, (*mem_obj)->range_start,\n\t\t   (*mem_obj)->range_end - (*mem_obj)->range_start + 1);\n\nkfd_gtt_out:\n\tmutex_unlock(&kfd->gtt_sa_lock);\n\treturn 0;\n\nkfd_gtt_no_free_chunk:\n\tpr_debug(\"Allocation failed with mem_obj = %p\\n\", *mem_obj);\n\tmutex_unlock(&kfd->gtt_sa_lock);\n\tkfree(*mem_obj);\n\treturn -ENOMEM;\n}\n\nint kfd_gtt_sa_free(struct kfd_node *node, struct kfd_mem_obj *mem_obj)\n{\n\tstruct kfd_dev *kfd = node->kfd;\n\n\t \n\tif (!mem_obj)\n\t\treturn 0;\n\n\tpr_debug(\"Free mem_obj = %p, range_start = %d, range_end = %d\\n\",\n\t\t\tmem_obj, mem_obj->range_start, mem_obj->range_end);\n\n\tmutex_lock(&kfd->gtt_sa_lock);\n\n\t \n\tbitmap_clear(kfd->gtt_sa_bitmap, mem_obj->range_start,\n\t\t     mem_obj->range_end - mem_obj->range_start + 1);\n\n\tmutex_unlock(&kfd->gtt_sa_lock);\n\n\tkfree(mem_obj);\n\treturn 0;\n}\n\nvoid kgd2kfd_set_sram_ecc_flag(struct kfd_dev *kfd)\n{\n\t \n\tif (kfd)\n\t\tatomic_inc(&kfd->nodes[0]->sram_ecc_flag);\n}\n\nvoid kfd_inc_compute_active(struct kfd_node *node)\n{\n\tif (atomic_inc_return(&node->kfd->compute_profile) == 1)\n\t\tamdgpu_amdkfd_set_compute_idle(node->adev, false);\n}\n\nvoid kfd_dec_compute_active(struct kfd_node *node)\n{\n\tint count = atomic_dec_return(&node->kfd->compute_profile);\n\n\tif (count == 0)\n\t\tamdgpu_amdkfd_set_compute_idle(node->adev, true);\n\tWARN_ONCE(count < 0, \"Compute profile ref. count error\");\n}\n\nvoid kgd2kfd_smi_event_throttle(struct kfd_dev *kfd, uint64_t throttle_bitmask)\n{\n\t \n\tif (kfd && kfd->init_complete)\n\t\tkfd_smi_event_update_thermal_throttling(kfd->nodes[0],\n\t\t\t\t\t\t\tthrottle_bitmask);\n}\n\n \nunsigned int kfd_get_num_sdma_engines(struct kfd_node *node)\n{\n\t \n\tif (!node->adev->gmc.xgmi.supported)\n\t\treturn node->adev->sdma.num_instances/(int)node->kfd->num_nodes;\n\n\treturn min(node->adev->sdma.num_instances/(int)node->kfd->num_nodes, 2);\n}\n\nunsigned int kfd_get_num_xgmi_sdma_engines(struct kfd_node *node)\n{\n\t \n\treturn node->adev->sdma.num_instances/(int)node->kfd->num_nodes -\n\t\tkfd_get_num_sdma_engines(node);\n}\n\nint kgd2kfd_check_and_lock_kfd(void)\n{\n\tmutex_lock(&kfd_processes_mutex);\n\tif (!hash_empty(kfd_processes_table) || kfd_is_locked()) {\n\t\tmutex_unlock(&kfd_processes_mutex);\n\t\treturn -EBUSY;\n\t}\n\n\t++kfd_locked;\n\tmutex_unlock(&kfd_processes_mutex);\n\n\treturn 0;\n}\n\nvoid kgd2kfd_unlock_kfd(void)\n{\n\tmutex_lock(&kfd_processes_mutex);\n\t--kfd_locked;\n\tmutex_unlock(&kfd_processes_mutex);\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\n \nint kfd_debugfs_hang_hws(struct kfd_node *dev)\n{\n\tif (dev->dqm->sched_policy != KFD_SCHED_POLICY_HWS) {\n\t\tpr_err(\"HWS is not enabled\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn dqm_debugfs_hang_hws(dev->dqm);\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}