{
  "module_name": "kfd_debug.c",
  "hash_id": "2e0db2a8d26d07b9b8b625f1bfe04d44f2139843c89e898efaf457cd44e00f36",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_debug.c",
  "human_readable_source": " \n\n#include \"kfd_debug.h\"\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_topology.h\"\n#include <linux/file.h>\n#include <uapi/linux/kfd_ioctl.h>\n\n#define MAX_WATCH_ADDRESSES\t4\n\nint kfd_dbg_ev_query_debug_event(struct kfd_process *process,\n\t\t      unsigned int *queue_id,\n\t\t      unsigned int *gpu_id,\n\t\t      uint64_t exception_clear_mask,\n\t\t      uint64_t *event_status)\n{\n\tstruct process_queue_manager *pqm;\n\tstruct process_queue_node *pqn;\n\tint i;\n\n\tif (!(process && process->debug_trap_enabled))\n\t\treturn -ENODATA;\n\n\tmutex_lock(&process->event_mutex);\n\t*event_status = 0;\n\t*queue_id = 0;\n\t*gpu_id = 0;\n\n\t \n\tpqm = &process->pqm;\n\tlist_for_each_entry(pqn, &pqm->queues, process_queue_list) {\n\t\tuint64_t tmp = process->exception_enable_mask;\n\n\t\tif (!pqn->q)\n\t\t\tcontinue;\n\n\t\ttmp &= pqn->q->properties.exception_status;\n\n\t\tif (!tmp)\n\t\t\tcontinue;\n\n\t\t*event_status = pqn->q->properties.exception_status;\n\t\t*queue_id = pqn->q->properties.queue_id;\n\t\t*gpu_id = pqn->q->device->id;\n\t\tpqn->q->properties.exception_status &= ~exception_clear_mask;\n\t\tgoto out;\n\t}\n\n\t \n\tfor (i = 0; i < process->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = process->pdds[i];\n\t\tuint64_t tmp = process->exception_enable_mask\n\t\t\t\t\t\t& pdd->exception_status;\n\n\t\tif (!tmp)\n\t\t\tcontinue;\n\n\t\t*event_status = pdd->exception_status;\n\t\t*gpu_id = pdd->dev->id;\n\t\tpdd->exception_status &= ~exception_clear_mask;\n\t\tgoto out;\n\t}\n\n\t \n\tif (process->exception_enable_mask & process->exception_status) {\n\t\t*event_status = process->exception_status;\n\t\tprocess->exception_status &= ~exception_clear_mask;\n\t}\n\nout:\n\tmutex_unlock(&process->event_mutex);\n\treturn *event_status ? 0 : -EAGAIN;\n}\n\nvoid debug_event_write_work_handler(struct work_struct *work)\n{\n\tstruct kfd_process *process;\n\n\tstatic const char write_data = '.';\n\tloff_t pos = 0;\n\n\tprocess = container_of(work,\n\t\t\tstruct kfd_process,\n\t\t\tdebug_event_workarea);\n\n\tkernel_write(process->dbg_ev_file, &write_data, 1, &pos);\n}\n\n \nbool kfd_dbg_ev_raise(uint64_t event_mask,\n\t\t\tstruct kfd_process *process, struct kfd_node *dev,\n\t\t\tunsigned int source_id, bool use_worker,\n\t\t\tvoid *exception_data, size_t exception_data_size)\n{\n\tstruct process_queue_manager *pqm;\n\tstruct process_queue_node *pqn;\n\tint i;\n\tstatic const char write_data = '.';\n\tloff_t pos = 0;\n\tbool is_subscribed = true;\n\n\tif (!(process && process->debug_trap_enabled))\n\t\treturn false;\n\n\tmutex_lock(&process->event_mutex);\n\n\tif (event_mask & KFD_EC_MASK_DEVICE) {\n\t\tfor (i = 0; i < process->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *pdd = process->pdds[i];\n\n\t\t\tif (pdd->dev != dev)\n\t\t\t\tcontinue;\n\n\t\t\tpdd->exception_status |= event_mask & KFD_EC_MASK_DEVICE;\n\n\t\t\tif (event_mask & KFD_EC_MASK(EC_DEVICE_MEMORY_VIOLATION)) {\n\t\t\t\tif (!pdd->vm_fault_exc_data) {\n\t\t\t\t\tpdd->vm_fault_exc_data = kmemdup(\n\t\t\t\t\t\t\texception_data,\n\t\t\t\t\t\t\texception_data_size,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\t\tif (!pdd->vm_fault_exc_data)\n\t\t\t\t\t\tpr_debug(\"Failed to allocate exception data memory\");\n\t\t\t\t} else {\n\t\t\t\t\tpr_debug(\"Debugger exception data not saved\\n\");\n\t\t\t\t\tprint_hex_dump_bytes(\"exception data: \",\n\t\t\t\t\t\t\tDUMP_PREFIX_OFFSET,\n\t\t\t\t\t\t\texception_data,\n\t\t\t\t\t\t\texception_data_size);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} else if (event_mask & KFD_EC_MASK_PROCESS) {\n\t\tprocess->exception_status |= event_mask & KFD_EC_MASK_PROCESS;\n\t} else {\n\t\tpqm = &process->pqm;\n\t\tlist_for_each_entry(pqn, &pqm->queues,\n\t\t\t\tprocess_queue_list) {\n\t\t\tint target_id;\n\n\t\t\tif (!pqn->q)\n\t\t\t\tcontinue;\n\n\t\t\ttarget_id = event_mask & KFD_EC_MASK(EC_QUEUE_NEW) ?\n\t\t\t\t\tpqn->q->properties.queue_id :\n\t\t\t\t\t\t\tpqn->q->doorbell_id;\n\n\t\t\tif (pqn->q->device != dev || target_id != source_id)\n\t\t\t\tcontinue;\n\n\t\t\tpqn->q->properties.exception_status |= event_mask;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (process->exception_enable_mask & event_mask) {\n\t\tif (use_worker)\n\t\t\tschedule_work(&process->debug_event_workarea);\n\t\telse\n\t\t\tkernel_write(process->dbg_ev_file,\n\t\t\t\t\t&write_data,\n\t\t\t\t\t1,\n\t\t\t\t\t&pos);\n\t} else {\n\t\tis_subscribed = false;\n\t}\n\n\tmutex_unlock(&process->event_mutex);\n\n\treturn is_subscribed;\n}\n\n \nbool kfd_set_dbg_ev_from_interrupt(struct kfd_node *dev,\n\t\t\t\t   unsigned int pasid,\n\t\t\t\t   uint32_t doorbell_id,\n\t\t\t\t   uint64_t trap_mask,\n\t\t\t\t   void *exception_data,\n\t\t\t\t   size_t exception_data_size)\n{\n\tstruct kfd_process *p;\n\tbool signaled_to_debugger_or_runtime = false;\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\n\tif (!p)\n\t\treturn false;\n\n\tif (!kfd_dbg_ev_raise(trap_mask, p, dev, doorbell_id, true,\n\t\t\t      exception_data, exception_data_size)) {\n\t\tstruct process_queue_manager *pqm;\n\t\tstruct process_queue_node *pqn;\n\n\t\tif (!!(trap_mask & KFD_EC_MASK_QUEUE) &&\n\t\t       p->runtime_info.runtime_state == DEBUG_RUNTIME_STATE_ENABLED) {\n\t\t\tmutex_lock(&p->mutex);\n\n\t\t\tpqm = &p->pqm;\n\t\t\tlist_for_each_entry(pqn, &pqm->queues,\n\t\t\t\t\t\t\tprocess_queue_list) {\n\n\t\t\t\tif (!(pqn->q && pqn->q->device == dev &&\n\t\t\t\t      pqn->q->doorbell_id == doorbell_id))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tkfd_send_exception_to_runtime(p, pqn->q->properties.queue_id,\n\t\t\t\t\t\t\t      trap_mask);\n\n\t\t\t\tsignaled_to_debugger_or_runtime = true;\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmutex_unlock(&p->mutex);\n\t\t} else if (trap_mask & KFD_EC_MASK(EC_DEVICE_MEMORY_VIOLATION)) {\n\t\t\tkfd_dqm_evict_pasid(dev->dqm, p->pasid);\n\t\t\tkfd_signal_vm_fault_event(dev, p->pasid, NULL,\n\t\t\t\t\t\t\texception_data);\n\n\t\t\tsignaled_to_debugger_or_runtime = true;\n\t\t}\n\t} else {\n\t\tsignaled_to_debugger_or_runtime = true;\n\t}\n\n\tkfd_unref_process(p);\n\n\treturn signaled_to_debugger_or_runtime;\n}\n\nint kfd_dbg_send_exception_to_runtime(struct kfd_process *p,\n\t\t\t\t\tunsigned int dev_id,\n\t\t\t\t\tunsigned int queue_id,\n\t\t\t\t\tuint64_t error_reason)\n{\n\tif (error_reason & KFD_EC_MASK(EC_DEVICE_MEMORY_VIOLATION)) {\n\t\tstruct kfd_process_device *pdd = NULL;\n\t\tstruct kfd_hsa_memory_exception_data *data;\n\t\tint i;\n\n\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\tif (p->pdds[i]->dev->id == dev_id) {\n\t\t\t\tpdd = p->pdds[i];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!pdd)\n\t\t\treturn -ENODEV;\n\n\t\tdata = (struct kfd_hsa_memory_exception_data *)\n\t\t\t\t\t\tpdd->vm_fault_exc_data;\n\n\t\tkfd_dqm_evict_pasid(pdd->dev->dqm, p->pasid);\n\t\tkfd_signal_vm_fault_event(pdd->dev, p->pasid, NULL, data);\n\t\terror_reason &= ~KFD_EC_MASK(EC_DEVICE_MEMORY_VIOLATION);\n\t}\n\n\tif (error_reason & (KFD_EC_MASK(EC_PROCESS_RUNTIME))) {\n\t\t \n\t\tup(&p->runtime_enable_sema);\n\t\terror_reason &= ~KFD_EC_MASK(EC_PROCESS_RUNTIME);\n\t}\n\n\tif (error_reason)\n\t\treturn kfd_send_exception_to_runtime(p, queue_id, error_reason);\n\n\treturn 0;\n}\n\nstatic int kfd_dbg_set_queue_workaround(struct queue *q, bool enable)\n{\n\tstruct mqd_update_info minfo = {0};\n\tint err;\n\n\tif (!q)\n\t\treturn 0;\n\n\tif (!kfd_dbg_has_cwsr_workaround(q->device))\n\t\treturn 0;\n\n\tif (enable && q->properties.is_user_cu_masked)\n\t\treturn -EBUSY;\n\n\tminfo.update_flag = enable ? UPDATE_FLAG_DBG_WA_ENABLE : UPDATE_FLAG_DBG_WA_DISABLE;\n\n\tq->properties.is_dbg_wa = enable;\n\terr = q->device->dqm->ops.update_queue(q->device->dqm, q, &minfo);\n\tif (err)\n\t\tq->properties.is_dbg_wa = false;\n\n\treturn err;\n}\n\nstatic int kfd_dbg_set_workaround(struct kfd_process *target, bool enable)\n{\n\tstruct process_queue_manager *pqm = &target->pqm;\n\tstruct process_queue_node *pqn;\n\tint r = 0;\n\n\tlist_for_each_entry(pqn, &pqm->queues, process_queue_list) {\n\t\tr = kfd_dbg_set_queue_workaround(pqn->q, enable);\n\t\tif (enable && r)\n\t\t\tgoto unwind;\n\t}\n\n\treturn 0;\n\nunwind:\n\tlist_for_each_entry(pqn, &pqm->queues, process_queue_list)\n\t\tkfd_dbg_set_queue_workaround(pqn->q, false);\n\n\tif (enable)\n\t\ttarget->runtime_info.runtime_state = r == -EBUSY ?\n\t\t\t\tDEBUG_RUNTIME_STATE_ENABLED_BUSY :\n\t\t\t\tDEBUG_RUNTIME_STATE_ENABLED_ERROR;\n\n\treturn r;\n}\n\nint kfd_dbg_set_mes_debug_mode(struct kfd_process_device *pdd, bool sq_trap_en)\n{\n\tuint32_t spi_dbg_cntl = pdd->spi_dbg_override | pdd->spi_dbg_launch_mode;\n\tuint32_t flags = pdd->process->dbg_flags;\n\n\tif (!kfd_dbg_is_per_vmid_supported(pdd->dev))\n\t\treturn 0;\n\n\treturn amdgpu_mes_set_shader_debugger(pdd->dev->adev, pdd->proc_ctx_gpu_addr, spi_dbg_cntl,\n\t\t\t\t\t\tpdd->watch_points, flags, sq_trap_en);\n}\n\n#define KFD_DEBUGGER_INVALID_WATCH_POINT_ID -1\nstatic int kfd_dbg_get_dev_watch_id(struct kfd_process_device *pdd, int *watch_id)\n{\n\tint i;\n\n\t*watch_id = KFD_DEBUGGER_INVALID_WATCH_POINT_ID;\n\n\tspin_lock(&pdd->dev->kfd->watch_points_lock);\n\n\tfor (i = 0; i < MAX_WATCH_ADDRESSES; i++) {\n\t\t \n\t\tif ((pdd->dev->kfd->alloc_watch_ids >> i) & 0x1)\n\t\t\tcontinue;\n\n\t\tpdd->alloc_watch_ids |= 0x1 << i;\n\t\tpdd->dev->kfd->alloc_watch_ids |= 0x1 << i;\n\t\t*watch_id = i;\n\t\tspin_unlock(&pdd->dev->kfd->watch_points_lock);\n\t\treturn 0;\n\t}\n\n\tspin_unlock(&pdd->dev->kfd->watch_points_lock);\n\n\treturn -ENOMEM;\n}\n\nstatic void kfd_dbg_clear_dev_watch_id(struct kfd_process_device *pdd, int watch_id)\n{\n\tspin_lock(&pdd->dev->kfd->watch_points_lock);\n\n\t \n\tif ((pdd->alloc_watch_ids >> watch_id) & 0x1) {\n\t\tpdd->alloc_watch_ids &= ~(0x1 << watch_id);\n\t\tpdd->dev->kfd->alloc_watch_ids &= ~(0x1 << watch_id);\n\t}\n\n\tspin_unlock(&pdd->dev->kfd->watch_points_lock);\n}\n\nstatic bool kfd_dbg_owns_dev_watch_id(struct kfd_process_device *pdd, int watch_id)\n{\n\tbool owns_watch_id = false;\n\n\tspin_lock(&pdd->dev->kfd->watch_points_lock);\n\towns_watch_id = watch_id < MAX_WATCH_ADDRESSES &&\n\t\t\t((pdd->alloc_watch_ids >> watch_id) & 0x1);\n\n\tspin_unlock(&pdd->dev->kfd->watch_points_lock);\n\n\treturn owns_watch_id;\n}\n\nint kfd_dbg_trap_clear_dev_address_watch(struct kfd_process_device *pdd,\n\t\t\t\t\tuint32_t watch_id)\n{\n\tint r;\n\n\tif (!kfd_dbg_owns_dev_watch_id(pdd, watch_id))\n\t\treturn -EINVAL;\n\n\tif (!pdd->dev->kfd->shared_resources.enable_mes) {\n\t\tr = debug_lock_and_unmap(pdd->dev->dqm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\tpdd->watch_points[watch_id] = pdd->dev->kfd2kgd->clear_address_watch(\n\t\t\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\t\t\twatch_id);\n\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\tr = debug_map_and_unlock(pdd->dev->dqm);\n\telse\n\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\tkfd_dbg_clear_dev_watch_id(pdd, watch_id);\n\n\treturn r;\n}\n\nint kfd_dbg_trap_set_dev_address_watch(struct kfd_process_device *pdd,\n\t\t\t\t\tuint64_t watch_address,\n\t\t\t\t\tuint32_t watch_address_mask,\n\t\t\t\t\tuint32_t *watch_id,\n\t\t\t\t\tuint32_t watch_mode)\n{\n\tint xcc_id, r = kfd_dbg_get_dev_watch_id(pdd, watch_id);\n\tuint32_t xcc_mask = pdd->dev->xcc_mask;\n\n\tif (r)\n\t\treturn r;\n\n\tif (!pdd->dev->kfd->shared_resources.enable_mes) {\n\t\tr = debug_lock_and_unmap(pdd->dev->dqm);\n\t\tif (r) {\n\t\t\tkfd_dbg_clear_dev_watch_id(pdd, *watch_id);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\tfor_each_inst(xcc_id, xcc_mask)\n\t\tpdd->watch_points[*watch_id] = pdd->dev->kfd2kgd->set_address_watch(\n\t\t\t\tpdd->dev->adev,\n\t\t\t\twatch_address,\n\t\t\t\twatch_address_mask,\n\t\t\t\t*watch_id,\n\t\t\t\twatch_mode,\n\t\t\t\tpdd->dev->vm_info.last_vmid_kfd,\n\t\t\t\txcc_id);\n\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\tr = debug_map_and_unlock(pdd->dev->dqm);\n\telse\n\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\t \n\tif (r)\n\t\tkfd_dbg_clear_dev_watch_id(pdd, *watch_id);\n\n\treturn 0;\n}\n\nstatic void kfd_dbg_clear_process_address_watch(struct kfd_process *target)\n{\n\tint i, j;\n\n\tfor (i = 0; i < target->n_pdds; i++)\n\t\tfor (j = 0; j < MAX_WATCH_ADDRESSES; j++)\n\t\t\tkfd_dbg_trap_clear_dev_address_watch(target->pdds[i], j);\n}\n\nint kfd_dbg_trap_set_flags(struct kfd_process *target, uint32_t *flags)\n{\n\tuint32_t prev_flags = target->dbg_flags;\n\tint i, r = 0, rewind_count = 0;\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tif (!kfd_dbg_is_per_vmid_supported(target->pdds[i]->dev) &&\n\t\t\t(*flags & KFD_DBG_TRAP_FLAG_SINGLE_MEM_OP)) {\n\t\t\t*flags = prev_flags;\n\t\t\treturn -EACCES;\n\t\t}\n\t}\n\n\ttarget->dbg_flags = *flags & KFD_DBG_TRAP_FLAG_SINGLE_MEM_OP;\n\t*flags = prev_flags;\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tif (!kfd_dbg_is_per_vmid_supported(pdd->dev))\n\t\t\tcontinue;\n\n\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tr = debug_refresh_runlist(pdd->dev->dqm);\n\t\telse\n\t\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\t\tif (r) {\n\t\t\ttarget->dbg_flags = prev_flags;\n\t\t\tbreak;\n\t\t}\n\n\t\trewind_count++;\n\t}\n\n\t \n\tif (r) {\n\t\ttarget->dbg_flags = prev_flags;\n\n\t\tfor (i = 0; i < rewind_count; i++) {\n\t\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\t\tif (!kfd_dbg_is_per_vmid_supported(pdd->dev))\n\t\t\t\tcontinue;\n\n\t\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\t\tdebug_refresh_runlist(pdd->dev->dqm);\n\t\t\telse\n\t\t\t\tkfd_dbg_set_mes_debug_mode(pdd, true);\n\t\t}\n\t}\n\n\treturn r;\n}\n\n \nvoid kfd_dbg_trap_deactivate(struct kfd_process *target, bool unwind, int unwind_count)\n{\n\tint i;\n\n\tif (!unwind) {\n\t\tuint32_t flags = 0;\n\t\tint resume_count = resume_queues(target, 0, NULL);\n\n\t\tif (resume_count)\n\t\t\tpr_debug(\"Resumed %d queues\\n\", resume_count);\n\n\t\tcancel_work_sync(&target->debug_event_workarea);\n\t\tkfd_dbg_clear_process_address_watch(target);\n\t\tkfd_dbg_trap_set_wave_launch_mode(target, 0);\n\n\t\tkfd_dbg_trap_set_flags(target, &flags);\n\t}\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\t \n\t\tif (unwind && i == unwind_count)\n\t\t\tbreak;\n\n\t\tkfd_process_set_trap_debug_flag(&pdd->qpd, false);\n\n\t\t \n\t\tif (kfd_dbg_is_rlc_restore_supported(pdd->dev))\n\t\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\t\tpdd->spi_dbg_override =\n\t\t\t\tpdd->dev->kfd2kgd->disable_debug_trap(\n\t\t\t\tpdd->dev->adev,\n\t\t\t\ttarget->runtime_info.ttmp_setup,\n\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\t\tif (!kfd_dbg_is_per_vmid_supported(pdd->dev) &&\n\t\t\t\trelease_debug_trap_vmid(pdd->dev->dqm, &pdd->qpd))\n\t\t\tpr_err(\"Failed to release debug vmid on [%i]\\n\", pdd->dev->id);\n\n\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tdebug_refresh_runlist(pdd->dev->dqm);\n\t\telse\n\t\t\tkfd_dbg_set_mes_debug_mode(pdd, !kfd_dbg_has_cwsr_workaround(pdd->dev));\n\t}\n\n\tkfd_dbg_set_workaround(target, false);\n}\n\nstatic void kfd_dbg_clean_exception_status(struct kfd_process *target)\n{\n\tstruct process_queue_manager *pqm;\n\tstruct process_queue_node *pqn;\n\tint i;\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tkfd_process_drain_interrupts(pdd);\n\n\t\tpdd->exception_status = 0;\n\t}\n\n\tpqm = &target->pqm;\n\tlist_for_each_entry(pqn, &pqm->queues, process_queue_list) {\n\t\tif (!pqn->q)\n\t\t\tcontinue;\n\n\t\tpqn->q->properties.exception_status = 0;\n\t}\n\n\ttarget->exception_status = 0;\n}\n\nint kfd_dbg_trap_disable(struct kfd_process *target)\n{\n\tif (!target->debug_trap_enabled)\n\t\treturn 0;\n\n\t \n\tif (target->runtime_info.runtime_state == DEBUG_RUNTIME_STATE_ENABLED)\n\t\tkfd_dbg_trap_deactivate(target, false, 0);\n\telse if (target->runtime_info.runtime_state != DEBUG_RUNTIME_STATE_DISABLED)\n\t\ttarget->runtime_info.runtime_state = DEBUG_RUNTIME_STATE_ENABLED;\n\n\tfput(target->dbg_ev_file);\n\ttarget->dbg_ev_file = NULL;\n\n\tif (target->debugger_process) {\n\t\tatomic_dec(&target->debugger_process->debugged_process_count);\n\t\ttarget->debugger_process = NULL;\n\t}\n\n\ttarget->debug_trap_enabled = false;\n\tkfd_dbg_clean_exception_status(target);\n\tkfd_unref_process(target);\n\n\treturn 0;\n}\n\nint kfd_dbg_trap_activate(struct kfd_process *target)\n{\n\tint i, r = 0;\n\n\tr = kfd_dbg_set_workaround(target, true);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tif (!kfd_dbg_is_per_vmid_supported(pdd->dev)) {\n\t\t\tr = reserve_debug_trap_vmid(pdd->dev->dqm, &pdd->qpd);\n\n\t\t\tif (r) {\n\t\t\t\ttarget->runtime_info.runtime_state = (r == -EBUSY) ?\n\t\t\t\t\t\t\tDEBUG_RUNTIME_STATE_ENABLED_BUSY :\n\t\t\t\t\t\t\tDEBUG_RUNTIME_STATE_ENABLED_ERROR;\n\n\t\t\t\tgoto unwind_err;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\t\tif (!(kfd_dbg_is_rlc_restore_supported(pdd->dev) ||\n\t\t\t\t\t\ttarget->runtime_info.ttmp_setup))\n\t\t\tpdd->dev->kfd2kgd->enable_debug_trap(pdd->dev->adev, true,\n\t\t\t\t\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\n\t\tpdd->spi_dbg_override = pdd->dev->kfd2kgd->enable_debug_trap(\n\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\tfalse,\n\t\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\n\t\tif (kfd_dbg_is_rlc_restore_supported(pdd->dev))\n\t\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\t\t \n\t\tkfd_process_set_trap_debug_flag(&pdd->qpd, true);\n\n\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tr = debug_refresh_runlist(pdd->dev->dqm);\n\t\telse\n\t\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\t\tif (r) {\n\t\t\ttarget->runtime_info.runtime_state =\n\t\t\t\t\tDEBUG_RUNTIME_STATE_ENABLED_ERROR;\n\t\t\tgoto unwind_err;\n\t\t}\n\t}\n\n\treturn 0;\n\nunwind_err:\n\t \n\tkfd_dbg_trap_deactivate(target, true, i);\n\treturn r;\n}\n\nint kfd_dbg_trap_enable(struct kfd_process *target, uint32_t fd,\n\t\t\tvoid __user *runtime_info, uint32_t *runtime_size)\n{\n\tstruct file *f;\n\tuint32_t copy_size;\n\tint i, r = 0;\n\n\tif (target->debug_trap_enabled)\n\t\treturn -EALREADY;\n\n\t \n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tif (!KFD_IS_SOC15(pdd->dev))\n\t\t\treturn -ENODEV;\n\n\t\tif (pdd->qpd.num_gws && (!kfd_dbg_has_gws_support(pdd->dev) ||\n\t\t\t\t\t kfd_dbg_has_cwsr_workaround(pdd->dev)))\n\t\t\treturn -EBUSY;\n\t}\n\n\tcopy_size = min((size_t)(*runtime_size), sizeof(target->runtime_info));\n\n\tf = fget(fd);\n\tif (!f) {\n\t\tpr_err(\"Failed to get file for (%i)\\n\", fd);\n\t\treturn -EBADF;\n\t}\n\n\ttarget->dbg_ev_file = f;\n\n\t \n\tif (target->runtime_info.runtime_state == DEBUG_RUNTIME_STATE_ENABLED)\n\t\tkfd_dbg_trap_activate(target);\n\n\t \n\tkref_get(&target->ref);\n\ttarget->debug_trap_enabled = true;\n\n\tif (target->debugger_process)\n\t\tatomic_inc(&target->debugger_process->debugged_process_count);\n\n\tif (copy_to_user(runtime_info, (void *)&target->runtime_info, copy_size)) {\n\t\tkfd_dbg_trap_deactivate(target, false, 0);\n\t\tr = -EFAULT;\n\t}\n\n\t*runtime_size = sizeof(target->runtime_info);\n\n\treturn r;\n}\n\nstatic int kfd_dbg_validate_trap_override_request(struct kfd_process *p,\n\t\t\t\t\t\tuint32_t trap_override,\n\t\t\t\t\t\tuint32_t trap_mask_request,\n\t\t\t\t\t\tuint32_t *trap_mask_supported)\n{\n\tint i = 0;\n\n\t*trap_mask_supported = 0xffffffff;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tint err = pdd->dev->kfd2kgd->validate_trap_override_request(\n\t\t\t\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\t\t\t\ttrap_override,\n\t\t\t\t\t\t\t\ttrap_mask_supported);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (trap_mask_request & ~*trap_mask_supported)\n\t\treturn -EACCES;\n\n\treturn 0;\n}\n\nint kfd_dbg_trap_set_wave_launch_override(struct kfd_process *target,\n\t\t\t\t\tuint32_t trap_override,\n\t\t\t\t\tuint32_t trap_mask_bits,\n\t\t\t\t\tuint32_t trap_mask_request,\n\t\t\t\t\tuint32_t *trap_mask_prev,\n\t\t\t\t\tuint32_t *trap_mask_supported)\n{\n\tint r = 0, i;\n\n\tr = kfd_dbg_validate_trap_override_request(target,\n\t\t\t\t\t\ttrap_override,\n\t\t\t\t\t\ttrap_mask_request,\n\t\t\t\t\t\ttrap_mask_supported);\n\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\t\tpdd->spi_dbg_override = pdd->dev->kfd2kgd->set_wave_launch_trap_override(\n\t\t\t\tpdd->dev->adev,\n\t\t\t\tpdd->dev->vm_info.last_vmid_kfd,\n\t\t\t\ttrap_override,\n\t\t\t\ttrap_mask_bits,\n\t\t\t\ttrap_mask_request,\n\t\t\t\ttrap_mask_prev,\n\t\t\t\tpdd->spi_dbg_override);\n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tr = debug_refresh_runlist(pdd->dev->dqm);\n\t\telse\n\t\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nint kfd_dbg_trap_set_wave_launch_mode(struct kfd_process *target,\n\t\t\t\t\tuint8_t wave_launch_mode)\n{\n\tint r = 0, i;\n\n\tif (wave_launch_mode != KFD_DBG_TRAP_WAVE_LAUNCH_MODE_NORMAL &&\n\t\t\twave_launch_mode != KFD_DBG_TRAP_WAVE_LAUNCH_MODE_HALT &&\n\t\t\twave_launch_mode != KFD_DBG_TRAP_WAVE_LAUNCH_MODE_DEBUG)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\t\tpdd->spi_dbg_launch_mode = pdd->dev->kfd2kgd->set_wave_launch_mode(\n\t\t\t\tpdd->dev->adev,\n\t\t\t\twave_launch_mode,\n\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\n\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tr = debug_refresh_runlist(pdd->dev->dqm);\n\t\telse\n\t\t\tr = kfd_dbg_set_mes_debug_mode(pdd, true);\n\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nint kfd_dbg_trap_query_exception_info(struct kfd_process *target,\n\t\tuint32_t source_id,\n\t\tuint32_t exception_code,\n\t\tbool clear_exception,\n\t\tvoid __user *info,\n\t\tuint32_t *info_size)\n{\n\tbool found = false;\n\tint r = 0;\n\tuint32_t copy_size, actual_info_size = 0;\n\tuint64_t *exception_status_ptr = NULL;\n\n\tif (!target)\n\t\treturn -EINVAL;\n\n\tif (!info || !info_size)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&target->event_mutex);\n\n\tif (KFD_DBG_EC_TYPE_IS_QUEUE(exception_code)) {\n\t\t \n\t\tstruct queue *queue = NULL;\n\t\tint i;\n\n\t\tfor (i = 0; i < target->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\t\t\tstruct qcm_process_device *qpd = &pdd->qpd;\n\n\t\t\tlist_for_each_entry(queue, &qpd->queues_list, list) {\n\t\t\t\tif (!found && queue->properties.queue_id == source_id) {\n\t\t\t\t\tfound = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (found)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!found) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!(queue->properties.exception_status & KFD_EC_MASK(exception_code))) {\n\t\t\tr = -ENODATA;\n\t\t\tgoto out;\n\t\t}\n\t\texception_status_ptr = &queue->properties.exception_status;\n\t} else if (KFD_DBG_EC_TYPE_IS_DEVICE(exception_code)) {\n\t\t \n\t\tstruct kfd_process_device *pdd = NULL;\n\t\tint i;\n\n\t\tfor (i = 0; i < target->n_pdds; i++) {\n\t\t\tpdd = target->pdds[i];\n\t\t\tif (pdd->dev->id == source_id) {\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!found) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!(pdd->exception_status & KFD_EC_MASK(exception_code))) {\n\t\t\tr = -ENODATA;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (exception_code == EC_DEVICE_MEMORY_VIOLATION) {\n\t\t\tcopy_size = min((size_t)(*info_size), pdd->vm_fault_exc_data_size);\n\n\t\t\tif (copy_to_user(info, pdd->vm_fault_exc_data, copy_size)) {\n\t\t\t\tr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tactual_info_size = pdd->vm_fault_exc_data_size;\n\t\t\tif (clear_exception) {\n\t\t\t\tkfree(pdd->vm_fault_exc_data);\n\t\t\t\tpdd->vm_fault_exc_data = NULL;\n\t\t\t\tpdd->vm_fault_exc_data_size = 0;\n\t\t\t}\n\t\t}\n\t\texception_status_ptr = &pdd->exception_status;\n\t} else if (KFD_DBG_EC_TYPE_IS_PROCESS(exception_code)) {\n\t\t \n\t\tif (!(target->exception_status & KFD_EC_MASK(exception_code))) {\n\t\t\tr = -ENODATA;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (exception_code == EC_PROCESS_RUNTIME) {\n\t\t\tcopy_size = min((size_t)(*info_size), sizeof(target->runtime_info));\n\n\t\t\tif (copy_to_user(info, (void *)&target->runtime_info, copy_size)) {\n\t\t\t\tr = -EFAULT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tactual_info_size = sizeof(target->runtime_info);\n\t\t}\n\n\t\texception_status_ptr = &target->exception_status;\n\t} else {\n\t\tpr_debug(\"Bad exception type [%i]\\n\", exception_code);\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*info_size = actual_info_size;\n\tif (clear_exception)\n\t\t*exception_status_ptr &= ~KFD_EC_MASK(exception_code);\nout:\n\tmutex_unlock(&target->event_mutex);\n\treturn r;\n}\n\nint kfd_dbg_trap_device_snapshot(struct kfd_process *target,\n\t\tuint64_t exception_clear_mask,\n\t\tvoid __user *user_info,\n\t\tuint32_t *number_of_device_infos,\n\t\tuint32_t *entry_size)\n{\n\tstruct kfd_dbg_device_info_entry device_info;\n\tuint32_t tmp_entry_size = *entry_size, tmp_num_devices;\n\tint i, r = 0;\n\n\tif (!(target && user_info && number_of_device_infos && entry_size))\n\t\treturn -EINVAL;\n\n\ttmp_num_devices = min_t(size_t, *number_of_device_infos, target->n_pdds);\n\t*number_of_device_infos = target->n_pdds;\n\t*entry_size = min_t(size_t, *entry_size, sizeof(device_info));\n\n\tif (!tmp_num_devices)\n\t\treturn 0;\n\n\tmemset(&device_info, 0, sizeof(device_info));\n\n\tmutex_lock(&target->event_mutex);\n\n\t \n\tfor (i = 0; i < tmp_num_devices; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\t\tstruct kfd_topology_device *topo_dev = kfd_topology_device_by_id(pdd->dev->id);\n\n\t\tdevice_info.gpu_id = pdd->dev->id;\n\t\tdevice_info.exception_status = pdd->exception_status;\n\t\tdevice_info.lds_base = pdd->lds_base;\n\t\tdevice_info.lds_limit = pdd->lds_limit;\n\t\tdevice_info.scratch_base = pdd->scratch_base;\n\t\tdevice_info.scratch_limit = pdd->scratch_limit;\n\t\tdevice_info.gpuvm_base = pdd->gpuvm_base;\n\t\tdevice_info.gpuvm_limit = pdd->gpuvm_limit;\n\t\tdevice_info.location_id = topo_dev->node_props.location_id;\n\t\tdevice_info.vendor_id = topo_dev->node_props.vendor_id;\n\t\tdevice_info.device_id = topo_dev->node_props.device_id;\n\t\tdevice_info.revision_id = pdd->dev->adev->pdev->revision;\n\t\tdevice_info.subsystem_vendor_id = pdd->dev->adev->pdev->subsystem_vendor;\n\t\tdevice_info.subsystem_device_id = pdd->dev->adev->pdev->subsystem_device;\n\t\tdevice_info.fw_version = pdd->dev->kfd->mec_fw_version;\n\t\tdevice_info.gfx_target_version =\n\t\t\ttopo_dev->node_props.gfx_target_version;\n\t\tdevice_info.simd_count = topo_dev->node_props.simd_count;\n\t\tdevice_info.max_waves_per_simd =\n\t\t\ttopo_dev->node_props.max_waves_per_simd;\n\t\tdevice_info.array_count = topo_dev->node_props.array_count;\n\t\tdevice_info.simd_arrays_per_engine =\n\t\t\ttopo_dev->node_props.simd_arrays_per_engine;\n\t\tdevice_info.num_xcc = NUM_XCC(pdd->dev->xcc_mask);\n\t\tdevice_info.capability = topo_dev->node_props.capability;\n\t\tdevice_info.debug_prop = topo_dev->node_props.debug_prop;\n\n\t\tif (exception_clear_mask)\n\t\t\tpdd->exception_status &= ~exception_clear_mask;\n\n\t\tif (copy_to_user(user_info, &device_info, *entry_size)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tuser_info += tmp_entry_size;\n\t}\n\n\tmutex_unlock(&target->event_mutex);\n\n\treturn r;\n}\n\nvoid kfd_dbg_set_enabled_debug_exception_mask(struct kfd_process *target,\n\t\t\t\t\tuint64_t exception_set_mask)\n{\n\tuint64_t found_mask = 0;\n\tstruct process_queue_manager *pqm;\n\tstruct process_queue_node *pqn;\n\tstatic const char write_data = '.';\n\tloff_t pos = 0;\n\tint i;\n\n\tmutex_lock(&target->event_mutex);\n\n\tfound_mask |= target->exception_status;\n\n\tpqm = &target->pqm;\n\tlist_for_each_entry(pqn, &pqm->queues, process_queue_list) {\n\t\tif (!pqn->q)\n\t\t\tcontinue;\n\n\t\tfound_mask |= pqn->q->properties.exception_status;\n\t}\n\n\tfor (i = 0; i < target->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = target->pdds[i];\n\n\t\tfound_mask |= pdd->exception_status;\n\t}\n\n\tif (exception_set_mask & found_mask)\n\t\tkernel_write(target->dbg_ev_file, &write_data, 1, &pos);\n\n\ttarget->exception_enable_mask = exception_set_mask;\n\n\tmutex_unlock(&target->event_mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}