{
  "module_name": "kfd_mqd_manager.c",
  "hash_id": "3f96c6fe6a80d8220ab9af8540a51b5e0dd4e24d6be3f077ab2ff10a9125417b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c",
  "human_readable_source": "\n \n\n#include \"kfd_mqd_manager.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"kfd_device_queue_manager.h\"\n\n \nint pipe_priority_map[] = {\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_LOW,\n\tKFD_PIPE_PRIORITY_CS_MEDIUM,\n\tKFD_PIPE_PRIORITY_CS_MEDIUM,\n\tKFD_PIPE_PRIORITY_CS_MEDIUM,\n\tKFD_PIPE_PRIORITY_CS_MEDIUM,\n\tKFD_PIPE_PRIORITY_CS_HIGH,\n\tKFD_PIPE_PRIORITY_CS_HIGH,\n\tKFD_PIPE_PRIORITY_CS_HIGH,\n\tKFD_PIPE_PRIORITY_CS_HIGH,\n\tKFD_PIPE_PRIORITY_CS_HIGH\n};\n\nstruct kfd_mem_obj *allocate_hiq_mqd(struct kfd_node *dev, struct queue_properties *q)\n{\n\tstruct kfd_mem_obj *mqd_mem_obj;\n\n\tmqd_mem_obj = kzalloc(sizeof(struct kfd_mem_obj), GFP_KERNEL);\n\tif (!mqd_mem_obj)\n\t\treturn NULL;\n\n\tmqd_mem_obj->gtt_mem = dev->dqm->hiq_sdma_mqd.gtt_mem;\n\tmqd_mem_obj->gpu_addr = dev->dqm->hiq_sdma_mqd.gpu_addr;\n\tmqd_mem_obj->cpu_ptr = dev->dqm->hiq_sdma_mqd.cpu_ptr;\n\n\treturn mqd_mem_obj;\n}\n\nstruct kfd_mem_obj *allocate_sdma_mqd(struct kfd_node *dev,\n\t\t\t\t\tstruct queue_properties *q)\n{\n\tstruct kfd_mem_obj *mqd_mem_obj;\n\tuint64_t offset;\n\n\tmqd_mem_obj = kzalloc(sizeof(struct kfd_mem_obj), GFP_KERNEL);\n\tif (!mqd_mem_obj)\n\t\treturn NULL;\n\n\toffset = (q->sdma_engine_id *\n\t\tdev->kfd->device_info.num_sdma_queues_per_engine +\n\t\tq->sdma_queue_id) *\n\t\tdev->dqm->mqd_mgrs[KFD_MQD_TYPE_SDMA]->mqd_size;\n\n\toffset += dev->dqm->mqd_mgrs[KFD_MQD_TYPE_HIQ]->mqd_size *\n\t\t  NUM_XCC(dev->xcc_mask);\n\n\tmqd_mem_obj->gtt_mem = (void *)((uint64_t)dev->dqm->hiq_sdma_mqd.gtt_mem\n\t\t\t\t+ offset);\n\tmqd_mem_obj->gpu_addr = dev->dqm->hiq_sdma_mqd.gpu_addr + offset;\n\tmqd_mem_obj->cpu_ptr = (uint32_t *)((uint64_t)\n\t\t\t\tdev->dqm->hiq_sdma_mqd.cpu_ptr + offset);\n\n\treturn mqd_mem_obj;\n}\n\nvoid free_mqd_hiq_sdma(struct mqd_manager *mm, void *mqd,\n\t\t\tstruct kfd_mem_obj *mqd_mem_obj)\n{\n\tWARN_ON(!mqd_mem_obj->gtt_mem);\n\tkfree(mqd_mem_obj);\n}\n\nvoid mqd_symmetrically_map_cu_mask(struct mqd_manager *mm,\n\t\tconst uint32_t *cu_mask, uint32_t cu_mask_count,\n\t\tuint32_t *se_mask, uint32_t inst)\n{\n\tstruct kfd_cu_info cu_info;\n\tuint32_t cu_per_sh[KFD_MAX_NUM_SE][KFD_MAX_NUM_SH_PER_SE] = {0};\n\tbool wgp_mode_req = KFD_GC_VERSION(mm->dev) >= IP_VERSION(10, 0, 0);\n\tuint32_t en_mask = wgp_mode_req ? 0x3 : 0x1;\n\tint i, se, sh, cu, cu_bitmap_sh_mul, cu_inc = wgp_mode_req ? 2 : 1;\n\tuint32_t cu_active_per_node;\n\tint inc = cu_inc * NUM_XCC(mm->dev->xcc_mask);\n\tint xcc_inst = inst + ffs(mm->dev->xcc_mask) - 1;\n\n\tamdgpu_amdkfd_get_cu_info(mm->dev->adev, &cu_info);\n\n\tcu_active_per_node = cu_info.cu_active_number / mm->dev->kfd->num_nodes;\n\tif (cu_mask_count > cu_active_per_node)\n\t\tcu_mask_count = cu_active_per_node;\n\n\t \n\tif (cu_info.num_shader_engines > KFD_MAX_NUM_SE) {\n\t\tpr_err(\"Exceeded KFD_MAX_NUM_SE, chip reports %d\\n\", cu_info.num_shader_engines);\n\t\treturn;\n\t}\n\tif (cu_info.num_shader_arrays_per_engine > KFD_MAX_NUM_SH_PER_SE) {\n\t\tpr_err(\"Exceeded KFD_MAX_NUM_SH, chip reports %d\\n\",\n\t\t\tcu_info.num_shader_arrays_per_engine * cu_info.num_shader_engines);\n\t\treturn;\n\t}\n\n\tcu_bitmap_sh_mul = (KFD_GC_VERSION(mm->dev) >= IP_VERSION(11, 0, 0) &&\n\t\t\t    KFD_GC_VERSION(mm->dev) < IP_VERSION(12, 0, 0)) ? 2 : 1;\n\n\t \n\tfor (se = 0; se < cu_info.num_shader_engines; se++)\n\t\tfor (sh = 0; sh < cu_info.num_shader_arrays_per_engine; sh++)\n\t\t\tcu_per_sh[se][sh] = hweight32(\n\t\t\t\tcu_info.cu_bitmap[xcc_inst][se % 4][sh + (se / 4) *\n\t\t\t\tcu_bitmap_sh_mul]);\n\n\t \n\tfor (i = 0; i < cu_info.num_shader_engines; i++)\n\t\tse_mask[i] = 0;\n\n\ti = inst;\n\tfor (cu = 0; cu < 16; cu += cu_inc) {\n\t\tfor (sh = 0; sh < cu_info.num_shader_arrays_per_engine; sh++) {\n\t\t\tfor (se = 0; se < cu_info.num_shader_engines; se++) {\n\t\t\t\tif (cu_per_sh[se][sh] > cu) {\n\t\t\t\t\tif (cu_mask[i / 32] & (en_mask << (i % 32)))\n\t\t\t\t\t\tse_mask[se] |= en_mask << (cu + sh * 16);\n\t\t\t\t\ti += inc;\n\t\t\t\t\tif (i >= cu_mask_count)\n\t\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nint kfd_hiq_load_mqd_kiq(struct mqd_manager *mm, void *mqd,\n\t\t     uint32_t pipe_id, uint32_t queue_id,\n\t\t     struct queue_properties *p, struct mm_struct *mms)\n{\n\treturn mm->dev->kfd2kgd->hiq_mqd_load(mm->dev->adev, mqd, pipe_id,\n\t\t\t\t\t      queue_id, p->doorbell_off, 0);\n}\n\nint kfd_destroy_mqd_cp(struct mqd_manager *mm, void *mqd,\n\t\tenum kfd_preempt_type type, unsigned int timeout,\n\t\tuint32_t pipe_id, uint32_t queue_id)\n{\n\treturn mm->dev->kfd2kgd->hqd_destroy(mm->dev->adev, mqd, type, timeout,\n\t\t\t\t\t\tpipe_id, queue_id, 0);\n}\n\nvoid kfd_free_mqd_cp(struct mqd_manager *mm, void *mqd,\n\t      struct kfd_mem_obj *mqd_mem_obj)\n{\n\tif (mqd_mem_obj->gtt_mem) {\n\t\tamdgpu_amdkfd_free_gtt_mem(mm->dev->adev, mqd_mem_obj->gtt_mem);\n\t\tkfree(mqd_mem_obj);\n\t} else {\n\t\tkfd_gtt_sa_free(mm->dev, mqd_mem_obj);\n\t}\n}\n\nbool kfd_is_occupied_cp(struct mqd_manager *mm, void *mqd,\n\t\t uint64_t queue_address, uint32_t pipe_id,\n\t\t uint32_t queue_id)\n{\n\treturn mm->dev->kfd2kgd->hqd_is_occupied(mm->dev->adev, queue_address,\n\t\t\t\t\t\tpipe_id, queue_id, 0);\n}\n\nint kfd_load_mqd_sdma(struct mqd_manager *mm, void *mqd,\n\t\t  uint32_t pipe_id, uint32_t queue_id,\n\t\t  struct queue_properties *p, struct mm_struct *mms)\n{\n\treturn mm->dev->kfd2kgd->hqd_sdma_load(mm->dev->adev, mqd,\n\t\t\t\t\t\t(uint32_t __user *)p->write_ptr,\n\t\t\t\t\t\tmms);\n}\n\n \nint kfd_destroy_mqd_sdma(struct mqd_manager *mm, void *mqd,\n\t\t     enum kfd_preempt_type type,\n\t\t     unsigned int timeout, uint32_t pipe_id,\n\t\t     uint32_t queue_id)\n{\n\treturn mm->dev->kfd2kgd->hqd_sdma_destroy(mm->dev->adev, mqd, timeout);\n}\n\nbool kfd_is_occupied_sdma(struct mqd_manager *mm, void *mqd,\n\t\t      uint64_t queue_address, uint32_t pipe_id,\n\t\t      uint32_t queue_id)\n{\n\treturn mm->dev->kfd2kgd->hqd_sdma_is_occupied(mm->dev->adev, mqd);\n}\n\nuint64_t kfd_hiq_mqd_stride(struct kfd_node *dev)\n{\n\treturn dev->dqm->mqd_mgrs[KFD_MQD_TYPE_HIQ]->mqd_size;\n}\n\nvoid kfd_get_hiq_xcc_mqd(struct kfd_node *dev, struct kfd_mem_obj *mqd_mem_obj,\n\t\t     uint32_t virtual_xcc_id)\n{\n\tuint64_t offset;\n\n\toffset = kfd_hiq_mqd_stride(dev) * virtual_xcc_id;\n\n\tmqd_mem_obj->gtt_mem = (virtual_xcc_id == 0) ?\n\t\t\tdev->dqm->hiq_sdma_mqd.gtt_mem : NULL;\n\tmqd_mem_obj->gpu_addr = dev->dqm->hiq_sdma_mqd.gpu_addr + offset;\n\tmqd_mem_obj->cpu_ptr = (uint32_t *)((uintptr_t)\n\t\t\t\tdev->dqm->hiq_sdma_mqd.cpu_ptr + offset);\n}\n\nuint64_t kfd_mqd_stride(struct mqd_manager *mm,\n\t\t\tstruct queue_properties *q)\n{\n\treturn mm->mqd_size;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}