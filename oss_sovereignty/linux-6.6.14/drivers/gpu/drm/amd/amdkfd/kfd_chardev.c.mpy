{
  "module_name": "kfd_chardev.c",
  "hash_id": "716cfd474121013ecc54b4ab984037deea7c47d7b4d2326e2bb54c8604a3c9f3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_chardev.c",
  "human_readable_source": "\n \n\n#include <linux/device.h>\n#include <linux/export.h>\n#include <linux/err.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/compat.h>\n#include <uapi/linux/kfd_ioctl.h>\n#include <linux/time.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/ptrace.h>\n#include <linux/dma-buf.h>\n#include <linux/fdtable.h>\n#include <linux/processor.h>\n#include \"kfd_priv.h\"\n#include \"kfd_device_queue_manager.h\"\n#include \"kfd_svm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"kfd_smi_events.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"kfd_debug.h\"\n\nstatic long kfd_ioctl(struct file *, unsigned int, unsigned long);\nstatic int kfd_open(struct inode *, struct file *);\nstatic int kfd_release(struct inode *, struct file *);\nstatic int kfd_mmap(struct file *, struct vm_area_struct *);\n\nstatic const char kfd_dev_name[] = \"kfd\";\n\nstatic const struct file_operations kfd_fops = {\n\t.owner = THIS_MODULE,\n\t.unlocked_ioctl = kfd_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.open = kfd_open,\n\t.release = kfd_release,\n\t.mmap = kfd_mmap,\n};\n\nstatic int kfd_char_dev_major = -1;\nstatic struct class *kfd_class;\nstruct device *kfd_device;\n\nstatic inline struct kfd_process_device *kfd_lock_pdd_by_id(struct kfd_process *p, __u32 gpu_id)\n{\n\tstruct kfd_process_device *pdd;\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, gpu_id);\n\n\tif (pdd)\n\t\treturn pdd;\n\n\tmutex_unlock(&p->mutex);\n\treturn NULL;\n}\n\nstatic inline void kfd_unlock_pdd(struct kfd_process_device *pdd)\n{\n\tmutex_unlock(&pdd->process->mutex);\n}\n\nint kfd_chardev_init(void)\n{\n\tint err = 0;\n\n\tkfd_char_dev_major = register_chrdev(0, kfd_dev_name, &kfd_fops);\n\terr = kfd_char_dev_major;\n\tif (err < 0)\n\t\tgoto err_register_chrdev;\n\n\tkfd_class = class_create(kfd_dev_name);\n\terr = PTR_ERR(kfd_class);\n\tif (IS_ERR(kfd_class))\n\t\tgoto err_class_create;\n\n\tkfd_device = device_create(kfd_class, NULL,\n\t\t\t\t\tMKDEV(kfd_char_dev_major, 0),\n\t\t\t\t\tNULL, kfd_dev_name);\n\terr = PTR_ERR(kfd_device);\n\tif (IS_ERR(kfd_device))\n\t\tgoto err_device_create;\n\n\treturn 0;\n\nerr_device_create:\n\tclass_destroy(kfd_class);\nerr_class_create:\n\tunregister_chrdev(kfd_char_dev_major, kfd_dev_name);\nerr_register_chrdev:\n\treturn err;\n}\n\nvoid kfd_chardev_exit(void)\n{\n\tdevice_destroy(kfd_class, MKDEV(kfd_char_dev_major, 0));\n\tclass_destroy(kfd_class);\n\tunregister_chrdev(kfd_char_dev_major, kfd_dev_name);\n\tkfd_device = NULL;\n}\n\n\nstatic int kfd_open(struct inode *inode, struct file *filep)\n{\n\tstruct kfd_process *process;\n\tbool is_32bit_user_mode;\n\n\tif (iminor(inode) != 0)\n\t\treturn -ENODEV;\n\n\tis_32bit_user_mode = in_compat_syscall();\n\n\tif (is_32bit_user_mode) {\n\t\tdev_warn(kfd_device,\n\t\t\t\"Process %d (32-bit) failed to open /dev/kfd\\n\"\n\t\t\t\"32-bit processes are not supported by amdkfd\\n\",\n\t\t\tcurrent->pid);\n\t\treturn -EPERM;\n\t}\n\n\tprocess = kfd_create_process(current);\n\tif (IS_ERR(process))\n\t\treturn PTR_ERR(process);\n\n\tif (kfd_process_init_cwsr_apu(process, filep)) {\n\t\tkfd_unref_process(process);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tfilep->private_data = process;\n\n\tdev_dbg(kfd_device, \"process %d opened, compat mode (32 bit) - %d\\n\",\n\t\tprocess->pasid, process->is_32bit_user_mode);\n\n\treturn 0;\n}\n\nstatic int kfd_release(struct inode *inode, struct file *filep)\n{\n\tstruct kfd_process *process = filep->private_data;\n\n\tif (process)\n\t\tkfd_unref_process(process);\n\n\treturn 0;\n}\n\nstatic int kfd_ioctl_get_version(struct file *filep, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_get_version_args *args = data;\n\n\targs->major_version = KFD_IOCTL_MAJOR_VERSION;\n\targs->minor_version = KFD_IOCTL_MINOR_VERSION;\n\n\treturn 0;\n}\n\nstatic int set_queue_properties_from_user(struct queue_properties *q_properties,\n\t\t\t\tstruct kfd_ioctl_create_queue_args *args)\n{\n\t \n\tif ((args->queue_percentage & 0xFF) > KFD_MAX_QUEUE_PERCENTAGE) {\n\t\tpr_err(\"Queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->queue_priority > KFD_MAX_QUEUE_PRIORITY) {\n\t\tpr_err(\"Queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->ring_base_address) &&\n\t\t(!access_ok((const void __user *) args->ring_base_address,\n\t\t\tsizeof(uint64_t)))) {\n\t\tpr_err(\"Can't access ring base address\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!is_power_of_2(args->ring_size) && (args->ring_size != 0)) {\n\t\tpr_err(\"Ring size must be a power of 2 or 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!access_ok((const void __user *) args->read_pointer_address,\n\t\t\tsizeof(uint32_t))) {\n\t\tpr_err(\"Can't access read pointer\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!access_ok((const void __user *) args->write_pointer_address,\n\t\t\tsizeof(uint32_t))) {\n\t\tpr_err(\"Can't access write pointer\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (args->eop_buffer_address &&\n\t\t!access_ok((const void __user *) args->eop_buffer_address,\n\t\t\tsizeof(uint32_t))) {\n\t\tpr_debug(\"Can't access eop buffer\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (args->ctx_save_restore_address &&\n\t\t!access_ok((const void __user *) args->ctx_save_restore_address,\n\t\t\tsizeof(uint32_t))) {\n\t\tpr_debug(\"Can't access ctx save restore buffer\");\n\t\treturn -EFAULT;\n\t}\n\n\tq_properties->is_interop = false;\n\tq_properties->is_gws = false;\n\tq_properties->queue_percent = args->queue_percentage & 0xFF;\n\t \n\tq_properties->pm4_target_xcc = (args->queue_percentage >> 8) & 0xFF;\n\tq_properties->priority = args->queue_priority;\n\tq_properties->queue_address = args->ring_base_address;\n\tq_properties->queue_size = args->ring_size;\n\tq_properties->read_ptr = (uint32_t *) args->read_pointer_address;\n\tq_properties->write_ptr = (uint32_t *) args->write_pointer_address;\n\tq_properties->eop_ring_buffer_address = args->eop_buffer_address;\n\tq_properties->eop_ring_buffer_size = args->eop_buffer_size;\n\tq_properties->ctx_save_restore_area_address =\n\t\t\targs->ctx_save_restore_address;\n\tq_properties->ctx_save_restore_area_size = args->ctx_save_restore_size;\n\tq_properties->ctl_stack_size = args->ctl_stack_size;\n\tif (args->queue_type == KFD_IOC_QUEUE_TYPE_COMPUTE ||\n\t\targs->queue_type == KFD_IOC_QUEUE_TYPE_COMPUTE_AQL)\n\t\tq_properties->type = KFD_QUEUE_TYPE_COMPUTE;\n\telse if (args->queue_type == KFD_IOC_QUEUE_TYPE_SDMA)\n\t\tq_properties->type = KFD_QUEUE_TYPE_SDMA;\n\telse if (args->queue_type == KFD_IOC_QUEUE_TYPE_SDMA_XGMI)\n\t\tq_properties->type = KFD_QUEUE_TYPE_SDMA_XGMI;\n\telse\n\t\treturn -ENOTSUPP;\n\n\tif (args->queue_type == KFD_IOC_QUEUE_TYPE_COMPUTE_AQL)\n\t\tq_properties->format = KFD_QUEUE_FORMAT_AQL;\n\telse\n\t\tq_properties->format = KFD_QUEUE_FORMAT_PM4;\n\n\tpr_debug(\"Queue Percentage: %d, %d\\n\",\n\t\t\tq_properties->queue_percent, args->queue_percentage);\n\n\tpr_debug(\"Queue Priority: %d, %d\\n\",\n\t\t\tq_properties->priority, args->queue_priority);\n\n\tpr_debug(\"Queue Address: 0x%llX, 0x%llX\\n\",\n\t\t\tq_properties->queue_address, args->ring_base_address);\n\n\tpr_debug(\"Queue Size: 0x%llX, %u\\n\",\n\t\t\tq_properties->queue_size, args->ring_size);\n\n\tpr_debug(\"Queue r/w Pointers: %px, %px\\n\",\n\t\t\tq_properties->read_ptr,\n\t\t\tq_properties->write_ptr);\n\n\tpr_debug(\"Queue Format: %d\\n\", q_properties->format);\n\n\tpr_debug(\"Queue EOP: 0x%llX\\n\", q_properties->eop_ring_buffer_address);\n\n\tpr_debug(\"Queue CTX save area: 0x%llX\\n\",\n\t\t\tq_properties->ctx_save_restore_area_address);\n\n\treturn 0;\n}\n\nstatic int kfd_ioctl_create_queue(struct file *filep, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_create_queue_args *args = data;\n\tstruct kfd_node *dev;\n\tint err = 0;\n\tunsigned int queue_id;\n\tstruct kfd_process_device *pdd;\n\tstruct queue_properties q_properties;\n\tuint32_t doorbell_offset_in_process = 0;\n\tstruct amdgpu_bo *wptr_bo = NULL;\n\n\tmemset(&q_properties, 0, sizeof(struct queue_properties));\n\n\tpr_debug(\"Creating queue ioctl\\n\");\n\n\terr = set_queue_properties_from_user(&q_properties, args);\n\tif (err)\n\t\treturn err;\n\n\tpr_debug(\"Looking for gpu id 0x%x\\n\", args->gpu_id);\n\n\tmutex_lock(&p->mutex);\n\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\tpr_debug(\"Could not find gpu id 0x%x\\n\", args->gpu_id);\n\t\terr = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\tdev = pdd->dev;\n\n\tpdd = kfd_bind_process_to_device(dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = -ESRCH;\n\t\tgoto err_bind_process;\n\t}\n\n\tif (!pdd->qpd.proc_doorbells) {\n\t\terr = kfd_alloc_process_doorbells(dev->kfd, pdd);\n\t\tif (err) {\n\t\t\tpr_debug(\"failed to allocate process doorbells\\n\");\n\t\t\tgoto err_bind_process;\n\t\t}\n\t}\n\n\t \n\tif (dev->kfd->shared_resources.enable_mes &&\n\t\t\t((dev->adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK)\n\t\t\t>> AMDGPU_MES_API_VERSION_SHIFT) >= 2) {\n\t\tstruct amdgpu_bo_va_mapping *wptr_mapping;\n\t\tstruct amdgpu_vm *wptr_vm;\n\n\t\twptr_vm = drm_priv_to_vm(pdd->drm_priv);\n\t\terr = amdgpu_bo_reserve(wptr_vm->root.bo, false);\n\t\tif (err)\n\t\t\tgoto err_wptr_map_gart;\n\n\t\twptr_mapping = amdgpu_vm_bo_lookup_mapping(\n\t\t\t\twptr_vm, args->write_pointer_address >> PAGE_SHIFT);\n\t\tamdgpu_bo_unreserve(wptr_vm->root.bo);\n\t\tif (!wptr_mapping) {\n\t\t\tpr_err(\"Failed to lookup wptr bo\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_wptr_map_gart;\n\t\t}\n\n\t\twptr_bo = wptr_mapping->bo_va->base.bo;\n\t\tif (wptr_bo->tbo.base.size > PAGE_SIZE) {\n\t\t\tpr_err(\"Requested GART mapping for wptr bo larger than one page\\n\");\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_wptr_map_gart;\n\t\t}\n\n\t\terr = amdgpu_amdkfd_map_gtt_bo_to_gart(dev->adev, wptr_bo);\n\t\tif (err) {\n\t\t\tpr_err(\"Failed to map wptr bo to GART\\n\");\n\t\t\tgoto err_wptr_map_gart;\n\t\t}\n\t}\n\n\tpr_debug(\"Creating queue for PASID 0x%x on gpu 0x%x\\n\",\n\t\t\tp->pasid,\n\t\t\tdev->id);\n\n\terr = pqm_create_queue(&p->pqm, dev, filep, &q_properties, &queue_id, wptr_bo,\n\t\t\tNULL, NULL, NULL, &doorbell_offset_in_process);\n\tif (err != 0)\n\t\tgoto err_create_queue;\n\n\targs->queue_id = queue_id;\n\n\n\t \n\targs->doorbell_offset = KFD_MMAP_TYPE_DOORBELL;\n\targs->doorbell_offset |= KFD_MMAP_GPU_ID(args->gpu_id);\n\tif (KFD_IS_SOC15(dev))\n\t\t \n\t\targs->doorbell_offset |= doorbell_offset_in_process;\n\n\tmutex_unlock(&p->mutex);\n\n\tpr_debug(\"Queue id %d was created successfully\\n\", args->queue_id);\n\n\tpr_debug(\"Ring buffer address == 0x%016llX\\n\",\n\t\t\targs->ring_base_address);\n\n\tpr_debug(\"Read ptr address    == 0x%016llX\\n\",\n\t\t\targs->read_pointer_address);\n\n\tpr_debug(\"Write ptr address   == 0x%016llX\\n\",\n\t\t\targs->write_pointer_address);\n\n\tkfd_dbg_ev_raise(KFD_EC_MASK(EC_QUEUE_NEW), p, dev, queue_id, false, NULL, 0);\n\treturn 0;\n\nerr_create_queue:\n\tif (wptr_bo)\n\t\tamdgpu_amdkfd_free_gtt_mem(dev->adev, wptr_bo);\nerr_wptr_map_gart:\nerr_bind_process:\nerr_pdd:\n\tmutex_unlock(&p->mutex);\n\treturn err;\n}\n\nstatic int kfd_ioctl_destroy_queue(struct file *filp, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tint retval;\n\tstruct kfd_ioctl_destroy_queue_args *args = data;\n\n\tpr_debug(\"Destroying queue id %d for pasid 0x%x\\n\",\n\t\t\t\targs->queue_id,\n\t\t\t\tp->pasid);\n\n\tmutex_lock(&p->mutex);\n\n\tretval = pqm_destroy_queue(&p->pqm, args->queue_id);\n\n\tmutex_unlock(&p->mutex);\n\treturn retval;\n}\n\nstatic int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tint retval;\n\tstruct kfd_ioctl_update_queue_args *args = data;\n\tstruct queue_properties properties;\n\n\t \n\tif ((args->queue_percentage & 0xFF) > KFD_MAX_QUEUE_PERCENTAGE) {\n\t\tpr_err(\"Queue percentage must be between 0 to KFD_MAX_QUEUE_PERCENTAGE\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->queue_priority > KFD_MAX_QUEUE_PRIORITY) {\n\t\tpr_err(\"Queue priority must be between 0 to KFD_MAX_QUEUE_PRIORITY\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((args->ring_base_address) &&\n\t\t(!access_ok((const void __user *) args->ring_base_address,\n\t\t\tsizeof(uint64_t)))) {\n\t\tpr_err(\"Can't access ring base address\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!is_power_of_2(args->ring_size) && (args->ring_size != 0)) {\n\t\tpr_err(\"Ring size must be a power of 2 or 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tproperties.queue_address = args->ring_base_address;\n\tproperties.queue_size = args->ring_size;\n\tproperties.queue_percent = args->queue_percentage & 0xFF;\n\t \n\tproperties.pm4_target_xcc = (args->queue_percentage >> 8) & 0xFF;\n\tproperties.priority = args->queue_priority;\n\n\tpr_debug(\"Updating queue id %d for pasid 0x%x\\n\",\n\t\t\targs->queue_id, p->pasid);\n\n\tmutex_lock(&p->mutex);\n\n\tretval = pqm_update_queue_properties(&p->pqm, args->queue_id, &properties);\n\n\tmutex_unlock(&p->mutex);\n\n\treturn retval;\n}\n\nstatic int kfd_ioctl_set_cu_mask(struct file *filp, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tint retval;\n\tconst int max_num_cus = 1024;\n\tstruct kfd_ioctl_set_cu_mask_args *args = data;\n\tstruct mqd_update_info minfo = {0};\n\tuint32_t __user *cu_mask_ptr = (uint32_t __user *)args->cu_mask_ptr;\n\tsize_t cu_mask_size = sizeof(uint32_t) * (args->num_cu_mask / 32);\n\n\tif ((args->num_cu_mask % 32) != 0) {\n\t\tpr_debug(\"num_cu_mask 0x%x must be a multiple of 32\",\n\t\t\t\targs->num_cu_mask);\n\t\treturn -EINVAL;\n\t}\n\n\tminfo.cu_mask.count = args->num_cu_mask;\n\tif (minfo.cu_mask.count == 0) {\n\t\tpr_debug(\"CU mask cannot be 0\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (minfo.cu_mask.count > max_num_cus) {\n\t\tpr_debug(\"CU mask cannot be greater than 1024 bits\");\n\t\tminfo.cu_mask.count = max_num_cus;\n\t\tcu_mask_size = sizeof(uint32_t) * (max_num_cus/32);\n\t}\n\n\tminfo.cu_mask.ptr = kzalloc(cu_mask_size, GFP_KERNEL);\n\tif (!minfo.cu_mask.ptr)\n\t\treturn -ENOMEM;\n\n\tretval = copy_from_user(minfo.cu_mask.ptr, cu_mask_ptr, cu_mask_size);\n\tif (retval) {\n\t\tpr_debug(\"Could not copy CU mask from userspace\");\n\t\tretval = -EFAULT;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&p->mutex);\n\n\tretval = pqm_update_mqd(&p->pqm, args->queue_id, &minfo);\n\n\tmutex_unlock(&p->mutex);\n\nout:\n\tkfree(minfo.cu_mask.ptr);\n\treturn retval;\n}\n\nstatic int kfd_ioctl_get_queue_wave_state(struct file *filep,\n\t\t\t\t\t  struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_queue_wave_state_args *args = data;\n\tint r;\n\n\tmutex_lock(&p->mutex);\n\n\tr = pqm_get_wave_state(&p->pqm, args->queue_id,\n\t\t\t       (void __user *)args->ctl_stack_address,\n\t\t\t       &args->ctl_stack_used_size,\n\t\t\t       &args->save_area_used_size);\n\n\tmutex_unlock(&p->mutex);\n\n\treturn r;\n}\n\nstatic int kfd_ioctl_set_memory_policy(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_set_memory_policy_args *args = data;\n\tint err = 0;\n\tstruct kfd_process_device *pdd;\n\tenum cache_policy default_policy, alternate_policy;\n\n\tif (args->default_policy != KFD_IOC_CACHE_POLICY_COHERENT\n\t    && args->default_policy != KFD_IOC_CACHE_POLICY_NONCOHERENT) {\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->alternate_policy != KFD_IOC_CACHE_POLICY_COHERENT\n\t    && args->alternate_policy != KFD_IOC_CACHE_POLICY_NONCOHERENT) {\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\tpr_debug(\"Could not find gpu id 0x%x\\n\", args->gpu_id);\n\t\terr = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\n\tpdd = kfd_bind_process_to_device(pdd->dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tdefault_policy = (args->default_policy == KFD_IOC_CACHE_POLICY_COHERENT)\n\t\t\t ? cache_policy_coherent : cache_policy_noncoherent;\n\n\talternate_policy =\n\t\t(args->alternate_policy == KFD_IOC_CACHE_POLICY_COHERENT)\n\t\t   ? cache_policy_coherent : cache_policy_noncoherent;\n\n\tif (!pdd->dev->dqm->ops.set_cache_memory_policy(pdd->dev->dqm,\n\t\t\t\t&pdd->qpd,\n\t\t\t\tdefault_policy,\n\t\t\t\talternate_policy,\n\t\t\t\t(void __user *)args->alternate_aperture_base,\n\t\t\t\targs->alternate_aperture_size))\n\t\terr = -EINVAL;\n\nout:\nerr_pdd:\n\tmutex_unlock(&p->mutex);\n\n\treturn err;\n}\n\nstatic int kfd_ioctl_set_trap_handler(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_set_trap_handler_args *args = data;\n\tint err = 0;\n\tstruct kfd_process_device *pdd;\n\n\tmutex_lock(&p->mutex);\n\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\terr = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\n\tpdd = kfd_bind_process_to_device(pdd->dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tkfd_process_set_trap_handler(&pdd->qpd, args->tba_addr, args->tma_addr);\n\nout:\nerr_pdd:\n\tmutex_unlock(&p->mutex);\n\n\treturn err;\n}\n\nstatic int kfd_ioctl_dbg_register(struct file *filep,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\n\nstatic int kfd_ioctl_dbg_unregister(struct file *filep,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\n\nstatic int kfd_ioctl_dbg_address_watch(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\n\n \nstatic int kfd_ioctl_dbg_wave_control(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\n\nstatic int kfd_ioctl_get_clock_counters(struct file *filep,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_clock_counters_args *args = data;\n\tstruct kfd_process_device *pdd;\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tmutex_unlock(&p->mutex);\n\tif (pdd)\n\t\t \n\t\targs->gpu_clock_counter = amdgpu_amdkfd_get_gpu_clock_counter(pdd->dev->adev);\n\telse\n\t\t \n\t\targs->gpu_clock_counter = 0;\n\n\t \n\targs->cpu_clock_counter = ktime_get_raw_ns();\n\targs->system_clock_counter = ktime_get_boottime_ns();\n\n\t \n\targs->system_clock_freq = 1000000000;\n\n\treturn 0;\n}\n\n\nstatic int kfd_ioctl_get_process_apertures(struct file *filp,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_process_apertures_args *args = data;\n\tstruct kfd_process_device_apertures *pAperture;\n\tint i;\n\n\tdev_dbg(kfd_device, \"get apertures for PASID 0x%x\", p->pasid);\n\n\targs->num_of_nodes = 0;\n\n\tmutex_lock(&p->mutex);\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tpAperture =\n\t\t\t&args->process_apertures[args->num_of_nodes];\n\t\tpAperture->gpu_id = pdd->dev->id;\n\t\tpAperture->lds_base = pdd->lds_base;\n\t\tpAperture->lds_limit = pdd->lds_limit;\n\t\tpAperture->gpuvm_base = pdd->gpuvm_base;\n\t\tpAperture->gpuvm_limit = pdd->gpuvm_limit;\n\t\tpAperture->scratch_base = pdd->scratch_base;\n\t\tpAperture->scratch_limit = pdd->scratch_limit;\n\n\t\tdev_dbg(kfd_device,\n\t\t\t\"node id %u\\n\", args->num_of_nodes);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpu id %u\\n\", pdd->dev->id);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"lds_base %llX\\n\", pdd->lds_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"lds_limit %llX\\n\", pdd->lds_limit);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpuvm_base %llX\\n\", pdd->gpuvm_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpuvm_limit %llX\\n\", pdd->gpuvm_limit);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"scratch_base %llX\\n\", pdd->scratch_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"scratch_limit %llX\\n\", pdd->scratch_limit);\n\n\t\tif (++args->num_of_nodes >= NUM_OF_SUPPORTED_GPUS)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&p->mutex);\n\n\treturn 0;\n}\n\nstatic int kfd_ioctl_get_process_apertures_new(struct file *filp,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_process_apertures_new_args *args = data;\n\tstruct kfd_process_device_apertures *pa;\n\tint ret;\n\tint i;\n\n\tdev_dbg(kfd_device, \"get apertures for PASID 0x%x\", p->pasid);\n\n\tif (args->num_of_nodes == 0) {\n\t\t \n\t\tmutex_lock(&p->mutex);\n\t\targs->num_of_nodes = p->n_pdds;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tpa = kzalloc((sizeof(struct kfd_process_device_apertures) *\n\t\t\t\targs->num_of_nodes), GFP_KERNEL);\n\tif (!pa)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&p->mutex);\n\n\tif (!p->n_pdds) {\n\t\targs->num_of_nodes = 0;\n\t\tkfree(pa);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tfor (i = 0; i < min(p->n_pdds, args->num_of_nodes); i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tpa[i].gpu_id = pdd->dev->id;\n\t\tpa[i].lds_base = pdd->lds_base;\n\t\tpa[i].lds_limit = pdd->lds_limit;\n\t\tpa[i].gpuvm_base = pdd->gpuvm_base;\n\t\tpa[i].gpuvm_limit = pdd->gpuvm_limit;\n\t\tpa[i].scratch_base = pdd->scratch_base;\n\t\tpa[i].scratch_limit = pdd->scratch_limit;\n\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpu id %u\\n\", pdd->dev->id);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"lds_base %llX\\n\", pdd->lds_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"lds_limit %llX\\n\", pdd->lds_limit);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpuvm_base %llX\\n\", pdd->gpuvm_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"gpuvm_limit %llX\\n\", pdd->gpuvm_limit);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"scratch_base %llX\\n\", pdd->scratch_base);\n\t\tdev_dbg(kfd_device,\n\t\t\t\"scratch_limit %llX\\n\", pdd->scratch_limit);\n\t}\n\tmutex_unlock(&p->mutex);\n\n\targs->num_of_nodes = i;\n\tret = copy_to_user(\n\t\t\t(void __user *)args->kfd_process_device_apertures_ptr,\n\t\t\tpa,\n\t\t\t(i * sizeof(struct kfd_process_device_apertures)));\n\tkfree(pa);\n\treturn ret ? -EFAULT : 0;\n\nout_unlock:\n\tmutex_unlock(&p->mutex);\n\treturn 0;\n}\n\nstatic int kfd_ioctl_create_event(struct file *filp, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_create_event_args *args = data;\n\tint err;\n\n\t \n\tif (args->event_page_offset) {\n\t\tmutex_lock(&p->mutex);\n\t\terr = kfd_kmap_event_page(p, args->event_page_offset);\n\t\tmutex_unlock(&p->mutex);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = kfd_event_create(filp, p, args->event_type,\n\t\t\t\targs->auto_reset != 0, args->node_id,\n\t\t\t\t&args->event_id, &args->event_trigger_data,\n\t\t\t\t&args->event_page_offset,\n\t\t\t\t&args->event_slot_index);\n\n\tpr_debug(\"Created event (id:0x%08x) (%s)\\n\", args->event_id, __func__);\n\treturn err;\n}\n\nstatic int kfd_ioctl_destroy_event(struct file *filp, struct kfd_process *p,\n\t\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_destroy_event_args *args = data;\n\n\treturn kfd_event_destroy(p, args->event_id);\n}\n\nstatic int kfd_ioctl_set_event(struct file *filp, struct kfd_process *p,\n\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_set_event_args *args = data;\n\n\treturn kfd_set_event(p, args->event_id);\n}\n\nstatic int kfd_ioctl_reset_event(struct file *filp, struct kfd_process *p,\n\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_reset_event_args *args = data;\n\n\treturn kfd_reset_event(p, args->event_id);\n}\n\nstatic int kfd_ioctl_wait_events(struct file *filp, struct kfd_process *p,\n\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_wait_events_args *args = data;\n\n\treturn kfd_wait_on_events(p, args->num_events,\n\t\t\t(void __user *)args->events_ptr,\n\t\t\t(args->wait_for_all != 0),\n\t\t\t&args->timeout, &args->wait_result);\n}\nstatic int kfd_ioctl_set_scratch_backing_va(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_set_scratch_backing_va_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tstruct kfd_node *dev;\n\tlong err;\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\terr = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\tdev = pdd->dev;\n\n\tpdd = kfd_bind_process_to_device(dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = PTR_ERR(pdd);\n\t\tgoto bind_process_to_device_fail;\n\t}\n\n\tpdd->qpd.sh_hidden_private_base = args->va_addr;\n\n\tmutex_unlock(&p->mutex);\n\n\tif (dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS &&\n\t    pdd->qpd.vmid != 0 && dev->kfd2kgd->set_scratch_backing_va)\n\t\tdev->kfd2kgd->set_scratch_backing_va(\n\t\t\tdev->adev, args->va_addr, pdd->qpd.vmid);\n\n\treturn 0;\n\nbind_process_to_device_fail:\nerr_pdd:\n\tmutex_unlock(&p->mutex);\n\treturn err;\n}\n\nstatic int kfd_ioctl_get_tile_config(struct file *filep,\n\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_tile_config_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tstruct tile_config config;\n\tint err = 0;\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tmutex_unlock(&p->mutex);\n\tif (!pdd)\n\t\treturn -EINVAL;\n\n\tamdgpu_amdkfd_get_tile_config(pdd->dev->adev, &config);\n\n\targs->gb_addr_config = config.gb_addr_config;\n\targs->num_banks = config.num_banks;\n\targs->num_ranks = config.num_ranks;\n\n\tif (args->num_tile_configs > config.num_tile_configs)\n\t\targs->num_tile_configs = config.num_tile_configs;\n\terr = copy_to_user((void __user *)args->tile_config_ptr,\n\t\t\tconfig.tile_config_ptr,\n\t\t\targs->num_tile_configs * sizeof(uint32_t));\n\tif (err) {\n\t\targs->num_tile_configs = 0;\n\t\treturn -EFAULT;\n\t}\n\n\tif (args->num_macro_tile_configs > config.num_macro_tile_configs)\n\t\targs->num_macro_tile_configs =\n\t\t\t\tconfig.num_macro_tile_configs;\n\terr = copy_to_user((void __user *)args->macro_tile_config_ptr,\n\t\t\tconfig.macro_tile_config_ptr,\n\t\t\targs->num_macro_tile_configs * sizeof(uint32_t));\n\tif (err) {\n\t\targs->num_macro_tile_configs = 0;\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int kfd_ioctl_acquire_vm(struct file *filep, struct kfd_process *p,\n\t\t\t\tvoid *data)\n{\n\tstruct kfd_ioctl_acquire_vm_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tstruct file *drm_file;\n\tint ret;\n\n\tdrm_file = fget(args->drm_fd);\n\tif (!drm_file)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\tret = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\n\tif (pdd->drm_file) {\n\t\tret = pdd->drm_file == drm_file ? 0 : -EBUSY;\n\t\tgoto err_drm_file;\n\t}\n\n\tret = kfd_process_device_init_vm(pdd, drm_file);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\t \n\tmutex_unlock(&p->mutex);\n\n\treturn 0;\n\nerr_unlock:\nerr_pdd:\nerr_drm_file:\n\tmutex_unlock(&p->mutex);\n\tfput(drm_file);\n\treturn ret;\n}\n\nbool kfd_dev_is_large_bar(struct kfd_node *dev)\n{\n\tif (debug_largebar) {\n\t\tpr_debug(\"Simulate large-bar allocation on non large-bar machine\\n\");\n\t\treturn true;\n\t}\n\n\tif (dev->local_mem_info.local_mem_size_private == 0 &&\n\t    dev->local_mem_info.local_mem_size_public > 0)\n\t\treturn true;\n\n\tif (dev->local_mem_info.local_mem_size_public == 0 &&\n\t    dev->kfd->adev->gmc.is_app_apu) {\n\t\tpr_debug(\"APP APU, Consider like a large bar system\\n\");\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int kfd_ioctl_get_available_memory(struct file *filep,\n\t\t\t\t\t  struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_available_memory_args *args = data;\n\tstruct kfd_process_device *pdd = kfd_lock_pdd_by_id(p, args->gpu_id);\n\n\tif (!pdd)\n\t\treturn -EINVAL;\n\targs->available = amdgpu_amdkfd_get_available_memory(pdd->dev->adev,\n\t\t\t\t\t\t\tpdd->dev->node_id);\n\tkfd_unlock_pdd(pdd);\n\treturn 0;\n}\n\nstatic int kfd_ioctl_alloc_memory_of_gpu(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_alloc_memory_of_gpu_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tvoid *mem;\n\tstruct kfd_node *dev;\n\tint idr_handle;\n\tlong err;\n\tuint64_t offset = args->mmap_offset;\n\tuint32_t flags = args->flags;\n\n\tif (args->size == 0)\n\t\treturn -EINVAL;\n\n#if IS_ENABLED(CONFIG_HSA_AMD_SVM)\n\t \n\tsvm_range_list_lock_and_flush_work(&p->svms, current->mm);\n\tmutex_lock(&p->svms.lock);\n\tmmap_write_unlock(current->mm);\n\tif (interval_tree_iter_first(&p->svms.objects,\n\t\t\t\t     args->va_addr >> PAGE_SHIFT,\n\t\t\t\t     (args->va_addr + args->size - 1) >> PAGE_SHIFT)) {\n\t\tpr_err(\"Address: 0x%llx already allocated by SVM\\n\",\n\t\t\targs->va_addr);\n\t\tmutex_unlock(&p->svms.lock);\n\t\treturn -EADDRINUSE;\n\t}\n\n\t \n\tif ((flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) &&\n\t    interval_tree_iter_first(&p->svms.objects,\n\t\t\t\t     args->mmap_offset >> PAGE_SHIFT,\n\t\t\t\t     (args->mmap_offset  + args->size - 1) >> PAGE_SHIFT)) {\n\t\tpr_err(\"User Buffer Address: 0x%llx already allocated by SVM\\n\",\n\t\t\targs->mmap_offset);\n\t\tmutex_unlock(&p->svms.lock);\n\t\treturn -EADDRINUSE;\n\t}\n\n\tmutex_unlock(&p->svms.lock);\n#endif\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\terr = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\n\tdev = pdd->dev;\n\n\tif ((flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) &&\n\t\t(flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) &&\n\t\t!kfd_dev_is_large_bar(dev)) {\n\t\tpr_err(\"Alloc host visible vram on small bar is not allowed\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_large_bar;\n\t}\n\n\tpdd = kfd_bind_process_to_device(dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = PTR_ERR(pdd);\n\t\tgoto err_unlock;\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {\n\t\tif (args->size != kfd_doorbell_process_slice(dev->kfd)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\toffset = kfd_get_process_doorbells(pdd);\n\t\tif (!offset) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_unlock;\n\t\t}\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP) {\n\t\tif (args->size != PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_unlock;\n\t\t}\n\t\toffset = dev->adev->rmmio_remap.bus_addr;\n\t\tif (!offset) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_unlock;\n\t\t}\n\t}\n\n\terr = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tdev->adev, args->va_addr, args->size,\n\t\tpdd->drm_priv, (struct kgd_mem **) &mem, &offset,\n\t\tflags, false);\n\n\tif (err)\n\t\tgoto err_unlock;\n\n\tidr_handle = kfd_process_device_create_obj_handle(pdd, mem);\n\tif (idr_handle < 0) {\n\t\terr = -EFAULT;\n\t\tgoto err_free;\n\t}\n\n\t \n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tuint64_t size = args->size;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM)\n\t\t\tsize >>= 1;\n\t\tWRITE_ONCE(pdd->vram_usage, pdd->vram_usage + PAGE_ALIGN(size));\n\t}\n\n\tmutex_unlock(&p->mutex);\n\n\targs->handle = MAKE_HANDLE(args->gpu_id, idr_handle);\n\targs->mmap_offset = offset;\n\n\t \n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)\n\t\targs->mmap_offset = KFD_MMAP_TYPE_MMIO\n\t\t\t\t\t| KFD_MMAP_GPU_ID(args->gpu_id);\n\n\treturn 0;\n\nerr_free:\n\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(dev->adev, (struct kgd_mem *)mem,\n\t\t\t\t\t       pdd->drm_priv, NULL);\nerr_unlock:\nerr_pdd:\nerr_large_bar:\n\tmutex_unlock(&p->mutex);\n\treturn err;\n}\n\nstatic int kfd_ioctl_free_memory_of_gpu(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_free_memory_of_gpu_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tvoid *mem;\n\tint ret;\n\tuint64_t size = 0;\n\n\tmutex_lock(&p->mutex);\n\t \n\tif (p->signal_handle && (p->signal_handle == args->handle)) {\n\t\tpr_err(\"Free signal BO is not allowed\\n\");\n\t\tret = -EPERM;\n\t\tgoto err_unlock;\n\t}\n\n\tpdd = kfd_process_device_data_by_id(p, GET_GPU_ID(args->handle));\n\tif (!pdd) {\n\t\tpr_err(\"Process device data doesn't exist\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_pdd;\n\t}\n\n\tmem = kfd_process_device_translate_handle(\n\t\tpdd, GET_IDR_HANDLE(args->handle));\n\tif (!mem) {\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tret = amdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->adev,\n\t\t\t\t(struct kgd_mem *)mem, pdd->drm_priv, &size);\n\n\t \n\tif (!ret)\n\t\tkfd_process_device_remove_obj_handle(\n\t\t\tpdd, GET_IDR_HANDLE(args->handle));\n\n\tWRITE_ONCE(pdd->vram_usage, pdd->vram_usage - size);\n\nerr_unlock:\nerr_pdd:\n\tmutex_unlock(&p->mutex);\n\treturn ret;\n}\n\nstatic int kfd_ioctl_map_memory_to_gpu(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_map_memory_to_gpu_args *args = data;\n\tstruct kfd_process_device *pdd, *peer_pdd;\n\tvoid *mem;\n\tstruct kfd_node *dev;\n\tlong err = 0;\n\tint i;\n\tuint32_t *devices_arr = NULL;\n\n\tif (!args->n_devices) {\n\t\tpr_debug(\"Device IDs array empty\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (args->n_success > args->n_devices) {\n\t\tpr_debug(\"n_success exceeds n_devices\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdevices_arr = kmalloc_array(args->n_devices, sizeof(*devices_arr),\n\t\t\t\t    GFP_KERNEL);\n\tif (!devices_arr)\n\t\treturn -ENOMEM;\n\n\terr = copy_from_user(devices_arr,\n\t\t\t     (void __user *)args->device_ids_array_ptr,\n\t\t\t     args->n_devices * sizeof(*devices_arr));\n\tif (err != 0) {\n\t\terr = -EFAULT;\n\t\tgoto copy_from_user_failed;\n\t}\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, GET_GPU_ID(args->handle));\n\tif (!pdd) {\n\t\terr = -EINVAL;\n\t\tgoto get_process_device_data_failed;\n\t}\n\tdev = pdd->dev;\n\n\tpdd = kfd_bind_process_to_device(dev, p);\n\tif (IS_ERR(pdd)) {\n\t\terr = PTR_ERR(pdd);\n\t\tgoto bind_process_to_device_failed;\n\t}\n\n\tmem = kfd_process_device_translate_handle(pdd,\n\t\t\t\t\t\tGET_IDR_HANDLE(args->handle));\n\tif (!mem) {\n\t\terr = -ENOMEM;\n\t\tgoto get_mem_obj_from_handle_failed;\n\t}\n\n\tfor (i = args->n_success; i < args->n_devices; i++) {\n\t\tpeer_pdd = kfd_process_device_data_by_id(p, devices_arr[i]);\n\t\tif (!peer_pdd) {\n\t\t\tpr_debug(\"Getting device by id failed for 0x%x\\n\",\n\t\t\t\t devices_arr[i]);\n\t\t\terr = -EINVAL;\n\t\t\tgoto get_mem_obj_from_handle_failed;\n\t\t}\n\n\t\tpeer_pdd = kfd_bind_process_to_device(peer_pdd->dev, p);\n\t\tif (IS_ERR(peer_pdd)) {\n\t\t\terr = PTR_ERR(peer_pdd);\n\t\t\tgoto get_mem_obj_from_handle_failed;\n\t\t}\n\n\t\terr = amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\t\tpeer_pdd->dev->adev, (struct kgd_mem *)mem,\n\t\t\tpeer_pdd->drm_priv);\n\t\tif (err) {\n\t\t\tstruct pci_dev *pdev = peer_pdd->dev->adev->pdev;\n\n\t\t\tdev_err(dev->adev->dev,\n\t\t\t       \"Failed to map peer:%04x:%02x:%02x.%d mem_domain:%d\\n\",\n\t\t\t       pci_domain_nr(pdev->bus),\n\t\t\t       pdev->bus->number,\n\t\t\t       PCI_SLOT(pdev->devfn),\n\t\t\t       PCI_FUNC(pdev->devfn),\n\t\t\t       ((struct kgd_mem *)mem)->domain);\n\t\t\tgoto map_memory_to_gpu_failed;\n\t\t}\n\t\targs->n_success = i+1;\n\t}\n\n\terr = amdgpu_amdkfd_gpuvm_sync_memory(dev->adev, (struct kgd_mem *) mem, true);\n\tif (err) {\n\t\tpr_debug(\"Sync memory failed, wait interrupted by user signal\\n\");\n\t\tgoto sync_memory_failed;\n\t}\n\n\tmutex_unlock(&p->mutex);\n\n\t \n\tfor (i = 0; i < args->n_devices; i++) {\n\t\tpeer_pdd = kfd_process_device_data_by_id(p, devices_arr[i]);\n\t\tif (WARN_ON_ONCE(!peer_pdd))\n\t\t\tcontinue;\n\t\tkfd_flush_tlb(peer_pdd, TLB_FLUSH_LEGACY);\n\t}\n\tkfree(devices_arr);\n\n\treturn err;\n\nget_process_device_data_failed:\nbind_process_to_device_failed:\nget_mem_obj_from_handle_failed:\nmap_memory_to_gpu_failed:\nsync_memory_failed:\n\tmutex_unlock(&p->mutex);\ncopy_from_user_failed:\n\tkfree(devices_arr);\n\n\treturn err;\n}\n\nstatic int kfd_ioctl_unmap_memory_from_gpu(struct file *filep,\n\t\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_unmap_memory_from_gpu_args *args = data;\n\tstruct kfd_process_device *pdd, *peer_pdd;\n\tvoid *mem;\n\tlong err = 0;\n\tuint32_t *devices_arr = NULL, i;\n\tbool flush_tlb;\n\n\tif (!args->n_devices) {\n\t\tpr_debug(\"Device IDs array empty\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (args->n_success > args->n_devices) {\n\t\tpr_debug(\"n_success exceeds n_devices\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdevices_arr = kmalloc_array(args->n_devices, sizeof(*devices_arr),\n\t\t\t\t    GFP_KERNEL);\n\tif (!devices_arr)\n\t\treturn -ENOMEM;\n\n\terr = copy_from_user(devices_arr,\n\t\t\t     (void __user *)args->device_ids_array_ptr,\n\t\t\t     args->n_devices * sizeof(*devices_arr));\n\tif (err != 0) {\n\t\terr = -EFAULT;\n\t\tgoto copy_from_user_failed;\n\t}\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, GET_GPU_ID(args->handle));\n\tif (!pdd) {\n\t\terr = -EINVAL;\n\t\tgoto bind_process_to_device_failed;\n\t}\n\n\tmem = kfd_process_device_translate_handle(pdd,\n\t\t\t\t\t\tGET_IDR_HANDLE(args->handle));\n\tif (!mem) {\n\t\terr = -ENOMEM;\n\t\tgoto get_mem_obj_from_handle_failed;\n\t}\n\n\tfor (i = args->n_success; i < args->n_devices; i++) {\n\t\tpeer_pdd = kfd_process_device_data_by_id(p, devices_arr[i]);\n\t\tif (!peer_pdd) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto get_mem_obj_from_handle_failed;\n\t\t}\n\t\terr = amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\t\tpeer_pdd->dev->adev, (struct kgd_mem *)mem, peer_pdd->drm_priv);\n\t\tif (err) {\n\t\t\tpr_err(\"Failed to unmap from gpu %d/%d\\n\",\n\t\t\t       i, args->n_devices);\n\t\t\tgoto unmap_memory_from_gpu_failed;\n\t\t}\n\t\targs->n_success = i+1;\n\t}\n\n\tflush_tlb = kfd_flush_tlb_after_unmap(pdd->dev->kfd);\n\tif (flush_tlb) {\n\t\terr = amdgpu_amdkfd_gpuvm_sync_memory(pdd->dev->adev,\n\t\t\t\t(struct kgd_mem *) mem, true);\n\t\tif (err) {\n\t\t\tpr_debug(\"Sync memory failed, wait interrupted by user signal\\n\");\n\t\t\tgoto sync_memory_failed;\n\t\t}\n\t}\n\tmutex_unlock(&p->mutex);\n\n\tif (flush_tlb) {\n\t\t \n\t\tfor (i = 0; i < args->n_devices; i++) {\n\t\t\tpeer_pdd = kfd_process_device_data_by_id(p, devices_arr[i]);\n\t\t\tif (WARN_ON_ONCE(!peer_pdd))\n\t\t\t\tcontinue;\n\t\t\tkfd_flush_tlb(peer_pdd, TLB_FLUSH_HEAVYWEIGHT);\n\t\t}\n\t}\n\tkfree(devices_arr);\n\n\treturn 0;\n\nbind_process_to_device_failed:\nget_mem_obj_from_handle_failed:\nunmap_memory_from_gpu_failed:\nsync_memory_failed:\n\tmutex_unlock(&p->mutex);\ncopy_from_user_failed:\n\tkfree(devices_arr);\n\treturn err;\n}\n\nstatic int kfd_ioctl_alloc_queue_gws(struct file *filep,\n\t\tstruct kfd_process *p, void *data)\n{\n\tint retval;\n\tstruct kfd_ioctl_alloc_queue_gws_args *args = data;\n\tstruct queue *q;\n\tstruct kfd_node *dev;\n\n\tmutex_lock(&p->mutex);\n\tq = pqm_get_user_queue(&p->pqm, args->queue_id);\n\n\tif (q) {\n\t\tdev = q->device;\n\t} else {\n\t\tretval = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!dev->gws) {\n\t\tretval = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (dev->dqm->sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tretval = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (p->debug_trap_enabled && (!kfd_dbg_has_gws_support(dev) ||\n\t\t\t\t      kfd_dbg_has_cwsr_workaround(dev))) {\n\t\tretval = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tretval = pqm_set_gws(&p->pqm, args->queue_id, args->num_gws ? dev->gws : NULL);\n\tmutex_unlock(&p->mutex);\n\n\targs->first_gws = 0;\n\treturn retval;\n\nout_unlock:\n\tmutex_unlock(&p->mutex);\n\treturn retval;\n}\n\nstatic int kfd_ioctl_get_dmabuf_info(struct file *filep,\n\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_get_dmabuf_info_args *args = data;\n\tstruct kfd_node *dev = NULL;\n\tstruct amdgpu_device *dmabuf_adev;\n\tvoid *metadata_buffer = NULL;\n\tuint32_t flags;\n\tint8_t xcp_id;\n\tunsigned int i;\n\tint r;\n\n\t \n\tfor (i = 0; kfd_topology_enum_kfd_devices(i, &dev) == 0; i++)\n\t\tif (dev)\n\t\t\tbreak;\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tif (args->metadata_ptr) {\n\t\tmetadata_buffer = kzalloc(args->metadata_size, GFP_KERNEL);\n\t\tif (!metadata_buffer)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tr = amdgpu_amdkfd_get_dmabuf_info(dev->adev, args->dmabuf_fd,\n\t\t\t\t\t  &dmabuf_adev, &args->size,\n\t\t\t\t\t  metadata_buffer, args->metadata_size,\n\t\t\t\t\t  &args->metadata_size, &flags, &xcp_id);\n\tif (r)\n\t\tgoto exit;\n\n\tif (xcp_id >= 0)\n\t\targs->gpu_id = dmabuf_adev->kfd.dev->nodes[xcp_id]->id;\n\telse\n\t\targs->gpu_id = dmabuf_adev->kfd.dev->nodes[0]->id;\n\targs->flags = flags;\n\n\t \n\tif (metadata_buffer) {\n\t\tr = copy_to_user((void __user *)args->metadata_ptr,\n\t\t\t\t metadata_buffer, args->metadata_size);\n\t\tif (r != 0)\n\t\t\tr = -EFAULT;\n\t}\n\nexit:\n\tkfree(metadata_buffer);\n\n\treturn r;\n}\n\nstatic int kfd_ioctl_import_dmabuf(struct file *filep,\n\t\t\t\t   struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_import_dmabuf_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_buf *dmabuf;\n\tint idr_handle;\n\tuint64_t size;\n\tvoid *mem;\n\tint r;\n\n\tdmabuf = dma_buf_get(args->dmabuf_fd);\n\tif (IS_ERR(dmabuf))\n\t\treturn PTR_ERR(dmabuf);\n\n\tmutex_lock(&p->mutex);\n\tpdd = kfd_process_device_data_by_id(p, args->gpu_id);\n\tif (!pdd) {\n\t\tr = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tpdd = kfd_bind_process_to_device(pdd->dev, p);\n\tif (IS_ERR(pdd)) {\n\t\tr = PTR_ERR(pdd);\n\t\tgoto err_unlock;\n\t}\n\n\tr = amdgpu_amdkfd_gpuvm_import_dmabuf(pdd->dev->adev, dmabuf,\n\t\t\t\t\t      args->va_addr, pdd->drm_priv,\n\t\t\t\t\t      (struct kgd_mem **)&mem, &size,\n\t\t\t\t\t      NULL);\n\tif (r)\n\t\tgoto err_unlock;\n\n\tidr_handle = kfd_process_device_create_obj_handle(pdd, mem);\n\tif (idr_handle < 0) {\n\t\tr = -EFAULT;\n\t\tgoto err_free;\n\t}\n\n\tmutex_unlock(&p->mutex);\n\tdma_buf_put(dmabuf);\n\n\targs->handle = MAKE_HANDLE(args->gpu_id, idr_handle);\n\n\treturn 0;\n\nerr_free:\n\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->adev, (struct kgd_mem *)mem,\n\t\t\t\t\t       pdd->drm_priv, NULL);\nerr_unlock:\n\tmutex_unlock(&p->mutex);\n\tdma_buf_put(dmabuf);\n\treturn r;\n}\n\nstatic int kfd_ioctl_export_dmabuf(struct file *filep,\n\t\t\t\t   struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_export_dmabuf_args *args = data;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_buf *dmabuf;\n\tstruct kfd_node *dev;\n\tvoid *mem;\n\tint ret = 0;\n\n\tdev = kfd_device_by_id(GET_GPU_ID(args->handle));\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&p->mutex);\n\n\tpdd = kfd_get_process_device_data(dev, p);\n\tif (!pdd) {\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tmem = kfd_process_device_translate_handle(pdd,\n\t\t\t\t\t\tGET_IDR_HANDLE(args->handle));\n\tif (!mem) {\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tret = amdgpu_amdkfd_gpuvm_export_dmabuf(mem, &dmabuf);\n\tmutex_unlock(&p->mutex);\n\tif (ret)\n\t\tgoto err_out;\n\n\tret = dma_buf_fd(dmabuf, args->flags);\n\tif (ret < 0) {\n\t\tdma_buf_put(dmabuf);\n\t\tgoto err_out;\n\t}\n\t \n\targs->dmabuf_fd = ret;\n\n\treturn 0;\n\nerr_unlock:\n\tmutex_unlock(&p->mutex);\nerr_out:\n\treturn ret;\n}\n\n \nstatic int kfd_ioctl_smi_events(struct file *filep,\n\t\t\t\tstruct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_smi_events_args *args = data;\n\tstruct kfd_process_device *pdd;\n\n\tmutex_lock(&p->mutex);\n\n\tpdd = kfd_process_device_data_by_id(p, args->gpuid);\n\tmutex_unlock(&p->mutex);\n\tif (!pdd)\n\t\treturn -EINVAL;\n\n\treturn kfd_smi_event_open(pdd->dev, &args->anon_fd);\n}\n\n#if IS_ENABLED(CONFIG_HSA_AMD_SVM)\n\nstatic int kfd_ioctl_set_xnack_mode(struct file *filep,\n\t\t\t\t    struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_set_xnack_mode_args *args = data;\n\tint r = 0;\n\n\tmutex_lock(&p->mutex);\n\tif (args->xnack_enabled >= 0) {\n\t\tif (!list_empty(&p->pqm.queues)) {\n\t\t\tpr_debug(\"Process has user queues running\\n\");\n\t\t\tr = -EBUSY;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (p->xnack_enabled == args->xnack_enabled)\n\t\t\tgoto out_unlock;\n\n\t\tif (args->xnack_enabled && !kfd_process_xnack_mode(p, true)) {\n\t\t\tr = -EPERM;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tr = svm_range_switch_xnack_reserve_mem(p, args->xnack_enabled);\n\t} else {\n\t\targs->xnack_enabled = p->xnack_enabled;\n\t}\n\nout_unlock:\n\tmutex_unlock(&p->mutex);\n\n\treturn r;\n}\n\nstatic int kfd_ioctl_svm(struct file *filep, struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_svm_args *args = data;\n\tint r = 0;\n\n\tpr_debug(\"start 0x%llx size 0x%llx op 0x%x nattr 0x%x\\n\",\n\t\t args->start_addr, args->size, args->op, args->nattr);\n\n\tif ((args->start_addr & ~PAGE_MASK) || (args->size & ~PAGE_MASK))\n\t\treturn -EINVAL;\n\tif (!args->start_addr || !args->size)\n\t\treturn -EINVAL;\n\n\tr = svm_ioctl(p, args->op, args->start_addr, args->size, args->nattr,\n\t\t      args->attrs);\n\n\treturn r;\n}\n#else\nstatic int kfd_ioctl_set_xnack_mode(struct file *filep,\n\t\t\t\t    struct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\nstatic int kfd_ioctl_svm(struct file *filep, struct kfd_process *p, void *data)\n{\n\treturn -EPERM;\n}\n#endif\n\nstatic int criu_checkpoint_process(struct kfd_process *p,\n\t\t\t     uint8_t __user *user_priv_data,\n\t\t\t     uint64_t *priv_offset)\n{\n\tstruct kfd_criu_process_priv_data process_priv;\n\tint ret;\n\n\tmemset(&process_priv, 0, sizeof(process_priv));\n\n\tprocess_priv.version = KFD_CRIU_PRIV_VERSION;\n\t \n\tprocess_priv.xnack_mode = p->xnack_enabled ? 1 : 0;\n\n\tret = copy_to_user(user_priv_data + *priv_offset,\n\t\t\t\t&process_priv, sizeof(process_priv));\n\n\tif (ret) {\n\t\tpr_err(\"Failed to copy process information to user\\n\");\n\t\tret = -EFAULT;\n\t}\n\n\t*priv_offset += sizeof(process_priv);\n\treturn ret;\n}\n\nstatic int criu_checkpoint_devices(struct kfd_process *p,\n\t\t\t     uint32_t num_devices,\n\t\t\t     uint8_t __user *user_addr,\n\t\t\t     uint8_t __user *user_priv_data,\n\t\t\t     uint64_t *priv_offset)\n{\n\tstruct kfd_criu_device_priv_data *device_priv = NULL;\n\tstruct kfd_criu_device_bucket *device_buckets = NULL;\n\tint ret = 0, i;\n\n\tdevice_buckets = kvzalloc(num_devices * sizeof(*device_buckets), GFP_KERNEL);\n\tif (!device_buckets) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tdevice_priv = kvzalloc(num_devices * sizeof(*device_priv), GFP_KERNEL);\n\tif (!device_priv) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tfor (i = 0; i < num_devices; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tdevice_buckets[i].user_gpu_id = pdd->user_gpu_id;\n\t\tdevice_buckets[i].actual_gpu_id = pdd->dev->id;\n\n\t\t \n\t}\n\n\tret = copy_to_user(user_addr, device_buckets, num_devices * sizeof(*device_buckets));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy device information to user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tret = copy_to_user(user_priv_data + *priv_offset,\n\t\t\t   device_priv,\n\t\t\t   num_devices * sizeof(*device_priv));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy device information to user\\n\");\n\t\tret = -EFAULT;\n\t}\n\t*priv_offset += num_devices * sizeof(*device_priv);\n\nexit:\n\tkvfree(device_buckets);\n\tkvfree(device_priv);\n\treturn ret;\n}\n\nstatic uint32_t get_process_num_bos(struct kfd_process *p)\n{\n\tuint32_t num_of_bos = 0;\n\tint i;\n\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\t\tvoid *mem;\n\t\tint id;\n\n\t\tidr_for_each_entry(&pdd->alloc_idr, mem, id) {\n\t\t\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\n\t\t\tif (!kgd_mem->va || kgd_mem->va > pdd->gpuvm_base)\n\t\t\t\tnum_of_bos++;\n\t\t}\n\t}\n\treturn num_of_bos;\n}\n\nstatic int criu_get_prime_handle(struct kgd_mem *mem, int flags,\n\t\t\t\t      u32 *shared_fd)\n{\n\tstruct dma_buf *dmabuf;\n\tint ret;\n\n\tret = amdgpu_amdkfd_gpuvm_export_dmabuf(mem, &dmabuf);\n\tif (ret) {\n\t\tpr_err(\"dmabuf export failed for the BO\\n\");\n\t\treturn ret;\n\t}\n\n\tret = dma_buf_fd(dmabuf, flags);\n\tif (ret < 0) {\n\t\tpr_err(\"dmabuf create fd failed, ret:%d\\n\", ret);\n\t\tgoto out_free_dmabuf;\n\t}\n\n\t*shared_fd = ret;\n\treturn 0;\n\nout_free_dmabuf:\n\tdma_buf_put(dmabuf);\n\treturn ret;\n}\n\nstatic int criu_checkpoint_bos(struct kfd_process *p,\n\t\t\t       uint32_t num_bos,\n\t\t\t       uint8_t __user *user_bos,\n\t\t\t       uint8_t __user *user_priv_data,\n\t\t\t       uint64_t *priv_offset)\n{\n\tstruct kfd_criu_bo_bucket *bo_buckets;\n\tstruct kfd_criu_bo_priv_data *bo_privs;\n\tint ret = 0, pdd_index, bo_index = 0, id;\n\tvoid *mem;\n\n\tbo_buckets = kvzalloc(num_bos * sizeof(*bo_buckets), GFP_KERNEL);\n\tif (!bo_buckets)\n\t\treturn -ENOMEM;\n\n\tbo_privs = kvzalloc(num_bos * sizeof(*bo_privs), GFP_KERNEL);\n\tif (!bo_privs) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tfor (pdd_index = 0; pdd_index < p->n_pdds; pdd_index++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[pdd_index];\n\t\tstruct amdgpu_bo *dumper_bo;\n\t\tstruct kgd_mem *kgd_mem;\n\n\t\tidr_for_each_entry(&pdd->alloc_idr, mem, id) {\n\t\t\tstruct kfd_criu_bo_bucket *bo_bucket;\n\t\t\tstruct kfd_criu_bo_priv_data *bo_priv;\n\t\t\tint i, dev_idx = 0;\n\n\t\t\tif (!mem) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto exit;\n\t\t\t}\n\n\t\t\tkgd_mem = (struct kgd_mem *)mem;\n\t\t\tdumper_bo = kgd_mem->bo;\n\n\t\t\t \n\t\t\tif (kgd_mem->va && kgd_mem->va <= pdd->gpuvm_base)\n\t\t\t\tcontinue;\n\n\t\t\tbo_bucket = &bo_buckets[bo_index];\n\t\t\tbo_priv = &bo_privs[bo_index];\n\n\t\t\tbo_bucket->gpu_id = pdd->user_gpu_id;\n\t\t\tbo_bucket->addr = (uint64_t)kgd_mem->va;\n\t\t\tbo_bucket->size = amdgpu_bo_size(dumper_bo);\n\t\t\tbo_bucket->alloc_flags = (uint32_t)kgd_mem->alloc_flags;\n\t\t\tbo_priv->idr_handle = id;\n\n\t\t\tif (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\t\tret = amdgpu_ttm_tt_get_userptr(&dumper_bo->tbo,\n\t\t\t\t\t\t\t\t&bo_priv->user_addr);\n\t\t\t\tif (ret) {\n\t\t\t\t\tpr_err(\"Failed to obtain user address for user-pointer bo\\n\");\n\t\t\t\t\tgoto exit;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (bo_bucket->alloc_flags\n\t\t\t    & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM | KFD_IOC_ALLOC_MEM_FLAGS_GTT)) {\n\t\t\t\tret = criu_get_prime_handle(kgd_mem,\n\t\t\t\t\t\tbo_bucket->alloc_flags &\n\t\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ? DRM_RDWR : 0,\n\t\t\t\t\t\t&bo_bucket->dmabuf_fd);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto exit;\n\t\t\t} else {\n\t\t\t\tbo_bucket->dmabuf_fd = KFD_INVALID_FD;\n\t\t\t}\n\n\t\t\tif (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL)\n\t\t\t\tbo_bucket->offset = KFD_MMAP_TYPE_DOORBELL |\n\t\t\t\t\tKFD_MMAP_GPU_ID(pdd->dev->id);\n\t\t\telse if (bo_bucket->alloc_flags &\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)\n\t\t\t\tbo_bucket->offset = KFD_MMAP_TYPE_MMIO |\n\t\t\t\t\tKFD_MMAP_GPU_ID(pdd->dev->id);\n\t\t\telse\n\t\t\t\tbo_bucket->offset = amdgpu_bo_mmap_offset(dumper_bo);\n\n\t\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\t\tif (amdgpu_amdkfd_bo_mapped_to_dev(p->pdds[i]->dev->adev, kgd_mem))\n\t\t\t\t\tbo_priv->mapped_gpuids[dev_idx++] = p->pdds[i]->user_gpu_id;\n\t\t\t}\n\n\t\t\tpr_debug(\"bo_size = 0x%llx, bo_addr = 0x%llx bo_offset = 0x%llx\\n\"\n\t\t\t\t\t\"gpu_id = 0x%x alloc_flags = 0x%x idr_handle = 0x%x\",\n\t\t\t\t\tbo_bucket->size,\n\t\t\t\t\tbo_bucket->addr,\n\t\t\t\t\tbo_bucket->offset,\n\t\t\t\t\tbo_bucket->gpu_id,\n\t\t\t\t\tbo_bucket->alloc_flags,\n\t\t\t\t\tbo_priv->idr_handle);\n\t\t\tbo_index++;\n\t\t}\n\t}\n\n\tret = copy_to_user(user_bos, bo_buckets, num_bos * sizeof(*bo_buckets));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy BO information to user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tret = copy_to_user(user_priv_data + *priv_offset, bo_privs, num_bos * sizeof(*bo_privs));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy BO priv information to user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\t*priv_offset += num_bos * sizeof(*bo_privs);\n\nexit:\n\twhile (ret && bo_index--) {\n\t\tif (bo_buckets[bo_index].alloc_flags\n\t\t    & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM | KFD_IOC_ALLOC_MEM_FLAGS_GTT))\n\t\t\tclose_fd(bo_buckets[bo_index].dmabuf_fd);\n\t}\n\n\tkvfree(bo_buckets);\n\tkvfree(bo_privs);\n\treturn ret;\n}\n\nstatic int criu_get_process_object_info(struct kfd_process *p,\n\t\t\t\t\tuint32_t *num_devices,\n\t\t\t\t\tuint32_t *num_bos,\n\t\t\t\t\tuint32_t *num_objects,\n\t\t\t\t\tuint64_t *objs_priv_size)\n{\n\tuint64_t queues_priv_data_size, svm_priv_data_size, priv_size;\n\tuint32_t num_queues, num_events, num_svm_ranges;\n\tint ret;\n\n\t*num_devices = p->n_pdds;\n\t*num_bos = get_process_num_bos(p);\n\n\tret = kfd_process_get_queue_info(p, &num_queues, &queues_priv_data_size);\n\tif (ret)\n\t\treturn ret;\n\n\tnum_events = kfd_get_num_events(p);\n\n\tret = svm_range_get_info(p, &num_svm_ranges, &svm_priv_data_size);\n\tif (ret)\n\t\treturn ret;\n\n\t*num_objects = num_queues + num_events + num_svm_ranges;\n\n\tif (objs_priv_size) {\n\t\tpriv_size = sizeof(struct kfd_criu_process_priv_data);\n\t\tpriv_size += *num_devices * sizeof(struct kfd_criu_device_priv_data);\n\t\tpriv_size += *num_bos * sizeof(struct kfd_criu_bo_priv_data);\n\t\tpriv_size += queues_priv_data_size;\n\t\tpriv_size += num_events * sizeof(struct kfd_criu_event_priv_data);\n\t\tpriv_size += svm_priv_data_size;\n\t\t*objs_priv_size = priv_size;\n\t}\n\treturn 0;\n}\n\nstatic int criu_checkpoint(struct file *filep,\n\t\t\t   struct kfd_process *p,\n\t\t\t   struct kfd_ioctl_criu_args *args)\n{\n\tint ret;\n\tuint32_t num_devices, num_bos, num_objects;\n\tuint64_t priv_size, priv_offset = 0, bo_priv_offset;\n\n\tif (!args->devices || !args->bos || !args->priv_data)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&p->mutex);\n\n\tif (!p->n_pdds) {\n\t\tpr_err(\"No pdd for given process\\n\");\n\t\tret = -ENODEV;\n\t\tgoto exit_unlock;\n\t}\n\n\t \n\tif (!p->queues_paused) {\n\t\tpr_err(\"Cannot dump process when queues are not in evicted state\\n\");\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto exit_unlock;\n\t}\n\n\tret = criu_get_process_object_info(p, &num_devices, &num_bos, &num_objects, &priv_size);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tif (num_devices != args->num_devices ||\n\t    num_bos != args->num_bos ||\n\t    num_objects != args->num_objects ||\n\t    priv_size != args->priv_data_size) {\n\n\t\tret = -EINVAL;\n\t\tgoto exit_unlock;\n\t}\n\n\t \n\tret = criu_checkpoint_process(p, (uint8_t __user *)args->priv_data, &priv_offset);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tret = criu_checkpoint_devices(p, num_devices, (uint8_t __user *)args->devices,\n\t\t\t\t(uint8_t __user *)args->priv_data, &priv_offset);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\t \n\tbo_priv_offset = priv_offset;\n\tpriv_offset += num_bos * sizeof(struct kfd_criu_bo_priv_data);\n\n\tif (num_objects) {\n\t\tret = kfd_criu_checkpoint_queues(p, (uint8_t __user *)args->priv_data,\n\t\t\t\t\t\t &priv_offset);\n\t\tif (ret)\n\t\t\tgoto exit_unlock;\n\n\t\tret = kfd_criu_checkpoint_events(p, (uint8_t __user *)args->priv_data,\n\t\t\t\t\t\t &priv_offset);\n\t\tif (ret)\n\t\t\tgoto exit_unlock;\n\n\t\tret = kfd_criu_checkpoint_svm(p, (uint8_t __user *)args->priv_data, &priv_offset);\n\t\tif (ret)\n\t\t\tgoto exit_unlock;\n\t}\n\n\t \n\tret = criu_checkpoint_bos(p, num_bos, (uint8_t __user *)args->bos,\n\t\t\t   (uint8_t __user *)args->priv_data, &bo_priv_offset);\n\nexit_unlock:\n\tmutex_unlock(&p->mutex);\n\tif (ret)\n\t\tpr_err(\"Failed to dump CRIU ret:%d\\n\", ret);\n\telse\n\t\tpr_debug(\"CRIU dump ret:%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int criu_restore_process(struct kfd_process *p,\n\t\t\t\tstruct kfd_ioctl_criu_args *args,\n\t\t\t\tuint64_t *priv_offset,\n\t\t\t\tuint64_t max_priv_data_size)\n{\n\tint ret = 0;\n\tstruct kfd_criu_process_priv_data process_priv;\n\n\tif (*priv_offset + sizeof(process_priv) > max_priv_data_size)\n\t\treturn -EINVAL;\n\n\tret = copy_from_user(&process_priv,\n\t\t\t\t(void __user *)(args->priv_data + *priv_offset),\n\t\t\t\tsizeof(process_priv));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy process private information from user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\t*priv_offset += sizeof(process_priv);\n\n\tif (process_priv.version != KFD_CRIU_PRIV_VERSION) {\n\t\tpr_err(\"Invalid CRIU API version (checkpointed:%d current:%d)\\n\",\n\t\t\tprocess_priv.version, KFD_CRIU_PRIV_VERSION);\n\t\treturn -EINVAL;\n\t}\n\n\tpr_debug(\"Setting XNACK mode\\n\");\n\tif (process_priv.xnack_mode && !kfd_process_xnack_mode(p, true)) {\n\t\tpr_err(\"xnack mode cannot be set\\n\");\n\t\tret = -EPERM;\n\t\tgoto exit;\n\t} else {\n\t\tpr_debug(\"set xnack mode: %d\\n\", process_priv.xnack_mode);\n\t\tp->xnack_enabled = process_priv.xnack_mode;\n\t}\n\nexit:\n\treturn ret;\n}\n\nstatic int criu_restore_devices(struct kfd_process *p,\n\t\t\t\tstruct kfd_ioctl_criu_args *args,\n\t\t\t\tuint64_t *priv_offset,\n\t\t\t\tuint64_t max_priv_data_size)\n{\n\tstruct kfd_criu_device_bucket *device_buckets;\n\tstruct kfd_criu_device_priv_data *device_privs;\n\tint ret = 0;\n\tuint32_t i;\n\n\tif (args->num_devices != p->n_pdds)\n\t\treturn -EINVAL;\n\n\tif (*priv_offset + (args->num_devices * sizeof(*device_privs)) > max_priv_data_size)\n\t\treturn -EINVAL;\n\n\tdevice_buckets = kmalloc_array(args->num_devices, sizeof(*device_buckets), GFP_KERNEL);\n\tif (!device_buckets)\n\t\treturn -ENOMEM;\n\n\tret = copy_from_user(device_buckets, (void __user *)args->devices,\n\t\t\t\targs->num_devices * sizeof(*device_buckets));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy devices buckets from user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tfor (i = 0; i < args->num_devices; i++) {\n\t\tstruct kfd_node *dev;\n\t\tstruct kfd_process_device *pdd;\n\t\tstruct file *drm_file;\n\n\t\t \n\n\t\tif (!device_buckets[i].user_gpu_id) {\n\t\t\tpr_err(\"Invalid user gpu_id\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tdev = kfd_device_by_id(device_buckets[i].actual_gpu_id);\n\t\tif (!dev) {\n\t\t\tpr_err(\"Failed to find device with gpu_id = %x\\n\",\n\t\t\t\tdevice_buckets[i].actual_gpu_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tpdd = kfd_get_process_device_data(dev, p);\n\t\tif (!pdd) {\n\t\t\tpr_err(\"Failed to get pdd for gpu_id = %x\\n\",\n\t\t\t\t\tdevice_buckets[i].actual_gpu_id);\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\t\tpdd->user_gpu_id = device_buckets[i].user_gpu_id;\n\n\t\tdrm_file = fget(device_buckets[i].drm_fd);\n\t\tif (!drm_file) {\n\t\t\tpr_err(\"Invalid render node file descriptor sent from plugin (%d)\\n\",\n\t\t\t\tdevice_buckets[i].drm_fd);\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (pdd->drm_file) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\n\t\t \n\t\tif (kfd_process_device_init_vm(pdd, drm_file)) {\n\t\t\tpr_err(\"could not init vm for given pdd\\n\");\n\t\t\t \n\t\t\tfput(drm_file);\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\t\t \n\t\tpdd = kfd_bind_process_to_device(dev, p);\n\t\tif (IS_ERR(pdd)) {\n\t\t\tret = PTR_ERR(pdd);\n\t\t\tgoto exit;\n\t\t}\n\n\t\tif (!pdd->qpd.proc_doorbells) {\n\t\t\tret = kfd_alloc_process_doorbells(dev->kfd, pdd);\n\t\t\tif (ret)\n\t\t\t\tgoto exit;\n\t\t}\n\t}\n\n\t \n\t*priv_offset += args->num_devices * sizeof(*device_privs);\n\nexit:\n\tkfree(device_buckets);\n\treturn ret;\n}\n\nstatic int criu_restore_memory_of_gpu(struct kfd_process_device *pdd,\n\t\t\t\t      struct kfd_criu_bo_bucket *bo_bucket,\n\t\t\t\t      struct kfd_criu_bo_priv_data *bo_priv,\n\t\t\t\t      struct kgd_mem **kgd_mem)\n{\n\tint idr_handle;\n\tint ret;\n\tconst bool criu_resume = true;\n\tu64 offset;\n\n\tif (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) {\n\t\tif (bo_bucket->size !=\n\t\t\t\tkfd_doorbell_process_slice(pdd->dev->kfd))\n\t\t\treturn -EINVAL;\n\n\t\toffset = kfd_get_process_doorbells(pdd);\n\t\tif (!offset)\n\t\t\treturn -ENOMEM;\n\t} else if (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP) {\n\t\t \n\t\tif (bo_bucket->size != PAGE_SIZE) {\n\t\t\tpr_err(\"Invalid page size\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\toffset = pdd->dev->adev->rmmio_remap.bus_addr;\n\t\tif (!offset) {\n\t\t\tpr_err(\"amdgpu_amdkfd_get_mmio_remap_phys_addr failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t} else if (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\toffset = bo_priv->user_addr;\n\t}\n\t \n\tret = amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(pdd->dev->adev, bo_bucket->addr,\n\t\t\t\t\t\t      bo_bucket->size, pdd->drm_priv, kgd_mem,\n\t\t\t\t\t\t      &offset, bo_bucket->alloc_flags, criu_resume);\n\tif (ret) {\n\t\tpr_err(\"Could not create the BO\\n\");\n\t\treturn ret;\n\t}\n\tpr_debug(\"New BO created: size:0x%llx addr:0x%llx offset:0x%llx\\n\",\n\t\t bo_bucket->size, bo_bucket->addr, offset);\n\n\t \n\tpr_debug(\"Restoring old IDR handle for the BO\");\n\tidr_handle = idr_alloc(&pdd->alloc_idr, *kgd_mem, bo_priv->idr_handle,\n\t\t\t       bo_priv->idr_handle + 1, GFP_KERNEL);\n\n\tif (idr_handle < 0) {\n\t\tpr_err(\"Could not allocate idr\\n\");\n\t\tamdgpu_amdkfd_gpuvm_free_memory_of_gpu(pdd->dev->adev, *kgd_mem, pdd->drm_priv,\n\t\t\t\t\t\t       NULL);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL)\n\t\tbo_bucket->restored_offset = KFD_MMAP_TYPE_DOORBELL | KFD_MMAP_GPU_ID(pdd->dev->id);\n\tif (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP) {\n\t\tbo_bucket->restored_offset = KFD_MMAP_TYPE_MMIO | KFD_MMAP_GPU_ID(pdd->dev->id);\n\t} else if (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tbo_bucket->restored_offset = offset;\n\t} else if (bo_bucket->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tbo_bucket->restored_offset = offset;\n\t\t \n\t\tWRITE_ONCE(pdd->vram_usage, pdd->vram_usage + bo_bucket->size);\n\t}\n\treturn 0;\n}\n\nstatic int criu_restore_bo(struct kfd_process *p,\n\t\t\t   struct kfd_criu_bo_bucket *bo_bucket,\n\t\t\t   struct kfd_criu_bo_priv_data *bo_priv)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct kgd_mem *kgd_mem;\n\tint ret;\n\tint j;\n\n\tpr_debug(\"Restoring BO size:0x%llx addr:0x%llx gpu_id:0x%x flags:0x%x idr_handle:0x%x\\n\",\n\t\t bo_bucket->size, bo_bucket->addr, bo_bucket->gpu_id, bo_bucket->alloc_flags,\n\t\t bo_priv->idr_handle);\n\n\tpdd = kfd_process_device_data_by_id(p, bo_bucket->gpu_id);\n\tif (!pdd) {\n\t\tpr_err(\"Failed to get pdd\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tret = criu_restore_memory_of_gpu(pdd, bo_bucket, bo_priv, &kgd_mem);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfor (j = 0; j < p->n_pdds; j++) {\n\t\tstruct kfd_node *peer;\n\t\tstruct kfd_process_device *peer_pdd;\n\n\t\tif (!bo_priv->mapped_gpuids[j])\n\t\t\tbreak;\n\n\t\tpeer_pdd = kfd_process_device_data_by_id(p, bo_priv->mapped_gpuids[j]);\n\t\tif (!peer_pdd)\n\t\t\treturn -EINVAL;\n\n\t\tpeer = peer_pdd->dev;\n\n\t\tpeer_pdd = kfd_bind_process_to_device(peer, p);\n\t\tif (IS_ERR(peer_pdd))\n\t\t\treturn PTR_ERR(peer_pdd);\n\n\t\tret = amdgpu_amdkfd_gpuvm_map_memory_to_gpu(peer->adev, kgd_mem,\n\t\t\t\t\t\t\t    peer_pdd->drm_priv);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map to gpu %d/%d\\n\", j, p->n_pdds);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tpr_debug(\"map memory was successful for the BO\\n\");\n\t \n\tif (bo_bucket->alloc_flags\n\t    & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM | KFD_IOC_ALLOC_MEM_FLAGS_GTT)) {\n\t\tret = criu_get_prime_handle(kgd_mem, DRM_RDWR,\n\t\t\t\t\t    &bo_bucket->dmabuf_fd);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tbo_bucket->dmabuf_fd = KFD_INVALID_FD;\n\t}\n\n\treturn 0;\n}\n\nstatic int criu_restore_bos(struct kfd_process *p,\n\t\t\t    struct kfd_ioctl_criu_args *args,\n\t\t\t    uint64_t *priv_offset,\n\t\t\t    uint64_t max_priv_data_size)\n{\n\tstruct kfd_criu_bo_bucket *bo_buckets = NULL;\n\tstruct kfd_criu_bo_priv_data *bo_privs = NULL;\n\tint ret = 0;\n\tuint32_t i = 0;\n\n\tif (*priv_offset + (args->num_bos * sizeof(*bo_privs)) > max_priv_data_size)\n\t\treturn -EINVAL;\n\n\t \n\tamdgpu_amdkfd_block_mmu_notifications(p->kgd_process_info);\n\n\tbo_buckets = kvmalloc_array(args->num_bos, sizeof(*bo_buckets), GFP_KERNEL);\n\tif (!bo_buckets)\n\t\treturn -ENOMEM;\n\n\tret = copy_from_user(bo_buckets, (void __user *)args->bos,\n\t\t\t     args->num_bos * sizeof(*bo_buckets));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy BOs information from user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\n\tbo_privs = kvmalloc_array(args->num_bos, sizeof(*bo_privs), GFP_KERNEL);\n\tif (!bo_privs) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tret = copy_from_user(bo_privs, (void __user *)args->priv_data + *priv_offset,\n\t\t\t     args->num_bos * sizeof(*bo_privs));\n\tif (ret) {\n\t\tpr_err(\"Failed to copy BOs information from user\\n\");\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\t*priv_offset += args->num_bos * sizeof(*bo_privs);\n\n\t \n\tfor (; i < args->num_bos; i++) {\n\t\tret = criu_restore_bo(p, &bo_buckets[i], &bo_privs[i]);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed to restore BO[%d] ret%d\\n\", i, ret);\n\t\t\tgoto exit;\n\t\t}\n\t}  \n\n\t \n\tret = copy_to_user((void __user *)args->bos,\n\t\t\t\tbo_buckets,\n\t\t\t\t(args->num_bos * sizeof(*bo_buckets)));\n\tif (ret)\n\t\tret = -EFAULT;\n\nexit:\n\twhile (ret && i--) {\n\t\tif (bo_buckets[i].alloc_flags\n\t\t   & (KFD_IOC_ALLOC_MEM_FLAGS_VRAM | KFD_IOC_ALLOC_MEM_FLAGS_GTT))\n\t\t\tclose_fd(bo_buckets[i].dmabuf_fd);\n\t}\n\tkvfree(bo_buckets);\n\tkvfree(bo_privs);\n\treturn ret;\n}\n\nstatic int criu_restore_objects(struct file *filep,\n\t\t\t\tstruct kfd_process *p,\n\t\t\t\tstruct kfd_ioctl_criu_args *args,\n\t\t\t\tuint64_t *priv_offset,\n\t\t\t\tuint64_t max_priv_data_size)\n{\n\tint ret = 0;\n\tuint32_t i;\n\n\tBUILD_BUG_ON(offsetof(struct kfd_criu_queue_priv_data, object_type));\n\tBUILD_BUG_ON(offsetof(struct kfd_criu_event_priv_data, object_type));\n\tBUILD_BUG_ON(offsetof(struct kfd_criu_svm_range_priv_data, object_type));\n\n\tfor (i = 0; i < args->num_objects; i++) {\n\t\tuint32_t object_type;\n\n\t\tif (*priv_offset + sizeof(object_type) > max_priv_data_size) {\n\t\t\tpr_err(\"Invalid private data size\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tret = get_user(object_type, (uint32_t __user *)(args->priv_data + *priv_offset));\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to copy private information from user\\n\");\n\t\t\tgoto exit;\n\t\t}\n\n\t\tswitch (object_type) {\n\t\tcase KFD_CRIU_OBJECT_TYPE_QUEUE:\n\t\t\tret = kfd_criu_restore_queue(p, (uint8_t __user *)args->priv_data,\n\t\t\t\t\t\t     priv_offset, max_priv_data_size);\n\t\t\tif (ret)\n\t\t\t\tgoto exit;\n\t\t\tbreak;\n\t\tcase KFD_CRIU_OBJECT_TYPE_EVENT:\n\t\t\tret = kfd_criu_restore_event(filep, p, (uint8_t __user *)args->priv_data,\n\t\t\t\t\t\t     priv_offset, max_priv_data_size);\n\t\t\tif (ret)\n\t\t\t\tgoto exit;\n\t\t\tbreak;\n\t\tcase KFD_CRIU_OBJECT_TYPE_SVM_RANGE:\n\t\t\tret = kfd_criu_restore_svm(p, (uint8_t __user *)args->priv_data,\n\t\t\t\t\t\t     priv_offset, max_priv_data_size);\n\t\t\tif (ret)\n\t\t\t\tgoto exit;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"Invalid object type:%u at index:%d\\n\", object_type, i);\n\t\t\tret = -EINVAL;\n\t\t\tgoto exit;\n\t\t}\n\t}\nexit:\n\treturn ret;\n}\n\nstatic int criu_restore(struct file *filep,\n\t\t\tstruct kfd_process *p,\n\t\t\tstruct kfd_ioctl_criu_args *args)\n{\n\tuint64_t priv_offset = 0;\n\tint ret = 0;\n\n\tpr_debug(\"CRIU restore (num_devices:%u num_bos:%u num_objects:%u priv_data_size:%llu)\\n\",\n\t\t args->num_devices, args->num_bos, args->num_objects, args->priv_data_size);\n\n\tif (!args->bos || !args->devices || !args->priv_data || !args->priv_data_size ||\n\t    !args->num_devices || !args->num_bos)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&p->mutex);\n\n\t \n\tret = kfd_process_evict_queues(p, KFD_QUEUE_EVICTION_CRIU_RESTORE);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\t \n\tret = criu_restore_process(p, args, &priv_offset, args->priv_data_size);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tret = criu_restore_devices(p, args, &priv_offset, args->priv_data_size);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tret = criu_restore_bos(p, args, &priv_offset, args->priv_data_size);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tret = criu_restore_objects(filep, p, args, &priv_offset, args->priv_data_size);\n\tif (ret)\n\t\tgoto exit_unlock;\n\n\tif (priv_offset != args->priv_data_size) {\n\t\tpr_err(\"Invalid private data size\\n\");\n\t\tret = -EINVAL;\n\t}\n\nexit_unlock:\n\tmutex_unlock(&p->mutex);\n\tif (ret)\n\t\tpr_err(\"Failed to restore CRIU ret:%d\\n\", ret);\n\telse\n\t\tpr_debug(\"CRIU restore successful\\n\");\n\n\treturn ret;\n}\n\nstatic int criu_unpause(struct file *filep,\n\t\t\tstruct kfd_process *p,\n\t\t\tstruct kfd_ioctl_criu_args *args)\n{\n\tint ret;\n\n\tmutex_lock(&p->mutex);\n\n\tif (!p->queues_paused) {\n\t\tmutex_unlock(&p->mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tret = kfd_process_restore_queues(p);\n\tif (ret)\n\t\tpr_err(\"Failed to unpause queues ret:%d\\n\", ret);\n\telse\n\t\tp->queues_paused = false;\n\n\tmutex_unlock(&p->mutex);\n\n\treturn ret;\n}\n\nstatic int criu_resume(struct file *filep,\n\t\t\tstruct kfd_process *p,\n\t\t\tstruct kfd_ioctl_criu_args *args)\n{\n\tstruct kfd_process *target = NULL;\n\tstruct pid *pid = NULL;\n\tint ret = 0;\n\n\tpr_debug(\"Inside %s, target pid for criu restore: %d\\n\", __func__,\n\t\t args->pid);\n\n\tpid = find_get_pid(args->pid);\n\tif (!pid) {\n\t\tpr_err(\"Cannot find pid info for %i\\n\", args->pid);\n\t\treturn -ESRCH;\n\t}\n\n\tpr_debug(\"calling kfd_lookup_process_by_pid\\n\");\n\ttarget = kfd_lookup_process_by_pid(pid);\n\n\tput_pid(pid);\n\n\tif (!target) {\n\t\tpr_debug(\"Cannot find process info for %i\\n\", args->pid);\n\t\treturn -ESRCH;\n\t}\n\n\tmutex_lock(&target->mutex);\n\tret = kfd_criu_resume_svm(target);\n\tif (ret) {\n\t\tpr_err(\"kfd_criu_resume_svm failed for %i\\n\", args->pid);\n\t\tgoto exit;\n\t}\n\n\tret =  amdgpu_amdkfd_criu_resume(target->kgd_process_info);\n\tif (ret)\n\t\tpr_err(\"amdgpu_amdkfd_criu_resume failed for %i\\n\", args->pid);\n\nexit:\n\tmutex_unlock(&target->mutex);\n\n\tkfd_unref_process(target);\n\treturn ret;\n}\n\nstatic int criu_process_info(struct file *filep,\n\t\t\t\tstruct kfd_process *p,\n\t\t\t\tstruct kfd_ioctl_criu_args *args)\n{\n\tint ret = 0;\n\n\tmutex_lock(&p->mutex);\n\n\tif (!p->n_pdds) {\n\t\tpr_err(\"No pdd for given process\\n\");\n\t\tret = -ENODEV;\n\t\tgoto err_unlock;\n\t}\n\n\tret = kfd_process_evict_queues(p, KFD_QUEUE_EVICTION_CRIU_CHECKPOINT);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tp->queues_paused = true;\n\n\targs->pid = task_pid_nr_ns(p->lead_thread,\n\t\t\t\t\ttask_active_pid_ns(p->lead_thread));\n\n\tret = criu_get_process_object_info(p, &args->num_devices, &args->num_bos,\n\t\t\t\t\t   &args->num_objects, &args->priv_data_size);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tdev_dbg(kfd_device, \"Num of devices:%u bos:%u objects:%u priv_data_size:%lld\\n\",\n\t\t\t\targs->num_devices, args->num_bos, args->num_objects,\n\t\t\t\targs->priv_data_size);\n\nerr_unlock:\n\tif (ret) {\n\t\tkfd_process_restore_queues(p);\n\t\tp->queues_paused = false;\n\t}\n\tmutex_unlock(&p->mutex);\n\treturn ret;\n}\n\nstatic int kfd_ioctl_criu(struct file *filep, struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_criu_args *args = data;\n\tint ret;\n\n\tdev_dbg(kfd_device, \"CRIU operation: %d\\n\", args->op);\n\tswitch (args->op) {\n\tcase KFD_CRIU_OP_PROCESS_INFO:\n\t\tret = criu_process_info(filep, p, args);\n\t\tbreak;\n\tcase KFD_CRIU_OP_CHECKPOINT:\n\t\tret = criu_checkpoint(filep, p, args);\n\t\tbreak;\n\tcase KFD_CRIU_OP_UNPAUSE:\n\t\tret = criu_unpause(filep, p, args);\n\t\tbreak;\n\tcase KFD_CRIU_OP_RESTORE:\n\t\tret = criu_restore(filep, p, args);\n\t\tbreak;\n\tcase KFD_CRIU_OP_RESUME:\n\t\tret = criu_resume(filep, p, args);\n\t\tbreak;\n\tdefault:\n\t\tdev_dbg(kfd_device, \"Unsupported CRIU operation:%d\\n\", args->op);\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\tdev_dbg(kfd_device, \"CRIU operation:%d err:%d\\n\", args->op, ret);\n\n\treturn ret;\n}\n\nstatic int runtime_enable(struct kfd_process *p, uint64_t r_debug,\n\t\t\tbool enable_ttmp_setup)\n{\n\tint i = 0, ret = 0;\n\n\tif (p->is_runtime_retry)\n\t\tgoto retry;\n\n\tif (p->runtime_info.runtime_state != DEBUG_RUNTIME_STATE_DISABLED)\n\t\treturn -EBUSY;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tif (pdd->qpd.queue_count)\n\t\t\treturn -EEXIST;\n\n\t\t \n\t\tif (pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\tkfd_dbg_set_mes_debug_mode(pdd, !kfd_dbg_has_cwsr_workaround(pdd->dev));\n\t}\n\n\tp->runtime_info.runtime_state = DEBUG_RUNTIME_STATE_ENABLED;\n\tp->runtime_info.r_debug = r_debug;\n\tp->runtime_info.ttmp_setup = enable_ttmp_setup;\n\n\tif (p->runtime_info.ttmp_setup) {\n\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\t\tif (!kfd_dbg_is_rlc_restore_supported(pdd->dev)) {\n\t\t\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, false);\n\t\t\t\tpdd->dev->kfd2kgd->enable_debug_trap(\n\t\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\t\ttrue,\n\t\t\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\t\t\t} else if (kfd_dbg_is_per_vmid_supported(pdd->dev)) {\n\t\t\t\tpdd->spi_dbg_override = pdd->dev->kfd2kgd->enable_debug_trap(\n\t\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\t\tfalse,\n\t\t\t\t\t\t0);\n\t\t\t}\n\t\t}\n\t}\n\nretry:\n\tif (p->debug_trap_enabled) {\n\t\tif (!p->is_runtime_retry) {\n\t\t\tkfd_dbg_trap_activate(p);\n\t\t\tkfd_dbg_ev_raise(KFD_EC_MASK(EC_PROCESS_RUNTIME),\n\t\t\t\t\tp, NULL, 0, false, NULL, 0);\n\t\t}\n\n\t\tmutex_unlock(&p->mutex);\n\t\tret = down_interruptible(&p->runtime_enable_sema);\n\t\tmutex_lock(&p->mutex);\n\n\t\tp->is_runtime_retry = !!ret;\n\t}\n\n\treturn ret;\n}\n\nstatic int runtime_disable(struct kfd_process *p)\n{\n\tint i = 0, ret;\n\tbool was_enabled = p->runtime_info.runtime_state == DEBUG_RUNTIME_STATE_ENABLED;\n\n\tp->runtime_info.runtime_state = DEBUG_RUNTIME_STATE_DISABLED;\n\tp->runtime_info.r_debug = 0;\n\n\tif (p->debug_trap_enabled) {\n\t\tif (was_enabled)\n\t\t\tkfd_dbg_trap_deactivate(p, false, 0);\n\n\t\tif (!p->is_runtime_retry)\n\t\t\tkfd_dbg_ev_raise(KFD_EC_MASK(EC_PROCESS_RUNTIME),\n\t\t\t\t\tp, NULL, 0, false, NULL, 0);\n\n\t\tmutex_unlock(&p->mutex);\n\t\tret = down_interruptible(&p->runtime_enable_sema);\n\t\tmutex_lock(&p->mutex);\n\n\t\tp->is_runtime_retry = !!ret;\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (was_enabled && p->runtime_info.ttmp_setup) {\n\t\tfor (i = 0; i < p->n_pdds; i++) {\n\t\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\t\tif (!kfd_dbg_is_rlc_restore_supported(pdd->dev))\n\t\t\t\tamdgpu_gfx_off_ctrl(pdd->dev->adev, true);\n\t\t}\n\t}\n\n\tp->runtime_info.ttmp_setup = false;\n\n\t \n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[i];\n\n\t\tif (kfd_dbg_is_per_vmid_supported(pdd->dev)) {\n\t\t\tpdd->spi_dbg_override =\n\t\t\t\t\tpdd->dev->kfd2kgd->disable_debug_trap(\n\t\t\t\t\tpdd->dev->adev,\n\t\t\t\t\tfalse,\n\t\t\t\t\tpdd->dev->vm_info.last_vmid_kfd);\n\n\t\t\tif (!pdd->dev->kfd->shared_resources.enable_mes)\n\t\t\t\tdebug_refresh_runlist(pdd->dev->dqm);\n\t\t\telse\n\t\t\t\tkfd_dbg_set_mes_debug_mode(pdd,\n\t\t\t\t\t\t\t   !kfd_dbg_has_cwsr_workaround(pdd->dev));\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int kfd_ioctl_runtime_enable(struct file *filep, struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_runtime_enable_args *args = data;\n\tint r;\n\n\tmutex_lock(&p->mutex);\n\n\tif (args->mode_mask & KFD_RUNTIME_ENABLE_MODE_ENABLE_MASK)\n\t\tr = runtime_enable(p, args->r_debug,\n\t\t\t\t!!(args->mode_mask & KFD_RUNTIME_ENABLE_MODE_TTMP_SAVE_MASK));\n\telse\n\t\tr = runtime_disable(p);\n\n\tmutex_unlock(&p->mutex);\n\n\treturn r;\n}\n\nstatic int kfd_ioctl_set_debug_trap(struct file *filep, struct kfd_process *p, void *data)\n{\n\tstruct kfd_ioctl_dbg_trap_args *args = data;\n\tstruct task_struct *thread = NULL;\n\tstruct mm_struct *mm = NULL;\n\tstruct pid *pid = NULL;\n\tstruct kfd_process *target = NULL;\n\tstruct kfd_process_device *pdd = NULL;\n\tint r = 0;\n\n\tif (sched_policy == KFD_SCHED_POLICY_NO_HWS) {\n\t\tpr_err(\"Debugging does not support sched_policy %i\", sched_policy);\n\t\treturn -EINVAL;\n\t}\n\n\tpid = find_get_pid(args->pid);\n\tif (!pid) {\n\t\tpr_debug(\"Cannot find pid info for %i\\n\", args->pid);\n\t\tr = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tthread = get_pid_task(pid, PIDTYPE_PID);\n\tif (!thread) {\n\t\tr = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tmm = get_task_mm(thread);\n\tif (!mm) {\n\t\tr = -ESRCH;\n\t\tgoto out;\n\t}\n\n\tif (args->op == KFD_IOC_DBG_TRAP_ENABLE) {\n\t\tbool create_process;\n\n\t\trcu_read_lock();\n\t\tcreate_process = thread && thread != current && ptrace_parent(thread) == current;\n\t\trcu_read_unlock();\n\n\t\ttarget = create_process ? kfd_create_process(thread) :\n\t\t\t\t\tkfd_lookup_process_by_pid(pid);\n\t} else {\n\t\ttarget = kfd_lookup_process_by_pid(pid);\n\t}\n\n\tif (IS_ERR_OR_NULL(target)) {\n\t\tpr_debug(\"Cannot find process PID %i to debug\\n\", args->pid);\n\t\tr = target ? PTR_ERR(target) : -ESRCH;\n\t\tgoto out;\n\t}\n\n\t \n\trcu_read_lock();\n\tif (target != p && args->op != KFD_IOC_DBG_TRAP_DISABLE\n\t\t\t\t&& ptrace_parent(target->lead_thread) != current) {\n\t\tpr_err(\"PID %i is not PTRACED and cannot be debugged\\n\", args->pid);\n\t\tr = -EPERM;\n\t}\n\trcu_read_unlock();\n\n\tif (r)\n\t\tgoto out;\n\n\tmutex_lock(&target->mutex);\n\n\tif (args->op != KFD_IOC_DBG_TRAP_ENABLE && !target->debug_trap_enabled) {\n\t\tpr_err(\"PID %i not debug enabled for op %i\\n\", args->pid, args->op);\n\t\tr = -EINVAL;\n\t\tgoto unlock_out;\n\t}\n\n\tif (target->runtime_info.runtime_state != DEBUG_RUNTIME_STATE_ENABLED &&\n\t\t\t(args->op == KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_OVERRIDE ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_SUSPEND_QUEUES ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_RESUME_QUEUES ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_SET_NODE_ADDRESS_WATCH ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_CLEAR_NODE_ADDRESS_WATCH ||\n\t\t\t args->op == KFD_IOC_DBG_TRAP_SET_FLAGS)) {\n\t\tr = -EPERM;\n\t\tgoto unlock_out;\n\t}\n\n\tif (args->op == KFD_IOC_DBG_TRAP_SET_NODE_ADDRESS_WATCH ||\n\t    args->op == KFD_IOC_DBG_TRAP_CLEAR_NODE_ADDRESS_WATCH) {\n\t\tint user_gpu_id = kfd_process_get_user_gpu_id(target,\n\t\t\t\targs->op == KFD_IOC_DBG_TRAP_SET_NODE_ADDRESS_WATCH ?\n\t\t\t\t\targs->set_node_address_watch.gpu_id :\n\t\t\t\t\targs->clear_node_address_watch.gpu_id);\n\n\t\tpdd = kfd_process_device_data_by_id(target, user_gpu_id);\n\t\tif (user_gpu_id == -EINVAL || !pdd) {\n\t\t\tr = -ENODEV;\n\t\t\tgoto unlock_out;\n\t\t}\n\t}\n\n\tswitch (args->op) {\n\tcase KFD_IOC_DBG_TRAP_ENABLE:\n\t\tif (target != p)\n\t\t\ttarget->debugger_process = p;\n\n\t\tr = kfd_dbg_trap_enable(target,\n\t\t\t\t\targs->enable.dbg_fd,\n\t\t\t\t\t(void __user *)args->enable.rinfo_ptr,\n\t\t\t\t\t&args->enable.rinfo_size);\n\t\tif (!r)\n\t\t\ttarget->exception_enable_mask = args->enable.exception_mask;\n\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_DISABLE:\n\t\tr = kfd_dbg_trap_disable(target);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SEND_RUNTIME_EVENT:\n\t\tr = kfd_dbg_send_exception_to_runtime(target,\n\t\t\t\targs->send_runtime_event.gpu_id,\n\t\t\t\targs->send_runtime_event.queue_id,\n\t\t\t\targs->send_runtime_event.exception_mask);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SET_EXCEPTIONS_ENABLED:\n\t\tkfd_dbg_set_enabled_debug_exception_mask(target,\n\t\t\t\targs->set_exceptions_enabled.exception_mask);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_OVERRIDE:\n\t\tr = kfd_dbg_trap_set_wave_launch_override(target,\n\t\t\t\targs->launch_override.override_mode,\n\t\t\t\targs->launch_override.enable_mask,\n\t\t\t\targs->launch_override.support_request_mask,\n\t\t\t\t&args->launch_override.enable_mask,\n\t\t\t\t&args->launch_override.support_request_mask);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SET_WAVE_LAUNCH_MODE:\n\t\tr = kfd_dbg_trap_set_wave_launch_mode(target,\n\t\t\t\targs->launch_mode.launch_mode);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SUSPEND_QUEUES:\n\t\tr = suspend_queues(target,\n\t\t\t\targs->suspend_queues.num_queues,\n\t\t\t\targs->suspend_queues.grace_period,\n\t\t\t\targs->suspend_queues.exception_mask,\n\t\t\t\t(uint32_t *)args->suspend_queues.queue_array_ptr);\n\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_RESUME_QUEUES:\n\t\tr = resume_queues(target, args->resume_queues.num_queues,\n\t\t\t\t(uint32_t *)args->resume_queues.queue_array_ptr);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SET_NODE_ADDRESS_WATCH:\n\t\tr = kfd_dbg_trap_set_dev_address_watch(pdd,\n\t\t\t\targs->set_node_address_watch.address,\n\t\t\t\targs->set_node_address_watch.mask,\n\t\t\t\t&args->set_node_address_watch.id,\n\t\t\t\targs->set_node_address_watch.mode);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_CLEAR_NODE_ADDRESS_WATCH:\n\t\tr = kfd_dbg_trap_clear_dev_address_watch(pdd,\n\t\t\t\targs->clear_node_address_watch.id);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_SET_FLAGS:\n\t\tr = kfd_dbg_trap_set_flags(target, &args->set_flags.flags);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_QUERY_DEBUG_EVENT:\n\t\tr = kfd_dbg_ev_query_debug_event(target,\n\t\t\t\t&args->query_debug_event.queue_id,\n\t\t\t\t&args->query_debug_event.gpu_id,\n\t\t\t\targs->query_debug_event.exception_mask,\n\t\t\t\t&args->query_debug_event.exception_mask);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_QUERY_EXCEPTION_INFO:\n\t\tr = kfd_dbg_trap_query_exception_info(target,\n\t\t\t\targs->query_exception_info.source_id,\n\t\t\t\targs->query_exception_info.exception_code,\n\t\t\t\targs->query_exception_info.clear_exception,\n\t\t\t\t(void __user *)args->query_exception_info.info_ptr,\n\t\t\t\t&args->query_exception_info.info_size);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_GET_QUEUE_SNAPSHOT:\n\t\tr = pqm_get_queue_snapshot(&target->pqm,\n\t\t\t\targs->queue_snapshot.exception_mask,\n\t\t\t\t(void __user *)args->queue_snapshot.snapshot_buf_ptr,\n\t\t\t\t&args->queue_snapshot.num_queues,\n\t\t\t\t&args->queue_snapshot.entry_size);\n\t\tbreak;\n\tcase KFD_IOC_DBG_TRAP_GET_DEVICE_SNAPSHOT:\n\t\tr = kfd_dbg_trap_device_snapshot(target,\n\t\t\t\targs->device_snapshot.exception_mask,\n\t\t\t\t(void __user *)args->device_snapshot.snapshot_buf_ptr,\n\t\t\t\t&args->device_snapshot.num_devices,\n\t\t\t\t&args->device_snapshot.entry_size);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"Invalid option: %i\\n\", args->op);\n\t\tr = -EINVAL;\n\t}\n\nunlock_out:\n\tmutex_unlock(&target->mutex);\n\nout:\n\tif (thread)\n\t\tput_task_struct(thread);\n\n\tif (mm)\n\t\tmmput(mm);\n\n\tif (pid)\n\t\tput_pid(pid);\n\n\tif (target)\n\t\tkfd_unref_process(target);\n\n\treturn r;\n}\n\n#define AMDKFD_IOCTL_DEF(ioctl, _func, _flags) \\\n\t[_IOC_NR(ioctl)] = {.cmd = ioctl, .func = _func, .flags = _flags, \\\n\t\t\t    .cmd_drv = 0, .name = #ioctl}\n\n \nstatic const struct amdkfd_ioctl_desc amdkfd_ioctls[] = {\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_VERSION,\n\t\t\tkfd_ioctl_get_version, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_CREATE_QUEUE,\n\t\t\tkfd_ioctl_create_queue, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DESTROY_QUEUE,\n\t\t\tkfd_ioctl_destroy_queue, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_MEMORY_POLICY,\n\t\t\tkfd_ioctl_set_memory_policy, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_CLOCK_COUNTERS,\n\t\t\tkfd_ioctl_get_clock_counters, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_PROCESS_APERTURES,\n\t\t\tkfd_ioctl_get_process_apertures, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_UPDATE_QUEUE,\n\t\t\tkfd_ioctl_update_queue, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_CREATE_EVENT,\n\t\t\tkfd_ioctl_create_event, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DESTROY_EVENT,\n\t\t\tkfd_ioctl_destroy_event, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_EVENT,\n\t\t\tkfd_ioctl_set_event, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_RESET_EVENT,\n\t\t\tkfd_ioctl_reset_event, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_WAIT_EVENTS,\n\t\t\tkfd_ioctl_wait_events, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_REGISTER_DEPRECATED,\n\t\t\tkfd_ioctl_dbg_register, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_UNREGISTER_DEPRECATED,\n\t\t\tkfd_ioctl_dbg_unregister, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_ADDRESS_WATCH_DEPRECATED,\n\t\t\tkfd_ioctl_dbg_address_watch, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_WAVE_CONTROL_DEPRECATED,\n\t\t\tkfd_ioctl_dbg_wave_control, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_SCRATCH_BACKING_VA,\n\t\t\tkfd_ioctl_set_scratch_backing_va, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_TILE_CONFIG,\n\t\t\tkfd_ioctl_get_tile_config, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_TRAP_HANDLER,\n\t\t\tkfd_ioctl_set_trap_handler, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_PROCESS_APERTURES_NEW,\n\t\t\tkfd_ioctl_get_process_apertures_new, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_ACQUIRE_VM,\n\t\t\tkfd_ioctl_acquire_vm, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_MEMORY_OF_GPU,\n\t\t\tkfd_ioctl_alloc_memory_of_gpu, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_FREE_MEMORY_OF_GPU,\n\t\t\tkfd_ioctl_free_memory_of_gpu, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_MAP_MEMORY_TO_GPU,\n\t\t\tkfd_ioctl_map_memory_to_gpu, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_UNMAP_MEMORY_FROM_GPU,\n\t\t\tkfd_ioctl_unmap_memory_from_gpu, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_CU_MASK,\n\t\t\tkfd_ioctl_set_cu_mask, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_QUEUE_WAVE_STATE,\n\t\t\tkfd_ioctl_get_queue_wave_state, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_GET_DMABUF_INFO,\n\t\t\t\tkfd_ioctl_get_dmabuf_info, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_IMPORT_DMABUF,\n\t\t\t\tkfd_ioctl_import_dmabuf, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_ALLOC_QUEUE_GWS,\n\t\t\tkfd_ioctl_alloc_queue_gws, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SMI_EVENTS,\n\t\t\tkfd_ioctl_smi_events, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SVM, kfd_ioctl_svm, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_SET_XNACK_MODE,\n\t\t\tkfd_ioctl_set_xnack_mode, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_CRIU_OP,\n\t\t\tkfd_ioctl_criu, KFD_IOC_FLAG_CHECKPOINT_RESTORE),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_AVAILABLE_MEMORY,\n\t\t\tkfd_ioctl_get_available_memory, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_EXPORT_DMABUF,\n\t\t\t\tkfd_ioctl_export_dmabuf, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_RUNTIME_ENABLE,\n\t\t\tkfd_ioctl_runtime_enable, 0),\n\n\tAMDKFD_IOCTL_DEF(AMDKFD_IOC_DBG_TRAP,\n\t\t\tkfd_ioctl_set_debug_trap, 0),\n};\n\n#define AMDKFD_CORE_IOCTL_COUNT\tARRAY_SIZE(amdkfd_ioctls)\n\nstatic long kfd_ioctl(struct file *filep, unsigned int cmd, unsigned long arg)\n{\n\tstruct kfd_process *process;\n\tamdkfd_ioctl_t *func;\n\tconst struct amdkfd_ioctl_desc *ioctl = NULL;\n\tunsigned int nr = _IOC_NR(cmd);\n\tchar stack_kdata[128];\n\tchar *kdata = NULL;\n\tunsigned int usize, asize;\n\tint retcode = -EINVAL;\n\tbool ptrace_attached = false;\n\n\tif (nr >= AMDKFD_CORE_IOCTL_COUNT)\n\t\tgoto err_i1;\n\n\tif ((nr >= AMDKFD_COMMAND_START) && (nr < AMDKFD_COMMAND_END)) {\n\t\tu32 amdkfd_size;\n\n\t\tioctl = &amdkfd_ioctls[nr];\n\n\t\tamdkfd_size = _IOC_SIZE(ioctl->cmd);\n\t\tusize = asize = _IOC_SIZE(cmd);\n\t\tif (amdkfd_size > asize)\n\t\t\tasize = amdkfd_size;\n\n\t\tcmd = ioctl->cmd;\n\t} else\n\t\tgoto err_i1;\n\n\tdev_dbg(kfd_device, \"ioctl cmd 0x%x (#0x%x), arg 0x%lx\\n\", cmd, nr, arg);\n\n\t \n\tprocess = filep->private_data;\n\n\trcu_read_lock();\n\tif ((ioctl->flags & KFD_IOC_FLAG_CHECKPOINT_RESTORE) &&\n\t    ptrace_parent(process->lead_thread) == current)\n\t\tptrace_attached = true;\n\trcu_read_unlock();\n\n\tif (process->lead_thread != current->group_leader\n\t    && !ptrace_attached) {\n\t\tdev_dbg(kfd_device, \"Using KFD FD in wrong process\\n\");\n\t\tretcode = -EBADF;\n\t\tgoto err_i1;\n\t}\n\n\t \n\tfunc = ioctl->func;\n\n\tif (unlikely(!func)) {\n\t\tdev_dbg(kfd_device, \"no function\\n\");\n\t\tretcode = -EINVAL;\n\t\tgoto err_i1;\n\t}\n\n\t \n\tif (unlikely(ioctl->flags & KFD_IOC_FLAG_CHECKPOINT_RESTORE)) {\n\t\tif (!capable(CAP_CHECKPOINT_RESTORE) &&\n\t\t\t\t\t\t!capable(CAP_SYS_ADMIN)) {\n\t\t\tretcode = -EACCES;\n\t\t\tgoto err_i1;\n\t\t}\n\t}\n\n\tif (cmd & (IOC_IN | IOC_OUT)) {\n\t\tif (asize <= sizeof(stack_kdata)) {\n\t\t\tkdata = stack_kdata;\n\t\t} else {\n\t\t\tkdata = kmalloc(asize, GFP_KERNEL);\n\t\t\tif (!kdata) {\n\t\t\t\tretcode = -ENOMEM;\n\t\t\t\tgoto err_i1;\n\t\t\t}\n\t\t}\n\t\tif (asize > usize)\n\t\t\tmemset(kdata + usize, 0, asize - usize);\n\t}\n\n\tif (cmd & IOC_IN) {\n\t\tif (copy_from_user(kdata, (void __user *)arg, usize) != 0) {\n\t\t\tretcode = -EFAULT;\n\t\t\tgoto err_i1;\n\t\t}\n\t} else if (cmd & IOC_OUT) {\n\t\tmemset(kdata, 0, usize);\n\t}\n\n\tretcode = func(filep, process, kdata);\n\n\tif (cmd & IOC_OUT)\n\t\tif (copy_to_user((void __user *)arg, kdata, usize) != 0)\n\t\t\tretcode = -EFAULT;\n\nerr_i1:\n\tif (!ioctl)\n\t\tdev_dbg(kfd_device, \"invalid ioctl: pid=%d, cmd=0x%02x, nr=0x%02x\\n\",\n\t\t\t  task_pid_nr(current), cmd, nr);\n\n\tif (kdata != stack_kdata)\n\t\tkfree(kdata);\n\n\tif (retcode)\n\t\tdev_dbg(kfd_device, \"ioctl cmd (#0x%x), arg 0x%lx, ret = %d\\n\",\n\t\t\t\tnr, arg, retcode);\n\n\treturn retcode;\n}\n\nstatic int kfd_mmio_mmap(struct kfd_node *dev, struct kfd_process *process,\n\t\t      struct vm_area_struct *vma)\n{\n\tphys_addr_t address;\n\n\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\taddress = dev->adev->rmmio_remap.bus_addr;\n\n\tvm_flags_set(vma, VM_IO | VM_DONTCOPY | VM_DONTEXPAND | VM_NORESERVE |\n\t\t\t\tVM_DONTDUMP | VM_PFNMAP);\n\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\n\tpr_debug(\"pasid 0x%x mapping mmio page\\n\"\n\t\t \"     target user address == 0x%08llX\\n\"\n\t\t \"     physical address    == 0x%08llX\\n\"\n\t\t \"     vm_flags            == 0x%04lX\\n\"\n\t\t \"     size                == 0x%04lX\\n\",\n\t\t process->pasid, (unsigned long long) vma->vm_start,\n\t\t address, vma->vm_flags, PAGE_SIZE);\n\n\treturn io_remap_pfn_range(vma,\n\t\t\t\tvma->vm_start,\n\t\t\t\taddress >> PAGE_SHIFT,\n\t\t\t\tPAGE_SIZE,\n\t\t\t\tvma->vm_page_prot);\n}\n\n\nstatic int kfd_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct kfd_process *process;\n\tstruct kfd_node *dev = NULL;\n\tunsigned long mmap_offset;\n\tunsigned int gpu_id;\n\n\tprocess = kfd_get_process(current);\n\tif (IS_ERR(process))\n\t\treturn PTR_ERR(process);\n\n\tmmap_offset = vma->vm_pgoff << PAGE_SHIFT;\n\tgpu_id = KFD_MMAP_GET_GPU_ID(mmap_offset);\n\tif (gpu_id)\n\t\tdev = kfd_device_by_id(gpu_id);\n\n\tswitch (mmap_offset & KFD_MMAP_TYPE_MASK) {\n\tcase KFD_MMAP_TYPE_DOORBELL:\n\t\tif (!dev)\n\t\t\treturn -ENODEV;\n\t\treturn kfd_doorbell_mmap(dev, process, vma);\n\n\tcase KFD_MMAP_TYPE_EVENTS:\n\t\treturn kfd_event_mmap(process, vma);\n\n\tcase KFD_MMAP_TYPE_RESERVED_MEM:\n\t\tif (!dev)\n\t\t\treturn -ENODEV;\n\t\treturn kfd_reserved_mem_mmap(dev, process, vma);\n\tcase KFD_MMAP_TYPE_MMIO:\n\t\tif (!dev)\n\t\t\treturn -ENODEV;\n\t\treturn kfd_mmio_mmap(dev, process, vma);\n\t}\n\n\treturn -EFAULT;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}