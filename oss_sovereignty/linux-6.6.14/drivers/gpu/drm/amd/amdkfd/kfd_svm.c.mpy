{
  "module_name": "kfd_svm.c",
  "hash_id": "459d843174fc8ab0006e7eedb7b773c006120fae349f167b5773c32e488c4c4f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_svm.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/sched/task.h>\n#include <linux/dynamic_debug.h>\n#include <drm/ttm/ttm_tt.h>\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_sync.h\"\n#include \"amdgpu_object.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu.h\"\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_svm.h\"\n#include \"kfd_migrate.h\"\n#include \"kfd_smi_events.h\"\n\n#ifdef dev_fmt\n#undef dev_fmt\n#endif\n#define dev_fmt(fmt) \"kfd_svm: %s: \" fmt, __func__\n\n#define AMDGPU_SVM_RANGE_RESTORE_DELAY_MS 1\n\n \n#define AMDGPU_SVM_RANGE_RETRY_FAULT_PENDING\t(2UL * NSEC_PER_MSEC)\n#if IS_ENABLED(CONFIG_DYNAMIC_DEBUG)\n#define dynamic_svm_range_dump(svms) \\\n\t_dynamic_func_call_no_desc(\"svm_range_dump\", svm_range_debug_dump, svms)\n#else\n#define dynamic_svm_range_dump(svms) \\\n\tdo { if (0) svm_range_debug_dump(svms); } while (0)\n#endif\n\n \nstatic uint64_t max_svm_range_pages;\n\nstruct criu_svm_metadata {\n\tstruct list_head list;\n\tstruct kfd_criu_svm_range_priv_data data;\n};\n\nstatic void svm_range_evict_svm_bo_worker(struct work_struct *work);\nstatic bool\nsvm_range_cpu_invalidate_pagetables(struct mmu_interval_notifier *mni,\n\t\t\t\t    const struct mmu_notifier_range *range,\n\t\t\t\t    unsigned long cur_seq);\nstatic int\nsvm_range_check_vm(struct kfd_process *p, uint64_t start, uint64_t last,\n\t\t   uint64_t *bo_s, uint64_t *bo_l);\nstatic const struct mmu_interval_notifier_ops svm_range_mn_ops = {\n\t.invalidate = svm_range_cpu_invalidate_pagetables,\n};\n\n \nstatic void svm_range_unlink(struct svm_range *prange)\n{\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t prange, prange->start, prange->last);\n\n\tif (prange->svm_bo) {\n\t\tspin_lock(&prange->svm_bo->list_lock);\n\t\tlist_del(&prange->svm_bo_list);\n\t\tspin_unlock(&prange->svm_bo->list_lock);\n\t}\n\n\tlist_del(&prange->list);\n\tif (prange->it_node.start != 0 && prange->it_node.last != 0)\n\t\tinterval_tree_remove(&prange->it_node, &prange->svms->objects);\n}\n\nstatic void\nsvm_range_add_notifier_locked(struct mm_struct *mm, struct svm_range *prange)\n{\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t prange, prange->start, prange->last);\n\n\tmmu_interval_notifier_insert_locked(&prange->notifier, mm,\n\t\t\t\t     prange->start << PAGE_SHIFT,\n\t\t\t\t     prange->npages << PAGE_SHIFT,\n\t\t\t\t     &svm_range_mn_ops);\n}\n\n \nstatic void svm_range_add_to_svms(struct svm_range *prange)\n{\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t prange, prange->start, prange->last);\n\n\tlist_move_tail(&prange->list, &prange->svms->list);\n\tprange->it_node.start = prange->start;\n\tprange->it_node.last = prange->last;\n\tinterval_tree_insert(&prange->it_node, &prange->svms->objects);\n}\n\nstatic void svm_range_remove_notifier(struct svm_range *prange)\n{\n\tpr_debug(\"remove notifier svms 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t prange->svms, prange,\n\t\t prange->notifier.interval_tree.start >> PAGE_SHIFT,\n\t\t prange->notifier.interval_tree.last >> PAGE_SHIFT);\n\n\tif (prange->notifier.interval_tree.start != 0 &&\n\t    prange->notifier.interval_tree.last != 0)\n\t\tmmu_interval_notifier_remove(&prange->notifier);\n}\n\nstatic bool\nsvm_is_valid_dma_mapping_addr(struct device *dev, dma_addr_t dma_addr)\n{\n\treturn dma_addr && !dma_mapping_error(dev, dma_addr) &&\n\t       !(dma_addr & SVM_RANGE_VRAM_DOMAIN);\n}\n\nstatic int\nsvm_range_dma_map_dev(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t      unsigned long offset, unsigned long npages,\n\t\t      unsigned long *hmm_pfns, uint32_t gpuidx)\n{\n\tenum dma_data_direction dir = DMA_BIDIRECTIONAL;\n\tdma_addr_t *addr = prange->dma_addr[gpuidx];\n\tstruct device *dev = adev->dev;\n\tstruct page *page;\n\tint i, r;\n\n\tif (!addr) {\n\t\taddr = kvcalloc(prange->npages, sizeof(*addr), GFP_KERNEL);\n\t\tif (!addr)\n\t\t\treturn -ENOMEM;\n\t\tprange->dma_addr[gpuidx] = addr;\n\t}\n\n\taddr += offset;\n\tfor (i = 0; i < npages; i++) {\n\t\tif (svm_is_valid_dma_mapping_addr(dev, addr[i]))\n\t\t\tdma_unmap_page(dev, addr[i], PAGE_SIZE, dir);\n\n\t\tpage = hmm_pfn_to_page(hmm_pfns[i]);\n\t\tif (is_zone_device_page(page)) {\n\t\t\tstruct amdgpu_device *bo_adev = prange->svm_bo->node->adev;\n\n\t\t\taddr[i] = (hmm_pfns[i] << PAGE_SHIFT) +\n\t\t\t\t   bo_adev->vm_manager.vram_base_offset -\n\t\t\t\t   bo_adev->kfd.pgmap.range.start;\n\t\t\taddr[i] |= SVM_RANGE_VRAM_DOMAIN;\n\t\t\tpr_debug_ratelimited(\"vram address: 0x%llx\\n\", addr[i]);\n\t\t\tcontinue;\n\t\t}\n\t\taddr[i] = dma_map_page(dev, page, 0, PAGE_SIZE, dir);\n\t\tr = dma_mapping_error(dev, addr[i]);\n\t\tif (r) {\n\t\t\tdev_err(dev, \"failed %d dma_map_page\\n\", r);\n\t\t\treturn r;\n\t\t}\n\t\tpr_debug_ratelimited(\"dma mapping 0x%llx for page addr 0x%lx\\n\",\n\t\t\t\t     addr[i] >> PAGE_SHIFT, page_to_pfn(page));\n\t}\n\treturn 0;\n}\n\nstatic int\nsvm_range_dma_map(struct svm_range *prange, unsigned long *bitmap,\n\t\t  unsigned long offset, unsigned long npages,\n\t\t  unsigned long *hmm_pfns)\n{\n\tstruct kfd_process *p;\n\tuint32_t gpuidx;\n\tint r;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\tfor_each_set_bit(gpuidx, bitmap, MAX_GPU_INSTANCE) {\n\t\tstruct kfd_process_device *pdd;\n\n\t\tpr_debug(\"mapping to gpu idx 0x%x\\n\", gpuidx);\n\t\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tr = svm_range_dma_map_dev(pdd->dev->adev, prange, offset, npages,\n\t\t\t\t\t  hmm_pfns, gpuidx);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nvoid svm_range_dma_unmap(struct device *dev, dma_addr_t *dma_addr,\n\t\t\t unsigned long offset, unsigned long npages)\n{\n\tenum dma_data_direction dir = DMA_BIDIRECTIONAL;\n\tint i;\n\n\tif (!dma_addr)\n\t\treturn;\n\n\tfor (i = offset; i < offset + npages; i++) {\n\t\tif (!svm_is_valid_dma_mapping_addr(dev, dma_addr[i]))\n\t\t\tcontinue;\n\t\tpr_debug_ratelimited(\"unmap 0x%llx\\n\", dma_addr[i] >> PAGE_SHIFT);\n\t\tdma_unmap_page(dev, dma_addr[i], PAGE_SIZE, dir);\n\t\tdma_addr[i] = 0;\n\t}\n}\n\nvoid svm_range_free_dma_mappings(struct svm_range *prange, bool unmap_dma)\n{\n\tstruct kfd_process_device *pdd;\n\tdma_addr_t *dma_addr;\n\tstruct device *dev;\n\tstruct kfd_process *p;\n\tuint32_t gpuidx;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\tfor (gpuidx = 0; gpuidx < MAX_GPU_INSTANCE; gpuidx++) {\n\t\tdma_addr = prange->dma_addr[gpuidx];\n\t\tif (!dma_addr)\n\t\t\tcontinue;\n\n\t\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\tcontinue;\n\t\t}\n\t\tdev = &pdd->dev->adev->pdev->dev;\n\t\tif (unmap_dma)\n\t\t\tsvm_range_dma_unmap(dev, dma_addr, 0, prange->npages);\n\t\tkvfree(dma_addr);\n\t\tprange->dma_addr[gpuidx] = NULL;\n\t}\n}\n\nstatic void svm_range_free(struct svm_range *prange, bool do_unmap)\n{\n\tuint64_t size = (prange->last - prange->start + 1) << PAGE_SHIFT;\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", prange->svms, prange,\n\t\t prange->start, prange->last);\n\n\tsvm_range_vram_node_free(prange);\n\tsvm_range_free_dma_mappings(prange, do_unmap);\n\n\tif (do_unmap && !p->xnack_enabled) {\n\t\tpr_debug(\"unreserve prange 0x%p size: 0x%llx\\n\", prange, size);\n\t\tamdgpu_amdkfd_unreserve_mem_limit(NULL, size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\t}\n\tmutex_destroy(&prange->lock);\n\tmutex_destroy(&prange->migrate_mutex);\n\tkfree(prange);\n}\n\nstatic void\nsvm_range_set_default_attributes(int32_t *location, int32_t *prefetch_loc,\n\t\t\t\t uint8_t *granularity, uint32_t *flags)\n{\n\t*location = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\t*prefetch_loc = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\t*granularity = 9;\n\t*flags =\n\t\tKFD_IOCTL_SVM_FLAG_HOST_ACCESS | KFD_IOCTL_SVM_FLAG_COHERENT;\n}\n\nstatic struct\nsvm_range *svm_range_new(struct svm_range_list *svms, uint64_t start,\n\t\t\t uint64_t last, bool update_mem_usage)\n{\n\tuint64_t size = last - start + 1;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\n\tprange = kzalloc(sizeof(*prange), GFP_KERNEL);\n\tif (!prange)\n\t\treturn NULL;\n\n\tp = container_of(svms, struct kfd_process, svms);\n\tif (!p->xnack_enabled && update_mem_usage &&\n\t    amdgpu_amdkfd_reserve_mem_limit(NULL, size << PAGE_SHIFT,\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0)) {\n\t\tpr_info(\"SVM mapping failed, exceeds resident system memory limit\\n\");\n\t\tkfree(prange);\n\t\treturn NULL;\n\t}\n\tprange->npages = size;\n\tprange->svms = svms;\n\tprange->start = start;\n\tprange->last = last;\n\tINIT_LIST_HEAD(&prange->list);\n\tINIT_LIST_HEAD(&prange->update_list);\n\tINIT_LIST_HEAD(&prange->svm_bo_list);\n\tINIT_LIST_HEAD(&prange->deferred_list);\n\tINIT_LIST_HEAD(&prange->child_list);\n\tatomic_set(&prange->invalid, 0);\n\tprange->validate_timestamp = 0;\n\tmutex_init(&prange->migrate_mutex);\n\tmutex_init(&prange->lock);\n\n\tif (p->xnack_enabled)\n\t\tbitmap_copy(prange->bitmap_access, svms->bitmap_supported,\n\t\t\t    MAX_GPU_INSTANCE);\n\n\tsvm_range_set_default_attributes(&prange->preferred_loc,\n\t\t\t\t\t &prange->prefetch_loc,\n\t\t\t\t\t &prange->granularity, &prange->flags);\n\n\tpr_debug(\"svms 0x%p [0x%llx 0x%llx]\\n\", svms, start, last);\n\n\treturn prange;\n}\n\nstatic bool svm_bo_ref_unless_zero(struct svm_range_bo *svm_bo)\n{\n\tif (!svm_bo || !kref_get_unless_zero(&svm_bo->kref))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void svm_range_bo_release(struct kref *kref)\n{\n\tstruct svm_range_bo *svm_bo;\n\n\tsvm_bo = container_of(kref, struct svm_range_bo, kref);\n\tpr_debug(\"svm_bo 0x%p\\n\", svm_bo);\n\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list)) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\t \n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\t\tmutex_lock(&prange->lock);\n\t\tprange->svm_bo = NULL;\n\t\tmutex_unlock(&prange->lock);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tif (!dma_fence_is_signaled(&svm_bo->eviction_fence->base)) {\n\t\t \n\t\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\t\tcancel_work_sync(&svm_bo->eviction_work);\n\t}\n\tdma_fence_put(&svm_bo->eviction_fence->base);\n\tamdgpu_bo_unref(&svm_bo->bo);\n\tkfree(svm_bo);\n}\n\nstatic void svm_range_bo_wq_release(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, release_work);\n\tsvm_range_bo_release(&svm_bo->kref);\n}\n\nstatic void svm_range_bo_release_async(struct kref *kref)\n{\n\tstruct svm_range_bo *svm_bo;\n\n\tsvm_bo = container_of(kref, struct svm_range_bo, kref);\n\tpr_debug(\"svm_bo 0x%p\\n\", svm_bo);\n\tINIT_WORK(&svm_bo->release_work, svm_range_bo_wq_release);\n\tschedule_work(&svm_bo->release_work);\n}\n\nvoid svm_range_bo_unref_async(struct svm_range_bo *svm_bo)\n{\n\tkref_put(&svm_bo->kref, svm_range_bo_release_async);\n}\n\nstatic void svm_range_bo_unref(struct svm_range_bo *svm_bo)\n{\n\tif (svm_bo)\n\t\tkref_put(&svm_bo->kref, svm_range_bo_release);\n}\n\nstatic bool\nsvm_range_validate_svm_bo(struct kfd_node *node, struct svm_range *prange)\n{\n\tmutex_lock(&prange->lock);\n\tif (!prange->svm_bo) {\n\t\tmutex_unlock(&prange->lock);\n\t\treturn false;\n\t}\n\tif (prange->ttm_res) {\n\t\t \n\t\tmutex_unlock(&prange->lock);\n\t\treturn true;\n\t}\n\tif (svm_bo_ref_unless_zero(prange->svm_bo)) {\n\t\t \n\t\tif (prange->svm_bo->node != node) {\n\t\t\tmutex_unlock(&prange->lock);\n\n\t\t\tspin_lock(&prange->svm_bo->list_lock);\n\t\t\tlist_del_init(&prange->svm_bo_list);\n\t\t\tspin_unlock(&prange->svm_bo->list_lock);\n\n\t\t\tsvm_range_bo_unref(prange->svm_bo);\n\t\t\treturn false;\n\t\t}\n\t\tif (READ_ONCE(prange->svm_bo->evicting)) {\n\t\t\tstruct dma_fence *f;\n\t\t\tstruct svm_range_bo *svm_bo;\n\t\t\t \n\t\t\tmutex_unlock(&prange->lock);\n\t\t\tsvm_bo = prange->svm_bo;\n\t\t\tf = dma_fence_get(&svm_bo->eviction_fence->base);\n\t\t\tsvm_range_bo_unref(prange->svm_bo);\n\t\t\t \n\t\t\tdma_fence_wait(f, false);\n\t\t\tdma_fence_put(f);\n\t\t} else {\n\t\t\t \n\t\t\tmutex_unlock(&prange->lock);\n\t\t\tpr_debug(\"reuse old bo svms 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t\t prange->svms, prange->start, prange->last);\n\n\t\t\tprange->ttm_res = prange->svm_bo->bo->tbo.resource;\n\t\t\treturn true;\n\t\t}\n\n\t} else {\n\t\tmutex_unlock(&prange->lock);\n\t}\n\n\t \n\twhile (!list_empty_careful(&prange->svm_bo_list) || prange->svm_bo)\n\t\tcond_resched();\n\n\treturn false;\n}\n\nstatic struct svm_range_bo *svm_range_bo_new(void)\n{\n\tstruct svm_range_bo *svm_bo;\n\n\tsvm_bo = kzalloc(sizeof(*svm_bo), GFP_KERNEL);\n\tif (!svm_bo)\n\t\treturn NULL;\n\n\tkref_init(&svm_bo->kref);\n\tINIT_LIST_HEAD(&svm_bo->range_list);\n\tspin_lock_init(&svm_bo->list_lock);\n\n\treturn svm_bo;\n}\n\nint\nsvm_range_vram_node_new(struct kfd_node *node, struct svm_range *prange,\n\t\t\tbool clear)\n{\n\tstruct amdgpu_bo_param bp;\n\tstruct svm_range_bo *svm_bo;\n\tstruct amdgpu_bo_user *ubo;\n\tstruct amdgpu_bo *bo;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\tpr_debug(\"pasid: %x svms 0x%p [0x%lx 0x%lx]\\n\", p->pasid, prange->svms,\n\t\t prange->start, prange->last);\n\n\tif (svm_range_validate_svm_bo(node, prange))\n\t\treturn 0;\n\n\tsvm_bo = svm_range_bo_new();\n\tif (!svm_bo) {\n\t\tpr_debug(\"failed to alloc svm bo\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"failed to get mm\\n\");\n\t\tkfree(svm_bo);\n\t\treturn -ESRCH;\n\t}\n\tsvm_bo->node = node;\n\tsvm_bo->eviction_fence =\n\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t   mm,\n\t\t\t\t\t   svm_bo);\n\tmmput(mm);\n\tINIT_WORK(&svm_bo->eviction_work, svm_range_evict_svm_bo_worker);\n\tsvm_bo->evicting = 0;\n\tmemset(&bp, 0, sizeof(bp));\n\tbp.size = prange->npages * PAGE_SIZE;\n\tbp.byte_align = PAGE_SIZE;\n\tbp.domain = AMDGPU_GEM_DOMAIN_VRAM;\n\tbp.flags = AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\tbp.flags |= clear ? AMDGPU_GEM_CREATE_VRAM_CLEARED : 0;\n\tbp.flags |= AMDGPU_GEM_CREATE_DISCARDABLE;\n\tbp.type = ttm_bo_type_device;\n\tbp.resv = NULL;\n\tif (node->xcp)\n\t\tbp.xcp_id_plus1 = node->xcp->id + 1;\n\n\tr = amdgpu_bo_create_user(node->adev, &bp, &ubo);\n\tif (r) {\n\t\tpr_debug(\"failed %d to create bo\\n\", r);\n\t\tgoto create_bo_failed;\n\t}\n\tbo = &ubo->bo;\n\n\tpr_debug(\"alloc bo at offset 0x%lx size 0x%lx on partition %d\\n\",\n\t\t bo->tbo.resource->start << PAGE_SHIFT, bp.size,\n\t\t bp.xcp_id_plus1 - 1);\n\n\tr = amdgpu_bo_reserve(bo, true);\n\tif (r) {\n\t\tpr_debug(\"failed %d to reserve bo\\n\", r);\n\t\tgoto reserve_bo_failed;\n\t}\n\n\tif (clear) {\n\t\tr = amdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to sync bo\\n\", r);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tgoto reserve_bo_failed;\n\t\t}\n\t}\n\n\tr = dma_resv_reserve_fences(bo->tbo.base.resv, 1);\n\tif (r) {\n\t\tpr_debug(\"failed %d to reserve bo\\n\", r);\n\t\tamdgpu_bo_unreserve(bo);\n\t\tgoto reserve_bo_failed;\n\t}\n\tamdgpu_bo_fence(bo, &svm_bo->eviction_fence->base, true);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tsvm_bo->bo = bo;\n\tprange->svm_bo = svm_bo;\n\tprange->ttm_res = bo->tbo.resource;\n\tprange->offset = 0;\n\n\tspin_lock(&svm_bo->list_lock);\n\tlist_add(&prange->svm_bo_list, &svm_bo->range_list);\n\tspin_unlock(&svm_bo->list_lock);\n\n\treturn 0;\n\nreserve_bo_failed:\n\tamdgpu_bo_unref(&bo);\ncreate_bo_failed:\n\tdma_fence_put(&svm_bo->eviction_fence->base);\n\tkfree(svm_bo);\n\tprange->ttm_res = NULL;\n\n\treturn r;\n}\n\nvoid svm_range_vram_node_free(struct svm_range *prange)\n{\n\t \n\tmutex_lock(&prange->lock);\n\t \n\tif (prange->ttm_res) {\n\t\tprange->ttm_res = NULL;\n\t\tmutex_unlock(&prange->lock);\n\t\tsvm_range_bo_unref(prange->svm_bo);\n\t} else\n\t\tmutex_unlock(&prange->lock);\n}\n\nstruct kfd_node *\nsvm_range_get_node_by_id(struct svm_range *prange, uint32_t gpu_id)\n{\n\tstruct kfd_process *p;\n\tstruct kfd_process_device *pdd;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\tpdd = kfd_process_device_data_by_id(p, gpu_id);\n\tif (!pdd) {\n\t\tpr_debug(\"failed to get kfd process device by id 0x%x\\n\", gpu_id);\n\t\treturn NULL;\n\t}\n\n\treturn pdd->dev;\n}\n\nstruct kfd_process_device *\nsvm_range_get_pdd_by_node(struct svm_range *prange, struct kfd_node *node)\n{\n\tstruct kfd_process *p;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\treturn kfd_get_process_device_data(node, p);\n}\n\nstatic int svm_range_bo_validate(void *param, struct amdgpu_bo *bo)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_VRAM);\n\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\nstatic int\nsvm_range_check_attr(struct kfd_process *p,\n\t\t     uint32_t nattr, struct kfd_ioctl_svm_attribute *attrs)\n{\n\tuint32_t i;\n\n\tfor (i = 0; i < nattr; i++) {\n\t\tuint32_t val = attrs[i].value;\n\t\tint gpuidx = MAX_GPU_INSTANCE;\n\n\t\tswitch (attrs[i].type) {\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFERRED_LOC:\n\t\t\tif (val != KFD_IOCTL_SVM_LOCATION_SYSMEM &&\n\t\t\t    val != KFD_IOCTL_SVM_LOCATION_UNDEFINED)\n\t\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p, val);\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\tif (val != KFD_IOCTL_SVM_LOCATION_SYSMEM)\n\t\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p, val);\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS:\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS_IN_PLACE:\n\t\tcase KFD_IOCTL_SVM_ATTR_NO_ACCESS:\n\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p, val);\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_CLR_FLAGS:\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_GRANULARITY:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_debug(\"unknown attr type 0x%x\\n\", attrs[i].type);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (gpuidx < 0) {\n\t\t\tpr_debug(\"no GPU 0x%x found\\n\", val);\n\t\t\treturn -EINVAL;\n\t\t} else if (gpuidx < MAX_GPU_INSTANCE &&\n\t\t\t   !test_bit(gpuidx, p->svms.bitmap_supported)) {\n\t\t\tpr_debug(\"GPU 0x%x not supported\\n\", val);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void\nsvm_range_apply_attrs(struct kfd_process *p, struct svm_range *prange,\n\t\t      uint32_t nattr, struct kfd_ioctl_svm_attribute *attrs,\n\t\t      bool *update_mapping)\n{\n\tuint32_t i;\n\tint gpuidx;\n\n\tfor (i = 0; i < nattr; i++) {\n\t\tswitch (attrs[i].type) {\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFERRED_LOC:\n\t\t\tprange->preferred_loc = attrs[i].value;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\tprange->prefetch_loc = attrs[i].value;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS:\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS_IN_PLACE:\n\t\tcase KFD_IOCTL_SVM_ATTR_NO_ACCESS:\n\t\t\tif (!p->xnack_enabled)\n\t\t\t\t*update_mapping = true;\n\n\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p,\n\t\t\t\t\t\t\t       attrs[i].value);\n\t\t\tif (attrs[i].type == KFD_IOCTL_SVM_ATTR_NO_ACCESS) {\n\t\t\t\tbitmap_clear(prange->bitmap_access, gpuidx, 1);\n\t\t\t\tbitmap_clear(prange->bitmap_aip, gpuidx, 1);\n\t\t\t} else if (attrs[i].type == KFD_IOCTL_SVM_ATTR_ACCESS) {\n\t\t\t\tbitmap_set(prange->bitmap_access, gpuidx, 1);\n\t\t\t\tbitmap_clear(prange->bitmap_aip, gpuidx, 1);\n\t\t\t} else {\n\t\t\t\tbitmap_clear(prange->bitmap_access, gpuidx, 1);\n\t\t\t\tbitmap_set(prange->bitmap_aip, gpuidx, 1);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\t\t*update_mapping = true;\n\t\t\tprange->flags |= attrs[i].value;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_CLR_FLAGS:\n\t\t\t*update_mapping = true;\n\t\t\tprange->flags &= ~attrs[i].value;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_GRANULARITY:\n\t\t\tprange->granularity = min_t(uint32_t, attrs[i].value, 0x3F);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ONCE(1, \"svm_range_check_attrs wasn't called?\");\n\t\t}\n\t}\n}\n\nstatic bool\nsvm_range_is_same_attrs(struct kfd_process *p, struct svm_range *prange,\n\t\t\tuint32_t nattr, struct kfd_ioctl_svm_attribute *attrs)\n{\n\tuint32_t i;\n\tint gpuidx;\n\n\tfor (i = 0; i < nattr; i++) {\n\t\tswitch (attrs[i].type) {\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFERRED_LOC:\n\t\t\tif (prange->preferred_loc != attrs[i].value)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\t \n\t\t\treturn false;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS:\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS_IN_PLACE:\n\t\tcase KFD_IOCTL_SVM_ATTR_NO_ACCESS:\n\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p,\n\t\t\t\t\t\t\t       attrs[i].value);\n\t\t\tif (attrs[i].type == KFD_IOCTL_SVM_ATTR_NO_ACCESS) {\n\t\t\t\tif (test_bit(gpuidx, prange->bitmap_access) ||\n\t\t\t\t    test_bit(gpuidx, prange->bitmap_aip))\n\t\t\t\t\treturn false;\n\t\t\t} else if (attrs[i].type == KFD_IOCTL_SVM_ATTR_ACCESS) {\n\t\t\t\tif (!test_bit(gpuidx, prange->bitmap_access))\n\t\t\t\t\treturn false;\n\t\t\t} else {\n\t\t\t\tif (!test_bit(gpuidx, prange->bitmap_aip))\n\t\t\t\t\treturn false;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\t\tif ((prange->flags & attrs[i].value) != attrs[i].value)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_CLR_FLAGS:\n\t\t\tif ((prange->flags & attrs[i].value) != 0)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_GRANULARITY:\n\t\t\tif (prange->granularity != attrs[i].value)\n\t\t\t\treturn false;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ONCE(1, \"svm_range_check_attrs wasn't called?\");\n\t\t}\n\t}\n\n\treturn true;\n}\n\n \nstatic void svm_range_debug_dump(struct svm_range_list *svms)\n{\n\tstruct interval_tree_node *node;\n\tstruct svm_range *prange;\n\n\tpr_debug(\"dump svms 0x%p list\\n\", svms);\n\tpr_debug(\"range\\tstart\\tpage\\tend\\t\\tlocation\\n\");\n\n\tlist_for_each_entry(prange, &svms->list, list) {\n\t\tpr_debug(\"0x%p 0x%lx\\t0x%llx\\t0x%llx\\t0x%x\\n\",\n\t\t\t prange, prange->start, prange->npages,\n\t\t\t prange->start + prange->npages - 1,\n\t\t\t prange->actual_loc);\n\t}\n\n\tpr_debug(\"dump svms 0x%p interval tree\\n\", svms);\n\tpr_debug(\"range\\tstart\\tpage\\tend\\t\\tlocation\\n\");\n\tnode = interval_tree_iter_first(&svms->objects, 0, ~0ULL);\n\twhile (node) {\n\t\tprange = container_of(node, struct svm_range, it_node);\n\t\tpr_debug(\"0x%p 0x%lx\\t0x%llx\\t0x%llx\\t0x%x\\n\",\n\t\t\t prange, prange->start, prange->npages,\n\t\t\t prange->start + prange->npages - 1,\n\t\t\t prange->actual_loc);\n\t\tnode = interval_tree_iter_next(node, 0, ~0ULL);\n\t}\n}\n\nstatic void *\nsvm_range_copy_array(void *psrc, size_t size, uint64_t num_elements,\n\t\t     uint64_t offset)\n{\n\tunsigned char *dst;\n\n\tdst = kvmalloc_array(num_elements, size, GFP_KERNEL);\n\tif (!dst)\n\t\treturn NULL;\n\tmemcpy(dst, (unsigned char *)psrc + offset, num_elements * size);\n\n\treturn (void *)dst;\n}\n\nstatic int\nsvm_range_copy_dma_addrs(struct svm_range *dst, struct svm_range *src)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_GPU_INSTANCE; i++) {\n\t\tif (!src->dma_addr[i])\n\t\t\tcontinue;\n\t\tdst->dma_addr[i] = svm_range_copy_array(src->dma_addr[i],\n\t\t\t\t\tsizeof(*src->dma_addr[i]), src->npages, 0);\n\t\tif (!dst->dma_addr[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nsvm_range_split_array(void *ppnew, void *ppold, size_t size,\n\t\t      uint64_t old_start, uint64_t old_n,\n\t\t      uint64_t new_start, uint64_t new_n)\n{\n\tunsigned char *new, *old, *pold;\n\tuint64_t d;\n\n\tif (!ppold)\n\t\treturn 0;\n\tpold = *(unsigned char **)ppold;\n\tif (!pold)\n\t\treturn 0;\n\n\td = (new_start - old_start) * size;\n\tnew = svm_range_copy_array(pold, size, new_n, d);\n\tif (!new)\n\t\treturn -ENOMEM;\n\td = (new_start == old_start) ? new_n * size : 0;\n\told = svm_range_copy_array(pold, size, old_n, d);\n\tif (!old) {\n\t\tkvfree(new);\n\t\treturn -ENOMEM;\n\t}\n\tkvfree(pold);\n\t*(void **)ppold = old;\n\t*(void **)ppnew = new;\n\n\treturn 0;\n}\n\nstatic int\nsvm_range_split_pages(struct svm_range *new, struct svm_range *old,\n\t\t      uint64_t start, uint64_t last)\n{\n\tuint64_t npages = last - start + 1;\n\tint i, r;\n\n\tfor (i = 0; i < MAX_GPU_INSTANCE; i++) {\n\t\tr = svm_range_split_array(&new->dma_addr[i], &old->dma_addr[i],\n\t\t\t\t\t  sizeof(*old->dma_addr[i]), old->start,\n\t\t\t\t\t  npages, new->start, new->npages);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nsvm_range_split_nodes(struct svm_range *new, struct svm_range *old,\n\t\t      uint64_t start, uint64_t last)\n{\n\tuint64_t npages = last - start + 1;\n\n\tpr_debug(\"svms 0x%p new prange 0x%p start 0x%lx [0x%llx 0x%llx]\\n\",\n\t\t new->svms, new, new->start, start, last);\n\n\tif (new->start == old->start) {\n\t\tnew->offset = old->offset;\n\t\told->offset += new->npages;\n\t} else {\n\t\tnew->offset = old->offset + npages;\n\t}\n\n\tnew->svm_bo = svm_range_bo_ref(old->svm_bo);\n\tnew->ttm_res = old->ttm_res;\n\n\tspin_lock(&new->svm_bo->list_lock);\n\tlist_add(&new->svm_bo_list, &new->svm_bo->range_list);\n\tspin_unlock(&new->svm_bo->list_lock);\n\n\treturn 0;\n}\n\n \nstatic int\nsvm_range_split_adjust(struct svm_range *new, struct svm_range *old,\n\t\t      uint64_t start, uint64_t last)\n{\n\tint r;\n\n\tpr_debug(\"svms 0x%p new 0x%lx old [0x%lx 0x%lx] => [0x%llx 0x%llx]\\n\",\n\t\t new->svms, new->start, old->start, old->last, start, last);\n\n\tif (new->start < old->start ||\n\t    new->last > old->last) {\n\t\tWARN_ONCE(1, \"invalid new range start or last\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tr = svm_range_split_pages(new, old, start, last);\n\tif (r)\n\t\treturn r;\n\n\tif (old->actual_loc && old->ttm_res) {\n\t\tr = svm_range_split_nodes(new, old, start, last);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\told->npages = last - start + 1;\n\told->start = start;\n\told->last = last;\n\tnew->flags = old->flags;\n\tnew->preferred_loc = old->preferred_loc;\n\tnew->prefetch_loc = old->prefetch_loc;\n\tnew->actual_loc = old->actual_loc;\n\tnew->granularity = old->granularity;\n\tnew->mapped_to_gpu = old->mapped_to_gpu;\n\tbitmap_copy(new->bitmap_access, old->bitmap_access, MAX_GPU_INSTANCE);\n\tbitmap_copy(new->bitmap_aip, old->bitmap_aip, MAX_GPU_INSTANCE);\n\n\treturn 0;\n}\n\n \nstatic int\nsvm_range_split(struct svm_range *prange, uint64_t start, uint64_t last,\n\t\tstruct svm_range **new)\n{\n\tuint64_t old_start = prange->start;\n\tuint64_t old_last = prange->last;\n\tstruct svm_range_list *svms;\n\tint r = 0;\n\n\tpr_debug(\"svms 0x%p [0x%llx 0x%llx] to [0x%llx 0x%llx]\\n\", prange->svms,\n\t\t old_start, old_last, start, last);\n\n\tif (old_start != start && old_last != last)\n\t\treturn -EINVAL;\n\tif (start < old_start || last > old_last)\n\t\treturn -EINVAL;\n\n\tsvms = prange->svms;\n\tif (old_start == start)\n\t\t*new = svm_range_new(svms, last + 1, old_last, false);\n\telse\n\t\t*new = svm_range_new(svms, old_start, start - 1, false);\n\tif (!*new)\n\t\treturn -ENOMEM;\n\n\tr = svm_range_split_adjust(*new, prange, start, last);\n\tif (r) {\n\t\tpr_debug(\"failed %d split [0x%llx 0x%llx] to [0x%llx 0x%llx]\\n\",\n\t\t\t r, old_start, old_last, start, last);\n\t\tsvm_range_free(*new, false);\n\t\t*new = NULL;\n\t}\n\n\treturn r;\n}\n\nstatic int\nsvm_range_split_tail(struct svm_range *prange,\n\t\t     uint64_t new_last, struct list_head *insert_list)\n{\n\tstruct svm_range *tail;\n\tint r = svm_range_split(prange, prange->start, new_last, &tail);\n\n\tif (!r)\n\t\tlist_add(&tail->list, insert_list);\n\treturn r;\n}\n\nstatic int\nsvm_range_split_head(struct svm_range *prange,\n\t\t     uint64_t new_start, struct list_head *insert_list)\n{\n\tstruct svm_range *head;\n\tint r = svm_range_split(prange, new_start, prange->last, &head);\n\n\tif (!r)\n\t\tlist_add(&head->list, insert_list);\n\treturn r;\n}\n\nstatic void\nsvm_range_add_child(struct svm_range *prange, struct mm_struct *mm,\n\t\t    struct svm_range *pchild, enum svm_work_list_ops op)\n{\n\tpr_debug(\"add child 0x%p [0x%lx 0x%lx] to prange 0x%p child list %d\\n\",\n\t\t pchild, pchild->start, pchild->last, prange, op);\n\n\tpchild->work_item.mm = mm;\n\tpchild->work_item.op = op;\n\tlist_add_tail(&pchild->child_list, &prange->child_list);\n}\n\n \nint\nsvm_range_split_by_granularity(struct kfd_process *p, struct mm_struct *mm,\n\t\t\t       unsigned long addr, struct svm_range *parent,\n\t\t\t       struct svm_range *prange)\n{\n\tstruct svm_range *head, *tail;\n\tunsigned long start, last, size;\n\tint r;\n\n\t \n\tsize = 1UL << prange->granularity;\n\tstart = ALIGN_DOWN(addr, size);\n\tlast = ALIGN(addr + 1, size) - 1;\n\n\tpr_debug(\"svms 0x%p split [0x%lx 0x%lx] to [0x%lx 0x%lx] size 0x%lx\\n\",\n\t\t prange->svms, prange->start, prange->last, start, last, size);\n\n\tif (start > prange->start) {\n\t\tr = svm_range_split(prange, start, prange->last, &head);\n\t\tif (r)\n\t\t\treturn r;\n\t\tsvm_range_add_child(parent, mm, head, SVM_OP_ADD_RANGE);\n\t}\n\n\tif (last < prange->last) {\n\t\tr = svm_range_split(prange, prange->start, last, &tail);\n\t\tif (r)\n\t\t\treturn r;\n\t\tsvm_range_add_child(parent, mm, tail, SVM_OP_ADD_RANGE);\n\t}\n\n\t \n\tif (p->xnack_enabled && prange->work_item.op == SVM_OP_ADD_RANGE) {\n\t\tprange->work_item.op = SVM_OP_ADD_RANGE_AND_MAP;\n\t\tpr_debug(\"change prange 0x%p [0x%lx 0x%lx] op %d\\n\",\n\t\t\t prange, prange->start, prange->last,\n\t\t\t SVM_OP_ADD_RANGE_AND_MAP);\n\t}\n\treturn 0;\n}\nstatic bool\nsvm_nodes_in_same_hive(struct kfd_node *node_a, struct kfd_node *node_b)\n{\n\treturn (node_a->adev == node_b->adev ||\n\t\tamdgpu_xgmi_same_hive(node_a->adev, node_b->adev));\n}\n\nstatic uint64_t\nsvm_range_get_pte_flags(struct kfd_node *node,\n\t\t\tstruct svm_range *prange, int domain)\n{\n\tstruct kfd_node *bo_node;\n\tuint32_t flags = prange->flags;\n\tuint32_t mapping_flags = 0;\n\tuint64_t pte_flags;\n\tbool snoop = (domain != SVM_RANGE_VRAM_DOMAIN);\n\tbool coherent = flags & KFD_IOCTL_SVM_FLAG_COHERENT;\n\tbool uncached = false;  \n\tunsigned int mtype_local;\n\n\tif (domain == SVM_RANGE_VRAM_DOMAIN)\n\t\tbo_node = prange->svm_bo->node;\n\n\tswitch (node->adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 1):\n\t\tif (domain == SVM_RANGE_VRAM_DOMAIN) {\n\t\t\tif (bo_node == node) {\n\t\t\t\tmapping_flags |= coherent ?\n\t\t\t\t\tAMDGPU_VM_MTYPE_CC : AMDGPU_VM_MTYPE_RW;\n\t\t\t} else {\n\t\t\t\tmapping_flags |= coherent ?\n\t\t\t\t\tAMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n\t\t\t\tif (svm_nodes_in_same_hive(node, bo_node))\n\t\t\t\t\tsnoop = true;\n\t\t\t}\n\t\t} else {\n\t\t\tmapping_flags |= coherent ?\n\t\t\t\tAMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n\t\t}\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 2):\n\t\tif (domain == SVM_RANGE_VRAM_DOMAIN) {\n\t\t\tif (bo_node == node) {\n\t\t\t\tmapping_flags |= coherent ?\n\t\t\t\t\tAMDGPU_VM_MTYPE_CC : AMDGPU_VM_MTYPE_RW;\n\t\t\t\tif (node->adev->gmc.xgmi.connected_to_cpu)\n\t\t\t\t\tsnoop = true;\n\t\t\t} else {\n\t\t\t\tmapping_flags |= coherent ?\n\t\t\t\t\tAMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n\t\t\t\tif (svm_nodes_in_same_hive(node, bo_node))\n\t\t\t\t\tsnoop = true;\n\t\t\t}\n\t\t} else {\n\t\t\tmapping_flags |= coherent ?\n\t\t\t\tAMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n\t\t}\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 3):\n\t\tmtype_local = amdgpu_mtype_local == 1 ? AMDGPU_VM_MTYPE_NC :\n\t\t\t     (amdgpu_mtype_local == 2 ? AMDGPU_VM_MTYPE_CC : AMDGPU_VM_MTYPE_RW);\n\t\tsnoop = true;\n\t\tif (uncached) {\n\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_UC;\n\t\t} else if (domain == SVM_RANGE_VRAM_DOMAIN) {\n\t\t\t \n\t\t\tif (bo_node->adev == node->adev &&\n\t\t\t    (!bo_node->xcp || !node->xcp || bo_node->xcp->mem_id == node->xcp->mem_id))\n\t\t\t\tmapping_flags |= mtype_local;\n\t\t\t \n\t\t\telse if (svm_nodes_in_same_hive(bo_node, node))\n\t\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_NC;\n\t\t\t \n\t\t\telse\n\t\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_UC;\n\t\t \n\t\t} else if (node->adev->flags & AMD_IS_APU) {\n\t\t\t \n\t\t\tif (num_possible_nodes() <= 1)\n\t\t\t\tmapping_flags |= mtype_local;\n\t\t\telse\n\t\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_NC;\n\t\t \n\t\t} else {\n\t\t\tmapping_flags |= AMDGPU_VM_MTYPE_UC;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tmapping_flags |= coherent ?\n\t\t\tAMDGPU_VM_MTYPE_UC : AMDGPU_VM_MTYPE_NC;\n\t}\n\n\tmapping_flags |= AMDGPU_VM_PAGE_READABLE | AMDGPU_VM_PAGE_WRITEABLE;\n\n\tif (flags & KFD_IOCTL_SVM_FLAG_GPU_RO)\n\t\tmapping_flags &= ~AMDGPU_VM_PAGE_WRITEABLE;\n\tif (flags & KFD_IOCTL_SVM_FLAG_GPU_EXEC)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\tpte_flags = AMDGPU_PTE_VALID;\n\tpte_flags |= (domain == SVM_RANGE_VRAM_DOMAIN) ? 0 : AMDGPU_PTE_SYSTEM;\n\tpte_flags |= snoop ? AMDGPU_PTE_SNOOPED : 0;\n\n\tpte_flags |= amdgpu_gem_va_map_flags(node->adev, mapping_flags);\n\treturn pte_flags;\n}\n\nstatic int\nsvm_range_unmap_from_gpu(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\t uint64_t start, uint64_t last,\n\t\t\t struct dma_fence **fence)\n{\n\tuint64_t init_pte_value = 0;\n\n\tpr_debug(\"[0x%llx 0x%llx]\\n\", start, last);\n\n\treturn amdgpu_vm_update_range(adev, vm, false, true, true, NULL, start,\n\t\t\t\t      last, init_pte_value, 0, 0, NULL, NULL,\n\t\t\t\t      fence);\n}\n\nstatic int\nsvm_range_unmap_from_gpus(struct svm_range *prange, unsigned long start,\n\t\t\t  unsigned long last, uint32_t trigger)\n{\n\tDECLARE_BITMAP(bitmap, MAX_GPU_INSTANCE);\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *fence = NULL;\n\tstruct kfd_process *p;\n\tuint32_t gpuidx;\n\tint r = 0;\n\n\tif (!prange->mapped_to_gpu) {\n\t\tpr_debug(\"prange 0x%p [0x%lx 0x%lx] not mapped to GPU\\n\",\n\t\t\t prange, prange->start, prange->last);\n\t\treturn 0;\n\t}\n\n\tif (prange->start == start && prange->last == last) {\n\t\tpr_debug(\"unmap svms 0x%p prange 0x%p\\n\", prange->svms, prange);\n\t\tprange->mapped_to_gpu = false;\n\t}\n\n\tbitmap_or(bitmap, prange->bitmap_access, prange->bitmap_aip,\n\t\t  MAX_GPU_INSTANCE);\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\tfor_each_set_bit(gpuidx, bitmap, MAX_GPU_INSTANCE) {\n\t\tpr_debug(\"unmap from gpu idx 0x%x\\n\", gpuidx);\n\t\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tkfd_smi_event_unmap_from_gpu(pdd->dev, p->lead_thread->pid,\n\t\t\t\t\t     start, last, trigger);\n\n\t\tr = svm_range_unmap_from_gpu(pdd->dev->adev,\n\t\t\t\t\t     drm_priv_to_vm(pdd->drm_priv),\n\t\t\t\t\t     start, last, &fence);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\tif (fence) {\n\t\t\tr = dma_fence_wait(fence, false);\n\t\t\tdma_fence_put(fence);\n\t\t\tfence = NULL;\n\t\t\tif (r)\n\t\t\t\tbreak;\n\t\t}\n\t\tkfd_flush_tlb(pdd, TLB_FLUSH_HEAVYWEIGHT);\n\t}\n\n\treturn r;\n}\n\nstatic int\nsvm_range_map_to_gpu(struct kfd_process_device *pdd, struct svm_range *prange,\n\t\t     unsigned long offset, unsigned long npages, bool readonly,\n\t\t     dma_addr_t *dma_addr, struct amdgpu_device *bo_adev,\n\t\t     struct dma_fence **fence, bool flush_tlb)\n{\n\tstruct amdgpu_device *adev = pdd->dev->adev;\n\tstruct amdgpu_vm *vm = drm_priv_to_vm(pdd->drm_priv);\n\tuint64_t pte_flags;\n\tunsigned long last_start;\n\tint last_domain;\n\tint r = 0;\n\tint64_t i, j;\n\n\tlast_start = prange->start + offset;\n\n\tpr_debug(\"svms 0x%p [0x%lx 0x%lx] readonly %d\\n\", prange->svms,\n\t\t last_start, last_start + npages - 1, readonly);\n\n\tfor (i = offset; i < offset + npages; i++) {\n\t\tlast_domain = dma_addr[i] & SVM_RANGE_VRAM_DOMAIN;\n\t\tdma_addr[i] &= ~SVM_RANGE_VRAM_DOMAIN;\n\n\t\t \n\t\tif (i < offset + npages - 1 &&\n\t\t    last_domain == (dma_addr[i + 1] & SVM_RANGE_VRAM_DOMAIN))\n\t\t\tcontinue;\n\n\t\tpr_debug(\"Mapping range [0x%lx 0x%llx] on domain: %s\\n\",\n\t\t\t last_start, prange->start + i, last_domain ? \"GPU\" : \"CPU\");\n\n\t\tpte_flags = svm_range_get_pte_flags(pdd->dev, prange, last_domain);\n\t\tif (readonly)\n\t\t\tpte_flags &= ~AMDGPU_PTE_WRITEABLE;\n\n\t\tpr_debug(\"svms 0x%p map [0x%lx 0x%llx] vram %d PTE 0x%llx\\n\",\n\t\t\t prange->svms, last_start, prange->start + i,\n\t\t\t (last_domain == SVM_RANGE_VRAM_DOMAIN) ? 1 : 0,\n\t\t\t pte_flags);\n\n\t\t \n\t\tr = amdgpu_vm_update_range(adev, vm, false, false, flush_tlb, NULL,\n\t\t\t\t\t   last_start, prange->start + i,\n\t\t\t\t\t   pte_flags,\n\t\t\t\t\t   (last_start - prange->start) << PAGE_SHIFT,\n\t\t\t\t\t   bo_adev ? bo_adev->vm_manager.vram_base_offset : 0,\n\t\t\t\t\t   NULL, dma_addr, &vm->last_update);\n\n\t\tfor (j = last_start - prange->start; j <= i; j++)\n\t\t\tdma_addr[j] |= last_domain;\n\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to map to gpu 0x%lx\\n\", r, prange->start);\n\t\t\tgoto out;\n\t\t}\n\t\tlast_start = prange->start + i + 1;\n\t}\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r) {\n\t\tpr_debug(\"failed %d to update directories 0x%lx\\n\", r,\n\t\t\t prange->start);\n\t\tgoto out;\n\t}\n\n\tif (fence)\n\t\t*fence = dma_fence_get(vm->last_update);\n\nout:\n\treturn r;\n}\n\nstatic int\nsvm_range_map_to_gpus(struct svm_range *prange, unsigned long offset,\n\t\t      unsigned long npages, bool readonly,\n\t\t      unsigned long *bitmap, bool wait, bool flush_tlb)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct amdgpu_device *bo_adev = NULL;\n\tstruct kfd_process *p;\n\tstruct dma_fence *fence = NULL;\n\tuint32_t gpuidx;\n\tint r = 0;\n\n\tif (prange->svm_bo && prange->ttm_res)\n\t\tbo_adev = prange->svm_bo->node->adev;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\tfor_each_set_bit(gpuidx, bitmap, MAX_GPU_INSTANCE) {\n\t\tpr_debug(\"mapping to gpu idx 0x%x\\n\", gpuidx);\n\t\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpdd = kfd_bind_process_to_device(pdd->dev, p);\n\t\tif (IS_ERR(pdd))\n\t\t\treturn -EINVAL;\n\n\t\tif (bo_adev && pdd->dev->adev != bo_adev &&\n\t\t    !amdgpu_xgmi_same_hive(pdd->dev->adev, bo_adev)) {\n\t\t\tpr_debug(\"cannot map to device idx %d\\n\", gpuidx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = svm_range_map_to_gpu(pdd, prange, offset, npages, readonly,\n\t\t\t\t\t prange->dma_addr[gpuidx],\n\t\t\t\t\t bo_adev, wait ? &fence : NULL,\n\t\t\t\t\t flush_tlb);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\tif (fence) {\n\t\t\tr = dma_fence_wait(fence, false);\n\t\t\tdma_fence_put(fence);\n\t\t\tfence = NULL;\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"failed %d to dma fence wait\\n\", r);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tkfd_flush_tlb(pdd, TLB_FLUSH_LEGACY);\n\t}\n\n\treturn r;\n}\n\nstruct svm_validate_context {\n\tstruct kfd_process *process;\n\tstruct svm_range *prange;\n\tbool intr;\n\tDECLARE_BITMAP(bitmap, MAX_GPU_INSTANCE);\n\tstruct drm_exec exec;\n};\n\nstatic int svm_range_reserve_bos(struct svm_validate_context *ctx, bool intr)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct amdgpu_vm *vm;\n\tuint32_t gpuidx;\n\tint r;\n\n\tdrm_exec_init(&ctx->exec, intr ? DRM_EXEC_INTERRUPTIBLE_WAIT: 0);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tfor_each_set_bit(gpuidx, ctx->bitmap, MAX_GPU_INSTANCE) {\n\t\t\tpdd = kfd_process_device_from_gpuidx(ctx->process, gpuidx);\n\t\t\tif (!pdd) {\n\t\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\t\tr = -EINVAL;\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t\tvm = drm_priv_to_vm(pdd->drm_priv);\n\n\t\t\tr = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(r)) {\n\t\t\t\tpr_debug(\"failed %d to reserve bo\\n\", r);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor_each_set_bit(gpuidx, ctx->bitmap, MAX_GPU_INSTANCE) {\n\t\tpdd = kfd_process_device_from_gpuidx(ctx->process, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to find device idx %d\\n\", gpuidx);\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\tr = amdgpu_vm_validate_pt_bos(pdd->dev->adev,\n\t\t\t\t\t      drm_priv_to_vm(pdd->drm_priv),\n\t\t\t\t\t      svm_range_bo_validate, NULL);\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d validate pt bos\\n\", r);\n\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\treturn 0;\n\nunreserve_out:\n\tdrm_exec_fini(&ctx->exec);\n\treturn r;\n}\n\nstatic void svm_range_unreserve_bos(struct svm_validate_context *ctx)\n{\n\tdrm_exec_fini(&ctx->exec);\n}\n\nstatic void *kfd_svm_page_owner(struct kfd_process *p, int32_t gpuidx)\n{\n\tstruct kfd_process_device *pdd;\n\n\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\tif (!pdd)\n\t\treturn NULL;\n\n\treturn SVM_ADEV_PGMAP_OWNER(pdd->dev->adev);\n}\n\n \nstatic int svm_range_validate_and_map(struct mm_struct *mm,\n\t\t\t\t      struct svm_range *prange, int32_t gpuidx,\n\t\t\t\t      bool intr, bool wait, bool flush_tlb)\n{\n\tstruct svm_validate_context *ctx;\n\tunsigned long start, end, addr;\n\tstruct kfd_process *p;\n\tvoid *owner;\n\tint32_t idx;\n\tint r = 0;\n\n\tctx = kzalloc(sizeof(struct svm_validate_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\tctx->process = container_of(prange->svms, struct kfd_process, svms);\n\tctx->prange = prange;\n\tctx->intr = intr;\n\n\tif (gpuidx < MAX_GPU_INSTANCE) {\n\t\tbitmap_zero(ctx->bitmap, MAX_GPU_INSTANCE);\n\t\tbitmap_set(ctx->bitmap, gpuidx, 1);\n\t} else if (ctx->process->xnack_enabled) {\n\t\tbitmap_copy(ctx->bitmap, prange->bitmap_aip, MAX_GPU_INSTANCE);\n\n\t\t \n\t\tif (prange->actual_loc) {\n\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(ctx->process,\n\t\t\t\t\t\t\tprange->actual_loc);\n\t\t\tif (gpuidx < 0) {\n\t\t\t\tWARN_ONCE(1, \"failed get device by id 0x%x\\n\",\n\t\t\t\t\t prange->actual_loc);\n\t\t\t\tr = -EINVAL;\n\t\t\t\tgoto free_ctx;\n\t\t\t}\n\t\t\tif (test_bit(gpuidx, prange->bitmap_access))\n\t\t\t\tbitmap_set(ctx->bitmap, gpuidx, 1);\n\t\t}\n\n\t\t \n\t\tif (bitmap_empty(ctx->bitmap, MAX_GPU_INSTANCE)) {\n\t\t\tif (prange->mapped_to_gpu ||\n\t\t\t    prange->flags & KFD_IOCTL_SVM_FLAG_GPU_ALWAYS_MAPPED)\n\t\t\t\tbitmap_copy(ctx->bitmap, prange->bitmap_access, MAX_GPU_INSTANCE);\n\t\t}\n\t} else {\n\t\tbitmap_or(ctx->bitmap, prange->bitmap_access,\n\t\t\t  prange->bitmap_aip, MAX_GPU_INSTANCE);\n\t}\n\n\tif (bitmap_empty(ctx->bitmap, MAX_GPU_INSTANCE)) {\n\t\tr = 0;\n\t\tgoto free_ctx;\n\t}\n\n\tif (prange->actual_loc && !prange->ttm_res) {\n\t\t \n\t\tWARN_ONCE(1, \"VRAM BO missing during validation\\n\");\n\t\tr = -EINVAL;\n\t\tgoto free_ctx;\n\t}\n\n\tsvm_range_reserve_bos(ctx, intr);\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\towner = kfd_svm_page_owner(p, find_first_bit(ctx->bitmap,\n\t\t\t\t\t\tMAX_GPU_INSTANCE));\n\tfor_each_set_bit(idx, ctx->bitmap, MAX_GPU_INSTANCE) {\n\t\tif (kfd_svm_page_owner(p, idx) != owner) {\n\t\t\towner = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tstart = prange->start << PAGE_SHIFT;\n\tend = (prange->last + 1) << PAGE_SHIFT;\n\tfor (addr = start; !r && addr < end; ) {\n\t\tstruct hmm_range *hmm_range;\n\t\tstruct vm_area_struct *vma;\n\t\tunsigned long next = 0;\n\t\tunsigned long offset;\n\t\tunsigned long npages;\n\t\tbool readonly;\n\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (vma) {\n\t\t\treadonly = !(vma->vm_flags & VM_WRITE);\n\n\t\t\tnext = min(vma->vm_end, end);\n\t\t\tnpages = (next - addr) >> PAGE_SHIFT;\n\t\t\tWRITE_ONCE(p->svms.faulting_task, current);\n\t\t\tr = amdgpu_hmm_range_get_pages(&prange->notifier, addr, npages,\n\t\t\t\t\t\t       readonly, owner, NULL,\n\t\t\t\t\t\t       &hmm_range);\n\t\t\tWRITE_ONCE(p->svms.faulting_task, NULL);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"failed %d to get svm range pages\\n\", r);\n\t\t\t\tif (r == -EBUSY)\n\t\t\t\t\tr = -EAGAIN;\n\t\t\t}\n\t\t} else {\n\t\t\tr = -EFAULT;\n\t\t}\n\n\t\tif (!r) {\n\t\t\toffset = (addr - start) >> PAGE_SHIFT;\n\t\t\tr = svm_range_dma_map(prange, ctx->bitmap, offset, npages,\n\t\t\t\t\t      hmm_range->hmm_pfns);\n\t\t\tif (r)\n\t\t\t\tpr_debug(\"failed %d to dma map range\\n\", r);\n\t\t}\n\n\t\tsvm_range_lock(prange);\n\t\tif (!r && amdgpu_hmm_range_get_pages_done(hmm_range)) {\n\t\t\tpr_debug(\"hmm update the range, need validate again\\n\");\n\t\t\tr = -EAGAIN;\n\t\t}\n\n\t\tif (!r && !list_empty(&prange->child_list)) {\n\t\t\tpr_debug(\"range split by unmap in parallel, validate again\\n\");\n\t\t\tr = -EAGAIN;\n\t\t}\n\n\t\tif (!r)\n\t\t\tr = svm_range_map_to_gpus(prange, offset, npages, readonly,\n\t\t\t\t\t\t  ctx->bitmap, wait, flush_tlb);\n\n\t\tif (!r && next == end)\n\t\t\tprange->mapped_to_gpu = true;\n\n\t\tsvm_range_unlock(prange);\n\n\t\taddr = next;\n\t}\n\n\tsvm_range_unreserve_bos(ctx);\n\tif (!r)\n\t\tprange->validate_timestamp = ktime_get_boottime();\n\nfree_ctx:\n\tkfree(ctx);\n\n\treturn r;\n}\n\n \nvoid\nsvm_range_list_lock_and_flush_work(struct svm_range_list *svms,\n\t\t\t\t   struct mm_struct *mm)\n{\nretry_flush_work:\n\tflush_work(&svms->deferred_list_work);\n\tmmap_write_lock(mm);\n\n\tif (list_empty(&svms->deferred_range_list))\n\t\treturn;\n\tmmap_write_unlock(mm);\n\tpr_debug(\"retry flush\\n\");\n\tgoto retry_flush_work;\n}\n\nstatic void svm_range_restore_work(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint evicted_ranges;\n\tint invalid;\n\tint r;\n\n\tsvms = container_of(dwork, struct svm_range_list, restore_work);\n\tevicted_ranges = atomic_read(&svms->evicted_ranges);\n\tif (!evicted_ranges)\n\t\treturn;\n\n\tpr_debug(\"restore svm ranges\\n\");\n\n\tp = container_of(svms, struct kfd_process, svms);\n\tprocess_info = p->kgd_process_info;\n\n\t \n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p process mm gone\\n\", svms);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\tsvm_range_list_lock_and_flush_work(svms, mm);\n\tmutex_lock(&svms->lock);\n\n\tevicted_ranges = atomic_read(&svms->evicted_ranges);\n\n\tlist_for_each_entry(prange, &svms->list, list) {\n\t\tinvalid = atomic_read(&prange->invalid);\n\t\tif (!invalid)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"restoring svms 0x%p prange 0x%p [0x%lx %lx] inv %d\\n\",\n\t\t\t prange->svms, prange, prange->start, prange->last,\n\t\t\t invalid);\n\n\t\t \n\t\tmutex_lock(&prange->migrate_mutex);\n\n\t\tr = svm_range_validate_and_map(mm, prange, MAX_GPU_INSTANCE,\n\t\t\t\t\t       false, true, false);\n\t\tif (r)\n\t\t\tpr_debug(\"failed %d to map 0x%lx to gpus\\n\", r,\n\t\t\t\t prange->start);\n\n\t\tmutex_unlock(&prange->migrate_mutex);\n\t\tif (r)\n\t\t\tgoto out_reschedule;\n\n\t\tif (atomic_cmpxchg(&prange->invalid, invalid, 0) != invalid)\n\t\t\tgoto out_reschedule;\n\t}\n\n\tif (atomic_cmpxchg(&svms->evicted_ranges, evicted_ranges, 0) !=\n\t    evicted_ranges)\n\t\tgoto out_reschedule;\n\n\tevicted_ranges = 0;\n\n\tr = kgd2kfd_resume_mm(mm);\n\tif (r) {\n\t\t \n\t\tpr_debug(\"failed %d to resume KFD\\n\", r);\n\t}\n\n\tpr_debug(\"restore svm ranges successfully\\n\");\n\nout_reschedule:\n\tmutex_unlock(&svms->lock);\n\tmmap_write_unlock(mm);\n\tmutex_unlock(&process_info->lock);\n\n\t \n\tif (evicted_ranges) {\n\t\tpr_debug(\"reschedule to restore svm range\\n\");\n\t\tschedule_delayed_work(&svms->restore_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_SVM_RANGE_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n}\n\n \nstatic int\nsvm_range_evict(struct svm_range *prange, struct mm_struct *mm,\n\t\tunsigned long start, unsigned long last,\n\t\tenum mmu_notifier_event event)\n{\n\tstruct svm_range_list *svms = prange->svms;\n\tstruct svm_range *pchild;\n\tstruct kfd_process *p;\n\tint r = 0;\n\n\tp = container_of(svms, struct kfd_process, svms);\n\n\tpr_debug(\"invalidate svms 0x%p prange [0x%lx 0x%lx] [0x%lx 0x%lx]\\n\",\n\t\t svms, prange->start, prange->last, start, last);\n\n\tif (!p->xnack_enabled ||\n\t    (prange->flags & KFD_IOCTL_SVM_FLAG_GPU_ALWAYS_MAPPED)) {\n\t\tint evicted_ranges;\n\t\tbool mapped = prange->mapped_to_gpu;\n\n\t\tlist_for_each_entry(pchild, &prange->child_list, child_list) {\n\t\t\tif (!pchild->mapped_to_gpu)\n\t\t\t\tcontinue;\n\t\t\tmapped = true;\n\t\t\tmutex_lock_nested(&pchild->lock, 1);\n\t\t\tif (pchild->start <= last && pchild->last >= start) {\n\t\t\t\tpr_debug(\"increment pchild invalid [0x%lx 0x%lx]\\n\",\n\t\t\t\t\t pchild->start, pchild->last);\n\t\t\t\tatomic_inc(&pchild->invalid);\n\t\t\t}\n\t\t\tmutex_unlock(&pchild->lock);\n\t\t}\n\n\t\tif (!mapped)\n\t\t\treturn r;\n\n\t\tif (prange->start <= last && prange->last >= start)\n\t\t\tatomic_inc(&prange->invalid);\n\n\t\tevicted_ranges = atomic_inc_return(&svms->evicted_ranges);\n\t\tif (evicted_ranges != 1)\n\t\t\treturn r;\n\n\t\tpr_debug(\"evicting svms 0x%p range [0x%lx 0x%lx]\\n\",\n\t\t\t prange->svms, prange->start, prange->last);\n\n\t\t \n\t\tr = kgd2kfd_quiesce_mm(mm, KFD_QUEUE_EVICTION_TRIGGER_SVM);\n\t\tif (r)\n\t\t\tpr_debug(\"failed to quiesce KFD\\n\");\n\n\t\tpr_debug(\"schedule to restore svm %p ranges\\n\", svms);\n\t\tschedule_delayed_work(&svms->restore_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_SVM_RANGE_RESTORE_DELAY_MS));\n\t} else {\n\t\tunsigned long s, l;\n\t\tuint32_t trigger;\n\n\t\tif (event == MMU_NOTIFY_MIGRATE)\n\t\t\ttrigger = KFD_SVM_UNMAP_TRIGGER_MMU_NOTIFY_MIGRATE;\n\t\telse\n\t\t\ttrigger = KFD_SVM_UNMAP_TRIGGER_MMU_NOTIFY;\n\n\t\tpr_debug(\"invalidate unmap svms 0x%p [0x%lx 0x%lx] from GPUs\\n\",\n\t\t\t prange->svms, start, last);\n\t\tlist_for_each_entry(pchild, &prange->child_list, child_list) {\n\t\t\tmutex_lock_nested(&pchild->lock, 1);\n\t\t\ts = max(start, pchild->start);\n\t\t\tl = min(last, pchild->last);\n\t\t\tif (l >= s)\n\t\t\t\tsvm_range_unmap_from_gpus(pchild, s, l, trigger);\n\t\t\tmutex_unlock(&pchild->lock);\n\t\t}\n\t\ts = max(start, prange->start);\n\t\tl = min(last, prange->last);\n\t\tif (l >= s)\n\t\t\tsvm_range_unmap_from_gpus(prange, s, l, trigger);\n\t}\n\n\treturn r;\n}\n\nstatic struct svm_range *svm_range_clone(struct svm_range *old)\n{\n\tstruct svm_range *new;\n\n\tnew = svm_range_new(old->svms, old->start, old->last, false);\n\tif (!new)\n\t\treturn NULL;\n\tif (svm_range_copy_dma_addrs(new, old)) {\n\t\tsvm_range_free(new, false);\n\t\treturn NULL;\n\t}\n\tif (old->svm_bo) {\n\t\tnew->ttm_res = old->ttm_res;\n\t\tnew->offset = old->offset;\n\t\tnew->svm_bo = svm_range_bo_ref(old->svm_bo);\n\t\tspin_lock(&new->svm_bo->list_lock);\n\t\tlist_add(&new->svm_bo_list, &new->svm_bo->range_list);\n\t\tspin_unlock(&new->svm_bo->list_lock);\n\t}\n\tnew->flags = old->flags;\n\tnew->preferred_loc = old->preferred_loc;\n\tnew->prefetch_loc = old->prefetch_loc;\n\tnew->actual_loc = old->actual_loc;\n\tnew->granularity = old->granularity;\n\tnew->mapped_to_gpu = old->mapped_to_gpu;\n\tbitmap_copy(new->bitmap_access, old->bitmap_access, MAX_GPU_INSTANCE);\n\tbitmap_copy(new->bitmap_aip, old->bitmap_aip, MAX_GPU_INSTANCE);\n\n\treturn new;\n}\n\nvoid svm_range_set_max_pages(struct amdgpu_device *adev)\n{\n\tuint64_t max_pages;\n\tuint64_t pages, _pages;\n\tuint64_t min_pages = 0;\n\tint i, id;\n\n\tfor (i = 0; i < adev->kfd.dev->num_nodes; i++) {\n\t\tif (adev->kfd.dev->nodes[i]->xcp)\n\t\t\tid = adev->kfd.dev->nodes[i]->xcp->id;\n\t\telse\n\t\t\tid = -1;\n\t\tpages = KFD_XCP_MEMORY_SIZE(adev, id) >> 17;\n\t\tpages = clamp(pages, 1ULL << 9, 1ULL << 18);\n\t\tpages = rounddown_pow_of_two(pages);\n\t\tmin_pages = min_not_zero(min_pages, pages);\n\t}\n\n\tdo {\n\t\tmax_pages = READ_ONCE(max_svm_range_pages);\n\t\t_pages = min_not_zero(max_pages, min_pages);\n\t} while (cmpxchg(&max_svm_range_pages, max_pages, _pages) != max_pages);\n}\n\nstatic int\nsvm_range_split_new(struct svm_range_list *svms, uint64_t start, uint64_t last,\n\t\t    uint64_t max_pages, struct list_head *insert_list,\n\t\t    struct list_head *update_list)\n{\n\tstruct svm_range *prange;\n\tuint64_t l;\n\n\tpr_debug(\"max_svm_range_pages 0x%llx adding [0x%llx 0x%llx]\\n\",\n\t\t max_pages, start, last);\n\n\twhile (last >= start) {\n\t\tl = min(last, ALIGN_DOWN(start + max_pages, max_pages) - 1);\n\n\t\tprange = svm_range_new(svms, start, l, true);\n\t\tif (!prange)\n\t\t\treturn -ENOMEM;\n\t\tlist_add(&prange->list, insert_list);\n\t\tlist_add(&prange->update_list, update_list);\n\n\t\tstart = l + 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int\nsvm_range_add(struct kfd_process *p, uint64_t start, uint64_t size,\n\t      uint32_t nattr, struct kfd_ioctl_svm_attribute *attrs,\n\t      struct list_head *update_list, struct list_head *insert_list,\n\t      struct list_head *remove_list)\n{\n\tunsigned long last = start + size - 1UL;\n\tstruct svm_range_list *svms = &p->svms;\n\tstruct interval_tree_node *node;\n\tstruct svm_range *prange;\n\tstruct svm_range *tmp;\n\tstruct list_head new_list;\n\tint r = 0;\n\n\tpr_debug(\"svms 0x%p [0x%llx 0x%lx]\\n\", &p->svms, start, last);\n\n\tINIT_LIST_HEAD(update_list);\n\tINIT_LIST_HEAD(insert_list);\n\tINIT_LIST_HEAD(remove_list);\n\tINIT_LIST_HEAD(&new_list);\n\n\tnode = interval_tree_iter_first(&svms->objects, start, last);\n\twhile (node) {\n\t\tstruct interval_tree_node *next;\n\t\tunsigned long next_start;\n\n\t\tpr_debug(\"found overlap node [0x%lx 0x%lx]\\n\", node->start,\n\t\t\t node->last);\n\n\t\tprange = container_of(node, struct svm_range, it_node);\n\t\tnext = interval_tree_iter_next(node, start, last);\n\t\tnext_start = min(node->last, last) + 1;\n\n\t\tif (svm_range_is_same_attrs(p, prange, nattr, attrs) &&\n\t\t    prange->mapped_to_gpu) {\n\t\t\t \n\t\t} else if (node->start < start || node->last > last) {\n\t\t\t \n\t\t\tstruct svm_range *old = prange;\n\n\t\t\tprange = svm_range_clone(old);\n\t\t\tif (!prange) {\n\t\t\t\tr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tlist_add(&old->update_list, remove_list);\n\t\t\tlist_add(&prange->list, insert_list);\n\t\t\tlist_add(&prange->update_list, update_list);\n\n\t\t\tif (node->start < start) {\n\t\t\t\tpr_debug(\"change old range start\\n\");\n\t\t\t\tr = svm_range_split_head(prange, start,\n\t\t\t\t\t\t\t insert_list);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (node->last > last) {\n\t\t\t\tpr_debug(\"change old range last\\n\");\n\t\t\t\tr = svm_range_split_tail(prange, last,\n\t\t\t\t\t\t\t insert_list);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tlist_add(&prange->update_list, update_list);\n\t\t}\n\n\t\t \n\t\tif (node->start > start) {\n\t\t\tr = svm_range_split_new(svms, start, node->start - 1,\n\t\t\t\t\t\tREAD_ONCE(max_svm_range_pages),\n\t\t\t\t\t\t&new_list, update_list);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tnode = next;\n\t\tstart = next_start;\n\t}\n\n\t \n\tif (start <= last)\n\t\tr = svm_range_split_new(svms, start, last,\n\t\t\t\t\tREAD_ONCE(max_svm_range_pages),\n\t\t\t\t\t&new_list, update_list);\n\nout:\n\tif (r) {\n\t\tlist_for_each_entry_safe(prange, tmp, insert_list, list)\n\t\t\tsvm_range_free(prange, false);\n\t\tlist_for_each_entry_safe(prange, tmp, &new_list, list)\n\t\t\tsvm_range_free(prange, true);\n\t} else {\n\t\tlist_splice(&new_list, insert_list);\n\t}\n\n\treturn r;\n}\n\nstatic void\nsvm_range_update_notifier_and_interval_tree(struct mm_struct *mm,\n\t\t\t\t\t    struct svm_range *prange)\n{\n\tunsigned long start;\n\tunsigned long last;\n\n\tstart = prange->notifier.interval_tree.start >> PAGE_SHIFT;\n\tlast = prange->notifier.interval_tree.last >> PAGE_SHIFT;\n\n\tif (prange->start == start && prange->last == last)\n\t\treturn;\n\n\tpr_debug(\"up notifier 0x%p prange 0x%p [0x%lx 0x%lx] [0x%lx 0x%lx]\\n\",\n\t\t  prange->svms, prange, start, last, prange->start,\n\t\t  prange->last);\n\n\tif (start != 0 && last != 0) {\n\t\tinterval_tree_remove(&prange->it_node, &prange->svms->objects);\n\t\tsvm_range_remove_notifier(prange);\n\t}\n\tprange->it_node.start = prange->start;\n\tprange->it_node.last = prange->last;\n\n\tinterval_tree_insert(&prange->it_node, &prange->svms->objects);\n\tsvm_range_add_notifier_locked(mm, prange);\n}\n\nstatic void\nsvm_range_handle_list_op(struct svm_range_list *svms, struct svm_range *prange,\n\t\t\t struct mm_struct *mm)\n{\n\tswitch (prange->work_item.op) {\n\tcase SVM_OP_NULL:\n\t\tpr_debug(\"NULL OP 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\tbreak;\n\tcase SVM_OP_UNMAP_RANGE:\n\t\tpr_debug(\"remove 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\tsvm_range_unlink(prange);\n\t\tsvm_range_remove_notifier(prange);\n\t\tsvm_range_free(prange, true);\n\t\tbreak;\n\tcase SVM_OP_UPDATE_RANGE_NOTIFIER:\n\t\tpr_debug(\"update notifier 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\tsvm_range_update_notifier_and_interval_tree(mm, prange);\n\t\tbreak;\n\tcase SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP:\n\t\tpr_debug(\"update and map 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\tsvm_range_update_notifier_and_interval_tree(mm, prange);\n\t\t \n\t\tbreak;\n\tcase SVM_OP_ADD_RANGE:\n\t\tpr_debug(\"add 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", svms, prange,\n\t\t\t prange->start, prange->last);\n\t\tsvm_range_add_to_svms(prange);\n\t\tsvm_range_add_notifier_locked(mm, prange);\n\t\tbreak;\n\tcase SVM_OP_ADD_RANGE_AND_MAP:\n\t\tpr_debug(\"add and map 0x%p prange 0x%p [0x%lx 0x%lx]\\n\", svms,\n\t\t\t prange, prange->start, prange->last);\n\t\tsvm_range_add_to_svms(prange);\n\t\tsvm_range_add_notifier_locked(mm, prange);\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Unknown prange 0x%p work op %d\\n\", prange,\n\t\t\t prange->work_item.op);\n\t}\n}\n\nstatic void svm_range_drain_retry_fault(struct svm_range_list *svms)\n{\n\tstruct kfd_process_device *pdd;\n\tstruct kfd_process *p;\n\tint drain;\n\tuint32_t i;\n\n\tp = container_of(svms, struct kfd_process, svms);\n\nrestart:\n\tdrain = atomic_read(&svms->drain_pagefaults);\n\tif (!drain)\n\t\treturn;\n\n\tfor_each_set_bit(i, svms->bitmap_supported, p->n_pdds) {\n\t\tpdd = p->pdds[i];\n\t\tif (!pdd)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"drain retry fault gpu %d svms %p\\n\", i, svms);\n\n\t\tamdgpu_ih_wait_on_checkpoint_process_ts(pdd->dev->adev,\n\t\t\t\tpdd->dev->adev->irq.retry_cam_enabled ?\n\t\t\t\t&pdd->dev->adev->irq.ih :\n\t\t\t\t&pdd->dev->adev->irq.ih1);\n\n\t\tif (pdd->dev->adev->irq.retry_cam_enabled)\n\t\t\tamdgpu_ih_wait_on_checkpoint_process_ts(pdd->dev->adev,\n\t\t\t\t&pdd->dev->adev->irq.ih_soft);\n\n\n\t\tpr_debug(\"drain retry fault gpu %d svms 0x%p done\\n\", i, svms);\n\t}\n\tif (atomic_cmpxchg(&svms->drain_pagefaults, drain, 0) != drain)\n\t\tgoto restart;\n}\n\nstatic void svm_range_deferred_list_work(struct work_struct *work)\n{\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct mm_struct *mm;\n\n\tsvms = container_of(work, struct svm_range_list, deferred_list_work);\n\tpr_debug(\"enter svms 0x%p\\n\", svms);\n\n\tspin_lock(&svms->deferred_list_lock);\n\twhile (!list_empty(&svms->deferred_range_list)) {\n\t\tprange = list_first_entry(&svms->deferred_range_list,\n\t\t\t\t\t  struct svm_range, deferred_list);\n\t\tspin_unlock(&svms->deferred_list_lock);\n\n\t\tpr_debug(\"prange 0x%p [0x%lx 0x%lx] op %d\\n\", prange,\n\t\t\t prange->start, prange->last, prange->work_item.op);\n\n\t\tmm = prange->work_item.mm;\nretry:\n\t\tmmap_write_lock(mm);\n\n\t\t \n\t\tif (unlikely(atomic_read(&svms->drain_pagefaults))) {\n\t\t\tmmap_write_unlock(mm);\n\t\t\tsvm_range_drain_retry_fault(svms);\n\t\t\tgoto retry;\n\t\t}\n\n\t\t \n\t\tspin_lock(&svms->deferred_list_lock);\n\t\tlist_del_init(&prange->deferred_list);\n\t\tspin_unlock(&svms->deferred_list_lock);\n\n\t\tmutex_lock(&svms->lock);\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\twhile (!list_empty(&prange->child_list)) {\n\t\t\tstruct svm_range *pchild;\n\n\t\t\tpchild = list_first_entry(&prange->child_list,\n\t\t\t\t\t\tstruct svm_range, child_list);\n\t\t\tpr_debug(\"child prange 0x%p op %d\\n\", pchild,\n\t\t\t\t pchild->work_item.op);\n\t\t\tlist_del_init(&pchild->child_list);\n\t\t\tsvm_range_handle_list_op(svms, pchild, mm);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tsvm_range_handle_list_op(svms, prange, mm);\n\t\tmutex_unlock(&svms->lock);\n\t\tmmap_write_unlock(mm);\n\n\t\t \n\t\tmmput(mm);\n\n\t\tspin_lock(&svms->deferred_list_lock);\n\t}\n\tspin_unlock(&svms->deferred_list_lock);\n\tpr_debug(\"exit svms 0x%p\\n\", svms);\n}\n\nvoid\nsvm_range_add_list_work(struct svm_range_list *svms, struct svm_range *prange,\n\t\t\tstruct mm_struct *mm, enum svm_work_list_ops op)\n{\n\tspin_lock(&svms->deferred_list_lock);\n\t \n\tif (!list_empty(&prange->deferred_list)) {\n\t\tpr_debug(\"update exist prange 0x%p work op %d\\n\", prange, op);\n\t\tWARN_ONCE(prange->work_item.mm != mm, \"unmatch mm\\n\");\n\t\tif (op != SVM_OP_NULL &&\n\t\t    prange->work_item.op != SVM_OP_UNMAP_RANGE)\n\t\t\tprange->work_item.op = op;\n\t} else {\n\t\tprange->work_item.op = op;\n\n\t\t \n\t\tmmget(mm);\n\t\tprange->work_item.mm = mm;\n\t\tlist_add_tail(&prange->deferred_list,\n\t\t\t      &prange->svms->deferred_range_list);\n\t\tpr_debug(\"add prange 0x%p [0x%lx 0x%lx] to work list op %d\\n\",\n\t\t\t prange, prange->start, prange->last, op);\n\t}\n\tspin_unlock(&svms->deferred_list_lock);\n}\n\nvoid schedule_deferred_list_work(struct svm_range_list *svms)\n{\n\tspin_lock(&svms->deferred_list_lock);\n\tif (!list_empty(&svms->deferred_range_list))\n\t\tschedule_work(&svms->deferred_list_work);\n\tspin_unlock(&svms->deferred_list_lock);\n}\n\nstatic void\nsvm_range_unmap_split(struct mm_struct *mm, struct svm_range *parent,\n\t\t      struct svm_range *prange, unsigned long start,\n\t\t      unsigned long last)\n{\n\tstruct svm_range *head;\n\tstruct svm_range *tail;\n\n\tif (prange->work_item.op == SVM_OP_UNMAP_RANGE) {\n\t\tpr_debug(\"prange 0x%p [0x%lx 0x%lx] is already freed\\n\", prange,\n\t\t\t prange->start, prange->last);\n\t\treturn;\n\t}\n\tif (start > prange->last || last < prange->start)\n\t\treturn;\n\n\thead = tail = prange;\n\tif (start > prange->start)\n\t\tsvm_range_split(prange, prange->start, start - 1, &tail);\n\tif (last < tail->last)\n\t\tsvm_range_split(tail, last + 1, tail->last, &head);\n\n\tif (head != prange && tail != prange) {\n\t\tsvm_range_add_child(parent, mm, head, SVM_OP_UNMAP_RANGE);\n\t\tsvm_range_add_child(parent, mm, tail, SVM_OP_ADD_RANGE);\n\t} else if (tail != prange) {\n\t\tsvm_range_add_child(parent, mm, tail, SVM_OP_UNMAP_RANGE);\n\t} else if (head != prange) {\n\t\tsvm_range_add_child(parent, mm, head, SVM_OP_UNMAP_RANGE);\n\t} else if (parent != prange) {\n\t\tprange->work_item.op = SVM_OP_UNMAP_RANGE;\n\t}\n}\n\nstatic void\nsvm_range_unmap_from_cpu(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t unsigned long start, unsigned long last)\n{\n\tuint32_t trigger = KFD_SVM_UNMAP_TRIGGER_UNMAP_FROM_CPU;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *pchild;\n\tstruct kfd_process *p;\n\tunsigned long s, l;\n\tbool unmap_parent;\n\n\tp = kfd_lookup_process_by_mm(mm);\n\tif (!p)\n\t\treturn;\n\tsvms = &p->svms;\n\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx] [0x%lx 0x%lx]\\n\", svms,\n\t\t prange, prange->start, prange->last, start, last);\n\n\t \n\tatomic_inc(&svms->drain_pagefaults);\n\n\tunmap_parent = start <= prange->start && last >= prange->last;\n\n\tlist_for_each_entry(pchild, &prange->child_list, child_list) {\n\t\tmutex_lock_nested(&pchild->lock, 1);\n\t\ts = max(start, pchild->start);\n\t\tl = min(last, pchild->last);\n\t\tif (l >= s)\n\t\t\tsvm_range_unmap_from_gpus(pchild, s, l, trigger);\n\t\tsvm_range_unmap_split(mm, prange, pchild, start, last);\n\t\tmutex_unlock(&pchild->lock);\n\t}\n\ts = max(start, prange->start);\n\tl = min(last, prange->last);\n\tif (l >= s)\n\t\tsvm_range_unmap_from_gpus(prange, s, l, trigger);\n\tsvm_range_unmap_split(mm, prange, prange, start, last);\n\n\tif (unmap_parent)\n\t\tsvm_range_add_list_work(svms, prange, mm, SVM_OP_UNMAP_RANGE);\n\telse\n\t\tsvm_range_add_list_work(svms, prange, mm,\n\t\t\t\t\tSVM_OP_UPDATE_RANGE_NOTIFIER);\n\tschedule_deferred_list_work(svms);\n\n\tkfd_unref_process(p);\n}\n\n \nstatic bool\nsvm_range_cpu_invalidate_pagetables(struct mmu_interval_notifier *mni,\n\t\t\t\t    const struct mmu_notifier_range *range,\n\t\t\t\t    unsigned long cur_seq)\n{\n\tstruct svm_range *prange;\n\tunsigned long start;\n\tunsigned long last;\n\n\tif (range->event == MMU_NOTIFY_RELEASE)\n\t\treturn true;\n\tif (!mmget_not_zero(mni->mm))\n\t\treturn true;\n\n\tstart = mni->interval_tree.start;\n\tlast = mni->interval_tree.last;\n\tstart = max(start, range->start) >> PAGE_SHIFT;\n\tlast = min(last, range->end - 1) >> PAGE_SHIFT;\n\tpr_debug(\"[0x%lx 0x%lx] range[0x%lx 0x%lx] notifier[0x%lx 0x%lx] %d\\n\",\n\t\t start, last, range->start >> PAGE_SHIFT,\n\t\t (range->end - 1) >> PAGE_SHIFT,\n\t\t mni->interval_tree.start >> PAGE_SHIFT,\n\t\t mni->interval_tree.last >> PAGE_SHIFT, range->event);\n\n\tprange = container_of(mni, struct svm_range, notifier);\n\n\tsvm_range_lock(prange);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tswitch (range->event) {\n\tcase MMU_NOTIFY_UNMAP:\n\t\tsvm_range_unmap_from_cpu(mni->mm, prange, start, last);\n\t\tbreak;\n\tdefault:\n\t\tsvm_range_evict(prange, mni->mm, start, last, range->event);\n\t\tbreak;\n\t}\n\n\tsvm_range_unlock(prange);\n\tmmput(mni->mm);\n\n\treturn true;\n}\n\n \nstruct svm_range *\nsvm_range_from_addr(struct svm_range_list *svms, unsigned long addr,\n\t\t    struct svm_range **parent)\n{\n\tstruct interval_tree_node *node;\n\tstruct svm_range *prange;\n\tstruct svm_range *pchild;\n\n\tnode = interval_tree_iter_first(&svms->objects, addr, addr);\n\tif (!node)\n\t\treturn NULL;\n\n\tprange = container_of(node, struct svm_range, it_node);\n\tpr_debug(\"address 0x%lx prange [0x%lx 0x%lx] node [0x%lx 0x%lx]\\n\",\n\t\t addr, prange->start, prange->last, node->start, node->last);\n\n\tif (addr >= prange->start && addr <= prange->last) {\n\t\tif (parent)\n\t\t\t*parent = prange;\n\t\treturn prange;\n\t}\n\tlist_for_each_entry(pchild, &prange->child_list, child_list)\n\t\tif (addr >= pchild->start && addr <= pchild->last) {\n\t\t\tpr_debug(\"found address 0x%lx pchild [0x%lx 0x%lx]\\n\",\n\t\t\t\t addr, pchild->start, pchild->last);\n\t\t\tif (parent)\n\t\t\t\t*parent = prange;\n\t\t\treturn pchild;\n\t\t}\n\n\treturn NULL;\n}\n\n \nstatic int32_t\nsvm_range_best_restore_location(struct svm_range *prange,\n\t\t\t\tstruct kfd_node *node,\n\t\t\t\tint32_t *gpuidx)\n{\n\tstruct kfd_node *bo_node, *preferred_node;\n\tstruct kfd_process *p;\n\tuint32_t gpuid;\n\tint r;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\tr = kfd_process_gpuid_from_node(p, node, &gpuid, gpuidx);\n\tif (r < 0) {\n\t\tpr_debug(\"failed to get gpuid from kgd\\n\");\n\t\treturn -1;\n\t}\n\n\tif (node->adev->gmc.is_app_apu)\n\t\treturn 0;\n\n\tif (prange->preferred_loc == gpuid ||\n\t    prange->preferred_loc == KFD_IOCTL_SVM_LOCATION_SYSMEM) {\n\t\treturn prange->preferred_loc;\n\t} else if (prange->preferred_loc != KFD_IOCTL_SVM_LOCATION_UNDEFINED) {\n\t\tpreferred_node = svm_range_get_node_by_id(prange, prange->preferred_loc);\n\t\tif (preferred_node && svm_nodes_in_same_hive(node, preferred_node))\n\t\t\treturn prange->preferred_loc;\n\t\t \n\t}\n\n\tif (test_bit(*gpuidx, prange->bitmap_access))\n\t\treturn gpuid;\n\n\tif (test_bit(*gpuidx, prange->bitmap_aip)) {\n\t\tif (!prange->actual_loc)\n\t\t\treturn 0;\n\n\t\tbo_node = svm_range_get_node_by_id(prange, prange->actual_loc);\n\t\tif (bo_node && svm_nodes_in_same_hive(node, bo_node))\n\t\t\treturn prange->actual_loc;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\treturn -1;\n}\n\nstatic int\nsvm_range_get_range_boundaries(struct kfd_process *p, int64_t addr,\n\t\t\t       unsigned long *start, unsigned long *last,\n\t\t\t       bool *is_heap_stack)\n{\n\tstruct vm_area_struct *vma;\n\tstruct interval_tree_node *node;\n\tunsigned long start_limit, end_limit;\n\n\tvma = vma_lookup(p->mm, addr << PAGE_SHIFT);\n\tif (!vma) {\n\t\tpr_debug(\"VMA does not exist in address [0x%llx]\\n\", addr);\n\t\treturn -EFAULT;\n\t}\n\n\t*is_heap_stack = vma_is_initial_heap(vma) || vma_is_initial_stack(vma);\n\n\tstart_limit = max(vma->vm_start >> PAGE_SHIFT,\n\t\t      (unsigned long)ALIGN_DOWN(addr, 2UL << 8));\n\tend_limit = min(vma->vm_end >> PAGE_SHIFT,\n\t\t    (unsigned long)ALIGN(addr + 1, 2UL << 8));\n\t \n\tnode = interval_tree_iter_first(&p->svms.objects, addr + 1, ULONG_MAX);\n\tif (node) {\n\t\tend_limit = min(end_limit, node->start);\n\t\t \n\t\tnode = container_of(rb_prev(&node->rb),\n\t\t\t\t    struct interval_tree_node, rb);\n\t} else {\n\t\t \n\t\tnode = container_of(rb_last(&p->svms.objects.rb_root),\n\t\t\t\t    struct interval_tree_node, rb);\n\t}\n\tif (node) {\n\t\tif (node->last >= addr) {\n\t\t\tWARN(1, \"Overlap with prev node and page fault addr\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tstart_limit = max(start_limit, node->last + 1);\n\t}\n\n\t*start = start_limit;\n\t*last = end_limit - 1;\n\n\tpr_debug(\"vma [0x%lx 0x%lx] range [0x%lx 0x%lx] is_heap_stack %d\\n\",\n\t\t vma->vm_start >> PAGE_SHIFT, vma->vm_end >> PAGE_SHIFT,\n\t\t *start, *last, *is_heap_stack);\n\n\treturn 0;\n}\n\nstatic int\nsvm_range_check_vm_userptr(struct kfd_process *p, uint64_t start, uint64_t last,\n\t\t\t   uint64_t *bo_s, uint64_t *bo_l)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct interval_tree_node *node;\n\tstruct amdgpu_bo *bo = NULL;\n\tunsigned long userptr;\n\tuint32_t i;\n\tint r;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct amdgpu_vm *vm;\n\n\t\tif (!p->pdds[i]->drm_priv)\n\t\t\tcontinue;\n\n\t\tvm = drm_priv_to_vm(p->pdds[i]->drm_priv);\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tnode = interval_tree_iter_first(&vm->va, 0, ~0ULL);\n\t\twhile (node) {\n\t\t\tmapping = container_of((struct rb_node *)node,\n\t\t\t\t\t       struct amdgpu_bo_va_mapping, rb);\n\t\t\tbo = mapping->bo_va->base.bo;\n\n\t\t\tif (!amdgpu_ttm_tt_affect_userptr(bo->tbo.ttm,\n\t\t\t\t\t\t\t start << PAGE_SHIFT,\n\t\t\t\t\t\t\t last << PAGE_SHIFT,\n\t\t\t\t\t\t\t &userptr)) {\n\t\t\t\tnode = interval_tree_iter_next(node, 0, ~0ULL);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tpr_debug(\"[0x%llx 0x%llx] already userptr mapped\\n\",\n\t\t\t\t start, last);\n\t\t\tif (bo_s && bo_l) {\n\t\t\t\t*bo_s = userptr >> PAGE_SHIFT;\n\t\t\t\t*bo_l = *bo_s + bo->tbo.ttm->num_pages - 1;\n\t\t\t}\n\t\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t\t\treturn -EADDRINUSE;\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\treturn 0;\n}\n\nstatic struct\nsvm_range *svm_range_create_unregistered_range(struct kfd_node *node,\n\t\t\t\t\t\tstruct kfd_process *p,\n\t\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\t\tint64_t addr)\n{\n\tstruct svm_range *prange = NULL;\n\tunsigned long start, last;\n\tuint32_t gpuid, gpuidx;\n\tbool is_heap_stack;\n\tuint64_t bo_s = 0;\n\tuint64_t bo_l = 0;\n\tint r;\n\n\tif (svm_range_get_range_boundaries(p, addr, &start, &last,\n\t\t\t\t\t   &is_heap_stack))\n\t\treturn NULL;\n\n\tr = svm_range_check_vm(p, start, last, &bo_s, &bo_l);\n\tif (r != -EADDRINUSE)\n\t\tr = svm_range_check_vm_userptr(p, start, last, &bo_s, &bo_l);\n\n\tif (r == -EADDRINUSE) {\n\t\tif (addr >= bo_s && addr <= bo_l)\n\t\t\treturn NULL;\n\n\t\t \n\t\tstart = addr;\n\t\tlast = addr;\n\t}\n\n\tprange = svm_range_new(&p->svms, start, last, true);\n\tif (!prange) {\n\t\tpr_debug(\"Failed to create prange in address [0x%llx]\\n\", addr);\n\t\treturn NULL;\n\t}\n\tif (kfd_process_gpuid_from_node(p, node, &gpuid, &gpuidx)) {\n\t\tpr_debug(\"failed to get gpuid from kgd\\n\");\n\t\tsvm_range_free(prange, true);\n\t\treturn NULL;\n\t}\n\n\tif (is_heap_stack)\n\t\tprange->preferred_loc = KFD_IOCTL_SVM_LOCATION_SYSMEM;\n\n\tsvm_range_add_to_svms(prange);\n\tsvm_range_add_notifier_locked(mm, prange);\n\n\treturn prange;\n}\n\n \nstatic bool svm_range_skip_recover(struct svm_range *prange)\n{\n\tstruct svm_range_list *svms = prange->svms;\n\n\tspin_lock(&svms->deferred_list_lock);\n\tif (list_empty(&prange->deferred_list) &&\n\t    list_empty(&prange->child_list)) {\n\t\tspin_unlock(&svms->deferred_list_lock);\n\t\treturn false;\n\t}\n\tspin_unlock(&svms->deferred_list_lock);\n\n\tif (prange->work_item.op == SVM_OP_UNMAP_RANGE) {\n\t\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx] unmapped\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\treturn true;\n\t}\n\tif (prange->work_item.op == SVM_OP_ADD_RANGE_AND_MAP ||\n\t    prange->work_item.op == SVM_OP_ADD_RANGE) {\n\t\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx] not added yet\\n\",\n\t\t\t svms, prange, prange->start, prange->last);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void\nsvm_range_count_fault(struct kfd_node *node, struct kfd_process *p,\n\t\t      int32_t gpuidx)\n{\n\tstruct kfd_process_device *pdd;\n\n\t \n\tif (gpuidx == MAX_GPU_INSTANCE) {\n\t\tuint32_t gpuid;\n\t\tint r;\n\n\t\tr = kfd_process_gpuid_from_node(p, node, &gpuid, &gpuidx);\n\t\tif (r < 0)\n\t\t\treturn;\n\t}\n\n\t \n\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\tif (pdd)\n\t\tWRITE_ONCE(pdd->faults, pdd->faults + 1);\n}\n\nstatic bool\nsvm_fault_allowed(struct vm_area_struct *vma, bool write_fault)\n{\n\tunsigned long requested = VM_READ;\n\n\tif (write_fault)\n\t\trequested |= VM_WRITE;\n\n\tpr_debug(\"requested 0x%lx, vma permission flags 0x%lx\\n\", requested,\n\t\tvma->vm_flags);\n\treturn (vma->vm_flags & requested) == requested;\n}\n\nint\nsvm_range_restore_pages(struct amdgpu_device *adev, unsigned int pasid,\n\t\t\tuint32_t vmid, uint32_t node_id,\n\t\t\tuint64_t addr, bool write_fault)\n{\n\tstruct mm_struct *mm = NULL;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tktime_t timestamp = ktime_get_boottime();\n\tstruct kfd_node *node;\n\tint32_t best_loc;\n\tint32_t gpuidx = MAX_GPU_INSTANCE;\n\tbool write_locked = false;\n\tstruct vm_area_struct *vma;\n\tbool migration = false;\n\tint r = 0;\n\n\tif (!KFD_IS_SVM_API_SUPPORTED(adev)) {\n\t\tpr_debug(\"device does not support SVM\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tp = kfd_lookup_process_by_pasid(pasid);\n\tif (!p) {\n\t\tpr_debug(\"kfd process not founded pasid 0x%x\\n\", pasid);\n\t\treturn 0;\n\t}\n\tsvms = &p->svms;\n\n\tpr_debug(\"restoring svms 0x%p fault address 0x%llx\\n\", svms, addr);\n\n\tif (atomic_read(&svms->drain_pagefaults)) {\n\t\tpr_debug(\"draining retry fault, drop fault 0x%llx\\n\", addr);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tif (!p->xnack_enabled) {\n\t\tpr_debug(\"XNACK not enabled for pasid 0x%x\\n\", pasid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_debug(\"svms 0x%p failed to get mm\\n\", svms);\n\t\tr = 0;\n\t\tgoto out;\n\t}\n\n\tnode = kfd_node_by_irq_ids(adev, node_id, vmid);\n\tif (!node) {\n\t\tpr_debug(\"kfd node does not exist node_id: %d, vmid: %d\\n\", node_id,\n\t\t\t vmid);\n\t\tr = -EFAULT;\n\t\tgoto out;\n\t}\n\tmmap_read_lock(mm);\nretry_write_locked:\n\tmutex_lock(&svms->lock);\n\tprange = svm_range_from_addr(svms, addr, NULL);\n\tif (!prange) {\n\t\tpr_debug(\"failed to find prange svms 0x%p address [0x%llx]\\n\",\n\t\t\t svms, addr);\n\t\tif (!write_locked) {\n\t\t\t \n\t\t\tmutex_unlock(&svms->lock);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tmmap_write_lock(mm);\n\t\t\twrite_locked = true;\n\t\t\tgoto retry_write_locked;\n\t\t}\n\t\tprange = svm_range_create_unregistered_range(node, p, mm, addr);\n\t\tif (!prange) {\n\t\t\tpr_debug(\"failed to create unregistered range svms 0x%p address [0x%llx]\\n\",\n\t\t\t\t svms, addr);\n\t\t\tmmap_write_downgrade(mm);\n\t\t\tr = -EFAULT;\n\t\t\tgoto out_unlock_svms;\n\t\t}\n\t}\n\tif (write_locked)\n\t\tmmap_write_downgrade(mm);\n\n\tmutex_lock(&prange->migrate_mutex);\n\n\tif (svm_range_skip_recover(prange)) {\n\t\tamdgpu_gmc_filter_faults_remove(node->adev, addr, pasid);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t \n\tif (ktime_before(timestamp, ktime_add_ns(prange->validate_timestamp,\n\t\t\t\tAMDGPU_SVM_RANGE_RETRY_FAULT_PENDING))) {\n\t\tpr_debug(\"svms 0x%p [0x%lx %lx] already restored\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\t \n\tvma = vma_lookup(mm, addr << PAGE_SHIFT);\n\tif (!vma) {\n\t\tpr_debug(\"address 0x%llx VMA is removed\\n\", addr);\n\t\tr = 0;\n\t\tgoto out_unlock_range;\n\t}\n\n\tif (!svm_fault_allowed(vma, write_fault)) {\n\t\tpr_debug(\"fault addr 0x%llx no %s permission\\n\", addr,\n\t\t\twrite_fault ? \"write\" : \"read\");\n\t\tr = -EPERM;\n\t\tgoto out_unlock_range;\n\t}\n\n\tbest_loc = svm_range_best_restore_location(prange, node, &gpuidx);\n\tif (best_loc == -1) {\n\t\tpr_debug(\"svms %p failed get best restore loc [0x%lx 0x%lx]\\n\",\n\t\t\t svms, prange->start, prange->last);\n\t\tr = -EACCES;\n\t\tgoto out_unlock_range;\n\t}\n\n\tpr_debug(\"svms %p [0x%lx 0x%lx] best restore 0x%x, actual loc 0x%x\\n\",\n\t\t svms, prange->start, prange->last, best_loc,\n\t\t prange->actual_loc);\n\n\tkfd_smi_event_page_fault_start(node, p->lead_thread->pid, addr,\n\t\t\t\t       write_fault, timestamp);\n\n\tif (prange->actual_loc != best_loc) {\n\t\tmigration = true;\n\t\tif (best_loc) {\n\t\t\tr = svm_migrate_to_vram(prange, best_loc, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU);\n\t\t\tif (r) {\n\t\t\t\tpr_debug(\"svm_migrate_to_vram failed (%d) at %llx, falling back to system memory\\n\",\n\t\t\t\t\t r, addr);\n\t\t\t\t \n\t\t\t\tif (prange->actual_loc)\n\t\t\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\t   KFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\t   NULL);\n\t\t\t\telse\n\t\t\t\t\tr = 0;\n\t\t\t}\n\t\t} else {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PAGEFAULT_GPU,\n\t\t\t\t\tNULL);\n\t\t}\n\t\tif (r) {\n\t\t\tpr_debug(\"failed %d to migrate svms %p [0x%lx 0x%lx]\\n\",\n\t\t\t\t r, svms, prange->start, prange->last);\n\t\t\tgoto out_unlock_range;\n\t\t}\n\t}\n\n\tr = svm_range_validate_and_map(mm, prange, gpuidx, false, false, false);\n\tif (r)\n\t\tpr_debug(\"failed %d to map svms 0x%p [0x%lx 0x%lx] to gpus\\n\",\n\t\t\t r, svms, prange->start, prange->last);\n\n\tkfd_smi_event_page_fault_end(node, p->lead_thread->pid, addr,\n\t\t\t\t     migration);\n\nout_unlock_range:\n\tmutex_unlock(&prange->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\n\n\tsvm_range_count_fault(node, p, gpuidx);\n\n\tmmput(mm);\nout:\n\tkfd_unref_process(p);\n\n\tif (r == -EAGAIN) {\n\t\tpr_debug(\"recover vm fault later\\n\");\n\t\tamdgpu_gmc_filter_faults_remove(node->adev, addr, pasid);\n\t\tr = 0;\n\t}\n\treturn r;\n}\n\nint\nsvm_range_switch_xnack_reserve_mem(struct kfd_process *p, bool xnack_enabled)\n{\n\tstruct svm_range *prange, *pchild;\n\tuint64_t reserved_size = 0;\n\tuint64_t size;\n\tint r = 0;\n\n\tpr_debug(\"switching xnack from %d to %d\\n\", p->xnack_enabled, xnack_enabled);\n\n\tmutex_lock(&p->svms.lock);\n\n\tlist_for_each_entry(prange, &p->svms.list, list) {\n\t\tsvm_range_lock(prange);\n\t\tlist_for_each_entry(pchild, &prange->child_list, child_list) {\n\t\t\tsize = (pchild->last - pchild->start + 1) << PAGE_SHIFT;\n\t\t\tif (xnack_enabled) {\n\t\t\t\tamdgpu_amdkfd_unreserve_mem_limit(NULL, size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\t\t\t} else {\n\t\t\t\tr = amdgpu_amdkfd_reserve_mem_limit(NULL, size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out_unlock;\n\t\t\t\treserved_size += size;\n\t\t\t}\n\t\t}\n\n\t\tsize = (prange->last - prange->start + 1) << PAGE_SHIFT;\n\t\tif (xnack_enabled) {\n\t\t\tamdgpu_amdkfd_unreserve_mem_limit(NULL, size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\t\t} else {\n\t\t\tr = amdgpu_amdkfd_reserve_mem_limit(NULL, size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out_unlock;\n\t\t\treserved_size += size;\n\t\t}\nout_unlock:\n\t\tsvm_range_unlock(prange);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\tif (r)\n\t\tamdgpu_amdkfd_unreserve_mem_limit(NULL, reserved_size,\n\t\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_USERPTR, 0);\n\telse\n\t\t \n\t\tp->xnack_enabled = xnack_enabled;\n\n\tmutex_unlock(&p->svms.lock);\n\treturn r;\n}\n\nvoid svm_range_list_fini(struct kfd_process *p)\n{\n\tstruct svm_range *prange;\n\tstruct svm_range *next;\n\n\tpr_debug(\"pasid 0x%x svms 0x%p\\n\", p->pasid, &p->svms);\n\n\tcancel_delayed_work_sync(&p->svms.restore_work);\n\n\t \n\tflush_work(&p->svms.deferred_list_work);\n\n\t \n\tatomic_inc(&p->svms.drain_pagefaults);\n\tsvm_range_drain_retry_fault(&p->svms);\n\n\tlist_for_each_entry_safe(prange, next, &p->svms.list, list) {\n\t\tsvm_range_unlink(prange);\n\t\tsvm_range_remove_notifier(prange);\n\t\tsvm_range_free(prange, true);\n\t}\n\n\tmutex_destroy(&p->svms.lock);\n\n\tpr_debug(\"pasid 0x%x svms 0x%p done\\n\", p->pasid, &p->svms);\n}\n\nint svm_range_list_init(struct kfd_process *p)\n{\n\tstruct svm_range_list *svms = &p->svms;\n\tint i;\n\n\tsvms->objects = RB_ROOT_CACHED;\n\tmutex_init(&svms->lock);\n\tINIT_LIST_HEAD(&svms->list);\n\tatomic_set(&svms->evicted_ranges, 0);\n\tatomic_set(&svms->drain_pagefaults, 0);\n\tINIT_DELAYED_WORK(&svms->restore_work, svm_range_restore_work);\n\tINIT_WORK(&svms->deferred_list_work, svm_range_deferred_list_work);\n\tINIT_LIST_HEAD(&svms->deferred_range_list);\n\tINIT_LIST_HEAD(&svms->criu_svm_metadata_list);\n\tspin_lock_init(&svms->deferred_list_lock);\n\n\tfor (i = 0; i < p->n_pdds; i++)\n\t\tif (KFD_IS_SVM_API_SUPPORTED(p->pdds[i]->dev->adev))\n\t\t\tbitmap_set(svms->bitmap_supported, i, 1);\n\n\treturn 0;\n}\n\n \nstatic int\nsvm_range_check_vm(struct kfd_process *p, uint64_t start, uint64_t last,\n\t\t   uint64_t *bo_s, uint64_t *bo_l)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct interval_tree_node *node;\n\tuint32_t i;\n\tint r;\n\n\tfor (i = 0; i < p->n_pdds; i++) {\n\t\tstruct amdgpu_vm *vm;\n\n\t\tif (!p->pdds[i]->drm_priv)\n\t\t\tcontinue;\n\n\t\tvm = drm_priv_to_vm(p->pdds[i]->drm_priv);\n\t\tr = amdgpu_bo_reserve(vm->root.bo, false);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tnode = interval_tree_iter_first(&vm->va, start, last);\n\t\tif (node) {\n\t\t\tpr_debug(\"range [0x%llx 0x%llx] already TTM mapped\\n\",\n\t\t\t\t start, last);\n\t\t\tmapping = container_of((struct rb_node *)node,\n\t\t\t\t\t       struct amdgpu_bo_va_mapping, rb);\n\t\t\tif (bo_s && bo_l) {\n\t\t\t\t*bo_s = mapping->start;\n\t\t\t\t*bo_l = mapping->last;\n\t\t\t}\n\t\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t\t\treturn -EADDRINUSE;\n\t\t}\n\t\tamdgpu_bo_unreserve(vm->root.bo);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nsvm_range_is_valid(struct kfd_process *p, uint64_t start, uint64_t size)\n{\n\tconst unsigned long device_vma = VM_IO | VM_PFNMAP | VM_MIXEDMAP;\n\tstruct vm_area_struct *vma;\n\tunsigned long end;\n\tunsigned long start_unchg = start;\n\n\tstart <<= PAGE_SHIFT;\n\tend = start + (size << PAGE_SHIFT);\n\tdo {\n\t\tvma = vma_lookup(p->mm, start);\n\t\tif (!vma || (vma->vm_flags & device_vma))\n\t\t\treturn -EFAULT;\n\t\tstart = min(end, vma->vm_end);\n\t} while (start < end);\n\n\treturn svm_range_check_vm(p, start_unchg, (end - 1) >> PAGE_SHIFT, NULL,\n\t\t\t\t  NULL);\n}\n\n \nstatic uint32_t\nsvm_range_best_prefetch_location(struct svm_range *prange)\n{\n\tDECLARE_BITMAP(bitmap, MAX_GPU_INSTANCE);\n\tuint32_t best_loc = prange->prefetch_loc;\n\tstruct kfd_process_device *pdd;\n\tstruct kfd_node *bo_node;\n\tstruct kfd_process *p;\n\tuint32_t gpuidx;\n\n\tp = container_of(prange->svms, struct kfd_process, svms);\n\n\tif (!best_loc || best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED)\n\t\tgoto out;\n\n\tbo_node = svm_range_get_node_by_id(prange, best_loc);\n\tif (!bo_node) {\n\t\tWARN_ONCE(1, \"failed to get valid kfd node at id%x\\n\", best_loc);\n\t\tbest_loc = 0;\n\t\tgoto out;\n\t}\n\n\tif (bo_node->adev->gmc.is_app_apu) {\n\t\tbest_loc = 0;\n\t\tgoto out;\n\t}\n\n\tif (p->xnack_enabled)\n\t\tbitmap_copy(bitmap, prange->bitmap_aip, MAX_GPU_INSTANCE);\n\telse\n\t\tbitmap_or(bitmap, prange->bitmap_access, prange->bitmap_aip,\n\t\t\t  MAX_GPU_INSTANCE);\n\n\tfor_each_set_bit(gpuidx, bitmap, MAX_GPU_INSTANCE) {\n\t\tpdd = kfd_process_device_from_gpuidx(p, gpuidx);\n\t\tif (!pdd) {\n\t\t\tpr_debug(\"failed to get device by idx 0x%x\\n\", gpuidx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (pdd->dev->adev == bo_node->adev)\n\t\t\tcontinue;\n\n\t\tif (!svm_nodes_in_same_hive(pdd->dev, bo_node)) {\n\t\t\tbest_loc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tpr_debug(\"xnack %d svms 0x%p [0x%lx 0x%lx] best loc 0x%x\\n\",\n\t\t p->xnack_enabled, &p->svms, prange->start, prange->last,\n\t\t best_loc);\n\n\treturn best_loc;\n}\n\n \nstatic int\nsvm_range_trigger_migration(struct mm_struct *mm, struct svm_range *prange,\n\t\t\t    bool *migrated)\n{\n\tuint32_t best_loc;\n\tint r = 0;\n\n\t*migrated = false;\n\tbest_loc = svm_range_best_prefetch_location(prange);\n\n\tif (best_loc == KFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t    best_loc == prange->actual_loc)\n\t\treturn 0;\n\n\tif (!best_loc) {\n\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_PREFETCH, NULL);\n\t\t*migrated = !r;\n\t\treturn r;\n\t}\n\n\tr = svm_migrate_to_vram(prange, best_loc, mm, KFD_MIGRATE_TRIGGER_PREFETCH);\n\t*migrated = !r;\n\n\treturn r;\n}\n\nint svm_range_schedule_evict_svm_bo(struct amdgpu_amdkfd_fence *fence)\n{\n\tif (!fence)\n\t\treturn -EINVAL;\n\n\tif (dma_fence_is_signaled(&fence->base))\n\t\treturn 0;\n\n\tif (fence->svm_bo) {\n\t\tWRITE_ONCE(fence->svm_bo->evicting, 1);\n\t\tschedule_work(&fence->svm_bo->eviction_work);\n\t}\n\n\treturn 0;\n}\n\nstatic void svm_range_evict_svm_bo_worker(struct work_struct *work)\n{\n\tstruct svm_range_bo *svm_bo;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = container_of(work, struct svm_range_bo, eviction_work);\n\tif (!svm_bo_ref_unless_zero(svm_bo))\n\t\treturn;  \n\n\tif (mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tmm = svm_bo->eviction_fence->mm;\n\t} else {\n\t\tsvm_range_bo_unref(svm_bo);\n\t\treturn;\n\t}\n\n\tmmap_read_lock(mm);\n\tspin_lock(&svm_bo->list_lock);\n\twhile (!list_empty(&svm_bo->range_list) && !r) {\n\t\tstruct svm_range *prange =\n\t\t\t\tlist_first_entry(&svm_bo->range_list,\n\t\t\t\t\t\tstruct svm_range, svm_bo_list);\n\t\tint retries = 3;\n\n\t\tlist_del_init(&prange->svm_bo_list);\n\t\tspin_unlock(&svm_bo->list_lock);\n\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms,\n\t\t\t prange->start, prange->last);\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\t\tdo {\n\t\t\tr = svm_migrate_vram_to_ram(prange, mm,\n\t\t\t\t\tKFD_MIGRATE_TRIGGER_TTM_EVICTION, NULL);\n\t\t} while (!r && prange->actual_loc && --retries);\n\n\t\tif (!r && prange->actual_loc)\n\t\t\tpr_info_once(\"Migration failed during eviction\");\n\n\t\tif (!prange->actual_loc) {\n\t\t\tmutex_lock(&prange->lock);\n\t\t\tprange->svm_bo = NULL;\n\t\t\tmutex_unlock(&prange->lock);\n\t\t}\n\t\tmutex_unlock(&prange->migrate_mutex);\n\n\t\tspin_lock(&svm_bo->list_lock);\n\t}\n\tspin_unlock(&svm_bo->list_lock);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\n\tdma_fence_signal(&svm_bo->eviction_fence->base);\n\n\t \n\tWARN_ONCE(!r && kref_read(&svm_bo->kref) != 1, \"This was not the last reference\\n\");\n\tsvm_range_bo_unref(svm_bo);\n}\n\nstatic int\nsvm_range_set_attr(struct kfd_process *p, struct mm_struct *mm,\n\t\t   uint64_t start, uint64_t size, uint32_t nattr,\n\t\t   struct kfd_ioctl_svm_attribute *attrs)\n{\n\tstruct amdkfd_process_info *process_info = p->kgd_process_info;\n\tstruct list_head update_list;\n\tstruct list_head insert_list;\n\tstruct list_head remove_list;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tstruct svm_range *next;\n\tbool update_mapping = false;\n\tbool flush_tlb;\n\tint r, ret = 0;\n\n\tpr_debug(\"pasid 0x%x svms 0x%p [0x%llx 0x%llx] pages 0x%llx\\n\",\n\t\t p->pasid, &p->svms, start, start + size - 1, size);\n\n\tr = svm_range_check_attr(p, nattr, attrs);\n\tif (r)\n\t\treturn r;\n\n\tsvms = &p->svms;\n\n\tmutex_lock(&process_info->lock);\n\n\tsvm_range_list_lock_and_flush_work(svms, mm);\n\n\tr = svm_range_is_valid(p, start, size);\n\tif (r) {\n\t\tpr_debug(\"invalid range r=%d\\n\", r);\n\t\tmmap_write_unlock(mm);\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&svms->lock);\n\n\t \n\tr = svm_range_add(p, start, size, nattr, attrs, &update_list,\n\t\t\t  &insert_list, &remove_list);\n\tif (r) {\n\t\tmutex_unlock(&svms->lock);\n\t\tmmap_write_unlock(mm);\n\t\tgoto out;\n\t}\n\t \n\tlist_for_each_entry_safe(prange, next, &insert_list, list) {\n\t\tsvm_range_add_to_svms(prange);\n\t\tsvm_range_add_notifier_locked(mm, prange);\n\t}\n\tlist_for_each_entry(prange, &update_list, update_list) {\n\t\tsvm_range_apply_attrs(p, prange, nattr, attrs, &update_mapping);\n\t\t \n\t}\n\tlist_for_each_entry_safe(prange, next, &remove_list, update_list) {\n\t\tpr_debug(\"unlink old 0x%p prange 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t prange->svms, prange, prange->start,\n\t\t\t prange->last);\n\t\tsvm_range_unlink(prange);\n\t\tsvm_range_remove_notifier(prange);\n\t\tsvm_range_free(prange, false);\n\t}\n\n\tmmap_write_downgrade(mm);\n\t \n\tlist_for_each_entry(prange, &update_list, update_list) {\n\t\tbool migrated;\n\n\t\tmutex_lock(&prange->migrate_mutex);\n\n\t\tr = svm_range_trigger_migration(mm, prange, &migrated);\n\t\tif (r)\n\t\t\tgoto out_unlock_range;\n\n\t\tif (migrated && (!p->xnack_enabled ||\n\t\t    (prange->flags & KFD_IOCTL_SVM_FLAG_GPU_ALWAYS_MAPPED)) &&\n\t\t    prange->mapped_to_gpu) {\n\t\t\tpr_debug(\"restore_work will update mappings of GPUs\\n\");\n\t\t\tmutex_unlock(&prange->migrate_mutex);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!migrated && !update_mapping) {\n\t\t\tmutex_unlock(&prange->migrate_mutex);\n\t\t\tcontinue;\n\t\t}\n\n\t\tflush_tlb = !migrated && update_mapping && prange->mapped_to_gpu;\n\n\t\tr = svm_range_validate_and_map(mm, prange, MAX_GPU_INSTANCE,\n\t\t\t\t\t       true, true, flush_tlb);\n\t\tif (r)\n\t\t\tpr_debug(\"failed %d to map svm range\\n\", r);\n\nout_unlock_range:\n\t\tmutex_unlock(&prange->migrate_mutex);\n\t\tif (r)\n\t\t\tret = r;\n\t}\n\n\tdynamic_svm_range_dump(svms);\n\n\tmutex_unlock(&svms->lock);\n\tmmap_read_unlock(mm);\nout:\n\tmutex_unlock(&process_info->lock);\n\n\tpr_debug(\"pasid 0x%x svms 0x%p [0x%llx 0x%llx] done, r=%d\\n\", p->pasid,\n\t\t &p->svms, start, start + size - 1, r);\n\n\treturn ret ? ret : r;\n}\n\nstatic int\nsvm_range_get_attr(struct kfd_process *p, struct mm_struct *mm,\n\t\t   uint64_t start, uint64_t size, uint32_t nattr,\n\t\t   struct kfd_ioctl_svm_attribute *attrs)\n{\n\tDECLARE_BITMAP(bitmap_access, MAX_GPU_INSTANCE);\n\tDECLARE_BITMAP(bitmap_aip, MAX_GPU_INSTANCE);\n\tbool get_preferred_loc = false;\n\tbool get_prefetch_loc = false;\n\tbool get_granularity = false;\n\tbool get_accessible = false;\n\tbool get_flags = false;\n\tuint64_t last = start + size - 1UL;\n\tuint8_t granularity = 0xff;\n\tstruct interval_tree_node *node;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tuint32_t prefetch_loc = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\tuint32_t location = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\tuint32_t flags_and = 0xffffffff;\n\tuint32_t flags_or = 0;\n\tint gpuidx;\n\tuint32_t i;\n\tint r = 0;\n\n\tpr_debug(\"svms 0x%p [0x%llx 0x%llx] nattr 0x%x\\n\", &p->svms, start,\n\t\t start + size - 1, nattr);\n\n\t \n\tflush_work(&p->svms.deferred_list_work);\n\n\tmmap_read_lock(mm);\n\tr = svm_range_is_valid(p, start, size);\n\tmmap_read_unlock(mm);\n\tif (r) {\n\t\tpr_debug(\"invalid range r=%d\\n\", r);\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < nattr; i++) {\n\t\tswitch (attrs[i].type) {\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFERRED_LOC:\n\t\t\tget_preferred_loc = true;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\tget_prefetch_loc = true;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS:\n\t\t\tget_accessible = true;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\tcase KFD_IOCTL_SVM_ATTR_CLR_FLAGS:\n\t\t\tget_flags = true;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_GRANULARITY:\n\t\t\tget_granularity = true;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS_IN_PLACE:\n\t\tcase KFD_IOCTL_SVM_ATTR_NO_ACCESS:\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tpr_debug(\"get invalid attr type 0x%x\\n\", attrs[i].type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tsvms = &p->svms;\n\n\tmutex_lock(&svms->lock);\n\n\tnode = interval_tree_iter_first(&svms->objects, start, last);\n\tif (!node) {\n\t\tpr_debug(\"range attrs not found return default values\\n\");\n\t\tsvm_range_set_default_attributes(&location, &prefetch_loc,\n\t\t\t\t\t\t &granularity, &flags_and);\n\t\tflags_or = flags_and;\n\t\tif (p->xnack_enabled)\n\t\t\tbitmap_copy(bitmap_access, svms->bitmap_supported,\n\t\t\t\t    MAX_GPU_INSTANCE);\n\t\telse\n\t\t\tbitmap_zero(bitmap_access, MAX_GPU_INSTANCE);\n\t\tbitmap_zero(bitmap_aip, MAX_GPU_INSTANCE);\n\t\tgoto fill_values;\n\t}\n\tbitmap_copy(bitmap_access, svms->bitmap_supported, MAX_GPU_INSTANCE);\n\tbitmap_copy(bitmap_aip, svms->bitmap_supported, MAX_GPU_INSTANCE);\n\n\twhile (node) {\n\t\tstruct interval_tree_node *next;\n\n\t\tprange = container_of(node, struct svm_range, it_node);\n\t\tnext = interval_tree_iter_next(node, start, last);\n\n\t\tif (get_preferred_loc) {\n\t\t\tif (prange->preferred_loc ==\n\t\t\t\t\tKFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t\t\t    (location != KFD_IOCTL_SVM_LOCATION_UNDEFINED &&\n\t\t\t     location != prange->preferred_loc)) {\n\t\t\t\tlocation = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\t\t\t\tget_preferred_loc = false;\n\t\t\t} else {\n\t\t\t\tlocation = prange->preferred_loc;\n\t\t\t}\n\t\t}\n\t\tif (get_prefetch_loc) {\n\t\t\tif (prange->prefetch_loc ==\n\t\t\t\t\tKFD_IOCTL_SVM_LOCATION_UNDEFINED ||\n\t\t\t    (prefetch_loc != KFD_IOCTL_SVM_LOCATION_UNDEFINED &&\n\t\t\t     prefetch_loc != prange->prefetch_loc)) {\n\t\t\t\tprefetch_loc = KFD_IOCTL_SVM_LOCATION_UNDEFINED;\n\t\t\t\tget_prefetch_loc = false;\n\t\t\t} else {\n\t\t\t\tprefetch_loc = prange->prefetch_loc;\n\t\t\t}\n\t\t}\n\t\tif (get_accessible) {\n\t\t\tbitmap_and(bitmap_access, bitmap_access,\n\t\t\t\t   prange->bitmap_access, MAX_GPU_INSTANCE);\n\t\t\tbitmap_and(bitmap_aip, bitmap_aip,\n\t\t\t\t   prange->bitmap_aip, MAX_GPU_INSTANCE);\n\t\t}\n\t\tif (get_flags) {\n\t\t\tflags_and &= prange->flags;\n\t\t\tflags_or |= prange->flags;\n\t\t}\n\n\t\tif (get_granularity && prange->granularity < granularity)\n\t\t\tgranularity = prange->granularity;\n\n\t\tnode = next;\n\t}\nfill_values:\n\tmutex_unlock(&svms->lock);\n\n\tfor (i = 0; i < nattr; i++) {\n\t\tswitch (attrs[i].type) {\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFERRED_LOC:\n\t\t\tattrs[i].value = location;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\tattrs[i].value = prefetch_loc;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_ACCESS:\n\t\t\tgpuidx = kfd_process_gpuidx_from_gpuid(p,\n\t\t\t\t\t\t\t       attrs[i].value);\n\t\t\tif (gpuidx < 0) {\n\t\t\t\tpr_debug(\"invalid gpuid %x\\n\", attrs[i].value);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (test_bit(gpuidx, bitmap_access))\n\t\t\t\tattrs[i].type = KFD_IOCTL_SVM_ATTR_ACCESS;\n\t\t\telse if (test_bit(gpuidx, bitmap_aip))\n\t\t\t\tattrs[i].type =\n\t\t\t\t\tKFD_IOCTL_SVM_ATTR_ACCESS_IN_PLACE;\n\t\t\telse\n\t\t\t\tattrs[i].type = KFD_IOCTL_SVM_ATTR_NO_ACCESS;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\t\tattrs[i].value = flags_and;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_CLR_FLAGS:\n\t\t\tattrs[i].value = ~flags_or;\n\t\t\tbreak;\n\t\tcase KFD_IOCTL_SVM_ATTR_GRANULARITY:\n\t\t\tattrs[i].value = (uint32_t)granularity;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint kfd_criu_resume_svm(struct kfd_process *p)\n{\n\tstruct kfd_ioctl_svm_attribute *set_attr_new, *set_attr = NULL;\n\tint nattr_common = 4, nattr_accessibility = 1;\n\tstruct criu_svm_metadata *criu_svm_md = NULL;\n\tstruct svm_range_list *svms = &p->svms;\n\tstruct criu_svm_metadata *next = NULL;\n\tuint32_t set_flags = 0xffffffff;\n\tint i, j, num_attrs, ret = 0;\n\tuint64_t set_attr_size;\n\tstruct mm_struct *mm;\n\n\tif (list_empty(&svms->criu_svm_metadata_list)) {\n\t\tpr_debug(\"No SVM data from CRIU restore stage 2\\n\");\n\t\treturn ret;\n\t}\n\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_err(\"failed to get mm for the target process\\n\");\n\t\treturn -ESRCH;\n\t}\n\n\tnum_attrs = nattr_common + (nattr_accessibility * p->n_pdds);\n\n\ti = j = 0;\n\tlist_for_each_entry(criu_svm_md, &svms->criu_svm_metadata_list, list) {\n\t\tpr_debug(\"criu_svm_md[%d]\\n\\tstart: 0x%llx size: 0x%llx (npages)\\n\",\n\t\t\t i, criu_svm_md->data.start_addr, criu_svm_md->data.size);\n\n\t\tfor (j = 0; j < num_attrs; j++) {\n\t\t\tpr_debug(\"\\ncriu_svm_md[%d]->attrs[%d].type : 0x%x\\ncriu_svm_md[%d]->attrs[%d].value : 0x%x\\n\",\n\t\t\t\t i, j, criu_svm_md->data.attrs[j].type,\n\t\t\t\t i, j, criu_svm_md->data.attrs[j].value);\n\t\t\tswitch (criu_svm_md->data.attrs[j].type) {\n\t\t\t \n\t\t\tcase KFD_IOCTL_SVM_ATTR_PREFETCH_LOC:\n\t\t\t\tif (criu_svm_md->data.attrs[j].value ==\n\t\t\t\t    KFD_IOCTL_SVM_LOCATION_UNDEFINED) {\n\t\t\t\t\tcriu_svm_md->data.attrs[j].type =\n\t\t\t\t\t\tKFD_IOCTL_SVM_ATTR_SET_FLAGS;\n\t\t\t\t\tcriu_svm_md->data.attrs[j].value = 0;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase KFD_IOCTL_SVM_ATTR_SET_FLAGS:\n\t\t\t\tset_flags = criu_svm_md->data.attrs[j].value;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tset_attr_size = sizeof(struct kfd_ioctl_svm_attribute) *\n\t\t\t\t\t\t(num_attrs + 1);\n\t\tset_attr_new = krealloc(set_attr, set_attr_size,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!set_attr_new) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto exit;\n\t\t}\n\t\tset_attr = set_attr_new;\n\n\t\tmemcpy(set_attr, criu_svm_md->data.attrs, num_attrs *\n\t\t\t\t\tsizeof(struct kfd_ioctl_svm_attribute));\n\t\tset_attr[num_attrs].type = KFD_IOCTL_SVM_ATTR_CLR_FLAGS;\n\t\tset_attr[num_attrs].value = ~set_flags;\n\n\t\tret = svm_range_set_attr(p, mm, criu_svm_md->data.start_addr,\n\t\t\t\t\t criu_svm_md->data.size, num_attrs + 1,\n\t\t\t\t\t set_attr);\n\t\tif (ret) {\n\t\t\tpr_err(\"CRIU: failed to set range attributes\\n\");\n\t\t\tgoto exit;\n\t\t}\n\n\t\ti++;\n\t}\nexit:\n\tkfree(set_attr);\n\tlist_for_each_entry_safe(criu_svm_md, next, &svms->criu_svm_metadata_list, list) {\n\t\tpr_debug(\"freeing criu_svm_md[]\\n\\tstart: 0x%llx\\n\",\n\t\t\t\t\t\tcriu_svm_md->data.start_addr);\n\t\tkfree(criu_svm_md);\n\t}\n\n\tmmput(mm);\n\treturn ret;\n\n}\n\nint kfd_criu_restore_svm(struct kfd_process *p,\n\t\t\t uint8_t __user *user_priv_ptr,\n\t\t\t uint64_t *priv_data_offset,\n\t\t\t uint64_t max_priv_data_size)\n{\n\tuint64_t svm_priv_data_size, svm_object_md_size, svm_attrs_size;\n\tint nattr_common = 4, nattr_accessibility = 1;\n\tstruct criu_svm_metadata *criu_svm_md = NULL;\n\tstruct svm_range_list *svms = &p->svms;\n\tuint32_t num_devices;\n\tint ret = 0;\n\n\tnum_devices = p->n_pdds;\n\t \n\n\tsvm_attrs_size = sizeof(struct kfd_ioctl_svm_attribute) *\n\t\t(nattr_common + nattr_accessibility * num_devices);\n\tsvm_object_md_size = sizeof(struct criu_svm_metadata) + svm_attrs_size;\n\n\tsvm_priv_data_size = sizeof(struct kfd_criu_svm_range_priv_data) +\n\t\t\t\t\t\t\t\tsvm_attrs_size;\n\n\tcriu_svm_md = kzalloc(svm_object_md_size, GFP_KERNEL);\n\tif (!criu_svm_md) {\n\t\tpr_err(\"failed to allocate memory to store svm metadata\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tif (*priv_data_offset + svm_priv_data_size > max_priv_data_size) {\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tret = copy_from_user(&criu_svm_md->data, user_priv_ptr + *priv_data_offset,\n\t\t\t     svm_priv_data_size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto exit;\n\t}\n\t*priv_data_offset += svm_priv_data_size;\n\n\tlist_add_tail(&criu_svm_md->list, &svms->criu_svm_metadata_list);\n\n\treturn 0;\n\n\nexit:\n\tkfree(criu_svm_md);\n\treturn ret;\n}\n\nint svm_range_get_info(struct kfd_process *p, uint32_t *num_svm_ranges,\n\t\t       uint64_t *svm_priv_data_size)\n{\n\tuint64_t total_size, accessibility_size, common_attr_size;\n\tint nattr_common = 4, nattr_accessibility = 1;\n\tint num_devices = p->n_pdds;\n\tstruct svm_range_list *svms;\n\tstruct svm_range *prange;\n\tuint32_t count = 0;\n\n\t*svm_priv_data_size = 0;\n\n\tsvms = &p->svms;\n\tif (!svms)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&svms->lock);\n\tlist_for_each_entry(prange, &svms->list, list) {\n\t\tpr_debug(\"prange: 0x%p start: 0x%lx\\t npages: 0x%llx\\t end: 0x%llx\\n\",\n\t\t\t prange, prange->start, prange->npages,\n\t\t\t prange->start + prange->npages - 1);\n\t\tcount++;\n\t}\n\tmutex_unlock(&svms->lock);\n\n\t*num_svm_ranges = count;\n\t \n\tif (*num_svm_ranges > 0) {\n\t\tcommon_attr_size = sizeof(struct kfd_ioctl_svm_attribute) *\n\t\t\tnattr_common;\n\t\taccessibility_size = sizeof(struct kfd_ioctl_svm_attribute) *\n\t\t\tnattr_accessibility * num_devices;\n\n\t\ttotal_size = sizeof(struct kfd_criu_svm_range_priv_data) +\n\t\t\tcommon_attr_size + accessibility_size;\n\n\t\t*svm_priv_data_size = *num_svm_ranges * total_size;\n\t}\n\n\tpr_debug(\"num_svm_ranges %u total_priv_size %llu\\n\", *num_svm_ranges,\n\t\t *svm_priv_data_size);\n\treturn 0;\n}\n\nint kfd_criu_checkpoint_svm(struct kfd_process *p,\n\t\t\t    uint8_t __user *user_priv_data,\n\t\t\t    uint64_t *priv_data_offset)\n{\n\tstruct kfd_criu_svm_range_priv_data *svm_priv = NULL;\n\tstruct kfd_ioctl_svm_attribute *query_attr = NULL;\n\tuint64_t svm_priv_data_size, query_attr_size = 0;\n\tint index, nattr_common = 4, ret = 0;\n\tstruct svm_range_list *svms;\n\tint num_devices = p->n_pdds;\n\tstruct svm_range *prange;\n\tstruct mm_struct *mm;\n\n\tsvms = &p->svms;\n\tif (!svms)\n\t\treturn -EINVAL;\n\n\tmm = get_task_mm(p->lead_thread);\n\tif (!mm) {\n\t\tpr_err(\"failed to get mm for the target process\\n\");\n\t\treturn -ESRCH;\n\t}\n\n\tquery_attr_size = sizeof(struct kfd_ioctl_svm_attribute) *\n\t\t\t\t(nattr_common + num_devices);\n\n\tquery_attr = kzalloc(query_attr_size, GFP_KERNEL);\n\tif (!query_attr) {\n\t\tret = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\tquery_attr[0].type = KFD_IOCTL_SVM_ATTR_PREFERRED_LOC;\n\tquery_attr[1].type = KFD_IOCTL_SVM_ATTR_PREFETCH_LOC;\n\tquery_attr[2].type = KFD_IOCTL_SVM_ATTR_SET_FLAGS;\n\tquery_attr[3].type = KFD_IOCTL_SVM_ATTR_GRANULARITY;\n\n\tfor (index = 0; index < num_devices; index++) {\n\t\tstruct kfd_process_device *pdd = p->pdds[index];\n\n\t\tquery_attr[index + nattr_common].type =\n\t\t\tKFD_IOCTL_SVM_ATTR_ACCESS;\n\t\tquery_attr[index + nattr_common].value = pdd->user_gpu_id;\n\t}\n\n\tsvm_priv_data_size = sizeof(*svm_priv) + query_attr_size;\n\n\tsvm_priv = kzalloc(svm_priv_data_size, GFP_KERNEL);\n\tif (!svm_priv) {\n\t\tret = -ENOMEM;\n\t\tgoto exit_query;\n\t}\n\n\tindex = 0;\n\tlist_for_each_entry(prange, &svms->list, list) {\n\n\t\tsvm_priv->object_type = KFD_CRIU_OBJECT_TYPE_SVM_RANGE;\n\t\tsvm_priv->start_addr = prange->start;\n\t\tsvm_priv->size = prange->npages;\n\t\tmemcpy(&svm_priv->attrs, query_attr, query_attr_size);\n\t\tpr_debug(\"CRIU: prange: 0x%p start: 0x%lx\\t npages: 0x%llx end: 0x%llx\\t size: 0x%llx\\n\",\n\t\t\t prange, prange->start, prange->npages,\n\t\t\t prange->start + prange->npages - 1,\n\t\t\t prange->npages * PAGE_SIZE);\n\n\t\tret = svm_range_get_attr(p, mm, svm_priv->start_addr,\n\t\t\t\t\t svm_priv->size,\n\t\t\t\t\t (nattr_common + num_devices),\n\t\t\t\t\t svm_priv->attrs);\n\t\tif (ret) {\n\t\t\tpr_err(\"CRIU: failed to obtain range attributes\\n\");\n\t\t\tgoto exit_priv;\n\t\t}\n\n\t\tif (copy_to_user(user_priv_data + *priv_data_offset, svm_priv,\n\t\t\t\t svm_priv_data_size)) {\n\t\t\tpr_err(\"Failed to copy svm priv to user\\n\");\n\t\t\tret = -EFAULT;\n\t\t\tgoto exit_priv;\n\t\t}\n\n\t\t*priv_data_offset += svm_priv_data_size;\n\n\t}\n\n\nexit_priv:\n\tkfree(svm_priv);\nexit_query:\n\tkfree(query_attr);\nexit:\n\tmmput(mm);\n\treturn ret;\n}\n\nint\nsvm_ioctl(struct kfd_process *p, enum kfd_ioctl_svm_op op, uint64_t start,\n\t  uint64_t size, uint32_t nattrs, struct kfd_ioctl_svm_attribute *attrs)\n{\n\tstruct mm_struct *mm = current->mm;\n\tint r;\n\n\tstart >>= PAGE_SHIFT;\n\tsize >>= PAGE_SHIFT;\n\n\tswitch (op) {\n\tcase KFD_IOCTL_SVM_OP_SET_ATTR:\n\t\tr = svm_range_set_attr(p, mm, start, size, nattrs, attrs);\n\t\tbreak;\n\tcase KFD_IOCTL_SVM_OP_GET_ATTR:\n\t\tr = svm_range_get_attr(p, mm, start, size, nattrs, attrs);\n\t\tbreak;\n\tdefault:\n\t\tr = EINVAL;\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}