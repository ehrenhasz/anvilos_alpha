{
  "module_name": "cwsr_trap_handler_gfx9.asm",
  "hash_id": "56fdc227e3b9676f89d5a97bdf69214204bfd9b2ea7a7f1b43d9f3a60a9ae9f5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/cwsr_trap_handler_gfx9.asm",
  "human_readable_source": "/*\n * Copyright 2016 Advanced Micro Devices, Inc.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE COPYRIGHT HOLDER(S) OR AUTHOR(S) BE LIABLE FOR ANY CLAIM, DAMAGES OR\n * OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n * ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n * OTHER DEALINGS IN THE SOFTWARE.\n */\n\n/* To compile this assembly code:\n *\n * gfx9:\n *   cpp -DASIC_FAMILY=CHIP_VEGAM cwsr_trap_handler_gfx9.asm -P -o gfx9.sp3\n *   sp3 gfx9.sp3 -hex gfx9.hex\n *\n * arcturus:\n *   cpp -DASIC_FAMILY=CHIP_ARCTURUS cwsr_trap_handler_gfx9.asm -P -o arcturus.sp3\n *   sp3 arcturus.sp3 -hex arcturus.hex\n *\n * aldebaran:\n *   cpp -DASIC_FAMILY=CHIP_ALDEBARAN cwsr_trap_handler_gfx9.asm -P -o aldebaran.sp3\n *   sp3 aldebaran.sp3 -hex aldebaran.hex\n *\n * gc_9_4_3:\n *   cpp -DASIC_FAMILY=GC_9_4_3 cwsr_trap_handler_gfx9.asm -P -o gc_9_4_3.sp3\n *   sp3 gc_9_4_3.sp3 -hex gc_9_4_3.hex\n */\n\n#define CHIP_VEGAM 18\n#define CHIP_ARCTURUS 23\n#define CHIP_ALDEBARAN 25\n#define CHIP_GC_9_4_3 26\n\nvar ACK_SQC_STORE\t\t    =\t1\t\t    //workaround for suspected SQC store bug causing incorrect stores under concurrency\nvar SAVE_AFTER_XNACK_ERROR\t    =\t1\t\t    //workaround for TCP store failure after XNACK error when ALLOW_REPLAY=0, for debugger\nvar SINGLE_STEP_MISSED_WORKAROUND   =\t(ASIC_FAMILY <= CHIP_ALDEBARAN)\t//workaround for lost MODE.DEBUG_EN exception when SAVECTX raised\n\n/**************************************************************************/\n/*\t\t\tvariables\t\t\t\t\t  */\n/**************************************************************************/\nvar SQ_WAVE_STATUS_SPI_PRIO_SHIFT  = 1\nvar SQ_WAVE_STATUS_SPI_PRIO_MASK   = 0x00000006\nvar SQ_WAVE_STATUS_HALT_MASK       = 0x2000\nvar SQ_WAVE_STATUS_PRE_SPI_PRIO_SHIFT   = 0\nvar SQ_WAVE_STATUS_PRE_SPI_PRIO_SIZE    = 1\nvar SQ_WAVE_STATUS_POST_SPI_PRIO_SHIFT  = 3\nvar SQ_WAVE_STATUS_POST_SPI_PRIO_SIZE   = 29\nvar SQ_WAVE_STATUS_ALLOW_REPLAY_MASK    = 0x400000\nvar SQ_WAVE_STATUS_ECC_ERR_MASK         = 0x20000\n\nvar SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT\t= 12\nvar SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE\t= 9\nvar SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE\t= 6\nvar SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE\t= 3\t\t\t//FIXME\t sq.blk still has 4 bits at this time while SQ programming guide has 3 bits\nvar SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT\t= 24\n\n#if ASIC_FAMILY >= CHIP_ALDEBARAN\nvar SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT\t= 6\nvar SQ_WAVE_GPR_ALLOC_ACCV_OFFSET_SHIFT\t= 12\nvar SQ_WAVE_GPR_ALLOC_ACCV_OFFSET_SIZE\t= 6\n#else\nvar SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT\t= 8\n#endif\n\nvar SQ_WAVE_TRAPSTS_SAVECTX_MASK    =\t0x400\nvar SQ_WAVE_TRAPSTS_EXCP_MASK\t    =\t0x1FF\nvar SQ_WAVE_TRAPSTS_SAVECTX_SHIFT   =\t10\nvar SQ_WAVE_TRAPSTS_ADDR_WATCH_MASK =\t0x80\nvar SQ_WAVE_TRAPSTS_ADDR_WATCH_SHIFT =\t7\nvar SQ_WAVE_TRAPSTS_MEM_VIOL_MASK   =\t0x100\nvar SQ_WAVE_TRAPSTS_MEM_VIOL_SHIFT  =\t8\nvar SQ_WAVE_TRAPSTS_HOST_TRAP_MASK  =\t0x400000\nvar SQ_WAVE_TRAPSTS_WAVE_BEGIN_MASK =\t0x800000\nvar SQ_WAVE_TRAPSTS_WAVE_END_MASK   =\t0x1000000\nvar SQ_WAVE_TRAPSTS_TRAP_AFTER_INST_MASK =  0x2000000\nvar SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK\t=   0x3FF\nvar SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT\t=   0x0\nvar SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE\t=   10\nvar SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK\t=   0xFFFFF800\nvar SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT\t=   11\nvar SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE\t=   21\nvar SQ_WAVE_TRAPSTS_ILLEGAL_INST_MASK\t=   0x800\nvar SQ_WAVE_TRAPSTS_EXCP_HI_MASK\t=   0x7000\nvar SQ_WAVE_TRAPSTS_XNACK_ERROR_MASK\t=   0x10000000\n\nvar SQ_WAVE_MODE_EXCP_EN_SHIFT\t\t=   12\nvar SQ_WAVE_MODE_EXCP_EN_ADDR_WATCH_SHIFT\t= 19\n\nvar SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT\t=   15\t\t\t//FIXME\nvar SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK\t= 0x1F8000\n\nvar SQ_WAVE_MODE_DEBUG_EN_MASK\t\t=   0x800\n\nvar TTMP_SAVE_RCNT_FIRST_REPLAY_SHIFT\t=   26\t\t\t// bits [31:26] unused by SPI debug data\nvar TTMP_SAVE_RCNT_FIRST_REPLAY_MASK\t=   0xFC000000\nvar TTMP_DEBUG_TRAP_ENABLED_SHIFT\t=   23\nvar TTMP_DEBUG_TRAP_ENABLED_MASK\t=   0x800000\n\n/*\tSave\t    */\nvar S_SAVE_BUF_RSRC_WORD1_STRIDE\t=   0x00040000\t\t//stride is 4 bytes\nvar S_SAVE_BUF_RSRC_WORD3_MISC\t\t=   0x00807FAC\t\t//SQ_SEL_X/Y/Z/W, BUF_NUM_FORMAT_FLOAT, (0 for MUBUF stride[17:14] when ADD_TID_ENABLE and BUF_DATA_FORMAT_32 for MTBUF), ADD_TID_ENABLE\nvar S_SAVE_PC_HI_TRAP_ID_MASK\t\t=   0x00FF0000\nvar S_SAVE_PC_HI_HT_MASK\t\t=   0x01000000\nvar S_SAVE_SPI_INIT_FIRST_WAVE_MASK\t=   0x04000000\t\t//bit[26]: FirstWaveInTG\nvar S_SAVE_SPI_INIT_FIRST_WAVE_SHIFT\t=   26\n\nvar s_save_spi_init_lo\t\t    =\texec_lo\nvar s_save_spi_init_hi\t\t    =\texec_hi\n\nvar s_save_pc_lo\t    =\tttmp0\t\t//{TTMP1, TTMP0} = {3'h0,pc_rewind[3:0], HT[0],trapID[7:0], PC[47:0]}\nvar s_save_pc_hi\t    =\tttmp1\nvar s_save_exec_lo\t    =\tttmp2\nvar s_save_exec_hi\t    =\tttmp3\nvar s_save_tmp\t\t    =\tttmp14\nvar s_save_trapsts\t    =\tttmp15\t\t//not really used until the end of the SAVE routine\nvar s_save_xnack_mask_lo    =\tttmp6\nvar s_save_xnack_mask_hi    =\tttmp7\nvar s_save_buf_rsrc0\t    =\tttmp8\nvar s_save_buf_rsrc1\t    =\tttmp9\nvar s_save_buf_rsrc2\t    =\tttmp10\nvar s_save_buf_rsrc3\t    =\tttmp11\nvar s_save_status\t    =\tttmp12\nvar s_save_mem_offset\t    =\tttmp4\nvar s_save_alloc_size\t    =\ts_save_trapsts\t\t//conflict\nvar s_save_m0\t\t    =\tttmp5\nvar s_save_ttmps_lo\t    =\ts_save_tmp\t\t//no conflict\nvar s_save_ttmps_hi\t    =\ts_save_trapsts\t\t//no conflict\n#if ASIC_FAMILY >= CHIP_GC_9_4_3\nvar s_save_ib_sts       =\tttmp13\n#else\nvar s_save_ib_sts       =\tttmp11\n#endif\n\n/*\tRestore\t    */\nvar S_RESTORE_BUF_RSRC_WORD1_STRIDE\t    =\tS_SAVE_BUF_RSRC_WORD1_STRIDE\nvar S_RESTORE_BUF_RSRC_WORD3_MISC\t    =\tS_SAVE_BUF_RSRC_WORD3_MISC\n\nvar S_RESTORE_SPI_INIT_FIRST_WAVE_MASK\t    =\t0x04000000\t    //bit[26]: FirstWaveInTG\nvar S_RESTORE_SPI_INIT_FIRST_WAVE_SHIFT\t    =\t26\n\nvar s_restore_spi_init_lo\t\t    =\texec_lo\nvar s_restore_spi_init_hi\t\t    =\texec_hi\n\nvar s_restore_mem_offset\t=   ttmp12\nvar s_restore_tmp2\t\t=   ttmp13\nvar s_restore_alloc_size\t=   ttmp3\nvar s_restore_tmp\t\t=   ttmp2\nvar s_restore_mem_offset_save\t=   s_restore_tmp\t//no conflict\nvar s_restore_accvgpr_offset_save = ttmp7\n\nvar s_restore_m0\t    =\ts_restore_alloc_size\t//no conflict\n\nvar s_restore_mode\t    =\ts_restore_accvgpr_offset_save\n\nvar s_restore_pc_lo\t    =\tttmp0\nvar s_restore_pc_hi\t    =\tttmp1\nvar s_restore_exec_lo\t    =\tttmp4\nvar s_restore_exec_hi\t    = \tttmp5\nvar s_restore_status\t    =\tttmp14\nvar s_restore_trapsts\t    =\tttmp15\nvar s_restore_xnack_mask_lo =\txnack_mask_lo\nvar s_restore_xnack_mask_hi =\txnack_mask_hi\nvar s_restore_buf_rsrc0\t    =\tttmp8\nvar s_restore_buf_rsrc1\t    =\tttmp9\nvar s_restore_buf_rsrc2\t    =\tttmp10\nvar s_restore_buf_rsrc3\t    =\tttmp11\nvar s_restore_ttmps_lo\t    =\ts_restore_tmp\t\t//no conflict\nvar s_restore_ttmps_hi\t    =\ts_restore_alloc_size\t//no conflict\n\n/**************************************************************************/\n/*\t\t\ttrap handler entry points\t\t\t  */\n/**************************************************************************/\n/* Shader Main*/\n\nshader main\n  asic(DEFAULT)\n  type(CS)\n\n\n\ts_branch L_SKIP_RESTORE\t\t\t\t\t    //NOT restore. might be a regular trap or save\n\nL_JUMP_TO_RESTORE:\n    s_branch L_RESTORE\t\t\t\t\t\t    //restore\n\nL_SKIP_RESTORE:\n\n    s_getreg_b32    s_save_status, hwreg(HW_REG_STATUS)\t\t\t\t    //save STATUS since we will change SCC\n\n    // Clear SPI_PRIO: do not save with elevated priority.\n    // Clear ECC_ERR: prevents SQC store and triggers FATAL_HALT if setreg'd.\n    s_andn2_b32     s_save_status, s_save_status, SQ_WAVE_STATUS_SPI_PRIO_MASK|SQ_WAVE_STATUS_ECC_ERR_MASK\n\n    s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)\n\n    s_and_b32       ttmp2, s_save_status, SQ_WAVE_STATUS_HALT_MASK\n    s_cbranch_scc0  L_NOT_HALTED\n\nL_HALTED:\n    // Host trap may occur while wave is halted.\n    s_and_b32       ttmp2, s_save_pc_hi, S_SAVE_PC_HI_TRAP_ID_MASK\n    s_cbranch_scc1  L_FETCH_2ND_TRAP\n\nL_CHECK_SAVE:\n    s_and_b32       ttmp2, s_save_trapsts, SQ_WAVE_TRAPSTS_SAVECTX_MASK    //check whether this is for save\n    s_cbranch_scc1  L_SAVE\t\t\t\t\t//this is the operation for save\n\n    // Wave is halted but neither host trap nor SAVECTX is raised.\n    // Caused by instruction fetch memory violation.\n    // Spin wait until context saved to prevent interrupt storm.\n    s_sleep         0x10\n    s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)\n    s_branch        L_CHECK_SAVE\n\nL_NOT_HALTED:\n    // Let second-level handle non-SAVECTX exception or trap.\n    // Any concurrent SAVECTX will be handled upon re-entry once halted.\n\n    // Check non-maskable exceptions. memory_violation, illegal_instruction\n    // and debugger (host trap, wave start/end, trap after instruction)\n    // exceptions always cause the wave to enter the trap handler.\n    s_and_b32       ttmp2, s_save_trapsts,      \\\n        SQ_WAVE_TRAPSTS_MEM_VIOL_MASK         | \\\n        SQ_WAVE_TRAPSTS_ILLEGAL_INST_MASK     | \\\n        SQ_WAVE_TRAPSTS_HOST_TRAP_MASK        | \\\n        SQ_WAVE_TRAPSTS_WAVE_BEGIN_MASK       | \\\n        SQ_WAVE_TRAPSTS_WAVE_END_MASK         | \\\n        SQ_WAVE_TRAPSTS_TRAP_AFTER_INST_MASK\n    s_cbranch_scc1  L_FETCH_2ND_TRAP\n\n    // Check for maskable exceptions in trapsts.excp and trapsts.excp_hi.\n    // Maskable exceptions only cause the wave to enter the trap handler if\n    // their respective bit in mode.excp_en is set.\n    s_and_b32       ttmp2, s_save_trapsts, SQ_WAVE_TRAPSTS_EXCP_MASK|SQ_WAVE_TRAPSTS_EXCP_HI_MASK\n    s_cbranch_scc0  L_CHECK_TRAP_ID\n\n    s_and_b32       ttmp3, s_save_trapsts, SQ_WAVE_TRAPSTS_ADDR_WATCH_MASK|SQ_WAVE_TRAPSTS_EXCP_HI_MASK\n    s_cbranch_scc0  L_NOT_ADDR_WATCH\n    s_bitset1_b32   ttmp2, SQ_WAVE_TRAPSTS_ADDR_WATCH_SHIFT // Check all addr_watch[123] exceptions against excp_en.addr_watch\n\nL_NOT_ADDR_WATCH:\n    s_getreg_b32    ttmp3, hwreg(HW_REG_MODE)\n    s_lshl_b32      ttmp2, ttmp2, SQ_WAVE_MODE_EXCP_EN_SHIFT\n    s_and_b32       ttmp2, ttmp2, ttmp3\n    s_cbranch_scc1  L_FETCH_2ND_TRAP\n\nL_CHECK_TRAP_ID:\n    // Check trap_id != 0\n    s_and_b32       ttmp2, s_save_pc_hi, S_SAVE_PC_HI_TRAP_ID_MASK\n    s_cbranch_scc1  L_FETCH_2ND_TRAP\n\nif SINGLE_STEP_MISSED_WORKAROUND\n    // Prioritize single step exception over context save.\n    // Second-level trap will halt wave and RFE, re-entering for SAVECTX.\n    s_getreg_b32    ttmp2, hwreg(HW_REG_MODE)\n    s_and_b32       ttmp2, ttmp2, SQ_WAVE_MODE_DEBUG_EN_MASK\n    s_cbranch_scc1  L_FETCH_2ND_TRAP\nend\n\n    s_and_b32       ttmp2, s_save_trapsts, SQ_WAVE_TRAPSTS_SAVECTX_MASK\n    s_cbranch_scc1  L_SAVE\n\nL_FETCH_2ND_TRAP:\n    // Preserve and clear scalar XNACK state before issuing scalar reads.\n    save_and_clear_ib_sts(ttmp14)\n\n    // Read second-level TBA/TMA from first-level TMA and jump if available.\n    // ttmp[2:5] and ttmp12 can be used (others hold SPI-initialized debug data)\n    // ttmp12 holds SQ_WAVE_STATUS\n    s_getreg_b32    ttmp14, hwreg(HW_REG_SQ_SHADER_TMA_LO)\n    s_getreg_b32    ttmp15, hwreg(HW_REG_SQ_SHADER_TMA_HI)\n    s_lshl_b64      [ttmp14, ttmp15], [ttmp14, ttmp15], 0x8\n\n    s_bitcmp1_b32   ttmp15, 0xF\n    s_cbranch_scc0  L_NO_SIGN_EXTEND_TMA\n    s_or_b32        ttmp15, ttmp15, 0xFFFF0000\nL_NO_SIGN_EXTEND_TMA:\n\n    s_load_dword    ttmp2, [ttmp14, ttmp15], 0x10 glc:1 // debug trap enabled flag\n    s_waitcnt       lgkmcnt(0)\n    s_lshl_b32      ttmp2, ttmp2, TTMP_DEBUG_TRAP_ENABLED_SHIFT\n    s_andn2_b32     s_save_ib_sts, s_save_ib_sts, TTMP_DEBUG_TRAP_ENABLED_MASK\n    s_or_b32        s_save_ib_sts, s_save_ib_sts, ttmp2\n\n    s_load_dwordx2  [ttmp2, ttmp3], [ttmp14, ttmp15], 0x0 glc:1 // second-level TBA\n    s_waitcnt       lgkmcnt(0)\n    s_load_dwordx2  [ttmp14, ttmp15], [ttmp14, ttmp15], 0x8 glc:1 // second-level TMA\n    s_waitcnt       lgkmcnt(0)\n\n    s_and_b64       [ttmp2, ttmp3], [ttmp2, ttmp3], [ttmp2, ttmp3]\n    s_cbranch_scc0  L_NO_NEXT_TRAP // second-level trap handler not been set\n    s_setpc_b64     [ttmp2, ttmp3] // jump to second-level trap handler\n\nL_NO_NEXT_TRAP:\n    // If not caused by trap then halt wave to prevent re-entry.\n    s_and_b32       ttmp2, s_save_pc_hi, (S_SAVE_PC_HI_TRAP_ID_MASK|S_SAVE_PC_HI_HT_MASK)\n    s_cbranch_scc1  L_TRAP_CASE\n    s_or_b32        s_save_status, s_save_status, SQ_WAVE_STATUS_HALT_MASK\n\n    // If the PC points to S_ENDPGM then context save will fail if STATUS.HALT is set.\n    // Rewind the PC to prevent this from occurring.\n    s_sub_u32       ttmp0, ttmp0, 0x8\n    s_subb_u32      ttmp1, ttmp1, 0x0\n\n    s_branch        L_EXIT_TRAP\n\nL_TRAP_CASE:\n    // Host trap will not cause trap re-entry.\n    s_and_b32       ttmp2, s_save_pc_hi, S_SAVE_PC_HI_HT_MASK\n    s_cbranch_scc1  L_EXIT_TRAP\n\n    // Advance past trap instruction to prevent re-entry.\n    s_add_u32       ttmp0, ttmp0, 0x4\n    s_addc_u32      ttmp1, ttmp1, 0x0\n\nL_EXIT_TRAP:\n    s_and_b32\tttmp1, ttmp1, 0xFFFF\n\n    restore_ib_sts(ttmp14)\n\n    // Restore SQ_WAVE_STATUS.\n    s_and_b64       exec, exec, exec // Restore STATUS.EXECZ, not writable by s_setreg_b32\n    s_and_b64       vcc, vcc, vcc    // Restore STATUS.VCCZ, not writable by s_setreg_b32\n    set_status_without_spi_prio(s_save_status, ttmp2)\n\n    s_rfe_b64       [ttmp0, ttmp1]\n\n    // *********\tEnd handling of non-CWSR traps\t *******************\n\n/**************************************************************************/\n/*\t\t\tsave routine\t\t\t\t\t  */\n/**************************************************************************/\n\nL_SAVE:\n    s_and_b32\t    s_save_pc_hi, s_save_pc_hi, 0x0000ffff    //pc[47:32]\n\n    s_mov_b32\t    s_save_tmp, 0\t\t\t\t\t\t\t    //clear saveCtx bit\n    s_setreg_b32    hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_SAVECTX_SHIFT, 1), s_save_tmp\t    //clear saveCtx bit\n\n    save_and_clear_ib_sts(s_save_tmp)\n\n    /*\t    inform SPI the readiness and wait for SPI's go signal */\n    s_mov_b32\t    s_save_exec_lo, exec_lo\t\t\t\t\t\t    //save EXEC and use EXEC for the go signal from SPI\n    s_mov_b32\t    s_save_exec_hi, exec_hi\n    s_mov_b64\t    exec,   0x0\t\t\t\t\t\t\t\t    //clear EXEC to get ready to receive\n\n\ts_sendmsg   sendmsg(MSG_SAVEWAVE)  //send SPI a message and wait for SPI's write to EXEC\n\n    // Set SPI_PRIO=2 to avoid starving instruction fetch in the waves we're waiting for.\n    s_or_b32 s_save_tmp, s_save_status, (2 << SQ_WAVE_STATUS_SPI_PRIO_SHIFT)\n    s_setreg_b32 hwreg(HW_REG_STATUS), s_save_tmp\n\n  L_SLEEP:\n    s_sleep 0x2\t\t       // sleep 1 (64clk) is not enough for 8 waves per SIMD, which will cause SQ hang, since the 7,8th wave could not get arbit to exec inst, while other waves are stuck into the sleep-loop and waiting for wrexec!=0\n\n\ts_cbranch_execz L_SLEEP\n\n    // Save trap temporaries 4-11, 13 initialized by SPI debug dispatch logic\n    // ttmp SR memory offset : size(VGPR)+size(SGPR)+0x40\n    get_vgpr_size_bytes(s_save_ttmps_lo)\n    get_sgpr_size_bytes(s_save_ttmps_hi)\n    s_add_u32\t    s_save_ttmps_lo, s_save_ttmps_lo, s_save_ttmps_hi\n    s_add_u32\t    s_save_ttmps_lo, s_save_ttmps_lo, s_save_spi_init_lo\n    s_addc_u32\t    s_save_ttmps_hi, s_save_spi_init_hi, 0x0\n    s_and_b32\t    s_save_ttmps_hi, s_save_ttmps_hi, 0xFFFF\n    s_store_dwordx4 [ttmp4, ttmp5, ttmp6, ttmp7], [s_save_ttmps_lo, s_save_ttmps_hi], 0x50 glc:1\n    ack_sqc_store_workaround()\n    s_store_dwordx4 [ttmp8, ttmp9, ttmp10, ttmp11], [s_save_ttmps_lo, s_save_ttmps_hi], 0x60 glc:1\n    ack_sqc_store_workaround()\n    s_store_dword   ttmp13, [s_save_ttmps_lo, s_save_ttmps_hi], 0x74 glc:1\n    ack_sqc_store_workaround()\n\n    /*\t    setup Resource Contants    */\n    s_mov_b32\t    s_save_buf_rsrc0,\ts_save_spi_init_lo\t\t\t\t\t\t\t//base_addr_lo\n    s_and_b32\t    s_save_buf_rsrc1,\ts_save_spi_init_hi, 0x0000FFFF\t\t\t\t\t\t//base_addr_hi\n    s_or_b32\t    s_save_buf_rsrc1,\ts_save_buf_rsrc1,  S_SAVE_BUF_RSRC_WORD1_STRIDE\n    s_mov_b32\t    s_save_buf_rsrc2,\t0\t\t\t\t\t\t\t\t\t//NUM_RECORDS initial value = 0 (in bytes) although not neccessarily inited\n    s_mov_b32\t    s_save_buf_rsrc3,\tS_SAVE_BUF_RSRC_WORD3_MISC\n\n    //FIXME  right now s_save_m0/s_save_mem_offset use tma_lo/tma_hi  (might need to save them before using them?)\n    s_mov_b32\t    s_save_m0,\t\tm0\t\t\t\t\t\t\t\t    //save M0\n\n    /*\t    global mem offset\t\t*/\n    s_mov_b32\t    s_save_mem_offset,\t0x0\t\t\t\t\t\t\t\t\t//mem offset initial value = 0\n\n\n\n\n    /*\t    save HW registers\t*/\n    //////////////////////////////\n\n  L_SAVE_HWREG:\n\t// HWREG SR memory offset : size(VGPR)+size(SGPR)\n       get_vgpr_size_bytes(s_save_mem_offset)\n       get_sgpr_size_bytes(s_save_tmp)\n       s_add_u32 s_save_mem_offset, s_save_mem_offset, s_save_tmp\n\n\n    s_mov_b32\t    s_save_buf_rsrc2, 0x4\t\t\t\t//NUM_RECORDS\tin bytes\n\ts_mov_b32\ts_save_buf_rsrc2,  0x1000000\t\t\t\t    //NUM_RECORDS in bytes\n\n\n    write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)\t\t\t//M0\n    write_hwreg_to_mem(s_save_pc_lo, s_save_buf_rsrc0, s_save_mem_offset)\t\t    //PC\n    write_hwreg_to_mem(s_save_pc_hi, s_save_buf_rsrc0, s_save_mem_offset)\n    write_hwreg_to_mem(s_save_exec_lo, s_save_buf_rsrc0, s_save_mem_offset)\t\t//EXEC\n    write_hwreg_to_mem(s_save_exec_hi, s_save_buf_rsrc0, s_save_mem_offset)\n    write_hwreg_to_mem(s_save_status, s_save_buf_rsrc0, s_save_mem_offset)\t\t//STATUS\n\n    //s_save_trapsts conflicts with s_save_alloc_size\n    s_getreg_b32    s_save_trapsts, hwreg(HW_REG_TRAPSTS)\n    write_hwreg_to_mem(s_save_trapsts, s_save_buf_rsrc0, s_save_mem_offset)\t\t//TRAPSTS\n\n    write_hwreg_to_mem(xnack_mask_lo, s_save_buf_rsrc0, s_save_mem_offset)\t    //XNACK_MASK_LO\n    write_hwreg_to_mem(xnack_mask_hi, s_save_buf_rsrc0, s_save_mem_offset)\t    //XNACK_MASK_HI\n\n    //use s_save_tmp would introduce conflict here between s_save_tmp and s_save_buf_rsrc2\n    s_getreg_b32    s_save_m0, hwreg(HW_REG_MODE)\t\t\t\t\t\t    //MODE\n    write_hwreg_to_mem(s_save_m0, s_save_buf_rsrc0, s_save_mem_offset)\n\n\n\n    /*\t    the first wave in the threadgroup\t */\n    s_and_b32\t    s_save_tmp, s_save_spi_init_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK\t// extract fisrt wave bit\n    s_mov_b32\t     s_save_exec_hi, 0x0\n    s_or_b32\t     s_save_exec_hi, s_save_tmp, s_save_exec_hi\t\t\t\t // save first wave bit to s_save_exec_hi.bits[26]\n\n\n    /*\t\tsave SGPRs\t*/\n\t// Save SGPR before LDS save, then the s0 to s4 can be used during LDS save...\n    //////////////////////////////\n\n    // SGPR SR memory offset : size(VGPR)\n    get_vgpr_size_bytes(s_save_mem_offset)\n    // TODO, change RSRC word to rearrange memory layout for SGPRS\n\n    s_getreg_b32    s_save_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE)\t\t//spgr_size\n    s_add_u32\t    s_save_alloc_size, s_save_alloc_size, 1\n    s_lshl_b32\t    s_save_alloc_size, s_save_alloc_size, 4\t\t\t    //Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value)\n\n\ts_lshl_b32\ts_save_buf_rsrc2,   s_save_alloc_size, 2\t\t    //NUM_RECORDS in bytes\n\n\ts_mov_b32\ts_save_buf_rsrc2,  0x1000000\t\t\t\t    //NUM_RECORDS in bytes\n\n\n    // backup s_save_buf_rsrc0,1 to s_save_pc_lo/hi, since write_16sgpr_to_mem function will change the rsrc0\n    //s_mov_b64 s_save_pc_lo, s_save_buf_rsrc0\n    s_mov_b64 s_save_xnack_mask_lo, s_save_buf_rsrc0\n    s_add_u32 s_save_buf_rsrc0, s_save_buf_rsrc0, s_save_mem_offset\n    s_addc_u32 s_save_buf_rsrc1, s_save_buf_rsrc1, 0\n\n    s_mov_b32\t    m0, 0x0\t\t\t    //SGPR initial index value =0\n    s_nop\t    0x0\t\t\t\t    //Manually inserted wait states\n  L_SAVE_SGPR_LOOP:\n    // SGPR is allocated in 16 SGPR granularity\n    s_movrels_b64   s0, s0     //s0 = s[0+m0], s1 = s[1+m0]\n    s_movrels_b64   s2, s2     //s2 = s[2+m0], s3 = s[3+m0]\n    s_movrels_b64   s4, s4     //s4 = s[4+m0], s5 = s[5+m0]\n    s_movrels_b64   s6, s6     //s6 = s[6+m0], s7 = s[7+m0]\n    s_movrels_b64   s8, s8     //s8 = s[8+m0], s9 = s[9+m0]\n    s_movrels_b64   s10, s10   //s10 = s[10+m0], s11 = s[11+m0]\n    s_movrels_b64   s12, s12   //s12 = s[12+m0], s13 = s[13+m0]\n    s_movrels_b64   s14, s14   //s14 = s[14+m0], s15 = s[15+m0]\n\n    write_16sgpr_to_mem(s0, s_save_buf_rsrc0, s_save_mem_offset) //PV: the best performance should be using s_buffer_store_dwordx4\n    s_add_u32\t    m0, m0, 16\t\t\t\t\t\t\t    //next sgpr index\n    s_cmp_lt_u32    m0, s_save_alloc_size\t\t\t\t\t    //scc = (m0 < s_save_alloc_size) ? 1 : 0\n    s_cbranch_scc1  L_SAVE_SGPR_LOOP\t\t\t\t\t//SGPR save is complete?\n    // restore s_save_buf_rsrc0,1\n    //s_mov_b64 s_save_buf_rsrc0, s_save_pc_lo\n    s_mov_b64 s_save_buf_rsrc0, s_save_xnack_mask_lo\n\n\n\n\n    /*\t\tsave first 4 VGPR, then LDS save could use   */\n\t// each wave will alloc 4 vgprs at least...\n    /////////////////////////////////////////////////////////////////////////////////////\n\n    s_mov_b32\t    s_save_mem_offset, 0\n    s_mov_b32\t    exec_lo, 0xFFFFFFFF\t\t\t\t\t\t    //need every thread from now on\n    s_mov_b32\t    exec_hi, 0xFFFFFFFF\n    s_mov_b32\t    xnack_mask_lo, 0x0\n    s_mov_b32\t    xnack_mask_hi, 0x0\n\n\ts_mov_b32\ts_save_buf_rsrc2,  0x1000000\t\t\t\t    //NUM_RECORDS in bytes\n\n\n    // VGPR Allocated in 4-GPR granularity\n\nif SAVE_AFTER_XNACK_ERROR\n\tcheck_if_tcp_store_ok()\n\ts_cbranch_scc1 L_SAVE_FIRST_VGPRS_WITH_TCP\n\n\twrite_vgprs_to_mem_with_sqc(v0, 4, s_save_buf_rsrc0, s_save_mem_offset)\n\ts_branch L_SAVE_LDS\n\nL_SAVE_FIRST_VGPRS_WITH_TCP:\nend\n\n    write_4vgprs_to_mem(s_save_buf_rsrc0, s_save_mem_offset)\n\n    /*\t\tsave LDS\t*/\n    //////////////////////////////\n\n  L_SAVE_LDS:\n\n\t// Change EXEC to all threads...\n    s_mov_b32\t    exec_lo, 0xFFFFFFFF\t  //need every thread from now on\n    s_mov_b32\t    exec_hi, 0xFFFFFFFF\n\n    s_getreg_b32    s_save_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE)\t\t    //lds_size\n    s_and_b32\t    s_save_alloc_size, s_save_alloc_size, 0xFFFFFFFF\t\t    //lds_size is zero?\n    s_cbranch_scc0  L_SAVE_LDS_DONE\t\t\t\t\t\t\t\t\t       //no lds used? jump to L_SAVE_DONE\n\n    s_barrier\t\t    //LDS is used? wait for other waves in the same TG\n    s_and_b32\t    s_save_tmp, s_save_exec_hi, S_SAVE_SPI_INIT_FIRST_WAVE_MASK\t\t       //exec is still used here\n    s_cbranch_scc0  L_SAVE_LDS_DONE\n\n\t// first wave do LDS save;\n\n    s_lshl_b32\t    s_save_alloc_size, s_save_alloc_size, 6\t\t\t    //LDS size in dwords = lds_size * 64dw\n    s_lshl_b32\t    s_save_alloc_size, s_save_alloc_size, 2\t\t\t    //LDS size in bytes\n    s_mov_b32\t    s_save_buf_rsrc2,  s_save_alloc_size\t\t\t    //NUM_RECORDS in bytes\n\n    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)\n    //\n    get_vgpr_size_bytes(s_save_mem_offset)\n    get_sgpr_size_bytes(s_save_tmp)\n    s_add_u32  s_save_mem_offset, s_save_mem_offset, s_save_tmp\n    s_add_u32 s_save_mem_offset, s_save_mem_offset, get_hwreg_size_bytes()\n\n\n\ts_mov_b32\ts_save_buf_rsrc2,  0x1000000\t\t      //NUM_RECORDS in bytes\n\n    s_mov_b32\t    m0, 0x0\t\t\t\t\t\t  //lds_offset initial value = 0\n\n\n      v_mbcnt_lo_u32_b32 v2, 0xffffffff, 0x0\n      v_mbcnt_hi_u32_b32 v3, 0xffffffff, v2\t// tid\n\nif SAVE_AFTER_XNACK_ERROR\n\tcheck_if_tcp_store_ok()\n\ts_cbranch_scc1 L_SAVE_LDS_WITH_TCP\n\n\tv_lshlrev_b32 v2, 2, v3\nL_SAVE_LDS_LOOP_SQC:\n\tds_read2_b32 v[0:1], v2 offset0:0 offset1:0x40\n\ts_waitcnt lgkmcnt(0)\n\n\twrite_vgprs_to_mem_with_sqc(v0, 2, s_save_buf_rsrc0, s_save_mem_offset)\n\n\tv_add_u32 v2, 0x200, v2\n\tv_cmp_lt_u32 vcc[0:1], v2, s_save_alloc_size\n\ts_cbranch_vccnz L_SAVE_LDS_LOOP_SQC\n\n\ts_branch L_SAVE_LDS_DONE\n\nL_SAVE_LDS_WITH_TCP:\nend\n\n      v_mul_i32_i24 v2, v3, 8\t// tid*8\n      v_mov_b32 v3, 256*2\n      s_mov_b32 m0, 0x10000\n      s_mov_b32 s0, s_save_buf_rsrc3\n      s_and_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0xFF7FFFFF\t  // disable add_tid\n      s_or_b32 s_save_buf_rsrc3, s_save_buf_rsrc3, 0x58000   //DFMT\n\nL_SAVE_LDS_LOOP_VECTOR:\n      ds_read_b64 v[0:1], v2\t//x =LDS[a], byte address\n      s_waitcnt lgkmcnt(0)\n      buffer_store_dwordx2  v[0:1], v2, s_save_buf_rsrc0, s_save_mem_offset offen:1  glc:1  slc:1\n//\ts_waitcnt vmcnt(0)\n//\tv_add_u32 v2, vcc[0:1], v2, v3\n      v_add_u32 v2, v2, v3\n      v_cmp_lt_u32 vcc[0:1], v2, s_save_alloc_size\n      s_cbranch_vccnz L_SAVE_LDS_LOOP_VECTOR\n\n      // restore rsrc3\n      s_mov_b32 s_save_buf_rsrc3, s0\n\nL_SAVE_LDS_DONE:\n\n\n    /*\t\tsave VGPRs  - set the Rest VGPRs\t*/\n    //////////////////////////////////////////////////////////////////////////////////////\n  L_SAVE_VGPR:\n    // VGPR SR memory offset: 0\n    // TODO rearrange the RSRC words to use swizzle for VGPR save...\n\n    s_mov_b32\t    s_save_mem_offset, (0+256*4)\t\t\t\t    // for the rest VGPRs\n    s_mov_b32\t    exec_lo, 0xFFFFFFFF\t\t\t\t\t\t    //need every thread from now on\n    s_mov_b32\t    exec_hi, 0xFFFFFFFF\n\n    get_num_arch_vgprs(s_save_alloc_size)\n    s_mov_b32\t    s_save_buf_rsrc2,  0x1000000\t\t\t\t    //NUM_RECORDS in bytes\n\n\n    // VGPR store using dw burst\n    s_mov_b32\t      m0, 0x4\t//VGPR initial index value =0\n    s_cmp_lt_u32      m0, s_save_alloc_size\n    s_cbranch_scc0    L_SAVE_VGPR_END\n\n\n    s_set_gpr_idx_on\tm0, 0x1 //M0[7:0] = M0[7:0] and M0[15:12] = 0x1\n    s_add_u32\t    s_save_alloc_size, s_save_alloc_size, 0x1000\t\t    //add 0x1000 since we compare m0 against it later\n\nif SAVE_AFTER_XNACK_ERROR\n\tcheck_if_tcp_store_ok()\n\ts_cbranch_scc1 L_SAVE_VGPR_LOOP\n\nL_SAVE_VGPR_LOOP_SQC:\n\twrite_vgprs_to_mem_with_sqc(v0, 4, s_save_buf_rsrc0, s_save_mem_offset)\n\n\ts_add_u32 m0, m0, 4\n\ts_cmp_lt_u32 m0, s_save_alloc_size\n\ts_cbranch_scc1 L_SAVE_VGPR_LOOP_SQC\n\n\ts_set_gpr_idx_off\n\ts_branch L_SAVE_VGPR_END\nend\n\n  L_SAVE_VGPR_LOOP:\n    v_mov_b32\t    v0, v0\t\t//v0 = v[0+m0]\n    v_mov_b32\t    v1, v1\t\t//v0 = v[0+m0]\n    v_mov_b32\t    v2, v2\t\t//v0 = v[0+m0]\n    v_mov_b32\t    v3, v3\t\t//v0 = v[0+m0]\n\n    write_4vgprs_to_mem(s_save_buf_rsrc0, s_save_mem_offset)\n\n    s_add_u32\t    m0, m0, 4\t\t\t\t\t\t\t    //next vgpr index\n    s_add_u32\t    s_save_mem_offset, s_save_mem_offset, 256*4\t\t\t    //every buffer_store_dword does 256 bytes\n    s_cmp_lt_u32    m0, s_save_alloc_size\t\t\t\t\t    //scc = (m0 < s_save_alloc_size) ? 1 : 0\n    s_cbranch_scc1  L_SAVE_VGPR_LOOP\t\t\t\t\t\t    //VGPR save is complete?\n    s_set_gpr_idx_off\n\nL_SAVE_VGPR_END:\n\n#if ASIC_FAMILY >= CHIP_ARCTURUS\n    // Save ACC VGPRs\n\n#if ASIC_FAMILY >= CHIP_ALDEBARAN\n    // ACC VGPR count may differ from ARCH VGPR count.\n    get_num_acc_vgprs(s_save_alloc_size, s_save_tmp)\n    s_and_b32       s_save_alloc_size, s_save_alloc_size, s_save_alloc_size\n    s_cbranch_scc0  L_SAVE_ACCVGPR_END\n    s_add_u32\t    s_save_alloc_size, s_save_alloc_size, 0x1000\t\t    //add 0x1000 since we compare m0 against it later\n#endif\n\n    s_mov_b32 m0, 0x0 //VGPR initial index value =0\n    s_set_gpr_idx_on m0, 0x1 //M0[7:0] = M0[7:0] and M0[15:12] = 0x1\n\nif SAVE_AFTER_XNACK_ERROR\n    check_if_tcp_store_ok()\n    s_cbranch_scc1 L_SAVE_ACCVGPR_LOOP\n\nL_SAVE_ACCVGPR_LOOP_SQC:\n    for var vgpr = 0; vgpr < 4; ++ vgpr\n        v_accvgpr_read v[vgpr], acc[vgpr]  // v[N] = acc[N+m0]\n    end\n\n    write_vgprs_to_mem_with_sqc(v0, 4, s_save_buf_rsrc0, s_save_mem_offset)\n\n    s_add_u32 m0, m0, 4\n    s_cmp_lt_u32 m0, s_save_alloc_size\n    s_cbranch_scc1 L_SAVE_ACCVGPR_LOOP_SQC\n\n    s_set_gpr_idx_off\n    s_branch L_SAVE_ACCVGPR_END\nend\n\nL_SAVE_ACCVGPR_LOOP:\n    for var vgpr = 0; vgpr < 4; ++ vgpr\n        v_accvgpr_read v[vgpr], acc[vgpr]  // v[N] = acc[N+m0]\n    end\n\n    write_4vgprs_to_mem(s_save_buf_rsrc0, s_save_mem_offset)\n\n    s_add_u32 m0, m0, 4\n    s_add_u32 s_save_mem_offset, s_save_mem_offset, 256*4\n    s_cmp_lt_u32 m0, s_save_alloc_size\n    s_cbranch_scc1 L_SAVE_ACCVGPR_LOOP\n    s_set_gpr_idx_off\n\nL_SAVE_ACCVGPR_END:\n#endif\n\n    s_branch\tL_END_PGM\n\n\n\n/**************************************************************************/\n/*\t\t\trestore routine\t\t\t\t\t  */\n/**************************************************************************/\n\nL_RESTORE:\n    /*\t    Setup Resource Contants    */\n    s_mov_b32\t    s_restore_buf_rsrc0,    s_restore_spi_init_lo\t\t\t\t\t\t\t    //base_addr_lo\n    s_and_b32\t    s_restore_buf_rsrc1,    s_restore_spi_init_hi, 0x0000FFFF\t\t\t\t\t\t    //base_addr_hi\n    s_or_b32\t    s_restore_buf_rsrc1,    s_restore_buf_rsrc1,  S_RESTORE_BUF_RSRC_WORD1_STRIDE\n    s_mov_b32\t    s_restore_buf_rsrc2,    0\t\t\t\t\t\t\t\t\t\t    //NUM_RECORDS initial value = 0 (in bytes)\n    s_mov_b32\t    s_restore_buf_rsrc3,    S_RESTORE_BUF_RSRC_WORD3_MISC\n\n    /*\t    global mem offset\t\t*/\n//  s_mov_b32\t    s_restore_mem_offset, 0x0\t\t\t\t    //mem offset initial value = 0\n\n    /*\t    the first wave in the threadgroup\t */\n    s_and_b32\t    s_restore_tmp, s_restore_spi_init_hi, S_RESTORE_SPI_INIT_FIRST_WAVE_MASK\n    s_cbranch_scc0  L_RESTORE_VGPR\n\n    /*\t\trestore LDS\t*/\n    //////////////////////////////\n  L_RESTORE_LDS:\n\n    s_mov_b32\t    exec_lo, 0xFFFFFFFF\t\t\t\t\t\t\t    //need every thread from now on   //be consistent with SAVE although can be moved ahead\n    s_mov_b32\t    exec_hi, 0xFFFFFFFF\n\n    s_getreg_b32    s_restore_alloc_size, hwreg(HW_REG_LDS_ALLOC,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT,SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE)\t\t//lds_size\n    s_and_b32\t    s_restore_alloc_size, s_restore_alloc_size, 0xFFFFFFFF\t\t    //lds_size is zero?\n    s_cbranch_scc0  L_RESTORE_VGPR\t\t\t\t\t\t\t    //no lds used? jump to L_RESTORE_VGPR\n    s_lshl_b32\t    s_restore_alloc_size, s_restore_alloc_size, 6\t\t\t    //LDS size in dwords = lds_size * 64dw\n    s_lshl_b32\t    s_restore_alloc_size, s_restore_alloc_size, 2\t\t\t    //LDS size in bytes\n    s_mov_b32\t    s_restore_buf_rsrc2,    s_restore_alloc_size\t\t\t    //NUM_RECORDS in bytes\n\n    // LDS at offset: size(VGPR)+SIZE(SGPR)+SIZE(HWREG)\n    //\n    get_vgpr_size_bytes(s_restore_mem_offset)\n    get_sgpr_size_bytes(s_restore_tmp)\n    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp\n    s_add_u32  s_restore_mem_offset, s_restore_mem_offset, get_hwreg_size_bytes()\t     //FIXME, Check if offset overflow???\n\n\n\ts_mov_b32\ts_restore_buf_rsrc2,  0x1000000\t\t\t\t\t    //NUM_RECORDS in bytes\n    s_mov_b32\t    m0, 0x0\t\t\t\t\t\t\t\t    //lds_offset initial value = 0\n\n  L_RESTORE_LDS_LOOP:\n\tbuffer_load_dword   v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1\t\t       // first 64DW\n\tbuffer_load_dword   v0, v0, s_restore_buf_rsrc0, s_restore_mem_offset lds:1 offset:256\t       // second 64DW\n    s_add_u32\t    m0, m0, 256*2\t\t\t\t\t\t// 128 DW\n    s_add_u32\t    s_restore_mem_offset, s_restore_mem_offset, 256*2\t\t//mem offset increased by 128DW\n    s_cmp_lt_u32    m0, s_restore_alloc_size\t\t\t\t\t//scc=(m0 < s_restore_alloc_size) ? 1 : 0\n    s_cbranch_scc1  L_RESTORE_LDS_LOOP\t\t\t\t\t\t\t    //LDS restore is complete?\n\n\n    /*\t\trestore VGPRs\t    */\n    //////////////////////////////\n  L_RESTORE_VGPR:\n    s_mov_b32\t    exec_lo, 0xFFFFFFFF\t\t\t\t\t\t\t    //need every thread from now on   //be consistent with SAVE although can be moved ahead\n    s_mov_b32\t    exec_hi, 0xFFFFFFFF\n    s_mov_b32\t    s_restore_buf_rsrc2,  0x1000000\t\t\t\t\t    //NUM_RECORDS in bytes\n\n    // Save ARCH VGPRs 4-N, then all ACC VGPRs, then ARCH VGPRs 0-3.\n    get_num_arch_vgprs(s_restore_alloc_size)\n    s_add_u32\t    s_restore_alloc_size, s_restore_alloc_size, 0x8000\t\t\t    //add 0x8000 since we compare m0 against it later\n\n    // ARCH VGPRs at offset: 0\n    s_mov_b32\t    s_restore_mem_offset, 0x0\n    s_mov_b32\t    s_restore_mem_offset_save, s_restore_mem_offset\t// restore start with v1, v0 will be the last\n    s_add_u32\t    s_restore_mem_offset, s_restore_mem_offset, 256*4\n    s_mov_b32\t    m0, 4\t\t\t\t//VGPR initial index value = 1\n    s_set_gpr_idx_on\tm0, 0x8\t\t\t\t\t\t\t\t    //M0[7:0] = M0[7:0] and M0[15:12] = 0x8\n\n  L_RESTORE_VGPR_LOOP:\n    read_4vgprs_from_mem(s_restore_buf_rsrc0, s_restore_mem_offset)\n    v_mov_b32\t    v0, v0\t\t\t\t\t\t\t\t    //v[0+m0] = v0\n    v_mov_b32\t    v1, v1\n    v_mov_b32\t    v2, v2\n    v_mov_b32\t    v3, v3\n    s_add_u32\t    m0, m0, 4\t\t\t\t\t\t\t\t    //next vgpr index\n    s_add_u32\t    s_restore_mem_offset, s_restore_mem_offset, 256*4\t\t\t\t//every buffer_load_dword does 256 bytes\n    s_cmp_lt_u32    m0, s_restore_alloc_size\t\t\t\t\t\t    //scc = (m0 < s_restore_alloc_size) ? 1 : 0\n    s_cbranch_scc1  L_RESTORE_VGPR_LOOP\t\t\t\t\t\t\t    //VGPR restore (except v0) is complete?\n\n#if ASIC_FAMILY >= CHIP_ALDEBARAN\n    // ACC VGPR count may differ from ARCH VGPR count.\n    get_num_acc_vgprs(s_restore_alloc_size, s_restore_tmp2)\n    s_and_b32       s_restore_alloc_size, s_restore_alloc_size, s_restore_alloc_size\n    s_cbranch_scc0  L_RESTORE_ACCVGPR_END\n    s_add_u32\t    s_restore_alloc_size, s_restore_alloc_size, 0x8000\t\t\t    //add 0x8000 since we compare m0 against it later\n#endif\n\n#if ASIC_FAMILY >= CHIP_ARCTURUS\n    // ACC VGPRs at offset: size(ARCH VGPRs)\n    s_mov_b32\t    m0, 0\n    s_set_gpr_idx_on\tm0, 0x8\t\t\t\t\t\t\t\t    //M0[7:0] = M0[7:0] and M0[15:12] = 0x8\n\n  L_RESTORE_ACCVGPR_LOOP:\n    read_4vgprs_from_mem(s_restore_buf_rsrc0, s_restore_mem_offset)\n\n    for var vgpr = 0; vgpr < 4; ++ vgpr\n        v_accvgpr_write acc[vgpr], v[vgpr]\n    end\n\n    s_add_u32\t    m0, m0, 4\t\t\t\t\t\t\t\t    //next vgpr index\n    s_add_u32\t    s_restore_mem_offset, s_restore_mem_offset, 256*4\t\t\t    //every buffer_load_dword does 256 bytes\n    s_cmp_lt_u32    m0, s_restore_alloc_size\t\t\t\t\t\t    //scc = (m0 < s_restore_alloc_size) ? 1 : 0\n    s_cbranch_scc1  L_RESTORE_ACCVGPR_LOOP\t\t\t\t\t\t    //VGPR restore (except v0) is complete?\n  L_RESTORE_ACCVGPR_END:\n#endif\n\n    s_set_gpr_idx_off\n\n    // Restore VGPRs 0-3 last, no longer needed.\n    read_4vgprs_from_mem(s_restore_buf_rsrc0, s_restore_mem_offset_save)\n\n    /*\t\trestore SGPRs\t    */\n    //////////////////////////////\n\n    // SGPR SR memory offset : size(VGPR)\n    get_vgpr_size_bytes(s_restore_mem_offset)\n    get_sgpr_size_bytes(s_restore_tmp)\n    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp\n    s_sub_u32 s_restore_mem_offset, s_restore_mem_offset, 16*4\t   // restore SGPR from S[n] to S[0], by 16 sgprs group\n    // TODO, change RSRC word to rearrange memory layout for SGPRS\n\n    s_getreg_b32    s_restore_alloc_size, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE)\t\t    //spgr_size\n    s_add_u32\t    s_restore_alloc_size, s_restore_alloc_size, 1\n    s_lshl_b32\t    s_restore_alloc_size, s_restore_alloc_size, 4\t\t\t    //Number of SGPRs = (sgpr_size + 1) * 16   (non-zero value)\n\n\ts_lshl_b32\ts_restore_buf_rsrc2,\ts_restore_alloc_size, 2\t\t\t    //NUM_RECORDS in bytes\n\ts_mov_b32\ts_restore_buf_rsrc2,  0x1000000\t\t\t\t\t    //NUM_RECORDS in bytes\n\n    s_mov_b32 m0, s_restore_alloc_size\n\n L_RESTORE_SGPR_LOOP:\n    read_16sgpr_from_mem(s0, s_restore_buf_rsrc0, s_restore_mem_offset)\t //PV: further performance improvement can be made\n    s_waitcnt\t    lgkmcnt(0)\t\t\t\t\t\t\t\t    //ensure data ready\n\n    s_sub_u32 m0, m0, 16    // Restore from S[n] to S[0]\n    s_nop 0 // hazard SALU M0=> S_MOVREL\n\n    s_movreld_b64   s0, s0\t//s[0+m0] = s0\n    s_movreld_b64   s2, s2\n    s_movreld_b64   s4, s4\n    s_movreld_b64   s6, s6\n    s_movreld_b64   s8, s8\n    s_movreld_b64   s10, s10\n    s_movreld_b64   s12, s12\n    s_movreld_b64   s14, s14\n\n    s_cmp_eq_u32    m0, 0\t\t//scc = (m0 < s_restore_alloc_size) ? 1 : 0\n    s_cbranch_scc0  L_RESTORE_SGPR_LOOP\t\t    //SGPR restore (except s0) is complete?\n\n    /*\t    restore HW registers    */\n    //////////////////////////////\n  L_RESTORE_HWREG:\n\n\n    // HWREG SR memory offset : size(VGPR)+size(SGPR)\n    get_vgpr_size_bytes(s_restore_mem_offset)\n    get_sgpr_size_bytes(s_restore_tmp)\n    s_add_u32 s_restore_mem_offset, s_restore_mem_offset, s_restore_tmp\n\n\n    s_mov_b32\t    s_restore_buf_rsrc2, 0x4\t\t\t\t\t\t    //NUM_RECORDS   in bytes\n\ts_mov_b32\ts_restore_buf_rsrc2,  0x1000000\t\t\t\t\t    //NUM_RECORDS in bytes\n\n    read_hwreg_from_mem(s_restore_m0, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //M0\n    read_hwreg_from_mem(s_restore_pc_lo, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t//PC\n    read_hwreg_from_mem(s_restore_pc_hi, s_restore_buf_rsrc0, s_restore_mem_offset)\n    read_hwreg_from_mem(s_restore_exec_lo, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //EXEC\n    read_hwreg_from_mem(s_restore_exec_hi, s_restore_buf_rsrc0, s_restore_mem_offset)\n    read_hwreg_from_mem(s_restore_status, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //STATUS\n    read_hwreg_from_mem(s_restore_trapsts, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //TRAPSTS\n    read_hwreg_from_mem(xnack_mask_lo, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //XNACK_MASK_LO\n    read_hwreg_from_mem(xnack_mask_hi, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t    //XNACK_MASK_HI\n    read_hwreg_from_mem(s_restore_mode, s_restore_buf_rsrc0, s_restore_mem_offset)\t\t//MODE\n\n    s_waitcnt\t    lgkmcnt(0)\t\t\t\t\t\t\t\t\t\t\t    //from now on, it is safe to restore STATUS and IB_STS\n\n    s_mov_b32\t    m0,\t\ts_restore_m0\n    s_mov_b32\t    exec_lo,\ts_restore_exec_lo\n    s_mov_b32\t    exec_hi,\ts_restore_exec_hi\n\n    s_and_b32\t    s_restore_m0, SQ_WAVE_TRAPSTS_PRE_SAVECTX_MASK, s_restore_trapsts\n    s_setreg_b32    hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_PRE_SAVECTX_SIZE), s_restore_m0\n    s_and_b32\t    s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_MASK, s_restore_trapsts\n    s_lshr_b32\t    s_restore_m0, s_restore_m0, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT\n    s_setreg_b32    hwreg(HW_REG_TRAPSTS, SQ_WAVE_TRAPSTS_POST_SAVECTX_SHIFT, SQ_WAVE_TRAPSTS_POST_SAVECTX_SIZE), s_restore_m0\n    //s_setreg_b32  hwreg(HW_REG_TRAPSTS),  s_restore_trapsts\t   //don't overwrite SAVECTX bit as it may be set through external SAVECTX during restore\n    s_setreg_b32    hwreg(HW_REG_MODE),\t    s_restore_mode\n\n    // Restore trap temporaries 4-11, 13 initialized by SPI debug dispatch logic\n    // ttmp SR memory offset : size(VGPR)+size(SGPR)+0x40\n    get_vgpr_size_bytes(s_restore_ttmps_lo)\n    get_sgpr_size_bytes(s_restore_ttmps_hi)\n    s_add_u32\t    s_restore_ttmps_lo, s_restore_ttmps_lo, s_restore_ttmps_hi\n    s_add_u32\t    s_restore_ttmps_lo, s_restore_ttmps_lo, s_restore_buf_rsrc0\n    s_addc_u32\t    s_restore_ttmps_hi, s_restore_buf_rsrc1, 0x0\n    s_and_b32\t    s_restore_ttmps_hi, s_restore_ttmps_hi, 0xFFFF\n    s_load_dwordx4  [ttmp4, ttmp5, ttmp6, ttmp7], [s_restore_ttmps_lo, s_restore_ttmps_hi], 0x50 glc:1\n    s_load_dwordx4  [ttmp8, ttmp9, ttmp10, ttmp11], [s_restore_ttmps_lo, s_restore_ttmps_hi], 0x60 glc:1\n    s_load_dword    ttmp13, [s_restore_ttmps_lo, s_restore_ttmps_hi], 0x74 glc:1\n    s_waitcnt\t    lgkmcnt(0)\n\n    restore_ib_sts(s_restore_tmp)\n\n    s_and_b32 s_restore_pc_hi, s_restore_pc_hi, 0x0000ffff\t//pc[47:32]\t   //Do it here in order not to affect STATUS\n    s_and_b64\t exec, exec, exec  // Restore STATUS.EXECZ, not writable by s_setreg_b32\n    s_and_b64\t vcc, vcc, vcc\t// Restore STATUS.VCCZ, not writable by s_setreg_b32\n    set_status_without_spi_prio(s_restore_status, s_restore_tmp) // SCC is included, which is changed by previous salu\n\n    s_barrier\t\t\t\t\t\t\t//barrier to ensure the readiness of LDS before access attempts from any other wave in the same TG //FIXME not performance-optimal at this time\n\n    s_rfe_b64 s_restore_pc_lo\t\t\t\t\t//Return to the main shader program and resume execution\n\n\n/**************************************************************************/\n/*\t\t\tthe END\t\t\t\t\t\t  */\n/**************************************************************************/\nL_END_PGM:\n    s_endpgm\n\nend\n\n\n/**************************************************************************/\n/*\t\t\tthe helper functions\t\t\t\t  */\n/**************************************************************************/\n\n//Only for save hwreg to mem\nfunction write_hwreg_to_mem(s, s_rsrc, s_mem_offset)\n\ts_mov_b32 exec_lo, m0\t\t\t//assuming exec_lo is not needed anymore from this point on\n\ts_mov_b32 m0, s_mem_offset\n\ts_buffer_store_dword s, s_rsrc, m0\tglc:1\n\tack_sqc_store_workaround()\n\ts_add_u32\ts_mem_offset, s_mem_offset, 4\n\ts_mov_b32   m0, exec_lo\nend\n\n\n// HWREG are saved before SGPRs, so all HWREG could be use.\nfunction write_16sgpr_to_mem(s, s_rsrc, s_mem_offset)\n\n\ts_buffer_store_dwordx4 s[0], s_rsrc, 0\tglc:1\n\tack_sqc_store_workaround()\n\ts_buffer_store_dwordx4 s[4], s_rsrc, 16\t glc:1\n\tack_sqc_store_workaround()\n\ts_buffer_store_dwordx4 s[8], s_rsrc, 32\t glc:1\n\tack_sqc_store_workaround()\n\ts_buffer_store_dwordx4 s[12], s_rsrc, 48 glc:1\n\tack_sqc_store_workaround()\n\ts_add_u32\ts_rsrc[0], s_rsrc[0], 4*16\n\ts_addc_u32\ts_rsrc[1], s_rsrc[1], 0x0\t      // +scc\nend\n\n\nfunction read_hwreg_from_mem(s, s_rsrc, s_mem_offset)\n    s_buffer_load_dword s, s_rsrc, s_mem_offset\t    glc:1\n    s_add_u32\t    s_mem_offset, s_mem_offset, 4\nend\n\nfunction read_16sgpr_from_mem(s, s_rsrc, s_mem_offset)\n    s_buffer_load_dwordx16 s, s_rsrc, s_mem_offset\tglc:1\n    s_sub_u32\t    s_mem_offset, s_mem_offset, 4*16\nend\n\nfunction check_if_tcp_store_ok\n\t// If STATUS.ALLOW_REPLAY=0 and TRAPSTS.XNACK_ERROR=1 then TCP stores will fail.\n\ts_and_b32 s_save_tmp, s_save_status, SQ_WAVE_STATUS_ALLOW_REPLAY_MASK\n\ts_cbranch_scc1 L_TCP_STORE_CHECK_DONE\n\n\ts_getreg_b32 s_save_tmp, hwreg(HW_REG_TRAPSTS)\n\ts_andn2_b32 s_save_tmp, SQ_WAVE_TRAPSTS_XNACK_ERROR_MASK, s_save_tmp\n\nL_TCP_STORE_CHECK_DONE:\nend\n\nfunction write_4vgprs_to_mem(s_rsrc, s_mem_offset)\n\tbuffer_store_dword v0, v0, s_rsrc, s_mem_offset slc:1 glc:1\n\tbuffer_store_dword v1, v0, s_rsrc, s_mem_offset slc:1 glc:1  offset:256\n\tbuffer_store_dword v2, v0, s_rsrc, s_mem_offset slc:1 glc:1  offset:256*2\n\tbuffer_store_dword v3, v0, s_rsrc, s_mem_offset slc:1 glc:1  offset:256*3\nend\n\nfunction read_4vgprs_from_mem(s_rsrc, s_mem_offset)\n\tbuffer_load_dword v0, v0, s_rsrc, s_mem_offset slc:1 glc:1\n\tbuffer_load_dword v1, v0, s_rsrc, s_mem_offset slc:1 glc:1 offset:256\n\tbuffer_load_dword v2, v0, s_rsrc, s_mem_offset slc:1 glc:1 offset:256*2\n\tbuffer_load_dword v3, v0, s_rsrc, s_mem_offset slc:1 glc:1 offset:256*3\n\ts_waitcnt vmcnt(0)\nend\n\nfunction write_vgpr_to_mem_with_sqc(v, s_rsrc, s_mem_offset)\n\ts_mov_b32 s4, 0\n\nL_WRITE_VGPR_LANE_LOOP:\n\tfor var lane = 0; lane < 4; ++ lane\n\t\tv_readlane_b32 s[lane], v, s4\n\t\ts_add_u32 s4, s4, 1\n\tend\n\n\ts_buffer_store_dwordx4 s[0:3], s_rsrc, s_mem_offset glc:1\n\tack_sqc_store_workaround()\n\n\ts_add_u32 s_mem_offset, s_mem_offset, 0x10\n\ts_cmp_eq_u32 s4, 0x40\n\ts_cbranch_scc0 L_WRITE_VGPR_LANE_LOOP\nend\n\nfunction write_vgprs_to_mem_with_sqc(v, n_vgprs, s_rsrc, s_mem_offset)\n\tfor var vgpr = 0; vgpr < n_vgprs; ++ vgpr\n\t\twrite_vgpr_to_mem_with_sqc(v[vgpr], s_rsrc, s_mem_offset)\n\tend\nend\n\nfunction get_lds_size_bytes(s_lds_size_byte)\n    // SQ LDS granularity is 64DW, while PGM_RSRC2.lds_size is in granularity 128DW\n    s_getreg_b32   s_lds_size_byte, hwreg(HW_REG_LDS_ALLOC, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SHIFT, SQ_WAVE_LDS_ALLOC_LDS_SIZE_SIZE)\t\t// lds_size\n    s_lshl_b32\t   s_lds_size_byte, s_lds_size_byte, 8\t\t\t    //LDS size in dwords = lds_size * 64 *4Bytes    // granularity 64DW\nend\n\nfunction get_vgpr_size_bytes(s_vgpr_size_byte)\n    s_getreg_b32   s_vgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE)\t //vpgr_size\n    s_add_u32\t   s_vgpr_size_byte, s_vgpr_size_byte, 1\n    s_lshl_b32\t   s_vgpr_size_byte, s_vgpr_size_byte, (2+8) //Number of VGPRs = (vgpr_size + 1) * 4 * 64 * 4\t(non-zero value)   //FIXME for GFX, zero is possible\n\n#if ASIC_FAMILY >= CHIP_ARCTURUS\n    s_lshl_b32     s_vgpr_size_byte, s_vgpr_size_byte, 1  // Double size for ACC VGPRs\n#endif\nend\n\nfunction get_sgpr_size_bytes(s_sgpr_size_byte)\n    s_getreg_b32   s_sgpr_size_byte, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_SGPR_SIZE_SIZE)\t //spgr_size\n    s_add_u32\t   s_sgpr_size_byte, s_sgpr_size_byte, 1\n    s_lshl_b32\t   s_sgpr_size_byte, s_sgpr_size_byte, 6 //Number of SGPRs = (sgpr_size + 1) * 16 *4   (non-zero value)\nend\n\nfunction get_hwreg_size_bytes\n    return 128 //HWREG size 128 bytes\nend\n\nfunction get_num_arch_vgprs(s_num_arch_vgprs)\n#if ASIC_FAMILY >= CHIP_ALDEBARAN\n    // VGPR count includes ACC VGPRs, use ACC VGPR offset for ARCH VGPR count.\n    s_getreg_b32    s_num_arch_vgprs, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_ACCV_OFFSET_SHIFT,SQ_WAVE_GPR_ALLOC_ACCV_OFFSET_SIZE)\n#else\n    s_getreg_b32    s_num_arch_vgprs, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE)\n#endif\n\n    // Number of VGPRs = (vgpr_size + 1) * 4\n    s_add_u32\t    s_num_arch_vgprs, s_num_arch_vgprs, 1\n    s_lshl_b32\t    s_num_arch_vgprs, s_num_arch_vgprs, 2\nend\n\n#if ASIC_FAMILY >= CHIP_ALDEBARAN\nfunction get_num_acc_vgprs(s_num_acc_vgprs, s_tmp)\n    // VGPR count = (GPR_ALLOC.VGPR_SIZE + 1) * 8\n    s_getreg_b32    s_num_acc_vgprs, hwreg(HW_REG_GPR_ALLOC,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SHIFT,SQ_WAVE_GPR_ALLOC_VGPR_SIZE_SIZE)\n    s_add_u32\t    s_num_acc_vgprs, s_num_acc_vgprs, 1\n    s_lshl_b32\t    s_num_acc_vgprs, s_num_acc_vgprs, 3\n\n    // ACC VGPR count = VGPR count - ARCH VGPR count.\n    get_num_arch_vgprs(s_tmp)\n    s_sub_u32\t    s_num_acc_vgprs, s_num_acc_vgprs, s_tmp\nend\n#endif\n\nfunction ack_sqc_store_workaround\n    if ACK_SQC_STORE\n        s_waitcnt lgkmcnt(0)\n    end\nend\n\nfunction set_status_without_spi_prio(status, tmp)\n    // Do not restore STATUS.SPI_PRIO since scheduler may have raised it.\n    s_lshr_b32      tmp, status, SQ_WAVE_STATUS_POST_SPI_PRIO_SHIFT\n    s_setreg_b32    hwreg(HW_REG_STATUS, SQ_WAVE_STATUS_POST_SPI_PRIO_SHIFT, SQ_WAVE_STATUS_POST_SPI_PRIO_SIZE), tmp\n    s_nop           0x2 // avoid S_SETREG => S_SETREG hazard\n    s_setreg_b32    hwreg(HW_REG_STATUS, SQ_WAVE_STATUS_PRE_SPI_PRIO_SHIFT, SQ_WAVE_STATUS_PRE_SPI_PRIO_SIZE), status\nend\n\nfunction save_and_clear_ib_sts(tmp)\n    // Save IB_STS.FIRST_REPLAY[15] and IB_STS.RCNT[20:16] into unused space s_save_ib_sts[31:26].\n    s_getreg_b32    tmp, hwreg(HW_REG_IB_STS)\n    s_and_b32       tmp, tmp, SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK\n    s_lshl_b32      tmp, tmp, (TTMP_SAVE_RCNT_FIRST_REPLAY_SHIFT - SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT)\n    s_andn2_b32     s_save_ib_sts, s_save_ib_sts, TTMP_SAVE_RCNT_FIRST_REPLAY_MASK\n    s_or_b32        s_save_ib_sts, s_save_ib_sts, tmp\n    s_setreg_imm32_b32 hwreg(HW_REG_IB_STS), 0x0\nend\n\nfunction restore_ib_sts(tmp)\n    s_lshr_b32      tmp, s_save_ib_sts, (TTMP_SAVE_RCNT_FIRST_REPLAY_SHIFT - SQ_WAVE_IB_STS_FIRST_REPLAY_SHIFT)\n    s_and_b32       tmp, tmp, SQ_WAVE_IB_STS_RCNT_FIRST_REPLAY_MASK\n    s_setreg_b32    hwreg(HW_REG_IB_STS), tmp\nend\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}