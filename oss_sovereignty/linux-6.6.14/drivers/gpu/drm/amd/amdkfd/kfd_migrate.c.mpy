{
  "module_name": "kfd_migrate.c",
  "hash_id": "3dcbcc6146e5a5ae980c7da9ac47ff73757351d2f9aeaf2ef8221d55ab7a6301",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdkfd/kfd_migrate.c",
  "human_readable_source": "\n \n#include <linux/types.h>\n#include <linux/hmm.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/migrate.h>\n#include \"amdgpu_sync.h\"\n#include \"amdgpu_object.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_res_cursor.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_svm.h\"\n#include \"kfd_migrate.h\"\n#include \"kfd_smi_events.h\"\n\n#ifdef dev_fmt\n#undef dev_fmt\n#endif\n#define dev_fmt(fmt) \"kfd_migrate: \" fmt\n\nstatic uint64_t\nsvm_migrate_direct_mapping_addr(struct amdgpu_device *adev, uint64_t addr)\n{\n\treturn addr + amdgpu_ttm_domain_start(adev, TTM_PL_VRAM);\n}\n\nstatic int\nsvm_migrate_gart_map(struct amdgpu_ring *ring, uint64_t npages,\n\t\t     dma_addr_t *addr, uint64_t *gart_addr, uint64_t flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_job *job;\n\tunsigned int num_dw, num_bytes;\n\tstruct dma_fence *fence;\n\tuint64_t src_addr, dst_addr;\n\tuint64_t pte_flags;\n\tvoid *cpu_addr;\n\tint r;\n\n\t \n\t*gart_addr = adev->gmc.gart_start;\n\n\tnum_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);\n\tnum_bytes = npages * 8;\n\n\tr = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     num_dw * 4 + num_bytes,\n\t\t\t\t     AMDGPU_IB_POOL_DELAYED,\n\t\t\t\t     &job);\n\tif (r)\n\t\treturn r;\n\n\tsrc_addr = num_dw * 4;\n\tsrc_addr += job->ibs[0].gpu_addr;\n\n\tdst_addr = amdgpu_bo_gpu_offset(adev->gart.bo);\n\tamdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,\n\t\t\t\tdst_addr, num_bytes, false);\n\n\tamdgpu_ring_pad_ib(ring, &job->ibs[0]);\n\tWARN_ON(job->ibs[0].length_dw > num_dw);\n\n\tpte_flags = AMDGPU_PTE_VALID | AMDGPU_PTE_READABLE;\n\tpte_flags |= AMDGPU_PTE_SYSTEM | AMDGPU_PTE_SNOOPED;\n\tif (!(flags & KFD_IOCTL_SVM_FLAG_GPU_RO))\n\t\tpte_flags |= AMDGPU_PTE_WRITEABLE;\n\tpte_flags |= adev->gart.gart_pte_flags;\n\n\tcpu_addr = &job->ibs[0].ptr[num_dw];\n\n\tamdgpu_gart_map(adev, 0, npages, addr, pte_flags, cpu_addr);\n\tfence = amdgpu_job_submit(job);\n\tdma_fence_put(fence);\n\n\treturn r;\n}\n\n \n\nstatic int\nsvm_migrate_copy_memory_gart(struct amdgpu_device *adev, dma_addr_t *sys,\n\t\t\t     uint64_t *vram, uint64_t npages,\n\t\t\t     enum MIGRATION_COPY_DIR direction,\n\t\t\t     struct dma_fence **mfence)\n{\n\tconst uint64_t GTT_MAX_PAGES = AMDGPU_GTT_MAX_TRANSFER_SIZE;\n\tstruct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;\n\tuint64_t gart_s, gart_d;\n\tstruct dma_fence *next;\n\tuint64_t size;\n\tint r;\n\n\tmutex_lock(&adev->mman.gtt_window_lock);\n\n\twhile (npages) {\n\t\tsize = min(GTT_MAX_PAGES, npages);\n\n\t\tif (direction == FROM_VRAM_TO_RAM) {\n\t\t\tgart_s = svm_migrate_direct_mapping_addr(adev, *vram);\n\t\t\tr = svm_migrate_gart_map(ring, size, sys, &gart_d, 0);\n\n\t\t} else if (direction == FROM_RAM_TO_VRAM) {\n\t\t\tr = svm_migrate_gart_map(ring, size, sys, &gart_s,\n\t\t\t\t\t\t KFD_IOCTL_SVM_FLAG_GPU_RO);\n\t\t\tgart_d = svm_migrate_direct_mapping_addr(adev, *vram);\n\t\t}\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"fail %d create gart mapping\\n\", r);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tr = amdgpu_copy_buffer(ring, gart_s, gart_d, size * PAGE_SIZE,\n\t\t\t\t       NULL, &next, false, true, false);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"fail %d to copy memory\\n\", r);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tdma_fence_put(*mfence);\n\t\t*mfence = next;\n\t\tnpages -= size;\n\t\tif (npages) {\n\t\t\tsys += size;\n\t\t\tvram += size;\n\t\t}\n\t}\n\nout_unlock:\n\tmutex_unlock(&adev->mman.gtt_window_lock);\n\n\treturn r;\n}\n\n \nstatic int\nsvm_migrate_copy_done(struct amdgpu_device *adev, struct dma_fence *mfence)\n{\n\tint r = 0;\n\n\tif (mfence) {\n\t\tr = dma_fence_wait(mfence, false);\n\t\tdma_fence_put(mfence);\n\t\tpr_debug(\"sdma copy memory fence done\\n\");\n\t}\n\n\treturn r;\n}\n\nunsigned long\nsvm_migrate_addr_to_pfn(struct amdgpu_device *adev, unsigned long addr)\n{\n\treturn (addr + adev->kfd.pgmap.range.start) >> PAGE_SHIFT;\n}\n\nstatic void\nsvm_migrate_get_vram_page(struct svm_range *prange, unsigned long pfn)\n{\n\tstruct page *page;\n\n\tpage = pfn_to_page(pfn);\n\tsvm_range_bo_ref(prange->svm_bo);\n\tpage->zone_device_data = prange->svm_bo;\n\tzone_device_page_init(page);\n}\n\nstatic void\nsvm_migrate_put_vram_page(struct amdgpu_device *adev, unsigned long addr)\n{\n\tstruct page *page;\n\n\tpage = pfn_to_page(svm_migrate_addr_to_pfn(adev, addr));\n\tunlock_page(page);\n\tput_page(page);\n}\n\nstatic unsigned long\nsvm_migrate_addr(struct amdgpu_device *adev, struct page *page)\n{\n\tunsigned long addr;\n\n\taddr = page_to_pfn(page) << PAGE_SHIFT;\n\treturn (addr - adev->kfd.pgmap.range.start);\n}\n\nstatic struct page *\nsvm_migrate_get_sys_page(struct vm_area_struct *vma, unsigned long addr)\n{\n\tstruct page *page;\n\n\tpage = alloc_page_vma(GFP_HIGHUSER, vma, addr);\n\tif (page)\n\t\tlock_page(page);\n\n\treturn page;\n}\n\nstatic void svm_migrate_put_sys_page(unsigned long addr)\n{\n\tstruct page *page;\n\n\tpage = pfn_to_page(addr >> PAGE_SHIFT);\n\tunlock_page(page);\n\tput_page(page);\n}\n\nstatic unsigned long svm_migrate_successful_pages(struct migrate_vma *migrate)\n{\n\tunsigned long cpages = 0;\n\tunsigned long i;\n\n\tfor (i = 0; i < migrate->npages; i++) {\n\t\tif (migrate->src[i] & MIGRATE_PFN_VALID &&\n\t\t    migrate->src[i] & MIGRATE_PFN_MIGRATE)\n\t\t\tcpages++;\n\t}\n\treturn cpages;\n}\n\nstatic unsigned long svm_migrate_unsuccessful_pages(struct migrate_vma *migrate)\n{\n\tunsigned long upages = 0;\n\tunsigned long i;\n\n\tfor (i = 0; i < migrate->npages; i++) {\n\t\tif (migrate->src[i] & MIGRATE_PFN_VALID &&\n\t\t    !(migrate->src[i] & MIGRATE_PFN_MIGRATE))\n\t\t\tupages++;\n\t}\n\treturn upages;\n}\n\nstatic int\nsvm_migrate_copy_to_vram(struct kfd_node *node, struct svm_range *prange,\n\t\t\t struct migrate_vma *migrate, struct dma_fence **mfence,\n\t\t\t dma_addr_t *scratch, uint64_t ttm_res_offset)\n{\n\tuint64_t npages = migrate->cpages;\n\tstruct amdgpu_device *adev = node->adev;\n\tstruct device *dev = adev->dev;\n\tstruct amdgpu_res_cursor cursor;\n\tdma_addr_t *src;\n\tuint64_t *dst;\n\tuint64_t i, j;\n\tint r;\n\n\tpr_debug(\"svms 0x%p [0x%lx 0x%lx 0x%llx]\\n\", prange->svms, prange->start,\n\t\t prange->last, ttm_res_offset);\n\n\tsrc = scratch;\n\tdst = (uint64_t *)(scratch + npages);\n\n\tamdgpu_res_first(prange->ttm_res, ttm_res_offset,\n\t\t\t npages << PAGE_SHIFT, &cursor);\n\tfor (i = j = 0; i < npages; i++) {\n\t\tstruct page *spage;\n\n\t\tdst[i] = cursor.start + (j << PAGE_SHIFT);\n\t\tmigrate->dst[i] = svm_migrate_addr_to_pfn(adev, dst[i]);\n\t\tsvm_migrate_get_vram_page(prange, migrate->dst[i]);\n\t\tmigrate->dst[i] = migrate_pfn(migrate->dst[i]);\n\n\t\tspage = migrate_pfn_to_page(migrate->src[i]);\n\t\tif (spage && !is_zone_device_page(spage)) {\n\t\t\tsrc[i] = dma_map_page(dev, spage, 0, PAGE_SIZE,\n\t\t\t\t\t      DMA_TO_DEVICE);\n\t\t\tr = dma_mapping_error(dev, src[i]);\n\t\t\tif (r) {\n\t\t\t\tdev_err(dev, \"%s: fail %d dma_map_page\\n\",\n\t\t\t\t\t__func__, r);\n\t\t\t\tgoto out_free_vram_pages;\n\t\t\t}\n\t\t} else {\n\t\t\tif (j) {\n\t\t\t\tr = svm_migrate_copy_memory_gart(\n\t\t\t\t\t\tadev, src + i - j,\n\t\t\t\t\t\tdst + i - j, j,\n\t\t\t\t\t\tFROM_RAM_TO_VRAM,\n\t\t\t\t\t\tmfence);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out_free_vram_pages;\n\t\t\t\tamdgpu_res_next(&cursor, (j + 1) << PAGE_SHIFT);\n\t\t\t\tj = 0;\n\t\t\t} else {\n\t\t\t\tamdgpu_res_next(&cursor, PAGE_SIZE);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_debug_ratelimited(\"dma mapping src to 0x%llx, pfn 0x%lx\\n\",\n\t\t\t\t     src[i] >> PAGE_SHIFT, page_to_pfn(spage));\n\n\t\tif (j >= (cursor.size >> PAGE_SHIFT) - 1 && i < npages - 1) {\n\t\t\tr = svm_migrate_copy_memory_gart(adev, src + i - j,\n\t\t\t\t\t\t\t dst + i - j, j + 1,\n\t\t\t\t\t\t\t FROM_RAM_TO_VRAM,\n\t\t\t\t\t\t\t mfence);\n\t\t\tif (r)\n\t\t\t\tgoto out_free_vram_pages;\n\t\t\tamdgpu_res_next(&cursor, (j + 1) * PAGE_SIZE);\n\t\t\tj = 0;\n\t\t} else {\n\t\t\tj++;\n\t\t}\n\t}\n\n\tr = svm_migrate_copy_memory_gart(adev, src + i - j, dst + i - j, j,\n\t\t\t\t\t FROM_RAM_TO_VRAM, mfence);\n\nout_free_vram_pages:\n\tif (r) {\n\t\tpr_debug(\"failed %d to copy memory to vram\\n\", r);\n\t\twhile (i--) {\n\t\t\tsvm_migrate_put_vram_page(adev, dst[i]);\n\t\t\tmigrate->dst[i] = 0;\n\t\t}\n\t}\n\n#ifdef DEBUG_FORCE_MIXED_DOMAINS\n\tfor (i = 0, j = 0; i < npages; i += 4, j++) {\n\t\tif (j & 1)\n\t\t\tcontinue;\n\t\tsvm_migrate_put_vram_page(adev, dst[i]);\n\t\tmigrate->dst[i] = 0;\n\t\tsvm_migrate_put_vram_page(adev, dst[i + 1]);\n\t\tmigrate->dst[i + 1] = 0;\n\t\tsvm_migrate_put_vram_page(adev, dst[i + 2]);\n\t\tmigrate->dst[i + 2] = 0;\n\t\tsvm_migrate_put_vram_page(adev, dst[i + 3]);\n\t\tmigrate->dst[i + 3] = 0;\n\t}\n#endif\n\n\treturn r;\n}\n\nstatic long\nsvm_migrate_vma_to_vram(struct kfd_node *node, struct svm_range *prange,\n\t\t\tstruct vm_area_struct *vma, uint64_t start,\n\t\t\tuint64_t end, uint32_t trigger, uint64_t ttm_res_offset)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tstruct amdgpu_device *adev = node->adev;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate = { 0 };\n\tunsigned long cpages = 0;\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.flags = MIGRATE_VMA_SELECT_SYSTEM;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(node, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      0, node->id, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate sys pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_vram(node, prange, &migrate, &mfence, scratch, ttm_res_offset);\n\tmigrate_vma_pages(&migrate);\n\n\tpr_debug(\"successful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\tsvm_migrate_successful_pages(&migrate), cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(node, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    0, node->id, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_node(prange, node);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_in, pdd->page_in + cpages);\n\n\t\treturn cpages;\n\t}\n\treturn r;\n}\n\n \nstatic int\nsvm_migrate_ram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\tstruct mm_struct *mm, uint32_t trigger)\n{\n\tunsigned long addr, start, end;\n\tstruct vm_area_struct *vma;\n\tuint64_t ttm_res_offset;\n\tstruct kfd_node *node;\n\tunsigned long cpages = 0;\n\tlong r = 0;\n\n\tif (prange->actual_loc == best_loc) {\n\t\tpr_debug(\"svms 0x%p [0x%lx 0x%lx] already on best_loc 0x%x\\n\",\n\t\t\t prange->svms, prange->start, prange->last, best_loc);\n\t\treturn 0;\n\t}\n\n\tnode = svm_range_get_node_by_id(prange, best_loc);\n\tif (!node) {\n\t\tpr_debug(\"failed to get kfd node by id 0x%x\\n\", best_loc);\n\t\treturn -ENODEV;\n\t}\n\n\tpr_debug(\"svms 0x%p [0x%lx 0x%lx] to gpu 0x%x\\n\", prange->svms,\n\t\t prange->start, prange->last, best_loc);\n\n\tstart = prange->start << PAGE_SHIFT;\n\tend = (prange->last + 1) << PAGE_SHIFT;\n\n\tr = svm_range_vram_node_new(node, prange, true);\n\tif (r) {\n\t\tdev_dbg(node->adev->dev, \"fail %ld to alloc vram\\n\", r);\n\t\treturn r;\n\t}\n\tttm_res_offset = prange->offset << PAGE_SHIFT;\n\n\tfor (addr = start; addr < end;) {\n\t\tunsigned long next;\n\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\tnext = min(vma->vm_end, end);\n\t\tr = svm_migrate_vma_to_vram(node, prange, vma, addr, next, trigger, ttm_res_offset);\n\t\tif (r < 0) {\n\t\t\tpr_debug(\"failed %ld to migrate\\n\", r);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tcpages += r;\n\t\t}\n\t\tttm_res_offset += next - addr;\n\t\taddr = next;\n\t}\n\n\tif (cpages) {\n\t\tprange->actual_loc = best_loc;\n\t\tsvm_range_free_dma_mappings(prange, true);\n\t} else {\n\t\tsvm_range_vram_node_free(prange);\n\t}\n\n\treturn r < 0 ? r : 0;\n}\n\nstatic void svm_migrate_page_free(struct page *page)\n{\n\tstruct svm_range_bo *svm_bo = page->zone_device_data;\n\n\tif (svm_bo) {\n\t\tpr_debug_ratelimited(\"ref: %d\\n\", kref_read(&svm_bo->kref));\n\t\tsvm_range_bo_unref_async(svm_bo);\n\t}\n}\n\nstatic int\nsvm_migrate_copy_to_ram(struct amdgpu_device *adev, struct svm_range *prange,\n\t\t\tstruct migrate_vma *migrate, struct dma_fence **mfence,\n\t\t\tdma_addr_t *scratch, uint64_t npages)\n{\n\tstruct device *dev = adev->dev;\n\tuint64_t *src;\n\tdma_addr_t *dst;\n\tstruct page *dpage;\n\tuint64_t i = 0, j;\n\tuint64_t addr;\n\tint r = 0;\n\n\tpr_debug(\"svms 0x%p [0x%lx 0x%lx]\\n\", prange->svms, prange->start,\n\t\t prange->last);\n\n\taddr = prange->start << PAGE_SHIFT;\n\n\tsrc = (uint64_t *)(scratch + npages);\n\tdst = scratch;\n\n\tfor (i = 0, j = 0; i < npages; i++, addr += PAGE_SIZE) {\n\t\tstruct page *spage;\n\n\t\tspage = migrate_pfn_to_page(migrate->src[i]);\n\t\tif (!spage || !is_zone_device_page(spage)) {\n\t\t\tpr_debug(\"invalid page. Could be in CPU already svms 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t\t prange->svms, prange->start, prange->last);\n\t\t\tif (j) {\n\t\t\t\tr = svm_migrate_copy_memory_gart(adev, dst + i - j,\n\t\t\t\t\t\t\t\t src + i - j, j,\n\t\t\t\t\t\t\t\t FROM_VRAM_TO_RAM,\n\t\t\t\t\t\t\t\t mfence);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out_oom;\n\t\t\t\tj = 0;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tsrc[i] = svm_migrate_addr(adev, spage);\n\t\tif (j > 0 && src[i] != src[i - 1] + PAGE_SIZE) {\n\t\t\tr = svm_migrate_copy_memory_gart(adev, dst + i - j,\n\t\t\t\t\t\t\t src + i - j, j,\n\t\t\t\t\t\t\t FROM_VRAM_TO_RAM,\n\t\t\t\t\t\t\t mfence);\n\t\t\tif (r)\n\t\t\t\tgoto out_oom;\n\t\t\tj = 0;\n\t\t}\n\n\t\tdpage = svm_migrate_get_sys_page(migrate->vma, addr);\n\t\tif (!dpage) {\n\t\t\tpr_debug(\"failed get page svms 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t\t prange->svms, prange->start, prange->last);\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_oom;\n\t\t}\n\n\t\tdst[i] = dma_map_page(dev, dpage, 0, PAGE_SIZE, DMA_FROM_DEVICE);\n\t\tr = dma_mapping_error(dev, dst[i]);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"%s: fail %d dma_map_page\\n\", __func__, r);\n\t\t\tgoto out_oom;\n\t\t}\n\n\t\tpr_debug_ratelimited(\"dma mapping dst to 0x%llx, pfn 0x%lx\\n\",\n\t\t\t\t     dst[i] >> PAGE_SHIFT, page_to_pfn(dpage));\n\n\t\tmigrate->dst[i] = migrate_pfn(page_to_pfn(dpage));\n\t\tj++;\n\t}\n\n\tr = svm_migrate_copy_memory_gart(adev, dst + i - j, src + i - j, j,\n\t\t\t\t\t FROM_VRAM_TO_RAM, mfence);\n\nout_oom:\n\tif (r) {\n\t\tpr_debug(\"failed %d copy to ram\\n\", r);\n\t\twhile (i--) {\n\t\t\tsvm_migrate_put_sys_page(dst[i]);\n\t\t\tmigrate->dst[i] = 0;\n\t\t}\n\t}\n\n\treturn r;\n}\n\n \nstatic long\nsvm_migrate_vma_to_ram(struct kfd_node *node, struct svm_range *prange,\n\t\t       struct vm_area_struct *vma, uint64_t start, uint64_t end,\n\t\t       uint32_t trigger, struct page *fault_page)\n{\n\tstruct kfd_process *p = container_of(prange->svms, struct kfd_process, svms);\n\tuint64_t npages = (end - start) >> PAGE_SHIFT;\n\tunsigned long upages = npages;\n\tunsigned long cpages = 0;\n\tstruct amdgpu_device *adev = node->adev;\n\tstruct kfd_process_device *pdd;\n\tstruct dma_fence *mfence = NULL;\n\tstruct migrate_vma migrate = { 0 };\n\tdma_addr_t *scratch;\n\tvoid *buf;\n\tint r = -ENOMEM;\n\n\tmemset(&migrate, 0, sizeof(migrate));\n\tmigrate.vma = vma;\n\tmigrate.start = start;\n\tmigrate.end = end;\n\tmigrate.pgmap_owner = SVM_ADEV_PGMAP_OWNER(adev);\n\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\tmigrate.flags = MIGRATE_VMA_SELECT_DEVICE_COHERENT;\n\telse\n\t\tmigrate.flags = MIGRATE_VMA_SELECT_DEVICE_PRIVATE;\n\n\tbuf = kvcalloc(npages,\n\t\t       2 * sizeof(*migrate.src) + sizeof(uint64_t) + sizeof(dma_addr_t),\n\t\t       GFP_KERNEL);\n\tif (!buf)\n\t\tgoto out;\n\n\tmigrate.src = buf;\n\tmigrate.dst = migrate.src + npages;\n\tmigrate.fault_page = fault_page;\n\tscratch = (dma_addr_t *)(migrate.dst + npages);\n\n\tkfd_smi_event_migration_start(node, p->lead_thread->pid,\n\t\t\t\t      start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t      node->id, 0, prange->prefetch_loc,\n\t\t\t\t      prange->preferred_loc, trigger);\n\n\tr = migrate_vma_setup(&migrate);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%s: vma setup fail %d range [0x%lx 0x%lx]\\n\",\n\t\t\t__func__, r, prange->start, prange->last);\n\t\tgoto out_free;\n\t}\n\n\tcpages = migrate.cpages;\n\tif (!cpages) {\n\t\tpr_debug(\"failed collect migrate device pages [0x%lx 0x%lx]\\n\",\n\t\t\t prange->start, prange->last);\n\t\tupages = svm_migrate_unsuccessful_pages(&migrate);\n\t\tgoto out_free;\n\t}\n\tif (cpages != npages)\n\t\tpr_debug(\"partial migration, 0x%lx/0x%llx pages migrated\\n\",\n\t\t\t cpages, npages);\n\telse\n\t\tpr_debug(\"0x%lx pages migrated\\n\", cpages);\n\n\tr = svm_migrate_copy_to_ram(adev, prange, &migrate, &mfence,\n\t\t\t\t    scratch, npages);\n\tmigrate_vma_pages(&migrate);\n\n\tupages = svm_migrate_unsuccessful_pages(&migrate);\n\tpr_debug(\"unsuccessful/cpages/npages 0x%lx/0x%lx/0x%lx\\n\",\n\t\t upages, cpages, migrate.npages);\n\n\tsvm_migrate_copy_done(adev, mfence);\n\tmigrate_vma_finalize(&migrate);\n\n\tkfd_smi_event_migration_end(node, p->lead_thread->pid,\n\t\t\t\t    start >> PAGE_SHIFT, end >> PAGE_SHIFT,\n\t\t\t\t    node->id, 0, trigger);\n\n\tsvm_range_dma_unmap(adev->dev, scratch, 0, npages);\n\nout_free:\n\tkvfree(buf);\nout:\n\tif (!r && cpages) {\n\t\tpdd = svm_range_get_pdd_by_node(prange, node);\n\t\tif (pdd)\n\t\t\tWRITE_ONCE(pdd->page_out, pdd->page_out + cpages);\n\t}\n\treturn r ? r : upages;\n}\n\n \nint svm_migrate_vram_to_ram(struct svm_range *prange, struct mm_struct *mm,\n\t\t\t    uint32_t trigger, struct page *fault_page)\n{\n\tstruct kfd_node *node;\n\tstruct vm_area_struct *vma;\n\tunsigned long addr;\n\tunsigned long start;\n\tunsigned long end;\n\tunsigned long upages = 0;\n\tlong r = 0;\n\n\tif (!prange->actual_loc) {\n\t\tpr_debug(\"[0x%lx 0x%lx] already migrated to ram\\n\",\n\t\t\t prange->start, prange->last);\n\t\treturn 0;\n\t}\n\n\tnode = svm_range_get_node_by_id(prange, prange->actual_loc);\n\tif (!node) {\n\t\tpr_debug(\"failed to get kfd node by id 0x%x\\n\", prange->actual_loc);\n\t\treturn -ENODEV;\n\t}\n\tpr_debug(\"svms 0x%p prange 0x%p [0x%lx 0x%lx] from gpu 0x%x to ram\\n\",\n\t\t prange->svms, prange, prange->start, prange->last,\n\t\t prange->actual_loc);\n\n\tstart = prange->start << PAGE_SHIFT;\n\tend = (prange->last + 1) << PAGE_SHIFT;\n\n\tfor (addr = start; addr < end;) {\n\t\tunsigned long next;\n\n\t\tvma = vma_lookup(mm, addr);\n\t\tif (!vma) {\n\t\t\tpr_debug(\"failed to find vma for prange %p\\n\", prange);\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tnext = min(vma->vm_end, end);\n\t\tr = svm_migrate_vma_to_ram(node, prange, vma, addr, next, trigger,\n\t\t\tfault_page);\n\t\tif (r < 0) {\n\t\t\tpr_debug(\"failed %ld to migrate prange %p\\n\", r, prange);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tupages += r;\n\t\t}\n\t\taddr = next;\n\t}\n\n\tif (r >= 0 && !upages) {\n\t\tsvm_range_vram_node_free(prange);\n\t\tprange->actual_loc = 0;\n\t}\n\n\treturn r < 0 ? r : 0;\n}\n\n \nstatic int\nsvm_migrate_vram_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t\t struct mm_struct *mm, uint32_t trigger)\n{\n\tint r, retries = 3;\n\n\t \n\n\tpr_debug(\"from gpu 0x%x to gpu 0x%x\\n\", prange->actual_loc, best_loc);\n\n\tdo {\n\t\tr = svm_migrate_vram_to_ram(prange, mm, trigger, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t} while (prange->actual_loc && --retries);\n\n\tif (prange->actual_loc)\n\t\treturn -EDEADLK;\n\n\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n}\n\nint\nsvm_migrate_to_vram(struct svm_range *prange, uint32_t best_loc,\n\t\t    struct mm_struct *mm, uint32_t trigger)\n{\n\tif  (!prange->actual_loc)\n\t\treturn svm_migrate_ram_to_vram(prange, best_loc, mm, trigger);\n\telse\n\t\treturn svm_migrate_vram_to_vram(prange, best_loc, mm, trigger);\n\n}\n\n \nstatic vm_fault_t svm_migrate_to_ram(struct vm_fault *vmf)\n{\n\tunsigned long addr = vmf->address;\n\tstruct svm_range_bo *svm_bo;\n\tenum svm_work_list_ops op;\n\tstruct svm_range *parent;\n\tstruct svm_range *prange;\n\tstruct kfd_process *p;\n\tstruct mm_struct *mm;\n\tint r = 0;\n\n\tsvm_bo = vmf->page->zone_device_data;\n\tif (!svm_bo) {\n\t\tpr_debug(\"failed get device page at addr 0x%lx\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tif (!mmget_not_zero(svm_bo->eviction_fence->mm)) {\n\t\tpr_debug(\"addr 0x%lx of process mm is destroyed\\n\", addr);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tmm = svm_bo->eviction_fence->mm;\n\tif (mm != vmf->vma->vm_mm)\n\t\tpr_debug(\"addr 0x%lx is COW mapping in child process\\n\", addr);\n\n\tp = kfd_lookup_process_by_mm(mm);\n\tif (!p) {\n\t\tpr_debug(\"failed find process at fault address 0x%lx\\n\", addr);\n\t\tr = VM_FAULT_SIGBUS;\n\t\tgoto out_mmput;\n\t}\n\tif (READ_ONCE(p->svms.faulting_task) == current) {\n\t\tpr_debug(\"skipping ram migration\\n\");\n\t\tr = 0;\n\t\tgoto out_unref_process;\n\t}\n\n\tpr_debug(\"CPU page fault svms 0x%p address 0x%lx\\n\", &p->svms, addr);\n\taddr >>= PAGE_SHIFT;\n\n\tmutex_lock(&p->svms.lock);\n\n\tprange = svm_range_from_addr(&p->svms, addr, &parent);\n\tif (!prange) {\n\t\tpr_debug(\"failed get range svms 0x%p addr 0x%lx\\n\", &p->svms, addr);\n\t\tr = -EFAULT;\n\t\tgoto out_unlock_svms;\n\t}\n\n\tmutex_lock(&parent->migrate_mutex);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->migrate_mutex, 1);\n\n\tif (!prange->actual_loc)\n\t\tgoto out_unlock_prange;\n\n\tsvm_range_lock(parent);\n\tif (prange != parent)\n\t\tmutex_lock_nested(&prange->lock, 1);\n\tr = svm_range_split_by_granularity(p, mm, addr, parent, prange);\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->lock);\n\tsvm_range_unlock(parent);\n\tif (r) {\n\t\tpr_debug(\"failed %d to split range by granularity\\n\", r);\n\t\tgoto out_unlock_prange;\n\t}\n\n\tr = svm_migrate_vram_to_ram(prange, vmf->vma->vm_mm,\n\t\t\t\t    KFD_MIGRATE_TRIGGER_PAGEFAULT_CPU,\n\t\t\t\t    vmf->page);\n\tif (r)\n\t\tpr_debug(\"failed %d migrate svms 0x%p range 0x%p [0x%lx 0x%lx]\\n\",\n\t\t\t r, prange->svms, prange, prange->start, prange->last);\n\n\t \n\tif (p->xnack_enabled && parent == prange)\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER_AND_MAP;\n\telse\n\t\top = SVM_OP_UPDATE_RANGE_NOTIFIER;\n\tsvm_range_add_list_work(&p->svms, parent, mm, op);\n\tschedule_deferred_list_work(&p->svms);\n\nout_unlock_prange:\n\tif (prange != parent)\n\t\tmutex_unlock(&prange->migrate_mutex);\n\tmutex_unlock(&parent->migrate_mutex);\nout_unlock_svms:\n\tmutex_unlock(&p->svms.lock);\nout_unref_process:\n\tpr_debug(\"CPU fault svms 0x%p address 0x%lx done\\n\", &p->svms, addr);\n\tkfd_unref_process(p);\nout_mmput:\n\tmmput(mm);\n\treturn r ? VM_FAULT_SIGBUS : 0;\n}\n\nstatic const struct dev_pagemap_ops svm_migrate_pgmap_ops = {\n\t.page_free\t\t= svm_migrate_page_free,\n\t.migrate_to_ram\t\t= svm_migrate_to_ram,\n};\n\n \n#define SVM_HMM_PAGE_STRUCT_SIZE(size) ((size)/PAGE_SIZE * sizeof(struct page))\n\nint kgd2kfd_init_zone_device(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_kfd_dev *kfddev = &adev->kfd;\n\tstruct dev_pagemap *pgmap;\n\tstruct resource *res = NULL;\n\tunsigned long size;\n\tvoid *r;\n\n\t \n\tif (adev->ip_versions[GC_HWIP][0] < IP_VERSION(9, 0, 1))\n\t\treturn -EINVAL;\n\n\tif (adev->gmc.is_app_apu)\n\t\treturn 0;\n\n\tpgmap = &kfddev->pgmap;\n\tmemset(pgmap, 0, sizeof(*pgmap));\n\n\t \n\tsize = ALIGN(adev->gmc.real_vram_size, 2ULL << 20);\n\tif (adev->gmc.xgmi.connected_to_cpu) {\n\t\tpgmap->range.start = adev->gmc.aper_base;\n\t\tpgmap->range.end = adev->gmc.aper_base + adev->gmc.aper_size - 1;\n\t\tpgmap->type = MEMORY_DEVICE_COHERENT;\n\t} else {\n\t\tres = devm_request_free_mem_region(adev->dev, &iomem_resource, size);\n\t\tif (IS_ERR(res))\n\t\t\treturn PTR_ERR(res);\n\t\tpgmap->range.start = res->start;\n\t\tpgmap->range.end = res->end;\n\t\tpgmap->type = MEMORY_DEVICE_PRIVATE;\n\t}\n\n\tpgmap->nr_range = 1;\n\tpgmap->ops = &svm_migrate_pgmap_ops;\n\tpgmap->owner = SVM_ADEV_PGMAP_OWNER(adev);\n\tpgmap->flags = 0;\n\t \n\tr = devm_memremap_pages(adev->dev, pgmap);\n\tif (IS_ERR(r)) {\n\t\tpr_err(\"failed to register HMM device memory\\n\");\n\t\tif (pgmap->type == MEMORY_DEVICE_PRIVATE)\n\t\t\tdevm_release_mem_region(adev->dev, res->start, resource_size(res));\n\t\t \n\t\tpgmap->type = 0;\n\t\treturn PTR_ERR(r);\n\t}\n\n\tpr_debug(\"reserve %ldMB system memory for VRAM pages struct\\n\",\n\t\t SVM_HMM_PAGE_STRUCT_SIZE(size) >> 20);\n\n\tamdgpu_amdkfd_reserve_system_mem(SVM_HMM_PAGE_STRUCT_SIZE(size));\n\n\tpr_info(\"HMM registered %ldMB device memory\\n\", size >> 20);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}