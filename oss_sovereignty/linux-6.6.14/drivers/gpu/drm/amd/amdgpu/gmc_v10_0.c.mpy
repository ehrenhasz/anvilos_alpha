{
  "module_name": "gmc_v10_0.c",
  "hash_id": "d6fde6d34f8747fee28a7399c5325876da80ac00f791920dc394b27d0797e429",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/gmc_v10_0.c",
  "human_readable_source": " \n#include <linux/firmware.h>\n#include <linux/pci.h>\n\n#include <drm/drm_cache.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"gmc_v10_0.h\"\n#include \"umc_v8_7.h\"\n\n#include \"athub/athub_2_0_0_sh_mask.h\"\n#include \"athub/athub_2_0_0_offset.h\"\n#include \"dcn/dcn_2_0_0_offset.h\"\n#include \"dcn/dcn_2_0_0_sh_mask.h\"\n#include \"oss/osssys_5_0_0_offset.h\"\n#include \"ivsrcid/vmc/irqsrcs_vmc_1_0.h\"\n#include \"navi10_enum.h\"\n\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"soc15_common.h\"\n\n#include \"nbio_v2_3.h\"\n\n#include \"gfxhub_v2_0.h\"\n#include \"gfxhub_v2_1.h\"\n#include \"mmhub_v2_0.h\"\n#include \"mmhub_v2_3.h\"\n#include \"athub_v2_0.h\"\n#include \"athub_v2_1.h\"\n\n#include \"amdgpu_reset.h\"\n\nstatic int gmc_v10_0_ecc_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t struct amdgpu_irq_src *src,\n\t\t\t\t\t unsigned int type,\n\t\t\t\t\t enum amdgpu_interrupt_state state)\n{\n\treturn 0;\n}\n\nstatic int\ngmc_v10_0_vm_fault_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_irq_src *src, unsigned int type,\n\t\t\t\t   enum amdgpu_interrupt_state state)\n{\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t \n\t\tamdgpu_gmc_set_vm_fault_masks(adev, AMDGPU_MMHUB0(0), false);\n\t\t \n\t\t \n\t\tif (!adev->in_s0ix)\n\t\t\tamdgpu_gmc_set_vm_fault_masks(adev, AMDGPU_GFXHUB(0), false);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t \n\t\tamdgpu_gmc_set_vm_fault_masks(adev, AMDGPU_MMHUB0(0), true);\n\t\t \n\t\t \n\t\tif (!adev->in_s0ix)\n\t\t\tamdgpu_gmc_set_vm_fault_masks(adev, AMDGPU_GFXHUB(0), true);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_irq_src *source,\n\t\t\t\t       struct amdgpu_iv_entry *entry)\n{\n\tuint32_t vmhub_index = entry->client_id == SOC15_IH_CLIENTID_VMC ?\n\t\t\t       AMDGPU_MMHUB0(0) : AMDGPU_GFXHUB(0);\n\tstruct amdgpu_vmhub *hub = &adev->vmhub[vmhub_index];\n\tbool retry_fault = !!(entry->src_data[1] & 0x80);\n\tbool write_fault = !!(entry->src_data[1] & 0x20);\n\tstruct amdgpu_task_info task_info;\n\tuint32_t status = 0;\n\tu64 addr;\n\n\taddr = (u64)entry->src_data[0] << 12;\n\taddr |= ((u64)entry->src_data[1] & 0xf) << 44;\n\n\tif (retry_fault) {\n\t\t \n\n\t\t \n\t\tif (entry->ih != &adev->irq.ih_soft &&\n\t\t    amdgpu_gmc_filter_faults(adev, entry->ih, addr, entry->pasid,\n\t\t\t\t\t     entry->timestamp))\n\t\t\treturn 1;\n\n\t\t \n\t\tif (entry->ih == &adev->irq.ih) {\n\t\t\tamdgpu_irq_delegate(adev, entry, 8);\n\t\t\treturn 1;\n\t\t}\n\n\t\t \n\t\tif (amdgpu_vm_handle_fault(adev, entry->pasid, 0, 0, addr, write_fault))\n\t\t\treturn 1;\n\t}\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tif ((entry->vmid_src == AMDGPU_GFXHUB(0)) &&\n\t\t    (adev->ip_versions[GC_HWIP][0] < IP_VERSION(10, 3, 0)))\n\t\t\tRREG32(hub->vm_l2_pro_fault_status);\n\n\t\tstatus = RREG32(hub->vm_l2_pro_fault_status);\n\t\tWREG32_P(hub->vm_l2_pro_fault_cntl, 1, ~1);\n\t}\n\n\tif (!printk_ratelimit())\n\t\treturn 0;\n\n\tmemset(&task_info, 0, sizeof(struct amdgpu_task_info));\n\tamdgpu_vm_get_task_info(adev, entry->pasid, &task_info);\n\n\tdev_err(adev->dev,\n\t\t\"[%s] page fault (src_id:%u ring:%u vmid:%u pasid:%u, for process %s pid %d thread %s pid %d)\\n\",\n\t\tentry->vmid_src ? \"mmhub\" : \"gfxhub\",\n\t\tentry->src_id, entry->ring_id, entry->vmid,\n\t\tentry->pasid, task_info.process_name, task_info.tgid,\n\t\ttask_info.task_name, task_info.pid);\n\tdev_err(adev->dev, \"  in page starting at address 0x%016llx from client 0x%x (%s)\\n\",\n\t\taddr, entry->client_id,\n\t\tsoc15_ih_clientid_name[entry->client_id]);\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\thub->vmhub_funcs->print_l2_protection_fault_status(adev,\n\t\t\t\t\t\t\t\t   status);\n\n\treturn 0;\n}\n\nstatic const struct amdgpu_irq_src_funcs gmc_v10_0_irq_funcs = {\n\t.set = gmc_v10_0_vm_fault_interrupt_state,\n\t.process = gmc_v10_0_process_interrupt,\n};\n\nstatic const struct amdgpu_irq_src_funcs gmc_v10_0_ecc_funcs = {\n\t.set = gmc_v10_0_ecc_interrupt_state,\n\t.process = amdgpu_umc_process_ecc_irq,\n};\n\nstatic void gmc_v10_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.vm_fault.num_types = 1;\n\tadev->gmc.vm_fault.funcs = &gmc_v10_0_irq_funcs;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tadev->gmc.ecc_irq.num_types = 1;\n\t\tadev->gmc.ecc_irq.funcs = &gmc_v10_0_ecc_funcs;\n\t}\n}\n\n \nstatic bool gmc_v10_0_use_invalidate_semaphore(struct amdgpu_device *adev,\n\t\t\t\t       uint32_t vmhub)\n{\n\treturn ((vmhub == AMDGPU_MMHUB0(0)) &&\n\t\t(!amdgpu_sriov_vf(adev)));\n}\n\nstatic bool gmc_v10_0_get_atc_vmid_pasid_mapping_info(\n\t\t\t\t\tstruct amdgpu_device *adev,\n\t\t\t\t\tuint8_t vmid, uint16_t *p_pasid)\n{\n\tuint32_t value;\n\n\tvalue = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)\n\t\t     + vmid);\n\t*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;\n\n\treturn !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);\n}\n\n \n\nstatic void gmc_v10_0_flush_vm_hub(struct amdgpu_device *adev, uint32_t vmid,\n\t\t\t\t   unsigned int vmhub, uint32_t flush_type)\n{\n\tbool use_semaphore = gmc_v10_0_use_invalidate_semaphore(adev, vmhub);\n\tstruct amdgpu_vmhub *hub = &adev->vmhub[vmhub];\n\tu32 inv_req = hub->vmhub_funcs->get_invalidate_req(vmid, flush_type);\n\tu32 tmp;\n\t \n\tconst unsigned int eng = 17;\n\tunsigned int i;\n\tunsigned char hub_ip = 0;\n\n\thub_ip = (vmhub == AMDGPU_GFXHUB(0)) ?\n\t\t   GC_HWIP : MMHUB_HWIP;\n\n\tspin_lock(&adev->gmc.invalidate_lock);\n\t \n\n\t \n\tif (use_semaphore) {\n\t\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t\t \n\t\t\ttmp = RREG32_RLC_NO_KIQ(hub->vm_inv_eng0_sem +\n\t\t\t\t\t hub->eng_distance * eng, hub_ip);\n\n\t\t\tif (tmp & 0x1)\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t\tif (i >= adev->usec_timeout)\n\t\t\tDRM_ERROR(\"Timeout waiting for sem acquire in VM flush!\\n\");\n\t}\n\n\tWREG32_RLC_NO_KIQ(hub->vm_inv_eng0_req +\n\t\t\t  hub->eng_distance * eng,\n\t\t\t  inv_req, hub_ip);\n\n\t \n\tif ((vmhub == AMDGPU_GFXHUB(0)) &&\n\t    (adev->ip_versions[GC_HWIP][0] < IP_VERSION(10, 3, 0)))\n\t\tRREG32_RLC_NO_KIQ(hub->vm_inv_eng0_req +\n\t\t\t\t  hub->eng_distance * eng, hub_ip);\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32_RLC_NO_KIQ(hub->vm_inv_eng0_ack +\n\t\t\t\t  hub->eng_distance * eng, hub_ip);\n\n\t\ttmp &= 1 << vmid;\n\t\tif (tmp)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\t \n\tif (use_semaphore)\n\t\t \n\t\tWREG32_RLC_NO_KIQ(hub->vm_inv_eng0_sem +\n\t\t\t\t  hub->eng_distance * eng, 0, hub_ip);\n\n\tspin_unlock(&adev->gmc.invalidate_lock);\n\n\tif (i < adev->usec_timeout)\n\t\treturn;\n\n\tDRM_ERROR(\"Timeout waiting for VM flush hub: %d!\\n\", vmhub);\n}\n\n \nstatic void gmc_v10_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,\n\t\t\t\t\tuint32_t vmhub, uint32_t flush_type)\n{\n\tstruct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;\n\tstruct dma_fence *fence;\n\tstruct amdgpu_job *job;\n\n\tint r;\n\n\t \n\tadev->hdp.funcs->flush_hdp(adev, NULL);\n\n\t \n\tif (adev->gfx.kiq[0].ring.sched.ready && !adev->enable_mes &&\n\t    (amdgpu_sriov_runtime(adev) || !amdgpu_sriov_vf(adev)) &&\n\t    down_read_trylock(&adev->reset_domain->sem)) {\n\t\tstruct amdgpu_vmhub *hub = &adev->vmhub[vmhub];\n\t\tconst unsigned int eng = 17;\n\t\tu32 inv_req = hub->vmhub_funcs->get_invalidate_req(vmid, flush_type);\n\t\tu32 req = hub->vm_inv_eng0_req + hub->eng_distance * eng;\n\t\tu32 ack = hub->vm_inv_eng0_ack + hub->eng_distance * eng;\n\n\t\tamdgpu_virt_kiq_reg_write_reg_wait(adev, req, ack, inv_req,\n\t\t\t\t1 << vmid);\n\n\t\tup_read(&adev->reset_domain->sem);\n\t\treturn;\n\t}\n\n\tmutex_lock(&adev->mman.gtt_window_lock);\n\n\tif (vmhub == AMDGPU_MMHUB0(0)) {\n\t\tgmc_v10_0_flush_vm_hub(adev, vmid, AMDGPU_MMHUB0(0), 0);\n\t\tmutex_unlock(&adev->mman.gtt_window_lock);\n\t\treturn;\n\t}\n\n\tBUG_ON(vmhub != AMDGPU_GFXHUB(0));\n\n\tif (!adev->mman.buffer_funcs_enabled ||\n\t    !adev->ib_pool_ready ||\n\t    amdgpu_in_reset(adev) ||\n\t    ring->sched.ready == false) {\n\t\tgmc_v10_0_flush_vm_hub(adev, vmid, AMDGPU_GFXHUB(0), 0);\n\t\tmutex_unlock(&adev->mman.gtt_window_lock);\n\t\treturn;\n\t}\n\n\t \n\tr = amdgpu_job_alloc_with_ib(ring->adev, &adev->mman.high_pr,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     16 * 4, AMDGPU_IB_POOL_IMMEDIATE,\n\t\t\t\t     &job);\n\tif (r)\n\t\tgoto error_alloc;\n\n\tjob->vm_pd_addr = amdgpu_gmc_pd_addr(adev->gart.bo);\n\tjob->vm_needs_flush = true;\n\tjob->ibs->ptr[job->ibs->length_dw++] = ring->funcs->nop;\n\tamdgpu_ring_pad_ib(ring, &job->ibs[0]);\n\tfence = amdgpu_job_submit(job);\n\n\tmutex_unlock(&adev->mman.gtt_window_lock);\n\n\tdma_fence_wait(fence, false);\n\tdma_fence_put(fence);\n\n\treturn;\n\nerror_alloc:\n\tmutex_unlock(&adev->mman.gtt_window_lock);\n\tDRM_ERROR(\"Error flushing GPU TLB using the SDMA (%d)!\\n\", r);\n}\n\n \nstatic int gmc_v10_0_flush_gpu_tlb_pasid(struct amdgpu_device *adev,\n\t\t\t\t\tuint16_t pasid, uint32_t flush_type,\n\t\t\t\t\tbool all_hub, uint32_t inst)\n{\n\tint vmid, i;\n\tsigned long r;\n\tuint32_t seq;\n\tuint16_t queried_pasid;\n\tbool ret;\n\tu32 usec_timeout = amdgpu_sriov_vf(adev) ? SRIOV_USEC_TIMEOUT : adev->usec_timeout;\n\tstruct amdgpu_ring *ring = &adev->gfx.kiq[0].ring;\n\tstruct amdgpu_kiq *kiq = &adev->gfx.kiq[0];\n\n\tif (amdgpu_emu_mode == 0 && ring->sched.ready) {\n\t\tspin_lock(&adev->gfx.kiq[0].ring_lock);\n\t\t \n\t\tamdgpu_ring_alloc(ring, kiq->pmf->invalidate_tlbs_size + 8);\n\t\tkiq->pmf->kiq_invalidate_tlbs(ring,\n\t\t\t\t\tpasid, flush_type, all_hub);\n\t\tr = amdgpu_fence_emit_polling(ring, &seq, MAX_KIQ_REG_WAIT);\n\t\tif (r) {\n\t\t\tamdgpu_ring_undo(ring);\n\t\t\tspin_unlock(&adev->gfx.kiq[0].ring_lock);\n\t\t\treturn -ETIME;\n\t\t}\n\n\t\tamdgpu_ring_commit(ring);\n\t\tspin_unlock(&adev->gfx.kiq[0].ring_lock);\n\t\tr = amdgpu_fence_wait_polling(ring, seq, usec_timeout);\n\t\tif (r < 1) {\n\t\t\tdev_err(adev->dev, \"wait for kiq fence error: %ld.\\n\", r);\n\t\t\treturn -ETIME;\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\tfor (vmid = 1; vmid < AMDGPU_NUM_VMID; vmid++) {\n\n\t\tret = gmc_v10_0_get_atc_vmid_pasid_mapping_info(adev, vmid,\n\t\t\t\t&queried_pasid);\n\t\tif (ret\t&& queried_pasid == pasid) {\n\t\t\tif (all_hub) {\n\t\t\t\tfor_each_set_bit(i, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS)\n\t\t\t\t\tgmc_v10_0_flush_gpu_tlb(adev, vmid,\n\t\t\t\t\t\t\ti, flush_type);\n\t\t\t} else {\n\t\t\t\tgmc_v10_0_flush_gpu_tlb(adev, vmid,\n\t\t\t\t\t\tAMDGPU_GFXHUB(0), flush_type);\n\t\t\t}\n\t\t\tif (!adev->enable_mes)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic uint64_t gmc_v10_0_emit_flush_gpu_tlb(struct amdgpu_ring *ring,\n\t\t\t\t\t     unsigned int vmid, uint64_t pd_addr)\n{\n\tbool use_semaphore = gmc_v10_0_use_invalidate_semaphore(ring->adev, ring->vm_hub);\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\tuint32_t req = hub->vmhub_funcs->get_invalidate_req(vmid, 0);\n\tunsigned int eng = ring->vm_inv_eng;\n\n\t \n\n\t \n\tif (use_semaphore)\n\t\t \n\t\tamdgpu_ring_emit_reg_wait(ring,\n\t\t\t\t\t  hub->vm_inv_eng0_sem +\n\t\t\t\t\t  hub->eng_distance * eng, 0x1, 0x1);\n\n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      lower_32_bits(pd_addr));\n\n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_hi32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      upper_32_bits(pd_addr));\n\n\tamdgpu_ring_emit_reg_write_reg_wait(ring, hub->vm_inv_eng0_req +\n\t\t\t\t\t    hub->eng_distance * eng,\n\t\t\t\t\t    hub->vm_inv_eng0_ack +\n\t\t\t\t\t    hub->eng_distance * eng,\n\t\t\t\t\t    req, 1 << vmid);\n\n\t \n\tif (use_semaphore)\n\t\t \n\t\tamdgpu_ring_emit_wreg(ring, hub->vm_inv_eng0_sem +\n\t\t\t\t      hub->eng_distance * eng, 0);\n\n\treturn pd_addr;\n}\n\nstatic void gmc_v10_0_emit_pasid_mapping(struct amdgpu_ring *ring, unsigned int vmid,\n\t\t\t\t\t unsigned int pasid)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t reg;\n\n\t \n\tif (ring->is_mes_queue)\n\t\treturn;\n\n\tif (ring->vm_hub == AMDGPU_GFXHUB(0))\n\t\treg = SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid;\n\telse\n\t\treg = SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid;\n\n\tamdgpu_ring_emit_wreg(ring, reg, pasid);\n}\n\n \n\nstatic uint64_t gmc_v10_0_map_mtype(struct amdgpu_device *adev, uint32_t flags)\n{\n\tswitch (flags) {\n\tcase AMDGPU_VM_MTYPE_DEFAULT:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_NC);\n\tcase AMDGPU_VM_MTYPE_NC:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_NC);\n\tcase AMDGPU_VM_MTYPE_WC:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_WC);\n\tcase AMDGPU_VM_MTYPE_CC:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_CC);\n\tcase AMDGPU_VM_MTYPE_UC:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_UC);\n\tdefault:\n\t\treturn AMDGPU_PTE_MTYPE_NV10(MTYPE_NC);\n\t}\n}\n\nstatic void gmc_v10_0_get_vm_pde(struct amdgpu_device *adev, int level,\n\t\t\t\t uint64_t *addr, uint64_t *flags)\n{\n\tif (!(*flags & AMDGPU_PDE_PTE) && !(*flags & AMDGPU_PTE_SYSTEM))\n\t\t*addr = amdgpu_gmc_vram_mc2pa(adev, *addr);\n\tBUG_ON(*addr & 0xFFFF00000000003FULL);\n\n\tif (!adev->gmc.translate_further)\n\t\treturn;\n\n\tif (level == AMDGPU_VM_PDB1) {\n\t\t \n\t\tif (!(*flags & AMDGPU_PDE_PTE))\n\t\t\t*flags |= AMDGPU_PDE_BFS(0x9);\n\n\t} else if (level == AMDGPU_VM_PDB0) {\n\t\tif (*flags & AMDGPU_PDE_PTE)\n\t\t\t*flags &= ~AMDGPU_PDE_PTE;\n\t\telse\n\t\t\t*flags |= AMDGPU_PTE_TF;\n\t}\n}\n\nstatic void gmc_v10_0_get_vm_pte(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_bo_va_mapping *mapping,\n\t\t\t\t uint64_t *flags)\n{\n\tstruct amdgpu_bo *bo = mapping->bo_va->base.bo;\n\n\t*flags &= ~AMDGPU_PTE_EXECUTABLE;\n\t*flags |= mapping->flags & AMDGPU_PTE_EXECUTABLE;\n\n\t*flags &= ~AMDGPU_PTE_MTYPE_NV10_MASK;\n\t*flags |= (mapping->flags & AMDGPU_PTE_MTYPE_NV10_MASK);\n\n\t*flags &= ~AMDGPU_PTE_NOALLOC;\n\t*flags |= (mapping->flags & AMDGPU_PTE_NOALLOC);\n\n\tif (mapping->flags & AMDGPU_PTE_PRT) {\n\t\t*flags |= AMDGPU_PTE_PRT;\n\t\t*flags |= AMDGPU_PTE_SNOOPED;\n\t\t*flags |= AMDGPU_PTE_LOG;\n\t\t*flags |= AMDGPU_PTE_SYSTEM;\n\t\t*flags &= ~AMDGPU_PTE_VALID;\n\t}\n\n\tif (bo && bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t       AMDGPU_GEM_CREATE_UNCACHED))\n\t\t*flags = (*flags & ~AMDGPU_PTE_MTYPE_NV10_MASK) |\n\t\t\t AMDGPU_PTE_MTYPE_NV10(MTYPE_UC);\n}\n\nstatic unsigned int gmc_v10_0_get_vbios_fb_size(struct amdgpu_device *adev)\n{\n\tu32 d1vga_control = RREG32_SOC15(DCE, 0, mmD1VGA_CONTROL);\n\tunsigned int size;\n\n\tif (REG_GET_FIELD(d1vga_control, D1VGA_CONTROL, D1VGA_MODE_ENABLE)) {\n\t\tsize = AMDGPU_VBIOS_VGA_ALLOCATION;\n\t} else {\n\t\tu32 viewport;\n\t\tu32 pitch;\n\n\t\tviewport = RREG32_SOC15(DCE, 0, mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION);\n\t\tpitch = RREG32_SOC15(DCE, 0, mmHUBPREQ0_DCSURF_SURFACE_PITCH);\n\t\tsize = (REG_GET_FIELD(viewport,\n\t\t\t\t\tHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION, PRI_VIEWPORT_HEIGHT) *\n\t\t\t\tREG_GET_FIELD(pitch, HUBPREQ0_DCSURF_SURFACE_PITCH, PITCH) *\n\t\t\t\t4);\n\t}\n\n\treturn size;\n}\n\nstatic const struct amdgpu_gmc_funcs gmc_v10_0_gmc_funcs = {\n\t.flush_gpu_tlb = gmc_v10_0_flush_gpu_tlb,\n\t.flush_gpu_tlb_pasid = gmc_v10_0_flush_gpu_tlb_pasid,\n\t.emit_flush_gpu_tlb = gmc_v10_0_emit_flush_gpu_tlb,\n\t.emit_pasid_mapping = gmc_v10_0_emit_pasid_mapping,\n\t.map_mtype = gmc_v10_0_map_mtype,\n\t.get_vm_pde = gmc_v10_0_get_vm_pde,\n\t.get_vm_pte = gmc_v10_0_get_vm_pte,\n\t.get_vbios_fb_size = gmc_v10_0_get_vbios_fb_size,\n};\n\nstatic void gmc_v10_0_set_gmc_funcs(struct amdgpu_device *adev)\n{\n\tif (adev->gmc.gmc_funcs == NULL)\n\t\tadev->gmc.gmc_funcs = &gmc_v10_0_gmc_funcs;\n}\n\nstatic void gmc_v10_0_set_umc_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[UMC_HWIP][0]) {\n\tcase IP_VERSION(8, 7, 0):\n\t\tadev->umc.max_ras_err_cnt_per_query = UMC_V8_7_TOTAL_CHANNEL_NUM;\n\t\tadev->umc.channel_inst_num = UMC_V8_7_CHANNEL_INSTANCE_NUM;\n\t\tadev->umc.umc_inst_num = UMC_V8_7_UMC_INSTANCE_NUM;\n\t\tadev->umc.channel_offs = UMC_V8_7_PER_CHANNEL_OFFSET_SIENNA;\n\t\tadev->umc.retire_unit = 1;\n\t\tadev->umc.channel_idx_tbl = &umc_v8_7_channel_idx_tbl[0][0];\n\t\tadev->umc.ras = &umc_v8_7_ras;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v10_0_set_mmhub_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[MMHUB_HWIP][0]) {\n\tcase IP_VERSION(2, 3, 0):\n\tcase IP_VERSION(2, 4, 0):\n\tcase IP_VERSION(2, 4, 1):\n\t\tadev->mmhub.funcs = &mmhub_v2_3_funcs;\n\t\tbreak;\n\tdefault:\n\t\tadev->mmhub.funcs = &mmhub_v2_0_funcs;\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v10_0_set_gfxhub_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(10, 3, 0):\n\tcase IP_VERSION(10, 3, 2):\n\tcase IP_VERSION(10, 3, 1):\n\tcase IP_VERSION(10, 3, 4):\n\tcase IP_VERSION(10, 3, 5):\n\tcase IP_VERSION(10, 3, 6):\n\tcase IP_VERSION(10, 3, 3):\n\tcase IP_VERSION(10, 3, 7):\n\t\tadev->gfxhub.funcs = &gfxhub_v2_1_funcs;\n\t\tbreak;\n\tdefault:\n\t\tadev->gfxhub.funcs = &gfxhub_v2_0_funcs;\n\t\tbreak;\n\t}\n}\n\n\nstatic int gmc_v10_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v10_0_set_mmhub_funcs(adev);\n\tgmc_v10_0_set_gfxhub_funcs(adev);\n\tgmc_v10_0_set_gmc_funcs(adev);\n\tgmc_v10_0_set_irq_funcs(adev);\n\tgmc_v10_0_set_umc_funcs(adev);\n\n\tadev->gmc.shared_aperture_start = 0x2000000000000000ULL;\n\tadev->gmc.shared_aperture_end =\n\t\tadev->gmc.shared_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.private_aperture_start = 0x1000000000000000ULL;\n\tadev->gmc.private_aperture_end =\n\t\tadev->gmc.private_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.noretry_flags = AMDGPU_VM_NORETRY_FLAGS_TF;\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_gmc_allocate_vm_inv_eng(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_gmc_ras_late_init(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_irq_get(adev, &adev->gmc.vm_fault, 0);\n}\n\nstatic void gmc_v10_0_vram_gtt_location(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_gmc *mc)\n{\n\tu64 base = 0;\n\n\tbase = adev->gfxhub.funcs->get_fb_location(adev);\n\n\t \n\tbase += adev->gmc.xgmi.physical_node_id * adev->gmc.xgmi.node_segment_size;\n\n\tamdgpu_gmc_vram_location(adev, &adev->gmc, base);\n\tamdgpu_gmc_gart_location(adev, mc);\n\tamdgpu_gmc_agp_location(adev, mc);\n\n\t \n\tadev->vm_manager.vram_base_offset = adev->gfxhub.funcs->get_mc_fb_offset(adev);\n\n\t \n\tadev->vm_manager.vram_base_offset +=\n\t\tadev->gmc.xgmi.physical_node_id * adev->gmc.xgmi.node_segment_size;\n}\n\n \nstatic int gmc_v10_0_mc_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t \n\tadev->gmc.mc_vram_size =\n\t\tadev->nbio.funcs->get_memsize(adev) * 1024ULL * 1024ULL;\n\tadev->gmc.real_vram_size = adev->gmc.mc_vram_size;\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tr = amdgpu_device_resize_fb_bar(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\tadev->gmc.aper_base = pci_resource_start(adev->pdev, 0);\n\tadev->gmc.aper_size = pci_resource_len(adev->pdev, 0);\n\n#ifdef CONFIG_X86_64\n\tif ((adev->flags & AMD_IS_APU) && !amdgpu_passthrough(adev)) {\n\t\tadev->gmc.aper_base = adev->gfxhub.funcs->get_mc_fb_offset(adev);\n\t\tadev->gmc.aper_size = adev->gmc.real_vram_size;\n\t}\n#endif\n\n\tadev->gmc.visible_vram_size = adev->gmc.aper_size;\n\n\t \n\tif (amdgpu_gart_size == -1) {\n\t\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\t\tdefault:\n\t\t\tadev->gmc.gart_size = 512ULL << 20;\n\t\t\tbreak;\n\t\tcase IP_VERSION(10, 3, 1):    \n\t\tcase IP_VERSION(10, 3, 3):    \n\t\tcase IP_VERSION(10, 3, 6):    \n\t\tcase IP_VERSION(10, 3, 7):    \n\t\t\tadev->gmc.gart_size = 1024ULL << 20;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tadev->gmc.gart_size = (u64)amdgpu_gart_size << 20;\n\t}\n\n\tgmc_v10_0_vram_gtt_location(adev, &adev->gmc);\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_gart_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->gart.bo) {\n\t\tWARN(1, \"NAVI10 PCIE GART already initialized\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tr = amdgpu_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tadev->gart.table_size = adev->gart.num_gpu_pages * 8;\n\tadev->gart.gart_pte_flags = AMDGPU_PTE_MTYPE_NV10(MTYPE_UC) |\n\t\t\t\t AMDGPU_PTE_EXECUTABLE;\n\n\treturn amdgpu_gart_table_vram_alloc(adev);\n}\n\nstatic int gmc_v10_0_sw_init(void *handle)\n{\n\tint r, vram_width = 0, vram_type = 0, vram_vendor = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->gfxhub.funcs->init(adev);\n\n\tadev->mmhub.funcs->init(adev);\n\n\tspin_lock_init(&adev->gmc.invalidate_lock);\n\n\tif ((adev->flags & AMD_IS_APU) && amdgpu_emu_mode == 1) {\n\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_DDR4;\n\t\tadev->gmc.vram_width = 64;\n\t} else if (amdgpu_emu_mode == 1) {\n\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_GDDR6;\n\t\tadev->gmc.vram_width = 1 * 128;  \n\t} else {\n\t\tr = amdgpu_atomfirmware_get_vram_info(adev,\n\t\t\t\t&vram_width, &vram_type, &vram_vendor);\n\t\tadev->gmc.vram_width = vram_width;\n\n\t\tadev->gmc.vram_type = vram_type;\n\t\tadev->gmc.vram_vendor = vram_vendor;\n\t}\n\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(10, 3, 0):\n\t\tadev->gmc.mall_size = 128 * 1024 * 1024;\n\t\tbreak;\n\tcase IP_VERSION(10, 3, 2):\n\t\tadev->gmc.mall_size = 96 * 1024 * 1024;\n\t\tbreak;\n\tcase IP_VERSION(10, 3, 4):\n\t\tadev->gmc.mall_size = 32 * 1024 * 1024;\n\t\tbreak;\n\tcase IP_VERSION(10, 3, 5):\n\t\tadev->gmc.mall_size = 16 * 1024 * 1024;\n\t\tbreak;\n\tdefault:\n\t\tadev->gmc.mall_size = 0;\n\t\tbreak;\n\t}\n\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(10, 1, 10):\n\tcase IP_VERSION(10, 1, 1):\n\tcase IP_VERSION(10, 1, 2):\n\tcase IP_VERSION(10, 1, 3):\n\tcase IP_VERSION(10, 1, 4):\n\tcase IP_VERSION(10, 3, 0):\n\tcase IP_VERSION(10, 3, 2):\n\tcase IP_VERSION(10, 3, 1):\n\tcase IP_VERSION(10, 3, 4):\n\tcase IP_VERSION(10, 3, 5):\n\tcase IP_VERSION(10, 3, 6):\n\tcase IP_VERSION(10, 3, 3):\n\tcase IP_VERSION(10, 3, 7):\n\t\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\t\tset_bit(AMDGPU_MMHUB0(0), adev->vmhubs_mask);\n\t\t \n\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 48);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VMC,\n\t\t\t      VMC_1_0__SRCID__VM_FAULT,\n\t\t\t      &adev->gmc.vm_fault);\n\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_UTCL2,\n\t\t\t      UTCL2_1_0__SRCID__FAULT,\n\t\t\t      &adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_DF, 0,\n\t\t\t\t      &adev->gmc.ecc_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tadev->gmc.mc_mask = 0xffffffffffffULL;  \n\n\tr = dma_set_mask_and_coherent(adev->dev, DMA_BIT_MASK(44));\n\tif (r) {\n\t\tdev_warn(adev->dev, \"amdgpu: No suitable DMA available.\\n\");\n\t\treturn r;\n\t}\n\n\tadev->need_swiotlb = drm_need_swiotlb(44);\n\n\tr = gmc_v10_0_mc_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_gmc_get_vbios_allocations(adev);\n\n\t \n\tr = amdgpu_bo_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = gmc_v10_0_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tadev->vm_manager.first_kfd_vmid = 8;\n\n\tamdgpu_vm_manager_init(adev);\n\n\tr = amdgpu_gmc_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n \nstatic void gmc_v10_0_gart_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_gart_table_vram_free(adev);\n}\n\nstatic int gmc_v10_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_vm_manager_fini(adev);\n\tgmc_v10_0_gart_fini(adev);\n\tamdgpu_gem_force_release(adev);\n\tamdgpu_bo_fini(adev);\n\n\treturn 0;\n}\n\nstatic void gmc_v10_0_init_golden_registers(struct amdgpu_device *adev)\n{\n}\n\n \nstatic int gmc_v10_0_gart_enable(struct amdgpu_device *adev)\n{\n\tint r;\n\tbool value;\n\n\tif (adev->gart.bo == NULL) {\n\t\tdev_err(adev->dev, \"No VRAM object for PCIE GART.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tamdgpu_gtt_mgr_recover(&adev->mman.gtt_mgr);\n\n\tif (!adev->in_s0ix) {\n\t\tr = adev->gfxhub.funcs->gart_enable(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = adev->mmhub.funcs->gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tadev->hdp.funcs->init_registers(adev);\n\n\t \n\tadev->hdp.funcs->flush_hdp(adev, NULL);\n\n\tvalue = (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_ALWAYS) ?\n\t\tfalse : true;\n\n\tif (!adev->in_s0ix)\n\t\tadev->gfxhub.funcs->set_fault_enable_default(adev, value);\n\tadev->mmhub.funcs->set_fault_enable_default(adev, value);\n\tgmc_v10_0_flush_gpu_tlb(adev, 0, AMDGPU_MMHUB0(0), 0);\n\tif (!adev->in_s0ix)\n\t\tgmc_v10_0_flush_gpu_tlb(adev, 0, AMDGPU_GFXHUB(0), 0);\n\n\tDRM_INFO(\"PCIE GART of %uM enabled (table at 0x%016llX).\\n\",\n\t\t (unsigned int)(adev->gmc.gart_size >> 20),\n\t\t (unsigned long long)amdgpu_bo_gpu_offset(adev->gart.bo));\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tgmc_v10_0_init_golden_registers(adev);\n\n\t \n\tif (!adev->in_s0ix && adev->gfxhub.funcs && adev->gfxhub.funcs->utcl2_harvest)\n\t\tadev->gfxhub.funcs->utcl2_harvest(adev);\n\n\tr = gmc_v10_0_gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_emu_mode == 1) {\n\t\tr = amdgpu_gmc_vram_checking(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (adev->umc.funcs && adev->umc.funcs->init_registers)\n\t\tadev->umc.funcs->init_registers(adev);\n\n\treturn 0;\n}\n\n \nstatic void gmc_v10_0_gart_disable(struct amdgpu_device *adev)\n{\n\tif (!adev->in_s0ix)\n\t\tadev->gfxhub.funcs->gart_disable(adev);\n\tadev->mmhub.funcs->gart_disable(adev);\n}\n\nstatic int gmc_v10_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v10_0_gart_disable(adev);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tDRM_DEBUG(\"For SRIOV client, shouldn't do anything.\\n\");\n\t\treturn 0;\n\t}\n\n\tamdgpu_irq_put(adev, &adev->gmc.vm_fault, 0);\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v10_0_hw_fini(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v10_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = gmc_v10_0_hw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vmid_reset_all(adev);\n\n\treturn 0;\n}\n\nstatic bool gmc_v10_0_is_idle(void *handle)\n{\n\t \n\treturn true;\n}\n\nstatic int gmc_v10_0_wait_for_idle(void *handle)\n{\n\t \n\treturn 0;\n}\n\nstatic int gmc_v10_0_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic int gmc_v10_0_set_clockgating_state(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tif (adev->in_s0ix && adev->ip_versions[DF_HWIP][0] > IP_VERSION(3, 0, 2)) {\n\t\tdev_dbg(adev->dev, \"keep mmhub clock gating being enabled for s0ix\\n\");\n\t\treturn 0;\n\t}\n\n\tr = adev->mmhub.funcs->set_clockgating(adev, state);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->ip_versions[ATHUB_HWIP][0] >= IP_VERSION(2, 1, 0))\n\t\treturn athub_v2_1_set_clockgating(adev, state);\n\telse\n\t\treturn athub_v2_0_set_clockgating(adev, state);\n}\n\nstatic void gmc_v10_0_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(10, 1, 3) ||\n\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(10, 1, 4))\n\t\treturn;\n\n\tadev->mmhub.funcs->get_clockgating(adev, flags);\n\n\tif (adev->ip_versions[ATHUB_HWIP][0] >= IP_VERSION(2, 1, 0))\n\t\tathub_v2_1_get_clockgating(adev, flags);\n\telse\n\t\tathub_v2_0_get_clockgating(adev, flags);\n}\n\nstatic int gmc_v10_0_set_powergating_state(void *handle,\n\t\t\t\t\t   enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nconst struct amd_ip_funcs gmc_v10_0_ip_funcs = {\n\t.name = \"gmc_v10_0\",\n\t.early_init = gmc_v10_0_early_init,\n\t.late_init = gmc_v10_0_late_init,\n\t.sw_init = gmc_v10_0_sw_init,\n\t.sw_fini = gmc_v10_0_sw_fini,\n\t.hw_init = gmc_v10_0_hw_init,\n\t.hw_fini = gmc_v10_0_hw_fini,\n\t.suspend = gmc_v10_0_suspend,\n\t.resume = gmc_v10_0_resume,\n\t.is_idle = gmc_v10_0_is_idle,\n\t.wait_for_idle = gmc_v10_0_wait_for_idle,\n\t.soft_reset = gmc_v10_0_soft_reset,\n\t.set_clockgating_state = gmc_v10_0_set_clockgating_state,\n\t.set_powergating_state = gmc_v10_0_set_powergating_state,\n\t.get_clockgating_state = gmc_v10_0_get_clockgating_state,\n};\n\nconst struct amdgpu_ip_block_version gmc_v10_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GMC,\n\t.major = 10,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &gmc_v10_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}