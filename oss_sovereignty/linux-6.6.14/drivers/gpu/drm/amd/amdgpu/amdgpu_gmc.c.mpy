{
  "module_name": "amdgpu_gmc.c",
  "hash_id": "d661ce97422b8e9836ae1957e3d79096941a49f2532349c61573227b3f28bb26",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_gmc.c",
  "human_readable_source": " \n\n#include <linux/io-64-nonatomic-lo-hi.h>\n#ifdef CONFIG_X86\n#include <asm/hypervisor.h>\n#endif\n\n#include \"amdgpu.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_xgmi.h\"\n\n#include <drm/drm_drv.h>\n#include <drm/ttm/ttm_tt.h>\n\n \nint amdgpu_gmc_pdb0_alloc(struct amdgpu_device *adev)\n{\n\tint r;\n\tstruct amdgpu_bo_param bp;\n\tu64 vram_size = adev->gmc.xgmi.node_segment_size * adev->gmc.xgmi.num_physical_nodes;\n\tuint32_t pde0_page_shift = adev->gmc.vmid0_page_table_block_size + 21;\n\tuint32_t npdes = (vram_size + (1ULL << pde0_page_shift) -1) >> pde0_page_shift;\n\n\tmemset(&bp, 0, sizeof(bp));\n\tbp.size = PAGE_ALIGN((npdes + 1) * 8);\n\tbp.byte_align = PAGE_SIZE;\n\tbp.domain = AMDGPU_GEM_DOMAIN_VRAM;\n\tbp.flags = AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED |\n\t\tAMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tbp.type = ttm_bo_type_kernel;\n\tbp.resv = NULL;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\n\tr = amdgpu_bo_create(adev, &bp, &adev->gmc.pdb0_bo);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_bo_reserve(adev->gmc.pdb0_bo, false);\n\tif (unlikely(r != 0))\n\t\tgoto bo_reserve_failure;\n\n\tr = amdgpu_bo_pin(adev->gmc.pdb0_bo, AMDGPU_GEM_DOMAIN_VRAM);\n\tif (r)\n\t\tgoto bo_pin_failure;\n\tr = amdgpu_bo_kmap(adev->gmc.pdb0_bo, &adev->gmc.ptr_pdb0);\n\tif (r)\n\t\tgoto bo_kmap_failure;\n\n\tamdgpu_bo_unreserve(adev->gmc.pdb0_bo);\n\treturn 0;\n\nbo_kmap_failure:\n\tamdgpu_bo_unpin(adev->gmc.pdb0_bo);\nbo_pin_failure:\n\tamdgpu_bo_unreserve(adev->gmc.pdb0_bo);\nbo_reserve_failure:\n\tamdgpu_bo_unref(&adev->gmc.pdb0_bo);\n\treturn r;\n}\n\n \nvoid amdgpu_gmc_get_pde_for_bo(struct amdgpu_bo *bo, int level,\n\t\t\t       uint64_t *addr, uint64_t *flags)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\n\tswitch (bo->tbo.resource->mem_type) {\n\tcase TTM_PL_TT:\n\t\t*addr = bo->tbo.ttm->dma_address[0];\n\t\tbreak;\n\tcase TTM_PL_VRAM:\n\t\t*addr = amdgpu_bo_gpu_offset(bo);\n\t\tbreak;\n\tdefault:\n\t\t*addr = 0;\n\t\tbreak;\n\t}\n\t*flags = amdgpu_ttm_tt_pde_flags(bo->tbo.ttm, bo->tbo.resource);\n\tamdgpu_gmc_get_vm_pde(adev, level, addr, flags);\n}\n\n \nuint64_t amdgpu_gmc_pd_addr(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tuint64_t pd_addr;\n\n\t \n\tif (adev->asic_type >= CHIP_VEGA10) {\n\t\tuint64_t flags = AMDGPU_PTE_VALID;\n\n\t\tamdgpu_gmc_get_pde_for_bo(bo, -1, &pd_addr, &flags);\n\t\tpd_addr |= flags;\n\t} else {\n\t\tpd_addr = amdgpu_bo_gpu_offset(bo);\n\t}\n\treturn pd_addr;\n}\n\n \nint amdgpu_gmc_set_pte_pde(struct amdgpu_device *adev, void *cpu_pt_addr,\n\t\t\t\tuint32_t gpu_page_idx, uint64_t addr,\n\t\t\t\tuint64_t flags)\n{\n\tvoid __iomem *ptr = (void *)cpu_pt_addr;\n\tuint64_t value;\n\n\t \n\tvalue = addr & 0x0000FFFFFFFFF000ULL;\n\tvalue |= flags;\n\twriteq(value, ptr + (gpu_page_idx * 8));\n\n\treturn 0;\n}\n\n \nuint64_t amdgpu_gmc_agp_addr(struct ttm_buffer_object *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\n\tif (bo->ttm->num_pages != 1 || bo->ttm->caching == ttm_cached)\n\t\treturn AMDGPU_BO_INVALID_OFFSET;\n\n\tif (bo->ttm->dma_address[0] + PAGE_SIZE >= adev->gmc.agp_size)\n\t\treturn AMDGPU_BO_INVALID_OFFSET;\n\n\treturn adev->gmc.agp_start + bo->ttm->dma_address[0];\n}\n\n \nvoid amdgpu_gmc_vram_location(struct amdgpu_device *adev, struct amdgpu_gmc *mc,\n\t\t\t      u64 base)\n{\n\tuint64_t vis_limit = (uint64_t)amdgpu_vis_vram_limit << 20;\n\tuint64_t limit = (uint64_t)amdgpu_vram_limit << 20;\n\n\tmc->vram_start = base;\n\tmc->vram_end = mc->vram_start + mc->mc_vram_size - 1;\n\tif (limit < mc->real_vram_size)\n\t\tmc->real_vram_size = limit;\n\n\tif (vis_limit && vis_limit < mc->visible_vram_size)\n\t\tmc->visible_vram_size = vis_limit;\n\n\tif (mc->real_vram_size < mc->visible_vram_size)\n\t\tmc->visible_vram_size = mc->real_vram_size;\n\n\tif (mc->xgmi.num_physical_nodes == 0) {\n\t\tmc->fb_start = mc->vram_start;\n\t\tmc->fb_end = mc->vram_end;\n\t}\n\tdev_info(adev->dev, \"VRAM: %lluM 0x%016llX - 0x%016llX (%lluM used)\\n\",\n\t\t\tmc->mc_vram_size >> 20, mc->vram_start,\n\t\t\tmc->vram_end, mc->real_vram_size >> 20);\n}\n\n \nvoid amdgpu_gmc_sysvm_location(struct amdgpu_device *adev, struct amdgpu_gmc *mc)\n{\n\tu64 hive_vram_start = 0;\n\tu64 hive_vram_end = mc->xgmi.node_segment_size * mc->xgmi.num_physical_nodes - 1;\n\tmc->vram_start = mc->xgmi.node_segment_size * mc->xgmi.physical_node_id;\n\tmc->vram_end = mc->vram_start + mc->xgmi.node_segment_size - 1;\n\tmc->gart_start = hive_vram_end + 1;\n\tmc->gart_end = mc->gart_start + mc->gart_size - 1;\n\tmc->fb_start = hive_vram_start;\n\tmc->fb_end = hive_vram_end;\n\tdev_info(adev->dev, \"VRAM: %lluM 0x%016llX - 0x%016llX (%lluM used)\\n\",\n\t\t\tmc->mc_vram_size >> 20, mc->vram_start,\n\t\t\tmc->vram_end, mc->real_vram_size >> 20);\n\tdev_info(adev->dev, \"GART: %lluM 0x%016llX - 0x%016llX\\n\",\n\t\t\tmc->gart_size >> 20, mc->gart_start, mc->gart_end);\n}\n\n \nvoid amdgpu_gmc_gart_location(struct amdgpu_device *adev, struct amdgpu_gmc *mc)\n{\n\tconst uint64_t four_gb = 0x100000000ULL;\n\tu64 size_af, size_bf;\n\t \n\tu64 max_mc_address = min(adev->gmc.mc_mask, AMDGPU_GMC_HOLE_START - 1);\n\n\t \n\tsize_bf = mc->fb_start;\n\tsize_af = max_mc_address + 1 - ALIGN(mc->fb_end + 1, four_gb);\n\n\tif (mc->gart_size > max(size_bf, size_af)) {\n\t\tdev_warn(adev->dev, \"limiting GART\\n\");\n\t\tmc->gart_size = max(size_bf, size_af);\n\t}\n\n\tif ((size_bf >= mc->gart_size && size_bf < size_af) ||\n\t    (size_af < mc->gart_size))\n\t\tmc->gart_start = 0;\n\telse\n\t\tmc->gart_start = max_mc_address - mc->gart_size + 1;\n\n\tmc->gart_start &= ~(four_gb - 1);\n\tmc->gart_end = mc->gart_start + mc->gart_size - 1;\n\tdev_info(adev->dev, \"GART: %lluM 0x%016llX - 0x%016llX\\n\",\n\t\t\tmc->gart_size >> 20, mc->gart_start, mc->gart_end);\n}\n\n \nvoid amdgpu_gmc_agp_location(struct amdgpu_device *adev, struct amdgpu_gmc *mc)\n{\n\tconst uint64_t sixteen_gb = 1ULL << 34;\n\tconst uint64_t sixteen_gb_mask = ~(sixteen_gb - 1);\n\tu64 size_af, size_bf;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tmc->agp_start = 0xffffffffffff;\n\t\tmc->agp_end = 0x0;\n\t\tmc->agp_size = 0;\n\n\t\treturn;\n\t}\n\n\tif (mc->fb_start > mc->gart_start) {\n\t\tsize_bf = (mc->fb_start & sixteen_gb_mask) -\n\t\t\tALIGN(mc->gart_end + 1, sixteen_gb);\n\t\tsize_af = mc->mc_mask + 1 - ALIGN(mc->fb_end + 1, sixteen_gb);\n\t} else {\n\t\tsize_bf = mc->fb_start & sixteen_gb_mask;\n\t\tsize_af = (mc->gart_start & sixteen_gb_mask) -\n\t\t\tALIGN(mc->fb_end + 1, sixteen_gb);\n\t}\n\n\tif (size_bf > size_af) {\n\t\tmc->agp_start = (mc->fb_start - size_bf) & sixteen_gb_mask;\n\t\tmc->agp_size = size_bf;\n\t} else {\n\t\tmc->agp_start = ALIGN(mc->fb_end + 1, sixteen_gb);\n\t\tmc->agp_size = size_af;\n\t}\n\n\tmc->agp_end = mc->agp_start + mc->agp_size - 1;\n\tdev_info(adev->dev, \"AGP: %lluM 0x%016llX - 0x%016llX\\n\",\n\t\t\tmc->agp_size >> 20, mc->agp_start, mc->agp_end);\n}\n\n \nstatic inline uint64_t amdgpu_gmc_fault_key(uint64_t addr, uint16_t pasid)\n{\n\treturn addr << 4 | pasid;\n}\n\n \nbool amdgpu_gmc_filter_faults(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ih_ring *ih, uint64_t addr,\n\t\t\t      uint16_t pasid, uint64_t timestamp)\n{\n\tstruct amdgpu_gmc *gmc = &adev->gmc;\n\tuint64_t stamp, key = amdgpu_gmc_fault_key(addr, pasid);\n\tstruct amdgpu_gmc_fault *fault;\n\tuint32_t hash;\n\n\t \n\tif (amdgpu_ih_ts_after(timestamp, ih->processed_timestamp))\n\t\treturn true;\n\n\t \n\tstamp = max(timestamp, AMDGPU_GMC_FAULT_TIMEOUT + 1) -\n\t\tAMDGPU_GMC_FAULT_TIMEOUT;\n\tif (gmc->fault_ring[gmc->last_fault].timestamp >= stamp)\n\t\treturn true;\n\n\t \n\thash = hash_64(key, AMDGPU_GMC_FAULT_HASH_ORDER);\n\tfault = &gmc->fault_ring[gmc->fault_hash[hash].idx];\n\twhile (fault->timestamp >= stamp) {\n\t\tuint64_t tmp;\n\n\t\tif (atomic64_read(&fault->key) == key) {\n\t\t\t \n\t\t\tif (fault->timestamp_expiry != 0 &&\n\t\t\t    amdgpu_ih_ts_after(fault->timestamp_expiry,\n\t\t\t\t\t       timestamp))\n\t\t\t\tbreak;\n\t\t\telse\n\t\t\t\treturn true;\n\t\t}\n\n\t\ttmp = fault->timestamp;\n\t\tfault = &gmc->fault_ring[fault->next];\n\n\t\t \n\t\tif (fault->timestamp >= tmp)\n\t\t\tbreak;\n\t}\n\n\t \n\tfault = &gmc->fault_ring[gmc->last_fault];\n\tatomic64_set(&fault->key, key);\n\tfault->timestamp = timestamp;\n\n\t \n\tfault->next = gmc->fault_hash[hash].idx;\n\tgmc->fault_hash[hash].idx = gmc->last_fault++;\n\treturn false;\n}\n\n \nvoid amdgpu_gmc_filter_faults_remove(struct amdgpu_device *adev, uint64_t addr,\n\t\t\t\t     uint16_t pasid)\n{\n\tstruct amdgpu_gmc *gmc = &adev->gmc;\n\tuint64_t key = amdgpu_gmc_fault_key(addr, pasid);\n\tstruct amdgpu_ih_ring *ih;\n\tstruct amdgpu_gmc_fault *fault;\n\tuint32_t last_wptr;\n\tuint64_t last_ts;\n\tuint32_t hash;\n\tuint64_t tmp;\n\n\tih = adev->irq.retry_cam_enabled ? &adev->irq.ih_soft : &adev->irq.ih1;\n\t \n\tlast_wptr = amdgpu_ih_get_wptr(adev, ih);\n\t \n\trmb();\n\t \n\tlast_ts = amdgpu_ih_decode_iv_ts(adev, ih, last_wptr, -1);\n\n\thash = hash_64(key, AMDGPU_GMC_FAULT_HASH_ORDER);\n\tfault = &gmc->fault_ring[gmc->fault_hash[hash].idx];\n\tdo {\n\t\tif (atomic64_read(&fault->key) == key) {\n\t\t\t \n\t\t\tfault->timestamp_expiry = last_ts;\n\t\t\tbreak;\n\t\t}\n\n\t\ttmp = fault->timestamp;\n\t\tfault = &gmc->fault_ring[fault->next];\n\t} while (fault->timestamp < tmp);\n}\n\nint amdgpu_gmc_ras_sw_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t \n\tr = amdgpu_umc_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_mmhub_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_hdp_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_mca_mp0_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_mca_mp1_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_mca_mpio_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_xgmi_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\nint amdgpu_gmc_ras_late_init(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\nvoid amdgpu_gmc_ras_fini(struct amdgpu_device *adev)\n{\n\n}\n\n\t \n#define AMDGPU_VMHUB_INV_ENG_BITMAP\t\t0x1FFF3\n\nint amdgpu_gmc_allocate_vm_inv_eng(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tunsigned vm_inv_engs[AMDGPU_MAX_VMHUBS] = {0};\n\tunsigned i;\n\tunsigned vmhub, inv_eng;\n\n\t \n\tfor_each_set_bit(i, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS) {\n\t\tvm_inv_engs[i] = AMDGPU_VMHUB_INV_ENG_BITMAP;\n\t\t \n\t\tif (adev->enable_mes)\n\t\t\tvm_inv_engs[i] &= ~(1 << 5);\n\t}\n\n\tfor (i = 0; i < adev->num_rings; ++i) {\n\t\tring = adev->rings[i];\n\t\tvmhub = ring->vm_hub;\n\n\t\tif (ring == &adev->mes.ring)\n\t\t\tcontinue;\n\n\t\tinv_eng = ffs(vm_inv_engs[vmhub]);\n\t\tif (!inv_eng) {\n\t\t\tdev_err(adev->dev, \"no VM inv eng for ring %s\\n\",\n\t\t\t\tring->name);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tring->vm_inv_eng = inv_eng - 1;\n\t\tvm_inv_engs[vmhub] &= ~(1 << ring->vm_inv_eng);\n\n\t\tdev_info(adev->dev, \"ring %s uses VM inv eng %u on hub %u\\n\",\n\t\t\t ring->name, ring->vm_inv_eng, ring->vm_hub);\n\t}\n\n\treturn 0;\n}\n\n \nvoid amdgpu_gmc_tmz_set(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\t \n\tcase IP_VERSION(9, 2, 2):\n\tcase IP_VERSION(9, 1, 0):\n\t \n\tcase IP_VERSION(9, 3, 0):\n\t \n\tcase IP_VERSION(10, 3, 7):\n\t \n\tcase IP_VERSION(11, 0, 1):\n\t\tif (amdgpu_tmz == 0) {\n\t\t\tadev->gmc.tmz_enabled = false;\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"Trusted Memory Zone (TMZ) feature disabled (cmd line)\\n\");\n\t\t} else {\n\t\t\tadev->gmc.tmz_enabled = true;\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"Trusted Memory Zone (TMZ) feature enabled\\n\");\n\t\t}\n\t\tbreak;\n\tcase IP_VERSION(10, 1, 10):\n\tcase IP_VERSION(10, 1, 1):\n\tcase IP_VERSION(10, 1, 2):\n\tcase IP_VERSION(10, 1, 3):\n\tcase IP_VERSION(10, 3, 0):\n\tcase IP_VERSION(10, 3, 2):\n\tcase IP_VERSION(10, 3, 4):\n\tcase IP_VERSION(10, 3, 5):\n\tcase IP_VERSION(10, 3, 6):\n\t \n\tcase IP_VERSION(10, 3, 1):\n\t \n\tcase IP_VERSION(10, 3, 3):\n\tcase IP_VERSION(11, 0, 4):\n\t\t \n\t\tif (amdgpu_tmz < 1) {\n\t\t\tadev->gmc.tmz_enabled = false;\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"Trusted Memory Zone (TMZ) feature disabled as experimental (default)\\n\");\n\t\t} else {\n\t\t\tadev->gmc.tmz_enabled = true;\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"Trusted Memory Zone (TMZ) feature enabled as experimental (cmd line)\\n\");\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tadev->gmc.tmz_enabled = false;\n\t\tdev_info(adev->dev,\n\t\t\t \"Trusted Memory Zone (TMZ) feature not supported\\n\");\n\t\tbreak;\n\t}\n}\n\n \nvoid amdgpu_gmc_noretry_set(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_gmc *gmc = &adev->gmc;\n\tuint32_t gc_ver = adev->ip_versions[GC_HWIP][0];\n\tbool noretry_default = (gc_ver == IP_VERSION(9, 0, 1) ||\n\t\t\t\tgc_ver == IP_VERSION(9, 3, 0) ||\n\t\t\t\tgc_ver == IP_VERSION(9, 4, 0) ||\n\t\t\t\tgc_ver == IP_VERSION(9, 4, 1) ||\n\t\t\t\tgc_ver == IP_VERSION(9, 4, 2) ||\n\t\t\t\tgc_ver == IP_VERSION(9, 4, 3) ||\n\t\t\t\tgc_ver >= IP_VERSION(10, 3, 0));\n\n\tgmc->noretry = (amdgpu_noretry == -1) ? noretry_default : amdgpu_noretry;\n}\n\nvoid amdgpu_gmc_set_vm_fault_masks(struct amdgpu_device *adev, int hub_type,\n\t\t\t\t   bool enable)\n{\n\tstruct amdgpu_vmhub *hub;\n\tu32 tmp, reg, i;\n\n\thub = &adev->vmhub[hub_type];\n\tfor (i = 0; i < 16; i++) {\n\t\treg = hub->vm_context0_cntl + hub->ctx_distance * i;\n\n\t\ttmp = (hub_type == AMDGPU_GFXHUB(0)) ?\n\t\t\tRREG32_SOC15_IP(GC, reg) :\n\t\t\tRREG32_SOC15_IP(MMHUB, reg);\n\n\t\tif (enable)\n\t\t\ttmp |= hub->vm_cntx_cntl_vm_fault;\n\t\telse\n\t\t\ttmp &= ~hub->vm_cntx_cntl_vm_fault;\n\n\t\t(hub_type == AMDGPU_GFXHUB(0)) ?\n\t\t\tWREG32_SOC15_IP(GC, reg, tmp) :\n\t\t\tWREG32_SOC15_IP(MMHUB, reg, tmp);\n\t}\n}\n\nvoid amdgpu_gmc_get_vbios_allocations(struct amdgpu_device *adev)\n{\n\tunsigned size;\n\n\t \n\tadev->mman.stolen_reserved_offset = 0;\n\tadev->mman.stolen_reserved_size = 0;\n\n\t \n\tswitch (adev->asic_type) {\n\tcase CHIP_VEGA10:\n\t\tadev->mman.keep_stolen_vga_memory = true;\n\t\t \n#ifdef CONFIG_X86\n\t\tif (amdgpu_sriov_vf(adev) && hypervisor_is_type(X86_HYPER_MS_HYPERV)) {\n\t\t\tadev->mman.stolen_reserved_offset = 0x500000;\n\t\t\tadev->mman.stolen_reserved_size = 0x200000;\n\t\t}\n#endif\n\t\tbreak;\n\tcase CHIP_RAVEN:\n\tcase CHIP_RENOIR:\n\t\tadev->mman.keep_stolen_vga_memory = true;\n\t\tbreak;\n\tcase CHIP_YELLOW_CARP:\n\t\tif (amdgpu_discovery == 0) {\n\t\t\tadev->mman.stolen_reserved_offset = 0x1ffb0000;\n\t\t\tadev->mman.stolen_reserved_size = 64 * PAGE_SIZE;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tadev->mman.keep_stolen_vga_memory = false;\n\t\tbreak;\n\t}\n\n\tif (amdgpu_sriov_vf(adev) ||\n\t    !amdgpu_device_has_display_hardware(adev)) {\n\t\tsize = 0;\n\t} else {\n\t\tsize = amdgpu_gmc_get_vbios_fb_size(adev);\n\n\t\tif (adev->mman.keep_stolen_vga_memory)\n\t\t\tsize = max(size, (unsigned)AMDGPU_VBIOS_VGA_ALLOCATION);\n\t}\n\n\t \n\tif ((adev->gmc.real_vram_size - size) < (8 * 1024 * 1024))\n\t\tsize = 0;\n\n\tif (size > AMDGPU_VBIOS_VGA_ALLOCATION) {\n\t\tadev->mman.stolen_vga_size = AMDGPU_VBIOS_VGA_ALLOCATION;\n\t\tadev->mman.stolen_extended_size = size - adev->mman.stolen_vga_size;\n\t} else {\n\t\tadev->mman.stolen_vga_size = size;\n\t\tadev->mman.stolen_extended_size = 0;\n\t}\n}\n\n \nvoid amdgpu_gmc_init_pdb0(struct amdgpu_device *adev)\n{\n\tint i;\n\tuint64_t flags = adev->gart.gart_pte_flags; \n\t \n\tu64 vram_size = adev->gmc.xgmi.node_segment_size * adev->gmc.xgmi.num_physical_nodes;\n\tu64 pde0_page_size = (1ULL<<adev->gmc.vmid0_page_table_block_size)<<21;\n\tu64 vram_addr = adev->vm_manager.vram_base_offset -\n\t\tadev->gmc.xgmi.physical_node_id * adev->gmc.xgmi.node_segment_size;\n\tu64 vram_end = vram_addr + vram_size;\n\tu64 gart_ptb_gpu_pa = amdgpu_gmc_vram_pa(adev, adev->gart.bo);\n\tint idx;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn;\n\n\tflags |= AMDGPU_PTE_VALID | AMDGPU_PTE_READABLE;\n\tflags |= AMDGPU_PTE_WRITEABLE;\n\tflags |= AMDGPU_PTE_SNOOPED;\n\tflags |= AMDGPU_PTE_FRAG((adev->gmc.vmid0_page_table_block_size + 9*1));\n\tflags |= AMDGPU_PDE_PTE;\n\n\t \n\tfor (i = 0; vram_addr < vram_end; i++, vram_addr += pde0_page_size)\n\t\tamdgpu_gmc_set_pte_pde(adev, adev->gmc.ptr_pdb0, i, vram_addr, flags);\n\n\t \n\tflags = AMDGPU_PTE_VALID;\n\tflags |= AMDGPU_PDE_BFS(0) | AMDGPU_PTE_SNOOPED;\n\t \n\tamdgpu_gmc_set_pte_pde(adev, adev->gmc.ptr_pdb0, i, gart_ptb_gpu_pa, flags);\n\tdrm_dev_exit(idx);\n}\n\n \nuint64_t amdgpu_gmc_vram_mc2pa(struct amdgpu_device *adev, uint64_t mc_addr)\n{\n\treturn mc_addr - adev->gmc.vram_start + adev->vm_manager.vram_base_offset;\n}\n\n \nuint64_t amdgpu_gmc_vram_pa(struct amdgpu_device *adev, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_gmc_vram_mc2pa(adev, amdgpu_bo_gpu_offset(bo));\n}\n\n \nuint64_t amdgpu_gmc_vram_cpu_pa(struct amdgpu_device *adev, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_bo_gpu_offset(bo) - adev->gmc.vram_start + adev->gmc.aper_base;\n}\n\nint amdgpu_gmc_vram_checking(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_bo *vram_bo = NULL;\n\tuint64_t vram_gpu = 0;\n\tvoid *vram_ptr = NULL;\n\n\tint ret, size = 0x100000;\n\tuint8_t cptr[10];\n\n\tret = amdgpu_bo_create_kernel(adev, size, PAGE_SIZE,\n\t\t\t\tAMDGPU_GEM_DOMAIN_VRAM,\n\t\t\t\t&vram_bo,\n\t\t\t\t&vram_gpu,\n\t\t\t\t&vram_ptr);\n\tif (ret)\n\t\treturn ret;\n\n\tmemset(vram_ptr, 0x86, size);\n\tmemset(cptr, 0x86, 10);\n\n\t \n\tret = memcmp(vram_ptr, cptr, 10);\n\tif (ret)\n\t\treturn ret;\n\n\tret = memcmp(vram_ptr + (size / 2), cptr, 10);\n\tif (ret)\n\t\treturn ret;\n\n\tret = memcmp(vram_ptr + size - 10, cptr, 10);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_free_kernel(&vram_bo, &vram_gpu,\n\t\t\t&vram_ptr);\n\n\treturn 0;\n}\n\nstatic ssize_t current_memory_partition_show(\n\tstruct device *dev, struct device_attribute *addr, char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tenum amdgpu_memory_partition mode;\n\n\tmode = adev->gmc.gmc_funcs->query_mem_partition_mode(adev);\n\tswitch (mode) {\n\tcase AMDGPU_NPS1_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS1\\n\");\n\tcase AMDGPU_NPS2_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS2\\n\");\n\tcase AMDGPU_NPS3_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS3\\n\");\n\tcase AMDGPU_NPS4_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS4\\n\");\n\tcase AMDGPU_NPS6_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS6\\n\");\n\tcase AMDGPU_NPS8_PARTITION_MODE:\n\t\treturn sysfs_emit(buf, \"NPS8\\n\");\n\tdefault:\n\t\treturn sysfs_emit(buf, \"UNKNOWN\\n\");\n\t}\n\n\treturn sysfs_emit(buf, \"UNKNOWN\\n\");\n}\n\nstatic DEVICE_ATTR_RO(current_memory_partition);\n\nint amdgpu_gmc_sysfs_init(struct amdgpu_device *adev)\n{\n\tif (!adev->gmc.gmc_funcs->query_mem_partition_mode)\n\t\treturn 0;\n\n\treturn device_create_file(adev->dev,\n\t\t\t\t  &dev_attr_current_memory_partition);\n}\n\nvoid amdgpu_gmc_sysfs_fini(struct amdgpu_device *adev)\n{\n\tdevice_remove_file(adev->dev, &dev_attr_current_memory_partition);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}