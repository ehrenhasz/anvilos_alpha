{
  "module_name": "amdgpu_ring_mux.c",
  "hash_id": "48d2f6d159c6a1bd22e0d27cd4df41c40f727b6f560807669f6464e62df3d3e3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ring_mux.c",
  "human_readable_source": " \n#include <linux/slab.h>\n#include <drm/drm_print.h>\n\n#include \"amdgpu_ring_mux.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu.h\"\n\n#define AMDGPU_MUX_RESUBMIT_JIFFIES_TIMEOUT (HZ / 2)\n#define AMDGPU_MAX_LAST_UNSIGNALED_THRESHOLD_US 10000\n\nstatic const struct ring_info {\n\tunsigned int hw_pio;\n\tconst char *ring_name;\n} sw_ring_info[] = {\n\t{ AMDGPU_RING_PRIO_DEFAULT, \"gfx_low\"},\n\t{ AMDGPU_RING_PRIO_2, \"gfx_high\"},\n};\n\nstatic struct kmem_cache *amdgpu_mux_chunk_slab;\n\nstatic inline struct amdgpu_mux_entry *amdgpu_ring_mux_sw_entry(struct amdgpu_ring_mux *mux,\n\t\t\t\t\t\t\t\tstruct amdgpu_ring *ring)\n{\n\treturn ring->entry_index < mux->ring_entry_size ?\n\t\t\t&mux->ring_entry[ring->entry_index] : NULL;\n}\n\n \nstatic void amdgpu_ring_mux_copy_pkt_from_sw_ring(struct amdgpu_ring_mux *mux,\n\t\t\t\t\t\t  struct amdgpu_ring *ring,\n\t\t\t\t\t\t  u64 s_start, u64 s_end)\n{\n\tu64 start, end;\n\tstruct amdgpu_ring *real_ring = mux->real_ring;\n\n\tstart = s_start & ring->buf_mask;\n\tend = s_end & ring->buf_mask;\n\n\tif (start == end) {\n\t\tDRM_ERROR(\"no more data copied from sw ring\\n\");\n\t\treturn;\n\t}\n\tif (start > end) {\n\t\tamdgpu_ring_alloc(real_ring, (ring->ring_size >> 2) + end - start);\n\t\tamdgpu_ring_write_multiple(real_ring, (void *)&ring->ring[start],\n\t\t\t\t\t   (ring->ring_size >> 2) - start);\n\t\tamdgpu_ring_write_multiple(real_ring, (void *)&ring->ring[0], end);\n\t} else {\n\t\tamdgpu_ring_alloc(real_ring, end - start);\n\t\tamdgpu_ring_write_multiple(real_ring, (void *)&ring->ring[start], end - start);\n\t}\n}\n\nstatic void amdgpu_mux_resubmit_chunks(struct amdgpu_ring_mux *mux)\n{\n\tstruct amdgpu_mux_entry *e = NULL;\n\tstruct amdgpu_mux_chunk *chunk;\n\tuint32_t seq, last_seq;\n\tint i;\n\n\t \n\tif (!mux->s_resubmit)\n\t\treturn;\n\n\tfor (i = 0; i < mux->num_ring_entries; i++) {\n\t\tif (mux->ring_entry[i].ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT) {\n\t\t\te = &mux->ring_entry[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!e) {\n\t\tDRM_ERROR(\"%s no low priority ring found\\n\", __func__);\n\t\treturn;\n\t}\n\n\tlast_seq = atomic_read(&e->ring->fence_drv.last_seq);\n\tseq = mux->seqno_to_resubmit;\n\tif (last_seq < seq) {\n\t\t \n\t\tlist_for_each_entry(chunk, &e->list, entry) {\n\t\t\tif (chunk->sync_seq > last_seq && chunk->sync_seq <= seq) {\n\t\t\t\tamdgpu_fence_update_start_timestamp(e->ring,\n\t\t\t\t\t\t\t\t    chunk->sync_seq,\n\t\t\t\t\t\t\t\t    ktime_get());\n\t\t\t\tif (chunk->sync_seq ==\n\t\t\t\t\tle32_to_cpu(*(e->ring->fence_drv.cpu_addr + 2))) {\n\t\t\t\t\tif (chunk->cntl_offset <= e->ring->buf_mask)\n\t\t\t\t\t\tamdgpu_ring_patch_cntl(e->ring,\n\t\t\t\t\t\t\t\t       chunk->cntl_offset);\n\t\t\t\t\tif (chunk->ce_offset <= e->ring->buf_mask)\n\t\t\t\t\t\tamdgpu_ring_patch_ce(e->ring, chunk->ce_offset);\n\t\t\t\t\tif (chunk->de_offset <= e->ring->buf_mask)\n\t\t\t\t\t\tamdgpu_ring_patch_de(e->ring, chunk->de_offset);\n\t\t\t\t}\n\t\t\t\tamdgpu_ring_mux_copy_pkt_from_sw_ring(mux, e->ring,\n\t\t\t\t\t\t\t\t      chunk->start,\n\t\t\t\t\t\t\t\t      chunk->end);\n\t\t\t\tmux->wptr_resubmit = chunk->end;\n\t\t\t\tamdgpu_ring_commit(mux->real_ring);\n\t\t\t}\n\t\t}\n\t}\n\n\tdel_timer(&mux->resubmit_timer);\n\tmux->s_resubmit = false;\n}\n\nstatic void amdgpu_ring_mux_schedule_resubmit(struct amdgpu_ring_mux *mux)\n{\n\tmod_timer(&mux->resubmit_timer, jiffies + AMDGPU_MUX_RESUBMIT_JIFFIES_TIMEOUT);\n}\n\nstatic void amdgpu_mux_resubmit_fallback(struct timer_list *t)\n{\n\tstruct amdgpu_ring_mux *mux = from_timer(mux, t, resubmit_timer);\n\n\tif (!spin_trylock(&mux->lock)) {\n\t\tamdgpu_ring_mux_schedule_resubmit(mux);\n\t\tDRM_ERROR(\"reschedule resubmit\\n\");\n\t\treturn;\n\t}\n\tamdgpu_mux_resubmit_chunks(mux);\n\tspin_unlock(&mux->lock);\n}\n\nint amdgpu_ring_mux_init(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring,\n\t\t\t unsigned int entry_size)\n{\n\tmux->real_ring = ring;\n\tmux->num_ring_entries = 0;\n\n\tmux->ring_entry = kcalloc(entry_size, sizeof(struct amdgpu_mux_entry), GFP_KERNEL);\n\tif (!mux->ring_entry)\n\t\treturn -ENOMEM;\n\n\tmux->ring_entry_size = entry_size;\n\tmux->s_resubmit = false;\n\n\tamdgpu_mux_chunk_slab = kmem_cache_create(\"amdgpu_mux_chunk\",\n\t\t\t\t\t\t  sizeof(struct amdgpu_mux_chunk), 0,\n\t\t\t\t\t\t  SLAB_HWCACHE_ALIGN, NULL);\n\tif (!amdgpu_mux_chunk_slab) {\n\t\tDRM_ERROR(\"create amdgpu_mux_chunk cache failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock_init(&mux->lock);\n\ttimer_setup(&mux->resubmit_timer, amdgpu_mux_resubmit_fallback, 0);\n\n\treturn 0;\n}\n\nvoid amdgpu_ring_mux_fini(struct amdgpu_ring_mux *mux)\n{\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_mux_chunk *chunk, *chunk2;\n\tint i;\n\n\tfor (i = 0; i < mux->num_ring_entries; i++) {\n\t\te = &mux->ring_entry[i];\n\t\tlist_for_each_entry_safe(chunk, chunk2, &e->list, entry) {\n\t\t\tlist_del(&chunk->entry);\n\t\t\tkmem_cache_free(amdgpu_mux_chunk_slab, chunk);\n\t\t}\n\t}\n\tkmem_cache_destroy(amdgpu_mux_chunk_slab);\n\tkfree(mux->ring_entry);\n\tmux->ring_entry = NULL;\n\tmux->num_ring_entries = 0;\n\tmux->ring_entry_size = 0;\n}\n\nint amdgpu_ring_mux_add_sw_ring(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_mux_entry *e;\n\n\tif (mux->num_ring_entries >= mux->ring_entry_size) {\n\t\tDRM_ERROR(\"add sw ring exceeding max entry size\\n\");\n\t\treturn -ENOENT;\n\t}\n\n\te = &mux->ring_entry[mux->num_ring_entries];\n\tring->entry_index = mux->num_ring_entries;\n\te->ring = ring;\n\n\tINIT_LIST_HEAD(&e->list);\n\tmux->num_ring_entries += 1;\n\treturn 0;\n}\n\nvoid amdgpu_ring_mux_set_wptr(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring, u64 wptr)\n{\n\tstruct amdgpu_mux_entry *e;\n\n\tspin_lock(&mux->lock);\n\n\tif (ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT)\n\t\tamdgpu_mux_resubmit_chunks(mux);\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry for sw ring\\n\");\n\t\tspin_unlock(&mux->lock);\n\t\treturn;\n\t}\n\n\t \n\tif (ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT && mux->pending_trailing_fence_signaled) {\n\t\tspin_unlock(&mux->lock);\n\t\treturn;\n\t}\n\n\te->sw_cptr = e->sw_wptr;\n\t \n\tif (ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT && e->sw_cptr < mux->wptr_resubmit)\n\t\te->sw_cptr = mux->wptr_resubmit;\n\te->sw_wptr = wptr;\n\te->start_ptr_in_hw_ring = mux->real_ring->wptr;\n\n\t \n\tif (ring->hw_prio > AMDGPU_RING_PRIO_DEFAULT || mux->wptr_resubmit < wptr) {\n\t\tamdgpu_ring_mux_copy_pkt_from_sw_ring(mux, ring, e->sw_cptr, wptr);\n\t\te->end_ptr_in_hw_ring = mux->real_ring->wptr;\n\t\tamdgpu_ring_commit(mux->real_ring);\n\t} else {\n\t\te->end_ptr_in_hw_ring = mux->real_ring->wptr;\n\t}\n\tspin_unlock(&mux->lock);\n}\n\nu64 amdgpu_ring_mux_get_wptr(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_mux_entry *e;\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry for sw ring\\n\");\n\t\treturn 0;\n\t}\n\n\treturn e->sw_wptr;\n}\n\n \nu64 amdgpu_ring_mux_get_rptr(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_mux_entry *e;\n\tu64 readp, offset, start, end;\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"no sw entry found!\\n\");\n\t\treturn 0;\n\t}\n\n\treadp = amdgpu_ring_get_rptr(mux->real_ring);\n\n\tstart = e->start_ptr_in_hw_ring & mux->real_ring->buf_mask;\n\tend = e->end_ptr_in_hw_ring & mux->real_ring->buf_mask;\n\tif (start > end) {\n\t\tif (readp <= end)\n\t\t\treadp += mux->real_ring->ring_size >> 2;\n\t\tend += mux->real_ring->ring_size >> 2;\n\t}\n\n\tif (start <= readp && readp <= end) {\n\t\toffset = readp - start;\n\t\te->sw_rptr = (e->sw_cptr + offset) & ring->buf_mask;\n\t} else if (readp < start) {\n\t\te->sw_rptr = e->sw_cptr;\n\t} else {\n\t\t \n\t\te->sw_rptr = e->sw_wptr;\n\t}\n\n\treturn e->sw_rptr;\n}\n\nu64 amdgpu_sw_ring_get_rptr_gfx(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\n\tWARN_ON(!ring->is_sw_ring);\n\treturn amdgpu_ring_mux_get_rptr(mux, ring);\n}\n\nu64 amdgpu_sw_ring_get_wptr_gfx(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\n\tWARN_ON(!ring->is_sw_ring);\n\treturn amdgpu_ring_mux_get_wptr(mux, ring);\n}\n\nvoid amdgpu_sw_ring_set_wptr_gfx(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\n\tWARN_ON(!ring->is_sw_ring);\n\tamdgpu_ring_mux_set_wptr(mux, ring, ring->wptr);\n}\n\n \nvoid amdgpu_sw_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tWARN_ON(!ring->is_sw_ring);\n}\n\nconst char *amdgpu_sw_ring_name(int idx)\n{\n\treturn idx < ARRAY_SIZE(sw_ring_info) ?\n\t\tsw_ring_info[idx].ring_name : NULL;\n}\n\nunsigned int amdgpu_sw_ring_priority(int idx)\n{\n\treturn idx < ARRAY_SIZE(sw_ring_info) ?\n\t\tsw_ring_info[idx].hw_pio : AMDGPU_RING_PRIO_DEFAULT;\n}\n\n \nstatic int amdgpu_mcbp_scan(struct amdgpu_ring_mux *mux)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, need_preempt;\n\n\tneed_preempt = 0;\n\tfor (i = 0; i < mux->num_ring_entries; i++) {\n\t\tring = mux->ring_entry[i].ring;\n\t\tif (ring->hw_prio > AMDGPU_RING_PRIO_DEFAULT &&\n\t\t    amdgpu_fence_count_emitted(ring) > 0)\n\t\t\treturn 0;\n\t\tif (ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT &&\n\t\t    amdgpu_fence_last_unsignaled_time_us(ring) >\n\t\t    AMDGPU_MAX_LAST_UNSIGNALED_THRESHOLD_US)\n\t\t\tneed_preempt = 1;\n\t}\n\treturn need_preempt && !mux->s_resubmit;\n}\n\n \nstatic int amdgpu_mcbp_trigger_preempt(struct amdgpu_ring_mux *mux)\n{\n\tint r;\n\n\tspin_lock(&mux->lock);\n\tmux->pending_trailing_fence_signaled = true;\n\tr = amdgpu_ring_preempt_ib(mux->real_ring);\n\tspin_unlock(&mux->lock);\n\treturn r;\n}\n\nvoid amdgpu_sw_ring_ib_begin(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\n\tWARN_ON(!ring->is_sw_ring);\n\tif (adev->gfx.mcbp && ring->hw_prio > AMDGPU_RING_PRIO_DEFAULT) {\n\t\tif (amdgpu_mcbp_scan(mux) > 0)\n\t\t\tamdgpu_mcbp_trigger_preempt(mux);\n\t\treturn;\n\t}\n\n\tamdgpu_ring_mux_start_ib(mux, ring);\n}\n\nvoid amdgpu_sw_ring_ib_end(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\n\tWARN_ON(!ring->is_sw_ring);\n\tif (ring->hw_prio > AMDGPU_RING_PRIO_DEFAULT)\n\t\treturn;\n\tamdgpu_ring_mux_end_ib(mux, ring);\n}\n\nvoid amdgpu_sw_ring_ib_mark_offset(struct amdgpu_ring *ring, enum amdgpu_ring_mux_offset_type type)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ring_mux *mux = &adev->gfx.muxer;\n\tunsigned offset;\n\n\tif (ring->hw_prio > AMDGPU_RING_PRIO_DEFAULT)\n\t\treturn;\n\n\toffset = ring->wptr & ring->buf_mask;\n\n\tamdgpu_ring_mux_ib_mark_offset(mux, ring, offset, type);\n}\n\nvoid amdgpu_ring_mux_start_ib(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_mux_chunk *chunk;\n\n\tspin_lock(&mux->lock);\n\tamdgpu_mux_resubmit_chunks(mux);\n\tspin_unlock(&mux->lock);\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry!\\n\");\n\t\treturn;\n\t}\n\n\tchunk = kmem_cache_alloc(amdgpu_mux_chunk_slab, GFP_KERNEL);\n\tif (!chunk) {\n\t\tDRM_ERROR(\"alloc amdgpu_mux_chunk_slab failed\\n\");\n\t\treturn;\n\t}\n\n\tchunk->start = ring->wptr;\n\t \n\tchunk->cntl_offset = ring->buf_mask + 1;\n\tchunk->de_offset = ring->buf_mask + 1;\n\tchunk->ce_offset = ring->buf_mask + 1;\n\tlist_add_tail(&chunk->entry, &e->list);\n}\n\nstatic void scan_and_remove_signaled_chunk(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tuint32_t last_seq = 0;\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_mux_chunk *chunk, *tmp;\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry!\\n\");\n\t\treturn;\n\t}\n\n\tlast_seq = atomic_read(&ring->fence_drv.last_seq);\n\n\tlist_for_each_entry_safe(chunk, tmp, &e->list, entry) {\n\t\tif (chunk->sync_seq <= last_seq) {\n\t\t\tlist_del(&chunk->entry);\n\t\t\tkmem_cache_free(amdgpu_mux_chunk_slab, chunk);\n\t\t}\n\t}\n}\n\nvoid amdgpu_ring_mux_ib_mark_offset(struct amdgpu_ring_mux *mux,\n\t\t\t\t    struct amdgpu_ring *ring, u64 offset,\n\t\t\t\t    enum amdgpu_ring_mux_offset_type type)\n{\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_mux_chunk *chunk;\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry!\\n\");\n\t\treturn;\n\t}\n\n\tchunk = list_last_entry(&e->list, struct amdgpu_mux_chunk, entry);\n\tif (!chunk) {\n\t\tDRM_ERROR(\"cannot find chunk!\\n\");\n\t\treturn;\n\t}\n\n\tswitch (type) {\n\tcase AMDGPU_MUX_OFFSET_TYPE_CONTROL:\n\t\tchunk->cntl_offset = offset;\n\t\tbreak;\n\tcase AMDGPU_MUX_OFFSET_TYPE_DE:\n\t\tchunk->de_offset = offset;\n\t\tbreak;\n\tcase AMDGPU_MUX_OFFSET_TYPE_CE:\n\t\tchunk->ce_offset = offset;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid type (%d)\\n\", type);\n\t\tbreak;\n\t}\n}\n\nvoid amdgpu_ring_mux_end_ib(struct amdgpu_ring_mux *mux, struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_mux_chunk *chunk;\n\n\te = amdgpu_ring_mux_sw_entry(mux, ring);\n\tif (!e) {\n\t\tDRM_ERROR(\"cannot find entry!\\n\");\n\t\treturn;\n\t}\n\n\tchunk = list_last_entry(&e->list, struct amdgpu_mux_chunk, entry);\n\tif (!chunk) {\n\t\tDRM_ERROR(\"cannot find chunk!\\n\");\n\t\treturn;\n\t}\n\n\tchunk->end = ring->wptr;\n\tchunk->sync_seq = READ_ONCE(ring->fence_drv.sync_seq);\n\n\tscan_and_remove_signaled_chunk(mux, ring);\n}\n\nbool amdgpu_mcbp_handle_trailing_fence_irq(struct amdgpu_ring_mux *mux)\n{\n\tstruct amdgpu_mux_entry *e;\n\tstruct amdgpu_ring *ring = NULL;\n\tint i;\n\n\tif (!mux->pending_trailing_fence_signaled)\n\t\treturn false;\n\n\tif (mux->real_ring->trail_seq != le32_to_cpu(*mux->real_ring->trail_fence_cpu_addr))\n\t\treturn false;\n\n\tfor (i = 0; i < mux->num_ring_entries; i++) {\n\t\te = &mux->ring_entry[i];\n\t\tif (e->ring->hw_prio <= AMDGPU_RING_PRIO_DEFAULT) {\n\t\t\tring = e->ring;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!ring) {\n\t\tDRM_ERROR(\"cannot find low priority ring\\n\");\n\t\treturn false;\n\t}\n\n\tamdgpu_fence_process(ring);\n\tif (amdgpu_fence_count_emitted(ring) > 0) {\n\t\tmux->s_resubmit = true;\n\t\tmux->seqno_to_resubmit = ring->fence_drv.sync_seq;\n\t\tamdgpu_ring_mux_schedule_resubmit(mux);\n\t}\n\n\tmux->pending_trailing_fence_signaled = false;\n\treturn true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}