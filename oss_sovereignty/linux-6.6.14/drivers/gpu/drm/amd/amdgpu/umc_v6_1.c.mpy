{
  "module_name": "umc_v6_1.c",
  "hash_id": "8ead90063986a806a5665000e4952cd31d8a6c9a58054438e4b0c2592fafa304",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/umc_v6_1.c",
  "human_readable_source": " \n#include \"umc_v6_1.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_umc.h\"\n#include \"amdgpu.h\"\n\n#include \"rsmu/rsmu_0_0_2_offset.h\"\n#include \"rsmu/rsmu_0_0_2_sh_mask.h\"\n#include \"umc/umc_6_1_1_offset.h\"\n#include \"umc/umc_6_1_1_sh_mask.h\"\n#include \"umc/umc_6_1_2_offset.h\"\n\n#define UMC_6_INST_DIST\t\t\t0x40000\n\nconst uint32_t\n\tumc_v6_1_channel_idx_tbl[UMC_V6_1_UMC_INSTANCE_NUM][UMC_V6_1_CHANNEL_INSTANCE_NUM] = {\n\t\t{2, 18, 11, 27},\t{4, 20, 13, 29},\n\t\t{1, 17, 8, 24},\t\t{7, 23, 14, 30},\n\t\t{10, 26, 3, 19},\t{12, 28, 5, 21},\n\t\t{9, 25, 0, 16},\t\t{15, 31, 6, 22}\n};\n\nstatic void umc_v6_1_enable_umc_index_mode(struct amdgpu_device *adev)\n{\n\tuint32_t rsmu_umc_addr, rsmu_umc_val;\n\n\trsmu_umc_addr = SOC15_REG_OFFSET(RSMU, 0,\n\t\t\tmmRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU);\n\trsmu_umc_val = RREG32_PCIE(rsmu_umc_addr * 4);\n\n\trsmu_umc_val = REG_SET_FIELD(rsmu_umc_val,\n\t\t\tRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU,\n\t\t\tRSMU_UMC_INDEX_MODE_EN, 1);\n\n\tWREG32_PCIE(rsmu_umc_addr * 4, rsmu_umc_val);\n}\n\nstatic void umc_v6_1_disable_umc_index_mode(struct amdgpu_device *adev)\n{\n\tuint32_t rsmu_umc_addr, rsmu_umc_val;\n\n\trsmu_umc_addr = SOC15_REG_OFFSET(RSMU, 0,\n\t\t\tmmRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU);\n\trsmu_umc_val = RREG32_PCIE(rsmu_umc_addr * 4);\n\n\trsmu_umc_val = REG_SET_FIELD(rsmu_umc_val,\n\t\t\tRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU,\n\t\t\tRSMU_UMC_INDEX_MODE_EN, 0);\n\n\tWREG32_PCIE(rsmu_umc_addr * 4, rsmu_umc_val);\n}\n\nstatic uint32_t umc_v6_1_get_umc_index_mode_state(struct amdgpu_device *adev)\n{\n\tuint32_t rsmu_umc_addr, rsmu_umc_val;\n\n\trsmu_umc_addr = SOC15_REG_OFFSET(RSMU, 0,\n\t\t\tmmRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU);\n\trsmu_umc_val = RREG32_PCIE(rsmu_umc_addr * 4);\n\n\treturn REG_GET_FIELD(rsmu_umc_val,\n\t\t\tRSMU_UMC_INDEX_REGISTER_NBIF_VG20_GPU,\n\t\t\tRSMU_UMC_INDEX_MODE_EN);\n}\n\nstatic inline uint32_t get_umc_6_reg_offset(struct amdgpu_device *adev,\n\t\t\t\t\t    uint32_t umc_inst,\n\t\t\t\t\t    uint32_t ch_inst)\n{\n\treturn adev->umc.channel_offs*ch_inst + UMC_6_INST_DIST*umc_inst;\n}\n\nstatic void umc_v6_1_clear_error_count_per_channel(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t umc_reg_offset)\n{\n\tuint32_t ecc_err_cnt_addr;\n\tuint32_t ecc_err_cnt_sel, ecc_err_cnt_sel_addr;\n\n\tif (adev->asic_type == CHIP_ARCTURUS) {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\t\tmmUMCCH0_0_EccErrCntSel_ARCT);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\t\tmmUMCCH0_0_EccErrCnt_ARCT);\n\t} else {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\t\tmmUMCCH0_0_EccErrCntSel);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\t\tmmUMCCH0_0_EccErrCnt);\n\t}\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr +\n\t\t\t\t\tumc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel,\n\t\t\t\t\tUMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 0);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4,\n\t\t\tecc_err_cnt_sel);\n\n\t \n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4,\n\t\t\tUMC_V6_1_CE_CNT_INIT);\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr +\n\t\t\t\t\tumc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel,\n\t\t\t\t\tUMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4,\n\t\t\tecc_err_cnt_sel);\n\n\t \n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4,\n\t\t\tUMC_V6_1_CE_CNT_INIT);\n}\n\nstatic void umc_v6_1_clear_error_count(struct amdgpu_device *adev)\n{\n\tuint32_t umc_inst        = 0;\n\tuint32_t ch_inst         = 0;\n\tuint32_t umc_reg_offset  = 0;\n\tuint32_t rsmu_umc_index_state =\n\t\t\t\tumc_v6_1_get_umc_index_mode_state(adev);\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_disable_umc_index_mode(adev);\n\n\tLOOP_UMC_INST_AND_CH(umc_inst, ch_inst) {\n\t\tumc_reg_offset = get_umc_6_reg_offset(adev,\n\t\t\t\t\t\tumc_inst,\n\t\t\t\t\t\tch_inst);\n\n\t\tumc_v6_1_clear_error_count_per_channel(adev,\n\t\t\t\t\t\tumc_reg_offset);\n\t}\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_enable_umc_index_mode(adev);\n}\n\nstatic void umc_v6_1_query_correctable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t umc_reg_offset,\n\t\t\t\t\t\t   unsigned long *error_count)\n{\n\tuint32_t ecc_err_cnt_sel, ecc_err_cnt_sel_addr;\n\tuint32_t ecc_err_cnt, ecc_err_cnt_addr;\n\tuint64_t mc_umc_status;\n\tuint32_t mc_umc_status_addr;\n\n\tif (adev->asic_type == CHIP_ARCTURUS) {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCntSel_ARCT);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCnt_ARCT);\n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0_ARCT);\n\t} else {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCntSel);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCnt);\n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0);\n\t}\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 0);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\n\tecc_err_cnt = RREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4);\n\t*error_count +=\n\t\t(REG_GET_FIELD(ecc_err_cnt, UMCCH0_0_EccErrCnt, EccErrCnt) -\n\t\t UMC_V6_1_CE_CNT_INIT);\n\n\t \n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\n\tecc_err_cnt = RREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4);\n\t*error_count +=\n\t\t(REG_GET_FIELD(ecc_err_cnt, UMCCH0_0_EccErrCnt, EccErrCnt) -\n\t\t UMC_V6_1_CE_CNT_INIT);\n\n\t \n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, ErrorCodeExt) == 6 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, CECC) == 1)\n\t\t*error_count += 1;\n}\n\nstatic void umc_v6_1_querry_uncorrectable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t      uint32_t umc_reg_offset,\n\t\t\t\t\t\t      unsigned long *error_count)\n{\n\tuint64_t mc_umc_status;\n\tuint32_t mc_umc_status_addr;\n\n\tif (adev->asic_type == CHIP_ARCTURUS) {\n\t\t \n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0_ARCT);\n\t} else {\n\t\t \n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0);\n\t}\n\n\t \n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\tif ((REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1) &&\n\t    (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Deferred) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, PCC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, TCC) == 1))\n\t\t*error_count += 1;\n}\n\nstatic void umc_v6_1_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t   void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\n\tuint32_t umc_inst        = 0;\n\tuint32_t ch_inst         = 0;\n\tuint32_t umc_reg_offset  = 0;\n\n\tuint32_t rsmu_umc_index_state = umc_v6_1_get_umc_index_mode_state(adev);\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_disable_umc_index_mode(adev);\n\n\tif ((adev->asic_type == CHIP_ARCTURUS) &&\n\t\tamdgpu_dpm_set_df_cstate(adev, DF_CSTATE_DISALLOW))\n\t\tDRM_WARN(\"Fail to disable DF-Cstate.\\n\");\n\n\tLOOP_UMC_INST_AND_CH(umc_inst, ch_inst) {\n\t\tumc_reg_offset = get_umc_6_reg_offset(adev,\n\t\t\t\t\t\t      umc_inst,\n\t\t\t\t\t\t      ch_inst);\n\n\t\tumc_v6_1_query_correctable_error_count(adev,\n\t\t\t\t\t\t       umc_reg_offset,\n\t\t\t\t\t\t       &(err_data->ce_count));\n\t\tumc_v6_1_querry_uncorrectable_error_count(adev,\n\t\t\t\t\t\t\t  umc_reg_offset,\n\t\t\t\t\t\t\t  &(err_data->ue_count));\n\t}\n\n\tif ((adev->asic_type == CHIP_ARCTURUS) &&\n\t\tamdgpu_dpm_set_df_cstate(adev, DF_CSTATE_ALLOW))\n\t\tDRM_WARN(\"Fail to enable DF-Cstate\\n\");\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_enable_umc_index_mode(adev);\n\n\tumc_v6_1_clear_error_count(adev);\n}\n\nstatic void umc_v6_1_query_error_address(struct amdgpu_device *adev,\n\t\t\t\t\t struct ras_err_data *err_data,\n\t\t\t\t\t uint32_t umc_reg_offset,\n\t\t\t\t\t uint32_t ch_inst,\n\t\t\t\t\t uint32_t umc_inst)\n{\n\tuint32_t lsb, mc_umc_status_addr;\n\tuint64_t mc_umc_status, err_addr, retired_page, mc_umc_addrt0;\n\tuint32_t channel_index = adev->umc.channel_idx_tbl[umc_inst * adev->umc.channel_inst_num + ch_inst];\n\n\tif (adev->asic_type == CHIP_ARCTURUS) {\n\t\t \n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0_ARCT);\n\t\tmc_umc_addrt0 =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_ADDRT0_ARCT);\n\t} else {\n\t\t \n\t\tmc_umc_status_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_STATUST0);\n\t\tmc_umc_addrt0 =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmMCA_UMC_UMC0_MCUMC_ADDRT0);\n\t}\n\n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\n\tif (mc_umc_status == 0)\n\t\treturn;\n\n\tif (!err_data->err_addr) {\n\t\t \n\t\tWREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4, 0x0ULL);\n\t\treturn;\n\t}\n\n\t \n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1) {\n\n\t\terr_addr = RREG64_PCIE((mc_umc_addrt0 + umc_reg_offset) * 4);\n\t\t \n\t\tlsb = REG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, LSB);\n\t\terr_addr = REG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, ErrorAddr);\n\t\terr_addr &= ~((0x1ULL << lsb) - 1);\n\n\t\t \n\t\tretired_page = ADDR_OF_8KB_BLOCK(err_addr) |\n\t\t\t\tADDR_OF_256B_BLOCK(channel_index) |\n\t\t\t\tOFFSET_IN_256B_BLOCK(err_addr);\n\n\t\tamdgpu_umc_fill_error_record(err_data, err_addr,\n\t\t\t\t\tretired_page, channel_index, umc_inst);\n\t}\n\n\t \n\tWREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4, 0x0ULL);\n}\n\nstatic void umc_v6_1_query_ras_error_address(struct amdgpu_device *adev,\n\t\t\t\t\t     void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\n\tuint32_t umc_inst        = 0;\n\tuint32_t ch_inst         = 0;\n\tuint32_t umc_reg_offset  = 0;\n\n\tuint32_t rsmu_umc_index_state = umc_v6_1_get_umc_index_mode_state(adev);\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_disable_umc_index_mode(adev);\n\n\tif ((adev->asic_type == CHIP_ARCTURUS) &&\n\t\tamdgpu_dpm_set_df_cstate(adev, DF_CSTATE_DISALLOW))\n\t\tDRM_WARN(\"Fail to disable DF-Cstate.\\n\");\n\n\tLOOP_UMC_INST_AND_CH(umc_inst, ch_inst) {\n\t\tumc_reg_offset = get_umc_6_reg_offset(adev,\n\t\t\t\t\t\t      umc_inst,\n\t\t\t\t\t\t      ch_inst);\n\n\t\tumc_v6_1_query_error_address(adev,\n\t\t\t\t\t     err_data,\n\t\t\t\t\t     umc_reg_offset,\n\t\t\t\t\t     ch_inst,\n\t\t\t\t\t     umc_inst);\n\t}\n\n\tif ((adev->asic_type == CHIP_ARCTURUS) &&\n\t\tamdgpu_dpm_set_df_cstate(adev, DF_CSTATE_ALLOW))\n\t\tDRM_WARN(\"Fail to enable DF-Cstate\\n\");\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_enable_umc_index_mode(adev);\n}\n\nstatic void umc_v6_1_err_cnt_init_per_channel(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t umc_reg_offset)\n{\n\tuint32_t ecc_err_cnt_sel, ecc_err_cnt_sel_addr;\n\tuint32_t ecc_err_cnt_addr;\n\n\tif (adev->asic_type == CHIP_ARCTURUS) {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCntSel_ARCT);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCnt_ARCT);\n\t} else {\n\t\t \n\t\tecc_err_cnt_sel_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCntSel);\n\t\tecc_err_cnt_addr =\n\t\t\tSOC15_REG_OFFSET(UMC, 0, mmUMCCH0_0_EccErrCnt);\n\t}\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 0);\n\t \n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrInt, 0x1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\t \n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4, UMC_V6_1_CE_CNT_INIT);\n\n\t \n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4, UMC_V6_1_CE_CNT_INIT);\n}\n\nstatic void umc_v6_1_err_cnt_init(struct amdgpu_device *adev)\n{\n\tuint32_t umc_inst        = 0;\n\tuint32_t ch_inst         = 0;\n\tuint32_t umc_reg_offset  = 0;\n\n\tuint32_t rsmu_umc_index_state = umc_v6_1_get_umc_index_mode_state(adev);\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_disable_umc_index_mode(adev);\n\n\tLOOP_UMC_INST_AND_CH(umc_inst, ch_inst) {\n\t\tumc_reg_offset = get_umc_6_reg_offset(adev,\n\t\t\t\t\t\t      umc_inst,\n\t\t\t\t\t\t      ch_inst);\n\n\t\tumc_v6_1_err_cnt_init_per_channel(adev, umc_reg_offset);\n\t}\n\n\tif (rsmu_umc_index_state)\n\t\tumc_v6_1_enable_umc_index_mode(adev);\n}\n\nconst struct amdgpu_ras_block_hw_ops umc_v6_1_ras_hw_ops = {\n\t.query_ras_error_count = umc_v6_1_query_ras_error_count,\n\t.query_ras_error_address = umc_v6_1_query_ras_error_address,\n};\n\nstruct amdgpu_umc_ras umc_v6_1_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &umc_v6_1_ras_hw_ops,\n\t},\n\t.err_cnt_init = umc_v6_1_err_cnt_init,\n};",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}