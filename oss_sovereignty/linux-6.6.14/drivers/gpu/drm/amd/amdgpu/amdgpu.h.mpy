{
  "module_name": "amdgpu.h",
  "hash_id": "99ffb79bd665835bf5ca11431419eaf43e4750c5dc368d62ee5a5e9a73894465",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu.h",
  "human_readable_source": " \n#ifndef __AMDGPU_H__\n#define __AMDGPU_H__\n\n#ifdef pr_fmt\n#undef pr_fmt\n#endif\n\n#define pr_fmt(fmt) \"amdgpu: \" fmt\n\n#ifdef dev_fmt\n#undef dev_fmt\n#endif\n\n#define dev_fmt(fmt) \"amdgpu: \" fmt\n\n#include \"amdgpu_ctx.h\"\n\n#include <linux/atomic.h>\n#include <linux/wait.h>\n#include <linux/list.h>\n#include <linux/kref.h>\n#include <linux/rbtree.h>\n#include <linux/hashtable.h>\n#include <linux/dma-fence.h>\n#include <linux/pci.h>\n\n#include <drm/ttm/ttm_bo.h>\n#include <drm/ttm/ttm_placement.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_gem.h>\n#include <drm/drm_ioctl.h>\n\n#include <kgd_kfd_interface.h>\n#include \"dm_pp_interface.h\"\n#include \"kgd_pp_interface.h\"\n\n#include \"amd_shared.h\"\n#include \"amdgpu_mode.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_irq.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_ttm.h\"\n#include \"amdgpu_psp.h\"\n#include \"amdgpu_gds.h\"\n#include \"amdgpu_sync.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_dpm.h\"\n#include \"amdgpu_acp.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"amdgpu_vcn.h\"\n#include \"amdgpu_jpeg.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_gfx.h\"\n#include \"amdgpu_sdma.h\"\n#include \"amdgpu_lsdma.h\"\n#include \"amdgpu_nbio.h\"\n#include \"amdgpu_hdp.h\"\n#include \"amdgpu_dm.h\"\n#include \"amdgpu_virt.h\"\n#include \"amdgpu_csa.h\"\n#include \"amdgpu_mes_ctx.h\"\n#include \"amdgpu_gart.h\"\n#include \"amdgpu_debugfs.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_bo_list.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_doorbell.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_discovery.h\"\n#include \"amdgpu_mes.h\"\n#include \"amdgpu_umc.h\"\n#include \"amdgpu_mmhub.h\"\n#include \"amdgpu_gfxhub.h\"\n#include \"amdgpu_df.h\"\n#include \"amdgpu_smuio.h\"\n#include \"amdgpu_fdinfo.h\"\n#include \"amdgpu_mca.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_xcp.h\"\n\n#define MAX_GPU_INSTANCE\t\t64\n\nstruct amdgpu_gpu_instance\n{\n\tstruct amdgpu_device\t\t*adev;\n\tint\t\t\t\tmgpu_fan_enabled;\n};\n\nstruct amdgpu_mgpu_info\n{\n\tstruct amdgpu_gpu_instance\tgpu_ins[MAX_GPU_INSTANCE];\n\tstruct mutex\t\t\tmutex;\n\tuint32_t\t\t\tnum_gpu;\n\tuint32_t\t\t\tnum_dgpu;\n\tuint32_t\t\t\tnum_apu;\n\n\t \n\tstruct delayed_work\t\tdelayed_reset_work;\n\tbool\t\t\t\tpending_reset;\n};\n\nenum amdgpu_ss {\n\tAMDGPU_SS_DRV_LOAD,\n\tAMDGPU_SS_DEV_D0,\n\tAMDGPU_SS_DEV_D3,\n\tAMDGPU_SS_DRV_UNLOAD\n};\n\nstruct amdgpu_watchdog_timer\n{\n\tbool timeout_fatal_disable;\n\tuint32_t period;  \n};\n\n#define AMDGPU_MAX_TIMEOUT_PARAM_LENGTH\t256\n\n \nextern int amdgpu_modeset;\nextern unsigned int amdgpu_vram_limit;\nextern int amdgpu_vis_vram_limit;\nextern int amdgpu_gart_size;\nextern int amdgpu_gtt_size;\nextern int amdgpu_moverate;\nextern int amdgpu_audio;\nextern int amdgpu_disp_priority;\nextern int amdgpu_hw_i2c;\nextern int amdgpu_pcie_gen2;\nextern int amdgpu_msi;\nextern char amdgpu_lockup_timeout[AMDGPU_MAX_TIMEOUT_PARAM_LENGTH];\nextern int amdgpu_dpm;\nextern int amdgpu_fw_load_type;\nextern int amdgpu_aspm;\nextern int amdgpu_runtime_pm;\nextern uint amdgpu_ip_block_mask;\nextern int amdgpu_bapm;\nextern int amdgpu_deep_color;\nextern int amdgpu_vm_size;\nextern int amdgpu_vm_block_size;\nextern int amdgpu_vm_fragment_size;\nextern int amdgpu_vm_fault_stop;\nextern int amdgpu_vm_debug;\nextern int amdgpu_vm_update_mode;\nextern int amdgpu_exp_hw_support;\nextern int amdgpu_dc;\nextern int amdgpu_sched_jobs;\nextern int amdgpu_sched_hw_submission;\nextern uint amdgpu_pcie_gen_cap;\nextern uint amdgpu_pcie_lane_cap;\nextern u64 amdgpu_cg_mask;\nextern uint amdgpu_pg_mask;\nextern uint amdgpu_sdma_phase_quantum;\nextern char *amdgpu_disable_cu;\nextern char *amdgpu_virtual_display;\nextern uint amdgpu_pp_feature_mask;\nextern uint amdgpu_force_long_training;\nextern int amdgpu_lbpw;\nextern int amdgpu_compute_multipipe;\nextern int amdgpu_gpu_recovery;\nextern int amdgpu_emu_mode;\nextern uint amdgpu_smu_memory_pool_size;\nextern int amdgpu_smu_pptable_id;\nextern uint amdgpu_dc_feature_mask;\nextern uint amdgpu_dc_debug_mask;\nextern uint amdgpu_dc_visual_confirm;\nextern uint amdgpu_dm_abm_level;\nextern int amdgpu_backlight;\nextern struct amdgpu_mgpu_info mgpu_info;\nextern int amdgpu_ras_enable;\nextern uint amdgpu_ras_mask;\nextern int amdgpu_bad_page_threshold;\nextern bool amdgpu_ignore_bad_page_threshold;\nextern struct amdgpu_watchdog_timer amdgpu_watchdog_timer;\nextern int amdgpu_async_gfx_ring;\nextern int amdgpu_mcbp;\nextern int amdgpu_discovery;\nextern int amdgpu_mes;\nextern int amdgpu_mes_kiq;\nextern int amdgpu_noretry;\nextern int amdgpu_force_asic_type;\nextern int amdgpu_smartshift_bias;\nextern int amdgpu_use_xgmi_p2p;\nextern int amdgpu_mtype_local;\nextern bool enforce_isolation;\n#ifdef CONFIG_HSA_AMD\nextern int sched_policy;\nextern bool debug_evictions;\nextern bool no_system_mem_limit;\nextern int halt_if_hws_hang;\n#else\nstatic const int __maybe_unused sched_policy = KFD_SCHED_POLICY_HWS;\nstatic const bool __maybe_unused debug_evictions;  \nstatic const bool __maybe_unused no_system_mem_limit;\nstatic const int __maybe_unused halt_if_hws_hang;\n#endif\n#ifdef CONFIG_HSA_AMD_P2P\nextern bool pcie_p2p;\n#endif\n\nextern int amdgpu_tmz;\nextern int amdgpu_reset_method;\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nextern int amdgpu_si_support;\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\nextern int amdgpu_cik_support;\n#endif\nextern int amdgpu_num_kcq;\n\n#define AMDGPU_VCNFW_LOG_SIZE (32 * 1024)\nextern int amdgpu_vcnfw_log;\nextern int amdgpu_sg_display;\n\nextern int amdgpu_user_partt_mode;\n\n#define AMDGPU_VM_MAX_NUM_CTX\t\t\t4096\n#define AMDGPU_SG_THRESHOLD\t\t\t(256*1024*1024)\n#define AMDGPU_WAIT_IDLE_TIMEOUT_IN_MS\t        3000\n#define AMDGPU_MAX_USEC_TIMEOUT\t\t\t100000\t \n#define AMDGPU_FENCE_JIFFIES_TIMEOUT\t\t(HZ / 2)\n#define AMDGPU_DEBUGFS_MAX_COMPONENTS\t\t32\n#define AMDGPUFB_CONN_LIMIT\t\t\t4\n#define AMDGPU_BIOS_NUM_SCRATCH\t\t\t16\n\n#define AMDGPU_VBIOS_VGA_ALLOCATION\t\t(9 * 1024 * 1024)  \n\n \n#define AMDGPU_ASIC_RESET_DATA                  0x39d5e86b\n\n \n#define AMDGPU_RESET_GFX\t\t\t(1 << 0)\n#define AMDGPU_RESET_COMPUTE\t\t\t(1 << 1)\n#define AMDGPU_RESET_DMA\t\t\t(1 << 2)\n#define AMDGPU_RESET_CP\t\t\t\t(1 << 3)\n#define AMDGPU_RESET_GRBM\t\t\t(1 << 4)\n#define AMDGPU_RESET_DMA1\t\t\t(1 << 5)\n#define AMDGPU_RESET_RLC\t\t\t(1 << 6)\n#define AMDGPU_RESET_SEM\t\t\t(1 << 7)\n#define AMDGPU_RESET_IH\t\t\t\t(1 << 8)\n#define AMDGPU_RESET_VMC\t\t\t(1 << 9)\n#define AMDGPU_RESET_MC\t\t\t\t(1 << 10)\n#define AMDGPU_RESET_DISPLAY\t\t\t(1 << 11)\n#define AMDGPU_RESET_UVD\t\t\t(1 << 12)\n#define AMDGPU_RESET_VCE\t\t\t(1 << 13)\n#define AMDGPU_RESET_VCE1\t\t\t(1 << 14)\n\n \n#define CIK_CURSOR_WIDTH 128\n#define CIK_CURSOR_HEIGHT 128\n\n \n#define AMDGPU_SMARTSHIFT_MAX_BIAS (100)\n#define AMDGPU_SMARTSHIFT_MIN_BIAS (-100)\n\n \n#define AMDGPU_SWCTF_EXTRA_DELAY\t\t50\n\nstruct amdgpu_xcp_mgr;\nstruct amdgpu_device;\nstruct amdgpu_irq_src;\nstruct amdgpu_fpriv;\nstruct amdgpu_bo_va_mapping;\nstruct kfd_vm_fault_info;\nstruct amdgpu_hive_info;\nstruct amdgpu_reset_context;\nstruct amdgpu_reset_control;\n\nenum amdgpu_cp_irq {\n\tAMDGPU_CP_IRQ_GFX_ME0_PIPE0_EOP = 0,\n\tAMDGPU_CP_IRQ_GFX_ME0_PIPE1_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE1_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE2_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE3_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE0_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE1_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE2_EOP,\n\tAMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE3_EOP,\n\n\tAMDGPU_CP_IRQ_LAST\n};\n\nenum amdgpu_thermal_irq {\n\tAMDGPU_THERMAL_IRQ_LOW_TO_HIGH = 0,\n\tAMDGPU_THERMAL_IRQ_HIGH_TO_LOW,\n\n\tAMDGPU_THERMAL_IRQ_LAST\n};\n\nenum amdgpu_kiq_irq {\n\tAMDGPU_CP_KIQ_IRQ_DRIVER0 = 0,\n\tAMDGPU_CP_KIQ_IRQ_LAST\n};\n#define SRIOV_USEC_TIMEOUT  1200000  \n#define MAX_KIQ_REG_WAIT       5000  \n#define MAX_KIQ_REG_BAILOUT_INTERVAL   5  \n#define MAX_KIQ_REG_TRY 1000\n\nint amdgpu_device_ip_set_clockgating_state(void *dev,\n\t\t\t\t\t   enum amd_ip_block_type block_type,\n\t\t\t\t\t   enum amd_clockgating_state state);\nint amdgpu_device_ip_set_powergating_state(void *dev,\n\t\t\t\t\t   enum amd_ip_block_type block_type,\n\t\t\t\t\t   enum amd_powergating_state state);\nvoid amdgpu_device_ip_get_clockgating_state(struct amdgpu_device *adev,\n\t\t\t\t\t    u64 *flags);\nint amdgpu_device_ip_wait_for_idle(struct amdgpu_device *adev,\n\t\t\t\t   enum amd_ip_block_type block_type);\nbool amdgpu_device_ip_is_idle(struct amdgpu_device *adev,\n\t\t\t      enum amd_ip_block_type block_type);\n\n#define AMDGPU_MAX_IP_NUM 16\n\nstruct amdgpu_ip_block_status {\n\tbool valid;\n\tbool sw;\n\tbool hw;\n\tbool late_initialized;\n\tbool hang;\n};\n\nstruct amdgpu_ip_block_version {\n\tconst enum amd_ip_block_type type;\n\tconst u32 major;\n\tconst u32 minor;\n\tconst u32 rev;\n\tconst struct amd_ip_funcs *funcs;\n};\n\n#define HW_REV(_Major, _Minor, _Rev) \\\n\t((((uint32_t) (_Major)) << 16) | ((uint32_t) (_Minor) << 8) | ((uint32_t) (_Rev)))\n\nstruct amdgpu_ip_block {\n\tstruct amdgpu_ip_block_status status;\n\tconst struct amdgpu_ip_block_version *version;\n};\n\nint amdgpu_device_ip_block_version_cmp(struct amdgpu_device *adev,\n\t\t\t\t       enum amd_ip_block_type type,\n\t\t\t\t       u32 major, u32 minor);\n\nstruct amdgpu_ip_block *\namdgpu_device_ip_get_ip_block(struct amdgpu_device *adev,\n\t\t\t      enum amd_ip_block_type type);\n\nint amdgpu_device_ip_block_add(struct amdgpu_device *adev,\n\t\t\t       const struct amdgpu_ip_block_version *ip_block_version);\n\n \nbool amdgpu_get_bios(struct amdgpu_device *adev);\nbool amdgpu_read_bios(struct amdgpu_device *adev);\nbool amdgpu_soc15_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t     u8 *bios, u32 length_bytes);\n \n\n#define AMDGPU_MAX_PPLL 3\n\nstruct amdgpu_clock {\n\tstruct amdgpu_pll ppll[AMDGPU_MAX_PPLL];\n\tstruct amdgpu_pll spll;\n\tstruct amdgpu_pll mpll;\n\t \n\tuint32_t default_mclk;\n\tuint32_t default_sclk;\n\tuint32_t default_dispclk;\n\tuint32_t current_dispclk;\n\tuint32_t dp_extclk;\n\tuint32_t max_pixel_clock;\n};\n\n \n\nstruct amdgpu_sa_manager {\n\tstruct drm_suballoc_manager\tbase;\n\tstruct amdgpu_bo\t\t*bo;\n\tuint64_t\t\t\tgpu_addr;\n\tvoid\t\t\t\t*cpu_ptr;\n};\n\nint amdgpu_fence_slab_init(void);\nvoid amdgpu_fence_slab_fini(void);\n\n \n\nstruct amdgpu_flip_work {\n\tstruct delayed_work\t\tflip_work;\n\tstruct work_struct\t\tunpin_work;\n\tstruct amdgpu_device\t\t*adev;\n\tint\t\t\t\tcrtc_id;\n\tu32\t\t\t\ttarget_vblank;\n\tuint64_t\t\t\tbase;\n\tstruct drm_pending_vblank_event *event;\n\tstruct amdgpu_bo\t\t*old_abo;\n\tunsigned\t\t\tshared_count;\n\tstruct dma_fence\t\t**shared;\n\tstruct dma_fence_cb\t\tcb;\n\tbool\t\t\t\tasync;\n};\n\n\n \n\nstruct amdgpu_fpriv {\n\tstruct amdgpu_vm\tvm;\n\tstruct amdgpu_bo_va\t*prt_va;\n\tstruct amdgpu_bo_va\t*csa_va;\n\tstruct mutex\t\tbo_list_lock;\n\tstruct idr\t\tbo_list_handles;\n\tstruct amdgpu_ctx_mgr\tctx_mgr;\n\t \n\tuint32_t\t\txcp_id;\n};\n\nint amdgpu_file_to_fpriv(struct file *filp, struct amdgpu_fpriv **fpriv);\n\n \n#define AMDGPU_MAX_WB 1024\t \n\nstruct amdgpu_wb {\n\tstruct amdgpu_bo\t*wb_obj;\n\tvolatile uint32_t\t*wb;\n\tuint64_t\t\tgpu_addr;\n\tu32\t\t\tnum_wb;\t \n\tunsigned long\t\tused[DIV_ROUND_UP(AMDGPU_MAX_WB, BITS_PER_LONG)];\n};\n\nint amdgpu_device_wb_get(struct amdgpu_device *adev, u32 *wb);\nvoid amdgpu_device_wb_free(struct amdgpu_device *adev, u32 wb);\n\n \nint amdgpu_benchmark(struct amdgpu_device *adev, int test_number);\n\n \nstruct amdgpu_allowed_register_entry {\n\tuint32_t reg_offset;\n\tbool grbm_indexed;\n};\n\nenum amd_reset_method {\n\tAMD_RESET_METHOD_NONE = -1,\n\tAMD_RESET_METHOD_LEGACY = 0,\n\tAMD_RESET_METHOD_MODE0,\n\tAMD_RESET_METHOD_MODE1,\n\tAMD_RESET_METHOD_MODE2,\n\tAMD_RESET_METHOD_BACO,\n\tAMD_RESET_METHOD_PCI,\n};\n\nstruct amdgpu_video_codec_info {\n\tu32 codec_type;\n\tu32 max_width;\n\tu32 max_height;\n\tu32 max_pixels_per_frame;\n\tu32 max_level;\n};\n\n#define codec_info_build(type, width, height, level) \\\n\t\t\t .codec_type = type,\\\n\t\t\t .max_width = width,\\\n\t\t\t .max_height = height,\\\n\t\t\t .max_pixels_per_frame = height * width,\\\n\t\t\t .max_level = level,\n\nstruct amdgpu_video_codecs {\n\tconst u32 codec_count;\n\tconst struct amdgpu_video_codec_info *codec_array;\n};\n\n \nstruct amdgpu_asic_funcs {\n\tbool (*read_disabled_bios)(struct amdgpu_device *adev);\n\tbool (*read_bios_from_rom)(struct amdgpu_device *adev,\n\t\t\t\t   u8 *bios, u32 length_bytes);\n\tint (*read_register)(struct amdgpu_device *adev, u32 se_num,\n\t\t\t     u32 sh_num, u32 reg_offset, u32 *value);\n\tvoid (*set_vga_state)(struct amdgpu_device *adev, bool state);\n\tint (*reset)(struct amdgpu_device *adev);\n\tenum amd_reset_method (*reset_method)(struct amdgpu_device *adev);\n\t \n\tu32 (*get_xclk)(struct amdgpu_device *adev);\n\t \n\tint (*set_uvd_clocks)(struct amdgpu_device *adev, u32 vclk, u32 dclk);\n\tint (*set_vce_clocks)(struct amdgpu_device *adev, u32 evclk, u32 ecclk);\n\t \n\tint (*get_pcie_lanes)(struct amdgpu_device *adev);\n\tvoid (*set_pcie_lanes)(struct amdgpu_device *adev, int lanes);\n\t \n\tu32 (*get_config_memsize)(struct amdgpu_device *adev);\n\t \n\tvoid (*flush_hdp)(struct amdgpu_device *adev, struct amdgpu_ring *ring);\n\t \n\tvoid (*invalidate_hdp)(struct amdgpu_device *adev,\n\t\t\t       struct amdgpu_ring *ring);\n\t \n\tbool (*need_full_reset)(struct amdgpu_device *adev);\n\t \n\tvoid (*init_doorbell_index)(struct amdgpu_device *adev);\n\t \n\tvoid (*get_pcie_usage)(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t       uint64_t *count1);\n\t \n\tbool (*need_reset_on_init)(struct amdgpu_device *adev);\n\t \n\tuint64_t (*get_pcie_replay_count)(struct amdgpu_device *adev);\n\t \n\tbool (*supports_baco)(struct amdgpu_device *adev);\n\t \n\tvoid (*pre_asic_init)(struct amdgpu_device *adev);\n\t \n\tint (*update_umd_stable_pstate)(struct amdgpu_device *adev, bool enter);\n\t \n\tint (*query_video_codecs)(struct amdgpu_device *adev, bool encode,\n\t\t\t\t  const struct amdgpu_video_codecs **codecs);\n\t \n\tu64 (*encode_ext_smn_addressing)(int ext_id);\n};\n\n \nint amdgpu_bo_list_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp);\n\nint amdgpu_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);\nint amdgpu_cs_fence_to_handle_ioctl(struct drm_device *dev, void *data,\n\t\t\t\t    struct drm_file *filp);\nint amdgpu_cs_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);\nint amdgpu_cs_wait_fences_ioctl(struct drm_device *dev, void *data,\n\t\t\t\tstruct drm_file *filp);\n\n \nstruct amdgpu_mem_scratch {\n\tstruct amdgpu_bo\t\t*robj;\n\tvolatile uint32_t\t\t*ptr;\n\tu64\t\t\t\tgpu_addr;\n};\n\n \nstruct cgs_device *amdgpu_cgs_create_device(struct amdgpu_device *adev);\nvoid amdgpu_cgs_destroy_device(struct cgs_device *cgs_device);\n\n \ntypedef uint32_t (*amdgpu_rreg_t)(struct amdgpu_device*, uint32_t);\ntypedef void (*amdgpu_wreg_t)(struct amdgpu_device*, uint32_t, uint32_t);\n\ntypedef uint32_t (*amdgpu_rreg_ext_t)(struct amdgpu_device*, uint64_t);\ntypedef void (*amdgpu_wreg_ext_t)(struct amdgpu_device*, uint64_t, uint32_t);\n\ntypedef uint64_t (*amdgpu_rreg64_t)(struct amdgpu_device*, uint32_t);\ntypedef void (*amdgpu_wreg64_t)(struct amdgpu_device*, uint32_t, uint64_t);\n\ntypedef uint32_t (*amdgpu_block_rreg_t)(struct amdgpu_device*, uint32_t, uint32_t);\ntypedef void (*amdgpu_block_wreg_t)(struct amdgpu_device*, uint32_t, uint32_t, uint32_t);\n\nstruct amdgpu_mmio_remap {\n\tu32 reg_offset;\n\tresource_size_t bus_addr;\n};\n\n \nenum amd_hw_ip_block_type {\n\tGC_HWIP = 1,\n\tHDP_HWIP,\n\tSDMA0_HWIP,\n\tSDMA1_HWIP,\n\tSDMA2_HWIP,\n\tSDMA3_HWIP,\n\tSDMA4_HWIP,\n\tSDMA5_HWIP,\n\tSDMA6_HWIP,\n\tSDMA7_HWIP,\n\tLSDMA_HWIP,\n\tMMHUB_HWIP,\n\tATHUB_HWIP,\n\tNBIO_HWIP,\n\tMP0_HWIP,\n\tMP1_HWIP,\n\tUVD_HWIP,\n\tVCN_HWIP = UVD_HWIP,\n\tJPEG_HWIP = VCN_HWIP,\n\tVCN1_HWIP,\n\tVCE_HWIP,\n\tDF_HWIP,\n\tDCE_HWIP,\n\tOSSSYS_HWIP,\n\tSMUIO_HWIP,\n\tPWR_HWIP,\n\tNBIF_HWIP,\n\tTHM_HWIP,\n\tCLK_HWIP,\n\tUMC_HWIP,\n\tRSMU_HWIP,\n\tXGMI_HWIP,\n\tDCI_HWIP,\n\tPCIE_HWIP,\n\tMAX_HWIP\n};\n\n#define HWIP_MAX_INSTANCE\t44\n\n#define HW_ID_MAX\t\t300\n#define IP_VERSION(mj, mn, rv) (((mj) << 16) | ((mn) << 8) | (rv))\n#define IP_VERSION_MAJ(ver) ((ver) >> 16)\n#define IP_VERSION_MIN(ver) (((ver) >> 8) & 0xFF)\n#define IP_VERSION_REV(ver) ((ver) & 0xFF)\n\nstruct amdgpu_ip_map_info {\n\t \n\tuint32_t \t\tdev_inst[MAX_HWIP][HWIP_MAX_INSTANCE];\n\tint8_t (*logical_to_dev_inst)(struct amdgpu_device *adev,\n\t\t\t\t      enum amd_hw_ip_block_type block,\n\t\t\t\t      int8_t inst);\n\tuint32_t (*logical_to_dev_mask)(struct amdgpu_device *adev,\n\t\t\t\t\tenum amd_hw_ip_block_type block,\n\t\t\t\t\tuint32_t mask);\n};\n\nstruct amd_powerplay {\n\tvoid *pp_handle;\n\tconst struct amd_pm_funcs *pp_funcs;\n};\n\nstruct ip_discovery_top;\n\n \n#define ASICID_IS_P20(did, rid)\t\t(((did == 0x67DF) && \\\n\t\t\t\t\t ((rid == 0xE3) || \\\n\t\t\t\t\t  (rid == 0xE4) || \\\n\t\t\t\t\t  (rid == 0xE5) || \\\n\t\t\t\t\t  (rid == 0xE7) || \\\n\t\t\t\t\t  (rid == 0xEF))) || \\\n\t\t\t\t\t ((did == 0x6FDF) && \\\n\t\t\t\t\t ((rid == 0xE7) || \\\n\t\t\t\t\t  (rid == 0xEF) || \\\n\t\t\t\t\t  (rid == 0xFF))))\n\n#define ASICID_IS_P30(did, rid)\t\t((did == 0x67DF) && \\\n\t\t\t\t\t((rid == 0xE1) || \\\n\t\t\t\t\t (rid == 0xF7)))\n\n \n#define ASICID_IS_P21(did, rid)\t\t(((did == 0x67EF) && \\\n\t\t\t\t\t ((rid == 0xE0) || \\\n\t\t\t\t\t  (rid == 0xE5))) || \\\n\t\t\t\t\t ((did == 0x67FF) && \\\n\t\t\t\t\t ((rid == 0xCF) || \\\n\t\t\t\t\t  (rid == 0xEF) || \\\n\t\t\t\t\t  (rid == 0xFF))))\n\n#define ASICID_IS_P31(did, rid)\t\t((did == 0x67EF) && \\\n\t\t\t\t\t((rid == 0xE2)))\n\n \n#define ASICID_IS_P23(did, rid)\t\t(((did == 0x6987) && \\\n\t\t\t\t\t ((rid == 0xC0) || \\\n\t\t\t\t\t  (rid == 0xC1) || \\\n\t\t\t\t\t  (rid == 0xC3) || \\\n\t\t\t\t\t  (rid == 0xC7))) || \\\n\t\t\t\t\t ((did == 0x6981) && \\\n\t\t\t\t\t ((rid == 0x00) || \\\n\t\t\t\t\t  (rid == 0x01) || \\\n\t\t\t\t\t  (rid == 0x10))))\n\nstruct amdgpu_mqd_prop {\n\tuint64_t mqd_gpu_addr;\n\tuint64_t hqd_base_gpu_addr;\n\tuint64_t rptr_gpu_addr;\n\tuint64_t wptr_gpu_addr;\n\tuint32_t queue_size;\n\tbool use_doorbell;\n\tuint32_t doorbell_index;\n\tuint64_t eop_gpu_addr;\n\tuint32_t hqd_pipe_priority;\n\tuint32_t hqd_queue_priority;\n\tbool hqd_active;\n};\n\nstruct amdgpu_mqd {\n\tunsigned mqd_size;\n\tint (*init_mqd)(struct amdgpu_device *adev, void *mqd,\n\t\t\tstruct amdgpu_mqd_prop *p);\n};\n\n#define AMDGPU_RESET_MAGIC_NUM 64\n#define AMDGPU_MAX_DF_PERFMONS 4\n#define AMDGPU_PRODUCT_NAME_LEN 64\nstruct amdgpu_reset_domain;\n\n \n#define AMDGPU_HAS_VRAM(_adev) ((_adev)->gmc.real_vram_size)\n\nstruct amdgpu_device {\n\tstruct device\t\t\t*dev;\n\tstruct pci_dev\t\t\t*pdev;\n\tstruct drm_device\t\tddev;\n\n#ifdef CONFIG_DRM_AMD_ACP\n\tstruct amdgpu_acp\t\tacp;\n#endif\n\tstruct amdgpu_hive_info *hive;\n\tstruct amdgpu_xcp_mgr *xcp_mgr;\n\t \n\tenum amd_asic_type\t\tasic_type;\n\tuint32_t\t\t\tfamily;\n\tuint32_t\t\t\trev_id;\n\tuint32_t\t\t\texternal_rev_id;\n\tunsigned long\t\t\tflags;\n\tunsigned long\t\t\tapu_flags;\n\tint\t\t\t\tusec_timeout;\n\tconst struct amdgpu_asic_funcs\t*asic_funcs;\n\tbool\t\t\t\tshutdown;\n\tbool\t\t\t\tneed_swiotlb;\n\tbool\t\t\t\taccel_working;\n\tstruct notifier_block\t\tacpi_nb;\n\tstruct amdgpu_i2c_chan\t\t*i2c_bus[AMDGPU_MAX_I2C_BUS];\n\tstruct debugfs_blob_wrapper     debugfs_vbios_blob;\n\tstruct debugfs_blob_wrapper     debugfs_discovery_blob;\n\tstruct mutex\t\t\tsrbm_mutex;\n\t \n\tstruct mutex                    grbm_idx_mutex;\n\tstruct dev_pm_domain\t\tvga_pm_domain;\n\tbool\t\t\t\thave_disp_power_ref;\n\tbool                            have_atomics_support;\n\n\t \n\tbool\t\t\t\tis_atom_fw;\n\tuint8_t\t\t\t\t*bios;\n\tuint32_t\t\t\tbios_size;\n\tuint32_t\t\t\tbios_scratch_reg_offset;\n\tuint32_t\t\t\tbios_scratch[AMDGPU_BIOS_NUM_SCRATCH];\n\n\t \n\tresource_size_t\t\t\trmmio_base;\n\tresource_size_t\t\t\trmmio_size;\n\tvoid __iomem\t\t\t*rmmio;\n\t \n\tspinlock_t mmio_idx_lock;\n\tstruct amdgpu_mmio_remap        rmmio_remap;\n\t \n\tspinlock_t smc_idx_lock;\n\tamdgpu_rreg_t\t\t\tsmc_rreg;\n\tamdgpu_wreg_t\t\t\tsmc_wreg;\n\t \n\tspinlock_t pcie_idx_lock;\n\tamdgpu_rreg_t\t\t\tpcie_rreg;\n\tamdgpu_wreg_t\t\t\tpcie_wreg;\n\tamdgpu_rreg_t\t\t\tpciep_rreg;\n\tamdgpu_wreg_t\t\t\tpciep_wreg;\n\tamdgpu_rreg_ext_t\t\tpcie_rreg_ext;\n\tamdgpu_wreg_ext_t\t\tpcie_wreg_ext;\n\tamdgpu_rreg64_t\t\t\tpcie_rreg64;\n\tamdgpu_wreg64_t\t\t\tpcie_wreg64;\n\t \n\tspinlock_t uvd_ctx_idx_lock;\n\tamdgpu_rreg_t\t\t\tuvd_ctx_rreg;\n\tamdgpu_wreg_t\t\t\tuvd_ctx_wreg;\n\t \n\tspinlock_t didt_idx_lock;\n\tamdgpu_rreg_t\t\t\tdidt_rreg;\n\tamdgpu_wreg_t\t\t\tdidt_wreg;\n\t \n\tspinlock_t gc_cac_idx_lock;\n\tamdgpu_rreg_t\t\t\tgc_cac_rreg;\n\tamdgpu_wreg_t\t\t\tgc_cac_wreg;\n\t \n\tspinlock_t se_cac_idx_lock;\n\tamdgpu_rreg_t\t\t\tse_cac_rreg;\n\tamdgpu_wreg_t\t\t\tse_cac_wreg;\n\t \n\tspinlock_t audio_endpt_idx_lock;\n\tamdgpu_block_rreg_t\t\taudio_endpt_rreg;\n\tamdgpu_block_wreg_t\t\taudio_endpt_wreg;\n\tstruct amdgpu_doorbell\t\tdoorbell;\n\n\t \n\tstruct amdgpu_clock            clock;\n\n\t \n\tstruct amdgpu_gmc\t\tgmc;\n\tstruct amdgpu_gart\t\tgart;\n\tdma_addr_t\t\t\tdummy_page_addr;\n\tstruct amdgpu_vm_manager\tvm_manager;\n\tstruct amdgpu_vmhub             vmhub[AMDGPU_MAX_VMHUBS];\n\tDECLARE_BITMAP(vmhubs_mask, AMDGPU_MAX_VMHUBS);\n\n\t \n\tstruct amdgpu_mman\t\tmman;\n\tstruct amdgpu_mem_scratch\tmem_scratch;\n\tstruct amdgpu_wb\t\twb;\n\tatomic64_t\t\t\tnum_bytes_moved;\n\tatomic64_t\t\t\tnum_evictions;\n\tatomic64_t\t\t\tnum_vram_cpu_page_faults;\n\tatomic_t\t\t\tgpu_reset_counter;\n\tatomic_t\t\t\tvram_lost_counter;\n\n\t \n\tstruct {\n\t\tspinlock_t\t\tlock;\n\t\ts64\t\t\tlast_update_us;\n\t\ts64\t\t\taccum_us;  \n\t\ts64\t\t\taccum_us_vis;  \n\t\tu32\t\t\tlog2_max_MBps;\n\t} mm_stats;\n\n\t \n\tbool\t\t\t\tenable_virtual_display;\n\tstruct amdgpu_vkms_output       *amdgpu_vkms_output;\n\tstruct amdgpu_mode_info\t\tmode_info;\n\t \n\tstruct delayed_work         hotplug_work;\n\tstruct amdgpu_irq_src\t\tcrtc_irq;\n\tstruct amdgpu_irq_src\t\tvline0_irq;\n\tstruct amdgpu_irq_src\t\tvupdate_irq;\n\tstruct amdgpu_irq_src\t\tpageflip_irq;\n\tstruct amdgpu_irq_src\t\thpd_irq;\n\tstruct amdgpu_irq_src\t\tdmub_trace_irq;\n\tstruct amdgpu_irq_src\t\tdmub_outbox_irq;\n\n\t \n\tu64\t\t\t\tfence_context;\n\tunsigned\t\t\tnum_rings;\n\tstruct amdgpu_ring\t\t*rings[AMDGPU_MAX_RINGS];\n\tstruct dma_fence __rcu\t\t*gang_submit;\n\tbool\t\t\t\tib_pool_ready;\n\tstruct amdgpu_sa_manager\tib_pools[AMDGPU_IB_POOL_MAX];\n\tstruct amdgpu_sched\t\tgpu_sched[AMDGPU_HW_IP_NUM][AMDGPU_RING_PRIO_MAX];\n\n\t \n\tstruct amdgpu_irq\t\tirq;\n\n\t \n\tstruct amd_powerplay\t\tpowerplay;\n\tstruct amdgpu_pm\t\tpm;\n\tu64\t\t\t\tcg_flags;\n\tu32\t\t\t\tpg_flags;\n\n\t \n\tstruct amdgpu_nbio\t\tnbio;\n\n\t \n\tstruct amdgpu_hdp\t\thdp;\n\n\t \n\tstruct amdgpu_smuio\t\tsmuio;\n\n\t \n\tstruct amdgpu_mmhub\t\tmmhub;\n\n\t \n\tstruct amdgpu_gfxhub\t\tgfxhub;\n\n\t \n\tstruct amdgpu_gfx\t\tgfx;\n\n\t \n\tstruct amdgpu_sdma\t\tsdma;\n\n\t \n\tstruct amdgpu_lsdma\t\tlsdma;\n\n\t \n\tstruct amdgpu_uvd\t\tuvd;\n\n\t \n\tstruct amdgpu_vce\t\tvce;\n\n\t \n\tstruct amdgpu_vcn\t\tvcn;\n\n\t \n\tstruct amdgpu_jpeg\t\tjpeg;\n\n\t \n\tstruct amdgpu_firmware\t\tfirmware;\n\n\t \n\tstruct psp_context\t\tpsp;\n\n\t \n\tstruct amdgpu_gds\t\tgds;\n\n\t \n\tstruct amdgpu_kfd_dev\t\tkfd;\n\n\t \n\tstruct amdgpu_umc\t\tumc;\n\n\t \n\tstruct amdgpu_display_manager dm;\n\n\t \n\tbool                            enable_mes;\n\tbool                            enable_mes_kiq;\n\tstruct amdgpu_mes               mes;\n\tstruct amdgpu_mqd               mqds[AMDGPU_HW_IP_NUM];\n\n\t \n\tstruct amdgpu_df                df;\n\n\t \n\tstruct amdgpu_mca               mca;\n\n\tstruct amdgpu_ip_block          ip_blocks[AMDGPU_MAX_IP_NUM];\n\tuint32_t\t\t        harvest_ip_mask;\n\tint\t\t\t\tnum_ip_blocks;\n\tstruct mutex\tmn_lock;\n\tDECLARE_HASHTABLE(mn_hash, 7);\n\n\t \n\tatomic64_t vram_pin_size;\n\tatomic64_t visible_pin_size;\n\tatomic64_t gart_pin_size;\n\n\t \n\tuint32_t\t\t*reg_offset[MAX_HWIP][HWIP_MAX_INSTANCE];\n\tstruct amdgpu_ip_map_info\tip_map;\n\n\t \n\tstruct delayed_work     delayed_init_work;\n\n\tstruct amdgpu_virt\tvirt;\n\n\t \n\tstruct list_head                shadow_list;\n\tstruct mutex                    shadow_list_lock;\n\n\t \n\tbool has_hw_reset;\n\tu8\t\t\t\treset_magic[AMDGPU_RESET_MAGIC_NUM];\n\n\t \n\tbool                            in_suspend;\n\tbool\t\t\t\tin_s3;\n\tbool\t\t\t\tin_s4;\n\tbool\t\t\t\tin_s0ix;\n\n\tenum pp_mp1_state               mp1_state;\n\tstruct amdgpu_doorbell_index doorbell_index;\n\n\tstruct mutex\t\t\tnotifier_lock;\n\n\tint asic_reset_res;\n\tstruct work_struct\t\txgmi_reset_work;\n\tstruct list_head\t\treset_list;\n\n\tlong\t\t\t\tgfx_timeout;\n\tlong\t\t\t\tsdma_timeout;\n\tlong\t\t\t\tvideo_timeout;\n\tlong\t\t\t\tcompute_timeout;\n\n\tuint64_t\t\t\tunique_id;\n\tuint64_t\tdf_perfmon_config_assign_mask[AMDGPU_MAX_DF_PERFMONS];\n\n\t \n\tbool                            in_runpm;\n\tbool                            has_pr3;\n\n\tbool                            ucode_sysfs_en;\n\n\t \n\tchar\t\t\t\tproduct_number[20];\n\tchar\t\t\t\tproduct_name[AMDGPU_PRODUCT_NAME_LEN];\n\tchar\t\t\t\tserial[20];\n\n\tatomic_t\t\t\tthrottling_logging_enabled;\n\tstruct ratelimit_state\t\tthrottling_logging_rs;\n\tuint32_t                        ras_hw_enabled;\n\tuint32_t                        ras_enabled;\n\n\tbool                            no_hw_access;\n\tstruct pci_saved_state          *pci_state;\n\tpci_channel_state_t\t\tpci_channel_state;\n\n\t \n\tbool\t\t\t\tbarrier_has_auto_waitcnt;\n\n\tstruct amdgpu_reset_control     *reset_cntl;\n\tuint32_t                        ip_versions[MAX_HWIP][HWIP_MAX_INSTANCE];\n\n\tbool\t\t\t\tram_is_direct_mapped;\n\n\tstruct list_head                ras_list;\n\n\tstruct ip_discovery_top         *ip_top;\n\n\tstruct amdgpu_reset_domain\t*reset_domain;\n\n\tstruct mutex\t\t\tbenchmark_mutex;\n\n\t \n\tuint32_t                        *reset_dump_reg_list;\n\tuint32_t\t\t\t*reset_dump_reg_value;\n\tint                             num_regs;\n#ifdef CONFIG_DEV_COREDUMP\n\tstruct amdgpu_task_info         reset_task_info;\n\tbool                            reset_vram_lost;\n\tstruct timespec64               reset_time;\n#endif\n\n\tbool                            scpm_enabled;\n\tuint32_t                        scpm_status;\n\n\tstruct work_struct\t\treset_work;\n\n\tbool                            job_hang;\n\tbool                            dc_enabled;\n\t \n\tuint32_t\t\t\taid_mask;\n};\n\nstatic inline struct amdgpu_device *drm_to_adev(struct drm_device *ddev)\n{\n\treturn container_of(ddev, struct amdgpu_device, ddev);\n}\n\nstatic inline struct drm_device *adev_to_drm(struct amdgpu_device *adev)\n{\n\treturn &adev->ddev;\n}\n\nstatic inline struct amdgpu_device *amdgpu_ttm_adev(struct ttm_device *bdev)\n{\n\treturn container_of(bdev, struct amdgpu_device, mman.bdev);\n}\n\nint amdgpu_device_init(struct amdgpu_device *adev,\n\t\t       uint32_t flags);\nvoid amdgpu_device_fini_hw(struct amdgpu_device *adev);\nvoid amdgpu_device_fini_sw(struct amdgpu_device *adev);\n\nint amdgpu_gpu_wait_for_idle(struct amdgpu_device *adev);\n\nvoid amdgpu_device_mm_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t     void *buf, size_t size, bool write);\nsize_t amdgpu_device_aper_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t\t void *buf, size_t size, bool write);\n\nvoid amdgpu_device_vram_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t       void *buf, size_t size, bool write);\nuint32_t amdgpu_device_wait_on_rreg(struct amdgpu_device *adev,\n\t\t\t    uint32_t inst, uint32_t reg_addr, char reg_name[],\n\t\t\t    uint32_t expected_value, uint32_t mask);\nuint32_t amdgpu_device_rreg(struct amdgpu_device *adev,\n\t\t\t    uint32_t reg, uint32_t acc_flags);\nu32 amdgpu_device_indirect_rreg_ext(struct amdgpu_device *adev,\n\t\t\t\t    u64 reg_addr);\nvoid amdgpu_device_wreg(struct amdgpu_device *adev,\n\t\t\tuint32_t reg, uint32_t v,\n\t\t\tuint32_t acc_flags);\nvoid amdgpu_device_indirect_wreg_ext(struct amdgpu_device *adev,\n\t\t\t\t     u64 reg_addr, u32 reg_data);\nvoid amdgpu_mm_wreg_mmio_rlc(struct amdgpu_device *adev,\n\t\t\t     uint32_t reg, uint32_t v, uint32_t xcc_id);\nvoid amdgpu_mm_wreg8(struct amdgpu_device *adev, uint32_t offset, uint8_t value);\nuint8_t amdgpu_mm_rreg8(struct amdgpu_device *adev, uint32_t offset);\n\nu32 amdgpu_device_indirect_rreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg_addr);\nu64 amdgpu_device_indirect_rreg64(struct amdgpu_device *adev,\n\t\t\t\t  u32 reg_addr);\nvoid amdgpu_device_indirect_wreg(struct amdgpu_device *adev,\n\t\t\t\t u32 reg_addr, u32 reg_data);\nvoid amdgpu_device_indirect_wreg64(struct amdgpu_device *adev,\n\t\t\t\t   u32 reg_addr, u64 reg_data);\nu32 amdgpu_device_get_rev_id(struct amdgpu_device *adev);\nbool amdgpu_device_asic_has_dc_support(enum amd_asic_type asic_type);\nbool amdgpu_device_has_dc_support(struct amdgpu_device *adev);\n\nvoid amdgpu_device_set_sriov_virtual_display(struct amdgpu_device *adev);\n\nint amdgpu_device_pre_asic_reset(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_reset_context *reset_context);\n\nint amdgpu_do_asic_reset(struct list_head *device_list_handle,\n\t\t\t struct amdgpu_reset_context *reset_context);\n\nint emu_soc_asic_init(struct amdgpu_device *adev);\n\n \n#define AMDGPU_REGS_NO_KIQ    (1<<1)\n#define AMDGPU_REGS_RLC\t(1<<2)\n\n#define RREG32_NO_KIQ(reg) amdgpu_device_rreg(adev, (reg), AMDGPU_REGS_NO_KIQ)\n#define WREG32_NO_KIQ(reg, v) amdgpu_device_wreg(adev, (reg), (v), AMDGPU_REGS_NO_KIQ)\n\n#define RREG32_KIQ(reg) amdgpu_kiq_rreg(adev, (reg))\n#define WREG32_KIQ(reg, v) amdgpu_kiq_wreg(adev, (reg), (v))\n\n#define RREG8(reg) amdgpu_mm_rreg8(adev, (reg))\n#define WREG8(reg, v) amdgpu_mm_wreg8(adev, (reg), (v))\n\n#define RREG32(reg) amdgpu_device_rreg(adev, (reg), 0)\n#define DREG32(reg) printk(KERN_INFO \"REGISTER: \" #reg \" : 0x%08X\\n\", amdgpu_device_rreg(adev, (reg), 0))\n#define WREG32(reg, v) amdgpu_device_wreg(adev, (reg), (v), 0)\n#define REG_SET(FIELD, v) (((v) << FIELD##_SHIFT) & FIELD##_MASK)\n#define REG_GET(FIELD, v) (((v) << FIELD##_SHIFT) & FIELD##_MASK)\n#define RREG32_PCIE(reg) adev->pcie_rreg(adev, (reg))\n#define WREG32_PCIE(reg, v) adev->pcie_wreg(adev, (reg), (v))\n#define RREG32_PCIE_PORT(reg) adev->pciep_rreg(adev, (reg))\n#define WREG32_PCIE_PORT(reg, v) adev->pciep_wreg(adev, (reg), (v))\n#define RREG32_PCIE_EXT(reg) adev->pcie_rreg_ext(adev, (reg))\n#define WREG32_PCIE_EXT(reg, v) adev->pcie_wreg_ext(adev, (reg), (v))\n#define RREG64_PCIE(reg) adev->pcie_rreg64(adev, (reg))\n#define WREG64_PCIE(reg, v) adev->pcie_wreg64(adev, (reg), (v))\n#define RREG32_SMC(reg) adev->smc_rreg(adev, (reg))\n#define WREG32_SMC(reg, v) adev->smc_wreg(adev, (reg), (v))\n#define RREG32_UVD_CTX(reg) adev->uvd_ctx_rreg(adev, (reg))\n#define WREG32_UVD_CTX(reg, v) adev->uvd_ctx_wreg(adev, (reg), (v))\n#define RREG32_DIDT(reg) adev->didt_rreg(adev, (reg))\n#define WREG32_DIDT(reg, v) adev->didt_wreg(adev, (reg), (v))\n#define RREG32_GC_CAC(reg) adev->gc_cac_rreg(adev, (reg))\n#define WREG32_GC_CAC(reg, v) adev->gc_cac_wreg(adev, (reg), (v))\n#define RREG32_SE_CAC(reg) adev->se_cac_rreg(adev, (reg))\n#define WREG32_SE_CAC(reg, v) adev->se_cac_wreg(adev, (reg), (v))\n#define RREG32_AUDIO_ENDPT(block, reg) adev->audio_endpt_rreg(adev, (block), (reg))\n#define WREG32_AUDIO_ENDPT(block, reg, v) adev->audio_endpt_wreg(adev, (block), (reg), (v))\n#define WREG32_P(reg, val, mask)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tuint32_t tmp_ = RREG32(reg);\t\t\t\\\n\t\ttmp_ &= (mask);\t\t\t\t\t\\\n\t\ttmp_ |= ((val) & ~(mask));\t\t\t\\\n\t\tWREG32(reg, tmp_);\t\t\t\t\\\n\t} while (0)\n#define WREG32_AND(reg, and) WREG32_P(reg, 0, and)\n#define WREG32_OR(reg, or) WREG32_P(reg, or, ~(or))\n#define WREG32_PLL_P(reg, val, mask)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tuint32_t tmp_ = RREG32_PLL(reg);\t\t\\\n\t\ttmp_ &= (mask);\t\t\t\t\t\\\n\t\ttmp_ |= ((val) & ~(mask));\t\t\t\\\n\t\tWREG32_PLL(reg, tmp_);\t\t\t\t\\\n\t} while (0)\n\n#define WREG32_SMC_P(_Reg, _Val, _Mask)                         \\\n\tdo {                                                    \\\n\t\tu32 tmp = RREG32_SMC(_Reg);                     \\\n\t\ttmp &= (_Mask);                                 \\\n\t\ttmp |= ((_Val) & ~(_Mask));                     \\\n\t\tWREG32_SMC(_Reg, tmp);                          \\\n\t} while (0)\n\n#define DREG32_SYS(sqf, adev, reg) seq_printf((sqf), #reg \" : 0x%08X\\n\", amdgpu_device_rreg((adev), (reg), false))\n\n#define REG_FIELD_SHIFT(reg, field) reg##__##field##__SHIFT\n#define REG_FIELD_MASK(reg, field) reg##__##field##_MASK\n\n#define REG_SET_FIELD(orig_val, reg, field, field_val)\t\t\t\\\n\t(((orig_val) & ~REG_FIELD_MASK(reg, field)) |\t\t\t\\\n\t (REG_FIELD_MASK(reg, field) & ((field_val) << REG_FIELD_SHIFT(reg, field))))\n\n#define REG_GET_FIELD(value, reg, field)\t\t\t\t\\\n\t(((value) & REG_FIELD_MASK(reg, field)) >> REG_FIELD_SHIFT(reg, field))\n\n#define WREG32_FIELD(reg, field, val)\t\\\n\tWREG32(mm##reg, (RREG32(mm##reg) & ~REG_FIELD_MASK(reg, field)) | (val) << REG_FIELD_SHIFT(reg, field))\n\n#define WREG32_FIELD_OFFSET(reg, offset, field, val)\t\\\n\tWREG32(mm##reg + offset, (RREG32(mm##reg + offset) & ~REG_FIELD_MASK(reg, field)) | (val) << REG_FIELD_SHIFT(reg, field))\n\n \n#define RBIOS8(i) (adev->bios[i])\n#define RBIOS16(i) (RBIOS8(i) | (RBIOS8((i)+1) << 8))\n#define RBIOS32(i) ((RBIOS16(i)) | (RBIOS16((i)+2) << 16))\n\n \n#define amdgpu_asic_set_vga_state(adev, state) \\\n    ((adev)->asic_funcs->set_vga_state ? (adev)->asic_funcs->set_vga_state((adev), (state)) : 0)\n#define amdgpu_asic_reset(adev) (adev)->asic_funcs->reset((adev))\n#define amdgpu_asic_reset_method(adev) (adev)->asic_funcs->reset_method((adev))\n#define amdgpu_asic_get_xclk(adev) (adev)->asic_funcs->get_xclk((adev))\n#define amdgpu_asic_set_uvd_clocks(adev, v, d) (adev)->asic_funcs->set_uvd_clocks((adev), (v), (d))\n#define amdgpu_asic_set_vce_clocks(adev, ev, ec) (adev)->asic_funcs->set_vce_clocks((adev), (ev), (ec))\n#define amdgpu_get_pcie_lanes(adev) (adev)->asic_funcs->get_pcie_lanes((adev))\n#define amdgpu_set_pcie_lanes(adev, l) (adev)->asic_funcs->set_pcie_lanes((adev), (l))\n#define amdgpu_asic_get_gpu_clock_counter(adev) (adev)->asic_funcs->get_gpu_clock_counter((adev))\n#define amdgpu_asic_read_disabled_bios(adev) (adev)->asic_funcs->read_disabled_bios((adev))\n#define amdgpu_asic_read_bios_from_rom(adev, b, l) (adev)->asic_funcs->read_bios_from_rom((adev), (b), (l))\n#define amdgpu_asic_read_register(adev, se, sh, offset, v)((adev)->asic_funcs->read_register((adev), (se), (sh), (offset), (v)))\n#define amdgpu_asic_get_config_memsize(adev) (adev)->asic_funcs->get_config_memsize((adev))\n#define amdgpu_asic_flush_hdp(adev, r) \\\n\t((adev)->asic_funcs->flush_hdp ? (adev)->asic_funcs->flush_hdp((adev), (r)) : (adev)->hdp.funcs->flush_hdp((adev), (r)))\n#define amdgpu_asic_invalidate_hdp(adev, r) \\\n\t((adev)->asic_funcs->invalidate_hdp ? (adev)->asic_funcs->invalidate_hdp((adev), (r)) : \\\n\t ((adev)->hdp.funcs->invalidate_hdp ? (adev)->hdp.funcs->invalidate_hdp((adev), (r)) : (void)0))\n#define amdgpu_asic_need_full_reset(adev) (adev)->asic_funcs->need_full_reset((adev))\n#define amdgpu_asic_init_doorbell_index(adev) (adev)->asic_funcs->init_doorbell_index((adev))\n#define amdgpu_asic_get_pcie_usage(adev, cnt0, cnt1) ((adev)->asic_funcs->get_pcie_usage((adev), (cnt0), (cnt1)))\n#define amdgpu_asic_need_reset_on_init(adev) (adev)->asic_funcs->need_reset_on_init((adev))\n#define amdgpu_asic_get_pcie_replay_count(adev) ((adev)->asic_funcs->get_pcie_replay_count((adev)))\n#define amdgpu_asic_supports_baco(adev) (adev)->asic_funcs->supports_baco((adev))\n#define amdgpu_asic_pre_asic_init(adev) (adev)->asic_funcs->pre_asic_init((adev))\n#define amdgpu_asic_update_umd_stable_pstate(adev, enter) \\\n\t((adev)->asic_funcs->update_umd_stable_pstate ? (adev)->asic_funcs->update_umd_stable_pstate((adev), (enter)) : 0)\n#define amdgpu_asic_query_video_codecs(adev, e, c) (adev)->asic_funcs->query_video_codecs((adev), (e), (c))\n\n#define amdgpu_inc_vram_lost(adev) atomic_inc(&((adev)->vram_lost_counter));\n\n#define BIT_MASK_UPPER(i) ((i) >= BITS_PER_LONG ? 0 : ~0UL << (i))\n#define for_each_inst(i, inst_mask)        \\\n\tfor (i = ffs(inst_mask); i-- != 0; \\\n\t     i = ffs(inst_mask & BIT_MASK_UPPER(i + 1)))\n\n#define MIN(X, Y) ((X) < (Y) ? (X) : (Y))\n\n \nbool amdgpu_device_has_job_running(struct amdgpu_device *adev);\nbool amdgpu_device_should_recover_gpu(struct amdgpu_device *adev);\nint amdgpu_device_gpu_recover(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_job *job,\n\t\t\t      struct amdgpu_reset_context *reset_context);\nvoid amdgpu_device_pci_config_reset(struct amdgpu_device *adev);\nint amdgpu_device_pci_reset(struct amdgpu_device *adev);\nbool amdgpu_device_need_post(struct amdgpu_device *adev);\nbool amdgpu_device_pcie_dynamic_switching_supported(void);\nbool amdgpu_device_should_use_aspm(struct amdgpu_device *adev);\nbool amdgpu_device_aspm_support_quirk(void);\n\nvoid amdgpu_cs_report_moved_bytes(struct amdgpu_device *adev, u64 num_bytes,\n\t\t\t\t  u64 num_vis_bytes);\nint amdgpu_device_resize_fb_bar(struct amdgpu_device *adev);\nvoid amdgpu_device_program_register_sequence(struct amdgpu_device *adev,\n\t\t\t\t\t     const u32 *registers,\n\t\t\t\t\t     const u32 array_size);\n\nint amdgpu_device_mode1_reset(struct amdgpu_device *adev);\nbool amdgpu_device_supports_atpx(struct drm_device *dev);\nbool amdgpu_device_supports_px(struct drm_device *dev);\nbool amdgpu_device_supports_boco(struct drm_device *dev);\nbool amdgpu_device_supports_smart_shift(struct drm_device *dev);\nbool amdgpu_device_supports_baco(struct drm_device *dev);\nbool amdgpu_device_is_peer_accessible(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_device *peer_adev);\nint amdgpu_device_baco_enter(struct drm_device *dev);\nint amdgpu_device_baco_exit(struct drm_device *dev);\n\nvoid amdgpu_device_flush_hdp(struct amdgpu_device *adev,\n\t\tstruct amdgpu_ring *ring);\nvoid amdgpu_device_invalidate_hdp(struct amdgpu_device *adev,\n\t\tstruct amdgpu_ring *ring);\n\nvoid amdgpu_device_halt(struct amdgpu_device *adev);\nu32 amdgpu_device_pcie_port_rreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg);\nvoid amdgpu_device_pcie_port_wreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg, u32 v);\nstruct dma_fence *amdgpu_device_switch_gang(struct amdgpu_device *adev,\n\t\t\t\t\t    struct dma_fence *gang);\nbool amdgpu_device_has_display_hardware(struct amdgpu_device *adev);\n\n \n#if defined(CONFIG_VGA_SWITCHEROO)\nvoid amdgpu_register_atpx_handler(void);\nvoid amdgpu_unregister_atpx_handler(void);\nbool amdgpu_has_atpx_dgpu_power_cntl(void);\nbool amdgpu_is_atpx_hybrid(void);\nbool amdgpu_atpx_dgpu_req_power_for_displays(void);\nbool amdgpu_has_atpx(void);\n#else\nstatic inline void amdgpu_register_atpx_handler(void) {}\nstatic inline void amdgpu_unregister_atpx_handler(void) {}\nstatic inline bool amdgpu_has_atpx_dgpu_power_cntl(void) { return false; }\nstatic inline bool amdgpu_is_atpx_hybrid(void) { return false; }\nstatic inline bool amdgpu_atpx_dgpu_req_power_for_displays(void) { return false; }\nstatic inline bool amdgpu_has_atpx(void) { return false; }\n#endif\n\n#if defined(CONFIG_VGA_SWITCHEROO) && defined(CONFIG_ACPI)\nvoid *amdgpu_atpx_get_dhandle(void);\n#else\nstatic inline void *amdgpu_atpx_get_dhandle(void) { return NULL; }\n#endif\n\n \nextern const struct drm_ioctl_desc amdgpu_ioctls_kms[];\nextern const int amdgpu_max_kms_ioctl;\n\nint amdgpu_driver_load_kms(struct amdgpu_device *adev, unsigned long flags);\nvoid amdgpu_driver_unload_kms(struct drm_device *dev);\nvoid amdgpu_driver_lastclose_kms(struct drm_device *dev);\nint amdgpu_driver_open_kms(struct drm_device *dev, struct drm_file *file_priv);\nvoid amdgpu_driver_postclose_kms(struct drm_device *dev,\n\t\t\t\t struct drm_file *file_priv);\nvoid amdgpu_driver_release_kms(struct drm_device *dev);\n\nint amdgpu_device_ip_suspend(struct amdgpu_device *adev);\nint amdgpu_device_suspend(struct drm_device *dev, bool fbcon);\nint amdgpu_device_resume(struct drm_device *dev, bool fbcon);\nu32 amdgpu_get_vblank_counter_kms(struct drm_crtc *crtc);\nint amdgpu_enable_vblank_kms(struct drm_crtc *crtc);\nvoid amdgpu_disable_vblank_kms(struct drm_crtc *crtc);\nint amdgpu_info_ioctl(struct drm_device *dev, void *data,\n\t\t      struct drm_file *filp);\n\n \nstruct amdgpu_afmt_acr {\n\tu32 clock;\n\n\tint n_32khz;\n\tint cts_32khz;\n\n\tint n_44_1khz;\n\tint cts_44_1khz;\n\n\tint n_48khz;\n\tint cts_48khz;\n\n};\n\nstruct amdgpu_afmt_acr amdgpu_afmt_acr(uint32_t clock);\n\n \n\nstruct amdgpu_numa_info {\n\tuint64_t size;\n\tint pxm;\n\tint nid;\n};\n\n \n#define AMDGPU_ATCS_PSC_DEV_STATE_D0\t\t0\n#define AMDGPU_ATCS_PSC_DEV_STATE_D3_HOT\t3\n#define AMDGPU_ATCS_PSC_DRV_STATE_OPR\t\t0\n#define AMDGPU_ATCS_PSC_DRV_STATE_NOT_OPR\t1\n\n#if defined(CONFIG_ACPI)\nint amdgpu_acpi_init(struct amdgpu_device *adev);\nvoid amdgpu_acpi_fini(struct amdgpu_device *adev);\nbool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev);\nbool amdgpu_acpi_is_power_shift_control_supported(void);\nint amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev,\n\t\t\t\t\t\tu8 perf_req, bool advertise);\nint amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t    u8 dev_state, bool drv_state);\nint amdgpu_acpi_smart_shift_update(struct drm_device *dev, enum amdgpu_ss ss_state);\nint amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev);\nint amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev, u64 *tmr_offset,\n\t\t\t     u64 *tmr_size);\nint amdgpu_acpi_get_mem_info(struct amdgpu_device *adev, int xcc_id,\n\t\t\t     struct amdgpu_numa_info *numa_info);\n\nvoid amdgpu_acpi_get_backlight_caps(struct amdgpu_dm_backlight_caps *caps);\nbool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev);\nvoid amdgpu_acpi_detect(void);\nvoid amdgpu_acpi_release(void);\n#else\nstatic inline int amdgpu_acpi_init(struct amdgpu_device *adev) { return 0; }\nstatic inline int amdgpu_acpi_get_tmr_info(struct amdgpu_device *adev,\n\t\t\t\t\t   u64 *tmr_offset, u64 *tmr_size)\n{\n\treturn -EINVAL;\n}\nstatic inline int amdgpu_acpi_get_mem_info(struct amdgpu_device *adev,\n\t\t\t\t\t   int xcc_id,\n\t\t\t\t\t   struct amdgpu_numa_info *numa_info)\n{\n\treturn -EINVAL;\n}\nstatic inline void amdgpu_acpi_fini(struct amdgpu_device *adev) { }\nstatic inline bool amdgpu_acpi_should_gpu_reset(struct amdgpu_device *adev) { return false; }\nstatic inline void amdgpu_acpi_detect(void) { }\nstatic inline void amdgpu_acpi_release(void) { }\nstatic inline bool amdgpu_acpi_is_power_shift_control_supported(void) { return false; }\nstatic inline int amdgpu_acpi_power_shift_control(struct amdgpu_device *adev,\n\t\t\t\t\t\t  u8 dev_state, bool drv_state) { return 0; }\nstatic inline int amdgpu_acpi_smart_shift_update(struct drm_device *dev,\n\t\t\t\t\t\t enum amdgpu_ss ss_state) { return 0; }\n#endif\n\n#if defined(CONFIG_ACPI) && defined(CONFIG_SUSPEND)\nbool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev);\nbool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev);\n#else\nstatic inline bool amdgpu_acpi_is_s0ix_active(struct amdgpu_device *adev) { return false; }\nstatic inline bool amdgpu_acpi_is_s3_active(struct amdgpu_device *adev) { return false; }\n#endif\n\n#if defined(CONFIG_DRM_AMD_DC)\nint amdgpu_dm_display_resume(struct amdgpu_device *adev );\n#else\nstatic inline int amdgpu_dm_display_resume(struct amdgpu_device *adev) { return 0; }\n#endif\n\n\nvoid amdgpu_register_gpu_instance(struct amdgpu_device *adev);\nvoid amdgpu_unregister_gpu_instance(struct amdgpu_device *adev);\n\npci_ers_result_t amdgpu_pci_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t   pci_channel_state_t state);\npci_ers_result_t amdgpu_pci_mmio_enabled(struct pci_dev *pdev);\npci_ers_result_t amdgpu_pci_slot_reset(struct pci_dev *pdev);\nvoid amdgpu_pci_resume(struct pci_dev *pdev);\n\nbool amdgpu_device_cache_pci_state(struct pci_dev *pdev);\nbool amdgpu_device_load_pci_state(struct pci_dev *pdev);\n\nbool amdgpu_device_skip_hw_access(struct amdgpu_device *adev);\n\nint amdgpu_device_set_cg_state(struct amdgpu_device *adev,\n\t\t\t       enum amd_clockgating_state state);\nint amdgpu_device_set_pg_state(struct amdgpu_device *adev,\n\t\t\t       enum amd_powergating_state state);\n\nstatic inline bool amdgpu_device_has_timeouts_enabled(struct amdgpu_device *adev)\n{\n\treturn amdgpu_gpu_recovery != 0 &&\n\t\tadev->gfx_timeout != MAX_SCHEDULE_TIMEOUT &&\n\t\tadev->compute_timeout != MAX_SCHEDULE_TIMEOUT &&\n\t\tadev->sdma_timeout != MAX_SCHEDULE_TIMEOUT &&\n\t\tadev->video_timeout != MAX_SCHEDULE_TIMEOUT;\n}\n\n#include \"amdgpu_object.h\"\n\nstatic inline bool amdgpu_is_tmz(struct amdgpu_device *adev)\n{\n       return adev->gmc.tmz_enabled;\n}\n\nint amdgpu_in_reset(struct amdgpu_device *adev);\n\nextern const struct attribute_group amdgpu_vram_mgr_attr_group;\nextern const struct attribute_group amdgpu_gtt_mgr_attr_group;\nextern const struct attribute_group amdgpu_flash_attr_group;\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}