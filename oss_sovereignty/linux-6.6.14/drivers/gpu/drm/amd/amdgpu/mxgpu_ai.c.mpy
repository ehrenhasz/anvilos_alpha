{
  "module_name": "mxgpu_ai.c",
  "hash_id": "8cf511d889e1f81816d3c6cbe49acf9d3497112d371c6809ceae68e364a29d4f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/mxgpu_ai.c",
  "human_readable_source": " \n\n#include \"amdgpu.h\"\n#include \"nbio/nbio_6_1_offset.h\"\n#include \"nbio/nbio_6_1_sh_mask.h\"\n#include \"gc/gc_9_0_offset.h\"\n#include \"gc/gc_9_0_sh_mask.h\"\n#include \"mp/mp_9_0_offset.h\"\n#include \"soc15.h\"\n#include \"vega10_ih.h\"\n#include \"soc15_common.h\"\n#include \"mxgpu_ai.h\"\n\n#include \"amdgpu_reset.h\"\n\nstatic void xgpu_ai_mailbox_send_ack(struct amdgpu_device *adev)\n{\n\tWREG8(AI_MAIBOX_CONTROL_RCV_OFFSET_BYTE, 2);\n}\n\nstatic void xgpu_ai_mailbox_set_valid(struct amdgpu_device *adev, bool val)\n{\n\tWREG8(AI_MAIBOX_CONTROL_TRN_OFFSET_BYTE, val ? 1 : 0);\n}\n\n \nstatic enum idh_event xgpu_ai_mailbox_peek_msg(struct amdgpu_device *adev)\n{\n\treturn RREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0,\n\t\t\t\tmmBIF_BX_PF0_MAILBOX_MSGBUF_RCV_DW0));\n}\n\n\nstatic int xgpu_ai_mailbox_rcv_msg(struct amdgpu_device *adev,\n\t\t\t\t   enum idh_event event)\n{\n\tu32 reg;\n\n\treg = RREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0,\n\t\t\t\t\t     mmBIF_BX_PF0_MAILBOX_MSGBUF_RCV_DW0));\n\tif (reg != event)\n\t\treturn -ENOENT;\n\n\txgpu_ai_mailbox_send_ack(adev);\n\n\treturn 0;\n}\n\nstatic uint8_t xgpu_ai_peek_ack(struct amdgpu_device *adev) {\n\treturn RREG8(AI_MAIBOX_CONTROL_TRN_OFFSET_BYTE) & 2;\n}\n\nstatic int xgpu_ai_poll_ack(struct amdgpu_device *adev)\n{\n\tint timeout  = AI_MAILBOX_POLL_ACK_TIMEDOUT;\n\tu8 reg;\n\n\tdo {\n\t\treg = RREG8(AI_MAIBOX_CONTROL_TRN_OFFSET_BYTE);\n\t\tif (reg & 2)\n\t\t\treturn 0;\n\n\t\tmdelay(5);\n\t\ttimeout -= 5;\n\t} while (timeout > 1);\n\n\tpr_err(\"Doesn't get TRN_MSG_ACK from pf in %d msec\\n\", AI_MAILBOX_POLL_ACK_TIMEDOUT);\n\n\treturn -ETIME;\n}\n\nstatic int xgpu_ai_poll_msg(struct amdgpu_device *adev, enum idh_event event)\n{\n\tint r, timeout = AI_MAILBOX_POLL_MSG_TIMEDOUT;\n\n\tdo {\n\t\tr = xgpu_ai_mailbox_rcv_msg(adev, event);\n\t\tif (!r)\n\t\t\treturn 0;\n\n\t\tmsleep(10);\n\t\ttimeout -= 10;\n\t} while (timeout > 1);\n\n\tpr_err(\"Doesn't get msg:%d from pf, error=%d\\n\", event, r);\n\n\treturn -ETIME;\n}\n\nstatic void xgpu_ai_mailbox_trans_msg (struct amdgpu_device *adev,\n\t      enum idh_request req, u32 data1, u32 data2, u32 data3) {\n\tu32 reg;\n\tint r;\n\tuint8_t trn;\n\n\t \n\tdo {\n\t\txgpu_ai_mailbox_set_valid(adev, false);\n\t\ttrn = xgpu_ai_peek_ack(adev);\n\t\tif (trn) {\n\t\t\tpr_err(\"trn=%x ACK should not assert! wait again !\\n\", trn);\n\t\t\tmsleep(1);\n\t\t}\n\t} while(trn);\n\n\treg = RREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0,\n\t\t\t\t\t     mmBIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW0));\n\treg = REG_SET_FIELD(reg, BIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW0,\n\t\t\t    MSGBUF_DATA, req);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW0),\n\t\t      reg);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW1),\n\t\t\t\tdata1);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW2),\n\t\t\t\tdata2);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_MSGBUF_TRN_DW3),\n\t\t\t\tdata3);\n\n\txgpu_ai_mailbox_set_valid(adev, true);\n\n\t \n\tr = xgpu_ai_poll_ack(adev);\n\tif (r)\n\t\tpr_err(\"Doesn't get ack from pf, continue\\n\");\n\n\txgpu_ai_mailbox_set_valid(adev, false);\n}\n\nstatic int xgpu_ai_send_access_requests(struct amdgpu_device *adev,\n\t\t\t\t\tenum idh_request req)\n{\n\tint r;\n\n\txgpu_ai_mailbox_trans_msg(adev, req, 0, 0, 0);\n\n\t \n\tif (req == IDH_REQ_GPU_INIT_ACCESS ||\n\t\treq == IDH_REQ_GPU_FINI_ACCESS ||\n\t\treq == IDH_REQ_GPU_RESET_ACCESS) {\n\t\tr = xgpu_ai_poll_msg(adev, IDH_READY_TO_ACCESS_GPU);\n\t\tif (r) {\n\t\t\tpr_err(\"Doesn't get READY_TO_ACCESS_GPU from pf, give up\\n\");\n\t\t\treturn r;\n\t\t}\n\t\t \n\t\tif (req == IDH_REQ_GPU_INIT_ACCESS || req == IDH_REQ_GPU_RESET_ACCESS) {\n\t\t\tadev->virt.fw_reserve.checksum_key =\n\t\t\t\tRREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0,\n\t\t\t\t\tmmBIF_BX_PF0_MAILBOX_MSGBUF_RCV_DW2));\n\t\t}\n\t} else if (req == IDH_REQ_GPU_INIT_DATA){\n\t\t \n\t\tr = xgpu_ai_poll_msg(adev, IDH_REQ_GPU_INIT_DATA_READY);\n\t\t \n\t\tadev->virt.req_init_data_ver = 0;\t\n\t}\n\n\treturn 0;\n}\n\nstatic int xgpu_ai_request_reset(struct amdgpu_device *adev)\n{\n\tint ret, i = 0;\n\n\twhile (i < AI_MAILBOX_POLL_MSG_REP_MAX) {\n\t\tret = xgpu_ai_send_access_requests(adev, IDH_REQ_GPU_RESET_ACCESS);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\ti++;\n\t}\n\n\treturn ret;\n}\n\nstatic int xgpu_ai_request_full_gpu_access(struct amdgpu_device *adev,\n\t\t\t\t\t   bool init)\n{\n\tenum idh_request req;\n\n\treq = init ? IDH_REQ_GPU_INIT_ACCESS : IDH_REQ_GPU_FINI_ACCESS;\n\treturn xgpu_ai_send_access_requests(adev, req);\n}\n\nstatic int xgpu_ai_release_full_gpu_access(struct amdgpu_device *adev,\n\t\t\t\t\t   bool init)\n{\n\tenum idh_request req;\n\tint r = 0;\n\n\treq = init ? IDH_REL_GPU_INIT_ACCESS : IDH_REL_GPU_FINI_ACCESS;\n\tr = xgpu_ai_send_access_requests(adev, req);\n\n\treturn r;\n}\n\nstatic int xgpu_ai_mailbox_ack_irq(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"get ack intr and do nothing.\\n\");\n\treturn 0;\n}\n\nstatic int xgpu_ai_set_mailbox_ack_irq(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 tmp = RREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_INT_CNTL));\n\n\ttmp = REG_SET_FIELD(tmp, BIF_BX_PF0_MAILBOX_INT_CNTL, ACK_INT_EN,\n\t\t\t\t(state == AMDGPU_IRQ_STATE_ENABLE) ? 1 : 0);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_INT_CNTL), tmp);\n\n\treturn 0;\n}\n\nstatic void xgpu_ai_mailbox_flr_work(struct work_struct *work)\n{\n\tstruct amdgpu_virt *virt = container_of(work, struct amdgpu_virt, flr_work);\n\tstruct amdgpu_device *adev = container_of(virt, struct amdgpu_device, virt);\n\tint timeout = AI_MAILBOX_POLL_FLR_TIMEDOUT;\n\n\t \n\tif (atomic_cmpxchg(&adev->reset_domain->in_gpu_reset, 0, 1) != 0)\n\t\treturn;\n\n\tdown_write(&adev->reset_domain->sem);\n\n\tamdgpu_virt_fini_data_exchange(adev);\n\n\txgpu_ai_mailbox_trans_msg(adev, IDH_READY_TO_RESET, 0, 0, 0);\n\n\tdo {\n\t\tif (xgpu_ai_mailbox_peek_msg(adev) == IDH_FLR_NOTIFICATION_CMPL)\n\t\t\tgoto flr_done;\n\n\t\tmsleep(10);\n\t\ttimeout -= 10;\n\t} while (timeout > 1);\n\nflr_done:\n\tatomic_set(&adev->reset_domain->in_gpu_reset, 0);\n\tup_write(&adev->reset_domain->sem);\n\n\t \n\tif (amdgpu_device_should_recover_gpu(adev)\n\t\t&& (!amdgpu_device_has_job_running(adev) ||\n\t\t\tadev->sdma_timeout == MAX_SCHEDULE_TIMEOUT)) {\n\t\tstruct amdgpu_reset_context reset_context;\n\t\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\t\treset_context.method = AMD_RESET_METHOD_NONE;\n\t\treset_context.reset_req_dev = adev;\n\t\tclear_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\t\tamdgpu_device_gpu_recover(adev, NULL, &reset_context);\n\t}\n}\n\nstatic int xgpu_ai_set_mailbox_rcv_irq(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_irq_src *src,\n\t\t\t\t       unsigned type,\n\t\t\t\t       enum amdgpu_interrupt_state state)\n{\n\tu32 tmp = RREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_INT_CNTL));\n\n\ttmp = REG_SET_FIELD(tmp, BIF_BX_PF0_MAILBOX_INT_CNTL, VALID_INT_EN,\n\t\t\t    (state == AMDGPU_IRQ_STATE_ENABLE) ? 1 : 0);\n\tWREG32_NO_KIQ(SOC15_REG_OFFSET(NBIO, 0, mmBIF_BX_PF0_MAILBOX_INT_CNTL), tmp);\n\n\treturn 0;\n}\n\nstatic int xgpu_ai_mailbox_rcv_irq(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_irq_src *source,\n\t\t\t\t   struct amdgpu_iv_entry *entry)\n{\n\tenum idh_event event = xgpu_ai_mailbox_peek_msg(adev);\n\n\tswitch (event) {\n\t\tcase IDH_FLR_NOTIFICATION:\n\t\tif (amdgpu_sriov_runtime(adev) && !amdgpu_in_reset(adev))\n\t\t\tWARN_ONCE(!amdgpu_reset_domain_schedule(adev->reset_domain,\n\t\t\t\t\t\t\t\t&adev->virt.flr_work),\n\t\t\t\t  \"Failed to queue work! at %s\",\n\t\t\t\t  __func__);\n\t\tbreak;\n\t\tcase IDH_QUERY_ALIVE:\n\t\t\txgpu_ai_mailbox_send_ack(adev);\n\t\t\tbreak;\n\t\t \n\t\tcase IDH_CLR_MSG_BUF:\n\t\tcase IDH_FLR_NOTIFICATION_CMPL:\n\t\tcase IDH_READY_TO_ACCESS_GPU:\n\t\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct amdgpu_irq_src_funcs xgpu_ai_mailbox_ack_irq_funcs = {\n\t.set = xgpu_ai_set_mailbox_ack_irq,\n\t.process = xgpu_ai_mailbox_ack_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs xgpu_ai_mailbox_rcv_irq_funcs = {\n\t.set = xgpu_ai_set_mailbox_rcv_irq,\n\t.process = xgpu_ai_mailbox_rcv_irq,\n};\n\nvoid xgpu_ai_mailbox_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->virt.ack_irq.num_types = 1;\n\tadev->virt.ack_irq.funcs = &xgpu_ai_mailbox_ack_irq_funcs;\n\tadev->virt.rcv_irq.num_types = 1;\n\tadev->virt.rcv_irq.funcs = &xgpu_ai_mailbox_rcv_irq_funcs;\n}\n\nint xgpu_ai_mailbox_add_irq_id(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_BIF, 135, &adev->virt.rcv_irq);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_BIF, 138, &adev->virt.ack_irq);\n\tif (r) {\n\t\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nint xgpu_ai_mailbox_get_irq(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = amdgpu_irq_get(adev, &adev->virt.rcv_irq, 0);\n\tif (r)\n\t\treturn r;\n\tr = amdgpu_irq_get(adev, &adev->virt.ack_irq, 0);\n\tif (r) {\n\t\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n\t\treturn r;\n\t}\n\n\tINIT_WORK(&adev->virt.flr_work, xgpu_ai_mailbox_flr_work);\n\n\treturn 0;\n}\n\nvoid xgpu_ai_mailbox_put_irq(struct amdgpu_device *adev)\n{\n\tamdgpu_irq_put(adev, &adev->virt.ack_irq, 0);\n\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n}\n\nstatic int xgpu_ai_request_init_data(struct amdgpu_device *adev)\n{\n\treturn xgpu_ai_send_access_requests(adev, IDH_REQ_GPU_INIT_DATA);\n}\n\nstatic void xgpu_ai_ras_poison_handler(struct amdgpu_device *adev)\n{\n\txgpu_ai_send_access_requests(adev, IDH_RAS_POISON);\n}\n\nconst struct amdgpu_virt_ops xgpu_ai_virt_ops = {\n\t.req_full_gpu\t= xgpu_ai_request_full_gpu_access,\n\t.rel_full_gpu\t= xgpu_ai_release_full_gpu_access,\n\t.reset_gpu = xgpu_ai_request_reset,\n\t.wait_reset = NULL,\n\t.trans_msg = xgpu_ai_mailbox_trans_msg,\n\t.req_init_data  = xgpu_ai_request_init_data,\n\t.ras_poison_handler = xgpu_ai_ras_poison_handler,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}