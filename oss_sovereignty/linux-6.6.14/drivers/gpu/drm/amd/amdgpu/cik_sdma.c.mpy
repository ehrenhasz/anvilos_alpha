{
  "module_name": "cik_sdma.c",
  "hash_id": "b3ceb7d3e7a87aeba6022dcb78e7d8c0b37ab500011d89d6d45cdebb6a266740",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/cik_sdma.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_trace.h\"\n#include \"cikd.h\"\n#include \"cik.h\"\n\n#include \"bif/bif_4_1_d.h\"\n#include \"bif/bif_4_1_sh_mask.h\"\n\n#include \"gca/gfx_7_2_d.h\"\n#include \"gca/gfx_7_2_enum.h\"\n#include \"gca/gfx_7_2_sh_mask.h\"\n\n#include \"gmc/gmc_7_1_d.h\"\n#include \"gmc/gmc_7_1_sh_mask.h\"\n\n#include \"oss/oss_2_0_d.h\"\n#include \"oss/oss_2_0_sh_mask.h\"\n\nstatic const u32 sdma_offsets[SDMA_MAX_INSTANCE] =\n{\n\tSDMA0_REGISTER_OFFSET,\n\tSDMA1_REGISTER_OFFSET\n};\n\nstatic void cik_sdma_set_ring_funcs(struct amdgpu_device *adev);\nstatic void cik_sdma_set_irq_funcs(struct amdgpu_device *adev);\nstatic void cik_sdma_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic int cik_sdma_soft_reset(void *handle);\n\nMODULE_FIRMWARE(\"amdgpu/bonaire_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/bonaire_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/hawaii_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/hawaii_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/kaveri_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/kaveri_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/kabini_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/kabini_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/mullins_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/mullins_sdma1.bin\");\n\nu32 amdgpu_cik_gpu_check_soft_reset(struct amdgpu_device *adev);\n\n\nstatic void cik_sdma_free_microcode(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ucode_release(&adev->sdma.instance[i].fw);\n}\n\n \n\n \nstatic int cik_sdma_init_microcode(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tchar fw_name[30];\n\tint err = 0, i;\n\n\tDRM_DEBUG(\"\\n\");\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_BONAIRE:\n\t\tchip_name = \"bonaire\";\n\t\tbreak;\n\tcase CHIP_HAWAII:\n\t\tchip_name = \"hawaii\";\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\t\tchip_name = \"kaveri\";\n\t\tbreak;\n\tcase CHIP_KABINI:\n\t\tchip_name = \"kabini\";\n\t\tbreak;\n\tcase CHIP_MULLINS:\n\t\tchip_name = \"mullins\";\n\t\tbreak;\n\tdefault: BUG();\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (i == 0)\n\t\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_sdma.bin\", chip_name);\n\t\telse\n\t\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_sdma1.bin\", chip_name);\n\t\terr = amdgpu_ucode_request(adev, &adev->sdma.instance[i].fw, fw_name);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\nout:\n\tif (err) {\n\t\tpr_err(\"cik_sdma: Failed to load firmware \\\"%s\\\"\\n\", fw_name);\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\t\tamdgpu_ucode_release(&adev->sdma.instance[i].fw);\n\t}\n\treturn err;\n}\n\n \nstatic uint64_t cik_sdma_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tu32 rptr;\n\n\trptr = *ring->rptr_cpu_addr;\n\n\treturn (rptr & 0x3fffc) >> 2;\n}\n\n \nstatic uint64_t cik_sdma_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\treturn (RREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[ring->me]) & 0x3fffc) >> 2;\n}\n\n \nstatic void cik_sdma_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[ring->me],\n\t       (ring->wptr << 2) & 0x3fffc);\n}\n\nstatic void cik_sdma_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tint i;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop |\n\t\t\t\t\t  SDMA_NOP_COUNT(count - 1));\n\t\telse\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop);\n}\n\n \nstatic void cik_sdma_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t  struct amdgpu_job *job,\n\t\t\t\t  struct amdgpu_ib *ib,\n\t\t\t\t  uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\tu32 extra_bits = vmid & 0xf;\n\n\t \n\tcik_sdma_ring_insert_nop(ring, (4 - lower_32_bits(ring->wptr)) & 7);\n\n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_INDIRECT_BUFFER, 0, extra_bits));\n\tamdgpu_ring_write(ring, ib->gpu_addr & 0xffffffe0);  \n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr) & 0xffffffff);\n\tamdgpu_ring_write(ring, ib->length_dw);\n\n}\n\n \nstatic void cik_sdma_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tu32 extra_bits = (SDMA_POLL_REG_MEM_EXTRA_OP(1) |\n\t\t\t  SDMA_POLL_REG_MEM_EXTRA_FUNC(3));  \n\tu32 ref_and_mask;\n\n\tif (ring->me == 0)\n\t\tref_and_mask = GPU_HDP_FLUSH_DONE__SDMA0_MASK;\n\telse\n\t\tref_and_mask = GPU_HDP_FLUSH_DONE__SDMA1_MASK;\n\n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_POLL_REG_MEM, 0, extra_bits));\n\tamdgpu_ring_write(ring, mmGPU_HDP_FLUSH_DONE << 2);\n\tamdgpu_ring_write(ring, mmGPU_HDP_FLUSH_REQ << 2);\n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, (0xfff << 16) | 10);  \n}\n\n \nstatic void cik_sdma_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t     unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_FENCE, 0, 0));\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_FENCE, 0, 0));\n\t\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_TRAP, 0, 0));\n}\n\n \nstatic void cik_sdma_gfx_stop(struct amdgpu_device *adev)\n{\n\tu32 rb_cntl;\n\tint i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\trb_cntl = RREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i]);\n\t\trb_cntl &= ~SDMA0_GFX_RB_CNTL__RB_ENABLE_MASK;\n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\n\t\tWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], 0);\n\t}\n}\n\n \nstatic void cik_sdma_rlc_stop(struct amdgpu_device *adev)\n{\n\t \n}\n\n \nstatic void cik_ctx_switch_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl, phase_quantum = 0;\n\tint i;\n\n\tif (amdgpu_sdma_phase_quantum) {\n\t\tunsigned value = amdgpu_sdma_phase_quantum;\n\t\tunsigned unit = 0;\n\n\t\twhile (value > (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\tSDMA0_PHASE0_QUANTUM__VALUE__SHIFT)) {\n\t\t\tvalue = (value + 1) >> 1;\n\t\t\tunit++;\n\t\t}\n\t\tif (unit > (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t    SDMA0_PHASE0_QUANTUM__UNIT__SHIFT)) {\n\t\t\tvalue = (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\t SDMA0_PHASE0_QUANTUM__VALUE__SHIFT);\n\t\t\tunit = (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t\tSDMA0_PHASE0_QUANTUM__UNIT__SHIFT);\n\t\t\tWARN_ONCE(1,\n\t\t\t\"clamping sdma_phase_quantum to %uK clock cycles\\n\",\n\t\t\t\t  value << unit);\n\t\t}\n\t\tphase_quantum =\n\t\t\tvalue << SDMA0_PHASE0_QUANTUM__VALUE__SHIFT |\n\t\t\tunit  << SDMA0_PHASE0_QUANTUM__UNIT__SHIFT;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tf32_cntl = RREG32(mmSDMA0_CNTL + sdma_offsets[i]);\n\t\tif (enable) {\n\t\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,\n\t\t\t\t\tAUTO_CTXSW_ENABLE, 1);\n\t\t\tif (amdgpu_sdma_phase_quantum) {\n\t\t\t\tWREG32(mmSDMA0_PHASE0_QUANTUM + sdma_offsets[i],\n\t\t\t\t       phase_quantum);\n\t\t\t\tWREG32(mmSDMA0_PHASE1_QUANTUM + sdma_offsets[i],\n\t\t\t\t       phase_quantum);\n\t\t\t}\n\t\t} else {\n\t\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,\n\t\t\t\t\tAUTO_CTXSW_ENABLE, 0);\n\t\t}\n\n\t\tWREG32(mmSDMA0_CNTL + sdma_offsets[i], f32_cntl);\n\t}\n}\n\n \nstatic void cik_sdma_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 me_cntl;\n\tint i;\n\n\tif (!enable) {\n\t\tcik_sdma_gfx_stop(adev);\n\t\tcik_sdma_rlc_stop(adev);\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tme_cntl = RREG32(mmSDMA0_F32_CNTL + sdma_offsets[i]);\n\t\tif (enable)\n\t\t\tme_cntl &= ~SDMA0_F32_CNTL__HALT_MASK;\n\t\telse\n\t\t\tme_cntl |= SDMA0_F32_CNTL__HALT_MASK;\n\t\tWREG32(mmSDMA0_F32_CNTL + sdma_offsets[i], me_cntl);\n\t}\n}\n\n \nstatic int cik_sdma_gfx_resume(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tu32 rb_cntl, ib_cntl;\n\tu32 rb_bufsz;\n\tint i, j, r;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tfor (j = 0; j < 16; j++) {\n\t\t\tcik_srbm_select(adev, 0, 0, 0, j);\n\t\t\t \n\t\t\tWREG32(mmSDMA0_GFX_VIRTUAL_ADDR + sdma_offsets[i], 0);\n\t\t\tWREG32(mmSDMA0_GFX_APE1_CNTL + sdma_offsets[i], 0);\n\t\t\t \n\t\t}\n\t\tcik_srbm_select(adev, 0, 0, 0, 0);\n\t\tmutex_unlock(&adev->srbm_mutex);\n\n\t\tWREG32(mmSDMA0_TILING_CONFIG + sdma_offsets[i],\n\t\t       adev->gfx.config.gb_addr_config & 0x70);\n\n\t\tWREG32(mmSDMA0_SEM_INCOMPLETE_TIMER_CNTL + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_SEM_WAIT_FAIL_TIMER_CNTL + sdma_offsets[i], 0);\n\n\t\t \n\t\trb_bufsz = order_base_2(ring->ring_size / 4);\n\t\trb_cntl = rb_bufsz << 1;\n#ifdef __BIG_ENDIAN\n\t\trb_cntl |= SDMA0_GFX_RB_CNTL__RB_SWAP_ENABLE_MASK |\n\t\t\tSDMA0_GFX_RB_CNTL__RPTR_WRITEBACK_SWAP_ENABLE_MASK;\n#endif\n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\n\n\t\t \n\t\tWREG32(mmSDMA0_GFX_RB_RPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_IB_RPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_IB_OFFSET + sdma_offsets[i], 0);\n\n\t\t \n\t\tWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_HI + sdma_offsets[i],\n\t\t       upper_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFF);\n\t\tWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_LO + sdma_offsets[i],\n\t\t       ((ring->rptr_gpu_addr) & 0xFFFFFFFC));\n\n\t\trb_cntl |= SDMA0_GFX_RB_CNTL__RPTR_WRITEBACK_ENABLE_MASK;\n\n\t\tWREG32(mmSDMA0_GFX_RB_BASE + sdma_offsets[i], ring->gpu_addr >> 8);\n\t\tWREG32(mmSDMA0_GFX_RB_BASE_HI + sdma_offsets[i], ring->gpu_addr >> 40);\n\n\t\tring->wptr = 0;\n\t\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], ring->wptr << 2);\n\n\t\t \n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i],\n\t\t       rb_cntl | SDMA0_GFX_RB_CNTL__RB_ENABLE_MASK);\n\n\t\tib_cntl = SDMA0_GFX_IB_CNTL__IB_ENABLE_MASK;\n#ifdef __BIG_ENDIAN\n\t\tib_cntl |= SDMA0_GFX_IB_CNTL__IB_SWAP_ENABLE_MASK;\n#endif\n\t\t \n\t\tWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], ib_cntl);\n\t}\n\n\tcik_sdma_enable(adev, true);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int cik_sdma_rlc_resume(struct amdgpu_device *adev)\n{\n\t \n\treturn 0;\n}\n\n \nstatic int cik_sdma_load_microcode(struct amdgpu_device *adev)\n{\n\tconst struct sdma_firmware_header_v1_0 *hdr;\n\tconst __le32 *fw_data;\n\tu32 fw_size;\n\tint i, j;\n\n\t \n\tcik_sdma_enable(adev, false);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (!adev->sdma.instance[i].fw)\n\t\t\treturn -EINVAL;\n\t\thdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\n\t\tamdgpu_ucode_print_sdma_hdr(&hdr->header);\n\t\tfw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\t\tadev->sdma.instance[i].fw_version = le32_to_cpu(hdr->header.ucode_version);\n\t\tadev->sdma.instance[i].feature_version = le32_to_cpu(hdr->ucode_feature_version);\n\t\tif (adev->sdma.instance[i].feature_version >= 20)\n\t\t\tadev->sdma.instance[i].burst_nop = true;\n\t\tfw_data = (const __le32 *)\n\t\t\t(adev->sdma.instance[i].fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\t\tWREG32(mmSDMA0_UCODE_ADDR + sdma_offsets[i], 0);\n\t\tfor (j = 0; j < fw_size; j++)\n\t\t\tWREG32(mmSDMA0_UCODE_DATA + sdma_offsets[i], le32_to_cpup(fw_data++));\n\t\tWREG32(mmSDMA0_UCODE_ADDR + sdma_offsets[i], adev->sdma.instance[i].fw_version);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int cik_sdma_start(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = cik_sdma_load_microcode(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tcik_sdma_enable(adev, false);\n\t \n\tcik_ctx_switch_enable(adev, true);\n\n\t \n\tr = cik_sdma_gfx_resume(adev);\n\tif (r)\n\t\treturn r;\n\tr = cik_sdma_rlc_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n \nstatic int cik_sdma_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\tr = amdgpu_ring_alloc(ring, 5);\n\tif (r)\n\t\tgoto error_free_wb;\n\n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_WRITE, SDMA_WRITE_SUB_OPCODE_LINEAR, 0));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, 1);  \n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\nerror_free_wb:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic int cik_sdma_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\tlong r;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, 256,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err0;\n\n\tib.ptr[0] = SDMA_PACKET(SDMA_OPCODE_WRITE,\n\t\t\t\tSDMA_WRITE_SUB_OPCODE_LINEAR, 0);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr);\n\tib.ptr[3] = 1;\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.length_dw = 5;\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tgoto err1;\n\t}\n\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic void cik_sdma_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t\t uint64_t pe, uint64_t src,\n\t\t\t\t unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = SDMA_PACKET(SDMA_OPCODE_COPY,\n\t\tSDMA_WRITE_SUB_OPCODE_LINEAR, 0);\n\tib->ptr[ib->length_dw++] = bytes;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n}\n\n \nstatic void cik_sdma_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t  uint64_t value, unsigned count,\n\t\t\t\t  uint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = SDMA_PACKET(SDMA_OPCODE_WRITE,\n\t\tSDMA_WRITE_SUB_OPCODE_LINEAR, 0);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = ndw;\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void cik_sdma_vm_set_pte_pde(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t    uint64_t addr, unsigned count,\n\t\t\t\t    uint32_t incr, uint64_t flags)\n{\n\t \n\tib->ptr[ib->length_dw++] = SDMA_PACKET(SDMA_OPCODE_GENERATE_PTE_PDE, 0, 0);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\tib->ptr[ib->length_dw++] = lower_32_bits(addr);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = incr;  \n\tib->ptr[ib->length_dw++] = 0;\n\tib->ptr[ib->length_dw++] = count;  \n}\n\n \nstatic void cik_sdma_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tu32 pad_count;\n\tint i;\n\n\tpad_count = (-ib->length_dw) & 7;\n\tfor (i = 0; i < pad_count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\t\tSDMA_PACKET(SDMA_OPCODE_NOP, 0, 0) |\n\t\t\t\t\tSDMA_NOP_COUNT(pad_count - 1);\n\t\telse\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\t\tSDMA_PACKET(SDMA_OPCODE_NOP, 0, 0);\n}\n\n \nstatic void cik_sdma_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_POLL_REG_MEM, 0,\n\t\t\t\t\t    SDMA_POLL_REG_MEM_EXTRA_OP(0) |\n\t\t\t\t\t    SDMA_POLL_REG_MEM_EXTRA_FUNC(3) |  \n\t\t\t\t\t    SDMA_POLL_REG_MEM_EXTRA_M));\n\tamdgpu_ring_write(ring, addr & 0xfffffffc);\n\tamdgpu_ring_write(ring, upper_32_bits(addr) & 0xffffffff);\n\tamdgpu_ring_write(ring, seq);  \n\tamdgpu_ring_write(ring, 0xffffffff);  \n\tamdgpu_ring_write(ring, (0xfff << 16) | 4);  \n}\n\n \nstatic void cik_sdma_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\tunsigned vmid, uint64_t pd_addr)\n{\n\tu32 extra_bits = (SDMA_POLL_REG_MEM_EXTRA_OP(0) |\n\t\t\t  SDMA_POLL_REG_MEM_EXTRA_FUNC(0));  \n\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_POLL_REG_MEM, 0, extra_bits));\n\tamdgpu_ring_write(ring, mmVM_INVALIDATE_REQUEST << 2);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, (0xfff << 16) | 10);  \n}\n\nstatic void cik_sdma_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t    uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, SDMA_PACKET(SDMA_OPCODE_SRBM_WRITE, 0, 0xf000));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void cik_enable_sdma_mgcg(struct amdgpu_device *adev,\n\t\t\t\t bool enable)\n{\n\tu32 orig, data;\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tWREG32(mmSDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET, 0x00000100);\n\t\tWREG32(mmSDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET, 0x00000100);\n\t} else {\n\t\torig = data = RREG32(mmSDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET);\n\t\tdata |= 0xff000000;\n\t\tif (data != orig)\n\t\t\tWREG32(mmSDMA0_CLK_CTRL + SDMA0_REGISTER_OFFSET, data);\n\n\t\torig = data = RREG32(mmSDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET);\n\t\tdata |= 0xff000000;\n\t\tif (data != orig)\n\t\t\tWREG32(mmSDMA0_CLK_CTRL + SDMA1_REGISTER_OFFSET, data);\n\t}\n}\n\nstatic void cik_enable_sdma_mgls(struct amdgpu_device *adev,\n\t\t\t\t bool enable)\n{\n\tu32 orig, data;\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS)) {\n\t\torig = data = RREG32(mmSDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET);\n\t\tdata |= 0x100;\n\t\tif (orig != data)\n\t\t\tWREG32(mmSDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET, data);\n\n\t\torig = data = RREG32(mmSDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET);\n\t\tdata |= 0x100;\n\t\tif (orig != data)\n\t\t\tWREG32(mmSDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET, data);\n\t} else {\n\t\torig = data = RREG32(mmSDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET);\n\t\tdata &= ~0x100;\n\t\tif (orig != data)\n\t\t\tWREG32(mmSDMA0_POWER_CNTL + SDMA0_REGISTER_OFFSET, data);\n\n\t\torig = data = RREG32(mmSDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET);\n\t\tdata &= ~0x100;\n\t\tif (orig != data)\n\t\t\tWREG32(mmSDMA0_POWER_CNTL + SDMA1_REGISTER_OFFSET, data);\n\t}\n}\n\nstatic int cik_sdma_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->sdma.num_instances = SDMA_MAX_INSTANCE;\n\n\tcik_sdma_set_ring_funcs(adev);\n\tcik_sdma_set_irq_funcs(adev);\n\tcik_sdma_set_buffer_funcs(adev);\n\tcik_sdma_set_vm_pte_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int cik_sdma_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r, i;\n\n\tr = cik_sdma_init_microcode(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load sdma firmware!\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 224,\n\t\t\t      &adev->sdma.trap_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 241,\n\t\t\t      &adev->sdma.illegal_inst_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 247,\n\t\t\t      &adev->sdma.illegal_inst_irq);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tsprintf(ring->name, \"sdma%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 1024,\n\t\t\t\t     &adev->sdma.trap_irq,\n\t\t\t\t     (i == 0) ? AMDGPU_SDMA_IRQ_INSTANCE0 :\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE1,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn r;\n}\n\nstatic int cik_sdma_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\n\tcik_sdma_free_microcode(adev);\n\treturn 0;\n}\n\nstatic int cik_sdma_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = cik_sdma_start(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn r;\n}\n\nstatic int cik_sdma_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcik_ctx_switch_enable(adev, false);\n\tcik_sdma_enable(adev, false);\n\n\treturn 0;\n}\n\nstatic int cik_sdma_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn cik_sdma_hw_fini(adev);\n}\n\nstatic int cik_sdma_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcik_sdma_soft_reset(handle);\n\n\treturn cik_sdma_hw_init(adev);\n}\n\nstatic bool cik_sdma_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(mmSRBM_STATUS2);\n\n\tif (tmp & (SRBM_STATUS2__SDMA_BUSY_MASK |\n\t\t\t\tSRBM_STATUS2__SDMA1_BUSY_MASK))\n\t    return false;\n\n\treturn true;\n}\n\nstatic int cik_sdma_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tu32 tmp;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(mmSRBM_STATUS2) & (SRBM_STATUS2__SDMA_BUSY_MASK |\n\t\t\t\tSRBM_STATUS2__SDMA1_BUSY_MASK);\n\n\t\tif (!tmp)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int cik_sdma_soft_reset(void *handle)\n{\n\tu32 srbm_soft_reset = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp;\n\n\t \n\ttmp = RREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET);\n\ttmp |= SDMA0_F32_CNTL__HALT_MASK;\n\tWREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET, tmp);\n\tsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA_MASK;\n\n\t \n\ttmp = RREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET);\n\ttmp |= SDMA0_F32_CNTL__HALT_MASK;\n\tWREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET, tmp);\n\tsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA1_MASK;\n\n\tif (srbm_soft_reset) {\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int cik_sdma_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_irq_src *src,\n\t\t\t\t       unsigned type,\n\t\t\t\t       enum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tswitch (type) {\n\tcase AMDGPU_SDMA_IRQ_INSTANCE0:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl &= ~SDMA0_CNTL__TRAP_ENABLE_MASK;\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl |= SDMA0_CNTL__TRAP_ENABLE_MASK;\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase AMDGPU_SDMA_IRQ_INSTANCE1:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl &= ~SDMA0_CNTL__TRAP_ENABLE_MASK;\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl |= SDMA0_CNTL__TRAP_ENABLE_MASK;\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int cik_sdma_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_irq_src *source,\n\t\t\t\t     struct amdgpu_iv_entry *entry)\n{\n\tu8 instance_id, queue_id;\n\n\tinstance_id = (entry->ring_id & 0x3) >> 0;\n\tqueue_id = (entry->ring_id & 0xc) >> 2;\n\tDRM_DEBUG(\"IH: SDMA trap\\n\");\n\tswitch (instance_id) {\n\tcase 0:\n\t\tswitch (queue_id) {\n\t\tcase 0:\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[0].ring);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 1:\n\t\tswitch (queue_id) {\n\t\tcase 0:\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[1].ring);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int cik_sdma_process_illegal_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t\t     struct amdgpu_irq_src *source,\n\t\t\t\t\t     struct amdgpu_iv_entry *entry)\n{\n\tu8 instance_id;\n\n\tDRM_ERROR(\"Illegal instruction in SDMA command stream\\n\");\n\tinstance_id = (entry->ring_id & 0x3) >> 0;\n\tdrm_sched_fault(&adev->sdma.instance[instance_id].ring.sched);\n\treturn 0;\n}\n\nstatic int cik_sdma_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tbool gate = false;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (state == AMD_CG_STATE_GATE)\n\t\tgate = true;\n\n\tcik_enable_sdma_mgcg(adev, gate);\n\tcik_enable_sdma_mgls(adev, gate);\n\n\treturn 0;\n}\n\nstatic int cik_sdma_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs cik_sdma_ip_funcs = {\n\t.name = \"cik_sdma\",\n\t.early_init = cik_sdma_early_init,\n\t.late_init = NULL,\n\t.sw_init = cik_sdma_sw_init,\n\t.sw_fini = cik_sdma_sw_fini,\n\t.hw_init = cik_sdma_hw_init,\n\t.hw_fini = cik_sdma_hw_fini,\n\t.suspend = cik_sdma_suspend,\n\t.resume = cik_sdma_resume,\n\t.is_idle = cik_sdma_is_idle,\n\t.wait_for_idle = cik_sdma_wait_for_idle,\n\t.soft_reset = cik_sdma_soft_reset,\n\t.set_clockgating_state = cik_sdma_set_clockgating_state,\n\t.set_powergating_state = cik_sdma_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs cik_sdma_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xf,\n\t.nop = SDMA_PACKET(SDMA_OPCODE_NOP, 0, 0),\n\t.support_64bit_ptrs = false,\n\t.get_rptr = cik_sdma_ring_get_rptr,\n\t.get_wptr = cik_sdma_ring_get_wptr,\n\t.set_wptr = cik_sdma_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\tCIK_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 +  \n\t\t9 + 9 + 9,  \n\t.emit_ib_size = 7 + 4,  \n\t.emit_ib = cik_sdma_ring_emit_ib,\n\t.emit_fence = cik_sdma_ring_emit_fence,\n\t.emit_pipeline_sync = cik_sdma_ring_emit_pipeline_sync,\n\t.emit_vm_flush = cik_sdma_ring_emit_vm_flush,\n\t.emit_hdp_flush = cik_sdma_ring_emit_hdp_flush,\n\t.test_ring = cik_sdma_ring_test_ring,\n\t.test_ib = cik_sdma_ring_test_ib,\n\t.insert_nop = cik_sdma_ring_insert_nop,\n\t.pad_ib = cik_sdma_ring_pad_ib,\n\t.emit_wreg = cik_sdma_ring_emit_wreg,\n};\n\nstatic void cik_sdma_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->sdma.instance[i].ring.funcs = &cik_sdma_ring_funcs;\n\t\tadev->sdma.instance[i].ring.me = i;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs cik_sdma_trap_irq_funcs = {\n\t.set = cik_sdma_set_trap_irq_state,\n\t.process = cik_sdma_process_trap_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs cik_sdma_illegal_inst_irq_funcs = {\n\t.process = cik_sdma_process_illegal_inst_irq,\n};\n\nstatic void cik_sdma_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = AMDGPU_SDMA_IRQ_LAST;\n\tadev->sdma.trap_irq.funcs = &cik_sdma_trap_irq_funcs;\n\tadev->sdma.illegal_inst_irq.funcs = &cik_sdma_illegal_inst_irq_funcs;\n}\n\n \nstatic void cik_sdma_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t      uint64_t src_offset,\n\t\t\t\t      uint64_t dst_offset,\n\t\t\t\t      uint32_t byte_count,\n\t\t\t\t      bool tmz)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PACKET(SDMA_OPCODE_COPY, SDMA_COPY_SUB_OPCODE_LINEAR, 0);\n\tib->ptr[ib->length_dw++] = byte_count;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n}\n\n \nstatic void cik_sdma_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t      uint32_t src_data,\n\t\t\t\t      uint64_t dst_offset,\n\t\t\t\t      uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PACKET(SDMA_OPCODE_CONSTANT_FILL, 0, 0);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = byte_count;\n}\n\nstatic const struct amdgpu_buffer_funcs cik_sdma_buffer_funcs = {\n\t.copy_max_bytes = 0x1fffff,\n\t.copy_num_dw = 7,\n\t.emit_copy_buffer = cik_sdma_emit_copy_buffer,\n\n\t.fill_max_bytes = 0x1fffff,\n\t.fill_num_dw = 5,\n\t.emit_fill_buffer = cik_sdma_emit_fill_buffer,\n};\n\nstatic void cik_sdma_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &cik_sdma_buffer_funcs;\n\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs cik_sdma_vm_pte_funcs = {\n\t.copy_pte_num_dw = 7,\n\t.copy_pte = cik_sdma_vm_copy_pte,\n\n\t.write_pte = cik_sdma_vm_write_pte,\n\t.set_pte_pde = cik_sdma_vm_set_pte_pde,\n};\n\nstatic void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &cik_sdma_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->vm_manager.vm_pte_scheds[i] =\n\t\t\t&adev->sdma.instance[i].ring.sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nconst struct amdgpu_ip_block_version cik_sdma_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 2,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &cik_sdma_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}