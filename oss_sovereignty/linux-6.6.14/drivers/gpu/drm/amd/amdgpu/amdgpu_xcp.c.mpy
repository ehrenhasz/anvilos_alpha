{
  "module_name": "amdgpu_xcp.c",
  "hash_id": "3e0db1ce5f5c810319412f3a5dee410fd1df45df545c5fda4dd16a92debc7c6a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_xcp.c",
  "human_readable_source": " \n#include \"amdgpu.h\"\n#include \"amdgpu_xcp.h\"\n#include \"amdgpu_drv.h\"\n\n#include <drm/drm_drv.h>\n#include \"../amdxcp/amdgpu_xcp_drv.h\"\n\nstatic int __amdgpu_xcp_run(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t    struct amdgpu_xcp_ip *xcp_ip, int xcp_state)\n{\n\tint (*run_func)(void *handle, uint32_t inst_mask);\n\tint ret = 0;\n\n\tif (!xcp_ip || !xcp_ip->valid || !xcp_ip->ip_funcs)\n\t\treturn 0;\n\n\trun_func = NULL;\n\n\tswitch (xcp_state) {\n\tcase AMDGPU_XCP_PREPARE_SUSPEND:\n\t\trun_func = xcp_ip->ip_funcs->prepare_suspend;\n\t\tbreak;\n\tcase AMDGPU_XCP_SUSPEND:\n\t\trun_func = xcp_ip->ip_funcs->suspend;\n\t\tbreak;\n\tcase AMDGPU_XCP_PREPARE_RESUME:\n\t\trun_func = xcp_ip->ip_funcs->prepare_resume;\n\t\tbreak;\n\tcase AMDGPU_XCP_RESUME:\n\t\trun_func = xcp_ip->ip_funcs->resume;\n\t\tbreak;\n\t}\n\n\tif (run_func)\n\t\tret = run_func(xcp_mgr->adev, xcp_ip->inst_mask);\n\n\treturn ret;\n}\n\nstatic int amdgpu_xcp_run_transition(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id,\n\t\t\t\t     int state)\n{\n\tstruct amdgpu_xcp_ip *xcp_ip;\n\tstruct amdgpu_xcp *xcp;\n\tint i, ret;\n\n\tif (xcp_id >= MAX_XCP || !xcp_mgr->xcp[xcp_id].valid)\n\t\treturn -EINVAL;\n\n\txcp = &xcp_mgr->xcp[xcp_id];\n\tfor (i = 0; i < AMDGPU_XCP_MAX_BLOCKS; ++i) {\n\t\txcp_ip = &xcp->ip[i];\n\t\tret = __amdgpu_xcp_run(xcp_mgr, xcp_ip, state);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nint amdgpu_xcp_prepare_suspend(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id)\n{\n\treturn amdgpu_xcp_run_transition(xcp_mgr, xcp_id,\n\t\t\t\t\t AMDGPU_XCP_PREPARE_SUSPEND);\n}\n\nint amdgpu_xcp_suspend(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id)\n{\n\treturn amdgpu_xcp_run_transition(xcp_mgr, xcp_id, AMDGPU_XCP_SUSPEND);\n}\n\nint amdgpu_xcp_prepare_resume(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id)\n{\n\treturn amdgpu_xcp_run_transition(xcp_mgr, xcp_id,\n\t\t\t\t\t AMDGPU_XCP_PREPARE_RESUME);\n}\n\nint amdgpu_xcp_resume(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id)\n{\n\treturn amdgpu_xcp_run_transition(xcp_mgr, xcp_id, AMDGPU_XCP_RESUME);\n}\n\nstatic void __amdgpu_xcp_add_block(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id,\n\t\t\t\t   struct amdgpu_xcp_ip *ip)\n{\n\tstruct amdgpu_xcp *xcp;\n\n\tif (!ip)\n\t\treturn;\n\n\txcp = &xcp_mgr->xcp[xcp_id];\n\txcp->ip[ip->ip_id] = *ip;\n\txcp->ip[ip->ip_id].valid = true;\n\n\txcp->valid = true;\n}\n\nint amdgpu_xcp_init(struct amdgpu_xcp_mgr *xcp_mgr, int num_xcps, int mode)\n{\n\tstruct amdgpu_device *adev = xcp_mgr->adev;\n\tstruct amdgpu_xcp_ip ip;\n\tuint8_t mem_id;\n\tint i, j, ret;\n\n\tif (!num_xcps || num_xcps > MAX_XCP)\n\t\treturn -EINVAL;\n\n\txcp_mgr->mode = mode;\n\n\tfor (i = 0; i < MAX_XCP; ++i)\n\t\txcp_mgr->xcp[i].valid = false;\n\n\t \n\txcp_mgr->num_xcp_per_mem_partition = num_xcps / xcp_mgr->adev->gmc.num_mem_partitions;\n\n\tfor (i = 0; i < num_xcps; ++i) {\n\t\tfor (j = AMDGPU_XCP_GFXHUB; j < AMDGPU_XCP_MAX_BLOCKS; ++j) {\n\t\t\tret = xcp_mgr->funcs->get_ip_details(xcp_mgr, i, j,\n\t\t\t\t\t\t\t     &ip);\n\t\t\tif (ret)\n\t\t\t\tcontinue;\n\n\t\t\t__amdgpu_xcp_add_block(xcp_mgr, i, &ip);\n\t\t}\n\n\t\txcp_mgr->xcp[i].id = i;\n\n\t\tif (xcp_mgr->funcs->get_xcp_mem_id) {\n\t\t\tret = xcp_mgr->funcs->get_xcp_mem_id(\n\t\t\t\txcp_mgr, &xcp_mgr->xcp[i], &mem_id);\n\t\t\tif (ret)\n\t\t\t\tcontinue;\n\t\t\telse\n\t\t\t\txcp_mgr->xcp[i].mem_id = mem_id;\n\t\t}\n\t}\n\n\txcp_mgr->num_xcps = num_xcps;\n\tamdgpu_xcp_update_partition_sched_list(adev);\n\n\treturn 0;\n}\n\nint amdgpu_xcp_switch_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr, int mode)\n{\n\tint ret, curr_mode, num_xcps = 0;\n\n\tif (!xcp_mgr || mode == AMDGPU_XCP_MODE_NONE)\n\t\treturn -EINVAL;\n\n\tif (xcp_mgr->mode == mode)\n\t\treturn 0;\n\n\tif (!xcp_mgr->funcs || !xcp_mgr->funcs->switch_partition_mode)\n\t\treturn 0;\n\n\tmutex_lock(&xcp_mgr->xcp_lock);\n\n\tcurr_mode = xcp_mgr->mode;\n\t \n\txcp_mgr->mode = AMDGPU_XCP_MODE_TRANS;\n\n\tret = xcp_mgr->funcs->switch_partition_mode(xcp_mgr, mode, &num_xcps);\n\n\tif (ret) {\n\t\t \n\t\tif (xcp_mgr->funcs->query_partition_mode)\n\t\t\txcp_mgr->mode = amdgpu_xcp_query_partition_mode(\n\t\t\t\txcp_mgr, AMDGPU_XCP_FL_LOCKED);\n\t\telse\n\t\t\txcp_mgr->mode = curr_mode;\n\n\t\tgoto out;\n\t}\n\nout:\n\tmutex_unlock(&xcp_mgr->xcp_lock);\n\n\treturn ret;\n}\n\nint amdgpu_xcp_query_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr, u32 flags)\n{\n\tint mode;\n\n\tif (xcp_mgr->mode == AMDGPU_XCP_MODE_NONE)\n\t\treturn xcp_mgr->mode;\n\n\tif (!xcp_mgr->funcs || !xcp_mgr->funcs->query_partition_mode)\n\t\treturn xcp_mgr->mode;\n\n\tif (!(flags & AMDGPU_XCP_FL_LOCKED))\n\t\tmutex_lock(&xcp_mgr->xcp_lock);\n\tmode = xcp_mgr->funcs->query_partition_mode(xcp_mgr);\n\tif (xcp_mgr->mode != AMDGPU_XCP_MODE_TRANS && mode != xcp_mgr->mode)\n\t\tdev_WARN(\n\t\t\txcp_mgr->adev->dev,\n\t\t\t\"Cached partition mode %d not matching with device mode %d\",\n\t\t\txcp_mgr->mode, mode);\n\n\tif (!(flags & AMDGPU_XCP_FL_LOCKED))\n\t\tmutex_unlock(&xcp_mgr->xcp_lock);\n\n\treturn mode;\n}\n\nstatic int amdgpu_xcp_dev_alloc(struct amdgpu_device *adev)\n{\n\tstruct drm_device *p_ddev;\n\tstruct drm_device *ddev;\n\tint i, ret;\n\n\tddev = adev_to_drm(adev);\n\n\t \n\tadev->xcp_mgr->xcp->ddev = ddev;\n\n\tfor (i = 1; i < MAX_XCP; i++) {\n\t\tret = amdgpu_xcp_drm_dev_alloc(&p_ddev);\n\t\tif (ret == -ENOSPC) {\n\t\t\tdev_warn(adev->dev,\n\t\t\t\"Skip xcp node #%d when out of drm node resource.\", i);\n\t\t\treturn 0;\n\t\t} else if (ret) {\n\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\tadev->xcp_mgr->xcp[i].rdev = p_ddev->render->dev;\n\t\tadev->xcp_mgr->xcp[i].pdev = p_ddev->primary->dev;\n\t\tadev->xcp_mgr->xcp[i].driver = (struct drm_driver *)p_ddev->driver;\n\t\tadev->xcp_mgr->xcp[i].vma_offset_manager = p_ddev->vma_offset_manager;\n\t\tp_ddev->render->dev = ddev;\n\t\tp_ddev->primary->dev = ddev;\n\t\tp_ddev->vma_offset_manager = ddev->vma_offset_manager;\n\t\tp_ddev->driver = &amdgpu_partition_driver;\n\t\tadev->xcp_mgr->xcp[i].ddev = p_ddev;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_xcp_mgr_init(struct amdgpu_device *adev, int init_mode,\n\t\t\tint init_num_xcps,\n\t\t\tstruct amdgpu_xcp_mgr_funcs *xcp_funcs)\n{\n\tstruct amdgpu_xcp_mgr *xcp_mgr;\n\n\tif (!xcp_funcs || !xcp_funcs->switch_partition_mode ||\n\t    !xcp_funcs->get_ip_details)\n\t\treturn -EINVAL;\n\n\txcp_mgr = kzalloc(sizeof(*xcp_mgr), GFP_KERNEL);\n\n\tif (!xcp_mgr)\n\t\treturn -ENOMEM;\n\n\txcp_mgr->adev = adev;\n\txcp_mgr->funcs = xcp_funcs;\n\txcp_mgr->mode = init_mode;\n\tmutex_init(&xcp_mgr->xcp_lock);\n\n\tif (init_mode != AMDGPU_XCP_MODE_NONE)\n\t\tamdgpu_xcp_init(xcp_mgr, init_num_xcps, init_mode);\n\n\tadev->xcp_mgr = xcp_mgr;\n\n\treturn amdgpu_xcp_dev_alloc(adev);\n}\n\nint amdgpu_xcp_get_partition(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t     enum AMDGPU_XCP_IP_BLOCK ip, int instance)\n{\n\tstruct amdgpu_xcp *xcp;\n\tint i, id_mask = 0;\n\n\tif (ip >= AMDGPU_XCP_MAX_BLOCKS)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < xcp_mgr->num_xcps; ++i) {\n\t\txcp = &xcp_mgr->xcp[i];\n\t\tif ((xcp->valid) && (xcp->ip[ip].valid) &&\n\t\t    (xcp->ip[ip].inst_mask & BIT(instance)))\n\t\t\tid_mask |= BIT(i);\n\t}\n\n\tif (!id_mask)\n\t\tid_mask = -ENXIO;\n\n\treturn id_mask;\n}\n\nint amdgpu_xcp_get_inst_details(struct amdgpu_xcp *xcp,\n\t\t\t\tenum AMDGPU_XCP_IP_BLOCK ip,\n\t\t\t\tuint32_t *inst_mask)\n{\n\tif (!xcp->valid || !inst_mask || !(xcp->ip[ip].valid))\n\t\treturn -EINVAL;\n\n\t*inst_mask = xcp->ip[ip].inst_mask;\n\n\treturn 0;\n}\n\nint amdgpu_xcp_dev_register(struct amdgpu_device *adev,\n\t\t\tconst struct pci_device_id *ent)\n{\n\tint i, ret;\n\n\tif (!adev->xcp_mgr)\n\t\treturn 0;\n\n\tfor (i = 1; i < MAX_XCP; i++) {\n\t\tif (!adev->xcp_mgr->xcp[i].ddev)\n\t\t\tbreak;\n\n\t\tret = drm_dev_register(adev->xcp_mgr->xcp[i].ddev, ent->driver_data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nvoid amdgpu_xcp_dev_unplug(struct amdgpu_device *adev)\n{\n\tstruct drm_device *p_ddev;\n\tint i;\n\n\tif (!adev->xcp_mgr)\n\t\treturn;\n\n\tfor (i = 1; i < MAX_XCP; i++) {\n\t\tif (!adev->xcp_mgr->xcp[i].ddev)\n\t\t\tbreak;\n\n\t\tp_ddev = adev->xcp_mgr->xcp[i].ddev;\n\t\tdrm_dev_unplug(p_ddev);\n\t\tp_ddev->render->dev = adev->xcp_mgr->xcp[i].rdev;\n\t\tp_ddev->primary->dev = adev->xcp_mgr->xcp[i].pdev;\n\t\tp_ddev->driver =  adev->xcp_mgr->xcp[i].driver;\n\t\tp_ddev->vma_offset_manager = adev->xcp_mgr->xcp[i].vma_offset_manager;\n\t}\n}\n\nint amdgpu_xcp_open_device(struct amdgpu_device *adev,\n\t\t\t   struct amdgpu_fpriv *fpriv,\n\t\t\t   struct drm_file *file_priv)\n{\n\tint i;\n\n\tif (!adev->xcp_mgr)\n\t\treturn 0;\n\n\tfpriv->xcp_id = AMDGPU_XCP_NO_PARTITION;\n\tfor (i = 0; i < MAX_XCP; ++i) {\n\t\tif (!adev->xcp_mgr->xcp[i].ddev)\n\t\t\tbreak;\n\n\t\tif (file_priv->minor == adev->xcp_mgr->xcp[i].ddev->render) {\n\t\t\tif (adev->xcp_mgr->xcp[i].valid == FALSE) {\n\t\t\t\tdev_err(adev->dev, \"renderD%d partition %d not valid!\",\n\t\t\t\t\t\tfile_priv->minor->index, i);\n\t\t\t\treturn -ENOENT;\n\t\t\t}\n\t\t\tdev_dbg(adev->dev, \"renderD%d partition %d opened!\",\n\t\t\t\t\tfile_priv->minor->index, i);\n\t\t\tfpriv->xcp_id = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfpriv->vm.mem_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ? -1 :\n\t\t\t\tadev->xcp_mgr->xcp[fpriv->xcp_id].mem_id;\n\treturn 0;\n}\n\nvoid amdgpu_xcp_release_sched(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_ctx_entity *entity)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tstruct amdgpu_ring *ring;\n\n\tif (!adev->xcp_mgr)\n\t\treturn;\n\n\tsched = entity->entity.rq->sched;\n\tif (sched->ready) {\n\t\tring = to_amdgpu_ring(entity->entity.rq->sched);\n\t\tatomic_dec(&adev->xcp_mgr->xcp[ring->xcp_id].ref_cnt);\n\t}\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}