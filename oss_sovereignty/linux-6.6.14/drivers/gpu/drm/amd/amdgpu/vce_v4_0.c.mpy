{
  "module_name": "vce_v4_0.c",
  "hash_id": "eb8c70b946e41d59979b842c4e7e906915091ef81122fee66e4242612a658a2e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/vce_v4_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_vce.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"soc15_common.h\"\n#include \"mmsch_v1_0.h\"\n\n#include \"vce/vce_4_0_offset.h\"\n#include \"vce/vce_4_0_default.h\"\n#include \"vce/vce_4_0_sh_mask.h\"\n#include \"mmhub/mmhub_1_0_offset.h\"\n#include \"mmhub/mmhub_1_0_sh_mask.h\"\n\n#include \"ivsrcid/vce/irqsrcs_vce_4_0.h\"\n\n#define VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK\t0x02\n\n#define VCE_V4_0_FW_SIZE\t(384 * 1024)\n#define VCE_V4_0_STACK_SIZE\t(64 * 1024)\n#define VCE_V4_0_DATA_SIZE\t((16 * 1024 * AMDGPU_MAX_VCE_HANDLES) + (52 * 1024))\n\nstatic void vce_v4_0_mc_resume(struct amdgpu_device *adev);\nstatic void vce_v4_0_set_ring_funcs(struct amdgpu_device *adev);\nstatic void vce_v4_0_set_irq_funcs(struct amdgpu_device *adev);\n\n \nstatic uint64_t vce_v4_0_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->me == 0)\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR));\n\telse if (ring->me == 1)\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR2));\n\telse\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR3));\n}\n\n \nstatic uint64_t vce_v4_0_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell)\n\t\treturn *ring->wptr_cpu_addr;\n\n\tif (ring->me == 0)\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR));\n\telse if (ring->me == 1)\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR2));\n\telse\n\t\treturn RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR3));\n}\n\n \nstatic void vce_v4_0_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\t*ring->wptr_cpu_addr = lower_32_bits(ring->wptr);\n\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t\treturn;\n\t}\n\n\tif (ring->me == 0)\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR),\n\t\t\tlower_32_bits(ring->wptr));\n\telse if (ring->me == 1)\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR2),\n\t\t\tlower_32_bits(ring->wptr));\n\telse\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR3),\n\t\t\tlower_32_bits(ring->wptr));\n}\n\nstatic int vce_v4_0_firmware_loaded(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tuint32_t status =\n\t\t\t\tRREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS));\n\n\t\t\tif (status & VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK)\n\t\t\t\treturn 0;\n\t\t\tmdelay(10);\n\t\t}\n\n\t\tDRM_ERROR(\"VCE not responding, trying to reset the ECPU!!!\\n\");\n\t\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SOFT_RESET),\n\t\t\t\tVCE_SOFT_RESET__ECPU_SOFT_RESET_MASK,\n\t\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SOFT_RESET), 0,\n\t\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int vce_v4_0_mmsch_start(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_mm_table *table)\n{\n\tuint32_t data = 0, loop;\n\tuint64_t addr = table->gpu_addr;\n\tstruct mmsch_v1_0_init_header *header = (struct mmsch_v1_0_init_header *)table->cpu_addr;\n\tuint32_t size;\n\n\tsize = header->header_size + header->vce_table_size + header->uvd_table_size;\n\n\t \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_CTX_ADDR_LO), lower_32_bits(addr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_CTX_ADDR_HI), upper_32_bits(addr));\n\n\t \n\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_VMID));\n\tdata &= ~VCE_MMSCH_VF_VMID__VF_CTX_VMID_MASK;\n\tdata |= (0 << VCE_MMSCH_VF_VMID__VF_CTX_VMID__SHIFT);  \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_VMID), data);\n\n\t \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_CTX_SIZE), size);\n\n\t \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_MAILBOX_RESP), 0);\n\n\tWDOORBELL32(adev->vce.ring[0].doorbell_index, 0);\n\t*adev->vce.ring[0].wptr_cpu_addr = 0;\n\tadev->vce.ring[0].wptr = 0;\n\tadev->vce.ring[0].wptr_old = 0;\n\n\t \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_MAILBOX_HOST), 0x10000001);\n\n\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_MAILBOX_RESP));\n\tloop = 1000;\n\twhile ((data & 0x10000002) != 0x10000002) {\n\t\tudelay(10);\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_MMSCH_VF_MAILBOX_RESP));\n\t\tloop--;\n\t\tif (!loop)\n\t\t\tbreak;\n\t}\n\n\tif (!loop) {\n\t\tdev_err(adev->dev, \"failed to init MMSCH, mmVCE_MMSCH_VF_MAILBOX_RESP = %x\\n\", data);\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_sriov_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tuint32_t offset, size;\n\tuint32_t table_size = 0;\n\tstruct mmsch_v1_0_cmd_direct_write direct_wt = { { 0 } };\n\tstruct mmsch_v1_0_cmd_direct_read_modify_write direct_rd_mod_wt = { { 0 } };\n\tstruct mmsch_v1_0_cmd_direct_polling direct_poll = { { 0 } };\n\tstruct mmsch_v1_0_cmd_end end = { { 0 } };\n\tuint32_t *init_table = adev->virt.mm_table.cpu_addr;\n\tstruct mmsch_v1_0_init_header *header = (struct mmsch_v1_0_init_header *)init_table;\n\n\tdirect_wt.cmd_header.command_type = MMSCH_COMMAND__DIRECT_REG_WRITE;\n\tdirect_rd_mod_wt.cmd_header.command_type = MMSCH_COMMAND__DIRECT_REG_READ_MODIFY_WRITE;\n\tdirect_poll.cmd_header.command_type = MMSCH_COMMAND__DIRECT_REG_POLLING;\n\tend.cmd_header.command_type = MMSCH_COMMAND__END;\n\n\tif (header->vce_table_offset == 0 && header->vce_table_size == 0) {\n\t\theader->version = MMSCH_VERSION;\n\t\theader->header_size = sizeof(struct mmsch_v1_0_init_header) >> 2;\n\n\t\tif (header->uvd_table_offset == 0 && header->uvd_table_size == 0)\n\t\t\theader->vce_table_offset = header->header_size;\n\t\telse\n\t\t\theader->vce_table_offset = header->uvd_table_size + header->uvd_table_offset;\n\n\t\tinit_table += header->vce_table_offset;\n\n\t\tring = &adev->vce.ring[0];\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_LO),\n\t\t\t\t\t    lower_32_bits(ring->gpu_addr));\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_HI),\n\t\t\t\t\t    upper_32_bits(ring->gpu_addr));\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_SIZE),\n\t\t\t\t\t    ring->ring_size / 4);\n\n\t\t \n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CTRL), 0x398000);\n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CACHE_CTRL), ~0x1, 0);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_SWAP_CNTL), 0);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_SWAP_CNTL1), 0);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VM_CTRL), 0);\n\n\t\toffset = AMDGPU_VCE_FIRMWARE_OFFSET;\n\t\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\t\tuint32_t low = adev->firmware.ucode[AMDGPU_UCODE_ID_VCE].tmr_mc_addr_lo;\n\t\t\tuint32_t hi = adev->firmware.ucode[AMDGPU_UCODE_ID_VCE].tmr_mc_addr_hi;\n\t\t\tuint64_t tmr_mc_addr = (uint64_t)(hi) << 32 | low;\n\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_40BIT_BAR0), tmr_mc_addr >> 8);\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_64BIT_BAR0),\n\t\t\t\t\t\t(tmr_mc_addr >> 40) & 0xff);\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET0), 0);\n\t\t} else {\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_40BIT_BAR0),\n\t\t\t\t\t\tadev->vce.gpu_addr >> 8);\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_64BIT_BAR0),\n\t\t\t\t\t\t(adev->vce.gpu_addr >> 40) & 0xff);\n\t\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET0),\n\t\t\t\t\t\toffset & ~0x0f000000);\n\n\t\t}\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_40BIT_BAR1),\n\t\t\t\t\t\tadev->vce.gpu_addr >> 8);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_64BIT_BAR1),\n\t\t\t\t\t\t(adev->vce.gpu_addr >> 40) & 0xff);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_40BIT_BAR2),\n\t\t\t\t\t\tadev->vce.gpu_addr >> 8);\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0,\n\t\t\t\t\t\tmmVCE_LMI_VCPU_CACHE_64BIT_BAR2),\n\t\t\t\t\t\t(adev->vce.gpu_addr >> 40) & 0xff);\n\n\t\tsize = VCE_V4_0_FW_SIZE;\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE0), size);\n\n\t\toffset = (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) ? offset + size : 0;\n\t\tsize = VCE_V4_0_STACK_SIZE;\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET1),\n\t\t\t\t\t(offset & ~0x0f000000) | (1 << 24));\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE1), size);\n\n\t\toffset += size;\n\t\tsize = VCE_V4_0_DATA_SIZE;\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET2),\n\t\t\t\t\t(offset & ~0x0f000000) | (2 << 24));\n\t\tMMSCH_V1_0_INSERT_DIRECT_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE2), size);\n\n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CTRL2), ~0x100, 0);\n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_SYS_INT_EN),\n\t\t\t\t\t\t   VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK,\n\t\t\t\t\t\t   VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK);\n\n\t\t \n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS),\n\t\t\t\t\t\t   VCE_STATUS__JOB_BUSY_MASK, ~VCE_STATUS__JOB_BUSY_MASK);\n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CNTL),\n\t\t\t\t\t\t   ~0x200001, VCE_VCPU_CNTL__CLK_EN_MASK);\n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_SOFT_RESET),\n\t\t\t\t\t\t   ~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK, 0);\n\n\t\tMMSCH_V1_0_INSERT_DIRECT_POLL(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS),\n\t\t\t\t\t      VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK,\n\t\t\t\t\t      VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK);\n\n\t\t \n\t\tMMSCH_V1_0_INSERT_DIRECT_RD_MOD_WT(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS),\n\t\t\t\t\t\t   ~VCE_STATUS__JOB_BUSY_MASK, 0);\n\n\t\t \n\t\tmemcpy((void *)init_table, &end, sizeof(struct mmsch_v1_0_cmd_end));\n\t\ttable_size += sizeof(struct mmsch_v1_0_cmd_end) / 4;\n\t\theader->vce_table_size = table_size;\n\t}\n\n\treturn vce_v4_0_mmsch_start(adev, &adev->virt.mm_table);\n}\n\n \nstatic int vce_v4_0_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint r;\n\n\tring = &adev->vce.ring[0];\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_LO), ring->gpu_addr);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_HI), upper_32_bits(ring->gpu_addr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_SIZE), ring->ring_size / 4);\n\n\tring = &adev->vce.ring[1];\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR2), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR2), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_LO2), ring->gpu_addr);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_HI2), upper_32_bits(ring->gpu_addr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_SIZE2), ring->ring_size / 4);\n\n\tring = &adev->vce.ring[2];\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_RPTR3), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_WPTR3), lower_32_bits(ring->wptr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_LO3), ring->gpu_addr);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_BASE_HI3), upper_32_bits(ring->gpu_addr));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_SIZE3), ring->ring_size / 4);\n\n\tvce_v4_0_mc_resume(adev);\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS), VCE_STATUS__JOB_BUSY_MASK,\n\t\t\t~VCE_STATUS__JOB_BUSY_MASK);\n\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CNTL), 1, ~0x200001);\n\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SOFT_RESET), 0,\n\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\tmdelay(100);\n\n\tr = vce_v4_0_firmware_loaded(adev);\n\n\t \n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS), 0, ~VCE_STATUS__JOB_BUSY_MASK);\n\n\tif (r) {\n\t\tDRM_ERROR(\"VCE not responding, giving up!!!\\n\");\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_stop(struct amdgpu_device *adev)\n{\n\n\t \n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CNTL), 0, ~0x200001);\n\n\t \n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SOFT_RESET),\n\t\t\tVCE_SOFT_RESET__ECPU_SOFT_RESET_MASK,\n\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\n\t \n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS), 0);\n\n\t \n\t \n\n\treturn 0;\n}\n\nstatic int vce_v4_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))  \n\t\tadev->vce.num_rings = 1;\n\telse\n\t\tadev->vce.num_rings = 3;\n\n\tvce_v4_0_set_ring_funcs(adev);\n\tvce_v4_0_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring;\n\n\tunsigned size;\n\tint r, i;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCE0, 167, &adev->vce.irq);\n\tif (r)\n\t\treturn r;\n\n\tsize  = VCE_V4_0_STACK_SIZE + VCE_V4_0_DATA_SIZE;\n\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP)\n\t\tsize += VCE_V4_0_FW_SIZE;\n\n\tr = amdgpu_vce_sw_init(adev, size);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tconst struct common_firmware_header *hdr;\n\t\tunsigned size = amdgpu_bo_size(adev->vce.vcpu_bo);\n\n\t\tadev->vce.saved_bo = kvmalloc(size, GFP_KERNEL);\n\t\tif (!adev->vce.saved_bo)\n\t\t\treturn -ENOMEM;\n\n\t\thdr = (const struct common_firmware_header *)adev->vce.fw->data;\n\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCE].ucode_id = AMDGPU_UCODE_ID_VCE;\n\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCE].fw = adev->vce.fw;\n\t\tadev->firmware.fw_size +=\n\t\t\tALIGN(le32_to_cpu(hdr->ucode_size_bytes), PAGE_SIZE);\n\t\tDRM_INFO(\"PSP loading VCE firmware\\n\");\n\t} else {\n\t\tr = amdgpu_vce_resume(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tenum amdgpu_ring_priority_level hw_prio = amdgpu_vce_get_ring_prio(i);\n\n\t\tring = &adev->vce.ring[i];\n\t\tring->vm_hub = AMDGPU_MMHUB0(0);\n\t\tsprintf(ring->name, \"vce%d\", i);\n\t\tif (amdgpu_sriov_vf(adev)) {\n\t\t\t \n\t\t\tring->use_doorbell = true;\n\n\t\t\t \n\t\t\tif (i == 0)\n\t\t\t\tring->doorbell_index = adev->doorbell_index.uvd_vce.vce_ring0_1 * 2;\n\t\t\telse\n\t\t\t\tring->doorbell_index = adev->doorbell_index.uvd_vce.vce_ring2_3 * 2 + 1;\n\t\t}\n\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->vce.irq, 0,\n\t\t\t\t     hw_prio, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\n\tr = amdgpu_vce_entity_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_virt_alloc_mm_table(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn r;\n}\n\nstatic int vce_v4_0_sw_fini(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tamdgpu_virt_free_mm_table(adev);\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tkvfree(adev->vce.saved_bo);\n\t\tadev->vce.saved_bo = NULL;\n\t}\n\n\tr = amdgpu_vce_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_sw_fini(adev);\n}\n\nstatic int vce_v4_0_hw_init(void *handle)\n{\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tr = vce_v4_0_sriov_start(adev);\n\telse\n\t\tr = vce_v4_0_start(adev);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tr = amdgpu_ring_test_helper(&adev->vce.ring[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tDRM_INFO(\"VCE initialized successfully.\\n\");\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tvce_v4_0_stop(adev);\n\t} else {\n\t\t \n\t\tDRM_DEBUG(\"For SRIOV client, shouldn't do anything.\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r, idx;\n\n\tif (adev->vce.vcpu_bo == NULL)\n\t\treturn 0;\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\t\tunsigned size = amdgpu_bo_size(adev->vce.vcpu_bo);\n\t\t\tvoid *ptr = adev->vce.cpu_addr;\n\n\t\t\tmemcpy_fromio(adev->vce.saved_bo, ptr, size);\n\t\t}\n\t\tdrm_dev_exit(idx);\n\t}\n\n\t \n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tif (adev->pm.dpm_enabled) {\n\t\tamdgpu_dpm_enable_vce(adev, false);\n\t} else {\n\t\tamdgpu_asic_set_vce_clocks(adev, 0, 0);\n\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_PG_STATE_GATE);\n\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_CG_STATE_GATE);\n\t}\n\n\tr = vce_v4_0_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_suspend(adev);\n}\n\nstatic int vce_v4_0_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r, idx;\n\n\tif (adev->vce.vcpu_bo == NULL)\n\t\treturn -EINVAL;\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\n\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\tunsigned size = amdgpu_bo_size(adev->vce.vcpu_bo);\n\t\t\tvoid *ptr = adev->vce.cpu_addr;\n\n\t\t\tmemcpy_toio(ptr, adev->vce.saved_bo, size);\n\t\t\tdrm_dev_exit(idx);\n\t\t}\n\t} else {\n\t\tr = amdgpu_vce_resume(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn vce_v4_0_hw_init(adev);\n}\n\nstatic void vce_v4_0_mc_resume(struct amdgpu_device *adev)\n{\n\tuint32_t offset, size;\n\tuint64_t tmr_mc_addr;\n\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_A), 0, ~(1 << 16));\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING), 0x1FF000, ~0xFF9FF000);\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_REG_CLOCK_GATING), 0x3F, ~0x3F);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_B), 0x1FF);\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CTRL), 0x00398000);\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CACHE_CTRL), 0x0, ~0x1);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_SWAP_CNTL), 0);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_SWAP_CNTL1), 0);\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VM_CTRL), 0);\n\n\toffset = AMDGPU_VCE_FIRMWARE_OFFSET;\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\ttmr_mc_addr = (uint64_t)(adev->firmware.ucode[AMDGPU_UCODE_ID_VCE].tmr_mc_addr_hi) << 32 |\n\t\t\t\t\t\t\t\t\t\tadev->firmware.ucode[AMDGPU_UCODE_ID_VCE].tmr_mc_addr_lo;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_40BIT_BAR0),\n\t\t\t(tmr_mc_addr >> 8));\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_64BIT_BAR0),\n\t\t\t(tmr_mc_addr >> 40) & 0xff);\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET0), 0);\n\t} else {\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_40BIT_BAR0),\n\t\t\t(adev->vce.gpu_addr >> 8));\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_64BIT_BAR0),\n\t\t\t(adev->vce.gpu_addr >> 40) & 0xff);\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET0), offset & ~0x0f000000);\n\t}\n\n\tsize = VCE_V4_0_FW_SIZE;\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE0), size);\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_40BIT_BAR1), (adev->vce.gpu_addr >> 8));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_64BIT_BAR1), (adev->vce.gpu_addr >> 40) & 0xff);\n\toffset = (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) ? offset + size : 0;\n\tsize = VCE_V4_0_STACK_SIZE;\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET1), (offset & ~0x0f000000) | (1 << 24));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE1), size);\n\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_40BIT_BAR2), (adev->vce.gpu_addr >> 8));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_VCPU_CACHE_64BIT_BAR2), (adev->vce.gpu_addr >> 40) & 0xff);\n\toffset += size;\n\tsize = VCE_V4_0_DATA_SIZE;\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_OFFSET2), (offset & ~0x0f000000) | (2 << 24));\n\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_VCPU_CACHE_SIZE2), size);\n\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_LMI_CTRL2), 0x0, ~0x100);\n\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SYS_INT_EN),\n\t\t\tVCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK,\n\t\t\t~VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK);\n}\n\nstatic int vce_v4_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\t \n\treturn 0;\n}\n\n#if 0\nstatic bool vce_v4_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 mask = 0;\n\n\tmask |= (adev->vce.harvest_config & AMDGPU_VCE_HARVEST_VCE0) ? 0 : SRBM_STATUS2__VCE0_BUSY_MASK;\n\tmask |= (adev->vce.harvest_config & AMDGPU_VCE_HARVEST_VCE1) ? 0 : SRBM_STATUS2__VCE1_BUSY_MASK;\n\n\treturn !(RREG32(mmSRBM_STATUS2) & mask);\n}\n\nstatic int vce_v4_0_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++)\n\t\tif (vce_v4_0_is_idle(handle))\n\t\t\treturn 0;\n\n\treturn -ETIMEDOUT;\n}\n\n#define  VCE_STATUS_VCPU_REPORT_AUTO_BUSY_MASK  0x00000008L    \n#define  VCE_STATUS_VCPU_REPORT_RB0_BUSY_MASK   0x00000010L    \n#define  VCE_STATUS_VCPU_REPORT_RB1_BUSY_MASK   0x00000020L    \n#define  AMDGPU_VCE_STATUS_BUSY_MASK (VCE_STATUS_VCPU_REPORT_AUTO_BUSY_MASK | \\\n\t\t\t\t      VCE_STATUS_VCPU_REPORT_RB0_BUSY_MASK)\n\nstatic bool vce_v4_0_check_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset = 0;\n\n\t \n\tmutex_lock(&adev->grbm_idx_mutex);\n\tWREG32_FIELD(GRBM_GFX_INDEX, INSTANCE_INDEX, 0);\n\tif (RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS) & AMDGPU_VCE_STATUS_BUSY_MASK) {\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE0, 1);\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE1, 1);\n\t}\n\tWREG32_FIELD(GRBM_GFX_INDEX, INSTANCE_INDEX, 0x10);\n\tif (RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_STATUS) & AMDGPU_VCE_STATUS_BUSY_MASK) {\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE0, 1);\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE1, 1);\n\t}\n\tWREG32_FIELD(GRBM_GFX_INDEX, INSTANCE_INDEX, 0);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\tif (srbm_soft_reset) {\n\t\tadev->vce.srbm_soft_reset = srbm_soft_reset;\n\t\treturn true;\n\t} else {\n\t\tadev->vce.srbm_soft_reset = 0;\n\t\treturn false;\n\t}\n}\n\nstatic int vce_v4_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\tsrbm_soft_reset = adev->vce.srbm_soft_reset;\n\n\tif (srbm_soft_reset) {\n\t\tu32 tmp;\n\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v4_0_pre_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\n\tmdelay(5);\n\n\treturn vce_v4_0_suspend(adev);\n}\n\n\nstatic int vce_v4_0_post_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\n\tmdelay(5);\n\n\treturn vce_v4_0_resume(adev);\n}\n\nstatic void vce_v4_0_override_vce_clock_gating(struct amdgpu_device *adev, bool override)\n{\n\tu32 tmp, data;\n\n\ttmp = data = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_ARB_CTRL));\n\tif (override)\n\t\tdata |= VCE_RB_ARB_CTRL__VCE_CGTT_OVERRIDE_MASK;\n\telse\n\t\tdata &= ~VCE_RB_ARB_CTRL__VCE_CGTT_OVERRIDE_MASK;\n\n\tif (tmp != data)\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_RB_ARB_CTRL), data);\n}\n\nstatic void vce_v4_0_set_vce_sw_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t     bool gated)\n{\n\tu32 data;\n\n\t \n\tvce_v4_0_override_vce_clock_gating(adev, true);\n\n\t \n\tif (gated) {\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_B));\n\t\tdata |= 0x1ff;\n\t\tdata &= ~0xef0000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_B), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING));\n\t\tdata |= 0x3ff000;\n\t\tdata &= ~0xffc00000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING_2));\n\t\tdata |= 0x2;\n\t\tdata &= ~0x00010000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING_2), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_REG_CLOCK_GATING));\n\t\tdata |= 0x37f;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_REG_CLOCK_GATING), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_DMA_DCLK_CTRL));\n\t\tdata |= VCE_UENC_DMA_DCLK_CTRL__WRDMCLK_FORCEON_MASK |\n\t\t\tVCE_UENC_DMA_DCLK_CTRL__RDDMCLK_FORCEON_MASK |\n\t\t\tVCE_UENC_DMA_DCLK_CTRL__REGCLK_FORCEON_MASK  |\n\t\t\t0x8;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_DMA_DCLK_CTRL), data);\n\t} else {\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_B));\n\t\tdata &= ~0x80010;\n\t\tdata |= 0xe70008;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_B), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING));\n\t\tdata |= 0xffc00000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING_2));\n\t\tdata |= 0x10000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING_2), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_REG_CLOCK_GATING));\n\t\tdata &= ~0xffc00000;\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_REG_CLOCK_GATING), data);\n\n\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_DMA_DCLK_CTRL));\n\t\tdata &= ~(VCE_UENC_DMA_DCLK_CTRL__WRDMCLK_FORCEON_MASK |\n\t\t\t  VCE_UENC_DMA_DCLK_CTRL__RDDMCLK_FORCEON_MASK |\n\t\t\t  VCE_UENC_DMA_DCLK_CTRL__REGCLK_FORCEON_MASK  |\n\t\t\t  0x8);\n\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_DMA_DCLK_CTRL), data);\n\t}\n\tvce_v4_0_override_vce_clock_gating(adev, false);\n}\n\nstatic void vce_v4_0_set_bypass_mode(struct amdgpu_device *adev, bool enable)\n{\n\tu32 tmp = RREG32_SMC(ixGCK_DFS_BYPASS_CNTL);\n\n\tif (enable)\n\t\ttmp |= GCK_DFS_BYPASS_CNTL__BYPASSECLK_MASK;\n\telse\n\t\ttmp &= ~GCK_DFS_BYPASS_CNTL__BYPASSECLK_MASK;\n\n\tWREG32_SMC(ixGCK_DFS_BYPASS_CNTL, tmp);\n}\n\nstatic int vce_v4_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool enable = (state == AMD_CG_STATE_GATE);\n\tint i;\n\n\tif ((adev->asic_type == CHIP_POLARIS10) ||\n\t\t(adev->asic_type == CHIP_TONGA) ||\n\t\t(adev->asic_type == CHIP_FIJI))\n\t\tvce_v4_0_set_bypass_mode(adev, enable);\n\n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_VCE_MGCG))\n\t\treturn 0;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < 2; i++) {\n\t\t \n\t\tif (adev->vce.harvest_config & (1 << i))\n\t\t\tcontinue;\n\n\t\tWREG32_FIELD(GRBM_GFX_INDEX, VCE_INSTANCE, i);\n\n\t\tif (enable) {\n\t\t\t \n\t\t\tuint32_t data = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_A);\n\t\t\tdata &= ~(0xf | 0xff0);\n\t\t\tdata |= ((0x0 << 0) | (0x04 << 4));\n\t\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_CLOCK_GATING_A, data);\n\n\t\t\t \n\t\t\tdata = RREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING);\n\t\t\tdata &= ~(0xf | 0xff0);\n\t\t\tdata |= ((0x0 << 0) | (0x04 << 4));\n\t\t\tWREG32(SOC15_REG_OFFSET(VCE, 0, mmVCE_UENC_CLOCK_GATING, data);\n\t\t}\n\n\t\tvce_v4_0_set_vce_sw_clock_gating(adev, enable);\n\t}\n\n\tWREG32_FIELD(GRBM_GFX_INDEX, VCE_INSTANCE, 0);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n#endif\n\nstatic int vce_v4_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\t \n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (state == AMD_PG_STATE_GATE)\n\t\treturn vce_v4_0_stop(adev);\n\telse\n\t\treturn vce_v4_0_start(adev);\n}\n\nstatic void vce_v4_0_ring_emit_ib(struct amdgpu_ring *ring, struct amdgpu_job *job,\n\t\t\t\t\tstruct amdgpu_ib *ib, uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, VCE_CMD_IB_VM);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nstatic void vce_v4_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,\n\t\t\tu64 seq, unsigned flags)\n{\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\tamdgpu_ring_write(ring, VCE_CMD_FENCE);\n\tamdgpu_ring_write(ring, addr);\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, seq);\n\tamdgpu_ring_write(ring, VCE_CMD_TRAP);\n}\n\nstatic void vce_v4_0_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tamdgpu_ring_write(ring, VCE_CMD_END);\n}\n\nstatic void vce_v4_0_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t   uint32_t val, uint32_t mask)\n{\n\tamdgpu_ring_write(ring, VCE_CMD_REG_WAIT);\n\tamdgpu_ring_write(ring,\treg << 2);\n\tamdgpu_ring_write(ring, mask);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void vce_v4_0_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t   unsigned int vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t \n\tvce_v4_0_emit_reg_wait(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t       vmid * hub->ctx_addr_distance,\n\t\t\t       lower_32_bits(pd_addr), 0xffffffff);\n}\n\nstatic void vce_v4_0_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t       uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, VCE_CMD_REG_WRITE);\n\tamdgpu_ring_write(ring,\treg << 2);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic int vce_v4_0_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tuint32_t val = 0;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tif (state == AMDGPU_IRQ_STATE_ENABLE)\n\t\t\tval |= VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK;\n\n\t\tWREG32_P(SOC15_REG_OFFSET(VCE, 0, mmVCE_SYS_INT_EN), val,\n\t\t\t\t~VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK);\n\t}\n\treturn 0;\n}\n\nstatic int vce_v4_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"IH: VCE\\n\");\n\n\tswitch (entry->src_data[0]) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\t\tamdgpu_fence_process(&adev->vce.ring[entry->src_data[0]]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nconst struct amd_ip_funcs vce_v4_0_ip_funcs = {\n\t.name = \"vce_v4_0\",\n\t.early_init = vce_v4_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = vce_v4_0_sw_init,\n\t.sw_fini = vce_v4_0_sw_fini,\n\t.hw_init = vce_v4_0_hw_init,\n\t.hw_fini = vce_v4_0_hw_fini,\n\t.suspend = vce_v4_0_suspend,\n\t.resume = vce_v4_0_resume,\n\t.is_idle = NULL  ,\n\t.wait_for_idle = NULL  ,\n\t.check_soft_reset = NULL  ,\n\t.pre_soft_reset = NULL  ,\n\t.soft_reset = NULL  ,\n\t.post_soft_reset = NULL  ,\n\t.set_clockgating_state = vce_v4_0_set_clockgating_state,\n\t.set_powergating_state = vce_v4_0_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs vce_v4_0_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCE,\n\t.align_mask = 0x3f,\n\t.nop = VCE_CMD_NO_OP,\n\t.support_64bit_ptrs = false,\n\t.no_user_fence = true,\n\t.get_rptr = vce_v4_0_ring_get_rptr,\n\t.get_wptr = vce_v4_0_ring_get_wptr,\n\t.set_wptr = vce_v4_0_ring_set_wptr,\n\t.parse_cs = amdgpu_vce_ring_parse_cs_vm,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 4 +\n\t\t4 +  \n\t\t5 + 5 +  \n\t\t1,  \n\t.emit_ib_size = 5,  \n\t.emit_ib = vce_v4_0_ring_emit_ib,\n\t.emit_vm_flush = vce_v4_0_emit_vm_flush,\n\t.emit_fence = vce_v4_0_ring_emit_fence,\n\t.test_ring = amdgpu_vce_ring_test_ring,\n\t.test_ib = amdgpu_vce_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.insert_end = vce_v4_0_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vce_ring_begin_use,\n\t.end_use = amdgpu_vce_ring_end_use,\n\t.emit_wreg = vce_v4_0_emit_wreg,\n\t.emit_reg_wait = vce_v4_0_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void vce_v4_0_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tadev->vce.ring[i].funcs = &vce_v4_0_ring_vm_funcs;\n\t\tadev->vce.ring[i].me = i;\n\t}\n\tDRM_INFO(\"VCE enabled in VM mode\\n\");\n}\n\nstatic const struct amdgpu_irq_src_funcs vce_v4_0_irq_funcs = {\n\t.set = vce_v4_0_set_interrupt_state,\n\t.process = vce_v4_0_process_interrupt,\n};\n\nstatic void vce_v4_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->vce.irq.num_types = 1;\n\tadev->vce.irq.funcs = &vce_v4_0_irq_funcs;\n};\n\nconst struct amdgpu_ip_block_version vce_v4_0_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_VCE,\n\t.major = 4,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &vce_v4_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}