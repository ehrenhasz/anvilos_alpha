{
  "module_name": "df_v3_6.c",
  "hash_id": "a429b138a5eabc3819fb1e31e699d9379c45b7e118871eb106e5a352e12c4744",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/df_v3_6.c",
  "human_readable_source": " \n#include \"amdgpu.h\"\n#include \"df_v3_6.h\"\n\n#include \"df/df_3_6_default.h\"\n#include \"df/df_3_6_offset.h\"\n#include \"df/df_3_6_sh_mask.h\"\n\n#define DF_3_6_SMN_REG_INST_DIST        0x8\n#define DF_3_6_INST_CNT                 8\n\n \n#define DF_V3_6_MAX_COUNTERS\t\t4\n\n \n#define DF_V3_6_GET_EVENT(x)\t\t(x & 0xFFUL)\n#define DF_V3_6_GET_INSTANCE(x)\t\t((x >> 8) & 0xFFUL)\n#define DF_V3_6_GET_UNITMASK(x)\t\t((x >> 16) & 0xFFUL)\n#define DF_V3_6_PERFMON_OVERFLOW\t0xFFFFFFFFFFFFULL\n\nstatic u32 df_v3_6_channel_number[] = {1, 2, 0, 4, 0, 8, 0,\n\t\t\t\t       16, 32, 0, 0, 0, 2, 4, 8};\n\nstatic uint64_t df_v3_6_get_fica(struct amdgpu_device *adev,\n\t\t\t\t uint32_t ficaa_val)\n{\n\tunsigned long flags, address, data;\n\tuint32_t ficadl_val, ficadh_val;\n\n\taddress = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessAddress3);\n\tWREG32(data, ficaa_val);\n\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessDataLo3);\n\tficadl_val = RREG32(data);\n\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessDataHi3);\n\tficadh_val = RREG32(data);\n\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\n\treturn (((ficadh_val & 0xFFFFFFFFFFFFFFFF) << 32) | ficadl_val);\n}\n\nstatic void df_v3_6_set_fica(struct amdgpu_device *adev, uint32_t ficaa_val,\n\t\t\t     uint32_t ficadl_val, uint32_t ficadh_val)\n{\n\tunsigned long flags, address, data;\n\n\taddress = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessAddress3);\n\tWREG32(data, ficaa_val);\n\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessDataLo3);\n\tWREG32(data, ficadl_val);\n\n\tWREG32(address, smnDF_PIE_AON_FabricIndirectConfigAccessDataHi3);\n\tWREG32(data, ficadh_val);\n\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nstatic void df_v3_6_perfmon_rreg(struct amdgpu_device *adev,\n\t\t\t    uint32_t lo_addr, uint32_t *lo_val,\n\t\t\t    uint32_t hi_addr, uint32_t *hi_val)\n{\n\tunsigned long flags, address, data;\n\n\taddress = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, lo_addr);\n\t*lo_val = RREG32(data);\n\tWREG32(address, hi_addr);\n\t*hi_val = RREG32(data);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nstatic void df_v3_6_perfmon_wreg(struct amdgpu_device *adev, uint32_t lo_addr,\n\t\t\t    uint32_t lo_val, uint32_t hi_addr, uint32_t hi_val)\n{\n\tunsigned long flags, address, data;\n\n\taddress = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, lo_addr);\n\tWREG32(data, lo_val);\n\tWREG32(address, hi_addr);\n\tWREG32(data, hi_val);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nstatic int df_v3_6_perfmon_arm_with_status(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t lo_addr, uint32_t lo_val,\n\t\t\t\t\t  uint32_t hi_addr, uint32_t  hi_val)\n{\n\tunsigned long flags, address, data;\n\tuint32_t lo_val_rb, hi_val_rb;\n\n\taddress = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, lo_addr);\n\tWREG32(data, lo_val);\n\tWREG32(address, hi_addr);\n\tWREG32(data, hi_val);\n\n\tWREG32(address, lo_addr);\n\tlo_val_rb = RREG32(data);\n\tWREG32(address, hi_addr);\n\thi_val_rb = RREG32(data);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\n\tif (!(lo_val == lo_val_rb && hi_val == hi_val_rb))\n\t\treturn -EBUSY;\n\n\treturn 0;\n}\n\n\n \n#define ARM_RETRY_USEC_TIMEOUT\t1000\n#define ARM_RETRY_USEC_INTERVAL\t100\nstatic int df_v3_6_perfmon_arm_with_retry(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t lo_addr, uint32_t lo_val,\n\t\t\t\t\t  uint32_t hi_addr, uint32_t  hi_val)\n{\n\tint countdown = ARM_RETRY_USEC_TIMEOUT;\n\n\twhile (countdown) {\n\n\t\tif (!df_v3_6_perfmon_arm_with_status(adev, lo_addr, lo_val,\n\t\t\t\t\t\t     hi_addr, hi_val))\n\t\t\tbreak;\n\n\t\tcountdown -= ARM_RETRY_USEC_INTERVAL;\n\t\tudelay(ARM_RETRY_USEC_INTERVAL);\n\t}\n\n\treturn countdown > 0 ? 0 : -ETIME;\n}\n\n \nstatic ssize_t df_v3_6_get_df_cntr_avail(struct device *dev,\n\t\tstruct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct amdgpu_device *adev;\n\tstruct drm_device *ddev;\n\tint i, count;\n\n\tddev = dev_get_drvdata(dev);\n\tadev = drm_to_adev(ddev);\n\tcount = 0;\n\n\tfor (i = 0; i < DF_V3_6_MAX_COUNTERS; i++) {\n\t\tif (adev->df_perfmon_config_assign_mask[i] == 0)\n\t\t\tcount++;\n\t}\n\n\treturn sysfs_emit(buf, \"%i\\n\", count);\n}\n\n \nstatic DEVICE_ATTR(df_cntr_avail, S_IRUGO, df_v3_6_get_df_cntr_avail, NULL);\n\nstatic void df_v3_6_query_hashes(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tadev->df.hash_status.hash_64k = false;\n\tadev->df.hash_status.hash_2m = false;\n\tadev->df.hash_status.hash_1g = false;\n\n\t \n\tif ((adev->asic_type == CHIP_ARCTURUS &&\n\t     adev->df.funcs->get_fb_channel_number(adev) == 0xe) ||\n\t     (adev->asic_type == CHIP_ALDEBARAN &&\n\t      adev->df.funcs->get_fb_channel_number(adev) == 0x1e)) {\n\t\ttmp = RREG32_SOC15(DF, 0, mmDF_CS_UMC_AON0_DfGlobalCtrl);\n\t\tadev->df.hash_status.hash_64k = REG_GET_FIELD(tmp,\n\t\t\t\t\t\tDF_CS_UMC_AON0_DfGlobalCtrl,\n\t\t\t\t\t\tGlbHashIntlvCtl64K);\n\t\tadev->df.hash_status.hash_2m = REG_GET_FIELD(tmp,\n\t\t\t\t\t\tDF_CS_UMC_AON0_DfGlobalCtrl,\n\t\t\t\t\t\tGlbHashIntlvCtl2M);\n\t\tadev->df.hash_status.hash_1g = REG_GET_FIELD(tmp,\n\t\t\t\t\t\tDF_CS_UMC_AON0_DfGlobalCtrl,\n\t\t\t\t\t\tGlbHashIntlvCtl1G);\n\t}\n}\n\n \nstatic void df_v3_6_sw_init(struct amdgpu_device *adev)\n{\n\tint i, ret;\n\n\tret = device_create_file(adev->dev, &dev_attr_df_cntr_avail);\n\tif (ret)\n\t\tDRM_ERROR(\"failed to create file for available df counters\\n\");\n\n\tfor (i = 0; i < AMDGPU_MAX_DF_PERFMONS; i++)\n\t\tadev->df_perfmon_config_assign_mask[i] = 0;\n\n\tdf_v3_6_query_hashes(adev);\n}\n\nstatic void df_v3_6_sw_fini(struct amdgpu_device *adev)\n{\n\n\tdevice_remove_file(adev->dev, &dev_attr_df_cntr_avail);\n\n}\n\nstatic void df_v3_6_enable_broadcast_mode(struct amdgpu_device *adev,\n\t\t\t\t\t  bool enable)\n{\n\tu32 tmp;\n\n\tif (enable) {\n\t\ttmp = RREG32_SOC15(DF, 0, mmFabricConfigAccessControl);\n\t\ttmp &= ~FabricConfigAccessControl__CfgRegInstAccEn_MASK;\n\t\tWREG32_SOC15(DF, 0, mmFabricConfigAccessControl, tmp);\n\t} else\n\t\tWREG32_SOC15(DF, 0, mmFabricConfigAccessControl,\n\t\t\t     mmFabricConfigAccessControl_DEFAULT);\n}\n\nstatic u32 df_v3_6_get_fb_channel_number(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tif (adev->asic_type == CHIP_ALDEBARAN) {\n\t\ttmp = RREG32_SOC15(DF, 0, mmDF_GCM_AON0_DramMegaBaseAddress0);\n\t\ttmp &=\n\t\tALDEBARAN_DF_CS_UMC_AON0_DramBaseAddress0__IntLvNumChan_MASK;\n\t} else {\n\t\ttmp = RREG32_SOC15(DF, 0, mmDF_CS_UMC_AON0_DramBaseAddress0);\n\t\ttmp &= DF_CS_UMC_AON0_DramBaseAddress0__IntLvNumChan_MASK;\n\t}\n\ttmp >>= DF_CS_UMC_AON0_DramBaseAddress0__IntLvNumChan__SHIFT;\n\n\treturn tmp;\n}\n\nstatic u32 df_v3_6_get_hbm_channel_number(struct amdgpu_device *adev)\n{\n\tint fb_channel_number;\n\n\tfb_channel_number = adev->df.funcs->get_fb_channel_number(adev);\n\tif (fb_channel_number >= ARRAY_SIZE(df_v3_6_channel_number))\n\t\tfb_channel_number = 0;\n\n\treturn df_v3_6_channel_number[fb_channel_number];\n}\n\nstatic void df_v3_6_update_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t     bool enable)\n{\n\tu32 tmp;\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_DF_MGCG) {\n\t\t \n\t\tadev->df.funcs->enable_broadcast_mode(adev, true);\n\n\t\tif (enable) {\n\t\t\ttmp = RREG32_SOC15(DF, 0,\n\t\t\t\t\tmmDF_PIE_AON0_DfGlobalClkGater);\n\t\t\ttmp &= ~DF_PIE_AON0_DfGlobalClkGater__MGCGMode_MASK;\n\t\t\ttmp |= DF_V3_6_MGCG_ENABLE_15_CYCLE_DELAY;\n\t\t\tWREG32_SOC15(DF, 0,\n\t\t\t\t\tmmDF_PIE_AON0_DfGlobalClkGater, tmp);\n\t\t} else {\n\t\t\ttmp = RREG32_SOC15(DF, 0,\n\t\t\t\t\tmmDF_PIE_AON0_DfGlobalClkGater);\n\t\t\ttmp &= ~DF_PIE_AON0_DfGlobalClkGater__MGCGMode_MASK;\n\t\t\ttmp |= DF_V3_6_MGCG_DISABLE;\n\t\t\tWREG32_SOC15(DF, 0,\n\t\t\t\t\tmmDF_PIE_AON0_DfGlobalClkGater, tmp);\n\t\t}\n\n\t\t \n\t\tadev->df.funcs->enable_broadcast_mode(adev, false);\n\t}\n}\n\nstatic void df_v3_6_get_clockgating_state(struct amdgpu_device *adev,\n\t\t\t\t\t  u64 *flags)\n{\n\tu32 tmp;\n\n\t \n\ttmp = RREG32_SOC15(DF, 0, mmDF_PIE_AON0_DfGlobalClkGater);\n\tif (tmp & DF_V3_6_MGCG_ENABLE_15_CYCLE_DELAY)\n\t\t*flags |= AMD_CG_SUPPORT_DF_MGCG;\n}\n\n \nstatic bool df_v3_6_pmc_has_counter(struct amdgpu_device *adev,\n\t\t\t\t      uint64_t config,\n\t\t\t\t      int counter_idx)\n{\n\n\treturn ((config & 0x0FFFFFFUL) ==\n\t\t\tadev->df_perfmon_config_assign_mask[counter_idx]);\n\n}\n\n \nstatic void df_v3_6_pmc_get_addr(struct amdgpu_device *adev,\n\t\t\t\t uint64_t config,\n\t\t\t\t int counter_idx,\n\t\t\t\t int is_ctrl,\n\t\t\t\t uint32_t *lo_base_addr,\n\t\t\t\t uint32_t *hi_base_addr)\n{\n\tif (!df_v3_6_pmc_has_counter(adev, config, counter_idx))\n\t\treturn;\n\n\tswitch (counter_idx) {\n\n\tcase 0:\n\t\t*lo_base_addr = is_ctrl ? smnPerfMonCtlLo4 : smnPerfMonCtrLo4;\n\t\t*hi_base_addr = is_ctrl ? smnPerfMonCtlHi4 : smnPerfMonCtrHi4;\n\t\tbreak;\n\tcase 1:\n\t\t*lo_base_addr = is_ctrl ? smnPerfMonCtlLo5 : smnPerfMonCtrLo5;\n\t\t*hi_base_addr = is_ctrl ? smnPerfMonCtlHi5 : smnPerfMonCtrHi5;\n\t\tbreak;\n\tcase 2:\n\t\t*lo_base_addr = is_ctrl ? smnPerfMonCtlLo6 : smnPerfMonCtrLo6;\n\t\t*hi_base_addr = is_ctrl ? smnPerfMonCtlHi6 : smnPerfMonCtrHi6;\n\t\tbreak;\n\tcase 3:\n\t\t*lo_base_addr = is_ctrl ? smnPerfMonCtlLo7 : smnPerfMonCtrLo7;\n\t\t*hi_base_addr = is_ctrl ? smnPerfMonCtlHi7 : smnPerfMonCtrHi7;\n\t\tbreak;\n\n\t}\n\n}\n\n \nstatic void df_v3_6_pmc_get_read_settings(struct amdgpu_device *adev,\n\t\t\t\t\t  uint64_t config,\n\t\t\t\t\t  int counter_idx,\n\t\t\t\t\t  uint32_t *lo_base_addr,\n\t\t\t\t\t  uint32_t *hi_base_addr)\n{\n\tdf_v3_6_pmc_get_addr(adev, config, counter_idx, 0, lo_base_addr,\n\t\t\t\t\t\t\t\thi_base_addr);\n}\n\n \nstatic int df_v3_6_pmc_get_ctrl_settings(struct amdgpu_device *adev,\n\t\t\t\t\t  uint64_t config,\n\t\t\t\t\t  int counter_idx,\n\t\t\t\t\t  uint32_t *lo_base_addr,\n\t\t\t\t\t  uint32_t *hi_base_addr,\n\t\t\t\t\t  uint32_t *lo_val,\n\t\t\t\t\t  uint32_t *hi_val,\n\t\t\t\t\t  bool is_enable)\n{\n\n\tuint32_t eventsel, instance, unitmask;\n\tuint32_t instance_10, instance_5432, instance_76;\n\n\tdf_v3_6_pmc_get_addr(adev, config, counter_idx, 1, lo_base_addr,\n\t\t\t\thi_base_addr);\n\n\tif ((*lo_base_addr == 0) || (*hi_base_addr == 0)) {\n\t\tDRM_ERROR(\"[DF PMC] addressing not retrieved! Lo: %x, Hi: %x\",\n\t\t\t\t*lo_base_addr, *hi_base_addr);\n\t\treturn -ENXIO;\n\t}\n\n\teventsel = DF_V3_6_GET_EVENT(config) & 0x3f;\n\tunitmask = DF_V3_6_GET_UNITMASK(config) & 0xf;\n\tinstance = DF_V3_6_GET_INSTANCE(config);\n\n\tinstance_10 = instance & 0x3;\n\tinstance_5432 = (instance >> 2) & 0xf;\n\tinstance_76 = (instance >> 6) & 0x3;\n\n\t*lo_val = (unitmask << 8) | (instance_10 << 6) | eventsel;\n\t*lo_val = is_enable ? *lo_val | (1 << 22) : *lo_val & ~(1 << 22);\n\t*hi_val = (instance_76 << 29) | instance_5432;\n\n\tDRM_DEBUG_DRIVER(\"config=%llx addr=%08x:%08x val=%08x:%08x\",\n\t\tconfig, *lo_base_addr, *hi_base_addr, *lo_val, *hi_val);\n\n\treturn 0;\n}\n\n \nstatic int df_v3_6_pmc_add_cntr(struct amdgpu_device *adev,\n\t\t\t\t   uint64_t config)\n{\n\tint i;\n\n\tfor (i = 0; i < DF_V3_6_MAX_COUNTERS; i++) {\n\t\tif (adev->df_perfmon_config_assign_mask[i] == 0U) {\n\t\t\tadev->df_perfmon_config_assign_mask[i] =\n\t\t\t\t\t\t\tconfig & 0x0FFFFFFUL;\n\t\t\treturn i;\n\t\t}\n\t}\n\n\treturn -ENOSPC;\n}\n\n#define DEFERRED_ARM_MASK\t(1 << 31)\nstatic int df_v3_6_pmc_set_deferred(struct amdgpu_device *adev,\n\t\t\t\t    uint64_t config, int counter_idx,\n\t\t\t\t    bool is_deferred)\n{\n\n\tif (!df_v3_6_pmc_has_counter(adev, config, counter_idx))\n\t\treturn -EINVAL;\n\n\tif (is_deferred)\n\t\tadev->df_perfmon_config_assign_mask[counter_idx] |=\n\t\t\t\t\t\t\tDEFERRED_ARM_MASK;\n\telse\n\t\tadev->df_perfmon_config_assign_mask[counter_idx] &=\n\t\t\t\t\t\t\t~DEFERRED_ARM_MASK;\n\n\treturn 0;\n}\n\nstatic bool df_v3_6_pmc_is_deferred(struct amdgpu_device *adev,\n\t\t\t\t    uint64_t config,\n\t\t\t\t    int counter_idx)\n{\n\treturn\t(df_v3_6_pmc_has_counter(adev, config, counter_idx) &&\n\t\t\t(adev->df_perfmon_config_assign_mask[counter_idx]\n\t\t\t\t& DEFERRED_ARM_MASK));\n\n}\n\n \nstatic void df_v3_6_pmc_release_cntr(struct amdgpu_device *adev,\n\t\t\t\t     uint64_t config,\n\t\t\t\t     int counter_idx)\n{\n\tif (df_v3_6_pmc_has_counter(adev, config, counter_idx))\n\t\tadev->df_perfmon_config_assign_mask[counter_idx] = 0ULL;\n}\n\n\nstatic void df_v3_6_reset_perfmon_cntr(struct amdgpu_device *adev,\n\t\t\t\t\t uint64_t config,\n\t\t\t\t\t int counter_idx)\n{\n\tuint32_t lo_base_addr = 0, hi_base_addr = 0;\n\n\tdf_v3_6_pmc_get_read_settings(adev, config, counter_idx, &lo_base_addr,\n\t\t\t\t      &hi_base_addr);\n\n\tif ((lo_base_addr == 0) || (hi_base_addr == 0))\n\t\treturn;\n\n\tdf_v3_6_perfmon_wreg(adev, lo_base_addr, 0, hi_base_addr, 0);\n}\n\n \nstatic int df_v3_6_pmc_start(struct amdgpu_device *adev, uint64_t config,\n\t\t\t     int counter_idx, int is_add)\n{\n\tuint32_t lo_base_addr, hi_base_addr, lo_val, hi_val;\n\tint err = 0, ret = 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_VEGA20:\n\tcase CHIP_ARCTURUS:\n\t\tif (is_add)\n\t\t\treturn df_v3_6_pmc_add_cntr(adev, config);\n\n\t\tret = df_v3_6_pmc_get_ctrl_settings(adev,\n\t\t\t\t\tconfig,\n\t\t\t\t\tcounter_idx,\n\t\t\t\t\t&lo_base_addr,\n\t\t\t\t\t&hi_base_addr,\n\t\t\t\t\t&lo_val,\n\t\t\t\t\t&hi_val,\n\t\t\t\t\ttrue);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\terr = df_v3_6_perfmon_arm_with_retry(adev,\n\t\t\t\t\t\t     lo_base_addr,\n\t\t\t\t\t\t     lo_val,\n\t\t\t\t\t\t     hi_base_addr,\n\t\t\t\t\t\t     hi_val);\n\n\t\tif (err)\n\t\t\tret = df_v3_6_pmc_set_deferred(adev, config,\n\t\t\t\t\t\t\tcounter_idx, true);\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int df_v3_6_pmc_stop(struct amdgpu_device *adev, uint64_t config,\n\t\t\t    int counter_idx, int is_remove)\n{\n\tuint32_t lo_base_addr, hi_base_addr, lo_val, hi_val;\n\tint ret = 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_VEGA20:\n\tcase CHIP_ARCTURUS:\n\t\tret = df_v3_6_pmc_get_ctrl_settings(adev,\n\t\t\tconfig,\n\t\t\tcounter_idx,\n\t\t\t&lo_base_addr,\n\t\t\t&hi_base_addr,\n\t\t\t&lo_val,\n\t\t\t&hi_val,\n\t\t\tfalse);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tdf_v3_6_perfmon_wreg(adev, lo_base_addr, lo_val,\n\t\t\t\t\t\t\thi_base_addr, hi_val);\n\n\t\tif (is_remove) {\n\t\t\tdf_v3_6_reset_perfmon_cntr(adev, config, counter_idx);\n\t\t\tdf_v3_6_pmc_release_cntr(adev, config, counter_idx);\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void df_v3_6_pmc_get_count(struct amdgpu_device *adev,\n\t\t\t\t  uint64_t config,\n\t\t\t\t  int counter_idx,\n\t\t\t\t  uint64_t *count)\n{\n\tuint32_t lo_base_addr = 0, hi_base_addr = 0, lo_val = 0, hi_val = 0;\n\t*count = 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_VEGA20:\n\tcase CHIP_ARCTURUS:\n\t\tdf_v3_6_pmc_get_read_settings(adev, config, counter_idx,\n\t\t\t\t\t\t&lo_base_addr, &hi_base_addr);\n\n\t\tif ((lo_base_addr == 0) || (hi_base_addr == 0))\n\t\t\treturn;\n\n\t\t \n\t\tif (df_v3_6_pmc_is_deferred(adev, config, counter_idx)) {\n\t\t\tint rearm_err = df_v3_6_perfmon_arm_with_status(adev,\n\t\t\t\t\t\t\tlo_base_addr, lo_val,\n\t\t\t\t\t\t\thi_base_addr, hi_val);\n\n\t\t\tif (rearm_err)\n\t\t\t\treturn;\n\n\t\t\tdf_v3_6_pmc_set_deferred(adev, config, counter_idx,\n\t\t\t\t\t\t\t\t\tfalse);\n\t\t}\n\n\t\tdf_v3_6_perfmon_rreg(adev, lo_base_addr, &lo_val,\n\t\t\t\thi_base_addr, &hi_val);\n\n\t\t*count  = ((hi_val | 0ULL) << 32) | (lo_val | 0ULL);\n\n\t\tif (*count >= DF_V3_6_PERFMON_OVERFLOW)\n\t\t\t*count = 0;\n\n\t\tDRM_DEBUG_DRIVER(\"config=%llx addr=%08x:%08x val=%08x:%08x\",\n\t\t\t config, lo_base_addr, hi_base_addr, lo_val, hi_val);\n\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic bool df_v3_6_query_ras_poison_mode(struct amdgpu_device *adev)\n{\n\tuint32_t hw_assert_msklo, hw_assert_mskhi;\n\tuint32_t v0, v1, v28, v31;\n\n\thw_assert_msklo = RREG32_SOC15(DF, 0,\n\t\t\t\tmmDF_CS_UMC_AON0_HardwareAssertMaskLow);\n\thw_assert_mskhi = RREG32_SOC15(DF, 0,\n\t\t\t\tmmDF_NCS_PG0_HardwareAssertMaskHigh);\n\n\tv0 = REG_GET_FIELD(hw_assert_msklo,\n\t\tDF_CS_UMC_AON0_HardwareAssertMaskLow, HWAssertMsk0);\n\tv1 = REG_GET_FIELD(hw_assert_msklo,\n\t\tDF_CS_UMC_AON0_HardwareAssertMaskLow, HWAssertMsk1);\n\tv28 = REG_GET_FIELD(hw_assert_mskhi,\n\t\tDF_NCS_PG0_HardwareAssertMaskHigh, HWAssertMsk28);\n\tv31 = REG_GET_FIELD(hw_assert_mskhi,\n\t\tDF_NCS_PG0_HardwareAssertMaskHigh, HWAssertMsk31);\n\n\tif (v0 && v1 && v28 && v31)\n\t\treturn true;\n\telse if (!v0 && !v1 && !v28 && !v31)\n\t\treturn false;\n\telse {\n\t\tdev_warn(adev->dev, \"DF poison setting is inconsistent(%d:%d:%d:%d)!\\n\",\n\t\t\t\tv0, v1, v28, v31);\n\t\treturn false;\n\t}\n}\n\nconst struct amdgpu_df_funcs df_v3_6_funcs = {\n\t.sw_init = df_v3_6_sw_init,\n\t.sw_fini = df_v3_6_sw_fini,\n\t.enable_broadcast_mode = df_v3_6_enable_broadcast_mode,\n\t.get_fb_channel_number = df_v3_6_get_fb_channel_number,\n\t.get_hbm_channel_number = df_v3_6_get_hbm_channel_number,\n\t.update_medium_grain_clock_gating =\n\t\t\tdf_v3_6_update_medium_grain_clock_gating,\n\t.get_clockgating_state = df_v3_6_get_clockgating_state,\n\t.pmc_start = df_v3_6_pmc_start,\n\t.pmc_stop = df_v3_6_pmc_stop,\n\t.pmc_get_count = df_v3_6_pmc_get_count,\n\t.get_fica = df_v3_6_get_fica,\n\t.set_fica = df_v3_6_set_fica,\n\t.query_ras_poison_mode = df_v3_6_query_ras_poison_mode,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}