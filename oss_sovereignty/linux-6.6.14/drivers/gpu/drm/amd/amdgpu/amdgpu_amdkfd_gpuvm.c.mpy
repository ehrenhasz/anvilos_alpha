{
  "module_name": "amdgpu_amdkfd_gpuvm.c",
  "hash_id": "3b1ed0bbb39048cfd23dbae645353b0142e91fff5c43f34c473fb829f6230555",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd_gpuvm.c",
  "human_readable_source": "\n \n#include <linux/dma-buf.h>\n#include <linux/list.h>\n#include <linux/pagemap.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/task.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_object.h\"\n#include \"amdgpu_gem.h\"\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_dma_buf.h\"\n#include <uapi/linux/kfd_ioctl.h>\n#include \"amdgpu_xgmi.h\"\n#include \"kfd_priv.h\"\n#include \"kfd_smi_events.h\"\n\n \n#define AMDGPU_USERPTR_RESTORE_DELAY_MS 1\n\n \n#define VRAM_AVAILABLITY_ALIGN (1 << 21)\n\n \nstatic struct {\n\tuint64_t max_system_mem_limit;\n\tuint64_t max_ttm_mem_limit;\n\tint64_t system_mem_used;\n\tint64_t ttm_mem_used;\n\tspinlock_t mem_limit_lock;\n} kfd_mem_limit;\n\nstatic const char * const domain_bit_to_string[] = {\n\t\t\"CPU\",\n\t\t\"GTT\",\n\t\t\"VRAM\",\n\t\t\"GDS\",\n\t\t\"GWS\",\n\t\t\"OA\"\n};\n\n#define domain_string(domain) domain_bit_to_string[ffs(domain)-1]\n\nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work);\n\nstatic bool kfd_mem_is_attached(struct amdgpu_vm *avm,\n\t\tstruct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list)\n\t\tif (entry->bo_va->base.vm == avm)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool reuse_dmamap(struct amdgpu_device *adev, struct amdgpu_device *bo_adev)\n{\n\treturn (adev->ram_is_direct_mapped && bo_adev->ram_is_direct_mapped) ||\n\t\t\t(adev->dev->iommu_group == bo_adev->dev->iommu_group);\n}\n\n \nvoid amdgpu_amdkfd_gpuvm_init_mem_limits(void)\n{\n\tstruct sysinfo si;\n\tuint64_t mem;\n\n\tif (kfd_mem_limit.max_system_mem_limit)\n\t\treturn;\n\n\tsi_meminfo(&si);\n\tmem = si.freeram - si.freehigh;\n\tmem *= si.mem_unit;\n\n\tspin_lock_init(&kfd_mem_limit.mem_limit_lock);\n\tkfd_mem_limit.max_system_mem_limit = mem - (mem >> 4);\n\tkfd_mem_limit.max_ttm_mem_limit = ttm_tt_pages_limit() << PAGE_SHIFT;\n\tpr_debug(\"Kernel memory limit %lluM, TTM limit %lluM\\n\",\n\t\t(kfd_mem_limit.max_system_mem_limit >> 20),\n\t\t(kfd_mem_limit.max_ttm_mem_limit >> 20));\n}\n\nvoid amdgpu_amdkfd_reserve_system_mem(uint64_t size)\n{\n\tkfd_mem_limit.system_mem_used += size;\n}\n\n \n\n#define ESTIMATE_PT_SIZE(mem_size) max(((mem_size) >> 14), AMDGPU_VM_RESERVED_VRAM)\n\n \nint amdgpu_amdkfd_reserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tsize_t system_mem_needed, ttm_mem_needed, vram_needed;\n\tint ret = 0;\n\tuint64_t vram_size = 0;\n\n\tsystem_mem_needed = 0;\n\tttm_mem_needed = 0;\n\tvram_needed = 0;\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tsystem_mem_needed = size;\n\t\tttm_mem_needed = size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\t \n\t\tvram_needed = size;\n\t\t \n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\treturn -EINVAL;\n\n\t\tvram_size = KFD_XCP_MEMORY_SIZE(adev, xcp_id);\n\t\tif (adev->gmc.is_app_apu) {\n\t\t\tsystem_mem_needed = size;\n\t\t\tttm_mem_needed = size;\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tsystem_mem_needed = size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (kfd_mem_limit.system_mem_used + system_mem_needed >\n\t    kfd_mem_limit.max_system_mem_limit)\n\t\tpr_debug(\"Set no_system_mem_limit=1 if using shared memory\\n\");\n\n\tif ((kfd_mem_limit.system_mem_used + system_mem_needed >\n\t     kfd_mem_limit.max_system_mem_limit && !no_system_mem_limit) ||\n\t    (kfd_mem_limit.ttm_mem_used + ttm_mem_needed >\n\t     kfd_mem_limit.max_ttm_mem_limit) ||\n\t    (adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] + vram_needed >\n\t     vram_size - reserved_for_pt)) {\n\t\tret = -ENOMEM;\n\t\tgoto release;\n\t}\n\n\t \n\tWARN_ONCE(vram_needed && !adev,\n\t\t  \"adev reference can't be null when vram is used\");\n\tif (adev && xcp_id >= 0) {\n\t\tadev->kfd.vram_used[xcp_id] += vram_needed;\n\t\tadev->kfd.vram_used_aligned[xcp_id] += adev->gmc.is_app_apu ?\n\t\t\t\tvram_needed :\n\t\t\t\tALIGN(vram_needed, VRAM_AVAILABLITY_ALIGN);\n\t}\n\tkfd_mem_limit.system_mem_used += system_mem_needed;\n\tkfd_mem_limit.ttm_mem_used += ttm_mem_needed;\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\treturn ret;\n}\n\nvoid amdgpu_amdkfd_unreserve_mem_limit(struct amdgpu_device *adev,\n\t\tuint64_t size, u32 alloc_flag, int8_t xcp_id)\n{\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tWARN_ONCE(!adev,\n\t\t\t  \"adev reference can't be null when alloc mem flags vram is set\");\n\t\tif (WARN_ONCE(xcp_id < 0, \"invalid XCP ID %d\", xcp_id))\n\t\t\tgoto release;\n\n\t\tif (adev) {\n\t\t\tadev->kfd.vram_used[xcp_id] -= size;\n\t\t\tif (adev->gmc.is_app_apu) {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -= size;\n\t\t\t\tkfd_mem_limit.system_mem_used -= size;\n\t\t\t\tkfd_mem_limit.ttm_mem_used -= size;\n\t\t\t} else {\n\t\t\t\tadev->kfd.vram_used_aligned[xcp_id] -=\n\t\t\t\t\tALIGN(size, VRAM_AVAILABLITY_ALIGN);\n\t\t\t}\n\t\t}\n\t} else if (alloc_flag & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\tkfd_mem_limit.system_mem_used -= size;\n\t} else if (!(alloc_flag &\n\t\t\t\t(KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tpr_err(\"%s: Invalid BO type %#x\\n\", __func__, alloc_flag);\n\t\tgoto release;\n\t}\n\tWARN_ONCE(adev && xcp_id >= 0 && adev->kfd.vram_used[xcp_id] < 0,\n\t\t  \"KFD VRAM memory accounting unbalanced for xcp: %d\", xcp_id);\n\tWARN_ONCE(kfd_mem_limit.ttm_mem_used < 0,\n\t\t  \"KFD TTM memory accounting unbalanced\");\n\tWARN_ONCE(kfd_mem_limit.system_mem_used < 0,\n\t\t  \"KFD system memory accounting unbalanced\");\n\nrelease:\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n}\n\nvoid amdgpu_amdkfd_release_notify(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tu32 alloc_flags = bo->kfd_bo->alloc_flags;\n\tu64 size = amdgpu_bo_size(bo);\n\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, size, alloc_flags,\n\t\t\t\t\t  bo->xcp_id);\n\n\tkfree(bo->kfd_bo);\n}\n\n \nstatic int\ncreate_dmamap_sg_bo(struct amdgpu_device *adev,\n\t\t struct kgd_mem *mem, struct amdgpu_bo **bo_out)\n{\n\tstruct drm_gem_object *gem_obj;\n\tint ret;\n\tuint64_t flags = 0;\n\n\tret = amdgpu_bo_reserve(mem->bo, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR)\n\t\tflags |= mem->bo->flags & (AMDGPU_GEM_CREATE_COHERENT |\n\t\t\t\t\tAMDGPU_GEM_CREATE_UNCACHED);\n\n\tret = amdgpu_gem_object_create(adev, mem->bo->tbo.base.size, 1,\n\t\t\tAMDGPU_GEM_DOMAIN_CPU, AMDGPU_GEM_CREATE_PREEMPTIBLE | flags,\n\t\t\tttm_bo_type_sg, mem->bo->tbo.base.resv, &gem_obj, 0);\n\n\tamdgpu_bo_unreserve(mem->bo);\n\n\tif (ret) {\n\t\tpr_err(\"Error in creating DMA mappable SG BO on domain: %d\\n\", ret);\n\t\treturn -EINVAL;\n\t}\n\n\t*bo_out = gem_to_amdgpu_bo(gem_obj);\n\t(*bo_out)->parent = amdgpu_bo_ref(mem->bo);\n\treturn ret;\n}\n\n \nstatic int amdgpu_amdkfd_remove_eviction_fence(struct amdgpu_bo *bo,\n\t\t\t\t\tstruct amdgpu_amdkfd_fence *ef)\n{\n\tstruct dma_fence *replacement;\n\n\tif (!ef)\n\t\treturn -EINVAL;\n\n\t \n\treplacement = dma_fence_get_stub();\n\tdma_resv_replace_fences(bo->tbo.base.resv, ef->base.context,\n\t\t\t\treplacement, DMA_RESV_USAGE_BOOKKEEP);\n\tdma_fence_put(replacement);\n\treturn 0;\n}\n\nint amdgpu_amdkfd_remove_fence_on_pt_pd_bos(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo *root = bo;\n\tstruct amdgpu_vm_bo_base *vm_bo;\n\tstruct amdgpu_vm *vm;\n\tstruct amdkfd_process_info *info;\n\tstruct amdgpu_amdkfd_fence *ef;\n\tint ret;\n\n\t \n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tvm_bo = root->vm_bo;\n\tif (!vm_bo)\n\t\treturn 0;\n\n\tvm = vm_bo->vm;\n\tif (!vm)\n\t\treturn 0;\n\n\tinfo = vm->process_info;\n\tif (!info || !info->eviction_fence)\n\t\treturn 0;\n\n\tef = container_of(dma_fence_get(&info->eviction_fence->base),\n\t\t\tstruct amdgpu_amdkfd_fence, base);\n\n\tBUG_ON(!dma_resv_trylock(bo->tbo.base.resv));\n\tret = amdgpu_amdkfd_remove_eviction_fence(bo, ef);\n\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tdma_fence_put(&ef->base);\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_bo_validate(struct amdgpu_bo *bo, uint32_t domain,\n\t\t\t\t     bool wait)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint ret;\n\n\tif (WARN(amdgpu_ttm_tt_get_usermm(bo->tbo.ttm),\n\t\t \"Called with userptr BO\"))\n\t\treturn -EINVAL;\n\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto validate_fail;\n\tif (wait)\n\t\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\nvalidate_fail:\n\treturn ret;\n}\n\nstatic int amdgpu_amdkfd_validate_vm_bo(void *_unused, struct amdgpu_bo *bo)\n{\n\treturn amdgpu_amdkfd_bo_validate(bo, bo->allowed_domains, false);\n}\n\n \nstatic int vm_validate_pt_pd_bos(struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_validate_pt_bos(adev, vm, amdgpu_amdkfd_validate_vm_bo, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to validate PT BOs\\n\");\n\t\treturn ret;\n\t}\n\n\tvm->pd_phys_addr = amdgpu_gmc_pd_addr(vm->root.bo);\n\n\treturn 0;\n}\n\nstatic int vm_update_pds(struct amdgpu_vm *vm, struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo *pd = vm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\tint ret;\n\n\tret = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_sync_fence(sync, vm->last_update);\n}\n\nstatic uint64_t get_pte_flags(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tuint32_t mapping_flags = AMDGPU_VM_PAGE_READABLE |\n\t\t\t\t AMDGPU_VM_MTYPE_DEFAULT;\n\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_WRITEABLE;\n\tif (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE)\n\t\tmapping_flags |= AMDGPU_VM_PAGE_EXECUTABLE;\n\n\treturn amdgpu_gem_va_map_flags(adev, mapping_flags);\n}\n\n \nstatic struct sg_table *create_sg_table(uint64_t addr, uint32_t size)\n{\n\tstruct sg_table *sg = kmalloc(sizeof(*sg), GFP_KERNEL);\n\n\tif (!sg)\n\t\treturn NULL;\n\tif (sg_alloc_table(sg, 1, GFP_KERNEL)) {\n\t\tkfree(sg);\n\t\treturn NULL;\n\t}\n\tsg_dma_address(sg->sgl) = addr;\n\tsg->sgl->length = size;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\tsg->sgl->dma_length = size;\n#endif\n\treturn sg;\n}\n\nstatic int\nkfd_mem_dmamap_userptr(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *src_ttm = mem->bo->tbo.ttm;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tint ret;\n\n\tif (WARN_ON(ttm->num_pages != src_ttm->num_pages))\n\t\treturn -EINVAL;\n\n\tttm->sg = kmalloc(sizeof(*ttm->sg), GFP_KERNEL);\n\tif (unlikely(!ttm->sg))\n\t\treturn -ENOMEM;\n\n\t \n\tret = sg_alloc_table_from_pages(ttm->sg, src_ttm->pages,\n\t\t\t\t\tttm->num_pages, 0,\n\t\t\t\t\t(u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t\tGFP_KERNEL);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\tret = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (unlikely(ret))\n\t\tgoto release_sg;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tgoto unmap_sg;\n\n\treturn 0;\n\nunmap_sg:\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\nrelease_sg:\n\tpr_err(\"DMA map userptr failed: %d\\n\", ret);\n\tsg_free_table(ttm->sg);\nfree_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tint ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n \nstatic int\nkfd_mem_dmamap_sg_bo(struct kgd_mem *mem,\n\t\t     struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\tdma_addr_t dma_addr;\n\tbool mmio;\n\tint ret;\n\n\t \n\tmmio = (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP);\n\tif (unlikely(ttm->sg)) {\n\t\tpr_err(\"SG Table of %d BO for peer device is UNEXPECTEDLY NON-NULL\", mmio);\n\t\treturn -EINVAL;\n\t}\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_addr = mem->bo->tbo.sg->sgl->dma_address;\n\tpr_debug(\"%d BO size: %d\\n\", mmio, mem->bo->tbo.sg->sgl->length);\n\tpr_debug(\"%d BO address before DMA mapping: %llx\\n\", mmio, dma_addr);\n\tdma_addr = dma_map_resource(adev->dev, dma_addr,\n\t\t\tmem->bo->tbo.sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tret = dma_mapping_error(adev->dev, dma_addr);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tpr_debug(\"%d BO address after DMA mapping: %llx\\n\", mmio, dma_addr);\n\n\tttm->sg = create_sg_table(dma_addr, mem->bo->tbo.sg->sgl->length);\n\tif (unlikely(!ttm->sg)) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap_sg;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(ret))\n\t\tgoto free_sg;\n\n\treturn ret;\n\nfree_sg:\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\nunmap_sg:\n\tdma_unmap_resource(adev->dev, dma_addr, mem->bo->tbo.sg->sgl->length,\n\t\t\t   dir, DMA_ATTR_SKIP_CPU_SYNC);\n\treturn ret;\n}\n\nstatic int\nkfd_mem_dmamap_attachment(struct kgd_mem *mem,\n\t\t\t  struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\treturn 0;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\treturn kfd_mem_dmamap_userptr(mem, attachment);\n\tcase KFD_MEM_ATT_DMABUF:\n\t\treturn kfd_mem_dmamap_dmabuf(attachment);\n\tcase KFD_MEM_ATT_SG:\n\t\treturn kfd_mem_dmamap_sg_bo(mem, attachment);\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\treturn -EINVAL;\n}\n\nstatic void\nkfd_mem_dmaunmap_userptr(struct kgd_mem *mem,\n\t\t\t struct kfd_mem_attachment *attachment)\n{\n\tenum dma_data_direction direction =\n\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tstruct ttm_operation_ctx ctx = {.interruptible = false};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\n\tif (unlikely(!ttm->sg))\n\t\treturn;\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_dmabuf(struct kfd_mem_attachment *attachment)\n{\n\t \n}\n\n \nstatic void\nkfd_mem_dmaunmap_sg_bo(struct kgd_mem *mem,\n\t\t       struct kfd_mem_attachment *attachment)\n{\n\tstruct ttm_operation_ctx ctx = {.interruptible = true};\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\tstruct amdgpu_device *adev = attachment->adev;\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tenum dma_data_direction dir;\n\n\tif (unlikely(!ttm->sg)) {\n\t\tpr_err(\"SG Table of BO is UNEXPECTEDLY NULL\");\n\t\treturn;\n\t}\n\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\tttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\n\tdir = mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tdma_unmap_resource(adev->dev, ttm->sg->sgl->dma_address,\n\t\t\tttm->sg->sgl->length, dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tsg_free_table(ttm->sg);\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\tbo->tbo.sg = NULL;\n}\n\nstatic void\nkfd_mem_dmaunmap_attachment(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *attachment)\n{\n\tswitch (attachment->type) {\n\tcase KFD_MEM_ATT_SHARED:\n\t\tbreak;\n\tcase KFD_MEM_ATT_USERPTR:\n\t\tkfd_mem_dmaunmap_userptr(mem, attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_DMABUF:\n\t\tkfd_mem_dmaunmap_dmabuf(attachment);\n\t\tbreak;\n\tcase KFD_MEM_ATT_SG:\n\t\tkfd_mem_dmaunmap_sg_bo(mem, attachment);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic int kfd_mem_export_dmabuf(struct kgd_mem *mem)\n{\n\tif (!mem->dmabuf) {\n\t\tstruct dma_buf *ret = amdgpu_gem_prime_export(\n\t\t\t&mem->bo->tbo.base,\n\t\t\tmem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE ?\n\t\t\t\tDRM_RDWR : 0);\n\t\tif (IS_ERR(ret))\n\t\t\treturn PTR_ERR(ret);\n\t\tmem->dmabuf = ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nkfd_mem_attach_dmabuf(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\t      struct amdgpu_bo **bo)\n{\n\tstruct drm_gem_object *gobj;\n\tint ret;\n\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\treturn ret;\n\n\tgobj = amdgpu_gem_prime_import(adev_to_drm(adev), mem->dmabuf);\n\tif (IS_ERR(gobj))\n\t\treturn PTR_ERR(gobj);\n\n\t*bo = gem_to_amdgpu_bo(gobj);\n\t(*bo)->flags |= AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\treturn 0;\n}\n\n \nstatic int kfd_mem_attach(struct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tstruct amdgpu_vm *vm, bool is_aql)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(mem->bo->tbo.bdev);\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tuint64_t va = mem->va;\n\tstruct kfd_mem_attachment *attachment[2] = {NULL, NULL};\n\tstruct amdgpu_bo *bo[2] = {NULL, NULL};\n\tbool same_hive = false;\n\tint i, ret;\n\n\tif (!va) {\n\t\tpr_err(\"Invalid VA when adding BO to VM\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif ((adev != bo_adev && !adev->gmc.is_app_apu) &&\n\t    ((mem->domain == AMDGPU_GEM_DOMAIN_VRAM) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL) ||\n\t     (mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP))) {\n\t\tif (mem->domain == AMDGPU_GEM_DOMAIN_VRAM)\n\t\t\tsame_hive = amdgpu_xgmi_same_hive(adev, bo_adev);\n\t\tif (!same_hive && !amdgpu_device_is_peer_accessible(bo_adev, adev))\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i <= is_aql; i++) {\n\t\tattachment[i] = kzalloc(sizeof(*attachment[i]), GFP_KERNEL);\n\t\tif (unlikely(!attachment[i])) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\tpr_debug(\"\\t add VA 0x%llx - 0x%llx to vm %p\\n\", va,\n\t\t\t va + bo_size, vm);\n\n\t\tif ((adev == bo_adev && !(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) ||\n\t\t    (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) && reuse_dmamap(adev, bo_adev)) ||\n\t\t\tsame_hive) {\n\t\t\t \n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = mem->bo;\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (i > 0) {\n\t\t\t \n\t\t\tattachment[i]->type = KFD_MEM_ATT_SHARED;\n\t\t\tbo[i] = bo[0];\n\t\t\tdrm_gem_object_get(&bo[i]->tbo.base);\n\t\t} else if (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\t\t \n\t\t\tattachment[i]->type = KFD_MEM_ATT_USERPTR;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t \n\t\t} else if (mem->bo->tbo.type == ttm_bo_type_sg) {\n\t\t\tWARN_ONCE(!(mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL ||\n\t\t\t\t    mem->alloc_flags & KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP),\n\t\t\t\t  \"Handing invalid SG BO in ATTACH request\");\n\t\t\tattachment[i]->type = KFD_MEM_ATT_SG;\n\t\t\tret = create_dmamap_sg_bo(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t \n\t\t} else if (mem->domain == AMDGPU_GEM_DOMAIN_GTT ||\n\t\t\t   mem->domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tattachment[i]->type = KFD_MEM_ATT_DMABUF;\n\t\t\tret = kfd_mem_attach_dmabuf(adev, mem, &bo[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto unwind;\n\t\t\tpr_debug(\"Employ DMABUF mechanism to enable peer GPU access\\n\");\n\t\t} else {\n\t\t\tWARN_ONCE(true, \"Handling invalid ATTACH request\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto unwind;\n\t\t}\n\n\t\t \n\t\tret = amdgpu_bo_reserve(bo[i], false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Unable to reserve BO during memory attach\");\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->bo_va = amdgpu_vm_bo_add(adev, vm, bo[i]);\n\t\tamdgpu_bo_unreserve(bo[i]);\n\t\tif (unlikely(!attachment[i]->bo_va)) {\n\t\t\tret = -ENOMEM;\n\t\t\tpr_err(\"Failed to add BO object to VM. ret == %d\\n\",\n\t\t\t       ret);\n\t\t\tgoto unwind;\n\t\t}\n\t\tattachment[i]->va = va;\n\t\tattachment[i]->pte_flags = get_pte_flags(adev, mem);\n\t\tattachment[i]->adev = adev;\n\t\tlist_add(&attachment[i]->list, &mem->attachments);\n\n\t\tva += bo_size;\n\t}\n\n\treturn 0;\n\nunwind:\n\tfor (; i >= 0; i--) {\n\t\tif (!attachment[i])\n\t\t\tcontinue;\n\t\tif (attachment[i]->bo_va) {\n\t\t\tamdgpu_bo_reserve(bo[i], true);\n\t\t\tamdgpu_vm_bo_del(adev, attachment[i]->bo_va);\n\t\t\tamdgpu_bo_unreserve(bo[i]);\n\t\t\tlist_del(&attachment[i]->list);\n\t\t}\n\t\tif (bo[i])\n\t\t\tdrm_gem_object_put(&bo[i]->tbo.base);\n\t\tkfree(attachment[i]);\n\t}\n\treturn ret;\n}\n\nstatic void kfd_mem_detach(struct kfd_mem_attachment *attachment)\n{\n\tstruct amdgpu_bo *bo = attachment->bo_va->base.bo;\n\n\tpr_debug(\"\\t remove VA 0x%llx in entry %p\\n\",\n\t\t\tattachment->va, attachment);\n\tamdgpu_vm_bo_del(attachment->adev, attachment->bo_va);\n\tdrm_gem_object_put(&bo->tbo.base);\n\tlist_del(&attachment->list);\n\tkfree(attachment);\n}\n\nstatic void add_kgd_mem_to_kfd_bo_list(struct kgd_mem *mem,\n\t\t\t\tstruct amdkfd_process_info *process_info,\n\t\t\t\tbool userptr)\n{\n\tmutex_lock(&process_info->lock);\n\tif (userptr)\n\t\tlist_add_tail(&mem->validate_list,\n\t\t\t      &process_info->userptr_valid_list);\n\telse\n\t\tlist_add_tail(&mem->validate_list, &process_info->kfd_bo_list);\n\tmutex_unlock(&process_info->lock);\n}\n\nstatic void remove_kgd_mem_from_kfd_bo_list(struct kgd_mem *mem,\n\t\tstruct amdkfd_process_info *process_info)\n{\n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n}\n\n \nstatic int init_user_pages(struct kgd_mem *mem, uint64_t user_addr,\n\t\t\t   bool criu_resume)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct hmm_range *range;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->lock);\n\n\tret = amdgpu_ttm_tt_set_userptr(&bo->tbo, user_addr, 0);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to set userptr: %d\\n\", __func__, ret);\n\t\tgoto out;\n\t}\n\n\tret = amdgpu_hmm_register(bo, user_addr);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to register MMU notifier: %d\\n\",\n\t\t       __func__, ret);\n\t\tgoto out;\n\t}\n\n\tif (criu_resume) {\n\t\t \n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tmem->invalid++;\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t\tmutex_unlock(&process_info->lock);\n\t\treturn 0;\n\t}\n\n\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages, &range);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to get user pages: %d\\n\", __func__, ret);\n\t\tgoto unregister_out;\n\t}\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"%s: Failed to reserve BO\\n\", __func__);\n\t\tgoto release_out;\n\t}\n\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (ret)\n\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\tamdgpu_bo_unreserve(bo);\n\nrelease_out:\n\tamdgpu_ttm_tt_get_user_pages_done(bo->tbo.ttm, range);\nunregister_out:\n\tif (ret)\n\t\tamdgpu_hmm_unregister(bo);\nout:\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\n \nstruct bo_vm_reservation_context {\n\t \n\tstruct drm_exec exec;\n\t \n\tunsigned int n_vms;\n\t \n\tstruct amdgpu_sync *sync;\n};\n\nenum bo_vm_match {\n\tBO_VM_NOT_MAPPED = 0,\t \n\tBO_VM_MAPPED,\t\t \n\tBO_VM_ALL,\t\t \n};\n\n \nstatic int reserve_bo_and_vm(struct kgd_mem *mem,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct bo_vm_reservation_context *ctx)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tWARN_ON(!vm);\n\n\tctx->n_vms = 1;\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tret = amdgpu_vm_lock_pd(vm, &ctx->exec, 2);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n \nstatic int reserve_bo_and_cond_vms(struct kgd_mem *mem,\n\t\t\t\tstruct amdgpu_vm *vm, enum bo_vm_match map_type,\n\t\t\t\tstruct bo_vm_reservation_context *ctx)\n{\n\tstruct kfd_mem_attachment *entry;\n\tstruct amdgpu_bo *bo = mem->bo;\n\tint ret;\n\n\tctx->sync = &mem->sync;\n\tdrm_exec_init(&ctx->exec, DRM_EXEC_INTERRUPTIBLE_WAIT);\n\tdrm_exec_until_all_locked(&ctx->exec) {\n\t\tctx->n_vms = 0;\n\t\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\t\tif ((vm && vm != entry->bo_va->base.vm) ||\n\t\t\t\t(entry->is_mapped != map_type\n\t\t\t\t&& map_type != BO_VM_ALL))\n\t\t\t\tcontinue;\n\n\t\t\tret = amdgpu_vm_lock_pd(entry->bo_va->base.vm,\n\t\t\t\t\t\t&ctx->exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto error;\n\t\t\t++ctx->n_vms;\n\t\t}\n\n\t\tret = drm_exec_prepare_obj(&ctx->exec, &bo->tbo.base, 1);\n\t\tdrm_exec_retry_on_contention(&ctx->exec);\n\t\tif (unlikely(ret))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tpr_err(\"Failed to reserve buffers in ttm.\\n\");\n\tdrm_exec_fini(&ctx->exec);\n\treturn ret;\n}\n\n \nstatic int unreserve_bo_and_vms(struct bo_vm_reservation_context *ctx,\n\t\t\t\t bool wait, bool intr)\n{\n\tint ret = 0;\n\n\tif (wait)\n\t\tret = amdgpu_sync_wait(ctx->sync, intr);\n\n\tdrm_exec_fini(&ctx->exec);\n\tctx->sync = NULL;\n\treturn ret;\n}\n\nstatic void unmap_bo_from_gpuvm(struct kgd_mem *mem,\n\t\t\t\tstruct kfd_mem_attachment *entry,\n\t\t\t\tstruct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\n\tamdgpu_vm_bo_unmap(adev, bo_va, entry->va);\n\n\tamdgpu_vm_clear_freed(adev, vm, &bo_va->last_pt_update);\n\n\tamdgpu_sync_fence(sync, bo_va->last_pt_update);\n\n\tkfd_mem_dmaunmap_attachment(mem, entry);\n}\n\nstatic int update_gpuvm_pte(struct kgd_mem *mem,\n\t\t\t    struct kfd_mem_attachment *entry,\n\t\t\t    struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_bo_va *bo_va = entry->bo_va;\n\tstruct amdgpu_device *adev = entry->adev;\n\tint ret;\n\n\tret = kfd_mem_dmamap_attachment(mem, entry);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (ret) {\n\t\tpr_err(\"amdgpu_vm_bo_update failed\\n\");\n\t\treturn ret;\n\t}\n\n\treturn amdgpu_sync_fence(sync, bo_va->last_pt_update);\n}\n\nstatic int map_bo_to_gpuvm(struct kgd_mem *mem,\n\t\t\t   struct kfd_mem_attachment *entry,\n\t\t\t   struct amdgpu_sync *sync,\n\t\t\t   bool no_update_pte)\n{\n\tint ret;\n\n\t \n\tret = amdgpu_vm_bo_map(entry->adev, entry->bo_va, entry->va, 0,\n\t\t\t       amdgpu_bo_size(entry->bo_va->base.bo),\n\t\t\t       entry->pte_flags);\n\tif (ret) {\n\t\tpr_err(\"Failed to map VA 0x%llx in vm. ret %d\\n\",\n\t\t\t\tentry->va, ret);\n\t\treturn ret;\n\t}\n\n\tif (no_update_pte)\n\t\treturn 0;\n\n\tret = update_gpuvm_pte(mem, entry, sync);\n\tif (ret) {\n\t\tpr_err(\"update_gpuvm_pte() failed\\n\");\n\t\tgoto update_gpuvm_pte_failed;\n\t}\n\n\treturn 0;\n\nupdate_gpuvm_pte_failed:\n\tunmap_bo_from_gpuvm(mem, entry, sync);\n\treturn ret;\n}\n\nstatic int process_validate_vms(struct amdkfd_process_info *process_info)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_validate_pt_pd_bos(peer_vm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_sync_pds_resv(struct amdkfd_process_info *process_info,\n\t\t\t\t struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *pd = peer_vm->root.bo;\n\n\t\tret = amdgpu_sync_resv(NULL, sync, pd->tbo.base.resv,\n\t\t\t\t       AMDGPU_SYNC_NE_OWNER,\n\t\t\t\t       AMDGPU_FENCE_OWNER_KFD);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_update_pds(struct amdkfd_process_info *process_info,\n\t\t\t      struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_vm *peer_vm;\n\tint ret;\n\n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tret = vm_update_pds(peer_vm, sync);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int init_kfd_vm(struct amdgpu_vm *vm, void **process_info,\n\t\t       struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *info = NULL;\n\tint ret;\n\n\tif (!*process_info) {\n\t\tinfo = kzalloc(sizeof(*info), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tmutex_init(&info->lock);\n\t\tmutex_init(&info->notifier_lock);\n\t\tINIT_LIST_HEAD(&info->vm_list_head);\n\t\tINIT_LIST_HEAD(&info->kfd_bo_list);\n\t\tINIT_LIST_HEAD(&info->userptr_valid_list);\n\t\tINIT_LIST_HEAD(&info->userptr_inval_list);\n\n\t\tinfo->eviction_fence =\n\t\t\tamdgpu_amdkfd_fence_create(dma_fence_context_alloc(1),\n\t\t\t\t\t\t   current->mm,\n\t\t\t\t\t\t   NULL);\n\t\tif (!info->eviction_fence) {\n\t\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto create_evict_fence_fail;\n\t\t}\n\n\t\tinfo->pid = get_task_pid(current->group_leader, PIDTYPE_PID);\n\t\tINIT_DELAYED_WORK(&info->restore_userptr_work,\n\t\t\t\t  amdgpu_amdkfd_restore_userptr_worker);\n\n\t\t*process_info = info;\n\t\t*ef = dma_fence_get(&info->eviction_fence->base);\n\t}\n\n\tvm->process_info = *process_info;\n\n\t \n\tret = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (ret)\n\t\tgoto reserve_pd_fail;\n\tret = vm_validate_pt_pd_bos(vm);\n\tif (ret) {\n\t\tpr_err(\"validate_pt_pd_bos() failed\\n\");\n\t\tgoto validate_pd_fail;\n\t}\n\tret = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t  AMDGPU_FENCE_OWNER_KFD, false);\n\tif (ret)\n\t\tgoto wait_pd_fail;\n\tret = dma_resv_reserve_fences(vm->root.bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(vm->root.bo->tbo.base.resv,\n\t\t\t   &vm->process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\n\t \n\tmutex_lock(&vm->process_info->lock);\n\tlist_add_tail(&vm->vm_list_node,\n\t\t\t&(vm->process_info->vm_list_head));\n\tvm->process_info->n_vms++;\n\tmutex_unlock(&vm->process_info->lock);\n\n\treturn 0;\n\nreserve_shared_fail:\nwait_pd_fail:\nvalidate_pd_fail:\n\tamdgpu_bo_unreserve(vm->root.bo);\nreserve_pd_fail:\n\tvm->process_info = NULL;\n\tif (info) {\n\t\t \n\t\tdma_fence_put(&info->eviction_fence->base);\n\t\tdma_fence_put(*ef);\n\t\t*ef = NULL;\n\t\t*process_info = NULL;\n\t\tput_pid(info->pid);\ncreate_evict_fence_fail:\n\t\tmutex_destroy(&info->lock);\n\t\tmutex_destroy(&info->notifier_lock);\n\t\tkfree(info);\n\t}\n\treturn ret;\n}\n\n \nstatic int amdgpu_amdkfd_gpuvm_pin_bo(struct amdgpu_bo *bo, u32 domain)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n\tif (ret)\n\t\tpr_err(\"Error in Pinning BO to domain: %d\\n\", domain);\n\n\tamdgpu_bo_sync_wait(bo, AMDGPU_FENCE_OWNER_KFD, false);\n\tamdgpu_bo_unreserve(bo);\n\n\treturn ret;\n}\n\n \nstatic void amdgpu_amdkfd_gpuvm_unpin_bo(struct amdgpu_bo *bo)\n{\n\tint ret = 0;\n\n\tret = amdgpu_bo_reserve(bo, false);\n\tif (unlikely(ret))\n\t\treturn;\n\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_set_vm_pasid(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_vm *avm, u32 pasid)\n\n{\n\tint ret;\n\n\t \n\tif (avm->pasid) {\n\t\tamdgpu_pasid_free(avm->pasid);\n\t\tamdgpu_vm_set_pasid(adev, avm, 0);\n\t}\n\n\tret = amdgpu_vm_set_pasid(adev, avm, pasid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_acquire_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *avm,\n\t\t\t\t\t   void **process_info,\n\t\t\t\t\t   struct dma_fence **ef)\n{\n\tint ret;\n\n\t \n\tif (avm->process_info)\n\t\treturn -EINVAL;\n\n\t \n\tret = amdgpu_vm_make_compute(adev, avm);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = init_kfd_vm(avm, process_info, ef);\n\tif (ret)\n\t\treturn ret;\n\n\tamdgpu_vm_set_task_info(avm);\n\n\treturn 0;\n}\n\nvoid amdgpu_amdkfd_gpuvm_destroy_cb(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm *vm)\n{\n\tstruct amdkfd_process_info *process_info = vm->process_info;\n\n\tif (!process_info)\n\t\treturn;\n\n\t \n\tmutex_lock(&process_info->lock);\n\tprocess_info->n_vms--;\n\tlist_del(&vm->vm_list_node);\n\tmutex_unlock(&process_info->lock);\n\n\tvm->process_info = NULL;\n\n\t \n\tif (!process_info->n_vms) {\n\t\tWARN_ON(!list_empty(&process_info->kfd_bo_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_valid_list));\n\t\tWARN_ON(!list_empty(&process_info->userptr_inval_list));\n\n\t\tdma_fence_put(&process_info->eviction_fence->base);\n\t\tcancel_delayed_work_sync(&process_info->restore_userptr_work);\n\t\tput_pid(process_info->pid);\n\t\tmutex_destroy(&process_info->lock);\n\t\tmutex_destroy(&process_info->notifier_lock);\n\t\tkfree(process_info);\n\t}\n}\n\nvoid amdgpu_amdkfd_gpuvm_release_process_vm(struct amdgpu_device *adev,\n\t\t\t\t\t    void *drm_priv)\n{\n\tstruct amdgpu_vm *avm;\n\n\tif (WARN_ON(!adev || !drm_priv))\n\t\treturn;\n\n\tavm = drm_priv_to_vm(drm_priv);\n\n\tpr_debug(\"Releasing process vm %p\\n\", avm);\n\n\t \n\tamdgpu_vm_release_compute(adev, avm);\n}\n\nuint64_t amdgpu_amdkfd_gpuvm_get_process_page_dir(void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_bo *pd = avm->root.bo;\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(pd->tbo.bdev);\n\n\tif (adev->asic_type < CHIP_VEGA10)\n\t\treturn avm->pd_phys_addr >> AMDGPU_GPU_PAGE_SHIFT;\n\treturn avm->pd_phys_addr;\n}\n\nvoid amdgpu_amdkfd_block_mmu_notifications(void *p)\n{\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tWRITE_ONCE(pinfo->block_mmu_notifications, true);\n\tmutex_unlock(&pinfo->lock);\n}\n\nint amdgpu_amdkfd_criu_resume(void *p)\n{\n\tint ret = 0;\n\tstruct amdkfd_process_info *pinfo = (struct amdkfd_process_info *)p;\n\n\tmutex_lock(&pinfo->lock);\n\tpr_debug(\"scheduling work\\n\");\n\tmutex_lock(&pinfo->notifier_lock);\n\tpinfo->evicted_bos++;\n\tmutex_unlock(&pinfo->notifier_lock);\n\tif (!READ_ONCE(pinfo->block_mmu_notifications)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tWRITE_ONCE(pinfo->block_mmu_notifications, false);\n\tschedule_delayed_work(&pinfo->restore_userptr_work, 0);\n\nout_unlock:\n\tmutex_unlock(&pinfo->lock);\n\treturn ret;\n}\n\nsize_t amdgpu_amdkfd_get_available_memory(struct amdgpu_device *adev,\n\t\t\t\t\t  uint8_t xcp_id)\n{\n\tuint64_t reserved_for_pt =\n\t\tESTIMATE_PT_SIZE(amdgpu_amdkfd_total_mem_size);\n\tssize_t available;\n\tuint64_t vram_available, system_mem_available, ttm_mem_available;\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tvram_available = KFD_XCP_MEMORY_SIZE(adev, xcp_id)\n\t\t- adev->kfd.vram_used_aligned[xcp_id]\n\t\t- atomic64_read(&adev->vram_pin_size)\n\t\t- reserved_for_pt;\n\n\tif (adev->gmc.is_app_apu) {\n\t\tsystem_mem_available = no_system_mem_limit ?\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit :\n\t\t\t\t\tkfd_mem_limit.max_system_mem_limit -\n\t\t\t\t\tkfd_mem_limit.system_mem_used;\n\n\t\tttm_mem_available = kfd_mem_limit.max_ttm_mem_limit -\n\t\t\t\tkfd_mem_limit.ttm_mem_used;\n\n\t\tavailable = min3(system_mem_available, ttm_mem_available,\n\t\t\t\t vram_available);\n\t\tavailable = ALIGN_DOWN(available, PAGE_SIZE);\n\t} else {\n\t\tavailable = ALIGN_DOWN(vram_available, VRAM_AVAILABLITY_ALIGN);\n\t}\n\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\tif (available < 0)\n\t\tavailable = 0;\n\n\treturn available;\n}\n\nint amdgpu_amdkfd_gpuvm_alloc_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, uint64_t va, uint64_t size,\n\t\tvoid *drm_priv, struct kgd_mem **mem,\n\t\tuint64_t *offset, uint32_t flags, bool criu_resume)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdgpu_fpriv *fpriv = container_of(avm, struct amdgpu_fpriv, vm);\n\tenum ttm_bo_type bo_type = ttm_bo_type_device;\n\tstruct sg_table *sg = NULL;\n\tuint64_t user_addr = 0;\n\tstruct amdgpu_bo *bo;\n\tstruct drm_gem_object *gobj = NULL;\n\tu32 domain, alloc_domain;\n\tuint64_t aligned_size;\n\tint8_t xcp_id = -1;\n\tu64 alloc_flags;\n\tint ret;\n\n\t \n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_VRAM) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\tif (adev->gmc.is_app_apu) {\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\t\talloc_flags = 0;\n\t\t} else {\n\t\t\talloc_flags = AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\t\t\talloc_flags |= (flags & KFD_IOC_ALLOC_MEM_FLAGS_PUBLIC) ?\n\t\t\tAMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED : 0;\n\t\t}\n\t\txcp_id = fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION ?\n\t\t\t\t\t0 : fpriv->xcp_id;\n\t} else if (flags & KFD_IOC_ALLOC_MEM_FLAGS_GTT) {\n\t\tdomain = alloc_domain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_flags = 0;\n\t} else {\n\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\talloc_domain = AMDGPU_GEM_DOMAIN_CPU;\n\t\talloc_flags = AMDGPU_GEM_CREATE_PREEMPTIBLE;\n\n\t\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_USERPTR) {\n\t\t\tif (!offset || !*offset)\n\t\t\t\treturn -EINVAL;\n\t\t\tuser_addr = untagged_addr(*offset);\n\t\t} else if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\t    KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\t\tbo_type = ttm_bo_type_sg;\n\t\t\tif (size > UINT_MAX)\n\t\t\t\treturn -EINVAL;\n\t\t\tsg = create_sg_table(*offset, size);\n\t\t\tif (!sg)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_COHERENT)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_COHERENT;\n\tif (flags & KFD_IOC_ALLOC_MEM_FLAGS_UNCACHED)\n\t\talloc_flags |= AMDGPU_GEM_CREATE_UNCACHED;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\t(*mem)->aql_queue = !!(flags & KFD_IOC_ALLOC_MEM_FLAGS_AQL_QUEUE_MEM);\n\n\t \n\tif ((*mem)->aql_queue)\n\t\tsize >>= 1;\n\taligned_size = PAGE_ALIGN(size);\n\n\t(*mem)->alloc_flags = flags;\n\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\tret = amdgpu_amdkfd_reserve_mem_limit(adev, aligned_size, flags,\n\t\t\t\t\t      xcp_id);\n\tif (ret) {\n\t\tpr_debug(\"Insufficient memory\\n\");\n\t\tgoto err_reserve_limit;\n\t}\n\n\tpr_debug(\"\\tcreate BO VA 0x%llx size 0x%llx domain %s xcp_id %d\\n\",\n\t\t va, (*mem)->aql_queue ? size << 1 : size,\n\t\t domain_string(alloc_domain), xcp_id);\n\n\tret = amdgpu_gem_object_create(adev, aligned_size, 1, alloc_domain, alloc_flags,\n\t\t\t\t       bo_type, NULL, &gobj, xcp_id + 1);\n\tif (ret) {\n\t\tpr_debug(\"Failed to create BO on domain %s. ret %d\\n\",\n\t\t\t domain_string(alloc_domain), ret);\n\t\tgoto err_bo_create;\n\t}\n\tret = drm_vma_node_allow(&gobj->vma_node, drm_priv);\n\tif (ret) {\n\t\tpr_debug(\"Failed to allow vma node access. ret %d\\n\", ret);\n\t\tgoto err_node_allow;\n\t}\n\tbo = gem_to_amdgpu_bo(gobj);\n\tif (bo_type == ttm_bo_type_sg) {\n\t\tbo->tbo.sg = sg;\n\t\tbo->tbo.ttm->sg = sg;\n\t}\n\tbo->kfd_bo = *mem;\n\t(*mem)->bo = bo;\n\tif (user_addr)\n\t\tbo->flags |= AMDGPU_AMDKFD_CREATE_USERPTR_BO;\n\n\t(*mem)->va = va;\n\t(*mem)->domain = domain;\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, user_addr);\n\n\tif (user_addr) {\n\t\tpr_debug(\"creating userptr BO for user_addr = %llx\\n\", user_addr);\n\t\tret = init_user_pages(*mem, user_addr, criu_resume);\n\t\tif (ret)\n\t\t\tgoto allocate_init_user_pages_failed;\n\t} else  if (flags & (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t\t\t\tKFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tret = amdgpu_amdkfd_gpuvm_pin_bo(bo, AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (ret) {\n\t\t\tpr_err(\"Pinning MMIO/DOORBELL BO during ALLOC FAILED\\n\");\n\t\t\tgoto err_pin_bo;\n\t\t}\n\t\tbo->allowed_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t\tbo->preferred_domains = AMDGPU_GEM_DOMAIN_GTT;\n\t}\n\n\tif (offset)\n\t\t*offset = amdgpu_bo_mmap_offset(bo);\n\n\treturn 0;\n\nallocate_init_user_pages_failed:\nerr_pin_bo:\n\tremove_kgd_mem_from_kfd_bo_list(*mem, avm->process_info);\n\tdrm_vma_node_revoke(&gobj->vma_node, drm_priv);\nerr_node_allow:\n\t \n\tgoto err_reserve_limit;\nerr_bo_create:\n\tamdgpu_amdkfd_unreserve_mem_limit(adev, aligned_size, flags, xcp_id);\nerr_reserve_limit:\n\tmutex_destroy(&(*mem)->lock);\n\tif (gobj)\n\t\tdrm_gem_object_put(gobj);\n\telse\n\t\tkfree(*mem);\nerr:\n\tif (sg) {\n\t\tsg_free_table(sg);\n\t\tkfree(sg);\n\t}\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_free_memory_of_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv,\n\t\tuint64_t *size)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tbool use_release_notifier = (mem->bo->kfd_bo == mem);\n\tstruct kfd_mem_attachment *entry, *tmp;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned int mapped_to_gpu_memory;\n\tint ret;\n\tbool is_imported = false;\n\n\tmutex_lock(&mem->lock);\n\n\t \n\tif (mem->alloc_flags &\n\t    (KFD_IOC_ALLOC_MEM_FLAGS_DOORBELL |\n\t     KFD_IOC_ALLOC_MEM_FLAGS_MMIO_REMAP)) {\n\t\tamdgpu_amdkfd_gpuvm_unpin_bo(mem->bo);\n\t}\n\n\tmapped_to_gpu_memory = mem->mapped_to_gpu_memory;\n\tis_imported = mem->is_imported;\n\tmutex_unlock(&mem->lock);\n\t \n\n\tif (mapped_to_gpu_memory > 0) {\n\t\tpr_debug(\"BO VA 0x%llx size 0x%lx is still mapped.\\n\",\n\t\t\t\tmem->va, bo_size);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tmutex_lock(&process_info->lock);\n\tlist_del(&mem->validate_list);\n\tmutex_unlock(&process_info->lock);\n\n\t \n\tif (amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm)) {\n\t\tamdgpu_hmm_unregister(mem->bo);\n\t\tmutex_lock(&process_info->notifier_lock);\n\t\tamdgpu_ttm_tt_discard_user_pages(mem->bo->tbo.ttm, mem->range);\n\t\tmutex_unlock(&process_info->notifier_lock);\n\t}\n\n\tret = reserve_bo_and_cond_vms(mem, NULL, BO_VM_ALL, &ctx);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\t \n\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\tprocess_info->eviction_fence);\n\tpr_debug(\"Release VA 0x%llx - 0x%llx\\n\", mem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue));\n\n\t \n\tlist_for_each_entry_safe(entry, tmp, &mem->attachments, list)\n\t\tkfd_mem_detach(entry);\n\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\t \n\tamdgpu_sync_free(&mem->sync);\n\n\t \n\tif (mem->bo->tbo.sg) {\n\t\tsg_free_table(mem->bo->tbo.sg);\n\t\tkfree(mem->bo->tbo.sg);\n\t}\n\n\t \n\tif (size) {\n\t\tif (!is_imported &&\n\t\t   (mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_VRAM ||\n\t\t   (adev->gmc.is_app_apu &&\n\t\t    mem->bo->preferred_domains == AMDGPU_GEM_DOMAIN_GTT)))\n\t\t\t*size = bo_size;\n\t\telse\n\t\t\t*size = 0;\n\t}\n\n\t \n\tdrm_vma_node_revoke(&mem->bo->tbo.base.vma_node, drm_priv);\n\tif (mem->dmabuf)\n\t\tdma_buf_put(mem->dmabuf);\n\tmutex_destroy(&mem->lock);\n\n\t \n\tdrm_gem_object_put(&mem->bo->tbo.base);\n\n\t \n\tif (!use_release_notifier)\n\t\tkfree(mem);\n\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_map_memory_to_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem,\n\t\tvoid *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tint ret;\n\tstruct amdgpu_bo *bo;\n\tuint32_t domain;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tunsigned long bo_size;\n\tbool is_invalid_userptr = false;\n\n\tbo = mem->bo;\n\tif (!bo) {\n\t\tpr_err(\"Invalid BO when mapping memory to GPU\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmutex_lock(&mem->process_info->lock);\n\n\t \n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tmutex_lock(&mem->process_info->notifier_lock);\n\t\tis_invalid_userptr = !!mem->invalid;\n\t\tmutex_unlock(&mem->process_info->notifier_lock);\n\t}\n\n\tmutex_lock(&mem->lock);\n\n\tdomain = mem->domain;\n\tbo_size = bo->tbo.base.size;\n\n\tpr_debug(\"Map VA 0x%llx - 0x%llx to vm %p domain %s\\n\",\n\t\t\tmem->va,\n\t\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\t\tavm, domain_string(domain));\n\n\tif (!kfd_mem_is_attached(avm, mem)) {\n\t\tret = kfd_mem_attach(adev, mem, avm, mem->aql_queue);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = reserve_bo_and_vm(mem, avm, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\t \n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_SYSTEM)\n\t\tis_invalid_userptr = true;\n\n\tret = vm_validate_pt_pd_bos(avm);\n\tif (unlikely(ret))\n\t\tgoto out_unreserve;\n\n\tif (mem->mapped_to_gpu_memory == 0 &&\n\t    !amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\t \n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, true);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Validate failed\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\t}\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t map VA 0x%llx - 0x%llx in entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tret = map_bo_to_gpuvm(mem, entry, ctx.sync,\n\t\t\t\t      is_invalid_userptr);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to map bo to gpuvm\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tret = vm_update_pds(avm, ctx.sync);\n\t\tif (ret) {\n\t\t\tpr_err(\"Failed to update page directories\\n\");\n\t\t\tgoto out_unreserve;\n\t\t}\n\n\t\tentry->is_mapped = true;\n\t\tmem->mapped_to_gpu_memory++;\n\t\tpr_debug(\"\\t INC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\tif (!amdgpu_ttm_tt_get_usermm(bo->tbo.ttm) && !bo->tbo.pin_count)\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &avm->process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tret = unreserve_bo_and_vms(&ctx, false, false);\n\n\tgoto out;\n\nout_unreserve:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->process_info->lock);\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_unmap_memory_from_gpu(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, void *drm_priv)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct amdkfd_process_info *process_info = avm->process_info;\n\tunsigned long bo_size = mem->bo->tbo.base.size;\n\tstruct kfd_mem_attachment *entry;\n\tstruct bo_vm_reservation_context ctx;\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\n\tret = reserve_bo_and_cond_vms(mem, avm, BO_VM_MAPPED, &ctx);\n\tif (unlikely(ret))\n\t\tgoto out;\n\t \n\tif (ctx.n_vms == 0) {\n\t\tret = -EINVAL;\n\t\tgoto unreserve_out;\n\t}\n\n\tret = vm_validate_pt_pd_bos(avm);\n\tif (unlikely(ret))\n\t\tgoto unreserve_out;\n\n\tpr_debug(\"Unmap VA 0x%llx - 0x%llx from vm %p\\n\",\n\t\tmem->va,\n\t\tmem->va + bo_size * (1 + mem->aql_queue),\n\t\tavm);\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->bo_va->base.vm != avm || !entry->is_mapped)\n\t\t\tcontinue;\n\n\t\tpr_debug(\"\\t unmap VA 0x%llx - 0x%llx from entry %p\\n\",\n\t\t\t entry->va, entry->va + bo_size, entry);\n\n\t\tunmap_bo_from_gpuvm(mem, entry, ctx.sync);\n\t\tentry->is_mapped = false;\n\n\t\tmem->mapped_to_gpu_memory--;\n\t\tpr_debug(\"\\t DEC mapping count %d\\n\",\n\t\t\t mem->mapped_to_gpu_memory);\n\t}\n\n\t \n\tif (mem->mapped_to_gpu_memory == 0 &&\n\t    !amdgpu_ttm_tt_get_usermm(mem->bo->tbo.ttm) &&\n\t    !mem->bo->tbo.pin_count)\n\t\tamdgpu_amdkfd_remove_eviction_fence(mem->bo,\n\t\t\t\t\t\tprocess_info->eviction_fence);\n\nunreserve_out:\n\tunreserve_bo_and_vms(&ctx, false, false);\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_sync_memory(\n\t\tstruct amdgpu_device *adev, struct kgd_mem *mem, bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tmutex_lock(&mem->lock);\n\tamdgpu_sync_clone(&mem->sync, &sync);\n\tmutex_unlock(&mem->lock);\n\n\tret = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn ret;\n}\n\n \nint amdgpu_amdkfd_map_gtt_bo_to_gart(struct amdgpu_device *adev, struct amdgpu_bo *bo)\n{\n\tint ret;\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto err_reserve_bo_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto err_pin_bo_failed;\n\t}\n\n\tret = amdgpu_ttm_alloc_gart(&bo->tbo);\n\tif (ret) {\n\t\tpr_err(\"Failed to bind bo to GART. ret %d\\n\", ret);\n\t\tgoto err_map_bo_gart_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, bo->vm_bo->vm->process_info->eviction_fence);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tbo = amdgpu_bo_ref(bo);\n\n\treturn 0;\n\nerr_map_bo_gart_failed:\n\tamdgpu_bo_unpin(bo);\nerr_pin_bo_failed:\n\tamdgpu_bo_unreserve(bo);\nerr_reserve_bo_failed:\n\n\treturn ret;\n}\n\n \nint amdgpu_amdkfd_gpuvm_map_gtt_bo_to_kernel(struct kgd_mem *mem,\n\t\t\t\t\t     void **kptr, uint64_t *size)\n{\n\tint ret;\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm)) {\n\t\tpr_err(\"userptr can't be mapped to kernel\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&mem->process_info->lock);\n\n\tret = amdgpu_bo_reserve(bo, true);\n\tif (ret) {\n\t\tpr_err(\"Failed to reserve bo. ret %d\\n\", ret);\n\t\tgoto bo_reserve_failed;\n\t}\n\n\tret = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_GTT);\n\tif (ret) {\n\t\tpr_err(\"Failed to pin bo. ret %d\\n\", ret);\n\t\tgoto pin_failed;\n\t}\n\n\tret = amdgpu_bo_kmap(bo, kptr);\n\tif (ret) {\n\t\tpr_err(\"Failed to map bo to kernel. ret %d\\n\", ret);\n\t\tgoto kmap_failed;\n\t}\n\n\tamdgpu_amdkfd_remove_eviction_fence(\n\t\tbo, mem->process_info->eviction_fence);\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tamdgpu_bo_unreserve(bo);\n\n\tmutex_unlock(&mem->process_info->lock);\n\treturn 0;\n\nkmap_failed:\n\tamdgpu_bo_unpin(bo);\npin_failed:\n\tamdgpu_bo_unreserve(bo);\nbo_reserve_failed:\n\tmutex_unlock(&mem->process_info->lock);\n\n\treturn ret;\n}\n\n \nvoid amdgpu_amdkfd_gpuvm_unmap_gtt_bo_from_kernel(struct kgd_mem *mem)\n{\n\tstruct amdgpu_bo *bo = mem->bo;\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_unreserve(bo);\n}\n\nint amdgpu_amdkfd_gpuvm_get_vm_fault_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct kfd_vm_fault_info *mem)\n{\n\tif (atomic_read(&adev->gmc.vm_fault_info_updated) == 1) {\n\t\t*mem = *adev->gmc.vm_fault_info;\n\t\tmb();  \n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\t}\n\treturn 0;\n}\n\nint amdgpu_amdkfd_gpuvm_import_dmabuf(struct amdgpu_device *adev,\n\t\t\t\t      struct dma_buf *dma_buf,\n\t\t\t\t      uint64_t va, void *drm_priv,\n\t\t\t\t      struct kgd_mem **mem, uint64_t *size,\n\t\t\t\t      uint64_t *mmap_offset)\n{\n\tstruct amdgpu_vm *avm = drm_priv_to_vm(drm_priv);\n\tstruct drm_gem_object *obj;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tobj = amdgpu_gem_prime_import(adev_to_drm(adev), dma_buf);\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\tbo = gem_to_amdgpu_bo(obj);\n\tif (!(bo->preferred_domains & (AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT))) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto err_put_obj;\n\t}\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem) {\n\t\tret = -ENOMEM;\n\t\tgoto err_put_obj;\n\t}\n\n\tret = drm_vma_node_allow(&obj->vma_node, drm_priv);\n\tif (ret)\n\t\tgoto err_free_mem;\n\n\tif (size)\n\t\t*size = amdgpu_bo_size(bo);\n\n\tif (mmap_offset)\n\t\t*mmap_offset = amdgpu_bo_mmap_offset(bo);\n\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\tmutex_init(&(*mem)->lock);\n\n\t(*mem)->alloc_flags =\n\t\t((bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) ?\n\t\tKFD_IOC_ALLOC_MEM_FLAGS_VRAM : KFD_IOC_ALLOC_MEM_FLAGS_GTT)\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_WRITABLE\n\t\t| KFD_IOC_ALLOC_MEM_FLAGS_EXECUTABLE;\n\n\tget_dma_buf(dma_buf);\n\t(*mem)->dmabuf = dma_buf;\n\t(*mem)->bo = bo;\n\t(*mem)->va = va;\n\t(*mem)->domain = (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) && !adev->gmc.is_app_apu ?\n\t\tAMDGPU_GEM_DOMAIN_VRAM : AMDGPU_GEM_DOMAIN_GTT;\n\n\t(*mem)->mapped_to_gpu_memory = 0;\n\t(*mem)->process_info = avm->process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, avm->process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\t(*mem)->is_imported = true;\n\n\treturn 0;\n\nerr_free_mem:\n\tkfree(*mem);\nerr_put_obj:\n\tdrm_gem_object_put(obj);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_gpuvm_export_dmabuf(struct kgd_mem *mem,\n\t\t\t\t      struct dma_buf **dma_buf)\n{\n\tint ret;\n\n\tmutex_lock(&mem->lock);\n\tret = kfd_mem_export_dmabuf(mem);\n\tif (ret)\n\t\tgoto out;\n\n\tget_dma_buf(mem->dmabuf);\n\t*dma_buf = mem->dmabuf;\nout:\n\tmutex_unlock(&mem->lock);\n\treturn ret;\n}\n\n \nint amdgpu_amdkfd_evict_userptr(struct mmu_interval_notifier *mni,\n\t\t\t\tunsigned long cur_seq, struct kgd_mem *mem)\n{\n\tstruct amdkfd_process_info *process_info = mem->process_info;\n\tint r = 0;\n\n\t \n\tif (READ_ONCE(process_info->block_mmu_notifications))\n\t\treturn 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tmmu_interval_set_seq(mni, cur_seq);\n\n\tmem->invalid++;\n\tif (++process_info->evicted_bos == 1) {\n\t\t \n\t\tr = kgd2kfd_quiesce_mm(mni->mm,\n\t\t\t\t       KFD_QUEUE_EVICTION_TRIGGER_USERPTR);\n\t\tif (r)\n\t\t\tpr_err(\"Failed to quiesce KFD\\n\");\n\t\tschedule_delayed_work(&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\t}\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn r;\n}\n\n \nstatic int update_invalid_user_pages(struct amdkfd_process_info *process_info,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tuint32_t invalid;\n\tint ret = 0;\n\n\tmutex_lock(&process_info->notifier_lock);\n\n\t \n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_valid_list,\n\t\t\t\t validate_list)\n\t\tif (mem->invalid)\n\t\t\tlist_move_tail(&mem->validate_list,\n\t\t\t\t       &process_info->userptr_inval_list);\n\n\t \n\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t    validate_list) {\n\t\tinvalid = mem->invalid;\n\t\tif (!invalid)\n\t\t\t \n\t\t\tcontinue;\n\n\t\tbo = mem->bo;\n\n\t\tamdgpu_ttm_tt_discard_user_pages(bo->tbo.ttm, mem->range);\n\t\tmem->range = NULL;\n\n\t\t \n\t\tmutex_unlock(&process_info->notifier_lock);\n\n\t\t \n\t\tif (bo->tbo.resource->mem_type != TTM_PL_SYSTEM) {\n\t\t\tif (amdgpu_bo_reserve(bo, true))\n\t\t\t\treturn -EAGAIN;\n\t\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tamdgpu_bo_unreserve(bo);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: Failed to invalidate userptr BO\\n\",\n\t\t\t\t       __func__);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tret = amdgpu_ttm_tt_get_user_pages(bo, bo->tbo.ttm->pages,\n\t\t\t\t\t\t   &mem->range);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Failed %d to get user pages\\n\", ret);\n\n\t\t\t \n\t\t\tif (ret != -EFAULT)\n\t\t\t\treturn ret;\n\n\t\t\tret = 0;\n\t\t}\n\n\t\tmutex_lock(&process_info->notifier_lock);\n\n\t\t \n\t\tif (mem->invalid != invalid) {\n\t\t\tret = -EAGAIN;\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t  \n\t\tif (mem->range)\n\t\t\tmem->invalid = 0;\n\t}\n\nunlock_out:\n\tmutex_unlock(&process_info->notifier_lock);\n\n\treturn ret;\n}\n\n \nstatic int validate_invalid_user_pages(struct amdkfd_process_info *process_info)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem, *tmp_mem;\n\tstruct amdgpu_bo *bo;\n\tint ret;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0);\n\t \n\tdrm_exec_until_all_locked(&exec) {\n\t\t \n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(mem, &process_info->userptr_inval_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto unreserve_out;\n\t\t}\n\t}\n\n\tret = process_validate_vms(process_info);\n\tif (ret)\n\t\tgoto unreserve_out;\n\n\t \n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tstruct kfd_mem_attachment *attachment;\n\n\t\tbo = mem->bo;\n\n\t\t \n\t\tif (bo->tbo.ttm->pages[0]) {\n\t\t\tamdgpu_bo_placement_from_domain(bo, mem->domain);\n\t\t\tret = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: failed to validate BO\\n\", __func__);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"%s: update PTE failed\\n\", __func__);\n\t\t\t\t \n\t\t\t\tmutex_lock(&process_info->notifier_lock);\n\t\t\t\tmem->invalid++;\n\t\t\t\tmutex_unlock(&process_info->notifier_lock);\n\t\t\t\tgoto unreserve_out;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tret = process_update_pds(process_info, &sync);\n\nunreserve_out:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_wait(&sync, false);\n\tamdgpu_sync_free(&sync);\n\n\treturn ret;\n}\n\n \nstatic int confirm_valid_user_pages_locked(struct amdkfd_process_info *process_info)\n{\n\tstruct kgd_mem *mem, *tmp_mem;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(mem, tmp_mem,\n\t\t\t\t &process_info->userptr_inval_list,\n\t\t\t\t validate_list) {\n\t\tbool valid;\n\n\t\t \n\t\tif (!mem->range)\n\t\t\t continue;\n\n\t\t \n\t\tvalid = amdgpu_ttm_tt_get_user_pages_done(\n\t\t\t\t\tmem->bo->tbo.ttm, mem->range);\n\n\t\tmem->range = NULL;\n\t\tif (!valid) {\n\t\t\tWARN(!mem->invalid, \"Invalid BO not marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mem->invalid) {\n\t\t\tWARN(1, \"Valid BO is marked invalid\");\n\t\t\tret = -EAGAIN;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlist_move_tail(&mem->validate_list,\n\t\t\t       &process_info->userptr_valid_list);\n\t}\n\n\treturn ret;\n}\n\n \nstatic void amdgpu_amdkfd_restore_userptr_worker(struct work_struct *work)\n{\n\tstruct delayed_work *dwork = to_delayed_work(work);\n\tstruct amdkfd_process_info *process_info =\n\t\tcontainer_of(dwork, struct amdkfd_process_info,\n\t\t\t     restore_userptr_work);\n\tstruct task_struct *usertask;\n\tstruct mm_struct *mm;\n\tuint32_t evicted_bos;\n\n\tmutex_lock(&process_info->notifier_lock);\n\tevicted_bos = process_info->evicted_bos;\n\tmutex_unlock(&process_info->notifier_lock);\n\tif (!evicted_bos)\n\t\treturn;\n\n\t \n\tusertask = get_pid_task(process_info->pid, PIDTYPE_PID);\n\tif (!usertask)\n\t\treturn;\n\tmm = get_task_mm(usertask);\n\tif (!mm) {\n\t\tput_task_struct(usertask);\n\t\treturn;\n\t}\n\n\tmutex_lock(&process_info->lock);\n\n\tif (update_invalid_user_pages(process_info, mm))\n\t\tgoto unlock_out;\n\t \n\tif (!list_empty(&process_info->userptr_inval_list)) {\n\t\tif (validate_invalid_user_pages(process_info))\n\t\t\tgoto unlock_out;\n\t}\n\t \n\tmutex_lock(&process_info->notifier_lock);\n\tif (process_info->evicted_bos != evicted_bos)\n\t\tgoto unlock_notifier_out;\n\n\tif (confirm_valid_user_pages_locked(process_info)) {\n\t\tWARN(1, \"User pages unexpectedly invalid\");\n\t\tgoto unlock_notifier_out;\n\t}\n\n\tprocess_info->evicted_bos = evicted_bos = 0;\n\n\tif (kgd2kfd_resume_mm(mm)) {\n\t\tpr_err(\"%s: Failed to resume KFD\\n\", __func__);\n\t\t \n\t}\n\nunlock_notifier_out:\n\tmutex_unlock(&process_info->notifier_lock);\nunlock_out:\n\tmutex_unlock(&process_info->lock);\n\n\t \n\tif (evicted_bos) {\n\t\tschedule_delayed_work(&process_info->restore_userptr_work,\n\t\t\tmsecs_to_jiffies(AMDGPU_USERPTR_RESTORE_DELAY_MS));\n\n\t\tkfd_smi_event_queue_restore_rescheduled(mm);\n\t}\n\tmmput(mm);\n\tput_task_struct(usertask);\n}\n\n \nint amdgpu_amdkfd_gpuvm_restore_process_bos(void *info, struct dma_fence **ef)\n{\n\tstruct amdkfd_process_info *process_info = info;\n\tstruct amdgpu_vm *peer_vm;\n\tstruct kgd_mem *mem;\n\tstruct amdgpu_amdkfd_fence *new_fence;\n\tstruct list_head duplicate_save;\n\tstruct amdgpu_sync sync_obj;\n\tunsigned long failed_size = 0;\n\tunsigned long total_size = 0;\n\tstruct drm_exec exec;\n\tint ret;\n\n\tINIT_LIST_HEAD(&duplicate_save);\n\n\tmutex_lock(&process_info->lock);\n\n\tdrm_exec_init(&exec, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t\t    vm_list_node) {\n\t\t\tret = amdgpu_vm_lock_pd(peer_vm, &exec, 2);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t\t    validate_list) {\n\t\t\tstruct drm_gem_object *gobj;\n\n\t\t\tgobj = &mem->bo->tbo.base;\n\t\t\tret = drm_exec_prepare_obj(&exec, gobj, 1);\n\t\t\tdrm_exec_retry_on_contention(&exec);\n\t\t\tif (unlikely(ret))\n\t\t\t\tgoto ttm_reserve_fail;\n\t\t}\n\t}\n\n\tamdgpu_sync_create(&sync_obj);\n\n\t \n\tret = process_validate_vms(process_info);\n\tif (ret)\n\t\tgoto validate_map_fail;\n\n\tret = process_sync_pds_resv(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: Failed to sync to PD BO moving fence. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t \n\tlist_for_each_entry(mem, &process_info->kfd_bo_list,\n\t\t\t    validate_list) {\n\n\t\tstruct amdgpu_bo *bo = mem->bo;\n\t\tuint32_t domain = mem->domain;\n\t\tstruct kfd_mem_attachment *attachment;\n\t\tstruct dma_resv_iter cursor;\n\t\tstruct dma_fence *fence;\n\n\t\ttotal_size += amdgpu_bo_size(bo);\n\n\t\tret = amdgpu_amdkfd_bo_validate(bo, domain, false);\n\t\tif (ret) {\n\t\t\tpr_debug(\"Memory eviction: Validate BOs failed\\n\");\n\t\t\tfailed_size += amdgpu_bo_size(bo);\n\t\t\tret = amdgpu_amdkfd_bo_validate(bo,\n\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT, false);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tdma_resv_for_each_fence(&cursor, bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_KERNEL, fence) {\n\t\t\tret = amdgpu_sync_fence(&sync_obj, fence);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: Sync BO fence failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t\tlist_for_each_entry(attachment, &mem->attachments, list) {\n\t\t\tif (!attachment->is_mapped)\n\t\t\t\tcontinue;\n\n\t\t\tif (attachment->bo_va->base.bo->tbo.pin_count)\n\t\t\t\tcontinue;\n\n\t\t\tkfd_mem_dmaunmap_attachment(mem, attachment);\n\t\t\tret = update_gpuvm_pte(mem, attachment, &sync_obj);\n\t\t\tif (ret) {\n\t\t\t\tpr_debug(\"Memory eviction: update PTE failed. Try again\\n\");\n\t\t\t\tgoto validate_map_fail;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (failed_size)\n\t\tpr_debug(\"0x%lx/0x%lx in system\\n\", failed_size, total_size);\n\n\t \n\tret = process_update_pds(process_info, &sync_obj);\n\tif (ret) {\n\t\tpr_debug(\"Memory eviction: update PDs failed. Try again\\n\");\n\t\tgoto validate_map_fail;\n\t}\n\n\t \n\tamdgpu_sync_wait(&sync_obj, false);\n\n\t \n\tnew_fence = amdgpu_amdkfd_fence_create(\n\t\t\t\tprocess_info->eviction_fence->base.context,\n\t\t\t\tprocess_info->eviction_fence->mm,\n\t\t\t\tNULL);\n\tif (!new_fence) {\n\t\tpr_err(\"Failed to create eviction fence\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto validate_map_fail;\n\t}\n\tdma_fence_put(&process_info->eviction_fence->base);\n\tprocess_info->eviction_fence = new_fence;\n\t*ef = dma_fence_get(&new_fence->base);\n\n\t \n\tlist_for_each_entry(mem, &process_info->kfd_bo_list, validate_list) {\n\t\tif (mem->bo->tbo.pin_count)\n\t\t\tcontinue;\n\n\t\tdma_resv_add_fence(mem->bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\t \n\tlist_for_each_entry(peer_vm, &process_info->vm_list_head,\n\t\t\t    vm_list_node) {\n\t\tstruct amdgpu_bo *bo = peer_vm->root.bo;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv,\n\t\t\t\t   &process_info->eviction_fence->base,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\nvalidate_map_fail:\n\tamdgpu_sync_free(&sync_obj);\nttm_reserve_fail:\n\tdrm_exec_fini(&exec);\n\tmutex_unlock(&process_info->lock);\n\treturn ret;\n}\n\nint amdgpu_amdkfd_add_gws_to_process(void *info, void *gws, struct kgd_mem **mem)\n{\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct amdgpu_bo *gws_bo = (struct amdgpu_bo *)gws;\n\tint ret;\n\n\tif (!info || !gws)\n\t\treturn -EINVAL;\n\n\t*mem = kzalloc(sizeof(struct kgd_mem), GFP_KERNEL);\n\tif (!*mem)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&(*mem)->lock);\n\tINIT_LIST_HEAD(&(*mem)->attachments);\n\t(*mem)->bo = amdgpu_bo_ref(gws_bo);\n\t(*mem)->domain = AMDGPU_GEM_DOMAIN_GWS;\n\t(*mem)->process_info = process_info;\n\tadd_kgd_mem_to_kfd_bo_list(*mem, process_info, false);\n\tamdgpu_sync_create(&(*mem)->sync);\n\n\n\t \n\tmutex_lock(&(*mem)->process_info->lock);\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\tgoto bo_reservation_failure;\n\t}\n\n\tret = amdgpu_amdkfd_bo_validate(gws_bo, AMDGPU_GEM_DOMAIN_GWS, true);\n\tif (ret) {\n\t\tpr_err(\"GWS BO validate failed %d\\n\", ret);\n\t\tgoto bo_validation_failure;\n\t}\n\t \n\tret = dma_resv_reserve_fences(gws_bo->tbo.base.resv, 1);\n\tif (ret)\n\t\tgoto reserve_shared_fail;\n\tdma_resv_add_fence(gws_bo->tbo.base.resv,\n\t\t\t   &process_info->eviction_fence->base,\n\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\tamdgpu_bo_unreserve(gws_bo);\n\tmutex_unlock(&(*mem)->process_info->lock);\n\n\treturn ret;\n\nreserve_shared_fail:\nbo_validation_failure:\n\tamdgpu_bo_unreserve(gws_bo);\nbo_reservation_failure:\n\tmutex_unlock(&(*mem)->process_info->lock);\n\tamdgpu_sync_free(&(*mem)->sync);\n\tremove_kgd_mem_from_kfd_bo_list(*mem, process_info);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&(*mem)->lock);\n\tkfree(*mem);\n\t*mem = NULL;\n\treturn ret;\n}\n\nint amdgpu_amdkfd_remove_gws_from_process(void *info, void *mem)\n{\n\tint ret;\n\tstruct amdkfd_process_info *process_info = (struct amdkfd_process_info *)info;\n\tstruct kgd_mem *kgd_mem = (struct kgd_mem *)mem;\n\tstruct amdgpu_bo *gws_bo = kgd_mem->bo;\n\n\t \n\tremove_kgd_mem_from_kfd_bo_list(kgd_mem, process_info);\n\n\tret = amdgpu_bo_reserve(gws_bo, false);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"Reserve gws bo failed %d\\n\", ret);\n\t\t \n\t\treturn ret;\n\t}\n\tamdgpu_amdkfd_remove_eviction_fence(gws_bo,\n\t\t\tprocess_info->eviction_fence);\n\tamdgpu_bo_unreserve(gws_bo);\n\tamdgpu_sync_free(&kgd_mem->sync);\n\tamdgpu_bo_unref(&gws_bo);\n\tmutex_destroy(&kgd_mem->lock);\n\tkfree(mem);\n\treturn 0;\n}\n\n \nint amdgpu_amdkfd_get_tile_config(struct amdgpu_device *adev,\n\t\t\t\tstruct tile_config *config)\n{\n\tconfig->gb_addr_config = adev->gfx.config.gb_addr_config;\n\tconfig->tile_config_ptr = adev->gfx.config.tile_mode_array;\n\tconfig->num_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.tile_mode_array);\n\tconfig->macro_tile_config_ptr =\n\t\t\tadev->gfx.config.macrotile_mode_array;\n\tconfig->num_macro_tile_configs =\n\t\t\tARRAY_SIZE(adev->gfx.config.macrotile_mode_array);\n\n\t \n\tconfig->num_banks = adev->gfx.config.num_banks;\n\tconfig->num_ranks = adev->gfx.config.num_ranks;\n\n\treturn 0;\n}\n\nbool amdgpu_amdkfd_bo_mapped_to_dev(struct amdgpu_device *adev, struct kgd_mem *mem)\n{\n\tstruct kfd_mem_attachment *entry;\n\n\tlist_for_each_entry(entry, &mem->attachments, list) {\n\t\tif (entry->is_mapped && entry->adev == adev)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nint kfd_debugfs_kfd_mem_limits(struct seq_file *m, void *data)\n{\n\n\tspin_lock(&kfd_mem_limit.mem_limit_lock);\n\tseq_printf(m, \"System mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.system_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_system_mem_limit >> 20));\n\tseq_printf(m, \"TTM mem used %lldM out of %lluM\\n\",\n\t\t  (kfd_mem_limit.ttm_mem_used >> 20),\n\t\t  (kfd_mem_limit.max_ttm_mem_limit >> 20));\n\tspin_unlock(&kfd_mem_limit.mem_limit_lock);\n\n\treturn 0;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}