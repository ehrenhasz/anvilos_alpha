{
  "module_name": "amdgpu_device.c",
  "hash_id": "5444926ad0ee5e51e2a4c97cbd8ac108818d00bf25b2fde888e22cc2f1df9d4a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c",
  "human_readable_source": " \n#include <linux/power_supply.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/console.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/pci.h>\n#include <linux/devcoredump.h>\n#include <generated/utsrelease.h>\n#include <linux/pci-p2pdma.h>\n#include <linux/apple-gmux.h>\n\n#include <drm/drm_aperture.h>\n#include <drm/drm_atomic_helper.h>\n#include <drm/drm_crtc_helper.h>\n#include <drm/drm_fb_helper.h>\n#include <drm/drm_probe_helper.h>\n#include <drm/amdgpu_drm.h>\n#include <linux/device.h>\n#include <linux/vgaarb.h>\n#include <linux/vga_switcheroo.h>\n#include <linux/efi.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_i2c.h\"\n#include \"atom.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amd_pcie.h\"\n#ifdef CONFIG_DRM_AMDGPU_SI\n#include \"si.h\"\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n#include \"cik.h\"\n#endif\n#include \"vi.h\"\n#include \"soc15.h\"\n#include \"nv.h\"\n#include \"bif/bif_4_1_d.h\"\n#include <linux/firmware.h>\n#include \"amdgpu_vf_error.h\"\n\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_pm.h\"\n\n#include \"amdgpu_xgmi.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_pmu.h\"\n#include \"amdgpu_fru_eeprom.h\"\n#include \"amdgpu_reset.h\"\n\n#include <linux/suspend.h>\n#include <drm/task_barrier.h>\n#include <linux/pm_runtime.h>\n\n#include <drm/drm_drv.h>\n\n#if IS_ENABLED(CONFIG_X86)\n#include <asm/intel-family.h>\n#endif\n\nMODULE_FIRMWARE(\"amdgpu/vega10_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega12_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/raven_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/picasso_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/raven2_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/arcturus_gpu_info.bin\");\nMODULE_FIRMWARE(\"amdgpu/navi12_gpu_info.bin\");\n\n#define AMDGPU_RESUME_MS\t\t2000\n#define AMDGPU_MAX_RETRY_LIMIT\t\t2\n#define AMDGPU_RETRY_SRIOV_RESET(r) ((r) == -EBUSY || (r) == -ETIMEDOUT || (r) == -EINVAL)\n\nstatic const struct drm_driver amdgpu_kms_driver;\n\nconst char *amdgpu_asic_name[] = {\n\t\"TAHITI\",\n\t\"PITCAIRN\",\n\t\"VERDE\",\n\t\"OLAND\",\n\t\"HAINAN\",\n\t\"BONAIRE\",\n\t\"KAVERI\",\n\t\"KABINI\",\n\t\"HAWAII\",\n\t\"MULLINS\",\n\t\"TOPAZ\",\n\t\"TONGA\",\n\t\"FIJI\",\n\t\"CARRIZO\",\n\t\"STONEY\",\n\t\"POLARIS10\",\n\t\"POLARIS11\",\n\t\"POLARIS12\",\n\t\"VEGAM\",\n\t\"VEGA10\",\n\t\"VEGA12\",\n\t\"VEGA20\",\n\t\"RAVEN\",\n\t\"ARCTURUS\",\n\t\"RENOIR\",\n\t\"ALDEBARAN\",\n\t\"NAVI10\",\n\t\"CYAN_SKILLFISH\",\n\t\"NAVI14\",\n\t\"NAVI12\",\n\t\"SIENNA_CICHLID\",\n\t\"NAVY_FLOUNDER\",\n\t\"VANGOGH\",\n\t\"DIMGREY_CAVEFISH\",\n\t\"BEIGE_GOBY\",\n\t\"YELLOW_CARP\",\n\t\"IP DISCOVERY\",\n\t\"LAST\",\n};\n\n \n\nstatic ssize_t amdgpu_device_get_pcie_replay_count(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tuint64_t cnt = amdgpu_asic_get_pcie_replay_count(adev);\n\n\treturn sysfs_emit(buf, \"%llu\\n\", cnt);\n}\n\nstatic DEVICE_ATTR(pcie_replay_count, 0444,\n\t\tamdgpu_device_get_pcie_replay_count, NULL);\n\nstatic void amdgpu_device_get_pcie_info(struct amdgpu_device *adev);\n\n\n \nbool amdgpu_device_supports_px(struct drm_device *dev)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\tif ((adev->flags & AMD_IS_PX) && !amdgpu_is_atpx_hybrid())\n\t\treturn true;\n\treturn false;\n}\n\n \nbool amdgpu_device_supports_boco(struct drm_device *dev)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\tif (adev->has_pr3 ||\n\t    ((adev->flags & AMD_IS_PX) && amdgpu_is_atpx_hybrid()))\n\t\treturn true;\n\treturn false;\n}\n\n \nbool amdgpu_device_supports_baco(struct drm_device *dev)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\treturn amdgpu_asic_supports_baco(adev);\n}\n\n \nbool amdgpu_device_supports_smart_shift(struct drm_device *dev)\n{\n\treturn (amdgpu_device_supports_boco(dev) &&\n\t\tamdgpu_acpi_is_power_shift_control_supported());\n}\n\n \n\n \nvoid amdgpu_device_mm_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t     void *buf, size_t size, bool write)\n{\n\tunsigned long flags;\n\tuint32_t hi = ~0, tmp = 0;\n\tuint32_t *data = buf;\n\tuint64_t last;\n\tint idx;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn;\n\n\tBUG_ON(!IS_ALIGNED(pos, 4) || !IS_ALIGNED(size, 4));\n\n\tspin_lock_irqsave(&adev->mmio_idx_lock, flags);\n\tfor (last = pos + size; pos < last; pos += 4) {\n\t\ttmp = pos >> 31;\n\n\t\tWREG32_NO_KIQ(mmMM_INDEX, ((uint32_t)pos) | 0x80000000);\n\t\tif (tmp != hi) {\n\t\t\tWREG32_NO_KIQ(mmMM_INDEX_HI, tmp);\n\t\t\thi = tmp;\n\t\t}\n\t\tif (write)\n\t\t\tWREG32_NO_KIQ(mmMM_DATA, *data++);\n\t\telse\n\t\t\t*data++ = RREG32_NO_KIQ(mmMM_DATA);\n\t}\n\n\tspin_unlock_irqrestore(&adev->mmio_idx_lock, flags);\n\tdrm_dev_exit(idx);\n}\n\n \nsize_t amdgpu_device_aper_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t\t void *buf, size_t size, bool write)\n{\n#ifdef CONFIG_64BIT\n\tvoid __iomem *addr;\n\tsize_t count = 0;\n\tuint64_t last;\n\n\tif (!adev->mman.aper_base_kaddr)\n\t\treturn 0;\n\n\tlast = min(pos + size, adev->gmc.visible_vram_size);\n\tif (last > pos) {\n\t\taddr = adev->mman.aper_base_kaddr + pos;\n\t\tcount = last - pos;\n\n\t\tif (write) {\n\t\t\tmemcpy_toio(addr, buf, count);\n\t\t\t \n\t\t\tmb();\n\t\t\tamdgpu_device_flush_hdp(adev, NULL);\n\t\t} else {\n\t\t\tamdgpu_device_invalidate_hdp(adev, NULL);\n\t\t\t \n\t\t\tmb();\n\t\t\tmemcpy_fromio(buf, addr, count);\n\t\t}\n\n\t}\n\n\treturn count;\n#else\n\treturn 0;\n#endif\n}\n\n \nvoid amdgpu_device_vram_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t       void *buf, size_t size, bool write)\n{\n\tsize_t count;\n\n\t \n\tcount = amdgpu_device_aper_access(adev, pos, buf, size, write);\n\tsize -= count;\n\tif (size) {\n\t\t \n\t\tpos += count;\n\t\tbuf += count;\n\t\tamdgpu_device_mm_access(adev, pos, buf, size, write);\n\t}\n}\n\n \n\n \nbool amdgpu_device_skip_hw_access(struct amdgpu_device *adev)\n{\n\tif (adev->no_hw_access)\n\t\treturn true;\n\n#ifdef CONFIG_LOCKDEP\n\t \n\tif (in_task()) {\n\t\tif (down_read_trylock(&adev->reset_domain->sem))\n\t\t\tup_read(&adev->reset_domain->sem);\n\t\telse\n\t\t\tlockdep_assert_held(&adev->reset_domain->sem);\n\t}\n#endif\n\treturn false;\n}\n\n \nuint32_t amdgpu_device_rreg(struct amdgpu_device *adev,\n\t\t\t    uint32_t reg, uint32_t acc_flags)\n{\n\tuint32_t ret;\n\n\tif (amdgpu_device_skip_hw_access(adev))\n\t\treturn 0;\n\n\tif ((reg * 4) < adev->rmmio_size) {\n\t\tif (!(acc_flags & AMDGPU_REGS_NO_KIQ) &&\n\t\t    amdgpu_sriov_runtime(adev) &&\n\t\t    down_read_trylock(&adev->reset_domain->sem)) {\n\t\t\tret = amdgpu_kiq_rreg(adev, reg);\n\t\t\tup_read(&adev->reset_domain->sem);\n\t\t} else {\n\t\t\tret = readl(((void __iomem *)adev->rmmio) + (reg * 4));\n\t\t}\n\t} else {\n\t\tret = adev->pcie_rreg(adev, reg * 4);\n\t}\n\n\ttrace_amdgpu_device_rreg(adev->pdev->device, reg, ret);\n\n\treturn ret;\n}\n\n \n\n \nuint8_t amdgpu_mm_rreg8(struct amdgpu_device *adev, uint32_t offset)\n{\n\tif (amdgpu_device_skip_hw_access(adev))\n\t\treturn 0;\n\n\tif (offset < adev->rmmio_size)\n\t\treturn (readb(adev->rmmio + offset));\n\tBUG();\n}\n\n \n\n \nvoid amdgpu_mm_wreg8(struct amdgpu_device *adev, uint32_t offset, uint8_t value)\n{\n\tif (amdgpu_device_skip_hw_access(adev))\n\t\treturn;\n\n\tif (offset < adev->rmmio_size)\n\t\twriteb(value, adev->rmmio + offset);\n\telse\n\t\tBUG();\n}\n\n \nvoid amdgpu_device_wreg(struct amdgpu_device *adev,\n\t\t\tuint32_t reg, uint32_t v,\n\t\t\tuint32_t acc_flags)\n{\n\tif (amdgpu_device_skip_hw_access(adev))\n\t\treturn;\n\n\tif ((reg * 4) < adev->rmmio_size) {\n\t\tif (!(acc_flags & AMDGPU_REGS_NO_KIQ) &&\n\t\t    amdgpu_sriov_runtime(adev) &&\n\t\t    down_read_trylock(&adev->reset_domain->sem)) {\n\t\t\tamdgpu_kiq_wreg(adev, reg, v);\n\t\t\tup_read(&adev->reset_domain->sem);\n\t\t} else {\n\t\t\twritel(v, ((void __iomem *)adev->rmmio) + (reg * 4));\n\t\t}\n\t} else {\n\t\tadev->pcie_wreg(adev, reg * 4, v);\n\t}\n\n\ttrace_amdgpu_device_wreg(adev->pdev->device, reg, v);\n}\n\n \nvoid amdgpu_mm_wreg_mmio_rlc(struct amdgpu_device *adev,\n\t\t\t     uint32_t reg, uint32_t v,\n\t\t\t     uint32_t xcc_id)\n{\n\tif (amdgpu_device_skip_hw_access(adev))\n\t\treturn;\n\n\tif (amdgpu_sriov_fullaccess(adev) &&\n\t    adev->gfx.rlc.funcs &&\n\t    adev->gfx.rlc.funcs->is_rlcg_access_range) {\n\t\tif (adev->gfx.rlc.funcs->is_rlcg_access_range(adev, reg))\n\t\t\treturn amdgpu_sriov_wreg(adev, reg, v, 0, 0, xcc_id);\n\t} else if ((reg * 4) >= adev->rmmio_size) {\n\t\tadev->pcie_wreg(adev, reg * 4, v);\n\t} else {\n\t\twritel(v, ((void __iomem *)adev->rmmio) + (reg * 4));\n\t}\n}\n\n \nu32 amdgpu_device_indirect_rreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg_addr)\n{\n\tunsigned long flags, pcie_index, pcie_data;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_data_offset;\n\tu32 r;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\tr = readl(pcie_data_offset);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\n\treturn r;\n}\n\nu32 amdgpu_device_indirect_rreg_ext(struct amdgpu_device *adev,\n\t\t\t\t    u64 reg_addr)\n{\n\tunsigned long flags, pcie_index, pcie_index_hi, pcie_data;\n\tu32 r;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_index_hi_offset;\n\tvoid __iomem *pcie_data_offset;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\tif (adev->nbio.funcs->get_pcie_index_hi_offset)\n\t\tpcie_index_hi = adev->nbio.funcs->get_pcie_index_hi_offset(adev);\n\telse\n\t\tpcie_index_hi = 0;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\tif (pcie_index_hi != 0)\n\t\tpcie_index_hi_offset = (void __iomem *)adev->rmmio +\n\t\t\t\tpcie_index_hi * 4;\n\n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\tif (pcie_index_hi != 0) {\n\t\twritel((reg_addr >> 32) & 0xff, pcie_index_hi_offset);\n\t\treadl(pcie_index_hi_offset);\n\t}\n\tr = readl(pcie_data_offset);\n\n\t \n\tif (pcie_index_hi != 0) {\n\t\twritel(0, pcie_index_hi_offset);\n\t\treadl(pcie_index_hi_offset);\n\t}\n\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\n\treturn r;\n}\n\n \nu64 amdgpu_device_indirect_rreg64(struct amdgpu_device *adev,\n\t\t\t\t  u32 reg_addr)\n{\n\tunsigned long flags, pcie_index, pcie_data;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_data_offset;\n\tu64 r;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\n\t \n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\tr = readl(pcie_data_offset);\n\t \n\twritel(reg_addr + 4, pcie_index_offset);\n\treadl(pcie_index_offset);\n\tr |= ((u64)readl(pcie_data_offset) << 32);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\n\treturn r;\n}\n\n \nvoid amdgpu_device_indirect_wreg(struct amdgpu_device *adev,\n\t\t\t\t u32 reg_addr, u32 reg_data)\n{\n\tunsigned long flags, pcie_index, pcie_data;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_data_offset;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\twritel(reg_data, pcie_data_offset);\n\treadl(pcie_data_offset);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nvoid amdgpu_device_indirect_wreg_ext(struct amdgpu_device *adev,\n\t\t\t\t     u64 reg_addr, u32 reg_data)\n{\n\tunsigned long flags, pcie_index, pcie_index_hi, pcie_data;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_index_hi_offset;\n\tvoid __iomem *pcie_data_offset;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\tif (adev->nbio.funcs->get_pcie_index_hi_offset)\n\t\tpcie_index_hi = adev->nbio.funcs->get_pcie_index_hi_offset(adev);\n\telse\n\t\tpcie_index_hi = 0;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\tif (pcie_index_hi != 0)\n\t\tpcie_index_hi_offset = (void __iomem *)adev->rmmio +\n\t\t\t\tpcie_index_hi * 4;\n\n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\tif (pcie_index_hi != 0) {\n\t\twritel((reg_addr >> 32) & 0xff, pcie_index_hi_offset);\n\t\treadl(pcie_index_hi_offset);\n\t}\n\twritel(reg_data, pcie_data_offset);\n\treadl(pcie_data_offset);\n\n\t \n\tif (pcie_index_hi != 0) {\n\t\twritel(0, pcie_index_hi_offset);\n\t\treadl(pcie_index_hi_offset);\n\t}\n\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nvoid amdgpu_device_indirect_wreg64(struct amdgpu_device *adev,\n\t\t\t\t   u32 reg_addr, u64 reg_data)\n{\n\tunsigned long flags, pcie_index, pcie_data;\n\tvoid __iomem *pcie_index_offset;\n\tvoid __iomem *pcie_data_offset;\n\n\tpcie_index = adev->nbio.funcs->get_pcie_index_offset(adev);\n\tpcie_data = adev->nbio.funcs->get_pcie_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tpcie_index_offset = (void __iomem *)adev->rmmio + pcie_index * 4;\n\tpcie_data_offset = (void __iomem *)adev->rmmio + pcie_data * 4;\n\n\t \n\twritel(reg_addr, pcie_index_offset);\n\treadl(pcie_index_offset);\n\twritel((u32)(reg_data & 0xffffffffULL), pcie_data_offset);\n\treadl(pcie_data_offset);\n\t \n\twritel(reg_addr + 4, pcie_index_offset);\n\treadl(pcie_index_offset);\n\twritel((u32)(reg_data >> 32), pcie_data_offset);\n\treadl(pcie_data_offset);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nu32 amdgpu_device_get_rev_id(struct amdgpu_device *adev)\n{\n\treturn adev->nbio.funcs->get_rev_id(adev);\n}\n\n \nstatic uint32_t amdgpu_invalid_rreg(struct amdgpu_device *adev, uint32_t reg)\n{\n\tDRM_ERROR(\"Invalid callback to read register 0x%04X\\n\", reg);\n\tBUG();\n\treturn 0;\n}\n\nstatic uint32_t amdgpu_invalid_rreg_ext(struct amdgpu_device *adev, uint64_t reg)\n{\n\tDRM_ERROR(\"Invalid callback to read register 0x%llX\\n\", reg);\n\tBUG();\n\treturn 0;\n}\n\n \nstatic void amdgpu_invalid_wreg(struct amdgpu_device *adev, uint32_t reg, uint32_t v)\n{\n\tDRM_ERROR(\"Invalid callback to write register 0x%04X with 0x%08X\\n\",\n\t\t  reg, v);\n\tBUG();\n}\n\nstatic void amdgpu_invalid_wreg_ext(struct amdgpu_device *adev, uint64_t reg, uint32_t v)\n{\n\tDRM_ERROR(\"Invalid callback to write register 0x%llX with 0x%08X\\n\",\n\t\t  reg, v);\n\tBUG();\n}\n\n \nstatic uint64_t amdgpu_invalid_rreg64(struct amdgpu_device *adev, uint32_t reg)\n{\n\tDRM_ERROR(\"Invalid callback to read 64 bit register 0x%04X\\n\", reg);\n\tBUG();\n\treturn 0;\n}\n\n \nstatic void amdgpu_invalid_wreg64(struct amdgpu_device *adev, uint32_t reg, uint64_t v)\n{\n\tDRM_ERROR(\"Invalid callback to write 64 bit register 0x%04X with 0x%08llX\\n\",\n\t\t  reg, v);\n\tBUG();\n}\n\n \nstatic uint32_t amdgpu_block_invalid_rreg(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t block, uint32_t reg)\n{\n\tDRM_ERROR(\"Invalid callback to read register 0x%04X in block 0x%04X\\n\",\n\t\t  reg, block);\n\tBUG();\n\treturn 0;\n}\n\n \nstatic void amdgpu_block_invalid_wreg(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t block,\n\t\t\t\t      uint32_t reg, uint32_t v)\n{\n\tDRM_ERROR(\"Invalid block callback to write register 0x%04X in block 0x%04X with 0x%08X\\n\",\n\t\t  reg, block, v);\n\tBUG();\n}\n\n \nstatic int amdgpu_device_asic_init(struct amdgpu_device *adev)\n{\n\tint ret;\n\n\tamdgpu_asic_pre_asic_init(adev);\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3) ||\n\t    adev->ip_versions[GC_HWIP][0] >= IP_VERSION(11, 0, 0)) {\n\t\tamdgpu_psp_wait_for_bootloader(adev);\n\t\tret = amdgpu_atomfirmware_asic_init(adev, true);\n\t\treturn ret;\n\t} else {\n\t\treturn amdgpu_atom_asic_init(adev->mode_info.atom_context);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_mem_scratch_init(struct amdgpu_device *adev)\n{\n\treturn amdgpu_bo_create_kernel(adev, AMDGPU_GPU_PAGE_SIZE, PAGE_SIZE,\n\t\t\t\t       AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t       AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t       &adev->mem_scratch.robj,\n\t\t\t\t       &adev->mem_scratch.gpu_addr,\n\t\t\t\t       (void **)&adev->mem_scratch.ptr);\n}\n\n \nstatic void amdgpu_device_mem_scratch_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_bo_free_kernel(&adev->mem_scratch.robj, NULL, NULL);\n}\n\n \nvoid amdgpu_device_program_register_sequence(struct amdgpu_device *adev,\n\t\t\t\t\t     const u32 *registers,\n\t\t\t\t\t     const u32 array_size)\n{\n\tu32 tmp, reg, and_mask, or_mask;\n\tint i;\n\n\tif (array_size % 3)\n\t\treturn;\n\n\tfor (i = 0; i < array_size; i += 3) {\n\t\treg = registers[i + 0];\n\t\tand_mask = registers[i + 1];\n\t\tor_mask = registers[i + 2];\n\n\t\tif (and_mask == 0xffffffff) {\n\t\t\ttmp = or_mask;\n\t\t} else {\n\t\t\ttmp = RREG32(reg);\n\t\t\ttmp &= ~and_mask;\n\t\t\tif (adev->family >= AMDGPU_FAMILY_AI)\n\t\t\t\ttmp |= (or_mask & and_mask);\n\t\t\telse\n\t\t\t\ttmp |= or_mask;\n\t\t}\n\t\tWREG32(reg, tmp);\n\t}\n}\n\n \nvoid amdgpu_device_pci_config_reset(struct amdgpu_device *adev)\n{\n\tpci_write_config_dword(adev->pdev, 0x7c, AMDGPU_ASIC_RESET_DATA);\n}\n\n \nint amdgpu_device_pci_reset(struct amdgpu_device *adev)\n{\n\treturn pci_reset_function(adev->pdev);\n}\n\n \n\n \nstatic void amdgpu_device_wb_fini(struct amdgpu_device *adev)\n{\n\tif (adev->wb.wb_obj) {\n\t\tamdgpu_bo_free_kernel(&adev->wb.wb_obj,\n\t\t\t\t      &adev->wb.gpu_addr,\n\t\t\t\t      (void **)&adev->wb.wb);\n\t\tadev->wb.wb_obj = NULL;\n\t}\n}\n\n \nstatic int amdgpu_device_wb_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->wb.wb_obj == NULL) {\n\t\t \n\t\tr = amdgpu_bo_create_kernel(adev, AMDGPU_MAX_WB * sizeof(uint32_t) * 8,\n\t\t\t\t\t    PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t    &adev->wb.wb_obj, &adev->wb.gpu_addr,\n\t\t\t\t\t    (void **)&adev->wb.wb);\n\t\tif (r) {\n\t\t\tdev_warn(adev->dev, \"(%d) create WB bo failed\\n\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->wb.num_wb = AMDGPU_MAX_WB;\n\t\tmemset(&adev->wb.used, 0, sizeof(adev->wb.used));\n\n\t\t \n\t\tmemset((char *)adev->wb.wb, 0, AMDGPU_MAX_WB * sizeof(uint32_t) * 8);\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_device_wb_get(struct amdgpu_device *adev, u32 *wb)\n{\n\tunsigned long offset = find_first_zero_bit(adev->wb.used, adev->wb.num_wb);\n\n\tif (offset < adev->wb.num_wb) {\n\t\t__set_bit(offset, adev->wb.used);\n\t\t*wb = offset << 3;  \n\t\treturn 0;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n}\n\n \nvoid amdgpu_device_wb_free(struct amdgpu_device *adev, u32 wb)\n{\n\twb >>= 3;\n\tif (wb < adev->wb.num_wb)\n\t\t__clear_bit(wb, adev->wb.used);\n}\n\n \nint amdgpu_device_resize_fb_bar(struct amdgpu_device *adev)\n{\n\tint rbar_size = pci_rebar_bytes_to_size(adev->gmc.real_vram_size);\n\tstruct pci_bus *root;\n\tstruct resource *res;\n\tunsigned int i;\n\tu16 cmd;\n\tint r;\n\n\tif (!IS_ENABLED(CONFIG_PHYS_ADDR_T_64BIT))\n\t\treturn 0;\n\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\t \n\tif (adev->gmc.real_vram_size &&\n\t    (pci_resource_len(adev->pdev, 0) >= adev->gmc.real_vram_size))\n\t\treturn 0;\n\n\t \n\troot = adev->pdev->bus;\n\twhile (root->parent)\n\t\troot = root->parent;\n\n\tpci_bus_for_each_resource(root, res, i) {\n\t\tif (res && res->flags & (IORESOURCE_MEM | IORESOURCE_MEM_64) &&\n\t\t    res->start > 0x100000000ull)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (!res)\n\t\treturn 0;\n\n\t \n\trbar_size = min(fls(pci_rebar_get_possible_sizes(adev->pdev, 0)) - 1,\n\t\t\trbar_size);\n\n\t \n\tpci_read_config_word(adev->pdev, PCI_COMMAND, &cmd);\n\tpci_write_config_word(adev->pdev, PCI_COMMAND,\n\t\t\t      cmd & ~PCI_COMMAND_MEMORY);\n\n\t \n\tamdgpu_doorbell_fini(adev);\n\tif (adev->asic_type >= CHIP_BONAIRE)\n\t\tpci_release_resource(adev->pdev, 2);\n\n\tpci_release_resource(adev->pdev, 0);\n\n\tr = pci_resize_resource(adev->pdev, 0, rbar_size);\n\tif (r == -ENOSPC)\n\t\tDRM_INFO(\"Not enough PCI address space for a large BAR.\");\n\telse if (r && r != -ENOTSUPP)\n\t\tDRM_ERROR(\"Problem resizing BAR0 (%d).\", r);\n\n\tpci_assign_unassigned_bus_resources(adev->pdev->bus);\n\n\t \n\tr = amdgpu_doorbell_init(adev);\n\tif (r || (pci_resource_flags(adev->pdev, 0) & IORESOURCE_UNSET))\n\t\treturn -ENODEV;\n\n\tpci_write_config_word(adev->pdev, PCI_COMMAND, cmd);\n\n\treturn 0;\n}\n\nstatic bool amdgpu_device_read_bios(struct amdgpu_device *adev)\n{\n\tif (hweight32(adev->aid_mask) && (adev->flags & AMD_IS_APU))\n\t\treturn false;\n\n\treturn true;\n}\n\n \n \nbool amdgpu_device_need_post(struct amdgpu_device *adev)\n{\n\tuint32_t reg;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn false;\n\n\tif (!amdgpu_device_read_bios(adev))\n\t\treturn false;\n\n\tif (amdgpu_passthrough(adev)) {\n\t\t \n\t\tif (adev->asic_type == CHIP_FIJI) {\n\t\t\tint err;\n\t\t\tuint32_t fw_ver;\n\n\t\t\terr = request_firmware(&adev->pm.fw, \"amdgpu/fiji_smc.bin\", adev->dev);\n\t\t\t \n\t\t\tif (err)\n\t\t\t\treturn true;\n\n\t\t\tfw_ver = *((uint32_t *)adev->pm.fw->data + 69);\n\t\t\tif (fw_ver < 0x00160e00)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tif (adev->gmc.xgmi.pending_reset)\n\t\treturn false;\n\n\tif (adev->has_hw_reset) {\n\t\tadev->has_hw_reset = false;\n\t\treturn true;\n\t}\n\n\t \n\tif (adev->asic_type >= CHIP_BONAIRE)\n\t\treturn amdgpu_atombios_scratch_need_asic_init(adev);\n\n\t \n\treg = amdgpu_asic_get_config_memsize(adev);\n\n\tif ((reg != 0) && (reg != 0xffffffff))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool amdgpu_device_pcie_dynamic_switching_supported(void)\n{\n#if IS_ENABLED(CONFIG_X86)\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\n\tif (c->x86_vendor == X86_VENDOR_INTEL)\n\t\treturn false;\n#endif\n\treturn true;\n}\n\n \nbool amdgpu_device_should_use_aspm(struct amdgpu_device *adev)\n{\n\tswitch (amdgpu_aspm) {\n\tcase -1:\n\t\tbreak;\n\tcase 0:\n\t\treturn false;\n\tcase 1:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n\treturn pcie_aspm_enabled(adev->pdev);\n}\n\nbool amdgpu_device_aspm_support_quirk(void)\n{\n#if IS_ENABLED(CONFIG_X86)\n\tstruct cpuinfo_x86 *c = &cpu_data(0);\n\n\treturn !(c->x86 == 6 && c->x86_model == INTEL_FAM6_ALDERLAKE);\n#else\n\treturn true;\n#endif\n}\n\n \n \nstatic unsigned int amdgpu_device_vga_set_decode(struct pci_dev *pdev,\n\t\tbool state)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(pci_get_drvdata(pdev));\n\n\tamdgpu_asic_set_vga_state(adev, state);\n\tif (state)\n\t\treturn VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM |\n\t\t       VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;\n\telse\n\t\treturn VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;\n}\n\n \nstatic void amdgpu_device_check_block_size(struct amdgpu_device *adev)\n{\n\t \n\tif (amdgpu_vm_block_size == -1)\n\t\treturn;\n\n\tif (amdgpu_vm_block_size < 9) {\n\t\tdev_warn(adev->dev, \"VM page table size (%d) too small\\n\",\n\t\t\t amdgpu_vm_block_size);\n\t\tamdgpu_vm_block_size = -1;\n\t}\n}\n\n \nstatic void amdgpu_device_check_vm_size(struct amdgpu_device *adev)\n{\n\t \n\tif (amdgpu_vm_size == -1)\n\t\treturn;\n\n\tif (amdgpu_vm_size < 1) {\n\t\tdev_warn(adev->dev, \"VM size (%d) too small, min is 1GB\\n\",\n\t\t\t amdgpu_vm_size);\n\t\tamdgpu_vm_size = -1;\n\t}\n}\n\nstatic void amdgpu_device_check_smu_prv_buffer_size(struct amdgpu_device *adev)\n{\n\tstruct sysinfo si;\n\tbool is_os_64 = (sizeof(void *) == 8);\n\tuint64_t total_memory;\n\tuint64_t dram_size_seven_GB = 0x1B8000000;\n\tuint64_t dram_size_three_GB = 0xB8000000;\n\n\tif (amdgpu_smu_memory_pool_size == 0)\n\t\treturn;\n\n\tif (!is_os_64) {\n\t\tDRM_WARN(\"Not 64-bit OS, feature not supported\\n\");\n\t\tgoto def_value;\n\t}\n\tsi_meminfo(&si);\n\ttotal_memory = (uint64_t)si.totalram * si.mem_unit;\n\n\tif ((amdgpu_smu_memory_pool_size == 1) ||\n\t\t(amdgpu_smu_memory_pool_size == 2)) {\n\t\tif (total_memory < dram_size_three_GB)\n\t\t\tgoto def_value1;\n\t} else if ((amdgpu_smu_memory_pool_size == 4) ||\n\t\t(amdgpu_smu_memory_pool_size == 8)) {\n\t\tif (total_memory < dram_size_seven_GB)\n\t\t\tgoto def_value1;\n\t} else {\n\t\tDRM_WARN(\"Smu memory pool size not supported\\n\");\n\t\tgoto def_value;\n\t}\n\tadev->pm.smu_prv_buffer_size = amdgpu_smu_memory_pool_size << 28;\n\n\treturn;\n\ndef_value1:\n\tDRM_WARN(\"No enough system memory\\n\");\ndef_value:\n\tadev->pm.smu_prv_buffer_size = 0;\n}\n\nstatic int amdgpu_device_init_apu_flags(struct amdgpu_device *adev)\n{\n\tif (!(adev->flags & AMD_IS_APU) ||\n\t    adev->asic_type < CHIP_RAVEN)\n\t\treturn 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_RAVEN:\n\t\tif (adev->pdev->device == 0x15dd)\n\t\t\tadev->apu_flags |= AMD_APU_IS_RAVEN;\n\t\tif (adev->pdev->device == 0x15d8)\n\t\t\tadev->apu_flags |= AMD_APU_IS_PICASSO;\n\t\tbreak;\n\tcase CHIP_RENOIR:\n\t\tif ((adev->pdev->device == 0x1636) ||\n\t\t    (adev->pdev->device == 0x164c))\n\t\t\tadev->apu_flags |= AMD_APU_IS_RENOIR;\n\t\telse\n\t\t\tadev->apu_flags |= AMD_APU_IS_GREEN_SARDINE;\n\t\tbreak;\n\tcase CHIP_VANGOGH:\n\t\tadev->apu_flags |= AMD_APU_IS_VANGOGH;\n\t\tbreak;\n\tcase CHIP_YELLOW_CARP:\n\t\tbreak;\n\tcase CHIP_CYAN_SKILLFISH:\n\t\tif ((adev->pdev->device == 0x13FE) ||\n\t\t    (adev->pdev->device == 0x143F))\n\t\t\tadev->apu_flags |= AMD_APU_IS_CYAN_SKILLFISH2;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_check_arguments(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sched_jobs < 4) {\n\t\tdev_warn(adev->dev, \"sched jobs (%d) must be at least 4\\n\",\n\t\t\t amdgpu_sched_jobs);\n\t\tamdgpu_sched_jobs = 4;\n\t} else if (!is_power_of_2(amdgpu_sched_jobs)) {\n\t\tdev_warn(adev->dev, \"sched jobs (%d) must be a power of 2\\n\",\n\t\t\t amdgpu_sched_jobs);\n\t\tamdgpu_sched_jobs = roundup_pow_of_two(amdgpu_sched_jobs);\n\t}\n\n\tif (amdgpu_gart_size != -1 && amdgpu_gart_size < 32) {\n\t\t \n\t\tdev_warn(adev->dev, \"gart size (%d) too small\\n\",\n\t\t\t amdgpu_gart_size);\n\t\tamdgpu_gart_size = -1;\n\t}\n\n\tif (amdgpu_gtt_size != -1 && amdgpu_gtt_size < 32) {\n\t\t \n\t\tdev_warn(adev->dev, \"gtt size (%d) too small\\n\",\n\t\t\t\t amdgpu_gtt_size);\n\t\tamdgpu_gtt_size = -1;\n\t}\n\n\t \n\tif (amdgpu_vm_fragment_size != -1 &&\n\t    (amdgpu_vm_fragment_size > 9 || amdgpu_vm_fragment_size < 4)) {\n\t\tdev_warn(adev->dev, \"valid range is between 4 and 9\\n\");\n\t\tamdgpu_vm_fragment_size = -1;\n\t}\n\n\tif (amdgpu_sched_hw_submission < 2) {\n\t\tdev_warn(adev->dev, \"sched hw submission jobs (%d) must be at least 2\\n\",\n\t\t\t amdgpu_sched_hw_submission);\n\t\tamdgpu_sched_hw_submission = 2;\n\t} else if (!is_power_of_2(amdgpu_sched_hw_submission)) {\n\t\tdev_warn(adev->dev, \"sched hw submission jobs (%d) must be a power of 2\\n\",\n\t\t\t amdgpu_sched_hw_submission);\n\t\tamdgpu_sched_hw_submission = roundup_pow_of_two(amdgpu_sched_hw_submission);\n\t}\n\n\tif (amdgpu_reset_method < -1 || amdgpu_reset_method > 4) {\n\t\tdev_warn(adev->dev, \"invalid option for reset method, reverting to default\\n\");\n\t\tamdgpu_reset_method = -1;\n\t}\n\n\tamdgpu_device_check_smu_prv_buffer_size(adev);\n\n\tamdgpu_device_check_vm_size(adev);\n\n\tamdgpu_device_check_block_size(adev);\n\n\tadev->firmware.load_type = amdgpu_ucode_get_load_type(adev, amdgpu_fw_load_type);\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_switcheroo_set_state(struct pci_dev *pdev,\n\t\t\t\t\tenum vga_switcheroo_state state)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tint r;\n\n\tif (amdgpu_device_supports_px(dev) && state == VGA_SWITCHEROO_OFF)\n\t\treturn;\n\n\tif (state == VGA_SWITCHEROO_ON) {\n\t\tpr_info(\"switched on\\n\");\n\t\t \n\t\tdev->switch_power_state = DRM_SWITCH_POWER_CHANGING;\n\n\t\tpci_set_power_state(pdev, PCI_D0);\n\t\tamdgpu_device_load_pci_state(pdev);\n\t\tr = pci_enable_device(pdev);\n\t\tif (r)\n\t\t\tDRM_WARN(\"pci_enable_device failed (%d)\\n\", r);\n\t\tamdgpu_device_resume(dev, true);\n\n\t\tdev->switch_power_state = DRM_SWITCH_POWER_ON;\n\t} else {\n\t\tpr_info(\"switched off\\n\");\n\t\tdev->switch_power_state = DRM_SWITCH_POWER_CHANGING;\n\t\tamdgpu_device_suspend(dev, true);\n\t\tamdgpu_device_cache_pci_state(pdev);\n\t\t \n\t\tpci_disable_device(pdev);\n\t\tpci_set_power_state(pdev, PCI_D3cold);\n\t\tdev->switch_power_state = DRM_SWITCH_POWER_OFF;\n\t}\n}\n\n \nstatic bool amdgpu_switcheroo_can_switch(struct pci_dev *pdev)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\n        \n\treturn atomic_read(&dev->open_count) == 0;\n}\n\nstatic const struct vga_switcheroo_client_ops amdgpu_switcheroo_ops = {\n\t.set_gpu_state = amdgpu_switcheroo_set_state,\n\t.reprobe = NULL,\n\t.can_switch = amdgpu_switcheroo_can_switch,\n};\n\n \nint amdgpu_device_ip_set_clockgating_state(void *dev,\n\t\t\t\t\t   enum amd_ip_block_type block_type,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = dev;\n\tint i, r = 0;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type != block_type)\n\t\t\tcontinue;\n\t\tif (!adev->ip_blocks[i].version->funcs->set_clockgating_state)\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->set_clockgating_state(\n\t\t\t(void *)adev, state);\n\t\tif (r)\n\t\t\tDRM_ERROR(\"set_clockgating_state of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t}\n\treturn r;\n}\n\n \nint amdgpu_device_ip_set_powergating_state(void *dev,\n\t\t\t\t\t   enum amd_ip_block_type block_type,\n\t\t\t\t\t   enum amd_powergating_state state)\n{\n\tstruct amdgpu_device *adev = dev;\n\tint i, r = 0;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type != block_type)\n\t\t\tcontinue;\n\t\tif (!adev->ip_blocks[i].version->funcs->set_powergating_state)\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->set_powergating_state(\n\t\t\t(void *)adev, state);\n\t\tif (r)\n\t\t\tDRM_ERROR(\"set_powergating_state of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t}\n\treturn r;\n}\n\n \nvoid amdgpu_device_ip_get_clockgating_state(struct amdgpu_device *adev,\n\t\t\t\t\t    u64 *flags)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->funcs->get_clockgating_state)\n\t\t\tadev->ip_blocks[i].version->funcs->get_clockgating_state((void *)adev, flags);\n\t}\n}\n\n \nint amdgpu_device_ip_wait_for_idle(struct amdgpu_device *adev,\n\t\t\t\t   enum amd_ip_block_type block_type)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == block_type) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->wait_for_idle((void *)adev);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n\n}\n\n \nbool amdgpu_device_ip_is_idle(struct amdgpu_device *adev,\n\t\t\t      enum amd_ip_block_type block_type)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == block_type)\n\t\t\treturn adev->ip_blocks[i].version->funcs->is_idle((void *)adev);\n\t}\n\treturn true;\n\n}\n\n \nstruct amdgpu_ip_block *\namdgpu_device_ip_get_ip_block(struct amdgpu_device *adev,\n\t\t\t      enum amd_ip_block_type type)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++)\n\t\tif (adev->ip_blocks[i].version->type == type)\n\t\t\treturn &adev->ip_blocks[i];\n\n\treturn NULL;\n}\n\n \nint amdgpu_device_ip_block_version_cmp(struct amdgpu_device *adev,\n\t\t\t\t       enum amd_ip_block_type type,\n\t\t\t\t       u32 major, u32 minor)\n{\n\tstruct amdgpu_ip_block *ip_block = amdgpu_device_ip_get_ip_block(adev, type);\n\n\tif (ip_block && ((ip_block->version->major > major) ||\n\t\t\t((ip_block->version->major == major) &&\n\t\t\t(ip_block->version->minor >= minor))))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nint amdgpu_device_ip_block_add(struct amdgpu_device *adev,\n\t\t\t       const struct amdgpu_ip_block_version *ip_block_version)\n{\n\tif (!ip_block_version)\n\t\treturn -EINVAL;\n\n\tswitch (ip_block_version->type) {\n\tcase AMD_IP_BLOCK_TYPE_VCN:\n\t\tif (adev->harvest_ip_mask & AMD_HARVEST_IP_VCN_MASK)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase AMD_IP_BLOCK_TYPE_JPEG:\n\t\tif (adev->harvest_ip_mask & AMD_HARVEST_IP_JPEG_MASK)\n\t\t\treturn 0;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tDRM_INFO(\"add ip block number %d <%s>\\n\", adev->num_ip_blocks,\n\t\t  ip_block_version->funcs->name);\n\n\tadev->ip_blocks[adev->num_ip_blocks++].version = ip_block_version;\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_device_enable_virtual_display(struct amdgpu_device *adev)\n{\n\tadev->enable_virtual_display = false;\n\n\tif (amdgpu_virtual_display) {\n\t\tconst char *pci_address_name = pci_name(adev->pdev);\n\t\tchar *pciaddstr, *pciaddstr_tmp, *pciaddname_tmp, *pciaddname;\n\n\t\tpciaddstr = kstrdup(amdgpu_virtual_display, GFP_KERNEL);\n\t\tpciaddstr_tmp = pciaddstr;\n\t\twhile ((pciaddname_tmp = strsep(&pciaddstr_tmp, \";\"))) {\n\t\t\tpciaddname = strsep(&pciaddname_tmp, \",\");\n\t\t\tif (!strcmp(\"all\", pciaddname)\n\t\t\t    || !strcmp(pci_address_name, pciaddname)) {\n\t\t\t\tlong num_crtc;\n\t\t\t\tint res = -1;\n\n\t\t\t\tadev->enable_virtual_display = true;\n\n\t\t\t\tif (pciaddname_tmp)\n\t\t\t\t\tres = kstrtol(pciaddname_tmp, 10,\n\t\t\t\t\t\t      &num_crtc);\n\n\t\t\t\tif (!res) {\n\t\t\t\t\tif (num_crtc < 1)\n\t\t\t\t\t\tnum_crtc = 1;\n\t\t\t\t\tif (num_crtc > 6)\n\t\t\t\t\t\tnum_crtc = 6;\n\t\t\t\t\tadev->mode_info.num_crtc = num_crtc;\n\t\t\t\t} else {\n\t\t\t\t\tadev->mode_info.num_crtc = 1;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tDRM_INFO(\"virtual display string:%s, %s:virtual_display:%d, num_crtc:%d\\n\",\n\t\t\t amdgpu_virtual_display, pci_address_name,\n\t\t\t adev->enable_virtual_display, adev->mode_info.num_crtc);\n\n\t\tkfree(pciaddstr);\n\t}\n}\n\nvoid amdgpu_device_set_sriov_virtual_display(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sriov_vf(adev) && !adev->enable_virtual_display) {\n\t\tadev->mode_info.num_crtc = 1;\n\t\tadev->enable_virtual_display = true;\n\t\tDRM_INFO(\"virtual_display:%d, num_crtc:%d\\n\",\n\t\t\t adev->enable_virtual_display, adev->mode_info.num_crtc);\n\t}\n}\n\n \nstatic int amdgpu_device_parse_gpu_info_fw(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tchar fw_name[40];\n\tint err;\n\tconst struct gpu_info_firmware_header_v1_0 *hdr;\n\n\tadev->firmware.gpu_info_fw = NULL;\n\n\tif (adev->mman.discovery_bin)\n\t\treturn 0;\n\n\tswitch (adev->asic_type) {\n\tdefault:\n\t\treturn 0;\n\tcase CHIP_VEGA10:\n\t\tchip_name = \"vega10\";\n\t\tbreak;\n\tcase CHIP_VEGA12:\n\t\tchip_name = \"vega12\";\n\t\tbreak;\n\tcase CHIP_RAVEN:\n\t\tif (adev->apu_flags & AMD_APU_IS_RAVEN2)\n\t\t\tchip_name = \"raven2\";\n\t\telse if (adev->apu_flags & AMD_APU_IS_PICASSO)\n\t\t\tchip_name = \"picasso\";\n\t\telse\n\t\t\tchip_name = \"raven\";\n\t\tbreak;\n\tcase CHIP_ARCTURUS:\n\t\tchip_name = \"arcturus\";\n\t\tbreak;\n\tcase CHIP_NAVI12:\n\t\tchip_name = \"navi12\";\n\t\tbreak;\n\t}\n\n\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_gpu_info.bin\", chip_name);\n\terr = amdgpu_ucode_request(adev, &adev->firmware.gpu_info_fw, fw_name);\n\tif (err) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Failed to get gpu_info firmware \\\"%s\\\"\\n\",\n\t\t\tfw_name);\n\t\tgoto out;\n\t}\n\n\thdr = (const struct gpu_info_firmware_header_v1_0 *)adev->firmware.gpu_info_fw->data;\n\tamdgpu_ucode_print_gpu_info_hdr(&hdr->header);\n\n\tswitch (hdr->version_major) {\n\tcase 1:\n\t{\n\t\tconst struct gpu_info_firmware_v1_0 *gpu_info_fw =\n\t\t\t(const struct gpu_info_firmware_v1_0 *)(adev->firmware.gpu_info_fw->data +\n\t\t\t\t\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\t\t \n\t\tif (adev->asic_type == CHIP_NAVI12)\n\t\t\tgoto parse_soc_bounding_box;\n\n\t\tadev->gfx.config.max_shader_engines = le32_to_cpu(gpu_info_fw->gc_num_se);\n\t\tadev->gfx.config.max_cu_per_sh = le32_to_cpu(gpu_info_fw->gc_num_cu_per_sh);\n\t\tadev->gfx.config.max_sh_per_se = le32_to_cpu(gpu_info_fw->gc_num_sh_per_se);\n\t\tadev->gfx.config.max_backends_per_se = le32_to_cpu(gpu_info_fw->gc_num_rb_per_se);\n\t\tadev->gfx.config.max_texture_channel_caches =\n\t\t\tle32_to_cpu(gpu_info_fw->gc_num_tccs);\n\t\tadev->gfx.config.max_gprs = le32_to_cpu(gpu_info_fw->gc_num_gprs);\n\t\tadev->gfx.config.max_gs_threads = le32_to_cpu(gpu_info_fw->gc_num_max_gs_thds);\n\t\tadev->gfx.config.gs_vgt_table_depth = le32_to_cpu(gpu_info_fw->gc_gs_table_depth);\n\t\tadev->gfx.config.gs_prim_buffer_depth = le32_to_cpu(gpu_info_fw->gc_gsprim_buff_depth);\n\t\tadev->gfx.config.double_offchip_lds_buf =\n\t\t\tle32_to_cpu(gpu_info_fw->gc_double_offchip_lds_buffer);\n\t\tadev->gfx.cu_info.wave_front_size = le32_to_cpu(gpu_info_fw->gc_wave_size);\n\t\tadev->gfx.cu_info.max_waves_per_simd =\n\t\t\tle32_to_cpu(gpu_info_fw->gc_max_waves_per_simd);\n\t\tadev->gfx.cu_info.max_scratch_slots_per_cu =\n\t\t\tle32_to_cpu(gpu_info_fw->gc_max_scratch_slots_per_cu);\n\t\tadev->gfx.cu_info.lds_size = le32_to_cpu(gpu_info_fw->gc_lds_size);\n\t\tif (hdr->version_minor >= 1) {\n\t\t\tconst struct gpu_info_firmware_v1_1 *gpu_info_fw =\n\t\t\t\t(const struct gpu_info_firmware_v1_1 *)(adev->firmware.gpu_info_fw->data +\n\t\t\t\t\t\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\t\t\tadev->gfx.config.num_sc_per_sh =\n\t\t\t\tle32_to_cpu(gpu_info_fw->num_sc_per_sh);\n\t\t\tadev->gfx.config.num_packer_per_sc =\n\t\t\t\tle32_to_cpu(gpu_info_fw->num_packer_per_sc);\n\t\t}\n\nparse_soc_bounding_box:\n\t\t \n\t\tif (hdr->version_minor == 2) {\n\t\t\tconst struct gpu_info_firmware_v1_2 *gpu_info_fw =\n\t\t\t\t(const struct gpu_info_firmware_v1_2 *)(adev->firmware.gpu_info_fw->data +\n\t\t\t\t\t\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\t\t\tadev->dm.soc_bounding_box = &gpu_info_fw->soc_bounding_box;\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tdev_err(adev->dev,\n\t\t\t\"Unsupported gpu_info table %d\\n\", hdr->header.ucode_version);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\nout:\n\treturn err;\n}\n\n \nstatic int amdgpu_device_ip_early_init(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *parent;\n\tint i, r;\n\tbool total;\n\n\tamdgpu_device_enable_virtual_display(adev);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tr = amdgpu_virt_request_full_gpu(adev, true);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_SI\n\tcase CHIP_VERDE:\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\tcase CHIP_OLAND:\n\tcase CHIP_HAINAN:\n\t\tadev->family = AMDGPU_FAMILY_SI;\n\t\tr = si_set_ip_blocks(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tbreak;\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_BONAIRE:\n\tcase CHIP_HAWAII:\n\tcase CHIP_KAVERI:\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n\t\tif (adev->flags & AMD_IS_APU)\n\t\t\tadev->family = AMDGPU_FAMILY_KV;\n\t\telse\n\t\t\tadev->family = AMDGPU_FAMILY_CI;\n\n\t\tr = cik_set_ip_blocks(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tbreak;\n#endif\n\tcase CHIP_TOPAZ:\n\tcase CHIP_TONGA:\n\tcase CHIP_FIJI:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tif (adev->flags & AMD_IS_APU)\n\t\t\tadev->family = AMDGPU_FAMILY_CZ;\n\t\telse\n\t\t\tadev->family = AMDGPU_FAMILY_VI;\n\n\t\tr = vi_set_ip_blocks(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tbreak;\n\tdefault:\n\t\tr = amdgpu_discovery_set_ip_blocks(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t\tbreak;\n\t}\n\n\tif (amdgpu_has_atpx() &&\n\t    (amdgpu_is_atpx_hybrid() ||\n\t     amdgpu_has_atpx_dgpu_power_cntl()) &&\n\t    ((adev->flags & AMD_IS_APU) == 0) &&\n\t    !dev_is_removable(&adev->pdev->dev))\n\t\tadev->flags |= AMD_IS_PX;\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tparent = pcie_find_root_port(adev->pdev);\n\t\tadev->has_pr3 = parent ? pci_pr3_present(parent) : false;\n\t}\n\n\n\tadev->pm.pp_feature = amdgpu_pp_feature_mask;\n\tif (amdgpu_sriov_vf(adev) || sched_policy == KFD_SCHED_POLICY_NO_HWS)\n\t\tadev->pm.pp_feature &= ~PP_GFXOFF_MASK;\n\tif (amdgpu_sriov_vf(adev) && adev->asic_type == CHIP_SIENNA_CICHLID)\n\t\tadev->pm.pp_feature &= ~PP_OVERDRIVE_MASK;\n\tif (!amdgpu_device_pcie_dynamic_switching_supported())\n\t\tadev->pm.pp_feature &= ~PP_PCIE_DPM_MASK;\n\n\ttotal = true;\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif ((amdgpu_ip_block_mask & (1 << i)) == 0) {\n\t\t\tDRM_WARN(\"disabled ip block: %d <%s>\\n\",\n\t\t\t\t  i, adev->ip_blocks[i].version->funcs->name);\n\t\t\tadev->ip_blocks[i].status.valid = false;\n\t\t} else {\n\t\t\tif (adev->ip_blocks[i].version->funcs->early_init) {\n\t\t\t\tr = adev->ip_blocks[i].version->funcs->early_init((void *)adev);\n\t\t\t\tif (r == -ENOENT) {\n\t\t\t\t\tadev->ip_blocks[i].status.valid = false;\n\t\t\t\t} else if (r) {\n\t\t\t\t\tDRM_ERROR(\"early_init of IP block <%s> failed %d\\n\",\n\t\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\t\ttotal = false;\n\t\t\t\t} else {\n\t\t\t\t\tadev->ip_blocks[i].status.valid = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tadev->ip_blocks[i].status.valid = true;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON) {\n\t\t\tr = amdgpu_device_parse_gpu_info_fw(adev);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\t \n\t\t\tif (amdgpu_device_read_bios(adev)) {\n\t\t\t\tif (!amdgpu_get_bios(adev))\n\t\t\t\t\treturn -EINVAL;\n\n\t\t\t\tr = amdgpu_atombios_init(adev);\n\t\t\t\tif (r) {\n\t\t\t\t\tdev_err(adev->dev, \"amdgpu_atombios_init failed\\n\");\n\t\t\t\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_ATOMBIOS_INIT_FAIL, 0, 0);\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tif (amdgpu_sriov_vf(adev))\n\t\t\t\tamdgpu_virt_init_data_exchange(adev);\n\n\t\t}\n\t}\n\tif (!total)\n\t\treturn -ENODEV;\n\n\tamdgpu_amdkfd_device_probe(adev);\n\tadev->cg_flags &= amdgpu_cg_mask;\n\tadev->pg_flags &= amdgpu_pg_mask;\n\n\treturn 0;\n}\n\nstatic int amdgpu_device_ip_hw_init_phase1(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.sw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON ||\n\t\t    (amdgpu_sriov_vf(adev) && (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP)) ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_IH) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->hw_init(adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"hw_init of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t\tadev->ip_blocks[i].status.hw = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_device_ip_hw_init_phase2(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.sw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->hw_init(adev);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"hw_init of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_device_fw_loading(struct amdgpu_device *adev)\n{\n\tint r = 0;\n\tint i;\n\tuint32_t smu_version;\n\n\tif (adev->asic_type >= CHIP_VEGA10) {\n\t\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\t\tif (adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_PSP)\n\t\t\t\tcontinue;\n\n\t\t\tif (!adev->ip_blocks[i].status.sw)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (adev->ip_blocks[i].status.hw == true)\n\t\t\t\tbreak;\n\n\t\t\tif (amdgpu_in_reset(adev) || adev->in_suspend) {\n\t\t\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\t\t\tif (r) {\n\t\t\t\t\tDRM_ERROR(\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tr = adev->ip_blocks[i].version->funcs->hw_init(adev);\n\t\t\t\tif (r) {\n\t\t\t\t\tDRM_ERROR(\"hw_init of IP block <%s> failed %d\\n\",\n\t\t\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tadev->ip_blocks[i].status.hw = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!amdgpu_sriov_vf(adev) || adev->asic_type == CHIP_TONGA)\n\t\tr = amdgpu_pm_load_smu_firmware(adev, &smu_version);\n\n\treturn r;\n}\n\nstatic int amdgpu_device_init_schedulers(struct amdgpu_device *adev)\n{\n\tlong timeout;\n\tint r, i;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\t \n\t\tif (!ring || ring->no_scheduler)\n\t\t\tcontinue;\n\n\t\tswitch (ring->funcs->type) {\n\t\tcase AMDGPU_RING_TYPE_GFX:\n\t\t\ttimeout = adev->gfx_timeout;\n\t\t\tbreak;\n\t\tcase AMDGPU_RING_TYPE_COMPUTE:\n\t\t\ttimeout = adev->compute_timeout;\n\t\t\tbreak;\n\t\tcase AMDGPU_RING_TYPE_SDMA:\n\t\t\ttimeout = adev->sdma_timeout;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\ttimeout = adev->video_timeout;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = drm_sched_init(&ring->sched, &amdgpu_sched_ops,\n\t\t\t\t   ring->num_hw_submission, 0,\n\t\t\t\t   timeout, adev->reset_domain->wq,\n\t\t\t\t   ring->sched_score, ring->name,\n\t\t\t\t   adev->dev);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Failed to create scheduler on ring %s.\\n\",\n\t\t\t\t  ring->name);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tamdgpu_xcp_update_partition_sched_list(adev);\n\n\treturn 0;\n}\n\n\n \nstatic int amdgpu_device_ip_init(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tr = amdgpu_ras_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->sw_init((void *)adev);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"sw_init of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\tgoto init_failed;\n\t\t}\n\t\tadev->ip_blocks[i].status.sw = true;\n\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON) {\n\t\t\t \n\t\t\tr = adev->ip_blocks[i].version->funcs->hw_init((void *)adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"hw_init %d failed %d\\n\", i, r);\n\t\t\t\tgoto init_failed;\n\t\t\t}\n\t\t\tadev->ip_blocks[i].status.hw = true;\n\t\t} else if (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC) {\n\t\t\t \n\t\t\t \n\t\t\tif (amdgpu_sriov_vf(adev))\n\t\t\t\tamdgpu_virt_exchange_data(adev);\n\n\t\t\tr = amdgpu_device_mem_scratch_init(adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"amdgpu_mem_scratch_init failed %d\\n\", r);\n\t\t\t\tgoto init_failed;\n\t\t\t}\n\t\t\tr = adev->ip_blocks[i].version->funcs->hw_init((void *)adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"hw_init %d failed %d\\n\", i, r);\n\t\t\t\tgoto init_failed;\n\t\t\t}\n\t\t\tr = amdgpu_device_wb_init(adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"amdgpu_device_wb_init failed %d\\n\", r);\n\t\t\t\tgoto init_failed;\n\t\t\t}\n\t\t\tadev->ip_blocks[i].status.hw = true;\n\n\t\t\t \n\t\t\tif (adev->gfx.mcbp) {\n\t\t\t\tr = amdgpu_allocate_static_csa(adev, &adev->virt.csa_obj,\n\t\t\t\t\t\t\t       AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\t       AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t\t\t       AMDGPU_CSA_SIZE);\n\t\t\t\tif (r) {\n\t\t\t\t\tDRM_ERROR(\"allocate CSA failed %d\\n\", r);\n\t\t\t\t\tgoto init_failed;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tamdgpu_virt_init_data_exchange(adev);\n\n\tr = amdgpu_ib_pool_init(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"IB initialization failed (%d).\\n\", r);\n\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_IB_INIT_FAIL, 0, r);\n\t\tgoto init_failed;\n\t}\n\n\tr = amdgpu_ucode_create_bo(adev);  \n\tif (r)\n\t\tgoto init_failed;\n\n\tr = amdgpu_device_ip_hw_init_phase1(adev);\n\tif (r)\n\t\tgoto init_failed;\n\n\tr = amdgpu_device_fw_loading(adev);\n\tif (r)\n\t\tgoto init_failed;\n\n\tr = amdgpu_device_ip_hw_init_phase2(adev);\n\tif (r)\n\t\tgoto init_failed;\n\n\t \n\tr = amdgpu_ras_recovery_init(adev);\n\tif (r)\n\t\tgoto init_failed;\n\n\t \n\tif (adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\tif (amdgpu_xgmi_add_device(adev) == 0) {\n\t\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\t\tstruct amdgpu_hive_info *hive = amdgpu_get_xgmi_hive(adev);\n\n\t\t\t\tif (WARN_ON(!hive)) {\n\t\t\t\t\tr = -ENOENT;\n\t\t\t\t\tgoto init_failed;\n\t\t\t\t}\n\n\t\t\t\tif (!hive->reset_domain ||\n\t\t\t\t    !amdgpu_reset_get_reset_domain(hive->reset_domain)) {\n\t\t\t\t\tr = -ENOENT;\n\t\t\t\t\tamdgpu_put_xgmi_hive(hive);\n\t\t\t\t\tgoto init_failed;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tamdgpu_reset_put_reset_domain(adev->reset_domain);\n\t\t\t\tadev->reset_domain = hive->reset_domain;\n\t\t\t\tamdgpu_put_xgmi_hive(hive);\n\t\t\t}\n\t\t}\n\t}\n\n\tr = amdgpu_device_init_schedulers(adev);\n\tif (r)\n\t\tgoto init_failed;\n\n\t \n\tif (!adev->gmc.xgmi.pending_reset) {\n\t\tkgd2kfd_init_zone_device(adev);\n\t\tamdgpu_amdkfd_device_init(adev);\n\t}\n\n\tamdgpu_fru_get_product_info(adev);\n\ninit_failed:\n\n\treturn r;\n}\n\n \nstatic void amdgpu_device_fill_reset_magic(struct amdgpu_device *adev)\n{\n\tmemcpy(adev->reset_magic, adev->gart.ptr, AMDGPU_RESET_MAGIC_NUM);\n}\n\n \nstatic bool amdgpu_device_check_vram_lost(struct amdgpu_device *adev)\n{\n\tif (memcmp(adev->gart.ptr, adev->reset_magic,\n\t\t\tAMDGPU_RESET_MAGIC_NUM))\n\t\treturn true;\n\n\tif (!amdgpu_in_reset(adev))\n\t\treturn false;\n\n\t \n\tswitch (amdgpu_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_BACO:\n\tcase AMD_RESET_METHOD_MODE1:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \n\nint amdgpu_device_set_cg_state(struct amdgpu_device *adev,\n\t\t\t       enum amd_clockgating_state state)\n{\n\tint i, j, r;\n\n\tif (amdgpu_emu_mode == 1)\n\t\treturn 0;\n\n\tfor (j = 0; j < adev->num_ip_blocks; j++) {\n\t\ti = state == AMD_CG_STATE_GATE ? j : adev->num_ip_blocks - j - 1;\n\t\tif (!adev->ip_blocks[i].status.late_initialized)\n\t\t\tcontinue;\n\t\t \n\t\tif (adev->in_s0ix &&\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GFX ||\n\t\t     adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\t \n\t\tif (adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_UVD &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_VCE &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_VCN &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_JPEG &&\n\t\t    adev->ip_blocks[i].version->funcs->set_clockgating_state) {\n\t\t\t \n\t\t\tr = adev->ip_blocks[i].version->funcs->set_clockgating_state((void *)adev,\n\t\t\t\t\t\t\t\t\t\t     state);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"set_clockgating_state(gate) of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_device_set_pg_state(struct amdgpu_device *adev,\n\t\t\t       enum amd_powergating_state state)\n{\n\tint i, j, r;\n\n\tif (amdgpu_emu_mode == 1)\n\t\treturn 0;\n\n\tfor (j = 0; j < adev->num_ip_blocks; j++) {\n\t\ti = state == AMD_PG_STATE_GATE ? j : adev->num_ip_blocks - j - 1;\n\t\tif (!adev->ip_blocks[i].status.late_initialized)\n\t\t\tcontinue;\n\t\t \n\t\tif (adev->in_s0ix &&\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GFX ||\n\t\t     adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\t\t \n\t\tif (adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_UVD &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_VCE &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_VCN &&\n\t\t    adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_JPEG &&\n\t\t    adev->ip_blocks[i].version->funcs->set_powergating_state) {\n\t\t\t \n\t\t\tr = adev->ip_blocks[i].version->funcs->set_powergating_state((void *)adev,\n\t\t\t\t\t\t\t\t\t\t\tstate);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"set_powergating_state(gate) of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_device_enable_mgpu_fan_boost(void)\n{\n\tstruct amdgpu_gpu_instance *gpu_ins;\n\tstruct amdgpu_device *adev;\n\tint i, ret = 0;\n\n\tmutex_lock(&mgpu_info.mutex);\n\n\t \n\tif (mgpu_info.num_dgpu < 2)\n\t\tgoto out;\n\n\tfor (i = 0; i < mgpu_info.num_dgpu; i++) {\n\t\tgpu_ins = &(mgpu_info.gpu_ins[i]);\n\t\tadev = gpu_ins->adev;\n\t\tif (!(adev->flags & AMD_IS_APU) &&\n\t\t    !gpu_ins->mgpu_fan_enabled) {\n\t\t\tret = amdgpu_dpm_enable_mgpu_fan_boost(adev);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tgpu_ins->mgpu_fan_enabled = 1;\n\t\t}\n\t}\n\nout:\n\tmutex_unlock(&mgpu_info.mutex);\n\n\treturn ret;\n}\n\n \nstatic int amdgpu_device_ip_late_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_gpu_instance *gpu_instance;\n\tint i = 0, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->funcs->late_init) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->late_init((void *)adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"late_init of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t\tadev->ip_blocks[i].status.late_initialized = true;\n\t}\n\n\tr = amdgpu_ras_late_init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu_ras_late_init failed %d\", r);\n\t\treturn r;\n\t}\n\n\tamdgpu_ras_set_error_query_ready(adev, true);\n\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_GATE);\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_GATE);\n\n\tamdgpu_device_fill_reset_magic(adev);\n\n\tr = amdgpu_device_enable_mgpu_fan_boost();\n\tif (r)\n\t\tDRM_ERROR(\"enable mgpu fan boost failed (%d).\\n\", r);\n\n\t \n\tif (amdgpu_passthrough(adev) &&\n\t    ((adev->asic_type == CHIP_ARCTURUS && adev->gmc.xgmi.num_physical_nodes > 1) ||\n\t     adev->asic_type == CHIP_ALDEBARAN))\n\t\tamdgpu_dpm_handle_passthrough_sbr(adev, true);\n\n\tif (adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\tmutex_lock(&mgpu_info.mutex);\n\n\t\t \n\t\tif (mgpu_info.num_dgpu == adev->gmc.xgmi.num_physical_nodes) {\n\t\t\tfor (i = 0; i < mgpu_info.num_gpu; i++) {\n\t\t\t\tgpu_instance = &(mgpu_info.gpu_ins[i]);\n\t\t\t\tif (gpu_instance->adev->flags & AMD_IS_APU)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tr = amdgpu_xgmi_set_pstate(gpu_instance->adev,\n\t\t\t\t\t\tAMDGPU_XGMI_PSTATE_MIN);\n\t\t\t\tif (r) {\n\t\t\t\t\tDRM_ERROR(\"pstate setting failed (%d).\\n\", r);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tmutex_unlock(&mgpu_info.mutex);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_device_smu_fini_early(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tif (adev->ip_versions[GC_HWIP][0] > IP_VERSION(9, 0, 0))\n\t\treturn;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SMC) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->hw_fini((void *)adev);\n\t\t\t \n\t\t\tif (r) {\n\t\t\t\tDRM_DEBUG(\"hw_fini of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t}\n\t\t\tadev->ip_blocks[i].status.hw = false;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int amdgpu_device_ip_fini_early(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].version->funcs->early_fini)\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->early_fini((void *)adev);\n\t\tif (r) {\n\t\t\tDRM_DEBUG(\"early_fini of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t}\n\t}\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\tamdgpu_amdkfd_suspend(adev, false);\n\n\t \n\tamdgpu_device_smu_fini_early(adev);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\n\t\tr = adev->ip_blocks[i].version->funcs->hw_fini((void *)adev);\n\t\t \n\t\tif (r) {\n\t\t\tDRM_DEBUG(\"hw_fini of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tif (amdgpu_virt_release_full_gpu(adev, false))\n\t\t\tDRM_ERROR(\"failed to release exclusive mode on fini\\n\");\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_fini(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tif (amdgpu_sriov_vf(adev) && adev->virt.ras_init_done)\n\t\tamdgpu_virt_release_ras_err_handler_data(adev);\n\n\tif (adev->gmc.xgmi.num_physical_nodes > 1)\n\t\tamdgpu_xgmi_remove_device(adev);\n\n\tamdgpu_amdkfd_device_fini_sw(adev);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!adev->ip_blocks[i].status.sw)\n\t\t\tcontinue;\n\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC) {\n\t\t\tamdgpu_ucode_free_bo(adev);\n\t\t\tamdgpu_free_static_csa(&adev->virt.csa_obj);\n\t\t\tamdgpu_device_wb_fini(adev);\n\t\t\tamdgpu_device_mem_scratch_fini(adev);\n\t\t\tamdgpu_ib_pool_fini(adev);\n\t\t}\n\n\t\tr = adev->ip_blocks[i].version->funcs->sw_fini((void *)adev);\n\t\t \n\t\tif (r) {\n\t\t\tDRM_DEBUG(\"sw_fini of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t}\n\t\tadev->ip_blocks[i].status.sw = false;\n\t\tadev->ip_blocks[i].status.valid = false;\n\t}\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!adev->ip_blocks[i].status.late_initialized)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->funcs->late_fini)\n\t\t\tadev->ip_blocks[i].version->funcs->late_fini((void *)adev);\n\t\tadev->ip_blocks[i].status.late_initialized = false;\n\t}\n\n\tamdgpu_ras_fini(adev);\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_device_delayed_init_work_handler(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(work, struct amdgpu_device, delayed_init_work.work);\n\tint r;\n\n\tr = amdgpu_ib_ring_tests(adev);\n\tif (r)\n\t\tDRM_ERROR(\"ib ring test failed (%d).\\n\", r);\n}\n\nstatic void amdgpu_device_delay_enable_gfx_off(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(work, struct amdgpu_device, gfx.gfx_off_delay_work.work);\n\n\tWARN_ON_ONCE(adev->gfx.gfx_off_state);\n\tWARN_ON_ONCE(adev->gfx.gfx_off_req_count);\n\n\tif (!amdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_GFX, true))\n\t\tadev->gfx.gfx_off_state = true;\n}\n\n \nstatic int amdgpu_device_ip_suspend_phase1(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tamdgpu_device_set_pg_state(adev, AMD_PG_STATE_UNGATE);\n\tamdgpu_device_set_cg_state(adev, AMD_CG_STATE_UNGATE);\n\n\t \n\tif (amdgpu_dpm_set_df_cstate(adev, DF_CSTATE_DISALLOW))\n\t\tdev_warn(adev->dev, \"Failed to disallow df cstate\");\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (adev->ip_blocks[i].version->type != AMD_IP_BLOCK_TYPE_DCE)\n\t\t\tcontinue;\n\n\t\t \n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\t\t \n\t\tif (r) {\n\t\t\tDRM_ERROR(\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->ip_blocks[i].status.hw = false;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_suspend_phase2(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tif (adev->in_s0ix)\n\t\tamdgpu_dpm_gfx_state_change(adev, sGpuChangeState_D3Entry);\n\n\tfor (i = adev->num_ip_blocks - 1; i >= 0; i--) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\t \n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_DCE)\n\t\t\tcontinue;\n\t\t \n\t\tif (amdgpu_ras_intr_triggered() &&\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP) {\n\t\t\tadev->ip_blocks[i].status.hw = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (adev->gmc.xgmi.pending_reset &&\n\t\t    !(adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC ||\n\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SMC ||\n\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON ||\n\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_IH)) {\n\t\t\tadev->ip_blocks[i].status.hw = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (adev->in_s0ix &&\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP ||\n\t\t     adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GFX ||\n\t\t     adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_MES))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (adev->in_s0ix &&\n\t\t    (adev->ip_versions[SDMA0_HWIP][0] >= IP_VERSION(5, 0, 0)) &&\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SDMA))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (amdgpu_in_reset(adev) &&\n\t\t    (adev->flags & AMD_IS_APU) && adev->gfx.imu.funcs &&\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP)\n\t\t\tcontinue;\n\n\t\t \n\t\tr = adev->ip_blocks[i].version->funcs->suspend(adev);\n\t\t \n\t\tif (r) {\n\t\t\tDRM_ERROR(\"suspend of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t}\n\t\tadev->ip_blocks[i].status.hw = false;\n\t\t \n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SMC) {\n\t\t\t\tr = amdgpu_dpm_set_mp1_state(adev, adev->mp1_state);\n\t\t\t\tif (r) {\n\t\t\t\t\tDRM_ERROR(\"SMC failed to set mp1 state %d, %d\\n\",\n\t\t\t\t\t\t\tadev->mp1_state, r);\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_device_ip_suspend(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_fini_data_exchange(adev);\n\t\tamdgpu_virt_request_full_gpu(adev, false);\n\t}\n\n\tr = amdgpu_device_ip_suspend_phase1(adev);\n\tif (r)\n\t\treturn r;\n\tr = amdgpu_device_ip_suspend_phase2(adev);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tamdgpu_virt_release_full_gpu(adev, false);\n\n\treturn r;\n}\n\nstatic int amdgpu_device_ip_reinit_early_sriov(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tstatic enum amd_ip_block_type ip_order[] = {\n\t\tAMD_IP_BLOCK_TYPE_COMMON,\n\t\tAMD_IP_BLOCK_TYPE_GMC,\n\t\tAMD_IP_BLOCK_TYPE_PSP,\n\t\tAMD_IP_BLOCK_TYPE_IH,\n\t};\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tint j;\n\t\tstruct amdgpu_ip_block *block;\n\n\t\tblock = &adev->ip_blocks[i];\n\t\tblock->status.hw = false;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(ip_order); j++) {\n\n\t\t\tif (block->version->type != ip_order[j] ||\n\t\t\t\t!block->status.valid)\n\t\t\t\tcontinue;\n\n\t\t\tr = block->version->funcs->hw_init(adev);\n\t\t\tDRM_INFO(\"RE-INIT-early: %s %s\\n\", block->version->funcs->name, r?\"failed\":\"succeeded\");\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tblock->status.hw = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_device_ip_reinit_late_sriov(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tstatic enum amd_ip_block_type ip_order[] = {\n\t\tAMD_IP_BLOCK_TYPE_SMC,\n\t\tAMD_IP_BLOCK_TYPE_DCE,\n\t\tAMD_IP_BLOCK_TYPE_GFX,\n\t\tAMD_IP_BLOCK_TYPE_SDMA,\n\t\tAMD_IP_BLOCK_TYPE_MES,\n\t\tAMD_IP_BLOCK_TYPE_UVD,\n\t\tAMD_IP_BLOCK_TYPE_VCE,\n\t\tAMD_IP_BLOCK_TYPE_VCN,\n\t\tAMD_IP_BLOCK_TYPE_JPEG\n\t};\n\n\tfor (i = 0; i < ARRAY_SIZE(ip_order); i++) {\n\t\tint j;\n\t\tstruct amdgpu_ip_block *block;\n\n\t\tfor (j = 0; j < adev->num_ip_blocks; j++) {\n\t\t\tblock = &adev->ip_blocks[j];\n\n\t\t\tif (block->version->type != ip_order[i] ||\n\t\t\t\t!block->status.valid ||\n\t\t\t\tblock->status.hw)\n\t\t\t\tcontinue;\n\n\t\t\tif (block->version->type == AMD_IP_BLOCK_TYPE_SMC)\n\t\t\t\tr = block->version->funcs->resume(adev);\n\t\t\telse\n\t\t\t\tr = block->version->funcs->hw_init(adev);\n\n\t\t\tDRM_INFO(\"RE-INIT-late: %s %s\\n\", block->version->funcs->name, r?\"failed\":\"succeeded\");\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tblock->status.hw = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_resume_phase1(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid || adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_IH ||\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP && amdgpu_sriov_vf(adev))) {\n\n\t\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t\tadev->ip_blocks[i].status.hw = true;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_resume_phase2(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid || adev->ip_blocks[i].status.hw)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_IH ||\n\t\t    adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP)\n\t\t\tcontinue;\n\t\tr = adev->ip_blocks[i].version->funcs->resume(adev);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"resume of IP block <%s> failed %d\\n\",\n\t\t\t\t  adev->ip_blocks[i].version->funcs->name, r);\n\t\t\treturn r;\n\t\t}\n\t\tadev->ip_blocks[i].status.hw = true;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_resume(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = amdgpu_device_ip_resume_phase1(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_device_fw_loading(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_device_ip_resume_phase2(adev);\n\n\treturn r;\n}\n\n \nstatic void amdgpu_device_detect_sriov_bios(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tif (adev->is_atom_fw) {\n\t\t\tif (amdgpu_atomfirmware_gpu_virtualization_supported(adev))\n\t\t\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_SRIOV_VBIOS;\n\t\t} else {\n\t\t\tif (amdgpu_atombios_has_gpu_virtualization_table(adev))\n\t\t\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_SRIOV_VBIOS;\n\t\t}\n\n\t\tif (!(adev->virt.caps & AMDGPU_SRIOV_CAPS_SRIOV_VBIOS))\n\t\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_NO_VBIOS, 0, 0);\n\t}\n}\n\n \nbool amdgpu_device_asic_has_dc_support(enum amd_asic_type asic_type)\n{\n\tswitch (asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_SI\n\tcase CHIP_HAINAN:\n#endif\n\tcase CHIP_TOPAZ:\n\t\t \n\t\treturn false;\n#if defined(CONFIG_DRM_AMD_DC)\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\tcase CHIP_VERDE:\n\tcase CHIP_OLAND:\n\t\t \n#if defined(CONFIG_DRM_AMD_DC_SI)\n\t\treturn amdgpu_dc > 0;\n#else\n\t\treturn false;\n#endif\n\tcase CHIP_BONAIRE:\n\tcase CHIP_KAVERI:\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n\t\t \n\t\treturn amdgpu_dc > 0;\n\tdefault:\n\t\treturn amdgpu_dc != 0;\n#else\n\tdefault:\n\t\tif (amdgpu_dc > 0)\n\t\t\tDRM_INFO_ONCE(\"Display Core has been requested via kernel parameter but isn't supported by ASIC, ignoring\\n\");\n\t\treturn false;\n#endif\n\t}\n}\n\n \nbool amdgpu_device_has_dc_support(struct amdgpu_device *adev)\n{\n\tif (adev->enable_virtual_display ||\n\t    (adev->harvest_ip_mask & AMD_HARVEST_IP_DMU_MASK))\n\t\treturn false;\n\n\treturn amdgpu_device_asic_has_dc_support(adev->asic_type);\n}\n\nstatic void amdgpu_device_xgmi_reset_func(struct work_struct *__work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(__work, struct amdgpu_device, xgmi_reset_work);\n\tstruct amdgpu_hive_info *hive = amdgpu_get_xgmi_hive(adev);\n\n\t \n\tif (WARN_ON(!hive))\n\t\treturn;\n\n\t \n\tif (amdgpu_asic_reset_method(adev) == AMD_RESET_METHOD_BACO) {\n\n\t\ttask_barrier_enter(&hive->tb);\n\t\tadev->asic_reset_res = amdgpu_device_baco_enter(adev_to_drm(adev));\n\n\t\tif (adev->asic_reset_res)\n\t\t\tgoto fail;\n\n\t\ttask_barrier_exit(&hive->tb);\n\t\tadev->asic_reset_res = amdgpu_device_baco_exit(adev_to_drm(adev));\n\n\t\tif (adev->asic_reset_res)\n\t\t\tgoto fail;\n\n\t\tif (adev->mmhub.ras && adev->mmhub.ras->ras_block.hw_ops &&\n\t\t    adev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\tadev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\t} else {\n\n\t\ttask_barrier_full(&hive->tb);\n\t\tadev->asic_reset_res =  amdgpu_asic_reset(adev);\n\t}\n\nfail:\n\tif (adev->asic_reset_res)\n\t\tDRM_WARN(\"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t adev->asic_reset_res, adev_to_drm(adev)->unique);\n\tamdgpu_put_xgmi_hive(hive);\n}\n\nstatic int amdgpu_device_get_job_timeout_settings(struct amdgpu_device *adev)\n{\n\tchar *input = amdgpu_lockup_timeout;\n\tchar *timeout_setting = NULL;\n\tint index = 0;\n\tlong timeout;\n\tint ret = 0;\n\n\t \n\tadev->gfx_timeout = msecs_to_jiffies(10000);\n\tadev->sdma_timeout = adev->video_timeout = adev->gfx_timeout;\n\tif (amdgpu_sriov_vf(adev))\n\t\tadev->compute_timeout = amdgpu_sriov_is_pp_one_vf(adev) ?\n\t\t\t\t\tmsecs_to_jiffies(60000) : msecs_to_jiffies(10000);\n\telse\n\t\tadev->compute_timeout =  msecs_to_jiffies(60000);\n\n\tif (strnlen(input, AMDGPU_MAX_TIMEOUT_PARAM_LENGTH)) {\n\t\twhile ((timeout_setting = strsep(&input, \",\")) &&\n\t\t\t\tstrnlen(timeout_setting, AMDGPU_MAX_TIMEOUT_PARAM_LENGTH)) {\n\t\t\tret = kstrtol(timeout_setting, 0, &timeout);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tif (timeout == 0) {\n\t\t\t\tindex++;\n\t\t\t\tcontinue;\n\t\t\t} else if (timeout < 0) {\n\t\t\t\ttimeout = MAX_SCHEDULE_TIMEOUT;\n\t\t\t\tdev_warn(adev->dev, \"lockup timeout disabled\");\n\t\t\t\tadd_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK);\n\t\t\t} else {\n\t\t\t\ttimeout = msecs_to_jiffies(timeout);\n\t\t\t}\n\n\t\t\tswitch (index++) {\n\t\t\tcase 0:\n\t\t\t\tadev->gfx_timeout = timeout;\n\t\t\t\tbreak;\n\t\t\tcase 1:\n\t\t\t\tadev->compute_timeout = timeout;\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tadev->sdma_timeout = timeout;\n\t\t\t\tbreak;\n\t\t\tcase 3:\n\t\t\t\tadev->video_timeout = timeout;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (index == 1) {\n\t\t\tadev->sdma_timeout = adev->video_timeout = adev->gfx_timeout;\n\t\t\tif (amdgpu_sriov_vf(adev) || amdgpu_passthrough(adev))\n\t\t\t\tadev->compute_timeout = adev->gfx_timeout;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic void amdgpu_device_check_iommu_direct_map(struct amdgpu_device *adev)\n{\n\tstruct iommu_domain *domain;\n\n\tdomain = iommu_get_domain_for_dev(adev->dev);\n\tif (!domain || domain->type == IOMMU_DOMAIN_IDENTITY)\n\t\tadev->ram_is_direct_mapped = true;\n}\n\nstatic const struct attribute *amdgpu_dev_attributes[] = {\n\t&dev_attr_pcie_replay_count.attr,\n\tNULL\n};\n\nstatic void amdgpu_device_set_mcbp(struct amdgpu_device *adev)\n{\n\tif (amdgpu_mcbp == 1)\n\t\tadev->gfx.mcbp = true;\n\telse if (amdgpu_mcbp == 0)\n\t\tadev->gfx.mcbp = false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tadev->gfx.mcbp = true;\n\n\tif (adev->gfx.mcbp)\n\t\tDRM_INFO(\"MCBP is enabled\\n\");\n}\n\n \nint amdgpu_device_init(struct amdgpu_device *adev,\n\t\t       uint32_t flags)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\tstruct pci_dev *pdev = adev->pdev;\n\tint r, i;\n\tbool px = false;\n\tu32 max_MBps;\n\tint tmp;\n\n\tadev->shutdown = false;\n\tadev->flags = flags;\n\n\tif (amdgpu_force_asic_type >= 0 && amdgpu_force_asic_type < CHIP_LAST)\n\t\tadev->asic_type = amdgpu_force_asic_type;\n\telse\n\t\tadev->asic_type = flags & AMD_ASIC_MASK;\n\n\tadev->usec_timeout = AMDGPU_MAX_USEC_TIMEOUT;\n\tif (amdgpu_emu_mode == 1)\n\t\tadev->usec_timeout *= 10;\n\tadev->gmc.gart_size = 512 * 1024 * 1024;\n\tadev->accel_working = false;\n\tadev->num_rings = 0;\n\tRCU_INIT_POINTER(adev->gang_submit, dma_fence_get_stub());\n\tadev->mman.buffer_funcs = NULL;\n\tadev->mman.buffer_funcs_ring = NULL;\n\tadev->vm_manager.vm_pte_funcs = NULL;\n\tadev->vm_manager.vm_pte_num_scheds = 0;\n\tadev->gmc.gmc_funcs = NULL;\n\tadev->harvest_ip_mask = 0x0;\n\tadev->fence_context = dma_fence_context_alloc(AMDGPU_MAX_RINGS);\n\tbitmap_zero(adev->gfx.pipe_reserve_bitmap, AMDGPU_MAX_COMPUTE_QUEUES);\n\n\tadev->smc_rreg = &amdgpu_invalid_rreg;\n\tadev->smc_wreg = &amdgpu_invalid_wreg;\n\tadev->pcie_rreg = &amdgpu_invalid_rreg;\n\tadev->pcie_wreg = &amdgpu_invalid_wreg;\n\tadev->pcie_rreg_ext = &amdgpu_invalid_rreg_ext;\n\tadev->pcie_wreg_ext = &amdgpu_invalid_wreg_ext;\n\tadev->pciep_rreg = &amdgpu_invalid_rreg;\n\tadev->pciep_wreg = &amdgpu_invalid_wreg;\n\tadev->pcie_rreg64 = &amdgpu_invalid_rreg64;\n\tadev->pcie_wreg64 = &amdgpu_invalid_wreg64;\n\tadev->uvd_ctx_rreg = &amdgpu_invalid_rreg;\n\tadev->uvd_ctx_wreg = &amdgpu_invalid_wreg;\n\tadev->didt_rreg = &amdgpu_invalid_rreg;\n\tadev->didt_wreg = &amdgpu_invalid_wreg;\n\tadev->gc_cac_rreg = &amdgpu_invalid_rreg;\n\tadev->gc_cac_wreg = &amdgpu_invalid_wreg;\n\tadev->audio_endpt_rreg = &amdgpu_block_invalid_rreg;\n\tadev->audio_endpt_wreg = &amdgpu_block_invalid_wreg;\n\n\tDRM_INFO(\"initializing kernel modesetting (%s 0x%04X:0x%04X 0x%04X:0x%04X 0x%02X).\\n\",\n\t\t amdgpu_asic_name[adev->asic_type], pdev->vendor, pdev->device,\n\t\t pdev->subsystem_vendor, pdev->subsystem_device, pdev->revision);\n\n\t \n\tmutex_init(&adev->firmware.mutex);\n\tmutex_init(&adev->pm.mutex);\n\tmutex_init(&adev->gfx.gpu_clock_mutex);\n\tmutex_init(&adev->srbm_mutex);\n\tmutex_init(&adev->gfx.pipe_reserve_mutex);\n\tmutex_init(&adev->gfx.gfx_off_mutex);\n\tmutex_init(&adev->gfx.partition_mutex);\n\tmutex_init(&adev->grbm_idx_mutex);\n\tmutex_init(&adev->mn_lock);\n\tmutex_init(&adev->virt.vf_errors.lock);\n\thash_init(adev->mn_hash);\n\tmutex_init(&adev->psp.mutex);\n\tmutex_init(&adev->notifier_lock);\n\tmutex_init(&adev->pm.stable_pstate_ctx_lock);\n\tmutex_init(&adev->benchmark_mutex);\n\n\tamdgpu_device_init_apu_flags(adev);\n\n\tr = amdgpu_device_check_arguments(adev);\n\tif (r)\n\t\treturn r;\n\n\tspin_lock_init(&adev->mmio_idx_lock);\n\tspin_lock_init(&adev->smc_idx_lock);\n\tspin_lock_init(&adev->pcie_idx_lock);\n\tspin_lock_init(&adev->uvd_ctx_idx_lock);\n\tspin_lock_init(&adev->didt_idx_lock);\n\tspin_lock_init(&adev->gc_cac_idx_lock);\n\tspin_lock_init(&adev->se_cac_idx_lock);\n\tspin_lock_init(&adev->audio_endpt_idx_lock);\n\tspin_lock_init(&adev->mm_stats.lock);\n\n\tINIT_LIST_HEAD(&adev->shadow_list);\n\tmutex_init(&adev->shadow_list_lock);\n\n\tINIT_LIST_HEAD(&adev->reset_list);\n\n\tINIT_LIST_HEAD(&adev->ras_list);\n\n\tINIT_DELAYED_WORK(&adev->delayed_init_work,\n\t\t\t  amdgpu_device_delayed_init_work_handler);\n\tINIT_DELAYED_WORK(&adev->gfx.gfx_off_delay_work,\n\t\t\t  amdgpu_device_delay_enable_gfx_off);\n\n\tINIT_WORK(&adev->xgmi_reset_work, amdgpu_device_xgmi_reset_func);\n\n\tadev->gfx.gfx_off_req_count = 1;\n\tadev->gfx.gfx_off_residency = 0;\n\tadev->gfx.gfx_off_entrycount = 0;\n\tadev->pm.ac_power = power_supply_is_system_supplied() > 0;\n\n\tatomic_set(&adev->throttling_logging_enabled, 1);\n\t \n\tratelimit_state_init(&adev->throttling_logging_rs, (60 - 1) * HZ, 1);\n\tratelimit_set_flags(&adev->throttling_logging_rs, RATELIMIT_MSG_ON_RELEASE);\n\n\t \n\t \n\tif (adev->asic_type >= CHIP_BONAIRE) {\n\t\tadev->rmmio_base = pci_resource_start(adev->pdev, 5);\n\t\tadev->rmmio_size = pci_resource_len(adev->pdev, 5);\n\t} else {\n\t\tadev->rmmio_base = pci_resource_start(adev->pdev, 2);\n\t\tadev->rmmio_size = pci_resource_len(adev->pdev, 2);\n\t}\n\n\tfor (i = 0; i < AMD_IP_BLOCK_TYPE_NUM; i++)\n\t\tatomic_set(&adev->pm.pwr_state[i], POWER_STATE_UNKNOWN);\n\n\tadev->rmmio = ioremap(adev->rmmio_base, adev->rmmio_size);\n\tif (!adev->rmmio)\n\t\treturn -ENOMEM;\n\n\tDRM_INFO(\"register mmio base: 0x%08X\\n\", (uint32_t)adev->rmmio_base);\n\tDRM_INFO(\"register mmio size: %u\\n\", (unsigned int)adev->rmmio_size);\n\n\t \n\tadev->reset_domain = amdgpu_reset_create_reset_domain(SINGLE_DEVICE, \"amdgpu-reset-dev\");\n\tif (!adev->reset_domain)\n\t\treturn -ENOMEM;\n\n\t \n\tamdgpu_detect_virtualization(adev);\n\n\tamdgpu_device_get_pcie_info(adev);\n\n\tr = amdgpu_device_get_job_timeout_settings(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"invalid lockup_timeout parameter syntax\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_device_ip_early_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_device_set_mcbp(adev);\n\n\t \n\tr = drm_aperture_remove_conflicting_pci_framebuffers(adev->pdev, &amdgpu_kms_driver);\n\tif (r)\n\t\treturn r;\n\n\t \n\tamdgpu_gmc_tmz_set(adev);\n\n\tamdgpu_gmc_noretry_set(adev);\n\t \n\tif (adev->gmc.xgmi.supported) {\n\t\tr = adev->gfxhub.funcs->get_xgmi_info(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tif (amdgpu_sriov_vf(adev)) {\n\t\tif (adev->virt.fw_reserve.p_pf2vf)\n\t\t\tadev->have_atomics_support = ((struct amd_sriov_msg_pf2vf_info *)\n\t\t\t\t\t\t      adev->virt.fw_reserve.p_pf2vf)->pcie_atomic_ops_support_flags ==\n\t\t\t\t(PCI_EXP_DEVCAP2_ATOMIC_COMP32 | PCI_EXP_DEVCAP2_ATOMIC_COMP64);\n\t \n\t} else if ((adev->flags & AMD_IS_APU) &&\n\t\t   (adev->ip_versions[GC_HWIP][0] > IP_VERSION(9, 0, 0))) {\n\t\tadev->have_atomics_support = true;\n\t} else {\n\t\tadev->have_atomics_support =\n\t\t\t!pci_enable_atomic_ops_to_root(adev->pdev,\n\t\t\t\t\t  PCI_EXP_DEVCAP2_ATOMIC_COMP32 |\n\t\t\t\t\t  PCI_EXP_DEVCAP2_ATOMIC_COMP64);\n\t}\n\n\tif (!adev->have_atomics_support)\n\t\tdev_info(adev->dev, \"PCIE atomic ops is not supported\\n\");\n\n\t \n\tamdgpu_doorbell_init(adev);\n\n\tif (amdgpu_emu_mode == 1) {\n\t\t \n\t\temu_soc_asic_init(adev);\n\t\tgoto fence_driver_init;\n\t}\n\n\tamdgpu_reset_init(adev);\n\n\t \n\tif (adev->bios)\n\t\tamdgpu_device_detect_sriov_bios(adev);\n\n\t \n\tif (!amdgpu_sriov_vf(adev) && amdgpu_asic_need_reset_on_init(adev)) {\n\t\tif (adev->gmc.xgmi.num_physical_nodes) {\n\t\t\tdev_info(adev->dev, \"Pending hive reset.\\n\");\n\t\t\tadev->gmc.xgmi.pending_reset = true;\n\t\t\t \n\t\t\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\t\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!(adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC ||\n\t\t\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_COMMON ||\n\t\t\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_IH ||\n\t\t\t\t      adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SMC)) {\n\t\t\t\t\tDRM_DEBUG(\"IP %s disabled for hw_init.\\n\",\n\t\t\t\t\t\tadev->ip_blocks[i].version->funcs->name);\n\t\t\t\t\tadev->ip_blocks[i].status.hw = true;\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\ttmp = amdgpu_reset_method;\n\t\t\t \n\t\t\tamdgpu_reset_method = AMD_RESET_METHOD_NONE;\n\t\t\tr = amdgpu_asic_reset(adev);\n\t\t\tamdgpu_reset_method = tmp;\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev, \"asic reset on init failed\\n\");\n\t\t\t\tgoto failed;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (amdgpu_device_need_post(adev)) {\n\t\tif (!adev->bios) {\n\t\t\tdev_err(adev->dev, \"no vBIOS found\\n\");\n\t\t\tr = -EINVAL;\n\t\t\tgoto failed;\n\t\t}\n\t\tDRM_INFO(\"GPU posting now...\\n\");\n\t\tr = amdgpu_device_asic_init(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"gpu post error!\\n\");\n\t\t\tgoto failed;\n\t\t}\n\t}\n\n\tif (adev->bios) {\n\t\tif (adev->is_atom_fw) {\n\t\t\t \n\t\t\tr = amdgpu_atomfirmware_get_clock_info(adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev, \"amdgpu_atomfirmware_get_clock_info failed\\n\");\n\t\t\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_ATOMBIOS_GET_CLOCK_FAIL, 0, 0);\n\t\t\t\tgoto failed;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tr = amdgpu_atombios_get_clock_info(adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev, \"amdgpu_atombios_get_clock_info failed\\n\");\n\t\t\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_ATOMBIOS_GET_CLOCK_FAIL, 0, 0);\n\t\t\t\tgoto failed;\n\t\t\t}\n\t\t\t \n\t\t\tif (!amdgpu_device_has_dc_support(adev))\n\t\t\t\tamdgpu_atombios_i2c_init(adev);\n\t\t}\n\t}\n\nfence_driver_init:\n\t \n\tr = amdgpu_fence_driver_sw_init(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"amdgpu_fence_driver_sw_init failed\\n\");\n\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_FENCE_INIT_FAIL, 0, 0);\n\t\tgoto failed;\n\t}\n\n\t \n\tdrm_mode_config_init(adev_to_drm(adev));\n\n\tr = amdgpu_device_ip_init(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"amdgpu_device_ip_init failed\\n\");\n\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_AMDGPU_INIT_FAIL, 0, 0);\n\t\tgoto release_ras_con;\n\t}\n\n\tamdgpu_fence_driver_hw_init(adev);\n\n\tdev_info(adev->dev,\n\t\t\"SE %d, SH per SE %d, CU per SH %d, active_cu_number %d\\n\",\n\t\t\tadev->gfx.config.max_shader_engines,\n\t\t\tadev->gfx.config.max_sh_per_se,\n\t\t\tadev->gfx.config.max_cu_per_sh,\n\t\t\tadev->gfx.cu_info.number);\n\n\tadev->accel_working = true;\n\n\tamdgpu_vm_check_compute_bug(adev);\n\n\t \n\tif (amdgpu_moverate >= 0)\n\t\tmax_MBps = amdgpu_moverate;\n\telse\n\t\tmax_MBps = 8;  \n\t \n\tadev->mm_stats.log2_max_MBps = ilog2(max(1u, max_MBps));\n\n\tr = amdgpu_atombios_sysfs_init(adev);\n\tif (r)\n\t\tdrm_err(&adev->ddev,\n\t\t\t\"registering atombios sysfs failed (%d).\\n\", r);\n\n\tr = amdgpu_pm_sysfs_init(adev);\n\tif (r)\n\t\tDRM_ERROR(\"registering pm sysfs failed (%d).\\n\", r);\n\n\tr = amdgpu_ucode_sysfs_init(adev);\n\tif (r) {\n\t\tadev->ucode_sysfs_en = false;\n\t\tDRM_ERROR(\"Creating firmware sysfs failed (%d).\\n\", r);\n\t} else\n\t\tadev->ucode_sysfs_en = true;\n\n\t \n\tamdgpu_register_gpu_instance(adev);\n\n\t \n\tif (!adev->gmc.xgmi.pending_reset) {\n\t\tr = amdgpu_device_ip_late_init(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"amdgpu_device_ip_late_init failed\\n\");\n\t\t\tamdgpu_vf_error_put(adev, AMDGIM_ERROR_VF_AMDGPU_LATE_INIT_FAIL, 0, r);\n\t\t\tgoto release_ras_con;\n\t\t}\n\t\t \n\t\tamdgpu_ras_resume(adev);\n\t\tqueue_delayed_work(system_wq, &adev->delayed_init_work,\n\t\t\t\t   msecs_to_jiffies(AMDGPU_RESUME_MS));\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_release_full_gpu(adev, true);\n\t\tflush_delayed_work(&adev->delayed_init_work);\n\t}\n\n\tr = sysfs_create_files(&adev->dev->kobj, amdgpu_dev_attributes);\n\tif (r)\n\t\tdev_err(adev->dev, \"Could not create amdgpu device attr\\n\");\n\n\tamdgpu_fru_sysfs_init(adev);\n\n\tif (IS_ENABLED(CONFIG_PERF_EVENTS))\n\t\tr = amdgpu_pmu_init(adev);\n\tif (r)\n\t\tdev_err(adev->dev, \"amdgpu_pmu_init failed\\n\");\n\n\t \n\tif (amdgpu_device_cache_pci_state(adev->pdev))\n\t\tpci_restore_state(pdev);\n\n\t \n\t \n\tif ((adev->pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA)\n\t\tvga_client_register(adev->pdev, amdgpu_device_vga_set_decode);\n\n\tpx = amdgpu_device_supports_px(ddev);\n\n\tif (px || (!dev_is_removable(&adev->pdev->dev) &&\n\t\t\t\tapple_gmux_detect(NULL, NULL)))\n\t\tvga_switcheroo_register_client(adev->pdev,\n\t\t\t\t\t       &amdgpu_switcheroo_ops, px);\n\n\tif (px)\n\t\tvga_switcheroo_init_domain_pm_ops(adev->dev, &adev->vga_pm_domain);\n\n\tif (adev->gmc.xgmi.pending_reset)\n\t\tqueue_delayed_work(system_wq, &mgpu_info.delayed_reset_work,\n\t\t\t\t   msecs_to_jiffies(AMDGPU_RESUME_MS));\n\n\tamdgpu_device_check_iommu_direct_map(adev);\n\n\treturn 0;\n\nrelease_ras_con:\n\tif (amdgpu_sriov_vf(adev))\n\t\tamdgpu_virt_release_full_gpu(adev, true);\n\n\t \n\tif (amdgpu_sriov_vf(adev) &&\n\t\t!amdgpu_sriov_runtime(adev) &&\n\t\tamdgpu_virt_mmio_blocked(adev) &&\n\t\t!amdgpu_virt_wait_reset(adev)) {\n\t\tdev_err(adev->dev, \"VF exclusive mode timeout\\n\");\n\t\t \n\t\tadev->virt.caps &= ~AMDGPU_SRIOV_CAPS_RUNTIME;\n\t\tadev->virt.ops = NULL;\n\t\tr = -EAGAIN;\n\t}\n\tamdgpu_release_ras_context(adev);\n\nfailed:\n\tamdgpu_vf_error_trans_all(adev);\n\n\treturn r;\n}\n\nstatic void amdgpu_device_unmap_mmio(struct amdgpu_device *adev)\n{\n\n\t \n\tunmap_mapping_range(adev->ddev.anon_inode->i_mapping, 0, 0, 1);\n\n\t \n\tamdgpu_doorbell_fini(adev);\n\n\tiounmap(adev->rmmio);\n\tadev->rmmio = NULL;\n\tif (adev->mman.aper_base_kaddr)\n\t\tiounmap(adev->mman.aper_base_kaddr);\n\tadev->mman.aper_base_kaddr = NULL;\n\n\t \n\tif (!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu) {\n\t\tarch_phys_wc_del(adev->gmc.vram_mtrr);\n\t\tarch_io_free_memtype_wc(adev->gmc.aper_base, adev->gmc.aper_size);\n\t}\n}\n\n \nvoid amdgpu_device_fini_hw(struct amdgpu_device *adev)\n{\n\tdev_info(adev->dev, \"amdgpu: finishing device.\\n\");\n\tflush_delayed_work(&adev->delayed_init_work);\n\tadev->shutdown = true;\n\n\t \n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_request_full_gpu(adev, false);\n\t\tamdgpu_virt_fini_data_exchange(adev);\n\t}\n\n\t \n\tamdgpu_irq_disable_all(adev);\n\tif (adev->mode_info.mode_config_initialized) {\n\t\tif (!drm_drv_uses_atomic_modeset(adev_to_drm(adev)))\n\t\t\tdrm_helper_force_disable_all(adev_to_drm(adev));\n\t\telse\n\t\t\tdrm_atomic_helper_shutdown(adev_to_drm(adev));\n\t}\n\tamdgpu_fence_driver_hw_fini(adev);\n\n\tif (adev->mman.initialized)\n\t\tdrain_workqueue(adev->mman.bdev.wq);\n\n\tif (adev->pm.sysfs_initialized)\n\t\tamdgpu_pm_sysfs_fini(adev);\n\tif (adev->ucode_sysfs_en)\n\t\tamdgpu_ucode_sysfs_fini(adev);\n\tsysfs_remove_files(&adev->dev->kobj, amdgpu_dev_attributes);\n\tamdgpu_fru_sysfs_fini(adev);\n\n\t \n\tamdgpu_ras_pre_fini(adev);\n\n\tamdgpu_device_ip_fini_early(adev);\n\n\tamdgpu_irq_fini_hw(adev);\n\n\tif (adev->mman.initialized)\n\t\tttm_device_clear_dma_mappings(&adev->mman.bdev);\n\n\tamdgpu_gart_dummy_page_fini(adev);\n\n\tif (drm_dev_is_unplugged(adev_to_drm(adev)))\n\t\tamdgpu_device_unmap_mmio(adev);\n\n}\n\nvoid amdgpu_device_fini_sw(struct amdgpu_device *adev)\n{\n\tint idx;\n\tbool px;\n\n\tamdgpu_fence_driver_sw_fini(adev);\n\tamdgpu_device_ip_fini(adev);\n\tamdgpu_ucode_release(&adev->firmware.gpu_info_fw);\n\tadev->accel_working = false;\n\tdma_fence_put(rcu_dereference_protected(adev->gang_submit, true));\n\n\tamdgpu_reset_fini(adev);\n\n\t \n\tif (!amdgpu_device_has_dc_support(adev))\n\t\tamdgpu_i2c_fini(adev);\n\n\tif (amdgpu_emu_mode != 1)\n\t\tamdgpu_atombios_fini(adev);\n\n\tkfree(adev->bios);\n\tadev->bios = NULL;\n\n\tpx = amdgpu_device_supports_px(adev_to_drm(adev));\n\n\tif (px || (!dev_is_removable(&adev->pdev->dev) &&\n\t\t\t\tapple_gmux_detect(NULL, NULL)))\n\t\tvga_switcheroo_unregister_client(adev->pdev);\n\n\tif (px)\n\t\tvga_switcheroo_fini_domain_pm_ops(adev->dev);\n\n\tif ((adev->pdev->class >> 8) == PCI_CLASS_DISPLAY_VGA)\n\t\tvga_client_unregister(adev->pdev);\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\n\t\tiounmap(adev->rmmio);\n\t\tadev->rmmio = NULL;\n\t\tamdgpu_doorbell_fini(adev);\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tif (IS_ENABLED(CONFIG_PERF_EVENTS))\n\t\tamdgpu_pmu_fini(adev);\n\tif (adev->mman.discovery_bin)\n\t\tamdgpu_discovery_fini(adev);\n\n\tamdgpu_reset_put_reset_domain(adev->reset_domain);\n\tadev->reset_domain = NULL;\n\n\tkfree(adev->pci_state);\n\n}\n\n \nstatic int amdgpu_device_evict_resources(struct amdgpu_device *adev)\n{\n\tint ret;\n\n\t \n\tif ((adev->in_s3 || adev->in_s0ix) && (adev->flags & AMD_IS_APU))\n\t\treturn 0;\n\n\tret = amdgpu_ttm_evict_resources(adev, TTM_PL_VRAM);\n\tif (ret)\n\t\tDRM_WARN(\"evicting device resources failed\\n\");\n\treturn ret;\n}\n\n \n \nint amdgpu_device_suspend(struct drm_device *dev, bool fbcon)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r = 0;\n\n\tif (dev->switch_power_state == DRM_SWITCH_POWER_OFF)\n\t\treturn 0;\n\n\tadev->in_suspend = true;\n\n\t \n\tr = amdgpu_device_evict_resources(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_fini_data_exchange(adev);\n\t\tr = amdgpu_virt_request_full_gpu(adev, false);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (amdgpu_acpi_smart_shift_update(dev, AMDGPU_SS_DEV_D3))\n\t\tDRM_WARN(\"smart shift update failed\\n\");\n\n\tif (fbcon)\n\t\tdrm_fb_helper_set_suspend_unlocked(adev_to_drm(adev)->fb_helper, true);\n\n\tcancel_delayed_work_sync(&adev->delayed_init_work);\n\tflush_delayed_work(&adev->gfx.gfx_off_delay_work);\n\n\tamdgpu_ras_suspend(adev);\n\n\tamdgpu_device_ip_suspend_phase1(adev);\n\n\tif (!adev->in_s0ix)\n\t\tamdgpu_amdkfd_suspend(adev, adev->in_runpm);\n\n\tr = amdgpu_device_evict_resources(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_fence_driver_hw_fini(adev);\n\n\tamdgpu_device_ip_suspend_phase2(adev);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tamdgpu_virt_release_full_gpu(adev, false);\n\n\treturn 0;\n}\n\n \nint amdgpu_device_resume(struct drm_device *dev, bool fbcon)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r = 0;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tr = amdgpu_virt_request_full_gpu(adev, true);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (dev->switch_power_state == DRM_SWITCH_POWER_OFF)\n\t\treturn 0;\n\n\tif (adev->in_s0ix)\n\t\tamdgpu_dpm_gfx_state_change(adev, sGpuChangeState_D0Entry);\n\n\t \n\tif (amdgpu_device_need_post(adev)) {\n\t\tr = amdgpu_device_asic_init(adev);\n\t\tif (r)\n\t\t\tdev_err(adev->dev, \"amdgpu asic init failed\\n\");\n\t}\n\n\tr = amdgpu_device_ip_resume(adev);\n\n\tif (r) {\n\t\tdev_err(adev->dev, \"amdgpu_device_ip_resume failed (%d).\\n\", r);\n\t\tgoto exit;\n\t}\n\tamdgpu_fence_driver_hw_init(adev);\n\n\tr = amdgpu_device_ip_late_init(adev);\n\tif (r)\n\t\tgoto exit;\n\n\tqueue_delayed_work(system_wq, &adev->delayed_init_work,\n\t\t\t   msecs_to_jiffies(AMDGPU_RESUME_MS));\n\n\tif (!adev->in_s0ix) {\n\t\tr = amdgpu_amdkfd_resume(adev, adev->in_runpm);\n\t\tif (r)\n\t\t\tgoto exit;\n\t}\n\nexit:\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_init_data_exchange(adev);\n\t\tamdgpu_virt_release_full_gpu(adev, true);\n\t}\n\n\tif (r)\n\t\treturn r;\n\n\t \n\tflush_delayed_work(&adev->delayed_init_work);\n\n\tif (fbcon)\n\t\tdrm_fb_helper_set_suspend_unlocked(adev_to_drm(adev)->fb_helper, false);\n\n\tamdgpu_ras_resume(adev);\n\n\tif (adev->mode_info.num_crtc) {\n\t\t \n#ifdef CONFIG_PM\n\t\tdev->dev->power.disable_depth++;\n#endif\n\t\tif (!adev->dc_enabled)\n\t\t\tdrm_helper_hpd_irq_event(dev);\n\t\telse\n\t\t\tdrm_kms_helper_hotplug_event(dev);\n#ifdef CONFIG_PM\n\t\tdev->dev->power.disable_depth--;\n#endif\n\t}\n\tadev->in_suspend = false;\n\n\tif (adev->enable_mes)\n\t\tamdgpu_mes_self_test(adev);\n\n\tif (amdgpu_acpi_smart_shift_update(dev, AMDGPU_SS_DEV_D0))\n\t\tDRM_WARN(\"smart shift update failed\\n\");\n\n\treturn 0;\n}\n\n \nstatic bool amdgpu_device_ip_check_soft_reset(struct amdgpu_device *adev)\n{\n\tint i;\n\tbool asic_hang = false;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn true;\n\n\tif (amdgpu_asic_need_full_reset(adev))\n\t\treturn true;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].version->funcs->check_soft_reset)\n\t\t\tadev->ip_blocks[i].status.hang =\n\t\t\t\tadev->ip_blocks[i].version->funcs->check_soft_reset(adev);\n\t\tif (adev->ip_blocks[i].status.hang) {\n\t\t\tdev_info(adev->dev, \"IP block:%s is hung!\\n\", adev->ip_blocks[i].version->funcs->name);\n\t\t\tasic_hang = true;\n\t\t}\n\t}\n\treturn asic_hang;\n}\n\n \nstatic int amdgpu_device_ip_pre_soft_reset(struct amdgpu_device *adev)\n{\n\tint i, r = 0;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].status.hang &&\n\t\t    adev->ip_blocks[i].version->funcs->pre_soft_reset) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->pre_soft_reset(adev);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool amdgpu_device_ip_need_full_reset(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (amdgpu_asic_need_full_reset(adev))\n\t\treturn true;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif ((adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_GMC) ||\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_SMC) ||\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_ACP) ||\n\t\t    (adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_DCE) ||\n\t\t     adev->ip_blocks[i].version->type == AMD_IP_BLOCK_TYPE_PSP) {\n\t\t\tif (adev->ip_blocks[i].status.hang) {\n\t\t\t\tdev_info(adev->dev, \"Some block need full reset!\\n\");\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\treturn false;\n}\n\n \nstatic int amdgpu_device_ip_soft_reset(struct amdgpu_device *adev)\n{\n\tint i, r = 0;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].status.hang &&\n\t\t    adev->ip_blocks[i].version->funcs->soft_reset) {\n\t\t\tr = adev->ip_blocks[i].version->funcs->soft_reset(adev);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_ip_post_soft_reset(struct amdgpu_device *adev)\n{\n\tint i, r = 0;\n\n\tfor (i = 0; i < adev->num_ip_blocks; i++) {\n\t\tif (!adev->ip_blocks[i].status.valid)\n\t\t\tcontinue;\n\t\tif (adev->ip_blocks[i].status.hang &&\n\t\t    adev->ip_blocks[i].version->funcs->post_soft_reset)\n\t\t\tr = adev->ip_blocks[i].version->funcs->post_soft_reset(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_device_recover_vram(struct amdgpu_device *adev)\n{\n\tstruct dma_fence *fence = NULL, *next = NULL;\n\tstruct amdgpu_bo *shadow;\n\tstruct amdgpu_bo_vm *vmbo;\n\tlong r = 1, tmo;\n\n\tif (amdgpu_sriov_runtime(adev))\n\t\ttmo = msecs_to_jiffies(8000);\n\telse\n\t\ttmo = msecs_to_jiffies(100);\n\n\tdev_info(adev->dev, \"recover vram bo from shadow start\\n\");\n\tmutex_lock(&adev->shadow_list_lock);\n\tlist_for_each_entry(vmbo, &adev->shadow_list, shadow_list) {\n\t\t \n\t\tif (!vmbo->shadow)\n\t\t\tcontinue;\n\t\tshadow = vmbo->shadow;\n\n\t\t \n\t\tif (shadow->tbo.resource->mem_type != TTM_PL_TT ||\n\t\t    shadow->tbo.resource->start == AMDGPU_BO_INVALID_OFFSET ||\n\t\t    shadow->parent->tbo.resource->mem_type != TTM_PL_VRAM)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_bo_restore_shadow(shadow, &next);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\tif (fence) {\n\t\t\ttmo = dma_fence_wait_timeout(fence, false, tmo);\n\t\t\tdma_fence_put(fence);\n\t\t\tfence = next;\n\t\t\tif (tmo == 0) {\n\t\t\t\tr = -ETIMEDOUT;\n\t\t\t\tbreak;\n\t\t\t} else if (tmo < 0) {\n\t\t\t\tr = tmo;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\tfence = next;\n\t\t}\n\t}\n\tmutex_unlock(&adev->shadow_list_lock);\n\n\tif (fence)\n\t\ttmo = dma_fence_wait_timeout(fence, false, tmo);\n\tdma_fence_put(fence);\n\n\tif (r < 0 || tmo <= 0) {\n\t\tdev_err(adev->dev, \"recover vram bo from shadow failed, r is %ld, tmo is %ld\\n\", r, tmo);\n\t\treturn -EIO;\n\t}\n\n\tdev_info(adev->dev, \"recover vram bo from shadow done\\n\");\n\treturn 0;\n}\n\n\n \nstatic int amdgpu_device_reset_sriov(struct amdgpu_device *adev,\n\t\t\t\t     bool from_hypervisor)\n{\n\tint r;\n\tstruct amdgpu_hive_info *hive = NULL;\n\tint retry_limit = 0;\n\nretry:\n\tamdgpu_amdkfd_pre_reset(adev);\n\n\tif (from_hypervisor)\n\t\tr = amdgpu_virt_request_full_gpu(adev, true);\n\telse\n\t\tr = amdgpu_virt_reset_gpu(adev);\n\tif (r)\n\t\treturn r;\n\tamdgpu_irq_gpu_reset_resume_helper(adev);\n\n\t \n\tamdgpu_virt_post_reset(adev);\n\n\t \n\tr = amdgpu_device_ip_reinit_early_sriov(adev);\n\tif (r)\n\t\tgoto error;\n\n\tamdgpu_virt_init_data_exchange(adev);\n\n\tr = amdgpu_device_fw_loading(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_device_ip_reinit_late_sriov(adev);\n\tif (r)\n\t\tgoto error;\n\n\thive = amdgpu_get_xgmi_hive(adev);\n\t \n\tif (hive && adev->gmc.xgmi.num_physical_nodes > 1)\n\t\tr = amdgpu_xgmi_update_topology(hive, adev);\n\n\tif (hive)\n\t\tamdgpu_put_xgmi_hive(hive);\n\n\tif (!r) {\n\t\tr = amdgpu_ib_ring_tests(adev);\n\n\t\tamdgpu_amdkfd_post_reset(adev);\n\t}\n\nerror:\n\tif (!r && adev->virt.gim_feature & AMDGIM_FEATURE_GIM_FLR_VRAMLOST) {\n\t\tamdgpu_inc_vram_lost(adev);\n\t\tr = amdgpu_device_recover_vram(adev);\n\t}\n\tamdgpu_virt_release_full_gpu(adev, true);\n\n\tif (AMDGPU_RETRY_SRIOV_RESET(r)) {\n\t\tif (retry_limit < AMDGPU_MAX_RETRY_LIMIT) {\n\t\t\tretry_limit++;\n\t\t\tgoto retry;\n\t\t} else\n\t\t\tDRM_ERROR(\"GPU reset retry is beyond the retry limit\\n\");\n\t}\n\n\treturn r;\n}\n\n \nbool amdgpu_device_has_job_running(struct amdgpu_device *adev)\n{\n\tint i;\n\tstruct drm_sched_job *job;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->sched.thread)\n\t\t\tcontinue;\n\n\t\tspin_lock(&ring->sched.job_list_lock);\n\t\tjob = list_first_entry_or_null(&ring->sched.pending_list,\n\t\t\t\t\t       struct drm_sched_job, list);\n\t\tspin_unlock(&ring->sched.job_list_lock);\n\t\tif (job)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool amdgpu_device_should_recover_gpu(struct amdgpu_device *adev)\n{\n\n\tif (amdgpu_gpu_recovery == 0)\n\t\tgoto disabled;\n\n\t \n\tif (!amdgpu_ras_is_poison_mode_supported(adev))\n\t\treturn true;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn true;\n\n\tif (amdgpu_gpu_recovery == -1) {\n\t\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_SI\n\t\tcase CHIP_VERDE:\n\t\tcase CHIP_TAHITI:\n\t\tcase CHIP_PITCAIRN:\n\t\tcase CHIP_OLAND:\n\t\tcase CHIP_HAINAN:\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\t\tcase CHIP_KAVERI:\n\t\tcase CHIP_KABINI:\n\t\tcase CHIP_MULLINS:\n#endif\n\t\tcase CHIP_CARRIZO:\n\t\tcase CHIP_STONEY:\n\t\tcase CHIP_CYAN_SKILLFISH:\n\t\t\tgoto disabled;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn true;\n\ndisabled:\n\t\tdev_info(adev->dev, \"GPU recovery disabled.\\n\");\n\t\treturn false;\n}\n\nint amdgpu_device_mode1_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint ret = 0;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\tdev_info(adev->dev, \"GPU mode1 reset\\n\");\n\n\t \n\tpci_clear_master(adev->pdev);\n\n\tamdgpu_device_cache_pci_state(adev->pdev);\n\n\tif (amdgpu_dpm_is_mode1_reset_supported(adev)) {\n\t\tdev_info(adev->dev, \"GPU smu mode1 reset\\n\");\n\t\tret = amdgpu_dpm_mode1_reset(adev);\n\t} else {\n\t\tdev_info(adev->dev, \"GPU psp mode1 reset\\n\");\n\t\tret = psp_gpu_reset(adev);\n\t}\n\n\tif (ret)\n\t\tgoto mode1_reset_failed;\n\n\tamdgpu_device_load_pci_state(adev->pdev);\n\tret = amdgpu_psp_wait_for_bootloader(adev);\n\tif (ret)\n\t\tgoto mode1_reset_failed;\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tu32 memsize = adev->nbio.funcs->get_memsize(adev);\n\n\t\tif (memsize != 0xffffffff)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout) {\n\t\tret = -ETIMEDOUT;\n\t\tgoto mode1_reset_failed;\n\t}\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn 0;\n\nmode1_reset_failed:\n\tdev_err(adev->dev, \"GPU mode1 reset failed\\n\");\n\treturn ret;\n}\n\nint amdgpu_device_pre_asic_reset(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_reset_context *reset_context)\n{\n\tint i, r = 0;\n\tstruct amdgpu_job *job = NULL;\n\tbool need_full_reset =\n\t\ttest_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\n\tif (reset_context->reset_req_dev == adev)\n\t\tjob = reset_context->job;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tamdgpu_virt_fini_data_exchange(adev);\n\t}\n\n\tamdgpu_fence_driver_isr_toggle(adev, true);\n\n\t \n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->sched.thread)\n\t\t\tcontinue;\n\n\t\t \n\t\tamdgpu_fence_driver_clear_job_fences(ring);\n\n\t\t \n\t\tamdgpu_fence_driver_force_completion(ring);\n\t}\n\n\tamdgpu_fence_driver_isr_toggle(adev, false);\n\n\tif (job && job->vm)\n\t\tdrm_sched_increase_karma(&job->base);\n\n\tr = amdgpu_reset_prepare_hwcontext(adev, reset_context);\n\t \n\tif (r == -EOPNOTSUPP)\n\t\tr = 0;\n\telse\n\t\treturn r;\n\n\t \n\tif (!amdgpu_sriov_vf(adev)) {\n\n\t\tif (!need_full_reset)\n\t\t\tneed_full_reset = amdgpu_device_ip_need_full_reset(adev);\n\n\t\tif (!need_full_reset && amdgpu_gpu_recovery &&\n\t\t    amdgpu_device_ip_check_soft_reset(adev)) {\n\t\t\tamdgpu_device_ip_pre_soft_reset(adev);\n\t\t\tr = amdgpu_device_ip_soft_reset(adev);\n\t\t\tamdgpu_device_ip_post_soft_reset(adev);\n\t\t\tif (r || amdgpu_device_ip_check_soft_reset(adev)) {\n\t\t\t\tdev_info(adev->dev, \"soft reset failed, will fallback to full reset!\\n\");\n\t\t\t\tneed_full_reset = true;\n\t\t\t}\n\t\t}\n\n\t\tif (need_full_reset)\n\t\t\tr = amdgpu_device_ip_suspend(adev);\n\t\tif (need_full_reset)\n\t\t\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\t\telse\n\t\t\tclear_bit(AMDGPU_NEED_FULL_RESET,\n\t\t\t\t  &reset_context->flags);\n\t}\n\n\treturn r;\n}\n\nstatic int amdgpu_reset_reg_dumps(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tlockdep_assert_held(&adev->reset_domain->sem);\n\n\tfor (i = 0; i < adev->num_regs; i++) {\n\t\tadev->reset_dump_reg_value[i] = RREG32(adev->reset_dump_reg_list[i]);\n\t\ttrace_amdgpu_reset_reg_dumps(adev->reset_dump_reg_list[i],\n\t\t\t\t\t     adev->reset_dump_reg_value[i]);\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_DEV_COREDUMP\nstatic ssize_t amdgpu_devcoredump_read(char *buffer, loff_t offset,\n\t\tsize_t count, void *data, size_t datalen)\n{\n\tstruct drm_printer p;\n\tstruct amdgpu_device *adev = data;\n\tstruct drm_print_iterator iter;\n\tint i;\n\n\titer.data = buffer;\n\titer.offset = 0;\n\titer.start = offset;\n\titer.remain = count;\n\n\tp = drm_coredump_printer(&iter);\n\n\tdrm_printf(&p, \"**** AMDGPU Device Coredump ****\\n\");\n\tdrm_printf(&p, \"kernel: \" UTS_RELEASE \"\\n\");\n\tdrm_printf(&p, \"module: \" KBUILD_MODNAME \"\\n\");\n\tdrm_printf(&p, \"time: %lld.%09ld\\n\", adev->reset_time.tv_sec, adev->reset_time.tv_nsec);\n\tif (adev->reset_task_info.pid)\n\t\tdrm_printf(&p, \"process_name: %s PID: %d\\n\",\n\t\t\t   adev->reset_task_info.process_name,\n\t\t\t   adev->reset_task_info.pid);\n\n\tif (adev->reset_vram_lost)\n\t\tdrm_printf(&p, \"VRAM is lost due to GPU reset!\\n\");\n\tif (adev->num_regs) {\n\t\tdrm_printf(&p, \"AMDGPU register dumps:\\nOffset:     Value:\\n\");\n\n\t\tfor (i = 0; i < adev->num_regs; i++)\n\t\t\tdrm_printf(&p, \"0x%08x: 0x%08x\\n\",\n\t\t\t\t   adev->reset_dump_reg_list[i],\n\t\t\t\t   adev->reset_dump_reg_value[i]);\n\t}\n\n\treturn count - iter.remain;\n}\n\nstatic void amdgpu_devcoredump_free(void *data)\n{\n}\n\nstatic void amdgpu_reset_capture_coredumpm(struct amdgpu_device *adev)\n{\n\tstruct drm_device *dev = adev_to_drm(adev);\n\n\tktime_get_ts64(&adev->reset_time);\n\tdev_coredumpm(dev->dev, THIS_MODULE, adev, 0, GFP_NOWAIT,\n\t\t      amdgpu_devcoredump_read, amdgpu_devcoredump_free);\n}\n#endif\n\nint amdgpu_do_asic_reset(struct list_head *device_list_handle,\n\t\t\t struct amdgpu_reset_context *reset_context)\n{\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tbool need_full_reset, skip_hw_reset, vram_lost = false;\n\tint r = 0;\n\tbool gpu_reset_for_dev_remove = 0;\n\n\t \n\ttmp_adev = list_first_entry(device_list_handle, struct amdgpu_device,\n\t\t\t\t    reset_list);\n\tamdgpu_reset_reg_dumps(tmp_adev);\n\n\treset_context->reset_device_list = device_list_handle;\n\tr = amdgpu_reset_perform_reset(tmp_adev, reset_context);\n\t \n\tif (r == -EOPNOTSUPP)\n\t\tr = 0;\n\telse\n\t\treturn r;\n\n\t \n\tneed_full_reset =\n\t\ttest_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\tskip_hw_reset = test_bit(AMDGPU_SKIP_HW_RESET, &reset_context->flags);\n\n\tgpu_reset_for_dev_remove =\n\t\ttest_bit(AMDGPU_RESET_FOR_DEVICE_REMOVE, &reset_context->flags) &&\n\t\t\ttest_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\n\t \n\tif (!skip_hw_reset && need_full_reset) {\n\t\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\t\t \n\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\ttmp_adev->gmc.xgmi.pending_reset = false;\n\t\t\t\tif (!queue_work(system_unbound_wq, &tmp_adev->xgmi_reset_work))\n\t\t\t\t\tr = -EALREADY;\n\t\t\t} else\n\t\t\t\tr = amdgpu_asic_reset(tmp_adev);\n\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev, \"ASIC reset failed with error, %d for drm dev, %s\",\n\t\t\t\t\t r, adev_to_drm(tmp_adev)->unique);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!r) {\n\t\t\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\t\t\tif (tmp_adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\t\t\tflush_work(&tmp_adev->xgmi_reset_work);\n\t\t\t\t\tr = tmp_adev->asic_reset_res;\n\t\t\t\t\tif (r)\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!r && amdgpu_ras_intr_triggered()) {\n\t\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\t\tif (tmp_adev->mmhub.ras && tmp_adev->mmhub.ras->ras_block.hw_ops &&\n\t\t\t    tmp_adev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\t\ttmp_adev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count(tmp_adev);\n\t\t}\n\n\t\tamdgpu_ras_intr_cleared();\n\t}\n\n\t \n\tif (gpu_reset_for_dev_remove) {\n\t\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list)\n\t\t\tamdgpu_device_ip_resume_phase1(tmp_adev);\n\n\t\tgoto end;\n\t}\n\n\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\tif (need_full_reset) {\n\t\t\t \n\t\t\tr = amdgpu_device_asic_init(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_warn(tmp_adev->dev, \"asic atom init failed!\");\n\t\t\t} else {\n\t\t\t\tdev_info(tmp_adev->dev, \"GPU reset succeeded, trying to resume\\n\");\n\n\t\t\t\tr = amdgpu_device_ip_resume_phase1(tmp_adev);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tvram_lost = amdgpu_device_check_vram_lost(tmp_adev);\n#ifdef CONFIG_DEV_COREDUMP\n\t\t\t\ttmp_adev->reset_vram_lost = vram_lost;\n\t\t\t\tmemset(&tmp_adev->reset_task_info, 0,\n\t\t\t\t\t\tsizeof(tmp_adev->reset_task_info));\n\t\t\t\tif (reset_context->job && reset_context->job->vm)\n\t\t\t\t\ttmp_adev->reset_task_info =\n\t\t\t\t\t\treset_context->job->vm->task_info;\n\t\t\t\tamdgpu_reset_capture_coredumpm(tmp_adev);\n#endif\n\t\t\t\tif (vram_lost) {\n\t\t\t\t\tDRM_INFO(\"VRAM is lost due to GPU reset!\\n\");\n\t\t\t\t\tamdgpu_inc_vram_lost(tmp_adev);\n\t\t\t\t}\n\n\t\t\t\tr = amdgpu_device_fw_loading(tmp_adev);\n\t\t\t\tif (r)\n\t\t\t\t\treturn r;\n\n\t\t\t\tr = amdgpu_device_ip_resume_phase2(tmp_adev);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tif (vram_lost)\n\t\t\t\t\tamdgpu_device_fill_reset_magic(tmp_adev);\n\n\t\t\t\t \n\t\t\t\tamdgpu_register_gpu_instance(tmp_adev);\n\n\t\t\t\tif (!reset_context->hive &&\n\t\t\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\t\t\tamdgpu_xgmi_add_device(tmp_adev);\n\n\t\t\t\tr = amdgpu_device_ip_late_init(tmp_adev);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto out;\n\n\t\t\t\tdrm_fb_helper_set_suspend_unlocked(adev_to_drm(tmp_adev)->fb_helper, false);\n\n\t\t\t\t \n\t\t\t\tif (!amdgpu_ras_eeprom_check_err_threshold(tmp_adev)) {\n\t\t\t\t\t \n\t\t\t\t\tamdgpu_ras_resume(tmp_adev);\n\t\t\t\t} else {\n\t\t\t\t\tr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tif (reset_context->hive &&\n\t\t\t\t    tmp_adev->gmc.xgmi.num_physical_nodes > 1)\n\t\t\t\t\tr = amdgpu_xgmi_update_topology(\n\t\t\t\t\t\treset_context->hive, tmp_adev);\n\t\t\t}\n\t\t}\n\nout:\n\t\tif (!r) {\n\t\t\tamdgpu_irq_gpu_reset_resume_helper(tmp_adev);\n\t\t\tr = amdgpu_ib_ring_tests(tmp_adev);\n\t\t\tif (r) {\n\t\t\t\tdev_err(tmp_adev->dev, \"ib ring test failed (%d).\\n\", r);\n\t\t\t\tneed_full_reset = true;\n\t\t\t\tr = -EAGAIN;\n\t\t\t\tgoto end;\n\t\t\t}\n\t\t}\n\n\t\tif (!r)\n\t\t\tr = amdgpu_device_recover_vram(tmp_adev);\n\t\telse\n\t\t\ttmp_adev->asic_reset_res = r;\n\t}\n\nend:\n\tif (need_full_reset)\n\t\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\telse\n\t\tclear_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\treturn r;\n}\n\nstatic void amdgpu_device_set_mp1_state(struct amdgpu_device *adev)\n{\n\n\tswitch (amdgpu_asic_reset_method(adev)) {\n\tcase AMD_RESET_METHOD_MODE1:\n\t\tadev->mp1_state = PP_MP1_STATE_SHUTDOWN;\n\t\tbreak;\n\tcase AMD_RESET_METHOD_MODE2:\n\t\tadev->mp1_state = PP_MP1_STATE_RESET;\n\t\tbreak;\n\tdefault:\n\t\tadev->mp1_state = PP_MP1_STATE_NONE;\n\t\tbreak;\n\t}\n}\n\nstatic void amdgpu_device_unset_mp1_state(struct amdgpu_device *adev)\n{\n\tamdgpu_vf_error_trans_all(adev);\n\tadev->mp1_state = PP_MP1_STATE_NONE;\n}\n\nstatic void amdgpu_device_resume_display_audio(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *p = NULL;\n\n\tp = pci_get_domain_bus_and_slot(pci_domain_nr(adev->pdev->bus),\n\t\t\tadev->pdev->bus->number, 1);\n\tif (p) {\n\t\tpm_runtime_enable(&(p->dev));\n\t\tpm_runtime_resume(&(p->dev));\n\t}\n\n\tpci_dev_put(p);\n}\n\nstatic int amdgpu_device_suspend_display_audio(struct amdgpu_device *adev)\n{\n\tenum amd_reset_method reset_method;\n\tstruct pci_dev *p = NULL;\n\tu64 expires;\n\n\t \n\treset_method = amdgpu_asic_reset_method(adev);\n\tif ((reset_method != AMD_RESET_METHOD_BACO) &&\n\t     (reset_method != AMD_RESET_METHOD_MODE1))\n\t\treturn -EINVAL;\n\n\tp = pci_get_domain_bus_and_slot(pci_domain_nr(adev->pdev->bus),\n\t\t\tadev->pdev->bus->number, 1);\n\tif (!p)\n\t\treturn -ENODEV;\n\n\texpires = pm_runtime_autosuspend_expiration(&(p->dev));\n\tif (!expires)\n\t\t \n\t\texpires = ktime_get_mono_fast_ns() + NSEC_PER_SEC * 4ULL;\n\n\twhile (!pm_runtime_status_suspended(&(p->dev))) {\n\t\tif (!pm_runtime_suspend(&(p->dev)))\n\t\t\tbreak;\n\n\t\tif (expires < ktime_get_mono_fast_ns()) {\n\t\t\tdev_warn(adev->dev, \"failed to suspend display audio\\n\");\n\t\t\tpci_dev_put(p);\n\t\t\t \n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t}\n\n\tpm_runtime_disable(&(p->dev));\n\n\tpci_dev_put(p);\n\treturn 0;\n}\n\nstatic inline void amdgpu_device_stop_pending_resets(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n#if defined(CONFIG_DEBUG_FS)\n\tif (!amdgpu_sriov_vf(adev))\n\t\tcancel_work(&adev->reset_work);\n#endif\n\n\tif (adev->kfd.dev)\n\t\tcancel_work(&adev->kfd.reset_work);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tcancel_work(&adev->virt.flr_work);\n\n\tif (con && adev->ras_enabled)\n\t\tcancel_work(&con->recovery_work);\n\n}\n\n \n\nint amdgpu_device_gpu_recover(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_job *job,\n\t\t\t      struct amdgpu_reset_context *reset_context)\n{\n\tstruct list_head device_list, *device_list_handle =  NULL;\n\tbool job_signaled = false;\n\tstruct amdgpu_hive_info *hive = NULL;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\tint i, r = 0;\n\tbool need_emergency_restart = false;\n\tbool audio_suspended = false;\n\tbool gpu_reset_for_dev_remove = false;\n\n\tgpu_reset_for_dev_remove =\n\t\t\ttest_bit(AMDGPU_RESET_FOR_DEVICE_REMOVE, &reset_context->flags) &&\n\t\t\t\ttest_bit(AMDGPU_NEED_FULL_RESET, &reset_context->flags);\n\n\t \n\tneed_emergency_restart = amdgpu_ras_need_emergency_restart(adev);\n\n\t \n\tif (need_emergency_restart && amdgpu_ras_get_context(adev) &&\n\t\tamdgpu_ras_get_context(adev)->reboot) {\n\t\tDRM_WARN(\"Emergency reboot.\");\n\n\t\tksys_sync_helper();\n\t\temergency_restart();\n\t}\n\n\tdev_info(adev->dev, \"GPU %s begin!\\n\",\n\t\tneed_emergency_restart ? \"jobs stop\":\"reset\");\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\thive = amdgpu_get_xgmi_hive(adev);\n\tif (hive)\n\t\tmutex_lock(&hive->hive_lock);\n\n\treset_context->job = job;\n\treset_context->hive = hive;\n\t \n\tINIT_LIST_HEAD(&device_list);\n\tif (!amdgpu_sriov_vf(adev) && (adev->gmc.xgmi.num_physical_nodes > 1)) {\n\t\tlist_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) {\n\t\t\tlist_add_tail(&tmp_adev->reset_list, &device_list);\n\t\t\tif (gpu_reset_for_dev_remove && adev->shutdown)\n\t\t\t\ttmp_adev->shutdown = true;\n\t\t}\n\t\tif (!list_is_first(&adev->reset_list, &device_list))\n\t\t\tlist_rotate_to_front(&adev->reset_list, &device_list);\n\t\tdevice_list_handle = &device_list;\n\t} else {\n\t\tlist_add_tail(&adev->reset_list, &device_list);\n\t\tdevice_list_handle = &device_list;\n\t}\n\n\t \n\ttmp_adev = list_first_entry(device_list_handle, struct amdgpu_device,\n\t\t\t\t    reset_list);\n\tamdgpu_device_lock_reset_domain(tmp_adev->reset_domain);\n\n\t \n\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\n\t\tamdgpu_device_set_mp1_state(tmp_adev);\n\n\t\t \n\t\tif (!amdgpu_device_suspend_display_audio(tmp_adev))\n\t\t\taudio_suspended = true;\n\n\t\tamdgpu_ras_set_error_query_ready(tmp_adev, false);\n\n\t\tcancel_delayed_work_sync(&tmp_adev->delayed_init_work);\n\n\t\tif (!amdgpu_sriov_vf(tmp_adev))\n\t\t\tamdgpu_amdkfd_pre_reset(tmp_adev);\n\n\t\t \n\t\tamdgpu_unregister_gpu_instance(tmp_adev);\n\n\t\tdrm_fb_helper_set_suspend_unlocked(adev_to_drm(tmp_adev)->fb_helper, true);\n\n\t\t \n\t\tif (!need_emergency_restart &&\n\t\t      amdgpu_device_ip_need_full_reset(tmp_adev))\n\t\t\tamdgpu_ras_suspend(tmp_adev);\n\n\t\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\t\tstruct amdgpu_ring *ring = tmp_adev->rings[i];\n\n\t\t\tif (!ring || !ring->sched.thread)\n\t\t\t\tcontinue;\n\n\t\t\tdrm_sched_stop(&ring->sched, job ? &job->base : NULL);\n\n\t\t\tif (need_emergency_restart)\n\t\t\t\tamdgpu_job_stop_all_jobs_on_sched(&ring->sched);\n\t\t}\n\t\tatomic_inc(&tmp_adev->gpu_reset_counter);\n\t}\n\n\tif (need_emergency_restart)\n\t\tgoto skip_sched_resume;\n\n\t \n\tif (job && dma_fence_is_signaled(&job->hw_fence)) {\n\t\tjob_signaled = true;\n\t\tdev_info(adev->dev, \"Guilty job already signaled, skipping HW reset\");\n\t\tgoto skip_hw_reset;\n\t}\n\nretry:\t \n\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\tif (gpu_reset_for_dev_remove) {\n\t\t\t \n\t\t\tamdgpu_device_smu_fini_early(tmp_adev);\n\t\t}\n\t\tr = amdgpu_device_pre_asic_reset(tmp_adev, reset_context);\n\t\t \n\t\tif (r) {\n\t\t\tdev_err(tmp_adev->dev, \"GPU pre asic reset failed with err, %d for drm dev, %s \",\n\t\t\t\t  r, adev_to_drm(tmp_adev)->unique);\n\t\t\ttmp_adev->asic_reset_res = r;\n\t\t}\n\n\t\t \n\t\tamdgpu_device_stop_pending_resets(tmp_adev);\n\t}\n\n\t \n\t \n\tif (amdgpu_sriov_vf(adev)) {\n\t\tr = amdgpu_device_reset_sriov(adev, job ? false : true);\n\t\tif (r)\n\t\t\tadev->asic_reset_res = r;\n\n\t\t \n\t\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 2) ||\n\t\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(11, 0, 3))\n\t\t\tamdgpu_ras_resume(adev);\n\t} else {\n\t\tr = amdgpu_do_asic_reset(device_list_handle, reset_context);\n\t\tif (r && r == -EAGAIN)\n\t\t\tgoto retry;\n\n\t\tif (!r && gpu_reset_for_dev_remove)\n\t\t\tgoto recover_end;\n\t}\n\nskip_hw_reset:\n\n\t \n\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\n\t\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\t\tstruct amdgpu_ring *ring = tmp_adev->rings[i];\n\n\t\t\tif (!ring || !ring->sched.thread)\n\t\t\t\tcontinue;\n\n\t\t\tdrm_sched_start(&ring->sched, true);\n\t\t}\n\n\t\tif (adev->enable_mes && adev->ip_versions[GC_HWIP][0] != IP_VERSION(11, 0, 3))\n\t\t\tamdgpu_mes_self_test(tmp_adev);\n\n\t\tif (!drm_drv_uses_atomic_modeset(adev_to_drm(tmp_adev)) && !job_signaled)\n\t\t\tdrm_helper_resume_force_mode(adev_to_drm(tmp_adev));\n\n\t\tif (tmp_adev->asic_reset_res)\n\t\t\tr = tmp_adev->asic_reset_res;\n\n\t\ttmp_adev->asic_reset_res = 0;\n\n\t\tif (r) {\n\t\t\t \n\t\t\tdev_info(tmp_adev->dev, \"GPU reset(%d) failed\\n\", atomic_read(&tmp_adev->gpu_reset_counter));\n\t\t\tamdgpu_vf_error_put(tmp_adev, AMDGIM_ERROR_VF_GPU_RESET_FAIL, 0, r);\n\t\t} else {\n\t\t\tdev_info(tmp_adev->dev, \"GPU reset(%d) succeeded!\\n\", atomic_read(&tmp_adev->gpu_reset_counter));\n\t\t\tif (amdgpu_acpi_smart_shift_update(adev_to_drm(tmp_adev), AMDGPU_SS_DEV_D0))\n\t\t\t\tDRM_WARN(\"smart shift update failed\\n\");\n\t\t}\n\t}\n\nskip_sched_resume:\n\tlist_for_each_entry(tmp_adev, device_list_handle, reset_list) {\n\t\t \n\t\tif (!need_emergency_restart && !amdgpu_sriov_vf(tmp_adev))\n\t\t\tamdgpu_amdkfd_post_reset(tmp_adev);\n\n\t\t \n\t\tif (!adev->kfd.init_complete)\n\t\t\tamdgpu_amdkfd_device_init(adev);\n\n\t\tif (audio_suspended)\n\t\t\tamdgpu_device_resume_display_audio(tmp_adev);\n\n\t\tamdgpu_device_unset_mp1_state(tmp_adev);\n\n\t\tamdgpu_ras_set_error_query_ready(tmp_adev, true);\n\t}\n\nrecover_end:\n\ttmp_adev = list_first_entry(device_list_handle, struct amdgpu_device,\n\t\t\t\t\t    reset_list);\n\tamdgpu_device_unlock_reset_domain(tmp_adev->reset_domain);\n\n\tif (hive) {\n\t\tmutex_unlock(&hive->hive_lock);\n\t\tamdgpu_put_xgmi_hive(hive);\n\t}\n\n\tif (r)\n\t\tdev_info(adev->dev, \"GPU reset end with ret = %d\\n\", r);\n\n\tatomic_set(&adev->reset_domain->reset_res, r);\n\treturn r;\n}\n\n \nstatic void amdgpu_device_get_pcie_info(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *pdev;\n\tenum pci_bus_speed speed_cap, platform_speed_cap;\n\tenum pcie_link_width platform_link_width;\n\n\tif (amdgpu_pcie_gen_cap)\n\t\tadev->pm.pcie_gen_mask = amdgpu_pcie_gen_cap;\n\n\tif (amdgpu_pcie_lane_cap)\n\t\tadev->pm.pcie_mlw_mask = amdgpu_pcie_lane_cap;\n\n\t \n\tif (pci_is_root_bus(adev->pdev->bus) && !amdgpu_passthrough(adev)) {\n\t\tif (adev->pm.pcie_gen_mask == 0)\n\t\t\tadev->pm.pcie_gen_mask = AMDGPU_DEFAULT_PCIE_GEN_MASK;\n\t\tif (adev->pm.pcie_mlw_mask == 0)\n\t\t\tadev->pm.pcie_mlw_mask = AMDGPU_DEFAULT_PCIE_MLW_MASK;\n\t\treturn;\n\t}\n\n\tif (adev->pm.pcie_gen_mask && adev->pm.pcie_mlw_mask)\n\t\treturn;\n\n\tpcie_bandwidth_available(adev->pdev, NULL,\n\t\t\t\t &platform_speed_cap, &platform_link_width);\n\n\tif (adev->pm.pcie_gen_mask == 0) {\n\t\t \n\t\tpdev = adev->pdev;\n\t\tspeed_cap = pcie_get_speed_cap(pdev);\n\t\tif (speed_cap == PCI_SPEED_UNKNOWN) {\n\t\t\tadev->pm.pcie_gen_mask |= (CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN3);\n\t\t} else {\n\t\t\tif (speed_cap == PCIE_SPEED_32_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN3 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN4 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN5);\n\t\t\telse if (speed_cap == PCIE_SPEED_16_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN3 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN4);\n\t\t\telse if (speed_cap == PCIE_SPEED_8_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN3);\n\t\t\telse if (speed_cap == PCIE_SPEED_5_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t  CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2);\n\t\t\telse\n\t\t\t\tadev->pm.pcie_gen_mask |= CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1;\n\t\t}\n\t\t \n\t\tif (platform_speed_cap == PCI_SPEED_UNKNOWN) {\n\t\t\tadev->pm.pcie_gen_mask |= (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2);\n\t\t} else {\n\t\t\tif (platform_speed_cap == PCIE_SPEED_32_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN4 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN5);\n\t\t\telse if (platform_speed_cap == PCIE_SPEED_16_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN4);\n\t\t\telse if (platform_speed_cap == PCIE_SPEED_8_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3);\n\t\t\telse if (platform_speed_cap == PCIE_SPEED_5_0GT)\n\t\t\t\tadev->pm.pcie_gen_mask |= (CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 |\n\t\t\t\t\t\t\t   CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2);\n\t\t\telse\n\t\t\t\tadev->pm.pcie_gen_mask |= CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1;\n\n\t\t}\n\t}\n\tif (adev->pm.pcie_mlw_mask == 0) {\n\t\tif (platform_link_width == PCIE_LNK_WIDTH_UNKNOWN) {\n\t\t\tadev->pm.pcie_mlw_mask |= AMDGPU_DEFAULT_PCIE_MLW_MASK;\n\t\t} else {\n\t\t\tswitch (platform_link_width) {\n\t\t\tcase PCIE_LNK_X32:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X32 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X16 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X12 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X8 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X4 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X16:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X16 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X12 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X8 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X4 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X12:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X12 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X8 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X4 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X8:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X8 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X4 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X4:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X4 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X2:\n\t\t\t\tadev->pm.pcie_mlw_mask = (CAIL_PCIE_LINK_WIDTH_SUPPORT_X2 |\n\t\t\t\t\t\t\t  CAIL_PCIE_LINK_WIDTH_SUPPORT_X1);\n\t\t\t\tbreak;\n\t\t\tcase PCIE_LNK_X1:\n\t\t\t\tadev->pm.pcie_mlw_mask = CAIL_PCIE_LINK_WIDTH_SUPPORT_X1;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nbool amdgpu_device_is_peer_accessible(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_device *peer_adev)\n{\n#ifdef CONFIG_HSA_AMD_P2P\n\tuint64_t address_mask = peer_adev->dev->dma_mask ?\n\t\t~*peer_adev->dev->dma_mask : ~((1ULL << 32) - 1);\n\tresource_size_t aper_limit =\n\t\tadev->gmc.aper_base + adev->gmc.aper_size - 1;\n\tbool p2p_access =\n\t\t!adev->gmc.xgmi.connected_to_cpu &&\n\t\t!(pci_p2pdma_distance(adev->pdev, peer_adev->dev, false) < 0);\n\n\treturn pcie_p2p && p2p_access && (adev->gmc.visible_vram_size &&\n\t\tadev->gmc.real_vram_size == adev->gmc.visible_vram_size &&\n\t\t!(adev->gmc.aper_base & address_mask ||\n\t\t  aper_limit & address_mask));\n#else\n\treturn false;\n#endif\n}\n\nint amdgpu_device_baco_enter(struct drm_device *dev)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\tif (!amdgpu_device_supports_baco(dev))\n\t\treturn -ENOTSUPP;\n\n\tif (ras && adev->ras_enabled &&\n\t    adev->nbio.funcs->enable_doorbell_interrupt)\n\t\tadev->nbio.funcs->enable_doorbell_interrupt(adev, false);\n\n\treturn amdgpu_dpm_baco_enter(adev);\n}\n\nint amdgpu_device_baco_exit(struct drm_device *dev)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\tint ret = 0;\n\n\tif (!amdgpu_device_supports_baco(dev))\n\t\treturn -ENOTSUPP;\n\n\tret = amdgpu_dpm_baco_exit(adev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ras && adev->ras_enabled &&\n\t    adev->nbio.funcs->enable_doorbell_interrupt)\n\t\tadev->nbio.funcs->enable_doorbell_interrupt(adev, true);\n\n\tif (amdgpu_passthrough(adev) &&\n\t    adev->nbio.funcs->clear_doorbell_interrupt)\n\t\tadev->nbio.funcs->clear_doorbell_interrupt(adev);\n\n\treturn 0;\n}\n\n \npci_ers_result_t amdgpu_pci_error_detected(struct pci_dev *pdev, pci_channel_state_t state)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint i;\n\n\tDRM_INFO(\"PCI error: detected callback, state(%d)!!\\n\", state);\n\n\tif (adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\tDRM_WARN(\"No support for XGMI hive yet...\");\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\tadev->pci_channel_state = state;\n\n\tswitch (state) {\n\tcase pci_channel_io_normal:\n\t\treturn PCI_ERS_RESULT_CAN_RECOVER;\n\t \n\tcase pci_channel_io_frozen:\n\t\t \n\t\tamdgpu_device_lock_reset_domain(adev->reset_domain);\n\t\tamdgpu_device_set_mp1_state(adev);\n\n\t\t \n\t\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\t\tif (!ring || !ring->sched.thread)\n\t\t\t\tcontinue;\n\n\t\t\tdrm_sched_stop(&ring->sched, NULL);\n\t\t}\n\t\tatomic_inc(&adev->gpu_reset_counter);\n\t\treturn PCI_ERS_RESULT_NEED_RESET;\n\tcase pci_channel_io_perm_failure:\n\t\t \n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\n \npci_ers_result_t amdgpu_pci_mmio_enabled(struct pci_dev *pdev)\n{\n\n\tDRM_INFO(\"PCI error: mmio enabled callback!!\\n\");\n\n\t \n\n\t \n\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\n \npci_ers_result_t amdgpu_pci_slot_reset(struct pci_dev *pdev)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r, i;\n\tstruct amdgpu_reset_context reset_context;\n\tu32 memsize;\n\tstruct list_head device_list;\n\n\tDRM_INFO(\"PCI error: slot reset callback!!\\n\");\n\n\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\tINIT_LIST_HEAD(&device_list);\n\tlist_add_tail(&adev->reset_list, &device_list);\n\n\t \n\tmsleep(500);\n\n\t \n\tamdgpu_device_load_pci_state(pdev);\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tmemsize = amdgpu_asic_get_config_memsize(adev);\n\n\t\tif (memsize != 0xffffffff)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\tif (memsize == 0xffffffff) {\n\t\tr = -ETIME;\n\t\tgoto out;\n\t}\n\n\treset_context.method = AMD_RESET_METHOD_NONE;\n\treset_context.reset_req_dev = adev;\n\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\tset_bit(AMDGPU_SKIP_HW_RESET, &reset_context.flags);\n\n\tadev->no_hw_access = true;\n\tr = amdgpu_device_pre_asic_reset(adev, &reset_context);\n\tadev->no_hw_access = false;\n\tif (r)\n\t\tgoto out;\n\n\tr = amdgpu_do_asic_reset(&device_list, &reset_context);\n\nout:\n\tif (!r) {\n\t\tif (amdgpu_device_cache_pci_state(adev->pdev))\n\t\t\tpci_restore_state(adev->pdev);\n\n\t\tDRM_INFO(\"PCIe error recovery succeeded\\n\");\n\t} else {\n\t\tDRM_ERROR(\"PCIe error recovery failed, err:%d\", r);\n\t\tamdgpu_device_unset_mp1_state(adev);\n\t\tamdgpu_device_unlock_reset_domain(adev->reset_domain);\n\t}\n\n\treturn r ? PCI_ERS_RESULT_DISCONNECT : PCI_ERS_RESULT_RECOVERED;\n}\n\n \nvoid amdgpu_pci_resume(struct pci_dev *pdev)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint i;\n\n\n\tDRM_INFO(\"PCI error: resume callback!!\\n\");\n\n\t \n\tif (adev->pci_channel_state != pci_channel_io_frozen)\n\t\treturn;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->sched.thread)\n\t\t\tcontinue;\n\n\t\tdrm_sched_start(&ring->sched, true);\n\t}\n\n\tamdgpu_device_unset_mp1_state(adev);\n\tamdgpu_device_unlock_reset_domain(adev->reset_domain);\n}\n\nbool amdgpu_device_cache_pci_state(struct pci_dev *pdev)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tr = pci_save_state(pdev);\n\tif (!r) {\n\t\tkfree(adev->pci_state);\n\n\t\tadev->pci_state = pci_store_saved_state(pdev);\n\n\t\tif (!adev->pci_state) {\n\t\t\tDRM_ERROR(\"Failed to store PCI saved state\");\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tDRM_WARN(\"Failed to save PCI state, err:%d\\n\", r);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nbool amdgpu_device_load_pci_state(struct pci_dev *pdev)\n{\n\tstruct drm_device *dev = pci_get_drvdata(pdev);\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tint r;\n\n\tif (!adev->pci_state)\n\t\treturn false;\n\n\tr = pci_load_saved_state(pdev, adev->pci_state);\n\n\tif (!r) {\n\t\tpci_restore_state(pdev);\n\t} else {\n\t\tDRM_WARN(\"Failed to load PCI state, err:%d\\n\", r);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nvoid amdgpu_device_flush_hdp(struct amdgpu_device *adev,\n\t\tstruct amdgpu_ring *ring)\n{\n#ifdef CONFIG_X86_64\n\tif ((adev->flags & AMD_IS_APU) && !amdgpu_passthrough(adev))\n\t\treturn;\n#endif\n\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\treturn;\n\n\tif (ring && ring->funcs->emit_hdp_flush)\n\t\tamdgpu_ring_emit_hdp_flush(ring);\n\telse\n\t\tamdgpu_asic_flush_hdp(adev, ring);\n}\n\nvoid amdgpu_device_invalidate_hdp(struct amdgpu_device *adev,\n\t\tstruct amdgpu_ring *ring)\n{\n#ifdef CONFIG_X86_64\n\tif ((adev->flags & AMD_IS_APU) && !amdgpu_passthrough(adev))\n\t\treturn;\n#endif\n\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\treturn;\n\n\tamdgpu_asic_invalidate_hdp(adev, ring);\n}\n\nint amdgpu_in_reset(struct amdgpu_device *adev)\n{\n\treturn atomic_read(&adev->reset_domain->in_gpu_reset);\n}\n\n \nvoid amdgpu_device_halt(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *pdev = adev->pdev;\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\n\tamdgpu_xcp_dev_unplug(adev);\n\tdrm_dev_unplug(ddev);\n\n\tamdgpu_irq_disable_all(adev);\n\n\tamdgpu_fence_driver_hw_fini(adev);\n\n\tadev->no_hw_access = true;\n\n\tamdgpu_device_unmap_mmio(adev);\n\n\tpci_disable_device(pdev);\n\tpci_wait_for_pending_transaction(pdev);\n}\n\nu32 amdgpu_device_pcie_port_rreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg)\n{\n\tunsigned long flags, address, data;\n\tu32 r;\n\n\taddress = adev->nbio.funcs->get_pcie_port_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_port_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, reg * 4);\n\t(void)RREG32(address);\n\tr = RREG32(data);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nvoid amdgpu_device_pcie_port_wreg(struct amdgpu_device *adev,\n\t\t\t\tu32 reg, u32 v)\n{\n\tunsigned long flags, address, data;\n\n\taddress = adev->nbio.funcs->get_pcie_port_index_offset(adev);\n\tdata = adev->nbio.funcs->get_pcie_port_data_offset(adev);\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32(address, reg * 4);\n\t(void)RREG32(address);\n\tWREG32(data, v);\n\t(void)RREG32(data);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\n \nstruct dma_fence *amdgpu_device_switch_gang(struct amdgpu_device *adev,\n\t\t\t\t\t    struct dma_fence *gang)\n{\n\tstruct dma_fence *old = NULL;\n\n\tdo {\n\t\tdma_fence_put(old);\n\t\trcu_read_lock();\n\t\told = dma_fence_get_rcu_safe(&adev->gang_submit);\n\t\trcu_read_unlock();\n\n\t\tif (old == gang)\n\t\t\tbreak;\n\n\t\tif (!dma_fence_is_signaled(old))\n\t\t\treturn old;\n\n\t} while (cmpxchg((struct dma_fence __force **)&adev->gang_submit,\n\t\t\t old, gang) != old);\n\n\tdma_fence_put(old);\n\treturn NULL;\n}\n\nbool amdgpu_device_has_display_hardware(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_SI\n\tcase CHIP_HAINAN:\n#endif\n\tcase CHIP_TOPAZ:\n\t\t \n\t\treturn false;\n#ifdef CONFIG_DRM_AMDGPU_SI\n\tcase CHIP_TAHITI:\n\tcase CHIP_PITCAIRN:\n\tcase CHIP_VERDE:\n\tcase CHIP_OLAND:\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_BONAIRE:\n\tcase CHIP_HAWAII:\n\tcase CHIP_KAVERI:\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n#endif\n\tcase CHIP_TONGA:\n\tcase CHIP_FIJI:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\t \n\t\treturn true;\n\tdefault:\n\t\t \n\t\tif (!adev->ip_versions[DCE_HWIP][0] ||\n\t\t    (adev->harvest_ip_mask & AMD_HARVEST_IP_DMU_MASK))\n\t\t\treturn false;\n\t\treturn true;\n\t}\n}\n\nuint32_t amdgpu_device_wait_on_rreg(struct amdgpu_device *adev,\n\t\tuint32_t inst, uint32_t reg_addr, char reg_name[],\n\t\tuint32_t expected_value, uint32_t mask)\n{\n\tuint32_t ret = 0;\n\tuint32_t old_ = 0;\n\tuint32_t tmp_ = RREG32(reg_addr);\n\tuint32_t loop = adev->usec_timeout;\n\n\twhile ((tmp_ & (mask)) != (expected_value)) {\n\t\tif (old_ != tmp_) {\n\t\t\tloop = adev->usec_timeout;\n\t\t\told_ = tmp_;\n\t\t} else\n\t\t\tudelay(1);\n\t\ttmp_ = RREG32(reg_addr);\n\t\tloop--;\n\t\tif (!loop) {\n\t\t\tDRM_WARN(\"Register(%d) [%s] failed to reach value 0x%08x != 0x%08xn\",\n\t\t\t\t  inst, reg_name, (uint32_t)expected_value,\n\t\t\t\t  (uint32_t)(tmp_ & (mask)));\n\t\t\tret = -ETIMEDOUT;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}