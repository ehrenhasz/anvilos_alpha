{
  "module_name": "umc_v6_7.c",
  "hash_id": "093b20f0e3be32978ffe7508bb2bc981a60725b4c86dc2b97cd7bafaf98effc5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/umc_v6_7.c",
  "human_readable_source": " \n#include \"umc_v6_7.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_umc.h\"\n#include \"amdgpu.h\"\n\n#include \"umc/umc_6_7_0_offset.h\"\n#include \"umc/umc_6_7_0_sh_mask.h\"\n\nconst uint32_t\n\tumc_v6_7_channel_idx_tbl_second[UMC_V6_7_UMC_INSTANCE_NUM][UMC_V6_7_CHANNEL_INSTANCE_NUM] = {\n\t\t{28, 20, 24, 16, 12, 4, 8, 0},\n\t\t{6, 30, 2, 26, 22, 14, 18, 10},\n\t\t{19, 11, 15, 7, 3, 27, 31, 23},\n\t\t{9, 1, 5, 29, 25, 17, 21, 13}\n};\nconst uint32_t\n\tumc_v6_7_channel_idx_tbl_first[UMC_V6_7_UMC_INSTANCE_NUM][UMC_V6_7_CHANNEL_INSTANCE_NUM] = {\n\t\t{19, 11, 15, 7,\t3, 27, 31, 23},\n\t\t{9, 1, 5, 29, 25, 17, 21, 13},\n\t\t{28, 20, 24, 16, 12, 4, 8, 0},\n\t\t{6, 30, 2, 26, 22, 14, 18, 10},\n};\n\nstatic inline uint32_t get_umc_v6_7_reg_offset(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t umc_inst,\n\t\t\t\t\t      uint32_t ch_inst)\n{\n\tuint32_t index = umc_inst * adev->umc.channel_inst_num + ch_inst;\n\n\t \n\tumc_inst = index / 4;\n\tch_inst = index % 4;\n\n\treturn adev->umc.channel_offs * ch_inst + UMC_V6_7_INST_DIST * umc_inst;\n}\n\nstatic void umc_v6_7_query_error_status_helper(struct amdgpu_device *adev,\n\t\t\t\t\t\t  uint64_t mc_umc_status, uint32_t umc_reg_offset)\n{\n\tuint32_t mc_umc_addr;\n\tuint64_t reg_value;\n\n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Deferred) == 1)\n\t\tdev_info(adev->dev, \"Deferred error, no user action is needed.\\n\");\n\n\tif (mc_umc_status)\n\t\tdev_info(adev->dev, \"MCA STATUS 0x%llx, umc_reg_offset 0x%x\\n\", mc_umc_status, umc_reg_offset);\n\n\t \n\tmc_umc_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_IPIDT0);\n\treg_value = RREG64_PCIE((mc_umc_addr + umc_reg_offset) * 4);\n\tif (reg_value)\n\t\tdev_info(adev->dev, \"MCA IPID 0x%llx, umc_reg_offset 0x%x\\n\", reg_value, umc_reg_offset);\n\n\t \n\tmc_umc_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_SYNDT0);\n\treg_value = RREG64_PCIE((mc_umc_addr + umc_reg_offset) * 4);\n\tif (reg_value)\n\t\tdev_info(adev->dev, \"MCA SYND 0x%llx, umc_reg_offset 0x%x\\n\", reg_value, umc_reg_offset);\n\n\t \n\tmc_umc_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_MISC0T0);\n\treg_value = RREG64_PCIE((mc_umc_addr + umc_reg_offset) * 4);\n\tif (reg_value)\n\t\tdev_info(adev->dev, \"MCA MISC0 0x%llx, umc_reg_offset 0x%x\\n\", reg_value, umc_reg_offset);\n}\n\nstatic void umc_v6_7_ecc_info_query_correctable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t umc_inst, uint32_t ch_inst,\n\t\t\t\t\t\t   unsigned long *error_count)\n{\n\tuint64_t mc_umc_status;\n\tuint32_t eccinfo_table_idx;\n\tuint32_t umc_reg_offset;\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\tumc_reg_offset = get_umc_v6_7_reg_offset(adev,\n\t\t\t\t\t\tumc_inst, ch_inst);\n\n\teccinfo_table_idx = umc_inst * adev->umc.channel_inst_num + ch_inst;\n\t \n\tmc_umc_status = ras->umc_ecc.ecc[eccinfo_table_idx].mca_umc_status;\n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, CECC) == 1) {\n\t\t*error_count += 1;\n\n\t\tumc_v6_7_query_error_status_helper(adev, mc_umc_status, umc_reg_offset);\n\n\t\tif (ras->umc_ecc.record_ce_addr_supported)\t{\n\t\t\tuint64_t err_addr, soc_pa;\n\t\t\tuint32_t channel_index =\n\t\t\t\tadev->umc.channel_idx_tbl[umc_inst * adev->umc.channel_inst_num + ch_inst];\n\n\t\t\terr_addr = ras->umc_ecc.ecc[eccinfo_table_idx].mca_ceumc_addr;\n\t\t\terr_addr = REG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, ErrorAddr);\n\t\t\t \n\t\t\tsoc_pa = ADDR_OF_8KB_BLOCK(err_addr) |\n\t\t\t\t\tADDR_OF_256B_BLOCK(channel_index) |\n\t\t\t\t\tOFFSET_IN_256B_BLOCK(err_addr);\n\n\t\t\t \n\t\t\tSET_CHANNEL_HASH(channel_index, soc_pa);\n\n\t\t\tdev_info(adev->dev, \"Error Address(PA): 0x%llx\\n\", soc_pa);\n\t\t}\n\t}\n}\n\nstatic void umc_v6_7_ecc_info_querry_uncorrectable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t\t  uint32_t umc_inst, uint32_t ch_inst,\n\t\t\t\t\t\t      unsigned long *error_count)\n{\n\tuint64_t mc_umc_status;\n\tuint32_t eccinfo_table_idx;\n\tuint32_t umc_reg_offset;\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\tumc_reg_offset = get_umc_v6_7_reg_offset(adev,\n\t\t\t\t\t\tumc_inst, ch_inst);\n\n\teccinfo_table_idx = umc_inst * adev->umc.channel_inst_num + ch_inst;\n\t \n\tmc_umc_status = ras->umc_ecc.ecc[eccinfo_table_idx].mca_umc_status;\n\tif ((REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1) &&\n\t    (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Deferred) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, PCC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, TCC) == 1)) {\n\t\t*error_count += 1;\n\n\t\tumc_v6_7_query_error_status_helper(adev, mc_umc_status, umc_reg_offset);\n\t}\n}\n\nstatic int umc_v6_7_ecc_info_querry_ecc_error_count(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_inst, uint32_t umc_inst,\n\t\t\t\t\tuint32_t ch_inst, void *data)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)data;\n\n\tumc_v6_7_ecc_info_query_correctable_error_count(adev,\n\t\tumc_inst, ch_inst,\n\t\t&(err_data->ce_count));\n\n\tumc_v6_7_ecc_info_querry_uncorrectable_error_count(adev,\n\t\tumc_inst, ch_inst,\n\t\t&(err_data->ue_count));\n\n\treturn 0;\n}\n\nstatic void umc_v6_7_ecc_info_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t   void *ras_error_status)\n{\n\tamdgpu_umc_loop_channels(adev,\n\t\tumc_v6_7_ecc_info_querry_ecc_error_count, ras_error_status);\n}\n\nvoid umc_v6_7_convert_error_address(struct amdgpu_device *adev,\n\t\t\t\t    struct ras_err_data *err_data, uint64_t err_addr,\n\t\t\t\t    uint32_t ch_inst, uint32_t umc_inst)\n{\n\tuint32_t channel_index;\n\tuint64_t soc_pa, retired_page, column;\n\n\tchannel_index =\n\t\tadev->umc.channel_idx_tbl[umc_inst * adev->umc.channel_inst_num + ch_inst];\n\t \n\tsoc_pa = ADDR_OF_8KB_BLOCK(err_addr) |\n\t\t\tADDR_OF_256B_BLOCK(channel_index) |\n\t\t\tOFFSET_IN_256B_BLOCK(err_addr);\n\n\t \n\tSET_CHANNEL_HASH(channel_index, soc_pa);\n\n\t \n\tsoc_pa &= ~(0x7ULL << UMC_V6_7_PA_C2_BIT);\n\n\t \n\tfor (column = 0; column < UMC_V6_7_NA_MAP_PA_NUM; column++) {\n\t\tretired_page = soc_pa | (column << UMC_V6_7_PA_C2_BIT);\n\t\tdev_info(adev->dev, \"Error Address(PA): 0x%llx\\n\", retired_page);\n\t\tamdgpu_umc_fill_error_record(err_data, err_addr,\n\t\t\tretired_page, channel_index, umc_inst);\n\n\t\t \n\t\tretired_page ^= (0x1ULL << UMC_V6_7_PA_R14_BIT);\n\t\tdev_info(adev->dev, \"Error Address(PA): 0x%llx\\n\", retired_page);\n\t\tamdgpu_umc_fill_error_record(err_data, err_addr,\n\t\t\tretired_page, channel_index, umc_inst);\n\t}\n}\n\nstatic int umc_v6_7_ecc_info_query_error_address(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_inst, uint32_t umc_inst,\n\t\t\t\t\tuint32_t ch_inst, void *data)\n{\n\tuint64_t mc_umc_status, err_addr;\n\tuint32_t eccinfo_table_idx;\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\tstruct ras_err_data *err_data = (struct ras_err_data *)data;\n\n\teccinfo_table_idx = umc_inst * adev->umc.channel_inst_num + ch_inst;\n\tmc_umc_status = ras->umc_ecc.ecc[eccinfo_table_idx].mca_umc_status;\n\n\tif (mc_umc_status == 0)\n\t\treturn 0;\n\n\tif (!err_data->err_addr)\n\t\treturn 0;\n\n\t \n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1) {\n\n\t\terr_addr = ras->umc_ecc.ecc[eccinfo_table_idx].mca_umc_addr;\n\t\terr_addr = REG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, ErrorAddr);\n\n\t\tumc_v6_7_convert_error_address(adev, err_data, err_addr,\n\t\t\t\t\tch_inst, umc_inst);\n\t}\n\n\treturn 0;\n}\n\nstatic void umc_v6_7_ecc_info_query_ras_error_address(struct amdgpu_device *adev,\n\t\t\t\t\t     void *ras_error_status)\n{\n\tamdgpu_umc_loop_channels(adev,\n\t    umc_v6_7_ecc_info_query_error_address, ras_error_status);\n}\n\nstatic void umc_v6_7_query_correctable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t umc_reg_offset,\n\t\t\t\t\t\t   unsigned long *error_count,\n\t\t\t\t\t\t   uint32_t ch_inst,\n\t\t\t\t\t\t   uint32_t umc_inst)\n{\n\tuint32_t ecc_err_cnt_sel, ecc_err_cnt_sel_addr;\n\tuint32_t ecc_err_cnt, ecc_err_cnt_addr;\n\tuint64_t mc_umc_status;\n\tuint32_t mc_umc_status_addr;\n\n\t \n\tecc_err_cnt_sel_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regUMCCH0_0_EccErrCntSel);\n\tecc_err_cnt_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regUMCCH0_0_EccErrCnt);\n\tmc_umc_status_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_STATUST0);\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 0);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\n\tecc_err_cnt = RREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4);\n\t*error_count +=\n\t\t(REG_GET_FIELD(ecc_err_cnt, UMCCH0_0_EccErrCnt, EccErrCnt) -\n\t\t UMC_V6_7_CE_CNT_INIT);\n\n\t \n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel, UMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4, ecc_err_cnt_sel);\n\n\tecc_err_cnt = RREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4);\n\t*error_count +=\n\t\t(REG_GET_FIELD(ecc_err_cnt, UMCCH0_0_EccErrCnt, EccErrCnt) -\n\t\t UMC_V6_7_CE_CNT_INIT);\n\n\t \n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, CECC) == 1) {\n\t\t*error_count += 1;\n\n\t\tumc_v6_7_query_error_status_helper(adev, mc_umc_status, umc_reg_offset);\n\n\t\t{\n\t\t\tuint64_t err_addr, soc_pa;\n\t\t\tuint32_t mc_umc_addrt0;\n\t\t\tuint32_t channel_index;\n\n\t\t\tmc_umc_addrt0 =\n\t\t\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_ADDRT0);\n\n\t\t\tchannel_index =\n\t\t\t\tadev->umc.channel_idx_tbl[umc_inst * adev->umc.channel_inst_num + ch_inst];\n\n\t\t\terr_addr = RREG64_PCIE((mc_umc_addrt0 + umc_reg_offset) * 4);\n\t\t\terr_addr = REG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, ErrorAddr);\n\n\t\t\t \n\t\t\tsoc_pa = ADDR_OF_8KB_BLOCK(err_addr) |\n\t\t\t\t\tADDR_OF_256B_BLOCK(channel_index) |\n\t\t\t\t\tOFFSET_IN_256B_BLOCK(err_addr);\n\n\t\t\t \n\t\t\tSET_CHANNEL_HASH(channel_index, soc_pa);\n\n\t\t\tdev_info(adev->dev, \"Error Address(PA): 0x%llx\\n\", soc_pa);\n\t\t}\n\t}\n}\n\nstatic void umc_v6_7_querry_uncorrectable_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t      uint32_t umc_reg_offset,\n\t\t\t\t\t\t      unsigned long *error_count)\n{\n\tuint64_t mc_umc_status;\n\tuint32_t mc_umc_status_addr;\n\n\tmc_umc_status_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_STATUST0);\n\n\t \n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\tif ((REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1) &&\n\t    (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Deferred) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, PCC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UC) == 1 ||\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, TCC) == 1)) {\n\t\t*error_count += 1;\n\n\t\tumc_v6_7_query_error_status_helper(adev, mc_umc_status, umc_reg_offset);\n\t}\n}\n\nstatic int umc_v6_7_reset_error_count_per_channel(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_inst, uint32_t umc_inst,\n\t\t\t\t\tuint32_t ch_inst, void *data)\n{\n\tuint32_t ecc_err_cnt_addr;\n\tuint32_t ecc_err_cnt_sel, ecc_err_cnt_sel_addr;\n\tuint32_t umc_reg_offset =\n\t\tget_umc_v6_7_reg_offset(adev, umc_inst, ch_inst);\n\n\tecc_err_cnt_sel_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\tregUMCCH0_0_EccErrCntSel);\n\tecc_err_cnt_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0,\n\t\t\t\tregUMCCH0_0_EccErrCnt);\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr +\n\t\t\t\t       umc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel,\n\t\t\t\t\tUMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 0);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4,\n\t\t\tecc_err_cnt_sel);\n\n\t \n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4,\n\t\t\tUMC_V6_7_CE_CNT_INIT);\n\n\t \n\tecc_err_cnt_sel = RREG32_PCIE((ecc_err_cnt_sel_addr +\n\t\t\t\t\tumc_reg_offset) * 4);\n\tecc_err_cnt_sel = REG_SET_FIELD(ecc_err_cnt_sel,\n\t\t\t\t\tUMCCH0_0_EccErrCntSel,\n\t\t\t\t\tEccErrCntCsSel, 1);\n\tWREG32_PCIE((ecc_err_cnt_sel_addr + umc_reg_offset) * 4,\n\t\t\tecc_err_cnt_sel);\n\n\t \n\tWREG32_PCIE((ecc_err_cnt_addr + umc_reg_offset) * 4,\n\t\t\tUMC_V6_7_CE_CNT_INIT);\n\n\treturn 0;\n}\n\nstatic void umc_v6_7_reset_error_count(struct amdgpu_device *adev)\n{\n\tamdgpu_umc_loop_channels(adev,\n\t\tumc_v6_7_reset_error_count_per_channel, NULL);\n}\n\nstatic int umc_v6_7_query_ecc_error_count(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_inst, uint32_t umc_inst,\n\t\t\t\t\tuint32_t ch_inst, void *data)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)data;\n\tuint32_t umc_reg_offset =\n\t\tget_umc_v6_7_reg_offset(adev, umc_inst, ch_inst);\n\n\tumc_v6_7_query_correctable_error_count(adev,\n\t\t\t\t\tumc_reg_offset,\n\t\t\t\t\t&(err_data->ce_count),\n\t\t\t\t\tch_inst, umc_inst);\n\n\tumc_v6_7_querry_uncorrectable_error_count(adev,\n\t\t\t\t\tumc_reg_offset,\n\t\t\t\t\t&(err_data->ue_count));\n\n\treturn 0;\n}\n\nstatic void umc_v6_7_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t   void *ras_error_status)\n{\n\tamdgpu_umc_loop_channels(adev,\n\t\tumc_v6_7_query_ecc_error_count, ras_error_status);\n\n\tumc_v6_7_reset_error_count(adev);\n}\n\nstatic int umc_v6_7_query_error_address(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t node_inst, uint32_t umc_inst,\n\t\t\t\t\tuint32_t ch_inst, void *data)\n{\n\tuint32_t mc_umc_status_addr;\n\tuint64_t mc_umc_status = 0, mc_umc_addrt0, err_addr;\n\tstruct ras_err_data *err_data = (struct ras_err_data *)data;\n\tuint32_t umc_reg_offset =\n\t\tget_umc_v6_7_reg_offset(adev, umc_inst, ch_inst);\n\n\tmc_umc_status_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_STATUST0);\n\tmc_umc_addrt0 =\n\t\tSOC15_REG_OFFSET(UMC, 0, regMCA_UMC_UMC0_MCUMC_ADDRT0);\n\n\tmc_umc_status = RREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4);\n\n\tif (mc_umc_status == 0)\n\t\treturn 0;\n\n\tif (!err_data->err_addr) {\n\t\t \n\t\tWREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4, 0x0ULL);\n\t\treturn 0;\n\t}\n\n\t \n\tif (REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, Val) == 1 &&\n\t    REG_GET_FIELD(mc_umc_status, MCA_UMC_UMC0_MCUMC_STATUST0, UECC) == 1) {\n\t\terr_addr = RREG64_PCIE((mc_umc_addrt0 + umc_reg_offset) * 4);\n\t\terr_addr =\n\t\t\tREG_GET_FIELD(err_addr, MCA_UMC_UMC0_MCUMC_ADDRT0, ErrorAddr);\n\n\t\tumc_v6_7_convert_error_address(adev, err_data, err_addr,\n\t\t\t\t\tch_inst, umc_inst);\n\t}\n\n\t \n\tWREG64_PCIE((mc_umc_status_addr + umc_reg_offset) * 4, 0x0ULL);\n\n\treturn 0;\n}\n\nstatic void umc_v6_7_query_ras_error_address(struct amdgpu_device *adev,\n\t\t\t\t\t     void *ras_error_status)\n{\n\tamdgpu_umc_loop_channels(adev,\n\t\tumc_v6_7_query_error_address, ras_error_status);\n}\n\nstatic uint32_t umc_v6_7_query_ras_poison_mode_per_channel(\n\t\t\t\t\t\tstruct amdgpu_device *adev,\n\t\t\t\t\t\tuint32_t umc_reg_offset)\n{\n\tuint32_t ecc_ctrl_addr, ecc_ctrl;\n\n\tecc_ctrl_addr =\n\t\tSOC15_REG_OFFSET(UMC, 0, regUMCCH0_0_EccCtrl);\n\tecc_ctrl = RREG32_PCIE((ecc_ctrl_addr +\n\t\t\t\t\tumc_reg_offset) * 4);\n\n\treturn REG_GET_FIELD(ecc_ctrl, UMCCH0_0_EccCtrl, UCFatalEn);\n}\n\nstatic bool umc_v6_7_query_ras_poison_mode(struct amdgpu_device *adev)\n{\n\tuint32_t umc_reg_offset  = 0;\n\n\t \n\tumc_reg_offset = get_umc_v6_7_reg_offset(adev, 0, 0);\n\treturn !umc_v6_7_query_ras_poison_mode_per_channel(adev, umc_reg_offset);\n}\n\nconst struct amdgpu_ras_block_hw_ops umc_v6_7_ras_hw_ops = {\n\t.query_ras_error_count = umc_v6_7_query_ras_error_count,\n\t.query_ras_error_address = umc_v6_7_query_ras_error_address,\n};\n\nstruct amdgpu_umc_ras umc_v6_7_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &umc_v6_7_ras_hw_ops,\n\t},\n\t.query_ras_poison_mode = umc_v6_7_query_ras_poison_mode,\n\t.ecc_info_query_ras_error_count = umc_v6_7_ecc_info_query_ras_error_count,\n\t.ecc_info_query_ras_error_address = umc_v6_7_ecc_info_query_ras_error_address,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}