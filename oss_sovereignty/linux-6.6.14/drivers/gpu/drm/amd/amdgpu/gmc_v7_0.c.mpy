{
  "module_name": "gmc_v7_0.c",
  "hash_id": "bd8c86c67d6e0ff73d5a25c868ccbebd8442dfbb7ca6b837c42160b812e7d7ca",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/drm_cache.h>\n#include \"amdgpu.h\"\n#include \"cikd.h\"\n#include \"cik.h\"\n#include \"gmc_v7_0.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_gem.h\"\n\n#include \"bif/bif_4_1_d.h\"\n#include \"bif/bif_4_1_sh_mask.h\"\n\n#include \"gmc/gmc_7_1_d.h\"\n#include \"gmc/gmc_7_1_sh_mask.h\"\n\n#include \"oss/oss_2_0_d.h\"\n#include \"oss/oss_2_0_sh_mask.h\"\n\n#include \"dce/dce_8_0_d.h\"\n#include \"dce/dce_8_0_sh_mask.h\"\n\n#include \"amdgpu_atombios.h\"\n\n#include \"ivsrcid/ivsrcid_vislands30.h\"\n\nstatic void gmc_v7_0_set_gmc_funcs(struct amdgpu_device *adev);\nstatic void gmc_v7_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int gmc_v7_0_wait_for_idle(void *handle);\n\nMODULE_FIRMWARE(\"amdgpu/bonaire_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/hawaii_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/topaz_mc.bin\");\n\nstatic const u32 golden_settings_iceland_a11[] = {\n\tmmVM_PRT_APERTURE0_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE1_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE2_LOW_ADDR, 0x0fffffff, 0x0fffffff,\n\tmmVM_PRT_APERTURE3_LOW_ADDR, 0x0fffffff, 0x0fffffff\n};\n\nstatic const u32 iceland_mgcg_cgcg_init[] = {\n\tmmMC_MEM_POWER_LS, 0xffffffff, 0x00000104\n};\n\nstatic void gmc_v7_0_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ticeland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(iceland_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tgolden_settings_iceland_a11,\n\t\t\t\t\t\t\tARRAY_SIZE(golden_settings_iceland_a11));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v7_0_mc_stop(struct amdgpu_device *adev)\n{\n\tu32 blackout;\n\n\tgmc_v7_0_wait_for_idle((void *)adev);\n\n\tblackout = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\n\tif (REG_GET_FIELD(blackout, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE) != 1) {\n\t\t \n\t\tWREG32(mmBIF_FB_EN, 0);\n\t\t \n\t\tblackout = REG_SET_FIELD(blackout,\n\t\t\t\t\t MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\n\t\tWREG32(mmMC_SHARED_BLACKOUT_CNTL, blackout | 1);\n\t}\n\t \n\tudelay(100);\n}\n\nstatic void gmc_v7_0_mc_resume(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\t \n\ttmp = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\n\ttmp = REG_SET_FIELD(tmp, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\n\tWREG32(mmMC_SHARED_BLACKOUT_CNTL, tmp);\n\t \n\ttmp = REG_SET_FIELD(0, BIF_FB_EN, FB_READ_EN, 1);\n\ttmp = REG_SET_FIELD(tmp, BIF_FB_EN, FB_WRITE_EN, 1);\n\tWREG32(mmBIF_FB_EN, tmp);\n}\n\n \nstatic int gmc_v7_0_init_microcode(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tchar fw_name[30];\n\tint err;\n\n\tDRM_DEBUG(\"\\n\");\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_BONAIRE:\n\t\tchip_name = \"bonaire\";\n\t\tbreak;\n\tcase CHIP_HAWAII:\n\t\tchip_name = \"hawaii\";\n\t\tbreak;\n\tcase CHIP_TOPAZ:\n\t\tchip_name = \"topaz\";\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mc.bin\", chip_name);\n\n\terr = amdgpu_ucode_request(adev, &adev->gmc.fw, fw_name);\n\tif (err) {\n\t\tpr_err(\"cik_mc: Failed to load firmware \\\"%s\\\"\\n\", fw_name);\n\t\tamdgpu_ucode_release(&adev->gmc.fw);\n\t}\n\treturn err;\n}\n\n \nstatic int gmc_v7_0_mc_load_microcode(struct amdgpu_device *adev)\n{\n\tconst struct mc_firmware_header_v1_0 *hdr;\n\tconst __le32 *fw_data = NULL;\n\tconst __le32 *io_mc_regs = NULL;\n\tu32 running;\n\tint i, ucode_size, regs_size;\n\n\tif (!adev->gmc.fw)\n\t\treturn -EINVAL;\n\n\thdr = (const struct mc_firmware_header_v1_0 *)adev->gmc.fw->data;\n\tamdgpu_ucode_print_mc_hdr(&hdr->header);\n\n\tadev->gmc.fw_version = le32_to_cpu(hdr->header.ucode_version);\n\tregs_size = le32_to_cpu(hdr->io_debug_size_bytes) / (4 * 2);\n\tio_mc_regs = (const __le32 *)\n\t\t(adev->gmc.fw->data + le32_to_cpu(hdr->io_debug_array_offset_bytes));\n\tucode_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\tfw_data = (const __le32 *)\n\t\t(adev->gmc.fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\trunning = REG_GET_FIELD(RREG32(mmMC_SEQ_SUP_CNTL), MC_SEQ_SUP_CNTL, RUN);\n\n\tif (running == 0) {\n\t\t \n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000010);\n\n\t\t \n\t\tfor (i = 0; i < regs_size; i++) {\n\t\t\tWREG32(mmMC_SEQ_IO_DEBUG_INDEX, le32_to_cpup(io_mc_regs++));\n\t\t\tWREG32(mmMC_SEQ_IO_DEBUG_DATA, le32_to_cpup(io_mc_regs++));\n\t\t}\n\t\t \n\t\tfor (i = 0; i < ucode_size; i++)\n\t\t\tWREG32(mmMC_SEQ_SUP_PGM, le32_to_cpup(fw_data++));\n\n\t\t \n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000004);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000001);\n\n\t\t \n\t\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t\tif (REG_GET_FIELD(RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL),\n\t\t\t\t\t  MC_SEQ_TRAIN_WAKEUP_CNTL, TRAIN_DONE_D0))\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\t\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t\tif (REG_GET_FIELD(RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL),\n\t\t\t\t\t  MC_SEQ_TRAIN_WAKEUP_CNTL, TRAIN_DONE_D1))\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void gmc_v7_0_vram_gtt_location(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gmc *mc)\n{\n\tu64 base = RREG32(mmMC_VM_FB_LOCATION) & 0xFFFF;\n\n\tbase <<= 24;\n\n\tamdgpu_gmc_vram_location(adev, mc, base);\n\tamdgpu_gmc_gart_location(adev, mc);\n}\n\n \nstatic void gmc_v7_0_mc_program(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\tint i, j;\n\n\t \n\tfor (i = 0, j = 0; i < 32; i++, j += 0x6) {\n\t\tWREG32((0xb05 + j), 0x00000000);\n\t\tWREG32((0xb06 + j), 0x00000000);\n\t\tWREG32((0xb07 + j), 0x00000000);\n\t\tWREG32((0xb08 + j), 0x00000000);\n\t\tWREG32((0xb09 + j), 0x00000000);\n\t}\n\tWREG32(mmHDP_REG_COHERENCY_FLUSH_CNTL, 0);\n\n\tif (gmc_v7_0_wait_for_idle((void *)adev))\n\t\tdev_warn(adev->dev, \"Wait for MC idle timedout !\\n\");\n\n\tif (adev->mode_info.num_crtc) {\n\t\t \n\t\ttmp = RREG32(mmVGA_HDP_CONTROL);\n\t\ttmp = REG_SET_FIELD(tmp, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 1);\n\t\tWREG32(mmVGA_HDP_CONTROL, tmp);\n\n\t\t \n\t\ttmp = RREG32(mmVGA_RENDER_CONTROL);\n\t\ttmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 0);\n\t\tWREG32(mmVGA_RENDER_CONTROL, tmp);\n\t}\n\t \n\tWREG32(mmMC_VM_SYSTEM_APERTURE_LOW_ADDR,\n\t       adev->gmc.vram_start >> 12);\n\tWREG32(mmMC_VM_SYSTEM_APERTURE_HIGH_ADDR,\n\t       adev->gmc.vram_end >> 12);\n\tWREG32(mmMC_VM_SYSTEM_APERTURE_DEFAULT_ADDR,\n\t       adev->mem_scratch.gpu_addr >> 12);\n\tWREG32(mmMC_VM_AGP_BASE, 0);\n\tWREG32(mmMC_VM_AGP_TOP, 0x0FFFFFFF);\n\tWREG32(mmMC_VM_AGP_BOT, 0x0FFFFFFF);\n\tif (gmc_v7_0_wait_for_idle((void *)adev))\n\t\tdev_warn(adev->dev, \"Wait for MC idle timedout !\\n\");\n\n\tWREG32(mmBIF_FB_EN, BIF_FB_EN__FB_READ_EN_MASK | BIF_FB_EN__FB_WRITE_EN_MASK);\n\n\ttmp = RREG32(mmHDP_MISC_CNTL);\n\ttmp = REG_SET_FIELD(tmp, HDP_MISC_CNTL, FLUSH_INVALIDATE_CACHE, 0);\n\tWREG32(mmHDP_MISC_CNTL, tmp);\n\n\ttmp = RREG32(mmHDP_HOST_PATH_CNTL);\n\tWREG32(mmHDP_HOST_PATH_CNTL, tmp);\n}\n\n \nstatic int gmc_v7_0_mc_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tadev->gmc.vram_width = amdgpu_atombios_get_vram_width(adev);\n\tif (!adev->gmc.vram_width) {\n\t\tu32 tmp;\n\t\tint chansize, numchan;\n\n\t\t \n\t\ttmp = RREG32(mmMC_ARB_RAMCFG);\n\t\tif (REG_GET_FIELD(tmp, MC_ARB_RAMCFG, CHANSIZE))\n\t\t\tchansize = 64;\n\t\telse\n\t\t\tchansize = 32;\n\n\t\ttmp = RREG32(mmMC_SHARED_CHMAP);\n\t\tswitch (REG_GET_FIELD(tmp, MC_SHARED_CHMAP, NOOFCHAN)) {\n\t\tcase 0:\n\t\tdefault:\n\t\t\tnumchan = 1;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tnumchan = 2;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tnumchan = 4;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tnumchan = 8;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tnumchan = 3;\n\t\t\tbreak;\n\t\tcase 5:\n\t\t\tnumchan = 6;\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\tnumchan = 10;\n\t\t\tbreak;\n\t\tcase 7:\n\t\t\tnumchan = 12;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tnumchan = 16;\n\t\t\tbreak;\n\t\t}\n\t\tadev->gmc.vram_width = numchan * chansize;\n\t}\n\t \n\tadev->gmc.mc_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\n\tadev->gmc.real_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tr = amdgpu_device_resize_fb_bar(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\tadev->gmc.aper_base = pci_resource_start(adev->pdev, 0);\n\tadev->gmc.aper_size = pci_resource_len(adev->pdev, 0);\n\n#ifdef CONFIG_X86_64\n\tif ((adev->flags & AMD_IS_APU) &&\n\t    adev->gmc.real_vram_size > adev->gmc.aper_size &&\n\t    !amdgpu_passthrough(adev)) {\n\t\tadev->gmc.aper_base = ((u64)RREG32(mmMC_VM_FB_OFFSET)) << 22;\n\t\tadev->gmc.aper_size = adev->gmc.real_vram_size;\n\t}\n#endif\n\n\tadev->gmc.visible_vram_size = adev->gmc.aper_size;\n\n\t \n\tif (amdgpu_gart_size == -1) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_TOPAZ:      \n\t\tdefault:\n\t\t\tadev->gmc.gart_size = 256ULL << 20;\n\t\t\tbreak;\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\t\tcase CHIP_BONAIRE:  \n\t\tcase CHIP_HAWAII:   \n\t\tcase CHIP_KAVERI:   \n\t\tcase CHIP_KABINI:   \n\t\tcase CHIP_MULLINS:  \n\t\t\tadev->gmc.gart_size = 1024ULL << 20;\n\t\t\tbreak;\n#endif\n\t\t}\n\t} else {\n\t\tadev->gmc.gart_size = (u64)amdgpu_gart_size << 20;\n\t}\n\n\tadev->gmc.gart_size += adev->pm.smu_prv_buffer_size;\n\tgmc_v7_0_vram_gtt_location(adev, &adev->gmc);\n\n\treturn 0;\n}\n\n \nstatic int gmc_v7_0_flush_gpu_tlb_pasid(struct amdgpu_device *adev,\n\t\t\t\t\tuint16_t pasid, uint32_t flush_type,\n\t\t\t\t\tbool all_hub, uint32_t inst)\n{\n\tint vmid;\n\tunsigned int tmp;\n\n\tif (amdgpu_in_reset(adev))\n\t\treturn -EIO;\n\n\tfor (vmid = 1; vmid < 16; vmid++) {\n\n\t\ttmp = RREG32(mmATC_VMID0_PASID_MAPPING + vmid);\n\t\tif ((tmp & ATC_VMID0_PASID_MAPPING__VALID_MASK) &&\n\t\t\t(tmp & ATC_VMID0_PASID_MAPPING__PASID_MASK) == pasid) {\n\t\t\tWREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);\n\t\t\tRREG32(mmVM_INVALIDATE_RESPONSE);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \n\n \nstatic void gmc_v7_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,\n\t\t\t\t\tuint32_t vmhub, uint32_t flush_type)\n{\n\t \n\tWREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);\n}\n\nstatic uint64_t gmc_v7_0_emit_flush_gpu_tlb(struct amdgpu_ring *ring,\n\t\t\t\t\t    unsigned int vmid, uint64_t pd_addr)\n{\n\tuint32_t reg;\n\n\tif (vmid < 8)\n\t\treg = mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + vmid;\n\telse\n\t\treg = mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + vmid - 8;\n\tamdgpu_ring_emit_wreg(ring, reg, pd_addr >> 12);\n\n\t \n\tamdgpu_ring_emit_wreg(ring, mmVM_INVALIDATE_REQUEST, 1 << vmid);\n\n\treturn pd_addr;\n}\n\nstatic void gmc_v7_0_emit_pasid_mapping(struct amdgpu_ring *ring, unsigned int vmid,\n\t\t\t\t\tunsigned int pasid)\n{\n\tamdgpu_ring_emit_wreg(ring, mmIH_VMID_0_LUT + vmid, pasid);\n}\n\nstatic void gmc_v7_0_get_vm_pde(struct amdgpu_device *adev, int level,\n\t\t\t\tuint64_t *addr, uint64_t *flags)\n{\n\tBUG_ON(*addr & 0xFFFFFF0000000FFFULL);\n}\n\nstatic void gmc_v7_0_get_vm_pte(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_bo_va_mapping *mapping,\n\t\t\t\tuint64_t *flags)\n{\n\t*flags &= ~AMDGPU_PTE_EXECUTABLE;\n\t*flags &= ~AMDGPU_PTE_PRT;\n}\n\n \nstatic void gmc_v7_0_set_fault_enable_default(struct amdgpu_device *adev,\n\t\t\t\t\t      bool value)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    RANGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    DUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    PDE0_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    VALID_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    READ_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    WRITE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n}\n\n \nstatic void gmc_v7_0_set_prt(struct amdgpu_device *adev, bool enable)\n{\n\tuint32_t tmp;\n\n\tif (enable && !adev->gmc.prt_warning) {\n\t\tdev_warn(adev->dev, \"Disabling VM faults because of PRT request!\\n\");\n\t\tadev->gmc.prt_warning = true;\n\t}\n\n\ttmp = RREG32(mmVM_PRT_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    CB_DISABLE_READ_FAULT_ON_UNMAPPED_ACCESS, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    CB_DISABLE_WRITE_FAULT_ON_UNMAPPED_ACCESS, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    TC_DISABLE_READ_FAULT_ON_UNMAPPED_ACCESS, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    TC_DISABLE_WRITE_FAULT_ON_UNMAPPED_ACCESS, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    L2_CACHE_STORE_INVALID_ENTRIES, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    L1_TLB_STORE_INVALID_ENTRIES, enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    MASK_PDE0_FAULT, enable);\n\tWREG32(mmVM_PRT_CNTL, tmp);\n\n\tif (enable) {\n\t\tuint32_t low = AMDGPU_VA_RESERVED_SIZE >> AMDGPU_GPU_PAGE_SHIFT;\n\t\tuint32_t high = adev->vm_manager.max_pfn -\n\t\t\t(AMDGPU_VA_RESERVED_SIZE >> AMDGPU_GPU_PAGE_SHIFT);\n\n\t\tWREG32(mmVM_PRT_APERTURE0_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE1_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE2_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE3_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE0_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE1_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE2_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE3_HIGH_ADDR, high);\n\t} else {\n\t\tWREG32(mmVM_PRT_APERTURE0_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE1_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE2_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE3_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE0_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE1_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE2_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE3_HIGH_ADDR, 0x0);\n\t}\n}\n\n \nstatic int gmc_v7_0_gart_enable(struct amdgpu_device *adev)\n{\n\tuint64_t table_addr;\n\tu32 tmp, field;\n\tint i;\n\n\tif (adev->gart.bo == NULL) {\n\t\tdev_err(adev->dev, \"No VRAM object for PCIE GART.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tamdgpu_gtt_mgr_recover(&adev->mman.gtt_mgr);\n\ttable_addr = amdgpu_bo_gpu_offset(adev->gart.bo);\n\n\t \n\ttmp = RREG32(mmMC_VM_MX_L1_TLB_CNTL);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_L1_TLB, 1);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_L1_FRAGMENT_PROCESSING, 1);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, SYSTEM_ACCESS_MODE, 3);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_ADVANCED_DRIVER_MODEL, 1);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, SYSTEM_APERTURE_UNMAPPED_ACCESS, 0);\n\tWREG32(mmMC_VM_MX_L1_TLB_CNTL, tmp);\n\t \n\ttmp = RREG32(mmVM_L2_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_L2_CACHE, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_L2_FRAGMENT_PROCESSING, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, EFFECTIVE_L2_QUEUE_SIZE, 7);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, CONTEXT1_IDENTITY_ACCESS_MODE, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_DEFAULT_PAGE_OUT_TO_SYSTEM_MEMORY, 1);\n\tWREG32(mmVM_L2_CNTL, tmp);\n\ttmp = REG_SET_FIELD(0, VM_L2_CNTL2, INVALIDATE_ALL_L1_TLBS, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL2, INVALIDATE_L2_CACHE, 1);\n\tWREG32(mmVM_L2_CNTL2, tmp);\n\n\tfield = adev->vm_manager.fragment_size;\n\ttmp = RREG32(mmVM_L2_CNTL3);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL3, L2_CACHE_BIGK_ASSOCIATIVITY, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL3, BANK_SELECT, field);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL3, L2_CACHE_BIGK_FRAGMENT_SIZE, field);\n\tWREG32(mmVM_L2_CNTL3, tmp);\n\t \n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_START_ADDR, adev->gmc.gart_start >> 12);\n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_END_ADDR, adev->gmc.gart_end >> 12);\n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR, table_addr >> 12);\n\tWREG32(mmVM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\n\t\t\t(u32)(adev->dummy_page_addr >> 12));\n\tWREG32(mmVM_CONTEXT0_CNTL2, 0);\n\ttmp = RREG32(mmVM_CONTEXT0_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT0_CNTL, ENABLE_CONTEXT, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT0_CNTL, PAGE_TABLE_DEPTH, 0);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT0_CNTL, RANGE_PROTECTION_FAULT_ENABLE_DEFAULT, 1);\n\tWREG32(mmVM_CONTEXT0_CNTL, tmp);\n\n\tWREG32(0x575, 0);\n\tWREG32(0x576, 0);\n\tWREG32(0x577, 0);\n\n\t \n\t \n\t \n\tWREG32(mmVM_CONTEXT1_PAGE_TABLE_START_ADDR, 0);\n\tWREG32(mmVM_CONTEXT1_PAGE_TABLE_END_ADDR, adev->vm_manager.max_pfn - 1);\n\tfor (i = 1; i < AMDGPU_NUM_VMID; i++) {\n\t\tif (i < 8)\n\t\t\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + i,\n\t\t\t       table_addr >> 12);\n\t\telse\n\t\t\tWREG32(mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + i - 8,\n\t\t\t       table_addr >> 12);\n\t}\n\n\t \n\tWREG32(mmVM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\n\t       (u32)(adev->dummy_page_addr >> 12));\n\tWREG32(mmVM_CONTEXT1_CNTL2, 4);\n\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL, ENABLE_CONTEXT, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL, PAGE_TABLE_DEPTH, 1);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL, PAGE_TABLE_BLOCK_SIZE,\n\t\t\t    adev->vm_manager.block_size - 9);\n\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n\tif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_ALWAYS)\n\t\tgmc_v7_0_set_fault_enable_default(adev, false);\n\telse\n\t\tgmc_v7_0_set_fault_enable_default(adev, true);\n\n\tif (adev->asic_type == CHIP_KAVERI) {\n\t\ttmp = RREG32(mmCHUB_CONTROL);\n\t\ttmp &= ~BYPASS_VM;\n\t\tWREG32(mmCHUB_CONTROL, tmp);\n\t}\n\n\tgmc_v7_0_flush_gpu_tlb(adev, 0, 0, 0);\n\tDRM_INFO(\"PCIE GART of %uM enabled (table at 0x%016llX).\\n\",\n\t\t (unsigned int)(adev->gmc.gart_size >> 20),\n\t\t (unsigned long long)table_addr);\n\treturn 0;\n}\n\nstatic int gmc_v7_0_gart_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->gart.bo) {\n\t\tWARN(1, \"R600 PCIE GART already initialized\\n\");\n\t\treturn 0;\n\t}\n\t \n\tr = amdgpu_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\tadev->gart.table_size = adev->gart.num_gpu_pages * 8;\n\tadev->gart.gart_pte_flags = 0;\n\treturn amdgpu_gart_table_vram_alloc(adev);\n}\n\n \nstatic void gmc_v7_0_gart_disable(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\t \n\tWREG32(mmVM_CONTEXT0_CNTL, 0);\n\tWREG32(mmVM_CONTEXT1_CNTL, 0);\n\t \n\ttmp = RREG32(mmMC_VM_MX_L1_TLB_CNTL);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_L1_TLB, 0);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_L1_FRAGMENT_PROCESSING, 0);\n\ttmp = REG_SET_FIELD(tmp, MC_VM_MX_L1_TLB_CNTL, ENABLE_ADVANCED_DRIVER_MODEL, 0);\n\tWREG32(mmMC_VM_MX_L1_TLB_CNTL, tmp);\n\t \n\ttmp = RREG32(mmVM_L2_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_L2_CNTL, ENABLE_L2_CACHE, 0);\n\tWREG32(mmVM_L2_CNTL, tmp);\n\tWREG32(mmVM_L2_CNTL2, 0);\n}\n\n \nstatic void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev, u32 status,\n\t\t\t\t     u32 addr, u32 mc_client, unsigned int pasid)\n{\n\tu32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);\n\tu32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t\t\tPROTECTIONS);\n\tchar block[5] = { mc_client >> 24, (mc_client >> 16) & 0xff,\n\t\t(mc_client >> 8) & 0xff, mc_client & 0xff, 0 };\n\tu32 mc_id;\n\n\tmc_id = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t      MEMORY_CLIENT_ID);\n\n\tdev_err(adev->dev, \"VM fault (0x%02x, vmid %d, pasid %d) at page %u, %s from '%s' (0x%08x) (%d)\\n\",\n\t       protections, vmid, pasid, addr,\n\t       REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t     MEMORY_CLIENT_RW) ?\n\t       \"write\" : \"read\", block, mc_client, mc_id);\n}\n\n\nstatic const u32 mc_cg_registers[] = {\n\tmmMC_HUB_MISC_HUB_CG,\n\tmmMC_HUB_MISC_SIP_CG,\n\tmmMC_HUB_MISC_VM_CG,\n\tmmMC_XPB_CLK_GAT,\n\tmmATC_MISC_CG,\n\tmmMC_CITF_MISC_WR_CG,\n\tmmMC_CITF_MISC_RD_CG,\n\tmmMC_CITF_MISC_VM_CG,\n\tmmVM_L2_CG,\n};\n\nstatic const u32 mc_cg_ls_en[] = {\n\tMC_HUB_MISC_HUB_CG__MEM_LS_ENABLE_MASK,\n\tMC_HUB_MISC_SIP_CG__MEM_LS_ENABLE_MASK,\n\tMC_HUB_MISC_VM_CG__MEM_LS_ENABLE_MASK,\n\tMC_XPB_CLK_GAT__MEM_LS_ENABLE_MASK,\n\tATC_MISC_CG__MEM_LS_ENABLE_MASK,\n\tMC_CITF_MISC_WR_CG__MEM_LS_ENABLE_MASK,\n\tMC_CITF_MISC_RD_CG__MEM_LS_ENABLE_MASK,\n\tMC_CITF_MISC_VM_CG__MEM_LS_ENABLE_MASK,\n\tVM_L2_CG__MEM_LS_ENABLE_MASK,\n};\n\nstatic const u32 mc_cg_en[] = {\n\tMC_HUB_MISC_HUB_CG__ENABLE_MASK,\n\tMC_HUB_MISC_SIP_CG__ENABLE_MASK,\n\tMC_HUB_MISC_VM_CG__ENABLE_MASK,\n\tMC_XPB_CLK_GAT__ENABLE_MASK,\n\tATC_MISC_CG__ENABLE_MASK,\n\tMC_CITF_MISC_WR_CG__ENABLE_MASK,\n\tMC_CITF_MISC_RD_CG__ENABLE_MASK,\n\tMC_CITF_MISC_VM_CG__ENABLE_MASK,\n\tVM_L2_CG__ENABLE_MASK,\n};\n\nstatic void gmc_v7_0_enable_mc_ls(struct amdgpu_device *adev,\n\t\t\t\t  bool enable)\n{\n\tint i;\n\tu32 orig, data;\n\n\tfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\n\t\torig = data = RREG32(mc_cg_registers[i]);\n\t\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_MC_LS))\n\t\t\tdata |= mc_cg_ls_en[i];\n\t\telse\n\t\t\tdata &= ~mc_cg_ls_en[i];\n\t\tif (data != orig)\n\t\t\tWREG32(mc_cg_registers[i], data);\n\t}\n}\n\nstatic void gmc_v7_0_enable_mc_mgcg(struct amdgpu_device *adev,\n\t\t\t\t    bool enable)\n{\n\tint i;\n\tu32 orig, data;\n\n\tfor (i = 0; i < ARRAY_SIZE(mc_cg_registers); i++) {\n\t\torig = data = RREG32(mc_cg_registers[i]);\n\t\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_MC_MGCG))\n\t\t\tdata |= mc_cg_en[i];\n\t\telse\n\t\t\tdata &= ~mc_cg_en[i];\n\t\tif (data != orig)\n\t\t\tWREG32(mc_cg_registers[i], data);\n\t}\n}\n\nstatic void gmc_v7_0_enable_bif_mgls(struct amdgpu_device *adev,\n\t\t\t\t     bool enable)\n{\n\tu32 orig, data;\n\n\torig = data = RREG32_PCIE(ixPCIE_CNTL2);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS)) {\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, SLV_MEM_LS_EN, 1);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, MST_MEM_LS_EN, 1);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, REPLAY_MEM_LS_EN, 1);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, SLV_MEM_AGGRESSIVE_LS_EN, 1);\n\t} else {\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, SLV_MEM_LS_EN, 0);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, MST_MEM_LS_EN, 0);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, REPLAY_MEM_LS_EN, 0);\n\t\tdata = REG_SET_FIELD(data, PCIE_CNTL2, SLV_MEM_AGGRESSIVE_LS_EN, 0);\n\t}\n\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_CNTL2, data);\n}\n\nstatic void gmc_v7_0_enable_hdp_mgcg(struct amdgpu_device *adev,\n\t\t\t\t     bool enable)\n{\n\tu32 orig, data;\n\n\torig = data = RREG32(mmHDP_HOST_PATH_CNTL);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG))\n\t\tdata = REG_SET_FIELD(data, HDP_HOST_PATH_CNTL, CLOCK_GATING_DIS, 0);\n\telse\n\t\tdata = REG_SET_FIELD(data, HDP_HOST_PATH_CNTL, CLOCK_GATING_DIS, 1);\n\n\tif (orig != data)\n\t\tWREG32(mmHDP_HOST_PATH_CNTL, data);\n}\n\nstatic void gmc_v7_0_enable_hdp_ls(struct amdgpu_device *adev,\n\t\t\t\t   bool enable)\n{\n\tu32 orig, data;\n\n\torig = data = RREG32(mmHDP_MEM_POWER_LS);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS))\n\t\tdata = REG_SET_FIELD(data, HDP_MEM_POWER_LS, LS_ENABLE, 1);\n\telse\n\t\tdata = REG_SET_FIELD(data, HDP_MEM_POWER_LS, LS_ENABLE, 0);\n\n\tif (orig != data)\n\t\tWREG32(mmHDP_MEM_POWER_LS, data);\n}\n\nstatic int gmc_v7_0_convert_vram_type(int mc_seq_vram_type)\n{\n\tswitch (mc_seq_vram_type) {\n\tcase MC_SEQ_MISC0__MT__GDDR1:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR1;\n\tcase MC_SEQ_MISC0__MT__DDR2:\n\t\treturn AMDGPU_VRAM_TYPE_DDR2;\n\tcase MC_SEQ_MISC0__MT__GDDR3:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR3;\n\tcase MC_SEQ_MISC0__MT__GDDR4:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR4;\n\tcase MC_SEQ_MISC0__MT__GDDR5:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR5;\n\tcase MC_SEQ_MISC0__MT__HBM:\n\t\treturn AMDGPU_VRAM_TYPE_HBM;\n\tcase MC_SEQ_MISC0__MT__DDR3:\n\t\treturn AMDGPU_VRAM_TYPE_DDR3;\n\tdefault:\n\t\treturn AMDGPU_VRAM_TYPE_UNKNOWN;\n\t}\n}\n\nstatic int gmc_v7_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v7_0_set_gmc_funcs(adev);\n\tgmc_v7_0_set_irq_funcs(adev);\n\n\tadev->gmc.shared_aperture_start = 0x2000000000000000ULL;\n\tadev->gmc.shared_aperture_end =\n\t\tadev->gmc.shared_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.private_aperture_start =\n\t\tadev->gmc.shared_aperture_end + 1;\n\tadev->gmc.private_aperture_end =\n\t\tadev->gmc.private_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.noretry_flags = AMDGPU_VM_NORETRY_FLAGS_TF;\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_vm_fault_stop != AMDGPU_VM_FAULT_STOP_ALWAYS)\n\t\treturn amdgpu_irq_get(adev, &adev->gmc.vm_fault, 0);\n\telse\n\t\treturn 0;\n}\n\nstatic unsigned int gmc_v7_0_get_vbios_fb_size(struct amdgpu_device *adev)\n{\n\tu32 d1vga_control = RREG32(mmD1VGA_CONTROL);\n\tunsigned int size;\n\n\tif (REG_GET_FIELD(d1vga_control, D1VGA_CONTROL, D1VGA_MODE_ENABLE)) {\n\t\tsize = AMDGPU_VBIOS_VGA_ALLOCATION;\n\t} else {\n\t\tu32 viewport = RREG32(mmVIEWPORT_SIZE);\n\n\t\tsize = (REG_GET_FIELD(viewport, VIEWPORT_SIZE, VIEWPORT_HEIGHT) *\n\t\t\tREG_GET_FIELD(viewport, VIEWPORT_SIZE, VIEWPORT_WIDTH) *\n\t\t\t4);\n\t}\n\n\treturn size;\n}\n\nstatic int gmc_v7_0_sw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_UNKNOWN;\n\t} else {\n\t\tu32 tmp = RREG32(mmMC_SEQ_MISC0);\n\n\t\ttmp &= MC_SEQ_MISC0__MT__MASK;\n\t\tadev->gmc.vram_type = gmc_v7_0_convert_vram_type(tmp);\n\t}\n\n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, VISLANDS30_IV_SRCID_GFX_PAGE_INV_FAULT, &adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, VISLANDS30_IV_SRCID_GFX_MEM_PROT_FAULT, &adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\t \n\tamdgpu_vm_adjust_size(adev, 64, 9, 1, 40);\n\n\t \n\tadev->gmc.mc_mask = 0xffffffffffULL;  \n\n\tr = dma_set_mask_and_coherent(adev->dev, DMA_BIT_MASK(40));\n\tif (r) {\n\t\tpr_warn(\"No suitable DMA available\\n\");\n\t\treturn r;\n\t}\n\tadev->need_swiotlb = drm_need_swiotlb(40);\n\n\tr = gmc_v7_0_init_microcode(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load mc firmware!\\n\");\n\t\treturn r;\n\t}\n\n\tr = gmc_v7_0_mc_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_gmc_get_vbios_allocations(adev);\n\n\t \n\tr = amdgpu_bo_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = gmc_v7_0_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tadev->vm_manager.first_kfd_vmid = 8;\n\tamdgpu_vm_manager_init(adev);\n\n\t \n\tif (adev->flags & AMD_IS_APU) {\n\t\tu64 tmp = RREG32(mmMC_VM_FB_OFFSET);\n\n\t\ttmp <<= 22;\n\t\tadev->vm_manager.vram_base_offset = tmp;\n\t} else {\n\t\tadev->vm_manager.vram_base_offset = 0;\n\t}\n\n\tadev->gmc.vm_fault_info = kmalloc(sizeof(struct kfd_vm_fault_info),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!adev->gmc.vm_fault_info)\n\t\treturn -ENOMEM;\n\tatomic_set(&adev->gmc.vm_fault_info_updated, 0);\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_gem_force_release(adev);\n\tamdgpu_vm_manager_fini(adev);\n\tkfree(adev->gmc.vm_fault_info);\n\tamdgpu_gart_table_vram_free(adev);\n\tamdgpu_bo_fini(adev);\n\tamdgpu_ucode_release(&adev->gmc.fw);\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v7_0_init_golden_registers(adev);\n\n\tgmc_v7_0_mc_program(adev);\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tr = gmc_v7_0_mc_load_microcode(adev);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Failed to load MC firmware!\\n\");\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tr = gmc_v7_0_gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_emu_mode == 1)\n\t\treturn amdgpu_gmc_vram_checking(adev);\n\telse\n\t\treturn r;\n}\n\nstatic int gmc_v7_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_irq_put(adev, &adev->gmc.vm_fault, 0);\n\tgmc_v7_0_gart_disable(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v7_0_hw_fini(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = gmc_v7_0_hw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vmid_reset_all(adev);\n\n\treturn 0;\n}\n\nstatic bool gmc_v7_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(mmSRBM_STATUS);\n\n\tif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\n\t\t   SRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK | SRBM_STATUS__VMC_BUSY_MASK))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int gmc_v7_0_wait_for_idle(void *handle)\n{\n\tunsigned int i;\n\tu32 tmp;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t \n\t\ttmp = RREG32(mmSRBM_STATUS) & (SRBM_STATUS__MCB_BUSY_MASK |\n\t\t\t\t\t       SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\n\t\t\t\t\t       SRBM_STATUS__MCC_BUSY_MASK |\n\t\t\t\t\t       SRBM_STATUS__MCD_BUSY_MASK |\n\t\t\t\t\t       SRBM_STATUS__VMC_BUSY_MASK);\n\t\tif (!tmp)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n\n}\n\nstatic int gmc_v7_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset = 0;\n\tu32 tmp = RREG32(mmSRBM_STATUS);\n\n\tif (tmp & SRBM_STATUS__VMC_BUSY_MASK)\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\n\t\t\t\t\t\tSRBM_SOFT_RESET, SOFT_RESET_VMC, 1);\n\n\tif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\n\t\t   SRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK)) {\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\n\t\t\t\t\t\t\tSRBM_SOFT_RESET, SOFT_RESET_MC, 1);\n\t}\n\n\tif (srbm_soft_reset) {\n\t\tgmc_v7_0_mc_stop(adev);\n\t\tif (gmc_v7_0_wait_for_idle((void *)adev))\n\t\t\tdev_warn(adev->dev, \"Wait for GMC idle timed out !\\n\");\n\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\n\t\tgmc_v7_0_mc_resume(adev);\n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_vm_fault_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t     struct amdgpu_irq_src *src,\n\t\t\t\t\t     unsigned int type,\n\t\t\t\t\t     enum amdgpu_interrupt_state state)\n{\n\tu32 tmp;\n\tu32 bits = (VM_CONTEXT1_CNTL__RANGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__DUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__PDE0_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__VALID_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__READ_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__WRITE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK);\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t \n\t\ttmp = RREG32(mmVM_CONTEXT0_CNTL);\n\t\ttmp &= ~bits;\n\t\tWREG32(mmVM_CONTEXT0_CNTL, tmp);\n\t\t \n\t\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\t\ttmp &= ~bits;\n\t\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t \n\t\ttmp = RREG32(mmVM_CONTEXT0_CNTL);\n\t\ttmp |= bits;\n\t\tWREG32(mmVM_CONTEXT0_CNTL, tmp);\n\t\t \n\t\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\t\ttmp |= bits;\n\t\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tu32 addr, status, mc_client, vmid;\n\n\taddr = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_ADDR);\n\tstatus = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_STATUS);\n\tmc_client = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_MCCLIENT);\n\t \n\tWREG32_P(mmVM_CONTEXT1_CNTL2, 1, ~1);\n\n\tif (!addr && !status)\n\t\treturn 0;\n\n\tif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_FIRST)\n\t\tgmc_v7_0_set_fault_enable_default(adev, false);\n\n\tif (printk_ratelimit()) {\n\t\tdev_err(adev->dev, \"GPU fault detected: %d 0x%08x\\n\",\n\t\t\tentry->src_id, entry->src_data[0]);\n\t\tdev_err(adev->dev, \"  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\\n\",\n\t\t\taddr);\n\t\tdev_err(adev->dev, \"  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\\n\",\n\t\t\tstatus);\n\t\tgmc_v7_0_vm_decode_fault(adev, status, addr, mc_client,\n\t\t\t\t\t entry->pasid);\n\t}\n\n\tvmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t     VMID);\n\tif (amdgpu_amdkfd_is_kfd_vmid(adev, vmid)\n\t\t&& !atomic_read(&adev->gmc.vm_fault_info_updated)) {\n\t\tstruct kfd_vm_fault_info *info = adev->gmc.vm_fault_info;\n\t\tu32 protections = REG_GET_FIELD(status,\n\t\t\t\t\tVM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t\t\tPROTECTIONS);\n\n\t\tinfo->vmid = vmid;\n\t\tinfo->mc_id = REG_GET_FIELD(status,\n\t\t\t\t\t    VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t\t\t    MEMORY_CLIENT_ID);\n\t\tinfo->status = status;\n\t\tinfo->page_addr = addr;\n\t\tinfo->prot_valid = protections & 0x7 ? true : false;\n\t\tinfo->prot_read = protections & 0x8 ? true : false;\n\t\tinfo->prot_write = protections & 0x10 ? true : false;\n\t\tinfo->prot_exec = protections & 0x20 ? true : false;\n\t\tmb();\n\t\tatomic_set(&adev->gmc.vm_fault_info_updated, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tbool gate = false;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (state == AMD_CG_STATE_GATE)\n\t\tgate = true;\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tgmc_v7_0_enable_mc_mgcg(adev, gate);\n\t\tgmc_v7_0_enable_mc_ls(adev, gate);\n\t}\n\tgmc_v7_0_enable_bif_mgls(adev, gate);\n\tgmc_v7_0_enable_hdp_mgcg(adev, gate);\n\tgmc_v7_0_enable_hdp_ls(adev, gate);\n\n\treturn 0;\n}\n\nstatic int gmc_v7_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs gmc_v7_0_ip_funcs = {\n\t.name = \"gmc_v7_0\",\n\t.early_init = gmc_v7_0_early_init,\n\t.late_init = gmc_v7_0_late_init,\n\t.sw_init = gmc_v7_0_sw_init,\n\t.sw_fini = gmc_v7_0_sw_fini,\n\t.hw_init = gmc_v7_0_hw_init,\n\t.hw_fini = gmc_v7_0_hw_fini,\n\t.suspend = gmc_v7_0_suspend,\n\t.resume = gmc_v7_0_resume,\n\t.is_idle = gmc_v7_0_is_idle,\n\t.wait_for_idle = gmc_v7_0_wait_for_idle,\n\t.soft_reset = gmc_v7_0_soft_reset,\n\t.set_clockgating_state = gmc_v7_0_set_clockgating_state,\n\t.set_powergating_state = gmc_v7_0_set_powergating_state,\n};\n\nstatic const struct amdgpu_gmc_funcs gmc_v7_0_gmc_funcs = {\n\t.flush_gpu_tlb = gmc_v7_0_flush_gpu_tlb,\n\t.flush_gpu_tlb_pasid = gmc_v7_0_flush_gpu_tlb_pasid,\n\t.emit_flush_gpu_tlb = gmc_v7_0_emit_flush_gpu_tlb,\n\t.emit_pasid_mapping = gmc_v7_0_emit_pasid_mapping,\n\t.set_prt = gmc_v7_0_set_prt,\n\t.get_vm_pde = gmc_v7_0_get_vm_pde,\n\t.get_vm_pte = gmc_v7_0_get_vm_pte,\n\t.get_vbios_fb_size = gmc_v7_0_get_vbios_fb_size,\n};\n\nstatic const struct amdgpu_irq_src_funcs gmc_v7_0_irq_funcs = {\n\t.set = gmc_v7_0_vm_fault_interrupt_state,\n\t.process = gmc_v7_0_process_interrupt,\n};\n\nstatic void gmc_v7_0_set_gmc_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.gmc_funcs = &gmc_v7_0_gmc_funcs;\n}\n\nstatic void gmc_v7_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.vm_fault.num_types = 1;\n\tadev->gmc.vm_fault.funcs = &gmc_v7_0_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version gmc_v7_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GMC,\n\t.major = 7,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &gmc_v7_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version gmc_v7_4_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GMC,\n\t.major = 7,\n\t.minor = 4,\n\t.rev = 0,\n\t.funcs = &gmc_v7_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}