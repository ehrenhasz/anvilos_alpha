{
  "module_name": "amdgpu_vm_pt.c",
  "hash_id": "c09b9c7158f6f8b57551fa3b793714493277341711023b9dc9ef8bcc6270dcb0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_pt.c",
  "human_readable_source": "\n \n\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_vm.h\"\n\n \nstruct amdgpu_vm_pt_cursor {\n\tuint64_t pfn;\n\tstruct amdgpu_vm_bo_base *parent;\n\tstruct amdgpu_vm_bo_base *entry;\n\tunsigned int level;\n};\n\n \nstatic unsigned int amdgpu_vm_pt_level_shift(struct amdgpu_device *adev,\n\t\t\t\t\t     unsigned int level)\n{\n\tswitch (level) {\n\tcase AMDGPU_VM_PDB2:\n\tcase AMDGPU_VM_PDB1:\n\tcase AMDGPU_VM_PDB0:\n\t\treturn 9 * (AMDGPU_VM_PDB0 - level) +\n\t\t\tadev->vm_manager.block_size;\n\tcase AMDGPU_VM_PTB:\n\t\treturn 0;\n\tdefault:\n\t\treturn ~0;\n\t}\n}\n\n \nstatic unsigned int amdgpu_vm_pt_num_entries(struct amdgpu_device *adev,\n\t\t\t\t\t     unsigned int level)\n{\n\tunsigned int shift;\n\n\tshift = amdgpu_vm_pt_level_shift(adev, adev->vm_manager.root_level);\n\tif (level == adev->vm_manager.root_level)\n\t\t \n\t\treturn round_up(adev->vm_manager.max_pfn, 1ULL << shift)\n\t\t\t>> shift;\n\telse if (level != AMDGPU_VM_PTB)\n\t\t \n\t\treturn 512;\n\n\t \n\treturn AMDGPU_VM_PTE_COUNT(adev);\n}\n\n \nstatic unsigned int amdgpu_vm_pt_num_ats_entries(struct amdgpu_device *adev)\n{\n\tunsigned int shift;\n\n\tshift = amdgpu_vm_pt_level_shift(adev, adev->vm_manager.root_level);\n\treturn AMDGPU_GMC_HOLE_START >> (shift + AMDGPU_GPU_PAGE_SHIFT);\n}\n\n \nstatic uint32_t amdgpu_vm_pt_entries_mask(struct amdgpu_device *adev,\n\t\t\t\t\t  unsigned int level)\n{\n\tif (level <= adev->vm_manager.root_level)\n\t\treturn 0xffffffff;\n\telse if (level != AMDGPU_VM_PTB)\n\t\treturn 0x1ff;\n\telse\n\t\treturn AMDGPU_VM_PTE_COUNT(adev) - 1;\n}\n\n \nstatic unsigned int amdgpu_vm_pt_size(struct amdgpu_device *adev,\n\t\t\t\t      unsigned int level)\n{\n\treturn AMDGPU_GPU_PAGE_ALIGN(amdgpu_vm_pt_num_entries(adev, level) * 8);\n}\n\n \nstatic struct amdgpu_vm_bo_base *\namdgpu_vm_pt_parent(struct amdgpu_vm_bo_base *pt)\n{\n\tstruct amdgpu_bo *parent = pt->bo->parent;\n\n\tif (!parent)\n\t\treturn NULL;\n\n\treturn parent->vm_bo;\n}\n\n \nstatic void amdgpu_vm_pt_start(struct amdgpu_device *adev,\n\t\t\t       struct amdgpu_vm *vm, uint64_t start,\n\t\t\t       struct amdgpu_vm_pt_cursor *cursor)\n{\n\tcursor->pfn = start;\n\tcursor->parent = NULL;\n\tcursor->entry = &vm->root;\n\tcursor->level = adev->vm_manager.root_level;\n}\n\n \nstatic bool amdgpu_vm_pt_descendant(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_vm_pt_cursor *cursor)\n{\n\tunsigned int mask, shift, idx;\n\n\tif ((cursor->level == AMDGPU_VM_PTB) || !cursor->entry ||\n\t    !cursor->entry->bo)\n\t\treturn false;\n\n\tmask = amdgpu_vm_pt_entries_mask(adev, cursor->level);\n\tshift = amdgpu_vm_pt_level_shift(adev, cursor->level);\n\n\t++cursor->level;\n\tidx = (cursor->pfn >> shift) & mask;\n\tcursor->parent = cursor->entry;\n\tcursor->entry = &to_amdgpu_bo_vm(cursor->entry->bo)->entries[idx];\n\treturn true;\n}\n\n \nstatic bool amdgpu_vm_pt_sibling(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_vm_pt_cursor *cursor)\n{\n\n\tunsigned int shift, num_entries;\n\tstruct amdgpu_bo_vm *parent;\n\n\t \n\tif (!cursor->parent)\n\t\treturn false;\n\n\t \n\tshift = amdgpu_vm_pt_level_shift(adev, cursor->level - 1);\n\tnum_entries = amdgpu_vm_pt_num_entries(adev, cursor->level - 1);\n\tparent = to_amdgpu_bo_vm(cursor->parent->bo);\n\n\tif (cursor->entry == &parent->entries[num_entries - 1])\n\t\treturn false;\n\n\tcursor->pfn += 1ULL << shift;\n\tcursor->pfn &= ~((1ULL << shift) - 1);\n\t++cursor->entry;\n\treturn true;\n}\n\n \nstatic bool amdgpu_vm_pt_ancestor(struct amdgpu_vm_pt_cursor *cursor)\n{\n\tif (!cursor->parent)\n\t\treturn false;\n\n\t--cursor->level;\n\tcursor->entry = cursor->parent;\n\tcursor->parent = amdgpu_vm_pt_parent(cursor->parent);\n\treturn true;\n}\n\n \nstatic void amdgpu_vm_pt_next(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_vm_pt_cursor *cursor)\n{\n\t \n\tif (amdgpu_vm_pt_descendant(adev, cursor))\n\t\treturn;\n\n\t \n\twhile (!amdgpu_vm_pt_sibling(adev, cursor)) {\n\t\t \n\t\tif (!amdgpu_vm_pt_ancestor(cursor)) {\n\t\t\tcursor->pfn = ~0ll;\n\t\t\treturn;\n\t\t}\n\t}\n}\n\n \nstatic void amdgpu_vm_pt_first_dfs(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_vm *vm,\n\t\t\t\t   struct amdgpu_vm_pt_cursor *start,\n\t\t\t\t   struct amdgpu_vm_pt_cursor *cursor)\n{\n\tif (start)\n\t\t*cursor = *start;\n\telse\n\t\tamdgpu_vm_pt_start(adev, vm, 0, cursor);\n\n\twhile (amdgpu_vm_pt_descendant(adev, cursor))\n\t\t;\n}\n\n \nstatic bool amdgpu_vm_pt_continue_dfs(struct amdgpu_vm_pt_cursor *start,\n\t\t\t\t      struct amdgpu_vm_bo_base *entry)\n{\n\treturn entry && (!start || entry != start->entry);\n}\n\n \nstatic void amdgpu_vm_pt_next_dfs(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_vm_pt_cursor *cursor)\n{\n\tif (!cursor->entry)\n\t\treturn;\n\n\tif (!cursor->parent)\n\t\tcursor->entry = NULL;\n\telse if (amdgpu_vm_pt_sibling(adev, cursor))\n\t\twhile (amdgpu_vm_pt_descendant(adev, cursor))\n\t\t\t;\n\telse\n\t\tamdgpu_vm_pt_ancestor(cursor);\n}\n\n \n#define for_each_amdgpu_vm_pt_dfs_safe(adev, vm, start, cursor, entry)\t\t\\\n\tfor (amdgpu_vm_pt_first_dfs((adev), (vm), (start), &(cursor)),\t\t\\\n\t     (entry) = (cursor).entry, amdgpu_vm_pt_next_dfs((adev), &(cursor));\\\n\t     amdgpu_vm_pt_continue_dfs((start), (entry));\t\t\t\\\n\t     (entry) = (cursor).entry, amdgpu_vm_pt_next_dfs((adev), &(cursor)))\n\n \nint amdgpu_vm_pt_clear(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t       struct amdgpu_bo_vm *vmbo, bool immediate)\n{\n\tunsigned int level = adev->vm_manager.root_level;\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_vm_update_params params;\n\tstruct amdgpu_bo *ancestor = &vmbo->bo;\n\tunsigned int entries, ats_entries;\n\tstruct amdgpu_bo *bo = &vmbo->bo;\n\tuint64_t addr;\n\tint r, idx;\n\n\t \n\tif (ancestor->parent) {\n\t\t++level;\n\t\twhile (ancestor->parent->parent) {\n\t\t\t++level;\n\t\t\tancestor = ancestor->parent;\n\t\t}\n\t}\n\n\tentries = amdgpu_bo_size(bo) / 8;\n\tif (!vm->pte_support_ats) {\n\t\tats_entries = 0;\n\n\t} else if (!bo->parent) {\n\t\tats_entries = amdgpu_vm_pt_num_ats_entries(adev);\n\t\tats_entries = min(ats_entries, entries);\n\t\tentries -= ats_entries;\n\n\t} else {\n\t\tstruct amdgpu_vm_bo_base *pt;\n\n\t\tpt = ancestor->vm_bo;\n\t\tats_entries = amdgpu_vm_pt_num_ats_entries(adev);\n\t\tif ((pt - to_amdgpu_bo_vm(vm->root.bo)->entries) >=\n\t\t    ats_entries) {\n\t\t\tats_entries = 0;\n\t\t} else {\n\t\t\tats_entries = entries;\n\t\t\tentries = 0;\n\t\t}\n\t}\n\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (r)\n\t\treturn r;\n\n\tif (vmbo->shadow) {\n\t\tstruct amdgpu_bo *shadow = vmbo->shadow;\n\n\t\tr = ttm_bo_validate(&shadow->tbo, &shadow->placement, &ctx);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn -ENODEV;\n\n\tr = vm->update_funcs->map_table(vmbo);\n\tif (r)\n\t\tgoto exit;\n\n\tmemset(&params, 0, sizeof(params));\n\tparams.adev = adev;\n\tparams.vm = vm;\n\tparams.immediate = immediate;\n\n\tr = vm->update_funcs->prepare(&params, NULL, AMDGPU_SYNC_EXPLICIT);\n\tif (r)\n\t\tgoto exit;\n\n\taddr = 0;\n\tif (ats_entries) {\n\t\tuint64_t value = 0, flags;\n\n\t\tflags = AMDGPU_PTE_DEFAULT_ATC;\n\t\tif (level != AMDGPU_VM_PTB) {\n\t\t\t \n\t\t\tflags |= AMDGPU_PDE_PTE;\n\t\t\tamdgpu_gmc_get_vm_pde(adev, level, &value, &flags);\n\t\t}\n\n\t\tr = vm->update_funcs->update(&params, vmbo, addr, 0,\n\t\t\t\t\t     ats_entries, value, flags);\n\t\tif (r)\n\t\t\tgoto exit;\n\n\t\taddr += ats_entries * 8;\n\t}\n\n\tif (entries) {\n\t\tuint64_t value = 0, flags = 0;\n\n\t\tif (adev->asic_type >= CHIP_VEGA10) {\n\t\t\tif (level != AMDGPU_VM_PTB) {\n\t\t\t\t \n\t\t\t\tflags |= AMDGPU_PDE_PTE;\n\t\t\t\tamdgpu_gmc_get_vm_pde(adev, level,\n\t\t\t\t\t\t      &value, &flags);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tflags = AMDGPU_PTE_EXECUTABLE;\n\t\t\t}\n\t\t}\n\n\t\tr = vm->update_funcs->update(&params, vmbo, addr, 0, entries,\n\t\t\t\t\t     value, flags);\n\t\tif (r)\n\t\t\tgoto exit;\n\t}\n\n\tr = vm->update_funcs->commit(&params, NULL);\nexit:\n\tdrm_dev_exit(idx);\n\treturn r;\n}\n\n \nint amdgpu_vm_pt_create(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\tint level, bool immediate, struct amdgpu_bo_vm **vmbo,\n\t\t\tint32_t xcp_id)\n{\n\tstruct amdgpu_bo_param bp;\n\tstruct amdgpu_bo *bo;\n\tstruct dma_resv *resv;\n\tunsigned int num_entries;\n\tint r;\n\n\tmemset(&bp, 0, sizeof(bp));\n\n\tbp.size = amdgpu_vm_pt_size(adev, level);\n\tbp.byte_align = AMDGPU_GPU_PAGE_SIZE;\n\n\tif (!adev->gmc.is_app_apu)\n\t\tbp.domain = AMDGPU_GEM_DOMAIN_VRAM;\n\telse\n\t\tbp.domain = AMDGPU_GEM_DOMAIN_GTT;\n\n\tbp.domain = amdgpu_bo_get_preferred_domain(adev, bp.domain);\n\tbp.flags = AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS |\n\t\tAMDGPU_GEM_CREATE_CPU_GTT_USWC;\n\n\tif (level < AMDGPU_VM_PTB)\n\t\tnum_entries = amdgpu_vm_pt_num_entries(adev, level);\n\telse\n\t\tnum_entries = 0;\n\n\tbp.bo_ptr_size = struct_size((*vmbo), entries, num_entries);\n\n\tif (vm->use_cpu_for_update)\n\t\tbp.flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\n\tbp.type = ttm_bo_type_kernel;\n\tbp.no_wait_gpu = immediate;\n\tbp.xcp_id_plus1 = xcp_id + 1;\n\n\tif (vm->root.bo)\n\t\tbp.resv = vm->root.bo->tbo.base.resv;\n\n\tr = amdgpu_bo_create_vm(adev, &bp, vmbo);\n\tif (r)\n\t\treturn r;\n\n\tbo = &(*vmbo)->bo;\n\tif (vm->is_compute_context || (adev->flags & AMD_IS_APU)) {\n\t\t(*vmbo)->shadow = NULL;\n\t\treturn 0;\n\t}\n\n\tif (!bp.resv)\n\t\tWARN_ON(dma_resv_lock(bo->tbo.base.resv,\n\t\t\t\t      NULL));\n\tresv = bp.resv;\n\tmemset(&bp, 0, sizeof(bp));\n\tbp.size = amdgpu_vm_pt_size(adev, level);\n\tbp.domain = AMDGPU_GEM_DOMAIN_GTT;\n\tbp.flags = AMDGPU_GEM_CREATE_CPU_GTT_USWC;\n\tbp.type = ttm_bo_type_kernel;\n\tbp.resv = bo->tbo.base.resv;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\tbp.xcp_id_plus1 = xcp_id + 1;\n\n\tr = amdgpu_bo_create(adev, &bp, &(*vmbo)->shadow);\n\n\tif (!resv)\n\t\tdma_resv_unlock(bo->tbo.base.resv);\n\n\tif (r) {\n\t\tamdgpu_bo_unref(&bo);\n\t\treturn r;\n\t}\n\n\tamdgpu_bo_add_to_shadow_list(*vmbo);\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_vm_pt_alloc(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_vm *vm,\n\t\t\t      struct amdgpu_vm_pt_cursor *cursor,\n\t\t\t      bool immediate)\n{\n\tstruct amdgpu_vm_bo_base *entry = cursor->entry;\n\tstruct amdgpu_bo *pt_bo;\n\tstruct amdgpu_bo_vm *pt;\n\tint r;\n\n\tif (entry->bo)\n\t\treturn 0;\n\n\tamdgpu_vm_eviction_unlock(vm);\n\tr = amdgpu_vm_pt_create(adev, vm, cursor->level, immediate, &pt,\n\t\t\t\tvm->root.bo->xcp_id);\n\tamdgpu_vm_eviction_lock(vm);\n\tif (r)\n\t\treturn r;\n\n\t \n\tpt_bo = &pt->bo;\n\tpt_bo->parent = amdgpu_bo_ref(cursor->parent->bo);\n\tamdgpu_vm_bo_base_init(entry, vm, pt_bo);\n\tr = amdgpu_vm_pt_clear(adev, vm, pt, immediate);\n\tif (r)\n\t\tgoto error_free_pt;\n\n\treturn 0;\n\nerror_free_pt:\n\tamdgpu_bo_unref(&pt->shadow);\n\tamdgpu_bo_unref(&pt_bo);\n\treturn r;\n}\n\n \nstatic void amdgpu_vm_pt_free(struct amdgpu_vm_bo_base *entry)\n{\n\tstruct amdgpu_bo *shadow;\n\n\tif (!entry->bo)\n\t\treturn;\n\n\tentry->bo->vm_bo = NULL;\n\tshadow = amdgpu_bo_shadowed(entry->bo);\n\tif (shadow) {\n\t\tttm_bo_set_bulk_move(&shadow->tbo, NULL);\n\t\tamdgpu_bo_unref(&shadow);\n\t}\n\tttm_bo_set_bulk_move(&entry->bo->tbo, NULL);\n\n\tspin_lock(&entry->vm->status_lock);\n\tlist_del(&entry->vm_status);\n\tspin_unlock(&entry->vm->status_lock);\n\tamdgpu_bo_unref(&entry->bo);\n}\n\nvoid amdgpu_vm_pt_free_work(struct work_struct *work)\n{\n\tstruct amdgpu_vm_bo_base *entry, *next;\n\tstruct amdgpu_vm *vm;\n\tLIST_HEAD(pt_freed);\n\n\tvm = container_of(work, struct amdgpu_vm, pt_free_work);\n\n\tspin_lock(&vm->status_lock);\n\tlist_splice_init(&vm->pt_freed, &pt_freed);\n\tspin_unlock(&vm->status_lock);\n\n\t \n\tamdgpu_bo_reserve(vm->root.bo, true);\n\n\tlist_for_each_entry_safe(entry, next, &pt_freed, vm_status)\n\t\tamdgpu_vm_pt_free(entry);\n\n\tamdgpu_bo_unreserve(vm->root.bo);\n}\n\n \nstatic void amdgpu_vm_pt_free_dfs(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_vm *vm,\n\t\t\t\t  struct amdgpu_vm_pt_cursor *start,\n\t\t\t\t  bool unlocked)\n{\n\tstruct amdgpu_vm_pt_cursor cursor;\n\tstruct amdgpu_vm_bo_base *entry;\n\n\tif (unlocked) {\n\t\tspin_lock(&vm->status_lock);\n\t\tfor_each_amdgpu_vm_pt_dfs_safe(adev, vm, start, cursor, entry)\n\t\t\tlist_move(&entry->vm_status, &vm->pt_freed);\n\n\t\tif (start)\n\t\t\tlist_move(&start->entry->vm_status, &vm->pt_freed);\n\t\tspin_unlock(&vm->status_lock);\n\t\tschedule_work(&vm->pt_free_work);\n\t\treturn;\n\t}\n\n\tfor_each_amdgpu_vm_pt_dfs_safe(adev, vm, start, cursor, entry)\n\t\tamdgpu_vm_pt_free(entry);\n\n\tif (start)\n\t\tamdgpu_vm_pt_free(start->entry);\n}\n\n \nvoid amdgpu_vm_pt_free_root(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tamdgpu_vm_pt_free_dfs(adev, vm, NULL, false);\n}\n\n \nbool amdgpu_vm_pt_is_root_clean(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm)\n{\n\tenum amdgpu_vm_level root = adev->vm_manager.root_level;\n\tunsigned int entries = amdgpu_vm_pt_num_entries(adev, root);\n\tunsigned int i = 0;\n\n\tfor (i = 0; i < entries; i++) {\n\t\tif (to_amdgpu_bo_vm(vm->root.bo)->entries[i].bo)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nint amdgpu_vm_pde_update(struct amdgpu_vm_update_params *params,\n\t\t\t struct amdgpu_vm_bo_base *entry)\n{\n\tstruct amdgpu_vm_bo_base *parent = amdgpu_vm_pt_parent(entry);\n\tstruct amdgpu_bo *bo = parent->bo, *pbo;\n\tstruct amdgpu_vm *vm = params->vm;\n\tuint64_t pde, pt, flags;\n\tunsigned int level;\n\n\tfor (level = 0, pbo = bo->parent; pbo; ++level)\n\t\tpbo = pbo->parent;\n\n\tlevel += params->adev->vm_manager.root_level;\n\tamdgpu_gmc_get_pde_for_bo(entry->bo, level, &pt, &flags);\n\tpde = (entry - to_amdgpu_bo_vm(parent->bo)->entries) * 8;\n\treturn vm->update_funcs->update(params, to_amdgpu_bo_vm(bo), pde, pt,\n\t\t\t\t\t1, 0, flags);\n}\n\n \nstatic void amdgpu_vm_pte_update_noretry_flags(struct amdgpu_device *adev,\n\t\t\t\t\t\tuint64_t *flags)\n{\n\t \n\tif ((*flags & AMDGPU_VM_NORETRY_FLAGS) == AMDGPU_VM_NORETRY_FLAGS) {\n\t\t*flags &= ~AMDGPU_VM_NORETRY_FLAGS;\n\t\t*flags |= adev->gmc.noretry_flags;\n\t}\n}\n\n \nstatic void amdgpu_vm_pte_update_flags(struct amdgpu_vm_update_params *params,\n\t\t\t\t       struct amdgpu_bo_vm *pt,\n\t\t\t\t       unsigned int level,\n\t\t\t\t       uint64_t pe, uint64_t addr,\n\t\t\t\t       unsigned int count, uint32_t incr,\n\t\t\t\t       uint64_t flags)\n{\n\tstruct amdgpu_device *adev = params->adev;\n\n\tif (level != AMDGPU_VM_PTB) {\n\t\tflags |= AMDGPU_PDE_PTE;\n\t\tamdgpu_gmc_get_vm_pde(adev, level, &addr, &flags);\n\n\t} else if (adev->asic_type >= CHIP_VEGA10 &&\n\t\t   !(flags & AMDGPU_PTE_VALID) &&\n\t\t   !(flags & AMDGPU_PTE_PRT)) {\n\n\t\t \n\t\tflags |= AMDGPU_PTE_EXECUTABLE;\n\t}\n\n\t \n\tif (level == AMDGPU_VM_PTB)\n\t\tamdgpu_vm_pte_update_noretry_flags(adev, &flags);\n\n\t \n\tif ((flags & AMDGPU_PTE_SYSTEM) && (adev->flags & AMD_IS_APU) &&\n\t    adev->gmc.gmc_funcs->override_vm_pte_flags &&\n\t    num_possible_nodes() > 1) {\n\t\tif (!params->pages_addr)\n\t\t\tamdgpu_gmc_override_vm_pte_flags(adev, params->vm,\n\t\t\t\t\t\t\t addr, &flags);\n\t\telse\n\t\t\tdev_dbg(adev->dev,\n\t\t\t\t\"override_vm_pte_flags skipped: non-contiguous\\n\");\n\t}\n\n\tparams->vm->update_funcs->update(params, pt, pe, addr, count, incr,\n\t\t\t\t\t flags);\n}\n\n \nstatic void amdgpu_vm_pte_fragment(struct amdgpu_vm_update_params *params,\n\t\t\t\t   uint64_t start, uint64_t end, uint64_t flags,\n\t\t\t\t   unsigned int *frag, uint64_t *frag_end)\n{\n\t \n\tunsigned int max_frag;\n\n\tif (params->adev->asic_type < CHIP_VEGA10)\n\t\tmax_frag = params->adev->vm_manager.fragment_size;\n\telse\n\t\tmax_frag = 31;\n\n\t \n\tif (params->pages_addr) {\n\t\t*frag = 0;\n\t\t*frag_end = end;\n\t\treturn;\n\t}\n\n\t \n\t*frag = min_t(unsigned int, ffs(start) - 1, fls64(end - start) - 1);\n\tif (*frag >= max_frag) {\n\t\t*frag = max_frag;\n\t\t*frag_end = end & ~((1ULL << max_frag) - 1);\n\t} else {\n\t\t*frag_end = start + (1 << *frag);\n\t}\n}\n\n \nint amdgpu_vm_ptes_update(struct amdgpu_vm_update_params *params,\n\t\t\t  uint64_t start, uint64_t end,\n\t\t\t  uint64_t dst, uint64_t flags)\n{\n\tstruct amdgpu_device *adev = params->adev;\n\tstruct amdgpu_vm_pt_cursor cursor;\n\tuint64_t frag_start = start, frag_end;\n\tunsigned int frag;\n\tint r;\n\n\t \n\tamdgpu_vm_pte_fragment(params, frag_start, end, flags, &frag,\n\t\t\t       &frag_end);\n\n\t \n\tamdgpu_vm_pt_start(adev, params->vm, start, &cursor);\n\twhile (cursor.pfn < end) {\n\t\tunsigned int shift, parent_shift, mask;\n\t\tuint64_t incr, entry_end, pe_start;\n\t\tstruct amdgpu_bo *pt;\n\n\t\tif (!params->unlocked) {\n\t\t\t \n\t\t\tr = amdgpu_vm_pt_alloc(params->adev, params->vm,\n\t\t\t\t\t       &cursor, params->immediate);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\tshift = amdgpu_vm_pt_level_shift(adev, cursor.level);\n\t\tparent_shift = amdgpu_vm_pt_level_shift(adev, cursor.level - 1);\n\t\tif (params->unlocked) {\n\t\t\t \n\t\t\tif (amdgpu_vm_pt_descendant(adev, &cursor))\n\t\t\t\tcontinue;\n\t\t} else if (adev->asic_type < CHIP_VEGA10 &&\n\t\t\t   (flags & AMDGPU_PTE_VALID)) {\n\t\t\t \n\t\t\tif (cursor.level != AMDGPU_VM_PTB) {\n\t\t\t\tif (!amdgpu_vm_pt_descendant(adev, &cursor))\n\t\t\t\t\treturn -ENOENT;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (frag < shift) {\n\t\t\t \n\t\t\tif (amdgpu_vm_pt_descendant(adev, &cursor))\n\t\t\t\tcontinue;\n\t\t} else if (frag >= parent_shift) {\n\t\t\t \n\t\t\tif (!amdgpu_vm_pt_ancestor(&cursor))\n\t\t\t\treturn -EINVAL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpt = cursor.entry->bo;\n\t\tif (!pt) {\n\t\t\t \n\t\t\tif (flags & AMDGPU_PTE_VALID)\n\t\t\t\treturn -ENOENT;\n\n\t\t\t \n\t\t\tif (!amdgpu_vm_pt_ancestor(&cursor))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tpt = cursor.entry->bo;\n\t\t\tshift = parent_shift;\n\t\t\tfrag_end = max(frag_end, ALIGN(frag_start + 1,\n\t\t\t\t   1ULL << shift));\n\t\t}\n\n\t\t \n\t\tincr = (uint64_t)AMDGPU_GPU_PAGE_SIZE << shift;\n\t\tmask = amdgpu_vm_pt_entries_mask(adev, cursor.level);\n\t\tpe_start = ((cursor.pfn >> shift) & mask) * 8;\n\t\tentry_end = ((uint64_t)mask + 1) << shift;\n\t\tentry_end += cursor.pfn & ~(entry_end - 1);\n\t\tentry_end = min(entry_end, end);\n\n\t\tdo {\n\t\t\tstruct amdgpu_vm *vm = params->vm;\n\t\t\tuint64_t upd_end = min(entry_end, frag_end);\n\t\t\tunsigned int nptes = (upd_end - frag_start) >> shift;\n\t\t\tuint64_t upd_flags = flags | AMDGPU_PTE_FRAG(frag);\n\n\t\t\t \n\t\t\tnptes = max(nptes, 1u);\n\n\t\t\ttrace_amdgpu_vm_update_ptes(params, frag_start, upd_end,\n\t\t\t\t\t\t    min(nptes, 32u), dst, incr,\n\t\t\t\t\t\t    upd_flags,\n\t\t\t\t\t\t    vm->task_info.tgid,\n\t\t\t\t\t\t    vm->immediate.fence_context);\n\t\t\tamdgpu_vm_pte_update_flags(params, to_amdgpu_bo_vm(pt),\n\t\t\t\t\t\t   cursor.level, pe_start, dst,\n\t\t\t\t\t\t   nptes, incr, upd_flags);\n\n\t\t\tpe_start += nptes * 8;\n\t\t\tdst += nptes * incr;\n\n\t\t\tfrag_start = upd_end;\n\t\t\tif (frag_start >= frag_end) {\n\t\t\t\t \n\t\t\t\tamdgpu_vm_pte_fragment(params, frag_start, end,\n\t\t\t\t\t\t       flags, &frag, &frag_end);\n\t\t\t\tif (frag < shift)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} while (frag_start < entry_end);\n\n\t\tif (amdgpu_vm_pt_descendant(adev, &cursor)) {\n\t\t\t \n\t\t\twhile (cursor.pfn < frag_start) {\n\t\t\t\t \n\t\t\t\tif (cursor.entry->bo) {\n\t\t\t\t\tparams->table_freed = true;\n\t\t\t\t\tamdgpu_vm_pt_free_dfs(adev, params->vm,\n\t\t\t\t\t\t\t      &cursor,\n\t\t\t\t\t\t\t      params->unlocked);\n\t\t\t\t}\n\t\t\t\tamdgpu_vm_pt_next(adev, &cursor);\n\t\t\t}\n\n\t\t} else if (frag >= shift) {\n\t\t\t \n\t\t\tamdgpu_vm_pt_next(adev, &cursor);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_vm_pt_map_tables(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_vm_pt_cursor cursor;\n\tstruct amdgpu_vm_bo_base *entry;\n\n\tfor_each_amdgpu_vm_pt_dfs_safe(adev, vm, NULL, cursor, entry) {\n\n\t\tstruct amdgpu_bo_vm *bo;\n\t\tint r;\n\n\t\tif (entry->bo) {\n\t\t\tbo = to_amdgpu_bo_vm(entry->bo);\n\t\t\tr = vm->update_funcs->map_table(bo);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}