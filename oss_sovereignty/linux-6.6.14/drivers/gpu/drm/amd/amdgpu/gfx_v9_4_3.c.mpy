{
  "module_name": "gfx_v9_4_3.c",
  "hash_id": "a120e1aa6afdfec06f717569d1632a4c3a1f4988fa081e73a216a826300467da",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/gfx_v9_4_3.c",
  "human_readable_source": " \n#include <linux/firmware.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_gfx.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"soc15_common.h\"\n#include \"vega10_enum.h\"\n\n#include \"v9_structs.h\"\n\n#include \"ivsrcid/gfx/irqsrcs_gfx_9_0.h\"\n\n#include \"gc/gc_9_4_3_offset.h\"\n#include \"gc/gc_9_4_3_sh_mask.h\"\n\n#include \"gfx_v9_4_3.h\"\n#include \"amdgpu_xcp.h\"\n\nMODULE_FIRMWARE(\"amdgpu/gc_9_4_3_mec.bin\");\nMODULE_FIRMWARE(\"amdgpu/gc_9_4_3_rlc.bin\");\n\n#define GFX9_MEC_HPD_SIZE 4096\n#define RLCG_UCODE_LOADING_START_ADDRESS 0x00002000L\n\n#define GOLDEN_GB_ADDR_CONFIG 0x2a114042\n#define CP_HQD_PERSISTENT_STATE_DEFAULT 0xbe05301\n\nstruct amdgpu_gfx_ras gfx_v9_4_3_ras;\n\nstatic void gfx_v9_4_3_set_ring_funcs(struct amdgpu_device *adev);\nstatic void gfx_v9_4_3_set_irq_funcs(struct amdgpu_device *adev);\nstatic void gfx_v9_4_3_set_gds_init(struct amdgpu_device *adev);\nstatic void gfx_v9_4_3_set_rlc_funcs(struct amdgpu_device *adev);\nstatic int gfx_v9_4_3_get_cu_info(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_cu_info *cu_info);\n\nstatic void gfx_v9_4_3_kiq_set_resources(struct amdgpu_ring *kiq_ring,\n\t\t\t\tuint64_t queue_mask)\n{\n\tamdgpu_ring_write(kiq_ring, PACKET3(PACKET3_SET_RESOURCES, 6));\n\tamdgpu_ring_write(kiq_ring,\n\t\tPACKET3_SET_RESOURCES_VMID_MASK(0) |\n\t\t \n\t\tPACKET3_SET_RESOURCES_QUEUE_TYPE(0));\n\tamdgpu_ring_write(kiq_ring,\n\t\t\tlower_32_bits(queue_mask));\t \n\tamdgpu_ring_write(kiq_ring,\n\t\t\tupper_32_bits(queue_mask));\t \n\tamdgpu_ring_write(kiq_ring, 0);\t \n\tamdgpu_ring_write(kiq_ring, 0);\t \n\tamdgpu_ring_write(kiq_ring, 0);\t \n\tamdgpu_ring_write(kiq_ring, 0);\t \n}\n\nstatic void gfx_v9_4_3_kiq_map_queues(struct amdgpu_ring *kiq_ring,\n\t\t\t\t struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = kiq_ring->adev;\n\tuint64_t mqd_addr = amdgpu_bo_gpu_offset(ring->mqd_obj);\n\tuint64_t wptr_addr = adev->wb.gpu_addr + (ring->wptr_offs * 4);\n\tuint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;\n\n\tamdgpu_ring_write(kiq_ring, PACKET3(PACKET3_MAP_QUEUES, 5));\n\t \n\tamdgpu_ring_write(kiq_ring,  \n\t\t\t PACKET3_MAP_QUEUES_QUEUE_SEL(0) |  \n\t\t\t PACKET3_MAP_QUEUES_VMID(0) |  \n\t\t\t PACKET3_MAP_QUEUES_QUEUE(ring->queue) |\n\t\t\t PACKET3_MAP_QUEUES_PIPE(ring->pipe) |\n\t\t\t PACKET3_MAP_QUEUES_ME((ring->me == 1 ? 0 : 1)) |\n\t\t\t  \n\t\t\t PACKET3_MAP_QUEUES_QUEUE_TYPE(0) |\n\t\t\t  \n\t\t\t PACKET3_MAP_QUEUES_ALLOC_FORMAT(0) |\n\t\t\t PACKET3_MAP_QUEUES_ENGINE_SEL(eng_sel) |\n\t\t\t  \n\t\t\t PACKET3_MAP_QUEUES_NUM_QUEUES(1));\n\tamdgpu_ring_write(kiq_ring,\n\t\t\tPACKET3_MAP_QUEUES_DOORBELL_OFFSET(ring->doorbell_index));\n\tamdgpu_ring_write(kiq_ring, lower_32_bits(mqd_addr));\n\tamdgpu_ring_write(kiq_ring, upper_32_bits(mqd_addr));\n\tamdgpu_ring_write(kiq_ring, lower_32_bits(wptr_addr));\n\tamdgpu_ring_write(kiq_ring, upper_32_bits(wptr_addr));\n}\n\nstatic void gfx_v9_4_3_kiq_unmap_queues(struct amdgpu_ring *kiq_ring,\n\t\t\t\t   struct amdgpu_ring *ring,\n\t\t\t\t   enum amdgpu_unmap_queues_action action,\n\t\t\t\t   u64 gpu_addr, u64 seq)\n{\n\tuint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;\n\n\tamdgpu_ring_write(kiq_ring, PACKET3(PACKET3_UNMAP_QUEUES, 4));\n\tamdgpu_ring_write(kiq_ring,  \n\t\t\t  PACKET3_UNMAP_QUEUES_ACTION(action) |\n\t\t\t  PACKET3_UNMAP_QUEUES_QUEUE_SEL(0) |\n\t\t\t  PACKET3_UNMAP_QUEUES_ENGINE_SEL(eng_sel) |\n\t\t\t  PACKET3_UNMAP_QUEUES_NUM_QUEUES(1));\n\tamdgpu_ring_write(kiq_ring,\n\t\t\tPACKET3_UNMAP_QUEUES_DOORBELL_OFFSET0(ring->doorbell_index));\n\n\tif (action == PREEMPT_QUEUES_NO_UNMAP) {\n\t\tamdgpu_ring_write(kiq_ring, lower_32_bits(gpu_addr));\n\t\tamdgpu_ring_write(kiq_ring, upper_32_bits(gpu_addr));\n\t\tamdgpu_ring_write(kiq_ring, seq);\n\t} else {\n\t\tamdgpu_ring_write(kiq_ring, 0);\n\t\tamdgpu_ring_write(kiq_ring, 0);\n\t\tamdgpu_ring_write(kiq_ring, 0);\n\t}\n}\n\nstatic void gfx_v9_4_3_kiq_query_status(struct amdgpu_ring *kiq_ring,\n\t\t\t\t   struct amdgpu_ring *ring,\n\t\t\t\t   u64 addr,\n\t\t\t\t   u64 seq)\n{\n\tuint32_t eng_sel = ring->funcs->type == AMDGPU_RING_TYPE_GFX ? 4 : 0;\n\n\tamdgpu_ring_write(kiq_ring, PACKET3(PACKET3_QUERY_STATUS, 5));\n\tamdgpu_ring_write(kiq_ring,\n\t\t\t  PACKET3_QUERY_STATUS_CONTEXT_ID(0) |\n\t\t\t  PACKET3_QUERY_STATUS_INTERRUPT_SEL(0) |\n\t\t\t  PACKET3_QUERY_STATUS_COMMAND(2));\n\t \n\tamdgpu_ring_write(kiq_ring,\n\t\t\tPACKET3_QUERY_STATUS_DOORBELL_OFFSET(ring->doorbell_index) |\n\t\t\tPACKET3_QUERY_STATUS_ENG_SEL(eng_sel));\n\tamdgpu_ring_write(kiq_ring, lower_32_bits(addr));\n\tamdgpu_ring_write(kiq_ring, upper_32_bits(addr));\n\tamdgpu_ring_write(kiq_ring, lower_32_bits(seq));\n\tamdgpu_ring_write(kiq_ring, upper_32_bits(seq));\n}\n\nstatic void gfx_v9_4_3_kiq_invalidate_tlbs(struct amdgpu_ring *kiq_ring,\n\t\t\t\tuint16_t pasid, uint32_t flush_type,\n\t\t\t\tbool all_hub)\n{\n\tamdgpu_ring_write(kiq_ring, PACKET3(PACKET3_INVALIDATE_TLBS, 0));\n\tamdgpu_ring_write(kiq_ring,\n\t\t\tPACKET3_INVALIDATE_TLBS_DST_SEL(1) |\n\t\t\tPACKET3_INVALIDATE_TLBS_ALL_HUB(all_hub) |\n\t\t\tPACKET3_INVALIDATE_TLBS_PASID(pasid) |\n\t\t\tPACKET3_INVALIDATE_TLBS_FLUSH_TYPE(flush_type));\n}\n\nstatic const struct kiq_pm4_funcs gfx_v9_4_3_kiq_pm4_funcs = {\n\t.kiq_set_resources = gfx_v9_4_3_kiq_set_resources,\n\t.kiq_map_queues = gfx_v9_4_3_kiq_map_queues,\n\t.kiq_unmap_queues = gfx_v9_4_3_kiq_unmap_queues,\n\t.kiq_query_status = gfx_v9_4_3_kiq_query_status,\n\t.kiq_invalidate_tlbs = gfx_v9_4_3_kiq_invalidate_tlbs,\n\t.set_resources_size = 8,\n\t.map_queues_size = 7,\n\t.unmap_queues_size = 6,\n\t.query_status_size = 7,\n\t.invalidate_tlbs_size = 2,\n};\n\nstatic void gfx_v9_4_3_set_kiq_pm4_funcs(struct amdgpu_device *adev)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++)\n\t\tadev->gfx.kiq[i].pmf = &gfx_v9_4_3_kiq_pm4_funcs;\n}\n\nstatic void gfx_v9_4_3_init_golden_registers(struct amdgpu_device *adev)\n{\n\tint i, num_xcc, dev_inst;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tdev_inst = GET_INST(GC, i);\n\n\t\tWREG32_SOC15(GC, dev_inst, regGB_ADDR_CONFIG,\n\t\t\t     GOLDEN_GB_ADDR_CONFIG);\n\t\t \n\t\tif (adev->rev_id == 0) {\n\t\t\tWREG32_FIELD15_PREREG(GC, dev_inst, TCP_UTCL1_CNTL1,\n\t\t\t\t\t      REDUCE_FIFO_DEPTH_BY_2, 2);\n\t\t} else {\n\t\t\tWREG32_FIELD15_PREREG(GC, dev_inst, TCP_UTCL1_CNTL2,\n\t\t\t\t\t\tSPARE, 0x1);\n\t\t}\n\t}\n}\n\nstatic void gfx_v9_4_3_write_data_to_reg(struct amdgpu_ring *ring, int eng_sel,\n\t\t\t\t       bool wc, uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\n\tamdgpu_ring_write(ring, WRITE_DATA_ENGINE_SEL(eng_sel) |\n\t\t\t\tWRITE_DATA_DST_SEL(0) |\n\t\t\t\t(wc ? WR_CONFIRM : 0));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void gfx_v9_4_3_wait_reg_mem(struct amdgpu_ring *ring, int eng_sel,\n\t\t\t\t  int mem_space, int opt, uint32_t addr0,\n\t\t\t\t  uint32_t addr1, uint32_t ref, uint32_t mask,\n\t\t\t\t  uint32_t inv)\n{\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_WAIT_REG_MEM, 5));\n\tamdgpu_ring_write(ring,\n\t\t\t\t  \n\t\t\t\t (WAIT_REG_MEM_MEM_SPACE(mem_space) |\n\t\t\t\t WAIT_REG_MEM_OPERATION(opt) |  \n\t\t\t\t WAIT_REG_MEM_FUNCTION(3) |   \n\t\t\t\t WAIT_REG_MEM_ENGINE(eng_sel)));\n\n\tif (mem_space)\n\t\tBUG_ON(addr0 & 0x3);  \n\tamdgpu_ring_write(ring, addr0);\n\tamdgpu_ring_write(ring, addr1);\n\tamdgpu_ring_write(ring, ref);\n\tamdgpu_ring_write(ring, mask);\n\tamdgpu_ring_write(ring, inv);  \n}\n\nstatic int gfx_v9_4_3_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tuint32_t scratch_reg0_offset, xcc_offset;\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t tmp = 0;\n\tunsigned i;\n\tint r;\n\n\t \n\txcc_offset = SOC15_REG_OFFSET(GC, 0, regSCRATCH_REG0);\n\tscratch_reg0_offset = SOC15_REG_OFFSET(GC, GET_INST(GC, ring->xcc_id), regSCRATCH_REG0);\n\tWREG32(scratch_reg0_offset, 0xCAFEDEAD);\n\n\tr = amdgpu_ring_alloc(ring, 3);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG, 1));\n\tamdgpu_ring_write(ring, xcc_offset - PACKET3_SET_UCONFIG_REG_START);\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(scratch_reg0_offset);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\treturn r;\n}\n\nstatic int gfx_v9_4_3_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\n\tunsigned index;\n\tuint64_t gpu_addr;\n\tuint32_t tmp;\n\tlong r;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\tadev->wb.wb[index] = cpu_to_le32(0xCAFEDEAD);\n\tmemset(&ib, 0, sizeof(ib));\n\n\tr = amdgpu_ib_get(adev, NULL, 20, AMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err1;\n\n\tib.ptr[0] = PACKET3(PACKET3_WRITE_DATA, 3);\n\tib.ptr[1] = WRITE_DATA_DST_SEL(5) | WR_CONFIRM;\n\tib.ptr[2] = lower_32_bits(gpu_addr);\n\tib.ptr[3] = upper_32_bits(gpu_addr);\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.length_dw = 5;\n\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err2;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err2;\n\t} else if (r < 0) {\n\t\tgoto err2;\n\t}\n\n\ttmp = adev->wb.wb[index];\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr2:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr1:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n\n \nstatic uint64_t gfx_v9_4_3_get_gpu_clock_counter(struct amdgpu_device *adev)\n{\n\tuint64_t clock;\n\n\tmutex_lock(&adev->gfx.gpu_clock_mutex);\n\tWREG32_SOC15(GC, GET_INST(GC, 0), regRLC_CAPTURE_GPU_CLOCK_COUNT, 1);\n\tclock = (uint64_t)RREG32_SOC15(GC, GET_INST(GC, 0), regRLC_GPU_CLOCK_COUNT_LSB) |\n\t\t((uint64_t)RREG32_SOC15(GC, GET_INST(GC, 0), regRLC_GPU_CLOCK_COUNT_MSB) << 32ULL);\n\tmutex_unlock(&adev->gfx.gpu_clock_mutex);\n\n\treturn clock;\n}\n\nstatic void gfx_v9_4_3_free_microcode(struct amdgpu_device *adev)\n{\n\tamdgpu_ucode_release(&adev->gfx.pfp_fw);\n\tamdgpu_ucode_release(&adev->gfx.me_fw);\n\tamdgpu_ucode_release(&adev->gfx.ce_fw);\n\tamdgpu_ucode_release(&adev->gfx.rlc_fw);\n\tamdgpu_ucode_release(&adev->gfx.mec_fw);\n\tamdgpu_ucode_release(&adev->gfx.mec2_fw);\n\n\tkfree(adev->gfx.rlc.register_list_format);\n}\n\nstatic int gfx_v9_4_3_init_rlc_microcode(struct amdgpu_device *adev,\n\t\t\t\t\t  const char *chip_name)\n{\n\tchar fw_name[30];\n\tint err;\n\tconst struct rlc_firmware_header_v2_0 *rlc_hdr;\n\tuint16_t version_major;\n\tuint16_t version_minor;\n\n\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_rlc.bin\", chip_name);\n\n\terr = amdgpu_ucode_request(adev, &adev->gfx.rlc_fw, fw_name);\n\tif (err)\n\t\tgoto out;\n\trlc_hdr = (const struct rlc_firmware_header_v2_0 *)adev->gfx.rlc_fw->data;\n\n\tversion_major = le16_to_cpu(rlc_hdr->header.header_version_major);\n\tversion_minor = le16_to_cpu(rlc_hdr->header.header_version_minor);\n\terr = amdgpu_gfx_rlc_init_microcode(adev, version_major, version_minor);\nout:\n\tif (err)\n\t\tamdgpu_ucode_release(&adev->gfx.rlc_fw);\n\n\treturn err;\n}\n\nstatic bool gfx_v9_4_3_should_disable_gfxoff(struct pci_dev *pdev)\n{\n\treturn true;\n}\n\nstatic void gfx_v9_4_3_check_if_need_gfxoff(struct amdgpu_device *adev)\n{\n\tif (gfx_v9_4_3_should_disable_gfxoff(adev->pdev))\n\t\tadev->pm.pp_feature &= ~PP_GFXOFF_MASK;\n}\n\nstatic int gfx_v9_4_3_init_cp_compute_microcode(struct amdgpu_device *adev,\n\t\t\t\t\t  const char *chip_name)\n{\n\tchar fw_name[30];\n\tint err;\n\n\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mec.bin\", chip_name);\n\n\terr = amdgpu_ucode_request(adev, &adev->gfx.mec_fw, fw_name);\n\tif (err)\n\t\tgoto out;\n\tamdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_MEC1);\n\tamdgpu_gfx_cp_init_microcode(adev, AMDGPU_UCODE_ID_CP_MEC1_JT);\n\n\tadev->gfx.mec2_fw_version = adev->gfx.mec_fw_version;\n\tadev->gfx.mec2_feature_version = adev->gfx.mec_feature_version;\n\n\tgfx_v9_4_3_check_if_need_gfxoff(adev);\n\nout:\n\tif (err)\n\t\tamdgpu_ucode_release(&adev->gfx.mec_fw);\n\treturn err;\n}\n\nstatic int gfx_v9_4_3_init_microcode(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tint r;\n\n\tchip_name = \"gc_9_4_3\";\n\n\tr = gfx_v9_4_3_init_rlc_microcode(adev, chip_name);\n\tif (r)\n\t\treturn r;\n\n\tr = gfx_v9_4_3_init_cp_compute_microcode(adev, chip_name);\n\tif (r)\n\t\treturn r;\n\n\treturn r;\n}\n\nstatic void gfx_v9_4_3_mec_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_bo_free_kernel(&adev->gfx.mec.hpd_eop_obj, NULL, NULL);\n\tamdgpu_bo_free_kernel(&adev->gfx.mec.mec_fw_obj, NULL, NULL);\n}\n\nstatic int gfx_v9_4_3_mec_init(struct amdgpu_device *adev)\n{\n\tint r, i, num_xcc;\n\tu32 *hpd;\n\tconst __le32 *fw_data;\n\tunsigned fw_size;\n\tu32 *fw;\n\tsize_t mec_hpd_size;\n\n\tconst struct gfx_firmware_header_v1_0 *mec_hdr;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++)\n\t\tbitmap_zero(adev->gfx.mec_bitmap[i].queue_bitmap,\n\t\t\tAMDGPU_MAX_COMPUTE_QUEUES);\n\n\t \n\tamdgpu_gfx_compute_queue_acquire(adev);\n\tmec_hpd_size =\n\t\tadev->gfx.num_compute_rings * num_xcc * GFX9_MEC_HPD_SIZE;\n\tif (mec_hpd_size) {\n\t\tr = amdgpu_bo_create_reserved(adev, mec_hpd_size, PAGE_SIZE,\n\t\t\t\t\t      AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t      AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t      &adev->gfx.mec.hpd_eop_obj,\n\t\t\t\t\t      &adev->gfx.mec.hpd_eop_gpu_addr,\n\t\t\t\t\t      (void **)&hpd);\n\t\tif (r) {\n\t\t\tdev_warn(adev->dev, \"(%d) create HDP EOP bo failed\\n\", r);\n\t\t\tgfx_v9_4_3_mec_fini(adev);\n\t\t\treturn r;\n\t\t}\n\n\t\tif (amdgpu_emu_mode == 1) {\n\t\t\tfor (i = 0; i < mec_hpd_size / 4; i++) {\n\t\t\t\tmemset((void *)(hpd + i), 0, 4);\n\t\t\t\tif (i % 50 == 0)\n\t\t\t\t\tmsleep(1);\n\t\t\t}\n\t\t} else {\n\t\t\tmemset(hpd, 0, mec_hpd_size);\n\t\t}\n\n\t\tamdgpu_bo_kunmap(adev->gfx.mec.hpd_eop_obj);\n\t\tamdgpu_bo_unreserve(adev->gfx.mec.hpd_eop_obj);\n\t}\n\n\tmec_hdr = (const struct gfx_firmware_header_v1_0 *)adev->gfx.mec_fw->data;\n\n\tfw_data = (const __le32 *)\n\t\t(adev->gfx.mec_fw->data +\n\t\t le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));\n\tfw_size = le32_to_cpu(mec_hdr->header.ucode_size_bytes);\n\n\tr = amdgpu_bo_create_reserved(adev, mec_hdr->header.ucode_size_bytes,\n\t\t\t\t      PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t      &adev->gfx.mec.mec_fw_obj,\n\t\t\t\t      &adev->gfx.mec.mec_fw_gpu_addr,\n\t\t\t\t      (void **)&fw);\n\tif (r) {\n\t\tdev_warn(adev->dev, \"(%d) create mec firmware bo failed\\n\", r);\n\t\tgfx_v9_4_3_mec_fini(adev);\n\t\treturn r;\n\t}\n\n\tmemcpy(fw, fw_data, fw_size);\n\n\tamdgpu_bo_kunmap(adev->gfx.mec.mec_fw_obj);\n\tamdgpu_bo_unreserve(adev->gfx.mec.mec_fw_obj);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_xcc_select_se_sh(struct amdgpu_device *adev, u32 se_num,\n\t\t\t\t\tu32 sh_num, u32 instance, int xcc_id)\n{\n\tu32 data;\n\n\tif (instance == 0xffffffff)\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(0, GRBM_GFX_INDEX,\n\t\t\t\t     INSTANCE_INDEX, instance);\n\n\tif (se_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX,\n\t\t\t\t     SE_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SE_INDEX, se_num);\n\n\tif (sh_num == 0xffffffff)\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX,\n\t\t\t\t     SH_BROADCAST_WRITES, 1);\n\telse\n\t\tdata = REG_SET_FIELD(data, GRBM_GFX_INDEX, SH_INDEX, sh_num);\n\n\tWREG32_SOC15_RLC_SHADOW_EX(reg, GC, GET_INST(GC, xcc_id), regGRBM_GFX_INDEX, data);\n}\n\nstatic uint32_t wave_read_ind(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd, uint32_t wave, uint32_t address)\n{\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(address << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK));\n\treturn RREG32_SOC15(GC, GET_INST(GC, xcc_id), regSQ_IND_DATA);\n}\n\nstatic void wave_read_regs(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd,\n\t\t\t   uint32_t wave, uint32_t thread,\n\t\t\t   uint32_t regno, uint32_t num, uint32_t *out)\n{\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regSQ_IND_INDEX,\n\t\t(wave << SQ_IND_INDEX__WAVE_ID__SHIFT) |\n\t\t(simd << SQ_IND_INDEX__SIMD_ID__SHIFT) |\n\t\t(regno << SQ_IND_INDEX__INDEX__SHIFT) |\n\t\t(thread << SQ_IND_INDEX__THREAD_ID__SHIFT) |\n\t\t(SQ_IND_INDEX__FORCE_READ_MASK) |\n\t\t(SQ_IND_INDEX__AUTO_INCR_MASK));\n\twhile (num--)\n\t\t*(out++) = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regSQ_IND_DATA);\n}\n\nstatic void gfx_v9_4_3_read_wave_data(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t xcc_id, uint32_t simd, uint32_t wave,\n\t\t\t\t      uint32_t *dst, int *no_fields)\n{\n\t \n\tdst[(*no_fields)++] = 1;\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_STATUS);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_PC_LO);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_PC_HI);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_EXEC_LO);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_EXEC_HI);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_HW_ID);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_INST_DW0);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_INST_DW1);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_GPR_ALLOC);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_LDS_ALLOC);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_TRAPSTS);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_IB_STS);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_IB_DBG0);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_M0);\n\tdst[(*no_fields)++] = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_MODE);\n}\n\nstatic void gfx_v9_4_3_read_wave_sgprs(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd,\n\t\t\t\t       uint32_t wave, uint32_t start,\n\t\t\t\t       uint32_t size, uint32_t *dst)\n{\n\twave_read_regs(adev, xcc_id, simd, wave, 0,\n\t\t       start + SQIND_WAVE_SGPRS_OFFSET, size, dst);\n}\n\nstatic void gfx_v9_4_3_read_wave_vgprs(struct amdgpu_device *adev, uint32_t xcc_id, uint32_t simd,\n\t\t\t\t       uint32_t wave, uint32_t thread,\n\t\t\t\t       uint32_t start, uint32_t size,\n\t\t\t\t       uint32_t *dst)\n{\n\twave_read_regs(adev, xcc_id, simd, wave, thread,\n\t\t       start + SQIND_WAVE_VGPRS_OFFSET, size, dst);\n}\n\nstatic void gfx_v9_4_3_select_me_pipe_q(struct amdgpu_device *adev,\n\t\t\t\t\tu32 me, u32 pipe, u32 q, u32 vm, u32 xcc_id)\n{\n\tsoc15_grbm_select(adev, me, pipe, q, vm, GET_INST(GC, xcc_id));\n}\n\n\nstatic int gfx_v9_4_3_switch_compute_partition(struct amdgpu_device *adev,\n\t\t\t\t\t\tint num_xccs_per_xcp)\n{\n\tint ret, i, num_xcc;\n\tu32 tmp = 0, regval;\n\n\tif (adev->psp.funcs) {\n\t\tret = psp_spatial_partition(&adev->psp,\n\t\t\t\t\t    NUM_XCC(adev->gfx.xcc_mask) /\n\t\t\t\t\t\t    num_xccs_per_xcp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\n\tfor (i = 0; i < num_xcc; i++) {\n\t\ttmp = REG_SET_FIELD(tmp, CP_HYP_XCP_CTL, NUM_XCC_IN_XCP,\n\t\t\t\t    num_xccs_per_xcp);\n\t\ttmp = REG_SET_FIELD(tmp, CP_HYP_XCP_CTL, VIRTUAL_XCC_ID,\n\t\t\t\t    i % num_xccs_per_xcp);\n\t\tregval = RREG32_SOC15(GC, GET_INST(GC, i), regCP_HYP_XCP_CTL);\n\t\tif (regval != tmp)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, i), regCP_HYP_XCP_CTL,\n\t\t\t\t     tmp);\n\t}\n\n\tadev->gfx.num_xcc_per_xcp = num_xccs_per_xcp;\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_ih_to_xcc_inst(struct amdgpu_device *adev, int ih_node)\n{\n\tint xcc;\n\n\txcc = hweight8(adev->gfx.xcc_mask & GENMASK(ih_node / 2, 0));\n\tif (!xcc) {\n\t\tdev_err(adev->dev, \"Couldn't find xcc mapping from IH node\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn xcc - 1;\n}\n\nstatic const struct amdgpu_gfx_funcs gfx_v9_4_3_gfx_funcs = {\n\t.get_gpu_clock_counter = &gfx_v9_4_3_get_gpu_clock_counter,\n\t.select_se_sh = &gfx_v9_4_3_xcc_select_se_sh,\n\t.read_wave_data = &gfx_v9_4_3_read_wave_data,\n\t.read_wave_sgprs = &gfx_v9_4_3_read_wave_sgprs,\n\t.read_wave_vgprs = &gfx_v9_4_3_read_wave_vgprs,\n\t.select_me_pipe_q = &gfx_v9_4_3_select_me_pipe_q,\n\t.switch_partition_mode = &gfx_v9_4_3_switch_compute_partition,\n\t.ih_node_to_logical_xcc = &gfx_v9_4_3_ih_to_xcc_inst,\n};\n\nstatic int gfx_v9_4_3_gpu_early_init(struct amdgpu_device *adev)\n{\n\tu32 gb_addr_config;\n\n\tadev->gfx.funcs = &gfx_v9_4_3_gfx_funcs;\n\tadev->gfx.ras = &gfx_v9_4_3_ras;\n\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 3):\n\t\tadev->gfx.config.max_hw_contexts = 8;\n\t\tadev->gfx.config.sc_prim_fifo_size_frontend = 0x20;\n\t\tadev->gfx.config.sc_prim_fifo_size_backend = 0x100;\n\t\tadev->gfx.config.sc_hiz_tile_fifo_size = 0x30;\n\t\tadev->gfx.config.sc_earlyz_tile_fifo_size = 0x4C0;\n\t\tgb_addr_config = RREG32_SOC15(GC, GET_INST(GC, 0), regGB_ADDR_CONFIG);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tadev->gfx.config.gb_addr_config = gb_addr_config;\n\n\tadev->gfx.config.gb_addr_config_fields.num_pipes = 1 <<\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tNUM_PIPES);\n\n\tadev->gfx.config.max_tile_pipes =\n\t\tadev->gfx.config.gb_addr_config_fields.num_pipes;\n\n\tadev->gfx.config.gb_addr_config_fields.num_banks = 1 <<\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tNUM_BANKS);\n\tadev->gfx.config.gb_addr_config_fields.max_compress_frags = 1 <<\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tMAX_COMPRESSED_FRAGS);\n\tadev->gfx.config.gb_addr_config_fields.num_rb_per_se = 1 <<\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tNUM_RB_PER_SE);\n\tadev->gfx.config.gb_addr_config_fields.num_se = 1 <<\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tNUM_SHADER_ENGINES);\n\tadev->gfx.config.gb_addr_config_fields.pipe_interleave_size = 1 << (8 +\n\t\t\tREG_GET_FIELD(\n\t\t\t\t\tadev->gfx.config.gb_addr_config,\n\t\t\t\t\tGB_ADDR_CONFIG,\n\t\t\t\t\tPIPE_INTERLEAVE_SIZE));\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_compute_ring_init(struct amdgpu_device *adev, int ring_id,\n\t\t\t\t        int xcc_id, int mec, int pipe, int queue)\n{\n\tunsigned irq_type;\n\tstruct amdgpu_ring *ring = &adev->gfx.compute_ring[ring_id];\n\tunsigned int hw_prio;\n\tuint32_t xcc_doorbell_start;\n\n\tring = &adev->gfx.compute_ring[xcc_id * adev->gfx.num_compute_rings +\n\t\t\t\t       ring_id];\n\n\t \n\tring->xcc_id = xcc_id;\n\tring->me = mec + 1;\n\tring->pipe = pipe;\n\tring->queue = queue;\n\n\tring->ring_obj = NULL;\n\tring->use_doorbell = true;\n\txcc_doorbell_start = adev->doorbell_index.mec_ring0 +\n\t\t\t     xcc_id * adev->doorbell_index.xcc_doorbell_range;\n\tring->doorbell_index = (xcc_doorbell_start + ring_id) << 1;\n\tring->eop_gpu_addr = adev->gfx.mec.hpd_eop_gpu_addr +\n\t\t\t     (ring_id + xcc_id * adev->gfx.num_compute_rings) *\n\t\t\t\t     GFX9_MEC_HPD_SIZE;\n\tring->vm_hub = AMDGPU_GFXHUB(xcc_id);\n\tsprintf(ring->name, \"comp_%d.%d.%d.%d\",\n\t\t\tring->xcc_id, ring->me, ring->pipe, ring->queue);\n\n\tirq_type = AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP\n\t\t+ ((ring->me - 1) * adev->gfx.mec.num_pipe_per_mec)\n\t\t+ ring->pipe;\n\thw_prio = amdgpu_gfx_is_high_priority_compute_queue(adev, ring) ?\n\t\t\tAMDGPU_GFX_PIPE_PRIO_HIGH : AMDGPU_GFX_PIPE_PRIO_NORMAL;\n\t \n\treturn amdgpu_ring_init(adev, ring, 1024, &adev->gfx.eop_irq, irq_type,\n\t\t\t\thw_prio, NULL);\n}\n\nstatic int gfx_v9_4_3_sw_init(void *handle)\n{\n\tint i, j, k, r, ring_id, xcc_id, num_xcc;\n\tstruct amdgpu_kiq *kiq;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->gfx.mec.num_mec = 2;\n\tadev->gfx.mec.num_pipe_per_mec = 4;\n\tadev->gfx.mec.num_queue_per_pipe = 8;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_EOP_INTERRUPT, &adev->gfx.eop_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_REG_FAULT,\n\t\t\t      &adev->gfx.priv_reg_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_GRBM_CP, GFX_9_0__SRCID__CP_PRIV_INSTR_FAULT,\n\t\t\t      &adev->gfx.priv_inst_irq);\n\tif (r)\n\t\treturn r;\n\n\tadev->gfx.gfx_current_status = AMDGPU_GFX_NORMAL_MODE;\n\n\tr = adev->gfx.rlc.funcs->init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to init rlc BOs!\\n\");\n\t\treturn r;\n\t}\n\n\tr = gfx_v9_4_3_mec_init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to init MEC BOs!\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tfor (xcc_id = 0; xcc_id < num_xcc; xcc_id++) {\n\t\tring_id = 0;\n\t\tfor (i = 0; i < adev->gfx.mec.num_mec; ++i) {\n\t\t\tfor (j = 0; j < adev->gfx.mec.num_queue_per_pipe; j++) {\n\t\t\t\tfor (k = 0; k < adev->gfx.mec.num_pipe_per_mec;\n\t\t\t\t     k++) {\n\t\t\t\t\tif (!amdgpu_gfx_is_mec_queue_enabled(\n\t\t\t\t\t\t\tadev, xcc_id, i, k, j))\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\tr = gfx_v9_4_3_compute_ring_init(adev,\n\t\t\t\t\t\t\t\t       ring_id,\n\t\t\t\t\t\t\t\t       xcc_id,\n\t\t\t\t\t\t\t\t       i, k, j);\n\t\t\t\t\tif (r)\n\t\t\t\t\t\treturn r;\n\n\t\t\t\t\tring_id++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tr = amdgpu_gfx_kiq_init(adev, GFX9_MEC_HPD_SIZE, xcc_id);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Failed to init KIQ BOs!\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tkiq = &adev->gfx.kiq[xcc_id];\n\t\tr = amdgpu_gfx_kiq_init_ring(adev, &kiq->ring, &kiq->irq, xcc_id);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tr = amdgpu_gfx_mqd_sw_init(adev,\n\t\t\t\tsizeof(struct v9_mqd_allocation), xcc_id);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = gfx_v9_4_3_gpu_early_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_gfx_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\tr = amdgpu_gfx_sysfs_init(adev);\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_3_sw_fini(void *handle)\n{\n\tint i, num_xcc;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < adev->gfx.num_compute_rings * num_xcc; i++)\n\t\tamdgpu_ring_fini(&adev->gfx.compute_ring[i]);\n\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tamdgpu_gfx_mqd_sw_fini(adev, i);\n\t\tamdgpu_gfx_kiq_free_ring(&adev->gfx.kiq[i].ring);\n\t\tamdgpu_gfx_kiq_fini(adev, i);\n\t}\n\n\tgfx_v9_4_3_mec_fini(adev);\n\tamdgpu_bo_unref(&adev->gfx.rlc.clear_state_obj);\n\tgfx_v9_4_3_free_microcode(adev);\n\tif (!amdgpu_sriov_vf(adev))\n\t\tamdgpu_gfx_sysfs_fini(adev);\n\n\treturn 0;\n}\n\n#define DEFAULT_SH_MEM_BASES\t(0x6000)\nstatic void gfx_v9_4_3_xcc_init_compute_vmid(struct amdgpu_device *adev,\n\t\t\t\t\t     int xcc_id)\n{\n\tint i;\n\tuint32_t sh_mem_config;\n\tuint32_t sh_mem_bases;\n\tuint32_t data;\n\n\t \n\tsh_mem_bases = DEFAULT_SH_MEM_BASES | (DEFAULT_SH_MEM_BASES << 16);\n\n\tsh_mem_config = SH_MEM_ADDRESS_MODE_64 |\n\t\t\tSH_MEM_ALIGNMENT_MODE_UNALIGNED <<\n\t\t\tSH_MEM_CONFIG__ALIGNMENT_MODE__SHIFT;\n\n\tmutex_lock(&adev->srbm_mutex);\n\tfor (i = adev->vm_manager.first_kfd_vmid; i < AMDGPU_NUM_VMID; i++) {\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, GET_INST(GC, xcc_id));\n\t\t \n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regSH_MEM_CONFIG, sh_mem_config);\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regSH_MEM_BASES, sh_mem_bases);\n\n\t\t \n\t\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regSPI_GDBG_PER_VMID_CNTL);\n\t\tdata = REG_SET_FIELD(data, SPI_GDBG_PER_VMID_CNTL, TRAP_EN, 1);\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regSPI_GDBG_PER_VMID_CNTL, data);\n\t}\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\tmutex_unlock(&adev->srbm_mutex);\n\n\t \n\tfor (i = adev->vm_manager.first_kfd_vmid; i < AMDGPU_NUM_VMID; i++) {\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_VMID0_BASE, 2 * i, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_VMID0_SIZE, 2 * i, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_GWS_VMID0, i, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_OA_VMID0, i, 0);\n\t}\n}\n\nstatic void gfx_v9_4_3_xcc_init_gds_vmid(struct amdgpu_device *adev, int xcc_id)\n{\n\tint vmid;\n\n\t \n\tfor (vmid = 1; vmid < AMDGPU_NUM_VMID; vmid++) {\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_VMID0_BASE, 2 * vmid, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_VMID0_SIZE, 2 * vmid, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_GWS_VMID0, vmid, 0);\n\t\tWREG32_SOC15_OFFSET(GC, GET_INST(GC, xcc_id), regGDS_OA_VMID0, vmid, 0);\n\t}\n}\n\nstatic void gfx_v9_4_3_xcc_constants_init(struct amdgpu_device *adev,\n\t\t\t\t\t  int xcc_id)\n{\n\tu32 tmp;\n\tint i;\n\n\t \n\t \n\tmutex_lock(&adev->srbm_mutex);\n\tfor (i = 0; i < adev->vm_manager.id_mgr[AMDGPU_GFXHUB(0)].num_ids; i++) {\n\t\tsoc15_grbm_select(adev, 0, 0, 0, i, GET_INST(GC, xcc_id));\n\t\t \n\t\tif (i == 0) {\n\t\t\ttmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,\n\t\t\t\t\t    SH_MEM_ALIGNMENT_MODE_UNALIGNED);\n\t\t\ttmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,\n\t\t\t\t\t    !!adev->gmc.noretry);\n\t\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t regSH_MEM_CONFIG, tmp);\n\t\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t regSH_MEM_BASES, 0);\n\t\t} else {\n\t\t\ttmp = REG_SET_FIELD(0, SH_MEM_CONFIG, ALIGNMENT_MODE,\n\t\t\t\t\t    SH_MEM_ALIGNMENT_MODE_UNALIGNED);\n\t\t\ttmp = REG_SET_FIELD(tmp, SH_MEM_CONFIG, RETRY_DISABLE,\n\t\t\t\t\t    !!adev->gmc.noretry);\n\t\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t regSH_MEM_CONFIG, tmp);\n\t\t\ttmp = REG_SET_FIELD(0, SH_MEM_BASES, PRIVATE_BASE,\n\t\t\t\t\t    (adev->gmc.private_aperture_start >>\n\t\t\t\t\t     48));\n\t\t\ttmp = REG_SET_FIELD(tmp, SH_MEM_BASES, SHARED_BASE,\n\t\t\t\t\t    (adev->gmc.shared_aperture_start >>\n\t\t\t\t\t     48));\n\t\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t regSH_MEM_BASES, tmp);\n\t\t}\n\t}\n\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, 0));\n\n\tmutex_unlock(&adev->srbm_mutex);\n\n\tgfx_v9_4_3_xcc_init_compute_vmid(adev, xcc_id);\n\tgfx_v9_4_3_xcc_init_gds_vmid(adev, xcc_id);\n}\n\nstatic void gfx_v9_4_3_constants_init(struct amdgpu_device *adev)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\n\tgfx_v9_4_3_get_cu_info(adev, &adev->gfx.cu_info);\n\tadev->gfx.config.db_debug2 =\n\t\tRREG32_SOC15(GC, GET_INST(GC, 0), regDB_DEBUG2);\n\n\tfor (i = 0; i < num_xcc; i++)\n\t\tgfx_v9_4_3_xcc_constants_init(adev, i);\n}\n\nstatic void\ngfx_v9_4_3_xcc_enable_save_restore_machine(struct amdgpu_device *adev,\n\t\t\t\t\t   int xcc_id)\n{\n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), RLC_SRM_CNTL, SRM_ENABLE, 1);\n}\n\nstatic void gfx_v9_4_3_xcc_init_pg(struct amdgpu_device *adev, int xcc_id)\n{\n\t \n\tif (adev->gfx.rlc.is_rlc_v2_1)\n\t\tgfx_v9_4_3_xcc_enable_save_restore_machine(adev, xcc_id);\n}\n\nstatic void gfx_v9_4_3_xcc_disable_gpa_mode(struct amdgpu_device *adev, int xcc_id)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCPC_PSP_DEBUG);\n\tdata |= CPC_PSP_DEBUG__UTCL2IUGPAOVERRIDE_MASK;\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCPC_PSP_DEBUG, data);\n}\n\nstatic bool gfx_v9_4_3_is_rlc_enabled(struct amdgpu_device *adev)\n{\n\tuint32_t rlc_setting;\n\n\t \n\trlc_setting = RREG32_SOC15(GC, GET_INST(GC, 0), regRLC_CNTL);\n\tif (!(rlc_setting & RLC_CNTL__RLC_ENABLE_F32_MASK))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void gfx_v9_4_3_xcc_set_safe_mode(struct amdgpu_device *adev, int xcc_id)\n{\n\tuint32_t data;\n\tunsigned i;\n\n\tdata = RLC_SAFE_MODE__CMD_MASK;\n\tdata |= (1 << RLC_SAFE_MODE__MESSAGE__SHIFT);\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_SAFE_MODE, data);\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (!REG_GET_FIELD(RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_SAFE_MODE), RLC_SAFE_MODE, CMD))\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic void gfx_v9_4_3_xcc_unset_safe_mode(struct amdgpu_device *adev,\n\t\t\t\t\t   int xcc_id)\n{\n\tuint32_t data;\n\n\tdata = RLC_SAFE_MODE__CMD_MASK;\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_SAFE_MODE, data);\n}\n\nstatic void gfx_v9_4_3_init_rlcg_reg_access_ctrl(struct amdgpu_device *adev)\n{\n\tint xcc_id, num_xcc;\n\tstruct amdgpu_rlcg_reg_access_ctrl *reg_access_ctrl;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (xcc_id = 0; xcc_id < num_xcc; xcc_id++) {\n\t\treg_access_ctrl = &adev->gfx.rlc.reg_access_ctrl[GET_INST(GC, xcc_id)];\n\t\treg_access_ctrl->scratch_reg0 = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regSCRATCH_REG0);\n\t\treg_access_ctrl->scratch_reg1 = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regSCRATCH_REG1);\n\t\treg_access_ctrl->scratch_reg2 = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regSCRATCH_REG2);\n\t\treg_access_ctrl->scratch_reg3 = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regSCRATCH_REG3);\n\t\treg_access_ctrl->grbm_cntl = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regGRBM_GFX_CNTL);\n\t\treg_access_ctrl->grbm_idx = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regGRBM_GFX_INDEX);\n\t\treg_access_ctrl->spare_int = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regRLC_SPARE_INT);\n\t}\n}\n\nstatic int gfx_v9_4_3_rlc_init(struct amdgpu_device *adev)\n{\n\t \n\tif (adev->gfx.rlc.funcs->update_spm_vmid)\n\t\tadev->gfx.rlc.funcs->update_spm_vmid(adev, 0xf);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_xcc_wait_for_rlc_serdes(struct amdgpu_device *adev,\n\t\t\t\t\t       int xcc_id)\n{\n\tu32 i, j, k;\n\tu32 mask;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tfor (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {\n\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, i, j, 0xffffffff,\n\t\t\t\t\t\t    xcc_id);\n\t\t\tfor (k = 0; k < adev->usec_timeout; k++) {\n\t\t\t\tif (RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_SERDES_CU_MASTER_BUSY) == 0)\n\t\t\t\t\tbreak;\n\t\t\t\tudelay(1);\n\t\t\t}\n\t\t\tif (k == adev->usec_timeout) {\n\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff,\n\t\t\t\t\t\t\t    0xffffffff,\n\t\t\t\t\t\t\t    0xffffffff, xcc_id);\n\t\t\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\t\t\tDRM_INFO(\"Timeout wait for RLC serdes %u,%u\\n\",\n\t\t\t\t\t i, j);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\t\t    xcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\tmask = RLC_SERDES_NONCU_MASTER_BUSY__SE_MASTER_BUSY_MASK |\n\t\tRLC_SERDES_NONCU_MASTER_BUSY__GC_MASTER_BUSY_MASK |\n\t\tRLC_SERDES_NONCU_MASTER_BUSY__TC0_MASTER_BUSY_MASK |\n\t\tRLC_SERDES_NONCU_MASTER_BUSY__TC1_MASTER_BUSY_MASK;\n\tfor (k = 0; k < adev->usec_timeout; k++) {\n\t\tif ((RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_SERDES_NONCU_MASTER_BUSY) & mask) == 0)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n}\n\nstatic void gfx_v9_4_3_xcc_enable_gui_idle_interrupt(struct amdgpu_device *adev,\n\t\t\t\t\t\t     bool enable, int xcc_id)\n{\n\tu32 tmp;\n\n\t \n\n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_INT_CNTL_RING0);\n\n\ttmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_BUSY_INT_ENABLE, enable ? 1 : 0);\n\ttmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CNTX_EMPTY_INT_ENABLE, enable ? 1 : 0);\n\ttmp = REG_SET_FIELD(tmp, CP_INT_CNTL_RING0, CMP_BUSY_INT_ENABLE, enable ? 1 : 0);\n\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_INT_CNTL_RING0, tmp);\n}\n\nstatic void gfx_v9_4_3_xcc_rlc_stop(struct amdgpu_device *adev, int xcc_id)\n{\n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), RLC_CNTL,\n\t\t\t      RLC_ENABLE_F32, 0);\n\tgfx_v9_4_3_xcc_enable_gui_idle_interrupt(adev, false, xcc_id);\n\tgfx_v9_4_3_xcc_wait_for_rlc_serdes(adev, xcc_id);\n}\n\nstatic void gfx_v9_4_3_rlc_stop(struct amdgpu_device *adev)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++)\n\t\tgfx_v9_4_3_xcc_rlc_stop(adev, i);\n}\n\nstatic void gfx_v9_4_3_xcc_rlc_reset(struct amdgpu_device *adev, int xcc_id)\n{\n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), GRBM_SOFT_RESET,\n\t\t\t      SOFT_RESET_RLC, 1);\n\tudelay(50);\n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), GRBM_SOFT_RESET,\n\t\t\t      SOFT_RESET_RLC, 0);\n\tudelay(50);\n}\n\nstatic void gfx_v9_4_3_rlc_reset(struct amdgpu_device *adev)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++)\n\t\tgfx_v9_4_3_xcc_rlc_reset(adev, i);\n}\n\nstatic void gfx_v9_4_3_xcc_rlc_start(struct amdgpu_device *adev, int xcc_id)\n{\n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), RLC_CNTL,\n\t\t\t      RLC_ENABLE_F32, 1);\n\tudelay(50);\n\n\t \n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tgfx_v9_4_3_xcc_enable_gui_idle_interrupt(adev, true, xcc_id);\n\t\tudelay(50);\n\t}\n}\n\nstatic void gfx_v9_4_3_rlc_start(struct amdgpu_device *adev)\n{\n#ifdef AMDGPU_RLC_DEBUG_RETRY\n\tu32 rlc_ucode_ver;\n#endif\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tgfx_v9_4_3_xcc_rlc_start(adev, i);\n#ifdef AMDGPU_RLC_DEBUG_RETRY\n\t\t \n\t\trlc_ucode_ver = RREG32_SOC15(GC, GET_INST(GC, i), regRLC_GPM_GENERAL_6);\n\t\tif (rlc_ucode_ver == 0x108) {\n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"Using rlc debug ucode. regRLC_GPM_GENERAL_6 ==0x08%x / fw_ver == %i \\n\",\n\t\t\t\t rlc_ucode_ver, adev->gfx.rlc_fw_version);\n\t\t\t \n\t\t\tWREG32_SOC15(GC, GET_INST(GC, i), regRLC_GPM_TIMER_INT_3, 0x9C4);\n\t\t\t \n\t\t\tWREG32_SOC15(GC, GET_INST(GC, i), regRLC_GPM_GENERAL_12, 0x100);\n\t\t}\n#endif\n\t}\n}\n\nstatic int gfx_v9_4_3_xcc_rlc_load_microcode(struct amdgpu_device *adev,\n\t\t\t\t\t     int xcc_id)\n{\n\tconst struct rlc_firmware_header_v2_0 *hdr;\n\tconst __le32 *fw_data;\n\tunsigned i, fw_size;\n\n\tif (!adev->gfx.rlc_fw)\n\t\treturn -EINVAL;\n\n\thdr = (const struct rlc_firmware_header_v2_0 *)adev->gfx.rlc_fw->data;\n\tamdgpu_ucode_print_rlc_hdr(&hdr->header);\n\n\tfw_data = (const __le32 *)(adev->gfx.rlc_fw->data +\n\t\t\t   le32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\tfw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_GPM_UCODE_ADDR,\n\t\t\tRLCG_UCODE_LOADING_START_ADDRESS);\n\tfor (i = 0; i < fw_size; i++) {\n\t\tif (amdgpu_emu_mode == 1 && i % 100 == 0) {\n\t\t\tdev_info(adev->dev, \"Write RLC ucode data %u DWs\\n\", i);\n\t\t\tmsleep(1);\n\t\t}\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_GPM_UCODE_DATA, le32_to_cpup(fw_data++));\n\t}\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_GPM_UCODE_ADDR, adev->gfx.rlc_fw_version);\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_rlc_resume(struct amdgpu_device *adev, int xcc_id)\n{\n\tint r;\n\n\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\tgfx_v9_4_3_xcc_rlc_stop(adev, xcc_id);\n\t\t \n\t\tr = gfx_v9_4_3_xcc_rlc_load_microcode(adev, xcc_id);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgfx_v9_4_3_xcc_rlc_start(adev, xcc_id);\n\t}\n\n\tamdgpu_gfx_rlc_enter_safe_mode(adev, xcc_id);\n\t \n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGCG_CGLS_CTRL, 0);\n\tgfx_v9_4_3_xcc_init_pg(adev, xcc_id);\n\tamdgpu_gfx_rlc_exit_safe_mode(adev, xcc_id);\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_rlc_resume(struct amdgpu_device *adev)\n{\n\tint r, i, num_xcc;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tr = gfx_v9_4_3_xcc_rlc_resume(adev, i);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_update_spm_vmid(struct amdgpu_device *adev,\n\t\t\t\t       unsigned vmid)\n{\n\tu32 reg, data;\n\n\treg = SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regRLC_SPM_MC_CNTL);\n\tif (amdgpu_sriov_is_pp_one_vf(adev))\n\t\tdata = RREG32_NO_KIQ(reg);\n\telse\n\t\tdata = RREG32(reg);\n\n\tdata &= ~RLC_SPM_MC_CNTL__RLC_SPM_VMID_MASK;\n\tdata |= (vmid & RLC_SPM_MC_CNTL__RLC_SPM_VMID_MASK) << RLC_SPM_MC_CNTL__RLC_SPM_VMID__SHIFT;\n\n\tif (amdgpu_sriov_is_pp_one_vf(adev))\n\t\tWREG32_SOC15_NO_KIQ(GC, GET_INST(GC, 0), regRLC_SPM_MC_CNTL, data);\n\telse\n\t\tWREG32_SOC15(GC, GET_INST(GC, 0), regRLC_SPM_MC_CNTL, data);\n}\n\nstatic const struct soc15_reg_rlcg rlcg_access_gc_9_4_3[] = {\n\t{SOC15_REG_ENTRY(GC, 0, regGRBM_GFX_INDEX)},\n\t{SOC15_REG_ENTRY(GC, 0, regSQ_IND_INDEX)},\n};\n\nstatic bool gfx_v9_4_3_check_rlcg_range(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t offset,\n\t\t\t\t\tstruct soc15_reg_rlcg *entries, int arr_size)\n{\n\tint i, inst;\n\tuint32_t reg;\n\n\tif (!entries)\n\t\treturn false;\n\n\tfor (i = 0; i < arr_size; i++) {\n\t\tconst struct soc15_reg_rlcg *entry;\n\n\t\tentry = &entries[i];\n\t\tinst = adev->ip_map.logical_to_dev_inst ?\n\t\t\t       adev->ip_map.logical_to_dev_inst(\n\t\t\t\t       adev, entry->hwip, entry->instance) :\n\t\t\t       entry->instance;\n\t\treg = adev->reg_offset[entry->hwip][inst][entry->segment] +\n\t\t      entry->reg;\n\t\tif (offset == reg)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool gfx_v9_4_3_is_rlcg_access_range(struct amdgpu_device *adev, u32 offset)\n{\n\treturn gfx_v9_4_3_check_rlcg_range(adev, offset,\n\t\t\t\t\t(void *)rlcg_access_gc_9_4_3,\n\t\t\t\t\tARRAY_SIZE(rlcg_access_gc_9_4_3));\n}\n\nstatic void gfx_v9_4_3_xcc_cp_compute_enable(struct amdgpu_device *adev,\n\t\t\t\t\t     bool enable, int xcc_id)\n{\n\tif (enable) {\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_MEC_CNTL, 0);\n\t} else {\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_MEC_CNTL,\n\t\t\t(CP_MEC_CNTL__MEC_ME1_HALT_MASK | CP_MEC_CNTL__MEC_ME2_HALT_MASK));\n\t\tadev->gfx.kiq[xcc_id].ring.sched.ready = false;\n\t}\n\tudelay(50);\n}\n\nstatic int gfx_v9_4_3_xcc_cp_compute_load_microcode(struct amdgpu_device *adev,\n\t\t\t\t\t\t    int xcc_id)\n{\n\tconst struct gfx_firmware_header_v1_0 *mec_hdr;\n\tconst __le32 *fw_data;\n\tunsigned i;\n\tu32 tmp;\n\tu32 mec_ucode_addr_offset;\n\tu32 mec_ucode_data_offset;\n\n\tif (!adev->gfx.mec_fw)\n\t\treturn -EINVAL;\n\n\tgfx_v9_4_3_xcc_cp_compute_enable(adev, false, xcc_id);\n\n\tmec_hdr = (const struct gfx_firmware_header_v1_0 *)adev->gfx.mec_fw->data;\n\tamdgpu_ucode_print_gfx_hdr(&mec_hdr->header);\n\n\tfw_data = (const __le32 *)\n\t\t(adev->gfx.mec_fw->data +\n\t\t le32_to_cpu(mec_hdr->header.ucode_array_offset_bytes));\n\ttmp = 0;\n\ttmp = REG_SET_FIELD(tmp, CP_CPC_IC_BASE_CNTL, VMID, 0);\n\ttmp = REG_SET_FIELD(tmp, CP_CPC_IC_BASE_CNTL, CACHE_POLICY, 0);\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_CPC_IC_BASE_CNTL, tmp);\n\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_CPC_IC_BASE_LO,\n\t\tadev->gfx.mec.mec_fw_gpu_addr & 0xFFFFF000);\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_CPC_IC_BASE_HI,\n\t\tupper_32_bits(adev->gfx.mec.mec_fw_gpu_addr));\n\n\tmec_ucode_addr_offset =\n\t\tSOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_MEC_ME1_UCODE_ADDR);\n\tmec_ucode_data_offset =\n\t\tSOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_MEC_ME1_UCODE_DATA);\n\n\t \n\tWREG32(mec_ucode_addr_offset, mec_hdr->jt_offset);\n\tfor (i = 0; i < mec_hdr->jt_size; i++)\n\t\tWREG32(mec_ucode_data_offset,\n\t\t       le32_to_cpup(fw_data + mec_hdr->jt_offset + i));\n\n\tWREG32(mec_ucode_addr_offset, adev->gfx.mec_fw_version);\n\t \n\n\treturn 0;\n}\n\n \nstatic void gfx_v9_4_3_xcc_kiq_setting(struct amdgpu_ring *ring, int xcc_id)\n{\n\tuint32_t tmp;\n\tstruct amdgpu_device *adev = ring->adev;\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CP_SCHEDULERS);\n\ttmp &= 0xffffff00;\n\ttmp |= (ring->me << 5) | (ring->pipe << 3) | (ring->queue);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regRLC_CP_SCHEDULERS, tmp);\n\ttmp |= 0x80;\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regRLC_CP_SCHEDULERS, tmp);\n}\n\nstatic void gfx_v9_4_3_mqd_set_priority(struct amdgpu_ring *ring, struct v9_mqd *mqd)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->funcs->type == AMDGPU_RING_TYPE_COMPUTE) {\n\t\tif (amdgpu_gfx_is_high_priority_compute_queue(adev, ring)) {\n\t\t\tmqd->cp_hqd_pipe_priority = AMDGPU_GFX_PIPE_PRIO_HIGH;\n\t\t\tmqd->cp_hqd_queue_priority =\n\t\t\t\tAMDGPU_GFX_QUEUE_PRIORITY_MAXIMUM;\n\t\t}\n\t}\n}\n\nstatic int gfx_v9_4_3_xcc_mqd_init(struct amdgpu_ring *ring, int xcc_id)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct v9_mqd *mqd = ring->mqd_ptr;\n\tuint64_t hqd_gpu_addr, wb_gpu_addr, eop_base_addr;\n\tuint32_t tmp;\n\n\tmqd->header = 0xC0310800;\n\tmqd->compute_pipelinestat_enable = 0x00000001;\n\tmqd->compute_static_thread_mgmt_se0 = 0xffffffff;\n\tmqd->compute_static_thread_mgmt_se1 = 0xffffffff;\n\tmqd->compute_static_thread_mgmt_se2 = 0xffffffff;\n\tmqd->compute_static_thread_mgmt_se3 = 0xffffffff;\n\tmqd->compute_misc_reserved = 0x00000003;\n\n\tmqd->dynamic_cu_mask_addr_lo =\n\t\tlower_32_bits(ring->mqd_gpu_addr\n\t\t\t      + offsetof(struct v9_mqd_allocation, dynamic_cu_mask));\n\tmqd->dynamic_cu_mask_addr_hi =\n\t\tupper_32_bits(ring->mqd_gpu_addr\n\t\t\t      + offsetof(struct v9_mqd_allocation, dynamic_cu_mask));\n\n\teop_base_addr = ring->eop_gpu_addr >> 8;\n\tmqd->cp_hqd_eop_base_addr_lo = eop_base_addr;\n\tmqd->cp_hqd_eop_base_addr_hi = upper_32_bits(eop_base_addr);\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_EOP_CONTROL);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_EOP_CONTROL, EOP_SIZE,\n\t\t\t(order_base_2(GFX9_MEC_HPD_SIZE / 4) - 1));\n\n\tmqd->cp_hqd_eop_control = tmp;\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_DOORBELL_CONTROL);\n\n\tif (ring->use_doorbell) {\n\t\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,\n\t\t\t\t    DOORBELL_OFFSET, ring->doorbell_index);\n\t\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,\n\t\t\t\t    DOORBELL_EN, 1);\n\t\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,\n\t\t\t\t    DOORBELL_SOURCE, 0);\n\t\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,\n\t\t\t\t    DOORBELL_HIT, 0);\n\t} else {\n\t\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_DOORBELL_CONTROL,\n\t\t\t\t\t DOORBELL_EN, 0);\n\t}\n\n\tmqd->cp_hqd_pq_doorbell_control = tmp;\n\n\t \n\tring->wptr = 0;\n\tmqd->cp_hqd_dequeue_request = 0;\n\tmqd->cp_hqd_pq_rptr = 0;\n\tmqd->cp_hqd_pq_wptr_lo = 0;\n\tmqd->cp_hqd_pq_wptr_hi = 0;\n\n\t \n\tmqd->cp_mqd_base_addr_lo = ring->mqd_gpu_addr & 0xfffffffc;\n\tmqd->cp_mqd_base_addr_hi = upper_32_bits(ring->mqd_gpu_addr);\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_MQD_CONTROL);\n\ttmp = REG_SET_FIELD(tmp, CP_MQD_CONTROL, VMID, 0);\n\tmqd->cp_mqd_control = tmp;\n\n\t \n\thqd_gpu_addr = ring->gpu_addr >> 8;\n\tmqd->cp_hqd_pq_base_lo = hqd_gpu_addr;\n\tmqd->cp_hqd_pq_base_hi = upper_32_bits(hqd_gpu_addr);\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_CONTROL);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, QUEUE_SIZE,\n\t\t\t    (order_base_2(ring->ring_size / 4) - 1));\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, RPTR_BLOCK_SIZE,\n\t\t\t((order_base_2(AMDGPU_GPU_PAGE_SIZE / 4) - 1) << 8));\n#ifdef __BIG_ENDIAN\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, ENDIAN_SWAP, 1);\n#endif\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, UNORD_DISPATCH, 0);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, ROQ_PQ_IB_FLIP, 0);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, PRIV_STATE, 1);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PQ_CONTROL, KMD_QUEUE, 1);\n\tmqd->cp_hqd_pq_control = tmp;\n\n\t \n\twb_gpu_addr = adev->wb.gpu_addr + (ring->rptr_offs * 4);\n\tmqd->cp_hqd_pq_rptr_report_addr_lo = wb_gpu_addr & 0xfffffffc;\n\tmqd->cp_hqd_pq_rptr_report_addr_hi =\n\t\tupper_32_bits(wb_gpu_addr) & 0xffff;\n\n\t \n\twb_gpu_addr = adev->wb.gpu_addr + (ring->wptr_offs * 4);\n\tmqd->cp_hqd_pq_wptr_poll_addr_lo = wb_gpu_addr & 0xfffffffc;\n\tmqd->cp_hqd_pq_wptr_poll_addr_hi = upper_32_bits(wb_gpu_addr) & 0xffff;\n\n\t \n\tring->wptr = 0;\n\tmqd->cp_hqd_pq_rptr = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_RPTR);\n\n\t \n\tmqd->cp_hqd_vmid = 0;\n\n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_PERSISTENT_STATE);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_PERSISTENT_STATE, PRELOAD_SIZE, 0x53);\n\tmqd->cp_hqd_persistent_state = tmp;\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_IB_CONTROL);\n\ttmp = REG_SET_FIELD(tmp, CP_HQD_IB_CONTROL, MIN_IB_AVAIL_SIZE, 3);\n\tmqd->cp_hqd_ib_control = tmp;\n\n\t \n\tgfx_v9_4_3_mqd_set_priority(ring, mqd);\n\tmqd->cp_hqd_quantum = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_QUANTUM);\n\n\t \n\tif (ring->funcs->type == AMDGPU_RING_TYPE_KIQ)\n\t\tmqd->cp_hqd_active = 1;\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kiq_init_register(struct amdgpu_ring *ring,\n\t\t\t\t\t    int xcc_id)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct v9_mqd *mqd = ring->mqd_ptr;\n\tint j;\n\n\t \n\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), CP_PQ_WPTR_POLL_CNTL, EN, 0);\n\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_EOP_BASE_ADDR,\n\t       mqd->cp_hqd_eop_base_addr_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_EOP_BASE_ADDR_HI,\n\t       mqd->cp_hqd_eop_base_addr_hi);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_EOP_CONTROL,\n\t       mqd->cp_hqd_eop_control);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_DOORBELL_CONTROL,\n\t       mqd->cp_hqd_pq_doorbell_control);\n\n\t \n\tif (RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE) & 1) {\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_DEQUEUE_REQUEST, 1);\n\t\tfor (j = 0; j < adev->usec_timeout; j++) {\n\t\t\tif (!(RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE) & 1))\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_DEQUEUE_REQUEST,\n\t\t       mqd->cp_hqd_dequeue_request);\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_RPTR,\n\t\t       mqd->cp_hqd_pq_rptr);\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_LO,\n\t\t       mqd->cp_hqd_pq_wptr_lo);\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_HI,\n\t\t       mqd->cp_hqd_pq_wptr_hi);\n\t}\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_MQD_BASE_ADDR,\n\t       mqd->cp_mqd_base_addr_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_MQD_BASE_ADDR_HI,\n\t       mqd->cp_mqd_base_addr_hi);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_MQD_CONTROL,\n\t       mqd->cp_mqd_control);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_BASE,\n\t       mqd->cp_hqd_pq_base_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_BASE_HI,\n\t       mqd->cp_hqd_pq_base_hi);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_CONTROL,\n\t       mqd->cp_hqd_pq_control);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_RPTR_REPORT_ADDR,\n\t\t\t\tmqd->cp_hqd_pq_rptr_report_addr_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_RPTR_REPORT_ADDR_HI,\n\t\t\t\tmqd->cp_hqd_pq_rptr_report_addr_hi);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_POLL_ADDR,\n\t       mqd->cp_hqd_pq_wptr_poll_addr_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_POLL_ADDR_HI,\n\t       mqd->cp_hqd_pq_wptr_poll_addr_hi);\n\n\t \n\tif (ring->use_doorbell) {\n\t\tWREG32_SOC15(\n\t\t\tGC, GET_INST(GC, xcc_id),\n\t\t\tregCP_MEC_DOORBELL_RANGE_LOWER,\n\t\t\t((adev->doorbell_index.kiq +\n\t\t\t  xcc_id * adev->doorbell_index.xcc_doorbell_range) *\n\t\t\t 2) << 2);\n\t\tWREG32_SOC15(\n\t\t\tGC, GET_INST(GC, xcc_id),\n\t\t\tregCP_MEC_DOORBELL_RANGE_UPPER,\n\t\t\t((adev->doorbell_index.userqueue_end +\n\t\t\t  xcc_id * adev->doorbell_index.xcc_doorbell_range) *\n\t\t\t 2) << 2);\n\t}\n\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_DOORBELL_CONTROL,\n\t       mqd->cp_hqd_pq_doorbell_control);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_LO,\n\t       mqd->cp_hqd_pq_wptr_lo);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_HI,\n\t       mqd->cp_hqd_pq_wptr_hi);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_VMID, mqd->cp_hqd_vmid);\n\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PERSISTENT_STATE,\n\t       mqd->cp_hqd_persistent_state);\n\n\t \n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE,\n\t       mqd->cp_hqd_active);\n\n\tif (ring->use_doorbell)\n\t\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), CP_PQ_STATUS, DOORBELL_ENABLE, 1);\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_q_fini_register(struct amdgpu_ring *ring,\n\t\t\t\t\t    int xcc_id)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tint j;\n\n\t \n\tif (RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE) & 1) {\n\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_DEQUEUE_REQUEST, 1);\n\n\t\tfor (j = 0; j < adev->usec_timeout; j++) {\n\t\t\tif (!(RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE) & 1))\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t\tif (j == AMDGPU_MAX_USEC_TIMEOUT) {\n\t\t\tDRM_DEBUG(\"%s dequeue request failed.\\n\", ring->name);\n\n\t\t\t \n\t\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_ACTIVE, 0);\n\t\t}\n\n\t\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_DEQUEUE_REQUEST,\n\t\t      0);\n\t}\n\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_IQ_TIMER, 0);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_IB_CONTROL, 0);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PERSISTENT_STATE, CP_HQD_PERSISTENT_STATE_DEFAULT);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_DOORBELL_CONTROL, 0x40000000);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_DOORBELL_CONTROL, 0);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_RPTR, 0);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_HI, 0);\n\tWREG32_SOC15_RLC(GC, GET_INST(GC, xcc_id), regCP_HQD_PQ_WPTR_LO, 0);\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kiq_init_queue(struct amdgpu_ring *ring, int xcc_id)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct v9_mqd *mqd = ring->mqd_ptr;\n\tstruct v9_mqd *tmp_mqd;\n\n\tgfx_v9_4_3_xcc_kiq_setting(ring, xcc_id);\n\n\t \n\ttmp_mqd = (struct v9_mqd *)adev->gfx.kiq[xcc_id].mqd_backup;\n\tif (amdgpu_in_reset(adev) && tmp_mqd->cp_hqd_pq_control) {\n\t\t \n\t\tif (adev->gfx.kiq[xcc_id].mqd_backup)\n\t\t\tmemcpy(mqd, adev->gfx.kiq[xcc_id].mqd_backup, sizeof(struct v9_mqd_allocation));\n\n\t\t \n\t\tring->wptr = 0;\n\t\tamdgpu_ring_clear_ring(ring);\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tsoc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, GET_INST(GC, xcc_id));\n\t\tgfx_v9_4_3_xcc_kiq_init_register(ring, xcc_id);\n\t\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\t\tmutex_unlock(&adev->srbm_mutex);\n\t} else {\n\t\tmemset((void *)mqd, 0, sizeof(struct v9_mqd_allocation));\n\t\t((struct v9_mqd_allocation *)mqd)->dynamic_cu_mask = 0xFFFFFFFF;\n\t\t((struct v9_mqd_allocation *)mqd)->dynamic_rb_mask = 0xFFFFFFFF;\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tif (amdgpu_sriov_vf(adev) && adev->in_suspend)\n\t\t\tamdgpu_ring_clear_ring(ring);\n\t\tsoc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, GET_INST(GC, xcc_id));\n\t\tgfx_v9_4_3_xcc_mqd_init(ring, xcc_id);\n\t\tgfx_v9_4_3_xcc_kiq_init_register(ring, xcc_id);\n\t\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\t\tmutex_unlock(&adev->srbm_mutex);\n\n\t\tif (adev->gfx.kiq[xcc_id].mqd_backup)\n\t\t\tmemcpy(adev->gfx.kiq[xcc_id].mqd_backup, mqd, sizeof(struct v9_mqd_allocation));\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kcq_init_queue(struct amdgpu_ring *ring, int xcc_id)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct v9_mqd *mqd = ring->mqd_ptr;\n\tint mqd_idx = ring - &adev->gfx.compute_ring[0];\n\tstruct v9_mqd *tmp_mqd;\n\n\t \n\ttmp_mqd = (struct v9_mqd *)adev->gfx.mec.mqd_backup[mqd_idx];\n\n\tif (!tmp_mqd->cp_hqd_pq_control ||\n\t    (!amdgpu_in_reset(adev) && !adev->in_suspend)) {\n\t\tmemset((void *)mqd, 0, sizeof(struct v9_mqd_allocation));\n\t\t((struct v9_mqd_allocation *)mqd)->dynamic_cu_mask = 0xFFFFFFFF;\n\t\t((struct v9_mqd_allocation *)mqd)->dynamic_rb_mask = 0xFFFFFFFF;\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tsoc15_grbm_select(adev, ring->me, ring->pipe, ring->queue, 0, GET_INST(GC, xcc_id));\n\t\tgfx_v9_4_3_xcc_mqd_init(ring, xcc_id);\n\t\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\t\tmutex_unlock(&adev->srbm_mutex);\n\n\t\tif (adev->gfx.mec.mqd_backup[mqd_idx])\n\t\t\tmemcpy(adev->gfx.mec.mqd_backup[mqd_idx], mqd, sizeof(struct v9_mqd_allocation));\n\t} else {\n\t\t \n\t\tif (adev->gfx.mec.mqd_backup[mqd_idx])\n\t\t\tmemcpy(mqd, adev->gfx.mec.mqd_backup[mqd_idx], sizeof(struct v9_mqd_allocation));\n\t\t \n\t\tring->wptr = 0;\n\t\tatomic64_set((atomic64_t *)&adev->wb.wb[ring->wptr_offs], 0);\n\t\tamdgpu_ring_clear_ring(ring);\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kcq_fini_register(struct amdgpu_device *adev, int xcc_id)\n{\n\tstruct amdgpu_ring *ring;\n\tint j;\n\n\tfor (j = 0; j < adev->gfx.num_compute_rings; j++) {\n\t\tring = &adev->gfx.compute_ring[j +  xcc_id * adev->gfx.num_compute_rings];\n\t\tif (!amdgpu_in_reset(adev) && !adev->in_suspend) {\n\t\t\tmutex_lock(&adev->srbm_mutex);\n\t\t\tsoc15_grbm_select(adev, ring->me,\n\t\t\t\t\tring->pipe,\n\t\t\t\t\tring->queue, 0, GET_INST(GC, xcc_id));\n\t\t\tgfx_v9_4_3_xcc_q_fini_register(ring, xcc_id);\n\t\t\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\t\t\tmutex_unlock(&adev->srbm_mutex);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kiq_resume(struct amdgpu_device *adev, int xcc_id)\n{\n\tstruct amdgpu_ring *ring;\n\tint r;\n\n\tring = &adev->gfx.kiq[xcc_id].ring;\n\n\tr = amdgpu_bo_reserve(ring->mqd_obj, false);\n\tif (unlikely(r != 0))\n\t\treturn r;\n\n\tr = amdgpu_bo_kmap(ring->mqd_obj, (void **)&ring->mqd_ptr);\n\tif (unlikely(r != 0)) {\n\t\tamdgpu_bo_unreserve(ring->mqd_obj);\n\t\treturn r;\n\t}\n\n\tgfx_v9_4_3_xcc_kiq_init_queue(ring, xcc_id);\n\tamdgpu_bo_kunmap(ring->mqd_obj);\n\tring->mqd_ptr = NULL;\n\tamdgpu_bo_unreserve(ring->mqd_obj);\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcc_kcq_resume(struct amdgpu_device *adev, int xcc_id)\n{\n\tstruct amdgpu_ring *ring = NULL;\n\tint r = 0, i;\n\n\tgfx_v9_4_3_xcc_cp_compute_enable(adev, true, xcc_id);\n\n\tfor (i = 0; i < adev->gfx.num_compute_rings; i++) {\n\t\tring = &adev->gfx.compute_ring[i + xcc_id * adev->gfx.num_compute_rings];\n\n\t\tr = amdgpu_bo_reserve(ring->mqd_obj, false);\n\t\tif (unlikely(r != 0))\n\t\t\tgoto done;\n\t\tr = amdgpu_bo_kmap(ring->mqd_obj, (void **)&ring->mqd_ptr);\n\t\tif (!r) {\n\t\t\tr = gfx_v9_4_3_xcc_kcq_init_queue(ring, xcc_id);\n\t\t\tamdgpu_bo_kunmap(ring->mqd_obj);\n\t\t\tring->mqd_ptr = NULL;\n\t\t}\n\t\tamdgpu_bo_unreserve(ring->mqd_obj);\n\t\tif (r)\n\t\t\tgoto done;\n\t}\n\n\tr = amdgpu_gfx_enable_kcq(adev, xcc_id);\ndone:\n\treturn r;\n}\n\nstatic int gfx_v9_4_3_xcc_cp_resume(struct amdgpu_device *adev, int xcc_id)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, j;\n\n\tgfx_v9_4_3_xcc_enable_gui_idle_interrupt(adev, false, xcc_id);\n\n\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\tgfx_v9_4_3_xcc_disable_gpa_mode(adev, xcc_id);\n\n\t\tr = gfx_v9_4_3_xcc_cp_compute_load_microcode(adev, xcc_id);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = gfx_v9_4_3_xcc_kiq_resume(adev, xcc_id);\n\tif (r)\n\t\treturn r;\n\n\tr = gfx_v9_4_3_xcc_kcq_resume(adev, xcc_id);\n\tif (r)\n\t\treturn r;\n\n\tfor (j = 0; j < adev->gfx.num_compute_rings; j++) {\n\t\tring = &adev->gfx.compute_ring\n\t\t\t\t[j + xcc_id * adev->gfx.num_compute_rings];\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tgfx_v9_4_3_xcc_enable_gui_idle_interrupt(adev, true, xcc_id);\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_cp_resume(struct amdgpu_device *adev)\n{\n\tint r = 0, i, num_xcc;\n\n\tif (amdgpu_xcp_query_partition_mode(adev->xcp_mgr,\n\t\t\t\t\t    AMDGPU_XCP_FL_NONE) ==\n\t    AMDGPU_UNKNOWN_COMPUTE_PARTITION_MODE)\n\t\tr = amdgpu_xcp_switch_partition_mode(adev->xcp_mgr,\n\t\t\t\t\t\t     amdgpu_user_partt_mode);\n\n\tif (r)\n\t\treturn r;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tr = gfx_v9_4_3_xcc_cp_resume(adev, i);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_xcc_cp_enable(struct amdgpu_device *adev, bool enable,\n\t\t\t\t     int xcc_id)\n{\n\tgfx_v9_4_3_xcc_cp_compute_enable(adev, enable, xcc_id);\n}\n\nstatic void gfx_v9_4_3_xcc_fini(struct amdgpu_device *adev, int xcc_id)\n{\n\tif (amdgpu_gfx_disable_kcq(adev, xcc_id))\n\t\tDRM_ERROR(\"XCD %d KCQ disable failed\\n\", xcc_id);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, xcc_id), CP_PQ_WPTR_POLL_CNTL, EN, 0);\n\t\treturn;\n\t}\n\n\t \n\tif (!amdgpu_in_reset(adev) && !adev->in_suspend) {\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tsoc15_grbm_select(adev, adev->gfx.kiq[xcc_id].ring.me,\n\t\t\t\t  adev->gfx.kiq[xcc_id].ring.pipe,\n\t\t\t\t  adev->gfx.kiq[xcc_id].ring.queue, 0,\n\t\t\t\t  GET_INST(GC, xcc_id));\n\t\tgfx_v9_4_3_xcc_q_fini_register(&adev->gfx.kiq[xcc_id].ring,\n\t\t\t\t\t\t xcc_id);\n\t\tsoc15_grbm_select(adev, 0, 0, 0, 0, GET_INST(GC, xcc_id));\n\t\tmutex_unlock(&adev->srbm_mutex);\n\t}\n\n\tgfx_v9_4_3_xcc_kcq_fini_register(adev, xcc_id);\n\tgfx_v9_4_3_xcc_cp_enable(adev, false, xcc_id);\n}\n\nstatic int gfx_v9_4_3_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\tgfx_v9_4_3_init_golden_registers(adev);\n\n\tgfx_v9_4_3_constants_init(adev);\n\n\tr = adev->gfx.rlc.funcs->resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = gfx_v9_4_3_cp_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn r;\n}\n\nstatic int gfx_v9_4_3_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, num_xcc;\n\n\tamdgpu_irq_put(adev, &adev->gfx.priv_reg_irq, 0);\n\tamdgpu_irq_put(adev, &adev->gfx.priv_inst_irq, 0);\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tgfx_v9_4_3_xcc_fini(adev, i);\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_suspend(void *handle)\n{\n\treturn gfx_v9_4_3_hw_fini(handle);\n}\n\nstatic int gfx_v9_4_3_resume(void *handle)\n{\n\treturn gfx_v9_4_3_hw_init(handle);\n}\n\nstatic bool gfx_v9_4_3_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tif (REG_GET_FIELD(RREG32_SOC15(GC, GET_INST(GC, i), regGRBM_STATUS),\n\t\t\t\t\tGRBM_STATUS, GUI_ACTIVE))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int gfx_v9_4_3_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (gfx_v9_4_3_is_idle(handle))\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int gfx_v9_4_3_soft_reset(void *handle)\n{\n\tu32 grbm_soft_reset = 0;\n\tu32 tmp;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_STATUS);\n\tif (tmp & (GRBM_STATUS__PA_BUSY_MASK | GRBM_STATUS__SC_BUSY_MASK |\n\t\t   GRBM_STATUS__BCI_BUSY_MASK | GRBM_STATUS__SX_BUSY_MASK |\n\t\t   GRBM_STATUS__TA_BUSY_MASK | GRBM_STATUS__VGT_BUSY_MASK |\n\t\t   GRBM_STATUS__DB_BUSY_MASK | GRBM_STATUS__CB_BUSY_MASK |\n\t\t   GRBM_STATUS__GDS_BUSY_MASK | GRBM_STATUS__SPI_BUSY_MASK |\n\t\t   GRBM_STATUS__IA_BUSY_MASK | GRBM_STATUS__IA_BUSY_NO_DMA_MASK)) {\n\t\tgrbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,\n\t\t\t\t\t\tGRBM_SOFT_RESET, SOFT_RESET_CP, 1);\n\t\tgrbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,\n\t\t\t\t\t\tGRBM_SOFT_RESET, SOFT_RESET_GFX, 1);\n\t}\n\n\tif (tmp & (GRBM_STATUS__CP_BUSY_MASK | GRBM_STATUS__CP_COHERENCY_BUSY_MASK)) {\n\t\tgrbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,\n\t\t\t\t\t\tGRBM_SOFT_RESET, SOFT_RESET_CP, 1);\n\t}\n\n\t \n\ttmp = RREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_STATUS2);\n\tif (REG_GET_FIELD(tmp, GRBM_STATUS2, RLC_BUSY))\n\t\tgrbm_soft_reset = REG_SET_FIELD(grbm_soft_reset,\n\t\t\t\t\t\tGRBM_SOFT_RESET, SOFT_RESET_RLC, 1);\n\n\n\tif (grbm_soft_reset) {\n\t\t \n\t\tadev->gfx.rlc.funcs->stop(adev);\n\n\t\t \n\t\tgfx_v9_4_3_xcc_cp_compute_enable(adev, false, 0);\n\n\t\tif (grbm_soft_reset) {\n\t\t\ttmp = RREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_SOFT_RESET);\n\t\t\ttmp |= grbm_soft_reset;\n\t\t\tdev_info(adev->dev, \"GRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_SOFT_RESET, tmp);\n\t\t\ttmp = RREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_SOFT_RESET);\n\n\t\t\tudelay(50);\n\n\t\t\ttmp &= ~grbm_soft_reset;\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_SOFT_RESET, tmp);\n\t\t\ttmp = RREG32_SOC15(GC, GET_INST(GC, 0), regGRBM_SOFT_RESET);\n\t\t}\n\n\t\t \n\t\tudelay(50);\n\t}\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_ring_emit_gds_switch(struct amdgpu_ring *ring,\n\t\t\t\t\t  uint32_t vmid,\n\t\t\t\t\t  uint32_t gds_base, uint32_t gds_size,\n\t\t\t\t\t  uint32_t gws_base, uint32_t gws_size,\n\t\t\t\t\t  uint32_t oa_base, uint32_t oa_size)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\t \n\tgfx_v9_4_3_write_data_to_reg(ring, 0, false,\n\t\t\t\t   SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regGDS_VMID0_BASE) + 2 * vmid,\n\t\t\t\t   gds_base);\n\n\t \n\tgfx_v9_4_3_write_data_to_reg(ring, 0, false,\n\t\t\t\t   SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regGDS_VMID0_SIZE) + 2 * vmid,\n\t\t\t\t   gds_size);\n\n\t \n\tgfx_v9_4_3_write_data_to_reg(ring, 0, false,\n\t\t\t\t   SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regGDS_GWS_VMID0) + vmid,\n\t\t\t\t   gws_size << GDS_GWS_VMID0__SIZE__SHIFT | gws_base);\n\n\t \n\tgfx_v9_4_3_write_data_to_reg(ring, 0, false,\n\t\t\t\t   SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regGDS_OA_VMID0) + vmid,\n\t\t\t\t   (1 << (oa_size + oa_base)) - (1 << oa_base));\n}\n\nstatic int gfx_v9_4_3_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->gfx.num_compute_rings = min(amdgpu_gfx_get_num_kcq(adev),\n\t\t\t\t\t  AMDGPU_MAX_COMPUTE_RINGS);\n\tgfx_v9_4_3_set_kiq_pm4_funcs(adev);\n\tgfx_v9_4_3_set_ring_funcs(adev);\n\tgfx_v9_4_3_set_irq_funcs(adev);\n\tgfx_v9_4_3_set_gds_init(adev);\n\tgfx_v9_4_3_set_rlc_funcs(adev);\n\n\t \n\tgfx_v9_4_3_init_rlcg_reg_access_ctrl(adev);\n\n\treturn gfx_v9_4_3_init_microcode(adev);\n}\n\nstatic int gfx_v9_4_3_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_irq_get(adev, &adev->gfx.priv_reg_irq, 0);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_get(adev, &adev->gfx.priv_inst_irq, 0);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->gfx.ras &&\n\t    adev->gfx.ras->enable_watchdog_timer)\n\t\tadev->gfx.ras->enable_watchdog_timer(adev);\n\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_xcc_update_sram_fgcg(struct amdgpu_device *adev,\n\t\t\t\t\t    bool enable, int xcc_id)\n{\n\tuint32_t def, data;\n\n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_GFX_FGCG))\n\t\treturn;\n\n\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t  regRLC_CGTT_MGCG_OVERRIDE);\n\n\tif (enable)\n\t\tdata &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_FGCG_OVERRIDE_MASK;\n\telse\n\t\tdata |= RLC_CGTT_MGCG_OVERRIDE__GFXIP_FGCG_OVERRIDE_MASK;\n\n\tif (def != data)\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t     regRLC_CGTT_MGCG_OVERRIDE, data);\n\n}\n\nstatic void gfx_v9_4_3_xcc_update_repeater_fgcg(struct amdgpu_device *adev,\n\t\t\t\t\t\tbool enable, int xcc_id)\n{\n\tuint32_t def, data;\n\n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_REPEATER_FGCG))\n\t\treturn;\n\n\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t  regRLC_CGTT_MGCG_OVERRIDE);\n\n\tif (enable)\n\t\tdata &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_REP_FGCG_OVERRIDE_MASK;\n\telse\n\t\tdata |= RLC_CGTT_MGCG_OVERRIDE__GFXIP_REP_FGCG_OVERRIDE_MASK;\n\n\tif (def != data)\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t     regRLC_CGTT_MGCG_OVERRIDE, data);\n}\n\nstatic void\ngfx_v9_4_3_xcc_update_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\tbool enable, int xcc_id)\n{\n\tuint32_t data, def;\n\n\t \n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGCG)) {\n\t\t \n\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE);\n\n\t\tdata &= ~(RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |\n\t\t\t  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |\n\t\t\t  RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK |\n\t\t\t  RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);\n\n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE, data);\n\n\t\t \n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_GFX_MGLS) {\n\t\t\t \n\t\t\tif (adev->cg_flags & AMD_CG_SUPPORT_GFX_RLC_LS) {\n\t\t\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_MEM_SLP_CNTL);\n\t\t\t\tdata |= RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;\n\t\t\t\tif (def != data)\n\t\t\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_MEM_SLP_CNTL, data);\n\t\t\t}\n\t\t\t \n\t\t\tif (adev->cg_flags & AMD_CG_SUPPORT_GFX_CP_LS) {\n\t\t\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_MEM_SLP_CNTL);\n\t\t\t\tdata |= CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;\n\t\t\t\tif (def != data)\n\t\t\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_MEM_SLP_CNTL, data);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE);\n\n\t\tdata |= (RLC_CGTT_MGCG_OVERRIDE__RLC_CGTT_SCLK_OVERRIDE_MASK |\n\t\t\t RLC_CGTT_MGCG_OVERRIDE__GRBM_CGTT_SCLK_OVERRIDE_MASK |\n\t\t\t RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK |\n\t\t\t RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGLS_OVERRIDE_MASK);\n\n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE, data);\n\n\t\t \n\t\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_MEM_SLP_CNTL);\n\t\tif (data & RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK) {\n\t\t\tdata &= ~RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK;\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_MEM_SLP_CNTL, data);\n\t\t}\n\n\t\t \n\t\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_MEM_SLP_CNTL);\n\t\tif (data & CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK) {\n\t\t\tdata &= ~CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK;\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_MEM_SLP_CNTL, data);\n\t\t}\n\t}\n\n}\n\nstatic void\ngfx_v9_4_3_xcc_update_coarse_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\tbool enable, int xcc_id)\n{\n\tuint32_t def, data;\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGCG)) {\n\n\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE);\n\t\t \n\t\tdata &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGCG_OVERRIDE_MASK;\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)\n\t\t\tdata &= ~RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;\n\t\telse\n\t\t\tdata |= RLC_CGTT_MGCG_OVERRIDE__GFXIP_CGLS_OVERRIDE_MASK;\n\t\t \n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGTT_MGCG_OVERRIDE, data);\n\n\t\t \n\t\tdef = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGCG_CGLS_CTRL);\n\n\t\tdata = (0x36\n\t\t\t<< RLC_CGCG_CGLS_CTRL__CGCG_GFX_IDLE_THRESHOLD__SHIFT) |\n\t\t       RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK;\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_GFX_CGLS)\n\t\t\tdata |= (0x000F << RLC_CGCG_CGLS_CTRL__CGLS_REP_COMPANSAT_DELAY__SHIFT) |\n\t\t\t\tRLC_CGCG_CGLS_CTRL__CGLS_EN_MASK;\n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGCG_CGLS_CTRL, data);\n\n\t\t \n\t\tdef = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_RB_WPTR_POLL_CNTL);\n\t\tdata = (0x0100 << CP_RB_WPTR_POLL_CNTL__POLL_FREQUENCY__SHIFT) |\n\t\t\t(0x0090 << CP_RB_WPTR_POLL_CNTL__IDLE_POLL_COUNT__SHIFT);\n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regCP_RB_WPTR_POLL_CNTL, data);\n\t} else {\n\t\tdef = data = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGCG_CGLS_CTRL);\n\t\t \n\t\tdata &= ~(RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK | RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK);\n\t\t \n\t\tif (def != data)\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regRLC_CGCG_CGLS_CTRL, data);\n\t}\n\n}\n\nstatic int gfx_v9_4_3_xcc_update_gfx_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t  bool enable, int xcc_id)\n{\n\tamdgpu_gfx_rlc_enter_safe_mode(adev, xcc_id);\n\n\tif (enable) {\n\t\t \n\t\tgfx_v9_4_3_xcc_update_sram_fgcg(adev, enable, xcc_id);\n\t\tgfx_v9_4_3_xcc_update_repeater_fgcg(adev, enable, xcc_id);\n\n\t\t \n\t\tgfx_v9_4_3_xcc_update_medium_grain_clock_gating(adev, enable,\n\t\t\t\t\t\t\t\txcc_id);\n\t\t \n\t\tgfx_v9_4_3_xcc_update_coarse_grain_clock_gating(adev, enable,\n\t\t\t\t\t\t\t\txcc_id);\n\t} else {\n\t\t \n\t\tgfx_v9_4_3_xcc_update_coarse_grain_clock_gating(adev, enable,\n\t\t\t\t\t\t\t\txcc_id);\n\t\t \n\t\tgfx_v9_4_3_xcc_update_medium_grain_clock_gating(adev, enable,\n\t\t\t\t\t\t\t\txcc_id);\n\n\t\t \n\t\tgfx_v9_4_3_xcc_update_sram_fgcg(adev, enable, xcc_id);\n\t\tgfx_v9_4_3_xcc_update_repeater_fgcg(adev, enable, xcc_id);\n\t}\n\n\tamdgpu_gfx_rlc_exit_safe_mode(adev, xcc_id);\n\n\treturn 0;\n}\n\nstatic const struct amdgpu_rlc_funcs gfx_v9_4_3_rlc_funcs = {\n\t.is_rlc_enabled = gfx_v9_4_3_is_rlc_enabled,\n\t.set_safe_mode = gfx_v9_4_3_xcc_set_safe_mode,\n\t.unset_safe_mode = gfx_v9_4_3_xcc_unset_safe_mode,\n\t.init = gfx_v9_4_3_rlc_init,\n\t.resume = gfx_v9_4_3_rlc_resume,\n\t.stop = gfx_v9_4_3_rlc_stop,\n\t.reset = gfx_v9_4_3_rlc_reset,\n\t.start = gfx_v9_4_3_rlc_start,\n\t.update_spm_vmid = gfx_v9_4_3_update_spm_vmid,\n\t.is_rlcg_access_range = gfx_v9_4_3_is_rlcg_access_range,\n};\n\nstatic int gfx_v9_4_3_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i, num_xcc;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 3):\n\t\tfor (i = 0; i < num_xcc; i++)\n\t\t\tgfx_v9_4_3_xcc_update_gfx_clock_gating(\n\t\t\t\tadev, state == AMD_CG_STATE_GATE, i);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t \n\tdata = RREG32_KIQ(SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regRLC_CGTT_MGCG_OVERRIDE));\n\tif (!(data & RLC_CGTT_MGCG_OVERRIDE__GFXIP_MGCG_OVERRIDE_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_GFX_MGCG;\n\n\t \n\tdata = RREG32_KIQ(SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regRLC_CGCG_CGLS_CTRL));\n\tif (data & RLC_CGCG_CGLS_CTRL__CGCG_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_GFX_CGCG;\n\n\t \n\tif (data & RLC_CGCG_CGLS_CTRL__CGLS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_GFX_CGLS;\n\n\t \n\tdata = RREG32_KIQ(SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regRLC_MEM_SLP_CNTL));\n\tif (data & RLC_MEM_SLP_CNTL__RLC_MEM_LS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_GFX_RLC_LS | AMD_CG_SUPPORT_GFX_MGLS;\n\n\t \n\tdata = RREG32_KIQ(SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regCP_MEM_SLP_CNTL));\n\tif (data & CP_MEM_SLP_CNTL__CP_MEM_LS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_GFX_CP_LS | AMD_CG_SUPPORT_GFX_MGLS;\n}\n\nstatic void gfx_v9_4_3_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 ref_and_mask, reg_mem_engine;\n\tconst struct nbio_hdp_flush_reg *nbio_hf_reg = adev->nbio.hdp_flush_reg;\n\n\tif (ring->funcs->type == AMDGPU_RING_TYPE_COMPUTE) {\n\t\tswitch (ring->me) {\n\t\tcase 1:\n\t\t\tref_and_mask = nbio_hf_reg->ref_and_mask_cp2 << ring->pipe;\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tref_and_mask = nbio_hf_reg->ref_and_mask_cp6 << ring->pipe;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t\treg_mem_engine = 0;\n\t} else {\n\t\tref_and_mask = nbio_hf_reg->ref_and_mask_cp0;\n\t\treg_mem_engine = 1;  \n\t}\n\n\tgfx_v9_4_3_wait_reg_mem(ring, reg_mem_engine, 0, 1,\n\t\t\t      adev->nbio.funcs->get_hdp_flush_req_offset(adev),\n\t\t\t      adev->nbio.funcs->get_hdp_flush_done_offset(adev),\n\t\t\t      ref_and_mask, ref_and_mask, 0x20);\n}\n\nstatic void gfx_v9_4_3_ring_emit_ib_compute(struct amdgpu_ring *ring,\n\t\t\t\t\t  struct amdgpu_job *job,\n\t\t\t\t\t  struct amdgpu_ib *ib,\n\t\t\t\t\t  uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\tu32 control = INDIRECT_BUFFER_VALID | ib->length_dw | (vmid << 24);\n\n\t \n\tif (ib->flags & AMDGPU_IB_FLAG_RESET_GDS_MAX_WAVE_ID) {\n\t\tamdgpu_ring_write(ring, PACKET3(PACKET3_SET_CONFIG_REG, 1));\n\t\tamdgpu_ring_write(ring, regGDS_COMPUTE_MAX_WAVE_ID);\n\t\tamdgpu_ring_write(ring, ring->adev->gds.gds_compute_max_wave_id);\n\t}\n\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_INDIRECT_BUFFER, 2));\n\tBUG_ON(ib->gpu_addr & 0x3);  \n\tamdgpu_ring_write(ring,\n#ifdef __BIG_ENDIAN\n\t\t\t\t(2 << 0) |\n#endif\n\t\t\t\tlower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, control);\n}\n\nstatic void gfx_v9_4_3_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,\n\t\t\t\t     u64 seq, unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\tbool int_sel = flags & AMDGPU_FENCE_FLAG_INT;\n\tbool writeback = flags & AMDGPU_FENCE_FLAG_TC_WB_ONLY;\n\n\t \n\tamdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));\n\tamdgpu_ring_write(ring, ((writeback ? (EOP_TC_WB_ACTION_EN |\n\t\t\t\t\t       EOP_TC_NC_ACTION_EN) :\n\t\t\t\t\t      (EOP_TCL1_ACTION_EN |\n\t\t\t\t\t       EOP_TC_ACTION_EN |\n\t\t\t\t\t       EOP_TC_WB_ACTION_EN |\n\t\t\t\t\t       EOP_TC_MD_ACTION_EN)) |\n\t\t\t\t EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |\n\t\t\t\t EVENT_INDEX(5)));\n\tamdgpu_ring_write(ring, DATA_SEL(write64bit ? 2 : 1) | INT_SEL(int_sel ? 2 : 0));\n\n\t \n\tif (write64bit)\n\t\tBUG_ON(addr & 0x7);\n\telse\n\t\tBUG_ON(addr & 0x3);\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\tamdgpu_ring_write(ring, 0);\n}\n\nstatic void gfx_v9_4_3_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tint usepfp = (ring->funcs->type == AMDGPU_RING_TYPE_GFX);\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\tgfx_v9_4_3_wait_reg_mem(ring, usepfp, 1, 0,\n\t\t\t      lower_32_bits(addr), upper_32_bits(addr),\n\t\t\t      seq, 0xffffffff, 4);\n}\n\nstatic void gfx_v9_4_3_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\tunsigned vmid, uint64_t pd_addr)\n{\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n}\n\nstatic u64 gfx_v9_4_3_ring_get_rptr_compute(struct amdgpu_ring *ring)\n{\n\treturn ring->adev->wb.wb[ring->rptr_offs];  \n}\n\nstatic u64 gfx_v9_4_3_ring_get_wptr_compute(struct amdgpu_ring *ring)\n{\n\tu64 wptr;\n\n\t \n\tif (ring->use_doorbell)\n\t\twptr = atomic64_read((atomic64_t *)&ring->adev->wb.wb[ring->wptr_offs]);\n\telse\n\t\tBUG();\n\treturn wptr;\n}\n\nstatic void gfx_v9_4_3_ring_set_wptr_compute(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\t \n\tif (ring->use_doorbell) {\n\t\tatomic64_set((atomic64_t *)&adev->wb.wb[ring->wptr_offs], ring->wptr);\n\t\tWDOORBELL64(ring->doorbell_index, ring->wptr);\n\t} else {\n\t\tBUG();  \n\t}\n}\n\nstatic void gfx_v9_4_3_ring_emit_fence_kiq(struct amdgpu_ring *ring, u64 addr,\n\t\t\t\t\t u64 seq, unsigned int flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\t \n\tBUG_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\t \n\tamdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\n\tamdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(0) |\n\t\t\t\t WRITE_DATA_DST_SEL(5) | WR_CONFIRM));\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\tif (flags & AMDGPU_FENCE_FLAG_INT) {\n\t\t \n\t\tamdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\n\t\tamdgpu_ring_write(ring, (WRITE_DATA_ENGINE_SEL(0) |\n\t\t\t\t\t WRITE_DATA_DST_SEL(0) | WR_CONFIRM));\n\t\tamdgpu_ring_write(ring, SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regCPC_INT_STATUS));\n\t\tamdgpu_ring_write(ring, 0);\n\t\tamdgpu_ring_write(ring, 0x20000000);  \n\t}\n}\n\nstatic void gfx_v9_4_3_ring_emit_rreg(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t    uint32_t reg_val_offs)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_COPY_DATA, 4));\n\tamdgpu_ring_write(ring, 0 |\t \n\t\t\t\t(5 << 8) |\t \n\t\t\t\t(1 << 20));\t \n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, lower_32_bits(adev->wb.gpu_addr +\n\t\t\t\treg_val_offs * 4));\n\tamdgpu_ring_write(ring, upper_32_bits(adev->wb.gpu_addr +\n\t\t\t\treg_val_offs * 4));\n}\n\nstatic void gfx_v9_4_3_ring_emit_wreg(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t    uint32_t val)\n{\n\tuint32_t cmd = 0;\n\n\tswitch (ring->funcs->type) {\n\tcase AMDGPU_RING_TYPE_GFX:\n\t\tcmd = WRITE_DATA_ENGINE_SEL(1) | WR_CONFIRM;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_KIQ:\n\t\tcmd = (1 << 16);  \n\t\tbreak;\n\tdefault:\n\t\tcmd = WR_CONFIRM;\n\t\tbreak;\n\t}\n\tamdgpu_ring_write(ring, PACKET3(PACKET3_WRITE_DATA, 3));\n\tamdgpu_ring_write(ring, cmd);\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void gfx_v9_4_3_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tgfx_v9_4_3_wait_reg_mem(ring, 0, 0, 0, reg, 0, val, mask, 0x20);\n}\n\nstatic void gfx_v9_4_3_ring_emit_reg_write_reg_wait(struct amdgpu_ring *ring,\n\t\t\t\t\t\t  uint32_t reg0, uint32_t reg1,\n\t\t\t\t\t\t  uint32_t ref, uint32_t mask)\n{\n\tamdgpu_ring_emit_reg_write_reg_wait_helper(ring, reg0, reg1,\n\t\t\t\t\t\t   ref, mask);\n}\n\nstatic void gfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\tstruct amdgpu_device *adev, int me, int pipe,\n\tenum amdgpu_interrupt_state state, int xcc_id)\n{\n\tu32 mec_int_cntl, mec_int_cntl_reg;\n\n\t \n\n\tif (me == 1) {\n\t\tswitch (pipe) {\n\t\tcase 0:\n\t\t\tmec_int_cntl_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_ME1_PIPE0_INT_CNTL);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\tmec_int_cntl_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_ME1_PIPE1_INT_CNTL);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tmec_int_cntl_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_ME1_PIPE2_INT_CNTL);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tmec_int_cntl_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, xcc_id), regCP_ME1_PIPE3_INT_CNTL);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_DEBUG(\"invalid pipe %d\\n\", pipe);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tDRM_DEBUG(\"invalid me %d\\n\", me);\n\t\treturn;\n\t}\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tmec_int_cntl = RREG32(mec_int_cntl_reg);\n\t\tmec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,\n\t\t\t\t\t     TIME_STAMP_INT_ENABLE, 0);\n\t\tWREG32(mec_int_cntl_reg, mec_int_cntl);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tmec_int_cntl = RREG32(mec_int_cntl_reg);\n\t\tmec_int_cntl = REG_SET_FIELD(mec_int_cntl, CP_ME1_PIPE0_INT_CNTL,\n\t\t\t\t\t     TIME_STAMP_INT_ENABLE, 1);\n\t\tWREG32(mec_int_cntl_reg, mec_int_cntl);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_3_set_priv_reg_fault_state(struct amdgpu_device *adev,\n\t\t\t\t\t     struct amdgpu_irq_src *source,\n\t\t\t\t\t     unsigned type,\n\t\t\t\t\t     enum amdgpu_interrupt_state state)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tfor (i = 0; i < num_xcc; i++)\n\t\t\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, i), CP_INT_CNTL_RING0,\n\t\t\t\tPRIV_REG_INT_ENABLE,\n\t\t\t\tstate == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_set_priv_inst_fault_state(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      unsigned type,\n\t\t\t\t\t      enum amdgpu_interrupt_state state)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tfor (i = 0; i < num_xcc; i++)\n\t\t\tWREG32_FIELD15_PREREG(GC, GET_INST(GC, i), CP_INT_CNTL_RING0,\n\t\t\t\tPRIV_INSTR_INT_ENABLE,\n\t\t\t\tstate == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_set_eop_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t    struct amdgpu_irq_src *src,\n\t\t\t\t\t    unsigned type,\n\t\t\t\t\t    enum amdgpu_interrupt_state state)\n{\n\tint i, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tswitch (type) {\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE0_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 1, 0, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE1_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 1, 1, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE2_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 1, 2, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC1_PIPE3_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 1, 3, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE0_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 2, 0, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE1_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 2, 1, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE2_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 2, 2, state, i);\n\t\t\tbreak;\n\t\tcase AMDGPU_CP_IRQ_COMPUTE_MEC2_PIPE3_EOP:\n\t\t\tgfx_v9_4_3_xcc_set_compute_eop_interrupt_state(\n\t\t\t\tadev, 2, 3, state, i);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_eop_irq(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_irq_src *source,\n\t\t\t    struct amdgpu_iv_entry *entry)\n{\n\tint i, xcc_id;\n\tu8 me_id, pipe_id, queue_id;\n\tstruct amdgpu_ring *ring;\n\n\tDRM_DEBUG(\"IH: CP EOP\\n\");\n\tme_id = (entry->ring_id & 0x0c) >> 2;\n\tpipe_id = (entry->ring_id & 0x03) >> 0;\n\tqueue_id = (entry->ring_id & 0x70) >> 4;\n\n\txcc_id = gfx_v9_4_3_ih_to_xcc_inst(adev, entry->node_id);\n\n\tif (xcc_id == -EINVAL)\n\t\treturn -EINVAL;\n\n\tswitch (me_id) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\t\tfor (i = 0; i < adev->gfx.num_compute_rings; i++) {\n\t\t\tring = &adev->gfx.compute_ring\n\t\t\t\t\t[i +\n\t\t\t\t\t xcc_id * adev->gfx.num_compute_rings];\n\t\t\t \n\n\t\t\tif ((ring->me == me_id) && (ring->pipe == pipe_id) && (ring->queue == queue_id))\n\t\t\t\tamdgpu_fence_process(ring);\n\t\t}\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_fault(struct amdgpu_device *adev,\n\t\t\t   struct amdgpu_iv_entry *entry)\n{\n\tu8 me_id, pipe_id, queue_id;\n\tstruct amdgpu_ring *ring;\n\tint i, xcc_id;\n\n\tme_id = (entry->ring_id & 0x0c) >> 2;\n\tpipe_id = (entry->ring_id & 0x03) >> 0;\n\tqueue_id = (entry->ring_id & 0x70) >> 4;\n\n\txcc_id = gfx_v9_4_3_ih_to_xcc_inst(adev, entry->node_id);\n\n\tif (xcc_id == -EINVAL)\n\t\treturn;\n\n\tswitch (me_id) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\t\tfor (i = 0; i < adev->gfx.num_compute_rings; i++) {\n\t\t\tring = &adev->gfx.compute_ring\n\t\t\t\t\t[i +\n\t\t\t\t\t xcc_id * adev->gfx.num_compute_rings];\n\t\t\tif (ring->me == me_id && ring->pipe == pipe_id &&\n\t\t\t    ring->queue == queue_id)\n\t\t\t\tdrm_sched_fault(&ring->sched);\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic int gfx_v9_4_3_priv_reg_irq(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_irq_src *source,\n\t\t\t\t struct amdgpu_iv_entry *entry)\n{\n\tDRM_ERROR(\"Illegal register access in command stream\\n\");\n\tgfx_v9_4_3_fault(adev, entry);\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_priv_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_irq_src *source,\n\t\t\t\t  struct amdgpu_iv_entry *entry)\n{\n\tDRM_ERROR(\"Illegal instruction in command stream\\n\");\n\tgfx_v9_4_3_fault(adev, entry);\n\treturn 0;\n}\n\nstatic void gfx_v9_4_3_emit_mem_sync(struct amdgpu_ring *ring)\n{\n\tconst unsigned int cp_coher_cntl =\n\t\t\tPACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_ICACHE_ACTION_ENA(1) |\n\t\t\tPACKET3_ACQUIRE_MEM_CP_COHER_CNTL_SH_KCACHE_ACTION_ENA(1) |\n\t\t\tPACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_ACTION_ENA(1) |\n\t\t\tPACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TCL1_ACTION_ENA(1) |\n\t\t\tPACKET3_ACQUIRE_MEM_CP_COHER_CNTL_TC_WB_ACTION_ENA(1);\n\n\t \n\tamdgpu_ring_write(ring, PACKET3(PACKET3_ACQUIRE_MEM, 5));\n\tamdgpu_ring_write(ring, cp_coher_cntl);  \n\tamdgpu_ring_write(ring, 0xffffffff);   \n\tamdgpu_ring_write(ring, 0xffffff);   \n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, 0);   \n\tamdgpu_ring_write(ring, 0x0000000A);  \n}\n\nstatic void gfx_v9_4_3_emit_wave_limit_cs(struct amdgpu_ring *ring,\n\t\t\t\t\tuint32_t pipe, bool enable)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t val;\n\tuint32_t wcl_cs_reg;\n\n\t \n\tval = enable ? 0x1 : 0x7f;\n\n\tswitch (pipe) {\n\tcase 0:\n\t\twcl_cs_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regSPI_WCL_PIPE_PERCENT_CS0);\n\t\tbreak;\n\tcase 1:\n\t\twcl_cs_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regSPI_WCL_PIPE_PERCENT_CS1);\n\t\tbreak;\n\tcase 2:\n\t\twcl_cs_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regSPI_WCL_PIPE_PERCENT_CS2);\n\t\tbreak;\n\tcase 3:\n\t\twcl_cs_reg = SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regSPI_WCL_PIPE_PERCENT_CS3);\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"invalid pipe %d\\n\", pipe);\n\t\treturn;\n\t}\n\n\tamdgpu_ring_emit_wreg(ring, wcl_cs_reg, val);\n\n}\nstatic void gfx_v9_4_3_emit_wave_limit(struct amdgpu_ring *ring, bool enable)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t val;\n\tint i;\n\n\t \n\tval = enable ? 0x1f : 0x07ffffff;\n\tamdgpu_ring_emit_wreg(ring,\n\t\t\t      SOC15_REG_OFFSET(GC, GET_INST(GC, 0), regSPI_WCL_PIPE_PERCENT_GFX),\n\t\t\t      val);\n\n\t \n\tfor (i = 0; i < adev->gfx.mec.num_pipe_per_mec; i++) {\n\t\tif (i != ring->pipe)\n\t\t\tgfx_v9_4_3_emit_wave_limit_cs(ring, i, enable);\n\n\t}\n}\n\nenum amdgpu_gfx_cp_ras_mem_id {\n\tAMDGPU_GFX_CP_MEM1 = 1,\n\tAMDGPU_GFX_CP_MEM2,\n\tAMDGPU_GFX_CP_MEM3,\n\tAMDGPU_GFX_CP_MEM4,\n\tAMDGPU_GFX_CP_MEM5,\n};\n\nenum amdgpu_gfx_gcea_ras_mem_id {\n\tAMDGPU_GFX_GCEA_IOWR_CMDMEM = 4,\n\tAMDGPU_GFX_GCEA_IORD_CMDMEM,\n\tAMDGPU_GFX_GCEA_GMIWR_CMDMEM,\n\tAMDGPU_GFX_GCEA_GMIRD_CMDMEM,\n\tAMDGPU_GFX_GCEA_DRAMWR_CMDMEM,\n\tAMDGPU_GFX_GCEA_DRAMRD_CMDMEM,\n\tAMDGPU_GFX_GCEA_MAM_DMEM0,\n\tAMDGPU_GFX_GCEA_MAM_DMEM1,\n\tAMDGPU_GFX_GCEA_MAM_DMEM2,\n\tAMDGPU_GFX_GCEA_MAM_DMEM3,\n\tAMDGPU_GFX_GCEA_MAM_AMEM0,\n\tAMDGPU_GFX_GCEA_MAM_AMEM1,\n\tAMDGPU_GFX_GCEA_MAM_AMEM2,\n\tAMDGPU_GFX_GCEA_MAM_AMEM3,\n\tAMDGPU_GFX_GCEA_MAM_AFLUSH_BUFFER,\n\tAMDGPU_GFX_GCEA_WRET_TAGMEM,\n\tAMDGPU_GFX_GCEA_RRET_TAGMEM,\n\tAMDGPU_GFX_GCEA_IOWR_DATAMEM,\n\tAMDGPU_GFX_GCEA_GMIWR_DATAMEM,\n\tAMDGPU_GFX_GCEA_DRAM_DATAMEM,\n};\n\nenum amdgpu_gfx_gc_cane_ras_mem_id {\n\tAMDGPU_GFX_GC_CANE_MEM0 = 0,\n};\n\nenum amdgpu_gfx_gcutcl2_ras_mem_id {\n\tAMDGPU_GFX_GCUTCL2_MEM2P512X95 = 160,\n};\n\nenum amdgpu_gfx_gds_ras_mem_id {\n\tAMDGPU_GFX_GDS_MEM0 = 0,\n};\n\nenum amdgpu_gfx_lds_ras_mem_id {\n\tAMDGPU_GFX_LDS_BANK0 = 0,\n\tAMDGPU_GFX_LDS_BANK1,\n\tAMDGPU_GFX_LDS_BANK2,\n\tAMDGPU_GFX_LDS_BANK3,\n\tAMDGPU_GFX_LDS_BANK4,\n\tAMDGPU_GFX_LDS_BANK5,\n\tAMDGPU_GFX_LDS_BANK6,\n\tAMDGPU_GFX_LDS_BANK7,\n\tAMDGPU_GFX_LDS_BANK8,\n\tAMDGPU_GFX_LDS_BANK9,\n\tAMDGPU_GFX_LDS_BANK10,\n\tAMDGPU_GFX_LDS_BANK11,\n\tAMDGPU_GFX_LDS_BANK12,\n\tAMDGPU_GFX_LDS_BANK13,\n\tAMDGPU_GFX_LDS_BANK14,\n\tAMDGPU_GFX_LDS_BANK15,\n\tAMDGPU_GFX_LDS_BANK16,\n\tAMDGPU_GFX_LDS_BANK17,\n\tAMDGPU_GFX_LDS_BANK18,\n\tAMDGPU_GFX_LDS_BANK19,\n\tAMDGPU_GFX_LDS_BANK20,\n\tAMDGPU_GFX_LDS_BANK21,\n\tAMDGPU_GFX_LDS_BANK22,\n\tAMDGPU_GFX_LDS_BANK23,\n\tAMDGPU_GFX_LDS_BANK24,\n\tAMDGPU_GFX_LDS_BANK25,\n\tAMDGPU_GFX_LDS_BANK26,\n\tAMDGPU_GFX_LDS_BANK27,\n\tAMDGPU_GFX_LDS_BANK28,\n\tAMDGPU_GFX_LDS_BANK29,\n\tAMDGPU_GFX_LDS_BANK30,\n\tAMDGPU_GFX_LDS_BANK31,\n\tAMDGPU_GFX_LDS_SP_BUFFER_A,\n\tAMDGPU_GFX_LDS_SP_BUFFER_B,\n};\n\nenum amdgpu_gfx_rlc_ras_mem_id {\n\tAMDGPU_GFX_RLC_GPMF32 = 1,\n\tAMDGPU_GFX_RLC_RLCVF32,\n\tAMDGPU_GFX_RLC_SCRATCH,\n\tAMDGPU_GFX_RLC_SRM_ARAM,\n\tAMDGPU_GFX_RLC_SRM_DRAM,\n\tAMDGPU_GFX_RLC_TCTAG,\n\tAMDGPU_GFX_RLC_SPM_SE,\n\tAMDGPU_GFX_RLC_SPM_GRBMT,\n};\n\nenum amdgpu_gfx_sp_ras_mem_id {\n\tAMDGPU_GFX_SP_SIMDID0 = 0,\n};\n\nenum amdgpu_gfx_spi_ras_mem_id {\n\tAMDGPU_GFX_SPI_MEM0 = 0,\n\tAMDGPU_GFX_SPI_MEM1,\n\tAMDGPU_GFX_SPI_MEM2,\n\tAMDGPU_GFX_SPI_MEM3,\n};\n\nenum amdgpu_gfx_sqc_ras_mem_id {\n\tAMDGPU_GFX_SQC_INST_CACHE_A = 100,\n\tAMDGPU_GFX_SQC_INST_CACHE_B = 101,\n\tAMDGPU_GFX_SQC_INST_CACHE_TAG_A = 102,\n\tAMDGPU_GFX_SQC_INST_CACHE_TAG_B = 103,\n\tAMDGPU_GFX_SQC_INST_CACHE_MISS_FIFO_A = 104,\n\tAMDGPU_GFX_SQC_INST_CACHE_MISS_FIFO_B = 105,\n\tAMDGPU_GFX_SQC_INST_CACHE_GATCL1_MISS_FIFO_A = 106,\n\tAMDGPU_GFX_SQC_INST_CACHE_GATCL1_MISS_FIFO_B = 107,\n\tAMDGPU_GFX_SQC_DATA_CACHE_A = 200,\n\tAMDGPU_GFX_SQC_DATA_CACHE_B = 201,\n\tAMDGPU_GFX_SQC_DATA_CACHE_TAG_A = 202,\n\tAMDGPU_GFX_SQC_DATA_CACHE_TAG_B = 203,\n\tAMDGPU_GFX_SQC_DATA_CACHE_MISS_FIFO_A = 204,\n\tAMDGPU_GFX_SQC_DATA_CACHE_MISS_FIFO_B = 205,\n\tAMDGPU_GFX_SQC_DATA_CACHE_HIT_FIFO_A = 206,\n\tAMDGPU_GFX_SQC_DATA_CACHE_HIT_FIFO_B = 207,\n\tAMDGPU_GFX_SQC_DIRTY_BIT_A = 208,\n\tAMDGPU_GFX_SQC_DIRTY_BIT_B = 209,\n\tAMDGPU_GFX_SQC_WRITE_DATA_BUFFER_CU0 = 210,\n\tAMDGPU_GFX_SQC_WRITE_DATA_BUFFER_CU1 = 211,\n\tAMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_DATA_CACHE_A = 212,\n\tAMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_DATA_CACHE_B = 213,\n\tAMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_INST_CACHE = 108,\n};\n\nenum amdgpu_gfx_sq_ras_mem_id {\n\tAMDGPU_GFX_SQ_SGPR_MEM0 = 0,\n\tAMDGPU_GFX_SQ_SGPR_MEM1,\n\tAMDGPU_GFX_SQ_SGPR_MEM2,\n\tAMDGPU_GFX_SQ_SGPR_MEM3,\n};\n\nenum amdgpu_gfx_ta_ras_mem_id {\n\tAMDGPU_GFX_TA_FS_AFIFO_RAM_LO = 1,\n\tAMDGPU_GFX_TA_FS_AFIFO_RAM_HI,\n\tAMDGPU_GFX_TA_FS_CFIFO_RAM,\n\tAMDGPU_GFX_TA_FSX_LFIFO,\n\tAMDGPU_GFX_TA_FS_DFIFO_RAM,\n};\n\nenum amdgpu_gfx_tcc_ras_mem_id {\n\tAMDGPU_GFX_TCC_MEM1 = 1,\n};\n\nenum amdgpu_gfx_tca_ras_mem_id {\n\tAMDGPU_GFX_TCA_MEM1 = 1,\n};\n\nenum amdgpu_gfx_tci_ras_mem_id {\n\tAMDGPU_GFX_TCIW_MEM = 1,\n};\n\nenum amdgpu_gfx_tcp_ras_mem_id {\n\tAMDGPU_GFX_TCP_LFIFO0 = 1,\n\tAMDGPU_GFX_TCP_SET0BANK0_RAM,\n\tAMDGPU_GFX_TCP_SET0BANK1_RAM,\n\tAMDGPU_GFX_TCP_SET0BANK2_RAM,\n\tAMDGPU_GFX_TCP_SET0BANK3_RAM,\n\tAMDGPU_GFX_TCP_SET1BANK0_RAM,\n\tAMDGPU_GFX_TCP_SET1BANK1_RAM,\n\tAMDGPU_GFX_TCP_SET1BANK2_RAM,\n\tAMDGPU_GFX_TCP_SET1BANK3_RAM,\n\tAMDGPU_GFX_TCP_SET2BANK0_RAM,\n\tAMDGPU_GFX_TCP_SET2BANK1_RAM,\n\tAMDGPU_GFX_TCP_SET2BANK2_RAM,\n\tAMDGPU_GFX_TCP_SET2BANK3_RAM,\n\tAMDGPU_GFX_TCP_SET3BANK0_RAM,\n\tAMDGPU_GFX_TCP_SET3BANK1_RAM,\n\tAMDGPU_GFX_TCP_SET3BANK2_RAM,\n\tAMDGPU_GFX_TCP_SET3BANK3_RAM,\n\tAMDGPU_GFX_TCP_VM_FIFO,\n\tAMDGPU_GFX_TCP_DB_TAGRAM0,\n\tAMDGPU_GFX_TCP_DB_TAGRAM1,\n\tAMDGPU_GFX_TCP_DB_TAGRAM2,\n\tAMDGPU_GFX_TCP_DB_TAGRAM3,\n\tAMDGPU_GFX_TCP_UTCL1_LFIFO_PROBE0,\n\tAMDGPU_GFX_TCP_UTCL1_LFIFO_PROBE1,\n\tAMDGPU_GFX_TCP_CMD_FIFO,\n};\n\nenum amdgpu_gfx_td_ras_mem_id {\n\tAMDGPU_GFX_TD_UTD_CS_FIFO_MEM = 1,\n\tAMDGPU_GFX_TD_UTD_SS_FIFO_LO_MEM,\n\tAMDGPU_GFX_TD_UTD_SS_FIFO_HI_MEM,\n};\n\nenum amdgpu_gfx_tcx_ras_mem_id {\n\tAMDGPU_GFX_TCX_FIFOD0 = 0,\n\tAMDGPU_GFX_TCX_FIFOD1,\n\tAMDGPU_GFX_TCX_FIFOD2,\n\tAMDGPU_GFX_TCX_FIFOD3,\n\tAMDGPU_GFX_TCX_FIFOD4,\n\tAMDGPU_GFX_TCX_FIFOD5,\n\tAMDGPU_GFX_TCX_FIFOD6,\n\tAMDGPU_GFX_TCX_FIFOD7,\n\tAMDGPU_GFX_TCX_FIFOB0,\n\tAMDGPU_GFX_TCX_FIFOB1,\n\tAMDGPU_GFX_TCX_FIFOB2,\n\tAMDGPU_GFX_TCX_FIFOB3,\n\tAMDGPU_GFX_TCX_FIFOB4,\n\tAMDGPU_GFX_TCX_FIFOB5,\n\tAMDGPU_GFX_TCX_FIFOB6,\n\tAMDGPU_GFX_TCX_FIFOB7,\n\tAMDGPU_GFX_TCX_FIFOA0,\n\tAMDGPU_GFX_TCX_FIFOA1,\n\tAMDGPU_GFX_TCX_FIFOA2,\n\tAMDGPU_GFX_TCX_FIFOA3,\n\tAMDGPU_GFX_TCX_FIFOA4,\n\tAMDGPU_GFX_TCX_FIFOA5,\n\tAMDGPU_GFX_TCX_FIFOA6,\n\tAMDGPU_GFX_TCX_FIFOA7,\n\tAMDGPU_GFX_TCX_CFIFO0,\n\tAMDGPU_GFX_TCX_CFIFO1,\n\tAMDGPU_GFX_TCX_CFIFO2,\n\tAMDGPU_GFX_TCX_CFIFO3,\n\tAMDGPU_GFX_TCX_CFIFO4,\n\tAMDGPU_GFX_TCX_CFIFO5,\n\tAMDGPU_GFX_TCX_CFIFO6,\n\tAMDGPU_GFX_TCX_CFIFO7,\n\tAMDGPU_GFX_TCX_FIFO_ACKB0,\n\tAMDGPU_GFX_TCX_FIFO_ACKB1,\n\tAMDGPU_GFX_TCX_FIFO_ACKB2,\n\tAMDGPU_GFX_TCX_FIFO_ACKB3,\n\tAMDGPU_GFX_TCX_FIFO_ACKB4,\n\tAMDGPU_GFX_TCX_FIFO_ACKB5,\n\tAMDGPU_GFX_TCX_FIFO_ACKB6,\n\tAMDGPU_GFX_TCX_FIFO_ACKB7,\n\tAMDGPU_GFX_TCX_FIFO_ACKD0,\n\tAMDGPU_GFX_TCX_FIFO_ACKD1,\n\tAMDGPU_GFX_TCX_FIFO_ACKD2,\n\tAMDGPU_GFX_TCX_FIFO_ACKD3,\n\tAMDGPU_GFX_TCX_FIFO_ACKD4,\n\tAMDGPU_GFX_TCX_FIFO_ACKD5,\n\tAMDGPU_GFX_TCX_FIFO_ACKD6,\n\tAMDGPU_GFX_TCX_FIFO_ACKD7,\n\tAMDGPU_GFX_TCX_DST_FIFOA0,\n\tAMDGPU_GFX_TCX_DST_FIFOA1,\n\tAMDGPU_GFX_TCX_DST_FIFOA2,\n\tAMDGPU_GFX_TCX_DST_FIFOA3,\n\tAMDGPU_GFX_TCX_DST_FIFOA4,\n\tAMDGPU_GFX_TCX_DST_FIFOA5,\n\tAMDGPU_GFX_TCX_DST_FIFOA6,\n\tAMDGPU_GFX_TCX_DST_FIFOA7,\n\tAMDGPU_GFX_TCX_DST_FIFOB0,\n\tAMDGPU_GFX_TCX_DST_FIFOB1,\n\tAMDGPU_GFX_TCX_DST_FIFOB2,\n\tAMDGPU_GFX_TCX_DST_FIFOB3,\n\tAMDGPU_GFX_TCX_DST_FIFOB4,\n\tAMDGPU_GFX_TCX_DST_FIFOB5,\n\tAMDGPU_GFX_TCX_DST_FIFOB6,\n\tAMDGPU_GFX_TCX_DST_FIFOB7,\n\tAMDGPU_GFX_TCX_DST_FIFOD0,\n\tAMDGPU_GFX_TCX_DST_FIFOD1,\n\tAMDGPU_GFX_TCX_DST_FIFOD2,\n\tAMDGPU_GFX_TCX_DST_FIFOD3,\n\tAMDGPU_GFX_TCX_DST_FIFOD4,\n\tAMDGPU_GFX_TCX_DST_FIFOD5,\n\tAMDGPU_GFX_TCX_DST_FIFOD6,\n\tAMDGPU_GFX_TCX_DST_FIFOD7,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB0,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB1,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB2,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB3,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB4,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB5,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB6,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKB7,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD0,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD1,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD2,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD3,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD4,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD5,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD6,\n\tAMDGPU_GFX_TCX_DST_FIFO_ACKD7,\n};\n\nenum amdgpu_gfx_atc_l2_ras_mem_id {\n\tAMDGPU_GFX_ATC_L2_MEM0 = 0,\n};\n\nenum amdgpu_gfx_utcl2_ras_mem_id {\n\tAMDGPU_GFX_UTCL2_MEM0 = 0,\n};\n\nenum amdgpu_gfx_vml2_ras_mem_id {\n\tAMDGPU_GFX_VML2_MEM0 = 0,\n};\n\nenum amdgpu_gfx_vml2_walker_ras_mem_id {\n\tAMDGPU_GFX_VML2_WALKER_MEM0 = 0,\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_cp_mem_list[] = {\n\t{AMDGPU_GFX_CP_MEM1, \"CP_MEM1\"},\n\t{AMDGPU_GFX_CP_MEM2, \"CP_MEM2\"},\n\t{AMDGPU_GFX_CP_MEM3, \"CP_MEM3\"},\n\t{AMDGPU_GFX_CP_MEM4, \"CP_MEM4\"},\n\t{AMDGPU_GFX_CP_MEM5, \"CP_MEM5\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_gcea_mem_list[] = {\n\t{AMDGPU_GFX_GCEA_IOWR_CMDMEM, \"GCEA_IOWR_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_IORD_CMDMEM, \"GCEA_IORD_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_GMIWR_CMDMEM, \"GCEA_GMIWR_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_GMIRD_CMDMEM, \"GCEA_GMIRD_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_DRAMWR_CMDMEM, \"GCEA_DRAMWR_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_DRAMRD_CMDMEM, \"GCEA_DRAMRD_CMDMEM\"},\n\t{AMDGPU_GFX_GCEA_MAM_DMEM0, \"GCEA_MAM_DMEM0\"},\n\t{AMDGPU_GFX_GCEA_MAM_DMEM1, \"GCEA_MAM_DMEM1\"},\n\t{AMDGPU_GFX_GCEA_MAM_DMEM2, \"GCEA_MAM_DMEM2\"},\n\t{AMDGPU_GFX_GCEA_MAM_DMEM3, \"GCEA_MAM_DMEM3\"},\n\t{AMDGPU_GFX_GCEA_MAM_AMEM0, \"GCEA_MAM_AMEM0\"},\n\t{AMDGPU_GFX_GCEA_MAM_AMEM1, \"GCEA_MAM_AMEM1\"},\n\t{AMDGPU_GFX_GCEA_MAM_AMEM2, \"GCEA_MAM_AMEM2\"},\n\t{AMDGPU_GFX_GCEA_MAM_AMEM3, \"GCEA_MAM_AMEM3\"},\n\t{AMDGPU_GFX_GCEA_MAM_AFLUSH_BUFFER, \"GCEA_MAM_AFLUSH_BUFFER\"},\n\t{AMDGPU_GFX_GCEA_WRET_TAGMEM, \"GCEA_WRET_TAGMEM\"},\n\t{AMDGPU_GFX_GCEA_RRET_TAGMEM, \"GCEA_RRET_TAGMEM\"},\n\t{AMDGPU_GFX_GCEA_IOWR_DATAMEM, \"GCEA_IOWR_DATAMEM\"},\n\t{AMDGPU_GFX_GCEA_GMIWR_DATAMEM, \"GCEA_GMIWR_DATAMEM\"},\n\t{AMDGPU_GFX_GCEA_DRAM_DATAMEM, \"GCEA_DRAM_DATAMEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_gc_cane_mem_list[] = {\n\t{AMDGPU_GFX_GC_CANE_MEM0, \"GC_CANE_MEM0\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_gcutcl2_mem_list[] = {\n\t{AMDGPU_GFX_GCUTCL2_MEM2P512X95, \"GCUTCL2_MEM2P512X95\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_gds_mem_list[] = {\n\t{AMDGPU_GFX_GDS_MEM0, \"GDS_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_lds_mem_list[] = {\n\t{AMDGPU_GFX_LDS_BANK0, \"LDS_BANK0\"},\n\t{AMDGPU_GFX_LDS_BANK1, \"LDS_BANK1\"},\n\t{AMDGPU_GFX_LDS_BANK2, \"LDS_BANK2\"},\n\t{AMDGPU_GFX_LDS_BANK3, \"LDS_BANK3\"},\n\t{AMDGPU_GFX_LDS_BANK4, \"LDS_BANK4\"},\n\t{AMDGPU_GFX_LDS_BANK5, \"LDS_BANK5\"},\n\t{AMDGPU_GFX_LDS_BANK6, \"LDS_BANK6\"},\n\t{AMDGPU_GFX_LDS_BANK7, \"LDS_BANK7\"},\n\t{AMDGPU_GFX_LDS_BANK8, \"LDS_BANK8\"},\n\t{AMDGPU_GFX_LDS_BANK9, \"LDS_BANK9\"},\n\t{AMDGPU_GFX_LDS_BANK10, \"LDS_BANK10\"},\n\t{AMDGPU_GFX_LDS_BANK11, \"LDS_BANK11\"},\n\t{AMDGPU_GFX_LDS_BANK12, \"LDS_BANK12\"},\n\t{AMDGPU_GFX_LDS_BANK13, \"LDS_BANK13\"},\n\t{AMDGPU_GFX_LDS_BANK14, \"LDS_BANK14\"},\n\t{AMDGPU_GFX_LDS_BANK15, \"LDS_BANK15\"},\n\t{AMDGPU_GFX_LDS_BANK16, \"LDS_BANK16\"},\n\t{AMDGPU_GFX_LDS_BANK17, \"LDS_BANK17\"},\n\t{AMDGPU_GFX_LDS_BANK18, \"LDS_BANK18\"},\n\t{AMDGPU_GFX_LDS_BANK19, \"LDS_BANK19\"},\n\t{AMDGPU_GFX_LDS_BANK20, \"LDS_BANK20\"},\n\t{AMDGPU_GFX_LDS_BANK21, \"LDS_BANK21\"},\n\t{AMDGPU_GFX_LDS_BANK22, \"LDS_BANK22\"},\n\t{AMDGPU_GFX_LDS_BANK23, \"LDS_BANK23\"},\n\t{AMDGPU_GFX_LDS_BANK24, \"LDS_BANK24\"},\n\t{AMDGPU_GFX_LDS_BANK25, \"LDS_BANK25\"},\n\t{AMDGPU_GFX_LDS_BANK26, \"LDS_BANK26\"},\n\t{AMDGPU_GFX_LDS_BANK27, \"LDS_BANK27\"},\n\t{AMDGPU_GFX_LDS_BANK28, \"LDS_BANK28\"},\n\t{AMDGPU_GFX_LDS_BANK29, \"LDS_BANK29\"},\n\t{AMDGPU_GFX_LDS_BANK30, \"LDS_BANK30\"},\n\t{AMDGPU_GFX_LDS_BANK31, \"LDS_BANK31\"},\n\t{AMDGPU_GFX_LDS_SP_BUFFER_A, \"LDS_SP_BUFFER_A\"},\n\t{AMDGPU_GFX_LDS_SP_BUFFER_B, \"LDS_SP_BUFFER_B\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_rlc_mem_list[] = {\n\t{AMDGPU_GFX_RLC_GPMF32, \"RLC_GPMF32\"},\n\t{AMDGPU_GFX_RLC_RLCVF32, \"RLC_RLCVF32\"},\n\t{AMDGPU_GFX_RLC_SCRATCH, \"RLC_SCRATCH\"},\n\t{AMDGPU_GFX_RLC_SRM_ARAM, \"RLC_SRM_ARAM\"},\n\t{AMDGPU_GFX_RLC_SRM_DRAM, \"RLC_SRM_DRAM\"},\n\t{AMDGPU_GFX_RLC_TCTAG, \"RLC_TCTAG\"},\n\t{AMDGPU_GFX_RLC_SPM_SE, \"RLC_SPM_SE\"},\n\t{AMDGPU_GFX_RLC_SPM_GRBMT, \"RLC_SPM_GRBMT\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_sp_mem_list[] = {\n\t{AMDGPU_GFX_SP_SIMDID0, \"SP_SIMDID0\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_spi_mem_list[] = {\n\t{AMDGPU_GFX_SPI_MEM0, \"SPI_MEM0\"},\n\t{AMDGPU_GFX_SPI_MEM1, \"SPI_MEM1\"},\n\t{AMDGPU_GFX_SPI_MEM2, \"SPI_MEM2\"},\n\t{AMDGPU_GFX_SPI_MEM3, \"SPI_MEM3\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_sqc_mem_list[] = {\n\t{AMDGPU_GFX_SQC_INST_CACHE_A, \"SQC_INST_CACHE_A\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_B, \"SQC_INST_CACHE_B\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_TAG_A, \"SQC_INST_CACHE_TAG_A\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_TAG_B, \"SQC_INST_CACHE_TAG_B\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_MISS_FIFO_A, \"SQC_INST_CACHE_MISS_FIFO_A\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_MISS_FIFO_B, \"SQC_INST_CACHE_MISS_FIFO_B\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_GATCL1_MISS_FIFO_A, \"SQC_INST_CACHE_GATCL1_MISS_FIFO_A\"},\n\t{AMDGPU_GFX_SQC_INST_CACHE_GATCL1_MISS_FIFO_B, \"SQC_INST_CACHE_GATCL1_MISS_FIFO_B\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_A, \"SQC_DATA_CACHE_A\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_B, \"SQC_DATA_CACHE_B\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_TAG_A, \"SQC_DATA_CACHE_TAG_A\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_TAG_B, \"SQC_DATA_CACHE_TAG_B\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_MISS_FIFO_A, \"SQC_DATA_CACHE_MISS_FIFO_A\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_MISS_FIFO_B, \"SQC_DATA_CACHE_MISS_FIFO_B\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_HIT_FIFO_A, \"SQC_DATA_CACHE_HIT_FIFO_A\"},\n\t{AMDGPU_GFX_SQC_DATA_CACHE_HIT_FIFO_B, \"SQC_DATA_CACHE_HIT_FIFO_B\"},\n\t{AMDGPU_GFX_SQC_DIRTY_BIT_A, \"SQC_DIRTY_BIT_A\"},\n\t{AMDGPU_GFX_SQC_DIRTY_BIT_B, \"SQC_DIRTY_BIT_B\"},\n\t{AMDGPU_GFX_SQC_WRITE_DATA_BUFFER_CU0, \"SQC_WRITE_DATA_BUFFER_CU0\"},\n\t{AMDGPU_GFX_SQC_WRITE_DATA_BUFFER_CU1, \"SQC_WRITE_DATA_BUFFER_CU1\"},\n\t{AMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_DATA_CACHE_A, \"SQC_UTCL1_MISS_LFIFO_DATA_CACHE_A\"},\n\t{AMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_DATA_CACHE_B, \"SQC_UTCL1_MISS_LFIFO_DATA_CACHE_B\"},\n\t{AMDGPU_GFX_SQC_UTCL1_MISS_LFIFO_INST_CACHE, \"SQC_UTCL1_MISS_LFIFO_INST_CACHE\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_sq_mem_list[] = {\n\t{AMDGPU_GFX_SQ_SGPR_MEM0, \"SQ_SGPR_MEM0\"},\n\t{AMDGPU_GFX_SQ_SGPR_MEM1, \"SQ_SGPR_MEM1\"},\n\t{AMDGPU_GFX_SQ_SGPR_MEM2, \"SQ_SGPR_MEM2\"},\n\t{AMDGPU_GFX_SQ_SGPR_MEM3, \"SQ_SGPR_MEM3\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_ta_mem_list[] = {\n\t{AMDGPU_GFX_TA_FS_AFIFO_RAM_LO, \"TA_FS_AFIFO_RAM_LO\"},\n\t{AMDGPU_GFX_TA_FS_AFIFO_RAM_HI, \"TA_FS_AFIFO_RAM_HI\"},\n\t{AMDGPU_GFX_TA_FS_CFIFO_RAM, \"TA_FS_CFIFO_RAM\"},\n\t{AMDGPU_GFX_TA_FSX_LFIFO, \"TA_FSX_LFIFO\"},\n\t{AMDGPU_GFX_TA_FS_DFIFO_RAM, \"TA_FS_DFIFO_RAM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_tcc_mem_list[] = {\n\t{AMDGPU_GFX_TCC_MEM1, \"TCC_MEM1\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_tca_mem_list[] = {\n\t{AMDGPU_GFX_TCA_MEM1, \"TCA_MEM1\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_tci_mem_list[] = {\n\t{AMDGPU_GFX_TCIW_MEM, \"TCIW_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_tcp_mem_list[] = {\n\t{AMDGPU_GFX_TCP_LFIFO0, \"TCP_LFIFO0\"},\n\t{AMDGPU_GFX_TCP_SET0BANK0_RAM, \"TCP_SET0BANK0_RAM\"},\n\t{AMDGPU_GFX_TCP_SET0BANK1_RAM, \"TCP_SET0BANK1_RAM\"},\n\t{AMDGPU_GFX_TCP_SET0BANK2_RAM, \"TCP_SET0BANK2_RAM\"},\n\t{AMDGPU_GFX_TCP_SET0BANK3_RAM, \"TCP_SET0BANK3_RAM\"},\n\t{AMDGPU_GFX_TCP_SET1BANK0_RAM, \"TCP_SET1BANK0_RAM\"},\n\t{AMDGPU_GFX_TCP_SET1BANK1_RAM, \"TCP_SET1BANK1_RAM\"},\n\t{AMDGPU_GFX_TCP_SET1BANK2_RAM, \"TCP_SET1BANK2_RAM\"},\n\t{AMDGPU_GFX_TCP_SET1BANK3_RAM, \"TCP_SET1BANK3_RAM\"},\n\t{AMDGPU_GFX_TCP_SET2BANK0_RAM, \"TCP_SET2BANK0_RAM\"},\n\t{AMDGPU_GFX_TCP_SET2BANK1_RAM, \"TCP_SET2BANK1_RAM\"},\n\t{AMDGPU_GFX_TCP_SET2BANK2_RAM, \"TCP_SET2BANK2_RAM\"},\n\t{AMDGPU_GFX_TCP_SET2BANK3_RAM, \"TCP_SET2BANK3_RAM\"},\n\t{AMDGPU_GFX_TCP_SET3BANK0_RAM, \"TCP_SET3BANK0_RAM\"},\n\t{AMDGPU_GFX_TCP_SET3BANK1_RAM, \"TCP_SET3BANK1_RAM\"},\n\t{AMDGPU_GFX_TCP_SET3BANK2_RAM, \"TCP_SET3BANK2_RAM\"},\n\t{AMDGPU_GFX_TCP_SET3BANK3_RAM, \"TCP_SET3BANK3_RAM\"},\n\t{AMDGPU_GFX_TCP_VM_FIFO, \"TCP_VM_FIFO\"},\n\t{AMDGPU_GFX_TCP_DB_TAGRAM0, \"TCP_DB_TAGRAM0\"},\n\t{AMDGPU_GFX_TCP_DB_TAGRAM1, \"TCP_DB_TAGRAM1\"},\n\t{AMDGPU_GFX_TCP_DB_TAGRAM2, \"TCP_DB_TAGRAM2\"},\n\t{AMDGPU_GFX_TCP_DB_TAGRAM3, \"TCP_DB_TAGRAM3\"},\n\t{AMDGPU_GFX_TCP_UTCL1_LFIFO_PROBE0, \"TCP_UTCL1_LFIFO_PROBE0\"},\n\t{AMDGPU_GFX_TCP_UTCL1_LFIFO_PROBE1, \"TCP_UTCL1_LFIFO_PROBE1\"},\n\t{AMDGPU_GFX_TCP_CMD_FIFO, \"TCP_CMD_FIFO\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_td_mem_list[] = {\n\t{AMDGPU_GFX_TD_UTD_CS_FIFO_MEM, \"TD_UTD_CS_FIFO_MEM\"},\n\t{AMDGPU_GFX_TD_UTD_SS_FIFO_LO_MEM, \"TD_UTD_SS_FIFO_LO_MEM\"},\n\t{AMDGPU_GFX_TD_UTD_SS_FIFO_HI_MEM, \"TD_UTD_SS_FIFO_HI_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_tcx_mem_list[] = {\n\t{AMDGPU_GFX_TCX_FIFOD0, \"TCX_FIFOD0\"},\n\t{AMDGPU_GFX_TCX_FIFOD1, \"TCX_FIFOD1\"},\n\t{AMDGPU_GFX_TCX_FIFOD2, \"TCX_FIFOD2\"},\n\t{AMDGPU_GFX_TCX_FIFOD3, \"TCX_FIFOD3\"},\n\t{AMDGPU_GFX_TCX_FIFOD4, \"TCX_FIFOD4\"},\n\t{AMDGPU_GFX_TCX_FIFOD5, \"TCX_FIFOD5\"},\n\t{AMDGPU_GFX_TCX_FIFOD6, \"TCX_FIFOD6\"},\n\t{AMDGPU_GFX_TCX_FIFOD7, \"TCX_FIFOD7\"},\n\t{AMDGPU_GFX_TCX_FIFOB0, \"TCX_FIFOB0\"},\n\t{AMDGPU_GFX_TCX_FIFOB1, \"TCX_FIFOB1\"},\n\t{AMDGPU_GFX_TCX_FIFOB2, \"TCX_FIFOB2\"},\n\t{AMDGPU_GFX_TCX_FIFOB3, \"TCX_FIFOB3\"},\n\t{AMDGPU_GFX_TCX_FIFOB4, \"TCX_FIFOB4\"},\n\t{AMDGPU_GFX_TCX_FIFOB5, \"TCX_FIFOB5\"},\n\t{AMDGPU_GFX_TCX_FIFOB6, \"TCX_FIFOB6\"},\n\t{AMDGPU_GFX_TCX_FIFOB7, \"TCX_FIFOB7\"},\n\t{AMDGPU_GFX_TCX_FIFOA0, \"TCX_FIFOA0\"},\n\t{AMDGPU_GFX_TCX_FIFOA1, \"TCX_FIFOA1\"},\n\t{AMDGPU_GFX_TCX_FIFOA2, \"TCX_FIFOA2\"},\n\t{AMDGPU_GFX_TCX_FIFOA3, \"TCX_FIFOA3\"},\n\t{AMDGPU_GFX_TCX_FIFOA4, \"TCX_FIFOA4\"},\n\t{AMDGPU_GFX_TCX_FIFOA5, \"TCX_FIFOA5\"},\n\t{AMDGPU_GFX_TCX_FIFOA6, \"TCX_FIFOA6\"},\n\t{AMDGPU_GFX_TCX_FIFOA7, \"TCX_FIFOA7\"},\n\t{AMDGPU_GFX_TCX_CFIFO0, \"TCX_CFIFO0\"},\n\t{AMDGPU_GFX_TCX_CFIFO1, \"TCX_CFIFO1\"},\n\t{AMDGPU_GFX_TCX_CFIFO2, \"TCX_CFIFO2\"},\n\t{AMDGPU_GFX_TCX_CFIFO3, \"TCX_CFIFO3\"},\n\t{AMDGPU_GFX_TCX_CFIFO4, \"TCX_CFIFO4\"},\n\t{AMDGPU_GFX_TCX_CFIFO5, \"TCX_CFIFO5\"},\n\t{AMDGPU_GFX_TCX_CFIFO6, \"TCX_CFIFO6\"},\n\t{AMDGPU_GFX_TCX_CFIFO7, \"TCX_CFIFO7\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB0, \"TCX_FIFO_ACKB0\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB1, \"TCX_FIFO_ACKB1\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB2, \"TCX_FIFO_ACKB2\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB3, \"TCX_FIFO_ACKB3\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB4, \"TCX_FIFO_ACKB4\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB5, \"TCX_FIFO_ACKB5\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB6, \"TCX_FIFO_ACKB6\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKB7, \"TCX_FIFO_ACKB7\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD0, \"TCX_FIFO_ACKD0\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD1, \"TCX_FIFO_ACKD1\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD2, \"TCX_FIFO_ACKD2\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD3, \"TCX_FIFO_ACKD3\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD4, \"TCX_FIFO_ACKD4\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD5, \"TCX_FIFO_ACKD5\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD6, \"TCX_FIFO_ACKD6\"},\n\t{AMDGPU_GFX_TCX_FIFO_ACKD7, \"TCX_FIFO_ACKD7\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA0, \"TCX_DST_FIFOA0\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA1, \"TCX_DST_FIFOA1\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA2, \"TCX_DST_FIFOA2\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA3, \"TCX_DST_FIFOA3\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA4, \"TCX_DST_FIFOA4\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA5, \"TCX_DST_FIFOA5\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA6, \"TCX_DST_FIFOA6\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOA7, \"TCX_DST_FIFOA7\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB0, \"TCX_DST_FIFOB0\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB1, \"TCX_DST_FIFOB1\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB2, \"TCX_DST_FIFOB2\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB3, \"TCX_DST_FIFOB3\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB4, \"TCX_DST_FIFOB4\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB5, \"TCX_DST_FIFOB5\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB6, \"TCX_DST_FIFOB6\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOB7, \"TCX_DST_FIFOB7\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD0, \"TCX_DST_FIFOD0\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD1, \"TCX_DST_FIFOD1\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD2, \"TCX_DST_FIFOD2\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD3, \"TCX_DST_FIFOD3\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD4, \"TCX_DST_FIFOD4\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD5, \"TCX_DST_FIFOD5\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD6, \"TCX_DST_FIFOD6\"},\n\t{AMDGPU_GFX_TCX_DST_FIFOD7, \"TCX_DST_FIFOD7\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB0, \"TCX_DST_FIFO_ACKB0\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB1, \"TCX_DST_FIFO_ACKB1\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB2, \"TCX_DST_FIFO_ACKB2\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB3, \"TCX_DST_FIFO_ACKB3\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB4, \"TCX_DST_FIFO_ACKB4\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB5, \"TCX_DST_FIFO_ACKB5\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB6, \"TCX_DST_FIFO_ACKB6\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKB7, \"TCX_DST_FIFO_ACKB7\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD0, \"TCX_DST_FIFO_ACKD0\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD1, \"TCX_DST_FIFO_ACKD1\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD2, \"TCX_DST_FIFO_ACKD2\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD3, \"TCX_DST_FIFO_ACKD3\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD4, \"TCX_DST_FIFO_ACKD4\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD5, \"TCX_DST_FIFO_ACKD5\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD6, \"TCX_DST_FIFO_ACKD6\"},\n\t{AMDGPU_GFX_TCX_DST_FIFO_ACKD7, \"TCX_DST_FIFO_ACKD7\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_atc_l2_mem_list[] = {\n\t{AMDGPU_GFX_ATC_L2_MEM, \"ATC_L2_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_utcl2_mem_list[] = {\n\t{AMDGPU_GFX_UTCL2_MEM, \"UTCL2_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_vml2_mem_list[] = {\n\t{AMDGPU_GFX_VML2_MEM, \"VML2_MEM\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry gfx_v9_4_3_ras_vml2_walker_mem_list[] = {\n\t{AMDGPU_GFX_VML2_WALKER_MEM, \"VML2_WALKER_MEM\"},\n};\n\nstatic const struct amdgpu_gfx_ras_mem_id_entry gfx_v9_4_3_ras_mem_list_array[AMDGPU_GFX_MEM_TYPE_NUM] = {\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_cp_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_gcea_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_gc_cane_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_gcutcl2_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_gds_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_lds_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_rlc_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_sp_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_spi_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_sqc_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_sq_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_ta_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_tcc_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_tca_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_tci_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_tcp_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_td_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_tcx_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_atc_l2_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_utcl2_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_vml2_mem_list)\n\tAMDGPU_GFX_MEMID_ENT(gfx_v9_4_3_ras_vml2_walker_mem_list)\n};\n\nstatic const struct amdgpu_gfx_ras_reg_entry gfx_v9_4_3_ce_reg_list[] = {\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regRLC_CE_ERR_STATUS_LOW, regRLC_CE_ERR_STATUS_HIGH),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"RLC\"},\n\t    AMDGPU_GFX_RLC_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPC_CE_ERR_STATUS_LO, regCPC_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPC\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPF_CE_ERR_STATUS_LO, regCPF_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPF\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPG_CE_ERR_STATUS_LO, regCPG_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPG\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGDS_CE_ERR_STATUS_LO, regGDS_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"GDS\"},\n\t    AMDGPU_GFX_GDS_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGC_CANE_CE_ERR_STATUS_LO, regGC_CANE_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CANE\"},\n\t    AMDGPU_GFX_GC_CANE_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSPI_CE_ERR_STATUS_LO, regSPI_CE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SPI\"},\n\t    AMDGPU_GFX_SPI_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSP0_CE_ERR_STATUS_LO, regSP0_CE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SP0\"},\n\t    AMDGPU_GFX_SP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSP1_CE_ERR_STATUS_LO, regSP1_CE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SP1\"},\n\t    AMDGPU_GFX_SP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSQ_CE_ERR_STATUS_LO, regSQ_CE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SQ\"},\n\t    AMDGPU_GFX_SQ_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSQC_CE_EDC_LO, regSQC_CE_EDC_HI),\n\t    5, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SQC\"},\n\t    AMDGPU_GFX_SQC_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCX_CE_ERR_STATUS_LO, regTCX_CE_ERR_STATUS_HI),\n\t    2, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCX\"},\n\t    AMDGPU_GFX_TCX_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCC_CE_ERR_STATUS_LO, regTCC_CE_ERR_STATUS_HI),\n\t    16, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCC\"},\n\t    AMDGPU_GFX_TCC_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTA_CE_EDC_LO, regTA_CE_EDC_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TA\"},\n\t    AMDGPU_GFX_TA_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCI_CE_EDC_LO_REG, regTCI_CE_EDC_HI_REG),\n\t    31, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCI\"},\n\t    AMDGPU_GFX_TCI_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCP_CE_EDC_LO_REG, regTCP_CE_EDC_HI_REG),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCP\"},\n\t    AMDGPU_GFX_TCP_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTD_CE_EDC_LO, regTD_CE_EDC_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TD\"},\n\t    AMDGPU_GFX_TD_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGCEA_CE_ERR_STATUS_LO, regGCEA_CE_ERR_STATUS_HI),\n\t    16, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"GCEA\"},\n\t    AMDGPU_GFX_GCEA_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regLDS_CE_ERR_STATUS_LO, regLDS_CE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"LDS\"},\n\t    AMDGPU_GFX_LDS_MEM, 1},\n};\n\nstatic const struct amdgpu_gfx_ras_reg_entry gfx_v9_4_3_ue_reg_list[] = {\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regRLC_UE_ERR_STATUS_LOW, regRLC_UE_ERR_STATUS_HIGH),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"RLC\"},\n\t    AMDGPU_GFX_RLC_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPC_UE_ERR_STATUS_LO, regCPC_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPC\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPF_UE_ERR_STATUS_LO, regCPF_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPF\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regCPG_UE_ERR_STATUS_LO, regCPG_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CPG\"},\n\t    AMDGPU_GFX_CP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGDS_UE_ERR_STATUS_LO, regGDS_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"GDS\"},\n\t    AMDGPU_GFX_GDS_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGC_CANE_UE_ERR_STATUS_LO, regGC_CANE_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"CANE\"},\n\t    AMDGPU_GFX_GC_CANE_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSPI_UE_ERR_STATUS_LO, regSPI_UE_ERR_STATUS_HI),\n\t    1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SPI\"},\n\t    AMDGPU_GFX_SPI_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSP0_UE_ERR_STATUS_LO, regSP0_UE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SP0\"},\n\t    AMDGPU_GFX_SP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSP1_UE_ERR_STATUS_LO, regSP1_UE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SP1\"},\n\t    AMDGPU_GFX_SP_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSQ_UE_ERR_STATUS_LO, regSQ_UE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SQ\"},\n\t    AMDGPU_GFX_SQ_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regSQC_UE_EDC_LO, regSQC_UE_EDC_HI),\n\t    5, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SQC\"},\n\t    AMDGPU_GFX_SQC_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCX_UE_ERR_STATUS_LO, regTCX_UE_ERR_STATUS_HI),\n\t    2, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCX\"},\n\t    AMDGPU_GFX_TCX_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCC_UE_ERR_STATUS_LO, regTCC_UE_ERR_STATUS_HI),\n\t    16, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCC\"},\n\t    AMDGPU_GFX_TCC_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTA_UE_EDC_LO, regTA_UE_EDC_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TA\"},\n\t    AMDGPU_GFX_TA_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCI_UE_EDC_LO_REG, regTCI_UE_EDC_HI_REG),\n\t    31, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCI\"},\n\t    AMDGPU_GFX_TCI_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCP_UE_EDC_LO_REG, regTCP_UE_EDC_HI_REG),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCP\"},\n\t    AMDGPU_GFX_TCP_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTD_UE_EDC_LO, regTD_UE_EDC_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TD\"},\n\t    AMDGPU_GFX_TD_MEM, 8},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regTCA_UE_ERR_STATUS_LO, regTCA_UE_ERR_STATUS_HI),\n\t    2, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"TCA\"},\n\t    AMDGPU_GFX_TCA_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regGCEA_UE_ERR_STATUS_LO, regGCEA_UE_ERR_STATUS_HI),\n\t    16, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"GCEA\"},\n\t    AMDGPU_GFX_GCEA_MEM, 1},\n\t{{AMDGPU_RAS_REG_ENTRY(GC, 0, regLDS_UE_ERR_STATUS_LO, regLDS_UE_ERR_STATUS_HI),\n\t    10, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"LDS\"},\n\t    AMDGPU_GFX_LDS_MEM, 1},\n};\n\nstatic const struct soc15_reg_entry gfx_v9_4_3_ea_err_status_regs = {\n\tSOC15_REG_ENTRY(GC, 0, regGCEA_ERR_STATUS), 0, 1, 16\n};\n\nstatic void gfx_v9_4_3_inst_query_ras_err_count(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status, int xcc_id)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tunsigned long ce_count = 0, ue_count = 0;\n\tuint32_t i, j, k;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_3_ce_reg_list); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_3_ce_reg_list[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_3_ce_reg_list[i].reg_entry.reg_inst; k++) {\n\t\t\t\t \n\t\t\t\tif (gfx_v9_4_3_ce_reg_list[i].se_num > 1 ||\n\t\t\t\t    gfx_v9_4_3_ce_reg_list[i].reg_entry.reg_inst > 1)\n\t\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, j, 0, k, xcc_id);\n\n\t\t\t\tamdgpu_ras_inst_query_ras_error_count(adev,\n\t\t\t\t\t&(gfx_v9_4_3_ce_reg_list[i].reg_entry),\n\t\t\t\t\t1,\n\t\t\t\t\tgfx_v9_4_3_ras_mem_list_array[gfx_v9_4_3_ce_reg_list[i].mem_id_type].mem_id_ent,\n\t\t\t\t\tgfx_v9_4_3_ras_mem_list_array[gfx_v9_4_3_ce_reg_list[i].mem_id_type].size,\n\t\t\t\t\tGET_INST(GC, xcc_id),\n\t\t\t\t\tAMDGPU_RAS_ERROR__SINGLE_CORRECTABLE,\n\t\t\t\t\t&ce_count);\n\n\t\t\t\tamdgpu_ras_inst_query_ras_error_count(adev,\n\t\t\t\t\t&(gfx_v9_4_3_ue_reg_list[i].reg_entry),\n\t\t\t\t\t1,\n\t\t\t\t\tgfx_v9_4_3_ras_mem_list_array[gfx_v9_4_3_ue_reg_list[i].mem_id_type].mem_id_ent,\n\t\t\t\t\tgfx_v9_4_3_ras_mem_list_array[gfx_v9_4_3_ue_reg_list[i].mem_id_type].size,\n\t\t\t\t\tGET_INST(GC, xcc_id),\n\t\t\t\t\tAMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE,\n\t\t\t\t\t&ue_count);\n\t\t\t}\n\t\t}\n\t}\n\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\t \n\terr_data->ce_count += ce_count;\n\terr_data->ue_count += ue_count;\n}\n\nstatic void gfx_v9_4_3_inst_reset_ras_err_count(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status, int xcc_id)\n{\n\tuint32_t i, j, k;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < ARRAY_SIZE(gfx_v9_4_3_ce_reg_list); i++) {\n\t\tfor (j = 0; j < gfx_v9_4_3_ce_reg_list[i].se_num; j++) {\n\t\t\tfor (k = 0; k < gfx_v9_4_3_ce_reg_list[i].reg_entry.reg_inst; k++) {\n\t\t\t\t \n\t\t\t\tif (gfx_v9_4_3_ce_reg_list[i].se_num > 1 ||\n\t\t\t\t    gfx_v9_4_3_ce_reg_list[i].reg_entry.reg_inst > 1)\n\t\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, j, 0, k, xcc_id);\n\n\t\t\t\tamdgpu_ras_inst_reset_ras_error_count(adev,\n\t\t\t\t\t&(gfx_v9_4_3_ce_reg_list[i].reg_entry),\n\t\t\t\t\t1,\n\t\t\t\t\tGET_INST(GC, xcc_id));\n\n\t\t\t\tamdgpu_ras_inst_reset_ras_error_count(adev,\n\t\t\t\t\t&(gfx_v9_4_3_ue_reg_list[i].reg_entry),\n\t\t\t\t\t1,\n\t\t\t\t\tGET_INST(GC, xcc_id));\n\t\t\t}\n\t\t}\n\t}\n\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_inst_query_ea_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tuint32_t i, j;\n\tuint32_t reg_value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tfor (i = 0; i < gfx_v9_4_3_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_3_ea_err_status_regs.instance; j++) {\n\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, i, 0, j, xcc_id);\n\t\t\treg_value = RREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\tregGCEA_ERR_STATUS);\n\t\t\tif (REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_WRRSP_STATUS) ||\n\t\t\t    REG_GET_FIELD(reg_value, GCEA_ERR_STATUS, SDP_RDRSP_DATAPARITY_ERROR)) {\n\t\t\t\tdev_warn(adev->dev,\n\t\t\t\t\t\"GCEA err detected at instance: %d, status: 0x%x!\\n\",\n\t\t\t\t\tj, reg_value);\n\t\t\t}\n\t\t\t \n\t\t\treg_value = REG_SET_FIELD(reg_value, GCEA_ERR_STATUS,\n\t\t\t\t\t\t  CLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regGCEA_ERR_STATUS,\n\t\t\t\t\treg_value);\n\t\t}\n\t}\n\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_inst_query_utc_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regUTCL2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX UTCL2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regUTCL2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regVML2_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regVML2_MEM_ECC_STATUS, 0x3);\n\t}\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\tregVML2_WALKER_MEM_ECC_STATUS);\n\tif (data) {\n\t\tdev_warn(adev->dev, \"GFX VML2 Walker Mem Ecc Status: 0x%x!\\n\", data);\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regVML2_WALKER_MEM_ECC_STATUS,\n\t\t\t\t0x3);\n\t}\n}\n\nstatic void gfx_v9_4_3_log_cu_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t status, int xcc_id)\n{\n\tstruct amdgpu_cu_info *cu_info = &adev->gfx.cu_info;\n\tuint32_t i, simd, wave;\n\tuint32_t wave_status;\n\tuint32_t wave_pc_lo, wave_pc_hi;\n\tuint32_t wave_exec_lo, wave_exec_hi;\n\tuint32_t wave_inst_dw0, wave_inst_dw1;\n\tuint32_t wave_ib_sts;\n\n\tfor (i = 0; i < 32; i++) {\n\t\tif (!((i << 1) & status))\n\t\t\tcontinue;\n\n\t\tsimd = i / cu_info->max_waves_per_simd;\n\t\twave = i % cu_info->max_waves_per_simd;\n\n\t\twave_status = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_STATUS);\n\t\twave_pc_lo = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_PC_LO);\n\t\twave_pc_hi = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_PC_HI);\n\t\twave_exec_lo =\n\t\t\twave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_EXEC_LO);\n\t\twave_exec_hi =\n\t\t\twave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_EXEC_HI);\n\t\twave_inst_dw0 =\n\t\t\twave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_INST_DW0);\n\t\twave_inst_dw1 =\n\t\t\twave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_INST_DW1);\n\t\twave_ib_sts = wave_read_ind(adev, xcc_id, simd, wave, ixSQ_WAVE_IB_STS);\n\n\t\tdev_info(\n\t\t\tadev->dev,\n\t\t\t\"\\t SIMD %d, Wave %d: status 0x%x, pc 0x%llx, exec 0x%llx, inst 0x%llx, ib_sts 0x%x\\n\",\n\t\t\tsimd, wave, wave_status,\n\t\t\t((uint64_t)wave_pc_hi << 32 | wave_pc_lo),\n\t\t\t((uint64_t)wave_exec_hi << 32 | wave_exec_lo),\n\t\t\t((uint64_t)wave_inst_dw1 << 32 | wave_inst_dw0),\n\t\t\twave_ib_sts);\n\t}\n}\n\nstatic void gfx_v9_4_3_inst_query_sq_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\tuint32_t status;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines; se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se; sh_idx++) {\n\t\t\tfor (cu_idx = 0; cu_idx < adev->gfx.config.max_cu_per_sh; cu_idx++) {\n\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx, xcc_id);\n\t\t\t\tstatus = RREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t\t      regSQ_TIMEOUT_STATUS);\n\t\t\t\tif (status != 0) {\n\t\t\t\t\tdev_info(\n\t\t\t\t\t\tadev->dev,\n\t\t\t\t\t\t\"GFX Watchdog Timeout: SE %d, SH %d, CU %d\\n\",\n\t\t\t\t\t\tse_idx, sh_idx, cu_idx);\n\t\t\t\t\tgfx_v9_4_3_log_cu_timeout_status(\n\t\t\t\t\t\tadev, status, xcc_id);\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t\tregSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_inst_query_ras_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status, int xcc_id)\n{\n\tgfx_v9_4_3_inst_query_ea_err_status(adev, xcc_id);\n\tgfx_v9_4_3_inst_query_utc_err_status(adev, xcc_id);\n\tgfx_v9_4_3_inst_query_sq_timeout_status(adev, xcc_id);\n}\n\nstatic void gfx_v9_4_3_inst_reset_utc_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regUTCL2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regVML2_MEM_ECC_STATUS, 0x3);\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regVML2_WALKER_MEM_ECC_STATUS, 0x3);\n}\n\nstatic void gfx_v9_4_3_inst_reset_ea_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tuint32_t i, j;\n\tuint32_t value;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < gfx_v9_4_3_ea_err_status_regs.se_num; i++) {\n\t\tfor (j = 0; j < gfx_v9_4_3_ea_err_status_regs.instance; j++) {\n\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, i, 0, j, xcc_id);\n\t\t\tvalue = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regGCEA_ERR_STATUS);\n\t\t\tvalue = REG_SET_FIELD(value, GCEA_ERR_STATUS,\n\t\t\t\t\t\tCLEAR_ERROR_STATUS, 0x1);\n\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regGCEA_ERR_STATUS, value);\n\t\t}\n\t}\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_inst_reset_sq_timeout_status(struct amdgpu_device *adev,\n\t\t\t\t\tint xcc_id)\n{\n\tuint32_t se_idx, sh_idx, cu_idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (se_idx = 0; se_idx < adev->gfx.config.max_shader_engines; se_idx++) {\n\t\tfor (sh_idx = 0; sh_idx < adev->gfx.config.max_sh_per_se; sh_idx++) {\n\t\t\tfor (cu_idx = 0; cu_idx < adev->gfx.config.max_cu_per_sh; cu_idx++) {\n\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, se_idx, sh_idx,\n\t\t\t\t\t\t\tcu_idx, xcc_id);\n\t\t\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id),\n\t\t\t\t\t\tregSQ_TIMEOUT_STATUS, 0);\n\t\t\t}\n\t\t}\n\t}\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_inst_reset_ras_err_status(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status, int xcc_id)\n{\n\tgfx_v9_4_3_inst_reset_utc_err_status(adev, xcc_id);\n\tgfx_v9_4_3_inst_reset_ea_err_status(adev, xcc_id);\n\tgfx_v9_4_3_inst_reset_sq_timeout_status(adev, xcc_id);\n}\n\nstatic void gfx_v9_4_3_inst_enable_watchdog_timer(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status, int xcc_id)\n{\n\tuint32_t i;\n\tuint32_t data;\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, 0), regSQ_TIMEOUT_CONFIG);\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, TIMEOUT_FATAL_DISABLE,\n\t\t\t     amdgpu_watchdog_timer.timeout_fatal_disable ? 1 : 0);\n\n\tif (amdgpu_watchdog_timer.timeout_fatal_disable &&\n\t    (amdgpu_watchdog_timer.period < 1 ||\n\t     amdgpu_watchdog_timer.period > 0x23)) {\n\t\tdev_warn(adev->dev, \"Watchdog period range is 1 to 0x23\\n\");\n\t\tamdgpu_watchdog_timer.period = 0x23;\n\t}\n\tdata = REG_SET_FIELD(data, SQ_TIMEOUT_CONFIG, PERIOD_SEL,\n\t\t\t     amdgpu_watchdog_timer.period);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\tgfx_v9_4_3_xcc_select_se_sh(adev, i, 0xffffffff, 0xffffffff, xcc_id);\n\t\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regSQ_TIMEOUT_CONFIG, data);\n\t}\n\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\txcc_id);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void gfx_v9_4_3_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\tvoid *ras_error_status)\n{\n\tamdgpu_gfx_ras_error_func(adev, ras_error_status,\n\t\t\tgfx_v9_4_3_inst_query_ras_err_count);\n}\n\nstatic void gfx_v9_4_3_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tamdgpu_gfx_ras_error_func(adev, NULL, gfx_v9_4_3_inst_reset_ras_err_count);\n}\n\nstatic void gfx_v9_4_3_query_ras_error_status(struct amdgpu_device *adev)\n{\n\tamdgpu_gfx_ras_error_func(adev, NULL, gfx_v9_4_3_inst_query_ras_err_status);\n}\n\nstatic void gfx_v9_4_3_reset_ras_error_status(struct amdgpu_device *adev)\n{\n\tamdgpu_gfx_ras_error_func(adev, NULL, gfx_v9_4_3_inst_reset_ras_err_status);\n}\n\nstatic void gfx_v9_4_3_enable_watchdog_timer(struct amdgpu_device *adev)\n{\n\tamdgpu_gfx_ras_error_func(adev, NULL, gfx_v9_4_3_inst_enable_watchdog_timer);\n}\n\nstatic const struct amd_ip_funcs gfx_v9_4_3_ip_funcs = {\n\t.name = \"gfx_v9_4_3\",\n\t.early_init = gfx_v9_4_3_early_init,\n\t.late_init = gfx_v9_4_3_late_init,\n\t.sw_init = gfx_v9_4_3_sw_init,\n\t.sw_fini = gfx_v9_4_3_sw_fini,\n\t.hw_init = gfx_v9_4_3_hw_init,\n\t.hw_fini = gfx_v9_4_3_hw_fini,\n\t.suspend = gfx_v9_4_3_suspend,\n\t.resume = gfx_v9_4_3_resume,\n\t.is_idle = gfx_v9_4_3_is_idle,\n\t.wait_for_idle = gfx_v9_4_3_wait_for_idle,\n\t.soft_reset = gfx_v9_4_3_soft_reset,\n\t.set_clockgating_state = gfx_v9_4_3_set_clockgating_state,\n\t.set_powergating_state = gfx_v9_4_3_set_powergating_state,\n\t.get_clockgating_state = gfx_v9_4_3_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ring_funcs gfx_v9_4_3_ring_funcs_compute = {\n\t.type = AMDGPU_RING_TYPE_COMPUTE,\n\t.align_mask = 0xff,\n\t.nop = PACKET3(PACKET3_NOP, 0x3FFF),\n\t.support_64bit_ptrs = true,\n\t.get_rptr = gfx_v9_4_3_ring_get_rptr_compute,\n\t.get_wptr = gfx_v9_4_3_ring_get_wptr_compute,\n\t.set_wptr = gfx_v9_4_3_ring_set_wptr_compute,\n\t.emit_frame_size =\n\t\t20 +  \n\t\t7 +  \n\t\t5 +  \n\t\t7 +  \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +\n\t\t2 +  \n\t\t8 + 8 + 8 +  \n\t\t7 +  \n\t\t5 +  \n\t\t15,  \n\t.emit_ib_size =\t7,  \n\t.emit_ib = gfx_v9_4_3_ring_emit_ib_compute,\n\t.emit_fence = gfx_v9_4_3_ring_emit_fence,\n\t.emit_pipeline_sync = gfx_v9_4_3_ring_emit_pipeline_sync,\n\t.emit_vm_flush = gfx_v9_4_3_ring_emit_vm_flush,\n\t.emit_gds_switch = gfx_v9_4_3_ring_emit_gds_switch,\n\t.emit_hdp_flush = gfx_v9_4_3_ring_emit_hdp_flush,\n\t.test_ring = gfx_v9_4_3_ring_test_ring,\n\t.test_ib = gfx_v9_4_3_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.emit_wreg = gfx_v9_4_3_ring_emit_wreg,\n\t.emit_reg_wait = gfx_v9_4_3_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = gfx_v9_4_3_ring_emit_reg_write_reg_wait,\n\t.emit_mem_sync = gfx_v9_4_3_emit_mem_sync,\n\t.emit_wave_limit = gfx_v9_4_3_emit_wave_limit,\n};\n\nstatic const struct amdgpu_ring_funcs gfx_v9_4_3_ring_funcs_kiq = {\n\t.type = AMDGPU_RING_TYPE_KIQ,\n\t.align_mask = 0xff,\n\t.nop = PACKET3(PACKET3_NOP, 0x3FFF),\n\t.support_64bit_ptrs = true,\n\t.get_rptr = gfx_v9_4_3_ring_get_rptr_compute,\n\t.get_wptr = gfx_v9_4_3_ring_get_wptr_compute,\n\t.set_wptr = gfx_v9_4_3_ring_set_wptr_compute,\n\t.emit_frame_size =\n\t\t20 +  \n\t\t7 +  \n\t\t5 +  \n\t\t7 +  \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 5 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 7 +\n\t\t2 +  \n\t\t8 + 8 + 8,  \n\t.emit_ib_size =\t7,  \n\t.emit_fence = gfx_v9_4_3_ring_emit_fence_kiq,\n\t.test_ring = gfx_v9_4_3_ring_test_ring,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.emit_rreg = gfx_v9_4_3_ring_emit_rreg,\n\t.emit_wreg = gfx_v9_4_3_ring_emit_wreg,\n\t.emit_reg_wait = gfx_v9_4_3_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = gfx_v9_4_3_ring_emit_reg_write_reg_wait,\n};\n\nstatic void gfx_v9_4_3_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i, j, num_xcc;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tfor (i = 0; i < num_xcc; i++) {\n\t\tadev->gfx.kiq[i].ring.funcs = &gfx_v9_4_3_ring_funcs_kiq;\n\n\t\tfor (j = 0; j < adev->gfx.num_compute_rings; j++)\n\t\t\tadev->gfx.compute_ring[j + i * adev->gfx.num_compute_rings].funcs\n\t\t\t\t\t= &gfx_v9_4_3_ring_funcs_compute;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs gfx_v9_4_3_eop_irq_funcs = {\n\t.set = gfx_v9_4_3_set_eop_interrupt_state,\n\t.process = gfx_v9_4_3_eop_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs gfx_v9_4_3_priv_reg_irq_funcs = {\n\t.set = gfx_v9_4_3_set_priv_reg_fault_state,\n\t.process = gfx_v9_4_3_priv_reg_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs gfx_v9_4_3_priv_inst_irq_funcs = {\n\t.set = gfx_v9_4_3_set_priv_inst_fault_state,\n\t.process = gfx_v9_4_3_priv_inst_irq,\n};\n\nstatic void gfx_v9_4_3_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->gfx.eop_irq.num_types = AMDGPU_CP_IRQ_LAST;\n\tadev->gfx.eop_irq.funcs = &gfx_v9_4_3_eop_irq_funcs;\n\n\tadev->gfx.priv_reg_irq.num_types = 1;\n\tadev->gfx.priv_reg_irq.funcs = &gfx_v9_4_3_priv_reg_irq_funcs;\n\n\tadev->gfx.priv_inst_irq.num_types = 1;\n\tadev->gfx.priv_inst_irq.funcs = &gfx_v9_4_3_priv_inst_irq_funcs;\n}\n\nstatic void gfx_v9_4_3_set_rlc_funcs(struct amdgpu_device *adev)\n{\n\tadev->gfx.rlc.funcs = &gfx_v9_4_3_rlc_funcs;\n}\n\n\nstatic void gfx_v9_4_3_set_gds_init(struct amdgpu_device *adev)\n{\n\t \n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 3):\n\t\t \n\t\tadev->gds.gds_size = 0;\n\t\tbreak;\n\tdefault:\n\t\tadev->gds.gds_size = 0x10000;\n\t\tbreak;\n\t}\n\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 3):\n\t\t \n\t\tadev->gds.gds_compute_max_wave_id = 0;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tadev->gds.gds_compute_max_wave_id = 0x7ff;\n\t\tbreak;\n\t}\n\n\tadev->gds.gws_size = 64;\n\tadev->gds.oa_size = 16;\n}\n\nstatic void gfx_v9_4_3_set_user_cu_inactive_bitmap(struct amdgpu_device *adev,\n\t\t\t\t\t\t u32 bitmap, int xcc_id)\n{\n\tu32 data;\n\n\tif (!bitmap)\n\t\treturn;\n\n\tdata = bitmap << GC_USER_SHADER_ARRAY_CONFIG__INACTIVE_CUS__SHIFT;\n\tdata &= GC_USER_SHADER_ARRAY_CONFIG__INACTIVE_CUS_MASK;\n\n\tWREG32_SOC15(GC, GET_INST(GC, xcc_id), regGC_USER_SHADER_ARRAY_CONFIG, data);\n}\n\nstatic u32 gfx_v9_4_3_get_cu_active_bitmap(struct amdgpu_device *adev, int xcc_id)\n{\n\tu32 data, mask;\n\n\tdata = RREG32_SOC15(GC, GET_INST(GC, xcc_id), regCC_GC_SHADER_ARRAY_CONFIG);\n\tdata |= RREG32_SOC15(GC, GET_INST(GC, xcc_id), regGC_USER_SHADER_ARRAY_CONFIG);\n\n\tdata &= CC_GC_SHADER_ARRAY_CONFIG__INACTIVE_CUS_MASK;\n\tdata >>= CC_GC_SHADER_ARRAY_CONFIG__INACTIVE_CUS__SHIFT;\n\n\tmask = amdgpu_gfx_create_bitmask(adev->gfx.config.max_cu_per_sh);\n\n\treturn (~data) & mask;\n}\n\nstatic int gfx_v9_4_3_get_cu_info(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_cu_info *cu_info)\n{\n\tint i, j, k, counter, xcc_id, active_cu_number = 0;\n\tu32 mask, bitmap, ao_bitmap, ao_cu_mask = 0;\n\tunsigned disable_masks[4 * 4];\n\n\tif (!adev || !cu_info)\n\t\treturn -EINVAL;\n\n\t \n\tif (adev->gfx.config.max_shader_engines *\n\t\tadev->gfx.config.max_sh_per_se > 16)\n\t\treturn -EINVAL;\n\n\tamdgpu_gfx_parse_disable_cu(disable_masks,\n\t\t\t\t    adev->gfx.config.max_shader_engines,\n\t\t\t\t    adev->gfx.config.max_sh_per_se);\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (xcc_id = 0; xcc_id < NUM_XCC(adev->gfx.xcc_mask); xcc_id++) {\n\t\tfor (i = 0; i < adev->gfx.config.max_shader_engines; i++) {\n\t\t\tfor (j = 0; j < adev->gfx.config.max_sh_per_se; j++) {\n\t\t\t\tmask = 1;\n\t\t\t\tao_bitmap = 0;\n\t\t\t\tcounter = 0;\n\t\t\t\tgfx_v9_4_3_xcc_select_se_sh(adev, i, j, 0xffffffff, xcc_id);\n\t\t\t\tgfx_v9_4_3_set_user_cu_inactive_bitmap(\n\t\t\t\t\tadev,\n\t\t\t\t\tdisable_masks[i * adev->gfx.config.max_sh_per_se + j],\n\t\t\t\t\txcc_id);\n\t\t\t\tbitmap = gfx_v9_4_3_get_cu_active_bitmap(adev, xcc_id);\n\n\t\t\t\tcu_info->bitmap[xcc_id][i][j] = bitmap;\n\n\t\t\t\tfor (k = 0; k < adev->gfx.config.max_cu_per_sh; k++) {\n\t\t\t\t\tif (bitmap & mask) {\n\t\t\t\t\t\tif (counter < adev->gfx.config.max_cu_per_sh)\n\t\t\t\t\t\t\tao_bitmap |= mask;\n\t\t\t\t\t\tcounter++;\n\t\t\t\t\t}\n\t\t\t\t\tmask <<= 1;\n\t\t\t\t}\n\t\t\t\tactive_cu_number += counter;\n\t\t\t\tif (i < 2 && j < 2)\n\t\t\t\t\tao_cu_mask |= (ao_bitmap << (i * 16 + j * 8));\n\t\t\t\tcu_info->ao_cu_bitmap[i][j] = ao_bitmap;\n\t\t\t}\n\t\t}\n\t\tgfx_v9_4_3_xcc_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff,\n\t\t\t\t\t    xcc_id);\n\t}\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\tcu_info->number = active_cu_number;\n\tcu_info->ao_cu_mask = ao_cu_mask;\n\tcu_info->simd_per_cu = NUM_SIMD_PER_CU;\n\n\treturn 0;\n}\n\nconst struct amdgpu_ip_block_version gfx_v9_4_3_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GFX,\n\t.major = 9,\n\t.minor = 4,\n\t.rev = 0,\n\t.funcs = &gfx_v9_4_3_ip_funcs,\n};\n\nstatic int gfx_v9_4_3_xcp_resume(void *handle, uint32_t inst_mask)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tuint32_t tmp_mask;\n\tint i, r;\n\n\t \n\t \n\n\ttmp_mask = inst_mask;\n\tfor_each_inst(i, tmp_mask)\n\t\tgfx_v9_4_3_xcc_constants_init(adev, i);\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\ttmp_mask = inst_mask;\n\t\tfor_each_inst(i, tmp_mask) {\n\t\t\tr = gfx_v9_4_3_xcc_rlc_resume(adev, i);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\ttmp_mask = inst_mask;\n\tfor_each_inst(i, tmp_mask) {\n\t\tr = gfx_v9_4_3_xcc_cp_resume(adev, i);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int gfx_v9_4_3_xcp_suspend(void *handle, uint32_t inst_mask)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor_each_inst(i, inst_mask)\n\t\tgfx_v9_4_3_xcc_fini(adev, i);\n\n\treturn 0;\n}\n\nstruct amdgpu_xcp_ip_funcs gfx_v9_4_3_xcp_funcs = {\n\t.suspend = &gfx_v9_4_3_xcp_suspend,\n\t.resume = &gfx_v9_4_3_xcp_resume\n};\n\nstruct amdgpu_ras_block_hw_ops  gfx_v9_4_3_ras_ops = {\n\t.query_ras_error_count = &gfx_v9_4_3_query_ras_error_count,\n\t.reset_ras_error_count = &gfx_v9_4_3_reset_ras_error_count,\n\t.query_ras_error_status = &gfx_v9_4_3_query_ras_error_status,\n\t.reset_ras_error_status = &gfx_v9_4_3_reset_ras_error_status,\n};\n\nstruct amdgpu_gfx_ras gfx_v9_4_3_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &gfx_v9_4_3_ras_ops,\n\t},\n\t.enable_watchdog_timer = &gfx_v9_4_3_enable_watchdog_timer,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}