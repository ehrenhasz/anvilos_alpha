{
  "module_name": "amdgpu_vcn.c",
  "hash_id": "f51dffce8a79354697f58ec0542024d9a4a3d46fdfed7e9b4f81e09c4d1ca874",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vcn.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/dmi.h>\n#include <linux/pci.h>\n#include <linux/debugfs.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_vcn.h\"\n#include \"soc15d.h\"\n\n \n#define FIRMWARE_RAVEN\t\t\t\"amdgpu/raven_vcn.bin\"\n#define FIRMWARE_PICASSO\t\t\"amdgpu/picasso_vcn.bin\"\n#define FIRMWARE_RAVEN2\t\t\t\"amdgpu/raven2_vcn.bin\"\n#define FIRMWARE_ARCTURUS\t\t\"amdgpu/arcturus_vcn.bin\"\n#define FIRMWARE_RENOIR\t\t\t\"amdgpu/renoir_vcn.bin\"\n#define FIRMWARE_GREEN_SARDINE\t\t\"amdgpu/green_sardine_vcn.bin\"\n#define FIRMWARE_NAVI10\t\t\t\"amdgpu/navi10_vcn.bin\"\n#define FIRMWARE_NAVI14\t\t\t\"amdgpu/navi14_vcn.bin\"\n#define FIRMWARE_NAVI12\t\t\t\"amdgpu/navi12_vcn.bin\"\n#define FIRMWARE_SIENNA_CICHLID\t\t\"amdgpu/sienna_cichlid_vcn.bin\"\n#define FIRMWARE_NAVY_FLOUNDER\t\t\"amdgpu/navy_flounder_vcn.bin\"\n#define FIRMWARE_VANGOGH\t\t\"amdgpu/vangogh_vcn.bin\"\n#define FIRMWARE_DIMGREY_CAVEFISH\t\"amdgpu/dimgrey_cavefish_vcn.bin\"\n#define FIRMWARE_ALDEBARAN\t\t\"amdgpu/aldebaran_vcn.bin\"\n#define FIRMWARE_BEIGE_GOBY\t\t\"amdgpu/beige_goby_vcn.bin\"\n#define FIRMWARE_YELLOW_CARP\t\t\"amdgpu/yellow_carp_vcn.bin\"\n#define FIRMWARE_VCN_3_1_2\t\t\"amdgpu/vcn_3_1_2.bin\"\n#define FIRMWARE_VCN4_0_0\t\t\"amdgpu/vcn_4_0_0.bin\"\n#define FIRMWARE_VCN4_0_2\t\t\"amdgpu/vcn_4_0_2.bin\"\n#define FIRMWARE_VCN4_0_3\t\t\"amdgpu/vcn_4_0_3.bin\"\n#define FIRMWARE_VCN4_0_4\t\t\"amdgpu/vcn_4_0_4.bin\"\n\nMODULE_FIRMWARE(FIRMWARE_RAVEN);\nMODULE_FIRMWARE(FIRMWARE_PICASSO);\nMODULE_FIRMWARE(FIRMWARE_RAVEN2);\nMODULE_FIRMWARE(FIRMWARE_ARCTURUS);\nMODULE_FIRMWARE(FIRMWARE_RENOIR);\nMODULE_FIRMWARE(FIRMWARE_GREEN_SARDINE);\nMODULE_FIRMWARE(FIRMWARE_ALDEBARAN);\nMODULE_FIRMWARE(FIRMWARE_NAVI10);\nMODULE_FIRMWARE(FIRMWARE_NAVI14);\nMODULE_FIRMWARE(FIRMWARE_NAVI12);\nMODULE_FIRMWARE(FIRMWARE_SIENNA_CICHLID);\nMODULE_FIRMWARE(FIRMWARE_NAVY_FLOUNDER);\nMODULE_FIRMWARE(FIRMWARE_VANGOGH);\nMODULE_FIRMWARE(FIRMWARE_DIMGREY_CAVEFISH);\nMODULE_FIRMWARE(FIRMWARE_BEIGE_GOBY);\nMODULE_FIRMWARE(FIRMWARE_YELLOW_CARP);\nMODULE_FIRMWARE(FIRMWARE_VCN_3_1_2);\nMODULE_FIRMWARE(FIRMWARE_VCN4_0_0);\nMODULE_FIRMWARE(FIRMWARE_VCN4_0_2);\nMODULE_FIRMWARE(FIRMWARE_VCN4_0_3);\nMODULE_FIRMWARE(FIRMWARE_VCN4_0_4);\n\nstatic void amdgpu_vcn_idle_work_handler(struct work_struct *work);\n\nint amdgpu_vcn_early_init(struct amdgpu_device *adev)\n{\n\tchar ucode_prefix[30];\n\tchar fw_name[40];\n\tint r;\n\n\tamdgpu_ucode_ip_version_decode(adev, UVD_HWIP, ucode_prefix, sizeof(ucode_prefix));\n\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s.bin\", ucode_prefix);\n\tr = amdgpu_ucode_request(adev, &adev->vcn.fw, fw_name);\n\tif (r)\n\t\tamdgpu_ucode_release(&adev->vcn.fw);\n\n\treturn r;\n}\n\nint amdgpu_vcn_sw_init(struct amdgpu_device *adev)\n{\n\tunsigned long bo_size;\n\tconst struct common_firmware_header *hdr;\n\tunsigned char fw_check;\n\tunsigned int fw_shared_size, log_offset;\n\tint i, r;\n\n\tINIT_DELAYED_WORK(&adev->vcn.idle_work, amdgpu_vcn_idle_work_handler);\n\tmutex_init(&adev->vcn.vcn_pg_lock);\n\tmutex_init(&adev->vcn.vcn1_jpeg1_workaround);\n\tatomic_set(&adev->vcn.total_submission_cnt, 0);\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++)\n\t\tatomic_set(&adev->vcn.inst[i].dpg_enc_submission_cnt, 0);\n\n\tif ((adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) &&\n\t    (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG))\n\t\tadev->vcn.indirect_sram = true;\n\n\t \n\tif (adev->ip_versions[UVD_HWIP][0] == IP_VERSION(3, 0, 2)) {\n\t\tconst char *bios_ver = dmi_get_system_info(DMI_BIOS_VERSION);\n\n\t\tif (bios_ver && (!strncmp(\"F7A0113\", bios_ver, 7) ||\n\t\t     !strncmp(\"F7A0114\", bios_ver, 7))) {\n\t\t\tadev->vcn.indirect_sram = false;\n\t\t\tdev_info(adev->dev,\n\t\t\t\t\"Steam Deck quirk: indirect SRAM disabled on BIOS %s\\n\", bios_ver);\n\t\t}\n\t}\n\n\thdr = (const struct common_firmware_header *)adev->vcn.fw->data;\n\tadev->vcn.fw_version = le32_to_cpu(hdr->ucode_version);\n\n\t \n\tfw_check = (le32_to_cpu(hdr->ucode_version) >> 20) & 0xf;\n\tif (fw_check) {\n\t\tunsigned int dec_ver, enc_major, enc_minor, vep, fw_rev;\n\n\t\tfw_rev = le32_to_cpu(hdr->ucode_version) & 0xfff;\n\t\tenc_minor = (le32_to_cpu(hdr->ucode_version) >> 12) & 0xff;\n\t\tenc_major = fw_check;\n\t\tdec_ver = (le32_to_cpu(hdr->ucode_version) >> 24) & 0xf;\n\t\tvep = (le32_to_cpu(hdr->ucode_version) >> 28) & 0xf;\n\t\tDRM_INFO(\"Found VCN firmware Version ENC: %u.%u DEC: %u VEP: %u Revision: %u\\n\",\n\t\t\tenc_major, enc_minor, dec_ver, vep, fw_rev);\n\t} else {\n\t\tunsigned int version_major, version_minor, family_id;\n\n\t\tfamily_id = le32_to_cpu(hdr->ucode_version) & 0xff;\n\t\tversion_major = (le32_to_cpu(hdr->ucode_version) >> 24) & 0xff;\n\t\tversion_minor = (le32_to_cpu(hdr->ucode_version) >> 8) & 0xff;\n\t\tDRM_INFO(\"Found VCN firmware Version: %u.%u Family ID: %u\\n\",\n\t\t\tversion_major, version_minor, family_id);\n\t}\n\n\tbo_size = AMDGPU_VCN_STACK_SIZE + AMDGPU_VCN_CONTEXT_SIZE;\n\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP)\n\t\tbo_size += AMDGPU_GPU_PAGE_ALIGN(le32_to_cpu(hdr->ucode_size_bytes) + 8);\n\n\tif (adev->ip_versions[UVD_HWIP][0] >= IP_VERSION(4, 0, 0)) {\n\t\tfw_shared_size = AMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_vcn4_fw_shared));\n\t\tlog_offset = offsetof(struct amdgpu_vcn4_fw_shared, fw_log);\n\t} else {\n\t\tfw_shared_size = AMDGPU_GPU_PAGE_ALIGN(sizeof(struct amdgpu_fw_shared));\n\t\tlog_offset = offsetof(struct amdgpu_fw_shared, fw_log);\n\t}\n\n\tbo_size += fw_shared_size;\n\n\tif (amdgpu_vcnfw_log)\n\t\tbo_size += AMDGPU_VCNFW_LOG_SIZE;\n\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\tcontinue;\n\n\t\tr = amdgpu_bo_create_kernel(adev, bo_size, PAGE_SIZE,\n\t\t\t\t\t    AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t    &adev->vcn.inst[i].vcpu_bo,\n\t\t\t\t\t    &adev->vcn.inst[i].gpu_addr,\n\t\t\t\t\t    &adev->vcn.inst[i].cpu_addr);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%d) failed to allocate vcn bo\\n\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tadev->vcn.inst[i].fw_shared.cpu_addr = adev->vcn.inst[i].cpu_addr +\n\t\t\t\tbo_size - fw_shared_size;\n\t\tadev->vcn.inst[i].fw_shared.gpu_addr = adev->vcn.inst[i].gpu_addr +\n\t\t\t\tbo_size - fw_shared_size;\n\n\t\tadev->vcn.inst[i].fw_shared.mem_size = fw_shared_size;\n\n\t\tif (amdgpu_vcnfw_log) {\n\t\t\tadev->vcn.inst[i].fw_shared.cpu_addr -= AMDGPU_VCNFW_LOG_SIZE;\n\t\t\tadev->vcn.inst[i].fw_shared.gpu_addr -= AMDGPU_VCNFW_LOG_SIZE;\n\t\t\tadev->vcn.inst[i].fw_shared.log_offset = log_offset;\n\t\t}\n\n\t\tif (adev->vcn.indirect_sram) {\n\t\t\tr = amdgpu_bo_create_kernel(adev, 64 * 2 * 4, PAGE_SIZE,\n\t\t\t\t\tAMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t&adev->vcn.inst[i].dpg_sram_bo,\n\t\t\t\t\t&adev->vcn.inst[i].dpg_sram_gpu_addr,\n\t\t\t\t\t&adev->vcn.inst[i].dpg_sram_cpu_addr);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev, \"VCN %d (%d) failed to allocate DPG bo\\n\", i, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_vcn_sw_fini(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\tfor (j = 0; j < adev->vcn.num_vcn_inst; ++j) {\n\t\tif (adev->vcn.harvest_config & (1 << j))\n\t\t\tcontinue;\n\n\t\tamdgpu_bo_free_kernel(\n\t\t\t&adev->vcn.inst[j].dpg_sram_bo,\n\t\t\t&adev->vcn.inst[j].dpg_sram_gpu_addr,\n\t\t\t(void **)&adev->vcn.inst[j].dpg_sram_cpu_addr);\n\n\t\tkvfree(adev->vcn.inst[j].saved_bo);\n\n\t\tamdgpu_bo_free_kernel(&adev->vcn.inst[j].vcpu_bo,\n\t\t\t\t\t  &adev->vcn.inst[j].gpu_addr,\n\t\t\t\t\t  (void **)&adev->vcn.inst[j].cpu_addr);\n\n\t\tamdgpu_ring_fini(&adev->vcn.inst[j].ring_dec);\n\n\t\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i)\n\t\t\tamdgpu_ring_fini(&adev->vcn.inst[j].ring_enc[i]);\n\t}\n\n\tamdgpu_ucode_release(&adev->vcn.fw);\n\tmutex_destroy(&adev->vcn.vcn1_jpeg1_workaround);\n\tmutex_destroy(&adev->vcn.vcn_pg_lock);\n\n\treturn 0;\n}\n\n \nstatic bool amdgpu_vcn_using_unified_queue(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tbool ret = false;\n\n\tif (adev->ip_versions[UVD_HWIP][0] >= IP_VERSION(4, 0, 0))\n\t\tret = true;\n\n\treturn ret;\n}\n\nbool amdgpu_vcn_is_disabled_vcn(struct amdgpu_device *adev, enum vcn_ring_type type, uint32_t vcn_instance)\n{\n\tbool ret = false;\n\tint vcn_config = adev->vcn.vcn_config[vcn_instance];\n\n\tif ((type == VCN_ENCODE_RING) && (vcn_config & VCN_BLOCK_ENCODE_DISABLE_MASK))\n\t\tret = true;\n\telse if ((type == VCN_DECODE_RING) && (vcn_config & VCN_BLOCK_DECODE_DISABLE_MASK))\n\t\tret = true;\n\telse if ((type == VCN_UNIFIED_RING) && (vcn_config & VCN_BLOCK_QUEUE_DISABLE_MASK))\n\t\tret = true;\n\n\treturn ret;\n}\n\nint amdgpu_vcn_suspend(struct amdgpu_device *adev)\n{\n\tunsigned int size;\n\tvoid *ptr;\n\tint i, idx;\n\n\tbool in_ras_intr = amdgpu_ras_intr_triggered();\n\n\tcancel_delayed_work_sync(&adev->vcn.idle_work);\n\n\t \n\tif (in_ras_intr)\n\t\treturn 0;\n\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; ++i) {\n\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\tcontinue;\n\t\tif (adev->vcn.inst[i].vcpu_bo == NULL)\n\t\t\treturn 0;\n\n\t\tsize = amdgpu_bo_size(adev->vcn.inst[i].vcpu_bo);\n\t\tptr = adev->vcn.inst[i].cpu_addr;\n\n\t\tadev->vcn.inst[i].saved_bo = kvmalloc(size, GFP_KERNEL);\n\t\tif (!adev->vcn.inst[i].saved_bo)\n\t\t\treturn -ENOMEM;\n\n\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\tmemcpy_fromio(adev->vcn.inst[i].saved_bo, ptr, size);\n\t\t\tdrm_dev_exit(idx);\n\t\t}\n\t}\n\treturn 0;\n}\n\nint amdgpu_vcn_resume(struct amdgpu_device *adev)\n{\n\tunsigned int size;\n\tvoid *ptr;\n\tint i, idx;\n\n\tfor (i = 0; i < adev->vcn.num_vcn_inst; ++i) {\n\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\tcontinue;\n\t\tif (adev->vcn.inst[i].vcpu_bo == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tsize = amdgpu_bo_size(adev->vcn.inst[i].vcpu_bo);\n\t\tptr = adev->vcn.inst[i].cpu_addr;\n\n\t\tif (adev->vcn.inst[i].saved_bo != NULL) {\n\t\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\t\tmemcpy_toio(ptr, adev->vcn.inst[i].saved_bo, size);\n\t\t\t\tdrm_dev_exit(idx);\n\t\t\t}\n\t\t\tkvfree(adev->vcn.inst[i].saved_bo);\n\t\t\tadev->vcn.inst[i].saved_bo = NULL;\n\t\t} else {\n\t\t\tconst struct common_firmware_header *hdr;\n\t\t\tunsigned int offset;\n\n\t\t\thdr = (const struct common_firmware_header *)adev->vcn.fw->data;\n\t\t\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\t\t\toffset = le32_to_cpu(hdr->ucode_array_offset_bytes);\n\t\t\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\t\t\tmemcpy_toio(adev->vcn.inst[i].cpu_addr, adev->vcn.fw->data + offset,\n\t\t\t\t\t\t    le32_to_cpu(hdr->ucode_size_bytes));\n\t\t\t\t\tdrm_dev_exit(idx);\n\t\t\t\t}\n\t\t\t\tsize -= le32_to_cpu(hdr->ucode_size_bytes);\n\t\t\t\tptr += le32_to_cpu(hdr->ucode_size_bytes);\n\t\t\t}\n\t\t\tmemset_io(ptr, 0, size);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_vcn_idle_work_handler(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(work, struct amdgpu_device, vcn.idle_work.work);\n\tunsigned int fences = 0, fence[AMDGPU_MAX_VCN_INSTANCES] = {0};\n\tunsigned int i, j;\n\tint r = 0;\n\n\tfor (j = 0; j < adev->vcn.num_vcn_inst; ++j) {\n\t\tif (adev->vcn.harvest_config & (1 << j))\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i)\n\t\t\tfence[j] += amdgpu_fence_count_emitted(&adev->vcn.inst[j].ring_enc[i]);\n\n\t\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\t{\n\t\t\tstruct dpg_pause_state new_state;\n\n\t\t\tif (fence[j] ||\n\t\t\t\tunlikely(atomic_read(&adev->vcn.inst[j].dpg_enc_submission_cnt)))\n\t\t\t\tnew_state.fw_based = VCN_DPG_STATE__PAUSE;\n\t\t\telse\n\t\t\t\tnew_state.fw_based = VCN_DPG_STATE__UNPAUSE;\n\n\t\t\tadev->vcn.pause_dpg_mode(adev, j, &new_state);\n\t\t}\n\n\t\tfence[j] += amdgpu_fence_count_emitted(&adev->vcn.inst[j].ring_dec);\n\t\tfences += fence[j];\n\t}\n\n\tif (!fences && !atomic_read(&adev->vcn.total_submission_cnt)) {\n\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCN,\n\t\t       AMD_PG_STATE_GATE);\n\t\tr = amdgpu_dpm_switch_power_profile(adev, PP_SMC_POWER_PROFILE_VIDEO,\n\t\t\t\tfalse);\n\t\tif (r)\n\t\t\tdev_warn(adev->dev, \"(%d) failed to disable video power profile mode\\n\", r);\n\t} else {\n\t\tschedule_delayed_work(&adev->vcn.idle_work, VCN_IDLE_TIMEOUT);\n\t}\n}\n\nvoid amdgpu_vcn_ring_begin_use(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tint r = 0;\n\n\tatomic_inc(&adev->vcn.total_submission_cnt);\n\n\tif (!cancel_delayed_work_sync(&adev->vcn.idle_work)) {\n\t\tr = amdgpu_dpm_switch_power_profile(adev, PP_SMC_POWER_PROFILE_VIDEO,\n\t\t\t\ttrue);\n\t\tif (r)\n\t\t\tdev_warn(adev->dev, \"(%d) failed to switch to video power profile mode\\n\", r);\n\t}\n\n\tmutex_lock(&adev->vcn.vcn_pg_lock);\n\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCN,\n\t       AMD_PG_STATE_UNGATE);\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG)\t{\n\t\tstruct dpg_pause_state new_state;\n\n\t\tif (ring->funcs->type == AMDGPU_RING_TYPE_VCN_ENC) {\n\t\t\tatomic_inc(&adev->vcn.inst[ring->me].dpg_enc_submission_cnt);\n\t\t\tnew_state.fw_based = VCN_DPG_STATE__PAUSE;\n\t\t} else {\n\t\t\tunsigned int fences = 0;\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < adev->vcn.num_enc_rings; ++i)\n\t\t\t\tfences += amdgpu_fence_count_emitted(&adev->vcn.inst[ring->me].ring_enc[i]);\n\n\t\t\tif (fences || atomic_read(&adev->vcn.inst[ring->me].dpg_enc_submission_cnt))\n\t\t\t\tnew_state.fw_based = VCN_DPG_STATE__PAUSE;\n\t\t\telse\n\t\t\t\tnew_state.fw_based = VCN_DPG_STATE__UNPAUSE;\n\t\t}\n\n\t\tadev->vcn.pause_dpg_mode(adev, ring->me, &new_state);\n\t}\n\tmutex_unlock(&adev->vcn.vcn_pg_lock);\n}\n\nvoid amdgpu_vcn_ring_end_use(struct amdgpu_ring *ring)\n{\n\tif (ring->adev->pg_flags & AMD_PG_SUPPORT_VCN_DPG &&\n\t\tring->funcs->type == AMDGPU_RING_TYPE_VCN_ENC)\n\t\tatomic_dec(&ring->adev->vcn.inst[ring->me].dpg_enc_submission_cnt);\n\n\tatomic_dec(&ring->adev->vcn.total_submission_cnt);\n\n\tschedule_delayed_work(&ring->adev->vcn.idle_work, VCN_IDLE_TIMEOUT);\n}\n\nint amdgpu_vcn_dec_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t tmp = 0;\n\tunsigned int i;\n\tint r;\n\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tWREG32(adev->vcn.inst[ring->me].external.scratch9, 0xCAFEDEAD);\n\tr = amdgpu_ring_alloc(ring, 3);\n\tif (r)\n\t\treturn r;\n\tamdgpu_ring_write(ring, PACKET0(adev->vcn.internal.scratch9, 0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(adev->vcn.inst[ring->me].external.scratch9);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\nint amdgpu_vcn_dec_sw_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t rptr;\n\tunsigned int i;\n\tint r;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tr = amdgpu_ring_alloc(ring, 16);\n\tif (r)\n\t\treturn r;\n\n\trptr = amdgpu_ring_get_rptr(ring);\n\n\tamdgpu_ring_write(ring, VCN_DEC_SW_CMD_END);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (amdgpu_ring_get_rptr(ring) != rptr)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\nstatic int amdgpu_vcn_dec_send_msg(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_ib *ib_msg,\n\t\t\t\t   struct dma_fence **fence)\n{\n\tu64 addr = AMDGPU_GPU_PAGE_ALIGN(ib_msg->gpu_addr);\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *f = NULL;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tint i, r;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, NULL, NULL,\n\t\t\t\t     64, AMDGPU_IB_POOL_DIRECT,\n\t\t\t\t     &job);\n\tif (r)\n\t\tgoto err;\n\n\tib = &job->ibs[0];\n\tib->ptr[0] = PACKET0(adev->vcn.internal.data0, 0);\n\tib->ptr[1] = addr;\n\tib->ptr[2] = PACKET0(adev->vcn.internal.data1, 0);\n\tib->ptr[3] = addr >> 32;\n\tib->ptr[4] = PACKET0(adev->vcn.internal.cmd, 0);\n\tib->ptr[5] = 0;\n\tfor (i = 6; i < 16; i += 2) {\n\t\tib->ptr[i] = PACKET0(adev->vcn.internal.nop, 0);\n\t\tib->ptr[i+1] = 0;\n\t}\n\tib->length_dw = 16;\n\n\tr = amdgpu_job_submit_direct(job, ring, &f);\n\tif (r)\n\t\tgoto err_free;\n\n\tamdgpu_ib_free(adev, ib_msg, f);\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\n\treturn 0;\n\nerr_free:\n\tamdgpu_job_free(job);\nerr:\n\tamdgpu_ib_free(adev, ib_msg, f);\n\treturn r;\n}\n\nstatic int amdgpu_vcn_dec_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\tstruct amdgpu_ib *ib)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t *msg;\n\tint r, i;\n\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, AMDGPU_GPU_PAGE_SIZE * 2,\n\t\t\tAMDGPU_IB_POOL_DIRECT,\n\t\t\tib);\n\tif (r)\n\t\treturn r;\n\n\tmsg = (uint32_t *)AMDGPU_GPU_PAGE_ALIGN((unsigned long)ib->ptr);\n\tmsg[0] = cpu_to_le32(0x00000028);\n\tmsg[1] = cpu_to_le32(0x00000038);\n\tmsg[2] = cpu_to_le32(0x00000001);\n\tmsg[3] = cpu_to_le32(0x00000000);\n\tmsg[4] = cpu_to_le32(handle);\n\tmsg[5] = cpu_to_le32(0x00000000);\n\tmsg[6] = cpu_to_le32(0x00000001);\n\tmsg[7] = cpu_to_le32(0x00000028);\n\tmsg[8] = cpu_to_le32(0x00000010);\n\tmsg[9] = cpu_to_le32(0x00000000);\n\tmsg[10] = cpu_to_le32(0x00000007);\n\tmsg[11] = cpu_to_le32(0x00000000);\n\tmsg[12] = cpu_to_le32(0x00000780);\n\tmsg[13] = cpu_to_le32(0x00000440);\n\tfor (i = 14; i < 1024; ++i)\n\t\tmsg[i] = cpu_to_le32(0x0);\n\n\treturn 0;\n}\n\nstatic int amdgpu_vcn_dec_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t\t  struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t *msg;\n\tint r, i;\n\n\tmemset(ib, 0, sizeof(*ib));\n\tr = amdgpu_ib_get(adev, NULL, AMDGPU_GPU_PAGE_SIZE * 2,\n\t\t\tAMDGPU_IB_POOL_DIRECT,\n\t\t\tib);\n\tif (r)\n\t\treturn r;\n\n\tmsg = (uint32_t *)AMDGPU_GPU_PAGE_ALIGN((unsigned long)ib->ptr);\n\tmsg[0] = cpu_to_le32(0x00000028);\n\tmsg[1] = cpu_to_le32(0x00000018);\n\tmsg[2] = cpu_to_le32(0x00000000);\n\tmsg[3] = cpu_to_le32(0x00000002);\n\tmsg[4] = cpu_to_le32(handle);\n\tmsg[5] = cpu_to_le32(0x00000000);\n\tfor (i = 6; i < 1024; ++i)\n\t\tmsg[i] = cpu_to_le32(0x0);\n\n\treturn 0;\n}\n\nint amdgpu_vcn_dec_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_ib ib;\n\tlong r;\n\n\tr = amdgpu_vcn_dec_get_create_msg(ring, 1, &ib);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vcn_dec_send_msg(ring, &ib, NULL);\n\tif (r)\n\t\tgoto error;\n\tr = amdgpu_vcn_dec_get_destroy_msg(ring, 1, &ib);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vcn_dec_send_msg(ring, &ib, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\telse if (r > 0)\n\t\tr = 0;\n\n\tdma_fence_put(fence);\nerror:\n\treturn r;\n}\n\nstatic uint32_t *amdgpu_vcn_unified_ring_ib_header(struct amdgpu_ib *ib,\n\t\t\t\t\t\tuint32_t ib_pack_in_dw, bool enc)\n{\n\tuint32_t *ib_checksum;\n\n\tib->ptr[ib->length_dw++] = 0x00000010;  \n\tib->ptr[ib->length_dw++] = 0x30000002;\n\tib_checksum = &ib->ptr[ib->length_dw++];\n\tib->ptr[ib->length_dw++] = ib_pack_in_dw;\n\n\tib->ptr[ib->length_dw++] = 0x00000010;  \n\tib->ptr[ib->length_dw++] = 0x30000001;\n\tib->ptr[ib->length_dw++] = enc ? 0x2 : 0x3;\n\tib->ptr[ib->length_dw++] = ib_pack_in_dw * sizeof(uint32_t);\n\n\treturn ib_checksum;\n}\n\nstatic void amdgpu_vcn_unified_ring_ib_checksum(uint32_t **ib_checksum,\n\t\t\t\t\t\tuint32_t ib_pack_in_dw)\n{\n\tuint32_t i;\n\tuint32_t checksum = 0;\n\n\tfor (i = 0; i < ib_pack_in_dw; i++)\n\t\tchecksum += *(*ib_checksum + 2 + i);\n\n\t**ib_checksum = checksum;\n}\n\nstatic int amdgpu_vcn_dec_sw_send_msg(struct amdgpu_ring *ring,\n\t\t\t\t      struct amdgpu_ib *ib_msg,\n\t\t\t\t      struct dma_fence **fence)\n{\n\tstruct amdgpu_vcn_decode_buffer *decode_buffer = NULL;\n\tunsigned int ib_size_dw = 64;\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *f = NULL;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tuint64_t addr = AMDGPU_GPU_PAGE_ALIGN(ib_msg->gpu_addr);\n\tbool sq = amdgpu_vcn_using_unified_queue(ring);\n\tuint32_t *ib_checksum;\n\tuint32_t ib_pack_in_dw;\n\tint i, r;\n\n\tif (sq)\n\t\tib_size_dw += 8;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, NULL, NULL,\n\t\t\t\t     ib_size_dw * 4, AMDGPU_IB_POOL_DIRECT,\n\t\t\t\t     &job);\n\tif (r)\n\t\tgoto err;\n\n\tib = &job->ibs[0];\n\tib->length_dw = 0;\n\n\t \n\tif (sq) {\n\t\tib_pack_in_dw = sizeof(struct amdgpu_vcn_decode_buffer) / sizeof(uint32_t)\n\t\t\t\t\t\t+ 4 + 2;  \n\t\tib_checksum = amdgpu_vcn_unified_ring_ib_header(ib, ib_pack_in_dw, false);\n\t}\n\n\tib->ptr[ib->length_dw++] = sizeof(struct amdgpu_vcn_decode_buffer) + 8;\n\tib->ptr[ib->length_dw++] = cpu_to_le32(AMDGPU_VCN_IB_FLAG_DECODE_BUFFER);\n\tdecode_buffer = (struct amdgpu_vcn_decode_buffer *)&(ib->ptr[ib->length_dw]);\n\tib->length_dw += sizeof(struct amdgpu_vcn_decode_buffer) / 4;\n\tmemset(decode_buffer, 0, sizeof(struct amdgpu_vcn_decode_buffer));\n\n\tdecode_buffer->valid_buf_flag |= cpu_to_le32(AMDGPU_VCN_CMD_FLAG_MSG_BUFFER);\n\tdecode_buffer->msg_buffer_address_hi = cpu_to_le32(addr >> 32);\n\tdecode_buffer->msg_buffer_address_lo = cpu_to_le32(addr);\n\n\tfor (i = ib->length_dw; i < ib_size_dw; ++i)\n\t\tib->ptr[i] = 0x0;\n\n\tif (sq)\n\t\tamdgpu_vcn_unified_ring_ib_checksum(&ib_checksum, ib_pack_in_dw);\n\n\tr = amdgpu_job_submit_direct(job, ring, &f);\n\tif (r)\n\t\tgoto err_free;\n\n\tamdgpu_ib_free(adev, ib_msg, f);\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\n\treturn 0;\n\nerr_free:\n\tamdgpu_job_free(job);\nerr:\n\tamdgpu_ib_free(adev, ib_msg, f);\n\treturn r;\n}\n\nint amdgpu_vcn_dec_sw_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_ib ib;\n\tlong r;\n\n\tr = amdgpu_vcn_dec_get_create_msg(ring, 1, &ib);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vcn_dec_sw_send_msg(ring, &ib, NULL);\n\tif (r)\n\t\tgoto error;\n\tr = amdgpu_vcn_dec_get_destroy_msg(ring, 1, &ib);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vcn_dec_sw_send_msg(ring, &ib, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\telse if (r > 0)\n\t\tr = 0;\n\n\tdma_fence_put(fence);\nerror:\n\treturn r;\n}\n\nint amdgpu_vcn_enc_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t rptr;\n\tunsigned int i;\n\tint r;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tr = amdgpu_ring_alloc(ring, 16);\n\tif (r)\n\t\treturn r;\n\n\trptr = amdgpu_ring_get_rptr(ring);\n\n\tamdgpu_ring_write(ring, VCN_ENC_CMD_END);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (amdgpu_ring_get_rptr(ring) != rptr)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\nstatic int amdgpu_vcn_enc_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t\t struct amdgpu_ib *ib_msg,\n\t\t\t\t\t struct dma_fence **fence)\n{\n\tunsigned int ib_size_dw = 16;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tstruct dma_fence *f = NULL;\n\tuint32_t *ib_checksum = NULL;\n\tuint64_t addr;\n\tbool sq = amdgpu_vcn_using_unified_queue(ring);\n\tint i, r;\n\n\tif (sq)\n\t\tib_size_dw += 8;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, NULL, NULL,\n\t\t\t\t     ib_size_dw * 4, AMDGPU_IB_POOL_DIRECT,\n\t\t\t\t     &job);\n\tif (r)\n\t\treturn r;\n\n\tib = &job->ibs[0];\n\taddr = AMDGPU_GPU_PAGE_ALIGN(ib_msg->gpu_addr);\n\n\tib->length_dw = 0;\n\n\tif (sq)\n\t\tib_checksum = amdgpu_vcn_unified_ring_ib_header(ib, 0x11, true);\n\n\tib->ptr[ib->length_dw++] = 0x00000018;\n\tib->ptr[ib->length_dw++] = 0x00000001;  \n\tib->ptr[ib->length_dw++] = handle;\n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = addr;\n\tib->ptr[ib->length_dw++] = 0x0000000b;\n\n\tib->ptr[ib->length_dw++] = 0x00000014;\n\tib->ptr[ib->length_dw++] = 0x00000002;  \n\tib->ptr[ib->length_dw++] = 0x0000001c;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\n\tib->ptr[ib->length_dw++] = 0x00000008;\n\tib->ptr[ib->length_dw++] = 0x08000001;  \n\n\tfor (i = ib->length_dw; i < ib_size_dw; ++i)\n\t\tib->ptr[i] = 0x0;\n\n\tif (sq)\n\t\tamdgpu_vcn_unified_ring_ib_checksum(&ib_checksum, 0x11);\n\n\tr = amdgpu_job_submit_direct(job, ring, &f);\n\tif (r)\n\t\tgoto err;\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\n\treturn 0;\n\nerr:\n\tamdgpu_job_free(job);\n\treturn r;\n}\n\nstatic int amdgpu_vcn_enc_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t\t  struct amdgpu_ib *ib_msg,\n\t\t\t\t\t  struct dma_fence **fence)\n{\n\tunsigned int ib_size_dw = 16;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tstruct dma_fence *f = NULL;\n\tuint32_t *ib_checksum = NULL;\n\tuint64_t addr;\n\tbool sq = amdgpu_vcn_using_unified_queue(ring);\n\tint i, r;\n\n\tif (sq)\n\t\tib_size_dw += 8;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, NULL, NULL,\n\t\t\t\t     ib_size_dw * 4, AMDGPU_IB_POOL_DIRECT,\n\t\t\t\t     &job);\n\tif (r)\n\t\treturn r;\n\n\tib = &job->ibs[0];\n\taddr = AMDGPU_GPU_PAGE_ALIGN(ib_msg->gpu_addr);\n\n\tib->length_dw = 0;\n\n\tif (sq)\n\t\tib_checksum = amdgpu_vcn_unified_ring_ib_header(ib, 0x11, true);\n\n\tib->ptr[ib->length_dw++] = 0x00000018;\n\tib->ptr[ib->length_dw++] = 0x00000001;\n\tib->ptr[ib->length_dw++] = handle;\n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = addr;\n\tib->ptr[ib->length_dw++] = 0x0000000b;\n\n\tib->ptr[ib->length_dw++] = 0x00000014;\n\tib->ptr[ib->length_dw++] = 0x00000002;\n\tib->ptr[ib->length_dw++] = 0x0000001c;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\n\tib->ptr[ib->length_dw++] = 0x00000008;\n\tib->ptr[ib->length_dw++] = 0x08000002;  \n\n\tfor (i = ib->length_dw; i < ib_size_dw; ++i)\n\t\tib->ptr[i] = 0x0;\n\n\tif (sq)\n\t\tamdgpu_vcn_unified_ring_ib_checksum(&ib_checksum, 0x11);\n\n\tr = amdgpu_job_submit_direct(job, ring, &f);\n\tif (r)\n\t\tgoto err;\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\n\treturn 0;\n\nerr:\n\tamdgpu_job_free(job);\n\treturn r;\n}\n\nint amdgpu_vcn_enc_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_ib ib;\n\tlong r;\n\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, (128 << 10) + AMDGPU_GPU_PAGE_SIZE,\n\t\t\tAMDGPU_IB_POOL_DIRECT,\n\t\t\t&ib);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vcn_enc_get_create_msg(ring, 1, &ib, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vcn_enc_get_destroy_msg(ring, 1, &ib, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\telse if (r > 0)\n\t\tr = 0;\n\nerror:\n\tamdgpu_ib_free(adev, &ib, fence);\n\tdma_fence_put(fence);\n\n\treturn r;\n}\n\nint amdgpu_vcn_unified_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tlong r;\n\n\tif (adev->ip_versions[UVD_HWIP][0] != IP_VERSION(4, 0, 3)) {\n\t\tr = amdgpu_vcn_enc_ring_test_ib(ring, timeout);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr =  amdgpu_vcn_dec_sw_ring_test_ib(ring, timeout);\n\nerror:\n\treturn r;\n}\n\nenum amdgpu_ring_priority_level amdgpu_vcn_get_enc_ring_prio(int ring)\n{\n\tswitch (ring) {\n\tcase 0:\n\t\treturn AMDGPU_RING_PRIO_0;\n\tcase 1:\n\t\treturn AMDGPU_RING_PRIO_1;\n\tcase 2:\n\t\treturn AMDGPU_RING_PRIO_2;\n\tdefault:\n\t\treturn AMDGPU_RING_PRIO_0;\n\t}\n}\n\nvoid amdgpu_vcn_setup_ucode(struct amdgpu_device *adev)\n{\n\tint i;\n\tunsigned int idx;\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tconst struct common_firmware_header *hdr;\n\n\t\thdr = (const struct common_firmware_header *)adev->vcn.fw->data;\n\n\t\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\t\tif (adev->vcn.harvest_config & (1 << i))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (i >= 2) {\n\t\t\t\tdev_info(adev->dev, \"More then 2 VCN FW instances!\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tidx = AMDGPU_UCODE_ID_VCN + i;\n\t\t\tadev->firmware.ucode[idx].ucode_id = idx;\n\t\t\tadev->firmware.ucode[idx].fw = adev->vcn.fw;\n\t\t\tadev->firmware.fw_size +=\n\t\t\t\tALIGN(le32_to_cpu(hdr->ucode_size_bytes), PAGE_SIZE);\n\n\t\t\tif (adev->ip_versions[UVD_HWIP][0] == IP_VERSION(4, 0, 3))\n\t\t\t\tbreak;\n\t\t}\n\t\tdev_info(adev->dev, \"Will use PSP to load VCN firmware\\n\");\n\t}\n}\n\n \n#if defined(CONFIG_DEBUG_FS)\nstatic ssize_t amdgpu_debugfs_vcn_fwlog_read(struct file *f, char __user *buf,\n\t\t\t\t\t     size_t size, loff_t *pos)\n{\n\tstruct amdgpu_vcn_inst *vcn;\n\tvoid *log_buf;\n\tvolatile struct amdgpu_vcn_fwlog *plog;\n\tunsigned int read_pos, write_pos, available, i, read_bytes = 0;\n\tunsigned int read_num[2] = {0};\n\n\tvcn = file_inode(f)->i_private;\n\tif (!vcn)\n\t\treturn -ENODEV;\n\n\tif (!vcn->fw_shared.cpu_addr || !amdgpu_vcnfw_log)\n\t\treturn -EFAULT;\n\n\tlog_buf = vcn->fw_shared.cpu_addr + vcn->fw_shared.mem_size;\n\n\tplog = (volatile struct amdgpu_vcn_fwlog *)log_buf;\n\tread_pos = plog->rptr;\n\twrite_pos = plog->wptr;\n\n\tif (read_pos > AMDGPU_VCNFW_LOG_SIZE || write_pos > AMDGPU_VCNFW_LOG_SIZE)\n\t\treturn -EFAULT;\n\n\tif (!size || (read_pos == write_pos))\n\t\treturn 0;\n\n\tif (write_pos > read_pos) {\n\t\tavailable = write_pos - read_pos;\n\t\tread_num[0] = min(size, (size_t)available);\n\t} else {\n\t\tread_num[0] = AMDGPU_VCNFW_LOG_SIZE - read_pos;\n\t\tavailable = read_num[0] + write_pos - plog->header_size;\n\t\tif (size > available)\n\t\t\tread_num[1] = write_pos - plog->header_size;\n\t\telse if (size > read_num[0])\n\t\t\tread_num[1] = size - read_num[0];\n\t\telse\n\t\t\tread_num[0] = size;\n\t}\n\n\tfor (i = 0; i < 2; i++) {\n\t\tif (read_num[i]) {\n\t\t\tif (read_pos == AMDGPU_VCNFW_LOG_SIZE)\n\t\t\t\tread_pos = plog->header_size;\n\t\t\tif (read_num[i] == copy_to_user((buf + read_bytes),\n\t\t\t\t\t\t\t(log_buf + read_pos), read_num[i]))\n\t\t\t\treturn -EFAULT;\n\n\t\t\tread_bytes += read_num[i];\n\t\t\tread_pos += read_num[i];\n\t\t}\n\t}\n\n\tplog->rptr = read_pos;\n\t*pos += read_bytes;\n\treturn read_bytes;\n}\n\nstatic const struct file_operations amdgpu_debugfs_vcnfwlog_fops = {\n\t.owner = THIS_MODULE,\n\t.read = amdgpu_debugfs_vcn_fwlog_read,\n\t.llseek = default_llseek\n};\n#endif\n\nvoid amdgpu_debugfs_vcn_fwlog_init(struct amdgpu_device *adev, uint8_t i,\n\t\t\t\t   struct amdgpu_vcn_inst *vcn)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\tchar name[32];\n\n\tsprintf(name, \"amdgpu_vcn_%d_fwlog\", i);\n\tdebugfs_create_file_size(name, S_IFREG | 0444, root, vcn,\n\t\t\t\t &amdgpu_debugfs_vcnfwlog_fops,\n\t\t\t\t AMDGPU_VCNFW_LOG_SIZE);\n#endif\n}\n\nvoid amdgpu_vcn_fwlog_init(struct amdgpu_vcn_inst *vcn)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tvolatile uint32_t *flag = vcn->fw_shared.cpu_addr;\n\tvoid *fw_log_cpu_addr = vcn->fw_shared.cpu_addr + vcn->fw_shared.mem_size;\n\tuint64_t fw_log_gpu_addr = vcn->fw_shared.gpu_addr + vcn->fw_shared.mem_size;\n\tvolatile struct amdgpu_vcn_fwlog *log_buf = fw_log_cpu_addr;\n\tvolatile struct amdgpu_fw_shared_fw_logging *fw_log = vcn->fw_shared.cpu_addr\n\t\t\t\t\t\t\t + vcn->fw_shared.log_offset;\n\t*flag |= cpu_to_le32(AMDGPU_VCN_FW_LOGGING_FLAG);\n\tfw_log->is_enabled = 1;\n\tfw_log->addr_lo = cpu_to_le32(fw_log_gpu_addr & 0xFFFFFFFF);\n\tfw_log->addr_hi = cpu_to_le32(fw_log_gpu_addr >> 32);\n\tfw_log->size = cpu_to_le32(AMDGPU_VCNFW_LOG_SIZE);\n\n\tlog_buf->header_size = sizeof(struct amdgpu_vcn_fwlog);\n\tlog_buf->buffer_size = AMDGPU_VCNFW_LOG_SIZE;\n\tlog_buf->rptr = log_buf->header_size;\n\tlog_buf->wptr = log_buf->header_size;\n\tlog_buf->wrapped = 0;\n#endif\n}\n\nint amdgpu_vcn_process_poison_irq(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tstruct ras_common_if *ras_if = adev->vcn.ras_if;\n\tstruct ras_dispatch_if ih_data = {\n\t\t.entry = entry,\n\t};\n\n\tif (!ras_if)\n\t\treturn 0;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tih_data.head = *ras_if;\n\t\tamdgpu_ras_interrupt_dispatch(adev, &ih_data);\n\t} else {\n\t\tif (adev->virt.ops && adev->virt.ops->ras_poison_handler)\n\t\t\tadev->virt.ops->ras_poison_handler(adev);\n\t\telse\n\t\t\tdev_warn(adev->dev,\n\t\t\t\t\"No ras_poison_handler interface in SRIOV for VCN!\\n\");\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_vcn_ras_late_init(struct amdgpu_device *adev, struct ras_common_if *ras_block)\n{\n\tint r, i;\n\n\tr = amdgpu_ras_block_late_init(adev, ras_block);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_ras_is_supported(adev, ras_block->block)) {\n\t\tfor (i = 0; i < adev->vcn.num_vcn_inst; i++) {\n\t\t\tif (adev->vcn.harvest_config & (1 << i) ||\n\t\t\t    !adev->vcn.inst[i].ras_poison_irq.funcs)\n\t\t\t\tcontinue;\n\n\t\t\tr = amdgpu_irq_get(adev, &adev->vcn.inst[i].ras_poison_irq, 0);\n\t\t\tif (r)\n\t\t\t\tgoto late_fini;\n\t\t}\n\t}\n\treturn 0;\n\nlate_fini:\n\tamdgpu_ras_block_late_fini(adev, ras_block);\n\treturn r;\n}\n\nint amdgpu_vcn_ras_sw_init(struct amdgpu_device *adev)\n{\n\tint err;\n\tstruct amdgpu_vcn_ras *ras;\n\n\tif (!adev->vcn.ras)\n\t\treturn 0;\n\n\tras = adev->vcn.ras;\n\terr = amdgpu_ras_register_ras_block(adev, &ras->ras_block);\n\tif (err) {\n\t\tdev_err(adev->dev, \"Failed to register vcn ras block!\\n\");\n\t\treturn err;\n\t}\n\n\tstrcpy(ras->ras_block.ras_comm.name, \"vcn\");\n\tras->ras_block.ras_comm.block = AMDGPU_RAS_BLOCK__VCN;\n\tras->ras_block.ras_comm.type = AMDGPU_RAS_ERROR__POISON;\n\tadev->vcn.ras_if = &ras->ras_block.ras_comm;\n\n\tif (!ras->ras_block.ras_late_init)\n\t\tras->ras_block.ras_late_init = amdgpu_vcn_ras_late_init;\n\n\treturn 0;\n}\n\nint amdgpu_vcn_psp_update_sram(struct amdgpu_device *adev, int inst_idx,\n\t\t\t       enum AMDGPU_UCODE_ID ucode_id)\n{\n\tstruct amdgpu_firmware_info ucode = {\n\t\t.ucode_id = (ucode_id ? ucode_id :\n\t\t\t    (inst_idx ? AMDGPU_UCODE_ID_VCN1_RAM :\n\t\t\t\t\tAMDGPU_UCODE_ID_VCN0_RAM)),\n\t\t.mc_addr = adev->vcn.inst[inst_idx].dpg_sram_gpu_addr,\n\t\t.ucode_size = ((uintptr_t)adev->vcn.inst[inst_idx].dpg_sram_curr_addr -\n\t\t\t      (uintptr_t)adev->vcn.inst[inst_idx].dpg_sram_cpu_addr),\n\t};\n\n\treturn psp_execute_ip_fw_load(&adev->psp, &ucode);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}