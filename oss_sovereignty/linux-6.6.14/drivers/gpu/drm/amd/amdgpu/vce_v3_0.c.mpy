{
  "module_name": "vce_v3_0.c",
  "hash_id": "e7de408da3a7e3b6f42a20239d4b7a838b775d30defa4708dd637ea49f5d0190",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/vce_v3_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_vce.h\"\n#include \"vid.h\"\n#include \"vce/vce_3_0_d.h\"\n#include \"vce/vce_3_0_sh_mask.h\"\n#include \"oss/oss_3_0_d.h\"\n#include \"oss/oss_3_0_sh_mask.h\"\n#include \"gca/gfx_8_0_d.h\"\n#include \"smu/smu_7_1_2_d.h\"\n#include \"smu/smu_7_1_2_sh_mask.h\"\n#include \"gca/gfx_8_0_sh_mask.h\"\n#include \"ivsrcid/ivsrcid_vislands30.h\"\n\n\n#define GRBM_GFX_INDEX__VCE_INSTANCE__SHIFT\t0x04\n#define GRBM_GFX_INDEX__VCE_INSTANCE_MASK\t0x10\n#define GRBM_GFX_INDEX__VCE_ALL_PIPE\t\t0x07\n\n#define mmVCE_LMI_VCPU_CACHE_40BIT_BAR0\t0x8616\n#define mmVCE_LMI_VCPU_CACHE_40BIT_BAR1\t0x8617\n#define mmVCE_LMI_VCPU_CACHE_40BIT_BAR2\t0x8618\n#define mmGRBM_GFX_INDEX_DEFAULT 0xE0000000\n\n#define VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK\t0x02\n\n#define VCE_V3_0_FW_SIZE\t(384 * 1024)\n#define VCE_V3_0_STACK_SIZE\t(64 * 1024)\n#define VCE_V3_0_DATA_SIZE\t((16 * 1024 * AMDGPU_MAX_VCE_HANDLES) + (52 * 1024))\n\n#define FW_52_8_3\t((52 << 24) | (8 << 16) | (3 << 8))\n\n#define GET_VCE_INSTANCE(i)  ((i) << GRBM_GFX_INDEX__VCE_INSTANCE__SHIFT \\\n\t\t\t\t\t| GRBM_GFX_INDEX__VCE_ALL_PIPE)\n\nstatic void vce_v3_0_mc_resume(struct amdgpu_device *adev, int idx);\nstatic void vce_v3_0_set_ring_funcs(struct amdgpu_device *adev);\nstatic void vce_v3_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int vce_v3_0_wait_for_idle(void *handle);\nstatic int vce_v3_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state);\n \nstatic uint64_t vce_v3_0_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 v;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tif (adev->vce.harvest_config == 0 ||\n\t\tadev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE1)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(0));\n\telse if (adev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE0)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(1));\n\n\tif (ring->me == 0)\n\t\tv = RREG32(mmVCE_RB_RPTR);\n\telse if (ring->me == 1)\n\t\tv = RREG32(mmVCE_RB_RPTR2);\n\telse\n\t\tv = RREG32(mmVCE_RB_RPTR3);\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn v;\n}\n\n \nstatic uint64_t vce_v3_0_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 v;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tif (adev->vce.harvest_config == 0 ||\n\t\tadev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE1)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(0));\n\telse if (adev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE0)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(1));\n\n\tif (ring->me == 0)\n\t\tv = RREG32(mmVCE_RB_WPTR);\n\telse if (ring->me == 1)\n\t\tv = RREG32(mmVCE_RB_WPTR2);\n\telse\n\t\tv = RREG32(mmVCE_RB_WPTR3);\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn v;\n}\n\n \nstatic void vce_v3_0_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tif (adev->vce.harvest_config == 0 ||\n\t\tadev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE1)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(0));\n\telse if (adev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE0)\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(1));\n\n\tif (ring->me == 0)\n\t\tWREG32(mmVCE_RB_WPTR, lower_32_bits(ring->wptr));\n\telse if (ring->me == 1)\n\t\tWREG32(mmVCE_RB_WPTR2, lower_32_bits(ring->wptr));\n\telse\n\t\tWREG32(mmVCE_RB_WPTR3, lower_32_bits(ring->wptr));\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\nstatic void vce_v3_0_override_vce_clock_gating(struct amdgpu_device *adev, bool override)\n{\n\tWREG32_FIELD(VCE_RB_ARB_CTRL, VCE_CGTT_OVERRIDE, override ? 1 : 0);\n}\n\nstatic void vce_v3_0_set_vce_sw_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t     bool gated)\n{\n\tu32 data;\n\n\t \n\tvce_v3_0_override_vce_clock_gating(adev, true);\n\n\t \n\tif (!gated) {\n\t\tdata = RREG32(mmVCE_CLOCK_GATING_B);\n\t\tdata |= 0x1ff;\n\t\tdata &= ~0xef0000;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, data);\n\n\t\tdata = RREG32(mmVCE_UENC_CLOCK_GATING);\n\t\tdata |= 0x3ff000;\n\t\tdata &= ~0xffc00000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING, data);\n\n\t\tdata = RREG32(mmVCE_UENC_CLOCK_GATING_2);\n\t\tdata |= 0x2;\n\t\tdata &= ~0x00010000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING_2, data);\n\n\t\tdata = RREG32(mmVCE_UENC_REG_CLOCK_GATING);\n\t\tdata |= 0x37f;\n\t\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, data);\n\n\t\tdata = RREG32(mmVCE_UENC_DMA_DCLK_CTRL);\n\t\tdata |= VCE_UENC_DMA_DCLK_CTRL__WRDMCLK_FORCEON_MASK |\n\t\t\tVCE_UENC_DMA_DCLK_CTRL__RDDMCLK_FORCEON_MASK |\n\t\t\tVCE_UENC_DMA_DCLK_CTRL__REGCLK_FORCEON_MASK  |\n\t\t\t0x8;\n\t\tWREG32(mmVCE_UENC_DMA_DCLK_CTRL, data);\n\t} else {\n\t\tdata = RREG32(mmVCE_CLOCK_GATING_B);\n\t\tdata &= ~0x80010;\n\t\tdata |= 0xe70008;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, data);\n\n\t\tdata = RREG32(mmVCE_UENC_CLOCK_GATING);\n\t\tdata |= 0xffc00000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING, data);\n\n\t\tdata = RREG32(mmVCE_UENC_CLOCK_GATING_2);\n\t\tdata |= 0x10000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING_2, data);\n\n\t\tdata = RREG32(mmVCE_UENC_REG_CLOCK_GATING);\n\t\tdata &= ~0x3ff;\n\t\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, data);\n\n\t\tdata = RREG32(mmVCE_UENC_DMA_DCLK_CTRL);\n\t\tdata &= ~(VCE_UENC_DMA_DCLK_CTRL__WRDMCLK_FORCEON_MASK |\n\t\t\t  VCE_UENC_DMA_DCLK_CTRL__RDDMCLK_FORCEON_MASK |\n\t\t\t  VCE_UENC_DMA_DCLK_CTRL__REGCLK_FORCEON_MASK  |\n\t\t\t  0x8);\n\t\tWREG32(mmVCE_UENC_DMA_DCLK_CTRL, data);\n\t}\n\tvce_v3_0_override_vce_clock_gating(adev, false);\n}\n\nstatic int vce_v3_0_firmware_loaded(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tuint32_t status = RREG32(mmVCE_STATUS);\n\n\t\t\tif (status & VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK)\n\t\t\t\treturn 0;\n\t\t\tmdelay(10);\n\t\t}\n\n\t\tDRM_ERROR(\"VCE not responding, trying to reset the ECPU!!!\\n\");\n\t\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 1);\n\t\tmdelay(10);\n\t\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 0);\n\t\tmdelay(10);\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\n \nstatic int vce_v3_0_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint idx, r;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (idx = 0; idx < 2; ++idx) {\n\t\tif (adev->vce.harvest_config & (1 << idx))\n\t\t\tcontinue;\n\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(idx));\n\n\t\t \n\t\tif (idx != 1 || adev->vce.harvest_config == AMDGPU_VCE_HARVEST_VCE0) {\n\t\t\tring = &adev->vce.ring[0];\n\t\t\tWREG32(mmVCE_RB_RPTR, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_WPTR, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_BASE_LO, ring->gpu_addr);\n\t\t\tWREG32(mmVCE_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\t\t\tWREG32(mmVCE_RB_SIZE, ring->ring_size / 4);\n\n\t\t\tring = &adev->vce.ring[1];\n\t\t\tWREG32(mmVCE_RB_RPTR2, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_WPTR2, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_BASE_LO2, ring->gpu_addr);\n\t\t\tWREG32(mmVCE_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\t\t\tWREG32(mmVCE_RB_SIZE2, ring->ring_size / 4);\n\n\t\t\tring = &adev->vce.ring[2];\n\t\t\tWREG32(mmVCE_RB_RPTR3, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_WPTR3, lower_32_bits(ring->wptr));\n\t\t\tWREG32(mmVCE_RB_BASE_LO3, ring->gpu_addr);\n\t\t\tWREG32(mmVCE_RB_BASE_HI3, upper_32_bits(ring->gpu_addr));\n\t\t\tWREG32(mmVCE_RB_SIZE3, ring->ring_size / 4);\n\t\t}\n\n\t\tvce_v3_0_mc_resume(adev, idx);\n\t\tWREG32_FIELD(VCE_STATUS, JOB_BUSY, 1);\n\n\t\tif (adev->asic_type >= CHIP_STONEY)\n\t\t\tWREG32_P(mmVCE_VCPU_CNTL, 1, ~0x200001);\n\t\telse\n\t\t\tWREG32_FIELD(VCE_VCPU_CNTL, CLK_EN, 1);\n\n\t\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 0);\n\t\tmdelay(100);\n\n\t\tr = vce_v3_0_firmware_loaded(adev);\n\n\t\t \n\t\tWREG32_FIELD(VCE_STATUS, JOB_BUSY, 0);\n\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"VCE not responding, giving up!!!\\n\");\n\t\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_stop(struct amdgpu_device *adev)\n{\n\tint idx;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (idx = 0; idx < 2; ++idx) {\n\t\tif (adev->vce.harvest_config & (1 << idx))\n\t\t\tcontinue;\n\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(idx));\n\n\t\tif (adev->asic_type >= CHIP_STONEY)\n\t\t\tWREG32_P(mmVCE_VCPU_CNTL, 0, ~0x200001);\n\t\telse\n\t\t\tWREG32_FIELD(VCE_VCPU_CNTL, CLK_EN, 0);\n\n\t\t \n\t\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 1);\n\n\t\t \n\t\tWREG32(mmVCE_STATUS, 0);\n\t}\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\n#define ixVCE_HARVEST_FUSE_MACRO__ADDRESS     0xC0014074\n#define VCE_HARVEST_FUSE_MACRO__SHIFT       27\n#define VCE_HARVEST_FUSE_MACRO__MASK        0x18000000\n\nstatic unsigned vce_v3_0_get_harvest_config(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\tif ((adev->asic_type == CHIP_FIJI) ||\n\t    (adev->asic_type == CHIP_STONEY))\n\t\treturn AMDGPU_VCE_HARVEST_VCE1;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\ttmp = (RREG32_SMC(ixVCE_HARVEST_FUSE_MACRO__ADDRESS) &\n\t\t       VCE_HARVEST_FUSE_MACRO__MASK) >>\n\t\t\tVCE_HARVEST_FUSE_MACRO__SHIFT;\n\telse\n\t\ttmp = (RREG32_SMC(ixCC_HARVEST_FUSES) &\n\t\t       CC_HARVEST_FUSES__VCE_DISABLE_MASK) >>\n\t\t\tCC_HARVEST_FUSES__VCE_DISABLE__SHIFT;\n\n\tswitch (tmp) {\n\tcase 1:\n\t\treturn AMDGPU_VCE_HARVEST_VCE0;\n\tcase 2:\n\t\treturn AMDGPU_VCE_HARVEST_VCE1;\n\tcase 3:\n\t\treturn AMDGPU_VCE_HARVEST_VCE0 | AMDGPU_VCE_HARVEST_VCE1;\n\tdefault:\n\t\tif ((adev->asic_type == CHIP_POLARIS10) ||\n\t\t    (adev->asic_type == CHIP_POLARIS11) ||\n\t\t    (adev->asic_type == CHIP_POLARIS12) ||\n\t\t    (adev->asic_type == CHIP_VEGAM))\n\t\t\treturn AMDGPU_VCE_HARVEST_VCE1;\n\n\t\treturn 0;\n\t}\n}\n\nstatic int vce_v3_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->vce.harvest_config = vce_v3_0_get_harvest_config(adev);\n\n\tif ((adev->vce.harvest_config &\n\t     (AMDGPU_VCE_HARVEST_VCE0 | AMDGPU_VCE_HARVEST_VCE1)) ==\n\t    (AMDGPU_VCE_HARVEST_VCE0 | AMDGPU_VCE_HARVEST_VCE1))\n\t\treturn -ENOENT;\n\n\tadev->vce.num_rings = 3;\n\n\tvce_v3_0_set_ring_funcs(adev);\n\tvce_v3_0_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, VISLANDS30_IV_SRCID_VCE_TRAP, &adev->vce.irq);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vce_sw_init(adev, VCE_V3_0_FW_SIZE +\n\t\t(VCE_V3_0_STACK_SIZE + VCE_V3_0_DATA_SIZE) * 2);\n\tif (r)\n\t\treturn r;\n\n\t \n\tif (adev->vce.fw_version < FW_52_8_3)\n\t\tadev->vce.num_rings = 2;\n\n\tr = amdgpu_vce_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tenum amdgpu_ring_priority_level hw_prio = amdgpu_vce_get_ring_prio(i);\n\n\t\tring = &adev->vce.ring[i];\n\t\tsprintf(ring->name, \"vce%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->vce.irq, 0,\n\t\t\t\t     hw_prio, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vce_entity_init(adev);\n\n\treturn r;\n}\n\nstatic int vce_v3_0_sw_fini(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vce_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_sw_fini(adev);\n}\n\nstatic int vce_v3_0_hw_init(void *handle)\n{\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tvce_v3_0_override_vce_clock_gating(adev, true);\n\n\tamdgpu_asic_set_vce_clocks(adev, 10000, 10000);\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tr = amdgpu_ring_test_helper(&adev->vce.ring[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tDRM_INFO(\"VCE initialized successfully.\\n\");\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_hw_fini(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tr = vce_v3_0_wait_for_idle(handle);\n\tif (r)\n\t\treturn r;\n\n\tvce_v3_0_stop(adev);\n\treturn vce_v3_0_set_clockgating_state(adev, AMD_CG_STATE_GATE);\n}\n\nstatic int vce_v3_0_suspend(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tif (adev->pm.dpm_enabled) {\n\t\tamdgpu_dpm_enable_vce(adev, false);\n\t} else {\n\t\tamdgpu_asic_set_vce_clocks(adev, 0, 0);\n\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_PG_STATE_GATE);\n\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_CG_STATE_GATE);\n\t}\n\n\tr = vce_v3_0_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_suspend(adev);\n}\n\nstatic int vce_v3_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vce_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn vce_v3_0_hw_init(adev);\n}\n\nstatic void vce_v3_0_mc_resume(struct amdgpu_device *adev, int idx)\n{\n\tuint32_t offset, size;\n\n\tWREG32_P(mmVCE_CLOCK_GATING_A, 0, ~(1 << 16));\n\tWREG32_P(mmVCE_UENC_CLOCK_GATING, 0x1FF000, ~0xFF9FF000);\n\tWREG32_P(mmVCE_UENC_REG_CLOCK_GATING, 0x3F, ~0x3F);\n\tWREG32(mmVCE_CLOCK_GATING_B, 0x1FF);\n\n\tWREG32(mmVCE_LMI_CTRL, 0x00398000);\n\tWREG32_P(mmVCE_LMI_CACHE_CTRL, 0x0, ~0x1);\n\tWREG32(mmVCE_LMI_SWAP_CNTL, 0);\n\tWREG32(mmVCE_LMI_SWAP_CNTL1, 0);\n\tWREG32(mmVCE_LMI_VM_CTRL, 0);\n\tWREG32_OR(mmVCE_VCPU_CNTL, 0x00100000);\n\n\tif (adev->asic_type >= CHIP_STONEY) {\n\t\tWREG32(mmVCE_LMI_VCPU_CACHE_40BIT_BAR0, (adev->vce.gpu_addr >> 8));\n\t\tWREG32(mmVCE_LMI_VCPU_CACHE_40BIT_BAR1, (adev->vce.gpu_addr >> 8));\n\t\tWREG32(mmVCE_LMI_VCPU_CACHE_40BIT_BAR2, (adev->vce.gpu_addr >> 8));\n\t} else\n\t\tWREG32(mmVCE_LMI_VCPU_CACHE_40BIT_BAR, (adev->vce.gpu_addr >> 8));\n\toffset = AMDGPU_VCE_FIRMWARE_OFFSET;\n\tsize = VCE_V3_0_FW_SIZE;\n\tWREG32(mmVCE_VCPU_CACHE_OFFSET0, offset & 0x7fffffff);\n\tWREG32(mmVCE_VCPU_CACHE_SIZE0, size);\n\n\tif (idx == 0) {\n\t\toffset += size;\n\t\tsize = VCE_V3_0_STACK_SIZE;\n\t\tWREG32(mmVCE_VCPU_CACHE_OFFSET1, offset & 0x7fffffff);\n\t\tWREG32(mmVCE_VCPU_CACHE_SIZE1, size);\n\t\toffset += size;\n\t\tsize = VCE_V3_0_DATA_SIZE;\n\t\tWREG32(mmVCE_VCPU_CACHE_OFFSET2, offset & 0x7fffffff);\n\t\tWREG32(mmVCE_VCPU_CACHE_SIZE2, size);\n\t} else {\n\t\toffset += size + VCE_V3_0_STACK_SIZE + VCE_V3_0_DATA_SIZE;\n\t\tsize = VCE_V3_0_STACK_SIZE;\n\t\tWREG32(mmVCE_VCPU_CACHE_OFFSET1, offset & 0xfffffff);\n\t\tWREG32(mmVCE_VCPU_CACHE_SIZE1, size);\n\t\toffset += size;\n\t\tsize = VCE_V3_0_DATA_SIZE;\n\t\tWREG32(mmVCE_VCPU_CACHE_OFFSET2, offset & 0xfffffff);\n\t\tWREG32(mmVCE_VCPU_CACHE_SIZE2, size);\n\t}\n\n\tWREG32_P(mmVCE_LMI_CTRL2, 0x0, ~0x100);\n\tWREG32_FIELD(VCE_SYS_INT_EN, VCE_SYS_INT_TRAP_INTERRUPT_EN, 1);\n}\n\nstatic bool vce_v3_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 mask = 0;\n\n\tmask |= (adev->vce.harvest_config & AMDGPU_VCE_HARVEST_VCE0) ? 0 : SRBM_STATUS2__VCE0_BUSY_MASK;\n\tmask |= (adev->vce.harvest_config & AMDGPU_VCE_HARVEST_VCE1) ? 0 : SRBM_STATUS2__VCE1_BUSY_MASK;\n\n\treturn !(RREG32(mmSRBM_STATUS2) & mask);\n}\n\nstatic int vce_v3_0_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++)\n\t\tif (vce_v3_0_is_idle(handle))\n\t\t\treturn 0;\n\n\treturn -ETIMEDOUT;\n}\n\n#define  VCE_STATUS_VCPU_REPORT_AUTO_BUSY_MASK  0x00000008L    \n#define  VCE_STATUS_VCPU_REPORT_RB0_BUSY_MASK   0x00000010L    \n#define  VCE_STATUS_VCPU_REPORT_RB1_BUSY_MASK   0x00000020L    \n#define  AMDGPU_VCE_STATUS_BUSY_MASK (VCE_STATUS_VCPU_REPORT_AUTO_BUSY_MASK | \\\n\t\t\t\t      VCE_STATUS_VCPU_REPORT_RB0_BUSY_MASK)\n\nstatic bool vce_v3_0_check_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset = 0;\n\n\t \n\tmutex_lock(&adev->grbm_idx_mutex);\n\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(0));\n\tif (RREG32(mmVCE_STATUS) & AMDGPU_VCE_STATUS_BUSY_MASK) {\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE0, 1);\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE1, 1);\n\t}\n\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(1));\n\tif (RREG32(mmVCE_STATUS) & AMDGPU_VCE_STATUS_BUSY_MASK) {\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE0, 1);\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset, SRBM_SOFT_RESET, SOFT_RESET_VCE1, 1);\n\t}\n\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(0));\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\tif (srbm_soft_reset) {\n\t\tadev->vce.srbm_soft_reset = srbm_soft_reset;\n\t\treturn true;\n\t} else {\n\t\tadev->vce.srbm_soft_reset = 0;\n\t\treturn false;\n\t}\n}\n\nstatic int vce_v3_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\tsrbm_soft_reset = adev->vce.srbm_soft_reset;\n\n\tif (srbm_soft_reset) {\n\t\tu32 tmp;\n\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_pre_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\n\tmdelay(5);\n\n\treturn vce_v3_0_suspend(adev);\n}\n\n\nstatic int vce_v3_0_post_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (!adev->vce.srbm_soft_reset)\n\t\treturn 0;\n\n\tmdelay(5);\n\n\treturn vce_v3_0_resume(adev);\n}\n\nstatic int vce_v3_0_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tuint32_t val = 0;\n\n\tif (state == AMDGPU_IRQ_STATE_ENABLE)\n\t\tval |= VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK;\n\n\tWREG32_P(mmVCE_SYS_INT_EN, val, ~VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK);\n\treturn 0;\n}\n\nstatic int vce_v3_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"IH: VCE\\n\");\n\n\tWREG32_FIELD(VCE_SYS_INT_STATUS, VCE_SYS_INT_TRAP_INTERRUPT_INT, 1);\n\n\tswitch (entry->src_data[0]) {\n\tcase 0:\n\tcase 1:\n\tcase 2:\n\t\tamdgpu_fence_process(&adev->vce.ring[entry->src_data[0]]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool enable = (state == AMD_CG_STATE_GATE);\n\tint i;\n\n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_VCE_MGCG))\n\t\treturn 0;\n\n\tmutex_lock(&adev->grbm_idx_mutex);\n\tfor (i = 0; i < 2; i++) {\n\t\t \n\t\tif (adev->vce.harvest_config & (1 << i))\n\t\t\tcontinue;\n\n\t\tWREG32(mmGRBM_GFX_INDEX, GET_VCE_INSTANCE(i));\n\n\t\tif (!enable) {\n\t\t\t \n\t\t\tuint32_t data = RREG32(mmVCE_CLOCK_GATING_A);\n\t\t\tdata &= ~(0xf | 0xff0);\n\t\t\tdata |= ((0x0 << 0) | (0x04 << 4));\n\t\t\tWREG32(mmVCE_CLOCK_GATING_A, data);\n\n\t\t\t \n\t\t\tdata = RREG32(mmVCE_UENC_CLOCK_GATING);\n\t\t\tdata &= ~(0xf | 0xff0);\n\t\t\tdata |= ((0x0 << 0) | (0x04 << 4));\n\t\t\tWREG32(mmVCE_UENC_CLOCK_GATING, data);\n\t\t}\n\n\t\tvce_v3_0_set_vce_sw_clock_gating(adev, enable);\n\t}\n\n\tWREG32(mmGRBM_GFX_INDEX, mmGRBM_GFX_INDEX_DEFAULT);\n\tmutex_unlock(&adev->grbm_idx_mutex);\n\n\treturn 0;\n}\n\nstatic int vce_v3_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\t \n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret = 0;\n\n\tif (state == AMD_PG_STATE_GATE) {\n\t\tret = vce_v3_0_stop(adev);\n\t\tif (ret)\n\t\t\tgoto out;\n\t} else {\n\t\tret = vce_v3_0_start(adev);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void vce_v3_0_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tmutex_lock(&adev->pm.mutex);\n\n\tif (adev->flags & AMD_IS_APU)\n\t\tdata = RREG32_SMC(ixCURRENT_PG_STATUS_APU);\n\telse\n\t\tdata = RREG32_SMC(ixCURRENT_PG_STATUS);\n\n\tif (data & CURRENT_PG_STATUS__VCE_PG_STATUS_MASK) {\n\t\tDRM_INFO(\"Cannot get clockgating state when VCE is powergated.\\n\");\n\t\tgoto out;\n\t}\n\n\tWREG32_FIELD(GRBM_GFX_INDEX, VCE_INSTANCE, 0);\n\n\t \n\tdata = RREG32(mmVCE_CLOCK_GATING_A);\n\tif (data & (0x04 << 4))\n\t\t*flags |= AMD_CG_SUPPORT_VCE_MGCG;\n\nout:\n\tmutex_unlock(&adev->pm.mutex);\n}\n\nstatic void vce_v3_0_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t  struct amdgpu_job *job,\n\t\t\t\t  struct amdgpu_ib *ib,\n\t\t\t\t  uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, VCE_CMD_IB_VM);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\nstatic void vce_v3_0_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t   unsigned int vmid, uint64_t pd_addr)\n{\n\tamdgpu_ring_write(ring, VCE_CMD_UPDATE_PTB);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, pd_addr >> 12);\n\n\tamdgpu_ring_write(ring, VCE_CMD_FLUSH_TLB);\n\tamdgpu_ring_write(ring, vmid);\n\tamdgpu_ring_write(ring, VCE_CMD_END);\n}\n\nstatic void vce_v3_0_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\tamdgpu_ring_write(ring, VCE_CMD_WAIT_GE);\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, seq);\n}\n\nstatic const struct amd_ip_funcs vce_v3_0_ip_funcs = {\n\t.name = \"vce_v3_0\",\n\t.early_init = vce_v3_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = vce_v3_0_sw_init,\n\t.sw_fini = vce_v3_0_sw_fini,\n\t.hw_init = vce_v3_0_hw_init,\n\t.hw_fini = vce_v3_0_hw_fini,\n\t.suspend = vce_v3_0_suspend,\n\t.resume = vce_v3_0_resume,\n\t.is_idle = vce_v3_0_is_idle,\n\t.wait_for_idle = vce_v3_0_wait_for_idle,\n\t.check_soft_reset = vce_v3_0_check_soft_reset,\n\t.pre_soft_reset = vce_v3_0_pre_soft_reset,\n\t.soft_reset = vce_v3_0_soft_reset,\n\t.post_soft_reset = vce_v3_0_post_soft_reset,\n\t.set_clockgating_state = vce_v3_0_set_clockgating_state,\n\t.set_powergating_state = vce_v3_0_set_powergating_state,\n\t.get_clockgating_state = vce_v3_0_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ring_funcs vce_v3_0_ring_phys_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCE,\n\t.align_mask = 0xf,\n\t.nop = VCE_CMD_NO_OP,\n\t.support_64bit_ptrs = false,\n\t.no_user_fence = true,\n\t.get_rptr = vce_v3_0_ring_get_rptr,\n\t.get_wptr = vce_v3_0_ring_get_wptr,\n\t.set_wptr = vce_v3_0_ring_set_wptr,\n\t.parse_cs = amdgpu_vce_ring_parse_cs,\n\t.emit_frame_size =\n\t\t4 +  \n\t\t6,  \n\t.emit_ib_size = 4,  \n\t.emit_ib = amdgpu_vce_ring_emit_ib,\n\t.emit_fence = amdgpu_vce_ring_emit_fence,\n\t.test_ring = amdgpu_vce_ring_test_ring,\n\t.test_ib = amdgpu_vce_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vce_ring_begin_use,\n\t.end_use = amdgpu_vce_ring_end_use,\n};\n\nstatic const struct amdgpu_ring_funcs vce_v3_0_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCE,\n\t.align_mask = 0xf,\n\t.nop = VCE_CMD_NO_OP,\n\t.support_64bit_ptrs = false,\n\t.no_user_fence = true,\n\t.get_rptr = vce_v3_0_ring_get_rptr,\n\t.get_wptr = vce_v3_0_ring_get_wptr,\n\t.set_wptr = vce_v3_0_ring_set_wptr,\n\t.parse_cs = amdgpu_vce_ring_parse_cs_vm,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t4 +  \n\t\t6 + 6,  \n\t.emit_ib_size = 5,  \n\t.emit_ib = vce_v3_0_ring_emit_ib,\n\t.emit_vm_flush = vce_v3_0_emit_vm_flush,\n\t.emit_pipeline_sync = vce_v3_0_emit_pipeline_sync,\n\t.emit_fence = amdgpu_vce_ring_emit_fence,\n\t.test_ring = amdgpu_vce_ring_test_ring,\n\t.test_ib = amdgpu_vce_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vce_ring_begin_use,\n\t.end_use = amdgpu_vce_ring_end_use,\n};\n\nstatic void vce_v3_0_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (adev->asic_type >= CHIP_STONEY) {\n\t\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\t\tadev->vce.ring[i].funcs = &vce_v3_0_ring_vm_funcs;\n\t\t\tadev->vce.ring[i].me = i;\n\t\t}\n\t\tDRM_INFO(\"VCE enabled in VM mode\\n\");\n\t} else {\n\t\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\t\tadev->vce.ring[i].funcs = &vce_v3_0_ring_phys_funcs;\n\t\t\tadev->vce.ring[i].me = i;\n\t\t}\n\t\tDRM_INFO(\"VCE enabled in physical mode\\n\");\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs vce_v3_0_irq_funcs = {\n\t.set = vce_v3_0_set_interrupt_state,\n\t.process = vce_v3_0_process_interrupt,\n};\n\nstatic void vce_v3_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->vce.irq.num_types = 1;\n\tadev->vce.irq.funcs = &vce_v3_0_irq_funcs;\n};\n\nconst struct amdgpu_ip_block_version vce_v3_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_VCE,\n\t.major = 3,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &vce_v3_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version vce_v3_1_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_VCE,\n\t.major = 3,\n\t.minor = 1,\n\t.rev = 0,\n\t.funcs = &vce_v3_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version vce_v3_4_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_VCE,\n\t.major = 3,\n\t.minor = 4,\n\t.rev = 0,\n\t.funcs = &vce_v3_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}