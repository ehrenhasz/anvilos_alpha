{
  "module_name": "mxgpu_nv.c",
  "hash_id": "ae8074a1740cb5403f5210b5aeb26fef728cd145b5976f47c6c5daf434548b46",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/mxgpu_nv.c",
  "human_readable_source": " \n\n#include \"amdgpu.h\"\n#include \"nbio/nbio_2_3_offset.h\"\n#include \"nbio/nbio_2_3_sh_mask.h\"\n#include \"gc/gc_10_1_0_offset.h\"\n#include \"gc/gc_10_1_0_sh_mask.h\"\n#include \"soc15.h\"\n#include \"navi10_ih.h\"\n#include \"soc15_common.h\"\n#include \"mxgpu_nv.h\"\n\n#include \"amdgpu_reset.h\"\n\nstatic void xgpu_nv_mailbox_send_ack(struct amdgpu_device *adev)\n{\n\tWREG8(NV_MAIBOX_CONTROL_RCV_OFFSET_BYTE, 2);\n}\n\nstatic void xgpu_nv_mailbox_set_valid(struct amdgpu_device *adev, bool val)\n{\n\tWREG8(NV_MAIBOX_CONTROL_TRN_OFFSET_BYTE, val ? 1 : 0);\n}\n\n \nstatic enum idh_event xgpu_nv_mailbox_peek_msg(struct amdgpu_device *adev)\n{\n\treturn RREG32_NO_KIQ(mmMAILBOX_MSGBUF_RCV_DW0);\n}\n\n\nstatic int xgpu_nv_mailbox_rcv_msg(struct amdgpu_device *adev,\n\t\t\t\t   enum idh_event event)\n{\n\tu32 reg;\n\n\treg = RREG32_NO_KIQ(mmMAILBOX_MSGBUF_RCV_DW0);\n\tif (reg != event)\n\t\treturn -ENOENT;\n\n\txgpu_nv_mailbox_send_ack(adev);\n\n\treturn 0;\n}\n\nstatic uint8_t xgpu_nv_peek_ack(struct amdgpu_device *adev)\n{\n\treturn RREG8(NV_MAIBOX_CONTROL_TRN_OFFSET_BYTE) & 2;\n}\n\nstatic int xgpu_nv_poll_ack(struct amdgpu_device *adev)\n{\n\tint timeout  = NV_MAILBOX_POLL_ACK_TIMEDOUT;\n\tu8 reg;\n\n\tdo {\n\t\treg = RREG8(NV_MAIBOX_CONTROL_TRN_OFFSET_BYTE);\n\t\tif (reg & 2)\n\t\t\treturn 0;\n\n\t\tmdelay(5);\n\t\ttimeout -= 5;\n\t} while (timeout > 1);\n\n\tpr_err(\"Doesn't get TRN_MSG_ACK from pf in %d msec\\n\", NV_MAILBOX_POLL_ACK_TIMEDOUT);\n\n\treturn -ETIME;\n}\n\nstatic int xgpu_nv_poll_msg(struct amdgpu_device *adev, enum idh_event event)\n{\n\tint r;\n\tuint64_t timeout, now;\n\n\tnow = (uint64_t)ktime_to_ms(ktime_get());\n\ttimeout = now + NV_MAILBOX_POLL_MSG_TIMEDOUT;\n\n\tdo {\n\t\tr = xgpu_nv_mailbox_rcv_msg(adev, event);\n\t\tif (!r)\n\t\t\treturn 0;\n\n\t\tmsleep(10);\n\t\tnow = (uint64_t)ktime_to_ms(ktime_get());\n\t} while (timeout > now);\n\n\n\treturn -ETIME;\n}\n\nstatic void xgpu_nv_mailbox_trans_msg (struct amdgpu_device *adev,\n\t      enum idh_request req, u32 data1, u32 data2, u32 data3)\n{\n\tint r;\n\tuint8_t trn;\n\n\t \n\tdo {\n\t\txgpu_nv_mailbox_set_valid(adev, false);\n\t\ttrn = xgpu_nv_peek_ack(adev);\n\t\tif (trn) {\n\t\t\tpr_err(\"trn=%x ACK should not assert! wait again !\\n\", trn);\n\t\t\tmsleep(1);\n\t\t}\n\t} while (trn);\n\n\tWREG32_NO_KIQ(mmMAILBOX_MSGBUF_TRN_DW0, req);\n\tWREG32_NO_KIQ(mmMAILBOX_MSGBUF_TRN_DW1, data1);\n\tWREG32_NO_KIQ(mmMAILBOX_MSGBUF_TRN_DW2, data2);\n\tWREG32_NO_KIQ(mmMAILBOX_MSGBUF_TRN_DW3, data3);\n\txgpu_nv_mailbox_set_valid(adev, true);\n\n\t \n\tr = xgpu_nv_poll_ack(adev);\n\tif (r)\n\t\tpr_err(\"Doesn't get ack from pf, continue\\n\");\n\n\txgpu_nv_mailbox_set_valid(adev, false);\n}\n\nstatic int xgpu_nv_send_access_requests(struct amdgpu_device *adev,\n\t\t\t\t\tenum idh_request req)\n{\n\tint r, retry = 1;\n\tenum idh_event event = -1;\n\nsend_request:\n\txgpu_nv_mailbox_trans_msg(adev, req, 0, 0, 0);\n\n\tswitch (req) {\n\tcase IDH_REQ_GPU_INIT_ACCESS:\n\tcase IDH_REQ_GPU_FINI_ACCESS:\n\tcase IDH_REQ_GPU_RESET_ACCESS:\n\t\tevent = IDH_READY_TO_ACCESS_GPU;\n\t\tbreak;\n\tcase IDH_REQ_GPU_INIT_DATA:\n\t\tevent = IDH_REQ_GPU_INIT_DATA_READY;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (event != -1) {\n\t\tr = xgpu_nv_poll_msg(adev, event);\n\t\tif (r) {\n\t\t\tif (retry++ < 2)\n\t\t\t\tgoto send_request;\n\n\t\t\tif (req != IDH_REQ_GPU_INIT_DATA) {\n\t\t\t\tpr_err(\"Doesn't get msg:%d from pf, error=%d\\n\", event, r);\n\t\t\t\treturn r;\n\t\t\t} else  \n\t\t\t\tadev->virt.req_init_data_ver = 0;\n\t\t} else {\n\t\t\tif (req == IDH_REQ_GPU_INIT_DATA) {\n\t\t\t\tadev->virt.req_init_data_ver =\n\t\t\t\t\tRREG32_NO_KIQ(mmMAILBOX_MSGBUF_RCV_DW1);\n\n\t\t\t\t \n\t\t\t\tif (adev->virt.req_init_data_ver < 1)\n\t\t\t\t\tadev->virt.req_init_data_ver = 1;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (req == IDH_REQ_GPU_INIT_ACCESS || req == IDH_REQ_GPU_RESET_ACCESS) {\n\t\t\tadev->virt.fw_reserve.checksum_key =\n\t\t\t\tRREG32_NO_KIQ(mmMAILBOX_MSGBUF_RCV_DW2);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int xgpu_nv_request_reset(struct amdgpu_device *adev)\n{\n\tint ret, i = 0;\n\n\twhile (i < NV_MAILBOX_POLL_MSG_REP_MAX) {\n\t\tret = xgpu_nv_send_access_requests(adev, IDH_REQ_GPU_RESET_ACCESS);\n\t\tif (!ret)\n\t\t\tbreak;\n\t\ti++;\n\t}\n\n\treturn ret;\n}\n\nstatic int xgpu_nv_request_full_gpu_access(struct amdgpu_device *adev,\n\t\t\t\t\t   bool init)\n{\n\tenum idh_request req;\n\n\treq = init ? IDH_REQ_GPU_INIT_ACCESS : IDH_REQ_GPU_FINI_ACCESS;\n\treturn xgpu_nv_send_access_requests(adev, req);\n}\n\nstatic int xgpu_nv_release_full_gpu_access(struct amdgpu_device *adev,\n\t\t\t\t\t   bool init)\n{\n\tenum idh_request req;\n\tint r = 0;\n\n\treq = init ? IDH_REL_GPU_INIT_ACCESS : IDH_REL_GPU_FINI_ACCESS;\n\tr = xgpu_nv_send_access_requests(adev, req);\n\n\treturn r;\n}\n\nstatic int xgpu_nv_request_init_data(struct amdgpu_device *adev)\n{\n\treturn xgpu_nv_send_access_requests(adev, IDH_REQ_GPU_INIT_DATA);\n}\n\nstatic int xgpu_nv_mailbox_ack_irq(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"get ack intr and do nothing.\\n\");\n\treturn 0;\n}\n\nstatic int xgpu_nv_set_mailbox_ack_irq(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 tmp = RREG32_NO_KIQ(mmMAILBOX_INT_CNTL);\n\n\tif (state == AMDGPU_IRQ_STATE_ENABLE)\n\t\ttmp |= 2;\n\telse\n\t\ttmp &= ~2;\n\n\tWREG32_NO_KIQ(mmMAILBOX_INT_CNTL, tmp);\n\n\treturn 0;\n}\n\nstatic void xgpu_nv_mailbox_flr_work(struct work_struct *work)\n{\n\tstruct amdgpu_virt *virt = container_of(work, struct amdgpu_virt, flr_work);\n\tstruct amdgpu_device *adev = container_of(virt, struct amdgpu_device, virt);\n\tint timeout = NV_MAILBOX_POLL_FLR_TIMEDOUT;\n\n\t \n\tif (atomic_cmpxchg(&adev->reset_domain->in_gpu_reset, 0, 1) != 0)\n\t\treturn;\n\n\tdown_write(&adev->reset_domain->sem);\n\n\tamdgpu_virt_fini_data_exchange(adev);\n\n\txgpu_nv_mailbox_trans_msg(adev, IDH_READY_TO_RESET, 0, 0, 0);\n\n\tdo {\n\t\tif (xgpu_nv_mailbox_peek_msg(adev) == IDH_FLR_NOTIFICATION_CMPL)\n\t\t\tgoto flr_done;\n\n\t\tmsleep(10);\n\t\ttimeout -= 10;\n\t} while (timeout > 1);\n\nflr_done:\n\tatomic_set(&adev->reset_domain->in_gpu_reset, 0);\n\tup_write(&adev->reset_domain->sem);\n\n\t \n\tif (amdgpu_device_should_recover_gpu(adev)\n\t\t&& (!amdgpu_device_has_job_running(adev) ||\n\t\tadev->sdma_timeout == MAX_SCHEDULE_TIMEOUT ||\n\t\tadev->gfx_timeout == MAX_SCHEDULE_TIMEOUT ||\n\t\tadev->compute_timeout == MAX_SCHEDULE_TIMEOUT ||\n\t\tadev->video_timeout == MAX_SCHEDULE_TIMEOUT)) {\n\t\tstruct amdgpu_reset_context reset_context;\n\t\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\t\treset_context.method = AMD_RESET_METHOD_NONE;\n\t\treset_context.reset_req_dev = adev;\n\t\tclear_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\t\tamdgpu_device_gpu_recover(adev, NULL, &reset_context);\n\t}\n}\n\nstatic int xgpu_nv_set_mailbox_rcv_irq(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_irq_src *src,\n\t\t\t\t       unsigned type,\n\t\t\t\t       enum amdgpu_interrupt_state state)\n{\n\tu32 tmp = RREG32_NO_KIQ(mmMAILBOX_INT_CNTL);\n\n\tif (state == AMDGPU_IRQ_STATE_ENABLE)\n\t\ttmp |= 1;\n\telse\n\t\ttmp &= ~1;\n\n\tWREG32_NO_KIQ(mmMAILBOX_INT_CNTL, tmp);\n\n\treturn 0;\n}\n\nstatic int xgpu_nv_mailbox_rcv_irq(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_irq_src *source,\n\t\t\t\t   struct amdgpu_iv_entry *entry)\n{\n\tenum idh_event event = xgpu_nv_mailbox_peek_msg(adev);\n\n\tswitch (event) {\n\tcase IDH_FLR_NOTIFICATION:\n\t\tif (amdgpu_sriov_runtime(adev) && !amdgpu_in_reset(adev))\n\t\t\tWARN_ONCE(!amdgpu_reset_domain_schedule(adev->reset_domain,\n\t\t\t\t   &adev->virt.flr_work),\n\t\t\t\t  \"Failed to queue work! at %s\",\n\t\t\t\t  __func__);\n\t\tbreak;\n\t\t \n\tcase IDH_CLR_MSG_BUF:\n\tcase IDH_FLR_NOTIFICATION_CMPL:\n\tcase IDH_READY_TO_ACCESS_GPU:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct amdgpu_irq_src_funcs xgpu_nv_mailbox_ack_irq_funcs = {\n\t.set = xgpu_nv_set_mailbox_ack_irq,\n\t.process = xgpu_nv_mailbox_ack_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs xgpu_nv_mailbox_rcv_irq_funcs = {\n\t.set = xgpu_nv_set_mailbox_rcv_irq,\n\t.process = xgpu_nv_mailbox_rcv_irq,\n};\n\nvoid xgpu_nv_mailbox_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->virt.ack_irq.num_types = 1;\n\tadev->virt.ack_irq.funcs = &xgpu_nv_mailbox_ack_irq_funcs;\n\tadev->virt.rcv_irq.num_types = 1;\n\tadev->virt.rcv_irq.funcs = &xgpu_nv_mailbox_rcv_irq_funcs;\n}\n\nint xgpu_nv_mailbox_add_irq_id(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_BIF, 135, &adev->virt.rcv_irq);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_BIF, 138, &adev->virt.ack_irq);\n\tif (r) {\n\t\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nint xgpu_nv_mailbox_get_irq(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tr = amdgpu_irq_get(adev, &adev->virt.rcv_irq, 0);\n\tif (r)\n\t\treturn r;\n\tr = amdgpu_irq_get(adev, &adev->virt.ack_irq, 0);\n\tif (r) {\n\t\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n\t\treturn r;\n\t}\n\n\tINIT_WORK(&adev->virt.flr_work, xgpu_nv_mailbox_flr_work);\n\n\treturn 0;\n}\n\nvoid xgpu_nv_mailbox_put_irq(struct amdgpu_device *adev)\n{\n\tamdgpu_irq_put(adev, &adev->virt.ack_irq, 0);\n\tamdgpu_irq_put(adev, &adev->virt.rcv_irq, 0);\n}\n\nstatic void xgpu_nv_ras_poison_handler(struct amdgpu_device *adev)\n{\n\txgpu_nv_send_access_requests(adev, IDH_RAS_POISON);\n}\n\nconst struct amdgpu_virt_ops xgpu_nv_virt_ops = {\n\t.req_full_gpu\t= xgpu_nv_request_full_gpu_access,\n\t.rel_full_gpu\t= xgpu_nv_release_full_gpu_access,\n\t.req_init_data  = xgpu_nv_request_init_data,\n\t.reset_gpu = xgpu_nv_request_reset,\n\t.wait_reset = NULL,\n\t.trans_msg = xgpu_nv_mailbox_trans_msg,\n\t.ras_poison_handler = xgpu_nv_ras_poison_handler,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}