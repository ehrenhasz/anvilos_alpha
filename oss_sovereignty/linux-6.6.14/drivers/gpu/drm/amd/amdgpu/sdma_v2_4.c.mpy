{
  "module_name": "sdma_v2_4.c",
  "hash_id": "c226c99ca4d99c82ef8643ce5e66eec95a9937b3600c247bd1afb3bb62da1ff7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n#include <linux/firmware.h>\n#include <linux/module.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_trace.h\"\n#include \"vi.h\"\n#include \"vid.h\"\n\n#include \"oss/oss_2_4_d.h\"\n#include \"oss/oss_2_4_sh_mask.h\"\n\n#include \"gmc/gmc_7_1_d.h\"\n#include \"gmc/gmc_7_1_sh_mask.h\"\n\n#include \"gca/gfx_8_0_d.h\"\n#include \"gca/gfx_8_0_enum.h\"\n#include \"gca/gfx_8_0_sh_mask.h\"\n\n#include \"bif/bif_5_0_d.h\"\n#include \"bif/bif_5_0_sh_mask.h\"\n\n#include \"iceland_sdma_pkt_open.h\"\n\n#include \"ivsrcid/ivsrcid_vislands30.h\"\n\nstatic void sdma_v2_4_set_ring_funcs(struct amdgpu_device *adev);\nstatic void sdma_v2_4_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic void sdma_v2_4_set_irq_funcs(struct amdgpu_device *adev);\n\nMODULE_FIRMWARE(\"amdgpu/topaz_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/topaz_sdma1.bin\");\n\nstatic const u32 sdma_offsets[SDMA_MAX_INSTANCE] =\n{\n\tSDMA0_REGISTER_OFFSET,\n\tSDMA1_REGISTER_OFFSET\n};\n\nstatic const u32 golden_settings_iceland_a11[] =\n{\n\tmmSDMA0_CHICKEN_BITS, 0xfc910007, 0x00810007,\n\tmmSDMA0_CLK_CTRL, 0xff000fff, 0x00000000,\n\tmmSDMA1_CHICKEN_BITS, 0xfc910007, 0x00810007,\n\tmmSDMA1_CLK_CTRL, 0xff000fff, 0x00000000,\n};\n\nstatic const u32 iceland_mgcg_cgcg_init[] =\n{\n\tmmSDMA0_CLK_CTRL, 0xff000ff0, 0x00000100,\n\tmmSDMA1_CLK_CTRL, 0xff000ff0, 0x00000100\n};\n\n \n\nstatic void sdma_v2_4_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ticeland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(iceland_mgcg_cgcg_init));\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tgolden_settings_iceland_a11,\n\t\t\t\t\t\t\tARRAY_SIZE(golden_settings_iceland_a11));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void sdma_v2_4_free_microcode(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ucode_release(&adev->sdma.instance[i].fw);\n}\n\n \nstatic int sdma_v2_4_init_microcode(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tchar fw_name[30];\n\tint err = 0, i;\n\tstruct amdgpu_firmware_info *info = NULL;\n\tconst struct common_firmware_header *header = NULL;\n\tconst struct sdma_firmware_header_v1_0 *hdr;\n\n\tDRM_DEBUG(\"\\n\");\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tchip_name = \"topaz\";\n\t\tbreak;\n\tdefault: BUG();\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (i == 0)\n\t\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_sdma.bin\", chip_name);\n\t\telse\n\t\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_sdma1.bin\", chip_name);\n\t\terr = amdgpu_ucode_request(adev, &adev->sdma.instance[i].fw, fw_name);\n\t\tif (err)\n\t\t\tgoto out;\n\t\thdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\n\t\tadev->sdma.instance[i].fw_version = le32_to_cpu(hdr->header.ucode_version);\n\t\tadev->sdma.instance[i].feature_version = le32_to_cpu(hdr->ucode_feature_version);\n\t\tif (adev->sdma.instance[i].feature_version >= 20)\n\t\t\tadev->sdma.instance[i].burst_nop = true;\n\n\t\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_SMU) {\n\t\t\tinfo = &adev->firmware.ucode[AMDGPU_UCODE_ID_SDMA0 + i];\n\t\t\tinfo->ucode_id = AMDGPU_UCODE_ID_SDMA0 + i;\n\t\t\tinfo->fw = adev->sdma.instance[i].fw;\n\t\t\theader = (const struct common_firmware_header *)info->fw->data;\n\t\t\tadev->firmware.fw_size +=\n\t\t\t\tALIGN(le32_to_cpu(header->ucode_size_bytes), PAGE_SIZE);\n\t\t}\n\t}\n\nout:\n\tif (err) {\n\t\tpr_err(\"sdma_v2_4: Failed to load firmware \\\"%s\\\"\\n\", fw_name);\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\t\tamdgpu_ucode_release(&adev->sdma.instance[i].fw);\n\t}\n\treturn err;\n}\n\n \nstatic uint64_t sdma_v2_4_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\t \n\treturn *ring->rptr_cpu_addr >> 2;\n}\n\n \nstatic uint64_t sdma_v2_4_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 wptr = RREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[ring->me]) >> 2;\n\n\treturn wptr;\n}\n\n \nstatic void sdma_v2_4_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[ring->me], ring->wptr << 2);\n}\n\nstatic void sdma_v2_4_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tint i;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(count - 1));\n\t\telse\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop);\n}\n\n \nstatic void sdma_v2_4_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_job *job,\n\t\t\t\t   struct amdgpu_ib *ib,\n\t\t\t\t   uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\t \n\tsdma_v2_4_ring_insert_nop(ring, (2 - lower_32_bits(ring->wptr)) & 7);\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_INDIRECT) |\n\t\t\t  SDMA_PKT_INDIRECT_HEADER_VMID(vmid & 0xf));\n\t \n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr) & 0xffffffe0);\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, 0);\n\n}\n\n \nstatic void sdma_v2_4_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tu32 ref_and_mask = 0;\n\n\tif (ring->me == 0)\n\t\tref_and_mask = REG_SET_FIELD(ref_and_mask, GPU_HDP_FLUSH_DONE, SDMA0, 1);\n\telse\n\t\tref_and_mask = REG_SET_FIELD(ref_and_mask, GPU_HDP_FLUSH_DONE, SDMA1, 1);\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(1) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));  \n\tamdgpu_ring_write(ring, mmGPU_HDP_FLUSH_DONE << 2);\n\tamdgpu_ring_write(ring, mmGPU_HDP_FLUSH_REQ << 2);\n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));  \n}\n\n \nstatic void sdma_v2_4_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t      unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\t\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_TRAP));\n\tamdgpu_ring_write(ring, SDMA_PKT_TRAP_INT_CONTEXT_INT_CONTEXT(0));\n}\n\n \nstatic void sdma_v2_4_gfx_stop(struct amdgpu_device *adev)\n{\n\tu32 rb_cntl, ib_cntl;\n\tint i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\trb_cntl = RREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i]);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, 0);\n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\n\t\tib_cntl = RREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i]);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, 0);\n\t\tWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v2_4_rlc_stop(struct amdgpu_device *adev)\n{\n\t \n}\n\n \nstatic void sdma_v2_4_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl;\n\tint i;\n\n\tif (!enable) {\n\t\tsdma_v2_4_gfx_stop(adev);\n\t\tsdma_v2_4_rlc_stop(adev);\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tf32_cntl = RREG32(mmSDMA0_F32_CNTL + sdma_offsets[i]);\n\t\tif (enable)\n\t\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, 0);\n\t\telse\n\t\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, 1);\n\t\tWREG32(mmSDMA0_F32_CNTL + sdma_offsets[i], f32_cntl);\n\t}\n}\n\n \nstatic int sdma_v2_4_gfx_resume(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tu32 rb_cntl, ib_cntl;\n\tu32 rb_bufsz;\n\tint i, j, r;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tfor (j = 0; j < 16; j++) {\n\t\t\tvi_srbm_select(adev, 0, 0, 0, j);\n\t\t\t \n\t\t\tWREG32(mmSDMA0_GFX_VIRTUAL_ADDR + sdma_offsets[i], 0);\n\t\t\tWREG32(mmSDMA0_GFX_APE1_CNTL + sdma_offsets[i], 0);\n\t\t}\n\t\tvi_srbm_select(adev, 0, 0, 0, 0);\n\t\tmutex_unlock(&adev->srbm_mutex);\n\n\t\tWREG32(mmSDMA0_TILING_CONFIG + sdma_offsets[i],\n\t\t       adev->gfx.config.gb_addr_config & 0x70);\n\n\t\tWREG32(mmSDMA0_SEM_WAIT_FAIL_TIMER_CNTL + sdma_offsets[i], 0);\n\n\t\t \n\t\trb_bufsz = order_base_2(ring->ring_size / 4);\n\t\trb_cntl = RREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i]);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SIZE, rb_bufsz);\n#ifdef __BIG_ENDIAN\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SWAP_ENABLE, 1);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL,\n\t\t\t\t\tRPTR_WRITEBACK_SWAP_ENABLE, 1);\n#endif\n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\n\n\t\t \n\t\tWREG32(mmSDMA0_GFX_RB_RPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_IB_RPTR + sdma_offsets[i], 0);\n\t\tWREG32(mmSDMA0_GFX_IB_OFFSET + sdma_offsets[i], 0);\n\n\t\t \n\t\tWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_HI + sdma_offsets[i],\n\t\t       upper_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFF);\n\t\tWREG32(mmSDMA0_GFX_RB_RPTR_ADDR_LO + sdma_offsets[i],\n\t\t       lower_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFC);\n\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RPTR_WRITEBACK_ENABLE, 1);\n\n\t\tWREG32(mmSDMA0_GFX_RB_BASE + sdma_offsets[i], ring->gpu_addr >> 8);\n\t\tWREG32(mmSDMA0_GFX_RB_BASE_HI + sdma_offsets[i], ring->gpu_addr >> 40);\n\n\t\tring->wptr = 0;\n\t\tWREG32(mmSDMA0_GFX_RB_WPTR + sdma_offsets[i], ring->wptr << 2);\n\n\t\t \n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, 1);\n\t\tWREG32(mmSDMA0_GFX_RB_CNTL + sdma_offsets[i], rb_cntl);\n\n\t\tib_cntl = RREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i]);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t\t \n\t\tWREG32(mmSDMA0_GFX_IB_CNTL + sdma_offsets[i], ib_cntl);\n\t}\n\n\tsdma_v2_4_enable(adev, true);\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sdma_v2_4_rlc_resume(struct amdgpu_device *adev)\n{\n\t \n\treturn 0;\n}\n\n\n \nstatic int sdma_v2_4_start(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t \n\tsdma_v2_4_enable(adev, false);\n\n\t \n\tr = sdma_v2_4_gfx_resume(adev);\n\tif (r)\n\t\treturn r;\n\tr = sdma_v2_4_rlc_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n \nstatic int sdma_v2_4_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\tr = amdgpu_ring_alloc(ring, 5);\n\tif (r)\n\t\tgoto error_free_wb;\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\t\t  SDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(1));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\nerror_free_wb:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\tlong r;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, 256,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err0;\n\n\tib.ptr[0] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr);\n\tib.ptr[3] = SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(1);\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.ptr[5] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[6] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[7] = SDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n\tib.length_dw = 8;\n\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tgoto err1;\n\t}\n\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic void sdma_v2_4_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t\t  uint64_t pe, uint64_t src,\n\t\t\t\t  unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\n\tib->ptr[ib->length_dw++] = bytes;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n}\n\n \nstatic void sdma_v2_4_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t   uint64_t value, unsigned count,\n\t\t\t\t   uint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib->ptr[ib->length_dw++] = pe;\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = ndw;\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void sdma_v2_4_vm_set_pte_pde(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t     uint64_t addr, unsigned count,\n\t\t\t\t     uint32_t incr, uint64_t flags)\n{\n\t \n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_GEN_PTEPDE);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\tib->ptr[ib->length_dw++] = lower_32_bits(addr);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = incr;  \n\tib->ptr[ib->length_dw++] = 0;\n\tib->ptr[ib->length_dw++] = count;  \n}\n\n \nstatic void sdma_v2_4_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tu32 pad_count;\n\tint i;\n\n\tpad_count = (-ib->length_dw) & 7;\n\tfor (i = 0; i < pad_count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP) |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);\n\t\telse\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n}\n\n \nstatic void sdma_v2_4_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3) |  \n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_MEM_POLL(1));\n\tamdgpu_ring_write(ring, addr & 0xfffffffc);\n\tamdgpu_ring_write(ring, upper_32_bits(addr) & 0xffffffff);\n\tamdgpu_ring_write(ring, seq);  \n\tamdgpu_ring_write(ring, 0xffffffff);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(4));  \n}\n\n \nstatic void sdma_v2_4_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\t unsigned vmid, uint64_t pd_addr)\n{\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(0));  \n\tamdgpu_ring_write(ring, mmVM_INVALIDATE_REQUEST << 2);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));  \n}\n\nstatic void sdma_v2_4_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t     uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\n\t\t\t  SDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic int sdma_v2_4_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->sdma.num_instances = SDMA_MAX_INSTANCE;\n\n\tsdma_v2_4_set_ring_funcs(adev);\n\tsdma_v2_4_set_buffer_funcs(adev);\n\tsdma_v2_4_set_vm_pte_funcs(adev);\n\tsdma_v2_4_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int sdma_v2_4_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, VISLANDS30_IV_SRCID_SDMA_TRAP,\n\t\t\t      &adev->sdma.trap_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 241,\n\t\t\t      &adev->sdma.illegal_inst_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, VISLANDS30_IV_SRCID_SDMA_SRBM_WRITE,\n\t\t\t      &adev->sdma.illegal_inst_irq);\n\tif (r)\n\t\treturn r;\n\n\tr = sdma_v2_4_init_microcode(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load sdma firmware!\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tring->use_doorbell = false;\n\t\tsprintf(ring->name, \"sdma%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 1024, &adev->sdma.trap_irq,\n\t\t\t\t     (i == 0) ? AMDGPU_SDMA_IRQ_INSTANCE0 :\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE1,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn r;\n}\n\nstatic int sdma_v2_4_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\n\tsdma_v2_4_free_microcode(adev);\n\treturn 0;\n}\n\nstatic int sdma_v2_4_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsdma_v2_4_init_golden_registers(adev);\n\n\tr = sdma_v2_4_start(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn r;\n}\n\nstatic int sdma_v2_4_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsdma_v2_4_enable(adev, false);\n\n\treturn 0;\n}\n\nstatic int sdma_v2_4_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v2_4_hw_fini(adev);\n}\n\nstatic int sdma_v2_4_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v2_4_hw_init(adev);\n}\n\nstatic bool sdma_v2_4_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(mmSRBM_STATUS2);\n\n\tif (tmp & (SRBM_STATUS2__SDMA_BUSY_MASK |\n\t\t   SRBM_STATUS2__SDMA1_BUSY_MASK))\n\t    return false;\n\n\treturn true;\n}\n\nstatic int sdma_v2_4_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tu32 tmp;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = RREG32(mmSRBM_STATUS2) & (SRBM_STATUS2__SDMA_BUSY_MASK |\n\t\t\t\tSRBM_STATUS2__SDMA1_BUSY_MASK);\n\n\t\tif (!tmp)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int sdma_v2_4_soft_reset(void *handle)\n{\n\tu32 srbm_soft_reset = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(mmSRBM_STATUS2);\n\n\tif (tmp & SRBM_STATUS2__SDMA_BUSY_MASK) {\n\t\t \n\t\ttmp = RREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET);\n\t\ttmp = REG_SET_FIELD(tmp, SDMA0_F32_CNTL, HALT, 0);\n\t\tWREG32(mmSDMA0_F32_CNTL + SDMA0_REGISTER_OFFSET, tmp);\n\t\tsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA_MASK;\n\t}\n\tif (tmp & SRBM_STATUS2__SDMA1_BUSY_MASK) {\n\t\t \n\t\ttmp = RREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET);\n\t\ttmp = REG_SET_FIELD(tmp, SDMA0_F32_CNTL, HALT, 0);\n\t\tWREG32(mmSDMA0_F32_CNTL + SDMA1_REGISTER_OFFSET, tmp);\n\t\tsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_SDMA1_MASK;\n\t}\n\n\tif (srbm_soft_reset) {\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v2_4_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *src,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tswitch (type) {\n\tcase AMDGPU_SDMA_IRQ_INSTANCE0:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 0);\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 1);\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase AMDGPU_SDMA_IRQ_INSTANCE1:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 0);\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE, 1);\n\t\t\tWREG32(mmSDMA0_CNTL + SDMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v2_4_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tu8 instance_id, queue_id;\n\n\tinstance_id = (entry->ring_id & 0x3) >> 0;\n\tqueue_id = (entry->ring_id & 0xc) >> 2;\n\tDRM_DEBUG(\"IH: SDMA trap\\n\");\n\tswitch (instance_id) {\n\tcase 0:\n\t\tswitch (queue_id) {\n\t\tcase 0:\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[0].ring);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 1:\n\t\tswitch (queue_id) {\n\t\tcase 0:\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[1].ring);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tbreak;\n\t\tcase 2:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v2_4_process_illegal_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tu8 instance_id, queue_id;\n\n\tDRM_ERROR(\"Illegal instruction in SDMA command stream\\n\");\n\tinstance_id = (entry->ring_id & 0x3) >> 0;\n\tqueue_id = (entry->ring_id & 0xc) >> 2;\n\n\tif (instance_id <= 1 && queue_id == 0)\n\t\tdrm_sched_fault(&adev->sdma.instance[instance_id].ring.sched);\n\treturn 0;\n}\n\nstatic int sdma_v2_4_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\t \n\treturn 0;\n}\n\nstatic int sdma_v2_4_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs sdma_v2_4_ip_funcs = {\n\t.name = \"sdma_v2_4\",\n\t.early_init = sdma_v2_4_early_init,\n\t.late_init = NULL,\n\t.sw_init = sdma_v2_4_sw_init,\n\t.sw_fini = sdma_v2_4_sw_fini,\n\t.hw_init = sdma_v2_4_hw_init,\n\t.hw_fini = sdma_v2_4_hw_fini,\n\t.suspend = sdma_v2_4_suspend,\n\t.resume = sdma_v2_4_resume,\n\t.is_idle = sdma_v2_4_is_idle,\n\t.wait_for_idle = sdma_v2_4_wait_for_idle,\n\t.soft_reset = sdma_v2_4_soft_reset,\n\t.set_clockgating_state = sdma_v2_4_set_clockgating_state,\n\t.set_powergating_state = sdma_v2_4_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v2_4_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xf,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = false,\n\t.secure_submission_supported = true,\n\t.get_rptr = sdma_v2_4_ring_get_rptr,\n\t.get_wptr = sdma_v2_4_ring_get_wptr,\n\t.set_wptr = sdma_v2_4_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\tVI_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 +  \n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 7 + 6,  \n\t.emit_ib = sdma_v2_4_ring_emit_ib,\n\t.emit_fence = sdma_v2_4_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v2_4_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v2_4_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v2_4_ring_emit_hdp_flush,\n\t.test_ring = sdma_v2_4_ring_test_ring,\n\t.test_ib = sdma_v2_4_ring_test_ib,\n\t.insert_nop = sdma_v2_4_ring_insert_nop,\n\t.pad_ib = sdma_v2_4_ring_pad_ib,\n\t.emit_wreg = sdma_v2_4_ring_emit_wreg,\n};\n\nstatic void sdma_v2_4_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->sdma.instance[i].ring.funcs = &sdma_v2_4_ring_funcs;\n\t\tadev->sdma.instance[i].ring.me = i;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs sdma_v2_4_trap_irq_funcs = {\n\t.set = sdma_v2_4_set_trap_irq_state,\n\t.process = sdma_v2_4_process_trap_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v2_4_illegal_inst_irq_funcs = {\n\t.process = sdma_v2_4_process_illegal_inst_irq,\n};\n\nstatic void sdma_v2_4_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = AMDGPU_SDMA_IRQ_LAST;\n\tadev->sdma.trap_irq.funcs = &sdma_v2_4_trap_irq_funcs;\n\tadev->sdma.illegal_inst_irq.funcs = &sdma_v2_4_illegal_inst_irq_funcs;\n}\n\n \nstatic void sdma_v2_4_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint64_t src_offset,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count,\n\t\t\t\t       bool tmz)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\n\tib->ptr[ib->length_dw++] = byte_count;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n}\n\n \nstatic void sdma_v2_4_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint32_t src_data,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_CONST_FILL);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = byte_count;\n}\n\nstatic const struct amdgpu_buffer_funcs sdma_v2_4_buffer_funcs = {\n\t.copy_max_bytes = 0x1fffff,\n\t.copy_num_dw = 7,\n\t.emit_copy_buffer = sdma_v2_4_emit_copy_buffer,\n\n\t.fill_max_bytes = 0x1fffff,\n\t.fill_num_dw = 7,\n\t.emit_fill_buffer = sdma_v2_4_emit_fill_buffer,\n};\n\nstatic void sdma_v2_4_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &sdma_v2_4_buffer_funcs;\n\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs sdma_v2_4_vm_pte_funcs = {\n\t.copy_pte_num_dw = 7,\n\t.copy_pte = sdma_v2_4_vm_copy_pte,\n\n\t.write_pte = sdma_v2_4_vm_write_pte,\n\t.set_pte_pde = sdma_v2_4_vm_set_pte_pde,\n};\n\nstatic void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &sdma_v2_4_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->vm_manager.vm_pte_scheds[i] =\n\t\t\t&adev->sdma.instance[i].ring.sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nconst struct amdgpu_ip_block_version sdma_v2_4_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 2,\n\t.minor = 4,\n\t.rev = 0,\n\t.funcs = &sdma_v2_4_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}