{
  "module_name": "amdgpu_mes.c",
  "hash_id": "63ce44593791f03d5ad3fad0399f442222b5676961b4c642c34cf90602688d68",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_mes.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <drm/drm_exec.h>\n\n#include \"amdgpu_mes.h\"\n#include \"amdgpu.h\"\n#include \"soc15_common.h\"\n#include \"amdgpu_mes_ctx.h\"\n\n#define AMDGPU_MES_MAX_NUM_OF_QUEUES_PER_PROCESS 1024\n#define AMDGPU_ONE_DOORBELL_SIZE 8\n\nint amdgpu_mes_doorbell_process_slice(struct amdgpu_device *adev)\n{\n\treturn roundup(AMDGPU_ONE_DOORBELL_SIZE *\n\t\t       AMDGPU_MES_MAX_NUM_OF_QUEUES_PER_PROCESS,\n\t\t       PAGE_SIZE);\n}\n\nstatic int amdgpu_mes_kernel_doorbell_get(struct amdgpu_device *adev,\n\t\t\t\t\t struct amdgpu_mes_process *process,\n\t\t\t\t\t int ip_type, uint64_t *doorbell_index)\n{\n\tunsigned int offset, found;\n\tstruct amdgpu_mes *mes = &adev->mes;\n\n\tif (ip_type == AMDGPU_RING_TYPE_SDMA)\n\t\toffset = adev->doorbell_index.sdma_engine[0];\n\telse\n\t\toffset = 0;\n\n\tfound = find_next_zero_bit(mes->doorbell_bitmap, mes->num_mes_dbs, offset);\n\tif (found >= mes->num_mes_dbs) {\n\t\tDRM_WARN(\"No doorbell available\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tset_bit(found, mes->doorbell_bitmap);\n\n\t \n\t*doorbell_index = mes->db_start_dw_offset + found * 2;\n\treturn 0;\n}\n\nstatic void amdgpu_mes_kernel_doorbell_free(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_mes_process *process,\n\t\t\t\t\t   uint32_t doorbell_index)\n{\n\tunsigned int old, rel_index;\n\tstruct amdgpu_mes *mes = &adev->mes;\n\n\t \n\trel_index = (doorbell_index - mes->db_start_dw_offset) / 2;\n\told = test_and_clear_bit(rel_index, mes->doorbell_bitmap);\n\tWARN_ON(!old);\n}\n\nstatic int amdgpu_mes_doorbell_init(struct amdgpu_device *adev)\n{\n\tint i;\n\tstruct amdgpu_mes *mes = &adev->mes;\n\n\t \n\tmes->doorbell_bitmap = bitmap_zalloc(PAGE_SIZE / sizeof(u32), GFP_KERNEL);\n\tif (!mes->doorbell_bitmap) {\n\t\tDRM_ERROR(\"Failed to allocate MES doorbell bitmap\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmes->num_mes_dbs = PAGE_SIZE / AMDGPU_ONE_DOORBELL_SIZE;\n\tfor (i = 0; i < AMDGPU_MES_PRIORITY_NUM_LEVELS; i++) {\n\t\tadev->mes.aggregated_doorbells[i] = mes->db_start_dw_offset + i * 2;\n\t\tset_bit(i, mes->doorbell_bitmap);\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_mes_doorbell_free(struct amdgpu_device *adev)\n{\n\tbitmap_free(adev->mes.doorbell_bitmap);\n}\n\nint amdgpu_mes_init(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tadev->mes.adev = adev;\n\n\tidr_init(&adev->mes.pasid_idr);\n\tidr_init(&adev->mes.gang_id_idr);\n\tidr_init(&adev->mes.queue_id_idr);\n\tida_init(&adev->mes.doorbell_ida);\n\tspin_lock_init(&adev->mes.queue_id_lock);\n\tspin_lock_init(&adev->mes.ring_lock);\n\tmutex_init(&adev->mes.mutex_hidden);\n\n\tadev->mes.total_max_queue = AMDGPU_FENCE_MES_QUEUE_ID_MASK;\n\tadev->mes.vmid_mask_mmhub = 0xffffff00;\n\tadev->mes.vmid_mask_gfxhub = 0xffffff00;\n\n\tfor (i = 0; i < AMDGPU_MES_MAX_COMPUTE_PIPES; i++) {\n\t\t \n\t\tif (i >= 4)\n\t\t\tcontinue;\n\t\tadev->mes.compute_hqd_mask[i] = 0xc;\n\t}\n\n\tfor (i = 0; i < AMDGPU_MES_MAX_GFX_PIPES; i++)\n\t\tadev->mes.gfx_hqd_mask[i] = i ? 0 : 0xfffffffe;\n\n\tfor (i = 0; i < AMDGPU_MES_MAX_SDMA_PIPES; i++) {\n\t\tif (adev->ip_versions[SDMA0_HWIP][0] < IP_VERSION(6, 0, 0))\n\t\t\tadev->mes.sdma_hqd_mask[i] = i ? 0 : 0x3fc;\n\t\t \n\t\telse if (adev->sdma.num_instances == 1)\n\t\t\tadev->mes.sdma_hqd_mask[i] = i ? 0 : 0xfc;\n\t\telse\n\t\t\tadev->mes.sdma_hqd_mask[i] = 0xfc;\n\t}\n\n\tr = amdgpu_device_wb_get(adev, &adev->mes.sch_ctx_offs);\n\tif (r) {\n\t\tdev_err(adev->dev,\n\t\t\t\"(%d) ring trail_fence_offs wb alloc failed\\n\", r);\n\t\tgoto error_ids;\n\t}\n\tadev->mes.sch_ctx_gpu_addr =\n\t\tadev->wb.gpu_addr + (adev->mes.sch_ctx_offs * 4);\n\tadev->mes.sch_ctx_ptr =\n\t\t(uint64_t *)&adev->wb.wb[adev->mes.sch_ctx_offs];\n\n\tr = amdgpu_device_wb_get(adev, &adev->mes.query_status_fence_offs);\n\tif (r) {\n\t\tamdgpu_device_wb_free(adev, adev->mes.sch_ctx_offs);\n\t\tdev_err(adev->dev,\n\t\t\t\"(%d) query_status_fence_offs wb alloc failed\\n\", r);\n\t\tgoto error_ids;\n\t}\n\tadev->mes.query_status_fence_gpu_addr =\n\t\tadev->wb.gpu_addr + (adev->mes.query_status_fence_offs * 4);\n\tadev->mes.query_status_fence_ptr =\n\t\t(uint64_t *)&adev->wb.wb[adev->mes.query_status_fence_offs];\n\n\tr = amdgpu_device_wb_get(adev, &adev->mes.read_val_offs);\n\tif (r) {\n\t\tamdgpu_device_wb_free(adev, adev->mes.sch_ctx_offs);\n\t\tamdgpu_device_wb_free(adev, adev->mes.query_status_fence_offs);\n\t\tdev_err(adev->dev,\n\t\t\t\"(%d) read_val_offs alloc failed\\n\", r);\n\t\tgoto error_ids;\n\t}\n\tadev->mes.read_val_gpu_addr =\n\t\tadev->wb.gpu_addr + (adev->mes.read_val_offs * 4);\n\tadev->mes.read_val_ptr =\n\t\t(uint32_t *)&adev->wb.wb[adev->mes.read_val_offs];\n\n\tr = amdgpu_mes_doorbell_init(adev);\n\tif (r)\n\t\tgoto error;\n\n\treturn 0;\n\nerror:\n\tamdgpu_device_wb_free(adev, adev->mes.sch_ctx_offs);\n\tamdgpu_device_wb_free(adev, adev->mes.query_status_fence_offs);\n\tamdgpu_device_wb_free(adev, adev->mes.read_val_offs);\nerror_ids:\n\tidr_destroy(&adev->mes.pasid_idr);\n\tidr_destroy(&adev->mes.gang_id_idr);\n\tidr_destroy(&adev->mes.queue_id_idr);\n\tida_destroy(&adev->mes.doorbell_ida);\n\tmutex_destroy(&adev->mes.mutex_hidden);\n\treturn r;\n}\n\nvoid amdgpu_mes_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_device_wb_free(adev, adev->mes.sch_ctx_offs);\n\tamdgpu_device_wb_free(adev, adev->mes.query_status_fence_offs);\n\tamdgpu_device_wb_free(adev, adev->mes.read_val_offs);\n\tamdgpu_mes_doorbell_free(adev);\n\n\tidr_destroy(&adev->mes.pasid_idr);\n\tidr_destroy(&adev->mes.gang_id_idr);\n\tidr_destroy(&adev->mes.queue_id_idr);\n\tida_destroy(&adev->mes.doorbell_ida);\n\tmutex_destroy(&adev->mes.mutex_hidden);\n}\n\nstatic void amdgpu_mes_queue_free_mqd(struct amdgpu_mes_queue *q)\n{\n\tamdgpu_bo_free_kernel(&q->mqd_obj,\n\t\t\t      &q->mqd_gpu_addr,\n\t\t\t      &q->mqd_cpu_ptr);\n}\n\nint amdgpu_mes_create_process(struct amdgpu_device *adev, int pasid,\n\t\t\t      struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_mes_process *process;\n\tint r;\n\n\t \n\tprocess = kzalloc(sizeof(struct amdgpu_mes_process), GFP_KERNEL);\n\tif (!process) {\n\t\tDRM_ERROR(\"no more memory to create mes process\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tr = amdgpu_bo_create_kernel(adev, AMDGPU_MES_PROC_CTX_SIZE, PAGE_SIZE,\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t    &process->proc_ctx_bo,\n\t\t\t\t    &process->proc_ctx_gpu_addr,\n\t\t\t\t    &process->proc_ctx_cpu_ptr);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to allocate process context bo\\n\");\n\t\tgoto clean_up_memory;\n\t}\n\tmemset(process->proc_ctx_cpu_ptr, 0, AMDGPU_MES_PROC_CTX_SIZE);\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\t \n\tr = idr_alloc(&adev->mes.pasid_idr, process, pasid, pasid + 1,\n\t\t      GFP_KERNEL);\n\tif (r < 0) {\n\t\tDRM_ERROR(\"failed to lock pasid=%d\\n\", pasid);\n\t\tgoto clean_up_ctx;\n\t}\n\n\tINIT_LIST_HEAD(&process->gang_list);\n\tprocess->vm = vm;\n\tprocess->pasid = pasid;\n\tprocess->process_quantum = adev->mes.default_process_quantum;\n\tprocess->pd_gpu_addr = amdgpu_bo_gpu_offset(vm->root.bo);\n\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn 0;\n\nclean_up_ctx:\n\tamdgpu_mes_unlock(&adev->mes);\n\tamdgpu_bo_free_kernel(&process->proc_ctx_bo,\n\t\t\t      &process->proc_ctx_gpu_addr,\n\t\t\t      &process->proc_ctx_cpu_ptr);\nclean_up_memory:\n\tkfree(process);\n\treturn r;\n}\n\nvoid amdgpu_mes_destroy_process(struct amdgpu_device *adev, int pasid)\n{\n\tstruct amdgpu_mes_process *process;\n\tstruct amdgpu_mes_gang *gang, *tmp1;\n\tstruct amdgpu_mes_queue *queue, *tmp2;\n\tstruct mes_remove_queue_input queue_input;\n\tunsigned long flags;\n\tint r;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tprocess = idr_find(&adev->mes.pasid_idr, pasid);\n\tif (!process) {\n\t\tDRM_WARN(\"pasid %d doesn't exist\\n\", pasid);\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\treturn;\n\t}\n\n\t \n\tlist_for_each_entry_safe(gang, tmp1, &process->gang_list, list) {\n\t\tlist_for_each_entry_safe(queue, tmp2, &gang->queue_list, list) {\n\t\t\tspin_lock_irqsave(&adev->mes.queue_id_lock, flags);\n\t\t\tidr_remove(&adev->mes.queue_id_idr, queue->queue_id);\n\t\t\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\n\n\t\t\tqueue_input.doorbell_offset = queue->doorbell_off;\n\t\t\tqueue_input.gang_context_addr = gang->gang_ctx_gpu_addr;\n\n\t\t\tr = adev->mes.funcs->remove_hw_queue(&adev->mes,\n\t\t\t\t\t\t\t     &queue_input);\n\t\t\tif (r)\n\t\t\t\tDRM_WARN(\"failed to remove hardware queue\\n\");\n\t\t}\n\n\t\tidr_remove(&adev->mes.gang_id_idr, gang->gang_id);\n\t}\n\n\tidr_remove(&adev->mes.pasid_idr, pasid);\n\tamdgpu_mes_unlock(&adev->mes);\n\n\t \n\tlist_for_each_entry_safe(gang, tmp1, &process->gang_list, list) {\n\t\t \n\t\tlist_for_each_entry_safe(queue, tmp2, &gang->queue_list, list) {\n\t\t\tamdgpu_mes_queue_free_mqd(queue);\n\t\t\tlist_del(&queue->list);\n\t\t\tkfree(queue);\n\t\t}\n\t\tamdgpu_bo_free_kernel(&gang->gang_ctx_bo,\n\t\t\t\t      &gang->gang_ctx_gpu_addr,\n\t\t\t\t      &gang->gang_ctx_cpu_ptr);\n\t\tlist_del(&gang->list);\n\t\tkfree(gang);\n\n\t}\n\tamdgpu_bo_free_kernel(&process->proc_ctx_bo,\n\t\t\t      &process->proc_ctx_gpu_addr,\n\t\t\t      &process->proc_ctx_cpu_ptr);\n\tkfree(process);\n}\n\nint amdgpu_mes_add_gang(struct amdgpu_device *adev, int pasid,\n\t\t\tstruct amdgpu_mes_gang_properties *gprops,\n\t\t\tint *gang_id)\n{\n\tstruct amdgpu_mes_process *process;\n\tstruct amdgpu_mes_gang *gang;\n\tint r;\n\n\t \n\tgang = kzalloc(sizeof(struct amdgpu_mes_gang), GFP_KERNEL);\n\tif (!gang) {\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tr = amdgpu_bo_create_kernel(adev, AMDGPU_MES_GANG_CTX_SIZE, PAGE_SIZE,\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t    &gang->gang_ctx_bo,\n\t\t\t\t    &gang->gang_ctx_gpu_addr,\n\t\t\t\t    &gang->gang_ctx_cpu_ptr);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to allocate process context bo\\n\");\n\t\tgoto clean_up_mem;\n\t}\n\tmemset(gang->gang_ctx_cpu_ptr, 0, AMDGPU_MES_GANG_CTX_SIZE);\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tprocess = idr_find(&adev->mes.pasid_idr, pasid);\n\tif (!process) {\n\t\tDRM_ERROR(\"pasid %d doesn't exist\\n\", pasid);\n\t\tr = -EINVAL;\n\t\tgoto clean_up_ctx;\n\t}\n\n\t \n\tr = idr_alloc(&adev->mes.gang_id_idr, gang, 1, 0,\n\t\t      GFP_KERNEL);\n\tif (r < 0) {\n\t\tDRM_ERROR(\"failed to allocate idr for gang\\n\");\n\t\tgoto clean_up_ctx;\n\t}\n\n\tgang->gang_id = r;\n\t*gang_id = r;\n\n\tINIT_LIST_HEAD(&gang->queue_list);\n\tgang->process = process;\n\tgang->priority = gprops->priority;\n\tgang->gang_quantum = gprops->gang_quantum ?\n\t\tgprops->gang_quantum : adev->mes.default_gang_quantum;\n\tgang->global_priority_level = gprops->global_priority_level;\n\tgang->inprocess_gang_priority = gprops->inprocess_gang_priority;\n\tlist_add_tail(&gang->list, &process->gang_list);\n\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn 0;\n\nclean_up_ctx:\n\tamdgpu_mes_unlock(&adev->mes);\n\tamdgpu_bo_free_kernel(&gang->gang_ctx_bo,\n\t\t\t      &gang->gang_ctx_gpu_addr,\n\t\t\t      &gang->gang_ctx_cpu_ptr);\nclean_up_mem:\n\tkfree(gang);\n\treturn r;\n}\n\nint amdgpu_mes_remove_gang(struct amdgpu_device *adev, int gang_id)\n{\n\tstruct amdgpu_mes_gang *gang;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tgang = idr_find(&adev->mes.gang_id_idr, gang_id);\n\tif (!gang) {\n\t\tDRM_ERROR(\"gang id %d doesn't exist\\n\", gang_id);\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!list_empty(&gang->queue_list)) {\n\t\tDRM_ERROR(\"queue list is not empty\\n\");\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\treturn -EBUSY;\n\t}\n\n\tidr_remove(&adev->mes.gang_id_idr, gang->gang_id);\n\tlist_del(&gang->list);\n\tamdgpu_mes_unlock(&adev->mes);\n\n\tamdgpu_bo_free_kernel(&gang->gang_ctx_bo,\n\t\t\t      &gang->gang_ctx_gpu_addr,\n\t\t\t      &gang->gang_ctx_cpu_ptr);\n\n\tkfree(gang);\n\n\treturn 0;\n}\n\nint amdgpu_mes_suspend(struct amdgpu_device *adev)\n{\n\tstruct idr *idp;\n\tstruct amdgpu_mes_process *process;\n\tstruct amdgpu_mes_gang *gang;\n\tstruct mes_suspend_gang_input input;\n\tint r, pasid;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tidp = &adev->mes.pasid_idr;\n\n\tidr_for_each_entry(idp, process, pasid) {\n\t\tlist_for_each_entry(gang, &process->gang_list, list) {\n\t\t\tr = adev->mes.funcs->suspend_gang(&adev->mes, &input);\n\t\t\tif (r)\n\t\t\t\tDRM_ERROR(\"failed to suspend pasid %d gangid %d\",\n\t\t\t\t\t pasid, gang->gang_id);\n\t\t}\n\t}\n\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn 0;\n}\n\nint amdgpu_mes_resume(struct amdgpu_device *adev)\n{\n\tstruct idr *idp;\n\tstruct amdgpu_mes_process *process;\n\tstruct amdgpu_mes_gang *gang;\n\tstruct mes_resume_gang_input input;\n\tint r, pasid;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tidp = &adev->mes.pasid_idr;\n\n\tidr_for_each_entry(idp, process, pasid) {\n\t\tlist_for_each_entry(gang, &process->gang_list, list) {\n\t\t\tr = adev->mes.funcs->resume_gang(&adev->mes, &input);\n\t\t\tif (r)\n\t\t\t\tDRM_ERROR(\"failed to resume pasid %d gangid %d\",\n\t\t\t\t\t pasid, gang->gang_id);\n\t\t}\n\t}\n\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn 0;\n}\n\nstatic int amdgpu_mes_queue_alloc_mqd(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_mes_queue *q,\n\t\t\t\t     struct amdgpu_mes_queue_properties *p)\n{\n\tstruct amdgpu_mqd *mqd_mgr = &adev->mqds[p->queue_type];\n\tu32 mqd_size = mqd_mgr->mqd_size;\n\tint r;\n\n\tr = amdgpu_bo_create_kernel(adev, mqd_size, PAGE_SIZE,\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t    &q->mqd_obj,\n\t\t\t\t    &q->mqd_gpu_addr, &q->mqd_cpu_ptr);\n\tif (r) {\n\t\tdev_warn(adev->dev, \"failed to create queue mqd bo (%d)\", r);\n\t\treturn r;\n\t}\n\tmemset(q->mqd_cpu_ptr, 0, mqd_size);\n\n\tr = amdgpu_bo_reserve(q->mqd_obj, false);\n\tif (unlikely(r != 0))\n\t\tgoto clean_up;\n\n\treturn 0;\n\nclean_up:\n\tamdgpu_bo_free_kernel(&q->mqd_obj,\n\t\t\t      &q->mqd_gpu_addr,\n\t\t\t      &q->mqd_cpu_ptr);\n\treturn r;\n}\n\nstatic void amdgpu_mes_queue_init_mqd(struct amdgpu_device *adev,\n\t\t\t\t     struct amdgpu_mes_queue *q,\n\t\t\t\t     struct amdgpu_mes_queue_properties *p)\n{\n\tstruct amdgpu_mqd *mqd_mgr = &adev->mqds[p->queue_type];\n\tstruct amdgpu_mqd_prop mqd_prop = {0};\n\n\tmqd_prop.mqd_gpu_addr = q->mqd_gpu_addr;\n\tmqd_prop.hqd_base_gpu_addr = p->hqd_base_gpu_addr;\n\tmqd_prop.rptr_gpu_addr = p->rptr_gpu_addr;\n\tmqd_prop.wptr_gpu_addr = p->wptr_gpu_addr;\n\tmqd_prop.queue_size = p->queue_size;\n\tmqd_prop.use_doorbell = true;\n\tmqd_prop.doorbell_index = p->doorbell_off;\n\tmqd_prop.eop_gpu_addr = p->eop_gpu_addr;\n\tmqd_prop.hqd_pipe_priority = p->hqd_pipe_priority;\n\tmqd_prop.hqd_queue_priority = p->hqd_queue_priority;\n\tmqd_prop.hqd_active = false;\n\n\tif (p->queue_type == AMDGPU_RING_TYPE_GFX ||\n\t    p->queue_type == AMDGPU_RING_TYPE_COMPUTE) {\n\t\tmutex_lock(&adev->srbm_mutex);\n\t\tamdgpu_gfx_select_me_pipe_q(adev, p->ring->me, p->ring->pipe, 0, 0, 0);\n\t}\n\n\tmqd_mgr->init_mqd(adev, q->mqd_cpu_ptr, &mqd_prop);\n\n\tif (p->queue_type == AMDGPU_RING_TYPE_GFX ||\n\t    p->queue_type == AMDGPU_RING_TYPE_COMPUTE) {\n\t\tamdgpu_gfx_select_me_pipe_q(adev, 0, 0, 0, 0, 0);\n\t\tmutex_unlock(&adev->srbm_mutex);\n\t}\n\n\tamdgpu_bo_unreserve(q->mqd_obj);\n}\n\nint amdgpu_mes_add_hw_queue(struct amdgpu_device *adev, int gang_id,\n\t\t\t    struct amdgpu_mes_queue_properties *qprops,\n\t\t\t    int *queue_id)\n{\n\tstruct amdgpu_mes_queue *queue;\n\tstruct amdgpu_mes_gang *gang;\n\tstruct mes_add_queue_input queue_input;\n\tunsigned long flags;\n\tint r;\n\n\tmemset(&queue_input, 0, sizeof(struct mes_add_queue_input));\n\n\t \n\tqueue = kzalloc(sizeof(struct amdgpu_mes_queue), GFP_KERNEL);\n\tif (!queue) {\n\t\tDRM_ERROR(\"Failed to allocate memory for queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tr = amdgpu_mes_queue_alloc_mqd(adev, queue, qprops);\n\tif (r)\n\t\tgoto clean_up_memory;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\tgang = idr_find(&adev->mes.gang_id_idr, gang_id);\n\tif (!gang) {\n\t\tDRM_ERROR(\"gang id %d doesn't exist\\n\", gang_id);\n\t\tr = -EINVAL;\n\t\tgoto clean_up_mqd;\n\t}\n\n\t \n\tspin_lock_irqsave(&adev->mes.queue_id_lock, flags);\n\tr = idr_alloc(&adev->mes.queue_id_idr, queue, 1, 0,\n\t\t      GFP_ATOMIC);\n\tif (r < 0) {\n\t\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\n\t\tgoto clean_up_mqd;\n\t}\n\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\n\t*queue_id = queue->queue_id = r;\n\n\t \n\tr = amdgpu_mes_kernel_doorbell_get(adev, gang->process,\n\t\t\t\t\t  qprops->queue_type,\n\t\t\t\t\t  &qprops->doorbell_off);\n\tif (r)\n\t\tgoto clean_up_queue_id;\n\n\t \n\tamdgpu_mes_queue_init_mqd(adev, queue, qprops);\n\n\t \n\tqueue_input.process_id = gang->process->pasid;\n\n\tqueue_input.page_table_base_addr =\n\t\tadev->vm_manager.vram_base_offset + gang->process->pd_gpu_addr -\n\t\tadev->gmc.vram_start;\n\n\tqueue_input.process_va_start = 0;\n\tqueue_input.process_va_end =\n\t\t(adev->vm_manager.max_pfn - 1) << AMDGPU_GPU_PAGE_SHIFT;\n\tqueue_input.process_quantum = gang->process->process_quantum;\n\tqueue_input.process_context_addr = gang->process->proc_ctx_gpu_addr;\n\tqueue_input.gang_quantum = gang->gang_quantum;\n\tqueue_input.gang_context_addr = gang->gang_ctx_gpu_addr;\n\tqueue_input.inprocess_gang_priority = gang->inprocess_gang_priority;\n\tqueue_input.gang_global_priority_level = gang->global_priority_level;\n\tqueue_input.doorbell_offset = qprops->doorbell_off;\n\tqueue_input.mqd_addr = queue->mqd_gpu_addr;\n\tqueue_input.wptr_addr = qprops->wptr_gpu_addr;\n\tqueue_input.wptr_mc_addr = qprops->wptr_mc_addr;\n\tqueue_input.queue_type = qprops->queue_type;\n\tqueue_input.paging = qprops->paging;\n\tqueue_input.is_kfd_process = 0;\n\n\tr = adev->mes.funcs->add_hw_queue(&adev->mes, &queue_input);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to add hardware queue to MES, doorbell=0x%llx\\n\",\n\t\t\t  qprops->doorbell_off);\n\t\tgoto clean_up_doorbell;\n\t}\n\n\tDRM_DEBUG(\"MES hw queue was added, pasid=%d, gang id=%d, \"\n\t\t  \"queue type=%d, doorbell=0x%llx\\n\",\n\t\t  gang->process->pasid, gang_id, qprops->queue_type,\n\t\t  qprops->doorbell_off);\n\n\tqueue->ring = qprops->ring;\n\tqueue->doorbell_off = qprops->doorbell_off;\n\tqueue->wptr_gpu_addr = qprops->wptr_gpu_addr;\n\tqueue->queue_type = qprops->queue_type;\n\tqueue->paging = qprops->paging;\n\tqueue->gang = gang;\n\tqueue->ring->mqd_ptr = queue->mqd_cpu_ptr;\n\tlist_add_tail(&queue->list, &gang->queue_list);\n\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn 0;\n\nclean_up_doorbell:\n\tamdgpu_mes_kernel_doorbell_free(adev, gang->process,\n\t\t\t\t       qprops->doorbell_off);\nclean_up_queue_id:\n\tspin_lock_irqsave(&adev->mes.queue_id_lock, flags);\n\tidr_remove(&adev->mes.queue_id_idr, queue->queue_id);\n\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\nclean_up_mqd:\n\tamdgpu_mes_unlock(&adev->mes);\n\tamdgpu_mes_queue_free_mqd(queue);\nclean_up_memory:\n\tkfree(queue);\n\treturn r;\n}\n\nint amdgpu_mes_remove_hw_queue(struct amdgpu_device *adev, int queue_id)\n{\n\tunsigned long flags;\n\tstruct amdgpu_mes_queue *queue;\n\tstruct amdgpu_mes_gang *gang;\n\tstruct mes_remove_queue_input queue_input;\n\tint r;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\n\t \n\tspin_lock_irqsave(&adev->mes.queue_id_lock, flags);\n\n\tqueue = idr_find(&adev->mes.queue_id_idr, queue_id);\n\tif (!queue) {\n\t\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\tDRM_ERROR(\"queue id %d doesn't exist\\n\", queue_id);\n\t\treturn -EINVAL;\n\t}\n\n\tidr_remove(&adev->mes.queue_id_idr, queue_id);\n\tspin_unlock_irqrestore(&adev->mes.queue_id_lock, flags);\n\n\tDRM_DEBUG(\"try to remove queue, doorbell off = 0x%llx\\n\",\n\t\t  queue->doorbell_off);\n\n\tgang = queue->gang;\n\tqueue_input.doorbell_offset = queue->doorbell_off;\n\tqueue_input.gang_context_addr = gang->gang_ctx_gpu_addr;\n\n\tr = adev->mes.funcs->remove_hw_queue(&adev->mes, &queue_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to remove hardware queue, queue id = %d\\n\",\n\t\t\t  queue_id);\n\n\tlist_del(&queue->list);\n\tamdgpu_mes_kernel_doorbell_free(adev, gang->process,\n\t\t\t\t       queue->doorbell_off);\n\tamdgpu_mes_unlock(&adev->mes);\n\n\tamdgpu_mes_queue_free_mqd(queue);\n\tkfree(queue);\n\treturn 0;\n}\n\nint amdgpu_mes_unmap_legacy_queue(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_ring *ring,\n\t\t\t\t  enum amdgpu_unmap_queues_action action,\n\t\t\t\t  u64 gpu_addr, u64 seq)\n{\n\tstruct mes_unmap_legacy_queue_input queue_input;\n\tint r;\n\n\tqueue_input.action = action;\n\tqueue_input.queue_type = ring->funcs->type;\n\tqueue_input.doorbell_offset = ring->doorbell_index;\n\tqueue_input.pipe_id = ring->pipe;\n\tqueue_input.queue_id = ring->queue;\n\tqueue_input.trail_fence_addr = gpu_addr;\n\tqueue_input.trail_fence_data = seq;\n\n\tr = adev->mes.funcs->unmap_legacy_queue(&adev->mes, &queue_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to unmap legacy queue\\n\");\n\n\treturn r;\n}\n\nuint32_t amdgpu_mes_rreg(struct amdgpu_device *adev, uint32_t reg)\n{\n\tstruct mes_misc_op_input op_input;\n\tint r, val = 0;\n\n\top_input.op = MES_MISC_OP_READ_REG;\n\top_input.read_reg.reg_offset = reg;\n\top_input.read_reg.buffer_addr = adev->mes.read_val_gpu_addr;\n\n\tif (!adev->mes.funcs->misc_op) {\n\t\tDRM_ERROR(\"mes rreg is not supported!\\n\");\n\t\tgoto error;\n\t}\n\n\tr = adev->mes.funcs->misc_op(&adev->mes, &op_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to read reg (0x%x)\\n\", reg);\n\telse\n\t\tval = *(adev->mes.read_val_ptr);\n\nerror:\n\treturn val;\n}\n\nint amdgpu_mes_wreg(struct amdgpu_device *adev,\n\t\t    uint32_t reg, uint32_t val)\n{\n\tstruct mes_misc_op_input op_input;\n\tint r;\n\n\top_input.op = MES_MISC_OP_WRITE_REG;\n\top_input.write_reg.reg_offset = reg;\n\top_input.write_reg.reg_value = val;\n\n\tif (!adev->mes.funcs->misc_op) {\n\t\tDRM_ERROR(\"mes wreg is not supported!\\n\");\n\t\tr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tr = adev->mes.funcs->misc_op(&adev->mes, &op_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to write reg (0x%x)\\n\", reg);\n\nerror:\n\treturn r;\n}\n\nint amdgpu_mes_reg_write_reg_wait(struct amdgpu_device *adev,\n\t\t\t\t  uint32_t reg0, uint32_t reg1,\n\t\t\t\t  uint32_t ref, uint32_t mask)\n{\n\tstruct mes_misc_op_input op_input;\n\tint r;\n\n\top_input.op = MES_MISC_OP_WRM_REG_WR_WAIT;\n\top_input.wrm_reg.reg0 = reg0;\n\top_input.wrm_reg.reg1 = reg1;\n\top_input.wrm_reg.ref = ref;\n\top_input.wrm_reg.mask = mask;\n\n\tif (!adev->mes.funcs->misc_op) {\n\t\tDRM_ERROR(\"mes reg_write_reg_wait is not supported!\\n\");\n\t\tr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tr = adev->mes.funcs->misc_op(&adev->mes, &op_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to reg_write_reg_wait\\n\");\n\nerror:\n\treturn r;\n}\n\nint amdgpu_mes_reg_wait(struct amdgpu_device *adev, uint32_t reg,\n\t\t\tuint32_t val, uint32_t mask)\n{\n\tstruct mes_misc_op_input op_input;\n\tint r;\n\n\top_input.op = MES_MISC_OP_WRM_REG_WAIT;\n\top_input.wrm_reg.reg0 = reg;\n\top_input.wrm_reg.ref = val;\n\top_input.wrm_reg.mask = mask;\n\n\tif (!adev->mes.funcs->misc_op) {\n\t\tDRM_ERROR(\"mes reg wait is not supported!\\n\");\n\t\tr = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tr = adev->mes.funcs->misc_op(&adev->mes, &op_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to reg_write_reg_wait\\n\");\n\nerror:\n\treturn r;\n}\n\nint amdgpu_mes_set_shader_debugger(struct amdgpu_device *adev,\n\t\t\t\tuint64_t process_context_addr,\n\t\t\t\tuint32_t spi_gdbg_per_vmid_cntl,\n\t\t\t\tconst uint32_t *tcp_watch_cntl,\n\t\t\t\tuint32_t flags,\n\t\t\t\tbool trap_en)\n{\n\tstruct mes_misc_op_input op_input = {0};\n\tint r;\n\n\tif (!adev->mes.funcs->misc_op) {\n\t\tDRM_ERROR(\"mes set shader debugger is not supported!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\top_input.op = MES_MISC_OP_SET_SHADER_DEBUGGER;\n\top_input.set_shader_debugger.process_context_addr = process_context_addr;\n\top_input.set_shader_debugger.flags.u32all = flags;\n\top_input.set_shader_debugger.spi_gdbg_per_vmid_cntl = spi_gdbg_per_vmid_cntl;\n\tmemcpy(op_input.set_shader_debugger.tcp_watch_cntl, tcp_watch_cntl,\n\t\t\tsizeof(op_input.set_shader_debugger.tcp_watch_cntl));\n\n\tif (((adev->mes.sched_version & AMDGPU_MES_API_VERSION_MASK) >>\n\t\t\tAMDGPU_MES_API_VERSION_SHIFT) >= 14)\n\t\top_input.set_shader_debugger.trap_en = trap_en;\n\n\tamdgpu_mes_lock(&adev->mes);\n\n\tr = adev->mes.funcs->misc_op(&adev->mes, &op_input);\n\tif (r)\n\t\tDRM_ERROR(\"failed to set_shader_debugger\\n\");\n\n\tamdgpu_mes_unlock(&adev->mes);\n\n\treturn r;\n}\n\nstatic void\namdgpu_mes_ring_to_queue_props(struct amdgpu_device *adev,\n\t\t\t       struct amdgpu_ring *ring,\n\t\t\t       struct amdgpu_mes_queue_properties *props)\n{\n\tprops->queue_type = ring->funcs->type;\n\tprops->hqd_base_gpu_addr = ring->gpu_addr;\n\tprops->rptr_gpu_addr = ring->rptr_gpu_addr;\n\tprops->wptr_gpu_addr = ring->wptr_gpu_addr;\n\tprops->wptr_mc_addr =\n\t\tring->mes_ctx->meta_data_mc_addr + ring->wptr_offs;\n\tprops->queue_size = ring->ring_size;\n\tprops->eop_gpu_addr = ring->eop_gpu_addr;\n\tprops->hqd_pipe_priority = AMDGPU_GFX_PIPE_PRIO_NORMAL;\n\tprops->hqd_queue_priority = AMDGPU_GFX_QUEUE_PRIORITY_MINIMUM;\n\tprops->paging = false;\n\tprops->ring = ring;\n}\n\n#define DEFINE_AMDGPU_MES_CTX_GET_OFFS_ENG(_eng)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n       if (id_offs < AMDGPU_MES_CTX_MAX_OFFS)\t\t\t\t\\\n\t\treturn offsetof(struct amdgpu_mes_ctx_meta_data,\t\\\n\t\t\t\t_eng[ring->idx].slots[id_offs]);        \\\n       else if (id_offs == AMDGPU_MES_CTX_RING_OFFS)\t\t\t\\\n\t\treturn offsetof(struct amdgpu_mes_ctx_meta_data,        \\\n\t\t\t\t_eng[ring->idx].ring);                  \\\n       else if (id_offs == AMDGPU_MES_CTX_IB_OFFS)\t\t\t\\\n\t\treturn offsetof(struct amdgpu_mes_ctx_meta_data,        \\\n\t\t\t\t_eng[ring->idx].ib);                    \\\n       else if (id_offs == AMDGPU_MES_CTX_PADDING_OFFS)\t\t\t\\\n\t\treturn offsetof(struct amdgpu_mes_ctx_meta_data,        \\\n\t\t\t\t_eng[ring->idx].padding);               \\\n} while(0)\n\nint amdgpu_mes_ctx_get_offs(struct amdgpu_ring *ring, unsigned int id_offs)\n{\n\tswitch (ring->funcs->type) {\n\tcase AMDGPU_RING_TYPE_GFX:\n\t\tDEFINE_AMDGPU_MES_CTX_GET_OFFS_ENG(gfx);\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_COMPUTE:\n\t\tDEFINE_AMDGPU_MES_CTX_GET_OFFS_ENG(compute);\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_SDMA:\n\t\tDEFINE_AMDGPU_MES_CTX_GET_OFFS_ENG(sdma);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tWARN_ON(1);\n\treturn -EINVAL;\n}\n\nint amdgpu_mes_add_ring(struct amdgpu_device *adev, int gang_id,\n\t\t\tint queue_type, int idx,\n\t\t\tstruct amdgpu_mes_ctx_data *ctx_data,\n\t\t\tstruct amdgpu_ring **out)\n{\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_mes_gang *gang;\n\tstruct amdgpu_mes_queue_properties qprops = {0};\n\tint r, queue_id, pasid;\n\n\t \n\tamdgpu_mes_lock(&adev->mes);\n\tgang = idr_find(&adev->mes.gang_id_idr, gang_id);\n\tif (!gang) {\n\t\tDRM_ERROR(\"gang id %d doesn't exist\\n\", gang_id);\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\treturn -EINVAL;\n\t}\n\tpasid = gang->process->pasid;\n\n\tring = kzalloc(sizeof(struct amdgpu_ring), GFP_KERNEL);\n\tif (!ring) {\n\t\tamdgpu_mes_unlock(&adev->mes);\n\t\treturn -ENOMEM;\n\t}\n\n\tring->ring_obj = NULL;\n\tring->use_doorbell = true;\n\tring->is_mes_queue = true;\n\tring->mes_ctx = ctx_data;\n\tring->idx = idx;\n\tring->no_scheduler = true;\n\n\tif (queue_type == AMDGPU_RING_TYPE_COMPUTE) {\n\t\tint offset = offsetof(struct amdgpu_mes_ctx_meta_data,\n\t\t\t\t      compute[ring->idx].mec_hpd);\n\t\tring->eop_gpu_addr =\n\t\t\tamdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);\n\t}\n\n\tswitch (queue_type) {\n\tcase AMDGPU_RING_TYPE_GFX:\n\t\tring->funcs = adev->gfx.gfx_ring[0].funcs;\n\t\tring->me = adev->gfx.gfx_ring[0].me;\n\t\tring->pipe = adev->gfx.gfx_ring[0].pipe;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_COMPUTE:\n\t\tring->funcs = adev->gfx.compute_ring[0].funcs;\n\t\tring->me = adev->gfx.compute_ring[0].me;\n\t\tring->pipe = adev->gfx.compute_ring[0].pipe;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_SDMA:\n\t\tring->funcs = adev->sdma.instance[0].ring.funcs;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tr = amdgpu_ring_init(adev, ring, 1024, NULL, 0,\n\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\tif (r)\n\t\tgoto clean_up_memory;\n\n\tamdgpu_mes_ring_to_queue_props(adev, ring, &qprops);\n\n\tdma_fence_wait(gang->process->vm->last_update, false);\n\tdma_fence_wait(ctx_data->meta_data_va->last_pt_update, false);\n\tamdgpu_mes_unlock(&adev->mes);\n\n\tr = amdgpu_mes_add_hw_queue(adev, gang_id, &qprops, &queue_id);\n\tif (r)\n\t\tgoto clean_up_ring;\n\n\tring->hw_queue_id = queue_id;\n\tring->doorbell_index = qprops.doorbell_off;\n\n\tif (queue_type == AMDGPU_RING_TYPE_GFX)\n\t\tsprintf(ring->name, \"gfx_%d.%d.%d\", pasid, gang_id, queue_id);\n\telse if (queue_type == AMDGPU_RING_TYPE_COMPUTE)\n\t\tsprintf(ring->name, \"compute_%d.%d.%d\", pasid, gang_id,\n\t\t\tqueue_id);\n\telse if (queue_type == AMDGPU_RING_TYPE_SDMA)\n\t\tsprintf(ring->name, \"sdma_%d.%d.%d\", pasid, gang_id,\n\t\t\tqueue_id);\n\telse\n\t\tBUG();\n\n\t*out = ring;\n\treturn 0;\n\nclean_up_ring:\n\tamdgpu_ring_fini(ring);\nclean_up_memory:\n\tkfree(ring);\n\tamdgpu_mes_unlock(&adev->mes);\n\treturn r;\n}\n\nvoid amdgpu_mes_remove_ring(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_ring *ring)\n{\n\tif (!ring)\n\t\treturn;\n\n\tamdgpu_mes_remove_hw_queue(adev, ring->hw_queue_id);\n\tamdgpu_ring_fini(ring);\n\tkfree(ring);\n}\n\nuint32_t amdgpu_mes_get_aggregated_doorbell_index(struct amdgpu_device *adev,\n\t\t\t\t\t\t   enum amdgpu_mes_priority_level prio)\n{\n\treturn adev->mes.aggregated_doorbells[prio];\n}\n\nint amdgpu_mes_ctx_alloc_meta_data(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_mes_ctx_data *ctx_data)\n{\n\tint r;\n\n\tr = amdgpu_bo_create_kernel(adev,\n\t\t\t    sizeof(struct amdgpu_mes_ctx_meta_data),\n\t\t\t    PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t    &ctx_data->meta_data_obj,\n\t\t\t    &ctx_data->meta_data_mc_addr,\n\t\t\t    &ctx_data->meta_data_ptr);\n\tif (r) {\n\t\tdev_warn(adev->dev, \"(%d) create CTX bo failed\\n\", r);\n\t\treturn r;\n\t}\n\n\tif (!ctx_data->meta_data_obj)\n\t\treturn -ENOMEM;\n\n\tmemset(ctx_data->meta_data_ptr, 0,\n\t       sizeof(struct amdgpu_mes_ctx_meta_data));\n\n\treturn 0;\n}\n\nvoid amdgpu_mes_ctx_free_meta_data(struct amdgpu_mes_ctx_data *ctx_data)\n{\n\tif (ctx_data->meta_data_obj)\n\t\tamdgpu_bo_free_kernel(&ctx_data->meta_data_obj,\n\t\t\t\t      &ctx_data->meta_data_mc_addr,\n\t\t\t\t      &ctx_data->meta_data_ptr);\n}\n\nint amdgpu_mes_ctx_map_meta_data(struct amdgpu_device *adev,\n\t\t\t\t struct amdgpu_vm *vm,\n\t\t\t\t struct amdgpu_mes_ctx_data *ctx_data)\n{\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct amdgpu_sync sync;\n\tstruct drm_exec exec;\n\tint r;\n\n\tamdgpu_sync_create(&sync);\n\n\tdrm_exec_init(&exec, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_lock_obj(&exec,\n\t\t\t\t      &ctx_data->meta_data_obj->tbo.base);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error_fini_exec;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto error_fini_exec;\n\t}\n\n\tbo_va = amdgpu_vm_bo_add(adev, vm, ctx_data->meta_data_obj);\n\tif (!bo_va) {\n\t\tDRM_ERROR(\"failed to create bo_va for meta data BO\\n\");\n\t\tr = -ENOMEM;\n\t\tgoto error_fini_exec;\n\t}\n\n\tr = amdgpu_vm_bo_map(adev, bo_va, ctx_data->meta_data_gpu_addr, 0,\n\t\t\t     sizeof(struct amdgpu_mes_ctx_meta_data),\n\t\t\t     AMDGPU_PTE_READABLE | AMDGPU_PTE_WRITEABLE |\n\t\t\t     AMDGPU_PTE_EXECUTABLE);\n\n\tif (r) {\n\t\tDRM_ERROR(\"failed to do bo_map on meta data, err=%d\\n\", r);\n\t\tgoto error_del_bo_va;\n\t}\n\n\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to do vm_bo_update on meta data\\n\");\n\t\tgoto error_del_bo_va;\n\t}\n\tamdgpu_sync_fence(&sync, bo_va->last_pt_update);\n\n\tr = amdgpu_vm_update_pdes(adev, vm, false);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to update pdes on meta data\\n\");\n\t\tgoto error_del_bo_va;\n\t}\n\tamdgpu_sync_fence(&sync, vm->last_update);\n\n\tamdgpu_sync_wait(&sync, false);\n\tdrm_exec_fini(&exec);\n\n\tamdgpu_sync_free(&sync);\n\tctx_data->meta_data_va = bo_va;\n\treturn 0;\n\nerror_del_bo_va:\n\tamdgpu_vm_bo_del(adev, bo_va);\n\nerror_fini_exec:\n\tdrm_exec_fini(&exec);\n\tamdgpu_sync_free(&sync);\n\treturn r;\n}\n\nint amdgpu_mes_ctx_unmap_meta_data(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_mes_ctx_data *ctx_data)\n{\n\tstruct amdgpu_bo_va *bo_va = ctx_data->meta_data_va;\n\tstruct amdgpu_bo *bo = ctx_data->meta_data_obj;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tstruct dma_fence *fence;\n\tstruct drm_exec exec;\n\tlong r;\n\n\tdrm_exec_init(&exec, 0);\n\tdrm_exec_until_all_locked(&exec) {\n\t\tr = drm_exec_lock_obj(&exec,\n\t\t\t\t      &ctx_data->meta_data_obj->tbo.base);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\n\t\tr = amdgpu_vm_lock_pd(vm, &exec, 0);\n\t\tdrm_exec_retry_on_contention(&exec);\n\t\tif (unlikely(r))\n\t\t\tgoto out_unlock;\n\t}\n\n\tamdgpu_vm_bo_del(adev, bo_va);\n\tif (!amdgpu_vm_ready(vm))\n\t\tgoto out_unlock;\n\n\tr = dma_resv_get_singleton(bo->tbo.base.resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t   &fence);\n\tif (r)\n\t\tgoto out_unlock;\n\tif (fence) {\n\t\tamdgpu_bo_fence(bo, fence, true);\n\t\tfence = NULL;\n\t}\n\n\tr = amdgpu_vm_clear_freed(adev, vm, &fence);\n\tif (r || !fence)\n\t\tgoto out_unlock;\n\n\tdma_fence_wait(fence, false);\n\tamdgpu_bo_fence(bo, fence, true);\n\tdma_fence_put(fence);\n\nout_unlock:\n\tif (unlikely(r < 0))\n\t\tdev_err(adev->dev, \"failed to clear page tables (%ld)\\n\", r);\n\tdrm_exec_fini(&exec);\n\n\treturn r;\n}\n\nstatic int amdgpu_mes_test_create_gang_and_queues(struct amdgpu_device *adev,\n\t\t\t\t\t  int pasid, int *gang_id,\n\t\t\t\t\t  int queue_type, int num_queue,\n\t\t\t\t\t  struct amdgpu_ring **added_rings,\n\t\t\t\t\t  struct amdgpu_mes_ctx_data *ctx_data)\n{\n\tstruct amdgpu_ring *ring;\n\tstruct amdgpu_mes_gang_properties gprops = {0};\n\tint r, j;\n\n\t \n\tgprops.priority = AMDGPU_MES_PRIORITY_LEVEL_NORMAL;\n\tgprops.gang_quantum = adev->mes.default_gang_quantum;\n\tgprops.inprocess_gang_priority = AMDGPU_MES_PRIORITY_LEVEL_NORMAL;\n\tgprops.priority_level = AMDGPU_MES_PRIORITY_LEVEL_NORMAL;\n\tgprops.global_priority_level = AMDGPU_MES_PRIORITY_LEVEL_NORMAL;\n\n\tr = amdgpu_mes_add_gang(adev, pasid, &gprops, gang_id);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to add gang\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tfor (j = 0; j < num_queue; j++) {\n\t\tr = amdgpu_mes_add_ring(adev, *gang_id, queue_type, j,\n\t\t\t\t\tctx_data, &ring);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"failed to add ring\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tDRM_INFO(\"ring %s was added\\n\", ring->name);\n\t\tadded_rings[j] = ring;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_mes_test_queues(struct amdgpu_ring **added_rings)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, r;\n\n\tfor (i = 0; i < AMDGPU_MES_CTX_MAX_RINGS; i++) {\n\t\tring = added_rings[i];\n\t\tif (!ring)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_ring_test_ib(ring, 1000 * 10);\n\t\tif (r) {\n\t\t\tDRM_DEV_ERROR(ring->adev->dev,\n\t\t\t\t      \"ring %s ib test failed (%d)\\n\",\n\t\t\t\t      ring->name, r);\n\t\t\treturn r;\n\t\t} else\n\t\t\tDRM_INFO(\"ring %s ib test pass\\n\", ring->name);\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_mes_self_test(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_vm *vm = NULL;\n\tstruct amdgpu_mes_ctx_data ctx_data = {0};\n\tstruct amdgpu_ring *added_rings[AMDGPU_MES_CTX_MAX_RINGS] = { NULL };\n\tint gang_ids[3] = {0};\n\tint queue_types[][2] = { { AMDGPU_RING_TYPE_GFX, 1 },\n\t\t\t\t { AMDGPU_RING_TYPE_COMPUTE, 1 },\n\t\t\t\t { AMDGPU_RING_TYPE_SDMA, 1} };\n\tint i, r, pasid, k = 0;\n\n\tpasid = amdgpu_pasid_alloc(16);\n\tif (pasid < 0) {\n\t\tdev_warn(adev->dev, \"No more PASIDs available!\");\n\t\tpasid = 0;\n\t}\n\n\tvm = kzalloc(sizeof(*vm), GFP_KERNEL);\n\tif (!vm) {\n\t\tr = -ENOMEM;\n\t\tgoto error_pasid;\n\t}\n\n\tr = amdgpu_vm_init(adev, vm, -1);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to initialize vm\\n\");\n\t\tgoto error_pasid;\n\t}\n\n\tr = amdgpu_mes_ctx_alloc_meta_data(adev, &ctx_data);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to alloc ctx meta data\\n\");\n\t\tgoto error_fini;\n\t}\n\n\tctx_data.meta_data_gpu_addr = AMDGPU_VA_RESERVED_SIZE;\n\tr = amdgpu_mes_ctx_map_meta_data(adev, vm, &ctx_data);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to map ctx meta data\\n\");\n\t\tgoto error_vm;\n\t}\n\n\tr = amdgpu_mes_create_process(adev, pasid, vm);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to create MES process\\n\");\n\t\tgoto error_vm;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(queue_types); i++) {\n\t\t \n\t\tif (adev->ip_versions[GC_HWIP][0] >= IP_VERSION(10, 3, 0) &&\n\t\t    adev->ip_versions[GC_HWIP][0] < IP_VERSION(11, 0, 0) &&\n\t\t    queue_types[i][0] == AMDGPU_RING_TYPE_SDMA)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_mes_test_create_gang_and_queues(adev, pasid,\n\t\t\t\t\t\t\t   &gang_ids[i],\n\t\t\t\t\t\t\t   queue_types[i][0],\n\t\t\t\t\t\t\t   queue_types[i][1],\n\t\t\t\t\t\t\t   &added_rings[k],\n\t\t\t\t\t\t\t   &ctx_data);\n\t\tif (r)\n\t\t\tgoto error_queues;\n\n\t\tk += queue_types[i][1];\n\t}\n\n\t \n\tamdgpu_mes_test_queues(added_rings);\n\nerror_queues:\n\t \n\tfor (i = 0; i < ARRAY_SIZE(added_rings); i++) {\n\t\tif (!added_rings[i])\n\t\t\tcontinue;\n\t\tamdgpu_mes_remove_ring(adev, added_rings[i]);\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(gang_ids); i++) {\n\t\tif (!gang_ids[i])\n\t\t\tcontinue;\n\t\tamdgpu_mes_remove_gang(adev, gang_ids[i]);\n\t}\n\n\tamdgpu_mes_destroy_process(adev, pasid);\n\nerror_vm:\n\tamdgpu_mes_ctx_unmap_meta_data(adev, &ctx_data);\n\nerror_fini:\n\tamdgpu_vm_fini(adev, vm);\n\nerror_pasid:\n\tif (pasid)\n\t\tamdgpu_pasid_free(pasid);\n\n\tamdgpu_mes_ctx_free_meta_data(&ctx_data);\n\tkfree(vm);\n\treturn 0;\n}\n\nint amdgpu_mes_init_microcode(struct amdgpu_device *adev, int pipe)\n{\n\tconst struct mes_firmware_header_v1_0 *mes_hdr;\n\tstruct amdgpu_firmware_info *info;\n\tchar ucode_prefix[30];\n\tchar fw_name[40];\n\tbool need_retry = false;\n\tint r;\n\n\tamdgpu_ucode_ip_version_decode(adev, GC_HWIP, ucode_prefix,\n\t\t\t\t       sizeof(ucode_prefix));\n\tif (adev->ip_versions[GC_HWIP][0] >= IP_VERSION(11, 0, 0)) {\n\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mes%s.bin\",\n\t\t\t ucode_prefix,\n\t\t\t pipe == AMDGPU_MES_SCHED_PIPE ? \"_2\" : \"1\");\n\t\tneed_retry = true;\n\t} else {\n\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mes%s.bin\",\n\t\t\t ucode_prefix,\n\t\t\t pipe == AMDGPU_MES_SCHED_PIPE ? \"\" : \"1\");\n\t}\n\n\tr = amdgpu_ucode_request(adev, &adev->mes.fw[pipe], fw_name);\n\tif (r && need_retry && pipe == AMDGPU_MES_SCHED_PIPE) {\n\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mes.bin\",\n\t\t\t ucode_prefix);\n\t\tDRM_INFO(\"try to fall back to %s\\n\", fw_name);\n\t\tr = amdgpu_ucode_request(adev, &adev->mes.fw[pipe],\n\t\t\t\t\t fw_name);\n\t}\n\n\tif (r)\n\t\tgoto out;\n\n\tmes_hdr = (const struct mes_firmware_header_v1_0 *)\n\t\tadev->mes.fw[pipe]->data;\n\tadev->mes.uc_start_addr[pipe] =\n\t\tle32_to_cpu(mes_hdr->mes_uc_start_addr_lo) |\n\t\t((uint64_t)(le32_to_cpu(mes_hdr->mes_uc_start_addr_hi)) << 32);\n\tadev->mes.data_start_addr[pipe] =\n\t\tle32_to_cpu(mes_hdr->mes_data_start_addr_lo) |\n\t\t((uint64_t)(le32_to_cpu(mes_hdr->mes_data_start_addr_hi)) << 32);\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP) {\n\t\tint ucode, ucode_data;\n\n\t\tif (pipe == AMDGPU_MES_SCHED_PIPE) {\n\t\t\tucode = AMDGPU_UCODE_ID_CP_MES;\n\t\t\tucode_data = AMDGPU_UCODE_ID_CP_MES_DATA;\n\t\t} else {\n\t\t\tucode = AMDGPU_UCODE_ID_CP_MES1;\n\t\t\tucode_data = AMDGPU_UCODE_ID_CP_MES1_DATA;\n\t\t}\n\n\t\tinfo = &adev->firmware.ucode[ucode];\n\t\tinfo->ucode_id = ucode;\n\t\tinfo->fw = adev->mes.fw[pipe];\n\t\tadev->firmware.fw_size +=\n\t\t\tALIGN(le32_to_cpu(mes_hdr->mes_ucode_size_bytes),\n\t\t\t      PAGE_SIZE);\n\n\t\tinfo = &adev->firmware.ucode[ucode_data];\n\t\tinfo->ucode_id = ucode_data;\n\t\tinfo->fw = adev->mes.fw[pipe];\n\t\tadev->firmware.fw_size +=\n\t\t\tALIGN(le32_to_cpu(mes_hdr->mes_ucode_data_size_bytes),\n\t\t\t      PAGE_SIZE);\n\t}\n\n\treturn 0;\nout:\n\tamdgpu_ucode_release(&adev->mes.fw[pipe]);\n\treturn r;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}