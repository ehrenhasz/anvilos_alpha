{
  "module_name": "amdgpu_pmu.c",
  "hash_id": "6cb54968d59e69e821f9c11d67353d5904e9356ecf9e8c1db95d7918db966991",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_pmu.c",
  "human_readable_source": " \n\n#include <linux/perf_event.h>\n#include <linux/init.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_pmu.h\"\n\n#define PMU_NAME_SIZE 32\n#define NUM_FORMATS_AMDGPU_PMU\t\t4\n#define NUM_FORMATS_DF_VEGA20\t\t3\n#define NUM_EVENTS_DF_VEGA20\t\t8\n#define NUM_EVENT_TYPES_VEGA20\t\t1\n#define NUM_EVENTS_VEGA20_XGMI\t\t2\n#define NUM_EVENTS_VEGA20_MAX\t\tNUM_EVENTS_VEGA20_XGMI\n#define NUM_EVENT_TYPES_ARCTURUS\t1\n#define NUM_EVENTS_ARCTURUS_XGMI\t6\n#define NUM_EVENTS_ARCTURUS_MAX\t\tNUM_EVENTS_ARCTURUS_XGMI\n\nstruct amdgpu_pmu_event_attribute {\n\tstruct device_attribute attr;\n\tconst char *event_str;\n\tunsigned int type;\n};\n\n \nstruct amdgpu_pmu_entry {\n\tstruct list_head entry;\n\tstruct amdgpu_device *adev;\n\tstruct pmu pmu;\n\tunsigned int pmu_perf_type;\n\tchar *pmu_type_name;\n\tchar *pmu_file_prefix;\n\tstruct attribute_group fmt_attr_group;\n\tstruct amdgpu_pmu_event_attribute *fmt_attr;\n\tstruct attribute_group evt_attr_group;\n\tstruct amdgpu_pmu_event_attribute *evt_attr;\n};\n\nstatic ssize_t amdgpu_pmu_event_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct amdgpu_pmu_event_attribute *amdgpu_pmu_attr;\n\n\tamdgpu_pmu_attr = container_of(attr, struct amdgpu_pmu_event_attribute,\n\t\t\t\t\t\t\t\t\tattr);\n\n\tif (!amdgpu_pmu_attr->type)\n\t\treturn sprintf(buf, \"%s\\n\", amdgpu_pmu_attr->event_str);\n\n\treturn sprintf(buf, \"%s,type=0x%x\\n\",\n\t\t\tamdgpu_pmu_attr->event_str, amdgpu_pmu_attr->type);\n}\n\nstatic LIST_HEAD(amdgpu_pmu_list);\n\n\nstruct amdgpu_pmu_attr {\n\tconst char *name;\n\tconst char *config;\n};\n\nstruct amdgpu_pmu_type {\n\tconst unsigned int type;\n\tconst unsigned int num_of_type;\n};\n\nstruct amdgpu_pmu_config {\n\tstruct amdgpu_pmu_attr *formats;\n\tunsigned int num_formats;\n\tstruct amdgpu_pmu_attr *events;\n\tunsigned int num_events;\n\tstruct amdgpu_pmu_type *types;\n\tunsigned int num_types;\n};\n\n \n\nstatic struct amdgpu_pmu_attr amdgpu_pmu_formats[NUM_FORMATS_AMDGPU_PMU] = {\n\t{ .name = \"event\", .config = \"config:0-7\" },\n\t{ .name = \"instance\", .config = \"config:8-15\" },\n\t{ .name = \"umask\", .config = \"config:16-23\"},\n\t{ .name = \"type\", .config = \"config:56-63\"}\n};\n\n \nstatic struct amdgpu_pmu_attr vega20_events[NUM_EVENTS_VEGA20_MAX] = {\n\t{ .name = \"xgmi_link0_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x46,umask=0x2\" },\n\t{ .name = \"xgmi_link1_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x47,umask=0x2\" }\n};\n\nstatic struct amdgpu_pmu_type vega20_types[NUM_EVENT_TYPES_VEGA20] = {\n\t{ .type = AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI,\n\t\t\t\t\t.num_of_type = NUM_EVENTS_VEGA20_XGMI }\n};\n\nstatic struct amdgpu_pmu_config vega20_config = {\n\t.formats = amdgpu_pmu_formats,\n\t.num_formats = ARRAY_SIZE(amdgpu_pmu_formats),\n\t.events = vega20_events,\n\t.num_events = ARRAY_SIZE(vega20_events),\n\t.types = vega20_types,\n\t.num_types = ARRAY_SIZE(vega20_types)\n};\n\n \nstatic struct amdgpu_pmu_attr df_vega20_formats[NUM_FORMATS_DF_VEGA20] = {\n\t{ .name = \"event\", .config = \"config:0-7\" },\n\t{ .name = \"instance\", .config = \"config:8-15\" },\n\t{ .name = \"umask\", .config = \"config:16-23\"}\n};\n\nstatic struct amdgpu_pmu_attr df_vega20_events[NUM_EVENTS_DF_VEGA20] = {\n\t{ .name = \"cake0_pcsout_txdata\",\n\t\t\t.config = \"event=0x7,instance=0x46,umask=0x2\" },\n\t{ .name = \"cake1_pcsout_txdata\",\n\t\t\t.config = \"event=0x7,instance=0x47,umask=0x2\" },\n\t{ .name = \"cake0_pcsout_txmeta\",\n\t\t\t.config = \"event=0x7,instance=0x46,umask=0x4\" },\n\t{ .name = \"cake1_pcsout_txmeta\",\n\t\t\t.config = \"event=0x7,instance=0x47,umask=0x4\" },\n\t{ .name = \"cake0_ftiinstat_reqalloc\",\n\t\t\t.config = \"event=0xb,instance=0x46,umask=0x4\" },\n\t{ .name = \"cake1_ftiinstat_reqalloc\",\n\t\t\t.config = \"event=0xb,instance=0x47,umask=0x4\" },\n\t{ .name = \"cake0_ftiinstat_rspalloc\",\n\t\t\t.config = \"event=0xb,instance=0x46,umask=0x8\" },\n\t{ .name = \"cake1_ftiinstat_rspalloc\",\n\t\t\t.config = \"event=0xb,instance=0x47,umask=0x8\" }\n};\n\nstatic struct amdgpu_pmu_config df_vega20_config = {\n\t.formats = df_vega20_formats,\n\t.num_formats = ARRAY_SIZE(df_vega20_formats),\n\t.events = df_vega20_events,\n\t.num_events = ARRAY_SIZE(df_vega20_events),\n\t.types = NULL,\n\t.num_types = 0\n};\n\n \nstatic struct amdgpu_pmu_attr arcturus_events[NUM_EVENTS_ARCTURUS_MAX] = {\n\t{ .name = \"xgmi_link0_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x4b,umask=0x2\" },\n\t{ .name = \"xgmi_link1_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x4c,umask=0x2\" },\n\t{ .name = \"xgmi_link2_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x4d,umask=0x2\" },\n\t{ .name = \"xgmi_link3_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x4e,umask=0x2\" },\n\t{ .name = \"xgmi_link4_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x4f,umask=0x2\" },\n\t{ .name = \"xgmi_link5_data_outbound\",\n\t\t\t.config = \"event=0x7,instance=0x50,umask=0x2\" }\n};\n\nstatic struct amdgpu_pmu_type arcturus_types[NUM_EVENT_TYPES_ARCTURUS] = {\n\t{ .type = AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI,\n\t\t\t\t.num_of_type = NUM_EVENTS_ARCTURUS_XGMI }\n};\n\nstatic struct amdgpu_pmu_config arcturus_config = {\n\t.formats = amdgpu_pmu_formats,\n\t.num_formats = ARRAY_SIZE(amdgpu_pmu_formats),\n\t.events = arcturus_events,\n\t.num_events = ARRAY_SIZE(arcturus_events),\n\t.types = arcturus_types,\n\t.num_types = ARRAY_SIZE(arcturus_types)\n};\n\n \nstatic int amdgpu_perf_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\thwc->config = event->attr.config;\n\thwc->config_base = AMDGPU_PMU_PERF_TYPE_NONE;\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_perf_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amdgpu_pmu_entry *pe = container_of(event->pmu,\n\t\t\t\t\t\t  struct amdgpu_pmu_entry,\n\t\t\t\t\t\t  pmu);\n\tint target_cntr = 0;\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif ((!pe->adev->df.funcs) ||\n\t    (!pe->adev->df.funcs->pmc_start))\n\t\treturn;\n\n\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\thwc->state = 0;\n\n\tswitch (hwc->config_base) {\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_DF:\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI:\n\t\tif (!(flags & PERF_EF_RELOAD)) {\n\t\t\ttarget_cntr = pe->adev->df.funcs->pmc_start(pe->adev,\n\t\t\t\t\t\thwc->config, 0  ,\n\t\t\t\t\t\t1  );\n\t\t\tif (target_cntr < 0)\n\t\t\t\tbreak;\n\n\t\t\thwc->idx = target_cntr;\n\t\t}\n\n\t\tpe->adev->df.funcs->pmc_start(pe->adev, hwc->config,\n\t\t\t\t\t\t\t\thwc->idx, 0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tperf_event_update_userpage(event);\n}\n\n \nstatic void amdgpu_perf_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amdgpu_pmu_entry *pe = container_of(event->pmu,\n\t\t\t\t\t\t  struct amdgpu_pmu_entry,\n\t\t\t\t\t\t  pmu);\n\tu64 count, prev;\n\n\tif ((!pe->adev->df.funcs) ||\n\t    (!pe->adev->df.funcs->pmc_get_count))\n\t\treturn;\n\n\tprev = local64_read(&hwc->prev_count);\n\tdo {\n\t\tswitch (hwc->config_base) {\n\t\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_DF:\n\t\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI:\n\t\t\tpe->adev->df.funcs->pmc_get_count(pe->adev,\n\t\t\t\t\t\thwc->config, hwc->idx, &count);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tcount = 0;\n\t\t\tbreak;\n\t\t}\n\t} while (!local64_try_cmpxchg(&hwc->prev_count, &prev, count));\n\n\tlocal64_add(count - prev, &event->count);\n}\n\n \nstatic void amdgpu_perf_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amdgpu_pmu_entry *pe = container_of(event->pmu,\n\t\t\t\t\t\t  struct amdgpu_pmu_entry,\n\t\t\t\t\t\t  pmu);\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\tif ((!pe->adev->df.funcs) ||\n\t    (!pe->adev->df.funcs->pmc_stop))\n\t\treturn;\n\n\tswitch (hwc->config_base) {\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_DF:\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI:\n\t\tpe->adev->df.funcs->pmc_stop(pe->adev, hwc->config, hwc->idx,\n\t\t\t\t\t\t\t\t\t0);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\thwc->state |= PERF_HES_STOPPED;\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\tamdgpu_perf_read(event);\n\thwc->state |= PERF_HES_UPTODATE;\n}\n\n \nstatic int amdgpu_perf_add(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint retval = 0, target_cntr;\n\tstruct amdgpu_pmu_entry *pe = container_of(event->pmu,\n\t\t\t\t\t\t  struct amdgpu_pmu_entry,\n\t\t\t\t\t\t  pmu);\n\n\tif ((!pe->adev->df.funcs) ||\n\t    (!pe->adev->df.funcs->pmc_start))\n\t\treturn -EINVAL;\n\n\tswitch (pe->pmu_perf_type) {\n\tcase AMDGPU_PMU_PERF_TYPE_DF:\n\t\thwc->config_base = AMDGPU_PMU_EVENT_CONFIG_TYPE_DF;\n\t\tbreak;\n\tcase AMDGPU_PMU_PERF_TYPE_ALL:\n\t\thwc->config_base = (hwc->config >>\n\t\t\t\t\tAMDGPU_PMU_EVENT_CONFIG_TYPE_SHIFT) &\n\t\t\t\t\tAMDGPU_PMU_EVENT_CONFIG_TYPE_MASK;\n\t\tbreak;\n\t}\n\n\tevent->hw.state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\tswitch (hwc->config_base) {\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_DF:\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI:\n\t\ttarget_cntr = pe->adev->df.funcs->pmc_start(pe->adev,\n\t\t\t\t\t\thwc->config, 0  ,\n\t\t\t\t\t\t1  );\n\t\tif (target_cntr < 0)\n\t\t\tretval = target_cntr;\n\t\telse\n\t\t\thwc->idx = target_cntr;\n\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (retval)\n\t\treturn retval;\n\n\tif (flags & PERF_EF_START)\n\t\tamdgpu_perf_start(event, PERF_EF_RELOAD);\n\n\treturn retval;\n}\n\n \nstatic void amdgpu_perf_del(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amdgpu_pmu_entry *pe = container_of(event->pmu,\n\t\t\t\t\t\t  struct amdgpu_pmu_entry,\n\t\t\t\t\t\t  pmu);\n\tif ((!pe->adev->df.funcs) ||\n\t    (!pe->adev->df.funcs->pmc_stop))\n\t\treturn;\n\n\tamdgpu_perf_stop(event, PERF_EF_UPDATE);\n\n\tswitch (hwc->config_base) {\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_DF:\n\tcase AMDGPU_PMU_EVENT_CONFIG_TYPE_XGMI:\n\t\tpe->adev->df.funcs->pmc_stop(pe->adev, hwc->config, hwc->idx,\n\t\t\t\t\t\t\t\t\t1);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void amdgpu_pmu_create_event_attrs_by_type(\n\t\t\t\tstruct attribute_group *attr_group,\n\t\t\t\tstruct amdgpu_pmu_event_attribute *pmu_attr,\n\t\t\t\tstruct amdgpu_pmu_attr events[],\n\t\t\t\tint s_offset,\n\t\t\t\tint e_offset,\n\t\t\t\tunsigned int type)\n{\n\tint i;\n\n\tpmu_attr += s_offset;\n\n\tfor (i = s_offset; i < e_offset; i++) {\n\t\tattr_group->attrs[i] = &pmu_attr->attr.attr;\n\t\tsysfs_attr_init(&pmu_attr->attr.attr);\n\t\tpmu_attr->attr.attr.name = events[i].name;\n\t\tpmu_attr->attr.attr.mode = 0444;\n\t\tpmu_attr->attr.show = amdgpu_pmu_event_show;\n\t\tpmu_attr->event_str = events[i].config;\n\t\tpmu_attr->type = type;\n\t\tpmu_attr++;\n\t}\n}\n\nstatic void amdgpu_pmu_create_attrs(struct attribute_group *attr_group,\n\t\t\t\tstruct amdgpu_pmu_event_attribute *pmu_attr,\n\t\t\t\tstruct amdgpu_pmu_attr events[],\n\t\t\t\tint num_events)\n{\n\tamdgpu_pmu_create_event_attrs_by_type(attr_group, pmu_attr, events, 0,\n\t\t\t\tnum_events, AMDGPU_PMU_EVENT_CONFIG_TYPE_NONE);\n}\n\n\nstatic int amdgpu_pmu_alloc_pmu_attrs(\n\t\t\t\tstruct attribute_group *fmt_attr_group,\n\t\t\t\tstruct amdgpu_pmu_event_attribute **fmt_attr,\n\t\t\t\tstruct attribute_group *evt_attr_group,\n\t\t\t\tstruct amdgpu_pmu_event_attribute **evt_attr,\n\t\t\t\tstruct amdgpu_pmu_config *config)\n{\n\t*fmt_attr = kcalloc(config->num_formats, sizeof(**fmt_attr),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\n\tif (!(*fmt_attr))\n\t\treturn -ENOMEM;\n\n\tfmt_attr_group->attrs = kcalloc(config->num_formats + 1,\n\t\t\t\tsizeof(*fmt_attr_group->attrs), GFP_KERNEL);\n\n\tif (!fmt_attr_group->attrs)\n\t\tgoto err_fmt_attr_grp;\n\n\t*evt_attr = kcalloc(config->num_events, sizeof(**evt_attr), GFP_KERNEL);\n\n\tif (!(*evt_attr))\n\t\tgoto err_evt_attr;\n\n\tevt_attr_group->attrs = kcalloc(config->num_events + 1,\n\t\t\t\tsizeof(*evt_attr_group->attrs), GFP_KERNEL);\n\n\tif (!evt_attr_group->attrs)\n\t\tgoto err_evt_attr_grp;\n\n\treturn 0;\nerr_evt_attr_grp:\n\tkfree(*evt_attr);\nerr_evt_attr:\n\tkfree(fmt_attr_group->attrs);\nerr_fmt_attr_grp:\n\tkfree(*fmt_attr);\n\treturn -ENOMEM;\n}\n\n \nstatic int init_pmu_entry_by_type_and_add(struct amdgpu_pmu_entry *pmu_entry,\n\t\t\tstruct amdgpu_pmu_config *config)\n{\n\tconst struct attribute_group *attr_groups[] = {\n\t\t&pmu_entry->fmt_attr_group,\n\t\t&pmu_entry->evt_attr_group,\n\t\tNULL\n\t};\n\tchar pmu_name[PMU_NAME_SIZE];\n\tint ret = 0, total_num_events = 0;\n\n\tpmu_entry->pmu = (struct pmu){\n\t\t.event_init = amdgpu_perf_event_init,\n\t\t.add = amdgpu_perf_add,\n\t\t.del = amdgpu_perf_del,\n\t\t.start = amdgpu_perf_start,\n\t\t.stop = amdgpu_perf_stop,\n\t\t.read = amdgpu_perf_read,\n\t\t.task_ctx_nr = perf_invalid_context,\n\t};\n\n\tret = amdgpu_pmu_alloc_pmu_attrs(&pmu_entry->fmt_attr_group,\n\t\t\t\t\t&pmu_entry->fmt_attr,\n\t\t\t\t\t&pmu_entry->evt_attr_group,\n\t\t\t\t\t&pmu_entry->evt_attr,\n\t\t\t\t\tconfig);\n\n\tif (ret)\n\t\tgoto err_out;\n\n\tamdgpu_pmu_create_attrs(&pmu_entry->fmt_attr_group, pmu_entry->fmt_attr,\n\t\t\t\t\tconfig->formats, config->num_formats);\n\n\tif (pmu_entry->pmu_perf_type == AMDGPU_PMU_PERF_TYPE_ALL) {\n\t\tint i;\n\n\t\tfor (i = 0; i < config->num_types; i++) {\n\t\t\tamdgpu_pmu_create_event_attrs_by_type(\n\t\t\t\t\t&pmu_entry->evt_attr_group,\n\t\t\t\t\tpmu_entry->evt_attr,\n\t\t\t\t\tconfig->events,\n\t\t\t\t\ttotal_num_events,\n\t\t\t\t\ttotal_num_events +\n\t\t\t\t\t\tconfig->types[i].num_of_type,\n\t\t\t\t\tconfig->types[i].type);\n\t\t\ttotal_num_events += config->types[i].num_of_type;\n\t\t}\n\t} else {\n\t\tamdgpu_pmu_create_attrs(&pmu_entry->evt_attr_group,\n\t\t\t\t\tpmu_entry->evt_attr,\n\t\t\t\t\tconfig->events, config->num_events);\n\t\ttotal_num_events = config->num_events;\n\t}\n\n\tpmu_entry->pmu.attr_groups = kmemdup(attr_groups, sizeof(attr_groups),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\n\tif (!pmu_entry->pmu.attr_groups) {\n\t\tret = -ENOMEM;\n\t\tgoto err_attr_group;\n\t}\n\n\tsnprintf(pmu_name, PMU_NAME_SIZE, \"%s_%d\", pmu_entry->pmu_file_prefix,\n\t\t\t\tadev_to_drm(pmu_entry->adev)->primary->index);\n\n\tret = perf_pmu_register(&pmu_entry->pmu, pmu_name, -1);\n\n\tif (ret)\n\t\tgoto err_register;\n\n\tif (pmu_entry->pmu_perf_type != AMDGPU_PMU_PERF_TYPE_ALL)\n\t\tpr_info(\"Detected AMDGPU %s Counters. # of Counters = %d.\\n\",\n\t\t\t\tpmu_entry->pmu_type_name, total_num_events);\n\telse\n\t\tpr_info(\"Detected AMDGPU %d Perf Events.\\n\", total_num_events);\n\n\n\tlist_add_tail(&pmu_entry->entry, &amdgpu_pmu_list);\n\n\treturn 0;\nerr_register:\n\tkfree(pmu_entry->pmu.attr_groups);\nerr_attr_group:\n\tkfree(pmu_entry->fmt_attr_group.attrs);\n\tkfree(pmu_entry->fmt_attr);\n\tkfree(pmu_entry->evt_attr_group.attrs);\n\tkfree(pmu_entry->evt_attr);\nerr_out:\n\tpr_warn(\"Error initializing AMDGPU %s PMUs.\\n\",\n\t\t\t\t\t\tpmu_entry->pmu_type_name);\n\treturn ret;\n}\n\n \nvoid amdgpu_pmu_fini(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_pmu_entry *pe, *temp;\n\n\tlist_for_each_entry_safe(pe, temp, &amdgpu_pmu_list, entry) {\n\t\tif (pe->adev != adev)\n\t\t\tcontinue;\n\t\tlist_del(&pe->entry);\n\t\tperf_pmu_unregister(&pe->pmu);\n\t\tkfree(pe->pmu.attr_groups);\n\t\tkfree(pe->fmt_attr_group.attrs);\n\t\tkfree(pe->fmt_attr);\n\t\tkfree(pe->evt_attr_group.attrs);\n\t\tkfree(pe->evt_attr);\n\t\tkfree(pe);\n\t}\n}\n\nstatic struct amdgpu_pmu_entry *create_pmu_entry(struct amdgpu_device *adev,\n\t\t\t\t\t\tunsigned int pmu_type,\n\t\t\t\t\t\tchar *pmu_type_name,\n\t\t\t\t\t\tchar *pmu_file_prefix)\n{\n\tstruct amdgpu_pmu_entry *pmu_entry;\n\n\tpmu_entry = kzalloc(sizeof(struct amdgpu_pmu_entry), GFP_KERNEL);\n\n\tif (!pmu_entry)\n\t\treturn pmu_entry;\n\n\tpmu_entry->adev = adev;\n\tpmu_entry->fmt_attr_group.name = \"format\";\n\tpmu_entry->fmt_attr_group.attrs = NULL;\n\tpmu_entry->evt_attr_group.name = \"events\";\n\tpmu_entry->evt_attr_group.attrs = NULL;\n\tpmu_entry->pmu_perf_type = pmu_type;\n\tpmu_entry->pmu_type_name = pmu_type_name;\n\tpmu_entry->pmu_file_prefix = pmu_file_prefix;\n\n\treturn pmu_entry;\n}\n\n \nint amdgpu_pmu_init(struct amdgpu_device *adev)\n{\n\tint ret = 0;\n\tstruct amdgpu_pmu_entry *pmu_entry, *pmu_entry_df;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_VEGA20:\n\t\tpmu_entry_df = create_pmu_entry(adev, AMDGPU_PMU_PERF_TYPE_DF,\n\t\t\t\t\t\t\"DF\", \"amdgpu_df\");\n\n\t\tif (!pmu_entry_df)\n\t\t\treturn -ENOMEM;\n\n\t\tret = init_pmu_entry_by_type_and_add(pmu_entry_df,\n\t\t\t\t\t\t\t&df_vega20_config);\n\n\t\tif (ret) {\n\t\t\tkfree(pmu_entry_df);\n\t\t\treturn ret;\n\t\t}\n\n\t\tpmu_entry = create_pmu_entry(adev, AMDGPU_PMU_PERF_TYPE_ALL,\n\t\t\t\t\t\t\"\", \"amdgpu\");\n\n\t\tif (!pmu_entry) {\n\t\t\tamdgpu_pmu_fini(adev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tret = init_pmu_entry_by_type_and_add(pmu_entry,\n\t\t\t\t\t\t\t&vega20_config);\n\n\t\tif (ret) {\n\t\t\tkfree(pmu_entry);\n\t\t\tamdgpu_pmu_fini(adev);\n\t\t\treturn ret;\n\t\t}\n\n\t\tbreak;\n\tcase CHIP_ARCTURUS:\n\t\tpmu_entry = create_pmu_entry(adev, AMDGPU_PMU_PERF_TYPE_ALL,\n\t\t\t\t\t\t\"\", \"amdgpu\");\n\t\tif (!pmu_entry)\n\t\t\treturn -ENOMEM;\n\n\t\tret = init_pmu_entry_by_type_and_add(pmu_entry,\n\t\t\t\t\t\t\t&arcturus_config);\n\n\t\tif (ret) {\n\t\t\tkfree(pmu_entry);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tbreak;\n\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}