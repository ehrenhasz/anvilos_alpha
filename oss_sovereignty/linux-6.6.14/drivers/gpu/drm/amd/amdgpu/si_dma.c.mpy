{
  "module_name": "si_dma.c",
  "hash_id": "8001452b04e34370bd0516106bd282a5af456a1a500ba2e3af977d1833169ae2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/si_dma.c",
  "human_readable_source": " \n\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"si.h\"\n#include \"sid.h\"\n\nconst u32 sdma_offsets[SDMA_MAX_INSTANCE] =\n{\n\tDMA0_REGISTER_OFFSET,\n\tDMA1_REGISTER_OFFSET\n};\n\nstatic void si_dma_set_ring_funcs(struct amdgpu_device *adev);\nstatic void si_dma_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void si_dma_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic void si_dma_set_irq_funcs(struct amdgpu_device *adev);\n\nstatic uint64_t si_dma_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\treturn *ring->rptr_cpu_addr;\n}\n\nstatic uint64_t si_dma_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 me = (ring == &adev->sdma.instance[0].ring) ? 0 : 1;\n\n\treturn (RREG32(DMA_RB_WPTR + sdma_offsets[me]) & 0x3fffc) >> 2;\n}\n\nstatic void si_dma_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 me = (ring == &adev->sdma.instance[0].ring) ? 0 : 1;\n\n\tWREG32(DMA_RB_WPTR + sdma_offsets[me], (ring->wptr << 2) & 0x3fffc);\n}\n\nstatic void si_dma_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\tstruct amdgpu_job *job,\n\t\t\t\tstruct amdgpu_ib *ib,\n\t\t\t\tuint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\t \n\twhile ((lower_32_bits(ring->wptr) & 7) != 5)\n\t\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0, 0));\n\tamdgpu_ring_write(ring, DMA_IB_PACKET(DMA_PACKET_INDIRECT_BUFFER, vmid, 0));\n\tamdgpu_ring_write(ring, (ib->gpu_addr & 0xFFFFFFE0));\n\tamdgpu_ring_write(ring, (ib->length_dw << 12) | (upper_32_bits(ib->gpu_addr) & 0xFF));\n\n}\n\n \nstatic void si_dma_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t      unsigned flags)\n{\n\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_FENCE, 0, 0, 0, 0));\n\tamdgpu_ring_write(ring, addr & 0xfffffffc);\n\tamdgpu_ring_write(ring, (upper_32_bits(addr) & 0xff));\n\tamdgpu_ring_write(ring, seq);\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_FENCE, 0, 0, 0, 0));\n\t\tamdgpu_ring_write(ring, addr & 0xfffffffc);\n\t\tamdgpu_ring_write(ring, (upper_32_bits(addr) & 0xff));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\t \n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_TRAP, 0, 0, 0, 0));\n}\n\nstatic void si_dma_stop(struct amdgpu_device *adev)\n{\n\tu32 rb_cntl;\n\tunsigned i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t \n\t\trb_cntl = RREG32(DMA_RB_CNTL + sdma_offsets[i]);\n\t\trb_cntl &= ~DMA_RB_ENABLE;\n\t\tWREG32(DMA_RB_CNTL + sdma_offsets[i], rb_cntl);\n\t}\n}\n\nstatic int si_dma_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tu32 rb_cntl, dma_cntl, ib_cntl, rb_bufsz;\n\tint i, r;\n\tuint64_t rptr_addr;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tWREG32(DMA_SEM_INCOMPLETE_TIMER_CNTL + sdma_offsets[i], 0);\n\t\tWREG32(DMA_SEM_WAIT_FAIL_TIMER_CNTL + sdma_offsets[i], 0);\n\n\t\t \n\t\trb_bufsz = order_base_2(ring->ring_size / 4);\n\t\trb_cntl = rb_bufsz << 1;\n#ifdef __BIG_ENDIAN\n\t\trb_cntl |= DMA_RB_SWAP_ENABLE | DMA_RPTR_WRITEBACK_SWAP_ENABLE;\n#endif\n\t\tWREG32(DMA_RB_CNTL + sdma_offsets[i], rb_cntl);\n\n\t\t \n\t\tWREG32(DMA_RB_RPTR + sdma_offsets[i], 0);\n\t\tWREG32(DMA_RB_WPTR + sdma_offsets[i], 0);\n\n\t\trptr_addr = ring->rptr_gpu_addr;\n\n\t\tWREG32(DMA_RB_RPTR_ADDR_LO + sdma_offsets[i], lower_32_bits(rptr_addr));\n\t\tWREG32(DMA_RB_RPTR_ADDR_HI + sdma_offsets[i], upper_32_bits(rptr_addr) & 0xFF);\n\n\t\trb_cntl |= DMA_RPTR_WRITEBACK_ENABLE;\n\n\t\tWREG32(DMA_RB_BASE + sdma_offsets[i], ring->gpu_addr >> 8);\n\n\t\t \n\t\tib_cntl = DMA_IB_ENABLE | CMD_VMID_FORCE;\n#ifdef __BIG_ENDIAN\n\t\tib_cntl |= DMA_IB_SWAP_ENABLE;\n#endif\n\t\tWREG32(DMA_IB_CNTL + sdma_offsets[i], ib_cntl);\n\n\t\tdma_cntl = RREG32(DMA_CNTL + sdma_offsets[i]);\n\t\tdma_cntl &= ~CTXEMPTY_INT_ENABLE;\n\t\tWREG32(DMA_CNTL + sdma_offsets[i], dma_cntl);\n\n\t\tring->wptr = 0;\n\t\tWREG32(DMA_RB_WPTR + sdma_offsets[i], ring->wptr << 2);\n\t\tWREG32(DMA_RB_CNTL + sdma_offsets[i], rb_cntl | DMA_RB_ENABLE);\n\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int si_dma_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\tr = amdgpu_ring_alloc(ring, 4);\n\tif (r)\n\t\tgoto error_free_wb;\n\n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 0, 1));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr) & 0xff);\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\nerror_free_wb:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic int si_dma_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\tlong r;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, 256,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err0;\n\n\tib.ptr[0] = DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 0, 1);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr) & 0xff;\n\tib.ptr[3] = 0xDEADBEEF;\n\tib.length_dw = 4;\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tgoto err1;\n\t}\n\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic void si_dma_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t       uint64_t pe, uint64_t src,\n\t\t\t       unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_COPY,\n\t\t\t\t\t      1, 0, 0, bytes);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe) & 0xff;\n\tib->ptr[ib->length_dw++] = upper_32_bits(src) & 0xff;\n}\n\n \nstatic void si_dma_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\tuint64_t value, unsigned count,\n\t\t\t\tuint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_WRITE, 0, 0, 0, ndw);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void si_dma_vm_set_pte_pde(struct amdgpu_ib *ib,\n\t\t\t\t     uint64_t pe,\n\t\t\t\t     uint64_t addr, unsigned count,\n\t\t\t\t     uint32_t incr, uint64_t flags)\n{\n\tuint64_t value;\n\tunsigned ndw;\n\n\twhile (count) {\n\t\tndw = count * 2;\n\t\tif (ndw > 0xFFFFE)\n\t\t\tndw = 0xFFFFE;\n\n\t\tif (flags & AMDGPU_PTE_VALID)\n\t\t\tvalue = addr;\n\t\telse\n\t\t\tvalue = 0;\n\n\t\t \n\t\tib->ptr[ib->length_dw++] = DMA_PTE_PDE_PACKET(ndw);\n\t\tib->ptr[ib->length_dw++] = pe;  \n\t\tib->ptr[ib->length_dw++] = upper_32_bits(pe) & 0xff;\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\t\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\t\tib->ptr[ib->length_dw++] = value;  \n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = incr;  \n\t\tib->ptr[ib->length_dw++] = 0;\n\t\tpe += ndw * 4;\n\t\taddr += (ndw / 2) * incr;\n\t\tcount -= ndw / 2;\n\t}\n}\n\n \nstatic void si_dma_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\twhile (ib->length_dw & 0x7)\n\t\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0, 0);\n}\n\n \nstatic void si_dma_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_POLL_REG_MEM, 0, 0, 0, 0) |\n\t\t\t  (1 << 27));  \n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, (0xff << 16) | upper_32_bits(addr));  \n\tamdgpu_ring_write(ring, 0xffffffff);  \n\tamdgpu_ring_write(ring, seq);  \n\tamdgpu_ring_write(ring, (3 << 28) | 0x20);  \n}\n\n \nstatic void si_dma_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t      unsigned vmid, uint64_t pd_addr)\n{\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t \n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_POLL_REG_MEM, 0, 0, 0, 0));\n\tamdgpu_ring_write(ring, VM_INVALIDATE_REQUEST);\n\tamdgpu_ring_write(ring, 0xff << 16);  \n\tamdgpu_ring_write(ring, 1 << vmid);  \n\tamdgpu_ring_write(ring, 0);  \n\tamdgpu_ring_write(ring, (0 << 28) | 0x20);  \n}\n\nstatic void si_dma_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t  uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, DMA_PACKET(DMA_PACKET_SRBM_WRITE, 0, 0, 0, 0));\n\tamdgpu_ring_write(ring, (0xf << 16) | reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic int si_dma_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->sdma.num_instances = 2;\n\n\tsi_dma_set_ring_funcs(adev);\n\tsi_dma_set_buffer_funcs(adev);\n\tsi_dma_set_vm_pte_funcs(adev);\n\tsi_dma_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int si_dma_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 224,\n\t\t\t      &adev->sdma.trap_irq);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 244,\n\t\t\t      &adev->sdma.trap_irq);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tring->use_doorbell = false;\n\t\tsprintf(ring->name, \"sdma%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 1024,\n\t\t\t\t     &adev->sdma.trap_irq,\n\t\t\t\t     (i == 0) ? AMDGPU_SDMA_IRQ_INSTANCE0 :\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE1,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn r;\n}\n\nstatic int si_dma_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\n\treturn 0;\n}\n\nstatic int si_dma_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_dma_start(adev);\n}\n\nstatic int si_dma_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsi_dma_stop(adev);\n\n\treturn 0;\n}\n\nstatic int si_dma_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_dma_hw_fini(adev);\n}\n\nstatic int si_dma_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn si_dma_hw_init(adev);\n}\n\nstatic bool si_dma_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(SRBM_STATUS2);\n\n\tif (tmp & (DMA_BUSY_MASK | DMA1_BUSY_MASK))\n\t    return false;\n\n\treturn true;\n}\n\nstatic int si_dma_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (si_dma_is_idle(handle))\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int si_dma_soft_reset(void *handle)\n{\n\tDRM_INFO(\"si_dma_soft_reset --- not implemented !!!!!!!\\n\");\n\treturn 0;\n}\n\nstatic int si_dma_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *src,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tswitch (type) {\n\tcase AMDGPU_SDMA_IRQ_INSTANCE0:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(DMA_CNTL + DMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl &= ~TRAP_ENABLE;\n\t\t\tWREG32(DMA_CNTL + DMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(DMA_CNTL + DMA0_REGISTER_OFFSET);\n\t\t\tsdma_cntl |= TRAP_ENABLE;\n\t\t\tWREG32(DMA_CNTL + DMA0_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase AMDGPU_SDMA_IRQ_INSTANCE1:\n\t\tswitch (state) {\n\t\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\t\tsdma_cntl = RREG32(DMA_CNTL + DMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl &= ~TRAP_ENABLE;\n\t\t\tWREG32(DMA_CNTL + DMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\t\tsdma_cntl = RREG32(DMA_CNTL + DMA1_REGISTER_OFFSET);\n\t\t\tsdma_cntl |= TRAP_ENABLE;\n\t\t\tWREG32(DMA_CNTL + DMA1_REGISTER_OFFSET, sdma_cntl);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int si_dma_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tif (entry->src_id == 224)\n\t\tamdgpu_fence_process(&adev->sdma.instance[0].ring);\n\telse\n\t\tamdgpu_fence_process(&adev->sdma.instance[1].ring);\n\treturn 0;\n}\n\nstatic int si_dma_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tu32 orig, data, offset;\n\tint i;\n\tbool enable;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tenable = (state == AMD_CG_STATE_GATE);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tif (i == 0)\n\t\t\t\toffset = DMA0_REGISTER_OFFSET;\n\t\t\telse\n\t\t\t\toffset = DMA1_REGISTER_OFFSET;\n\t\t\torig = data = RREG32(DMA_POWER_CNTL + offset);\n\t\t\tdata &= ~MEM_POWER_OVERRIDE;\n\t\t\tif (data != orig)\n\t\t\t\tWREG32(DMA_POWER_CNTL + offset, data);\n\t\t\tWREG32(DMA_CLK_CTRL + offset, 0x00000100);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tif (i == 0)\n\t\t\t\toffset = DMA0_REGISTER_OFFSET;\n\t\t\telse\n\t\t\t\toffset = DMA1_REGISTER_OFFSET;\n\t\t\torig = data = RREG32(DMA_POWER_CNTL + offset);\n\t\t\tdata |= MEM_POWER_OVERRIDE;\n\t\t\tif (data != orig)\n\t\t\t\tWREG32(DMA_POWER_CNTL + offset, data);\n\n\t\t\torig = data = RREG32(DMA_CLK_CTRL + offset);\n\t\t\tdata = 0xff000000;\n\t\t\tif (data != orig)\n\t\t\t\tWREG32(DMA_CLK_CTRL + offset, data);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int si_dma_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\tu32 tmp;\n\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tWREG32(DMA_PGFSM_WRITE,  0x00002000);\n\tWREG32(DMA_PGFSM_CONFIG, 0x100010ff);\n\n\tfor (tmp = 0; tmp < 5; tmp++)\n\t\tWREG32(DMA_PGFSM_WRITE, 0);\n\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs si_dma_ip_funcs = {\n\t.name = \"si_dma\",\n\t.early_init = si_dma_early_init,\n\t.late_init = NULL,\n\t.sw_init = si_dma_sw_init,\n\t.sw_fini = si_dma_sw_fini,\n\t.hw_init = si_dma_hw_init,\n\t.hw_fini = si_dma_hw_fini,\n\t.suspend = si_dma_suspend,\n\t.resume = si_dma_resume,\n\t.is_idle = si_dma_is_idle,\n\t.wait_for_idle = si_dma_wait_for_idle,\n\t.soft_reset = si_dma_soft_reset,\n\t.set_clockgating_state = si_dma_set_clockgating_state,\n\t.set_powergating_state = si_dma_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs si_dma_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xf,\n\t.nop = DMA_PACKET(DMA_PACKET_NOP, 0, 0, 0, 0),\n\t.support_64bit_ptrs = false,\n\t.get_rptr = si_dma_ring_get_rptr,\n\t.get_wptr = si_dma_ring_get_wptr,\n\t.set_wptr = si_dma_ring_set_wptr,\n\t.emit_frame_size =\n\t\t3 + 3 +  \n\t\t6 +  \n\t\tSI_FLUSH_GPU_TLB_NUM_WREG * 3 + 6 +  \n\t\t9 + 9 + 9,  \n\t.emit_ib_size = 7 + 3,  \n\t.emit_ib = si_dma_ring_emit_ib,\n\t.emit_fence = si_dma_ring_emit_fence,\n\t.emit_pipeline_sync = si_dma_ring_emit_pipeline_sync,\n\t.emit_vm_flush = si_dma_ring_emit_vm_flush,\n\t.test_ring = si_dma_ring_test_ring,\n\t.test_ib = si_dma_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = si_dma_ring_pad_ib,\n\t.emit_wreg = si_dma_ring_emit_wreg,\n};\n\nstatic void si_dma_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tadev->sdma.instance[i].ring.funcs = &si_dma_ring_funcs;\n}\n\nstatic const struct amdgpu_irq_src_funcs si_dma_trap_irq_funcs = {\n\t.set = si_dma_set_trap_irq_state,\n\t.process = si_dma_process_trap_irq,\n};\n\nstatic void si_dma_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = AMDGPU_SDMA_IRQ_LAST;\n\tadev->sdma.trap_irq.funcs = &si_dma_trap_irq_funcs;\n}\n\n \nstatic void si_dma_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint64_t src_offset,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count,\n\t\t\t\t       bool tmz)\n{\n\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_COPY,\n\t\t\t\t\t      1, 0, 0, byte_count);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset) & 0xff;\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset) & 0xff;\n}\n\n \nstatic void si_dma_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint32_t src_data,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = DMA_PACKET(DMA_PACKET_CONSTANT_FILL,\n\t\t\t\t\t      0, 0, 0, byte_count / 4);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset) << 16;\n}\n\n\nstatic const struct amdgpu_buffer_funcs si_dma_buffer_funcs = {\n\t.copy_max_bytes = 0xffff8,\n\t.copy_num_dw = 5,\n\t.emit_copy_buffer = si_dma_emit_copy_buffer,\n\n\t.fill_max_bytes = 0xffff8,\n\t.fill_num_dw = 4,\n\t.emit_fill_buffer = si_dma_emit_fill_buffer,\n};\n\nstatic void si_dma_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &si_dma_buffer_funcs;\n\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs si_dma_vm_pte_funcs = {\n\t.copy_pte_num_dw = 5,\n\t.copy_pte = si_dma_vm_copy_pte,\n\n\t.write_pte = si_dma_vm_write_pte,\n\t.set_pte_pde = si_dma_vm_set_pte_pde,\n};\n\nstatic void si_dma_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &si_dma_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->vm_manager.vm_pte_scheds[i] =\n\t\t\t&adev->sdma.instance[i].ring.sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nconst struct amdgpu_ip_block_version si_dma_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &si_dma_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}