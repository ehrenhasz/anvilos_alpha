{
  "module_name": "amdgpu_ib.c",
  "hash_id": "76e84485b31dc2d219c6d8ff420a297a2aae03328909518ec3f5a69eeebbb56e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ib.c",
  "human_readable_source": " \n#include <linux/seq_file.h>\n#include <linux/slab.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"atom.h\"\n#include \"amdgpu_trace.h\"\n\n#define AMDGPU_IB_TEST_TIMEOUT\tmsecs_to_jiffies(1000)\n#define AMDGPU_IB_TEST_GFX_XGMI_TIMEOUT\tmsecs_to_jiffies(2000)\n\n \n\n \nint amdgpu_ib_get(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t  unsigned int size, enum amdgpu_ib_pool_type pool_type,\n\t\t  struct amdgpu_ib *ib)\n{\n\tint r;\n\n\tif (size) {\n\t\tr = amdgpu_sa_bo_new(&adev->ib_pools[pool_type],\n\t\t\t\t     &ib->sa_bo, size);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"failed to get a new IB (%d)\\n\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tib->ptr = amdgpu_sa_bo_cpu_addr(ib->sa_bo);\n\t\t \n\t\tib->flags = AMDGPU_IB_FLAG_EMIT_MEM_SYNC;\n\n\t\tif (!vm)\n\t\t\tib->gpu_addr = amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\t}\n\n\treturn 0;\n}\n\n \nvoid amdgpu_ib_free(struct amdgpu_device *adev, struct amdgpu_ib *ib,\n\t\t    struct dma_fence *f)\n{\n\tamdgpu_sa_bo_free(adev, &ib->sa_bo, f);\n}\n\n \nint amdgpu_ib_schedule(struct amdgpu_ring *ring, unsigned int num_ibs,\n\t\t       struct amdgpu_ib *ibs, struct amdgpu_job *job,\n\t\t       struct dma_fence **f)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib *ib = &ibs[0];\n\tstruct dma_fence *tmp = NULL;\n\tbool need_ctx_switch;\n\tunsigned int patch_offset = ~0;\n\tstruct amdgpu_vm *vm;\n\tuint64_t fence_ctx;\n\tuint32_t status = 0, alloc_size;\n\tunsigned int fence_flags = 0;\n\tbool secure, init_shadow;\n\tu64 shadow_va, csa_va, gds_va;\n\tint vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tunsigned int i;\n\tint r = 0;\n\tbool need_pipe_sync = false;\n\n\tif (num_ibs == 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (job) {\n\t\tvm = job->vm;\n\t\tfence_ctx = job->base.s_fence ?\n\t\t\tjob->base.s_fence->scheduled.context : 0;\n\t\tshadow_va = job->shadow_va;\n\t\tcsa_va = job->csa_va;\n\t\tgds_va = job->gds_va;\n\t\tinit_shadow = job->init_shadow;\n\t} else {\n\t\tvm = NULL;\n\t\tfence_ctx = 0;\n\t\tshadow_va = 0;\n\t\tcsa_va = 0;\n\t\tgds_va = 0;\n\t\tinit_shadow = false;\n\t}\n\n\tif (!ring->sched.ready && !ring->is_mes_queue) {\n\t\tdev_err(adev->dev, \"couldn't schedule ib on ring <%s>\\n\", ring->name);\n\t\treturn -EINVAL;\n\t}\n\n\tif (vm && !job->vmid && !ring->is_mes_queue) {\n\t\tdev_err(adev->dev, \"VM IB without ID\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif ((ib->flags & AMDGPU_IB_FLAGS_SECURE) &&\n\t    (!ring->funcs->secure_submission_supported)) {\n\t\tdev_err(adev->dev, \"secure submissions not supported on ring <%s>\\n\", ring->name);\n\t\treturn -EINVAL;\n\t}\n\n\talloc_size = ring->funcs->emit_frame_size + num_ibs *\n\t\tring->funcs->emit_ib_size;\n\n\tr = amdgpu_ring_alloc(ring, alloc_size);\n\tif (r) {\n\t\tdev_err(adev->dev, \"scheduling IB failed (%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\tneed_ctx_switch = ring->current_ctx != fence_ctx;\n\tif (ring->funcs->emit_pipeline_sync && job &&\n\t    ((tmp = amdgpu_sync_get_fence(&job->explicit_sync)) ||\n\t     (amdgpu_sriov_vf(adev) && need_ctx_switch) ||\n\t     amdgpu_vm_need_pipeline_sync(ring, job))) {\n\t\tneed_pipe_sync = true;\n\n\t\tif (tmp)\n\t\t\ttrace_amdgpu_ib_pipe_sync(job, tmp);\n\n\t\tdma_fence_put(tmp);\n\t}\n\n\tif ((ib->flags & AMDGPU_IB_FLAG_EMIT_MEM_SYNC) && ring->funcs->emit_mem_sync)\n\t\tring->funcs->emit_mem_sync(ring);\n\n\tif (ring->funcs->emit_wave_limit &&\n\t    ring->hw_prio == AMDGPU_GFX_PIPE_PRIO_HIGH)\n\t\tring->funcs->emit_wave_limit(ring, true);\n\n\tif (ring->funcs->insert_start)\n\t\tring->funcs->insert_start(ring);\n\n\tif (job) {\n\t\tr = amdgpu_vm_flush(ring, job, need_pipe_sync);\n\t\tif (r) {\n\t\t\tamdgpu_ring_undo(ring);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tamdgpu_ring_ib_begin(ring);\n\n\tif (ring->funcs->emit_gfx_shadow)\n\t\tamdgpu_ring_emit_gfx_shadow(ring, shadow_va, csa_va, gds_va,\n\t\t\t\t\t    init_shadow, vmid);\n\n\tif (ring->funcs->init_cond_exec)\n\t\tpatch_offset = amdgpu_ring_init_cond_exec(ring);\n\n\tamdgpu_device_flush_hdp(adev, ring);\n\n\tif (need_ctx_switch)\n\t\tstatus |= AMDGPU_HAVE_CTX_SWITCH;\n\n\tif (job && ring->funcs->emit_cntxcntl) {\n\t\tstatus |= job->preamble_status;\n\t\tstatus |= job->preemption_status;\n\t\tamdgpu_ring_emit_cntxcntl(ring, status);\n\t}\n\n\t \n\tsecure = false;\n\tif (job && ring->funcs->emit_frame_cntl) {\n\t\tsecure = ib->flags & AMDGPU_IB_FLAGS_SECURE;\n\t\tamdgpu_ring_emit_frame_cntl(ring, true, secure);\n\t}\n\n\tfor (i = 0; i < num_ibs; ++i) {\n\t\tib = &ibs[i];\n\n\t\tif (job && ring->funcs->emit_frame_cntl) {\n\t\t\tif (secure != !!(ib->flags & AMDGPU_IB_FLAGS_SECURE)) {\n\t\t\t\tamdgpu_ring_emit_frame_cntl(ring, false, secure);\n\t\t\t\tsecure = !secure;\n\t\t\t\tamdgpu_ring_emit_frame_cntl(ring, true, secure);\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_ring_emit_ib(ring, job, ib, status);\n\t\tstatus &= ~AMDGPU_HAVE_CTX_SWITCH;\n\t}\n\n\tif (job && ring->funcs->emit_frame_cntl)\n\t\tamdgpu_ring_emit_frame_cntl(ring, false, secure);\n\n\tamdgpu_device_invalidate_hdp(adev, ring);\n\n\tif (ib->flags & AMDGPU_IB_FLAG_TC_WB_NOT_INVALIDATE)\n\t\tfence_flags |= AMDGPU_FENCE_FLAG_TC_WB_ONLY;\n\n\t \n\tif (job && job->uf_addr) {\n\t\tamdgpu_ring_emit_fence(ring, job->uf_addr, job->uf_sequence,\n\t\t\t\t       fence_flags | AMDGPU_FENCE_FLAG_64BIT);\n\t}\n\n\tif (ring->funcs->emit_gfx_shadow) {\n\t\tamdgpu_ring_emit_gfx_shadow(ring, 0, 0, 0, false, 0);\n\n\t\tif (ring->funcs->init_cond_exec) {\n\t\t\tunsigned int ce_offset = ~0;\n\n\t\t\tce_offset = amdgpu_ring_init_cond_exec(ring);\n\t\t\tif (ce_offset != ~0 && ring->funcs->patch_cond_exec)\n\t\t\t\tamdgpu_ring_patch_cond_exec(ring, ce_offset);\n\t\t}\n\t}\n\n\tr = amdgpu_fence_emit(ring, f, job, fence_flags);\n\tif (r) {\n\t\tdev_err(adev->dev, \"failed to emit fence (%d)\\n\", r);\n\t\tif (job && job->vmid)\n\t\t\tamdgpu_vmid_reset(adev, ring->vm_hub, job->vmid);\n\t\tamdgpu_ring_undo(ring);\n\t\treturn r;\n\t}\n\n\tif (ring->funcs->insert_end)\n\t\tring->funcs->insert_end(ring);\n\n\tif (patch_offset != ~0 && ring->funcs->patch_cond_exec)\n\t\tamdgpu_ring_patch_cond_exec(ring, patch_offset);\n\n\tring->current_ctx = fence_ctx;\n\tif (vm && ring->funcs->emit_switch_buffer)\n\t\tamdgpu_ring_emit_switch_buffer(ring);\n\n\tif (ring->funcs->emit_wave_limit &&\n\t    ring->hw_prio == AMDGPU_GFX_PIPE_PRIO_HIGH)\n\t\tring->funcs->emit_wave_limit(ring, false);\n\n\tamdgpu_ring_ib_end(ring);\n\tamdgpu_ring_commit(ring);\n\treturn 0;\n}\n\n \nint amdgpu_ib_pool_init(struct amdgpu_device *adev)\n{\n\tint r, i;\n\n\tif (adev->ib_pool_ready)\n\t\treturn 0;\n\n\tfor (i = 0; i < AMDGPU_IB_POOL_MAX; i++) {\n\t\tr = amdgpu_sa_bo_manager_init(adev, &adev->ib_pools[i],\n\t\t\t\t\t      AMDGPU_IB_POOL_SIZE, 256,\n\t\t\t\t\t      AMDGPU_GEM_DOMAIN_GTT);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\tadev->ib_pool_ready = true;\n\n\treturn 0;\n\nerror:\n\twhile (i--)\n\t\tamdgpu_sa_bo_manager_fini(adev, &adev->ib_pools[i]);\n\treturn r;\n}\n\n \nvoid amdgpu_ib_pool_fini(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (!adev->ib_pool_ready)\n\t\treturn;\n\n\tfor (i = 0; i < AMDGPU_IB_POOL_MAX; i++)\n\t\tamdgpu_sa_bo_manager_fini(adev, &adev->ib_pools[i]);\n\tadev->ib_pool_ready = false;\n}\n\n \nint amdgpu_ib_ring_tests(struct amdgpu_device *adev)\n{\n\tlong tmo_gfx, tmo_mm;\n\tint r, ret = 0;\n\tunsigned int i;\n\n\ttmo_mm = tmo_gfx = AMDGPU_IB_TEST_TIMEOUT;\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\ttmo_mm = 8 * AMDGPU_IB_TEST_TIMEOUT;\n\t}\n\n\tif (amdgpu_sriov_runtime(adev)) {\n\t\t \n\t\ttmo_gfx = 8 * AMDGPU_IB_TEST_TIMEOUT;\n\t} else if (adev->gmc.xgmi.hive_id) {\n\t\ttmo_gfx = AMDGPU_IB_TEST_GFX_XGMI_TIMEOUT;\n\t}\n\n\tfor (i = 0; i < adev->num_rings; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\t\tlong tmo;\n\n\t\t \n\t\tif (!ring->sched.ready || !ring->funcs->test_ib)\n\t\t\tcontinue;\n\n\t\tif (adev->enable_mes &&\n\t\t    ring->funcs->type == AMDGPU_RING_TYPE_KIQ)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (ring->funcs->type == AMDGPU_RING_TYPE_UVD ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_VCE ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_UVD_ENC ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_VCN_DEC ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_VCN_ENC ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_VCN_JPEG)\n\t\t\ttmo = tmo_mm;\n\t\telse\n\t\t\ttmo = tmo_gfx;\n\n\t\tr = amdgpu_ring_test_ib(ring, tmo);\n\t\tif (!r) {\n\t\t\tDRM_DEV_DEBUG(adev->dev, \"ib test on %s succeeded\\n\",\n\t\t\t\t      ring->name);\n\t\t\tcontinue;\n\t\t}\n\n\t\tring->sched.ready = false;\n\t\tDRM_DEV_ERROR(adev->dev, \"IB test failed on %s (%d).\\n\",\n\t\t\t  ring->name, r);\n\n\t\tif (ring == &adev->gfx.gfx_ring[0]) {\n\t\t\t \n\t\t\tadev->accel_working = false;\n\t\t\treturn r;\n\n\t\t} else {\n\t\t\tret = r;\n\t\t}\n\t}\n\treturn ret;\n}\n\n \n#if defined(CONFIG_DEBUG_FS)\n\nstatic int amdgpu_debugfs_sa_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\n\tseq_puts(m, \"--------------------- DELAYED ---------------------\\n\");\n\tamdgpu_sa_bo_dump_debug_info(&adev->ib_pools[AMDGPU_IB_POOL_DELAYED],\n\t\t\t\t     m);\n\tseq_puts(m, \"-------------------- IMMEDIATE --------------------\\n\");\n\tamdgpu_sa_bo_dump_debug_info(&adev->ib_pools[AMDGPU_IB_POOL_IMMEDIATE],\n\t\t\t\t     m);\n\tseq_puts(m, \"--------------------- DIRECT ----------------------\\n\");\n\tamdgpu_sa_bo_dump_debug_info(&adev->ib_pools[AMDGPU_IB_POOL_DIRECT], m);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_sa_info);\n\n#endif\n\nvoid amdgpu_debugfs_sa_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_sa_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_sa_info_fops);\n\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}