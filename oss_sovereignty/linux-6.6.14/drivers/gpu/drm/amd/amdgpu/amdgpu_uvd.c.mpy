{
  "module_name": "amdgpu_uvd.c",
  "hash_id": "1a2ffa25339fa1e5982442dff21818a8452405d25d897d865ea8c080cd904c3e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_uvd.c",
  "human_readable_source": " \n \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n\n#include <drm/drm.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_cs.h\"\n#include \"cikd.h\"\n#include \"uvd/uvd_4_2_d.h\"\n\n#include \"amdgpu_ras.h\"\n\n \n#define UVD_IDLE_TIMEOUT\tmsecs_to_jiffies(1000)\n\n \n#define FW_1_65_10\t((1 << 24) | (65 << 16) | (10 << 8))\n#define FW_1_87_11\t((1 << 24) | (87 << 16) | (11 << 8))\n#define FW_1_87_12\t((1 << 24) | (87 << 16) | (12 << 8))\n#define FW_1_37_15\t((1 << 24) | (37 << 16) | (15 << 8))\n\n \n#define FW_1_66_16\t((1 << 24) | (66 << 16) | (16 << 8))\n\n \n#ifdef CONFIG_DRM_AMDGPU_SI\n#define FIRMWARE_TAHITI\t\t\"amdgpu/tahiti_uvd.bin\"\n#define FIRMWARE_VERDE\t\t\"amdgpu/verde_uvd.bin\"\n#define FIRMWARE_PITCAIRN\t\"amdgpu/pitcairn_uvd.bin\"\n#define FIRMWARE_OLAND\t\t\"amdgpu/oland_uvd.bin\"\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n#define FIRMWARE_BONAIRE\t\"amdgpu/bonaire_uvd.bin\"\n#define FIRMWARE_KABINI\t\"amdgpu/kabini_uvd.bin\"\n#define FIRMWARE_KAVERI\t\"amdgpu/kaveri_uvd.bin\"\n#define FIRMWARE_HAWAII\t\"amdgpu/hawaii_uvd.bin\"\n#define FIRMWARE_MULLINS\t\"amdgpu/mullins_uvd.bin\"\n#endif\n#define FIRMWARE_TONGA\t\t\"amdgpu/tonga_uvd.bin\"\n#define FIRMWARE_CARRIZO\t\"amdgpu/carrizo_uvd.bin\"\n#define FIRMWARE_FIJI\t\t\"amdgpu/fiji_uvd.bin\"\n#define FIRMWARE_STONEY\t\t\"amdgpu/stoney_uvd.bin\"\n#define FIRMWARE_POLARIS10\t\"amdgpu/polaris10_uvd.bin\"\n#define FIRMWARE_POLARIS11\t\"amdgpu/polaris11_uvd.bin\"\n#define FIRMWARE_POLARIS12\t\"amdgpu/polaris12_uvd.bin\"\n#define FIRMWARE_VEGAM\t\t\"amdgpu/vegam_uvd.bin\"\n\n#define FIRMWARE_VEGA10\t\t\"amdgpu/vega10_uvd.bin\"\n#define FIRMWARE_VEGA12\t\t\"amdgpu/vega12_uvd.bin\"\n#define FIRMWARE_VEGA20\t\t\"amdgpu/vega20_uvd.bin\"\n\n \n#define UVD_GPCOM_VCPU_CMD\t\t0x03c3\n#define UVD_GPCOM_VCPU_DATA0\t0x03c4\n#define UVD_GPCOM_VCPU_DATA1\t0x03c5\n#define UVD_NO_OP\t\t\t\t0x03ff\n#define UVD_BASE_SI\t\t\t\t0x3800\n\n \nstruct amdgpu_uvd_cs_ctx {\n\tstruct amdgpu_cs_parser *parser;\n\tunsigned int reg, count;\n\tunsigned int data0, data1;\n\tunsigned int idx;\n\tstruct amdgpu_ib *ib;\n\n\t \n\tbool has_msg_cmd;\n\n\t \n\tunsigned int *buf_sizes;\n};\n\n#ifdef CONFIG_DRM_AMDGPU_SI\nMODULE_FIRMWARE(FIRMWARE_TAHITI);\nMODULE_FIRMWARE(FIRMWARE_VERDE);\nMODULE_FIRMWARE(FIRMWARE_PITCAIRN);\nMODULE_FIRMWARE(FIRMWARE_OLAND);\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\nMODULE_FIRMWARE(FIRMWARE_BONAIRE);\nMODULE_FIRMWARE(FIRMWARE_KABINI);\nMODULE_FIRMWARE(FIRMWARE_KAVERI);\nMODULE_FIRMWARE(FIRMWARE_HAWAII);\nMODULE_FIRMWARE(FIRMWARE_MULLINS);\n#endif\nMODULE_FIRMWARE(FIRMWARE_TONGA);\nMODULE_FIRMWARE(FIRMWARE_CARRIZO);\nMODULE_FIRMWARE(FIRMWARE_FIJI);\nMODULE_FIRMWARE(FIRMWARE_STONEY);\nMODULE_FIRMWARE(FIRMWARE_POLARIS10);\nMODULE_FIRMWARE(FIRMWARE_POLARIS11);\nMODULE_FIRMWARE(FIRMWARE_POLARIS12);\nMODULE_FIRMWARE(FIRMWARE_VEGAM);\n\nMODULE_FIRMWARE(FIRMWARE_VEGA10);\nMODULE_FIRMWARE(FIRMWARE_VEGA12);\nMODULE_FIRMWARE(FIRMWARE_VEGA20);\n\nstatic void amdgpu_uvd_idle_work_handler(struct work_struct *work);\nstatic void amdgpu_uvd_force_into_uvd_segment(struct amdgpu_bo *abo);\n\nstatic int amdgpu_uvd_create_msg_bo_helper(struct amdgpu_device *adev,\n\t\t\t\t\t   uint32_t size,\n\t\t\t\t\t   struct amdgpu_bo **bo_ptr)\n{\n\tstruct ttm_operation_ctx ctx = { true, false };\n\tstruct amdgpu_bo *bo = NULL;\n\tvoid *addr;\n\tint r;\n\n\tr = amdgpu_bo_create_reserved(adev, size, PAGE_SIZE,\n\t\t\t\t      AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t      &bo, NULL, &addr);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->uvd.address_64_bit)\n\t\tgoto succ;\n\n\tamdgpu_bo_kunmap(bo);\n\tamdgpu_bo_unpin(bo);\n\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_VRAM);\n\tamdgpu_uvd_force_into_uvd_segment(bo);\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (r)\n\t\tgoto err;\n\tr = amdgpu_bo_pin(bo, AMDGPU_GEM_DOMAIN_VRAM);\n\tif (r)\n\t\tgoto err_pin;\n\tr = amdgpu_bo_kmap(bo, &addr);\n\tif (r)\n\t\tgoto err_kmap;\nsucc:\n\tamdgpu_bo_unreserve(bo);\n\t*bo_ptr = bo;\n\treturn 0;\nerr_kmap:\n\tamdgpu_bo_unpin(bo);\nerr_pin:\nerr:\n\tamdgpu_bo_unreserve(bo);\n\tamdgpu_bo_unref(&bo);\n\treturn r;\n}\n\nint amdgpu_uvd_sw_init(struct amdgpu_device *adev)\n{\n\tunsigned long bo_size;\n\tconst char *fw_name;\n\tconst struct common_firmware_header *hdr;\n\tunsigned int family_id;\n\tint i, j, r;\n\n\tINIT_DELAYED_WORK(&adev->uvd.idle_work, amdgpu_uvd_idle_work_handler);\n\n\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_SI\n\tcase CHIP_TAHITI:\n\t\tfw_name = FIRMWARE_TAHITI;\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tfw_name = FIRMWARE_VERDE;\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tfw_name = FIRMWARE_PITCAIRN;\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tfw_name = FIRMWARE_OLAND;\n\t\tbreak;\n#endif\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_BONAIRE:\n\t\tfw_name = FIRMWARE_BONAIRE;\n\t\tbreak;\n\tcase CHIP_KABINI:\n\t\tfw_name = FIRMWARE_KABINI;\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\t\tfw_name = FIRMWARE_KAVERI;\n\t\tbreak;\n\tcase CHIP_HAWAII:\n\t\tfw_name = FIRMWARE_HAWAII;\n\t\tbreak;\n\tcase CHIP_MULLINS:\n\t\tfw_name = FIRMWARE_MULLINS;\n\t\tbreak;\n#endif\n\tcase CHIP_TONGA:\n\t\tfw_name = FIRMWARE_TONGA;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tfw_name = FIRMWARE_FIJI;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tfw_name = FIRMWARE_CARRIZO;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tfw_name = FIRMWARE_STONEY;\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\t\tfw_name = FIRMWARE_POLARIS10;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\t\tfw_name = FIRMWARE_POLARIS11;\n\t\tbreak;\n\tcase CHIP_POLARIS12:\n\t\tfw_name = FIRMWARE_POLARIS12;\n\t\tbreak;\n\tcase CHIP_VEGA10:\n\t\tfw_name = FIRMWARE_VEGA10;\n\t\tbreak;\n\tcase CHIP_VEGA12:\n\t\tfw_name = FIRMWARE_VEGA12;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tfw_name = FIRMWARE_VEGAM;\n\t\tbreak;\n\tcase CHIP_VEGA20:\n\t\tfw_name = FIRMWARE_VEGA20;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tr = amdgpu_ucode_request(adev, &adev->uvd.fw, fw_name);\n\tif (r) {\n\t\tdev_err(adev->dev, \"amdgpu_uvd: Can't validate firmware \\\"%s\\\"\\n\",\n\t\t\tfw_name);\n\t\tamdgpu_ucode_release(&adev->uvd.fw);\n\t\treturn r;\n\t}\n\n\t \n\tadev->uvd.max_handles = AMDGPU_DEFAULT_UVD_HANDLES;\n\n\thdr = (const struct common_firmware_header *)adev->uvd.fw->data;\n\tfamily_id = le32_to_cpu(hdr->ucode_version) & 0xff;\n\n\tif (adev->asic_type < CHIP_VEGA20) {\n\t\tunsigned int version_major, version_minor;\n\n\t\tversion_major = (le32_to_cpu(hdr->ucode_version) >> 24) & 0xff;\n\t\tversion_minor = (le32_to_cpu(hdr->ucode_version) >> 8) & 0xff;\n\t\tDRM_INFO(\"Found UVD firmware Version: %u.%u Family ID: %u\\n\",\n\t\t\tversion_major, version_minor, family_id);\n\n\t\t \n\t\tif ((version_major > 0x01) ||\n\t\t    ((version_major == 0x01) && (version_minor >= 0x50)))\n\t\t\tadev->uvd.max_handles = AMDGPU_MAX_UVD_HANDLES;\n\n\t\tadev->uvd.fw_version = ((version_major << 24) | (version_minor << 16) |\n\t\t\t\t\t(family_id << 8));\n\n\t\tif ((adev->asic_type == CHIP_POLARIS10 ||\n\t\t     adev->asic_type == CHIP_POLARIS11) &&\n\t\t    (adev->uvd.fw_version < FW_1_66_16))\n\t\t\tDRM_ERROR(\"POLARIS10/11 UVD firmware version %u.%u is too old.\\n\",\n\t\t\t\t  version_major, version_minor);\n\t} else {\n\t\tunsigned int enc_major, enc_minor, dec_minor;\n\n\t\tdec_minor = (le32_to_cpu(hdr->ucode_version) >> 8) & 0xff;\n\t\tenc_minor = (le32_to_cpu(hdr->ucode_version) >> 24) & 0x3f;\n\t\tenc_major = (le32_to_cpu(hdr->ucode_version) >> 30) & 0x3;\n\t\tDRM_INFO(\"Found UVD firmware ENC: %u.%u DEC: .%u Family ID: %u\\n\",\n\t\t\tenc_major, enc_minor, dec_minor, family_id);\n\n\t\tadev->uvd.max_handles = AMDGPU_MAX_UVD_HANDLES;\n\n\t\tadev->uvd.fw_version = le32_to_cpu(hdr->ucode_version);\n\t}\n\n\tbo_size = AMDGPU_UVD_STACK_SIZE + AMDGPU_UVD_HEAP_SIZE\n\t\t  +  AMDGPU_UVD_SESSION_SIZE * adev->uvd.max_handles;\n\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP)\n\t\tbo_size += AMDGPU_GPU_PAGE_ALIGN(le32_to_cpu(hdr->ucode_size_bytes) + 8);\n\n\tfor (j = 0; j < adev->uvd.num_uvd_inst; j++) {\n\t\tif (adev->uvd.harvest_config & (1 << j))\n\t\t\tcontinue;\n\t\tr = amdgpu_bo_create_kernel(adev, bo_size, PAGE_SIZE,\n\t\t\t\t\t    AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t\t    &adev->uvd.inst[j].vcpu_bo,\n\t\t\t\t\t    &adev->uvd.inst[j].gpu_addr,\n\t\t\t\t\t    &adev->uvd.inst[j].cpu_addr);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%d) failed to allocate UVD bo\\n\", r);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tfor (i = 0; i < adev->uvd.max_handles; ++i) {\n\t\tatomic_set(&adev->uvd.handles[i], 0);\n\t\tadev->uvd.filp[i] = NULL;\n\t}\n\n\t \n\tif (!amdgpu_device_ip_block_version_cmp(adev, AMD_IP_BLOCK_TYPE_UVD, 5, 0))\n\t\tadev->uvd.address_64_bit = true;\n\n\tr = amdgpu_uvd_create_msg_bo_helper(adev, 128 << 10, &adev->uvd.ib_bo);\n\tif (r)\n\t\treturn r;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TONGA:\n\t\tadev->uvd.use_ctx_buf = adev->uvd.fw_version >= FW_1_65_10;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tadev->uvd.use_ctx_buf = adev->uvd.fw_version >= FW_1_87_11;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tadev->uvd.use_ctx_buf = adev->uvd.fw_version >= FW_1_87_12;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tadev->uvd.use_ctx_buf = adev->uvd.fw_version >= FW_1_37_15;\n\t\tbreak;\n\tdefault:\n\t\tadev->uvd.use_ctx_buf = adev->asic_type >= CHIP_POLARIS10;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_uvd_sw_fini(struct amdgpu_device *adev)\n{\n\tvoid *addr = amdgpu_bo_kptr(adev->uvd.ib_bo);\n\tint i, j;\n\n\tdrm_sched_entity_destroy(&adev->uvd.entity);\n\n\tfor (j = 0; j < adev->uvd.num_uvd_inst; ++j) {\n\t\tif (adev->uvd.harvest_config & (1 << j))\n\t\t\tcontinue;\n\t\tkvfree(adev->uvd.inst[j].saved_bo);\n\n\t\tamdgpu_bo_free_kernel(&adev->uvd.inst[j].vcpu_bo,\n\t\t\t\t      &adev->uvd.inst[j].gpu_addr,\n\t\t\t\t      (void **)&adev->uvd.inst[j].cpu_addr);\n\n\t\tamdgpu_ring_fini(&adev->uvd.inst[j].ring);\n\n\t\tfor (i = 0; i < AMDGPU_MAX_UVD_ENC_RINGS; ++i)\n\t\t\tamdgpu_ring_fini(&adev->uvd.inst[j].ring_enc[i]);\n\t}\n\tamdgpu_bo_free_kernel(&adev->uvd.ib_bo, NULL, &addr);\n\tamdgpu_ucode_release(&adev->uvd.fw);\n\n\treturn 0;\n}\n\n \nint amdgpu_uvd_entity_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tstruct drm_gpu_scheduler *sched;\n\tint r;\n\n\tring = &adev->uvd.inst[0].ring;\n\tsched = &ring->sched;\n\tr = drm_sched_entity_init(&adev->uvd.entity, DRM_SCHED_PRIORITY_NORMAL,\n\t\t\t\t  &sched, 1, NULL);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed setting up UVD kernel entity.\\n\");\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_uvd_suspend(struct amdgpu_device *adev)\n{\n\tunsigned int size;\n\tvoid *ptr;\n\tint i, j, idx;\n\tbool in_ras_intr = amdgpu_ras_intr_triggered();\n\n\tcancel_delayed_work_sync(&adev->uvd.idle_work);\n\n\t \n\tif (adev->asic_type < CHIP_POLARIS10) {\n\t\tfor (i = 0; i < adev->uvd.max_handles; ++i)\n\t\t\tif (atomic_read(&adev->uvd.handles[i]))\n\t\t\t\tbreak;\n\n\t\tif (i == adev->uvd.max_handles)\n\t\t\treturn 0;\n\t}\n\n\tfor (j = 0; j < adev->uvd.num_uvd_inst; ++j) {\n\t\tif (adev->uvd.harvest_config & (1 << j))\n\t\t\tcontinue;\n\t\tif (adev->uvd.inst[j].vcpu_bo == NULL)\n\t\t\tcontinue;\n\n\t\tsize = amdgpu_bo_size(adev->uvd.inst[j].vcpu_bo);\n\t\tptr = adev->uvd.inst[j].cpu_addr;\n\n\t\tadev->uvd.inst[j].saved_bo = kvmalloc(size, GFP_KERNEL);\n\t\tif (!adev->uvd.inst[j].saved_bo)\n\t\t\treturn -ENOMEM;\n\n\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\t \n\t\t\tif (in_ras_intr)\n\t\t\t\tmemset(adev->uvd.inst[j].saved_bo, 0, size);\n\t\t\telse\n\t\t\t\tmemcpy_fromio(adev->uvd.inst[j].saved_bo, ptr, size);\n\n\t\t\tdrm_dev_exit(idx);\n\t\t}\n\t}\n\n\tif (in_ras_intr)\n\t\tDRM_WARN(\"UVD VCPU state may lost due to RAS ERREVENT_ATHUB_INTERRUPT\\n\");\n\n\treturn 0;\n}\n\nint amdgpu_uvd_resume(struct amdgpu_device *adev)\n{\n\tunsigned int size;\n\tvoid *ptr;\n\tint i, idx;\n\n\tfor (i = 0; i < adev->uvd.num_uvd_inst; i++) {\n\t\tif (adev->uvd.harvest_config & (1 << i))\n\t\t\tcontinue;\n\t\tif (adev->uvd.inst[i].vcpu_bo == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tsize = amdgpu_bo_size(adev->uvd.inst[i].vcpu_bo);\n\t\tptr = adev->uvd.inst[i].cpu_addr;\n\n\t\tif (adev->uvd.inst[i].saved_bo != NULL) {\n\t\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\t\tmemcpy_toio(ptr, adev->uvd.inst[i].saved_bo, size);\n\t\t\t\tdrm_dev_exit(idx);\n\t\t\t}\n\t\t\tkvfree(adev->uvd.inst[i].saved_bo);\n\t\t\tadev->uvd.inst[i].saved_bo = NULL;\n\t\t} else {\n\t\t\tconst struct common_firmware_header *hdr;\n\t\t\tunsigned int offset;\n\n\t\t\thdr = (const struct common_firmware_header *)adev->uvd.fw->data;\n\t\t\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\t\t\toffset = le32_to_cpu(hdr->ucode_array_offset_bytes);\n\t\t\t\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\t\t\t\tmemcpy_toio(adev->uvd.inst[i].cpu_addr, adev->uvd.fw->data + offset,\n\t\t\t\t\t\t    le32_to_cpu(hdr->ucode_size_bytes));\n\t\t\t\t\tdrm_dev_exit(idx);\n\t\t\t\t}\n\t\t\t\tsize -= le32_to_cpu(hdr->ucode_size_bytes);\n\t\t\t\tptr += le32_to_cpu(hdr->ucode_size_bytes);\n\t\t\t}\n\t\t\tmemset_io(ptr, 0, size);\n\t\t\t \n\t\t\tamdgpu_fence_driver_force_completion(&adev->uvd.inst[i].ring);\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid amdgpu_uvd_free_handles(struct amdgpu_device *adev, struct drm_file *filp)\n{\n\tstruct amdgpu_ring *ring = &adev->uvd.inst[0].ring;\n\tint i, r;\n\n\tfor (i = 0; i < adev->uvd.max_handles; ++i) {\n\t\tuint32_t handle = atomic_read(&adev->uvd.handles[i]);\n\n\t\tif (handle != 0 && adev->uvd.filp[i] == filp) {\n\t\t\tstruct dma_fence *fence;\n\n\t\t\tr = amdgpu_uvd_get_destroy_msg(ring, handle, false,\n\t\t\t\t\t\t       &fence);\n\t\t\tif (r) {\n\t\t\t\tDRM_ERROR(\"Error destroying UVD %d!\\n\", r);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdma_fence_wait(fence, false);\n\t\t\tdma_fence_put(fence);\n\n\t\t\tadev->uvd.filp[i] = NULL;\n\t\t\tatomic_set(&adev->uvd.handles[i], 0);\n\t\t}\n\t}\n}\n\nstatic void amdgpu_uvd_force_into_uvd_segment(struct amdgpu_bo *abo)\n{\n\tint i;\n\n\tfor (i = 0; i < abo->placement.num_placement; ++i) {\n\t\tabo->placements[i].fpfn = 0 >> PAGE_SHIFT;\n\t\tabo->placements[i].lpfn = (256 * 1024 * 1024) >> PAGE_SHIFT;\n\t}\n}\n\nstatic u64 amdgpu_uvd_get_addr_from_ctx(struct amdgpu_uvd_cs_ctx *ctx)\n{\n\tuint32_t lo, hi;\n\tuint64_t addr;\n\n\tlo = amdgpu_ib_get_value(ctx->ib, ctx->data0);\n\thi = amdgpu_ib_get_value(ctx->ib, ctx->data1);\n\taddr = ((uint64_t)lo) | (((uint64_t)hi) << 32);\n\n\treturn addr;\n}\n\n \nstatic int amdgpu_uvd_cs_pass1(struct amdgpu_uvd_cs_ctx *ctx)\n{\n\tstruct ttm_operation_ctx tctx = { false, false };\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct amdgpu_bo *bo;\n\tuint32_t cmd;\n\tuint64_t addr = amdgpu_uvd_get_addr_from_ctx(ctx);\n\tint r = 0;\n\n\tr = amdgpu_cs_find_mapping(ctx->parser, addr, &bo, &mapping);\n\tif (r) {\n\t\tDRM_ERROR(\"Can't find BO for addr 0x%08llx\\n\", addr);\n\t\treturn r;\n\t}\n\n\tif (!ctx->parser->adev->uvd.address_64_bit) {\n\t\t \n\t\tcmd = amdgpu_ib_get_value(ctx->ib, ctx->idx) >> 1;\n\t\tif (cmd == 0x0 || cmd == 0x3) {\n\t\t\t \n\t\t\tuint32_t domain = AMDGPU_GEM_DOMAIN_VRAM;\n\n\t\t\tamdgpu_bo_placement_from_domain(bo, domain);\n\t\t}\n\t\tamdgpu_uvd_force_into_uvd_segment(bo);\n\n\t\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &tctx);\n\t}\n\n\treturn r;\n}\n\n \nstatic int amdgpu_uvd_cs_msg_decode(struct amdgpu_device *adev, uint32_t *msg,\n\tunsigned int buf_sizes[])\n{\n\tunsigned int stream_type = msg[4];\n\tunsigned int width = msg[6];\n\tunsigned int height = msg[7];\n\tunsigned int dpb_size = msg[9];\n\tunsigned int pitch = msg[28];\n\tunsigned int level = msg[57];\n\n\tunsigned int width_in_mb = width / 16;\n\tunsigned int height_in_mb = ALIGN(height / 16, 2);\n\tunsigned int fs_in_mb = width_in_mb * height_in_mb;\n\n\tunsigned int image_size, tmp, min_dpb_size, num_dpb_buffer;\n\tunsigned int min_ctx_size = ~0;\n\n\timage_size = width * height;\n\timage_size += image_size / 2;\n\timage_size = ALIGN(image_size, 1024);\n\n\tswitch (stream_type) {\n\tcase 0:  \n\t\tswitch (level) {\n\t\tcase 30:\n\t\t\tnum_dpb_buffer = 8100 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 31:\n\t\t\tnum_dpb_buffer = 18000 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tnum_dpb_buffer = 20480 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 41:\n\t\t\tnum_dpb_buffer = 32768 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 42:\n\t\t\tnum_dpb_buffer = 34816 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 50:\n\t\t\tnum_dpb_buffer = 110400 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 51:\n\t\t\tnum_dpb_buffer = 184320 / fs_in_mb;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnum_dpb_buffer = 184320 / fs_in_mb;\n\t\t\tbreak;\n\t\t}\n\t\tnum_dpb_buffer++;\n\t\tif (num_dpb_buffer > 17)\n\t\t\tnum_dpb_buffer = 17;\n\n\t\t \n\t\tmin_dpb_size = image_size * num_dpb_buffer;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * num_dpb_buffer * 192;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 32;\n\t\tbreak;\n\n\tcase 1:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 128;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * 64;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * 128;\n\n\t\t \n\t\ttmp = max(width_in_mb, height_in_mb);\n\t\tmin_dpb_size += ALIGN(tmp * 7 * 16, 64);\n\t\tbreak;\n\n\tcase 3:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\t\tbreak;\n\n\tcase 4:  \n\n\t\t \n\t\tmin_dpb_size = image_size * 3;\n\n\t\t \n\t\tmin_dpb_size += width_in_mb * height_in_mb * 64;\n\n\t\t \n\t\tmin_dpb_size += ALIGN(width_in_mb * height_in_mb * 32, 64);\n\t\tbreak;\n\n\tcase 7:  \n\t\tswitch (level) {\n\t\tcase 30:\n\t\t\tnum_dpb_buffer = 8100 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 31:\n\t\t\tnum_dpb_buffer = 18000 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 32:\n\t\t\tnum_dpb_buffer = 20480 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 41:\n\t\t\tnum_dpb_buffer = 32768 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 42:\n\t\t\tnum_dpb_buffer = 34816 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 50:\n\t\t\tnum_dpb_buffer = 110400 / fs_in_mb;\n\t\t\tbreak;\n\t\tcase 51:\n\t\t\tnum_dpb_buffer = 184320 / fs_in_mb;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnum_dpb_buffer = 184320 / fs_in_mb;\n\t\t\tbreak;\n\t\t}\n\t\tnum_dpb_buffer++;\n\t\tif (num_dpb_buffer > 17)\n\t\t\tnum_dpb_buffer = 17;\n\n\t\t \n\t\tmin_dpb_size = image_size * num_dpb_buffer;\n\n\t\tif (!adev->uvd.use_ctx_buf) {\n\t\t\t \n\t\t\tmin_dpb_size +=\n\t\t\t\twidth_in_mb * height_in_mb * num_dpb_buffer * 192;\n\n\t\t\t \n\t\t\tmin_dpb_size += width_in_mb * height_in_mb * 32;\n\t\t} else {\n\t\t\t \n\t\t\tmin_ctx_size =\n\t\t\t\twidth_in_mb * height_in_mb * num_dpb_buffer * 192;\n\t\t}\n\t\tbreak;\n\n\tcase 8:  \n\t\tmin_dpb_size = 0;\n\t\tbreak;\n\n\tcase 16:  \n\t\timage_size = (ALIGN(width, 16) * ALIGN(height, 16) * 3) / 2;\n\t\timage_size = ALIGN(image_size, 256);\n\n\t\tnum_dpb_buffer = (le32_to_cpu(msg[59]) & 0xff) + 2;\n\t\tmin_dpb_size = image_size * num_dpb_buffer;\n\t\tmin_ctx_size = ((width + 255) / 16) * ((height + 255) / 16)\n\t\t\t\t\t   * 16 * num_dpb_buffer + 52 * 1024;\n\t\tbreak;\n\n\tdefault:\n\t\tDRM_ERROR(\"UVD codec not handled %d!\\n\", stream_type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (width > pitch) {\n\t\tDRM_ERROR(\"Invalid UVD decoding target pitch!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (dpb_size < min_dpb_size) {\n\t\tDRM_ERROR(\"Invalid dpb_size in UVD message (%d / %d)!\\n\",\n\t\t\t  dpb_size, min_dpb_size);\n\t\treturn -EINVAL;\n\t}\n\n\tbuf_sizes[0x1] = dpb_size;\n\tbuf_sizes[0x2] = image_size;\n\tbuf_sizes[0x4] = min_ctx_size;\n\t \n\tadev->uvd.decode_image_width = width;\n\treturn 0;\n}\n\n \nstatic int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx,\n\t\t\t     struct amdgpu_bo *bo, unsigned int offset)\n{\n\tstruct amdgpu_device *adev = ctx->parser->adev;\n\tint32_t *msg, msg_type, handle;\n\tvoid *ptr;\n\tlong r;\n\tint i;\n\n\tif (offset & 0x3F) {\n\t\tDRM_ERROR(\"UVD messages must be 64 byte aligned!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tr = amdgpu_bo_kmap(bo, &ptr);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed mapping the UVD) message (%ld)!\\n\", r);\n\t\treturn r;\n\t}\n\n\tmsg = ptr + offset;\n\n\tmsg_type = msg[1];\n\thandle = msg[2];\n\n\tif (handle == 0) {\n\t\tamdgpu_bo_kunmap(bo);\n\t\tDRM_ERROR(\"Invalid UVD handle!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (msg_type) {\n\tcase 0:\n\t\t \n\t\tamdgpu_bo_kunmap(bo);\n\n\t\t \n\t\tfor (i = 0; i < adev->uvd.max_handles; ++i) {\n\t\t\tif (atomic_read(&adev->uvd.handles[i]) == handle) {\n\t\t\t\tDRM_ERROR(\")Handle 0x%x already in use!\\n\",\n\t\t\t\t\t  handle);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (!atomic_cmpxchg(&adev->uvd.handles[i], 0, handle)) {\n\t\t\t\tadev->uvd.filp[i] = ctx->parser->filp;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tDRM_ERROR(\"No more free UVD handles!\\n\");\n\t\treturn -ENOSPC;\n\n\tcase 1:\n\t\t \n\t\tr = amdgpu_uvd_cs_msg_decode(adev, msg, ctx->buf_sizes);\n\t\tamdgpu_bo_kunmap(bo);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tfor (i = 0; i < adev->uvd.max_handles; ++i) {\n\t\t\tif (atomic_read(&adev->uvd.handles[i]) == handle) {\n\t\t\t\tif (adev->uvd.filp[i] != ctx->parser->filp) {\n\t\t\t\t\tDRM_ERROR(\"UVD handle collision detected!\\n\");\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\n\t\tDRM_ERROR(\"Invalid UVD handle 0x%x!\\n\", handle);\n\t\treturn -ENOENT;\n\n\tcase 2:\n\t\t \n\t\tfor (i = 0; i < adev->uvd.max_handles; ++i)\n\t\t\tatomic_cmpxchg(&adev->uvd.handles[i], handle, 0);\n\t\tamdgpu_bo_kunmap(bo);\n\t\treturn 0;\n\n\tdefault:\n\t\tDRM_ERROR(\"Illegal UVD message type (%d)!\\n\", msg_type);\n\t}\n\n\tamdgpu_bo_kunmap(bo);\n\treturn -EINVAL;\n}\n\n \nstatic int amdgpu_uvd_cs_pass2(struct amdgpu_uvd_cs_ctx *ctx)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct amdgpu_bo *bo;\n\tuint32_t cmd;\n\tuint64_t start, end;\n\tuint64_t addr = amdgpu_uvd_get_addr_from_ctx(ctx);\n\tint r;\n\n\tr = amdgpu_cs_find_mapping(ctx->parser, addr, &bo, &mapping);\n\tif (r) {\n\t\tDRM_ERROR(\"Can't find BO for addr 0x%08llx\\n\", addr);\n\t\treturn r;\n\t}\n\n\tstart = amdgpu_bo_gpu_offset(bo);\n\n\tend = (mapping->last + 1 - mapping->start);\n\tend = end * AMDGPU_GPU_PAGE_SIZE + start;\n\n\taddr -= mapping->start * AMDGPU_GPU_PAGE_SIZE;\n\tstart += addr;\n\n\tamdgpu_ib_set_value(ctx->ib, ctx->data0, lower_32_bits(start));\n\tamdgpu_ib_set_value(ctx->ib, ctx->data1, upper_32_bits(start));\n\n\tcmd = amdgpu_ib_get_value(ctx->ib, ctx->idx) >> 1;\n\tif (cmd < 0x4) {\n\t\tif ((end - start) < ctx->buf_sizes[cmd]) {\n\t\t\tDRM_ERROR(\"buffer (%d) to small (%d / %d)!\\n\", cmd,\n\t\t\t\t  (unsigned int)(end - start),\n\t\t\t\t  ctx->buf_sizes[cmd]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t} else if (cmd == 0x206) {\n\t\tif ((end - start) < ctx->buf_sizes[4]) {\n\t\t\tDRM_ERROR(\"buffer (%d) to small (%d / %d)!\\n\", cmd,\n\t\t\t\t\t  (unsigned int)(end - start),\n\t\t\t\t\t  ctx->buf_sizes[4]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if ((cmd != 0x100) && (cmd != 0x204)) {\n\t\tDRM_ERROR(\"invalid UVD command %X!\\n\", cmd);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!ctx->parser->adev->uvd.address_64_bit) {\n\t\tif ((start >> 28) != ((end - 1) >> 28)) {\n\t\t\tDRM_ERROR(\"reloc %llx-%llx crossing 256MB boundary!\\n\",\n\t\t\t\t  start, end);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((cmd == 0 || cmd == 0x3) &&\n\t\t    (start >> 28) != (ctx->parser->adev->uvd.inst->gpu_addr >> 28)) {\n\t\t\tDRM_ERROR(\"msg/fb buffer %llx-%llx out of 256MB segment!\\n\",\n\t\t\t\t  start, end);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (cmd == 0) {\n\t\tctx->has_msg_cmd = true;\n\t\tr = amdgpu_uvd_cs_msg(ctx, bo, addr);\n\t\tif (r)\n\t\t\treturn r;\n\t} else if (!ctx->has_msg_cmd) {\n\t\tDRM_ERROR(\"Message needed before other commands are send!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_uvd_cs_reg(struct amdgpu_uvd_cs_ctx *ctx,\n\t\t\t     int (*cb)(struct amdgpu_uvd_cs_ctx *ctx))\n{\n\tint i, r;\n\n\tctx->idx++;\n\tfor (i = 0; i <= ctx->count; ++i) {\n\t\tunsigned int reg = ctx->reg + i;\n\n\t\tif (ctx->idx >= ctx->ib->length_dw) {\n\t\t\tDRM_ERROR(\"Register command after end of CS!\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tswitch (reg) {\n\t\tcase mmUVD_GPCOM_VCPU_DATA0:\n\t\t\tctx->data0 = ctx->idx;\n\t\t\tbreak;\n\t\tcase mmUVD_GPCOM_VCPU_DATA1:\n\t\t\tctx->data1 = ctx->idx;\n\t\t\tbreak;\n\t\tcase mmUVD_GPCOM_VCPU_CMD:\n\t\t\tr = cb(ctx);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase mmUVD_ENGINE_CNTL:\n\t\tcase mmUVD_NO_OP:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Invalid reg 0x%X!\\n\", reg);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tctx->idx++;\n\t}\n\treturn 0;\n}\n\n \nstatic int amdgpu_uvd_cs_packets(struct amdgpu_uvd_cs_ctx *ctx,\n\t\t\t\t int (*cb)(struct amdgpu_uvd_cs_ctx *ctx))\n{\n\tint r;\n\n\tfor (ctx->idx = 0 ; ctx->idx < ctx->ib->length_dw; ) {\n\t\tuint32_t cmd = amdgpu_ib_get_value(ctx->ib, ctx->idx);\n\t\tunsigned int type = CP_PACKET_GET_TYPE(cmd);\n\n\t\tswitch (type) {\n\t\tcase PACKET_TYPE0:\n\t\t\tctx->reg = CP_PACKET0_GET_REG(cmd);\n\t\t\tctx->count = CP_PACKET_GET_COUNT(cmd);\n\t\t\tr = amdgpu_uvd_cs_reg(ctx, cb);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t\tbreak;\n\t\tcase PACKET_TYPE2:\n\t\t\t++ctx->idx;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tDRM_ERROR(\"Unknown packet type %d !\\n\", type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nint amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser,\n\t\t\t     struct amdgpu_job *job,\n\t\t\t     struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_uvd_cs_ctx ctx = {};\n\tunsigned int buf_sizes[] = {\n\t\t[0x00000000]\t=\t2048,\n\t\t[0x00000001]\t=\t0xFFFFFFFF,\n\t\t[0x00000002]\t=\t0xFFFFFFFF,\n\t\t[0x00000003]\t=\t2048,\n\t\t[0x00000004]\t=\t0xFFFFFFFF,\n\t};\n\tint r;\n\n\tjob->vm = NULL;\n\tib->gpu_addr = amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\n\tif (ib->length_dw % 16) {\n\t\tDRM_ERROR(\"UVD IB length (%d) not 16 dwords aligned!\\n\",\n\t\t\t  ib->length_dw);\n\t\treturn -EINVAL;\n\t}\n\n\tctx.parser = parser;\n\tctx.buf_sizes = buf_sizes;\n\tctx.ib = ib;\n\n\t \n\tif (!parser->adev->uvd.address_64_bit) {\n\t\t \n\t\tr = amdgpu_uvd_cs_packets(&ctx, amdgpu_uvd_cs_pass1);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_uvd_cs_packets(&ctx, amdgpu_uvd_cs_pass2);\n\tif (r)\n\t\treturn r;\n\n\tif (!ctx.has_msg_cmd) {\n\t\tDRM_ERROR(\"UVD-IBs need a msg command!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_uvd_send_msg(struct amdgpu_ring *ring, struct amdgpu_bo *bo,\n\t\t\t       bool direct, struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *f = NULL;\n\tuint32_t offset, data[4];\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tuint64_t addr;\n\tint i, r;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, &adev->uvd.entity,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     64, direct ? AMDGPU_IB_POOL_DIRECT :\n\t\t\t\t     AMDGPU_IB_POOL_DELAYED, &job);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->asic_type >= CHIP_VEGA10)\n\t\toffset = adev->reg_offset[UVD_HWIP][ring->me][1];\n\telse\n\t\toffset = UVD_BASE_SI;\n\n\tdata[0] = PACKET0(offset + UVD_GPCOM_VCPU_DATA0, 0);\n\tdata[1] = PACKET0(offset + UVD_GPCOM_VCPU_DATA1, 0);\n\tdata[2] = PACKET0(offset + UVD_GPCOM_VCPU_CMD, 0);\n\tdata[3] = PACKET0(offset + UVD_NO_OP, 0);\n\n\tib = &job->ibs[0];\n\taddr = amdgpu_bo_gpu_offset(bo);\n\tib->ptr[0] = data[0];\n\tib->ptr[1] = addr;\n\tib->ptr[2] = data[1];\n\tib->ptr[3] = addr >> 32;\n\tib->ptr[4] = data[2];\n\tib->ptr[5] = 0;\n\tfor (i = 6; i < 16; i += 2) {\n\t\tib->ptr[i] = data[3];\n\t\tib->ptr[i+1] = 0;\n\t}\n\tib->length_dw = 16;\n\n\tif (direct) {\n\t\tr = amdgpu_job_submit_direct(job, ring, &f);\n\t\tif (r)\n\t\t\tgoto err_free;\n\t} else {\n\t\tr = drm_sched_job_add_resv_dependencies(&job->base,\n\t\t\t\t\t\t\tbo->tbo.base.resv,\n\t\t\t\t\t\t\tDMA_RESV_USAGE_KERNEL);\n\t\tif (r)\n\t\t\tgoto err_free;\n\n\t\tf = amdgpu_job_submit(job);\n\t}\n\n\tamdgpu_bo_reserve(bo, true);\n\tamdgpu_bo_fence(bo, f, false);\n\tamdgpu_bo_unreserve(bo);\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\n\treturn 0;\n\nerr_free:\n\tamdgpu_job_free(job);\n\treturn r;\n}\n\n \nint amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t      struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_bo *bo = adev->uvd.ib_bo;\n\tuint32_t *msg;\n\tint i;\n\n\tmsg = amdgpu_bo_kptr(bo);\n\t \n\tmsg[0] = cpu_to_le32(0x00000de4);\n\tmsg[1] = cpu_to_le32(0x00000000);\n\tmsg[2] = cpu_to_le32(handle);\n\tmsg[3] = cpu_to_le32(0x00000000);\n\tmsg[4] = cpu_to_le32(0x00000000);\n\tmsg[5] = cpu_to_le32(0x00000000);\n\tmsg[6] = cpu_to_le32(0x00000000);\n\tmsg[7] = cpu_to_le32(0x00000780);\n\tmsg[8] = cpu_to_le32(0x00000440);\n\tmsg[9] = cpu_to_le32(0x00000000);\n\tmsg[10] = cpu_to_le32(0x01b37000);\n\tfor (i = 11; i < 1024; ++i)\n\t\tmsg[i] = cpu_to_le32(0x0);\n\n\treturn amdgpu_uvd_send_msg(ring, bo, true, fence);\n\n}\n\nint amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t       bool direct, struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_bo *bo = NULL;\n\tuint32_t *msg;\n\tint r, i;\n\n\tif (direct) {\n\t\tbo = adev->uvd.ib_bo;\n\t} else {\n\t\tr = amdgpu_uvd_create_msg_bo_helper(adev, 4096, &bo);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tmsg = amdgpu_bo_kptr(bo);\n\t \n\tmsg[0] = cpu_to_le32(0x00000de4);\n\tmsg[1] = cpu_to_le32(0x00000002);\n\tmsg[2] = cpu_to_le32(handle);\n\tmsg[3] = cpu_to_le32(0x00000000);\n\tfor (i = 4; i < 1024; ++i)\n\t\tmsg[i] = cpu_to_le32(0x0);\n\n\tr = amdgpu_uvd_send_msg(ring, bo, direct, fence);\n\n\tif (!direct)\n\t\tamdgpu_bo_free_kernel(&bo, NULL, (void **)&msg);\n\n\treturn r;\n}\n\nstatic void amdgpu_uvd_idle_work_handler(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(work, struct amdgpu_device, uvd.idle_work.work);\n\tunsigned int fences = 0, i, j;\n\n\tfor (i = 0; i < adev->uvd.num_uvd_inst; ++i) {\n\t\tif (adev->uvd.harvest_config & (1 << i))\n\t\t\tcontinue;\n\t\tfences += amdgpu_fence_count_emitted(&adev->uvd.inst[i].ring);\n\t\tfor (j = 0; j < adev->uvd.num_enc_rings; ++j)\n\t\t\tfences += amdgpu_fence_count_emitted(&adev->uvd.inst[i].ring_enc[j]);\n\t}\n\n\tif (fences == 0) {\n\t\tif (adev->pm.dpm_enabled) {\n\t\t\tamdgpu_dpm_enable_uvd(adev, false);\n\t\t} else {\n\t\t\tamdgpu_asic_set_uvd_clocks(adev, 0, 0);\n\t\t\t \n\t\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\n\t\t\t\t\t\t\t       AMD_PG_STATE_GATE);\n\t\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\n\t\t\t\t\t\t\t       AMD_CG_STATE_GATE);\n\t\t}\n\t} else {\n\t\tschedule_delayed_work(&adev->uvd.idle_work, UVD_IDLE_TIMEOUT);\n\t}\n}\n\nvoid amdgpu_uvd_ring_begin_use(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tbool set_clocks;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tset_clocks = !cancel_delayed_work_sync(&adev->uvd.idle_work);\n\tif (set_clocks) {\n\t\tif (adev->pm.dpm_enabled) {\n\t\t\tamdgpu_dpm_enable_uvd(adev, true);\n\t\t} else {\n\t\t\tamdgpu_asic_set_uvd_clocks(adev, 53300, 40000);\n\t\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\n\t\t\t\t\t\t\t       AMD_CG_STATE_UNGATE);\n\t\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_UVD,\n\t\t\t\t\t\t\t       AMD_PG_STATE_UNGATE);\n\t\t}\n\t}\n}\n\nvoid amdgpu_uvd_ring_end_use(struct amdgpu_ring *ring)\n{\n\tif (!amdgpu_sriov_vf(ring->adev))\n\t\tschedule_delayed_work(&ring->adev->uvd.idle_work, UVD_IDLE_TIMEOUT);\n}\n\n \nint amdgpu_uvd_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct dma_fence *fence;\n\tlong r;\n\n\tr = amdgpu_uvd_get_create_msg(ring, 1, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tdma_fence_put(fence);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\tif (r < 0)\n\t\tgoto error;\n\n\tr = amdgpu_uvd_get_destroy_msg(ring, 1, true, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\telse if (r > 0)\n\t\tr = 0;\n\n\tdma_fence_put(fence);\n\nerror:\n\treturn r;\n}\n\n \nuint32_t amdgpu_uvd_used_handles(struct amdgpu_device *adev)\n{\n\tunsigned int i;\n\tuint32_t used_handles = 0;\n\n\tfor (i = 0; i < adev->uvd.max_handles; ++i) {\n\t\t \n\t\tif (atomic_read(&adev->uvd.handles[i]))\n\t\t\tused_handles++;\n\t}\n\n\treturn used_handles;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}