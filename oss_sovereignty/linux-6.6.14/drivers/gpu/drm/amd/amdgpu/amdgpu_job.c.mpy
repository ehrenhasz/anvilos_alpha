{
  "module_name": "amdgpu_job.c",
  "hash_id": "acc7dcf8cda42aa2383f4af762d412730361cb45c96e9b0810ce6a0892d5f816",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_job.c",
  "human_readable_source": " \n#include <linux/kthread.h>\n#include <linux/wait.h>\n#include <linux/sched.h>\n\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_reset.h\"\n\nstatic enum drm_gpu_sched_stat amdgpu_job_timedout(struct drm_sched_job *s_job)\n{\n\tstruct amdgpu_ring *ring = to_amdgpu_ring(s_job->sched);\n\tstruct amdgpu_job *job = to_amdgpu_job(s_job);\n\tstruct amdgpu_task_info ti;\n\tstruct amdgpu_device *adev = ring->adev;\n\tint idx;\n\tint r;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tDRM_INFO(\"%s - device unplugged skipping recovery on scheduler:%s\",\n\t\t\t __func__, s_job->sched->name);\n\n\t\t \n\t\treturn DRM_GPU_SCHED_STAT_ENODEV;\n\t}\n\n\tmemset(&ti, 0, sizeof(struct amdgpu_task_info));\n\tadev->job_hang = true;\n\n\tif (amdgpu_gpu_recovery &&\n\t    amdgpu_ring_soft_recovery(ring, job->vmid, s_job->s_fence->parent)) {\n\t\tDRM_ERROR(\"ring %s timeout, but soft recovered\\n\",\n\t\t\t  s_job->sched->name);\n\t\tgoto exit;\n\t}\n\n\tamdgpu_vm_get_task_info(ring->adev, job->pasid, &ti);\n\tDRM_ERROR(\"ring %s timeout, signaled seq=%u, emitted seq=%u\\n\",\n\t\t  job->base.sched->name, atomic_read(&ring->fence_drv.last_seq),\n\t\t  ring->fence_drv.sync_seq);\n\tDRM_ERROR(\"Process information: process %s pid %d thread %s pid %d\\n\",\n\t\t  ti.process_name, ti.tgid, ti.task_name, ti.pid);\n\n\tdma_fence_set_error(&s_job->s_fence->finished, -ETIME);\n\n\tif (amdgpu_device_should_recover_gpu(ring->adev)) {\n\t\tstruct amdgpu_reset_context reset_context;\n\t\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\t\treset_context.method = AMD_RESET_METHOD_NONE;\n\t\treset_context.reset_req_dev = adev;\n\t\tclear_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\t\tr = amdgpu_device_gpu_recover(ring->adev, job, &reset_context);\n\t\tif (r)\n\t\t\tDRM_ERROR(\"GPU Recovery Failed: %d\\n\", r);\n\t} else {\n\t\tdrm_sched_suspend_timeout(&ring->sched);\n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\tadev->virt.tdr_debug = true;\n\t}\n\nexit:\n\tadev->job_hang = false;\n\tdrm_dev_exit(idx);\n\treturn DRM_GPU_SCHED_STAT_NOMINAL;\n}\n\nint amdgpu_job_alloc(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t     struct drm_sched_entity *entity, void *owner,\n\t\t     unsigned int num_ibs, struct amdgpu_job **job)\n{\n\tif (num_ibs == 0)\n\t\treturn -EINVAL;\n\n\t*job = kzalloc(struct_size(*job, ibs, num_ibs), GFP_KERNEL);\n\tif (!*job)\n\t\treturn -ENOMEM;\n\n\t \n\t(*job)->base.sched = &adev->rings[0]->sched;\n\t(*job)->vm = vm;\n\n\tamdgpu_sync_create(&(*job)->explicit_sync);\n\t(*job)->generation = amdgpu_vm_generation(adev, vm);\n\t(*job)->vm_pd_addr = AMDGPU_BO_INVALID_OFFSET;\n\n\tif (!entity)\n\t\treturn 0;\n\n\treturn drm_sched_job_init(&(*job)->base, entity, owner);\n}\n\nint amdgpu_job_alloc_with_ib(struct amdgpu_device *adev,\n\t\t\t     struct drm_sched_entity *entity, void *owner,\n\t\t\t     size_t size, enum amdgpu_ib_pool_type pool_type,\n\t\t\t     struct amdgpu_job **job)\n{\n\tint r;\n\n\tr = amdgpu_job_alloc(adev, NULL, entity, owner, 1, job);\n\tif (r)\n\t\treturn r;\n\n\t(*job)->num_ibs = 1;\n\tr = amdgpu_ib_get(adev, NULL, size, pool_type, &(*job)->ibs[0]);\n\tif (r) {\n\t\tif (entity)\n\t\t\tdrm_sched_job_cleanup(&(*job)->base);\n\t\tkfree(*job);\n\t}\n\n\treturn r;\n}\n\nvoid amdgpu_job_set_resources(struct amdgpu_job *job, struct amdgpu_bo *gds,\n\t\t\t      struct amdgpu_bo *gws, struct amdgpu_bo *oa)\n{\n\tif (gds) {\n\t\tjob->gds_base = amdgpu_bo_gpu_offset(gds) >> PAGE_SHIFT;\n\t\tjob->gds_size = amdgpu_bo_size(gds) >> PAGE_SHIFT;\n\t}\n\tif (gws) {\n\t\tjob->gws_base = amdgpu_bo_gpu_offset(gws) >> PAGE_SHIFT;\n\t\tjob->gws_size = amdgpu_bo_size(gws) >> PAGE_SHIFT;\n\t}\n\tif (oa) {\n\t\tjob->oa_base = amdgpu_bo_gpu_offset(oa) >> PAGE_SHIFT;\n\t\tjob->oa_size = amdgpu_bo_size(oa) >> PAGE_SHIFT;\n\t}\n}\n\nvoid amdgpu_job_free_resources(struct amdgpu_job *job)\n{\n\tstruct amdgpu_ring *ring = to_amdgpu_ring(job->base.sched);\n\tstruct dma_fence *f;\n\tunsigned i;\n\n\t \n\tif (job->base.s_fence && job->base.s_fence->finished.ops)\n\t\tf = &job->base.s_fence->finished;\n\telse if (job->hw_fence.ops)\n\t\tf = &job->hw_fence;\n\telse\n\t\tf = NULL;\n\n\tfor (i = 0; i < job->num_ibs; ++i)\n\t\tamdgpu_ib_free(ring->adev, &job->ibs[i], f);\n}\n\nstatic void amdgpu_job_free_cb(struct drm_sched_job *s_job)\n{\n\tstruct amdgpu_job *job = to_amdgpu_job(s_job);\n\n\tdrm_sched_job_cleanup(s_job);\n\n\tamdgpu_sync_free(&job->explicit_sync);\n\n\t \n\tif (!job->hw_fence.ops)\n\t\tkfree(job);\n\telse\n\t\tdma_fence_put(&job->hw_fence);\n}\n\nvoid amdgpu_job_set_gang_leader(struct amdgpu_job *job,\n\t\t\t\tstruct amdgpu_job *leader)\n{\n\tstruct dma_fence *fence = &leader->base.s_fence->scheduled;\n\n\tWARN_ON(job->gang_submit);\n\n\t \n\tif (job != leader)\n\t\tdma_fence_get(fence);\n\tjob->gang_submit = fence;\n}\n\nvoid amdgpu_job_free(struct amdgpu_job *job)\n{\n\tif (job->base.entity)\n\t\tdrm_sched_job_cleanup(&job->base);\n\n\tamdgpu_job_free_resources(job);\n\tamdgpu_sync_free(&job->explicit_sync);\n\tif (job->gang_submit != &job->base.s_fence->scheduled)\n\t\tdma_fence_put(job->gang_submit);\n\n\tif (!job->hw_fence.ops)\n\t\tkfree(job);\n\telse\n\t\tdma_fence_put(&job->hw_fence);\n}\n\nstruct dma_fence *amdgpu_job_submit(struct amdgpu_job *job)\n{\n\tstruct dma_fence *f;\n\n\tdrm_sched_job_arm(&job->base);\n\tf = dma_fence_get(&job->base.s_fence->finished);\n\tamdgpu_job_free_resources(job);\n\tdrm_sched_entity_push_job(&job->base);\n\n\treturn f;\n}\n\nint amdgpu_job_submit_direct(struct amdgpu_job *job, struct amdgpu_ring *ring,\n\t\t\t     struct dma_fence **fence)\n{\n\tint r;\n\n\tjob->base.sched = &ring->sched;\n\tr = amdgpu_ib_schedule(ring, job->num_ibs, job->ibs, job, fence);\n\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_job_free(job);\n\treturn 0;\n}\n\nstatic struct dma_fence *\namdgpu_job_prepare_job(struct drm_sched_job *sched_job,\n\t\t      struct drm_sched_entity *s_entity)\n{\n\tstruct amdgpu_ring *ring = to_amdgpu_ring(s_entity->rq->sched);\n\tstruct amdgpu_job *job = to_amdgpu_job(sched_job);\n\tstruct dma_fence *fence = NULL;\n\tint r;\n\n\t \n\tr = drm_sched_entity_error(s_entity);\n\tif (r && r != -ENODATA)\n\t\tgoto error;\n\n\tif (!fence && job->gang_submit)\n\t\tfence = amdgpu_device_switch_gang(ring->adev, job->gang_submit);\n\n\twhile (!fence && job->vm && !job->vmid) {\n\t\tr = amdgpu_vmid_grab(job->vm, ring, job, &fence);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Error getting VM ID (%d)\\n\", r);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\treturn fence;\n\nerror:\n\tdma_fence_set_error(&job->base.s_fence->finished, r);\n\treturn NULL;\n}\n\nstatic struct dma_fence *amdgpu_job_run(struct drm_sched_job *sched_job)\n{\n\tstruct amdgpu_ring *ring = to_amdgpu_ring(sched_job->sched);\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *fence = NULL, *finished;\n\tstruct amdgpu_job *job;\n\tint r = 0;\n\n\tjob = to_amdgpu_job(sched_job);\n\tfinished = &job->base.s_fence->finished;\n\n\ttrace_amdgpu_sched_run_job(job);\n\n\t \n\tif (job->generation != amdgpu_vm_generation(adev, job->vm) ||\n\t    (job->job_run_counter && job->gang_submit))\n\t\tdma_fence_set_error(finished, -ECANCELED);\n\n\tif (finished->error < 0) {\n\t\tDRM_INFO(\"Skip scheduling IBs!\\n\");\n\t} else {\n\t\tr = amdgpu_ib_schedule(ring, job->num_ibs, job->ibs, job,\n\t\t\t\t       &fence);\n\t\tif (r)\n\t\t\tDRM_ERROR(\"Error scheduling IBs (%d)\\n\", r);\n\t}\n\n\tjob->job_run_counter++;\n\tamdgpu_job_free_resources(job);\n\n\tfence = r ? ERR_PTR(r) : fence;\n\treturn fence;\n}\n\n#define to_drm_sched_job(sched_job)\t\t\\\n\t\tcontainer_of((sched_job), struct drm_sched_job, queue_node)\n\nvoid amdgpu_job_stop_all_jobs_on_sched(struct drm_gpu_scheduler *sched)\n{\n\tstruct drm_sched_job *s_job;\n\tstruct drm_sched_entity *s_entity = NULL;\n\tint i;\n\n\t \n\tfor (i = DRM_SCHED_PRIORITY_COUNT - 1; i >= DRM_SCHED_PRIORITY_MIN; i--) {\n\t\tstruct drm_sched_rq *rq = &sched->sched_rq[i];\n\t\tspin_lock(&rq->lock);\n\t\tlist_for_each_entry(s_entity, &rq->entities, list) {\n\t\t\twhile ((s_job = to_drm_sched_job(spsc_queue_pop(&s_entity->job_queue)))) {\n\t\t\t\tstruct drm_sched_fence *s_fence = s_job->s_fence;\n\n\t\t\t\tdma_fence_signal(&s_fence->scheduled);\n\t\t\t\tdma_fence_set_error(&s_fence->finished, -EHWPOISON);\n\t\t\t\tdma_fence_signal(&s_fence->finished);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&rq->lock);\n\t}\n\n\t \n\tlist_for_each_entry(s_job, &sched->pending_list, list) {\n\t\tstruct drm_sched_fence *s_fence = s_job->s_fence;\n\n\t\tdma_fence_set_error(&s_fence->finished, -EHWPOISON);\n\t\tdma_fence_signal(&s_fence->finished);\n\t}\n}\n\nconst struct drm_sched_backend_ops amdgpu_sched_ops = {\n\t.prepare_job = amdgpu_job_prepare_job,\n\t.run_job = amdgpu_job_run,\n\t.timedout_job = amdgpu_job_timedout,\n\t.free_job = amdgpu_job_free_cb\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}