{
  "module_name": "amdgpu_vm.h",
  "hash_id": "26dab5093c516c777ba79e95321d505c6f2d75b56dce1d4812add8363f5a9098",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.h",
  "human_readable_source": " \n#ifndef __AMDGPU_VM_H__\n#define __AMDGPU_VM_H__\n\n#include <linux/idr.h>\n#include <linux/kfifo.h>\n#include <linux/rbtree.h>\n#include <drm/gpu_scheduler.h>\n#include <drm/drm_file.h>\n#include <drm/ttm/ttm_bo.h>\n#include <linux/sched/mm.h>\n\n#include \"amdgpu_sync.h\"\n#include \"amdgpu_ring.h\"\n#include \"amdgpu_ids.h\"\n\nstruct drm_exec;\n\nstruct amdgpu_bo_va;\nstruct amdgpu_job;\nstruct amdgpu_bo_list_entry;\nstruct amdgpu_bo_vm;\nstruct amdgpu_mem_stats;\n\n \n\n \n#define AMDGPU_VM_MAX_UPDATE_SIZE\t0x3FFFF\n\n \n#define AMDGPU_VM_PTE_COUNT(adev) (1 << (adev)->vm_manager.block_size)\n\n#define AMDGPU_PTE_VALID\t(1ULL << 0)\n#define AMDGPU_PTE_SYSTEM\t(1ULL << 1)\n#define AMDGPU_PTE_SNOOPED\t(1ULL << 2)\n\n \n#define AMDGPU_PTE_TMZ\t\t(1ULL << 3)\n\n \n#define AMDGPU_PTE_EXECUTABLE\t(1ULL << 4)\n\n#define AMDGPU_PTE_READABLE\t(1ULL << 5)\n#define AMDGPU_PTE_WRITEABLE\t(1ULL << 6)\n\n#define AMDGPU_PTE_FRAG(x)\t((x & 0x1fULL) << 7)\n\n \n#define AMDGPU_PTE_PRT\t\t(1ULL << 51)\n\n \n#define AMDGPU_PDE_PTE\t\t(1ULL << 54)\n\n#define AMDGPU_PTE_LOG          (1ULL << 55)\n\n \n#define AMDGPU_PTE_TF\t\t(1ULL << 56)\n\n \n#define AMDGPU_PTE_NOALLOC\t(1ULL << 58)\n\n \n#define AMDGPU_PDE_BFS(a)\t((uint64_t)a << 59)\n\n \n#define AMDGPU_VM_NORETRY_FLAGS\t(AMDGPU_PTE_EXECUTABLE | AMDGPU_PDE_PTE | \\\n\t\t\t\tAMDGPU_PTE_TF)\n\n \n#define AMDGPU_VM_NORETRY_FLAGS_TF (AMDGPU_PTE_VALID | AMDGPU_PTE_SYSTEM | \\\n\t\t\t\t   AMDGPU_PTE_PRT)\n \n#define AMDGPU_PTE_MTYPE_VG10(a)\t((uint64_t)(a) << 57)\n#define AMDGPU_PTE_MTYPE_VG10_MASK\tAMDGPU_PTE_MTYPE_VG10(3ULL)\n\n#define AMDGPU_MTYPE_NC 0\n#define AMDGPU_MTYPE_CC 2\n\n#define AMDGPU_PTE_DEFAULT_ATC  (AMDGPU_PTE_SYSTEM      \\\n                                | AMDGPU_PTE_SNOOPED    \\\n                                | AMDGPU_PTE_EXECUTABLE \\\n                                | AMDGPU_PTE_READABLE   \\\n                                | AMDGPU_PTE_WRITEABLE  \\\n                                | AMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_CC))\n\n \n#define AMDGPU_PTE_MTYPE_NV10(a)       ((uint64_t)(a) << 48)\n#define AMDGPU_PTE_MTYPE_NV10_MASK     AMDGPU_PTE_MTYPE_NV10(7ULL)\n\n \n#define AMDGPU_VM_FAULT_STOP_NEVER\t0\n#define AMDGPU_VM_FAULT_STOP_FIRST\t1\n#define AMDGPU_VM_FAULT_STOP_ALWAYS\t2\n\n \n#define AMDGPU_VM_RESERVED_VRAM\t\t(8ULL << 20)\n\n \n#define AMDGPU_MAX_VMHUBS\t\t\t13\n#define AMDGPU_GFXHUB(x)\t\t\t(x)\n#define AMDGPU_MMHUB0(x)\t\t\t(8 + x)\n#define AMDGPU_MMHUB1(x)\t\t\t(8 + 4 + x)\n\n \n#define AMDGPU_VA_RESERVED_SIZE\t\t\t(2ULL << 20)\n\n \n#define AMDGPU_VM_USE_CPU_FOR_GFX (1 << 0)\n#define AMDGPU_VM_USE_CPU_FOR_COMPUTE (1 << 1)\n\n \nenum amdgpu_vm_level {\n\tAMDGPU_VM_PDB2,\n\tAMDGPU_VM_PDB1,\n\tAMDGPU_VM_PDB0,\n\tAMDGPU_VM_PTB\n};\n\n \nstruct amdgpu_vm_bo_base {\n\t \n\tstruct amdgpu_vm\t\t*vm;\n\tstruct amdgpu_bo\t\t*bo;\n\n\t \n\tstruct amdgpu_vm_bo_base\t*next;\n\n\t \n\tstruct list_head\t\tvm_status;\n\n\t \n\tbool\t\t\t\tmoved;\n};\n\n \nstruct amdgpu_vm_pte_funcs {\n\t \n\tunsigned\tcopy_pte_num_dw;\n\n\t \n\tvoid (*copy_pte)(struct amdgpu_ib *ib,\n\t\t\t uint64_t pe, uint64_t src,\n\t\t\t unsigned count);\n\n\t \n\tvoid (*write_pte)(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t  uint64_t value, unsigned count,\n\t\t\t  uint32_t incr);\n\t \n\tvoid (*set_pte_pde)(struct amdgpu_ib *ib,\n\t\t\t    uint64_t pe,\n\t\t\t    uint64_t addr, unsigned count,\n\t\t\t    uint32_t incr, uint64_t flags);\n};\n\nstruct amdgpu_task_info {\n\tchar\tprocess_name[TASK_COMM_LEN];\n\tchar\ttask_name[TASK_COMM_LEN];\n\tpid_t\tpid;\n\tpid_t\ttgid;\n};\n\n \nstruct amdgpu_vm_update_params {\n\n\t \n\tstruct amdgpu_device *adev;\n\n\t \n\tstruct amdgpu_vm *vm;\n\n\t \n\tbool immediate;\n\n\t \n\tbool unlocked;\n\n\t \n\tdma_addr_t *pages_addr;\n\n\t \n\tstruct amdgpu_job *job;\n\n\t \n\tunsigned int num_dw_left;\n\n\t \n\tbool table_freed;\n};\n\nstruct amdgpu_vm_update_funcs {\n\tint (*map_table)(struct amdgpu_bo_vm *bo);\n\tint (*prepare)(struct amdgpu_vm_update_params *p, struct dma_resv *resv,\n\t\t       enum amdgpu_sync_mode sync_mode);\n\tint (*update)(struct amdgpu_vm_update_params *p,\n\t\t      struct amdgpu_bo_vm *bo, uint64_t pe, uint64_t addr,\n\t\t      unsigned count, uint32_t incr, uint64_t flags);\n\tint (*commit)(struct amdgpu_vm_update_params *p,\n\t\t      struct dma_fence **fence);\n};\n\nstruct amdgpu_vm {\n\t \n\tstruct rb_root_cached\tva;\n\n\t \n\tstruct mutex\t\teviction_lock;\n\tbool\t\t\tevicting;\n\tunsigned int\t\tsaved_flags;\n\n\t \n\tspinlock_t\t\tstatus_lock;\n\n\t \n\tstruct list_head\tevicted;\n\n\t \n\tstruct list_head\trelocated;\n\n\t \n\tstruct list_head\tmoved;\n\n\t \n\tstruct list_head\tidle;\n\n\t \n\tstruct list_head\tinvalidated;\n\n\t \n\tstruct list_head\tfreed;\n\n\t \n\tstruct list_head        done;\n\n\t \n\tstruct list_head\tpt_freed;\n\tstruct work_struct\tpt_free_work;\n\n\t \n\tstruct amdgpu_vm_bo_base     root;\n\tstruct dma_fence\t*last_update;\n\n\t \n\tstruct drm_sched_entity\timmediate;\n\tstruct drm_sched_entity\tdelayed;\n\n\t \n\tatomic64_t\t\ttlb_seq;\n\tstruct dma_fence\t*last_tlb_flush;\n\n\t \n\tuint64_t\t\tgeneration;\n\n\t \n\tstruct dma_fence\t*last_unlocked;\n\n\tunsigned int\t\tpasid;\n\tbool\t\t\treserved_vmid[AMDGPU_MAX_VMHUBS];\n\n\t \n\tbool\t\t\t\t\tuse_cpu_for_update;\n\n\t \n\tconst struct amdgpu_vm_update_funcs\t*update_funcs;\n\n\t \n\tbool\t\t\tpte_support_ats;\n\n\t \n\tDECLARE_KFIFO(faults, u64, 128);\n\n\t \n\tstruct amdkfd_process_info *process_info;\n\n\t \n\tstruct list_head\tvm_list_node;\n\n\t \n\tuint64_t\t\tpd_phys_addr;\n\n\t \n\tstruct amdgpu_task_info task_info;\n\n\t \n\tstruct ttm_lru_bulk_move lru_bulk_move;\n\t \n\tbool\t\t\tis_compute_context;\n\n\t \n\tint8_t\t\t\tmem_id;\n};\n\nstruct amdgpu_vm_manager {\n\t \n\tstruct amdgpu_vmid_mgr\t\t\tid_mgr[AMDGPU_MAX_VMHUBS];\n\tunsigned int\t\t\t\tfirst_kfd_vmid;\n\tbool\t\t\t\t\tconcurrent_flush;\n\n\t \n\tu64\t\t\t\t\tfence_context;\n\tunsigned\t\t\t\tseqno[AMDGPU_MAX_RINGS];\n\n\tuint64_t\t\t\t\tmax_pfn;\n\tuint32_t\t\t\t\tnum_level;\n\tuint32_t\t\t\t\tblock_size;\n\tuint32_t\t\t\t\tfragment_size;\n\tenum amdgpu_vm_level\t\t\troot_level;\n\t \n\tu64\t\t\t\t\tvram_base_offset;\n\t \n\tconst struct amdgpu_vm_pte_funcs\t*vm_pte_funcs;\n\tstruct drm_gpu_scheduler\t\t*vm_pte_scheds[AMDGPU_MAX_RINGS];\n\tunsigned\t\t\t\tvm_pte_num_scheds;\n\tstruct amdgpu_ring\t\t\t*page_fault;\n\n\t \n\tspinlock_t\t\t\t\tprt_lock;\n\tatomic_t\t\t\t\tnum_prt_users;\n\n\t \n\tint\t\t\t\t\tvm_update_mode;\n\n\t \n\tstruct xarray\t\t\t\tpasids;\n};\n\nstruct amdgpu_bo_va_mapping;\n\n#define amdgpu_vm_copy_pte(adev, ib, pe, src, count) ((adev)->vm_manager.vm_pte_funcs->copy_pte((ib), (pe), (src), (count)))\n#define amdgpu_vm_write_pte(adev, ib, pe, value, count, incr) ((adev)->vm_manager.vm_pte_funcs->write_pte((ib), (pe), (value), (count), (incr)))\n#define amdgpu_vm_set_pte_pde(adev, ib, pe, addr, count, incr, flags) ((adev)->vm_manager.vm_pte_funcs->set_pte_pde((ib), (pe), (addr), (count), (incr), (flags)))\n\nextern const struct amdgpu_vm_update_funcs amdgpu_vm_cpu_funcs;\nextern const struct amdgpu_vm_update_funcs amdgpu_vm_sdma_funcs;\n\nvoid amdgpu_vm_manager_init(struct amdgpu_device *adev);\nvoid amdgpu_vm_manager_fini(struct amdgpu_device *adev);\n\nint amdgpu_vm_set_pasid(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\tu32 pasid);\n\nlong amdgpu_vm_wait_idle(struct amdgpu_vm *vm, long timeout);\nint amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm, int32_t xcp_id);\nint amdgpu_vm_make_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);\nvoid amdgpu_vm_release_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm);\nvoid amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm);\nint amdgpu_vm_lock_pd(struct amdgpu_vm *vm, struct drm_exec *exec,\n\t\t      unsigned int num_fences);\nbool amdgpu_vm_ready(struct amdgpu_vm *vm);\nuint64_t amdgpu_vm_generation(struct amdgpu_device *adev, struct amdgpu_vm *vm);\nint amdgpu_vm_validate_pt_bos(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\t      int (*callback)(void *p, struct amdgpu_bo *bo),\n\t\t\t      void *param);\nint amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job, bool need_pipe_sync);\nint amdgpu_vm_update_pdes(struct amdgpu_device *adev,\n\t\t\t  struct amdgpu_vm *vm, bool immediate);\nint amdgpu_vm_clear_freed(struct amdgpu_device *adev,\n\t\t\t  struct amdgpu_vm *vm,\n\t\t\t  struct dma_fence **fence);\nint amdgpu_vm_handle_moved(struct amdgpu_device *adev,\n\t\t\t   struct amdgpu_vm *vm);\nvoid amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,\n\t\t\t    struct amdgpu_vm *vm, struct amdgpu_bo *bo);\nint amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\t   bool immediate, bool unlocked, bool flush_tlb,\n\t\t\t   struct dma_resv *resv, uint64_t start, uint64_t last,\n\t\t\t   uint64_t flags, uint64_t offset, uint64_t vram_base,\n\t\t\t   struct ttm_resource *res, dma_addr_t *pages_addr,\n\t\t\t   struct dma_fence **fence);\nint amdgpu_vm_bo_update(struct amdgpu_device *adev,\n\t\t\tstruct amdgpu_bo_va *bo_va,\n\t\t\tbool clear);\nbool amdgpu_vm_evictable(struct amdgpu_bo *bo);\nvoid amdgpu_vm_bo_invalidate(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_bo *bo, bool evicted);\nuint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr);\nstruct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,\n\t\t\t\t       struct amdgpu_bo *bo);\nstruct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_vm *vm,\n\t\t\t\t      struct amdgpu_bo *bo);\nint amdgpu_vm_bo_map(struct amdgpu_device *adev,\n\t\t     struct amdgpu_bo_va *bo_va,\n\t\t     uint64_t addr, uint64_t offset,\n\t\t     uint64_t size, uint64_t flags);\nint amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_bo_va *bo_va,\n\t\t\t     uint64_t addr, uint64_t offset,\n\t\t\t     uint64_t size, uint64_t flags);\nint amdgpu_vm_bo_unmap(struct amdgpu_device *adev,\n\t\t       struct amdgpu_bo_va *bo_va,\n\t\t       uint64_t addr);\nint amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm,\n\t\t\t\tuint64_t saddr, uint64_t size);\nstruct amdgpu_bo_va_mapping *amdgpu_vm_bo_lookup_mapping(struct amdgpu_vm *vm,\n\t\t\t\t\t\t\t uint64_t addr);\nvoid amdgpu_vm_bo_trace_cs(struct amdgpu_vm *vm, struct ww_acquire_ctx *ticket);\nvoid amdgpu_vm_bo_del(struct amdgpu_device *adev,\n\t\t      struct amdgpu_bo_va *bo_va);\nvoid amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size,\n\t\t\t   uint32_t fragment_size_default, unsigned max_level,\n\t\t\t   unsigned max_bits);\nint amdgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp);\nbool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,\n\t\t\t\t  struct amdgpu_job *job);\nvoid amdgpu_vm_check_compute_bug(struct amdgpu_device *adev);\n\nvoid amdgpu_vm_get_task_info(struct amdgpu_device *adev, u32 pasid,\n\t\t\t     struct amdgpu_task_info *task_info);\nbool amdgpu_vm_handle_fault(struct amdgpu_device *adev, u32 pasid,\n\t\t\t    u32 vmid, u32 node_id, uint64_t addr,\n\t\t\t    bool write_fault);\n\nvoid amdgpu_vm_set_task_info(struct amdgpu_vm *vm);\n\nvoid amdgpu_vm_move_to_lru_tail(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm);\nvoid amdgpu_vm_get_memory(struct amdgpu_vm *vm,\n\t\t\t  struct amdgpu_mem_stats *stats);\n\nint amdgpu_vm_pt_clear(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t       struct amdgpu_bo_vm *vmbo, bool immediate);\nint amdgpu_vm_pt_create(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\tint level, bool immediate, struct amdgpu_bo_vm **vmbo,\n\t\t\tint32_t xcp_id);\nvoid amdgpu_vm_pt_free_root(struct amdgpu_device *adev, struct amdgpu_vm *vm);\nbool amdgpu_vm_pt_is_root_clean(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm);\n\nint amdgpu_vm_pde_update(struct amdgpu_vm_update_params *params,\n\t\t\t struct amdgpu_vm_bo_base *entry);\nint amdgpu_vm_ptes_update(struct amdgpu_vm_update_params *params,\n\t\t\t  uint64_t start, uint64_t end,\n\t\t\t  uint64_t dst, uint64_t flags);\nvoid amdgpu_vm_pt_free_work(struct work_struct *work);\n\n#if defined(CONFIG_DEBUG_FS)\nvoid amdgpu_debugfs_vm_bo_info(struct amdgpu_vm *vm, struct seq_file *m);\n#endif\n\nint amdgpu_vm_pt_map_tables(struct amdgpu_device *adev, struct amdgpu_vm *vm);\n\n \nstatic inline uint64_t amdgpu_vm_tlb_seq(struct amdgpu_vm *vm)\n{\n\tunsigned long flags;\n\tspinlock_t *lock;\n\n\t \n\trcu_read_lock();\n\tlock = vm->last_tlb_flush->lock;\n\trcu_read_unlock();\n\n\tspin_lock_irqsave(lock, flags);\n\tspin_unlock_irqrestore(lock, flags);\n\n\treturn atomic64_read(&vm->tlb_seq);\n}\n\n \nstatic inline void amdgpu_vm_eviction_lock(struct amdgpu_vm *vm)\n{\n\tmutex_lock(&vm->eviction_lock);\n\tvm->saved_flags = memalloc_noreclaim_save();\n}\n\nstatic inline bool amdgpu_vm_eviction_trylock(struct amdgpu_vm *vm)\n{\n\tif (mutex_trylock(&vm->eviction_lock)) {\n\t\tvm->saved_flags = memalloc_noreclaim_save();\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void amdgpu_vm_eviction_unlock(struct amdgpu_vm *vm)\n{\n\tmemalloc_noreclaim_restore(vm->saved_flags);\n\tmutex_unlock(&vm->eviction_lock);\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}