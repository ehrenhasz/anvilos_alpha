{
  "module_name": "dce_v8_0.c",
  "hash_id": "3a843462768e097184105f73cbf479fa68957daa7922cbb89704a9ca66eb0c62",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c",
  "human_readable_source": " \n\n#include <drm/drm_fourcc.h>\n#include <drm/drm_modeset_helper.h>\n#include <drm/drm_modeset_helper_vtables.h>\n#include <drm/drm_vblank.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_i2c.h\"\n#include \"cikd.h\"\n#include \"atom.h\"\n#include \"amdgpu_atombios.h\"\n#include \"atombios_crtc.h\"\n#include \"atombios_encoders.h\"\n#include \"amdgpu_pll.h\"\n#include \"amdgpu_connectors.h\"\n#include \"amdgpu_display.h\"\n#include \"dce_v8_0.h\"\n\n#include \"dce/dce_8_0_d.h\"\n#include \"dce/dce_8_0_sh_mask.h\"\n\n#include \"gca/gfx_7_2_enum.h\"\n\n#include \"gmc/gmc_7_1_d.h\"\n#include \"gmc/gmc_7_1_sh_mask.h\"\n\n#include \"oss/oss_2_0_d.h\"\n#include \"oss/oss_2_0_sh_mask.h\"\n\nstatic void dce_v8_0_set_display_funcs(struct amdgpu_device *adev);\nstatic void dce_v8_0_set_irq_funcs(struct amdgpu_device *adev);\n\nstatic const u32 crtc_offsets[6] = {\n\tCRTC0_REGISTER_OFFSET,\n\tCRTC1_REGISTER_OFFSET,\n\tCRTC2_REGISTER_OFFSET,\n\tCRTC3_REGISTER_OFFSET,\n\tCRTC4_REGISTER_OFFSET,\n\tCRTC5_REGISTER_OFFSET\n};\n\nstatic const u32 hpd_offsets[] = {\n\tHPD0_REGISTER_OFFSET,\n\tHPD1_REGISTER_OFFSET,\n\tHPD2_REGISTER_OFFSET,\n\tHPD3_REGISTER_OFFSET,\n\tHPD4_REGISTER_OFFSET,\n\tHPD5_REGISTER_OFFSET\n};\n\nstatic const uint32_t dig_offsets[] = {\n\tCRTC0_REGISTER_OFFSET,\n\tCRTC1_REGISTER_OFFSET,\n\tCRTC2_REGISTER_OFFSET,\n\tCRTC3_REGISTER_OFFSET,\n\tCRTC4_REGISTER_OFFSET,\n\tCRTC5_REGISTER_OFFSET,\n\t(0x13830 - 0x7030) >> 2,\n};\n\nstatic const struct {\n\tuint32_t\treg;\n\tuint32_t\tvblank;\n\tuint32_t\tvline;\n\tuint32_t\thpd;\n\n} interrupt_status_offsets[6] = { {\n\t.reg = mmDISP_INTERRUPT_STATUS,\n\t.vblank = DISP_INTERRUPT_STATUS__LB_D1_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS__LB_D1_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS__DC_HPD1_INTERRUPT_MASK\n}, {\n\t.reg = mmDISP_INTERRUPT_STATUS_CONTINUE,\n\t.vblank = DISP_INTERRUPT_STATUS_CONTINUE__LB_D2_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS_CONTINUE__LB_D2_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS_CONTINUE__DC_HPD2_INTERRUPT_MASK\n}, {\n\t.reg = mmDISP_INTERRUPT_STATUS_CONTINUE2,\n\t.vblank = DISP_INTERRUPT_STATUS_CONTINUE2__LB_D3_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS_CONTINUE2__LB_D3_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS_CONTINUE2__DC_HPD3_INTERRUPT_MASK\n}, {\n\t.reg = mmDISP_INTERRUPT_STATUS_CONTINUE3,\n\t.vblank = DISP_INTERRUPT_STATUS_CONTINUE3__LB_D4_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS_CONTINUE3__LB_D4_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS_CONTINUE3__DC_HPD4_INTERRUPT_MASK\n}, {\n\t.reg = mmDISP_INTERRUPT_STATUS_CONTINUE4,\n\t.vblank = DISP_INTERRUPT_STATUS_CONTINUE4__LB_D5_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS_CONTINUE4__LB_D5_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS_CONTINUE4__DC_HPD5_INTERRUPT_MASK\n}, {\n\t.reg = mmDISP_INTERRUPT_STATUS_CONTINUE5,\n\t.vblank = DISP_INTERRUPT_STATUS_CONTINUE5__LB_D6_VBLANK_INTERRUPT_MASK,\n\t.vline = DISP_INTERRUPT_STATUS_CONTINUE5__LB_D6_VLINE_INTERRUPT_MASK,\n\t.hpd = DISP_INTERRUPT_STATUS_CONTINUE5__DC_HPD6_INTERRUPT_MASK\n} };\n\nstatic u32 dce_v8_0_audio_endpt_rreg(struct amdgpu_device *adev,\n\t\t\t\t     u32 block_offset, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->audio_endpt_idx_lock, flags);\n\tWREG32(mmAZALIA_F0_CODEC_ENDPOINT_INDEX + block_offset, reg);\n\tr = RREG32(mmAZALIA_F0_CODEC_ENDPOINT_DATA + block_offset);\n\tspin_unlock_irqrestore(&adev->audio_endpt_idx_lock, flags);\n\n\treturn r;\n}\n\nstatic void dce_v8_0_audio_endpt_wreg(struct amdgpu_device *adev,\n\t\t\t\t      u32 block_offset, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->audio_endpt_idx_lock, flags);\n\tWREG32(mmAZALIA_F0_CODEC_ENDPOINT_INDEX + block_offset, reg);\n\tWREG32(mmAZALIA_F0_CODEC_ENDPOINT_DATA + block_offset, v);\n\tspin_unlock_irqrestore(&adev->audio_endpt_idx_lock, flags);\n}\n\nstatic u32 dce_v8_0_vblank_get_counter(struct amdgpu_device *adev, int crtc)\n{\n\tif (crtc >= adev->mode_info.num_crtc)\n\t\treturn 0;\n\telse\n\t\treturn RREG32(mmCRTC_STATUS_FRAME_COUNT + crtc_offsets[crtc]);\n}\n\nstatic void dce_v8_0_pageflip_interrupt_init(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t \n\tfor (i = 0; i < adev->mode_info.num_crtc; i++)\n\t\tamdgpu_irq_get(adev, &adev->pageflip_irq, i);\n}\n\nstatic void dce_v8_0_pageflip_interrupt_fini(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t \n\tfor (i = 0; i < adev->mode_info.num_crtc; i++)\n\t\tamdgpu_irq_put(adev, &adev->pageflip_irq, i);\n}\n\n \nstatic void dce_v8_0_page_flip(struct amdgpu_device *adev,\n\t\t\t       int crtc_id, u64 crtc_base, bool async)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = adev->mode_info.crtcs[crtc_id];\n\tstruct drm_framebuffer *fb = amdgpu_crtc->base.primary->fb;\n\n\t \n\tWREG32(mmGRPH_FLIP_CONTROL + amdgpu_crtc->crtc_offset, async ?\n\t       GRPH_FLIP_CONTROL__GRPH_SURFACE_UPDATE_H_RETRACE_EN_MASK : 0);\n\t \n\tWREG32(mmGRPH_PITCH + amdgpu_crtc->crtc_offset,\n\t       fb->pitches[0] / fb->format->cpp[0]);\n\t \n\tWREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS_HIGH + amdgpu_crtc->crtc_offset,\n\t       upper_32_bits(crtc_base));\n\t \n\tWREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS + amdgpu_crtc->crtc_offset,\n\t       lower_32_bits(crtc_base));\n\t \n\tRREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS + amdgpu_crtc->crtc_offset);\n}\n\nstatic int dce_v8_0_crtc_get_scanoutpos(struct amdgpu_device *adev, int crtc,\n\t\t\t\t\tu32 *vbl, u32 *position)\n{\n\tif ((crtc < 0) || (crtc >= adev->mode_info.num_crtc))\n\t\treturn -EINVAL;\n\n\t*vbl = RREG32(mmCRTC_V_BLANK_START_END + crtc_offsets[crtc]);\n\t*position = RREG32(mmCRTC_STATUS_POSITION + crtc_offsets[crtc]);\n\n\treturn 0;\n}\n\n \nstatic bool dce_v8_0_hpd_sense(struct amdgpu_device *adev,\n\t\t\t       enum amdgpu_hpd_id hpd)\n{\n\tbool connected = false;\n\n\tif (hpd >= adev->mode_info.num_hpd)\n\t\treturn connected;\n\n\tif (RREG32(mmDC_HPD1_INT_STATUS + hpd_offsets[hpd]) &\n\t    DC_HPD1_INT_STATUS__DC_HPD1_SENSE_MASK)\n\t\tconnected = true;\n\n\treturn connected;\n}\n\n \nstatic void dce_v8_0_hpd_set_polarity(struct amdgpu_device *adev,\n\t\t\t\t      enum amdgpu_hpd_id hpd)\n{\n\tu32 tmp;\n\tbool connected = dce_v8_0_hpd_sense(adev, hpd);\n\n\tif (hpd >= adev->mode_info.num_hpd)\n\t\treturn;\n\n\ttmp = RREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[hpd]);\n\tif (connected)\n\t\ttmp &= ~DC_HPD1_INT_CONTROL__DC_HPD1_INT_POLARITY_MASK;\n\telse\n\t\ttmp |= DC_HPD1_INT_CONTROL__DC_HPD1_INT_POLARITY_MASK;\n\tWREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[hpd], tmp);\n}\n\n \nstatic void dce_v8_0_hpd_init(struct amdgpu_device *adev)\n{\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_connector *connector;\n\tstruct drm_connector_list_iter iter;\n\tu32 tmp;\n\n\tdrm_connector_list_iter_begin(dev, &iter);\n\tdrm_for_each_connector_iter(connector, &iter) {\n\t\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\n\t\tif (amdgpu_connector->hpd.hpd >= adev->mode_info.num_hpd)\n\t\t\tcontinue;\n\n\t\ttmp = RREG32(mmDC_HPD1_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd]);\n\t\ttmp |= DC_HPD1_CONTROL__DC_HPD1_EN_MASK;\n\t\tWREG32(mmDC_HPD1_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd], tmp);\n\n\t\tif (connector->connector_type == DRM_MODE_CONNECTOR_eDP ||\n\t\t    connector->connector_type == DRM_MODE_CONNECTOR_LVDS) {\n\t\t\t \n\t\t\ttmp = RREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd]);\n\t\t\ttmp &= ~DC_HPD1_INT_CONTROL__DC_HPD1_INT_EN_MASK;\n\t\t\tWREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd], tmp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdce_v8_0_hpd_set_polarity(adev, amdgpu_connector->hpd.hpd);\n\t\tamdgpu_irq_get(adev, &adev->hpd_irq, amdgpu_connector->hpd.hpd);\n\t}\n\tdrm_connector_list_iter_end(&iter);\n}\n\n \nstatic void dce_v8_0_hpd_fini(struct amdgpu_device *adev)\n{\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_connector *connector;\n\tstruct drm_connector_list_iter iter;\n\tu32 tmp;\n\n\tdrm_connector_list_iter_begin(dev, &iter);\n\tdrm_for_each_connector_iter(connector, &iter) {\n\t\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\n\t\tif (amdgpu_connector->hpd.hpd >= adev->mode_info.num_hpd)\n\t\t\tcontinue;\n\n\t\ttmp = RREG32(mmDC_HPD1_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd]);\n\t\ttmp &= ~DC_HPD1_CONTROL__DC_HPD1_EN_MASK;\n\t\tWREG32(mmDC_HPD1_CONTROL + hpd_offsets[amdgpu_connector->hpd.hpd], tmp);\n\n\t\tamdgpu_irq_put(adev, &adev->hpd_irq, amdgpu_connector->hpd.hpd);\n\t}\n\tdrm_connector_list_iter_end(&iter);\n}\n\nstatic u32 dce_v8_0_hpd_get_gpio_reg(struct amdgpu_device *adev)\n{\n\treturn mmDC_GPIO_HPD_A;\n}\n\nstatic bool dce_v8_0_is_display_hung(struct amdgpu_device *adev)\n{\n\tu32 crtc_hung = 0;\n\tu32 crtc_status[6];\n\tu32 i, j, tmp;\n\n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tif (RREG32(mmCRTC_CONTROL + crtc_offsets[i]) & CRTC_CONTROL__CRTC_MASTER_EN_MASK) {\n\t\t\tcrtc_status[i] = RREG32(mmCRTC_STATUS_HV_COUNT + crtc_offsets[i]);\n\t\t\tcrtc_hung |= (1 << i);\n\t\t}\n\t}\n\n\tfor (j = 0; j < 10; j++) {\n\t\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\t\tif (crtc_hung & (1 << i)) {\n\t\t\t\ttmp = RREG32(mmCRTC_STATUS_HV_COUNT + crtc_offsets[i]);\n\t\t\t\tif (tmp != crtc_status[i])\n\t\t\t\t\tcrtc_hung &= ~(1 << i);\n\t\t\t}\n\t\t}\n\t\tif (crtc_hung == 0)\n\t\t\treturn false;\n\t\tudelay(100);\n\t}\n\n\treturn true;\n}\n\nstatic void dce_v8_0_set_vga_render_state(struct amdgpu_device *adev,\n\t\t\t\t\t  bool render)\n{\n\tu32 tmp;\n\n\t \n\ttmp = RREG32(mmVGA_HDP_CONTROL);\n\tif (render)\n\t\ttmp = REG_SET_FIELD(tmp, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 0);\n\telse\n\t\ttmp = REG_SET_FIELD(tmp, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 1);\n\tWREG32(mmVGA_HDP_CONTROL, tmp);\n\n\t \n\ttmp = RREG32(mmVGA_RENDER_CONTROL);\n\tif (render)\n\t\ttmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 1);\n\telse\n\t\ttmp = REG_SET_FIELD(tmp, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 0);\n\tWREG32(mmVGA_RENDER_CONTROL, tmp);\n}\n\nstatic int dce_v8_0_get_num_crtc(struct amdgpu_device *adev)\n{\n\tint num_crtc = 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_BONAIRE:\n\tcase CHIP_HAWAII:\n\t\tnum_crtc = 6;\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\t\tnum_crtc = 4;\n\t\tbreak;\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n\t\tnum_crtc = 2;\n\t\tbreak;\n\tdefault:\n\t\tnum_crtc = 0;\n\t}\n\treturn num_crtc;\n}\n\nvoid dce_v8_0_disable_dce(struct amdgpu_device *adev)\n{\n\t \n\tif (amdgpu_atombios_has_dce_engine_info(adev)) {\n\t\tu32 tmp;\n\t\tint crtc_enabled, i;\n\n\t\tdce_v8_0_set_vga_render_state(adev, false);\n\n\t\t \n\t\tfor (i = 0; i < dce_v8_0_get_num_crtc(adev); i++) {\n\t\t\tcrtc_enabled = REG_GET_FIELD(RREG32(mmCRTC_CONTROL + crtc_offsets[i]),\n\t\t\t\t\t\t\t\t\t CRTC_CONTROL, CRTC_MASTER_EN);\n\t\t\tif (crtc_enabled) {\n\t\t\t\tWREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 1);\n\t\t\t\ttmp = RREG32(mmCRTC_CONTROL + crtc_offsets[i]);\n\t\t\t\ttmp = REG_SET_FIELD(tmp, CRTC_CONTROL, CRTC_MASTER_EN, 0);\n\t\t\t\tWREG32(mmCRTC_CONTROL + crtc_offsets[i], tmp);\n\t\t\t\tWREG32(mmCRTC_UPDATE_LOCK + crtc_offsets[i], 0);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void dce_v8_0_program_fmt(struct drm_encoder *encoder)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(encoder->crtc);\n\tstruct drm_connector *connector = amdgpu_get_connector_for_encoder(encoder);\n\tint bpc = 0;\n\tu32 tmp = 0;\n\tenum amdgpu_connector_dither dither = AMDGPU_FMT_DITHER_DISABLE;\n\n\tif (connector) {\n\t\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\t\tbpc = amdgpu_connector_get_monitor_bpc(connector);\n\t\tdither = amdgpu_connector->dither;\n\t}\n\n\t \n\tif (amdgpu_encoder->devices & ATOM_DEVICE_LCD_SUPPORT)\n\t\treturn;\n\n\t \n\tif ((amdgpu_encoder->encoder_id == ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1) ||\n\t    (amdgpu_encoder->encoder_id == ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2))\n\t\treturn;\n\n\tif (bpc == 0)\n\t\treturn;\n\n\tswitch (bpc) {\n\tcase 6:\n\t\tif (dither == AMDGPU_FMT_DITHER_ENABLE)\n\t\t\t \n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_FRAME_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_HIGHPASS_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_EN_MASK |\n\t\t\t\t(0 << FMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_DEPTH__SHIFT));\n\t\telse\n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_EN_MASK |\n\t\t\t(0 << FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_DEPTH__SHIFT));\n\t\tbreak;\n\tcase 8:\n\t\tif (dither == AMDGPU_FMT_DITHER_ENABLE)\n\t\t\t \n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_FRAME_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_HIGHPASS_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_RGB_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_EN_MASK |\n\t\t\t\t(1 << FMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_DEPTH__SHIFT));\n\t\telse\n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_EN_MASK |\n\t\t\t(1 << FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_DEPTH__SHIFT));\n\t\tbreak;\n\tcase 10:\n\t\tif (dither == AMDGPU_FMT_DITHER_ENABLE)\n\t\t\t \n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_FRAME_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_HIGHPASS_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_RGB_RANDOM_ENABLE_MASK |\n\t\t\t\tFMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_EN_MASK |\n\t\t\t\t(2 << FMT_BIT_DEPTH_CONTROL__FMT_SPATIAL_DITHER_DEPTH__SHIFT));\n\t\telse\n\t\t\ttmp |= (FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_EN_MASK |\n\t\t\t(2 << FMT_BIT_DEPTH_CONTROL__FMT_TRUNCATE_DEPTH__SHIFT));\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\tWREG32(mmFMT_BIT_DEPTH_CONTROL + amdgpu_crtc->crtc_offset, tmp);\n}\n\n\n \n \nstatic u32 dce_v8_0_line_buffer_adjust(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_crtc *amdgpu_crtc,\n\t\t\t\t       struct drm_display_mode *mode)\n{\n\tu32 tmp, buffer_alloc, i;\n\tu32 pipe_offset = amdgpu_crtc->crtc_id * 0x8;\n\t \n\tif (amdgpu_crtc->base.enabled && mode) {\n\t\tif (mode->crtc_hdisplay < 1920) {\n\t\t\ttmp = 1;\n\t\t\tbuffer_alloc = 2;\n\t\t} else if (mode->crtc_hdisplay < 2560) {\n\t\t\ttmp = 2;\n\t\t\tbuffer_alloc = 2;\n\t\t} else if (mode->crtc_hdisplay < 4096) {\n\t\t\ttmp = 0;\n\t\t\tbuffer_alloc = (adev->flags & AMD_IS_APU) ? 2 : 4;\n\t\t} else {\n\t\t\tDRM_DEBUG_KMS(\"Mode too big for LB!\\n\");\n\t\t\ttmp = 0;\n\t\t\tbuffer_alloc = (adev->flags & AMD_IS_APU) ? 2 : 4;\n\t\t}\n\t} else {\n\t\ttmp = 1;\n\t\tbuffer_alloc = 0;\n\t}\n\n\tWREG32(mmLB_MEMORY_CTRL + amdgpu_crtc->crtc_offset,\n\t      (tmp << LB_MEMORY_CTRL__LB_MEMORY_CONFIG__SHIFT) |\n\t      (0x6B0 << LB_MEMORY_CTRL__LB_MEMORY_SIZE__SHIFT));\n\n\tWREG32(mmPIPE0_DMIF_BUFFER_CONTROL + pipe_offset,\n\t       (buffer_alloc << PIPE0_DMIF_BUFFER_CONTROL__DMIF_BUFFERS_ALLOCATED__SHIFT));\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmPIPE0_DMIF_BUFFER_CONTROL + pipe_offset) &\n\t\t    PIPE0_DMIF_BUFFER_CONTROL__DMIF_BUFFERS_ALLOCATION_COMPLETED_MASK)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (amdgpu_crtc->base.enabled && mode) {\n\t\tswitch (tmp) {\n\t\tcase 0:\n\t\tdefault:\n\t\t\treturn 4096 * 2;\n\t\tcase 1:\n\t\t\treturn 1920 * 2;\n\t\tcase 2:\n\t\t\treturn 2560 * 2;\n\t\t}\n\t}\n\n\t \n\treturn 0;\n}\n\n \nstatic u32 cik_get_number_of_dram_channels(struct amdgpu_device *adev)\n{\n\tu32 tmp = RREG32(mmMC_SHARED_CHMAP);\n\n\tswitch ((tmp & MC_SHARED_CHMAP__NOOFCHAN_MASK) >> MC_SHARED_CHMAP__NOOFCHAN__SHIFT) {\n\tcase 0:\n\tdefault:\n\t\treturn 1;\n\tcase 1:\n\t\treturn 2;\n\tcase 2:\n\t\treturn 4;\n\tcase 3:\n\t\treturn 8;\n\tcase 4:\n\t\treturn 3;\n\tcase 5:\n\t\treturn 6;\n\tcase 6:\n\t\treturn 10;\n\tcase 7:\n\t\treturn 12;\n\tcase 8:\n\t\treturn 16;\n\t}\n}\n\nstruct dce8_wm_params {\n\tu32 dram_channels;  \n\tu32 yclk;           \n\tu32 sclk;           \n\tu32 disp_clk;       \n\tu32 src_width;      \n\tu32 active_time;    \n\tu32 blank_time;     \n\tbool interlaced;     \n\tfixed20_12 vsc;     \n\tu32 num_heads;      \n\tu32 bytes_per_pixel;  \n\tu32 lb_size;        \n\tu32 vtaps;          \n};\n\n \nstatic u32 dce_v8_0_dram_bandwidth(struct dce8_wm_params *wm)\n{\n\t \n\tfixed20_12 dram_efficiency;  \n\tfixed20_12 yclk, dram_channels, bandwidth;\n\tfixed20_12 a;\n\n\ta.full = dfixed_const(1000);\n\tyclk.full = dfixed_const(wm->yclk);\n\tyclk.full = dfixed_div(yclk, a);\n\tdram_channels.full = dfixed_const(wm->dram_channels * 4);\n\ta.full = dfixed_const(10);\n\tdram_efficiency.full = dfixed_const(7);\n\tdram_efficiency.full = dfixed_div(dram_efficiency, a);\n\tbandwidth.full = dfixed_mul(dram_channels, yclk);\n\tbandwidth.full = dfixed_mul(bandwidth, dram_efficiency);\n\n\treturn dfixed_trunc(bandwidth);\n}\n\n \nstatic u32 dce_v8_0_dram_bandwidth_for_display(struct dce8_wm_params *wm)\n{\n\t \n\tfixed20_12 disp_dram_allocation;  \n\tfixed20_12 yclk, dram_channels, bandwidth;\n\tfixed20_12 a;\n\n\ta.full = dfixed_const(1000);\n\tyclk.full = dfixed_const(wm->yclk);\n\tyclk.full = dfixed_div(yclk, a);\n\tdram_channels.full = dfixed_const(wm->dram_channels * 4);\n\ta.full = dfixed_const(10);\n\tdisp_dram_allocation.full = dfixed_const(3);  \n\tdisp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);\n\tbandwidth.full = dfixed_mul(dram_channels, yclk);\n\tbandwidth.full = dfixed_mul(bandwidth, disp_dram_allocation);\n\n\treturn dfixed_trunc(bandwidth);\n}\n\n \nstatic u32 dce_v8_0_data_return_bandwidth(struct dce8_wm_params *wm)\n{\n\t \n\tfixed20_12 return_efficiency;  \n\tfixed20_12 sclk, bandwidth;\n\tfixed20_12 a;\n\n\ta.full = dfixed_const(1000);\n\tsclk.full = dfixed_const(wm->sclk);\n\tsclk.full = dfixed_div(sclk, a);\n\ta.full = dfixed_const(10);\n\treturn_efficiency.full = dfixed_const(8);\n\treturn_efficiency.full = dfixed_div(return_efficiency, a);\n\ta.full = dfixed_const(32);\n\tbandwidth.full = dfixed_mul(a, sclk);\n\tbandwidth.full = dfixed_mul(bandwidth, return_efficiency);\n\n\treturn dfixed_trunc(bandwidth);\n}\n\n \nstatic u32 dce_v8_0_dmif_request_bandwidth(struct dce8_wm_params *wm)\n{\n\t \n\tfixed20_12 disp_clk_request_efficiency;  \n\tfixed20_12 disp_clk, bandwidth;\n\tfixed20_12 a, b;\n\n\ta.full = dfixed_const(1000);\n\tdisp_clk.full = dfixed_const(wm->disp_clk);\n\tdisp_clk.full = dfixed_div(disp_clk, a);\n\ta.full = dfixed_const(32);\n\tb.full = dfixed_mul(a, disp_clk);\n\n\ta.full = dfixed_const(10);\n\tdisp_clk_request_efficiency.full = dfixed_const(8);\n\tdisp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);\n\n\tbandwidth.full = dfixed_mul(b, disp_clk_request_efficiency);\n\n\treturn dfixed_trunc(bandwidth);\n}\n\n \nstatic u32 dce_v8_0_available_bandwidth(struct dce8_wm_params *wm)\n{\n\t \n\tu32 dram_bandwidth = dce_v8_0_dram_bandwidth(wm);\n\tu32 data_return_bandwidth = dce_v8_0_data_return_bandwidth(wm);\n\tu32 dmif_req_bandwidth = dce_v8_0_dmif_request_bandwidth(wm);\n\n\treturn min(dram_bandwidth, min(data_return_bandwidth, dmif_req_bandwidth));\n}\n\n \nstatic u32 dce_v8_0_average_bandwidth(struct dce8_wm_params *wm)\n{\n\t \n\tfixed20_12 bpp;\n\tfixed20_12 line_time;\n\tfixed20_12 src_width;\n\tfixed20_12 bandwidth;\n\tfixed20_12 a;\n\n\ta.full = dfixed_const(1000);\n\tline_time.full = dfixed_const(wm->active_time + wm->blank_time);\n\tline_time.full = dfixed_div(line_time, a);\n\tbpp.full = dfixed_const(wm->bytes_per_pixel);\n\tsrc_width.full = dfixed_const(wm->src_width);\n\tbandwidth.full = dfixed_mul(src_width, bpp);\n\tbandwidth.full = dfixed_mul(bandwidth, wm->vsc);\n\tbandwidth.full = dfixed_div(bandwidth, line_time);\n\n\treturn dfixed_trunc(bandwidth);\n}\n\n \nstatic u32 dce_v8_0_latency_watermark(struct dce8_wm_params *wm)\n{\n\t \n\tu32 mc_latency = 2000;  \n\tu32 available_bandwidth = dce_v8_0_available_bandwidth(wm);\n\tu32 worst_chunk_return_time = (512 * 8 * 1000) / available_bandwidth;\n\tu32 cursor_line_pair_return_time = (128 * 4 * 1000) / available_bandwidth;\n\tu32 dc_latency = 40000000 / wm->disp_clk;  \n\tu32 other_heads_data_return_time = ((wm->num_heads + 1) * worst_chunk_return_time) +\n\t\t(wm->num_heads * cursor_line_pair_return_time);\n\tu32 latency = mc_latency + other_heads_data_return_time + dc_latency;\n\tu32 max_src_lines_per_dst_line, lb_fill_bw, line_fill_time;\n\tu32 tmp, dmif_size = 12288;\n\tfixed20_12 a, b, c;\n\n\tif (wm->num_heads == 0)\n\t\treturn 0;\n\n\ta.full = dfixed_const(2);\n\tb.full = dfixed_const(1);\n\tif ((wm->vsc.full > a.full) ||\n\t    ((wm->vsc.full > b.full) && (wm->vtaps >= 3)) ||\n\t    (wm->vtaps >= 5) ||\n\t    ((wm->vsc.full >= a.full) && wm->interlaced))\n\t\tmax_src_lines_per_dst_line = 4;\n\telse\n\t\tmax_src_lines_per_dst_line = 2;\n\n\ta.full = dfixed_const(available_bandwidth);\n\tb.full = dfixed_const(wm->num_heads);\n\ta.full = dfixed_div(a, b);\n\ttmp = div_u64((u64) dmif_size * (u64) wm->disp_clk, mc_latency + 512);\n\ttmp = min(dfixed_trunc(a), tmp);\n\n\tlb_fill_bw = min(tmp, wm->disp_clk * wm->bytes_per_pixel / 1000);\n\n\ta.full = dfixed_const(max_src_lines_per_dst_line * wm->src_width * wm->bytes_per_pixel);\n\tb.full = dfixed_const(1000);\n\tc.full = dfixed_const(lb_fill_bw);\n\tb.full = dfixed_div(c, b);\n\ta.full = dfixed_div(a, b);\n\tline_fill_time = dfixed_trunc(a);\n\n\tif (line_fill_time < wm->active_time)\n\t\treturn latency;\n\telse\n\t\treturn latency + (line_fill_time - wm->active_time);\n\n}\n\n \nstatic bool dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(struct dce8_wm_params *wm)\n{\n\tif (dce_v8_0_average_bandwidth(wm) <=\n\t    (dce_v8_0_dram_bandwidth_for_display(wm) / wm->num_heads))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\n \nstatic bool dce_v8_0_average_bandwidth_vs_available_bandwidth(struct dce8_wm_params *wm)\n{\n\tif (dce_v8_0_average_bandwidth(wm) <=\n\t    (dce_v8_0_available_bandwidth(wm) / wm->num_heads))\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\n \nstatic bool dce_v8_0_check_latency_hiding(struct dce8_wm_params *wm)\n{\n\tu32 lb_partitions = wm->lb_size / wm->src_width;\n\tu32 line_time = wm->active_time + wm->blank_time;\n\tu32 latency_tolerant_lines;\n\tu32 latency_hiding;\n\tfixed20_12 a;\n\n\ta.full = dfixed_const(1);\n\tif (wm->vsc.full > a.full)\n\t\tlatency_tolerant_lines = 1;\n\telse {\n\t\tif (lb_partitions <= (wm->vtaps + 1))\n\t\t\tlatency_tolerant_lines = 1;\n\t\telse\n\t\t\tlatency_tolerant_lines = 2;\n\t}\n\n\tlatency_hiding = (latency_tolerant_lines * line_time + wm->blank_time);\n\n\tif (dce_v8_0_latency_watermark(wm) <= latency_hiding)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\n \nstatic void dce_v8_0_program_watermarks(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_crtc *amdgpu_crtc,\n\t\t\t\t\tu32 lb_size, u32 num_heads)\n{\n\tstruct drm_display_mode *mode = &amdgpu_crtc->base.mode;\n\tstruct dce8_wm_params wm_low, wm_high;\n\tu32 active_time;\n\tu32 line_time = 0;\n\tu32 latency_watermark_a = 0, latency_watermark_b = 0;\n\tu32 tmp, wm_mask, lb_vblank_lead_lines = 0;\n\n\tif (amdgpu_crtc->base.enabled && num_heads && mode) {\n\t\tactive_time = (u32) div_u64((u64)mode->crtc_hdisplay * 1000000,\n\t\t\t\t\t    (u32)mode->clock);\n\t\tline_time = (u32) div_u64((u64)mode->crtc_htotal * 1000000,\n\t\t\t\t\t  (u32)mode->clock);\n\t\tline_time = min(line_time, (u32)65535);\n\n\t\t \n\t\tif (adev->pm.dpm_enabled) {\n\t\t\twm_high.yclk =\n\t\t\t\tamdgpu_dpm_get_mclk(adev, false) * 10;\n\t\t\twm_high.sclk =\n\t\t\t\tamdgpu_dpm_get_sclk(adev, false) * 10;\n\t\t} else {\n\t\t\twm_high.yclk = adev->pm.current_mclk * 10;\n\t\t\twm_high.sclk = adev->pm.current_sclk * 10;\n\t\t}\n\n\t\twm_high.disp_clk = mode->clock;\n\t\twm_high.src_width = mode->crtc_hdisplay;\n\t\twm_high.active_time = active_time;\n\t\twm_high.blank_time = line_time - wm_high.active_time;\n\t\twm_high.interlaced = false;\n\t\tif (mode->flags & DRM_MODE_FLAG_INTERLACE)\n\t\t\twm_high.interlaced = true;\n\t\twm_high.vsc = amdgpu_crtc->vsc;\n\t\twm_high.vtaps = 1;\n\t\tif (amdgpu_crtc->rmx_type != RMX_OFF)\n\t\t\twm_high.vtaps = 2;\n\t\twm_high.bytes_per_pixel = 4;  \n\t\twm_high.lb_size = lb_size;\n\t\twm_high.dram_channels = cik_get_number_of_dram_channels(adev);\n\t\twm_high.num_heads = num_heads;\n\n\t\t \n\t\tlatency_watermark_a = min(dce_v8_0_latency_watermark(&wm_high), (u32)65535);\n\n\t\t \n\t\t \n\t\tif (!dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(&wm_high) ||\n\t\t    !dce_v8_0_average_bandwidth_vs_available_bandwidth(&wm_high) ||\n\t\t    !dce_v8_0_check_latency_hiding(&wm_high) ||\n\t\t    (adev->mode_info.disp_priority == 2)) {\n\t\t\tDRM_DEBUG_KMS(\"force priority to high\\n\");\n\t\t}\n\n\t\t \n\t\tif (adev->pm.dpm_enabled) {\n\t\t\twm_low.yclk =\n\t\t\t\tamdgpu_dpm_get_mclk(adev, true) * 10;\n\t\t\twm_low.sclk =\n\t\t\t\tamdgpu_dpm_get_sclk(adev, true) * 10;\n\t\t} else {\n\t\t\twm_low.yclk = adev->pm.current_mclk * 10;\n\t\t\twm_low.sclk = adev->pm.current_sclk * 10;\n\t\t}\n\n\t\twm_low.disp_clk = mode->clock;\n\t\twm_low.src_width = mode->crtc_hdisplay;\n\t\twm_low.active_time = active_time;\n\t\twm_low.blank_time = line_time - wm_low.active_time;\n\t\twm_low.interlaced = false;\n\t\tif (mode->flags & DRM_MODE_FLAG_INTERLACE)\n\t\t\twm_low.interlaced = true;\n\t\twm_low.vsc = amdgpu_crtc->vsc;\n\t\twm_low.vtaps = 1;\n\t\tif (amdgpu_crtc->rmx_type != RMX_OFF)\n\t\t\twm_low.vtaps = 2;\n\t\twm_low.bytes_per_pixel = 4;  \n\t\twm_low.lb_size = lb_size;\n\t\twm_low.dram_channels = cik_get_number_of_dram_channels(adev);\n\t\twm_low.num_heads = num_heads;\n\n\t\t \n\t\tlatency_watermark_b = min(dce_v8_0_latency_watermark(&wm_low), (u32)65535);\n\n\t\t \n\t\t \n\t\tif (!dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(&wm_low) ||\n\t\t    !dce_v8_0_average_bandwidth_vs_available_bandwidth(&wm_low) ||\n\t\t    !dce_v8_0_check_latency_hiding(&wm_low) ||\n\t\t    (adev->mode_info.disp_priority == 2)) {\n\t\t\tDRM_DEBUG_KMS(\"force priority to high\\n\");\n\t\t}\n\t\tlb_vblank_lead_lines = DIV_ROUND_UP(lb_size, mode->crtc_hdisplay);\n\t}\n\n\t \n\twm_mask = RREG32(mmDPG_WATERMARK_MASK_CONTROL + amdgpu_crtc->crtc_offset);\n\ttmp = wm_mask;\n\ttmp &= ~(3 << DPG_WATERMARK_MASK_CONTROL__URGENCY_WATERMARK_MASK__SHIFT);\n\ttmp |= (1 << DPG_WATERMARK_MASK_CONTROL__URGENCY_WATERMARK_MASK__SHIFT);\n\tWREG32(mmDPG_WATERMARK_MASK_CONTROL + amdgpu_crtc->crtc_offset, tmp);\n\tWREG32(mmDPG_PIPE_URGENCY_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((latency_watermark_a << DPG_PIPE_URGENCY_CONTROL__URGENCY_LOW_WATERMARK__SHIFT) |\n\t\t(line_time << DPG_PIPE_URGENCY_CONTROL__URGENCY_HIGH_WATERMARK__SHIFT)));\n\t \n\ttmp = RREG32(mmDPG_WATERMARK_MASK_CONTROL + amdgpu_crtc->crtc_offset);\n\ttmp &= ~(3 << DPG_WATERMARK_MASK_CONTROL__URGENCY_WATERMARK_MASK__SHIFT);\n\ttmp |= (2 << DPG_WATERMARK_MASK_CONTROL__URGENCY_WATERMARK_MASK__SHIFT);\n\tWREG32(mmDPG_WATERMARK_MASK_CONTROL + amdgpu_crtc->crtc_offset, tmp);\n\tWREG32(mmDPG_PIPE_URGENCY_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((latency_watermark_b << DPG_PIPE_URGENCY_CONTROL__URGENCY_LOW_WATERMARK__SHIFT) |\n\t\t(line_time << DPG_PIPE_URGENCY_CONTROL__URGENCY_HIGH_WATERMARK__SHIFT)));\n\t \n\tWREG32(mmDPG_WATERMARK_MASK_CONTROL + amdgpu_crtc->crtc_offset, wm_mask);\n\n\t \n\tamdgpu_crtc->line_time = line_time;\n\tamdgpu_crtc->wm_high = latency_watermark_a;\n\tamdgpu_crtc->wm_low = latency_watermark_b;\n\t \n\tamdgpu_crtc->lb_vblank_lead_lines = lb_vblank_lead_lines;\n}\n\n \nstatic void dce_v8_0_bandwidth_update(struct amdgpu_device *adev)\n{\n\tstruct drm_display_mode *mode = NULL;\n\tu32 num_heads = 0, lb_size;\n\tint i;\n\n\tamdgpu_display_update_priority(adev);\n\n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tif (adev->mode_info.crtcs[i]->base.enabled)\n\t\t\tnum_heads++;\n\t}\n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tmode = &adev->mode_info.crtcs[i]->base.mode;\n\t\tlb_size = dce_v8_0_line_buffer_adjust(adev, adev->mode_info.crtcs[i], mode);\n\t\tdce_v8_0_program_watermarks(adev, adev->mode_info.crtcs[i],\n\t\t\t\t\t    lb_size, num_heads);\n\t}\n}\n\nstatic void dce_v8_0_audio_get_connected_pins(struct amdgpu_device *adev)\n{\n\tint i;\n\tu32 offset, tmp;\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++) {\n\t\toffset = adev->mode_info.audio.pin[i].offset;\n\t\ttmp = RREG32_AUDIO_ENDPT(offset,\n\t\t\t\t\t ixAZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_CONFIGURATION_DEFAULT);\n\t\tif (((tmp &\n\t\tAZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_CONFIGURATION_DEFAULT__PORT_CONNECTIVITY_MASK) >>\n\t\tAZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_CONFIGURATION_DEFAULT__PORT_CONNECTIVITY__SHIFT) == 1)\n\t\t\tadev->mode_info.audio.pin[i].connected = false;\n\t\telse\n\t\t\tadev->mode_info.audio.pin[i].connected = true;\n\t}\n}\n\nstatic struct amdgpu_audio_pin *dce_v8_0_audio_get_pin(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tdce_v8_0_audio_get_connected_pins(adev);\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++) {\n\t\tif (adev->mode_info.audio.pin[i].connected)\n\t\t\treturn &adev->mode_info.audio.pin[i];\n\t}\n\tDRM_ERROR(\"No connected audio pins found!\\n\");\n\treturn NULL;\n}\n\nstatic void dce_v8_0_afmt_audio_select_pin(struct drm_encoder *encoder)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(encoder->dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tu32 offset;\n\n\tif (!dig || !dig->afmt || !dig->afmt->pin)\n\t\treturn;\n\n\toffset = dig->afmt->offset;\n\n\tWREG32(mmAFMT_AUDIO_SRC_CONTROL + offset,\n\t       (dig->afmt->pin->id << AFMT_AUDIO_SRC_CONTROL__AFMT_AUDIO_SRC_SELECT__SHIFT));\n}\n\nstatic void dce_v8_0_audio_write_latency_fields(struct drm_encoder *encoder,\n\t\t\t\t\t\tstruct drm_display_mode *mode)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tstruct drm_connector *connector;\n\tstruct drm_connector_list_iter iter;\n\tstruct amdgpu_connector *amdgpu_connector = NULL;\n\tu32 tmp = 0, offset;\n\n\tif (!dig || !dig->afmt || !dig->afmt->pin)\n\t\treturn;\n\n\toffset = dig->afmt->pin->offset;\n\n\tdrm_connector_list_iter_begin(dev, &iter);\n\tdrm_for_each_connector_iter(connector, &iter) {\n\t\tif (connector->encoder == encoder) {\n\t\t\tamdgpu_connector = to_amdgpu_connector(connector);\n\t\t\tbreak;\n\t\t}\n\t}\n\tdrm_connector_list_iter_end(&iter);\n\n\tif (!amdgpu_connector) {\n\t\tDRM_ERROR(\"Couldn't find encoder's connector\\n\");\n\t\treturn;\n\t}\n\n\tif (mode->flags & DRM_MODE_FLAG_INTERLACE) {\n\t\tif (connector->latency_present[1])\n\t\t\ttmp =\n\t\t\t(connector->video_latency[1] <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__VIDEO_LIPSYNC__SHIFT) |\n\t\t\t(connector->audio_latency[1] <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__AUDIO_LIPSYNC__SHIFT);\n\t\telse\n\t\t\ttmp =\n\t\t\t(0 <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__VIDEO_LIPSYNC__SHIFT) |\n\t\t\t(0 <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__AUDIO_LIPSYNC__SHIFT);\n\t} else {\n\t\tif (connector->latency_present[0])\n\t\t\ttmp =\n\t\t\t(connector->video_latency[0] <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__VIDEO_LIPSYNC__SHIFT) |\n\t\t\t(connector->audio_latency[0] <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__AUDIO_LIPSYNC__SHIFT);\n\t\telse\n\t\t\ttmp =\n\t\t\t(0 <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__VIDEO_LIPSYNC__SHIFT) |\n\t\t\t(0 <<\n\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC__AUDIO_LIPSYNC__SHIFT);\n\n\t}\n\tWREG32_AUDIO_ENDPT(offset, ixAZALIA_F0_CODEC_PIN_CONTROL_RESPONSE_LIPSYNC, tmp);\n}\n\nstatic void dce_v8_0_audio_write_speaker_allocation(struct drm_encoder *encoder)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tstruct drm_connector *connector;\n\tstruct drm_connector_list_iter iter;\n\tstruct amdgpu_connector *amdgpu_connector = NULL;\n\tu32 offset, tmp;\n\tu8 *sadb = NULL;\n\tint sad_count;\n\n\tif (!dig || !dig->afmt || !dig->afmt->pin)\n\t\treturn;\n\n\toffset = dig->afmt->pin->offset;\n\n\tdrm_connector_list_iter_begin(dev, &iter);\n\tdrm_for_each_connector_iter(connector, &iter) {\n\t\tif (connector->encoder == encoder) {\n\t\t\tamdgpu_connector = to_amdgpu_connector(connector);\n\t\t\tbreak;\n\t\t}\n\t}\n\tdrm_connector_list_iter_end(&iter);\n\n\tif (!amdgpu_connector) {\n\t\tDRM_ERROR(\"Couldn't find encoder's connector\\n\");\n\t\treturn;\n\t}\n\n\tsad_count = drm_edid_to_speaker_allocation(amdgpu_connector_edid(connector), &sadb);\n\tif (sad_count < 0) {\n\t\tDRM_ERROR(\"Couldn't read Speaker Allocation Data Block: %d\\n\", sad_count);\n\t\tsad_count = 0;\n\t}\n\n\t \n\ttmp = RREG32_AUDIO_ENDPT(offset, ixAZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER);\n\ttmp &= ~(AZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER__DP_CONNECTION_MASK |\n\t\tAZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER__SPEAKER_ALLOCATION_MASK);\n\t \n\ttmp |= AZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER__HDMI_CONNECTION_MASK;\n\tif (sad_count)\n\t\ttmp |= (sadb[0] << AZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER__SPEAKER_ALLOCATION__SHIFT);\n\telse\n\t\ttmp |= (5 << AZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER__SPEAKER_ALLOCATION__SHIFT);  \n\tWREG32_AUDIO_ENDPT(offset, ixAZALIA_F0_CODEC_PIN_CONTROL_CHANNEL_SPEAKER, tmp);\n\n\tkfree(sadb);\n}\n\nstatic void dce_v8_0_audio_write_sad_regs(struct drm_encoder *encoder)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tu32 offset;\n\tstruct drm_connector *connector;\n\tstruct drm_connector_list_iter iter;\n\tstruct amdgpu_connector *amdgpu_connector = NULL;\n\tstruct cea_sad *sads;\n\tint i, sad_count;\n\n\tstatic const u16 eld_reg_to_type[][2] = {\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR0, HDMI_AUDIO_CODING_TYPE_PCM },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR1, HDMI_AUDIO_CODING_TYPE_AC3 },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR2, HDMI_AUDIO_CODING_TYPE_MPEG1 },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR3, HDMI_AUDIO_CODING_TYPE_MP3 },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR4, HDMI_AUDIO_CODING_TYPE_MPEG2 },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR5, HDMI_AUDIO_CODING_TYPE_AAC_LC },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR6, HDMI_AUDIO_CODING_TYPE_DTS },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR7, HDMI_AUDIO_CODING_TYPE_ATRAC },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR9, HDMI_AUDIO_CODING_TYPE_EAC3 },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR10, HDMI_AUDIO_CODING_TYPE_DTS_HD },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR11, HDMI_AUDIO_CODING_TYPE_MLP },\n\t\t{ ixAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR13, HDMI_AUDIO_CODING_TYPE_WMA_PRO },\n\t};\n\n\tif (!dig || !dig->afmt || !dig->afmt->pin)\n\t\treturn;\n\n\toffset = dig->afmt->pin->offset;\n\n\tdrm_connector_list_iter_begin(dev, &iter);\n\tdrm_for_each_connector_iter(connector, &iter) {\n\t\tif (connector->encoder == encoder) {\n\t\t\tamdgpu_connector = to_amdgpu_connector(connector);\n\t\t\tbreak;\n\t\t}\n\t}\n\tdrm_connector_list_iter_end(&iter);\n\n\tif (!amdgpu_connector) {\n\t\tDRM_ERROR(\"Couldn't find encoder's connector\\n\");\n\t\treturn;\n\t}\n\n\tsad_count = drm_edid_to_sad(amdgpu_connector_edid(connector), &sads);\n\tif (sad_count < 0)\n\t\tDRM_ERROR(\"Couldn't read SADs: %d\\n\", sad_count);\n\tif (sad_count <= 0)\n\t\treturn;\n\tBUG_ON(!sads);\n\n\tfor (i = 0; i < ARRAY_SIZE(eld_reg_to_type); i++) {\n\t\tu32 value = 0;\n\t\tu8 stereo_freqs = 0;\n\t\tint max_channels = -1;\n\t\tint j;\n\n\t\tfor (j = 0; j < sad_count; j++) {\n\t\t\tstruct cea_sad *sad = &sads[j];\n\n\t\t\tif (sad->format == eld_reg_to_type[i][1]) {\n\t\t\t\tif (sad->channels > max_channels) {\n\t\t\t\t\tvalue = (sad->channels <<\n\t\t\t\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR0__MAX_CHANNELS__SHIFT) |\n\t\t\t\t\t\t(sad->byte2 <<\n\t\t\t\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR0__DESCRIPTOR_BYTE_2__SHIFT) |\n\t\t\t\t\t\t(sad->freq <<\n\t\t\t\t\t\t AZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR0__SUPPORTED_FREQUENCIES__SHIFT);\n\t\t\t\t\tmax_channels = sad->channels;\n\t\t\t\t}\n\n\t\t\t\tif (sad->format == HDMI_AUDIO_CODING_TYPE_PCM)\n\t\t\t\t\tstereo_freqs |= sad->freq;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tvalue |= (stereo_freqs <<\n\t\t\tAZALIA_F0_CODEC_PIN_CONTROL_AUDIO_DESCRIPTOR0__SUPPORTED_FREQUENCIES_STEREO__SHIFT);\n\n\t\tWREG32_AUDIO_ENDPT(offset, eld_reg_to_type[i][0], value);\n\t}\n\n\tkfree(sads);\n}\n\nstatic void dce_v8_0_audio_enable(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_audio_pin *pin,\n\t\t\t\t  bool enable)\n{\n\tif (!pin)\n\t\treturn;\n\n\tWREG32_AUDIO_ENDPT(pin->offset, ixAZALIA_F0_CODEC_PIN_CONTROL_HOT_PLUG_CONTROL,\n\t\tenable ? AZALIA_F0_CODEC_PIN_CONTROL_HOT_PLUG_CONTROL__AUDIO_ENABLED_MASK : 0);\n}\n\nstatic const u32 pin_offsets[7] = {\n\t(0x1780 - 0x1780),\n\t(0x1786 - 0x1780),\n\t(0x178c - 0x1780),\n\t(0x1792 - 0x1780),\n\t(0x1798 - 0x1780),\n\t(0x179d - 0x1780),\n\t(0x17a4 - 0x1780),\n};\n\nstatic int dce_v8_0_audio_init(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (!amdgpu_audio)\n\t\treturn 0;\n\n\tadev->mode_info.audio.enabled = true;\n\n\tif (adev->asic_type == CHIP_KAVERI)  \n\t\tadev->mode_info.audio.num_pins = 7;\n\telse if ((adev->asic_type == CHIP_KABINI) ||\n\t\t (adev->asic_type == CHIP_MULLINS))  \n\t\tadev->mode_info.audio.num_pins = 3;\n\telse if ((adev->asic_type == CHIP_BONAIRE) ||\n\t\t (adev->asic_type == CHIP_HAWAII)) \n\t\tadev->mode_info.audio.num_pins = 7;\n\telse\n\t\tadev->mode_info.audio.num_pins = 3;\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++) {\n\t\tadev->mode_info.audio.pin[i].channels = -1;\n\t\tadev->mode_info.audio.pin[i].rate = -1;\n\t\tadev->mode_info.audio.pin[i].bits_per_sample = -1;\n\t\tadev->mode_info.audio.pin[i].status_bits = 0;\n\t\tadev->mode_info.audio.pin[i].category_code = 0;\n\t\tadev->mode_info.audio.pin[i].connected = false;\n\t\tadev->mode_info.audio.pin[i].offset = pin_offsets[i];\n\t\tadev->mode_info.audio.pin[i].id = i;\n\t\t \n\t\t \n\t\tdce_v8_0_audio_enable(adev, &adev->mode_info.audio.pin[i], false);\n\t}\n\n\treturn 0;\n}\n\nstatic void dce_v8_0_audio_fini(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (!amdgpu_audio)\n\t\treturn;\n\n\tif (!adev->mode_info.audio.enabled)\n\t\treturn;\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++)\n\t\tdce_v8_0_audio_enable(adev, &adev->mode_info.audio.pin[i], false);\n\n\tadev->mode_info.audio.enabled = false;\n}\n\n \nstatic void dce_v8_0_afmt_update_ACR(struct drm_encoder *encoder, uint32_t clock)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_afmt_acr acr = amdgpu_afmt_acr(clock);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tuint32_t offset = dig->afmt->offset;\n\n\tWREG32(mmHDMI_ACR_32_0 + offset, (acr.cts_32khz << HDMI_ACR_32_0__HDMI_ACR_CTS_32__SHIFT));\n\tWREG32(mmHDMI_ACR_32_1 + offset, acr.n_32khz);\n\n\tWREG32(mmHDMI_ACR_44_0 + offset, (acr.cts_44_1khz << HDMI_ACR_44_0__HDMI_ACR_CTS_44__SHIFT));\n\tWREG32(mmHDMI_ACR_44_1 + offset, acr.n_44_1khz);\n\n\tWREG32(mmHDMI_ACR_48_0 + offset, (acr.cts_48khz << HDMI_ACR_48_0__HDMI_ACR_CTS_48__SHIFT));\n\tWREG32(mmHDMI_ACR_48_1 + offset, acr.n_48khz);\n}\n\n \nstatic void dce_v8_0_afmt_update_avi_infoframe(struct drm_encoder *encoder,\n\t\t\t\t\t       void *buffer, size_t size)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tuint32_t offset = dig->afmt->offset;\n\tuint8_t *frame = buffer + 3;\n\tuint8_t *header = buffer;\n\n\tWREG32(mmAFMT_AVI_INFO0 + offset,\n\t\tframe[0x0] | (frame[0x1] << 8) | (frame[0x2] << 16) | (frame[0x3] << 24));\n\tWREG32(mmAFMT_AVI_INFO1 + offset,\n\t\tframe[0x4] | (frame[0x5] << 8) | (frame[0x6] << 16) | (frame[0x7] << 24));\n\tWREG32(mmAFMT_AVI_INFO2 + offset,\n\t\tframe[0x8] | (frame[0x9] << 8) | (frame[0xA] << 16) | (frame[0xB] << 24));\n\tWREG32(mmAFMT_AVI_INFO3 + offset,\n\t\tframe[0xC] | (frame[0xD] << 8) | (header[1] << 24));\n}\n\nstatic void dce_v8_0_audio_set_dto(struct drm_encoder *encoder, u32 clock)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(encoder->crtc);\n\tu32 dto_phase = 24 * 1000;\n\tu32 dto_modulo = clock;\n\n\tif (!dig || !dig->afmt)\n\t\treturn;\n\n\t \n\t \n\tWREG32(mmDCCG_AUDIO_DTO_SOURCE, (amdgpu_crtc->crtc_id << DCCG_AUDIO_DTO_SOURCE__DCCG_AUDIO_DTO0_SOURCE_SEL__SHIFT));\n\tWREG32(mmDCCG_AUDIO_DTO0_PHASE, dto_phase);\n\tWREG32(mmDCCG_AUDIO_DTO0_MODULE, dto_modulo);\n}\n\n \nstatic void dce_v8_0_afmt_setmode(struct drm_encoder *encoder,\n\t\t\t\t  struct drm_display_mode *mode)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\tstruct drm_connector *connector = amdgpu_get_connector_for_encoder(encoder);\n\tu8 buffer[HDMI_INFOFRAME_HEADER_SIZE + HDMI_AVI_INFOFRAME_SIZE];\n\tstruct hdmi_avi_infoframe frame;\n\tuint32_t offset, val;\n\tssize_t err;\n\tint bpc = 8;\n\n\tif (!dig || !dig->afmt)\n\t\treturn;\n\n\t \n\tif (!dig->afmt->enabled)\n\t\treturn;\n\n\toffset = dig->afmt->offset;\n\n\t \n\tif (encoder->crtc) {\n\t\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(encoder->crtc);\n\t\tbpc = amdgpu_crtc->bpc;\n\t}\n\n\t \n\tdig->afmt->pin = dce_v8_0_audio_get_pin(adev);\n\tdce_v8_0_audio_enable(adev, dig->afmt->pin, false);\n\n\tdce_v8_0_audio_set_dto(encoder, mode->clock);\n\n\tWREG32(mmHDMI_VBI_PACKET_CONTROL + offset,\n\t       HDMI_VBI_PACKET_CONTROL__HDMI_NULL_SEND_MASK);  \n\n\tWREG32(mmAFMT_AUDIO_CRC_CONTROL + offset, 0x1000);\n\n\tval = RREG32(mmHDMI_CONTROL + offset);\n\tval &= ~HDMI_CONTROL__HDMI_DEEP_COLOR_ENABLE_MASK;\n\tval &= ~HDMI_CONTROL__HDMI_DEEP_COLOR_DEPTH_MASK;\n\n\tswitch (bpc) {\n\tcase 0:\n\tcase 6:\n\tcase 8:\n\tcase 16:\n\tdefault:\n\t\tDRM_DEBUG(\"%s: Disabling hdmi deep color for %d bpc.\\n\",\n\t\t\t  connector->name, bpc);\n\t\tbreak;\n\tcase 10:\n\t\tval |= HDMI_CONTROL__HDMI_DEEP_COLOR_ENABLE_MASK;\n\t\tval |= 1 << HDMI_CONTROL__HDMI_DEEP_COLOR_DEPTH__SHIFT;\n\t\tDRM_DEBUG(\"%s: Enabling hdmi deep color 30 for 10 bpc.\\n\",\n\t\t\t  connector->name);\n\t\tbreak;\n\tcase 12:\n\t\tval |= HDMI_CONTROL__HDMI_DEEP_COLOR_ENABLE_MASK;\n\t\tval |= 2 << HDMI_CONTROL__HDMI_DEEP_COLOR_DEPTH__SHIFT;\n\t\tDRM_DEBUG(\"%s: Enabling hdmi deep color 36 for 12 bpc.\\n\",\n\t\t\t  connector->name);\n\t\tbreak;\n\t}\n\n\tWREG32(mmHDMI_CONTROL + offset, val);\n\n\tWREG32(mmHDMI_VBI_PACKET_CONTROL + offset,\n\t       HDMI_VBI_PACKET_CONTROL__HDMI_NULL_SEND_MASK |  \n\t       HDMI_VBI_PACKET_CONTROL__HDMI_GC_SEND_MASK |  \n\t       HDMI_VBI_PACKET_CONTROL__HDMI_GC_CONT_MASK);  \n\n\tWREG32(mmHDMI_INFOFRAME_CONTROL0 + offset,\n\t       HDMI_INFOFRAME_CONTROL0__HDMI_AUDIO_INFO_SEND_MASK |  \n\t       HDMI_INFOFRAME_CONTROL0__HDMI_AUDIO_INFO_CONT_MASK);  \n\n\tWREG32(mmAFMT_INFOFRAME_CONTROL0 + offset,\n\t       AFMT_INFOFRAME_CONTROL0__AFMT_AUDIO_INFO_UPDATE_MASK);  \n\n\tWREG32(mmHDMI_INFOFRAME_CONTROL1 + offset,\n\t       (2 << HDMI_INFOFRAME_CONTROL1__HDMI_AUDIO_INFO_LINE__SHIFT));  \n\n\tWREG32(mmHDMI_GC + offset, 0);  \n\n\tWREG32(mmHDMI_AUDIO_PACKET_CONTROL + offset,\n\t       (1 << HDMI_AUDIO_PACKET_CONTROL__HDMI_AUDIO_DELAY_EN__SHIFT) |  \n\t       (3 << HDMI_AUDIO_PACKET_CONTROL__HDMI_AUDIO_PACKETS_PER_LINE__SHIFT));  \n\n\tWREG32(mmAFMT_AUDIO_PACKET_CONTROL + offset,\n\t       AFMT_AUDIO_PACKET_CONTROL__AFMT_60958_CS_UPDATE_MASK);  \n\n\t \n\n\tif (bpc > 8)\n\t\tWREG32(mmHDMI_ACR_PACKET_CONTROL + offset,\n\t\t       HDMI_ACR_PACKET_CONTROL__HDMI_ACR_AUTO_SEND_MASK);  \n\telse\n\t\tWREG32(mmHDMI_ACR_PACKET_CONTROL + offset,\n\t\t       HDMI_ACR_PACKET_CONTROL__HDMI_ACR_SOURCE_MASK |  \n\t\t       HDMI_ACR_PACKET_CONTROL__HDMI_ACR_AUTO_SEND_MASK);  \n\n\tdce_v8_0_afmt_update_ACR(encoder, mode->clock);\n\n\tWREG32(mmAFMT_60958_0 + offset,\n\t       (1 << AFMT_60958_0__AFMT_60958_CS_CHANNEL_NUMBER_L__SHIFT));\n\n\tWREG32(mmAFMT_60958_1 + offset,\n\t       (2 << AFMT_60958_1__AFMT_60958_CS_CHANNEL_NUMBER_R__SHIFT));\n\n\tWREG32(mmAFMT_60958_2 + offset,\n\t       (3 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_2__SHIFT) |\n\t       (4 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_3__SHIFT) |\n\t       (5 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_4__SHIFT) |\n\t       (6 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_5__SHIFT) |\n\t       (7 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_6__SHIFT) |\n\t       (8 << AFMT_60958_2__AFMT_60958_CS_CHANNEL_NUMBER_7__SHIFT));\n\n\tdce_v8_0_audio_write_speaker_allocation(encoder);\n\n\n\tWREG32(mmAFMT_AUDIO_PACKET_CONTROL2 + offset,\n\t       (0xff << AFMT_AUDIO_PACKET_CONTROL2__AFMT_AUDIO_CHANNEL_ENABLE__SHIFT));\n\n\tdce_v8_0_afmt_audio_select_pin(encoder);\n\tdce_v8_0_audio_write_sad_regs(encoder);\n\tdce_v8_0_audio_write_latency_fields(encoder, mode);\n\n\terr = drm_hdmi_avi_infoframe_from_display_mode(&frame, connector, mode);\n\tif (err < 0) {\n\t\tDRM_ERROR(\"failed to setup AVI infoframe: %zd\\n\", err);\n\t\treturn;\n\t}\n\n\terr = hdmi_avi_infoframe_pack(&frame, buffer, sizeof(buffer));\n\tif (err < 0) {\n\t\tDRM_ERROR(\"failed to pack AVI infoframe: %zd\\n\", err);\n\t\treturn;\n\t}\n\n\tdce_v8_0_afmt_update_avi_infoframe(encoder, buffer, sizeof(buffer));\n\n\tWREG32_OR(mmHDMI_INFOFRAME_CONTROL0 + offset,\n\t\t  HDMI_INFOFRAME_CONTROL0__HDMI_AVI_INFO_SEND_MASK |  \n\t\t  HDMI_INFOFRAME_CONTROL0__HDMI_AVI_INFO_CONT_MASK);  \n\n\tWREG32_P(mmHDMI_INFOFRAME_CONTROL1 + offset,\n\t\t (2 << HDMI_INFOFRAME_CONTROL1__HDMI_AVI_INFO_LINE__SHIFT),  \n\t\t ~HDMI_INFOFRAME_CONTROL1__HDMI_AVI_INFO_LINE_MASK);\n\n\tWREG32_OR(mmAFMT_AUDIO_PACKET_CONTROL + offset,\n\t\t  AFMT_AUDIO_PACKET_CONTROL__AFMT_AUDIO_SAMPLE_SEND_MASK);  \n\n\tWREG32(mmAFMT_RAMP_CONTROL0 + offset, 0x00FFFFFF);\n\tWREG32(mmAFMT_RAMP_CONTROL1 + offset, 0x007FFFFF);\n\tWREG32(mmAFMT_RAMP_CONTROL2 + offset, 0x00000001);\n\tWREG32(mmAFMT_RAMP_CONTROL3 + offset, 0x00000001);\n\n\t \n\tdce_v8_0_audio_enable(adev, dig->afmt->pin, true);\n}\n\nstatic void dce_v8_0_afmt_enable(struct drm_encoder *encoder, bool enable)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\n\tif (!dig || !dig->afmt)\n\t\treturn;\n\n\t \n\tif (enable && dig->afmt->enabled)\n\t\treturn;\n\tif (!enable && !dig->afmt->enabled)\n\t\treturn;\n\n\tif (!enable && dig->afmt->pin) {\n\t\tdce_v8_0_audio_enable(adev, dig->afmt->pin, false);\n\t\tdig->afmt->pin = NULL;\n\t}\n\n\tdig->afmt->enabled = enable;\n\n\tDRM_DEBUG(\"%sabling AFMT interface @ 0x%04X for encoder 0x%x\\n\",\n\t\t  enable ? \"En\" : \"Dis\", dig->afmt->offset, amdgpu_encoder->encoder_id);\n}\n\nstatic int dce_v8_0_afmt_init(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->mode_info.num_dig; i++)\n\t\tadev->mode_info.afmt[i] = NULL;\n\n\t \n\tfor (i = 0; i < adev->mode_info.num_dig; i++) {\n\t\tadev->mode_info.afmt[i] = kzalloc(sizeof(struct amdgpu_afmt), GFP_KERNEL);\n\t\tif (adev->mode_info.afmt[i]) {\n\t\t\tadev->mode_info.afmt[i]->offset = dig_offsets[i];\n\t\t\tadev->mode_info.afmt[i]->id = i;\n\t\t} else {\n\t\t\tint j;\n\t\t\tfor (j = 0; j < i; j++) {\n\t\t\t\tkfree(adev->mode_info.afmt[j]);\n\t\t\t\tadev->mode_info.afmt[j] = NULL;\n\t\t\t}\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void dce_v8_0_afmt_fini(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->mode_info.num_dig; i++) {\n\t\tkfree(adev->mode_info.afmt[i]);\n\t\tadev->mode_info.afmt[i] = NULL;\n\t}\n}\n\nstatic const u32 vga_control_regs[6] = {\n\tmmD1VGA_CONTROL,\n\tmmD2VGA_CONTROL,\n\tmmD3VGA_CONTROL,\n\tmmD4VGA_CONTROL,\n\tmmD5VGA_CONTROL,\n\tmmD6VGA_CONTROL,\n};\n\nstatic void dce_v8_0_vga_enable(struct drm_crtc *crtc, bool enable)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tu32 vga_control;\n\n\tvga_control = RREG32(vga_control_regs[amdgpu_crtc->crtc_id]) & ~1;\n\tif (enable)\n\t\tWREG32(vga_control_regs[amdgpu_crtc->crtc_id], vga_control | 1);\n\telse\n\t\tWREG32(vga_control_regs[amdgpu_crtc->crtc_id], vga_control);\n}\n\nstatic void dce_v8_0_grph_enable(struct drm_crtc *crtc, bool enable)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\tif (enable)\n\t\tWREG32(mmGRPH_ENABLE + amdgpu_crtc->crtc_offset, 1);\n\telse\n\t\tWREG32(mmGRPH_ENABLE + amdgpu_crtc->crtc_offset, 0);\n}\n\nstatic int dce_v8_0_crtc_do_set_base(struct drm_crtc *crtc,\n\t\t\t\t     struct drm_framebuffer *fb,\n\t\t\t\t     int x, int y, int atomic)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct drm_framebuffer *target_fb;\n\tstruct drm_gem_object *obj;\n\tstruct amdgpu_bo *abo;\n\tuint64_t fb_location, tiling_flags;\n\tuint32_t fb_format, fb_pitch_pixels;\n\tu32 fb_swap = (GRPH_ENDIAN_NONE << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n\tu32 pipe_config;\n\tu32 viewport_w, viewport_h;\n\tint r;\n\tbool bypass_lut = false;\n\n\t \n\tif (!atomic && !crtc->primary->fb) {\n\t\tDRM_DEBUG_KMS(\"No FB bound\\n\");\n\t\treturn 0;\n\t}\n\n\tif (atomic)\n\t\ttarget_fb = fb;\n\telse\n\t\ttarget_fb = crtc->primary->fb;\n\n\t \n\tobj = target_fb->obj[0];\n\tabo = gem_to_amdgpu_bo(obj);\n\tr = amdgpu_bo_reserve(abo, false);\n\tif (unlikely(r != 0))\n\t\treturn r;\n\n\tif (!atomic) {\n\t\tr = amdgpu_bo_pin(abo, AMDGPU_GEM_DOMAIN_VRAM);\n\t\tif (unlikely(r != 0)) {\n\t\t\tamdgpu_bo_unreserve(abo);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\tfb_location = amdgpu_bo_gpu_offset(abo);\n\n\tamdgpu_bo_get_tiling_flags(abo, &tiling_flags);\n\tamdgpu_bo_unreserve(abo);\n\n\tpipe_config = AMDGPU_TILING_GET(tiling_flags, PIPE_CONFIG);\n\n\tswitch (target_fb->format->format) {\n\tcase DRM_FORMAT_C8:\n\t\tfb_format = ((GRPH_DEPTH_8BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_INDEXED << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n\t\tbreak;\n\tcase DRM_FORMAT_XRGB4444:\n\tcase DRM_FORMAT_ARGB4444:\n\t\tfb_format = ((GRPH_DEPTH_16BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_ARGB4444 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN16 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tcase DRM_FORMAT_XRGB1555:\n\tcase DRM_FORMAT_ARGB1555:\n\t\tfb_format = ((GRPH_DEPTH_16BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_ARGB1555 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN16 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tcase DRM_FORMAT_BGRX5551:\n\tcase DRM_FORMAT_BGRA5551:\n\t\tfb_format = ((GRPH_DEPTH_16BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_BGRA5551 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN16 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tcase DRM_FORMAT_RGB565:\n\t\tfb_format = ((GRPH_DEPTH_16BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_ARGB565 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN16 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tcase DRM_FORMAT_XRGB8888:\n\tcase DRM_FORMAT_ARGB8888:\n\t\tfb_format = ((GRPH_DEPTH_32BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_ARGB8888 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN32 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tcase DRM_FORMAT_XRGB2101010:\n\tcase DRM_FORMAT_ARGB2101010:\n\t\tfb_format = ((GRPH_DEPTH_32BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_ARGB2101010 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN32 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\t \n\t\tbypass_lut = true;\n\t\tbreak;\n\tcase DRM_FORMAT_BGRX1010102:\n\tcase DRM_FORMAT_BGRA1010102:\n\t\tfb_format = ((GRPH_DEPTH_32BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t     (GRPH_FORMAT_BGRA1010102 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap = (GRPH_ENDIAN_8IN32 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\t \n\t\tbypass_lut = true;\n\t\tbreak;\n\tcase DRM_FORMAT_XBGR8888:\n\tcase DRM_FORMAT_ABGR8888:\n\t\tfb_format = ((GRPH_DEPTH_32BPP << GRPH_CONTROL__GRPH_DEPTH__SHIFT) |\n\t\t\t\t(GRPH_FORMAT_ARGB8888 << GRPH_CONTROL__GRPH_FORMAT__SHIFT));\n\t\tfb_swap = ((GRPH_RED_SEL_B << GRPH_SWAP_CNTL__GRPH_RED_CROSSBAR__SHIFT) |\n\t\t\t(GRPH_BLUE_SEL_R << GRPH_SWAP_CNTL__GRPH_BLUE_CROSSBAR__SHIFT));\n#ifdef __BIG_ENDIAN\n\t\tfb_swap |= (GRPH_ENDIAN_8IN32 << GRPH_SWAP_CNTL__GRPH_ENDIAN_SWAP__SHIFT);\n#endif\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unsupported screen format %p4cc\\n\",\n\t\t\t  &target_fb->format->format);\n\t\treturn -EINVAL;\n\t}\n\n\tif (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE) == ARRAY_2D_TILED_THIN1) {\n\t\tunsigned bankw, bankh, mtaspect, tile_split, num_banks;\n\n\t\tbankw = AMDGPU_TILING_GET(tiling_flags, BANK_WIDTH);\n\t\tbankh = AMDGPU_TILING_GET(tiling_flags, BANK_HEIGHT);\n\t\tmtaspect = AMDGPU_TILING_GET(tiling_flags, MACRO_TILE_ASPECT);\n\t\ttile_split = AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT);\n\t\tnum_banks = AMDGPU_TILING_GET(tiling_flags, NUM_BANKS);\n\n\t\tfb_format |= (num_banks << GRPH_CONTROL__GRPH_NUM_BANKS__SHIFT);\n\t\tfb_format |= (GRPH_ARRAY_2D_TILED_THIN1 << GRPH_CONTROL__GRPH_ARRAY_MODE__SHIFT);\n\t\tfb_format |= (tile_split << GRPH_CONTROL__GRPH_TILE_SPLIT__SHIFT);\n\t\tfb_format |= (bankw << GRPH_CONTROL__GRPH_BANK_WIDTH__SHIFT);\n\t\tfb_format |= (bankh << GRPH_CONTROL__GRPH_BANK_HEIGHT__SHIFT);\n\t\tfb_format |= (mtaspect << GRPH_CONTROL__GRPH_MACRO_TILE_ASPECT__SHIFT);\n\t\tfb_format |= (DISPLAY_MICRO_TILING << GRPH_CONTROL__GRPH_MICRO_TILE_MODE__SHIFT);\n\t} else if (AMDGPU_TILING_GET(tiling_flags, ARRAY_MODE) == ARRAY_1D_TILED_THIN1) {\n\t\tfb_format |= (GRPH_ARRAY_1D_TILED_THIN1 << GRPH_CONTROL__GRPH_ARRAY_MODE__SHIFT);\n\t}\n\n\tfb_format |= (pipe_config << GRPH_CONTROL__GRPH_PIPE_CONFIG__SHIFT);\n\n\tdce_v8_0_vga_enable(crtc, false);\n\n\t \n\tWREG32(mmGRPH_FLIP_CONTROL + amdgpu_crtc->crtc_offset, 0);\n\n\tWREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS_HIGH + amdgpu_crtc->crtc_offset,\n\t       upper_32_bits(fb_location));\n\tWREG32(mmGRPH_SECONDARY_SURFACE_ADDRESS_HIGH + amdgpu_crtc->crtc_offset,\n\t       upper_32_bits(fb_location));\n\tWREG32(mmGRPH_PRIMARY_SURFACE_ADDRESS + amdgpu_crtc->crtc_offset,\n\t       (u32)fb_location & GRPH_PRIMARY_SURFACE_ADDRESS__GRPH_PRIMARY_SURFACE_ADDRESS_MASK);\n\tWREG32(mmGRPH_SECONDARY_SURFACE_ADDRESS + amdgpu_crtc->crtc_offset,\n\t       (u32) fb_location & GRPH_SECONDARY_SURFACE_ADDRESS__GRPH_SECONDARY_SURFACE_ADDRESS_MASK);\n\tWREG32(mmGRPH_CONTROL + amdgpu_crtc->crtc_offset, fb_format);\n\tWREG32(mmGRPH_SWAP_CNTL + amdgpu_crtc->crtc_offset, fb_swap);\n\n\t \n\tWREG32_P(mmGRPH_LUT_10BIT_BYPASS_CONTROL + amdgpu_crtc->crtc_offset,\n\t\t (bypass_lut ? LUT_10BIT_BYPASS_EN : 0),\n\t\t ~LUT_10BIT_BYPASS_EN);\n\n\tif (bypass_lut)\n\t\tDRM_DEBUG_KMS(\"Bypassing hardware LUT due to 10 bit fb scanout.\\n\");\n\n\tWREG32(mmGRPH_SURFACE_OFFSET_X + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmGRPH_SURFACE_OFFSET_Y + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmGRPH_X_START + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmGRPH_Y_START + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmGRPH_X_END + amdgpu_crtc->crtc_offset, target_fb->width);\n\tWREG32(mmGRPH_Y_END + amdgpu_crtc->crtc_offset, target_fb->height);\n\n\tfb_pitch_pixels = target_fb->pitches[0] / target_fb->format->cpp[0];\n\tWREG32(mmGRPH_PITCH + amdgpu_crtc->crtc_offset, fb_pitch_pixels);\n\n\tdce_v8_0_grph_enable(crtc, true);\n\n\tWREG32(mmLB_DESKTOP_HEIGHT + amdgpu_crtc->crtc_offset,\n\t       target_fb->height);\n\n\tx &= ~3;\n\ty &= ~1;\n\tWREG32(mmVIEWPORT_START + amdgpu_crtc->crtc_offset,\n\t       (x << 16) | y);\n\tviewport_w = crtc->mode.hdisplay;\n\tviewport_h = (crtc->mode.vdisplay + 1) & ~1;\n\tWREG32(mmVIEWPORT_SIZE + amdgpu_crtc->crtc_offset,\n\t       (viewport_w << 16) | viewport_h);\n\n\t \n\tWREG32(mmMASTER_UPDATE_MODE + amdgpu_crtc->crtc_offset, 0);\n\n\tif (!atomic && fb && fb != crtc->primary->fb) {\n\t\tabo = gem_to_amdgpu_bo(fb->obj[0]);\n\t\tr = amdgpu_bo_reserve(abo, true);\n\t\tif (unlikely(r != 0))\n\t\t\treturn r;\n\t\tamdgpu_bo_unpin(abo);\n\t\tamdgpu_bo_unreserve(abo);\n\t}\n\n\t \n\tdce_v8_0_bandwidth_update(adev);\n\n\treturn 0;\n}\n\nstatic void dce_v8_0_set_interleave(struct drm_crtc *crtc,\n\t\t\t\t    struct drm_display_mode *mode)\n{\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\n\tif (mode->flags & DRM_MODE_FLAG_INTERLACE)\n\t\tWREG32(mmLB_DATA_FORMAT + amdgpu_crtc->crtc_offset,\n\t\t       LB_DATA_FORMAT__INTERLEAVE_EN__SHIFT);\n\telse\n\t\tWREG32(mmLB_DATA_FORMAT + amdgpu_crtc->crtc_offset, 0);\n}\n\nstatic void dce_v8_0_crtc_load_lut(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tu16 *r, *g, *b;\n\tint i;\n\n\tDRM_DEBUG_KMS(\"%d\\n\", amdgpu_crtc->crtc_id);\n\n\tWREG32(mmINPUT_CSC_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((INPUT_CSC_BYPASS << INPUT_CSC_CONTROL__INPUT_CSC_GRPH_MODE__SHIFT) |\n\t\t(INPUT_CSC_BYPASS << INPUT_CSC_CONTROL__INPUT_CSC_OVL_MODE__SHIFT)));\n\tWREG32(mmPRESCALE_GRPH_CONTROL + amdgpu_crtc->crtc_offset,\n\t       PRESCALE_GRPH_CONTROL__GRPH_PRESCALE_BYPASS_MASK);\n\tWREG32(mmPRESCALE_OVL_CONTROL + amdgpu_crtc->crtc_offset,\n\t       PRESCALE_OVL_CONTROL__OVL_PRESCALE_BYPASS_MASK);\n\tWREG32(mmINPUT_GAMMA_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((INPUT_GAMMA_USE_LUT << INPUT_GAMMA_CONTROL__GRPH_INPUT_GAMMA_MODE__SHIFT) |\n\t\t(INPUT_GAMMA_USE_LUT << INPUT_GAMMA_CONTROL__OVL_INPUT_GAMMA_MODE__SHIFT)));\n\n\tWREG32(mmDC_LUT_CONTROL + amdgpu_crtc->crtc_offset, 0);\n\n\tWREG32(mmDC_LUT_BLACK_OFFSET_BLUE + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmDC_LUT_BLACK_OFFSET_GREEN + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmDC_LUT_BLACK_OFFSET_RED + amdgpu_crtc->crtc_offset, 0);\n\n\tWREG32(mmDC_LUT_WHITE_OFFSET_BLUE + amdgpu_crtc->crtc_offset, 0xffff);\n\tWREG32(mmDC_LUT_WHITE_OFFSET_GREEN + amdgpu_crtc->crtc_offset, 0xffff);\n\tWREG32(mmDC_LUT_WHITE_OFFSET_RED + amdgpu_crtc->crtc_offset, 0xffff);\n\n\tWREG32(mmDC_LUT_RW_MODE + amdgpu_crtc->crtc_offset, 0);\n\tWREG32(mmDC_LUT_WRITE_EN_MASK + amdgpu_crtc->crtc_offset, 0x00000007);\n\n\tWREG32(mmDC_LUT_RW_INDEX + amdgpu_crtc->crtc_offset, 0);\n\tr = crtc->gamma_store;\n\tg = r + crtc->gamma_size;\n\tb = g + crtc->gamma_size;\n\tfor (i = 0; i < 256; i++) {\n\t\tWREG32(mmDC_LUT_30_COLOR + amdgpu_crtc->crtc_offset,\n\t\t       ((*r++ & 0xffc0) << 14) |\n\t\t       ((*g++ & 0xffc0) << 4) |\n\t\t       (*b++ >> 6));\n\t}\n\n\tWREG32(mmDEGAMMA_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((DEGAMMA_BYPASS << DEGAMMA_CONTROL__GRPH_DEGAMMA_MODE__SHIFT) |\n\t\t(DEGAMMA_BYPASS << DEGAMMA_CONTROL__OVL_DEGAMMA_MODE__SHIFT) |\n\t\t(DEGAMMA_BYPASS << DEGAMMA_CONTROL__CURSOR_DEGAMMA_MODE__SHIFT)));\n\tWREG32(mmGAMUT_REMAP_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((GAMUT_REMAP_BYPASS << GAMUT_REMAP_CONTROL__GRPH_GAMUT_REMAP_MODE__SHIFT) |\n\t\t(GAMUT_REMAP_BYPASS << GAMUT_REMAP_CONTROL__OVL_GAMUT_REMAP_MODE__SHIFT)));\n\tWREG32(mmREGAMMA_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((REGAMMA_BYPASS << REGAMMA_CONTROL__GRPH_REGAMMA_MODE__SHIFT) |\n\t\t(REGAMMA_BYPASS << REGAMMA_CONTROL__OVL_REGAMMA_MODE__SHIFT)));\n\tWREG32(mmOUTPUT_CSC_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ((OUTPUT_CSC_BYPASS << OUTPUT_CSC_CONTROL__OUTPUT_CSC_GRPH_MODE__SHIFT) |\n\t\t(OUTPUT_CSC_BYPASS << OUTPUT_CSC_CONTROL__OUTPUT_CSC_OVL_MODE__SHIFT)));\n\t \n\tWREG32(0x1a50 + amdgpu_crtc->crtc_offset, 0);\n\t \n\tWREG32(mmALPHA_CONTROL + amdgpu_crtc->crtc_offset,\n\t       ALPHA_CONTROL__CURSOR_ALPHA_BLND_ENA_MASK);\n}\n\nstatic int dce_v8_0_pick_dig_encoder(struct drm_encoder *encoder)\n{\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\n\tswitch (amdgpu_encoder->encoder_id) {\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY:\n\t\tif (dig->linkb)\n\t\t\treturn 1;\n\t\telse\n\t\t\treturn 0;\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY1:\n\t\tif (dig->linkb)\n\t\t\treturn 3;\n\t\telse\n\t\t\treturn 2;\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY2:\n\t\tif (dig->linkb)\n\t\t\treturn 5;\n\t\telse\n\t\t\treturn 4;\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY3:\n\t\treturn 6;\n\tdefault:\n\t\tDRM_ERROR(\"invalid encoder_id: 0x%x\\n\", amdgpu_encoder->encoder_id);\n\t\treturn 0;\n\t}\n}\n\n \nstatic u32 dce_v8_0_pick_pll(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tu32 pll_in_use;\n\tint pll;\n\n\tif (ENCODER_MODE_IS_DP(amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder))) {\n\t\tif (adev->clock.dp_extclk)\n\t\t\t \n\t\t\treturn ATOM_PPLL_INVALID;\n\t\telse {\n\t\t\t \n\t\t\tpll = amdgpu_pll_get_shared_dp_ppll(crtc);\n\t\t\tif (pll != ATOM_PPLL_INVALID)\n\t\t\t\treturn pll;\n\t\t}\n\t} else {\n\t\t \n\t\tpll = amdgpu_pll_get_shared_nondp_ppll(crtc);\n\t\tif (pll != ATOM_PPLL_INVALID)\n\t\t\treturn pll;\n\t}\n\t \n\tif ((adev->asic_type == CHIP_KABINI) ||\n\t    (adev->asic_type == CHIP_MULLINS)) {\n\t\t \n\t\tpll_in_use = amdgpu_pll_get_use_mask(crtc);\n\t\tif (!(pll_in_use & (1 << ATOM_PPLL2)))\n\t\t\treturn ATOM_PPLL2;\n\t\tif (!(pll_in_use & (1 << ATOM_PPLL1)))\n\t\t\treturn ATOM_PPLL1;\n\t\tDRM_ERROR(\"unable to allocate a PPLL\\n\");\n\t\treturn ATOM_PPLL_INVALID;\n\t} else {\n\t\t \n\t\tpll_in_use = amdgpu_pll_get_use_mask(crtc);\n\t\tif (!(pll_in_use & (1 << ATOM_PPLL2)))\n\t\t\treturn ATOM_PPLL2;\n\t\tif (!(pll_in_use & (1 << ATOM_PPLL1)))\n\t\t\treturn ATOM_PPLL1;\n\t\tif (!(pll_in_use & (1 << ATOM_PPLL0)))\n\t\t\treturn ATOM_PPLL0;\n\t\tDRM_ERROR(\"unable to allocate a PPLL\\n\");\n\t\treturn ATOM_PPLL_INVALID;\n\t}\n\treturn ATOM_PPLL_INVALID;\n}\n\nstatic void dce_v8_0_lock_cursor(struct drm_crtc *crtc, bool lock)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(crtc->dev);\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tuint32_t cur_lock;\n\n\tcur_lock = RREG32(mmCUR_UPDATE + amdgpu_crtc->crtc_offset);\n\tif (lock)\n\t\tcur_lock |= CUR_UPDATE__CURSOR_UPDATE_LOCK_MASK;\n\telse\n\t\tcur_lock &= ~CUR_UPDATE__CURSOR_UPDATE_LOCK_MASK;\n\tWREG32(mmCUR_UPDATE + amdgpu_crtc->crtc_offset, cur_lock);\n}\n\nstatic void dce_v8_0_hide_cursor(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct amdgpu_device *adev = drm_to_adev(crtc->dev);\n\n\tWREG32(mmCUR_CONTROL + amdgpu_crtc->crtc_offset,\n\t       (CURSOR_24_8_PRE_MULT << CUR_CONTROL__CURSOR_MODE__SHIFT) |\n\t       (CURSOR_URGENT_1_2 << CUR_CONTROL__CURSOR_URGENT_CONTROL__SHIFT));\n}\n\nstatic void dce_v8_0_show_cursor(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct amdgpu_device *adev = drm_to_adev(crtc->dev);\n\n\tWREG32(mmCUR_SURFACE_ADDRESS_HIGH + amdgpu_crtc->crtc_offset,\n\t       upper_32_bits(amdgpu_crtc->cursor_addr));\n\tWREG32(mmCUR_SURFACE_ADDRESS + amdgpu_crtc->crtc_offset,\n\t       lower_32_bits(amdgpu_crtc->cursor_addr));\n\n\tWREG32(mmCUR_CONTROL + amdgpu_crtc->crtc_offset,\n\t       CUR_CONTROL__CURSOR_EN_MASK |\n\t       (CURSOR_24_8_PRE_MULT << CUR_CONTROL__CURSOR_MODE__SHIFT) |\n\t       (CURSOR_URGENT_1_2 << CUR_CONTROL__CURSOR_URGENT_CONTROL__SHIFT));\n}\n\nstatic int dce_v8_0_cursor_move_locked(struct drm_crtc *crtc,\n\t\t\t\t       int x, int y)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct amdgpu_device *adev = drm_to_adev(crtc->dev);\n\tint xorigin = 0, yorigin = 0;\n\n\tamdgpu_crtc->cursor_x = x;\n\tamdgpu_crtc->cursor_y = y;\n\n\t \n\tx += crtc->x;\n\ty += crtc->y;\n\tDRM_DEBUG(\"x %d y %d c->x %d c->y %d\\n\", x, y, crtc->x, crtc->y);\n\n\tif (x < 0) {\n\t\txorigin = min(-x, amdgpu_crtc->max_cursor_width - 1);\n\t\tx = 0;\n\t}\n\tif (y < 0) {\n\t\tyorigin = min(-y, amdgpu_crtc->max_cursor_height - 1);\n\t\ty = 0;\n\t}\n\n\tWREG32(mmCUR_POSITION + amdgpu_crtc->crtc_offset, (x << 16) | y);\n\tWREG32(mmCUR_HOT_SPOT + amdgpu_crtc->crtc_offset, (xorigin << 16) | yorigin);\n\tWREG32(mmCUR_SIZE + amdgpu_crtc->crtc_offset,\n\t       ((amdgpu_crtc->cursor_width - 1) << 16) | (amdgpu_crtc->cursor_height - 1));\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_crtc_cursor_move(struct drm_crtc *crtc,\n\t\t\t\t     int x, int y)\n{\n\tint ret;\n\n\tdce_v8_0_lock_cursor(crtc, true);\n\tret = dce_v8_0_cursor_move_locked(crtc, x, y);\n\tdce_v8_0_lock_cursor(crtc, false);\n\n\treturn ret;\n}\n\nstatic int dce_v8_0_crtc_cursor_set2(struct drm_crtc *crtc,\n\t\t\t\t     struct drm_file *file_priv,\n\t\t\t\t     uint32_t handle,\n\t\t\t\t     uint32_t width,\n\t\t\t\t     uint32_t height,\n\t\t\t\t     int32_t hot_x,\n\t\t\t\t     int32_t hot_y)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_gem_object *obj;\n\tstruct amdgpu_bo *aobj;\n\tint ret;\n\n\tif (!handle) {\n\t\t \n\t\tdce_v8_0_hide_cursor(crtc);\n\t\tobj = NULL;\n\t\tgoto unpin;\n\t}\n\n\tif ((width > amdgpu_crtc->max_cursor_width) ||\n\t    (height > amdgpu_crtc->max_cursor_height)) {\n\t\tDRM_ERROR(\"bad cursor width or height %d x %d\\n\", width, height);\n\t\treturn -EINVAL;\n\t}\n\n\tobj = drm_gem_object_lookup(file_priv, handle);\n\tif (!obj) {\n\t\tDRM_ERROR(\"Cannot find cursor object %x for crtc %d\\n\", handle, amdgpu_crtc->crtc_id);\n\t\treturn -ENOENT;\n\t}\n\n\taobj = gem_to_amdgpu_bo(obj);\n\tret = amdgpu_bo_reserve(aobj, false);\n\tif (ret != 0) {\n\t\tdrm_gem_object_put(obj);\n\t\treturn ret;\n\t}\n\n\tret = amdgpu_bo_pin(aobj, AMDGPU_GEM_DOMAIN_VRAM);\n\tamdgpu_bo_unreserve(aobj);\n\tif (ret) {\n\t\tDRM_ERROR(\"Failed to pin new cursor BO (%d)\\n\", ret);\n\t\tdrm_gem_object_put(obj);\n\t\treturn ret;\n\t}\n\tamdgpu_crtc->cursor_addr = amdgpu_bo_gpu_offset(aobj);\n\n\tdce_v8_0_lock_cursor(crtc, true);\n\n\tif (width != amdgpu_crtc->cursor_width ||\n\t    height != amdgpu_crtc->cursor_height ||\n\t    hot_x != amdgpu_crtc->cursor_hot_x ||\n\t    hot_y != amdgpu_crtc->cursor_hot_y) {\n\t\tint x, y;\n\n\t\tx = amdgpu_crtc->cursor_x + amdgpu_crtc->cursor_hot_x - hot_x;\n\t\ty = amdgpu_crtc->cursor_y + amdgpu_crtc->cursor_hot_y - hot_y;\n\n\t\tdce_v8_0_cursor_move_locked(crtc, x, y);\n\n\t\tamdgpu_crtc->cursor_width = width;\n\t\tamdgpu_crtc->cursor_height = height;\n\t\tamdgpu_crtc->cursor_hot_x = hot_x;\n\t\tamdgpu_crtc->cursor_hot_y = hot_y;\n\t}\n\n\tdce_v8_0_show_cursor(crtc);\n\tdce_v8_0_lock_cursor(crtc, false);\n\nunpin:\n\tif (amdgpu_crtc->cursor_bo) {\n\t\tstruct amdgpu_bo *aobj = gem_to_amdgpu_bo(amdgpu_crtc->cursor_bo);\n\t\tret = amdgpu_bo_reserve(aobj, true);\n\t\tif (likely(ret == 0)) {\n\t\t\tamdgpu_bo_unpin(aobj);\n\t\t\tamdgpu_bo_unreserve(aobj);\n\t\t}\n\t\tdrm_gem_object_put(amdgpu_crtc->cursor_bo);\n\t}\n\n\tamdgpu_crtc->cursor_bo = obj;\n\treturn 0;\n}\n\nstatic void dce_v8_0_cursor_reset(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\n\tif (amdgpu_crtc->cursor_bo) {\n\t\tdce_v8_0_lock_cursor(crtc, true);\n\n\t\tdce_v8_0_cursor_move_locked(crtc, amdgpu_crtc->cursor_x,\n\t\t\t\t\t    amdgpu_crtc->cursor_y);\n\n\t\tdce_v8_0_show_cursor(crtc);\n\n\t\tdce_v8_0_lock_cursor(crtc, false);\n\t}\n}\n\nstatic int dce_v8_0_crtc_gamma_set(struct drm_crtc *crtc, u16 *red, u16 *green,\n\t\t\t\t   u16 *blue, uint32_t size,\n\t\t\t\t   struct drm_modeset_acquire_ctx *ctx)\n{\n\tdce_v8_0_crtc_load_lut(crtc);\n\n\treturn 0;\n}\n\nstatic void dce_v8_0_crtc_destroy(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\n\tdrm_crtc_cleanup(crtc);\n\tkfree(amdgpu_crtc);\n}\n\nstatic const struct drm_crtc_funcs dce_v8_0_crtc_funcs = {\n\t.cursor_set2 = dce_v8_0_crtc_cursor_set2,\n\t.cursor_move = dce_v8_0_crtc_cursor_move,\n\t.gamma_set = dce_v8_0_crtc_gamma_set,\n\t.set_config = amdgpu_display_crtc_set_config,\n\t.destroy = dce_v8_0_crtc_destroy,\n\t.page_flip_target = amdgpu_display_crtc_page_flip_target,\n\t.get_vblank_counter = amdgpu_get_vblank_counter_kms,\n\t.enable_vblank = amdgpu_enable_vblank_kms,\n\t.disable_vblank = amdgpu_disable_vblank_kms,\n\t.get_vblank_timestamp = drm_crtc_vblank_helper_get_vblank_timestamp,\n};\n\nstatic void dce_v8_0_crtc_dpms(struct drm_crtc *crtc, int mode)\n{\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tunsigned type;\n\n\tswitch (mode) {\n\tcase DRM_MODE_DPMS_ON:\n\t\tamdgpu_crtc->enabled = true;\n\t\tamdgpu_atombios_crtc_enable(crtc, ATOM_ENABLE);\n\t\tdce_v8_0_vga_enable(crtc, true);\n\t\tamdgpu_atombios_crtc_blank(crtc, ATOM_DISABLE);\n\t\tdce_v8_0_vga_enable(crtc, false);\n\t\t \n\t\ttype = amdgpu_display_crtc_idx_to_irq_type(adev,\n\t\t\t\t\t\tamdgpu_crtc->crtc_id);\n\t\tamdgpu_irq_update(adev, &adev->crtc_irq, type);\n\t\tamdgpu_irq_update(adev, &adev->pageflip_irq, type);\n\t\tdrm_crtc_vblank_on(crtc);\n\t\tdce_v8_0_crtc_load_lut(crtc);\n\t\tbreak;\n\tcase DRM_MODE_DPMS_STANDBY:\n\tcase DRM_MODE_DPMS_SUSPEND:\n\tcase DRM_MODE_DPMS_OFF:\n\t\tdrm_crtc_vblank_off(crtc);\n\t\tif (amdgpu_crtc->enabled) {\n\t\t\tdce_v8_0_vga_enable(crtc, true);\n\t\t\tamdgpu_atombios_crtc_blank(crtc, ATOM_ENABLE);\n\t\t\tdce_v8_0_vga_enable(crtc, false);\n\t\t}\n\t\tamdgpu_atombios_crtc_enable(crtc, ATOM_DISABLE);\n\t\tamdgpu_crtc->enabled = false;\n\t\tbreak;\n\t}\n\t \n\tamdgpu_dpm_compute_clocks(adev);\n}\n\nstatic void dce_v8_0_crtc_prepare(struct drm_crtc *crtc)\n{\n\t \n\tamdgpu_atombios_crtc_powergate(crtc, ATOM_DISABLE);\n\tamdgpu_atombios_crtc_lock(crtc, ATOM_ENABLE);\n\tdce_v8_0_crtc_dpms(crtc, DRM_MODE_DPMS_OFF);\n}\n\nstatic void dce_v8_0_crtc_commit(struct drm_crtc *crtc)\n{\n\tdce_v8_0_crtc_dpms(crtc, DRM_MODE_DPMS_ON);\n\tamdgpu_atombios_crtc_lock(crtc, ATOM_DISABLE);\n}\n\nstatic void dce_v8_0_crtc_disable(struct drm_crtc *crtc)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_atom_ss ss;\n\tint i;\n\n\tdce_v8_0_crtc_dpms(crtc, DRM_MODE_DPMS_OFF);\n\tif (crtc->primary->fb) {\n\t\tint r;\n\t\tstruct amdgpu_bo *abo;\n\n\t\tabo = gem_to_amdgpu_bo(crtc->primary->fb->obj[0]);\n\t\tr = amdgpu_bo_reserve(abo, true);\n\t\tif (unlikely(r))\n\t\t\tDRM_ERROR(\"failed to reserve abo before unpin\\n\");\n\t\telse {\n\t\t\tamdgpu_bo_unpin(abo);\n\t\t\tamdgpu_bo_unreserve(abo);\n\t\t}\n\t}\n\t \n\tdce_v8_0_grph_enable(crtc, false);\n\n\tamdgpu_atombios_crtc_powergate(crtc, ATOM_ENABLE);\n\n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tif (adev->mode_info.crtcs[i] &&\n\t\t    adev->mode_info.crtcs[i]->enabled &&\n\t\t    i != amdgpu_crtc->crtc_id &&\n\t\t    amdgpu_crtc->pll_id == adev->mode_info.crtcs[i]->pll_id) {\n\t\t\t \n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tswitch (amdgpu_crtc->pll_id) {\n\tcase ATOM_PPLL1:\n\tcase ATOM_PPLL2:\n\t\t \n\t\tamdgpu_atombios_crtc_program_pll(crtc, amdgpu_crtc->crtc_id, amdgpu_crtc->pll_id,\n\t\t\t\t\t\t 0, 0, ATOM_DISABLE, 0, 0, 0, 0, 0, false, &ss);\n\t\tbreak;\n\tcase ATOM_PPLL0:\n\t\t \n\t\tif ((adev->asic_type == CHIP_KAVERI) ||\n\t\t    (adev->asic_type == CHIP_BONAIRE) ||\n\t\t    (adev->asic_type == CHIP_HAWAII))\n\t\t\tamdgpu_atombios_crtc_program_pll(crtc, amdgpu_crtc->crtc_id, amdgpu_crtc->pll_id,\n\t\t\t\t\t\t  0, 0, ATOM_DISABLE, 0, 0, 0, 0, 0, false, &ss);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\ndone:\n\tamdgpu_crtc->pll_id = ATOM_PPLL_INVALID;\n\tamdgpu_crtc->adjusted_clock = 0;\n\tamdgpu_crtc->encoder = NULL;\n\tamdgpu_crtc->connector = NULL;\n}\n\nstatic int dce_v8_0_crtc_mode_set(struct drm_crtc *crtc,\n\t\t\t\t  struct drm_display_mode *mode,\n\t\t\t\t  struct drm_display_mode *adjusted_mode,\n\t\t\t\t  int x, int y, struct drm_framebuffer *old_fb)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\n\tif (!amdgpu_crtc->adjusted_clock)\n\t\treturn -EINVAL;\n\n\tamdgpu_atombios_crtc_set_pll(crtc, adjusted_mode);\n\tamdgpu_atombios_crtc_set_dtd_timing(crtc, adjusted_mode);\n\tdce_v8_0_crtc_do_set_base(crtc, old_fb, x, y, 0);\n\tamdgpu_atombios_crtc_overscan_setup(crtc, mode, adjusted_mode);\n\tamdgpu_atombios_crtc_scaler_setup(crtc);\n\tdce_v8_0_cursor_reset(crtc);\n\t \n\tamdgpu_crtc->hw_mode = *adjusted_mode;\n\n\treturn 0;\n}\n\nstatic bool dce_v8_0_crtc_mode_fixup(struct drm_crtc *crtc,\n\t\t\t\t     const struct drm_display_mode *mode,\n\t\t\t\t     struct drm_display_mode *adjusted_mode)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc = to_amdgpu_crtc(crtc);\n\tstruct drm_device *dev = crtc->dev;\n\tstruct drm_encoder *encoder;\n\n\t \n\tlist_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {\n\t\tif (encoder->crtc == crtc) {\n\t\t\tamdgpu_crtc->encoder = encoder;\n\t\t\tamdgpu_crtc->connector = amdgpu_get_connector_for_encoder(encoder);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif ((amdgpu_crtc->encoder == NULL) || (amdgpu_crtc->connector == NULL)) {\n\t\tamdgpu_crtc->encoder = NULL;\n\t\tamdgpu_crtc->connector = NULL;\n\t\treturn false;\n\t}\n\tif (!amdgpu_display_crtc_scaling_mode_fixup(crtc, mode, adjusted_mode))\n\t\treturn false;\n\tif (amdgpu_atombios_crtc_prepare_pll(crtc, adjusted_mode))\n\t\treturn false;\n\t \n\tamdgpu_crtc->pll_id = dce_v8_0_pick_pll(crtc);\n\t \n\tif ((amdgpu_crtc->pll_id == ATOM_PPLL_INVALID) &&\n\t    !ENCODER_MODE_IS_DP(amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder)))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int dce_v8_0_crtc_set_base(struct drm_crtc *crtc, int x, int y,\n\t\t\t\t  struct drm_framebuffer *old_fb)\n{\n\treturn dce_v8_0_crtc_do_set_base(crtc, old_fb, x, y, 0);\n}\n\nstatic int dce_v8_0_crtc_set_base_atomic(struct drm_crtc *crtc,\n\t\t\t\t\t struct drm_framebuffer *fb,\n\t\t\t\t\t int x, int y, enum mode_set_atomic state)\n{\n\treturn dce_v8_0_crtc_do_set_base(crtc, fb, x, y, 1);\n}\n\nstatic const struct drm_crtc_helper_funcs dce_v8_0_crtc_helper_funcs = {\n\t.dpms = dce_v8_0_crtc_dpms,\n\t.mode_fixup = dce_v8_0_crtc_mode_fixup,\n\t.mode_set = dce_v8_0_crtc_mode_set,\n\t.mode_set_base = dce_v8_0_crtc_set_base,\n\t.mode_set_base_atomic = dce_v8_0_crtc_set_base_atomic,\n\t.prepare = dce_v8_0_crtc_prepare,\n\t.commit = dce_v8_0_crtc_commit,\n\t.disable = dce_v8_0_crtc_disable,\n\t.get_scanout_position = amdgpu_crtc_get_scanout_position,\n};\n\nstatic int dce_v8_0_crtc_init(struct amdgpu_device *adev, int index)\n{\n\tstruct amdgpu_crtc *amdgpu_crtc;\n\n\tamdgpu_crtc = kzalloc(sizeof(struct amdgpu_crtc) +\n\t\t\t      (AMDGPUFB_CONN_LIMIT * sizeof(struct drm_connector *)), GFP_KERNEL);\n\tif (amdgpu_crtc == NULL)\n\t\treturn -ENOMEM;\n\n\tdrm_crtc_init(adev_to_drm(adev), &amdgpu_crtc->base, &dce_v8_0_crtc_funcs);\n\n\tdrm_mode_crtc_set_gamma_size(&amdgpu_crtc->base, 256);\n\tamdgpu_crtc->crtc_id = index;\n\tadev->mode_info.crtcs[index] = amdgpu_crtc;\n\n\tamdgpu_crtc->max_cursor_width = CIK_CURSOR_WIDTH;\n\tamdgpu_crtc->max_cursor_height = CIK_CURSOR_HEIGHT;\n\tadev_to_drm(adev)->mode_config.cursor_width = amdgpu_crtc->max_cursor_width;\n\tadev_to_drm(adev)->mode_config.cursor_height = amdgpu_crtc->max_cursor_height;\n\n\tamdgpu_crtc->crtc_offset = crtc_offsets[amdgpu_crtc->crtc_id];\n\n\tamdgpu_crtc->pll_id = ATOM_PPLL_INVALID;\n\tamdgpu_crtc->adjusted_clock = 0;\n\tamdgpu_crtc->encoder = NULL;\n\tamdgpu_crtc->connector = NULL;\n\tdrm_crtc_helper_add(&amdgpu_crtc->base, &dce_v8_0_crtc_helper_funcs);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->audio_endpt_rreg = &dce_v8_0_audio_endpt_rreg;\n\tadev->audio_endpt_wreg = &dce_v8_0_audio_endpt_wreg;\n\n\tdce_v8_0_set_display_funcs(adev);\n\n\tadev->mode_info.num_crtc = dce_v8_0_get_num_crtc(adev);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_BONAIRE:\n\tcase CHIP_HAWAII:\n\t\tadev->mode_info.num_hpd = 6;\n\t\tadev->mode_info.num_dig = 6;\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\t\tadev->mode_info.num_hpd = 6;\n\t\tadev->mode_info.num_dig = 7;\n\t\tbreak;\n\tcase CHIP_KABINI:\n\tcase CHIP_MULLINS:\n\t\tadev->mode_info.num_hpd = 6;\n\t\tadev->mode_info.num_dig = 6;  \n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tdce_v8_0_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_sw_init(void *handle)\n{\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, i + 1, &adev->crtc_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 8; i < 20; i += 2) {\n\t\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, i, &adev->pageflip_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 42, &adev->hpd_irq);\n\tif (r)\n\t\treturn r;\n\n\tadev_to_drm(adev)->mode_config.funcs = &amdgpu_mode_funcs;\n\n\tadev_to_drm(adev)->mode_config.async_page_flip = true;\n\n\tadev_to_drm(adev)->mode_config.max_width = 16384;\n\tadev_to_drm(adev)->mode_config.max_height = 16384;\n\n\tadev_to_drm(adev)->mode_config.preferred_depth = 24;\n\tif (adev->asic_type == CHIP_HAWAII)\n\t\t \n\t\tadev_to_drm(adev)->mode_config.prefer_shadow = 0;\n\telse\n\t\tadev_to_drm(adev)->mode_config.prefer_shadow = 1;\n\n\tadev_to_drm(adev)->mode_config.fb_modifiers_not_supported = true;\n\n\tr = amdgpu_display_modeset_create_props(adev);\n\tif (r)\n\t\treturn r;\n\n\tadev_to_drm(adev)->mode_config.max_width = 16384;\n\tadev_to_drm(adev)->mode_config.max_height = 16384;\n\n\t \n\tfor (i = 0; i < adev->mode_info.num_crtc; i++) {\n\t\tr = dce_v8_0_crtc_init(adev, i);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (amdgpu_atombios_get_connector_info_from_object_table(adev))\n\t\tamdgpu_display_print_display_setup(adev_to_drm(adev));\n\telse\n\t\treturn -EINVAL;\n\n\t \n\tr = dce_v8_0_afmt_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = dce_v8_0_audio_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\t \n\tadev_to_drm(adev)->vblank_disable_immediate = true;\n\n\tr = drm_vblank_init(adev_to_drm(adev), adev->mode_info.num_crtc);\n\tif (r)\n\t\treturn r;\n\n\t \n\tINIT_DELAYED_WORK(&adev->hotplug_work,\n\t\t  amdgpu_display_hotplug_work_func);\n\n\tdrm_kms_helper_poll_init(adev_to_drm(adev));\n\n\tadev->mode_info.mode_config_initialized = true;\n\treturn 0;\n}\n\nstatic int dce_v8_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tkfree(adev->mode_info.bios_hardcoded_edid);\n\n\tdrm_kms_helper_poll_fini(adev_to_drm(adev));\n\n\tdce_v8_0_audio_fini(adev);\n\n\tdce_v8_0_afmt_fini(adev);\n\n\tdrm_mode_config_cleanup(adev_to_drm(adev));\n\tadev->mode_info.mode_config_initialized = false;\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_hw_init(void *handle)\n{\n\tint i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tdce_v8_0_set_vga_render_state(adev, false);\n\t \n\tamdgpu_atombios_encoder_init_dig(adev);\n\tamdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);\n\n\t \n\tdce_v8_0_hpd_init(adev);\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++) {\n\t\tdce_v8_0_audio_enable(adev, &adev->mode_info.audio.pin[i], false);\n\t}\n\n\tdce_v8_0_pageflip_interrupt_init(adev);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_hw_fini(void *handle)\n{\n\tint i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tdce_v8_0_hpd_fini(adev);\n\n\tfor (i = 0; i < adev->mode_info.audio.num_pins; i++) {\n\t\tdce_v8_0_audio_enable(adev, &adev->mode_info.audio.pin[i], false);\n\t}\n\n\tdce_v8_0_pageflip_interrupt_fini(adev);\n\n\tflush_delayed_work(&adev->hotplug_work);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_display_suspend_helper(adev);\n\tif (r)\n\t\treturn r;\n\n\tadev->mode_info.bl_level =\n\t\tamdgpu_atombios_encoder_get_backlight_level_from_reg(adev);\n\n\treturn dce_v8_0_hw_fini(handle);\n}\n\nstatic int dce_v8_0_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret;\n\n\tamdgpu_atombios_encoder_set_backlight_level_to_reg(adev,\n\t\t\t\t\t\t\t   adev->mode_info.bl_level);\n\n\tret = dce_v8_0_hw_init(handle);\n\n\t \n\tif (adev->mode_info.bl_encoder) {\n\t\tu8 bl_level = amdgpu_display_backlight_get_level(adev,\n\t\t\t\t\t\t\t\t  adev->mode_info.bl_encoder);\n\t\tamdgpu_display_backlight_set_level(adev, adev->mode_info.bl_encoder,\n\t\t\t\t\t\t    bl_level);\n\t}\n\tif (ret)\n\t\treturn ret;\n\n\treturn amdgpu_display_resume_helper(adev);\n}\n\nstatic bool dce_v8_0_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int dce_v8_0_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int dce_v8_0_soft_reset(void *handle)\n{\n\tu32 srbm_soft_reset = 0, tmp;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (dce_v8_0_is_display_hung(adev))\n\t\tsrbm_soft_reset |= SRBM_SOFT_RESET__SOFT_RESET_DC_MASK;\n\n\tif (srbm_soft_reset) {\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\t \n\t\tudelay(50);\n\t}\n\treturn 0;\n}\n\nstatic void dce_v8_0_set_crtc_vblank_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t\t     int crtc,\n\t\t\t\t\t\t     enum amdgpu_interrupt_state state)\n{\n\tu32 reg_block, lb_interrupt_mask;\n\n\tif (crtc >= adev->mode_info.num_crtc) {\n\t\tDRM_DEBUG(\"invalid crtc %d\\n\", crtc);\n\t\treturn;\n\t}\n\n\tswitch (crtc) {\n\tcase 0:\n\t\treg_block = CRTC0_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 1:\n\t\treg_block = CRTC1_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 2:\n\t\treg_block = CRTC2_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 3:\n\t\treg_block = CRTC3_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 4:\n\t\treg_block = CRTC4_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 5:\n\t\treg_block = CRTC5_REGISTER_OFFSET;\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"invalid crtc %d\\n\", crtc);\n\t\treturn;\n\t}\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tlb_interrupt_mask = RREG32(mmLB_INTERRUPT_MASK + reg_block);\n\t\tlb_interrupt_mask &= ~LB_INTERRUPT_MASK__VBLANK_INTERRUPT_MASK_MASK;\n\t\tWREG32(mmLB_INTERRUPT_MASK + reg_block, lb_interrupt_mask);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tlb_interrupt_mask = RREG32(mmLB_INTERRUPT_MASK + reg_block);\n\t\tlb_interrupt_mask |= LB_INTERRUPT_MASK__VBLANK_INTERRUPT_MASK_MASK;\n\t\tWREG32(mmLB_INTERRUPT_MASK + reg_block, lb_interrupt_mask);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void dce_v8_0_set_crtc_vline_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t\t    int crtc,\n\t\t\t\t\t\t    enum amdgpu_interrupt_state state)\n{\n\tu32 reg_block, lb_interrupt_mask;\n\n\tif (crtc >= adev->mode_info.num_crtc) {\n\t\tDRM_DEBUG(\"invalid crtc %d\\n\", crtc);\n\t\treturn;\n\t}\n\n\tswitch (crtc) {\n\tcase 0:\n\t\treg_block = CRTC0_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 1:\n\t\treg_block = CRTC1_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 2:\n\t\treg_block = CRTC2_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 3:\n\t\treg_block = CRTC3_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 4:\n\t\treg_block = CRTC4_REGISTER_OFFSET;\n\t\tbreak;\n\tcase 5:\n\t\treg_block = CRTC5_REGISTER_OFFSET;\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"invalid crtc %d\\n\", crtc);\n\t\treturn;\n\t}\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tlb_interrupt_mask = RREG32(mmLB_INTERRUPT_MASK + reg_block);\n\t\tlb_interrupt_mask &= ~LB_INTERRUPT_MASK__VLINE_INTERRUPT_MASK_MASK;\n\t\tWREG32(mmLB_INTERRUPT_MASK + reg_block, lb_interrupt_mask);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tlb_interrupt_mask = RREG32(mmLB_INTERRUPT_MASK + reg_block);\n\t\tlb_interrupt_mask |= LB_INTERRUPT_MASK__VLINE_INTERRUPT_MASK_MASK;\n\t\tWREG32(mmLB_INTERRUPT_MASK + reg_block, lb_interrupt_mask);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int dce_v8_0_set_hpd_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t    struct amdgpu_irq_src *src,\n\t\t\t\t\t    unsigned type,\n\t\t\t\t\t    enum amdgpu_interrupt_state state)\n{\n\tu32 dc_hpd_int_cntl;\n\n\tif (type >= adev->mode_info.num_hpd) {\n\t\tDRM_DEBUG(\"invalid hdp %d\\n\", type);\n\t\treturn 0;\n\t}\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tdc_hpd_int_cntl = RREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[type]);\n\t\tdc_hpd_int_cntl &= ~DC_HPD1_INT_CONTROL__DC_HPD1_INT_EN_MASK;\n\t\tWREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[type], dc_hpd_int_cntl);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tdc_hpd_int_cntl = RREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[type]);\n\t\tdc_hpd_int_cntl |= DC_HPD1_INT_CONTROL__DC_HPD1_INT_EN_MASK;\n\t\tWREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[type], dc_hpd_int_cntl);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_set_crtc_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t     struct amdgpu_irq_src *src,\n\t\t\t\t\t     unsigned type,\n\t\t\t\t\t     enum amdgpu_interrupt_state state)\n{\n\tswitch (type) {\n\tcase AMDGPU_CRTC_IRQ_VBLANK1:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 0, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VBLANK2:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 1, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VBLANK3:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 2, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VBLANK4:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 3, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VBLANK5:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 4, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VBLANK6:\n\t\tdce_v8_0_set_crtc_vblank_interrupt_state(adev, 5, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE1:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 0, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE2:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 1, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE3:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 2, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE4:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 3, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE5:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 4, state);\n\t\tbreak;\n\tcase AMDGPU_CRTC_IRQ_VLINE6:\n\t\tdce_v8_0_set_crtc_vline_interrupt_state(adev, 5, state);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int dce_v8_0_crtc_irq(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_irq_src *source,\n\t\t\t     struct amdgpu_iv_entry *entry)\n{\n\tunsigned crtc = entry->src_id - 1;\n\tuint32_t disp_int = RREG32(interrupt_status_offsets[crtc].reg);\n\tunsigned int irq_type = amdgpu_display_crtc_idx_to_irq_type(adev,\n\t\t\t\t\t\t\t\t    crtc);\n\n\tswitch (entry->src_data[0]) {\n\tcase 0:  \n\t\tif (disp_int & interrupt_status_offsets[crtc].vblank)\n\t\t\tWREG32(mmLB_VBLANK_STATUS + crtc_offsets[crtc], LB_VBLANK_STATUS__VBLANK_ACK_MASK);\n\t\telse\n\t\t\tDRM_DEBUG(\"IH: IH event w/o asserted irq bit?\\n\");\n\n\t\tif (amdgpu_irq_enabled(adev, source, irq_type)) {\n\t\t\tdrm_handle_vblank(adev_to_drm(adev), crtc);\n\t\t}\n\t\tDRM_DEBUG(\"IH: D%d vblank\\n\", crtc + 1);\n\t\tbreak;\n\tcase 1:  \n\t\tif (disp_int & interrupt_status_offsets[crtc].vline)\n\t\t\tWREG32(mmLB_VLINE_STATUS + crtc_offsets[crtc], LB_VLINE_STATUS__VLINE_ACK_MASK);\n\t\telse\n\t\t\tDRM_DEBUG(\"IH: IH event w/o asserted irq bit?\\n\");\n\n\t\tDRM_DEBUG(\"IH: D%d vline\\n\", crtc + 1);\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEBUG(\"Unhandled interrupt: %d %d\\n\", entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_set_pageflip_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t\t struct amdgpu_irq_src *src,\n\t\t\t\t\t\t unsigned type,\n\t\t\t\t\t\t enum amdgpu_interrupt_state state)\n{\n\tu32 reg;\n\n\tif (type >= adev->mode_info.num_crtc) {\n\t\tDRM_ERROR(\"invalid pageflip crtc %d\\n\", type);\n\t\treturn -EINVAL;\n\t}\n\n\treg = RREG32(mmGRPH_INTERRUPT_CONTROL + crtc_offsets[type]);\n\tif (state == AMDGPU_IRQ_STATE_DISABLE)\n\t\tWREG32(mmGRPH_INTERRUPT_CONTROL + crtc_offsets[type],\n\t\t       reg & ~GRPH_INTERRUPT_CONTROL__GRPH_PFLIP_INT_MASK_MASK);\n\telse\n\t\tWREG32(mmGRPH_INTERRUPT_CONTROL + crtc_offsets[type],\n\t\t       reg | GRPH_INTERRUPT_CONTROL__GRPH_PFLIP_INT_MASK_MASK);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_pageflip_irq(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tunsigned long flags;\n\tunsigned crtc_id;\n\tstruct amdgpu_crtc *amdgpu_crtc;\n\tstruct amdgpu_flip_work *works;\n\n\tcrtc_id = (entry->src_id - 8) >> 1;\n\tamdgpu_crtc = adev->mode_info.crtcs[crtc_id];\n\n\tif (crtc_id >= adev->mode_info.num_crtc) {\n\t\tDRM_ERROR(\"invalid pageflip crtc %d\\n\", crtc_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (RREG32(mmGRPH_INTERRUPT_STATUS + crtc_offsets[crtc_id]) &\n\t    GRPH_INTERRUPT_STATUS__GRPH_PFLIP_INT_OCCURRED_MASK)\n\t\tWREG32(mmGRPH_INTERRUPT_STATUS + crtc_offsets[crtc_id],\n\t\t       GRPH_INTERRUPT_STATUS__GRPH_PFLIP_INT_CLEAR_MASK);\n\n\t \n\tif (amdgpu_crtc == NULL)\n\t\treturn 0;\n\n\tspin_lock_irqsave(&adev_to_drm(adev)->event_lock, flags);\n\tworks = amdgpu_crtc->pflip_works;\n\tif (amdgpu_crtc->pflip_status != AMDGPU_FLIP_SUBMITTED) {\n\t\tDRM_DEBUG_DRIVER(\"amdgpu_crtc->pflip_status = %d != \"\n\t\t\t\t\t\t\"AMDGPU_FLIP_SUBMITTED(%d)\\n\",\n\t\t\t\t\t\tamdgpu_crtc->pflip_status,\n\t\t\t\t\t\tAMDGPU_FLIP_SUBMITTED);\n\t\tspin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags);\n\t\treturn 0;\n\t}\n\n\t \n\tamdgpu_crtc->pflip_status = AMDGPU_FLIP_NONE;\n\tamdgpu_crtc->pflip_works = NULL;\n\n\t \n\tif (works->event)\n\t\tdrm_crtc_send_vblank_event(&amdgpu_crtc->base, works->event);\n\n\tspin_unlock_irqrestore(&adev_to_drm(adev)->event_lock, flags);\n\n\tdrm_crtc_vblank_put(&amdgpu_crtc->base);\n\tschedule_work(&works->unpin_work);\n\n\treturn 0;\n}\n\nstatic int dce_v8_0_hpd_irq(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_irq_src *source,\n\t\t\t    struct amdgpu_iv_entry *entry)\n{\n\tuint32_t disp_int, mask, tmp;\n\tunsigned hpd;\n\n\tif (entry->src_data[0] >= adev->mode_info.num_hpd) {\n\t\tDRM_DEBUG(\"Unhandled interrupt: %d %d\\n\", entry->src_id, entry->src_data[0]);\n\t\treturn 0;\n\t}\n\n\thpd = entry->src_data[0];\n\tdisp_int = RREG32(interrupt_status_offsets[hpd].reg);\n\tmask = interrupt_status_offsets[hpd].hpd;\n\n\tif (disp_int & mask) {\n\t\ttmp = RREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[hpd]);\n\t\ttmp |= DC_HPD1_INT_CONTROL__DC_HPD1_INT_ACK_MASK;\n\t\tWREG32(mmDC_HPD1_INT_CONTROL + hpd_offsets[hpd], tmp);\n\t\tschedule_delayed_work(&adev->hotplug_work, 0);\n\t\tDRM_DEBUG(\"IH: HPD%d\\n\", hpd + 1);\n\t}\n\n\treturn 0;\n\n}\n\nstatic int dce_v8_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int dce_v8_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs dce_v8_0_ip_funcs = {\n\t.name = \"dce_v8_0\",\n\t.early_init = dce_v8_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = dce_v8_0_sw_init,\n\t.sw_fini = dce_v8_0_sw_fini,\n\t.hw_init = dce_v8_0_hw_init,\n\t.hw_fini = dce_v8_0_hw_fini,\n\t.suspend = dce_v8_0_suspend,\n\t.resume = dce_v8_0_resume,\n\t.is_idle = dce_v8_0_is_idle,\n\t.wait_for_idle = dce_v8_0_wait_for_idle,\n\t.soft_reset = dce_v8_0_soft_reset,\n\t.set_clockgating_state = dce_v8_0_set_clockgating_state,\n\t.set_powergating_state = dce_v8_0_set_powergating_state,\n};\n\nstatic void\ndce_v8_0_encoder_mode_set(struct drm_encoder *encoder,\n\t\t\t  struct drm_display_mode *mode,\n\t\t\t  struct drm_display_mode *adjusted_mode)\n{\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\n\tamdgpu_encoder->pixel_clock = adjusted_mode->clock;\n\n\t \n\tamdgpu_atombios_encoder_dpms(encoder, DRM_MODE_DPMS_OFF);\n\n\t \n\tdce_v8_0_set_interleave(encoder->crtc, mode);\n\n\tif (amdgpu_atombios_encoder_get_encoder_mode(encoder) == ATOM_ENCODER_MODE_HDMI) {\n\t\tdce_v8_0_afmt_enable(encoder, true);\n\t\tdce_v8_0_afmt_setmode(encoder, adjusted_mode);\n\t}\n}\n\nstatic void dce_v8_0_encoder_prepare(struct drm_encoder *encoder)\n{\n\tstruct amdgpu_device *adev = drm_to_adev(encoder->dev);\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct drm_connector *connector = amdgpu_get_connector_for_encoder(encoder);\n\n\tif ((amdgpu_encoder->active_device &\n\t     (ATOM_DEVICE_DFP_SUPPORT | ATOM_DEVICE_LCD_SUPPORT)) ||\n\t    (amdgpu_encoder_get_dp_bridge_encoder_id(encoder) !=\n\t     ENCODER_OBJECT_ID_NONE)) {\n\t\tstruct amdgpu_encoder_atom_dig *dig = amdgpu_encoder->enc_priv;\n\t\tif (dig) {\n\t\t\tdig->dig_encoder = dce_v8_0_pick_dig_encoder(encoder);\n\t\t\tif (amdgpu_encoder->active_device & ATOM_DEVICE_DFP_SUPPORT)\n\t\t\t\tdig->afmt = adev->mode_info.afmt[dig->dig_encoder];\n\t\t}\n\t}\n\n\tamdgpu_atombios_scratch_regs_lock(adev, true);\n\n\tif (connector) {\n\t\tstruct amdgpu_connector *amdgpu_connector = to_amdgpu_connector(connector);\n\n\t\t \n\t\tif (amdgpu_connector->router.cd_valid)\n\t\t\tamdgpu_i2c_router_select_cd_port(amdgpu_connector);\n\n\t\t \n\t\tif (connector->connector_type == DRM_MODE_CONNECTOR_eDP)\n\t\t\tamdgpu_atombios_encoder_set_edp_panel_power(connector,\n\t\t\t\t\t\t\t     ATOM_TRANSMITTER_ACTION_POWER_ON);\n\t}\n\n\t \n\tamdgpu_atombios_encoder_set_crtc_source(encoder);\n\t \n\tdce_v8_0_program_fmt(encoder);\n}\n\nstatic void dce_v8_0_encoder_commit(struct drm_encoder *encoder)\n{\n\tstruct drm_device *dev = encoder->dev;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\n\t \n\tamdgpu_atombios_encoder_dpms(encoder, DRM_MODE_DPMS_ON);\n\tamdgpu_atombios_scratch_regs_lock(adev, false);\n}\n\nstatic void dce_v8_0_encoder_disable(struct drm_encoder *encoder)\n{\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tstruct amdgpu_encoder_atom_dig *dig;\n\n\tamdgpu_atombios_encoder_dpms(encoder, DRM_MODE_DPMS_OFF);\n\n\tif (amdgpu_atombios_encoder_is_digital(encoder)) {\n\t\tif (amdgpu_atombios_encoder_get_encoder_mode(encoder) == ATOM_ENCODER_MODE_HDMI)\n\t\t\tdce_v8_0_afmt_enable(encoder, false);\n\t\tdig = amdgpu_encoder->enc_priv;\n\t\tdig->dig_encoder = -1;\n\t}\n\tamdgpu_encoder->active_device = 0;\n}\n\n \nstatic void dce_v8_0_ext_prepare(struct drm_encoder *encoder)\n{\n\n}\n\nstatic void dce_v8_0_ext_commit(struct drm_encoder *encoder)\n{\n\n}\n\nstatic void\ndce_v8_0_ext_mode_set(struct drm_encoder *encoder,\n\t\t      struct drm_display_mode *mode,\n\t\t      struct drm_display_mode *adjusted_mode)\n{\n\n}\n\nstatic void dce_v8_0_ext_disable(struct drm_encoder *encoder)\n{\n\n}\n\nstatic void\ndce_v8_0_ext_dpms(struct drm_encoder *encoder, int mode)\n{\n\n}\n\nstatic const struct drm_encoder_helper_funcs dce_v8_0_ext_helper_funcs = {\n\t.dpms = dce_v8_0_ext_dpms,\n\t.prepare = dce_v8_0_ext_prepare,\n\t.mode_set = dce_v8_0_ext_mode_set,\n\t.commit = dce_v8_0_ext_commit,\n\t.disable = dce_v8_0_ext_disable,\n\t \n};\n\nstatic const struct drm_encoder_helper_funcs dce_v8_0_dig_helper_funcs = {\n\t.dpms = amdgpu_atombios_encoder_dpms,\n\t.mode_fixup = amdgpu_atombios_encoder_mode_fixup,\n\t.prepare = dce_v8_0_encoder_prepare,\n\t.mode_set = dce_v8_0_encoder_mode_set,\n\t.commit = dce_v8_0_encoder_commit,\n\t.disable = dce_v8_0_encoder_disable,\n\t.detect = amdgpu_atombios_encoder_dig_detect,\n};\n\nstatic const struct drm_encoder_helper_funcs dce_v8_0_dac_helper_funcs = {\n\t.dpms = amdgpu_atombios_encoder_dpms,\n\t.mode_fixup = amdgpu_atombios_encoder_mode_fixup,\n\t.prepare = dce_v8_0_encoder_prepare,\n\t.mode_set = dce_v8_0_encoder_mode_set,\n\t.commit = dce_v8_0_encoder_commit,\n\t.detect = amdgpu_atombios_encoder_dac_detect,\n};\n\nstatic void dce_v8_0_encoder_destroy(struct drm_encoder *encoder)\n{\n\tstruct amdgpu_encoder *amdgpu_encoder = to_amdgpu_encoder(encoder);\n\tif (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT))\n\t\tamdgpu_atombios_encoder_fini_backlight(amdgpu_encoder);\n\tkfree(amdgpu_encoder->enc_priv);\n\tdrm_encoder_cleanup(encoder);\n\tkfree(amdgpu_encoder);\n}\n\nstatic const struct drm_encoder_funcs dce_v8_0_encoder_funcs = {\n\t.destroy = dce_v8_0_encoder_destroy,\n};\n\nstatic void dce_v8_0_encoder_add(struct amdgpu_device *adev,\n\t\t\t\t uint32_t encoder_enum,\n\t\t\t\t uint32_t supported_device,\n\t\t\t\t u16 caps)\n{\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tstruct drm_encoder *encoder;\n\tstruct amdgpu_encoder *amdgpu_encoder;\n\n\t \n\tlist_for_each_entry(encoder, &dev->mode_config.encoder_list, head) {\n\t\tamdgpu_encoder = to_amdgpu_encoder(encoder);\n\t\tif (amdgpu_encoder->encoder_enum == encoder_enum) {\n\t\t\tamdgpu_encoder->devices |= supported_device;\n\t\t\treturn;\n\t\t}\n\n\t}\n\n\t \n\tamdgpu_encoder = kzalloc(sizeof(struct amdgpu_encoder), GFP_KERNEL);\n\tif (!amdgpu_encoder)\n\t\treturn;\n\n\tencoder = &amdgpu_encoder->base;\n\tswitch (adev->mode_info.num_crtc) {\n\tcase 1:\n\t\tencoder->possible_crtcs = 0x1;\n\t\tbreak;\n\tcase 2:\n\tdefault:\n\t\tencoder->possible_crtcs = 0x3;\n\t\tbreak;\n\tcase 4:\n\t\tencoder->possible_crtcs = 0xf;\n\t\tbreak;\n\tcase 6:\n\t\tencoder->possible_crtcs = 0x3f;\n\t\tbreak;\n\t}\n\n\tamdgpu_encoder->enc_priv = NULL;\n\n\tamdgpu_encoder->encoder_enum = encoder_enum;\n\tamdgpu_encoder->encoder_id = (encoder_enum & OBJECT_ID_MASK) >> OBJECT_ID_SHIFT;\n\tamdgpu_encoder->devices = supported_device;\n\tamdgpu_encoder->rmx_type = RMX_OFF;\n\tamdgpu_encoder->underscan_type = UNDERSCAN_OFF;\n\tamdgpu_encoder->is_ext_encoder = false;\n\tamdgpu_encoder->caps = caps;\n\n\tswitch (amdgpu_encoder->encoder_id) {\n\tcase ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC1:\n\tcase ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DAC2:\n\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t DRM_MODE_ENCODER_DAC, NULL);\n\t\tdrm_encoder_helper_add(encoder, &dce_v8_0_dac_helper_funcs);\n\t\tbreak;\n\tcase ENCODER_OBJECT_ID_INTERNAL_KLDSCP_DVO1:\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY:\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY1:\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY2:\n\tcase ENCODER_OBJECT_ID_INTERNAL_UNIPHY3:\n\t\tif (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT)) {\n\t\t\tamdgpu_encoder->rmx_type = RMX_FULL;\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_LVDS, NULL);\n\t\t\tamdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);\n\t\t} else if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT)) {\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_DAC, NULL);\n\t\t\tamdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);\n\t\t} else {\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_TMDS, NULL);\n\t\t\tamdgpu_encoder->enc_priv = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);\n\t\t}\n\t\tdrm_encoder_helper_add(encoder, &dce_v8_0_dig_helper_funcs);\n\t\tbreak;\n\tcase ENCODER_OBJECT_ID_SI170B:\n\tcase ENCODER_OBJECT_ID_CH7303:\n\tcase ENCODER_OBJECT_ID_EXTERNAL_SDVOA:\n\tcase ENCODER_OBJECT_ID_EXTERNAL_SDVOB:\n\tcase ENCODER_OBJECT_ID_TITFP513:\n\tcase ENCODER_OBJECT_ID_VT1623:\n\tcase ENCODER_OBJECT_ID_HDMI_SI1930:\n\tcase ENCODER_OBJECT_ID_TRAVIS:\n\tcase ENCODER_OBJECT_ID_NUTMEG:\n\t\t \n\t\tamdgpu_encoder->is_ext_encoder = true;\n\t\tif (amdgpu_encoder->devices & (ATOM_DEVICE_LCD_SUPPORT))\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_LVDS, NULL);\n\t\telse if (amdgpu_encoder->devices & (ATOM_DEVICE_CRT_SUPPORT))\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_DAC, NULL);\n\t\telse\n\t\t\tdrm_encoder_init(dev, encoder, &dce_v8_0_encoder_funcs,\n\t\t\t\t\t DRM_MODE_ENCODER_TMDS, NULL);\n\t\tdrm_encoder_helper_add(encoder, &dce_v8_0_ext_helper_funcs);\n\t\tbreak;\n\t}\n}\n\nstatic const struct amdgpu_display_funcs dce_v8_0_display_funcs = {\n\t.bandwidth_update = &dce_v8_0_bandwidth_update,\n\t.vblank_get_counter = &dce_v8_0_vblank_get_counter,\n\t.backlight_set_level = &amdgpu_atombios_encoder_set_backlight_level,\n\t.backlight_get_level = &amdgpu_atombios_encoder_get_backlight_level,\n\t.hpd_sense = &dce_v8_0_hpd_sense,\n\t.hpd_set_polarity = &dce_v8_0_hpd_set_polarity,\n\t.hpd_get_gpio_reg = &dce_v8_0_hpd_get_gpio_reg,\n\t.page_flip = &dce_v8_0_page_flip,\n\t.page_flip_get_scanoutpos = &dce_v8_0_crtc_get_scanoutpos,\n\t.add_encoder = &dce_v8_0_encoder_add,\n\t.add_connector = &amdgpu_connector_add,\n};\n\nstatic void dce_v8_0_set_display_funcs(struct amdgpu_device *adev)\n{\n\tadev->mode_info.funcs = &dce_v8_0_display_funcs;\n}\n\nstatic const struct amdgpu_irq_src_funcs dce_v8_0_crtc_irq_funcs = {\n\t.set = dce_v8_0_set_crtc_interrupt_state,\n\t.process = dce_v8_0_crtc_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs dce_v8_0_pageflip_irq_funcs = {\n\t.set = dce_v8_0_set_pageflip_interrupt_state,\n\t.process = dce_v8_0_pageflip_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs dce_v8_0_hpd_irq_funcs = {\n\t.set = dce_v8_0_set_hpd_interrupt_state,\n\t.process = dce_v8_0_hpd_irq,\n};\n\nstatic void dce_v8_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tif (adev->mode_info.num_crtc > 0)\n\t\tadev->crtc_irq.num_types = AMDGPU_CRTC_IRQ_VLINE1 + adev->mode_info.num_crtc;\n\telse\n\t\tadev->crtc_irq.num_types = 0;\n\tadev->crtc_irq.funcs = &dce_v8_0_crtc_irq_funcs;\n\n\tadev->pageflip_irq.num_types = adev->mode_info.num_crtc;\n\tadev->pageflip_irq.funcs = &dce_v8_0_pageflip_irq_funcs;\n\n\tadev->hpd_irq.num_types = adev->mode_info.num_hpd;\n\tadev->hpd_irq.funcs = &dce_v8_0_hpd_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version dce_v8_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_DCE,\n\t.major = 8,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &dce_v8_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version dce_v8_1_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_DCE,\n\t.major = 8,\n\t.minor = 1,\n\t.rev = 0,\n\t.funcs = &dce_v8_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version dce_v8_2_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_DCE,\n\t.major = 8,\n\t.minor = 2,\n\t.rev = 0,\n\t.funcs = &dce_v8_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version dce_v8_3_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_DCE,\n\t.major = 8,\n\t.minor = 3,\n\t.rev = 0,\n\t.funcs = &dce_v8_0_ip_funcs,\n};\n\nconst struct amdgpu_ip_block_version dce_v8_5_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_DCE,\n\t.major = 8,\n\t.minor = 5,\n\t.rev = 0,\n\t.funcs = &dce_v8_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}