{
  "module_name": "gmc_v9_0.c",
  "hash_id": "ffef2af70d340ed5e24b5ef2bde70a0f52e0f103baf55265f5c64b29e13b8b0f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/gmc_v9_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/pci.h>\n\n#include <drm/drm_cache.h>\n\n#include \"amdgpu.h\"\n#include \"gmc_v9_0.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_gem.h\"\n\n#include \"gc/gc_9_0_sh_mask.h\"\n#include \"dce/dce_12_0_offset.h\"\n#include \"dce/dce_12_0_sh_mask.h\"\n#include \"vega10_enum.h\"\n#include \"mmhub/mmhub_1_0_offset.h\"\n#include \"athub/athub_1_0_sh_mask.h\"\n#include \"athub/athub_1_0_offset.h\"\n#include \"oss/osssys_4_0_offset.h\"\n\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"soc15_common.h\"\n#include \"umc/umc_6_0_sh_mask.h\"\n\n#include \"gfxhub_v1_0.h\"\n#include \"mmhub_v1_0.h\"\n#include \"athub_v1_0.h\"\n#include \"gfxhub_v1_1.h\"\n#include \"gfxhub_v1_2.h\"\n#include \"mmhub_v9_4.h\"\n#include \"mmhub_v1_7.h\"\n#include \"mmhub_v1_8.h\"\n#include \"umc_v6_1.h\"\n#include \"umc_v6_0.h\"\n#include \"umc_v6_7.h\"\n#include \"hdp_v4_0.h\"\n#include \"mca_v3_0.h\"\n\n#include \"ivsrcid/vmc/irqsrcs_vmc_1_0.h\"\n\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_xgmi.h\"\n\n#include \"amdgpu_reset.h\"\n\n \n#define mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION                                                          0x055d\n#define mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION_BASE_IDX                                                 2\n#define HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION__PRI_VIEWPORT_WIDTH__SHIFT                                        0x0\n#define HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION__PRI_VIEWPORT_HEIGHT__SHIFT                                       0x10\n#define HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION__PRI_VIEWPORT_WIDTH_MASK                                          0x00003FFFL\n#define HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION__PRI_VIEWPORT_HEIGHT_MASK                                         0x3FFF0000L\n#define mmDCHUBBUB_SDPIF_MMIO_CNTRL_0                                                                  0x049d\n#define mmDCHUBBUB_SDPIF_MMIO_CNTRL_0_BASE_IDX                                                         2\n\n#define mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION_DCN2                                                          0x05ea\n#define mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION_DCN2_BASE_IDX                                                 2\n\n#define MAX_MEM_RANGES 8\n\nstatic const char * const gfxhub_client_ids[] = {\n\t\"CB\",\n\t\"DB\",\n\t\"IA\",\n\t\"WD\",\n\t\"CPF\",\n\t\"CPC\",\n\t\"CPG\",\n\t\"RLC\",\n\t\"TCP\",\n\t\"SQC (inst)\",\n\t\"SQC (data)\",\n\t\"SQG\",\n\t\"PA\",\n};\n\nstatic const char *mmhub_client_ids_raven[][2] = {\n\t[0][0] = \"MP1\",\n\t[1][0] = \"MP0\",\n\t[2][0] = \"VCN\",\n\t[3][0] = \"VCNU\",\n\t[4][0] = \"HDP\",\n\t[5][0] = \"DCE\",\n\t[13][0] = \"UTCL2\",\n\t[19][0] = \"TLS\",\n\t[26][0] = \"OSS\",\n\t[27][0] = \"SDMA0\",\n\t[0][1] = \"MP1\",\n\t[1][1] = \"MP0\",\n\t[2][1] = \"VCN\",\n\t[3][1] = \"VCNU\",\n\t[4][1] = \"HDP\",\n\t[5][1] = \"XDP\",\n\t[6][1] = \"DBGU0\",\n\t[7][1] = \"DCE\",\n\t[8][1] = \"DCEDWB0\",\n\t[9][1] = \"DCEDWB1\",\n\t[26][1] = \"OSS\",\n\t[27][1] = \"SDMA0\",\n};\n\nstatic const char *mmhub_client_ids_renoir[][2] = {\n\t[0][0] = \"MP1\",\n\t[1][0] = \"MP0\",\n\t[2][0] = \"HDP\",\n\t[4][0] = \"DCEDMC\",\n\t[5][0] = \"DCEVGA\",\n\t[13][0] = \"UTCL2\",\n\t[19][0] = \"TLS\",\n\t[26][0] = \"OSS\",\n\t[27][0] = \"SDMA0\",\n\t[28][0] = \"VCN\",\n\t[29][0] = \"VCNU\",\n\t[30][0] = \"JPEG\",\n\t[0][1] = \"MP1\",\n\t[1][1] = \"MP0\",\n\t[2][1] = \"HDP\",\n\t[3][1] = \"XDP\",\n\t[6][1] = \"DBGU0\",\n\t[7][1] = \"DCEDMC\",\n\t[8][1] = \"DCEVGA\",\n\t[9][1] = \"DCEDWB\",\n\t[26][1] = \"OSS\",\n\t[27][1] = \"SDMA0\",\n\t[28][1] = \"VCN\",\n\t[29][1] = \"VCNU\",\n\t[30][1] = \"JPEG\",\n};\n\nstatic const char *mmhub_client_ids_vega10[][2] = {\n\t[0][0] = \"MP0\",\n\t[1][0] = \"UVD\",\n\t[2][0] = \"UVDU\",\n\t[3][0] = \"HDP\",\n\t[13][0] = \"UTCL2\",\n\t[14][0] = \"OSS\",\n\t[15][0] = \"SDMA1\",\n\t[32+0][0] = \"VCE0\",\n\t[32+1][0] = \"VCE0U\",\n\t[32+2][0] = \"XDMA\",\n\t[32+3][0] = \"DCE\",\n\t[32+4][0] = \"MP1\",\n\t[32+14][0] = \"SDMA0\",\n\t[0][1] = \"MP0\",\n\t[1][1] = \"UVD\",\n\t[2][1] = \"UVDU\",\n\t[3][1] = \"DBGU0\",\n\t[4][1] = \"HDP\",\n\t[5][1] = \"XDP\",\n\t[14][1] = \"OSS\",\n\t[15][1] = \"SDMA0\",\n\t[32+0][1] = \"VCE0\",\n\t[32+1][1] = \"VCE0U\",\n\t[32+2][1] = \"XDMA\",\n\t[32+3][1] = \"DCE\",\n\t[32+4][1] = \"DCEDWB\",\n\t[32+5][1] = \"MP1\",\n\t[32+6][1] = \"DBGU1\",\n\t[32+14][1] = \"SDMA1\",\n};\n\nstatic const char *mmhub_client_ids_vega12[][2] = {\n\t[0][0] = \"MP0\",\n\t[1][0] = \"VCE0\",\n\t[2][0] = \"VCE0U\",\n\t[3][0] = \"HDP\",\n\t[13][0] = \"UTCL2\",\n\t[14][0] = \"OSS\",\n\t[15][0] = \"SDMA1\",\n\t[32+0][0] = \"DCE\",\n\t[32+1][0] = \"XDMA\",\n\t[32+2][0] = \"UVD\",\n\t[32+3][0] = \"UVDU\",\n\t[32+4][0] = \"MP1\",\n\t[32+15][0] = \"SDMA0\",\n\t[0][1] = \"MP0\",\n\t[1][1] = \"VCE0\",\n\t[2][1] = \"VCE0U\",\n\t[3][1] = \"DBGU0\",\n\t[4][1] = \"HDP\",\n\t[5][1] = \"XDP\",\n\t[14][1] = \"OSS\",\n\t[15][1] = \"SDMA0\",\n\t[32+0][1] = \"DCE\",\n\t[32+1][1] = \"DCEDWB\",\n\t[32+2][1] = \"XDMA\",\n\t[32+3][1] = \"UVD\",\n\t[32+4][1] = \"UVDU\",\n\t[32+5][1] = \"MP1\",\n\t[32+6][1] = \"DBGU1\",\n\t[32+15][1] = \"SDMA1\",\n};\n\nstatic const char *mmhub_client_ids_vega20[][2] = {\n\t[0][0] = \"XDMA\",\n\t[1][0] = \"DCE\",\n\t[2][0] = \"VCE0\",\n\t[3][0] = \"VCE0U\",\n\t[4][0] = \"UVD\",\n\t[5][0] = \"UVD1U\",\n\t[13][0] = \"OSS\",\n\t[14][0] = \"HDP\",\n\t[15][0] = \"SDMA0\",\n\t[32+0][0] = \"UVD\",\n\t[32+1][0] = \"UVDU\",\n\t[32+2][0] = \"MP1\",\n\t[32+3][0] = \"MP0\",\n\t[32+12][0] = \"UTCL2\",\n\t[32+14][0] = \"SDMA1\",\n\t[0][1] = \"XDMA\",\n\t[1][1] = \"DCE\",\n\t[2][1] = \"DCEDWB\",\n\t[3][1] = \"VCE0\",\n\t[4][1] = \"VCE0U\",\n\t[5][1] = \"UVD1\",\n\t[6][1] = \"UVD1U\",\n\t[7][1] = \"DBGU0\",\n\t[8][1] = \"XDP\",\n\t[13][1] = \"OSS\",\n\t[14][1] = \"HDP\",\n\t[15][1] = \"SDMA0\",\n\t[32+0][1] = \"UVD\",\n\t[32+1][1] = \"UVDU\",\n\t[32+2][1] = \"DBGU1\",\n\t[32+3][1] = \"MP1\",\n\t[32+4][1] = \"MP0\",\n\t[32+14][1] = \"SDMA1\",\n};\n\nstatic const char *mmhub_client_ids_arcturus[][2] = {\n\t[0][0] = \"DBGU1\",\n\t[1][0] = \"XDP\",\n\t[2][0] = \"MP1\",\n\t[14][0] = \"HDP\",\n\t[171][0] = \"JPEG\",\n\t[172][0] = \"VCN\",\n\t[173][0] = \"VCNU\",\n\t[203][0] = \"JPEG1\",\n\t[204][0] = \"VCN1\",\n\t[205][0] = \"VCN1U\",\n\t[256][0] = \"SDMA0\",\n\t[257][0] = \"SDMA1\",\n\t[258][0] = \"SDMA2\",\n\t[259][0] = \"SDMA3\",\n\t[260][0] = \"SDMA4\",\n\t[261][0] = \"SDMA5\",\n\t[262][0] = \"SDMA6\",\n\t[263][0] = \"SDMA7\",\n\t[384][0] = \"OSS\",\n\t[0][1] = \"DBGU1\",\n\t[1][1] = \"XDP\",\n\t[2][1] = \"MP1\",\n\t[14][1] = \"HDP\",\n\t[171][1] = \"JPEG\",\n\t[172][1] = \"VCN\",\n\t[173][1] = \"VCNU\",\n\t[203][1] = \"JPEG1\",\n\t[204][1] = \"VCN1\",\n\t[205][1] = \"VCN1U\",\n\t[256][1] = \"SDMA0\",\n\t[257][1] = \"SDMA1\",\n\t[258][1] = \"SDMA2\",\n\t[259][1] = \"SDMA3\",\n\t[260][1] = \"SDMA4\",\n\t[261][1] = \"SDMA5\",\n\t[262][1] = \"SDMA6\",\n\t[263][1] = \"SDMA7\",\n\t[384][1] = \"OSS\",\n};\n\nstatic const char *mmhub_client_ids_aldebaran[][2] = {\n\t[2][0] = \"MP1\",\n\t[3][0] = \"MP0\",\n\t[32+1][0] = \"DBGU_IO0\",\n\t[32+2][0] = \"DBGU_IO2\",\n\t[32+4][0] = \"MPIO\",\n\t[96+11][0] = \"JPEG0\",\n\t[96+12][0] = \"VCN0\",\n\t[96+13][0] = \"VCNU0\",\n\t[128+11][0] = \"JPEG1\",\n\t[128+12][0] = \"VCN1\",\n\t[128+13][0] = \"VCNU1\",\n\t[160+1][0] = \"XDP\",\n\t[160+14][0] = \"HDP\",\n\t[256+0][0] = \"SDMA0\",\n\t[256+1][0] = \"SDMA1\",\n\t[256+2][0] = \"SDMA2\",\n\t[256+3][0] = \"SDMA3\",\n\t[256+4][0] = \"SDMA4\",\n\t[384+0][0] = \"OSS\",\n\t[2][1] = \"MP1\",\n\t[3][1] = \"MP0\",\n\t[32+1][1] = \"DBGU_IO0\",\n\t[32+2][1] = \"DBGU_IO2\",\n\t[32+4][1] = \"MPIO\",\n\t[96+11][1] = \"JPEG0\",\n\t[96+12][1] = \"VCN0\",\n\t[96+13][1] = \"VCNU0\",\n\t[128+11][1] = \"JPEG1\",\n\t[128+12][1] = \"VCN1\",\n\t[128+13][1] = \"VCNU1\",\n\t[160+1][1] = \"XDP\",\n\t[160+14][1] = \"HDP\",\n\t[256+0][1] = \"SDMA0\",\n\t[256+1][1] = \"SDMA1\",\n\t[256+2][1] = \"SDMA2\",\n\t[256+3][1] = \"SDMA3\",\n\t[256+4][1] = \"SDMA4\",\n\t[384+0][1] = \"OSS\",\n};\n\nstatic const struct soc15_reg_golden golden_settings_mmhub_1_0_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(MMHUB, 0, mmDAGB1_WRCLI2, 0x00000007, 0xfe5fe0fa),\n\tSOC15_REG_GOLDEN_VALUE(MMHUB, 0, mmMMEA1_DRAM_WR_CLI2GRP_MAP0, 0x00000030, 0x55555565)\n};\n\nstatic const struct soc15_reg_golden golden_settings_athub_1_0_0[] = {\n\tSOC15_REG_GOLDEN_VALUE(ATHUB, 0, mmRPB_ARB_CNTL, 0x0000ff00, 0x00000800),\n\tSOC15_REG_GOLDEN_VALUE(ATHUB, 0, mmRPB_ARB_CNTL2, 0x00ff00ff, 0x00080008)\n};\n\nstatic const uint32_t ecc_umc_mcumc_ctrl_addrs[] = {\n\t(0x000143c0 + 0x00000000),\n\t(0x000143c0 + 0x00000800),\n\t(0x000143c0 + 0x00001000),\n\t(0x000143c0 + 0x00001800),\n\t(0x000543c0 + 0x00000000),\n\t(0x000543c0 + 0x00000800),\n\t(0x000543c0 + 0x00001000),\n\t(0x000543c0 + 0x00001800),\n\t(0x000943c0 + 0x00000000),\n\t(0x000943c0 + 0x00000800),\n\t(0x000943c0 + 0x00001000),\n\t(0x000943c0 + 0x00001800),\n\t(0x000d43c0 + 0x00000000),\n\t(0x000d43c0 + 0x00000800),\n\t(0x000d43c0 + 0x00001000),\n\t(0x000d43c0 + 0x00001800),\n\t(0x001143c0 + 0x00000000),\n\t(0x001143c0 + 0x00000800),\n\t(0x001143c0 + 0x00001000),\n\t(0x001143c0 + 0x00001800),\n\t(0x001543c0 + 0x00000000),\n\t(0x001543c0 + 0x00000800),\n\t(0x001543c0 + 0x00001000),\n\t(0x001543c0 + 0x00001800),\n\t(0x001943c0 + 0x00000000),\n\t(0x001943c0 + 0x00000800),\n\t(0x001943c0 + 0x00001000),\n\t(0x001943c0 + 0x00001800),\n\t(0x001d43c0 + 0x00000000),\n\t(0x001d43c0 + 0x00000800),\n\t(0x001d43c0 + 0x00001000),\n\t(0x001d43c0 + 0x00001800),\n};\n\nstatic const uint32_t ecc_umc_mcumc_ctrl_mask_addrs[] = {\n\t(0x000143e0 + 0x00000000),\n\t(0x000143e0 + 0x00000800),\n\t(0x000143e0 + 0x00001000),\n\t(0x000143e0 + 0x00001800),\n\t(0x000543e0 + 0x00000000),\n\t(0x000543e0 + 0x00000800),\n\t(0x000543e0 + 0x00001000),\n\t(0x000543e0 + 0x00001800),\n\t(0x000943e0 + 0x00000000),\n\t(0x000943e0 + 0x00000800),\n\t(0x000943e0 + 0x00001000),\n\t(0x000943e0 + 0x00001800),\n\t(0x000d43e0 + 0x00000000),\n\t(0x000d43e0 + 0x00000800),\n\t(0x000d43e0 + 0x00001000),\n\t(0x000d43e0 + 0x00001800),\n\t(0x001143e0 + 0x00000000),\n\t(0x001143e0 + 0x00000800),\n\t(0x001143e0 + 0x00001000),\n\t(0x001143e0 + 0x00001800),\n\t(0x001543e0 + 0x00000000),\n\t(0x001543e0 + 0x00000800),\n\t(0x001543e0 + 0x00001000),\n\t(0x001543e0 + 0x00001800),\n\t(0x001943e0 + 0x00000000),\n\t(0x001943e0 + 0x00000800),\n\t(0x001943e0 + 0x00001000),\n\t(0x001943e0 + 0x00001800),\n\t(0x001d43e0 + 0x00000000),\n\t(0x001d43e0 + 0x00000800),\n\t(0x001d43e0 + 0x00001000),\n\t(0x001d43e0 + 0x00001800),\n};\n\nstatic int gmc_v9_0_ecc_interrupt_state(struct amdgpu_device *adev,\n\t\tstruct amdgpu_irq_src *src,\n\t\tunsigned int type,\n\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 bits, i, tmp, reg;\n\n\t \n\tif (adev->asic_type >= CHIP_VEGA20)\n\t\treturn 0;\n\n\tbits = 0x7f;\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tfor (i = 0; i < ARRAY_SIZE(ecc_umc_mcumc_ctrl_addrs); i++) {\n\t\t\treg = ecc_umc_mcumc_ctrl_addrs[i];\n\t\t\ttmp = RREG32(reg);\n\t\t\ttmp &= ~bits;\n\t\t\tWREG32(reg, tmp);\n\t\t}\n\t\tfor (i = 0; i < ARRAY_SIZE(ecc_umc_mcumc_ctrl_mask_addrs); i++) {\n\t\t\treg = ecc_umc_mcumc_ctrl_mask_addrs[i];\n\t\t\ttmp = RREG32(reg);\n\t\t\ttmp &= ~bits;\n\t\t\tWREG32(reg, tmp);\n\t\t}\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tfor (i = 0; i < ARRAY_SIZE(ecc_umc_mcumc_ctrl_addrs); i++) {\n\t\t\treg = ecc_umc_mcumc_ctrl_addrs[i];\n\t\t\ttmp = RREG32(reg);\n\t\t\ttmp |= bits;\n\t\t\tWREG32(reg, tmp);\n\t\t}\n\t\tfor (i = 0; i < ARRAY_SIZE(ecc_umc_mcumc_ctrl_mask_addrs); i++) {\n\t\t\treg = ecc_umc_mcumc_ctrl_mask_addrs[i];\n\t\t\ttmp = RREG32(reg);\n\t\t\ttmp |= bits;\n\t\t\tWREG32(reg, tmp);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_vm_fault_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *src,\n\t\t\t\t\tunsigned int type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tstruct amdgpu_vmhub *hub;\n\tu32 tmp, reg, bits, i, j;\n\n\tbits = VM_CONTEXT1_CNTL__RANGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__DUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__PDE0_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__VALID_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__READ_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__WRITE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\tVM_CONTEXT1_CNTL__EXECUTE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK;\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tfor_each_set_bit(j, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS) {\n\t\t\thub = &adev->vmhub[j];\n\t\t\tfor (i = 0; i < 16; i++) {\n\t\t\t\treg = hub->vm_context0_cntl + i;\n\n\t\t\t\t \n\t\t\t\tif (adev->in_s0ix && (j == AMDGPU_GFXHUB(0)))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (j >= AMDGPU_MMHUB0(0))\n\t\t\t\t\ttmp = RREG32_SOC15_IP(MMHUB, reg);\n\t\t\t\telse\n\t\t\t\t\ttmp = RREG32_SOC15_IP(GC, reg);\n\n\t\t\t\ttmp &= ~bits;\n\n\t\t\t\tif (j >= AMDGPU_MMHUB0(0))\n\t\t\t\t\tWREG32_SOC15_IP(MMHUB, reg, tmp);\n\t\t\t\telse\n\t\t\t\t\tWREG32_SOC15_IP(GC, reg, tmp);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\tfor_each_set_bit(j, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS) {\n\t\t\thub = &adev->vmhub[j];\n\t\t\tfor (i = 0; i < 16; i++) {\n\t\t\t\treg = hub->vm_context0_cntl + i;\n\n\t\t\t\t \n\t\t\t\tif (adev->in_s0ix && (j == AMDGPU_GFXHUB(0)))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (j >= AMDGPU_MMHUB0(0))\n\t\t\t\t\ttmp = RREG32_SOC15_IP(MMHUB, reg);\n\t\t\t\telse\n\t\t\t\t\ttmp = RREG32_SOC15_IP(GC, reg);\n\n\t\t\t\ttmp |= bits;\n\n\t\t\t\tif (j >= AMDGPU_MMHUB0(0))\n\t\t\t\t\tWREG32_SOC15_IP(MMHUB, reg, tmp);\n\t\t\t\telse\n\t\t\t\t\tWREG32_SOC15_IP(GC, reg, tmp);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tbool retry_fault = !!(entry->src_data[1] & 0x80);\n\tbool write_fault = !!(entry->src_data[1] & 0x20);\n\tuint32_t status = 0, cid = 0, rw = 0;\n\tstruct amdgpu_task_info task_info;\n\tstruct amdgpu_vmhub *hub;\n\tconst char *mmhub_cid;\n\tconst char *hub_name;\n\tu64 addr;\n\tuint32_t cam_index = 0;\n\tint ret, xcc_id = 0;\n\tuint32_t node_id;\n\n\tnode_id = entry->node_id;\n\n\taddr = (u64)entry->src_data[0] << 12;\n\taddr |= ((u64)entry->src_data[1] & 0xf) << 44;\n\n\tif (entry->client_id == SOC15_IH_CLIENTID_VMC) {\n\t\thub_name = \"mmhub0\";\n\t\thub = &adev->vmhub[AMDGPU_MMHUB0(node_id / 4)];\n\t} else if (entry->client_id == SOC15_IH_CLIENTID_VMC1) {\n\t\thub_name = \"mmhub1\";\n\t\thub = &adev->vmhub[AMDGPU_MMHUB1(0)];\n\t} else {\n\t\thub_name = \"gfxhub0\";\n\t\tif (adev->gfx.funcs->ih_node_to_logical_xcc) {\n\t\t\txcc_id = adev->gfx.funcs->ih_node_to_logical_xcc(adev,\n\t\t\t\tnode_id);\n\t\t\tif (xcc_id < 0)\n\t\t\t\txcc_id = 0;\n\t\t}\n\t\thub = &adev->vmhub[xcc_id];\n\t}\n\n\tif (retry_fault) {\n\t\tif (adev->irq.retry_cam_enabled) {\n\t\t\t \n\t\t\tif (entry->ih == &adev->irq.ih) {\n\t\t\t\tamdgpu_irq_delegate(adev, entry, 8);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\tcam_index = entry->src_data[2] & 0x3ff;\n\n\t\t\tret = amdgpu_vm_handle_fault(adev, entry->pasid, entry->vmid, node_id,\n\t\t\t\t\t\t     addr, write_fault);\n\t\t\tWDOORBELL32(adev->irq.retry_cam_doorbell_index, cam_index);\n\t\t\tif (ret)\n\t\t\t\treturn 1;\n\t\t} else {\n\t\t\t \n\t\t\tif (entry->ih != &adev->irq.ih_soft &&\n\t\t\t    amdgpu_gmc_filter_faults(adev, entry->ih, addr, entry->pasid,\n\t\t\t\t\t     entry->timestamp))\n\t\t\t\treturn 1;\n\n\t\t\t \n\t\t\tif (entry->ih == &adev->irq.ih) {\n\t\t\t\tamdgpu_irq_delegate(adev, entry, 8);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (amdgpu_vm_handle_fault(adev, entry->pasid, entry->vmid, node_id,\n\t\t\t\t\t\t   addr, write_fault))\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (!printk_ratelimit())\n\t\treturn 0;\n\n\n\tmemset(&task_info, 0, sizeof(struct amdgpu_task_info));\n\tamdgpu_vm_get_task_info(adev, entry->pasid, &task_info);\n\n\tdev_err(adev->dev,\n\t\t\"[%s] %s page fault (src_id:%u ring:%u vmid:%u pasid:%u, for process %s pid %d thread %s pid %d)\\n\",\n\t\thub_name, retry_fault ? \"retry\" : \"no-retry\",\n\t\tentry->src_id, entry->ring_id, entry->vmid,\n\t\tentry->pasid, task_info.process_name, task_info.tgid,\n\t\ttask_info.task_name, task_info.pid);\n\tdev_err(adev->dev, \"  in page starting at address 0x%016llx from IH client 0x%x (%s)\\n\",\n\t\taddr, entry->client_id,\n\t\tsoc15_ih_clientid_name[entry->client_id]);\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\tdev_err(adev->dev, \"  cookie node_id %d fault from die %s%d%s\\n\",\n\t\t\tnode_id, node_id % 4 == 3 ? \"RSV\" : \"AID\", node_id / 4,\n\t\t\tnode_id % 4 == 1 ? \".XCD0\" : node_id % 4 == 2 ? \".XCD1\" : \"\");\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\t \n\tif ((entry->vmid_src == AMDGPU_GFXHUB(0)) &&\n\t    (adev->ip_versions[GC_HWIP][0] < IP_VERSION(9, 4, 2)))\n\t\tRREG32(hub->vm_l2_pro_fault_status);\n\n\tstatus = RREG32(hub->vm_l2_pro_fault_status);\n\tcid = REG_GET_FIELD(status, VM_L2_PROTECTION_FAULT_STATUS, CID);\n\trw = REG_GET_FIELD(status, VM_L2_PROTECTION_FAULT_STATUS, RW);\n\tWREG32_P(hub->vm_l2_pro_fault_cntl, 1, ~1);\n\n\tdev_err(adev->dev,\n\t\t\"VM_L2_PROTECTION_FAULT_STATUS:0x%08X\\n\",\n\t\tstatus);\n\tif (entry->vmid_src == AMDGPU_GFXHUB(0)) {\n\t\tdev_err(adev->dev, \"\\t Faulty UTCL2 client ID: %s (0x%x)\\n\",\n\t\t\tcid >= ARRAY_SIZE(gfxhub_client_ids) ? \"unknown\" :\n\t\t\tgfxhub_client_ids[cid],\n\t\t\tcid);\n\t} else {\n\t\tswitch (adev->ip_versions[MMHUB_HWIP][0]) {\n\t\tcase IP_VERSION(9, 0, 0):\n\t\t\tmmhub_cid = mmhub_client_ids_vega10[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 3, 0):\n\t\t\tmmhub_cid = mmhub_client_ids_vega12[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 4, 0):\n\t\t\tmmhub_cid = mmhub_client_ids_vega20[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 4, 1):\n\t\t\tmmhub_cid = mmhub_client_ids_arcturus[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 1, 0):\n\t\tcase IP_VERSION(9, 2, 0):\n\t\t\tmmhub_cid = mmhub_client_ids_raven[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(1, 5, 0):\n\t\tcase IP_VERSION(2, 4, 0):\n\t\t\tmmhub_cid = mmhub_client_ids_renoir[cid][rw];\n\t\t\tbreak;\n\t\tcase IP_VERSION(1, 8, 0):\n\t\tcase IP_VERSION(9, 4, 2):\n\t\t\tmmhub_cid = mmhub_client_ids_aldebaran[cid][rw];\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tmmhub_cid = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tdev_err(adev->dev, \"\\t Faulty UTCL2 client ID: %s (0x%x)\\n\",\n\t\t\tmmhub_cid ? mmhub_cid : \"unknown\", cid);\n\t}\n\tdev_err(adev->dev, \"\\t MORE_FAULTS: 0x%lx\\n\",\n\t\tREG_GET_FIELD(status,\n\t\tVM_L2_PROTECTION_FAULT_STATUS, MORE_FAULTS));\n\tdev_err(adev->dev, \"\\t WALKER_ERROR: 0x%lx\\n\",\n\t\tREG_GET_FIELD(status,\n\t\tVM_L2_PROTECTION_FAULT_STATUS, WALKER_ERROR));\n\tdev_err(adev->dev, \"\\t PERMISSION_FAULTS: 0x%lx\\n\",\n\t\tREG_GET_FIELD(status,\n\t\tVM_L2_PROTECTION_FAULT_STATUS, PERMISSION_FAULTS));\n\tdev_err(adev->dev, \"\\t MAPPING_ERROR: 0x%lx\\n\",\n\t\tREG_GET_FIELD(status,\n\t\tVM_L2_PROTECTION_FAULT_STATUS, MAPPING_ERROR));\n\tdev_err(adev->dev, \"\\t RW: 0x%x\\n\", rw);\n\treturn 0;\n}\n\nstatic const struct amdgpu_irq_src_funcs gmc_v9_0_irq_funcs = {\n\t.set = gmc_v9_0_vm_fault_interrupt_state,\n\t.process = gmc_v9_0_process_interrupt,\n};\n\n\nstatic const struct amdgpu_irq_src_funcs gmc_v9_0_ecc_funcs = {\n\t.set = gmc_v9_0_ecc_interrupt_state,\n\t.process = amdgpu_umc_process_ecc_irq,\n};\n\nstatic void gmc_v9_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.vm_fault.num_types = 1;\n\tadev->gmc.vm_fault.funcs = &gmc_v9_0_irq_funcs;\n\n\tif (!amdgpu_sriov_vf(adev) &&\n\t    !adev->gmc.xgmi.connected_to_cpu) {\n\t\tadev->gmc.ecc_irq.num_types = 1;\n\t\tadev->gmc.ecc_irq.funcs = &gmc_v9_0_ecc_funcs;\n\t}\n}\n\nstatic uint32_t gmc_v9_0_get_invalidate_req(unsigned int vmid,\n\t\t\t\t\tuint32_t flush_type)\n{\n\tu32 req = 0;\n\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ,\n\t\t\t    PER_VMID_INVALIDATE_REQ, 1 << vmid);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, FLUSH_TYPE, flush_type);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, INVALIDATE_L2_PTES, 1);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, INVALIDATE_L2_PDE0, 1);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, INVALIDATE_L2_PDE1, 1);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, INVALIDATE_L2_PDE2, 1);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ, INVALIDATE_L1_PTES, 1);\n\treq = REG_SET_FIELD(req, VM_INVALIDATE_ENG0_REQ,\n\t\t\t    CLEAR_PROTECTION_FAULT_STATUS_ADDR,\t0);\n\n\treturn req;\n}\n\n \nstatic bool gmc_v9_0_use_invalidate_semaphore(struct amdgpu_device *adev,\n\t\t\t\t       uint32_t vmhub)\n{\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 2) ||\n\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\treturn false;\n\n\treturn ((vmhub == AMDGPU_MMHUB0(0) ||\n\t\t vmhub == AMDGPU_MMHUB1(0)) &&\n\t\t(!amdgpu_sriov_vf(adev)) &&\n\t\t(!(!(adev->apu_flags & AMD_APU_IS_RAVEN2) &&\n\t\t   (adev->apu_flags & AMD_APU_IS_PICASSO))));\n}\n\nstatic bool gmc_v9_0_get_atc_vmid_pasid_mapping_info(struct amdgpu_device *adev,\n\t\t\t\t\tuint8_t vmid, uint16_t *p_pasid)\n{\n\tuint32_t value;\n\n\tvalue = RREG32(SOC15_REG_OFFSET(ATHUB, 0, mmATC_VMID0_PASID_MAPPING)\n\t\t     + vmid);\n\t*p_pasid = value & ATC_VMID0_PASID_MAPPING__PASID_MASK;\n\n\treturn !!(value & ATC_VMID0_PASID_MAPPING__VALID_MASK);\n}\n\n \n\n \nstatic void gmc_v9_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,\n\t\t\t\t\tuint32_t vmhub, uint32_t flush_type)\n{\n\tbool use_semaphore = gmc_v9_0_use_invalidate_semaphore(adev, vmhub);\n\tconst unsigned int eng = 17;\n\tu32 j, inv_req, inv_req2, tmp;\n\tstruct amdgpu_vmhub *hub;\n\n\tBUG_ON(vmhub >= AMDGPU_MAX_VMHUBS);\n\n\thub = &adev->vmhub[vmhub];\n\tif (adev->gmc.xgmi.num_physical_nodes &&\n\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 0)) {\n\t\t \n\t\tinv_req = gmc_v9_0_get_invalidate_req(vmid, 2);\n\t\tinv_req2 = gmc_v9_0_get_invalidate_req(vmid, flush_type);\n\t} else if (flush_type == 2 &&\n\t\t   adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3) &&\n\t\t   adev->rev_id == 0) {\n\t\tinv_req = gmc_v9_0_get_invalidate_req(vmid, 0);\n\t\tinv_req2 = gmc_v9_0_get_invalidate_req(vmid, flush_type);\n\t} else {\n\t\tinv_req = gmc_v9_0_get_invalidate_req(vmid, flush_type);\n\t\tinv_req2 = 0;\n\t}\n\n\t \n\tif (adev->gfx.kiq[0].ring.sched.ready &&\n\t    (amdgpu_sriov_runtime(adev) || !amdgpu_sriov_vf(adev)) &&\n\t    down_read_trylock(&adev->reset_domain->sem)) {\n\t\tuint32_t req = hub->vm_inv_eng0_req + hub->eng_distance * eng;\n\t\tuint32_t ack = hub->vm_inv_eng0_ack + hub->eng_distance * eng;\n\n\t\tamdgpu_virt_kiq_reg_write_reg_wait(adev, req, ack, inv_req,\n\t\t\t\t\t\t   1 << vmid);\n\t\tup_read(&adev->reset_domain->sem);\n\t\treturn;\n\t}\n\n\tspin_lock(&adev->gmc.invalidate_lock);\n\n\t \n\n\t \n\tif (use_semaphore) {\n\t\tfor (j = 0; j < adev->usec_timeout; j++) {\n\t\t\t \n\t\t\tif (vmhub >= AMDGPU_MMHUB0(0))\n\t\t\t\ttmp = RREG32_SOC15_IP_NO_KIQ(MMHUB, hub->vm_inv_eng0_sem + hub->eng_distance * eng);\n\t\t\telse\n\t\t\t\ttmp = RREG32_SOC15_IP_NO_KIQ(GC, hub->vm_inv_eng0_sem + hub->eng_distance * eng);\n\t\t\tif (tmp & 0x1)\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t\tif (j >= adev->usec_timeout)\n\t\t\tDRM_ERROR(\"Timeout waiting for sem acquire in VM flush!\\n\");\n\t}\n\n\tdo {\n\t\tif (vmhub >= AMDGPU_MMHUB0(0))\n\t\t\tWREG32_SOC15_IP_NO_KIQ(MMHUB, hub->vm_inv_eng0_req + hub->eng_distance * eng, inv_req);\n\t\telse\n\t\t\tWREG32_SOC15_IP_NO_KIQ(GC, hub->vm_inv_eng0_req + hub->eng_distance * eng, inv_req);\n\n\t\t \n\t\tif ((vmhub == AMDGPU_GFXHUB(0)) &&\n\t\t    (adev->ip_versions[GC_HWIP][0] < IP_VERSION(9, 4, 2)))\n\t\t\tRREG32_NO_KIQ(hub->vm_inv_eng0_req +\n\t\t\t\t      hub->eng_distance * eng);\n\n\t\tfor (j = 0; j < adev->usec_timeout; j++) {\n\t\t\tif (vmhub >= AMDGPU_MMHUB0(0))\n\t\t\t\ttmp = RREG32_SOC15_IP_NO_KIQ(MMHUB, hub->vm_inv_eng0_ack + hub->eng_distance * eng);\n\t\t\telse\n\t\t\t\ttmp = RREG32_SOC15_IP_NO_KIQ(GC, hub->vm_inv_eng0_ack + hub->eng_distance * eng);\n\t\t\tif (tmp & (1 << vmid))\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t\tinv_req = inv_req2;\n\t\tinv_req2 = 0;\n\t} while (inv_req);\n\n\t \n\tif (use_semaphore) {\n\t\t \n\t\tif (vmhub >= AMDGPU_MMHUB0(0))\n\t\t\tWREG32_SOC15_IP_NO_KIQ(MMHUB, hub->vm_inv_eng0_sem + hub->eng_distance * eng, 0);\n\t\telse\n\t\t\tWREG32_SOC15_IP_NO_KIQ(GC, hub->vm_inv_eng0_sem + hub->eng_distance * eng, 0);\n\t}\n\n\tspin_unlock(&adev->gmc.invalidate_lock);\n\n\tif (j < adev->usec_timeout)\n\t\treturn;\n\n\tDRM_ERROR(\"Timeout waiting for VM flush ACK!\\n\");\n}\n\n \nstatic int gmc_v9_0_flush_gpu_tlb_pasid(struct amdgpu_device *adev,\n\t\t\t\t\tuint16_t pasid, uint32_t flush_type,\n\t\t\t\t\tbool all_hub, uint32_t inst)\n{\n\tint vmid, i;\n\tsigned long r;\n\tuint32_t seq;\n\tuint16_t queried_pasid;\n\tbool ret;\n\tu32 usec_timeout = amdgpu_sriov_vf(adev) ? SRIOV_USEC_TIMEOUT : adev->usec_timeout;\n\tstruct amdgpu_ring *ring = &adev->gfx.kiq[inst].ring;\n\tstruct amdgpu_kiq *kiq = &adev->gfx.kiq[inst];\n\n\tif (amdgpu_in_reset(adev))\n\t\treturn -EIO;\n\n\tif (ring->sched.ready && down_read_trylock(&adev->reset_domain->sem)) {\n\t\t \n\t\tbool vega20_xgmi_wa = (adev->gmc.xgmi.num_physical_nodes &&\n\t\t\t\t       adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 0));\n\t\t \n\t\tunsigned int ndw = kiq->pmf->invalidate_tlbs_size + 8;\n\n\t\tif (vega20_xgmi_wa)\n\t\t\tndw += kiq->pmf->invalidate_tlbs_size;\n\n\t\tspin_lock(&adev->gfx.kiq[inst].ring_lock);\n\t\t \n\t\tamdgpu_ring_alloc(ring, ndw);\n\t\tif (vega20_xgmi_wa)\n\t\t\tkiq->pmf->kiq_invalidate_tlbs(ring,\n\t\t\t\t\t\t      pasid, 2, all_hub);\n\n\t\tif (flush_type == 2 &&\n\t\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3) &&\n\t\t    adev->rev_id == 0)\n\t\t\tkiq->pmf->kiq_invalidate_tlbs(ring,\n\t\t\t\t\t\tpasid, 0, all_hub);\n\n\t\tkiq->pmf->kiq_invalidate_tlbs(ring,\n\t\t\t\t\tpasid, flush_type, all_hub);\n\t\tr = amdgpu_fence_emit_polling(ring, &seq, MAX_KIQ_REG_WAIT);\n\t\tif (r) {\n\t\t\tamdgpu_ring_undo(ring);\n\t\t\tspin_unlock(&adev->gfx.kiq[inst].ring_lock);\n\t\t\tup_read(&adev->reset_domain->sem);\n\t\t\treturn -ETIME;\n\t\t}\n\n\t\tamdgpu_ring_commit(ring);\n\t\tspin_unlock(&adev->gfx.kiq[inst].ring_lock);\n\t\tr = amdgpu_fence_wait_polling(ring, seq, usec_timeout);\n\t\tif (r < 1) {\n\t\t\tdev_err(adev->dev, \"wait for kiq fence error: %ld.\\n\", r);\n\t\t\tup_read(&adev->reset_domain->sem);\n\t\t\treturn -ETIME;\n\t\t}\n\t\tup_read(&adev->reset_domain->sem);\n\t\treturn 0;\n\t}\n\n\tfor (vmid = 1; vmid < 16; vmid++) {\n\n\t\tret = gmc_v9_0_get_atc_vmid_pasid_mapping_info(adev, vmid,\n\t\t\t\t&queried_pasid);\n\t\tif (ret && queried_pasid == pasid) {\n\t\t\tif (all_hub) {\n\t\t\t\tfor_each_set_bit(i, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS)\n\t\t\t\t\tgmc_v9_0_flush_gpu_tlb(adev, vmid,\n\t\t\t\t\t\t\ti, flush_type);\n\t\t\t} else {\n\t\t\t\tgmc_v9_0_flush_gpu_tlb(adev, vmid,\n\t\t\t\t\t\tAMDGPU_GFXHUB(0), flush_type);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n\n}\n\nstatic uint64_t gmc_v9_0_emit_flush_gpu_tlb(struct amdgpu_ring *ring,\n\t\t\t\t\t    unsigned int vmid, uint64_t pd_addr)\n{\n\tbool use_semaphore = gmc_v9_0_use_invalidate_semaphore(ring->adev, ring->vm_hub);\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_vmhub *hub = &adev->vmhub[ring->vm_hub];\n\tuint32_t req = gmc_v9_0_get_invalidate_req(vmid, 0);\n\tunsigned int eng = ring->vm_inv_eng;\n\n\t \n\n\t \n\tif (use_semaphore)\n\t\t \n\t\tamdgpu_ring_emit_reg_wait(ring,\n\t\t\t\t\t  hub->vm_inv_eng0_sem +\n\t\t\t\t\t  hub->eng_distance * eng, 0x1, 0x1);\n\n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      lower_32_bits(pd_addr));\n\n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_hi32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      upper_32_bits(pd_addr));\n\n\tamdgpu_ring_emit_reg_write_reg_wait(ring, hub->vm_inv_eng0_req +\n\t\t\t\t\t    hub->eng_distance * eng,\n\t\t\t\t\t    hub->vm_inv_eng0_ack +\n\t\t\t\t\t    hub->eng_distance * eng,\n\t\t\t\t\t    req, 1 << vmid);\n\n\t \n\tif (use_semaphore)\n\t\t \n\t\tamdgpu_ring_emit_wreg(ring, hub->vm_inv_eng0_sem +\n\t\t\t\t      hub->eng_distance * eng, 0);\n\n\treturn pd_addr;\n}\n\nstatic void gmc_v9_0_emit_pasid_mapping(struct amdgpu_ring *ring, unsigned int vmid,\n\t\t\t\t\tunsigned int pasid)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t reg;\n\n\t \n\tif (ring->vm_hub == AMDGPU_MMHUB1(0))\n\t\treturn;\n\n\tif (ring->vm_hub == AMDGPU_GFXHUB(0))\n\t\treg = SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT) + vmid;\n\telse\n\t\treg = SOC15_REG_OFFSET(OSSSYS, 0, mmIH_VMID_0_LUT_MM) + vmid;\n\n\tamdgpu_ring_emit_wreg(ring, reg, pasid);\n}\n\n \n\nstatic uint64_t gmc_v9_0_map_mtype(struct amdgpu_device *adev, uint32_t flags)\n\n{\n\tswitch (flags) {\n\tcase AMDGPU_VM_MTYPE_DEFAULT:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_NC);\n\tcase AMDGPU_VM_MTYPE_NC:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_NC);\n\tcase AMDGPU_VM_MTYPE_WC:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_WC);\n\tcase AMDGPU_VM_MTYPE_RW:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_RW);\n\tcase AMDGPU_VM_MTYPE_CC:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_CC);\n\tcase AMDGPU_VM_MTYPE_UC:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_UC);\n\tdefault:\n\t\treturn AMDGPU_PTE_MTYPE_VG10(MTYPE_NC);\n\t}\n}\n\nstatic void gmc_v9_0_get_vm_pde(struct amdgpu_device *adev, int level,\n\t\t\t\tuint64_t *addr, uint64_t *flags)\n{\n\tif (!(*flags & AMDGPU_PDE_PTE) && !(*flags & AMDGPU_PTE_SYSTEM))\n\t\t*addr = amdgpu_gmc_vram_mc2pa(adev, *addr);\n\tBUG_ON(*addr & 0xFFFF00000000003FULL);\n\n\tif (!adev->gmc.translate_further)\n\t\treturn;\n\n\tif (level == AMDGPU_VM_PDB1) {\n\t\t \n\t\tif (!(*flags & AMDGPU_PDE_PTE))\n\t\t\t*flags |= AMDGPU_PDE_BFS(0x9);\n\n\t} else if (level == AMDGPU_VM_PDB0) {\n\t\tif (*flags & AMDGPU_PDE_PTE) {\n\t\t\t*flags &= ~AMDGPU_PDE_PTE;\n\t\t\tif (!(*flags & AMDGPU_PTE_VALID))\n\t\t\t\t*addr |= 1 << PAGE_SHIFT;\n\t\t} else {\n\t\t\t*flags |= AMDGPU_PTE_TF;\n\t\t}\n\t}\n}\n\nstatic void gmc_v9_0_get_coherence_flags(struct amdgpu_device *adev,\n\t\t\t\t\t struct amdgpu_bo *bo,\n\t\t\t\t\t struct amdgpu_bo_va_mapping *mapping,\n\t\t\t\t\t uint64_t *flags)\n{\n\tstruct amdgpu_device *bo_adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tbool is_vram = bo->tbo.resource->mem_type == TTM_PL_VRAM;\n\tbool coherent = bo->flags & AMDGPU_GEM_CREATE_COHERENT;\n\tbool uncached = bo->flags & AMDGPU_GEM_CREATE_UNCACHED;\n\tstruct amdgpu_vm *vm = mapping->bo_va->base.vm;\n\tunsigned int mtype_local, mtype;\n\tbool snoop = false;\n\tbool is_local;\n\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 1):\n\tcase IP_VERSION(9, 4, 2):\n\t\tif (is_vram) {\n\t\t\tif (bo_adev == adev) {\n\t\t\t\tif (uncached)\n\t\t\t\t\tmtype = MTYPE_UC;\n\t\t\t\telse if (coherent)\n\t\t\t\t\tmtype = MTYPE_CC;\n\t\t\t\telse\n\t\t\t\t\tmtype = MTYPE_RW;\n\t\t\t\t \n\t\t\t\tif ((adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 2) ||\n\t\t\t\t     adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3)) &&\n\t\t\t\t    adev->gmc.xgmi.connected_to_cpu)\n\t\t\t\t\tsnoop = true;\n\t\t\t} else {\n\t\t\t\tif (uncached || coherent)\n\t\t\t\t\tmtype = MTYPE_UC;\n\t\t\t\telse\n\t\t\t\t\tmtype = MTYPE_NC;\n\t\t\t\tif (mapping->bo_va->is_xgmi)\n\t\t\t\t\tsnoop = true;\n\t\t\t}\n\t\t} else {\n\t\t\tif (uncached || coherent)\n\t\t\t\tmtype = MTYPE_UC;\n\t\t\telse\n\t\t\t\tmtype = MTYPE_NC;\n\t\t\t \n\t\t\tsnoop = true;\n\t\t}\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 3):\n\t\t \n\t\tmtype_local = MTYPE_RW;\n\t\tif (amdgpu_mtype_local == 1) {\n\t\t\tDRM_INFO_ONCE(\"Using MTYPE_NC for local memory\\n\");\n\t\t\tmtype_local = MTYPE_NC;\n\t\t} else if (amdgpu_mtype_local == 2) {\n\t\t\tDRM_INFO_ONCE(\"Using MTYPE_CC for local memory\\n\");\n\t\t\tmtype_local = MTYPE_CC;\n\t\t} else {\n\t\t\tDRM_INFO_ONCE(\"Using MTYPE_RW for local memory\\n\");\n\t\t}\n\t\tis_local = (!is_vram && (adev->flags & AMD_IS_APU) &&\n\t\t\t    num_possible_nodes() <= 1) ||\n\t\t\t   (is_vram && adev == bo_adev &&\n\t\t\t    KFD_XCP_MEM_ID(adev, bo->xcp_id) == vm->mem_id);\n\t\tsnoop = true;\n\t\tif (uncached) {\n\t\t\tmtype = MTYPE_UC;\n\t\t} else if (adev->flags & AMD_IS_APU) {\n\t\t\tmtype = is_local ? mtype_local : MTYPE_NC;\n\t\t} else {\n\t\t\t \n\t\t\tif (is_local)\n\t\t\t\tmtype = mtype_local;\n\t\t\telse if (is_vram)\n\t\t\t\tmtype = MTYPE_NC;\n\t\t\telse\n\t\t\t\tmtype = MTYPE_UC;\n\t\t}\n\n\t\tbreak;\n\tdefault:\n\t\tif (uncached || coherent)\n\t\t\tmtype = MTYPE_UC;\n\t\telse\n\t\t\tmtype = MTYPE_NC;\n\n\t\t \n\t\tif (!is_vram)\n\t\t\tsnoop = true;\n\t}\n\n\tif (mtype != MTYPE_NC)\n\t\t*flags = (*flags & ~AMDGPU_PTE_MTYPE_VG10_MASK) |\n\t\t\t AMDGPU_PTE_MTYPE_VG10(mtype);\n\t*flags |= snoop ? AMDGPU_PTE_SNOOPED : 0;\n}\n\nstatic void gmc_v9_0_get_vm_pte(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_bo_va_mapping *mapping,\n\t\t\t\tuint64_t *flags)\n{\n\tstruct amdgpu_bo *bo = mapping->bo_va->base.bo;\n\n\t*flags &= ~AMDGPU_PTE_EXECUTABLE;\n\t*flags |= mapping->flags & AMDGPU_PTE_EXECUTABLE;\n\n\t*flags &= ~AMDGPU_PTE_MTYPE_VG10_MASK;\n\t*flags |= mapping->flags & AMDGPU_PTE_MTYPE_VG10_MASK;\n\n\tif (mapping->flags & AMDGPU_PTE_PRT) {\n\t\t*flags |= AMDGPU_PTE_PRT;\n\t\t*flags &= ~AMDGPU_PTE_VALID;\n\t}\n\n\tif (bo && bo->tbo.resource)\n\t\tgmc_v9_0_get_coherence_flags(adev, mapping->bo_va->base.bo,\n\t\t\t\t\t     mapping, flags);\n}\n\nstatic void gmc_v9_0_override_vm_pte_flags(struct amdgpu_device *adev,\n\t\t\t\t\t   struct amdgpu_vm *vm,\n\t\t\t\t\t   uint64_t addr, uint64_t *flags)\n{\n\tint local_node, nid;\n\n\t \n\tif (adev->ip_versions[GC_HWIP][0] != IP_VERSION(9, 4, 3))\n\t\treturn;\n\n\t \n\tif (!adev->ram_is_direct_mapped) {\n\t\tdev_dbg(adev->dev, \"RAM is not direct mapped\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif ((*flags & AMDGPU_PTE_MTYPE_VG10_MASK) !=\n\t    AMDGPU_PTE_MTYPE_VG10(MTYPE_NC)) {\n\t\tdev_dbg(adev->dev, \"MTYPE is not NC\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (adev->gmc.is_app_apu && vm->mem_id >= 0) {\n\t\tlocal_node = adev->gmc.mem_partitions[vm->mem_id].numa.node;\n\t} else {\n\t\tdev_dbg(adev->dev, \"Only native mode APU is supported.\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (!page_is_ram(addr >> PAGE_SHIFT)) {\n\t\tdev_dbg(adev->dev, \"Page is not RAM.\\n\");\n\t\treturn;\n\t}\n\tnid = pfn_to_nid(addr >> PAGE_SHIFT);\n\tdev_dbg(adev->dev, \"vm->mem_id=%d, local_node=%d, nid=%d\\n\",\n\t\tvm->mem_id, local_node, nid);\n\tif (nid == local_node) {\n\t\tuint64_t old_flags = *flags;\n\t\tunsigned int mtype_local = MTYPE_RW;\n\n\t\tif (amdgpu_mtype_local == 1)\n\t\t\tmtype_local = MTYPE_NC;\n\t\telse if (amdgpu_mtype_local == 2)\n\t\t\tmtype_local = MTYPE_CC;\n\n\t\t*flags = (*flags & ~AMDGPU_PTE_MTYPE_VG10_MASK) |\n\t\t\t AMDGPU_PTE_MTYPE_VG10(mtype_local);\n\t\tdev_dbg(adev->dev, \"flags updated from %llx to %llx\\n\",\n\t\t\told_flags, *flags);\n\t}\n}\n\nstatic unsigned int gmc_v9_0_get_vbios_fb_size(struct amdgpu_device *adev)\n{\n\tu32 d1vga_control = RREG32_SOC15(DCE, 0, mmD1VGA_CONTROL);\n\tunsigned int size;\n\n\t \n\n\tif (REG_GET_FIELD(d1vga_control, D1VGA_CONTROL, D1VGA_MODE_ENABLE)) {\n\t\tsize = AMDGPU_VBIOS_VGA_ALLOCATION;\n\t} else {\n\t\tu32 viewport;\n\n\t\tswitch (adev->ip_versions[DCE_HWIP][0]) {\n\t\tcase IP_VERSION(1, 0, 0):\n\t\tcase IP_VERSION(1, 0, 1):\n\t\t\tviewport = RREG32_SOC15(DCE, 0, mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION);\n\t\t\tsize = (REG_GET_FIELD(viewport,\n\t\t\t\t\t      HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION, PRI_VIEWPORT_HEIGHT) *\n\t\t\t\tREG_GET_FIELD(viewport,\n\t\t\t\t\t      HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION, PRI_VIEWPORT_WIDTH) *\n\t\t\t\t4);\n\t\t\tbreak;\n\t\tcase IP_VERSION(2, 1, 0):\n\t\t\tviewport = RREG32_SOC15(DCE, 0, mmHUBP0_DCSURF_PRI_VIEWPORT_DIMENSION_DCN2);\n\t\t\tsize = (REG_GET_FIELD(viewport,\n\t\t\t\t\t      HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION, PRI_VIEWPORT_HEIGHT) *\n\t\t\t\tREG_GET_FIELD(viewport,\n\t\t\t\t\t      HUBP0_DCSURF_PRI_VIEWPORT_DIMENSION, PRI_VIEWPORT_WIDTH) *\n\t\t\t\t4);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tviewport = RREG32_SOC15(DCE, 0, mmSCL0_VIEWPORT_SIZE);\n\t\t\tsize = (REG_GET_FIELD(viewport, SCL0_VIEWPORT_SIZE, VIEWPORT_HEIGHT) *\n\t\t\t\tREG_GET_FIELD(viewport, SCL0_VIEWPORT_SIZE, VIEWPORT_WIDTH) *\n\t\t\t\t4);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn size;\n}\n\nstatic enum amdgpu_memory_partition\ngmc_v9_0_get_memory_partition(struct amdgpu_device *adev, u32 *supp_modes)\n{\n\tenum amdgpu_memory_partition mode = UNKNOWN_MEMORY_PARTITION_MODE;\n\n\tif (adev->nbio.funcs->get_memory_partition_mode)\n\t\tmode = adev->nbio.funcs->get_memory_partition_mode(adev,\n\t\t\t\t\t\t\t\t   supp_modes);\n\n\treturn mode;\n}\n\nstatic enum amdgpu_memory_partition\ngmc_v9_0_query_memory_partition(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn AMDGPU_NPS1_PARTITION_MODE;\n\n\treturn gmc_v9_0_get_memory_partition(adev, NULL);\n}\n\nstatic const struct amdgpu_gmc_funcs gmc_v9_0_gmc_funcs = {\n\t.flush_gpu_tlb = gmc_v9_0_flush_gpu_tlb,\n\t.flush_gpu_tlb_pasid = gmc_v9_0_flush_gpu_tlb_pasid,\n\t.emit_flush_gpu_tlb = gmc_v9_0_emit_flush_gpu_tlb,\n\t.emit_pasid_mapping = gmc_v9_0_emit_pasid_mapping,\n\t.map_mtype = gmc_v9_0_map_mtype,\n\t.get_vm_pde = gmc_v9_0_get_vm_pde,\n\t.get_vm_pte = gmc_v9_0_get_vm_pte,\n\t.override_vm_pte_flags = gmc_v9_0_override_vm_pte_flags,\n\t.get_vbios_fb_size = gmc_v9_0_get_vbios_fb_size,\n\t.query_mem_partition_mode = &gmc_v9_0_query_memory_partition,\n};\n\nstatic void gmc_v9_0_set_gmc_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.gmc_funcs = &gmc_v9_0_gmc_funcs;\n}\n\nstatic void gmc_v9_0_set_umc_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[UMC_HWIP][0]) {\n\tcase IP_VERSION(6, 0, 0):\n\t\tadev->umc.funcs = &umc_v6_0_funcs;\n\t\tbreak;\n\tcase IP_VERSION(6, 1, 1):\n\t\tadev->umc.max_ras_err_cnt_per_query = UMC_V6_1_TOTAL_CHANNEL_NUM;\n\t\tadev->umc.channel_inst_num = UMC_V6_1_CHANNEL_INSTANCE_NUM;\n\t\tadev->umc.umc_inst_num = UMC_V6_1_UMC_INSTANCE_NUM;\n\t\tadev->umc.channel_offs = UMC_V6_1_PER_CHANNEL_OFFSET_VG20;\n\t\tadev->umc.retire_unit = 1;\n\t\tadev->umc.channel_idx_tbl = &umc_v6_1_channel_idx_tbl[0][0];\n\t\tadev->umc.ras = &umc_v6_1_ras;\n\t\tbreak;\n\tcase IP_VERSION(6, 1, 2):\n\t\tadev->umc.max_ras_err_cnt_per_query = UMC_V6_1_TOTAL_CHANNEL_NUM;\n\t\tadev->umc.channel_inst_num = UMC_V6_1_CHANNEL_INSTANCE_NUM;\n\t\tadev->umc.umc_inst_num = UMC_V6_1_UMC_INSTANCE_NUM;\n\t\tadev->umc.channel_offs = UMC_V6_1_PER_CHANNEL_OFFSET_ARCT;\n\t\tadev->umc.retire_unit = 1;\n\t\tadev->umc.channel_idx_tbl = &umc_v6_1_channel_idx_tbl[0][0];\n\t\tadev->umc.ras = &umc_v6_1_ras;\n\t\tbreak;\n\tcase IP_VERSION(6, 7, 0):\n\t\tadev->umc.max_ras_err_cnt_per_query =\n\t\t\tUMC_V6_7_TOTAL_CHANNEL_NUM * UMC_V6_7_BAD_PAGE_NUM_PER_CHANNEL;\n\t\tadev->umc.channel_inst_num = UMC_V6_7_CHANNEL_INSTANCE_NUM;\n\t\tadev->umc.umc_inst_num = UMC_V6_7_UMC_INSTANCE_NUM;\n\t\tadev->umc.channel_offs = UMC_V6_7_PER_CHANNEL_OFFSET;\n\t\tadev->umc.retire_unit = (UMC_V6_7_NA_MAP_PA_NUM * 2);\n\t\tif (!adev->gmc.xgmi.connected_to_cpu)\n\t\t\tadev->umc.ras = &umc_v6_7_ras;\n\t\tif (1 & adev->smuio.funcs->get_die_id(adev))\n\t\t\tadev->umc.channel_idx_tbl = &umc_v6_7_channel_idx_tbl_first[0][0];\n\t\telse\n\t\t\tadev->umc.channel_idx_tbl = &umc_v6_7_channel_idx_tbl_second[0][0];\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v9_0_set_mmhub_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[MMHUB_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 1):\n\t\tadev->mmhub.funcs = &mmhub_v9_4_funcs;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 2):\n\t\tadev->mmhub.funcs = &mmhub_v1_7_funcs;\n\t\tbreak;\n\tcase IP_VERSION(1, 8, 0):\n\t\tadev->mmhub.funcs = &mmhub_v1_8_funcs;\n\t\tbreak;\n\tdefault:\n\t\tadev->mmhub.funcs = &mmhub_v1_0_funcs;\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v9_0_set_mmhub_ras_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[MMHUB_HWIP][0]) {\n\tcase IP_VERSION(9, 4, 0):\n\t\tadev->mmhub.ras = &mmhub_v1_0_ras;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 1):\n\t\tadev->mmhub.ras = &mmhub_v9_4_ras;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 2):\n\t\tadev->mmhub.ras = &mmhub_v1_7_ras;\n\t\tbreak;\n\tcase IP_VERSION(1, 8, 0):\n\t\tadev->mmhub.ras = &mmhub_v1_8_ras;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v9_0_set_gfxhub_funcs(struct amdgpu_device *adev)\n{\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\tadev->gfxhub.funcs = &gfxhub_v1_2_funcs;\n\telse\n\t\tadev->gfxhub.funcs = &gfxhub_v1_0_funcs;\n}\n\nstatic void gmc_v9_0_set_hdp_ras_funcs(struct amdgpu_device *adev)\n{\n\tadev->hdp.ras = &hdp_v4_0_ras;\n}\n\nstatic void gmc_v9_0_set_mca_ras_funcs(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_mca *mca = &adev->mca;\n\n\t \n\tswitch (adev->ip_versions[UMC_HWIP][0]) {\n\tcase IP_VERSION(6, 7, 0):\n\t\tif (!adev->gmc.xgmi.connected_to_cpu) {\n\t\t\tmca->mp0.ras = &mca_v3_0_mp0_ras;\n\t\t\tmca->mp1.ras = &mca_v3_0_mp1_ras;\n\t\t\tmca->mpio.ras = &mca_v3_0_mpio_ras;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void gmc_v9_0_set_xgmi_ras_funcs(struct amdgpu_device *adev)\n{\n\tif (!adev->gmc.xgmi.connected_to_cpu)\n\t\tadev->gmc.xgmi.ras = &xgmi_ras;\n}\n\nstatic int gmc_v9_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 0) ||\n\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 1) ||\n\t    adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\tadev->gmc.xgmi.supported = true;\n\n\tif (adev->ip_versions[XGMI_HWIP][0] == IP_VERSION(6, 1, 0)) {\n\t\tadev->gmc.xgmi.supported = true;\n\t\tadev->gmc.xgmi.connected_to_cpu =\n\t\t\tadev->smuio.funcs->is_host_gpu_xgmi_supported(adev);\n\t}\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3)) {\n\t\tenum amdgpu_pkg_type pkg_type =\n\t\t\tadev->smuio.funcs->get_pkg_type(adev);\n\t\t \n\t\tadev->gmc.is_app_apu = (pkg_type == AMDGPU_PKG_TYPE_APU &&\n\t\t\t\t\t!pci_resource_len(adev->pdev, 0));\n\t}\n\n\tgmc_v9_0_set_gmc_funcs(adev);\n\tgmc_v9_0_set_irq_funcs(adev);\n\tgmc_v9_0_set_umc_funcs(adev);\n\tgmc_v9_0_set_mmhub_funcs(adev);\n\tgmc_v9_0_set_mmhub_ras_funcs(adev);\n\tgmc_v9_0_set_gfxhub_funcs(adev);\n\tgmc_v9_0_set_hdp_ras_funcs(adev);\n\tgmc_v9_0_set_mca_ras_funcs(adev);\n\tgmc_v9_0_set_xgmi_ras_funcs(adev);\n\n\tadev->gmc.shared_aperture_start = 0x2000000000000000ULL;\n\tadev->gmc.shared_aperture_end =\n\t\tadev->gmc.shared_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.private_aperture_start = 0x1000000000000000ULL;\n\tadev->gmc.private_aperture_end =\n\t\tadev->gmc.private_aperture_start + (4ULL << 30) - 1;\n\tadev->gmc.noretry_flags = AMDGPU_VM_NORETRY_FLAGS_TF;\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_gmc_allocate_vm_inv_eng(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tif (!amdgpu_sriov_vf(adev) &&\n\t    (adev->ip_versions[UMC_HWIP][0] == IP_VERSION(6, 0, 0))) {\n\t\tif (!(adev->ras_enabled & (1 << AMDGPU_RAS_BLOCK__UMC))) {\n\t\t\tif (adev->df.funcs &&\n\t\t\t    adev->df.funcs->enable_ecc_force_par_wr_rmw)\n\t\t\t\tadev->df.funcs->enable_ecc_force_par_wr_rmw(adev, false);\n\t\t}\n\t}\n\n\tif (!amdgpu_persistent_edc_harvesting_supported(adev)) {\n\t\tif (adev->mmhub.ras && adev->mmhub.ras->ras_block.hw_ops &&\n\t\t    adev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\tadev->mmhub.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\n\t\tif (adev->hdp.ras && adev->hdp.ras->ras_block.hw_ops &&\n\t\t    adev->hdp.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\tadev->hdp.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\t}\n\n\tr = amdgpu_gmc_ras_late_init(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_irq_get(adev, &adev->gmc.vm_fault, 0);\n}\n\nstatic void gmc_v9_0_vram_gtt_location(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_gmc *mc)\n{\n\tu64 base = adev->mmhub.funcs->get_fb_location(adev);\n\n\t \n\tbase += adev->gmc.xgmi.physical_node_id * adev->gmc.xgmi.node_segment_size;\n\tif (adev->gmc.xgmi.connected_to_cpu) {\n\t\tamdgpu_gmc_sysvm_location(adev, mc);\n\t} else {\n\t\tamdgpu_gmc_vram_location(adev, mc, base);\n\t\tamdgpu_gmc_gart_location(adev, mc);\n\t\tamdgpu_gmc_agp_location(adev, mc);\n\t}\n\t \n\tadev->vm_manager.vram_base_offset = adev->gfxhub.funcs->get_mc_fb_offset(adev);\n\n\t \n\tadev->vm_manager.vram_base_offset +=\n\t\tadev->gmc.xgmi.physical_node_id * adev->gmc.xgmi.node_segment_size;\n}\n\n \nstatic int gmc_v9_0_mc_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t \n\tif (!adev->gmc.is_app_apu) {\n\t\tadev->gmc.mc_vram_size =\n\t\t\tadev->nbio.funcs->get_memsize(adev) * 1024ULL * 1024ULL;\n\t} else {\n\t\tDRM_DEBUG(\"Set mc_vram_size = 0 for APP APU\\n\");\n\t\tadev->gmc.mc_vram_size = 0;\n\t}\n\tadev->gmc.real_vram_size = adev->gmc.mc_vram_size;\n\n\tif (!(adev->flags & AMD_IS_APU) &&\n\t    !adev->gmc.xgmi.connected_to_cpu) {\n\t\tr = amdgpu_device_resize_fb_bar(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\tadev->gmc.aper_base = pci_resource_start(adev->pdev, 0);\n\tadev->gmc.aper_size = pci_resource_len(adev->pdev, 0);\n\n#ifdef CONFIG_X86_64\n\t \n\n\t \n\tif ((!amdgpu_sriov_vf(adev) &&\n\t\t(adev->flags & AMD_IS_APU) && !amdgpu_passthrough(adev)) ||\n\t    (adev->gmc.xgmi.supported &&\n\t     adev->gmc.xgmi.connected_to_cpu)) {\n\t\tadev->gmc.aper_base =\n\t\t\tadev->gfxhub.funcs->get_mc_fb_offset(adev) +\n\t\t\tadev->gmc.xgmi.physical_node_id *\n\t\t\tadev->gmc.xgmi.node_segment_size;\n\t\tadev->gmc.aper_size = adev->gmc.real_vram_size;\n\t}\n\n#endif\n\tadev->gmc.visible_vram_size = adev->gmc.aper_size;\n\n\t \n\tif (amdgpu_gart_size == -1) {\n\t\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\t\tcase IP_VERSION(9, 0, 1):   \n\t\tcase IP_VERSION(9, 2, 1):   \n\t\tcase IP_VERSION(9, 4, 0):\n\t\tcase IP_VERSION(9, 4, 1):\n\t\tcase IP_VERSION(9, 4, 2):\n\t\tcase IP_VERSION(9, 4, 3):\n\t\tdefault:\n\t\t\tadev->gmc.gart_size = 512ULL << 20;\n\t\t\tbreak;\n\t\tcase IP_VERSION(9, 1, 0):    \n\t\tcase IP_VERSION(9, 2, 2):    \n\t\tcase IP_VERSION(9, 3, 0):\n\t\t\tadev->gmc.gart_size = 1024ULL << 20;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tadev->gmc.gart_size = (u64)amdgpu_gart_size << 20;\n\t}\n\n\tadev->gmc.gart_size += adev->pm.smu_prv_buffer_size;\n\n\tgmc_v9_0_vram_gtt_location(adev, &adev->gmc);\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_gart_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->gart.bo) {\n\t\tWARN(1, \"VEGA10 PCIE GART already initialized\\n\");\n\t\treturn 0;\n\t}\n\n\tif (adev->gmc.xgmi.connected_to_cpu) {\n\t\tadev->gmc.vmid0_page_table_depth = 1;\n\t\tadev->gmc.vmid0_page_table_block_size = 12;\n\t} else {\n\t\tadev->gmc.vmid0_page_table_depth = 0;\n\t\tadev->gmc.vmid0_page_table_block_size = 0;\n\t}\n\n\t \n\tr = amdgpu_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\tadev->gart.table_size = adev->gart.num_gpu_pages * 8;\n\tadev->gart.gart_pte_flags = AMDGPU_PTE_MTYPE_VG10(MTYPE_UC) |\n\t\t\t\t AMDGPU_PTE_EXECUTABLE;\n\n\tif (!adev->gmc.real_vram_size) {\n\t\tdev_info(adev->dev, \"Put GART in system memory for APU\\n\");\n\t\tr = amdgpu_gart_table_ram_alloc(adev);\n\t\tif (r)\n\t\t\tdev_err(adev->dev, \"Failed to allocate GART in system memory\\n\");\n\t} else {\n\t\tr = amdgpu_gart_table_vram_alloc(adev);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\t\tr = amdgpu_gmc_pdb0_alloc(adev);\n\t}\n\n\treturn r;\n}\n\n \nstatic void gmc_v9_0_save_registers(struct amdgpu_device *adev)\n{\n\tif ((adev->ip_versions[DCE_HWIP][0] == IP_VERSION(1, 0, 0)) ||\n\t    (adev->ip_versions[DCE_HWIP][0] == IP_VERSION(1, 0, 1)))\n\t\tadev->gmc.sdpif_register = RREG32_SOC15(DCE, 0, mmDCHUBBUB_SDPIF_MMIO_CNTRL_0);\n}\n\nstatic bool gmc_v9_0_validate_partition_info(struct amdgpu_device *adev)\n{\n\tenum amdgpu_memory_partition mode;\n\tu32 supp_modes;\n\tbool valid;\n\n\tmode = gmc_v9_0_get_memory_partition(adev, &supp_modes);\n\n\t \n\tif ((mode != UNKNOWN_MEMORY_PARTITION_MODE) &&\n\t    !(BIT(mode - 1) & supp_modes))\n\t\treturn false;\n\n\tswitch (mode) {\n\tcase UNKNOWN_MEMORY_PARTITION_MODE:\n\tcase AMDGPU_NPS1_PARTITION_MODE:\n\t\tvalid = (adev->gmc.num_mem_partitions == 1);\n\t\tbreak;\n\tcase AMDGPU_NPS2_PARTITION_MODE:\n\t\tvalid = (adev->gmc.num_mem_partitions == 2);\n\t\tbreak;\n\tcase AMDGPU_NPS4_PARTITION_MODE:\n\t\tvalid = (adev->gmc.num_mem_partitions == 3 ||\n\t\t\t adev->gmc.num_mem_partitions == 4);\n\t\tbreak;\n\tdefault:\n\t\tvalid = false;\n\t}\n\n\treturn valid;\n}\n\nstatic bool gmc_v9_0_is_node_present(int *node_ids, int num_ids, int nid)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < num_ids; ++i)\n\t\tif (node_ids[i] == nid)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic void\ngmc_v9_0_init_acpi_mem_ranges(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_mem_partition_info *mem_ranges)\n{\n\tint num_ranges = 0, ret, mem_groups;\n\tstruct amdgpu_numa_info numa_info;\n\tint node_ids[MAX_MEM_RANGES];\n\tint num_xcc, xcc_id;\n\tuint32_t xcc_mask;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\txcc_mask = (1U << num_xcc) - 1;\n\tmem_groups = hweight32(adev->aid_mask);\n\n\tfor_each_inst(xcc_id, xcc_mask)\t{\n\t\tret = amdgpu_acpi_get_mem_info(adev, xcc_id, &numa_info);\n\t\tif (ret)\n\t\t\tcontinue;\n\n\t\tif (numa_info.nid == NUMA_NO_NODE) {\n\t\t\tmem_ranges[0].size = numa_info.size;\n\t\t\tmem_ranges[0].numa.node = numa_info.nid;\n\t\t\tnum_ranges = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (gmc_v9_0_is_node_present(node_ids, num_ranges,\n\t\t\t\t\t     numa_info.nid))\n\t\t\tcontinue;\n\n\t\tnode_ids[num_ranges] = numa_info.nid;\n\t\tmem_ranges[num_ranges].numa.node = numa_info.nid;\n\t\tmem_ranges[num_ranges].size = numa_info.size;\n\t\t++num_ranges;\n\t}\n\n\tadev->gmc.num_mem_partitions = num_ranges;\n\n\t \n\tif (adev->gmc.num_mem_partitions == 1) {\n\t\tmem_ranges[0].size = mem_ranges[0].size * (mem_groups - 1);\n\t\tdo_div(mem_ranges[0].size, mem_groups);\n\t}\n}\n\nstatic void\ngmc_v9_0_init_sw_mem_ranges(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_mem_partition_info *mem_ranges)\n{\n\tenum amdgpu_memory_partition mode;\n\tu32 start_addr = 0, size;\n\tint i;\n\n\tmode = gmc_v9_0_query_memory_partition(adev);\n\n\tswitch (mode) {\n\tcase UNKNOWN_MEMORY_PARTITION_MODE:\n\tcase AMDGPU_NPS1_PARTITION_MODE:\n\t\tadev->gmc.num_mem_partitions = 1;\n\t\tbreak;\n\tcase AMDGPU_NPS2_PARTITION_MODE:\n\t\tadev->gmc.num_mem_partitions = 2;\n\t\tbreak;\n\tcase AMDGPU_NPS4_PARTITION_MODE:\n\t\tif (adev->flags & AMD_IS_APU)\n\t\t\tadev->gmc.num_mem_partitions = 3;\n\t\telse\n\t\t\tadev->gmc.num_mem_partitions = 4;\n\t\tbreak;\n\tdefault:\n\t\tadev->gmc.num_mem_partitions = 1;\n\t\tbreak;\n\t}\n\n\tsize = adev->gmc.real_vram_size >> AMDGPU_GPU_PAGE_SHIFT;\n\tsize /= adev->gmc.num_mem_partitions;\n\n\tfor (i = 0; i < adev->gmc.num_mem_partitions; ++i) {\n\t\tmem_ranges[i].range.fpfn = start_addr;\n\t\tmem_ranges[i].size = ((u64)size << AMDGPU_GPU_PAGE_SHIFT);\n\t\tmem_ranges[i].range.lpfn = start_addr + size - 1;\n\t\tstart_addr += size;\n\t}\n\n\t \n\tmem_ranges[adev->gmc.num_mem_partitions - 1].range.lpfn =\n\t\t(adev->gmc.real_vram_size >> AMDGPU_GPU_PAGE_SHIFT) - 1;\n\tmem_ranges[adev->gmc.num_mem_partitions - 1].size =\n\t\tadev->gmc.real_vram_size -\n\t\t((u64)mem_ranges[adev->gmc.num_mem_partitions - 1].range.fpfn\n\t\t << AMDGPU_GPU_PAGE_SHIFT);\n}\n\nstatic int gmc_v9_0_init_mem_ranges(struct amdgpu_device *adev)\n{\n\tbool valid;\n\n\tadev->gmc.mem_partitions = kzalloc(\n\t\tMAX_MEM_RANGES * sizeof(struct amdgpu_mem_partition_info),\n\t\tGFP_KERNEL);\n\n\tif (!adev->gmc.mem_partitions)\n\t\treturn -ENOMEM;\n\n\t \n\tif (adev->gmc.is_app_apu)\n\t\tgmc_v9_0_init_acpi_mem_ranges(adev, adev->gmc.mem_partitions);\n\telse\n\t\tgmc_v9_0_init_sw_mem_ranges(adev, adev->gmc.mem_partitions);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\tvalid = true;\n\telse\n\t\tvalid = gmc_v9_0_validate_partition_info(adev);\n\tif (!valid) {\n\t\t \n\t\tdev_WARN(adev->dev,\n\t\t\t \"Mem ranges not matching with hardware config\");\n\t}\n\n\treturn 0;\n}\n\nstatic void gmc_v9_4_3_init_vram_info(struct amdgpu_device *adev)\n{\n\tstatic const u32 regBIF_BIOS_SCRATCH_4 = 0x50;\n\tu32 vram_info;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tvram_info = RREG32(regBIF_BIOS_SCRATCH_4);\n\t\tadev->gmc.vram_vendor = vram_info & 0xF;\n\t}\n\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_HBM;\n\tadev->gmc.vram_width = 128 * 64;\n}\n\nstatic int gmc_v9_0_sw_init(void *handle)\n{\n\tint r, vram_width = 0, vram_type = 0, vram_vendor = 0, dma_addr_bits;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tunsigned long inst_mask = adev->aid_mask;\n\n\tadev->gfxhub.funcs->init(adev);\n\n\tadev->mmhub.funcs->init(adev);\n\n\tspin_lock_init(&adev->gmc.invalidate_lock);\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3)) {\n\t\tgmc_v9_4_3_init_vram_info(adev);\n\t} else if (!adev->bios) {\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_DDR4;\n\t\t\tadev->gmc.vram_width = 64 * 64;\n\t\t} else {\n\t\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_HBM;\n\t\t\tadev->gmc.vram_width = 128 * 64;\n\t\t}\n\t} else {\n\t\tr = amdgpu_atomfirmware_get_vram_info(adev,\n\t\t\t&vram_width, &vram_type, &vram_vendor);\n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\t \n\t\t\tadev->gmc.vram_width = 2048;\n\t\telse if (amdgpu_emu_mode != 1)\n\t\t\tadev->gmc.vram_width = vram_width;\n\n\t\tif (!adev->gmc.vram_width) {\n\t\t\tint chansize, numchan;\n\n\t\t\t \n\t\t\tif (adev->flags & AMD_IS_APU)\n\t\t\t\tchansize = 64;\n\t\t\telse\n\t\t\t\tchansize = 128;\n\t\t\tif (adev->df.funcs &&\n\t\t\t    adev->df.funcs->get_hbm_channel_number) {\n\t\t\t\tnumchan = adev->df.funcs->get_hbm_channel_number(adev);\n\t\t\t\tadev->gmc.vram_width = numchan * chansize;\n\t\t\t}\n\t\t}\n\n\t\tadev->gmc.vram_type = vram_type;\n\t\tadev->gmc.vram_vendor = vram_vendor;\n\t}\n\tswitch (adev->ip_versions[GC_HWIP][0]) {\n\tcase IP_VERSION(9, 1, 0):\n\tcase IP_VERSION(9, 2, 2):\n\t\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\t\tset_bit(AMDGPU_MMHUB0(0), adev->vmhubs_mask);\n\n\t\tif (adev->rev_id == 0x0 || adev->rev_id == 0x1) {\n\t\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 48);\n\t\t} else {\n\t\t\t \n\t\t\tamdgpu_vm_adjust_size(adev, 128 * 1024 + 512, 9, 2, 48);\n\t\t\tadev->gmc.translate_further =\n\t\t\t\tadev->vm_manager.num_level > 1;\n\t\t}\n\t\tbreak;\n\tcase IP_VERSION(9, 0, 1):\n\tcase IP_VERSION(9, 2, 1):\n\tcase IP_VERSION(9, 4, 0):\n\tcase IP_VERSION(9, 3, 0):\n\tcase IP_VERSION(9, 4, 2):\n\t\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\t\tset_bit(AMDGPU_MMHUB0(0), adev->vmhubs_mask);\n\n\t\t \n\t\t \n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 47);\n\t\telse\n\t\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 48);\n\t\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 2))\n\t\t\tadev->gmc.translate_further = adev->vm_manager.num_level > 1;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 1):\n\t\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\t\tset_bit(AMDGPU_MMHUB0(0), adev->vmhubs_mask);\n\t\tset_bit(AMDGPU_MMHUB1(0), adev->vmhubs_mask);\n\n\t\t \n\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 48);\n\t\tadev->gmc.translate_further = adev->vm_manager.num_level > 1;\n\t\tbreak;\n\tcase IP_VERSION(9, 4, 3):\n\t\tbitmap_set(adev->vmhubs_mask, AMDGPU_GFXHUB(0),\n\t\t\t\t  NUM_XCC(adev->gfx.xcc_mask));\n\n\t\tinst_mask <<= AMDGPU_MMHUB0(0);\n\t\tbitmap_or(adev->vmhubs_mask, adev->vmhubs_mask, &inst_mask, 32);\n\n\t\tamdgpu_vm_adjust_size(adev, 256 * 1024, 9, 3, 48);\n\t\tadev->gmc.translate_further = adev->vm_manager.num_level > 1;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VMC, VMC_1_0__SRCID__VM_FAULT,\n\t\t\t\t&adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 1)) {\n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VMC1, VMC_1_0__SRCID__VM_FAULT,\n\t\t\t\t\t&adev->gmc.vm_fault);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_UTCL2, UTCL2_1_0__SRCID__FAULT,\n\t\t\t\t&adev->gmc.vm_fault);\n\n\tif (r)\n\t\treturn r;\n\n\tif (!amdgpu_sriov_vf(adev) &&\n\t    !adev->gmc.xgmi.connected_to_cpu) {\n\t\t \n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_DF, 0,\n\t\t\t\t      &adev->gmc.ecc_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tadev->gmc.mc_mask = 0xffffffffffffULL;  \n\n\tdma_addr_bits = adev->ip_versions[GC_HWIP][0] >= IP_VERSION(9, 4, 2) ? 48:44;\n\tr = dma_set_mask_and_coherent(adev->dev, DMA_BIT_MASK(dma_addr_bits));\n\tif (r) {\n\t\tdev_warn(adev->dev, \"amdgpu: No suitable DMA available.\\n\");\n\t\treturn r;\n\t}\n\tadev->need_swiotlb = drm_need_swiotlb(dma_addr_bits);\n\n\tr = gmc_v9_0_mc_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_gmc_get_vbios_allocations(adev);\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3)) {\n\t\tr = gmc_v9_0_init_mem_ranges(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_bo_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = gmc_v9_0_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tadev->vm_manager.first_kfd_vmid =\n\t\t(adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 1) ||\n\t\t adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 2) ||\n\t\t adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3)) ? 3 : 8;\n\n\tamdgpu_vm_manager_init(adev);\n\n\tgmc_v9_0_save_registers(adev);\n\n\tr = amdgpu_gmc_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\tamdgpu_gmc_sysfs_init(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\tamdgpu_gmc_sysfs_fini(adev);\n\n\tamdgpu_gmc_ras_fini(adev);\n\tamdgpu_gem_force_release(adev);\n\tamdgpu_vm_manager_fini(adev);\n\tif (!adev->gmc.real_vram_size) {\n\t\tdev_info(adev->dev, \"Put GART in system memory for APU free\\n\");\n\t\tamdgpu_gart_table_ram_free(adev);\n\t} else {\n\t\tamdgpu_gart_table_vram_free(adev);\n\t}\n\tamdgpu_bo_free_kernel(&adev->gmc.pdb0_bo, NULL, &adev->gmc.ptr_pdb0);\n\tamdgpu_bo_fini(adev);\n\n\tadev->gmc.num_mem_partitions = 0;\n\tkfree(adev->gmc.mem_partitions);\n\n\treturn 0;\n}\n\nstatic void gmc_v9_0_init_golden_registers(struct amdgpu_device *adev)\n{\n\n\tswitch (adev->ip_versions[MMHUB_HWIP][0]) {\n\tcase IP_VERSION(9, 0, 0):\n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\tbreak;\n\t\tfallthrough;\n\tcase IP_VERSION(9, 4, 0):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_mmhub_1_0_0,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_mmhub_1_0_0));\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_athub_1_0_0,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_athub_1_0_0));\n\t\tbreak;\n\tcase IP_VERSION(9, 1, 0):\n\tcase IP_VERSION(9, 2, 0):\n\t\t \n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_athub_1_0_0,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_athub_1_0_0));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nvoid gmc_v9_0_restore_registers(struct amdgpu_device *adev)\n{\n\tif ((adev->ip_versions[DCE_HWIP][0] == IP_VERSION(1, 0, 0)) ||\n\t    (adev->ip_versions[DCE_HWIP][0] == IP_VERSION(1, 0, 1))) {\n\t\tWREG32_SOC15(DCE, 0, mmDCHUBBUB_SDPIF_MMIO_CNTRL_0, adev->gmc.sdpif_register);\n\t\tWARN_ON(adev->gmc.sdpif_register !=\n\t\t\tRREG32_SOC15(DCE, 0, mmDCHUBBUB_SDPIF_MMIO_CNTRL_0));\n\t}\n}\n\n \nstatic int gmc_v9_0_gart_enable(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\tamdgpu_gmc_init_pdb0(adev);\n\n\tif (adev->gart.bo == NULL) {\n\t\tdev_err(adev->dev, \"No VRAM object for PCIE GART.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tamdgpu_gtt_mgr_recover(&adev->mman.gtt_mgr);\n\n\tif (!adev->in_s0ix) {\n\t\tr = adev->gfxhub.funcs->gart_enable(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = adev->mmhub.funcs->gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tDRM_INFO(\"PCIE GART of %uM enabled.\\n\",\n\t\t (unsigned int)(adev->gmc.gart_size >> 20));\n\tif (adev->gmc.pdb0_bo)\n\t\tDRM_INFO(\"PDB0 located at 0x%016llX\\n\",\n\t\t\t\t(unsigned long long)amdgpu_bo_gpu_offset(adev->gmc.pdb0_bo));\n\tDRM_INFO(\"PTB located at 0x%016llX\\n\",\n\t\t\t(unsigned long long)amdgpu_bo_gpu_offset(adev->gart.bo));\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool value;\n\tint i, r;\n\n\t \n\tgmc_v9_0_init_golden_registers(adev);\n\n\tif (adev->mode_info.num_crtc) {\n\t\t \n\t\tWREG32_FIELD15(DCE, 0, VGA_HDP_CONTROL, VGA_MEMORY_DISABLE, 1);\n\t\t \n\t\tWREG32_FIELD15(DCE, 0, VGA_RENDER_CONTROL, VGA_VSTATUS_CNTL, 0);\n\t}\n\n\tif (adev->mmhub.funcs->update_power_gating)\n\t\tadev->mmhub.funcs->update_power_gating(adev, true);\n\n\tadev->hdp.funcs->init_registers(adev);\n\n\t \n\tadev->hdp.funcs->flush_hdp(adev, NULL);\n\n\tif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_ALWAYS)\n\t\tvalue = false;\n\telse\n\t\tvalue = true;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tif (!adev->in_s0ix)\n\t\t\tadev->gfxhub.funcs->set_fault_enable_default(adev, value);\n\t\tadev->mmhub.funcs->set_fault_enable_default(adev, value);\n\t}\n\tfor_each_set_bit(i, adev->vmhubs_mask, AMDGPU_MAX_VMHUBS) {\n\t\tif (adev->in_s0ix && (i == AMDGPU_GFXHUB(0)))\n\t\t\tcontinue;\n\t\tgmc_v9_0_flush_gpu_tlb(adev, 0, i, 0);\n\t}\n\n\tif (adev->umc.funcs && adev->umc.funcs->init_registers)\n\t\tadev->umc.funcs->init_registers(adev);\n\n\tr = gmc_v9_0_gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_emu_mode == 1)\n\t\treturn amdgpu_gmc_vram_checking(adev);\n\telse\n\t\treturn r;\n}\n\n \nstatic void gmc_v9_0_gart_disable(struct amdgpu_device *adev)\n{\n\tif (!adev->in_s0ix)\n\t\tadev->gfxhub.funcs->gart_disable(adev);\n\tadev->mmhub.funcs->gart_disable(adev);\n}\n\nstatic int gmc_v9_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v9_0_gart_disable(adev);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tDRM_DEBUG(\"For SRIOV client, shouldn't do anything.\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (adev->mmhub.funcs->update_power_gating)\n\t\tadev->mmhub.funcs->update_power_gating(adev, false);\n\n\tamdgpu_irq_put(adev, &adev->gmc.vm_fault, 0);\n\n\treturn 0;\n}\n\nstatic int gmc_v9_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn gmc_v9_0_hw_fini(adev);\n}\n\nstatic int gmc_v9_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = gmc_v9_0_hw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vmid_reset_all(adev);\n\n\treturn 0;\n}\n\nstatic bool gmc_v9_0_is_idle(void *handle)\n{\n\t \n\treturn true;\n}\n\nstatic int gmc_v9_0_wait_for_idle(void *handle)\n{\n\t \n\treturn 0;\n}\n\nstatic int gmc_v9_0_soft_reset(void *handle)\n{\n\t \n\treturn 0;\n}\n\nstatic int gmc_v9_0_set_clockgating_state(void *handle,\n\t\t\t\t\tenum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->mmhub.funcs->set_clockgating(adev, state);\n\n\tathub_v1_0_set_clockgating(adev, state);\n\n\treturn 0;\n}\n\nstatic void gmc_v9_0_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->mmhub.funcs->get_clockgating(adev, flags);\n\n\tathub_v1_0_get_clockgating(adev, flags);\n}\n\nstatic int gmc_v9_0_set_powergating_state(void *handle,\n\t\t\t\t\tenum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nconst struct amd_ip_funcs gmc_v9_0_ip_funcs = {\n\t.name = \"gmc_v9_0\",\n\t.early_init = gmc_v9_0_early_init,\n\t.late_init = gmc_v9_0_late_init,\n\t.sw_init = gmc_v9_0_sw_init,\n\t.sw_fini = gmc_v9_0_sw_fini,\n\t.hw_init = gmc_v9_0_hw_init,\n\t.hw_fini = gmc_v9_0_hw_fini,\n\t.suspend = gmc_v9_0_suspend,\n\t.resume = gmc_v9_0_resume,\n\t.is_idle = gmc_v9_0_is_idle,\n\t.wait_for_idle = gmc_v9_0_wait_for_idle,\n\t.soft_reset = gmc_v9_0_soft_reset,\n\t.set_clockgating_state = gmc_v9_0_set_clockgating_state,\n\t.set_powergating_state = gmc_v9_0_set_powergating_state,\n\t.get_clockgating_state = gmc_v9_0_get_clockgating_state,\n};\n\nconst struct amdgpu_ip_block_version gmc_v9_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GMC,\n\t.major = 9,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &gmc_v9_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}