{
  "module_name": "amdgpu_ttm.c",
  "hash_id": "cee82c5c9ac82023db640fb03fc1de14d3ba46f173c66e559fe4ea63c581760c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c",
  "human_readable_source": " \n \n\n#include <linux/dma-mapping.h>\n#include <linux/iommu.h>\n#include <linux/pagemap.h>\n#include <linux/sched/task.h>\n#include <linux/sched/mm.h>\n#include <linux/seq_file.h>\n#include <linux/slab.h>\n#include <linux/swap.h>\n#include <linux/dma-buf.h>\n#include <linux/sizes.h>\n#include <linux/module.h>\n\n#include <drm/drm_drv.h>\n#include <drm/ttm/ttm_bo.h>\n#include <drm/ttm/ttm_placement.h>\n#include <drm/ttm/ttm_range_manager.h>\n#include <drm/ttm/ttm_tt.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_object.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_sdma.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_hmm.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_res_cursor.h\"\n#include \"bif/bif_4_1_d.h\"\n\nMODULE_IMPORT_NS(DMA_BUF);\n\n#define AMDGPU_TTM_VRAM_MAX_DW_READ\t((size_t)128)\n\nstatic int amdgpu_ttm_backend_bind(struct ttm_device *bdev,\n\t\t\t\t   struct ttm_tt *ttm,\n\t\t\t\t   struct ttm_resource *bo_mem);\nstatic void amdgpu_ttm_backend_unbind(struct ttm_device *bdev,\n\t\t\t\t      struct ttm_tt *ttm);\n\nstatic int amdgpu_ttm_init_on_chip(struct amdgpu_device *adev,\n\t\t\t\t    unsigned int type,\n\t\t\t\t    uint64_t size_in_page)\n{\n\treturn ttm_range_man_init(&adev->mman.bdev, type,\n\t\t\t\t  false, size_in_page);\n}\n\n \nstatic void amdgpu_evict_flags(struct ttm_buffer_object *bo,\n\t\t\t\tstruct ttm_placement *placement)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct amdgpu_bo *abo;\n\tstatic const struct ttm_place placements = {\n\t\t.fpfn = 0,\n\t\t.lpfn = 0,\n\t\t.mem_type = TTM_PL_SYSTEM,\n\t\t.flags = 0\n\t};\n\n\t \n\tif (bo->type == ttm_bo_type_sg) {\n\t\tplacement->num_placement = 0;\n\t\tplacement->num_busy_placement = 0;\n\t\treturn;\n\t}\n\n\t \n\tif (!amdgpu_bo_is_amdgpu_bo(bo)) {\n\t\tplacement->placement = &placements;\n\t\tplacement->busy_placement = &placements;\n\t\tplacement->num_placement = 1;\n\t\tplacement->num_busy_placement = 1;\n\t\treturn;\n\t}\n\n\tabo = ttm_to_amdgpu_bo(bo);\n\tif (abo->flags & AMDGPU_GEM_CREATE_DISCARDABLE) {\n\t\tplacement->num_placement = 0;\n\t\tplacement->num_busy_placement = 0;\n\t\treturn;\n\t}\n\n\tswitch (bo->resource->mem_type) {\n\tcase AMDGPU_PL_GDS:\n\tcase AMDGPU_PL_GWS:\n\tcase AMDGPU_PL_OA:\n\tcase AMDGPU_PL_DOORBELL:\n\t\tplacement->num_placement = 0;\n\t\tplacement->num_busy_placement = 0;\n\t\treturn;\n\n\tcase TTM_PL_VRAM:\n\t\tif (!adev->mman.buffer_funcs_enabled) {\n\t\t\t \n\t\t\tamdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_CPU);\n\t\t} else if (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t\t   !(abo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED) &&\n\t\t\t   amdgpu_bo_in_cpu_visible_vram(abo)) {\n\n\t\t\t \n\t\t\tamdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t\tabo->placements[0].fpfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;\n\t\t\tabo->placements[0].lpfn = 0;\n\t\t\tabo->placement.busy_placement = &abo->placements[1];\n\t\t\tabo->placement.num_busy_placement = 1;\n\t\t} else {\n\t\t\t \n\t\t\tamdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_GTT |\n\t\t\t\t\t\t\tAMDGPU_GEM_DOMAIN_CPU);\n\t\t}\n\t\tbreak;\n\tcase TTM_PL_TT:\n\tcase AMDGPU_PL_PREEMPT:\n\tdefault:\n\t\tamdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_CPU);\n\t\tbreak;\n\t}\n\t*placement = abo->placement;\n}\n\n \nstatic int amdgpu_ttm_map_buffer(struct ttm_buffer_object *bo,\n\t\t\t\t struct ttm_resource *mem,\n\t\t\t\t struct amdgpu_res_cursor *mm_cur,\n\t\t\t\t unsigned int window, struct amdgpu_ring *ring,\n\t\t\t\t bool tmz, uint64_t *size, uint64_t *addr)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned int offset, num_pages, num_dw, num_bytes;\n\tuint64_t src_addr, dst_addr;\n\tstruct amdgpu_job *job;\n\tvoid *cpu_addr;\n\tuint64_t flags;\n\tunsigned int i;\n\tint r;\n\n\tBUG_ON(adev->mman.buffer_funcs->copy_max_bytes <\n\t       AMDGPU_GTT_MAX_TRANSFER_SIZE * 8);\n\n\tif (WARN_ON(mem->mem_type == AMDGPU_PL_PREEMPT))\n\t\treturn -EINVAL;\n\n\t \n\tif (!tmz && mem->start != AMDGPU_BO_INVALID_OFFSET) {\n\t\t*addr = amdgpu_ttm_domain_start(adev, mem->mem_type) +\n\t\t\tmm_cur->start;\n\t\treturn 0;\n\t}\n\n\n\t \n\toffset = mm_cur->start & ~PAGE_MASK;\n\n\tnum_pages = PFN_UP(*size + offset);\n\tnum_pages = min_t(uint32_t, num_pages, AMDGPU_GTT_MAX_TRANSFER_SIZE);\n\n\t*size = min(*size, (uint64_t)num_pages * PAGE_SIZE - offset);\n\n\t*addr = adev->gmc.gart_start;\n\t*addr += (u64)window * AMDGPU_GTT_MAX_TRANSFER_SIZE *\n\t\tAMDGPU_GPU_PAGE_SIZE;\n\t*addr += offset;\n\n\tnum_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);\n\tnum_bytes = num_pages * 8 * AMDGPU_GPU_PAGES_IN_CPU_PAGE;\n\n\tr = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     num_dw * 4 + num_bytes,\n\t\t\t\t     AMDGPU_IB_POOL_DELAYED, &job);\n\tif (r)\n\t\treturn r;\n\n\tsrc_addr = num_dw * 4;\n\tsrc_addr += job->ibs[0].gpu_addr;\n\n\tdst_addr = amdgpu_bo_gpu_offset(adev->gart.bo);\n\tdst_addr += window * AMDGPU_GTT_MAX_TRANSFER_SIZE * 8;\n\tamdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr,\n\t\t\t\tdst_addr, num_bytes, false);\n\n\tamdgpu_ring_pad_ib(ring, &job->ibs[0]);\n\tWARN_ON(job->ibs[0].length_dw > num_dw);\n\n\tflags = amdgpu_ttm_tt_pte_flags(adev, bo->ttm, mem);\n\tif (tmz)\n\t\tflags |= AMDGPU_PTE_TMZ;\n\n\tcpu_addr = &job->ibs[0].ptr[num_dw];\n\n\tif (mem->mem_type == TTM_PL_TT) {\n\t\tdma_addr_t *dma_addr;\n\n\t\tdma_addr = &bo->ttm->dma_address[mm_cur->start >> PAGE_SHIFT];\n\t\tamdgpu_gart_map(adev, 0, num_pages, dma_addr, flags, cpu_addr);\n\t} else {\n\t\tdma_addr_t dma_address;\n\n\t\tdma_address = mm_cur->start;\n\t\tdma_address += adev->vm_manager.vram_base_offset;\n\n\t\tfor (i = 0; i < num_pages; ++i) {\n\t\t\tamdgpu_gart_map(adev, i << PAGE_SHIFT, 1, &dma_address,\n\t\t\t\t\tflags, cpu_addr);\n\t\t\tdma_address += PAGE_SIZE;\n\t\t}\n\t}\n\n\tdma_fence_put(amdgpu_job_submit(job));\n\treturn 0;\n}\n\n \nint amdgpu_ttm_copy_mem_to_mem(struct amdgpu_device *adev,\n\t\t\t       const struct amdgpu_copy_mem *src,\n\t\t\t       const struct amdgpu_copy_mem *dst,\n\t\t\t       uint64_t size, bool tmz,\n\t\t\t       struct dma_resv *resv,\n\t\t\t       struct dma_fence **f)\n{\n\tstruct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;\n\tstruct amdgpu_res_cursor src_mm, dst_mm;\n\tstruct dma_fence *fence = NULL;\n\tint r = 0;\n\n\tif (!adev->mman.buffer_funcs_enabled) {\n\t\tDRM_ERROR(\"Trying to move memory with ring turned off.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tamdgpu_res_first(src->mem, src->offset, size, &src_mm);\n\tamdgpu_res_first(dst->mem, dst->offset, size, &dst_mm);\n\n\tmutex_lock(&adev->mman.gtt_window_lock);\n\twhile (src_mm.remaining) {\n\t\tuint64_t from, to, cur_size;\n\t\tstruct dma_fence *next;\n\n\t\t \n\t\tcur_size = min3(src_mm.size, dst_mm.size, 256ULL << 20);\n\n\t\t \n\t\tr = amdgpu_ttm_map_buffer(src->bo, src->mem, &src_mm,\n\t\t\t\t\t  0, ring, tmz, &cur_size, &from);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tr = amdgpu_ttm_map_buffer(dst->bo, dst->mem, &dst_mm,\n\t\t\t\t\t  1, ring, tmz, &cur_size, &to);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tr = amdgpu_copy_buffer(ring, from, to, cur_size,\n\t\t\t\t       resv, &next, false, true, tmz);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tdma_fence_put(fence);\n\t\tfence = next;\n\n\t\tamdgpu_res_next(&src_mm, cur_size);\n\t\tamdgpu_res_next(&dst_mm, cur_size);\n\t}\nerror:\n\tmutex_unlock(&adev->mman.gtt_window_lock);\n\tif (f)\n\t\t*f = dma_fence_get(fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\n \nstatic int amdgpu_move_blit(struct ttm_buffer_object *bo,\n\t\t\t    bool evict,\n\t\t\t    struct ttm_resource *new_mem,\n\t\t\t    struct ttm_resource *old_mem)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);\n\tstruct amdgpu_copy_mem src, dst;\n\tstruct dma_fence *fence = NULL;\n\tint r;\n\n\tsrc.bo = bo;\n\tdst.bo = bo;\n\tsrc.mem = old_mem;\n\tdst.mem = new_mem;\n\tsrc.offset = 0;\n\tdst.offset = 0;\n\n\tr = amdgpu_ttm_copy_mem_to_mem(adev, &src, &dst,\n\t\t\t\t       new_mem->size,\n\t\t\t\t       amdgpu_bo_encrypted(abo),\n\t\t\t\t       bo->base.resv, &fence);\n\tif (r)\n\t\tgoto error;\n\n\t \n\tif (old_mem->mem_type == TTM_PL_VRAM &&\n\t    (abo->flags & AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE)) {\n\t\tstruct dma_fence *wipe_fence = NULL;\n\n\t\tr = amdgpu_fill_buffer(abo, AMDGPU_POISON, NULL, &wipe_fence,\n\t\t\t\t\tfalse);\n\t\tif (r) {\n\t\t\tgoto error;\n\t\t} else if (wipe_fence) {\n\t\t\tdma_fence_put(fence);\n\t\t\tfence = wipe_fence;\n\t\t}\n\t}\n\n\t \n\tif (bo->type == ttm_bo_type_kernel)\n\t\tr = ttm_bo_move_accel_cleanup(bo, fence, true, false, new_mem);\n\telse\n\t\tr = ttm_bo_move_accel_cleanup(bo, fence, evict, true, new_mem);\n\tdma_fence_put(fence);\n\treturn r;\n\nerror:\n\tif (fence)\n\t\tdma_fence_wait(fence, false);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\n \nstatic bool amdgpu_mem_visible(struct amdgpu_device *adev,\n\t\t\t       struct ttm_resource *mem)\n{\n\tu64 mem_size = (u64)mem->size;\n\tstruct amdgpu_res_cursor cursor;\n\tu64 end;\n\n\tif (mem->mem_type == TTM_PL_SYSTEM ||\n\t    mem->mem_type == TTM_PL_TT)\n\t\treturn true;\n\tif (mem->mem_type != TTM_PL_VRAM)\n\t\treturn false;\n\n\tamdgpu_res_first(mem, 0, mem_size, &cursor);\n\tend = cursor.start + cursor.size;\n\twhile (cursor.remaining) {\n\t\tamdgpu_res_next(&cursor, cursor.size);\n\n\t\tif (!cursor.remaining)\n\t\t\tbreak;\n\n\t\t \n\t\tif (end != cursor.start)\n\t\t\treturn false;\n\n\t\tend = cursor.start + cursor.size;\n\t}\n\n\treturn end <= adev->gmc.visible_vram_size;\n}\n\n \nstatic int amdgpu_bo_move(struct ttm_buffer_object *bo, bool evict,\n\t\t\t  struct ttm_operation_ctx *ctx,\n\t\t\t  struct ttm_resource *new_mem,\n\t\t\t  struct ttm_place *hop)\n{\n\tstruct amdgpu_device *adev;\n\tstruct amdgpu_bo *abo;\n\tstruct ttm_resource *old_mem = bo->resource;\n\tint r;\n\n\tif (new_mem->mem_type == TTM_PL_TT ||\n\t    new_mem->mem_type == AMDGPU_PL_PREEMPT) {\n\t\tr = amdgpu_ttm_backend_bind(bo->bdev, bo->ttm, new_mem);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tabo = ttm_to_amdgpu_bo(bo);\n\tadev = amdgpu_ttm_adev(bo->bdev);\n\n\tif (!old_mem || (old_mem->mem_type == TTM_PL_SYSTEM &&\n\t\t\t bo->ttm == NULL)) {\n\t\tttm_bo_move_null(bo, new_mem);\n\t\tgoto out;\n\t}\n\tif (old_mem->mem_type == TTM_PL_SYSTEM &&\n\t    (new_mem->mem_type == TTM_PL_TT ||\n\t     new_mem->mem_type == AMDGPU_PL_PREEMPT)) {\n\t\tttm_bo_move_null(bo, new_mem);\n\t\tgoto out;\n\t}\n\tif ((old_mem->mem_type == TTM_PL_TT ||\n\t     old_mem->mem_type == AMDGPU_PL_PREEMPT) &&\n\t    new_mem->mem_type == TTM_PL_SYSTEM) {\n\t\tr = ttm_bo_wait_ctx(bo, ctx);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tamdgpu_ttm_backend_unbind(bo->bdev, bo->ttm);\n\t\tttm_resource_free(bo, &bo->resource);\n\t\tttm_bo_assign_mem(bo, new_mem);\n\t\tgoto out;\n\t}\n\n\tif (old_mem->mem_type == AMDGPU_PL_GDS ||\n\t    old_mem->mem_type == AMDGPU_PL_GWS ||\n\t    old_mem->mem_type == AMDGPU_PL_OA ||\n\t    old_mem->mem_type == AMDGPU_PL_DOORBELL ||\n\t    new_mem->mem_type == AMDGPU_PL_GDS ||\n\t    new_mem->mem_type == AMDGPU_PL_GWS ||\n\t    new_mem->mem_type == AMDGPU_PL_OA ||\n\t    new_mem->mem_type == AMDGPU_PL_DOORBELL) {\n\t\t \n\t\tttm_bo_move_null(bo, new_mem);\n\t\tgoto out;\n\t}\n\n\tif (bo->type == ttm_bo_type_device &&\n\t    new_mem->mem_type == TTM_PL_VRAM &&\n\t    old_mem->mem_type != TTM_PL_VRAM) {\n\t\t \n\t\tabo->flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\t}\n\n\tif (adev->mman.buffer_funcs_enabled) {\n\t\tif (((old_mem->mem_type == TTM_PL_SYSTEM &&\n\t\t      new_mem->mem_type == TTM_PL_VRAM) ||\n\t\t     (old_mem->mem_type == TTM_PL_VRAM &&\n\t\t      new_mem->mem_type == TTM_PL_SYSTEM))) {\n\t\t\thop->fpfn = 0;\n\t\t\thop->lpfn = 0;\n\t\t\thop->mem_type = TTM_PL_TT;\n\t\t\thop->flags = TTM_PL_FLAG_TEMPORARY;\n\t\t\treturn -EMULTIHOP;\n\t\t}\n\n\t\tr = amdgpu_move_blit(bo, evict, new_mem, old_mem);\n\t} else {\n\t\tr = -ENODEV;\n\t}\n\n\tif (r) {\n\t\t \n\t\tif (!amdgpu_mem_visible(adev, old_mem) ||\n\t\t    !amdgpu_mem_visible(adev, new_mem)) {\n\t\t\tpr_err(\"Move buffer fallback to memcpy unavailable\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\tr = ttm_bo_move_memcpy(bo, ctx, new_mem);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\nout:\n\t \n\tatomic64_add(bo->base.size, &adev->num_bytes_moved);\n\tamdgpu_bo_move_notify(bo, evict, new_mem);\n\treturn 0;\n}\n\n \nstatic int amdgpu_ttm_io_mem_reserve(struct ttm_device *bdev,\n\t\t\t\t     struct ttm_resource *mem)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tsize_t bus_size = (size_t)mem->size;\n\n\tswitch (mem->mem_type) {\n\tcase TTM_PL_SYSTEM:\n\t\t \n\t\treturn 0;\n\tcase TTM_PL_TT:\n\tcase AMDGPU_PL_PREEMPT:\n\t\tbreak;\n\tcase TTM_PL_VRAM:\n\t\tmem->bus.offset = mem->start << PAGE_SHIFT;\n\t\t \n\t\tif ((mem->bus.offset + bus_size) > adev->gmc.visible_vram_size)\n\t\t\treturn -EINVAL;\n\n\t\tif (adev->mman.aper_base_kaddr &&\n\t\t    mem->placement & TTM_PL_FLAG_CONTIGUOUS)\n\t\t\tmem->bus.addr = (u8 *)adev->mman.aper_base_kaddr +\n\t\t\t\t\tmem->bus.offset;\n\n\t\tmem->bus.offset += adev->gmc.aper_base;\n\t\tmem->bus.is_iomem = true;\n\t\tbreak;\n\tcase AMDGPU_PL_DOORBELL:\n\t\tmem->bus.offset = mem->start << PAGE_SHIFT;\n\t\tmem->bus.offset += adev->doorbell.base;\n\t\tmem->bus.is_iomem = true;\n\t\tmem->bus.caching = ttm_uncached;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic unsigned long amdgpu_ttm_io_mem_pfn(struct ttm_buffer_object *bo,\n\t\t\t\t\t   unsigned long page_offset)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct amdgpu_res_cursor cursor;\n\n\tamdgpu_res_first(bo->resource, (u64)page_offset << PAGE_SHIFT, 0,\n\t\t\t &cursor);\n\n\tif (bo->resource->mem_type == AMDGPU_PL_DOORBELL)\n\t\treturn ((uint64_t)(adev->doorbell.base + cursor.start)) >> PAGE_SHIFT;\n\n\treturn (adev->gmc.aper_base + cursor.start) >> PAGE_SHIFT;\n}\n\n \n\nuint64_t amdgpu_ttm_domain_start(struct amdgpu_device *adev, uint32_t type)\n{\n\tswitch (type) {\n\tcase TTM_PL_TT:\n\t\treturn adev->gmc.gart_start;\n\tcase TTM_PL_VRAM:\n\t\treturn adev->gmc.vram_start;\n\t}\n\n\treturn 0;\n}\n\n \nstruct amdgpu_ttm_tt {\n\tstruct ttm_tt\tttm;\n\tstruct drm_gem_object\t*gobj;\n\tu64\t\t\toffset;\n\tuint64_t\t\tuserptr;\n\tstruct task_struct\t*usertask;\n\tuint32_t\t\tuserflags;\n\tbool\t\t\tbound;\n\tint32_t\t\t\tpool_id;\n};\n\n#define ttm_to_amdgpu_ttm_tt(ptr)\tcontainer_of(ptr, struct amdgpu_ttm_tt, ttm)\n\n#ifdef CONFIG_DRM_AMDGPU_USERPTR\n \nint amdgpu_ttm_tt_get_user_pages(struct amdgpu_bo *bo, struct page **pages,\n\t\t\t\t struct hmm_range **range)\n{\n\tstruct ttm_tt *ttm = bo->tbo.ttm;\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tunsigned long start = gtt->userptr;\n\tstruct vm_area_struct *vma;\n\tstruct mm_struct *mm;\n\tbool readonly;\n\tint r = 0;\n\n\t \n\t*range = NULL;\n\n\tmm = bo->notifier.mm;\n\tif (unlikely(!mm)) {\n\t\tDRM_DEBUG_DRIVER(\"BO is not registered?\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!mmget_not_zero(mm))  \n\t\treturn -ESRCH;\n\n\tmmap_read_lock(mm);\n\tvma = vma_lookup(mm, start);\n\tif (unlikely(!vma)) {\n\t\tr = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\tif (unlikely((gtt->userflags & AMDGPU_GEM_USERPTR_ANONONLY) &&\n\t\tvma->vm_file)) {\n\t\tr = -EPERM;\n\t\tgoto out_unlock;\n\t}\n\n\treadonly = amdgpu_ttm_tt_is_readonly(ttm);\n\tr = amdgpu_hmm_range_get_pages(&bo->notifier, start, ttm->num_pages,\n\t\t\t\t       readonly, NULL, pages, range);\nout_unlock:\n\tmmap_read_unlock(mm);\n\tif (r)\n\t\tpr_debug(\"failed %d to get user pages 0x%lx\\n\", r, start);\n\n\tmmput(mm);\n\n\treturn r;\n}\n\n \nvoid amdgpu_ttm_tt_discard_user_pages(struct ttm_tt *ttm,\n\t\t\t\t      struct hmm_range *range)\n{\n\tstruct amdgpu_ttm_tt *gtt = (void *)ttm;\n\n\tif (gtt && gtt->userptr && range)\n\t\tamdgpu_hmm_range_get_pages_done(range);\n}\n\n \nbool amdgpu_ttm_tt_get_user_pages_done(struct ttm_tt *ttm,\n\t\t\t\t       struct hmm_range *range)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (!gtt || !gtt->userptr || !range)\n\t\treturn false;\n\n\tDRM_DEBUG_DRIVER(\"user_pages_done 0x%llx pages 0x%x\\n\",\n\t\tgtt->userptr, ttm->num_pages);\n\n\tWARN_ONCE(!range->hmm_pfns, \"No user pages to check\\n\");\n\n\treturn !amdgpu_hmm_range_get_pages_done(range);\n}\n#endif\n\n \nvoid amdgpu_ttm_tt_set_user_pages(struct ttm_tt *ttm, struct page **pages)\n{\n\tunsigned long i;\n\n\tfor (i = 0; i < ttm->num_pages; ++i)\n\t\tttm->pages[i] = pages ? pages[i] : NULL;\n}\n\n \nstatic int amdgpu_ttm_tt_pin_userptr(struct ttm_device *bdev,\n\t\t\t\t     struct ttm_tt *ttm)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tint write = !(gtt->userflags & AMDGPU_GEM_USERPTR_READONLY);\n\tenum dma_data_direction direction = write ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\tint r;\n\n\t \n\tr = sg_alloc_table_from_pages(ttm->sg, ttm->pages, ttm->num_pages, 0,\n\t\t\t\t      (u64)ttm->num_pages << PAGE_SHIFT,\n\t\t\t\t      GFP_KERNEL);\n\tif (r)\n\t\tgoto release_sg;\n\n\t \n\tr = dma_map_sgtable(adev->dev, ttm->sg, direction, 0);\n\tif (r)\n\t\tgoto release_sg;\n\n\t \n\tdrm_prime_sg_to_dma_addr_array(ttm->sg, gtt->ttm.dma_address,\n\t\t\t\t       ttm->num_pages);\n\n\treturn 0;\n\nrelease_sg:\n\tkfree(ttm->sg);\n\tttm->sg = NULL;\n\treturn r;\n}\n\n \nstatic void amdgpu_ttm_tt_unpin_userptr(struct ttm_device *bdev,\n\t\t\t\t\tstruct ttm_tt *ttm)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tint write = !(gtt->userflags & AMDGPU_GEM_USERPTR_READONLY);\n\tenum dma_data_direction direction = write ?\n\t\tDMA_BIDIRECTIONAL : DMA_TO_DEVICE;\n\n\t \n\tif (!ttm->sg || !ttm->sg->sgl)\n\t\treturn;\n\n\t \n\tdma_unmap_sgtable(adev->dev, ttm->sg, direction, 0);\n\tsg_free_table(ttm->sg);\n}\n\n \nstatic void amdgpu_ttm_gart_bind_gfx9_mqd(struct amdgpu_device *adev,\n\t\t\t\tstruct ttm_tt *ttm, uint64_t flags)\n{\n\tstruct amdgpu_ttm_tt *gtt = (void *)ttm;\n\tuint64_t total_pages = ttm->num_pages;\n\tint num_xcc = max(1U, adev->gfx.num_xcc_per_xcp);\n\tuint64_t page_idx, pages_per_xcc;\n\tint i;\n\tuint64_t ctrl_flags = (flags & ~AMDGPU_PTE_MTYPE_VG10_MASK) |\n\t\t\tAMDGPU_PTE_MTYPE_VG10(AMDGPU_MTYPE_NC);\n\n\tpages_per_xcc = total_pages;\n\tdo_div(pages_per_xcc, num_xcc);\n\n\tfor (i = 0, page_idx = 0; i < num_xcc; i++, page_idx += pages_per_xcc) {\n\t\t \n\t\tamdgpu_gart_bind(adev,\n\t\t\t\tgtt->offset + (page_idx << PAGE_SHIFT),\n\t\t\t\t1, &gtt->ttm.dma_address[page_idx], flags);\n\t\t \n\t\tamdgpu_gart_bind(adev,\n\t\t\t\tgtt->offset + ((page_idx + 1) << PAGE_SHIFT),\n\t\t\t\tpages_per_xcc - 1,\n\t\t\t\t&gtt->ttm.dma_address[page_idx + 1],\n\t\t\t\tctrl_flags);\n\t}\n}\n\nstatic void amdgpu_ttm_gart_bind(struct amdgpu_device *adev,\n\t\t\t\t struct ttm_buffer_object *tbo,\n\t\t\t\t uint64_t flags)\n{\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(tbo);\n\tstruct ttm_tt *ttm = tbo->ttm;\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (amdgpu_bo_encrypted(abo))\n\t\tflags |= AMDGPU_PTE_TMZ;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_CP_MQD_GFX9) {\n\t\tamdgpu_ttm_gart_bind_gfx9_mqd(adev, ttm, flags);\n\t} else {\n\t\tamdgpu_gart_bind(adev, gtt->offset, ttm->num_pages,\n\t\t\t\t gtt->ttm.dma_address, flags);\n\t}\n}\n\n \nstatic int amdgpu_ttm_backend_bind(struct ttm_device *bdev,\n\t\t\t\t   struct ttm_tt *ttm,\n\t\t\t\t   struct ttm_resource *bo_mem)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tuint64_t flags;\n\tint r;\n\n\tif (!bo_mem)\n\t\treturn -EINVAL;\n\n\tif (gtt->bound)\n\t\treturn 0;\n\n\tif (gtt->userptr) {\n\t\tr = amdgpu_ttm_tt_pin_userptr(bdev, ttm);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"failed to pin userptr\\n\");\n\t\t\treturn r;\n\t\t}\n\t} else if (ttm->page_flags & TTM_TT_FLAG_EXTERNAL) {\n\t\tif (!ttm->sg) {\n\t\t\tstruct dma_buf_attachment *attach;\n\t\t\tstruct sg_table *sgt;\n\n\t\t\tattach = gtt->gobj->import_attach;\n\t\t\tsgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);\n\t\t\tif (IS_ERR(sgt))\n\t\t\t\treturn PTR_ERR(sgt);\n\n\t\t\tttm->sg = sgt;\n\t\t}\n\n\t\tdrm_prime_sg_to_dma_addr_array(ttm->sg, gtt->ttm.dma_address,\n\t\t\t\t\t       ttm->num_pages);\n\t}\n\n\tif (!ttm->num_pages) {\n\t\tWARN(1, \"nothing to bind %u pages for mreg %p back %p!\\n\",\n\t\t     ttm->num_pages, bo_mem, ttm);\n\t}\n\n\tif (bo_mem->mem_type != TTM_PL_TT ||\n\t    !amdgpu_gtt_mgr_has_gart_addr(bo_mem)) {\n\t\tgtt->offset = AMDGPU_BO_INVALID_OFFSET;\n\t\treturn 0;\n\t}\n\n\t \n\tflags = amdgpu_ttm_tt_pte_flags(adev, ttm, bo_mem);\n\n\t \n\tgtt->offset = (u64)bo_mem->start << PAGE_SHIFT;\n\tamdgpu_gart_bind(adev, gtt->offset, ttm->num_pages,\n\t\t\t gtt->ttm.dma_address, flags);\n\tgtt->bound = true;\n\treturn 0;\n}\n\n \nint amdgpu_ttm_alloc_gart(struct ttm_buffer_object *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(bo->ttm);\n\tstruct ttm_placement placement;\n\tstruct ttm_place placements;\n\tstruct ttm_resource *tmp;\n\tuint64_t addr, flags;\n\tint r;\n\n\tif (bo->resource->start != AMDGPU_BO_INVALID_OFFSET)\n\t\treturn 0;\n\n\taddr = amdgpu_gmc_agp_addr(bo);\n\tif (addr != AMDGPU_BO_INVALID_OFFSET) {\n\t\tbo->resource->start = addr >> PAGE_SHIFT;\n\t\treturn 0;\n\t}\n\n\t \n\tplacement.num_placement = 1;\n\tplacement.placement = &placements;\n\tplacement.num_busy_placement = 1;\n\tplacement.busy_placement = &placements;\n\tplacements.fpfn = 0;\n\tplacements.lpfn = adev->gmc.gart_size >> PAGE_SHIFT;\n\tplacements.mem_type = TTM_PL_TT;\n\tplacements.flags = bo->resource->placement;\n\n\tr = ttm_bo_mem_space(bo, &placement, &tmp, &ctx);\n\tif (unlikely(r))\n\t\treturn r;\n\n\t \n\tflags = amdgpu_ttm_tt_pte_flags(adev, bo->ttm, tmp);\n\n\t \n\tgtt->offset = (u64)tmp->start << PAGE_SHIFT;\n\tamdgpu_ttm_gart_bind(adev, bo, flags);\n\tamdgpu_gart_invalidate_tlb(adev);\n\tttm_resource_free(bo, &bo->resource);\n\tttm_bo_assign_mem(bo, tmp);\n\n\treturn 0;\n}\n\n \nvoid amdgpu_ttm_recover_gart(struct ttm_buffer_object *tbo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(tbo->bdev);\n\tuint64_t flags;\n\n\tif (!tbo->ttm)\n\t\treturn;\n\n\tflags = amdgpu_ttm_tt_pte_flags(adev, tbo->ttm, tbo->resource);\n\tamdgpu_ttm_gart_bind(adev, tbo, flags);\n}\n\n \nstatic void amdgpu_ttm_backend_unbind(struct ttm_device *bdev,\n\t\t\t\t      struct ttm_tt *ttm)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\t \n\tif (gtt->userptr) {\n\t\tamdgpu_ttm_tt_unpin_userptr(bdev, ttm);\n\t} else if (ttm->sg && gtt->gobj->import_attach) {\n\t\tstruct dma_buf_attachment *attach;\n\n\t\tattach = gtt->gobj->import_attach;\n\t\tdma_buf_unmap_attachment(attach, ttm->sg, DMA_BIDIRECTIONAL);\n\t\tttm->sg = NULL;\n\t}\n\n\tif (!gtt->bound)\n\t\treturn;\n\n\tif (gtt->offset == AMDGPU_BO_INVALID_OFFSET)\n\t\treturn;\n\n\t \n\tamdgpu_gart_unbind(adev, gtt->offset, ttm->num_pages);\n\tgtt->bound = false;\n}\n\nstatic void amdgpu_ttm_backend_destroy(struct ttm_device *bdev,\n\t\t\t\t       struct ttm_tt *ttm)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (gtt->usertask)\n\t\tput_task_struct(gtt->usertask);\n\n\tttm_tt_fini(&gtt->ttm);\n\tkfree(gtt);\n}\n\n \nstatic struct ttm_tt *amdgpu_ttm_tt_create(struct ttm_buffer_object *bo,\n\t\t\t\t\t   uint32_t page_flags)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);\n\tstruct amdgpu_ttm_tt *gtt;\n\tenum ttm_caching caching;\n\n\tgtt = kzalloc(sizeof(struct amdgpu_ttm_tt), GFP_KERNEL);\n\tif (!gtt)\n\t\treturn NULL;\n\n\tgtt->gobj = &bo->base;\n\tif (adev->gmc.mem_partitions && abo->xcp_id >= 0)\n\t\tgtt->pool_id = KFD_XCP_MEM_ID(adev, abo->xcp_id);\n\telse\n\t\tgtt->pool_id = abo->xcp_id;\n\n\tif (abo->flags & AMDGPU_GEM_CREATE_CPU_GTT_USWC)\n\t\tcaching = ttm_write_combined;\n\telse\n\t\tcaching = ttm_cached;\n\n\t \n\tif (ttm_sg_tt_init(&gtt->ttm, bo, page_flags, caching)) {\n\t\tkfree(gtt);\n\t\treturn NULL;\n\t}\n\treturn &gtt->ttm;\n}\n\n \nstatic int amdgpu_ttm_tt_populate(struct ttm_device *bdev,\n\t\t\t\t  struct ttm_tt *ttm,\n\t\t\t\t  struct ttm_operation_ctx *ctx)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bdev);\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tstruct ttm_pool *pool;\n\tpgoff_t i;\n\tint ret;\n\n\t \n\tif (gtt->userptr) {\n\t\tttm->sg = kzalloc(sizeof(struct sg_table), GFP_KERNEL);\n\t\tif (!ttm->sg)\n\t\t\treturn -ENOMEM;\n\t\treturn 0;\n\t}\n\n\tif (ttm->page_flags & TTM_TT_FLAG_EXTERNAL)\n\t\treturn 0;\n\n\tif (adev->mman.ttm_pools && gtt->pool_id >= 0)\n\t\tpool = &adev->mman.ttm_pools[gtt->pool_id];\n\telse\n\t\tpool = &adev->mman.bdev.pool;\n\tret = ttm_pool_alloc(pool, ttm, ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (i = 0; i < ttm->num_pages; ++i)\n\t\tttm->pages[i]->mapping = bdev->dev_mapping;\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_ttm_tt_unpopulate(struct ttm_device *bdev,\n\t\t\t\t     struct ttm_tt *ttm)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tstruct amdgpu_device *adev;\n\tstruct ttm_pool *pool;\n\tpgoff_t i;\n\n\tamdgpu_ttm_backend_unbind(bdev, ttm);\n\n\tif (gtt->userptr) {\n\t\tamdgpu_ttm_tt_set_user_pages(ttm, NULL);\n\t\tkfree(ttm->sg);\n\t\tttm->sg = NULL;\n\t\treturn;\n\t}\n\n\tif (ttm->page_flags & TTM_TT_FLAG_EXTERNAL)\n\t\treturn;\n\n\tfor (i = 0; i < ttm->num_pages; ++i)\n\t\tttm->pages[i]->mapping = NULL;\n\n\tadev = amdgpu_ttm_adev(bdev);\n\n\tif (adev->mman.ttm_pools && gtt->pool_id >= 0)\n\t\tpool = &adev->mman.ttm_pools[gtt->pool_id];\n\telse\n\t\tpool = &adev->mman.bdev.pool;\n\n\treturn ttm_pool_free(pool, ttm);\n}\n\n \nint amdgpu_ttm_tt_get_userptr(const struct ttm_buffer_object *tbo,\n\t\t\t      uint64_t *user_addr)\n{\n\tstruct amdgpu_ttm_tt *gtt;\n\n\tif (!tbo->ttm)\n\t\treturn -EINVAL;\n\n\tgtt = (void *)tbo->ttm;\n\t*user_addr = gtt->userptr;\n\treturn 0;\n}\n\n \nint amdgpu_ttm_tt_set_userptr(struct ttm_buffer_object *bo,\n\t\t\t      uint64_t addr, uint32_t flags)\n{\n\tstruct amdgpu_ttm_tt *gtt;\n\n\tif (!bo->ttm) {\n\t\t \n\t\tbo->ttm = amdgpu_ttm_tt_create(bo, 0);\n\t\tif (bo->ttm == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tbo->ttm->page_flags |= TTM_TT_FLAG_EXTERNAL;\n\n\tgtt = ttm_to_amdgpu_ttm_tt(bo->ttm);\n\tgtt->userptr = addr;\n\tgtt->userflags = flags;\n\n\tif (gtt->usertask)\n\t\tput_task_struct(gtt->usertask);\n\tgtt->usertask = current->group_leader;\n\tget_task_struct(gtt->usertask);\n\n\treturn 0;\n}\n\n \nstruct mm_struct *amdgpu_ttm_tt_get_usermm(struct ttm_tt *ttm)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (gtt == NULL)\n\t\treturn NULL;\n\n\tif (gtt->usertask == NULL)\n\t\treturn NULL;\n\n\treturn gtt->usertask->mm;\n}\n\n \nbool amdgpu_ttm_tt_affect_userptr(struct ttm_tt *ttm, unsigned long start,\n\t\t\t\t  unsigned long end, unsigned long *userptr)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\tunsigned long size;\n\n\tif (gtt == NULL || !gtt->userptr)\n\t\treturn false;\n\n\t \n\tsize = (unsigned long)gtt->ttm.num_pages * PAGE_SIZE;\n\tif (gtt->userptr > end || gtt->userptr + size <= start)\n\t\treturn false;\n\n\tif (userptr)\n\t\t*userptr = gtt->userptr;\n\treturn true;\n}\n\n \nbool amdgpu_ttm_tt_is_userptr(struct ttm_tt *ttm)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (gtt == NULL || !gtt->userptr)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool amdgpu_ttm_tt_is_readonly(struct ttm_tt *ttm)\n{\n\tstruct amdgpu_ttm_tt *gtt = ttm_to_amdgpu_ttm_tt(ttm);\n\n\tif (gtt == NULL)\n\t\treturn false;\n\n\treturn !!(gtt->userflags & AMDGPU_GEM_USERPTR_READONLY);\n}\n\n \nuint64_t amdgpu_ttm_tt_pde_flags(struct ttm_tt *ttm, struct ttm_resource *mem)\n{\n\tuint64_t flags = 0;\n\n\tif (mem && mem->mem_type != TTM_PL_SYSTEM)\n\t\tflags |= AMDGPU_PTE_VALID;\n\n\tif (mem && (mem->mem_type == TTM_PL_TT ||\n\t\t    mem->mem_type == AMDGPU_PL_DOORBELL ||\n\t\t    mem->mem_type == AMDGPU_PL_PREEMPT)) {\n\t\tflags |= AMDGPU_PTE_SYSTEM;\n\n\t\tif (ttm->caching == ttm_cached)\n\t\t\tflags |= AMDGPU_PTE_SNOOPED;\n\t}\n\n\tif (mem && mem->mem_type == TTM_PL_VRAM &&\n\t\t\tmem->bus.caching == ttm_cached)\n\t\tflags |= AMDGPU_PTE_SNOOPED;\n\n\treturn flags;\n}\n\n \nuint64_t amdgpu_ttm_tt_pte_flags(struct amdgpu_device *adev, struct ttm_tt *ttm,\n\t\t\t\t struct ttm_resource *mem)\n{\n\tuint64_t flags = amdgpu_ttm_tt_pde_flags(ttm, mem);\n\n\tflags |= adev->gart.gart_pte_flags;\n\tflags |= AMDGPU_PTE_READABLE;\n\n\tif (!amdgpu_ttm_tt_is_readonly(ttm))\n\t\tflags |= AMDGPU_PTE_WRITEABLE;\n\n\treturn flags;\n}\n\n \nstatic bool amdgpu_ttm_bo_eviction_valuable(struct ttm_buffer_object *bo,\n\t\t\t\t\t    const struct ttm_place *place)\n{\n\tstruct dma_resv_iter resv_cursor;\n\tstruct dma_fence *f;\n\n\tif (!amdgpu_bo_is_amdgpu_bo(bo))\n\t\treturn ttm_bo_eviction_valuable(bo, place);\n\n\t \n\tif (bo->resource->mem_type == TTM_PL_SYSTEM)\n\t\treturn true;\n\n\tif (bo->type == ttm_bo_type_kernel &&\n\t    !amdgpu_vm_evictable(ttm_to_amdgpu_bo(bo)))\n\t\treturn false;\n\n\t \n\tdma_resv_for_each_fence(&resv_cursor, bo->base.resv,\n\t\t\t\tDMA_RESV_USAGE_BOOKKEEP, f) {\n\t\tif (amdkfd_fence_check_mm(f, current->mm))\n\t\t\treturn false;\n\t}\n\n\t \n\tif (bo->resource->mem_type == AMDGPU_PL_PREEMPT)\n\t\treturn false;\n\n\tif (bo->resource->mem_type == TTM_PL_TT &&\n\t    amdgpu_bo_encrypted(ttm_to_amdgpu_bo(bo)))\n\t\treturn false;\n\n\treturn ttm_bo_eviction_valuable(bo, place);\n}\n\nstatic void amdgpu_ttm_vram_mm_access(struct amdgpu_device *adev, loff_t pos,\n\t\t\t\t      void *buf, size_t size, bool write)\n{\n\twhile (size) {\n\t\tuint64_t aligned_pos = ALIGN_DOWN(pos, 4);\n\t\tuint64_t bytes = 4 - (pos & 0x3);\n\t\tuint32_t shift = (pos & 0x3) * 8;\n\t\tuint32_t mask = 0xffffffff << shift;\n\t\tuint32_t value = 0;\n\n\t\tif (size < bytes) {\n\t\t\tmask &= 0xffffffff >> (bytes - size) * 8;\n\t\t\tbytes = size;\n\t\t}\n\n\t\tif (mask != 0xffffffff) {\n\t\t\tamdgpu_device_mm_access(adev, aligned_pos, &value, 4, false);\n\t\t\tif (write) {\n\t\t\t\tvalue &= ~mask;\n\t\t\t\tvalue |= (*(uint32_t *)buf << shift) & mask;\n\t\t\t\tamdgpu_device_mm_access(adev, aligned_pos, &value, 4, true);\n\t\t\t} else {\n\t\t\t\tvalue = (value & mask) >> shift;\n\t\t\t\tmemcpy(buf, &value, bytes);\n\t\t\t}\n\t\t} else {\n\t\t\tamdgpu_device_mm_access(adev, aligned_pos, buf, 4, write);\n\t\t}\n\n\t\tpos += bytes;\n\t\tbuf += bytes;\n\t\tsize -= bytes;\n\t}\n}\n\nstatic int amdgpu_ttm_access_memory_sdma(struct ttm_buffer_object *bo,\n\t\t\t\t\tunsigned long offset, void *buf,\n\t\t\t\t\tint len, int write)\n{\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_res_cursor src_mm;\n\tstruct amdgpu_job *job;\n\tstruct dma_fence *fence;\n\tuint64_t src_addr, dst_addr;\n\tunsigned int num_dw;\n\tint r, idx;\n\n\tif (len != PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (!adev->mman.sdma_access_ptr)\n\t\treturn -EACCES;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn -ENODEV;\n\n\tif (write)\n\t\tmemcpy(adev->mman.sdma_access_ptr, buf, len);\n\n\tnum_dw = ALIGN(adev->mman.buffer_funcs->copy_num_dw, 8);\n\tr = amdgpu_job_alloc_with_ib(adev, &adev->mman.high_pr,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     num_dw * 4, AMDGPU_IB_POOL_DELAYED,\n\t\t\t\t     &job);\n\tif (r)\n\t\tgoto out;\n\n\tamdgpu_res_first(abo->tbo.resource, offset, len, &src_mm);\n\tsrc_addr = amdgpu_ttm_domain_start(adev, bo->resource->mem_type) +\n\t\tsrc_mm.start;\n\tdst_addr = amdgpu_bo_gpu_offset(adev->mman.sdma_access_bo);\n\tif (write)\n\t\tswap(src_addr, dst_addr);\n\n\tamdgpu_emit_copy_buffer(adev, &job->ibs[0], src_addr, dst_addr,\n\t\t\t\tPAGE_SIZE, false);\n\n\tamdgpu_ring_pad_ib(adev->mman.buffer_funcs_ring, &job->ibs[0]);\n\tWARN_ON(job->ibs[0].length_dw > num_dw);\n\n\tfence = amdgpu_job_submit(job);\n\n\tif (!dma_fence_wait_timeout(fence, false, adev->sdma_timeout))\n\t\tr = -ETIMEDOUT;\n\tdma_fence_put(fence);\n\n\tif (!(r || write))\n\t\tmemcpy(buf, adev->mman.sdma_access_ptr, len);\nout:\n\tdrm_dev_exit(idx);\n\treturn r;\n}\n\n \nstatic int amdgpu_ttm_access_memory(struct ttm_buffer_object *bo,\n\t\t\t\t    unsigned long offset, void *buf, int len,\n\t\t\t\t    int write)\n{\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct amdgpu_res_cursor cursor;\n\tint ret = 0;\n\n\tif (bo->resource->mem_type != TTM_PL_VRAM)\n\t\treturn -EIO;\n\n\tif (amdgpu_device_has_timeouts_enabled(adev) &&\n\t\t\t!amdgpu_ttm_access_memory_sdma(bo, offset, buf, len, write))\n\t\treturn len;\n\n\tamdgpu_res_first(bo->resource, offset, len, &cursor);\n\twhile (cursor.remaining) {\n\t\tsize_t count, size = cursor.size;\n\t\tloff_t pos = cursor.start;\n\n\t\tcount = amdgpu_device_aper_access(adev, pos, buf, size, write);\n\t\tsize -= count;\n\t\tif (size) {\n\t\t\t \n\t\t\tpos += count;\n\t\t\tbuf += count;\n\t\t\tamdgpu_ttm_vram_mm_access(adev, pos, buf, size, write);\n\t\t}\n\n\t\tret += cursor.size;\n\t\tbuf += cursor.size;\n\t\tamdgpu_res_next(&cursor, cursor.size);\n\t}\n\n\treturn ret;\n}\n\nstatic void\namdgpu_bo_delete_mem_notify(struct ttm_buffer_object *bo)\n{\n\tamdgpu_bo_move_notify(bo, false, NULL);\n}\n\nstatic struct ttm_device_funcs amdgpu_bo_driver = {\n\t.ttm_tt_create = &amdgpu_ttm_tt_create,\n\t.ttm_tt_populate = &amdgpu_ttm_tt_populate,\n\t.ttm_tt_unpopulate = &amdgpu_ttm_tt_unpopulate,\n\t.ttm_tt_destroy = &amdgpu_ttm_backend_destroy,\n\t.eviction_valuable = amdgpu_ttm_bo_eviction_valuable,\n\t.evict_flags = &amdgpu_evict_flags,\n\t.move = &amdgpu_bo_move,\n\t.delete_mem_notify = &amdgpu_bo_delete_mem_notify,\n\t.release_notify = &amdgpu_bo_release_notify,\n\t.io_mem_reserve = &amdgpu_ttm_io_mem_reserve,\n\t.io_mem_pfn = amdgpu_ttm_io_mem_pfn,\n\t.access_memory = &amdgpu_ttm_access_memory,\n};\n\n \n \nstatic void amdgpu_ttm_fw_reserve_vram_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_bo_free_kernel(&adev->mman.fw_vram_usage_reserved_bo,\n\t\tNULL, &adev->mman.fw_vram_usage_va);\n}\n\n \n \nstatic void amdgpu_ttm_drv_reserve_vram_fini(struct amdgpu_device *adev)\n{\n\tamdgpu_bo_free_kernel(&adev->mman.drv_vram_usage_reserved_bo,\n\t\t\t\t\t\t  NULL,\n\t\t\t\t\t\t  &adev->mman.drv_vram_usage_va);\n}\n\n \nstatic int amdgpu_ttm_fw_reserve_vram_init(struct amdgpu_device *adev)\n{\n\tuint64_t vram_size = adev->gmc.visible_vram_size;\n\n\tadev->mman.fw_vram_usage_va = NULL;\n\tadev->mman.fw_vram_usage_reserved_bo = NULL;\n\n\tif (adev->mman.fw_vram_usage_size == 0 ||\n\t    adev->mman.fw_vram_usage_size > vram_size)\n\t\treturn 0;\n\n\treturn amdgpu_bo_create_kernel_at(adev,\n\t\t\t\t\t  adev->mman.fw_vram_usage_start_offset,\n\t\t\t\t\t  adev->mman.fw_vram_usage_size,\n\t\t\t\t\t  &adev->mman.fw_vram_usage_reserved_bo,\n\t\t\t\t\t  &adev->mman.fw_vram_usage_va);\n}\n\n \nstatic int amdgpu_ttm_drv_reserve_vram_init(struct amdgpu_device *adev)\n{\n\tu64 vram_size = adev->gmc.visible_vram_size;\n\n\tadev->mman.drv_vram_usage_va = NULL;\n\tadev->mman.drv_vram_usage_reserved_bo = NULL;\n\n\tif (adev->mman.drv_vram_usage_size == 0 ||\n\t    adev->mman.drv_vram_usage_size > vram_size)\n\t\treturn 0;\n\n\treturn amdgpu_bo_create_kernel_at(adev,\n\t\t\t\t\t  adev->mman.drv_vram_usage_start_offset,\n\t\t\t\t\t  adev->mman.drv_vram_usage_size,\n\t\t\t\t\t  &adev->mman.drv_vram_usage_reserved_bo,\n\t\t\t\t\t  &adev->mman.drv_vram_usage_va);\n}\n\n \n\n \nstatic int amdgpu_ttm_training_reserve_vram_fini(struct amdgpu_device *adev)\n{\n\tstruct psp_memory_training_context *ctx = &adev->psp.mem_train_ctx;\n\n\tctx->init = PSP_MEM_TRAIN_NOT_SUPPORT;\n\tamdgpu_bo_free_kernel(&ctx->c2p_bo, NULL, NULL);\n\tctx->c2p_bo = NULL;\n\n\treturn 0;\n}\n\nstatic void amdgpu_ttm_training_data_block_init(struct amdgpu_device *adev,\n\t\t\t\t\t\tuint32_t reserve_size)\n{\n\tstruct psp_memory_training_context *ctx = &adev->psp.mem_train_ctx;\n\n\tmemset(ctx, 0, sizeof(*ctx));\n\n\tctx->c2p_train_data_offset =\n\t\tALIGN((adev->gmc.mc_vram_size - reserve_size - SZ_1M), SZ_1M);\n\tctx->p2c_train_data_offset =\n\t\t(adev->gmc.mc_vram_size - GDDR6_MEM_TRAINING_OFFSET);\n\tctx->train_data_size =\n\t\tGDDR6_MEM_TRAINING_DATA_SIZE_IN_BYTES;\n\n\tDRM_DEBUG(\"train_data_size:%llx,p2c_train_data_offset:%llx,c2p_train_data_offset:%llx.\\n\",\n\t\t\tctx->train_data_size,\n\t\t\tctx->p2c_train_data_offset,\n\t\t\tctx->c2p_train_data_offset);\n}\n\n \nstatic int amdgpu_ttm_reserve_tmr(struct amdgpu_device *adev)\n{\n\tstruct psp_memory_training_context *ctx = &adev->psp.mem_train_ctx;\n\tbool mem_train_support = false;\n\tuint32_t reserve_size = 0;\n\tint ret;\n\n\tif (adev->bios && !amdgpu_sriov_vf(adev)) {\n\t\tif (amdgpu_atomfirmware_mem_training_supported(adev))\n\t\t\tmem_train_support = true;\n\t\telse\n\t\t\tDRM_DEBUG(\"memory training does not support!\\n\");\n\t}\n\n\t \n\tif (adev->bios)\n\t\treserve_size =\n\t\t\tamdgpu_atomfirmware_get_fw_reserved_fb_size(adev);\n\n\tif (!adev->bios && adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 3))\n\t\treserve_size = max(reserve_size, (uint32_t)280 << 20);\n\telse if (!reserve_size)\n\t\treserve_size = DISCOVERY_TMR_OFFSET;\n\n\tif (mem_train_support) {\n\t\t \n\t\tamdgpu_ttm_training_data_block_init(adev, reserve_size);\n\t\tret = amdgpu_bo_create_kernel_at(adev,\n\t\t\t\t\t\t ctx->c2p_train_data_offset,\n\t\t\t\t\t\t ctx->train_data_size,\n\t\t\t\t\t\t &ctx->c2p_bo,\n\t\t\t\t\t\t NULL);\n\t\tif (ret) {\n\t\t\tDRM_ERROR(\"alloc c2p_bo failed(%d)!\\n\", ret);\n\t\t\tamdgpu_ttm_training_reserve_vram_fini(adev);\n\t\t\treturn ret;\n\t\t}\n\t\tctx->init = PSP_MEM_TRAIN_RESERVE_SUCCESS;\n\t}\n\n\tif (!adev->gmc.is_app_apu) {\n\t\tret = amdgpu_bo_create_kernel_at(\n\t\t\tadev, adev->gmc.real_vram_size - reserve_size,\n\t\t\treserve_size, &adev->mman.fw_reserved_memory, NULL);\n\t\tif (ret) {\n\t\t\tDRM_ERROR(\"alloc tmr failed(%d)!\\n\", ret);\n\t\t\tamdgpu_bo_free_kernel(&adev->mman.fw_reserved_memory,\n\t\t\t\t\t      NULL, NULL);\n\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\tDRM_DEBUG_DRIVER(\"backdoor fw loading path for PSP TMR, no reservation needed\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_ttm_pools_init(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (!adev->gmc.is_app_apu || !adev->gmc.num_mem_partitions)\n\t\treturn 0;\n\n\tadev->mman.ttm_pools = kcalloc(adev->gmc.num_mem_partitions,\n\t\t\t\t       sizeof(*adev->mman.ttm_pools),\n\t\t\t\t       GFP_KERNEL);\n\tif (!adev->mman.ttm_pools)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < adev->gmc.num_mem_partitions; i++) {\n\t\tttm_pool_init(&adev->mman.ttm_pools[i], adev->dev,\n\t\t\t      adev->gmc.mem_partitions[i].numa.node,\n\t\t\t      false, false);\n\t}\n\treturn 0;\n}\n\nstatic void amdgpu_ttm_pools_fini(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tif (!adev->gmc.is_app_apu || !adev->mman.ttm_pools)\n\t\treturn;\n\n\tfor (i = 0; i < adev->gmc.num_mem_partitions; i++)\n\t\tttm_pool_fini(&adev->mman.ttm_pools[i]);\n\n\tkfree(adev->mman.ttm_pools);\n\tadev->mman.ttm_pools = NULL;\n}\n\n \nint amdgpu_ttm_init(struct amdgpu_device *adev)\n{\n\tuint64_t gtt_size;\n\tint r;\n\n\tmutex_init(&adev->mman.gtt_window_lock);\n\n\t \n\tr = ttm_device_init(&adev->mman.bdev, &amdgpu_bo_driver, adev->dev,\n\t\t\t       adev_to_drm(adev)->anon_inode->i_mapping,\n\t\t\t       adev_to_drm(adev)->vma_offset_manager,\n\t\t\t       adev->need_swiotlb,\n\t\t\t       dma_addressing_limited(adev->dev));\n\tif (r) {\n\t\tDRM_ERROR(\"failed initializing buffer object driver(%d).\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_ttm_pools_init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to init ttm pools(%d).\\n\", r);\n\t\treturn r;\n\t}\n\tadev->mman.initialized = true;\n\n\t \n\tr = amdgpu_vram_mgr_init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing VRAM heap.\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tamdgpu_ttm_set_buffer_funcs_status(adev, false);\n#ifdef CONFIG_64BIT\n#ifdef CONFIG_X86\n\tif (adev->gmc.xgmi.connected_to_cpu)\n\t\tadev->mman.aper_base_kaddr = ioremap_cache(adev->gmc.aper_base,\n\t\t\t\tadev->gmc.visible_vram_size);\n\n\telse if (adev->gmc.is_app_apu)\n\t\tDRM_DEBUG_DRIVER(\n\t\t\t\"No need to ioremap when real vram size is 0\\n\");\n\telse\n#endif\n\t\tadev->mman.aper_base_kaddr = ioremap_wc(adev->gmc.aper_base,\n\t\t\t\tadev->gmc.visible_vram_size);\n#endif\n\n\t \n\tr = amdgpu_ttm_fw_reserve_vram_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = amdgpu_ttm_drv_reserve_vram_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tif (adev->mman.discovery_bin) {\n\t\tr = amdgpu_ttm_reserve_tmr(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tif (!adev->gmc.is_app_apu) {\n\t\tr = amdgpu_bo_create_kernel_at(adev, 0,\n\t\t\t\t\t       adev->mman.stolen_vga_size,\n\t\t\t\t\t       &adev->mman.stolen_vga_memory,\n\t\t\t\t\t       NULL);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_bo_create_kernel_at(adev, adev->mman.stolen_vga_size,\n\t\t\t\t\t       adev->mman.stolen_extended_size,\n\t\t\t\t\t       &adev->mman.stolen_extended_memory,\n\t\t\t\t\t       NULL);\n\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_bo_create_kernel_at(adev,\n\t\t\t\t\t       adev->mman.stolen_reserved_offset,\n\t\t\t\t\t       adev->mman.stolen_reserved_size,\n\t\t\t\t\t       &adev->mman.stolen_reserved_memory,\n\t\t\t\t\t       NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t} else {\n\t\tDRM_DEBUG_DRIVER(\"Skipped stolen memory reservation\\n\");\n\t}\n\n\tDRM_INFO(\"amdgpu: %uM of VRAM memory ready\\n\",\n\t\t (unsigned int)(adev->gmc.real_vram_size / (1024 * 1024)));\n\n\t \n\tif (amdgpu_gtt_size == -1)\n\t\tgtt_size = ttm_tt_pages_limit() << PAGE_SHIFT;\n\telse\n\t\tgtt_size = (uint64_t)amdgpu_gtt_size << 20;\n\n\t \n\tr = amdgpu_gtt_mgr_init(adev, gtt_size);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing GTT heap.\\n\");\n\t\treturn r;\n\t}\n\tDRM_INFO(\"amdgpu: %uM of GTT memory ready.\\n\",\n\t\t (unsigned int)(gtt_size / (1024 * 1024)));\n\n\t \n\tr = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_DOORBELL, adev->doorbell.size / PAGE_SIZE);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing doorbell heap.\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_doorbell_create_kernel_doorbells(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to initialize kernel doorbells.\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_preempt_mgr_init(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing PREEMPT heap.\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tr = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_GDS, adev->gds.gds_size);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing GDS heap.\\n\");\n\t\treturn r;\n\t}\n\n\tr = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_GWS, adev->gds.gws_size);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing gws heap.\\n\");\n\t\treturn r;\n\t}\n\n\tr = amdgpu_ttm_init_on_chip(adev, AMDGPU_PL_OA, adev->gds.oa_size);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed initializing oa heap.\\n\");\n\t\treturn r;\n\t}\n\tif (amdgpu_bo_create_kernel(adev, PAGE_SIZE, PAGE_SIZE,\n\t\t\t\tAMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t&adev->mman.sdma_access_bo, NULL,\n\t\t\t\t&adev->mman.sdma_access_ptr))\n\t\tDRM_WARN(\"Debug VRAM access will use slowpath MM access\\n\");\n\n\treturn 0;\n}\n\n \nvoid amdgpu_ttm_fini(struct amdgpu_device *adev)\n{\n\tint idx;\n\n\tif (!adev->mman.initialized)\n\t\treturn;\n\n\tamdgpu_ttm_pools_fini(adev);\n\n\tamdgpu_ttm_training_reserve_vram_fini(adev);\n\t \n\tif (!adev->gmc.is_app_apu) {\n\t\tamdgpu_bo_free_kernel(&adev->mman.stolen_vga_memory, NULL, NULL);\n\t\tamdgpu_bo_free_kernel(&adev->mman.stolen_extended_memory, NULL, NULL);\n\t\t \n\t\tamdgpu_bo_free_kernel(&adev->mman.fw_reserved_memory, NULL,\n\t\t\t\t      NULL);\n\t\tif (adev->mman.stolen_reserved_size)\n\t\t\tamdgpu_bo_free_kernel(&adev->mman.stolen_reserved_memory,\n\t\t\t\t\t      NULL, NULL);\n\t}\n\tamdgpu_bo_free_kernel(&adev->mman.sdma_access_bo, NULL,\n\t\t\t\t\t&adev->mman.sdma_access_ptr);\n\tamdgpu_ttm_fw_reserve_vram_fini(adev);\n\tamdgpu_ttm_drv_reserve_vram_fini(adev);\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\n\t\tif (adev->mman.aper_base_kaddr)\n\t\t\tiounmap(adev->mman.aper_base_kaddr);\n\t\tadev->mman.aper_base_kaddr = NULL;\n\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tamdgpu_vram_mgr_fini(adev);\n\tamdgpu_gtt_mgr_fini(adev);\n\tamdgpu_preempt_mgr_fini(adev);\n\tttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_GDS);\n\tttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_GWS);\n\tttm_range_man_fini(&adev->mman.bdev, AMDGPU_PL_OA);\n\tttm_device_fini(&adev->mman.bdev);\n\tadev->mman.initialized = false;\n\tDRM_INFO(\"amdgpu: ttm finalized\\n\");\n}\n\n \nvoid amdgpu_ttm_set_buffer_funcs_status(struct amdgpu_device *adev, bool enable)\n{\n\tstruct ttm_resource_manager *man = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);\n\tuint64_t size;\n\tint r;\n\n\tif (!adev->mman.initialized || amdgpu_in_reset(adev) ||\n\t    adev->mman.buffer_funcs_enabled == enable || adev->gmc.is_app_apu)\n\t\treturn;\n\n\tif (enable) {\n\t\tstruct amdgpu_ring *ring;\n\t\tstruct drm_gpu_scheduler *sched;\n\n\t\tring = adev->mman.buffer_funcs_ring;\n\t\tsched = &ring->sched;\n\t\tr = drm_sched_entity_init(&adev->mman.high_pr,\n\t\t\t\t\t  DRM_SCHED_PRIORITY_KERNEL, &sched,\n\t\t\t\t\t  1, NULL);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Failed setting up TTM BO move entity (%d)\\n\",\n\t\t\t\t  r);\n\t\t\treturn;\n\t\t}\n\n\t\tr = drm_sched_entity_init(&adev->mman.low_pr,\n\t\t\t\t\t  DRM_SCHED_PRIORITY_NORMAL, &sched,\n\t\t\t\t\t  1, NULL);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Failed setting up TTM BO move entity (%d)\\n\",\n\t\t\t\t  r);\n\t\t\tgoto error_free_entity;\n\t\t}\n\t} else {\n\t\tdrm_sched_entity_destroy(&adev->mman.high_pr);\n\t\tdrm_sched_entity_destroy(&adev->mman.low_pr);\n\t\tdma_fence_put(man->move);\n\t\tman->move = NULL;\n\t}\n\n\t \n\tif (enable)\n\t\tsize = adev->gmc.real_vram_size;\n\telse\n\t\tsize = adev->gmc.visible_vram_size;\n\tman->size = size;\n\tadev->mman.buffer_funcs_enabled = enable;\n\n\treturn;\n\nerror_free_entity:\n\tdrm_sched_entity_destroy(&adev->mman.high_pr);\n}\n\nstatic int amdgpu_ttm_prepare_job(struct amdgpu_device *adev,\n\t\t\t\t  bool direct_submit,\n\t\t\t\t  unsigned int num_dw,\n\t\t\t\t  struct dma_resv *resv,\n\t\t\t\t  bool vm_needs_flush,\n\t\t\t\t  struct amdgpu_job **job,\n\t\t\t\t  bool delayed)\n{\n\tenum amdgpu_ib_pool_type pool = direct_submit ?\n\t\tAMDGPU_IB_POOL_DIRECT :\n\t\tAMDGPU_IB_POOL_DELAYED;\n\tint r;\n\tstruct drm_sched_entity *entity = delayed ? &adev->mman.low_pr :\n\t\t\t\t\t\t    &adev->mman.high_pr;\n\tr = amdgpu_job_alloc_with_ib(adev, entity,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     num_dw * 4, pool, job);\n\tif (r)\n\t\treturn r;\n\n\tif (vm_needs_flush) {\n\t\t(*job)->vm_pd_addr = amdgpu_gmc_pd_addr(adev->gmc.pdb0_bo ?\n\t\t\t\t\t\t\tadev->gmc.pdb0_bo :\n\t\t\t\t\t\t\tadev->gart.bo);\n\t\t(*job)->vm_needs_flush = true;\n\t}\n\tif (!resv)\n\t\treturn 0;\n\n\treturn drm_sched_job_add_resv_dependencies(&(*job)->base, resv,\n\t\t\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n}\n\nint amdgpu_copy_buffer(struct amdgpu_ring *ring, uint64_t src_offset,\n\t\t       uint64_t dst_offset, uint32_t byte_count,\n\t\t       struct dma_resv *resv,\n\t\t       struct dma_fence **fence, bool direct_submit,\n\t\t       bool vm_needs_flush, bool tmz)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned int num_loops, num_dw;\n\tstruct amdgpu_job *job;\n\tuint32_t max_bytes;\n\tunsigned int i;\n\tint r;\n\n\tif (!direct_submit && !ring->sched.ready) {\n\t\tDRM_ERROR(\"Trying to move memory with ring turned off.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmax_bytes = adev->mman.buffer_funcs->copy_max_bytes;\n\tnum_loops = DIV_ROUND_UP(byte_count, max_bytes);\n\tnum_dw = ALIGN(num_loops * adev->mman.buffer_funcs->copy_num_dw, 8);\n\tr = amdgpu_ttm_prepare_job(adev, direct_submit, num_dw,\n\t\t\t\t   resv, vm_needs_flush, &job, false);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < num_loops; i++) {\n\t\tuint32_t cur_size_in_bytes = min(byte_count, max_bytes);\n\n\t\tamdgpu_emit_copy_buffer(adev, &job->ibs[0], src_offset,\n\t\t\t\t\tdst_offset, cur_size_in_bytes, tmz);\n\n\t\tsrc_offset += cur_size_in_bytes;\n\t\tdst_offset += cur_size_in_bytes;\n\t\tbyte_count -= cur_size_in_bytes;\n\t}\n\n\tamdgpu_ring_pad_ib(ring, &job->ibs[0]);\n\tWARN_ON(job->ibs[0].length_dw > num_dw);\n\tif (direct_submit)\n\t\tr = amdgpu_job_submit_direct(job, ring, fence);\n\telse\n\t\t*fence = amdgpu_job_submit(job);\n\tif (r)\n\t\tgoto error_free;\n\n\treturn r;\n\nerror_free:\n\tamdgpu_job_free(job);\n\tDRM_ERROR(\"Error scheduling IBs (%d)\\n\", r);\n\treturn r;\n}\n\nstatic int amdgpu_ttm_fill_mem(struct amdgpu_ring *ring, uint32_t src_data,\n\t\t\t       uint64_t dst_addr, uint32_t byte_count,\n\t\t\t       struct dma_resv *resv,\n\t\t\t       struct dma_fence **fence,\n\t\t\t       bool vm_needs_flush, bool delayed)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned int num_loops, num_dw;\n\tstruct amdgpu_job *job;\n\tuint32_t max_bytes;\n\tunsigned int i;\n\tint r;\n\n\tmax_bytes = adev->mman.buffer_funcs->fill_max_bytes;\n\tnum_loops = DIV_ROUND_UP_ULL(byte_count, max_bytes);\n\tnum_dw = ALIGN(num_loops * adev->mman.buffer_funcs->fill_num_dw, 8);\n\tr = amdgpu_ttm_prepare_job(adev, false, num_dw, resv, vm_needs_flush,\n\t\t\t\t   &job, delayed);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < num_loops; i++) {\n\t\tuint32_t cur_size = min(byte_count, max_bytes);\n\n\t\tamdgpu_emit_fill_buffer(adev, &job->ibs[0], src_data, dst_addr,\n\t\t\t\t\tcur_size);\n\n\t\tdst_addr += cur_size;\n\t\tbyte_count -= cur_size;\n\t}\n\n\tamdgpu_ring_pad_ib(ring, &job->ibs[0]);\n\tWARN_ON(job->ibs[0].length_dw > num_dw);\n\t*fence = amdgpu_job_submit(job);\n\treturn 0;\n}\n\nint amdgpu_fill_buffer(struct amdgpu_bo *bo,\n\t\t\tuint32_t src_data,\n\t\t\tstruct dma_resv *resv,\n\t\t\tstruct dma_fence **f,\n\t\t\tbool delayed)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_res_cursor dst;\n\tint r;\n\n\tif (!adev->mman.buffer_funcs_enabled) {\n\t\tDRM_ERROR(\"Trying to clear memory with ring turned off.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tamdgpu_res_first(bo->tbo.resource, 0, amdgpu_bo_size(bo), &dst);\n\n\tmutex_lock(&adev->mman.gtt_window_lock);\n\twhile (dst.remaining) {\n\t\tstruct dma_fence *next;\n\t\tuint64_t cur_size, to;\n\n\t\t \n\t\tcur_size = min(dst.size, 256ULL << 20);\n\n\t\tr = amdgpu_ttm_map_buffer(&bo->tbo, bo->tbo.resource, &dst,\n\t\t\t\t\t  1, ring, false, &cur_size, &to);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tr = amdgpu_ttm_fill_mem(ring, src_data, to, cur_size, resv,\n\t\t\t\t\t&next, true, delayed);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tdma_fence_put(fence);\n\t\tfence = next;\n\n\t\tamdgpu_res_next(&dst, cur_size);\n\t}\nerror:\n\tmutex_unlock(&adev->mman.gtt_window_lock);\n\tif (f)\n\t\t*f = dma_fence_get(fence);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\n \nint amdgpu_ttm_evict_resources(struct amdgpu_device *adev, int mem_type)\n{\n\tstruct ttm_resource_manager *man;\n\n\tswitch (mem_type) {\n\tcase TTM_PL_VRAM:\n\tcase TTM_PL_TT:\n\tcase AMDGPU_PL_GWS:\n\tcase AMDGPU_PL_GDS:\n\tcase AMDGPU_PL_OA:\n\t\tman = ttm_manager_type(&adev->mman.bdev, mem_type);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Trying to evict invalid memory type\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn ttm_resource_manager_evict_all(&adev->mman.bdev, man);\n}\n\n#if defined(CONFIG_DEBUG_FS)\n\nstatic int amdgpu_ttm_page_pool_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\n\treturn ttm_pool_debugfs(&adev->mman.bdev.pool, m);\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_ttm_page_pool);\n\n \nstatic ssize_t amdgpu_ttm_vram_read(struct file *f, char __user *buf,\n\t\t\t\t    size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev = file_inode(f)->i_private;\n\tssize_t result = 0;\n\n\tif (size & 0x3 || *pos & 0x3)\n\t\treturn -EINVAL;\n\n\tif (*pos >= adev->gmc.mc_vram_size)\n\t\treturn -ENXIO;\n\n\tsize = min(size, (size_t)(adev->gmc.mc_vram_size - *pos));\n\twhile (size) {\n\t\tsize_t bytes = min(size, AMDGPU_TTM_VRAM_MAX_DW_READ * 4);\n\t\tuint32_t value[AMDGPU_TTM_VRAM_MAX_DW_READ];\n\n\t\tamdgpu_device_vram_access(adev, *pos, value, bytes, false);\n\t\tif (copy_to_user(buf, value, bytes))\n\t\t\treturn -EFAULT;\n\n\t\tresult += bytes;\n\t\tbuf += bytes;\n\t\t*pos += bytes;\n\t\tsize -= bytes;\n\t}\n\n\treturn result;\n}\n\n \nstatic ssize_t amdgpu_ttm_vram_write(struct file *f, const char __user *buf,\n\t\t\t\t    size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev = file_inode(f)->i_private;\n\tssize_t result = 0;\n\tint r;\n\n\tif (size & 0x3 || *pos & 0x3)\n\t\treturn -EINVAL;\n\n\tif (*pos >= adev->gmc.mc_vram_size)\n\t\treturn -ENXIO;\n\n\twhile (size) {\n\t\tuint32_t value;\n\n\t\tif (*pos >= adev->gmc.mc_vram_size)\n\t\t\treturn result;\n\n\t\tr = get_user(value, (uint32_t *)buf);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tamdgpu_device_mm_access(adev, *pos, &value, 4, true);\n\n\t\tresult += 4;\n\t\tbuf += 4;\n\t\t*pos += 4;\n\t\tsize -= 4;\n\t}\n\n\treturn result;\n}\n\nstatic const struct file_operations amdgpu_ttm_vram_fops = {\n\t.owner = THIS_MODULE,\n\t.read = amdgpu_ttm_vram_read,\n\t.write = amdgpu_ttm_vram_write,\n\t.llseek = default_llseek,\n};\n\n \nstatic ssize_t amdgpu_iomem_read(struct file *f, char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev = file_inode(f)->i_private;\n\tstruct iommu_domain *dom;\n\tssize_t result = 0;\n\tint r;\n\n\t \n\tdom = iommu_get_domain_for_dev(adev->dev);\n\n\twhile (size) {\n\t\tphys_addr_t addr = *pos & PAGE_MASK;\n\t\tloff_t off = *pos & ~PAGE_MASK;\n\t\tsize_t bytes = PAGE_SIZE - off;\n\t\tunsigned long pfn;\n\t\tstruct page *p;\n\t\tvoid *ptr;\n\n\t\tbytes = min(bytes, size);\n\n\t\t \n\t\taddr = dom ? iommu_iova_to_phys(dom, addr) : addr;\n\n\t\tpfn = addr >> PAGE_SHIFT;\n\t\tif (!pfn_valid(pfn))\n\t\t\treturn -EPERM;\n\n\t\tp = pfn_to_page(pfn);\n\t\tif (p->mapping != adev->mman.bdev.dev_mapping)\n\t\t\treturn -EPERM;\n\n\t\tptr = kmap_local_page(p);\n\t\tr = copy_to_user(buf, ptr + off, bytes);\n\t\tkunmap_local(ptr);\n\t\tif (r)\n\t\t\treturn -EFAULT;\n\n\t\tsize -= bytes;\n\t\t*pos += bytes;\n\t\tresult += bytes;\n\t}\n\n\treturn result;\n}\n\n \nstatic ssize_t amdgpu_iomem_write(struct file *f, const char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev = file_inode(f)->i_private;\n\tstruct iommu_domain *dom;\n\tssize_t result = 0;\n\tint r;\n\n\tdom = iommu_get_domain_for_dev(adev->dev);\n\n\twhile (size) {\n\t\tphys_addr_t addr = *pos & PAGE_MASK;\n\t\tloff_t off = *pos & ~PAGE_MASK;\n\t\tsize_t bytes = PAGE_SIZE - off;\n\t\tunsigned long pfn;\n\t\tstruct page *p;\n\t\tvoid *ptr;\n\n\t\tbytes = min(bytes, size);\n\n\t\taddr = dom ? iommu_iova_to_phys(dom, addr) : addr;\n\n\t\tpfn = addr >> PAGE_SHIFT;\n\t\tif (!pfn_valid(pfn))\n\t\t\treturn -EPERM;\n\n\t\tp = pfn_to_page(pfn);\n\t\tif (p->mapping != adev->mman.bdev.dev_mapping)\n\t\t\treturn -EPERM;\n\n\t\tptr = kmap_local_page(p);\n\t\tr = copy_from_user(ptr + off, buf, bytes);\n\t\tkunmap_local(ptr);\n\t\tif (r)\n\t\t\treturn -EFAULT;\n\n\t\tsize -= bytes;\n\t\t*pos += bytes;\n\t\tresult += bytes;\n\t}\n\n\treturn result;\n}\n\nstatic const struct file_operations amdgpu_ttm_iomem_fops = {\n\t.owner = THIS_MODULE,\n\t.read = amdgpu_iomem_read,\n\t.write = amdgpu_iomem_write,\n\t.llseek = default_llseek\n};\n\n#endif\n\nvoid amdgpu_ttm_debugfs_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file_size(\"amdgpu_vram\", 0444, root, adev,\n\t\t\t\t &amdgpu_ttm_vram_fops, adev->gmc.mc_vram_size);\n\tdebugfs_create_file(\"amdgpu_iomem\", 0444, root, adev,\n\t\t\t    &amdgpu_ttm_iomem_fops);\n\tdebugfs_create_file(\"ttm_page_pool\", 0444, root, adev,\n\t\t\t    &amdgpu_ttm_page_pool_fops);\n\tttm_resource_manager_create_debugfs(ttm_manager_type(&adev->mman.bdev,\n\t\t\t\t\t\t\t     TTM_PL_VRAM),\n\t\t\t\t\t    root, \"amdgpu_vram_mm\");\n\tttm_resource_manager_create_debugfs(ttm_manager_type(&adev->mman.bdev,\n\t\t\t\t\t\t\t     TTM_PL_TT),\n\t\t\t\t\t    root, \"amdgpu_gtt_mm\");\n\tttm_resource_manager_create_debugfs(ttm_manager_type(&adev->mman.bdev,\n\t\t\t\t\t\t\t     AMDGPU_PL_GDS),\n\t\t\t\t\t    root, \"amdgpu_gds_mm\");\n\tttm_resource_manager_create_debugfs(ttm_manager_type(&adev->mman.bdev,\n\t\t\t\t\t\t\t     AMDGPU_PL_GWS),\n\t\t\t\t\t    root, \"amdgpu_gws_mm\");\n\tttm_resource_manager_create_debugfs(ttm_manager_type(&adev->mman.bdev,\n\t\t\t\t\t\t\t     AMDGPU_PL_OA),\n\t\t\t\t\t    root, \"amdgpu_oa_mm\");\n\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}