{
  "module_name": "amdgpu_fence.c",
  "hash_id": "5e1e12e48b857ff8f4a74a870430b1decee3dd68002a0ec8807e26bfb46c919d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c",
  "human_readable_source": " \n \n#include <linux/seq_file.h>\n#include <linux/atomic.h>\n#include <linux/wait.h>\n#include <linux/kref.h>\n#include <linux/slab.h>\n#include <linux/firmware.h>\n#include <linux/pm_runtime.h>\n\n#include <drm/drm_drv.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_reset.h\"\n\n \n\nstruct amdgpu_fence {\n\tstruct dma_fence base;\n\n\t \n\tstruct amdgpu_ring\t\t*ring;\n\tktime_t\t\t\t\tstart_timestamp;\n};\n\nstatic struct kmem_cache *amdgpu_fence_slab;\n\nint amdgpu_fence_slab_init(void)\n{\n\tamdgpu_fence_slab = kmem_cache_create(\n\t\t\"amdgpu_fence\", sizeof(struct amdgpu_fence), 0,\n\t\tSLAB_HWCACHE_ALIGN, NULL);\n\tif (!amdgpu_fence_slab)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid amdgpu_fence_slab_fini(void)\n{\n\trcu_barrier();\n\tkmem_cache_destroy(amdgpu_fence_slab);\n}\n \nstatic const struct dma_fence_ops amdgpu_fence_ops;\nstatic const struct dma_fence_ops amdgpu_job_fence_ops;\nstatic inline struct amdgpu_fence *to_amdgpu_fence(struct dma_fence *f)\n{\n\tstruct amdgpu_fence *__f = container_of(f, struct amdgpu_fence, base);\n\n\tif (__f->base.ops == &amdgpu_fence_ops ||\n\t    __f->base.ops == &amdgpu_job_fence_ops)\n\t\treturn __f;\n\n\treturn NULL;\n}\n\n \nstatic void amdgpu_fence_write(struct amdgpu_ring *ring, u32 seq)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\n\tif (drv->cpu_addr)\n\t\t*drv->cpu_addr = cpu_to_le32(seq);\n}\n\n \nstatic u32 amdgpu_fence_read(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\tu32 seq = 0;\n\n\tif (drv->cpu_addr)\n\t\tseq = le32_to_cpu(*drv->cpu_addr);\n\telse\n\t\tseq = atomic_read(&drv->last_seq);\n\n\treturn seq;\n}\n\n \nint amdgpu_fence_emit(struct amdgpu_ring *ring, struct dma_fence **f, struct amdgpu_job *job,\n\t\t      unsigned int flags)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct dma_fence *fence;\n\tstruct amdgpu_fence *am_fence;\n\tstruct dma_fence __rcu **ptr;\n\tuint32_t seq;\n\tint r;\n\n\tif (job == NULL) {\n\t\t \n\t\tam_fence = kmem_cache_alloc(amdgpu_fence_slab, GFP_ATOMIC);\n\t\tif (am_fence == NULL)\n\t\t\treturn -ENOMEM;\n\t\tfence = &am_fence->base;\n\t\tam_fence->ring = ring;\n\t} else {\n\t\t \n\t\tfence = &job->hw_fence;\n\t}\n\n\tseq = ++ring->fence_drv.sync_seq;\n\tif (job && job->job_run_counter) {\n\t\t \n\t\tfence->seqno = seq;\n\t\t \n\t\tdma_fence_get(fence);\n\t} else {\n\t\tif (job) {\n\t\t\tdma_fence_init(fence, &amdgpu_job_fence_ops,\n\t\t\t\t       &ring->fence_drv.lock,\n\t\t\t\t       adev->fence_context + ring->idx, seq);\n\t\t\t \n\t\t\tdma_fence_get(fence);\n\t\t} else {\n\t\t\tdma_fence_init(fence, &amdgpu_fence_ops,\n\t\t\t\t       &ring->fence_drv.lock,\n\t\t\t\t       adev->fence_context + ring->idx, seq);\n\t\t}\n\t}\n\n\tamdgpu_ring_emit_fence(ring, ring->fence_drv.gpu_addr,\n\t\t\t       seq, flags | AMDGPU_FENCE_FLAG_INT);\n\tpm_runtime_get_noresume(adev_to_drm(adev)->dev);\n\tptr = &ring->fence_drv.fences[seq & ring->fence_drv.num_fences_mask];\n\tif (unlikely(rcu_dereference_protected(*ptr, 1))) {\n\t\tstruct dma_fence *old;\n\n\t\trcu_read_lock();\n\t\told = dma_fence_get_rcu_safe(ptr);\n\t\trcu_read_unlock();\n\n\t\tif (old) {\n\t\t\tr = dma_fence_wait(old, false);\n\t\t\tdma_fence_put(old);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\tto_amdgpu_fence(fence)->start_timestamp = ktime_get();\n\n\t \n\trcu_assign_pointer(*ptr, dma_fence_get(fence));\n\n\t*f = fence;\n\n\treturn 0;\n}\n\n \nint amdgpu_fence_emit_polling(struct amdgpu_ring *ring, uint32_t *s,\n\t\t\t      uint32_t timeout)\n{\n\tuint32_t seq;\n\tsigned long r;\n\n\tif (!s)\n\t\treturn -EINVAL;\n\n\tseq = ++ring->fence_drv.sync_seq;\n\tr = amdgpu_fence_wait_polling(ring,\n\t\t\t\t      seq - ring->fence_drv.num_fences_mask,\n\t\t\t\t      timeout);\n\tif (r < 1)\n\t\treturn -ETIMEDOUT;\n\n\tamdgpu_ring_emit_fence(ring, ring->fence_drv.gpu_addr,\n\t\t\t       seq, 0);\n\n\t*s = seq;\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_fence_schedule_fallback(struct amdgpu_ring *ring)\n{\n\tmod_timer(&ring->fence_drv.fallback_timer,\n\t\t  jiffies + AMDGPU_FENCE_JIFFIES_TIMEOUT);\n}\n\n \nbool amdgpu_fence_process(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t seq, last_seq;\n\n\tdo {\n\t\tlast_seq = atomic_read(&ring->fence_drv.last_seq);\n\t\tseq = amdgpu_fence_read(ring);\n\n\t} while (atomic_cmpxchg(&drv->last_seq, last_seq, seq) != last_seq);\n\n\tif (del_timer(&ring->fence_drv.fallback_timer) &&\n\t    seq != ring->fence_drv.sync_seq)\n\t\tamdgpu_fence_schedule_fallback(ring);\n\n\tif (unlikely(seq == last_seq))\n\t\treturn false;\n\n\tlast_seq &= drv->num_fences_mask;\n\tseq &= drv->num_fences_mask;\n\n\tdo {\n\t\tstruct dma_fence *fence, **ptr;\n\n\t\t++last_seq;\n\t\tlast_seq &= drv->num_fences_mask;\n\t\tptr = &drv->fences[last_seq];\n\n\t\t \n\t\tfence = rcu_dereference_protected(*ptr, 1);\n\t\tRCU_INIT_POINTER(*ptr, NULL);\n\n\t\tif (!fence)\n\t\t\tcontinue;\n\n\t\tdma_fence_signal(fence);\n\t\tdma_fence_put(fence);\n\t\tpm_runtime_mark_last_busy(adev_to_drm(adev)->dev);\n\t\tpm_runtime_put_autosuspend(adev_to_drm(adev)->dev);\n\t} while (last_seq != seq);\n\n\treturn true;\n}\n\n \nstatic void amdgpu_fence_fallback(struct timer_list *t)\n{\n\tstruct amdgpu_ring *ring = from_timer(ring, t,\n\t\t\t\t\t      fence_drv.fallback_timer);\n\n\tif (amdgpu_fence_process(ring))\n\t\tDRM_WARN(\"Fence fallback timer expired on ring %s\\n\", ring->name);\n}\n\n \nint amdgpu_fence_wait_empty(struct amdgpu_ring *ring)\n{\n\tuint64_t seq = READ_ONCE(ring->fence_drv.sync_seq);\n\tstruct dma_fence *fence, **ptr;\n\tint r;\n\n\tif (!seq)\n\t\treturn 0;\n\n\tptr = &ring->fence_drv.fences[seq & ring->fence_drv.num_fences_mask];\n\trcu_read_lock();\n\tfence = rcu_dereference(*ptr);\n\tif (!fence || !dma_fence_get_rcu(fence)) {\n\t\trcu_read_unlock();\n\t\treturn 0;\n\t}\n\trcu_read_unlock();\n\n\tr = dma_fence_wait(fence, false);\n\tdma_fence_put(fence);\n\treturn r;\n}\n\n \nsigned long amdgpu_fence_wait_polling(struct amdgpu_ring *ring,\n\t\t\t\t      uint32_t wait_seq,\n\t\t\t\t      signed long timeout)\n{\n\n\twhile ((int32_t)(wait_seq - amdgpu_fence_read(ring)) > 0 && timeout > 0) {\n\t\tudelay(2);\n\t\ttimeout -= 2;\n\t}\n\treturn timeout > 0 ? timeout : 0;\n}\n \nunsigned int amdgpu_fence_count_emitted(struct amdgpu_ring *ring)\n{\n\tuint64_t emitted;\n\n\t \n\temitted = 0x100000000ull;\n\temitted -= atomic_read(&ring->fence_drv.last_seq);\n\temitted += READ_ONCE(ring->fence_drv.sync_seq);\n\treturn lower_32_bits(emitted);\n}\n\n \nu64 amdgpu_fence_last_unsignaled_time_us(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\tstruct dma_fence *fence;\n\tuint32_t last_seq, sync_seq;\n\n\tlast_seq = atomic_read(&ring->fence_drv.last_seq);\n\tsync_seq = READ_ONCE(ring->fence_drv.sync_seq);\n\tif (last_seq == sync_seq)\n\t\treturn 0;\n\n\t++last_seq;\n\tlast_seq &= drv->num_fences_mask;\n\tfence = drv->fences[last_seq];\n\tif (!fence)\n\t\treturn 0;\n\n\treturn ktime_us_delta(ktime_get(),\n\t\tto_amdgpu_fence(fence)->start_timestamp);\n}\n\n \nvoid amdgpu_fence_update_start_timestamp(struct amdgpu_ring *ring, uint32_t seq, ktime_t timestamp)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\tstruct dma_fence *fence;\n\n\tseq &= drv->num_fences_mask;\n\tfence = drv->fences[seq];\n\tif (!fence)\n\t\treturn;\n\n\tto_amdgpu_fence(fence)->start_timestamp = timestamp;\n}\n\n \nint amdgpu_fence_driver_start_ring(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_irq_src *irq_src,\n\t\t\t\t   unsigned int irq_type)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint64_t index;\n\n\tif (ring->funcs->type != AMDGPU_RING_TYPE_UVD) {\n\t\tring->fence_drv.cpu_addr = ring->fence_cpu_addr;\n\t\tring->fence_drv.gpu_addr = ring->fence_gpu_addr;\n\t} else {\n\t\t \n\t\tindex = ALIGN(adev->uvd.fw->size, 8);\n\t\tring->fence_drv.cpu_addr = adev->uvd.inst[ring->me].cpu_addr + index;\n\t\tring->fence_drv.gpu_addr = adev->uvd.inst[ring->me].gpu_addr + index;\n\t}\n\tamdgpu_fence_write(ring, atomic_read(&ring->fence_drv.last_seq));\n\n\tring->fence_drv.irq_src = irq_src;\n\tring->fence_drv.irq_type = irq_type;\n\tring->fence_drv.initialized = true;\n\n\tDRM_DEV_DEBUG(adev->dev, \"fence driver on ring %s use gpu addr 0x%016llx\\n\",\n\t\t      ring->name, ring->fence_drv.gpu_addr);\n\treturn 0;\n}\n\n \nint amdgpu_fence_driver_init_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (!adev)\n\t\treturn -EINVAL;\n\n\tif (!is_power_of_2(ring->num_hw_submission))\n\t\treturn -EINVAL;\n\n\tring->fence_drv.cpu_addr = NULL;\n\tring->fence_drv.gpu_addr = 0;\n\tring->fence_drv.sync_seq = 0;\n\tatomic_set(&ring->fence_drv.last_seq, 0);\n\tring->fence_drv.initialized = false;\n\n\ttimer_setup(&ring->fence_drv.fallback_timer, amdgpu_fence_fallback, 0);\n\n\tring->fence_drv.num_fences_mask = ring->num_hw_submission * 2 - 1;\n\tspin_lock_init(&ring->fence_drv.lock);\n\tring->fence_drv.fences = kcalloc(ring->num_hw_submission * 2, sizeof(void *),\n\t\t\t\t\t GFP_KERNEL);\n\n\tif (!ring->fence_drv.fences)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nint amdgpu_fence_driver_sw_init(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\n \nstatic bool amdgpu_fence_need_ring_interrupt_restore(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tbool is_gfx_power_domain = false;\n\n\tswitch (ring->funcs->type) {\n\tcase AMDGPU_RING_TYPE_SDMA:\n\t \n\t\tif (adev->ip_versions[SDMA0_HWIP][0] >= IP_VERSION(5, 0, 0))\n\t\t\tis_gfx_power_domain = true;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_GFX:\n\tcase AMDGPU_RING_TYPE_COMPUTE:\n\tcase AMDGPU_RING_TYPE_KIQ:\n\tcase AMDGPU_RING_TYPE_MES:\n\t\tis_gfx_power_domain = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn !(adev->in_s0ix && is_gfx_power_domain);\n}\n\n \nvoid amdgpu_fence_driver_hw_fini(struct amdgpu_device *adev)\n{\n\tint i, r;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; i++) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->fence_drv.initialized)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!drm_dev_is_unplugged(adev_to_drm(adev)))\n\t\t\tr = amdgpu_fence_wait_empty(ring);\n\t\telse\n\t\t\tr = -ENODEV;\n\t\t \n\t\tif (r)\n\t\t\tamdgpu_fence_driver_force_completion(ring);\n\n\t\tif (!drm_dev_is_unplugged(adev_to_drm(adev)) &&\n\t\t    ring->fence_drv.irq_src &&\n\t\t    amdgpu_fence_need_ring_interrupt_restore(ring))\n\t\t\tamdgpu_irq_put(adev, ring->fence_drv.irq_src,\n\t\t\t\t       ring->fence_drv.irq_type);\n\n\t\tdel_timer_sync(&ring->fence_drv.fallback_timer);\n\t}\n}\n\n \nvoid amdgpu_fence_driver_isr_toggle(struct amdgpu_device *adev, bool stop)\n{\n\tint i;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; i++) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->fence_drv.initialized || !ring->fence_drv.irq_src)\n\t\t\tcontinue;\n\n\t\tif (stop)\n\t\t\tdisable_irq(adev->irq.irq);\n\t\telse\n\t\t\tenable_irq(adev->irq.irq);\n\t}\n}\n\nvoid amdgpu_fence_driver_sw_fini(struct amdgpu_device *adev)\n{\n\tunsigned int i, j;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; i++) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->fence_drv.initialized)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (ring->sched.ops)\n\t\t\tdrm_sched_fini(&ring->sched);\n\n\t\tfor (j = 0; j <= ring->fence_drv.num_fences_mask; ++j)\n\t\t\tdma_fence_put(ring->fence_drv.fences[j]);\n\t\tkfree(ring->fence_drv.fences);\n\t\tring->fence_drv.fences = NULL;\n\t\tring->fence_drv.initialized = false;\n\t}\n}\n\n \nvoid amdgpu_fence_driver_hw_init(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; i++) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->fence_drv.initialized)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (ring->fence_drv.irq_src &&\n\t\t    amdgpu_fence_need_ring_interrupt_restore(ring))\n\t\t\tamdgpu_irq_get(adev, ring->fence_drv.irq_src,\n\t\t\t\t       ring->fence_drv.irq_type);\n\t}\n}\n\n \nvoid amdgpu_fence_driver_clear_job_fences(struct amdgpu_ring *ring)\n{\n\tint i;\n\tstruct dma_fence *old, **ptr;\n\n\tfor (i = 0; i <= ring->fence_drv.num_fences_mask; i++) {\n\t\tptr = &ring->fence_drv.fences[i];\n\t\told = rcu_dereference_protected(*ptr, 1);\n\t\tif (old && old->ops == &amdgpu_job_fence_ops) {\n\t\t\tstruct amdgpu_job *job;\n\n\t\t\t \n\t\t\tjob = container_of(old, struct amdgpu_job, hw_fence);\n\t\t\tif (!job->base.s_fence && !dma_fence_is_signaled(old))\n\t\t\t\tdma_fence_signal(old);\n\t\t\tRCU_INIT_POINTER(*ptr, NULL);\n\t\t\tdma_fence_put(old);\n\t\t}\n\t}\n}\n\n \nvoid amdgpu_fence_driver_set_error(struct amdgpu_ring *ring, int error)\n{\n\tstruct amdgpu_fence_driver *drv = &ring->fence_drv;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&drv->lock, flags);\n\tfor (unsigned int i = 0; i <= drv->num_fences_mask; ++i) {\n\t\tstruct dma_fence *fence;\n\n\t\tfence = rcu_dereference_protected(drv->fences[i],\n\t\t\t\t\t\t  lockdep_is_held(&drv->lock));\n\t\tif (fence && !dma_fence_is_signaled_locked(fence))\n\t\t\tdma_fence_set_error(fence, error);\n\t}\n\tspin_unlock_irqrestore(&drv->lock, flags);\n}\n\n \nvoid amdgpu_fence_driver_force_completion(struct amdgpu_ring *ring)\n{\n\tamdgpu_fence_driver_set_error(ring, -ECANCELED);\n\tamdgpu_fence_write(ring, ring->fence_drv.sync_seq);\n\tamdgpu_fence_process(ring);\n}\n\n \n\nstatic const char *amdgpu_fence_get_driver_name(struct dma_fence *fence)\n{\n\treturn \"amdgpu\";\n}\n\nstatic const char *amdgpu_fence_get_timeline_name(struct dma_fence *f)\n{\n\treturn (const char *)to_amdgpu_fence(f)->ring->name;\n}\n\nstatic const char *amdgpu_job_fence_get_timeline_name(struct dma_fence *f)\n{\n\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence);\n\n\treturn (const char *)to_amdgpu_ring(job->base.sched)->name;\n}\n\n \nstatic bool amdgpu_fence_enable_signaling(struct dma_fence *f)\n{\n\tif (!timer_pending(&to_amdgpu_fence(f)->ring->fence_drv.fallback_timer))\n\t\tamdgpu_fence_schedule_fallback(to_amdgpu_fence(f)->ring);\n\n\treturn true;\n}\n\n \nstatic bool amdgpu_job_fence_enable_signaling(struct dma_fence *f)\n{\n\tstruct amdgpu_job *job = container_of(f, struct amdgpu_job, hw_fence);\n\n\tif (!timer_pending(&to_amdgpu_ring(job->base.sched)->fence_drv.fallback_timer))\n\t\tamdgpu_fence_schedule_fallback(to_amdgpu_ring(job->base.sched));\n\n\treturn true;\n}\n\n \nstatic void amdgpu_fence_free(struct rcu_head *rcu)\n{\n\tstruct dma_fence *f = container_of(rcu, struct dma_fence, rcu);\n\n\t \n\tkmem_cache_free(amdgpu_fence_slab, to_amdgpu_fence(f));\n}\n\n \nstatic void amdgpu_job_fence_free(struct rcu_head *rcu)\n{\n\tstruct dma_fence *f = container_of(rcu, struct dma_fence, rcu);\n\n\t \n\tkfree(container_of(f, struct amdgpu_job, hw_fence));\n}\n\n \nstatic void amdgpu_fence_release(struct dma_fence *f)\n{\n\tcall_rcu(&f->rcu, amdgpu_fence_free);\n}\n\n \nstatic void amdgpu_job_fence_release(struct dma_fence *f)\n{\n\tcall_rcu(&f->rcu, amdgpu_job_fence_free);\n}\n\nstatic const struct dma_fence_ops amdgpu_fence_ops = {\n\t.get_driver_name = amdgpu_fence_get_driver_name,\n\t.get_timeline_name = amdgpu_fence_get_timeline_name,\n\t.enable_signaling = amdgpu_fence_enable_signaling,\n\t.release = amdgpu_fence_release,\n};\n\nstatic const struct dma_fence_ops amdgpu_job_fence_ops = {\n\t.get_driver_name = amdgpu_fence_get_driver_name,\n\t.get_timeline_name = amdgpu_job_fence_get_timeline_name,\n\t.enable_signaling = amdgpu_job_fence_enable_signaling,\n\t.release = amdgpu_job_fence_release,\n};\n\n \n#if defined(CONFIG_DEBUG_FS)\nstatic int amdgpu_debugfs_fence_info_show(struct seq_file *m, void *unused)\n{\n\tstruct amdgpu_device *adev = m->private;\n\tint i;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (!ring || !ring->fence_drv.initialized)\n\t\t\tcontinue;\n\n\t\tamdgpu_fence_process(ring);\n\n\t\tseq_printf(m, \"--- ring %d (%s) ---\\n\", i, ring->name);\n\t\tseq_printf(m, \"Last signaled fence          0x%08x\\n\",\n\t\t\t   atomic_read(&ring->fence_drv.last_seq));\n\t\tseq_printf(m, \"Last emitted                 0x%08x\\n\",\n\t\t\t   ring->fence_drv.sync_seq);\n\n\t\tif (ring->funcs->type == AMDGPU_RING_TYPE_GFX ||\n\t\t    ring->funcs->type == AMDGPU_RING_TYPE_SDMA) {\n\t\t\tseq_printf(m, \"Last signaled trailing fence 0x%08x\\n\",\n\t\t\t\t   le32_to_cpu(*ring->trail_fence_cpu_addr));\n\t\t\tseq_printf(m, \"Last emitted                 0x%08x\\n\",\n\t\t\t\t   ring->trail_seq);\n\t\t}\n\n\t\tif (ring->funcs->type != AMDGPU_RING_TYPE_GFX)\n\t\t\tcontinue;\n\n\t\t \n\t\tseq_printf(m, \"Last preempted               0x%08x\\n\",\n\t\t\t   le32_to_cpu(*(ring->fence_drv.cpu_addr + 2)));\n\t\t \n\t\tseq_printf(m, \"Last reset                   0x%08x\\n\",\n\t\t\t   le32_to_cpu(*(ring->fence_drv.cpu_addr + 4)));\n\t\t \n\t\tseq_printf(m, \"Last both                    0x%08x\\n\",\n\t\t\t   le32_to_cpu(*(ring->fence_drv.cpu_addr + 6)));\n\t}\n\treturn 0;\n}\n\n \nstatic int gpu_recover_get(void *data, u64 *val)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)data;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tint r;\n\n\tr = pm_runtime_get_sync(dev->dev);\n\tif (r < 0) {\n\t\tpm_runtime_put_autosuspend(dev->dev);\n\t\treturn 0;\n\t}\n\n\tif (amdgpu_reset_domain_schedule(adev->reset_domain, &adev->reset_work))\n\t\tflush_work(&adev->reset_work);\n\n\t*val = atomic_read(&adev->reset_domain->reset_res);\n\n\tpm_runtime_mark_last_busy(dev->dev);\n\tpm_runtime_put_autosuspend(dev->dev);\n\n\treturn 0;\n}\n\nDEFINE_SHOW_ATTRIBUTE(amdgpu_debugfs_fence_info);\nDEFINE_DEBUGFS_ATTRIBUTE(amdgpu_debugfs_gpu_recover_fops, gpu_recover_get, NULL,\n\t\t\t \"%lld\\n\");\n\nstatic void amdgpu_debugfs_reset_work(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev = container_of(work, struct amdgpu_device,\n\t\t\t\t\t\t  reset_work);\n\n\tstruct amdgpu_reset_context reset_context;\n\n\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\treset_context.method = AMD_RESET_METHOD_NONE;\n\treset_context.reset_req_dev = adev;\n\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\tamdgpu_device_gpu_recover(adev, NULL, &reset_context);\n}\n\n#endif\n\nvoid amdgpu_debugfs_fence_init(struct amdgpu_device *adev)\n{\n#if defined(CONFIG_DEBUG_FS)\n\tstruct drm_minor *minor = adev_to_drm(adev)->primary;\n\tstruct dentry *root = minor->debugfs_root;\n\n\tdebugfs_create_file(\"amdgpu_fence_info\", 0444, root, adev,\n\t\t\t    &amdgpu_debugfs_fence_info_fops);\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\n\t\tINIT_WORK(&adev->reset_work, amdgpu_debugfs_reset_work);\n\t\tdebugfs_create_file(\"amdgpu_gpu_recover\", 0444, root, adev,\n\t\t\t\t    &amdgpu_debugfs_gpu_recover_fops);\n\t}\n#endif\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}