{
  "module_name": "amdgpu_xgmi.c",
  "hash_id": "53852916b145c5d2437dd51d9d59594cea63bbf6f9aaf6ab7d1a17829b0563c1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_xgmi.c",
  "human_readable_source": " \n#include <linux/list.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_xgmi.h\"\n#include \"amdgpu_ras.h\"\n#include \"soc15.h\"\n#include \"df/df_3_6_offset.h\"\n#include \"xgmi/xgmi_4_0_0_smn.h\"\n#include \"xgmi/xgmi_4_0_0_sh_mask.h\"\n#include \"xgmi/xgmi_6_1_0_sh_mask.h\"\n#include \"wafl/wafl2_4_0_0_smn.h\"\n#include \"wafl/wafl2_4_0_0_sh_mask.h\"\n\n#include \"amdgpu_reset.h\"\n\n#define smnPCS_XGMI3X16_PCS_ERROR_STATUS 0x11a0020c\n#define smnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK   0x11a00218\n#define smnPCS_GOPX1_PCS_ERROR_STATUS    0x12200210\n#define smnPCS_GOPX1_PCS_ERROR_NONCORRECTABLE_MASK      0x12200218\n\nstatic DEFINE_MUTEX(xgmi_mutex);\n\n#define AMDGPU_MAX_XGMI_DEVICE_PER_HIVE\t\t4\n\nstatic LIST_HEAD(xgmi_hive_list);\n\nstatic const int xgmi_pcs_err_status_reg_vg20[] = {\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x100000,\n};\n\nstatic const int wafl_pcs_err_status_reg_vg20[] = {\n\tsmnPCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS,\n\tsmnPCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS + 0x100000,\n};\n\nstatic const int xgmi_pcs_err_status_reg_arct[] = {\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x100000,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x500000,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x600000,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x700000,\n\tsmnXGMI0_PCS_GOPX16_PCS_ERROR_STATUS + 0x800000,\n};\n\n \nstatic const int wafl_pcs_err_status_reg_arct[] = {\n\tsmnPCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS,\n\tsmnPCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS + 0x100000,\n};\n\nstatic const int xgmi3x16_pcs_err_status_reg_aldebaran[] = {\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x100000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x200000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x300000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x400000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x500000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x600000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_STATUS + 0x700000\n};\n\nstatic const int xgmi3x16_pcs_err_noncorrectable_mask_reg_aldebaran[] = {\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x100000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x200000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x300000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x400000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x500000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x600000,\n\tsmnPCS_XGMI3X16_PCS_ERROR_NONCORRECTABLE_MASK + 0x700000\n};\n\nstatic const int walf_pcs_err_status_reg_aldebaran[] = {\n\tsmnPCS_GOPX1_PCS_ERROR_STATUS,\n\tsmnPCS_GOPX1_PCS_ERROR_STATUS + 0x100000\n};\n\nstatic const int walf_pcs_err_noncorrectable_mask_reg_aldebaran[] = {\n\tsmnPCS_GOPX1_PCS_ERROR_NONCORRECTABLE_MASK,\n\tsmnPCS_GOPX1_PCS_ERROR_NONCORRECTABLE_MASK + 0x100000\n};\n\nstatic const struct amdgpu_pcs_ras_field xgmi_pcs_ras_fields[] = {\n\t{\"XGMI PCS DataLossErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, DataLossErr)},\n\t{\"XGMI PCS TrainingErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, TrainingErr)},\n\t{\"XGMI PCS CRCErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, CRCErr)},\n\t{\"XGMI PCS BERExceededErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, BERExceededErr)},\n\t{\"XGMI PCS TxMetaDataErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, TxMetaDataErr)},\n\t{\"XGMI PCS ReplayBufParityErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ReplayBufParityErr)},\n\t{\"XGMI PCS DataParityErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, DataParityErr)},\n\t{\"XGMI PCS ReplayFifoOverflowErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ReplayFifoOverflowErr)},\n\t{\"XGMI PCS ReplayFifoUnderflowErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ReplayFifoUnderflowErr)},\n\t{\"XGMI PCS ElasticFifoOverflowErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ElasticFifoOverflowErr)},\n\t{\"XGMI PCS DeskewErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, DeskewErr)},\n\t{\"XGMI PCS DataStartupLimitErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, DataStartupLimitErr)},\n\t{\"XGMI PCS FCInitTimeoutErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, FCInitTimeoutErr)},\n\t{\"XGMI PCS RecoveryTimeoutErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, RecoveryTimeoutErr)},\n\t{\"XGMI PCS ReadySerialTimeoutErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ReadySerialTimeoutErr)},\n\t{\"XGMI PCS ReadySerialAttemptErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, ReadySerialAttemptErr)},\n\t{\"XGMI PCS RecoveryAttemptErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, RecoveryAttemptErr)},\n\t{\"XGMI PCS RecoveryRelockAttemptErr\",\n\t SOC15_REG_FIELD(XGMI0_PCS_GOPX16_PCS_ERROR_STATUS, RecoveryRelockAttemptErr)},\n};\n\nstatic const struct amdgpu_pcs_ras_field wafl_pcs_ras_fields[] = {\n\t{\"WAFL PCS DataLossErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, DataLossErr)},\n\t{\"WAFL PCS TrainingErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, TrainingErr)},\n\t{\"WAFL PCS CRCErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, CRCErr)},\n\t{\"WAFL PCS BERExceededErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, BERExceededErr)},\n\t{\"WAFL PCS TxMetaDataErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, TxMetaDataErr)},\n\t{\"WAFL PCS ReplayBufParityErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ReplayBufParityErr)},\n\t{\"WAFL PCS DataParityErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, DataParityErr)},\n\t{\"WAFL PCS ReplayFifoOverflowErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ReplayFifoOverflowErr)},\n\t{\"WAFL PCS ReplayFifoUnderflowErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ReplayFifoUnderflowErr)},\n\t{\"WAFL PCS ElasticFifoOverflowErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ElasticFifoOverflowErr)},\n\t{\"WAFL PCS DeskewErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, DeskewErr)},\n\t{\"WAFL PCS DataStartupLimitErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, DataStartupLimitErr)},\n\t{\"WAFL PCS FCInitTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, FCInitTimeoutErr)},\n\t{\"WAFL PCS RecoveryTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, RecoveryTimeoutErr)},\n\t{\"WAFL PCS ReadySerialTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ReadySerialTimeoutErr)},\n\t{\"WAFL PCS ReadySerialAttemptErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, ReadySerialAttemptErr)},\n\t{\"WAFL PCS RecoveryAttemptErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, RecoveryAttemptErr)},\n\t{\"WAFL PCS RecoveryRelockAttemptErr\",\n\t SOC15_REG_FIELD(PCS_GOPX1_0_PCS_GOPX1_PCS_ERROR_STATUS, RecoveryRelockAttemptErr)},\n};\n\nstatic const struct amdgpu_pcs_ras_field xgmi3x16_pcs_ras_fields[] = {\n\t{\"XGMI3X16 PCS DataLossErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, DataLossErr)},\n\t{\"XGMI3X16 PCS TrainingErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, TrainingErr)},\n\t{\"XGMI3X16 PCS FlowCtrlAckErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, FlowCtrlAckErr)},\n\t{\"XGMI3X16 PCS RxFifoUnderflowErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RxFifoUnderflowErr)},\n\t{\"XGMI3X16 PCS RxFifoOverflowErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RxFifoOverflowErr)},\n\t{\"XGMI3X16 PCS CRCErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, CRCErr)},\n\t{\"XGMI3X16 PCS BERExceededErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, BERExceededErr)},\n\t{\"XGMI3X16 PCS TxVcidDataErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, TxVcidDataErr)},\n\t{\"XGMI3X16 PCS ReplayBufParityErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReplayBufParityErr)},\n\t{\"XGMI3X16 PCS DataParityErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, DataParityErr)},\n\t{\"XGMI3X16 PCS ReplayFifoOverflowErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReplayFifoOverflowErr)},\n\t{\"XGMI3X16 PCS ReplayFifoUnderflowErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReplayFifoUnderflowErr)},\n\t{\"XGMI3X16 PCS ElasticFifoOverflowErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ElasticFifoOverflowErr)},\n\t{\"XGMI3X16 PCS DeskewErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, DeskewErr)},\n\t{\"XGMI3X16 PCS FlowCtrlCRCErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, FlowCtrlCRCErr)},\n\t{\"XGMI3X16 PCS DataStartupLimitErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, DataStartupLimitErr)},\n\t{\"XGMI3X16 PCS FCInitTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, FCInitTimeoutErr)},\n\t{\"XGMI3X16 PCS RecoveryTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RecoveryTimeoutErr)},\n\t{\"XGMI3X16 PCS ReadySerialTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReadySerialTimeoutErr)},\n\t{\"XGMI3X16 PCS ReadySerialAttemptErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReadySerialAttemptErr)},\n\t{\"XGMI3X16 PCS RecoveryAttemptErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RecoveryAttemptErr)},\n\t{\"XGMI3X16 PCS RecoveryRelockAttemptErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RecoveryRelockAttemptErr)},\n\t{\"XGMI3X16 PCS ReplayAttemptErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, ReplayAttemptErr)},\n\t{\"XGMI3X16 PCS SyncHdrErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, SyncHdrErr)},\n\t{\"XGMI3X16 PCS TxReplayTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, TxReplayTimeoutErr)},\n\t{\"XGMI3X16 PCS RxReplayTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RxReplayTimeoutErr)},\n\t{\"XGMI3X16 PCS LinkSubTxTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, LinkSubTxTimeoutErr)},\n\t{\"XGMI3X16 PCS LinkSubRxTimeoutErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, LinkSubRxTimeoutErr)},\n\t{\"XGMI3X16 PCS RxCMDPktErr\",\n\t SOC15_REG_FIELD(PCS_XGMI3X16_PCS_ERROR_STATUS, RxCMDPktErr)},\n};\n\n \n\nstatic struct attribute amdgpu_xgmi_hive_id = {\n\t.name = \"xgmi_hive_id\",\n\t.mode = S_IRUGO\n};\n\nstatic struct attribute *amdgpu_xgmi_hive_attrs[] = {\n\t&amdgpu_xgmi_hive_id,\n\tNULL\n};\nATTRIBUTE_GROUPS(amdgpu_xgmi_hive);\n\nstatic ssize_t amdgpu_xgmi_show_attrs(struct kobject *kobj,\n\tstruct attribute *attr, char *buf)\n{\n\tstruct amdgpu_hive_info *hive = container_of(\n\t\tkobj, struct amdgpu_hive_info, kobj);\n\n\tif (attr == &amdgpu_xgmi_hive_id)\n\t\treturn snprintf(buf, PAGE_SIZE, \"%llu\\n\", hive->hive_id);\n\n\treturn 0;\n}\n\nstatic void amdgpu_xgmi_hive_release(struct kobject *kobj)\n{\n\tstruct amdgpu_hive_info *hive = container_of(\n\t\tkobj, struct amdgpu_hive_info, kobj);\n\n\tamdgpu_reset_put_reset_domain(hive->reset_domain);\n\thive->reset_domain = NULL;\n\n\tmutex_destroy(&hive->hive_lock);\n\tkfree(hive);\n}\n\nstatic const struct sysfs_ops amdgpu_xgmi_hive_ops = {\n\t.show = amdgpu_xgmi_show_attrs,\n};\n\nstatic const struct kobj_type amdgpu_xgmi_hive_type = {\n\t.release = amdgpu_xgmi_hive_release,\n\t.sysfs_ops = &amdgpu_xgmi_hive_ops,\n\t.default_groups = amdgpu_xgmi_hive_groups,\n};\n\nstatic ssize_t amdgpu_xgmi_show_device_id(struct device *dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\n\treturn sysfs_emit(buf, \"%llu\\n\", adev->gmc.xgmi.node_id);\n\n}\n\nstatic ssize_t amdgpu_xgmi_show_num_hops(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct psp_xgmi_topology_info *top = &adev->psp.xgmi_context.top_info;\n\tint i;\n\n\tfor (i = 0; i < top->num_nodes; i++)\n\t\tsprintf(buf + 3 * i, \"%02x \", top->nodes[i].num_hops);\n\n\treturn sysfs_emit(buf, \"%s\\n\", buf);\n}\n\nstatic ssize_t amdgpu_xgmi_show_num_links(struct device *dev,\n\t\t\t\t\tstruct device_attribute *attr,\n\t\t\t\t\tchar *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tstruct psp_xgmi_topology_info *top = &adev->psp.xgmi_context.top_info;\n\tint i;\n\n\tfor (i = 0; i < top->num_nodes; i++)\n\t\tsprintf(buf + 3 * i, \"%02x \", top->nodes[i].num_links);\n\n\treturn sysfs_emit(buf, \"%s\\n\", buf);\n}\n\n#define AMDGPU_XGMI_SET_FICAA(o)\t((o) | 0x456801)\nstatic ssize_t amdgpu_xgmi_show_error(struct device *dev,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      char *buf)\n{\n\tstruct drm_device *ddev = dev_get_drvdata(dev);\n\tstruct amdgpu_device *adev = drm_to_adev(ddev);\n\tuint32_t ficaa_pie_ctl_in, ficaa_pie_status_in;\n\tuint64_t fica_out;\n\tunsigned int error_count = 0;\n\n\tficaa_pie_ctl_in = AMDGPU_XGMI_SET_FICAA(0x200);\n\tficaa_pie_status_in = AMDGPU_XGMI_SET_FICAA(0x208);\n\n\tif ((!adev->df.funcs) ||\n\t    (!adev->df.funcs->get_fica) ||\n\t    (!adev->df.funcs->set_fica))\n\t\treturn -EINVAL;\n\n\tfica_out = adev->df.funcs->get_fica(adev, ficaa_pie_ctl_in);\n\tif (fica_out != 0x1f)\n\t\tpr_err(\"xGMI error counters not enabled!\\n\");\n\n\tfica_out = adev->df.funcs->get_fica(adev, ficaa_pie_status_in);\n\n\tif ((fica_out & 0xffff) == 2)\n\t\terror_count = ((fica_out >> 62) & 0x1) + (fica_out >> 63);\n\n\tadev->df.funcs->set_fica(adev, ficaa_pie_status_in, 0, 0);\n\n\treturn sysfs_emit(buf, \"%u\\n\", error_count);\n}\n\n\nstatic DEVICE_ATTR(xgmi_device_id, S_IRUGO, amdgpu_xgmi_show_device_id, NULL);\nstatic DEVICE_ATTR(xgmi_error, S_IRUGO, amdgpu_xgmi_show_error, NULL);\nstatic DEVICE_ATTR(xgmi_num_hops, S_IRUGO, amdgpu_xgmi_show_num_hops, NULL);\nstatic DEVICE_ATTR(xgmi_num_links, S_IRUGO, amdgpu_xgmi_show_num_links, NULL);\n\nstatic int amdgpu_xgmi_sysfs_add_dev_info(struct amdgpu_device *adev,\n\t\t\t\t\t struct amdgpu_hive_info *hive)\n{\n\tint ret = 0;\n\tchar node[10] = { 0 };\n\n\t \n\tret = device_create_file(adev->dev, &dev_attr_xgmi_device_id);\n\tif (ret) {\n\t\tdev_err(adev->dev, \"XGMI: Failed to create device file xgmi_device_id\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = device_create_file(adev->dev, &dev_attr_xgmi_error);\n\tif (ret)\n\t\tpr_err(\"failed to create xgmi_error\\n\");\n\n\t \n\tret = device_create_file(adev->dev, &dev_attr_xgmi_num_hops);\n\tif (ret)\n\t\tpr_err(\"failed to create xgmi_num_hops\\n\");\n\n\t \n\tret = device_create_file(adev->dev, &dev_attr_xgmi_num_links);\n\tif (ret)\n\t\tpr_err(\"failed to create xgmi_num_links\\n\");\n\n\t \n\tif (hive->kobj.parent != (&adev->dev->kobj)) {\n\t\tret = sysfs_create_link(&adev->dev->kobj, &hive->kobj,\n\t\t\t\t\t\"xgmi_hive_info\");\n\t\tif (ret) {\n\t\t\tdev_err(adev->dev, \"XGMI: Failed to create link to hive info\");\n\t\t\tgoto remove_file;\n\t\t}\n\t}\n\n\tsprintf(node, \"node%d\", atomic_read(&hive->number_devices));\n\t \n\tret = sysfs_create_link(&hive->kobj, &adev->dev->kobj, node);\n\tif (ret) {\n\t\tdev_err(adev->dev, \"XGMI: Failed to create link from hive info\");\n\t\tgoto remove_link;\n\t}\n\n\tgoto success;\n\n\nremove_link:\n\tsysfs_remove_link(&adev->dev->kobj, adev_to_drm(adev)->unique);\n\nremove_file:\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_device_id);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_error);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_num_hops);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_num_links);\n\nsuccess:\n\treturn ret;\n}\n\nstatic void amdgpu_xgmi_sysfs_rem_dev_info(struct amdgpu_device *adev,\n\t\t\t\t\t  struct amdgpu_hive_info *hive)\n{\n\tchar node[10];\n\tmemset(node, 0, sizeof(node));\n\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_device_id);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_error);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_num_hops);\n\tdevice_remove_file(adev->dev, &dev_attr_xgmi_num_links);\n\n\tif (hive->kobj.parent != (&adev->dev->kobj))\n\t\tsysfs_remove_link(&adev->dev->kobj,\"xgmi_hive_info\");\n\n\tsprintf(node, \"node%d\", atomic_read(&hive->number_devices));\n\tsysfs_remove_link(&hive->kobj, node);\n\n}\n\n\n\nstruct amdgpu_hive_info *amdgpu_get_xgmi_hive(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_hive_info *hive = NULL;\n\tint ret;\n\n\tif (!adev->gmc.xgmi.hive_id)\n\t\treturn NULL;\n\n\tif (adev->hive) {\n\t\tkobject_get(&adev->hive->kobj);\n\t\treturn adev->hive;\n\t}\n\n\tmutex_lock(&xgmi_mutex);\n\n\tlist_for_each_entry(hive, &xgmi_hive_list, node)  {\n\t\tif (hive->hive_id == adev->gmc.xgmi.hive_id)\n\t\t\tgoto pro_end;\n\t}\n\n\thive = kzalloc(sizeof(*hive), GFP_KERNEL);\n\tif (!hive) {\n\t\tdev_err(adev->dev, \"XGMI: allocation failed\\n\");\n\t\tret = -ENOMEM;\n\t\thive = NULL;\n\t\tgoto pro_end;\n\t}\n\n\t \n\tret = kobject_init_and_add(&hive->kobj,\n\t\t\t&amdgpu_xgmi_hive_type,\n\t\t\t&adev->dev->kobj,\n\t\t\t\"%s\", \"xgmi_hive_info\");\n\tif (ret) {\n\t\tdev_err(adev->dev, \"XGMI: failed initializing kobject for xgmi hive\\n\");\n\t\tkobject_put(&hive->kobj);\n\t\thive = NULL;\n\t\tgoto pro_end;\n\t}\n\n\t \n\tif (!amdgpu_sriov_vf(adev)) {\n\t \n\t\tif (adev->reset_domain->type != XGMI_HIVE) {\n\t\t\thive->reset_domain =\n\t\t\t\tamdgpu_reset_create_reset_domain(XGMI_HIVE, \"amdgpu-reset-hive\");\n\t\t\tif (!hive->reset_domain) {\n\t\t\t\tdev_err(adev->dev, \"XGMI: failed initializing reset domain for xgmi hive\\n\");\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tkobject_put(&hive->kobj);\n\t\t\t\thive = NULL;\n\t\t\t\tgoto pro_end;\n\t\t\t}\n\t\t} else {\n\t\t\tamdgpu_reset_get_reset_domain(adev->reset_domain);\n\t\t\thive->reset_domain = adev->reset_domain;\n\t\t}\n\t}\n\n\thive->hive_id = adev->gmc.xgmi.hive_id;\n\tINIT_LIST_HEAD(&hive->device_list);\n\tINIT_LIST_HEAD(&hive->node);\n\tmutex_init(&hive->hive_lock);\n\tatomic_set(&hive->number_devices, 0);\n\ttask_barrier_init(&hive->tb);\n\thive->pstate = AMDGPU_XGMI_PSTATE_UNKNOWN;\n\thive->hi_req_gpu = NULL;\n\n\t \n\thive->hi_req_count = AMDGPU_MAX_XGMI_DEVICE_PER_HIVE;\n\tlist_add_tail(&hive->node, &xgmi_hive_list);\n\npro_end:\n\tif (hive)\n\t\tkobject_get(&hive->kobj);\n\tmutex_unlock(&xgmi_mutex);\n\treturn hive;\n}\n\nvoid amdgpu_put_xgmi_hive(struct amdgpu_hive_info *hive)\n{\n\tif (hive)\n\t\tkobject_put(&hive->kobj);\n}\n\nint amdgpu_xgmi_set_pstate(struct amdgpu_device *adev, int pstate)\n{\n\tint ret = 0;\n\tstruct amdgpu_hive_info *hive;\n\tstruct amdgpu_device *request_adev;\n\tbool is_hi_req = pstate == AMDGPU_XGMI_PSTATE_MAX_VEGA20;\n\tbool init_low;\n\n\thive = amdgpu_get_xgmi_hive(adev);\n\tif (!hive)\n\t\treturn 0;\n\n\trequest_adev = hive->hi_req_gpu ? hive->hi_req_gpu : adev;\n\tinit_low = hive->pstate == AMDGPU_XGMI_PSTATE_UNKNOWN;\n\tamdgpu_put_xgmi_hive(hive);\n\t \n\treturn 0;\n\n\tif (!hive || adev->asic_type != CHIP_VEGA20)\n\t\treturn 0;\n\n\tmutex_lock(&hive->hive_lock);\n\n\tif (is_hi_req)\n\t\thive->hi_req_count++;\n\telse\n\t\thive->hi_req_count--;\n\n\t \n\tif (hive->pstate == pstate ||\n\t\t\t(!is_hi_req && hive->hi_req_count && !init_low))\n\t\tgoto out;\n\n\tdev_dbg(request_adev->dev, \"Set xgmi pstate %d.\\n\", pstate);\n\n\tret = amdgpu_dpm_set_xgmi_pstate(request_adev, pstate);\n\tif (ret) {\n\t\tdev_err(request_adev->dev,\n\t\t\t\"XGMI: Set pstate failure on device %llx, hive %llx, ret %d\",\n\t\t\trequest_adev->gmc.xgmi.node_id,\n\t\t\trequest_adev->gmc.xgmi.hive_id, ret);\n\t\tgoto out;\n\t}\n\n\tif (init_low)\n\t\thive->pstate = hive->hi_req_count ?\n\t\t\t\t\thive->pstate : AMDGPU_XGMI_PSTATE_MIN;\n\telse {\n\t\thive->pstate = pstate;\n\t\thive->hi_req_gpu = pstate != AMDGPU_XGMI_PSTATE_MIN ?\n\t\t\t\t\t\t\tadev : NULL;\n\t}\nout:\n\tmutex_unlock(&hive->hive_lock);\n\treturn ret;\n}\n\nint amdgpu_xgmi_update_topology(struct amdgpu_hive_info *hive, struct amdgpu_device *adev)\n{\n\tint ret;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\t \n\tret = psp_xgmi_set_topology_info(&adev->psp,\n\t\t\t\t\t atomic_read(&hive->number_devices),\n\t\t\t\t\t &adev->psp.xgmi_context.top_info);\n\tif (ret)\n\t\tdev_err(adev->dev,\n\t\t\t\"XGMI: Set topology failure on device %llx, hive %llx, ret %d\",\n\t\t\tadev->gmc.xgmi.node_id,\n\t\t\tadev->gmc.xgmi.hive_id, ret);\n\n\treturn ret;\n}\n\n\n \nint amdgpu_xgmi_get_hops_count(struct amdgpu_device *adev,\n\t\tstruct amdgpu_device *peer_adev)\n{\n\tstruct psp_xgmi_topology_info *top = &adev->psp.xgmi_context.top_info;\n\tuint8_t num_hops_mask = 0x7;\n\tint i;\n\n\tfor (i = 0 ; i < top->num_nodes; ++i)\n\t\tif (top->nodes[i].node_id == peer_adev->gmc.xgmi.node_id)\n\t\t\treturn top->nodes[i].num_hops & num_hops_mask;\n\treturn\t-EINVAL;\n}\n\nint amdgpu_xgmi_get_num_links(struct amdgpu_device *adev,\n\t\tstruct amdgpu_device *peer_adev)\n{\n\tstruct psp_xgmi_topology_info *top = &adev->psp.xgmi_context.top_info;\n\tint i;\n\n\tfor (i = 0 ; i < top->num_nodes; ++i)\n\t\tif (top->nodes[i].node_id == peer_adev->gmc.xgmi.node_id)\n\t\t\treturn top->nodes[i].num_links;\n\treturn\t-EINVAL;\n}\n\n \nstatic int amdgpu_xgmi_initialize_hive_get_data_partition(struct amdgpu_hive_info *hive,\n\t\t\t\t\t\t\tbool set_extended_data)\n{\n\tstruct amdgpu_device *tmp_adev;\n\tint ret;\n\n\tlist_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) {\n\t\tret = psp_xgmi_initialize(&tmp_adev->psp, set_extended_data, false);\n\t\tif (ret) {\n\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\"XGMI: Failed to initialize xgmi session for data partition %i\\n\",\n\t\t\t\tset_extended_data);\n\t\t\treturn ret;\n\t\t}\n\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_xgmi_add_device(struct amdgpu_device *adev)\n{\n\tstruct psp_xgmi_topology_info *top_info;\n\tstruct amdgpu_hive_info *hive;\n\tstruct amdgpu_xgmi\t*entry;\n\tstruct amdgpu_device *tmp_adev = NULL;\n\n\tint count = 0, ret = 0;\n\n\tif (!adev->gmc.xgmi.supported)\n\t\treturn 0;\n\n\tif (!adev->gmc.xgmi.pending_reset &&\n\t    amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_PSP)) {\n\t\tret = psp_xgmi_initialize(&adev->psp, false, true);\n\t\tif (ret) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"XGMI: Failed to initialize xgmi session\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = psp_xgmi_get_hive_id(&adev->psp, &adev->gmc.xgmi.hive_id);\n\t\tif (ret) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"XGMI: Failed to get hive id\\n\");\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = psp_xgmi_get_node_id(&adev->psp, &adev->gmc.xgmi.node_id);\n\t\tif (ret) {\n\t\t\tdev_err(adev->dev,\n\t\t\t\t\"XGMI: Failed to get node id\\n\");\n\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\tadev->gmc.xgmi.hive_id = 16;\n\t\tadev->gmc.xgmi.node_id = adev->gmc.xgmi.physical_node_id + 16;\n\t}\n\n\thive = amdgpu_get_xgmi_hive(adev);\n\tif (!hive) {\n\t\tret = -EINVAL;\n\t\tdev_err(adev->dev,\n\t\t\t\"XGMI: node 0x%llx, can not match hive 0x%llx in the hive list.\\n\",\n\t\t\tadev->gmc.xgmi.node_id, adev->gmc.xgmi.hive_id);\n\t\tgoto exit;\n\t}\n\tmutex_lock(&hive->hive_lock);\n\n\ttop_info = &adev->psp.xgmi_context.top_info;\n\n\tlist_add_tail(&adev->gmc.xgmi.head, &hive->device_list);\n\tlist_for_each_entry(entry, &hive->device_list, head)\n\t\ttop_info->nodes[count++].node_id = entry->node_id;\n\ttop_info->num_nodes = count;\n\tatomic_set(&hive->number_devices, count);\n\n\ttask_barrier_add_task(&hive->tb);\n\n\tif (!adev->gmc.xgmi.pending_reset &&\n\t    amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_PSP)) {\n\t\tlist_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) {\n\t\t\t \n\t\t\tif (tmp_adev != adev) {\n\t\t\t\ttop_info = &tmp_adev->psp.xgmi_context.top_info;\n\t\t\t\ttop_info->nodes[count - 1].node_id =\n\t\t\t\t\tadev->gmc.xgmi.node_id;\n\t\t\t\ttop_info->num_nodes = count;\n\t\t\t}\n\t\t\tret = amdgpu_xgmi_update_topology(hive, tmp_adev);\n\t\t\tif (ret)\n\t\t\t\tgoto exit_unlock;\n\t\t}\n\n\t\t \n\t\tlist_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) {\n\t\t\tret = psp_xgmi_get_topology_info(&tmp_adev->psp, count,\n\t\t\t\t\t&tmp_adev->psp.xgmi_context.top_info, false);\n\t\t\tif (ret) {\n\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\"XGMI: Get topology failure on device %llx, hive %llx, ret %d\",\n\t\t\t\t\ttmp_adev->gmc.xgmi.node_id,\n\t\t\t\t\ttmp_adev->gmc.xgmi.hive_id, ret);\n\t\t\t\t \n\t\t\t\tgoto exit_unlock;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (adev->psp.xgmi_context.supports_extended_data) {\n\n\t\t\t \n\t\t\tret = amdgpu_xgmi_initialize_hive_get_data_partition(hive, true);\n\t\t\tif (ret)\n\t\t\t\tgoto exit_unlock;\n\n\t\t\t \n\t\t\tlist_for_each_entry(tmp_adev, &hive->device_list, gmc.xgmi.head) {\n\t\t\t\tret = psp_xgmi_get_topology_info(&tmp_adev->psp, count,\n\t\t\t\t\t\t&tmp_adev->psp.xgmi_context.top_info, true);\n\t\t\t\tif (ret) {\n\t\t\t\t\tdev_err(tmp_adev->dev,\n\t\t\t\t\t\t\"XGMI: Get topology for extended data failure on device %llx, hive %llx, ret %d\",\n\t\t\t\t\t\ttmp_adev->gmc.xgmi.node_id,\n\t\t\t\t\t\ttmp_adev->gmc.xgmi.hive_id, ret);\n\t\t\t\t\tgoto exit_unlock;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tret = amdgpu_xgmi_initialize_hive_get_data_partition(hive, false);\n\t\t\tif (ret)\n\t\t\t\tgoto exit_unlock;\n\n\t\t}\n\t}\n\n\tif (!ret && !adev->gmc.xgmi.pending_reset)\n\t\tret = amdgpu_xgmi_sysfs_add_dev_info(adev, hive);\n\nexit_unlock:\n\tmutex_unlock(&hive->hive_lock);\nexit:\n\tif (!ret) {\n\t\tadev->hive = hive;\n\t\tdev_info(adev->dev, \"XGMI: Add node %d, hive 0x%llx.\\n\",\n\t\t\t adev->gmc.xgmi.physical_node_id, adev->gmc.xgmi.hive_id);\n\t} else {\n\t\tamdgpu_put_xgmi_hive(hive);\n\t\tdev_err(adev->dev, \"XGMI: Failed to add node %d, hive 0x%llx ret: %d\\n\",\n\t\t\tadev->gmc.xgmi.physical_node_id, adev->gmc.xgmi.hive_id,\n\t\t\tret);\n\t}\n\n\treturn ret;\n}\n\nint amdgpu_xgmi_remove_device(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_hive_info *hive = adev->hive;\n\n\tif (!adev->gmc.xgmi.supported)\n\t\treturn -EINVAL;\n\n\tif (!hive)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&hive->hive_lock);\n\ttask_barrier_rem_task(&hive->tb);\n\tamdgpu_xgmi_sysfs_rem_dev_info(adev, hive);\n\tif (hive->hi_req_gpu == adev)\n\t\thive->hi_req_gpu = NULL;\n\tlist_del(&adev->gmc.xgmi.head);\n\tmutex_unlock(&hive->hive_lock);\n\n\tamdgpu_put_xgmi_hive(hive);\n\tadev->hive = NULL;\n\n\tif (atomic_dec_return(&hive->number_devices) == 0) {\n\t\t \n\t\tmutex_lock(&xgmi_mutex);\n\t\tlist_del(&hive->node);\n\t\tmutex_unlock(&xgmi_mutex);\n\n\t\tamdgpu_put_xgmi_hive(hive);\n\t}\n\n\treturn 0;\n}\n\nstatic int amdgpu_xgmi_ras_late_init(struct amdgpu_device *adev, struct ras_common_if *ras_block)\n{\n\tif (!adev->gmc.xgmi.supported ||\n\t    adev->gmc.xgmi.num_physical_nodes == 0)\n\t\treturn 0;\n\n\tadev->gmc.xgmi.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\n\treturn amdgpu_ras_block_late_init(adev, ras_block);\n}\n\nuint64_t amdgpu_xgmi_get_relative_phy_addr(struct amdgpu_device *adev,\n\t\t\t\t\t   uint64_t addr)\n{\n\tstruct amdgpu_xgmi *xgmi = &adev->gmc.xgmi;\n\treturn (addr + xgmi->physical_node_id * xgmi->node_segment_size);\n}\n\nstatic void pcs_clear_status(struct amdgpu_device *adev, uint32_t pcs_status_reg)\n{\n\tWREG32_PCIE(pcs_status_reg, 0xFFFFFFFF);\n\tWREG32_PCIE(pcs_status_reg, 0);\n}\n\nstatic void amdgpu_xgmi_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_ARCTURUS:\n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi_pcs_err_status_reg_arct); i++)\n\t\t\tpcs_clear_status(adev,\n\t\t\t\t\t xgmi_pcs_err_status_reg_arct[i]);\n\t\tbreak;\n\tcase CHIP_VEGA20:\n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi_pcs_err_status_reg_vg20); i++)\n\t\t\tpcs_clear_status(adev,\n\t\t\t\t\t xgmi_pcs_err_status_reg_vg20[i]);\n\t\tbreak;\n\tcase CHIP_ALDEBARAN:\n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi3x16_pcs_err_status_reg_aldebaran); i++)\n\t\t\tpcs_clear_status(adev,\n\t\t\t\t\t xgmi3x16_pcs_err_status_reg_aldebaran[i]);\n\t\tfor (i = 0; i < ARRAY_SIZE(walf_pcs_err_status_reg_aldebaran); i++)\n\t\t\tpcs_clear_status(adev,\n\t\t\t\t\t walf_pcs_err_status_reg_aldebaran[i]);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int amdgpu_xgmi_query_pcs_error_status(struct amdgpu_device *adev,\n\t\t\t\t\t      uint32_t value,\n\t\t\t\t\t\t  uint32_t mask_value,\n\t\t\t\t\t      uint32_t *ue_count,\n\t\t\t\t\t      uint32_t *ce_count,\n\t\t\t\t\t      bool is_xgmi_pcs,\n\t\t\t\t\t\t  bool check_mask)\n{\n\tint i;\n\tint ue_cnt = 0;\n\tconst struct amdgpu_pcs_ras_field *pcs_ras_fields = NULL;\n\tuint32_t field_array_size = 0;\n\n\tif (is_xgmi_pcs) {\n\t\tif (adev->ip_versions[XGMI_HWIP][0] == IP_VERSION(6, 1, 0)) {\n\t\t\tpcs_ras_fields = &xgmi3x16_pcs_ras_fields[0];\n\t\t\tfield_array_size = ARRAY_SIZE(xgmi3x16_pcs_ras_fields);\n\t\t} else {\n\t\t\tpcs_ras_fields = &xgmi_pcs_ras_fields[0];\n\t\t\tfield_array_size = ARRAY_SIZE(xgmi_pcs_ras_fields);\n\t\t}\n\t} else {\n\t\tpcs_ras_fields = &wafl_pcs_ras_fields[0];\n\t\tfield_array_size = ARRAY_SIZE(wafl_pcs_ras_fields);\n\t}\n\n\tif (check_mask)\n\t\tvalue = value & ~mask_value;\n\n\t \n\tfor (i = 0; value && i < field_array_size; i++) {\n\t\tue_cnt = (value &\n\t\t\t\tpcs_ras_fields[i].pcs_err_mask) >>\n\t\t\t\tpcs_ras_fields[i].pcs_err_shift;\n\t\tif (ue_cnt) {\n\t\t\tdev_info(adev->dev, \"%s detected\\n\",\n\t\t\t\t pcs_ras_fields[i].err_name);\n\t\t\t*ue_count += ue_cnt;\n\t\t}\n\n\t\t \n\t\tvalue &= ~(pcs_ras_fields[i].pcs_err_mask);\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_xgmi_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t     void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tint i;\n\tuint32_t data, mask_data = 0;\n\tuint32_t ue_cnt = 0, ce_cnt = 0;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__XGMI_WAFL))\n\t\treturn ;\n\n\terr_data->ue_count = 0;\n\terr_data->ce_count = 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_ARCTURUS:\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi_pcs_err_status_reg_arct); i++) {\n\t\t\tdata = RREG32_PCIE(xgmi_pcs_err_status_reg_arct[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, true, false);\n\t\t}\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(wafl_pcs_err_status_reg_arct); i++) {\n\t\t\tdata = RREG32_PCIE(wafl_pcs_err_status_reg_arct[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, false, false);\n\t\t}\n\t\tbreak;\n\tcase CHIP_VEGA20:\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi_pcs_err_status_reg_vg20); i++) {\n\t\t\tdata = RREG32_PCIE(xgmi_pcs_err_status_reg_vg20[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, true, false);\n\t\t}\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(wafl_pcs_err_status_reg_vg20); i++) {\n\t\t\tdata = RREG32_PCIE(wafl_pcs_err_status_reg_vg20[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, false, false);\n\t\t}\n\t\tbreak;\n\tcase CHIP_ALDEBARAN:\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(xgmi3x16_pcs_err_status_reg_aldebaran); i++) {\n\t\t\tdata = RREG32_PCIE(xgmi3x16_pcs_err_status_reg_aldebaran[i]);\n\t\t\tmask_data =\n\t\t\t\tRREG32_PCIE(xgmi3x16_pcs_err_noncorrectable_mask_reg_aldebaran[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, true, true);\n\t\t}\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(walf_pcs_err_status_reg_aldebaran); i++) {\n\t\t\tdata = RREG32_PCIE(walf_pcs_err_status_reg_aldebaran[i]);\n\t\t\tmask_data =\n\t\t\t\tRREG32_PCIE(walf_pcs_err_noncorrectable_mask_reg_aldebaran[i]);\n\t\t\tif (data)\n\t\t\t\tamdgpu_xgmi_query_pcs_error_status(adev, data,\n\t\t\t\t\t\tmask_data, &ue_cnt, &ce_cnt, false, true);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_warn(adev->dev, \"XGMI RAS error query not supported\");\n\t\tbreak;\n\t}\n\n\tadev->gmc.xgmi.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\n\terr_data->ue_count += ue_cnt;\n\terr_data->ce_count += ce_cnt;\n}\n\n \nstatic int amdgpu_ras_error_inject_xgmi(struct amdgpu_device *adev,\n\t\t\tvoid *inject_if, uint32_t instance_mask)\n{\n\tint ret = 0;\n\tstruct ta_ras_trigger_error_input *block_info =\n\t\t\t\t(struct ta_ras_trigger_error_input *)inject_if;\n\n\tif (amdgpu_dpm_set_df_cstate(adev, DF_CSTATE_DISALLOW))\n\t\tdev_warn(adev->dev, \"Failed to disallow df cstate\");\n\n\tif (amdgpu_dpm_allow_xgmi_power_down(adev, false))\n\t\tdev_warn(adev->dev, \"Failed to disallow XGMI power down\");\n\n\tret = psp_ras_trigger_error(&adev->psp, block_info, instance_mask);\n\n\tif (amdgpu_ras_intr_triggered())\n\t\treturn ret;\n\n\tif (amdgpu_dpm_allow_xgmi_power_down(adev, true))\n\t\tdev_warn(adev->dev, \"Failed to allow XGMI power down\");\n\n\tif (amdgpu_dpm_set_df_cstate(adev, DF_CSTATE_ALLOW))\n\t\tdev_warn(adev->dev, \"Failed to allow df cstate\");\n\n\treturn ret;\n}\n\nstruct amdgpu_ras_block_hw_ops  xgmi_ras_hw_ops = {\n\t.query_ras_error_count = amdgpu_xgmi_query_ras_error_count,\n\t.reset_ras_error_count = amdgpu_xgmi_reset_ras_error_count,\n\t.ras_error_inject = amdgpu_ras_error_inject_xgmi,\n};\n\nstruct amdgpu_xgmi_ras xgmi_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &xgmi_ras_hw_ops,\n\t\t.ras_late_init = amdgpu_xgmi_ras_late_init,\n\t},\n};\n\nint amdgpu_xgmi_ras_sw_init(struct amdgpu_device *adev)\n{\n\tint err;\n\tstruct amdgpu_xgmi_ras *ras;\n\n\tif (!adev->gmc.xgmi.ras)\n\t\treturn 0;\n\n\tras = adev->gmc.xgmi.ras;\n\terr = amdgpu_ras_register_ras_block(adev, &ras->ras_block);\n\tif (err) {\n\t\tdev_err(adev->dev, \"Failed to register xgmi_wafl_pcs ras block!\\n\");\n\t\treturn err;\n\t}\n\n\tstrcpy(ras->ras_block.ras_comm.name, \"xgmi_wafl\");\n\tras->ras_block.ras_comm.block = AMDGPU_RAS_BLOCK__XGMI_WAFL;\n\tras->ras_block.ras_comm.type = AMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE;\n\tadev->gmc.xgmi.ras_if = &ras->ras_block.ras_comm;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}