{
  "module_name": "sdma_v4_4_2.c",
  "hash_id": "44044acabc874e6dd22001a272562164fb03f71c294784b6a821ce63b0be36e0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/sdma_v4_4_2.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_xcp.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_trace.h\"\n\n#include \"sdma/sdma_4_4_2_offset.h\"\n#include \"sdma/sdma_4_4_2_sh_mask.h\"\n\n#include \"soc15_common.h\"\n#include \"soc15.h\"\n#include \"vega10_sdma_pkt_open.h\"\n\n#include \"ivsrcid/sdma0/irqsrcs_sdma0_4_0.h\"\n#include \"ivsrcid/sdma1/irqsrcs_sdma1_4_0.h\"\n\n#include \"amdgpu_ras.h\"\n\nMODULE_FIRMWARE(\"amdgpu/sdma_4_4_2.bin\");\n\n#define WREG32_SDMA(instance, offset, value) \\\n\tWREG32(sdma_v4_4_2_get_reg_offset(adev, (instance), (offset)), value)\n#define RREG32_SDMA(instance, offset) \\\n\tRREG32(sdma_v4_4_2_get_reg_offset(adev, (instance), (offset)))\n\nstatic void sdma_v4_4_2_set_ring_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_4_2_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_4_2_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_4_2_set_irq_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_4_2_set_ras_funcs(struct amdgpu_device *adev);\n\nstatic u32 sdma_v4_4_2_get_reg_offset(struct amdgpu_device *adev,\n\t\tu32 instance, u32 offset)\n{\n\tu32 dev_inst = GET_INST(SDMA0, instance);\n\n\treturn (adev->reg_offset[SDMA0_HWIP][dev_inst][0] + offset);\n}\n\nstatic unsigned sdma_v4_4_2_seq_to_irq_id(int seq_num)\n{\n\tswitch (seq_num) {\n\tcase 0:\n\t\treturn SOC15_IH_CLIENTID_SDMA0;\n\tcase 1:\n\t\treturn SOC15_IH_CLIENTID_SDMA1;\n\tcase 2:\n\t\treturn SOC15_IH_CLIENTID_SDMA2;\n\tcase 3:\n\t\treturn SOC15_IH_CLIENTID_SDMA3;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int sdma_v4_4_2_irq_id_to_seq(unsigned client_id)\n{\n\tswitch (client_id) {\n\tcase SOC15_IH_CLIENTID_SDMA0:\n\t\treturn 0;\n\tcase SOC15_IH_CLIENTID_SDMA1:\n\t\treturn 1;\n\tcase SOC15_IH_CLIENTID_SDMA2:\n\t\treturn 2;\n\tcase SOC15_IH_CLIENTID_SDMA3:\n\t\treturn 3;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void sdma_v4_4_2_inst_init_golden_registers(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t inst_mask)\n{\n\tu32 val;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tval = RREG32_SDMA(i, regSDMA_GB_ADDR_CONFIG);\n\t\tval = REG_SET_FIELD(val, SDMA_GB_ADDR_CONFIG, NUM_BANKS, 4);\n\t\tval = REG_SET_FIELD(val, SDMA_GB_ADDR_CONFIG,\n\t\t\t\t    PIPE_INTERLEAVE_SIZE, 0);\n\t\tWREG32_SDMA(i, regSDMA_GB_ADDR_CONFIG, val);\n\n\t\tval = RREG32_SDMA(i, regSDMA_GB_ADDR_CONFIG_READ);\n\t\tval = REG_SET_FIELD(val, SDMA_GB_ADDR_CONFIG_READ, NUM_BANKS,\n\t\t\t\t    4);\n\t\tval = REG_SET_FIELD(val, SDMA_GB_ADDR_CONFIG_READ,\n\t\t\t\t    PIPE_INTERLEAVE_SIZE, 0);\n\t\tWREG32_SDMA(i, regSDMA_GB_ADDR_CONFIG_READ, val);\n\t}\n}\n\n \nstatic int sdma_v4_4_2_init_microcode(struct amdgpu_device *adev)\n{\n\tint ret, i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 4, 2)) {\n\t\t\tret = amdgpu_sdma_init_microcode(adev, 0, true);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tret = amdgpu_sdma_init_microcode(adev, i, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic uint64_t sdma_v4_4_2_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tu64 *rptr;\n\n\t \n\trptr = ((u64 *)&ring->adev->wb.wb[ring->rptr_offs]);\n\n\tDRM_DEBUG(\"rptr before shift == 0x%016llx\\n\", *rptr);\n\treturn ((*rptr) >> 2);\n}\n\n \nstatic uint64_t sdma_v4_4_2_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu64 wptr;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\twptr = READ_ONCE(*((u64 *)&adev->wb.wb[ring->wptr_offs]));\n\t\tDRM_DEBUG(\"wptr/doorbell before shift == 0x%016llx\\n\", wptr);\n\t} else {\n\t\twptr = RREG32_SDMA(ring->me, regSDMA_GFX_RB_WPTR_HI);\n\t\twptr = wptr << 32;\n\t\twptr |= RREG32_SDMA(ring->me, regSDMA_GFX_RB_WPTR);\n\t\tDRM_DEBUG(\"wptr before shift [%i] wptr == 0x%016llx\\n\",\n\t\t\t\tring->me, wptr);\n\t}\n\n\treturn wptr >> 2;\n}\n\n \nstatic void sdma_v4_4_2_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tDRM_DEBUG(\"Setting write pointer\\n\");\n\tif (ring->use_doorbell) {\n\t\tu64 *wb = (u64 *)&adev->wb.wb[ring->wptr_offs];\n\n\t\tDRM_DEBUG(\"Using doorbell -- \"\n\t\t\t\t\"wptr_offs == 0x%08x \"\n\t\t\t\t\"lower_32_bits(ring->wptr) << 2 == 0x%08x \"\n\t\t\t\t\"upper_32_bits(ring->wptr) << 2 == 0x%08x\\n\",\n\t\t\t\tring->wptr_offs,\n\t\t\t\tlower_32_bits(ring->wptr << 2),\n\t\t\t\tupper_32_bits(ring->wptr << 2));\n\t\t \n\t\tWRITE_ONCE(*wb, (ring->wptr << 2));\n\t\tDRM_DEBUG(\"calling WDOORBELL64(0x%08x, 0x%016llx)\\n\",\n\t\t\t\tring->doorbell_index, ring->wptr << 2);\n\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t} else {\n\t\tDRM_DEBUG(\"Not using doorbell -- \"\n\t\t\t\t\"regSDMA%i_GFX_RB_WPTR == 0x%08x \"\n\t\t\t\t\"regSDMA%i_GFX_RB_WPTR_HI == 0x%08x\\n\",\n\t\t\t\tring->me,\n\t\t\t\tlower_32_bits(ring->wptr << 2),\n\t\t\t\tring->me,\n\t\t\t\tupper_32_bits(ring->wptr << 2));\n\t\tWREG32_SDMA(ring->me, regSDMA_GFX_RB_WPTR,\n\t\t\t    lower_32_bits(ring->wptr << 2));\n\t\tWREG32_SDMA(ring->me, regSDMA_GFX_RB_WPTR_HI,\n\t\t\t    upper_32_bits(ring->wptr << 2));\n\t}\n}\n\n \nstatic uint64_t sdma_v4_4_2_page_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu64 wptr;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\twptr = READ_ONCE(*((u64 *)&adev->wb.wb[ring->wptr_offs]));\n\t} else {\n\t\twptr = RREG32_SDMA(ring->me, regSDMA_PAGE_RB_WPTR_HI);\n\t\twptr = wptr << 32;\n\t\twptr |= RREG32_SDMA(ring->me, regSDMA_PAGE_RB_WPTR);\n\t}\n\n\treturn wptr >> 2;\n}\n\n \nstatic void sdma_v4_4_2_page_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell) {\n\t\tu64 *wb = (u64 *)&adev->wb.wb[ring->wptr_offs];\n\n\t\t \n\t\tWRITE_ONCE(*wb, (ring->wptr << 2));\n\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t} else {\n\t\tuint64_t wptr = ring->wptr << 2;\n\n\t\tWREG32_SDMA(ring->me, regSDMA_PAGE_RB_WPTR,\n\t\t\t    lower_32_bits(wptr));\n\t\tWREG32_SDMA(ring->me, regSDMA_PAGE_RB_WPTR_HI,\n\t\t\t    upper_32_bits(wptr));\n\t}\n}\n\nstatic void sdma_v4_4_2_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tint i;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(count - 1));\n\t\telse\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop);\n}\n\n \nstatic void sdma_v4_4_2_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_job *job,\n\t\t\t\t   struct amdgpu_ib *ib,\n\t\t\t\t   uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\t \n\tsdma_v4_4_2_ring_insert_nop(ring, (2 - lower_32_bits(ring->wptr)) & 7);\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_INDIRECT) |\n\t\t\t  SDMA_PKT_INDIRECT_HEADER_VMID(vmid & 0xf));\n\t \n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr) & 0xffffffe0);\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, 0);\n\n}\n\nstatic void sdma_v4_4_2_wait_reg_mem(struct amdgpu_ring *ring,\n\t\t\t\t   int mem_space, int hdp,\n\t\t\t\t   uint32_t addr0, uint32_t addr1,\n\t\t\t\t   uint32_t ref, uint32_t mask,\n\t\t\t\t   uint32_t inv)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(hdp) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_MEM_POLL(mem_space) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));  \n\tif (mem_space) {\n\t\t \n\t\tamdgpu_ring_write(ring, addr0);\n\t\tamdgpu_ring_write(ring, addr1);\n\t} else {\n\t\t \n\t\tamdgpu_ring_write(ring, addr0 << 2);\n\t\tamdgpu_ring_write(ring, addr1 << 2);\n\t}\n\tamdgpu_ring_write(ring, ref);  \n\tamdgpu_ring_write(ring, mask);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(inv));  \n}\n\n \nstatic void sdma_v4_4_2_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 ref_and_mask = 0;\n\tconst struct nbio_hdp_flush_reg *nbio_hf_reg = adev->nbio.hdp_flush_reg;\n\n\tref_and_mask = nbio_hf_reg->ref_and_mask_sdma0 << ring->me;\n\n\tsdma_v4_4_2_wait_reg_mem(ring, 0, 1,\n\t\t\t       adev->nbio.funcs->get_hdp_flush_done_offset(adev),\n\t\t\t       adev->nbio.funcs->get_hdp_flush_req_offset(adev),\n\t\t\t       ref_and_mask, ref_and_mask, 10);\n}\n\n \nstatic void sdma_v4_4_2_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t      unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\t \n\tBUG_ON(addr & 0x3);\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\t\t \n\t\tBUG_ON(addr & 0x3);\n\t\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_TRAP));\n\tamdgpu_ring_write(ring, SDMA_PKT_TRAP_INT_CONTEXT_INT_CONTEXT(0));\n}\n\n\n \nstatic void sdma_v4_4_2_inst_gfx_stop(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t inst_mask)\n{\n\tstruct amdgpu_ring *sdma[AMDGPU_MAX_SDMA_INSTANCES];\n\tu32 rb_cntl, ib_cntl;\n\tint i, unset = 0;\n\n\tfor_each_inst(i, inst_mask) {\n\t\tsdma[i] = &adev->sdma.instance[i].ring;\n\n\t\tif ((adev->mman.buffer_funcs_ring == sdma[i]) && unset != 1) {\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, false);\n\t\t\tunset = 1;\n\t\t}\n\n\t\trb_cntl = RREG32_SDMA(i, regSDMA_GFX_RB_CNTL);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_GFX_RB_CNTL, RB_ENABLE, 0);\n\t\tWREG32_SDMA(i, regSDMA_GFX_RB_CNTL, rb_cntl);\n\t\tib_cntl = RREG32_SDMA(i, regSDMA_GFX_IB_CNTL);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_GFX_IB_CNTL, IB_ENABLE, 0);\n\t\tWREG32_SDMA(i, regSDMA_GFX_IB_CNTL, ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v4_4_2_inst_rlc_stop(struct amdgpu_device *adev,\n\t\t\t\t      uint32_t inst_mask)\n{\n\t \n}\n\n \nstatic void sdma_v4_4_2_inst_page_stop(struct amdgpu_device *adev,\n\t\t\t\t       uint32_t inst_mask)\n{\n\tstruct amdgpu_ring *sdma[AMDGPU_MAX_SDMA_INSTANCES];\n\tu32 rb_cntl, ib_cntl;\n\tint i;\n\tbool unset = false;\n\n\tfor_each_inst(i, inst_mask) {\n\t\tsdma[i] = &adev->sdma.instance[i].page;\n\n\t\tif ((adev->mman.buffer_funcs_ring == sdma[i]) &&\n\t\t\t(!unset)) {\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, false);\n\t\t\tunset = true;\n\t\t}\n\n\t\trb_cntl = RREG32_SDMA(i, regSDMA_PAGE_RB_CNTL);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_PAGE_RB_CNTL,\n\t\t\t\t\tRB_ENABLE, 0);\n\t\tWREG32_SDMA(i, regSDMA_PAGE_RB_CNTL, rb_cntl);\n\t\tib_cntl = RREG32_SDMA(i, regSDMA_PAGE_IB_CNTL);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_PAGE_IB_CNTL,\n\t\t\t\t\tIB_ENABLE, 0);\n\t\tWREG32_SDMA(i, regSDMA_PAGE_IB_CNTL, ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v4_4_2_inst_ctx_switch_enable(struct amdgpu_device *adev,\n\t\t\t\t\t       bool enable, uint32_t inst_mask)\n{\n\tu32 f32_cntl, phase_quantum = 0;\n\tint i;\n\n\tif (amdgpu_sdma_phase_quantum) {\n\t\tunsigned value = amdgpu_sdma_phase_quantum;\n\t\tunsigned unit = 0;\n\n\t\twhile (value > (SDMA_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\tSDMA_PHASE0_QUANTUM__VALUE__SHIFT)) {\n\t\t\tvalue = (value + 1) >> 1;\n\t\t\tunit++;\n\t\t}\n\t\tif (unit > (SDMA_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t    SDMA_PHASE0_QUANTUM__UNIT__SHIFT)) {\n\t\t\tvalue = (SDMA_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\t SDMA_PHASE0_QUANTUM__VALUE__SHIFT);\n\t\t\tunit = (SDMA_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t\tSDMA_PHASE0_QUANTUM__UNIT__SHIFT);\n\t\t\tWARN_ONCE(1,\n\t\t\t\"clamping sdma_phase_quantum to %uK clock cycles\\n\",\n\t\t\t\t  value << unit);\n\t\t}\n\t\tphase_quantum =\n\t\t\tvalue << SDMA_PHASE0_QUANTUM__VALUE__SHIFT |\n\t\t\tunit  << SDMA_PHASE0_QUANTUM__UNIT__SHIFT;\n\t}\n\n\tfor_each_inst(i, inst_mask) {\n\t\tf32_cntl = RREG32_SDMA(i, regSDMA_CNTL);\n\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA_CNTL,\n\t\t\t\tAUTO_CTXSW_ENABLE, enable ? 1 : 0);\n\t\tif (enable && amdgpu_sdma_phase_quantum) {\n\t\t\tWREG32_SDMA(i, regSDMA_PHASE0_QUANTUM, phase_quantum);\n\t\t\tWREG32_SDMA(i, regSDMA_PHASE1_QUANTUM, phase_quantum);\n\t\t\tWREG32_SDMA(i, regSDMA_PHASE2_QUANTUM, phase_quantum);\n\t\t}\n\t\tWREG32_SDMA(i, regSDMA_CNTL, f32_cntl);\n\n\t\t \n\t\tWREG32_SDMA(i, regSDMA_UTCL1_TIMEOUT, 0x00800080);\n\t}\n}\n\n \nstatic void sdma_v4_4_2_inst_enable(struct amdgpu_device *adev, bool enable,\n\t\t\t\t    uint32_t inst_mask)\n{\n\tu32 f32_cntl;\n\tint i;\n\n\tif (!enable) {\n\t\tsdma_v4_4_2_inst_gfx_stop(adev, inst_mask);\n\t\tsdma_v4_4_2_inst_rlc_stop(adev, inst_mask);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsdma_v4_4_2_inst_page_stop(adev, inst_mask);\n\n\t\t \n\t\tif (!amdgpu_sriov_vf(adev) && amdgpu_in_reset(adev))\n\t\t\treturn;\n\t}\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_PSP)\n\t\treturn;\n\n\tfor_each_inst(i, inst_mask) {\n\t\tf32_cntl = RREG32_SDMA(i, regSDMA_F32_CNTL);\n\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA_F32_CNTL, HALT, enable ? 0 : 1);\n\t\tWREG32_SDMA(i, regSDMA_F32_CNTL, f32_cntl);\n\t}\n}\n\n \nstatic uint32_t sdma_v4_4_2_rb_cntl(struct amdgpu_ring *ring, uint32_t rb_cntl)\n{\n\t \n\tuint32_t rb_bufsz = order_base_2(ring->ring_size / 4);\n\n\tbarrier();  \nstatic void sdma_v4_4_2_gfx_resume(struct amdgpu_device *adev, unsigned int i)\n{\n\tstruct amdgpu_ring *ring = &adev->sdma.instance[i].ring;\n\tu32 rb_cntl, ib_cntl, wptr_poll_cntl;\n\tu32 wb_offset;\n\tu32 doorbell;\n\tu32 doorbell_offset;\n\tu64 wptr_gpu_addr;\n\n\twb_offset = (ring->rptr_offs * 4);\n\n\trb_cntl = RREG32_SDMA(i, regSDMA_GFX_RB_CNTL);\n\trb_cntl = sdma_v4_4_2_rb_cntl(ring, rb_cntl);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_CNTL, rb_cntl);\n\n\t \n\tWREG32_SDMA(i, regSDMA_GFX_RB_RPTR, 0);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_RPTR_HI, 0);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_WPTR, 0);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_WPTR_HI, 0);\n\n\t \n\tWREG32_SDMA(i, regSDMA_GFX_RB_RPTR_ADDR_HI,\n\t       upper_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFF);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_RPTR_ADDR_LO,\n\t       lower_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFC);\n\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_GFX_RB_CNTL,\n\t\t\t\tRPTR_WRITEBACK_ENABLE, 1);\n\n\tWREG32_SDMA(i, regSDMA_GFX_RB_BASE, ring->gpu_addr >> 8);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_BASE_HI, ring->gpu_addr >> 40);\n\n\tring->wptr = 0;\n\n\t \n\tWREG32_SDMA(i, regSDMA_GFX_MINOR_PTR_UPDATE, 1);\n\n\tdoorbell = RREG32_SDMA(i, regSDMA_GFX_DOORBELL);\n\tdoorbell_offset = RREG32_SDMA(i, regSDMA_GFX_DOORBELL_OFFSET);\n\n\tdoorbell = REG_SET_FIELD(doorbell, SDMA_GFX_DOORBELL, ENABLE,\n\t\t\t\t ring->use_doorbell);\n\tdoorbell_offset = REG_SET_FIELD(doorbell_offset,\n\t\t\t\t\tSDMA_GFX_DOORBELL_OFFSET,\n\t\t\t\t\tOFFSET, ring->doorbell_index);\n\tWREG32_SDMA(i, regSDMA_GFX_DOORBELL, doorbell);\n\tWREG32_SDMA(i, regSDMA_GFX_DOORBELL_OFFSET, doorbell_offset);\n\n\tsdma_v4_4_2_ring_set_wptr(ring);\n\n\t \n\tWREG32_SDMA(i, regSDMA_GFX_MINOR_PTR_UPDATE, 0);\n\n\t \n\twptr_gpu_addr = adev->wb.gpu_addr + (ring->wptr_offs * 4);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_WPTR_POLL_ADDR_LO,\n\t\t    lower_32_bits(wptr_gpu_addr));\n\tWREG32_SDMA(i, regSDMA_GFX_RB_WPTR_POLL_ADDR_HI,\n\t\t    upper_32_bits(wptr_gpu_addr));\n\twptr_poll_cntl = RREG32_SDMA(i, regSDMA_GFX_RB_WPTR_POLL_CNTL);\n\twptr_poll_cntl = REG_SET_FIELD(wptr_poll_cntl,\n\t\t\t\t       SDMA_GFX_RB_WPTR_POLL_CNTL,\n\t\t\t\t       F32_POLL_ENABLE, amdgpu_sriov_vf(adev)? 1 : 0);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_WPTR_POLL_CNTL, wptr_poll_cntl);\n\n\t \n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_GFX_RB_CNTL, RB_ENABLE, 1);\n\tWREG32_SDMA(i, regSDMA_GFX_RB_CNTL, rb_cntl);\n\n\tib_cntl = RREG32_SDMA(i, regSDMA_GFX_IB_CNTL);\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_GFX_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_GFX_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t \n\tWREG32_SDMA(i, regSDMA_GFX_IB_CNTL, ib_cntl);\n}\n\n \nstatic void sdma_v4_4_2_page_resume(struct amdgpu_device *adev, unsigned int i)\n{\n\tstruct amdgpu_ring *ring = &adev->sdma.instance[i].page;\n\tu32 rb_cntl, ib_cntl, wptr_poll_cntl;\n\tu32 wb_offset;\n\tu32 doorbell;\n\tu32 doorbell_offset;\n\tu64 wptr_gpu_addr;\n\n\twb_offset = (ring->rptr_offs * 4);\n\n\trb_cntl = RREG32_SDMA(i, regSDMA_PAGE_RB_CNTL);\n\trb_cntl = sdma_v4_4_2_rb_cntl(ring, rb_cntl);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_CNTL, rb_cntl);\n\n\t \n\tWREG32_SDMA(i, regSDMA_PAGE_RB_RPTR, 0);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_RPTR_HI, 0);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_WPTR, 0);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_WPTR_HI, 0);\n\n\t \n\tWREG32_SDMA(i, regSDMA_PAGE_RB_RPTR_ADDR_HI,\n\t       upper_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFF);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_RPTR_ADDR_LO,\n\t       lower_32_bits(adev->wb.gpu_addr + wb_offset) & 0xFFFFFFFC);\n\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_PAGE_RB_CNTL,\n\t\t\t\tRPTR_WRITEBACK_ENABLE, 1);\n\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_BASE, ring->gpu_addr >> 8);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_BASE_HI, ring->gpu_addr >> 40);\n\n\tring->wptr = 0;\n\n\t \n\tWREG32_SDMA(i, regSDMA_PAGE_MINOR_PTR_UPDATE, 1);\n\n\tdoorbell = RREG32_SDMA(i, regSDMA_PAGE_DOORBELL);\n\tdoorbell_offset = RREG32_SDMA(i, regSDMA_PAGE_DOORBELL_OFFSET);\n\n\tdoorbell = REG_SET_FIELD(doorbell, SDMA_PAGE_DOORBELL, ENABLE,\n\t\t\t\t ring->use_doorbell);\n\tdoorbell_offset = REG_SET_FIELD(doorbell_offset,\n\t\t\t\t\tSDMA_PAGE_DOORBELL_OFFSET,\n\t\t\t\t\tOFFSET, ring->doorbell_index);\n\tWREG32_SDMA(i, regSDMA_PAGE_DOORBELL, doorbell);\n\tWREG32_SDMA(i, regSDMA_PAGE_DOORBELL_OFFSET, doorbell_offset);\n\n\t \n\tsdma_v4_4_2_page_ring_set_wptr(ring);\n\n\t \n\tWREG32_SDMA(i, regSDMA_PAGE_MINOR_PTR_UPDATE, 0);\n\n\t \n\twptr_gpu_addr = adev->wb.gpu_addr + (ring->wptr_offs * 4);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_WPTR_POLL_ADDR_LO,\n\t\t    lower_32_bits(wptr_gpu_addr));\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_WPTR_POLL_ADDR_HI,\n\t\t    upper_32_bits(wptr_gpu_addr));\n\twptr_poll_cntl = RREG32_SDMA(i, regSDMA_PAGE_RB_WPTR_POLL_CNTL);\n\twptr_poll_cntl = REG_SET_FIELD(wptr_poll_cntl,\n\t\t\t\t       SDMA_PAGE_RB_WPTR_POLL_CNTL,\n\t\t\t\t       F32_POLL_ENABLE, amdgpu_sriov_vf(adev)? 1 : 0);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_WPTR_POLL_CNTL, wptr_poll_cntl);\n\n\t \n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA_PAGE_RB_CNTL, RB_ENABLE, 1);\n\tWREG32_SDMA(i, regSDMA_PAGE_RB_CNTL, rb_cntl);\n\n\tib_cntl = RREG32_SDMA(i, regSDMA_PAGE_IB_CNTL);\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_PAGE_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA_PAGE_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t \n\tWREG32_SDMA(i, regSDMA_PAGE_IB_CNTL, ib_cntl);\n}\n\nstatic void sdma_v4_4_2_init_pg(struct amdgpu_device *adev)\n{\n\n}\n\n \nstatic int sdma_v4_4_2_inst_rlc_resume(struct amdgpu_device *adev,\n\t\t\t\t       uint32_t inst_mask)\n{\n\tsdma_v4_4_2_init_pg(adev);\n\n\treturn 0;\n}\n\n \nstatic int sdma_v4_4_2_inst_load_microcode(struct amdgpu_device *adev,\n\t\t\t\t\t   uint32_t inst_mask)\n{\n\tconst struct sdma_firmware_header_v1_0 *hdr;\n\tconst __le32 *fw_data;\n\tu32 fw_size;\n\tint i, j;\n\n\t \n\tsdma_v4_4_2_inst_enable(adev, false, inst_mask);\n\n\tfor_each_inst(i, inst_mask) {\n\t\tif (!adev->sdma.instance[i].fw)\n\t\t\treturn -EINVAL;\n\n\t\thdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\n\t\tamdgpu_ucode_print_sdma_hdr(&hdr->header);\n\t\tfw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\n\t\tfw_data = (const __le32 *)\n\t\t\t(adev->sdma.instance[i].fw->data +\n\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\t\tWREG32_SDMA(i, regSDMA_UCODE_ADDR, 0);\n\n\t\tfor (j = 0; j < fw_size; j++)\n\t\t\tWREG32_SDMA(i, regSDMA_UCODE_DATA,\n\t\t\t\t    le32_to_cpup(fw_data++));\n\n\t\tWREG32_SDMA(i, regSDMA_UCODE_ADDR,\n\t\t\t    adev->sdma.instance[i].fw_version);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sdma_v4_4_2_inst_start(struct amdgpu_device *adev,\n\t\t\t\t  uint32_t inst_mask)\n{\n\tstruct amdgpu_ring *ring;\n\tuint32_t tmp_mask;\n\tint i, r = 0;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tsdma_v4_4_2_inst_ctx_switch_enable(adev, false, inst_mask);\n\t\tsdma_v4_4_2_inst_enable(adev, false, inst_mask);\n\t} else {\n\t\t \n\t\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP &&\n\t\t    adev->sdma.instance[0].fw) {\n\t\t\tr = sdma_v4_4_2_inst_load_microcode(adev, inst_mask);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\t \n\t\tsdma_v4_4_2_inst_enable(adev, true, inst_mask);\n\t\t \n\t\tsdma_v4_4_2_inst_ctx_switch_enable(adev, true, inst_mask);\n\t}\n\n\t \n\ttmp_mask = inst_mask;\n\tfor_each_inst(i, tmp_mask) {\n\t\tuint32_t temp;\n\n\t\tWREG32_SDMA(i, regSDMA_SEM_WAIT_FAIL_TIMER_CNTL, 0);\n\t\tsdma_v4_4_2_gfx_resume(adev, i);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsdma_v4_4_2_page_resume(adev, i);\n\n\t\t \n\t\ttemp = RREG32_SDMA(i, regSDMA_CNTL);\n\t\ttemp = REG_SET_FIELD(temp, SDMA_CNTL, UTC_L1_ENABLE, 1);\n\t\t \n\t\ttemp = REG_SET_FIELD(temp, SDMA_CNTL, CTXEMPTY_INT_ENABLE, 1);\n\t\tWREG32_SDMA(i, regSDMA_CNTL, temp);\n\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\t\t\t \n\t\t\t\ttemp = RREG32_SDMA(i, regSDMA_F32_CNTL);\n\t\t\t\ttemp = REG_SET_FIELD(temp, SDMA_F32_CNTL, HALT, 0);\n\t\t\t\tWREG32_SDMA(i, regSDMA_F32_CNTL, temp);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tsdma_v4_4_2_inst_ctx_switch_enable(adev, true, inst_mask);\n\t\tsdma_v4_4_2_inst_enable(adev, true, inst_mask);\n\t} else {\n\t\tr = sdma_v4_4_2_inst_rlc_resume(adev, inst_mask);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\ttmp_mask = inst_mask;\n\tfor_each_inst(i, tmp_mask) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tstruct amdgpu_ring *page = &adev->sdma.instance[i].page;\n\n\t\t\tr = amdgpu_ring_test_helper(page);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (adev->mman.buffer_funcs_ring == page)\n\t\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t\t}\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn r;\n}\n\n \nstatic int sdma_v4_4_2_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\tr = amdgpu_ring_alloc(ring, 5);\n\tif (r)\n\t\tgoto error_free_wb;\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\t\t  SDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\nerror_free_wb:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic int sdma_v4_4_2_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tlong r;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, 256,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err0;\n\n\tib.ptr[0] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr);\n\tib.ptr[3] = SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0);\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.ptr[5] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[6] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[7] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.length_dw = 8;\n\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tgoto err1;\n\t}\n\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n\n \nstatic void sdma_v4_4_2_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t\t  uint64_t pe, uint64_t src,\n\t\t\t\t  unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\n\tib->ptr[ib->length_dw++] = bytes - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\n}\n\n \nstatic void sdma_v4_4_2_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t   uint64_t value, unsigned count,\n\t\t\t\t   uint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = ndw - 1;\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void sdma_v4_4_2_vm_set_pte_pde(struct amdgpu_ib *ib,\n\t\t\t\t     uint64_t pe,\n\t\t\t\t     uint64_t addr, unsigned count,\n\t\t\t\t     uint32_t incr, uint64_t flags)\n{\n\t \n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_PTEPDE);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\tib->ptr[ib->length_dw++] = lower_32_bits(addr);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = incr;  \n\tib->ptr[ib->length_dw++] = 0;\n\tib->ptr[ib->length_dw++] = count - 1;  \n}\n\n \nstatic void sdma_v4_4_2_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tu32 pad_count;\n\tint i;\n\n\tpad_count = (-ib->length_dw) & 7;\n\tfor (i = 0; i < pad_count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP) |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);\n\t\telse\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n}\n\n\n \nstatic void sdma_v4_4_2_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tsdma_v4_4_2_wait_reg_mem(ring, 1, 0,\n\t\t\t       addr & 0xfffffffc,\n\t\t\t       upper_32_bits(addr) & 0xffffffff,\n\t\t\t       seq, 0xffffffff, 4);\n}\n\n\n \nstatic void sdma_v4_4_2_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\t unsigned vmid, uint64_t pd_addr)\n{\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n}\n\nstatic void sdma_v4_4_2_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t     uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\n\t\t\t  SDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void sdma_v4_4_2_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t\t uint32_t val, uint32_t mask)\n{\n\tsdma_v4_4_2_wait_reg_mem(ring, 0, 0, reg, 0, val, mask, 10);\n}\n\nstatic bool sdma_v4_4_2_fw_support_paging_queue(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 4, 2):\n\t\treturn false;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int sdma_v4_4_2_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = sdma_v4_4_2_init_microcode(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load sdma firmware!\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tif (sdma_v4_4_2_fw_support_paging_queue(adev))\n\t\tadev->sdma.has_page_queue = true;\n\n\tsdma_v4_4_2_set_ring_funcs(adev);\n\tsdma_v4_4_2_set_buffer_funcs(adev);\n\tsdma_v4_4_2_set_vm_pte_funcs(adev);\n\tsdma_v4_4_2_set_irq_funcs(adev);\n\tsdma_v4_4_2_set_ras_funcs(adev);\n\n\treturn 0;\n}\n\n#if 0\nstatic int sdma_v4_4_2_process_ras_data_cb(struct amdgpu_device *adev,\n\t\tvoid *err_data,\n\t\tstruct amdgpu_iv_entry *entry);\n#endif\n\nstatic int sdma_v4_4_2_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n#if 0\n\tstruct ras_ih_if ih_info = {\n\t\t.cb = sdma_v4_4_2_process_ras_data_cb,\n\t};\n#endif\n\tif (!amdgpu_persistent_edc_harvesting_supported(adev)) {\n\t\tif (adev->sdma.ras && adev->sdma.ras->ras_block.hw_ops &&\n\t\t    adev->sdma.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\tadev->sdma.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 aid_id;\n\n\t \n\tfor (i = 0; i < adev->sdma.num_inst_per_aid; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_TRAP,\n\t\t\t\t      &adev->sdma.trap_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tfor (i = 0; i < adev->sdma.num_inst_per_aid; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_SRAM_ECC,\n\t\t\t\t      &adev->sdma.ecc_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tfor (i = 0; i < adev->sdma.num_inst_per_aid; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_VM_HOLE,\n\t\t\t\t      &adev->sdma.vm_hole_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_DOORBELL_INVALID,\n\t\t\t\t      &adev->sdma.doorbell_invalid_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_POLL_TIMEOUT,\n\t\t\t\t      &adev->sdma.pool_timeout_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_4_2_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_SRBMWRITE,\n\t\t\t\t      &adev->sdma.srbm_write_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tring->use_doorbell = true;\n\t\taid_id = adev->sdma.instance[i].aid_id;\n\n\t\tDRM_DEBUG(\"SDMA %d use_doorbell being set to: [%s]\\n\", i,\n\t\t\t\tring->use_doorbell?\"true\":\"false\");\n\n\t\t \n\t\tring->doorbell_index = adev->doorbell_index.sdma_engine[i] << 1;\n\t\tring->vm_hub = AMDGPU_MMHUB0(aid_id);\n\n\t\tsprintf(ring->name, \"sdma%d.%d\", aid_id,\n\t\t\t\ti % adev->sdma.num_inst_per_aid);\n\t\tr = amdgpu_ring_init(adev, ring, 1024, &adev->sdma.trap_irq,\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE0 + i,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tring = &adev->sdma.instance[i].page;\n\t\t\tring->ring_obj = NULL;\n\t\t\tring->use_doorbell = true;\n\n\t\t\t \n\t\t\tring->doorbell_index =\n\t\t\t\t(adev->doorbell_index.sdma_engine[i] + 1) << 1;\n\t\t\tring->vm_hub = AMDGPU_MMHUB0(aid_id);\n\n\t\t\tsprintf(ring->name, \"page%d.%d\", aid_id,\n\t\t\t\t\ti % adev->sdma.num_inst_per_aid);\n\t\t\tr = amdgpu_ring_init(adev, ring, 1024,\n\t\t\t\t\t     &adev->sdma.trap_irq,\n\t\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE0 + i,\n\t\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (amdgpu_sdma_ras_sw_init(adev)) {\n\t\tdev_err(adev->dev, \"fail to initialize sdma ras block\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\nstatic int sdma_v4_4_2_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tamdgpu_ring_fini(&adev->sdma.instance[i].page);\n\t}\n\n\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 4, 2))\n\t\tamdgpu_sdma_destroy_inst_ctx(adev, true);\n\telse\n\t\tamdgpu_sdma_destroy_inst_ctx(adev, false);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tuint32_t inst_mask;\n\n\tinst_mask = GENMASK(adev->sdma.num_instances - 1, 0);\n\tif (!amdgpu_sriov_vf(adev))\n\t\tsdma_v4_4_2_inst_init_golden_registers(adev, inst_mask);\n\n\tr = sdma_v4_4_2_inst_start(adev, inst_mask);\n\n\treturn r;\n}\n\nstatic int sdma_v4_4_2_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tuint32_t inst_mask;\n\tint i;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tinst_mask = GENMASK(adev->sdma.num_instances - 1, 0);\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tamdgpu_irq_put(adev, &adev->sdma.ecc_irq,\n\t\t\t\t       AMDGPU_SDMA_IRQ_INSTANCE0 + i);\n\t\t}\n\t}\n\n\tsdma_v4_4_2_inst_ctx_switch_enable(adev, false, inst_mask);\n\tsdma_v4_4_2_inst_enable(adev, false, inst_mask);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_set_clockgating_state(void *handle,\n\t\t\t\t\t     enum amd_clockgating_state state);\n\nstatic int sdma_v4_4_2_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_in_reset(adev))\n\t\tsdma_v4_4_2_set_clockgating_state(adev, AMD_CG_STATE_UNGATE);\n\n\treturn sdma_v4_4_2_hw_fini(adev);\n}\n\nstatic int sdma_v4_4_2_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v4_4_2_hw_init(adev);\n}\n\nstatic bool sdma_v4_4_2_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tu32 tmp = RREG32_SDMA(i, regSDMA_STATUS_REG);\n\n\t\tif (!(tmp & SDMA_STATUS_REG__IDLE_MASK))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int sdma_v4_4_2_wait_for_idle(void *handle)\n{\n\tunsigned i, j;\n\tu32 sdma[AMDGPU_MAX_SDMA_INSTANCES];\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tfor (j = 0; j < adev->sdma.num_instances; j++) {\n\t\t\tsdma[j] = RREG32_SDMA(j, regSDMA_STATUS_REG);\n\t\t\tif (!(sdma[j] & SDMA_STATUS_REG__IDLE_MASK))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j == adev->sdma.num_instances)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int sdma_v4_4_2_soft_reset(void *handle)\n{\n\t \n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tsdma_cntl = RREG32_SDMA(type, regSDMA_CNTL);\n\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA_CNTL, TRAP_ENABLE,\n\t\t       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\tWREG32_SDMA(type, regSDMA_CNTL, sdma_cntl);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tuint32_t instance, i;\n\n\tDRM_DEBUG(\"IH: SDMA trap\\n\");\n\tinstance = sdma_v4_4_2_irq_id_to_seq(entry->client_id);\n\n\t \n\tfor (i = instance; i < adev->sdma.num_instances;\n\t     i += adev->sdma.num_inst_per_aid) {\n\t\tif (adev->sdma.instance[i].aid_id ==\n\t\t    node_id_to_phys_map[entry->node_id])\n\t\t\tbreak;\n\t}\n\n\tif (i >= adev->sdma.num_instances) {\n\t\tdev_WARN_ONCE(\n\t\t\tadev->dev, 1,\n\t\t\t\"Couldn't find the right sdma instance in trap handler\");\n\t\treturn 0;\n\t}\n\n\tswitch (entry->ring_id) {\n\tcase 0:\n\t\tamdgpu_fence_process(&adev->sdma.instance[i].ring);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n#if 0\nstatic int sdma_v4_4_2_process_ras_data_cb(struct amdgpu_device *adev,\n\t\tvoid *err_data,\n\t\tstruct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\n\t \n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA))\n\t\tgoto out;\n\n\tinstance = sdma_v4_4_2_irq_id_to_seq(entry->client_id);\n\tif (instance < 0)\n\t\tgoto out;\n\n\tamdgpu_sdma_process_ras_data_cb(adev, err_data, entry);\n\nout:\n\treturn AMDGPU_RAS_SUCCESS;\n}\n#endif\n\nstatic int sdma_v4_4_2_process_illegal_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\n\tDRM_ERROR(\"Illegal instruction in SDMA command stream\\n\");\n\n\tinstance = sdma_v4_4_2_irq_id_to_seq(entry->client_id);\n\tif (instance < 0)\n\t\treturn 0;\n\n\tswitch (entry->ring_id) {\n\tcase 0:\n\t\tdrm_sched_fault(&adev->sdma.instance[instance].ring.sched);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_set_ecc_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tsdma_cntl = RREG32_SDMA(type, regSDMA_CNTL);\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA_CNTL,\n\t\t\t\t\t  DRAM_ECC_INT_ENABLE, 0);\n\t\tWREG32_SDMA(type, regSDMA_CNTL, sdma_cntl);\n\t\tbreak;\n\t \n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_print_iv_entry(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\tstruct amdgpu_task_info task_info;\n\tu64 addr;\n\n\tinstance = sdma_v4_4_2_irq_id_to_seq(entry->client_id);\n\tif (instance < 0 || instance >= adev->sdma.num_instances) {\n\t\tdev_err(adev->dev, \"sdma instance invalid %d\\n\", instance);\n\t\treturn -EINVAL;\n\t}\n\n\taddr = (u64)entry->src_data[0] << 12;\n\taddr |= ((u64)entry->src_data[1] & 0xf) << 44;\n\n\tmemset(&task_info, 0, sizeof(struct amdgpu_task_info));\n\tamdgpu_vm_get_task_info(adev, entry->pasid, &task_info);\n\n\tdev_dbg_ratelimited(adev->dev,\n\t\t   \"[sdma%d] address:0x%016llx src_id:%u ring:%u vmid:%u \"\n\t\t   \"pasid:%u, for process %s pid %d thread %s pid %d\\n\",\n\t\t   instance, addr, entry->src_id, entry->ring_id, entry->vmid,\n\t\t   entry->pasid, task_info.process_name, task_info.tgid,\n\t\t   task_info.task_name, task_info.pid);\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_process_vm_hole_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev, \"MC or SEM address in VM hole\\n\");\n\tsdma_v4_4_2_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_process_doorbell_invalid_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\n\tdev_dbg_ratelimited(adev->dev, \"SDMA received a doorbell from BIF with byte_enable !=0xff\\n\");\n\tsdma_v4_4_2_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_process_pool_timeout_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev,\n\t\t\"Polling register/memory timeout executing POLL_REG/MEM with finite timer\\n\");\n\tsdma_v4_4_2_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_process_srbm_write_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev,\n\t\t\"SDMA gets an Register Write SRBM_WRITE command in non-privilege command buffer\\n\");\n\tsdma_v4_4_2_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic void sdma_v4_4_2_inst_update_medium_grain_light_sleep(\n\tstruct amdgpu_device *adev, bool enable, uint32_t inst_mask)\n{\n\tuint32_t data, def;\n\tint i;\n\n\t \n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS))\n\t\treturn;\n\n\tif (enable) {\n\t\tfor_each_inst(i, inst_mask) {\n\t\t\t \n\t\t\tdef = data = RREG32_SDMA(i, regSDMA_POWER_CNTL);\n\t\t\tdata |= SDMA_POWER_CNTL__MEM_POWER_OVERRIDE_MASK;\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, regSDMA_POWER_CNTL, data);\n\t\t}\n\t} else {\n\t\tfor_each_inst(i, inst_mask) {\n\t\t\t \n\t\t\tdef = data = RREG32_SDMA(i, regSDMA_POWER_CNTL);\n\t\t\tdata &= ~SDMA_POWER_CNTL__MEM_POWER_OVERRIDE_MASK;\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, regSDMA_POWER_CNTL, data);\n\t\t}\n\t}\n}\n\nstatic void sdma_v4_4_2_inst_update_medium_grain_clock_gating(\n\tstruct amdgpu_device *adev, bool enable, uint32_t inst_mask)\n{\n\tuint32_t data, def;\n\tint i;\n\n\t \n\tif (!(adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG))\n\t\treturn;\n\n\tif (enable) {\n\t\tfor_each_inst(i, inst_mask) {\n\t\t\tdef = data = RREG32_SDMA(i, regSDMA_CLK_CTRL);\n\t\t\tdata &= ~(SDMA_CLK_CTRL__SOFT_OVERRIDE5_MASK |\n\t\t\t\t  SDMA_CLK_CTRL__SOFT_OVERRIDE4_MASK |\n\t\t\t\t  SDMA_CLK_CTRL__SOFT_OVERRIDE3_MASK |\n\t\t\t\t  SDMA_CLK_CTRL__SOFT_OVERRIDE2_MASK |\n\t\t\t\t  SDMA_CLK_CTRL__SOFT_OVERRIDE1_MASK |\n\t\t\t\t  SDMA_CLK_CTRL__SOFT_OVERRIDE0_MASK);\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, regSDMA_CLK_CTRL, data);\n\t\t}\n\t} else {\n\t\tfor_each_inst(i, inst_mask) {\n\t\t\tdef = data = RREG32_SDMA(i, regSDMA_CLK_CTRL);\n\t\t\tdata |= (SDMA_CLK_CTRL__SOFT_OVERRIDE5_MASK |\n\t\t\t\t SDMA_CLK_CTRL__SOFT_OVERRIDE4_MASK |\n\t\t\t\t SDMA_CLK_CTRL__SOFT_OVERRIDE3_MASK |\n\t\t\t\t SDMA_CLK_CTRL__SOFT_OVERRIDE2_MASK |\n\t\t\t\t SDMA_CLK_CTRL__SOFT_OVERRIDE1_MASK |\n\t\t\t\t SDMA_CLK_CTRL__SOFT_OVERRIDE0_MASK);\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, regSDMA_CLK_CTRL, data);\n\t\t}\n\t}\n}\n\nstatic int sdma_v4_4_2_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tuint32_t inst_mask;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tinst_mask = GENMASK(adev->sdma.num_instances - 1, 0);\n\n\tsdma_v4_4_2_inst_update_medium_grain_clock_gating(\n\t\tadev, state == AMD_CG_STATE_GATE, inst_mask);\n\tsdma_v4_4_2_inst_update_medium_grain_light_sleep(\n\t\tadev, state == AMD_CG_STATE_GATE, inst_mask);\n\treturn 0;\n}\n\nstatic int sdma_v4_4_2_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic void sdma_v4_4_2_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t \n\tdata = RREG32(SOC15_REG_OFFSET(SDMA0, GET_INST(SDMA0, 0), regSDMA_CLK_CTRL));\n\tif (!(data & SDMA_CLK_CTRL__SOFT_OVERRIDE5_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_SDMA_MGCG;\n\n\t \n\tdata = RREG32(SOC15_REG_OFFSET(SDMA0, GET_INST(SDMA0, 0), regSDMA_POWER_CNTL));\n\tif (data & SDMA_POWER_CNTL__MEM_POWER_OVERRIDE_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_SDMA_LS;\n}\n\nconst struct amd_ip_funcs sdma_v4_4_2_ip_funcs = {\n\t.name = \"sdma_v4_4_2\",\n\t.early_init = sdma_v4_4_2_early_init,\n\t.late_init = sdma_v4_4_2_late_init,\n\t.sw_init = sdma_v4_4_2_sw_init,\n\t.sw_fini = sdma_v4_4_2_sw_fini,\n\t.hw_init = sdma_v4_4_2_hw_init,\n\t.hw_fini = sdma_v4_4_2_hw_fini,\n\t.suspend = sdma_v4_4_2_suspend,\n\t.resume = sdma_v4_4_2_resume,\n\t.is_idle = sdma_v4_4_2_is_idle,\n\t.wait_for_idle = sdma_v4_4_2_wait_for_idle,\n\t.soft_reset = sdma_v4_4_2_soft_reset,\n\t.set_clockgating_state = sdma_v4_4_2_set_clockgating_state,\n\t.set_powergating_state = sdma_v4_4_2_set_powergating_state,\n\t.get_clockgating_state = sdma_v4_4_2_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v4_4_2_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xff,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = true,\n\t.get_rptr = sdma_v4_4_2_ring_get_rptr,\n\t.get_wptr = sdma_v4_4_2_ring_get_wptr,\n\t.set_wptr = sdma_v4_4_2_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\t \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 6 +\n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 7 + 6,  \n\t.emit_ib = sdma_v4_4_2_ring_emit_ib,\n\t.emit_fence = sdma_v4_4_2_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v4_4_2_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v4_4_2_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v4_4_2_ring_emit_hdp_flush,\n\t.test_ring = sdma_v4_4_2_ring_test_ring,\n\t.test_ib = sdma_v4_4_2_ring_test_ib,\n\t.insert_nop = sdma_v4_4_2_ring_insert_nop,\n\t.pad_ib = sdma_v4_4_2_ring_pad_ib,\n\t.emit_wreg = sdma_v4_4_2_ring_emit_wreg,\n\t.emit_reg_wait = sdma_v4_4_2_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v4_4_2_page_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xff,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = true,\n\t.get_rptr = sdma_v4_4_2_ring_get_rptr,\n\t.get_wptr = sdma_v4_4_2_page_ring_get_wptr,\n\t.set_wptr = sdma_v4_4_2_page_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\t \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 6 +\n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 7 + 6,  \n\t.emit_ib = sdma_v4_4_2_ring_emit_ib,\n\t.emit_fence = sdma_v4_4_2_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v4_4_2_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v4_4_2_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v4_4_2_ring_emit_hdp_flush,\n\t.test_ring = sdma_v4_4_2_ring_test_ring,\n\t.test_ib = sdma_v4_4_2_ring_test_ib,\n\t.insert_nop = sdma_v4_4_2_ring_insert_nop,\n\t.pad_ib = sdma_v4_4_2_ring_pad_ib,\n\t.emit_wreg = sdma_v4_4_2_ring_emit_wreg,\n\t.emit_reg_wait = sdma_v4_4_2_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void sdma_v4_4_2_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i, dev_inst;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->sdma.instance[i].ring.funcs = &sdma_v4_4_2_ring_funcs;\n\t\tadev->sdma.instance[i].ring.me = i;\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tadev->sdma.instance[i].page.funcs =\n\t\t\t\t&sdma_v4_4_2_page_ring_funcs;\n\t\t\tadev->sdma.instance[i].page.me = i;\n\t\t}\n\n\t\tdev_inst = GET_INST(SDMA0, i);\n\t\t \n\t\tadev->sdma.instance[i].aid_id =\n\t\t\tdev_inst / adev->sdma.num_inst_per_aid;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_trap_irq_funcs = {\n\t.set = sdma_v4_4_2_set_trap_irq_state,\n\t.process = sdma_v4_4_2_process_trap_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_illegal_inst_irq_funcs = {\n\t.process = sdma_v4_4_2_process_illegal_inst_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_ecc_irq_funcs = {\n\t.set = sdma_v4_4_2_set_ecc_irq_state,\n\t.process = amdgpu_sdma_process_ecc_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_vm_hole_irq_funcs = {\n\t.process = sdma_v4_4_2_process_vm_hole_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_doorbell_invalid_irq_funcs = {\n\t.process = sdma_v4_4_2_process_doorbell_invalid_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_pool_timeout_irq_funcs = {\n\t.process = sdma_v4_4_2_process_pool_timeout_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_4_2_srbm_write_irq_funcs = {\n\t.process = sdma_v4_4_2_process_srbm_write_irq,\n};\n\nstatic void sdma_v4_4_2_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.ecc_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.vm_hole_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.doorbell_invalid_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.pool_timeout_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.srbm_write_irq.num_types = adev->sdma.num_instances;\n\n\tadev->sdma.trap_irq.funcs = &sdma_v4_4_2_trap_irq_funcs;\n\tadev->sdma.illegal_inst_irq.funcs = &sdma_v4_4_2_illegal_inst_irq_funcs;\n\tadev->sdma.ecc_irq.funcs = &sdma_v4_4_2_ecc_irq_funcs;\n\tadev->sdma.vm_hole_irq.funcs = &sdma_v4_4_2_vm_hole_irq_funcs;\n\tadev->sdma.doorbell_invalid_irq.funcs = &sdma_v4_4_2_doorbell_invalid_irq_funcs;\n\tadev->sdma.pool_timeout_irq.funcs = &sdma_v4_4_2_pool_timeout_irq_funcs;\n\tadev->sdma.srbm_write_irq.funcs = &sdma_v4_4_2_srbm_write_irq_funcs;\n}\n\n \nstatic void sdma_v4_4_2_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint64_t src_offset,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count,\n\t\t\t\t       bool tmz)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_TMZ(tmz ? 1 : 0);\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n}\n\n \nstatic void sdma_v4_4_2_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint32_t src_data,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_CONST_FILL);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n}\n\nstatic const struct amdgpu_buffer_funcs sdma_v4_4_2_buffer_funcs = {\n\t.copy_max_bytes = 0x400000,\n\t.copy_num_dw = 7,\n\t.emit_copy_buffer = sdma_v4_4_2_emit_copy_buffer,\n\n\t.fill_max_bytes = 0x400000,\n\t.fill_num_dw = 5,\n\t.emit_fill_buffer = sdma_v4_4_2_emit_fill_buffer,\n};\n\nstatic void sdma_v4_4_2_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &sdma_v4_4_2_buffer_funcs;\n\tif (adev->sdma.has_page_queue)\n\t\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].page;\n\telse\n\t\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs sdma_v4_4_2_vm_pte_funcs = {\n\t.copy_pte_num_dw = 7,\n\t.copy_pte = sdma_v4_4_2_vm_copy_pte,\n\n\t.write_pte = sdma_v4_4_2_vm_write_pte,\n\t.set_pte_pde = sdma_v4_4_2_vm_set_pte_pde,\n};\n\nstatic void sdma_v4_4_2_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &sdma_v4_4_2_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsched = &adev->sdma.instance[i].page.sched;\n\t\telse\n\t\t\tsched = &adev->sdma.instance[i].ring.sched;\n\t\tadev->vm_manager.vm_pte_scheds[i] = sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nconst struct amdgpu_ip_block_version sdma_v4_4_2_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 4,\n\t.minor = 4,\n\t.rev = 0,\n\t.funcs = &sdma_v4_4_2_ip_funcs,\n};\n\nstatic int sdma_v4_4_2_xcp_resume(void *handle, uint32_t inst_mask)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\tsdma_v4_4_2_inst_init_golden_registers(adev, inst_mask);\n\n\tr = sdma_v4_4_2_inst_start(adev, inst_mask);\n\n\treturn r;\n}\n\nstatic int sdma_v4_4_2_xcp_suspend(void *handle, uint32_t inst_mask)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tuint32_t tmp_mask = inst_mask;\n\tint i;\n\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor_each_inst(i, tmp_mask) {\n\t\t\tamdgpu_irq_put(adev, &adev->sdma.ecc_irq,\n\t\t\t\t       AMDGPU_SDMA_IRQ_INSTANCE0 + i);\n\t\t}\n\t}\n\n\tsdma_v4_4_2_inst_ctx_switch_enable(adev, false, inst_mask);\n\tsdma_v4_4_2_inst_enable(adev, false, inst_mask);\n\n\treturn 0;\n}\n\nstruct amdgpu_xcp_ip_funcs sdma_v4_4_2_xcp_funcs = {\n\t.suspend = &sdma_v4_4_2_xcp_suspend,\n\t.resume = &sdma_v4_4_2_xcp_resume\n};\n\nstatic const struct amdgpu_ras_err_status_reg_entry sdma_v4_2_2_ue_reg_list[] = {\n\t{AMDGPU_RAS_REG_ENTRY(SDMA0, 0, regSDMA_UE_ERR_STATUS_LO, regSDMA_UE_ERR_STATUS_HI),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"SDMA\"},\n};\n\nstatic const struct amdgpu_ras_memory_id_entry sdma_v4_4_2_ras_memory_list[] = {\n\t{AMDGPU_SDMA_MBANK_DATA_BUF0, \"SDMA_MBANK_DATA_BUF0\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF1, \"SDMA_MBANK_DATA_BUF1\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF2, \"SDMA_MBANK_DATA_BUF2\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF3, \"SDMA_MBANK_DATA_BUF3\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF4, \"SDMA_MBANK_DATA_BUF4\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF5, \"SDMA_MBANK_DATA_BUF5\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF6, \"SDMA_MBANK_DATA_BUF6\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF7, \"SDMA_MBANK_DATA_BUF7\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF8, \"SDMA_MBANK_DATA_BUF8\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF9, \"SDMA_MBANK_DATA_BUF9\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF10, \"SDMA_MBANK_DATA_BUF10\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF11, \"SDMA_MBANK_DATA_BUF11\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF12, \"SDMA_MBANK_DATA_BUF12\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF13, \"SDMA_MBANK_DATA_BUF13\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF14, \"SDMA_MBANK_DATA_BUF14\"},\n\t{AMDGPU_SDMA_MBANK_DATA_BUF15, \"SDMA_MBANK_DATA_BUF15\"},\n\t{AMDGPU_SDMA_UCODE_BUF, \"SDMA_UCODE_BUF\"},\n\t{AMDGPU_SDMA_RB_CMD_BUF, \"SDMA_RB_CMD_BUF\"},\n\t{AMDGPU_SDMA_IB_CMD_BUF, \"SDMA_IB_CMD_BUF\"},\n\t{AMDGPU_SDMA_UTCL1_RD_FIFO, \"SDMA_UTCL1_RD_FIFO\"},\n\t{AMDGPU_SDMA_UTCL1_RDBST_FIFO, \"SDMA_UTCL1_RDBST_FIFO\"},\n\t{AMDGPU_SDMA_UTCL1_WR_FIFO, \"SDMA_UTCL1_WR_FIFO\"},\n\t{AMDGPU_SDMA_DATA_LUT_FIFO, \"SDMA_DATA_LUT_FIFO\"},\n\t{AMDGPU_SDMA_SPLIT_DAT_BUF, \"SDMA_SPLIT_DAT_BUF\"},\n};\n\nstatic void sdma_v4_4_2_inst_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t sdma_inst,\n\t\t\t\t\t\t   void *ras_err_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_err_status;\n\tuint32_t sdma_dev_inst = GET_INST(SDMA0, sdma_inst);\n\n\t \n\tamdgpu_ras_inst_query_ras_error_count(adev,\n\t\t\t\t\tsdma_v4_2_2_ue_reg_list,\n\t\t\t\t\tARRAY_SIZE(sdma_v4_2_2_ue_reg_list),\n\t\t\t\t\tsdma_v4_4_2_ras_memory_list,\n\t\t\t\t\tARRAY_SIZE(sdma_v4_4_2_ras_memory_list),\n\t\t\t\t\tsdma_dev_inst,\n\t\t\t\t\tAMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE,\n\t\t\t\t\t&err_data->ue_count);\n}\n\nstatic void sdma_v4_4_2_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t      void *ras_err_status)\n{\n\tuint32_t inst_mask;\n\tint i = 0;\n\n\tinst_mask = GENMASK(adev->sdma.num_instances - 1, 0);\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor_each_inst(i, inst_mask)\n\t\t\tsdma_v4_4_2_inst_query_ras_error_count(adev, i, ras_err_status);\n\t} else {\n\t\tdev_warn(adev->dev, \"SDMA RAS is not supported\\n\");\n\t}\n}\n\nstatic void sdma_v4_4_2_inst_reset_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t sdma_inst)\n{\n\tuint32_t sdma_dev_inst = GET_INST(SDMA0, sdma_inst);\n\n\tamdgpu_ras_inst_reset_ras_error_count(adev,\n\t\t\t\t\tsdma_v4_2_2_ue_reg_list,\n\t\t\t\t\tARRAY_SIZE(sdma_v4_2_2_ue_reg_list),\n\t\t\t\t\tsdma_dev_inst);\n}\n\nstatic void sdma_v4_4_2_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tuint32_t inst_mask;\n\tint i = 0;\n\n\tinst_mask = GENMASK(adev->sdma.num_instances - 1, 0);\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor_each_inst(i, inst_mask)\n\t\t\tsdma_v4_4_2_inst_reset_ras_error_count(adev, i);\n\t} else {\n\t\tdev_warn(adev->dev, \"SDMA RAS is not supported\\n\");\n\t}\n}\n\nstatic const struct amdgpu_ras_block_hw_ops sdma_v4_4_2_ras_hw_ops = {\n\t.query_ras_error_count = sdma_v4_4_2_query_ras_error_count,\n\t.reset_ras_error_count = sdma_v4_4_2_reset_ras_error_count,\n};\n\nstatic struct amdgpu_sdma_ras sdma_v4_4_2_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &sdma_v4_4_2_ras_hw_ops,\n\t},\n};\n\nstatic void sdma_v4_4_2_set_ras_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.ras = &sdma_v4_4_2_ras;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}