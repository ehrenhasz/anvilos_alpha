{
  "module_name": "vi.c",
  "hash_id": "84187d851bac3b8636eba3ce7f7f9cbec22bca3efe6721429eed45dce34c1ee6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/vi.c",
  "human_readable_source": " \n\n#include <linux/pci.h>\n#include <linux/slab.h>\n\n#include <drm/amdgpu_drm.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_atombios.h\"\n#include \"amdgpu_ih.h\"\n#include \"amdgpu_uvd.h\"\n#include \"amdgpu_vce.h\"\n#include \"amdgpu_ucode.h\"\n#include \"atom.h\"\n#include \"amd_pcie.h\"\n\n#include \"gmc/gmc_8_1_d.h\"\n#include \"gmc/gmc_8_1_sh_mask.h\"\n\n#include \"oss/oss_3_0_d.h\"\n#include \"oss/oss_3_0_sh_mask.h\"\n\n#include \"bif/bif_5_0_d.h\"\n#include \"bif/bif_5_0_sh_mask.h\"\n\n#include \"gca/gfx_8_0_d.h\"\n#include \"gca/gfx_8_0_sh_mask.h\"\n\n#include \"smu/smu_7_1_1_d.h\"\n#include \"smu/smu_7_1_1_sh_mask.h\"\n\n#include \"uvd/uvd_5_0_d.h\"\n#include \"uvd/uvd_5_0_sh_mask.h\"\n\n#include \"vce/vce_3_0_d.h\"\n#include \"vce/vce_3_0_sh_mask.h\"\n\n#include \"dce/dce_10_0_d.h\"\n#include \"dce/dce_10_0_sh_mask.h\"\n\n#include \"vid.h\"\n#include \"vi.h\"\n#include \"gmc_v8_0.h\"\n#include \"gmc_v7_0.h\"\n#include \"gfx_v8_0.h\"\n#include \"sdma_v2_4.h\"\n#include \"sdma_v3_0.h\"\n#include \"dce_v10_0.h\"\n#include \"dce_v11_0.h\"\n#include \"iceland_ih.h\"\n#include \"tonga_ih.h\"\n#include \"cz_ih.h\"\n#include \"uvd_v5_0.h\"\n#include \"uvd_v6_0.h\"\n#include \"vce_v3_0.h\"\n#if defined(CONFIG_DRM_AMD_ACP)\n#include \"amdgpu_acp.h\"\n#endif\n#include \"amdgpu_vkms.h\"\n#include \"mxgpu_vi.h\"\n#include \"amdgpu_dm.h\"\n\n#define ixPCIE_LC_L1_PM_SUBSTATE\t0x100100C6\n#define PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK\t0x00000001L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK\t0x00000002L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK\t0x00000004L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK\t\t0x00000008L\n#define PCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK\t\t0x00000010L\n#define ixPCIE_L1_PM_SUB_CNTL\t0x378\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK\t0x00000004L\n#define PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK\t0x00000008L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK\t0x00000001L\n#define PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK\t0x00000002L\n#define PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK\t\t0x00200000L\n#define LINK_CAP\t0x64\n#define PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK\t0x00040000L\n#define ixCPM_CONTROL\t0x1400118\n#define ixPCIE_LC_CNTL7\t0x100100BC\n#define PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK\t0x00000400L\n#define PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT\t0x00000007\n#define PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT\t0x00000009\n#define CPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK\t0x01000000L\n#define PCIE_L1_PM_SUB_CNTL\t0x378\n#define ASIC_IS_P22(asic_type, rid)\t((asic_type >= CHIP_POLARIS10) && \\\n\t\t\t\t\t\t\t\t\t(asic_type <= CHIP_POLARIS12) && \\\n\t\t\t\t\t\t\t\t\t(rid >= 0x6E))\n \nstatic const struct amdgpu_video_codecs topaz_video_codecs_encode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n \nstatic const struct amdgpu_video_codec_info tonga_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_encode_array),\n\t.codec_array = tonga_video_codecs_encode_array,\n};\n\n \nstatic const struct amdgpu_video_codec_info polaris_video_codecs_encode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 2304,\n\t\t.max_pixels_per_frame = 4096 * 2304,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs polaris_video_codecs_encode =\n{\n\t.codec_count = ARRAY_SIZE(polaris_video_codecs_encode_array),\n\t.codec_array = polaris_video_codecs_encode_array,\n};\n\n \nstatic const struct amdgpu_video_codecs topaz_video_codecs_decode =\n{\n\t.codec_count = 0,\n\t.codec_array = NULL,\n};\n\n \nstatic const struct amdgpu_video_codec_info tonga_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs tonga_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(tonga_video_codecs_decode_array),\n\t.codec_array = tonga_video_codecs_decode_array,\n};\n\n \nstatic const struct amdgpu_video_codec_info cz_video_codecs_decode_array[] =\n{\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG2,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 3,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 5,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_MPEG4_AVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 52,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_VC1,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 4,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_HEVC,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 186,\n\t},\n\t{\n\t\t.codec_type = AMDGPU_INFO_VIDEO_CAPS_CODEC_IDX_JPEG,\n\t\t.max_width = 4096,\n\t\t.max_height = 4096,\n\t\t.max_pixels_per_frame = 4096 * 4096,\n\t\t.max_level = 0,\n\t},\n};\n\nstatic const struct amdgpu_video_codecs cz_video_codecs_decode =\n{\n\t.codec_count = ARRAY_SIZE(cz_video_codecs_decode_array),\n\t.codec_array = cz_video_codecs_decode_array,\n};\n\nstatic int vi_query_video_codecs(struct amdgpu_device *adev, bool encode,\n\t\t\t\t const struct amdgpu_video_codecs **codecs)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tif (encode)\n\t\t\t*codecs = &topaz_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &topaz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_TONGA:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &tonga_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tif (encode)\n\t\t\t*codecs = &polaris_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tcase CHIP_FIJI:\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tif (encode)\n\t\t\t*codecs = &tonga_video_codecs_encode;\n\t\telse\n\t\t\t*codecs = &cz_video_codecs_decode;\n\t\treturn 0;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n \nstatic u32 vi_pcie_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tr = RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_pcie_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->pcie_idx_lock, flags);\n\tWREG32_NO_KIQ(mmPCIE_INDEX, reg);\n\t(void)RREG32_NO_KIQ(mmPCIE_INDEX);\n\tWREG32_NO_KIQ(mmPCIE_DATA, v);\n\t(void)RREG32_NO_KIQ(mmPCIE_DATA);\n\tspin_unlock_irqrestore(&adev->pcie_idx_lock, flags);\n}\n\nstatic u32 vi_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tr = RREG32_NO_KIQ(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32_NO_KIQ(mmSMC_IND_INDEX_11, (reg));\n\tWREG32_NO_KIQ(mmSMC_IND_DATA_11, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\n \n#define mmMP0PUB_IND_INDEX                                                      0x180\n#define mmMP0PUB_IND_DATA                                                       0x181\n\nstatic u32 cz_smc_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tr = RREG32(mmMP0PUB_IND_DATA);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\treturn r;\n}\n\nstatic void cz_smc_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\tWREG32(mmMP0PUB_IND_INDEX, (reg));\n\tWREG32(mmMP0PUB_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n}\n\nstatic u32 vi_uvd_ctx_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tr = RREG32(mmUVD_CTX_DATA);\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_uvd_ctx_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->uvd_ctx_idx_lock, flags);\n\tWREG32(mmUVD_CTX_INDEX, ((reg) & 0x1ff));\n\tWREG32(mmUVD_CTX_DATA, (v));\n\tspin_unlock_irqrestore(&adev->uvd_ctx_idx_lock, flags);\n}\n\nstatic u32 vi_didt_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tr = RREG32(mmDIDT_IND_DATA);\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_didt_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->didt_idx_lock, flags);\n\tWREG32(mmDIDT_IND_INDEX, (reg));\n\tWREG32(mmDIDT_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->didt_idx_lock, flags);\n}\n\nstatic u32 vi_gc_cac_rreg(struct amdgpu_device *adev, u32 reg)\n{\n\tunsigned long flags;\n\tu32 r;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tr = RREG32(mmGC_CAC_IND_DATA);\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n\treturn r;\n}\n\nstatic void vi_gc_cac_wreg(struct amdgpu_device *adev, u32 reg, u32 v)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&adev->gc_cac_idx_lock, flags);\n\tWREG32(mmGC_CAC_IND_INDEX, (reg));\n\tWREG32(mmGC_CAC_IND_DATA, (v));\n\tspin_unlock_irqrestore(&adev->gc_cac_idx_lock, flags);\n}\n\n\nstatic const u32 tonga_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 fiji_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, 0xC060000C,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 iceland_mgcg_cgcg_init[] =\n{\n\tmmPCIE_INDEX, 0xffffffff, ixPCIE_CNTL2,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmSMC_IND_INDEX_4, 0xffffffff, ixCGTT_ROM_CLK_CTRL0,\n\tmmSMC_IND_DATA_4, 0xc0000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 cz_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00600100,\n\tmmPCIE_INDEX, 0xffffffff, 0x0140001c,\n\tmmPCIE_DATA, 0x000f0000, 0x00000000,\n\tmmCGTT_DRM_CLK_CTRL0, 0xff000fff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xc0000fff, 0x00000104,\n};\n\nstatic const u32 stoney_mgcg_cgcg_init[] =\n{\n\tmmCGTT_DRM_CLK_CTRL0, 0xffffffff, 0x00000100,\n\tmmHDP_XDP_CGTT_BLK_CTRL, 0xffffffff, 0x00000104,\n\tmmHDP_HOST_PATH_CNTL, 0xffffffff, 0x0f000027,\n};\n\nstatic void vi_init_golden_registers(struct amdgpu_device *adev)\n{\n\t \n\tmutex_lock(&adev->grbm_idx_mutex);\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\txgpu_vi_init_golden_registers(adev);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn;\n\t}\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ticeland_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(iceland_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tfiji_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(fiji_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\ttonga_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(tonga_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tcz_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(cz_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_program_register_sequence(adev,\n\t\t\t\t\t\t\tstoney_mgcg_cgcg_init,\n\t\t\t\t\t\t\tARRAY_SIZE(stoney_mgcg_cgcg_init));\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\tdefault:\n\t\tbreak;\n\t}\n\tmutex_unlock(&adev->grbm_idx_mutex);\n}\n\n \nstatic u32 vi_get_xclk(struct amdgpu_device *adev)\n{\n\tu32 reference_clock = adev->clock.spll.reference_freq;\n\tu32 tmp;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_STONEY:\n\t\t\t \n\t\t\treturn 10000;\n\t\tdefault:\n\t\t\treturn reference_clock;\n\t\t}\n\t}\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL_2, MUX_TCLK_TO_XCLK))\n\t\treturn 1000;\n\n\ttmp = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\tif (REG_GET_FIELD(tmp, CG_CLKPIN_CNTL, XTALIN_DIVIDE))\n\t\treturn reference_clock / 4;\n\n\treturn reference_clock;\n}\n\n \nvoid vi_srbm_select(struct amdgpu_device *adev,\n\t\t     u32 me, u32 pipe, u32 queue, u32 vmid)\n{\n\tu32 srbm_gfx_cntl = 0;\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, PIPEID, pipe);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, MEID, me);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, VMID, vmid);\n\tsrbm_gfx_cntl = REG_SET_FIELD(srbm_gfx_cntl, SRBM_GFX_CNTL, QUEUEID, queue);\n\tWREG32(mmSRBM_GFX_CNTL, srbm_gfx_cntl);\n}\n\nstatic bool vi_read_disabled_bios(struct amdgpu_device *adev)\n{\n\tu32 bus_cntl;\n\tu32 d1vga_control = 0;\n\tu32 d2vga_control = 0;\n\tu32 vga_render_control = 0;\n\tu32 rom_cntl;\n\tbool r;\n\n\tbus_cntl = RREG32(mmBUS_CNTL);\n\tif (adev->mode_info.num_crtc) {\n\t\td1vga_control = RREG32(mmD1VGA_CONTROL);\n\t\td2vga_control = RREG32(mmD2VGA_CONTROL);\n\t\tvga_render_control = RREG32(mmVGA_RENDER_CONTROL);\n\t}\n\trom_cntl = RREG32_SMC(ixROM_CNTL);\n\n\t \n\tWREG32(mmBUS_CNTL, (bus_cntl & ~BUS_CNTL__BIOS_ROM_DIS_MASK));\n\tif (adev->mode_info.num_crtc) {\n\t\t \n\t\tWREG32(mmD1VGA_CONTROL,\n\t\t       (d1vga_control & ~(D1VGA_CONTROL__D1VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D1VGA_CONTROL__D1VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmD2VGA_CONTROL,\n\t\t       (d2vga_control & ~(D2VGA_CONTROL__D2VGA_MODE_ENABLE_MASK |\n\t\t\t\t\t  D2VGA_CONTROL__D2VGA_TIMING_SELECT_MASK)));\n\t\tWREG32(mmVGA_RENDER_CONTROL,\n\t\t       (vga_render_control & ~VGA_RENDER_CONTROL__VGA_VSTATUS_CNTL_MASK));\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl | ROM_CNTL__SCK_OVERWRITE_MASK);\n\n\tr = amdgpu_read_bios(adev);\n\n\t \n\tWREG32(mmBUS_CNTL, bus_cntl);\n\tif (adev->mode_info.num_crtc) {\n\t\tWREG32(mmD1VGA_CONTROL, d1vga_control);\n\t\tWREG32(mmD2VGA_CONTROL, d2vga_control);\n\t\tWREG32(mmVGA_RENDER_CONTROL, vga_render_control);\n\t}\n\tWREG32_SMC(ixROM_CNTL, rom_cntl);\n\treturn r;\n}\n\nstatic bool vi_read_bios_from_rom(struct amdgpu_device *adev,\n\t\t\t\t  u8 *bios, u32 length_bytes)\n{\n\tu32 *dw_ptr;\n\tunsigned long flags;\n\tu32 i, length_dw;\n\n\tif (bios == NULL)\n\t\treturn false;\n\tif (length_bytes == 0)\n\t\treturn false;\n\t \n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\tdw_ptr = (u32 *)bios;\n\tlength_dw = ALIGN(length_bytes, 4) / 4;\n\t \n\tspin_lock_irqsave(&adev->smc_idx_lock, flags);\n\t \n\tWREG32(mmSMC_IND_INDEX_11, ixROM_INDEX);\n\tWREG32(mmSMC_IND_DATA_11, 0);\n\t \n\tWREG32(mmSMC_IND_INDEX_11, ixROM_DATA);\n\tfor (i = 0; i < length_dw; i++)\n\t\tdw_ptr[i] = RREG32(mmSMC_IND_DATA_11);\n\tspin_unlock_irqrestore(&adev->smc_idx_lock, flags);\n\n\treturn true;\n}\n\nstatic const struct amdgpu_allowed_register_entry vi_allowed_read_registers[] = {\n\t{mmGRBM_STATUS},\n\t{mmGRBM_STATUS2},\n\t{mmGRBM_STATUS_SE0},\n\t{mmGRBM_STATUS_SE1},\n\t{mmGRBM_STATUS_SE2},\n\t{mmGRBM_STATUS_SE3},\n\t{mmSRBM_STATUS},\n\t{mmSRBM_STATUS2},\n\t{mmSRBM_STATUS3},\n\t{mmSDMA0_STATUS_REG + SDMA0_REGISTER_OFFSET},\n\t{mmSDMA0_STATUS_REG + SDMA1_REGISTER_OFFSET},\n\t{mmCP_STAT},\n\t{mmCP_STALLED_STAT1},\n\t{mmCP_STALLED_STAT2},\n\t{mmCP_STALLED_STAT3},\n\t{mmCP_CPF_BUSY_STAT},\n\t{mmCP_CPF_STALLED_STAT1},\n\t{mmCP_CPF_STATUS},\n\t{mmCP_CPC_BUSY_STAT},\n\t{mmCP_CPC_STALLED_STAT1},\n\t{mmCP_CPC_STATUS},\n\t{mmGB_ADDR_CONFIG},\n\t{mmMC_ARB_RAMCFG},\n\t{mmGB_TILE_MODE0},\n\t{mmGB_TILE_MODE1},\n\t{mmGB_TILE_MODE2},\n\t{mmGB_TILE_MODE3},\n\t{mmGB_TILE_MODE4},\n\t{mmGB_TILE_MODE5},\n\t{mmGB_TILE_MODE6},\n\t{mmGB_TILE_MODE7},\n\t{mmGB_TILE_MODE8},\n\t{mmGB_TILE_MODE9},\n\t{mmGB_TILE_MODE10},\n\t{mmGB_TILE_MODE11},\n\t{mmGB_TILE_MODE12},\n\t{mmGB_TILE_MODE13},\n\t{mmGB_TILE_MODE14},\n\t{mmGB_TILE_MODE15},\n\t{mmGB_TILE_MODE16},\n\t{mmGB_TILE_MODE17},\n\t{mmGB_TILE_MODE18},\n\t{mmGB_TILE_MODE19},\n\t{mmGB_TILE_MODE20},\n\t{mmGB_TILE_MODE21},\n\t{mmGB_TILE_MODE22},\n\t{mmGB_TILE_MODE23},\n\t{mmGB_TILE_MODE24},\n\t{mmGB_TILE_MODE25},\n\t{mmGB_TILE_MODE26},\n\t{mmGB_TILE_MODE27},\n\t{mmGB_TILE_MODE28},\n\t{mmGB_TILE_MODE29},\n\t{mmGB_TILE_MODE30},\n\t{mmGB_TILE_MODE31},\n\t{mmGB_MACROTILE_MODE0},\n\t{mmGB_MACROTILE_MODE1},\n\t{mmGB_MACROTILE_MODE2},\n\t{mmGB_MACROTILE_MODE3},\n\t{mmGB_MACROTILE_MODE4},\n\t{mmGB_MACROTILE_MODE5},\n\t{mmGB_MACROTILE_MODE6},\n\t{mmGB_MACROTILE_MODE7},\n\t{mmGB_MACROTILE_MODE8},\n\t{mmGB_MACROTILE_MODE9},\n\t{mmGB_MACROTILE_MODE10},\n\t{mmGB_MACROTILE_MODE11},\n\t{mmGB_MACROTILE_MODE12},\n\t{mmGB_MACROTILE_MODE13},\n\t{mmGB_MACROTILE_MODE14},\n\t{mmGB_MACROTILE_MODE15},\n\t{mmCC_RB_BACKEND_DISABLE, true},\n\t{mmGC_USER_RB_BACKEND_DISABLE, true},\n\t{mmGB_BACKEND_MAP, false},\n\t{mmPA_SC_RASTER_CONFIG, true},\n\t{mmPA_SC_RASTER_CONFIG_1, true},\n};\n\nstatic uint32_t vi_get_register_value(struct amdgpu_device *adev,\n\t\t\t\t      bool indexed, u32 se_num,\n\t\t\t\t      u32 sh_num, u32 reg_offset)\n{\n\tif (indexed) {\n\t\tuint32_t val;\n\t\tunsigned se_idx = (se_num == 0xffffffff) ? 0 : se_num;\n\t\tunsigned sh_idx = (sh_num == 0xffffffff) ? 0 : sh_num;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmCC_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].rb_backend_disable;\n\t\tcase mmGC_USER_RB_BACKEND_DISABLE:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].user_rb_backend_disable;\n\t\tcase mmPA_SC_RASTER_CONFIG:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config;\n\t\tcase mmPA_SC_RASTER_CONFIG_1:\n\t\t\treturn adev->gfx.config.rb_config[se_idx][sh_idx].raster_config_1;\n\t\t}\n\n\t\tmutex_lock(&adev->grbm_idx_mutex);\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, se_num, sh_num, 0xffffffff, 0);\n\n\t\tval = RREG32(reg_offset);\n\n\t\tif (se_num != 0xffffffff || sh_num != 0xffffffff)\n\t\t\tamdgpu_gfx_select_se_sh(adev, 0xffffffff, 0xffffffff, 0xffffffff, 0);\n\t\tmutex_unlock(&adev->grbm_idx_mutex);\n\t\treturn val;\n\t} else {\n\t\tunsigned idx;\n\n\t\tswitch (reg_offset) {\n\t\tcase mmGB_ADDR_CONFIG:\n\t\t\treturn adev->gfx.config.gb_addr_config;\n\t\tcase mmMC_ARB_RAMCFG:\n\t\t\treturn adev->gfx.config.mc_arb_ramcfg;\n\t\tcase mmGB_TILE_MODE0:\n\t\tcase mmGB_TILE_MODE1:\n\t\tcase mmGB_TILE_MODE2:\n\t\tcase mmGB_TILE_MODE3:\n\t\tcase mmGB_TILE_MODE4:\n\t\tcase mmGB_TILE_MODE5:\n\t\tcase mmGB_TILE_MODE6:\n\t\tcase mmGB_TILE_MODE7:\n\t\tcase mmGB_TILE_MODE8:\n\t\tcase mmGB_TILE_MODE9:\n\t\tcase mmGB_TILE_MODE10:\n\t\tcase mmGB_TILE_MODE11:\n\t\tcase mmGB_TILE_MODE12:\n\t\tcase mmGB_TILE_MODE13:\n\t\tcase mmGB_TILE_MODE14:\n\t\tcase mmGB_TILE_MODE15:\n\t\tcase mmGB_TILE_MODE16:\n\t\tcase mmGB_TILE_MODE17:\n\t\tcase mmGB_TILE_MODE18:\n\t\tcase mmGB_TILE_MODE19:\n\t\tcase mmGB_TILE_MODE20:\n\t\tcase mmGB_TILE_MODE21:\n\t\tcase mmGB_TILE_MODE22:\n\t\tcase mmGB_TILE_MODE23:\n\t\tcase mmGB_TILE_MODE24:\n\t\tcase mmGB_TILE_MODE25:\n\t\tcase mmGB_TILE_MODE26:\n\t\tcase mmGB_TILE_MODE27:\n\t\tcase mmGB_TILE_MODE28:\n\t\tcase mmGB_TILE_MODE29:\n\t\tcase mmGB_TILE_MODE30:\n\t\tcase mmGB_TILE_MODE31:\n\t\t\tidx = (reg_offset - mmGB_TILE_MODE0);\n\t\t\treturn adev->gfx.config.tile_mode_array[idx];\n\t\tcase mmGB_MACROTILE_MODE0:\n\t\tcase mmGB_MACROTILE_MODE1:\n\t\tcase mmGB_MACROTILE_MODE2:\n\t\tcase mmGB_MACROTILE_MODE3:\n\t\tcase mmGB_MACROTILE_MODE4:\n\t\tcase mmGB_MACROTILE_MODE5:\n\t\tcase mmGB_MACROTILE_MODE6:\n\t\tcase mmGB_MACROTILE_MODE7:\n\t\tcase mmGB_MACROTILE_MODE8:\n\t\tcase mmGB_MACROTILE_MODE9:\n\t\tcase mmGB_MACROTILE_MODE10:\n\t\tcase mmGB_MACROTILE_MODE11:\n\t\tcase mmGB_MACROTILE_MODE12:\n\t\tcase mmGB_MACROTILE_MODE13:\n\t\tcase mmGB_MACROTILE_MODE14:\n\t\tcase mmGB_MACROTILE_MODE15:\n\t\t\tidx = (reg_offset - mmGB_MACROTILE_MODE0);\n\t\t\treturn adev->gfx.config.macrotile_mode_array[idx];\n\t\tdefault:\n\t\t\treturn RREG32(reg_offset);\n\t\t}\n\t}\n}\n\nstatic int vi_read_register(struct amdgpu_device *adev, u32 se_num,\n\t\t\t    u32 sh_num, u32 reg_offset, u32 *value)\n{\n\tuint32_t i;\n\n\t*value = 0;\n\tfor (i = 0; i < ARRAY_SIZE(vi_allowed_read_registers); i++) {\n\t\tbool indexed = vi_allowed_read_registers[i].grbm_indexed;\n\n\t\tif (reg_offset != vi_allowed_read_registers[i].reg_offset)\n\t\t\tcontinue;\n\n\t\t*value = vi_get_register_value(adev, indexed, se_num, sh_num,\n\t\t\t\t\t       reg_offset);\n\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\n \nstatic int vi_asic_pci_config_reset(struct amdgpu_device *adev)\n{\n\tu32 i;\n\tint r = -EINVAL;\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, true);\n\n\t \n\tpci_clear_master(adev->pdev);\n\t \n\tamdgpu_device_pci_config_reset(adev);\n\n\tudelay(100);\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (RREG32(mmCONFIG_MEMSIZE) != 0xffffffff) {\n\t\t\t \n\t\t\tpci_set_master(adev->pdev);\n\t\t\tadev->has_hw_reset = true;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t\tudelay(1);\n\t}\n\n\tamdgpu_atombios_scratch_regs_engine_hung(adev, false);\n\n\treturn r;\n}\n\nstatic bool vi_asic_supports_baco(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\treturn amdgpu_dpm_is_baco_supported(adev);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic enum amd_reset_method\nvi_asic_reset_method(struct amdgpu_device *adev)\n{\n\tbool baco_reset;\n\n\tif (amdgpu_reset_method == AMD_RESET_METHOD_LEGACY ||\n\t    amdgpu_reset_method == AMD_RESET_METHOD_BACO)\n\t\treturn amdgpu_reset_method;\n\n\tif (amdgpu_reset_method != -1)\n\t\tdev_warn(adev->dev, \"Specified reset method:%d isn't supported, using AUTO instead.\\n\",\n\t\t\t\t  amdgpu_reset_method);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\t\tbaco_reset = amdgpu_dpm_is_baco_supported(adev);\n\t\tbreak;\n\tdefault:\n\t\tbaco_reset = false;\n\t\tbreak;\n\t}\n\n\tif (baco_reset)\n\t\treturn AMD_RESET_METHOD_BACO;\n\telse\n\t\treturn AMD_RESET_METHOD_LEGACY;\n}\n\n \nstatic int vi_asic_reset(struct amdgpu_device *adev)\n{\n\tint r;\n\n\t \n\tif (adev->flags & AMD_IS_APU)\n\t\treturn 0;\n\n\tif (vi_asic_reset_method(adev) == AMD_RESET_METHOD_BACO) {\n\t\tdev_info(adev->dev, \"BACO reset\\n\");\n\t\tr = amdgpu_dpm_baco_reset(adev);\n\t} else {\n\t\tdev_info(adev->dev, \"PCI CONFIG reset\\n\");\n\t\tr = vi_asic_pci_config_reset(adev);\n\t}\n\n\treturn r;\n}\n\nstatic u32 vi_get_config_memsize(struct amdgpu_device *adev)\n{\n\treturn RREG32(mmCONFIG_MEMSIZE);\n}\n\nstatic int vi_set_uvd_clock(struct amdgpu_device *adev, u32 clock,\n\t\t\tu32 cntl_reg, u32 status_reg)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tuint32_t tmp;\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       clock, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\ttmp = RREG32_SMC(cntl_reg);\n\n\tif (adev->flags & AMD_IS_APU)\n\t\ttmp &= ~CG_DCLK_CNTL__DCLK_DIVIDER_MASK;\n\telse\n\t\ttmp &= ~(CG_DCLK_CNTL__DCLK_DIR_CNTL_EN_MASK |\n\t\t\t\tCG_DCLK_CNTL__DCLK_DIVIDER_MASK);\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(cntl_reg, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\ttmp = RREG32_SMC(status_reg);\n\t\tif (adev->flags & AMD_IS_APU) {\n\t\t\tif (tmp & 0x10000)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (tmp & CG_DCLK_STATUS__DCLK_STATUS_MASK)\n\t\t\t\tbreak;\n\t\t}\n\t\tmdelay(10);\n\t}\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\treturn 0;\n}\n\n#define ixGNB_CLK1_DFS_CNTL 0xD82200F0\n#define ixGNB_CLK1_STATUS   0xD822010C\n#define ixGNB_CLK2_DFS_CNTL 0xD8220110\n#define ixGNB_CLK2_STATUS   0xD822012C\n#define ixGNB_CLK3_DFS_CNTL 0xD8220130\n#define ixGNB_CLK3_STATUS   0xD822014C\n\nstatic int vi_set_uvd_clocks(struct amdgpu_device *adev, u32 vclk, u32 dclk)\n{\n\tint r;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixGNB_CLK2_DFS_CNTL, ixGNB_CLK2_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixGNB_CLK1_DFS_CNTL, ixGNB_CLK1_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t} else {\n\t\tr = vi_set_uvd_clock(adev, vclk, ixCG_VCLK_CNTL, ixCG_VCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = vi_set_uvd_clock(adev, dclk, ixCG_DCLK_CNTL, ixCG_DCLK_STATUS);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_set_vce_clocks(struct amdgpu_device *adev, u32 evclk, u32 ecclk)\n{\n\tint r, i;\n\tstruct atom_clock_dividers dividers;\n\tu32 tmp;\n\tu32 reg_ctrl;\n\tu32 reg_status;\n\tu32 status_mask;\n\tu32 reg_mask;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\treg_ctrl = ixGNB_CLK3_DFS_CNTL;\n\t\treg_status = ixGNB_CLK3_STATUS;\n\t\tstatus_mask = 0x00010000;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t} else {\n\t\treg_ctrl = ixCG_ECLK_CNTL;\n\t\treg_status = ixCG_ECLK_STATUS;\n\t\tstatus_mask = CG_ECLK_STATUS__ECLK_STATUS_MASK;\n\t\treg_mask = CG_ECLK_CNTL__ECLK_DIR_CNTL_EN_MASK | CG_ECLK_CNTL__ECLK_DIVIDER_MASK;\n\t}\n\n\tr = amdgpu_atombios_get_clock_dividers(adev,\n\t\t\t\t\t       COMPUTE_GPUCLK_INPUT_FLAG_DEFAULT_GPUCLK,\n\t\t\t\t\t       ecclk, false, &dividers);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\ttmp = RREG32_SMC(reg_ctrl);\n\ttmp &= ~reg_mask;\n\ttmp |= dividers.post_divider;\n\tWREG32_SMC(reg_ctrl, tmp);\n\n\tfor (i = 0; i < 100; i++) {\n\t\tif (RREG32_SMC(reg_status) & status_mask)\n\t\t\tbreak;\n\t\tmdelay(10);\n\t}\n\n\tif (i == 100)\n\t\treturn -ETIMEDOUT;\n\n\treturn 0;\n}\n\nstatic void vi_enable_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, orig;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata |= PCIE_LC_CNTL__LC_L0S_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L0S_INACTIVITY__SHIFT;\n\tdata |= PCIE_LC_CNTL__LC_L1_INACTIVITY_DEFAULT <<\n\t\t\tPCIE_LC_CNTL__LC_L1_INACTIVITY__SHIFT;\n\tdata &= ~PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tdata |= PCIE_LC_CNTL__LC_DELAY_L1_EXIT_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n}\n\nstatic void vi_program_aspm(struct amdgpu_device *adev)\n{\n\tu32 data, data1, orig;\n\tbool bL1SS = false;\n\tbool bClkReqSupport = true;\n\n\tif (!amdgpu_device_should_use_aspm(adev) || !amdgpu_device_pcie_dynamic_switching_supported())\n\t\treturn;\n\n\tif (adev->flags & AMD_IS_APU ||\n\t    adev->asic_type < CHIP_POLARIS10)\n\t\treturn;\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\tdata &= ~PCIE_LC_CNTL__LC_L1_INACTIVITY_MASK;\n\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\tdata |= PCIE_LC_CNTL__LC_PMI_TO_L1_DIS_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata &= ~PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_MASK;\n\tdata |= 0x0024 << PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS__SHIFT;\n\tdata |= PCIE_LC_N_FTS_CNTL__LC_XMIT_N_FTS_OVERRIDE_EN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_N_FTS_CNTL, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL3);\n\tdata |= PCIE_LC_CNTL3__LC_GO_TO_RECOVERY_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL3, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_P_CNTL);\n\tdata |= PCIE_P_CNTL__P_IGNORE_EDB_ERR_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_P_CNTL, data);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_L1_PM_SUBSTATE);\n\tpci_read_config_dword(adev->pdev, PCIE_L1_PM_SUB_CNTL, &data1);\n\tif (data & PCIE_LC_L1_PM_SUBSTATE__LC_L1_SUBSTATES_OVERRIDE_EN_MASK &&\n\t    (data & (PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_2_OVERRIDE_MASK |\n\t\t    PCIE_LC_L1_PM_SUBSTATE__LC_PCI_PM_L1_1_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_2_OVERRIDE_MASK |\n\t\t\tPCIE_LC_L1_PM_SUBSTATE__LC_ASPM_L1_1_OVERRIDE_MASK))) {\n\t\tbL1SS = true;\n\t} else if (data1 & (PCIE_L1_PM_SUB_CNTL__ASPM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__ASPM_L1_1_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_2_EN_MASK |\n\t    PCIE_L1_PM_SUB_CNTL__PCI_PM_L1_1_EN_MASK)) {\n\t\tbL1SS = true;\n\t}\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL6);\n\tdata |= PCIE_LC_CNTL6__LC_L1_POWERDOWN_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_CNTL6, data);\n\n\torig = data = RREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL);\n\tdata |= PCIE_LC_LINK_WIDTH_CNTL__LC_DYN_LANES_PWR_STATE_MASK;\n\tif (orig != data)\n\t\tWREG32_PCIE(ixPCIE_LC_LINK_WIDTH_CNTL, data);\n\n\tpci_read_config_dword(adev->pdev, LINK_CAP, &data);\n\tif (!(data & PCIE_LINK_CAP__CLOCK_POWER_MANAGEMENT_MASK))\n\t\tbClkReqSupport = false;\n\n\tif (bClkReqSupport) {\n\t\torig = data = RREG32_SMC(ixTHM_CLK_CNTL);\n\t\tdata &= ~(THM_CLK_CNTL__CMON_CLK_SEL_MASK | THM_CLK_CNTL__TMON_CLK_SEL_MASK);\n\t\tdata |= (1 << THM_CLK_CNTL__CMON_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << THM_CLK_CNTL__TMON_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixTHM_CLK_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMISC_CLK_CTRL);\n\t\tdata &= ~(MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL_MASK |\n\t\t\tMISC_CLK_CTRL__ZCLK_SEL_MASK | MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL_MASK);\n\t\tdata |= (1 << MISC_CLK_CTRL__DEEP_SLEEP_CLK_SEL__SHIFT) |\n\t\t\t\t(1 << MISC_CLK_CTRL__ZCLK_SEL__SHIFT);\n\t\tdata |= (0x20 << MISC_CLK_CTRL__DFT_SMS_PG_CLK_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMISC_CLK_CTRL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL);\n\t\tdata |= CG_CLKPIN_CNTL__XTALIN_DIVIDE_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixCG_CLKPIN_CNTL_2);\n\t\tdata |= CG_CLKPIN_CNTL_2__ENABLE_XCLK_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixCG_CLKPIN_CNTL, data);\n\n\t\torig = data = RREG32_SMC(ixMPLL_BYPASSCLK_SEL);\n\t\tdata &= ~MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL_MASK;\n\t\tdata |= (4 << MPLL_BYPASSCLK_SEL__MPLL_CLKOUT_SEL__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_SMC(ixMPLL_BYPASSCLK_SEL, data);\n\n\t\torig = data = RREG32_PCIE(ixCPM_CONTROL);\n\t\tdata |= (CPM_CONTROL__REFCLK_XSTCLK_ENABLE_MASK |\n\t\t\t\tCPM_CONTROL__CLKREQb_UNGATE_TXCLK_ENABLE_MASK);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixCPM_CONTROL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_CONFIG_CNTL);\n\t\tdata &= ~PCIE_CONFIG_CNTL__DYN_CLK_LATENCY_MASK;\n\t\tdata |= (0xE << PCIE_CONFIG_CNTL__DYN_CLK_LATENCY__SHIFT);\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_CONFIG_CNTL, data);\n\n\t\torig = data = RREG32(mmBIF_CLK_CTRL);\n\t\tdata |= BIF_CLK_CTRL__BIF_XSTCLK_READY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32(mmBIF_CLK_CTRL, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL7);\n\t\tdata |= PCIE_LC_CNTL7__LC_L1_SIDEBAND_CLKREQ_PDWN_EN_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL7, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_HW_DEBUG);\n\t\tdata |= PCIE_HW_DEBUG__HW_01_DEBUG_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_HW_DEBUG, data);\n\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL2);\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L23_MASK;\n\t\tdata |= PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (bL1SS)\n\t\t\tdata &= ~PCIE_LC_CNTL2__LC_ALLOW_PDWN_IN_L1_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL2, data);\n\n\t}\n\n\tvi_enable_aspm(adev);\n\n\tdata = RREG32_PCIE(ixPCIE_LC_N_FTS_CNTL);\n\tdata1 = RREG32_PCIE(ixPCIE_LC_STATUS1);\n\tif (((data & PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) == PCIE_LC_N_FTS_CNTL__LC_N_FTS_MASK) &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_XMIT_MASK &&\n\t    data1 & PCIE_LC_STATUS1__LC_REVERSE_RCVR_MASK) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_CNTL);\n\t\tdata &= ~PCIE_LC_CNTL__LC_L0S_INACTIVITY_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_CNTL, data);\n\t}\n\n\tif ((adev->asic_type == CHIP_POLARIS12 &&\n\t    !(ASICID_IS_P23(adev->pdev->device, adev->pdev->revision))) ||\n\t    ASIC_IS_P22(adev->asic_type, adev->external_rev_id)) {\n\t\torig = data = RREG32_PCIE(ixPCIE_LC_TRAINING_CNTL);\n\t\tdata &= ~PCIE_LC_TRAINING_CNTL__LC_DISABLE_TRAINING_BIT_ARCH_MASK;\n\t\tif (orig != data)\n\t\t\tWREG32_PCIE(ixPCIE_LC_TRAINING_CNTL, data);\n\t}\n}\n\nstatic void vi_enable_doorbell_aperture(struct amdgpu_device *adev,\n\t\t\t\t\tbool enable)\n{\n\tu32 tmp;\n\n\t \n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\ttmp = RREG32(mmBIF_DOORBELL_APER_EN);\n\tif (enable)\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 1);\n\telse\n\t\ttmp = REG_SET_FIELD(tmp, BIF_DOORBELL_APER_EN, BIF_DOORBELL_APER_EN, 0);\n\n\tWREG32(mmBIF_DOORBELL_APER_EN, tmp);\n}\n\n#define ATI_REV_ID_FUSE_MACRO__ADDRESS      0xC0014044\n#define ATI_REV_ID_FUSE_MACRO__SHIFT        9\n#define ATI_REV_ID_FUSE_MACRO__MASK         0x00001E00\n\nstatic uint32_t vi_get_rev_id(struct amdgpu_device *adev)\n{\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn (RREG32_SMC(ATI_REV_ID_FUSE_MACRO__ADDRESS) & ATI_REV_ID_FUSE_MACRO__MASK)\n\t\t\t>> ATI_REV_ID_FUSE_MACRO__SHIFT;\n\telse\n\t\treturn (RREG32(mmPCIE_EFUSE4) & PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID_MASK)\n\t\t\t>> PCIE_EFUSE4__STRAP_BIF_ATI_REV_ID__SHIFT;\n}\n\nstatic void vi_flush_hdp(struct amdgpu_device *adev, struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t\tRREG32(mmHDP_MEM_COHERENCY_FLUSH_CNTL);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_MEM_COHERENCY_FLUSH_CNTL, 1);\n\t}\n}\n\nstatic void vi_invalidate_hdp(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ring *ring)\n{\n\tif (!ring || !ring->funcs->emit_wreg) {\n\t\tWREG32(mmHDP_DEBUG0, 1);\n\t\tRREG32(mmHDP_DEBUG0);\n\t} else {\n\t\tamdgpu_ring_emit_wreg(ring, mmHDP_DEBUG0, 1);\n\t}\n}\n\nstatic bool vi_need_full_reset(struct amdgpu_device *adev)\n{\n\tswitch (adev->asic_type) {\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\t \n\t\treturn false;\n\tcase CHIP_FIJI:\n\tcase CHIP_TONGA:\n\t\t \n\t\treturn true;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_TOPAZ:\n\tdefault:\n\t\t \n\t\treturn true;\n\t}\n}\n\nstatic void vi_get_pcie_usage(struct amdgpu_device *adev, uint64_t *count0,\n\t\t\t      uint64_t *count1)\n{\n\tuint32_t perfctr = 0;\n\tuint64_t cnt0_of, cnt1_of;\n\tint tmp;\n\n\t \n\tif (adev->flags & AMD_IS_APU)\n\t\treturn;\n\n\t \n\t \n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT0_SEL, 40);\n\tperfctr = REG_SET_FIELD(perfctr, PCIE_PERF_CNTL_TXCLK, EVENT1_SEL, 104);\n\n\t \n\tWREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK, perfctr);\n\t \n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000005);\n\n\tmsleep(1000);\n\n\t \n\tWREG32_PCIE(ixPCIE_PERF_COUNT_CNTL, 0x00000002);\n\n\t \n\ttmp = RREG32_PCIE(ixPCIE_PERF_CNTL_TXCLK);\n\tcnt0_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER0_UPPER);\n\tcnt1_of = REG_GET_FIELD(tmp, PCIE_PERF_CNTL_TXCLK, COUNTER1_UPPER);\n\n\t \n\t*count0 = RREG32_PCIE(ixPCIE_PERF_COUNT0_TXCLK) | (cnt0_of << 32);\n\t*count1 = RREG32_PCIE(ixPCIE_PERF_COUNT1_TXCLK) | (cnt1_of << 32);\n}\n\nstatic uint64_t vi_get_pcie_replay_count(struct amdgpu_device *adev)\n{\n\tuint64_t nak_r, nak_g;\n\n\t \n\tnak_r = RREG32_PCIE(ixPCIE_RX_NUM_NAK);\n\tnak_g = RREG32_PCIE(ixPCIE_RX_NUM_NAK_GENERATED);\n\n\t \n\treturn (nak_r + nak_g);\n}\n\nstatic bool vi_need_reset_on_init(struct amdgpu_device *adev)\n{\n\tu32 clock_cntl, pc;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\treturn false;\n\n\t \n\tclock_cntl = RREG32_SMC(ixSMC_SYSCON_CLOCK_CNTL_0);\n\tpc = RREG32_SMC(ixSMC_PC_C);\n\tif ((0 == REG_GET_FIELD(clock_cntl, SMC_SYSCON_CLOCK_CNTL_0, ck_disable)) &&\n\t    (0x20100 <= pc))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void vi_pre_asic_init(struct amdgpu_device *adev)\n{\n}\n\nstatic const struct amdgpu_asic_funcs vi_asic_funcs =\n{\n\t.read_disabled_bios = &vi_read_disabled_bios,\n\t.read_bios_from_rom = &vi_read_bios_from_rom,\n\t.read_register = &vi_read_register,\n\t.reset = &vi_asic_reset,\n\t.reset_method = &vi_asic_reset_method,\n\t.get_xclk = &vi_get_xclk,\n\t.set_uvd_clocks = &vi_set_uvd_clocks,\n\t.set_vce_clocks = &vi_set_vce_clocks,\n\t.get_config_memsize = &vi_get_config_memsize,\n\t.flush_hdp = &vi_flush_hdp,\n\t.invalidate_hdp = &vi_invalidate_hdp,\n\t.need_full_reset = &vi_need_full_reset,\n\t.init_doorbell_index = &legacy_doorbell_index_init,\n\t.get_pcie_usage = &vi_get_pcie_usage,\n\t.need_reset_on_init = &vi_need_reset_on_init,\n\t.get_pcie_replay_count = &vi_get_pcie_replay_count,\n\t.supports_baco = &vi_asic_supports_baco,\n\t.pre_asic_init = &vi_pre_asic_init,\n\t.query_video_codecs = &vi_query_video_codecs,\n};\n\n#define CZ_REV_BRISTOL(rev)\t \\\n\t((rev >= 0xC8 && rev <= 0xCE) || (rev >= 0xE1 && rev <= 0xE6))\n\nstatic int vi_common_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tadev->smc_rreg = &cz_smc_rreg;\n\t\tadev->smc_wreg = &cz_smc_wreg;\n\t} else {\n\t\tadev->smc_rreg = &vi_smc_rreg;\n\t\tadev->smc_wreg = &vi_smc_wreg;\n\t}\n\tadev->pcie_rreg = &vi_pcie_rreg;\n\tadev->pcie_wreg = &vi_pcie_wreg;\n\tadev->uvd_ctx_rreg = &vi_uvd_ctx_rreg;\n\tadev->uvd_ctx_wreg = &vi_uvd_ctx_wreg;\n\tadev->didt_rreg = &vi_didt_rreg;\n\tadev->didt_wreg = &vi_didt_wreg;\n\tadev->gc_cac_rreg = &vi_gc_cac_rreg;\n\tadev->gc_cac_wreg = &vi_gc_cac_wreg;\n\n\tadev->asic_funcs = &vi_asic_funcs;\n\n\tadev->rev_id = vi_get_rev_id(adev);\n\tadev->external_rev_id = 0xFF;\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\tadev->cg_flags = 0;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = 0x1;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x3c;\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x14;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x5A;\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x50;\n\t\tbreak;\n\tcase CHIP_POLARIS12:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_3D_CGLS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_BIF_MGCG |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_ROM_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_MGCG |\n\t\t\tAMD_CG_SUPPORT_MC_LS |\n\t\t\tAMD_CG_SUPPORT_DRM_LS |\n\t\t\tAMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x64;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tadev->cg_flags = 0;\n\t\t\t \n\t\tadev->pg_flags = 0;\n\t\tadev->external_rev_id = adev->rev_id + 0x6E;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\t \n\t\tadev->pg_flags = 0;\n\t\tif (adev->rev_id != 0x00 || CZ_REV_BRISTOL(adev->pdev->revision)) {\n\t\t\tadev->pg_flags |= AMD_PG_SUPPORT_GFX_SMG |\n\t\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\t\tAMD_PG_SUPPORT_VCE;\n\t\t}\n\t\tadev->external_rev_id = adev->rev_id + 0x1;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tadev->cg_flags = AMD_CG_SUPPORT_UVD_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGCG |\n\t\t\tAMD_CG_SUPPORT_GFX_MGLS |\n\t\t\tAMD_CG_SUPPORT_GFX_RLC_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CP_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGTS_LS |\n\t\t\tAMD_CG_SUPPORT_GFX_CGLS |\n\t\t\tAMD_CG_SUPPORT_BIF_LS |\n\t\t\tAMD_CG_SUPPORT_HDP_MGCG |\n\t\t\tAMD_CG_SUPPORT_HDP_LS |\n\t\t\tAMD_CG_SUPPORT_SDMA_MGCG |\n\t\t\tAMD_CG_SUPPORT_SDMA_LS |\n\t\t\tAMD_CG_SUPPORT_VCE_MGCG;\n\t\tadev->pg_flags = AMD_PG_SUPPORT_GFX_PG |\n\t\t\tAMD_PG_SUPPORT_GFX_SMG |\n\t\t\tAMD_PG_SUPPORT_GFX_PIPELINE |\n\t\t\tAMD_PG_SUPPORT_CP |\n\t\t\tAMD_PG_SUPPORT_UVD |\n\t\t\tAMD_PG_SUPPORT_VCE;\n\t\tadev->external_rev_id = adev->rev_id + 0x61;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tamdgpu_virt_init_setting(adev);\n\t\txgpu_vi_mailbox_set_irq_funcs(adev);\n\t}\n\n\treturn 0;\n}\n\nstatic int vi_common_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_get_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_add_irq_id(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_sw_fini(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tvi_init_golden_registers(adev);\n\t \n\tvi_program_aspm(adev);\n\t \n\tvi_enable_doorbell_aperture(adev, true);\n\n\treturn 0;\n}\n\nstatic int vi_common_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tvi_enable_doorbell_aperture(adev, false);\n\n\tif (amdgpu_sriov_vf(adev))\n\t\txgpu_vi_mailbox_put_irq(adev);\n\n\treturn 0;\n}\n\nstatic int vi_common_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_fini(adev);\n}\n\nstatic int vi_common_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn vi_common_hw_init(adev);\n}\n\nstatic bool vi_common_is_idle(void *handle)\n{\n\treturn true;\n}\n\nstatic int vi_common_wait_for_idle(void *handle)\n{\n\treturn 0;\n}\n\nstatic int vi_common_soft_reset(void *handle)\n{\n\treturn 0;\n}\n\nstatic void vi_update_bif_medium_grain_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t\t\t   bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_PCIE(ixPCIE_CNTL2);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS))\n\t\tdata |= PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK;\n\telse\n\t\tdata &= ~(PCIE_CNTL2__SLV_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__MST_MEM_LS_EN_MASK |\n\t\t\t\tPCIE_CNTL2__REPLAY_MEM_LS_EN_MASK);\n\n\tif (temp != data)\n\t\tWREG32_PCIE(ixPCIE_CNTL2, data);\n}\n\nstatic void vi_update_hdp_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_HOST_PATH_CNTL);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG))\n\t\tdata &= ~HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\telse\n\t\tdata |= HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_HOST_PATH_CNTL, data);\n}\n\nstatic void vi_update_hdp_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(mmHDP_MEM_POWER_LS);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS))\n\t\tdata |= HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\telse\n\t\tdata &= ~HDP_MEM_POWER_LS__LS_ENABLE_MASK;\n\n\tif (temp != data)\n\t\tWREG32(mmHDP_MEM_POWER_LS, data);\n}\n\nstatic void vi_update_drm_light_sleep(struct amdgpu_device *adev,\n\t\t\t\t      bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32(0x157a);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS))\n\t\tdata |= 1;\n\telse\n\t\tdata &= ~1;\n\n\tif (temp != data)\n\t\tWREG32(0x157a, data);\n}\n\n\nstatic void vi_update_rom_medium_grain_clock_gating(struct amdgpu_device *adev,\n\t\t\t\t\t\t    bool enable)\n{\n\tuint32_t temp, data;\n\n\ttemp = data = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG))\n\t\tdata &= ~(CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK);\n\telse\n\t\tdata |= CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK |\n\t\t\t\tCGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE1_MASK;\n\n\tif (temp != data)\n\t\tWREG32_SMC(ixCGTT_ROM_CLK_CTRL0, data);\n}\n\nstatic int vi_common_set_clockgating_state_by_smu(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tuint32_t msg_id, pp_state = 0;\n\tuint32_t pp_support_state = 0;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_MC_LS | AMD_CG_SUPPORT_MC_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_MC_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_MC,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_SDMA_LS | AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_SDMA,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & (AMD_CG_SUPPORT_HDP_LS | AMD_CG_SUPPORT_HDP_MGCG)) {\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_LS) {\n\t\t\tpp_support_state = PP_STATE_SUPPORT_LS;\n\t\t\tpp_state = PP_STATE_LS;\n\t\t}\n\t\tif (adev->cg_flags & AMD_CG_SUPPORT_HDP_MGCG) {\n\t\t\tpp_support_state |= PP_STATE_SUPPORT_CG;\n\t\t\tpp_state |= PP_STATE_CG;\n\t\t}\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_HDP,\n\t\t\t       pp_support_state,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_LS) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t        pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\tif (adev->cg_flags & AMD_CG_SUPPORT_BIF_MGCG) {\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_BIF,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_DRM_LS) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_LS;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_DRM,\n\t\t\t       PP_STATE_SUPPORT_LS,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\n\tif (adev->cg_flags & AMD_CG_SUPPORT_ROM_MGCG) {\n\n\t\tif (state == AMD_CG_STATE_UNGATE)\n\t\t\tpp_state = 0;\n\t\telse\n\t\t\tpp_state = PP_STATE_CG;\n\n\t\tmsg_id = PP_CG_MSG_ID(PP_GROUP_SYS,\n\t\t\t       PP_BLOCK_SYS_ROM,\n\t\t\t       PP_STATE_SUPPORT_CG,\n\t\t\t       pp_state);\n\t\tamdgpu_dpm_set_clockgating_by_smu(adev, msg_id);\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_clockgating_state(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_FIJI:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_rom_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\tcase CHIP_STONEY:\n\t\tvi_update_bif_medium_grain_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_medium_grain_clock_gating(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_hdp_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tvi_update_drm_light_sleep(adev,\n\t\t\t\tstate == AMD_CG_STATE_GATE);\n\t\tbreak;\n\tcase CHIP_TONGA:\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tvi_common_set_clockgating_state_by_smu(adev, state);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int vi_common_set_powergating_state(void *handle,\n\t\t\t\t\t    enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic void vi_common_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t \n\tdata = RREG32_PCIE(ixPCIE_CNTL2);\n\tif (data & PCIE_CNTL2__SLV_MEM_LS_EN_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_BIF_LS;\n\n\t \n\tdata = RREG32(mmHDP_MEM_POWER_LS);\n\tif (data & HDP_MEM_POWER_LS__LS_ENABLE_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_HDP_LS;\n\n\t \n\tdata = RREG32(mmHDP_HOST_PATH_CNTL);\n\tif (!(data & HDP_HOST_PATH_CNTL__CLOCK_GATING_DIS_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_HDP_MGCG;\n\n\t \n\tdata = RREG32_SMC(ixCGTT_ROM_CLK_CTRL0);\n\tif (!(data & CGTT_ROM_CLK_CTRL0__SOFT_OVERRIDE0_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_ROM_MGCG;\n}\n\nstatic const struct amd_ip_funcs vi_common_ip_funcs = {\n\t.name = \"vi_common\",\n\t.early_init = vi_common_early_init,\n\t.late_init = vi_common_late_init,\n\t.sw_init = vi_common_sw_init,\n\t.sw_fini = vi_common_sw_fini,\n\t.hw_init = vi_common_hw_init,\n\t.hw_fini = vi_common_hw_fini,\n\t.suspend = vi_common_suspend,\n\t.resume = vi_common_resume,\n\t.is_idle = vi_common_is_idle,\n\t.wait_for_idle = vi_common_wait_for_idle,\n\t.soft_reset = vi_common_soft_reset,\n\t.set_clockgating_state = vi_common_set_clockgating_state,\n\t.set_powergating_state = vi_common_set_powergating_state,\n\t.get_clockgating_state = vi_common_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ip_block_version vi_common_ip_block =\n{\n\t.type = AMD_IP_BLOCK_TYPE_COMMON,\n\t.major = 1,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &vi_common_ip_funcs,\n};\n\nvoid vi_set_virt_ops(struct amdgpu_device *adev)\n{\n\tadev->virt.ops = &xgpu_vi_virt_ops;\n}\n\nint vi_set_ip_blocks(struct amdgpu_device *adev)\n{\n\tamdgpu_device_set_sriov_virtual_display(adev);\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TOPAZ:\n\t\t \n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v7_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &iceland_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v2_4_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_5_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_1_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_TONGA:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v10_0_ip_block);\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\tamdgpu_device_ip_block_add(adev, &uvd_v5_0_ip_block);\n\t\t\tamdgpu_device_ip_block_add(adev, &vce_v3_0_ip_block);\n\t\t}\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\tcase CHIP_VEGAM:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &tonga_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_3_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_1_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tamdgpu_device_ip_block_add(adev, &vi_common_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gmc_v8_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &cz_ih_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &gfx_v8_1_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &sdma_v3_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &pp_smu_ip_block);\n\t\tif (adev->enable_virtual_display)\n\t\t\tamdgpu_device_ip_block_add(adev, &amdgpu_vkms_ip_block);\n#if defined(CONFIG_DRM_AMD_DC)\n\t\telse if (amdgpu_device_has_dc_support(adev))\n\t\t\tamdgpu_device_ip_block_add(adev, &dm_ip_block);\n#endif\n\t\telse\n\t\t\tamdgpu_device_ip_block_add(adev, &dce_v11_0_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &uvd_v6_2_ip_block);\n\t\tamdgpu_device_ip_block_add(adev, &vce_v3_4_ip_block);\n#if defined(CONFIG_DRM_AMD_ACP)\n\t\tamdgpu_device_ip_block_add(adev, &acp_ip_block);\n#endif\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid legacy_doorbell_index_init(struct amdgpu_device *adev)\n{\n\tadev->doorbell_index.kiq = AMDGPU_DOORBELL_KIQ;\n\tadev->doorbell_index.mec_ring0 = AMDGPU_DOORBELL_MEC_RING0;\n\tadev->doorbell_index.mec_ring1 = AMDGPU_DOORBELL_MEC_RING1;\n\tadev->doorbell_index.mec_ring2 = AMDGPU_DOORBELL_MEC_RING2;\n\tadev->doorbell_index.mec_ring3 = AMDGPU_DOORBELL_MEC_RING3;\n\tadev->doorbell_index.mec_ring4 = AMDGPU_DOORBELL_MEC_RING4;\n\tadev->doorbell_index.mec_ring5 = AMDGPU_DOORBELL_MEC_RING5;\n\tadev->doorbell_index.mec_ring6 = AMDGPU_DOORBELL_MEC_RING6;\n\tadev->doorbell_index.mec_ring7 = AMDGPU_DOORBELL_MEC_RING7;\n\tadev->doorbell_index.gfx_ring0 = AMDGPU_DOORBELL_GFX_RING0;\n\tadev->doorbell_index.sdma_engine[0] = AMDGPU_DOORBELL_sDMA_ENGINE0;\n\tadev->doorbell_index.sdma_engine[1] = AMDGPU_DOORBELL_sDMA_ENGINE1;\n\tadev->doorbell_index.ih = AMDGPU_DOORBELL_IH;\n\tadev->doorbell_index.max_assignment = AMDGPU_DOORBELL_MAX_ASSIGNMENT;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}