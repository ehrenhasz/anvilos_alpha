{
  "module_name": "amdgpu_ctx.c",
  "hash_id": "4ebc9b4e6174c03f006878ce560145785e997f9845b8304b4aaf62fea4ac0002",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ctx.c",
  "human_readable_source": " \n\n#include <drm/drm_auth.h>\n#include <drm/drm_drv.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_sched.h\"\n#include \"amdgpu_ras.h\"\n#include <linux/nospec.h>\n\n#define to_amdgpu_ctx_entity(e)\t\\\n\tcontainer_of((e), struct amdgpu_ctx_entity, entity)\n\nconst unsigned int amdgpu_ctx_num_entities[AMDGPU_HW_IP_NUM] = {\n\t[AMDGPU_HW_IP_GFX]\t=\t1,\n\t[AMDGPU_HW_IP_COMPUTE]\t=\t4,\n\t[AMDGPU_HW_IP_DMA]\t=\t2,\n\t[AMDGPU_HW_IP_UVD]\t=\t1,\n\t[AMDGPU_HW_IP_VCE]\t=\t1,\n\t[AMDGPU_HW_IP_UVD_ENC]\t=\t1,\n\t[AMDGPU_HW_IP_VCN_DEC]\t=\t1,\n\t[AMDGPU_HW_IP_VCN_ENC]\t=\t1,\n\t[AMDGPU_HW_IP_VCN_JPEG]\t=\t1,\n};\n\nbool amdgpu_ctx_priority_is_valid(int32_t ctx_prio)\n{\n\tswitch (ctx_prio) {\n\tcase AMDGPU_CTX_PRIORITY_VERY_LOW:\n\tcase AMDGPU_CTX_PRIORITY_LOW:\n\tcase AMDGPU_CTX_PRIORITY_NORMAL:\n\tcase AMDGPU_CTX_PRIORITY_HIGH:\n\tcase AMDGPU_CTX_PRIORITY_VERY_HIGH:\n\t\treturn true;\n\tdefault:\n\tcase AMDGPU_CTX_PRIORITY_UNSET:\n\t\t \n\t\treturn false;\n\t}\n}\n\nstatic enum drm_sched_priority\namdgpu_ctx_to_drm_sched_prio(int32_t ctx_prio)\n{\n\tswitch (ctx_prio) {\n\tcase AMDGPU_CTX_PRIORITY_UNSET:\n\t\tpr_warn_once(\"AMD-->DRM context priority value UNSET-->NORMAL\");\n\t\treturn DRM_SCHED_PRIORITY_NORMAL;\n\n\tcase AMDGPU_CTX_PRIORITY_VERY_LOW:\n\t\treturn DRM_SCHED_PRIORITY_MIN;\n\n\tcase AMDGPU_CTX_PRIORITY_LOW:\n\t\treturn DRM_SCHED_PRIORITY_MIN;\n\n\tcase AMDGPU_CTX_PRIORITY_NORMAL:\n\t\treturn DRM_SCHED_PRIORITY_NORMAL;\n\n\tcase AMDGPU_CTX_PRIORITY_HIGH:\n\t\treturn DRM_SCHED_PRIORITY_HIGH;\n\n\tcase AMDGPU_CTX_PRIORITY_VERY_HIGH:\n\t\treturn DRM_SCHED_PRIORITY_HIGH;\n\n\t \n\tdefault:\n\t\tWARN(1, \"Invalid context priority %d\\n\", ctx_prio);\n\t\treturn DRM_SCHED_PRIORITY_NORMAL;\n\t}\n\n}\n\nstatic int amdgpu_ctx_priority_permit(struct drm_file *filp,\n\t\t\t\t      int32_t priority)\n{\n\t \n\tif (priority <= AMDGPU_CTX_PRIORITY_NORMAL)\n\t\treturn 0;\n\n\tif (capable(CAP_SYS_NICE))\n\t\treturn 0;\n\n\tif (drm_is_current_master(filp))\n\t\treturn 0;\n\n\treturn -EACCES;\n}\n\nstatic enum amdgpu_gfx_pipe_priority amdgpu_ctx_prio_to_gfx_pipe_prio(int32_t prio)\n{\n\tswitch (prio) {\n\tcase AMDGPU_CTX_PRIORITY_HIGH:\n\tcase AMDGPU_CTX_PRIORITY_VERY_HIGH:\n\t\treturn AMDGPU_GFX_PIPE_PRIO_HIGH;\n\tdefault:\n\t\treturn AMDGPU_GFX_PIPE_PRIO_NORMAL;\n\t}\n}\n\nstatic enum amdgpu_ring_priority_level amdgpu_ctx_sched_prio_to_ring_prio(int32_t prio)\n{\n\tswitch (prio) {\n\tcase AMDGPU_CTX_PRIORITY_HIGH:\n\t\treturn AMDGPU_RING_PRIO_1;\n\tcase AMDGPU_CTX_PRIORITY_VERY_HIGH:\n\t\treturn AMDGPU_RING_PRIO_2;\n\tdefault:\n\t\treturn AMDGPU_RING_PRIO_0;\n\t}\n}\n\nstatic unsigned int amdgpu_ctx_get_hw_prio(struct amdgpu_ctx *ctx, u32 hw_ip)\n{\n\tstruct amdgpu_device *adev = ctx->mgr->adev;\n\tunsigned int hw_prio;\n\tint32_t ctx_prio;\n\n\tctx_prio = (ctx->override_priority == AMDGPU_CTX_PRIORITY_UNSET) ?\n\t\t\tctx->init_priority : ctx->override_priority;\n\n\tswitch (hw_ip) {\n\tcase AMDGPU_HW_IP_GFX:\n\tcase AMDGPU_HW_IP_COMPUTE:\n\t\thw_prio = amdgpu_ctx_prio_to_gfx_pipe_prio(ctx_prio);\n\t\tbreak;\n\tcase AMDGPU_HW_IP_VCE:\n\tcase AMDGPU_HW_IP_VCN_ENC:\n\t\thw_prio = amdgpu_ctx_sched_prio_to_ring_prio(ctx_prio);\n\t\tbreak;\n\tdefault:\n\t\thw_prio = AMDGPU_RING_PRIO_DEFAULT;\n\t\tbreak;\n\t}\n\n\thw_ip = array_index_nospec(hw_ip, AMDGPU_HW_IP_NUM);\n\tif (adev->gpu_sched[hw_ip][hw_prio].num_scheds == 0)\n\t\thw_prio = AMDGPU_RING_PRIO_DEFAULT;\n\n\treturn hw_prio;\n}\n\n \nstatic ktime_t amdgpu_ctx_fence_time(struct dma_fence *fence)\n{\n\tstruct drm_sched_fence *s_fence;\n\n\tif (!fence)\n\t\treturn ns_to_ktime(0);\n\n\t \n\ts_fence = to_drm_sched_fence(fence);\n\tif (!test_bit(DMA_FENCE_FLAG_TIMESTAMP_BIT, &s_fence->scheduled.flags))\n\t\treturn ns_to_ktime(0);\n\n\t \n\tif (!test_bit(DMA_FENCE_FLAG_TIMESTAMP_BIT, &s_fence->finished.flags))\n\t\treturn ktime_sub(ktime_get(), s_fence->scheduled.timestamp);\n\n\treturn ktime_sub(s_fence->finished.timestamp,\n\t\t\t s_fence->scheduled.timestamp);\n}\n\nstatic ktime_t amdgpu_ctx_entity_time(struct amdgpu_ctx *ctx,\n\t\t\t\t      struct amdgpu_ctx_entity *centity)\n{\n\tktime_t res = ns_to_ktime(0);\n\tuint32_t i;\n\n\tspin_lock(&ctx->ring_lock);\n\tfor (i = 0; i < amdgpu_sched_jobs; i++) {\n\t\tres = ktime_add(res, amdgpu_ctx_fence_time(centity->fences[i]));\n\t}\n\tspin_unlock(&ctx->ring_lock);\n\treturn res;\n}\n\nstatic int amdgpu_ctx_init_entity(struct amdgpu_ctx *ctx, u32 hw_ip,\n\t\t\t\t  const u32 ring)\n{\n\tstruct drm_gpu_scheduler **scheds = NULL, *sched = NULL;\n\tstruct amdgpu_device *adev = ctx->mgr->adev;\n\tstruct amdgpu_ctx_entity *entity;\n\tenum drm_sched_priority drm_prio;\n\tunsigned int hw_prio, num_scheds;\n\tint32_t ctx_prio;\n\tint r;\n\n\tentity = kzalloc(struct_size(entity, fences, amdgpu_sched_jobs),\n\t\t\t GFP_KERNEL);\n\tif (!entity)\n\t\treturn  -ENOMEM;\n\n\tctx_prio = (ctx->override_priority == AMDGPU_CTX_PRIORITY_UNSET) ?\n\t\t\tctx->init_priority : ctx->override_priority;\n\tentity->hw_ip = hw_ip;\n\tentity->sequence = 1;\n\thw_prio = amdgpu_ctx_get_hw_prio(ctx, hw_ip);\n\tdrm_prio = amdgpu_ctx_to_drm_sched_prio(ctx_prio);\n\n\thw_ip = array_index_nospec(hw_ip, AMDGPU_HW_IP_NUM);\n\n\tif (!(adev)->xcp_mgr) {\n\t\tscheds = adev->gpu_sched[hw_ip][hw_prio].sched;\n\t\tnum_scheds = adev->gpu_sched[hw_ip][hw_prio].num_scheds;\n\t} else {\n\t\tstruct amdgpu_fpriv *fpriv;\n\n\t\tfpriv = container_of(ctx->ctx_mgr, struct amdgpu_fpriv, ctx_mgr);\n\t\tr = amdgpu_xcp_select_scheds(adev, hw_ip, hw_prio, fpriv,\n\t\t\t\t\t\t&num_scheds, &scheds);\n\t\tif (r)\n\t\t\tgoto cleanup_entity;\n\t}\n\n\t \n\tif (hw_ip == AMDGPU_HW_IP_VCN_ENC ||\n\t    hw_ip == AMDGPU_HW_IP_VCN_DEC ||\n\t    hw_ip == AMDGPU_HW_IP_UVD_ENC ||\n\t    hw_ip == AMDGPU_HW_IP_UVD) {\n\t\tsched = drm_sched_pick_best(scheds, num_scheds);\n\t\tscheds = &sched;\n\t\tnum_scheds = 1;\n\t}\n\n\tr = drm_sched_entity_init(&entity->entity, drm_prio, scheds, num_scheds,\n\t\t\t\t  &ctx->guilty);\n\tif (r)\n\t\tgoto error_free_entity;\n\n\t \n\tif (cmpxchg(&ctx->entities[hw_ip][ring], NULL, entity))\n\t\tgoto cleanup_entity;\n\n\treturn 0;\n\ncleanup_entity:\n\tdrm_sched_entity_fini(&entity->entity);\n\nerror_free_entity:\n\tkfree(entity);\n\n\treturn r;\n}\n\nstatic ktime_t amdgpu_ctx_fini_entity(struct amdgpu_device *adev,\n\t\t\t\t  struct amdgpu_ctx_entity *entity)\n{\n\tktime_t res = ns_to_ktime(0);\n\tint i;\n\n\tif (!entity)\n\t\treturn res;\n\n\tfor (i = 0; i < amdgpu_sched_jobs; ++i) {\n\t\tres = ktime_add(res, amdgpu_ctx_fence_time(entity->fences[i]));\n\t\tdma_fence_put(entity->fences[i]);\n\t}\n\n\tamdgpu_xcp_release_sched(adev, entity);\n\n\tkfree(entity);\n\treturn res;\n}\n\nstatic int amdgpu_ctx_get_stable_pstate(struct amdgpu_ctx *ctx,\n\t\t\t\t\tu32 *stable_pstate)\n{\n\tstruct amdgpu_device *adev = ctx->mgr->adev;\n\tenum amd_dpm_forced_level current_level;\n\n\tcurrent_level = amdgpu_dpm_get_performance_level(adev);\n\n\tswitch (current_level) {\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD:\n\t\t*stable_pstate = AMDGPU_CTX_STABLE_PSTATE_STANDARD;\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK:\n\t\t*stable_pstate = AMDGPU_CTX_STABLE_PSTATE_MIN_SCLK;\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_MIN_MCLK:\n\t\t*stable_pstate = AMDGPU_CTX_STABLE_PSTATE_MIN_MCLK;\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_PEAK:\n\t\t*stable_pstate = AMDGPU_CTX_STABLE_PSTATE_PEAK;\n\t\tbreak;\n\tdefault:\n\t\t*stable_pstate = AMDGPU_CTX_STABLE_PSTATE_NONE;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int amdgpu_ctx_init(struct amdgpu_ctx_mgr *mgr, int32_t priority,\n\t\t\t   struct drm_file *filp, struct amdgpu_ctx *ctx)\n{\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\tu32 current_stable_pstate;\n\tint r;\n\n\tr = amdgpu_ctx_priority_permit(filp, priority);\n\tif (r)\n\t\treturn r;\n\n\tmemset(ctx, 0, sizeof(*ctx));\n\n\tkref_init(&ctx->refcount);\n\tctx->mgr = mgr;\n\tspin_lock_init(&ctx->ring_lock);\n\n\tctx->reset_counter = atomic_read(&mgr->adev->gpu_reset_counter);\n\tctx->reset_counter_query = ctx->reset_counter;\n\tctx->generation = amdgpu_vm_generation(mgr->adev, &fpriv->vm);\n\tctx->init_priority = priority;\n\tctx->override_priority = AMDGPU_CTX_PRIORITY_UNSET;\n\n\tr = amdgpu_ctx_get_stable_pstate(ctx, &current_stable_pstate);\n\tif (r)\n\t\treturn r;\n\n\tif (mgr->adev->pm.stable_pstate_ctx)\n\t\tctx->stable_pstate = mgr->adev->pm.stable_pstate_ctx->stable_pstate;\n\telse\n\t\tctx->stable_pstate = current_stable_pstate;\n\n\tctx->ctx_mgr = &(fpriv->ctx_mgr);\n\treturn 0;\n}\n\nstatic int amdgpu_ctx_set_stable_pstate(struct amdgpu_ctx *ctx,\n\t\t\t\t\tu32 stable_pstate)\n{\n\tstruct amdgpu_device *adev = ctx->mgr->adev;\n\tenum amd_dpm_forced_level level;\n\tu32 current_stable_pstate;\n\tint r;\n\n\tmutex_lock(&adev->pm.stable_pstate_ctx_lock);\n\tif (adev->pm.stable_pstate_ctx && adev->pm.stable_pstate_ctx != ctx) {\n\t\tr = -EBUSY;\n\t\tgoto done;\n\t}\n\n\tr = amdgpu_ctx_get_stable_pstate(ctx, &current_stable_pstate);\n\tif (r || (stable_pstate == current_stable_pstate))\n\t\tgoto done;\n\n\tswitch (stable_pstate) {\n\tcase AMDGPU_CTX_STABLE_PSTATE_NONE:\n\t\tlevel = AMD_DPM_FORCED_LEVEL_AUTO;\n\t\tbreak;\n\tcase AMDGPU_CTX_STABLE_PSTATE_STANDARD:\n\t\tlevel = AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD;\n\t\tbreak;\n\tcase AMDGPU_CTX_STABLE_PSTATE_MIN_SCLK:\n\t\tlevel = AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK;\n\t\tbreak;\n\tcase AMDGPU_CTX_STABLE_PSTATE_MIN_MCLK:\n\t\tlevel = AMD_DPM_FORCED_LEVEL_PROFILE_MIN_MCLK;\n\t\tbreak;\n\tcase AMDGPU_CTX_STABLE_PSTATE_PEAK:\n\t\tlevel = AMD_DPM_FORCED_LEVEL_PROFILE_PEAK;\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tr = amdgpu_dpm_force_performance_level(adev, level);\n\n\tif (level == AMD_DPM_FORCED_LEVEL_AUTO)\n\t\tadev->pm.stable_pstate_ctx = NULL;\n\telse\n\t\tadev->pm.stable_pstate_ctx = ctx;\ndone:\n\tmutex_unlock(&adev->pm.stable_pstate_ctx_lock);\n\n\treturn r;\n}\n\nstatic void amdgpu_ctx_fini(struct kref *ref)\n{\n\tstruct amdgpu_ctx *ctx = container_of(ref, struct amdgpu_ctx, refcount);\n\tstruct amdgpu_ctx_mgr *mgr = ctx->mgr;\n\tstruct amdgpu_device *adev = mgr->adev;\n\tunsigned i, j, idx;\n\n\tif (!adev)\n\t\treturn;\n\n\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i) {\n\t\tfor (j = 0; j < AMDGPU_MAX_ENTITY_NUM; ++j) {\n\t\t\tktime_t spend;\n\n\t\t\tspend = amdgpu_ctx_fini_entity(adev, ctx->entities[i][j]);\n\t\t\tatomic64_add(ktime_to_ns(spend), &mgr->time_spend[i]);\n\t\t}\n\t}\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tamdgpu_ctx_set_stable_pstate(ctx, ctx->stable_pstate);\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tkfree(ctx);\n}\n\nint amdgpu_ctx_get_entity(struct amdgpu_ctx *ctx, u32 hw_ip, u32 instance,\n\t\t\t  u32 ring, struct drm_sched_entity **entity)\n{\n\tint r;\n\tstruct drm_sched_entity *ctx_entity;\n\n\tif (hw_ip >= AMDGPU_HW_IP_NUM) {\n\t\tDRM_ERROR(\"unknown HW IP type: %d\\n\", hw_ip);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (instance != 0) {\n\t\tDRM_DEBUG(\"invalid ip instance: %d\\n\", instance);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ring >= amdgpu_ctx_num_entities[hw_ip]) {\n\t\tDRM_DEBUG(\"invalid ring: %d %d\\n\", hw_ip, ring);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ctx->entities[hw_ip][ring] == NULL) {\n\t\tr = amdgpu_ctx_init_entity(ctx, hw_ip, ring);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tctx_entity = &ctx->entities[hw_ip][ring]->entity;\n\tr = drm_sched_entity_error(ctx_entity);\n\tif (r) {\n\t\tDRM_DEBUG(\"error entity %p\\n\", ctx_entity);\n\t\treturn r;\n\t}\n\n\t*entity = ctx_entity;\n\treturn 0;\n}\n\nstatic int amdgpu_ctx_alloc(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_fpriv *fpriv,\n\t\t\t    struct drm_file *filp,\n\t\t\t    int32_t priority,\n\t\t\t    uint32_t *id)\n{\n\tstruct amdgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;\n\tstruct amdgpu_ctx *ctx;\n\tint r;\n\n\tctx = kmalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&mgr->lock);\n\tr = idr_alloc(&mgr->ctx_handles, ctx, 1, AMDGPU_VM_MAX_NUM_CTX, GFP_KERNEL);\n\tif (r < 0) {\n\t\tmutex_unlock(&mgr->lock);\n\t\tkfree(ctx);\n\t\treturn r;\n\t}\n\n\t*id = (uint32_t)r;\n\tr = amdgpu_ctx_init(mgr, priority, filp, ctx);\n\tif (r) {\n\t\tidr_remove(&mgr->ctx_handles, *id);\n\t\t*id = 0;\n\t\tkfree(ctx);\n\t}\n\tmutex_unlock(&mgr->lock);\n\treturn r;\n}\n\nstatic void amdgpu_ctx_do_release(struct kref *ref)\n{\n\tstruct amdgpu_ctx *ctx;\n\tu32 i, j;\n\n\tctx = container_of(ref, struct amdgpu_ctx, refcount);\n\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i) {\n\t\tfor (j = 0; j < amdgpu_ctx_num_entities[i]; ++j) {\n\t\t\tif (!ctx->entities[i][j])\n\t\t\t\tcontinue;\n\n\t\t\tdrm_sched_entity_destroy(&ctx->entities[i][j]->entity);\n\t\t}\n\t}\n\n\tamdgpu_ctx_fini(ref);\n}\n\nstatic int amdgpu_ctx_free(struct amdgpu_fpriv *fpriv, uint32_t id)\n{\n\tstruct amdgpu_ctx_mgr *mgr = &fpriv->ctx_mgr;\n\tstruct amdgpu_ctx *ctx;\n\n\tmutex_lock(&mgr->lock);\n\tctx = idr_remove(&mgr->ctx_handles, id);\n\tif (ctx)\n\t\tkref_put(&ctx->refcount, amdgpu_ctx_do_release);\n\tmutex_unlock(&mgr->lock);\n\treturn ctx ? 0 : -EINVAL;\n}\n\nstatic int amdgpu_ctx_query(struct amdgpu_device *adev,\n\t\t\t    struct amdgpu_fpriv *fpriv, uint32_t id,\n\t\t\t    union drm_amdgpu_ctx_out *out)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct amdgpu_ctx_mgr *mgr;\n\tunsigned reset_counter;\n\n\tif (!fpriv)\n\t\treturn -EINVAL;\n\n\tmgr = &fpriv->ctx_mgr;\n\tmutex_lock(&mgr->lock);\n\tctx = idr_find(&mgr->ctx_handles, id);\n\tif (!ctx) {\n\t\tmutex_unlock(&mgr->lock);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tout->state.flags = 0x0;\n\tout->state.hangs = 0x0;\n\n\t \n\treset_counter = atomic_read(&adev->gpu_reset_counter);\n\t \n\tif (ctx->reset_counter_query == reset_counter)\n\t\tout->state.reset_status = AMDGPU_CTX_NO_RESET;\n\telse\n\t\tout->state.reset_status = AMDGPU_CTX_UNKNOWN_RESET;\n\tctx->reset_counter_query = reset_counter;\n\n\tmutex_unlock(&mgr->lock);\n\treturn 0;\n}\n\n#define AMDGPU_RAS_COUNTE_DELAY_MS 3000\n\nstatic int amdgpu_ctx_query2(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_fpriv *fpriv, uint32_t id,\n\t\t\t     union drm_amdgpu_ctx_out *out)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct amdgpu_ctx *ctx;\n\tstruct amdgpu_ctx_mgr *mgr;\n\n\tif (!fpriv)\n\t\treturn -EINVAL;\n\n\tmgr = &fpriv->ctx_mgr;\n\tmutex_lock(&mgr->lock);\n\tctx = idr_find(&mgr->ctx_handles, id);\n\tif (!ctx) {\n\t\tmutex_unlock(&mgr->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tout->state.flags = 0x0;\n\tout->state.hangs = 0x0;\n\n\tif (ctx->reset_counter != atomic_read(&adev->gpu_reset_counter))\n\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_RESET;\n\n\tif (ctx->generation != amdgpu_vm_generation(adev, &fpriv->vm))\n\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_VRAMLOST;\n\n\tif (atomic_read(&ctx->guilty))\n\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_GUILTY;\n\n\tif (amdgpu_in_reset(adev))\n\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_RESET_IN_PROGRESS;\n\n\tif (adev->ras_enabled && con) {\n\t\t \n\t\tint ce_count, ue_count;\n\n\t\tce_count = atomic_read(&con->ras_ce_count);\n\t\tue_count = atomic_read(&con->ras_ue_count);\n\n\t\tif (ce_count != ctx->ras_counter_ce) {\n\t\t\tctx->ras_counter_ce = ce_count;\n\t\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_RAS_CE;\n\t\t}\n\n\t\tif (ue_count != ctx->ras_counter_ue) {\n\t\t\tctx->ras_counter_ue = ue_count;\n\t\t\tout->state.flags |= AMDGPU_CTX_QUERY2_FLAGS_RAS_UE;\n\t\t}\n\n\t\tschedule_delayed_work(&con->ras_counte_delay_work,\n\t\t\t\t      msecs_to_jiffies(AMDGPU_RAS_COUNTE_DELAY_MS));\n\t}\n\n\tmutex_unlock(&mgr->lock);\n\treturn 0;\n}\n\nstatic int amdgpu_ctx_stable_pstate(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_fpriv *fpriv, uint32_t id,\n\t\t\t\t    bool set, u32 *stable_pstate)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct amdgpu_ctx_mgr *mgr;\n\tint r;\n\n\tif (!fpriv)\n\t\treturn -EINVAL;\n\n\tmgr = &fpriv->ctx_mgr;\n\tmutex_lock(&mgr->lock);\n\tctx = idr_find(&mgr->ctx_handles, id);\n\tif (!ctx) {\n\t\tmutex_unlock(&mgr->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tif (set)\n\t\tr = amdgpu_ctx_set_stable_pstate(ctx, *stable_pstate);\n\telse\n\t\tr = amdgpu_ctx_get_stable_pstate(ctx, stable_pstate);\n\n\tmutex_unlock(&mgr->lock);\n\treturn r;\n}\n\nint amdgpu_ctx_ioctl(struct drm_device *dev, void *data,\n\t\t     struct drm_file *filp)\n{\n\tint r;\n\tuint32_t id, stable_pstate;\n\tint32_t priority;\n\n\tunion drm_amdgpu_ctx *args = data;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\tid = args->in.ctx_id;\n\tpriority = args->in.priority;\n\n\t \n\tif (!amdgpu_ctx_priority_is_valid(priority))\n\t\tpriority = AMDGPU_CTX_PRIORITY_NORMAL;\n\n\tswitch (args->in.op) {\n\tcase AMDGPU_CTX_OP_ALLOC_CTX:\n\t\tr = amdgpu_ctx_alloc(adev, fpriv, filp, priority, &id);\n\t\targs->out.alloc.ctx_id = id;\n\t\tbreak;\n\tcase AMDGPU_CTX_OP_FREE_CTX:\n\t\tr = amdgpu_ctx_free(fpriv, id);\n\t\tbreak;\n\tcase AMDGPU_CTX_OP_QUERY_STATE:\n\t\tr = amdgpu_ctx_query(adev, fpriv, id, &args->out);\n\t\tbreak;\n\tcase AMDGPU_CTX_OP_QUERY_STATE2:\n\t\tr = amdgpu_ctx_query2(adev, fpriv, id, &args->out);\n\t\tbreak;\n\tcase AMDGPU_CTX_OP_GET_STABLE_PSTATE:\n\t\tif (args->in.flags)\n\t\t\treturn -EINVAL;\n\t\tr = amdgpu_ctx_stable_pstate(adev, fpriv, id, false, &stable_pstate);\n\t\tif (!r)\n\t\t\targs->out.pstate.flags = stable_pstate;\n\t\tbreak;\n\tcase AMDGPU_CTX_OP_SET_STABLE_PSTATE:\n\t\tif (args->in.flags & ~AMDGPU_CTX_STABLE_PSTATE_FLAGS_MASK)\n\t\t\treturn -EINVAL;\n\t\tstable_pstate = args->in.flags & AMDGPU_CTX_STABLE_PSTATE_FLAGS_MASK;\n\t\tif (stable_pstate > AMDGPU_CTX_STABLE_PSTATE_PEAK)\n\t\t\treturn -EINVAL;\n\t\tr = amdgpu_ctx_stable_pstate(adev, fpriv, id, true, &stable_pstate);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\nstruct amdgpu_ctx *amdgpu_ctx_get(struct amdgpu_fpriv *fpriv, uint32_t id)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct amdgpu_ctx_mgr *mgr;\n\n\tif (!fpriv)\n\t\treturn NULL;\n\n\tmgr = &fpriv->ctx_mgr;\n\n\tmutex_lock(&mgr->lock);\n\tctx = idr_find(&mgr->ctx_handles, id);\n\tif (ctx)\n\t\tkref_get(&ctx->refcount);\n\tmutex_unlock(&mgr->lock);\n\treturn ctx;\n}\n\nint amdgpu_ctx_put(struct amdgpu_ctx *ctx)\n{\n\tif (ctx == NULL)\n\t\treturn -EINVAL;\n\n\tkref_put(&ctx->refcount, amdgpu_ctx_do_release);\n\treturn 0;\n}\n\nuint64_t amdgpu_ctx_add_fence(struct amdgpu_ctx *ctx,\n\t\t\t      struct drm_sched_entity *entity,\n\t\t\t      struct dma_fence *fence)\n{\n\tstruct amdgpu_ctx_entity *centity = to_amdgpu_ctx_entity(entity);\n\tuint64_t seq = centity->sequence;\n\tstruct dma_fence *other = NULL;\n\tunsigned idx = 0;\n\n\tidx = seq & (amdgpu_sched_jobs - 1);\n\tother = centity->fences[idx];\n\tWARN_ON(other && !dma_fence_is_signaled(other));\n\n\tdma_fence_get(fence);\n\n\tspin_lock(&ctx->ring_lock);\n\tcentity->fences[idx] = fence;\n\tcentity->sequence++;\n\tspin_unlock(&ctx->ring_lock);\n\n\tatomic64_add(ktime_to_ns(amdgpu_ctx_fence_time(other)),\n\t\t     &ctx->mgr->time_spend[centity->hw_ip]);\n\n\tdma_fence_put(other);\n\treturn seq;\n}\n\nstruct dma_fence *amdgpu_ctx_get_fence(struct amdgpu_ctx *ctx,\n\t\t\t\t       struct drm_sched_entity *entity,\n\t\t\t\t       uint64_t seq)\n{\n\tstruct amdgpu_ctx_entity *centity = to_amdgpu_ctx_entity(entity);\n\tstruct dma_fence *fence;\n\n\tspin_lock(&ctx->ring_lock);\n\n\tif (seq == ~0ull)\n\t\tseq = centity->sequence - 1;\n\n\tif (seq >= centity->sequence) {\n\t\tspin_unlock(&ctx->ring_lock);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\n\tif (seq + amdgpu_sched_jobs < centity->sequence) {\n\t\tspin_unlock(&ctx->ring_lock);\n\t\treturn NULL;\n\t}\n\n\tfence = dma_fence_get(centity->fences[seq & (amdgpu_sched_jobs - 1)]);\n\tspin_unlock(&ctx->ring_lock);\n\n\treturn fence;\n}\n\nstatic void amdgpu_ctx_set_entity_priority(struct amdgpu_ctx *ctx,\n\t\t\t\t\t   struct amdgpu_ctx_entity *aentity,\n\t\t\t\t\t   int hw_ip,\n\t\t\t\t\t   int32_t priority)\n{\n\tstruct amdgpu_device *adev = ctx->mgr->adev;\n\tunsigned int hw_prio;\n\tstruct drm_gpu_scheduler **scheds = NULL;\n\tunsigned num_scheds;\n\n\t \n\tdrm_sched_entity_set_priority(&aentity->entity,\n\t\t\t\t      amdgpu_ctx_to_drm_sched_prio(priority));\n\n\t \n\tif (hw_ip == AMDGPU_HW_IP_COMPUTE || hw_ip == AMDGPU_HW_IP_GFX) {\n\t\thw_prio = amdgpu_ctx_get_hw_prio(ctx, hw_ip);\n\t\thw_prio = array_index_nospec(hw_prio, AMDGPU_RING_PRIO_MAX);\n\t\tscheds = adev->gpu_sched[hw_ip][hw_prio].sched;\n\t\tnum_scheds = adev->gpu_sched[hw_ip][hw_prio].num_scheds;\n\t\tdrm_sched_entity_modify_sched(&aentity->entity, scheds,\n\t\t\t\t\t      num_scheds);\n\t}\n}\n\nvoid amdgpu_ctx_priority_override(struct amdgpu_ctx *ctx,\n\t\t\t\t  int32_t priority)\n{\n\tint32_t ctx_prio;\n\tunsigned i, j;\n\n\tctx->override_priority = priority;\n\n\tctx_prio = (ctx->override_priority == AMDGPU_CTX_PRIORITY_UNSET) ?\n\t\t\tctx->init_priority : ctx->override_priority;\n\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i) {\n\t\tfor (j = 0; j < amdgpu_ctx_num_entities[i]; ++j) {\n\t\t\tif (!ctx->entities[i][j])\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_ctx_set_entity_priority(ctx, ctx->entities[i][j],\n\t\t\t\t\t\t       i, ctx_prio);\n\t\t}\n\t}\n}\n\nint amdgpu_ctx_wait_prev_fence(struct amdgpu_ctx *ctx,\n\t\t\t       struct drm_sched_entity *entity)\n{\n\tstruct amdgpu_ctx_entity *centity = to_amdgpu_ctx_entity(entity);\n\tstruct dma_fence *other;\n\tunsigned idx;\n\tlong r;\n\n\tspin_lock(&ctx->ring_lock);\n\tidx = centity->sequence & (amdgpu_sched_jobs - 1);\n\tother = dma_fence_get(centity->fences[idx]);\n\tspin_unlock(&ctx->ring_lock);\n\n\tif (!other)\n\t\treturn 0;\n\n\tr = dma_fence_wait(other, true);\n\tif (r < 0 && r != -ERESTARTSYS)\n\t\tDRM_ERROR(\"Error (%ld) waiting for fence!\\n\", r);\n\n\tdma_fence_put(other);\n\treturn r;\n}\n\nvoid amdgpu_ctx_mgr_init(struct amdgpu_ctx_mgr *mgr,\n\t\t\t struct amdgpu_device *adev)\n{\n\tunsigned int i;\n\n\tmgr->adev = adev;\n\tmutex_init(&mgr->lock);\n\tidr_init_base(&mgr->ctx_handles, 1);\n\n\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i)\n\t\tatomic64_set(&mgr->time_spend[i], 0);\n}\n\nlong amdgpu_ctx_mgr_entity_flush(struct amdgpu_ctx_mgr *mgr, long timeout)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct idr *idp;\n\tuint32_t id, i, j;\n\n\tidp = &mgr->ctx_handles;\n\n\tmutex_lock(&mgr->lock);\n\tidr_for_each_entry(idp, ctx, id) {\n\t\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i) {\n\t\t\tfor (j = 0; j < amdgpu_ctx_num_entities[i]; ++j) {\n\t\t\t\tstruct drm_sched_entity *entity;\n\n\t\t\t\tif (!ctx->entities[i][j])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tentity = &ctx->entities[i][j]->entity;\n\t\t\t\ttimeout = drm_sched_entity_flush(entity, timeout);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&mgr->lock);\n\treturn timeout;\n}\n\nvoid amdgpu_ctx_mgr_entity_fini(struct amdgpu_ctx_mgr *mgr)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct idr *idp;\n\tuint32_t id, i, j;\n\n\tidp = &mgr->ctx_handles;\n\n\tidr_for_each_entry(idp, ctx, id) {\n\t\tif (kref_read(&ctx->refcount) != 1) {\n\t\t\tDRM_ERROR(\"ctx %p is still alive\\n\", ctx);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (i = 0; i < AMDGPU_HW_IP_NUM; ++i) {\n\t\t\tfor (j = 0; j < amdgpu_ctx_num_entities[i]; ++j) {\n\t\t\t\tstruct drm_sched_entity *entity;\n\n\t\t\t\tif (!ctx->entities[i][j])\n\t\t\t\t\tcontinue;\n\n\t\t\t\tentity = &ctx->entities[i][j]->entity;\n\t\t\t\tdrm_sched_entity_fini(entity);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid amdgpu_ctx_mgr_fini(struct amdgpu_ctx_mgr *mgr)\n{\n\tstruct amdgpu_ctx *ctx;\n\tstruct idr *idp;\n\tuint32_t id;\n\n\tamdgpu_ctx_mgr_entity_fini(mgr);\n\n\tidp = &mgr->ctx_handles;\n\n\tidr_for_each_entry(idp, ctx, id) {\n\t\tif (kref_put(&ctx->refcount, amdgpu_ctx_fini) != 1)\n\t\t\tDRM_ERROR(\"ctx %p is still alive\\n\", ctx);\n\t}\n\n\tidr_destroy(&mgr->ctx_handles);\n\tmutex_destroy(&mgr->lock);\n}\n\nvoid amdgpu_ctx_mgr_usage(struct amdgpu_ctx_mgr *mgr,\n\t\t\t  ktime_t usage[AMDGPU_HW_IP_NUM])\n{\n\tstruct amdgpu_ctx *ctx;\n\tunsigned int hw_ip, i;\n\tuint32_t id;\n\n\t \n\tmutex_lock(&mgr->lock);\n\tfor (hw_ip = 0; hw_ip < AMDGPU_HW_IP_NUM; ++hw_ip) {\n\t\tuint64_t ns = atomic64_read(&mgr->time_spend[hw_ip]);\n\n\t\tusage[hw_ip] = ns_to_ktime(ns);\n\t}\n\n\tidr_for_each_entry(&mgr->ctx_handles, ctx, id) {\n\t\tfor (hw_ip = 0; hw_ip < AMDGPU_HW_IP_NUM; ++hw_ip) {\n\t\t\tfor (i = 0; i < amdgpu_ctx_num_entities[hw_ip]; ++i) {\n\t\t\t\tstruct amdgpu_ctx_entity *centity;\n\t\t\t\tktime_t spend;\n\n\t\t\t\tcentity = ctx->entities[hw_ip][i];\n\t\t\t\tif (!centity)\n\t\t\t\t\tcontinue;\n\t\t\t\tspend = amdgpu_ctx_entity_time(ctx, centity);\n\t\t\t\tusage[hw_ip] = ktime_add(usage[hw_ip], spend);\n\t\t\t}\n\t\t}\n\t}\n\tmutex_unlock(&mgr->lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}