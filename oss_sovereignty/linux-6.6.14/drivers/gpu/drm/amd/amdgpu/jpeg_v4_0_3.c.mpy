{
  "module_name": "jpeg_v4_0_3.c",
  "hash_id": "1b599fa26fba95ca3288d19e45fe36861d4e18a47bb4fb48df39c7a36b85463b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/jpeg_v4_0_3.c",
  "human_readable_source": " \n\n#include \"amdgpu.h\"\n#include \"amdgpu_jpeg.h\"\n#include \"soc15.h\"\n#include \"soc15d.h\"\n#include \"jpeg_v4_0_3.h\"\n#include \"mmsch_v4_0_3.h\"\n\n#include \"vcn/vcn_4_0_3_offset.h\"\n#include \"vcn/vcn_4_0_3_sh_mask.h\"\n#include \"ivsrcid/vcn/irqsrcs_vcn_4_0.h\"\n\nenum jpeg_engin_status {\n\tUVD_PGFSM_STATUS__UVDJ_PWR_ON  = 0,\n\tUVD_PGFSM_STATUS__UVDJ_PWR_OFF = 2,\n};\n\nstatic void jpeg_v4_0_3_set_dec_ring_funcs(struct amdgpu_device *adev);\nstatic void jpeg_v4_0_3_set_irq_funcs(struct amdgpu_device *adev);\nstatic int jpeg_v4_0_3_set_powergating_state(void *handle,\n\t\t\t\tenum amd_powergating_state state);\nstatic void jpeg_v4_0_3_set_ras_funcs(struct amdgpu_device *adev);\nstatic void jpeg_v4_0_3_dec_ring_set_wptr(struct amdgpu_ring *ring);\n\nstatic int amdgpu_ih_srcid_jpeg[] = {\n\tVCN_4_0__SRCID__JPEG_DECODE,\n\tVCN_4_0__SRCID__JPEG1_DECODE,\n\tVCN_4_0__SRCID__JPEG2_DECODE,\n\tVCN_4_0__SRCID__JPEG3_DECODE,\n\tVCN_4_0__SRCID__JPEG4_DECODE,\n\tVCN_4_0__SRCID__JPEG5_DECODE,\n\tVCN_4_0__SRCID__JPEG6_DECODE,\n\tVCN_4_0__SRCID__JPEG7_DECODE\n};\n\n \nstatic int jpeg_v4_0_3_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->jpeg.num_jpeg_rings = AMDGPU_MAX_JPEG_RINGS;\n\n\tjpeg_v4_0_3_set_dec_ring_funcs(adev);\n\tjpeg_v4_0_3_set_irq_funcs(adev);\n\tjpeg_v4_0_3_set_ras_funcs(adev);\n\n\treturn 0;\n}\n\n \nstatic int jpeg_v4_0_3_sw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring;\n\tint i, j, r, jpeg_inst;\n\n\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t \n\t\tr = amdgpu_irq_add_id(adev, SOC15_IH_CLIENTID_VCN,\n\t\t\t\tamdgpu_ih_srcid_jpeg[j], &adev->jpeg.inst->irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_jpeg_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_jpeg_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tjpeg_inst = GET_INST(JPEG, i);\n\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\tring = &adev->jpeg.inst[i].ring_dec[j];\n\t\t\tring->use_doorbell = true;\n\t\t\tring->vm_hub = AMDGPU_MMHUB0(adev->jpeg.inst[i].aid_id);\n\t\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\t\tring->doorbell_index =\n\t\t\t\t\t(adev->doorbell_index.vcn.vcn_ring0_1 << 1) +\n\t\t\t\t\t1 + j + 9 * jpeg_inst;\n\t\t\t} else {\n\t\t\t\tif (j < 4)\n\t\t\t\t\tring->doorbell_index =\n\t\t\t\t\t\t(adev->doorbell_index.vcn.vcn_ring0_1 << 1) +\n\t\t\t\t\t\t4 + j + 32 * jpeg_inst;\n\t\t\t\telse\n\t\t\t\t\tring->doorbell_index =\n\t\t\t\t\t\t(adev->doorbell_index.vcn.vcn_ring0_1 << 1) +\n\t\t\t\t\t\t8 + j + 32 * jpeg_inst;\n\t\t\t}\n\t\t\tsprintf(ring->name, \"jpeg_dec_%d.%d\", adev->jpeg.inst[i].aid_id, j);\n\t\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->jpeg.inst->irq, 0,\n\t\t\t\t\t\tAMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tadev->jpeg.internal.jpeg_pitch[j] =\n\t\t\t\tregUVD_JRBC0_UVD_JRBC_SCRATCH0_INTERNAL_OFFSET;\n\t\t\tadev->jpeg.inst[i].external.jpeg_pitch[j] =\n\t\t\t\tSOC15_REG_OFFSET1(\n\t\t\t\t\tJPEG, jpeg_inst,\n\t\t\t\t\tregUVD_JRBC0_UVD_JRBC_SCRATCH0,\n\t\t\t\t\t(j ? (0x40 * j - 0xc80) : 0));\n\t\t}\n\t}\n\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__JPEG)) {\n\t\tr = amdgpu_jpeg_ras_sw_init(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"Failed to initialize jpeg ras block!\\n\");\n\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int jpeg_v4_0_3_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_jpeg_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_jpeg_sw_fini(adev);\n\n\treturn r;\n}\n\nstatic int jpeg_v4_0_3_start_sriov(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tuint64_t ctx_addr;\n\tuint32_t param, resp, expected;\n\tuint32_t tmp, timeout;\n\n\tstruct amdgpu_mm_table *table = &adev->virt.mm_table;\n\tuint32_t *table_loc;\n\tuint32_t table_size;\n\tuint32_t size, size_dw, item_offset;\n\tuint32_t init_status;\n\tint i, j, jpeg_inst;\n\n\tstruct mmsch_v4_0_cmd_direct_write\n\t\tdirect_wt = { {0} };\n\tstruct mmsch_v4_0_cmd_end end = { {0} };\n\tstruct mmsch_v4_0_3_init_header header;\n\n\tdirect_wt.cmd_header.command_type =\n\t\tMMSCH_COMMAND__DIRECT_REG_WRITE;\n\tend.cmd_header.command_type =\n\t\tMMSCH_COMMAND__END;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; i++) {\n\t\tjpeg_inst = GET_INST(JPEG, i);\n\n\t\tmemset(&header, 0, sizeof(struct mmsch_v4_0_3_init_header));\n\t\theader.version = MMSCH_VERSION;\n\t\theader.total_size = sizeof(struct mmsch_v4_0_3_init_header) >> 2;\n\n\t\ttable_loc = (uint32_t *)table->cpu_addr;\n\t\ttable_loc += header.total_size;\n\n\t\titem_offset = header.total_size;\n\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; j++) {\n\t\t\tring = &adev->jpeg.inst[i].ring_dec[j];\n\t\t\ttable_size = 0;\n\n\t\t\ttmp = SOC15_REG_OFFSET(JPEG, 0, regUVD_JMI0_UVD_LMI_JRBC_RB_64BIT_BAR_LOW);\n\t\t\tMMSCH_V4_0_INSERT_DIRECT_WT(tmp, lower_32_bits(ring->gpu_addr));\n\t\t\ttmp = SOC15_REG_OFFSET(JPEG, 0, regUVD_JMI0_UVD_LMI_JRBC_RB_64BIT_BAR_HIGH);\n\t\t\tMMSCH_V4_0_INSERT_DIRECT_WT(tmp, upper_32_bits(ring->gpu_addr));\n\t\t\ttmp = SOC15_REG_OFFSET(JPEG, 0, regUVD_JRBC0_UVD_JRBC_RB_SIZE);\n\t\t\tMMSCH_V4_0_INSERT_DIRECT_WT(tmp, ring->ring_size / 4);\n\n\t\t\tif (j <= 3) {\n\t\t\t\theader.mjpegdec0[j].table_offset = item_offset;\n\t\t\t\theader.mjpegdec0[j].init_status = 0;\n\t\t\t\theader.mjpegdec0[j].table_size = table_size;\n\t\t\t} else {\n\t\t\t\theader.mjpegdec1[j - 4].table_offset = item_offset;\n\t\t\t\theader.mjpegdec1[j - 4].init_status = 0;\n\t\t\t\theader.mjpegdec1[j - 4].table_size = table_size;\n\t\t\t}\n\t\t\theader.total_size += table_size;\n\t\t\titem_offset += table_size;\n\t\t}\n\n\t\tMMSCH_V4_0_INSERT_END();\n\n\t\t \n\t\tsize = sizeof(struct mmsch_v4_0_3_init_header);\n\t\ttable_loc = (uint32_t *)table->cpu_addr;\n\t\tmemcpy((void *)table_loc, &header, size);\n\n\t\tctx_addr = table->gpu_addr;\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_CTX_ADDR_LO, lower_32_bits(ctx_addr));\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_CTX_ADDR_HI, upper_32_bits(ctx_addr));\n\n\t\ttmp = RREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_VMID);\n\t\ttmp &= ~MMSCH_VF_VMID__VF_CTX_VMID_MASK;\n\t\ttmp |= (0 << MMSCH_VF_VMID__VF_CTX_VMID__SHIFT);\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_VMID, tmp);\n\n\t\tsize = header.total_size;\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_CTX_SIZE, size);\n\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_MAILBOX_RESP, 0);\n\n\t\tparam = 0x00000001;\n\t\tWREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_MAILBOX_HOST, param);\n\t\ttmp = 0;\n\t\ttimeout = 1000;\n\t\tresp = 0;\n\t\texpected = MMSCH_VF_MAILBOX_RESP__OK;\n\t\tinit_status =\n\t\t\t((struct mmsch_v4_0_3_init_header *)(table_loc))->mjpegdec0[i].init_status;\n\t\twhile (resp != expected) {\n\t\t\tresp = RREG32_SOC15(VCN, jpeg_inst, regMMSCH_VF_MAILBOX_RESP);\n\n\t\t\tif (resp != 0)\n\t\t\t\tbreak;\n\t\t\tudelay(10);\n\t\t\ttmp = tmp + 10;\n\t\t\tif (tmp >= timeout) {\n\t\t\t\tDRM_ERROR(\"failed to init MMSCH. TIME-OUT after %d usec\"\\\n\t\t\t\t\t\" waiting for regMMSCH_VF_MAILBOX_RESP \"\\\n\t\t\t\t\t\"(expected=0x%08x, readback=0x%08x)\\n\",\n\t\t\t\t\ttmp, expected, resp);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t}\n\t\tif (resp != expected && resp != MMSCH_VF_MAILBOX_RESP__INCOMPLETE &&\n\t\t\t\tinit_status != MMSCH_VF_ENGINE_STATUS__PASS)\n\t\t\tDRM_ERROR(\"MMSCH init status is incorrect! readback=0x%08x, header init status for jpeg: %x\\n\",\n\t\t\t\t\tresp, init_status);\n\n\t}\n\treturn 0;\n}\n\n \nstatic int jpeg_v4_0_3_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring;\n\tint i, j, r, jpeg_inst;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tr = jpeg_v4_0_3_start_sriov(adev);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tfor (i = 0; i < adev->vcn.num_vcn_inst; ++i) {\n\t\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\t\tring = &adev->jpeg.inst[i].ring_dec[j];\n\t\t\t\tring->wptr = 0;\n\t\t\t\tring->wptr_old = 0;\n\t\t\t\tjpeg_v4_0_3_dec_ring_set_wptr(ring);\n\t\t\t\tring->sched.ready = true;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\t\tjpeg_inst = GET_INST(JPEG, i);\n\n\t\t\tring = adev->jpeg.inst[i].ring_dec;\n\n\t\t\tif (ring->use_doorbell)\n\t\t\t\tadev->nbio.funcs->vcn_doorbell_range(\n\t\t\t\t\tadev, ring->use_doorbell,\n\t\t\t\t\t(adev->doorbell_index.vcn.vcn_ring0_1 << 1) +\n\t\t\t\t\t\t9 * jpeg_inst,\n\t\t\t\t\tadev->jpeg.inst[i].aid_id);\n\n\t\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\t\tring = &adev->jpeg.inst[i].ring_dec[j];\n\t\t\t\tif (ring->use_doorbell)\n\t\t\t\t\tWREG32_SOC15_OFFSET(\n\t\t\t\t\t\tVCN, GET_INST(VCN, i),\n\t\t\t\t\t\tregVCN_JPEG_DB_CTRL,\n\t\t\t\t\t\t(ring->pipe ? (ring->pipe - 0x15) : 0),\n\t\t\t\t\t\tring->doorbell_index\n\t\t\t\t\t\t\t<< VCN_JPEG_DB_CTRL__OFFSET__SHIFT |\n\t\t\t\t\t\t\tVCN_JPEG_DB_CTRL__EN_MASK);\n\t\t\t\tr = amdgpu_ring_test_helper(ring);\n\t\t\t\tif (r)\n\t\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t}\n\tDRM_DEV_INFO(adev->dev, \"JPEG decode initialized successfully.\\n\");\n\n\treturn 0;\n}\n\n \nstatic int jpeg_v4_0_3_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret = 0;\n\n\tcancel_delayed_work_sync(&adev->jpeg.idle_work);\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tif (adev->jpeg.cur_state != AMD_PG_STATE_GATE)\n\t\t\tret = jpeg_v4_0_3_set_powergating_state(adev, AMD_PG_STATE_GATE);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int jpeg_v4_0_3_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = jpeg_v4_0_3_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_jpeg_suspend(adev);\n\n\treturn r;\n}\n\n \nstatic int jpeg_v4_0_3_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = amdgpu_jpeg_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = jpeg_v4_0_3_hw_init(adev);\n\n\treturn r;\n}\n\nstatic void jpeg_v4_0_3_disable_clock_gating(struct amdgpu_device *adev, int inst_idx)\n{\n\tint i, jpeg_inst;\n\tuint32_t data;\n\n\tjpeg_inst = GET_INST(JPEG, inst_idx);\n\tdata = RREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_JPEG_MGCG) {\n\t\tdata |= 1 << JPEG_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\t\tdata &= (~(JPEG_CGC_CTRL__JPEG0_DEC_MODE_MASK << 1));\n\t} else {\n\t\tdata &= ~JPEG_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\t}\n\n\tdata |= 1 << JPEG_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << JPEG_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_GATE);\n\tdata &= ~(JPEG_CGC_GATE__JMCIF_MASK | JPEG_CGC_GATE__JRBBM_MASK);\n\tfor (i = 0; i < adev->jpeg.num_jpeg_rings; ++i)\n\t\tdata &= ~(JPEG_CGC_GATE__JPEG0_DEC_MASK << i);\n\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_GATE, data);\n}\n\nstatic void jpeg_v4_0_3_enable_clock_gating(struct amdgpu_device *adev, int inst_idx)\n{\n\tint i, jpeg_inst;\n\tuint32_t data;\n\n\tjpeg_inst = GET_INST(JPEG, inst_idx);\n\tdata = RREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_CTRL);\n\tif (adev->cg_flags & AMD_CG_SUPPORT_JPEG_MGCG) {\n\t\tdata |= 1 << JPEG_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\t\tdata |= (JPEG_CGC_CTRL__JPEG0_DEC_MODE_MASK << 1);\n\t} else {\n\t\tdata &= ~JPEG_CGC_CTRL__DYN_CLOCK_MODE__SHIFT;\n\t}\n\n\tdata |= 1 << JPEG_CGC_CTRL__CLK_GATE_DLY_TIMER__SHIFT;\n\tdata |= 4 << JPEG_CGC_CTRL__CLK_OFF_DELAY__SHIFT;\n\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_CTRL, data);\n\n\tdata = RREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_GATE);\n\tdata |= (JPEG_CGC_GATE__JMCIF_MASK | JPEG_CGC_GATE__JRBBM_MASK);\n\tfor (i = 0; i < adev->jpeg.num_jpeg_rings; ++i)\n\t\tdata |= (JPEG_CGC_GATE__JPEG0_DEC_MASK << i);\n\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_CGC_GATE, data);\n}\n\n \nstatic int jpeg_v4_0_3_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, j, jpeg_inst;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tjpeg_inst = GET_INST(JPEG, i);\n\n\t\tWREG32_SOC15(JPEG, jpeg_inst, regUVD_PGFSM_CONFIG,\n\t\t\t     1 << UVD_PGFSM_CONFIG__UVDJ_PWR_CONFIG__SHIFT);\n\t\tSOC15_WAIT_ON_RREG(\n\t\t\tJPEG, jpeg_inst, regUVD_PGFSM_STATUS,\n\t\t\tUVD_PGFSM_STATUS__UVDJ_PWR_ON\n\t\t\t\t<< UVD_PGFSM_STATUS__UVDJ_PWR_STATUS__SHIFT,\n\t\t\tUVD_PGFSM_STATUS__UVDJ_PWR_STATUS_MASK);\n\n\t\t \n\t\tWREG32_P(SOC15_REG_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t  regUVD_JPEG_POWER_STATUS),\n\t\t\t 0, ~UVD_JPEG_POWER_STATUS__JPEG_POWER_STATUS_MASK);\n\n\t\t \n\t\tjpeg_v4_0_3_disable_clock_gating(adev, i);\n\n\t\t \n\t\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_DEC_GFX8_ADDR_CONFIG,\n\t\t\t     adev->gfx.config.gb_addr_config);\n\t\tWREG32_SOC15(JPEG, jpeg_inst, regJPEG_DEC_GFX10_ADDR_CONFIG,\n\t\t\t     adev->gfx.config.gb_addr_config);\n\n\t\t \n\t\tWREG32_P(SOC15_REG_OFFSET(JPEG, jpeg_inst, regUVD_JMI_CNTL), 0,\n\t\t\t ~UVD_JMI_CNTL__SOFT_RESET_MASK);\n\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\tunsigned int reg_offset = (j?(0x40 * j - 0xc80):0);\n\n\t\t\tring = &adev->jpeg.inst[i].ring_dec[j];\n\n\t\t\t \n\t\t\tWREG32_P(SOC15_REG_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t\t  regJPEG_SYS_INT_EN),\n\t\t\t\t JPEG_SYS_INT_EN__DJRBC0_MASK << j,\n\t\t\t\t ~(JPEG_SYS_INT_EN__DJRBC0_MASK << j));\n\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JMI0_UVD_LMI_JRBC_RB_VMID,\n\t\t\t\t\t    reg_offset, 0);\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_CNTL,\n\t\t\t\t\t    reg_offset,\n\t\t\t\t\t    (0x00000001L | 0x00000002L));\n\t\t\tWREG32_SOC15_OFFSET(\n\t\t\t\tJPEG, jpeg_inst,\n\t\t\t\tregUVD_JMI0_UVD_LMI_JRBC_RB_64BIT_BAR_LOW,\n\t\t\t\treg_offset, lower_32_bits(ring->gpu_addr));\n\t\t\tWREG32_SOC15_OFFSET(\n\t\t\t\tJPEG, jpeg_inst,\n\t\t\t\tregUVD_JMI0_UVD_LMI_JRBC_RB_64BIT_BAR_HIGH,\n\t\t\t\treg_offset, upper_32_bits(ring->gpu_addr));\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_RPTR,\n\t\t\t\t\t    reg_offset, 0);\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_WPTR,\n\t\t\t\t\t    reg_offset, 0);\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_CNTL,\n\t\t\t\t\t    reg_offset, 0x00000002L);\n\t\t\tWREG32_SOC15_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_SIZE,\n\t\t\t\t\t    reg_offset, ring->ring_size / 4);\n\t\t\tring->wptr = RREG32_SOC15_OFFSET(\n\t\t\t\tJPEG, jpeg_inst, regUVD_JRBC0_UVD_JRBC_RB_WPTR,\n\t\t\t\treg_offset);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int jpeg_v4_0_3_stop(struct amdgpu_device *adev)\n{\n\tint i, jpeg_inst;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tjpeg_inst = GET_INST(JPEG, i);\n\t\t \n\t\tWREG32_P(SOC15_REG_OFFSET(JPEG, jpeg_inst, regUVD_JMI_CNTL),\n\t\t\t UVD_JMI_CNTL__SOFT_RESET_MASK,\n\t\t\t ~UVD_JMI_CNTL__SOFT_RESET_MASK);\n\n\t\tjpeg_v4_0_3_enable_clock_gating(adev, i);\n\n\t\t \n\t\tWREG32_P(SOC15_REG_OFFSET(JPEG, jpeg_inst,\n\t\t\t\t\t  regUVD_JPEG_POWER_STATUS),\n\t\t\t UVD_JPEG_POWER_STATUS__JPEG_POWER_STATUS_MASK,\n\t\t\t ~UVD_JPEG_POWER_STATUS__JPEG_POWER_STATUS_MASK);\n\n\t\tWREG32_SOC15(JPEG, jpeg_inst, regUVD_PGFSM_CONFIG,\n\t\t\t     2 << UVD_PGFSM_CONFIG__UVDJ_PWR_CONFIG__SHIFT);\n\t\tSOC15_WAIT_ON_RREG(\n\t\t\tJPEG, jpeg_inst, regUVD_PGFSM_STATUS,\n\t\t\tUVD_PGFSM_STATUS__UVDJ_PWR_OFF\n\t\t\t\t<< UVD_PGFSM_STATUS__UVDJ_PWR_STATUS__SHIFT,\n\t\t\tUVD_PGFSM_STATUS__UVDJ_PWR_STATUS_MASK);\n\t}\n\n\treturn 0;\n}\n\n \nstatic uint64_t jpeg_v4_0_3_dec_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\treturn RREG32_SOC15_OFFSET(\n\t\tJPEG, GET_INST(JPEG, ring->me), regUVD_JRBC0_UVD_JRBC_RB_RPTR,\n\t\tring->pipe ? (0x40 * ring->pipe - 0xc80) : 0);\n}\n\n \nstatic uint64_t jpeg_v4_0_3_dec_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell)\n\t\treturn adev->wb.wb[ring->wptr_offs];\n\telse\n\t\treturn RREG32_SOC15_OFFSET(\n\t\t\tJPEG, GET_INST(JPEG, ring->me),\n\t\t\tregUVD_JRBC0_UVD_JRBC_RB_WPTR,\n\t\t\tring->pipe ? (0x40 * ring->pipe - 0xc80) : 0);\n}\n\n \nstatic void jpeg_v4_0_3_dec_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell) {\n\t\tadev->wb.wb[ring->wptr_offs] = lower_32_bits(ring->wptr);\n\t\tWDOORBELL32(ring->doorbell_index, lower_32_bits(ring->wptr));\n\t} else {\n\t\tWREG32_SOC15_OFFSET(JPEG, GET_INST(JPEG, ring->me),\n\t\t\t\t    regUVD_JRBC0_UVD_JRBC_RB_WPTR,\n\t\t\t\t    (ring->pipe ? (0x40 * ring->pipe - 0xc80) :\n\t\t\t\t\t\t  0),\n\t\t\t\t    lower_32_bits(ring->wptr));\n\t}\n}\n\n \nstatic void jpeg_v4_0_3_dec_ring_insert_start(struct amdgpu_ring *ring)\n{\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_EXTERNAL_REG_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x62a04);  \n\n\tamdgpu_ring_write(ring, PACKETJ(JRBC_DEC_EXTERNAL_REG_WRITE_ADDR,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x80004000);\n}\n\n \nstatic void jpeg_v4_0_3_dec_ring_insert_end(struct amdgpu_ring *ring)\n{\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_EXTERNAL_REG_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x62a04);\n\n\tamdgpu_ring_write(ring, PACKETJ(JRBC_DEC_EXTERNAL_REG_WRITE_ADDR,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x00004000);\n}\n\n \nstatic void jpeg_v4_0_3_dec_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\tunsigned int flags)\n{\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JPEG_GPCOM_DATA0_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, seq);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JPEG_GPCOM_DATA1_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, seq);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_RB_MEM_WR_64BIT_BAR_LOW_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_RB_MEM_WR_64BIT_BAR_HIGH_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JPEG_GPCOM_CMD_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x8);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JPEG_GPCOM_CMD_INTERNAL_OFFSET,\n\t\t0, PACKETJ_CONDITION_CHECK0, PACKETJ_TYPE4));\n\tamdgpu_ring_write(ring, 0);\n\n\tif (ring->adev->jpeg.inst[ring->me].aid_id) {\n\t\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_EXTERNAL_MCM_ADDR_INTERNAL_OFFSET,\n\t\t\t0, PACKETJ_CONDITION_CHECK0, PACKETJ_TYPE0));\n\t\tamdgpu_ring_write(ring, 0x4);\n\t} else {\n\t\tamdgpu_ring_write(ring, PACKETJ(0, 0, 0, PACKETJ_TYPE6));\n\t\tamdgpu_ring_write(ring, 0);\n\t}\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_EXTERNAL_REG_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x3fbc);\n\n\tif (ring->adev->jpeg.inst[ring->me].aid_id) {\n\t\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_EXTERNAL_MCM_ADDR_INTERNAL_OFFSET,\n\t\t\t0, PACKETJ_CONDITION_CHECK0, PACKETJ_TYPE0));\n\t\tamdgpu_ring_write(ring, 0x0);\n\t} else {\n\t\tamdgpu_ring_write(ring, PACKETJ(0, 0, 0, PACKETJ_TYPE6));\n\t\tamdgpu_ring_write(ring, 0);\n\t}\n\n\tamdgpu_ring_write(ring, PACKETJ(JRBC_DEC_EXTERNAL_REG_WRITE_ADDR,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x1);\n\n\tamdgpu_ring_write(ring, PACKETJ(0, 0, 0, PACKETJ_TYPE7));\n\tamdgpu_ring_write(ring, 0);\n}\n\n \nstatic void jpeg_v4_0_3_dec_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\tstruct amdgpu_job *job,\n\t\t\t\tstruct amdgpu_ib *ib,\n\t\t\t\tuint32_t flags)\n{\n\tunsigned int vmid = AMDGPU_JOB_GET_VMID(job);\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_LMI_JRBC_IB_VMID_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, (vmid | (vmid << 4)));\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_LMI_JPEG_VMID_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, (vmid | (vmid << 4)));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_IB_64BIT_BAR_LOW_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_IB_64BIT_BAR_HIGH_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_IB_SIZE_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, ib->length_dw);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_RB_MEM_RD_64BIT_BAR_LOW_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, lower_32_bits(ring->gpu_addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_LMI_JRBC_RB_MEM_RD_64BIT_BAR_HIGH_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, upper_32_bits(ring->gpu_addr));\n\n\tamdgpu_ring_write(ring,\tPACKETJ(0, 0, PACKETJ_CONDITION_CHECK0, PACKETJ_TYPE2));\n\tamdgpu_ring_write(ring, 0);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_RB_COND_RD_TIMER_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x01400200);\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_RB_REF_DATA_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x2);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_STATUS_INTERNAL_OFFSET,\n\t\t0, PACKETJ_CONDITION_CHECK3, PACKETJ_TYPE3));\n\tamdgpu_ring_write(ring, 0x2);\n}\n\nstatic void jpeg_v4_0_3_dec_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\tuint32_t val, uint32_t mask)\n{\n\tuint32_t reg_offset = (reg << 2);\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_RB_COND_RD_TIMER_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, 0x01400200);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_RB_REF_DATA_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tamdgpu_ring_write(ring, val);\n\n\tamdgpu_ring_write(ring, PACKETJ(regUVD_JRBC_EXTERNAL_REG_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tif (reg_offset >= 0x10000 && reg_offset <= 0x105ff) {\n\t\tamdgpu_ring_write(ring, 0);\n\t\tamdgpu_ring_write(ring,\n\t\t\tPACKETJ((reg_offset >> 2), 0, 0, PACKETJ_TYPE3));\n\t} else {\n\t\tamdgpu_ring_write(ring, reg_offset);\n\t\tamdgpu_ring_write(ring,\tPACKETJ(JRBC_DEC_EXTERNAL_REG_WRITE_ADDR,\n\t\t\t0, 0, PACKETJ_TYPE3));\n\t}\n\tamdgpu_ring_write(ring, mask);\n}\n\nstatic void jpeg_v4_0_3_dec_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\tunsigned int vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\tuint32_t data0, data1, mask;\n\n\tpd_addr = amdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n\n\t \n\tdata0 = hub->ctx0_ptb_addr_lo32 + vmid * hub->ctx_addr_distance;\n\tdata1 = lower_32_bits(pd_addr);\n\tmask = 0xffffffff;\n\tjpeg_v4_0_3_dec_ring_emit_reg_wait(ring, data0, data1, mask);\n}\n\nstatic void jpeg_v4_0_3_dec_ring_emit_wreg(struct amdgpu_ring *ring, uint32_t reg, uint32_t val)\n{\n\tuint32_t reg_offset = (reg << 2);\n\n\tamdgpu_ring_write(ring,\tPACKETJ(regUVD_JRBC_EXTERNAL_REG_INTERNAL_OFFSET,\n\t\t0, 0, PACKETJ_TYPE0));\n\tif (reg_offset >= 0x10000 && reg_offset <= 0x105ff) {\n\t\tamdgpu_ring_write(ring, 0);\n\t\tamdgpu_ring_write(ring,\n\t\t\tPACKETJ((reg_offset >> 2), 0, 0, PACKETJ_TYPE0));\n\t} else {\n\t\tamdgpu_ring_write(ring, reg_offset);\n\t\tamdgpu_ring_write(ring,\tPACKETJ(JRBC_DEC_EXTERNAL_REG_WRITE_ADDR,\n\t\t\t0, 0, PACKETJ_TYPE0));\n\t}\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void jpeg_v4_0_3_dec_ring_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tint i;\n\n\tWARN_ON(ring->wptr % 2 || count % 2);\n\n\tfor (i = 0; i < count / 2; i++) {\n\t\tamdgpu_ring_write(ring, PACKETJ(0, 0, 0, PACKETJ_TYPE6));\n\t\tamdgpu_ring_write(ring, 0);\n\t}\n}\n\nstatic bool jpeg_v4_0_3_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool ret = false;\n\tint i, j;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\tunsigned int reg_offset = (j?(0x40 * j - 0xc80):0);\n\n\t\t\tret &= ((RREG32_SOC15_OFFSET(\n\t\t\t\t\t JPEG, GET_INST(JPEG, i),\n\t\t\t\t\t regUVD_JRBC0_UVD_JRBC_STATUS,\n\t\t\t\t\t reg_offset) &\n\t\t\t\t UVD_JRBC0_UVD_JRBC_STATUS__RB_JOB_DONE_MASK) ==\n\t\t\t\tUVD_JRBC0_UVD_JRBC_STATUS__RB_JOB_DONE_MASK);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int jpeg_v4_0_3_wait_for_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret = 0;\n\tint i, j;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\tunsigned int reg_offset = (j?(0x40 * j - 0xc80):0);\n\n\t\t\tret &= SOC15_WAIT_ON_RREG_OFFSET(\n\t\t\t\tJPEG, GET_INST(JPEG, i),\n\t\t\t\tregUVD_JRBC0_UVD_JRBC_STATUS, reg_offset,\n\t\t\t\tUVD_JRBC0_UVD_JRBC_STATUS__RB_JOB_DONE_MASK,\n\t\t\t\tUVD_JRBC0_UVD_JRBC_STATUS__RB_JOB_DONE_MASK);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int jpeg_v4_0_3_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tbool enable = state == AMD_CG_STATE_GATE;\n\tint i;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tif (enable) {\n\t\t\tif (!jpeg_v4_0_3_is_idle(handle))\n\t\t\t\treturn -EBUSY;\n\t\t\tjpeg_v4_0_3_enable_clock_gating(adev, i);\n\t\t} else {\n\t\t\tjpeg_v4_0_3_disable_clock_gating(adev, i);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int jpeg_v4_0_3_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint ret;\n\n\tif (state == adev->jpeg.cur_state)\n\t\treturn 0;\n\n\tif (state == AMD_PG_STATE_GATE)\n\t\tret = jpeg_v4_0_3_stop(adev);\n\telse\n\t\tret = jpeg_v4_0_3_start(adev);\n\n\tif (!ret)\n\t\tadev->jpeg.cur_state = state;\n\n\treturn ret;\n}\n\nstatic int jpeg_v4_0_3_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned int type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\treturn 0;\n}\n\nstatic int jpeg_v4_0_3_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tuint32_t i, inst;\n\n\ti = node_id_to_phys_map[entry->node_id];\n\tDRM_DEV_DEBUG(adev->dev, \"IH: JPEG TRAP\\n\");\n\n\tfor (inst = 0; inst < adev->jpeg.num_jpeg_inst; ++inst)\n\t\tif (adev->jpeg.inst[inst].aid_id == i)\n\t\t\tbreak;\n\n\tif (inst >= adev->jpeg.num_jpeg_inst) {\n\t\tdev_WARN_ONCE(adev->dev, 1,\n\t\t\t      \"Interrupt received for unknown JPEG instance %d\",\n\t\t\t      entry->node_id);\n\t\treturn 0;\n\t}\n\n\tswitch (entry->src_id) {\n\tcase VCN_4_0__SRCID__JPEG_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[0]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG1_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[1]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG2_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[2]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG3_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[3]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG4_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[4]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG5_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[5]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG6_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[6]);\n\t\tbreak;\n\tcase VCN_4_0__SRCID__JPEG7_DECODE:\n\t\tamdgpu_fence_process(&adev->jpeg.inst[inst].ring_dec[7]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_DEV_ERROR(adev->dev, \"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs jpeg_v4_0_3_ip_funcs = {\n\t.name = \"jpeg_v4_0_3\",\n\t.early_init = jpeg_v4_0_3_early_init,\n\t.late_init = NULL,\n\t.sw_init = jpeg_v4_0_3_sw_init,\n\t.sw_fini = jpeg_v4_0_3_sw_fini,\n\t.hw_init = jpeg_v4_0_3_hw_init,\n\t.hw_fini = jpeg_v4_0_3_hw_fini,\n\t.suspend = jpeg_v4_0_3_suspend,\n\t.resume = jpeg_v4_0_3_resume,\n\t.is_idle = jpeg_v4_0_3_is_idle,\n\t.wait_for_idle = jpeg_v4_0_3_wait_for_idle,\n\t.check_soft_reset = NULL,\n\t.pre_soft_reset = NULL,\n\t.soft_reset = NULL,\n\t.post_soft_reset = NULL,\n\t.set_clockgating_state = jpeg_v4_0_3_set_clockgating_state,\n\t.set_powergating_state = jpeg_v4_0_3_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs jpeg_v4_0_3_dec_ring_vm_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCN_JPEG,\n\t.align_mask = 0xf,\n\t.get_rptr = jpeg_v4_0_3_dec_ring_get_rptr,\n\t.get_wptr = jpeg_v4_0_3_dec_ring_get_wptr,\n\t.set_wptr = jpeg_v4_0_3_dec_ring_set_wptr,\n\t.emit_frame_size =\n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 6 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 8 +\n\t\t8 +  \n\t\t22 + 22 +  \n\t\t8 + 16,\n\t.emit_ib_size = 22,  \n\t.emit_ib = jpeg_v4_0_3_dec_ring_emit_ib,\n\t.emit_fence = jpeg_v4_0_3_dec_ring_emit_fence,\n\t.emit_vm_flush = jpeg_v4_0_3_dec_ring_emit_vm_flush,\n\t.test_ring = amdgpu_jpeg_dec_ring_test_ring,\n\t.test_ib = amdgpu_jpeg_dec_ring_test_ib,\n\t.insert_nop = jpeg_v4_0_3_dec_ring_nop,\n\t.insert_start = jpeg_v4_0_3_dec_ring_insert_start,\n\t.insert_end = jpeg_v4_0_3_dec_ring_insert_end,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_jpeg_ring_begin_use,\n\t.end_use = amdgpu_jpeg_ring_end_use,\n\t.emit_wreg = jpeg_v4_0_3_dec_ring_emit_wreg,\n\t.emit_reg_wait = jpeg_v4_0_3_dec_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void jpeg_v4_0_3_set_dec_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i, j, jpeg_inst;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tfor (j = 0; j < adev->jpeg.num_jpeg_rings; ++j) {\n\t\t\tadev->jpeg.inst[i].ring_dec[j].funcs = &jpeg_v4_0_3_dec_ring_vm_funcs;\n\t\t\tadev->jpeg.inst[i].ring_dec[j].me = i;\n\t\t\tadev->jpeg.inst[i].ring_dec[j].pipe = j;\n\t\t}\n\t\tjpeg_inst = GET_INST(JPEG, i);\n\t\tadev->jpeg.inst[i].aid_id =\n\t\t\tjpeg_inst / adev->jpeg.num_inst_per_aid;\n\t}\n\tDRM_DEV_INFO(adev->dev, \"JPEG decode is enabled in VM mode\\n\");\n}\n\nstatic const struct amdgpu_irq_src_funcs jpeg_v4_0_3_irq_funcs = {\n\t.set = jpeg_v4_0_3_set_interrupt_state,\n\t.process = jpeg_v4_0_3_process_interrupt,\n};\n\nstatic void jpeg_v4_0_3_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; ++i) {\n\t\tadev->jpeg.inst->irq.num_types += adev->jpeg.num_jpeg_rings;\n\t}\n\tadev->jpeg.inst->irq.funcs = &jpeg_v4_0_3_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version jpeg_v4_0_3_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_JPEG,\n\t.major = 4,\n\t.minor = 0,\n\t.rev = 3,\n\t.funcs = &jpeg_v4_0_3_ip_funcs,\n};\n\nstatic const struct amdgpu_ras_err_status_reg_entry jpeg_v4_0_3_ue_reg_list[] = {\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG0S, regVCN_UE_ERR_STATUS_HI_JPEG0S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG0S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG0D, regVCN_UE_ERR_STATUS_HI_JPEG0D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG0D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG1S, regVCN_UE_ERR_STATUS_HI_JPEG1S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG1S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG1D, regVCN_UE_ERR_STATUS_HI_JPEG1D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG1D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG2S, regVCN_UE_ERR_STATUS_HI_JPEG2S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG2S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG2D, regVCN_UE_ERR_STATUS_HI_JPEG2D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG2D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG3S, regVCN_UE_ERR_STATUS_HI_JPEG3S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG3S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG3D, regVCN_UE_ERR_STATUS_HI_JPEG3D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG3D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG4S, regVCN_UE_ERR_STATUS_HI_JPEG4S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG4S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG4D, regVCN_UE_ERR_STATUS_HI_JPEG4D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG4D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG5S, regVCN_UE_ERR_STATUS_HI_JPEG5S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG5S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG5D, regVCN_UE_ERR_STATUS_HI_JPEG5D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG5D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG6S, regVCN_UE_ERR_STATUS_HI_JPEG6S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG6S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG6D, regVCN_UE_ERR_STATUS_HI_JPEG6D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG6D\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG7S, regVCN_UE_ERR_STATUS_HI_JPEG7S),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG7S\"},\n\t{AMDGPU_RAS_REG_ENTRY(JPEG, 0, regVCN_UE_ERR_STATUS_LO_JPEG7D, regVCN_UE_ERR_STATUS_HI_JPEG7D),\n\t1, (AMDGPU_RAS_ERR_INFO_VALID | AMDGPU_RAS_ERR_STATUS_VALID), \"JPEG7D\"},\n};\n\nstatic void jpeg_v4_0_3_inst_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t jpeg_inst,\n\t\t\t\t\t\t   void *ras_err_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_err_status;\n\n\t \n\tamdgpu_ras_inst_query_ras_error_count(adev,\n\t\t\tjpeg_v4_0_3_ue_reg_list,\n\t\t\tARRAY_SIZE(jpeg_v4_0_3_ue_reg_list),\n\t\t\tNULL, 0, GET_INST(VCN, jpeg_inst),\n\t\t\tAMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE,\n\t\t\t&err_data->ue_count);\n}\n\nstatic void jpeg_v4_0_3_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t      void *ras_err_status)\n{\n\tuint32_t i;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__JPEG)) {\n\t\tdev_warn(adev->dev, \"JPEG RAS is not supported\\n\");\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; i++)\n\t\tjpeg_v4_0_3_inst_query_ras_error_count(adev, i, ras_err_status);\n}\n\nstatic void jpeg_v4_0_3_inst_reset_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t\t   uint32_t jpeg_inst)\n{\n\tamdgpu_ras_inst_reset_ras_error_count(adev,\n\t\t\tjpeg_v4_0_3_ue_reg_list,\n\t\t\tARRAY_SIZE(jpeg_v4_0_3_ue_reg_list),\n\t\t\tGET_INST(VCN, jpeg_inst));\n}\n\nstatic void jpeg_v4_0_3_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tuint32_t i;\n\n\tif (!amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__JPEG)) {\n\t\tdev_warn(adev->dev, \"JPEG RAS is not supported\\n\");\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < adev->jpeg.num_jpeg_inst; i++)\n\t\tjpeg_v4_0_3_inst_reset_ras_error_count(adev, i);\n}\n\nstatic const struct amdgpu_ras_block_hw_ops jpeg_v4_0_3_ras_hw_ops = {\n\t.query_ras_error_count = jpeg_v4_0_3_query_ras_error_count,\n\t.reset_ras_error_count = jpeg_v4_0_3_reset_ras_error_count,\n};\n\nstatic struct amdgpu_jpeg_ras jpeg_v4_0_3_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &jpeg_v4_0_3_ras_hw_ops,\n\t},\n};\n\nstatic void jpeg_v4_0_3_set_ras_funcs(struct amdgpu_device *adev)\n{\n\tadev->jpeg.ras = &jpeg_v4_0_3_ras;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}