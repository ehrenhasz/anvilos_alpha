{
  "module_name": "amdgpu_vm.c",
  "hash_id": "bf2af09f8a0d89aa14aae07fb566cab8f126aa31a4678e076dcd904d4e55cf46",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c",
  "human_readable_source": " \n\n#include <linux/dma-fence-array.h>\n#include <linux/interval_tree_generic.h>\n#include <linux/idr.h>\n#include <linux/dma-buf.h>\n\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include <drm/ttm/ttm_tt.h>\n#include <drm/drm_exec.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_gmc.h\"\n#include \"amdgpu_xgmi.h\"\n#include \"amdgpu_dma_buf.h\"\n#include \"amdgpu_res_cursor.h\"\n#include \"kfd_svm.h\"\n\n \n\n#define START(node) ((node)->start)\n#define LAST(node) ((node)->last)\n\nINTERVAL_TREE_DEFINE(struct amdgpu_bo_va_mapping, rb, uint64_t, __subtree_last,\n\t\t     START, LAST, static, amdgpu_vm_it)\n\n#undef START\n#undef LAST\n\n \nstruct amdgpu_prt_cb {\n\n\t \n\tstruct amdgpu_device *adev;\n\n\t \n\tstruct dma_fence_cb cb;\n};\n\n \nstruct amdgpu_vm_tlb_seq_struct {\n\t \n\tstruct amdgpu_vm *vm;\n\n\t \n\tstruct dma_fence_cb cb;\n};\n\n \nint amdgpu_vm_set_pasid(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\tu32 pasid)\n{\n\tint r;\n\n\tif (vm->pasid == pasid)\n\t\treturn 0;\n\n\tif (vm->pasid) {\n\t\tr = xa_err(xa_erase_irq(&adev->vm_manager.pasids, vm->pasid));\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tvm->pasid = 0;\n\t}\n\n\tif (pasid) {\n\t\tr = xa_err(xa_store_irq(&adev->vm_manager.pasids, pasid, vm,\n\t\t\t\t\tGFP_KERNEL));\n\t\tif (r < 0)\n\t\t\treturn r;\n\n\t\tvm->pasid = pasid;\n\t}\n\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_vm_bo_evicted(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tstruct amdgpu_vm *vm = vm_bo->vm;\n\tstruct amdgpu_bo *bo = vm_bo->bo;\n\n\tvm_bo->moved = true;\n\tspin_lock(&vm_bo->vm->status_lock);\n\tif (bo->tbo.type == ttm_bo_type_kernel)\n\t\tlist_move(&vm_bo->vm_status, &vm->evicted);\n\telse\n\t\tlist_move_tail(&vm_bo->vm_status, &vm->evicted);\n\tspin_unlock(&vm_bo->vm->status_lock);\n}\n \nstatic void amdgpu_vm_bo_moved(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tspin_lock(&vm_bo->vm->status_lock);\n\tlist_move(&vm_bo->vm_status, &vm_bo->vm->moved);\n\tspin_unlock(&vm_bo->vm->status_lock);\n}\n\n \nstatic void amdgpu_vm_bo_idle(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tspin_lock(&vm_bo->vm->status_lock);\n\tlist_move(&vm_bo->vm_status, &vm_bo->vm->idle);\n\tspin_unlock(&vm_bo->vm->status_lock);\n\tvm_bo->moved = false;\n}\n\n \nstatic void amdgpu_vm_bo_invalidated(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tspin_lock(&vm_bo->vm->status_lock);\n\tlist_move(&vm_bo->vm_status, &vm_bo->vm->invalidated);\n\tspin_unlock(&vm_bo->vm->status_lock);\n}\n\n \nstatic void amdgpu_vm_bo_relocated(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tif (vm_bo->bo->parent) {\n\t\tspin_lock(&vm_bo->vm->status_lock);\n\t\tlist_move(&vm_bo->vm_status, &vm_bo->vm->relocated);\n\t\tspin_unlock(&vm_bo->vm->status_lock);\n\t} else {\n\t\tamdgpu_vm_bo_idle(vm_bo);\n\t}\n}\n\n \nstatic void amdgpu_vm_bo_done(struct amdgpu_vm_bo_base *vm_bo)\n{\n\tspin_lock(&vm_bo->vm->status_lock);\n\tlist_move(&vm_bo->vm_status, &vm_bo->vm->done);\n\tspin_unlock(&vm_bo->vm->status_lock);\n}\n\n \nstatic void amdgpu_vm_bo_reset_state_machine(struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_vm_bo_base *vm_bo, *tmp;\n\n\tspin_lock(&vm->status_lock);\n\tlist_splice_init(&vm->done, &vm->invalidated);\n\tlist_for_each_entry(vm_bo, &vm->invalidated, vm_status)\n\t\tvm_bo->moved = true;\n\tlist_for_each_entry_safe(vm_bo, tmp, &vm->idle, vm_status) {\n\t\tstruct amdgpu_bo *bo = vm_bo->bo;\n\n\t\tvm_bo->moved = true;\n\t\tif (!bo || bo->tbo.type != ttm_bo_type_kernel)\n\t\t\tlist_move(&vm_bo->vm_status, &vm_bo->vm->moved);\n\t\telse if (bo->parent)\n\t\t\tlist_move(&vm_bo->vm_status, &vm_bo->vm->relocated);\n\t}\n\tspin_unlock(&vm->status_lock);\n}\n\n \nvoid amdgpu_vm_bo_base_init(struct amdgpu_vm_bo_base *base,\n\t\t\t    struct amdgpu_vm *vm, struct amdgpu_bo *bo)\n{\n\tbase->vm = vm;\n\tbase->bo = bo;\n\tbase->next = NULL;\n\tINIT_LIST_HEAD(&base->vm_status);\n\n\tif (!bo)\n\t\treturn;\n\tbase->next = bo->vm_bo;\n\tbo->vm_bo = base;\n\n\tif (bo->tbo.base.resv != vm->root.bo->tbo.base.resv)\n\t\treturn;\n\n\tdma_resv_assert_held(vm->root.bo->tbo.base.resv);\n\n\tttm_bo_set_bulk_move(&bo->tbo, &vm->lru_bulk_move);\n\tif (bo->tbo.type == ttm_bo_type_kernel && bo->parent)\n\t\tamdgpu_vm_bo_relocated(base);\n\telse\n\t\tamdgpu_vm_bo_idle(base);\n\n\tif (bo->preferred_domains &\n\t    amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type))\n\t\treturn;\n\n\t \n\tamdgpu_vm_bo_evicted(base);\n}\n\n \nint amdgpu_vm_lock_pd(struct amdgpu_vm *vm, struct drm_exec *exec,\n\t\t      unsigned int num_fences)\n{\n\t \n\treturn drm_exec_prepare_obj(exec, &vm->root.bo->tbo.base,\n\t\t\t\t    2 + num_fences);\n}\n\n \nvoid amdgpu_vm_move_to_lru_tail(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm)\n{\n\tspin_lock(&adev->mman.bdev.lru_lock);\n\tttm_lru_bulk_move_tail(&vm->lru_bulk_move);\n\tspin_unlock(&adev->mman.bdev.lru_lock);\n}\n\n \nstatic int amdgpu_vm_init_entities(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_vm *vm)\n{\n\tint r;\n\n\tr = drm_sched_entity_init(&vm->immediate, DRM_SCHED_PRIORITY_NORMAL,\n\t\t\t\t  adev->vm_manager.vm_pte_scheds,\n\t\t\t\t  adev->vm_manager.vm_pte_num_scheds, NULL);\n\tif (r)\n\t\tgoto error;\n\n\treturn drm_sched_entity_init(&vm->delayed, DRM_SCHED_PRIORITY_NORMAL,\n\t\t\t\t     adev->vm_manager.vm_pte_scheds,\n\t\t\t\t     adev->vm_manager.vm_pte_num_scheds, NULL);\n\nerror:\n\tdrm_sched_entity_destroy(&vm->immediate);\n\treturn r;\n}\n\n \nstatic void amdgpu_vm_fini_entities(struct amdgpu_vm *vm)\n{\n\tdrm_sched_entity_destroy(&vm->immediate);\n\tdrm_sched_entity_destroy(&vm->delayed);\n}\n\n \nuint64_t amdgpu_vm_generation(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tuint64_t result = (u64)atomic_read(&adev->vram_lost_counter) << 32;\n\n\tif (!vm)\n\t\treturn result;\n\n\tresult += vm->generation;\n\t \n\tif (drm_sched_entity_error(&vm->delayed))\n\t\t++result;\n\n\treturn result;\n}\n\n \nint amdgpu_vm_validate_pt_bos(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\t      int (*validate)(void *p, struct amdgpu_bo *bo),\n\t\t\t      void *param)\n{\n\tstruct amdgpu_vm_bo_base *bo_base;\n\tstruct amdgpu_bo *shadow;\n\tstruct amdgpu_bo *bo;\n\tint r;\n\n\tif (drm_sched_entity_error(&vm->delayed)) {\n\t\t++vm->generation;\n\t\tamdgpu_vm_bo_reset_state_machine(vm);\n\t\tamdgpu_vm_fini_entities(vm);\n\t\tr = amdgpu_vm_init_entities(adev, vm);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tspin_lock(&vm->status_lock);\n\twhile (!list_empty(&vm->evicted)) {\n\t\tbo_base = list_first_entry(&vm->evicted,\n\t\t\t\t\t   struct amdgpu_vm_bo_base,\n\t\t\t\t\t   vm_status);\n\t\tspin_unlock(&vm->status_lock);\n\n\t\tbo = bo_base->bo;\n\t\tshadow = amdgpu_bo_shadowed(bo);\n\n\t\tr = validate(param, bo);\n\t\tif (r)\n\t\t\treturn r;\n\t\tif (shadow) {\n\t\t\tr = validate(param, shadow);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\tif (bo->tbo.type != ttm_bo_type_kernel) {\n\t\t\tamdgpu_vm_bo_moved(bo_base);\n\t\t} else {\n\t\t\tvm->update_funcs->map_table(to_amdgpu_bo_vm(bo));\n\t\t\tamdgpu_vm_bo_relocated(bo_base);\n\t\t}\n\t\tspin_lock(&vm->status_lock);\n\t}\n\tspin_unlock(&vm->status_lock);\n\n\tamdgpu_vm_eviction_lock(vm);\n\tvm->evicting = false;\n\tamdgpu_vm_eviction_unlock(vm);\n\n\treturn 0;\n}\n\n \nbool amdgpu_vm_ready(struct amdgpu_vm *vm)\n{\n\tbool empty;\n\tbool ret;\n\n\tamdgpu_vm_eviction_lock(vm);\n\tret = !vm->evicting;\n\tamdgpu_vm_eviction_unlock(vm);\n\n\tspin_lock(&vm->status_lock);\n\tempty = list_empty(&vm->evicted);\n\tspin_unlock(&vm->status_lock);\n\n\treturn ret && empty;\n}\n\n \nvoid amdgpu_vm_check_compute_bug(struct amdgpu_device *adev)\n{\n\tconst struct amdgpu_ip_block *ip_block;\n\tbool has_compute_vm_bug;\n\tstruct amdgpu_ring *ring;\n\tint i;\n\n\thas_compute_vm_bug = false;\n\n\tip_block = amdgpu_device_ip_get_ip_block(adev, AMD_IP_BLOCK_TYPE_GFX);\n\tif (ip_block) {\n\t\t \n\t\tif (ip_block->version->major <= 7)\n\t\t\thas_compute_vm_bug = true;\n\t\telse if (ip_block->version->major == 8)\n\t\t\tif (adev->gfx.mec_fw_version < 673)\n\t\t\t\thas_compute_vm_bug = true;\n\t}\n\n\tfor (i = 0; i < adev->num_rings; i++) {\n\t\tring = adev->rings[i];\n\t\tif (ring->funcs->type == AMDGPU_RING_TYPE_COMPUTE)\n\t\t\t \n\t\t\tring->has_compute_vm_bug = has_compute_vm_bug;\n\t\telse\n\t\t\tring->has_compute_vm_bug = false;\n\t}\n}\n\n \nbool amdgpu_vm_need_pipeline_sync(struct amdgpu_ring *ring,\n\t\t\t\t  struct amdgpu_job *job)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\n\tif (job->vmid == 0)\n\t\treturn false;\n\n\tif (job->vm_needs_flush || ring->has_compute_vm_bug)\n\t\treturn true;\n\n\tif (ring->funcs->emit_gds_switch && job->gds_switch_needed)\n\t\treturn true;\n\n\tif (amdgpu_vmid_had_gpu_reset(adev, &id_mgr->ids[job->vmid]))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nint amdgpu_vm_flush(struct amdgpu_ring *ring, struct amdgpu_job *job,\n\t\t    bool need_pipe_sync)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tstruct amdgpu_vmid *id = &id_mgr->ids[job->vmid];\n\tbool spm_update_needed = job->spm_update_needed;\n\tbool gds_switch_needed = ring->funcs->emit_gds_switch &&\n\t\tjob->gds_switch_needed;\n\tbool vm_flush_needed = job->vm_needs_flush;\n\tstruct dma_fence *fence = NULL;\n\tbool pasid_mapping_needed = false;\n\tunsigned patch_offset = 0;\n\tint r;\n\n\tif (amdgpu_vmid_had_gpu_reset(adev, id)) {\n\t\tgds_switch_needed = true;\n\t\tvm_flush_needed = true;\n\t\tpasid_mapping_needed = true;\n\t\tspm_update_needed = true;\n\t}\n\n\tmutex_lock(&id_mgr->lock);\n\tif (id->pasid != job->pasid || !id->pasid_mapping ||\n\t    !dma_fence_is_signaled(id->pasid_mapping))\n\t\tpasid_mapping_needed = true;\n\tmutex_unlock(&id_mgr->lock);\n\n\tgds_switch_needed &= !!ring->funcs->emit_gds_switch;\n\tvm_flush_needed &= !!ring->funcs->emit_vm_flush  &&\n\t\t\tjob->vm_pd_addr != AMDGPU_BO_INVALID_OFFSET;\n\tpasid_mapping_needed &= adev->gmc.gmc_funcs->emit_pasid_mapping &&\n\t\tring->funcs->emit_wreg;\n\n\tif (!vm_flush_needed && !gds_switch_needed && !need_pipe_sync)\n\t\treturn 0;\n\n\tamdgpu_ring_ib_begin(ring);\n\tif (ring->funcs->init_cond_exec)\n\t\tpatch_offset = amdgpu_ring_init_cond_exec(ring);\n\n\tif (need_pipe_sync)\n\t\tamdgpu_ring_emit_pipeline_sync(ring);\n\n\tif (vm_flush_needed) {\n\t\ttrace_amdgpu_vm_flush(ring, job->vmid, job->vm_pd_addr);\n\t\tamdgpu_ring_emit_vm_flush(ring, job->vmid, job->vm_pd_addr);\n\t}\n\n\tif (pasid_mapping_needed)\n\t\tamdgpu_gmc_emit_pasid_mapping(ring, job->vmid, job->pasid);\n\n\tif (spm_update_needed && adev->gfx.rlc.funcs->update_spm_vmid)\n\t\tadev->gfx.rlc.funcs->update_spm_vmid(adev, job->vmid);\n\n\tif (!ring->is_mes_queue && ring->funcs->emit_gds_switch &&\n\t    gds_switch_needed) {\n\t\tamdgpu_ring_emit_gds_switch(ring, job->vmid, job->gds_base,\n\t\t\t\t\t    job->gds_size, job->gws_base,\n\t\t\t\t\t    job->gws_size, job->oa_base,\n\t\t\t\t\t    job->oa_size);\n\t}\n\n\tif (vm_flush_needed || pasid_mapping_needed) {\n\t\tr = amdgpu_fence_emit(ring, &fence, NULL, 0);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (vm_flush_needed) {\n\t\tmutex_lock(&id_mgr->lock);\n\t\tdma_fence_put(id->last_flush);\n\t\tid->last_flush = dma_fence_get(fence);\n\t\tid->current_gpu_reset_count =\n\t\t\tatomic_read(&adev->gpu_reset_counter);\n\t\tmutex_unlock(&id_mgr->lock);\n\t}\n\n\tif (pasid_mapping_needed) {\n\t\tmutex_lock(&id_mgr->lock);\n\t\tid->pasid = job->pasid;\n\t\tdma_fence_put(id->pasid_mapping);\n\t\tid->pasid_mapping = dma_fence_get(fence);\n\t\tmutex_unlock(&id_mgr->lock);\n\t}\n\tdma_fence_put(fence);\n\n\tif (ring->funcs->patch_cond_exec)\n\t\tamdgpu_ring_patch_cond_exec(ring, patch_offset);\n\n\t \n\tif (ring->funcs->emit_switch_buffer) {\n\t\tamdgpu_ring_emit_switch_buffer(ring);\n\t\tamdgpu_ring_emit_switch_buffer(ring);\n\t}\n\tamdgpu_ring_ib_end(ring);\n\treturn 0;\n}\n\n \nstruct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm,\n\t\t\t\t       struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_vm_bo_base *base;\n\n\tfor (base = bo->vm_bo; base; base = base->next) {\n\t\tif (base->vm != vm)\n\t\t\tcontinue;\n\n\t\treturn container_of(base, struct amdgpu_bo_va, base);\n\t}\n\treturn NULL;\n}\n\n \nuint64_t amdgpu_vm_map_gart(const dma_addr_t *pages_addr, uint64_t addr)\n{\n\tuint64_t result;\n\n\t \n\tresult = pages_addr[addr >> PAGE_SHIFT];\n\n\t \n\tresult |= addr & (~PAGE_MASK);\n\n\tresult &= 0xFFFFFFFFFFFFF000ULL;\n\n\treturn result;\n}\n\n \nint amdgpu_vm_update_pdes(struct amdgpu_device *adev,\n\t\t\t  struct amdgpu_vm *vm, bool immediate)\n{\n\tstruct amdgpu_vm_update_params params;\n\tstruct amdgpu_vm_bo_base *entry;\n\tbool flush_tlb_needed = false;\n\tLIST_HEAD(relocated);\n\tint r, idx;\n\n\tspin_lock(&vm->status_lock);\n\tlist_splice_init(&vm->relocated, &relocated);\n\tspin_unlock(&vm->status_lock);\n\n\tif (list_empty(&relocated))\n\t\treturn 0;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn -ENODEV;\n\n\tmemset(&params, 0, sizeof(params));\n\tparams.adev = adev;\n\tparams.vm = vm;\n\tparams.immediate = immediate;\n\n\tr = vm->update_funcs->prepare(&params, NULL, AMDGPU_SYNC_EXPLICIT);\n\tif (r)\n\t\tgoto error;\n\n\tlist_for_each_entry(entry, &relocated, vm_status) {\n\t\t \n\t\tflush_tlb_needed |= entry->moved;\n\n\t\tr = amdgpu_vm_pde_update(&params, entry);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tr = vm->update_funcs->commit(&params, &vm->last_update);\n\tif (r)\n\t\tgoto error;\n\n\tif (flush_tlb_needed)\n\t\tatomic64_inc(&vm->tlb_seq);\n\n\twhile (!list_empty(&relocated)) {\n\t\tentry = list_first_entry(&relocated, struct amdgpu_vm_bo_base,\n\t\t\t\t\t vm_status);\n\t\tamdgpu_vm_bo_idle(entry);\n\t}\n\nerror:\n\tdrm_dev_exit(idx);\n\treturn r;\n}\n\n \nstatic void amdgpu_vm_tlb_seq_cb(struct dma_fence *fence,\n\t\t\t\t struct dma_fence_cb *cb)\n{\n\tstruct amdgpu_vm_tlb_seq_struct *tlb_cb;\n\n\ttlb_cb = container_of(cb, typeof(*tlb_cb), cb);\n\tatomic64_inc(&tlb_cb->vm->tlb_seq);\n\tkfree(tlb_cb);\n}\n\n \nint amdgpu_vm_update_range(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t\t   bool immediate, bool unlocked, bool flush_tlb,\n\t\t\t   struct dma_resv *resv, uint64_t start, uint64_t last,\n\t\t\t   uint64_t flags, uint64_t offset, uint64_t vram_base,\n\t\t\t   struct ttm_resource *res, dma_addr_t *pages_addr,\n\t\t\t   struct dma_fence **fence)\n{\n\tstruct amdgpu_vm_update_params params;\n\tstruct amdgpu_vm_tlb_seq_struct *tlb_cb;\n\tstruct amdgpu_res_cursor cursor;\n\tenum amdgpu_sync_mode sync_mode;\n\tint r, idx;\n\n\tif (!drm_dev_enter(adev_to_drm(adev), &idx))\n\t\treturn -ENODEV;\n\n\ttlb_cb = kmalloc(sizeof(*tlb_cb), GFP_KERNEL);\n\tif (!tlb_cb) {\n\t\tr = -ENOMEM;\n\t\tgoto error_unlock;\n\t}\n\n\t \n\tflush_tlb |= adev->gmc.xgmi.num_physical_nodes &&\n\t\t     adev->ip_versions[GC_HWIP][0] == IP_VERSION(9, 4, 0);\n\n\t \n\tflush_tlb |= adev->ip_versions[GC_HWIP][0] < IP_VERSION(9, 0, 0);\n\n\tmemset(&params, 0, sizeof(params));\n\tparams.adev = adev;\n\tparams.vm = vm;\n\tparams.immediate = immediate;\n\tparams.pages_addr = pages_addr;\n\tparams.unlocked = unlocked;\n\n\t \n\tif (!(flags & AMDGPU_PTE_VALID))\n\t\tsync_mode = AMDGPU_SYNC_EQ_OWNER;\n\telse\n\t\tsync_mode = AMDGPU_SYNC_EXPLICIT;\n\n\tamdgpu_vm_eviction_lock(vm);\n\tif (vm->evicting) {\n\t\tr = -EBUSY;\n\t\tgoto error_free;\n\t}\n\n\tif (!unlocked && !dma_fence_is_signaled(vm->last_unlocked)) {\n\t\tstruct dma_fence *tmp = dma_fence_get_stub();\n\n\t\tamdgpu_bo_fence(vm->root.bo, vm->last_unlocked, true);\n\t\tswap(vm->last_unlocked, tmp);\n\t\tdma_fence_put(tmp);\n\t}\n\n\tr = vm->update_funcs->prepare(&params, resv, sync_mode);\n\tif (r)\n\t\tgoto error_free;\n\n\tamdgpu_res_first(pages_addr ? NULL : res, offset,\n\t\t\t (last - start + 1) * AMDGPU_GPU_PAGE_SIZE, &cursor);\n\twhile (cursor.remaining) {\n\t\tuint64_t tmp, num_entries, addr;\n\n\t\tnum_entries = cursor.size >> AMDGPU_GPU_PAGE_SHIFT;\n\t\tif (pages_addr) {\n\t\t\tbool contiguous = true;\n\n\t\t\tif (num_entries > AMDGPU_GPU_PAGES_IN_CPU_PAGE) {\n\t\t\t\tuint64_t pfn = cursor.start >> PAGE_SHIFT;\n\t\t\t\tuint64_t count;\n\n\t\t\t\tcontiguous = pages_addr[pfn + 1] ==\n\t\t\t\t\tpages_addr[pfn] + PAGE_SIZE;\n\n\t\t\t\ttmp = num_entries /\n\t\t\t\t\tAMDGPU_GPU_PAGES_IN_CPU_PAGE;\n\t\t\t\tfor (count = 2; count < tmp; ++count) {\n\t\t\t\t\tuint64_t idx = pfn + count;\n\n\t\t\t\t\tif (contiguous != (pages_addr[idx] ==\n\t\t\t\t\t    pages_addr[idx - 1] + PAGE_SIZE))\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (!contiguous)\n\t\t\t\t\tcount--;\n\t\t\t\tnum_entries = count *\n\t\t\t\t\tAMDGPU_GPU_PAGES_IN_CPU_PAGE;\n\t\t\t}\n\n\t\t\tif (!contiguous) {\n\t\t\t\taddr = cursor.start;\n\t\t\t\tparams.pages_addr = pages_addr;\n\t\t\t} else {\n\t\t\t\taddr = pages_addr[cursor.start >> PAGE_SHIFT];\n\t\t\t\tparams.pages_addr = NULL;\n\t\t\t}\n\n\t\t} else if (flags & (AMDGPU_PTE_VALID | AMDGPU_PTE_PRT)) {\n\t\t\taddr = vram_base + cursor.start;\n\t\t} else {\n\t\t\taddr = 0;\n\t\t}\n\n\t\ttmp = start + num_entries;\n\t\tr = amdgpu_vm_ptes_update(&params, start, tmp, addr, flags);\n\t\tif (r)\n\t\t\tgoto error_free;\n\n\t\tamdgpu_res_next(&cursor, num_entries * AMDGPU_GPU_PAGE_SIZE);\n\t\tstart = tmp;\n\t}\n\n\tr = vm->update_funcs->commit(&params, fence);\n\n\tif (flush_tlb || params.table_freed) {\n\t\ttlb_cb->vm = vm;\n\t\tif (fence && *fence &&\n\t\t    !dma_fence_add_callback(*fence, &tlb_cb->cb,\n\t\t\t\t\t   amdgpu_vm_tlb_seq_cb)) {\n\t\t\tdma_fence_put(vm->last_tlb_flush);\n\t\t\tvm->last_tlb_flush = dma_fence_get(*fence);\n\t\t} else {\n\t\t\tamdgpu_vm_tlb_seq_cb(NULL, &tlb_cb->cb);\n\t\t}\n\t\ttlb_cb = NULL;\n\t}\n\nerror_free:\n\tkfree(tlb_cb);\n\nerror_unlock:\n\tamdgpu_vm_eviction_unlock(vm);\n\tdrm_dev_exit(idx);\n\treturn r;\n}\n\nstatic void amdgpu_vm_bo_get_memory(struct amdgpu_bo_va *bo_va,\n\t\t\t\t    struct amdgpu_mem_stats *stats)\n{\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\n\tif (!bo)\n\t\treturn;\n\n\t \n\tif (bo->tbo.base.resv != vm->root.bo->tbo.base.resv &&\n\t    !dma_resv_trylock(bo->tbo.base.resv))\n\t\treturn;\n\n\tamdgpu_bo_get_memory(bo, stats);\n\tif (bo->tbo.base.resv != vm->root.bo->tbo.base.resv)\n\t    dma_resv_unlock(bo->tbo.base.resv);\n}\n\nvoid amdgpu_vm_get_memory(struct amdgpu_vm *vm,\n\t\t\t  struct amdgpu_mem_stats *stats)\n{\n\tstruct amdgpu_bo_va *bo_va, *tmp;\n\n\tspin_lock(&vm->status_lock);\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->idle, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->evicted, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->relocated, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->moved, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->invalidated, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->done, base.vm_status)\n\t\tamdgpu_vm_bo_get_memory(bo_va, stats);\n\tspin_unlock(&vm->status_lock);\n}\n\n \nint amdgpu_vm_bo_update(struct amdgpu_device *adev, struct amdgpu_bo_va *bo_va,\n\t\t\tbool clear)\n{\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tdma_addr_t *pages_addr = NULL;\n\tstruct ttm_resource *mem;\n\tstruct dma_fence **last_update;\n\tbool flush_tlb = clear;\n\tstruct dma_resv *resv;\n\tuint64_t vram_base;\n\tuint64_t flags;\n\tint r;\n\n\tif (clear || !bo) {\n\t\tmem = NULL;\n\t\tresv = vm->root.bo->tbo.base.resv;\n\t} else {\n\t\tstruct drm_gem_object *obj = &bo->tbo.base;\n\n\t\tresv = bo->tbo.base.resv;\n\t\tif (obj->import_attach && bo_va->is_xgmi) {\n\t\t\tstruct dma_buf *dma_buf = obj->import_attach->dmabuf;\n\t\t\tstruct drm_gem_object *gobj = dma_buf->priv;\n\t\t\tstruct amdgpu_bo *abo = gem_to_amdgpu_bo(gobj);\n\n\t\t\tif (abo->tbo.resource &&\n\t\t\t    abo->tbo.resource->mem_type == TTM_PL_VRAM)\n\t\t\t\tbo = gem_to_amdgpu_bo(gobj);\n\t\t}\n\t\tmem = bo->tbo.resource;\n\t\tif (mem && (mem->mem_type == TTM_PL_TT ||\n\t\t\t    mem->mem_type == AMDGPU_PL_PREEMPT))\n\t\t\tpages_addr = bo->tbo.ttm->dma_address;\n\t}\n\n\tif (bo) {\n\t\tstruct amdgpu_device *bo_adev;\n\n\t\tflags = amdgpu_ttm_tt_pte_flags(adev, bo->tbo.ttm, mem);\n\n\t\tif (amdgpu_bo_encrypted(bo))\n\t\t\tflags |= AMDGPU_PTE_TMZ;\n\n\t\tbo_adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\t\tvram_base = bo_adev->vm_manager.vram_base_offset;\n\t} else {\n\t\tflags = 0x0;\n\t\tvram_base = 0;\n\t}\n\n\tif (clear || (bo && bo->tbo.base.resv ==\n\t\t      vm->root.bo->tbo.base.resv))\n\t\tlast_update = &vm->last_update;\n\telse\n\t\tlast_update = &bo_va->last_pt_update;\n\n\tif (!clear && bo_va->base.moved) {\n\t\tflush_tlb = true;\n\t\tlist_splice_init(&bo_va->valids, &bo_va->invalids);\n\n\t} else if (bo_va->cleared != clear) {\n\t\tlist_splice_init(&bo_va->valids, &bo_va->invalids);\n\t}\n\n\tlist_for_each_entry(mapping, &bo_va->invalids, list) {\n\t\tuint64_t update_flags = flags;\n\n\t\t \n\t\tif (!(mapping->flags & AMDGPU_PTE_READABLE))\n\t\t\tupdate_flags &= ~AMDGPU_PTE_READABLE;\n\t\tif (!(mapping->flags & AMDGPU_PTE_WRITEABLE))\n\t\t\tupdate_flags &= ~AMDGPU_PTE_WRITEABLE;\n\n\t\t \n\t\tamdgpu_gmc_get_vm_pte(adev, mapping, &update_flags);\n\n\t\ttrace_amdgpu_vm_bo_update(mapping);\n\n\t\tr = amdgpu_vm_update_range(adev, vm, false, false, flush_tlb,\n\t\t\t\t\t   resv, mapping->start, mapping->last,\n\t\t\t\t\t   update_flags, mapping->offset,\n\t\t\t\t\t   vram_base, mem, pages_addr,\n\t\t\t\t\t   last_update);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tif (bo && bo->tbo.base.resv == vm->root.bo->tbo.base.resv) {\n\t\tuint32_t mem_type = bo->tbo.resource->mem_type;\n\n\t\tif (!(bo->preferred_domains &\n\t\t      amdgpu_mem_type_to_domain(mem_type)))\n\t\t\tamdgpu_vm_bo_evicted(&bo_va->base);\n\t\telse\n\t\t\tamdgpu_vm_bo_idle(&bo_va->base);\n\t} else {\n\t\tamdgpu_vm_bo_done(&bo_va->base);\n\t}\n\n\tlist_splice_init(&bo_va->invalids, &bo_va->valids);\n\tbo_va->cleared = clear;\n\tbo_va->base.moved = false;\n\n\tif (trace_amdgpu_vm_bo_mapping_enabled()) {\n\t\tlist_for_each_entry(mapping, &bo_va->valids, list)\n\t\t\ttrace_amdgpu_vm_bo_mapping(mapping);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_vm_update_prt_state(struct amdgpu_device *adev)\n{\n\tunsigned long flags;\n\tbool enable;\n\n\tspin_lock_irqsave(&adev->vm_manager.prt_lock, flags);\n\tenable = !!atomic_read(&adev->vm_manager.num_prt_users);\n\tadev->gmc.gmc_funcs->set_prt(adev, enable);\n\tspin_unlock_irqrestore(&adev->vm_manager.prt_lock, flags);\n}\n\n \nstatic void amdgpu_vm_prt_get(struct amdgpu_device *adev)\n{\n\tif (!adev->gmc.gmc_funcs->set_prt)\n\t\treturn;\n\n\tif (atomic_inc_return(&adev->vm_manager.num_prt_users) == 1)\n\t\tamdgpu_vm_update_prt_state(adev);\n}\n\n \nstatic void amdgpu_vm_prt_put(struct amdgpu_device *adev)\n{\n\tif (atomic_dec_return(&adev->vm_manager.num_prt_users) == 0)\n\t\tamdgpu_vm_update_prt_state(adev);\n}\n\n \nstatic void amdgpu_vm_prt_cb(struct dma_fence *fence, struct dma_fence_cb *_cb)\n{\n\tstruct amdgpu_prt_cb *cb = container_of(_cb, struct amdgpu_prt_cb, cb);\n\n\tamdgpu_vm_prt_put(cb->adev);\n\tkfree(cb);\n}\n\n \nstatic void amdgpu_vm_add_prt_cb(struct amdgpu_device *adev,\n\t\t\t\t struct dma_fence *fence)\n{\n\tstruct amdgpu_prt_cb *cb;\n\n\tif (!adev->gmc.gmc_funcs->set_prt)\n\t\treturn;\n\n\tcb = kmalloc(sizeof(struct amdgpu_prt_cb), GFP_KERNEL);\n\tif (!cb) {\n\t\t \n\t\tif (fence)\n\t\t\tdma_fence_wait(fence, false);\n\n\t\tamdgpu_vm_prt_put(adev);\n\t} else {\n\t\tcb->adev = adev;\n\t\tif (!fence || dma_fence_add_callback(fence, &cb->cb,\n\t\t\t\t\t\t     amdgpu_vm_prt_cb))\n\t\t\tamdgpu_vm_prt_cb(fence, &cb->cb);\n\t}\n}\n\n \nstatic void amdgpu_vm_free_mapping(struct amdgpu_device *adev,\n\t\t\t\t   struct amdgpu_vm *vm,\n\t\t\t\t   struct amdgpu_bo_va_mapping *mapping,\n\t\t\t\t   struct dma_fence *fence)\n{\n\tif (mapping->flags & AMDGPU_PTE_PRT)\n\t\tamdgpu_vm_add_prt_cb(adev, fence);\n\tkfree(mapping);\n}\n\n \nstatic void amdgpu_vm_prt_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tstruct dma_resv *resv = vm->root.bo->tbo.base.resv;\n\tstruct dma_resv_iter cursor;\n\tstruct dma_fence *fence;\n\n\tdma_resv_for_each_fence(&cursor, resv, DMA_RESV_USAGE_BOOKKEEP, fence) {\n\t\t \n\t\tamdgpu_vm_prt_get(adev);\n\t\tamdgpu_vm_add_prt_cb(adev, fence);\n\t}\n}\n\n \nint amdgpu_vm_clear_freed(struct amdgpu_device *adev,\n\t\t\t  struct amdgpu_vm *vm,\n\t\t\t  struct dma_fence **fence)\n{\n\tstruct dma_resv *resv = vm->root.bo->tbo.base.resv;\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tuint64_t init_pte_value = 0;\n\tstruct dma_fence *f = NULL;\n\tint r;\n\n\twhile (!list_empty(&vm->freed)) {\n\t\tmapping = list_first_entry(&vm->freed,\n\t\t\tstruct amdgpu_bo_va_mapping, list);\n\t\tlist_del(&mapping->list);\n\n\t\tif (vm->pte_support_ats &&\n\t\t    mapping->start < AMDGPU_GMC_HOLE_START)\n\t\t\tinit_pte_value = AMDGPU_PTE_DEFAULT_ATC;\n\n\t\tr = amdgpu_vm_update_range(adev, vm, false, false, true, resv,\n\t\t\t\t\t   mapping->start, mapping->last,\n\t\t\t\t\t   init_pte_value, 0, 0, NULL, NULL,\n\t\t\t\t\t   &f);\n\t\tamdgpu_vm_free_mapping(adev, vm, mapping, f);\n\t\tif (r) {\n\t\t\tdma_fence_put(f);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (fence && f) {\n\t\tdma_fence_put(*fence);\n\t\t*fence = f;\n\t} else {\n\t\tdma_fence_put(f);\n\t}\n\n\treturn 0;\n\n}\n\n \nint amdgpu_vm_handle_moved(struct amdgpu_device *adev,\n\t\t\t   struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_bo_va *bo_va;\n\tstruct dma_resv *resv;\n\tbool clear;\n\tint r;\n\n\tspin_lock(&vm->status_lock);\n\twhile (!list_empty(&vm->moved)) {\n\t\tbo_va = list_first_entry(&vm->moved, struct amdgpu_bo_va,\n\t\t\t\t\t base.vm_status);\n\t\tspin_unlock(&vm->status_lock);\n\n\t\t \n\t\tr = amdgpu_vm_bo_update(adev, bo_va, false);\n\t\tif (r)\n\t\t\treturn r;\n\t\tspin_lock(&vm->status_lock);\n\t}\n\n\twhile (!list_empty(&vm->invalidated)) {\n\t\tbo_va = list_first_entry(&vm->invalidated, struct amdgpu_bo_va,\n\t\t\t\t\t base.vm_status);\n\t\tresv = bo_va->base.bo->tbo.base.resv;\n\t\tspin_unlock(&vm->status_lock);\n\n\t\t \n\t\tif (!amdgpu_vm_debug && dma_resv_trylock(resv))\n\t\t\tclear = false;\n\t\t \n\t\telse\n\t\t\tclear = true;\n\n\t\tr = amdgpu_vm_bo_update(adev, bo_va, clear);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (!clear)\n\t\t\tdma_resv_unlock(resv);\n\t\tspin_lock(&vm->status_lock);\n\t}\n\tspin_unlock(&vm->status_lock);\n\n\treturn 0;\n}\n\n \nstruct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_vm *vm,\n\t\t\t\t      struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_bo_va *bo_va;\n\n\tbo_va = kzalloc(sizeof(struct amdgpu_bo_va), GFP_KERNEL);\n\tif (bo_va == NULL) {\n\t\treturn NULL;\n\t}\n\tamdgpu_vm_bo_base_init(&bo_va->base, vm, bo);\n\n\tbo_va->ref_count = 1;\n\tbo_va->last_pt_update = dma_fence_get_stub();\n\tINIT_LIST_HEAD(&bo_va->valids);\n\tINIT_LIST_HEAD(&bo_va->invalids);\n\n\tif (!bo)\n\t\treturn bo_va;\n\n\tdma_resv_assert_held(bo->tbo.base.resv);\n\tif (amdgpu_dmabuf_is_xgmi_accessible(adev, bo)) {\n\t\tbo_va->is_xgmi = true;\n\t\t \n\t\tamdgpu_xgmi_set_pstate(adev, AMDGPU_XGMI_PSTATE_MAX_VEGA20);\n\t}\n\n\treturn bo_va;\n}\n\n\n \nstatic void amdgpu_vm_bo_insert_map(struct amdgpu_device *adev,\n\t\t\t\t    struct amdgpu_bo_va *bo_va,\n\t\t\t\t    struct amdgpu_bo_va_mapping *mapping)\n{\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\n\tmapping->bo_va = bo_va;\n\tlist_add(&mapping->list, &bo_va->invalids);\n\tamdgpu_vm_it_insert(mapping, &vm->va);\n\n\tif (mapping->flags & AMDGPU_PTE_PRT)\n\t\tamdgpu_vm_prt_get(adev);\n\n\tif (bo && bo->tbo.base.resv == vm->root.bo->tbo.base.resv &&\n\t    !bo_va->base.moved) {\n\t\tamdgpu_vm_bo_moved(&bo_va->base);\n\t}\n\ttrace_amdgpu_vm_bo_map(bo_va, mapping);\n}\n\n \nint amdgpu_vm_bo_map(struct amdgpu_device *adev,\n\t\t     struct amdgpu_bo_va *bo_va,\n\t\t     uint64_t saddr, uint64_t offset,\n\t\t     uint64_t size, uint64_t flags)\n{\n\tstruct amdgpu_bo_va_mapping *mapping, *tmp;\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tuint64_t eaddr;\n\n\t \n\tif (saddr & ~PAGE_MASK || offset & ~PAGE_MASK || size & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (saddr + size <= saddr || offset + size <= offset)\n\t\treturn -EINVAL;\n\n\t \n\teaddr = saddr + size - 1;\n\tif ((bo && offset + size > amdgpu_bo_size(bo)) ||\n\t    (eaddr >= adev->vm_manager.max_pfn << AMDGPU_GPU_PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\tsaddr /= AMDGPU_GPU_PAGE_SIZE;\n\teaddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\ttmp = amdgpu_vm_it_iter_first(&vm->va, saddr, eaddr);\n\tif (tmp) {\n\t\t \n\t\tdev_err(adev->dev, \"bo %p va 0x%010Lx-0x%010Lx conflict with \"\n\t\t\t\"0x%010Lx-0x%010Lx\\n\", bo, saddr, eaddr,\n\t\t\ttmp->start, tmp->last + 1);\n\t\treturn -EINVAL;\n\t}\n\n\tmapping = kmalloc(sizeof(*mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\treturn -ENOMEM;\n\n\tmapping->start = saddr;\n\tmapping->last = eaddr;\n\tmapping->offset = offset;\n\tmapping->flags = flags;\n\n\tamdgpu_vm_bo_insert_map(adev, bo_va, mapping);\n\n\treturn 0;\n}\n\n \nint amdgpu_vm_bo_replace_map(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_bo_va *bo_va,\n\t\t\t     uint64_t saddr, uint64_t offset,\n\t\t\t     uint64_t size, uint64_t flags)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\tuint64_t eaddr;\n\tint r;\n\n\t \n\tif (saddr & ~PAGE_MASK || offset & ~PAGE_MASK || size & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (saddr + size <= saddr || offset + size <= offset)\n\t\treturn -EINVAL;\n\n\t \n\teaddr = saddr + size - 1;\n\tif ((bo && offset + size > amdgpu_bo_size(bo)) ||\n\t    (eaddr >= adev->vm_manager.max_pfn << AMDGPU_GPU_PAGE_SHIFT))\n\t\treturn -EINVAL;\n\n\t \n\tmapping = kmalloc(sizeof(*mapping), GFP_KERNEL);\n\tif (!mapping)\n\t\treturn -ENOMEM;\n\n\tr = amdgpu_vm_bo_clear_mappings(adev, bo_va->base.vm, saddr, size);\n\tif (r) {\n\t\tkfree(mapping);\n\t\treturn r;\n\t}\n\n\tsaddr /= AMDGPU_GPU_PAGE_SIZE;\n\teaddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tmapping->start = saddr;\n\tmapping->last = eaddr;\n\tmapping->offset = offset;\n\tmapping->flags = flags;\n\n\tamdgpu_vm_bo_insert_map(adev, bo_va, mapping);\n\n\treturn 0;\n}\n\n \nint amdgpu_vm_bo_unmap(struct amdgpu_device *adev,\n\t\t       struct amdgpu_bo_va *bo_va,\n\t\t       uint64_t saddr)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tbool valid = true;\n\n\tsaddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tlist_for_each_entry(mapping, &bo_va->valids, list) {\n\t\tif (mapping->start == saddr)\n\t\t\tbreak;\n\t}\n\n\tif (&mapping->list == &bo_va->valids) {\n\t\tvalid = false;\n\n\t\tlist_for_each_entry(mapping, &bo_va->invalids, list) {\n\t\t\tif (mapping->start == saddr)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (&mapping->list == &bo_va->invalids)\n\t\t\treturn -ENOENT;\n\t}\n\n\tlist_del(&mapping->list);\n\tamdgpu_vm_it_remove(mapping, &vm->va);\n\tmapping->bo_va = NULL;\n\ttrace_amdgpu_vm_bo_unmap(bo_va, mapping);\n\n\tif (valid)\n\t\tlist_add(&mapping->list, &vm->freed);\n\telse\n\t\tamdgpu_vm_free_mapping(adev, vm, mapping,\n\t\t\t\t       bo_va->last_pt_update);\n\n\treturn 0;\n}\n\n \nint amdgpu_vm_bo_clear_mappings(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_vm *vm,\n\t\t\t\tuint64_t saddr, uint64_t size)\n{\n\tstruct amdgpu_bo_va_mapping *before, *after, *tmp, *next;\n\tLIST_HEAD(removed);\n\tuint64_t eaddr;\n\n\teaddr = saddr + size - 1;\n\tsaddr /= AMDGPU_GPU_PAGE_SIZE;\n\teaddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\t \n\tbefore = kzalloc(sizeof(*before), GFP_KERNEL);\n\tif (!before)\n\t\treturn -ENOMEM;\n\tINIT_LIST_HEAD(&before->list);\n\n\tafter = kzalloc(sizeof(*after), GFP_KERNEL);\n\tif (!after) {\n\t\tkfree(before);\n\t\treturn -ENOMEM;\n\t}\n\tINIT_LIST_HEAD(&after->list);\n\n\t \n\ttmp = amdgpu_vm_it_iter_first(&vm->va, saddr, eaddr);\n\twhile (tmp) {\n\t\t \n\t\tif (tmp->start < saddr) {\n\t\t\tbefore->start = tmp->start;\n\t\t\tbefore->last = saddr - 1;\n\t\t\tbefore->offset = tmp->offset;\n\t\t\tbefore->flags = tmp->flags;\n\t\t\tbefore->bo_va = tmp->bo_va;\n\t\t\tlist_add(&before->list, &tmp->bo_va->invalids);\n\t\t}\n\n\t\t \n\t\tif (tmp->last > eaddr) {\n\t\t\tafter->start = eaddr + 1;\n\t\t\tafter->last = tmp->last;\n\t\t\tafter->offset = tmp->offset;\n\t\t\tafter->offset += (after->start - tmp->start) << PAGE_SHIFT;\n\t\t\tafter->flags = tmp->flags;\n\t\t\tafter->bo_va = tmp->bo_va;\n\t\t\tlist_add(&after->list, &tmp->bo_va->invalids);\n\t\t}\n\n\t\tlist_del(&tmp->list);\n\t\tlist_add(&tmp->list, &removed);\n\n\t\ttmp = amdgpu_vm_it_iter_next(tmp, saddr, eaddr);\n\t}\n\n\t \n\tlist_for_each_entry_safe(tmp, next, &removed, list) {\n\t\tamdgpu_vm_it_remove(tmp, &vm->va);\n\t\tlist_del(&tmp->list);\n\n\t\tif (tmp->start < saddr)\n\t\t    tmp->start = saddr;\n\t\tif (tmp->last > eaddr)\n\t\t    tmp->last = eaddr;\n\n\t\ttmp->bo_va = NULL;\n\t\tlist_add(&tmp->list, &vm->freed);\n\t\ttrace_amdgpu_vm_bo_unmap(NULL, tmp);\n\t}\n\n\t \n\tif (!list_empty(&before->list)) {\n\t\tstruct amdgpu_bo *bo = before->bo_va->base.bo;\n\n\t\tamdgpu_vm_it_insert(before, &vm->va);\n\t\tif (before->flags & AMDGPU_PTE_PRT)\n\t\t\tamdgpu_vm_prt_get(adev);\n\n\t\tif (bo && bo->tbo.base.resv == vm->root.bo->tbo.base.resv &&\n\t\t    !before->bo_va->base.moved)\n\t\t\tamdgpu_vm_bo_moved(&before->bo_va->base);\n\t} else {\n\t\tkfree(before);\n\t}\n\n\t \n\tif (!list_empty(&after->list)) {\n\t\tstruct amdgpu_bo *bo = after->bo_va->base.bo;\n\n\t\tamdgpu_vm_it_insert(after, &vm->va);\n\t\tif (after->flags & AMDGPU_PTE_PRT)\n\t\t\tamdgpu_vm_prt_get(adev);\n\n\t\tif (bo && bo->tbo.base.resv == vm->root.bo->tbo.base.resv &&\n\t\t    !after->bo_va->base.moved)\n\t\t\tamdgpu_vm_bo_moved(&after->bo_va->base);\n\t} else {\n\t\tkfree(after);\n\t}\n\n\treturn 0;\n}\n\n \nstruct amdgpu_bo_va_mapping *amdgpu_vm_bo_lookup_mapping(struct amdgpu_vm *vm,\n\t\t\t\t\t\t\t uint64_t addr)\n{\n\treturn amdgpu_vm_it_iter_first(&vm->va, addr, addr);\n}\n\n \nvoid amdgpu_vm_bo_trace_cs(struct amdgpu_vm *vm, struct ww_acquire_ctx *ticket)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\n\tif (!trace_amdgpu_vm_bo_cs_enabled())\n\t\treturn;\n\n\tfor (mapping = amdgpu_vm_it_iter_first(&vm->va, 0, U64_MAX); mapping;\n\t     mapping = amdgpu_vm_it_iter_next(mapping, 0, U64_MAX)) {\n\t\tif (mapping->bo_va && mapping->bo_va->base.bo) {\n\t\t\tstruct amdgpu_bo *bo;\n\n\t\t\tbo = mapping->bo_va->base.bo;\n\t\t\tif (dma_resv_locking_ctx(bo->tbo.base.resv) !=\n\t\t\t    ticket)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\ttrace_amdgpu_vm_bo_cs(mapping);\n\t}\n}\n\n \nvoid amdgpu_vm_bo_del(struct amdgpu_device *adev,\n\t\t      struct amdgpu_bo_va *bo_va)\n{\n\tstruct amdgpu_bo_va_mapping *mapping, *next;\n\tstruct amdgpu_bo *bo = bo_va->base.bo;\n\tstruct amdgpu_vm *vm = bo_va->base.vm;\n\tstruct amdgpu_vm_bo_base **base;\n\n\tdma_resv_assert_held(vm->root.bo->tbo.base.resv);\n\n\tif (bo) {\n\t\tdma_resv_assert_held(bo->tbo.base.resv);\n\t\tif (bo->tbo.base.resv == vm->root.bo->tbo.base.resv)\n\t\t\tttm_bo_set_bulk_move(&bo->tbo, NULL);\n\n\t\tfor (base = &bo_va->base.bo->vm_bo; *base;\n\t\t     base = &(*base)->next) {\n\t\t\tif (*base != &bo_va->base)\n\t\t\t\tcontinue;\n\n\t\t\t*base = bo_va->base.next;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_lock(&vm->status_lock);\n\tlist_del(&bo_va->base.vm_status);\n\tspin_unlock(&vm->status_lock);\n\n\tlist_for_each_entry_safe(mapping, next, &bo_va->valids, list) {\n\t\tlist_del(&mapping->list);\n\t\tamdgpu_vm_it_remove(mapping, &vm->va);\n\t\tmapping->bo_va = NULL;\n\t\ttrace_amdgpu_vm_bo_unmap(bo_va, mapping);\n\t\tlist_add(&mapping->list, &vm->freed);\n\t}\n\tlist_for_each_entry_safe(mapping, next, &bo_va->invalids, list) {\n\t\tlist_del(&mapping->list);\n\t\tamdgpu_vm_it_remove(mapping, &vm->va);\n\t\tamdgpu_vm_free_mapping(adev, vm, mapping,\n\t\t\t\t       bo_va->last_pt_update);\n\t}\n\n\tdma_fence_put(bo_va->last_pt_update);\n\n\tif (bo && bo_va->is_xgmi)\n\t\tamdgpu_xgmi_set_pstate(adev, AMDGPU_XGMI_PSTATE_MIN);\n\n\tkfree(bo_va);\n}\n\n \nbool amdgpu_vm_evictable(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_vm_bo_base *bo_base = bo->vm_bo;\n\n\t \n\tif (!bo_base || !bo_base->vm)\n\t\treturn true;\n\n\t \n\tif (!dma_resv_test_signaled(bo->tbo.base.resv, DMA_RESV_USAGE_BOOKKEEP))\n\t\treturn false;\n\n\t \n\tif (!amdgpu_vm_eviction_trylock(bo_base->vm))\n\t\treturn false;\n\n\t \n\tif (!dma_fence_is_signaled(bo_base->vm->last_unlocked)) {\n\t\tamdgpu_vm_eviction_unlock(bo_base->vm);\n\t\treturn false;\n\t}\n\n\tbo_base->vm->evicting = true;\n\tamdgpu_vm_eviction_unlock(bo_base->vm);\n\treturn true;\n}\n\n \nvoid amdgpu_vm_bo_invalidate(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_bo *bo, bool evicted)\n{\n\tstruct amdgpu_vm_bo_base *bo_base;\n\n\t \n\tif (bo->parent && (amdgpu_bo_shadowed(bo->parent) == bo))\n\t\tbo = bo->parent;\n\n\tfor (bo_base = bo->vm_bo; bo_base; bo_base = bo_base->next) {\n\t\tstruct amdgpu_vm *vm = bo_base->vm;\n\n\t\tif (evicted && bo->tbo.base.resv == vm->root.bo->tbo.base.resv) {\n\t\t\tamdgpu_vm_bo_evicted(bo_base);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bo_base->moved)\n\t\t\tcontinue;\n\t\tbo_base->moved = true;\n\n\t\tif (bo->tbo.type == ttm_bo_type_kernel)\n\t\t\tamdgpu_vm_bo_relocated(bo_base);\n\t\telse if (bo->tbo.base.resv == vm->root.bo->tbo.base.resv)\n\t\t\tamdgpu_vm_bo_moved(bo_base);\n\t\telse\n\t\t\tamdgpu_vm_bo_invalidated(bo_base);\n\t}\n}\n\n \nstatic uint32_t amdgpu_vm_get_block_size(uint64_t vm_size)\n{\n\t \n\tunsigned bits = ilog2(vm_size) + 18;\n\n\t \n\tif (vm_size <= 8)\n\t\treturn (bits - 9);\n\telse\n\t\treturn ((bits + 3) / 2);\n}\n\n \nvoid amdgpu_vm_adjust_size(struct amdgpu_device *adev, uint32_t min_vm_size,\n\t\t\t   uint32_t fragment_size_default, unsigned max_level,\n\t\t\t   unsigned max_bits)\n{\n\tunsigned int max_size = 1 << (max_bits - 30);\n\tunsigned int vm_size;\n\tuint64_t tmp;\n\n\t \n\tif (amdgpu_vm_size != -1) {\n\t\tvm_size = amdgpu_vm_size;\n\t\tif (vm_size > max_size) {\n\t\t\tdev_warn(adev->dev, \"VM size (%d) too large, max is %u GB\\n\",\n\t\t\t\t amdgpu_vm_size, max_size);\n\t\t\tvm_size = max_size;\n\t\t}\n\t} else {\n\t\tstruct sysinfo si;\n\t\tunsigned int phys_ram_gb;\n\n\t\t \n\t\tsi_meminfo(&si);\n\t\tphys_ram_gb = ((uint64_t)si.totalram * si.mem_unit +\n\t\t\t       (1 << 30) - 1) >> 30;\n\t\tvm_size = roundup_pow_of_two(\n\t\t\tmin(max(phys_ram_gb * 3, min_vm_size), max_size));\n\t}\n\n\tadev->vm_manager.max_pfn = (uint64_t)vm_size << 18;\n\n\ttmp = roundup_pow_of_two(adev->vm_manager.max_pfn);\n\tif (amdgpu_vm_block_size != -1)\n\t\ttmp >>= amdgpu_vm_block_size - 9;\n\ttmp = DIV_ROUND_UP(fls64(tmp) - 1, 9) - 1;\n\tadev->vm_manager.num_level = min(max_level, (unsigned)tmp);\n\tswitch (adev->vm_manager.num_level) {\n\tcase 3:\n\t\tadev->vm_manager.root_level = AMDGPU_VM_PDB2;\n\t\tbreak;\n\tcase 2:\n\t\tadev->vm_manager.root_level = AMDGPU_VM_PDB1;\n\t\tbreak;\n\tcase 1:\n\t\tadev->vm_manager.root_level = AMDGPU_VM_PDB0;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(adev->dev, \"VMPT only supports 2~4+1 levels\\n\");\n\t}\n\t \n\tif (amdgpu_vm_block_size != -1)\n\t\tadev->vm_manager.block_size =\n\t\t\tmin((unsigned)amdgpu_vm_block_size, max_bits\n\t\t\t    - AMDGPU_GPU_PAGE_SHIFT\n\t\t\t    - 9 * adev->vm_manager.num_level);\n\telse if (adev->vm_manager.num_level > 1)\n\t\tadev->vm_manager.block_size = 9;\n\telse\n\t\tadev->vm_manager.block_size = amdgpu_vm_get_block_size(tmp);\n\n\tif (amdgpu_vm_fragment_size == -1)\n\t\tadev->vm_manager.fragment_size = fragment_size_default;\n\telse\n\t\tadev->vm_manager.fragment_size = amdgpu_vm_fragment_size;\n\n\tDRM_INFO(\"vm size is %u GB, %u levels, block size is %u-bit, fragment size is %u-bit\\n\",\n\t\t vm_size, adev->vm_manager.num_level + 1,\n\t\t adev->vm_manager.block_size,\n\t\t adev->vm_manager.fragment_size);\n}\n\n \nlong amdgpu_vm_wait_idle(struct amdgpu_vm *vm, long timeout)\n{\n\ttimeout = dma_resv_wait_timeout(vm->root.bo->tbo.base.resv,\n\t\t\t\t\tDMA_RESV_USAGE_BOOKKEEP,\n\t\t\t\t\ttrue, timeout);\n\tif (timeout <= 0)\n\t\treturn timeout;\n\n\treturn dma_fence_wait_timeout(vm->last_unlocked, true, timeout);\n}\n\n \nint amdgpu_vm_init(struct amdgpu_device *adev, struct amdgpu_vm *vm,\n\t\t   int32_t xcp_id)\n{\n\tstruct amdgpu_bo *root_bo;\n\tstruct amdgpu_bo_vm *root;\n\tint r, i;\n\n\tvm->va = RB_ROOT_CACHED;\n\tfor (i = 0; i < AMDGPU_MAX_VMHUBS; i++)\n\t\tvm->reserved_vmid[i] = NULL;\n\tINIT_LIST_HEAD(&vm->evicted);\n\tINIT_LIST_HEAD(&vm->relocated);\n\tINIT_LIST_HEAD(&vm->moved);\n\tINIT_LIST_HEAD(&vm->idle);\n\tINIT_LIST_HEAD(&vm->invalidated);\n\tspin_lock_init(&vm->status_lock);\n\tINIT_LIST_HEAD(&vm->freed);\n\tINIT_LIST_HEAD(&vm->done);\n\tINIT_LIST_HEAD(&vm->pt_freed);\n\tINIT_WORK(&vm->pt_free_work, amdgpu_vm_pt_free_work);\n\tINIT_KFIFO(vm->faults);\n\n\tr = amdgpu_vm_init_entities(adev, vm);\n\tif (r)\n\t\treturn r;\n\n\tvm->pte_support_ats = false;\n\tvm->is_compute_context = false;\n\n\tvm->use_cpu_for_update = !!(adev->vm_manager.vm_update_mode &\n\t\t\t\t    AMDGPU_VM_USE_CPU_FOR_GFX);\n\n\tDRM_DEBUG_DRIVER(\"VM update mode is %s\\n\",\n\t\t\t vm->use_cpu_for_update ? \"CPU\" : \"SDMA\");\n\tWARN_ONCE((vm->use_cpu_for_update &&\n\t\t   !amdgpu_gmc_vram_full_visible(&adev->gmc)),\n\t\t  \"CPU update of VM recommended only for large BAR system\\n\");\n\n\tif (vm->use_cpu_for_update)\n\t\tvm->update_funcs = &amdgpu_vm_cpu_funcs;\n\telse\n\t\tvm->update_funcs = &amdgpu_vm_sdma_funcs;\n\n\tvm->last_update = dma_fence_get_stub();\n\tvm->last_unlocked = dma_fence_get_stub();\n\tvm->last_tlb_flush = dma_fence_get_stub();\n\tvm->generation = 0;\n\n\tmutex_init(&vm->eviction_lock);\n\tvm->evicting = false;\n\n\tr = amdgpu_vm_pt_create(adev, vm, adev->vm_manager.root_level,\n\t\t\t\tfalse, &root, xcp_id);\n\tif (r)\n\t\tgoto error_free_delayed;\n\n\troot_bo = amdgpu_bo_ref(&root->bo);\n\tr = amdgpu_bo_reserve(root_bo, true);\n\tif (r) {\n\t\tamdgpu_bo_unref(&root->shadow);\n\t\tamdgpu_bo_unref(&root_bo);\n\t\tgoto error_free_delayed;\n\t}\n\n\tamdgpu_vm_bo_base_init(&vm->root, vm, root_bo);\n\tr = dma_resv_reserve_fences(root_bo->tbo.base.resv, 1);\n\tif (r)\n\t\tgoto error_free_root;\n\n\tr = amdgpu_vm_pt_clear(adev, vm, root, false);\n\tif (r)\n\t\tgoto error_free_root;\n\n\tamdgpu_bo_unreserve(vm->root.bo);\n\tamdgpu_bo_unref(&root_bo);\n\n\treturn 0;\n\nerror_free_root:\n\tamdgpu_vm_pt_free_root(adev, vm);\n\tamdgpu_bo_unreserve(vm->root.bo);\n\tamdgpu_bo_unref(&root_bo);\n\nerror_free_delayed:\n\tdma_fence_put(vm->last_tlb_flush);\n\tdma_fence_put(vm->last_unlocked);\n\tamdgpu_vm_fini_entities(vm);\n\n\treturn r;\n}\n\n \nint amdgpu_vm_make_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tbool pte_support_ats = (adev->asic_type == CHIP_RAVEN);\n\tint r;\n\n\tr = amdgpu_bo_reserve(vm->root.bo, true);\n\tif (r)\n\t\treturn r;\n\n\t \n\tif (pte_support_ats != vm->pte_support_ats) {\n\t\t \n\t\tif (!amdgpu_vm_pt_is_root_clean(adev, vm)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto unreserve_bo;\n\t\t}\n\n\t\tvm->pte_support_ats = pte_support_ats;\n\t\tr = amdgpu_vm_pt_clear(adev, vm, to_amdgpu_bo_vm(vm->root.bo),\n\t\t\t\t       false);\n\t\tif (r)\n\t\t\tgoto unreserve_bo;\n\t}\n\n\t \n\tvm->use_cpu_for_update = !!(adev->vm_manager.vm_update_mode &\n\t\t\t\t    AMDGPU_VM_USE_CPU_FOR_COMPUTE);\n\tDRM_DEBUG_DRIVER(\"VM update mode is %s\\n\",\n\t\t\t vm->use_cpu_for_update ? \"CPU\" : \"SDMA\");\n\tWARN_ONCE((vm->use_cpu_for_update &&\n\t\t   !amdgpu_gmc_vram_full_visible(&adev->gmc)),\n\t\t  \"CPU update of VM recommended only for large BAR system\\n\");\n\n\tif (vm->use_cpu_for_update) {\n\t\t \n\t\tr = amdgpu_bo_sync_wait(vm->root.bo,\n\t\t\t\t\tAMDGPU_FENCE_OWNER_UNDEFINED, true);\n\t\tif (r)\n\t\t\tgoto unreserve_bo;\n\n\t\tvm->update_funcs = &amdgpu_vm_cpu_funcs;\n\t\tr = amdgpu_vm_pt_map_tables(adev, vm);\n\t\tif (r)\n\t\t\tgoto unreserve_bo;\n\n\t} else {\n\t\tvm->update_funcs = &amdgpu_vm_sdma_funcs;\n\t}\n\n\tdma_fence_put(vm->last_update);\n\tvm->last_update = dma_fence_get_stub();\n\tvm->is_compute_context = true;\n\n\t \n\tamdgpu_bo_unref(&to_amdgpu_bo_vm(vm->root.bo)->shadow);\n\n\tgoto unreserve_bo;\n\nunreserve_bo:\n\tamdgpu_bo_unreserve(vm->root.bo);\n\treturn r;\n}\n\n \nvoid amdgpu_vm_release_compute(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tamdgpu_vm_set_pasid(adev, vm, 0);\n\tvm->is_compute_context = false;\n}\n\n \nvoid amdgpu_vm_fini(struct amdgpu_device *adev, struct amdgpu_vm *vm)\n{\n\tstruct amdgpu_bo_va_mapping *mapping, *tmp;\n\tbool prt_fini_needed = !!adev->gmc.gmc_funcs->set_prt;\n\tstruct amdgpu_bo *root;\n\tunsigned long flags;\n\tint i;\n\n\tamdgpu_amdkfd_gpuvm_destroy_cb(adev, vm);\n\n\tflush_work(&vm->pt_free_work);\n\n\troot = amdgpu_bo_ref(vm->root.bo);\n\tamdgpu_bo_reserve(root, true);\n\tamdgpu_vm_set_pasid(adev, vm, 0);\n\tdma_fence_wait(vm->last_unlocked, false);\n\tdma_fence_put(vm->last_unlocked);\n\tdma_fence_wait(vm->last_tlb_flush, false);\n\t \n\tspin_lock_irqsave(vm->last_tlb_flush->lock, flags);\n\tspin_unlock_irqrestore(vm->last_tlb_flush->lock, flags);\n\tdma_fence_put(vm->last_tlb_flush);\n\n\tlist_for_each_entry_safe(mapping, tmp, &vm->freed, list) {\n\t\tif (mapping->flags & AMDGPU_PTE_PRT && prt_fini_needed) {\n\t\t\tamdgpu_vm_prt_fini(adev, vm);\n\t\t\tprt_fini_needed = false;\n\t\t}\n\n\t\tlist_del(&mapping->list);\n\t\tamdgpu_vm_free_mapping(adev, vm, mapping, NULL);\n\t}\n\n\tamdgpu_vm_pt_free_root(adev, vm);\n\tamdgpu_bo_unreserve(root);\n\tamdgpu_bo_unref(&root);\n\tWARN_ON(vm->root.bo);\n\n\tamdgpu_vm_fini_entities(vm);\n\n\tif (!RB_EMPTY_ROOT(&vm->va.rb_root)) {\n\t\tdev_err(adev->dev, \"still active bo inside vm\\n\");\n\t}\n\trbtree_postorder_for_each_entry_safe(mapping, tmp,\n\t\t\t\t\t     &vm->va.rb_root, rb) {\n\t\t \n\t\tlist_del(&mapping->list);\n\t\tkfree(mapping);\n\t}\n\n\tdma_fence_put(vm->last_update);\n\n\tfor (i = 0; i < AMDGPU_MAX_VMHUBS; i++) {\n\t\tif (vm->reserved_vmid[i]) {\n\t\t\tamdgpu_vmid_free_reserved(adev, i);\n\t\t\tvm->reserved_vmid[i] = false;\n\t\t}\n\t}\n\n}\n\n \nvoid amdgpu_vm_manager_init(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\t \n\tadev->vm_manager.concurrent_flush = !(adev->asic_type < CHIP_VEGA10 ||\n\t\t\t\t\t      adev->asic_type == CHIP_NAVI10 ||\n\t\t\t\t\t      adev->asic_type == CHIP_NAVI14);\n\tamdgpu_vmid_mgr_init(adev);\n\n\tadev->vm_manager.fence_context =\n\t\tdma_fence_context_alloc(AMDGPU_MAX_RINGS);\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; ++i)\n\t\tadev->vm_manager.seqno[i] = 0;\n\n\tspin_lock_init(&adev->vm_manager.prt_lock);\n\tatomic_set(&adev->vm_manager.num_prt_users, 0);\n\n\t \n#ifdef CONFIG_X86_64\n\tif (amdgpu_vm_update_mode == -1) {\n\t\t \n\t\tif (amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t\t    !amdgpu_sriov_vf_mmio_access_protection(adev))\n\t\t\tadev->vm_manager.vm_update_mode =\n\t\t\t\tAMDGPU_VM_USE_CPU_FOR_COMPUTE;\n\t\telse\n\t\t\tadev->vm_manager.vm_update_mode = 0;\n\t} else\n\t\tadev->vm_manager.vm_update_mode = amdgpu_vm_update_mode;\n#else\n\tadev->vm_manager.vm_update_mode = 0;\n#endif\n\n\txa_init_flags(&adev->vm_manager.pasids, XA_FLAGS_LOCK_IRQ);\n}\n\n \nvoid amdgpu_vm_manager_fini(struct amdgpu_device *adev)\n{\n\tWARN_ON(!xa_empty(&adev->vm_manager.pasids));\n\txa_destroy(&adev->vm_manager.pasids);\n\n\tamdgpu_vmid_mgr_fini(adev);\n}\n\n \nint amdgpu_vm_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\n{\n\tunion drm_amdgpu_vm *args = data;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tstruct amdgpu_fpriv *fpriv = filp->driver_priv;\n\n\t \n\tif (args->in.flags)\n\t\treturn -EINVAL;\n\n\tswitch (args->in.op) {\n\tcase AMDGPU_VM_OP_RESERVE_VMID:\n\t\t \n\t\tif (!fpriv->vm.reserved_vmid[AMDGPU_GFXHUB(0)]) {\n\t\t\tamdgpu_vmid_alloc_reserved(adev, AMDGPU_GFXHUB(0));\n\t\t\tfpriv->vm.reserved_vmid[AMDGPU_GFXHUB(0)] = true;\n\t\t}\n\n\t\tbreak;\n\tcase AMDGPU_VM_OP_UNRESERVE_VMID:\n\t\tif (fpriv->vm.reserved_vmid[AMDGPU_GFXHUB(0)]) {\n\t\t\tamdgpu_vmid_free_reserved(adev, AMDGPU_GFXHUB(0));\n\t\t\tfpriv->vm.reserved_vmid[AMDGPU_GFXHUB(0)] = false;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nvoid amdgpu_vm_get_task_info(struct amdgpu_device *adev, u32 pasid,\n\t\t\t struct amdgpu_task_info *task_info)\n{\n\tstruct amdgpu_vm *vm;\n\tunsigned long flags;\n\n\txa_lock_irqsave(&adev->vm_manager.pasids, flags);\n\n\tvm = xa_load(&adev->vm_manager.pasids, pasid);\n\tif (vm)\n\t\t*task_info = vm->task_info;\n\n\txa_unlock_irqrestore(&adev->vm_manager.pasids, flags);\n}\n\n \nvoid amdgpu_vm_set_task_info(struct amdgpu_vm *vm)\n{\n\tif (vm->task_info.pid)\n\t\treturn;\n\n\tvm->task_info.pid = current->pid;\n\tget_task_comm(vm->task_info.task_name, current);\n\n\tif (current->group_leader->mm != current->mm)\n\t\treturn;\n\n\tvm->task_info.tgid = current->group_leader->pid;\n\tget_task_comm(vm->task_info.process_name, current->group_leader);\n}\n\n \nbool amdgpu_vm_handle_fault(struct amdgpu_device *adev, u32 pasid,\n\t\t\t    u32 vmid, u32 node_id, uint64_t addr,\n\t\t\t    bool write_fault)\n{\n\tbool is_compute_context = false;\n\tstruct amdgpu_bo *root;\n\tunsigned long irqflags;\n\tuint64_t value, flags;\n\tstruct amdgpu_vm *vm;\n\tint r;\n\n\txa_lock_irqsave(&adev->vm_manager.pasids, irqflags);\n\tvm = xa_load(&adev->vm_manager.pasids, pasid);\n\tif (vm) {\n\t\troot = amdgpu_bo_ref(vm->root.bo);\n\t\tis_compute_context = vm->is_compute_context;\n\t} else {\n\t\troot = NULL;\n\t}\n\txa_unlock_irqrestore(&adev->vm_manager.pasids, irqflags);\n\n\tif (!root)\n\t\treturn false;\n\n\taddr /= AMDGPU_GPU_PAGE_SIZE;\n\n\tif (is_compute_context && !svm_range_restore_pages(adev, pasid, vmid,\n\t    node_id, addr, write_fault)) {\n\t\tamdgpu_bo_unref(&root);\n\t\treturn true;\n\t}\n\n\tr = amdgpu_bo_reserve(root, true);\n\tif (r)\n\t\tgoto error_unref;\n\n\t \n\txa_lock_irqsave(&adev->vm_manager.pasids, irqflags);\n\tvm = xa_load(&adev->vm_manager.pasids, pasid);\n\tif (vm && vm->root.bo != root)\n\t\tvm = NULL;\n\txa_unlock_irqrestore(&adev->vm_manager.pasids, irqflags);\n\tif (!vm)\n\t\tgoto error_unlock;\n\n\tflags = AMDGPU_PTE_VALID | AMDGPU_PTE_SNOOPED |\n\t\tAMDGPU_PTE_SYSTEM;\n\n\tif (is_compute_context) {\n\t\t \n\t\tflags = AMDGPU_VM_NORETRY_FLAGS;\n\t\tvalue = 0;\n\t} else if (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_NEVER) {\n\t\t \n\t\tvalue = adev->dummy_page_addr;\n\t\tflags |= AMDGPU_PTE_EXECUTABLE | AMDGPU_PTE_READABLE |\n\t\t\tAMDGPU_PTE_WRITEABLE;\n\n\t} else {\n\t\t \n\t\tvalue = 0;\n\t}\n\n\tr = dma_resv_reserve_fences(root->tbo.base.resv, 1);\n\tif (r) {\n\t\tpr_debug(\"failed %d to reserve fence slot\\n\", r);\n\t\tgoto error_unlock;\n\t}\n\n\tr = amdgpu_vm_update_range(adev, vm, true, false, false, NULL, addr,\n\t\t\t\t   addr, flags, value, 0, NULL, NULL, NULL);\n\tif (r)\n\t\tgoto error_unlock;\n\n\tr = amdgpu_vm_update_pdes(adev, vm, true);\n\nerror_unlock:\n\tamdgpu_bo_unreserve(root);\n\tif (r < 0)\n\t\tDRM_ERROR(\"Can't handle page fault (%d)\\n\", r);\n\nerror_unref:\n\tamdgpu_bo_unref(&root);\n\n\treturn false;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n \nvoid amdgpu_debugfs_vm_bo_info(struct amdgpu_vm *vm, struct seq_file *m)\n{\n\tstruct amdgpu_bo_va *bo_va, *tmp;\n\tu64 total_idle = 0;\n\tu64 total_evicted = 0;\n\tu64 total_relocated = 0;\n\tu64 total_moved = 0;\n\tu64 total_invalidated = 0;\n\tu64 total_done = 0;\n\tunsigned int total_idle_objs = 0;\n\tunsigned int total_evicted_objs = 0;\n\tunsigned int total_relocated_objs = 0;\n\tunsigned int total_moved_objs = 0;\n\tunsigned int total_invalidated_objs = 0;\n\tunsigned int total_done_objs = 0;\n\tunsigned int id = 0;\n\n\tspin_lock(&vm->status_lock);\n\tseq_puts(m, \"\\tIdle BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->idle, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_idle += amdgpu_bo_print_info(id++, bo_va->base.bo, m);\n\t}\n\ttotal_idle_objs = id;\n\tid = 0;\n\n\tseq_puts(m, \"\\tEvicted BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->evicted, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_evicted += amdgpu_bo_print_info(id++, bo_va->base.bo, m);\n\t}\n\ttotal_evicted_objs = id;\n\tid = 0;\n\n\tseq_puts(m, \"\\tRelocated BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->relocated, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_relocated += amdgpu_bo_print_info(id++, bo_va->base.bo, m);\n\t}\n\ttotal_relocated_objs = id;\n\tid = 0;\n\n\tseq_puts(m, \"\\tMoved BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->moved, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_moved += amdgpu_bo_print_info(id++, bo_va->base.bo, m);\n\t}\n\ttotal_moved_objs = id;\n\tid = 0;\n\n\tseq_puts(m, \"\\tInvalidated BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->invalidated, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_invalidated += amdgpu_bo_print_info(id++,\tbo_va->base.bo, m);\n\t}\n\ttotal_invalidated_objs = id;\n\tid = 0;\n\n\tseq_puts(m, \"\\tDone BOs:\\n\");\n\tlist_for_each_entry_safe(bo_va, tmp, &vm->done, base.vm_status) {\n\t\tif (!bo_va->base.bo)\n\t\t\tcontinue;\n\t\ttotal_done += amdgpu_bo_print_info(id++, bo_va->base.bo, m);\n\t}\n\tspin_unlock(&vm->status_lock);\n\ttotal_done_objs = id;\n\n\tseq_printf(m, \"\\tTotal idle size:        %12lld\\tobjs:\\t%d\\n\", total_idle,\n\t\t   total_idle_objs);\n\tseq_printf(m, \"\\tTotal evicted size:     %12lld\\tobjs:\\t%d\\n\", total_evicted,\n\t\t   total_evicted_objs);\n\tseq_printf(m, \"\\tTotal relocated size:   %12lld\\tobjs:\\t%d\\n\", total_relocated,\n\t\t   total_relocated_objs);\n\tseq_printf(m, \"\\tTotal moved size:       %12lld\\tobjs:\\t%d\\n\", total_moved,\n\t\t   total_moved_objs);\n\tseq_printf(m, \"\\tTotal invalidated size: %12lld\\tobjs:\\t%d\\n\", total_invalidated,\n\t\t   total_invalidated_objs);\n\tseq_printf(m, \"\\tTotal done size:        %12lld\\tobjs:\\t%d\\n\", total_done,\n\t\t   total_done_objs);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}