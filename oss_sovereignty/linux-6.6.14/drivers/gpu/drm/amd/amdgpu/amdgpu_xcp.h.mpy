{
  "module_name": "amdgpu_xcp.h",
  "hash_id": "c84f99965b17f7bcabf50ee624db552b1f1f48a361c00df33a232b2533e68292",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_xcp.h",
  "human_readable_source": " \n\n#ifndef AMDGPU_XCP_H\n#define AMDGPU_XCP_H\n\n#include <linux/pci.h>\n#include <linux/xarray.h>\n\n#include \"amdgpu_ctx.h\"\n\n#define MAX_XCP 8\n\n#define AMDGPU_XCP_MODE_NONE -1\n#define AMDGPU_XCP_MODE_TRANS -2\n\n#define AMDGPU_XCP_FL_NONE 0\n#define AMDGPU_XCP_FL_LOCKED (1 << 0)\n\n#define AMDGPU_XCP_NO_PARTITION (~0)\n\nstruct amdgpu_fpriv;\n\nenum AMDGPU_XCP_IP_BLOCK {\n\tAMDGPU_XCP_GFXHUB,\n\tAMDGPU_XCP_GFX,\n\tAMDGPU_XCP_SDMA,\n\tAMDGPU_XCP_VCN,\n\tAMDGPU_XCP_MAX_BLOCKS\n};\n\nenum AMDGPU_XCP_STATE {\n\tAMDGPU_XCP_PREPARE_SUSPEND,\n\tAMDGPU_XCP_SUSPEND,\n\tAMDGPU_XCP_PREPARE_RESUME,\n\tAMDGPU_XCP_RESUME,\n};\n\nstruct amdgpu_xcp_ip_funcs {\n\tint (*prepare_suspend)(void *handle, uint32_t inst_mask);\n\tint (*suspend)(void *handle, uint32_t inst_mask);\n\tint (*prepare_resume)(void *handle, uint32_t inst_mask);\n\tint (*resume)(void *handle, uint32_t inst_mask);\n};\n\nstruct amdgpu_xcp_ip {\n\tstruct amdgpu_xcp_ip_funcs *ip_funcs;\n\tuint32_t inst_mask;\n\n\tenum AMDGPU_XCP_IP_BLOCK ip_id;\n\tbool valid;\n};\n\nstruct amdgpu_xcp {\n\tstruct amdgpu_xcp_ip ip[AMDGPU_XCP_MAX_BLOCKS];\n\n\tuint8_t id;\n\tuint8_t mem_id;\n\tbool valid;\n\tatomic_t\tref_cnt;\n\tstruct drm_device *ddev;\n\tstruct drm_device *rdev;\n\tstruct drm_device *pdev;\n\tstruct drm_driver *driver;\n\tstruct drm_vma_offset_manager *vma_offset_manager;\n\tstruct amdgpu_sched\tgpu_sched[AMDGPU_HW_IP_NUM][AMDGPU_RING_PRIO_MAX];\n};\n\nstruct amdgpu_xcp_mgr {\n\tstruct amdgpu_device *adev;\n\tstruct mutex xcp_lock;\n\tstruct amdgpu_xcp_mgr_funcs *funcs;\n\n\tstruct amdgpu_xcp xcp[MAX_XCP];\n\tuint8_t num_xcps;\n\tint8_t mode;\n\n\t  \n\tunsigned int num_xcp_per_mem_partition;\n};\n\nstruct amdgpu_xcp_mgr_funcs {\n\tint (*switch_partition_mode)(struct amdgpu_xcp_mgr *xcp_mgr, int mode,\n\t\t\t\t     int *num_xcps);\n\tint (*query_partition_mode)(struct amdgpu_xcp_mgr *xcp_mgr);\n\tint (*get_ip_details)(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id,\n\t\t\t      enum AMDGPU_XCP_IP_BLOCK ip_id,\n\t\t\t      struct amdgpu_xcp_ip *ip);\n\tint (*get_xcp_mem_id)(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t      struct amdgpu_xcp *xcp, uint8_t *mem_id);\n\n\tint (*prepare_suspend)(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\n\tint (*suspend)(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\n\tint (*prepare_resume)(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\n\tint (*resume)(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\n\tint (*select_scheds)(struct amdgpu_device *adev,\n\t\t\t\t  u32 hw_ip, u32 hw_prio, struct amdgpu_fpriv *fpriv,\n\t\t\t\t  unsigned int *num_scheds, struct drm_gpu_scheduler ***scheds);\n\tint (*update_partition_sched_list)(struct amdgpu_device *adev);\n};\n\nint amdgpu_xcp_prepare_suspend(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\nint amdgpu_xcp_suspend(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\nint amdgpu_xcp_prepare_resume(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\nint amdgpu_xcp_resume(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id);\n\nint amdgpu_xcp_mgr_init(struct amdgpu_device *adev, int init_mode,\n\t\t\tint init_xcps, struct amdgpu_xcp_mgr_funcs *xcp_funcs);\nint amdgpu_xcp_init(struct amdgpu_xcp_mgr *xcp_mgr, int num_xcps, int mode);\nint amdgpu_xcp_query_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr, u32 flags);\nint amdgpu_xcp_switch_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr, int mode);\nint amdgpu_xcp_get_partition(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t     enum AMDGPU_XCP_IP_BLOCK ip, int instance);\n\nint amdgpu_xcp_get_inst_details(struct amdgpu_xcp *xcp,\n\t\t\t\tenum AMDGPU_XCP_IP_BLOCK ip,\n\t\t\t\tuint32_t *inst_mask);\n\nint amdgpu_xcp_dev_register(struct amdgpu_device *adev,\n\t\t\t\tconst struct pci_device_id *ent);\nvoid amdgpu_xcp_dev_unplug(struct amdgpu_device *adev);\nint amdgpu_xcp_open_device(struct amdgpu_device *adev,\n\t\t\t   struct amdgpu_fpriv *fpriv,\n\t\t\t   struct drm_file *file_priv);\nvoid amdgpu_xcp_release_sched(struct amdgpu_device *adev,\n\t\t\t      struct amdgpu_ctx_entity *entity);\n\n#define amdgpu_xcp_select_scheds(adev, e, c, d, x, y) \\\n\t((adev)->xcp_mgr && (adev)->xcp_mgr->funcs && \\\n\t(adev)->xcp_mgr->funcs->select_scheds ? \\\n\t(adev)->xcp_mgr->funcs->select_scheds((adev), (e), (c), (d), (x), (y)) : -ENOENT)\n#define amdgpu_xcp_update_partition_sched_list(adev) \\\n\t((adev)->xcp_mgr && (adev)->xcp_mgr->funcs && \\\n\t(adev)->xcp_mgr->funcs->update_partition_sched_list ? \\\n\t(adev)->xcp_mgr->funcs->update_partition_sched_list(adev) : 0)\n\nstatic inline int amdgpu_xcp_get_num_xcp(struct amdgpu_xcp_mgr *xcp_mgr)\n{\n\tif (!xcp_mgr)\n\t\treturn 1;\n\telse\n\t\treturn xcp_mgr->num_xcps;\n}\n\nstatic inline struct amdgpu_xcp *\namdgpu_get_next_xcp(struct amdgpu_xcp_mgr *xcp_mgr, int *from)\n{\n\tif (!xcp_mgr)\n\t\treturn NULL;\n\n\twhile (*from < MAX_XCP) {\n\t\tif (xcp_mgr->xcp[*from].valid)\n\t\t\treturn &xcp_mgr->xcp[*from];\n\t\t++(*from);\n\t}\n\n\treturn NULL;\n}\n\n#define for_each_xcp(xcp_mgr, xcp, i)                            \\\n\tfor (i = 0, xcp = amdgpu_get_next_xcp(xcp_mgr, &i); xcp; \\\n\t     xcp = amdgpu_get_next_xcp(xcp_mgr, &i))\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}