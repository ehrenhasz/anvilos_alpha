{
  "module_name": "aqua_vanjaram.c",
  "hash_id": "6b6e02851f3ec8902ab78c1895f3dd6002f77ce8b574e24ac89b8a4259e4b022",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/aqua_vanjaram.c",
  "human_readable_source": " \n#include \"amdgpu.h\"\n#include \"soc15.h\"\n\n#include \"soc15_common.h\"\n#include \"amdgpu_xcp.h\"\n#include \"gfx_v9_4_3.h\"\n#include \"gfxhub_v1_2.h\"\n#include \"sdma_v4_4_2.h\"\n\n#define XCP_INST_MASK(num_inst, xcp_id)                                        \\\n\t(num_inst ? GENMASK(num_inst - 1, 0) << (xcp_id * num_inst) : 0)\n\n#define AMDGPU_XCP_OPS_KFD\t(1 << 0)\n\nvoid aqua_vanjaram_doorbell_index_init(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tadev->doorbell_index.kiq = AMDGPU_DOORBELL_LAYOUT1_KIQ_START;\n\n\tadev->doorbell_index.mec_ring0 = AMDGPU_DOORBELL_LAYOUT1_MEC_RING_START;\n\n\tadev->doorbell_index.userqueue_start = AMDGPU_DOORBELL_LAYOUT1_USERQUEUE_START;\n\tadev->doorbell_index.userqueue_end = AMDGPU_DOORBELL_LAYOUT1_USERQUEUE_END;\n\tadev->doorbell_index.xcc_doorbell_range = AMDGPU_DOORBELL_LAYOUT1_XCC_RANGE;\n\n\tadev->doorbell_index.sdma_doorbell_range = 20;\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tadev->doorbell_index.sdma_engine[i] =\n\t\t\tAMDGPU_DOORBELL_LAYOUT1_sDMA_ENGINE_START +\n\t\t\ti * (adev->doorbell_index.sdma_doorbell_range >> 1);\n\n\tadev->doorbell_index.ih = AMDGPU_DOORBELL_LAYOUT1_IH;\n\tadev->doorbell_index.vcn.vcn_ring0_1 = AMDGPU_DOORBELL_LAYOUT1_VCN_START;\n\n\tadev->doorbell_index.first_non_cp = AMDGPU_DOORBELL_LAYOUT1_FIRST_NON_CP;\n\tadev->doorbell_index.last_non_cp = AMDGPU_DOORBELL_LAYOUT1_LAST_NON_CP;\n\n\tadev->doorbell_index.max_assignment = AMDGPU_DOORBELL_LAYOUT1_MAX_ASSIGNMENT << 1;\n}\n\nstatic void aqua_vanjaram_set_xcp_id(struct amdgpu_device *adev,\n\t\t\t     uint32_t inst_idx, struct amdgpu_ring *ring)\n{\n\tint xcp_id;\n\tenum AMDGPU_XCP_IP_BLOCK ip_blk;\n\tuint32_t inst_mask;\n\n\tring->xcp_id = AMDGPU_XCP_NO_PARTITION;\n\tif (adev->xcp_mgr->mode == AMDGPU_XCP_MODE_NONE)\n\t\treturn;\n\n\tinst_mask = 1 << inst_idx;\n\n\tswitch (ring->funcs->type) {\n\tcase AMDGPU_HW_IP_GFX:\n\tcase AMDGPU_RING_TYPE_COMPUTE:\n\tcase AMDGPU_RING_TYPE_KIQ:\n\t\tip_blk = AMDGPU_XCP_GFX;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_SDMA:\n\t\tip_blk = AMDGPU_XCP_SDMA;\n\t\tbreak;\n\tcase AMDGPU_RING_TYPE_VCN_ENC:\n\tcase AMDGPU_RING_TYPE_VCN_JPEG:\n\t\tip_blk = AMDGPU_XCP_VCN;\n\t\tif (adev->xcp_mgr->mode == AMDGPU_CPX_PARTITION_MODE)\n\t\t\tinst_mask = 1 << (inst_idx * 2);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Not support ring type %d!\", ring->funcs->type);\n\t\treturn;\n\t}\n\n\tfor (xcp_id = 0; xcp_id < adev->xcp_mgr->num_xcps; xcp_id++) {\n\t\tif (adev->xcp_mgr->xcp[xcp_id].ip[ip_blk].inst_mask & inst_mask) {\n\t\t\tring->xcp_id = xcp_id;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void aqua_vanjaram_xcp_gpu_sched_update(\n\t\tstruct amdgpu_device *adev,\n\t\tstruct amdgpu_ring *ring,\n\t\tunsigned int sel_xcp_id)\n{\n\tunsigned int *num_gpu_sched;\n\n\tnum_gpu_sched = &adev->xcp_mgr->xcp[sel_xcp_id]\n\t\t\t.gpu_sched[ring->funcs->type][ring->hw_prio].num_scheds;\n\tadev->xcp_mgr->xcp[sel_xcp_id].gpu_sched[ring->funcs->type][ring->hw_prio]\n\t\t\t.sched[(*num_gpu_sched)++] = &ring->sched;\n\tDRM_DEBUG(\"%s :[%d] gpu_sched[%d][%d] = %d\", ring->name,\n\t\t\tsel_xcp_id, ring->funcs->type,\n\t\t\tring->hw_prio, *num_gpu_sched);\n}\n\nstatic int aqua_vanjaram_xcp_sched_list_update(\n\t\tstruct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint i;\n\n\tfor (i = 0; i < MAX_XCP; i++) {\n\t\tatomic_set(&adev->xcp_mgr->xcp[i].ref_cnt, 0);\n\t\tmemset(adev->xcp_mgr->xcp[i].gpu_sched, 0, sizeof(adev->xcp_mgr->xcp->gpu_sched));\n\t}\n\n\tif (adev->xcp_mgr->mode == AMDGPU_XCP_MODE_NONE)\n\t\treturn 0;\n\n\tfor (i = 0; i < AMDGPU_MAX_RINGS; i++) {\n\t\tring = adev->rings[i];\n\t\tif (!ring || !ring->sched.ready || ring->no_scheduler)\n\t\t\tcontinue;\n\n\t\taqua_vanjaram_xcp_gpu_sched_update(adev, ring, ring->xcp_id);\n\n\t\t \n\t\tif ((ring->funcs->type == AMDGPU_RING_TYPE_VCN_ENC ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_VCN_JPEG) &&\n\t\t\tadev->xcp_mgr->mode == AMDGPU_CPX_PARTITION_MODE)\n\t\t\taqua_vanjaram_xcp_gpu_sched_update(adev, ring, ring->xcp_id + 1);\n\t}\n\n\treturn 0;\n}\n\nstatic int aqua_vanjaram_update_partition_sched_list(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->num_rings; i++) {\n\t\tstruct amdgpu_ring *ring = adev->rings[i];\n\n\t\tif (ring->funcs->type == AMDGPU_RING_TYPE_COMPUTE ||\n\t\t\tring->funcs->type == AMDGPU_RING_TYPE_KIQ)\n\t\t\taqua_vanjaram_set_xcp_id(adev, ring->xcc_id, ring);\n\t\telse\n\t\t\taqua_vanjaram_set_xcp_id(adev, ring->me, ring);\n\t}\n\n\treturn aqua_vanjaram_xcp_sched_list_update(adev);\n}\n\nstatic int aqua_vanjaram_select_scheds(\n\t\tstruct amdgpu_device *adev,\n\t\tu32 hw_ip,\n\t\tu32 hw_prio,\n\t\tstruct amdgpu_fpriv *fpriv,\n\t\tunsigned int *num_scheds,\n\t\tstruct drm_gpu_scheduler ***scheds)\n{\n\tu32 sel_xcp_id;\n\tint i;\n\n\tif (fpriv->xcp_id == AMDGPU_XCP_NO_PARTITION) {\n\t\tu32 least_ref_cnt = ~0;\n\n\t\tfpriv->xcp_id = 0;\n\t\tfor (i = 0; i < adev->xcp_mgr->num_xcps; i++) {\n\t\t\tu32 total_ref_cnt;\n\n\t\t\ttotal_ref_cnt = atomic_read(&adev->xcp_mgr->xcp[i].ref_cnt);\n\t\t\tif (total_ref_cnt < least_ref_cnt) {\n\t\t\t\tfpriv->xcp_id = i;\n\t\t\t\tleast_ref_cnt = total_ref_cnt;\n\t\t\t}\n\t\t}\n\t}\n\tsel_xcp_id = fpriv->xcp_id;\n\n\tif (adev->xcp_mgr->xcp[sel_xcp_id].gpu_sched[hw_ip][hw_prio].num_scheds) {\n\t\t*num_scheds = adev->xcp_mgr->xcp[fpriv->xcp_id].gpu_sched[hw_ip][hw_prio].num_scheds;\n\t\t*scheds = adev->xcp_mgr->xcp[fpriv->xcp_id].gpu_sched[hw_ip][hw_prio].sched;\n\t\tatomic_inc(&adev->xcp_mgr->xcp[sel_xcp_id].ref_cnt);\n\t\tDRM_DEBUG(\"Selected partition #%d\", sel_xcp_id);\n\t} else {\n\t\tDRM_ERROR(\"Failed to schedule partition #%d.\", sel_xcp_id);\n\t\treturn -ENOENT;\n\t}\n\n\treturn 0;\n}\n\nstatic int8_t aqua_vanjaram_logical_to_dev_inst(struct amdgpu_device *adev,\n\t\t\t\t\t enum amd_hw_ip_block_type block,\n\t\t\t\t\t int8_t inst)\n{\n\tint8_t dev_inst;\n\n\tswitch (block) {\n\tcase GC_HWIP:\n\tcase SDMA0_HWIP:\n\t \n\tcase VCN_HWIP:\n\t\tdev_inst = adev->ip_map.dev_inst[block][inst];\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tdev_inst = inst;\n\t\tbreak;\n\t}\n\n\treturn dev_inst;\n}\n\nstatic uint32_t aqua_vanjaram_logical_to_dev_mask(struct amdgpu_device *adev,\n\t\t\t\t\t enum amd_hw_ip_block_type block,\n\t\t\t\t\t uint32_t mask)\n{\n\tuint32_t dev_mask = 0;\n\tint8_t log_inst, dev_inst;\n\n\twhile (mask) {\n\t\tlog_inst = ffs(mask) - 1;\n\t\tdev_inst = aqua_vanjaram_logical_to_dev_inst(adev, block, log_inst);\n\t\tdev_mask |= (1 << dev_inst);\n\t\tmask &= ~(1 << log_inst);\n\t}\n\n\treturn dev_mask;\n}\n\nstatic void aqua_vanjaram_populate_ip_map(struct amdgpu_device *adev,\n\t\t\t\t\t  enum amd_hw_ip_block_type ip_block,\n\t\t\t\t\t  uint32_t inst_mask)\n{\n\tint l = 0, i;\n\n\twhile (inst_mask) {\n\t\ti = ffs(inst_mask) - 1;\n\t\tadev->ip_map.dev_inst[ip_block][l++] = i;\n\t\tinst_mask &= ~(1 << i);\n\t}\n\tfor (; l < HWIP_MAX_INSTANCE; l++)\n\t\tadev->ip_map.dev_inst[ip_block][l] = -1;\n}\n\nvoid aqua_vanjaram_ip_map_init(struct amdgpu_device *adev)\n{\n\tu32 ip_map[][2] = {\n\t\t{ GC_HWIP, adev->gfx.xcc_mask },\n\t\t{ SDMA0_HWIP, adev->sdma.sdma_mask },\n\t\t{ VCN_HWIP, adev->vcn.inst_mask },\n\t};\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ip_map); ++i)\n\t\taqua_vanjaram_populate_ip_map(adev, ip_map[i][0], ip_map[i][1]);\n\n\tadev->ip_map.logical_to_dev_inst = aqua_vanjaram_logical_to_dev_inst;\n\tadev->ip_map.logical_to_dev_mask = aqua_vanjaram_logical_to_dev_mask;\n}\n\n \nu64 aqua_vanjaram_encode_ext_smn_addressing(int ext_id)\n{\n\tu64 ext_offset;\n\n\t \n\tif (ext_id == 0)\n\t\treturn 0;\n\n\t \n\text_offset = ((u64)(ext_id & 0x3) << 32) | (1ULL << 34);\n\n\treturn ext_offset;\n}\n\nstatic int aqua_vanjaram_query_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr)\n{\n\tenum amdgpu_gfx_partition mode = AMDGPU_UNKNOWN_COMPUTE_PARTITION_MODE;\n\tstruct amdgpu_device *adev = xcp_mgr->adev;\n\n\tif (adev->nbio.funcs->get_compute_partition_mode)\n\t\tmode = adev->nbio.funcs->get_compute_partition_mode(adev);\n\n\treturn mode;\n}\n\nstatic int __aqua_vanjaram_get_xcc_per_xcp(struct amdgpu_xcp_mgr *xcp_mgr, int mode)\n{\n\tint num_xcc, num_xcc_per_xcp = 0;\n\n\tnum_xcc = NUM_XCC(xcp_mgr->adev->gfx.xcc_mask);\n\n\tswitch (mode) {\n\tcase AMDGPU_SPX_PARTITION_MODE:\n\t\tnum_xcc_per_xcp = num_xcc;\n\t\tbreak;\n\tcase AMDGPU_DPX_PARTITION_MODE:\n\t\tnum_xcc_per_xcp = num_xcc / 2;\n\t\tbreak;\n\tcase AMDGPU_TPX_PARTITION_MODE:\n\t\tnum_xcc_per_xcp = num_xcc / 3;\n\t\tbreak;\n\tcase AMDGPU_QPX_PARTITION_MODE:\n\t\tnum_xcc_per_xcp = num_xcc / 4;\n\t\tbreak;\n\tcase AMDGPU_CPX_PARTITION_MODE:\n\t\tnum_xcc_per_xcp = 1;\n\t\tbreak;\n\t}\n\n\treturn num_xcc_per_xcp;\n}\n\nstatic int __aqua_vanjaram_get_xcp_ip_info(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id,\n\t\t\t\t    enum AMDGPU_XCP_IP_BLOCK ip_id,\n\t\t\t\t    struct amdgpu_xcp_ip *ip)\n{\n\tstruct amdgpu_device *adev = xcp_mgr->adev;\n\tint num_xcc_xcp, num_sdma_xcp, num_vcn_xcp;\n\tint num_sdma, num_vcn;\n\n\tnum_sdma = adev->sdma.num_instances;\n\tnum_vcn = adev->vcn.num_vcn_inst;\n\n\tswitch (xcp_mgr->mode) {\n\tcase AMDGPU_SPX_PARTITION_MODE:\n\t\tnum_sdma_xcp = num_sdma;\n\t\tnum_vcn_xcp = num_vcn;\n\t\tbreak;\n\tcase AMDGPU_DPX_PARTITION_MODE:\n\t\tnum_sdma_xcp = num_sdma / 2;\n\t\tnum_vcn_xcp = num_vcn / 2;\n\t\tbreak;\n\tcase AMDGPU_TPX_PARTITION_MODE:\n\t\tnum_sdma_xcp = num_sdma / 3;\n\t\tnum_vcn_xcp = num_vcn / 3;\n\t\tbreak;\n\tcase AMDGPU_QPX_PARTITION_MODE:\n\t\tnum_sdma_xcp = num_sdma / 4;\n\t\tnum_vcn_xcp = num_vcn / 4;\n\t\tbreak;\n\tcase AMDGPU_CPX_PARTITION_MODE:\n\t\tnum_sdma_xcp = 2;\n\t\tnum_vcn_xcp = num_vcn ? 1 : 0;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tnum_xcc_xcp = adev->gfx.num_xcc_per_xcp;\n\n\tswitch (ip_id) {\n\tcase AMDGPU_XCP_GFXHUB:\n\t\tip->inst_mask = XCP_INST_MASK(num_xcc_xcp, xcp_id);\n\t\tip->ip_funcs = &gfxhub_v1_2_xcp_funcs;\n\t\tbreak;\n\tcase AMDGPU_XCP_GFX:\n\t\tip->inst_mask = XCP_INST_MASK(num_xcc_xcp, xcp_id);\n\t\tip->ip_funcs = &gfx_v9_4_3_xcp_funcs;\n\t\tbreak;\n\tcase AMDGPU_XCP_SDMA:\n\t\tip->inst_mask = XCP_INST_MASK(num_sdma_xcp, xcp_id);\n\t\tip->ip_funcs = &sdma_v4_4_2_xcp_funcs;\n\t\tbreak;\n\tcase AMDGPU_XCP_VCN:\n\t\tip->inst_mask = XCP_INST_MASK(num_vcn_xcp, xcp_id);\n\t\t \n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tip->ip_id = ip_id;\n\n\treturn 0;\n}\n\nstatic enum amdgpu_gfx_partition\n__aqua_vanjaram_get_auto_mode(struct amdgpu_xcp_mgr *xcp_mgr)\n{\n\tstruct amdgpu_device *adev = xcp_mgr->adev;\n\tint num_xcc;\n\n\tnum_xcc = NUM_XCC(xcp_mgr->adev->gfx.xcc_mask);\n\n\tif (adev->gmc.num_mem_partitions == 1)\n\t\treturn AMDGPU_SPX_PARTITION_MODE;\n\n\tif (adev->gmc.num_mem_partitions == num_xcc)\n\t\treturn AMDGPU_CPX_PARTITION_MODE;\n\n\tif (adev->gmc.num_mem_partitions == num_xcc / 2)\n\t\treturn (adev->flags & AMD_IS_APU) ? AMDGPU_TPX_PARTITION_MODE :\n\t\t\t\t\t\t    AMDGPU_QPX_PARTITION_MODE;\n\n\tif (adev->gmc.num_mem_partitions == 2 && !(adev->flags & AMD_IS_APU))\n\t\treturn AMDGPU_DPX_PARTITION_MODE;\n\n\treturn AMDGPU_UNKNOWN_COMPUTE_PARTITION_MODE;\n}\n\nstatic bool __aqua_vanjaram_is_valid_mode(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t\t\t  enum amdgpu_gfx_partition mode)\n{\n\tstruct amdgpu_device *adev = xcp_mgr->adev;\n\tint num_xcc, num_xccs_per_xcp;\n\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\tswitch (mode) {\n\tcase AMDGPU_SPX_PARTITION_MODE:\n\t\treturn adev->gmc.num_mem_partitions == 1 && num_xcc > 0;\n\tcase AMDGPU_DPX_PARTITION_MODE:\n\t\treturn adev->gmc.num_mem_partitions != 8 && (num_xcc % 4) == 0;\n\tcase AMDGPU_TPX_PARTITION_MODE:\n\t\treturn (adev->gmc.num_mem_partitions == 1 ||\n\t\t\tadev->gmc.num_mem_partitions == 3) &&\n\t\t       ((num_xcc % 3) == 0);\n\tcase AMDGPU_QPX_PARTITION_MODE:\n\t\tnum_xccs_per_xcp = num_xcc / 4;\n\t\treturn (adev->gmc.num_mem_partitions == 1 ||\n\t\t\tadev->gmc.num_mem_partitions == 4) &&\n\t\t       (num_xccs_per_xcp >= 2);\n\tcase AMDGPU_CPX_PARTITION_MODE:\n\t\treturn ((num_xcc > 1) &&\n\t\t       (adev->gmc.num_mem_partitions == 1 || adev->gmc.num_mem_partitions == 4) &&\n\t\t       (num_xcc % adev->gmc.num_mem_partitions) == 0);\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn false;\n}\n\nstatic int __aqua_vanjaram_pre_partition_switch(struct amdgpu_xcp_mgr *xcp_mgr, u32 flags)\n{\n\t \n\n\tif (flags & AMDGPU_XCP_OPS_KFD)\n\t\tamdgpu_amdkfd_device_fini_sw(xcp_mgr->adev);\n\n\treturn 0;\n}\n\nstatic int __aqua_vanjaram_post_partition_switch(struct amdgpu_xcp_mgr *xcp_mgr, u32 flags)\n{\n\tint ret = 0;\n\n\tif (flags & AMDGPU_XCP_OPS_KFD) {\n\t\tamdgpu_amdkfd_device_probe(xcp_mgr->adev);\n\t\tamdgpu_amdkfd_device_init(xcp_mgr->adev);\n\t\t \n\t\tif (!xcp_mgr->adev->kfd.init_complete)\n\t\t\tret = -EIO;\n\t}\n\n\treturn ret;\n}\n\nstatic int aqua_vanjaram_switch_partition_mode(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t\t\t       int mode, int *num_xcps)\n{\n\tint num_xcc_per_xcp, num_xcc, ret;\n\tstruct amdgpu_device *adev;\n\tu32 flags = 0;\n\n\tadev = xcp_mgr->adev;\n\tnum_xcc = NUM_XCC(adev->gfx.xcc_mask);\n\n\tif (mode == AMDGPU_AUTO_COMPUTE_PARTITION_MODE) {\n\t\tmode = __aqua_vanjaram_get_auto_mode(xcp_mgr);\n\t} else if (!__aqua_vanjaram_is_valid_mode(xcp_mgr, mode)) {\n\t\tdev_err(adev->dev,\n\t\t\t\"Invalid compute partition mode requested, requested: %s, available memory partitions: %d\",\n\t\t\tamdgpu_gfx_compute_mode_desc(mode), adev->gmc.num_mem_partitions);\n\t\treturn -EINVAL;\n\t}\n\n\tif (adev->kfd.init_complete)\n\t\tflags |= AMDGPU_XCP_OPS_KFD;\n\n\tif (flags & AMDGPU_XCP_OPS_KFD) {\n\t\tret = amdgpu_amdkfd_check_and_lock_kfd(adev);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tret = __aqua_vanjaram_pre_partition_switch(xcp_mgr, flags);\n\tif (ret)\n\t\tgoto unlock;\n\n\tnum_xcc_per_xcp = __aqua_vanjaram_get_xcc_per_xcp(xcp_mgr, mode);\n\tif (adev->gfx.funcs->switch_partition_mode)\n\t\tadev->gfx.funcs->switch_partition_mode(xcp_mgr->adev,\n\t\t\t\t\t\t       num_xcc_per_xcp);\n\n\t \n\t*num_xcps = num_xcc / num_xcc_per_xcp;\n\tamdgpu_xcp_init(xcp_mgr, *num_xcps, mode);\n\n\tret = __aqua_vanjaram_post_partition_switch(xcp_mgr, flags);\nunlock:\n\tif (flags & AMDGPU_XCP_OPS_KFD)\n\t\tamdgpu_amdkfd_unlock_kfd(adev);\nout:\n\treturn ret;\n}\n\nstatic int __aqua_vanjaram_get_xcp_mem_id(struct amdgpu_device *adev,\n\t\t\t\t\t  int xcc_id, uint8_t *mem_id)\n{\n\t \n\t*mem_id = xcc_id / adev->gfx.num_xcc_per_xcp;\n\t*mem_id /= adev->xcp_mgr->num_xcp_per_mem_partition;\n\n\treturn 0;\n}\n\nstatic int aqua_vanjaram_get_xcp_mem_id(struct amdgpu_xcp_mgr *xcp_mgr,\n\t\t\t\t\tstruct amdgpu_xcp *xcp, uint8_t *mem_id)\n{\n\tstruct amdgpu_numa_info numa_info;\n\tstruct amdgpu_device *adev;\n\tuint32_t xcc_mask;\n\tint r, i, xcc_id;\n\n\tadev = xcp_mgr->adev;\n\t \n\t \n\tif (adev->gmc.num_mem_partitions == 1) {\n\t\t \n\t\t*mem_id = 0;\n\t\treturn 0;\n\t}\n\n\tr = amdgpu_xcp_get_inst_details(xcp, AMDGPU_XCP_GFX, &xcc_mask);\n\tif (r || !xcc_mask)\n\t\treturn -EINVAL;\n\n\txcc_id = ffs(xcc_mask) - 1;\n\tif (!adev->gmc.is_app_apu)\n\t\treturn __aqua_vanjaram_get_xcp_mem_id(adev, xcc_id, mem_id);\n\n\tr = amdgpu_acpi_get_mem_info(adev, xcc_id, &numa_info);\n\n\tif (r)\n\t\treturn r;\n\n\tr = -EINVAL;\n\tfor (i = 0; i < adev->gmc.num_mem_partitions; ++i) {\n\t\tif (adev->gmc.mem_partitions[i].numa.node == numa_info.nid) {\n\t\t\t*mem_id = i;\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstatic int aqua_vanjaram_get_xcp_ip_details(struct amdgpu_xcp_mgr *xcp_mgr, int xcp_id,\n\t\t\t\t     enum AMDGPU_XCP_IP_BLOCK ip_id,\n\t\t\t\t     struct amdgpu_xcp_ip *ip)\n{\n\tif (!ip)\n\t\treturn -EINVAL;\n\n\treturn __aqua_vanjaram_get_xcp_ip_info(xcp_mgr, xcp_id, ip_id, ip);\n}\n\nstruct amdgpu_xcp_mgr_funcs aqua_vanjaram_xcp_funcs = {\n\t.switch_partition_mode = &aqua_vanjaram_switch_partition_mode,\n\t.query_partition_mode = &aqua_vanjaram_query_partition_mode,\n\t.get_ip_details = &aqua_vanjaram_get_xcp_ip_details,\n\t.get_xcp_mem_id = &aqua_vanjaram_get_xcp_mem_id,\n\t.select_scheds = &aqua_vanjaram_select_scheds,\n\t.update_partition_sched_list = &aqua_vanjaram_update_partition_sched_list\n};\n\nstatic int aqua_vanjaram_xcp_mgr_init(struct amdgpu_device *adev)\n{\n\tint ret;\n\n\tret = amdgpu_xcp_mgr_init(adev, AMDGPU_UNKNOWN_COMPUTE_PARTITION_MODE, 1,\n\t\t\t\t  &aqua_vanjaram_xcp_funcs);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\n\treturn ret;\n}\n\nint aqua_vanjaram_init_soc_config(struct amdgpu_device *adev)\n{\n\tu32 mask, inst_mask = adev->sdma.sdma_mask;\n\tint ret, i;\n\n\t \n\tadev->sdma.num_inst_per_aid = 4;\n\tadev->sdma.num_instances = NUM_SDMA(adev->sdma.sdma_mask);\n\n\tadev->aid_mask = i = 1;\n\tinst_mask >>= adev->sdma.num_inst_per_aid;\n\n\tfor (mask = (1 << adev->sdma.num_inst_per_aid) - 1; inst_mask;\n\t     inst_mask >>= adev->sdma.num_inst_per_aid, ++i) {\n\t\tif ((inst_mask & mask) == mask)\n\t\t\tadev->aid_mask |= (1 << i);\n\t}\n\n\t \n\tadev->vcn.harvest_config = 0;\n\tadev->vcn.num_inst_per_aid = 1;\n\tadev->vcn.num_vcn_inst = hweight32(adev->vcn.inst_mask);\n\tadev->jpeg.harvest_config = 0;\n\tadev->jpeg.num_inst_per_aid = 1;\n\tadev->jpeg.num_jpeg_inst = hweight32(adev->jpeg.inst_mask);\n\n\tret = aqua_vanjaram_xcp_mgr_init(adev);\n\tif (ret)\n\t\treturn ret;\n\n\taqua_vanjaram_ip_map_init(adev);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}