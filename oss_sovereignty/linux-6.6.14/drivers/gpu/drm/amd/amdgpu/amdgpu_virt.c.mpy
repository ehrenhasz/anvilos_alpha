{
  "module_name": "amdgpu_virt.c",
  "hash_id": "880530fab15fe29bc6eba1a76466f8513950cfc118f053e5392e470fcf60d2c0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_virt.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n\n#ifdef CONFIG_X86\n#include <asm/hypervisor.h>\n#endif\n\n#include <drm/drm_drv.h>\n#include <xen/xen.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ras.h\"\n#include \"vi.h\"\n#include \"soc15.h\"\n#include \"nv.h\"\n\n#define POPULATE_UCODE_INFO(vf2pf_info, ucode, ver) \\\n\tdo { \\\n\t\tvf2pf_info->ucode_info[ucode].id = ucode; \\\n\t\tvf2pf_info->ucode_info[ucode].version = ver; \\\n\t} while (0)\n\nbool amdgpu_virt_mmio_blocked(struct amdgpu_device *adev)\n{\n\t \n\t \n\t \n\treturn RREG32_NO_KIQ(0xc040) == 0xffffffff;\n}\n\nvoid amdgpu_virt_init_setting(struct amdgpu_device *adev)\n{\n\tstruct drm_device *ddev = adev_to_drm(adev);\n\n\t \n\tif (adev->asic_type != CHIP_ALDEBARAN &&\n\t    adev->asic_type != CHIP_ARCTURUS &&\n\t    ((adev->pdev->class >> 8) != PCI_CLASS_ACCELERATOR_PROCESSING)) {\n\t\tif (adev->mode_info.num_crtc == 0)\n\t\t\tadev->mode_info.num_crtc = 1;\n\t\tadev->enable_virtual_display = true;\n\t}\n\tddev->driver_features &= ~DRIVER_ATOMIC;\n\tadev->cg_flags = 0;\n\tadev->pg_flags = 0;\n\n\t \n\tif (amdgpu_num_kcq == -1)\n\t\tamdgpu_num_kcq = 2;\n}\n\nvoid amdgpu_virt_kiq_reg_write_reg_wait(struct amdgpu_device *adev,\n\t\t\t\t\tuint32_t reg0, uint32_t reg1,\n\t\t\t\t\tuint32_t ref, uint32_t mask)\n{\n\tstruct amdgpu_kiq *kiq = &adev->gfx.kiq[0];\n\tstruct amdgpu_ring *ring = &kiq->ring;\n\tsigned long r, cnt = 0;\n\tunsigned long flags;\n\tuint32_t seq;\n\n\tif (adev->mes.ring.sched.ready) {\n\t\tamdgpu_mes_reg_write_reg_wait(adev, reg0, reg1,\n\t\t\t\t\t      ref, mask);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&kiq->ring_lock, flags);\n\tamdgpu_ring_alloc(ring, 32);\n\tamdgpu_ring_emit_reg_write_reg_wait(ring, reg0, reg1,\n\t\t\t\t\t    ref, mask);\n\tr = amdgpu_fence_emit_polling(ring, &seq, MAX_KIQ_REG_WAIT);\n\tif (r)\n\t\tgoto failed_undo;\n\n\tamdgpu_ring_commit(ring);\n\tspin_unlock_irqrestore(&kiq->ring_lock, flags);\n\n\tr = amdgpu_fence_wait_polling(ring, seq, MAX_KIQ_REG_WAIT);\n\n\t \n\tif (r < 1 && in_interrupt())\n\t\tgoto failed_kiq;\n\n\tmight_sleep();\n\twhile (r < 1 && cnt++ < MAX_KIQ_REG_TRY) {\n\n\t\tmsleep(MAX_KIQ_REG_BAILOUT_INTERVAL);\n\t\tr = amdgpu_fence_wait_polling(ring, seq, MAX_KIQ_REG_WAIT);\n\t}\n\n\tif (cnt > MAX_KIQ_REG_TRY)\n\t\tgoto failed_kiq;\n\n\treturn;\n\nfailed_undo:\n\tamdgpu_ring_undo(ring);\n\tspin_unlock_irqrestore(&kiq->ring_lock, flags);\nfailed_kiq:\n\tdev_err(adev->dev, \"failed to write reg %x wait reg %x\\n\", reg0, reg1);\n}\n\n \nint amdgpu_virt_request_full_gpu(struct amdgpu_device *adev, bool init)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tint r;\n\n\tif (virt->ops && virt->ops->req_full_gpu) {\n\t\tr = virt->ops->req_full_gpu(adev, init);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tadev->virt.caps &= ~AMDGPU_SRIOV_CAPS_RUNTIME;\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_virt_release_full_gpu(struct amdgpu_device *adev, bool init)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tint r;\n\n\tif (virt->ops && virt->ops->rel_full_gpu) {\n\t\tr = virt->ops->rel_full_gpu(adev, init);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_RUNTIME;\n\t}\n\treturn 0;\n}\n\n \nint amdgpu_virt_reset_gpu(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tint r;\n\n\tif (virt->ops && virt->ops->reset_gpu) {\n\t\tr = virt->ops->reset_gpu(adev);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tadev->virt.caps &= ~AMDGPU_SRIOV_CAPS_RUNTIME;\n\t}\n\n\treturn 0;\n}\n\nvoid amdgpu_virt_request_init_data(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\n\tif (virt->ops && virt->ops->req_init_data)\n\t\tvirt->ops->req_init_data(adev);\n\n\tif (adev->virt.req_init_data_ver > 0)\n\t\tDRM_INFO(\"host supports REQ_INIT_DATA handshake\\n\");\n\telse\n\t\tDRM_WARN(\"host doesn't support REQ_INIT_DATA handshake\\n\");\n}\n\n \nint amdgpu_virt_wait_reset(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\n\tif (!virt->ops || !virt->ops->wait_reset)\n\t\treturn -EINVAL;\n\n\treturn virt->ops->wait_reset(adev);\n}\n\n \nint amdgpu_virt_alloc_mm_table(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (!amdgpu_sriov_vf(adev) || adev->virt.mm_table.gpu_addr)\n\t\treturn 0;\n\n\tr = amdgpu_bo_create_kernel(adev, PAGE_SIZE, PAGE_SIZE,\n\t\t\t\t    AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t    &adev->virt.mm_table.bo,\n\t\t\t\t    &adev->virt.mm_table.gpu_addr,\n\t\t\t\t    (void *)&adev->virt.mm_table.cpu_addr);\n\tif (r) {\n\t\tDRM_ERROR(\"failed to alloc mm table and error = %d.\\n\", r);\n\t\treturn r;\n\t}\n\n\tmemset((void *)adev->virt.mm_table.cpu_addr, 0, PAGE_SIZE);\n\tDRM_INFO(\"MM table gpu addr = 0x%llx, cpu addr = %p.\\n\",\n\t\t adev->virt.mm_table.gpu_addr,\n\t\t adev->virt.mm_table.cpu_addr);\n\treturn 0;\n}\n\n \nvoid amdgpu_virt_free_mm_table(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_sriov_vf(adev) || !adev->virt.mm_table.gpu_addr)\n\t\treturn;\n\n\tamdgpu_bo_free_kernel(&adev->virt.mm_table.bo,\n\t\t\t      &adev->virt.mm_table.gpu_addr,\n\t\t\t      (void *)&adev->virt.mm_table.cpu_addr);\n\tadev->virt.mm_table.gpu_addr = 0;\n}\n\n\nunsigned int amd_sriov_msg_checksum(void *obj,\n\t\t\t\tunsigned long obj_size,\n\t\t\t\tunsigned int key,\n\t\t\t\tunsigned int checksum)\n{\n\tunsigned int ret = key;\n\tunsigned long i = 0;\n\tunsigned char *pos;\n\n\tpos = (char *)obj;\n\t \n\tfor (i = 0; i < obj_size; ++i)\n\t\tret += *(pos + i);\n\t \n\tpos = (char *)&checksum;\n\tfor (i = 0; i < sizeof(checksum); ++i)\n\t\tret -= *(pos + i);\n\treturn ret;\n}\n\nstatic int amdgpu_virt_init_ras_err_handler_data(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data **data = &virt->virt_eh_data;\n\t \n\tunsigned int align_space = 512;\n\tvoid *bps = NULL;\n\tstruct amdgpu_bo **bps_bo = NULL;\n\n\t*data = kmalloc(sizeof(struct amdgpu_virt_ras_err_handler_data), GFP_KERNEL);\n\tif (!*data)\n\t\tgoto data_failure;\n\n\tbps = kmalloc_array(align_space, sizeof((*data)->bps), GFP_KERNEL);\n\tif (!bps)\n\t\tgoto bps_failure;\n\n\tbps_bo = kmalloc_array(align_space, sizeof((*data)->bps_bo), GFP_KERNEL);\n\tif (!bps_bo)\n\t\tgoto bps_bo_failure;\n\n\t(*data)->bps = bps;\n\t(*data)->bps_bo = bps_bo;\n\t(*data)->count = 0;\n\t(*data)->last_reserved = 0;\n\n\tvirt->ras_init_done = true;\n\n\treturn 0;\n\nbps_bo_failure:\n\tkfree(bps);\nbps_failure:\n\tkfree(*data);\ndata_failure:\n\treturn -ENOMEM;\n}\n\nstatic void amdgpu_virt_ras_release_bp(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data *data = virt->virt_eh_data;\n\tstruct amdgpu_bo *bo;\n\tint i;\n\n\tif (!data)\n\t\treturn;\n\n\tfor (i = data->last_reserved - 1; i >= 0; i--) {\n\t\tbo = data->bps_bo[i];\n\t\tamdgpu_bo_free_kernel(&bo, NULL, NULL);\n\t\tdata->bps_bo[i] = bo;\n\t\tdata->last_reserved = i;\n\t}\n}\n\nvoid amdgpu_virt_release_ras_err_handler_data(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data *data = virt->virt_eh_data;\n\n\tvirt->ras_init_done = false;\n\n\tif (!data)\n\t\treturn;\n\n\tamdgpu_virt_ras_release_bp(adev);\n\n\tkfree(data->bps);\n\tkfree(data->bps_bo);\n\tkfree(data);\n\tvirt->virt_eh_data = NULL;\n}\n\nstatic void amdgpu_virt_ras_add_bps(struct amdgpu_device *adev,\n\t\tstruct eeprom_table_record *bps, int pages)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data *data = virt->virt_eh_data;\n\n\tif (!data)\n\t\treturn;\n\n\tmemcpy(&data->bps[data->count], bps, pages * sizeof(*data->bps));\n\tdata->count += pages;\n}\n\nstatic void amdgpu_virt_ras_reserve_bps(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data *data = virt->virt_eh_data;\n\tstruct amdgpu_bo *bo = NULL;\n\tuint64_t bp;\n\tint i;\n\n\tif (!data)\n\t\treturn;\n\n\tfor (i = data->last_reserved; i < data->count; i++) {\n\t\tbp = data->bps[i].retired_page;\n\n\t\t \n\t\tif (amdgpu_bo_create_kernel_at(adev, bp << AMDGPU_GPU_PAGE_SHIFT,\n\t\t\t\t\t       AMDGPU_GPU_PAGE_SIZE,\n\t\t\t\t\t       &bo, NULL))\n\t\t\tDRM_DEBUG(\"RAS WARN: reserve vram for retired page %llx fail\\n\", bp);\n\n\t\tdata->bps_bo[i] = bo;\n\t\tdata->last_reserved = i + 1;\n\t\tbo = NULL;\n\t}\n}\n\nstatic bool amdgpu_virt_ras_check_bad_page(struct amdgpu_device *adev,\n\t\tuint64_t retired_page)\n{\n\tstruct amdgpu_virt *virt = &adev->virt;\n\tstruct amdgpu_virt_ras_err_handler_data *data = virt->virt_eh_data;\n\tint i;\n\n\tif (!data)\n\t\treturn true;\n\n\tfor (i = 0; i < data->count; i++)\n\t\tif (retired_page == data->bps[i].retired_page)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic void amdgpu_virt_add_bad_page(struct amdgpu_device *adev,\n\t\tuint64_t bp_block_offset, uint32_t bp_block_size)\n{\n\tstruct eeprom_table_record bp;\n\tuint64_t retired_page;\n\tuint32_t bp_idx, bp_cnt;\n\tvoid *vram_usage_va = NULL;\n\n\tif (adev->mman.fw_vram_usage_va)\n\t\tvram_usage_va = adev->mman.fw_vram_usage_va;\n\telse\n\t\tvram_usage_va = adev->mman.drv_vram_usage_va;\n\n\tif (bp_block_size) {\n\t\tbp_cnt = bp_block_size / sizeof(uint64_t);\n\t\tfor (bp_idx = 0; bp_idx < bp_cnt; bp_idx++) {\n\t\t\tretired_page = *(uint64_t *)(vram_usage_va +\n\t\t\t\t\tbp_block_offset + bp_idx * sizeof(uint64_t));\n\t\t\tbp.retired_page = retired_page;\n\n\t\t\tif (amdgpu_virt_ras_check_bad_page(adev, retired_page))\n\t\t\t\tcontinue;\n\n\t\t\tamdgpu_virt_ras_add_bps(adev, &bp, 1);\n\n\t\t\tamdgpu_virt_ras_reserve_bps(adev);\n\t\t}\n\t}\n}\n\nstatic int amdgpu_virt_read_pf2vf_data(struct amdgpu_device *adev)\n{\n\tstruct amd_sriov_msg_pf2vf_info_header *pf2vf_info = adev->virt.fw_reserve.p_pf2vf;\n\tuint32_t checksum;\n\tuint32_t checkval;\n\n\tuint32_t i;\n\tuint32_t tmp;\n\n\tif (adev->virt.fw_reserve.p_pf2vf == NULL)\n\t\treturn -EINVAL;\n\n\tif (pf2vf_info->size > 1024) {\n\t\tDRM_ERROR(\"invalid pf2vf message size\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (pf2vf_info->version) {\n\tcase 1:\n\t\tchecksum = ((struct amdgim_pf2vf_info_v1 *)pf2vf_info)->checksum;\n\t\tcheckval = amd_sriov_msg_checksum(\n\t\t\tadev->virt.fw_reserve.p_pf2vf, pf2vf_info->size,\n\t\t\tadev->virt.fw_reserve.checksum_key, checksum);\n\t\tif (checksum != checkval) {\n\t\t\tDRM_ERROR(\"invalid pf2vf message\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tadev->virt.gim_feature =\n\t\t\t((struct amdgim_pf2vf_info_v1 *)pf2vf_info)->feature_flags;\n\t\tbreak;\n\tcase 2:\n\t\t \n\t\tchecksum = ((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->checksum;\n\t\tcheckval = amd_sriov_msg_checksum(\n\t\t\tadev->virt.fw_reserve.p_pf2vf, pf2vf_info->size,\n\t\t\t0, checksum);\n\t\tif (checksum != checkval) {\n\t\t\tDRM_ERROR(\"invalid pf2vf message\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tadev->virt.vf2pf_update_interval_ms =\n\t\t\t((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->vf2pf_update_interval_ms;\n\t\tadev->virt.gim_feature =\n\t\t\t((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->feature_flags.all;\n\t\tadev->virt.reg_access =\n\t\t\t((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->reg_access_flags.all;\n\n\t\tadev->virt.decode_max_dimension_pixels = 0;\n\t\tadev->virt.decode_max_frame_pixels = 0;\n\t\tadev->virt.encode_max_dimension_pixels = 0;\n\t\tadev->virt.encode_max_frame_pixels = 0;\n\t\tadev->virt.is_mm_bw_enabled = false;\n\t\tfor (i = 0; i < AMD_SRIOV_MSG_RESERVE_VCN_INST; i++) {\n\t\t\ttmp = ((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->mm_bw_management[i].decode_max_dimension_pixels;\n\t\t\tadev->virt.decode_max_dimension_pixels = max(tmp, adev->virt.decode_max_dimension_pixels);\n\n\t\t\ttmp = ((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->mm_bw_management[i].decode_max_frame_pixels;\n\t\t\tadev->virt.decode_max_frame_pixels = max(tmp, adev->virt.decode_max_frame_pixels);\n\n\t\t\ttmp = ((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->mm_bw_management[i].encode_max_dimension_pixels;\n\t\t\tadev->virt.encode_max_dimension_pixels = max(tmp, adev->virt.encode_max_dimension_pixels);\n\n\t\t\ttmp = ((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->mm_bw_management[i].encode_max_frame_pixels;\n\t\t\tadev->virt.encode_max_frame_pixels = max(tmp, adev->virt.encode_max_frame_pixels);\n\t\t}\n\t\tif ((adev->virt.decode_max_dimension_pixels > 0) || (adev->virt.encode_max_dimension_pixels > 0))\n\t\t\tadev->virt.is_mm_bw_enabled = true;\n\n\t\tadev->unique_id =\n\t\t\t((struct amd_sriov_msg_pf2vf_info *)pf2vf_info)->uuid;\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"invalid pf2vf version\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (adev->virt.vf2pf_update_interval_ms < 200 || adev->virt.vf2pf_update_interval_ms > 10000)\n\t\tadev->virt.vf2pf_update_interval_ms = 2000;\n\n\treturn 0;\n}\n\nstatic void amdgpu_virt_populate_vf2pf_ucode_info(struct amdgpu_device *adev)\n{\n\tstruct amd_sriov_msg_vf2pf_info *vf2pf_info;\n\tvf2pf_info = (struct amd_sriov_msg_vf2pf_info *) adev->virt.fw_reserve.p_vf2pf;\n\n\tif (adev->virt.fw_reserve.p_vf2pf == NULL)\n\t\treturn;\n\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_VCE,      adev->vce.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_UVD,      adev->uvd.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_MC,       adev->gmc.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_ME,       adev->gfx.me_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_PFP,      adev->gfx.pfp_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_CE,       adev->gfx.ce_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_RLC,      adev->gfx.rlc_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_RLC_SRLC, adev->gfx.rlc_srlc_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_RLC_SRLG, adev->gfx.rlc_srlg_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_RLC_SRLS, adev->gfx.rlc_srls_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_MEC,      adev->gfx.mec_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_MEC2,     adev->gfx.mec2_fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_SOS,      adev->psp.sos.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_ASD,\n\t\t\t    adev->psp.asd_context.bin_desc.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_TA_RAS,\n\t\t\t    adev->psp.ras_context.context.bin_desc.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_TA_XGMI,\n\t\t\t    adev->psp.xgmi_context.context.bin_desc.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_SMC,      adev->pm.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_SDMA,     adev->sdma.instance[0].fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_SDMA2,    adev->sdma.instance[1].fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_VCN,      adev->vcn.fw_version);\n\tPOPULATE_UCODE_INFO(vf2pf_info, AMD_SRIOV_UCODE_ID_DMCU,     adev->dm.dmcu_fw_version);\n}\n\nstatic int amdgpu_virt_write_vf2pf_data(struct amdgpu_device *adev)\n{\n\tstruct amd_sriov_msg_vf2pf_info *vf2pf_info;\n\n\tvf2pf_info = (struct amd_sriov_msg_vf2pf_info *) adev->virt.fw_reserve.p_vf2pf;\n\n\tif (adev->virt.fw_reserve.p_vf2pf == NULL)\n\t\treturn -EINVAL;\n\n\tmemset(vf2pf_info, 0, sizeof(struct amd_sriov_msg_vf2pf_info));\n\n\tvf2pf_info->header.size = sizeof(struct amd_sriov_msg_vf2pf_info);\n\tvf2pf_info->header.version = AMD_SRIOV_MSG_FW_VRAM_VF2PF_VER;\n\n#ifdef MODULE\n\tif (THIS_MODULE->version != NULL)\n\t\tstrcpy(vf2pf_info->driver_version, THIS_MODULE->version);\n\telse\n#endif\n\t\tstrcpy(vf2pf_info->driver_version, \"N/A\");\n\n\tvf2pf_info->pf2vf_version_required = 0;  \n\tvf2pf_info->driver_cert = 0;\n\tvf2pf_info->os_info.all = 0;\n\n\tvf2pf_info->fb_usage =\n\t\tttm_resource_manager_usage(&adev->mman.vram_mgr.manager) >> 20;\n\tvf2pf_info->fb_vis_usage =\n\t\tamdgpu_vram_mgr_vis_usage(&adev->mman.vram_mgr) >> 20;\n\tvf2pf_info->fb_size = adev->gmc.real_vram_size >> 20;\n\tvf2pf_info->fb_vis_size = adev->gmc.visible_vram_size >> 20;\n\n\tamdgpu_virt_populate_vf2pf_ucode_info(adev);\n\n\t \n\tvf2pf_info->gfx_usage = 0;\n\tvf2pf_info->compute_usage = 0;\n\tvf2pf_info->encode_usage = 0;\n\tvf2pf_info->decode_usage = 0;\n\n\tvf2pf_info->dummy_page_addr = (uint64_t)adev->dummy_page_addr;\n\tvf2pf_info->checksum =\n\t\tamd_sriov_msg_checksum(\n\t\tvf2pf_info, vf2pf_info->header.size, 0, 0);\n\n\treturn 0;\n}\n\nstatic void amdgpu_virt_update_vf2pf_work_item(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev = container_of(work, struct amdgpu_device, virt.vf2pf_work.work);\n\tint ret;\n\n\tret = amdgpu_virt_read_pf2vf_data(adev);\n\tif (ret)\n\t\tgoto out;\n\tamdgpu_virt_write_vf2pf_data(adev);\n\nout:\n\tschedule_delayed_work(&(adev->virt.vf2pf_work), adev->virt.vf2pf_update_interval_ms);\n}\n\nvoid amdgpu_virt_fini_data_exchange(struct amdgpu_device *adev)\n{\n\tif (adev->virt.vf2pf_update_interval_ms != 0) {\n\t\tDRM_INFO(\"clean up the vf2pf work item\\n\");\n\t\tcancel_delayed_work_sync(&adev->virt.vf2pf_work);\n\t\tadev->virt.vf2pf_update_interval_ms = 0;\n\t}\n}\n\nvoid amdgpu_virt_init_data_exchange(struct amdgpu_device *adev)\n{\n\tadev->virt.fw_reserve.p_pf2vf = NULL;\n\tadev->virt.fw_reserve.p_vf2pf = NULL;\n\tadev->virt.vf2pf_update_interval_ms = 0;\n\n\tif (adev->mman.fw_vram_usage_va && adev->mman.drv_vram_usage_va) {\n\t\tDRM_WARN(\"Currently fw_vram and drv_vram should not have values at the same time!\");\n\t} else if (adev->mman.fw_vram_usage_va || adev->mman.drv_vram_usage_va) {\n\t\t \n\t\tamdgpu_virt_exchange_data(adev);\n\n\t\tINIT_DELAYED_WORK(&adev->virt.vf2pf_work, amdgpu_virt_update_vf2pf_work_item);\n\t\tschedule_delayed_work(&(adev->virt.vf2pf_work), msecs_to_jiffies(adev->virt.vf2pf_update_interval_ms));\n\t} else if (adev->bios != NULL) {\n\t\t \n\t\tadev->virt.fw_reserve.p_pf2vf =\n\t\t\t(struct amd_sriov_msg_pf2vf_info_header *)\n\t\t\t(adev->bios + (AMD_SRIOV_MSG_PF2VF_OFFSET_KB << 10));\n\n\t\tamdgpu_virt_read_pf2vf_data(adev);\n\t}\n}\n\n\nvoid amdgpu_virt_exchange_data(struct amdgpu_device *adev)\n{\n\tuint64_t bp_block_offset = 0;\n\tuint32_t bp_block_size = 0;\n\tstruct amd_sriov_msg_pf2vf_info *pf2vf_v2 = NULL;\n\n\tif (adev->mman.fw_vram_usage_va || adev->mman.drv_vram_usage_va) {\n\t\tif (adev->mman.fw_vram_usage_va) {\n\t\t\tadev->virt.fw_reserve.p_pf2vf =\n\t\t\t\t(struct amd_sriov_msg_pf2vf_info_header *)\n\t\t\t\t(adev->mman.fw_vram_usage_va + (AMD_SRIOV_MSG_PF2VF_OFFSET_KB << 10));\n\t\t\tadev->virt.fw_reserve.p_vf2pf =\n\t\t\t\t(struct amd_sriov_msg_vf2pf_info_header *)\n\t\t\t\t(adev->mman.fw_vram_usage_va + (AMD_SRIOV_MSG_VF2PF_OFFSET_KB << 10));\n\t\t} else if (adev->mman.drv_vram_usage_va) {\n\t\t\tadev->virt.fw_reserve.p_pf2vf =\n\t\t\t\t(struct amd_sriov_msg_pf2vf_info_header *)\n\t\t\t\t(adev->mman.drv_vram_usage_va + (AMD_SRIOV_MSG_PF2VF_OFFSET_KB << 10));\n\t\t\tadev->virt.fw_reserve.p_vf2pf =\n\t\t\t\t(struct amd_sriov_msg_vf2pf_info_header *)\n\t\t\t\t(adev->mman.drv_vram_usage_va + (AMD_SRIOV_MSG_VF2PF_OFFSET_KB << 10));\n\t\t}\n\n\t\tamdgpu_virt_read_pf2vf_data(adev);\n\t\tamdgpu_virt_write_vf2pf_data(adev);\n\n\t\t \n\t\tif (adev->virt.fw_reserve.p_pf2vf->version == 2) {\n\t\t\tpf2vf_v2 = (struct amd_sriov_msg_pf2vf_info *)adev->virt.fw_reserve.p_pf2vf;\n\n\t\t\tbp_block_offset = ((uint64_t)pf2vf_v2->bp_block_offset_low & 0xFFFFFFFF) |\n\t\t\t\t((((uint64_t)pf2vf_v2->bp_block_offset_high) << 32) & 0xFFFFFFFF00000000);\n\t\t\tbp_block_size = pf2vf_v2->bp_block_size;\n\n\t\t\tif (bp_block_size && !adev->virt.ras_init_done)\n\t\t\t\tamdgpu_virt_init_ras_err_handler_data(adev);\n\n\t\t\tif (adev->virt.ras_init_done)\n\t\t\t\tamdgpu_virt_add_bad_page(adev, bp_block_offset, bp_block_size);\n\t\t}\n\t}\n}\n\nvoid amdgpu_detect_virtualization(struct amdgpu_device *adev)\n{\n\tuint32_t reg;\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TONGA:\n\tcase CHIP_FIJI:\n\t\treg = RREG32(mmBIF_IOV_FUNC_IDENTIFIER);\n\t\tbreak;\n\tcase CHIP_VEGA10:\n\tcase CHIP_VEGA20:\n\tcase CHIP_NAVI10:\n\tcase CHIP_NAVI12:\n\tcase CHIP_SIENNA_CICHLID:\n\tcase CHIP_ARCTURUS:\n\tcase CHIP_ALDEBARAN:\n\tcase CHIP_IP_DISCOVERY:\n\t\treg = RREG32(mmRCC_IOV_FUNC_IDENTIFIER);\n\t\tbreak;\n\tdefault:  \n\t\treg = 0;\n\t\tbreak;\n\t}\n\n\tif (reg & 1)\n\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_IS_VF;\n\n\tif (reg & 0x80000000)\n\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_ENABLE_IOV;\n\n\tif (!reg) {\n\t\t \n\t\tif (is_virtual_machine() && !xen_initial_domain())\n\t\t\tadev->virt.caps |= AMDGPU_PASSTHROUGH_MODE;\n\t}\n\n\tif (amdgpu_sriov_vf(adev) && adev->asic_type == CHIP_SIENNA_CICHLID)\n\t\t \n\t\tadev->virt.caps |= AMDGPU_VF_MMIO_ACCESS_PROTECT;\n\n\t \n\tif (amdgpu_sriov_vf(adev)) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_TONGA:\n\t\tcase CHIP_FIJI:\n\t\t\tvi_set_virt_ops(adev);\n\t\t\tbreak;\n\t\tcase CHIP_VEGA10:\n\t\t\tsoc15_set_virt_ops(adev);\n#ifdef CONFIG_X86\n\t\t\t \n\t\t\tif (!hypervisor_is_type(X86_HYPER_MS_HYPERV))\n#endif\n\t\t\t\t \n\t\t\t\tamdgpu_virt_request_init_data(adev);\n\t\t\tbreak;\n\t\tcase CHIP_VEGA20:\n\t\tcase CHIP_ARCTURUS:\n\t\tcase CHIP_ALDEBARAN:\n\t\t\tsoc15_set_virt_ops(adev);\n\t\t\tbreak;\n\t\tcase CHIP_NAVI10:\n\t\tcase CHIP_NAVI12:\n\t\tcase CHIP_SIENNA_CICHLID:\n\t\tcase CHIP_IP_DISCOVERY:\n\t\t\tnv_set_virt_ops(adev);\n\t\t\t \n\t\t\tamdgpu_virt_request_init_data(adev);\n\t\t\tbreak;\n\t\tdefault:  \n\t\t\tDRM_ERROR(\"Unknown asic type: %d!\\n\", adev->asic_type);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic bool amdgpu_virt_access_debugfs_is_mmio(struct amdgpu_device *adev)\n{\n\treturn amdgpu_sriov_is_debug(adev) ? true : false;\n}\n\nstatic bool amdgpu_virt_access_debugfs_is_kiq(struct amdgpu_device *adev)\n{\n\treturn amdgpu_sriov_is_normal(adev) ? true : false;\n}\n\nint amdgpu_virt_enable_access_debugfs(struct amdgpu_device *adev)\n{\n\tif (!amdgpu_sriov_vf(adev) ||\n\t    amdgpu_virt_access_debugfs_is_kiq(adev))\n\t\treturn 0;\n\n\tif (amdgpu_virt_access_debugfs_is_mmio(adev))\n\t\tadev->virt.caps &= ~AMDGPU_SRIOV_CAPS_RUNTIME;\n\telse\n\t\treturn -EPERM;\n\n\treturn 0;\n}\n\nvoid amdgpu_virt_disable_access_debugfs(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sriov_vf(adev))\n\t\tadev->virt.caps |= AMDGPU_SRIOV_CAPS_RUNTIME;\n}\n\nenum amdgpu_sriov_vf_mode amdgpu_virt_get_sriov_vf_mode(struct amdgpu_device *adev)\n{\n\tenum amdgpu_sriov_vf_mode mode;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tif (amdgpu_sriov_is_pp_one_vf(adev))\n\t\t\tmode = SRIOV_VF_MODE_ONE_VF;\n\t\telse\n\t\t\tmode = SRIOV_VF_MODE_MULTI_VF;\n\t} else {\n\t\tmode = SRIOV_VF_MODE_BARE_METAL;\n\t}\n\n\treturn mode;\n}\n\nvoid amdgpu_virt_post_reset(struct amdgpu_device *adev)\n{\n\tif (adev->ip_versions[GC_HWIP][0] == IP_VERSION(11, 0, 3)) {\n\t\t \n\t\tadev->gfx.is_poweron = false;\n\t}\n}\n\nbool amdgpu_virt_fw_load_skip_check(struct amdgpu_device *adev, uint32_t ucode_id)\n{\n\tswitch (adev->ip_versions[MP0_HWIP][0]) {\n\tcase IP_VERSION(13, 0, 0):\n\t\t \n\t\tif (ucode_id == AMDGPU_UCODE_ID_VCN1 ||\n\t\t    ucode_id == AMDGPU_UCODE_ID_VCN)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\tcase IP_VERSION(11, 0, 9):\n\tcase IP_VERSION(11, 0, 7):\n\t\t \n\t\tif (ucode_id == AMDGPU_UCODE_ID_RLC_G\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SMC)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\tcase IP_VERSION(13, 0, 10):\n\t\t \n\t\tif (ucode_id == AMDGPU_UCODE_ID_CAP\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_PFP\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_ME\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_MEC\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_PFP_P0_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_PFP_P1_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_ME_P0_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_ME_P1_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_MEC_P0_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_MEC_P1_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_MEC_P2_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_RS64_MEC_P3_STACK\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_MES\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_MES_DATA\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_MES1\n\t\t|| ucode_id == AMDGPU_UCODE_ID_CP_MES1_DATA\n\t\t|| ucode_id == AMDGPU_UCODE_ID_VCN1\n\t\t|| ucode_id == AMDGPU_UCODE_ID_VCN)\n\t\t\treturn false;\n\t\telse\n\t\t\treturn true;\n\tdefault:\n\t\t \n\t\tif (ucode_id == AMDGPU_UCODE_ID_SDMA0\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA1\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA2\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA3\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA4\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA5\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA6\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SDMA7\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_G\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_CNTL\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_GPM_MEM\n\t\t    || ucode_id == AMDGPU_UCODE_ID_RLC_RESTORE_LIST_SRM_MEM\n\t\t    || ucode_id == AMDGPU_UCODE_ID_SMC)\n\t\t\treturn true;\n\t\telse\n\t\t\treturn false;\n\t}\n}\n\nvoid amdgpu_virt_update_sriov_video_codec(struct amdgpu_device *adev,\n\t\t\tstruct amdgpu_video_codec_info *encode, uint32_t encode_array_size,\n\t\t\tstruct amdgpu_video_codec_info *decode, uint32_t decode_array_size)\n{\n\tuint32_t i;\n\n\tif (!adev->virt.is_mm_bw_enabled)\n\t\treturn;\n\n\tif (encode) {\n\t\tfor (i = 0; i < encode_array_size; i++) {\n\t\t\tencode[i].max_width = adev->virt.encode_max_dimension_pixels;\n\t\t\tencode[i].max_pixels_per_frame = adev->virt.encode_max_frame_pixels;\n\t\t\tif (encode[i].max_width > 0)\n\t\t\t\tencode[i].max_height = encode[i].max_pixels_per_frame / encode[i].max_width;\n\t\t\telse\n\t\t\t\tencode[i].max_height = 0;\n\t\t}\n\t}\n\n\tif (decode) {\n\t\tfor (i = 0; i < decode_array_size; i++) {\n\t\t\tdecode[i].max_width = adev->virt.decode_max_dimension_pixels;\n\t\t\tdecode[i].max_pixels_per_frame = adev->virt.decode_max_frame_pixels;\n\t\t\tif (decode[i].max_width > 0)\n\t\t\t\tdecode[i].max_height = decode[i].max_pixels_per_frame / decode[i].max_width;\n\t\t\telse\n\t\t\t\tdecode[i].max_height = 0;\n\t\t}\n\t}\n}\n\nstatic bool amdgpu_virt_get_rlcg_reg_access_flag(struct amdgpu_device *adev,\n\t\t\t\t\t\t u32 acc_flags, u32 hwip,\n\t\t\t\t\t\t bool write, u32 *rlcg_flag)\n{\n\tbool ret = false;\n\n\tswitch (hwip) {\n\tcase GC_HWIP:\n\t\tif (amdgpu_sriov_reg_indirect_gc(adev)) {\n\t\t\t*rlcg_flag =\n\t\t\t\twrite ? AMDGPU_RLCG_GC_WRITE : AMDGPU_RLCG_GC_READ;\n\t\t\tret = true;\n\t\t \n\t\t} else if ((acc_flags & AMDGPU_REGS_RLC) &&\n\t\t\t\t!(acc_flags & AMDGPU_REGS_NO_KIQ) && write) {\n\t\t\t*rlcg_flag = AMDGPU_RLCG_GC_WRITE_LEGACY;\n\t\t\tret = true;\n\t\t}\n\t\tbreak;\n\tcase MMHUB_HWIP:\n\t\tif (amdgpu_sriov_reg_indirect_mmhub(adev) &&\n\t\t    (acc_flags & AMDGPU_REGS_RLC) && write) {\n\t\t\t*rlcg_flag = AMDGPU_RLCG_MMHUB_WRITE;\n\t\t\tret = true;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic u32 amdgpu_virt_rlcg_reg_rw(struct amdgpu_device *adev, u32 offset, u32 v, u32 flag, u32 xcc_id)\n{\n\tstruct amdgpu_rlcg_reg_access_ctrl *reg_access_ctrl;\n\tuint32_t timeout = 50000;\n\tuint32_t i, tmp;\n\tuint32_t ret = 0;\n\tvoid *scratch_reg0;\n\tvoid *scratch_reg1;\n\tvoid *scratch_reg2;\n\tvoid *scratch_reg3;\n\tvoid *spare_int;\n\n\tif (!adev->gfx.rlc.rlcg_reg_access_supported) {\n\t\tdev_err(adev->dev,\n\t\t\t\"indirect registers access through rlcg is not available\\n\");\n\t\treturn 0;\n\t}\n\n\tif (adev->gfx.xcc_mask && (((1 << xcc_id) & adev->gfx.xcc_mask) == 0)) {\n\t\tdev_err(adev->dev, \"invalid xcc\\n\");\n\t\treturn 0;\n\t}\n\n\treg_access_ctrl = &adev->gfx.rlc.reg_access_ctrl[xcc_id];\n\tscratch_reg0 = (void __iomem *)adev->rmmio + 4 * reg_access_ctrl->scratch_reg0;\n\tscratch_reg1 = (void __iomem *)adev->rmmio + 4 * reg_access_ctrl->scratch_reg1;\n\tscratch_reg2 = (void __iomem *)adev->rmmio + 4 * reg_access_ctrl->scratch_reg2;\n\tscratch_reg3 = (void __iomem *)adev->rmmio + 4 * reg_access_ctrl->scratch_reg3;\n\tif (reg_access_ctrl->spare_int)\n\t\tspare_int = (void __iomem *)adev->rmmio + 4 * reg_access_ctrl->spare_int;\n\n\tif (offset == reg_access_ctrl->grbm_cntl) {\n\t\t \n\t\twritel(v, scratch_reg2);\n\t\tif (flag == AMDGPU_RLCG_GC_WRITE_LEGACY)\n\t\t\twritel(v, ((void __iomem *)adev->rmmio) + (offset * 4));\n\t} else if (offset == reg_access_ctrl->grbm_idx) {\n\t\t \n\t\twritel(v, scratch_reg3);\n\t\tif (flag == AMDGPU_RLCG_GC_WRITE_LEGACY)\n\t\t\twritel(v, ((void __iomem *)adev->rmmio) + (offset * 4));\n\t} else {\n\t\t \n\t\twritel(v, scratch_reg0);\n\t\twritel((offset | flag), scratch_reg1);\n\t\tif (reg_access_ctrl->spare_int)\n\t\t\twritel(1, spare_int);\n\n\t\tfor (i = 0; i < timeout; i++) {\n\t\t\ttmp = readl(scratch_reg1);\n\t\t\tif (!(tmp & AMDGPU_RLCG_SCRATCH1_ADDRESS_MASK))\n\t\t\t\tbreak;\n\t\t\tudelay(10);\n\t\t}\n\n\t\tif (i >= timeout) {\n\t\t\tif (amdgpu_sriov_rlcg_error_report_enabled(adev)) {\n\t\t\t\tif (tmp & AMDGPU_RLCG_VFGATE_DISABLED) {\n\t\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\t\"vfgate is disabled, rlcg failed to program reg: 0x%05x\\n\", offset);\n\t\t\t\t} else if (tmp & AMDGPU_RLCG_WRONG_OPERATION_TYPE) {\n\t\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\t\"wrong operation type, rlcg failed to program reg: 0x%05x\\n\", offset);\n\t\t\t\t} else if (tmp & AMDGPU_RLCG_REG_NOT_IN_RANGE) {\n\t\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\t\"register is not in range, rlcg failed to program reg: 0x%05x\\n\", offset);\n\t\t\t\t} else {\n\t\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\t\"unknown error type, rlcg failed to program reg: 0x%05x\\n\", offset);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_err(adev->dev,\n\t\t\t\t\t\"timeout: rlcg faled to program reg: 0x%05x\\n\", offset);\n\t\t\t}\n\t\t}\n\t}\n\n\tret = readl(scratch_reg0);\n\treturn ret;\n}\n\nvoid amdgpu_sriov_wreg(struct amdgpu_device *adev,\n\t\t       u32 offset, u32 value,\n\t\t       u32 acc_flags, u32 hwip, u32 xcc_id)\n{\n\tu32 rlcg_flag;\n\n\tif (!amdgpu_sriov_runtime(adev) &&\n\t\tamdgpu_virt_get_rlcg_reg_access_flag(adev, acc_flags, hwip, true, &rlcg_flag)) {\n\t\tamdgpu_virt_rlcg_reg_rw(adev, offset, value, rlcg_flag, xcc_id);\n\t\treturn;\n\t}\n\n\tif (acc_flags & AMDGPU_REGS_NO_KIQ)\n\t\tWREG32_NO_KIQ(offset, value);\n\telse\n\t\tWREG32(offset, value);\n}\n\nu32 amdgpu_sriov_rreg(struct amdgpu_device *adev,\n\t\t      u32 offset, u32 acc_flags, u32 hwip, u32 xcc_id)\n{\n\tu32 rlcg_flag;\n\n\tif (!amdgpu_sriov_runtime(adev) &&\n\t\tamdgpu_virt_get_rlcg_reg_access_flag(adev, acc_flags, hwip, false, &rlcg_flag))\n\t\treturn amdgpu_virt_rlcg_reg_rw(adev, offset, 0, rlcg_flag, xcc_id);\n\n\tif (acc_flags & AMDGPU_REGS_NO_KIQ)\n\t\treturn RREG32_NO_KIQ(offset);\n\telse\n\t\treturn RREG32(offset);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}