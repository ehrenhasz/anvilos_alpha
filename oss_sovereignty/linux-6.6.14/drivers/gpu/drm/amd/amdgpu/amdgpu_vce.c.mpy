{
  "module_name": "amdgpu_vce.c",
  "hash_id": "7724633faa84c8ca515da59f56f3ab27c597890cf9c853fb37ac467b73fe9804",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n\n#include <drm/drm.h>\n#include <drm/drm_drv.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_pm.h\"\n#include \"amdgpu_vce.h\"\n#include \"amdgpu_cs.h\"\n#include \"cikd.h\"\n\n \n#define VCE_IDLE_TIMEOUT\tmsecs_to_jiffies(1000)\n\n \n#ifdef CONFIG_DRM_AMDGPU_CIK\n#define FIRMWARE_BONAIRE\t\"amdgpu/bonaire_vce.bin\"\n#define FIRMWARE_KABINI\t\"amdgpu/kabini_vce.bin\"\n#define FIRMWARE_KAVERI\t\"amdgpu/kaveri_vce.bin\"\n#define FIRMWARE_HAWAII\t\"amdgpu/hawaii_vce.bin\"\n#define FIRMWARE_MULLINS\t\"amdgpu/mullins_vce.bin\"\n#endif\n#define FIRMWARE_TONGA\t\t\"amdgpu/tonga_vce.bin\"\n#define FIRMWARE_CARRIZO\t\"amdgpu/carrizo_vce.bin\"\n#define FIRMWARE_FIJI\t\t\"amdgpu/fiji_vce.bin\"\n#define FIRMWARE_STONEY\t\t\"amdgpu/stoney_vce.bin\"\n#define FIRMWARE_POLARIS10\t\"amdgpu/polaris10_vce.bin\"\n#define FIRMWARE_POLARIS11\t\"amdgpu/polaris11_vce.bin\"\n#define FIRMWARE_POLARIS12\t\"amdgpu/polaris12_vce.bin\"\n#define FIRMWARE_VEGAM\t\t\"amdgpu/vegam_vce.bin\"\n\n#define FIRMWARE_VEGA10\t\t\"amdgpu/vega10_vce.bin\"\n#define FIRMWARE_VEGA12\t\t\"amdgpu/vega12_vce.bin\"\n#define FIRMWARE_VEGA20\t\t\"amdgpu/vega20_vce.bin\"\n\n#ifdef CONFIG_DRM_AMDGPU_CIK\nMODULE_FIRMWARE(FIRMWARE_BONAIRE);\nMODULE_FIRMWARE(FIRMWARE_KABINI);\nMODULE_FIRMWARE(FIRMWARE_KAVERI);\nMODULE_FIRMWARE(FIRMWARE_HAWAII);\nMODULE_FIRMWARE(FIRMWARE_MULLINS);\n#endif\nMODULE_FIRMWARE(FIRMWARE_TONGA);\nMODULE_FIRMWARE(FIRMWARE_CARRIZO);\nMODULE_FIRMWARE(FIRMWARE_FIJI);\nMODULE_FIRMWARE(FIRMWARE_STONEY);\nMODULE_FIRMWARE(FIRMWARE_POLARIS10);\nMODULE_FIRMWARE(FIRMWARE_POLARIS11);\nMODULE_FIRMWARE(FIRMWARE_POLARIS12);\nMODULE_FIRMWARE(FIRMWARE_VEGAM);\n\nMODULE_FIRMWARE(FIRMWARE_VEGA10);\nMODULE_FIRMWARE(FIRMWARE_VEGA12);\nMODULE_FIRMWARE(FIRMWARE_VEGA20);\n\nstatic void amdgpu_vce_idle_work_handler(struct work_struct *work);\nstatic int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t     struct dma_fence **fence);\nstatic int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t      bool direct, struct dma_fence **fence);\n\n \nint amdgpu_vce_sw_init(struct amdgpu_device *adev, unsigned long size)\n{\n\tconst char *fw_name;\n\tconst struct common_firmware_header *hdr;\n\tunsigned int ucode_version, version_major, version_minor, binary_id;\n\tint i, r;\n\n\tswitch (adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\tcase CHIP_BONAIRE:\n\t\tfw_name = FIRMWARE_BONAIRE;\n\t\tbreak;\n\tcase CHIP_KAVERI:\n\t\tfw_name = FIRMWARE_KAVERI;\n\t\tbreak;\n\tcase CHIP_KABINI:\n\t\tfw_name = FIRMWARE_KABINI;\n\t\tbreak;\n\tcase CHIP_HAWAII:\n\t\tfw_name = FIRMWARE_HAWAII;\n\t\tbreak;\n\tcase CHIP_MULLINS:\n\t\tfw_name = FIRMWARE_MULLINS;\n\t\tbreak;\n#endif\n\tcase CHIP_TONGA:\n\t\tfw_name = FIRMWARE_TONGA;\n\t\tbreak;\n\tcase CHIP_CARRIZO:\n\t\tfw_name = FIRMWARE_CARRIZO;\n\t\tbreak;\n\tcase CHIP_FIJI:\n\t\tfw_name = FIRMWARE_FIJI;\n\t\tbreak;\n\tcase CHIP_STONEY:\n\t\tfw_name = FIRMWARE_STONEY;\n\t\tbreak;\n\tcase CHIP_POLARIS10:\n\t\tfw_name = FIRMWARE_POLARIS10;\n\t\tbreak;\n\tcase CHIP_POLARIS11:\n\t\tfw_name = FIRMWARE_POLARIS11;\n\t\tbreak;\n\tcase CHIP_POLARIS12:\n\t\tfw_name = FIRMWARE_POLARIS12;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tfw_name = FIRMWARE_VEGAM;\n\t\tbreak;\n\tcase CHIP_VEGA10:\n\t\tfw_name = FIRMWARE_VEGA10;\n\t\tbreak;\n\tcase CHIP_VEGA12:\n\t\tfw_name = FIRMWARE_VEGA12;\n\t\tbreak;\n\tcase CHIP_VEGA20:\n\t\tfw_name = FIRMWARE_VEGA20;\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tr = amdgpu_ucode_request(adev, &adev->vce.fw, fw_name);\n\tif (r) {\n\t\tdev_err(adev->dev, \"amdgpu_vce: Can't validate firmware \\\"%s\\\"\\n\",\n\t\t\tfw_name);\n\t\tamdgpu_ucode_release(&adev->vce.fw);\n\t\treturn r;\n\t}\n\n\thdr = (const struct common_firmware_header *)adev->vce.fw->data;\n\n\tucode_version = le32_to_cpu(hdr->ucode_version);\n\tversion_major = (ucode_version >> 20) & 0xfff;\n\tversion_minor = (ucode_version >> 8) & 0xfff;\n\tbinary_id = ucode_version & 0xff;\n\tDRM_INFO(\"Found VCE firmware Version: %d.%d Binary ID: %d\\n\",\n\t\tversion_major, version_minor, binary_id);\n\tadev->vce.fw_version = ((version_major << 24) | (version_minor << 16) |\n\t\t\t\t(binary_id << 8));\n\n\tr = amdgpu_bo_create_kernel(adev, size, PAGE_SIZE,\n\t\t\t\t    AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t    AMDGPU_GEM_DOMAIN_GTT,\n\t\t\t\t    &adev->vce.vcpu_bo,\n\t\t\t\t    &adev->vce.gpu_addr, &adev->vce.cpu_addr);\n\tif (r) {\n\t\tdev_err(adev->dev, \"(%d) failed to allocate VCE bo\\n\", r);\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\n\t\tatomic_set(&adev->vce.handles[i], 0);\n\t\tadev->vce.filp[i] = NULL;\n\t}\n\n\tINIT_DELAYED_WORK(&adev->vce.idle_work, amdgpu_vce_idle_work_handler);\n\tmutex_init(&adev->vce.idle_mutex);\n\n\treturn 0;\n}\n\n \nint amdgpu_vce_sw_fini(struct amdgpu_device *adev)\n{\n\tunsigned int i;\n\n\tif (adev->vce.vcpu_bo == NULL)\n\t\treturn 0;\n\n\tdrm_sched_entity_destroy(&adev->vce.entity);\n\n\tamdgpu_bo_free_kernel(&adev->vce.vcpu_bo, &adev->vce.gpu_addr,\n\t\t(void **)&adev->vce.cpu_addr);\n\n\tfor (i = 0; i < adev->vce.num_rings; i++)\n\t\tamdgpu_ring_fini(&adev->vce.ring[i]);\n\n\tamdgpu_ucode_release(&adev->vce.fw);\n\tmutex_destroy(&adev->vce.idle_mutex);\n\n\treturn 0;\n}\n\n \nint amdgpu_vce_entity_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tstruct drm_gpu_scheduler *sched;\n\tint r;\n\n\tring = &adev->vce.ring[0];\n\tsched = &ring->sched;\n\tr = drm_sched_entity_init(&adev->vce.entity, DRM_SCHED_PRIORITY_NORMAL,\n\t\t\t\t  &sched, 1, NULL);\n\tif (r != 0) {\n\t\tDRM_ERROR(\"Failed setting up VCE run queue.\\n\");\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_vce_suspend(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tif (adev->vce.vcpu_bo == NULL)\n\t\treturn 0;\n\n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i)\n\t\tif (atomic_read(&adev->vce.handles[i]))\n\t\t\tbreak;\n\n\tif (i == AMDGPU_MAX_VCE_HANDLES)\n\t\treturn 0;\n\n\t \n\treturn -EINVAL;\n}\n\n \nint amdgpu_vce_resume(struct amdgpu_device *adev)\n{\n\tvoid *cpu_addr;\n\tconst struct common_firmware_header *hdr;\n\tunsigned int offset;\n\tint r, idx;\n\n\tif (adev->vce.vcpu_bo == NULL)\n\t\treturn -EINVAL;\n\n\tr = amdgpu_bo_reserve(adev->vce.vcpu_bo, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"(%d) failed to reserve VCE bo\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = amdgpu_bo_kmap(adev->vce.vcpu_bo, &cpu_addr);\n\tif (r) {\n\t\tamdgpu_bo_unreserve(adev->vce.vcpu_bo);\n\t\tdev_err(adev->dev, \"(%d) VCE map failed\\n\", r);\n\t\treturn r;\n\t}\n\n\thdr = (const struct common_firmware_header *)adev->vce.fw->data;\n\toffset = le32_to_cpu(hdr->ucode_array_offset_bytes);\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tmemcpy_toio(cpu_addr, adev->vce.fw->data + offset,\n\t\t\t    adev->vce.fw->size - offset);\n\t\tdrm_dev_exit(idx);\n\t}\n\n\tamdgpu_bo_kunmap(adev->vce.vcpu_bo);\n\n\tamdgpu_bo_unreserve(adev->vce.vcpu_bo);\n\n\treturn 0;\n}\n\n \nstatic void amdgpu_vce_idle_work_handler(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev =\n\t\tcontainer_of(work, struct amdgpu_device, vce.idle_work.work);\n\tunsigned int i, count = 0;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++)\n\t\tcount += amdgpu_fence_count_emitted(&adev->vce.ring[i]);\n\n\tif (count == 0) {\n\t\tif (adev->pm.dpm_enabled) {\n\t\t\tamdgpu_dpm_enable_vce(adev, false);\n\t\t} else {\n\t\t\tamdgpu_asic_set_vce_clocks(adev, 0, 0);\n\t\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t\t       AMD_PG_STATE_GATE);\n\t\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t\t       AMD_CG_STATE_GATE);\n\t\t}\n\t} else {\n\t\tschedule_delayed_work(&adev->vce.idle_work, VCE_IDLE_TIMEOUT);\n\t}\n}\n\n \nvoid amdgpu_vce_ring_begin_use(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tbool set_clocks;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tmutex_lock(&adev->vce.idle_mutex);\n\tset_clocks = !cancel_delayed_work_sync(&adev->vce.idle_work);\n\tif (set_clocks) {\n\t\tif (adev->pm.dpm_enabled) {\n\t\t\tamdgpu_dpm_enable_vce(adev, true);\n\t\t} else {\n\t\t\tamdgpu_asic_set_vce_clocks(adev, 53300, 40000);\n\t\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t\t       AMD_CG_STATE_UNGATE);\n\t\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t\t       AMD_PG_STATE_UNGATE);\n\n\t\t}\n\t}\n\tmutex_unlock(&adev->vce.idle_mutex);\n}\n\n \nvoid amdgpu_vce_ring_end_use(struct amdgpu_ring *ring)\n{\n\tif (!amdgpu_sriov_vf(ring->adev))\n\t\tschedule_delayed_work(&ring->adev->vce.idle_work, VCE_IDLE_TIMEOUT);\n}\n\n \nvoid amdgpu_vce_free_handles(struct amdgpu_device *adev, struct drm_file *filp)\n{\n\tstruct amdgpu_ring *ring = &adev->vce.ring[0];\n\tint i, r;\n\n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\n\t\tuint32_t handle = atomic_read(&adev->vce.handles[i]);\n\n\t\tif (!handle || adev->vce.filp[i] != filp)\n\t\t\tcontinue;\n\n\t\tr = amdgpu_vce_get_destroy_msg(ring, handle, false, NULL);\n\t\tif (r)\n\t\t\tDRM_ERROR(\"Error destroying VCE handle (%d)!\\n\", r);\n\n\t\tadev->vce.filp[i] = NULL;\n\t\tatomic_set(&adev->vce.handles[i], 0);\n\t}\n}\n\n \nstatic int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t     struct dma_fence **fence)\n{\n\tconst unsigned int ib_size_dw = 1024;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tstruct amdgpu_ib ib_msg;\n\tstruct dma_fence *f = NULL;\n\tuint64_t addr;\n\tint i, r;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, &ring->adev->vce.entity,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     ib_size_dw * 4, AMDGPU_IB_POOL_DIRECT,\n\t\t\t\t     &job);\n\tif (r)\n\t\treturn r;\n\n\tmemset(&ib_msg, 0, sizeof(ib_msg));\n\t \n\tr = amdgpu_ib_get(ring->adev, NULL, AMDGPU_GPU_PAGE_SIZE * 2,\n\t\t\t  AMDGPU_IB_POOL_DIRECT,\n\t\t\t  &ib_msg);\n\tif (r)\n\t\tgoto err;\n\n\tib = &job->ibs[0];\n\t \n\taddr = AMDGPU_GPU_PAGE_ALIGN(ib_msg.gpu_addr);\n\n\t \n\tib->length_dw = 0;\n\tib->ptr[ib->length_dw++] = 0x0000000c;  \n\tib->ptr[ib->length_dw++] = 0x00000001;  \n\tib->ptr[ib->length_dw++] = handle;\n\n\tif ((ring->adev->vce.fw_version >> 24) >= 52)\n\t\tib->ptr[ib->length_dw++] = 0x00000040;  \n\telse\n\t\tib->ptr[ib->length_dw++] = 0x00000030;  \n\tib->ptr[ib->length_dw++] = 0x01000001;  \n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tib->ptr[ib->length_dw++] = 0x00000042;\n\tib->ptr[ib->length_dw++] = 0x0000000a;\n\tib->ptr[ib->length_dw++] = 0x00000001;\n\tib->ptr[ib->length_dw++] = 0x00000080;\n\tib->ptr[ib->length_dw++] = 0x00000060;\n\tib->ptr[ib->length_dw++] = 0x00000100;\n\tib->ptr[ib->length_dw++] = 0x00000100;\n\tib->ptr[ib->length_dw++] = 0x0000000c;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tif ((ring->adev->vce.fw_version >> 24) >= 52) {\n\t\tib->ptr[ib->length_dw++] = 0x00000000;\n\t\tib->ptr[ib->length_dw++] = 0x00000000;\n\t\tib->ptr[ib->length_dw++] = 0x00000000;\n\t\tib->ptr[ib->length_dw++] = 0x00000000;\n\t}\n\n\tib->ptr[ib->length_dw++] = 0x00000014;  \n\tib->ptr[ib->length_dw++] = 0x05000005;  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = addr;\n\tib->ptr[ib->length_dw++] = 0x00000001;\n\n\tfor (i = ib->length_dw; i < ib_size_dw; ++i)\n\t\tib->ptr[i] = 0x0;\n\n\tr = amdgpu_job_submit_direct(job, ring, &f);\n\tamdgpu_ib_free(ring->adev, &ib_msg, f);\n\tif (r)\n\t\tgoto err;\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\treturn 0;\n\nerr:\n\tamdgpu_job_free(job);\n\treturn r;\n}\n\n \nstatic int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring, uint32_t handle,\n\t\t\t\t      bool direct, struct dma_fence **fence)\n{\n\tconst unsigned int ib_size_dw = 1024;\n\tstruct amdgpu_job *job;\n\tstruct amdgpu_ib *ib;\n\tstruct dma_fence *f = NULL;\n\tint i, r;\n\n\tr = amdgpu_job_alloc_with_ib(ring->adev, &ring->adev->vce.entity,\n\t\t\t\t     AMDGPU_FENCE_OWNER_UNDEFINED,\n\t\t\t\t     ib_size_dw * 4,\n\t\t\t\t     direct ? AMDGPU_IB_POOL_DIRECT :\n\t\t\t\t     AMDGPU_IB_POOL_DELAYED, &job);\n\tif (r)\n\t\treturn r;\n\n\tib = &job->ibs[0];\n\n\t \n\tib->length_dw = 0;\n\tib->ptr[ib->length_dw++] = 0x0000000c;  \n\tib->ptr[ib->length_dw++] = 0x00000001;  \n\tib->ptr[ib->length_dw++] = handle;\n\n\tib->ptr[ib->length_dw++] = 0x00000020;  \n\tib->ptr[ib->length_dw++] = 0x00000002;  \n\tib->ptr[ib->length_dw++] = 0xffffffff;  \n\tib->ptr[ib->length_dw++] = 0x00000001;  \n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tib->ptr[ib->length_dw++] = 0x00000000;\n\tib->ptr[ib->length_dw++] = 0xffffffff;  \n\tib->ptr[ib->length_dw++] = 0x00000000;\n\n\tib->ptr[ib->length_dw++] = 0x00000008;  \n\tib->ptr[ib->length_dw++] = 0x02000001;  \n\n\tfor (i = ib->length_dw; i < ib_size_dw; ++i)\n\t\tib->ptr[i] = 0x0;\n\n\tif (direct)\n\t\tr = amdgpu_job_submit_direct(job, ring, &f);\n\telse\n\t\tf = amdgpu_job_submit(job);\n\tif (r)\n\t\tgoto err;\n\n\tif (fence)\n\t\t*fence = dma_fence_get(f);\n\tdma_fence_put(f);\n\treturn 0;\n\nerr:\n\tamdgpu_job_free(job);\n\treturn r;\n}\n\n \nstatic int amdgpu_vce_validate_bo(struct amdgpu_cs_parser *p,\n\t\t\t\t  struct amdgpu_ib *ib, int lo, int hi,\n\t\t\t\t  unsigned int size, int32_t index)\n{\n\tint64_t offset = ((uint64_t)size) * ((int64_t)index);\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tunsigned int i, fpfn, lpfn;\n\tstruct amdgpu_bo *bo;\n\tuint64_t addr;\n\tint r;\n\n\taddr = ((uint64_t)amdgpu_ib_get_value(ib, lo)) |\n\t       ((uint64_t)amdgpu_ib_get_value(ib, hi)) << 32;\n\tif (index >= 0) {\n\t\taddr += offset;\n\t\tfpfn = PAGE_ALIGN(offset) >> PAGE_SHIFT;\n\t\tlpfn = 0x100000000ULL >> PAGE_SHIFT;\n\t} else {\n\t\tfpfn = 0;\n\t\tlpfn = (0x100000000ULL - PAGE_ALIGN(offset)) >> PAGE_SHIFT;\n\t}\n\n\tr = amdgpu_cs_find_mapping(p, addr, &bo, &mapping);\n\tif (r) {\n\t\tDRM_ERROR(\"Can't find BO for addr 0x%010llx %d %d %d %d\\n\",\n\t\t\t  addr, lo, hi, size, index);\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < bo->placement.num_placement; ++i) {\n\t\tbo->placements[i].fpfn = max(bo->placements[i].fpfn, fpfn);\n\t\tbo->placements[i].lpfn = bo->placements[i].lpfn ?\n\t\t\tmin(bo->placements[i].lpfn, lpfn) : lpfn;\n\t}\n\treturn ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n}\n\n\n \nstatic int amdgpu_vce_cs_reloc(struct amdgpu_cs_parser *p, struct amdgpu_ib *ib,\n\t\t\t       int lo, int hi, unsigned int size, uint32_t index)\n{\n\tstruct amdgpu_bo_va_mapping *mapping;\n\tstruct amdgpu_bo *bo;\n\tuint64_t addr;\n\tint r;\n\n\tif (index == 0xffffffff)\n\t\tindex = 0;\n\n\taddr = ((uint64_t)amdgpu_ib_get_value(ib, lo)) |\n\t       ((uint64_t)amdgpu_ib_get_value(ib, hi)) << 32;\n\taddr += ((uint64_t)size) * ((uint64_t)index);\n\n\tr = amdgpu_cs_find_mapping(p, addr, &bo, &mapping);\n\tif (r) {\n\t\tDRM_ERROR(\"Can't find BO for addr 0x%010llx %d %d %d %d\\n\",\n\t\t\t  addr, lo, hi, size, index);\n\t\treturn r;\n\t}\n\n\tif ((addr + (uint64_t)size) >\n\t    (mapping->last + 1) * AMDGPU_GPU_PAGE_SIZE) {\n\t\tDRM_ERROR(\"BO too small for addr 0x%010llx %d %d\\n\",\n\t\t\t  addr, lo, hi);\n\t\treturn -EINVAL;\n\t}\n\n\taddr -= mapping->start * AMDGPU_GPU_PAGE_SIZE;\n\taddr += amdgpu_bo_gpu_offset(bo);\n\taddr -= ((uint64_t)size) * ((uint64_t)index);\n\n\tamdgpu_ib_set_value(ib, lo, lower_32_bits(addr));\n\tamdgpu_ib_set_value(ib, hi, upper_32_bits(addr));\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_vce_validate_handle(struct amdgpu_cs_parser *p,\n\t\t\t\t      uint32_t handle, uint32_t *allocated)\n{\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\n\t\tif (atomic_read(&p->adev->vce.handles[i]) == handle) {\n\t\t\tif (p->adev->vce.filp[i] != p->filp) {\n\t\t\t\tDRM_ERROR(\"VCE handle collision detected!\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\treturn i;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i) {\n\t\tif (!atomic_cmpxchg(&p->adev->vce.handles[i], 0, handle)) {\n\t\t\tp->adev->vce.filp[i] = p->filp;\n\t\t\tp->adev->vce.img_size[i] = 0;\n\t\t\t*allocated |= 1 << i;\n\t\t\treturn i;\n\t\t}\n\t}\n\n\tDRM_ERROR(\"No more free VCE handles!\\n\");\n\treturn -EINVAL;\n}\n\n \nint amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p,\n\t\t\t     struct amdgpu_job *job,\n\t\t\t     struct amdgpu_ib *ib)\n{\n\tunsigned int fb_idx = 0, bs_idx = 0;\n\tint session_idx = -1;\n\tuint32_t destroyed = 0;\n\tuint32_t created = 0;\n\tuint32_t allocated = 0;\n\tuint32_t tmp, handle = 0;\n\tuint32_t *size = &tmp;\n\tunsigned int idx;\n\tint i, r = 0;\n\n\tjob->vm = NULL;\n\tib->gpu_addr = amdgpu_sa_bo_gpu_addr(ib->sa_bo);\n\n\tfor (idx = 0; idx < ib->length_dw;) {\n\t\tuint32_t len = amdgpu_ib_get_value(ib, idx);\n\t\tuint32_t cmd = amdgpu_ib_get_value(ib, idx + 1);\n\n\t\tif ((len < 8) || (len & 3)) {\n\t\t\tDRM_ERROR(\"invalid VCE command length (%d)!\\n\", len);\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tswitch (cmd) {\n\t\tcase 0x00000002:  \n\t\t\tfb_idx = amdgpu_ib_get_value(ib, idx + 6);\n\t\t\tbs_idx = amdgpu_ib_get_value(ib, idx + 7);\n\t\t\tbreak;\n\n\t\tcase 0x03000001:  \n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 10, idx + 9,\n\t\t\t\t\t\t   0, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 12, idx + 11,\n\t\t\t\t\t\t   0, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x05000001:  \n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t   0, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x05000004:  \n\t\t\ttmp = amdgpu_ib_get_value(ib, idx + 4);\n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t   tmp, bs_idx);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x05000005:  \n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t   4096, fb_idx);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x0500000d:  \n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t   0, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tr = amdgpu_vce_validate_bo(p, ib, idx + 8, idx + 7,\n\t\t\t\t\t\t   0, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\n\t\tidx += len / 4;\n\t}\n\n\tfor (idx = 0; idx < ib->length_dw;) {\n\t\tuint32_t len = amdgpu_ib_get_value(ib, idx);\n\t\tuint32_t cmd = amdgpu_ib_get_value(ib, idx + 1);\n\n\t\tswitch (cmd) {\n\t\tcase 0x00000001:  \n\t\t\thandle = amdgpu_ib_get_value(ib, idx + 2);\n\t\t\tsession_idx = amdgpu_vce_validate_handle(p, handle,\n\t\t\t\t\t\t\t\t &allocated);\n\t\t\tif (session_idx < 0) {\n\t\t\t\tr = session_idx;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsize = &p->adev->vce.img_size[session_idx];\n\t\t\tbreak;\n\n\t\tcase 0x00000002:  \n\t\t\tfb_idx = amdgpu_ib_get_value(ib, idx + 6);\n\t\t\tbs_idx = amdgpu_ib_get_value(ib, idx + 7);\n\t\t\tbreak;\n\n\t\tcase 0x01000001:  \n\t\t\tcreated |= 1 << session_idx;\n\t\t\tif (destroyed & (1 << session_idx)) {\n\t\t\t\tdestroyed &= ~(1 << session_idx);\n\t\t\t\tallocated |= 1 << session_idx;\n\n\t\t\t} else if (!(allocated & (1 << session_idx))) {\n\t\t\t\tDRM_ERROR(\"Handle already in use!\\n\");\n\t\t\t\tr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t*size = amdgpu_ib_get_value(ib, idx + 8) *\n\t\t\t\tamdgpu_ib_get_value(ib, idx + 10) *\n\t\t\t\t8 * 3 / 2;\n\t\t\tbreak;\n\n\t\tcase 0x04000001:  \n\t\tcase 0x04000002:  \n\t\tcase 0x04000005:  \n\t\tcase 0x04000007:  \n\t\tcase 0x04000008:  \n\t\tcase 0x04000009:  \n\t\tcase 0x05000002:  \n\t\tcase 0x05000009:  \n\t\t\tbreak;\n\n\t\tcase 0x0500000c:  \n\t\t\tswitch (p->adev->asic_type) {\n#ifdef CONFIG_DRM_AMDGPU_CIK\n\t\t\tcase CHIP_KAVERI:\n\t\t\tcase CHIP_MULLINS:\n#endif\n\t\t\tcase CHIP_CARRIZO:\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 0x03000001:  \n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 10, idx + 9,\n\t\t\t\t\t\t*size, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 12, idx + 11,\n\t\t\t\t\t\t*size / 3, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x02000001:  \n\t\t\tdestroyed |= 1 << session_idx;\n\t\t\tbreak;\n\n\t\tcase 0x05000001:  \n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t*size * 2, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x05000004:  \n\t\t\ttmp = amdgpu_ib_get_value(ib, idx + 4);\n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\ttmp, bs_idx);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x05000005:  \n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 3, idx + 2,\n\t\t\t\t\t\t4096, fb_idx);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tcase 0x0500000d:  \n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 3,\n\t\t\t\t\t\tidx + 2, *size, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tr = amdgpu_vce_cs_reloc(p, ib, idx + 8,\n\t\t\t\t\t\tidx + 7, *size / 12, 0);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tDRM_ERROR(\"invalid VCE command (0x%x)!\\n\", cmd);\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (session_idx == -1) {\n\t\t\tDRM_ERROR(\"no session command at start of IB\\n\");\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tidx += len / 4;\n\t}\n\n\tif (allocated & ~created) {\n\t\tDRM_ERROR(\"New session without create command!\\n\");\n\t\tr = -ENOENT;\n\t}\n\nout:\n\tif (!r) {\n\t\t \n\t\ttmp = destroyed;\n\t} else {\n\t\t \n\t\ttmp = allocated;\n\t}\n\n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i)\n\t\tif (tmp & (1 << i))\n\t\t\tatomic_set(&p->adev->vce.handles[i], 0);\n\n\treturn r;\n}\n\n \nint amdgpu_vce_ring_parse_cs_vm(struct amdgpu_cs_parser *p,\n\t\t\t\tstruct amdgpu_job *job,\n\t\t\t\tstruct amdgpu_ib *ib)\n{\n\tint session_idx = -1;\n\tuint32_t destroyed = 0;\n\tuint32_t created = 0;\n\tuint32_t allocated = 0;\n\tuint32_t tmp, handle = 0;\n\tint i, r = 0, idx = 0;\n\n\twhile (idx < ib->length_dw) {\n\t\tuint32_t len = amdgpu_ib_get_value(ib, idx);\n\t\tuint32_t cmd = amdgpu_ib_get_value(ib, idx + 1);\n\n\t\tif ((len < 8) || (len & 3)) {\n\t\t\tDRM_ERROR(\"invalid VCE command length (%d)!\\n\", len);\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tswitch (cmd) {\n\t\tcase 0x00000001:  \n\t\t\thandle = amdgpu_ib_get_value(ib, idx + 2);\n\t\t\tsession_idx = amdgpu_vce_validate_handle(p, handle,\n\t\t\t\t\t\t\t\t &allocated);\n\t\t\tif (session_idx < 0) {\n\t\t\t\tr = session_idx;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase 0x01000001:  \n\t\t\tcreated |= 1 << session_idx;\n\t\t\tif (destroyed & (1 << session_idx)) {\n\t\t\t\tdestroyed &= ~(1 << session_idx);\n\t\t\t\tallocated |= 1 << session_idx;\n\n\t\t\t} else if (!(allocated & (1 << session_idx))) {\n\t\t\t\tDRM_ERROR(\"Handle already in use!\\n\");\n\t\t\t\tr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase 0x02000001:  \n\t\t\tdestroyed |= 1 << session_idx;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tif (session_idx == -1) {\n\t\t\tDRM_ERROR(\"no session command at start of IB\\n\");\n\t\t\tr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tidx += len / 4;\n\t}\n\n\tif (allocated & ~created) {\n\t\tDRM_ERROR(\"New session without create command!\\n\");\n\t\tr = -ENOENT;\n\t}\n\nout:\n\tif (!r) {\n\t\t \n\t\ttmp = destroyed;\n\t\tamdgpu_ib_free(p->adev, ib, NULL);\n\t} else {\n\t\t \n\t\ttmp = allocated;\n\t}\n\n\tfor (i = 0; i < AMDGPU_MAX_VCE_HANDLES; ++i)\n\t\tif (tmp & (1 << i))\n\t\t\tatomic_set(&p->adev->vce.handles[i], 0);\n\n\treturn r;\n}\n\n \nvoid amdgpu_vce_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\tstruct amdgpu_job *job,\n\t\t\t\tstruct amdgpu_ib *ib,\n\t\t\t\tuint32_t flags)\n{\n\tamdgpu_ring_write(ring, VCE_CMD_IB);\n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n}\n\n \nvoid amdgpu_vce_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\tunsigned int flags)\n{\n\tWARN_ON(flags & AMDGPU_FENCE_FLAG_64BIT);\n\n\tamdgpu_ring_write(ring, VCE_CMD_FENCE);\n\tamdgpu_ring_write(ring, addr);\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, seq);\n\tamdgpu_ring_write(ring, VCE_CMD_TRAP);\n\tamdgpu_ring_write(ring, VCE_CMD_END);\n}\n\n \nint amdgpu_vce_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t rptr;\n\tunsigned int i;\n\tint r, timeout = adev->usec_timeout;\n\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tr = amdgpu_ring_alloc(ring, 16);\n\tif (r)\n\t\treturn r;\n\n\trptr = amdgpu_ring_get_rptr(ring);\n\n\tamdgpu_ring_write(ring, VCE_CMD_END);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < timeout; i++) {\n\t\tif (amdgpu_ring_get_rptr(ring) != rptr)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= timeout)\n\t\tr = -ETIMEDOUT;\n\n\treturn r;\n}\n\n \nint amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct dma_fence *fence = NULL;\n\tlong r;\n\n\t \n\tif (ring != &ring->adev->vce.ring[0])\n\t\treturn 0;\n\n\tr = amdgpu_vce_get_create_msg(ring, 1, NULL);\n\tif (r)\n\t\tgoto error;\n\n\tr = amdgpu_vce_get_destroy_msg(ring, 1, true, &fence);\n\tif (r)\n\t\tgoto error;\n\n\tr = dma_fence_wait_timeout(fence, false, timeout);\n\tif (r == 0)\n\t\tr = -ETIMEDOUT;\n\telse if (r > 0)\n\t\tr = 0;\n\nerror:\n\tdma_fence_put(fence);\n\treturn r;\n}\n\nenum amdgpu_ring_priority_level amdgpu_vce_get_ring_prio(int ring)\n{\n\tswitch (ring) {\n\tcase 0:\n\t\treturn AMDGPU_RING_PRIO_0;\n\tcase 1:\n\t\treturn AMDGPU_RING_PRIO_1;\n\tcase 2:\n\t\treturn AMDGPU_RING_PRIO_2;\n\tdefault:\n\t\treturn AMDGPU_RING_PRIO_0;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}