{
  "module_name": "gmc_v6_0.c",
  "hash_id": "6f36a252d77a9ff578373bed9b9926995baaadc3ff4409da82a00a904e87b868",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/gmc_v6_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include <drm/drm_cache.h>\n#include \"amdgpu.h\"\n#include \"gmc_v6_0.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_gem.h\"\n\n#include \"bif/bif_3_0_d.h\"\n#include \"bif/bif_3_0_sh_mask.h\"\n#include \"oss/oss_1_0_d.h\"\n#include \"oss/oss_1_0_sh_mask.h\"\n#include \"gmc/gmc_6_0_d.h\"\n#include \"gmc/gmc_6_0_sh_mask.h\"\n#include \"dce/dce_6_0_d.h\"\n#include \"dce/dce_6_0_sh_mask.h\"\n#include \"si_enums.h\"\n\nstatic void gmc_v6_0_set_gmc_funcs(struct amdgpu_device *adev);\nstatic void gmc_v6_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int gmc_v6_0_wait_for_idle(void *handle);\n\nMODULE_FIRMWARE(\"amdgpu/tahiti_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/pitcairn_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/verde_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/oland_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/hainan_mc.bin\");\nMODULE_FIRMWARE(\"amdgpu/si58_mc.bin\");\n\n#define MC_SEQ_MISC0__MT__MASK   0xf0000000\n#define MC_SEQ_MISC0__MT__GDDR1  0x10000000\n#define MC_SEQ_MISC0__MT__DDR2   0x20000000\n#define MC_SEQ_MISC0__MT__GDDR3  0x30000000\n#define MC_SEQ_MISC0__MT__GDDR4  0x40000000\n#define MC_SEQ_MISC0__MT__GDDR5  0x50000000\n#define MC_SEQ_MISC0__MT__HBM    0x60000000\n#define MC_SEQ_MISC0__MT__DDR3   0xB0000000\n\nstatic void gmc_v6_0_mc_stop(struct amdgpu_device *adev)\n{\n\tu32 blackout;\n\n\tgmc_v6_0_wait_for_idle((void *)adev);\n\n\tblackout = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\n\tif (REG_GET_FIELD(blackout, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE) != 1) {\n\t\t \n\t\tWREG32(mmBIF_FB_EN, 0);\n\t\t \n\t\tblackout = REG_SET_FIELD(blackout,\n\t\t\t\t\t MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\n\t\tWREG32(mmMC_SHARED_BLACKOUT_CNTL, blackout | 1);\n\t}\n\t \n\tudelay(100);\n\n}\n\nstatic void gmc_v6_0_mc_resume(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\t \n\ttmp = RREG32(mmMC_SHARED_BLACKOUT_CNTL);\n\ttmp = REG_SET_FIELD(tmp, MC_SHARED_BLACKOUT_CNTL, BLACKOUT_MODE, 0);\n\tWREG32(mmMC_SHARED_BLACKOUT_CNTL, tmp);\n\t \n\ttmp = REG_SET_FIELD(0, BIF_FB_EN, FB_READ_EN, 1);\n\ttmp = REG_SET_FIELD(tmp, BIF_FB_EN, FB_WRITE_EN, 1);\n\tWREG32(mmBIF_FB_EN, tmp);\n}\n\nstatic int gmc_v6_0_init_microcode(struct amdgpu_device *adev)\n{\n\tconst char *chip_name;\n\tchar fw_name[30];\n\tint err;\n\tbool is_58_fw = false;\n\n\tDRM_DEBUG(\"\\n\");\n\n\tswitch (adev->asic_type) {\n\tcase CHIP_TAHITI:\n\t\tchip_name = \"tahiti\";\n\t\tbreak;\n\tcase CHIP_PITCAIRN:\n\t\tchip_name = \"pitcairn\";\n\t\tbreak;\n\tcase CHIP_VERDE:\n\t\tchip_name = \"verde\";\n\t\tbreak;\n\tcase CHIP_OLAND:\n\t\tchip_name = \"oland\";\n\t\tbreak;\n\tcase CHIP_HAINAN:\n\t\tchip_name = \"hainan\";\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\t \n\tif (((RREG32(mmMC_SEQ_MISC0) & 0xff000000) >> 24) == 0x58)\n\t\tis_58_fw = true;\n\n\tif (is_58_fw)\n\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/si58_mc.bin\");\n\telse\n\t\tsnprintf(fw_name, sizeof(fw_name), \"amdgpu/%s_mc.bin\", chip_name);\n\terr = amdgpu_ucode_request(adev, &adev->gmc.fw, fw_name);\n\tif (err) {\n\t\tdev_err(adev->dev,\n\t\t       \"si_mc: Failed to load firmware \\\"%s\\\"\\n\",\n\t\t       fw_name);\n\t\tamdgpu_ucode_release(&adev->gmc.fw);\n\t}\n\treturn err;\n}\n\nstatic int gmc_v6_0_mc_load_microcode(struct amdgpu_device *adev)\n{\n\tconst __le32 *new_fw_data = NULL;\n\tu32 running;\n\tconst __le32 *new_io_mc_regs = NULL;\n\tint i, regs_size, ucode_size;\n\tconst struct mc_firmware_header_v1_0 *hdr;\n\n\tif (!adev->gmc.fw)\n\t\treturn -EINVAL;\n\n\thdr = (const struct mc_firmware_header_v1_0 *)adev->gmc.fw->data;\n\n\tamdgpu_ucode_print_mc_hdr(&hdr->header);\n\n\tadev->gmc.fw_version = le32_to_cpu(hdr->header.ucode_version);\n\tregs_size = le32_to_cpu(hdr->io_debug_size_bytes) / (4 * 2);\n\tnew_io_mc_regs = (const __le32 *)\n\t\t(adev->gmc.fw->data + le32_to_cpu(hdr->io_debug_array_offset_bytes));\n\tucode_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\tnew_fw_data = (const __le32 *)\n\t\t(adev->gmc.fw->data + le32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\trunning = RREG32(mmMC_SEQ_SUP_CNTL) & MC_SEQ_SUP_CNTL__RUN_MASK;\n\n\tif (running == 0) {\n\n\t\t \n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000010);\n\n\t\t \n\t\tfor (i = 0; i < regs_size; i++) {\n\t\t\tWREG32(mmMC_SEQ_IO_DEBUG_INDEX, le32_to_cpup(new_io_mc_regs++));\n\t\t\tWREG32(mmMC_SEQ_IO_DEBUG_DATA, le32_to_cpup(new_io_mc_regs++));\n\t\t}\n\t\t \n\t\tfor (i = 0; i < ucode_size; i++)\n\t\t\tWREG32(mmMC_SEQ_SUP_PGM, le32_to_cpup(new_fw_data++));\n\n\t\t \n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000008);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000004);\n\t\tWREG32(mmMC_SEQ_SUP_CNTL, 0x00000001);\n\n\t\t \n\t\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t\tif (RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL) & MC_SEQ_TRAIN_WAKEUP_CNTL__TRAIN_DONE_D0_MASK)\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\t\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\t\tif (RREG32(mmMC_SEQ_TRAIN_WAKEUP_CNTL) & MC_SEQ_TRAIN_WAKEUP_CNTL__TRAIN_DONE_D1_MASK)\n\t\t\t\tbreak;\n\t\t\tudelay(1);\n\t\t}\n\n\t}\n\n\treturn 0;\n}\n\nstatic void gmc_v6_0_vram_gtt_location(struct amdgpu_device *adev,\n\t\t\t\t       struct amdgpu_gmc *mc)\n{\n\tu64 base = RREG32(mmMC_VM_FB_LOCATION) & 0xFFFF;\n\n\tbase <<= 24;\n\n\tamdgpu_gmc_vram_location(adev, mc, base);\n\tamdgpu_gmc_gart_location(adev, mc);\n}\n\nstatic void gmc_v6_0_mc_program(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0, j = 0; i < 32; i++, j += 0x6) {\n\t\tWREG32((0xb05 + j), 0x00000000);\n\t\tWREG32((0xb06 + j), 0x00000000);\n\t\tWREG32((0xb07 + j), 0x00000000);\n\t\tWREG32((0xb08 + j), 0x00000000);\n\t\tWREG32((0xb09 + j), 0x00000000);\n\t}\n\tWREG32(mmHDP_REG_COHERENCY_FLUSH_CNTL, 0);\n\n\tif (gmc_v6_0_wait_for_idle((void *)adev))\n\t\tdev_warn(adev->dev, \"Wait for MC idle timedout !\\n\");\n\n\tif (adev->mode_info.num_crtc) {\n\t\tu32 tmp;\n\n\t\t \n\t\ttmp = RREG32(mmVGA_HDP_CONTROL);\n\t\ttmp |= VGA_HDP_CONTROL__VGA_MEMORY_DISABLE_MASK;\n\t\tWREG32(mmVGA_HDP_CONTROL, tmp);\n\n\t\t \n\t\ttmp = RREG32(mmVGA_RENDER_CONTROL);\n\t\ttmp &= ~VGA_VSTATUS_CNTL;\n\t\tWREG32(mmVGA_RENDER_CONTROL, tmp);\n\t}\n\t \n\tWREG32(mmMC_VM_SYSTEM_APERTURE_LOW_ADDR,\n\t       adev->gmc.vram_start >> 12);\n\tWREG32(mmMC_VM_SYSTEM_APERTURE_HIGH_ADDR,\n\t       adev->gmc.vram_end >> 12);\n\tWREG32(mmMC_VM_SYSTEM_APERTURE_DEFAULT_ADDR,\n\t       adev->mem_scratch.gpu_addr >> 12);\n\tWREG32(mmMC_VM_AGP_BASE, 0);\n\tWREG32(mmMC_VM_AGP_TOP, 0x0FFFFFFF);\n\tWREG32(mmMC_VM_AGP_BOT, 0x0FFFFFFF);\n\n\tif (gmc_v6_0_wait_for_idle((void *)adev))\n\t\tdev_warn(adev->dev, \"Wait for MC idle timedout !\\n\");\n}\n\nstatic int gmc_v6_0_mc_init(struct amdgpu_device *adev)\n{\n\n\tu32 tmp;\n\tint chansize, numchan;\n\tint r;\n\n\ttmp = RREG32(mmMC_ARB_RAMCFG);\n\tif (tmp & (1 << 11))\n\t\tchansize = 16;\n\telse if (tmp & MC_ARB_RAMCFG__CHANSIZE_MASK)\n\t\tchansize = 64;\n\telse\n\t\tchansize = 32;\n\n\ttmp = RREG32(mmMC_SHARED_CHMAP);\n\tswitch ((tmp & MC_SHARED_CHMAP__NOOFCHAN_MASK) >> MC_SHARED_CHMAP__NOOFCHAN__SHIFT) {\n\tcase 0:\n\tdefault:\n\t\tnumchan = 1;\n\t\tbreak;\n\tcase 1:\n\t\tnumchan = 2;\n\t\tbreak;\n\tcase 2:\n\t\tnumchan = 4;\n\t\tbreak;\n\tcase 3:\n\t\tnumchan = 8;\n\t\tbreak;\n\tcase 4:\n\t\tnumchan = 3;\n\t\tbreak;\n\tcase 5:\n\t\tnumchan = 6;\n\t\tbreak;\n\tcase 6:\n\t\tnumchan = 10;\n\t\tbreak;\n\tcase 7:\n\t\tnumchan = 12;\n\t\tbreak;\n\tcase 8:\n\t\tnumchan = 16;\n\t\tbreak;\n\t}\n\tadev->gmc.vram_width = numchan * chansize;\n\t \n\tadev->gmc.mc_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\n\tadev->gmc.real_vram_size = RREG32(mmCONFIG_MEMSIZE) * 1024ULL * 1024ULL;\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tr = amdgpu_device_resize_fb_bar(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\tadev->gmc.aper_base = pci_resource_start(adev->pdev, 0);\n\tadev->gmc.aper_size = pci_resource_len(adev->pdev, 0);\n\tadev->gmc.visible_vram_size = adev->gmc.aper_size;\n\n\t \n\tif (amdgpu_gart_size == -1) {\n\t\tswitch (adev->asic_type) {\n\t\tcase CHIP_HAINAN:     \n\t\tdefault:\n\t\t\tadev->gmc.gart_size = 256ULL << 20;\n\t\t\tbreak;\n\t\tcase CHIP_VERDE:     \n\t\tcase CHIP_TAHITI:    \n\t\tcase CHIP_PITCAIRN:  \n\t\tcase CHIP_OLAND:     \n\t\t\tadev->gmc.gart_size = 1024ULL << 20;\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tadev->gmc.gart_size = (u64)amdgpu_gart_size << 20;\n\t}\n\n\tadev->gmc.gart_size += adev->pm.smu_prv_buffer_size;\n\tgmc_v6_0_vram_gtt_location(adev, &adev->gmc);\n\n\treturn 0;\n}\n\nstatic void gmc_v6_0_flush_gpu_tlb(struct amdgpu_device *adev, uint32_t vmid,\n\t\t\t\t\tuint32_t vmhub, uint32_t flush_type)\n{\n\tWREG32(mmVM_INVALIDATE_REQUEST, 1 << vmid);\n}\n\nstatic uint64_t gmc_v6_0_emit_flush_gpu_tlb(struct amdgpu_ring *ring,\n\t\t\t\t\t    unsigned int vmid, uint64_t pd_addr)\n{\n\tuint32_t reg;\n\n\t \n\tif (vmid < 8)\n\t\treg = mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + vmid;\n\telse\n\t\treg = mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + (vmid - 8);\n\tamdgpu_ring_emit_wreg(ring, reg, pd_addr >> 12);\n\n\t \n\tamdgpu_ring_emit_wreg(ring, mmVM_INVALIDATE_REQUEST, 1 << vmid);\n\n\treturn pd_addr;\n}\n\nstatic void gmc_v6_0_get_vm_pde(struct amdgpu_device *adev, int level,\n\t\t\t\tuint64_t *addr, uint64_t *flags)\n{\n\tBUG_ON(*addr & 0xFFFFFF0000000FFFULL);\n}\n\nstatic void gmc_v6_0_get_vm_pte(struct amdgpu_device *adev,\n\t\t\t\tstruct amdgpu_bo_va_mapping *mapping,\n\t\t\t\tuint64_t *flags)\n{\n\t*flags &= ~AMDGPU_PTE_EXECUTABLE;\n\t*flags &= ~AMDGPU_PTE_PRT;\n}\n\nstatic void gmc_v6_0_set_fault_enable_default(struct amdgpu_device *adev,\n\t\t\t\t\t      bool value)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    RANGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    DUMMY_PAGE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    PDE0_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    VALID_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    READ_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\ttmp = REG_SET_FIELD(tmp, VM_CONTEXT1_CNTL,\n\t\t\t    WRITE_PROTECTION_FAULT_ENABLE_DEFAULT, value);\n\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n}\n\n  \nstatic void gmc_v6_0_set_prt(struct amdgpu_device *adev, bool enable)\n{\n\tu32 tmp;\n\n\tif (enable && !adev->gmc.prt_warning) {\n\t\tdev_warn(adev->dev, \"Disabling VM faults because of PRT request!\\n\");\n\t\tadev->gmc.prt_warning = true;\n\t}\n\n\ttmp = RREG32(mmVM_PRT_CNTL);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    CB_DISABLE_FAULT_ON_UNMAPPED_ACCESS,\n\t\t\t    enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    TC_DISABLE_FAULT_ON_UNMAPPED_ACCESS,\n\t\t\t    enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    L2_CACHE_STORE_INVALID_ENTRIES,\n\t\t\t    enable);\n\ttmp = REG_SET_FIELD(tmp, VM_PRT_CNTL,\n\t\t\t    L1_TLB_STORE_INVALID_ENTRIES,\n\t\t\t    enable);\n\tWREG32(mmVM_PRT_CNTL, tmp);\n\n\tif (enable) {\n\t\tuint32_t low = AMDGPU_VA_RESERVED_SIZE >> AMDGPU_GPU_PAGE_SHIFT;\n\t\tuint32_t high = adev->vm_manager.max_pfn -\n\t\t\t(AMDGPU_VA_RESERVED_SIZE >> AMDGPU_GPU_PAGE_SHIFT);\n\n\t\tWREG32(mmVM_PRT_APERTURE0_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE1_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE2_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE3_LOW_ADDR, low);\n\t\tWREG32(mmVM_PRT_APERTURE0_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE1_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE2_HIGH_ADDR, high);\n\t\tWREG32(mmVM_PRT_APERTURE3_HIGH_ADDR, high);\n\t} else {\n\t\tWREG32(mmVM_PRT_APERTURE0_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE1_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE2_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE3_LOW_ADDR, 0xfffffff);\n\t\tWREG32(mmVM_PRT_APERTURE0_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE1_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE2_HIGH_ADDR, 0x0);\n\t\tWREG32(mmVM_PRT_APERTURE3_HIGH_ADDR, 0x0);\n\t}\n}\n\nstatic int gmc_v6_0_gart_enable(struct amdgpu_device *adev)\n{\n\tuint64_t table_addr;\n\tu32 field;\n\tint i;\n\n\tif (adev->gart.bo == NULL) {\n\t\tdev_err(adev->dev, \"No VRAM object for PCIE GART.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tamdgpu_gtt_mgr_recover(&adev->mman.gtt_mgr);\n\n\ttable_addr = amdgpu_bo_gpu_offset(adev->gart.bo);\n\n\t \n\tWREG32(mmMC_VM_MX_L1_TLB_CNTL,\n\t       (0xA << 7) |\n\t       MC_VM_MX_L1_TLB_CNTL__ENABLE_L1_TLB_MASK |\n\t       MC_VM_MX_L1_TLB_CNTL__ENABLE_L1_FRAGMENT_PROCESSING_MASK |\n\t       MC_VM_MX_L1_TLB_CNTL__SYSTEM_ACCESS_MODE_MASK |\n\t       MC_VM_MX_L1_TLB_CNTL__ENABLE_ADVANCED_DRIVER_MODEL_MASK |\n\t       (0UL << MC_VM_MX_L1_TLB_CNTL__SYSTEM_APERTURE_UNMAPPED_ACCESS__SHIFT));\n\t \n\tWREG32(mmVM_L2_CNTL,\n\t       VM_L2_CNTL__ENABLE_L2_CACHE_MASK |\n\t       VM_L2_CNTL__ENABLE_L2_FRAGMENT_PROCESSING_MASK |\n\t       VM_L2_CNTL__ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE_MASK |\n\t       VM_L2_CNTL__ENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE_MASK |\n\t       (7UL << VM_L2_CNTL__EFFECTIVE_L2_QUEUE_SIZE__SHIFT) |\n\t       (1UL << VM_L2_CNTL__CONTEXT1_IDENTITY_ACCESS_MODE__SHIFT));\n\tWREG32(mmVM_L2_CNTL2,\n\t       VM_L2_CNTL2__INVALIDATE_ALL_L1_TLBS_MASK |\n\t       VM_L2_CNTL2__INVALIDATE_L2_CACHE_MASK);\n\n\tfield = adev->vm_manager.fragment_size;\n\tWREG32(mmVM_L2_CNTL3,\n\t       VM_L2_CNTL3__L2_CACHE_BIGK_ASSOCIATIVITY_MASK |\n\t       (field << VM_L2_CNTL3__BANK_SELECT__SHIFT) |\n\t       (field << VM_L2_CNTL3__L2_CACHE_BIGK_FRAGMENT_SIZE__SHIFT));\n\t \n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_START_ADDR, adev->gmc.gart_start >> 12);\n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_END_ADDR, adev->gmc.gart_end >> 12);\n\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR, table_addr >> 12);\n\tWREG32(mmVM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR,\n\t\t\t(u32)(adev->dummy_page_addr >> 12));\n\tWREG32(mmVM_CONTEXT0_CNTL2, 0);\n\tWREG32(mmVM_CONTEXT0_CNTL,\n\t       VM_CONTEXT0_CNTL__ENABLE_CONTEXT_MASK |\n\t       (0UL << VM_CONTEXT0_CNTL__PAGE_TABLE_DEPTH__SHIFT) |\n\t       VM_CONTEXT0_CNTL__RANGE_PROTECTION_FAULT_ENABLE_DEFAULT_MASK);\n\n\tWREG32(0x575, 0);\n\tWREG32(0x576, 0);\n\tWREG32(0x577, 0);\n\n\t \n\t \n\tWREG32(mmVM_CONTEXT1_PAGE_TABLE_START_ADDR, 0);\n\tWREG32(mmVM_CONTEXT1_PAGE_TABLE_END_ADDR, adev->vm_manager.max_pfn - 1);\n\t \n\tfor (i = 1; i < AMDGPU_NUM_VMID; i++) {\n\t\tif (i < 8)\n\t\t\tWREG32(mmVM_CONTEXT0_PAGE_TABLE_BASE_ADDR + i,\n\t\t\t       table_addr >> 12);\n\t\telse\n\t\t\tWREG32(mmVM_CONTEXT8_PAGE_TABLE_BASE_ADDR + i - 8,\n\t\t\t       table_addr >> 12);\n\t}\n\n\t \n\tWREG32(mmVM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR,\n\t       (u32)(adev->dummy_page_addr >> 12));\n\tWREG32(mmVM_CONTEXT1_CNTL2, 4);\n\tWREG32(mmVM_CONTEXT1_CNTL,\n\t       VM_CONTEXT1_CNTL__ENABLE_CONTEXT_MASK |\n\t       (1UL << VM_CONTEXT1_CNTL__PAGE_TABLE_DEPTH__SHIFT) |\n\t       ((adev->vm_manager.block_size - 9)\n\t       << VM_CONTEXT1_CNTL__PAGE_TABLE_BLOCK_SIZE__SHIFT));\n\tif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_ALWAYS)\n\t\tgmc_v6_0_set_fault_enable_default(adev, false);\n\telse\n\t\tgmc_v6_0_set_fault_enable_default(adev, true);\n\n\tgmc_v6_0_flush_gpu_tlb(adev, 0, 0, 0);\n\tdev_info(adev->dev, \"PCIE GART of %uM enabled (table at 0x%016llX).\\n\",\n\t\t (unsigned int)(adev->gmc.gart_size >> 20),\n\t\t (unsigned long long)table_addr);\n\treturn 0;\n}\n\nstatic int gmc_v6_0_gart_init(struct amdgpu_device *adev)\n{\n\tint r;\n\n\tif (adev->gart.bo) {\n\t\tdev_warn(adev->dev, \"gmc_v6_0 PCIE GART already initialized\\n\");\n\t\treturn 0;\n\t}\n\tr = amdgpu_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\tadev->gart.table_size = adev->gart.num_gpu_pages * 8;\n\tadev->gart.gart_pte_flags = 0;\n\treturn amdgpu_gart_table_vram_alloc(adev);\n}\n\nstatic void gmc_v6_0_gart_disable(struct amdgpu_device *adev)\n{\n\t \n\n\t \n\tWREG32(mmVM_CONTEXT0_CNTL, 0);\n\tWREG32(mmVM_CONTEXT1_CNTL, 0);\n\t \n\tWREG32(mmMC_VM_MX_L1_TLB_CNTL,\n\t       MC_VM_MX_L1_TLB_CNTL__SYSTEM_ACCESS_MODE_MASK |\n\t       (0UL << MC_VM_MX_L1_TLB_CNTL__SYSTEM_APERTURE_UNMAPPED_ACCESS__SHIFT));\n\t \n\tWREG32(mmVM_L2_CNTL,\n\t       VM_L2_CNTL__ENABLE_L2_PTE_CACHE_LRU_UPDATE_BY_WRITE_MASK |\n\t       VM_L2_CNTL__ENABLE_L2_PDE0_CACHE_LRU_UPDATE_BY_WRITE_MASK |\n\t       (7UL << VM_L2_CNTL__EFFECTIVE_L2_QUEUE_SIZE__SHIFT) |\n\t       (1UL << VM_L2_CNTL__CONTEXT1_IDENTITY_ACCESS_MODE__SHIFT));\n\tWREG32(mmVM_L2_CNTL2, 0);\n\tWREG32(mmVM_L2_CNTL3,\n\t       VM_L2_CNTL3__L2_CACHE_BIGK_ASSOCIATIVITY_MASK |\n\t       (0UL << VM_L2_CNTL3__L2_CACHE_BIGK_FRAGMENT_SIZE__SHIFT));\n}\n\nstatic void gmc_v6_0_vm_decode_fault(struct amdgpu_device *adev,\n\t\t\t\t     u32 status, u32 addr, u32 mc_client)\n{\n\tu32 mc_id;\n\tu32 vmid = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS, VMID);\n\tu32 protections = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t\t\tPROTECTIONS);\n\tchar block[5] = { mc_client >> 24, (mc_client >> 16) & 0xff,\n\t\t(mc_client >> 8) & 0xff, mc_client & 0xff, 0 };\n\n\tmc_id = REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t      MEMORY_CLIENT_ID);\n\n\tdev_err(adev->dev, \"VM fault (0x%02x, vmid %d) at page %u, %s from '%s' (0x%08x) (%d)\\n\",\n\t       protections, vmid, addr,\n\t       REG_GET_FIELD(status, VM_CONTEXT1_PROTECTION_FAULT_STATUS,\n\t\t\t     MEMORY_CLIENT_RW) ?\n\t       \"write\" : \"read\", block, mc_client, mc_id);\n}\n\n \n\nstatic int gmc_v6_0_convert_vram_type(int mc_seq_vram_type)\n{\n\tswitch (mc_seq_vram_type) {\n\tcase MC_SEQ_MISC0__MT__GDDR1:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR1;\n\tcase MC_SEQ_MISC0__MT__DDR2:\n\t\treturn AMDGPU_VRAM_TYPE_DDR2;\n\tcase MC_SEQ_MISC0__MT__GDDR3:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR3;\n\tcase MC_SEQ_MISC0__MT__GDDR4:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR4;\n\tcase MC_SEQ_MISC0__MT__GDDR5:\n\t\treturn AMDGPU_VRAM_TYPE_GDDR5;\n\tcase MC_SEQ_MISC0__MT__DDR3:\n\t\treturn AMDGPU_VRAM_TYPE_DDR3;\n\tdefault:\n\t\treturn AMDGPU_VRAM_TYPE_UNKNOWN;\n\t}\n}\n\nstatic int gmc_v6_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v6_0_set_gmc_funcs(adev);\n\tgmc_v6_0_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_vm_fault_stop != AMDGPU_VM_FAULT_STOP_ALWAYS)\n\t\treturn amdgpu_irq_get(adev, &adev->gmc.vm_fault, 0);\n\telse\n\t\treturn 0;\n}\n\nstatic unsigned int gmc_v6_0_get_vbios_fb_size(struct amdgpu_device *adev)\n{\n\tu32 d1vga_control = RREG32(mmD1VGA_CONTROL);\n\tunsigned int size;\n\n\tif (REG_GET_FIELD(d1vga_control, D1VGA_CONTROL, D1VGA_MODE_ENABLE)) {\n\t\tsize = AMDGPU_VBIOS_VGA_ALLOCATION;\n\t} else {\n\t\tu32 viewport = RREG32(mmVIEWPORT_SIZE);\n\n\t\tsize = (REG_GET_FIELD(viewport, VIEWPORT_SIZE, VIEWPORT_HEIGHT) *\n\t\t\tREG_GET_FIELD(viewport, VIEWPORT_SIZE, VIEWPORT_WIDTH) *\n\t\t\t4);\n\t}\n\treturn size;\n}\n\nstatic int gmc_v6_0_sw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tset_bit(AMDGPU_GFXHUB(0), adev->vmhubs_mask);\n\n\tif (adev->flags & AMD_IS_APU) {\n\t\tadev->gmc.vram_type = AMDGPU_VRAM_TYPE_UNKNOWN;\n\t} else {\n\t\tu32 tmp = RREG32(mmMC_SEQ_MISC0);\n\n\t\ttmp &= MC_SEQ_MISC0__MT__MASK;\n\t\tadev->gmc.vram_type = gmc_v6_0_convert_vram_type(tmp);\n\t}\n\n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 146, &adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 147, &adev->gmc.vm_fault);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vm_adjust_size(adev, 64, 9, 1, 40);\n\n\tadev->gmc.mc_mask = 0xffffffffffULL;\n\n\tr = dma_set_mask_and_coherent(adev->dev, DMA_BIT_MASK(40));\n\tif (r) {\n\t\tdev_warn(adev->dev, \"No suitable DMA available.\\n\");\n\t\treturn r;\n\t}\n\tadev->need_swiotlb = drm_need_swiotlb(40);\n\n\tr = gmc_v6_0_init_microcode(adev);\n\tif (r) {\n\t\tdev_err(adev->dev, \"Failed to load mc firmware!\\n\");\n\t\treturn r;\n\t}\n\n\tr = gmc_v6_0_mc_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_gmc_get_vbios_allocations(adev);\n\n\tr = amdgpu_bo_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tr = gmc_v6_0_gart_init(adev);\n\tif (r)\n\t\treturn r;\n\n\t \n\tadev->vm_manager.first_kfd_vmid = 8;\n\tamdgpu_vm_manager_init(adev);\n\n\t \n\tif (adev->flags & AMD_IS_APU) {\n\t\tu64 tmp = RREG32(mmMC_VM_FB_OFFSET);\n\n\t\ttmp <<= 22;\n\t\tadev->vm_manager.vram_base_offset = tmp;\n\t} else {\n\t\tadev->vm_manager.vram_base_offset = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_gem_force_release(adev);\n\tamdgpu_vm_manager_fini(adev);\n\tamdgpu_gart_table_vram_free(adev);\n\tamdgpu_bo_fini(adev);\n\tamdgpu_ucode_release(&adev->gmc.fw);\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_hw_init(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v6_0_mc_program(adev);\n\n\tif (!(adev->flags & AMD_IS_APU)) {\n\t\tr = gmc_v6_0_mc_load_microcode(adev);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"Failed to load MC firmware!\\n\");\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tr = gmc_v6_0_gart_enable(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (amdgpu_emu_mode == 1)\n\t\treturn amdgpu_gmc_vram_checking(adev);\n\telse\n\t\treturn r;\n}\n\nstatic int gmc_v6_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_irq_put(adev, &adev->gmc.vm_fault, 0);\n\tgmc_v6_0_gart_disable(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tgmc_v6_0_hw_fini(adev);\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = gmc_v6_0_hw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tamdgpu_vmid_reset_all(adev);\n\n\treturn 0;\n}\n\nstatic bool gmc_v6_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp = RREG32(mmSRBM_STATUS);\n\n\tif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\n\t\t   SRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK | SRBM_STATUS__VMC_BUSY_MASK))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int gmc_v6_0_wait_for_idle(void *handle)\n{\n\tunsigned int i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (gmc_v6_0_is_idle(handle))\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n\n}\n\nstatic int gmc_v6_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 srbm_soft_reset = 0;\n\tu32 tmp = RREG32(mmSRBM_STATUS);\n\n\tif (tmp & SRBM_STATUS__VMC_BUSY_MASK)\n\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\n\t\t\t\t\t\tSRBM_SOFT_RESET, SOFT_RESET_VMC, 1);\n\n\tif (tmp & (SRBM_STATUS__MCB_BUSY_MASK | SRBM_STATUS__MCB_NON_DISPLAY_BUSY_MASK |\n\t\t   SRBM_STATUS__MCC_BUSY_MASK | SRBM_STATUS__MCD_BUSY_MASK)) {\n\t\tif (!(adev->flags & AMD_IS_APU))\n\t\t\tsrbm_soft_reset = REG_SET_FIELD(srbm_soft_reset,\n\t\t\t\t\t\t\tSRBM_SOFT_RESET, SOFT_RESET_MC, 1);\n\t}\n\n\tif (srbm_soft_reset) {\n\t\tgmc_v6_0_mc_stop(adev);\n\t\tif (gmc_v6_0_wait_for_idle(adev))\n\t\t\tdev_warn(adev->dev, \"Wait for GMC idle timed out !\\n\");\n\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\t\ttmp |= srbm_soft_reset;\n\t\tdev_info(adev->dev, \"SRBM_SOFT_RESET=0x%08X\\n\", tmp);\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\ttmp &= ~srbm_soft_reset;\n\t\tWREG32(mmSRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32(mmSRBM_SOFT_RESET);\n\n\t\tudelay(50);\n\n\t\tgmc_v6_0_mc_resume(adev);\n\t\tudelay(50);\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_vm_fault_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\t     struct amdgpu_irq_src *src,\n\t\t\t\t\t     unsigned int type,\n\t\t\t\t\t     enum amdgpu_interrupt_state state)\n{\n\tu32 tmp;\n\tu32 bits = (VM_CONTEXT1_CNTL__RANGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__DUMMY_PAGE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__PDE0_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__VALID_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__READ_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK |\n\t\t    VM_CONTEXT1_CNTL__WRITE_PROTECTION_FAULT_ENABLE_INTERRUPT_MASK);\n\n\tswitch (state) {\n\tcase AMDGPU_IRQ_STATE_DISABLE:\n\t\ttmp = RREG32(mmVM_CONTEXT0_CNTL);\n\t\ttmp &= ~bits;\n\t\tWREG32(mmVM_CONTEXT0_CNTL, tmp);\n\t\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\t\ttmp &= ~bits;\n\t\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n\t\tbreak;\n\tcase AMDGPU_IRQ_STATE_ENABLE:\n\t\ttmp = RREG32(mmVM_CONTEXT0_CNTL);\n\t\ttmp |= bits;\n\t\tWREG32(mmVM_CONTEXT0_CNTL, tmp);\n\t\ttmp = RREG32(mmVM_CONTEXT1_CNTL);\n\t\ttmp |= bits;\n\t\tWREG32(mmVM_CONTEXT1_CNTL, tmp);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tu32 addr, status;\n\n\taddr = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_ADDR);\n\tstatus = RREG32(mmVM_CONTEXT1_PROTECTION_FAULT_STATUS);\n\tWREG32_P(mmVM_CONTEXT1_CNTL2, 1, ~1);\n\n\tif (!addr && !status)\n\t\treturn 0;\n\n\tif (amdgpu_vm_fault_stop == AMDGPU_VM_FAULT_STOP_FIRST)\n\t\tgmc_v6_0_set_fault_enable_default(adev, false);\n\n\tif (printk_ratelimit()) {\n\t\tdev_err(adev->dev, \"GPU fault detected: %d 0x%08x\\n\",\n\t\t\tentry->src_id, entry->src_data[0]);\n\t\tdev_err(adev->dev, \"  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\\n\",\n\t\t\taddr);\n\t\tdev_err(adev->dev, \"  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\\n\",\n\t\t\tstatus);\n\t\tgmc_v6_0_vm_decode_fault(adev, status, addr, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int gmc_v6_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int gmc_v6_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic const struct amd_ip_funcs gmc_v6_0_ip_funcs = {\n\t.name = \"gmc_v6_0\",\n\t.early_init = gmc_v6_0_early_init,\n\t.late_init = gmc_v6_0_late_init,\n\t.sw_init = gmc_v6_0_sw_init,\n\t.sw_fini = gmc_v6_0_sw_fini,\n\t.hw_init = gmc_v6_0_hw_init,\n\t.hw_fini = gmc_v6_0_hw_fini,\n\t.suspend = gmc_v6_0_suspend,\n\t.resume = gmc_v6_0_resume,\n\t.is_idle = gmc_v6_0_is_idle,\n\t.wait_for_idle = gmc_v6_0_wait_for_idle,\n\t.soft_reset = gmc_v6_0_soft_reset,\n\t.set_clockgating_state = gmc_v6_0_set_clockgating_state,\n\t.set_powergating_state = gmc_v6_0_set_powergating_state,\n};\n\nstatic const struct amdgpu_gmc_funcs gmc_v6_0_gmc_funcs = {\n\t.flush_gpu_tlb = gmc_v6_0_flush_gpu_tlb,\n\t.emit_flush_gpu_tlb = gmc_v6_0_emit_flush_gpu_tlb,\n\t.set_prt = gmc_v6_0_set_prt,\n\t.get_vm_pde = gmc_v6_0_get_vm_pde,\n\t.get_vm_pte = gmc_v6_0_get_vm_pte,\n\t.get_vbios_fb_size = gmc_v6_0_get_vbios_fb_size,\n};\n\nstatic const struct amdgpu_irq_src_funcs gmc_v6_0_irq_funcs = {\n\t.set = gmc_v6_0_vm_fault_interrupt_state,\n\t.process = gmc_v6_0_process_interrupt,\n};\n\nstatic void gmc_v6_0_set_gmc_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.gmc_funcs = &gmc_v6_0_gmc_funcs;\n}\n\nstatic void gmc_v6_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->gmc.vm_fault.num_types = 1;\n\tadev->gmc.vm_fault.funcs = &gmc_v6_0_irq_funcs;\n}\n\nconst struct amdgpu_ip_block_version gmc_v6_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_GMC,\n\t.major = 6,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &gmc_v6_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}