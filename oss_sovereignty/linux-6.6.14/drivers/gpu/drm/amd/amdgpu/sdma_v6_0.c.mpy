{
  "module_name": "sdma_v6_0.c",
  "hash_id": "e9f67eccad172063d309d5dc8817118e2faf554a40aff1f2adf8b00e25ed5f9f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/sdma_v6_0.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_trace.h\"\n\n#include \"gc/gc_11_0_0_offset.h\"\n#include \"gc/gc_11_0_0_sh_mask.h\"\n#include \"gc/gc_11_0_0_default.h\"\n#include \"hdp/hdp_6_0_0_offset.h\"\n#include \"ivsrcid/gfx/irqsrcs_gfx_11_0_0.h\"\n\n#include \"soc15_common.h\"\n#include \"soc15.h\"\n#include \"sdma_v6_0_0_pkt_open.h\"\n#include \"nbio_v4_3.h\"\n#include \"sdma_common.h\"\n#include \"sdma_v6_0.h\"\n#include \"v11_structs.h\"\n\nMODULE_FIRMWARE(\"amdgpu/sdma_6_0_0.bin\");\nMODULE_FIRMWARE(\"amdgpu/sdma_6_0_1.bin\");\nMODULE_FIRMWARE(\"amdgpu/sdma_6_0_2.bin\");\nMODULE_FIRMWARE(\"amdgpu/sdma_6_0_3.bin\");\nMODULE_FIRMWARE(\"amdgpu/sdma_6_1_0.bin\");\n\n#define SDMA1_REG_OFFSET 0x600\n#define SDMA0_HYP_DEC_REG_START 0x5880\n#define SDMA0_HYP_DEC_REG_END 0x589a\n#define SDMA1_HYP_DEC_REG_OFFSET 0x20\n\nstatic void sdma_v6_0_set_ring_funcs(struct amdgpu_device *adev);\nstatic void sdma_v6_0_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void sdma_v6_0_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic void sdma_v6_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic int sdma_v6_0_start(struct amdgpu_device *adev);\n\nstatic u32 sdma_v6_0_get_reg_offset(struct amdgpu_device *adev, u32 instance, u32 internal_offset)\n{\n\tu32 base;\n\n\tif (internal_offset >= SDMA0_HYP_DEC_REG_START &&\n\t    internal_offset <= SDMA0_HYP_DEC_REG_END) {\n\t\tbase = adev->reg_offset[GC_HWIP][0][1];\n\t\tif (instance != 0)\n\t\t\tinternal_offset += SDMA1_HYP_DEC_REG_OFFSET * instance;\n\t} else {\n\t\tbase = adev->reg_offset[GC_HWIP][0][0];\n\t\tif (instance == 1)\n\t\t\tinternal_offset += SDMA1_REG_OFFSET;\n\t}\n\n\treturn base + internal_offset;\n}\n\nstatic unsigned sdma_v6_0_ring_init_cond_exec(struct amdgpu_ring *ring)\n{\n\tunsigned ret;\n\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_COND_EXE));\n\tamdgpu_ring_write(ring, lower_32_bits(ring->cond_exe_gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(ring->cond_exe_gpu_addr));\n\tamdgpu_ring_write(ring, 1);\n\tret = ring->wptr & ring->buf_mask; \n\tamdgpu_ring_write(ring, 0x55aa55aa); \n\n\treturn ret;\n}\n\nstatic void sdma_v6_0_ring_patch_cond_exec(struct amdgpu_ring *ring,\n\t\t\t\t\t   unsigned offset)\n{\n\tunsigned cur;\n\n\tBUG_ON(offset > ring->buf_mask);\n\tBUG_ON(ring->ring[offset] != 0x55aa55aa);\n\n\tcur = (ring->wptr - 1) & ring->buf_mask;\n\tif (cur > offset)\n\t\tring->ring[offset] = cur - offset;\n\telse\n\t\tring->ring[offset] = (ring->buf_mask + 1) - offset + cur;\n}\n\n \nstatic uint64_t sdma_v6_0_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tu64 *rptr;\n\n\t \n\trptr = (u64 *)ring->rptr_cpu_addr;\n\n\tDRM_DEBUG(\"rptr before shift == 0x%016llx\\n\", *rptr);\n\treturn ((*rptr) >> 2);\n}\n\n \nstatic uint64_t sdma_v6_0_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tu64 wptr = 0;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\twptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));\n\t\tDRM_DEBUG(\"wptr/doorbell before shift == 0x%016llx\\n\", wptr);\n\t}\n\n\treturn wptr >> 2;\n}\n\n \nstatic void sdma_v6_0_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tuint32_t *wptr_saved;\n\tuint32_t *is_queue_unmap;\n\tuint64_t aggregated_db_index;\n\tuint32_t mqd_size = adev->mqds[AMDGPU_HW_IP_DMA].mqd_size;\n\n\tDRM_DEBUG(\"Setting write pointer\\n\");\n\n\tif (ring->is_mes_queue) {\n\t\twptr_saved = (uint32_t *)(ring->mqd_ptr + mqd_size);\n\t\tis_queue_unmap = (uint32_t *)(ring->mqd_ptr + mqd_size +\n\t\t\t\t\t      sizeof(uint32_t));\n\t\taggregated_db_index =\n\t\t\tamdgpu_mes_get_aggregated_doorbell_index(adev,\n\t\t\t\t\t\t\t ring->hw_prio);\n\n\t\tatomic64_set((atomic64_t *)ring->wptr_cpu_addr,\n\t\t\t     ring->wptr << 2);\n\t\t*wptr_saved = ring->wptr << 2;\n\t\tif (*is_queue_unmap) {\n\t\t\tWDOORBELL64(aggregated_db_index, ring->wptr << 2);\n\t\t\tDRM_DEBUG(\"calling WDOORBELL64(0x%08x, 0x%016llx)\\n\",\n\t\t\t\t\tring->doorbell_index, ring->wptr << 2);\n\t\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t\t} else {\n\t\t\tDRM_DEBUG(\"calling WDOORBELL64(0x%08x, 0x%016llx)\\n\",\n\t\t\t\t\tring->doorbell_index, ring->wptr << 2);\n\t\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\n\t\t\tif (*is_queue_unmap)\n\t\t\t\tWDOORBELL64(aggregated_db_index,\n\t\t\t\t\t    ring->wptr << 2);\n\t\t}\n\t} else {\n\t\tif (ring->use_doorbell) {\n\t\t\tDRM_DEBUG(\"Using doorbell -- \"\n\t\t\t\t  \"wptr_offs == 0x%08x \"\n\t\t\t\t  \"lower_32_bits(ring->wptr) << 2 == 0x%08x \"\n\t\t\t\t  \"upper_32_bits(ring->wptr) << 2 == 0x%08x\\n\",\n\t\t\t\t  ring->wptr_offs,\n\t\t\t\t  lower_32_bits(ring->wptr << 2),\n\t\t\t\t  upper_32_bits(ring->wptr << 2));\n\t\t\t \n\t\t\tatomic64_set((atomic64_t *)ring->wptr_cpu_addr,\n\t\t\t\t     ring->wptr << 2);\n\t\t\tDRM_DEBUG(\"calling WDOORBELL64(0x%08x, 0x%016llx)\\n\",\n\t\t\t\t  ring->doorbell_index, ring->wptr << 2);\n\t\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t\t} else {\n\t\t\tDRM_DEBUG(\"Not using doorbell -- \"\n\t\t\t\t  \"regSDMA%i_GFX_RB_WPTR == 0x%08x \"\n\t\t\t\t  \"regSDMA%i_GFX_RB_WPTR_HI == 0x%08x\\n\",\n\t\t\t\t  ring->me,\n\t\t\t\t  lower_32_bits(ring->wptr << 2),\n\t\t\t\t  ring->me,\n\t\t\t\t  upper_32_bits(ring->wptr << 2));\n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev,\n\t\t\t\t        ring->me, regSDMA0_QUEUE0_RB_WPTR),\n\t\t\t\t\tlower_32_bits(ring->wptr << 2));\n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev,\n\t\t\t\t        ring->me, regSDMA0_QUEUE0_RB_WPTR_HI),\n\t\t\t\t\tupper_32_bits(ring->wptr << 2));\n\t\t}\n\t}\n}\n\nstatic void sdma_v6_0_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tint i;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(count - 1));\n\t\telse\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop);\n}\n\n \nstatic void sdma_v6_0_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_job *job,\n\t\t\t\t   struct amdgpu_ib *ib,\n\t\t\t\t   uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\tuint64_t csa_mc_addr = amdgpu_sdma_get_csa_mc_addr(ring, vmid);\n\n\t \n\tsdma_v6_0_ring_insert_nop(ring, (2 - lower_32_bits(ring->wptr)) & 7);\n\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_INDIRECT) |\n\t\t\t  SDMA_PKT_INDIRECT_HEADER_VMID(vmid & 0xf));\n\t \n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr) & 0xffffffe0);\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n\tamdgpu_ring_write(ring, lower_32_bits(csa_mc_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(csa_mc_addr));\n}\n\n \nstatic void sdma_v6_0_ring_emit_mem_sync(struct amdgpu_ring *ring)\n{\n        uint32_t gcr_cntl = SDMA_GCR_GL2_INV | SDMA_GCR_GL2_WB | SDMA_GCR_GLM_INV |\n                            SDMA_GCR_GL1_INV | SDMA_GCR_GLV_INV | SDMA_GCR_GLK_INV |\n                            SDMA_GCR_GLI_INV(1);\n\n         \n        amdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_GCR_REQ));\n        amdgpu_ring_write(ring, SDMA_PKT_GCR_REQ_PAYLOAD1_BASE_VA_31_7(0));\n        amdgpu_ring_write(ring, SDMA_PKT_GCR_REQ_PAYLOAD2_GCR_CONTROL_15_0(gcr_cntl) |\n                          SDMA_PKT_GCR_REQ_PAYLOAD2_BASE_VA_47_32(0));\n        amdgpu_ring_write(ring, SDMA_PKT_GCR_REQ_PAYLOAD3_LIMIT_VA_31_7(0) |\n                          SDMA_PKT_GCR_REQ_PAYLOAD3_GCR_CONTROL_18_16(gcr_cntl >> 16));\n        amdgpu_ring_write(ring, SDMA_PKT_GCR_REQ_PAYLOAD4_LIMIT_VA_47_32(0) |\n                          SDMA_PKT_GCR_REQ_PAYLOAD4_VMID(0));\n}\n\n\n \nstatic void sdma_v6_0_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 ref_and_mask = 0;\n\tconst struct nbio_hdp_flush_reg *nbio_hf_reg = adev->nbio.hdp_flush_reg;\n\n\tref_and_mask = nbio_hf_reg->ref_and_mask_sdma0 << ring->me;\n\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(1) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));  \n\tamdgpu_ring_write(ring, (adev->nbio.funcs->get_hdp_flush_done_offset(adev)) << 2);\n\tamdgpu_ring_write(ring, (adev->nbio.funcs->get_hdp_flush_req_offset(adev)) << 2);\n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, ref_and_mask);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));  \n}\n\n \nstatic void sdma_v6_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t      unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_FENCE) |\n\t\t\t  SDMA_PKT_FENCE_HEADER_MTYPE(0x3));  \n\t \n\tBUG_ON(addr & 0x3);\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_FENCE) |\n\t\t\t\t  SDMA_PKT_FENCE_HEADER_MTYPE(0x3));\n\t\t \n\t\tBUG_ON(addr & 0x3);\n\t\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\n\tif (flags & AMDGPU_FENCE_FLAG_INT) {\n\t\tuint32_t ctx = ring->is_mes_queue ?\n\t\t\t(ring->hw_queue_id | AMDGPU_FENCE_MES_QUEUE_FLAG) : 0;\n\t\t \n\t\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_TRAP));\n\t\tamdgpu_ring_write(ring, SDMA_PKT_TRAP_INT_CONTEXT_INT_CONTEXT(ctx));\n\t}\n}\n\n \nstatic void sdma_v6_0_gfx_stop(struct amdgpu_device *adev)\n{\n\tu32 rb_cntl, ib_cntl;\n\tint i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\trb_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_CNTL));\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RB_ENABLE, 0);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_CNTL), rb_cntl);\n\t\tib_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_IB_CNTL));\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_QUEUE0_IB_CNTL, IB_ENABLE, 0);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_IB_CNTL), ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v6_0_rlc_stop(struct amdgpu_device *adev)\n{\n\t \n}\n\n \nstatic void sdma_v6_0_ctxempty_int_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl;\n\tint i;\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tf32_cntl = RREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_CNTL));\n\t\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,\n\t\t\t\t\tCTXEMPTY_INT_ENABLE, enable ? 1 : 0);\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_CNTL), f32_cntl);\n\t\t}\n\t}\n}\n\n \nstatic void sdma_v6_0_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl;\n\tint i;\n\n\tif (!enable) {\n\t\tsdma_v6_0_gfx_stop(adev);\n\t\tsdma_v6_0_rlc_stop(adev);\n\t}\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tf32_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL));\n\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, enable ? 0 : 1);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL), f32_cntl);\n\t}\n}\n\n \nstatic int sdma_v6_0_gfx_resume(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tu32 rb_cntl, ib_cntl;\n\tu32 rb_bufsz;\n\tu32 doorbell;\n\tu32 doorbell_offset;\n\tu32 temp;\n\tu64 wptr_gpu_addr;\n\tint i, r;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tif (!amdgpu_sriov_vf(adev))\n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_SEM_WAIT_FAIL_TIMER_CNTL), 0);\n\n\t\t \n\t\trb_bufsz = order_base_2(ring->ring_size / 4);\n\t\trb_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_CNTL));\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RB_SIZE, rb_bufsz);\n#ifdef __BIG_ENDIAN\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RB_SWAP_ENABLE, 1);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL,\n\t\t\t\t\tRPTR_WRITEBACK_SWAP_ENABLE, 1);\n#endif\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RB_PRIV, 1);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_CNTL), rb_cntl);\n\n\t\t \n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_RPTR), 0);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_RPTR_HI), 0);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR), 0);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR_HI), 0);\n\n\t\t \n\t\twptr_gpu_addr = ring->wptr_gpu_addr;\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR_POLL_ADDR_LO),\n\t\t       lower_32_bits(wptr_gpu_addr));\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR_POLL_ADDR_HI),\n\t\t       upper_32_bits(wptr_gpu_addr));\n\n\t\t \n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_RPTR_ADDR_HI),\n\t\t       upper_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFF);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_RPTR_ADDR_LO),\n\t\t       lower_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFC);\n\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RPTR_WRITEBACK_ENABLE, 1);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, WPTR_POLL_ENABLE, 0);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, F32_WPTR_POLL_ENABLE, 1);\n\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_BASE), ring->gpu_addr >> 8);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_BASE_HI), ring->gpu_addr >> 40);\n\n\t\tring->wptr = 0;\n\n\t\t \n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_MINOR_PTR_UPDATE), 1);\n\n\t\tif (!amdgpu_sriov_vf(adev)) {  \n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR), lower_32_bits(ring->wptr) << 2);\n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_WPTR_HI), upper_32_bits(ring->wptr) << 2);\n\t\t}\n\n\t\tdoorbell = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_DOORBELL));\n\t\tdoorbell_offset = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_DOORBELL_OFFSET));\n\n\t\tif (ring->use_doorbell) {\n\t\t\tdoorbell = REG_SET_FIELD(doorbell, SDMA0_QUEUE0_DOORBELL, ENABLE, 1);\n\t\t\tdoorbell_offset = REG_SET_FIELD(doorbell_offset, SDMA0_QUEUE0_DOORBELL_OFFSET,\n\t\t\t\t\tOFFSET, ring->doorbell_index);\n\t\t} else {\n\t\t\tdoorbell = REG_SET_FIELD(doorbell, SDMA0_QUEUE0_DOORBELL, ENABLE, 0);\n\t\t}\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_DOORBELL), doorbell);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_DOORBELL_OFFSET), doorbell_offset);\n\n\t\tif (i == 0)\n\t\t\tadev->nbio.funcs->sdma_doorbell_range(adev, i, ring->use_doorbell,\n\t\t\t\t\t\t      ring->doorbell_index,\n\t\t\t\t\t\t      adev->doorbell_index.sdma_doorbell_range * adev->sdma.num_instances);\n\n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\tsdma_v6_0_ring_set_wptr(ring);\n\n\t\t \n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_MINOR_PTR_UPDATE), 0);\n\n\t\t \n\t\ttemp = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UTCL1_CNTL));\n\t\ttemp = REG_SET_FIELD(temp, SDMA0_UTCL1_CNTL, RESP_MODE, 3);\n\t\ttemp = REG_SET_FIELD(temp, SDMA0_UTCL1_CNTL, REDO_DELAY, 9);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UTCL1_CNTL), temp);\n\n\t\t \n\t\ttemp = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UTCL1_PAGE));\n\t\t \n\t\ttemp &= 0xFF0FFF;\n\t\ttemp |= ((CACHE_READ_POLICY_L2__DEFAULT << 12) |\n\t\t\t (CACHE_WRITE_POLICY_L2__DEFAULT << 14) |\n\t\t\t SDMA0_UTCL1_PAGE__LLC_NOALLOC_MASK);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UTCL1_PAGE), temp);\n\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\t \n\t\t\ttemp = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL));\n\t\t\ttemp = REG_SET_FIELD(temp, SDMA0_F32_CNTL, HALT, 0);\n\t\t\ttemp = REG_SET_FIELD(temp, SDMA0_F32_CNTL, TH1_RESET, 0);\n\t\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL), temp);\n\t\t}\n\n\t\t \n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_QUEUE0_RB_CNTL, RB_ENABLE, 1);\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_RB_CNTL), rb_cntl);\n\n\t\tib_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_IB_CNTL));\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_QUEUE0_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_QUEUE0_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t\t \n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_IB_CNTL), ib_cntl);\n\n\t\tif (amdgpu_sriov_vf(adev))\n\t\t\tsdma_v6_0_enable(adev, true);\n\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sdma_v6_0_rlc_resume(struct amdgpu_device *adev)\n{\n\treturn 0;\n}\n\n \nstatic int sdma_v6_0_load_microcode(struct amdgpu_device *adev)\n{\n\tconst struct sdma_firmware_header_v2_0 *hdr;\n\tconst __le32 *fw_data;\n\tu32 fw_size;\n\tint i, j;\n\tbool use_broadcast;\n\n\t \n\tsdma_v6_0_enable(adev, false);\n\n\tif (!adev->sdma.instance[0].fw)\n\t\treturn -EINVAL;\n\n\t \n\tuse_broadcast = true;\n\n\tif (use_broadcast) {\n\t\tdev_info(adev->dev, \"Use broadcast method to load SDMA firmware\\n\");\n\t\t \n\t\thdr = (const struct sdma_firmware_header_v2_0 *)adev->sdma.instance[0].fw->data;\n\t\tamdgpu_ucode_print_sdma_hdr(&hdr->header);\n\t\tfw_size = le32_to_cpu(hdr->ctx_jt_offset + hdr->ctx_jt_size) / 4;\n\n\t\tfw_data = (const __le32 *)\n\t\t\t(adev->sdma.instance[0].fw->data +\n\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\t\tWREG32(sdma_v6_0_get_reg_offset(adev, 0, regSDMA0_BROADCAST_UCODE_ADDR), 0);\n\n\t\tfor (j = 0; j < fw_size; j++) {\n\t\t\tif (amdgpu_emu_mode == 1 && j % 500 == 0)\n\t\t\t\tmsleep(1);\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, 0, regSDMA0_BROADCAST_UCODE_DATA), le32_to_cpup(fw_data++));\n\t\t}\n\n\t\t \n\t\tfw_size = le32_to_cpu(hdr->ctl_jt_offset + hdr->ctl_jt_size) / 4;\n\n\t\tfw_data = (const __le32 *)\n\t\t\t(adev->sdma.instance[0].fw->data +\n\t\t\t\tle32_to_cpu(hdr->ctl_ucode_offset));\n\n\t\tWREG32(sdma_v6_0_get_reg_offset(adev, 0, regSDMA0_BROADCAST_UCODE_ADDR), 0x8000);\n\n\t\tfor (j = 0; j < fw_size; j++) {\n\t\t\tif (amdgpu_emu_mode == 1 && j % 500 == 0)\n\t\t\t\tmsleep(1);\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, 0, regSDMA0_BROADCAST_UCODE_DATA), le32_to_cpup(fw_data++));\n\t\t}\n\t} else {\n\t\tdev_info(adev->dev, \"Use legacy method to load SDMA firmware\\n\");\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\t \n\t\t\thdr = (const struct sdma_firmware_header_v2_0 *)adev->sdma.instance[0].fw->data;\n\t\t\tamdgpu_ucode_print_sdma_hdr(&hdr->header);\n\t\t\tfw_size = le32_to_cpu(hdr->ctx_jt_offset + hdr->ctx_jt_size) / 4;\n\n\t\t\tfw_data = (const __le32 *)\n\t\t\t\t(adev->sdma.instance[0].fw->data +\n\t\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_ADDR), 0);\n\n\t\t\tfor (j = 0; j < fw_size; j++) {\n\t\t\t\tif (amdgpu_emu_mode == 1 && j % 500 == 0)\n\t\t\t\t\tmsleep(1);\n\t\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_DATA), le32_to_cpup(fw_data++));\n\t\t\t}\n\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_ADDR), adev->sdma.instance[0].fw_version);\n\n\t\t\t \n\t\t\tfw_size = le32_to_cpu(hdr->ctl_jt_offset + hdr->ctl_jt_size) / 4;\n\n\t\t\tfw_data = (const __le32 *)\n\t\t\t\t(adev->sdma.instance[0].fw->data +\n\t\t\t\t\tle32_to_cpu(hdr->ctl_ucode_offset));\n\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_ADDR), 0x8000);\n\n\t\t\tfor (j = 0; j < fw_size; j++) {\n\t\t\t\tif (amdgpu_emu_mode == 1 && j % 500 == 0)\n\t\t\t\t\tmsleep(1);\n\t\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_DATA), le32_to_cpup(fw_data++));\n\t\t\t}\n\n\t\t\tWREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_UCODE_ADDR), adev->sdma.instance[0].fw_version);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v6_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 tmp;\n\tint i;\n\n\tsdma_v6_0_gfx_stop(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\ttmp = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_FREEZE));\n\t\ttmp |= SDMA0_FREEZE__FREEZE_MASK;\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_FREEZE), tmp);\n\t\ttmp = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL));\n\t\ttmp |= SDMA0_F32_CNTL__HALT_MASK;\n\t\ttmp |= SDMA0_F32_CNTL__TH1_RESET_MASK;\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_F32_CNTL), tmp);\n\n\t\tWREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, i, regSDMA0_QUEUE0_PREEMPT), 0);\n\n\t\tudelay(100);\n\n\t\ttmp = GRBM_SOFT_RESET__SOFT_RESET_SDMA0_MASK << i;\n\t\tWREG32_SOC15(GC, 0, regGRBM_SOFT_RESET, tmp);\n\t\ttmp = RREG32_SOC15(GC, 0, regGRBM_SOFT_RESET);\n\n\t\tudelay(100);\n\n\t\tWREG32_SOC15(GC, 0, regGRBM_SOFT_RESET, 0);\n\t\ttmp = RREG32_SOC15(GC, 0, regGRBM_SOFT_RESET);\n\n\t\tudelay(100);\n\t}\n\n\treturn sdma_v6_0_start(adev);\n}\n\nstatic bool sdma_v6_0_check_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tstruct amdgpu_ring *ring;\n\tint i, r;\n\tlong tmo = msecs_to_jiffies(1000);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tr = amdgpu_ring_test_ib(ring, tmo);\n\t\tif (r)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int sdma_v6_0_start(struct amdgpu_device *adev)\n{\n\tint r = 0;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tsdma_v6_0_enable(adev, false);\n\n\t\t \n\t\tr = sdma_v6_0_gfx_resume(adev);\n\t\treturn r;\n\t}\n\n\tif (adev->firmware.load_type == AMDGPU_FW_LOAD_DIRECT) {\n\t\tr = sdma_v6_0_load_microcode(adev);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\t \n\t\tif (amdgpu_emu_mode == 1)\n\t\t\tmsleep(1000);\n\t}\n\n\t \n\tsdma_v6_0_enable(adev, true);\n\t \n\tsdma_v6_0_ctxempty_int_enable(adev, true);\n\n\t \n\tr = sdma_v6_0_gfx_resume(adev);\n\tif (r)\n\t\treturn r;\n\tr = sdma_v6_0_rlc_resume(adev);\n\n\treturn r;\n}\n\nstatic int sdma_v6_0_mqd_init(struct amdgpu_device *adev, void *mqd,\n\t\t\t      struct amdgpu_mqd_prop *prop)\n{\n\tstruct v11_sdma_mqd *m = mqd;\n\tuint64_t wb_gpu_addr;\n\n\tm->sdmax_rlcx_rb_cntl =\n\t\torder_base_2(prop->queue_size / 4) << SDMA0_QUEUE0_RB_CNTL__RB_SIZE__SHIFT |\n\t\t1 << SDMA0_QUEUE0_RB_CNTL__RPTR_WRITEBACK_ENABLE__SHIFT |\n\t\t4 << SDMA0_QUEUE0_RB_CNTL__RPTR_WRITEBACK_TIMER__SHIFT |\n\t\t1 << SDMA0_QUEUE0_RB_CNTL__F32_WPTR_POLL_ENABLE__SHIFT;\n\n\tm->sdmax_rlcx_rb_base = lower_32_bits(prop->hqd_base_gpu_addr >> 8);\n\tm->sdmax_rlcx_rb_base_hi = upper_32_bits(prop->hqd_base_gpu_addr >> 8);\n\n\twb_gpu_addr = prop->wptr_gpu_addr;\n\tm->sdmax_rlcx_rb_wptr_poll_addr_lo = lower_32_bits(wb_gpu_addr);\n\tm->sdmax_rlcx_rb_wptr_poll_addr_hi = upper_32_bits(wb_gpu_addr);\n\n\twb_gpu_addr = prop->rptr_gpu_addr;\n\tm->sdmax_rlcx_rb_rptr_addr_lo = lower_32_bits(wb_gpu_addr);\n\tm->sdmax_rlcx_rb_rptr_addr_hi = upper_32_bits(wb_gpu_addr);\n\n\tm->sdmax_rlcx_ib_cntl = RREG32_SOC15_IP(GC, sdma_v6_0_get_reg_offset(adev, 0,\n\t\t\t\t\t\t\tregSDMA0_QUEUE0_IB_CNTL));\n\n\tm->sdmax_rlcx_doorbell_offset =\n\t\tprop->doorbell_index << SDMA0_QUEUE0_DOORBELL_OFFSET__OFFSET__SHIFT;\n\n\tm->sdmax_rlcx_doorbell = REG_SET_FIELD(0, SDMA0_QUEUE0_DOORBELL, ENABLE, 1);\n\n\tm->sdmax_rlcx_skip_cntl = 0;\n\tm->sdmax_rlcx_context_status = 0;\n\tm->sdmax_rlcx_doorbell_log = 0;\n\n\tm->sdmax_rlcx_rb_aql_cntl = regSDMA0_QUEUE0_RB_AQL_CNTL_DEFAULT;\n\tm->sdmax_rlcx_dummy_reg = regSDMA0_QUEUE0_DUMMY_REG_DEFAULT;\n\n\treturn 0;\n}\n\nstatic void sdma_v6_0_set_mqd_funcs(struct amdgpu_device *adev)\n{\n\tadev->mqds[AMDGPU_HW_IP_DMA].mqd_size = sizeof(struct v11_sdma_mqd);\n\tadev->mqds[AMDGPU_HW_IP_DMA].init_mqd = sdma_v6_0_mqd_init;\n}\n\n \nstatic int sdma_v6_0_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\tvolatile uint32_t *cpu_ptr = NULL;\n\n\ttmp = 0xCAFEDEAD;\n\n\tif (ring->is_mes_queue) {\n\t\tuint32_t offset = 0;\n\t\toffset = amdgpu_mes_ctx_get_offs(ring,\n\t\t\t\t\t AMDGPU_MES_CTX_PADDING_OFFS);\n\t\tgpu_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);\n\t\tcpu_ptr = amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);\n\t\t*cpu_ptr = tmp;\n\t} else {\n\t\tr = amdgpu_device_wb_get(adev, &index);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%d) failed to allocate wb slot\\n\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\t\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\t}\n\n\tr = amdgpu_ring_alloc(ring, 5);\n\tif (r) {\n\t\tDRM_ERROR(\"amdgpu: dma failed to lock ring %d (%d).\\n\", ring->idx, r);\n\t\tamdgpu_device_wb_free(adev, index);\n\t\treturn r;\n\t}\n\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_WRITE) |\n\t\t\t  SDMA_PKT_COPY_LINEAR_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (ring->is_mes_queue)\n\t\t\ttmp = le32_to_cpu(*cpu_ptr);\n\t\telse\n\t\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tif (amdgpu_emu_mode == 1)\n\t\t\tmsleep(1);\n\t\telse\n\t\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\n\tif (!ring->is_mes_queue)\n\t\tamdgpu_device_wb_free(adev, index);\n\n\treturn r;\n}\n\n \nstatic int sdma_v6_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tlong r;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\tvolatile uint32_t *cpu_ptr = NULL;\n\n\ttmp = 0xCAFEDEAD;\n\tmemset(&ib, 0, sizeof(ib));\n\n\tif (ring->is_mes_queue) {\n\t\tuint32_t offset = 0;\n\t\toffset = amdgpu_mes_ctx_get_offs(ring, AMDGPU_MES_CTX_IB_OFFS);\n\t\tib.gpu_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);\n\t\tib.ptr = (void *)amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);\n\n\t\toffset = amdgpu_mes_ctx_get_offs(ring,\n\t\t\t\t\t AMDGPU_MES_CTX_PADDING_OFFS);\n\t\tgpu_addr = amdgpu_mes_ctx_get_offs_gpu_addr(ring, offset);\n\t\tcpu_ptr = amdgpu_mes_ctx_get_offs_cpu_addr(ring, offset);\n\t\t*cpu_ptr = tmp;\n\t} else {\n\t\tr = amdgpu_device_wb_get(adev, &index);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%ld) failed to allocate wb slot\\n\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\t\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\t\tr = amdgpu_ib_get(adev, NULL, 256, AMDGPU_IB_POOL_DIRECT, &ib);\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"amdgpu: failed to get ib (%ld).\\n\", r);\n\t\t\tgoto err0;\n\t\t}\n\t}\n\n\tib.ptr[0] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr);\n\tib.ptr[3] = SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0);\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.ptr[5] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[6] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[7] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.length_dw = 8;\n\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tDRM_ERROR(\"amdgpu: IB test timed out\\n\");\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tDRM_ERROR(\"amdgpu: fence wait failed (%ld).\\n\", r);\n\t\tgoto err1;\n\t}\n\n\tif (ring->is_mes_queue)\n\t\ttmp = le32_to_cpu(*cpu_ptr);\n\telse\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tif (!ring->is_mes_queue)\n\t\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n\n \nstatic void sdma_v6_0_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t\t  uint64_t pe, uint64_t src,\n\t\t\t\t  unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\n\tib->ptr[ib->length_dw++] = bytes - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\n}\n\n \nstatic void sdma_v6_0_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t   uint64_t value, unsigned count,\n\t\t\t\t   uint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = ndw - 1;\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void sdma_v6_0_vm_set_pte_pde(struct amdgpu_ib *ib,\n\t\t\t\t     uint64_t pe,\n\t\t\t\t     uint64_t addr, unsigned count,\n\t\t\t\t     uint32_t incr, uint64_t flags)\n{\n\t \n\tib->ptr[ib->length_dw++] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_PTEPDE);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\tib->ptr[ib->length_dw++] = lower_32_bits(addr);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = incr;  \n\tib->ptr[ib->length_dw++] = 0;\n\tib->ptr[ib->length_dw++] = count - 1;  \n}\n\n \nstatic void sdma_v6_0_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tu32 pad_count;\n\tint i;\n\n\tpad_count = (-ib->length_dw) & 0x7;\n\tfor (i = 0; i < pad_count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_NOP) |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);\n\t\telse\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_NOP);\n}\n\n \nstatic void sdma_v6_0_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3) |  \n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_MEM_POLL(1));\n\tamdgpu_ring_write(ring, addr & 0xfffffffc);\n\tamdgpu_ring_write(ring, upper_32_bits(addr) & 0xffffffff);\n\tamdgpu_ring_write(ring, seq);  \n\tamdgpu_ring_write(ring, 0xffffffff);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(4));  \n}\n\n \nstatic void sdma_v6_0_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\t unsigned vmid, uint64_t pd_addr)\n{\n\tstruct amdgpu_vmhub *hub = &ring->adev->vmhub[ring->vm_hub];\n\tuint32_t req = hub->vmhub_funcs->get_invalidate_req(vmid, 0);\n\n\t \n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_lo32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      lower_32_bits(pd_addr));\n\tamdgpu_ring_emit_wreg(ring, hub->ctx0_ptb_addr_hi32 +\n\t\t\t      (hub->ctx_addr_distance * vmid),\n\t\t\t      upper_32_bits(pd_addr));\n\n\t \n\tamdgpu_ring_write(ring,\n\t\t\t  SDMA_PKT_VM_INVALIDATION_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_VM_INVALIDATION_HEADER_SUB_OP(SDMA_SUBOP_VM_INVALIDATION) |\n\t\t\t  SDMA_PKT_VM_INVALIDATION_HEADER_GFX_ENG_ID(ring->vm_inv_eng) |\n\t\t\t  SDMA_PKT_VM_INVALIDATION_HEADER_MM_ENG_ID(0x1f));\n\tamdgpu_ring_write(ring, req);\n\tamdgpu_ring_write(ring, 0xFFFFFFFF);\n\tamdgpu_ring_write(ring,\n\t\t\t  SDMA_PKT_VM_INVALIDATION_ADDRESSRANGEHI_INVALIDATEACK(1 << vmid) |\n\t\t\t  SDMA_PKT_VM_INVALIDATION_ADDRESSRANGEHI_ADDRESSRANGEHI(0x1F));\n}\n\nstatic void sdma_v6_0_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t     uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_SRBM_WRITE) |\n\t\t\t  SDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void sdma_v6_0_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t\t uint32_t val, uint32_t mask)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(0) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));  \n\tamdgpu_ring_write(ring, reg << 2);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, val);  \n\tamdgpu_ring_write(ring, mask);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(10));\n}\n\nstatic void sdma_v6_0_ring_emit_reg_write_reg_wait(struct amdgpu_ring *ring,\n\t\t\t\t\t\t   uint32_t reg0, uint32_t reg1,\n\t\t\t\t\t\t   uint32_t ref, uint32_t mask)\n{\n\tamdgpu_ring_emit_wreg(ring, reg0, ref);\n\t \n\tamdgpu_ring_emit_reg_wait(ring, reg0, 0, 0);\n\tamdgpu_ring_emit_reg_wait(ring, reg1, mask, mask);\n}\n\nstatic struct amdgpu_sdma_ras sdma_v6_0_3_ras = {\n\t.ras_block = {\n\t\t.ras_late_init = amdgpu_ras_block_late_init,\n\t},\n};\n\nstatic void sdma_v6_0_set_ras_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(6, 0, 3):\n\t\tadev->sdma.ras = &sdma_v6_0_3_ras;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n}\n\nstatic int sdma_v6_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsdma_v6_0_set_ring_funcs(adev);\n\tsdma_v6_0_set_buffer_funcs(adev);\n\tsdma_v6_0_set_vm_pte_funcs(adev);\n\tsdma_v6_0_set_irq_funcs(adev);\n\tsdma_v6_0_set_mqd_funcs(adev);\n\tsdma_v6_0_set_ras_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int sdma_v6_0_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tr = amdgpu_irq_add_id(adev, SOC21_IH_CLIENTID_GFX,\n\t\t\t      GFX_11_0_0__SRCID__SDMA_TRAP,\n\t\t\t      &adev->sdma.trap_irq);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_sdma_init_microcode(adev, 0, true);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load sdma firmware!\\n\");\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tring->use_doorbell = true;\n\t\tring->me = i;\n\n\t\tDRM_DEBUG(\"SDMA %d use_doorbell being set to: [%s]\\n\", i,\n\t\t\t\tring->use_doorbell?\"true\":\"false\");\n\n\t\tring->doorbell_index =\n\t\t\t(adev->doorbell_index.sdma_engine[i] << 1); \n\n\t\tring->vm_hub = AMDGPU_GFXHUB(0);\n\t\tsprintf(ring->name, \"sdma%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 1024,\n\t\t\t\t     &adev->sdma.trap_irq,\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE0 + i,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (amdgpu_sdma_ras_sw_init(adev)) {\n\t\tdev_err(adev->dev, \"Failed to initialize sdma ras block!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\nstatic int sdma_v6_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\n\tamdgpu_sdma_destroy_inst_ctx(adev, true);\n\n\treturn 0;\n}\n\nstatic int sdma_v6_0_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v6_0_start(adev);\n}\n\nstatic int sdma_v6_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\t\treturn 0;\n\t}\n\n\tsdma_v6_0_ctxempty_int_enable(adev, false);\n\tsdma_v6_0_enable(adev, false);\n\n\treturn 0;\n}\n\nstatic int sdma_v6_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v6_0_hw_fini(adev);\n}\n\nstatic int sdma_v6_0_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn sdma_v6_0_hw_init(adev);\n}\n\nstatic bool sdma_v6_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tu32 tmp = RREG32(sdma_v6_0_get_reg_offset(adev, i, regSDMA0_STATUS_REG));\n\n\t\tif (!(tmp & SDMA0_STATUS_REG__IDLE_MASK))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int sdma_v6_0_wait_for_idle(void *handle)\n{\n\tunsigned i;\n\tu32 sdma0, sdma1;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tsdma0 = RREG32(sdma_v6_0_get_reg_offset(adev, 0, regSDMA0_STATUS_REG));\n\t\tsdma1 = RREG32(sdma_v6_0_get_reg_offset(adev, 1, regSDMA0_STATUS_REG));\n\n\t\tif (sdma0 & sdma1 & SDMA0_STATUS_REG__IDLE_MASK)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int sdma_v6_0_ring_preempt_ib(struct amdgpu_ring *ring)\n{\n\tint i, r = 0;\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 index = 0;\n\tu64 sdma_gfx_preempt;\n\n\tamdgpu_sdma_get_index_from_ring(ring, &index);\n\tsdma_gfx_preempt =\n\t\tsdma_v6_0_get_reg_offset(adev, index, regSDMA0_QUEUE0_PREEMPT);\n\n\t \n\tamdgpu_ring_set_preempt_cond_exec(ring, false);\n\n\t \n\tring->trail_seq += 1;\n\tamdgpu_ring_alloc(ring, 10);\n\tsdma_v6_0_ring_emit_fence(ring, ring->trail_fence_gpu_addr,\n\t\t\t\t  ring->trail_seq, 0);\n\tamdgpu_ring_commit(ring);\n\n\t \n\tWREG32(sdma_gfx_preempt, 1);\n\n\t \n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (ring->trail_seq ==\n\t\t    le32_to_cpu(*(ring->trail_fence_cpu_addr)))\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout) {\n\t\tr = -EINVAL;\n\t\tDRM_ERROR(\"ring %d failed to be preempted\\n\", ring->idx);\n\t}\n\n\t \n\tWREG32(sdma_gfx_preempt, 0);\n\n\t \n\tamdgpu_ring_set_preempt_cond_exec(ring, true);\n\treturn r;\n}\n\nstatic int sdma_v6_0_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tu32 reg_offset = sdma_v6_0_get_reg_offset(adev, type, regSDMA0_CNTL);\n\n\tif (!amdgpu_sriov_vf(adev)) {\n\t\tsdma_cntl = RREG32(reg_offset);\n\t\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE,\n\t\t\t\tstate == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\t\tWREG32(reg_offset, sdma_cntl);\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v6_0_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tint instances, queue;\n\tuint32_t mes_queue_id = entry->src_data[0];\n\n\tDRM_DEBUG(\"IH: SDMA trap\\n\");\n\n\tif (adev->enable_mes && (mes_queue_id & AMDGPU_FENCE_MES_QUEUE_FLAG)) {\n\t\tstruct amdgpu_mes_queue *queue;\n\n\t\tmes_queue_id &= AMDGPU_FENCE_MES_QUEUE_ID_MASK;\n\n\t\tspin_lock(&adev->mes.queue_id_lock);\n\t\tqueue = idr_find(&adev->mes.queue_id_idr, mes_queue_id);\n\t\tif (queue) {\n\t\t\tDRM_DEBUG(\"process smda queue id = %d\\n\", mes_queue_id);\n\t\t\tamdgpu_fence_process(queue->ring);\n\t\t}\n\t\tspin_unlock(&adev->mes.queue_id_lock);\n\t\treturn 0;\n\t}\n\n\tqueue = entry->ring_id & 0xf;\n\tinstances = (entry->ring_id & 0xf0) >> 4;\n\tif (instances > 1) {\n\t\tDRM_ERROR(\"IH: wrong ring_ID detected, as wrong sdma instance\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (entry->client_id) {\n\tcase SOC21_IH_CLIENTID_GFX:\n\t\tswitch (queue) {\n\t\tcase 0:\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[instances].ring);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v6_0_process_illegal_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\treturn 0;\n}\n\nstatic int sdma_v6_0_set_clockgating_state(void *handle,\n\t\t\t\t\t   enum amd_clockgating_state state)\n{\n\treturn 0;\n}\n\nstatic int sdma_v6_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\treturn 0;\n}\n\nstatic void sdma_v6_0_get_clockgating_state(void *handle, u64 *flags)\n{\n}\n\nconst struct amd_ip_funcs sdma_v6_0_ip_funcs = {\n\t.name = \"sdma_v6_0\",\n\t.early_init = sdma_v6_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = sdma_v6_0_sw_init,\n\t.sw_fini = sdma_v6_0_sw_fini,\n\t.hw_init = sdma_v6_0_hw_init,\n\t.hw_fini = sdma_v6_0_hw_fini,\n\t.suspend = sdma_v6_0_suspend,\n\t.resume = sdma_v6_0_resume,\n\t.is_idle = sdma_v6_0_is_idle,\n\t.wait_for_idle = sdma_v6_0_wait_for_idle,\n\t.soft_reset = sdma_v6_0_soft_reset,\n\t.check_soft_reset = sdma_v6_0_check_soft_reset,\n\t.set_clockgating_state = sdma_v6_0_set_clockgating_state,\n\t.set_powergating_state = sdma_v6_0_set_powergating_state,\n\t.get_clockgating_state = sdma_v6_0_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v6_0_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xf,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = true,\n\t.secure_submission_supported = true,\n\t.get_rptr = sdma_v6_0_ring_get_rptr,\n\t.get_wptr = sdma_v6_0_ring_get_wptr,\n\t.set_wptr = sdma_v6_0_ring_set_wptr,\n\t.emit_frame_size =\n\t\t5 +  \n\t\t6 +  \n\t\t6 +  \n\t\t \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 6 +\n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 5 + 7 + 6,  \n\t.emit_ib = sdma_v6_0_ring_emit_ib,\n\t.emit_mem_sync = sdma_v6_0_ring_emit_mem_sync,\n\t.emit_fence = sdma_v6_0_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v6_0_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v6_0_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v6_0_ring_emit_hdp_flush,\n\t.test_ring = sdma_v6_0_ring_test_ring,\n\t.test_ib = sdma_v6_0_ring_test_ib,\n\t.insert_nop = sdma_v6_0_ring_insert_nop,\n\t.pad_ib = sdma_v6_0_ring_pad_ib,\n\t.emit_wreg = sdma_v6_0_ring_emit_wreg,\n\t.emit_reg_wait = sdma_v6_0_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = sdma_v6_0_ring_emit_reg_write_reg_wait,\n\t.init_cond_exec = sdma_v6_0_ring_init_cond_exec,\n\t.patch_cond_exec = sdma_v6_0_ring_patch_cond_exec,\n\t.preempt_ib = sdma_v6_0_ring_preempt_ib,\n};\n\nstatic void sdma_v6_0_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->sdma.instance[i].ring.funcs = &sdma_v6_0_ring_funcs;\n\t\tadev->sdma.instance[i].ring.me = i;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs sdma_v6_0_trap_irq_funcs = {\n\t.set = sdma_v6_0_set_trap_irq_state,\n\t.process = sdma_v6_0_process_trap_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v6_0_illegal_inst_irq_funcs = {\n\t.process = sdma_v6_0_process_illegal_inst_irq,\n};\n\nstatic void sdma_v6_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = AMDGPU_SDMA_IRQ_INSTANCE0 +\n\t\t\t\t\tadev->sdma.num_instances;\n\tadev->sdma.trap_irq.funcs = &sdma_v6_0_trap_irq_funcs;\n\tadev->sdma.illegal_inst_irq.funcs = &sdma_v6_0_illegal_inst_irq_funcs;\n}\n\n \nstatic void sdma_v6_0_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint64_t src_offset,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count,\n\t\t\t\t       bool tmz)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_TMZ(tmz ? 1 : 0);\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n}\n\n \nstatic void sdma_v6_0_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint32_t src_data,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_COPY_LINEAR_HEADER_OP(SDMA_OP_CONST_FILL);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n}\n\nstatic const struct amdgpu_buffer_funcs sdma_v6_0_buffer_funcs = {\n\t.copy_max_bytes = 0x400000,\n\t.copy_num_dw = 7,\n\t.emit_copy_buffer = sdma_v6_0_emit_copy_buffer,\n\n\t.fill_max_bytes = 0x400000,\n\t.fill_num_dw = 5,\n\t.emit_fill_buffer = sdma_v6_0_emit_fill_buffer,\n};\n\nstatic void sdma_v6_0_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &sdma_v6_0_buffer_funcs;\n\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs sdma_v6_0_vm_pte_funcs = {\n\t.copy_pte_num_dw = 7,\n\t.copy_pte = sdma_v6_0_vm_copy_pte,\n\t.write_pte = sdma_v6_0_vm_write_pte,\n\t.set_pte_pde = sdma_v6_0_vm_set_pte_pde,\n};\n\nstatic void sdma_v6_0_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &sdma_v6_0_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->vm_manager.vm_pte_scheds[i] =\n\t\t\t&adev->sdma.instance[i].ring.sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nconst struct amdgpu_ip_block_version sdma_v6_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 6,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &sdma_v6_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}