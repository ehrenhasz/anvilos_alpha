{
  "module_name": "amdgpu_vm_sdma.c",
  "hash_id": "4fec09b680f7e86d71dd5190058b4d19480cf2827ffff8bc52f02cbeea6271ad",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_vm_sdma.c",
  "human_readable_source": " \n\n#include \"amdgpu_vm.h\"\n#include \"amdgpu_job.h\"\n#include \"amdgpu_object.h\"\n#include \"amdgpu_trace.h\"\n\n#define AMDGPU_VM_SDMA_MIN_NUM_DW\t256u\n#define AMDGPU_VM_SDMA_MAX_NUM_DW\t(16u * 1024u)\n\n \nstatic int amdgpu_vm_sdma_map_table(struct amdgpu_bo_vm *table)\n{\n\tint r;\n\n\tr = amdgpu_ttm_alloc_gart(&table->bo.tbo);\n\tif (r)\n\t\treturn r;\n\n\tif (table->shadow)\n\t\tr = amdgpu_ttm_alloc_gart(&table->shadow->tbo);\n\n\treturn r;\n}\n\n \nstatic int amdgpu_vm_sdma_alloc_job(struct amdgpu_vm_update_params *p,\n\t\t\t\t    unsigned int count)\n{\n\tenum amdgpu_ib_pool_type pool = p->immediate ? AMDGPU_IB_POOL_IMMEDIATE\n\t\t: AMDGPU_IB_POOL_DELAYED;\n\tstruct drm_sched_entity *entity = p->immediate ? &p->vm->immediate\n\t\t: &p->vm->delayed;\n\tunsigned int ndw;\n\tint r;\n\n\t \n\tndw = AMDGPU_VM_SDMA_MIN_NUM_DW;\n\tif (p->pages_addr)\n\t\tndw += count * 2;\n\tndw = min(ndw, AMDGPU_VM_SDMA_MAX_NUM_DW);\n\n\tr = amdgpu_job_alloc_with_ib(p->adev, entity, AMDGPU_FENCE_OWNER_VM,\n\t\t\t\t     ndw * 4, pool, &p->job);\n\tif (r)\n\t\treturn r;\n\n\tp->num_dw_left = ndw;\n\treturn 0;\n}\n\n \nstatic int amdgpu_vm_sdma_prepare(struct amdgpu_vm_update_params *p,\n\t\t\t\t  struct dma_resv *resv,\n\t\t\t\t  enum amdgpu_sync_mode sync_mode)\n{\n\tstruct amdgpu_sync sync;\n\tint r;\n\n\tr = amdgpu_vm_sdma_alloc_job(p, 0);\n\tif (r)\n\t\treturn r;\n\n\tif (!resv)\n\t\treturn 0;\n\n\tamdgpu_sync_create(&sync);\n\tr = amdgpu_sync_resv(p->adev, &sync, resv, sync_mode, p->vm);\n\tif (!r)\n\t\tr = amdgpu_sync_push_to_job(&sync, p->job);\n\tamdgpu_sync_free(&sync);\n\treturn r;\n}\n\n \nstatic int amdgpu_vm_sdma_commit(struct amdgpu_vm_update_params *p,\n\t\t\t\t struct dma_fence **fence)\n{\n\tstruct amdgpu_ib *ib = p->job->ibs;\n\tstruct amdgpu_ring *ring;\n\tstruct dma_fence *f;\n\n\tring = container_of(p->vm->delayed.rq->sched, struct amdgpu_ring,\n\t\t\t    sched);\n\n\tWARN_ON(ib->length_dw == 0);\n\tamdgpu_ring_pad_ib(ring, ib);\n\tWARN_ON(ib->length_dw > p->num_dw_left);\n\tf = amdgpu_job_submit(p->job);\n\n\tif (p->unlocked) {\n\t\tstruct dma_fence *tmp = dma_fence_get(f);\n\n\t\tswap(p->vm->last_unlocked, tmp);\n\t\tdma_fence_put(tmp);\n\t} else {\n\t\tdma_resv_add_fence(p->vm->root.bo->tbo.base.resv, f,\n\t\t\t\t   DMA_RESV_USAGE_BOOKKEEP);\n\t}\n\n\tif (fence && !p->immediate) {\n\t\t \n\t\tset_bit(DRM_SCHED_FENCE_DONT_PIPELINE, &f->flags);\n\t\tswap(*fence, f);\n\t}\n\tdma_fence_put(f);\n\treturn 0;\n}\n\n \nstatic void amdgpu_vm_sdma_copy_ptes(struct amdgpu_vm_update_params *p,\n\t\t\t\t     struct amdgpu_bo *bo, uint64_t pe,\n\t\t\t\t     unsigned count)\n{\n\tstruct amdgpu_ib *ib = p->job->ibs;\n\tuint64_t src = ib->gpu_addr;\n\n\tsrc += p->num_dw_left * 4;\n\n\tpe += amdgpu_bo_gpu_offset_no_check(bo);\n\ttrace_amdgpu_vm_copy_ptes(pe, src, count, p->immediate);\n\n\tamdgpu_vm_copy_pte(p->adev, ib, pe, src, count);\n}\n\n \nstatic void amdgpu_vm_sdma_set_ptes(struct amdgpu_vm_update_params *p,\n\t\t\t\t    struct amdgpu_bo *bo, uint64_t pe,\n\t\t\t\t    uint64_t addr, unsigned count,\n\t\t\t\t    uint32_t incr, uint64_t flags)\n{\n\tstruct amdgpu_ib *ib = p->job->ibs;\n\n\tpe += amdgpu_bo_gpu_offset_no_check(bo);\n\ttrace_amdgpu_vm_set_ptes(pe, addr, count, incr, flags, p->immediate);\n\tif (count < 3) {\n\t\tamdgpu_vm_write_pte(p->adev, ib, pe, addr | flags,\n\t\t\t\t    count, incr);\n\t} else {\n\t\tamdgpu_vm_set_pte_pde(p->adev, ib, pe, addr,\n\t\t\t\t      count, incr, flags);\n\t}\n}\n\n \nstatic int amdgpu_vm_sdma_update(struct amdgpu_vm_update_params *p,\n\t\t\t\t struct amdgpu_bo_vm *vmbo, uint64_t pe,\n\t\t\t\t uint64_t addr, unsigned count, uint32_t incr,\n\t\t\t\t uint64_t flags)\n{\n\tstruct amdgpu_bo *bo = &vmbo->bo;\n\tstruct dma_resv_iter cursor;\n\tunsigned int i, ndw, nptes;\n\tstruct dma_fence *fence;\n\tuint64_t *pte;\n\tint r;\n\n\t \n\tdma_resv_iter_begin(&cursor, bo->tbo.base.resv, DMA_RESV_USAGE_KERNEL);\n\tdma_resv_for_each_fence_unlocked(&cursor, fence) {\n\t\tdma_fence_get(fence);\n\t\tr = drm_sched_job_add_dependency(&p->job->base, fence);\n\t\tif (r) {\n\t\t\tdma_fence_put(fence);\n\t\t\tdma_resv_iter_end(&cursor);\n\t\t\treturn r;\n\t\t}\n\t}\n\tdma_resv_iter_end(&cursor);\n\n\tdo {\n\t\tndw = p->num_dw_left;\n\t\tndw -= p->job->ibs->length_dw;\n\n\t\tif (ndw < 32) {\n\t\t\tr = amdgpu_vm_sdma_commit(p, NULL);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tr = amdgpu_vm_sdma_alloc_job(p, count);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\tif (!p->pages_addr) {\n\t\t\t \n\t\t\tif (vmbo->shadow)\n\t\t\t\tamdgpu_vm_sdma_set_ptes(p, vmbo->shadow, pe, addr,\n\t\t\t\t\t\t\tcount, incr, flags);\n\t\t\tamdgpu_vm_sdma_set_ptes(p, bo, pe, addr, count,\n\t\t\t\t\t\tincr, flags);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tndw -= p->adev->vm_manager.vm_pte_funcs->copy_pte_num_dw *\n\t\t\t(vmbo->shadow ? 2 : 1);\n\n\t\t \n\t\tndw -= 7;\n\n\t\tnptes = min(count, ndw / 2);\n\n\t\t \n\t\tp->num_dw_left -= nptes * 2;\n\t\tpte = (uint64_t *)&(p->job->ibs->ptr[p->num_dw_left]);\n\t\tfor (i = 0; i < nptes; ++i, addr += incr) {\n\t\t\tpte[i] = amdgpu_vm_map_gart(p->pages_addr, addr);\n\t\t\tpte[i] |= flags;\n\t\t}\n\n\t\tif (vmbo->shadow)\n\t\t\tamdgpu_vm_sdma_copy_ptes(p, vmbo->shadow, pe, nptes);\n\t\tamdgpu_vm_sdma_copy_ptes(p, bo, pe, nptes);\n\n\t\tpe += nptes * 8;\n\t\tcount -= nptes;\n\t} while (count);\n\n\treturn 0;\n}\n\nconst struct amdgpu_vm_update_funcs amdgpu_vm_sdma_funcs = {\n\t.map_table = amdgpu_vm_sdma_map_table,\n\t.prepare = amdgpu_vm_sdma_prepare,\n\t.update = amdgpu_vm_sdma_update,\n\t.commit = amdgpu_vm_sdma_commit\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}