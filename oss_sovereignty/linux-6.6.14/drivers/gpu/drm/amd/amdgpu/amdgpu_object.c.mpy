{
  "module_name": "amdgpu_object.c",
  "hash_id": "ebe0cf776a1bbfe6c524722e46df17ed97a75290b7bad6ee35cbe3d4c016bbde",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c",
  "human_readable_source": " \n \n#include <linux/list.h>\n#include <linux/slab.h>\n#include <linux/dma-buf.h>\n\n#include <drm/drm_drv.h>\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_cache.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_amdkfd.h\"\n\n \n\nstatic void amdgpu_bo_destroy(struct ttm_buffer_object *tbo)\n{\n\tstruct amdgpu_bo *bo = ttm_to_amdgpu_bo(tbo);\n\n\tamdgpu_bo_kunmap(bo);\n\n\tif (bo->tbo.base.import_attach)\n\t\tdrm_prime_gem_destroy(&bo->tbo.base, bo->tbo.sg);\n\tdrm_gem_object_release(&bo->tbo.base);\n\tamdgpu_bo_unref(&bo->parent);\n\tkvfree(bo);\n}\n\nstatic void amdgpu_bo_user_destroy(struct ttm_buffer_object *tbo)\n{\n\tstruct amdgpu_bo *bo = ttm_to_amdgpu_bo(tbo);\n\tstruct amdgpu_bo_user *ubo;\n\n\tubo = to_amdgpu_bo_user(bo);\n\tkfree(ubo->metadata);\n\tamdgpu_bo_destroy(tbo);\n}\n\nstatic void amdgpu_bo_vm_destroy(struct ttm_buffer_object *tbo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(tbo->bdev);\n\tstruct amdgpu_bo *shadow_bo = ttm_to_amdgpu_bo(tbo), *bo;\n\tstruct amdgpu_bo_vm *vmbo;\n\n\tbo = shadow_bo->parent;\n\tvmbo = to_amdgpu_bo_vm(bo);\n\t \n\tif (!list_empty(&vmbo->shadow_list)) {\n\t\tmutex_lock(&adev->shadow_list_lock);\n\t\tlist_del_init(&vmbo->shadow_list);\n\t\tmutex_unlock(&adev->shadow_list_lock);\n\t}\n\n\tamdgpu_bo_destroy(tbo);\n}\n\n \nbool amdgpu_bo_is_amdgpu_bo(struct ttm_buffer_object *bo)\n{\n\tif (bo->destroy == &amdgpu_bo_destroy ||\n\t    bo->destroy == &amdgpu_bo_user_destroy ||\n\t    bo->destroy == &amdgpu_bo_vm_destroy)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nvoid amdgpu_bo_placement_from_domain(struct amdgpu_bo *abo, u32 domain)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(abo->tbo.bdev);\n\tstruct ttm_placement *placement = &abo->placement;\n\tstruct ttm_place *places = abo->placements;\n\tu64 flags = abo->flags;\n\tu32 c = 0;\n\n\tif (domain & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\tunsigned int visible_pfn = adev->gmc.visible_vram_size >> PAGE_SHIFT;\n\t\tint8_t mem_id = KFD_XCP_MEM_ID(adev, abo->xcp_id);\n\n\t\tif (adev->gmc.mem_partitions && mem_id >= 0) {\n\t\t\tplaces[c].fpfn = adev->gmc.mem_partitions[mem_id].range.fpfn;\n\t\t\t \n\t\t\tplaces[c].lpfn = adev->gmc.mem_partitions[mem_id].range.lpfn + 1;\n\t\t} else {\n\t\t\tplaces[c].fpfn = 0;\n\t\t\tplaces[c].lpfn = 0;\n\t\t}\n\t\tplaces[c].mem_type = TTM_PL_VRAM;\n\t\tplaces[c].flags = 0;\n\n\t\tif (flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)\n\t\t\tplaces[c].lpfn = min_not_zero(places[c].lpfn, visible_pfn);\n\t\telse\n\t\t\tplaces[c].flags |= TTM_PL_FLAG_TOPDOWN;\n\n\t\tif (flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS)\n\t\t\tplaces[c].flags |= TTM_PL_FLAG_CONTIGUOUS;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_DOORBELL) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = AMDGPU_PL_DOORBELL;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_GTT) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type =\n\t\t\tabo->flags & AMDGPU_GEM_CREATE_PREEMPTIBLE ?\n\t\t\tAMDGPU_PL_PREEMPT : TTM_PL_TT;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_CPU) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = TTM_PL_SYSTEM;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_GDS) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = AMDGPU_PL_GDS;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_GWS) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = AMDGPU_PL_GWS;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (domain & AMDGPU_GEM_DOMAIN_OA) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = AMDGPU_PL_OA;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tif (!c) {\n\t\tplaces[c].fpfn = 0;\n\t\tplaces[c].lpfn = 0;\n\t\tplaces[c].mem_type = TTM_PL_SYSTEM;\n\t\tplaces[c].flags = 0;\n\t\tc++;\n\t}\n\n\tBUG_ON(c > AMDGPU_BO_MAX_PLACEMENTS);\n\n\tplacement->num_placement = c;\n\tplacement->placement = places;\n\n\tplacement->num_busy_placement = c;\n\tplacement->busy_placement = places;\n}\n\n \nint amdgpu_bo_create_reserved(struct amdgpu_device *adev,\n\t\t\t      unsigned long size, int align,\n\t\t\t      u32 domain, struct amdgpu_bo **bo_ptr,\n\t\t\t      u64 *gpu_addr, void **cpu_addr)\n{\n\tstruct amdgpu_bo_param bp;\n\tbool free = false;\n\tint r;\n\n\tif (!size) {\n\t\tamdgpu_bo_unref(bo_ptr);\n\t\treturn 0;\n\t}\n\n\tmemset(&bp, 0, sizeof(bp));\n\tbp.size = size;\n\tbp.byte_align = align;\n\tbp.domain = domain;\n\tbp.flags = cpu_addr ? AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED\n\t\t: AMDGPU_GEM_CREATE_NO_CPU_ACCESS;\n\tbp.flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\tbp.type = ttm_bo_type_kernel;\n\tbp.resv = NULL;\n\tbp.bo_ptr_size = sizeof(struct amdgpu_bo);\n\n\tif (!*bo_ptr) {\n\t\tr = amdgpu_bo_create(adev, &bp, bo_ptr);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%d) failed to allocate kernel bo\\n\",\n\t\t\t\tr);\n\t\t\treturn r;\n\t\t}\n\t\tfree = true;\n\t}\n\n\tr = amdgpu_bo_reserve(*bo_ptr, false);\n\tif (r) {\n\t\tdev_err(adev->dev, \"(%d) failed to reserve kernel bo\\n\", r);\n\t\tgoto error_free;\n\t}\n\n\tr = amdgpu_bo_pin(*bo_ptr, domain);\n\tif (r) {\n\t\tdev_err(adev->dev, \"(%d) kernel bo pin failed\\n\", r);\n\t\tgoto error_unreserve;\n\t}\n\n\tr = amdgpu_ttm_alloc_gart(&(*bo_ptr)->tbo);\n\tif (r) {\n\t\tdev_err(adev->dev, \"%p bind failed\\n\", *bo_ptr);\n\t\tgoto error_unpin;\n\t}\n\n\tif (gpu_addr)\n\t\t*gpu_addr = amdgpu_bo_gpu_offset(*bo_ptr);\n\n\tif (cpu_addr) {\n\t\tr = amdgpu_bo_kmap(*bo_ptr, cpu_addr);\n\t\tif (r) {\n\t\t\tdev_err(adev->dev, \"(%d) kernel bo map failed\\n\", r);\n\t\t\tgoto error_unpin;\n\t\t}\n\t}\n\n\treturn 0;\n\nerror_unpin:\n\tamdgpu_bo_unpin(*bo_ptr);\nerror_unreserve:\n\tamdgpu_bo_unreserve(*bo_ptr);\n\nerror_free:\n\tif (free)\n\t\tamdgpu_bo_unref(bo_ptr);\n\n\treturn r;\n}\n\n \nint amdgpu_bo_create_kernel(struct amdgpu_device *adev,\n\t\t\t    unsigned long size, int align,\n\t\t\t    u32 domain, struct amdgpu_bo **bo_ptr,\n\t\t\t    u64 *gpu_addr, void **cpu_addr)\n{\n\tint r;\n\n\tr = amdgpu_bo_create_reserved(adev, size, align, domain, bo_ptr,\n\t\t\t\t      gpu_addr, cpu_addr);\n\n\tif (r)\n\t\treturn r;\n\n\tif (*bo_ptr)\n\t\tamdgpu_bo_unreserve(*bo_ptr);\n\n\treturn 0;\n}\n\n \nint amdgpu_bo_create_kernel_at(struct amdgpu_device *adev,\n\t\t\t       uint64_t offset, uint64_t size,\n\t\t\t       struct amdgpu_bo **bo_ptr, void **cpu_addr)\n{\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tunsigned int i;\n\tint r;\n\n\toffset &= PAGE_MASK;\n\tsize = ALIGN(size, PAGE_SIZE);\n\n\tr = amdgpu_bo_create_reserved(adev, size, PAGE_SIZE,\n\t\t\t\t      AMDGPU_GEM_DOMAIN_VRAM, bo_ptr, NULL,\n\t\t\t\t      cpu_addr);\n\tif (r)\n\t\treturn r;\n\n\tif ((*bo_ptr) == NULL)\n\t\treturn 0;\n\n\t \n\tif (cpu_addr)\n\t\tamdgpu_bo_kunmap(*bo_ptr);\n\n\tttm_resource_free(&(*bo_ptr)->tbo, &(*bo_ptr)->tbo.resource);\n\n\tfor (i = 0; i < (*bo_ptr)->placement.num_placement; ++i) {\n\t\t(*bo_ptr)->placements[i].fpfn = offset >> PAGE_SHIFT;\n\t\t(*bo_ptr)->placements[i].lpfn = (offset + size) >> PAGE_SHIFT;\n\t}\n\tr = ttm_bo_mem_space(&(*bo_ptr)->tbo, &(*bo_ptr)->placement,\n\t\t\t     &(*bo_ptr)->tbo.resource, &ctx);\n\tif (r)\n\t\tgoto error;\n\n\tif (cpu_addr) {\n\t\tr = amdgpu_bo_kmap(*bo_ptr, cpu_addr);\n\t\tif (r)\n\t\t\tgoto error;\n\t}\n\n\tamdgpu_bo_unreserve(*bo_ptr);\n\treturn 0;\n\nerror:\n\tamdgpu_bo_unreserve(*bo_ptr);\n\tamdgpu_bo_unref(bo_ptr);\n\treturn r;\n}\n\n \nvoid amdgpu_bo_free_kernel(struct amdgpu_bo **bo, u64 *gpu_addr,\n\t\t\t   void **cpu_addr)\n{\n\tif (*bo == NULL)\n\t\treturn;\n\n\tWARN_ON(amdgpu_ttm_adev((*bo)->tbo.bdev)->in_suspend);\n\n\tif (likely(amdgpu_bo_reserve(*bo, true) == 0)) {\n\t\tif (cpu_addr)\n\t\t\tamdgpu_bo_kunmap(*bo);\n\n\t\tamdgpu_bo_unpin(*bo);\n\t\tamdgpu_bo_unreserve(*bo);\n\t}\n\tamdgpu_bo_unref(bo);\n\n\tif (gpu_addr)\n\t\t*gpu_addr = 0;\n\n\tif (cpu_addr)\n\t\t*cpu_addr = NULL;\n}\n\n \nstatic bool amdgpu_bo_validate_size(struct amdgpu_device *adev,\n\t\t\t\t\t  unsigned long size, u32 domain)\n{\n\tstruct ttm_resource_manager *man = NULL;\n\n\t \n\tif (domain & AMDGPU_GEM_DOMAIN_GTT) {\n\t\tman = ttm_manager_type(&adev->mman.bdev, TTM_PL_TT);\n\n\t\tif (man && size < man->size)\n\t\t\treturn true;\n\t\telse if (!man)\n\t\t\tWARN_ON_ONCE(\"GTT domain requested but GTT mem manager uninitialized\");\n\t\tgoto fail;\n\t} else if (domain & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\tman = ttm_manager_type(&adev->mman.bdev, TTM_PL_VRAM);\n\n\t\tif (man && size < man->size)\n\t\t\treturn true;\n\t\tgoto fail;\n\t}\n\n\t \n\treturn true;\n\nfail:\n\tif (man)\n\t\tDRM_DEBUG(\"BO size %lu > total memory in domain: %llu\\n\", size,\n\t\t\t  man->size);\n\treturn false;\n}\n\nbool amdgpu_bo_support_uswc(u64 bo_flags)\n{\n\n#ifdef CONFIG_X86_32\n\t \n\treturn false;\n#elif defined(CONFIG_X86) && !defined(CONFIG_X86_PAT)\n\t \n\n#ifndef CONFIG_COMPILE_TEST\n#warning Please enable CONFIG_MTRR and CONFIG_X86_PAT for better performance \\\n\t thanks to write-combining\n#endif\n\n\tif (bo_flags & AMDGPU_GEM_CREATE_CPU_GTT_USWC)\n\t\tDRM_INFO_ONCE(\"Please enable CONFIG_MTRR and CONFIG_X86_PAT for \"\n\t\t\t      \"better performance thanks to write-combining\\n\");\n\treturn false;\n#else\n\t \n\tif (!drm_arch_can_wc_memory())\n\t\treturn false;\n\n\treturn true;\n#endif\n}\n\n \nint amdgpu_bo_create(struct amdgpu_device *adev,\n\t\t\t       struct amdgpu_bo_param *bp,\n\t\t\t       struct amdgpu_bo **bo_ptr)\n{\n\tstruct ttm_operation_ctx ctx = {\n\t\t.interruptible = (bp->type != ttm_bo_type_kernel),\n\t\t.no_wait_gpu = bp->no_wait_gpu,\n\t\t \n\t\t.gfp_retry_mayfail = true,\n\t\t.allow_res_evict = bp->type != ttm_bo_type_kernel,\n\t\t.resv = bp->resv\n\t};\n\tstruct amdgpu_bo *bo;\n\tunsigned long page_align, size = bp->size;\n\tint r;\n\n\t \n\tif (bp->domain & (AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA)) {\n\t\t \n\t\tpage_align = bp->byte_align;\n\t\tsize <<= PAGE_SHIFT;\n\n\t} else if (bp->domain & AMDGPU_GEM_DOMAIN_GDS) {\n\t\t \n\t\tpage_align = ALIGN(bp->byte_align, 4);\n\t\tsize = ALIGN(size, 4) << PAGE_SHIFT;\n\t} else {\n\t\t \n\t\tpage_align = ALIGN(bp->byte_align, PAGE_SIZE) >> PAGE_SHIFT;\n\t\tsize = ALIGN(size, PAGE_SIZE);\n\t}\n\n\tif (!amdgpu_bo_validate_size(adev, size, bp->domain))\n\t\treturn -ENOMEM;\n\n\tBUG_ON(bp->bo_ptr_size < sizeof(struct amdgpu_bo));\n\n\t*bo_ptr = NULL;\n\tbo = kvzalloc(bp->bo_ptr_size, GFP_KERNEL);\n\tif (bo == NULL)\n\t\treturn -ENOMEM;\n\tdrm_gem_private_object_init(adev_to_drm(adev), &bo->tbo.base, size);\n\tbo->vm_bo = NULL;\n\tbo->preferred_domains = bp->preferred_domain ? bp->preferred_domain :\n\t\tbp->domain;\n\tbo->allowed_domains = bo->preferred_domains;\n\tif (bp->type != ttm_bo_type_kernel &&\n\t    !(bp->flags & AMDGPU_GEM_CREATE_DISCARDABLE) &&\n\t    bo->allowed_domains == AMDGPU_GEM_DOMAIN_VRAM)\n\t\tbo->allowed_domains |= AMDGPU_GEM_DOMAIN_GTT;\n\n\tbo->flags = bp->flags;\n\n\tif (adev->gmc.mem_partitions)\n\t\t \n\t\tbo->xcp_id = bp->xcp_id_plus1 - 1;\n\telse\n\t\t \n\t\tbo->xcp_id = 0;\n\n\tif (!amdgpu_bo_support_uswc(bo->flags))\n\t\tbo->flags &= ~AMDGPU_GEM_CREATE_CPU_GTT_USWC;\n\n\tif (adev->ras_enabled)\n\t\tbo->flags |= AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE;\n\n\tbo->tbo.bdev = &adev->mman.bdev;\n\tif (bp->domain & (AMDGPU_GEM_DOMAIN_GWS | AMDGPU_GEM_DOMAIN_OA |\n\t\t\t  AMDGPU_GEM_DOMAIN_GDS))\n\t\tamdgpu_bo_placement_from_domain(bo, AMDGPU_GEM_DOMAIN_CPU);\n\telse\n\t\tamdgpu_bo_placement_from_domain(bo, bp->domain);\n\tif (bp->type == ttm_bo_type_kernel)\n\t\tbo->tbo.priority = 1;\n\n\tif (!bp->destroy)\n\t\tbp->destroy = &amdgpu_bo_destroy;\n\n\tr = ttm_bo_init_reserved(&adev->mman.bdev, &bo->tbo, bp->type,\n\t\t\t\t &bo->placement, page_align, &ctx,  NULL,\n\t\t\t\t bp->resv, bp->destroy);\n\tif (unlikely(r != 0))\n\t\treturn r;\n\n\tif (!amdgpu_gmc_vram_full_visible(&adev->gmc) &&\n\t    bo->tbo.resource->mem_type == TTM_PL_VRAM &&\n\t    amdgpu_bo_in_cpu_visible_vram(bo))\n\t\tamdgpu_cs_report_moved_bytes(adev, ctx.bytes_moved,\n\t\t\t\t\t     ctx.bytes_moved);\n\telse\n\t\tamdgpu_cs_report_moved_bytes(adev, ctx.bytes_moved, 0);\n\n\tif (bp->flags & AMDGPU_GEM_CREATE_VRAM_CLEARED &&\n\t    bo->tbo.resource->mem_type == TTM_PL_VRAM) {\n\t\tstruct dma_fence *fence;\n\n\t\tr = amdgpu_fill_buffer(bo, 0, bo->tbo.base.resv, &fence, true);\n\t\tif (unlikely(r))\n\t\t\tgoto fail_unreserve;\n\n\t\tdma_resv_add_fence(bo->tbo.base.resv, fence,\n\t\t\t\t   DMA_RESV_USAGE_KERNEL);\n\t\tdma_fence_put(fence);\n\t}\n\tif (!bp->resv)\n\t\tamdgpu_bo_unreserve(bo);\n\t*bo_ptr = bo;\n\n\ttrace_amdgpu_bo_create(bo);\n\n\t \n\tif (bp->type == ttm_bo_type_device)\n\t\tbo->flags &= ~AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\n\treturn 0;\n\nfail_unreserve:\n\tif (!bp->resv)\n\t\tdma_resv_unlock(bo->tbo.base.resv);\n\tamdgpu_bo_unref(&bo);\n\treturn r;\n}\n\n \n\nint amdgpu_bo_create_user(struct amdgpu_device *adev,\n\t\t\t  struct amdgpu_bo_param *bp,\n\t\t\t  struct amdgpu_bo_user **ubo_ptr)\n{\n\tstruct amdgpu_bo *bo_ptr;\n\tint r;\n\n\tbp->bo_ptr_size = sizeof(struct amdgpu_bo_user);\n\tbp->destroy = &amdgpu_bo_user_destroy;\n\tr = amdgpu_bo_create(adev, bp, &bo_ptr);\n\tif (r)\n\t\treturn r;\n\n\t*ubo_ptr = to_amdgpu_bo_user(bo_ptr);\n\treturn r;\n}\n\n \n\nint amdgpu_bo_create_vm(struct amdgpu_device *adev,\n\t\t\tstruct amdgpu_bo_param *bp,\n\t\t\tstruct amdgpu_bo_vm **vmbo_ptr)\n{\n\tstruct amdgpu_bo *bo_ptr;\n\tint r;\n\n\t \n\tBUG_ON(bp->bo_ptr_size < sizeof(struct amdgpu_bo_vm));\n\tr = amdgpu_bo_create(adev, bp, &bo_ptr);\n\tif (r)\n\t\treturn r;\n\n\t*vmbo_ptr = to_amdgpu_bo_vm(bo_ptr);\n\treturn r;\n}\n\n \nvoid amdgpu_bo_add_to_shadow_list(struct amdgpu_bo_vm *vmbo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(vmbo->bo.tbo.bdev);\n\n\tmutex_lock(&adev->shadow_list_lock);\n\tlist_add_tail(&vmbo->shadow_list, &adev->shadow_list);\n\tvmbo->shadow->parent = amdgpu_bo_ref(&vmbo->bo);\n\tvmbo->shadow->tbo.destroy = &amdgpu_bo_vm_destroy;\n\tmutex_unlock(&adev->shadow_list_lock);\n}\n\n \nint amdgpu_bo_restore_shadow(struct amdgpu_bo *shadow, struct dma_fence **fence)\n\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(shadow->tbo.bdev);\n\tstruct amdgpu_ring *ring = adev->mman.buffer_funcs_ring;\n\tuint64_t shadow_addr, parent_addr;\n\n\tshadow_addr = amdgpu_bo_gpu_offset(shadow);\n\tparent_addr = amdgpu_bo_gpu_offset(shadow->parent);\n\n\treturn amdgpu_copy_buffer(ring, shadow_addr, parent_addr,\n\t\t\t\t  amdgpu_bo_size(shadow), NULL, fence,\n\t\t\t\t  true, false, false);\n}\n\n \nint amdgpu_bo_kmap(struct amdgpu_bo *bo, void **ptr)\n{\n\tvoid *kptr;\n\tlong r;\n\n\tif (bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS)\n\t\treturn -EPERM;\n\n\tr = dma_resv_wait_timeout(bo->tbo.base.resv, DMA_RESV_USAGE_KERNEL,\n\t\t\t\t  false, MAX_SCHEDULE_TIMEOUT);\n\tif (r < 0)\n\t\treturn r;\n\n\tkptr = amdgpu_bo_kptr(bo);\n\tif (kptr) {\n\t\tif (ptr)\n\t\t\t*ptr = kptr;\n\t\treturn 0;\n\t}\n\n\tr = ttm_bo_kmap(&bo->tbo, 0, PFN_UP(bo->tbo.base.size), &bo->kmap);\n\tif (r)\n\t\treturn r;\n\n\tif (ptr)\n\t\t*ptr = amdgpu_bo_kptr(bo);\n\n\treturn 0;\n}\n\n \nvoid *amdgpu_bo_kptr(struct amdgpu_bo *bo)\n{\n\tbool is_iomem;\n\n\treturn ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\n}\n\n \nvoid amdgpu_bo_kunmap(struct amdgpu_bo *bo)\n{\n\tif (bo->kmap.bo)\n\t\tttm_bo_kunmap(&bo->kmap);\n}\n\n \nstruct amdgpu_bo *amdgpu_bo_ref(struct amdgpu_bo *bo)\n{\n\tif (bo == NULL)\n\t\treturn NULL;\n\n\tttm_bo_get(&bo->tbo);\n\treturn bo;\n}\n\n \nvoid amdgpu_bo_unref(struct amdgpu_bo **bo)\n{\n\tstruct ttm_buffer_object *tbo;\n\n\tif ((*bo) == NULL)\n\t\treturn;\n\n\ttbo = &((*bo)->tbo);\n\tttm_bo_put(tbo);\n\t*bo = NULL;\n}\n\n \nint amdgpu_bo_pin_restricted(struct amdgpu_bo *bo, u32 domain,\n\t\t\t     u64 min_offset, u64 max_offset)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tint r, i;\n\n\tif (amdgpu_ttm_tt_get_usermm(bo->tbo.ttm))\n\t\treturn -EPERM;\n\n\tif (WARN_ON_ONCE(min_offset > max_offset))\n\t\treturn -EINVAL;\n\n\t \n\tif (bo->preferred_domains & domain)\n\t\tdomain = bo->preferred_domains & domain;\n\n\t \n\tif (bo->tbo.base.import_attach) {\n\t\tif (domain & AMDGPU_GEM_DOMAIN_GTT)\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (bo->tbo.pin_count) {\n\t\tuint32_t mem_type = bo->tbo.resource->mem_type;\n\t\tuint32_t mem_flags = bo->tbo.resource->placement;\n\n\t\tif (!(domain & amdgpu_mem_type_to_domain(mem_type)))\n\t\t\treturn -EINVAL;\n\n\t\tif ((mem_type == TTM_PL_VRAM) &&\n\t\t    (bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS) &&\n\t\t    !(mem_flags & TTM_PL_FLAG_CONTIGUOUS))\n\t\t\treturn -EINVAL;\n\n\t\tttm_bo_pin(&bo->tbo);\n\n\t\tif (max_offset != 0) {\n\t\t\tu64 domain_start = amdgpu_ttm_domain_start(adev,\n\t\t\t\t\t\t\t\t   mem_type);\n\t\t\tWARN_ON_ONCE(max_offset <\n\t\t\t\t     (amdgpu_bo_gpu_offset(bo) - domain_start));\n\t\t}\n\n\t\treturn 0;\n\t}\n\n\t \n\tdomain = amdgpu_bo_get_preferred_domain(adev, domain);\n\n\tif (bo->tbo.base.import_attach)\n\t\tdma_buf_pin(bo->tbo.base.import_attach);\n\n\t \n\tif (!(bo->flags & AMDGPU_GEM_CREATE_NO_CPU_ACCESS))\n\t\tbo->flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\tamdgpu_bo_placement_from_domain(bo, domain);\n\tfor (i = 0; i < bo->placement.num_placement; i++) {\n\t\tunsigned int fpfn, lpfn;\n\n\t\tfpfn = min_offset >> PAGE_SHIFT;\n\t\tlpfn = max_offset >> PAGE_SHIFT;\n\n\t\tif (fpfn > bo->placements[i].fpfn)\n\t\t\tbo->placements[i].fpfn = fpfn;\n\t\tif (!bo->placements[i].lpfn ||\n\t\t    (lpfn && lpfn < bo->placements[i].lpfn))\n\t\t\tbo->placements[i].lpfn = lpfn;\n\t}\n\n\tr = ttm_bo_validate(&bo->tbo, &bo->placement, &ctx);\n\tif (unlikely(r)) {\n\t\tdev_err(adev->dev, \"%p pin failed\\n\", bo);\n\t\tgoto error;\n\t}\n\n\tttm_bo_pin(&bo->tbo);\n\n\tdomain = amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type);\n\tif (domain == AMDGPU_GEM_DOMAIN_VRAM) {\n\t\tatomic64_add(amdgpu_bo_size(bo), &adev->vram_pin_size);\n\t\tatomic64_add(amdgpu_vram_mgr_bo_visible_size(bo),\n\t\t\t     &adev->visible_pin_size);\n\t} else if (domain == AMDGPU_GEM_DOMAIN_GTT) {\n\t\tatomic64_add(amdgpu_bo_size(bo), &adev->gart_pin_size);\n\t}\n\nerror:\n\treturn r;\n}\n\n \nint amdgpu_bo_pin(struct amdgpu_bo *bo, u32 domain)\n{\n\tbo->flags |= AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS;\n\treturn amdgpu_bo_pin_restricted(bo, domain, 0, 0);\n}\n\n \nvoid amdgpu_bo_unpin(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\n\tttm_bo_unpin(&bo->tbo);\n\tif (bo->tbo.pin_count)\n\t\treturn;\n\n\tif (bo->tbo.base.import_attach)\n\t\tdma_buf_unpin(bo->tbo.base.import_attach);\n\n\tif (bo->tbo.resource->mem_type == TTM_PL_VRAM) {\n\t\tatomic64_sub(amdgpu_bo_size(bo), &adev->vram_pin_size);\n\t\tatomic64_sub(amdgpu_vram_mgr_bo_visible_size(bo),\n\t\t\t     &adev->visible_pin_size);\n\t} else if (bo->tbo.resource->mem_type == TTM_PL_TT) {\n\t\tatomic64_sub(amdgpu_bo_size(bo), &adev->gart_pin_size);\n\t}\n\n}\n\nstatic const char * const amdgpu_vram_names[] = {\n\t\"UNKNOWN\",\n\t\"GDDR1\",\n\t\"DDR2\",\n\t\"GDDR3\",\n\t\"GDDR4\",\n\t\"GDDR5\",\n\t\"HBM\",\n\t\"DDR3\",\n\t\"DDR4\",\n\t\"GDDR6\",\n\t\"DDR5\",\n\t\"LPDDR4\",\n\t\"LPDDR5\"\n};\n\n \nint amdgpu_bo_init(struct amdgpu_device *adev)\n{\n\t \n\tif (!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu) {\n\t\t \n\t\tint r = arch_io_reserve_memtype_wc(adev->gmc.aper_base,\n\t\t\t\tadev->gmc.aper_size);\n\n\t\tif (r) {\n\t\t\tDRM_ERROR(\"Unable to set WC memtype for the aperture base\\n\");\n\t\t\treturn r;\n\t\t}\n\n\t\t \n\t\tadev->gmc.vram_mtrr = arch_phys_wc_add(adev->gmc.aper_base,\n\t\t\t\tadev->gmc.aper_size);\n\t}\n\n\tDRM_INFO(\"Detected VRAM RAM=%lluM, BAR=%lluM\\n\",\n\t\t adev->gmc.mc_vram_size >> 20,\n\t\t (unsigned long long)adev->gmc.aper_size >> 20);\n\tDRM_INFO(\"RAM width %dbits %s\\n\",\n\t\t adev->gmc.vram_width, amdgpu_vram_names[adev->gmc.vram_type]);\n\treturn amdgpu_ttm_init(adev);\n}\n\n \nvoid amdgpu_bo_fini(struct amdgpu_device *adev)\n{\n\tint idx;\n\n\tamdgpu_ttm_fini(adev);\n\n\tif (drm_dev_enter(adev_to_drm(adev), &idx)) {\n\t\tif (!adev->gmc.xgmi.connected_to_cpu && !adev->gmc.is_app_apu) {\n\t\t\tarch_phys_wc_del(adev->gmc.vram_mtrr);\n\t\t\tarch_io_free_memtype_wc(adev->gmc.aper_base, adev->gmc.aper_size);\n\t\t}\n\t\tdrm_dev_exit(idx);\n\t}\n}\n\n \nint amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo, u64 tiling_flags)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tstruct amdgpu_bo_user *ubo;\n\n\tBUG_ON(bo->tbo.type == ttm_bo_type_kernel);\n\tif (adev->family <= AMDGPU_FAMILY_CZ &&\n\t    AMDGPU_TILING_GET(tiling_flags, TILE_SPLIT) > 6)\n\t\treturn -EINVAL;\n\n\tubo = to_amdgpu_bo_user(bo);\n\tubo->tiling_flags = tiling_flags;\n\treturn 0;\n}\n\n \nvoid amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo, u64 *tiling_flags)\n{\n\tstruct amdgpu_bo_user *ubo;\n\n\tBUG_ON(bo->tbo.type == ttm_bo_type_kernel);\n\tdma_resv_assert_held(bo->tbo.base.resv);\n\tubo = to_amdgpu_bo_user(bo);\n\n\tif (tiling_flags)\n\t\t*tiling_flags = ubo->tiling_flags;\n}\n\n \nint amdgpu_bo_set_metadata(struct amdgpu_bo *bo, void *metadata,\n\t\t\t   u32 metadata_size, uint64_t flags)\n{\n\tstruct amdgpu_bo_user *ubo;\n\tvoid *buffer;\n\n\tBUG_ON(bo->tbo.type == ttm_bo_type_kernel);\n\tubo = to_amdgpu_bo_user(bo);\n\tif (!metadata_size) {\n\t\tif (ubo->metadata_size) {\n\t\t\tkfree(ubo->metadata);\n\t\t\tubo->metadata = NULL;\n\t\t\tubo->metadata_size = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (metadata == NULL)\n\t\treturn -EINVAL;\n\n\tbuffer = kmemdup(metadata, metadata_size, GFP_KERNEL);\n\tif (buffer == NULL)\n\t\treturn -ENOMEM;\n\n\tkfree(ubo->metadata);\n\tubo->metadata_flags = flags;\n\tubo->metadata = buffer;\n\tubo->metadata_size = metadata_size;\n\n\treturn 0;\n}\n\n \nint amdgpu_bo_get_metadata(struct amdgpu_bo *bo, void *buffer,\n\t\t\t   size_t buffer_size, uint32_t *metadata_size,\n\t\t\t   uint64_t *flags)\n{\n\tstruct amdgpu_bo_user *ubo;\n\n\tif (!buffer && !metadata_size)\n\t\treturn -EINVAL;\n\n\tBUG_ON(bo->tbo.type == ttm_bo_type_kernel);\n\tubo = to_amdgpu_bo_user(bo);\n\tif (metadata_size)\n\t\t*metadata_size = ubo->metadata_size;\n\n\tif (buffer) {\n\t\tif (buffer_size < ubo->metadata_size)\n\t\t\treturn -EINVAL;\n\n\t\tif (ubo->metadata_size)\n\t\t\tmemcpy(buffer, ubo->metadata, ubo->metadata_size);\n\t}\n\n\tif (flags)\n\t\t*flags = ubo->metadata_flags;\n\n\treturn 0;\n}\n\n \nvoid amdgpu_bo_move_notify(struct ttm_buffer_object *bo,\n\t\t\t   bool evict,\n\t\t\t   struct ttm_resource *new_mem)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct amdgpu_bo *abo;\n\tstruct ttm_resource *old_mem = bo->resource;\n\n\tif (!amdgpu_bo_is_amdgpu_bo(bo))\n\t\treturn;\n\n\tabo = ttm_to_amdgpu_bo(bo);\n\tamdgpu_vm_bo_invalidate(adev, abo, evict);\n\n\tamdgpu_bo_kunmap(abo);\n\n\tif (abo->tbo.base.dma_buf && !abo->tbo.base.import_attach &&\n\t    bo->resource->mem_type != TTM_PL_SYSTEM)\n\t\tdma_buf_move_notify(abo->tbo.base.dma_buf);\n\n\t \n\tif (evict)\n\t\tatomic64_inc(&adev->num_evictions);\n\n\t \n\tif (!new_mem)\n\t\treturn;\n\n\t \n\ttrace_amdgpu_bo_move(abo, new_mem->mem_type, old_mem->mem_type);\n}\n\nvoid amdgpu_bo_get_memory(struct amdgpu_bo *bo,\n\t\t\t  struct amdgpu_mem_stats *stats)\n{\n\tuint64_t size = amdgpu_bo_size(bo);\n\tunsigned int domain;\n\n\t \n\tif (!bo->tbo.resource)\n\t\treturn;\n\n\tdomain = amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type);\n\tswitch (domain) {\n\tcase AMDGPU_GEM_DOMAIN_VRAM:\n\t\tstats->vram += size;\n\t\tif (amdgpu_bo_in_cpu_visible_vram(bo))\n\t\t\tstats->visible_vram += size;\n\t\tbreak;\n\tcase AMDGPU_GEM_DOMAIN_GTT:\n\t\tstats->gtt += size;\n\t\tbreak;\n\tcase AMDGPU_GEM_DOMAIN_CPU:\n\tdefault:\n\t\tstats->cpu += size;\n\t\tbreak;\n\t}\n\n\tif (bo->preferred_domains & AMDGPU_GEM_DOMAIN_VRAM) {\n\t\tstats->requested_vram += size;\n\t\tif (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)\n\t\t\tstats->requested_visible_vram += size;\n\n\t\tif (domain != AMDGPU_GEM_DOMAIN_VRAM) {\n\t\t\tstats->evicted_vram += size;\n\t\t\tif (bo->flags & AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED)\n\t\t\t\tstats->evicted_visible_vram += size;\n\t\t}\n\t} else if (bo->preferred_domains & AMDGPU_GEM_DOMAIN_GTT) {\n\t\tstats->requested_gtt += size;\n\t}\n}\n\n \nvoid amdgpu_bo_release_notify(struct ttm_buffer_object *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct dma_fence *fence = NULL;\n\tstruct amdgpu_bo *abo;\n\tint r;\n\n\tif (!amdgpu_bo_is_amdgpu_bo(bo))\n\t\treturn;\n\n\tabo = ttm_to_amdgpu_bo(bo);\n\n\tif (abo->kfd_bo)\n\t\tamdgpu_amdkfd_release_notify(abo);\n\n\t \n\tWARN_ON_ONCE(bo->type == ttm_bo_type_kernel\n\t\t\t&& bo->base.resv != &bo->base._resv);\n\tif (bo->base.resv == &bo->base._resv)\n\t\tamdgpu_amdkfd_remove_fence_on_pt_pd_bos(abo);\n\n\tif (!bo->resource || bo->resource->mem_type != TTM_PL_VRAM ||\n\t    !(abo->flags & AMDGPU_GEM_CREATE_VRAM_WIPE_ON_RELEASE) ||\n\t    adev->in_suspend || drm_dev_is_unplugged(adev_to_drm(adev)))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!dma_resv_trylock(bo->base.resv)))\n\t\treturn;\n\n\tr = amdgpu_fill_buffer(abo, AMDGPU_POISON, bo->base.resv, &fence, true);\n\tif (!WARN_ON(r)) {\n\t\tamdgpu_bo_fence(abo, fence, false);\n\t\tdma_fence_put(fence);\n\t}\n\n\tdma_resv_unlock(bo->base.resv);\n}\n\n \nvm_fault_t amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->bdev);\n\tstruct ttm_operation_ctx ctx = { false, false };\n\tstruct amdgpu_bo *abo = ttm_to_amdgpu_bo(bo);\n\tint r;\n\n\t \n\tabo->flags |= AMDGPU_GEM_CREATE_CPU_ACCESS_REQUIRED;\n\n\tif (bo->resource->mem_type != TTM_PL_VRAM)\n\t\treturn 0;\n\n\tif (amdgpu_bo_in_cpu_visible_vram(abo))\n\t\treturn 0;\n\n\t \n\tif (abo->tbo.pin_count > 0)\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tatomic64_inc(&adev->num_vram_cpu_page_faults);\n\tamdgpu_bo_placement_from_domain(abo, AMDGPU_GEM_DOMAIN_VRAM |\n\t\t\t\t\tAMDGPU_GEM_DOMAIN_GTT);\n\n\t \n\tabo->placement.num_busy_placement = 1;\n\tabo->placement.busy_placement = &abo->placements[1];\n\n\tr = ttm_bo_validate(bo, &abo->placement, &ctx);\n\tif (unlikely(r == -EBUSY || r == -ERESTARTSYS))\n\t\treturn VM_FAULT_NOPAGE;\n\telse if (unlikely(r))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tif (bo->resource->mem_type == TTM_PL_VRAM &&\n\t    !amdgpu_bo_in_cpu_visible_vram(abo))\n\t\treturn VM_FAULT_SIGBUS;\n\n\tttm_bo_move_to_lru_tail_unlocked(bo);\n\treturn 0;\n}\n\n \nvoid amdgpu_bo_fence(struct amdgpu_bo *bo, struct dma_fence *fence,\n\t\t     bool shared)\n{\n\tstruct dma_resv *resv = bo->tbo.base.resv;\n\tint r;\n\n\tr = dma_resv_reserve_fences(resv, 1);\n\tif (r) {\n\t\t \n\t\tdma_fence_wait(fence, false);\n\t\treturn;\n\t}\n\n\tdma_resv_add_fence(resv, fence, shared ? DMA_RESV_USAGE_READ :\n\t\t\t   DMA_RESV_USAGE_WRITE);\n}\n\n \nint amdgpu_bo_sync_wait_resv(struct amdgpu_device *adev, struct dma_resv *resv,\n\t\t\t     enum amdgpu_sync_mode sync_mode, void *owner,\n\t\t\t     bool intr)\n{\n\tstruct amdgpu_sync sync;\n\tint r;\n\n\tamdgpu_sync_create(&sync);\n\tamdgpu_sync_resv(adev, &sync, resv, sync_mode, owner);\n\tr = amdgpu_sync_wait(&sync, intr);\n\tamdgpu_sync_free(&sync);\n\treturn r;\n}\n\n \nint amdgpu_bo_sync_wait(struct amdgpu_bo *bo, void *owner, bool intr)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\n\treturn amdgpu_bo_sync_wait_resv(adev, bo->tbo.base.resv,\n\t\t\t\t\tAMDGPU_SYNC_NE_OWNER, owner, intr);\n}\n\n \nu64 amdgpu_bo_gpu_offset(struct amdgpu_bo *bo)\n{\n\tWARN_ON_ONCE(bo->tbo.resource->mem_type == TTM_PL_SYSTEM);\n\tWARN_ON_ONCE(!dma_resv_is_locked(bo->tbo.base.resv) &&\n\t\t     !bo->tbo.pin_count && bo->tbo.type != ttm_bo_type_kernel);\n\tWARN_ON_ONCE(bo->tbo.resource->start == AMDGPU_BO_INVALID_OFFSET);\n\tWARN_ON_ONCE(bo->tbo.resource->mem_type == TTM_PL_VRAM &&\n\t\t     !(bo->flags & AMDGPU_GEM_CREATE_VRAM_CONTIGUOUS));\n\n\treturn amdgpu_bo_gpu_offset_no_check(bo);\n}\n\n \nu64 amdgpu_bo_gpu_offset_no_check(struct amdgpu_bo *bo)\n{\n\tstruct amdgpu_device *adev = amdgpu_ttm_adev(bo->tbo.bdev);\n\tuint64_t offset;\n\n\toffset = (bo->tbo.resource->start << PAGE_SHIFT) +\n\t\t amdgpu_ttm_domain_start(adev, bo->tbo.resource->mem_type);\n\n\treturn amdgpu_gmc_sign_extend(offset);\n}\n\n \nuint32_t amdgpu_bo_get_preferred_domain(struct amdgpu_device *adev,\n\t\t\t\t\t    uint32_t domain)\n{\n\tif ((domain == (AMDGPU_GEM_DOMAIN_VRAM | AMDGPU_GEM_DOMAIN_GTT)) &&\n\t    ((adev->asic_type == CHIP_CARRIZO) || (adev->asic_type == CHIP_STONEY))) {\n\t\tdomain = AMDGPU_GEM_DOMAIN_VRAM;\n\t\tif (adev->gmc.real_vram_size <= AMDGPU_SG_THRESHOLD)\n\t\t\tdomain = AMDGPU_GEM_DOMAIN_GTT;\n\t}\n\treturn domain;\n}\n\n#if defined(CONFIG_DEBUG_FS)\n#define amdgpu_bo_print_flag(m, bo, flag)\t\t        \\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tif (bo->flags & (AMDGPU_GEM_CREATE_ ## flag)) {\t\\\n\t\t\tseq_printf((m), \" \" #flag);\t\t\\\n\t\t}\t\t\t\t\t\t\\\n\t} while (0)\n\n \nu64 amdgpu_bo_print_info(int id, struct amdgpu_bo *bo, struct seq_file *m)\n{\n\tstruct dma_buf_attachment *attachment;\n\tstruct dma_buf *dma_buf;\n\tconst char *placement;\n\tunsigned int pin_count;\n\tu64 size;\n\n\tif (dma_resv_trylock(bo->tbo.base.resv)) {\n\t\tunsigned int domain;\n\t\tdomain = amdgpu_mem_type_to_domain(bo->tbo.resource->mem_type);\n\t\tswitch (domain) {\n\t\tcase AMDGPU_GEM_DOMAIN_VRAM:\n\t\t\tif (amdgpu_bo_in_cpu_visible_vram(bo))\n\t\t\t\tplacement = \"VRAM VISIBLE\";\n\t\t\telse\n\t\t\t\tplacement = \"VRAM\";\n\t\t\tbreak;\n\t\tcase AMDGPU_GEM_DOMAIN_GTT:\n\t\t\tplacement = \"GTT\";\n\t\t\tbreak;\n\t\tcase AMDGPU_GEM_DOMAIN_CPU:\n\t\tdefault:\n\t\t\tplacement = \"CPU\";\n\t\t\tbreak;\n\t\t}\n\t\tdma_resv_unlock(bo->tbo.base.resv);\n\t} else {\n\t\tplacement = \"UNKNOWN\";\n\t}\n\n\tsize = amdgpu_bo_size(bo);\n\tseq_printf(m, \"\\t\\t0x%08x: %12lld byte %s\",\n\t\t\tid, size, placement);\n\n\tpin_count = READ_ONCE(bo->tbo.pin_count);\n\tif (pin_count)\n\t\tseq_printf(m, \" pin count %d\", pin_count);\n\n\tdma_buf = READ_ONCE(bo->tbo.base.dma_buf);\n\tattachment = READ_ONCE(bo->tbo.base.import_attach);\n\n\tif (attachment)\n\t\tseq_printf(m, \" imported from ino:%lu\", file_inode(dma_buf->file)->i_ino);\n\telse if (dma_buf)\n\t\tseq_printf(m, \" exported as ino:%lu\", file_inode(dma_buf->file)->i_ino);\n\n\tamdgpu_bo_print_flag(m, bo, CPU_ACCESS_REQUIRED);\n\tamdgpu_bo_print_flag(m, bo, NO_CPU_ACCESS);\n\tamdgpu_bo_print_flag(m, bo, CPU_GTT_USWC);\n\tamdgpu_bo_print_flag(m, bo, VRAM_CLEARED);\n\tamdgpu_bo_print_flag(m, bo, VRAM_CONTIGUOUS);\n\tamdgpu_bo_print_flag(m, bo, VM_ALWAYS_VALID);\n\tamdgpu_bo_print_flag(m, bo, EXPLICIT_SYNC);\n\n\tseq_puts(m, \"\\n\");\n\n\treturn size;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}