{
  "module_name": "amdgpu_ids.c",
  "hash_id": "2b9c414403b7d2eb0d42d4b8625547a74f8cd70ec19b8179b750259b8a22731e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ids.c",
  "human_readable_source": " \n#include \"amdgpu_ids.h\"\n\n#include <linux/idr.h>\n#include <linux/dma-fence-array.h>\n\n\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n\n \nstatic DEFINE_IDA(amdgpu_pasid_ida);\n\n \nstruct amdgpu_pasid_cb {\n\tstruct dma_fence_cb cb;\n\tu32 pasid;\n};\n\n \nint amdgpu_pasid_alloc(unsigned int bits)\n{\n\tint pasid = -EINVAL;\n\n\tfor (bits = min(bits, 31U); bits > 0; bits--) {\n\t\tpasid = ida_simple_get(&amdgpu_pasid_ida,\n\t\t\t\t       1U << (bits - 1), 1U << bits,\n\t\t\t\t       GFP_KERNEL);\n\t\tif (pasid != -ENOSPC)\n\t\t\tbreak;\n\t}\n\n\tif (pasid >= 0)\n\t\ttrace_amdgpu_pasid_allocated(pasid);\n\n\treturn pasid;\n}\n\n \nvoid amdgpu_pasid_free(u32 pasid)\n{\n\ttrace_amdgpu_pasid_freed(pasid);\n\tida_simple_remove(&amdgpu_pasid_ida, pasid);\n}\n\nstatic void amdgpu_pasid_free_cb(struct dma_fence *fence,\n\t\t\t\t struct dma_fence_cb *_cb)\n{\n\tstruct amdgpu_pasid_cb *cb =\n\t\tcontainer_of(_cb, struct amdgpu_pasid_cb, cb);\n\n\tamdgpu_pasid_free(cb->pasid);\n\tdma_fence_put(fence);\n\tkfree(cb);\n}\n\n \nvoid amdgpu_pasid_free_delayed(struct dma_resv *resv,\n\t\t\t       u32 pasid)\n{\n\tstruct amdgpu_pasid_cb *cb;\n\tstruct dma_fence *fence;\n\tint r;\n\n\tr = dma_resv_get_singleton(resv, DMA_RESV_USAGE_BOOKKEEP, &fence);\n\tif (r)\n\t\tgoto fallback;\n\n\tif (!fence) {\n\t\tamdgpu_pasid_free(pasid);\n\t\treturn;\n\t}\n\n\tcb = kmalloc(sizeof(*cb), GFP_KERNEL);\n\tif (!cb) {\n\t\t \n\t\tdma_fence_wait(fence, false);\n\t\tdma_fence_put(fence);\n\t\tamdgpu_pasid_free(pasid);\n\t} else {\n\t\tcb->pasid = pasid;\n\t\tif (dma_fence_add_callback(fence, &cb->cb,\n\t\t\t\t\t   amdgpu_pasid_free_cb))\n\t\t\tamdgpu_pasid_free_cb(fence, &cb->cb);\n\t}\n\n\treturn;\n\nfallback:\n\t \n\tdma_resv_wait_timeout(resv, DMA_RESV_USAGE_BOOKKEEP,\n\t\t\t      false, MAX_SCHEDULE_TIMEOUT);\n\tamdgpu_pasid_free(pasid);\n}\n\n \n\n \nbool amdgpu_vmid_had_gpu_reset(struct amdgpu_device *adev,\n\t\t\t       struct amdgpu_vmid *id)\n{\n\treturn id->current_gpu_reset_count !=\n\t\tatomic_read(&adev->gpu_reset_counter);\n}\n\n \nstatic bool amdgpu_vmid_gds_switch_needed(struct amdgpu_vmid *id,\n\t\t\t\t\t  struct amdgpu_job *job)\n{\n\treturn id->gds_base != job->gds_base ||\n\t\tid->gds_size != job->gds_size ||\n\t\tid->gws_base != job->gws_base ||\n\t\tid->gws_size != job->gws_size ||\n\t\tid->oa_base != job->oa_base ||\n\t\tid->oa_size != job->oa_size;\n}\n\n \nstatic bool amdgpu_vmid_compatible(struct amdgpu_vmid *id,\n\t\t\t\t   struct amdgpu_job *job)\n{\n\treturn  id->pd_gpu_addr == job->vm_pd_addr &&\n\t\t!amdgpu_vmid_gds_switch_needed(id, job);\n}\n\n \nstatic int amdgpu_vmid_grab_idle(struct amdgpu_vm *vm,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_vmid **idle,\n\t\t\t\t struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tstruct dma_fence **fences;\n\tunsigned i;\n\n\tif (!dma_fence_is_signaled(ring->vmid_wait)) {\n\t\t*fence = dma_fence_get(ring->vmid_wait);\n\t\treturn 0;\n\t}\n\n\tfences = kmalloc_array(id_mgr->num_ids, sizeof(void *), GFP_KERNEL);\n\tif (!fences)\n\t\treturn -ENOMEM;\n\n\t \n\ti = 0;\n\tlist_for_each_entry((*idle), &id_mgr->ids_lru, list) {\n\t\t \n\t\tstruct amdgpu_ring *r = adev->vm_manager.concurrent_flush ?\n\t\t\tNULL : ring;\n\n\t\tfences[i] = amdgpu_sync_peek_fence(&(*idle)->active, r);\n\t\tif (!fences[i])\n\t\t\tbreak;\n\t\t++i;\n\t}\n\n\t \n\tif (&(*idle)->list == &id_mgr->ids_lru) {\n\t\tu64 fence_context = adev->vm_manager.fence_context + ring->idx;\n\t\tunsigned seqno = ++adev->vm_manager.seqno[ring->idx];\n\t\tstruct dma_fence_array *array;\n\t\tunsigned j;\n\n\t\t*idle = NULL;\n\t\tfor (j = 0; j < i; ++j)\n\t\t\tdma_fence_get(fences[j]);\n\n\t\tarray = dma_fence_array_create(i, fences, fence_context,\n\t\t\t\t\t       seqno, true);\n\t\tif (!array) {\n\t\t\tfor (j = 0; j < i; ++j)\n\t\t\t\tdma_fence_put(fences[j]);\n\t\t\tkfree(fences);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t*fence = dma_fence_get(&array->base);\n\t\tdma_fence_put(ring->vmid_wait);\n\t\tring->vmid_wait = &array->base;\n\t\treturn 0;\n\t}\n\tkfree(fences);\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_vmid_grab_reserved(struct amdgpu_vm *vm,\n\t\t\t\t     struct amdgpu_ring *ring,\n\t\t\t\t     struct amdgpu_job *job,\n\t\t\t\t     struct amdgpu_vmid **id,\n\t\t\t\t     struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tuint64_t fence_context = adev->fence_context + ring->idx;\n\tbool needs_flush = vm->use_cpu_for_update;\n\tuint64_t updates = amdgpu_vm_tlb_seq(vm);\n\tint r;\n\n\t*id = id_mgr->reserved;\n\tif ((*id)->owner != vm->immediate.fence_context ||\n\t    !amdgpu_vmid_compatible(*id, job) ||\n\t    (*id)->flushed_updates < updates ||\n\t    !(*id)->last_flush ||\n\t    ((*id)->last_flush->context != fence_context &&\n\t     !dma_fence_is_signaled((*id)->last_flush))) {\n\t\tstruct dma_fence *tmp;\n\n\t\t \n\t\tif (adev->vm_manager.concurrent_flush)\n\t\t\tring = NULL;\n\n\t\t \n\t\t(*id)->pd_gpu_addr = 0;\n\t\ttmp = amdgpu_sync_peek_fence(&(*id)->active, ring);\n\t\tif (tmp) {\n\t\t\t*id = NULL;\n\t\t\t*fence = dma_fence_get(tmp);\n\t\t\treturn 0;\n\t\t}\n\t\tneeds_flush = true;\n\t}\n\n\t \n\tr = amdgpu_sync_fence(&(*id)->active, &job->base.s_fence->finished);\n\tif (r)\n\t\treturn r;\n\n\tjob->vm_needs_flush = needs_flush;\n\tjob->spm_update_needed = true;\n\treturn 0;\n}\n\n \nstatic int amdgpu_vmid_grab_used(struct amdgpu_vm *vm,\n\t\t\t\t struct amdgpu_ring *ring,\n\t\t\t\t struct amdgpu_job *job,\n\t\t\t\t struct amdgpu_vmid **id,\n\t\t\t\t struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tuint64_t fence_context = adev->fence_context + ring->idx;\n\tuint64_t updates = amdgpu_vm_tlb_seq(vm);\n\tint r;\n\n\tjob->vm_needs_flush = vm->use_cpu_for_update;\n\n\t \n\tlist_for_each_entry_reverse((*id), &id_mgr->ids_lru, list) {\n\t\tbool needs_flush = vm->use_cpu_for_update;\n\n\t\t \n\t\tif ((*id)->owner != vm->immediate.fence_context)\n\t\t\tcontinue;\n\n\t\tif (!amdgpu_vmid_compatible(*id, job))\n\t\t\tcontinue;\n\n\t\tif (!(*id)->last_flush ||\n\t\t    ((*id)->last_flush->context != fence_context &&\n\t\t     !dma_fence_is_signaled((*id)->last_flush)))\n\t\t\tneeds_flush = true;\n\n\t\tif ((*id)->flushed_updates < updates)\n\t\t\tneeds_flush = true;\n\n\t\tif (needs_flush && !adev->vm_manager.concurrent_flush)\n\t\t\tcontinue;\n\n\t\t \n\t\tr = amdgpu_sync_fence(&(*id)->active,\n\t\t\t\t      &job->base.s_fence->finished);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tjob->vm_needs_flush |= needs_flush;\n\t\treturn 0;\n\t}\n\n\t*id = NULL;\n\treturn 0;\n}\n\n \nint amdgpu_vmid_grab(struct amdgpu_vm *vm, struct amdgpu_ring *ring,\n\t\t     struct amdgpu_job *job, struct dma_fence **fence)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned vmhub = ring->vm_hub;\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tstruct amdgpu_vmid *idle = NULL;\n\tstruct amdgpu_vmid *id = NULL;\n\tint r = 0;\n\n\tmutex_lock(&id_mgr->lock);\n\tr = amdgpu_vmid_grab_idle(vm, ring, &idle, fence);\n\tif (r || !idle)\n\t\tgoto error;\n\n\tif (vm->reserved_vmid[vmhub] || (enforce_isolation && (vmhub == AMDGPU_GFXHUB(0)))) {\n\t\tr = amdgpu_vmid_grab_reserved(vm, ring, job, &id, fence);\n\t\tif (r || !id)\n\t\t\tgoto error;\n\t} else {\n\t\tr = amdgpu_vmid_grab_used(vm, ring, job, &id, fence);\n\t\tif (r)\n\t\t\tgoto error;\n\n\t\tif (!id) {\n\t\t\t \n\t\t\tid = idle;\n\n\t\t\t \n\t\t\tr = amdgpu_sync_fence(&id->active,\n\t\t\t\t\t      &job->base.s_fence->finished);\n\t\t\tif (r)\n\t\t\t\tgoto error;\n\n\t\t\tjob->vm_needs_flush = true;\n\t\t}\n\n\t\tlist_move_tail(&id->list, &id_mgr->ids_lru);\n\t}\n\n\tjob->gds_switch_needed = amdgpu_vmid_gds_switch_needed(id, job);\n\tif (job->vm_needs_flush) {\n\t\tid->flushed_updates = amdgpu_vm_tlb_seq(vm);\n\t\tdma_fence_put(id->last_flush);\n\t\tid->last_flush = NULL;\n\t}\n\tjob->vmid = id - id_mgr->ids;\n\tjob->pasid = vm->pasid;\n\n\tid->gds_base = job->gds_base;\n\tid->gds_size = job->gds_size;\n\tid->gws_base = job->gws_base;\n\tid->gws_size = job->gws_size;\n\tid->oa_base = job->oa_base;\n\tid->oa_size = job->oa_size;\n\tid->pd_gpu_addr = job->vm_pd_addr;\n\tid->owner = vm->immediate.fence_context;\n\n\ttrace_amdgpu_vm_grab_id(vm, ring, job);\n\nerror:\n\tmutex_unlock(&id_mgr->lock);\n\treturn r;\n}\n\nint amdgpu_vmid_alloc_reserved(struct amdgpu_device *adev,\n\t\t\t       unsigned vmhub)\n{\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\n\tmutex_lock(&id_mgr->lock);\n\n\t++id_mgr->reserved_use_count;\n\tif (!id_mgr->reserved) {\n\t\tstruct amdgpu_vmid *id;\n\n\t\tid = list_first_entry(&id_mgr->ids_lru, struct amdgpu_vmid,\n\t\t\t\t      list);\n\t\t \n\t\tlist_del_init(&id->list);\n\t\tid_mgr->reserved = id;\n\t}\n\n\tmutex_unlock(&id_mgr->lock);\n\treturn 0;\n}\n\nvoid amdgpu_vmid_free_reserved(struct amdgpu_device *adev,\n\t\t\t       unsigned vmhub)\n{\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\n\tmutex_lock(&id_mgr->lock);\n\tif (!--id_mgr->reserved_use_count) {\n\t\t \n\t\tlist_add(&id_mgr->reserved->list, &id_mgr->ids_lru);\n\t\tid_mgr->reserved = NULL;\n\t}\n\n\tmutex_unlock(&id_mgr->lock);\n}\n\n \nvoid amdgpu_vmid_reset(struct amdgpu_device *adev, unsigned vmhub,\n\t\t       unsigned vmid)\n{\n\tstruct amdgpu_vmid_mgr *id_mgr = &adev->vm_manager.id_mgr[vmhub];\n\tstruct amdgpu_vmid *id = &id_mgr->ids[vmid];\n\n\tmutex_lock(&id_mgr->lock);\n\tid->owner = 0;\n\tid->gds_base = 0;\n\tid->gds_size = 0;\n\tid->gws_base = 0;\n\tid->gws_size = 0;\n\tid->oa_base = 0;\n\tid->oa_size = 0;\n\tmutex_unlock(&id_mgr->lock);\n}\n\n \nvoid amdgpu_vmid_reset_all(struct amdgpu_device *adev)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < AMDGPU_MAX_VMHUBS; ++i) {\n\t\tstruct amdgpu_vmid_mgr *id_mgr =\n\t\t\t&adev->vm_manager.id_mgr[i];\n\n\t\tfor (j = 1; j < id_mgr->num_ids; ++j)\n\t\t\tamdgpu_vmid_reset(adev, i, j);\n\t}\n}\n\n \nvoid amdgpu_vmid_mgr_init(struct amdgpu_device *adev)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < AMDGPU_MAX_VMHUBS; ++i) {\n\t\tstruct amdgpu_vmid_mgr *id_mgr =\n\t\t\t&adev->vm_manager.id_mgr[i];\n\n\t\tmutex_init(&id_mgr->lock);\n\t\tINIT_LIST_HEAD(&id_mgr->ids_lru);\n\t\tid_mgr->reserved_use_count = 0;\n\n\t\t \n\t\tid_mgr->num_ids = adev->vm_manager.first_kfd_vmid;\n\n\t\t \n\t\tfor (j = 1; j < id_mgr->num_ids; ++j) {\n\t\t\tamdgpu_vmid_reset(adev, i, j);\n\t\t\tamdgpu_sync_create(&id_mgr->ids[j].active);\n\t\t\tlist_add_tail(&id_mgr->ids[j].list, &id_mgr->ids_lru);\n\t\t}\n\t}\n\t \n\tif (enforce_isolation)\n\t\tamdgpu_vmid_alloc_reserved(adev, AMDGPU_GFXHUB(0));\n\n}\n\n \nvoid amdgpu_vmid_mgr_fini(struct amdgpu_device *adev)\n{\n\tunsigned i, j;\n\n\tfor (i = 0; i < AMDGPU_MAX_VMHUBS; ++i) {\n\t\tstruct amdgpu_vmid_mgr *id_mgr =\n\t\t\t&adev->vm_manager.id_mgr[i];\n\n\t\tmutex_destroy(&id_mgr->lock);\n\t\tfor (j = 0; j < AMDGPU_NUM_VMID; ++j) {\n\t\t\tstruct amdgpu_vmid *id = &id_mgr->ids[j];\n\n\t\t\tamdgpu_sync_free(&id->active);\n\t\t\tdma_fence_put(id->last_flush);\n\t\t\tdma_fence_put(id->pasid_mapping);\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}