{
  "module_name": "amdgpu_ras.c",
  "hash_id": "530236fd44b8f30230d973ffb98d57f54f33ab3223112f6500a04580432855ca",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_ras.c",
  "human_readable_source": " \n#include <linux/debugfs.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/uaccess.h>\n#include <linux/reboot.h>\n#include <linux/syscalls.h>\n#include <linux/pm_runtime.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ras.h\"\n#include \"amdgpu_atomfirmware.h\"\n#include \"amdgpu_xgmi.h\"\n#include \"ivsrcid/nbio/irqsrcs_nbif_7_4.h\"\n#include \"nbio_v4_3.h\"\n#include \"nbio_v7_9.h\"\n#include \"atom.h\"\n#include \"amdgpu_reset.h\"\n\n#ifdef CONFIG_X86_MCE_AMD\n#include <asm/mce.h>\n\nstatic bool notifier_registered;\n#endif\nstatic const char *RAS_FS_NAME = \"ras\";\n\nconst char *ras_error_string[] = {\n\t\"none\",\n\t\"parity\",\n\t\"single_correctable\",\n\t\"multi_uncorrectable\",\n\t\"poison\",\n};\n\nconst char *ras_block_string[] = {\n\t\"umc\",\n\t\"sdma\",\n\t\"gfx\",\n\t\"mmhub\",\n\t\"athub\",\n\t\"pcie_bif\",\n\t\"hdp\",\n\t\"xgmi_wafl\",\n\t\"df\",\n\t\"smn\",\n\t\"sem\",\n\t\"mp0\",\n\t\"mp1\",\n\t\"fuse\",\n\t\"mca\",\n\t\"vcn\",\n\t\"jpeg\",\n};\n\nconst char *ras_mca_block_string[] = {\n\t\"mca_mp0\",\n\t\"mca_mp1\",\n\t\"mca_mpio\",\n\t\"mca_iohc\",\n};\n\nstruct amdgpu_ras_block_list {\n\t \n\tstruct list_head node;\n\n\tstruct amdgpu_ras_block_object *ras_obj;\n};\n\nconst char *get_ras_block_str(struct ras_common_if *ras_block)\n{\n\tif (!ras_block)\n\t\treturn \"NULL\";\n\n\tif (ras_block->block >= AMDGPU_RAS_BLOCK_COUNT)\n\t\treturn \"OUT OF RANGE\";\n\n\tif (ras_block->block == AMDGPU_RAS_BLOCK__MCA)\n\t\treturn ras_mca_block_string[ras_block->sub_block_index];\n\n\treturn ras_block_string[ras_block->block];\n}\n\n#define ras_block_str(_BLOCK_) \\\n\t(((_BLOCK_) < ARRAY_SIZE(ras_block_string)) ? ras_block_string[_BLOCK_] : \"Out Of Range\")\n\n#define ras_err_str(i) (ras_error_string[ffs(i)])\n\n#define RAS_DEFAULT_FLAGS (AMDGPU_RAS_FLAG_INIT_BY_VBIOS)\n\n \n#define\tRAS_UMC_INJECT_ADDR_LIMIT\t(0x1ULL << 52)\n\n \n#define RAS_BAD_PAGE_COVER              (100 * 1024 * 1024ULL)\n\nenum amdgpu_ras_retire_page_reservation {\n\tAMDGPU_RAS_RETIRE_PAGE_RESERVED,\n\tAMDGPU_RAS_RETIRE_PAGE_PENDING,\n\tAMDGPU_RAS_RETIRE_PAGE_FAULT,\n};\n\natomic_t amdgpu_ras_in_intr = ATOMIC_INIT(0);\n\nstatic bool amdgpu_ras_check_bad_page_unlock(struct amdgpu_ras *con,\n\t\t\t\tuint64_t addr);\nstatic bool amdgpu_ras_check_bad_page(struct amdgpu_device *adev,\n\t\t\t\tuint64_t addr);\n#ifdef CONFIG_X86_MCE_AMD\nstatic void amdgpu_register_bad_pages_mca_notifier(struct amdgpu_device *adev);\nstruct mce_notifier_adev_list {\n\tstruct amdgpu_device *devs[MAX_GPU_INSTANCE];\n\tint num_gpu;\n};\nstatic struct mce_notifier_adev_list mce_adev_list;\n#endif\n\nvoid amdgpu_ras_set_error_query_ready(struct amdgpu_device *adev, bool ready)\n{\n\tif (adev && amdgpu_ras_get_context(adev))\n\t\tamdgpu_ras_get_context(adev)->error_query_ready = ready;\n}\n\nstatic bool amdgpu_ras_get_error_query_ready(struct amdgpu_device *adev)\n{\n\tif (adev && amdgpu_ras_get_context(adev))\n\t\treturn amdgpu_ras_get_context(adev)->error_query_ready;\n\n\treturn false;\n}\n\nstatic int amdgpu_reserve_page_direct(struct amdgpu_device *adev, uint64_t address)\n{\n\tstruct ras_err_data err_data = {0, 0, 0, NULL};\n\tstruct eeprom_table_record err_rec;\n\n\tif ((address >= adev->gmc.mc_vram_size) ||\n\t    (address >= RAS_UMC_INJECT_ADDR_LIMIT)) {\n\t\tdev_warn(adev->dev,\n\t\t         \"RAS WARN: input address 0x%llx is invalid.\\n\",\n\t\t         address);\n\t\treturn -EINVAL;\n\t}\n\n\tif (amdgpu_ras_check_bad_page(adev, address)) {\n\t\tdev_warn(adev->dev,\n\t\t\t \"RAS WARN: 0x%llx has already been marked as bad page!\\n\",\n\t\t\t address);\n\t\treturn 0;\n\t}\n\n\tmemset(&err_rec, 0x0, sizeof(struct eeprom_table_record));\n\terr_data.err_addr = &err_rec;\n\tamdgpu_umc_fill_error_record(&err_data, address, address, 0, 0);\n\n\tif (amdgpu_bad_page_threshold != 0) {\n\t\tamdgpu_ras_add_bad_pages(adev, err_data.err_addr,\n\t\t\t\t\t err_data.err_addr_cnt);\n\t\tamdgpu_ras_save_bad_pages(adev, NULL);\n\t}\n\n\tdev_warn(adev->dev, \"WARNING: THIS IS ONLY FOR TEST PURPOSES AND WILL CORRUPT RAS EEPROM\\n\");\n\tdev_warn(adev->dev, \"Clear EEPROM:\\n\");\n\tdev_warn(adev->dev, \"    echo 1 > /sys/kernel/debug/dri/0/ras/ras_eeprom_reset\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t amdgpu_ras_debugfs_read(struct file *f, char __user *buf,\n\t\t\t\t\tsize_t size, loff_t *pos)\n{\n\tstruct ras_manager *obj = (struct ras_manager *)file_inode(f)->i_private;\n\tstruct ras_query_if info = {\n\t\t.head = obj->head,\n\t};\n\tssize_t s;\n\tchar val[128];\n\n\tif (amdgpu_ras_query_error_status(obj->adev, &info))\n\t\treturn -EINVAL;\n\n\t \n\tif (obj->adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 2) &&\n\t    obj->adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 4)) {\n\t\tif (amdgpu_ras_reset_error_status(obj->adev, info.head.block))\n\t\t\tdev_warn(obj->adev->dev, \"Failed to reset error counter and error status\");\n\t}\n\n\ts = snprintf(val, sizeof(val), \"%s: %lu\\n%s: %lu\\n\",\n\t\t\t\"ue\", info.ue_count,\n\t\t\t\"ce\", info.ce_count);\n\tif (*pos >= s)\n\t\treturn 0;\n\n\ts -= *pos;\n\ts = min_t(u64, s, size);\n\n\n\tif (copy_to_user(buf, &val[*pos], s))\n\t\treturn -EINVAL;\n\n\t*pos += s;\n\n\treturn s;\n}\n\nstatic const struct file_operations amdgpu_ras_debugfs_ops = {\n\t.owner = THIS_MODULE,\n\t.read = amdgpu_ras_debugfs_read,\n\t.write = NULL,\n\t.llseek = default_llseek\n};\n\nstatic int amdgpu_ras_find_block_id_by_name(const char *name, int *block_id)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(ras_block_string); i++) {\n\t\t*block_id = i;\n\t\tif (strcmp(name, ras_block_string[i]) == 0)\n\t\t\treturn 0;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int amdgpu_ras_debugfs_ctrl_parse_data(struct file *f,\n\t\tconst char __user *buf, size_t size,\n\t\tloff_t *pos, struct ras_debug_if *data)\n{\n\tssize_t s = min_t(u64, 64, size);\n\tchar str[65];\n\tchar block_name[33];\n\tchar err[9] = \"ue\";\n\tint op = -1;\n\tint block_id;\n\tuint32_t sub_block;\n\tu64 address, value;\n\t \n\tu32 instance_mask = 0;\n\n\tif (*pos)\n\t\treturn -EINVAL;\n\t*pos = size;\n\n\tmemset(str, 0, sizeof(str));\n\tmemset(data, 0, sizeof(*data));\n\n\tif (copy_from_user(str, buf, s))\n\t\treturn -EINVAL;\n\n\tif (sscanf(str, \"disable %32s\", block_name) == 1)\n\t\top = 0;\n\telse if (sscanf(str, \"enable %32s %8s\", block_name, err) == 2)\n\t\top = 1;\n\telse if (sscanf(str, \"inject %32s %8s\", block_name, err) == 2)\n\t\top = 2;\n\telse if (strstr(str, \"retire_page\") != NULL)\n\t\top = 3;\n\telse if (str[0] && str[1] && str[2] && str[3])\n\t\t \n\t\treturn -EINVAL;\n\n\tif (op != -1) {\n\t\tif (op == 3) {\n\t\t\tif (sscanf(str, \"%*s 0x%llx\", &address) != 1 &&\n\t\t\t    sscanf(str, \"%*s %llu\", &address) != 1)\n\t\t\t\treturn -EINVAL;\n\n\t\t\tdata->op = op;\n\t\t\tdata->inject.address = address;\n\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (amdgpu_ras_find_block_id_by_name(block_name, &block_id))\n\t\t\treturn -EINVAL;\n\n\t\tdata->head.block = block_id;\n\t\t \n\t\tif (!memcmp(\"ue\", err, 2))\n\t\t\tdata->head.type = AMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE;\n\t\telse if (!memcmp(\"ce\", err, 2))\n\t\t\tdata->head.type = AMDGPU_RAS_ERROR__SINGLE_CORRECTABLE;\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tdata->op = op;\n\n\t\tif (op == 2) {\n\t\t\tif (sscanf(str, \"%*s %*s %*s 0x%x 0x%llx 0x%llx 0x%x\",\n\t\t\t\t   &sub_block, &address, &value, &instance_mask) != 4 &&\n\t\t\t    sscanf(str, \"%*s %*s %*s %u %llu %llu %u\",\n\t\t\t\t   &sub_block, &address, &value, &instance_mask) != 4 &&\n\t\t\t\tsscanf(str, \"%*s %*s %*s 0x%x 0x%llx 0x%llx\",\n\t\t\t\t   &sub_block, &address, &value) != 3 &&\n\t\t\t    sscanf(str, \"%*s %*s %*s %u %llu %llu\",\n\t\t\t\t   &sub_block, &address, &value) != 3)\n\t\t\t\treturn -EINVAL;\n\t\t\tdata->head.sub_block_index = sub_block;\n\t\t\tdata->inject.address = address;\n\t\t\tdata->inject.value = value;\n\t\t\tdata->inject.instance_mask = instance_mask;\n\t\t}\n\t} else {\n\t\tif (size < sizeof(*data))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(data, buf, sizeof(*data)))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void amdgpu_ras_instance_mask_check(struct amdgpu_device *adev,\n\t\t\t\tstruct ras_debug_if *data)\n{\n\tint num_xcc = adev->gfx.xcc_mask ? NUM_XCC(adev->gfx.xcc_mask) : 1;\n\tuint32_t mask, inst_mask = data->inject.instance_mask;\n\n\t \n\tif (num_xcc <= 1 && inst_mask) {\n\t\tdata->inject.instance_mask = 0;\n\t\tdev_dbg(adev->dev,\n\t\t\t\"RAS inject mask(0x%x) isn't supported and force it to 0.\\n\",\n\t\t\tinst_mask);\n\n\t\treturn;\n\t}\n\n\tswitch (data->head.block) {\n\tcase AMDGPU_RAS_BLOCK__GFX:\n\t\tmask = GENMASK(num_xcc - 1, 0);\n\t\tbreak;\n\tcase AMDGPU_RAS_BLOCK__SDMA:\n\t\tmask = GENMASK(adev->sdma.num_instances - 1, 0);\n\t\tbreak;\n\tcase AMDGPU_RAS_BLOCK__VCN:\n\tcase AMDGPU_RAS_BLOCK__JPEG:\n\t\tmask = GENMASK(adev->vcn.num_vcn_inst - 1, 0);\n\t\tbreak;\n\tdefault:\n\t\tmask = inst_mask;\n\t\tbreak;\n\t}\n\n\t \n\tdata->inject.instance_mask &= mask;\n\tif (inst_mask != data->inject.instance_mask)\n\t\tdev_dbg(adev->dev,\n\t\t\t\"Adjust RAS inject mask 0x%x to 0x%x\\n\",\n\t\t\tinst_mask, data->inject.instance_mask);\n}\n\n \nstatic ssize_t amdgpu_ras_debugfs_ctrl_write(struct file *f,\n\t\t\t\t\t     const char __user *buf,\n\t\t\t\t\t     size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)file_inode(f)->i_private;\n\tstruct ras_debug_if data;\n\tint ret = 0;\n\n\tif (!amdgpu_ras_get_error_query_ready(adev)) {\n\t\tdev_warn(adev->dev, \"RAS WARN: error injection \"\n\t\t\t\t\"currently inaccessible\\n\");\n\t\treturn size;\n\t}\n\n\tret = amdgpu_ras_debugfs_ctrl_parse_data(f, buf, size, pos, &data);\n\tif (ret)\n\t\treturn ret;\n\n\tif (data.op == 3) {\n\t\tret = amdgpu_reserve_page_direct(adev, data.inject.address);\n\t\tif (!ret)\n\t\t\treturn size;\n\t\telse\n\t\t\treturn ret;\n\t}\n\n\tif (!amdgpu_ras_is_supported(adev, data.head.block))\n\t\treturn -EINVAL;\n\n\tswitch (data.op) {\n\tcase 0:\n\t\tret = amdgpu_ras_feature_enable(adev, &data.head, 0);\n\t\tbreak;\n\tcase 1:\n\t\tret = amdgpu_ras_feature_enable(adev, &data.head, 1);\n\t\tbreak;\n\tcase 2:\n\t\tif ((data.inject.address >= adev->gmc.mc_vram_size &&\n\t\t    adev->gmc.mc_vram_size) ||\n\t\t    (data.inject.address >= RAS_UMC_INJECT_ADDR_LIMIT)) {\n\t\t\tdev_warn(adev->dev, \"RAS WARN: input address \"\n\t\t\t\t\t\"0x%llx is invalid.\",\n\t\t\t\t\tdata.inject.address);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif ((data.head.block == AMDGPU_RAS_BLOCK__UMC) &&\n\t\t    amdgpu_ras_check_bad_page(adev, data.inject.address)) {\n\t\t\tdev_warn(adev->dev, \"RAS WARN: inject: 0x%llx has \"\n\t\t\t\t \"already been marked as bad!\\n\",\n\t\t\t\t data.inject.address);\n\t\t\tbreak;\n\t\t}\n\n\t\tamdgpu_ras_instance_mask_check(adev, &data);\n\n\t\t \n\t\tret = amdgpu_ras_error_inject(adev, &data.inject);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn size;\n}\n\n \nstatic ssize_t amdgpu_ras_debugfs_eeprom_write(struct file *f,\n\t\t\t\t\t       const char __user *buf,\n\t\t\t\t\t       size_t size, loff_t *pos)\n{\n\tstruct amdgpu_device *adev =\n\t\t(struct amdgpu_device *)file_inode(f)->i_private;\n\tint ret;\n\n\tret = amdgpu_ras_eeprom_reset_table(\n\t\t&(amdgpu_ras_get_context(adev)->eeprom_control));\n\n\tif (!ret) {\n\t\t \n\t\tamdgpu_ras_get_context(adev)->flags = RAS_DEFAULT_FLAGS;\n\t\treturn size;\n\t} else {\n\t\treturn ret;\n\t}\n}\n\nstatic const struct file_operations amdgpu_ras_debugfs_ctrl_ops = {\n\t.owner = THIS_MODULE,\n\t.read = NULL,\n\t.write = amdgpu_ras_debugfs_ctrl_write,\n\t.llseek = default_llseek\n};\n\nstatic const struct file_operations amdgpu_ras_debugfs_eeprom_ops = {\n\t.owner = THIS_MODULE,\n\t.read = NULL,\n\t.write = amdgpu_ras_debugfs_eeprom_write,\n\t.llseek = default_llseek\n};\n\n \nstatic ssize_t amdgpu_ras_sysfs_read(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct ras_manager *obj = container_of(attr, struct ras_manager, sysfs_attr);\n\tstruct ras_query_if info = {\n\t\t.head = obj->head,\n\t};\n\n\tif (!amdgpu_ras_get_error_query_ready(obj->adev))\n\t\treturn sysfs_emit(buf, \"Query currently inaccessible\\n\");\n\n\tif (amdgpu_ras_query_error_status(obj->adev, &info))\n\t\treturn -EINVAL;\n\n\tif (obj->adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 2) &&\n\t    obj->adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 4)) {\n\t\tif (amdgpu_ras_reset_error_status(obj->adev, info.head.block))\n\t\t\tdev_warn(obj->adev->dev, \"Failed to reset error counter and error status\");\n\t}\n\n\treturn sysfs_emit(buf, \"%s: %lu\\n%s: %lu\\n\", \"ue\", info.ue_count,\n\t\t\t  \"ce\", info.ce_count);\n}\n\n \n\n#define get_obj(obj) do { (obj)->use++; } while (0)\n#define alive_obj(obj) ((obj)->use)\n\nstatic inline void put_obj(struct ras_manager *obj)\n{\n\tif (obj && (--obj->use == 0))\n\t\tlist_del(&obj->node);\n\tif (obj && (obj->use < 0))\n\t\tDRM_ERROR(\"RAS ERROR: Unbalance obj(%s) use\\n\", get_ras_block_str(&obj->head));\n}\n\n \nstatic struct ras_manager *amdgpu_ras_create_obj(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj;\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn NULL;\n\n\tif (head->block >= AMDGPU_RAS_BLOCK_COUNT)\n\t\treturn NULL;\n\n\tif (head->block == AMDGPU_RAS_BLOCK__MCA) {\n\t\tif (head->sub_block_index >= AMDGPU_RAS_MCA_BLOCK__LAST)\n\t\t\treturn NULL;\n\n\t\tobj = &con->objs[AMDGPU_RAS_BLOCK__LAST + head->sub_block_index];\n\t} else\n\t\tobj = &con->objs[head->block];\n\n\t \n\tif (alive_obj(obj))\n\t\treturn NULL;\n\n\tobj->head = *head;\n\tobj->adev = adev;\n\tlist_add(&obj->node, &con->head);\n\tget_obj(obj);\n\n\treturn obj;\n}\n\n \nstruct ras_manager *amdgpu_ras_find_obj(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj;\n\tint i;\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn NULL;\n\n\tif (head) {\n\t\tif (head->block >= AMDGPU_RAS_BLOCK_COUNT)\n\t\t\treturn NULL;\n\n\t\tif (head->block == AMDGPU_RAS_BLOCK__MCA) {\n\t\t\tif (head->sub_block_index >= AMDGPU_RAS_MCA_BLOCK__LAST)\n\t\t\t\treturn NULL;\n\n\t\t\tobj = &con->objs[AMDGPU_RAS_BLOCK__LAST + head->sub_block_index];\n\t\t} else\n\t\t\tobj = &con->objs[head->block];\n\n\t\tif (alive_obj(obj))\n\t\t\treturn obj;\n\t} else {\n\t\tfor (i = 0; i < AMDGPU_RAS_BLOCK_COUNT + AMDGPU_RAS_MCA_BLOCK_COUNT; i++) {\n\t\t\tobj = &con->objs[i];\n\t\t\tif (alive_obj(obj))\n\t\t\t\treturn obj;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n \n\n \nstatic int amdgpu_ras_is_feature_allowed(struct amdgpu_device *adev,\n\t\t\t\t\t struct ras_common_if *head)\n{\n\treturn adev->ras_hw_enabled & BIT(head->block);\n}\n\nstatic int amdgpu_ras_is_feature_enabled(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\treturn con->features & BIT(head->block);\n}\n\n \nstatic int __amdgpu_ras_feature_enable(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head, int enable)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, head);\n\n\t \n\tif (!amdgpu_ras_is_feature_allowed(adev, head))\n\t\treturn 0;\n\n\tif (enable) {\n\t\tif (!obj) {\n\t\t\tobj = amdgpu_ras_create_obj(adev, head);\n\t\t\tif (!obj)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\t \n\t\t\tget_obj(obj);\n\t\t}\n\t\tcon->features |= BIT(head->block);\n\t} else {\n\t\tif (obj && amdgpu_ras_is_feature_enabled(adev, head)) {\n\t\t\tcon->features &= ~BIT(head->block);\n\t\t\tput_obj(obj);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_ras_feature_enable(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head, bool enable)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tunion ta_ras_cmd_input *info;\n\tint ret;\n\n\tif (!con)\n\t\treturn -EINVAL;\n\n\t \n\tif (enable &&\n\t    head->block != AMDGPU_RAS_BLOCK__GFX &&\n\t    !amdgpu_ras_is_feature_allowed(adev, head))\n\t\treturn 0;\n\n\t \n\tif (head->block == AMDGPU_RAS_BLOCK__GFX &&\n\t    !amdgpu_sriov_vf(adev) &&\n\t    !amdgpu_ras_intr_triggered()) {\n\t\tinfo = kzalloc(sizeof(union ta_ras_cmd_input), GFP_KERNEL);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\n\t\tif (!enable) {\n\t\t\tinfo->disable_features = (struct ta_ras_disable_features_input) {\n\t\t\t\t.block_id =  amdgpu_ras_block_to_ta(head->block),\n\t\t\t\t.error_type = amdgpu_ras_error_to_ta(head->type),\n\t\t\t};\n\t\t} else {\n\t\t\tinfo->enable_features = (struct ta_ras_enable_features_input) {\n\t\t\t\t.block_id =  amdgpu_ras_block_to_ta(head->block),\n\t\t\t\t.error_type = amdgpu_ras_error_to_ta(head->type),\n\t\t\t};\n\t\t}\n\n\t\tret = psp_ras_enable_features(&adev->psp, info, enable);\n\t\tif (ret) {\n\t\t\tdev_err(adev->dev, \"ras %s %s failed poison:%d ret:%d\\n\",\n\t\t\t\tenable ? \"enable\":\"disable\",\n\t\t\t\tget_ras_block_str(head),\n\t\t\t\tamdgpu_ras_is_poison_mode_supported(adev), ret);\n\t\t\tkfree(info);\n\t\t\treturn ret;\n\t\t}\n\n\t\tkfree(info);\n\t}\n\n\t \n\t__amdgpu_ras_feature_enable(adev, head, enable);\n\n\treturn 0;\n}\n\n \nint amdgpu_ras_feature_enable_on_boot(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head, bool enable)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tint ret;\n\n\tif (!con)\n\t\treturn -EINVAL;\n\n\tif (con->flags & AMDGPU_RAS_FLAG_INIT_BY_VBIOS) {\n\t\tif (enable) {\n\t\t\t \n\t\t\tret = amdgpu_ras_feature_enable(adev, head, 1);\n\t\t\t \n\t\t\tif (ret == -EINVAL) {\n\t\t\t\tret = __amdgpu_ras_feature_enable(adev, head, 1);\n\t\t\t\tif (!ret)\n\t\t\t\t\tdev_info(adev->dev,\n\t\t\t\t\t\t\"RAS INFO: %s setup object\\n\",\n\t\t\t\t\t\tget_ras_block_str(head));\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tret = __amdgpu_ras_feature_enable(adev, head, 1);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\t \n\t\t\tif (head->block == AMDGPU_RAS_BLOCK__GFX)\n\t\t\t\tcon->features |= BIT(head->block);\n\n\t\t\tret = amdgpu_ras_feature_enable(adev, head, 0);\n\n\t\t\t \n\t\t\tif (adev->ras_enabled && head->block == AMDGPU_RAS_BLOCK__GFX)\n\t\t\t\tcon->features &= ~BIT(head->block);\n\t\t}\n\t} else\n\t\tret = amdgpu_ras_feature_enable(adev, head, enable);\n\n\treturn ret;\n}\n\nstatic int amdgpu_ras_disable_all_features(struct amdgpu_device *adev,\n\t\tbool bypass)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj, *tmp;\n\n\tlist_for_each_entry_safe(obj, tmp, &con->head, node) {\n\t\t \n\t\tif (bypass) {\n\t\t\tif (__amdgpu_ras_feature_enable(adev, &obj->head, 0))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (amdgpu_ras_feature_enable(adev, &obj->head, 0))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn con->features;\n}\n\nstatic int amdgpu_ras_enable_all_features(struct amdgpu_device *adev,\n\t\tbool bypass)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tint i;\n\tconst enum amdgpu_ras_error_type default_ras_type = AMDGPU_RAS_ERROR__NONE;\n\n\tfor (i = 0; i < AMDGPU_RAS_BLOCK_COUNT; i++) {\n\t\tstruct ras_common_if head = {\n\t\t\t.block = i,\n\t\t\t.type = default_ras_type,\n\t\t\t.sub_block_index = 0,\n\t\t};\n\n\t\tif (i == AMDGPU_RAS_BLOCK__MCA)\n\t\t\tcontinue;\n\n\t\tif (bypass) {\n\t\t\t \n\t\t\tif (__amdgpu_ras_feature_enable(adev, &head, 1))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (amdgpu_ras_feature_enable(adev, &head, 1))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = 0; i < AMDGPU_RAS_MCA_BLOCK_COUNT; i++) {\n\t\tstruct ras_common_if head = {\n\t\t\t.block = AMDGPU_RAS_BLOCK__MCA,\n\t\t\t.type = default_ras_type,\n\t\t\t.sub_block_index = i,\n\t\t};\n\n\t\tif (bypass) {\n\t\t\t \n\t\t\tif (__amdgpu_ras_feature_enable(adev, &head, 1))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (amdgpu_ras_feature_enable(adev, &head, 1))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn con->features;\n}\n \n\nstatic int amdgpu_ras_block_match_default(struct amdgpu_ras_block_object *block_obj,\n\t\tenum amdgpu_ras_block block)\n{\n\tif (!block_obj)\n\t\treturn -EINVAL;\n\n\tif (block_obj->ras_comm.block == block)\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic struct amdgpu_ras_block_object *amdgpu_ras_get_ras_block(struct amdgpu_device *adev,\n\t\t\t\t\tenum amdgpu_ras_block block, uint32_t sub_block_index)\n{\n\tstruct amdgpu_ras_block_list *node, *tmp;\n\tstruct amdgpu_ras_block_object *obj;\n\n\tif (block >= AMDGPU_RAS_BLOCK__LAST)\n\t\treturn NULL;\n\n\tlist_for_each_entry_safe(node, tmp, &adev->ras_list, node) {\n\t\tif (!node->ras_obj) {\n\t\t\tdev_warn(adev->dev, \"Warning: abnormal ras list node.\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tobj = node->ras_obj;\n\t\tif (obj->ras_block_match) {\n\t\t\tif (obj->ras_block_match(obj, block, sub_block_index) == 0)\n\t\t\t\treturn obj;\n\t\t} else {\n\t\t\tif (amdgpu_ras_block_match_default(obj, block) == 0)\n\t\t\t\treturn obj;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic void amdgpu_ras_get_ecc_info(struct amdgpu_device *adev, struct ras_err_data *err_data)\n{\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\tint ret = 0;\n\n\t \n\tret = amdgpu_dpm_get_ecc_info(adev, (void *)&(ras->umc_ecc));\n\tif (ret == -EOPNOTSUPP) {\n\t\tif (adev->umc.ras && adev->umc.ras->ras_block.hw_ops &&\n\t\t\tadev->umc.ras->ras_block.hw_ops->query_ras_error_count)\n\t\t\tadev->umc.ras->ras_block.hw_ops->query_ras_error_count(adev, err_data);\n\n\t\t \n\t\tif (adev->umc.ras && adev->umc.ras->ras_block.hw_ops &&\n\t\t    adev->umc.ras->ras_block.hw_ops->query_ras_error_address)\n\t\t\tadev->umc.ras->ras_block.hw_ops->query_ras_error_address(adev, err_data);\n\t} else if (!ret) {\n\t\tif (adev->umc.ras &&\n\t\t\tadev->umc.ras->ecc_info_query_ras_error_count)\n\t\t\tadev->umc.ras->ecc_info_query_ras_error_count(adev, err_data);\n\n\t\tif (adev->umc.ras &&\n\t\t\tadev->umc.ras->ecc_info_query_ras_error_address)\n\t\t\tadev->umc.ras->ecc_info_query_ras_error_address(adev, err_data);\n\t}\n}\n\n \nint amdgpu_ras_query_error_status(struct amdgpu_device *adev,\n\t\t\t\t  struct ras_query_if *info)\n{\n\tstruct amdgpu_ras_block_object *block_obj = NULL;\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, &info->head);\n\tstruct ras_err_data err_data = {0, 0, 0, NULL};\n\n\tif (!obj)\n\t\treturn -EINVAL;\n\n\tif (info->head.block == AMDGPU_RAS_BLOCK__UMC) {\n\t\tamdgpu_ras_get_ecc_info(adev, &err_data);\n\t} else {\n\t\tblock_obj = amdgpu_ras_get_ras_block(adev, info->head.block, 0);\n\t\tif (!block_obj || !block_obj->hw_ops)   {\n\t\t\tdev_dbg_once(adev->dev, \"%s doesn't config RAS function\\n\",\n\t\t\t\t     get_ras_block_str(&info->head));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (block_obj->hw_ops->query_ras_error_count)\n\t\t\tblock_obj->hw_ops->query_ras_error_count(adev, &err_data);\n\n\t\tif ((info->head.block == AMDGPU_RAS_BLOCK__SDMA) ||\n\t\t    (info->head.block == AMDGPU_RAS_BLOCK__GFX) ||\n\t\t    (info->head.block == AMDGPU_RAS_BLOCK__MMHUB)) {\n\t\t\t\tif (block_obj->hw_ops->query_ras_error_status)\n\t\t\t\t\tblock_obj->hw_ops->query_ras_error_status(adev);\n\t\t\t}\n\t}\n\n\tobj->err_data.ue_count += err_data.ue_count;\n\tobj->err_data.ce_count += err_data.ce_count;\n\n\tinfo->ue_count = obj->err_data.ue_count;\n\tinfo->ce_count = obj->err_data.ce_count;\n\n\tif (err_data.ce_count) {\n\t\tif (!adev->aid_mask &&\n\t\t    adev->smuio.funcs &&\n\t\t    adev->smuio.funcs->get_socket_id &&\n\t\t    adev->smuio.funcs->get_die_id) {\n\t\t\tdev_info(adev->dev, \"socket: %d, die: %d \"\n\t\t\t\t\t\"%ld correctable hardware errors \"\n\t\t\t\t\t\"detected in %s block, no user \"\n\t\t\t\t\t\"action is needed.\\n\",\n\t\t\t\t\tadev->smuio.funcs->get_socket_id(adev),\n\t\t\t\t\tadev->smuio.funcs->get_die_id(adev),\n\t\t\t\t\tobj->err_data.ce_count,\n\t\t\t\t\tget_ras_block_str(&info->head));\n\t\t} else {\n\t\t\tdev_info(adev->dev, \"%ld correctable hardware errors \"\n\t\t\t\t\t\"detected in %s block, no user \"\n\t\t\t\t\t\"action is needed.\\n\",\n\t\t\t\t\tobj->err_data.ce_count,\n\t\t\t\t\tget_ras_block_str(&info->head));\n\t\t}\n\t}\n\tif (err_data.ue_count) {\n\t\tif (!adev->aid_mask &&\n\t\t    adev->smuio.funcs &&\n\t\t    adev->smuio.funcs->get_socket_id &&\n\t\t    adev->smuio.funcs->get_die_id) {\n\t\t\tdev_info(adev->dev, \"socket: %d, die: %d \"\n\t\t\t\t\t\"%ld uncorrectable hardware errors \"\n\t\t\t\t\t\"detected in %s block\\n\",\n\t\t\t\t\tadev->smuio.funcs->get_socket_id(adev),\n\t\t\t\t\tadev->smuio.funcs->get_die_id(adev),\n\t\t\t\t\tobj->err_data.ue_count,\n\t\t\t\t\tget_ras_block_str(&info->head));\n\t\t} else {\n\t\t\tdev_info(adev->dev, \"%ld uncorrectable hardware errors \"\n\t\t\t\t\t\"detected in %s block\\n\",\n\t\t\t\t\tobj->err_data.ue_count,\n\t\t\t\t\tget_ras_block_str(&info->head));\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint amdgpu_ras_reset_error_status(struct amdgpu_device *adev,\n\t\tenum amdgpu_ras_block block)\n{\n\tstruct amdgpu_ras_block_object *block_obj = amdgpu_ras_get_ras_block(adev, block, 0);\n\n\tif (!amdgpu_ras_is_supported(adev, block))\n\t\treturn -EINVAL;\n\n\tif (!block_obj || !block_obj->hw_ops)   {\n\t\tdev_dbg_once(adev->dev, \"%s doesn't config RAS function\\n\",\n\t\t\t     ras_block_str(block));\n\t\treturn -EINVAL;\n\t}\n\n\tif (block_obj->hw_ops->reset_ras_error_count)\n\t\tblock_obj->hw_ops->reset_ras_error_count(adev);\n\n\tif ((block == AMDGPU_RAS_BLOCK__GFX) ||\n\t    (block == AMDGPU_RAS_BLOCK__MMHUB)) {\n\t\tif (block_obj->hw_ops->reset_ras_error_status)\n\t\t\tblock_obj->hw_ops->reset_ras_error_status(adev);\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_ras_error_inject(struct amdgpu_device *adev,\n\t\tstruct ras_inject_if *info)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, &info->head);\n\tstruct ta_ras_trigger_error_input block_info = {\n\t\t.block_id =  amdgpu_ras_block_to_ta(info->head.block),\n\t\t.inject_error_type = amdgpu_ras_error_to_ta(info->head.type),\n\t\t.sub_block_index = info->head.sub_block_index,\n\t\t.address = info->address,\n\t\t.value = info->value,\n\t};\n\tint ret = -EINVAL;\n\tstruct amdgpu_ras_block_object *block_obj = amdgpu_ras_get_ras_block(adev,\n\t\t\t\t\t\t\tinfo->head.block,\n\t\t\t\t\t\t\tinfo->head.sub_block_index);\n\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tif (!obj)\n\t\treturn -EINVAL;\n\n\tif (!block_obj || !block_obj->hw_ops)\t{\n\t\tdev_dbg_once(adev->dev, \"%s doesn't config RAS function\\n\",\n\t\t\t     get_ras_block_str(&info->head));\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (adev->gmc.xgmi.num_physical_nodes > 1 &&\n\t    info->head.block != AMDGPU_RAS_BLOCK__GFX) {\n\t\tblock_info.address =\n\t\t\tamdgpu_xgmi_get_relative_phy_addr(adev,\n\t\t\t\t\t\t\t  block_info.address);\n\t}\n\n\tif (block_obj->hw_ops->ras_error_inject) {\n\t\tif (info->head.block == AMDGPU_RAS_BLOCK__GFX)\n\t\t\tret = block_obj->hw_ops->ras_error_inject(adev, info, info->instance_mask);\n\t\telse  \n\t\t\tret = block_obj->hw_ops->ras_error_inject(adev, &block_info,\n\t\t\t\t\t\tinfo->instance_mask);\n\t} else {\n\t\t \n\t\tret = psp_ras_trigger_error(&adev->psp, &block_info, info->instance_mask);\n\t}\n\n\tif (ret)\n\t\tdev_err(adev->dev, \"ras inject %s failed %d\\n\",\n\t\t\tget_ras_block_str(&info->head), ret);\n\n\treturn ret;\n}\n\n \nstatic int amdgpu_ras_query_error_count_helper(struct amdgpu_device *adev,\n\t\t\t\t\t       unsigned long *ce_count,\n\t\t\t\t\t       unsigned long *ue_count,\n\t\t\t\t\t       struct ras_query_if *query_info)\n{\n\tint ret;\n\n\tif (!query_info)\n\t\t \n\t\treturn 0;\n\n\tret = amdgpu_ras_query_error_status(adev, query_info);\n\tif (ret)\n\t\treturn ret;\n\n\t*ce_count += query_info->ce_count;\n\t*ue_count += query_info->ue_count;\n\n\t \n\tif (adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 2) &&\n\t    adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 4)) {\n\t\tif (amdgpu_ras_reset_error_status(adev, query_info->head.block))\n\t\t\tdev_warn(adev->dev,\n\t\t\t\t \"Failed to reset error counter and error status\\n\");\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_ras_query_error_count(struct amdgpu_device *adev,\n\t\t\t\t unsigned long *ce_count,\n\t\t\t\t unsigned long *ue_count,\n\t\t\t\t struct ras_query_if *query_info)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj;\n\tunsigned long ce, ue;\n\tint ret;\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (!ce_count && !ue_count)\n\t\treturn 0;\n\n\tce = 0;\n\tue = 0;\n\tif (!query_info) {\n\t\t \n\t\tlist_for_each_entry(obj, &con->head, node) {\n\t\t\tstruct ras_query_if info = {\n\t\t\t\t.head = obj->head,\n\t\t\t};\n\n\t\t\tret = amdgpu_ras_query_error_count_helper(adev, &ce, &ue, &info);\n\t\t}\n\t} else {\n\t\t \n\t\tret = amdgpu_ras_query_error_count_helper(adev, &ce, &ue, query_info);\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (ce_count)\n\t\t*ce_count = ce;\n\n\tif (ue_count)\n\t\t*ue_count = ue;\n\n\treturn 0;\n}\n \n\n\n \n\nstatic int amdgpu_ras_badpages_read(struct amdgpu_device *adev,\n\t\tstruct ras_badpage **bps, unsigned int *count);\n\nstatic char *amdgpu_ras_badpage_flags_str(unsigned int flags)\n{\n\tswitch (flags) {\n\tcase AMDGPU_RAS_RETIRE_PAGE_RESERVED:\n\t\treturn \"R\";\n\tcase AMDGPU_RAS_RETIRE_PAGE_PENDING:\n\t\treturn \"P\";\n\tcase AMDGPU_RAS_RETIRE_PAGE_FAULT:\n\tdefault:\n\t\treturn \"F\";\n\t}\n}\n\n \n\nstatic ssize_t amdgpu_ras_sysfs_badpages_read(struct file *f,\n\t\tstruct kobject *kobj, struct bin_attribute *attr,\n\t\tchar *buf, loff_t ppos, size_t count)\n{\n\tstruct amdgpu_ras *con =\n\t\tcontainer_of(attr, struct amdgpu_ras, badpages_attr);\n\tstruct amdgpu_device *adev = con->adev;\n\tconst unsigned int element_size =\n\t\tsizeof(\"0xabcdabcd : 0x12345678 : R\\n\") - 1;\n\tunsigned int start = div64_ul(ppos + element_size - 1, element_size);\n\tunsigned int end = div64_ul(ppos + count - 1, element_size);\n\tssize_t s = 0;\n\tstruct ras_badpage *bps = NULL;\n\tunsigned int bps_count = 0;\n\n\tmemset(buf, 0, count);\n\n\tif (amdgpu_ras_badpages_read(adev, &bps, &bps_count))\n\t\treturn 0;\n\n\tfor (; start < end && start < bps_count; start++)\n\t\ts += scnprintf(&buf[s], element_size + 1,\n\t\t\t\t\"0x%08x : 0x%08x : %1s\\n\",\n\t\t\t\tbps[start].bp,\n\t\t\t\tbps[start].size,\n\t\t\t\tamdgpu_ras_badpage_flags_str(bps[start].flags));\n\n\tkfree(bps);\n\n\treturn s;\n}\n\nstatic ssize_t amdgpu_ras_sysfs_features_read(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct amdgpu_ras *con =\n\t\tcontainer_of(attr, struct amdgpu_ras, features_attr);\n\n\treturn sysfs_emit(buf, \"feature mask: 0x%x\\n\", con->features);\n}\n\nstatic void amdgpu_ras_sysfs_remove_bad_page_node(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\tif (adev->dev->kobj.sd)\n\t\tsysfs_remove_file_from_group(&adev->dev->kobj,\n\t\t\t\t&con->badpages_attr.attr,\n\t\t\t\tRAS_FS_NAME);\n}\n\nstatic int amdgpu_ras_sysfs_remove_feature_node(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct attribute *attrs[] = {\n\t\t&con->features_attr.attr,\n\t\tNULL\n\t};\n\tstruct attribute_group group = {\n\t\t.name = RAS_FS_NAME,\n\t\t.attrs = attrs,\n\t};\n\n\tif (adev->dev->kobj.sd)\n\t\tsysfs_remove_group(&adev->dev->kobj, &group);\n\n\treturn 0;\n}\n\nint amdgpu_ras_sysfs_create(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, head);\n\n\tif (!obj || obj->attr_inuse)\n\t\treturn -EINVAL;\n\n\tget_obj(obj);\n\n\tsnprintf(obj->fs_data.sysfs_name, sizeof(obj->fs_data.sysfs_name),\n\t\t\"%s_err_count\", head->name);\n\n\tobj->sysfs_attr = (struct device_attribute){\n\t\t.attr = {\n\t\t\t.name = obj->fs_data.sysfs_name,\n\t\t\t.mode = S_IRUGO,\n\t\t},\n\t\t\t.show = amdgpu_ras_sysfs_read,\n\t};\n\tsysfs_attr_init(&obj->sysfs_attr.attr);\n\n\tif (sysfs_add_file_to_group(&adev->dev->kobj,\n\t\t\t\t&obj->sysfs_attr.attr,\n\t\t\t\tRAS_FS_NAME)) {\n\t\tput_obj(obj);\n\t\treturn -EINVAL;\n\t}\n\n\tobj->attr_inuse = 1;\n\n\treturn 0;\n}\n\nint amdgpu_ras_sysfs_remove(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, head);\n\n\tif (!obj || !obj->attr_inuse)\n\t\treturn -EINVAL;\n\n\tif (adev->dev->kobj.sd)\n\t\tsysfs_remove_file_from_group(&adev->dev->kobj,\n\t\t\t\t&obj->sysfs_attr.attr,\n\t\t\t\tRAS_FS_NAME);\n\tobj->attr_inuse = 0;\n\tput_obj(obj);\n\n\treturn 0;\n}\n\nstatic int amdgpu_ras_sysfs_remove_all(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj, *tmp;\n\n\tlist_for_each_entry_safe(obj, tmp, &con->head, node) {\n\t\tamdgpu_ras_sysfs_remove(adev, &obj->head);\n\t}\n\n\tif (amdgpu_bad_page_threshold != 0)\n\t\tamdgpu_ras_sysfs_remove_bad_page_node(adev);\n\n\tamdgpu_ras_sysfs_remove_feature_node(adev);\n\n\treturn 0;\n}\n \n\n \n \nstatic struct dentry *amdgpu_ras_debugfs_create_ctrl_node(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct amdgpu_ras_eeprom_control *eeprom = &con->eeprom_control;\n\tstruct drm_minor  *minor = adev_to_drm(adev)->primary;\n\tstruct dentry     *dir;\n\n\tdir = debugfs_create_dir(RAS_FS_NAME, minor->debugfs_root);\n\tdebugfs_create_file(\"ras_ctrl\", S_IWUGO | S_IRUGO, dir, adev,\n\t\t\t    &amdgpu_ras_debugfs_ctrl_ops);\n\tdebugfs_create_file(\"ras_eeprom_reset\", S_IWUGO | S_IRUGO, dir, adev,\n\t\t\t    &amdgpu_ras_debugfs_eeprom_ops);\n\tdebugfs_create_u32(\"bad_page_cnt_threshold\", 0444, dir,\n\t\t\t   &con->bad_page_cnt_threshold);\n\tdebugfs_create_u32(\"ras_num_recs\", 0444, dir, &eeprom->ras_num_recs);\n\tdebugfs_create_x32(\"ras_hw_enabled\", 0444, dir, &adev->ras_hw_enabled);\n\tdebugfs_create_x32(\"ras_enabled\", 0444, dir, &adev->ras_enabled);\n\tdebugfs_create_file(\"ras_eeprom_size\", S_IRUGO, dir, adev,\n\t\t\t    &amdgpu_ras_debugfs_eeprom_size_ops);\n\tcon->de_ras_eeprom_table = debugfs_create_file(\"ras_eeprom_table\",\n\t\t\t\t\t\t       S_IRUGO, dir, adev,\n\t\t\t\t\t\t       &amdgpu_ras_debugfs_eeprom_table_ops);\n\tamdgpu_ras_debugfs_set_ret_size(&con->eeprom_control);\n\n\t \n\tdebugfs_create_bool(\"auto_reboot\", S_IWUGO | S_IRUGO, dir, &con->reboot);\n\n\t \n\tdebugfs_create_bool(\"disable_ras_err_cnt_harvest\", 0644, dir,\n\t\t\t    &con->disable_ras_err_cnt_harvest);\n\treturn dir;\n}\n\nstatic void amdgpu_ras_debugfs_create(struct amdgpu_device *adev,\n\t\t\t\t      struct ras_fs_if *head,\n\t\t\t\t      struct dentry *dir)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, &head->head);\n\n\tif (!obj || !dir)\n\t\treturn;\n\n\tget_obj(obj);\n\n\tmemcpy(obj->fs_data.debugfs_name,\n\t\t\thead->debugfs_name,\n\t\t\tsizeof(obj->fs_data.debugfs_name));\n\n\tdebugfs_create_file(obj->fs_data.debugfs_name, S_IWUGO | S_IRUGO, dir,\n\t\t\t    obj, &amdgpu_ras_debugfs_ops);\n}\n\nvoid amdgpu_ras_debugfs_create_all(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct dentry *dir;\n\tstruct ras_manager *obj;\n\tstruct ras_fs_if fs_info;\n\n\t \n\tif (!IS_ENABLED(CONFIG_DEBUG_FS) || !con)\n\t\treturn;\n\n\tdir = amdgpu_ras_debugfs_create_ctrl_node(adev);\n\n\tlist_for_each_entry(obj, &con->head, node) {\n\t\tif (amdgpu_ras_is_supported(adev, obj->head.block) &&\n\t\t\t(obj->attr_inuse == 1)) {\n\t\t\tsprintf(fs_info.debugfs_name, \"%s_err_inject\",\n\t\t\t\t\tget_ras_block_str(&obj->head));\n\t\t\tfs_info.head = obj->head;\n\t\t\tamdgpu_ras_debugfs_create(adev, &fs_info, dir);\n\t\t}\n\t}\n}\n\n \n\n \nstatic BIN_ATTR(gpu_vram_bad_pages, S_IRUGO,\n\t\tamdgpu_ras_sysfs_badpages_read, NULL, 0);\nstatic DEVICE_ATTR(features, S_IRUGO,\n\t\tamdgpu_ras_sysfs_features_read, NULL);\nstatic int amdgpu_ras_fs_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct attribute_group group = {\n\t\t.name = RAS_FS_NAME,\n\t};\n\tstruct attribute *attrs[] = {\n\t\t&con->features_attr.attr,\n\t\tNULL\n\t};\n\tstruct bin_attribute *bin_attrs[] = {\n\t\tNULL,\n\t\tNULL,\n\t};\n\tint r;\n\n\t \n\tcon->features_attr = dev_attr_features;\n\tgroup.attrs = attrs;\n\tsysfs_attr_init(attrs[0]);\n\n\tif (amdgpu_bad_page_threshold != 0) {\n\t\t \n\t\tbin_attr_gpu_vram_bad_pages.private = NULL;\n\t\tcon->badpages_attr = bin_attr_gpu_vram_bad_pages;\n\t\tbin_attrs[0] = &con->badpages_attr;\n\t\tgroup.bin_attrs = bin_attrs;\n\t\tsysfs_bin_attr_init(bin_attrs[0]);\n\t}\n\n\tr = sysfs_create_group(&adev->dev->kobj, &group);\n\tif (r)\n\t\tdev_err(adev->dev, \"Failed to create RAS sysfs group!\");\n\n\treturn 0;\n}\n\nstatic int amdgpu_ras_fs_fini(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *con_obj, *ip_obj, *tmp;\n\n\tif (IS_ENABLED(CONFIG_DEBUG_FS)) {\n\t\tlist_for_each_entry_safe(con_obj, tmp, &con->head, node) {\n\t\t\tip_obj = amdgpu_ras_find_obj(adev, &con_obj->head);\n\t\t\tif (ip_obj)\n\t\t\t\tput_obj(ip_obj);\n\t\t}\n\t}\n\n\tamdgpu_ras_sysfs_remove_all(adev);\n\treturn 0;\n}\n \n\n \n\n \nvoid amdgpu_ras_interrupt_fatal_error_handler(struct amdgpu_device *adev)\n{\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn;\n\n\tif (adev->nbio.ras &&\n\t    adev->nbio.ras->handle_ras_controller_intr_no_bifring)\n\t\tadev->nbio.ras->handle_ras_controller_intr_no_bifring(adev);\n\n\tif (adev->nbio.ras &&\n\t    adev->nbio.ras->handle_ras_err_event_athub_intr_no_bifring)\n\t\tadev->nbio.ras->handle_ras_err_event_athub_intr_no_bifring(adev);\n}\n\nstatic void amdgpu_ras_interrupt_poison_consumption_handler(struct ras_manager *obj,\n\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tbool poison_stat = false;\n\tstruct amdgpu_device *adev = obj->adev;\n\tstruct amdgpu_ras_block_object *block_obj =\n\t\tamdgpu_ras_get_ras_block(adev, obj->head.block, 0);\n\n\tif (!block_obj)\n\t\treturn;\n\n\t \n\tif (block_obj->hw_ops && block_obj->hw_ops->query_poison_status) {\n\t\tpoison_stat = block_obj->hw_ops->query_poison_status(adev);\n\t\tif (!poison_stat) {\n\t\t\t \n\t\t\tdev_info(adev->dev, \"No RAS poison status in %s poison IH.\\n\",\n\t\t\t\t\tblock_obj->ras_comm.name);\n\n\t\t\treturn;\n\t\t}\n\t}\n\n\tamdgpu_umc_poison_handler(adev, false);\n\n\tif (block_obj->hw_ops && block_obj->hw_ops->handle_poison_consumption)\n\t\tpoison_stat = block_obj->hw_ops->handle_poison_consumption(adev);\n\n\t \n\tif (poison_stat) {\n\t\tdev_info(adev->dev, \"GPU reset for %s RAS poison consumption is issued!\\n\",\n\t\t\t\tblock_obj->ras_comm.name);\n\t\tamdgpu_ras_reset_gpu(adev);\n\t} else {\n\t\tamdgpu_gfx_poison_consumption_handler(adev, entry);\n\t}\n}\n\nstatic void amdgpu_ras_interrupt_poison_creation_handler(struct ras_manager *obj,\n\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tdev_info(obj->adev->dev,\n\t\t\"Poison is created, no user action is needed.\\n\");\n}\n\nstatic void amdgpu_ras_interrupt_umc_handler(struct ras_manager *obj,\n\t\t\t\tstruct amdgpu_iv_entry *entry)\n{\n\tstruct ras_ih_data *data = &obj->ih_data;\n\tstruct ras_err_data err_data = {0, 0, 0, NULL};\n\tint ret;\n\n\tif (!data->cb)\n\t\treturn;\n\n\t \n\tret = data->cb(obj->adev, &err_data, entry);\n\t \n\tif (ret == AMDGPU_RAS_SUCCESS) {\n\t\t \n\t\tobj->err_data.ue_count += err_data.ue_count;\n\t\tobj->err_data.ce_count += err_data.ce_count;\n\t}\n}\n\nstatic void amdgpu_ras_interrupt_handler(struct ras_manager *obj)\n{\n\tstruct ras_ih_data *data = &obj->ih_data;\n\tstruct amdgpu_iv_entry entry;\n\n\twhile (data->rptr != data->wptr) {\n\t\trmb();\n\t\tmemcpy(&entry, &data->ring[data->rptr],\n\t\t\t\tdata->element_size);\n\n\t\twmb();\n\t\tdata->rptr = (data->aligned_element_size +\n\t\t\t\tdata->rptr) % data->ring_size;\n\n\t\tif (amdgpu_ras_is_poison_mode_supported(obj->adev)) {\n\t\t\tif (obj->head.block == AMDGPU_RAS_BLOCK__UMC)\n\t\t\t\tamdgpu_ras_interrupt_poison_creation_handler(obj, &entry);\n\t\t\telse\n\t\t\t\tamdgpu_ras_interrupt_poison_consumption_handler(obj, &entry);\n\t\t} else {\n\t\t\tif (obj->head.block == AMDGPU_RAS_BLOCK__UMC)\n\t\t\t\tamdgpu_ras_interrupt_umc_handler(obj, &entry);\n\t\t\telse\n\t\t\t\tdev_warn(obj->adev->dev,\n\t\t\t\t\t\"No RAS interrupt handler for non-UMC block with poison disabled.\\n\");\n\t\t}\n\t}\n}\n\nstatic void amdgpu_ras_interrupt_process_handler(struct work_struct *work)\n{\n\tstruct ras_ih_data *data =\n\t\tcontainer_of(work, struct ras_ih_data, ih_work);\n\tstruct ras_manager *obj =\n\t\tcontainer_of(data, struct ras_manager, ih_data);\n\n\tamdgpu_ras_interrupt_handler(obj);\n}\n\nint amdgpu_ras_interrupt_dispatch(struct amdgpu_device *adev,\n\t\tstruct ras_dispatch_if *info)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, &info->head);\n\tstruct ras_ih_data *data = &obj->ih_data;\n\n\tif (!obj)\n\t\treturn -EINVAL;\n\n\tif (data->inuse == 0)\n\t\treturn 0;\n\n\t \n\tmemcpy(&data->ring[data->wptr], info->entry,\n\t\t\tdata->element_size);\n\n\twmb();\n\tdata->wptr = (data->aligned_element_size +\n\t\t\tdata->wptr) % data->ring_size;\n\n\tschedule_work(&data->ih_work);\n\n\treturn 0;\n}\n\nint amdgpu_ras_interrupt_remove_handler(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, head);\n\tstruct ras_ih_data *data;\n\n\tif (!obj)\n\t\treturn -EINVAL;\n\n\tdata = &obj->ih_data;\n\tif (data->inuse == 0)\n\t\treturn 0;\n\n\tcancel_work_sync(&data->ih_work);\n\n\tkfree(data->ring);\n\tmemset(data, 0, sizeof(*data));\n\tput_obj(obj);\n\n\treturn 0;\n}\n\nint amdgpu_ras_interrupt_add_handler(struct amdgpu_device *adev,\n\t\tstruct ras_common_if *head)\n{\n\tstruct ras_manager *obj = amdgpu_ras_find_obj(adev, head);\n\tstruct ras_ih_data *data;\n\tstruct amdgpu_ras_block_object *ras_obj;\n\n\tif (!obj) {\n\t\t \n\t\tobj = amdgpu_ras_create_obj(adev, head);\n\t\tif (!obj)\n\t\t\treturn -EINVAL;\n\t} else\n\t\tget_obj(obj);\n\n\tras_obj = container_of(head, struct amdgpu_ras_block_object, ras_comm);\n\n\tdata = &obj->ih_data;\n\t \n\t*data = (struct ras_ih_data) {\n\t\t.inuse = 0,\n\t\t.cb = ras_obj->ras_cb,\n\t\t.element_size = sizeof(struct amdgpu_iv_entry),\n\t\t.rptr = 0,\n\t\t.wptr = 0,\n\t};\n\n\tINIT_WORK(&data->ih_work, amdgpu_ras_interrupt_process_handler);\n\n\tdata->aligned_element_size = ALIGN(data->element_size, 8);\n\t \n\tdata->ring_size = 64 * data->aligned_element_size;\n\tdata->ring = kmalloc(data->ring_size, GFP_KERNEL);\n\tif (!data->ring) {\n\t\tput_obj(obj);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tdata->inuse = 1;\n\n\treturn 0;\n}\n\nstatic int amdgpu_ras_interrupt_remove_all(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj, *tmp;\n\n\tlist_for_each_entry_safe(obj, tmp, &con->head, node) {\n\t\tamdgpu_ras_interrupt_remove_handler(adev, &obj->head);\n\t}\n\n\treturn 0;\n}\n \n\n \nstatic void amdgpu_ras_log_on_err_counter(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj;\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn;\n\n\tlist_for_each_entry(obj, &con->head, node) {\n\t\tstruct ras_query_if info = {\n\t\t\t.head = obj->head,\n\t\t};\n\n\t\t \n\t\tif (info.head.block == AMDGPU_RAS_BLOCK__PCIE_BIF)\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((info.head.block == AMDGPU_RAS_BLOCK__UMC) &&\n\t\t\t(adev->ip_versions[MP1_HWIP][0] == IP_VERSION(13, 0, 2)))\n\t\t\tcontinue;\n\n\t\tamdgpu_ras_query_error_status(adev, &info);\n\n\t\tif (adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 2) &&\n\t\t    adev->ip_versions[MP0_HWIP][0] != IP_VERSION(11, 0, 4) &&\n\t\t    adev->ip_versions[MP0_HWIP][0] != IP_VERSION(13, 0, 0)) {\n\t\t\tif (amdgpu_ras_reset_error_status(adev, info.head.block))\n\t\t\t\tdev_warn(adev->dev, \"Failed to reset error counter and error status\");\n\t\t}\n\t}\n}\n\n \nstatic void amdgpu_ras_error_status_query(struct amdgpu_device *adev,\n\t\t\t\t\t  struct ras_query_if *info)\n{\n\tstruct amdgpu_ras_block_object *block_obj;\n\t \n\tif ((info->head.block != AMDGPU_RAS_BLOCK__GFX) &&\n\t\t(info->head.block != AMDGPU_RAS_BLOCK__MMHUB))\n\t\treturn;\n\n\tblock_obj = amdgpu_ras_get_ras_block(adev,\n\t\t\t\t\tinfo->head.block,\n\t\t\t\t\tinfo->head.sub_block_index);\n\n\tif (!block_obj || !block_obj->hw_ops) {\n\t\tdev_dbg_once(adev->dev, \"%s doesn't config RAS function\\n\",\n\t\t\t     get_ras_block_str(&info->head));\n\t\treturn;\n\t}\n\n\tif (block_obj->hw_ops->query_ras_error_status)\n\t\tblock_obj->hw_ops->query_ras_error_status(adev);\n\n}\n\nstatic void amdgpu_ras_query_err_status(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj;\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn;\n\n\tlist_for_each_entry(obj, &con->head, node) {\n\t\tstruct ras_query_if info = {\n\t\t\t.head = obj->head,\n\t\t};\n\n\t\tamdgpu_ras_error_status_query(adev, &info);\n\t}\n}\n\n \n\n \nstatic int amdgpu_ras_badpages_read(struct amdgpu_device *adev,\n\t\tstruct ras_badpage **bps, unsigned int *count)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_err_handler_data *data;\n\tint i = 0;\n\tint ret = 0, status;\n\n\tif (!con || !con->eh_data || !bps || !count)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&con->recovery_lock);\n\tdata = con->eh_data;\n\tif (!data || data->count == 0) {\n\t\t*bps = NULL;\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t*bps = kmalloc(sizeof(struct ras_badpage) * data->count, GFP_KERNEL);\n\tif (!*bps) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (; i < data->count; i++) {\n\t\t(*bps)[i] = (struct ras_badpage){\n\t\t\t.bp = data->bps[i].retired_page,\n\t\t\t.size = AMDGPU_GPU_PAGE_SIZE,\n\t\t\t.flags = AMDGPU_RAS_RETIRE_PAGE_RESERVED,\n\t\t};\n\t\tstatus = amdgpu_vram_mgr_query_page_status(&adev->mman.vram_mgr,\n\t\t\t\tdata->bps[i].retired_page);\n\t\tif (status == -EBUSY)\n\t\t\t(*bps)[i].flags = AMDGPU_RAS_RETIRE_PAGE_PENDING;\n\t\telse if (status == -ENOENT)\n\t\t\t(*bps)[i].flags = AMDGPU_RAS_RETIRE_PAGE_FAULT;\n\t}\n\n\t*count = data->count;\nout:\n\tmutex_unlock(&con->recovery_lock);\n\treturn ret;\n}\n\nstatic void amdgpu_ras_do_recovery(struct work_struct *work)\n{\n\tstruct amdgpu_ras *ras =\n\t\tcontainer_of(work, struct amdgpu_ras, recovery_work);\n\tstruct amdgpu_device *remote_adev = NULL;\n\tstruct amdgpu_device *adev = ras->adev;\n\tstruct list_head device_list, *device_list_handle =  NULL;\n\n\tif (!ras->disable_ras_err_cnt_harvest) {\n\t\tstruct amdgpu_hive_info *hive = amdgpu_get_xgmi_hive(adev);\n\n\t\t \n\t\tif  (hive && adev->gmc.xgmi.num_physical_nodes > 1) {\n\t\t\tdevice_list_handle = &hive->device_list;\n\t\t} else {\n\t\t\tINIT_LIST_HEAD(&device_list);\n\t\t\tlist_add_tail(&adev->gmc.xgmi.head, &device_list);\n\t\t\tdevice_list_handle = &device_list;\n\t\t}\n\n\t\tlist_for_each_entry(remote_adev,\n\t\t\t\tdevice_list_handle, gmc.xgmi.head) {\n\t\t\tamdgpu_ras_query_err_status(remote_adev);\n\t\t\tamdgpu_ras_log_on_err_counter(remote_adev);\n\t\t}\n\n\t\tamdgpu_put_xgmi_hive(hive);\n\t}\n\n\tif (amdgpu_device_should_recover_gpu(ras->adev)) {\n\t\tstruct amdgpu_reset_context reset_context;\n\t\tmemset(&reset_context, 0, sizeof(reset_context));\n\n\t\treset_context.method = AMD_RESET_METHOD_NONE;\n\t\treset_context.reset_req_dev = adev;\n\n\t\t \n\t\tif (!amdgpu_ras_is_poison_mode_supported(ras->adev))\n\t\t\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\t\telse {\n\t\t\tclear_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\t\t\tif (ras->gpu_reset_flags & AMDGPU_RAS_GPU_RESET_MODE2_RESET) {\n\t\t\t\tras->gpu_reset_flags &= ~AMDGPU_RAS_GPU_RESET_MODE2_RESET;\n\t\t\t\treset_context.method = AMD_RESET_METHOD_MODE2;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (ras->gpu_reset_flags & AMDGPU_RAS_GPU_RESET_MODE1_RESET) {\n\t\t\t\tras->gpu_reset_flags &= ~AMDGPU_RAS_GPU_RESET_MODE1_RESET;\n\t\t\t\tset_bit(AMDGPU_NEED_FULL_RESET, &reset_context.flags);\n\n\t\t\t\tpsp_fatal_error_recovery_quirk(&adev->psp);\n\t\t\t}\n\t\t}\n\n\t\tamdgpu_device_gpu_recover(ras->adev, NULL, &reset_context);\n\t}\n\tatomic_set(&ras->in_recovery, 0);\n}\n\n \nstatic int amdgpu_ras_realloc_eh_data_space(struct amdgpu_device *adev,\n\t\tstruct ras_err_handler_data *data, int pages)\n{\n\tunsigned int old_space = data->count + data->space_left;\n\tunsigned int new_space = old_space + pages;\n\tunsigned int align_space = ALIGN(new_space, 512);\n\tvoid *bps = kmalloc(align_space * sizeof(*data->bps), GFP_KERNEL);\n\n\tif (!bps) {\n\t\treturn -ENOMEM;\n\t}\n\n\tif (data->bps) {\n\t\tmemcpy(bps, data->bps,\n\t\t\t\tdata->count * sizeof(*data->bps));\n\t\tkfree(data->bps);\n\t}\n\n\tdata->bps = bps;\n\tdata->space_left += align_space - old_space;\n\treturn 0;\n}\n\n \nint amdgpu_ras_add_bad_pages(struct amdgpu_device *adev,\n\t\tstruct eeprom_table_record *bps, int pages)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_err_handler_data *data;\n\tint ret = 0;\n\tuint32_t i;\n\n\tif (!con || !con->eh_data || !bps || pages <= 0)\n\t\treturn 0;\n\n\tmutex_lock(&con->recovery_lock);\n\tdata = con->eh_data;\n\tif (!data)\n\t\tgoto out;\n\n\tfor (i = 0; i < pages; i++) {\n\t\tif (amdgpu_ras_check_bad_page_unlock(con,\n\t\t\tbps[i].retired_page << AMDGPU_GPU_PAGE_SHIFT))\n\t\t\tcontinue;\n\n\t\tif (!data->space_left &&\n\t\t\tamdgpu_ras_realloc_eh_data_space(adev, data, 256)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tamdgpu_vram_mgr_reserve_range(&adev->mman.vram_mgr,\n\t\t\tbps[i].retired_page << AMDGPU_GPU_PAGE_SHIFT,\n\t\t\tAMDGPU_GPU_PAGE_SIZE);\n\n\t\tmemcpy(&data->bps[data->count], &bps[i], sizeof(*data->bps));\n\t\tdata->count++;\n\t\tdata->space_left--;\n\t}\nout:\n\tmutex_unlock(&con->recovery_lock);\n\n\treturn ret;\n}\n\n \nint amdgpu_ras_save_bad_pages(struct amdgpu_device *adev,\n\t\tunsigned long *new_cnt)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_err_handler_data *data;\n\tstruct amdgpu_ras_eeprom_control *control;\n\tint save_count;\n\n\tif (!con || !con->eh_data) {\n\t\tif (new_cnt)\n\t\t\t*new_cnt = 0;\n\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&con->recovery_lock);\n\tcontrol = &con->eeprom_control;\n\tdata = con->eh_data;\n\tsave_count = data->count - control->ras_num_recs;\n\tmutex_unlock(&con->recovery_lock);\n\n\tif (new_cnt)\n\t\t*new_cnt = save_count / adev->umc.retire_unit;\n\n\t \n\tif (save_count > 0) {\n\t\tif (amdgpu_ras_eeprom_append(control,\n\t\t\t\t\t     &data->bps[control->ras_num_recs],\n\t\t\t\t\t     save_count)) {\n\t\t\tdev_err(adev->dev, \"Failed to save EEPROM table data!\");\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tdev_info(adev->dev, \"Saved %d pages to EEPROM table.\\n\", save_count);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int amdgpu_ras_load_bad_pages(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras_eeprom_control *control =\n\t\t&adev->psp.ras_context.ras->eeprom_control;\n\tstruct eeprom_table_record *bps;\n\tint ret;\n\n\t \n\tif (control->ras_num_recs == 0 || amdgpu_bad_page_threshold == 0)\n\t\treturn 0;\n\n\tbps = kcalloc(control->ras_num_recs, sizeof(*bps), GFP_KERNEL);\n\tif (!bps)\n\t\treturn -ENOMEM;\n\n\tret = amdgpu_ras_eeprom_read(control, bps, control->ras_num_recs);\n\tif (ret)\n\t\tdev_err(adev->dev, \"Failed to load EEPROM table records!\");\n\telse\n\t\tret = amdgpu_ras_add_bad_pages(adev, bps, control->ras_num_recs);\n\n\tkfree(bps);\n\treturn ret;\n}\n\nstatic bool amdgpu_ras_check_bad_page_unlock(struct amdgpu_ras *con,\n\t\t\t\tuint64_t addr)\n{\n\tstruct ras_err_handler_data *data = con->eh_data;\n\tint i;\n\n\taddr >>= AMDGPU_GPU_PAGE_SHIFT;\n\tfor (i = 0; i < data->count; i++)\n\t\tif (addr == data->bps[i].retired_page)\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool amdgpu_ras_check_bad_page(struct amdgpu_device *adev,\n\t\t\t\tuint64_t addr)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tbool ret = false;\n\n\tif (!con || !con->eh_data)\n\t\treturn ret;\n\n\tmutex_lock(&con->recovery_lock);\n\tret = amdgpu_ras_check_bad_page_unlock(con, addr);\n\tmutex_unlock(&con->recovery_lock);\n\treturn ret;\n}\n\nstatic void amdgpu_ras_validate_threshold(struct amdgpu_device *adev,\n\t\t\t\t\t  uint32_t max_count)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\t \n\n\tif (amdgpu_bad_page_threshold < 0) {\n\t\tu64 val = adev->gmc.mc_vram_size;\n\n\t\tdo_div(val, RAS_BAD_PAGE_COVER);\n\t\tcon->bad_page_cnt_threshold = min(lower_32_bits(val),\n\t\t\t\t\t\t  max_count);\n\t} else {\n\t\tcon->bad_page_cnt_threshold = min_t(int, max_count,\n\t\t\t\t\t\t    amdgpu_bad_page_threshold);\n\t}\n}\n\nint amdgpu_ras_recovery_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_err_handler_data **data;\n\tu32  max_eeprom_records_count = 0;\n\tbool exc_err_limit = false;\n\tint ret;\n\n\tif (!con || amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\t \n\tcon->adev = adev;\n\n\tif (!adev->ras_enabled)\n\t\treturn 0;\n\n\tdata = &con->eh_data;\n\t*data = kmalloc(sizeof(**data), GFP_KERNEL | __GFP_ZERO);\n\tif (!*data) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmutex_init(&con->recovery_lock);\n\tINIT_WORK(&con->recovery_work, amdgpu_ras_do_recovery);\n\tatomic_set(&con->in_recovery, 0);\n\tcon->eeprom_control.bad_channel_bitmap = 0;\n\n\tmax_eeprom_records_count = amdgpu_ras_eeprom_max_record_count(&con->eeprom_control);\n\tamdgpu_ras_validate_threshold(adev, max_eeprom_records_count);\n\n\t \n\tif (adev->gmc.xgmi.pending_reset)\n\t\treturn 0;\n\tret = amdgpu_ras_eeprom_init(&con->eeprom_control, &exc_err_limit);\n\t \n\tif (exc_err_limit || ret)\n\t\tgoto free;\n\n\tif (con->eeprom_control.ras_num_recs) {\n\t\tret = amdgpu_ras_load_bad_pages(adev);\n\t\tif (ret)\n\t\t\tgoto free;\n\n\t\tamdgpu_dpm_send_hbm_bad_pages_num(adev, con->eeprom_control.ras_num_recs);\n\n\t\tif (con->update_channel_flag == true) {\n\t\t\tamdgpu_dpm_send_hbm_bad_channel_flag(adev, con->eeprom_control.bad_channel_bitmap);\n\t\t\tcon->update_channel_flag = false;\n\t\t}\n\t}\n\n#ifdef CONFIG_X86_MCE_AMD\n\tif ((adev->asic_type == CHIP_ALDEBARAN) &&\n\t    (adev->gmc.xgmi.connected_to_cpu))\n\t\tamdgpu_register_bad_pages_mca_notifier(adev);\n#endif\n\treturn 0;\n\nfree:\n\tkfree((*data)->bps);\n\tkfree(*data);\n\tcon->eh_data = NULL;\nout:\n\tdev_warn(adev->dev, \"Failed to initialize ras recovery! (%d)\\n\", ret);\n\n\t \n\tif (!exc_err_limit)\n\t\tret = 0;\n\telse\n\t\tret = -EINVAL;\n\n\treturn ret;\n}\n\nstatic int amdgpu_ras_recovery_fini(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_err_handler_data *data = con->eh_data;\n\n\t \n\tif (!data)\n\t\treturn 0;\n\n\tcancel_work_sync(&con->recovery_work);\n\n\tmutex_lock(&con->recovery_lock);\n\tcon->eh_data = NULL;\n\tkfree(data->bps);\n\tkfree(data);\n\tmutex_unlock(&con->recovery_lock);\n\n\treturn 0;\n}\n \n\nstatic bool amdgpu_ras_asic_supported(struct amdgpu_device *adev)\n{\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tswitch (adev->ip_versions[MP0_HWIP][0]) {\n\t\tcase IP_VERSION(13, 0, 2):\n\t\tcase IP_VERSION(13, 0, 6):\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (adev->asic_type == CHIP_IP_DISCOVERY) {\n\t\tswitch (adev->ip_versions[MP0_HWIP][0]) {\n\t\tcase IP_VERSION(13, 0, 0):\n\t\tcase IP_VERSION(13, 0, 6):\n\t\tcase IP_VERSION(13, 0, 10):\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn adev->asic_type == CHIP_VEGA10 ||\n\t\tadev->asic_type == CHIP_VEGA20 ||\n\t\tadev->asic_type == CHIP_ARCTURUS ||\n\t\tadev->asic_type == CHIP_ALDEBARAN ||\n\t\tadev->asic_type == CHIP_SIENNA_CICHLID;\n}\n\n \nstatic void amdgpu_ras_get_quirks(struct amdgpu_device *adev)\n{\n\tstruct atom_context *ctx = adev->mode_info.atom_context;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (strnstr(ctx->vbios_pn, \"D16406\",\n\t\t    sizeof(ctx->vbios_pn)) ||\n\t\tstrnstr(ctx->vbios_pn, \"D36002\",\n\t\t\tsizeof(ctx->vbios_pn)))\n\t\tadev->ras_hw_enabled |= (1 << AMDGPU_RAS_BLOCK__GFX);\n}\n\n \nstatic void amdgpu_ras_check_supported(struct amdgpu_device *adev)\n{\n\tadev->ras_hw_enabled = adev->ras_enabled = 0;\n\n\tif (!amdgpu_ras_asic_supported(adev))\n\t\treturn;\n\n\tif (!adev->gmc.xgmi.connected_to_cpu &&\t!adev->gmc.is_app_apu) {\n\t\tif (amdgpu_atomfirmware_mem_ecc_supported(adev)) {\n\t\t\tdev_info(adev->dev, \"MEM ECC is active.\\n\");\n\t\t\tadev->ras_hw_enabled |= (1 << AMDGPU_RAS_BLOCK__UMC |\n\t\t\t\t\t\t   1 << AMDGPU_RAS_BLOCK__DF);\n\t\t} else {\n\t\t\tdev_info(adev->dev, \"MEM ECC is not presented.\\n\");\n\t\t}\n\n\t\tif (amdgpu_atomfirmware_sram_ecc_supported(adev)) {\n\t\t\tdev_info(adev->dev, \"SRAM ECC is active.\\n\");\n\t\t\tif (!amdgpu_sriov_vf(adev))\n\t\t\t\tadev->ras_hw_enabled |= ~(1 << AMDGPU_RAS_BLOCK__UMC |\n\t\t\t\t\t\t\t    1 << AMDGPU_RAS_BLOCK__DF);\n\t\t\telse\n\t\t\t\tadev->ras_hw_enabled |= (1 << AMDGPU_RAS_BLOCK__PCIE_BIF |\n\t\t\t\t\t\t\t\t1 << AMDGPU_RAS_BLOCK__SDMA |\n\t\t\t\t\t\t\t\t1 << AMDGPU_RAS_BLOCK__GFX);\n\n\t\t\t \n\t\t\tif (adev->ip_versions[VCN_HWIP][0] == IP_VERSION(2, 6, 0) ||\n\t\t\t    adev->ip_versions[VCN_HWIP][0] == IP_VERSION(4, 0, 0))\n\t\t\t\tadev->ras_hw_enabled |= (1 << AMDGPU_RAS_BLOCK__VCN |\n\t\t\t\t\t\t\t1 << AMDGPU_RAS_BLOCK__JPEG);\n\t\t\telse\n\t\t\t\tadev->ras_hw_enabled &= ~(1 << AMDGPU_RAS_BLOCK__VCN |\n\t\t\t\t\t\t\t1 << AMDGPU_RAS_BLOCK__JPEG);\n\n\t\t\t \n\t\t\tif (!adev->gmc.xgmi.num_physical_nodes)\n\t\t\t\tadev->ras_hw_enabled &= ~(1 << AMDGPU_RAS_BLOCK__XGMI_WAFL);\n\t\t} else {\n\t\t\tdev_info(adev->dev, \"SRAM ECC is not presented.\\n\");\n\t\t}\n\t} else {\n\t\t \n\t\tadev->ras_hw_enabled |= (1 << AMDGPU_RAS_BLOCK__GFX |\n\t\t\t\t\t   1 << AMDGPU_RAS_BLOCK__SDMA |\n\t\t\t\t\t   1 << AMDGPU_RAS_BLOCK__MMHUB);\n\t}\n\n\tamdgpu_ras_get_quirks(adev);\n\n\t \n\tadev->ras_hw_enabled &= AMDGPU_RAS_BLOCK_MASK;\n\n\n\t \n\tif (adev->ip_versions[MP0_HWIP][0] == IP_VERSION(13, 0, 6) &&\n\t    adev->gmc.is_app_apu)\n\t\tadev->ras_enabled = amdgpu_ras_enable != 1 ? 0 :\n\t\t\tadev->ras_hw_enabled & amdgpu_ras_mask;\n\telse\n\t\tadev->ras_enabled = amdgpu_ras_enable == 0 ? 0 :\n\t\t\tadev->ras_hw_enabled & amdgpu_ras_mask;\n}\n\nstatic void amdgpu_ras_counte_dw(struct work_struct *work)\n{\n\tstruct amdgpu_ras *con = container_of(work, struct amdgpu_ras,\n\t\t\t\t\t      ras_counte_delay_work.work);\n\tstruct amdgpu_device *adev = con->adev;\n\tstruct drm_device *dev = adev_to_drm(adev);\n\tunsigned long ce_count, ue_count;\n\tint res;\n\n\tres = pm_runtime_get_sync(dev->dev);\n\tif (res < 0)\n\t\tgoto Out;\n\n\t \n\tif (amdgpu_ras_query_error_count(adev, &ce_count, &ue_count, NULL) == 0) {\n\t\tatomic_set(&con->ras_ce_count, ce_count);\n\t\tatomic_set(&con->ras_ue_count, ue_count);\n\t}\n\n\tpm_runtime_mark_last_busy(dev->dev);\nOut:\n\tpm_runtime_put_autosuspend(dev->dev);\n}\n\nstatic void amdgpu_ras_query_poison_mode(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tbool df_poison, umc_poison;\n\n\t \n\tif (amdgpu_sriov_vf(adev) || !con)\n\t\treturn;\n\n\t \n\tif (adev->gmc.xgmi.connected_to_cpu) {\n\t\t \n\t\tcon->poison_supported = true;\n\t} else if (adev->df.funcs &&\n\t    adev->df.funcs->query_ras_poison_mode &&\n\t    adev->umc.ras &&\n\t    adev->umc.ras->query_ras_poison_mode) {\n\t\tdf_poison =\n\t\t\tadev->df.funcs->query_ras_poison_mode(adev);\n\t\tumc_poison =\n\t\t\tadev->umc.ras->query_ras_poison_mode(adev);\n\n\t\t \n\t\tif (df_poison && umc_poison)\n\t\t\tcon->poison_supported = true;\n\t\telse if (df_poison != umc_poison)\n\t\t\tdev_warn(adev->dev,\n\t\t\t\t\"Poison setting is inconsistent in DF/UMC(%d:%d)!\\n\",\n\t\t\t\tdf_poison, umc_poison);\n\t}\n}\n\nint amdgpu_ras_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tint r;\n\n\tif (con)\n\t\treturn 0;\n\n\tcon = kmalloc(sizeof(struct amdgpu_ras) +\n\t\t\tsizeof(struct ras_manager) * AMDGPU_RAS_BLOCK_COUNT +\n\t\t\tsizeof(struct ras_manager) * AMDGPU_RAS_MCA_BLOCK_COUNT,\n\t\t\tGFP_KERNEL|__GFP_ZERO);\n\tif (!con)\n\t\treturn -ENOMEM;\n\n\tcon->adev = adev;\n\tINIT_DELAYED_WORK(&con->ras_counte_delay_work, amdgpu_ras_counte_dw);\n\tatomic_set(&con->ras_ce_count, 0);\n\tatomic_set(&con->ras_ue_count, 0);\n\n\tcon->objs = (struct ras_manager *)(con + 1);\n\n\tamdgpu_ras_set_context(adev, con);\n\n\tamdgpu_ras_check_supported(adev);\n\n\tif (!adev->ras_enabled || adev->asic_type == CHIP_VEGA10) {\n\t\t \n\t\tif (!adev->ras_enabled && adev->asic_type == CHIP_VEGA20) {\n\t\t\tcon->features |= BIT(AMDGPU_RAS_BLOCK__GFX);\n\n\t\t\treturn 0;\n\t\t}\n\n\t\tr = 0;\n\t\tgoto release_con;\n\t}\n\n\tcon->update_channel_flag = false;\n\tcon->features = 0;\n\tINIT_LIST_HEAD(&con->head);\n\t \n\tcon->flags = RAS_DEFAULT_FLAGS;\n\n\t \n\tswitch (adev->ip_versions[NBIO_HWIP][0]) {\n\tcase IP_VERSION(7, 4, 0):\n\tcase IP_VERSION(7, 4, 1):\n\tcase IP_VERSION(7, 4, 4):\n\t\tif (!adev->gmc.xgmi.connected_to_cpu)\n\t\t\tadev->nbio.ras = &nbio_v7_4_ras;\n\t\tbreak;\n\tcase IP_VERSION(4, 3, 0):\n\t\tif (adev->ras_hw_enabled & (1 << AMDGPU_RAS_BLOCK__DF))\n\t\t\t \n\t\t\tadev->nbio.ras = &nbio_v4_3_ras;\n\t\tbreak;\n\tcase IP_VERSION(7, 9, 0):\n\t\tif (!adev->gmc.is_app_apu)\n\t\t\tadev->nbio.ras = &nbio_v7_9_ras;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\t \n\tr = amdgpu_nbio_ras_sw_init(adev);\n\tif (r)\n\t\treturn r;\n\n\tif (adev->nbio.ras &&\n\t    adev->nbio.ras->init_ras_controller_interrupt) {\n\t\tr = adev->nbio.ras->init_ras_controller_interrupt(adev);\n\t\tif (r)\n\t\t\tgoto release_con;\n\t}\n\n\tif (adev->nbio.ras &&\n\t    adev->nbio.ras->init_ras_err_event_athub_interrupt) {\n\t\tr = adev->nbio.ras->init_ras_err_event_athub_interrupt(adev);\n\t\tif (r)\n\t\t\tgoto release_con;\n\t}\n\n\tamdgpu_ras_query_poison_mode(adev);\n\n\tif (amdgpu_ras_fs_init(adev)) {\n\t\tr = -EINVAL;\n\t\tgoto release_con;\n\t}\n\n\tdev_info(adev->dev, \"RAS INFO: ras initialized successfully, \"\n\t\t \"hardware ability[%x] ras_mask[%x]\\n\",\n\t\t adev->ras_hw_enabled, adev->ras_enabled);\n\n\treturn 0;\nrelease_con:\n\tamdgpu_ras_set_context(adev, NULL);\n\tkfree(con);\n\n\treturn r;\n}\n\nint amdgpu_persistent_edc_harvesting_supported(struct amdgpu_device *adev)\n{\n\tif (adev->gmc.xgmi.connected_to_cpu ||\n\t    adev->gmc.is_app_apu)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int amdgpu_persistent_edc_harvesting(struct amdgpu_device *adev,\n\t\t\t\t\tstruct ras_common_if *ras_block)\n{\n\tstruct ras_query_if info = {\n\t\t.head = *ras_block,\n\t};\n\n\tif (!amdgpu_persistent_edc_harvesting_supported(adev))\n\t\treturn 0;\n\n\tif (amdgpu_ras_query_error_status(adev, &info) != 0)\n\t\tDRM_WARN(\"RAS init harvest failure\");\n\n\tif (amdgpu_ras_reset_error_status(adev, ras_block->block) != 0)\n\t\tDRM_WARN(\"RAS init harvest reset failure\");\n\n\treturn 0;\n}\n\nbool amdgpu_ras_is_poison_mode_supported(struct amdgpu_device *adev)\n{\n       struct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n       if (!con)\n               return false;\n\n       return con->poison_supported;\n}\n\n \nint amdgpu_ras_block_late_init(struct amdgpu_device *adev,\n\t\t\t struct ras_common_if *ras_block)\n{\n\tstruct amdgpu_ras_block_object *ras_obj = NULL;\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_query_if *query_info;\n\tunsigned long ue_count, ce_count;\n\tint r;\n\n\t \n\tif (!amdgpu_ras_is_supported(adev, ras_block->block)) {\n\t\tamdgpu_ras_feature_enable_on_boot(adev, ras_block, 0);\n\t\treturn 0;\n\t}\n\n\tr = amdgpu_ras_feature_enable_on_boot(adev, ras_block, 1);\n\tif (r) {\n\t\tif (adev->in_suspend || amdgpu_in_reset(adev)) {\n\t\t\t \n\t\t\tgoto cleanup;\n\t\t} else\n\t\t\treturn r;\n\t}\n\n\t \n\tamdgpu_persistent_edc_harvesting(adev, ras_block);\n\n\t \n\tif (adev->in_suspend || amdgpu_in_reset(adev))\n\t\treturn 0;\n\n\tras_obj = container_of(ras_block, struct amdgpu_ras_block_object, ras_comm);\n\tif (ras_obj->ras_cb || (ras_obj->hw_ops &&\n\t    (ras_obj->hw_ops->query_poison_status ||\n\t    ras_obj->hw_ops->handle_poison_consumption))) {\n\t\tr = amdgpu_ras_interrupt_add_handler(adev, ras_block);\n\t\tif (r)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (ras_obj->hw_ops &&\n\t    (ras_obj->hw_ops->query_ras_error_count ||\n\t     ras_obj->hw_ops->query_ras_error_status)) {\n\t\tr = amdgpu_ras_sysfs_create(adev, ras_block);\n\t\tif (r)\n\t\t\tgoto interrupt;\n\n\t\t \n\t\tquery_info = kzalloc(sizeof(*query_info), GFP_KERNEL);\n\t\tif (!query_info)\n\t\t\treturn -ENOMEM;\n\t\tmemcpy(&query_info->head, ras_block, sizeof(struct ras_common_if));\n\n\t\tif (amdgpu_ras_query_error_count(adev, &ce_count, &ue_count, query_info) == 0) {\n\t\t\tatomic_set(&con->ras_ce_count, ce_count);\n\t\t\tatomic_set(&con->ras_ue_count, ue_count);\n\t\t}\n\n\t\tkfree(query_info);\n\t}\n\n\treturn 0;\n\ninterrupt:\n\tif (ras_obj->ras_cb)\n\t\tamdgpu_ras_interrupt_remove_handler(adev, ras_block);\ncleanup:\n\tamdgpu_ras_feature_enable(adev, ras_block, 0);\n\treturn r;\n}\n\nstatic int amdgpu_ras_block_late_init_default(struct amdgpu_device *adev,\n\t\t\t struct ras_common_if *ras_block)\n{\n\treturn amdgpu_ras_block_late_init(adev, ras_block);\n}\n\n \nvoid amdgpu_ras_block_late_fini(struct amdgpu_device *adev,\n\t\t\t  struct ras_common_if *ras_block)\n{\n\tstruct amdgpu_ras_block_object *ras_obj;\n\tif (!ras_block)\n\t\treturn;\n\n\tamdgpu_ras_sysfs_remove(adev, ras_block);\n\n\tras_obj = container_of(ras_block, struct amdgpu_ras_block_object, ras_comm);\n\tif (ras_obj->ras_cb)\n\t\tamdgpu_ras_interrupt_remove_handler(adev, ras_block);\n}\n\nstatic void amdgpu_ras_block_late_fini_default(struct amdgpu_device *adev,\n\t\t\t  struct ras_common_if *ras_block)\n{\n\treturn amdgpu_ras_block_late_fini(adev, ras_block);\n}\n\n \nvoid amdgpu_ras_resume(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\tstruct ras_manager *obj, *tmp;\n\n\tif (!adev->ras_enabled || !con) {\n\t\t \n\t\tamdgpu_release_ras_context(adev);\n\n\t\treturn;\n\t}\n\n\tif (con->flags & AMDGPU_RAS_FLAG_INIT_BY_VBIOS) {\n\t\t \n\t\tamdgpu_ras_enable_all_features(adev, 1);\n\n\t\t \n\t\tlist_for_each_entry_safe(obj, tmp, &con->head, node) {\n\t\t\tif (!amdgpu_ras_is_supported(adev, obj->head.block)) {\n\t\t\t\tamdgpu_ras_feature_enable(adev, &obj->head, 0);\n\t\t\t\t \n\t\t\t\tWARN_ON(alive_obj(obj));\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid amdgpu_ras_suspend(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn;\n\n\tamdgpu_ras_disable_all_features(adev, 0);\n\t \n\tif (con->features)\n\t\tamdgpu_ras_disable_all_features(adev, 1);\n}\n\nint amdgpu_ras_late_init(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras_block_list *node, *tmp;\n\tstruct amdgpu_ras_block_object *obj;\n\tint r;\n\n\t \n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tlist_for_each_entry_safe(node, tmp, &adev->ras_list, node) {\n\t\tif (!node->ras_obj) {\n\t\t\tdev_warn(adev->dev, \"Warning: abnormal ras list node.\\n\");\n\t\t\tcontinue;\n\t\t}\n\n\t\tobj = node->ras_obj;\n\t\tif (obj->ras_late_init) {\n\t\t\tr = obj->ras_late_init(adev, &obj->ras_comm);\n\t\t\tif (r) {\n\t\t\t\tdev_err(adev->dev, \"%s failed to execute ras_late_init! ret:%d\\n\",\n\t\t\t\t\tobj->ras_comm.name, r);\n\t\t\t\treturn r;\n\t\t\t}\n\t\t} else\n\t\t\tamdgpu_ras_block_late_init_default(adev, &obj->ras_comm);\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_ras_pre_fini(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn 0;\n\n\n\t \n\tif (con->features)\n\t\tamdgpu_ras_disable_all_features(adev, 0);\n\tamdgpu_ras_recovery_fini(adev);\n\treturn 0;\n}\n\nint amdgpu_ras_fini(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras_block_list *ras_node, *tmp;\n\tstruct amdgpu_ras_block_object *obj = NULL;\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\tif (!adev->ras_enabled || !con)\n\t\treturn 0;\n\n\tlist_for_each_entry_safe(ras_node, tmp, &adev->ras_list, node) {\n\t\tif (ras_node->ras_obj) {\n\t\t\tobj = ras_node->ras_obj;\n\t\t\tif (amdgpu_ras_is_supported(adev, obj->ras_comm.block) &&\n\t\t\t    obj->ras_fini)\n\t\t\t\tobj->ras_fini(adev, &obj->ras_comm);\n\t\t\telse\n\t\t\t\tamdgpu_ras_block_late_fini_default(adev, &obj->ras_comm);\n\t\t}\n\n\t\t \n\t\tlist_del(&ras_node->node);\n\t\tkfree(ras_node);\n\t}\n\n\tamdgpu_ras_fs_fini(adev);\n\tamdgpu_ras_interrupt_remove_all(adev);\n\n\tWARN(con->features, \"Feature mask is not cleared\");\n\n\tif (con->features)\n\t\tamdgpu_ras_disable_all_features(adev, 1);\n\n\tcancel_delayed_work_sync(&con->ras_counte_delay_work);\n\n\tamdgpu_ras_set_context(adev, NULL);\n\tkfree(con);\n\n\treturn 0;\n}\n\nvoid amdgpu_ras_global_ras_isr(struct amdgpu_device *adev)\n{\n\tif (atomic_cmpxchg(&amdgpu_ras_in_intr, 0, 1) == 0) {\n\t\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\t\tdev_info(adev->dev, \"uncorrectable hardware error\"\n\t\t\t\"(ERREVENT_ATHUB_INTERRUPT) detected!\\n\");\n\n\t\tras->gpu_reset_flags |= AMDGPU_RAS_GPU_RESET_MODE1_RESET;\n\t\tamdgpu_ras_reset_gpu(adev);\n\t}\n}\n\nbool amdgpu_ras_need_emergency_restart(struct amdgpu_device *adev)\n{\n\tif (adev->asic_type == CHIP_VEGA20 &&\n\t    adev->pm.fw_version <= 0x283400) {\n\t\treturn !(amdgpu_asic_reset_method(adev) == AMD_RESET_METHOD_BACO) &&\n\t\t\t\tamdgpu_ras_intr_triggered();\n\t}\n\n\treturn false;\n}\n\nvoid amdgpu_release_ras_context(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *con = amdgpu_ras_get_context(adev);\n\n\tif (!con)\n\t\treturn;\n\n\tif (!adev->ras_enabled && con->features & BIT(AMDGPU_RAS_BLOCK__GFX)) {\n\t\tcon->features &= ~BIT(AMDGPU_RAS_BLOCK__GFX);\n\t\tamdgpu_ras_set_context(adev, NULL);\n\t\tkfree(con);\n\t}\n}\n\n#ifdef CONFIG_X86_MCE_AMD\nstatic struct amdgpu_device *find_adev(uint32_t node_id)\n{\n\tint i;\n\tstruct amdgpu_device *adev = NULL;\n\n\tfor (i = 0; i < mce_adev_list.num_gpu; i++) {\n\t\tadev = mce_adev_list.devs[i];\n\n\t\tif (adev && adev->gmc.xgmi.connected_to_cpu &&\n\t\t    adev->gmc.xgmi.physical_node_id == node_id)\n\t\t\tbreak;\n\t\tadev = NULL;\n\t}\n\n\treturn adev;\n}\n\n#define GET_MCA_IPID_GPUID(m)\t(((m) >> 44) & 0xF)\n#define GET_UMC_INST(m)\t\t(((m) >> 21) & 0x7)\n#define GET_CHAN_INDEX(m)\t((((m) >> 12) & 0x3) | (((m) >> 18) & 0x4))\n#define GPU_ID_OFFSET\t\t8\n\nstatic int amdgpu_bad_page_notifier(struct notifier_block *nb,\n\t\t\t\t    unsigned long val, void *data)\n{\n\tstruct mce *m = (struct mce *)data;\n\tstruct amdgpu_device *adev = NULL;\n\tuint32_t gpu_id = 0;\n\tuint32_t umc_inst = 0, ch_inst = 0;\n\n\t \n\tif (!m || !((smca_get_bank_type(m->extcpu, m->bank) == SMCA_UMC_V2) &&\n\t\t    (XEC(m->status, 0x3f) == 0x0)))\n\t\treturn NOTIFY_DONE;\n\n\t \n\tif (mce_is_correctable(m))\n\t\treturn NOTIFY_OK;\n\n\t \n\tgpu_id = GET_MCA_IPID_GPUID(m->ipid) - GPU_ID_OFFSET;\n\n\tadev = find_adev(gpu_id);\n\tif (!adev) {\n\t\tDRM_WARN(\"%s: Unable to find adev for gpu_id: %d\\n\", __func__,\n\t\t\t\t\t\t\t\tgpu_id);\n\t\treturn NOTIFY_DONE;\n\t}\n\n\t \n\tumc_inst = GET_UMC_INST(m->ipid);\n\tch_inst = GET_CHAN_INDEX(m->ipid);\n\n\tdev_info(adev->dev, \"Uncorrectable error detected in UMC inst: %d, chan_idx: %d\",\n\t\t\t     umc_inst, ch_inst);\n\n\tif (!amdgpu_umc_page_retirement_mca(adev, m->addr, ch_inst, umc_inst))\n\t\treturn NOTIFY_OK;\n\telse\n\t\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block amdgpu_bad_page_nb = {\n\t.notifier_call  = amdgpu_bad_page_notifier,\n\t.priority       = MCE_PRIO_UC,\n};\n\nstatic void amdgpu_register_bad_pages_mca_notifier(struct amdgpu_device *adev)\n{\n\t \n\tmce_adev_list.devs[mce_adev_list.num_gpu++] = adev;\n\n\t \n\tif (notifier_registered == false) {\n\t\tmce_register_decode_chain(&amdgpu_bad_page_nb);\n\t\tnotifier_registered = true;\n\t}\n}\n#endif\n\nstruct amdgpu_ras *amdgpu_ras_get_context(struct amdgpu_device *adev)\n{\n\tif (!adev)\n\t\treturn NULL;\n\n\treturn adev->psp.ras_context.ras;\n}\n\nint amdgpu_ras_set_context(struct amdgpu_device *adev, struct amdgpu_ras *ras_con)\n{\n\tif (!adev)\n\t\treturn -EINVAL;\n\n\tadev->psp.ras_context.ras = ras_con;\n\treturn 0;\n}\n\n \nint amdgpu_ras_is_supported(struct amdgpu_device *adev,\n\t\tunsigned int block)\n{\n\tint ret = 0;\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\tif (block >= AMDGPU_RAS_BLOCK_COUNT)\n\t\treturn 0;\n\n\tret = ras && (adev->ras_enabled & (1 << block));\n\n\t \n\tif (!ret &&\n\t    (block == AMDGPU_RAS_BLOCK__GFX ||\n\t     block == AMDGPU_RAS_BLOCK__SDMA ||\n\t     block == AMDGPU_RAS_BLOCK__VCN ||\n\t     block == AMDGPU_RAS_BLOCK__JPEG) &&\n\t    amdgpu_ras_is_poison_mode_supported(adev) &&\n\t    amdgpu_ras_get_ras_block(adev, block, 0))\n\t\tret = 1;\n\n\treturn ret;\n}\n\nint amdgpu_ras_reset_gpu(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ras *ras = amdgpu_ras_get_context(adev);\n\n\tif (atomic_cmpxchg(&ras->in_recovery, 0, 1) == 0)\n\t\tamdgpu_reset_domain_schedule(ras->adev->reset_domain, &ras->recovery_work);\n\treturn 0;\n}\n\n\n \nint amdgpu_ras_register_ras_block(struct amdgpu_device *adev,\n\t\tstruct amdgpu_ras_block_object *ras_block_obj)\n{\n\tstruct amdgpu_ras_block_list *ras_node;\n\tif (!adev || !ras_block_obj)\n\t\treturn -EINVAL;\n\n\tras_node = kzalloc(sizeof(*ras_node), GFP_KERNEL);\n\tif (!ras_node)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ras_node->node);\n\tras_node->ras_obj = ras_block_obj;\n\tlist_add_tail(&ras_node->node, &adev->ras_list);\n\n\treturn 0;\n}\n\nvoid amdgpu_ras_get_error_type_name(uint32_t err_type, char *err_type_name)\n{\n\tif (!err_type_name)\n\t\treturn;\n\n\tswitch (err_type) {\n\tcase AMDGPU_RAS_ERROR__SINGLE_CORRECTABLE:\n\t\tsprintf(err_type_name, \"correctable\");\n\t\tbreak;\n\tcase AMDGPU_RAS_ERROR__MULTI_UNCORRECTABLE:\n\t\tsprintf(err_type_name, \"uncorrectable\");\n\t\tbreak;\n\tdefault:\n\t\tsprintf(err_type_name, \"unknown\");\n\t\tbreak;\n\t}\n}\n\nbool amdgpu_ras_inst_get_memory_id_field(struct amdgpu_device *adev,\n\t\t\t\t\t const struct amdgpu_ras_err_status_reg_entry *reg_entry,\n\t\t\t\t\t uint32_t instance,\n\t\t\t\t\t uint32_t *memory_id)\n{\n\tuint32_t err_status_lo_data, err_status_lo_offset;\n\n\tif (!reg_entry)\n\t\treturn false;\n\n\terr_status_lo_offset =\n\t\tAMDGPU_RAS_REG_ENTRY_OFFSET(reg_entry->hwip, instance,\n\t\t\t\t\t    reg_entry->seg_lo, reg_entry->reg_lo);\n\terr_status_lo_data = RREG32(err_status_lo_offset);\n\n\tif ((reg_entry->flags & AMDGPU_RAS_ERR_STATUS_VALID) &&\n\t    !REG_GET_FIELD(err_status_lo_data, ERR_STATUS_LO, ERR_STATUS_VALID_FLAG))\n\t\treturn false;\n\n\t*memory_id = REG_GET_FIELD(err_status_lo_data, ERR_STATUS_LO, MEMORY_ID);\n\n\treturn true;\n}\n\nbool amdgpu_ras_inst_get_err_cnt_field(struct amdgpu_device *adev,\n\t\t\t\t       const struct amdgpu_ras_err_status_reg_entry *reg_entry,\n\t\t\t\t       uint32_t instance,\n\t\t\t\t       unsigned long *err_cnt)\n{\n\tuint32_t err_status_hi_data, err_status_hi_offset;\n\n\tif (!reg_entry)\n\t\treturn false;\n\n\terr_status_hi_offset =\n\t\tAMDGPU_RAS_REG_ENTRY_OFFSET(reg_entry->hwip, instance,\n\t\t\t\t\t    reg_entry->seg_hi, reg_entry->reg_hi);\n\terr_status_hi_data = RREG32(err_status_hi_offset);\n\n\tif ((reg_entry->flags & AMDGPU_RAS_ERR_INFO_VALID) &&\n\t    !REG_GET_FIELD(err_status_hi_data, ERR_STATUS_HI, ERR_INFO_VALID_FLAG))\n\t\t \n\t\tdev_dbg(adev->dev, \"Invalid err_info field\\n\");\n\n\t \n\t*err_cnt = REG_GET_FIELD(err_status_hi_data, ERR_STATUS, ERR_CNT);\n\n\treturn true;\n}\n\nvoid amdgpu_ras_inst_query_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t   const struct amdgpu_ras_err_status_reg_entry *reg_list,\n\t\t\t\t\t   uint32_t reg_list_size,\n\t\t\t\t\t   const struct amdgpu_ras_memory_id_entry *mem_list,\n\t\t\t\t\t   uint32_t mem_list_size,\n\t\t\t\t\t   uint32_t instance,\n\t\t\t\t\t   uint32_t err_type,\n\t\t\t\t\t   unsigned long *err_count)\n{\n\tuint32_t memory_id;\n\tunsigned long err_cnt;\n\tchar err_type_name[16];\n\tuint32_t i, j;\n\n\tfor (i = 0; i < reg_list_size; i++) {\n\t\t \n\t\tif (!amdgpu_ras_inst_get_memory_id_field(adev, &reg_list[i],\n\t\t\t\t\t\t\t instance, &memory_id))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!amdgpu_ras_inst_get_err_cnt_field(adev, &reg_list[i],\n\t\t\t\t\t\t       instance, &err_cnt) ||\n\t\t    !err_cnt)\n\t\t\tcontinue;\n\n\t\t*err_count += err_cnt;\n\n\t\t \n\t\tamdgpu_ras_get_error_type_name(err_type, err_type_name);\n\t\tif (!mem_list) {\n\t\t\t \n\t\t\tdev_info(adev->dev,\n\t\t\t\t \"%ld %s hardware errors detected in %s, instance: %d, memory_id: %d\\n\",\n\t\t\t\t err_cnt, err_type_name,\n\t\t\t\t reg_list[i].block_name,\n\t\t\t\t instance, memory_id);\n\t\t} else {\n\t\t\tfor (j = 0; j < mem_list_size; j++) {\n\t\t\t\tif (memory_id == mem_list[j].memory_id) {\n\t\t\t\t\tdev_info(adev->dev,\n\t\t\t\t\t\t \"%ld %s hardware errors detected in %s, instance: %d, memory block: %s\\n\",\n\t\t\t\t\t\t err_cnt, err_type_name,\n\t\t\t\t\t\t reg_list[i].block_name,\n\t\t\t\t\t\t instance, mem_list[j].name);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid amdgpu_ras_inst_reset_ras_error_count(struct amdgpu_device *adev,\n\t\t\t\t\t   const struct amdgpu_ras_err_status_reg_entry *reg_list,\n\t\t\t\t\t   uint32_t reg_list_size,\n\t\t\t\t\t   uint32_t instance)\n{\n\tuint32_t err_status_lo_offset, err_status_hi_offset;\n\tuint32_t i;\n\n\tfor (i = 0; i < reg_list_size; i++) {\n\t\terr_status_lo_offset =\n\t\t\tAMDGPU_RAS_REG_ENTRY_OFFSET(reg_list[i].hwip, instance,\n\t\t\t\t\t\t    reg_list[i].seg_lo, reg_list[i].reg_lo);\n\t\terr_status_hi_offset =\n\t\t\tAMDGPU_RAS_REG_ENTRY_OFFSET(reg_list[i].hwip, instance,\n\t\t\t\t\t\t    reg_list[i].seg_hi, reg_list[i].reg_hi);\n\t\tWREG32(err_status_lo_offset, 0);\n\t\tWREG32(err_status_hi_offset, 0);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}