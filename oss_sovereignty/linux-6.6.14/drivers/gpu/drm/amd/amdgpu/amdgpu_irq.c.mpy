{
  "module_name": "amdgpu_irq.c",
  "hash_id": "c24ed12fbd2eee81aa31bccd97195484ca17fe43f11ecc678b2188d8d2cc75b1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c",
  "human_readable_source": " \n\n \n\n#include <linux/irq.h>\n#include <linux/pci.h>\n\n#include <drm/drm_vblank.h>\n#include <drm/amdgpu_drm.h>\n#include <drm/drm_drv.h>\n#include \"amdgpu.h\"\n#include \"amdgpu_ih.h\"\n#include \"atom.h\"\n#include \"amdgpu_connectors.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_amdkfd.h\"\n#include \"amdgpu_ras.h\"\n\n#include <linux/pm_runtime.h>\n\n#ifdef CONFIG_DRM_AMD_DC\n#include \"amdgpu_dm_irq.h\"\n#endif\n\n#define AMDGPU_WAIT_IDLE_TIMEOUT 200\n\nconst char *soc15_ih_clientid_name[] = {\n\t\"IH\",\n\t\"SDMA2 or ACP\",\n\t\"ATHUB\",\n\t\"BIF\",\n\t\"SDMA3 or DCE\",\n\t\"SDMA4 or ISP\",\n\t\"VMC1 or PCIE0\",\n\t\"RLC\",\n\t\"SDMA0\",\n\t\"SDMA1\",\n\t\"SE0SH\",\n\t\"SE1SH\",\n\t\"SE2SH\",\n\t\"SE3SH\",\n\t\"VCN1 or UVD1\",\n\t\"THM\",\n\t\"VCN or UVD\",\n\t\"SDMA5 or VCE0\",\n\t\"VMC\",\n\t\"SDMA6 or XDMA\",\n\t\"GRBM_CP\",\n\t\"ATS\",\n\t\"ROM_SMUIO\",\n\t\"DF\",\n\t\"SDMA7 or VCE1\",\n\t\"PWR\",\n\t\"reserved\",\n\t\"UTCL2\",\n\t\"EA\",\n\t\"UTCL2LOG\",\n\t\"MP0\",\n\t\"MP1\"\n};\n\nconst int node_id_to_phys_map[NODEID_MAX] = {\n\t[AID0_NODEID] = 0,\n\t[XCD0_NODEID] = 0,\n\t[XCD1_NODEID] = 1,\n\t[AID1_NODEID] = 1,\n\t[XCD2_NODEID] = 2,\n\t[XCD3_NODEID] = 3,\n\t[AID2_NODEID] = 2,\n\t[XCD4_NODEID] = 4,\n\t[XCD5_NODEID] = 5,\n\t[AID3_NODEID] = 3,\n\t[XCD6_NODEID] = 6,\n\t[XCD7_NODEID] = 7,\n};\n\n \nvoid amdgpu_irq_disable_all(struct amdgpu_device *adev)\n{\n\tunsigned long irqflags;\n\tunsigned int i, j, k;\n\tint r;\n\n\tspin_lock_irqsave(&adev->irq.lock, irqflags);\n\tfor (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; ++i) {\n\t\tif (!adev->irq.client[i].sources)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; ++j) {\n\t\t\tstruct amdgpu_irq_src *src = adev->irq.client[i].sources[j];\n\n\t\t\tif (!src || !src->funcs->set || !src->num_types)\n\t\t\t\tcontinue;\n\n\t\t\tfor (k = 0; k < src->num_types; ++k) {\n\t\t\t\tr = src->funcs->set(adev, src, k,\n\t\t\t\t\t\t    AMDGPU_IRQ_STATE_DISABLE);\n\t\t\t\tif (r)\n\t\t\t\t\tDRM_ERROR(\"error disabling interrupt (%d)\\n\",\n\t\t\t\t\t\t  r);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&adev->irq.lock, irqflags);\n}\n\n \nstatic irqreturn_t amdgpu_irq_handler(int irq, void *arg)\n{\n\tstruct drm_device *dev = (struct drm_device *) arg;\n\tstruct amdgpu_device *adev = drm_to_adev(dev);\n\tirqreturn_t ret;\n\n\tret = amdgpu_ih_process(adev, &adev->irq.ih);\n\tif (ret == IRQ_HANDLED)\n\t\tpm_runtime_mark_last_busy(dev->dev);\n\n\tamdgpu_ras_interrupt_fatal_error_handler(adev);\n\n\treturn ret;\n}\n\n \nstatic void amdgpu_irq_handle_ih1(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev = container_of(work, struct amdgpu_device,\n\t\t\t\t\t\t  irq.ih1_work);\n\n\tamdgpu_ih_process(adev, &adev->irq.ih1);\n}\n\n \nstatic void amdgpu_irq_handle_ih2(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev = container_of(work, struct amdgpu_device,\n\t\t\t\t\t\t  irq.ih2_work);\n\n\tamdgpu_ih_process(adev, &adev->irq.ih2);\n}\n\n \nstatic void amdgpu_irq_handle_ih_soft(struct work_struct *work)\n{\n\tstruct amdgpu_device *adev = container_of(work, struct amdgpu_device,\n\t\t\t\t\t\t  irq.ih_soft_work);\n\n\tamdgpu_ih_process(adev, &adev->irq.ih_soft);\n}\n\n \nstatic bool amdgpu_msi_ok(struct amdgpu_device *adev)\n{\n\tif (amdgpu_msi == 1)\n\t\treturn true;\n\telse if (amdgpu_msi == 0)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void amdgpu_restore_msix(struct amdgpu_device *adev)\n{\n\tu16 ctrl;\n\n\tpci_read_config_word(adev->pdev, adev->pdev->msix_cap + PCI_MSIX_FLAGS, &ctrl);\n\tif (!(ctrl & PCI_MSIX_FLAGS_ENABLE))\n\t\treturn;\n\n\t \n\tctrl &= ~PCI_MSIX_FLAGS_ENABLE;\n\tpci_write_config_word(adev->pdev, adev->pdev->msix_cap + PCI_MSIX_FLAGS, ctrl);\n\tctrl |= PCI_MSIX_FLAGS_ENABLE;\n\tpci_write_config_word(adev->pdev, adev->pdev->msix_cap + PCI_MSIX_FLAGS, ctrl);\n}\n\n \nint amdgpu_irq_init(struct amdgpu_device *adev)\n{\n\tint r = 0;\n\tunsigned int irq;\n\n\tspin_lock_init(&adev->irq.lock);\n\n\t \n\tadev->irq.msi_enabled = false;\n\n\tif (amdgpu_msi_ok(adev)) {\n\t\tint nvec = pci_msix_vec_count(adev->pdev);\n\t\tunsigned int flags;\n\n\t\tif (nvec <= 0)\n\t\t\tflags = PCI_IRQ_MSI;\n\t\telse\n\t\t\tflags = PCI_IRQ_MSI | PCI_IRQ_MSIX;\n\n\t\t \n\t\tnvec = pci_alloc_irq_vectors(adev->pdev, 1, 1, flags);\n\t\tif (nvec > 0) {\n\t\t\tadev->irq.msi_enabled = true;\n\t\t\tdev_dbg(adev->dev, \"using MSI/MSI-X.\\n\");\n\t\t}\n\t}\n\n\tINIT_WORK(&adev->irq.ih1_work, amdgpu_irq_handle_ih1);\n\tINIT_WORK(&adev->irq.ih2_work, amdgpu_irq_handle_ih2);\n\tINIT_WORK(&adev->irq.ih_soft_work, amdgpu_irq_handle_ih_soft);\n\n\t \n\tr = pci_irq_vector(adev->pdev, 0);\n\tif (r < 0)\n\t\treturn r;\n\tirq = r;\n\n\t \n\tr = request_irq(irq, amdgpu_irq_handler, IRQF_SHARED, adev_to_drm(adev)->driver->name,\n\t\t\tadev_to_drm(adev));\n\tif (r)\n\t\treturn r;\n\tadev->irq.installed = true;\n\tadev->irq.irq = irq;\n\tadev_to_drm(adev)->max_vblank_count = 0x00ffffff;\n\n\tDRM_DEBUG(\"amdgpu: irq initialized.\\n\");\n\treturn 0;\n}\n\n\nvoid amdgpu_irq_fini_hw(struct amdgpu_device *adev)\n{\n\tif (adev->irq.installed) {\n\t\tfree_irq(adev->irq.irq, adev_to_drm(adev));\n\t\tadev->irq.installed = false;\n\t\tif (adev->irq.msi_enabled)\n\t\t\tpci_free_irq_vectors(adev->pdev);\n\t}\n\n\tamdgpu_ih_ring_fini(adev, &adev->irq.ih_soft);\n\tamdgpu_ih_ring_fini(adev, &adev->irq.ih);\n\tamdgpu_ih_ring_fini(adev, &adev->irq.ih1);\n\tamdgpu_ih_ring_fini(adev, &adev->irq.ih2);\n}\n\n \nvoid amdgpu_irq_fini_sw(struct amdgpu_device *adev)\n{\n\tunsigned int i, j;\n\n\tfor (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; ++i) {\n\t\tif (!adev->irq.client[i].sources)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; ++j) {\n\t\t\tstruct amdgpu_irq_src *src = adev->irq.client[i].sources[j];\n\n\t\t\tif (!src)\n\t\t\t\tcontinue;\n\n\t\t\tkfree(src->enabled_types);\n\t\t\tsrc->enabled_types = NULL;\n\t\t}\n\t\tkfree(adev->irq.client[i].sources);\n\t\tadev->irq.client[i].sources = NULL;\n\t}\n}\n\n \nint amdgpu_irq_add_id(struct amdgpu_device *adev,\n\t\t      unsigned int client_id, unsigned int src_id,\n\t\t      struct amdgpu_irq_src *source)\n{\n\tif (client_id >= AMDGPU_IRQ_CLIENTID_MAX)\n\t\treturn -EINVAL;\n\n\tif (src_id >= AMDGPU_MAX_IRQ_SRC_ID)\n\t\treturn -EINVAL;\n\n\tif (!source->funcs)\n\t\treturn -EINVAL;\n\n\tif (!adev->irq.client[client_id].sources) {\n\t\tadev->irq.client[client_id].sources =\n\t\t\tkcalloc(AMDGPU_MAX_IRQ_SRC_ID,\n\t\t\t\tsizeof(struct amdgpu_irq_src *),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!adev->irq.client[client_id].sources)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (adev->irq.client[client_id].sources[src_id] != NULL)\n\t\treturn -EINVAL;\n\n\tif (source->num_types && !source->enabled_types) {\n\t\tatomic_t *types;\n\n\t\ttypes = kcalloc(source->num_types, sizeof(atomic_t),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!types)\n\t\t\treturn -ENOMEM;\n\n\t\tsource->enabled_types = types;\n\t}\n\n\tadev->irq.client[client_id].sources[src_id] = source;\n\treturn 0;\n}\n\n \nvoid amdgpu_irq_dispatch(struct amdgpu_device *adev,\n\t\t\t struct amdgpu_ih_ring *ih)\n{\n\tu32 ring_index = ih->rptr >> 2;\n\tstruct amdgpu_iv_entry entry;\n\tunsigned int client_id, src_id;\n\tstruct amdgpu_irq_src *src;\n\tbool handled = false;\n\tint r;\n\n\tentry.ih = ih;\n\tentry.iv_entry = (const uint32_t *)&ih->ring[ring_index];\n\tamdgpu_ih_decode_iv(adev, &entry);\n\n\ttrace_amdgpu_iv(ih - &adev->irq.ih, &entry);\n\n\tclient_id = entry.client_id;\n\tsrc_id = entry.src_id;\n\n\tif (client_id >= AMDGPU_IRQ_CLIENTID_MAX) {\n\t\tDRM_DEBUG(\"Invalid client_id in IV: %d\\n\", client_id);\n\n\t} else\tif (src_id >= AMDGPU_MAX_IRQ_SRC_ID) {\n\t\tDRM_DEBUG(\"Invalid src_id in IV: %d\\n\", src_id);\n\n\t} else if ((client_id == AMDGPU_IRQ_CLIENTID_LEGACY) &&\n\t\t   adev->irq.virq[src_id]) {\n\t\tgeneric_handle_domain_irq(adev->irq.domain, src_id);\n\n\t} else if (!adev->irq.client[client_id].sources) {\n\t\tDRM_DEBUG(\"Unregistered interrupt client_id: %d src_id: %d\\n\",\n\t\t\t  client_id, src_id);\n\n\t} else if ((src = adev->irq.client[client_id].sources[src_id])) {\n\t\tr = src->funcs->process(adev, src, &entry);\n\t\tif (r < 0)\n\t\t\tDRM_ERROR(\"error processing interrupt (%d)\\n\", r);\n\t\telse if (r)\n\t\t\thandled = true;\n\n\t} else {\n\t\tDRM_DEBUG(\"Unregistered interrupt src_id: %d of client_id:%d\\n\",\n\t\t\tsrc_id, client_id);\n\t}\n\n\t \n\tif (!handled)\n\t\tamdgpu_amdkfd_interrupt(adev, entry.iv_entry);\n\n\tif (amdgpu_ih_ts_after(ih->processed_timestamp, entry.timestamp))\n\t\tih->processed_timestamp = entry.timestamp;\n}\n\n \nvoid amdgpu_irq_delegate(struct amdgpu_device *adev,\n\t\t\t struct amdgpu_iv_entry *entry,\n\t\t\t unsigned int num_dw)\n{\n\tamdgpu_ih_ring_write(adev, &adev->irq.ih_soft, entry->iv_entry, num_dw);\n\tschedule_work(&adev->irq.ih_soft_work);\n}\n\n \nint amdgpu_irq_update(struct amdgpu_device *adev,\n\t\t\t     struct amdgpu_irq_src *src, unsigned int type)\n{\n\tunsigned long irqflags;\n\tenum amdgpu_interrupt_state state;\n\tint r;\n\n\tspin_lock_irqsave(&adev->irq.lock, irqflags);\n\n\t \n\tif (amdgpu_irq_enabled(adev, src, type))\n\t\tstate = AMDGPU_IRQ_STATE_ENABLE;\n\telse\n\t\tstate = AMDGPU_IRQ_STATE_DISABLE;\n\n\tr = src->funcs->set(adev, src, type, state);\n\tspin_unlock_irqrestore(&adev->irq.lock, irqflags);\n\treturn r;\n}\n\n \nvoid amdgpu_irq_gpu_reset_resume_helper(struct amdgpu_device *adev)\n{\n\tint i, j, k;\n\n\tif (amdgpu_sriov_vf(adev) || amdgpu_passthrough(adev))\n\t\tamdgpu_restore_msix(adev);\n\n\tfor (i = 0; i < AMDGPU_IRQ_CLIENTID_MAX; ++i) {\n\t\tif (!adev->irq.client[i].sources)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < AMDGPU_MAX_IRQ_SRC_ID; ++j) {\n\t\t\tstruct amdgpu_irq_src *src = adev->irq.client[i].sources[j];\n\n\t\t\tif (!src || !src->funcs || !src->funcs->set)\n\t\t\t\tcontinue;\n\t\t\tfor (k = 0; k < src->num_types; k++)\n\t\t\t\tamdgpu_irq_update(adev, src, k);\n\t\t}\n\t}\n}\n\n \nint amdgpu_irq_get(struct amdgpu_device *adev, struct amdgpu_irq_src *src,\n\t\t   unsigned int type)\n{\n\tif (!adev->irq.installed)\n\t\treturn -ENOENT;\n\n\tif (type >= src->num_types)\n\t\treturn -EINVAL;\n\n\tif (!src->enabled_types || !src->funcs->set)\n\t\treturn -EINVAL;\n\n\tif (atomic_inc_return(&src->enabled_types[type]) == 1)\n\t\treturn amdgpu_irq_update(adev, src, type);\n\n\treturn 0;\n}\n\n \nint amdgpu_irq_put(struct amdgpu_device *adev, struct amdgpu_irq_src *src,\n\t\t   unsigned int type)\n{\n\tif (!adev->irq.installed)\n\t\treturn -ENOENT;\n\n\tif (type >= src->num_types)\n\t\treturn -EINVAL;\n\n\tif (!src->enabled_types || !src->funcs->set)\n\t\treturn -EINVAL;\n\n\tif (WARN_ON(!amdgpu_irq_enabled(adev, src, type)))\n\t\treturn -EINVAL;\n\n\tif (atomic_dec_and_test(&src->enabled_types[type]))\n\t\treturn amdgpu_irq_update(adev, src, type);\n\n\treturn 0;\n}\n\n \nbool amdgpu_irq_enabled(struct amdgpu_device *adev, struct amdgpu_irq_src *src,\n\t\t\tunsigned int type)\n{\n\tif (!adev->irq.installed)\n\t\treturn false;\n\n\tif (type >= src->num_types)\n\t\treturn false;\n\n\tif (!src->enabled_types || !src->funcs->set)\n\t\treturn false;\n\n\treturn !!atomic_read(&src->enabled_types[type]);\n}\n\n \nstatic void amdgpu_irq_mask(struct irq_data *irqd)\n{\n\t \n}\n\nstatic void amdgpu_irq_unmask(struct irq_data *irqd)\n{\n\t \n}\n\n \nstatic struct irq_chip amdgpu_irq_chip = {\n\t.name = \"amdgpu-ih\",\n\t.irq_mask = amdgpu_irq_mask,\n\t.irq_unmask = amdgpu_irq_unmask,\n};\n\n \nstatic int amdgpu_irqdomain_map(struct irq_domain *d,\n\t\t\t\tunsigned int irq, irq_hw_number_t hwirq)\n{\n\tif (hwirq >= AMDGPU_MAX_IRQ_SRC_ID)\n\t\treturn -EPERM;\n\n\tirq_set_chip_and_handler(irq,\n\t\t\t\t &amdgpu_irq_chip, handle_simple_irq);\n\treturn 0;\n}\n\n \nstatic const struct irq_domain_ops amdgpu_hw_irqdomain_ops = {\n\t.map = amdgpu_irqdomain_map,\n};\n\n \nint amdgpu_irq_add_domain(struct amdgpu_device *adev)\n{\n\tadev->irq.domain = irq_domain_add_linear(NULL, AMDGPU_MAX_IRQ_SRC_ID,\n\t\t\t\t\t\t &amdgpu_hw_irqdomain_ops, adev);\n\tif (!adev->irq.domain) {\n\t\tDRM_ERROR(\"GPU irq add domain failed\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n \nvoid amdgpu_irq_remove_domain(struct amdgpu_device *adev)\n{\n\tif (adev->irq.domain) {\n\t\tirq_domain_remove(adev->irq.domain);\n\t\tadev->irq.domain = NULL;\n\t}\n}\n\n \nunsigned int amdgpu_irq_create_mapping(struct amdgpu_device *adev, unsigned int src_id)\n{\n\tadev->irq.virq[src_id] = irq_create_mapping(adev->irq.domain, src_id);\n\n\treturn adev->irq.virq[src_id];\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}