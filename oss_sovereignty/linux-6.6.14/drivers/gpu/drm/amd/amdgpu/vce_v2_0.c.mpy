{
  "module_name": "vce_v2_0.c",
  "hash_id": "0084c3d44a25a12f4a3d1574d8caf04a5a9d9f2f5419bf2fb3d669b4d279c613",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/vce_v2_0.c",
  "human_readable_source": " \n\n#include <linux/firmware.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_vce.h\"\n#include \"cikd.h\"\n#include \"vce/vce_2_0_d.h\"\n#include \"vce/vce_2_0_sh_mask.h\"\n#include \"smu/smu_7_0_1_d.h\"\n#include \"smu/smu_7_0_1_sh_mask.h\"\n#include \"oss/oss_2_0_d.h\"\n#include \"oss/oss_2_0_sh_mask.h\"\n\n#define VCE_V2_0_FW_SIZE\t(256 * 1024)\n#define VCE_V2_0_STACK_SIZE\t(64 * 1024)\n#define VCE_V2_0_DATA_SIZE\t(23552 * AMDGPU_MAX_VCE_HANDLES)\n#define VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK\t0x02\n\nstatic void vce_v2_0_set_ring_funcs(struct amdgpu_device *adev);\nstatic void vce_v2_0_set_irq_funcs(struct amdgpu_device *adev);\n\n \nstatic uint64_t vce_v2_0_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->me == 0)\n\t\treturn RREG32(mmVCE_RB_RPTR);\n\telse\n\t\treturn RREG32(mmVCE_RB_RPTR2);\n}\n\n \nstatic uint64_t vce_v2_0_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->me == 0)\n\t\treturn RREG32(mmVCE_RB_WPTR);\n\telse\n\t\treturn RREG32(mmVCE_RB_WPTR2);\n}\n\n \nstatic void vce_v2_0_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->me == 0)\n\t\tWREG32(mmVCE_RB_WPTR, lower_32_bits(ring->wptr));\n\telse\n\t\tWREG32(mmVCE_RB_WPTR2, lower_32_bits(ring->wptr));\n}\n\nstatic int vce_v2_0_lmi_clean(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tuint32_t status = RREG32(mmVCE_LMI_STATUS);\n\n\t\t\tif (status & 0x337f)\n\t\t\t\treturn 0;\n\t\t\tmdelay(10);\n\t\t}\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nstatic int vce_v2_0_firmware_loaded(struct amdgpu_device *adev)\n{\n\tint i, j;\n\n\tfor (i = 0; i < 10; ++i) {\n\t\tfor (j = 0; j < 100; ++j) {\n\t\t\tuint32_t status = RREG32(mmVCE_STATUS);\n\n\t\t\tif (status & VCE_STATUS_VCPU_REPORT_FW_LOADED_MASK)\n\t\t\t\treturn 0;\n\t\t\tmdelay(10);\n\t\t}\n\n\t\tDRM_ERROR(\"VCE not responding, trying to reset the ECPU!!!\\n\");\n\t\tWREG32_P(mmVCE_SOFT_RESET,\n\t\t\tVCE_SOFT_RESET__ECPU_SOFT_RESET_MASK,\n\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t\tWREG32_P(mmVCE_SOFT_RESET, 0,\n\t\t\t~VCE_SOFT_RESET__ECPU_SOFT_RESET_MASK);\n\t\tmdelay(10);\n\t}\n\n\treturn -ETIMEDOUT;\n}\n\nstatic void vce_v2_0_disable_cg(struct amdgpu_device *adev)\n{\n\tWREG32(mmVCE_CGTT_CLK_OVERRIDE, 7);\n}\n\nstatic void vce_v2_0_init_cg(struct amdgpu_device *adev)\n{\n\tu32 tmp;\n\n\ttmp = RREG32(mmVCE_CLOCK_GATING_A);\n\ttmp &= ~0xfff;\n\ttmp |= ((0 << 0) | (4 << 4));\n\ttmp |= 0x40000;\n\tWREG32(mmVCE_CLOCK_GATING_A, tmp);\n\n\ttmp = RREG32(mmVCE_UENC_CLOCK_GATING);\n\ttmp &= ~0xfff;\n\ttmp |= ((0 << 0) | (4 << 4));\n\tWREG32(mmVCE_UENC_CLOCK_GATING, tmp);\n\n\ttmp = RREG32(mmVCE_CLOCK_GATING_B);\n\ttmp |= 0x10;\n\ttmp &= ~0x100000;\n\tWREG32(mmVCE_CLOCK_GATING_B, tmp);\n}\n\nstatic void vce_v2_0_mc_resume(struct amdgpu_device *adev)\n{\n\tuint32_t size, offset;\n\n\tWREG32_P(mmVCE_CLOCK_GATING_A, 0, ~(1 << 16));\n\tWREG32_P(mmVCE_UENC_CLOCK_GATING, 0x1FF000, ~0xFF9FF000);\n\tWREG32_P(mmVCE_UENC_REG_CLOCK_GATING, 0x3F, ~0x3F);\n\tWREG32(mmVCE_CLOCK_GATING_B, 0xf7);\n\n\tWREG32(mmVCE_LMI_CTRL, 0x00398000);\n\tWREG32_P(mmVCE_LMI_CACHE_CTRL, 0x0, ~0x1);\n\tWREG32(mmVCE_LMI_SWAP_CNTL, 0);\n\tWREG32(mmVCE_LMI_SWAP_CNTL1, 0);\n\tWREG32(mmVCE_LMI_VM_CTRL, 0);\n\n\tWREG32(mmVCE_LMI_VCPU_CACHE_40BIT_BAR, (adev->vce.gpu_addr >> 8));\n\n\toffset = AMDGPU_VCE_FIRMWARE_OFFSET;\n\tsize = VCE_V2_0_FW_SIZE;\n\tWREG32(mmVCE_VCPU_CACHE_OFFSET0, offset & 0x7fffffff);\n\tWREG32(mmVCE_VCPU_CACHE_SIZE0, size);\n\n\toffset += size;\n\tsize = VCE_V2_0_STACK_SIZE;\n\tWREG32(mmVCE_VCPU_CACHE_OFFSET1, offset & 0x7fffffff);\n\tWREG32(mmVCE_VCPU_CACHE_SIZE1, size);\n\n\toffset += size;\n\tsize = VCE_V2_0_DATA_SIZE;\n\tWREG32(mmVCE_VCPU_CACHE_OFFSET2, offset & 0x7fffffff);\n\tWREG32(mmVCE_VCPU_CACHE_SIZE2, size);\n\n\tWREG32_P(mmVCE_LMI_CTRL2, 0x0, ~0x100);\n\tWREG32_FIELD(VCE_SYS_INT_EN, VCE_SYS_INT_TRAP_INTERRUPT_EN, 1);\n}\n\nstatic bool vce_v2_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\treturn !(RREG32(mmSRBM_STATUS2) & SRBM_STATUS2__VCE_BUSY_MASK);\n}\n\nstatic int vce_v2_0_wait_for_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tunsigned i;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tif (vce_v2_0_is_idle(handle))\n\t\t\treturn 0;\n\t}\n\treturn -ETIMEDOUT;\n}\n\n \nstatic int vce_v2_0_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint r;\n\n\t \n\tWREG32_P(mmVCE_STATUS, 1, ~1);\n\n\tvce_v2_0_init_cg(adev);\n\tvce_v2_0_disable_cg(adev);\n\n\tvce_v2_0_mc_resume(adev);\n\n\tring = &adev->vce.ring[0];\n\tWREG32(mmVCE_RB_RPTR, lower_32_bits(ring->wptr));\n\tWREG32(mmVCE_RB_WPTR, lower_32_bits(ring->wptr));\n\tWREG32(mmVCE_RB_BASE_LO, ring->gpu_addr);\n\tWREG32(mmVCE_RB_BASE_HI, upper_32_bits(ring->gpu_addr));\n\tWREG32(mmVCE_RB_SIZE, ring->ring_size / 4);\n\n\tring = &adev->vce.ring[1];\n\tWREG32(mmVCE_RB_RPTR2, lower_32_bits(ring->wptr));\n\tWREG32(mmVCE_RB_WPTR2, lower_32_bits(ring->wptr));\n\tWREG32(mmVCE_RB_BASE_LO2, ring->gpu_addr);\n\tWREG32(mmVCE_RB_BASE_HI2, upper_32_bits(ring->gpu_addr));\n\tWREG32(mmVCE_RB_SIZE2, ring->ring_size / 4);\n\n\tWREG32_FIELD(VCE_VCPU_CNTL, CLK_EN, 1);\n\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 1);\n\tmdelay(100);\n\tWREG32_FIELD(VCE_SOFT_RESET, ECPU_SOFT_RESET, 0);\n\n\tr = vce_v2_0_firmware_loaded(adev);\n\n\t \n\tWREG32_P(mmVCE_STATUS, 0, ~1);\n\n\tif (r) {\n\t\tDRM_ERROR(\"VCE not responding, giving up!!!\\n\");\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_stop(struct amdgpu_device *adev)\n{\n\tint i;\n\tint status;\n\n\tif (vce_v2_0_lmi_clean(adev)) {\n\t\tDRM_INFO(\"vce is not idle \\n\");\n\t\treturn 0;\n\t}\n\n\tif (vce_v2_0_wait_for_idle(adev)) {\n\t\tDRM_INFO(\"VCE is busy, Can't set clock gating\");\n\t\treturn 0;\n\t}\n\n\t \n\tWREG32_P(mmVCE_LMI_CTRL2, 1 << 8, ~(1 << 8));\n\n\tfor (i = 0; i < 100; ++i) {\n\t\tstatus = RREG32(mmVCE_LMI_STATUS);\n\t\tif (status & 0x240)\n\t\t\tbreak;\n\t\tmdelay(1);\n\t}\n\n\tWREG32_P(mmVCE_VCPU_CNTL, 0, ~0x80001);\n\n\t \n\tWREG32_P(mmVCE_SOFT_RESET, 1, ~0x1);\n\n\tWREG32(mmVCE_STATUS, 0);\n\n\treturn 0;\n}\n\nstatic void vce_v2_0_set_sw_cg(struct amdgpu_device *adev, bool gated)\n{\n\tu32 tmp;\n\n\tif (gated) {\n\t\ttmp = RREG32(mmVCE_CLOCK_GATING_B);\n\t\ttmp |= 0xe70000;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, tmp);\n\n\t\ttmp = RREG32(mmVCE_UENC_CLOCK_GATING);\n\t\ttmp |= 0xff000000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING, tmp);\n\n\t\ttmp = RREG32(mmVCE_UENC_REG_CLOCK_GATING);\n\t\ttmp &= ~0x3fc;\n\t\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, tmp);\n\n\t\tWREG32(mmVCE_CGTT_CLK_OVERRIDE, 0);\n\t} else {\n\t\ttmp = RREG32(mmVCE_CLOCK_GATING_B);\n\t\ttmp |= 0xe7;\n\t\ttmp &= ~0xe70000;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, tmp);\n\n\t\ttmp = RREG32(mmVCE_UENC_CLOCK_GATING);\n\t\ttmp |= 0x1fe000;\n\t\ttmp &= ~0xff000000;\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING, tmp);\n\n\t\ttmp = RREG32(mmVCE_UENC_REG_CLOCK_GATING);\n\t\ttmp |= 0x3fc;\n\t\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, tmp);\n\t}\n}\n\nstatic void vce_v2_0_set_dyn_cg(struct amdgpu_device *adev, bool gated)\n{\n\tu32 orig, tmp;\n\n \n\ttmp = RREG32(mmVCE_CLOCK_GATING_B);\n\ttmp &= ~0x00060006;\n\n \n\tif (gated) {\n\t\ttmp |= 0xe10000;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, tmp);\n\t} else {\n\t\ttmp |= 0xe1;\n\t\ttmp &= ~0xe10000;\n\t\tWREG32(mmVCE_CLOCK_GATING_B, tmp);\n\t}\n\n\torig = tmp = RREG32(mmVCE_UENC_CLOCK_GATING);\n\ttmp &= ~0x1fe000;\n\ttmp &= ~0xff000000;\n\tif (tmp != orig)\n\t\tWREG32(mmVCE_UENC_CLOCK_GATING, tmp);\n\n\torig = tmp = RREG32(mmVCE_UENC_REG_CLOCK_GATING);\n\ttmp &= ~0x3fc;\n\tif (tmp != orig)\n\t\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, tmp);\n\n\t \n\tWREG32(mmVCE_UENC_REG_CLOCK_GATING, 0x00);\n\n\tif(gated)\n\t\tWREG32(mmVCE_CGTT_CLK_OVERRIDE, 0);\n}\n\nstatic void vce_v2_0_enable_mgcg(struct amdgpu_device *adev, bool enable,\n\t\t\t\t\t\t\t\tbool sw_cg)\n{\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_VCE_MGCG)) {\n\t\tif (sw_cg)\n\t\t\tvce_v2_0_set_sw_cg(adev, true);\n\t\telse\n\t\t\tvce_v2_0_set_dyn_cg(adev, true);\n\t} else {\n\t\tvce_v2_0_disable_cg(adev);\n\n\t\tif (sw_cg)\n\t\t\tvce_v2_0_set_sw_cg(adev, false);\n\t\telse\n\t\t\tvce_v2_0_set_dyn_cg(adev, false);\n\t}\n}\n\nstatic int vce_v2_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tadev->vce.num_rings = 2;\n\n\tvce_v2_0_set_ring_funcs(adev);\n\tvce_v2_0_set_irq_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tr = amdgpu_irq_add_id(adev, AMDGPU_IRQ_CLIENTID_LEGACY, 167, &adev->vce.irq);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vce_sw_init(adev, VCE_V2_0_FW_SIZE +\n\t\tVCE_V2_0_STACK_SIZE + VCE_V2_0_DATA_SIZE);\n\tif (r)\n\t\treturn r;\n\n\tr = amdgpu_vce_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tenum amdgpu_ring_priority_level hw_prio = amdgpu_vce_get_ring_prio(i);\n\n\t\tring = &adev->vce.ring[i];\n\t\tsprintf(ring->name, \"vce%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 512, &adev->vce.irq, 0,\n\t\t\t\t     hw_prio, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = amdgpu_vce_entity_init(adev);\n\n\treturn r;\n}\n\nstatic int vce_v2_0_sw_fini(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vce_suspend(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_sw_fini(adev);\n}\n\nstatic int vce_v2_0_hw_init(void *handle)\n{\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tamdgpu_asic_set_vce_clocks(adev, 10000, 10000);\n\tvce_v2_0_enable_mgcg(adev, true, false);\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tr = amdgpu_ring_test_helper(&adev->vce.ring[i]);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tDRM_INFO(\"VCE initialized successfully.\\n\");\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_suspend(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\n\t \n\tcancel_delayed_work_sync(&adev->vce.idle_work);\n\n\tif (adev->pm.dpm_enabled) {\n\t\tamdgpu_dpm_enable_vce(adev, false);\n\t} else {\n\t\tamdgpu_asic_set_vce_clocks(adev, 0, 0);\n\t\tamdgpu_device_ip_set_powergating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_PG_STATE_GATE);\n\t\tamdgpu_device_ip_set_clockgating_state(adev, AMD_IP_BLOCK_TYPE_VCE,\n\t\t\t\t\t\t       AMD_CG_STATE_GATE);\n\t}\n\n\tr = vce_v2_0_hw_fini(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn amdgpu_vce_suspend(adev);\n}\n\nstatic int vce_v2_0_resume(void *handle)\n{\n\tint r;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tr = amdgpu_vce_resume(adev);\n\tif (r)\n\t\treturn r;\n\n\treturn vce_v2_0_hw_init(adev);\n}\n\nstatic int vce_v2_0_soft_reset(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tWREG32_FIELD(SRBM_SOFT_RESET, SOFT_RESET_VCE, 1);\n\tmdelay(5);\n\n\treturn vce_v2_0_start(adev);\n}\n\nstatic int vce_v2_0_set_interrupt_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tuint32_t val = 0;\n\n\tif (state == AMDGPU_IRQ_STATE_ENABLE)\n\t\tval |= VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK;\n\n\tWREG32_P(mmVCE_SYS_INT_EN, val, ~VCE_SYS_INT_EN__VCE_SYS_INT_TRAP_INTERRUPT_EN_MASK);\n\treturn 0;\n}\n\nstatic int vce_v2_0_process_interrupt(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tDRM_DEBUG(\"IH: VCE\\n\");\n\tswitch (entry->src_data[0]) {\n\tcase 0:\n\tcase 1:\n\t\tamdgpu_fence_process(&adev->vce.ring[entry->src_data[0]]);\n\t\tbreak;\n\tdefault:\n\t\tDRM_ERROR(\"Unhandled interrupt: %d %d\\n\",\n\t\t\t  entry->src_id, entry->src_data[0]);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tbool gate = false;\n\tbool sw_cg = false;\n\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (state == AMD_CG_STATE_GATE) {\n\t\tgate = true;\n\t\tsw_cg = true;\n\t}\n\n\tvce_v2_0_enable_mgcg(adev, gate, sw_cg);\n\n\treturn 0;\n}\n\nstatic int vce_v2_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\t \n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (state == AMD_PG_STATE_GATE)\n\t\treturn vce_v2_0_stop(adev);\n\telse\n\t\treturn vce_v2_0_start(adev);\n}\n\nstatic const struct amd_ip_funcs vce_v2_0_ip_funcs = {\n\t.name = \"vce_v2_0\",\n\t.early_init = vce_v2_0_early_init,\n\t.late_init = NULL,\n\t.sw_init = vce_v2_0_sw_init,\n\t.sw_fini = vce_v2_0_sw_fini,\n\t.hw_init = vce_v2_0_hw_init,\n\t.hw_fini = vce_v2_0_hw_fini,\n\t.suspend = vce_v2_0_suspend,\n\t.resume = vce_v2_0_resume,\n\t.is_idle = vce_v2_0_is_idle,\n\t.wait_for_idle = vce_v2_0_wait_for_idle,\n\t.soft_reset = vce_v2_0_soft_reset,\n\t.set_clockgating_state = vce_v2_0_set_clockgating_state,\n\t.set_powergating_state = vce_v2_0_set_powergating_state,\n};\n\nstatic const struct amdgpu_ring_funcs vce_v2_0_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_VCE,\n\t.align_mask = 0xf,\n\t.nop = VCE_CMD_NO_OP,\n\t.support_64bit_ptrs = false,\n\t.no_user_fence = true,\n\t.get_rptr = vce_v2_0_ring_get_rptr,\n\t.get_wptr = vce_v2_0_ring_get_wptr,\n\t.set_wptr = vce_v2_0_ring_set_wptr,\n\t.parse_cs = amdgpu_vce_ring_parse_cs,\n\t.emit_frame_size = 6,  \n\t.emit_ib_size = 4,  \n\t.emit_ib = amdgpu_vce_ring_emit_ib,\n\t.emit_fence = amdgpu_vce_ring_emit_fence,\n\t.test_ring = amdgpu_vce_ring_test_ring,\n\t.test_ib = amdgpu_vce_ring_test_ib,\n\t.insert_nop = amdgpu_ring_insert_nop,\n\t.pad_ib = amdgpu_ring_generic_pad_ib,\n\t.begin_use = amdgpu_vce_ring_begin_use,\n\t.end_use = amdgpu_vce_ring_end_use,\n};\n\nstatic void vce_v2_0_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->vce.num_rings; i++) {\n\t\tadev->vce.ring[i].funcs = &vce_v2_0_ring_funcs;\n\t\tadev->vce.ring[i].me = i;\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs vce_v2_0_irq_funcs = {\n\t.set = vce_v2_0_set_interrupt_state,\n\t.process = vce_v2_0_process_interrupt,\n};\n\nstatic void vce_v2_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->vce.irq.num_types = 1;\n\tadev->vce.irq.funcs = &vce_v2_0_irq_funcs;\n};\n\nconst struct amdgpu_ip_block_version vce_v2_0_ip_block =\n{\n\t\t.type = AMD_IP_BLOCK_TYPE_VCE,\n\t\t.major = 2,\n\t\t.minor = 0,\n\t\t.rev = 0,\n\t\t.funcs = &vce_v2_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}