{
  "module_name": "sdma_v4_0.c",
  "hash_id": "0036238b7c30693c3bb641fdef9c27f0f761b73f0155ba5e40612a8c8b0d3a03",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/sdma_v4_0.c",
  "human_readable_source": " \n\n#include <linux/delay.h>\n#include <linux/firmware.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_ucode.h\"\n#include \"amdgpu_trace.h\"\n\n#include \"sdma0/sdma0_4_2_offset.h\"\n#include \"sdma0/sdma0_4_2_sh_mask.h\"\n#include \"sdma1/sdma1_4_2_offset.h\"\n#include \"sdma1/sdma1_4_2_sh_mask.h\"\n#include \"sdma2/sdma2_4_2_2_offset.h\"\n#include \"sdma2/sdma2_4_2_2_sh_mask.h\"\n#include \"sdma3/sdma3_4_2_2_offset.h\"\n#include \"sdma3/sdma3_4_2_2_sh_mask.h\"\n#include \"sdma4/sdma4_4_2_2_offset.h\"\n#include \"sdma4/sdma4_4_2_2_sh_mask.h\"\n#include \"sdma5/sdma5_4_2_2_offset.h\"\n#include \"sdma5/sdma5_4_2_2_sh_mask.h\"\n#include \"sdma6/sdma6_4_2_2_offset.h\"\n#include \"sdma6/sdma6_4_2_2_sh_mask.h\"\n#include \"sdma7/sdma7_4_2_2_offset.h\"\n#include \"sdma7/sdma7_4_2_2_sh_mask.h\"\n#include \"sdma0/sdma0_4_1_default.h\"\n\n#include \"soc15_common.h\"\n#include \"soc15.h\"\n#include \"vega10_sdma_pkt_open.h\"\n\n#include \"ivsrcid/sdma0/irqsrcs_sdma0_4_0.h\"\n#include \"ivsrcid/sdma1/irqsrcs_sdma1_4_0.h\"\n\n#include \"amdgpu_ras.h\"\n#include \"sdma_v4_4.h\"\n\nMODULE_FIRMWARE(\"amdgpu/vega10_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega10_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega12_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega12_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega20_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/vega20_sdma1.bin\");\nMODULE_FIRMWARE(\"amdgpu/raven_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/picasso_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/raven2_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/arcturus_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/renoir_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/green_sardine_sdma.bin\");\nMODULE_FIRMWARE(\"amdgpu/aldebaran_sdma.bin\");\n\n#define SDMA0_POWER_CNTL__ON_OFF_CONDITION_HOLD_TIME_MASK  0x000000F8L\n#define SDMA0_POWER_CNTL__ON_OFF_STATUS_DURATION_TIME_MASK 0xFC000000L\n\n#define WREG32_SDMA(instance, offset, value) \\\n\tWREG32(sdma_v4_0_get_reg_offset(adev, (instance), (offset)), value)\n#define RREG32_SDMA(instance, offset) \\\n\tRREG32(sdma_v4_0_get_reg_offset(adev, (instance), (offset)))\n\nstatic void sdma_v4_0_set_ring_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_0_set_buffer_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_0_set_vm_pte_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_0_set_irq_funcs(struct amdgpu_device *adev);\nstatic void sdma_v4_0_set_ras_funcs(struct amdgpu_device *adev);\n\nstatic const struct soc15_reg_golden golden_settings_sdma_4[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CHICKEN_BITS, 0xfe931f07, 0x02831d07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CLK_CTRL, 0xff000ff0, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_PAGE_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_PAGE_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_POWER_CNTL, 0x003ff006, 0x0003c000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_PAGE, 0x000003ff, 0x000003c0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_WATERMK, 0xfc000000, 0x00000000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CLK_CTRL, 0xffffffff, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GFX_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GFX_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_PAGE_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_PAGE_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_POWER_CNTL, 0x003ff000, 0x0003c000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC0_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC0_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC1_IB_CNTL, 0x800f0100, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC1_RB_WPTR_POLL_CNTL, 0x0000fff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_PAGE, 0x000003ff, 0x000003c0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_WATERMK, 0xfc000000, 0x00000000)\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_vg10[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CHICKEN_BITS, 0xfe931f07, 0x02831d07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_vg12[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00104001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CHICKEN_BITS, 0xfe931f07, 0x02831d07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG, 0x0018773f, 0x00104001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_4_1[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CHICKEN_BITS, 0xfe931f07, 0x02831d07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CLK_CTRL, 0xffffffff, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_IB_CNTL, 0x800f0111, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_POWER_CNTL, 0xfc3fffff, 0x40000051),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_IB_CNTL, 0x800f0111, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_IB_CNTL, 0x800f0111, 0x00000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_PAGE, 0x000003ff, 0x000003e0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_WATERMK, 0xfc000000, 0x00000000)\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma0_4_2_init[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_WPTR_POLL_CNTL, 0xfffffff0, 0x00403000),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma0_4_2[] =\n{\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CLK_CTRL, 0xffffffff, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_PAGE_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_PAGE_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RD_BURST_CNTL, 0x0000000f, 0x00000003),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_WPTR_POLL_CNTL, 0xfffffff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC2_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC2_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC3_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC3_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC4_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC4_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC5_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC5_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC6_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC6_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC7_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC7_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_PAGE, 0x000003ff, 0x000003c0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma1_4_2[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CLK_CTRL, 0xffffffff, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GFX_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GFX_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_PAGE_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_PAGE_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RD_BURST_CNTL, 0x0000000f, 0x00000003),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC0_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC0_RB_WPTR_POLL_CNTL, 0xfffffff0, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC1_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC1_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC2_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC2_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC3_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC3_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC4_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC4_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC5_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC5_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC6_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC6_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC7_RB_RPTR_ADDR_LO, 0xfffffffd, 0x00000001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_RLC7_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_PAGE, 0x000003ff, 0x000003c0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_rv1[] =\n{\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00000002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00000002)\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_rv2[] =\n{\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00003001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00003001)\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_arct[] =\n{\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA5, 0, mmSDMA5_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA5, 0, mmSDMA5_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA5, 0, mmSDMA5_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA5, 0, mmSDMA5_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA6, 0, mmSDMA6_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA6, 0, mmSDMA6_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA6, 0, mmSDMA6_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA6, 0, mmSDMA6_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA7, 0, mmSDMA7_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA7, 0, mmSDMA7_GB_ADDR_CONFIG, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA7, 0, mmSDMA7_GB_ADDR_CONFIG_READ, 0x0000773f, 0x00004002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA7, 0, mmSDMA7_UTCL1_TIMEOUT, 0xffffffff, 0x00010001)\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_aldebaran[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA1, 0, mmSDMA1_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA2, 0, mmSDMA2_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA2_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA3, 0, mmSDMA3_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_GB_ADDR_CONFIG, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00104002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA4, 0, mmSDMA4_UTCL1_TIMEOUT, 0xffffffff, 0x00010001),\n};\n\nstatic const struct soc15_reg_golden golden_settings_sdma_4_3[] = {\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CHICKEN_BITS, 0xfe931f07, 0x02831f07),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_CLK_CTRL, 0xffffffff, 0x3f000100),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG, 0x0018773f, 0x00000002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GB_ADDR_CONFIG_READ, 0x0018773f, 0x00000002),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_GFX_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_POWER_CNTL, 0x003fff07, 0x40000051),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC0_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_RLC1_RB_WPTR_POLL_CNTL, 0xfffffff7, 0x00403000),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_PAGE, 0x000003ff, 0x000003e0),\n\tSOC15_REG_GOLDEN_VALUE(SDMA0, 0, mmSDMA0_UTCL1_WATERMK, 0xfc000000, 0x03fbe1fe)\n};\n\nstatic const struct soc15_ras_field_entry sdma_v4_0_ras_fields[] = {\n\t{ \"SDMA_UCODE_BUF_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_UCODE_BUF_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_RB_CMD_BUF_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_RB_CMD_BUF_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_IB_CMD_BUF_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_IB_CMD_BUF_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_UTCL1_RD_FIFO_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_UTCL1_RD_FIFO_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_UTCL1_RDBST_FIFO_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_UTCL1_RDBST_FIFO_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_DATA_LUT_FIFO_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_DATA_LUT_FIFO_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF0_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF0_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF1_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF1_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF2_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF2_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF3_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF3_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF4_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF4_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF5_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF5_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF6_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF6_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF7_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF7_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF8_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF8_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF9_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF9_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF10_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF10_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF11_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF11_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF12_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF12_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF13_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF13_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF14_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF14_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MBANK_DATA_BUF15_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MBANK_DATA_BUF15_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_SPLIT_DAT_BUF_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_SPLIT_DAT_BUF_SED),\n\t0, 0,\n\t},\n\t{ \"SDMA_MC_WR_ADDR_FIFO_SED\", SOC15_REG_ENTRY(SDMA0, 0, mmSDMA0_EDC_COUNTER),\n\tSOC15_REG_FIELD(SDMA0_EDC_COUNTER, SDMA_MC_WR_ADDR_FIFO_SED),\n\t0, 0,\n\t},\n};\n\nstatic u32 sdma_v4_0_get_reg_offset(struct amdgpu_device *adev,\n\t\tu32 instance, u32 offset)\n{\n\tswitch (instance) {\n\tcase 0:\n\t\treturn (adev->reg_offset[SDMA0_HWIP][0][0] + offset);\n\tcase 1:\n\t\treturn (adev->reg_offset[SDMA1_HWIP][0][0] + offset);\n\tcase 2:\n\t\treturn (adev->reg_offset[SDMA2_HWIP][0][1] + offset);\n\tcase 3:\n\t\treturn (adev->reg_offset[SDMA3_HWIP][0][1] + offset);\n\tcase 4:\n\t\treturn (adev->reg_offset[SDMA4_HWIP][0][1] + offset);\n\tcase 5:\n\t\treturn (adev->reg_offset[SDMA5_HWIP][0][1] + offset);\n\tcase 6:\n\t\treturn (adev->reg_offset[SDMA6_HWIP][0][1] + offset);\n\tcase 7:\n\t\treturn (adev->reg_offset[SDMA7_HWIP][0][1] + offset);\n\tdefault:\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic unsigned sdma_v4_0_seq_to_irq_id(int seq_num)\n{\n\tswitch (seq_num) {\n\tcase 0:\n\t\treturn SOC15_IH_CLIENTID_SDMA0;\n\tcase 1:\n\t\treturn SOC15_IH_CLIENTID_SDMA1;\n\tcase 2:\n\t\treturn SOC15_IH_CLIENTID_SDMA2;\n\tcase 3:\n\t\treturn SOC15_IH_CLIENTID_SDMA3;\n\tcase 4:\n\t\treturn SOC15_IH_CLIENTID_SDMA4;\n\tcase 5:\n\t\treturn SOC15_IH_CLIENTID_SDMA5;\n\tcase 6:\n\t\treturn SOC15_IH_CLIENTID_SDMA6;\n\tcase 7:\n\t\treturn SOC15_IH_CLIENTID_SDMA7;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic int sdma_v4_0_irq_id_to_seq(unsigned client_id)\n{\n\tswitch (client_id) {\n\tcase SOC15_IH_CLIENTID_SDMA0:\n\t\treturn 0;\n\tcase SOC15_IH_CLIENTID_SDMA1:\n\t\treturn 1;\n\tcase SOC15_IH_CLIENTID_SDMA2:\n\t\treturn 2;\n\tcase SOC15_IH_CLIENTID_SDMA3:\n\t\treturn 3;\n\tcase SOC15_IH_CLIENTID_SDMA4:\n\t\treturn 4;\n\tcase SOC15_IH_CLIENTID_SDMA5:\n\t\treturn 5;\n\tcase SOC15_IH_CLIENTID_SDMA6:\n\t\treturn 6;\n\tcase SOC15_IH_CLIENTID_SDMA7:\n\t\treturn 7;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nstatic void sdma_v4_0_init_golden_registers(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 0, 0):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_4,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_4));\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_vg10,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_vg10));\n\t\tbreak;\n\tcase IP_VERSION(4, 0, 1):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_4,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_4));\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_vg12,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_vg12));\n\t\tbreak;\n\tcase IP_VERSION(4, 2, 0):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma0_4_2_init,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma0_4_2_init));\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma0_4_2,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma0_4_2));\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma1_4_2,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma1_4_2));\n\t\tbreak;\n\tcase IP_VERSION(4, 2, 2):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_arct,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_arct));\n\t\tbreak;\n\tcase IP_VERSION(4, 4, 0):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_aldebaran,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_aldebaran));\n\t\tbreak;\n\tcase IP_VERSION(4, 1, 0):\n\tcase IP_VERSION(4, 1, 1):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_4_1,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_4_1));\n\t\tif (adev->apu_flags & AMD_APU_IS_RAVEN2)\n\t\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\t\tgolden_settings_sdma_rv2,\n\t\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_rv2));\n\t\telse\n\t\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\t\tgolden_settings_sdma_rv1,\n\t\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_rv1));\n\t\tbreak;\n\tcase IP_VERSION(4, 1, 2):\n\t\tsoc15_program_register_sequence(adev,\n\t\t\t\t\t\tgolden_settings_sdma_4_3,\n\t\t\t\t\t\tARRAY_SIZE(golden_settings_sdma_4_3));\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void sdma_v4_0_setup_ulv(struct amdgpu_device *adev)\n{\n\tint i;\n\n\t \n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 0, 0):\n\t\tif (adev->pdev->device == 0x6860)\n\t\t\tbreak;\n\t\treturn;\n\tcase IP_VERSION(4, 2, 0):\n\t\tif (adev->pdev->device == 0x66a1)\n\t\t\tbreak;\n\t\treturn;\n\tdefault:\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tuint32_t temp;\n\n\t\ttemp = RREG32_SDMA(i, mmSDMA0_ULV_CNTL);\n\t\ttemp = REG_SET_FIELD(temp, SDMA0_ULV_CNTL, HYSTERESIS, 0x0);\n\t\tWREG32_SDMA(i, mmSDMA0_ULV_CNTL, temp);\n\t}\n}\n\n \n\n\n\nstatic int sdma_v4_0_init_microcode(struct amdgpu_device *adev)\n{\n\tint ret, i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 2) ||\n                    adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 4, 0)) {\n\t\t\t \n\t\t\tret = amdgpu_sdma_init_microcode(adev, 0, true);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tret = amdgpu_sdma_init_microcode(adev, i, false);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic uint64_t sdma_v4_0_ring_get_rptr(struct amdgpu_ring *ring)\n{\n\tu64 *rptr;\n\n\t \n\trptr = ((u64 *)ring->rptr_cpu_addr);\n\n\tDRM_DEBUG(\"rptr before shift == 0x%016llx\\n\", *rptr);\n\treturn ((*rptr) >> 2);\n}\n\n \nstatic uint64_t sdma_v4_0_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu64 wptr;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\twptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));\n\t\tDRM_DEBUG(\"wptr/doorbell before shift == 0x%016llx\\n\", wptr);\n\t} else {\n\t\twptr = RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI);\n\t\twptr = wptr << 32;\n\t\twptr |= RREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR);\n\t\tDRM_DEBUG(\"wptr before shift [%i] wptr == 0x%016llx\\n\",\n\t\t\t\tring->me, wptr);\n\t}\n\n\treturn wptr >> 2;\n}\n\n \nstatic void sdma_v4_0_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tDRM_DEBUG(\"Setting write pointer\\n\");\n\tif (ring->use_doorbell) {\n\t\tu64 *wb = (u64 *)ring->wptr_cpu_addr;\n\n\t\tDRM_DEBUG(\"Using doorbell -- \"\n\t\t\t\t\"wptr_offs == 0x%08x \"\n\t\t\t\t\"lower_32_bits(ring->wptr << 2) == 0x%08x \"\n\t\t\t\t\"upper_32_bits(ring->wptr << 2) == 0x%08x\\n\",\n\t\t\t\tring->wptr_offs,\n\t\t\t\tlower_32_bits(ring->wptr << 2),\n\t\t\t\tupper_32_bits(ring->wptr << 2));\n\t\t \n\t\tWRITE_ONCE(*wb, (ring->wptr << 2));\n\t\tDRM_DEBUG(\"calling WDOORBELL64(0x%08x, 0x%016llx)\\n\",\n\t\t\t\tring->doorbell_index, ring->wptr << 2);\n\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t} else {\n\t\tDRM_DEBUG(\"Not using doorbell -- \"\n\t\t\t\t\"mmSDMA%i_GFX_RB_WPTR == 0x%08x \"\n\t\t\t\t\"mmSDMA%i_GFX_RB_WPTR_HI == 0x%08x\\n\",\n\t\t\t\tring->me,\n\t\t\t\tlower_32_bits(ring->wptr << 2),\n\t\t\t\tring->me,\n\t\t\t\tupper_32_bits(ring->wptr << 2));\n\t\tWREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR,\n\t\t\t    lower_32_bits(ring->wptr << 2));\n\t\tWREG32_SDMA(ring->me, mmSDMA0_GFX_RB_WPTR_HI,\n\t\t\t    upper_32_bits(ring->wptr << 2));\n\t}\n}\n\n \nstatic uint64_t sdma_v4_0_page_ring_get_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu64 wptr;\n\n\tif (ring->use_doorbell) {\n\t\t \n\t\twptr = READ_ONCE(*((u64 *)ring->wptr_cpu_addr));\n\t} else {\n\t\twptr = RREG32_SDMA(ring->me, mmSDMA0_PAGE_RB_WPTR_HI);\n\t\twptr = wptr << 32;\n\t\twptr |= RREG32_SDMA(ring->me, mmSDMA0_PAGE_RB_WPTR);\n\t}\n\n\treturn wptr >> 2;\n}\n\n \nstatic void sdma_v4_0_page_ring_set_wptr(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\n\tif (ring->use_doorbell) {\n\t\tu64 *wb = (u64 *)ring->wptr_cpu_addr;\n\n\t\t \n\t\tWRITE_ONCE(*wb, (ring->wptr << 2));\n\t\tWDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n\t} else {\n\t\tuint64_t wptr = ring->wptr << 2;\n\n\t\tWREG32_SDMA(ring->me, mmSDMA0_PAGE_RB_WPTR,\n\t\t\t    lower_32_bits(wptr));\n\t\tWREG32_SDMA(ring->me, mmSDMA0_PAGE_RB_WPTR_HI,\n\t\t\t    upper_32_bits(wptr));\n\t}\n}\n\nstatic void sdma_v4_0_ring_insert_nop(struct amdgpu_ring *ring, uint32_t count)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tint i;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(count - 1));\n\t\telse\n\t\t\tamdgpu_ring_write(ring, ring->funcs->nop);\n}\n\n \nstatic void sdma_v4_0_ring_emit_ib(struct amdgpu_ring *ring,\n\t\t\t\t   struct amdgpu_job *job,\n\t\t\t\t   struct amdgpu_ib *ib,\n\t\t\t\t   uint32_t flags)\n{\n\tunsigned vmid = AMDGPU_JOB_GET_VMID(job);\n\n\t \n\tsdma_v4_0_ring_insert_nop(ring, (2 - lower_32_bits(ring->wptr)) & 7);\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_INDIRECT) |\n\t\t\t  SDMA_PKT_INDIRECT_HEADER_VMID(vmid & 0xf));\n\t \n\tamdgpu_ring_write(ring, lower_32_bits(ib->gpu_addr) & 0xffffffe0);\n\tamdgpu_ring_write(ring, upper_32_bits(ib->gpu_addr));\n\tamdgpu_ring_write(ring, ib->length_dw);\n\tamdgpu_ring_write(ring, 0);\n\tamdgpu_ring_write(ring, 0);\n\n}\n\nstatic void sdma_v4_0_wait_reg_mem(struct amdgpu_ring *ring,\n\t\t\t\t   int mem_space, int hdp,\n\t\t\t\t   uint32_t addr0, uint32_t addr1,\n\t\t\t\t   uint32_t ref, uint32_t mask,\n\t\t\t\t   uint32_t inv)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_POLL_REGMEM) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_HDP_FLUSH(hdp) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_MEM_POLL(mem_space) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_HEADER_FUNC(3));  \n\tif (mem_space) {\n\t\t \n\t\tamdgpu_ring_write(ring, addr0);\n\t\tamdgpu_ring_write(ring, addr1);\n\t} else {\n\t\t \n\t\tamdgpu_ring_write(ring, addr0 << 2);\n\t\tamdgpu_ring_write(ring, addr1 << 2);\n\t}\n\tamdgpu_ring_write(ring, ref);  \n\tamdgpu_ring_write(ring, mask);  \n\tamdgpu_ring_write(ring, SDMA_PKT_POLL_REGMEM_DW5_RETRY_COUNT(0xfff) |\n\t\t\t  SDMA_PKT_POLL_REGMEM_DW5_INTERVAL(inv));  \n}\n\n \nstatic void sdma_v4_0_ring_emit_hdp_flush(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tu32 ref_and_mask = 0;\n\tconst struct nbio_hdp_flush_reg *nbio_hf_reg = adev->nbio.hdp_flush_reg;\n\n\tref_and_mask = nbio_hf_reg->ref_and_mask_sdma0 << ring->me;\n\n\tsdma_v4_0_wait_reg_mem(ring, 0, 1,\n\t\t\t       adev->nbio.funcs->get_hdp_flush_done_offset(adev),\n\t\t\t       adev->nbio.funcs->get_hdp_flush_req_offset(adev),\n\t\t\t       ref_and_mask, ref_and_mask, 10);\n}\n\n \nstatic void sdma_v4_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr, u64 seq,\n\t\t\t\t      unsigned flags)\n{\n\tbool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\t \n\tBUG_ON(addr & 0x3);\n\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\tamdgpu_ring_write(ring, lower_32_bits(seq));\n\n\t \n\tif (write64bit) {\n\t\taddr += 4;\n\t\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_FENCE));\n\t\t \n\t\tBUG_ON(addr & 0x3);\n\t\tamdgpu_ring_write(ring, lower_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(addr));\n\t\tamdgpu_ring_write(ring, upper_32_bits(seq));\n\t}\n\n\t \n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_TRAP));\n\tamdgpu_ring_write(ring, SDMA_PKT_TRAP_INT_CONTEXT_INT_CONTEXT(0));\n}\n\n\n \nstatic void sdma_v4_0_gfx_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 rb_cntl, ib_cntl;\n\tint i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\trb_cntl = RREG32_SDMA(i, mmSDMA0_GFX_RB_CNTL);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, enable ? 1 : 0);\n\t\tWREG32_SDMA(i, mmSDMA0_GFX_RB_CNTL, rb_cntl);\n\t\tib_cntl = RREG32_SDMA(i, mmSDMA0_GFX_IB_CNTL);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, enable ? 1 : 0);\n\t\tWREG32_SDMA(i, mmSDMA0_GFX_IB_CNTL, ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v4_0_rlc_stop(struct amdgpu_device *adev)\n{\n\t \n}\n\n \nstatic void sdma_v4_0_page_stop(struct amdgpu_device *adev)\n{\n\tu32 rb_cntl, ib_cntl;\n\tint i;\n\n\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\trb_cntl = RREG32_SDMA(i, mmSDMA0_PAGE_RB_CNTL);\n\t\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_PAGE_RB_CNTL,\n\t\t\t\t\tRB_ENABLE, 0);\n\t\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_CNTL, rb_cntl);\n\t\tib_cntl = RREG32_SDMA(i, mmSDMA0_PAGE_IB_CNTL);\n\t\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_PAGE_IB_CNTL,\n\t\t\t\t\tIB_ENABLE, 0);\n\t\tWREG32_SDMA(i, mmSDMA0_PAGE_IB_CNTL, ib_cntl);\n\t}\n}\n\n \nstatic void sdma_v4_0_ctx_switch_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl, phase_quantum = 0;\n\tint i;\n\n\tif (amdgpu_sdma_phase_quantum) {\n\t\tunsigned value = amdgpu_sdma_phase_quantum;\n\t\tunsigned unit = 0;\n\n\t\twhile (value > (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\tSDMA0_PHASE0_QUANTUM__VALUE__SHIFT)) {\n\t\t\tvalue = (value + 1) >> 1;\n\t\t\tunit++;\n\t\t}\n\t\tif (unit > (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t    SDMA0_PHASE0_QUANTUM__UNIT__SHIFT)) {\n\t\t\tvalue = (SDMA0_PHASE0_QUANTUM__VALUE_MASK >>\n\t\t\t\t SDMA0_PHASE0_QUANTUM__VALUE__SHIFT);\n\t\t\tunit = (SDMA0_PHASE0_QUANTUM__UNIT_MASK >>\n\t\t\t\tSDMA0_PHASE0_QUANTUM__UNIT__SHIFT);\n\t\t\tWARN_ONCE(1,\n\t\t\t\"clamping sdma_phase_quantum to %uK clock cycles\\n\",\n\t\t\t\t  value << unit);\n\t\t}\n\t\tphase_quantum =\n\t\t\tvalue << SDMA0_PHASE0_QUANTUM__VALUE__SHIFT |\n\t\t\tunit  << SDMA0_PHASE0_QUANTUM__UNIT__SHIFT;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tf32_cntl = RREG32_SDMA(i, mmSDMA0_CNTL);\n\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_CNTL,\n\t\t\t\tAUTO_CTXSW_ENABLE, enable ? 1 : 0);\n\t\tif (enable && amdgpu_sdma_phase_quantum) {\n\t\t\tWREG32_SDMA(i, mmSDMA0_PHASE0_QUANTUM, phase_quantum);\n\t\t\tWREG32_SDMA(i, mmSDMA0_PHASE1_QUANTUM, phase_quantum);\n\t\t\tWREG32_SDMA(i, mmSDMA0_PHASE2_QUANTUM, phase_quantum);\n\t\t}\n\t\tWREG32_SDMA(i, mmSDMA0_CNTL, f32_cntl);\n\n\t\t \n\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 2) &&\n\t\t    adev->sdma.instance[i].fw_version >= 14)\n\t\t\tWREG32_SDMA(i, mmSDMA0_PUB_DUMMY_REG2, enable);\n\t\t \n\t\tWREG32_SDMA(i, mmSDMA0_UTCL1_TIMEOUT, 0x00800080);\n\t}\n\n}\n\n \nstatic void sdma_v4_0_enable(struct amdgpu_device *adev, bool enable)\n{\n\tu32 f32_cntl;\n\tint i;\n\n\tif (!enable) {\n\t\tsdma_v4_0_gfx_enable(adev, enable);\n\t\tsdma_v4_0_rlc_stop(adev);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsdma_v4_0_page_stop(adev);\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tf32_cntl = RREG32_SDMA(i, mmSDMA0_F32_CNTL);\n\t\tf32_cntl = REG_SET_FIELD(f32_cntl, SDMA0_F32_CNTL, HALT, enable ? 0 : 1);\n\t\tWREG32_SDMA(i, mmSDMA0_F32_CNTL, f32_cntl);\n\t}\n}\n\n \nstatic uint32_t sdma_v4_0_rb_cntl(struct amdgpu_ring *ring, uint32_t rb_cntl)\n{\n\t \n\tuint32_t rb_bufsz = order_base_2(ring->ring_size / 4);\n\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SIZE, rb_bufsz);\n#ifdef __BIG_ENDIAN\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_SWAP_ENABLE, 1);\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL,\n\t\t\t\tRPTR_WRITEBACK_SWAP_ENABLE, 1);\n#endif\n\treturn rb_cntl;\n}\n\n \nstatic void sdma_v4_0_gfx_resume(struct amdgpu_device *adev, unsigned int i)\n{\n\tstruct amdgpu_ring *ring = &adev->sdma.instance[i].ring;\n\tu32 rb_cntl, ib_cntl, wptr_poll_cntl;\n\tu32 doorbell;\n\tu32 doorbell_offset;\n\tu64 wptr_gpu_addr;\n\n\trb_cntl = RREG32_SDMA(i, mmSDMA0_GFX_RB_CNTL);\n\trb_cntl = sdma_v4_0_rb_cntl(ring, rb_cntl);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_CNTL, rb_cntl);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_RPTR, 0);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_RPTR_HI, 0);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR, 0);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR_HI, 0);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_RPTR_ADDR_HI,\n\t       upper_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFF);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_RPTR_ADDR_LO,\n\t       lower_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFC);\n\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL,\n\t\t\t\tRPTR_WRITEBACK_ENABLE, 1);\n\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_BASE, ring->gpu_addr >> 8);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_BASE_HI, ring->gpu_addr >> 40);\n\n\tring->wptr = 0;\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_GFX_MINOR_PTR_UPDATE, 1);\n\n\tdoorbell = RREG32_SDMA(i, mmSDMA0_GFX_DOORBELL);\n\tdoorbell_offset = RREG32_SDMA(i, mmSDMA0_GFX_DOORBELL_OFFSET);\n\n\tdoorbell = REG_SET_FIELD(doorbell, SDMA0_GFX_DOORBELL, ENABLE,\n\t\t\t\t ring->use_doorbell);\n\tdoorbell_offset = REG_SET_FIELD(doorbell_offset,\n\t\t\t\t\tSDMA0_GFX_DOORBELL_OFFSET,\n\t\t\t\t\tOFFSET, ring->doorbell_index);\n\tWREG32_SDMA(i, mmSDMA0_GFX_DOORBELL, doorbell);\n\tWREG32_SDMA(i, mmSDMA0_GFX_DOORBELL_OFFSET, doorbell_offset);\n\n\tsdma_v4_0_ring_set_wptr(ring);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_GFX_MINOR_PTR_UPDATE, 0);\n\n\t \n\twptr_gpu_addr = ring->wptr_gpu_addr;\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR_POLL_ADDR_LO,\n\t\t    lower_32_bits(wptr_gpu_addr));\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR_POLL_ADDR_HI,\n\t\t    upper_32_bits(wptr_gpu_addr));\n\twptr_poll_cntl = RREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR_POLL_CNTL);\n\twptr_poll_cntl = REG_SET_FIELD(wptr_poll_cntl,\n\t\t\t\t       SDMA0_GFX_RB_WPTR_POLL_CNTL,\n\t\t\t\t       F32_POLL_ENABLE, amdgpu_sriov_vf(adev)? 1 : 0);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_WPTR_POLL_CNTL, wptr_poll_cntl);\n\n\t \n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_GFX_RB_CNTL, RB_ENABLE, 1);\n\tWREG32_SDMA(i, mmSDMA0_GFX_RB_CNTL, rb_cntl);\n\n\tib_cntl = RREG32_SDMA(i, mmSDMA0_GFX_IB_CNTL);\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_GFX_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t \n\tWREG32_SDMA(i, mmSDMA0_GFX_IB_CNTL, ib_cntl);\n}\n\n \nstatic void sdma_v4_0_page_resume(struct amdgpu_device *adev, unsigned int i)\n{\n\tstruct amdgpu_ring *ring = &adev->sdma.instance[i].page;\n\tu32 rb_cntl, ib_cntl, wptr_poll_cntl;\n\tu32 doorbell;\n\tu32 doorbell_offset;\n\tu64 wptr_gpu_addr;\n\n\trb_cntl = RREG32_SDMA(i, mmSDMA0_PAGE_RB_CNTL);\n\trb_cntl = sdma_v4_0_rb_cntl(ring, rb_cntl);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_CNTL, rb_cntl);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_RPTR, 0);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_RPTR_HI, 0);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR, 0);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR_HI, 0);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_RPTR_ADDR_HI,\n\t       upper_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFF);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_RPTR_ADDR_LO,\n\t       lower_32_bits(ring->rptr_gpu_addr) & 0xFFFFFFFC);\n\n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_PAGE_RB_CNTL,\n\t\t\t\tRPTR_WRITEBACK_ENABLE, 1);\n\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_BASE, ring->gpu_addr >> 8);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_BASE_HI, ring->gpu_addr >> 40);\n\n\tring->wptr = 0;\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_PAGE_MINOR_PTR_UPDATE, 1);\n\n\tdoorbell = RREG32_SDMA(i, mmSDMA0_PAGE_DOORBELL);\n\tdoorbell_offset = RREG32_SDMA(i, mmSDMA0_PAGE_DOORBELL_OFFSET);\n\n\tdoorbell = REG_SET_FIELD(doorbell, SDMA0_PAGE_DOORBELL, ENABLE,\n\t\t\t\t ring->use_doorbell);\n\tdoorbell_offset = REG_SET_FIELD(doorbell_offset,\n\t\t\t\t\tSDMA0_PAGE_DOORBELL_OFFSET,\n\t\t\t\t\tOFFSET, ring->doorbell_index);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_DOORBELL, doorbell);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_DOORBELL_OFFSET, doorbell_offset);\n\n\t \n\tsdma_v4_0_page_ring_set_wptr(ring);\n\n\t \n\tWREG32_SDMA(i, mmSDMA0_PAGE_MINOR_PTR_UPDATE, 0);\n\n\t \n\twptr_gpu_addr = ring->wptr_gpu_addr;\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR_POLL_ADDR_LO,\n\t\t    lower_32_bits(wptr_gpu_addr));\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR_POLL_ADDR_HI,\n\t\t    upper_32_bits(wptr_gpu_addr));\n\twptr_poll_cntl = RREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR_POLL_CNTL);\n\twptr_poll_cntl = REG_SET_FIELD(wptr_poll_cntl,\n\t\t\t\t       SDMA0_PAGE_RB_WPTR_POLL_CNTL,\n\t\t\t\t       F32_POLL_ENABLE, amdgpu_sriov_vf(adev)? 1 : 0);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_WPTR_POLL_CNTL, wptr_poll_cntl);\n\n\t \n\trb_cntl = REG_SET_FIELD(rb_cntl, SDMA0_PAGE_RB_CNTL, RB_ENABLE, 1);\n\tWREG32_SDMA(i, mmSDMA0_PAGE_RB_CNTL, rb_cntl);\n\n\tib_cntl = RREG32_SDMA(i, mmSDMA0_PAGE_IB_CNTL);\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_PAGE_IB_CNTL, IB_ENABLE, 1);\n#ifdef __BIG_ENDIAN\n\tib_cntl = REG_SET_FIELD(ib_cntl, SDMA0_PAGE_IB_CNTL, IB_SWAP_ENABLE, 1);\n#endif\n\t \n\tWREG32_SDMA(i, mmSDMA0_PAGE_IB_CNTL, ib_cntl);\n}\n\nstatic void\nsdma_v4_1_update_power_gating(struct amdgpu_device *adev, bool enable)\n{\n\tuint32_t def, data;\n\n\tif (enable && (adev->pg_flags & AMD_PG_SUPPORT_SDMA)) {\n\t\t \n\t\tdef = data = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL));\n\t\tdata |= SDMA0_CNTL__CTXEMPTY_INT_ENABLE_MASK;\n\n\t\tif (data != def)\n\t\t\tWREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL), data);\n\t} else {\n\t\t \n\t\tdef = data = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL));\n\t\tdata &= ~SDMA0_CNTL__CTXEMPTY_INT_ENABLE_MASK;\n\t\tif (data != def)\n\t\t\tWREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL), data);\n\t}\n}\n\nstatic void sdma_v4_1_init_power_gating(struct amdgpu_device *adev)\n{\n\tuint32_t def, data;\n\n\t \n\tdef = data = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_POWER_CNTL));\n\tdata |= SDMA0_POWER_CNTL__PG_CNTL_ENABLE_MASK;\n\tif (data != def)\n\t\tWREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_POWER_CNTL), data);\n\n\t \n\tdef = data = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL));\n\tdata |= SDMA0_CNTL__CTXEMPTY_INT_ENABLE_MASK;\n\tif (data != def)\n\t\tWREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CNTL), data);\n\n\t \n\tdef = data = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_POWER_CNTL));\n\tdata &= ~SDMA0_POWER_CNTL__ON_OFF_CONDITION_HOLD_TIME_MASK;\n\tdata |= (mmSDMA0_POWER_CNTL_DEFAULT & SDMA0_POWER_CNTL__ON_OFF_CONDITION_HOLD_TIME_MASK);\n\t \n\tdata &= ~SDMA0_POWER_CNTL__ON_OFF_STATUS_DURATION_TIME_MASK;\n\tdata |= (mmSDMA0_POWER_CNTL_DEFAULT & SDMA0_POWER_CNTL__ON_OFF_STATUS_DURATION_TIME_MASK);\n\tif(data != def)\n\t\tWREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_POWER_CNTL), data);\n}\n\nstatic void sdma_v4_0_init_pg(struct amdgpu_device *adev)\n{\n\tif (!(adev->pg_flags & AMD_PG_SUPPORT_SDMA))\n\t\treturn;\n\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 1, 0):\n        case IP_VERSION(4, 1, 1):\n\tcase IP_VERSION(4, 1, 2):\n\t\tsdma_v4_1_init_power_gating(adev);\n\t\tsdma_v4_1_update_power_gating(adev, true);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\n \nstatic int sdma_v4_0_rlc_resume(struct amdgpu_device *adev)\n{\n\tsdma_v4_0_init_pg(adev);\n\n\treturn 0;\n}\n\n \nstatic int sdma_v4_0_load_microcode(struct amdgpu_device *adev)\n{\n\tconst struct sdma_firmware_header_v1_0 *hdr;\n\tconst __le32 *fw_data;\n\tu32 fw_size;\n\tint i, j;\n\n\t \n\tsdma_v4_0_enable(adev, false);\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (!adev->sdma.instance[i].fw)\n\t\t\treturn -EINVAL;\n\n\t\thdr = (const struct sdma_firmware_header_v1_0 *)adev->sdma.instance[i].fw->data;\n\t\tamdgpu_ucode_print_sdma_hdr(&hdr->header);\n\t\tfw_size = le32_to_cpu(hdr->header.ucode_size_bytes) / 4;\n\n\t\tfw_data = (const __le32 *)\n\t\t\t(adev->sdma.instance[i].fw->data +\n\t\t\t\tle32_to_cpu(hdr->header.ucode_array_offset_bytes));\n\n\t\tWREG32_SDMA(i, mmSDMA0_UCODE_ADDR, 0);\n\n\t\tfor (j = 0; j < fw_size; j++)\n\t\t\tWREG32_SDMA(i, mmSDMA0_UCODE_DATA,\n\t\t\t\t    le32_to_cpup(fw_data++));\n\n\t\tWREG32_SDMA(i, mmSDMA0_UCODE_ADDR,\n\t\t\t    adev->sdma.instance[i].fw_version);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int sdma_v4_0_start(struct amdgpu_device *adev)\n{\n\tstruct amdgpu_ring *ring;\n\tint i, r = 0;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tsdma_v4_0_ctx_switch_enable(adev, false);\n\t\tsdma_v4_0_enable(adev, false);\n\t} else {\n\n\t\tif (adev->firmware.load_type != AMDGPU_FW_LOAD_PSP) {\n\t\t\tr = sdma_v4_0_load_microcode(adev);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\n\t\t \n\t\tsdma_v4_0_enable(adev, true);\n\t\t \n\t\tsdma_v4_0_ctx_switch_enable(adev, true);\n\t}\n\n\t \n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tuint32_t temp;\n\n\t\tWREG32_SDMA(i, mmSDMA0_SEM_WAIT_FAIL_TIMER_CNTL, 0);\n\t\tsdma_v4_0_gfx_resume(adev, i);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsdma_v4_0_page_resume(adev, i);\n\n\t\t \n\t\ttemp = RREG32_SDMA(i, mmSDMA0_CNTL);\n\t\ttemp = REG_SET_FIELD(temp, SDMA0_CNTL, UTC_L1_ENABLE, 1);\n\t\tWREG32_SDMA(i, mmSDMA0_CNTL, temp);\n\n\t\tif (!amdgpu_sriov_vf(adev)) {\n\t\t\t \n\t\t\ttemp = RREG32_SDMA(i, mmSDMA0_F32_CNTL);\n\t\t\ttemp = REG_SET_FIELD(temp, SDMA0_F32_CNTL, HALT, 0);\n\t\t\tWREG32_SDMA(i, mmSDMA0_F32_CNTL, temp);\n\t\t}\n\t}\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\tsdma_v4_0_ctx_switch_enable(adev, true);\n\t\tsdma_v4_0_enable(adev, true);\n\t} else {\n\t\tr = sdma_v4_0_rlc_resume(adev);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\n\t\tr = amdgpu_ring_test_helper(ring);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tstruct amdgpu_ring *page = &adev->sdma.instance[i].page;\n\n\t\t\tr = amdgpu_ring_test_helper(page);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\n\t\t\tif (adev->mman.buffer_funcs_ring == page)\n\t\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t\t}\n\n\t\tif (adev->mman.buffer_funcs_ring == ring)\n\t\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t}\n\n\treturn r;\n}\n\n \nstatic int sdma_v4_0_ring_test_ring(struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tunsigned i;\n\tunsigned index;\n\tint r;\n\tu32 tmp;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\n\tr = amdgpu_ring_alloc(ring, 5);\n\tif (r)\n\t\tgoto error_free_wb;\n\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\t\t  SDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR));\n\tamdgpu_ring_write(ring, lower_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, upper_32_bits(gpu_addr));\n\tamdgpu_ring_write(ring, SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0));\n\tamdgpu_ring_write(ring, 0xDEADBEEF);\n\tamdgpu_ring_commit(ring);\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\t\tif (tmp == 0xDEADBEEF)\n\t\t\tbreak;\n\t\tudelay(1);\n\t}\n\n\tif (i >= adev->usec_timeout)\n\t\tr = -ETIMEDOUT;\n\nerror_free_wb:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n \nstatic int sdma_v4_0_ring_test_ib(struct amdgpu_ring *ring, long timeout)\n{\n\tstruct amdgpu_device *adev = ring->adev;\n\tstruct amdgpu_ib ib;\n\tstruct dma_fence *f = NULL;\n\tunsigned index;\n\tlong r;\n\tu32 tmp = 0;\n\tu64 gpu_addr;\n\n\tr = amdgpu_device_wb_get(adev, &index);\n\tif (r)\n\t\treturn r;\n\n\tgpu_addr = adev->wb.gpu_addr + (index * 4);\n\ttmp = 0xCAFEDEAD;\n\tadev->wb.wb[index] = cpu_to_le32(tmp);\n\tmemset(&ib, 0, sizeof(ib));\n\tr = amdgpu_ib_get(adev, NULL, 256,\n\t\t\t\t\tAMDGPU_IB_POOL_DIRECT, &ib);\n\tif (r)\n\t\tgoto err0;\n\n\tib.ptr[0] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib.ptr[1] = lower_32_bits(gpu_addr);\n\tib.ptr[2] = upper_32_bits(gpu_addr);\n\tib.ptr[3] = SDMA_PKT_WRITE_UNTILED_DW_3_COUNT(0);\n\tib.ptr[4] = 0xDEADBEEF;\n\tib.ptr[5] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[6] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.ptr[7] = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP);\n\tib.length_dw = 8;\n\n\tr = amdgpu_ib_schedule(ring, 1, &ib, NULL, &f);\n\tif (r)\n\t\tgoto err1;\n\n\tr = dma_fence_wait_timeout(f, false, timeout);\n\tif (r == 0) {\n\t\tr = -ETIMEDOUT;\n\t\tgoto err1;\n\t} else if (r < 0) {\n\t\tgoto err1;\n\t}\n\ttmp = le32_to_cpu(adev->wb.wb[index]);\n\tif (tmp == 0xDEADBEEF)\n\t\tr = 0;\n\telse\n\t\tr = -EINVAL;\n\nerr1:\n\tamdgpu_ib_free(adev, &ib, NULL);\n\tdma_fence_put(f);\nerr0:\n\tamdgpu_device_wb_free(adev, index);\n\treturn r;\n}\n\n\n \nstatic void sdma_v4_0_vm_copy_pte(struct amdgpu_ib *ib,\n\t\t\t\t  uint64_t pe, uint64_t src,\n\t\t\t\t  unsigned count)\n{\n\tunsigned bytes = count * 8;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR);\n\tib->ptr[ib->length_dw++] = bytes - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\n}\n\n \nstatic void sdma_v4_0_vm_write_pte(struct amdgpu_ib *ib, uint64_t pe,\n\t\t\t\t   uint64_t value, unsigned count,\n\t\t\t\t   uint32_t incr)\n{\n\tunsigned ndw = count * 2;\n\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_WRITE) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_WRITE_LINEAR);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);\n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = ndw - 1;\n\tfor (; ndw > 0; ndw -= 2) {\n\t\tib->ptr[ib->length_dw++] = lower_32_bits(value);\n\t\tib->ptr[ib->length_dw++] = upper_32_bits(value);\n\t\tvalue += incr;\n\t}\n}\n\n \nstatic void sdma_v4_0_vm_set_pte_pde(struct amdgpu_ib *ib,\n\t\t\t\t     uint64_t pe,\n\t\t\t\t     uint64_t addr, unsigned count,\n\t\t\t\t     uint32_t incr, uint64_t flags)\n{\n\t \n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_PTEPDE);\n\tib->ptr[ib->length_dw++] = lower_32_bits(pe);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(pe);\n\tib->ptr[ib->length_dw++] = lower_32_bits(flags);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(flags);\n\tib->ptr[ib->length_dw++] = lower_32_bits(addr);  \n\tib->ptr[ib->length_dw++] = upper_32_bits(addr);\n\tib->ptr[ib->length_dw++] = incr;  \n\tib->ptr[ib->length_dw++] = 0;\n\tib->ptr[ib->length_dw++] = count - 1;  \n}\n\n \nstatic void sdma_v4_0_ring_pad_ib(struct amdgpu_ring *ring, struct amdgpu_ib *ib)\n{\n\tstruct amdgpu_sdma_instance *sdma = amdgpu_sdma_get_instance_from_ring(ring);\n\tu32 pad_count;\n\tint i;\n\n\tpad_count = (-ib->length_dw) & 7;\n\tfor (i = 0; i < pad_count; i++)\n\t\tif (sdma && sdma->burst_nop && (i == 0))\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP) |\n\t\t\t\tSDMA_PKT_NOP_HEADER_COUNT(pad_count - 1);\n\t\telse\n\t\t\tib->ptr[ib->length_dw++] =\n\t\t\t\tSDMA_PKT_HEADER_OP(SDMA_OP_NOP);\n}\n\n\n \nstatic void sdma_v4_0_ring_emit_pipeline_sync(struct amdgpu_ring *ring)\n{\n\tuint32_t seq = ring->fence_drv.sync_seq;\n\tuint64_t addr = ring->fence_drv.gpu_addr;\n\n\t \n\tsdma_v4_0_wait_reg_mem(ring, 1, 0,\n\t\t\t       addr & 0xfffffffc,\n\t\t\t       upper_32_bits(addr) & 0xffffffff,\n\t\t\t       seq, 0xffffffff, 4);\n}\n\n\n \nstatic void sdma_v4_0_ring_emit_vm_flush(struct amdgpu_ring *ring,\n\t\t\t\t\t unsigned vmid, uint64_t pd_addr)\n{\n\tamdgpu_gmc_emit_flush_gpu_tlb(ring, vmid, pd_addr);\n}\n\nstatic void sdma_v4_0_ring_emit_wreg(struct amdgpu_ring *ring,\n\t\t\t\t     uint32_t reg, uint32_t val)\n{\n\tamdgpu_ring_write(ring, SDMA_PKT_HEADER_OP(SDMA_OP_SRBM_WRITE) |\n\t\t\t  SDMA_PKT_SRBM_WRITE_HEADER_BYTE_EN(0xf));\n\tamdgpu_ring_write(ring, reg);\n\tamdgpu_ring_write(ring, val);\n}\n\nstatic void sdma_v4_0_ring_emit_reg_wait(struct amdgpu_ring *ring, uint32_t reg,\n\t\t\t\t\t uint32_t val, uint32_t mask)\n{\n\tsdma_v4_0_wait_reg_mem(ring, 0, 0, reg, 0, val, mask, 10);\n}\n\nstatic bool sdma_v4_0_fw_support_paging_queue(struct amdgpu_device *adev)\n{\n\tuint fw_version = adev->sdma.instance[0].fw_version;\n\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 0, 0):\n\t\treturn fw_version >= 430;\n\tcase IP_VERSION(4, 0, 1):\n\t\t \n\t\treturn false;\n\tcase IP_VERSION(4, 2, 0):\n\t\treturn fw_version >= 123;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int sdma_v4_0_early_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint r;\n\n\tr = sdma_v4_0_init_microcode(adev);\n\tif (r) {\n\t\tDRM_ERROR(\"Failed to load sdma firmware!\\n\");\n\t\treturn r;\n\t}\n\n\t \n\tif ((adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 0, 0)) &&\n\t    amdgpu_sriov_vf((adev)))\n\t\tadev->sdma.has_page_queue = false;\n\telse if (sdma_v4_0_fw_support_paging_queue(adev))\n\t\tadev->sdma.has_page_queue = true;\n\n\tsdma_v4_0_set_ring_funcs(adev);\n\tsdma_v4_0_set_buffer_funcs(adev);\n\tsdma_v4_0_set_vm_pte_funcs(adev);\n\tsdma_v4_0_set_irq_funcs(adev);\n\tsdma_v4_0_set_ras_funcs(adev);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_ras_data_cb(struct amdgpu_device *adev,\n\t\tvoid *err_data,\n\t\tstruct amdgpu_iv_entry *entry);\n\nstatic int sdma_v4_0_late_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tsdma_v4_0_setup_ulv(adev);\n\n\tif (!amdgpu_persistent_edc_harvesting_supported(adev)) {\n\t\tif (adev->sdma.ras && adev->sdma.ras->ras_block.hw_ops &&\n\t\t    adev->sdma.ras->ras_block.hw_ops->reset_ras_error_count)\n\t\t\tadev->sdma.ras->ras_block.hw_ops->reset_ras_error_count(adev);\n\t}\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_sw_init(void *handle)\n{\n\tstruct amdgpu_ring *ring;\n\tint r, i;\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_TRAP,\n\t\t\t\t      &adev->sdma.trap_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_SRAM_ECC,\n\t\t\t\t      &adev->sdma.ecc_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_VM_HOLE,\n\t\t\t\t      &adev->sdma.vm_hole_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_DOORBELL_INVALID,\n\t\t\t\t      &adev->sdma.doorbell_invalid_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_POLL_TIMEOUT,\n\t\t\t\t      &adev->sdma.pool_timeout_irq);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = amdgpu_irq_add_id(adev, sdma_v4_0_seq_to_irq_id(i),\n\t\t\t\t      SDMA0_4_0__SRCID__SDMA_SRBMWRITE,\n\t\t\t\t      &adev->sdma.srbm_write_irq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tring = &adev->sdma.instance[i].ring;\n\t\tring->ring_obj = NULL;\n\t\tring->use_doorbell = true;\n\n\t\tDRM_DEBUG(\"SDMA %d use_doorbell being set to: [%s]\\n\", i,\n\t\t\t\tring->use_doorbell?\"true\":\"false\");\n\n\t\t \n\t\tring->doorbell_index = adev->doorbell_index.sdma_engine[i] << 1;\n\n\t\t \n\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 2) && i >= 5)\n\t\t\tring->vm_hub = AMDGPU_MMHUB1(0);\n\t\telse\n\t\t\tring->vm_hub = AMDGPU_MMHUB0(0);\n\n\t\tsprintf(ring->name, \"sdma%d\", i);\n\t\tr = amdgpu_ring_init(adev, ring, 1024, &adev->sdma.trap_irq,\n\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE0 + i,\n\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tring = &adev->sdma.instance[i].page;\n\t\t\tring->ring_obj = NULL;\n\t\t\tring->use_doorbell = true;\n\n\t\t\t \n\t\t\tif (adev->ip_versions[SDMA0_HWIP][0] >= IP_VERSION(4, 0, 0) &&\n\t\t\t    adev->ip_versions[SDMA0_HWIP][0] < IP_VERSION(4, 2, 0)) {\n\t\t\t\tring->doorbell_index =\n\t\t\t\t\tadev->doorbell_index.sdma_engine[i] << 1;\n\t\t\t\tring->doorbell_index += 0x400;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tring->doorbell_index =\n\t\t\t\t\t(adev->doorbell_index.sdma_engine[i] + 1) << 1;\n\t\t\t}\n\n\t\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 2) && i >= 5)\n\t\t\t\tring->vm_hub = AMDGPU_MMHUB1(0);\n\t\t\telse\n\t\t\t\tring->vm_hub = AMDGPU_MMHUB0(0);\n\n\t\t\tsprintf(ring->name, \"page%d\", i);\n\t\t\tr = amdgpu_ring_init(adev, ring, 1024,\n\t\t\t\t\t     &adev->sdma.trap_irq,\n\t\t\t\t\t     AMDGPU_SDMA_IRQ_INSTANCE0 + i,\n\t\t\t\t\t     AMDGPU_RING_PRIO_DEFAULT, NULL);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t}\n\t}\n\n\tif (amdgpu_sdma_ras_sw_init(adev)) {\n\t\tdev_err(adev->dev, \"Failed to initialize sdma ras block!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn r;\n}\n\nstatic int sdma_v4_0_sw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tamdgpu_ring_fini(&adev->sdma.instance[i].ring);\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tamdgpu_ring_fini(&adev->sdma.instance[i].page);\n\t}\n\n\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 2) ||\n            adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 4, 0))\n\t\tamdgpu_sdma_destroy_inst_ctx(adev, true);\n\telse\n\t\tamdgpu_sdma_destroy_inst_ctx(adev, false);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_hw_init(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (adev->flags & AMD_IS_APU)\n\t\tamdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_SDMA, false);\n\n\tif (!amdgpu_sriov_vf(adev))\n\t\tsdma_v4_0_init_golden_registers(adev);\n\n\treturn sdma_v4_0_start(adev);\n}\n\nstatic int sdma_v4_0_hw_fini(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint i;\n\n\tif (amdgpu_sriov_vf(adev)) {\n\t\t \n\t\tamdgpu_sdma_unset_buffer_funcs_helper(adev);\n\t\treturn 0;\n\t}\n\n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tamdgpu_irq_put(adev, &adev->sdma.ecc_irq,\n\t\t\t\t       AMDGPU_SDMA_IRQ_INSTANCE0 + i);\n\t\t}\n\t}\n\n\tsdma_v4_0_ctx_switch_enable(adev, false);\n\tsdma_v4_0_enable(adev, false);\n\n\tif (adev->flags & AMD_IS_APU)\n\t\tamdgpu_dpm_set_powergating_by_smu(adev, AMD_IP_BLOCK_TYPE_SDMA, true);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_suspend(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tif (adev->in_s0ix) {\n\t\tsdma_v4_0_gfx_enable(adev, false);\n\t\treturn 0;\n\t}\n\n\treturn sdma_v4_0_hw_fini(adev);\n}\n\nstatic int sdma_v4_0_resume(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\t \n\tif (adev->in_s0ix) {\n\t\tsdma_v4_0_enable(adev, true);\n\t\tsdma_v4_0_gfx_enable(adev, true);\n\t\tamdgpu_ttm_set_buffer_funcs_status(adev, true);\n\t\treturn 0;\n\t}\n\n\treturn sdma_v4_0_hw_init(adev);\n}\n\nstatic bool sdma_v4_0_is_idle(void *handle)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tu32 i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tu32 tmp = RREG32_SDMA(i, mmSDMA0_STATUS_REG);\n\n\t\tif (!(tmp & SDMA0_STATUS_REG__IDLE_MASK))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int sdma_v4_0_wait_for_idle(void *handle)\n{\n\tunsigned i, j;\n\tu32 sdma[AMDGPU_MAX_SDMA_INSTANCES];\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tfor (i = 0; i < adev->usec_timeout; i++) {\n\t\tfor (j = 0; j < adev->sdma.num_instances; j++) {\n\t\t\tsdma[j] = RREG32_SDMA(j, mmSDMA0_STATUS_REG);\n\t\t\tif (!(sdma[j] & SDMA0_STATUS_REG__IDLE_MASK))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j == adev->sdma.num_instances)\n\t\t\treturn 0;\n\t\tudelay(1);\n\t}\n\treturn -ETIMEDOUT;\n}\n\nstatic int sdma_v4_0_soft_reset(void *handle)\n{\n\t \n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_set_trap_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_cntl;\n\n\tsdma_cntl = RREG32_SDMA(type, mmSDMA0_CNTL);\n\tsdma_cntl = REG_SET_FIELD(sdma_cntl, SDMA0_CNTL, TRAP_ENABLE,\n\t\t       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\tWREG32_SDMA(type, mmSDMA0_CNTL, sdma_cntl);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_trap_irq(struct amdgpu_device *adev,\n\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tuint32_t instance;\n\n\tDRM_DEBUG(\"IH: SDMA trap\\n\");\n\tinstance = sdma_v4_0_irq_id_to_seq(entry->client_id);\n\tswitch (entry->ring_id) {\n\tcase 0:\n\t\tamdgpu_fence_process(&adev->sdma.instance[instance].ring);\n\t\tbreak;\n\tcase 1:\n\t\tif (adev->ip_versions[SDMA0_HWIP][0] == IP_VERSION(4, 2, 0))\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[instance].page);\n\t\tbreak;\n\tcase 2:\n\t\t \n\t\tbreak;\n\tcase 3:\n\t\tif (adev->ip_versions[SDMA0_HWIP][0] != IP_VERSION(4, 2, 0))\n\t\t\tamdgpu_fence_process(&adev->sdma.instance[instance].page);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_ras_data_cb(struct amdgpu_device *adev,\n\t\tvoid *err_data,\n\t\tstruct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\n\t \n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__GFX))\n\t\tgoto out;\n\n\tinstance = sdma_v4_0_irq_id_to_seq(entry->client_id);\n\tif (instance < 0)\n\t\tgoto out;\n\n\tamdgpu_sdma_process_ras_data_cb(adev, err_data, entry);\n\nout:\n\treturn AMDGPU_RAS_SUCCESS;\n}\n\nstatic int sdma_v4_0_process_illegal_inst_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\n\tDRM_ERROR(\"Illegal instruction in SDMA command stream\\n\");\n\n\tinstance = sdma_v4_0_irq_id_to_seq(entry->client_id);\n\tif (instance < 0)\n\t\treturn 0;\n\n\tswitch (entry->ring_id) {\n\tcase 0:\n\t\tdrm_sched_fault(&adev->sdma.instance[instance].ring.sched);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int sdma_v4_0_set_ecc_irq_state(struct amdgpu_device *adev,\n\t\t\t\t\tstruct amdgpu_irq_src *source,\n\t\t\t\t\tunsigned type,\n\t\t\t\t\tenum amdgpu_interrupt_state state)\n{\n\tu32 sdma_edc_config;\n\n\tsdma_edc_config = RREG32_SDMA(type, mmSDMA0_EDC_CONFIG);\n\tsdma_edc_config = REG_SET_FIELD(sdma_edc_config, SDMA0_EDC_CONFIG, ECC_INT_ENABLE,\n\t\t       state == AMDGPU_IRQ_STATE_ENABLE ? 1 : 0);\n\tWREG32_SDMA(type, mmSDMA0_EDC_CONFIG, sdma_edc_config);\n\n\treturn 0;\n}\n\nstatic int sdma_v4_0_print_iv_entry(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tint instance;\n\tstruct amdgpu_task_info task_info;\n\tu64 addr;\n\n\tinstance = sdma_v4_0_irq_id_to_seq(entry->client_id);\n\tif (instance < 0 || instance >= adev->sdma.num_instances) {\n\t\tdev_err(adev->dev, \"sdma instance invalid %d\\n\", instance);\n\t\treturn -EINVAL;\n\t}\n\n\taddr = (u64)entry->src_data[0] << 12;\n\taddr |= ((u64)entry->src_data[1] & 0xf) << 44;\n\n\tmemset(&task_info, 0, sizeof(struct amdgpu_task_info));\n\tamdgpu_vm_get_task_info(adev, entry->pasid, &task_info);\n\n\tdev_dbg_ratelimited(adev->dev,\n\t\t   \"[sdma%d] address:0x%016llx src_id:%u ring:%u vmid:%u \"\n\t\t   \"pasid:%u, for process %s pid %d thread %s pid %d\\n\",\n\t\t   instance, addr, entry->src_id, entry->ring_id, entry->vmid,\n\t\t   entry->pasid, task_info.process_name, task_info.tgid,\n\t\t   task_info.task_name, task_info.pid);\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_vm_hole_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev, \"MC or SEM address in VM hole\\n\");\n\tsdma_v4_0_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_doorbell_invalid_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev, \"SDMA received a doorbell from BIF with byte_enable !=0xff\\n\");\n\tsdma_v4_0_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_pool_timeout_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev,\n\t\t\"Polling register/memory timeout executing POLL_REG/MEM with finite timer\\n\");\n\tsdma_v4_0_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic int sdma_v4_0_process_srbm_write_irq(struct amdgpu_device *adev,\n\t\t\t\t\t      struct amdgpu_irq_src *source,\n\t\t\t\t\t      struct amdgpu_iv_entry *entry)\n{\n\tdev_dbg_ratelimited(adev->dev,\n\t\t\"SDMA gets an Register Write SRBM_WRITE command in non-privilege command buffer\\n\");\n\tsdma_v4_0_print_iv_entry(adev, entry);\n\treturn 0;\n}\n\nstatic void sdma_v4_0_update_medium_grain_clock_gating(\n\t\tstruct amdgpu_device *adev,\n\t\tbool enable)\n{\n\tuint32_t data, def;\n\tint i;\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_SDMA_MGCG)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tdef = data = RREG32_SDMA(i, mmSDMA0_CLK_CTRL);\n\t\t\tdata &= ~(SDMA0_CLK_CTRL__SOFT_OVERRIDE7_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE6_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE5_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE4_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE3_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE2_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE1_MASK |\n\t\t\t\t  SDMA0_CLK_CTRL__SOFT_OVERRIDE0_MASK);\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, mmSDMA0_CLK_CTRL, data);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\tdef = data = RREG32_SDMA(i, mmSDMA0_CLK_CTRL);\n\t\t\tdata |= (SDMA0_CLK_CTRL__SOFT_OVERRIDE7_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE6_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE5_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE4_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE3_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE2_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE1_MASK |\n\t\t\t\t SDMA0_CLK_CTRL__SOFT_OVERRIDE0_MASK);\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(i, mmSDMA0_CLK_CTRL, data);\n\t\t}\n\t}\n}\n\n\nstatic void sdma_v4_0_update_medium_grain_light_sleep(\n\t\tstruct amdgpu_device *adev,\n\t\tbool enable)\n{\n\tuint32_t data, def;\n\tint i;\n\n\tif (enable && (adev->cg_flags & AMD_CG_SUPPORT_SDMA_LS)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t\t \n\t\t\tdef = data = RREG32_SDMA(0, mmSDMA0_POWER_CNTL);\n\t\t\tdata |= SDMA0_POWER_CNTL__MEM_POWER_OVERRIDE_MASK;\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(0, mmSDMA0_POWER_CNTL, data);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\t \n\t\t\tdef = data = RREG32_SDMA(0, mmSDMA0_POWER_CNTL);\n\t\t\tdata &= ~SDMA0_POWER_CNTL__MEM_POWER_OVERRIDE_MASK;\n\t\t\tif (def != data)\n\t\t\t\tWREG32_SDMA(0, mmSDMA0_POWER_CNTL, data);\n\t\t}\n\t}\n}\n\nstatic int sdma_v4_0_set_clockgating_state(void *handle,\n\t\t\t\t\t  enum amd_clockgating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\treturn 0;\n\n\tsdma_v4_0_update_medium_grain_clock_gating(adev,\n\t\t\tstate == AMD_CG_STATE_GATE);\n\tsdma_v4_0_update_medium_grain_light_sleep(adev,\n\t\t\tstate == AMD_CG_STATE_GATE);\n\treturn 0;\n}\n\nstatic int sdma_v4_0_set_powergating_state(void *handle,\n\t\t\t\t\t  enum amd_powergating_state state)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 1, 0):\n\tcase IP_VERSION(4, 1, 1):\n\tcase IP_VERSION(4, 1, 2):\n\t\tsdma_v4_1_update_power_gating(adev,\n\t\t\t\tstate == AMD_PG_STATE_GATE);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void sdma_v4_0_get_clockgating_state(void *handle, u64 *flags)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)handle;\n\tint data;\n\n\tif (amdgpu_sriov_vf(adev))\n\t\t*flags = 0;\n\n\t \n\tdata = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_CLK_CTRL));\n\tif (!(data & SDMA0_CLK_CTRL__SOFT_OVERRIDE7_MASK))\n\t\t*flags |= AMD_CG_SUPPORT_SDMA_MGCG;\n\n\t \n\tdata = RREG32(SOC15_REG_OFFSET(SDMA0, 0, mmSDMA0_POWER_CNTL));\n\tif (data & SDMA0_POWER_CNTL__MEM_POWER_OVERRIDE_MASK)\n\t\t*flags |= AMD_CG_SUPPORT_SDMA_LS;\n}\n\nconst struct amd_ip_funcs sdma_v4_0_ip_funcs = {\n\t.name = \"sdma_v4_0\",\n\t.early_init = sdma_v4_0_early_init,\n\t.late_init = sdma_v4_0_late_init,\n\t.sw_init = sdma_v4_0_sw_init,\n\t.sw_fini = sdma_v4_0_sw_fini,\n\t.hw_init = sdma_v4_0_hw_init,\n\t.hw_fini = sdma_v4_0_hw_fini,\n\t.suspend = sdma_v4_0_suspend,\n\t.resume = sdma_v4_0_resume,\n\t.is_idle = sdma_v4_0_is_idle,\n\t.wait_for_idle = sdma_v4_0_wait_for_idle,\n\t.soft_reset = sdma_v4_0_soft_reset,\n\t.set_clockgating_state = sdma_v4_0_set_clockgating_state,\n\t.set_powergating_state = sdma_v4_0_set_powergating_state,\n\t.get_clockgating_state = sdma_v4_0_get_clockgating_state,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v4_0_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xff,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = true,\n\t.secure_submission_supported = true,\n\t.get_rptr = sdma_v4_0_ring_get_rptr,\n\t.get_wptr = sdma_v4_0_ring_get_wptr,\n\t.set_wptr = sdma_v4_0_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\t \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 6 +\n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 7 + 6,  \n\t.emit_ib = sdma_v4_0_ring_emit_ib,\n\t.emit_fence = sdma_v4_0_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v4_0_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v4_0_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v4_0_ring_emit_hdp_flush,\n\t.test_ring = sdma_v4_0_ring_test_ring,\n\t.test_ib = sdma_v4_0_ring_test_ib,\n\t.insert_nop = sdma_v4_0_ring_insert_nop,\n\t.pad_ib = sdma_v4_0_ring_pad_ib,\n\t.emit_wreg = sdma_v4_0_ring_emit_wreg,\n\t.emit_reg_wait = sdma_v4_0_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic const struct amdgpu_ring_funcs sdma_v4_0_page_ring_funcs = {\n\t.type = AMDGPU_RING_TYPE_SDMA,\n\t.align_mask = 0xff,\n\t.nop = SDMA_PKT_NOP_HEADER_OP(SDMA_OP_NOP),\n\t.support_64bit_ptrs = true,\n\t.secure_submission_supported = true,\n\t.get_rptr = sdma_v4_0_ring_get_rptr,\n\t.get_wptr = sdma_v4_0_page_ring_get_wptr,\n\t.set_wptr = sdma_v4_0_page_ring_set_wptr,\n\t.emit_frame_size =\n\t\t6 +  \n\t\t3 +  \n\t\t6 +  \n\t\t \n\t\tSOC15_FLUSH_GPU_TLB_NUM_WREG * 3 +\n\t\tSOC15_FLUSH_GPU_TLB_NUM_REG_WAIT * 6 +\n\t\t10 + 10 + 10,  \n\t.emit_ib_size = 7 + 6,  \n\t.emit_ib = sdma_v4_0_ring_emit_ib,\n\t.emit_fence = sdma_v4_0_ring_emit_fence,\n\t.emit_pipeline_sync = sdma_v4_0_ring_emit_pipeline_sync,\n\t.emit_vm_flush = sdma_v4_0_ring_emit_vm_flush,\n\t.emit_hdp_flush = sdma_v4_0_ring_emit_hdp_flush,\n\t.test_ring = sdma_v4_0_ring_test_ring,\n\t.test_ib = sdma_v4_0_ring_test_ib,\n\t.insert_nop = sdma_v4_0_ring_insert_nop,\n\t.pad_ib = sdma_v4_0_ring_pad_ib,\n\t.emit_wreg = sdma_v4_0_ring_emit_wreg,\n\t.emit_reg_wait = sdma_v4_0_ring_emit_reg_wait,\n\t.emit_reg_write_reg_wait = amdgpu_ring_emit_reg_write_reg_wait_helper,\n};\n\nstatic void sdma_v4_0_set_ring_funcs(struct amdgpu_device *adev)\n{\n\tint i;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tadev->sdma.instance[i].ring.funcs = &sdma_v4_0_ring_funcs;\n\t\tadev->sdma.instance[i].ring.me = i;\n\t\tif (adev->sdma.has_page_queue) {\n\t\t\tadev->sdma.instance[i].page.funcs =\n\t\t\t\t\t&sdma_v4_0_page_ring_funcs;\n\t\t\tadev->sdma.instance[i].page.me = i;\n\t\t}\n\t}\n}\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_trap_irq_funcs = {\n\t.set = sdma_v4_0_set_trap_irq_state,\n\t.process = sdma_v4_0_process_trap_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_illegal_inst_irq_funcs = {\n\t.process = sdma_v4_0_process_illegal_inst_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_ecc_irq_funcs = {\n\t.set = sdma_v4_0_set_ecc_irq_state,\n\t.process = amdgpu_sdma_process_ecc_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_vm_hole_irq_funcs = {\n\t.process = sdma_v4_0_process_vm_hole_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_doorbell_invalid_irq_funcs = {\n\t.process = sdma_v4_0_process_doorbell_invalid_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_pool_timeout_irq_funcs = {\n\t.process = sdma_v4_0_process_pool_timeout_irq,\n};\n\nstatic const struct amdgpu_irq_src_funcs sdma_v4_0_srbm_write_irq_funcs = {\n\t.process = sdma_v4_0_process_srbm_write_irq,\n};\n\nstatic void sdma_v4_0_set_irq_funcs(struct amdgpu_device *adev)\n{\n\tadev->sdma.trap_irq.num_types = adev->sdma.num_instances;\n\tadev->sdma.ecc_irq.num_types = adev->sdma.num_instances;\n\t \n\tswitch (adev->sdma.num_instances) {\n\tcase 5:\n\tcase 8:\n\t\tadev->sdma.vm_hole_irq.num_types = adev->sdma.num_instances;\n\t\tadev->sdma.doorbell_invalid_irq.num_types = adev->sdma.num_instances;\n\t\tadev->sdma.pool_timeout_irq.num_types = adev->sdma.num_instances;\n\t\tadev->sdma.srbm_write_irq.num_types = adev->sdma.num_instances;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\tadev->sdma.trap_irq.funcs = &sdma_v4_0_trap_irq_funcs;\n\tadev->sdma.illegal_inst_irq.funcs = &sdma_v4_0_illegal_inst_irq_funcs;\n\tadev->sdma.ecc_irq.funcs = &sdma_v4_0_ecc_irq_funcs;\n\tadev->sdma.vm_hole_irq.funcs = &sdma_v4_0_vm_hole_irq_funcs;\n\tadev->sdma.doorbell_invalid_irq.funcs = &sdma_v4_0_doorbell_invalid_irq_funcs;\n\tadev->sdma.pool_timeout_irq.funcs = &sdma_v4_0_pool_timeout_irq_funcs;\n\tadev->sdma.srbm_write_irq.funcs = &sdma_v4_0_srbm_write_irq_funcs;\n}\n\n \nstatic void sdma_v4_0_emit_copy_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint64_t src_offset,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count,\n\t\t\t\t       bool tmz)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_COPY) |\n\t\tSDMA_PKT_HEADER_SUB_OP(SDMA_SUBOP_COPY_LINEAR) |\n\t\tSDMA_PKT_COPY_LINEAR_HEADER_TMZ(tmz ? 1 : 0);\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n\tib->ptr[ib->length_dw++] = 0;  \n\tib->ptr[ib->length_dw++] = lower_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(src_offset);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n}\n\n \nstatic void sdma_v4_0_emit_fill_buffer(struct amdgpu_ib *ib,\n\t\t\t\t       uint32_t src_data,\n\t\t\t\t       uint64_t dst_offset,\n\t\t\t\t       uint32_t byte_count)\n{\n\tib->ptr[ib->length_dw++] = SDMA_PKT_HEADER_OP(SDMA_OP_CONST_FILL);\n\tib->ptr[ib->length_dw++] = lower_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = upper_32_bits(dst_offset);\n\tib->ptr[ib->length_dw++] = src_data;\n\tib->ptr[ib->length_dw++] = byte_count - 1;\n}\n\nstatic const struct amdgpu_buffer_funcs sdma_v4_0_buffer_funcs = {\n\t.copy_max_bytes = 0x400000,\n\t.copy_num_dw = 7,\n\t.emit_copy_buffer = sdma_v4_0_emit_copy_buffer,\n\n\t.fill_max_bytes = 0x400000,\n\t.fill_num_dw = 5,\n\t.emit_fill_buffer = sdma_v4_0_emit_fill_buffer,\n};\n\nstatic void sdma_v4_0_set_buffer_funcs(struct amdgpu_device *adev)\n{\n\tadev->mman.buffer_funcs = &sdma_v4_0_buffer_funcs;\n\tif (adev->sdma.has_page_queue)\n\t\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].page;\n\telse\n\t\tadev->mman.buffer_funcs_ring = &adev->sdma.instance[0].ring;\n}\n\nstatic const struct amdgpu_vm_pte_funcs sdma_v4_0_vm_pte_funcs = {\n\t.copy_pte_num_dw = 7,\n\t.copy_pte = sdma_v4_0_vm_copy_pte,\n\n\t.write_pte = sdma_v4_0_vm_write_pte,\n\t.set_pte_pde = sdma_v4_0_vm_set_pte_pde,\n};\n\nstatic void sdma_v4_0_set_vm_pte_funcs(struct amdgpu_device *adev)\n{\n\tstruct drm_gpu_scheduler *sched;\n\tunsigned i;\n\n\tadev->vm_manager.vm_pte_funcs = &sdma_v4_0_vm_pte_funcs;\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (adev->sdma.has_page_queue)\n\t\t\tsched = &adev->sdma.instance[i].page.sched;\n\t\telse\n\t\t\tsched = &adev->sdma.instance[i].ring.sched;\n\t\tadev->vm_manager.vm_pte_scheds[i] = sched;\n\t}\n\tadev->vm_manager.vm_pte_num_scheds = adev->sdma.num_instances;\n}\n\nstatic void sdma_v4_0_get_ras_error_count(uint32_t value,\n\t\t\t\t\tuint32_t instance,\n\t\t\t\t\tuint32_t *sec_count)\n{\n\tuint32_t i;\n\tuint32_t sec_cnt;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(sdma_v4_0_ras_fields); i++) {\n\t\t \n\t\tsec_cnt = (value &\n\t\t\tsdma_v4_0_ras_fields[i].sec_count_mask) >>\n\t\t\tsdma_v4_0_ras_fields[i].sec_count_shift;\n\t\tif (sec_cnt) {\n\t\t\tDRM_INFO(\"Detected %s in SDMA%d, SED %d\\n\",\n\t\t\t\tsdma_v4_0_ras_fields[i].name,\n\t\t\t\tinstance, sec_cnt);\n\t\t\t*sec_count += sec_cnt;\n\t\t}\n\t}\n}\n\nstatic int sdma_v4_0_query_ras_error_count_by_instance(struct amdgpu_device *adev,\n\t\t\tuint32_t instance, void *ras_error_status)\n{\n\tstruct ras_err_data *err_data = (struct ras_err_data *)ras_error_status;\n\tuint32_t sec_count = 0;\n\tuint32_t reg_value = 0;\n\n\treg_value = RREG32_SDMA(instance, mmSDMA0_EDC_COUNTER);\n\t \n\tif (reg_value)\n\t\tsdma_v4_0_get_ras_error_count(reg_value,\n\t\t\t\tinstance, &sec_count);\n\t \n\terr_data->ce_count += sec_count;\n\t \n\terr_data->ue_count = 0;\n\n\treturn 0;\n};\n\nstatic void sdma_v4_0_query_ras_error_count(struct amdgpu_device *adev,  void *ras_error_status)\n{\n\tint i = 0;\n\n\tfor (i = 0; i < adev->sdma.num_instances; i++) {\n\t\tif (sdma_v4_0_query_ras_error_count_by_instance(adev, i, ras_error_status)) {\n\t\t\tdev_err(adev->dev, \"Query ras error count failed in SDMA%d\\n\", i);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void sdma_v4_0_reset_ras_error_count(struct amdgpu_device *adev)\n{\n\tint i;\n\n\t \n\tif (amdgpu_ras_is_supported(adev, AMDGPU_RAS_BLOCK__SDMA)) {\n\t\tfor (i = 0; i < adev->sdma.num_instances; i++)\n\t\t\tRREG32_SDMA(i, mmSDMA0_EDC_COUNTER);\n\t}\n}\n\nconst struct amdgpu_ras_block_hw_ops sdma_v4_0_ras_hw_ops = {\n\t.query_ras_error_count = sdma_v4_0_query_ras_error_count,\n\t.reset_ras_error_count = sdma_v4_0_reset_ras_error_count,\n};\n\nstatic struct amdgpu_sdma_ras sdma_v4_0_ras = {\n\t.ras_block = {\n\t\t.hw_ops = &sdma_v4_0_ras_hw_ops,\n\t\t.ras_cb = sdma_v4_0_process_ras_data_cb,\n\t},\n};\n\nstatic void sdma_v4_0_set_ras_funcs(struct amdgpu_device *adev)\n{\n\tswitch (adev->ip_versions[SDMA0_HWIP][0]) {\n\tcase IP_VERSION(4, 2, 0):\n\tcase IP_VERSION(4, 2, 2):\n\t\tadev->sdma.ras = &sdma_v4_0_ras;\n\t\tbreak;\n\tcase IP_VERSION(4, 4, 0):\n\t\tadev->sdma.ras = &sdma_v4_4_ras;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n}\n\nconst struct amdgpu_ip_block_version sdma_v4_0_ip_block = {\n\t.type = AMD_IP_BLOCK_TYPE_SDMA,\n\t.major = 4,\n\t.minor = 0,\n\t.rev = 0,\n\t.funcs = &sdma_v4_0_ip_funcs,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}