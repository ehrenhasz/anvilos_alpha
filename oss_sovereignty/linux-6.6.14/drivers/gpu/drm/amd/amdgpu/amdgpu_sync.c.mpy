{
  "module_name": "amdgpu_sync.c",
  "hash_id": "c028a22ea616a4bb9753abe3f102d561884643fd002c2455d5d5568bd67e72be",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c",
  "human_readable_source": "\n \n \n\n#include <linux/dma-fence-chain.h>\n\n#include \"amdgpu.h\"\n#include \"amdgpu_trace.h\"\n#include \"amdgpu_amdkfd.h\"\n\nstruct amdgpu_sync_entry {\n\tstruct hlist_node\tnode;\n\tstruct dma_fence\t*fence;\n};\n\nstatic struct kmem_cache *amdgpu_sync_slab;\n\n \nvoid amdgpu_sync_create(struct amdgpu_sync *sync)\n{\n\thash_init(sync->fences);\n}\n\n \nstatic bool amdgpu_sync_same_dev(struct amdgpu_device *adev,\n\t\t\t\t struct dma_fence *f)\n{\n\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(f);\n\n\tif (s_fence) {\n\t\tstruct amdgpu_ring *ring;\n\n\t\tring = container_of(s_fence->sched, struct amdgpu_ring, sched);\n\t\treturn ring->adev == adev;\n\t}\n\n\treturn false;\n}\n\n \nstatic void *amdgpu_sync_get_owner(struct dma_fence *f)\n{\n\tstruct drm_sched_fence *s_fence;\n\tstruct amdgpu_amdkfd_fence *kfd_fence;\n\n\tif (!f)\n\t\treturn AMDGPU_FENCE_OWNER_UNDEFINED;\n\n\ts_fence = to_drm_sched_fence(f);\n\tif (s_fence)\n\t\treturn s_fence->owner;\n\n\tkfd_fence = to_amdgpu_amdkfd_fence(f);\n\tif (kfd_fence)\n\t\treturn AMDGPU_FENCE_OWNER_KFD;\n\n\treturn AMDGPU_FENCE_OWNER_UNDEFINED;\n}\n\n \nstatic void amdgpu_sync_keep_later(struct dma_fence **keep,\n\t\t\t\t   struct dma_fence *fence)\n{\n\tif (*keep && dma_fence_is_later(*keep, fence))\n\t\treturn;\n\n\tdma_fence_put(*keep);\n\t*keep = dma_fence_get(fence);\n}\n\n \nstatic bool amdgpu_sync_add_later(struct amdgpu_sync *sync, struct dma_fence *f)\n{\n\tstruct amdgpu_sync_entry *e;\n\n\thash_for_each_possible(sync->fences, e, node, f->context) {\n\t\tif (unlikely(e->fence->context != f->context))\n\t\t\tcontinue;\n\n\t\tamdgpu_sync_keep_later(&e->fence, f);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nint amdgpu_sync_fence(struct amdgpu_sync *sync, struct dma_fence *f)\n{\n\tstruct amdgpu_sync_entry *e;\n\n\tif (!f)\n\t\treturn 0;\n\n\tif (amdgpu_sync_add_later(sync, f))\n\t\treturn 0;\n\n\te = kmem_cache_alloc(amdgpu_sync_slab, GFP_KERNEL);\n\tif (!e)\n\t\treturn -ENOMEM;\n\n\thash_add(sync->fences, &e->node, f->context);\n\te->fence = dma_fence_get(f);\n\treturn 0;\n}\n\n \nstatic bool amdgpu_sync_test_fence(struct amdgpu_device *adev,\n\t\t\t\t   enum amdgpu_sync_mode mode,\n\t\t\t\t   void *owner, struct dma_fence *f)\n{\n\tvoid *fence_owner = amdgpu_sync_get_owner(f);\n\n\t \n\tif (fence_owner == AMDGPU_FENCE_OWNER_UNDEFINED)\n\t\treturn true;\n\n\t \n\tif (fence_owner == AMDGPU_FENCE_OWNER_KFD &&\n\t    owner != AMDGPU_FENCE_OWNER_UNDEFINED)\n\t\treturn false;\n\n\t \n\tif (fence_owner == AMDGPU_FENCE_OWNER_VM &&\n\t    owner != AMDGPU_FENCE_OWNER_UNDEFINED)\n\t\treturn false;\n\n\t \n\tswitch (mode) {\n\tcase AMDGPU_SYNC_ALWAYS:\n\t\treturn true;\n\n\tcase AMDGPU_SYNC_NE_OWNER:\n\t\tif (amdgpu_sync_same_dev(adev, f) &&\n\t\t    fence_owner == owner)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase AMDGPU_SYNC_EQ_OWNER:\n\t\tif (amdgpu_sync_same_dev(adev, f) &&\n\t\t    fence_owner != owner)\n\t\t\treturn false;\n\t\tbreak;\n\n\tcase AMDGPU_SYNC_EXPLICIT:\n\t\treturn false;\n\t}\n\n\tWARN(debug_evictions && fence_owner == AMDGPU_FENCE_OWNER_KFD,\n\t     \"Adding eviction fence to sync obj\");\n\treturn true;\n}\n\n \nint amdgpu_sync_resv(struct amdgpu_device *adev, struct amdgpu_sync *sync,\n\t\t     struct dma_resv *resv, enum amdgpu_sync_mode mode,\n\t\t     void *owner)\n{\n\tstruct dma_resv_iter cursor;\n\tstruct dma_fence *f;\n\tint r;\n\n\tif (resv == NULL)\n\t\treturn -EINVAL;\n\n\t \n\tdma_resv_for_each_fence(&cursor, resv, DMA_RESV_USAGE_BOOKKEEP, f) {\n\t\tdma_fence_chain_for_each(f, f) {\n\t\t\tstruct dma_fence *tmp = dma_fence_chain_contained(f);\n\n\t\t\tif (amdgpu_sync_test_fence(adev, mode, owner, tmp)) {\n\t\t\t\tr = amdgpu_sync_fence(sync, f);\n\t\t\t\tdma_fence_put(f);\n\t\t\t\tif (r)\n\t\t\t\t\treturn r;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic void amdgpu_sync_entry_free(struct amdgpu_sync_entry *e)\n{\n\thash_del(&e->node);\n\tdma_fence_put(e->fence);\n\tkmem_cache_free(amdgpu_sync_slab, e);\n}\n\n \nstruct dma_fence *amdgpu_sync_peek_fence(struct amdgpu_sync *sync,\n\t\t\t\t\t struct amdgpu_ring *ring)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tint i;\n\n\thash_for_each_safe(sync->fences, i, tmp, e, node) {\n\t\tstruct dma_fence *f = e->fence;\n\t\tstruct drm_sched_fence *s_fence = to_drm_sched_fence(f);\n\n\t\tif (dma_fence_is_signaled(f)) {\n\t\t\tamdgpu_sync_entry_free(e);\n\t\t\tcontinue;\n\t\t}\n\t\tif (ring && s_fence) {\n\t\t\t \n\t\t\tif (s_fence->sched == &ring->sched) {\n\t\t\t\tif (dma_fence_is_signaled(&s_fence->scheduled))\n\t\t\t\t\tcontinue;\n\n\t\t\t\treturn &s_fence->scheduled;\n\t\t\t}\n\t\t}\n\n\t\treturn f;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct dma_fence *amdgpu_sync_get_fence(struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tstruct dma_fence *f;\n\tint i;\n\n\thash_for_each_safe(sync->fences, i, tmp, e, node) {\n\n\t\tf = e->fence;\n\n\t\thash_del(&e->node);\n\t\tkmem_cache_free(amdgpu_sync_slab, e);\n\n\t\tif (!dma_fence_is_signaled(f))\n\t\t\treturn f;\n\n\t\tdma_fence_put(f);\n\t}\n\treturn NULL;\n}\n\n \nint amdgpu_sync_clone(struct amdgpu_sync *source, struct amdgpu_sync *clone)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tstruct dma_fence *f;\n\tint i, r;\n\n\thash_for_each_safe(source->fences, i, tmp, e, node) {\n\t\tf = e->fence;\n\t\tif (!dma_fence_is_signaled(f)) {\n\t\t\tr = amdgpu_sync_fence(clone, f);\n\t\t\tif (r)\n\t\t\t\treturn r;\n\t\t} else {\n\t\t\tamdgpu_sync_entry_free(e);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint amdgpu_sync_push_to_job(struct amdgpu_sync *sync, struct amdgpu_job *job)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tstruct dma_fence *f;\n\tint i, r;\n\n\thash_for_each_safe(sync->fences, i, tmp, e, node) {\n\t\tf = e->fence;\n\t\tif (dma_fence_is_signaled(f)) {\n\t\t\tamdgpu_sync_entry_free(e);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdma_fence_get(f);\n\t\tr = drm_sched_job_add_dependency(&job->base, f);\n\t\tif (r) {\n\t\t\tdma_fence_put(f);\n\t\t\treturn r;\n\t\t}\n\t}\n\treturn 0;\n}\n\nint amdgpu_sync_wait(struct amdgpu_sync *sync, bool intr)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tint i, r;\n\n\thash_for_each_safe(sync->fences, i, tmp, e, node) {\n\t\tr = dma_fence_wait(e->fence, intr);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tamdgpu_sync_entry_free(e);\n\t}\n\n\treturn 0;\n}\n\n \nvoid amdgpu_sync_free(struct amdgpu_sync *sync)\n{\n\tstruct amdgpu_sync_entry *e;\n\tstruct hlist_node *tmp;\n\tunsigned int i;\n\n\thash_for_each_safe(sync->fences, i, tmp, e, node)\n\t\tamdgpu_sync_entry_free(e);\n}\n\n \nint amdgpu_sync_init(void)\n{\n\tamdgpu_sync_slab = kmem_cache_create(\n\t\t\"amdgpu_sync\", sizeof(struct amdgpu_sync_entry), 0,\n\t\tSLAB_HWCACHE_ALIGN, NULL);\n\tif (!amdgpu_sync_slab)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nvoid amdgpu_sync_fini(void)\n{\n\tkmem_cache_destroy(amdgpu_sync_slab);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}