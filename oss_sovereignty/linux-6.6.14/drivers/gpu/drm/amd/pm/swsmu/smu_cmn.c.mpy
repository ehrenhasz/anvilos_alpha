{
  "module_name": "smu_cmn.c",
  "hash_id": "3d48082104f6d024e6330d8c41e9117f60ee3f8a28e5efd5df6f63f16e8273df",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/pm/swsmu/smu_cmn.c",
  "human_readable_source": " \n\n#define SWSMU_CODE_LAYER_L4\n\n#include \"amdgpu.h\"\n#include \"amdgpu_smu.h\"\n#include \"smu_cmn.h\"\n#include \"soc15_common.h\"\n\n \n#undef pr_err\n#undef pr_warn\n#undef pr_info\n#undef pr_debug\n\n#define MP1_C2PMSG_90__CONTENT_MASK                                                                    0xFFFFFFFFL\n\nconst int link_speed[] = {25, 50, 80, 160, 320, 640};\n\n#undef __SMU_DUMMY_MAP\n#define __SMU_DUMMY_MAP(type)\t#type\nstatic const char * const __smu_message_names[] = {\n\tSMU_MESSAGE_TYPES\n};\n\n#define smu_cmn_call_asic_func(intf, smu, args...)                             \\\n\t((smu)->ppt_funcs ? ((smu)->ppt_funcs->intf ?                          \\\n\t\t\t\t     (smu)->ppt_funcs->intf(smu, ##args) :     \\\n\t\t\t\t     -ENOTSUPP) :                              \\\n\t\t\t    -EINVAL)\n\nstatic const char *smu_get_message_name(struct smu_context *smu,\n\t\t\t\t\tenum smu_message_type type)\n{\n\tif (type < 0 || type >= SMU_MSG_MAX_COUNT)\n\t\treturn \"unknown smu message\";\n\n\treturn __smu_message_names[type];\n}\n\nstatic void smu_cmn_read_arg(struct smu_context *smu,\n\t\t\t     uint32_t *arg)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\n\t*arg = RREG32(smu->param_reg);\n}\n\n \n#define SMU_RESP_NONE           0\n#define SMU_RESP_OK             1\n#define SMU_RESP_CMD_FAIL       0xFF\n#define SMU_RESP_CMD_UNKNOWN    0xFE\n#define SMU_RESP_CMD_BAD_PREREQ 0xFD\n#define SMU_RESP_BUSY_OTHER     0xFC\n#define SMU_RESP_DEBUG_END      0xFB\n\n \nstatic u32 __smu_cmn_poll_stat(struct smu_context *smu)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\tint timeout = adev->usec_timeout * 20;\n\tu32 reg;\n\n\tfor ( ; timeout > 0; timeout--) {\n\t\treg = RREG32(smu->resp_reg);\n\t\tif ((reg & MP1_C2PMSG_90__CONTENT_MASK) != 0)\n\t\t\tbreak;\n\n\t\tudelay(1);\n\t}\n\n\treturn reg;\n}\n\nstatic void __smu_cmn_reg_print_error(struct smu_context *smu,\n\t\t\t\t      u32 reg_c2pmsg_90,\n\t\t\t\t      int msg_index,\n\t\t\t\t      u32 param,\n\t\t\t\t      enum smu_message_type msg)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\tconst char *message = smu_get_message_name(smu, msg);\n\tu32 msg_idx, prm;\n\n\tswitch (reg_c2pmsg_90) {\n\tcase SMU_RESP_NONE: {\n\t\tmsg_idx = RREG32(smu->msg_reg);\n\t\tprm     = RREG32(smu->param_reg);\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: I'm not done with your previous command: SMN_C2PMSG_66:0x%08X SMN_C2PMSG_82:0x%08X\",\n\t\t\t\t    msg_idx, prm);\n\t\t}\n\t\tbreak;\n\tcase SMU_RESP_OK:\n\t\t \n\t\tbreak;\n\tcase SMU_RESP_CMD_FAIL:\n\t\t \n\t\tbreak;\n\tcase SMU_RESP_CMD_UNKNOWN:\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: unknown command: index:%d param:0x%08X message:%s\",\n\t\t\t\t    msg_index, param, message);\n\t\tbreak;\n\tcase SMU_RESP_CMD_BAD_PREREQ:\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: valid command, bad prerequisites: index:%d param:0x%08X message:%s\",\n\t\t\t\t    msg_index, param, message);\n\t\tbreak;\n\tcase SMU_RESP_BUSY_OTHER:\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: I'm very busy for your command: index:%d param:0x%08X message:%s\",\n\t\t\t\t    msg_index, param, message);\n\t\tbreak;\n\tcase SMU_RESP_DEBUG_END:\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: I'm debugging!\");\n\t\tbreak;\n\tdefault:\n\t\tdev_err_ratelimited(adev->dev,\n\t\t\t\t    \"SMU: response:0x%08X for index:%d param:0x%08X message:%s?\",\n\t\t\t\t    reg_c2pmsg_90, msg_index, param, message);\n\t\tbreak;\n\t}\n}\n\nstatic int __smu_cmn_reg2errno(struct smu_context *smu, u32 reg_c2pmsg_90)\n{\n\tint res;\n\n\tswitch (reg_c2pmsg_90) {\n\tcase SMU_RESP_NONE:\n\t\t \n\t\tres = -ETIME;\n\t\tbreak;\n\tcase SMU_RESP_OK:\n\t\tres = 0;\n\t\tbreak;\n\tcase SMU_RESP_CMD_FAIL:\n\t\t \n\t\tres = -EIO;\n\t\tbreak;\n\tcase SMU_RESP_CMD_UNKNOWN:\n\t\t \n\t\tres = -EOPNOTSUPP;\n\t\tbreak;\n\tcase SMU_RESP_CMD_BAD_PREREQ:\n\t\t \n\t\tres = -EINVAL;\n\t\tbreak;\n\tcase SMU_RESP_BUSY_OTHER:\n\t\t \n\t\tres = -EBUSY;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tres = -EREMOTEIO;\n\t\tbreak;\n\t}\n\n\treturn res;\n}\n\nstatic void __smu_cmn_send_msg(struct smu_context *smu,\n\t\t\t       u16 msg,\n\t\t\t       u32 param)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\n\tWREG32(smu->resp_reg, 0);\n\tWREG32(smu->param_reg, param);\n\tWREG32(smu->msg_reg, msg);\n}\n\nstatic int __smu_cmn_send_debug_msg(struct smu_context *smu,\n\t\t\t       u32 msg,\n\t\t\t       u32 param)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\n\tWREG32(smu->debug_param_reg, param);\n\tWREG32(smu->debug_msg_reg, msg);\n\tWREG32(smu->debug_resp_reg, 0);\n\n\treturn 0;\n}\n \nint smu_cmn_send_msg_without_waiting(struct smu_context *smu,\n\t\t\t\t     uint16_t msg_index,\n\t\t\t\t     uint32_t param)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\tu32 reg;\n\tint res;\n\n\tif (adev->no_hw_access)\n\t\treturn 0;\n\n\treg = __smu_cmn_poll_stat(smu);\n\tres = __smu_cmn_reg2errno(smu, reg);\n\tif (reg == SMU_RESP_NONE ||\n\t    res == -EREMOTEIO)\n\t\tgoto Out;\n\t__smu_cmn_send_msg(smu, msg_index, param);\n\tres = 0;\nOut:\n\tif (unlikely(adev->pm.smu_debug_mask & SMU_DEBUG_HALT_ON_ERROR) &&\n\t    res && (res != -ETIME)) {\n\t\tamdgpu_device_halt(adev);\n\t\tWARN_ON(1);\n\t}\n\n\treturn res;\n}\n\n \nint smu_cmn_wait_for_response(struct smu_context *smu)\n{\n\tu32 reg;\n\tint res;\n\n\treg = __smu_cmn_poll_stat(smu);\n\tres = __smu_cmn_reg2errno(smu, reg);\n\n\tif (unlikely(smu->adev->pm.smu_debug_mask & SMU_DEBUG_HALT_ON_ERROR) &&\n\t    res && (res != -ETIME)) {\n\t\tamdgpu_device_halt(smu->adev);\n\t\tWARN_ON(1);\n\t}\n\n\treturn res;\n}\n\n \nint smu_cmn_send_smc_msg_with_param(struct smu_context *smu,\n\t\t\t\t    enum smu_message_type msg,\n\t\t\t\t    uint32_t param,\n\t\t\t\t    uint32_t *read_arg)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\tint res, index;\n\tu32 reg;\n\n\tif (adev->no_hw_access)\n\t\treturn 0;\n\n\tindex = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t       CMN2ASIC_MAPPING_MSG,\n\t\t\t\t\t       msg);\n\tif (index < 0)\n\t\treturn index == -EACCES ? 0 : index;\n\n\tmutex_lock(&smu->message_lock);\n\treg = __smu_cmn_poll_stat(smu);\n\tres = __smu_cmn_reg2errno(smu, reg);\n\tif (reg == SMU_RESP_NONE ||\n\t    res == -EREMOTEIO) {\n\t\t__smu_cmn_reg_print_error(smu, reg, index, param, msg);\n\t\tgoto Out;\n\t}\n\t__smu_cmn_send_msg(smu, (uint16_t) index, param);\n\treg = __smu_cmn_poll_stat(smu);\n\tres = __smu_cmn_reg2errno(smu, reg);\n\tif (res != 0)\n\t\t__smu_cmn_reg_print_error(smu, reg, index, param, msg);\n\tif (read_arg)\n\t\tsmu_cmn_read_arg(smu, read_arg);\nOut:\n\tif (unlikely(adev->pm.smu_debug_mask & SMU_DEBUG_HALT_ON_ERROR) && res) {\n\t\tamdgpu_device_halt(adev);\n\t\tWARN_ON(1);\n\t}\n\n\tmutex_unlock(&smu->message_lock);\n\treturn res;\n}\n\nint smu_cmn_send_smc_msg(struct smu_context *smu,\n\t\t\t enum smu_message_type msg,\n\t\t\t uint32_t *read_arg)\n{\n\treturn smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t       msg,\n\t\t\t\t\t       0,\n\t\t\t\t\t       read_arg);\n}\n\nint smu_cmn_send_debug_smc_msg(struct smu_context *smu,\n\t\t\t uint32_t msg)\n{\n\treturn __smu_cmn_send_debug_msg(smu, msg, 0);\n}\n\nint smu_cmn_send_debug_smc_msg_with_param(struct smu_context *smu,\n\t\t\t uint32_t msg, uint32_t param)\n{\n\treturn __smu_cmn_send_debug_msg(smu, msg, param);\n}\n\nint smu_cmn_to_asic_specific_index(struct smu_context *smu,\n\t\t\t\t   enum smu_cmn2asic_mapping_type type,\n\t\t\t\t   uint32_t index)\n{\n\tstruct cmn2asic_msg_mapping msg_mapping;\n\tstruct cmn2asic_mapping mapping;\n\n\tswitch (type) {\n\tcase CMN2ASIC_MAPPING_MSG:\n\t\tif (index >= SMU_MSG_MAX_COUNT ||\n\t\t    !smu->message_map)\n\t\t\treturn -EINVAL;\n\n\t\tmsg_mapping = smu->message_map[index];\n\t\tif (!msg_mapping.valid_mapping)\n\t\t\treturn -EINVAL;\n\n\t\tif (amdgpu_sriov_vf(smu->adev) &&\n\t\t    !msg_mapping.valid_in_vf)\n\t\t\treturn -EACCES;\n\n\t\treturn msg_mapping.map_to;\n\n\tcase CMN2ASIC_MAPPING_CLK:\n\t\tif (index >= SMU_CLK_COUNT ||\n\t\t    !smu->clock_map)\n\t\t\treturn -EINVAL;\n\n\t\tmapping = smu->clock_map[index];\n\t\tif (!mapping.valid_mapping)\n\t\t\treturn -EINVAL;\n\n\t\treturn mapping.map_to;\n\n\tcase CMN2ASIC_MAPPING_FEATURE:\n\t\tif (index >= SMU_FEATURE_COUNT ||\n\t\t    !smu->feature_map)\n\t\t\treturn -EINVAL;\n\n\t\tmapping = smu->feature_map[index];\n\t\tif (!mapping.valid_mapping)\n\t\t\treturn -EINVAL;\n\n\t\treturn mapping.map_to;\n\n\tcase CMN2ASIC_MAPPING_TABLE:\n\t\tif (index >= SMU_TABLE_COUNT ||\n\t\t    !smu->table_map)\n\t\t\treturn -EINVAL;\n\n\t\tmapping = smu->table_map[index];\n\t\tif (!mapping.valid_mapping)\n\t\t\treturn -EINVAL;\n\n\t\treturn mapping.map_to;\n\n\tcase CMN2ASIC_MAPPING_PWR:\n\t\tif (index >= SMU_POWER_SOURCE_COUNT ||\n\t\t    !smu->pwr_src_map)\n\t\t\treturn -EINVAL;\n\n\t\tmapping = smu->pwr_src_map[index];\n\t\tif (!mapping.valid_mapping)\n\t\t\treturn -EINVAL;\n\n\t\treturn mapping.map_to;\n\n\tcase CMN2ASIC_MAPPING_WORKLOAD:\n\t\tif (index >= PP_SMC_POWER_PROFILE_COUNT ||\n\t\t    !smu->workload_map)\n\t\t\treturn -EINVAL;\n\n\t\tmapping = smu->workload_map[index];\n\t\tif (!mapping.valid_mapping)\n\t\t\treturn -ENOTSUPP;\n\n\t\treturn mapping.map_to;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nint smu_cmn_feature_is_supported(struct smu_context *smu,\n\t\t\t\t enum smu_feature_mask mask)\n{\n\tstruct smu_feature *feature = &smu->smu_feature;\n\tint feature_id;\n\n\tfeature_id = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t    CMN2ASIC_MAPPING_FEATURE,\n\t\t\t\t\t\t    mask);\n\tif (feature_id < 0)\n\t\treturn 0;\n\n\tWARN_ON(feature_id > feature->feature_num);\n\n\treturn test_bit(feature_id, feature->supported);\n}\n\nstatic int __smu_get_enabled_features(struct smu_context *smu,\n\t\t\t       uint64_t *enabled_features)\n{\n\treturn smu_cmn_call_asic_func(get_enabled_mask, smu, enabled_features);\n}\n\nint smu_cmn_feature_is_enabled(struct smu_context *smu,\n\t\t\t       enum smu_feature_mask mask)\n{\n\tstruct amdgpu_device *adev = smu->adev;\n\tuint64_t enabled_features;\n\tint feature_id;\n\n\tif (__smu_get_enabled_features(smu, &enabled_features)) {\n\t\tdev_err(adev->dev, \"Failed to retrieve enabled ppfeatures!\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (enabled_features == ULLONG_MAX)\n\t\treturn 1;\n\n\tfeature_id = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t    CMN2ASIC_MAPPING_FEATURE,\n\t\t\t\t\t\t    mask);\n\tif (feature_id < 0)\n\t\treturn 0;\n\n\treturn test_bit(feature_id, (unsigned long *)&enabled_features);\n}\n\nbool smu_cmn_clk_dpm_is_enabled(struct smu_context *smu,\n\t\t\t\tenum smu_clk_type clk_type)\n{\n\tenum smu_feature_mask feature_id = 0;\n\n\tswitch (clk_type) {\n\tcase SMU_MCLK:\n\tcase SMU_UCLK:\n\t\tfeature_id = SMU_FEATURE_DPM_UCLK_BIT;\n\t\tbreak;\n\tcase SMU_GFXCLK:\n\tcase SMU_SCLK:\n\t\tfeature_id = SMU_FEATURE_DPM_GFXCLK_BIT;\n\t\tbreak;\n\tcase SMU_SOCCLK:\n\t\tfeature_id = SMU_FEATURE_DPM_SOCCLK_BIT;\n\t\tbreak;\n\tcase SMU_VCLK:\n\tcase SMU_VCLK1:\n\t\tfeature_id = SMU_FEATURE_DPM_VCLK_BIT;\n\t\tbreak;\n\tcase SMU_DCLK:\n\tcase SMU_DCLK1:\n\t\tfeature_id = SMU_FEATURE_DPM_DCLK_BIT;\n\t\tbreak;\n\tcase SMU_FCLK:\n\t\tfeature_id = SMU_FEATURE_DPM_FCLK_BIT;\n\t\tbreak;\n\tdefault:\n\t\treturn true;\n\t}\n\n\tif (!smu_cmn_feature_is_enabled(smu, feature_id))\n\t\treturn false;\n\n\treturn true;\n}\n\nint smu_cmn_get_enabled_mask(struct smu_context *smu,\n\t\t\t     uint64_t *feature_mask)\n{\n\tuint32_t *feature_mask_high;\n\tuint32_t *feature_mask_low;\n\tint ret = 0, index = 0;\n\n\tif (!feature_mask)\n\t\treturn -EINVAL;\n\n\tfeature_mask_low = &((uint32_t *)feature_mask)[0];\n\tfeature_mask_high = &((uint32_t *)feature_mask)[1];\n\n\tindex = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\tCMN2ASIC_MAPPING_MSG,\n\t\t\t\t\t\tSMU_MSG_GetEnabledSmuFeatures);\n\tif (index > 0) {\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t      SMU_MSG_GetEnabledSmuFeatures,\n\t\t\t\t\t\t      0,\n\t\t\t\t\t\t      feature_mask_low);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t      SMU_MSG_GetEnabledSmuFeatures,\n\t\t\t\t\t\t      1,\n\t\t\t\t\t\t      feature_mask_high);\n\t} else {\n\t\tret = smu_cmn_send_smc_msg(smu,\n\t\t\t\t\t   SMU_MSG_GetEnabledSmuFeaturesHigh,\n\t\t\t\t\t   feature_mask_high);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = smu_cmn_send_smc_msg(smu,\n\t\t\t\t\t   SMU_MSG_GetEnabledSmuFeaturesLow,\n\t\t\t\t\t   feature_mask_low);\n\t}\n\n\treturn ret;\n}\n\nuint64_t smu_cmn_get_indep_throttler_status(\n\t\t\t\t\tconst unsigned long dep_status,\n\t\t\t\t\tconst uint8_t *throttler_map)\n{\n\tuint64_t indep_status = 0;\n\tuint8_t dep_bit = 0;\n\n\tfor_each_set_bit(dep_bit, &dep_status, 32)\n\t\tindep_status |= 1ULL << throttler_map[dep_bit];\n\n\treturn indep_status;\n}\n\nint smu_cmn_feature_update_enable_state(struct smu_context *smu,\n\t\t\t\t\tuint64_t feature_mask,\n\t\t\t\t\tbool enabled)\n{\n\tint ret = 0;\n\n\tif (enabled) {\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t  SMU_MSG_EnableSmuFeaturesLow,\n\t\t\t\t\t\t  lower_32_bits(feature_mask),\n\t\t\t\t\t\t  NULL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t  SMU_MSG_EnableSmuFeaturesHigh,\n\t\t\t\t\t\t  upper_32_bits(feature_mask),\n\t\t\t\t\t\t  NULL);\n\t} else {\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t  SMU_MSG_DisableSmuFeaturesLow,\n\t\t\t\t\t\t  lower_32_bits(feature_mask),\n\t\t\t\t\t\t  NULL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = smu_cmn_send_smc_msg_with_param(smu,\n\t\t\t\t\t\t  SMU_MSG_DisableSmuFeaturesHigh,\n\t\t\t\t\t\t  upper_32_bits(feature_mask),\n\t\t\t\t\t\t  NULL);\n\t}\n\n\treturn ret;\n}\n\nint smu_cmn_feature_set_enabled(struct smu_context *smu,\n\t\t\t\tenum smu_feature_mask mask,\n\t\t\t\tbool enable)\n{\n\tint feature_id;\n\n\tfeature_id = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t    CMN2ASIC_MAPPING_FEATURE,\n\t\t\t\t\t\t    mask);\n\tif (feature_id < 0)\n\t\treturn -EINVAL;\n\n\treturn smu_cmn_feature_update_enable_state(smu,\n\t\t\t\t\t       1ULL << feature_id,\n\t\t\t\t\t       enable);\n}\n\n#undef __SMU_DUMMY_MAP\n#define __SMU_DUMMY_MAP(fea)\t#fea\nstatic const char *__smu_feature_names[] = {\n\tSMU_FEATURE_MASKS\n};\n\nstatic const char *smu_get_feature_name(struct smu_context *smu,\n\t\t\t\t\tenum smu_feature_mask feature)\n{\n\tif (feature < 0 || feature >= SMU_FEATURE_COUNT)\n\t\treturn \"unknown smu feature\";\n\treturn __smu_feature_names[feature];\n}\n\nsize_t smu_cmn_get_pp_feature_mask(struct smu_context *smu,\n\t\t\t\t   char *buf)\n{\n\tint8_t sort_feature[max(SMU_FEATURE_COUNT, SMU_FEATURE_MAX)];\n\tuint64_t feature_mask;\n\tint i, feature_index;\n\tuint32_t count = 0;\n\tsize_t size = 0;\n\n\tif (__smu_get_enabled_features(smu, &feature_mask))\n\t\treturn 0;\n\n\tsize =  sysfs_emit_at(buf, size, \"features high: 0x%08x low: 0x%08x\\n\",\n\t\t\tupper_32_bits(feature_mask), lower_32_bits(feature_mask));\n\n\tmemset(sort_feature, -1, sizeof(sort_feature));\n\n\tfor (i = 0; i < SMU_FEATURE_COUNT; i++) {\n\t\tfeature_index = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t\t       CMN2ASIC_MAPPING_FEATURE,\n\t\t\t\t\t\t\t       i);\n\t\tif (feature_index < 0)\n\t\t\tcontinue;\n\n\t\tsort_feature[feature_index] = i;\n\t}\n\n\tsize += sysfs_emit_at(buf, size, \"%-2s. %-20s  %-3s : %-s\\n\",\n\t\t\t\"No\", \"Feature\", \"Bit\", \"State\");\n\n\tfor (feature_index = 0; feature_index < SMU_FEATURE_MAX; feature_index++) {\n\t\tif (sort_feature[feature_index] < 0)\n\t\t\tcontinue;\n\n\t\tsize += sysfs_emit_at(buf, size, \"%02d. %-20s (%2d) : %s\\n\",\n\t\t\t\tcount++,\n\t\t\t\tsmu_get_feature_name(smu, sort_feature[feature_index]),\n\t\t\t\tfeature_index,\n\t\t\t\t!!test_bit(feature_index, (unsigned long *)&feature_mask) ?\n\t\t\t\t\"enabled\" : \"disabled\");\n\t}\n\n\treturn size;\n}\n\nint smu_cmn_set_pp_feature_mask(struct smu_context *smu,\n\t\t\t\tuint64_t new_mask)\n{\n\tint ret = 0;\n\tuint64_t feature_mask;\n\tuint64_t feature_2_enabled = 0;\n\tuint64_t feature_2_disabled = 0;\n\n\tret = __smu_get_enabled_features(smu, &feature_mask);\n\tif (ret)\n\t\treturn ret;\n\n\tfeature_2_enabled  = ~feature_mask & new_mask;\n\tfeature_2_disabled = feature_mask & ~new_mask;\n\n\tif (feature_2_enabled) {\n\t\tret = smu_cmn_feature_update_enable_state(smu,\n\t\t\t\t\t\t\t  feature_2_enabled,\n\t\t\t\t\t\t\t  true);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (feature_2_disabled) {\n\t\tret = smu_cmn_feature_update_enable_state(smu,\n\t\t\t\t\t\t\t  feature_2_disabled,\n\t\t\t\t\t\t\t  false);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\n \nint smu_cmn_disable_all_features_with_exception(struct smu_context *smu,\n\t\t\t\t\t\tenum smu_feature_mask mask)\n{\n\tuint64_t features_to_disable = U64_MAX;\n\tint skipped_feature_id;\n\n\tif (mask != SMU_FEATURE_COUNT) {\n\t\tskipped_feature_id = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t\t\t    CMN2ASIC_MAPPING_FEATURE,\n\t\t\t\t\t\t\t\t    mask);\n\t\tif (skipped_feature_id < 0)\n\t\t\treturn -EINVAL;\n\n\t\tfeatures_to_disable &= ~(1ULL << skipped_feature_id);\n\t}\n\n\treturn smu_cmn_feature_update_enable_state(smu,\n\t\t\t\t\t\t   features_to_disable,\n\t\t\t\t\t\t   0);\n}\n\nint smu_cmn_get_smc_version(struct smu_context *smu,\n\t\t\t    uint32_t *if_version,\n\t\t\t    uint32_t *smu_version)\n{\n\tint ret = 0;\n\n\tif (!if_version && !smu_version)\n\t\treturn -EINVAL;\n\n\tif (smu->smc_fw_if_version && smu->smc_fw_version)\n\t{\n\t\tif (if_version)\n\t\t\t*if_version = smu->smc_fw_if_version;\n\n\t\tif (smu_version)\n\t\t\t*smu_version = smu->smc_fw_version;\n\n\t\treturn 0;\n\t}\n\n\tif (if_version) {\n\t\tret = smu_cmn_send_smc_msg(smu, SMU_MSG_GetDriverIfVersion, if_version);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tsmu->smc_fw_if_version = *if_version;\n\t}\n\n\tif (smu_version) {\n\t\tret = smu_cmn_send_smc_msg(smu, SMU_MSG_GetSmuVersion, smu_version);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tsmu->smc_fw_version = *smu_version;\n\t}\n\n\treturn ret;\n}\n\nint smu_cmn_update_table(struct smu_context *smu,\n\t\t\t enum smu_table_id table_index,\n\t\t\t int argument,\n\t\t\t void *table_data,\n\t\t\t bool drv2smu)\n{\n\tstruct smu_table_context *smu_table = &smu->smu_table;\n\tstruct amdgpu_device *adev = smu->adev;\n\tstruct smu_table *table = &smu_table->driver_table;\n\tint table_id = smu_cmn_to_asic_specific_index(smu,\n\t\t\t\t\t\t      CMN2ASIC_MAPPING_TABLE,\n\t\t\t\t\t\t      table_index);\n\tuint32_t table_size;\n\tint ret = 0;\n\tif (!table_data || table_id >= SMU_TABLE_COUNT || table_id < 0)\n\t\treturn -EINVAL;\n\n\ttable_size = smu_table->tables[table_index].size;\n\n\tif (drv2smu) {\n\t\tmemcpy(table->cpu_addr, table_data, table_size);\n\t\t \n\t\tamdgpu_asic_flush_hdp(adev, NULL);\n\t}\n\n\tret = smu_cmn_send_smc_msg_with_param(smu, drv2smu ?\n\t\t\t\t\t  SMU_MSG_TransferTableDram2Smu :\n\t\t\t\t\t  SMU_MSG_TransferTableSmu2Dram,\n\t\t\t\t\t  table_id | ((argument & 0xFFFF) << 16),\n\t\t\t\t\t  NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!drv2smu) {\n\t\tamdgpu_asic_invalidate_hdp(adev, NULL);\n\t\tmemcpy(table_data, table->cpu_addr, table_size);\n\t}\n\n\treturn 0;\n}\n\nint smu_cmn_write_watermarks_table(struct smu_context *smu)\n{\n\tvoid *watermarks_table = smu->smu_table.watermarks_table;\n\n\tif (!watermarks_table)\n\t\treturn -EINVAL;\n\n\treturn smu_cmn_update_table(smu,\n\t\t\t\t    SMU_TABLE_WATERMARKS,\n\t\t\t\t    0,\n\t\t\t\t    watermarks_table,\n\t\t\t\t    true);\n}\n\nint smu_cmn_write_pptable(struct smu_context *smu)\n{\n\tvoid *pptable = smu->smu_table.driver_pptable;\n\n\treturn smu_cmn_update_table(smu,\n\t\t\t\t    SMU_TABLE_PPTABLE,\n\t\t\t\t    0,\n\t\t\t\t    pptable,\n\t\t\t\t    true);\n}\n\nint smu_cmn_get_metrics_table(struct smu_context *smu,\n\t\t\t      void *metrics_table,\n\t\t\t      bool bypass_cache)\n{\n\tstruct smu_table_context *smu_table = &smu->smu_table;\n\tuint32_t table_size =\n\t\tsmu_table->tables[SMU_TABLE_SMU_METRICS].size;\n\tint ret = 0;\n\n\tif (bypass_cache ||\n\t    !smu_table->metrics_time ||\n\t    time_after(jiffies, smu_table->metrics_time + msecs_to_jiffies(1))) {\n\t\tret = smu_cmn_update_table(smu,\n\t\t\t\t       SMU_TABLE_SMU_METRICS,\n\t\t\t\t       0,\n\t\t\t\t       smu_table->metrics_table,\n\t\t\t\t       false);\n\t\tif (ret) {\n\t\t\tdev_info(smu->adev->dev, \"Failed to export SMU metrics table!\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\tsmu_table->metrics_time = jiffies;\n\t}\n\n\tif (metrics_table)\n\t\tmemcpy(metrics_table, smu_table->metrics_table, table_size);\n\n\treturn 0;\n}\n\nint smu_cmn_get_combo_pptable(struct smu_context *smu)\n{\n\tvoid *pptable = smu->smu_table.combo_pptable;\n\n\treturn smu_cmn_update_table(smu,\n\t\t\t\t    SMU_TABLE_COMBO_PPTABLE,\n\t\t\t\t    0,\n\t\t\t\t    pptable,\n\t\t\t\t    false);\n}\n\nvoid smu_cmn_init_soft_gpu_metrics(void *table, uint8_t frev, uint8_t crev)\n{\n\tstruct metrics_table_header *header = (struct metrics_table_header *)table;\n\tuint16_t structure_size;\n\n#define METRICS_VERSION(a, b)\t((a << 16) | b)\n\n\tswitch (METRICS_VERSION(frev, crev)) {\n\tcase METRICS_VERSION(1, 0):\n\t\tstructure_size = sizeof(struct gpu_metrics_v1_0);\n\t\tbreak;\n\tcase METRICS_VERSION(1, 1):\n\t\tstructure_size = sizeof(struct gpu_metrics_v1_1);\n\t\tbreak;\n\tcase METRICS_VERSION(1, 2):\n\t\tstructure_size = sizeof(struct gpu_metrics_v1_2);\n\t\tbreak;\n\tcase METRICS_VERSION(1, 3):\n\t\tstructure_size = sizeof(struct gpu_metrics_v1_3);\n\t\tbreak;\n\tcase METRICS_VERSION(2, 0):\n\t\tstructure_size = sizeof(struct gpu_metrics_v2_0);\n\t\tbreak;\n\tcase METRICS_VERSION(2, 1):\n\t\tstructure_size = sizeof(struct gpu_metrics_v2_1);\n\t\tbreak;\n\tcase METRICS_VERSION(2, 2):\n\t\tstructure_size = sizeof(struct gpu_metrics_v2_2);\n\t\tbreak;\n\tcase METRICS_VERSION(2, 3):\n\t\tstructure_size = sizeof(struct gpu_metrics_v2_3);\n\t\tbreak;\n\tcase METRICS_VERSION(2, 4):\n\t\tstructure_size = sizeof(struct gpu_metrics_v2_4);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n#undef METRICS_VERSION\n\n\tmemset(header, 0xFF, structure_size);\n\n\theader->format_revision = frev;\n\theader->content_revision = crev;\n\theader->structure_size = structure_size;\n\n}\n\nint smu_cmn_set_mp1_state(struct smu_context *smu,\n\t\t\t  enum pp_mp1_state mp1_state)\n{\n\tenum smu_message_type msg;\n\tint ret;\n\n\tswitch (mp1_state) {\n\tcase PP_MP1_STATE_SHUTDOWN:\n\t\tmsg = SMU_MSG_PrepareMp1ForShutdown;\n\t\tbreak;\n\tcase PP_MP1_STATE_UNLOAD:\n\t\tmsg = SMU_MSG_PrepareMp1ForUnload;\n\t\tbreak;\n\tcase PP_MP1_STATE_RESET:\n\t\tmsg = SMU_MSG_PrepareMp1ForReset;\n\t\tbreak;\n\tcase PP_MP1_STATE_NONE:\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tret = smu_cmn_send_smc_msg(smu, msg, NULL);\n\tif (ret)\n\t\tdev_err(smu->adev->dev, \"[PrepareMp1] Failed!\\n\");\n\n\treturn ret;\n}\n\nbool smu_cmn_is_audio_func_enabled(struct amdgpu_device *adev)\n{\n\tstruct pci_dev *p = NULL;\n\tbool snd_driver_loaded;\n\n\t \n\tp = pci_get_domain_bus_and_slot(pci_domain_nr(adev->pdev->bus),\n\t\t\tadev->pdev->bus->number, 1);\n\tif (!p)\n\t\treturn true;\n\n\tsnd_driver_loaded = pci_is_enabled(p) ? true : false;\n\n\tpci_dev_put(p);\n\n\treturn snd_driver_loaded;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}