{
  "module_name": "smu7_hwmgr.c",
  "hash_id": "968ffee1a8b068e886b647cb624aa1d2e08e3fbc3e12731bd4919cb3e48b7eb9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/amd/pm/powerplay/hwmgr/smu7_hwmgr.c",
  "human_readable_source": " \n#include \"pp_debug.h\"\n#include <linux/delay.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/slab.h>\n#include <asm/div64.h>\n#if IS_ENABLED(CONFIG_X86_64)\n#include <asm/intel-family.h>\n#endif\n#include <drm/amdgpu_drm.h>\n#include \"ppatomctrl.h\"\n#include \"atombios.h\"\n#include \"pptable_v1_0.h\"\n#include \"pppcielanes.h\"\n#include \"amd_pcie_helpers.h\"\n#include \"hardwaremanager.h\"\n#include \"process_pptables_v1_0.h\"\n#include \"cgs_common.h\"\n\n#include \"smu7_common.h\"\n\n#include \"hwmgr.h\"\n#include \"smu7_hwmgr.h\"\n#include \"smu_ucode_xfer_vi.h\"\n#include \"smu7_powertune.h\"\n#include \"smu7_dyn_defaults.h\"\n#include \"smu7_thermal.h\"\n#include \"smu7_clockpowergating.h\"\n#include \"processpptables.h\"\n#include \"pp_thermal.h\"\n#include \"smu7_baco.h\"\n#include \"smu7_smumgr.h\"\n#include \"polaris10_smumgr.h\"\n\n#include \"ivsrcid/ivsrcid_vislands30.h\"\n\n#define MC_CG_ARB_FREQ_F0           0x0a\n#define MC_CG_ARB_FREQ_F1           0x0b\n#define MC_CG_ARB_FREQ_F2           0x0c\n#define MC_CG_ARB_FREQ_F3           0x0d\n\n#define MC_CG_SEQ_DRAMCONF_S0       0x05\n#define MC_CG_SEQ_DRAMCONF_S1       0x06\n#define MC_CG_SEQ_YCLK_SUSPEND      0x04\n#define MC_CG_SEQ_YCLK_RESUME       0x0a\n\n#define SMC_CG_IND_START            0xc0030000\n#define SMC_CG_IND_END              0xc0040000\n\n#define MEM_FREQ_LOW_LATENCY        25000\n#define MEM_FREQ_HIGH_LATENCY       80000\n\n#define MEM_LATENCY_HIGH            45\n#define MEM_LATENCY_LOW             35\n#define MEM_LATENCY_ERR             0xFFFF\n\n#define MC_SEQ_MISC0_GDDR5_SHIFT 28\n#define MC_SEQ_MISC0_GDDR5_MASK  0xf0000000\n#define MC_SEQ_MISC0_GDDR5_VALUE 5\n\n#define PCIE_BUS_CLK                10000\n#define TCLK                        (PCIE_BUS_CLK / 10)\n\nstatic struct profile_mode_setting smu7_profiling[7] = {\n\t\t\t\t\t {0, 0, 0, 0, 0, 0, 0, 0},\n\t\t\t\t\t {1, 0, 100, 30, 1, 0, 100, 10},\n\t\t\t\t\t {1, 10, 0, 30, 0, 0, 0, 0},\n\t\t\t\t\t {0, 0, 0, 0, 1, 10, 16, 31},\n\t\t\t\t\t {1, 0, 11, 50, 1, 0, 100, 10},\n\t\t\t\t\t {1, 0, 5, 30, 0, 0, 0, 0},\n\t\t\t\t\t {0, 0, 0, 0, 0, 0, 0, 0},\n};\n\n#define PPSMC_MSG_SetVBITimeout_VEGAM    ((uint16_t) 0x310)\n\n#define ixPWR_SVI2_PLANE1_LOAD                     0xC0200280\n#define PWR_SVI2_PLANE1_LOAD__PSI1_MASK                    0x00000020L\n#define PWR_SVI2_PLANE1_LOAD__PSI0_EN_MASK                 0x00000040L\n#define PWR_SVI2_PLANE1_LOAD__PSI1__SHIFT                  0x00000005\n#define PWR_SVI2_PLANE1_LOAD__PSI0_EN__SHIFT               0x00000006\n\n#define STRAP_EVV_REVISION_MSB\t\t2211\n#define STRAP_EVV_REVISION_LSB\t\t2208\n\n \nenum DPM_EVENT_SRC {\n\tDPM_EVENT_SRC_ANALOG = 0,\n\tDPM_EVENT_SRC_EXTERNAL = 1,\n\tDPM_EVENT_SRC_DIGITAL = 2,\n\tDPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,\n\tDPM_EVENT_SRC_DIGITAL_OR_EXTERNAL = 4\n};\n\n#define ixDIDT_SQ_EDC_CTRL                         0x0013\n#define ixDIDT_SQ_EDC_THRESHOLD                    0x0014\n#define ixDIDT_SQ_EDC_STALL_PATTERN_1_2            0x0015\n#define ixDIDT_SQ_EDC_STALL_PATTERN_3_4            0x0016\n#define ixDIDT_SQ_EDC_STALL_PATTERN_5_6            0x0017\n#define ixDIDT_SQ_EDC_STALL_PATTERN_7              0x0018\n\n#define ixDIDT_TD_EDC_CTRL                         0x0053\n#define ixDIDT_TD_EDC_THRESHOLD                    0x0054\n#define ixDIDT_TD_EDC_STALL_PATTERN_1_2            0x0055\n#define ixDIDT_TD_EDC_STALL_PATTERN_3_4            0x0056\n#define ixDIDT_TD_EDC_STALL_PATTERN_5_6            0x0057\n#define ixDIDT_TD_EDC_STALL_PATTERN_7              0x0058\n\n#define ixDIDT_TCP_EDC_CTRL                        0x0073\n#define ixDIDT_TCP_EDC_THRESHOLD                   0x0074\n#define ixDIDT_TCP_EDC_STALL_PATTERN_1_2           0x0075\n#define ixDIDT_TCP_EDC_STALL_PATTERN_3_4           0x0076\n#define ixDIDT_TCP_EDC_STALL_PATTERN_5_6           0x0077\n#define ixDIDT_TCP_EDC_STALL_PATTERN_7             0x0078\n\n#define ixDIDT_DB_EDC_CTRL                         0x0033\n#define ixDIDT_DB_EDC_THRESHOLD                    0x0034\n#define ixDIDT_DB_EDC_STALL_PATTERN_1_2            0x0035\n#define ixDIDT_DB_EDC_STALL_PATTERN_3_4            0x0036\n#define ixDIDT_DB_EDC_STALL_PATTERN_5_6            0x0037\n#define ixDIDT_DB_EDC_STALL_PATTERN_7              0x0038\n\nuint32_t DIDTEDCConfig_P12[] = {\n    ixDIDT_SQ_EDC_STALL_PATTERN_1_2,\n    ixDIDT_SQ_EDC_STALL_PATTERN_3_4,\n    ixDIDT_SQ_EDC_STALL_PATTERN_5_6,\n    ixDIDT_SQ_EDC_STALL_PATTERN_7,\n    ixDIDT_SQ_EDC_THRESHOLD,\n    ixDIDT_SQ_EDC_CTRL,\n    ixDIDT_TD_EDC_STALL_PATTERN_1_2,\n    ixDIDT_TD_EDC_STALL_PATTERN_3_4,\n    ixDIDT_TD_EDC_STALL_PATTERN_5_6,\n    ixDIDT_TD_EDC_STALL_PATTERN_7,\n    ixDIDT_TD_EDC_THRESHOLD,\n    ixDIDT_TD_EDC_CTRL,\n    ixDIDT_TCP_EDC_STALL_PATTERN_1_2,\n    ixDIDT_TCP_EDC_STALL_PATTERN_3_4,\n    ixDIDT_TCP_EDC_STALL_PATTERN_5_6,\n    ixDIDT_TCP_EDC_STALL_PATTERN_7,\n    ixDIDT_TCP_EDC_THRESHOLD,\n    ixDIDT_TCP_EDC_CTRL,\n    ixDIDT_DB_EDC_STALL_PATTERN_1_2,\n    ixDIDT_DB_EDC_STALL_PATTERN_3_4,\n    ixDIDT_DB_EDC_STALL_PATTERN_5_6,\n    ixDIDT_DB_EDC_STALL_PATTERN_7,\n    ixDIDT_DB_EDC_THRESHOLD,\n    ixDIDT_DB_EDC_CTRL,\n    0xFFFFFFFF \n};\n\nstatic const unsigned long PhwVIslands_Magic = (unsigned long)(PHM_VIslands_Magic);\nstatic int smu7_force_clock_level(struct pp_hwmgr *hwmgr,\n\t\tenum pp_clock_type type, uint32_t mask);\nstatic int smu7_notify_has_display(struct pp_hwmgr *hwmgr);\n\nstatic struct smu7_power_state *cast_phw_smu7_power_state(\n\t\t\t\t  struct pp_hw_power_state *hw_ps)\n{\n\tPP_ASSERT_WITH_CODE((PhwVIslands_Magic == hw_ps->magic),\n\t\t\t\t\"Invalid Powerstate Type!\",\n\t\t\t\t return NULL);\n\n\treturn (struct smu7_power_state *)hw_ps;\n}\n\nstatic const struct smu7_power_state *cast_const_phw_smu7_power_state(\n\t\t\t\t const struct pp_hw_power_state *hw_ps)\n{\n\tPP_ASSERT_WITH_CODE((PhwVIslands_Magic == hw_ps->magic),\n\t\t\t\t\"Invalid Powerstate Type!\",\n\t\t\t\t return NULL);\n\n\treturn (const struct smu7_power_state *)hw_ps;\n}\n\n \nstatic int smu7_get_mc_microcode_version(struct pp_hwmgr *hwmgr)\n{\n\tcgs_write_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_INDEX, 0x9F);\n\n\thwmgr->microcode_version_info.MC = cgs_read_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_DATA);\n\n\treturn 0;\n}\n\nstatic uint16_t smu7_get_current_pcie_speed(struct pp_hwmgr *hwmgr)\n{\n\tuint32_t speedCntl = 0;\n\n\t \n\tspeedCntl = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__PCIE,\n\t\t\tixPCIE_LC_SPEED_CNTL);\n\treturn((uint16_t)PHM_GET_FIELD(speedCntl,\n\t\t\tPCIE_LC_SPEED_CNTL, LC_CURRENT_DATA_RATE));\n}\n\nstatic int smu7_get_current_pcie_lane_number(struct pp_hwmgr *hwmgr)\n{\n\tuint32_t link_width;\n\n\t \n\tlink_width = PHM_READ_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,\n\t\t\tPCIE_LC_LINK_WIDTH_CNTL, LC_LINK_WIDTH_RD);\n\n\tPP_ASSERT_WITH_CODE((7 >= link_width),\n\t\t\t\"Invalid PCIe lane width!\", return 0);\n\n\treturn decode_pcie_lane_width(link_width);\n}\n\n \nstatic int smu7_enable_smc_voltage_controller(struct pp_hwmgr *hwmgr)\n{\n\tif (hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t    hwmgr->chip_id <= CHIP_VEGAM) {\n\t\tPHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\tCGS_IND_REG__SMC, PWR_SVI2_PLANE1_LOAD, PSI1, 0);\n\t\tPHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\tCGS_IND_REG__SMC, PWR_SVI2_PLANE1_LOAD, PSI0_EN, 0);\n\t}\n\n\tif (hwmgr->feature_mask & PP_SMC_VOLTAGE_CONTROL_MASK)\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_Voltage_Cntl_Enable, NULL);\n\n\treturn 0;\n}\n\n \nstatic bool smu7_voltage_control(const struct pp_hwmgr *hwmgr)\n{\n\tconst struct smu7_hwmgr *data =\n\t\t\t(const struct smu7_hwmgr *)(hwmgr->backend);\n\n\treturn (SMU7_VOLTAGE_CONTROL_NONE != data->voltage_control);\n}\n\n \nstatic int smu7_enable_voltage_control(struct pp_hwmgr *hwmgr)\n{\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tGENERAL_PWRMGT, VOLT_PWRMGT_EN, 1);\n\n\treturn 0;\n}\n\nstatic int phm_get_svi2_voltage_table_v0(pp_atomctrl_voltage_table *voltage_table,\n\t\tstruct phm_clock_voltage_dependency_table *voltage_dependency_table\n\t\t)\n{\n\tuint32_t i;\n\n\tPP_ASSERT_WITH_CODE((NULL != voltage_table),\n\t\t\t\"Voltage Dependency Table empty.\", return -EINVAL;);\n\n\tvoltage_table->mask_low = 0;\n\tvoltage_table->phase_delay = 0;\n\tvoltage_table->count = voltage_dependency_table->count;\n\n\tfor (i = 0; i < voltage_dependency_table->count; i++) {\n\t\tvoltage_table->entries[i].value =\n\t\t\tvoltage_dependency_table->entries[i].v;\n\t\tvoltage_table->entries[i].smio_low = 0;\n\t}\n\n\treturn 0;\n}\n\n\n \nstatic int smu7_construct_voltage_tables(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tint result = 0;\n\tuint32_t tmp;\n\n\tif (SMU7_VOLTAGE_CONTROL_BY_GPIO == data->mvdd_control) {\n\t\tresult = atomctrl_get_voltage_table_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT,\n\t\t\t\t&(data->mvdd_voltage_table));\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to retrieve MVDD table.\",\n\t\t\t\treturn result);\n\t} else if (SMU7_VOLTAGE_CONTROL_BY_SVID2 == data->mvdd_control) {\n\t\tif (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\t\tresult = phm_get_svi2_mvdd_voltage_table(&(data->mvdd_voltage_table),\n\t\t\t\t\ttable_info->vdd_dep_on_mclk);\n\t\telse if (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\t\tresult = phm_get_svi2_voltage_table_v0(&(data->mvdd_voltage_table),\n\t\t\t\t\thwmgr->dyn_state.mvdd_dependency_on_mclk);\n\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to retrieve SVI2 MVDD table from dependency table.\",\n\t\t\t\treturn result;);\n\t}\n\n\tif (SMU7_VOLTAGE_CONTROL_BY_GPIO == data->vddci_control) {\n\t\tresult = atomctrl_get_voltage_table_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT,\n\t\t\t\t&(data->vddci_voltage_table));\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to retrieve VDDCI table.\",\n\t\t\t\treturn result);\n\t} else if (SMU7_VOLTAGE_CONTROL_BY_SVID2 == data->vddci_control) {\n\t\tif (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\t\tresult = phm_get_svi2_vddci_voltage_table(&(data->vddci_voltage_table),\n\t\t\t\t\ttable_info->vdd_dep_on_mclk);\n\t\telse if (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\t\tresult = phm_get_svi2_voltage_table_v0(&(data->vddci_voltage_table),\n\t\t\t\t\thwmgr->dyn_state.vddci_dependency_on_mclk);\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to retrieve SVI2 VDDCI table from dependency table.\",\n\t\t\t\treturn result);\n\t}\n\n\tif (SMU7_VOLTAGE_CONTROL_BY_SVID2 == data->vdd_gfx_control) {\n\t\t \n\t\tresult = phm_get_svi2_vdd_voltage_table(&(data->vddgfx_voltage_table),\n\t\t\t\t\ttable_info->vddgfx_lookup_table);\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\"Failed to retrieve SVI2 VDDGFX table from lookup table.\", return result;);\n\t}\n\n\n\tif (SMU7_VOLTAGE_CONTROL_BY_GPIO == data->voltage_control) {\n\t\tresult = atomctrl_get_voltage_table_v3(hwmgr,\n\t\t\t\t\tVOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_GPIO_LUT,\n\t\t\t\t\t&data->vddc_voltage_table);\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\"Failed to retrieve VDDC table.\", return result;);\n\t} else if (SMU7_VOLTAGE_CONTROL_BY_SVID2 == data->voltage_control) {\n\n\t\tif (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\t\tresult = phm_get_svi2_voltage_table_v0(&data->vddc_voltage_table,\n\t\t\t\t\thwmgr->dyn_state.vddc_dependency_on_mclk);\n\t\telse if (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\t\tresult = phm_get_svi2_vdd_voltage_table(&(data->vddc_voltage_table),\n\t\t\t\ttable_info->vddc_lookup_table);\n\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\"Failed to retrieve SVI2 VDDC table from dependency table.\", return result;);\n\t}\n\n\ttmp = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_VDDC);\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(data->vddc_voltage_table.count <= tmp),\n\t\t\"Too many voltage values for VDDC. Trimming to fit state table.\",\n\t\t\tphm_trim_voltage_table_to_fit_state_table(tmp,\n\t\t\t\t\t\t&(data->vddc_voltage_table)));\n\n\ttmp = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_VDDGFX);\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(data->vddgfx_voltage_table.count <= tmp),\n\t\t\"Too many voltage values for VDDC. Trimming to fit state table.\",\n\t\t\tphm_trim_voltage_table_to_fit_state_table(tmp,\n\t\t\t\t\t\t&(data->vddgfx_voltage_table)));\n\n\ttmp = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_VDDCI);\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(data->vddci_voltage_table.count <= tmp),\n\t\t\"Too many voltage values for VDDCI. Trimming to fit state table.\",\n\t\t\tphm_trim_voltage_table_to_fit_state_table(tmp,\n\t\t\t\t\t&(data->vddci_voltage_table)));\n\n\ttmp = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_MVDD);\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(data->mvdd_voltage_table.count <= tmp),\n\t\t\"Too many voltage values for MVDD. Trimming to fit state table.\",\n\t\t\tphm_trim_voltage_table_to_fit_state_table(tmp,\n\t\t\t\t\t\t&(data->mvdd_voltage_table)));\n\n\treturn 0;\n}\n\n \nstatic int smu7_program_static_screen_threshold_parameters(\n\t\t\t\t\t\t\tstruct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tCG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD_UNIT,\n\t\t\tdata->static_screen_threshold_unit);\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tCG_STATIC_SCREEN_PARAMETER, STATIC_SCREEN_THRESHOLD,\n\t\t\tdata->static_screen_threshold);\n\n\treturn 0;\n}\n\n \nstatic int smu7_enable_display_gap(struct pp_hwmgr *hwmgr)\n{\n\tuint32_t display_gap =\n\t\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tixCG_DISPLAY_GAP_CNTL);\n\n\tdisplay_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL,\n\t\t\tDISP_GAP, DISPLAY_GAP_IGNORE);\n\n\tdisplay_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL,\n\t\t\tDISP_GAP_MCHG, DISPLAY_GAP_VBLANK);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tixCG_DISPLAY_GAP_CNTL, display_gap);\n\n\treturn 0;\n}\n\n \nstatic int smu7_program_voting_clients(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tint i;\n\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tSCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 0);\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tSCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 0);\n\n\tfor (i = 0; i < 8; i++)\n\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tixCG_FREQ_TRAN_VOTING_0 + i * 4,\n\t\t\t\t\tdata->voting_rights_clients[i]);\n\treturn 0;\n}\n\nstatic int smu7_clear_voting_clients(struct pp_hwmgr *hwmgr)\n{\n\tint i;\n\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tSCLK_PWRMGT_CNTL, RESET_SCLK_CNT, 1);\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tSCLK_PWRMGT_CNTL, RESET_BUSY_CNT, 1);\n\n\tfor (i = 0; i < 8; i++)\n\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\tixCG_FREQ_TRAN_VOTING_0 + i * 4, 0);\n\n\treturn 0;\n}\n\n \nstatic int smu7_copy_and_switch_arb_sets(struct pp_hwmgr *hwmgr,\n\t\tuint32_t arb_src, uint32_t arb_dest)\n{\n\tuint32_t mc_arb_dram_timing;\n\tuint32_t mc_arb_dram_timing2;\n\tuint32_t burst_time;\n\tuint32_t mc_cg_config;\n\n\tswitch (arb_src) {\n\tcase MC_CG_ARB_FREQ_F0:\n\t\tmc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING);\n\t\tmc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2);\n\t\tburst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0);\n\t\tbreak;\n\tcase MC_CG_ARB_FREQ_F1:\n\t\tmc_arb_dram_timing  = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1);\n\t\tmc_arb_dram_timing2 = cgs_read_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1);\n\t\tburst_time = PHM_READ_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (arb_dest) {\n\tcase MC_CG_ARB_FREQ_F0:\n\t\tcgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING, mc_arb_dram_timing);\n\t\tcgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2, mc_arb_dram_timing2);\n\t\tPHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE0, burst_time);\n\t\tbreak;\n\tcase MC_CG_ARB_FREQ_F1:\n\t\tcgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING_1, mc_arb_dram_timing);\n\t\tcgs_write_register(hwmgr->device, mmMC_ARB_DRAM_TIMING2_1, mc_arb_dram_timing2);\n\t\tPHM_WRITE_FIELD(hwmgr->device, MC_ARB_BURST_TIME, STATE1, burst_time);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tmc_cg_config = cgs_read_register(hwmgr->device, mmMC_CG_CONFIG);\n\tmc_cg_config |= 0x0000000F;\n\tcgs_write_register(hwmgr->device, mmMC_CG_CONFIG, mc_cg_config);\n\tPHM_WRITE_FIELD(hwmgr->device, MC_ARB_CG, CG_ARB_REQ, arb_dest);\n\n\treturn 0;\n}\n\nstatic int smu7_reset_to_default(struct pp_hwmgr *hwmgr)\n{\n\treturn smum_send_msg_to_smc(hwmgr, PPSMC_MSG_ResetToDefaults, NULL);\n}\n\n \nstatic int smu7_initial_switch_from_arbf0_to_f1(struct pp_hwmgr *hwmgr)\n{\n\treturn smu7_copy_and_switch_arb_sets(hwmgr,\n\t\t\tMC_CG_ARB_FREQ_F0, MC_CG_ARB_FREQ_F1);\n}\n\nstatic int smu7_force_switch_to_arbf0(struct pp_hwmgr *hwmgr)\n{\n\tuint32_t tmp;\n\n\ttmp = (cgs_read_ind_register(hwmgr->device,\n\t\t\tCGS_IND_REG__SMC, ixSMC_SCRATCH9) &\n\t\t\t0x0000ff00) >> 8;\n\n\tif (tmp == MC_CG_ARB_FREQ_F0)\n\t\treturn 0;\n\n\treturn smu7_copy_and_switch_arb_sets(hwmgr,\n\t\t\ttmp, MC_CG_ARB_FREQ_F0);\n}\n\nstatic uint16_t smu7_override_pcie_speed(struct pp_hwmgr *hwmgr)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)(hwmgr->adev);\n\tuint16_t pcie_gen = 0;\n\n\tif (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN4 &&\n\t    adev->pm.pcie_gen_mask & CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN4)\n\t\tpcie_gen = 3;\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3 &&\n\t\tadev->pm.pcie_gen_mask & CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\tpcie_gen = 2;\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN2 &&\n\t\tadev->pm.pcie_gen_mask & CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN2)\n\t\tpcie_gen = 1;\n\telse if (adev->pm.pcie_gen_mask & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN1 &&\n\t\tadev->pm.pcie_gen_mask & CAIL_ASIC_PCIE_LINK_SPEED_SUPPORT_GEN1)\n\t\tpcie_gen = 0;\n\n\treturn pcie_gen;\n}\n\nstatic uint16_t smu7_override_pcie_width(struct pp_hwmgr *hwmgr)\n{\n\tstruct amdgpu_device *adev = (struct amdgpu_device *)(hwmgr->adev);\n\tuint16_t pcie_width = 0;\n\n\tif (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X16)\n\t\tpcie_width = 16;\n\telse if (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X12)\n\t\tpcie_width = 12;\n\telse if (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X8)\n\t\tpcie_width = 8;\n\telse if (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X4)\n\t\tpcie_width = 4;\n\telse if (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X2)\n\t\tpcie_width = 2;\n\telse if (adev->pm.pcie_mlw_mask & CAIL_PCIE_LINK_WIDTH_SUPPORT_X1)\n\t\tpcie_width = 1;\n\n\treturn pcie_width;\n}\n\nstatic int smu7_setup_default_pcie_table(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tstruct phm_ppt_v1_pcie_table *pcie_table = NULL;\n\n\tuint32_t i, max_entry;\n\tuint32_t tmp;\n\n\tPP_ASSERT_WITH_CODE((data->use_pcie_performance_levels ||\n\t\t\tdata->use_pcie_power_saving_levels), \"No pcie performance levels!\",\n\t\t\treturn -EINVAL);\n\n\tif (table_info != NULL)\n\t\tpcie_table = table_info->pcie_table;\n\n\tif (data->use_pcie_performance_levels &&\n\t\t\t!data->use_pcie_power_saving_levels) {\n\t\tdata->pcie_gen_power_saving = data->pcie_gen_performance;\n\t\tdata->pcie_lane_power_saving = data->pcie_lane_performance;\n\t} else if (!data->use_pcie_performance_levels &&\n\t\t\tdata->use_pcie_power_saving_levels) {\n\t\tdata->pcie_gen_performance = data->pcie_gen_power_saving;\n\t\tdata->pcie_lane_performance = data->pcie_lane_power_saving;\n\t}\n\ttmp = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_LINK);\n\tphm_reset_single_dpm_table(&data->dpm_table.pcie_speed_table,\n\t\t\t\t\ttmp,\n\t\t\t\t\tMAX_REGULAR_DPM_NUMBER);\n\n\tif (pcie_table != NULL) {\n\t\t \n\t\tmax_entry = (tmp < pcie_table->count) ? tmp : pcie_table->count;\n\t\tfor (i = 1; i < max_entry; i++) {\n\t\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, i - 1,\n\t\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\t\tpcie_table->entries[i].gen_speed),\n\t\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\t\tpcie_table->entries[i].lane_width));\n\t\t}\n\t\tdata->dpm_table.pcie_speed_table.count = max_entry - 1;\n\t\tsmum_update_smc_table(hwmgr, SMU_BIF_TABLE);\n\t} else {\n\t\t \n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 0,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Min_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 1,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Min_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 2,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Max_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 3,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Max_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 4,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Max_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, 5,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Max_PCIEGen),\n\t\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\t\tPP_Max_PCIELane));\n\n\t\tdata->dpm_table.pcie_speed_table.count = 6;\n\t}\n\t \n\tif (hwmgr->chip_family == AMDGPU_FAMILY_CI) {\n\t\tfor (i = 0; i <= data->dpm_table.pcie_speed_table.count; i++)\n\t\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table, i,\n\t\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\t\tPP_Max_PCIEGen),\n\t\t\t\tdata->vbios_boot_state.pcie_lane_bootup_value);\n\t} else {\n\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table,\n\t\t\tdata->dpm_table.pcie_speed_table.count,\n\t\t\tget_pcie_gen_support(data->pcie_gen_cap,\n\t\t\t\t\tPP_Min_PCIEGen),\n\t\t\tget_pcie_lane_support(data->pcie_lane_cap,\n\t\t\t\t\tPP_Max_PCIELane));\n\n\t\tif (data->pcie_dpm_key_disabled)\n\t\t\tphm_setup_pcie_table_entry(&data->dpm_table.pcie_speed_table,\n\t\t\t\tdata->dpm_table.pcie_speed_table.count,\n\t\t\t\tsmu7_override_pcie_speed(hwmgr), smu7_override_pcie_width(hwmgr));\n\t}\n\treturn 0;\n}\n\nstatic int smu7_reset_dpm_tables(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tmemset(&(data->dpm_table), 0x00, sizeof(data->dpm_table));\n\n\tphm_reset_single_dpm_table(\n\t\t\t&data->dpm_table.sclk_table,\n\t\t\t\tsmum_get_mac_definition(hwmgr,\n\t\t\t\t\tSMU_MAX_LEVELS_GRAPHICS),\n\t\t\t\t\tMAX_REGULAR_DPM_NUMBER);\n\tphm_reset_single_dpm_table(\n\t\t\t&data->dpm_table.mclk_table,\n\t\t\tsmum_get_mac_definition(hwmgr,\n\t\t\t\tSMU_MAX_LEVELS_MEMORY), MAX_REGULAR_DPM_NUMBER);\n\n\tphm_reset_single_dpm_table(\n\t\t\t&data->dpm_table.vddc_table,\n\t\t\t\tsmum_get_mac_definition(hwmgr,\n\t\t\t\t\tSMU_MAX_LEVELS_VDDC),\n\t\t\t\t\tMAX_REGULAR_DPM_NUMBER);\n\tphm_reset_single_dpm_table(\n\t\t\t&data->dpm_table.vddci_table,\n\t\t\tsmum_get_mac_definition(hwmgr,\n\t\t\t\tSMU_MAX_LEVELS_VDDCI), MAX_REGULAR_DPM_NUMBER);\n\n\tphm_reset_single_dpm_table(\n\t\t\t&data->dpm_table.mvdd_table,\n\t\t\t\tsmum_get_mac_definition(hwmgr,\n\t\t\t\t\tSMU_MAX_LEVELS_MVDD),\n\t\t\t\t\tMAX_REGULAR_DPM_NUMBER);\n\treturn 0;\n}\n \n\nstatic int smu7_setup_dpm_tables_v0(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_clock_voltage_dependency_table *allowed_vdd_sclk_table =\n\t\thwmgr->dyn_state.vddc_dependency_on_sclk;\n\tstruct phm_clock_voltage_dependency_table *allowed_vdd_mclk_table =\n\t\thwmgr->dyn_state.vddc_dependency_on_mclk;\n\tstruct phm_cac_leakage_table *std_voltage_table =\n\t\thwmgr->dyn_state.cac_leakage_table;\n\tuint32_t i;\n\n\tPP_ASSERT_WITH_CODE(allowed_vdd_sclk_table != NULL,\n\t\t\"SCLK dependency table is missing. This table is mandatory\", return -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_vdd_sclk_table->count >= 1,\n\t\t\"SCLK dependency table has to have is missing. This table is mandatory\", return -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,\n\t\t\"MCLK dependency table is missing. This table is mandatory\", return -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_vdd_mclk_table->count >= 1,\n\t\t\"VMCLK dependency table has to have is missing. This table is mandatory\", return -EINVAL);\n\n\n\t \n\tdata->dpm_table.sclk_table.count = 0;\n\n\tfor (i = 0; i < allowed_vdd_sclk_table->count; i++) {\n\t\tif (i == 0 || data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count-1].value !=\n\t\t\t\tallowed_vdd_sclk_table->entries[i].clk) {\n\t\t\tdata->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].value =\n\t\t\t\tallowed_vdd_sclk_table->entries[i].clk;\n\t\t\tdata->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].enabled = (i == 0) ? 1 : 0;\n\t\t\tdata->dpm_table.sclk_table.count++;\n\t\t}\n\t}\n\n\tPP_ASSERT_WITH_CODE(allowed_vdd_mclk_table != NULL,\n\t\t\"MCLK dependency table is missing. This table is mandatory\", return -EINVAL);\n\t \n\tdata->dpm_table.mclk_table.count = 0;\n\tfor (i = 0; i < allowed_vdd_mclk_table->count; i++) {\n\t\tif (i == 0 || data->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count-1].value !=\n\t\t\tallowed_vdd_mclk_table->entries[i].clk) {\n\t\t\tdata->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].value =\n\t\t\t\tallowed_vdd_mclk_table->entries[i].clk;\n\t\t\tdata->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].enabled = (i == 0) ? 1 : 0;\n\t\t\tdata->dpm_table.mclk_table.count++;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < allowed_vdd_sclk_table->count; i++) {\n\t\tdata->dpm_table.vddc_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;\n\t\tdata->dpm_table.vddc_table.dpm_levels[i].param1 = std_voltage_table->entries[i].Leakage;\n\t\t \n\t\tdata->dpm_table.vddc_table.dpm_levels[i].enabled = true;\n\t}\n\n\tdata->dpm_table.vddc_table.count = allowed_vdd_sclk_table->count;\n\tallowed_vdd_mclk_table = hwmgr->dyn_state.vddci_dependency_on_mclk;\n\n\tif (NULL != allowed_vdd_mclk_table) {\n\t\t \n\t\tfor (i = 0; i < allowed_vdd_mclk_table->count; i++) {\n\t\t\tdata->dpm_table.vddci_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;\n\t\t\tdata->dpm_table.vddci_table.dpm_levels[i].enabled = true;\n\t\t}\n\t\tdata->dpm_table.vddci_table.count = allowed_vdd_mclk_table->count;\n\t}\n\n\tallowed_vdd_mclk_table = hwmgr->dyn_state.mvdd_dependency_on_mclk;\n\n\tif (NULL != allowed_vdd_mclk_table) {\n\t\t \n\t\tfor (i = 0; i < allowed_vdd_mclk_table->count; i++) {\n\t\t\tdata->dpm_table.mvdd_table.dpm_levels[i].value = allowed_vdd_mclk_table->entries[i].v;\n\t\t\tdata->dpm_table.mvdd_table.dpm_levels[i].enabled = true;\n\t\t}\n\t\tdata->dpm_table.mvdd_table.count = allowed_vdd_mclk_table->count;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_setup_dpm_tables_v1(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tuint32_t i;\n\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table;\n\n\tif (table_info == NULL)\n\t\treturn -EINVAL;\n\n\tdep_sclk_table = table_info->vdd_dep_on_sclk;\n\tdep_mclk_table = table_info->vdd_dep_on_mclk;\n\n\tPP_ASSERT_WITH_CODE(dep_sclk_table != NULL,\n\t\t\t\"SCLK dependency table is missing.\",\n\t\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(dep_sclk_table->count >= 1,\n\t\t\t\"SCLK dependency table count is 0.\",\n\t\t\treturn -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(dep_mclk_table != NULL,\n\t\t\t\"MCLK dependency table is missing.\",\n\t\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(dep_mclk_table->count >= 1,\n\t\t\t\"MCLK dependency table count is 0\",\n\t\t\treturn -EINVAL);\n\n\t \n\tdata->dpm_table.sclk_table.count = 0;\n\tfor (i = 0; i < dep_sclk_table->count; i++) {\n\t\tif (i == 0 || data->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count - 1].value !=\n\t\t\t\t\t\tdep_sclk_table->entries[i].clk) {\n\n\t\t\tdata->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].value =\n\t\t\t\t\tdep_sclk_table->entries[i].clk;\n\n\t\t\tdata->dpm_table.sclk_table.dpm_levels[data->dpm_table.sclk_table.count].enabled =\n\t\t\t\t\ti == 0;\n\t\t\tdata->dpm_table.sclk_table.count++;\n\t\t}\n\t}\n\tif (hwmgr->platform_descriptor.overdriveLimit.engineClock == 0)\n\t\thwmgr->platform_descriptor.overdriveLimit.engineClock = dep_sclk_table->entries[i-1].clk;\n\t \n\tdata->dpm_table.mclk_table.count = 0;\n\tfor (i = 0; i < dep_mclk_table->count; i++) {\n\t\tif (i == 0 || data->dpm_table.mclk_table.dpm_levels\n\t\t\t\t[data->dpm_table.mclk_table.count - 1].value !=\n\t\t\t\t\t\tdep_mclk_table->entries[i].clk) {\n\t\t\tdata->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].value =\n\t\t\t\t\t\t\tdep_mclk_table->entries[i].clk;\n\t\t\tdata->dpm_table.mclk_table.dpm_levels[data->dpm_table.mclk_table.count].enabled =\n\t\t\t\t\t\t\ti == 0;\n\t\t\tdata->dpm_table.mclk_table.count++;\n\t\t}\n\t}\n\n\tif (hwmgr->platform_descriptor.overdriveLimit.memoryClock == 0)\n\t\thwmgr->platform_descriptor.overdriveLimit.memoryClock = dep_mclk_table->entries[i-1].clk;\n\treturn 0;\n}\n\nstatic int smu7_odn_initial_default_setting(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_odn_dpm_table *odn_table = &(data->odn_dpm_table);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tuint32_t i;\n\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table;\n\tstruct phm_odn_performance_level *entries;\n\n\tif (table_info == NULL)\n\t\treturn -EINVAL;\n\n\tdep_sclk_table = table_info->vdd_dep_on_sclk;\n\tdep_mclk_table = table_info->vdd_dep_on_mclk;\n\n\todn_table->odn_core_clock_dpm_levels.num_of_pl =\n\t\t\t\t\t\tdata->golden_dpm_table.sclk_table.count;\n\tentries = odn_table->odn_core_clock_dpm_levels.entries;\n\tfor (i = 0; i < data->golden_dpm_table.sclk_table.count; i++) {\n\t\tentries[i].clock = data->golden_dpm_table.sclk_table.dpm_levels[i].value;\n\t\tentries[i].enabled = true;\n\t\tentries[i].vddc = dep_sclk_table->entries[i].vddc;\n\t}\n\n\tsmu_get_voltage_dependency_table_ppt_v1(dep_sclk_table,\n\t\t(struct phm_ppt_v1_clock_voltage_dependency_table *)&(odn_table->vdd_dependency_on_sclk));\n\n\todn_table->odn_memory_clock_dpm_levels.num_of_pl =\n\t\t\t\t\t\tdata->golden_dpm_table.mclk_table.count;\n\tentries = odn_table->odn_memory_clock_dpm_levels.entries;\n\tfor (i = 0; i < data->golden_dpm_table.mclk_table.count; i++) {\n\t\tentries[i].clock = data->golden_dpm_table.mclk_table.dpm_levels[i].value;\n\t\tentries[i].enabled = true;\n\t\tentries[i].vddc = dep_mclk_table->entries[i].vddc;\n\t}\n\n\tsmu_get_voltage_dependency_table_ppt_v1(dep_mclk_table,\n\t\t(struct phm_ppt_v1_clock_voltage_dependency_table *)&(odn_table->vdd_dependency_on_mclk));\n\n\treturn 0;\n}\n\nstatic void smu7_setup_voltage_range_from_vbios(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table;\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tuint32_t min_vddc = 0;\n\tuint32_t max_vddc = 0;\n\n\tif (!table_info)\n\t\treturn;\n\n\tdep_sclk_table = table_info->vdd_dep_on_sclk;\n\n\tatomctrl_get_voltage_range(hwmgr, &max_vddc, &min_vddc);\n\n\tif (min_vddc == 0 || min_vddc > 2000\n\t\t|| min_vddc > dep_sclk_table->entries[0].vddc)\n\t\tmin_vddc = dep_sclk_table->entries[0].vddc;\n\n\tif (max_vddc == 0 || max_vddc > 2000\n\t\t|| max_vddc < dep_sclk_table->entries[dep_sclk_table->count-1].vddc)\n\t\tmax_vddc = dep_sclk_table->entries[dep_sclk_table->count-1].vddc;\n\n\tdata->odn_dpm_table.min_vddc = min_vddc;\n\tdata->odn_dpm_table.max_vddc = max_vddc;\n}\n\nstatic void smu7_check_dpm_table_updated(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_odn_dpm_table *odn_table = &(data->odn_dpm_table);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tuint32_t i;\n\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_table;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *odn_dep_table;\n\n\tif (table_info == NULL)\n\t\treturn;\n\n\tfor (i = 0; i < data->dpm_table.sclk_table.count; i++) {\n\t\tif (odn_table->odn_core_clock_dpm_levels.entries[i].clock !=\n\t\t\t\t\tdata->dpm_table.sclk_table.dpm_levels[i].value) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = 0; i < data->dpm_table.mclk_table.count; i++) {\n\t\tif (odn_table->odn_memory_clock_dpm_levels.entries[i].clock !=\n\t\t\t\t\tdata->dpm_table.mclk_table.dpm_levels[i].value) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdep_table = table_info->vdd_dep_on_mclk;\n\todn_dep_table = (struct phm_ppt_v1_clock_voltage_dependency_table *)&(odn_table->vdd_dependency_on_mclk);\n\n\tfor (i = 0; i < dep_table->count; i++) {\n\t\tif (dep_table->entries[i].vddc != odn_dep_table->entries[i].vddc) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_VDDC | DPMTABLE_OD_UPDATE_MCLK;\n\t\t\treturn;\n\t\t}\n\t}\n\n\tdep_table = table_info->vdd_dep_on_sclk;\n\todn_dep_table = (struct phm_ppt_v1_clock_voltage_dependency_table *)&(odn_table->vdd_dependency_on_sclk);\n\tfor (i = 0; i < dep_table->count; i++) {\n\t\tif (dep_table->entries[i].vddc != odn_dep_table->entries[i].vddc) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_VDDC | DPMTABLE_OD_UPDATE_SCLK;\n\t\t\treturn;\n\t\t}\n\t}\n\tif (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_VDDC) {\n\t\tdata->need_update_smu7_dpm_table &= ~DPMTABLE_OD_UPDATE_VDDC;\n\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK | DPMTABLE_OD_UPDATE_MCLK;\n\t}\n}\n\nstatic int smu7_setup_default_dpm_tables(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tsmu7_reset_dpm_tables(hwmgr);\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\tsmu7_setup_dpm_tables_v1(hwmgr);\n\telse if (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\tsmu7_setup_dpm_tables_v0(hwmgr);\n\n\tsmu7_setup_default_pcie_table(hwmgr);\n\n\t \n\tmemcpy(&(data->golden_dpm_table), &(data->dpm_table),\n\t\t\tsizeof(struct smu7_dpm_table));\n\n\t \n\tif (hwmgr->od_enabled) {\n\t\tif (data->odn_dpm_table.max_vddc) {\n\t\t\tsmu7_check_dpm_table_updated(hwmgr);\n\t\t} else {\n\t\t\tsmu7_setup_voltage_range_from_vbios(hwmgr);\n\t\t\tsmu7_odn_initial_default_setting(hwmgr);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_enable_vrhot_gpio_interrupt(struct pp_hwmgr *hwmgr)\n{\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_RegulatorHot))\n\t\treturn smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_EnableVRHotGPIOInterrupt,\n\t\t\t\tNULL);\n\n\treturn 0;\n}\n\nstatic int smu7_enable_sclk_control(struct pp_hwmgr *hwmgr)\n{\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,\n\t\t\tSCLK_PWRMGT_OFF, 0);\n\treturn 0;\n}\n\nstatic int smu7_enable_ulv(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (data->ulv_supported)\n\t\treturn smum_send_msg_to_smc(hwmgr, PPSMC_MSG_EnableULV, NULL);\n\n\treturn 0;\n}\n\nstatic int smu7_disable_ulv(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (data->ulv_supported)\n\t\treturn smum_send_msg_to_smc(hwmgr, PPSMC_MSG_DisableULV, NULL);\n\n\treturn 0;\n}\n\nstatic int smu7_enable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)\n{\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_SclkDeepSleep)) {\n\t\tif (smum_send_msg_to_smc(hwmgr, PPSMC_MSG_MASTER_DeepSleep_ON, NULL))\n\t\t\tPP_ASSERT_WITH_CODE(false,\n\t\t\t\t\t\"Attempt to enable Master Deep Sleep switch failed!\",\n\t\t\t\t\treturn -EINVAL);\n\t} else {\n\t\tif (smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_MASTER_DeepSleep_OFF,\n\t\t\t\tNULL)) {\n\t\t\tPP_ASSERT_WITH_CODE(false,\n\t\t\t\t\t\"Attempt to disable Master Deep Sleep switch failed!\",\n\t\t\t\t\treturn -EINVAL);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_disable_deep_sleep_master_switch(struct pp_hwmgr *hwmgr)\n{\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_SclkDeepSleep)) {\n\t\tif (smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_MASTER_DeepSleep_OFF,\n\t\t\t\tNULL)) {\n\t\t\tPP_ASSERT_WITH_CODE(false,\n\t\t\t\t\t\"Attempt to disable Master Deep Sleep switch failed!\",\n\t\t\t\t\treturn -EINVAL);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_disable_sclk_vce_handshake(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t soft_register_value = 0;\n\tuint32_t handshake_disables_offset = data->soft_regs_start\n\t\t\t\t+ smum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, HandshakeDisables);\n\n\tsoft_register_value = cgs_read_ind_register(hwmgr->device,\n\t\t\t\tCGS_IND_REG__SMC, handshake_disables_offset);\n\tsoft_register_value |= SMU7_VCE_SCLK_HANDSHAKE_DISABLE;\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\thandshake_disables_offset, soft_register_value);\n\treturn 0;\n}\n\nstatic int smu7_disable_handshake_uvd(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t soft_register_value = 0;\n\tuint32_t handshake_disables_offset = data->soft_regs_start\n\t\t\t\t+ smum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, HandshakeDisables);\n\n\tsoft_register_value = cgs_read_ind_register(hwmgr->device,\n\t\t\t\tCGS_IND_REG__SMC, handshake_disables_offset);\n\tsoft_register_value |= smum_get_mac_definition(hwmgr,\n\t\t\t\t\tSMU_UVD_MCLK_HANDSHAKE_DISABLE);\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\thandshake_disables_offset, soft_register_value);\n\treturn 0;\n}\n\nstatic int smu7_enable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\tif (!data->sclk_dpm_key_disabled) {\n\t\tif (hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t\t    hwmgr->chip_id <= CHIP_VEGAM)\n\t\t\tsmu7_disable_sclk_vce_handshake(hwmgr);\n\n\t\tPP_ASSERT_WITH_CODE(\n\t\t(0 == smum_send_msg_to_smc(hwmgr, PPSMC_MSG_DPM_Enable, NULL)),\n\t\t\"Failed to enable SCLK DPM during DPM Start Function!\",\n\t\treturn -EINVAL);\n\t}\n\n\t \n\tif (0 == data->mclk_dpm_key_disabled) {\n\t\tif (!(hwmgr->feature_mask & PP_UVD_HANDSHAKE_MASK))\n\t\t\tsmu7_disable_handshake_uvd(hwmgr);\n\n\t\tPP_ASSERT_WITH_CODE(\n\t\t\t\t(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_MCLKDPM_Enable,\n\t\t\t\t\t\tNULL)),\n\t\t\t\t\"Failed to enable MCLK DPM during DPM Start Function!\",\n\t\t\t\treturn -EINVAL);\n\n\t\tif ((hwmgr->chip_family == AMDGPU_FAMILY_CI) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS10) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS11) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS12) ||\n\t\t    (hwmgr->chip_id == CHIP_TONGA) ||\n\t\t    (hwmgr->chip_id == CHIP_TOPAZ))\n\t\t\tPHM_WRITE_FIELD(hwmgr->device, MC_SEQ_CNTL_3, CAC_EN, 0x1);\n\n\n\t\tif (hwmgr->chip_family == AMDGPU_FAMILY_CI) {\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d30, 0x5);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d3c, 0x5);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d80, 0x100005);\n\t\t\tudelay(10);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d30, 0x400005);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d3c, 0x400005);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, 0xc0400d80, 0x500005);\n\t\t} else {\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC0_CNTL, 0x5);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC1_CNTL, 0x5);\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_CPL_CNTL, 0x100005);\n\t\t\tudelay(10);\n\t\t\tif (hwmgr->chip_id == CHIP_VEGAM) {\n\t\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC0_CNTL, 0x400009);\n\t\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC1_CNTL, 0x400009);\n\t\t\t} else {\n\t\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC0_CNTL, 0x400005);\n\t\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_MC1_CNTL, 0x400005);\n\t\t\t}\n\t\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixLCAC_CPL_CNTL, 0x500005);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_start_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,\n\t\t\tGLOBAL_PWRMGT_EN, 1);\n\n\t \n\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,\n\t\t\tDYNAMIC_PM_EN, 1);\n\n\t \n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tdata->soft_regs_start +\n\t\t\tsmum_get_offsetof(hwmgr, SMU_SoftRegisters,\n\t\t\t\t\t\tVoltageChangeTimeout), 0x1000);\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__PCIE,\n\t\t\tSWRST_COMMAND_1, RESETLC, 0x0);\n\n\tif (hwmgr->chip_family == AMDGPU_FAMILY_CI)\n\t\tcgs_write_register(hwmgr->device, 0x1488,\n\t\t\t(cgs_read_register(hwmgr->device, 0x1488) & ~0x1));\n\n\tif (smu7_enable_sclk_mclk_dpm(hwmgr)) {\n\t\tpr_err(\"Failed to enable Sclk DPM and Mclk DPM!\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (0 == data->pcie_dpm_key_disabled) {\n\t\tPP_ASSERT_WITH_CODE(\n\t\t\t\t(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_PCIeDPM_Enable,\n\t\t\t\t\t\tNULL)),\n\t\t\t\t\"Failed to enable pcie DPM during DPM Start Function!\",\n\t\t\t\treturn -EINVAL);\n\t} else {\n\t\tPP_ASSERT_WITH_CODE(\n\t\t\t\t(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_PCIeDPM_Disable,\n\t\t\t\t\t\tNULL)),\n\t\t\t\t\"Failed to disable pcie DPM during DPM Start Function!\",\n\t\t\t\treturn -EINVAL);\n\t}\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\tPHM_PlatformCaps_Falcon_QuickTransition)) {\n\t\tPP_ASSERT_WITH_CODE((0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_EnableACDCGPIOInterrupt,\n\t\t\t\tNULL)),\n\t\t\t\t\"Failed to enable AC DC GPIO Interrupt!\",\n\t\t\t\t);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_disable_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\tif (!data->sclk_dpm_key_disabled) {\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to disable SCLK DPM when DPM is disabled\",\n\t\t\t\treturn 0);\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_DPM_Disable, NULL);\n\t}\n\n\t \n\tif (!data->mclk_dpm_key_disabled) {\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to disable MCLK DPM when DPM is disabled\",\n\t\t\t\treturn 0);\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_MCLKDPM_Disable, NULL);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_stop_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,\n\t\t\tGLOBAL_PWRMGT_EN, 0);\n\t \n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, SCLK_PWRMGT_CNTL,\n\t\t\tDYNAMIC_PM_EN, 0);\n\n\t \n\tif (!data->pcie_dpm_key_disabled) {\n\t\tPP_ASSERT_WITH_CODE(\n\t\t\t\t(smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_PCIeDPM_Disable,\n\t\t\t\t\t\tNULL) == 0),\n\t\t\t\t\"Failed to disable pcie DPM during DPM Stop Function!\",\n\t\t\t\treturn -EINVAL);\n\t}\n\n\tsmu7_disable_sclk_mclk_dpm(hwmgr);\n\n\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\"Trying to disable voltage DPM when DPM is disabled\",\n\t\t\treturn 0);\n\n\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_Voltage_Cntl_Disable, NULL);\n\n\treturn 0;\n}\n\nstatic void smu7_set_dpm_event_sources(struct pp_hwmgr *hwmgr, uint32_t sources)\n{\n\tbool protection;\n\tenum DPM_EVENT_SRC src;\n\n\tswitch (sources) {\n\tdefault:\n\t\tpr_err(\"Unknown throttling event sources.\");\n\t\tfallthrough;\n\tcase 0:\n\t\tprotection = false;\n\t\t \n\t\tbreak;\n\tcase (1 << PHM_AutoThrottleSource_Thermal):\n\t\tprotection = true;\n\t\tsrc = DPM_EVENT_SRC_DIGITAL;\n\t\tbreak;\n\tcase (1 << PHM_AutoThrottleSource_External):\n\t\tprotection = true;\n\t\tsrc = DPM_EVENT_SRC_EXTERNAL;\n\t\tbreak;\n\tcase (1 << PHM_AutoThrottleSource_External) |\n\t\t\t(1 << PHM_AutoThrottleSource_Thermal):\n\t\tprotection = true;\n\t\tsrc = DPM_EVENT_SRC_DIGITAL_OR_EXTERNAL;\n\t\tbreak;\n\t}\n\t \n\tif (protection) {\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, CG_THERMAL_CTRL,\n\t\t\t\tDPM_EVENT_SRC, src);\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,\n\t\t\t\tTHERMAL_PROTECTION_DIS,\n\t\t\t\t!phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\t\tPHM_PlatformCaps_ThermalController));\n\t} else\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC, GENERAL_PWRMGT,\n\t\t\t\tTHERMAL_PROTECTION_DIS, 1);\n}\n\nstatic int smu7_enable_auto_throttle_source(struct pp_hwmgr *hwmgr,\n\t\tPHM_AutoThrottleSource source)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (!(data->active_auto_throttle_sources & (1 << source))) {\n\t\tdata->active_auto_throttle_sources |= 1 << source;\n\t\tsmu7_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);\n\t}\n\treturn 0;\n}\n\nstatic int smu7_enable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)\n{\n\treturn smu7_enable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);\n}\n\nstatic int smu7_disable_auto_throttle_source(struct pp_hwmgr *hwmgr,\n\t\tPHM_AutoThrottleSource source)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (data->active_auto_throttle_sources & (1 << source)) {\n\t\tdata->active_auto_throttle_sources &= ~(1 << source);\n\t\tsmu7_set_dpm_event_sources(hwmgr, data->active_auto_throttle_sources);\n\t}\n\treturn 0;\n}\n\nstatic int smu7_disable_thermal_auto_throttle(struct pp_hwmgr *hwmgr)\n{\n\treturn smu7_disable_auto_throttle_source(hwmgr, PHM_AutoThrottleSource_Thermal);\n}\n\nstatic int smu7_pcie_performance_request(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tdata->pcie_performance_request = true;\n\n\treturn 0;\n}\n\nstatic int smu7_program_edc_didt_registers(struct pp_hwmgr *hwmgr,\n\t\t\t\t\t   uint32_t *cac_config_regs,\n\t\t\t\t\t   AtomCtrl_EDCLeakgeTable *edc_leakage_table)\n{\n\tuint32_t data, i = 0;\n\n\twhile (cac_config_regs[i] != 0xFFFFFFFF) {\n\t\tdata = edc_leakage_table->DIDT_REG[i];\n\t\tcgs_write_ind_register(hwmgr->device,\n\t\t\t\t       CGS_IND_REG__DIDT,\n\t\t\t\t       cac_config_regs[i],\n\t\t\t\t       data);\n\t\ti++;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_populate_edc_leakage_registers(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tint ret = 0;\n\n\tif (!data->disable_edc_leakage_controller &&\n\t    data->edc_hilo_leakage_offset_from_vbios.usEdcDidtLoDpm7TableOffset &&\n\t    data->edc_hilo_leakage_offset_from_vbios.usEdcDidtHiDpm7TableOffset) {\n\t\tret = smu7_program_edc_didt_registers(hwmgr,\n\t\t\t\t\t\t      DIDTEDCConfig_P12,\n\t\t\t\t\t\t      &data->edc_leakage_table);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t   (PPSMC_Msg)PPSMC_MSG_EnableEDCController,\n\t\t\t\t\t   NULL);\n\t} else {\n\t\tret = smum_send_msg_to_smc(hwmgr,\n\t\t\t\t\t   (PPSMC_Msg)PPSMC_MSG_DisableEDCController,\n\t\t\t\t\t   NULL);\n\t}\n\n\treturn ret;\n}\n\nstatic void smu7_populate_umdpstate_clocks(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_dpm_table *golden_dpm_table = &data->golden_dpm_table;\n\tint32_t tmp_sclk, count, percentage;\n\n\tif (golden_dpm_table->mclk_table.count == 1) {\n\t\tpercentage = 70;\n\t\thwmgr->pstate_mclk = golden_dpm_table->mclk_table.dpm_levels[0].value;\n\t} else {\n\t\tpercentage = 100 * golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count - 1].value /\n\t\t\t\tgolden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 1].value;\n\t\thwmgr->pstate_mclk = golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 2].value;\n\t}\n\n\ttmp_sclk = hwmgr->pstate_mclk * percentage / 100;\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tstruct phm_clock_voltage_dependency_table *vddc_dependency_on_sclk =\n\t\t\thwmgr->dyn_state.vddc_dependency_on_sclk;\n\n\t\tfor (count = vddc_dependency_on_sclk->count - 1; count >= 0; count--) {\n\t\t\tif (tmp_sclk >= vddc_dependency_on_sclk->entries[count].clk) {\n\t\t\t\thwmgr->pstate_sclk = vddc_dependency_on_sclk->entries[count].clk;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (count < 0)\n\t\t\thwmgr->pstate_sclk = vddc_dependency_on_sclk->entries[0].clk;\n\n\t\thwmgr->pstate_sclk_peak =\n\t\t\tvddc_dependency_on_sclk->entries[vddc_dependency_on_sclk->count - 1].clk;\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\t\tstruct phm_ppt_v1_clock_voltage_dependency_table *vdd_dep_on_sclk =\n\t\t\ttable_info->vdd_dep_on_sclk;\n\n\t\tfor (count = vdd_dep_on_sclk->count - 1; count >= 0; count--) {\n\t\t\tif (tmp_sclk >= vdd_dep_on_sclk->entries[count].clk) {\n\t\t\t\thwmgr->pstate_sclk = vdd_dep_on_sclk->entries[count].clk;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (count < 0)\n\t\t\thwmgr->pstate_sclk = vdd_dep_on_sclk->entries[0].clk;\n\n\t\thwmgr->pstate_sclk_peak =\n\t\t\tvdd_dep_on_sclk->entries[vdd_dep_on_sclk->count - 1].clk;\n\t}\n\n\thwmgr->pstate_mclk_peak =\n\t\tgolden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 1].value;\n\n\t \n\thwmgr->pstate_sclk /= 100;\n\thwmgr->pstate_mclk /= 100;\n\thwmgr->pstate_sclk_peak /= 100;\n\thwmgr->pstate_mclk_peak /= 100;\n}\n\nstatic int smu7_enable_dpm_tasks(struct pp_hwmgr *hwmgr)\n{\n\tint tmp_result = 0;\n\tint result = 0;\n\n\tif (smu7_voltage_control(hwmgr)) {\n\t\ttmp_result = smu7_enable_voltage_control(hwmgr);\n\t\tPP_ASSERT_WITH_CODE(tmp_result == 0,\n\t\t\t\t\"Failed to enable voltage control!\",\n\t\t\t\tresult = tmp_result);\n\n\t\ttmp_result = smu7_construct_voltage_tables(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to construct voltage tables!\",\n\t\t\t\tresult = tmp_result);\n\t}\n\tsmum_initialize_mc_reg_table(hwmgr);\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_EngineSpreadSpectrumSupport))\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\tGENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 1);\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_ThermalController))\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\tGENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 0);\n\n\ttmp_result = smu7_program_static_screen_threshold_parameters(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to program static screen threshold parameters!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smu7_enable_display_gap(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable display gap!\", result = tmp_result);\n\n\ttmp_result = smu7_program_voting_clients(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to program voting clients!\", result = tmp_result);\n\n\ttmp_result = smum_process_firmware_header(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to process firmware header!\", result = tmp_result);\n\n\tif (hwmgr->chip_id != CHIP_VEGAM) {\n\t\ttmp_result = smu7_initial_switch_from_arbf0_to_f1(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to initialize switch from ArbF0 to F1!\",\n\t\t\t\tresult = tmp_result);\n\t}\n\n\tresult = smu7_setup_default_dpm_tables(hwmgr);\n\tPP_ASSERT_WITH_CODE(0 == result,\n\t\t\t\"Failed to setup default DPM tables!\", return result);\n\n\ttmp_result = smum_init_smc_table(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to initialize SMC table!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_vrhot_gpio_interrupt(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable VR hot GPIO interrupt!\", result = tmp_result);\n\n\tif (hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t    hwmgr->chip_id <= CHIP_VEGAM) {\n\t\ttmp_result = smu7_notify_has_display(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to enable display setting!\", result = tmp_result);\n\t} else {\n\t\tsmum_send_msg_to_smc(hwmgr, (PPSMC_Msg)PPSMC_NoDisplay, NULL);\n\t}\n\n\tif (hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t    hwmgr->chip_id <= CHIP_VEGAM) {\n\t\ttmp_result = smu7_populate_edc_leakage_registers(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to populate edc leakage registers!\", result = tmp_result);\n\t}\n\n\ttmp_result = smu7_enable_sclk_control(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable SCLK control!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_smc_voltage_controller(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable voltage control!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_ulv(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable ULV!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_deep_sleep_master_switch(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable deep sleep master switch!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_didt_config(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to enable deep sleep master switch!\", result = tmp_result);\n\n\ttmp_result = smu7_start_dpm(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to start DPM!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_smc_cac(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable SMC CAC!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_power_containment(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable power containment!\", result = tmp_result);\n\n\ttmp_result = smu7_power_control_set_level(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to power control set level!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_thermal_auto_throttle(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable thermal auto throttle!\", result = tmp_result);\n\n\ttmp_result = smu7_pcie_performance_request(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"pcie performance request failed!\", result = tmp_result);\n\n\tsmu7_populate_umdpstate_clocks(hwmgr);\n\n\treturn 0;\n}\n\nstatic int smu7_avfs_control(struct pp_hwmgr *hwmgr, bool enable)\n{\n\tif (!hwmgr->avfs_supported)\n\t\treturn 0;\n\n\tif (enable) {\n\t\tif (!PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\tCGS_IND_REG__SMC, FEATURE_STATUS, AVS_ON)) {\n\t\t\tPP_ASSERT_WITH_CODE(!smum_send_msg_to_smc(\n\t\t\t\t\thwmgr, PPSMC_MSG_EnableAvfs, NULL),\n\t\t\t\t\t\"Failed to enable AVFS!\",\n\t\t\t\t\treturn -EINVAL);\n\t\t}\n\t} else if (PHM_READ_VFPF_INDIRECT_FIELD(hwmgr->device,\n\t\t\tCGS_IND_REG__SMC, FEATURE_STATUS, AVS_ON)) {\n\t\tPP_ASSERT_WITH_CODE(!smum_send_msg_to_smc(\n\t\t\t\thwmgr, PPSMC_MSG_DisableAvfs, NULL),\n\t\t\t\t\"Failed to disable AVFS!\",\n\t\t\t\treturn -EINVAL);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_update_avfs(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (!hwmgr->avfs_supported)\n\t\treturn 0;\n\n\tif (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_VDDC) {\n\t\tsmu7_avfs_control(hwmgr, false);\n\t} else if (data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {\n\t\tsmu7_avfs_control(hwmgr, false);\n\t\tsmu7_avfs_control(hwmgr, true);\n\t} else {\n\t\tsmu7_avfs_control(hwmgr, true);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_disable_dpm_tasks(struct pp_hwmgr *hwmgr)\n{\n\tint tmp_result, result = 0;\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_ThermalController))\n\t\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\tGENERAL_PWRMGT, THERMAL_PROTECTION_DIS, 1);\n\n\ttmp_result = smu7_disable_power_containment(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable power containment!\", result = tmp_result);\n\n\ttmp_result = smu7_disable_smc_cac(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable SMC CAC!\", result = tmp_result);\n\n\ttmp_result = smu7_disable_didt_config(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable DIDT!\", result = tmp_result);\n\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tCG_SPLL_SPREAD_SPECTRUM, SSEN, 0);\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tGENERAL_PWRMGT, DYN_SPREAD_SPECTRUM_EN, 0);\n\n\ttmp_result = smu7_disable_thermal_auto_throttle(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable thermal auto throttle!\", result = tmp_result);\n\n\ttmp_result = smu7_avfs_control(hwmgr, false);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable AVFS!\", result = tmp_result);\n\n\ttmp_result = smu7_stop_dpm(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to stop DPM!\", result = tmp_result);\n\n\ttmp_result = smu7_disable_deep_sleep_master_switch(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable deep sleep master switch!\", result = tmp_result);\n\n\ttmp_result = smu7_disable_ulv(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to disable ULV!\", result = tmp_result);\n\n\ttmp_result = smu7_clear_voting_clients(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to clear voting clients!\", result = tmp_result);\n\n\ttmp_result = smu7_reset_to_default(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to reset to default!\", result = tmp_result);\n\n\ttmp_result = smum_stop_smc(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to stop smc!\", result = tmp_result);\n\n\ttmp_result = smu7_force_switch_to_arbf0(hwmgr);\n\tPP_ASSERT_WITH_CODE((tmp_result == 0),\n\t\t\t\"Failed to force to switch arbf0!\", result = tmp_result);\n\n\treturn result;\n}\n\nstatic void smu7_init_dpm_defaults(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\tuint8_t tmp1, tmp2;\n\tuint16_t tmp3 = 0;\n\n\tdata->dll_default_on = false;\n\tdata->mclk_dpm0_activity_target = 0xa;\n\tdata->vddc_vddgfx_delta = 300;\n\tdata->static_screen_threshold = SMU7_STATICSCREENTHRESHOLD_DFLT;\n\tdata->static_screen_threshold_unit = SMU7_STATICSCREENTHRESHOLDUNIT_DFLT;\n\tdata->voting_rights_clients[0] = SMU7_VOTINGRIGHTSCLIENTS_DFLT0;\n\tdata->voting_rights_clients[1] = SMU7_VOTINGRIGHTSCLIENTS_DFLT1;\n\tdata->voting_rights_clients[2] = SMU7_VOTINGRIGHTSCLIENTS_DFLT2;\n\tdata->voting_rights_clients[3] = SMU7_VOTINGRIGHTSCLIENTS_DFLT3;\n\tdata->voting_rights_clients[4] = SMU7_VOTINGRIGHTSCLIENTS_DFLT4;\n\tdata->voting_rights_clients[5] = SMU7_VOTINGRIGHTSCLIENTS_DFLT5;\n\tdata->voting_rights_clients[6] = SMU7_VOTINGRIGHTSCLIENTS_DFLT6;\n\tdata->voting_rights_clients[7] = SMU7_VOTINGRIGHTSCLIENTS_DFLT7;\n\n\tdata->mclk_dpm_key_disabled = hwmgr->feature_mask & PP_MCLK_DPM_MASK ? false : true;\n\tdata->sclk_dpm_key_disabled = hwmgr->feature_mask & PP_SCLK_DPM_MASK ? false : true;\n\tdata->pcie_dpm_key_disabled = !(hwmgr->feature_mask & PP_PCIE_DPM_MASK);\n\t \n\tdata->voltage_control = SMU7_VOLTAGE_CONTROL_NONE;\n\tdata->vddci_control = SMU7_VOLTAGE_CONTROL_NONE;\n\tdata->mvdd_control = SMU7_VOLTAGE_CONTROL_NONE;\n\tdata->enable_tdc_limit_feature = true;\n\tdata->enable_pkg_pwr_tracking_feature = true;\n\tdata->force_pcie_gen = PP_PCIEGenInvalid;\n\tdata->ulv_supported = hwmgr->feature_mask & PP_ULV_MASK ? true : false;\n\tdata->current_profile_setting.bupdate_sclk = 1;\n\tdata->current_profile_setting.sclk_up_hyst = 0;\n\tdata->current_profile_setting.sclk_down_hyst = 100;\n\tdata->current_profile_setting.sclk_activity = SMU7_SCLK_TARGETACTIVITY_DFLT;\n\tdata->current_profile_setting.bupdate_mclk = 1;\n\tif (hwmgr->chip_id >= CHIP_POLARIS10) {\n\t\tif (adev->gmc.vram_width == 256) {\n\t\t\tdata->current_profile_setting.mclk_up_hyst = 10;\n\t\t\tdata->current_profile_setting.mclk_down_hyst = 60;\n\t\t\tdata->current_profile_setting.mclk_activity = 25;\n\t\t} else if (adev->gmc.vram_width == 128) {\n\t\t\tdata->current_profile_setting.mclk_up_hyst = 5;\n\t\t\tdata->current_profile_setting.mclk_down_hyst = 16;\n\t\t\tdata->current_profile_setting.mclk_activity = 20;\n\t\t} else if (adev->gmc.vram_width == 64) {\n\t\t\tdata->current_profile_setting.mclk_up_hyst = 3;\n\t\t\tdata->current_profile_setting.mclk_down_hyst = 16;\n\t\t\tdata->current_profile_setting.mclk_activity = 20;\n\t\t}\n\t} else {\n\t\tdata->current_profile_setting.mclk_up_hyst = 0;\n\t\tdata->current_profile_setting.mclk_down_hyst = 100;\n\t\tdata->current_profile_setting.mclk_activity = SMU7_MCLK_TARGETACTIVITY_DFLT;\n\t}\n\thwmgr->workload_mask = 1 << hwmgr->workload_prority[PP_SMC_POWER_PROFILE_FULLSCREEN3D];\n\thwmgr->power_profile_mode = PP_SMC_POWER_PROFILE_FULLSCREEN3D;\n\thwmgr->default_power_profile_mode = PP_SMC_POWER_PROFILE_FULLSCREEN3D;\n\n\tif (hwmgr->chip_id  == CHIP_HAWAII) {\n\t\tdata->thermal_temp_setting.temperature_low = 94500;\n\t\tdata->thermal_temp_setting.temperature_high = 95000;\n\t\tdata->thermal_temp_setting.temperature_shutdown = 104000;\n\t} else {\n\t\tdata->thermal_temp_setting.temperature_low = 99500;\n\t\tdata->thermal_temp_setting.temperature_high = 100000;\n\t\tdata->thermal_temp_setting.temperature_shutdown = 104000;\n\t}\n\n\tdata->fast_watermark_threshold = 100;\n\tif (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\tVOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_SVID2))\n\t\tdata->voltage_control = SMU7_VOLTAGE_CONTROL_BY_SVID2;\n\telse if (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\tVOLTAGE_TYPE_VDDC, VOLTAGE_OBJ_GPIO_LUT))\n\t\tdata->voltage_control = SMU7_VOLTAGE_CONTROL_BY_GPIO;\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_ControlVDDGFX)) {\n\t\tif (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\tVOLTAGE_TYPE_VDDGFX, VOLTAGE_OBJ_SVID2)) {\n\t\t\tdata->vdd_gfx_control = SMU7_VOLTAGE_CONTROL_BY_SVID2;\n\t\t}\n\t}\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_EnableMVDDControl)) {\n\t\tif (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_GPIO_LUT))\n\t\t\tdata->mvdd_control = SMU7_VOLTAGE_CONTROL_BY_GPIO;\n\t\telse if (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_MVDDC, VOLTAGE_OBJ_SVID2))\n\t\t\tdata->mvdd_control = SMU7_VOLTAGE_CONTROL_BY_SVID2;\n\t}\n\n\tif (SMU7_VOLTAGE_CONTROL_NONE == data->vdd_gfx_control)\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_ControlVDDGFX);\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_ControlVDDCI)) {\n\t\tif (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_GPIO_LUT))\n\t\t\tdata->vddci_control = SMU7_VOLTAGE_CONTROL_BY_GPIO;\n\t\telse if (atomctrl_is_voltage_controlled_by_gpio_v3(hwmgr,\n\t\t\t\tVOLTAGE_TYPE_VDDCI, VOLTAGE_OBJ_SVID2))\n\t\t\tdata->vddci_control = SMU7_VOLTAGE_CONTROL_BY_SVID2;\n\t}\n\n\tif (data->mvdd_control == SMU7_VOLTAGE_CONTROL_NONE)\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\tPHM_PlatformCaps_EnableMVDDControl);\n\n\tif (data->vddci_control == SMU7_VOLTAGE_CONTROL_NONE)\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\tPHM_PlatformCaps_ControlVDDCI);\n\n\tdata->vddc_phase_shed_control = 1;\n\tif ((hwmgr->chip_id == CHIP_POLARIS12) ||\n\t    ASICID_IS_P20(adev->pdev->device, adev->pdev->revision) ||\n\t    ASICID_IS_P21(adev->pdev->device, adev->pdev->revision) ||\n\t    ASICID_IS_P30(adev->pdev->device, adev->pdev->revision) ||\n\t    ASICID_IS_P31(adev->pdev->device, adev->pdev->revision)) {\n\t\tif (data->voltage_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\t\tatomctrl_get_svi2_info(hwmgr, VOLTAGE_TYPE_VDDC, &tmp1, &tmp2,\n\t\t\t\t\t\t\t&tmp3);\n\t\t\ttmp3 = (tmp3 >> 5) & 0x3;\n\t\t\tdata->vddc_phase_shed_control = ((tmp3 << 1) | (tmp3 >> 1)) & 0x3;\n\t\t}\n\t} else if (hwmgr->chip_family == AMDGPU_FAMILY_CI) {\n\t\tdata->vddc_phase_shed_control = 1;\n\t}\n\n\tif ((hwmgr->pp_table_version != PP_TABLE_V0) && (hwmgr->feature_mask & PP_CLOCK_STRETCH_MASK)\n\t\t&& (table_info->cac_dtp_table->usClockStretchAmount != 0))\n\t\tphm_cap_set(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\tPHM_PlatformCaps_ClockStretcher);\n\n\tdata->pcie_gen_performance.max = PP_PCIEGen1;\n\tdata->pcie_gen_performance.min = PP_PCIEGen3;\n\tdata->pcie_gen_power_saving.max = PP_PCIEGen1;\n\tdata->pcie_gen_power_saving.min = PP_PCIEGen3;\n\tdata->pcie_lane_performance.max = 0;\n\tdata->pcie_lane_performance.min = 16;\n\tdata->pcie_lane_power_saving.max = 0;\n\tdata->pcie_lane_power_saving.min = 16;\n\n\n\tif (adev->pg_flags & AMD_PG_SUPPORT_UVD)\n\t\tphm_cap_set(hwmgr->platform_descriptor.platformCaps,\n\t\t\t      PHM_PlatformCaps_UVDPowerGating);\n\tif (adev->pg_flags & AMD_PG_SUPPORT_VCE)\n\t\tphm_cap_set(hwmgr->platform_descriptor.platformCaps,\n\t\t\t      PHM_PlatformCaps_VCEPowerGating);\n\n\tdata->disable_edc_leakage_controller = true;\n\tif (((adev->asic_type == CHIP_POLARIS10) && hwmgr->is_kicker) ||\n\t    ((adev->asic_type == CHIP_POLARIS11) && hwmgr->is_kicker) ||\n\t    (adev->asic_type == CHIP_POLARIS12) ||\n\t    (adev->asic_type == CHIP_VEGAM))\n\t\tdata->disable_edc_leakage_controller = false;\n\n\tif (!atomctrl_is_asic_internal_ss_supported(hwmgr)) {\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_MemorySpreadSpectrumSupport);\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_EngineSpreadSpectrumSupport);\n\t}\n\n\tif ((adev->pdev->device == 0x699F) &&\n\t    (adev->pdev->revision == 0xCF)) {\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\tPHM_PlatformCaps_PowerContainment);\n\t\tdata->enable_tdc_limit_feature = false;\n\t\tdata->enable_pkg_pwr_tracking_feature = false;\n\t\tdata->disable_edc_leakage_controller = true;\n\t\tphm_cap_unset(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\tPHM_PlatformCaps_ClockStretcher);\n\t}\n}\n\nstatic int smu7_calculate_ro_range(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\tuint32_t asicrev1, evv_revision, max = 0, min = 0;\n\n\tatomctrl_read_efuse(hwmgr, STRAP_EVV_REVISION_LSB, STRAP_EVV_REVISION_MSB,\n\t\t\t&evv_revision);\n\n\tatomctrl_read_efuse(hwmgr, 568, 579, &asicrev1);\n\n\tif (ASICID_IS_P20(adev->pdev->device, adev->pdev->revision) ||\n\t    ASICID_IS_P30(adev->pdev->device, adev->pdev->revision)) {\n\t\tmin = 1200;\n\t\tmax = 2500;\n\t} else if (ASICID_IS_P21(adev->pdev->device, adev->pdev->revision) ||\n\t\t   ASICID_IS_P31(adev->pdev->device, adev->pdev->revision)) {\n\t\tmin = 900;\n\t\tmax = 2100;\n\t} else if (hwmgr->chip_id == CHIP_POLARIS10) {\n\t\tif (adev->pdev->subsystem_vendor == 0x106B) {\n\t\t\tmin = 1000;\n\t\t\tmax = 2300;\n\t\t} else {\n\t\t\tif (evv_revision == 0) {\n\t\t\t\tmin = 1000;\n\t\t\t\tmax = 2300;\n\t\t\t} else if (evv_revision == 1) {\n\t\t\t\tif (asicrev1 == 326) {\n\t\t\t\t\tmin = 1200;\n\t\t\t\t\tmax = 2500;\n\t\t\t\t\t \n\t\t\t\t} else {\n\t\t\t\t\tmin = 1200;\n\t\t\t\t\tmax = 2000;\n\t\t\t\t}\n\t\t\t} else if (evv_revision == 2) {\n\t\t\t\tmin = 1200;\n\t\t\t\tmax = 2500;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tmin = 1100;\n\t\tmax = 2100;\n\t}\n\n\tdata->ro_range_minimum = min;\n\tdata->ro_range_maximum = max;\n\n\t \n\n\treturn 0;\n}\n\n \nstatic int smu7_get_evv_voltages(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint16_t vv_id;\n\tuint16_t vddc = 0;\n\tuint16_t vddgfx = 0;\n\tuint16_t i, j;\n\tuint32_t sclk = 0;\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *sclk_table = NULL;\n\n\tif (hwmgr->chip_id == CHIP_POLARIS10 ||\n\t    hwmgr->chip_id == CHIP_POLARIS11 ||\n\t    hwmgr->chip_id == CHIP_POLARIS12)\n\t\tsmu7_calculate_ro_range(hwmgr);\n\n\tfor (i = 0; i < SMU7_MAX_LEAKAGE_COUNT; i++) {\n\t\tvv_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;\n\n\t\tif (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\t\tif ((hwmgr->pp_table_version == PP_TABLE_V1)\n\t\t\t    && !phm_get_sclk_for_voltage_evv(hwmgr,\n\t\t\t\t\t\ttable_info->vddgfx_lookup_table, vv_id, &sclk)) {\n\t\t\t\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\t\t\tPHM_PlatformCaps_ClockStretcher)) {\n\t\t\t\t\tsclk_table = table_info->vdd_dep_on_sclk;\n\n\t\t\t\t\tfor (j = 1; j < sclk_table->count; j++) {\n\t\t\t\t\t\tif (sclk_table->entries[j].clk == sclk &&\n\t\t\t\t\t\t\t\tsclk_table->entries[j].cks_enable == 0) {\n\t\t\t\t\t\t\tsclk += 5000;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (0 == atomctrl_get_voltage_evv_on_sclk\n\t\t\t\t    (hwmgr, VOLTAGE_TYPE_VDDGFX, sclk,\n\t\t\t\t     vv_id, &vddgfx)) {\n\t\t\t\t\t \n\t\t\t\t\tPP_ASSERT_WITH_CODE((vddgfx < 2000 && vddgfx != 0), \"Invalid VDDGFX value!\", return -EINVAL);\n\n\t\t\t\t\t \n\t\t\t\t\tif (vddgfx != 0 && vddgfx != vv_id) {\n\t\t\t\t\t\tdata->vddcgfx_leakage.actual_voltage[data->vddcgfx_leakage.count] = vddgfx;\n\t\t\t\t\t\tdata->vddcgfx_leakage.leakage_id[data->vddcgfx_leakage.count] = vv_id;\n\t\t\t\t\t\tdata->vddcgfx_leakage.count++;\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tpr_info(\"Error retrieving EVV voltage value!\\n\");\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tif ((hwmgr->pp_table_version == PP_TABLE_V0)\n\t\t\t\t|| !phm_get_sclk_for_voltage_evv(hwmgr,\n\t\t\t\t\ttable_info->vddc_lookup_table, vv_id, &sclk)) {\n\t\t\t\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\t\tPHM_PlatformCaps_ClockStretcher)) {\n\t\t\t\t\tif (table_info == NULL)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\tsclk_table = table_info->vdd_dep_on_sclk;\n\n\t\t\t\t\tfor (j = 1; j < sclk_table->count; j++) {\n\t\t\t\t\t\tif (sclk_table->entries[j].clk == sclk &&\n\t\t\t\t\t\t\t\tsclk_table->entries[j].cks_enable == 0) {\n\t\t\t\t\t\t\tsclk += 5000;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (phm_get_voltage_evv_on_sclk(hwmgr,\n\t\t\t\t\t\t\tVOLTAGE_TYPE_VDDC,\n\t\t\t\t\t\t\tsclk, vv_id, &vddc) == 0) {\n\t\t\t\t\tif (vddc >= 2000 || vddc == 0)\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t} else {\n\t\t\t\t\tpr_debug(\"failed to retrieving EVV voltage!\\n\");\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tif (vddc != 0 && vddc != vv_id) {\n\t\t\t\t\tdata->vddc_leakage.actual_voltage[data->vddc_leakage.count] = (uint16_t)(vddc);\n\t\t\t\t\tdata->vddc_leakage.leakage_id[data->vddc_leakage.count] = vv_id;\n\t\t\t\t\tdata->vddc_leakage.count++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void smu7_patch_ppt_v1_with_vdd_leakage(struct pp_hwmgr *hwmgr,\n\t\tuint16_t *voltage, struct smu7_leakage_voltage *leakage_table)\n{\n\tuint32_t index;\n\n\t \n\tfor (index = 0; index < leakage_table->count; index++) {\n\t\t \n\t\t \n\t\tif (leakage_table->leakage_id[index] == *voltage) {\n\t\t\t*voltage = leakage_table->actual_voltage[index];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (*voltage > ATOM_VIRTUAL_VOLTAGE_ID0)\n\t\tpr_info(\"Voltage value looks like a Leakage ID but it's not patched\\n\");\n}\n\n \nstatic int smu7_patch_lookup_table_with_leakage(struct pp_hwmgr *hwmgr,\n\t\tphm_ppt_v1_voltage_lookup_table *lookup_table,\n\t\tstruct smu7_leakage_voltage *leakage_table)\n{\n\tuint32_t i;\n\n\tfor (i = 0; i < lookup_table->count; i++)\n\t\tsmu7_patch_ppt_v1_with_vdd_leakage(hwmgr,\n\t\t\t\t&lookup_table->entries[i].us_vdd, leakage_table);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_clock_voltage_limits_with_vddc_leakage(\n\t\tstruct pp_hwmgr *hwmgr, struct smu7_leakage_voltage *leakage_table,\n\t\tuint16_t *vddc)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tsmu7_patch_ppt_v1_with_vdd_leakage(hwmgr, (uint16_t *)vddc, leakage_table);\n\thwmgr->dyn_state.max_clock_voltage_on_dc.vddc =\n\t\t\ttable_info->max_clock_voltage_on_dc.vddc;\n\treturn 0;\n}\n\nstatic int smu7_patch_voltage_dependency_tables_with_lookup_table(\n\t\tstruct pp_hwmgr *hwmgr)\n{\n\tuint8_t entry_id;\n\tuint8_t voltage_id;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *sclk_table =\n\t\t\ttable_info->vdd_dep_on_sclk;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *mclk_table =\n\t\t\ttable_info->vdd_dep_on_mclk;\n\tstruct phm_ppt_v1_mm_clock_voltage_dependency_table *mm_table =\n\t\t\ttable_info->mm_dep_table;\n\n\tif (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\tfor (entry_id = 0; entry_id < sclk_table->count; ++entry_id) {\n\t\t\tvoltage_id = sclk_table->entries[entry_id].vddInd;\n\t\t\tsclk_table->entries[entry_id].vddgfx =\n\t\t\t\ttable_info->vddgfx_lookup_table->entries[voltage_id].us_vdd;\n\t\t}\n\t} else {\n\t\tfor (entry_id = 0; entry_id < sclk_table->count; ++entry_id) {\n\t\t\tvoltage_id = sclk_table->entries[entry_id].vddInd;\n\t\t\tsclk_table->entries[entry_id].vddc =\n\t\t\t\ttable_info->vddc_lookup_table->entries[voltage_id].us_vdd;\n\t\t}\n\t}\n\n\tfor (entry_id = 0; entry_id < mclk_table->count; ++entry_id) {\n\t\tvoltage_id = mclk_table->entries[entry_id].vddInd;\n\t\tmclk_table->entries[entry_id].vddc =\n\t\t\ttable_info->vddc_lookup_table->entries[voltage_id].us_vdd;\n\t}\n\n\tfor (entry_id = 0; entry_id < mm_table->count; ++entry_id) {\n\t\tvoltage_id = mm_table->entries[entry_id].vddcInd;\n\t\tmm_table->entries[entry_id].vddc =\n\t\t\ttable_info->vddc_lookup_table->entries[voltage_id].us_vdd;\n\t}\n\n\treturn 0;\n\n}\n\nstatic int phm_add_voltage(struct pp_hwmgr *hwmgr,\n\t\t\tphm_ppt_v1_voltage_lookup_table *look_up_table,\n\t\t\tphm_ppt_v1_voltage_lookup_record *record)\n{\n\tuint32_t i;\n\n\tPP_ASSERT_WITH_CODE((NULL != look_up_table),\n\t\t\"Lookup Table empty.\", return -EINVAL);\n\tPP_ASSERT_WITH_CODE((0 != look_up_table->count),\n\t\t\"Lookup Table empty.\", return -EINVAL);\n\n\ti = smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_VDDGFX);\n\tPP_ASSERT_WITH_CODE((i >= look_up_table->count),\n\t\t\"Lookup Table is full.\", return -EINVAL);\n\n\t \n\tfor (i = 0; i < look_up_table->count; i++) {\n\t\tif (look_up_table->entries[i].us_vdd == record->us_vdd) {\n\t\t\tif (look_up_table->entries[i].us_calculated == 1)\n\t\t\t\treturn 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tlook_up_table->entries[i].us_calculated = 1;\n\tlook_up_table->entries[i].us_vdd = record->us_vdd;\n\tlook_up_table->entries[i].us_cac_low = record->us_cac_low;\n\tlook_up_table->entries[i].us_cac_mid = record->us_cac_mid;\n\tlook_up_table->entries[i].us_cac_high = record->us_cac_high;\n\t \n\tif (i == look_up_table->count)\n\t\tlook_up_table->count++;\n\n\treturn 0;\n}\n\n\nstatic int smu7_calc_voltage_dependency_tables(struct pp_hwmgr *hwmgr)\n{\n\tuint8_t entry_id;\n\tstruct phm_ppt_v1_voltage_lookup_record v_record;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\tphm_ppt_v1_clock_voltage_dependency_table *sclk_table = pptable_info->vdd_dep_on_sclk;\n\tphm_ppt_v1_clock_voltage_dependency_table *mclk_table = pptable_info->vdd_dep_on_mclk;\n\n\tif (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\tfor (entry_id = 0; entry_id < sclk_table->count; ++entry_id) {\n\t\t\tif (sclk_table->entries[entry_id].vdd_offset & (1 << 15))\n\t\t\t\tv_record.us_vdd = sclk_table->entries[entry_id].vddgfx +\n\t\t\t\t\tsclk_table->entries[entry_id].vdd_offset - 0xFFFF;\n\t\t\telse\n\t\t\t\tv_record.us_vdd = sclk_table->entries[entry_id].vddgfx +\n\t\t\t\t\tsclk_table->entries[entry_id].vdd_offset;\n\n\t\t\tsclk_table->entries[entry_id].vddc =\n\t\t\t\tv_record.us_cac_low = v_record.us_cac_mid =\n\t\t\t\tv_record.us_cac_high = v_record.us_vdd;\n\n\t\t\tphm_add_voltage(hwmgr, pptable_info->vddc_lookup_table, &v_record);\n\t\t}\n\n\t\tfor (entry_id = 0; entry_id < mclk_table->count; ++entry_id) {\n\t\t\tif (mclk_table->entries[entry_id].vdd_offset & (1 << 15))\n\t\t\t\tv_record.us_vdd = mclk_table->entries[entry_id].vddc +\n\t\t\t\t\tmclk_table->entries[entry_id].vdd_offset - 0xFFFF;\n\t\t\telse\n\t\t\t\tv_record.us_vdd = mclk_table->entries[entry_id].vddc +\n\t\t\t\t\tmclk_table->entries[entry_id].vdd_offset;\n\n\t\t\tmclk_table->entries[entry_id].vddgfx = v_record.us_cac_low =\n\t\t\t\tv_record.us_cac_mid = v_record.us_cac_high = v_record.us_vdd;\n\t\t\tphm_add_voltage(hwmgr, pptable_info->vddgfx_lookup_table, &v_record);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_calc_mm_voltage_dependency_table(struct pp_hwmgr *hwmgr)\n{\n\tuint8_t entry_id;\n\tstruct phm_ppt_v1_voltage_lookup_record v_record;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *pptable_info = (struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tphm_ppt_v1_mm_clock_voltage_dependency_table *mm_table = pptable_info->mm_dep_table;\n\n\tif (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\tfor (entry_id = 0; entry_id < mm_table->count; entry_id++) {\n\t\t\tif (mm_table->entries[entry_id].vddgfx_offset & (1 << 15))\n\t\t\t\tv_record.us_vdd = mm_table->entries[entry_id].vddc +\n\t\t\t\t\tmm_table->entries[entry_id].vddgfx_offset - 0xFFFF;\n\t\t\telse\n\t\t\t\tv_record.us_vdd = mm_table->entries[entry_id].vddc +\n\t\t\t\t\tmm_table->entries[entry_id].vddgfx_offset;\n\n\t\t\t \n\t\t\tmm_table->entries[entry_id].vddgfx = v_record.us_cac_low =\n\t\t\t\tv_record.us_cac_mid = v_record.us_cac_high = v_record.us_vdd;\n\t\t\tphm_add_voltage(hwmgr, pptable_info->vddgfx_lookup_table, &v_record);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_sort_lookup_table(struct pp_hwmgr *hwmgr,\n\t\tstruct phm_ppt_v1_voltage_lookup_table *lookup_table)\n{\n\tuint32_t table_size, i, j;\n\ttable_size = lookup_table->count;\n\n\tPP_ASSERT_WITH_CODE(0 != lookup_table->count,\n\t\t\"Lookup table is empty\", return -EINVAL);\n\n\t \n\tfor (i = 0; i < table_size - 1; i++) {\n\t\tfor (j = i + 1; j > 0; j--) {\n\t\t\tif (lookup_table->entries[j].us_vdd <\n\t\t\t\t\tlookup_table->entries[j - 1].us_vdd) {\n\t\t\t\tswap(lookup_table->entries[j - 1],\n\t\t\t\t     lookup_table->entries[j]);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_complete_dependency_tables(struct pp_hwmgr *hwmgr)\n{\n\tint result = 0;\n\tint tmp_result;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\tif (data->vdd_gfx_control == SMU7_VOLTAGE_CONTROL_BY_SVID2) {\n\t\ttmp_result = smu7_patch_lookup_table_with_leakage(hwmgr,\n\t\t\ttable_info->vddgfx_lookup_table, &(data->vddcgfx_leakage));\n\t\tif (tmp_result != 0)\n\t\t\tresult = tmp_result;\n\n\t\tsmu7_patch_ppt_v1_with_vdd_leakage(hwmgr,\n\t\t\t&table_info->max_clock_voltage_on_dc.vddgfx, &(data->vddcgfx_leakage));\n\t} else {\n\n\t\ttmp_result = smu7_patch_lookup_table_with_leakage(hwmgr,\n\t\t\t\ttable_info->vddc_lookup_table, &(data->vddc_leakage));\n\t\tif (tmp_result)\n\t\t\tresult = tmp_result;\n\n\t\ttmp_result = smu7_patch_clock_voltage_limits_with_vddc_leakage(hwmgr,\n\t\t\t\t&(data->vddc_leakage), &table_info->max_clock_voltage_on_dc.vddc);\n\t\tif (tmp_result)\n\t\t\tresult = tmp_result;\n\t}\n\n\ttmp_result = smu7_patch_voltage_dependency_tables_with_lookup_table(hwmgr);\n\tif (tmp_result)\n\t\tresult = tmp_result;\n\n\ttmp_result = smu7_calc_voltage_dependency_tables(hwmgr);\n\tif (tmp_result)\n\t\tresult = tmp_result;\n\n\ttmp_result = smu7_calc_mm_voltage_dependency_table(hwmgr);\n\tif (tmp_result)\n\t\tresult = tmp_result;\n\n\ttmp_result = smu7_sort_lookup_table(hwmgr, table_info->vddgfx_lookup_table);\n\tif (tmp_result)\n\t\tresult = tmp_result;\n\n\ttmp_result = smu7_sort_lookup_table(hwmgr, table_info->vddc_lookup_table);\n\tif (tmp_result)\n\t\tresult = tmp_result;\n\n\treturn result;\n}\n\nstatic int smu7_find_highest_vddc(struct pp_hwmgr *hwmgr)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *allowed_sclk_vdd_table =\n\t\t\t\t\t\ttable_info->vdd_dep_on_sclk;\n\tstruct phm_ppt_v1_voltage_lookup_table *lookup_table =\n\t\t\t\t\t\ttable_info->vddc_lookup_table;\n\tuint16_t highest_voltage;\n\tuint32_t i;\n\n\thighest_voltage = allowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;\n\n\tfor (i = 0; i < lookup_table->count; i++) {\n\t\tif (lookup_table->entries[i].us_vdd < ATOM_VIRTUAL_VOLTAGE_ID0 &&\n\t\t    lookup_table->entries[i].us_vdd > highest_voltage)\n\t\t\thighest_voltage = lookup_table->entries[i].us_vdd;\n\t}\n\n\treturn highest_voltage;\n}\n\nstatic int smu7_set_private_data_based_on_pptable_v1(struct pp_hwmgr *hwmgr)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *allowed_sclk_vdd_table =\n\t\t\t\t\t\ttable_info->vdd_dep_on_sclk;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *allowed_mclk_vdd_table =\n\t\t\t\t\t\ttable_info->vdd_dep_on_mclk;\n\n\tPP_ASSERT_WITH_CODE(allowed_sclk_vdd_table != NULL,\n\t\t\"VDD dependency on SCLK table is missing.\",\n\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_sclk_vdd_table->count >= 1,\n\t\t\"VDD dependency on SCLK table has to have is missing.\",\n\t\treturn -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(allowed_mclk_vdd_table != NULL,\n\t\t\"VDD dependency on MCLK table is missing\",\n\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_mclk_vdd_table->count >= 1,\n\t\t\"VDD dependency on MCLK table has to have is missing.\",\n\t\treturn -EINVAL);\n\n\ttable_info->max_clock_voltage_on_ac.sclk =\n\t\tallowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].clk;\n\ttable_info->max_clock_voltage_on_ac.mclk =\n\t\tallowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].clk;\n\tif (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM)\n\t\ttable_info->max_clock_voltage_on_ac.vddc =\n\t\t\tsmu7_find_highest_vddc(hwmgr);\n\telse\n\t\ttable_info->max_clock_voltage_on_ac.vddc =\n\t\t\tallowed_sclk_vdd_table->entries[allowed_sclk_vdd_table->count - 1].vddc;\n\ttable_info->max_clock_voltage_on_ac.vddci =\n\t\tallowed_mclk_vdd_table->entries[allowed_mclk_vdd_table->count - 1].vddci;\n\n\thwmgr->dyn_state.max_clock_voltage_on_ac.sclk = table_info->max_clock_voltage_on_ac.sclk;\n\thwmgr->dyn_state.max_clock_voltage_on_ac.mclk = table_info->max_clock_voltage_on_ac.mclk;\n\thwmgr->dyn_state.max_clock_voltage_on_ac.vddc = table_info->max_clock_voltage_on_ac.vddc;\n\thwmgr->dyn_state.max_clock_voltage_on_ac.vddci = table_info->max_clock_voltage_on_ac.vddci;\n\n\treturn 0;\n}\n\nstatic int smu7_patch_voltage_workaround(struct pp_hwmgr *hwmgr)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t       (struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table;\n\tstruct phm_ppt_v1_voltage_lookup_table *lookup_table;\n\tuint32_t i;\n\tuint32_t hw_revision, sub_vendor_id, sub_sys_id;\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\n\tif (table_info != NULL) {\n\t\tdep_mclk_table = table_info->vdd_dep_on_mclk;\n\t\tlookup_table = table_info->vddc_lookup_table;\n\t} else\n\t\treturn 0;\n\n\thw_revision = adev->pdev->revision;\n\tsub_sys_id = adev->pdev->subsystem_device;\n\tsub_vendor_id = adev->pdev->subsystem_vendor;\n\n\tif (adev->pdev->device == 0x67DF && hw_revision == 0xC7 &&\n\t    ((sub_sys_id == 0xb37 && sub_vendor_id == 0x1002) ||\n\t     (sub_sys_id == 0x4a8 && sub_vendor_id == 0x1043) ||\n\t     (sub_sys_id == 0x9480 && sub_vendor_id == 0x1682))) {\n\n\t\tPHM_WRITE_VFPF_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\t\t      CGS_IND_REG__SMC,\n\t\t\t\t\t      PWR_CKS_CNTL,\n\t\t\t\t\t      CKS_STRETCH_AMOUNT,\n\t\t\t\t\t      0x3);\n\n\t\tif (lookup_table->entries[dep_mclk_table->entries[dep_mclk_table->count-1].vddInd].us_vdd >= 1000)\n\t\t\treturn 0;\n\n\t\tfor (i = 0; i < lookup_table->count; i++) {\n\t\t\tif (lookup_table->entries[i].us_vdd < 0xff01 && lookup_table->entries[i].us_vdd >= 1000) {\n\t\t\t\tdep_mclk_table->entries[dep_mclk_table->count-1].vddInd = (uint8_t) i;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_thermal_parameter_init(struct pp_hwmgr *hwmgr)\n{\n\tstruct pp_atomctrl_gpio_pin_assignment gpio_pin_assignment;\n\tuint32_t temp_reg;\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\n\tif (atomctrl_get_pp_assign_pin(hwmgr, VDDC_PCC_GPIO_PINID, &gpio_pin_assignment)) {\n\t\ttemp_reg = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCNB_PWRMGT_CNTL);\n\t\tswitch (gpio_pin_assignment.uc_gpio_pin_bit_shift) {\n\t\tcase 0:\n\t\t\ttemp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x1);\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\ttemp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW_MODE, 0x2);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\ttemp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, GNB_SLOW, 0x1);\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\ttemp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, FORCE_NB_PS1, 0x1);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\ttemp_reg = PHM_SET_FIELD(temp_reg, CNB_PWRMGT_CNTL, DPM_ENABLED, 0x1);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCNB_PWRMGT_CNTL, temp_reg);\n\t}\n\n\tif (table_info == NULL)\n\t\treturn 0;\n\n\tif (table_info->cac_dtp_table->usDefaultTargetOperatingTemp != 0 &&\n\t\thwmgr->thermal_controller.advanceFanControlParameters.ucFanControlMode) {\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanPWMMinLimit =\n\t\t\t(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanPWMMaxLimit =\n\t\t\t(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanPWM;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanPWMStep = 1;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanRPMMaxLimit = 100;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanRPMMinLimit =\n\t\t\t(uint16_t)hwmgr->thermal_controller.advanceFanControlParameters.ucMinimumPWMLimit;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usFanRPMStep = 1;\n\n\t\ttable_info->cac_dtp_table->usDefaultTargetOperatingTemp = (table_info->cac_dtp_table->usDefaultTargetOperatingTemp >= 50) ?\n\t\t\t\t\t\t\t\t(table_info->cac_dtp_table->usDefaultTargetOperatingTemp - 50) : 0;\n\n\t\ttable_info->cac_dtp_table->usOperatingTempMaxLimit = table_info->cac_dtp_table->usDefaultTargetOperatingTemp;\n\t\ttable_info->cac_dtp_table->usOperatingTempStep = 1;\n\t\ttable_info->cac_dtp_table->usOperatingTempHyst = 1;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usMaxFanPWM =\n\t\t\t       hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanPWM;\n\n\t\thwmgr->thermal_controller.advanceFanControlParameters.usMaxFanRPM =\n\t\t\t       hwmgr->thermal_controller.advanceFanControlParameters.usDefaultMaxFanRPM;\n\n\t\thwmgr->dyn_state.cac_dtp_table->usOperatingTempMinLimit =\n\t\t\t       table_info->cac_dtp_table->usOperatingTempMinLimit;\n\n\t\thwmgr->dyn_state.cac_dtp_table->usOperatingTempMaxLimit =\n\t\t\t       table_info->cac_dtp_table->usOperatingTempMaxLimit;\n\n\t\thwmgr->dyn_state.cac_dtp_table->usDefaultTargetOperatingTemp =\n\t\t\t       table_info->cac_dtp_table->usDefaultTargetOperatingTemp;\n\n\t\thwmgr->dyn_state.cac_dtp_table->usOperatingTempStep =\n\t\t\t       table_info->cac_dtp_table->usOperatingTempStep;\n\n\t\thwmgr->dyn_state.cac_dtp_table->usTargetOperatingTemp =\n\t\t\t       table_info->cac_dtp_table->usTargetOperatingTemp;\n\t\tif (hwmgr->feature_mask & PP_OD_FUZZY_FAN_CONTROL_MASK)\n\t\t\tphm_cap_set(hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t\tPHM_PlatformCaps_ODFuzzyFanControlSupport);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void smu7_patch_ppt_v0_with_vdd_leakage(struct pp_hwmgr *hwmgr,\n\t\tuint32_t *voltage, struct smu7_leakage_voltage *leakage_table)\n{\n\tuint32_t index;\n\n\t \n\tfor (index = 0; index < leakage_table->count; index++) {\n\t\t \n\t\t \n\t\tif (leakage_table->leakage_id[index] == *voltage) {\n\t\t\t*voltage = leakage_table->actual_voltage[index];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (*voltage > ATOM_VIRTUAL_VOLTAGE_ID0)\n\t\tpr_info(\"Voltage value looks like a Leakage ID but it's not patched\\n\");\n}\n\n\nstatic int smu7_patch_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t      struct phm_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_vddci(struct pp_hwmgr *hwmgr,\n\t\t\t       struct phm_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t\t\t&data->vddci_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_vce_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t\t  struct phm_vce_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\n\nstatic int smu7_patch_uvd_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t\t  struct phm_uvd_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_vddc_shed_limit(struct pp_hwmgr *hwmgr,\n\t\t\t\t\t struct phm_phase_shedding_limits_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].Voltage,\n\t\t\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_samu_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t\t   struct phm_samu_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_acp_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t\t  struct phm_acp_clock_voltage_dependency_table *tab)\n{\n\tuint16_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab)\n\t\tfor (i = 0; i < tab->count; i++)\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &tab->entries[i].v,\n\t\t\t\t\t&data->vddc_leakage);\n\n\treturn 0;\n}\n\nstatic int smu7_patch_limits_vddc(struct pp_hwmgr *hwmgr,\n\t\t\t\t  struct phm_clock_and_voltage_limits *tab)\n{\n\tuint32_t vddc, vddci;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab) {\n\t\tvddc = tab->vddc;\n\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &vddc,\n\t\t\t\t\t\t   &data->vddc_leakage);\n\t\ttab->vddc = vddc;\n\t\tvddci = tab->vddci;\n\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &vddci,\n\t\t\t\t\t\t   &data->vddci_leakage);\n\t\ttab->vddci = vddci;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_patch_cac_vddc(struct pp_hwmgr *hwmgr, struct phm_cac_leakage_table *tab)\n{\n\tuint32_t i;\n\tuint32_t vddc;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (tab) {\n\t\tfor (i = 0; i < tab->count; i++) {\n\t\t\tvddc = (uint32_t)(tab->entries[i].Vddc);\n\t\t\tsmu7_patch_ppt_v0_with_vdd_leakage(hwmgr, &vddc, &data->vddc_leakage);\n\t\t\ttab->entries[i].Vddc = (uint16_t)vddc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_patch_dependency_tables_with_leakage(struct pp_hwmgr *hwmgr)\n{\n\tint tmp;\n\n\ttmp = smu7_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dependency_on_sclk);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dependency_on_mclk);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_vddc(hwmgr, hwmgr->dyn_state.vddc_dep_on_dal_pwrl);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_vddci(hwmgr, hwmgr->dyn_state.vddci_dependency_on_mclk);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_vce_vddc(hwmgr, hwmgr->dyn_state.vce_clock_voltage_dependency_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_uvd_vddc(hwmgr, hwmgr->dyn_state.uvd_clock_voltage_dependency_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_samu_vddc(hwmgr, hwmgr->dyn_state.samu_clock_voltage_dependency_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_acp_vddc(hwmgr, hwmgr->dyn_state.acp_clock_voltage_dependency_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_vddc_shed_limit(hwmgr, hwmgr->dyn_state.vddc_phase_shed_limits_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_limits_vddc(hwmgr, &hwmgr->dyn_state.max_clock_voltage_on_ac);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_limits_vddc(hwmgr, &hwmgr->dyn_state.max_clock_voltage_on_dc);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\ttmp = smu7_patch_cac_vddc(hwmgr, hwmgr->dyn_state.cac_leakage_table);\n\tif (tmp)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n\nstatic int smu7_set_private_data_based_on_pptable_v0(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tstruct phm_clock_voltage_dependency_table *allowed_sclk_vddc_table = hwmgr->dyn_state.vddc_dependency_on_sclk;\n\tstruct phm_clock_voltage_dependency_table *allowed_mclk_vddc_table = hwmgr->dyn_state.vddc_dependency_on_mclk;\n\tstruct phm_clock_voltage_dependency_table *allowed_mclk_vddci_table = hwmgr->dyn_state.vddci_dependency_on_mclk;\n\n\tPP_ASSERT_WITH_CODE(allowed_sclk_vddc_table != NULL,\n\t\t\"VDDC dependency on SCLK table is missing. This table is mandatory\",\n\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_sclk_vddc_table->count >= 1,\n\t\t\"VDDC dependency on SCLK table has to have is missing. This table is mandatory\",\n\t\treturn -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(allowed_mclk_vddc_table != NULL,\n\t\t\"VDDC dependency on MCLK table is missing. This table is mandatory\",\n\t\treturn -EINVAL);\n\tPP_ASSERT_WITH_CODE(allowed_mclk_vddc_table->count >= 1,\n\t\t\"VDD dependency on MCLK table has to have is missing. This table is mandatory\",\n\t\treturn -EINVAL);\n\n\tdata->min_vddc_in_pptable = (uint16_t)allowed_sclk_vddc_table->entries[0].v;\n\tdata->max_vddc_in_pptable = (uint16_t)allowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].v;\n\n\thwmgr->dyn_state.max_clock_voltage_on_ac.sclk =\n\t\tallowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].clk;\n\thwmgr->dyn_state.max_clock_voltage_on_ac.mclk =\n\t\tallowed_mclk_vddc_table->entries[allowed_mclk_vddc_table->count - 1].clk;\n\thwmgr->dyn_state.max_clock_voltage_on_ac.vddc =\n\t\tallowed_sclk_vddc_table->entries[allowed_sclk_vddc_table->count - 1].v;\n\n\tif (allowed_mclk_vddci_table != NULL && allowed_mclk_vddci_table->count >= 1) {\n\t\tdata->min_vddci_in_pptable = (uint16_t)allowed_mclk_vddci_table->entries[0].v;\n\t\tdata->max_vddci_in_pptable = (uint16_t)allowed_mclk_vddci_table->entries[allowed_mclk_vddci_table->count - 1].v;\n\t}\n\n\tif (hwmgr->dyn_state.vddci_dependency_on_mclk != NULL && hwmgr->dyn_state.vddci_dependency_on_mclk->count >= 1)\n\t\thwmgr->dyn_state.max_clock_voltage_on_ac.vddci = hwmgr->dyn_state.vddci_dependency_on_mclk->entries[hwmgr->dyn_state.vddci_dependency_on_mclk->count - 1].v;\n\n\treturn 0;\n}\n\nstatic int smu7_hwmgr_backend_fini(struct pp_hwmgr *hwmgr)\n{\n\tkfree(hwmgr->dyn_state.vddc_dep_on_dal_pwrl);\n\thwmgr->dyn_state.vddc_dep_on_dal_pwrl = NULL;\n\tkfree(hwmgr->backend);\n\thwmgr->backend = NULL;\n\n\treturn 0;\n}\n\nstatic int smu7_get_elb_voltages(struct pp_hwmgr *hwmgr)\n{\n\tuint16_t virtual_voltage_id, vddc, vddci, efuse_voltage_id;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tint i;\n\n\tif (atomctrl_get_leakage_id_from_efuse(hwmgr, &efuse_voltage_id) == 0) {\n\t\tfor (i = 0; i < SMU7_MAX_LEAKAGE_COUNT; i++) {\n\t\t\tvirtual_voltage_id = ATOM_VIRTUAL_VOLTAGE_ID0 + i;\n\t\t\tif (atomctrl_get_leakage_vddc_base_on_leakage(hwmgr, &vddc, &vddci,\n\t\t\t\t\t\t\t\tvirtual_voltage_id,\n\t\t\t\t\t\t\t\tefuse_voltage_id) == 0) {\n\t\t\t\tif (vddc != 0 && vddc != virtual_voltage_id) {\n\t\t\t\t\tdata->vddc_leakage.actual_voltage[data->vddc_leakage.count] = vddc;\n\t\t\t\t\tdata->vddc_leakage.leakage_id[data->vddc_leakage.count] = virtual_voltage_id;\n\t\t\t\t\tdata->vddc_leakage.count++;\n\t\t\t\t}\n\t\t\t\tif (vddci != 0 && vddci != virtual_voltage_id) {\n\t\t\t\t\tdata->vddci_leakage.actual_voltage[data->vddci_leakage.count] = vddci;\n\t\t\t\t\tdata->vddci_leakage.leakage_id[data->vddci_leakage.count] = virtual_voltage_id;\n\t\t\t\t\tdata->vddci_leakage.count++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n#define LEAKAGE_ID_MSB\t\t\t463\n#define LEAKAGE_ID_LSB\t\t\t454\n\nstatic int smu7_update_edc_leakage_table(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t efuse;\n\tuint16_t offset;\n\tint ret = 0;\n\n\tif (data->disable_edc_leakage_controller)\n\t\treturn 0;\n\n\tret = atomctrl_get_edc_hilo_leakage_offset_table(hwmgr,\n\t\t\t\t\t\t\t &data->edc_hilo_leakage_offset_from_vbios);\n\tif (ret)\n\t\treturn ret;\n\n\tif (data->edc_hilo_leakage_offset_from_vbios.usEdcDidtLoDpm7TableOffset &&\n\t    data->edc_hilo_leakage_offset_from_vbios.usEdcDidtHiDpm7TableOffset) {\n\t\tatomctrl_read_efuse(hwmgr, LEAKAGE_ID_LSB, LEAKAGE_ID_MSB, &efuse);\n\t\tif (efuse < data->edc_hilo_leakage_offset_from_vbios.usHiLoLeakageThreshold)\n\t\t\toffset = data->edc_hilo_leakage_offset_from_vbios.usEdcDidtLoDpm7TableOffset;\n\t\telse\n\t\t\toffset = data->edc_hilo_leakage_offset_from_vbios.usEdcDidtHiDpm7TableOffset;\n\n\t\tret = atomctrl_get_edc_leakage_table(hwmgr,\n\t\t\t\t\t\t     &data->edc_leakage_table,\n\t\t\t\t\t\t     offset);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic int smu7_hwmgr_backend_init(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data;\n\tint result = 0;\n\n\tdata = kzalloc(sizeof(struct smu7_hwmgr), GFP_KERNEL);\n\tif (data == NULL)\n\t\treturn -ENOMEM;\n\n\thwmgr->backend = data;\n\tsmu7_patch_voltage_workaround(hwmgr);\n\tsmu7_init_dpm_defaults(hwmgr);\n\n\t \n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_EVV)) {\n\t\tresult = smu7_get_evv_voltages(hwmgr);\n\t\tif (result) {\n\t\t\tpr_info(\"Get EVV Voltage Failed.  Abort Driver loading!\\n\");\n\t\t\tkfree(hwmgr->backend);\n\t\t\thwmgr->backend = NULL;\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tsmu7_get_elb_voltages(hwmgr);\n\t}\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tsmu7_complete_dependency_tables(hwmgr);\n\t\tsmu7_set_private_data_based_on_pptable_v1(hwmgr);\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tsmu7_patch_dependency_tables_with_leakage(hwmgr);\n\t\tsmu7_set_private_data_based_on_pptable_v0(hwmgr);\n\t}\n\n\t \n\tresult = phm_initializa_dynamic_state_adjustment_rule_settings(hwmgr);\n\n\tif (0 == result) {\n\t\tstruct amdgpu_device *adev = hwmgr->adev;\n\n\t\tdata->is_tlu_enabled = false;\n\n\t\thwmgr->platform_descriptor.hardwareActivityPerformanceLevels =\n\t\t\t\t\t\t\tSMU7_MAX_HARDWARE_POWERLEVELS;\n\t\thwmgr->platform_descriptor.hardwarePerformanceLevels = 2;\n\t\thwmgr->platform_descriptor.minimumClocksReductionPercentage = 50;\n\n\t\tdata->pcie_gen_cap = adev->pm.pcie_gen_mask;\n\t\tif (data->pcie_gen_cap & CAIL_PCIE_LINK_SPEED_SUPPORT_GEN3)\n\t\t\tdata->pcie_spc_cap = 20;\n\t\telse\n\t\t\tdata->pcie_spc_cap = 16;\n\t\tdata->pcie_lane_cap = adev->pm.pcie_mlw_mask;\n\n\t\thwmgr->platform_descriptor.vbiosInterruptId = 0x20000400;  \n \n\t\thwmgr->platform_descriptor.clockStep.engineClock = 500;\n\t\thwmgr->platform_descriptor.clockStep.memoryClock = 500;\n\t\tsmu7_thermal_parameter_init(hwmgr);\n\t} else {\n\t\t \n\t\tsmu7_hwmgr_backend_fini(hwmgr);\n\t}\n\n\tresult = smu7_update_edc_leakage_table(hwmgr);\n\tif (result) {\n\t\tsmu7_hwmgr_backend_fini(hwmgr);\n\t\treturn result;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_force_dpm_highest(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t level, tmp;\n\n\tif (!data->pcie_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {\n\t\t\tlevel = 0;\n\t\t\ttmp = data->dpm_level_enable_mask.pcie_dpm_enable_mask;\n\t\t\twhile (tmp >>= 1)\n\t\t\t\tlevel++;\n\n\t\t\tif (level)\n\t\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_PCIeDPM_ForceLevel, level,\n\t\t\t\t\t\tNULL);\n\t\t}\n\t}\n\n\tif (!data->sclk_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {\n\t\t\tlevel = 0;\n\t\t\ttmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;\n\t\t\twhile (tmp >>= 1)\n\t\t\t\tlevel++;\n\n\t\t\tif (level)\n\t\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_SCLKDPM_SetEnabledMask,\n\t\t\t\t\t\t(1 << level),\n\t\t\t\t\t\tNULL);\n\t\t}\n\t}\n\n\tif (!data->mclk_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {\n\t\t\tlevel = 0;\n\t\t\ttmp = data->dpm_level_enable_mask.mclk_dpm_enable_mask;\n\t\t\twhile (tmp >>= 1)\n\t\t\t\tlevel++;\n\n\t\t\tif (level)\n\t\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\tPPSMC_MSG_MCLKDPM_SetEnabledMask,\n\t\t\t\t\t\t(1 << level),\n\t\t\t\t\t\tNULL);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_upload_dpm_level_enable_mask(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\tphm_apply_dal_min_voltage_request(hwmgr);\n \n\n\tif (!data->sclk_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.sclk_dpm_enable_mask)\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\tPPSMC_MSG_SCLKDPM_SetEnabledMask,\n\t\t\t\t\tdata->dpm_level_enable_mask.sclk_dpm_enable_mask,\n\t\t\t\t\tNULL);\n\t}\n\n\tif (!data->mclk_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.mclk_dpm_enable_mask)\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\tPPSMC_MSG_MCLKDPM_SetEnabledMask,\n\t\t\t\t\tdata->dpm_level_enable_mask.mclk_dpm_enable_mask,\n\t\t\t\t\tNULL);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_unforce_dpm_levels(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (!smum_is_dpm_running(hwmgr))\n\t\treturn -EINVAL;\n\n\tif (!data->pcie_dpm_key_disabled) {\n\t\tsmum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_PCIeDPM_UnForceLevel,\n\t\t\t\tNULL);\n\t}\n\n\treturn smu7_upload_dpm_level_enable_mask(hwmgr);\n}\n\nstatic int smu7_force_dpm_lowest(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data =\n\t\t\t(struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t level;\n\n\tif (!data->sclk_dpm_key_disabled)\n\t\tif (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {\n\t\t\tlevel = phm_get_lowest_enabled_level(hwmgr,\n\t\t\t\t\t\t\t      data->dpm_level_enable_mask.sclk_dpm_enable_mask);\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\t\t    PPSMC_MSG_SCLKDPM_SetEnabledMask,\n\t\t\t\t\t\t\t    (1 << level),\n\t\t\t\t\t\t\t    NULL);\n\n\t}\n\n\tif (!data->mclk_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.mclk_dpm_enable_mask) {\n\t\t\tlevel = phm_get_lowest_enabled_level(hwmgr,\n\t\t\t\t\t\t\t      data->dpm_level_enable_mask.mclk_dpm_enable_mask);\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\t\t    PPSMC_MSG_MCLKDPM_SetEnabledMask,\n\t\t\t\t\t\t\t    (1 << level),\n\t\t\t\t\t\t\t    NULL);\n\t\t}\n\t}\n\n\tif (!data->pcie_dpm_key_disabled) {\n\t\tif (data->dpm_level_enable_mask.pcie_dpm_enable_mask) {\n\t\t\tlevel = phm_get_lowest_enabled_level(hwmgr,\n\t\t\t\t\t\t\t      data->dpm_level_enable_mask.pcie_dpm_enable_mask);\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t\t\t    PPSMC_MSG_PCIeDPM_ForceLevel,\n\t\t\t\t\t\t\t    (level),\n\t\t\t\t\t\t\t    NULL);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_profiling_clk(struct pp_hwmgr *hwmgr, enum amd_dpm_forced_level level,\n\t\t\t\tuint32_t *sclk_mask, uint32_t *mclk_mask, uint32_t *pcie_mask)\n{\n\tuint32_t percentage;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_dpm_table *golden_dpm_table = &data->golden_dpm_table;\n\tint32_t tmp_mclk;\n\tint32_t tmp_sclk;\n\tint32_t count;\n\n\tif (golden_dpm_table->mclk_table.count < 1)\n\t\treturn -EINVAL;\n\n\tpercentage = 100 * golden_dpm_table->sclk_table.dpm_levels[golden_dpm_table->sclk_table.count - 1].value /\n\t\t\tgolden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 1].value;\n\n\tif (golden_dpm_table->mclk_table.count == 1) {\n\t\tpercentage = 70;\n\t\ttmp_mclk = golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 1].value;\n\t\t*mclk_mask = golden_dpm_table->mclk_table.count - 1;\n\t} else {\n\t\ttmp_mclk = golden_dpm_table->mclk_table.dpm_levels[golden_dpm_table->mclk_table.count - 2].value;\n\t\t*mclk_mask = golden_dpm_table->mclk_table.count - 2;\n\t}\n\n\ttmp_sclk = tmp_mclk * percentage / 100;\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tfor (count = hwmgr->dyn_state.vddc_dependency_on_sclk->count-1;\n\t\t\tcount >= 0; count--) {\n\t\t\tif (tmp_sclk >= hwmgr->dyn_state.vddc_dependency_on_sclk->entries[count].clk) {\n\t\t\t\t*sclk_mask = count;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (count < 0 || level == AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK)\n\t\t\t*sclk_mask = 0;\n\n\t\tif (level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK)\n\t\t\t*sclk_mask = hwmgr->dyn_state.vddc_dependency_on_sclk->count-1;\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tstruct phm_ppt_v1_information *table_info =\n\t\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\n\t\tfor (count = table_info->vdd_dep_on_sclk->count-1; count >= 0; count--) {\n\t\t\tif (tmp_sclk >= table_info->vdd_dep_on_sclk->entries[count].clk) {\n\t\t\t\t*sclk_mask = count;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (count < 0 || level == AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK)\n\t\t\t*sclk_mask = 0;\n\n\t\tif (level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK)\n\t\t\t*sclk_mask = table_info->vdd_dep_on_sclk->count - 1;\n\t}\n\n\tif (level == AMD_DPM_FORCED_LEVEL_PROFILE_MIN_MCLK)\n\t\t*mclk_mask = 0;\n\telse if (level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK)\n\t\t*mclk_mask = golden_dpm_table->mclk_table.count - 1;\n\n\t*pcie_mask = data->dpm_table.pcie_speed_table.count - 1;\n\n\treturn 0;\n}\n\nstatic int smu7_force_dpm_level(struct pp_hwmgr *hwmgr,\n\t\t\t\tenum amd_dpm_forced_level level)\n{\n\tint ret = 0;\n\tuint32_t sclk_mask = 0;\n\tuint32_t mclk_mask = 0;\n\tuint32_t pcie_mask = 0;\n\n\tswitch (level) {\n\tcase AMD_DPM_FORCED_LEVEL_HIGH:\n\t\tret = smu7_force_dpm_highest(hwmgr);\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_LOW:\n\t\tret = smu7_force_dpm_lowest(hwmgr);\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_AUTO:\n\t\tret = smu7_unforce_dpm_levels(hwmgr);\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_STANDARD:\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_MIN_SCLK:\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_MIN_MCLK:\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_PEAK:\n\t\tret = smu7_get_profiling_clk(hwmgr, level, &sclk_mask, &mclk_mask, &pcie_mask);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tsmu7_force_clock_level(hwmgr, PP_SCLK, 1<<sclk_mask);\n\t\tsmu7_force_clock_level(hwmgr, PP_MCLK, 1<<mclk_mask);\n\t\tsmu7_force_clock_level(hwmgr, PP_PCIE, 1<<pcie_mask);\n\t\tbreak;\n\tcase AMD_DPM_FORCED_LEVEL_MANUAL:\n\tcase AMD_DPM_FORCED_LEVEL_PROFILE_EXIT:\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!ret) {\n\t\tif (level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK && hwmgr->dpm_level != AMD_DPM_FORCED_LEVEL_PROFILE_PEAK)\n\t\t\tsmu7_fan_ctrl_set_fan_speed_pwm(hwmgr, 255);\n\t\telse if (level != AMD_DPM_FORCED_LEVEL_PROFILE_PEAK && hwmgr->dpm_level == AMD_DPM_FORCED_LEVEL_PROFILE_PEAK)\n\t\t\tsmu7_fan_ctrl_reset_fan_speed_to_default(hwmgr);\n\t}\n\treturn ret;\n}\n\nstatic int smu7_get_power_state_size(struct pp_hwmgr *hwmgr)\n{\n\treturn sizeof(struct smu7_power_state);\n}\n\nstatic int smu7_vblank_too_short(struct pp_hwmgr *hwmgr,\n\t\t\t\t uint32_t vblank_time_us)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t switch_limit_us;\n\n\tswitch (hwmgr->chip_id) {\n\tcase CHIP_POLARIS10:\n\tcase CHIP_POLARIS11:\n\tcase CHIP_POLARIS12:\n\t\tif (hwmgr->is_kicker || (hwmgr->chip_id == CHIP_POLARIS12))\n\t\t\tswitch_limit_us = data->is_memory_gddr5 ? 450 : 150;\n\t\telse\n\t\t\tswitch_limit_us = data->is_memory_gddr5 ? 200 : 150;\n\t\tbreak;\n\tcase CHIP_VEGAM:\n\t\tswitch_limit_us = 30;\n\t\tbreak;\n\tdefault:\n\t\tswitch_limit_us = data->is_memory_gddr5 ? 450 : 150;\n\t\tbreak;\n\t}\n\n\tif (vblank_time_us < switch_limit_us)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic int smu7_apply_state_adjust_rules(struct pp_hwmgr *hwmgr,\n\t\t\t\tstruct pp_power_state *request_ps,\n\t\t\tconst struct pp_power_state *current_ps)\n{\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\tstruct smu7_power_state *smu7_ps =\n\t\t\t\tcast_phw_smu7_power_state(&request_ps->hardware);\n\tuint32_t sclk;\n\tuint32_t mclk;\n\tstruct PP_Clocks minimum_clocks = {0};\n\tbool disable_mclk_switching;\n\tbool disable_mclk_switching_for_frame_lock;\n\tbool disable_mclk_switching_for_display;\n\tconst struct phm_clock_and_voltage_limits *max_limits;\n\tuint32_t i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tint32_t count;\n\tint32_t stable_pstate_sclk = 0, stable_pstate_mclk = 0;\n\tuint32_t latency;\n\tbool latency_allowed = false;\n\n\tdata->battery_state = (PP_StateUILabel_Battery ==\n\t\t\trequest_ps->classification.ui_label);\n\tdata->mclk_ignore_signal = false;\n\n\tmax_limits = adev->pm.ac_power ?\n\t\t\t&(hwmgr->dyn_state.max_clock_voltage_on_ac) :\n\t\t\t&(hwmgr->dyn_state.max_clock_voltage_on_dc);\n\n\t \n\tif (!adev->pm.ac_power) {\n\t\tfor (i = 0; i < smu7_ps->performance_level_count; i++) {\n\t\t\tif (smu7_ps->performance_levels[i].memory_clock > max_limits->mclk)\n\t\t\t\tsmu7_ps->performance_levels[i].memory_clock = max_limits->mclk;\n\t\t\tif (smu7_ps->performance_levels[i].engine_clock > max_limits->sclk)\n\t\t\t\tsmu7_ps->performance_levels[i].engine_clock = max_limits->sclk;\n\t\t}\n\t}\n\n\tminimum_clocks.engineClock = hwmgr->display_config->min_core_set_clock;\n\tminimum_clocks.memoryClock = hwmgr->display_config->min_mem_set_clock;\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_StablePState)) {\n\t\tmax_limits = &(hwmgr->dyn_state.max_clock_voltage_on_ac);\n\t\tstable_pstate_sclk = (max_limits->sclk * 75) / 100;\n\n\t\tfor (count = table_info->vdd_dep_on_sclk->count - 1;\n\t\t\t\tcount >= 0; count--) {\n\t\t\tif (stable_pstate_sclk >=\n\t\t\t\t\ttable_info->vdd_dep_on_sclk->entries[count].clk) {\n\t\t\t\tstable_pstate_sclk =\n\t\t\t\t\t\ttable_info->vdd_dep_on_sclk->entries[count].clk;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (count < 0)\n\t\t\tstable_pstate_sclk = table_info->vdd_dep_on_sclk->entries[0].clk;\n\n\t\tstable_pstate_mclk = max_limits->mclk;\n\n\t\tminimum_clocks.engineClock = stable_pstate_sclk;\n\t\tminimum_clocks.memoryClock = stable_pstate_mclk;\n\t}\n\n\tdisable_mclk_switching_for_frame_lock = phm_cap_enabled(\n\t\t\t\t    hwmgr->platform_descriptor.platformCaps,\n\t\t\t\t    PHM_PlatformCaps_DisableMclkSwitchingForFrameLock);\n\n\tdisable_mclk_switching_for_display = ((1 < hwmgr->display_config->num_display) &&\n\t\t\t\t\t\t!hwmgr->display_config->multi_monitor_in_sync) ||\n\t\t\t\t\t\t(hwmgr->display_config->num_display &&\n\t\t\t\t\t\tsmu7_vblank_too_short(hwmgr, hwmgr->display_config->min_vblank_time));\n\n\tdisable_mclk_switching = disable_mclk_switching_for_frame_lock ||\n\t\t\t\t\t disable_mclk_switching_for_display;\n\n\tif (hwmgr->display_config->num_display == 0) {\n\t\tif (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM)\n\t\t\tdata->mclk_ignore_signal = true;\n\t\telse\n\t\t\tdisable_mclk_switching = false;\n\t}\n\n\tsclk = smu7_ps->performance_levels[0].engine_clock;\n\tmclk = smu7_ps->performance_levels[0].memory_clock;\n\n\tif (disable_mclk_switching &&\n\t    (!(hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t    hwmgr->chip_id <= CHIP_VEGAM)))\n\t\tmclk = smu7_ps->performance_levels\n\t\t[smu7_ps->performance_level_count - 1].memory_clock;\n\n\tif (sclk < minimum_clocks.engineClock)\n\t\tsclk = (minimum_clocks.engineClock > max_limits->sclk) ?\n\t\t\t\tmax_limits->sclk : minimum_clocks.engineClock;\n\n\tif (mclk < minimum_clocks.memoryClock)\n\t\tmclk = (minimum_clocks.memoryClock > max_limits->mclk) ?\n\t\t\t\tmax_limits->mclk : minimum_clocks.memoryClock;\n\n\tsmu7_ps->performance_levels[0].engine_clock = sclk;\n\tsmu7_ps->performance_levels[0].memory_clock = mclk;\n\n\tsmu7_ps->performance_levels[1].engine_clock =\n\t\t(smu7_ps->performance_levels[1].engine_clock >=\n\t\t\t\tsmu7_ps->performance_levels[0].engine_clock) ?\n\t\t\t\t\t\tsmu7_ps->performance_levels[1].engine_clock :\n\t\t\t\t\t\tsmu7_ps->performance_levels[0].engine_clock;\n\n\tif (disable_mclk_switching) {\n\t\tif (mclk < smu7_ps->performance_levels[1].memory_clock)\n\t\t\tmclk = smu7_ps->performance_levels[1].memory_clock;\n\n\t\tif (hwmgr->chip_id >= CHIP_POLARIS10 && hwmgr->chip_id <= CHIP_VEGAM) {\n\t\t\tif (disable_mclk_switching_for_display) {\n\t\t\t\t \n\t\t\t\tlatency = hwmgr->display_config->dce_tolerable_mclk_in_active_latency;\n\t\t\t\tfor (i = 0; i < data->mclk_latency_table.count; i++) {\n\t\t\t\t\tif (data->mclk_latency_table.entries[i].latency <= latency) {\n\t\t\t\t\t\tlatency_allowed = true;\n\n\t\t\t\t\t\tif ((data->mclk_latency_table.entries[i].frequency >=\n\t\t\t\t\t\t\t\tsmu7_ps->performance_levels[0].memory_clock) &&\n\t\t\t\t\t\t    (data->mclk_latency_table.entries[i].frequency <=\n\t\t\t\t\t\t\t\tsmu7_ps->performance_levels[1].memory_clock)) {\n\t\t\t\t\t\t\tmclk = data->mclk_latency_table.entries[i].frequency;\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif ((i >= data->mclk_latency_table.count - 1) && !latency_allowed) {\n\t\t\t\t\tdata->mclk_ignore_signal = true;\n\t\t\t\t} else {\n\t\t\t\t\tdata->mclk_ignore_signal = false;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (disable_mclk_switching_for_frame_lock)\n\t\t\t\tmclk = smu7_ps->performance_levels[1].memory_clock;\n\t\t}\n\n\t\tsmu7_ps->performance_levels[0].memory_clock = mclk;\n\n\t\tif (!(hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t\t      hwmgr->chip_id <= CHIP_VEGAM))\n\t\t\tsmu7_ps->performance_levels[1].memory_clock = mclk;\n\t} else {\n\t\tif (smu7_ps->performance_levels[1].memory_clock <\n\t\t\t\tsmu7_ps->performance_levels[0].memory_clock)\n\t\t\tsmu7_ps->performance_levels[1].memory_clock =\n\t\t\t\t\tsmu7_ps->performance_levels[0].memory_clock;\n\t}\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_StablePState)) {\n\t\tfor (i = 0; i < smu7_ps->performance_level_count; i++) {\n\t\t\tsmu7_ps->performance_levels[i].engine_clock = stable_pstate_sclk;\n\t\t\tsmu7_ps->performance_levels[i].memory_clock = stable_pstate_mclk;\n\t\t\tsmu7_ps->performance_levels[i].pcie_gen = data->pcie_gen_performance.max;\n\t\t\tsmu7_ps->performance_levels[i].pcie_lane = data->pcie_gen_performance.max;\n\t\t}\n\t}\n\treturn 0;\n}\n\n\nstatic uint32_t smu7_dpm_get_mclk(struct pp_hwmgr *hwmgr, bool low)\n{\n\tstruct pp_power_state  *ps;\n\tstruct smu7_power_state  *smu7_ps;\n\n\tif (hwmgr == NULL)\n\t\treturn -EINVAL;\n\n\tps = hwmgr->request_ps;\n\n\tif (ps == NULL)\n\t\treturn -EINVAL;\n\n\tsmu7_ps = cast_phw_smu7_power_state(&ps->hardware);\n\n\tif (low)\n\t\treturn smu7_ps->performance_levels[0].memory_clock;\n\telse\n\t\treturn smu7_ps->performance_levels\n\t\t\t\t[smu7_ps->performance_level_count-1].memory_clock;\n}\n\nstatic uint32_t smu7_dpm_get_sclk(struct pp_hwmgr *hwmgr, bool low)\n{\n\tstruct pp_power_state  *ps;\n\tstruct smu7_power_state  *smu7_ps;\n\n\tif (hwmgr == NULL)\n\t\treturn -EINVAL;\n\n\tps = hwmgr->request_ps;\n\n\tif (ps == NULL)\n\t\treturn -EINVAL;\n\n\tsmu7_ps = cast_phw_smu7_power_state(&ps->hardware);\n\n\tif (low)\n\t\treturn smu7_ps->performance_levels[0].engine_clock;\n\telse\n\t\treturn smu7_ps->performance_levels\n\t\t\t\t[smu7_ps->performance_level_count-1].engine_clock;\n}\n\nstatic int smu7_dpm_patch_boot_state(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tstruct pp_hw_power_state *hw_ps)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_power_state *ps = (struct smu7_power_state *)hw_ps;\n\tATOM_FIRMWARE_INFO_V2_2 *fw_info;\n\tuint16_t size;\n\tuint8_t frev, crev;\n\tint index = GetIndexIntoMasterTable(DATA, FirmwareInfo);\n\n\t \n\tfw_info = (ATOM_FIRMWARE_INFO_V2_2 *)smu_atom_get_data_table(hwmgr->adev, index,\n\t\t\t&size, &frev, &crev);\n\tif (!fw_info)\n\t\t \n\t\treturn 0;\n\n\t \n\tdata->vbios_boot_state.sclk_bootup_value =\n\t\t\tle32_to_cpu(fw_info->ulDefaultEngineClock);\n\tdata->vbios_boot_state.mclk_bootup_value =\n\t\t\tle32_to_cpu(fw_info->ulDefaultMemoryClock);\n\tdata->vbios_boot_state.mvdd_bootup_value =\n\t\t\tle16_to_cpu(fw_info->usBootUpMVDDCVoltage);\n\tdata->vbios_boot_state.vddc_bootup_value =\n\t\t\tle16_to_cpu(fw_info->usBootUpVDDCVoltage);\n\tdata->vbios_boot_state.vddci_bootup_value =\n\t\t\tle16_to_cpu(fw_info->usBootUpVDDCIVoltage);\n\tdata->vbios_boot_state.pcie_gen_bootup_value =\n\t\t\tsmu7_get_current_pcie_speed(hwmgr);\n\n\tdata->vbios_boot_state.pcie_lane_bootup_value =\n\t\t\t(uint16_t)smu7_get_current_pcie_lane_number(hwmgr);\n\n\t \n\tps->performance_levels[0].memory_clock = data->vbios_boot_state.mclk_bootup_value;\n\tps->performance_levels[0].engine_clock = data->vbios_boot_state.sclk_bootup_value;\n\tps->performance_levels[0].pcie_gen = data->vbios_boot_state.pcie_gen_bootup_value;\n\tps->performance_levels[0].pcie_lane = data->vbios_boot_state.pcie_lane_bootup_value;\n\n\treturn 0;\n}\n\nstatic int smu7_get_number_of_powerplay_table_entries(struct pp_hwmgr *hwmgr)\n{\n\tint result;\n\tunsigned long ret = 0;\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tresult = pp_tables_get_num_of_entries(hwmgr, &ret);\n\t\treturn result ? 0 : ret;\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tresult = get_number_of_powerplay_table_entries_v1_0(hwmgr);\n\t\treturn result;\n\t}\n\treturn 0;\n}\n\nstatic int smu7_get_pp_table_entry_callback_func_v1(struct pp_hwmgr *hwmgr,\n\t\tvoid *state, struct pp_power_state *power_state,\n\t\tvoid *pp_table, uint32_t classification_flag)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_power_state  *smu7_power_state =\n\t\t\t(struct smu7_power_state *)(&(power_state->hardware));\n\tstruct smu7_performance_level *performance_level;\n\tATOM_Tonga_State *state_entry = (ATOM_Tonga_State *)state;\n\tATOM_Tonga_POWERPLAYTABLE *powerplay_table =\n\t\t\t(ATOM_Tonga_POWERPLAYTABLE *)pp_table;\n\tPPTable_Generic_SubTable_Header *sclk_dep_table =\n\t\t\t(PPTable_Generic_SubTable_Header *)\n\t\t\t(((unsigned long)powerplay_table) +\n\t\t\t\tle16_to_cpu(powerplay_table->usSclkDependencyTableOffset));\n\n\tATOM_Tonga_MCLK_Dependency_Table *mclk_dep_table =\n\t\t\t(ATOM_Tonga_MCLK_Dependency_Table *)\n\t\t\t(((unsigned long)powerplay_table) +\n\t\t\t\tle16_to_cpu(powerplay_table->usMclkDependencyTableOffset));\n\n\t \n\tpower_state->classification.ui_label =\n\t\t\t(le16_to_cpu(state_entry->usClassification) &\n\t\t\tATOM_PPLIB_CLASSIFICATION_UI_MASK) >>\n\t\t\tATOM_PPLIB_CLASSIFICATION_UI_SHIFT;\n\tpower_state->classification.flags = classification_flag;\n\t \n\n\tpower_state->classification.temporary_state = false;\n\tpower_state->classification.to_be_deleted = false;\n\n\tpower_state->validation.disallowOnDC =\n\t\t\t(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &\n\t\t\t\t\tATOM_Tonga_DISALLOW_ON_DC));\n\n\tpower_state->pcie.lanes = 0;\n\n\tpower_state->display.disableFrameModulation = false;\n\tpower_state->display.limitRefreshrate = false;\n\tpower_state->display.enableVariBright =\n\t\t\t(0 != (le32_to_cpu(state_entry->ulCapsAndSettings) &\n\t\t\t\t\tATOM_Tonga_ENABLE_VARIBRIGHT));\n\n\tpower_state->validation.supportedPowerLevels = 0;\n\tpower_state->uvd_clocks.VCLK = 0;\n\tpower_state->uvd_clocks.DCLK = 0;\n\tpower_state->temperatures.min = 0;\n\tpower_state->temperatures.max = 0;\n\n\tperformance_level = &(smu7_power_state->performance_levels\n\t\t\t[smu7_power_state->performance_level_count++]);\n\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(smu7_power_state->performance_level_count < smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_GRAPHICS)),\n\t\t\t\"Performance levels exceeds SMC limit!\",\n\t\t\treturn -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(smu7_power_state->performance_level_count <\n\t\t\t\t\thwmgr->platform_descriptor.hardwareActivityPerformanceLevels),\n\t\t\t\"Performance levels exceeds Driver limit!\",\n\t\t\treturn -EINVAL);\n\n\t \n\tperformance_level->memory_clock = mclk_dep_table->entries\n\t\t\t[state_entry->ucMemoryClockIndexLow].ulMclk;\n\tif (sclk_dep_table->ucRevId == 0)\n\t\tperformance_level->engine_clock = ((ATOM_Tonga_SCLK_Dependency_Table *)sclk_dep_table)->entries\n\t\t\t[state_entry->ucEngineClockIndexLow].ulSclk;\n\telse if (sclk_dep_table->ucRevId == 1)\n\t\tperformance_level->engine_clock = ((ATOM_Polaris_SCLK_Dependency_Table *)sclk_dep_table)->entries\n\t\t\t[state_entry->ucEngineClockIndexLow].ulSclk;\n\tperformance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,\n\t\t\tstate_entry->ucPCIEGenLow);\n\tperformance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,\n\t\t\tstate_entry->ucPCIELaneLow);\n\n\tperformance_level = &(smu7_power_state->performance_levels\n\t\t\t[smu7_power_state->performance_level_count++]);\n\tperformance_level->memory_clock = mclk_dep_table->entries\n\t\t\t[state_entry->ucMemoryClockIndexHigh].ulMclk;\n\n\tif (sclk_dep_table->ucRevId == 0)\n\t\tperformance_level->engine_clock = ((ATOM_Tonga_SCLK_Dependency_Table *)sclk_dep_table)->entries\n\t\t\t[state_entry->ucEngineClockIndexHigh].ulSclk;\n\telse if (sclk_dep_table->ucRevId == 1)\n\t\tperformance_level->engine_clock = ((ATOM_Polaris_SCLK_Dependency_Table *)sclk_dep_table)->entries\n\t\t\t[state_entry->ucEngineClockIndexHigh].ulSclk;\n\n\tperformance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap,\n\t\t\tstate_entry->ucPCIEGenHigh);\n\tperformance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap,\n\t\t\tstate_entry->ucPCIELaneHigh);\n\n\treturn 0;\n}\n\nstatic int smu7_get_pp_table_entry_v1(struct pp_hwmgr *hwmgr,\n\t\tunsigned long entry_index, struct pp_power_state *state)\n{\n\tint result;\n\tstruct smu7_power_state *ps;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)(hwmgr->pptable);\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =\n\t\t\ttable_info->vdd_dep_on_mclk;\n\n\tstate->hardware.magic = PHM_VIslands_Magic;\n\n\tps = (struct smu7_power_state *)(&state->hardware);\n\n\tresult = get_powerplay_table_entry_v1_0(hwmgr, entry_index, state,\n\t\t\tsmu7_get_pp_table_entry_callback_func_v1);\n\n\t \n\tif (dep_mclk_table != NULL && dep_mclk_table->count == 1) {\n\t\tif (dep_mclk_table->entries[0].clk !=\n\t\t\t\tdata->vbios_boot_state.mclk_bootup_value)\n\t\t\tpr_debug(\"Single MCLK entry VDDCI/MCLK dependency table \"\n\t\t\t\t\t\"does not match VBIOS boot MCLK level\");\n\t\tif (dep_mclk_table->entries[0].vddci !=\n\t\t\t\tdata->vbios_boot_state.vddci_bootup_value)\n\t\t\tpr_debug(\"Single VDDCI entry VDDCI/MCLK dependency table \"\n\t\t\t\t\t\"does not match VBIOS boot VDDCI level\");\n\t}\n\n\t \n\tif (!state->validation.disallowOnDC)\n\t\tps->dc_compatible = true;\n\n\tif (state->classification.flags & PP_StateClassificationFlag_ACPI)\n\t\tdata->acpi_pcie_gen = ps->performance_levels[0].pcie_gen;\n\n\tps->uvd_clks.vclk = state->uvd_clocks.VCLK;\n\tps->uvd_clks.dclk = state->uvd_clocks.DCLK;\n\n\tif (!result) {\n\t\tuint32_t i;\n\n\t\tswitch (state->classification.ui_label) {\n\t\tcase PP_StateUILabel_Performance:\n\t\t\tdata->use_pcie_performance_levels = true;\n\t\t\tfor (i = 0; i < ps->performance_level_count; i++) {\n\t\t\t\tif (data->pcie_gen_performance.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_performance.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_gen_performance.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_performance.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_lane_performance.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_performance.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\t\t\t\tif (data->pcie_lane_performance.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_performance.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase PP_StateUILabel_Battery:\n\t\t\tdata->use_pcie_power_saving_levels = true;\n\n\t\t\tfor (i = 0; i < ps->performance_level_count; i++) {\n\t\t\t\tif (data->pcie_gen_power_saving.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_power_saving.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_gen_power_saving.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_power_saving.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_lane_power_saving.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_power_saving.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\n\t\t\t\tif (data->pcie_lane_power_saving.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_power_saving.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_get_pp_table_entry_callback_func_v0(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tstruct pp_hw_power_state *power_state,\n\t\t\t\t\tunsigned int index, const void *clock_info)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_power_state  *ps = cast_phw_smu7_power_state(power_state);\n\tconst ATOM_PPLIB_CI_CLOCK_INFO *visland_clk_info = clock_info;\n\tstruct smu7_performance_level *performance_level;\n\tuint32_t engine_clock, memory_clock;\n\tuint16_t pcie_gen_from_bios;\n\n\tengine_clock = visland_clk_info->ucEngineClockHigh << 16 | visland_clk_info->usEngineClockLow;\n\tmemory_clock = visland_clk_info->ucMemoryClockHigh << 16 | visland_clk_info->usMemoryClockLow;\n\n\tif (!(data->mc_micro_code_feature & DISABLE_MC_LOADMICROCODE) && memory_clock > data->highest_mclk)\n\t\tdata->highest_mclk = memory_clock;\n\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(ps->performance_level_count < smum_get_mac_definition(hwmgr, SMU_MAX_LEVELS_GRAPHICS)),\n\t\t\t\"Performance levels exceeds SMC limit!\",\n\t\t\treturn -EINVAL);\n\n\tPP_ASSERT_WITH_CODE(\n\t\t\t(ps->performance_level_count <\n\t\t\t\t\thwmgr->platform_descriptor.hardwareActivityPerformanceLevels),\n\t\t\t\"Performance levels exceeds Driver limit, Skip!\",\n\t\t\treturn 0);\n\n\tperformance_level = &(ps->performance_levels\n\t\t\t[ps->performance_level_count++]);\n\n\t \n\tperformance_level->memory_clock = memory_clock;\n\tperformance_level->engine_clock = engine_clock;\n\n\tpcie_gen_from_bios = visland_clk_info->ucPCIEGen;\n\n\tperformance_level->pcie_gen = get_pcie_gen_support(data->pcie_gen_cap, pcie_gen_from_bios);\n\tperformance_level->pcie_lane = get_pcie_lane_support(data->pcie_lane_cap, visland_clk_info->usPCIELane);\n\n\treturn 0;\n}\n\nstatic int smu7_get_pp_table_entry_v0(struct pp_hwmgr *hwmgr,\n\t\tunsigned long entry_index, struct pp_power_state *state)\n{\n\tint result;\n\tstruct smu7_power_state *ps;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_clock_voltage_dependency_table *dep_mclk_table =\n\t\t\thwmgr->dyn_state.vddci_dependency_on_mclk;\n\n\tmemset(&state->hardware, 0x00, sizeof(struct pp_hw_power_state));\n\n\tstate->hardware.magic = PHM_VIslands_Magic;\n\n\tps = (struct smu7_power_state *)(&state->hardware);\n\n\tresult = pp_tables_get_entry(hwmgr, entry_index, state,\n\t\t\tsmu7_get_pp_table_entry_callback_func_v0);\n\n\t \n\tif (dep_mclk_table != NULL && dep_mclk_table->count == 1) {\n\t\tif (dep_mclk_table->entries[0].clk !=\n\t\t\t\tdata->vbios_boot_state.mclk_bootup_value)\n\t\t\tpr_debug(\"Single MCLK entry VDDCI/MCLK dependency table \"\n\t\t\t\t\t\"does not match VBIOS boot MCLK level\");\n\t\tif (dep_mclk_table->entries[0].v !=\n\t\t\t\tdata->vbios_boot_state.vddci_bootup_value)\n\t\t\tpr_debug(\"Single VDDCI entry VDDCI/MCLK dependency table \"\n\t\t\t\t\t\"does not match VBIOS boot VDDCI level\");\n\t}\n\n\t \n\tif (!state->validation.disallowOnDC)\n\t\tps->dc_compatible = true;\n\n\tif (state->classification.flags & PP_StateClassificationFlag_ACPI)\n\t\tdata->acpi_pcie_gen = ps->performance_levels[0].pcie_gen;\n\n\tps->uvd_clks.vclk = state->uvd_clocks.VCLK;\n\tps->uvd_clks.dclk = state->uvd_clocks.DCLK;\n\n\tif (!result) {\n\t\tuint32_t i;\n\n\t\tswitch (state->classification.ui_label) {\n\t\tcase PP_StateUILabel_Performance:\n\t\t\tdata->use_pcie_performance_levels = true;\n\n\t\t\tfor (i = 0; i < ps->performance_level_count; i++) {\n\t\t\t\tif (data->pcie_gen_performance.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_performance.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_gen_performance.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_performance.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_lane_performance.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_performance.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\n\t\t\t\tif (data->pcie_lane_performance.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_performance.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase PP_StateUILabel_Battery:\n\t\t\tdata->use_pcie_power_saving_levels = true;\n\n\t\t\tfor (i = 0; i < ps->performance_level_count; i++) {\n\t\t\t\tif (data->pcie_gen_power_saving.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_power_saving.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_gen_power_saving.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_gen)\n\t\t\t\t\tdata->pcie_gen_power_saving.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_gen;\n\n\t\t\t\tif (data->pcie_lane_power_saving.max <\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_power_saving.max =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\n\t\t\t\tif (data->pcie_lane_power_saving.min >\n\t\t\t\t\t\tps->performance_levels[i].pcie_lane)\n\t\t\t\t\tdata->pcie_lane_power_saving.min =\n\t\t\t\t\t\t\tps->performance_levels[i].pcie_lane;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int smu7_get_pp_table_entry(struct pp_hwmgr *hwmgr,\n\t\tunsigned long entry_index, struct pp_power_state *state)\n{\n\tif (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\treturn smu7_get_pp_table_entry_v0(hwmgr, entry_index, state);\n\telse if (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\treturn smu7_get_pp_table_entry_v1(hwmgr, entry_index, state);\n\n\treturn 0;\n}\n\nstatic int smu7_get_gpu_power(struct pp_hwmgr *hwmgr, u32 *query)\n{\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\tint i;\n\tu32 tmp = 0;\n\n\tif (!query)\n\t\treturn -EINVAL;\n\n\t \n\tif ((adev->asic_type != CHIP_HAWAII) &&\n\t    (adev->asic_type != CHIP_BONAIRE) &&\n\t    (adev->asic_type != CHIP_FIJI) &&\n\t    (adev->asic_type != CHIP_TONGA)) {\n\t\tsmum_send_msg_to_smc_with_parameter(hwmgr, PPSMC_MSG_GetCurrPkgPwr, 0, &tmp);\n\t\t*query = tmp;\n\n\t\tif (tmp != 0)\n\t\t\treturn 0;\n\t}\n\n\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_PmStatusLogStart, NULL);\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\t\t\tixSMU_PM_STATUS_95, 0);\n\n\tfor (i = 0; i < 10; i++) {\n\t\tmsleep(500);\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_PmStatusLogSample, NULL);\n\t\ttmp = cgs_read_ind_register(hwmgr->device,\n\t\t\t\t\t\tCGS_IND_REG__SMC,\n\t\t\t\t\t\tixSMU_PM_STATUS_95);\n\t\tif (tmp != 0)\n\t\t\tbreak;\n\t}\n\t*query = tmp;\n\n\treturn 0;\n}\n\nstatic int smu7_read_sensor(struct pp_hwmgr *hwmgr, int idx,\n\t\t\t    void *value, int *size)\n{\n\tuint32_t sclk, mclk, activity_percent;\n\tuint32_t offset, val_vid;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\t \n\tif (*size < 4)\n\t\treturn -EINVAL;\n\n\tswitch (idx) {\n\tcase AMDGPU_PP_SENSOR_GFX_SCLK:\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_API_GetSclkFrequency, &sclk);\n\t\t*((uint32_t *)value) = sclk;\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_GFX_MCLK:\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_API_GetMclkFrequency, &mclk);\n\t\t*((uint32_t *)value) = mclk;\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_GPU_LOAD:\n\tcase AMDGPU_PP_SENSOR_MEM_LOAD:\n\t\toffset = data->soft_regs_start + smum_get_offsetof(hwmgr,\n\t\t\t\t\t\t\t\tSMU_SoftRegisters,\n\t\t\t\t\t\t\t\t(idx == AMDGPU_PP_SENSOR_GPU_LOAD) ?\n\t\t\t\t\t\t\t\tAverageGraphicsActivity :\n\t\t\t\t\t\t\t\tAverageMemoryActivity);\n\n\t\tactivity_percent = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, offset);\n\t\tactivity_percent += 0x80;\n\t\tactivity_percent >>= 8;\n\t\t*((uint32_t *)value) = activity_percent > 100 ? 100 : activity_percent;\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_GPU_TEMP:\n\t\t*((uint32_t *)value) = smu7_thermal_get_temperature(hwmgr);\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_UVD_POWER:\n\t\t*((uint32_t *)value) = data->uvd_power_gated ? 0 : 1;\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_VCE_POWER:\n\t\t*((uint32_t *)value) = data->vce_power_gated ? 0 : 1;\n\t\t*size = 4;\n\t\treturn 0;\n\tcase AMDGPU_PP_SENSOR_GPU_INPUT_POWER:\n\t\treturn smu7_get_gpu_power(hwmgr, (uint32_t *)value);\n\tcase AMDGPU_PP_SENSOR_VDDGFX:\n\t\tif ((data->vr_config & VRCONF_VDDGFX_MASK) ==\n\t\t    (VR_SVI2_PLANE_2 << VRCONF_VDDGFX_SHIFT))\n\t\t\tval_vid = PHM_READ_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\t\tCGS_IND_REG__SMC, PWR_SVI2_STATUS, PLANE2_VID);\n\t\telse\n\t\t\tval_vid = PHM_READ_INDIRECT_FIELD(hwmgr->device,\n\t\t\t\t\tCGS_IND_REG__SMC, PWR_SVI2_STATUS, PLANE1_VID);\n\n\t\t*((uint32_t *)value) = (uint32_t)convert_to_vddc(val_vid);\n\t\treturn 0;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int smu7_find_dpm_states_clocks_in_dpm_table(struct pp_hwmgr *hwmgr, const void *input)\n{\n\tconst struct phm_set_power_state_input *states =\n\t\t\t(const struct phm_set_power_state_input *)input;\n\tconst struct smu7_power_state *smu7_ps =\n\t\t\tcast_const_phw_smu7_power_state(states->pnew_state);\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);\n\tuint32_t sclk = smu7_ps->performance_levels\n\t\t\t[smu7_ps->performance_level_count - 1].engine_clock;\n\tstruct smu7_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);\n\tuint32_t mclk = smu7_ps->performance_levels\n\t\t\t[smu7_ps->performance_level_count - 1].memory_clock;\n\tstruct PP_Clocks min_clocks = {0};\n\tuint32_t i;\n\n\tfor (i = 0; i < sclk_table->count; i++) {\n\t\tif (sclk == sclk_table->dpm_levels[i].value)\n\t\t\tbreak;\n\t}\n\n\tif (i >= sclk_table->count) {\n\t\tif (sclk > sclk_table->dpm_levels[i-1].value) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_SCLK;\n\t\t\tsclk_table->dpm_levels[i-1].value = sclk;\n\t\t}\n\t} else {\n\t \n\t\tif (data->display_timing.min_clock_in_sr != min_clocks.engineClockInSR &&\n\t\t\t(min_clocks.engineClockInSR >= SMU7_MINIMUM_ENGINE_CLOCK ||\n\t\t\t\tdata->display_timing.min_clock_in_sr >= SMU7_MINIMUM_ENGINE_CLOCK))\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_SCLK;\n\t}\n\n\tfor (i = 0; i < mclk_table->count; i++) {\n\t\tif (mclk == mclk_table->dpm_levels[i].value)\n\t\t\tbreak;\n\t}\n\n\tif (i >= mclk_table->count) {\n\t\tif (mclk > mclk_table->dpm_levels[i-1].value) {\n\t\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_MCLK;\n\t\t\tmclk_table->dpm_levels[i-1].value = mclk;\n\t\t}\n\t}\n\n\tif (data->display_timing.num_existing_displays != hwmgr->display_config->num_display)\n\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_UPDATE_MCLK;\n\n\treturn 0;\n}\n\nstatic uint16_t smu7_get_maximum_link_speed(struct pp_hwmgr *hwmgr,\n\t\tconst struct smu7_power_state *smu7_ps)\n{\n\tuint32_t i;\n\tuint32_t sclk, max_sclk = 0;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_dpm_table *dpm_table = &data->dpm_table;\n\n\tfor (i = 0; i < smu7_ps->performance_level_count; i++) {\n\t\tsclk = smu7_ps->performance_levels[i].engine_clock;\n\t\tif (max_sclk < sclk)\n\t\t\tmax_sclk = sclk;\n\t}\n\n\tfor (i = 0; i < dpm_table->sclk_table.count; i++) {\n\t\tif (dpm_table->sclk_table.dpm_levels[i].value == max_sclk)\n\t\t\treturn (uint16_t) ((i >= dpm_table->pcie_speed_table.count) ?\n\t\t\t\t\tdpm_table->pcie_speed_table.dpm_levels\n\t\t\t\t\t[dpm_table->pcie_speed_table.count - 1].value :\n\t\t\t\t\tdpm_table->pcie_speed_table.dpm_levels[i].value);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_request_link_speed_change_before_state_change(\n\t\tstruct pp_hwmgr *hwmgr, const void *input)\n{\n\tconst struct phm_set_power_state_input *states =\n\t\t\t(const struct phm_set_power_state_input *)input;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tconst struct smu7_power_state *smu7_nps =\n\t\t\tcast_const_phw_smu7_power_state(states->pnew_state);\n\tconst struct smu7_power_state *polaris10_cps =\n\t\t\tcast_const_phw_smu7_power_state(states->pcurrent_state);\n\n\tuint16_t target_link_speed = smu7_get_maximum_link_speed(hwmgr, smu7_nps);\n\tuint16_t current_link_speed;\n\n\tif (data->force_pcie_gen == PP_PCIEGenInvalid)\n\t\tcurrent_link_speed = smu7_get_maximum_link_speed(hwmgr, polaris10_cps);\n\telse\n\t\tcurrent_link_speed = data->force_pcie_gen;\n\n\tdata->force_pcie_gen = PP_PCIEGenInvalid;\n\tdata->pspp_notify_required = false;\n\n\tif (target_link_speed > current_link_speed) {\n\t\tswitch (target_link_speed) {\n#ifdef CONFIG_ACPI\n\t\tcase PP_PCIEGen3:\n\t\t\tif (0 == amdgpu_acpi_pcie_performance_request(hwmgr->adev, PCIE_PERF_REQ_GEN3, false))\n\t\t\t\tbreak;\n\t\t\tdata->force_pcie_gen = PP_PCIEGen2;\n\t\t\tif (current_link_speed == PP_PCIEGen2)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase PP_PCIEGen2:\n\t\t\tif (0 == amdgpu_acpi_pcie_performance_request(hwmgr->adev, PCIE_PERF_REQ_GEN2, false))\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n#endif\n\t\tdefault:\n\t\t\tdata->force_pcie_gen = smu7_get_current_pcie_speed(hwmgr);\n\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tif (target_link_speed < current_link_speed)\n\t\t\tdata->pspp_notify_required = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_freeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (0 == data->need_update_smu7_dpm_table)\n\t\treturn 0;\n\n\tif ((0 == data->sclk_dpm_key_disabled) &&\n\t\t(data->need_update_smu7_dpm_table &\n\t\t\t(DPMTABLE_OD_UPDATE_SCLK | DPMTABLE_UPDATE_SCLK))) {\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to freeze SCLK DPM when DPM is disabled\",\n\t\t\t\t);\n\t\tPP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_SCLKDPM_FreezeLevel,\n\t\t\t\tNULL),\n\t\t\t\t\"Failed to freeze SCLK DPM during FreezeSclkMclkDPM Function!\",\n\t\t\t\treturn -EINVAL);\n\t}\n\n\tif ((0 == data->mclk_dpm_key_disabled) &&\n\t\t!data->mclk_ignore_signal &&\n\t\t(data->need_update_smu7_dpm_table &\n\t\t DPMTABLE_OD_UPDATE_MCLK)) {\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to freeze MCLK DPM when DPM is disabled\",\n\t\t\t\t);\n\t\tPP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_MCLKDPM_FreezeLevel,\n\t\t\t\tNULL),\n\t\t\t\t\"Failed to freeze MCLK DPM during FreezeSclkMclkDPM Function!\",\n\t\t\t\treturn -EINVAL);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_populate_and_upload_sclk_mclk_dpm_levels(\n\t\tstruct pp_hwmgr *hwmgr, const void *input)\n{\n\tint result = 0;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_dpm_table *dpm_table = &data->dpm_table;\n\tuint32_t count;\n\tstruct smu7_odn_dpm_table *odn_table = &(data->odn_dpm_table);\n\tstruct phm_odn_clock_levels *odn_sclk_table = &(odn_table->odn_core_clock_dpm_levels);\n\tstruct phm_odn_clock_levels *odn_mclk_table = &(odn_table->odn_memory_clock_dpm_levels);\n\n\tif (0 == data->need_update_smu7_dpm_table)\n\t\treturn 0;\n\n\tif (hwmgr->od_enabled && data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_SCLK) {\n\t\tfor (count = 0; count < dpm_table->sclk_table.count; count++) {\n\t\t\tdpm_table->sclk_table.dpm_levels[count].enabled = odn_sclk_table->entries[count].enabled;\n\t\t\tdpm_table->sclk_table.dpm_levels[count].value = odn_sclk_table->entries[count].clock;\n\t\t}\n\t}\n\n\tif (hwmgr->od_enabled && data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK) {\n\t\tfor (count = 0; count < dpm_table->mclk_table.count; count++) {\n\t\t\tdpm_table->mclk_table.dpm_levels[count].enabled = odn_mclk_table->entries[count].enabled;\n\t\t\tdpm_table->mclk_table.dpm_levels[count].value = odn_mclk_table->entries[count].clock;\n\t\t}\n\t}\n\n\tif (data->need_update_smu7_dpm_table &\n\t\t\t(DPMTABLE_OD_UPDATE_SCLK | DPMTABLE_UPDATE_SCLK)) {\n\t\tresult = smum_populate_all_graphic_levels(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to populate SCLK during PopulateNewDPMClocksStates Function!\",\n\t\t\t\treturn result);\n\t}\n\n\tif (data->need_update_smu7_dpm_table &\n\t\t\t(DPMTABLE_OD_UPDATE_MCLK | DPMTABLE_UPDATE_MCLK)) {\n\t\t \n\t\tresult = smum_populate_all_memory_levels(hwmgr);\n\t\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\t\"Failed to populate MCLK during PopulateNewDPMClocksStates Function!\",\n\t\t\t\treturn result);\n\t}\n\n\treturn result;\n}\n\nstatic int smu7_trim_single_dpm_states(struct pp_hwmgr *hwmgr,\n\t\t\t  struct smu7_single_dpm_table *dpm_table,\n\t\t\tuint32_t low_limit, uint32_t high_limit)\n{\n\tuint32_t i;\n\n\t \n\tbool force_trim = (low_limit == high_limit);\n\tfor (i = 0; i < dpm_table->count; i++) {\n\t \n\t\tif ((!hwmgr->od_enabled || force_trim)\n\t\t\t&& (dpm_table->dpm_levels[i].value < low_limit\n\t\t\t|| dpm_table->dpm_levels[i].value > high_limit))\n\t\t\tdpm_table->dpm_levels[i].enabled = false;\n\t\telse\n\t\t\tdpm_table->dpm_levels[i].enabled = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_trim_dpm_states(struct pp_hwmgr *hwmgr,\n\t\tconst struct smu7_power_state *smu7_ps)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t high_limit_count;\n\n\tPP_ASSERT_WITH_CODE((smu7_ps->performance_level_count >= 1),\n\t\t\t\"power state did not have any performance level\",\n\t\t\treturn -EINVAL);\n\n\thigh_limit_count = (1 == smu7_ps->performance_level_count) ? 0 : 1;\n\n\tsmu7_trim_single_dpm_states(hwmgr,\n\t\t\t&(data->dpm_table.sclk_table),\n\t\t\tsmu7_ps->performance_levels[0].engine_clock,\n\t\t\tsmu7_ps->performance_levels[high_limit_count].engine_clock);\n\n\tsmu7_trim_single_dpm_states(hwmgr,\n\t\t\t&(data->dpm_table.mclk_table),\n\t\t\tsmu7_ps->performance_levels[0].memory_clock,\n\t\t\tsmu7_ps->performance_levels[high_limit_count].memory_clock);\n\n\treturn 0;\n}\n\nstatic int smu7_generate_dpm_level_enable_mask(\n\t\tstruct pp_hwmgr *hwmgr, const void *input)\n{\n\tint result = 0;\n\tconst struct phm_set_power_state_input *states =\n\t\t\t(const struct phm_set_power_state_input *)input;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tconst struct smu7_power_state *smu7_ps =\n\t\t\tcast_const_phw_smu7_power_state(states->pnew_state);\n\n\n\tresult = smu7_trim_dpm_states(hwmgr, smu7_ps);\n\tif (result)\n\t\treturn result;\n\n\tdata->dpm_level_enable_mask.sclk_dpm_enable_mask =\n\t\t\tphm_get_dpm_level_enable_mask_value(&data->dpm_table.sclk_table);\n\tdata->dpm_level_enable_mask.mclk_dpm_enable_mask =\n\t\t\tphm_get_dpm_level_enable_mask_value(&data->dpm_table.mclk_table);\n\tdata->dpm_level_enable_mask.pcie_dpm_enable_mask =\n\t\t\tphm_get_dpm_level_enable_mask_value(&data->dpm_table.pcie_speed_table);\n\n\treturn 0;\n}\n\nstatic int smu7_unfreeze_sclk_mclk_dpm(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (0 == data->need_update_smu7_dpm_table)\n\t\treturn 0;\n\n\tif ((0 == data->sclk_dpm_key_disabled) &&\n\t\t(data->need_update_smu7_dpm_table &\n\t\t(DPMTABLE_OD_UPDATE_SCLK | DPMTABLE_UPDATE_SCLK))) {\n\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to Unfreeze SCLK DPM when DPM is disabled\",\n\t\t\t\t);\n\t\tPP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_SCLKDPM_UnfreezeLevel,\n\t\t\t\tNULL),\n\t\t\t\"Failed to unfreeze SCLK DPM during UnFreezeSclkMclkDPM Function!\",\n\t\t\treturn -EINVAL);\n\t}\n\n\tif ((0 == data->mclk_dpm_key_disabled) &&\n\t\t!data->mclk_ignore_signal &&\n\t\t(data->need_update_smu7_dpm_table & DPMTABLE_OD_UPDATE_MCLK)) {\n\n\t\tPP_ASSERT_WITH_CODE(true == smum_is_dpm_running(hwmgr),\n\t\t\t\t\"Trying to Unfreeze MCLK DPM when DPM is disabled\",\n\t\t\t\t);\n\t\tPP_ASSERT_WITH_CODE(0 == smum_send_msg_to_smc(hwmgr,\n\t\t\t\tPPSMC_MSG_MCLKDPM_UnfreezeLevel,\n\t\t\t\tNULL),\n\t\t    \"Failed to unfreeze MCLK DPM during UnFreezeSclkMclkDPM Function!\",\n\t\t    return -EINVAL);\n\t}\n\n\tdata->need_update_smu7_dpm_table &= DPMTABLE_OD_UPDATE_VDDC;\n\n\treturn 0;\n}\n\nstatic int smu7_notify_link_speed_change_after_state_change(\n\t\tstruct pp_hwmgr *hwmgr, const void *input)\n{\n\tconst struct phm_set_power_state_input *states =\n\t\t\t(const struct phm_set_power_state_input *)input;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tconst struct smu7_power_state *smu7_ps =\n\t\t\tcast_const_phw_smu7_power_state(states->pnew_state);\n\tuint16_t target_link_speed = smu7_get_maximum_link_speed(hwmgr, smu7_ps);\n\tuint8_t  request;\n\n\tif (data->pspp_notify_required) {\n\t\tif (target_link_speed == PP_PCIEGen3)\n\t\t\trequest = PCIE_PERF_REQ_GEN3;\n\t\telse if (target_link_speed == PP_PCIEGen2)\n\t\t\trequest = PCIE_PERF_REQ_GEN2;\n\t\telse\n\t\t\trequest = PCIE_PERF_REQ_GEN1;\n\n\t\tif (request == PCIE_PERF_REQ_GEN1 &&\n\t\t\t\tsmu7_get_current_pcie_speed(hwmgr) > 0)\n\t\t\treturn 0;\n\n#ifdef CONFIG_ACPI\n\t\tif (amdgpu_acpi_pcie_performance_request(hwmgr->adev, request, false)) {\n\t\t\tif (PP_PCIEGen2 == target_link_speed)\n\t\t\t\tpr_info(\"PSPP request to switch to Gen2 from Gen3 Failed!\");\n\t\t\telse\n\t\t\t\tpr_info(\"PSPP request to switch to Gen1 from Gen2 Failed!\");\n\t\t}\n#endif\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_notify_no_display(struct pp_hwmgr *hwmgr)\n{\n\treturn (smum_send_msg_to_smc(hwmgr, (PPSMC_Msg)PPSMC_NoDisplay, NULL) == 0) ?  0 : -EINVAL;\n}\n\nstatic int smu7_notify_has_display(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (hwmgr->feature_mask & PP_VBI_TIME_SUPPORT_MASK) {\n\t\tif (hwmgr->chip_id == CHIP_VEGAM)\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t(PPSMC_Msg)PPSMC_MSG_SetVBITimeout_VEGAM, data->frame_time_x2,\n\t\t\t\t\tNULL);\n\t\telse\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\t(PPSMC_Msg)PPSMC_MSG_SetVBITimeout, data->frame_time_x2,\n\t\t\t\t\tNULL);\n\t\tdata->last_sent_vbi_timeout = data->frame_time_x2;\n\t}\n\n\treturn (smum_send_msg_to_smc(hwmgr, (PPSMC_Msg)PPSMC_HasDisplay, NULL) == 0) ?  0 : -EINVAL;\n}\n\nstatic int smu7_notify_smc_display(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tint result = 0;\n\n\tif (data->mclk_ignore_signal)\n\t\tresult = smu7_notify_no_display(hwmgr);\n\telse\n\t\tresult = smu7_notify_has_display(hwmgr);\n\n\treturn result;\n}\n\nstatic int smu7_set_power_state_tasks(struct pp_hwmgr *hwmgr, const void *input)\n{\n\tint tmp_result, result = 0;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\ttmp_result = smu7_find_dpm_states_clocks_in_dpm_table(hwmgr, input);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to find DPM states clocks in DPM table!\",\n\t\t\tresult = tmp_result);\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_PCIEPerformanceRequest)) {\n\t\ttmp_result =\n\t\t\tsmu7_request_link_speed_change_before_state_change(hwmgr, input);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to request link speed change before state change!\",\n\t\t\t\tresult = tmp_result);\n\t}\n\n\ttmp_result = smu7_freeze_sclk_mclk_dpm(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to freeze SCLK MCLK DPM!\", result = tmp_result);\n\n\ttmp_result = smu7_populate_and_upload_sclk_mclk_dpm_levels(hwmgr, input);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to populate and upload SCLK MCLK DPM levels!\",\n\t\t\tresult = tmp_result);\n\n\t \n\tif (hwmgr->hardcode_pp_table != NULL)\n\t\tdata->need_update_smu7_dpm_table |= DPMTABLE_OD_UPDATE_VDDC;\n\n\ttmp_result = smu7_update_avfs(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to update avfs voltages!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smu7_generate_dpm_level_enable_mask(hwmgr, input);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to generate DPM level enabled mask!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smum_update_sclk_threshold(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to update SCLK threshold!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smu7_unfreeze_sclk_mclk_dpm(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to unfreeze SCLK MCLK DPM!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smu7_upload_dpm_level_enable_mask(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to upload DPM level enabled mask!\",\n\t\t\tresult = tmp_result);\n\n\ttmp_result = smu7_notify_smc_display(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to notify smc display settings!\",\n\t\t\tresult = tmp_result);\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_PCIEPerformanceRequest)) {\n\t\ttmp_result =\n\t\t\tsmu7_notify_link_speed_change_after_state_change(hwmgr, input);\n\t\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\t\"Failed to notify link speed change after state change!\",\n\t\t\t\tresult = tmp_result);\n\t}\n\tdata->apply_optimized_settings = false;\n\treturn result;\n}\n\nstatic int smu7_set_max_fan_pwm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_pwm)\n{\n\thwmgr->thermal_controller.\n\tadvanceFanControlParameters.usMaxFanPWM = us_max_fan_pwm;\n\n\treturn smum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\tPPSMC_MSG_SetFanPwmMax, us_max_fan_pwm,\n\t\t\tNULL);\n}\n\nstatic int\nsmu7_notify_smc_display_config_after_ps_adjustment(struct pp_hwmgr *hwmgr)\n{\n\treturn 0;\n}\n\n \nstatic int smu7_program_display_gap(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t display_gap = cgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL);\n\tuint32_t display_gap2;\n\tuint32_t pre_vbi_time_in_us;\n\tuint32_t frame_time_in_us;\n\tuint32_t ref_clock, refresh_rate;\n\n\tdisplay_gap = PHM_SET_FIELD(display_gap, CG_DISPLAY_GAP_CNTL, DISP_GAP, (hwmgr->display_config->num_display > 0) ? DISPLAY_GAP_VBLANK_OR_WM : DISPLAY_GAP_IGNORE);\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL, display_gap);\n\n\tref_clock =  amdgpu_asic_get_xclk((struct amdgpu_device *)hwmgr->adev);\n\trefresh_rate = hwmgr->display_config->vrefresh;\n\n\tif (0 == refresh_rate)\n\t\trefresh_rate = 60;\n\n\tframe_time_in_us = 1000000 / refresh_rate;\n\n\tpre_vbi_time_in_us = frame_time_in_us - 200 - hwmgr->display_config->min_vblank_time;\n\n\tdata->frame_time_x2 = frame_time_in_us * 2 / 100;\n\n\tif (data->frame_time_x2 < 280) {\n\t\tpr_debug(\"%s: enforce minimal VBITimeout: %d -> 280\\n\", __func__, data->frame_time_x2);\n\t\tdata->frame_time_x2 = 280;\n\t}\n\n\tdisplay_gap2 = pre_vbi_time_in_us * (ref_clock / 100);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_DISPLAY_GAP_CNTL2, display_gap2);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tdata->soft_regs_start + smum_get_offsetof(hwmgr,\n\t\t\t\t\t\t\tSMU_SoftRegisters,\n\t\t\t\t\t\t\tPreVBlankGap), 0x64);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tdata->soft_regs_start + smum_get_offsetof(hwmgr,\n\t\t\t\t\t\t\tSMU_SoftRegisters,\n\t\t\t\t\t\t\tVBlankTimeout),\n\t\t\t\t\t(frame_time_in_us - pre_vbi_time_in_us));\n\n\treturn 0;\n}\n\nstatic int smu7_display_configuration_changed_task(struct pp_hwmgr *hwmgr)\n{\n\treturn smu7_program_display_gap(hwmgr);\n}\n\n \nstatic int smu7_set_max_fan_rpm_output(struct pp_hwmgr *hwmgr, uint16_t us_max_fan_rpm)\n{\n\thwmgr->thermal_controller.\n\tadvanceFanControlParameters.usMaxFanRPM = us_max_fan_rpm;\n\n\treturn smum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\tPPSMC_MSG_SetFanRpmMax, us_max_fan_rpm,\n\t\t\tNULL);\n}\n\nstatic const struct amdgpu_irq_src_funcs smu7_irq_funcs = {\n\t.process = phm_irq_process,\n};\n\nstatic int smu7_register_irq_handlers(struct pp_hwmgr *hwmgr)\n{\n\tstruct amdgpu_irq_src *source =\n\t\tkzalloc(sizeof(struct amdgpu_irq_src), GFP_KERNEL);\n\n\tif (!source)\n\t\treturn -ENOMEM;\n\n\tsource->funcs = &smu7_irq_funcs;\n\n\tamdgpu_irq_add_id((struct amdgpu_device *)(hwmgr->adev),\n\t\t\tAMDGPU_IRQ_CLIENTID_LEGACY,\n\t\t\tVISLANDS30_IV_SRCID_CG_TSS_THERMAL_LOW_TO_HIGH,\n\t\t\tsource);\n\tamdgpu_irq_add_id((struct amdgpu_device *)(hwmgr->adev),\n\t\t\tAMDGPU_IRQ_CLIENTID_LEGACY,\n\t\t\tVISLANDS30_IV_SRCID_CG_TSS_THERMAL_HIGH_TO_LOW,\n\t\t\tsource);\n\n\t \n\tamdgpu_irq_add_id((struct amdgpu_device *)(hwmgr->adev),\n\t\t\tAMDGPU_IRQ_CLIENTID_LEGACY,\n\t\t\tVISLANDS30_IV_SRCID_GPIO_19,\n\t\t\tsource);\n\n\treturn 0;\n}\n\nstatic bool\nsmu7_check_smc_update_required_for_display_configuration(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tbool is_update_required = false;\n\n\tif (data->display_timing.num_existing_displays != hwmgr->display_config->num_display)\n\t\tis_update_required = true;\n\n\tif (data->display_timing.vrefresh != hwmgr->display_config->vrefresh)\n\t\tis_update_required = true;\n\n\tif (hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t    hwmgr->chip_id <= CHIP_VEGAM &&\n\t    data->last_sent_vbi_timeout != data->frame_time_x2)\n\t\tis_update_required = true;\n\n\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps, PHM_PlatformCaps_SclkDeepSleep)) {\n\t\tif (data->display_timing.min_clock_in_sr != hwmgr->display_config->min_core_set_clock_in_sr &&\n\t\t\t(data->display_timing.min_clock_in_sr >= SMU7_MINIMUM_ENGINE_CLOCK ||\n\t\t\thwmgr->display_config->min_core_set_clock_in_sr >= SMU7_MINIMUM_ENGINE_CLOCK))\n\t\t\tis_update_required = true;\n\t}\n\treturn is_update_required;\n}\n\nstatic inline bool smu7_are_power_levels_equal(const struct smu7_performance_level *pl1,\n\t\t\t\t\t\t\t   const struct smu7_performance_level *pl2)\n{\n\treturn ((pl1->memory_clock == pl2->memory_clock) &&\n\t\t  (pl1->engine_clock == pl2->engine_clock) &&\n\t\t  (pl1->pcie_gen == pl2->pcie_gen) &&\n\t\t  (pl1->pcie_lane == pl2->pcie_lane));\n}\n\nstatic int smu7_check_states_equal(struct pp_hwmgr *hwmgr,\n\t\tconst struct pp_hw_power_state *pstate1,\n\t\tconst struct pp_hw_power_state *pstate2, bool *equal)\n{\n\tconst struct smu7_power_state *psa;\n\tconst struct smu7_power_state *psb;\n\tint i;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (pstate1 == NULL || pstate2 == NULL || equal == NULL)\n\t\treturn -EINVAL;\n\n\tpsa = cast_const_phw_smu7_power_state(pstate1);\n\tpsb = cast_const_phw_smu7_power_state(pstate2);\n\t \n\tif (psa->performance_level_count != psb->performance_level_count) {\n\t\t*equal = false;\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < psa->performance_level_count; i++) {\n\t\tif (!smu7_are_power_levels_equal(&(psa->performance_levels[i]), &(psb->performance_levels[i]))) {\n\t\t\t \n\t\t\t*equal = false;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\t*equal = ((psa->uvd_clks.vclk == psb->uvd_clks.vclk) && (psa->uvd_clks.dclk == psb->uvd_clks.dclk));\n\t*equal &= ((psa->vce_clks.evclk == psb->vce_clks.evclk) && (psa->vce_clks.ecclk == psb->vce_clks.ecclk));\n\t*equal &= (psa->sclk_threshold == psb->sclk_threshold);\n\t \n\t*equal &= !(data->need_update_smu7_dpm_table & (DPMTABLE_OD_UPDATE_SCLK |\n\t\t\t\t\t\t\tDPMTABLE_OD_UPDATE_MCLK |\n\t\t\t\t\t\t\tDPMTABLE_OD_UPDATE_VDDC));\n\n\treturn 0;\n}\n\nstatic int smu7_check_mc_firmware(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tuint32_t tmp;\n\n\t \n\n\tsmu7_get_mc_microcode_version(hwmgr);\n\n\tdata->need_long_memory_training = false;\n\n\tcgs_write_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_INDEX,\n\t\t\t\t\t\t\tixMC_IO_DEBUG_UP_13);\n\ttmp = cgs_read_register(hwmgr->device, mmMC_SEQ_IO_DEBUG_DATA);\n\n\tif (tmp & (1 << 23)) {\n\t\tdata->mem_latency_high = MEM_LATENCY_HIGH;\n\t\tdata->mem_latency_low = MEM_LATENCY_LOW;\n\t\tif ((hwmgr->chip_id == CHIP_POLARIS10) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS11) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS12))\n\t\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_EnableFFC, NULL);\n\t} else {\n\t\tdata->mem_latency_high = 330;\n\t\tdata->mem_latency_low = 330;\n\t\tif ((hwmgr->chip_id == CHIP_POLARIS10) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS11) ||\n\t\t    (hwmgr->chip_id == CHIP_POLARIS12))\n\t\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_DisableFFC, NULL);\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_read_clock_registers(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tdata->clock_registers.vCG_SPLL_FUNC_CNTL         =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL);\n\tdata->clock_registers.vCG_SPLL_FUNC_CNTL_2       =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_2);\n\tdata->clock_registers.vCG_SPLL_FUNC_CNTL_3       =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_3);\n\tdata->clock_registers.vCG_SPLL_FUNC_CNTL_4       =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_FUNC_CNTL_4);\n\tdata->clock_registers.vCG_SPLL_SPREAD_SPECTRUM   =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM);\n\tdata->clock_registers.vCG_SPLL_SPREAD_SPECTRUM_2 =\n\t\tcgs_read_ind_register(hwmgr->device, CGS_IND_REG__SMC, ixCG_SPLL_SPREAD_SPECTRUM_2);\n\tdata->clock_registers.vDLL_CNTL                  =\n\t\tcgs_read_register(hwmgr->device, mmDLL_CNTL);\n\tdata->clock_registers.vMCLK_PWRMGT_CNTL          =\n\t\tcgs_read_register(hwmgr->device, mmMCLK_PWRMGT_CNTL);\n\tdata->clock_registers.vMPLL_AD_FUNC_CNTL         =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_AD_FUNC_CNTL);\n\tdata->clock_registers.vMPLL_DQ_FUNC_CNTL         =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_DQ_FUNC_CNTL);\n\tdata->clock_registers.vMPLL_FUNC_CNTL            =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL);\n\tdata->clock_registers.vMPLL_FUNC_CNTL_1          =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_1);\n\tdata->clock_registers.vMPLL_FUNC_CNTL_2          =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_FUNC_CNTL_2);\n\tdata->clock_registers.vMPLL_SS1                  =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_SS1);\n\tdata->clock_registers.vMPLL_SS2                  =\n\t\tcgs_read_register(hwmgr->device, mmMPLL_SS2);\n\treturn 0;\n\n}\n\n \nstatic int smu7_get_memory_type(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct amdgpu_device *adev = hwmgr->adev;\n\n\tdata->is_memory_gddr5 = (adev->gmc.vram_type == AMDGPU_VRAM_TYPE_GDDR5);\n\n\treturn 0;\n}\n\n \nstatic int smu7_enable_acpi_power_management(struct pp_hwmgr *hwmgr)\n{\n\tPHM_WRITE_INDIRECT_FIELD(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\tGENERAL_PWRMGT, STATIC_PM_EN, 1);\n\n\treturn 0;\n}\n\n \nstatic int smu7_init_power_gate_state(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tdata->uvd_power_gated = false;\n\tdata->vce_power_gated = false;\n\n\treturn 0;\n}\n\nstatic int smu7_init_sclk_threshold(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tdata->low_sclk_interrupt_threshold = 0;\n\treturn 0;\n}\n\nstatic int smu7_setup_asic_task(struct pp_hwmgr *hwmgr)\n{\n\tint tmp_result, result = 0;\n\n\tsmu7_check_mc_firmware(hwmgr);\n\n\ttmp_result = smu7_read_clock_registers(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to read clock registers!\", result = tmp_result);\n\n\ttmp_result = smu7_get_memory_type(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to get memory type!\", result = tmp_result);\n\n\ttmp_result = smu7_enable_acpi_power_management(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to enable ACPI power management!\", result = tmp_result);\n\n\ttmp_result = smu7_init_power_gate_state(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to init power gate state!\", result = tmp_result);\n\n\ttmp_result = smu7_get_mc_microcode_version(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to get MC microcode version!\", result = tmp_result);\n\n\ttmp_result = smu7_init_sclk_threshold(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == tmp_result),\n\t\t\t\"Failed to init sclk threshold!\", result = tmp_result);\n\n\treturn result;\n}\n\nstatic int smu7_force_clock_level(struct pp_hwmgr *hwmgr,\n\t\tenum pp_clock_type type, uint32_t mask)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (mask == 0)\n\t\treturn -EINVAL;\n\n\tswitch (type) {\n\tcase PP_SCLK:\n\t\tif (!data->sclk_dpm_key_disabled)\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\tPPSMC_MSG_SCLKDPM_SetEnabledMask,\n\t\t\t\t\tdata->dpm_level_enable_mask.sclk_dpm_enable_mask & mask,\n\t\t\t\t\tNULL);\n\t\tbreak;\n\tcase PP_MCLK:\n\t\tif (!data->mclk_dpm_key_disabled)\n\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\tPPSMC_MSG_MCLKDPM_SetEnabledMask,\n\t\t\t\t\tdata->dpm_level_enable_mask.mclk_dpm_enable_mask & mask,\n\t\t\t\t\tNULL);\n\t\tbreak;\n\tcase PP_PCIE:\n\t{\n\t\tuint32_t tmp = mask & data->dpm_level_enable_mask.pcie_dpm_enable_mask;\n\n\t\tif (!data->pcie_dpm_key_disabled) {\n\t\t\tif (fls(tmp) != ffs(tmp))\n\t\t\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_PCIeDPM_UnForceLevel,\n\t\t\t\t\t\tNULL);\n\t\t\telse\n\t\t\t\tsmum_send_msg_to_smc_with_parameter(hwmgr,\n\t\t\t\t\tPPSMC_MSG_PCIeDPM_ForceLevel,\n\t\t\t\t\tfls(tmp) - 1,\n\t\t\t\t\tNULL);\n\t\t}\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_print_clock_levels(struct pp_hwmgr *hwmgr,\n\t\tenum pp_clock_type type, char *buf)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);\n\tstruct smu7_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);\n\tstruct smu7_single_dpm_table *pcie_table = &(data->dpm_table.pcie_speed_table);\n\tstruct smu7_odn_dpm_table *odn_table = &(data->odn_dpm_table);\n\tstruct phm_odn_clock_levels *odn_sclk_table = &(odn_table->odn_core_clock_dpm_levels);\n\tstruct phm_odn_clock_levels *odn_mclk_table = &(odn_table->odn_memory_clock_dpm_levels);\n\tint size = 0;\n\tuint32_t i, now, clock, pcie_speed;\n\n\tswitch (type) {\n\tcase PP_SCLK:\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_API_GetSclkFrequency, &clock);\n\n\t\tfor (i = 0; i < sclk_table->count; i++) {\n\t\t\tif (clock > sclk_table->dpm_levels[i].value)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\tnow = i;\n\n\t\tfor (i = 0; i < sclk_table->count; i++)\n\t\t\tsize += sprintf(buf + size, \"%d: %uMhz %s\\n\",\n\t\t\t\t\ti, sclk_table->dpm_levels[i].value / 100,\n\t\t\t\t\t(i == now) ? \"*\" : \"\");\n\t\tbreak;\n\tcase PP_MCLK:\n\t\tsmum_send_msg_to_smc(hwmgr, PPSMC_MSG_API_GetMclkFrequency, &clock);\n\n\t\tfor (i = 0; i < mclk_table->count; i++) {\n\t\t\tif (clock > mclk_table->dpm_levels[i].value)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\tnow = i;\n\n\t\tfor (i = 0; i < mclk_table->count; i++)\n\t\t\tsize += sprintf(buf + size, \"%d: %uMhz %s\\n\",\n\t\t\t\t\ti, mclk_table->dpm_levels[i].value / 100,\n\t\t\t\t\t(i == now) ? \"*\" : \"\");\n\t\tbreak;\n\tcase PP_PCIE:\n\t\tpcie_speed = smu7_get_current_pcie_speed(hwmgr);\n\t\tfor (i = 0; i < pcie_table->count; i++) {\n\t\t\tif (pcie_speed != pcie_table->dpm_levels[i].value)\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\t}\n\t\tnow = i;\n\n\t\tfor (i = 0; i < pcie_table->count; i++)\n\t\t\tsize += sprintf(buf + size, \"%d: %s %s\\n\", i,\n\t\t\t\t\t(pcie_table->dpm_levels[i].value == 0) ? \"2.5GT/s, x8\" :\n\t\t\t\t\t(pcie_table->dpm_levels[i].value == 1) ? \"5.0GT/s, x16\" :\n\t\t\t\t\t(pcie_table->dpm_levels[i].value == 2) ? \"8.0GT/s, x16\" : \"\",\n\t\t\t\t\t(i == now) ? \"*\" : \"\");\n\t\tbreak;\n\tcase OD_SCLK:\n\t\tif (hwmgr->od_enabled) {\n\t\t\tsize += sprintf(buf + size, \"%s:\\n\", \"OD_SCLK\");\n\t\t\tfor (i = 0; i < odn_sclk_table->num_of_pl; i++)\n\t\t\t\tsize += sprintf(buf + size, \"%d: %10uMHz %10umV\\n\",\n\t\t\t\t\ti, odn_sclk_table->entries[i].clock/100,\n\t\t\t\t\todn_sclk_table->entries[i].vddc);\n\t\t}\n\t\tbreak;\n\tcase OD_MCLK:\n\t\tif (hwmgr->od_enabled) {\n\t\t\tsize += sprintf(buf + size, \"%s:\\n\", \"OD_MCLK\");\n\t\t\tfor (i = 0; i < odn_mclk_table->num_of_pl; i++)\n\t\t\t\tsize += sprintf(buf + size, \"%d: %10uMHz %10umV\\n\",\n\t\t\t\t\ti, odn_mclk_table->entries[i].clock/100,\n\t\t\t\t\todn_mclk_table->entries[i].vddc);\n\t\t}\n\t\tbreak;\n\tcase OD_RANGE:\n\t\tif (hwmgr->od_enabled) {\n\t\t\tsize += sprintf(buf + size, \"%s:\\n\", \"OD_RANGE\");\n\t\t\tsize += sprintf(buf + size, \"SCLK: %7uMHz %10uMHz\\n\",\n\t\t\t\tdata->golden_dpm_table.sclk_table.dpm_levels[0].value/100,\n\t\t\t\thwmgr->platform_descriptor.overdriveLimit.engineClock/100);\n\t\t\tsize += sprintf(buf + size, \"MCLK: %7uMHz %10uMHz\\n\",\n\t\t\t\tdata->golden_dpm_table.mclk_table.dpm_levels[0].value/100,\n\t\t\t\thwmgr->platform_descriptor.overdriveLimit.memoryClock/100);\n\t\t\tsize += sprintf(buf + size, \"VDDC: %7umV %11umV\\n\",\n\t\t\t\tdata->odn_dpm_table.min_vddc,\n\t\t\t\tdata->odn_dpm_table.max_vddc);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn size;\n}\n\nstatic void smu7_set_fan_control_mode(struct pp_hwmgr *hwmgr, uint32_t mode)\n{\n\tswitch (mode) {\n\tcase AMD_FAN_CTRL_NONE:\n\t\tsmu7_fan_ctrl_set_fan_speed_pwm(hwmgr, 255);\n\t\tbreak;\n\tcase AMD_FAN_CTRL_MANUAL:\n\t\tif (phm_cap_enabled(hwmgr->platform_descriptor.platformCaps,\n\t\t\tPHM_PlatformCaps_MicrocodeFanControl))\n\t\t\tsmu7_fan_ctrl_stop_smc_fan_control(hwmgr);\n\t\tbreak;\n\tcase AMD_FAN_CTRL_AUTO:\n\t\tif (!smu7_fan_ctrl_set_static_mode(hwmgr, mode))\n\t\t\tsmu7_fan_ctrl_start_smc_fan_control(hwmgr);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic uint32_t smu7_get_fan_control_mode(struct pp_hwmgr *hwmgr)\n{\n\treturn hwmgr->fan_ctrl_enabled ? AMD_FAN_CTRL_AUTO : AMD_FAN_CTRL_MANUAL;\n}\n\nstatic int smu7_get_sclk_od(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);\n\tstruct smu7_single_dpm_table *golden_sclk_table =\n\t\t\t&(data->golden_dpm_table.sclk_table);\n\tint value = sclk_table->dpm_levels[sclk_table->count - 1].value;\n\tint golden_value = golden_sclk_table->dpm_levels\n\t\t\t[golden_sclk_table->count - 1].value;\n\n\tvalue -= golden_value;\n\tvalue = DIV_ROUND_UP(value * 100, golden_value);\n\n\treturn value;\n}\n\nstatic int smu7_set_sclk_od(struct pp_hwmgr *hwmgr, uint32_t value)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *golden_sclk_table =\n\t\t\t&(data->golden_dpm_table.sclk_table);\n\tstruct pp_power_state  *ps;\n\tstruct smu7_power_state  *smu7_ps;\n\n\tif (value > 20)\n\t\tvalue = 20;\n\n\tps = hwmgr->request_ps;\n\n\tif (ps == NULL)\n\t\treturn -EINVAL;\n\n\tsmu7_ps = cast_phw_smu7_power_state(&ps->hardware);\n\n\tsmu7_ps->performance_levels[smu7_ps->performance_level_count - 1].engine_clock =\n\t\t\tgolden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value *\n\t\t\tvalue / 100 +\n\t\t\tgolden_sclk_table->dpm_levels[golden_sclk_table->count - 1].value;\n\n\treturn 0;\n}\n\nstatic int smu7_get_mclk_od(struct pp_hwmgr *hwmgr)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);\n\tstruct smu7_single_dpm_table *golden_mclk_table =\n\t\t\t&(data->golden_dpm_table.mclk_table);\n        int value = mclk_table->dpm_levels[mclk_table->count - 1].value;\n\tint golden_value = golden_mclk_table->dpm_levels\n\t\t\t[golden_mclk_table->count - 1].value;\n\n\tvalue -= golden_value;\n\tvalue = DIV_ROUND_UP(value * 100, golden_value);\n\n\treturn value;\n}\n\nstatic int smu7_set_mclk_od(struct pp_hwmgr *hwmgr, uint32_t value)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *golden_mclk_table =\n\t\t\t&(data->golden_dpm_table.mclk_table);\n\tstruct pp_power_state  *ps;\n\tstruct smu7_power_state  *smu7_ps;\n\n\tif (value > 20)\n\t\tvalue = 20;\n\n\tps = hwmgr->request_ps;\n\n\tif (ps == NULL)\n\t\treturn -EINVAL;\n\n\tsmu7_ps = cast_phw_smu7_power_state(&ps->hardware);\n\n\tsmu7_ps->performance_levels[smu7_ps->performance_level_count - 1].memory_clock =\n\t\t\tgolden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value *\n\t\t\tvalue / 100 +\n\t\t\tgolden_mclk_table->dpm_levels[golden_mclk_table->count - 1].value;\n\n\treturn 0;\n}\n\n\nstatic int smu7_get_sclks(struct pp_hwmgr *hwmgr, struct amd_pp_clocks *clocks)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table = NULL;\n\tstruct phm_clock_voltage_dependency_table *sclk_table;\n\tint i;\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tif (table_info == NULL || table_info->vdd_dep_on_sclk == NULL)\n\t\t\treturn -EINVAL;\n\t\tdep_sclk_table = table_info->vdd_dep_on_sclk;\n\t\tfor (i = 0; i < dep_sclk_table->count; i++)\n\t\t\tclocks->clock[i] = dep_sclk_table->entries[i].clk * 10;\n\t\tclocks->count = dep_sclk_table->count;\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tsclk_table = hwmgr->dyn_state.vddc_dependency_on_sclk;\n\t\tfor (i = 0; i < sclk_table->count; i++)\n\t\t\tclocks->clock[i] = sclk_table->entries[i].clk * 10;\n\t\tclocks->count = sclk_table->count;\n\t}\n\n\treturn 0;\n}\n\nstatic uint32_t smu7_get_mem_latency(struct pp_hwmgr *hwmgr, uint32_t clk)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (clk >= MEM_FREQ_LOW_LATENCY && clk < MEM_FREQ_HIGH_LATENCY)\n\t\treturn data->mem_latency_high;\n\telse if (clk >= MEM_FREQ_HIGH_LATENCY)\n\t\treturn data->mem_latency_low;\n\telse\n\t\treturn MEM_LATENCY_ERR;\n}\n\nstatic int smu7_get_mclks(struct pp_hwmgr *hwmgr, struct amd_pp_clocks *clocks)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table;\n\tint i;\n\tstruct phm_clock_voltage_dependency_table *mclk_table;\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1) {\n\t\tif (table_info == NULL)\n\t\t\treturn -EINVAL;\n\t\tdep_mclk_table = table_info->vdd_dep_on_mclk;\n\t\tfor (i = 0; i < dep_mclk_table->count; i++) {\n\t\t\tclocks->clock[i] = dep_mclk_table->entries[i].clk * 10;\n\t\t\tclocks->latency[i] = smu7_get_mem_latency(hwmgr,\n\t\t\t\t\t\tdep_mclk_table->entries[i].clk);\n\t\t}\n\t\tclocks->count = dep_mclk_table->count;\n\t} else if (hwmgr->pp_table_version == PP_TABLE_V0) {\n\t\tmclk_table = hwmgr->dyn_state.vddc_dependency_on_mclk;\n\t\tfor (i = 0; i < mclk_table->count; i++)\n\t\t\tclocks->clock[i] = mclk_table->entries[i].clk * 10;\n\t\tclocks->count = mclk_table->count;\n\t}\n\treturn 0;\n}\n\nstatic int smu7_get_clock_by_type(struct pp_hwmgr *hwmgr, enum amd_pp_clock_type type,\n\t\t\t\t\t\tstruct amd_pp_clocks *clocks)\n{\n\tswitch (type) {\n\tcase amd_pp_sys_clock:\n\t\tsmu7_get_sclks(hwmgr, clocks);\n\t\tbreak;\n\tcase amd_pp_mem_clock:\n\t\tsmu7_get_mclks(hwmgr, clocks);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_sclks_with_latency(struct pp_hwmgr *hwmgr,\n\t\t\t\t       struct pp_clock_levels_with_latency *clocks)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table =\n\t\t\ttable_info->vdd_dep_on_sclk;\n\tint i;\n\n\tclocks->num_levels = 0;\n\tfor (i = 0; i < dep_sclk_table->count; i++) {\n\t\tif (dep_sclk_table->entries[i].clk) {\n\t\t\tclocks->data[clocks->num_levels].clocks_in_khz =\n\t\t\t\tdep_sclk_table->entries[i].clk * 10;\n\t\t\tclocks->num_levels++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_mclks_with_latency(struct pp_hwmgr *hwmgr,\n\t\t\t\t       struct pp_clock_levels_with_latency *clocks)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =\n\t\t\ttable_info->vdd_dep_on_mclk;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tint i;\n\n\tclocks->num_levels = 0;\n\tdata->mclk_latency_table.count = 0;\n\tfor (i = 0; i < dep_mclk_table->count; i++) {\n\t\tif (dep_mclk_table->entries[i].clk) {\n\t\t\tclocks->data[clocks->num_levels].clocks_in_khz =\n\t\t\t\t\tdep_mclk_table->entries[i].clk * 10;\n\t\t\tdata->mclk_latency_table.entries[data->mclk_latency_table.count].frequency =\n\t\t\t\t\tdep_mclk_table->entries[i].clk;\n\t\t\tclocks->data[clocks->num_levels].latency_in_us =\n\t\t\t\tdata->mclk_latency_table.entries[data->mclk_latency_table.count].latency =\n\t\t\t\t\tsmu7_get_mem_latency(hwmgr, dep_mclk_table->entries[i].clk);\n\t\t\tclocks->num_levels++;\n\t\t\tdata->mclk_latency_table.count++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_clock_by_type_with_latency(struct pp_hwmgr *hwmgr,\n\t\t\t\t\t       enum amd_pp_clock_type type,\n\t\t\t\t\t       struct pp_clock_levels_with_latency *clocks)\n{\n\tif (!(hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t      hwmgr->chip_id <= CHIP_VEGAM))\n\t\treturn -EINVAL;\n\n\tswitch (type) {\n\tcase amd_pp_sys_clock:\n\t\tsmu7_get_sclks_with_latency(hwmgr, clocks);\n\t\tbreak;\n\tcase amd_pp_mem_clock:\n\t\tsmu7_get_mclks_with_latency(hwmgr, clocks);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_set_watermarks_for_clocks_ranges(struct pp_hwmgr *hwmgr,\n\t\t\t\t\t\t void *clock_range)\n{\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_mclk_table =\n\t\t\ttable_info->vdd_dep_on_mclk;\n\tstruct phm_ppt_v1_clock_voltage_dependency_table *dep_sclk_table =\n\t\t\ttable_info->vdd_dep_on_sclk;\n\tstruct polaris10_smumgr *smu_data =\n\t\t\t(struct polaris10_smumgr *)(hwmgr->smu_backend);\n\tSMU74_Discrete_DpmTable  *table = &(smu_data->smc_state_table);\n\tstruct dm_pp_wm_sets_with_clock_ranges *watermarks =\n\t\t\t(struct dm_pp_wm_sets_with_clock_ranges *)clock_range;\n\tuint32_t i, j, k;\n\tbool valid_entry;\n\n\tif (!(hwmgr->chip_id >= CHIP_POLARIS10 &&\n\t      hwmgr->chip_id <= CHIP_VEGAM))\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < dep_mclk_table->count; i++) {\n\t\tfor (j = 0; j < dep_sclk_table->count; j++) {\n\t\t\tvalid_entry = false;\n\t\t\tfor (k = 0; k < watermarks->num_wm_sets; k++) {\n\t\t\t\tif (dep_sclk_table->entries[i].clk >= watermarks->wm_clk_ranges[k].wm_min_eng_clk_in_khz / 10 &&\n\t\t\t\t    dep_sclk_table->entries[i].clk < watermarks->wm_clk_ranges[k].wm_max_eng_clk_in_khz / 10 &&\n\t\t\t\t    dep_mclk_table->entries[i].clk >= watermarks->wm_clk_ranges[k].wm_min_mem_clk_in_khz / 10 &&\n\t\t\t\t    dep_mclk_table->entries[i].clk < watermarks->wm_clk_ranges[k].wm_max_mem_clk_in_khz / 10) {\n\t\t\t\t\tvalid_entry = true;\n\t\t\t\t\ttable->DisplayWatermark[i][j] = watermarks->wm_clk_ranges[k].wm_set_id;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tPP_ASSERT_WITH_CODE(valid_entry,\n\t\t\t\t\t\"Clock is not in range of specified clock range for watermark from DAL!  Using highest water mark set.\",\n\t\t\t\t\ttable->DisplayWatermark[i][j] = watermarks->wm_clk_ranges[k - 1].wm_set_id);\n\t\t}\n\t}\n\n\treturn smu7_copy_bytes_to_smc(hwmgr,\n\t\t\t\t      smu_data->smu7_data.dpm_table_start + offsetof(SMU74_Discrete_DpmTable, DisplayWatermark),\n\t\t\t\t      (uint8_t *)table->DisplayWatermark,\n\t\t\t\t      sizeof(uint8_t) * SMU74_MAX_LEVELS_MEMORY * SMU74_MAX_LEVELS_GRAPHICS,\n\t\t\t\t      SMC_RAM_END);\n}\n\nstatic int smu7_notify_cac_buffer_info(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tuint32_t virtual_addr_low,\n\t\t\t\t\tuint32_t virtual_addr_hi,\n\t\t\t\t\tuint32_t mc_addr_low,\n\t\t\t\t\tuint32_t mc_addr_hi,\n\t\t\t\t\tuint32_t size)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tdata->soft_regs_start +\n\t\t\t\t\tsmum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, DRAM_LOG_ADDR_H),\n\t\t\t\t\tmc_addr_hi);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tdata->soft_regs_start +\n\t\t\t\t\tsmum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, DRAM_LOG_ADDR_L),\n\t\t\t\t\tmc_addr_low);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tdata->soft_regs_start +\n\t\t\t\t\tsmum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, DRAM_LOG_PHY_ADDR_H),\n\t\t\t\t\tvirtual_addr_hi);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tdata->soft_regs_start +\n\t\t\t\t\tsmum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, DRAM_LOG_PHY_ADDR_L),\n\t\t\t\t\tvirtual_addr_low);\n\n\tcgs_write_ind_register(hwmgr->device, CGS_IND_REG__SMC,\n\t\t\t\t\tdata->soft_regs_start +\n\t\t\t\t\tsmum_get_offsetof(hwmgr,\n\t\t\t\t\tSMU_SoftRegisters, DRAM_LOG_BUFF_SIZE),\n\t\t\t\t\tsize);\n\treturn 0;\n}\n\nstatic int smu7_get_max_high_clocks(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tstruct amd_pp_simple_clock_info *clocks)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct smu7_single_dpm_table *sclk_table = &(data->dpm_table.sclk_table);\n\tstruct smu7_single_dpm_table *mclk_table = &(data->dpm_table.mclk_table);\n\n\tif (clocks == NULL)\n\t\treturn -EINVAL;\n\n\tclocks->memory_max_clock = mclk_table->count > 1 ?\n\t\t\t\tmclk_table->dpm_levels[mclk_table->count-1].value :\n\t\t\t\tmclk_table->dpm_levels[0].value;\n\tclocks->engine_max_clock = sclk_table->count > 1 ?\n\t\t\t\tsclk_table->dpm_levels[sclk_table->count-1].value :\n\t\t\t\tsclk_table->dpm_levels[0].value;\n\treturn 0;\n}\n\nstatic int smu7_get_thermal_temperature_range(struct pp_hwmgr *hwmgr,\n\t\tstruct PP_TemperatureRange *thermal_data)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct phm_ppt_v1_information *table_info =\n\t\t\t(struct phm_ppt_v1_information *)hwmgr->pptable;\n\n\tmemcpy(thermal_data, &SMU7ThermalPolicy[0], sizeof(struct PP_TemperatureRange));\n\n\tif (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\tthermal_data->max = table_info->cac_dtp_table->usSoftwareShutdownTemp *\n\t\t\tPP_TEMPERATURE_UNITS_PER_CENTIGRADES;\n\telse if (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\tthermal_data->max = data->thermal_temp_setting.temperature_shutdown *\n\t\t\tPP_TEMPERATURE_UNITS_PER_CENTIGRADES;\n\n\tthermal_data->sw_ctf_threshold = thermal_data->max;\n\n\treturn 0;\n}\n\nstatic bool smu7_check_clk_voltage_valid(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tenum PP_OD_DPM_TABLE_COMMAND type,\n\t\t\t\t\tuint32_t clk,\n\t\t\t\t\tuint32_t voltage)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tif (voltage < data->odn_dpm_table.min_vddc || voltage > data->odn_dpm_table.max_vddc) {\n\t\tpr_info(\"OD voltage is out of range [%d - %d] mV\\n\",\n\t\t\t\t\t\tdata->odn_dpm_table.min_vddc,\n\t\t\t\t\t\tdata->odn_dpm_table.max_vddc);\n\t\treturn false;\n\t}\n\n\tif (type == PP_OD_EDIT_SCLK_VDDC_TABLE) {\n\t\tif (data->golden_dpm_table.sclk_table.dpm_levels[0].value > clk ||\n\t\t\thwmgr->platform_descriptor.overdriveLimit.engineClock < clk) {\n\t\t\tpr_info(\"OD engine clock is out of range [%d - %d] MHz\\n\",\n\t\t\t\tdata->golden_dpm_table.sclk_table.dpm_levels[0].value/100,\n\t\t\t\thwmgr->platform_descriptor.overdriveLimit.engineClock/100);\n\t\t\treturn false;\n\t\t}\n\t} else if (type == PP_OD_EDIT_MCLK_VDDC_TABLE) {\n\t\tif (data->golden_dpm_table.mclk_table.dpm_levels[0].value > clk ||\n\t\t\thwmgr->platform_descriptor.overdriveLimit.memoryClock < clk) {\n\t\t\tpr_info(\"OD memory clock is out of range [%d - %d] MHz\\n\",\n\t\t\t\tdata->golden_dpm_table.mclk_table.dpm_levels[0].value/100,\n\t\t\t\thwmgr->platform_descriptor.overdriveLimit.memoryClock/100);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int smu7_odn_edit_dpm_table(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tenum PP_OD_DPM_TABLE_COMMAND type,\n\t\t\t\t\tlong *input, uint32_t size)\n{\n\tuint32_t i;\n\tstruct phm_odn_clock_levels *podn_dpm_table_in_backend = NULL;\n\tstruct smu7_odn_clock_voltage_dependency_table *podn_vdd_dep_in_backend = NULL;\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\n\tuint32_t input_clk;\n\tuint32_t input_vol;\n\tuint32_t input_level;\n\n\tPP_ASSERT_WITH_CODE(input, \"NULL user input for clock and voltage\",\n\t\t\t\treturn -EINVAL);\n\n\tif (!hwmgr->od_enabled) {\n\t\tpr_info(\"OverDrive feature not enabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (PP_OD_EDIT_SCLK_VDDC_TABLE == type) {\n\t\tpodn_dpm_table_in_backend = &data->odn_dpm_table.odn_core_clock_dpm_levels;\n\t\tpodn_vdd_dep_in_backend = &data->odn_dpm_table.vdd_dependency_on_sclk;\n\t\tPP_ASSERT_WITH_CODE((podn_dpm_table_in_backend && podn_vdd_dep_in_backend),\n\t\t\t\t\"Failed to get ODN SCLK and Voltage tables\",\n\t\t\t\treturn -EINVAL);\n\t} else if (PP_OD_EDIT_MCLK_VDDC_TABLE == type) {\n\t\tpodn_dpm_table_in_backend = &data->odn_dpm_table.odn_memory_clock_dpm_levels;\n\t\tpodn_vdd_dep_in_backend = &data->odn_dpm_table.vdd_dependency_on_mclk;\n\n\t\tPP_ASSERT_WITH_CODE((podn_dpm_table_in_backend && podn_vdd_dep_in_backend),\n\t\t\t\"Failed to get ODN MCLK and Voltage tables\",\n\t\t\treturn -EINVAL);\n\t} else if (PP_OD_RESTORE_DEFAULT_TABLE == type) {\n\t\tsmu7_odn_initial_default_setting(hwmgr);\n\t\treturn 0;\n\t} else if (PP_OD_COMMIT_DPM_TABLE == type) {\n\t\tsmu7_check_dpm_table_updated(hwmgr);\n\t\treturn 0;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < size; i += 3) {\n\t\tif (i + 3 > size || input[i] >= podn_dpm_table_in_backend->num_of_pl) {\n\t\t\tpr_info(\"invalid clock voltage input \\n\");\n\t\t\treturn 0;\n\t\t}\n\t\tinput_level = input[i];\n\t\tinput_clk = input[i+1] * 100;\n\t\tinput_vol = input[i+2];\n\n\t\tif (smu7_check_clk_voltage_valid(hwmgr, type, input_clk, input_vol)) {\n\t\t\tpodn_dpm_table_in_backend->entries[input_level].clock = input_clk;\n\t\t\tpodn_vdd_dep_in_backend->entries[input_level].clk = input_clk;\n\t\t\tpodn_dpm_table_in_backend->entries[input_level].vddc = input_vol;\n\t\t\tpodn_vdd_dep_in_backend->entries[input_level].vddc = input_vol;\n\t\t\tpodn_vdd_dep_in_backend->entries[input_level].vddgfx = input_vol;\n\t\t} else {\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_power_profile_mode(struct pp_hwmgr *hwmgr, char *buf)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t i, size = 0;\n\tuint32_t len;\n\n\tstatic const char *title[8] = {\"NUM\",\n\t\t\t\"MODE_NAME\",\n\t\t\t\"SCLK_UP_HYST\",\n\t\t\t\"SCLK_DOWN_HYST\",\n\t\t\t\"SCLK_ACTIVE_LEVEL\",\n\t\t\t\"MCLK_UP_HYST\",\n\t\t\t\"MCLK_DOWN_HYST\",\n\t\t\t\"MCLK_ACTIVE_LEVEL\"};\n\n\tif (!buf)\n\t\treturn -EINVAL;\n\n\tphm_get_sysfs_buf(&buf, &size);\n\n\tsize += sysfs_emit_at(buf, size, \"%s %16s %16s %16s %16s %16s %16s %16s\\n\",\n\t\t\ttitle[0], title[1], title[2], title[3],\n\t\t\ttitle[4], title[5], title[6], title[7]);\n\n\tlen = ARRAY_SIZE(smu7_profiling);\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (i == hwmgr->power_profile_mode) {\n\t\t\tsize += sysfs_emit_at(buf, size, \"%3d %14s %s: %8d %16d %16d %16d %16d %16d\\n\",\n\t\t\ti, amdgpu_pp_profile_name[i], \"*\",\n\t\t\tdata->current_profile_setting.sclk_up_hyst,\n\t\t\tdata->current_profile_setting.sclk_down_hyst,\n\t\t\tdata->current_profile_setting.sclk_activity,\n\t\t\tdata->current_profile_setting.mclk_up_hyst,\n\t\t\tdata->current_profile_setting.mclk_down_hyst,\n\t\t\tdata->current_profile_setting.mclk_activity);\n\t\t\tcontinue;\n\t\t}\n\t\tif (smu7_profiling[i].bupdate_sclk)\n\t\t\tsize += sysfs_emit_at(buf, size, \"%3d %16s: %8d %16d %16d \",\n\t\t\ti, amdgpu_pp_profile_name[i], smu7_profiling[i].sclk_up_hyst,\n\t\t\tsmu7_profiling[i].sclk_down_hyst,\n\t\t\tsmu7_profiling[i].sclk_activity);\n\t\telse\n\t\t\tsize += sysfs_emit_at(buf, size, \"%3d %16s: %8s %16s %16s \",\n\t\t\ti, amdgpu_pp_profile_name[i], \"-\", \"-\", \"-\");\n\n\t\tif (smu7_profiling[i].bupdate_mclk)\n\t\t\tsize += sysfs_emit_at(buf, size, \"%16d %16d %16d\\n\",\n\t\t\tsmu7_profiling[i].mclk_up_hyst,\n\t\t\tsmu7_profiling[i].mclk_down_hyst,\n\t\t\tsmu7_profiling[i].mclk_activity);\n\t\telse\n\t\t\tsize += sysfs_emit_at(buf, size, \"%16s %16s %16s\\n\",\n\t\t\t\"-\", \"-\", \"-\");\n\t}\n\n\treturn size;\n}\n\nstatic void smu7_patch_compute_profile_mode(struct pp_hwmgr *hwmgr,\n\t\t\t\t\tenum PP_SMC_POWER_PROFILE requst)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tuint32_t tmp, level;\n\n\tif (requst == PP_SMC_POWER_PROFILE_COMPUTE) {\n\t\tif (data->dpm_level_enable_mask.sclk_dpm_enable_mask) {\n\t\t\tlevel = 0;\n\t\t\ttmp = data->dpm_level_enable_mask.sclk_dpm_enable_mask;\n\t\t\twhile (tmp >>= 1)\n\t\t\t\tlevel++;\n\t\t\tif (level > 0)\n\t\t\t\tsmu7_force_clock_level(hwmgr, PP_SCLK, 3 << (level-1));\n\t\t}\n\t} else if (hwmgr->power_profile_mode == PP_SMC_POWER_PROFILE_COMPUTE) {\n\t\tsmu7_force_clock_level(hwmgr, PP_SCLK, data->dpm_level_enable_mask.sclk_dpm_enable_mask);\n\t}\n}\n\nstatic int smu7_set_power_profile_mode(struct pp_hwmgr *hwmgr, long *input, uint32_t size)\n{\n\tstruct smu7_hwmgr *data = (struct smu7_hwmgr *)(hwmgr->backend);\n\tstruct profile_mode_setting tmp;\n\tenum PP_SMC_POWER_PROFILE mode;\n\n\tif (input == NULL)\n\t\treturn -EINVAL;\n\n\tmode = input[size];\n\tswitch (mode) {\n\tcase PP_SMC_POWER_PROFILE_CUSTOM:\n\t\tif (size < 8 && size != 0)\n\t\t\treturn -EINVAL;\n\t\t \n\t\ttmp = smu7_profiling[PP_SMC_POWER_PROFILE_CUSTOM];\n\t\tif (size == 0) {\n\t\t\tif (tmp.bupdate_sclk == 0 && tmp.bupdate_mclk == 0)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\ttmp.bupdate_sclk = input[0];\n\t\t\ttmp.sclk_up_hyst = input[1];\n\t\t\ttmp.sclk_down_hyst = input[2];\n\t\t\ttmp.sclk_activity = input[3];\n\t\t\ttmp.bupdate_mclk = input[4];\n\t\t\ttmp.mclk_up_hyst = input[5];\n\t\t\ttmp.mclk_down_hyst = input[6];\n\t\t\ttmp.mclk_activity = input[7];\n\t\t\tsmu7_profiling[PP_SMC_POWER_PROFILE_CUSTOM] = tmp;\n\t\t}\n\t\tif (!smum_update_dpm_settings(hwmgr, &tmp)) {\n\t\t\tmemcpy(&data->current_profile_setting, &tmp, sizeof(struct profile_mode_setting));\n\t\t\thwmgr->power_profile_mode = mode;\n\t\t}\n\t\tbreak;\n\tcase PP_SMC_POWER_PROFILE_FULLSCREEN3D:\n\tcase PP_SMC_POWER_PROFILE_POWERSAVING:\n\tcase PP_SMC_POWER_PROFILE_VIDEO:\n\tcase PP_SMC_POWER_PROFILE_VR:\n\tcase PP_SMC_POWER_PROFILE_COMPUTE:\n\t\tif (mode == hwmgr->power_profile_mode)\n\t\t\treturn 0;\n\n\t\tmemcpy(&tmp, &smu7_profiling[mode], sizeof(struct profile_mode_setting));\n\t\tif (!smum_update_dpm_settings(hwmgr, &tmp)) {\n\t\t\tif (tmp.bupdate_sclk) {\n\t\t\t\tdata->current_profile_setting.bupdate_sclk = tmp.bupdate_sclk;\n\t\t\t\tdata->current_profile_setting.sclk_up_hyst = tmp.sclk_up_hyst;\n\t\t\t\tdata->current_profile_setting.sclk_down_hyst = tmp.sclk_down_hyst;\n\t\t\t\tdata->current_profile_setting.sclk_activity = tmp.sclk_activity;\n\t\t\t}\n\t\t\tif (tmp.bupdate_mclk) {\n\t\t\t\tdata->current_profile_setting.bupdate_mclk = tmp.bupdate_mclk;\n\t\t\t\tdata->current_profile_setting.mclk_up_hyst = tmp.mclk_up_hyst;\n\t\t\t\tdata->current_profile_setting.mclk_down_hyst = tmp.mclk_down_hyst;\n\t\t\t\tdata->current_profile_setting.mclk_activity = tmp.mclk_activity;\n\t\t\t}\n\t\t\tsmu7_patch_compute_profile_mode(hwmgr, mode);\n\t\t\thwmgr->power_profile_mode = mode;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int smu7_get_performance_level(struct pp_hwmgr *hwmgr, const struct pp_hw_power_state *state,\n\t\t\t\tPHM_PerformanceLevelDesignation designation, uint32_t index,\n\t\t\t\tPHM_PerformanceLevel *level)\n{\n\tconst struct smu7_power_state *ps;\n\tuint32_t i;\n\n\tif (level == NULL || hwmgr == NULL || state == NULL)\n\t\treturn -EINVAL;\n\n\tps = cast_const_phw_smu7_power_state(state);\n\n\ti = index > ps->performance_level_count - 1 ?\n\t\t\tps->performance_level_count - 1 : index;\n\n\tlevel->coreClock = ps->performance_levels[i].engine_clock;\n\tlevel->memory_clock = ps->performance_levels[i].memory_clock;\n\n\treturn 0;\n}\n\nstatic int smu7_power_off_asic(struct pp_hwmgr *hwmgr)\n{\n\tint result;\n\n\tresult = smu7_disable_dpm_tasks(hwmgr);\n\tPP_ASSERT_WITH_CODE((0 == result),\n\t\t\t\"[disable_dpm_tasks] Failed to disable DPM!\",\n\t\t\t);\n\n\treturn result;\n}\n\nstatic const struct pp_hwmgr_func smu7_hwmgr_funcs = {\n\t.backend_init = &smu7_hwmgr_backend_init,\n\t.backend_fini = &smu7_hwmgr_backend_fini,\n\t.asic_setup = &smu7_setup_asic_task,\n\t.dynamic_state_management_enable = &smu7_enable_dpm_tasks,\n\t.apply_state_adjust_rules = smu7_apply_state_adjust_rules,\n\t.force_dpm_level = &smu7_force_dpm_level,\n\t.power_state_set = smu7_set_power_state_tasks,\n\t.get_power_state_size = smu7_get_power_state_size,\n\t.get_mclk = smu7_dpm_get_mclk,\n\t.get_sclk = smu7_dpm_get_sclk,\n\t.patch_boot_state = smu7_dpm_patch_boot_state,\n\t.get_pp_table_entry = smu7_get_pp_table_entry,\n\t.get_num_of_pp_table_entries = smu7_get_number_of_powerplay_table_entries,\n\t.powerdown_uvd = smu7_powerdown_uvd,\n\t.powergate_uvd = smu7_powergate_uvd,\n\t.powergate_vce = smu7_powergate_vce,\n\t.disable_clock_power_gating = smu7_disable_clock_power_gating,\n\t.update_clock_gatings = smu7_update_clock_gatings,\n\t.notify_smc_display_config_after_ps_adjustment = smu7_notify_smc_display_config_after_ps_adjustment,\n\t.display_config_changed = smu7_display_configuration_changed_task,\n\t.set_max_fan_pwm_output = smu7_set_max_fan_pwm_output,\n\t.set_max_fan_rpm_output = smu7_set_max_fan_rpm_output,\n\t.stop_thermal_controller = smu7_thermal_stop_thermal_controller,\n\t.get_fan_speed_info = smu7_fan_ctrl_get_fan_speed_info,\n\t.get_fan_speed_pwm = smu7_fan_ctrl_get_fan_speed_pwm,\n\t.set_fan_speed_pwm = smu7_fan_ctrl_set_fan_speed_pwm,\n\t.reset_fan_speed_to_default = smu7_fan_ctrl_reset_fan_speed_to_default,\n\t.get_fan_speed_rpm = smu7_fan_ctrl_get_fan_speed_rpm,\n\t.set_fan_speed_rpm = smu7_fan_ctrl_set_fan_speed_rpm,\n\t.uninitialize_thermal_controller = smu7_thermal_ctrl_uninitialize_thermal_controller,\n\t.register_irq_handlers = smu7_register_irq_handlers,\n\t.check_smc_update_required_for_display_configuration = smu7_check_smc_update_required_for_display_configuration,\n\t.check_states_equal = smu7_check_states_equal,\n\t.set_fan_control_mode = smu7_set_fan_control_mode,\n\t.get_fan_control_mode = smu7_get_fan_control_mode,\n\t.force_clock_level = smu7_force_clock_level,\n\t.print_clock_levels = smu7_print_clock_levels,\n\t.powergate_gfx = smu7_powergate_gfx,\n\t.get_sclk_od = smu7_get_sclk_od,\n\t.set_sclk_od = smu7_set_sclk_od,\n\t.get_mclk_od = smu7_get_mclk_od,\n\t.set_mclk_od = smu7_set_mclk_od,\n\t.get_clock_by_type = smu7_get_clock_by_type,\n\t.get_clock_by_type_with_latency = smu7_get_clock_by_type_with_latency,\n\t.set_watermarks_for_clocks_ranges = smu7_set_watermarks_for_clocks_ranges,\n\t.read_sensor = smu7_read_sensor,\n\t.dynamic_state_management_disable = smu7_disable_dpm_tasks,\n\t.avfs_control = smu7_avfs_control,\n\t.disable_smc_firmware_ctf = smu7_thermal_disable_alert,\n\t.start_thermal_controller = smu7_start_thermal_controller,\n\t.notify_cac_buffer_info = smu7_notify_cac_buffer_info,\n\t.get_max_high_clocks = smu7_get_max_high_clocks,\n\t.get_thermal_temperature_range = smu7_get_thermal_temperature_range,\n\t.odn_edit_dpm_table = smu7_odn_edit_dpm_table,\n\t.set_power_limit = smu7_set_power_limit,\n\t.get_power_profile_mode = smu7_get_power_profile_mode,\n\t.set_power_profile_mode = smu7_set_power_profile_mode,\n\t.get_performance_level = smu7_get_performance_level,\n\t.get_asic_baco_capability = smu7_baco_get_capability,\n\t.get_asic_baco_state = smu7_baco_get_state,\n\t.set_asic_baco_state = smu7_baco_set_state,\n\t.power_off_asic = smu7_power_off_asic,\n};\n\nuint8_t smu7_get_sleep_divider_id_from_clock(uint32_t clock,\n\t\tuint32_t clock_insr)\n{\n\tuint8_t i;\n\tuint32_t temp;\n\tuint32_t min = max(clock_insr, (uint32_t)SMU7_MINIMUM_ENGINE_CLOCK);\n\n\tPP_ASSERT_WITH_CODE((clock >= min), \"Engine clock can't satisfy stutter requirement!\", return 0);\n\tfor (i = SMU7_MAX_DEEPSLEEP_DIVIDER_ID;  ; i--) {\n\t\ttemp = clock >> i;\n\n\t\tif (temp >= min || i == 0)\n\t\t\tbreak;\n\t}\n\treturn i;\n}\n\nint smu7_init_function_pointers(struct pp_hwmgr *hwmgr)\n{\n\thwmgr->hwmgr_func = &smu7_hwmgr_funcs;\n\tif (hwmgr->pp_table_version == PP_TABLE_V0)\n\t\thwmgr->pptable_func = &pptable_funcs;\n\telse if (hwmgr->pp_table_version == PP_TABLE_V1)\n\t\thwmgr->pptable_func = &pptable_v1_0_funcs;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}