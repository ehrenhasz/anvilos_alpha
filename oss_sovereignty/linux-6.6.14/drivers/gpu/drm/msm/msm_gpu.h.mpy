{
  "module_name": "msm_gpu.h",
  "hash_id": "5ec2616fa06d08044db3f18a7f48bd186139641a04f27dfc87cb7624db8695e6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/msm_gpu.h",
  "human_readable_source": " \n \n\n#ifndef __MSM_GPU_H__\n#define __MSM_GPU_H__\n\n#include <linux/adreno-smmu-priv.h>\n#include <linux/clk.h>\n#include <linux/devfreq.h>\n#include <linux/interconnect.h>\n#include <linux/pm_opp.h>\n#include <linux/regulator/consumer.h>\n\n#include \"msm_drv.h\"\n#include \"msm_fence.h\"\n#include \"msm_ringbuffer.h\"\n#include \"msm_gem.h\"\n\nstruct msm_gem_submit;\nstruct msm_gpu_perfcntr;\nstruct msm_gpu_state;\nstruct msm_file_private;\n\nstruct msm_gpu_config {\n\tconst char *ioname;\n\tunsigned int nr_rings;\n};\n\n \nstruct msm_gpu_funcs {\n\tint (*get_param)(struct msm_gpu *gpu, struct msm_file_private *ctx,\n\t\t\t uint32_t param, uint64_t *value, uint32_t *len);\n\tint (*set_param)(struct msm_gpu *gpu, struct msm_file_private *ctx,\n\t\t\t uint32_t param, uint64_t value, uint32_t len);\n\tint (*hw_init)(struct msm_gpu *gpu);\n\n\t \n\tint (*ucode_load)(struct msm_gpu *gpu);\n\n\tint (*pm_suspend)(struct msm_gpu *gpu);\n\tint (*pm_resume)(struct msm_gpu *gpu);\n\tvoid (*submit)(struct msm_gpu *gpu, struct msm_gem_submit *submit);\n\tvoid (*flush)(struct msm_gpu *gpu, struct msm_ringbuffer *ring);\n\tirqreturn_t (*irq)(struct msm_gpu *irq);\n\tstruct msm_ringbuffer *(*active_ring)(struct msm_gpu *gpu);\n\tvoid (*recover)(struct msm_gpu *gpu);\n\tvoid (*destroy)(struct msm_gpu *gpu);\n#if defined(CONFIG_DEBUG_FS) || defined(CONFIG_DEV_COREDUMP)\n\t \n\tvoid (*show)(struct msm_gpu *gpu, struct msm_gpu_state *state,\n\t\t\tstruct drm_printer *p);\n\t \n\tvoid (*debugfs_init)(struct msm_gpu *gpu, struct drm_minor *minor);\n#endif\n\t \n\tu64 (*gpu_busy)(struct msm_gpu *gpu, unsigned long *out_sample_rate);\n\tstruct msm_gpu_state *(*gpu_state_get)(struct msm_gpu *gpu);\n\tint (*gpu_state_put)(struct msm_gpu_state *state);\n\tunsigned long (*gpu_get_freq)(struct msm_gpu *gpu);\n\t \n\tvoid (*gpu_set_freq)(struct msm_gpu *gpu, struct dev_pm_opp *opp,\n\t\t\t     bool suspended);\n\tstruct msm_gem_address_space *(*create_address_space)\n\t\t(struct msm_gpu *gpu, struct platform_device *pdev);\n\tstruct msm_gem_address_space *(*create_private_address_space)\n\t\t(struct msm_gpu *gpu);\n\tuint32_t (*get_rptr)(struct msm_gpu *gpu, struct msm_ringbuffer *ring);\n\n\t \n\tbool (*progress)(struct msm_gpu *gpu, struct msm_ringbuffer *ring);\n};\n\n \nstruct msm_gpu_fault_info {\n\tu64 ttbr0;\n\tunsigned long iova;\n\tint flags;\n\tconst char *type;\n\tconst char *block;\n};\n\n \nstruct msm_gpu_devfreq {\n\t \n\tstruct devfreq *devfreq;\n\n\t \n\tstruct mutex lock;\n\n\t \n\tunsigned long idle_freq;\n\n\t \n\tstruct dev_pm_qos_request boost_freq;\n\n\t \n\tu64 busy_cycles;\n\n\t \n\tktime_t time;\n\n\t \n\tktime_t idle_time;\n\n\t \n\tstruct msm_hrtimer_work idle_work;\n\n\t \n\tstruct msm_hrtimer_work boost_work;\n\n\t \n\tbool suspended;\n};\n\nstruct msm_gpu {\n\tconst char *name;\n\tstruct drm_device *dev;\n\tstruct platform_device *pdev;\n\tconst struct msm_gpu_funcs *funcs;\n\n\tstruct adreno_smmu_priv adreno_smmu;\n\n\t \n\tspinlock_t perf_lock;\n\tbool perfcntr_active;\n\tstruct {\n\t\tbool active;\n\t\tktime_t time;\n\t} last_sample;\n\tuint32_t totaltime, activetime;     \n\tuint32_t last_cntrs[5];             \n\tconst struct msm_gpu_perfcntr *perfcntrs;\n\tuint32_t num_perfcntrs;\n\n\tstruct msm_ringbuffer *rb[MSM_GPU_MAX_RINGS];\n\tint nr_rings;\n\n\t \n\trefcount_t sysprof_active;\n\n\t \n\tint cur_ctx_seqno;\n\n\t \n\tstruct mutex lock;\n\n\t \n\tint active_submits;\n\n\t \n\tstruct mutex active_lock;\n\n\t \n\tbool needs_hw_init;\n\n\t \n\tint global_faults;\n\n\tvoid __iomem *mmio;\n\tint irq;\n\n\tstruct msm_gem_address_space *aspace;\n\n\t \n\tstruct regulator *gpu_reg, *gpu_cx;\n\tstruct clk_bulk_data *grp_clks;\n\tint nr_clocks;\n\tstruct clk *ebi1_clk, *core_clk, *rbbmtimer_clk;\n\tuint32_t fast_rate;\n\n\t \n#define DRM_MSM_INACTIVE_PERIOD   66  \n\n#define DRM_MSM_HANGCHECK_DEFAULT_PERIOD 500  \n#define DRM_MSM_HANGCHECK_PROGRESS_RETRIES 3\n\tstruct timer_list hangcheck_timer;\n\n\t \n\tstruct msm_gpu_fault_info fault_info;\n\n\t \n\tstruct kthread_work fault_work;\n\n\t \n\tstruct kthread_work recover_work;\n\n\t \n\twait_queue_head_t retire_event;\n\n\t \n\tstruct kthread_work retire_work;\n\n\t \n\tstruct kthread_worker *worker;\n\n\tstruct drm_gem_object *memptrs_bo;\n\n\tstruct msm_gpu_devfreq devfreq;\n\n\tuint32_t suspend_count;\n\n\tstruct msm_gpu_state *crashstate;\n\n\t \n\tbool hw_apriv;\n\n\t \n\tbool allow_relocs;\n\n\tstruct thermal_cooling_device *cooling;\n};\n\nstatic inline struct msm_gpu *dev_to_gpu(struct device *dev)\n{\n\tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(dev);\n\n\tif (!adreno_smmu)\n\t\treturn NULL;\n\n\treturn container_of(adreno_smmu, struct msm_gpu, adreno_smmu);\n}\n\n \n#define MSM_GPU_RINGBUFFER_SZ SZ_32K\n#define MSM_GPU_RINGBUFFER_BLKSIZE 32\n\n#define MSM_GPU_RB_CNTL_DEFAULT \\\n\t\t(AXXX_CP_RB_CNTL_BUFSZ(ilog2(MSM_GPU_RINGBUFFER_SZ / 8)) | \\\n\t\tAXXX_CP_RB_CNTL_BLKSZ(ilog2(MSM_GPU_RINGBUFFER_BLKSIZE / 8)))\n\nstatic inline bool msm_gpu_active(struct msm_gpu *gpu)\n{\n\tint i;\n\n\tfor (i = 0; i < gpu->nr_rings; i++) {\n\t\tstruct msm_ringbuffer *ring = gpu->rb[i];\n\n\t\tif (fence_after(ring->fctx->last_fence, ring->memptrs->fence))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \n\nstruct msm_gpu_perfcntr {\n\tuint32_t select_reg;\n\tuint32_t sample_reg;\n\tuint32_t select_val;\n\tconst char *name;\n};\n\n \n#define NR_SCHED_PRIORITIES (1 + DRM_SCHED_PRIORITY_HIGH - DRM_SCHED_PRIORITY_MIN)\n\n \nstruct msm_file_private {\n\trwlock_t queuelock;\n\tstruct list_head submitqueues;\n\tint queueid;\n\tstruct msm_gem_address_space *aspace;\n\tstruct kref ref;\n\tint seqno;\n\n\t \n\tint sysprof;\n\n\t \n\tchar *comm;\n\n\t \n\tchar *cmdline;\n\n\t \n\tuint64_t elapsed_ns;\n\n\t \n\tuint64_t cycles;\n\n\t \n\tstruct drm_sched_entity *entities[NR_SCHED_PRIORITIES * MSM_GPU_MAX_RINGS];\n};\n\n \nstatic inline int msm_gpu_convert_priority(struct msm_gpu *gpu, int prio,\n\t\tunsigned *ring_nr, enum drm_sched_priority *sched_prio)\n{\n\tunsigned rn, sp;\n\n\trn = div_u64_rem(prio, NR_SCHED_PRIORITIES, &sp);\n\n\t \n\tsp = NR_SCHED_PRIORITIES - sp - 1;\n\n\tif (rn >= gpu->nr_rings)\n\t\treturn -EINVAL;\n\n\t*ring_nr = rn;\n\t*sched_prio = sp;\n\n\treturn 0;\n}\n\n \nstruct msm_gpu_submitqueue {\n\tint id;\n\tu32 flags;\n\tu32 ring_nr;\n\tint faults;\n\tuint32_t last_fence;\n\tstruct msm_file_private *ctx;\n\tstruct list_head node;\n\tstruct idr fence_idr;\n\tstruct spinlock idr_lock;\n\tstruct mutex lock;\n\tstruct kref ref;\n\tstruct drm_sched_entity *entity;\n};\n\nstruct msm_gpu_state_bo {\n\tu64 iova;\n\tsize_t size;\n\tvoid *data;\n\tbool encoded;\n\tchar name[32];\n};\n\nstruct msm_gpu_state {\n\tstruct kref ref;\n\tstruct timespec64 time;\n\n\tstruct {\n\t\tu64 iova;\n\t\tu32 fence;\n\t\tu32 seqno;\n\t\tu32 rptr;\n\t\tu32 wptr;\n\t\tvoid *data;\n\t\tint data_size;\n\t\tbool encoded;\n\t} ring[MSM_GPU_MAX_RINGS];\n\n\tint nr_registers;\n\tu32 *registers;\n\n\tu32 rbbm_status;\n\n\tchar *comm;\n\tchar *cmd;\n\n\tstruct msm_gpu_fault_info fault_info;\n\n\tint nr_bos;\n\tstruct msm_gpu_state_bo *bos;\n};\n\nstatic inline void gpu_write(struct msm_gpu *gpu, u32 reg, u32 data)\n{\n\tmsm_writel(data, gpu->mmio + (reg << 2));\n}\n\nstatic inline u32 gpu_read(struct msm_gpu *gpu, u32 reg)\n{\n\treturn msm_readl(gpu->mmio + (reg << 2));\n}\n\nstatic inline void gpu_rmw(struct msm_gpu *gpu, u32 reg, u32 mask, u32 or)\n{\n\tmsm_rmw(gpu->mmio + (reg << 2), mask, or);\n}\n\nstatic inline u64 gpu_read64(struct msm_gpu *gpu, u32 reg)\n{\n\tu64 val;\n\n\t \n\n\t \n\tval = (u64) msm_readl(gpu->mmio + (reg << 2));\n\tval |= ((u64) msm_readl(gpu->mmio + ((reg + 1) << 2)) << 32);\n\n\treturn val;\n}\n\nstatic inline void gpu_write64(struct msm_gpu *gpu, u32 reg, u64 val)\n{\n\t \n\tmsm_writel(lower_32_bits(val), gpu->mmio + (reg << 2));\n\tmsm_writel(upper_32_bits(val), gpu->mmio + ((reg + 1) << 2));\n}\n\nint msm_gpu_pm_suspend(struct msm_gpu *gpu);\nint msm_gpu_pm_resume(struct msm_gpu *gpu);\n\nvoid msm_gpu_show_fdinfo(struct msm_gpu *gpu, struct msm_file_private *ctx,\n\t\t\t struct drm_printer *p);\n\nint msm_submitqueue_init(struct drm_device *drm, struct msm_file_private *ctx);\nstruct msm_gpu_submitqueue *msm_submitqueue_get(struct msm_file_private *ctx,\n\t\tu32 id);\nint msm_submitqueue_create(struct drm_device *drm,\n\t\tstruct msm_file_private *ctx,\n\t\tu32 prio, u32 flags, u32 *id);\nint msm_submitqueue_query(struct drm_device *drm, struct msm_file_private *ctx,\n\t\tstruct drm_msm_submitqueue_query *args);\nint msm_submitqueue_remove(struct msm_file_private *ctx, u32 id);\nvoid msm_submitqueue_close(struct msm_file_private *ctx);\n\nvoid msm_submitqueue_destroy(struct kref *kref);\n\nint msm_file_private_set_sysprof(struct msm_file_private *ctx,\n\t\t\t\t struct msm_gpu *gpu, int sysprof);\nvoid __msm_file_private_destroy(struct kref *kref);\n\nstatic inline void msm_file_private_put(struct msm_file_private *ctx)\n{\n\tkref_put(&ctx->ref, __msm_file_private_destroy);\n}\n\nstatic inline struct msm_file_private *msm_file_private_get(\n\tstruct msm_file_private *ctx)\n{\n\tkref_get(&ctx->ref);\n\treturn ctx;\n}\n\nvoid msm_devfreq_init(struct msm_gpu *gpu);\nvoid msm_devfreq_cleanup(struct msm_gpu *gpu);\nvoid msm_devfreq_resume(struct msm_gpu *gpu);\nvoid msm_devfreq_suspend(struct msm_gpu *gpu);\nvoid msm_devfreq_boost(struct msm_gpu *gpu, unsigned factor);\nvoid msm_devfreq_active(struct msm_gpu *gpu);\nvoid msm_devfreq_idle(struct msm_gpu *gpu);\n\nint msm_gpu_hw_init(struct msm_gpu *gpu);\n\nvoid msm_gpu_perfcntr_start(struct msm_gpu *gpu);\nvoid msm_gpu_perfcntr_stop(struct msm_gpu *gpu);\nint msm_gpu_perfcntr_sample(struct msm_gpu *gpu, uint32_t *activetime,\n\t\tuint32_t *totaltime, uint32_t ncntrs, uint32_t *cntrs);\n\nvoid msm_gpu_retire(struct msm_gpu *gpu);\nvoid msm_gpu_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit);\n\nint msm_gpu_init(struct drm_device *drm, struct platform_device *pdev,\n\t\tstruct msm_gpu *gpu, const struct msm_gpu_funcs *funcs,\n\t\tconst char *name, struct msm_gpu_config *config);\n\nstruct msm_gem_address_space *\nmsm_gpu_create_private_address_space(struct msm_gpu *gpu, struct task_struct *task);\n\nvoid msm_gpu_cleanup(struct msm_gpu *gpu);\n\nstruct msm_gpu *adreno_load_gpu(struct drm_device *dev);\nvoid __init adreno_register(void);\nvoid __exit adreno_unregister(void);\n\nstatic inline void msm_submitqueue_put(struct msm_gpu_submitqueue *queue)\n{\n\tif (queue)\n\t\tkref_put(&queue->ref, msm_submitqueue_destroy);\n}\n\nstatic inline struct msm_gpu_state *msm_gpu_crashstate_get(struct msm_gpu *gpu)\n{\n\tstruct msm_gpu_state *state = NULL;\n\n\tmutex_lock(&gpu->lock);\n\n\tif (gpu->crashstate) {\n\t\tkref_get(&gpu->crashstate->ref);\n\t\tstate = gpu->crashstate;\n\t}\n\n\tmutex_unlock(&gpu->lock);\n\n\treturn state;\n}\n\nstatic inline void msm_gpu_crashstate_put(struct msm_gpu *gpu)\n{\n\tmutex_lock(&gpu->lock);\n\n\tif (gpu->crashstate) {\n\t\tif (gpu->funcs->gpu_state_put(gpu->crashstate))\n\t\t\tgpu->crashstate = NULL;\n\t}\n\n\tmutex_unlock(&gpu->lock);\n}\n\n \n#define check_apriv(gpu, flags) \\\n\t(((gpu)->hw_apriv ? MSM_BO_MAP_PRIV : 0) | (flags))\n\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}