{
  "module_name": "adreno_gpu.c",
  "hash_id": "9b4a1f66d57f10bf4de12a082462de5f32344c43ca2bb11fc592e6801b4a46ee",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/adreno/adreno_gpu.c",
  "human_readable_source": "\n \n\n#include <linux/ascii85.h>\n#include <linux/interconnect.h>\n#include <linux/firmware/qcom/qcom_scm.h>\n#include <linux/kernel.h>\n#include <linux/of_address.h>\n#include <linux/pm_opp.h>\n#include <linux/slab.h>\n#include <linux/soc/qcom/mdt_loader.h>\n#include <linux/nvmem-consumer.h>\n#include <soc/qcom/ocmem.h>\n#include \"adreno_gpu.h\"\n#include \"a6xx_gpu.h\"\n#include \"msm_gem.h\"\n#include \"msm_mmu.h\"\n\nstatic u64 address_space_size = 0;\nMODULE_PARM_DESC(address_space_size, \"Override for size of processes private GPU address space\");\nmodule_param(address_space_size, ullong, 0600);\n\nstatic bool zap_available = true;\n\nstatic int zap_shader_load_mdt(struct msm_gpu *gpu, const char *fwname,\n\t\tu32 pasid)\n{\n\tstruct device *dev = &gpu->pdev->dev;\n\tconst struct firmware *fw;\n\tconst char *signed_fwname = NULL;\n\tstruct device_node *np, *mem_np;\n\tstruct resource r;\n\tphys_addr_t mem_phys;\n\tssize_t mem_size;\n\tvoid *mem_region = NULL;\n\tint ret;\n\n\tif (!IS_ENABLED(CONFIG_ARCH_QCOM)) {\n\t\tzap_available = false;\n\t\treturn -EINVAL;\n\t}\n\n\tnp = of_get_child_by_name(dev->of_node, \"zap-shader\");\n\tif (!np) {\n\t\tzap_available = false;\n\t\treturn -ENODEV;\n\t}\n\n\tmem_np = of_parse_phandle(np, \"memory-region\", 0);\n\tof_node_put(np);\n\tif (!mem_np) {\n\t\tzap_available = false;\n\t\treturn -EINVAL;\n\t}\n\n\tret = of_address_to_resource(mem_np, 0, &r);\n\tof_node_put(mem_np);\n\tif (ret)\n\t\treturn ret;\n\n\tmem_phys = r.start;\n\n\t \n\tof_property_read_string_index(np, \"firmware-name\", 0, &signed_fwname);\n\tif (signed_fwname) {\n\t\tfwname = signed_fwname;\n\t\tret = request_firmware_direct(&fw, fwname, gpu->dev->dev);\n\t\tif (ret)\n\t\t\tfw = ERR_PTR(ret);\n\t} else if (fwname) {\n\t\t \n\t\tfw = adreno_request_fw(to_adreno_gpu(gpu), fwname);\n\t} else {\n\t\t \n\t\treturn -ENODEV;\n\t}\n\n\tif (IS_ERR(fw)) {\n\t\tDRM_DEV_ERROR(dev, \"Unable to load %s\\n\", fwname);\n\t\treturn PTR_ERR(fw);\n\t}\n\n\t \n\tmem_size = qcom_mdt_get_size(fw);\n\tif (mem_size < 0) {\n\t\tret = mem_size;\n\t\tgoto out;\n\t}\n\n\tif (mem_size > resource_size(&r)) {\n\t\tDRM_DEV_ERROR(dev,\n\t\t\t\"memory region is too small to load the MDT\\n\");\n\t\tret = -E2BIG;\n\t\tgoto out;\n\t}\n\n\t \n\tmem_region = memremap(mem_phys, mem_size,  MEMREMAP_WC);\n\tif (!mem_region) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (signed_fwname || (to_adreno_gpu(gpu)->fwloc == FW_LOCATION_LEGACY)) {\n\t\tret = qcom_mdt_load(dev, fw, fwname, pasid,\n\t\t\t\tmem_region, mem_phys, mem_size, NULL);\n\t} else {\n\t\tchar *newname;\n\n\t\tnewname = kasprintf(GFP_KERNEL, \"qcom/%s\", fwname);\n\n\t\tret = qcom_mdt_load(dev, fw, newname, pasid,\n\t\t\t\tmem_region, mem_phys, mem_size, NULL);\n\t\tkfree(newname);\n\t}\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = qcom_scm_pas_auth_and_reset(pasid);\n\n\t \n\tif (ret == -EOPNOTSUPP)\n\t\tzap_available = false;\n\telse if (ret)\n\t\tDRM_DEV_ERROR(dev, \"Unable to authorize the image\\n\");\n\nout:\n\tif (mem_region)\n\t\tmemunmap(mem_region);\n\n\trelease_firmware(fw);\n\n\treturn ret;\n}\n\nint adreno_zap_shader_load(struct msm_gpu *gpu, u32 pasid)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct platform_device *pdev = gpu->pdev;\n\n\t \n\tif (!zap_available)\n\t\treturn -ENODEV;\n\n\t \n\tif (!qcom_scm_is_available()) {\n\t\tDRM_DEV_ERROR(&pdev->dev, \"SCM is not available\\n\");\n\t\treturn -EPROBE_DEFER;\n\t}\n\n\treturn zap_shader_load_mdt(gpu, adreno_gpu->info->zapfw, pasid);\n}\n\nstruct msm_gem_address_space *\nadreno_create_address_space(struct msm_gpu *gpu,\n\t\t\t    struct platform_device *pdev)\n{\n\treturn adreno_iommu_create_address_space(gpu, pdev, 0);\n}\n\nstruct msm_gem_address_space *\nadreno_iommu_create_address_space(struct msm_gpu *gpu,\n\t\t\t\t  struct platform_device *pdev,\n\t\t\t\t  unsigned long quirks)\n{\n\tstruct iommu_domain_geometry *geometry;\n\tstruct msm_mmu *mmu;\n\tstruct msm_gem_address_space *aspace;\n\tu64 start, size;\n\n\tmmu = msm_iommu_gpu_new(&pdev->dev, gpu, quirks);\n\tif (IS_ERR_OR_NULL(mmu))\n\t\treturn ERR_CAST(mmu);\n\n\tgeometry = msm_iommu_get_geometry(mmu);\n\tif (IS_ERR(geometry))\n\t\treturn ERR_CAST(geometry);\n\n\t \n\tstart = max_t(u64, SZ_16M, geometry->aperture_start);\n\tsize = geometry->aperture_end - start + 1;\n\n\taspace = msm_gem_address_space_create(mmu, \"gpu\",\n\t\tstart & GENMASK_ULL(48, 0), size);\n\n\tif (IS_ERR(aspace) && !IS_ERR(mmu))\n\t\tmmu->funcs->destroy(mmu);\n\n\treturn aspace;\n}\n\nu64 adreno_private_address_space_size(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\n\tif (address_space_size)\n\t\treturn address_space_size;\n\n\tif (adreno_gpu->info->address_space_size)\n\t\treturn adreno_gpu->info->address_space_size;\n\n\treturn SZ_4G;\n}\n\n#define ARM_SMMU_FSR_TF                 BIT(1)\n#define ARM_SMMU_FSR_PF\t\t\tBIT(3)\n#define ARM_SMMU_FSR_EF\t\t\tBIT(4)\n\nint adreno_fault_handler(struct msm_gpu *gpu, unsigned long iova, int flags,\n\t\t\t struct adreno_smmu_fault_info *info, const char *block,\n\t\t\t u32 scratch[4])\n{\n\tconst char *type = \"UNKNOWN\";\n\tbool do_devcoredump = info && !READ_ONCE(gpu->crashstate);\n\n\t \n\tif (!do_devcoredump) {\n\t\tgpu->aspace->mmu->funcs->resume_translation(gpu->aspace->mmu);\n\t}\n\n\t \n\tif (!info) {\n\t\tpr_warn_ratelimited(\"*** gpu fault: iova=%.16lx flags=%d (%u,%u,%u,%u)\\n\",\n\t\t\tiova, flags,\n\t\t\tscratch[0], scratch[1], scratch[2], scratch[3]);\n\n\t\treturn 0;\n\t}\n\n\tif (info->fsr & ARM_SMMU_FSR_TF)\n\t\ttype = \"TRANSLATION\";\n\telse if (info->fsr & ARM_SMMU_FSR_PF)\n\t\ttype = \"PERMISSION\";\n\telse if (info->fsr & ARM_SMMU_FSR_EF)\n\t\ttype = \"EXTERNAL\";\n\n\tpr_warn_ratelimited(\"*** gpu fault: ttbr0=%.16llx iova=%.16lx dir=%s type=%s source=%s (%u,%u,%u,%u)\\n\",\n\t\t\tinfo->ttbr0, iova,\n\t\t\tflags & IOMMU_FAULT_WRITE ? \"WRITE\" : \"READ\",\n\t\t\ttype, block,\n\t\t\tscratch[0], scratch[1], scratch[2], scratch[3]);\n\n\tif (do_devcoredump) {\n\t\t \n\t\tdel_timer(&gpu->hangcheck_timer);\n\n\t\tgpu->fault_info.ttbr0 = info->ttbr0;\n\t\tgpu->fault_info.iova  = iova;\n\t\tgpu->fault_info.flags = flags;\n\t\tgpu->fault_info.type  = type;\n\t\tgpu->fault_info.block = block;\n\n\t\tkthread_queue_work(gpu->worker, &gpu->fault_work);\n\t}\n\n\treturn 0;\n}\n\nint adreno_get_param(struct msm_gpu *gpu, struct msm_file_private *ctx,\n\t\t     uint32_t param, uint64_t *value, uint32_t *len)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\n\t \n\tif (*len != 0)\n\t\treturn -EINVAL;\n\n\tswitch (param) {\n\tcase MSM_PARAM_GPU_ID:\n\t\t*value = adreno_gpu->info->revn;\n\t\treturn 0;\n\tcase MSM_PARAM_GMEM_SIZE:\n\t\t*value = adreno_gpu->info->gmem;\n\t\treturn 0;\n\tcase MSM_PARAM_GMEM_BASE:\n\t\t*value = !adreno_is_a650_family(adreno_gpu) ? 0x100000 : 0;\n\t\treturn 0;\n\tcase MSM_PARAM_CHIP_ID:\n\t\t*value = adreno_gpu->chip_id;\n\t\tif (!adreno_gpu->info->revn)\n\t\t\t*value |= ((uint64_t) adreno_gpu->speedbin) << 32;\n\t\treturn 0;\n\tcase MSM_PARAM_MAX_FREQ:\n\t\t*value = adreno_gpu->base.fast_rate;\n\t\treturn 0;\n\tcase MSM_PARAM_TIMESTAMP:\n\t\tif (adreno_gpu->funcs->get_timestamp) {\n\t\t\tint ret;\n\n\t\t\tpm_runtime_get_sync(&gpu->pdev->dev);\n\t\t\tret = adreno_gpu->funcs->get_timestamp(gpu, value);\n\t\t\tpm_runtime_put_autosuspend(&gpu->pdev->dev);\n\n\t\t\treturn ret;\n\t\t}\n\t\treturn -EINVAL;\n\tcase MSM_PARAM_PRIORITIES:\n\t\t*value = gpu->nr_rings * NR_SCHED_PRIORITIES;\n\t\treturn 0;\n\tcase MSM_PARAM_PP_PGTABLE:\n\t\t*value = 0;\n\t\treturn 0;\n\tcase MSM_PARAM_FAULTS:\n\t\tif (ctx->aspace)\n\t\t\t*value = gpu->global_faults + ctx->aspace->faults;\n\t\telse\n\t\t\t*value = gpu->global_faults;\n\t\treturn 0;\n\tcase MSM_PARAM_SUSPENDS:\n\t\t*value = gpu->suspend_count;\n\t\treturn 0;\n\tcase MSM_PARAM_VA_START:\n\t\tif (ctx->aspace == gpu->aspace)\n\t\t\treturn -EINVAL;\n\t\t*value = ctx->aspace->va_start;\n\t\treturn 0;\n\tcase MSM_PARAM_VA_SIZE:\n\t\tif (ctx->aspace == gpu->aspace)\n\t\t\treturn -EINVAL;\n\t\t*value = ctx->aspace->va_size;\n\t\treturn 0;\n\tdefault:\n\t\tDBG(\"%s: invalid param: %u\", gpu->name, param);\n\t\treturn -EINVAL;\n\t}\n}\n\nint adreno_set_param(struct msm_gpu *gpu, struct msm_file_private *ctx,\n\t\t     uint32_t param, uint64_t value, uint32_t len)\n{\n\tswitch (param) {\n\tcase MSM_PARAM_COMM:\n\tcase MSM_PARAM_CMDLINE:\n\t\t \n\t\tif (len > PAGE_SIZE)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tif (len != 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\tswitch (param) {\n\tcase MSM_PARAM_COMM:\n\tcase MSM_PARAM_CMDLINE: {\n\t\tchar *str, **paramp;\n\n\t\tstr = memdup_user_nul(u64_to_user_ptr(value), len);\n\t\tif (IS_ERR(str))\n\t\t\treturn PTR_ERR(str);\n\n\t\tmutex_lock(&gpu->lock);\n\n\t\tif (param == MSM_PARAM_COMM) {\n\t\t\tparamp = &ctx->comm;\n\t\t} else {\n\t\t\tparamp = &ctx->cmdline;\n\t\t}\n\n\t\tkfree(*paramp);\n\t\t*paramp = str;\n\n\t\tmutex_unlock(&gpu->lock);\n\n\t\treturn 0;\n\t}\n\tcase MSM_PARAM_SYSPROF:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EPERM;\n\t\treturn msm_file_private_set_sysprof(ctx, gpu, value);\n\tdefault:\n\t\tDBG(\"%s: invalid param: %u\", gpu->name, param);\n\t\treturn -EINVAL;\n\t}\n}\n\nconst struct firmware *\nadreno_request_fw(struct adreno_gpu *adreno_gpu, const char *fwname)\n{\n\tstruct drm_device *drm = adreno_gpu->base.dev;\n\tconst struct firmware *fw = NULL;\n\tchar *newname;\n\tint ret;\n\n\tnewname = kasprintf(GFP_KERNEL, \"qcom/%s\", fwname);\n\tif (!newname)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tif ((adreno_gpu->fwloc == FW_LOCATION_UNKNOWN) ||\n\t    (adreno_gpu->fwloc == FW_LOCATION_NEW)) {\n\n\t\tret = request_firmware_direct(&fw, newname, drm->dev);\n\t\tif (!ret) {\n\t\t\tDRM_DEV_INFO(drm->dev, \"loaded %s from new location\\n\",\n\t\t\t\tnewname);\n\t\t\tadreno_gpu->fwloc = FW_LOCATION_NEW;\n\t\t\tgoto out;\n\t\t} else if (adreno_gpu->fwloc != FW_LOCATION_UNKNOWN) {\n\t\t\tDRM_DEV_ERROR(drm->dev, \"failed to load %s: %d\\n\",\n\t\t\t\tnewname, ret);\n\t\t\tfw = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif ((adreno_gpu->fwloc == FW_LOCATION_UNKNOWN) ||\n\t    (adreno_gpu->fwloc == FW_LOCATION_LEGACY)) {\n\n\t\tret = request_firmware_direct(&fw, fwname, drm->dev);\n\t\tif (!ret) {\n\t\t\tDRM_DEV_INFO(drm->dev, \"loaded %s from legacy location\\n\",\n\t\t\t\tnewname);\n\t\t\tadreno_gpu->fwloc = FW_LOCATION_LEGACY;\n\t\t\tgoto out;\n\t\t} else if (adreno_gpu->fwloc != FW_LOCATION_UNKNOWN) {\n\t\t\tDRM_DEV_ERROR(drm->dev, \"failed to load %s: %d\\n\",\n\t\t\t\tfwname, ret);\n\t\t\tfw = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif ((adreno_gpu->fwloc == FW_LOCATION_UNKNOWN) ||\n\t    (adreno_gpu->fwloc == FW_LOCATION_HELPER)) {\n\n\t\tret = request_firmware(&fw, newname, drm->dev);\n\t\tif (!ret) {\n\t\t\tDRM_DEV_INFO(drm->dev, \"loaded %s with helper\\n\",\n\t\t\t\tnewname);\n\t\t\tadreno_gpu->fwloc = FW_LOCATION_HELPER;\n\t\t\tgoto out;\n\t\t} else if (adreno_gpu->fwloc != FW_LOCATION_UNKNOWN) {\n\t\t\tDRM_DEV_ERROR(drm->dev, \"failed to load %s: %d\\n\",\n\t\t\t\tnewname, ret);\n\t\t\tfw = ERR_PTR(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tDRM_DEV_ERROR(drm->dev, \"failed to load %s\\n\", fwname);\n\tfw = ERR_PTR(-ENOENT);\nout:\n\tkfree(newname);\n\treturn fw;\n}\n\nint adreno_load_fw(struct adreno_gpu *adreno_gpu)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(adreno_gpu->info->fw); i++) {\n\t\tconst struct firmware *fw;\n\n\t\tif (!adreno_gpu->info->fw[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tif (adreno_has_gmu_wrapper(adreno_gpu) && i == ADRENO_FW_GMU)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (adreno_gpu->fw[i])\n\t\t\tcontinue;\n\n\t\tfw = adreno_request_fw(adreno_gpu, adreno_gpu->info->fw[i]);\n\t\tif (IS_ERR(fw))\n\t\t\treturn PTR_ERR(fw);\n\n\t\tadreno_gpu->fw[i] = fw;\n\t}\n\n\treturn 0;\n}\n\nstruct drm_gem_object *adreno_fw_create_bo(struct msm_gpu *gpu,\n\t\tconst struct firmware *fw, u64 *iova)\n{\n\tstruct drm_gem_object *bo;\n\tvoid *ptr;\n\n\tptr = msm_gem_kernel_new(gpu->dev, fw->size - 4,\n\t\tMSM_BO_WC | MSM_BO_GPU_READONLY, gpu->aspace, &bo, iova);\n\n\tif (IS_ERR(ptr))\n\t\treturn ERR_CAST(ptr);\n\n\tmemcpy(ptr, &fw->data[4], fw->size - 4);\n\n\tmsm_gem_put_vaddr(bo);\n\n\treturn bo;\n}\n\nint adreno_hw_init(struct msm_gpu *gpu)\n{\n\tVERB(\"%s\", gpu->name);\n\n\tfor (int i = 0; i < gpu->nr_rings; i++) {\n\t\tstruct msm_ringbuffer *ring = gpu->rb[i];\n\n\t\tif (!ring)\n\t\t\tcontinue;\n\n\t\tring->cur = ring->start;\n\t\tring->next = ring->start;\n\t\tring->memptrs->rptr = 0;\n\n\t\t \n\t\tif (fence_before(ring->fctx->last_fence, ring->memptrs->fence)) {\n\t\t\tring->memptrs->fence = ring->fctx->last_fence;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic uint32_t get_rptr(struct adreno_gpu *adreno_gpu,\n\t\tstruct msm_ringbuffer *ring)\n{\n\tstruct msm_gpu *gpu = &adreno_gpu->base;\n\n\treturn gpu->funcs->get_rptr(gpu, ring);\n}\n\nstruct msm_ringbuffer *adreno_active_ring(struct msm_gpu *gpu)\n{\n\treturn gpu->rb[0];\n}\n\nvoid adreno_recover(struct msm_gpu *gpu)\n{\n\tstruct drm_device *dev = gpu->dev;\n\tint ret;\n\n\t\n\t\n\n\tgpu->funcs->pm_suspend(gpu);\n\tgpu->funcs->pm_resume(gpu);\n\n\tret = msm_gpu_hw_init(gpu);\n\tif (ret) {\n\t\tDRM_DEV_ERROR(dev->dev, \"gpu hw init failed: %d\\n\", ret);\n\t\t \n\t}\n}\n\nvoid adreno_flush(struct msm_gpu *gpu, struct msm_ringbuffer *ring, u32 reg)\n{\n\tuint32_t wptr;\n\n\t \n\tring->cur = ring->next;\n\n\t \n\twptr = get_wptr(ring);\n\n\t \n\tmb();\n\n\tgpu_write(gpu, reg, wptr);\n}\n\nbool adreno_idle(struct msm_gpu *gpu, struct msm_ringbuffer *ring)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tuint32_t wptr = get_wptr(ring);\n\n\t \n\tif (!spin_until(get_rptr(adreno_gpu, ring) == wptr))\n\t\treturn true;\n\n\t \n\tDRM_ERROR(\"%s: timeout waiting to drain ringbuffer %d rptr/wptr = %X/%X\\n\",\n\t\tgpu->name, ring->id, get_rptr(adreno_gpu, ring), wptr);\n\n\treturn false;\n}\n\nint adreno_gpu_state_get(struct msm_gpu *gpu, struct msm_gpu_state *state)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint i, count = 0;\n\n\tWARN_ON(!mutex_is_locked(&gpu->lock));\n\n\tkref_init(&state->ref);\n\n\tktime_get_real_ts64(&state->time);\n\n\tfor (i = 0; i < gpu->nr_rings; i++) {\n\t\tint size = 0, j;\n\n\t\tstate->ring[i].fence = gpu->rb[i]->memptrs->fence;\n\t\tstate->ring[i].iova = gpu->rb[i]->iova;\n\t\tstate->ring[i].seqno = gpu->rb[i]->fctx->last_fence;\n\t\tstate->ring[i].rptr = get_rptr(adreno_gpu, gpu->rb[i]);\n\t\tstate->ring[i].wptr = get_wptr(gpu->rb[i]);\n\n\t\t \n\t\tsize = state->ring[i].wptr;\n\n\t\t \n\t\tfor (j = state->ring[i].wptr; j < MSM_GPU_RINGBUFFER_SZ >> 2; j++)\n\t\t\tif (gpu->rb[i]->start[j])\n\t\t\t\tsize = j + 1;\n\n\t\tif (size) {\n\t\t\tstate->ring[i].data = kvmalloc(size << 2, GFP_KERNEL);\n\t\t\tif (state->ring[i].data) {\n\t\t\t\tmemcpy(state->ring[i].data, gpu->rb[i]->start, size << 2);\n\t\t\t\tstate->ring[i].data_size = size << 2;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (!adreno_gpu->registers)\n\t\treturn 0;\n\n\t \n\tfor (i = 0; adreno_gpu->registers[i] != ~0; i += 2)\n\t\tcount += adreno_gpu->registers[i + 1] -\n\t\t\tadreno_gpu->registers[i] + 1;\n\n\tstate->registers = kcalloc(count * 2, sizeof(u32), GFP_KERNEL);\n\tif (state->registers) {\n\t\tint pos = 0;\n\n\t\tfor (i = 0; adreno_gpu->registers[i] != ~0; i += 2) {\n\t\t\tu32 start = adreno_gpu->registers[i];\n\t\t\tu32 end   = adreno_gpu->registers[i + 1];\n\t\t\tu32 addr;\n\n\t\t\tfor (addr = start; addr <= end; addr++) {\n\t\t\t\tstate->registers[pos++] = addr;\n\t\t\t\tstate->registers[pos++] = gpu_read(gpu, addr);\n\t\t\t}\n\t\t}\n\n\t\tstate->nr_registers = count;\n\t}\n\n\treturn 0;\n}\n\nvoid adreno_gpu_state_destroy(struct msm_gpu_state *state)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(state->ring); i++)\n\t\tkvfree(state->ring[i].data);\n\n\tfor (i = 0; state->bos && i < state->nr_bos; i++)\n\t\tkvfree(state->bos[i].data);\n\n\tkfree(state->bos);\n\tkfree(state->comm);\n\tkfree(state->cmd);\n\tkfree(state->registers);\n}\n\nstatic void adreno_gpu_state_kref_destroy(struct kref *kref)\n{\n\tstruct msm_gpu_state *state = container_of(kref,\n\t\tstruct msm_gpu_state, ref);\n\n\tadreno_gpu_state_destroy(state);\n\tkfree(state);\n}\n\nint adreno_gpu_state_put(struct msm_gpu_state *state)\n{\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn 1;\n\n\treturn kref_put(&state->ref, adreno_gpu_state_kref_destroy);\n}\n\n#if defined(CONFIG_DEBUG_FS) || defined(CONFIG_DEV_COREDUMP)\n\nstatic char *adreno_gpu_ascii85_encode(u32 *src, size_t len)\n{\n\tvoid *buf;\n\tsize_t buf_itr = 0, buffer_size;\n\tchar out[ASCII85_BUFSZ];\n\tlong l;\n\tint i;\n\n\tif (!src || !len)\n\t\treturn NULL;\n\n\tl = ascii85_encode_len(len);\n\n\t \n\tbuffer_size = (l * 5) + 1;\n\n\tbuf = kvmalloc(buffer_size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn NULL;\n\n\tfor (i = 0; i < l; i++)\n\t\tbuf_itr += scnprintf(buf + buf_itr, buffer_size - buf_itr, \"%s\",\n\t\t\t\tascii85_encode(src[i], out));\n\n\treturn buf;\n}\n\n \nvoid adreno_show_object(struct drm_printer *p, void **ptr, int len,\n\t\tbool *encoded)\n{\n\tif (!*ptr || !len)\n\t\treturn;\n\n\tif (!*encoded) {\n\t\tlong datalen, i;\n\t\tu32 *buf = *ptr;\n\n\t\t \n\t\tfor (datalen = 0, i = 0; i < len >> 2; i++)\n\t\t\tif (buf[i])\n\t\t\t\tdatalen = ((i + 1) << 2);\n\n\t\t \n\t\t*ptr = adreno_gpu_ascii85_encode(buf, datalen);\n\n\t\tkvfree(buf);\n\n\t\t*encoded = true;\n\t}\n\n\tif (!*ptr)\n\t\treturn;\n\n\tdrm_puts(p, \"    data: !!ascii85 |\\n\");\n\tdrm_puts(p, \"     \");\n\n\tdrm_puts(p, *ptr);\n\n\tdrm_puts(p, \"\\n\");\n}\n\nvoid adreno_show(struct msm_gpu *gpu, struct msm_gpu_state *state,\n\t\tstruct drm_printer *p)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint i;\n\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn;\n\n\tdrm_printf(p, \"revision: %u (%\"ADRENO_CHIPID_FMT\")\\n\",\n\t\t\tadreno_gpu->info->revn,\n\t\t\tADRENO_CHIPID_ARGS(adreno_gpu->chip_id));\n\t \n\tif (state->fault_info.ttbr0) {\n\t\tconst struct msm_gpu_fault_info *info = &state->fault_info;\n\n\t\tdrm_puts(p, \"fault-info:\\n\");\n\t\tdrm_printf(p, \"  - ttbr0=%.16llx\\n\", info->ttbr0);\n\t\tdrm_printf(p, \"  - iova=%.16lx\\n\", info->iova);\n\t\tdrm_printf(p, \"  - dir=%s\\n\", info->flags & IOMMU_FAULT_WRITE ? \"WRITE\" : \"READ\");\n\t\tdrm_printf(p, \"  - type=%s\\n\", info->type);\n\t\tdrm_printf(p, \"  - source=%s\\n\", info->block);\n\t}\n\n\tdrm_printf(p, \"rbbm-status: 0x%08x\\n\", state->rbbm_status);\n\n\tdrm_puts(p, \"ringbuffer:\\n\");\n\n\tfor (i = 0; i < gpu->nr_rings; i++) {\n\t\tdrm_printf(p, \"  - id: %d\\n\", i);\n\t\tdrm_printf(p, \"    iova: 0x%016llx\\n\", state->ring[i].iova);\n\t\tdrm_printf(p, \"    last-fence: %u\\n\", state->ring[i].seqno);\n\t\tdrm_printf(p, \"    retired-fence: %u\\n\", state->ring[i].fence);\n\t\tdrm_printf(p, \"    rptr: %u\\n\", state->ring[i].rptr);\n\t\tdrm_printf(p, \"    wptr: %u\\n\", state->ring[i].wptr);\n\t\tdrm_printf(p, \"    size: %u\\n\", MSM_GPU_RINGBUFFER_SZ);\n\n\t\tadreno_show_object(p, &state->ring[i].data,\n\t\t\tstate->ring[i].data_size, &state->ring[i].encoded);\n\t}\n\n\tif (state->bos) {\n\t\tdrm_puts(p, \"bos:\\n\");\n\n\t\tfor (i = 0; i < state->nr_bos; i++) {\n\t\t\tdrm_printf(p, \"  - iova: 0x%016llx\\n\",\n\t\t\t\tstate->bos[i].iova);\n\t\t\tdrm_printf(p, \"    size: %zd\\n\", state->bos[i].size);\n\t\t\tdrm_printf(p, \"    name: %-32s\\n\", state->bos[i].name);\n\n\t\t\tadreno_show_object(p, &state->bos[i].data,\n\t\t\t\tstate->bos[i].size, &state->bos[i].encoded);\n\t\t}\n\t}\n\n\tif (state->nr_registers) {\n\t\tdrm_puts(p, \"registers:\\n\");\n\n\t\tfor (i = 0; i < state->nr_registers; i++) {\n\t\t\tdrm_printf(p, \"  - { offset: 0x%04x, value: 0x%08x }\\n\",\n\t\t\t\tstate->registers[i * 2] << 2,\n\t\t\t\tstate->registers[(i * 2) + 1]);\n\t\t}\n\t}\n}\n#endif\n\n \nvoid adreno_dump_info(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint i;\n\n\tprintk(\"revision: %u (%\"ADRENO_CHIPID_FMT\")\\n\",\n\t\t\tadreno_gpu->info->revn,\n\t\t\tADRENO_CHIPID_ARGS(adreno_gpu->chip_id));\n\n\tfor (i = 0; i < gpu->nr_rings; i++) {\n\t\tstruct msm_ringbuffer *ring = gpu->rb[i];\n\n\t\tprintk(\"rb %d: fence:    %d/%d\\n\", i,\n\t\t\tring->memptrs->fence,\n\t\t\tring->fctx->last_fence);\n\n\t\tprintk(\"rptr:     %d\\n\", get_rptr(adreno_gpu, ring));\n\t\tprintk(\"rb wptr:  %d\\n\", get_wptr(ring));\n\t}\n}\n\n \nvoid adreno_dump(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint i;\n\n\tif (!adreno_gpu->registers)\n\t\treturn;\n\n\t \n\tprintk(\"IO:region %s 00000000 00020000\\n\", gpu->name);\n\tfor (i = 0; adreno_gpu->registers[i] != ~0; i += 2) {\n\t\tuint32_t start = adreno_gpu->registers[i];\n\t\tuint32_t end   = adreno_gpu->registers[i+1];\n\t\tuint32_t addr;\n\n\t\tfor (addr = start; addr <= end; addr++) {\n\t\t\tuint32_t val = gpu_read(gpu, addr);\n\t\t\tprintk(\"IO:R %08x %08x\\n\", addr<<2, val);\n\t\t}\n\t}\n}\n\nstatic uint32_t ring_freewords(struct msm_ringbuffer *ring)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(ring->gpu);\n\tuint32_t size = MSM_GPU_RINGBUFFER_SZ >> 2;\n\t \n\tuint32_t wptr = ring->next - ring->start;\n\tuint32_t rptr = get_rptr(adreno_gpu, ring);\n\treturn (rptr + (size - 1) - wptr) % size;\n}\n\nvoid adreno_wait_ring(struct msm_ringbuffer *ring, uint32_t ndwords)\n{\n\tif (spin_until(ring_freewords(ring) >= ndwords))\n\t\tDRM_DEV_ERROR(ring->gpu->dev->dev,\n\t\t\t\"timeout waiting for space in ringbuffer %d\\n\",\n\t\t\tring->id);\n}\n\nstatic int adreno_get_pwrlevels(struct device *dev,\n\t\tstruct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tunsigned long freq = ULONG_MAX;\n\tstruct dev_pm_opp *opp;\n\tint ret;\n\n\tgpu->fast_rate = 0;\n\n\t \n\tret = devm_pm_opp_of_add_table(dev);\n\tif (ret == -ENODEV) {\n\t\t \n\t\tif (adreno_is_a2xx(adreno_gpu)) {\n\t\t\tdev_warn(dev, \"Unable to find the OPP table. Falling back to 200 MHz.\\n\");\n\t\t\tdev_pm_opp_add(dev, 200000000, 0);\n\t\t} else if (adreno_is_a320(adreno_gpu)) {\n\t\t\tdev_warn(dev, \"Unable to find the OPP table. Falling back to 450 MHz.\\n\");\n\t\t\tdev_pm_opp_add(dev, 450000000, 0);\n\t\t} else {\n\t\t\tDRM_DEV_ERROR(dev, \"Unable to find the OPP table\\n\");\n\t\t\treturn -ENODEV;\n\t\t}\n\t} else if (ret) {\n\t\tDRM_DEV_ERROR(dev, \"Unable to set the OPP table\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\topp = dev_pm_opp_find_freq_floor(dev, &freq);\n\tif (IS_ERR(opp))\n\t\treturn PTR_ERR(opp);\n\n\tgpu->fast_rate = freq;\n\tdev_pm_opp_put(opp);\n\n\tDBG(\"fast_rate=%u, slow_rate=27000000\", gpu->fast_rate);\n\n\treturn 0;\n}\n\nint adreno_gpu_ocmem_init(struct device *dev, struct adreno_gpu *adreno_gpu,\n\t\t\t  struct adreno_ocmem *adreno_ocmem)\n{\n\tstruct ocmem_buf *ocmem_hdl;\n\tstruct ocmem *ocmem;\n\n\tocmem = of_get_ocmem(dev);\n\tif (IS_ERR(ocmem)) {\n\t\tif (PTR_ERR(ocmem) == -ENODEV) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\n\t\treturn PTR_ERR(ocmem);\n\t}\n\n\tocmem_hdl = ocmem_allocate(ocmem, OCMEM_GRAPHICS, adreno_gpu->info->gmem);\n\tif (IS_ERR(ocmem_hdl))\n\t\treturn PTR_ERR(ocmem_hdl);\n\n\tadreno_ocmem->ocmem = ocmem;\n\tadreno_ocmem->base = ocmem_hdl->addr;\n\tadreno_ocmem->hdl = ocmem_hdl;\n\n\tif (WARN_ON(ocmem_hdl->len != adreno_gpu->info->gmem))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid adreno_gpu_ocmem_cleanup(struct adreno_ocmem *adreno_ocmem)\n{\n\tif (adreno_ocmem && adreno_ocmem->base)\n\t\tocmem_free(adreno_ocmem->ocmem, OCMEM_GRAPHICS,\n\t\t\t   adreno_ocmem->hdl);\n}\n\nint adreno_read_speedbin(struct device *dev, u32 *speedbin)\n{\n\treturn nvmem_cell_read_variable_le_u32(dev, \"speed_bin\", speedbin);\n}\n\nint adreno_gpu_init(struct drm_device *drm, struct platform_device *pdev,\n\t\tstruct adreno_gpu *adreno_gpu,\n\t\tconst struct adreno_gpu_funcs *funcs, int nr_rings)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct adreno_platform_config *config = dev->platform_data;\n\tstruct msm_gpu_config adreno_gpu_config  = { 0 };\n\tstruct msm_gpu *gpu = &adreno_gpu->base;\n\tconst char *gpu_name;\n\tu32 speedbin;\n\tint ret;\n\n\tadreno_gpu->funcs = funcs;\n\tadreno_gpu->info = config->info;\n\tadreno_gpu->chip_id = config->chip_id;\n\n\tgpu->allow_relocs = config->info->family < ADRENO_6XX_GEN1;\n\n\t \n\tif (adreno_has_gmu_wrapper(adreno_gpu) ||\n\t    adreno_gpu->info->family < ADRENO_6XX_GEN1) {\n\t\t \n\t\tif (IS_ERR(devm_clk_get(dev, \"core\"))) {\n\t\t\t \n\t\t\tdevm_pm_opp_set_clkname(dev, \"core_clk\");\n\t\t} else\n\t\t\tdevm_pm_opp_set_clkname(dev, \"core\");\n\t}\n\n\tif (adreno_read_speedbin(dev, &speedbin) || !speedbin)\n\t\tspeedbin = 0xffff;\n\tadreno_gpu->speedbin = (uint16_t) (0xffff & speedbin);\n\n\tgpu_name = devm_kasprintf(dev, GFP_KERNEL, \"%\"ADRENO_CHIPID_FMT,\n\t\t\tADRENO_CHIPID_ARGS(config->chip_id));\n\tif (!gpu_name)\n\t\treturn -ENOMEM;\n\n\tadreno_gpu_config.ioname = \"kgsl_3d0_reg_memory\";\n\n\tadreno_gpu_config.nr_rings = nr_rings;\n\n\tret = adreno_get_pwrlevels(dev, gpu);\n\tif (ret)\n\t\treturn ret;\n\n\tpm_runtime_set_autosuspend_delay(dev,\n\t\tadreno_gpu->info->inactive_period);\n\tpm_runtime_use_autosuspend(dev);\n\n\treturn msm_gpu_init(drm, pdev, &adreno_gpu->base, &funcs->base,\n\t\t\tgpu_name, &adreno_gpu_config);\n}\n\nvoid adreno_gpu_cleanup(struct adreno_gpu *adreno_gpu)\n{\n\tstruct msm_gpu *gpu = &adreno_gpu->base;\n\tstruct msm_drm_private *priv = gpu->dev ? gpu->dev->dev_private : NULL;\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(adreno_gpu->info->fw); i++)\n\t\trelease_firmware(adreno_gpu->fw[i]);\n\n\tif (priv && pm_runtime_enabled(&priv->gpu_pdev->dev))\n\t\tpm_runtime_disable(&priv->gpu_pdev->dev);\n\n\tmsm_gpu_cleanup(&adreno_gpu->base);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}