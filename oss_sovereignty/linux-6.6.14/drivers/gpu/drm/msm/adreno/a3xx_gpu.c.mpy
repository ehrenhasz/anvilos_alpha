{
  "module_name": "a3xx_gpu.c",
  "hash_id": "432945c98d024b388c6c9260d8be16d866b3f7157083fb3f805645ab99afdf06",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/adreno/a3xx_gpu.c",
  "human_readable_source": "\n \n\n#include \"a3xx_gpu.h\"\n\n#define A3XX_INT0_MASK \\\n\t(A3XX_INT0_RBBM_AHB_ERROR |        \\\n\t A3XX_INT0_RBBM_ATB_BUS_OVERFLOW | \\\n\t A3XX_INT0_CP_T0_PACKET_IN_IB |    \\\n\t A3XX_INT0_CP_OPCODE_ERROR |       \\\n\t A3XX_INT0_CP_RESERVED_BIT_ERROR | \\\n\t A3XX_INT0_CP_HW_FAULT |           \\\n\t A3XX_INT0_CP_IB1_INT |            \\\n\t A3XX_INT0_CP_IB2_INT |            \\\n\t A3XX_INT0_CP_RB_INT |             \\\n\t A3XX_INT0_CP_REG_PROTECT_FAULT |  \\\n\t A3XX_INT0_CP_AHB_ERROR_HALT |     \\\n\t A3XX_INT0_CACHE_FLUSH_TS |        \\\n\t A3XX_INT0_UCHE_OOB_ACCESS)\n\nextern bool hang_debug;\n\nstatic void a3xx_dump(struct msm_gpu *gpu);\nstatic bool a3xx_idle(struct msm_gpu *gpu);\n\nstatic void a3xx_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n{\n\tstruct msm_ringbuffer *ring = submit->ring;\n\tunsigned int i;\n\n\tfor (i = 0; i < submit->nr_cmds; i++) {\n\t\tswitch (submit->cmd[i].type) {\n\t\tcase MSM_SUBMIT_CMD_IB_TARGET_BUF:\n\t\t\t \n\t\t\tbreak;\n\t\tcase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\n\t\t\t \n\t\t\tif (gpu->cur_ctx_seqno == submit->queue->ctx->seqno)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase MSM_SUBMIT_CMD_BUF:\n\t\t\tOUT_PKT3(ring, CP_INDIRECT_BUFFER_PFD, 2);\n\t\t\tOUT_RING(ring, lower_32_bits(submit->cmd[i].iova));\n\t\t\tOUT_RING(ring, submit->cmd[i].size);\n\t\t\tOUT_PKT2(ring);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tOUT_PKT0(ring, REG_AXXX_CP_SCRATCH_REG2, 1);\n\tOUT_RING(ring, submit->seqno);\n\n\t \n\tOUT_PKT3(ring, CP_EVENT_WRITE, 1);\n\tOUT_RING(ring, HLSQ_FLUSH);\n\n\t \n\tOUT_PKT3(ring, CP_WAIT_FOR_IDLE, 1);\n\tOUT_RING(ring, 0x00000000);\n\n\t \n\tOUT_PKT3(ring, CP_EVENT_WRITE, 3);\n\tOUT_RING(ring, CACHE_FLUSH_TS | CP_EVENT_WRITE_0_IRQ);\n\tOUT_RING(ring, rbmemptr(ring, fence));\n\tOUT_RING(ring, submit->seqno);\n\n#if 0\n\t \n\tOUT_PKT3(ring, CP_SET_CONSTANT, 2);\n\tOUT_RING(ring, CP_REG(REG_A3XX_HLSQ_CL_KERNEL_GROUP_X_REG));\n\tOUT_RING(ring, 0x00000000);\n#endif\n\n\tadreno_flush(gpu, ring, REG_AXXX_CP_RB_WPTR);\n}\n\nstatic bool a3xx_me_init(struct msm_gpu *gpu)\n{\n\tstruct msm_ringbuffer *ring = gpu->rb[0];\n\n\tOUT_PKT3(ring, CP_ME_INIT, 17);\n\tOUT_RING(ring, 0x000003f7);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000080);\n\tOUT_RING(ring, 0x00000100);\n\tOUT_RING(ring, 0x00000180);\n\tOUT_RING(ring, 0x00006600);\n\tOUT_RING(ring, 0x00000150);\n\tOUT_RING(ring, 0x0000014e);\n\tOUT_RING(ring, 0x00000154);\n\tOUT_RING(ring, 0x00000001);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\n\tadreno_flush(gpu, ring, REG_AXXX_CP_RB_WPTR);\n\treturn a3xx_idle(gpu);\n}\n\nstatic int a3xx_hw_init(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a3xx_gpu *a3xx_gpu = to_a3xx_gpu(adreno_gpu);\n\tuint32_t *ptr, len;\n\tint i, ret;\n\n\tDBG(\"%s\", gpu->name);\n\n\tif (adreno_is_a305(adreno_gpu)) {\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF1, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_RD_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_WR_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_DDR_OUT_MAX_BURST, 0x0000303);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF1, 0x10101010);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_GATE_OFF_WRREQ_EN, 0x0000ff);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ARB_CTL, 0x00000030);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO_EN, 0x0000003c);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO, 0x003c003c);\n\t} else if (adreno_is_a306(adreno_gpu)) {\n\t\tgpu_write(gpu, REG_A3XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x0003);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_RD_LIM_CONF0, 0x0000000a);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_WR_LIM_CONF0, 0x0000000a);\n\t} else if (adreno_is_a320(adreno_gpu)) {\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF1, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_RD_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_WR_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_DDR_OUT_MAX_BURST, 0x0000303);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF0, 0x10101010);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF1, 0x10101010);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_GATE_OFF_WRREQ_EN, 0x0000ff);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ARB_CTL, 0x00000030);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO_EN, 0x0000003c);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO, 0x003c003c);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT, 0x000000ff);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT_CONF, 0x000000a4);\n\n\t} else if (adreno_is_a330v2(adreno_gpu)) {\n\t\t \n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT, 0x0001003f);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT_CONF, 0x000000a4);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_GATE_OFF_WRREQ_EN, 0x00003f);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_DDR_OUT_MAX_BURST, 0x0000303);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x0003);\n\n\t} else if (adreno_is_a330(adreno_gpu)) {\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF0, 0x18181818);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_RD_LIM_CONF1, 0x18181818);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_RD_LIM_CONF0, 0x18181818);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_WR_LIM_CONF0, 0x18181818);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_DDR_OUT_MAX_BURST, 0x0000303);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF0, 0x18181818);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_IN_WR_LIM_CONF1, 0x18181818);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_GATE_OFF_WRREQ_EN, 0x00003f);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ARB_CTL, 0x00000030);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x0001);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO_EN, 0x0000003f);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_OUT_AXI_AOOO, 0x003f003f);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT, 0x0001003f);\n\t\tgpu_write(gpu, REG_A3XX_VBIF_ABIT_SORT_CONF, 0x000000a4);\n\t\t \n\t\tgpu_write(gpu, REG_A3XX_VBIF_CLKON, 0x00000001);\n\n\t} else {\n\t\tBUG();\n\t}\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_GPU_BUSY_MASKED, 0xffffffff);\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_SP_HYST_CNT, 0x10);\n\tgpu_write(gpu, REG_A3XX_RBBM_WAIT_IDLE_CLOCKS_CTL, 0x10);\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_AHB_CTL0, 0x00000001);\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_AHB_CTL1, 0xa6ffffff);\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_RBBM_CTL, 0x00030000);\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_INTERFACE_HANG_INT_CTL, 0x00010fff);\n\n\t \n\tgpu_write(gpu, REG_A3XX_UCHE_CACHE_MODE_CONTROL_REG, 0x00000001);\n\n\t \n\tif (adreno_is_a306(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_CLOCK_CTL, 0xaaaaaaaa);\n\telse if (adreno_is_a320(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_CLOCK_CTL, 0xbfffffff);\n\telse if (adreno_is_a330v2(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_CLOCK_CTL, 0xaaaaaaaa);\n\telse if (adreno_is_a330(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_CLOCK_CTL, 0xbffcffff);\n\n\tif (adreno_is_a330v2(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_GPR0_CTL, 0x05515455);\n\telse if (adreno_is_a330(adreno_gpu))\n\t\tgpu_write(gpu, REG_A3XX_RBBM_GPR0_CTL, 0x00000000);\n\n\t \n\tif (a3xx_gpu->ocmem.hdl) {\n\t\tgpu_write(gpu, REG_A3XX_RB_GMEM_BASE_ADDR,\n\t\t\t(unsigned int)(a3xx_gpu->ocmem.base >> 14));\n\t}\n\n\t \n\tgpu_write(gpu, REG_A3XX_RBBM_PERFCTR_CTL, 0x01);\n\n\t \n\tfor (i = 0; i < gpu->num_perfcntrs; i++) {\n\t\tconst struct msm_gpu_perfcntr *perfcntr = &gpu->perfcntrs[i];\n\t\tgpu_write(gpu, perfcntr->select_reg, perfcntr->select_val);\n\t}\n\n\tgpu_write(gpu, REG_A3XX_RBBM_INT_0_MASK, A3XX_INT0_MASK);\n\n\tret = adreno_hw_init(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tgpu_write(gpu, REG_AXXX_CP_RB_CNTL,\n\t\tMSM_GPU_RB_CNTL_DEFAULT | AXXX_CP_RB_CNTL_NO_UPDATE);\n\n\t \n\tgpu_write(gpu, REG_AXXX_CP_RB_BASE, lower_32_bits(gpu->rb[0]->iova));\n\n\t \n\tgpu_write(gpu, REG_A3XX_CP_PROTECT_CTRL, 0x00000007);\n\n\t \n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(0), 0x63000040);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(1), 0x62000080);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(2), 0x600000cc);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(3), 0x60000108);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(4), 0x64000140);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(5), 0x66000400);\n\n\t \n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(6), 0x65000700);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(7), 0x610007d8);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(8), 0x620007e0);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(9), 0x61001178);\n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(10), 0x64001180);\n\n\t \n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(11), 0x60003300);\n\n\t \n\tgpu_write(gpu, REG_A3XX_CP_PROTECT(12), 0x6b00c000);\n\n\t \n\n\t \n\tptr = (uint32_t *)(adreno_gpu->fw[ADRENO_FW_PM4]->data);\n\tlen = adreno_gpu->fw[ADRENO_FW_PM4]->size / 4;\n\tDBG(\"loading PM4 ucode version: %x\", ptr[1]);\n\n\tgpu_write(gpu, REG_AXXX_CP_DEBUG,\n\t\t\tAXXX_CP_DEBUG_DYNAMIC_CLK_DISABLE |\n\t\t\tAXXX_CP_DEBUG_MIU_128BIT_WRITE_ENABLE);\n\tgpu_write(gpu, REG_AXXX_CP_ME_RAM_WADDR, 0);\n\tfor (i = 1; i < len; i++)\n\t\tgpu_write(gpu, REG_AXXX_CP_ME_RAM_DATA, ptr[i]);\n\n\t \n\tptr = (uint32_t *)(adreno_gpu->fw[ADRENO_FW_PFP]->data);\n\tlen = adreno_gpu->fw[ADRENO_FW_PFP]->size / 4;\n\tDBG(\"loading PFP ucode version: %x\", ptr[5]);\n\n\tgpu_write(gpu, REG_A3XX_CP_PFP_UCODE_ADDR, 0);\n\tfor (i = 1; i < len; i++)\n\t\tgpu_write(gpu, REG_A3XX_CP_PFP_UCODE_DATA, ptr[i]);\n\n\t \n\tif (adreno_is_a305(adreno_gpu) || adreno_is_a306(adreno_gpu) ||\n\t\t\tadreno_is_a320(adreno_gpu)) {\n\t\tgpu_write(gpu, REG_AXXX_CP_QUEUE_THRESHOLDS,\n\t\t\t\tAXXX_CP_QUEUE_THRESHOLDS_CSQ_IB1_START(2) |\n\t\t\t\tAXXX_CP_QUEUE_THRESHOLDS_CSQ_IB2_START(6) |\n\t\t\t\tAXXX_CP_QUEUE_THRESHOLDS_CSQ_ST_START(14));\n\t} else if (adreno_is_a330(adreno_gpu)) {\n\t\t \n\t\tgpu_write(gpu, REG_AXXX_CP_QUEUE_THRESHOLDS, 0x003e2008);\n\t}\n\n\t \n\tgpu_write(gpu, REG_AXXX_CP_ME_CNTL, 0);\n\n\treturn a3xx_me_init(gpu) ? 0 : -EINVAL;\n}\n\nstatic void a3xx_recover(struct msm_gpu *gpu)\n{\n\tint i;\n\n\tadreno_dump_info(gpu);\n\n\tfor (i = 0; i < 8; i++) {\n\t\tprintk(\"CP_SCRATCH_REG%d: %u\\n\", i,\n\t\t\tgpu_read(gpu, REG_AXXX_CP_SCRATCH_REG0 + i));\n\t}\n\n\t \n\tif (hang_debug)\n\t\ta3xx_dump(gpu);\n\n\tgpu_write(gpu, REG_A3XX_RBBM_SW_RESET_CMD, 1);\n\tgpu_read(gpu, REG_A3XX_RBBM_SW_RESET_CMD);\n\tgpu_write(gpu, REG_A3XX_RBBM_SW_RESET_CMD, 0);\n\tadreno_recover(gpu);\n}\n\nstatic void a3xx_destroy(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a3xx_gpu *a3xx_gpu = to_a3xx_gpu(adreno_gpu);\n\n\tDBG(\"%s\", gpu->name);\n\n\tadreno_gpu_cleanup(adreno_gpu);\n\n\tadreno_gpu_ocmem_cleanup(&a3xx_gpu->ocmem);\n\n\tkfree(a3xx_gpu);\n}\n\nstatic bool a3xx_idle(struct msm_gpu *gpu)\n{\n\t \n\tif (!adreno_idle(gpu, gpu->rb[0]))\n\t\treturn false;\n\n\t \n\tif (spin_until(!(gpu_read(gpu, REG_A3XX_RBBM_STATUS) &\n\t\t\tA3XX_RBBM_STATUS_GPU_BUSY))) {\n\t\tDRM_ERROR(\"%s: timeout waiting for GPU to idle!\\n\", gpu->name);\n\n\t\t \n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic irqreturn_t a3xx_irq(struct msm_gpu *gpu)\n{\n\tuint32_t status;\n\n\tstatus = gpu_read(gpu, REG_A3XX_RBBM_INT_0_STATUS);\n\tDBG(\"%s: %08x\", gpu->name, status);\n\n\t\n\n\tgpu_write(gpu, REG_A3XX_RBBM_INT_CLEAR_CMD, status);\n\n\tmsm_gpu_retire(gpu);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic const unsigned int a3xx_registers[] = {\n\t0x0000, 0x0002, 0x0010, 0x0012, 0x0018, 0x0018, 0x0020, 0x0027,\n\t0x0029, 0x002b, 0x002e, 0x0033, 0x0040, 0x0042, 0x0050, 0x005c,\n\t0x0060, 0x006c, 0x0080, 0x0082, 0x0084, 0x0088, 0x0090, 0x00e5,\n\t0x00ea, 0x00ed, 0x0100, 0x0100, 0x0110, 0x0123, 0x01c0, 0x01c1,\n\t0x01c3, 0x01c5, 0x01c7, 0x01c7, 0x01d5, 0x01d9, 0x01dc, 0x01dd,\n\t0x01ea, 0x01ea, 0x01ee, 0x01f1, 0x01f5, 0x01f5, 0x01fc, 0x01ff,\n\t0x0440, 0x0440, 0x0443, 0x0443, 0x0445, 0x0445, 0x044d, 0x044f,\n\t0x0452, 0x0452, 0x0454, 0x046f, 0x047c, 0x047c, 0x047f, 0x047f,\n\t0x0578, 0x057f, 0x0600, 0x0602, 0x0605, 0x0607, 0x060a, 0x060e,\n\t0x0612, 0x0614, 0x0c01, 0x0c02, 0x0c06, 0x0c1d, 0x0c3d, 0x0c3f,\n\t0x0c48, 0x0c4b, 0x0c80, 0x0c80, 0x0c88, 0x0c8b, 0x0ca0, 0x0cb7,\n\t0x0cc0, 0x0cc1, 0x0cc6, 0x0cc7, 0x0ce4, 0x0ce5, 0x0e00, 0x0e05,\n\t0x0e0c, 0x0e0c, 0x0e22, 0x0e23, 0x0e41, 0x0e45, 0x0e64, 0x0e65,\n\t0x0e80, 0x0e82, 0x0e84, 0x0e89, 0x0ea0, 0x0ea1, 0x0ea4, 0x0ea7,\n\t0x0ec4, 0x0ecb, 0x0ee0, 0x0ee0, 0x0f00, 0x0f01, 0x0f03, 0x0f09,\n\t0x2040, 0x2040, 0x2044, 0x2044, 0x2048, 0x204d, 0x2068, 0x2069,\n\t0x206c, 0x206d, 0x2070, 0x2070, 0x2072, 0x2072, 0x2074, 0x2075,\n\t0x2079, 0x207a, 0x20c0, 0x20d3, 0x20e4, 0x20ef, 0x2100, 0x2109,\n\t0x210c, 0x210c, 0x210e, 0x210e, 0x2110, 0x2111, 0x2114, 0x2115,\n\t0x21e4, 0x21e4, 0x21ea, 0x21ea, 0x21ec, 0x21ed, 0x21f0, 0x21f0,\n\t0x2200, 0x2212, 0x2214, 0x2217, 0x221a, 0x221a, 0x2240, 0x227e,\n\t0x2280, 0x228b, 0x22c0, 0x22c0, 0x22c4, 0x22ce, 0x22d0, 0x22d8,\n\t0x22df, 0x22e6, 0x22e8, 0x22e9, 0x22ec, 0x22ec, 0x22f0, 0x22f7,\n\t0x22ff, 0x22ff, 0x2340, 0x2343, 0x2440, 0x2440, 0x2444, 0x2444,\n\t0x2448, 0x244d, 0x2468, 0x2469, 0x246c, 0x246d, 0x2470, 0x2470,\n\t0x2472, 0x2472, 0x2474, 0x2475, 0x2479, 0x247a, 0x24c0, 0x24d3,\n\t0x24e4, 0x24ef, 0x2500, 0x2509, 0x250c, 0x250c, 0x250e, 0x250e,\n\t0x2510, 0x2511, 0x2514, 0x2515, 0x25e4, 0x25e4, 0x25ea, 0x25ea,\n\t0x25ec, 0x25ed, 0x25f0, 0x25f0, 0x2600, 0x2612, 0x2614, 0x2617,\n\t0x261a, 0x261a, 0x2640, 0x267e, 0x2680, 0x268b, 0x26c0, 0x26c0,\n\t0x26c4, 0x26ce, 0x26d0, 0x26d8, 0x26df, 0x26e6, 0x26e8, 0x26e9,\n\t0x26ec, 0x26ec, 0x26f0, 0x26f7, 0x26ff, 0x26ff, 0x2740, 0x2743,\n\t0x300c, 0x300e, 0x301c, 0x301d, 0x302a, 0x302a, 0x302c, 0x302d,\n\t0x3030, 0x3031, 0x3034, 0x3036, 0x303c, 0x303c, 0x305e, 0x305f,\n\t~0    \n};\n\n \nstatic void a3xx_dump(struct msm_gpu *gpu)\n{\n\tprintk(\"status:   %08x\\n\",\n\t\t\tgpu_read(gpu, REG_A3XX_RBBM_STATUS));\n\tadreno_dump(gpu);\n}\n\nstatic struct msm_gpu_state *a3xx_gpu_state_get(struct msm_gpu *gpu)\n{\n\tstruct msm_gpu_state *state = kzalloc(sizeof(*state), GFP_KERNEL);\n\n\tif (!state)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tadreno_gpu_state_get(gpu, state);\n\n\tstate->rbbm_status = gpu_read(gpu, REG_A3XX_RBBM_STATUS);\n\n\treturn state;\n}\n\nstatic u64 a3xx_gpu_busy(struct msm_gpu *gpu, unsigned long *out_sample_rate)\n{\n\tu64 busy_cycles;\n\n\tbusy_cycles = gpu_read64(gpu, REG_A3XX_RBBM_PERFCTR_RBBM_1_LO);\n\t*out_sample_rate = clk_get_rate(gpu->core_clk);\n\n\treturn busy_cycles;\n}\n\nstatic u32 a3xx_get_rptr(struct msm_gpu *gpu, struct msm_ringbuffer *ring)\n{\n\tring->memptrs->rptr = gpu_read(gpu, REG_AXXX_CP_RB_RPTR);\n\treturn ring->memptrs->rptr;\n}\n\nstatic const struct adreno_gpu_funcs funcs = {\n\t.base = {\n\t\t.get_param = adreno_get_param,\n\t\t.set_param = adreno_set_param,\n\t\t.hw_init = a3xx_hw_init,\n\t\t.pm_suspend = msm_gpu_pm_suspend,\n\t\t.pm_resume = msm_gpu_pm_resume,\n\t\t.recover = a3xx_recover,\n\t\t.submit = a3xx_submit,\n\t\t.active_ring = adreno_active_ring,\n\t\t.irq = a3xx_irq,\n\t\t.destroy = a3xx_destroy,\n#if defined(CONFIG_DEBUG_FS) || defined(CONFIG_DEV_COREDUMP)\n\t\t.show = adreno_show,\n#endif\n\t\t.gpu_busy = a3xx_gpu_busy,\n\t\t.gpu_state_get = a3xx_gpu_state_get,\n\t\t.gpu_state_put = adreno_gpu_state_put,\n\t\t.create_address_space = adreno_create_address_space,\n\t\t.get_rptr = a3xx_get_rptr,\n\t},\n};\n\nstatic const struct msm_gpu_perfcntr perfcntrs[] = {\n\t{ REG_A3XX_SP_PERFCOUNTER6_SELECT, REG_A3XX_RBBM_PERFCTR_SP_6_LO,\n\t\t\tSP_ALU_ACTIVE_CYCLES, \"ALUACTIVE\" },\n\t{ REG_A3XX_SP_PERFCOUNTER7_SELECT, REG_A3XX_RBBM_PERFCTR_SP_7_LO,\n\t\t\tSP_FS_FULL_ALU_INSTRUCTIONS, \"ALUFULL\" },\n};\n\nstruct msm_gpu *a3xx_gpu_init(struct drm_device *dev)\n{\n\tstruct a3xx_gpu *a3xx_gpu = NULL;\n\tstruct adreno_gpu *adreno_gpu;\n\tstruct msm_gpu *gpu;\n\tstruct msm_drm_private *priv = dev->dev_private;\n\tstruct platform_device *pdev = priv->gpu_pdev;\n\tstruct icc_path *ocmem_icc_path;\n\tstruct icc_path *icc_path;\n\tint ret;\n\n\tif (!pdev) {\n\t\tDRM_DEV_ERROR(dev->dev, \"no a3xx device\\n\");\n\t\tret = -ENXIO;\n\t\tgoto fail;\n\t}\n\n\ta3xx_gpu = kzalloc(sizeof(*a3xx_gpu), GFP_KERNEL);\n\tif (!a3xx_gpu) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tadreno_gpu = &a3xx_gpu->base;\n\tgpu = &adreno_gpu->base;\n\n\tgpu->perfcntrs = perfcntrs;\n\tgpu->num_perfcntrs = ARRAY_SIZE(perfcntrs);\n\n\tadreno_gpu->registers = a3xx_registers;\n\n\tret = adreno_gpu_init(dev, pdev, adreno_gpu, &funcs, 1);\n\tif (ret)\n\t\tgoto fail;\n\n\t \n\tif (adreno_is_a330(adreno_gpu)) {\n\t\tret = adreno_gpu_ocmem_init(&adreno_gpu->base.pdev->dev,\n\t\t\t\t\t    adreno_gpu, &a3xx_gpu->ocmem);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\n\tif (!gpu->aspace) {\n\t\t \n\t\tDRM_DEV_ERROR(dev->dev, \"No memory protection without IOMMU\\n\");\n\t\tif (!allow_vram_carveout) {\n\t\t\tret = -ENXIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\ticc_path = devm_of_icc_get(&pdev->dev, \"gfx-mem\");\n\tif (IS_ERR(icc_path)) {\n\t\tret = PTR_ERR(icc_path);\n\t\tgoto fail;\n\t}\n\n\tocmem_icc_path = devm_of_icc_get(&pdev->dev, \"ocmem\");\n\tif (IS_ERR(ocmem_icc_path)) {\n\t\tret = PTR_ERR(ocmem_icc_path);\n\t\t \n\t\tif (ret != -ENODATA)\n\t\t\tgoto fail;\n\t\tocmem_icc_path = NULL;\n\t}\n\n\n\t \n\ticc_set_bw(icc_path, 0, Bps_to_icc(gpu->fast_rate) * 8);\n\ticc_set_bw(ocmem_icc_path, 0, Bps_to_icc(gpu->fast_rate) * 8);\n\n\treturn gpu;\n\nfail:\n\tif (a3xx_gpu)\n\t\ta3xx_destroy(&a3xx_gpu->base.base);\n\n\treturn ERR_PTR(ret);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}