{
  "module_name": "a6xx_gpu_state.c",
  "hash_id": "f2a953183e3b0a297ca4befc8006bb934ceac51f126809013974a764088b5ce5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/adreno/a6xx_gpu_state.c",
  "human_readable_source": "\n \n\n#include <linux/ascii85.h>\n#include \"msm_gem.h\"\n#include \"a6xx_gpu.h\"\n#include \"a6xx_gmu.h\"\n#include \"a6xx_gpu_state.h\"\n#include \"a6xx_gmu.xml.h\"\n\nstruct a6xx_gpu_state_obj {\n\tconst void *handle;\n\tu32 *data;\n};\n\nstruct a6xx_gpu_state {\n\tstruct msm_gpu_state base;\n\n\tstruct a6xx_gpu_state_obj *gmu_registers;\n\tint nr_gmu_registers;\n\n\tstruct a6xx_gpu_state_obj *registers;\n\tint nr_registers;\n\n\tstruct a6xx_gpu_state_obj *shaders;\n\tint nr_shaders;\n\n\tstruct a6xx_gpu_state_obj *clusters;\n\tint nr_clusters;\n\n\tstruct a6xx_gpu_state_obj *dbgahb_clusters;\n\tint nr_dbgahb_clusters;\n\n\tstruct a6xx_gpu_state_obj *indexed_regs;\n\tint nr_indexed_regs;\n\n\tstruct a6xx_gpu_state_obj *debugbus;\n\tint nr_debugbus;\n\n\tstruct a6xx_gpu_state_obj *vbif_debugbus;\n\n\tstruct a6xx_gpu_state_obj *cx_debugbus;\n\tint nr_cx_debugbus;\n\n\tstruct msm_gpu_state_bo *gmu_log;\n\tstruct msm_gpu_state_bo *gmu_hfi;\n\tstruct msm_gpu_state_bo *gmu_debug;\n\n\ts32 hfi_queue_history[2][HFI_HISTORY_SZ];\n\n\tstruct list_head objs;\n\n\tbool gpu_initialized;\n};\n\nstatic inline int CRASHDUMP_WRITE(u64 *in, u32 reg, u32 val)\n{\n\tin[0] = val;\n\tin[1] = (((u64) reg) << 44 | (1 << 21) | 1);\n\n\treturn 2;\n}\n\nstatic inline int CRASHDUMP_READ(u64 *in, u32 reg, u32 dwords, u64 target)\n{\n\tin[0] = target;\n\tin[1] = (((u64) reg) << 44 | dwords);\n\n\treturn 2;\n}\n\nstatic inline int CRASHDUMP_FINI(u64 *in)\n{\n\tin[0] = 0;\n\tin[1] = 0;\n\n\treturn 2;\n}\n\nstruct a6xx_crashdumper {\n\tvoid *ptr;\n\tstruct drm_gem_object *bo;\n\tu64 iova;\n};\n\nstruct a6xx_state_memobj {\n\tstruct list_head node;\n\tunsigned long long data[];\n};\n\nstatic void *state_kcalloc(struct a6xx_gpu_state *a6xx_state, int nr, size_t objsize)\n{\n\tstruct a6xx_state_memobj *obj =\n\t\tkvzalloc((nr * objsize) + sizeof(*obj), GFP_KERNEL);\n\n\tif (!obj)\n\t\treturn NULL;\n\n\tlist_add_tail(&obj->node, &a6xx_state->objs);\n\treturn &obj->data;\n}\n\nstatic void *state_kmemdup(struct a6xx_gpu_state *a6xx_state, void *src,\n\t\tsize_t size)\n{\n\tvoid *dst = state_kcalloc(a6xx_state, 1, size);\n\n\tif (dst)\n\t\tmemcpy(dst, src, size);\n\treturn dst;\n}\n\n \n#define A6XX_CD_DATA_OFFSET 8192\n#define A6XX_CD_DATA_SIZE  (SZ_1M - 8192)\n\nstatic int a6xx_crashdumper_init(struct msm_gpu *gpu,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tdumper->ptr = msm_gem_kernel_new(gpu->dev,\n\t\tSZ_1M, MSM_BO_WC, gpu->aspace,\n\t\t&dumper->bo, &dumper->iova);\n\n\tif (!IS_ERR(dumper->ptr))\n\t\tmsm_gem_object_set_name(dumper->bo, \"crashdump\");\n\n\treturn PTR_ERR_OR_ZERO(dumper->ptr);\n}\n\nstatic int a6xx_crashdumper_run(struct msm_gpu *gpu,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a6xx_gpu *a6xx_gpu = to_a6xx_gpu(adreno_gpu);\n\tu32 val;\n\tint ret;\n\n\tif (IS_ERR_OR_NULL(dumper->ptr))\n\t\treturn -EINVAL;\n\n\tif (!a6xx_gmu_sptprac_is_on(&a6xx_gpu->gmu))\n\t\treturn -EINVAL;\n\n\t \n\twmb();\n\n\tgpu_write64(gpu, REG_A6XX_CP_CRASH_SCRIPT_BASE, dumper->iova);\n\n\tgpu_write(gpu, REG_A6XX_CP_CRASH_DUMP_CNTL, 1);\n\n\tret = gpu_poll_timeout(gpu, REG_A6XX_CP_CRASH_DUMP_STATUS, val,\n\t\tval & 0x02, 100, 10000);\n\n\tgpu_write(gpu, REG_A6XX_CP_CRASH_DUMP_CNTL, 0);\n\n\treturn ret;\n}\n\n \nstatic int debugbus_read(struct msm_gpu *gpu, u32 block, u32 offset,\n\t\tu32 *data)\n{\n\tu32 reg = A6XX_DBGC_CFG_DBGBUS_SEL_D_PING_INDEX(offset) |\n\t\tA6XX_DBGC_CFG_DBGBUS_SEL_D_PING_BLK_SEL(block);\n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_SEL_A, reg);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_SEL_B, reg);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_SEL_C, reg);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_SEL_D, reg);\n\n\t \n\tudelay(1);\n\n\tdata[0] = gpu_read(gpu, REG_A6XX_DBGC_CFG_DBGBUS_TRACE_BUF2);\n\tdata[1] = gpu_read(gpu, REG_A6XX_DBGC_CFG_DBGBUS_TRACE_BUF1);\n\n\treturn 2;\n}\n\n#define cxdbg_write(ptr, offset, val) \\\n\tmsm_writel((val), (ptr) + ((offset) << 2))\n\n#define cxdbg_read(ptr, offset) \\\n\tmsm_readl((ptr) + ((offset) << 2))\n\n \nstatic int cx_debugbus_read(void __iomem *cxdbg, u32 block, u32 offset,\n\t\tu32 *data)\n{\n\tu32 reg = A6XX_CX_DBGC_CFG_DBGBUS_SEL_A_PING_INDEX(offset) |\n\t\tA6XX_CX_DBGC_CFG_DBGBUS_SEL_A_PING_BLK_SEL(block);\n\n\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_SEL_A, reg);\n\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_SEL_B, reg);\n\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_SEL_C, reg);\n\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_SEL_D, reg);\n\n\t \n\tudelay(1);\n\n\tdata[0] = cxdbg_read(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF2);\n\tdata[1] = cxdbg_read(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_TRACE_BUF1);\n\n\treturn 2;\n}\n\n \nstatic int vbif_debugbus_read(struct msm_gpu *gpu, u32 ctrl0, u32 ctrl1,\n\t\tu32 reg, int count, u32 *data)\n{\n\tint i;\n\n\tgpu_write(gpu, ctrl0, reg);\n\n\tfor (i = 0; i < count; i++) {\n\t\tgpu_write(gpu, ctrl1, i);\n\t\tdata[i] = gpu_read(gpu, REG_A6XX_VBIF_TEST_BUS_OUT);\n\t}\n\n\treturn count;\n}\n\n#define AXI_ARB_BLOCKS 2\n#define XIN_AXI_BLOCKS 5\n#define XIN_CORE_BLOCKS 4\n\n#define VBIF_DEBUGBUS_BLOCK_SIZE \\\n\t((16 * AXI_ARB_BLOCKS) + \\\n\t (18 * XIN_AXI_BLOCKS) + \\\n\t (12 * XIN_CORE_BLOCKS))\n\nstatic void a6xx_get_vbif_debugbus_block(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_gpu_state_obj *obj)\n{\n\tu32 clk, *ptr;\n\tint i;\n\n\tobj->data = state_kcalloc(a6xx_state, VBIF_DEBUGBUS_BLOCK_SIZE,\n\t\tsizeof(u32));\n\tif (!obj->data)\n\t\treturn;\n\n\tobj->handle = NULL;\n\n\t \n\tclk = gpu_read(gpu, REG_A6XX_VBIF_CLKON);\n\n\t \n\tgpu_write(gpu, REG_A6XX_VBIF_CLKON,\n\t\tclk | A6XX_VBIF_CLKON_FORCE_ON_TESTBUS);\n\n\t \n\tgpu_write(gpu, REG_A6XX_VBIF_TEST_BUS1_CTRL0, 0);\n\n\t \n\tgpu_write(gpu, REG_A6XX_VBIF_TEST_BUS_OUT_CTRL, 1);\n\n\tptr = obj->data;\n\n\tfor (i = 0; i < AXI_ARB_BLOCKS; i++)\n\t\tptr += vbif_debugbus_read(gpu,\n\t\t\tREG_A6XX_VBIF_TEST_BUS2_CTRL0,\n\t\t\tREG_A6XX_VBIF_TEST_BUS2_CTRL1,\n\t\t\t1 << (i + 16), 16, ptr);\n\n\tfor (i = 0; i < XIN_AXI_BLOCKS; i++)\n\t\tptr += vbif_debugbus_read(gpu,\n\t\t\tREG_A6XX_VBIF_TEST_BUS2_CTRL0,\n\t\t\tREG_A6XX_VBIF_TEST_BUS2_CTRL1,\n\t\t\t1 << i, 18, ptr);\n\n\t \n\tgpu_write(gpu, REG_A6XX_VBIF_TEST_BUS2_CTRL0, 0);\n\n\tfor (i = 0; i < XIN_CORE_BLOCKS; i++)\n\t\tptr += vbif_debugbus_read(gpu,\n\t\t\tREG_A6XX_VBIF_TEST_BUS1_CTRL0,\n\t\t\tREG_A6XX_VBIF_TEST_BUS1_CTRL1,\n\t\t\t1 << i, 12, ptr);\n\n\t \n\tgpu_write(gpu, REG_A6XX_VBIF_CLKON, clk);\n}\n\nstatic void a6xx_get_debugbus_block(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_debugbus_block *block,\n\t\tstruct a6xx_gpu_state_obj *obj)\n{\n\tint i;\n\tu32 *ptr;\n\n\tobj->data = state_kcalloc(a6xx_state, block->count, sizeof(u64));\n\tif (!obj->data)\n\t\treturn;\n\n\tobj->handle = block;\n\n\tfor (ptr = obj->data, i = 0; i < block->count; i++)\n\t\tptr += debugbus_read(gpu, block->id, i, ptr);\n}\n\nstatic void a6xx_get_cx_debugbus_block(void __iomem *cxdbg,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_debugbus_block *block,\n\t\tstruct a6xx_gpu_state_obj *obj)\n{\n\tint i;\n\tu32 *ptr;\n\n\tobj->data = state_kcalloc(a6xx_state, block->count, sizeof(u64));\n\tif (!obj->data)\n\t\treturn;\n\n\tobj->handle = block;\n\n\tfor (ptr = obj->data, i = 0; i < block->count; i++)\n\t\tptr += cx_debugbus_read(cxdbg, block->id, i, ptr);\n}\n\nstatic void a6xx_get_debugbus(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state)\n{\n\tstruct resource *res;\n\tvoid __iomem *cxdbg = NULL;\n\tint nr_debugbus_blocks;\n\n\t \n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_CNTLT,\n\t\tA6XX_DBGC_CFG_DBGBUS_CNTLT_SEGT(0xf));\n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_CNTLM,\n\t\tA6XX_DBGC_CFG_DBGBUS_CNTLM_ENABLE(0xf));\n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_IVTL_0, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_IVTL_1, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_IVTL_2, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_IVTL_3, 0);\n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_BYTEL_0, 0x76543210);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_BYTEL_1, 0xFEDCBA98);\n\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_MASKL_0, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_MASKL_1, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_MASKL_2, 0);\n\tgpu_write(gpu, REG_A6XX_DBGC_CFG_DBGBUS_MASKL_3, 0);\n\n\t \n\tres = platform_get_resource_byname(gpu->pdev, IORESOURCE_MEM,\n\t\t\t\"cx_dbgc\");\n\n\tif (res)\n\t\tcxdbg = ioremap(res->start, resource_size(res));\n\n\tif (cxdbg) {\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_CNTLT,\n\t\t\tA6XX_DBGC_CFG_DBGBUS_CNTLT_SEGT(0xf));\n\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_CNTLM,\n\t\t\tA6XX_DBGC_CFG_DBGBUS_CNTLM_ENABLE(0xf));\n\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_IVTL_0, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_IVTL_1, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_IVTL_2, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_IVTL_3, 0);\n\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_0,\n\t\t\t0x76543210);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_BYTEL_1,\n\t\t\t0xFEDCBA98);\n\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_MASKL_0, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_MASKL_1, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_MASKL_2, 0);\n\t\tcxdbg_write(cxdbg, REG_A6XX_CX_DBGC_CFG_DBGBUS_MASKL_3, 0);\n\t}\n\n\tnr_debugbus_blocks = ARRAY_SIZE(a6xx_debugbus_blocks) +\n\t\t(a6xx_has_gbif(to_adreno_gpu(gpu)) ? 1 : 0);\n\n\tif (adreno_is_a650_family(to_adreno_gpu(gpu)))\n\t\tnr_debugbus_blocks += ARRAY_SIZE(a650_debugbus_blocks);\n\n\ta6xx_state->debugbus = state_kcalloc(a6xx_state, nr_debugbus_blocks,\n\t\t\tsizeof(*a6xx_state->debugbus));\n\n\tif (a6xx_state->debugbus) {\n\t\tint i;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(a6xx_debugbus_blocks); i++)\n\t\t\ta6xx_get_debugbus_block(gpu,\n\t\t\t\ta6xx_state,\n\t\t\t\t&a6xx_debugbus_blocks[i],\n\t\t\t\t&a6xx_state->debugbus[i]);\n\n\t\ta6xx_state->nr_debugbus = ARRAY_SIZE(a6xx_debugbus_blocks);\n\n\t\t \n\t\tif (a6xx_has_gbif(to_adreno_gpu(gpu))) {\n\t\t\ta6xx_get_debugbus_block(gpu, a6xx_state,\n\t\t\t\t&a6xx_gbif_debugbus_block,\n\t\t\t\t&a6xx_state->debugbus[i]);\n\n\t\t\ta6xx_state->nr_debugbus += 1;\n\t\t}\n\n\n\t\tif (adreno_is_a650_family(to_adreno_gpu(gpu))) {\n\t\t\tfor (i = 0; i < ARRAY_SIZE(a650_debugbus_blocks); i++)\n\t\t\t\ta6xx_get_debugbus_block(gpu,\n\t\t\t\t\ta6xx_state,\n\t\t\t\t\t&a650_debugbus_blocks[i],\n\t\t\t\t\t&a6xx_state->debugbus[i]);\n\t\t}\n\t}\n\n\t \n\tif (!a6xx_has_gbif(to_adreno_gpu(gpu))) {\n\t\ta6xx_state->vbif_debugbus =\n\t\t\tstate_kcalloc(a6xx_state, 1,\n\t\t\t\t\tsizeof(*a6xx_state->vbif_debugbus));\n\n\t\tif (a6xx_state->vbif_debugbus)\n\t\t\ta6xx_get_vbif_debugbus_block(gpu, a6xx_state,\n\t\t\t\t\ta6xx_state->vbif_debugbus);\n\t}\n\n\tif (cxdbg) {\n\t\ta6xx_state->cx_debugbus =\n\t\t\tstate_kcalloc(a6xx_state,\n\t\t\tARRAY_SIZE(a6xx_cx_debugbus_blocks),\n\t\t\tsizeof(*a6xx_state->cx_debugbus));\n\n\t\tif (a6xx_state->cx_debugbus) {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < ARRAY_SIZE(a6xx_cx_debugbus_blocks); i++)\n\t\t\t\ta6xx_get_cx_debugbus_block(cxdbg,\n\t\t\t\t\ta6xx_state,\n\t\t\t\t\t&a6xx_cx_debugbus_blocks[i],\n\t\t\t\t\t&a6xx_state->cx_debugbus[i]);\n\n\t\t\ta6xx_state->nr_cx_debugbus =\n\t\t\t\tARRAY_SIZE(a6xx_cx_debugbus_blocks);\n\t\t}\n\n\t\tiounmap(cxdbg);\n\t}\n}\n\n#define RANGE(reg, a) ((reg)[(a) + 1] - (reg)[(a)] + 1)\n\n \nstatic void a6xx_get_dbgahb_cluster(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_dbgahb_cluster *dbgahb,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tu64 *in = dumper->ptr;\n\tu64 out = dumper->iova + A6XX_CD_DATA_OFFSET;\n\tsize_t datasize;\n\tint i, regcount = 0;\n\n\tfor (i = 0; i < A6XX_NUM_CONTEXTS; i++) {\n\t\tint j;\n\n\t\tin += CRASHDUMP_WRITE(in, REG_A6XX_HLSQ_DBG_READ_SEL,\n\t\t\t(dbgahb->statetype + i * 2) << 8);\n\n\t\tfor (j = 0; j < dbgahb->count; j += 2) {\n\t\t\tint count = RANGE(dbgahb->registers, j);\n\t\t\tu32 offset = REG_A6XX_HLSQ_DBG_AHB_READ_APERTURE +\n\t\t\t\tdbgahb->registers[j] - (dbgahb->base >> 2);\n\n\t\t\tin += CRASHDUMP_READ(in, offset, count, out);\n\n\t\t\tout += count * sizeof(u32);\n\n\t\t\tif (i == 0)\n\t\t\t\tregcount += count;\n\t\t}\n\t}\n\n\tCRASHDUMP_FINI(in);\n\n\tdatasize = regcount * A6XX_NUM_CONTEXTS * sizeof(u32);\n\n\tif (WARN_ON(datasize > A6XX_CD_DATA_SIZE))\n\t\treturn;\n\n\tif (a6xx_crashdumper_run(gpu, dumper))\n\t\treturn;\n\n\tobj->handle = dbgahb;\n\tobj->data = state_kmemdup(a6xx_state, dumper->ptr + A6XX_CD_DATA_OFFSET,\n\t\tdatasize);\n}\n\nstatic void a6xx_get_dbgahb_clusters(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tint i;\n\n\ta6xx_state->dbgahb_clusters = state_kcalloc(a6xx_state,\n\t\tARRAY_SIZE(a6xx_dbgahb_clusters),\n\t\tsizeof(*a6xx_state->dbgahb_clusters));\n\n\tif (!a6xx_state->dbgahb_clusters)\n\t\treturn;\n\n\ta6xx_state->nr_dbgahb_clusters = ARRAY_SIZE(a6xx_dbgahb_clusters);\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_dbgahb_clusters); i++)\n\t\ta6xx_get_dbgahb_cluster(gpu, a6xx_state,\n\t\t\t&a6xx_dbgahb_clusters[i],\n\t\t\t&a6xx_state->dbgahb_clusters[i], dumper);\n}\n\n \nstatic void a6xx_get_cluster(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_cluster *cluster,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tu64 *in = dumper->ptr;\n\tu64 out = dumper->iova + A6XX_CD_DATA_OFFSET;\n\tsize_t datasize;\n\tint i, regcount = 0;\n\tu32 id = cluster->id;\n\n\t \n\tif (!adreno_is_a660_family(adreno_gpu) &&\n\t\t\tcluster->registers == a660_fe_cluster)\n\t\treturn;\n\n\tif (adreno_is_a650_family(adreno_gpu) &&\n\t\t\tcluster->registers == a6xx_ps_cluster)\n\t\tid = CLUSTER_VPC_PS;\n\n\t \n\tif (cluster->sel_reg)\n\t\tin += CRASHDUMP_WRITE(in, cluster->sel_reg, cluster->sel_val);\n\n\tfor (i = 0; i < A6XX_NUM_CONTEXTS; i++) {\n\t\tint j;\n\n\t\tin += CRASHDUMP_WRITE(in, REG_A6XX_CP_APERTURE_CNTL_CD,\n\t\t\t(id << 8) | (i << 4) | i);\n\n\t\tfor (j = 0; j < cluster->count; j += 2) {\n\t\t\tint count = RANGE(cluster->registers, j);\n\n\t\t\tin += CRASHDUMP_READ(in, cluster->registers[j],\n\t\t\t\tcount, out);\n\n\t\t\tout += count * sizeof(u32);\n\n\t\t\tif (i == 0)\n\t\t\t\tregcount += count;\n\t\t}\n\t}\n\n\tCRASHDUMP_FINI(in);\n\n\tdatasize = regcount * A6XX_NUM_CONTEXTS * sizeof(u32);\n\n\tif (WARN_ON(datasize > A6XX_CD_DATA_SIZE))\n\t\treturn;\n\n\tif (a6xx_crashdumper_run(gpu, dumper))\n\t\treturn;\n\n\tobj->handle = cluster;\n\tobj->data = state_kmemdup(a6xx_state, dumper->ptr + A6XX_CD_DATA_OFFSET,\n\t\tdatasize);\n}\n\nstatic void a6xx_get_clusters(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tint i;\n\n\ta6xx_state->clusters = state_kcalloc(a6xx_state,\n\t\tARRAY_SIZE(a6xx_clusters), sizeof(*a6xx_state->clusters));\n\n\tif (!a6xx_state->clusters)\n\t\treturn;\n\n\ta6xx_state->nr_clusters = ARRAY_SIZE(a6xx_clusters);\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_clusters); i++)\n\t\ta6xx_get_cluster(gpu, a6xx_state, &a6xx_clusters[i],\n\t\t\t&a6xx_state->clusters[i], dumper);\n}\n\n \nstatic void a6xx_get_shader_block(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_shader_block *block,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tu64 *in = dumper->ptr;\n\tsize_t datasize = block->size * A6XX_NUM_SHADER_BANKS * sizeof(u32);\n\tint i;\n\n\tif (WARN_ON(datasize > A6XX_CD_DATA_SIZE))\n\t\treturn;\n\n\tfor (i = 0; i < A6XX_NUM_SHADER_BANKS; i++) {\n\t\tin += CRASHDUMP_WRITE(in, REG_A6XX_HLSQ_DBG_READ_SEL,\n\t\t\t(block->type << 8) | i);\n\n\t\tin += CRASHDUMP_READ(in, REG_A6XX_HLSQ_DBG_AHB_READ_APERTURE,\n\t\t\tblock->size, dumper->iova + A6XX_CD_DATA_OFFSET);\n\t}\n\n\tCRASHDUMP_FINI(in);\n\n\tif (a6xx_crashdumper_run(gpu, dumper))\n\t\treturn;\n\n\tobj->handle = block;\n\tobj->data = state_kmemdup(a6xx_state, dumper->ptr + A6XX_CD_DATA_OFFSET,\n\t\tdatasize);\n}\n\nstatic void a6xx_get_shaders(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tint i;\n\n\ta6xx_state->shaders = state_kcalloc(a6xx_state,\n\t\tARRAY_SIZE(a6xx_shader_blocks), sizeof(*a6xx_state->shaders));\n\n\tif (!a6xx_state->shaders)\n\t\treturn;\n\n\ta6xx_state->nr_shaders = ARRAY_SIZE(a6xx_shader_blocks);\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_shader_blocks); i++)\n\t\ta6xx_get_shader_block(gpu, a6xx_state, &a6xx_shader_blocks[i],\n\t\t\t&a6xx_state->shaders[i], dumper);\n}\n\n \nstatic void a6xx_get_crashdumper_hlsq_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_registers *regs,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tstruct a6xx_crashdumper *dumper)\n\n{\n\tu64 *in = dumper->ptr;\n\tu64 out = dumper->iova + A6XX_CD_DATA_OFFSET;\n\tint i, regcount = 0;\n\n\tin += CRASHDUMP_WRITE(in, REG_A6XX_HLSQ_DBG_READ_SEL, regs->val1);\n\n\tfor (i = 0; i < regs->count; i += 2) {\n\t\tu32 count = RANGE(regs->registers, i);\n\t\tu32 offset = REG_A6XX_HLSQ_DBG_AHB_READ_APERTURE +\n\t\t\tregs->registers[i] - (regs->val0 >> 2);\n\n\t\tin += CRASHDUMP_READ(in, offset, count, out);\n\n\t\tout += count * sizeof(u32);\n\t\tregcount += count;\n\t}\n\n\tCRASHDUMP_FINI(in);\n\n\tif (WARN_ON((regcount * sizeof(u32)) > A6XX_CD_DATA_SIZE))\n\t\treturn;\n\n\tif (a6xx_crashdumper_run(gpu, dumper))\n\t\treturn;\n\n\tobj->handle = regs;\n\tobj->data = state_kmemdup(a6xx_state, dumper->ptr + A6XX_CD_DATA_OFFSET,\n\t\tregcount * sizeof(u32));\n}\n\n \nstatic void a6xx_get_crashdumper_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_registers *regs,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tstruct a6xx_crashdumper *dumper)\n\n{\n\tu64 *in = dumper->ptr;\n\tu64 out = dumper->iova + A6XX_CD_DATA_OFFSET;\n\tint i, regcount = 0;\n\n\t \n\tif (!adreno_is_a660_family(to_adreno_gpu(gpu)) &&\n\t\t\t(regs->registers == a660_registers))\n\t\treturn;\n\n\t \n\tif (regs->val0)\n\t\tin += CRASHDUMP_WRITE(in, regs->val0, regs->val1);\n\n\tfor (i = 0; i < regs->count; i += 2) {\n\t\tu32 count = RANGE(regs->registers, i);\n\n\t\tin += CRASHDUMP_READ(in, regs->registers[i], count, out);\n\n\t\tout += count * sizeof(u32);\n\t\tregcount += count;\n\t}\n\n\tCRASHDUMP_FINI(in);\n\n\tif (WARN_ON((regcount * sizeof(u32)) > A6XX_CD_DATA_SIZE))\n\t\treturn;\n\n\tif (a6xx_crashdumper_run(gpu, dumper))\n\t\treturn;\n\n\tobj->handle = regs;\n\tobj->data = state_kmemdup(a6xx_state, dumper->ptr + A6XX_CD_DATA_OFFSET,\n\t\tregcount * sizeof(u32));\n}\n\n \nstatic void a6xx_get_ahb_gpu_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_registers *regs,\n\t\tstruct a6xx_gpu_state_obj *obj)\n{\n\tint i, regcount = 0, index = 0;\n\n\t \n\tif (!adreno_is_a660_family(to_adreno_gpu(gpu)) &&\n\t\t\t(regs->registers == a660_registers))\n\t\treturn;\n\n\tfor (i = 0; i < regs->count; i += 2)\n\t\tregcount += RANGE(regs->registers, i);\n\n\tobj->handle = (const void *) regs;\n\tobj->data = state_kcalloc(a6xx_state, regcount, sizeof(u32));\n\tif (!obj->data)\n\t\treturn;\n\n\tfor (i = 0; i < regs->count; i += 2) {\n\t\tu32 count = RANGE(regs->registers, i);\n\t\tint j;\n\n\t\tfor (j = 0; j < count; j++)\n\t\t\tobj->data[index++] = gpu_read(gpu,\n\t\t\t\tregs->registers[i] + j);\n\t}\n}\n\n \nstatic void _a6xx_get_gmu_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tconst struct a6xx_registers *regs,\n\t\tstruct a6xx_gpu_state_obj *obj,\n\t\tbool rscc)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a6xx_gpu *a6xx_gpu = to_a6xx_gpu(adreno_gpu);\n\tstruct a6xx_gmu *gmu = &a6xx_gpu->gmu;\n\tint i, regcount = 0, index = 0;\n\n\tfor (i = 0; i < regs->count; i += 2)\n\t\tregcount += RANGE(regs->registers, i);\n\n\tobj->handle = (const void *) regs;\n\tobj->data = state_kcalloc(a6xx_state, regcount, sizeof(u32));\n\tif (!obj->data)\n\t\treturn;\n\n\tfor (i = 0; i < regs->count; i += 2) {\n\t\tu32 count = RANGE(regs->registers, i);\n\t\tint j;\n\n\t\tfor (j = 0; j < count; j++) {\n\t\t\tu32 offset = regs->registers[i] + j;\n\t\t\tu32 val;\n\n\t\t\tif (rscc)\n\t\t\t\tval = gmu_read_rscc(gmu, offset);\n\t\t\telse\n\t\t\t\tval = gmu_read(gmu, offset);\n\n\t\t\tobj->data[index++] = val;\n\t\t}\n\t}\n}\n\nstatic void a6xx_get_gmu_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a6xx_gpu *a6xx_gpu = to_a6xx_gpu(adreno_gpu);\n\n\ta6xx_state->gmu_registers = state_kcalloc(a6xx_state,\n\t\t3, sizeof(*a6xx_state->gmu_registers));\n\n\tif (!a6xx_state->gmu_registers)\n\t\treturn;\n\n\ta6xx_state->nr_gmu_registers = 3;\n\n\t \n\t_a6xx_get_gmu_registers(gpu, a6xx_state, &a6xx_gmu_reglist[0],\n\t\t&a6xx_state->gmu_registers[0], false);\n\t_a6xx_get_gmu_registers(gpu, a6xx_state, &a6xx_gmu_reglist[1],\n\t\t&a6xx_state->gmu_registers[1], true);\n\n\tif (!a6xx_gmu_gx_is_on(&a6xx_gpu->gmu))\n\t\treturn;\n\n\t \n\tgpu_write(gpu, REG_A6XX_GMU_AO_AHB_FENCE_CTRL, 0);\n\n\t_a6xx_get_gmu_registers(gpu, a6xx_state, &a6xx_gmu_reglist[2],\n\t\t&a6xx_state->gmu_registers[2], false);\n}\n\nstatic struct msm_gpu_state_bo *a6xx_snapshot_gmu_bo(\n\t\tstruct a6xx_gpu_state *a6xx_state, struct a6xx_gmu_bo *bo)\n{\n\tstruct msm_gpu_state_bo *snapshot;\n\n\tif (!bo->size)\n\t\treturn NULL;\n\n\tsnapshot = state_kcalloc(a6xx_state, 1, sizeof(*snapshot));\n\tif (!snapshot)\n\t\treturn NULL;\n\n\tsnapshot->iova = bo->iova;\n\tsnapshot->size = bo->size;\n\tsnapshot->data = kvzalloc(snapshot->size, GFP_KERNEL);\n\tif (!snapshot->data)\n\t\treturn NULL;\n\n\tmemcpy(snapshot->data, bo->virt, bo->size);\n\n\treturn snapshot;\n}\n\nstatic void a6xx_snapshot_gmu_hfi_history(struct msm_gpu *gpu,\n\t\t\t\t\t  struct a6xx_gpu_state *a6xx_state)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a6xx_gpu *a6xx_gpu = to_a6xx_gpu(adreno_gpu);\n\tstruct a6xx_gmu *gmu = &a6xx_gpu->gmu;\n\tunsigned i, j;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(gmu->queues) != ARRAY_SIZE(a6xx_state->hfi_queue_history));\n\n\tfor (i = 0; i < ARRAY_SIZE(gmu->queues); i++) {\n\t\tstruct a6xx_hfi_queue *queue = &gmu->queues[i];\n\t\tfor (j = 0; j < HFI_HISTORY_SZ; j++) {\n\t\t\tunsigned idx = (j + queue->history_idx) % HFI_HISTORY_SZ;\n\t\t\ta6xx_state->hfi_queue_history[i][j] = queue->history[idx];\n\t\t}\n\t}\n}\n\n#define A6XX_GBIF_REGLIST_SIZE   1\nstatic void a6xx_get_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_crashdumper *dumper)\n{\n\tint i, count = ARRAY_SIZE(a6xx_ahb_reglist) +\n\t\tARRAY_SIZE(a6xx_reglist) +\n\t\tARRAY_SIZE(a6xx_hlsq_reglist) + A6XX_GBIF_REGLIST_SIZE;\n\tint index = 0;\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\n\ta6xx_state->registers = state_kcalloc(a6xx_state,\n\t\tcount, sizeof(*a6xx_state->registers));\n\n\tif (!a6xx_state->registers)\n\t\treturn;\n\n\ta6xx_state->nr_registers = count;\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_ahb_reglist); i++)\n\t\ta6xx_get_ahb_gpu_registers(gpu,\n\t\t\ta6xx_state, &a6xx_ahb_reglist[i],\n\t\t\t&a6xx_state->registers[index++]);\n\n\tif (a6xx_has_gbif(adreno_gpu))\n\t\ta6xx_get_ahb_gpu_registers(gpu,\n\t\t\t\ta6xx_state, &a6xx_gbif_reglist,\n\t\t\t\t&a6xx_state->registers[index++]);\n\telse\n\t\ta6xx_get_ahb_gpu_registers(gpu,\n\t\t\t\ta6xx_state, &a6xx_vbif_reglist,\n\t\t\t\t&a6xx_state->registers[index++]);\n\tif (!dumper) {\n\t\t \n\t\tfor (i = 0; i < ARRAY_SIZE(a6xx_reglist); i++)\n\t\t\ta6xx_get_ahb_gpu_registers(gpu,\n\t\t\t\ta6xx_state, &a6xx_reglist[i],\n\t\t\t\t&a6xx_state->registers[index++]);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_reglist); i++)\n\t\ta6xx_get_crashdumper_registers(gpu,\n\t\t\ta6xx_state, &a6xx_reglist[i],\n\t\t\t&a6xx_state->registers[index++],\n\t\t\tdumper);\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_hlsq_reglist); i++)\n\t\ta6xx_get_crashdumper_hlsq_registers(gpu,\n\t\t\ta6xx_state, &a6xx_hlsq_reglist[i],\n\t\t\t&a6xx_state->registers[index++],\n\t\t\tdumper);\n}\n\nstatic u32 a6xx_get_cp_roq_size(struct msm_gpu *gpu)\n{\n\t \n\treturn gpu_read(gpu, REG_A6XX_CP_ROQ_THRESHOLDS_2) >> 14;\n}\n\n \nstatic void a6xx_get_indexed_regs(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state,\n\t\tstruct a6xx_indexed_registers *indexed,\n\t\tstruct a6xx_gpu_state_obj *obj)\n{\n\tint i;\n\n\tobj->handle = (const void *) indexed;\n\tif (indexed->count_fn)\n\t\tindexed->count = indexed->count_fn(gpu);\n\n\tobj->data = state_kcalloc(a6xx_state, indexed->count, sizeof(u32));\n\tif (!obj->data)\n\t\treturn;\n\n\t \n\tgpu_write(gpu, indexed->addr, 0);\n\n\t \n\tfor (i = 0; i < indexed->count; i++)\n\t\tobj->data[i] = gpu_read(gpu, indexed->data);\n}\n\nstatic void a6xx_get_indexed_registers(struct msm_gpu *gpu,\n\t\tstruct a6xx_gpu_state *a6xx_state)\n{\n\tu32 mempool_size;\n\tint count = ARRAY_SIZE(a6xx_indexed_reglist) + 1;\n\tint i;\n\n\ta6xx_state->indexed_regs = state_kcalloc(a6xx_state, count,\n\t\tsizeof(*a6xx_state->indexed_regs));\n\tif (!a6xx_state->indexed_regs)\n\t\treturn;\n\n\tfor (i = 0; i < ARRAY_SIZE(a6xx_indexed_reglist); i++)\n\t\ta6xx_get_indexed_regs(gpu, a6xx_state, &a6xx_indexed_reglist[i],\n\t\t\t&a6xx_state->indexed_regs[i]);\n\n\tif (adreno_is_a650_family(to_adreno_gpu(gpu))) {\n\t\tu32 val;\n\n\t\tval = gpu_read(gpu, REG_A6XX_CP_CHICKEN_DBG);\n\t\tgpu_write(gpu, REG_A6XX_CP_CHICKEN_DBG, val | 4);\n\n\t\t \n\t\ta6xx_get_indexed_regs(gpu, a6xx_state, &a6xx_cp_mempool_indexed,\n\t\t\t&a6xx_state->indexed_regs[i]);\n\n\t\tgpu_write(gpu, REG_A6XX_CP_CHICKEN_DBG, val);\n\t\ta6xx_state->nr_indexed_regs = count;\n\t\treturn;\n\t}\n\n\t \n\tmempool_size = gpu_read(gpu, REG_A6XX_CP_MEM_POOL_SIZE);\n\tgpu_write(gpu, REG_A6XX_CP_MEM_POOL_SIZE, 0);\n\n\t \n\ta6xx_get_indexed_regs(gpu, a6xx_state, &a6xx_cp_mempool_indexed,\n\t\t&a6xx_state->indexed_regs[i]);\n\n\t \n\ta6xx_state->indexed_regs[i].data[0x2000] = mempool_size;\n\n\t \n\tgpu_write(gpu, REG_A6XX_CP_MEM_POOL_SIZE, mempool_size);\n\n\ta6xx_state->nr_indexed_regs = count;\n}\n\nstruct msm_gpu_state *a6xx_gpu_state_get(struct msm_gpu *gpu)\n{\n\tstruct a6xx_crashdumper _dumper = { 0 }, *dumper = NULL;\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a6xx_gpu *a6xx_gpu = to_a6xx_gpu(adreno_gpu);\n\tstruct a6xx_gpu_state *a6xx_state = kzalloc(sizeof(*a6xx_state),\n\t\tGFP_KERNEL);\n\tbool stalled = !!(gpu_read(gpu, REG_A6XX_RBBM_STATUS3) &\n\t\t\tA6XX_RBBM_STATUS3_SMMU_STALLED_ON_FAULT);\n\n\tif (!a6xx_state)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&a6xx_state->objs);\n\n\t \n\tadreno_gpu_state_get(gpu, &a6xx_state->base);\n\n\tif (!adreno_has_gmu_wrapper(adreno_gpu)) {\n\t\ta6xx_get_gmu_registers(gpu, a6xx_state);\n\n\t\ta6xx_state->gmu_log = a6xx_snapshot_gmu_bo(a6xx_state, &a6xx_gpu->gmu.log);\n\t\ta6xx_state->gmu_hfi = a6xx_snapshot_gmu_bo(a6xx_state, &a6xx_gpu->gmu.hfi);\n\t\ta6xx_state->gmu_debug = a6xx_snapshot_gmu_bo(a6xx_state, &a6xx_gpu->gmu.debug);\n\n\t\ta6xx_snapshot_gmu_hfi_history(gpu, a6xx_state);\n\t}\n\n\t \n\tif (!adreno_has_gmu_wrapper(adreno_gpu) && !a6xx_gmu_gx_is_on(&a6xx_gpu->gmu))\n\t\treturn &a6xx_state->base;\n\n\t \n\ta6xx_get_indexed_registers(gpu, a6xx_state);\n\n\t \n\tif (!stalled && !gpu->needs_hw_init &&\n\t    !a6xx_crashdumper_init(gpu, &_dumper)) {\n\t\tdumper = &_dumper;\n\t}\n\n\ta6xx_get_registers(gpu, a6xx_state, dumper);\n\n\tif (dumper) {\n\t\ta6xx_get_shaders(gpu, a6xx_state, dumper);\n\t\ta6xx_get_clusters(gpu, a6xx_state, dumper);\n\t\ta6xx_get_dbgahb_clusters(gpu, a6xx_state, dumper);\n\n\t\tmsm_gem_kernel_put(dumper->bo, gpu->aspace);\n\t}\n\n\tif (snapshot_debugbus)\n\t\ta6xx_get_debugbus(gpu, a6xx_state);\n\n\ta6xx_state->gpu_initialized = !gpu->needs_hw_init;\n\n\treturn  &a6xx_state->base;\n}\n\nstatic void a6xx_gpu_state_destroy(struct kref *kref)\n{\n\tstruct a6xx_state_memobj *obj, *tmp;\n\tstruct msm_gpu_state *state = container_of(kref,\n\t\t\tstruct msm_gpu_state, ref);\n\tstruct a6xx_gpu_state *a6xx_state = container_of(state,\n\t\t\tstruct a6xx_gpu_state, base);\n\n\tif (a6xx_state->gmu_log)\n\t\tkvfree(a6xx_state->gmu_log->data);\n\n\tif (a6xx_state->gmu_hfi)\n\t\tkvfree(a6xx_state->gmu_hfi->data);\n\n\tif (a6xx_state->gmu_debug)\n\t\tkvfree(a6xx_state->gmu_debug->data);\n\n\tlist_for_each_entry_safe(obj, tmp, &a6xx_state->objs, node) {\n\t\tlist_del(&obj->node);\n\t\tkvfree(obj);\n\t}\n\n\tadreno_gpu_state_destroy(state);\n\tkfree(a6xx_state);\n}\n\nint a6xx_gpu_state_put(struct msm_gpu_state *state)\n{\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn 1;\n\n\treturn kref_put(&state->ref, a6xx_gpu_state_destroy);\n}\n\nstatic void a6xx_show_registers(const u32 *registers, u32 *data, size_t count,\n\t\tstruct drm_printer *p)\n{\n\tint i, index = 0;\n\n\tif (!data)\n\t\treturn;\n\n\tfor (i = 0; i < count; i += 2) {\n\t\tu32 count = RANGE(registers, i);\n\t\tu32 offset = registers[i];\n\t\tint j;\n\n\t\tfor (j = 0; j < count; index++, offset++, j++) {\n\t\t\tif (data[index] == 0xdeafbead)\n\t\t\t\tcontinue;\n\n\t\t\tdrm_printf(p, \"  - { offset: 0x%06x, value: 0x%08x }\\n\",\n\t\t\t\toffset << 2, data[index]);\n\t\t}\n\t}\n}\n\nstatic void print_ascii85(struct drm_printer *p, size_t len, u32 *data)\n{\n\tchar out[ASCII85_BUFSZ];\n\tlong i, l, datalen = 0;\n\n\tfor (i = 0; i < len >> 2; i++) {\n\t\tif (data[i])\n\t\t\tdatalen = (i + 1) << 2;\n\t}\n\n\tif (datalen == 0)\n\t\treturn;\n\n\tdrm_puts(p, \"    data: !!ascii85 |\\n\");\n\tdrm_puts(p, \"      \");\n\n\n\tl = ascii85_encode_len(datalen);\n\n\tfor (i = 0; i < l; i++)\n\t\tdrm_puts(p, ascii85_encode(data[i], out));\n\n\tdrm_puts(p, \"\\n\");\n}\n\nstatic void print_name(struct drm_printer *p, const char *fmt, const char *name)\n{\n\tdrm_puts(p, fmt);\n\tdrm_puts(p, name);\n\tdrm_puts(p, \"\\n\");\n}\n\nstatic void a6xx_show_shader(struct a6xx_gpu_state_obj *obj,\n\t\tstruct drm_printer *p)\n{\n\tconst struct a6xx_shader_block *block = obj->handle;\n\tint i;\n\n\tif (!obj->handle)\n\t\treturn;\n\n\tprint_name(p, \"  - type: \", block->name);\n\n\tfor (i = 0; i < A6XX_NUM_SHADER_BANKS; i++) {\n\t\tdrm_printf(p, \"    - bank: %d\\n\", i);\n\t\tdrm_printf(p, \"      size: %d\\n\", block->size);\n\n\t\tif (!obj->data)\n\t\t\tcontinue;\n\n\t\tprint_ascii85(p, block->size << 2,\n\t\t\tobj->data + (block->size * i));\n\t}\n}\n\nstatic void a6xx_show_cluster_data(const u32 *registers, int size, u32 *data,\n\t\tstruct drm_printer *p)\n{\n\tint ctx, index = 0;\n\n\tfor (ctx = 0; ctx < A6XX_NUM_CONTEXTS; ctx++) {\n\t\tint j;\n\n\t\tdrm_printf(p, \"    - context: %d\\n\", ctx);\n\n\t\tfor (j = 0; j < size; j += 2) {\n\t\t\tu32 count = RANGE(registers, j);\n\t\t\tu32 offset = registers[j];\n\t\t\tint k;\n\n\t\t\tfor (k = 0; k < count; index++, offset++, k++) {\n\t\t\t\tif (data[index] == 0xdeafbead)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tdrm_printf(p, \"      - { offset: 0x%06x, value: 0x%08x }\\n\",\n\t\t\t\t\toffset << 2, data[index]);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void a6xx_show_dbgahb_cluster(struct a6xx_gpu_state_obj *obj,\n\t\tstruct drm_printer *p)\n{\n\tconst struct a6xx_dbgahb_cluster *dbgahb = obj->handle;\n\n\tif (dbgahb) {\n\t\tprint_name(p, \"  - cluster-name: \", dbgahb->name);\n\t\ta6xx_show_cluster_data(dbgahb->registers, dbgahb->count,\n\t\t\tobj->data, p);\n\t}\n}\n\nstatic void a6xx_show_cluster(struct a6xx_gpu_state_obj *obj,\n\t\tstruct drm_printer *p)\n{\n\tconst struct a6xx_cluster *cluster = obj->handle;\n\n\tif (cluster) {\n\t\tprint_name(p, \"  - cluster-name: \", cluster->name);\n\t\ta6xx_show_cluster_data(cluster->registers, cluster->count,\n\t\t\tobj->data, p);\n\t}\n}\n\nstatic void a6xx_show_indexed_regs(struct a6xx_gpu_state_obj *obj,\n\t\tstruct drm_printer *p)\n{\n\tconst struct a6xx_indexed_registers *indexed = obj->handle;\n\n\tif (!indexed)\n\t\treturn;\n\n\tprint_name(p, \"  - regs-name: \", indexed->name);\n\tdrm_printf(p, \"    dwords: %d\\n\", indexed->count);\n\n\tprint_ascii85(p, indexed->count << 2, obj->data);\n}\n\nstatic void a6xx_show_debugbus_block(const struct a6xx_debugbus_block *block,\n\t\tu32 *data, struct drm_printer *p)\n{\n\tif (block) {\n\t\tprint_name(p, \"  - debugbus-block: \", block->name);\n\n\t\t \n\t\tdrm_printf(p, \"    count: %d\\n\", block->count << 1);\n\n\t\tprint_ascii85(p, block->count << 3, data);\n\t}\n}\n\nstatic void a6xx_show_debugbus(struct a6xx_gpu_state *a6xx_state,\n\t\tstruct drm_printer *p)\n{\n\tint i;\n\n\tfor (i = 0; i < a6xx_state->nr_debugbus; i++) {\n\t\tstruct a6xx_gpu_state_obj *obj = &a6xx_state->debugbus[i];\n\n\t\ta6xx_show_debugbus_block(obj->handle, obj->data, p);\n\t}\n\n\tif (a6xx_state->vbif_debugbus) {\n\t\tstruct a6xx_gpu_state_obj *obj = a6xx_state->vbif_debugbus;\n\n\t\tdrm_puts(p, \"  - debugbus-block: A6XX_DBGBUS_VBIF\\n\");\n\t\tdrm_printf(p, \"    count: %d\\n\", VBIF_DEBUGBUS_BLOCK_SIZE);\n\n\t\t \n\t\tprint_ascii85(p, VBIF_DEBUGBUS_BLOCK_SIZE << 2, obj->data);\n\t}\n\n\tfor (i = 0; i < a6xx_state->nr_cx_debugbus; i++) {\n\t\tstruct a6xx_gpu_state_obj *obj = &a6xx_state->cx_debugbus[i];\n\n\t\ta6xx_show_debugbus_block(obj->handle, obj->data, p);\n\t}\n}\n\nvoid a6xx_show(struct msm_gpu *gpu, struct msm_gpu_state *state,\n\t\tstruct drm_printer *p)\n{\n\tstruct a6xx_gpu_state *a6xx_state = container_of(state,\n\t\t\tstruct a6xx_gpu_state, base);\n\tint i;\n\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn;\n\n\tdrm_printf(p, \"gpu-initialized: %d\\n\", a6xx_state->gpu_initialized);\n\n\tadreno_show(gpu, state, p);\n\n\tdrm_puts(p, \"gmu-log:\\n\");\n\tif (a6xx_state->gmu_log) {\n\t\tstruct msm_gpu_state_bo *gmu_log = a6xx_state->gmu_log;\n\n\t\tdrm_printf(p, \"    iova: 0x%016llx\\n\", gmu_log->iova);\n\t\tdrm_printf(p, \"    size: %zu\\n\", gmu_log->size);\n\t\tadreno_show_object(p, &gmu_log->data, gmu_log->size,\n\t\t\t\t&gmu_log->encoded);\n\t}\n\n\tdrm_puts(p, \"gmu-hfi:\\n\");\n\tif (a6xx_state->gmu_hfi) {\n\t\tstruct msm_gpu_state_bo *gmu_hfi = a6xx_state->gmu_hfi;\n\t\tunsigned i, j;\n\n\t\tdrm_printf(p, \"    iova: 0x%016llx\\n\", gmu_hfi->iova);\n\t\tdrm_printf(p, \"    size: %zu\\n\", gmu_hfi->size);\n\t\tfor (i = 0; i < ARRAY_SIZE(a6xx_state->hfi_queue_history); i++) {\n\t\t\tdrm_printf(p, \"    queue-history[%u]:\", i);\n\t\t\tfor (j = 0; j < HFI_HISTORY_SZ; j++) {\n\t\t\t\tdrm_printf(p, \" %d\", a6xx_state->hfi_queue_history[i][j]);\n\t\t\t}\n\t\t\tdrm_printf(p, \"\\n\");\n\t\t}\n\t\tadreno_show_object(p, &gmu_hfi->data, gmu_hfi->size,\n\t\t\t\t&gmu_hfi->encoded);\n\t}\n\n\tdrm_puts(p, \"gmu-debug:\\n\");\n\tif (a6xx_state->gmu_debug) {\n\t\tstruct msm_gpu_state_bo *gmu_debug = a6xx_state->gmu_debug;\n\n\t\tdrm_printf(p, \"    iova: 0x%016llx\\n\", gmu_debug->iova);\n\t\tdrm_printf(p, \"    size: %zu\\n\", gmu_debug->size);\n\t\tadreno_show_object(p, &gmu_debug->data, gmu_debug->size,\n\t\t\t\t&gmu_debug->encoded);\n\t}\n\n\tdrm_puts(p, \"registers:\\n\");\n\tfor (i = 0; i < a6xx_state->nr_registers; i++) {\n\t\tstruct a6xx_gpu_state_obj *obj = &a6xx_state->registers[i];\n\t\tconst struct a6xx_registers *regs = obj->handle;\n\n\t\tif (!obj->handle)\n\t\t\tcontinue;\n\n\t\ta6xx_show_registers(regs->registers, obj->data, regs->count, p);\n\t}\n\n\tdrm_puts(p, \"registers-gmu:\\n\");\n\tfor (i = 0; i < a6xx_state->nr_gmu_registers; i++) {\n\t\tstruct a6xx_gpu_state_obj *obj = &a6xx_state->gmu_registers[i];\n\t\tconst struct a6xx_registers *regs = obj->handle;\n\n\t\tif (!obj->handle)\n\t\t\tcontinue;\n\n\t\ta6xx_show_registers(regs->registers, obj->data, regs->count, p);\n\t}\n\n\tdrm_puts(p, \"indexed-registers:\\n\");\n\tfor (i = 0; i < a6xx_state->nr_indexed_regs; i++)\n\t\ta6xx_show_indexed_regs(&a6xx_state->indexed_regs[i], p);\n\n\tdrm_puts(p, \"shader-blocks:\\n\");\n\tfor (i = 0; i < a6xx_state->nr_shaders; i++)\n\t\ta6xx_show_shader(&a6xx_state->shaders[i], p);\n\n\tdrm_puts(p, \"clusters:\\n\");\n\tfor (i = 0; i < a6xx_state->nr_clusters; i++)\n\t\ta6xx_show_cluster(&a6xx_state->clusters[i], p);\n\n\tfor (i = 0; i < a6xx_state->nr_dbgahb_clusters; i++)\n\t\ta6xx_show_dbgahb_cluster(&a6xx_state->dbgahb_clusters[i], p);\n\n\tdrm_puts(p, \"debugbus:\\n\");\n\ta6xx_show_debugbus(a6xx_state, p);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}