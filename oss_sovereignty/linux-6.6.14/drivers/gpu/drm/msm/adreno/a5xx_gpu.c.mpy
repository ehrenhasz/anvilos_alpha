{
  "module_name": "a5xx_gpu.c",
  "hash_id": "243596eefb442fa5d0e2a6187cccf94373ff4a55032f08e6a11626f8786082a4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/adreno/a5xx_gpu.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/cpumask.h>\n#include <linux/firmware/qcom/qcom_scm.h>\n#include <linux/pm_opp.h>\n#include <linux/nvmem-consumer.h>\n#include <linux/slab.h>\n#include \"msm_gem.h\"\n#include \"msm_mmu.h\"\n#include \"a5xx_gpu.h\"\n\nextern bool hang_debug;\nstatic void a5xx_dump(struct msm_gpu *gpu);\n\n#define GPU_PAS_ID 13\n\nstatic void update_shadow_rptr(struct msm_gpu *gpu, struct msm_ringbuffer *ring)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\n\tif (a5xx_gpu->has_whereami) {\n\t\tOUT_PKT7(ring, CP_WHERE_AM_I, 2);\n\t\tOUT_RING(ring, lower_32_bits(shadowptr(a5xx_gpu, ring)));\n\t\tOUT_RING(ring, upper_32_bits(shadowptr(a5xx_gpu, ring)));\n\t}\n}\n\nvoid a5xx_flush(struct msm_gpu *gpu, struct msm_ringbuffer *ring,\n\t\tbool sync)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tuint32_t wptr;\n\tunsigned long flags;\n\n\t \n\tif (sync)\n\t\tupdate_shadow_rptr(gpu, ring);\n\n\tspin_lock_irqsave(&ring->preempt_lock, flags);\n\n\t \n\tring->cur = ring->next;\n\n\t \n\twptr = get_wptr(ring);\n\n\tspin_unlock_irqrestore(&ring->preempt_lock, flags);\n\n\t \n\tmb();\n\n\t \n\tif (a5xx_gpu->cur_ring == ring && !a5xx_in_preempt(a5xx_gpu))\n\t\tgpu_write(gpu, REG_A5XX_CP_RB_WPTR, wptr);\n}\n\nstatic void a5xx_submit_in_rb(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n{\n\tstruct msm_ringbuffer *ring = submit->ring;\n\tstruct drm_gem_object *obj;\n\tuint32_t *ptr, dwords;\n\tunsigned int i;\n\n\tfor (i = 0; i < submit->nr_cmds; i++) {\n\t\tswitch (submit->cmd[i].type) {\n\t\tcase MSM_SUBMIT_CMD_IB_TARGET_BUF:\n\t\t\tbreak;\n\t\tcase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\n\t\t\tif (gpu->cur_ctx_seqno == submit->queue->ctx->seqno)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase MSM_SUBMIT_CMD_BUF:\n\t\t\t \n\t\t\tobj = submit->bos[submit->cmd[i].idx].obj;\n\t\t\tdwords = submit->cmd[i].size;\n\n\t\t\tptr = msm_gem_get_vaddr(obj);\n\n\t\t\t \n\t\t\tif (WARN_ON(IS_ERR_OR_NULL(ptr)))\n\t\t\t\treturn;\n\n\t\t\tfor (i = 0; i < dwords; i++) {\n\t\t\t\t \n\t\t\t\tadreno_wait_ring(ring, 1);\n\t\t\t\tOUT_RING(ring, ptr[i]);\n\t\t\t}\n\n\t\t\tmsm_gem_put_vaddr(obj);\n\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ta5xx_flush(gpu, ring, true);\n\ta5xx_preempt_trigger(gpu);\n\n\t \n\ta5xx_idle(gpu, ring);\n\tring->memptrs->fence = submit->seqno;\n\tmsm_gpu_retire(gpu);\n}\n\nstatic void a5xx_submit(struct msm_gpu *gpu, struct msm_gem_submit *submit)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tstruct msm_ringbuffer *ring = submit->ring;\n\tunsigned int i, ibs = 0;\n\n\tif (IS_ENABLED(CONFIG_DRM_MSM_GPU_SUDO) && submit->in_rb) {\n\t\tgpu->cur_ctx_seqno = 0;\n\t\ta5xx_submit_in_rb(gpu, submit);\n\t\treturn;\n\t}\n\n\tOUT_PKT7(ring, CP_PREEMPT_ENABLE_GLOBAL, 1);\n\tOUT_RING(ring, 0x02);\n\n\t \n\tOUT_PKT7(ring, CP_SET_PROTECTED_MODE, 1);\n\tOUT_RING(ring, 0);\n\n\t \n\tOUT_PKT4(ring, REG_A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_LO, 2);\n\tOUT_RING(ring, lower_32_bits(a5xx_gpu->preempt_iova[submit->ring->id]));\n\tOUT_RING(ring, upper_32_bits(a5xx_gpu->preempt_iova[submit->ring->id]));\n\n\t \n\tOUT_PKT7(ring, CP_SET_PROTECTED_MODE, 1);\n\tOUT_RING(ring, 1);\n\n\t \n\tOUT_PKT7(ring, CP_PREEMPT_ENABLE_LOCAL, 1);\n\tOUT_RING(ring, 0x1);\n\n\t \n\tOUT_PKT7(ring, CP_YIELD_ENABLE, 1);\n\tOUT_RING(ring, 0x02);\n\n\t \n\tfor (i = 0; i < submit->nr_cmds; i++) {\n\t\tswitch (submit->cmd[i].type) {\n\t\tcase MSM_SUBMIT_CMD_IB_TARGET_BUF:\n\t\t\tbreak;\n\t\tcase MSM_SUBMIT_CMD_CTX_RESTORE_BUF:\n\t\t\tif (gpu->cur_ctx_seqno == submit->queue->ctx->seqno)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase MSM_SUBMIT_CMD_BUF:\n\t\t\tOUT_PKT7(ring, CP_INDIRECT_BUFFER_PFE, 3);\n\t\t\tOUT_RING(ring, lower_32_bits(submit->cmd[i].iova));\n\t\t\tOUT_RING(ring, upper_32_bits(submit->cmd[i].iova));\n\t\t\tOUT_RING(ring, submit->cmd[i].size);\n\t\t\tibs++;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif ((ibs % 32) == 0)\n\t\t\tupdate_shadow_rptr(gpu, ring);\n\t}\n\n\t \n\tOUT_PKT7(ring, CP_SET_RENDER_MODE, 5);\n\tOUT_RING(ring, 0);\n\tOUT_RING(ring, 0);\n\tOUT_RING(ring, 0);\n\tOUT_RING(ring, 0);\n\tOUT_RING(ring, 0);\n\n\t \n\tOUT_PKT7(ring, CP_YIELD_ENABLE, 1);\n\tOUT_RING(ring, 0x01);\n\n\t \n\tOUT_PKT4(ring, REG_A5XX_CP_SCRATCH_REG(2), 1);\n\tOUT_RING(ring, submit->seqno);\n\n\t \n\tOUT_PKT7(ring, CP_EVENT_WRITE, 4);\n\tOUT_RING(ring, CP_EVENT_WRITE_0_EVENT(CACHE_FLUSH_TS) |\n\t\tCP_EVENT_WRITE_0_IRQ);\n\tOUT_RING(ring, lower_32_bits(rbmemptr(ring, fence)));\n\tOUT_RING(ring, upper_32_bits(rbmemptr(ring, fence)));\n\tOUT_RING(ring, submit->seqno);\n\n\t \n\tOUT_PKT7(ring, CP_CONTEXT_SWITCH_YIELD, 4);\n\t \n\tOUT_RING(ring, 0x00);\n\tOUT_RING(ring, 0x00);\n\t \n\tOUT_RING(ring, 0x01);\n\t \n\tOUT_RING(ring, 0x01);\n\n\t \n\ta5xx_flush(gpu, ring, false);\n\n\t \n\ta5xx_preempt_trigger(gpu);\n}\n\nstatic const struct adreno_five_hwcg_regs {\n\tu32 offset;\n\tu32 value;\n} a5xx_hwcg[] = {\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP0, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP1, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP2, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP3, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP0, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP1, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP2, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP3, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP0, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP1, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP2, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP3, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP1, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP2, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP3, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP2, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP3, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP2, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP3, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP0, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP1, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP2, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP3, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP1, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP2, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP3, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP1, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP2, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP3, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP0, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP1, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP2, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP3, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP1, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP2, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP3, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP1, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP2, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP3, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP0, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP1, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP2, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP3, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_UCHE, 0x00444444},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB2, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB3, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB0, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB1, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB2, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB3, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU1, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU2, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU3, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU2, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU3, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_0, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_1, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_2, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_3, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RAC, 0x00010011},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_GPC, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_VFD, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_HYST_GPC, 0x04104004},\n\t{REG_A5XX_RBBM_CLOCK_HYST_VFD, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_VFD, 0x00002222}\n}, a50x_hwcg[] = {\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP0, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP0, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP0, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP0, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP0, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP0, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_UCHE, 0x00FFFFF4},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB0, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_0, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RAC, 0x00010011},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_GPC, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_VFD, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_HYST_GPC, 0x04104004},\n\t{REG_A5XX_RBBM_CLOCK_HYST_VFD, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_VFD, 0x00002222},\n}, a512_hwcg[] = {\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP0, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_SP1, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP0, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_SP1, 0x02222220},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP0, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_HYST_SP1, 0x0000F3CF},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP0, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_SP1, 0x00000080},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TP1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_TP1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP0, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_TP1, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TP1, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP0, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST2_TP1, 0x77777777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP0, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_HYST3_TP1, 0x00007777},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TP1, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP0, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY2_TP1, 0x11111111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP0, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_DELAY3_TP1, 0x00001111},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL3_UCHE, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL4_UCHE, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_UCHE, 0x00444444},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_UCHE, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB0, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RB1, 0x22222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB0, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RB1, 0x00222222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU0, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_CCU1, 0x00022220},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_RAC, 0x05522222},\n\t{REG_A5XX_RBBM_CLOCK_CNTL2_RAC, 0x00505555},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU0, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RB_CCU1, 0x04040404},\n\t{REG_A5XX_RBBM_CLOCK_HYST_RAC, 0x07444044},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_0, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RB_CCU_L1_1, 0x00000002},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_RAC, 0x00010011},\n\t{REG_A5XX_RBBM_CLOCK_CNTL_TSE_RAS_RBBM, 0x04222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_GPC, 0x02222222},\n\t{REG_A5XX_RBBM_CLOCK_MODE_VFD, 0x00002222},\n\t{REG_A5XX_RBBM_CLOCK_HYST_TSE_RAS_RBBM, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_HYST_GPC, 0x04104004},\n\t{REG_A5XX_RBBM_CLOCK_HYST_VFD, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_HLSQ, 0x00000000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_TSE_RAS_RBBM, 0x00004000},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_GPC, 0x00000200},\n\t{REG_A5XX_RBBM_CLOCK_DELAY_VFD, 0x00002222},\n};\n\nvoid a5xx_set_hwcg(struct msm_gpu *gpu, bool state)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tconst struct adreno_five_hwcg_regs *regs;\n\tunsigned int i, sz;\n\n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu)) {\n\t\tregs = a50x_hwcg;\n\t\tsz = ARRAY_SIZE(a50x_hwcg);\n\t} else if (adreno_is_a509(adreno_gpu) || adreno_is_a512(adreno_gpu)) {\n\t\tregs = a512_hwcg;\n\t\tsz = ARRAY_SIZE(a512_hwcg);\n\t} else {\n\t\tregs = a5xx_hwcg;\n\t\tsz = ARRAY_SIZE(a5xx_hwcg);\n\t}\n\n\tfor (i = 0; i < sz; i++)\n\t\tgpu_write(gpu, regs[i].offset,\n\t\t\t  state ? regs[i].value : 0);\n\n\tif (adreno_is_a540(adreno_gpu)) {\n\t\tgpu_write(gpu, REG_A5XX_RBBM_CLOCK_DELAY_GPMU, state ? 0x00000770 : 0);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_CLOCK_HYST_GPMU, state ? 0x00000004 : 0);\n\t}\n\n\tgpu_write(gpu, REG_A5XX_RBBM_CLOCK_CNTL, state ? 0xAAA8AA00 : 0);\n\tgpu_write(gpu, REG_A5XX_RBBM_ISDB_CNT, state ? 0x182 : 0x180);\n}\n\nstatic int a5xx_me_init(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct msm_ringbuffer *ring = gpu->rb[0];\n\n\tOUT_PKT7(ring, CP_ME_INIT, 8);\n\n\tOUT_RING(ring, 0x0000002F);\n\n\t \n\tOUT_RING(ring, 0x00000003);\n\n\t \n\tOUT_RING(ring, 0x20000000);\n\n\t \n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\n\t \n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a530(adreno_gpu)) {\n\t\t \n\t\tOUT_RING(ring, 0x0000000B);\n\t} else if (adreno_is_a510(adreno_gpu)) {\n\t\t \n\t\tOUT_RING(ring, 0x00000001);\n\t} else {\n\t\t \n\t\tOUT_RING(ring, 0x00000000);\n\t}\n\n\tOUT_RING(ring, 0x00000000);\n\tOUT_RING(ring, 0x00000000);\n\n\ta5xx_flush(gpu, ring, true);\n\treturn a5xx_idle(gpu, ring) ? 0 : -EINVAL;\n}\n\nstatic int a5xx_preempt_start(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tstruct msm_ringbuffer *ring = gpu->rb[0];\n\n\tif (gpu->nr_rings == 1)\n\t\treturn 0;\n\n\t \n\tOUT_PKT7(ring, CP_SET_PROTECTED_MODE, 1);\n\tOUT_RING(ring, 0);\n\n\t \n\tOUT_PKT4(ring, REG_A5XX_CP_CONTEXT_SWITCH_SAVE_ADDR_LO, 2);\n\tOUT_RING(ring, lower_32_bits(a5xx_gpu->preempt_iova[ring->id]));\n\tOUT_RING(ring, upper_32_bits(a5xx_gpu->preempt_iova[ring->id]));\n\n\t \n\tOUT_PKT7(ring, CP_SET_PROTECTED_MODE, 1);\n\tOUT_RING(ring, 1);\n\n\tOUT_PKT7(ring, CP_PREEMPT_ENABLE_GLOBAL, 1);\n\tOUT_RING(ring, 0x00);\n\n\tOUT_PKT7(ring, CP_PREEMPT_ENABLE_LOCAL, 1);\n\tOUT_RING(ring, 0x01);\n\n\tOUT_PKT7(ring, CP_YIELD_ENABLE, 1);\n\tOUT_RING(ring, 0x01);\n\n\t \n\tOUT_PKT7(ring, CP_CONTEXT_SWITCH_YIELD, 4);\n\tOUT_RING(ring, 0x00);\n\tOUT_RING(ring, 0x00);\n\tOUT_RING(ring, 0x01);\n\tOUT_RING(ring, 0x01);\n\n\t \n\ta5xx_flush(gpu, ring, false);\n\n\treturn a5xx_idle(gpu, ring) ? 0 : -EINVAL;\n}\n\nstatic void a5xx_ucode_check_version(struct a5xx_gpu *a5xx_gpu,\n\t\tstruct drm_gem_object *obj)\n{\n\tu32 *buf = msm_gem_get_vaddr(obj);\n\n\tif (IS_ERR(buf))\n\t\treturn;\n\n\t \n\tif (((buf[0] & 0xf) == 0xa) && (buf[2] & 0xf) >= 1)\n\t\ta5xx_gpu->has_whereami = true;\n\n\tmsm_gem_put_vaddr(obj);\n}\n\nstatic int a5xx_ucode_load(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tint ret;\n\n\tif (!a5xx_gpu->pm4_bo) {\n\t\ta5xx_gpu->pm4_bo = adreno_fw_create_bo(gpu,\n\t\t\tadreno_gpu->fw[ADRENO_FW_PM4], &a5xx_gpu->pm4_iova);\n\n\n\t\tif (IS_ERR(a5xx_gpu->pm4_bo)) {\n\t\t\tret = PTR_ERR(a5xx_gpu->pm4_bo);\n\t\t\ta5xx_gpu->pm4_bo = NULL;\n\t\t\tDRM_DEV_ERROR(gpu->dev->dev, \"could not allocate PM4: %d\\n\",\n\t\t\t\tret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmsm_gem_object_set_name(a5xx_gpu->pm4_bo, \"pm4fw\");\n\t}\n\n\tif (!a5xx_gpu->pfp_bo) {\n\t\ta5xx_gpu->pfp_bo = adreno_fw_create_bo(gpu,\n\t\t\tadreno_gpu->fw[ADRENO_FW_PFP], &a5xx_gpu->pfp_iova);\n\n\t\tif (IS_ERR(a5xx_gpu->pfp_bo)) {\n\t\t\tret = PTR_ERR(a5xx_gpu->pfp_bo);\n\t\t\ta5xx_gpu->pfp_bo = NULL;\n\t\t\tDRM_DEV_ERROR(gpu->dev->dev, \"could not allocate PFP: %d\\n\",\n\t\t\t\tret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tmsm_gem_object_set_name(a5xx_gpu->pfp_bo, \"pfpfw\");\n\t\ta5xx_ucode_check_version(a5xx_gpu, a5xx_gpu->pfp_bo);\n\t}\n\n\tif (a5xx_gpu->has_whereami) {\n\t\tif (!a5xx_gpu->shadow_bo) {\n\t\t\ta5xx_gpu->shadow = msm_gem_kernel_new(gpu->dev,\n\t\t\t\tsizeof(u32) * gpu->nr_rings,\n\t\t\t\tMSM_BO_WC | MSM_BO_MAP_PRIV,\n\t\t\t\tgpu->aspace, &a5xx_gpu->shadow_bo,\n\t\t\t\t&a5xx_gpu->shadow_iova);\n\n\t\t\tif (IS_ERR(a5xx_gpu->shadow))\n\t\t\t\treturn PTR_ERR(a5xx_gpu->shadow);\n\n\t\t\tmsm_gem_object_set_name(a5xx_gpu->shadow_bo, \"shadow\");\n\t\t}\n\t} else if (gpu->nr_rings > 1) {\n\t\t \n\t\ta5xx_preempt_fini(gpu);\n\t\tgpu->nr_rings = 1;\n\t}\n\n\treturn 0;\n}\n\n#define SCM_GPU_ZAP_SHADER_RESUME 0\n\nstatic int a5xx_zap_shader_resume(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint ret;\n\n\t \n\tif (adreno_is_a506(adreno_gpu))\n\t\treturn 0;\n\n\tret = qcom_scm_set_remote_state(SCM_GPU_ZAP_SHADER_RESUME, GPU_PAS_ID);\n\tif (ret)\n\t\tDRM_ERROR(\"%s: zap-shader resume failed: %d\\n\",\n\t\t\tgpu->name, ret);\n\n\treturn ret;\n}\n\nstatic int a5xx_zap_shader_init(struct msm_gpu *gpu)\n{\n\tstatic bool loaded;\n\tint ret;\n\n\t \n\tif (loaded)\n\t\treturn a5xx_zap_shader_resume(gpu);\n\n\tret = adreno_zap_shader_load(gpu, GPU_PAS_ID);\n\n\tloaded = !ret;\n\treturn ret;\n}\n\n#define A5XX_INT_MASK (A5XX_RBBM_INT_0_MASK_RBBM_AHB_ERROR | \\\n\t  A5XX_RBBM_INT_0_MASK_RBBM_TRANSFER_TIMEOUT | \\\n\t  A5XX_RBBM_INT_0_MASK_RBBM_ME_MS_TIMEOUT | \\\n\t  A5XX_RBBM_INT_0_MASK_RBBM_PFP_MS_TIMEOUT | \\\n\t  A5XX_RBBM_INT_0_MASK_RBBM_ETS_MS_TIMEOUT | \\\n\t  A5XX_RBBM_INT_0_MASK_RBBM_ATB_ASYNC_OVERFLOW | \\\n\t  A5XX_RBBM_INT_0_MASK_CP_HW_ERROR | \\\n\t  A5XX_RBBM_INT_0_MASK_MISC_HANG_DETECT | \\\n\t  A5XX_RBBM_INT_0_MASK_CP_SW | \\\n\t  A5XX_RBBM_INT_0_MASK_CP_CACHE_FLUSH_TS | \\\n\t  A5XX_RBBM_INT_0_MASK_UCHE_OOB_ACCESS | \\\n\t  A5XX_RBBM_INT_0_MASK_GPMU_VOLTAGE_DROOP)\n\nstatic int a5xx_hw_init(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tu32 regbit;\n\tint ret;\n\n\tgpu_write(gpu, REG_A5XX_VBIF_ROUND_ROBIN_QOS_ARB, 0x00000003);\n\n\tif (adreno_is_a509(adreno_gpu) || adreno_is_a512(adreno_gpu) ||\n\t    adreno_is_a540(adreno_gpu))\n\t\tgpu_write(gpu, REG_A5XX_VBIF_GATE_OFF_WRREQ_EN, 0x00000009);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_PERFCTR_GPU_BUSY_MASKED, 0xFFFFFFFF);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_AHB_CNTL0, 0x00000001);\n\n\tif (adreno_gpu->info->quirks & ADRENO_QUIRK_FAULT_DETECT_MASK) {\n\t\t \n\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL11,\n\t\t\t0xF0000000);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL12,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL13,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL14,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL15,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL16,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL17,\n\t\t\t0xFFFFFFFF);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_MASK_CNTL18,\n\t\t\t0xFFFFFFFF);\n\t}\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_INTERFACE_HANG_INT_CNTL,\n\t\t(1 << 30) | 0xFFFF);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_PERFCTR_CNTL, 0x01);\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PERFCTR_CP_SEL_0, PERF_CP_ALWAYS_COUNT);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_PERFCTR_RBBM_SEL_0, 6);\n\n\t \n\tgpu_write(gpu, REG_A5XX_UCHE_CACHE_WAYS, 0x02);\n\n\t \n\tgpu_write(gpu, REG_A5XX_UCHE_TRAP_BASE_LO, 0xFFFF0000);\n\tgpu_write(gpu, REG_A5XX_UCHE_TRAP_BASE_HI, 0x0001FFFF);\n\tgpu_write(gpu, REG_A5XX_UCHE_WRITE_THRU_BASE_LO, 0xFFFF0000);\n\tgpu_write(gpu, REG_A5XX_UCHE_WRITE_THRU_BASE_HI, 0x0001FFFF);\n\n\t \n\tgpu_write(gpu, REG_A5XX_UCHE_GMEM_RANGE_MIN_LO, 0x00100000);\n\tgpu_write(gpu, REG_A5XX_UCHE_GMEM_RANGE_MIN_HI, 0x00000000);\n\tgpu_write(gpu, REG_A5XX_UCHE_GMEM_RANGE_MAX_LO,\n\t\t0x00100000 + adreno_gpu->info->gmem - 1);\n\tgpu_write(gpu, REG_A5XX_UCHE_GMEM_RANGE_MAX_HI, 0x00000000);\n\n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu) ||\n\t    adreno_is_a510(adreno_gpu)) {\n\t\tgpu_write(gpu, REG_A5XX_CP_MEQ_THRESHOLDS, 0x20);\n\t\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu))\n\t\t\tgpu_write(gpu, REG_A5XX_CP_MERCIU_SIZE, 0x400);\n\t\telse\n\t\t\tgpu_write(gpu, REG_A5XX_CP_MERCIU_SIZE, 0x20);\n\t\tgpu_write(gpu, REG_A5XX_CP_ROQ_THRESHOLDS_2, 0x40000030);\n\t\tgpu_write(gpu, REG_A5XX_CP_ROQ_THRESHOLDS_1, 0x20100D0A);\n\t} else {\n\t\tgpu_write(gpu, REG_A5XX_CP_MEQ_THRESHOLDS, 0x40);\n\t\tif (adreno_is_a530(adreno_gpu))\n\t\t\tgpu_write(gpu, REG_A5XX_CP_MERCIU_SIZE, 0x40);\n\t\telse\n\t\t\tgpu_write(gpu, REG_A5XX_CP_MERCIU_SIZE, 0x400);\n\t\tgpu_write(gpu, REG_A5XX_CP_ROQ_THRESHOLDS_2, 0x80000060);\n\t\tgpu_write(gpu, REG_A5XX_CP_ROQ_THRESHOLDS_1, 0x40201B16);\n\t}\n\n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu))\n\t\tgpu_write(gpu, REG_A5XX_PC_DBG_ECO_CNTL,\n\t\t\t  (0x100 << 11 | 0x100 << 22));\n\telse if (adreno_is_a509(adreno_gpu) || adreno_is_a510(adreno_gpu) ||\n\t\t adreno_is_a512(adreno_gpu))\n\t\tgpu_write(gpu, REG_A5XX_PC_DBG_ECO_CNTL,\n\t\t\t  (0x200 << 11 | 0x200 << 22));\n\telse\n\t\tgpu_write(gpu, REG_A5XX_PC_DBG_ECO_CNTL,\n\t\t\t  (0x400 << 11 | 0x300 << 22));\n\n\tif (adreno_gpu->info->quirks & ADRENO_QUIRK_TWO_PASS_USE_WFI)\n\t\tgpu_rmw(gpu, REG_A5XX_PC_DBG_ECO_CNTL, 0, (1 << 8));\n\n\t \n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu) ||\n\t    adreno_is_a509(adreno_gpu) || adreno_is_a512(adreno_gpu))\n\t\tgpu_rmw(gpu, REG_A5XX_RB_DBG_ECO_CNTL, 0, (1 << 9));\n\n\t \n\tgpu_write(gpu, REG_A5XX_UCHE_MODE_CNTL, BIT(29));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_CHICKEN_DBG, 0x02000000);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_AHB_CNTL1, 0xA6FFFFFF);\n\n\t \n\tif (adreno_is_a510(adreno_gpu))\n\t\tgpu_rmw(gpu, REG_A5XX_RB_DBG_ECO_CNTL, (1 << 11), 0);\n\n\t \n\ta5xx_set_hwcg(gpu, true);\n\n\tgpu_write(gpu, REG_A5XX_RBBM_AHB_CNTL2, 0x0000003F);\n\n\t \n\tif (adreno_is_a540(adreno_gpu) || adreno_is_a530(adreno_gpu))\n\t\tregbit = 2;\n\telse\n\t\tregbit = 1;\n\n\tgpu_write(gpu, REG_A5XX_TPL1_MODE_CNTL, regbit << 7);\n\tgpu_write(gpu, REG_A5XX_RB_MODE_CNTL, regbit << 1);\n\n\tif (adreno_is_a509(adreno_gpu) || adreno_is_a512(adreno_gpu) ||\n\t    adreno_is_a540(adreno_gpu))\n\t\tgpu_write(gpu, REG_A5XX_UCHE_DBG_ECO_CNTL_2, regbit);\n\n\t \n\tgpu_rmw(gpu, REG_A5XX_VPC_DBG_ECO_CNTL, 0, (1 << 10));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT_CNTL, 0x00000007);\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(0), ADRENO_PROTECT_RW(0x04, 4));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(1), ADRENO_PROTECT_RW(0x08, 8));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(2), ADRENO_PROTECT_RW(0x10, 16));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(3), ADRENO_PROTECT_RW(0x20, 32));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(4), ADRENO_PROTECT_RW(0x40, 64));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(5), ADRENO_PROTECT_RW(0x80, 64));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(6),\n\t\tADRENO_PROTECT_RW(REG_A5XX_RBBM_SECVID_TSB_TRUSTED_BASE_LO,\n\t\t\t16));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(7),\n\t\tADRENO_PROTECT_RW(REG_A5XX_RBBM_SECVID_TRUST_CNTL, 2));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(8), ADRENO_PROTECT_RW(0x800, 64));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(9), ADRENO_PROTECT_RW(0x840, 8));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(10), ADRENO_PROTECT_RW(0x880, 32));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(11), ADRENO_PROTECT_RW(0xAA0, 1));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(12), ADRENO_PROTECT_RW(0xCC0, 1));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(13), ADRENO_PROTECT_RW(0xCF0, 2));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(14), ADRENO_PROTECT_RW(0xE68, 8));\n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(15), ADRENO_PROTECT_RW(0xE70, 16));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(16), ADRENO_PROTECT_RW(0xE80, 16));\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PROTECT(17),\n\t\t\tADRENO_PROTECT_RW(0x10000, 0x8000));\n\n\tgpu_write(gpu, REG_A5XX_RBBM_SECVID_TSB_CNTL, 0);\n\t \n\tgpu_write64(gpu, REG_A5XX_RBBM_SECVID_TSB_TRUSTED_BASE_LO, 0x00000000);\n\tgpu_write(gpu, REG_A5XX_RBBM_SECVID_TSB_TRUSTED_SIZE, 0x00000000);\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_VSC_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_GRAS_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_RB_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_PC_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_HLSQ_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_VFD_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_VPC_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_UCHE_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_SP_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_TPL1_ADDR_MODE_CNTL, 0x1);\n\tgpu_write(gpu, REG_A5XX_RBBM_SECVID_TSB_ADDR_MODE_CNTL, 0x1);\n\n\t \n\tif (adreno_gpu->info->quirks & ADRENO_QUIRK_LMLOADKILL_DISABLE) {\n\t\tgpu_rmw(gpu, REG_A5XX_VPC_DBG_ECO_CNTL, 0, BIT(23));\n\t\tgpu_rmw(gpu, REG_A5XX_HLSQ_DBG_ECO_CNTL, BIT(18), 0);\n\t}\n\n\tret = adreno_hw_init(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (adreno_is_a530(adreno_gpu) || adreno_is_a540(adreno_gpu))\n\t\ta5xx_gpmu_ucode_init(gpu);\n\n\tgpu_write64(gpu, REG_A5XX_CP_ME_INSTR_BASE_LO, a5xx_gpu->pm4_iova);\n\tgpu_write64(gpu, REG_A5XX_CP_PFP_INSTR_BASE_LO, a5xx_gpu->pfp_iova);\n\n\t \n\tgpu_write64(gpu, REG_A5XX_CP_RB_BASE, gpu->rb[0]->iova);\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_RB_CNTL,\n\t\tMSM_GPU_RB_CNTL_DEFAULT | AXXX_CP_RB_CNTL_NO_UPDATE);\n\n\t \n\tif (a5xx_gpu->shadow_bo) {\n\t\tgpu_write64(gpu, REG_A5XX_CP_RB_RPTR_ADDR,\n\t\t\t    shadowptr(a5xx_gpu, gpu->rb[0]));\n\t}\n\n\ta5xx_preempt_hw_init(gpu);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_INT_0_MASK, A5XX_INT_MASK);\n\n\t \n\tgpu_write(gpu, REG_A5XX_CP_PFP_ME_CNTL, 0);\n\tret = a5xx_me_init(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\tret = a5xx_power_init(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (adreno_is_a530(adreno_gpu)) {\n\t\tOUT_PKT7(gpu->rb[0], CP_EVENT_WRITE, 1);\n\t\tOUT_RING(gpu->rb[0], CP_EVENT_WRITE_0_EVENT(STAT_EVENT));\n\n\t\ta5xx_flush(gpu, gpu->rb[0], true);\n\t\tif (!a5xx_idle(gpu, gpu->rb[0]))\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = a5xx_zap_shader_init(gpu);\n\tif (!ret) {\n\t\tOUT_PKT7(gpu->rb[0], CP_SET_SECURE_MODE, 1);\n\t\tOUT_RING(gpu->rb[0], 0x00000000);\n\n\t\ta5xx_flush(gpu, gpu->rb[0], true);\n\t\tif (!a5xx_idle(gpu, gpu->rb[0]))\n\t\t\treturn -EINVAL;\n\t} else if (ret == -ENODEV) {\n\t\t \n\t\tdev_warn_once(gpu->dev->dev,\n\t\t\t\"Zap shader not enabled - using SECVID_TRUST_CNTL instead\\n\");\n\t\tgpu_write(gpu, REG_A5XX_RBBM_SECVID_TRUST_CNTL, 0x0);\n\t} else {\n\t\treturn ret;\n\t}\n\n\t \n\ta5xx_preempt_start(gpu);\n\n\treturn 0;\n}\n\nstatic void a5xx_recover(struct msm_gpu *gpu)\n{\n\tint i;\n\n\tadreno_dump_info(gpu);\n\n\tfor (i = 0; i < 8; i++) {\n\t\tprintk(\"CP_SCRATCH_REG%d: %u\\n\", i,\n\t\t\tgpu_read(gpu, REG_A5XX_CP_SCRATCH_REG(i)));\n\t}\n\n\tif (hang_debug)\n\t\ta5xx_dump(gpu);\n\n\tgpu_write(gpu, REG_A5XX_RBBM_SW_RESET_CMD, 1);\n\tgpu_read(gpu, REG_A5XX_RBBM_SW_RESET_CMD);\n\tgpu_write(gpu, REG_A5XX_RBBM_SW_RESET_CMD, 0);\n\tadreno_recover(gpu);\n}\n\nstatic void a5xx_destroy(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\n\tDBG(\"%s\", gpu->name);\n\n\ta5xx_preempt_fini(gpu);\n\n\tif (a5xx_gpu->pm4_bo) {\n\t\tmsm_gem_unpin_iova(a5xx_gpu->pm4_bo, gpu->aspace);\n\t\tdrm_gem_object_put(a5xx_gpu->pm4_bo);\n\t}\n\n\tif (a5xx_gpu->pfp_bo) {\n\t\tmsm_gem_unpin_iova(a5xx_gpu->pfp_bo, gpu->aspace);\n\t\tdrm_gem_object_put(a5xx_gpu->pfp_bo);\n\t}\n\n\tif (a5xx_gpu->gpmu_bo) {\n\t\tmsm_gem_unpin_iova(a5xx_gpu->gpmu_bo, gpu->aspace);\n\t\tdrm_gem_object_put(a5xx_gpu->gpmu_bo);\n\t}\n\n\tif (a5xx_gpu->shadow_bo) {\n\t\tmsm_gem_unpin_iova(a5xx_gpu->shadow_bo, gpu->aspace);\n\t\tdrm_gem_object_put(a5xx_gpu->shadow_bo);\n\t}\n\n\tadreno_gpu_cleanup(adreno_gpu);\n\tkfree(a5xx_gpu);\n}\n\nstatic inline bool _a5xx_check_idle(struct msm_gpu *gpu)\n{\n\tif (gpu_read(gpu, REG_A5XX_RBBM_STATUS) & ~A5XX_RBBM_STATUS_HI_BUSY)\n\t\treturn false;\n\n\t \n\treturn !(gpu_read(gpu, REG_A5XX_RBBM_INT_0_STATUS) &\n\t\tA5XX_RBBM_INT_0_MASK_MISC_HANG_DETECT);\n}\n\nbool a5xx_idle(struct msm_gpu *gpu, struct msm_ringbuffer *ring)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\n\tif (ring != a5xx_gpu->cur_ring) {\n\t\tWARN(1, \"Tried to idle a non-current ringbuffer\\n\");\n\t\treturn false;\n\t}\n\n\t \n\tif (!adreno_idle(gpu, ring))\n\t\treturn false;\n\n\tif (spin_until(_a5xx_check_idle(gpu))) {\n\t\tDRM_ERROR(\"%s: %ps: timeout waiting for GPU to idle: status %8.8X irq %8.8X rptr/wptr %d/%d\\n\",\n\t\t\tgpu->name, __builtin_return_address(0),\n\t\t\tgpu_read(gpu, REG_A5XX_RBBM_STATUS),\n\t\t\tgpu_read(gpu, REG_A5XX_RBBM_INT_0_STATUS),\n\t\t\tgpu_read(gpu, REG_A5XX_CP_RB_RPTR),\n\t\t\tgpu_read(gpu, REG_A5XX_CP_RB_WPTR));\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int a5xx_fault_handler(void *arg, unsigned long iova, int flags, void *data)\n{\n\tstruct msm_gpu *gpu = arg;\n\tstruct adreno_smmu_fault_info *info = data;\n\tchar block[12] = \"unknown\";\n\tu32 scratch[] = {\n\t\t\tgpu_read(gpu, REG_A5XX_CP_SCRATCH_REG(4)),\n\t\t\tgpu_read(gpu, REG_A5XX_CP_SCRATCH_REG(5)),\n\t\t\tgpu_read(gpu, REG_A5XX_CP_SCRATCH_REG(6)),\n\t\t\tgpu_read(gpu, REG_A5XX_CP_SCRATCH_REG(7)),\n\t};\n\n\tif (info)\n\t\tsnprintf(block, sizeof(block), \"%x\", info->fsynr1);\n\n\treturn adreno_fault_handler(gpu, iova, flags, info, block, scratch);\n}\n\nstatic void a5xx_cp_err_irq(struct msm_gpu *gpu)\n{\n\tu32 status = gpu_read(gpu, REG_A5XX_CP_INTERRUPT_STATUS);\n\n\tif (status & A5XX_CP_INT_CP_OPCODE_ERROR) {\n\t\tu32 val;\n\n\t\tgpu_write(gpu, REG_A5XX_CP_PFP_STAT_ADDR, 0);\n\n\t\t \n\n\t\tgpu_read(gpu, REG_A5XX_CP_PFP_STAT_DATA);\n\t\tval = gpu_read(gpu, REG_A5XX_CP_PFP_STAT_DATA);\n\n\t\tdev_err_ratelimited(gpu->dev->dev, \"CP | opcode error | possible opcode=0x%8.8X\\n\",\n\t\t\tval);\n\t}\n\n\tif (status & A5XX_CP_INT_CP_HW_FAULT_ERROR)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"CP | HW fault | status=0x%8.8X\\n\",\n\t\t\tgpu_read(gpu, REG_A5XX_CP_HW_FAULT));\n\n\tif (status & A5XX_CP_INT_CP_DMA_ERROR)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"CP | DMA error\\n\");\n\n\tif (status & A5XX_CP_INT_CP_REGISTER_PROTECTION_ERROR) {\n\t\tu32 val = gpu_read(gpu, REG_A5XX_CP_PROTECT_STATUS);\n\n\t\tdev_err_ratelimited(gpu->dev->dev,\n\t\t\t\"CP | protected mode error | %s | addr=0x%8.8X | status=0x%8.8X\\n\",\n\t\t\tval & (1 << 24) ? \"WRITE\" : \"READ\",\n\t\t\t(val & 0xFFFFF) >> 2, val);\n\t}\n\n\tif (status & A5XX_CP_INT_CP_AHB_ERROR) {\n\t\tu32 status = gpu_read(gpu, REG_A5XX_CP_AHB_FAULT);\n\t\tconst char *access[16] = { \"reserved\", \"reserved\",\n\t\t\t\"timestamp lo\", \"timestamp hi\", \"pfp read\", \"pfp write\",\n\t\t\t\"\", \"\", \"me read\", \"me write\", \"\", \"\", \"crashdump read\",\n\t\t\t\"crashdump write\" };\n\n\t\tdev_err_ratelimited(gpu->dev->dev,\n\t\t\t\"CP | AHB error | addr=%X access=%s error=%d | status=0x%8.8X\\n\",\n\t\t\tstatus & 0xFFFFF, access[(status >> 24) & 0xF],\n\t\t\t(status & (1 << 31)), status);\n\t}\n}\n\nstatic void a5xx_rbbm_err_irq(struct msm_gpu *gpu, u32 status)\n{\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_AHB_ERROR) {\n\t\tu32 val = gpu_read(gpu, REG_A5XX_RBBM_AHB_ERROR_STATUS);\n\n\t\tdev_err_ratelimited(gpu->dev->dev,\n\t\t\t\"RBBM | AHB bus error | %s | addr=0x%X | ports=0x%X:0x%X\\n\",\n\t\t\tval & (1 << 28) ? \"WRITE\" : \"READ\",\n\t\t\t(val & 0xFFFFF) >> 2, (val >> 20) & 0x3,\n\t\t\t(val >> 24) & 0xF);\n\n\t\t \n\t\tgpu_write(gpu, REG_A5XX_RBBM_AHB_CMD, (1 << 4));\n\n\t\t \n\t\tgpu_write(gpu, REG_A5XX_RBBM_INT_CLEAR_CMD,\n\t\t\tA5XX_RBBM_INT_0_MASK_RBBM_AHB_ERROR);\n\t}\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_TRANSFER_TIMEOUT)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | AHB transfer timeout\\n\");\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_ME_MS_TIMEOUT)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | ME master split | status=0x%X\\n\",\n\t\t\tgpu_read(gpu, REG_A5XX_RBBM_AHB_ME_SPLIT_STATUS));\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_PFP_MS_TIMEOUT)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | PFP master split | status=0x%X\\n\",\n\t\t\tgpu_read(gpu, REG_A5XX_RBBM_AHB_PFP_SPLIT_STATUS));\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_ETS_MS_TIMEOUT)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | ETS master split | status=0x%X\\n\",\n\t\t\tgpu_read(gpu, REG_A5XX_RBBM_AHB_ETS_SPLIT_STATUS));\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_ATB_ASYNC_OVERFLOW)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | ATB ASYNC overflow\\n\");\n\n\tif (status & A5XX_RBBM_INT_0_MASK_RBBM_ATB_BUS_OVERFLOW)\n\t\tdev_err_ratelimited(gpu->dev->dev, \"RBBM | ATB bus overflow\\n\");\n}\n\nstatic void a5xx_uche_err_irq(struct msm_gpu *gpu)\n{\n\tuint64_t addr = (uint64_t) gpu_read(gpu, REG_A5XX_UCHE_TRAP_LOG_HI);\n\n\taddr |= gpu_read(gpu, REG_A5XX_UCHE_TRAP_LOG_LO);\n\n\tdev_err_ratelimited(gpu->dev->dev, \"UCHE | Out of bounds access | addr=0x%llX\\n\",\n\t\taddr);\n}\n\nstatic void a5xx_gpmu_err_irq(struct msm_gpu *gpu)\n{\n\tdev_err_ratelimited(gpu->dev->dev, \"GPMU | voltage droop\\n\");\n}\n\nstatic void a5xx_fault_detect_irq(struct msm_gpu *gpu)\n{\n\tstruct drm_device *dev = gpu->dev;\n\tstruct msm_ringbuffer *ring = gpu->funcs->active_ring(gpu);\n\n\t \n\tif (gpu_read(gpu, REG_A5XX_RBBM_STATUS3) & BIT(24))\n\t\treturn;\n\n\tDRM_DEV_ERROR(dev->dev, \"gpu fault ring %d fence %x status %8.8X rb %4.4x/%4.4x ib1 %16.16llX/%4.4x ib2 %16.16llX/%4.4x\\n\",\n\t\tring ? ring->id : -1, ring ? ring->fctx->last_fence : 0,\n\t\tgpu_read(gpu, REG_A5XX_RBBM_STATUS),\n\t\tgpu_read(gpu, REG_A5XX_CP_RB_RPTR),\n\t\tgpu_read(gpu, REG_A5XX_CP_RB_WPTR),\n\t\tgpu_read64(gpu, REG_A5XX_CP_IB1_BASE),\n\t\tgpu_read(gpu, REG_A5XX_CP_IB1_BUFSZ),\n\t\tgpu_read64(gpu, REG_A5XX_CP_IB2_BASE),\n\t\tgpu_read(gpu, REG_A5XX_CP_IB2_BUFSZ));\n\n\t \n\tdel_timer(&gpu->hangcheck_timer);\n\n\tkthread_queue_work(gpu->worker, &gpu->recover_work);\n}\n\n#define RBBM_ERROR_MASK \\\n\t(A5XX_RBBM_INT_0_MASK_RBBM_AHB_ERROR | \\\n\tA5XX_RBBM_INT_0_MASK_RBBM_TRANSFER_TIMEOUT | \\\n\tA5XX_RBBM_INT_0_MASK_RBBM_ME_MS_TIMEOUT | \\\n\tA5XX_RBBM_INT_0_MASK_RBBM_PFP_MS_TIMEOUT | \\\n\tA5XX_RBBM_INT_0_MASK_RBBM_ETS_MS_TIMEOUT | \\\n\tA5XX_RBBM_INT_0_MASK_RBBM_ATB_ASYNC_OVERFLOW)\n\nstatic irqreturn_t a5xx_irq(struct msm_gpu *gpu)\n{\n\tstruct msm_drm_private *priv = gpu->dev->dev_private;\n\tu32 status = gpu_read(gpu, REG_A5XX_RBBM_INT_0_STATUS);\n\n\t \n\tgpu_write(gpu, REG_A5XX_RBBM_INT_CLEAR_CMD,\n\t\tstatus & ~A5XX_RBBM_INT_0_MASK_RBBM_AHB_ERROR);\n\n\tif (priv->disable_err_irq) {\n\t\tstatus &= A5XX_RBBM_INT_0_MASK_CP_CACHE_FLUSH_TS |\n\t\t\t  A5XX_RBBM_INT_0_MASK_CP_SW;\n\t}\n\n\t \n\tif (status & RBBM_ERROR_MASK)\n\t\ta5xx_rbbm_err_irq(gpu, status);\n\n\tif (status & A5XX_RBBM_INT_0_MASK_CP_HW_ERROR)\n\t\ta5xx_cp_err_irq(gpu);\n\n\tif (status & A5XX_RBBM_INT_0_MASK_MISC_HANG_DETECT)\n\t\ta5xx_fault_detect_irq(gpu);\n\n\tif (status & A5XX_RBBM_INT_0_MASK_UCHE_OOB_ACCESS)\n\t\ta5xx_uche_err_irq(gpu);\n\n\tif (status & A5XX_RBBM_INT_0_MASK_GPMU_VOLTAGE_DROOP)\n\t\ta5xx_gpmu_err_irq(gpu);\n\n\tif (status & A5XX_RBBM_INT_0_MASK_CP_CACHE_FLUSH_TS) {\n\t\ta5xx_preempt_trigger(gpu);\n\t\tmsm_gpu_retire(gpu);\n\t}\n\n\tif (status & A5XX_RBBM_INT_0_MASK_CP_SW)\n\t\ta5xx_preempt_irq(gpu);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic const u32 a5xx_registers[] = {\n\t0x0000, 0x0002, 0x0004, 0x0020, 0x0022, 0x0026, 0x0029, 0x002B,\n\t0x002E, 0x0035, 0x0038, 0x0042, 0x0044, 0x0044, 0x0047, 0x0095,\n\t0x0097, 0x00BB, 0x03A0, 0x0464, 0x0469, 0x046F, 0x04D2, 0x04D3,\n\t0x04E0, 0x0533, 0x0540, 0x0555, 0x0800, 0x081A, 0x081F, 0x0841,\n\t0x0860, 0x0860, 0x0880, 0x08A0, 0x0B00, 0x0B12, 0x0B15, 0x0B28,\n\t0x0B78, 0x0B7F, 0x0BB0, 0x0BBD, 0x0BC0, 0x0BC6, 0x0BD0, 0x0C53,\n\t0x0C60, 0x0C61, 0x0C80, 0x0C82, 0x0C84, 0x0C85, 0x0C90, 0x0C98,\n\t0x0CA0, 0x0CA0, 0x0CB0, 0x0CB2, 0x2180, 0x2185, 0x2580, 0x2585,\n\t0x0CC1, 0x0CC1, 0x0CC4, 0x0CC7, 0x0CCC, 0x0CCC, 0x0CD0, 0x0CD8,\n\t0x0CE0, 0x0CE5, 0x0CE8, 0x0CE8, 0x0CEC, 0x0CF1, 0x0CFB, 0x0D0E,\n\t0x2100, 0x211E, 0x2140, 0x2145, 0x2500, 0x251E, 0x2540, 0x2545,\n\t0x0D10, 0x0D17, 0x0D20, 0x0D23, 0x0D30, 0x0D30, 0x20C0, 0x20C0,\n\t0x24C0, 0x24C0, 0x0E40, 0x0E43, 0x0E4A, 0x0E4A, 0x0E50, 0x0E57,\n\t0x0E60, 0x0E7C, 0x0E80, 0x0E8E, 0x0E90, 0x0E96, 0x0EA0, 0x0EA8,\n\t0x0EB0, 0x0EB2, 0xE140, 0xE147, 0xE150, 0xE187, 0xE1A0, 0xE1A9,\n\t0xE1B0, 0xE1B6, 0xE1C0, 0xE1C7, 0xE1D0, 0xE1D1, 0xE200, 0xE201,\n\t0xE210, 0xE21C, 0xE240, 0xE268, 0xE000, 0xE006, 0xE010, 0xE09A,\n\t0xE0A0, 0xE0A4, 0xE0AA, 0xE0EB, 0xE100, 0xE105, 0xE380, 0xE38F,\n\t0xE3B0, 0xE3B0, 0xE400, 0xE405, 0xE408, 0xE4E9, 0xE4F0, 0xE4F0,\n\t0xE280, 0xE280, 0xE282, 0xE2A3, 0xE2A5, 0xE2C2, 0xE940, 0xE947,\n\t0xE950, 0xE987, 0xE9A0, 0xE9A9, 0xE9B0, 0xE9B6, 0xE9C0, 0xE9C7,\n\t0xE9D0, 0xE9D1, 0xEA00, 0xEA01, 0xEA10, 0xEA1C, 0xEA40, 0xEA68,\n\t0xE800, 0xE806, 0xE810, 0xE89A, 0xE8A0, 0xE8A4, 0xE8AA, 0xE8EB,\n\t0xE900, 0xE905, 0xEB80, 0xEB8F, 0xEBB0, 0xEBB0, 0xEC00, 0xEC05,\n\t0xEC08, 0xECE9, 0xECF0, 0xECF0, 0xEA80, 0xEA80, 0xEA82, 0xEAA3,\n\t0xEAA5, 0xEAC2, 0xA800, 0xA800, 0xA820, 0xA828, 0xA840, 0xA87D,\n\t0XA880, 0xA88D, 0xA890, 0xA8A3, 0xA8D0, 0xA8D8, 0xA8E0, 0xA8F5,\n\t0xAC60, 0xAC60, ~0,\n};\n\nstatic void a5xx_dump(struct msm_gpu *gpu)\n{\n\tDRM_DEV_INFO(gpu->dev->dev, \"status:   %08x\\n\",\n\t\tgpu_read(gpu, REG_A5XX_RBBM_STATUS));\n\tadreno_dump(gpu);\n}\n\nstatic int a5xx_pm_resume(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tint ret;\n\n\t \n\tret = msm_gpu_pm_resume(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (!(adreno_is_a530(adreno_gpu) || adreno_is_a540(adreno_gpu))) {\n\t\t \n\t\tgpu_write(gpu, REG_A5XX_RBBM_CLOCK_CNTL, 0x00000055);\n\t\ta5xx_set_hwcg(gpu, true);\n\t\t \n\t\tgpu_rmw(gpu, REG_A5XX_RBBM_CLOCK_CNTL, 0xff, 0);\n\t\treturn 0;\n\t}\n\n\t \n\tgpu_write(gpu, REG_A5XX_GPMU_RBCCU_POWER_CNTL, 0x778000);\n\n\t \n\tudelay(3);\n\n\tret = spin_usecs(gpu, 20, REG_A5XX_GPMU_RBCCU_PWR_CLK_STATUS,\n\t\t(1 << 20), (1 << 20));\n\tif (ret) {\n\t\tDRM_ERROR(\"%s: timeout waiting for RBCCU GDSC enable: %X\\n\",\n\t\t\tgpu->name,\n\t\t\tgpu_read(gpu, REG_A5XX_GPMU_RBCCU_PWR_CLK_STATUS));\n\t\treturn ret;\n\t}\n\n\t \n\tgpu_write(gpu, REG_A5XX_GPMU_SP_POWER_CNTL, 0x778000);\n\tret = spin_usecs(gpu, 20, REG_A5XX_GPMU_SP_PWR_CLK_STATUS,\n\t\t(1 << 20), (1 << 20));\n\tif (ret)\n\t\tDRM_ERROR(\"%s: timeout waiting for SP GDSC enable\\n\",\n\t\t\tgpu->name);\n\n\treturn ret;\n}\n\nstatic int a5xx_pm_suspend(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\tu32 mask = 0xf;\n\tint i, ret;\n\n\t \n\tif (adreno_is_a506(adreno_gpu) || adreno_is_a508(adreno_gpu) ||\n\t    adreno_is_a510(adreno_gpu))\n\t\tmask = 0x7;\n\n\t \n\tgpu_write(gpu, REG_A5XX_VBIF_XIN_HALT_CTRL0, mask);\n\tspin_until((gpu_read(gpu, REG_A5XX_VBIF_XIN_HALT_CTRL1) &\n\t\t\t\tmask) == mask);\n\n\tgpu_write(gpu, REG_A5XX_VBIF_XIN_HALT_CTRL0, 0);\n\n\t \n\tif (adreno_is_a510(adreno_gpu) || adreno_is_a530(adreno_gpu)) {\n\t\tgpu_write(gpu, REG_A5XX_RBBM_BLOCK_SW_RESET_CMD, 0x003C0000);\n\t\tgpu_write(gpu, REG_A5XX_RBBM_BLOCK_SW_RESET_CMD, 0x00000000);\n\t}\n\n\tret = msm_gpu_pm_suspend(gpu);\n\tif (ret)\n\t\treturn ret;\n\n\tif (a5xx_gpu->has_whereami)\n\t\tfor (i = 0; i < gpu->nr_rings; i++)\n\t\t\ta5xx_gpu->shadow[i] = 0;\n\n\treturn 0;\n}\n\nstatic int a5xx_get_timestamp(struct msm_gpu *gpu, uint64_t *value)\n{\n\t*value = gpu_read64(gpu, REG_A5XX_RBBM_ALWAYSON_COUNTER_LO);\n\n\treturn 0;\n}\n\nstruct a5xx_crashdumper {\n\tvoid *ptr;\n\tstruct drm_gem_object *bo;\n\tu64 iova;\n};\n\nstruct a5xx_gpu_state {\n\tstruct msm_gpu_state base;\n\tu32 *hlsqregs;\n};\n\nstatic int a5xx_crashdumper_init(struct msm_gpu *gpu,\n\t\tstruct a5xx_crashdumper *dumper)\n{\n\tdumper->ptr = msm_gem_kernel_new(gpu->dev,\n\t\tSZ_1M, MSM_BO_WC, gpu->aspace,\n\t\t&dumper->bo, &dumper->iova);\n\n\tif (!IS_ERR(dumper->ptr))\n\t\tmsm_gem_object_set_name(dumper->bo, \"crashdump\");\n\n\treturn PTR_ERR_OR_ZERO(dumper->ptr);\n}\n\nstatic int a5xx_crashdumper_run(struct msm_gpu *gpu,\n\t\tstruct a5xx_crashdumper *dumper)\n{\n\tu32 val;\n\n\tif (IS_ERR_OR_NULL(dumper->ptr))\n\t\treturn -EINVAL;\n\n\tgpu_write64(gpu, REG_A5XX_CP_CRASH_SCRIPT_BASE_LO, dumper->iova);\n\n\tgpu_write(gpu, REG_A5XX_CP_CRASH_DUMP_CNTL, 1);\n\n\treturn gpu_poll_timeout(gpu, REG_A5XX_CP_CRASH_DUMP_CNTL, val,\n\t\tval & 0x04, 100, 10000);\n}\n\n \nstatic const struct {\n\tu32 type;\n\tu32 regoffset;\n\tu32 count;\n} a5xx_hlsq_aperture_regs[] = {\n\t{ 0x35, 0xe00, 0x32 },    \n\t{ 0x31, 0x2080, 0x1 },    \n\t{ 0x33, 0x2480, 0x1 },    \n\t{ 0x32, 0xe780, 0x62 },   \n\t{ 0x34, 0xef80, 0x62 },   \n\t{ 0x3f, 0x0ec0, 0x40 },   \n\t{ 0x3d, 0x2040, 0x1 },    \n\t{ 0x3b, 0x2440, 0x1 },    \n\t{ 0x3e, 0xe580, 0x170 },  \n\t{ 0x3c, 0xed80, 0x170 },  \n\t{ 0x3a, 0x0f00, 0x1c },   \n\t{ 0x38, 0x2000, 0xa },    \n\t{ 0x36, 0x2400, 0xa },    \n\t{ 0x39, 0xe700, 0x80 },   \n\t{ 0x37, 0xef00, 0x80 },   \n};\n\nstatic void a5xx_gpu_state_get_hlsq_regs(struct msm_gpu *gpu,\n\t\tstruct a5xx_gpu_state *a5xx_state)\n{\n\tstruct a5xx_crashdumper dumper = { 0 };\n\tu32 offset, count = 0;\n\tu64 *ptr;\n\tint i;\n\n\tif (a5xx_crashdumper_init(gpu, &dumper))\n\t\treturn;\n\n\t \n\tptr = dumper.ptr;\n\n\t \n\toffset = dumper.iova + (256 * SZ_1K);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(a5xx_hlsq_aperture_regs); i++)\n\t\tcount += a5xx_hlsq_aperture_regs[i].count;\n\n\ta5xx_state->hlsqregs = kcalloc(count, sizeof(u32), GFP_KERNEL);\n\tif (!a5xx_state->hlsqregs)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(a5xx_hlsq_aperture_regs); i++) {\n\t\tu32 type = a5xx_hlsq_aperture_regs[i].type;\n\t\tu32 c = a5xx_hlsq_aperture_regs[i].count;\n\n\t\t \n\t\t*ptr++ = ((u64) type << 8);\n\t\t*ptr++ = (((u64) REG_A5XX_HLSQ_DBG_READ_SEL) << 44) |\n\t\t\t(1 << 21) | 1;\n\n\t\t*ptr++ = offset;\n\t\t*ptr++ = (((u64) REG_A5XX_HLSQ_DBG_AHB_READ_APERTURE) << 44)\n\t\t\t| c;\n\n\t\toffset += c * sizeof(u32);\n\t}\n\n\t \n\t*ptr++ = 0;\n\t*ptr++ = 0;\n\n\tif (a5xx_crashdumper_run(gpu, &dumper)) {\n\t\tkfree(a5xx_state->hlsqregs);\n\t\tmsm_gem_kernel_put(dumper.bo, gpu->aspace);\n\t\treturn;\n\t}\n\n\t \n\tmemcpy(a5xx_state->hlsqregs, dumper.ptr + (256 * SZ_1K),\n\t\tcount * sizeof(u32));\n\n\tmsm_gem_kernel_put(dumper.bo, gpu->aspace);\n}\n\nstatic struct msm_gpu_state *a5xx_gpu_state_get(struct msm_gpu *gpu)\n{\n\tstruct a5xx_gpu_state *a5xx_state = kzalloc(sizeof(*a5xx_state),\n\t\t\tGFP_KERNEL);\n\tbool stalled = !!(gpu_read(gpu, REG_A5XX_RBBM_STATUS3) & BIT(24));\n\n\tif (!a5xx_state)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\ta5xx_set_hwcg(gpu, false);\n\n\t \n\tadreno_gpu_state_get(gpu, &(a5xx_state->base));\n\n\ta5xx_state->base.rbbm_status = gpu_read(gpu, REG_A5XX_RBBM_STATUS);\n\n\t \n\tif (!stalled)\n\t\ta5xx_gpu_state_get_hlsq_regs(gpu, a5xx_state);\n\n\ta5xx_set_hwcg(gpu, true);\n\n\treturn &a5xx_state->base;\n}\n\nstatic void a5xx_gpu_state_destroy(struct kref *kref)\n{\n\tstruct msm_gpu_state *state = container_of(kref,\n\t\tstruct msm_gpu_state, ref);\n\tstruct a5xx_gpu_state *a5xx_state = container_of(state,\n\t\tstruct a5xx_gpu_state, base);\n\n\tkfree(a5xx_state->hlsqregs);\n\n\tadreno_gpu_state_destroy(state);\n\tkfree(a5xx_state);\n}\n\nstatic int a5xx_gpu_state_put(struct msm_gpu_state *state)\n{\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn 1;\n\n\treturn kref_put(&state->ref, a5xx_gpu_state_destroy);\n}\n\n\n#if defined(CONFIG_DEBUG_FS) || defined(CONFIG_DEV_COREDUMP)\nstatic void a5xx_show(struct msm_gpu *gpu, struct msm_gpu_state *state,\n\t\t      struct drm_printer *p)\n{\n\tint i, j;\n\tu32 pos = 0;\n\tstruct a5xx_gpu_state *a5xx_state = container_of(state,\n\t\tstruct a5xx_gpu_state, base);\n\n\tif (IS_ERR_OR_NULL(state))\n\t\treturn;\n\n\tadreno_show(gpu, state, p);\n\n\t \n\tif (!a5xx_state->hlsqregs)\n\t\treturn;\n\n\tdrm_printf(p, \"registers-hlsq:\\n\");\n\n\tfor (i = 0; i < ARRAY_SIZE(a5xx_hlsq_aperture_regs); i++) {\n\t\tu32 o = a5xx_hlsq_aperture_regs[i].regoffset;\n\t\tu32 c = a5xx_hlsq_aperture_regs[i].count;\n\n\t\tfor (j = 0; j < c; j++, pos++, o++) {\n\t\t\t \n\t\t\tif (a5xx_state->hlsqregs[pos] == 0xdeadbeef)\n\t\t\t\tcontinue;\n\n\t\t\tdrm_printf(p, \"  - { offset: 0x%04x, value: 0x%08x }\\n\",\n\t\t\t\to << 2, a5xx_state->hlsqregs[pos]);\n\t\t}\n\t}\n}\n#endif\n\nstatic struct msm_ringbuffer *a5xx_active_ring(struct msm_gpu *gpu)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\n\treturn a5xx_gpu->cur_ring;\n}\n\nstatic u64 a5xx_gpu_busy(struct msm_gpu *gpu, unsigned long *out_sample_rate)\n{\n\tu64 busy_cycles;\n\n\tbusy_cycles = gpu_read64(gpu, REG_A5XX_RBBM_PERFCTR_RBBM_0_LO);\n\t*out_sample_rate = clk_get_rate(gpu->core_clk);\n\n\treturn busy_cycles;\n}\n\nstatic uint32_t a5xx_get_rptr(struct msm_gpu *gpu, struct msm_ringbuffer *ring)\n{\n\tstruct adreno_gpu *adreno_gpu = to_adreno_gpu(gpu);\n\tstruct a5xx_gpu *a5xx_gpu = to_a5xx_gpu(adreno_gpu);\n\n\tif (a5xx_gpu->has_whereami)\n\t\treturn a5xx_gpu->shadow[ring->id];\n\n\treturn ring->memptrs->rptr = gpu_read(gpu, REG_A5XX_CP_RB_RPTR);\n}\n\nstatic const struct adreno_gpu_funcs funcs = {\n\t.base = {\n\t\t.get_param = adreno_get_param,\n\t\t.set_param = adreno_set_param,\n\t\t.hw_init = a5xx_hw_init,\n\t\t.ucode_load = a5xx_ucode_load,\n\t\t.pm_suspend = a5xx_pm_suspend,\n\t\t.pm_resume = a5xx_pm_resume,\n\t\t.recover = a5xx_recover,\n\t\t.submit = a5xx_submit,\n\t\t.active_ring = a5xx_active_ring,\n\t\t.irq = a5xx_irq,\n\t\t.destroy = a5xx_destroy,\n#if defined(CONFIG_DEBUG_FS) || defined(CONFIG_DEV_COREDUMP)\n\t\t.show = a5xx_show,\n#endif\n#if defined(CONFIG_DEBUG_FS)\n\t\t.debugfs_init = a5xx_debugfs_init,\n#endif\n\t\t.gpu_busy = a5xx_gpu_busy,\n\t\t.gpu_state_get = a5xx_gpu_state_get,\n\t\t.gpu_state_put = a5xx_gpu_state_put,\n\t\t.create_address_space = adreno_create_address_space,\n\t\t.get_rptr = a5xx_get_rptr,\n\t},\n\t.get_timestamp = a5xx_get_timestamp,\n};\n\nstatic void check_speed_bin(struct device *dev)\n{\n\tstruct nvmem_cell *cell;\n\tu32 val;\n\n\t \n\tval = 0x80;\n\n\tcell = nvmem_cell_get(dev, \"speed_bin\");\n\n\tif (!IS_ERR(cell)) {\n\t\tvoid *buf = nvmem_cell_read(cell, NULL);\n\n\t\tif (!IS_ERR(buf)) {\n\t\t\tu8 bin = *((u8 *) buf);\n\n\t\t\tval = (1 << bin);\n\t\t\tkfree(buf);\n\t\t}\n\n\t\tnvmem_cell_put(cell);\n\t}\n\n\tdevm_pm_opp_set_supported_hw(dev, &val, 1);\n}\n\nstruct msm_gpu *a5xx_gpu_init(struct drm_device *dev)\n{\n\tstruct msm_drm_private *priv = dev->dev_private;\n\tstruct platform_device *pdev = priv->gpu_pdev;\n\tstruct adreno_platform_config *config = pdev->dev.platform_data;\n\tstruct a5xx_gpu *a5xx_gpu = NULL;\n\tstruct adreno_gpu *adreno_gpu;\n\tstruct msm_gpu *gpu;\n\tunsigned int nr_rings;\n\tint ret;\n\n\tif (!pdev) {\n\t\tDRM_DEV_ERROR(dev->dev, \"No A5XX device is defined\\n\");\n\t\treturn ERR_PTR(-ENXIO);\n\t}\n\n\ta5xx_gpu = kzalloc(sizeof(*a5xx_gpu), GFP_KERNEL);\n\tif (!a5xx_gpu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tadreno_gpu = &a5xx_gpu->base;\n\tgpu = &adreno_gpu->base;\n\n\tadreno_gpu->registers = a5xx_registers;\n\n\ta5xx_gpu->lm_leakage = 0x4E001A;\n\n\tcheck_speed_bin(&pdev->dev);\n\n\tnr_rings = 4;\n\n\tif (config->info->revn == 510)\n\t\tnr_rings = 1;\n\n\tret = adreno_gpu_init(dev, pdev, adreno_gpu, &funcs, nr_rings);\n\tif (ret) {\n\t\ta5xx_destroy(&(a5xx_gpu->base.base));\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tif (gpu->aspace)\n\t\tmsm_mmu_set_fault_handler(gpu->aspace->mmu, gpu, a5xx_fault_handler);\n\n\t \n\ta5xx_preempt_init(gpu);\n\n\treturn gpu;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}