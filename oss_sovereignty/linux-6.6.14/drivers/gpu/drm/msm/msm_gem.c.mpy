{
  "module_name": "msm_gem.c",
  "hash_id": "4e1deaa415e6eb679a539543b6e795bb432ed15de09019d0d3700a2061c24b65",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/msm_gem.c",
  "human_readable_source": "\n \n\n#include <linux/dma-map-ops.h>\n#include <linux/vmalloc.h>\n#include <linux/spinlock.h>\n#include <linux/shmem_fs.h>\n#include <linux/dma-buf.h>\n#include <linux/pfn_t.h>\n\n#include <drm/drm_prime.h>\n\n#include \"msm_drv.h\"\n#include \"msm_fence.h\"\n#include \"msm_gem.h\"\n#include \"msm_gpu.h\"\n#include \"msm_mmu.h\"\n\nstatic dma_addr_t physaddr(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\treturn (((dma_addr_t)msm_obj->vram_node->start) << PAGE_SHIFT) +\n\t\t\tpriv->vram.paddr;\n}\n\nstatic bool use_pages(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\treturn !msm_obj->vram_node;\n}\n\n \n\nstatic void sync_for_device(struct msm_gem_object *msm_obj)\n{\n\tstruct device *dev = msm_obj->base.dev->dev;\n\n\tdma_map_sgtable(dev, msm_obj->sgt, DMA_BIDIRECTIONAL, 0);\n}\n\nstatic void sync_for_cpu(struct msm_gem_object *msm_obj)\n{\n\tstruct device *dev = msm_obj->base.dev->dev;\n\n\tdma_unmap_sgtable(dev, msm_obj->sgt, DMA_BIDIRECTIONAL, 0);\n}\n\nstatic void update_lru_active(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tGEM_WARN_ON(!msm_obj->pages);\n\n\tif (msm_obj->pin_count) {\n\t\tdrm_gem_lru_move_tail_locked(&priv->lru.pinned, obj);\n\t} else if (msm_obj->madv == MSM_MADV_WILLNEED) {\n\t\tdrm_gem_lru_move_tail_locked(&priv->lru.willneed, obj);\n\t} else {\n\t\tGEM_WARN_ON(msm_obj->madv != MSM_MADV_DONTNEED);\n\n\t\tdrm_gem_lru_move_tail_locked(&priv->lru.dontneed, obj);\n\t}\n}\n\nstatic void update_lru_locked(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(&msm_obj->base);\n\n\tif (!msm_obj->pages) {\n\t\tGEM_WARN_ON(msm_obj->pin_count);\n\n\t\tdrm_gem_lru_move_tail_locked(&priv->lru.unbacked, obj);\n\t} else {\n\t\tupdate_lru_active(obj);\n\t}\n}\n\nstatic void update_lru(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\n\tmutex_lock(&priv->lru.lock);\n\tupdate_lru_locked(obj);\n\tmutex_unlock(&priv->lru.lock);\n}\n\n \nstatic struct page **get_pages_vram(struct drm_gem_object *obj, int npages)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tdma_addr_t paddr;\n\tstruct page **p;\n\tint ret, i;\n\n\tp = kvmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspin_lock(&priv->vram.lock);\n\tret = drm_mm_insert_node(&priv->vram.mm, msm_obj->vram_node, npages);\n\tspin_unlock(&priv->vram.lock);\n\tif (ret) {\n\t\tkvfree(p);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tpaddr = physaddr(obj);\n\tfor (i = 0; i < npages; i++) {\n\t\tp[i] = pfn_to_page(__phys_to_pfn(paddr));\n\t\tpaddr += PAGE_SIZE;\n\t}\n\n\treturn p;\n}\n\nstatic struct page **get_pages(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\n\tif (!msm_obj->pages) {\n\t\tstruct drm_device *dev = obj->dev;\n\t\tstruct page **p;\n\t\tint npages = obj->size >> PAGE_SHIFT;\n\n\t\tif (use_pages(obj))\n\t\t\tp = drm_gem_get_pages(obj);\n\t\telse\n\t\t\tp = get_pages_vram(obj, npages);\n\n\t\tif (IS_ERR(p)) {\n\t\t\tDRM_DEV_ERROR(dev->dev, \"could not get pages: %ld\\n\",\n\t\t\t\t\tPTR_ERR(p));\n\t\t\treturn p;\n\t\t}\n\n\t\tmsm_obj->pages = p;\n\n\t\tmsm_obj->sgt = drm_prime_pages_to_sg(obj->dev, p, npages);\n\t\tif (IS_ERR(msm_obj->sgt)) {\n\t\t\tvoid *ptr = ERR_CAST(msm_obj->sgt);\n\n\t\t\tDRM_DEV_ERROR(dev->dev, \"failed to allocate sgt\\n\");\n\t\t\tmsm_obj->sgt = NULL;\n\t\t\treturn ptr;\n\t\t}\n\n\t\t \n\t\tif (msm_obj->flags & MSM_BO_WC)\n\t\t\tsync_for_device(msm_obj);\n\n\t\tupdate_lru(obj);\n\t}\n\n\treturn msm_obj->pages;\n}\n\nstatic void put_pages_vram(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\n\tspin_lock(&priv->vram.lock);\n\tdrm_mm_remove_node(msm_obj->vram_node);\n\tspin_unlock(&priv->vram.lock);\n\n\tkvfree(msm_obj->pages);\n}\n\nstatic void put_pages(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tif (msm_obj->pages) {\n\t\tif (msm_obj->sgt) {\n\t\t\t \n\t\t\tif (msm_obj->flags & MSM_BO_WC)\n\t\t\t\tsync_for_cpu(msm_obj);\n\n\t\t\tsg_free_table(msm_obj->sgt);\n\t\t\tkfree(msm_obj->sgt);\n\t\t\tmsm_obj->sgt = NULL;\n\t\t}\n\n\t\tif (use_pages(obj))\n\t\t\tdrm_gem_put_pages(obj, msm_obj->pages, true, false);\n\t\telse\n\t\t\tput_pages_vram(obj);\n\n\t\tmsm_obj->pages = NULL;\n\t\tupdate_lru(obj);\n\t}\n}\n\nstatic struct page **msm_gem_pin_pages_locked(struct drm_gem_object *obj,\n\t\t\t\t\t      unsigned madv)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\n\tif (GEM_WARN_ON(msm_obj->madv > madv)) {\n\t\tDRM_DEV_ERROR(obj->dev->dev, \"Invalid madv state: %u vs %u\\n\",\n\t\t\tmsm_obj->madv, madv);\n\t\treturn ERR_PTR(-EBUSY);\n\t}\n\n\treturn get_pages(obj);\n}\n\n \nvoid msm_gem_pin_obj_locked(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\n\tmsm_gem_assert_locked(obj);\n\n\tto_msm_bo(obj)->pin_count++;\n\tdrm_gem_lru_move_tail_locked(&priv->lru.pinned, obj);\n}\n\nstatic void pin_obj_locked(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\n\tmutex_lock(&priv->lru.lock);\n\tmsm_gem_pin_obj_locked(obj);\n\tmutex_unlock(&priv->lru.lock);\n}\n\nstruct page **msm_gem_pin_pages(struct drm_gem_object *obj)\n{\n\tstruct page **p;\n\n\tmsm_gem_lock(obj);\n\tp = msm_gem_pin_pages_locked(obj, MSM_MADV_WILLNEED);\n\tif (!IS_ERR(p))\n\t\tpin_obj_locked(obj);\n\tmsm_gem_unlock(obj);\n\n\treturn p;\n}\n\nvoid msm_gem_unpin_pages(struct drm_gem_object *obj)\n{\n\tmsm_gem_lock(obj);\n\tmsm_gem_unpin_locked(obj);\n\tmsm_gem_unlock(obj);\n}\n\nstatic pgprot_t msm_gem_pgprot(struct msm_gem_object *msm_obj, pgprot_t prot)\n{\n\tif (msm_obj->flags & MSM_BO_WC)\n\t\treturn pgprot_writecombine(prot);\n\treturn prot;\n}\n\nstatic vm_fault_t msm_gem_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct drm_gem_object *obj = vma->vm_private_data;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct page **pages;\n\tunsigned long pfn;\n\tpgoff_t pgoff;\n\tint err;\n\tvm_fault_t ret;\n\n\t \n\terr = msm_gem_lock_interruptible(obj);\n\tif (err) {\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\tif (GEM_WARN_ON(msm_obj->madv != MSM_MADV_WILLNEED)) {\n\t\tmsm_gem_unlock(obj);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\t \n\tpages = get_pages(obj);\n\tif (IS_ERR(pages)) {\n\t\tret = vmf_error(PTR_ERR(pages));\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tpgoff = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\n\n\tpfn = page_to_pfn(pages[pgoff]);\n\n\tVERB(\"Inserting %p pfn %lx, pa %lx\", (void *)vmf->address,\n\t\t\tpfn, pfn << PAGE_SHIFT);\n\n\tret = vmf_insert_pfn(vma, vmf->address, pfn);\n\nout_unlock:\n\tmsm_gem_unlock(obj);\nout:\n\treturn ret;\n}\n\n \nstatic uint64_t mmap_offset(struct drm_gem_object *obj)\n{\n\tstruct drm_device *dev = obj->dev;\n\tint ret;\n\n\tmsm_gem_assert_locked(obj);\n\n\t \n\tret = drm_gem_create_mmap_offset(obj);\n\n\tif (ret) {\n\t\tDRM_DEV_ERROR(dev->dev, \"could not allocate mmap offset\\n\");\n\t\treturn 0;\n\t}\n\n\treturn drm_vma_node_offset_addr(&obj->vma_node);\n}\n\nuint64_t msm_gem_mmap_offset(struct drm_gem_object *obj)\n{\n\tuint64_t offset;\n\n\tmsm_gem_lock(obj);\n\toffset = mmap_offset(obj);\n\tmsm_gem_unlock(obj);\n\treturn offset;\n}\n\nstatic struct msm_gem_vma *add_vma(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_gem_vma *vma;\n\n\tmsm_gem_assert_locked(obj);\n\n\tvma = msm_gem_vma_new(aspace);\n\tif (!vma)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlist_add_tail(&vma->list, &msm_obj->vmas);\n\n\treturn vma;\n}\n\nstatic struct msm_gem_vma *lookup_vma(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_gem_vma *vma;\n\n\tmsm_gem_assert_locked(obj);\n\n\tlist_for_each_entry(vma, &msm_obj->vmas, list) {\n\t\tif (vma->aspace == aspace)\n\t\t\treturn vma;\n\t}\n\n\treturn NULL;\n}\n\nstatic void del_vma(struct msm_gem_vma *vma)\n{\n\tif (!vma)\n\t\treturn;\n\n\tlist_del(&vma->list);\n\tkfree(vma);\n}\n\n \nstatic void\nput_iova_spaces(struct drm_gem_object *obj, bool close)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_gem_vma *vma;\n\n\tmsm_gem_assert_locked(obj);\n\n\tlist_for_each_entry(vma, &msm_obj->vmas, list) {\n\t\tif (vma->aspace) {\n\t\t\tmsm_gem_vma_purge(vma);\n\t\t\tif (close)\n\t\t\t\tmsm_gem_vma_close(vma);\n\t\t}\n\t}\n}\n\n \nstatic void\nput_iova_vmas(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct msm_gem_vma *vma, *tmp;\n\n\tmsm_gem_assert_locked(obj);\n\n\tlist_for_each_entry_safe(vma, tmp, &msm_obj->vmas, list) {\n\t\tdel_vma(vma);\n\t}\n}\n\nstatic struct msm_gem_vma *get_vma_locked(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace,\n\t\tu64 range_start, u64 range_end)\n{\n\tstruct msm_gem_vma *vma;\n\n\tmsm_gem_assert_locked(obj);\n\n\tvma = lookup_vma(obj, aspace);\n\n\tif (!vma) {\n\t\tint ret;\n\n\t\tvma = add_vma(obj, aspace);\n\t\tif (IS_ERR(vma))\n\t\t\treturn vma;\n\n\t\tret = msm_gem_vma_init(vma, obj->size,\n\t\t\trange_start, range_end);\n\t\tif (ret) {\n\t\t\tdel_vma(vma);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t} else {\n\t\tGEM_WARN_ON(vma->iova < range_start);\n\t\tGEM_WARN_ON((vma->iova + obj->size) > range_end);\n\t}\n\n\treturn vma;\n}\n\nint msm_gem_pin_vma_locked(struct drm_gem_object *obj, struct msm_gem_vma *vma)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct page **pages;\n\tint prot = IOMMU_READ;\n\n\tif (!(msm_obj->flags & MSM_BO_GPU_READONLY))\n\t\tprot |= IOMMU_WRITE;\n\n\tif (msm_obj->flags & MSM_BO_MAP_PRIV)\n\t\tprot |= IOMMU_PRIV;\n\n\tif (msm_obj->flags & MSM_BO_CACHED_COHERENT)\n\t\tprot |= IOMMU_CACHE;\n\n\tmsm_gem_assert_locked(obj);\n\n\tpages = msm_gem_pin_pages_locked(obj, MSM_MADV_WILLNEED);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\treturn msm_gem_vma_map(vma, prot, msm_obj->sgt, obj->size);\n}\n\nvoid msm_gem_unpin_locked(struct drm_gem_object *obj)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\n\tmutex_lock(&priv->lru.lock);\n\tmsm_obj->pin_count--;\n\tGEM_WARN_ON(msm_obj->pin_count < 0);\n\tupdate_lru_locked(obj);\n\tmutex_unlock(&priv->lru.lock);\n}\n\n \nvoid msm_gem_unpin_active(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_obj->pin_count--;\n\tGEM_WARN_ON(msm_obj->pin_count < 0);\n\tupdate_lru_active(obj);\n}\n\nstruct msm_gem_vma *msm_gem_get_vma_locked(struct drm_gem_object *obj,\n\t\t\t\t\t   struct msm_gem_address_space *aspace)\n{\n\treturn get_vma_locked(obj, aspace, 0, U64_MAX);\n}\n\nstatic int get_and_pin_iova_range_locked(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace, uint64_t *iova,\n\t\tu64 range_start, u64 range_end)\n{\n\tstruct msm_gem_vma *vma;\n\tint ret;\n\n\tmsm_gem_assert_locked(obj);\n\n\tvma = get_vma_locked(obj, aspace, range_start, range_end);\n\tif (IS_ERR(vma))\n\t\treturn PTR_ERR(vma);\n\n\tret = msm_gem_pin_vma_locked(obj, vma);\n\tif (!ret) {\n\t\t*iova = vma->iova;\n\t\tpin_obj_locked(obj);\n\t}\n\n\treturn ret;\n}\n\n \nint msm_gem_get_and_pin_iova_range(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace, uint64_t *iova,\n\t\tu64 range_start, u64 range_end)\n{\n\tint ret;\n\n\tmsm_gem_lock(obj);\n\tret = get_and_pin_iova_range_locked(obj, aspace, iova, range_start, range_end);\n\tmsm_gem_unlock(obj);\n\n\treturn ret;\n}\n\n \nint msm_gem_get_and_pin_iova(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace, uint64_t *iova)\n{\n\treturn msm_gem_get_and_pin_iova_range(obj, aspace, iova, 0, U64_MAX);\n}\n\n \nint msm_gem_get_iova(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace, uint64_t *iova)\n{\n\tstruct msm_gem_vma *vma;\n\tint ret = 0;\n\n\tmsm_gem_lock(obj);\n\tvma = get_vma_locked(obj, aspace, 0, U64_MAX);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t} else {\n\t\t*iova = vma->iova;\n\t}\n\tmsm_gem_unlock(obj);\n\n\treturn ret;\n}\n\nstatic int clear_iova(struct drm_gem_object *obj,\n\t\t      struct msm_gem_address_space *aspace)\n{\n\tstruct msm_gem_vma *vma = lookup_vma(obj, aspace);\n\n\tif (!vma)\n\t\treturn 0;\n\n\tmsm_gem_vma_purge(vma);\n\tmsm_gem_vma_close(vma);\n\tdel_vma(vma);\n\n\treturn 0;\n}\n\n \nint msm_gem_set_iova(struct drm_gem_object *obj,\n\t\t     struct msm_gem_address_space *aspace, uint64_t iova)\n{\n\tint ret = 0;\n\n\tmsm_gem_lock(obj);\n\tif (!iova) {\n\t\tret = clear_iova(obj, aspace);\n\t} else {\n\t\tstruct msm_gem_vma *vma;\n\t\tvma = get_vma_locked(obj, aspace, iova, iova + obj->size);\n\t\tif (IS_ERR(vma)) {\n\t\t\tret = PTR_ERR(vma);\n\t\t} else if (GEM_WARN_ON(vma->iova != iova)) {\n\t\t\tclear_iova(obj, aspace);\n\t\t\tret = -EBUSY;\n\t\t}\n\t}\n\tmsm_gem_unlock(obj);\n\n\treturn ret;\n}\n\n \nvoid msm_gem_unpin_iova(struct drm_gem_object *obj,\n\t\tstruct msm_gem_address_space *aspace)\n{\n\tstruct msm_gem_vma *vma;\n\n\tmsm_gem_lock(obj);\n\tvma = lookup_vma(obj, aspace);\n\tif (!GEM_WARN_ON(!vma)) {\n\t\tmsm_gem_unpin_locked(obj);\n\t}\n\tmsm_gem_unlock(obj);\n}\n\nint msm_gem_dumb_create(struct drm_file *file, struct drm_device *dev,\n\t\tstruct drm_mode_create_dumb *args)\n{\n\targs->pitch = align_pitch(args->width, args->bpp);\n\targs->size  = PAGE_ALIGN(args->pitch * args->height);\n\treturn msm_gem_new_handle(dev, file, args->size,\n\t\t\tMSM_BO_SCANOUT | MSM_BO_WC, &args->handle, \"dumb\");\n}\n\nint msm_gem_dumb_map_offset(struct drm_file *file, struct drm_device *dev,\n\t\tuint32_t handle, uint64_t *offset)\n{\n\tstruct drm_gem_object *obj;\n\tint ret = 0;\n\n\t \n\tobj = drm_gem_object_lookup(file, handle);\n\tif (obj == NULL) {\n\t\tret = -ENOENT;\n\t\tgoto fail;\n\t}\n\n\t*offset = msm_gem_mmap_offset(obj);\n\n\tdrm_gem_object_put(obj);\n\nfail:\n\treturn ret;\n}\n\nstatic void *get_vaddr(struct drm_gem_object *obj, unsigned madv)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct page **pages;\n\tint ret = 0;\n\n\tmsm_gem_assert_locked(obj);\n\n\tif (obj->import_attach)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tpages = msm_gem_pin_pages_locked(obj, madv);\n\tif (IS_ERR(pages))\n\t\treturn ERR_CAST(pages);\n\n\tpin_obj_locked(obj);\n\n\t \n\tmsm_obj->vmap_count++;\n\n\tif (!msm_obj->vaddr) {\n\t\tmsm_obj->vaddr = vmap(pages, obj->size >> PAGE_SHIFT,\n\t\t\t\tVM_MAP, msm_gem_pgprot(msm_obj, PAGE_KERNEL));\n\t\tif (msm_obj->vaddr == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\treturn msm_obj->vaddr;\n\nfail:\n\tmsm_obj->vmap_count--;\n\tmsm_gem_unpin_locked(obj);\n\treturn ERR_PTR(ret);\n}\n\nvoid *msm_gem_get_vaddr_locked(struct drm_gem_object *obj)\n{\n\treturn get_vaddr(obj, MSM_MADV_WILLNEED);\n}\n\nvoid *msm_gem_get_vaddr(struct drm_gem_object *obj)\n{\n\tvoid *ret;\n\n\tmsm_gem_lock(obj);\n\tret = msm_gem_get_vaddr_locked(obj);\n\tmsm_gem_unlock(obj);\n\n\treturn ret;\n}\n\n \nvoid *msm_gem_get_vaddr_active(struct drm_gem_object *obj)\n{\n\treturn get_vaddr(obj, __MSM_MADV_PURGED);\n}\n\nvoid msm_gem_put_vaddr_locked(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\tGEM_WARN_ON(msm_obj->vmap_count < 1);\n\n\tmsm_obj->vmap_count--;\n\tmsm_gem_unpin_locked(obj);\n}\n\nvoid msm_gem_put_vaddr(struct drm_gem_object *obj)\n{\n\tmsm_gem_lock(obj);\n\tmsm_gem_put_vaddr_locked(obj);\n\tmsm_gem_unlock(obj);\n}\n\n \nint msm_gem_madvise(struct drm_gem_object *obj, unsigned madv)\n{\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_lock(obj);\n\n\tmutex_lock(&priv->lru.lock);\n\n\tif (msm_obj->madv != __MSM_MADV_PURGED)\n\t\tmsm_obj->madv = madv;\n\n\tmadv = msm_obj->madv;\n\n\t \n\tupdate_lru_locked(obj);\n\n\tmutex_unlock(&priv->lru.lock);\n\n\tmsm_gem_unlock(obj);\n\n\treturn (madv != __MSM_MADV_PURGED);\n}\n\nvoid msm_gem_purge(struct drm_gem_object *obj)\n{\n\tstruct drm_device *dev = obj->dev;\n\tstruct msm_drm_private *priv = obj->dev->dev_private;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\tGEM_WARN_ON(!is_purgeable(msm_obj));\n\n\t \n\tput_iova_spaces(obj, true);\n\n\tmsm_gem_vunmap(obj);\n\n\tdrm_vma_node_unmap(&obj->vma_node, dev->anon_inode->i_mapping);\n\n\tput_pages(obj);\n\n\tput_iova_vmas(obj);\n\n\tmutex_lock(&priv->lru.lock);\n\t \n\tmsm_obj->madv = __MSM_MADV_PURGED;\n\tmutex_unlock(&priv->lru.lock);\n\n\tdrm_gem_free_mmap_offset(obj);\n\n\t \n\tshmem_truncate_range(file_inode(obj->filp), 0, (loff_t)-1);\n\n\tinvalidate_mapping_pages(file_inode(obj->filp)->i_mapping,\n\t\t\t0, (loff_t)-1);\n}\n\n \nvoid msm_gem_evict(struct drm_gem_object *obj)\n{\n\tstruct drm_device *dev = obj->dev;\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\tGEM_WARN_ON(is_unevictable(msm_obj));\n\n\t \n\tput_iova_spaces(obj, false);\n\n\tdrm_vma_node_unmap(&obj->vma_node, dev->anon_inode->i_mapping);\n\n\tput_pages(obj);\n}\n\nvoid msm_gem_vunmap(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tmsm_gem_assert_locked(obj);\n\n\tif (!msm_obj->vaddr || GEM_WARN_ON(!is_vunmapable(msm_obj)))\n\t\treturn;\n\n\tvunmap(msm_obj->vaddr);\n\tmsm_obj->vaddr = NULL;\n}\n\nbool msm_gem_active(struct drm_gem_object *obj)\n{\n\tmsm_gem_assert_locked(obj);\n\n\tif (to_msm_bo(obj)->pin_count)\n\t\treturn true;\n\n\treturn !dma_resv_test_signaled(obj->resv, dma_resv_usage_rw(true));\n}\n\nint msm_gem_cpu_prep(struct drm_gem_object *obj, uint32_t op, ktime_t *timeout)\n{\n\tbool write = !!(op & MSM_PREP_WRITE);\n\tunsigned long remain =\n\t\top & MSM_PREP_NOSYNC ? 0 : timeout_to_jiffies(timeout);\n\tlong ret;\n\n\tif (op & MSM_PREP_BOOST) {\n\t\tdma_resv_set_deadline(obj->resv, dma_resv_usage_rw(write),\n\t\t\t\t      ktime_get());\n\t}\n\n\tret = dma_resv_wait_timeout(obj->resv, dma_resv_usage_rw(write),\n\t\t\t\t    true,  remain);\n\tif (ret == 0)\n\t\treturn remain == 0 ? -EBUSY : -ETIMEDOUT;\n\telse if (ret < 0)\n\t\treturn ret;\n\n\t \n\n\treturn 0;\n}\n\nint msm_gem_cpu_fini(struct drm_gem_object *obj)\n{\n\t \n\treturn 0;\n}\n\n#ifdef CONFIG_DEBUG_FS\nvoid msm_gem_describe(struct drm_gem_object *obj, struct seq_file *m,\n\t\tstruct msm_gem_stats *stats)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct dma_resv *robj = obj->resv;\n\tstruct msm_gem_vma *vma;\n\tuint64_t off = drm_vma_node_start(&obj->vma_node);\n\tconst char *madv;\n\n\tmsm_gem_lock(obj);\n\n\tstats->all.count++;\n\tstats->all.size += obj->size;\n\n\tif (msm_gem_active(obj)) {\n\t\tstats->active.count++;\n\t\tstats->active.size += obj->size;\n\t}\n\n\tif (msm_obj->pages) {\n\t\tstats->resident.count++;\n\t\tstats->resident.size += obj->size;\n\t}\n\n\tswitch (msm_obj->madv) {\n\tcase __MSM_MADV_PURGED:\n\t\tstats->purged.count++;\n\t\tstats->purged.size += obj->size;\n\t\tmadv = \" purged\";\n\t\tbreak;\n\tcase MSM_MADV_DONTNEED:\n\t\tstats->purgeable.count++;\n\t\tstats->purgeable.size += obj->size;\n\t\tmadv = \" purgeable\";\n\t\tbreak;\n\tcase MSM_MADV_WILLNEED:\n\tdefault:\n\t\tmadv = \"\";\n\t\tbreak;\n\t}\n\n\tseq_printf(m, \"%08x: %c %2d (%2d) %08llx %p\",\n\t\t\tmsm_obj->flags, msm_gem_active(obj) ? 'A' : 'I',\n\t\t\tobj->name, kref_read(&obj->refcount),\n\t\t\toff, msm_obj->vaddr);\n\n\tseq_printf(m, \" %08zu %9s %-32s\\n\", obj->size, madv, msm_obj->name);\n\n\tif (!list_empty(&msm_obj->vmas)) {\n\n\t\tseq_puts(m, \"      vmas:\");\n\n\t\tlist_for_each_entry(vma, &msm_obj->vmas, list) {\n\t\t\tconst char *name, *comm;\n\t\t\tif (vma->aspace) {\n\t\t\t\tstruct msm_gem_address_space *aspace = vma->aspace;\n\t\t\t\tstruct task_struct *task =\n\t\t\t\t\tget_pid_task(aspace->pid, PIDTYPE_PID);\n\t\t\t\tif (task) {\n\t\t\t\t\tcomm = kstrdup(task->comm, GFP_KERNEL);\n\t\t\t\t\tput_task_struct(task);\n\t\t\t\t} else {\n\t\t\t\t\tcomm = NULL;\n\t\t\t\t}\n\t\t\t\tname = aspace->name;\n\t\t\t} else {\n\t\t\t\tname = comm = NULL;\n\t\t\t}\n\t\t\tseq_printf(m, \" [%s%s%s: aspace=%p, %08llx,%s]\",\n\t\t\t\tname, comm ? \":\" : \"\", comm ? comm : \"\",\n\t\t\t\tvma->aspace, vma->iova,\n\t\t\t\tvma->mapped ? \"mapped\" : \"unmapped\");\n\t\t\tkfree(comm);\n\t\t}\n\n\t\tseq_puts(m, \"\\n\");\n\t}\n\n\tdma_resv_describe(robj, m);\n\tmsm_gem_unlock(obj);\n}\n\nvoid msm_gem_describe_objects(struct list_head *list, struct seq_file *m)\n{\n\tstruct msm_gem_stats stats = {};\n\tstruct msm_gem_object *msm_obj;\n\n\tseq_puts(m, \"   flags       id ref  offset   kaddr            size     madv      name\\n\");\n\tlist_for_each_entry(msm_obj, list, node) {\n\t\tstruct drm_gem_object *obj = &msm_obj->base;\n\t\tseq_puts(m, \"   \");\n\t\tmsm_gem_describe(obj, m, &stats);\n\t}\n\n\tseq_printf(m, \"Total:     %4d objects, %9zu bytes\\n\",\n\t\t\tstats.all.count, stats.all.size);\n\tseq_printf(m, \"Active:    %4d objects, %9zu bytes\\n\",\n\t\t\tstats.active.count, stats.active.size);\n\tseq_printf(m, \"Resident:  %4d objects, %9zu bytes\\n\",\n\t\t\tstats.resident.count, stats.resident.size);\n\tseq_printf(m, \"Purgeable: %4d objects, %9zu bytes\\n\",\n\t\t\tstats.purgeable.count, stats.purgeable.size);\n\tseq_printf(m, \"Purged:    %4d objects, %9zu bytes\\n\",\n\t\t\tstats.purged.count, stats.purged.size);\n}\n#endif\n\n \nstatic void msm_gem_free_object(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tstruct drm_device *dev = obj->dev;\n\tstruct msm_drm_private *priv = dev->dev_private;\n\n\tmutex_lock(&priv->obj_lock);\n\tlist_del(&msm_obj->node);\n\tmutex_unlock(&priv->obj_lock);\n\n\tput_iova_spaces(obj, true);\n\n\tif (obj->import_attach) {\n\t\tGEM_WARN_ON(msm_obj->vaddr);\n\n\t\t \n\t\tkvfree(msm_obj->pages);\n\n\t\tput_iova_vmas(obj);\n\n\t\tdrm_prime_gem_destroy(obj, msm_obj->sgt);\n\t} else {\n\t\tmsm_gem_vunmap(obj);\n\t\tput_pages(obj);\n\t\tput_iova_vmas(obj);\n\t}\n\n\tdrm_gem_object_release(obj);\n\n\tkfree(msm_obj);\n}\n\nstatic int msm_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\n\tvm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_page_prot = msm_gem_pgprot(msm_obj, vm_get_page_prot(vma->vm_flags));\n\n\treturn 0;\n}\n\n \nint msm_gem_new_handle(struct drm_device *dev, struct drm_file *file,\n\t\tuint32_t size, uint32_t flags, uint32_t *handle,\n\t\tchar *name)\n{\n\tstruct drm_gem_object *obj;\n\tint ret;\n\n\tobj = msm_gem_new(dev, size, flags);\n\n\tif (IS_ERR(obj))\n\t\treturn PTR_ERR(obj);\n\n\tif (name)\n\t\tmsm_gem_object_set_name(obj, \"%s\", name);\n\n\tret = drm_gem_handle_create(file, obj, handle);\n\n\t \n\tdrm_gem_object_put(obj);\n\n\treturn ret;\n}\n\nstatic enum drm_gem_object_status msm_gem_status(struct drm_gem_object *obj)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(obj);\n\tenum drm_gem_object_status status = 0;\n\n\tif (msm_obj->pages)\n\t\tstatus |= DRM_GEM_OBJECT_RESIDENT;\n\n\tif (msm_obj->madv == MSM_MADV_DONTNEED)\n\t\tstatus |= DRM_GEM_OBJECT_PURGEABLE;\n\n\treturn status;\n}\n\nstatic const struct vm_operations_struct vm_ops = {\n\t.fault = msm_gem_fault,\n\t.open = drm_gem_vm_open,\n\t.close = drm_gem_vm_close,\n};\n\nstatic const struct drm_gem_object_funcs msm_gem_object_funcs = {\n\t.free = msm_gem_free_object,\n\t.pin = msm_gem_prime_pin,\n\t.unpin = msm_gem_prime_unpin,\n\t.get_sg_table = msm_gem_prime_get_sg_table,\n\t.vmap = msm_gem_prime_vmap,\n\t.vunmap = msm_gem_prime_vunmap,\n\t.mmap = msm_gem_object_mmap,\n\t.status = msm_gem_status,\n\t.vm_ops = &vm_ops,\n};\n\nstatic int msm_gem_new_impl(struct drm_device *dev,\n\t\tuint32_t size, uint32_t flags,\n\t\tstruct drm_gem_object **obj)\n{\n\tstruct msm_drm_private *priv = dev->dev_private;\n\tstruct msm_gem_object *msm_obj;\n\n\tswitch (flags & MSM_BO_CACHE_MASK) {\n\tcase MSM_BO_CACHED:\n\tcase MSM_BO_WC:\n\t\tbreak;\n\tcase MSM_BO_CACHED_COHERENT:\n\t\tif (priv->has_cached_coherent)\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\tDRM_DEV_DEBUG(dev->dev, \"invalid cache flag: %x\\n\",\n\t\t\t\t(flags & MSM_BO_CACHE_MASK));\n\t\treturn -EINVAL;\n\t}\n\n\tmsm_obj = kzalloc(sizeof(*msm_obj), GFP_KERNEL);\n\tif (!msm_obj)\n\t\treturn -ENOMEM;\n\n\tmsm_obj->flags = flags;\n\tmsm_obj->madv = MSM_MADV_WILLNEED;\n\n\tINIT_LIST_HEAD(&msm_obj->node);\n\tINIT_LIST_HEAD(&msm_obj->vmas);\n\n\t*obj = &msm_obj->base;\n\t(*obj)->funcs = &msm_gem_object_funcs;\n\n\treturn 0;\n}\n\nstruct drm_gem_object *msm_gem_new(struct drm_device *dev, uint32_t size, uint32_t flags)\n{\n\tstruct msm_drm_private *priv = dev->dev_private;\n\tstruct msm_gem_object *msm_obj;\n\tstruct drm_gem_object *obj = NULL;\n\tbool use_vram = false;\n\tint ret;\n\n\tsize = PAGE_ALIGN(size);\n\n\tif (!msm_use_mmu(dev))\n\t\tuse_vram = true;\n\telse if ((flags & (MSM_BO_STOLEN | MSM_BO_SCANOUT)) && priv->vram.size)\n\t\tuse_vram = true;\n\n\tif (GEM_WARN_ON(use_vram && !priv->vram.size))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tif (size == 0)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tret = msm_gem_new_impl(dev, size, flags, &obj);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tmsm_obj = to_msm_bo(obj);\n\n\tif (use_vram) {\n\t\tstruct msm_gem_vma *vma;\n\t\tstruct page **pages;\n\n\t\tdrm_gem_private_object_init(dev, obj, size);\n\n\t\tmsm_gem_lock(obj);\n\n\t\tvma = add_vma(obj, NULL);\n\t\tmsm_gem_unlock(obj);\n\t\tif (IS_ERR(vma)) {\n\t\t\tret = PTR_ERR(vma);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tto_msm_bo(obj)->vram_node = &vma->node;\n\n\t\tmsm_gem_lock(obj);\n\t\tpages = get_pages(obj);\n\t\tmsm_gem_unlock(obj);\n\t\tif (IS_ERR(pages)) {\n\t\t\tret = PTR_ERR(pages);\n\t\t\tgoto fail;\n\t\t}\n\n\t\tvma->iova = physaddr(obj);\n\t} else {\n\t\tret = drm_gem_object_init(dev, obj, size);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t\t \n\t\tmapping_set_gfp_mask(obj->filp->f_mapping, GFP_HIGHUSER);\n\t}\n\n\tdrm_gem_lru_move_tail(&priv->lru.unbacked, obj);\n\n\tmutex_lock(&priv->obj_lock);\n\tlist_add_tail(&msm_obj->node, &priv->objects);\n\tmutex_unlock(&priv->obj_lock);\n\n\tret = drm_gem_create_mmap_offset(obj);\n\tif (ret)\n\t\tgoto fail;\n\n\treturn obj;\n\nfail:\n\tdrm_gem_object_put(obj);\n\treturn ERR_PTR(ret);\n}\n\nstruct drm_gem_object *msm_gem_import(struct drm_device *dev,\n\t\tstruct dma_buf *dmabuf, struct sg_table *sgt)\n{\n\tstruct msm_drm_private *priv = dev->dev_private;\n\tstruct msm_gem_object *msm_obj;\n\tstruct drm_gem_object *obj;\n\tuint32_t size;\n\tint ret, npages;\n\n\t \n\tif (!msm_use_mmu(dev)) {\n\t\tDRM_DEV_ERROR(dev->dev, \"cannot import without IOMMU\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tsize = PAGE_ALIGN(dmabuf->size);\n\n\tret = msm_gem_new_impl(dev, size, MSM_BO_WC, &obj);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\tdrm_gem_private_object_init(dev, obj, size);\n\n\tnpages = size / PAGE_SIZE;\n\n\tmsm_obj = to_msm_bo(obj);\n\tmsm_gem_lock(obj);\n\tmsm_obj->sgt = sgt;\n\tmsm_obj->pages = kvmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);\n\tif (!msm_obj->pages) {\n\t\tmsm_gem_unlock(obj);\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\tret = drm_prime_sg_to_page_array(sgt, msm_obj->pages, npages);\n\tif (ret) {\n\t\tmsm_gem_unlock(obj);\n\t\tgoto fail;\n\t}\n\n\tmsm_gem_unlock(obj);\n\n\tdrm_gem_lru_move_tail(&priv->lru.pinned, obj);\n\n\tmutex_lock(&priv->obj_lock);\n\tlist_add_tail(&msm_obj->node, &priv->objects);\n\tmutex_unlock(&priv->obj_lock);\n\n\tret = drm_gem_create_mmap_offset(obj);\n\tif (ret)\n\t\tgoto fail;\n\n\treturn obj;\n\nfail:\n\tdrm_gem_object_put(obj);\n\treturn ERR_PTR(ret);\n}\n\nvoid *msm_gem_kernel_new(struct drm_device *dev, uint32_t size,\n\t\tuint32_t flags, struct msm_gem_address_space *aspace,\n\t\tstruct drm_gem_object **bo, uint64_t *iova)\n{\n\tvoid *vaddr;\n\tstruct drm_gem_object *obj = msm_gem_new(dev, size, flags);\n\tint ret;\n\n\tif (IS_ERR(obj))\n\t\treturn ERR_CAST(obj);\n\n\tif (iova) {\n\t\tret = msm_gem_get_and_pin_iova(obj, aspace, iova);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\n\tvaddr = msm_gem_get_vaddr(obj);\n\tif (IS_ERR(vaddr)) {\n\t\tmsm_gem_unpin_iova(obj, aspace);\n\t\tret = PTR_ERR(vaddr);\n\t\tgoto err;\n\t}\n\n\tif (bo)\n\t\t*bo = obj;\n\n\treturn vaddr;\nerr:\n\tdrm_gem_object_put(obj);\n\n\treturn ERR_PTR(ret);\n\n}\n\nvoid msm_gem_kernel_put(struct drm_gem_object *bo,\n\t\tstruct msm_gem_address_space *aspace)\n{\n\tif (IS_ERR_OR_NULL(bo))\n\t\treturn;\n\n\tmsm_gem_put_vaddr(bo);\n\tmsm_gem_unpin_iova(bo, aspace);\n\tdrm_gem_object_put(bo);\n}\n\nvoid msm_gem_object_set_name(struct drm_gem_object *bo, const char *fmt, ...)\n{\n\tstruct msm_gem_object *msm_obj = to_msm_bo(bo);\n\tva_list ap;\n\n\tif (!fmt)\n\t\treturn;\n\n\tva_start(ap, fmt);\n\tvsnprintf(msm_obj->name, sizeof(msm_obj->name), fmt, ap);\n\tva_end(ap);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}