{
  "module_name": "msm_gpummu.c",
  "hash_id": "599b277f6d47aeb07492f86c743f3ece910a7b3ec3561864885ee8dd5c20cd1f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/msm_gpummu.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n\n#include \"msm_drv.h\"\n#include \"msm_mmu.h\"\n#include \"adreno/adreno_gpu.h\"\n#include \"adreno/a2xx.xml.h\"\n\nstruct msm_gpummu {\n\tstruct msm_mmu base;\n\tstruct msm_gpu *gpu;\n\tdma_addr_t pt_base;\n\tuint32_t *table;\n};\n#define to_msm_gpummu(x) container_of(x, struct msm_gpummu, base)\n\n#define GPUMMU_VA_START SZ_16M\n#define GPUMMU_VA_RANGE (0xfff * SZ_64K)\n#define GPUMMU_PAGE_SIZE SZ_4K\n#define TABLE_SIZE (sizeof(uint32_t) * GPUMMU_VA_RANGE / GPUMMU_PAGE_SIZE)\n\nstatic void msm_gpummu_detach(struct msm_mmu *mmu)\n{\n}\n\nstatic int msm_gpummu_map(struct msm_mmu *mmu, uint64_t iova,\n\t\tstruct sg_table *sgt, size_t len, int prot)\n{\n\tstruct msm_gpummu *gpummu = to_msm_gpummu(mmu);\n\tunsigned idx = (iova - GPUMMU_VA_START) / GPUMMU_PAGE_SIZE;\n\tstruct sg_dma_page_iter dma_iter;\n\tunsigned prot_bits = 0;\n\n\tif (prot & IOMMU_WRITE)\n\t\tprot_bits |= 1;\n\tif (prot & IOMMU_READ)\n\t\tprot_bits |= 2;\n\n\tfor_each_sgtable_dma_page(sgt, &dma_iter, 0) {\n\t\tdma_addr_t addr = sg_page_iter_dma_address(&dma_iter);\n\t\tint i;\n\n\t\tfor (i = 0; i < PAGE_SIZE; i += GPUMMU_PAGE_SIZE)\n\t\t\tgpummu->table[idx++] = (addr + i) | prot_bits;\n\t}\n\n\t \n\tgpu_write(gpummu->gpu, REG_A2XX_MH_MMU_INVALIDATE,\n\t\tA2XX_MH_MMU_INVALIDATE_INVALIDATE_ALL |\n\t\tA2XX_MH_MMU_INVALIDATE_INVALIDATE_TC);\n\treturn 0;\n}\n\nstatic int msm_gpummu_unmap(struct msm_mmu *mmu, uint64_t iova, size_t len)\n{\n\tstruct msm_gpummu *gpummu = to_msm_gpummu(mmu);\n\tunsigned idx = (iova - GPUMMU_VA_START) / GPUMMU_PAGE_SIZE;\n\tunsigned i;\n\n\tfor (i = 0; i < len / GPUMMU_PAGE_SIZE; i++, idx++)\n                gpummu->table[idx] = 0;\n\n\tgpu_write(gpummu->gpu, REG_A2XX_MH_MMU_INVALIDATE,\n\t\tA2XX_MH_MMU_INVALIDATE_INVALIDATE_ALL |\n\t\tA2XX_MH_MMU_INVALIDATE_INVALIDATE_TC);\n\treturn 0;\n}\n\nstatic void msm_gpummu_resume_translation(struct msm_mmu *mmu)\n{\n}\n\nstatic void msm_gpummu_destroy(struct msm_mmu *mmu)\n{\n\tstruct msm_gpummu *gpummu = to_msm_gpummu(mmu);\n\n\tdma_free_attrs(mmu->dev, TABLE_SIZE, gpummu->table, gpummu->pt_base,\n\t\tDMA_ATTR_FORCE_CONTIGUOUS);\n\n\tkfree(gpummu);\n}\n\nstatic const struct msm_mmu_funcs funcs = {\n\t\t.detach = msm_gpummu_detach,\n\t\t.map = msm_gpummu_map,\n\t\t.unmap = msm_gpummu_unmap,\n\t\t.destroy = msm_gpummu_destroy,\n\t\t.resume_translation = msm_gpummu_resume_translation,\n};\n\nstruct msm_mmu *msm_gpummu_new(struct device *dev, struct msm_gpu *gpu)\n{\n\tstruct msm_gpummu *gpummu;\n\n\tgpummu = kzalloc(sizeof(*gpummu), GFP_KERNEL);\n\tif (!gpummu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tgpummu->table = dma_alloc_attrs(dev, TABLE_SIZE + 32, &gpummu->pt_base,\n\t\tGFP_KERNEL | __GFP_ZERO, DMA_ATTR_FORCE_CONTIGUOUS);\n\tif (!gpummu->table) {\n\t\tkfree(gpummu);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tgpummu->gpu = gpu;\n\tmsm_mmu_init(&gpummu->base, dev, &funcs, MSM_MMU_GPUMMU);\n\n\treturn &gpummu->base;\n}\n\nvoid msm_gpummu_params(struct msm_mmu *mmu, dma_addr_t *pt_base,\n\t\tdma_addr_t *tran_error)\n{\n\tdma_addr_t base = to_msm_gpummu(mmu)->pt_base;\n\n\t*pt_base = base;\n\t*tran_error = base + TABLE_SIZE;  \n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}