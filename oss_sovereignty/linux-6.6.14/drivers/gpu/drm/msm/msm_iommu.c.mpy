{
  "module_name": "msm_iommu.c",
  "hash_id": "5dea14093fbca8030ef1d9992803d5cd73224bf254593f3b44a28e8111ad0594",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/msm/msm_iommu.c",
  "human_readable_source": "\n \n\n#include <linux/adreno-smmu-priv.h>\n#include <linux/io-pgtable.h>\n#include \"msm_drv.h\"\n#include \"msm_mmu.h\"\n\nstruct msm_iommu {\n\tstruct msm_mmu base;\n\tstruct iommu_domain *domain;\n\tatomic_t pagetables;\n};\n\n#define to_msm_iommu(x) container_of(x, struct msm_iommu, base)\n\nstruct msm_iommu_pagetable {\n\tstruct msm_mmu base;\n\tstruct msm_mmu *parent;\n\tstruct io_pgtable_ops *pgtbl_ops;\n\tunsigned long pgsize_bitmap;\t \n\tphys_addr_t ttbr;\n\tu32 asid;\n};\nstatic struct msm_iommu_pagetable *to_pagetable(struct msm_mmu *mmu)\n{\n\treturn container_of(mmu, struct msm_iommu_pagetable, base);\n}\n\n \nstatic size_t calc_pgsize(struct msm_iommu_pagetable *pagetable,\n\t\t\t   unsigned long iova, phys_addr_t paddr,\n\t\t\t   size_t size, size_t *count)\n{\n\tunsigned int pgsize_idx, pgsize_idx_next;\n\tunsigned long pgsizes;\n\tsize_t offset, pgsize, pgsize_next;\n\tunsigned long addr_merge = paddr | iova;\n\n\t \n\tpgsizes = pagetable->pgsize_bitmap & GENMASK(__fls(size), 0);\n\n\t \n\tif (likely(addr_merge))\n\t\tpgsizes &= GENMASK(__ffs(addr_merge), 0);\n\n\t \n\tBUG_ON(!pgsizes);\n\n\t \n\tpgsize_idx = __fls(pgsizes);\n\tpgsize = BIT(pgsize_idx);\n\tif (!count)\n\t\treturn pgsize;\n\n\t \n\tpgsizes = pagetable->pgsize_bitmap & ~GENMASK(pgsize_idx, 0);\n\tif (!pgsizes)\n\t\tgoto out_set_count;\n\n\tpgsize_idx_next = __ffs(pgsizes);\n\tpgsize_next = BIT(pgsize_idx_next);\n\n\t \n\tif ((iova ^ paddr) & (pgsize_next - 1))\n\t\tgoto out_set_count;\n\n\t \n\toffset = pgsize_next - (addr_merge & (pgsize_next - 1));\n\n\t \n\tif (offset + pgsize_next <= size)\n\t\tsize = offset;\n\nout_set_count:\n\t*count = size >> pgsize_idx;\n\treturn pgsize;\n}\n\nstatic int msm_iommu_pagetable_unmap(struct msm_mmu *mmu, u64 iova,\n\t\tsize_t size)\n{\n\tstruct msm_iommu_pagetable *pagetable = to_pagetable(mmu);\n\tstruct io_pgtable_ops *ops = pagetable->pgtbl_ops;\n\n\twhile (size) {\n\t\tsize_t unmapped, pgsize, count;\n\n\t\tpgsize = calc_pgsize(pagetable, iova, iova, size, &count);\n\n\t\tunmapped = ops->unmap_pages(ops, iova, pgsize, count, NULL);\n\t\tif (!unmapped)\n\t\t\tbreak;\n\n\t\tiova += unmapped;\n\t\tsize -= unmapped;\n\t}\n\n\tiommu_flush_iotlb_all(to_msm_iommu(pagetable->parent)->domain);\n\n\treturn (size == 0) ? 0 : -EINVAL;\n}\n\nstatic int msm_iommu_pagetable_map(struct msm_mmu *mmu, u64 iova,\n\t\tstruct sg_table *sgt, size_t len, int prot)\n{\n\tstruct msm_iommu_pagetable *pagetable = to_pagetable(mmu);\n\tstruct io_pgtable_ops *ops = pagetable->pgtbl_ops;\n\tstruct scatterlist *sg;\n\tu64 addr = iova;\n\tunsigned int i;\n\n\tfor_each_sgtable_sg(sgt, sg, i) {\n\t\tsize_t size = sg->length;\n\t\tphys_addr_t phys = sg_phys(sg);\n\n\t\twhile (size) {\n\t\t\tsize_t pgsize, count, mapped = 0;\n\t\t\tint ret;\n\n\t\t\tpgsize = calc_pgsize(pagetable, addr, phys, size, &count);\n\n\t\t\tret = ops->map_pages(ops, addr, phys, pgsize, count,\n\t\t\t\t\t     prot, GFP_KERNEL, &mapped);\n\n\t\t\t \n\t\t\tphys += mapped;\n\t\t\taddr += mapped;\n\t\t\tsize -= mapped;\n\n\t\t\tif (ret) {\n\t\t\t\tmsm_iommu_pagetable_unmap(mmu, iova, addr - iova);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void msm_iommu_pagetable_destroy(struct msm_mmu *mmu)\n{\n\tstruct msm_iommu_pagetable *pagetable = to_pagetable(mmu);\n\tstruct msm_iommu *iommu = to_msm_iommu(pagetable->parent);\n\tstruct adreno_smmu_priv *adreno_smmu =\n\t\tdev_get_drvdata(pagetable->parent->dev);\n\n\t \n\tif (atomic_dec_return(&iommu->pagetables) == 0)\n\t\tadreno_smmu->set_ttbr0_cfg(adreno_smmu->cookie, NULL);\n\n\tfree_io_pgtable_ops(pagetable->pgtbl_ops);\n\tkfree(pagetable);\n}\n\nint msm_iommu_pagetable_params(struct msm_mmu *mmu,\n\t\tphys_addr_t *ttbr, int *asid)\n{\n\tstruct msm_iommu_pagetable *pagetable;\n\n\tif (mmu->type != MSM_MMU_IOMMU_PAGETABLE)\n\t\treturn -EINVAL;\n\n\tpagetable = to_pagetable(mmu);\n\n\tif (ttbr)\n\t\t*ttbr = pagetable->ttbr;\n\n\tif (asid)\n\t\t*asid = pagetable->asid;\n\n\treturn 0;\n}\n\nstruct iommu_domain_geometry *msm_iommu_get_geometry(struct msm_mmu *mmu)\n{\n\tstruct msm_iommu *iommu = to_msm_iommu(mmu);\n\n\treturn &iommu->domain->geometry;\n}\n\nstatic const struct msm_mmu_funcs pagetable_funcs = {\n\t\t.map = msm_iommu_pagetable_map,\n\t\t.unmap = msm_iommu_pagetable_unmap,\n\t\t.destroy = msm_iommu_pagetable_destroy,\n};\n\nstatic void msm_iommu_tlb_flush_all(void *cookie)\n{\n}\n\nstatic void msm_iommu_tlb_flush_walk(unsigned long iova, size_t size,\n\t\tsize_t granule, void *cookie)\n{\n}\n\nstatic void msm_iommu_tlb_add_page(struct iommu_iotlb_gather *gather,\n\t\tunsigned long iova, size_t granule, void *cookie)\n{\n}\n\nstatic const struct iommu_flush_ops null_tlb_ops = {\n\t.tlb_flush_all = msm_iommu_tlb_flush_all,\n\t.tlb_flush_walk = msm_iommu_tlb_flush_walk,\n\t.tlb_add_page = msm_iommu_tlb_add_page,\n};\n\nstatic int msm_fault_handler(struct iommu_domain *domain, struct device *dev,\n\t\tunsigned long iova, int flags, void *arg);\n\nstruct msm_mmu *msm_iommu_pagetable_create(struct msm_mmu *parent)\n{\n\tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(parent->dev);\n\tstruct msm_iommu *iommu = to_msm_iommu(parent);\n\tstruct msm_iommu_pagetable *pagetable;\n\tconst struct io_pgtable_cfg *ttbr1_cfg = NULL;\n\tstruct io_pgtable_cfg ttbr0_cfg;\n\tint ret;\n\n\t \n\tif (adreno_smmu->cookie)\n\t\tttbr1_cfg = adreno_smmu->get_ttbr1_cfg(adreno_smmu->cookie);\n\n\t \n\tif (WARN_ONCE(!ttbr1_cfg, \"No per-process page tables\"))\n\t\treturn ERR_PTR(-ENODEV);\n\n\tpagetable = kzalloc(sizeof(*pagetable), GFP_KERNEL);\n\tif (!pagetable)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmsm_mmu_init(&pagetable->base, parent->dev, &pagetable_funcs,\n\t\tMSM_MMU_IOMMU_PAGETABLE);\n\n\t \n\tttbr0_cfg = *ttbr1_cfg;\n\n\t \n\tttbr0_cfg.quirks &= ~IO_PGTABLE_QUIRK_ARM_TTBR1;\n\tttbr0_cfg.tlb = &null_tlb_ops;\n\n\tpagetable->pgtbl_ops = alloc_io_pgtable_ops(ARM_64_LPAE_S1,\n\t\t&ttbr0_cfg, iommu->domain);\n\n\tif (!pagetable->pgtbl_ops) {\n\t\tkfree(pagetable);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\tif (atomic_inc_return(&iommu->pagetables) == 1) {\n\t\tret = adreno_smmu->set_ttbr0_cfg(adreno_smmu->cookie, &ttbr0_cfg);\n\t\tif (ret) {\n\t\t\tfree_io_pgtable_ops(pagetable->pgtbl_ops);\n\t\t\tkfree(pagetable);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\n\t \n\tpagetable->parent = parent;\n\tpagetable->pgsize_bitmap = ttbr0_cfg.pgsize_bitmap;\n\tpagetable->ttbr = ttbr0_cfg.arm_lpae_s1_cfg.ttbr;\n\n\t \n\tpagetable->asid = 0;\n\n\treturn &pagetable->base;\n}\n\nstatic int msm_fault_handler(struct iommu_domain *domain, struct device *dev,\n\t\tunsigned long iova, int flags, void *arg)\n{\n\tstruct msm_iommu *iommu = arg;\n\tstruct msm_mmu *mmu = &iommu->base;\n\tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(iommu->base.dev);\n\tstruct adreno_smmu_fault_info info, *ptr = NULL;\n\n\tif (adreno_smmu->get_fault_info) {\n\t\tadreno_smmu->get_fault_info(adreno_smmu->cookie, &info);\n\t\tptr = &info;\n\t}\n\n\tif (iommu->base.handler)\n\t\treturn iommu->base.handler(iommu->base.arg, iova, flags, ptr);\n\n\tpr_warn_ratelimited(\"*** fault: iova=%16lx, flags=%d\\n\", iova, flags);\n\n\tif (mmu->funcs->resume_translation)\n\t\tmmu->funcs->resume_translation(mmu);\n\n\treturn 0;\n}\n\nstatic void msm_iommu_resume_translation(struct msm_mmu *mmu)\n{\n\tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(mmu->dev);\n\n\tif (adreno_smmu->resume_translation)\n\t\tadreno_smmu->resume_translation(adreno_smmu->cookie, true);\n}\n\nstatic void msm_iommu_detach(struct msm_mmu *mmu)\n{\n\tstruct msm_iommu *iommu = to_msm_iommu(mmu);\n\n\tiommu_detach_device(iommu->domain, mmu->dev);\n}\n\nstatic int msm_iommu_map(struct msm_mmu *mmu, uint64_t iova,\n\t\tstruct sg_table *sgt, size_t len, int prot)\n{\n\tstruct msm_iommu *iommu = to_msm_iommu(mmu);\n\tsize_t ret;\n\n\t \n\tif (iova & BIT_ULL(48))\n\t\tiova |= GENMASK_ULL(63, 49);\n\n\tret = iommu_map_sgtable(iommu->domain, iova, sgt, prot);\n\tWARN_ON(!ret);\n\n\treturn (ret == len) ? 0 : -EINVAL;\n}\n\nstatic int msm_iommu_unmap(struct msm_mmu *mmu, uint64_t iova, size_t len)\n{\n\tstruct msm_iommu *iommu = to_msm_iommu(mmu);\n\n\tif (iova & BIT_ULL(48))\n\t\tiova |= GENMASK_ULL(63, 49);\n\n\tiommu_unmap(iommu->domain, iova, len);\n\n\treturn 0;\n}\n\nstatic void msm_iommu_destroy(struct msm_mmu *mmu)\n{\n\tstruct msm_iommu *iommu = to_msm_iommu(mmu);\n\tiommu_domain_free(iommu->domain);\n\tkfree(iommu);\n}\n\nstatic const struct msm_mmu_funcs funcs = {\n\t\t.detach = msm_iommu_detach,\n\t\t.map = msm_iommu_map,\n\t\t.unmap = msm_iommu_unmap,\n\t\t.destroy = msm_iommu_destroy,\n\t\t.resume_translation = msm_iommu_resume_translation,\n};\n\nstruct msm_mmu *msm_iommu_new(struct device *dev, unsigned long quirks)\n{\n\tstruct iommu_domain *domain;\n\tstruct msm_iommu *iommu;\n\tint ret;\n\n\tdomain = iommu_domain_alloc(dev->bus);\n\tif (!domain)\n\t\treturn NULL;\n\n\tiommu_set_pgtable_quirks(domain, quirks);\n\n\tiommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\n\tif (!iommu) {\n\t\tiommu_domain_free(domain);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tiommu->domain = domain;\n\tmsm_mmu_init(&iommu->base, dev, &funcs, MSM_MMU_IOMMU);\n\n\tatomic_set(&iommu->pagetables, 0);\n\n\tret = iommu_attach_device(iommu->domain, dev);\n\tif (ret) {\n\t\tiommu_domain_free(domain);\n\t\tkfree(iommu);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn &iommu->base;\n}\n\nstruct msm_mmu *msm_iommu_gpu_new(struct device *dev, struct msm_gpu *gpu, unsigned long quirks)\n{\n\tstruct adreno_smmu_priv *adreno_smmu = dev_get_drvdata(dev);\n\tstruct msm_iommu *iommu;\n\tstruct msm_mmu *mmu;\n\n\tmmu = msm_iommu_new(dev, quirks);\n\tif (IS_ERR_OR_NULL(mmu))\n\t\treturn mmu;\n\n\tiommu = to_msm_iommu(mmu);\n\tiommu_set_fault_handler(iommu->domain, msm_fault_handler, iommu);\n\n\t \n\tif (adreno_smmu->set_stall)\n\t\tadreno_smmu->set_stall(adreno_smmu->cookie, true);\n\n\treturn mmu;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}