{
  "module_name": "virtgpu_vq.c",
  "hash_id": "da5da1f091b993e3a52aa6f93add3f97780b1c23de9a35942c952fe65d804e53",
  "original_prompt": "Ingested from linux-6.6.14/drivers/gpu/drm/virtio/virtgpu_vq.c",
  "human_readable_source": " \n\n#include <linux/dma-mapping.h>\n#include <linux/virtio.h>\n#include <linux/virtio_config.h>\n#include <linux/virtio_ring.h>\n\n#include <drm/drm_edid.h>\n\n#include \"virtgpu_drv.h\"\n#include \"virtgpu_trace.h\"\n\n#define MAX_INLINE_CMD_SIZE   96\n#define MAX_INLINE_RESP_SIZE  24\n#define VBUFFER_SIZE          (sizeof(struct virtio_gpu_vbuffer) \\\n\t\t\t       + MAX_INLINE_CMD_SIZE\t\t \\\n\t\t\t       + MAX_INLINE_RESP_SIZE)\n\nstatic void convert_to_hw_box(struct virtio_gpu_box *dst,\n\t\t\t      const struct drm_virtgpu_3d_box *src)\n{\n\tdst->x = cpu_to_le32(src->x);\n\tdst->y = cpu_to_le32(src->y);\n\tdst->z = cpu_to_le32(src->z);\n\tdst->w = cpu_to_le32(src->w);\n\tdst->h = cpu_to_le32(src->h);\n\tdst->d = cpu_to_le32(src->d);\n}\n\nvoid virtio_gpu_ctrl_ack(struct virtqueue *vq)\n{\n\tstruct drm_device *dev = vq->vdev->priv;\n\tstruct virtio_gpu_device *vgdev = dev->dev_private;\n\n\tschedule_work(&vgdev->ctrlq.dequeue_work);\n}\n\nvoid virtio_gpu_cursor_ack(struct virtqueue *vq)\n{\n\tstruct drm_device *dev = vq->vdev->priv;\n\tstruct virtio_gpu_device *vgdev = dev->dev_private;\n\n\tschedule_work(&vgdev->cursorq.dequeue_work);\n}\n\nint virtio_gpu_alloc_vbufs(struct virtio_gpu_device *vgdev)\n{\n\tvgdev->vbufs = kmem_cache_create(\"virtio-gpu-vbufs\",\n\t\t\t\t\t VBUFFER_SIZE,\n\t\t\t\t\t __alignof__(struct virtio_gpu_vbuffer),\n\t\t\t\t\t 0, NULL);\n\tif (!vgdev->vbufs)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid virtio_gpu_free_vbufs(struct virtio_gpu_device *vgdev)\n{\n\tkmem_cache_destroy(vgdev->vbufs);\n\tvgdev->vbufs = NULL;\n}\n\nstatic struct virtio_gpu_vbuffer*\nvirtio_gpu_get_vbuf(struct virtio_gpu_device *vgdev,\n\t\t    int size, int resp_size, void *resp_buf,\n\t\t    virtio_gpu_resp_cb resp_cb)\n{\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tvbuf = kmem_cache_zalloc(vgdev->vbufs, GFP_KERNEL | __GFP_NOFAIL);\n\n\tBUG_ON(size > MAX_INLINE_CMD_SIZE ||\n\t       size < sizeof(struct virtio_gpu_ctrl_hdr));\n\tvbuf->buf = (void *)vbuf + sizeof(*vbuf);\n\tvbuf->size = size;\n\n\tvbuf->resp_cb = resp_cb;\n\tvbuf->resp_size = resp_size;\n\tif (resp_size <= MAX_INLINE_RESP_SIZE)\n\t\tvbuf->resp_buf = (void *)vbuf->buf + size;\n\telse\n\t\tvbuf->resp_buf = resp_buf;\n\tBUG_ON(!vbuf->resp_buf);\n\treturn vbuf;\n}\n\nstatic struct virtio_gpu_ctrl_hdr *\nvirtio_gpu_vbuf_ctrl_hdr(struct virtio_gpu_vbuffer *vbuf)\n{\n\t \n\treturn (struct virtio_gpu_ctrl_hdr *)vbuf->buf;\n}\n\nstatic struct virtio_gpu_update_cursor*\nvirtio_gpu_alloc_cursor(struct virtio_gpu_device *vgdev,\n\t\t\tstruct virtio_gpu_vbuffer **vbuffer_p)\n{\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tvbuf = virtio_gpu_get_vbuf\n\t\t(vgdev, sizeof(struct virtio_gpu_update_cursor),\n\t\t 0, NULL, NULL);\n\tif (IS_ERR(vbuf)) {\n\t\t*vbuffer_p = NULL;\n\t\treturn ERR_CAST(vbuf);\n\t}\n\t*vbuffer_p = vbuf;\n\treturn (struct virtio_gpu_update_cursor *)vbuf->buf;\n}\n\nstatic void *virtio_gpu_alloc_cmd_resp(struct virtio_gpu_device *vgdev,\n\t\t\t\t       virtio_gpu_resp_cb cb,\n\t\t\t\t       struct virtio_gpu_vbuffer **vbuffer_p,\n\t\t\t\t       int cmd_size, int resp_size,\n\t\t\t\t       void *resp_buf)\n{\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tvbuf = virtio_gpu_get_vbuf(vgdev, cmd_size,\n\t\t\t\t   resp_size, resp_buf, cb);\n\t*vbuffer_p = vbuf;\n\treturn (struct virtio_gpu_command *)vbuf->buf;\n}\n\nstatic void *virtio_gpu_alloc_cmd(struct virtio_gpu_device *vgdev,\n\t\t\t\t  struct virtio_gpu_vbuffer **vbuffer_p,\n\t\t\t\t  int size)\n{\n\treturn virtio_gpu_alloc_cmd_resp(vgdev, NULL, vbuffer_p, size,\n\t\t\t\t\t sizeof(struct virtio_gpu_ctrl_hdr),\n\t\t\t\t\t NULL);\n}\n\nstatic void *virtio_gpu_alloc_cmd_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t     struct virtio_gpu_vbuffer **vbuffer_p,\n\t\t\t\t     int size,\n\t\t\t\t     virtio_gpu_resp_cb cb)\n{\n\treturn virtio_gpu_alloc_cmd_resp(vgdev, cb, vbuffer_p, size,\n\t\t\t\t\t sizeof(struct virtio_gpu_ctrl_hdr),\n\t\t\t\t\t NULL);\n}\n\nstatic void free_vbuf(struct virtio_gpu_device *vgdev,\n\t\t      struct virtio_gpu_vbuffer *vbuf)\n{\n\tif (vbuf->resp_size > MAX_INLINE_RESP_SIZE)\n\t\tkfree(vbuf->resp_buf);\n\tkvfree(vbuf->data_buf);\n\tkmem_cache_free(vgdev->vbufs, vbuf);\n}\n\nstatic void reclaim_vbufs(struct virtqueue *vq, struct list_head *reclaim_list)\n{\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tunsigned int len;\n\tint freed = 0;\n\n\twhile ((vbuf = virtqueue_get_buf(vq, &len))) {\n\t\tlist_add_tail(&vbuf->list, reclaim_list);\n\t\tfreed++;\n\t}\n\tif (freed == 0)\n\t\tDRM_DEBUG(\"Huh? zero vbufs reclaimed\");\n}\n\nvoid virtio_gpu_dequeue_ctrl_func(struct work_struct *work)\n{\n\tstruct virtio_gpu_device *vgdev =\n\t\tcontainer_of(work, struct virtio_gpu_device,\n\t\t\t     ctrlq.dequeue_work);\n\tstruct list_head reclaim_list;\n\tstruct virtio_gpu_vbuffer *entry, *tmp;\n\tstruct virtio_gpu_ctrl_hdr *resp;\n\tu64 fence_id;\n\n\tINIT_LIST_HEAD(&reclaim_list);\n\tspin_lock(&vgdev->ctrlq.qlock);\n\tdo {\n\t\tvirtqueue_disable_cb(vgdev->ctrlq.vq);\n\t\treclaim_vbufs(vgdev->ctrlq.vq, &reclaim_list);\n\n\t} while (!virtqueue_enable_cb(vgdev->ctrlq.vq));\n\tspin_unlock(&vgdev->ctrlq.qlock);\n\n\tlist_for_each_entry(entry, &reclaim_list, list) {\n\t\tresp = (struct virtio_gpu_ctrl_hdr *)entry->resp_buf;\n\n\t\ttrace_virtio_gpu_cmd_response(vgdev->ctrlq.vq, resp, entry->seqno);\n\n\t\tif (resp->type != cpu_to_le32(VIRTIO_GPU_RESP_OK_NODATA)) {\n\t\t\tif (le32_to_cpu(resp->type) >= VIRTIO_GPU_RESP_ERR_UNSPEC) {\n\t\t\t\tstruct virtio_gpu_ctrl_hdr *cmd;\n\t\t\t\tcmd = virtio_gpu_vbuf_ctrl_hdr(entry);\n\t\t\t\tDRM_ERROR_RATELIMITED(\"response 0x%x (command 0x%x)\\n\",\n\t\t\t\t\t\t      le32_to_cpu(resp->type),\n\t\t\t\t\t\t      le32_to_cpu(cmd->type));\n\t\t\t} else\n\t\t\t\tDRM_DEBUG(\"response 0x%x\\n\", le32_to_cpu(resp->type));\n\t\t}\n\t\tif (resp->flags & cpu_to_le32(VIRTIO_GPU_FLAG_FENCE)) {\n\t\t\tfence_id = le64_to_cpu(resp->fence_id);\n\t\t\tvirtio_gpu_fence_event_process(vgdev, fence_id);\n\t\t}\n\t\tif (entry->resp_cb)\n\t\t\tentry->resp_cb(vgdev, entry);\n\t}\n\twake_up(&vgdev->ctrlq.ack_queue);\n\n\tlist_for_each_entry_safe(entry, tmp, &reclaim_list, list) {\n\t\tif (entry->objs)\n\t\t\tvirtio_gpu_array_put_free_delayed(vgdev, entry->objs);\n\t\tlist_del(&entry->list);\n\t\tfree_vbuf(vgdev, entry);\n\t}\n}\n\nvoid virtio_gpu_dequeue_cursor_func(struct work_struct *work)\n{\n\tstruct virtio_gpu_device *vgdev =\n\t\tcontainer_of(work, struct virtio_gpu_device,\n\t\t\t     cursorq.dequeue_work);\n\tstruct list_head reclaim_list;\n\tstruct virtio_gpu_vbuffer *entry, *tmp;\n\n\tINIT_LIST_HEAD(&reclaim_list);\n\tspin_lock(&vgdev->cursorq.qlock);\n\tdo {\n\t\tvirtqueue_disable_cb(vgdev->cursorq.vq);\n\t\treclaim_vbufs(vgdev->cursorq.vq, &reclaim_list);\n\t} while (!virtqueue_enable_cb(vgdev->cursorq.vq));\n\tspin_unlock(&vgdev->cursorq.qlock);\n\n\tlist_for_each_entry_safe(entry, tmp, &reclaim_list, list) {\n\t\tstruct virtio_gpu_ctrl_hdr *resp =\n\t\t\t(struct virtio_gpu_ctrl_hdr *)entry->resp_buf;\n\n\t\ttrace_virtio_gpu_cmd_response(vgdev->cursorq.vq, resp, entry->seqno);\n\t\tlist_del(&entry->list);\n\t\tfree_vbuf(vgdev, entry);\n\t}\n\twake_up(&vgdev->cursorq.ack_queue);\n}\n\n \nstatic struct sg_table *vmalloc_to_sgt(char *data, uint32_t size, int *sg_ents)\n{\n\tint ret, s, i;\n\tstruct sg_table *sgt;\n\tstruct scatterlist *sg;\n\tstruct page *pg;\n\n\tif (WARN_ON(!PAGE_ALIGNED(data)))\n\t\treturn NULL;\n\n\tsgt = kmalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt)\n\t\treturn NULL;\n\n\t*sg_ents = DIV_ROUND_UP(size, PAGE_SIZE);\n\tret = sg_alloc_table(sgt, *sg_ents, GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(sgt);\n\t\treturn NULL;\n\t}\n\n\tfor_each_sgtable_sg(sgt, sg, i) {\n\t\tpg = vmalloc_to_page(data);\n\t\tif (!pg) {\n\t\t\tsg_free_table(sgt);\n\t\t\tkfree(sgt);\n\t\t\treturn NULL;\n\t\t}\n\n\t\ts = min_t(int, PAGE_SIZE, size);\n\t\tsg_set_page(sg, pg, s, 0);\n\n\t\tsize -= s;\n\t\tdata += s;\n\t}\n\n\treturn sgt;\n}\n\nstatic int virtio_gpu_queue_ctrl_sgs(struct virtio_gpu_device *vgdev,\n\t\t\t\t     struct virtio_gpu_vbuffer *vbuf,\n\t\t\t\t     struct virtio_gpu_fence *fence,\n\t\t\t\t     int elemcnt,\n\t\t\t\t     struct scatterlist **sgs,\n\t\t\t\t     int outcnt,\n\t\t\t\t     int incnt)\n{\n\tstruct virtqueue *vq = vgdev->ctrlq.vq;\n\tint ret, idx;\n\n\tif (!drm_dev_enter(vgdev->ddev, &idx)) {\n\t\tif (fence && vbuf->objs)\n\t\t\tvirtio_gpu_array_unlock_resv(vbuf->objs);\n\t\tfree_vbuf(vgdev, vbuf);\n\t\treturn -ENODEV;\n\t}\n\n\tif (vgdev->has_indirect)\n\t\telemcnt = 1;\n\nagain:\n\tspin_lock(&vgdev->ctrlq.qlock);\n\n\tif (vq->num_free < elemcnt) {\n\t\tspin_unlock(&vgdev->ctrlq.qlock);\n\t\tvirtio_gpu_notify(vgdev);\n\t\twait_event(vgdev->ctrlq.ack_queue, vq->num_free >= elemcnt);\n\t\tgoto again;\n\t}\n\n\t \n\tif (fence) {\n\t\tvirtio_gpu_fence_emit(vgdev, virtio_gpu_vbuf_ctrl_hdr(vbuf),\n\t\t\t\t      fence);\n\t\tif (vbuf->objs) {\n\t\t\tvirtio_gpu_array_add_fence(vbuf->objs, &fence->f);\n\t\t\tvirtio_gpu_array_unlock_resv(vbuf->objs);\n\t\t}\n\t}\n\n\tret = virtqueue_add_sgs(vq, sgs, outcnt, incnt, vbuf, GFP_ATOMIC);\n\tWARN_ON(ret);\n\n\tvbuf->seqno = ++vgdev->ctrlq.seqno;\n\ttrace_virtio_gpu_cmd_queue(vq, virtio_gpu_vbuf_ctrl_hdr(vbuf), vbuf->seqno);\n\n\tatomic_inc(&vgdev->pending_commands);\n\n\tspin_unlock(&vgdev->ctrlq.qlock);\n\n\tdrm_dev_exit(idx);\n\treturn 0;\n}\n\nstatic int virtio_gpu_queue_fenced_ctrl_buffer(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t       struct virtio_gpu_vbuffer *vbuf,\n\t\t\t\t\t       struct virtio_gpu_fence *fence)\n{\n\tstruct scatterlist *sgs[3], vcmd, vout, vresp;\n\tstruct sg_table *sgt = NULL;\n\tint elemcnt = 0, outcnt = 0, incnt = 0, ret;\n\n\t \n\tsg_init_one(&vcmd, vbuf->buf, vbuf->size);\n\telemcnt++;\n\tsgs[outcnt] = &vcmd;\n\toutcnt++;\n\n\t \n\tif (vbuf->data_size) {\n\t\tif (is_vmalloc_addr(vbuf->data_buf)) {\n\t\t\tint sg_ents;\n\t\t\tsgt = vmalloc_to_sgt(vbuf->data_buf, vbuf->data_size,\n\t\t\t\t\t     &sg_ents);\n\t\t\tif (!sgt) {\n\t\t\t\tif (fence && vbuf->objs)\n\t\t\t\t\tvirtio_gpu_array_unlock_resv(vbuf->objs);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\n\t\t\telemcnt += sg_ents;\n\t\t\tsgs[outcnt] = sgt->sgl;\n\t\t} else {\n\t\t\tsg_init_one(&vout, vbuf->data_buf, vbuf->data_size);\n\t\t\telemcnt++;\n\t\t\tsgs[outcnt] = &vout;\n\t\t}\n\t\toutcnt++;\n\t}\n\n\t \n\tif (vbuf->resp_size) {\n\t\tsg_init_one(&vresp, vbuf->resp_buf, vbuf->resp_size);\n\t\telemcnt++;\n\t\tsgs[outcnt + incnt] = &vresp;\n\t\tincnt++;\n\t}\n\n\tret = virtio_gpu_queue_ctrl_sgs(vgdev, vbuf, fence, elemcnt, sgs, outcnt,\n\t\t\t\t\tincnt);\n\n\tif (sgt) {\n\t\tsg_free_table(sgt);\n\t\tkfree(sgt);\n\t}\n\treturn ret;\n}\n\nvoid virtio_gpu_notify(struct virtio_gpu_device *vgdev)\n{\n\tbool notify;\n\n\tif (!atomic_read(&vgdev->pending_commands))\n\t\treturn;\n\n\tspin_lock(&vgdev->ctrlq.qlock);\n\tatomic_set(&vgdev->pending_commands, 0);\n\tnotify = virtqueue_kick_prepare(vgdev->ctrlq.vq);\n\tspin_unlock(&vgdev->ctrlq.qlock);\n\n\tif (notify)\n\t\tvirtqueue_notify(vgdev->ctrlq.vq);\n}\n\nstatic int virtio_gpu_queue_ctrl_buffer(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tstruct virtio_gpu_vbuffer *vbuf)\n{\n\treturn virtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, NULL);\n}\n\nstatic void virtio_gpu_queue_cursor(struct virtio_gpu_device *vgdev,\n\t\t\t\t    struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtqueue *vq = vgdev->cursorq.vq;\n\tstruct scatterlist *sgs[1], ccmd;\n\tint idx, ret, outcnt;\n\tbool notify;\n\n\tif (!drm_dev_enter(vgdev->ddev, &idx)) {\n\t\tfree_vbuf(vgdev, vbuf);\n\t\treturn;\n\t}\n\n\tsg_init_one(&ccmd, vbuf->buf, vbuf->size);\n\tsgs[0] = &ccmd;\n\toutcnt = 1;\n\n\tspin_lock(&vgdev->cursorq.qlock);\nretry:\n\tret = virtqueue_add_sgs(vq, sgs, outcnt, 0, vbuf, GFP_ATOMIC);\n\tif (ret == -ENOSPC) {\n\t\tspin_unlock(&vgdev->cursorq.qlock);\n\t\twait_event(vgdev->cursorq.ack_queue, vq->num_free >= outcnt);\n\t\tspin_lock(&vgdev->cursorq.qlock);\n\t\tgoto retry;\n\t} else {\n\t\tvbuf->seqno = ++vgdev->cursorq.seqno;\n\t\ttrace_virtio_gpu_cmd_queue(vq,\n\t\t\tvirtio_gpu_vbuf_ctrl_hdr(vbuf),\n\t\t\tvbuf->seqno);\n\n\t\tnotify = virtqueue_kick_prepare(vq);\n\t}\n\n\tspin_unlock(&vgdev->cursorq.qlock);\n\n\tif (notify)\n\t\tvirtqueue_notify(vq);\n\n\tdrm_dev_exit(idx);\n}\n\n \n\n \nvoid virtio_gpu_cmd_create_resource(struct virtio_gpu_device *vgdev,\n\t\t\t\t    struct virtio_gpu_object *bo,\n\t\t\t\t    struct virtio_gpu_object_params *params,\n\t\t\t\t    struct virtio_gpu_object_array *objs,\n\t\t\t\t    struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_resource_create_2d *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_CREATE_2D);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->format = cpu_to_le32(params->format);\n\tcmd_p->width = cpu_to_le32(params->width);\n\tcmd_p->height = cpu_to_le32(params->height);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n\tbo->created = true;\n}\n\nstatic void virtio_gpu_cmd_unref_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t    struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_object *bo;\n\n\tbo = vbuf->resp_cb_data;\n\tvbuf->resp_cb_data = NULL;\n\n\tvirtio_gpu_cleanup_object(bo);\n}\n\nvoid virtio_gpu_cmd_unref_resource(struct virtio_gpu_device *vgdev,\n\t\t\t\t   struct virtio_gpu_object *bo)\n{\n\tstruct virtio_gpu_resource_unref *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tint ret;\n\n\tcmd_p = virtio_gpu_alloc_cmd_cb(vgdev, &vbuf, sizeof(*cmd_p),\n\t\t\t\t\tvirtio_gpu_cmd_unref_cb);\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_UNREF);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\n\tvbuf->resp_cb_data = bo;\n\tret = virtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\tif (ret < 0)\n\t\tvirtio_gpu_cleanup_object(bo);\n}\n\nvoid virtio_gpu_cmd_set_scanout(struct virtio_gpu_device *vgdev,\n\t\t\t\tuint32_t scanout_id, uint32_t resource_id,\n\t\t\t\tuint32_t width, uint32_t height,\n\t\t\t\tuint32_t x, uint32_t y)\n{\n\tstruct virtio_gpu_set_scanout *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_SET_SCANOUT);\n\tcmd_p->resource_id = cpu_to_le32(resource_id);\n\tcmd_p->scanout_id = cpu_to_le32(scanout_id);\n\tcmd_p->r.width = cpu_to_le32(width);\n\tcmd_p->r.height = cpu_to_le32(height);\n\tcmd_p->r.x = cpu_to_le32(x);\n\tcmd_p->r.y = cpu_to_le32(y);\n\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid virtio_gpu_cmd_resource_flush(struct virtio_gpu_device *vgdev,\n\t\t\t\t   uint32_t resource_id,\n\t\t\t\t   uint32_t x, uint32_t y,\n\t\t\t\t   uint32_t width, uint32_t height,\n\t\t\t\t   struct virtio_gpu_object_array *objs,\n\t\t\t\t   struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_resource_flush *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_FLUSH);\n\tcmd_p->resource_id = cpu_to_le32(resource_id);\n\tcmd_p->r.width = cpu_to_le32(width);\n\tcmd_p->r.height = cpu_to_le32(height);\n\tcmd_p->r.x = cpu_to_le32(x);\n\tcmd_p->r.y = cpu_to_le32(y);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nvoid virtio_gpu_cmd_transfer_to_host_2d(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tuint64_t offset,\n\t\t\t\t\tuint32_t width, uint32_t height,\n\t\t\t\t\tuint32_t x, uint32_t y,\n\t\t\t\t\tstruct virtio_gpu_object_array *objs,\n\t\t\t\t\tstruct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_transfer_to_host_2d *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\n\tif (virtio_gpu_is_shmem(bo) && use_dma_api)\n\t\tdma_sync_sgtable_for_device(vgdev->vdev->dev.parent,\n\t\t\t\t\t    bo->base.sgt, DMA_TO_DEVICE);\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_TO_HOST_2D);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->offset = cpu_to_le64(offset);\n\tcmd_p->r.width = cpu_to_le32(width);\n\tcmd_p->r.height = cpu_to_le32(height);\n\tcmd_p->r.x = cpu_to_le32(x);\n\tcmd_p->r.y = cpu_to_le32(y);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nstatic void\nvirtio_gpu_cmd_resource_attach_backing(struct virtio_gpu_device *vgdev,\n\t\t\t\t       uint32_t resource_id,\n\t\t\t\t       struct virtio_gpu_mem_entry *ents,\n\t\t\t\t       uint32_t nents,\n\t\t\t\t       struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_resource_attach_backing *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_ATTACH_BACKING);\n\tcmd_p->resource_id = cpu_to_le32(resource_id);\n\tcmd_p->nr_entries = cpu_to_le32(nents);\n\n\tvbuf->data_buf = ents;\n\tvbuf->data_size = sizeof(*ents) * nents;\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nstatic void virtio_gpu_cmd_get_display_info_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t       struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_resp_display_info *resp =\n\t\t(struct virtio_gpu_resp_display_info *)vbuf->resp_buf;\n\tint i;\n\n\tspin_lock(&vgdev->display_info_lock);\n\tfor (i = 0; i < vgdev->num_scanouts; i++) {\n\t\tvgdev->outputs[i].info = resp->pmodes[i];\n\t\tif (resp->pmodes[i].enabled) {\n\t\t\tDRM_DEBUG(\"output %d: %dx%d+%d+%d\", i,\n\t\t\t\t  le32_to_cpu(resp->pmodes[i].r.width),\n\t\t\t\t  le32_to_cpu(resp->pmodes[i].r.height),\n\t\t\t\t  le32_to_cpu(resp->pmodes[i].r.x),\n\t\t\t\t  le32_to_cpu(resp->pmodes[i].r.y));\n\t\t} else {\n\t\t\tDRM_DEBUG(\"output %d: disabled\", i);\n\t\t}\n\t}\n\n\tvgdev->display_info_pending = false;\n\tspin_unlock(&vgdev->display_info_lock);\n\twake_up(&vgdev->resp_wq);\n\n\tif (!drm_helper_hpd_irq_event(vgdev->ddev))\n\t\tdrm_kms_helper_hotplug_event(vgdev->ddev);\n}\n\nstatic void virtio_gpu_cmd_get_capset_info_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t      struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_get_capset_info *cmd =\n\t\t(struct virtio_gpu_get_capset_info *)vbuf->buf;\n\tstruct virtio_gpu_resp_capset_info *resp =\n\t\t(struct virtio_gpu_resp_capset_info *)vbuf->resp_buf;\n\tint i = le32_to_cpu(cmd->capset_index);\n\n\tspin_lock(&vgdev->display_info_lock);\n\tif (vgdev->capsets) {\n\t\tvgdev->capsets[i].id = le32_to_cpu(resp->capset_id);\n\t\tvgdev->capsets[i].max_version = le32_to_cpu(resp->capset_max_version);\n\t\tvgdev->capsets[i].max_size = le32_to_cpu(resp->capset_max_size);\n\t} else {\n\t\tDRM_ERROR(\"invalid capset memory.\");\n\t}\n\tspin_unlock(&vgdev->display_info_lock);\n\twake_up(&vgdev->resp_wq);\n}\n\nstatic void virtio_gpu_cmd_capset_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t     struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_get_capset *cmd =\n\t\t(struct virtio_gpu_get_capset *)vbuf->buf;\n\tstruct virtio_gpu_resp_capset *resp =\n\t\t(struct virtio_gpu_resp_capset *)vbuf->resp_buf;\n\tstruct virtio_gpu_drv_cap_cache *cache_ent;\n\n\tspin_lock(&vgdev->display_info_lock);\n\tlist_for_each_entry(cache_ent, &vgdev->cap_cache, head) {\n\t\tif (cache_ent->version == le32_to_cpu(cmd->capset_version) &&\n\t\t    cache_ent->id == le32_to_cpu(cmd->capset_id)) {\n\t\t\tmemcpy(cache_ent->caps_cache, resp->capset_data,\n\t\t\t       cache_ent->size);\n\t\t\t \n\t\t\tsmp_wmb();\n\t\t\tatomic_set(&cache_ent->is_valid, 1);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&vgdev->display_info_lock);\n\twake_up_all(&vgdev->resp_wq);\n}\n\nstatic int virtio_get_edid_block(void *data, u8 *buf,\n\t\t\t\t unsigned int block, size_t len)\n{\n\tstruct virtio_gpu_resp_edid *resp = data;\n\tsize_t start = block * EDID_LENGTH;\n\n\tif (start + len > le32_to_cpu(resp->size))\n\t\treturn -EINVAL;\n\tmemcpy(buf, resp->edid + start, len);\n\treturn 0;\n}\n\nstatic void virtio_gpu_cmd_get_edid_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t       struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_cmd_get_edid *cmd =\n\t\t(struct virtio_gpu_cmd_get_edid *)vbuf->buf;\n\tstruct virtio_gpu_resp_edid *resp =\n\t\t(struct virtio_gpu_resp_edid *)vbuf->resp_buf;\n\tuint32_t scanout = le32_to_cpu(cmd->scanout);\n\tstruct virtio_gpu_output *output;\n\tstruct edid *new_edid, *old_edid;\n\n\tif (scanout >= vgdev->num_scanouts)\n\t\treturn;\n\toutput = vgdev->outputs + scanout;\n\n\tnew_edid = drm_do_get_edid(&output->conn, virtio_get_edid_block, resp);\n\tdrm_connector_update_edid_property(&output->conn, new_edid);\n\n\tspin_lock(&vgdev->display_info_lock);\n\told_edid = output->edid;\n\toutput->edid = new_edid;\n\tspin_unlock(&vgdev->display_info_lock);\n\n\tkfree(old_edid);\n\twake_up(&vgdev->resp_wq);\n}\n\nint virtio_gpu_cmd_get_display_info(struct virtio_gpu_device *vgdev)\n{\n\tstruct virtio_gpu_ctrl_hdr *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tvoid *resp_buf;\n\n\tresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_display_info),\n\t\t\t   GFP_KERNEL);\n\tif (!resp_buf)\n\t\treturn -ENOMEM;\n\n\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t(vgdev, &virtio_gpu_cmd_get_display_info_cb, &vbuf,\n\t\t sizeof(*cmd_p), sizeof(struct virtio_gpu_resp_display_info),\n\t\t resp_buf);\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tvgdev->display_info_pending = true;\n\tcmd_p->type = cpu_to_le32(VIRTIO_GPU_CMD_GET_DISPLAY_INFO);\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\treturn 0;\n}\n\nint virtio_gpu_cmd_get_capset_info(struct virtio_gpu_device *vgdev, int idx)\n{\n\tstruct virtio_gpu_get_capset_info *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tvoid *resp_buf;\n\n\tresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_capset_info),\n\t\t\t   GFP_KERNEL);\n\tif (!resp_buf)\n\t\treturn -ENOMEM;\n\n\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t(vgdev, &virtio_gpu_cmd_get_capset_info_cb, &vbuf,\n\t\t sizeof(*cmd_p), sizeof(struct virtio_gpu_resp_capset_info),\n\t\t resp_buf);\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_GET_CAPSET_INFO);\n\tcmd_p->capset_index = cpu_to_le32(idx);\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\treturn 0;\n}\n\nint virtio_gpu_cmd_get_capset(struct virtio_gpu_device *vgdev,\n\t\t\t      int idx, int version,\n\t\t\t      struct virtio_gpu_drv_cap_cache **cache_p)\n{\n\tstruct virtio_gpu_get_capset *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tint max_size;\n\tstruct virtio_gpu_drv_cap_cache *cache_ent;\n\tstruct virtio_gpu_drv_cap_cache *search_ent;\n\tvoid *resp_buf;\n\n\t*cache_p = NULL;\n\n\tif (idx >= vgdev->num_capsets)\n\t\treturn -EINVAL;\n\n\tif (version > vgdev->capsets[idx].max_version)\n\t\treturn -EINVAL;\n\n\tcache_ent = kzalloc(sizeof(*cache_ent), GFP_KERNEL);\n\tif (!cache_ent)\n\t\treturn -ENOMEM;\n\n\tmax_size = vgdev->capsets[idx].max_size;\n\tcache_ent->caps_cache = kmalloc(max_size, GFP_KERNEL);\n\tif (!cache_ent->caps_cache) {\n\t\tkfree(cache_ent);\n\t\treturn -ENOMEM;\n\t}\n\n\tresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_capset) + max_size,\n\t\t\t   GFP_KERNEL);\n\tif (!resp_buf) {\n\t\tkfree(cache_ent->caps_cache);\n\t\tkfree(cache_ent);\n\t\treturn -ENOMEM;\n\t}\n\n\tcache_ent->version = version;\n\tcache_ent->id = vgdev->capsets[idx].id;\n\tatomic_set(&cache_ent->is_valid, 0);\n\tcache_ent->size = max_size;\n\tspin_lock(&vgdev->display_info_lock);\n\t \n\tlist_for_each_entry(search_ent, &vgdev->cap_cache, head) {\n\t\tif (search_ent->id == vgdev->capsets[idx].id &&\n\t\t    search_ent->version == version) {\n\t\t\t*cache_p = search_ent;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!*cache_p)\n\t\tlist_add_tail(&cache_ent->head, &vgdev->cap_cache);\n\tspin_unlock(&vgdev->display_info_lock);\n\n\tif (*cache_p) {\n\t\t \n\t\tkfree(resp_buf);\n\t\tkfree(cache_ent->caps_cache);\n\t\tkfree(cache_ent);\n\t\treturn 0;\n\t}\n\n\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t(vgdev, &virtio_gpu_cmd_capset_cb, &vbuf, sizeof(*cmd_p),\n\t\t sizeof(struct virtio_gpu_resp_capset) + max_size,\n\t\t resp_buf);\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_GET_CAPSET);\n\tcmd_p->capset_id = cpu_to_le32(vgdev->capsets[idx].id);\n\tcmd_p->capset_version = cpu_to_le32(version);\n\t*cache_p = cache_ent;\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\n\treturn 0;\n}\n\nint virtio_gpu_cmd_get_edids(struct virtio_gpu_device *vgdev)\n{\n\tstruct virtio_gpu_cmd_get_edid *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tvoid *resp_buf;\n\tint scanout;\n\n\tif (WARN_ON(!vgdev->has_edid))\n\t\treturn -EINVAL;\n\n\tfor (scanout = 0; scanout < vgdev->num_scanouts; scanout++) {\n\t\tresp_buf = kzalloc(sizeof(struct virtio_gpu_resp_edid),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!resp_buf)\n\t\t\treturn -ENOMEM;\n\n\t\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t\t(vgdev, &virtio_gpu_cmd_get_edid_cb, &vbuf,\n\t\t\t sizeof(*cmd_p), sizeof(struct virtio_gpu_resp_edid),\n\t\t\t resp_buf);\n\t\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_GET_EDID);\n\t\tcmd_p->scanout = cpu_to_le32(scanout);\n\t\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\t}\n\n\treturn 0;\n}\n\nvoid virtio_gpu_cmd_context_create(struct virtio_gpu_device *vgdev, uint32_t id,\n\t\t\t\t   uint32_t context_init, uint32_t nlen,\n\t\t\t\t   const char *name)\n{\n\tstruct virtio_gpu_ctx_create *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_CREATE);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(id);\n\tcmd_p->nlen = cpu_to_le32(nlen);\n\tcmd_p->context_init = cpu_to_le32(context_init);\n\tstrscpy(cmd_p->debug_name, name, sizeof(cmd_p->debug_name));\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid virtio_gpu_cmd_context_destroy(struct virtio_gpu_device *vgdev,\n\t\t\t\t    uint32_t id)\n{\n\tstruct virtio_gpu_ctx_destroy *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_DESTROY);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(id);\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid virtio_gpu_cmd_context_attach_resource(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t    uint32_t ctx_id,\n\t\t\t\t\t    struct virtio_gpu_object_array *objs)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_ctx_resource *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_ATTACH_RESOURCE);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid virtio_gpu_cmd_context_detach_resource(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t    uint32_t ctx_id,\n\t\t\t\t\t    struct virtio_gpu_object_array *objs)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_ctx_resource *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_CTX_DETACH_RESOURCE);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid\nvirtio_gpu_cmd_resource_create_3d(struct virtio_gpu_device *vgdev,\n\t\t\t\t  struct virtio_gpu_object *bo,\n\t\t\t\t  struct virtio_gpu_object_params *params,\n\t\t\t\t  struct virtio_gpu_object_array *objs,\n\t\t\t\t  struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_resource_create_3d *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_CREATE_3D);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->format = cpu_to_le32(params->format);\n\tcmd_p->width = cpu_to_le32(params->width);\n\tcmd_p->height = cpu_to_le32(params->height);\n\n\tcmd_p->target = cpu_to_le32(params->target);\n\tcmd_p->bind = cpu_to_le32(params->bind);\n\tcmd_p->depth = cpu_to_le32(params->depth);\n\tcmd_p->array_size = cpu_to_le32(params->array_size);\n\tcmd_p->last_level = cpu_to_le32(params->last_level);\n\tcmd_p->nr_samples = cpu_to_le32(params->nr_samples);\n\tcmd_p->flags = cpu_to_le32(params->flags);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n\n\tbo->created = true;\n}\n\nvoid virtio_gpu_cmd_transfer_to_host_3d(struct virtio_gpu_device *vgdev,\n\t\t\t\t\tuint32_t ctx_id,\n\t\t\t\t\tuint64_t offset, uint32_t level,\n\t\t\t\t\tuint32_t stride,\n\t\t\t\t\tuint32_t layer_stride,\n\t\t\t\t\tstruct drm_virtgpu_3d_box *box,\n\t\t\t\t\tstruct virtio_gpu_object_array *objs,\n\t\t\t\t\tstruct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_transfer_host_3d *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tbool use_dma_api = !virtio_has_dma_quirk(vgdev->vdev);\n\n\tif (virtio_gpu_is_shmem(bo) && use_dma_api)\n\t\tdma_sync_sgtable_for_device(vgdev->vdev->dev.parent,\n\t\t\t\t\t    bo->base.sgt, DMA_TO_DEVICE);\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_TO_HOST_3D);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tconvert_to_hw_box(&cmd_p->box, box);\n\tcmd_p->offset = cpu_to_le64(offset);\n\tcmd_p->level = cpu_to_le32(level);\n\tcmd_p->stride = cpu_to_le32(stride);\n\tcmd_p->layer_stride = cpu_to_le32(layer_stride);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nvoid virtio_gpu_cmd_transfer_from_host_3d(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t  uint32_t ctx_id,\n\t\t\t\t\t  uint64_t offset, uint32_t level,\n\t\t\t\t\t  uint32_t stride,\n\t\t\t\t\t  uint32_t layer_stride,\n\t\t\t\t\t  struct drm_virtgpu_3d_box *box,\n\t\t\t\t\t  struct virtio_gpu_object_array *objs,\n\t\t\t\t\t  struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_transfer_host_3d *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_TRANSFER_FROM_HOST_3D);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tconvert_to_hw_box(&cmd_p->box, box);\n\tcmd_p->offset = cpu_to_le64(offset);\n\tcmd_p->level = cpu_to_le32(level);\n\tcmd_p->stride = cpu_to_le32(stride);\n\tcmd_p->layer_stride = cpu_to_le32(layer_stride);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nvoid virtio_gpu_cmd_submit(struct virtio_gpu_device *vgdev,\n\t\t\t   void *data, uint32_t data_size,\n\t\t\t   uint32_t ctx_id,\n\t\t\t   struct virtio_gpu_object_array *objs,\n\t\t\t   struct virtio_gpu_fence *fence)\n{\n\tstruct virtio_gpu_cmd_submit *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tvbuf->data_buf = data;\n\tvbuf->data_size = data_size;\n\tvbuf->objs = objs;\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_SUBMIT_3D);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(ctx_id);\n\tcmd_p->size = cpu_to_le32(data_size);\n\n\tvirtio_gpu_queue_fenced_ctrl_buffer(vgdev, vbuf, fence);\n}\n\nvoid virtio_gpu_object_attach(struct virtio_gpu_device *vgdev,\n\t\t\t      struct virtio_gpu_object *obj,\n\t\t\t      struct virtio_gpu_mem_entry *ents,\n\t\t\t      unsigned int nents)\n{\n\tvirtio_gpu_cmd_resource_attach_backing(vgdev, obj->hw_res_handle,\n\t\t\t\t\t       ents, nents, NULL);\n}\n\nvoid virtio_gpu_cursor_ping(struct virtio_gpu_device *vgdev,\n\t\t\t    struct virtio_gpu_output *output)\n{\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tstruct virtio_gpu_update_cursor *cur_p;\n\n\toutput->cursor.pos.scanout_id = cpu_to_le32(output->index);\n\tcur_p = virtio_gpu_alloc_cursor(vgdev, &vbuf);\n\tmemcpy(cur_p, &output->cursor, sizeof(output->cursor));\n\tvirtio_gpu_queue_cursor(vgdev, vbuf);\n}\n\nstatic void virtio_gpu_cmd_resource_uuid_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t    struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_object *obj =\n\t\tgem_to_virtio_gpu_obj(vbuf->objs->objs[0]);\n\tstruct virtio_gpu_resp_resource_uuid *resp =\n\t\t(struct virtio_gpu_resp_resource_uuid *)vbuf->resp_buf;\n\tuint32_t resp_type = le32_to_cpu(resp->hdr.type);\n\n\tspin_lock(&vgdev->resource_export_lock);\n\tWARN_ON(obj->uuid_state != STATE_INITIALIZING);\n\n\tif (resp_type == VIRTIO_GPU_RESP_OK_RESOURCE_UUID &&\n\t    obj->uuid_state == STATE_INITIALIZING) {\n\t\timport_uuid(&obj->uuid, resp->uuid);\n\t\tobj->uuid_state = STATE_OK;\n\t} else {\n\t\tobj->uuid_state = STATE_ERR;\n\t}\n\tspin_unlock(&vgdev->resource_export_lock);\n\n\twake_up_all(&vgdev->resp_wq);\n}\n\nint\nvirtio_gpu_cmd_resource_assign_uuid(struct virtio_gpu_device *vgdev,\n\t\t\t\t    struct virtio_gpu_object_array *objs)\n{\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_resource_assign_uuid *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tstruct virtio_gpu_resp_resource_uuid *resp_buf;\n\n\tresp_buf = kzalloc(sizeof(*resp_buf), GFP_KERNEL);\n\tif (!resp_buf) {\n\t\tspin_lock(&vgdev->resource_export_lock);\n\t\tbo->uuid_state = STATE_ERR;\n\t\tspin_unlock(&vgdev->resource_export_lock);\n\t\tvirtio_gpu_array_put_free(objs);\n\t\treturn -ENOMEM;\n\t}\n\n\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t(vgdev, virtio_gpu_cmd_resource_uuid_cb, &vbuf, sizeof(*cmd_p),\n\t\t sizeof(struct virtio_gpu_resp_resource_uuid), resp_buf);\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_ASSIGN_UUID);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\n\tvbuf->objs = objs;\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\treturn 0;\n}\n\nstatic void virtio_gpu_cmd_resource_map_cb(struct virtio_gpu_device *vgdev,\n\t\t\t\t\t   struct virtio_gpu_vbuffer *vbuf)\n{\n\tstruct virtio_gpu_object *bo =\n\t\tgem_to_virtio_gpu_obj(vbuf->objs->objs[0]);\n\tstruct virtio_gpu_resp_map_info *resp =\n\t\t(struct virtio_gpu_resp_map_info *)vbuf->resp_buf;\n\tstruct virtio_gpu_object_vram *vram = to_virtio_gpu_vram(bo);\n\tuint32_t resp_type = le32_to_cpu(resp->hdr.type);\n\n\tspin_lock(&vgdev->host_visible_lock);\n\n\tif (resp_type == VIRTIO_GPU_RESP_OK_MAP_INFO) {\n\t\tvram->map_info = resp->map_info;\n\t\tvram->map_state = STATE_OK;\n\t} else {\n\t\tvram->map_state = STATE_ERR;\n\t}\n\n\tspin_unlock(&vgdev->host_visible_lock);\n\twake_up_all(&vgdev->resp_wq);\n}\n\nint virtio_gpu_cmd_map(struct virtio_gpu_device *vgdev,\n\t\t       struct virtio_gpu_object_array *objs, uint64_t offset)\n{\n\tstruct virtio_gpu_resource_map_blob *cmd_p;\n\tstruct virtio_gpu_object *bo = gem_to_virtio_gpu_obj(objs->objs[0]);\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tstruct virtio_gpu_resp_map_info *resp_buf;\n\n\tresp_buf = kzalloc(sizeof(*resp_buf), GFP_KERNEL);\n\tif (!resp_buf)\n\t\treturn -ENOMEM;\n\n\tcmd_p = virtio_gpu_alloc_cmd_resp\n\t\t(vgdev, virtio_gpu_cmd_resource_map_cb, &vbuf, sizeof(*cmd_p),\n\t\t sizeof(struct virtio_gpu_resp_map_info), resp_buf);\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_MAP_BLOB);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->offset = cpu_to_le64(offset);\n\tvbuf->objs = objs;\n\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\treturn 0;\n}\n\nvoid virtio_gpu_cmd_unmap(struct virtio_gpu_device *vgdev,\n\t\t\t  struct virtio_gpu_object *bo)\n{\n\tstruct virtio_gpu_resource_unmap_blob *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_UNMAP_BLOB);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n\nvoid\nvirtio_gpu_cmd_resource_create_blob(struct virtio_gpu_device *vgdev,\n\t\t\t\t    struct virtio_gpu_object *bo,\n\t\t\t\t    struct virtio_gpu_object_params *params,\n\t\t\t\t    struct virtio_gpu_mem_entry *ents,\n\t\t\t\t    uint32_t nents)\n{\n\tstruct virtio_gpu_resource_create_blob *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_RESOURCE_CREATE_BLOB);\n\tcmd_p->hdr.ctx_id = cpu_to_le32(params->ctx_id);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->blob_mem = cpu_to_le32(params->blob_mem);\n\tcmd_p->blob_flags = cpu_to_le32(params->blob_flags);\n\tcmd_p->blob_id = cpu_to_le64(params->blob_id);\n\tcmd_p->size = cpu_to_le64(params->size);\n\tcmd_p->nr_entries = cpu_to_le32(nents);\n\n\tvbuf->data_buf = ents;\n\tvbuf->data_size = sizeof(*ents) * nents;\n\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n\tbo->created = true;\n}\n\nvoid virtio_gpu_cmd_set_scanout_blob(struct virtio_gpu_device *vgdev,\n\t\t\t\t     uint32_t scanout_id,\n\t\t\t\t     struct virtio_gpu_object *bo,\n\t\t\t\t     struct drm_framebuffer *fb,\n\t\t\t\t     uint32_t width, uint32_t height,\n\t\t\t\t     uint32_t x, uint32_t y)\n{\n\tuint32_t i;\n\tstruct virtio_gpu_set_scanout_blob *cmd_p;\n\tstruct virtio_gpu_vbuffer *vbuf;\n\tuint32_t format = virtio_gpu_translate_format(fb->format->format);\n\n\tcmd_p = virtio_gpu_alloc_cmd(vgdev, &vbuf, sizeof(*cmd_p));\n\tmemset(cmd_p, 0, sizeof(*cmd_p));\n\n\tcmd_p->hdr.type = cpu_to_le32(VIRTIO_GPU_CMD_SET_SCANOUT_BLOB);\n\tcmd_p->resource_id = cpu_to_le32(bo->hw_res_handle);\n\tcmd_p->scanout_id = cpu_to_le32(scanout_id);\n\n\tcmd_p->format = cpu_to_le32(format);\n\tcmd_p->width  = cpu_to_le32(fb->width);\n\tcmd_p->height = cpu_to_le32(fb->height);\n\n\tfor (i = 0; i < 4; i++) {\n\t\tcmd_p->strides[i] = cpu_to_le32(fb->pitches[i]);\n\t\tcmd_p->offsets[i] = cpu_to_le32(fb->offsets[i]);\n\t}\n\n\tcmd_p->r.width = cpu_to_le32(width);\n\tcmd_p->r.height = cpu_to_le32(height);\n\tcmd_p->r.x = cpu_to_le32(x);\n\tcmd_p->r.y = cpu_to_le32(y);\n\n\tvirtio_gpu_queue_ctrl_buffer(vgdev, vbuf);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}