{
  "module_name": "xpc_uv.c",
  "hash_id": "481fcc8ae69c1d3c376655749afab4a8056ac68a7e1d2151b48f6efcc813a147",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-xp/xpc_uv.c",
  "human_readable_source": " \n\n \n\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/cpu.h>\n#include <linux/module.h>\n#include <linux/err.h>\n#include <linux/slab.h>\n#include <linux/numa.h>\n#include <asm/uv/uv_hub.h>\n#if defined CONFIG_X86_64\n#include <asm/uv/bios.h>\n#include <asm/uv/uv_irq.h>\n#elif defined CONFIG_IA64_SGI_UV\n#include <asm/sn/intr.h>\n#include <asm/sn/sn_sal.h>\n#endif\n#include \"../sgi-gru/gru.h\"\n#include \"../sgi-gru/grukservices.h\"\n#include \"xpc.h\"\n\n#if defined CONFIG_IA64_SGI_UV\nstruct uv_IO_APIC_route_entry {\n\t__u64\tvector\t\t:  8,\n\t\tdelivery_mode\t:  3,\n\t\tdest_mode\t:  1,\n\t\tdelivery_status\t:  1,\n\t\tpolarity\t:  1,\n\t\t__reserved_1\t:  1,\n\t\ttrigger\t\t:  1,\n\t\tmask\t\t:  1,\n\t\t__reserved_2\t: 15,\n\t\tdest\t\t: 32;\n};\n\n#define sn_partition_id 0\n#endif\n\nstatic struct xpc_heartbeat_uv *xpc_heartbeat_uv;\n\n#define XPC_ACTIVATE_MSG_SIZE_UV\t(1 * GRU_CACHE_LINE_BYTES)\n#define XPC_ACTIVATE_MQ_SIZE_UV\t\t(4 * XP_MAX_NPARTITIONS_UV * \\\n\t\t\t\t\t XPC_ACTIVATE_MSG_SIZE_UV)\n#define XPC_ACTIVATE_IRQ_NAME\t\t\"xpc_activate\"\n\n#define XPC_NOTIFY_MSG_SIZE_UV\t\t(2 * GRU_CACHE_LINE_BYTES)\n#define XPC_NOTIFY_MQ_SIZE_UV\t\t(4 * XP_MAX_NPARTITIONS_UV * \\\n\t\t\t\t\t XPC_NOTIFY_MSG_SIZE_UV)\n#define XPC_NOTIFY_IRQ_NAME\t\t\"xpc_notify\"\n\nstatic int xpc_mq_node = NUMA_NO_NODE;\n\nstatic struct xpc_gru_mq_uv *xpc_activate_mq_uv;\nstatic struct xpc_gru_mq_uv *xpc_notify_mq_uv;\n\nstatic int\nxpc_setup_partitions_uv(void)\n{\n\tshort partid;\n\tstruct xpc_partition_uv *part_uv;\n\n\tfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\n\t\tpart_uv = &xpc_partitions[partid].sn.uv;\n\n\t\tmutex_init(&part_uv->cached_activate_gru_mq_desc_mutex);\n\t\tspin_lock_init(&part_uv->flags_lock);\n\t\tpart_uv->remote_act_state = XPC_P_AS_INACTIVE;\n\t}\n\treturn 0;\n}\n\nstatic void\nxpc_teardown_partitions_uv(void)\n{\n\tshort partid;\n\tstruct xpc_partition_uv *part_uv;\n\tunsigned long irq_flags;\n\n\tfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\n\t\tpart_uv = &xpc_partitions[partid].sn.uv;\n\n\t\tif (part_uv->cached_activate_gru_mq_desc != NULL) {\n\t\t\tmutex_lock(&part_uv->cached_activate_gru_mq_desc_mutex);\n\t\t\tspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\n\t\t\tpart_uv->flags &= ~XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\n\t\t\tspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\n\t\t\tkfree(part_uv->cached_activate_gru_mq_desc);\n\t\t\tpart_uv->cached_activate_gru_mq_desc = NULL;\n\t\t\tmutex_unlock(&part_uv->\n\t\t\t\t     cached_activate_gru_mq_desc_mutex);\n\t\t}\n\t}\n}\n\nstatic int\nxpc_get_gru_mq_irq_uv(struct xpc_gru_mq_uv *mq, int cpu, char *irq_name)\n{\n\tint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\n\n#if defined CONFIG_X86_64\n\tmq->irq = uv_setup_irq(irq_name, cpu, mq->mmr_blade, mq->mmr_offset,\n\t\t\tUV_AFFINITY_CPU);\n\tif (mq->irq < 0)\n\t\treturn mq->irq;\n\n\tmq->mmr_value = uv_read_global_mmr64(mmr_pnode, mq->mmr_offset);\n\n#elif defined CONFIG_IA64_SGI_UV\n\tif (strcmp(irq_name, XPC_ACTIVATE_IRQ_NAME) == 0)\n\t\tmq->irq = SGI_XPC_ACTIVATE;\n\telse if (strcmp(irq_name, XPC_NOTIFY_IRQ_NAME) == 0)\n\t\tmq->irq = SGI_XPC_NOTIFY;\n\telse\n\t\treturn -EINVAL;\n\n\tmq->mmr_value = (unsigned long)cpu_physical_id(cpu) << 32 | mq->irq;\n\tuv_write_global_mmr64(mmr_pnode, mq->mmr_offset, mq->mmr_value);\n#else\n\t#error not a supported configuration\n#endif\n\n\treturn 0;\n}\n\nstatic void\nxpc_release_gru_mq_irq_uv(struct xpc_gru_mq_uv *mq)\n{\n#if defined CONFIG_X86_64\n\tuv_teardown_irq(mq->irq);\n\n#elif defined CONFIG_IA64_SGI_UV\n\tint mmr_pnode;\n\tunsigned long mmr_value;\n\n\tmmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\n\tmmr_value = 1UL << 16;\n\n\tuv_write_global_mmr64(mmr_pnode, mq->mmr_offset, mmr_value);\n#else\n\t#error not a supported configuration\n#endif\n}\n\nstatic int\nxpc_gru_mq_watchlist_alloc_uv(struct xpc_gru_mq_uv *mq)\n{\n\tint ret;\n\n#if defined CONFIG_IA64_SGI_UV\n\tint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\n\n\tret = sn_mq_watchlist_alloc(mmr_pnode, (void *)uv_gpa(mq->address),\n\t\t\t\t    mq->order, &mq->mmr_offset);\n\tif (ret < 0) {\n\t\tdev_err(xpc_part, \"sn_mq_watchlist_alloc() failed, ret=%d\\n\",\n\t\t\tret);\n\t\treturn -EBUSY;\n\t}\n#elif defined CONFIG_X86_64\n\tret = uv_bios_mq_watchlist_alloc(uv_gpa(mq->address),\n\t\t\t\t\t mq->order, &mq->mmr_offset);\n\tif (ret < 0) {\n\t\tdev_err(xpc_part, \"uv_bios_mq_watchlist_alloc() failed, \"\n\t\t\t\"ret=%d\\n\", ret);\n\t\treturn ret;\n\t}\n#else\n\t#error not a supported configuration\n#endif\n\n\tmq->watchlist_num = ret;\n\treturn 0;\n}\n\nstatic void\nxpc_gru_mq_watchlist_free_uv(struct xpc_gru_mq_uv *mq)\n{\n\tint ret;\n\tint mmr_pnode = uv_blade_to_pnode(mq->mmr_blade);\n\n#if defined CONFIG_X86_64\n\tret = uv_bios_mq_watchlist_free(mmr_pnode, mq->watchlist_num);\n\tBUG_ON(ret != BIOS_STATUS_SUCCESS);\n#elif defined CONFIG_IA64_SGI_UV\n\tret = sn_mq_watchlist_free(mmr_pnode, mq->watchlist_num);\n\tBUG_ON(ret != SALRET_OK);\n#else\n\t#error not a supported configuration\n#endif\n}\n\nstatic struct xpc_gru_mq_uv *\nxpc_create_gru_mq_uv(unsigned int mq_size, int cpu, char *irq_name,\n\t\t     irq_handler_t irq_handler)\n{\n\tenum xp_retval xp_ret;\n\tint ret;\n\tint nid;\n\tint nasid;\n\tint pg_order;\n\tstruct page *page;\n\tstruct xpc_gru_mq_uv *mq;\n\tstruct uv_IO_APIC_route_entry *mmr_value;\n\n\tmq = kmalloc(sizeof(struct xpc_gru_mq_uv), GFP_KERNEL);\n\tif (mq == NULL) {\n\t\tdev_err(xpc_part, \"xpc_create_gru_mq_uv() failed to kmalloc() \"\n\t\t\t\"a xpc_gru_mq_uv structure\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto out_0;\n\t}\n\n\tmq->gru_mq_desc = kzalloc(sizeof(struct gru_message_queue_desc),\n\t\t\t\t  GFP_KERNEL);\n\tif (mq->gru_mq_desc == NULL) {\n\t\tdev_err(xpc_part, \"xpc_create_gru_mq_uv() failed to kmalloc() \"\n\t\t\t\"a gru_message_queue_desc structure\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto out_1;\n\t}\n\n\tpg_order = get_order(mq_size);\n\tmq->order = pg_order + PAGE_SHIFT;\n\tmq_size = 1UL << mq->order;\n\n\tmq->mmr_blade = uv_cpu_to_blade_id(cpu);\n\n\tnid = cpu_to_node(cpu);\n\tpage = __alloc_pages_node(nid,\n\t\t\t\t      GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,\n\t\t\t\t      pg_order);\n\tif (page == NULL) {\n\t\tdev_err(xpc_part, \"xpc_create_gru_mq_uv() failed to alloc %d \"\n\t\t\t\"bytes of memory on nid=%d for GRU mq\\n\", mq_size, nid);\n\t\tret = -ENOMEM;\n\t\tgoto out_2;\n\t}\n\tmq->address = page_address(page);\n\n\t \n\tret = xpc_gru_mq_watchlist_alloc_uv(mq);\n\tif (ret != 0)\n\t\tgoto out_3;\n\n\tret = xpc_get_gru_mq_irq_uv(mq, cpu, irq_name);\n\tif (ret != 0)\n\t\tgoto out_4;\n\n\tret = request_irq(mq->irq, irq_handler, 0, irq_name, NULL);\n\tif (ret != 0) {\n\t\tdev_err(xpc_part, \"request_irq(irq=%d) returned error=%d\\n\",\n\t\t\tmq->irq, -ret);\n\t\tgoto out_5;\n\t}\n\n\tnasid = UV_PNODE_TO_NASID(uv_cpu_to_pnode(cpu));\n\n\tmmr_value = (struct uv_IO_APIC_route_entry *)&mq->mmr_value;\n\tret = gru_create_message_queue(mq->gru_mq_desc, mq->address, mq_size,\n\t\t\t\t     nasid, mmr_value->vector, mmr_value->dest);\n\tif (ret != 0) {\n\t\tdev_err(xpc_part, \"gru_create_message_queue() returned \"\n\t\t\t\"error=%d\\n\", ret);\n\t\tret = -EINVAL;\n\t\tgoto out_6;\n\t}\n\n\t \n\txp_ret = xp_expand_memprotect(xp_pa(mq->address), mq_size);\n\tif (xp_ret != xpSuccess) {\n\t\tret = -EACCES;\n\t\tgoto out_6;\n\t}\n\n\treturn mq;\n\n\t \nout_6:\n\tfree_irq(mq->irq, NULL);\nout_5:\n\txpc_release_gru_mq_irq_uv(mq);\nout_4:\n\txpc_gru_mq_watchlist_free_uv(mq);\nout_3:\n\tfree_pages((unsigned long)mq->address, pg_order);\nout_2:\n\tkfree(mq->gru_mq_desc);\nout_1:\n\tkfree(mq);\nout_0:\n\treturn ERR_PTR(ret);\n}\n\nstatic void\nxpc_destroy_gru_mq_uv(struct xpc_gru_mq_uv *mq)\n{\n\tunsigned int mq_size;\n\tint pg_order;\n\tint ret;\n\n\t \n\tmq_size = 1UL << mq->order;\n\tret = xp_restrict_memprotect(xp_pa(mq->address), mq_size);\n\tBUG_ON(ret != xpSuccess);\n\n\t \n\tfree_irq(mq->irq, NULL);\n\txpc_release_gru_mq_irq_uv(mq);\n\n\t \n\txpc_gru_mq_watchlist_free_uv(mq);\n\n\tpg_order = mq->order - PAGE_SHIFT;\n\tfree_pages((unsigned long)mq->address, pg_order);\n\n\tkfree(mq);\n}\n\nstatic enum xp_retval\nxpc_send_gru_msg(struct gru_message_queue_desc *gru_mq_desc, void *msg,\n\t\t size_t msg_size)\n{\n\tenum xp_retval xp_ret;\n\tint ret;\n\n\twhile (1) {\n\t\tret = gru_send_message_gpa(gru_mq_desc, msg, msg_size);\n\t\tif (ret == MQE_OK) {\n\t\t\txp_ret = xpSuccess;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret == MQE_QUEUE_FULL) {\n\t\t\tdev_dbg(xpc_chan, \"gru_send_message_gpa() returned \"\n\t\t\t\t\"error=MQE_QUEUE_FULL\\n\");\n\t\t\t \n\t\t\t \n\t\t\t(void)msleep_interruptible(10);\n\t\t} else if (ret == MQE_CONGESTION) {\n\t\t\tdev_dbg(xpc_chan, \"gru_send_message_gpa() returned \"\n\t\t\t\t\"error=MQE_CONGESTION\\n\");\n\t\t\t \n\t\t\t \n\t\t} else {\n\t\t\t \n\t\t\tdev_err(xpc_chan, \"gru_send_message_gpa() returned \"\n\t\t\t\t\"error=%d\\n\", ret);\n\t\t\txp_ret = xpGruSendMqError;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn xp_ret;\n}\n\nstatic void\nxpc_process_activate_IRQ_rcvd_uv(void)\n{\n\tunsigned long irq_flags;\n\tshort partid;\n\tstruct xpc_partition *part;\n\tu8 act_state_req;\n\n\tDBUG_ON(xpc_activate_IRQ_rcvd == 0);\n\n\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\tfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\n\t\tpart = &xpc_partitions[partid];\n\n\t\tif (part->sn.uv.act_state_req == 0)\n\t\t\tcontinue;\n\n\t\txpc_activate_IRQ_rcvd--;\n\t\tBUG_ON(xpc_activate_IRQ_rcvd < 0);\n\n\t\tact_state_req = part->sn.uv.act_state_req;\n\t\tpart->sn.uv.act_state_req = 0;\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\tif (act_state_req == XPC_P_ASR_ACTIVATE_UV) {\n\t\t\tif (part->act_state == XPC_P_AS_INACTIVE)\n\t\t\t\txpc_activate_partition(part);\n\t\t\telse if (part->act_state == XPC_P_AS_DEACTIVATING)\n\t\t\t\tXPC_DEACTIVATE_PARTITION(part, xpReactivating);\n\n\t\t} else if (act_state_req == XPC_P_ASR_REACTIVATE_UV) {\n\t\t\tif (part->act_state == XPC_P_AS_INACTIVE)\n\t\t\t\txpc_activate_partition(part);\n\t\t\telse\n\t\t\t\tXPC_DEACTIVATE_PARTITION(part, xpReactivating);\n\n\t\t} else if (act_state_req == XPC_P_ASR_DEACTIVATE_UV) {\n\t\t\tXPC_DEACTIVATE_PARTITION(part, part->sn.uv.reason);\n\n\t\t} else {\n\t\t\tBUG();\n\t\t}\n\n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (xpc_activate_IRQ_rcvd == 0)\n\t\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n}\n\nstatic void\nxpc_handle_activate_mq_msg_uv(struct xpc_partition *part,\n\t\t\t      struct xpc_activate_mq_msghdr_uv *msg_hdr,\n\t\t\t      int part_setup,\n\t\t\t      int *wakeup_hb_checker)\n{\n\tunsigned long irq_flags;\n\tstruct xpc_partition_uv *part_uv = &part->sn.uv;\n\tstruct xpc_openclose_args *args;\n\n\tpart_uv->remote_act_state = msg_hdr->act_state;\n\n\tswitch (msg_hdr->type) {\n\tcase XPC_ACTIVATE_MQ_MSG_SYNC_ACT_STATE_UV:\n\t\t \n\t\tbreak;\n\n\tcase XPC_ACTIVATE_MQ_MSG_ACTIVATE_REQ_UV: {\n\t\tstruct xpc_activate_mq_msg_activate_req_uv *msg;\n\n\t\t \n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_activate_req_uv, hdr);\n\n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (part_uv->act_state_req == 0)\n\t\t\txpc_activate_IRQ_rcvd++;\n\t\tpart_uv->act_state_req = XPC_P_ASR_ACTIVATE_UV;\n\t\tpart->remote_rp_pa = msg->rp_gpa;  \n\t\tpart->remote_rp_ts_jiffies = msg_hdr->rp_ts_jiffies;\n\t\tpart_uv->heartbeat_gpa = msg->heartbeat_gpa;\n\n\t\tif (msg->activate_gru_mq_desc_gpa !=\n\t\t    part_uv->activate_gru_mq_desc_gpa) {\n\t\t\tspin_lock(&part_uv->flags_lock);\n\t\t\tpart_uv->flags &= ~XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\n\t\t\tspin_unlock(&part_uv->flags_lock);\n\t\t\tpart_uv->activate_gru_mq_desc_gpa =\n\t\t\t    msg->activate_gru_mq_desc_gpa;\n\t\t}\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\t(*wakeup_hb_checker)++;\n\t\tbreak;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_DEACTIVATE_REQ_UV: {\n\t\tstruct xpc_activate_mq_msg_deactivate_req_uv *msg;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_deactivate_req_uv, hdr);\n\n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (part_uv->act_state_req == 0)\n\t\t\txpc_activate_IRQ_rcvd++;\n\t\tpart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\n\t\tpart_uv->reason = msg->reason;\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\t(*wakeup_hb_checker)++;\n\t\treturn;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREQUEST_UV: {\n\t\tstruct xpc_activate_mq_msg_chctl_closerequest_uv *msg;\n\n\t\tif (!part_setup)\n\t\t\tbreak;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_chctl_closerequest_uv,\n\t\t\t\t   hdr);\n\t\targs = &part->remote_openclose_args[msg->ch_number];\n\t\targs->reason = msg->reason;\n\n\t\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\t\tpart->chctl.flags[msg->ch_number] |= XPC_CHCTL_CLOSEREQUEST;\n\t\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\t\txpc_wakeup_channel_mgr(part);\n\t\tbreak;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREPLY_UV: {\n\t\tstruct xpc_activate_mq_msg_chctl_closereply_uv *msg;\n\n\t\tif (!part_setup)\n\t\t\tbreak;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_chctl_closereply_uv,\n\t\t\t\t   hdr);\n\n\t\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\t\tpart->chctl.flags[msg->ch_number] |= XPC_CHCTL_CLOSEREPLY;\n\t\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\t\txpc_wakeup_channel_mgr(part);\n\t\tbreak;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREQUEST_UV: {\n\t\tstruct xpc_activate_mq_msg_chctl_openrequest_uv *msg;\n\n\t\tif (!part_setup)\n\t\t\tbreak;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_chctl_openrequest_uv,\n\t\t\t\t   hdr);\n\t\targs = &part->remote_openclose_args[msg->ch_number];\n\t\targs->entry_size = msg->entry_size;\n\t\targs->local_nentries = msg->local_nentries;\n\n\t\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\t\tpart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENREQUEST;\n\t\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\t\txpc_wakeup_channel_mgr(part);\n\t\tbreak;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREPLY_UV: {\n\t\tstruct xpc_activate_mq_msg_chctl_openreply_uv *msg;\n\n\t\tif (!part_setup)\n\t\t\tbreak;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\t   xpc_activate_mq_msg_chctl_openreply_uv, hdr);\n\t\targs = &part->remote_openclose_args[msg->ch_number];\n\t\targs->remote_nentries = msg->remote_nentries;\n\t\targs->local_nentries = msg->local_nentries;\n\t\targs->local_msgqueue_pa = msg->notify_gru_mq_desc_gpa;\n\n\t\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\t\tpart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENREPLY;\n\t\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\t\txpc_wakeup_channel_mgr(part);\n\t\tbreak;\n\t}\n\tcase XPC_ACTIVATE_MQ_MSG_CHCTL_OPENCOMPLETE_UV: {\n\t\tstruct xpc_activate_mq_msg_chctl_opencomplete_uv *msg;\n\n\t\tif (!part_setup)\n\t\t\tbreak;\n\n\t\tmsg = container_of(msg_hdr, struct\n\t\t\t\txpc_activate_mq_msg_chctl_opencomplete_uv, hdr);\n\t\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\t\tpart->chctl.flags[msg->ch_number] |= XPC_CHCTL_OPENCOMPLETE;\n\t\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\t\txpc_wakeup_channel_mgr(part);\n\t}\n\t\tfallthrough;\n\tcase XPC_ACTIVATE_MQ_MSG_MARK_ENGAGED_UV:\n\t\tspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\n\t\tpart_uv->flags |= XPC_P_ENGAGED_UV;\n\t\tspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\n\t\tbreak;\n\n\tcase XPC_ACTIVATE_MQ_MSG_MARK_DISENGAGED_UV:\n\t\tspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\n\t\tpart_uv->flags &= ~XPC_P_ENGAGED_UV;\n\t\tspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(xpc_part, \"received unknown activate_mq msg type=%d \"\n\t\t\t\"from partition=%d\\n\", msg_hdr->type, XPC_PARTID(part));\n\n\t\t \n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (part_uv->act_state_req == 0)\n\t\t\txpc_activate_IRQ_rcvd++;\n\t\tpart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\n\t\tpart_uv->reason = xpBadMsgType;\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\t(*wakeup_hb_checker)++;\n\t\treturn;\n\t}\n\n\tif (msg_hdr->rp_ts_jiffies != part->remote_rp_ts_jiffies &&\n\t    part->remote_rp_ts_jiffies != 0) {\n\t\t \n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (part_uv->act_state_req == 0)\n\t\t\txpc_activate_IRQ_rcvd++;\n\t\tpart_uv->act_state_req = XPC_P_ASR_REACTIVATE_UV;\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\t(*wakeup_hb_checker)++;\n\t}\n}\n\nstatic irqreturn_t\nxpc_handle_activate_IRQ_uv(int irq, void *dev_id)\n{\n\tstruct xpc_activate_mq_msghdr_uv *msg_hdr;\n\tshort partid;\n\tstruct xpc_partition *part;\n\tint wakeup_hb_checker = 0;\n\tint part_referenced;\n\n\twhile (1) {\n\t\tmsg_hdr = gru_get_next_message(xpc_activate_mq_uv->gru_mq_desc);\n\t\tif (msg_hdr == NULL)\n\t\t\tbreak;\n\n\t\tpartid = msg_hdr->partid;\n\t\tif (partid < 0 || partid >= XP_MAX_NPARTITIONS_UV) {\n\t\t\tdev_err(xpc_part, \"xpc_handle_activate_IRQ_uv() \"\n\t\t\t\t\"received invalid partid=0x%x in message\\n\",\n\t\t\t\tpartid);\n\t\t} else {\n\t\t\tpart = &xpc_partitions[partid];\n\n\t\t\tpart_referenced = xpc_part_ref(part);\n\t\t\txpc_handle_activate_mq_msg_uv(part, msg_hdr,\n\t\t\t\t\t\t      part_referenced,\n\t\t\t\t\t\t      &wakeup_hb_checker);\n\t\t\tif (part_referenced)\n\t\t\t\txpc_part_deref(part);\n\t\t}\n\n\t\tgru_free_message(xpc_activate_mq_uv->gru_mq_desc, msg_hdr);\n\t}\n\n\tif (wakeup_hb_checker)\n\t\twake_up_interruptible(&xpc_activate_IRQ_wq);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic enum xp_retval\nxpc_cache_remote_gru_mq_desc_uv(struct gru_message_queue_desc *gru_mq_desc,\n\t\t\t\tunsigned long gru_mq_desc_gpa)\n{\n\tenum xp_retval ret;\n\n\tret = xp_remote_memcpy(uv_gpa(gru_mq_desc), gru_mq_desc_gpa,\n\t\t\t       sizeof(struct gru_message_queue_desc));\n\tif (ret == xpSuccess)\n\t\tgru_mq_desc->mq = NULL;\n\n\treturn ret;\n}\n\nstatic enum xp_retval\nxpc_send_activate_IRQ_uv(struct xpc_partition *part, void *msg, size_t msg_size,\n\t\t\t int msg_type)\n{\n\tstruct xpc_activate_mq_msghdr_uv *msg_hdr = msg;\n\tstruct xpc_partition_uv *part_uv = &part->sn.uv;\n\tstruct gru_message_queue_desc *gru_mq_desc;\n\tunsigned long irq_flags;\n\tenum xp_retval ret;\n\n\tDBUG_ON(msg_size > XPC_ACTIVATE_MSG_SIZE_UV);\n\n\tmsg_hdr->type = msg_type;\n\tmsg_hdr->partid = xp_partition_id;\n\tmsg_hdr->act_state = part->act_state;\n\tmsg_hdr->rp_ts_jiffies = xpc_rsvd_page->ts_jiffies;\n\n\tmutex_lock(&part_uv->cached_activate_gru_mq_desc_mutex);\nagain:\n\tif (!(part_uv->flags & XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV)) {\n\t\tgru_mq_desc = part_uv->cached_activate_gru_mq_desc;\n\t\tif (gru_mq_desc == NULL) {\n\t\t\tgru_mq_desc = kmalloc(sizeof(struct\n\t\t\t\t\t      gru_message_queue_desc),\n\t\t\t\t\t      GFP_ATOMIC);\n\t\t\tif (gru_mq_desc == NULL) {\n\t\t\t\tret = xpNoMemory;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\tpart_uv->cached_activate_gru_mq_desc = gru_mq_desc;\n\t\t}\n\n\t\tret = xpc_cache_remote_gru_mq_desc_uv(gru_mq_desc,\n\t\t\t\t\t\t      part_uv->\n\t\t\t\t\t\t      activate_gru_mq_desc_gpa);\n\t\tif (ret != xpSuccess)\n\t\t\tgoto done;\n\n\t\tspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\n\t\tpart_uv->flags |= XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV;\n\t\tspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\n\t}\n\n\t \n\tret = xpc_send_gru_msg(part_uv->cached_activate_gru_mq_desc, msg,\n\t\t\t       msg_size);\n\tif (ret != xpSuccess) {\n\t\tsmp_rmb();\t \n\t\tif (!(part_uv->flags & XPC_P_CACHED_ACTIVATE_GRU_MQ_DESC_UV))\n\t\t\tgoto again;\n\t}\ndone:\n\tmutex_unlock(&part_uv->cached_activate_gru_mq_desc_mutex);\n\treturn ret;\n}\n\nstatic void\nxpc_send_activate_IRQ_part_uv(struct xpc_partition *part, void *msg,\n\t\t\t      size_t msg_size, int msg_type)\n{\n\tenum xp_retval ret;\n\n\tret = xpc_send_activate_IRQ_uv(part, msg, msg_size, msg_type);\n\tif (unlikely(ret != xpSuccess))\n\t\tXPC_DEACTIVATE_PARTITION(part, ret);\n}\n\nstatic void\nxpc_send_activate_IRQ_ch_uv(struct xpc_channel *ch, unsigned long *irq_flags,\n\t\t\t void *msg, size_t msg_size, int msg_type)\n{\n\tstruct xpc_partition *part = &xpc_partitions[ch->partid];\n\tenum xp_retval ret;\n\n\tret = xpc_send_activate_IRQ_uv(part, msg, msg_size, msg_type);\n\tif (unlikely(ret != xpSuccess)) {\n\t\tif (irq_flags != NULL)\n\t\t\tspin_unlock_irqrestore(&ch->lock, *irq_flags);\n\n\t\tXPC_DEACTIVATE_PARTITION(part, ret);\n\n\t\tif (irq_flags != NULL)\n\t\t\tspin_lock_irqsave(&ch->lock, *irq_flags);\n\t}\n}\n\nstatic void\nxpc_send_local_activate_IRQ_uv(struct xpc_partition *part, int act_state_req)\n{\n\tunsigned long irq_flags;\n\tstruct xpc_partition_uv *part_uv = &part->sn.uv;\n\n\t \n\n\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\tif (part_uv->act_state_req == 0)\n\t\txpc_activate_IRQ_rcvd++;\n\tpart_uv->act_state_req = act_state_req;\n\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\twake_up_interruptible(&xpc_activate_IRQ_wq);\n}\n\nstatic enum xp_retval\nxpc_get_partition_rsvd_page_pa_uv(void *buf, u64 *cookie, unsigned long *rp_pa,\n\t\t\t\t  size_t *len)\n{\n\ts64 status;\n\tenum xp_retval ret;\n\n#if defined CONFIG_X86_64\n\tstatus = uv_bios_reserved_page_pa((u64)buf, cookie, (u64 *)rp_pa,\n\t\t\t\t\t  (u64 *)len);\n\tif (status == BIOS_STATUS_SUCCESS)\n\t\tret = xpSuccess;\n\telse if (status == BIOS_STATUS_MORE_PASSES)\n\t\tret = xpNeedMoreInfo;\n\telse\n\t\tret = xpBiosError;\n\n#elif defined CONFIG_IA64_SGI_UV\n\tstatus = sn_partition_reserved_page_pa((u64)buf, cookie, rp_pa, len);\n\tif (status == SALRET_OK)\n\t\tret = xpSuccess;\n\telse if (status == SALRET_MORE_PASSES)\n\t\tret = xpNeedMoreInfo;\n\telse\n\t\tret = xpSalError;\n\n#else\n\t#error not a supported configuration\n#endif\n\n\treturn ret;\n}\n\nstatic int\nxpc_setup_rsvd_page_uv(struct xpc_rsvd_page *rp)\n{\n\txpc_heartbeat_uv =\n\t    &xpc_partitions[sn_partition_id].sn.uv.cached_heartbeat;\n\trp->sn.uv.heartbeat_gpa = uv_gpa(xpc_heartbeat_uv);\n\trp->sn.uv.activate_gru_mq_desc_gpa =\n\t    uv_gpa(xpc_activate_mq_uv->gru_mq_desc);\n\treturn 0;\n}\n\nstatic void\nxpc_allow_hb_uv(short partid)\n{\n}\n\nstatic void\nxpc_disallow_hb_uv(short partid)\n{\n}\n\nstatic void\nxpc_disallow_all_hbs_uv(void)\n{\n}\n\nstatic void\nxpc_increment_heartbeat_uv(void)\n{\n\txpc_heartbeat_uv->value++;\n}\n\nstatic void\nxpc_offline_heartbeat_uv(void)\n{\n\txpc_increment_heartbeat_uv();\n\txpc_heartbeat_uv->offline = 1;\n}\n\nstatic void\nxpc_online_heartbeat_uv(void)\n{\n\txpc_increment_heartbeat_uv();\n\txpc_heartbeat_uv->offline = 0;\n}\n\nstatic void\nxpc_heartbeat_init_uv(void)\n{\n\txpc_heartbeat_uv->value = 1;\n\txpc_heartbeat_uv->offline = 0;\n}\n\nstatic void\nxpc_heartbeat_exit_uv(void)\n{\n\txpc_offline_heartbeat_uv();\n}\n\nstatic enum xp_retval\nxpc_get_remote_heartbeat_uv(struct xpc_partition *part)\n{\n\tstruct xpc_partition_uv *part_uv = &part->sn.uv;\n\tenum xp_retval ret;\n\n\tret = xp_remote_memcpy(uv_gpa(&part_uv->cached_heartbeat),\n\t\t\t       part_uv->heartbeat_gpa,\n\t\t\t       sizeof(struct xpc_heartbeat_uv));\n\tif (ret != xpSuccess)\n\t\treturn ret;\n\n\tif (part_uv->cached_heartbeat.value == part->last_heartbeat &&\n\t    !part_uv->cached_heartbeat.offline) {\n\n\t\tret = xpNoHeartbeat;\n\t} else {\n\t\tpart->last_heartbeat = part_uv->cached_heartbeat.value;\n\t}\n\treturn ret;\n}\n\nstatic void\nxpc_request_partition_activation_uv(struct xpc_rsvd_page *remote_rp,\n\t\t\t\t    unsigned long remote_rp_gpa, int nasid)\n{\n\tshort partid = remote_rp->SAL_partid;\n\tstruct xpc_partition *part = &xpc_partitions[partid];\n\tstruct xpc_activate_mq_msg_activate_req_uv msg;\n\n\tpart->remote_rp_pa = remote_rp_gpa;  \n\tpart->remote_rp_ts_jiffies = remote_rp->ts_jiffies;\n\tpart->sn.uv.heartbeat_gpa = remote_rp->sn.uv.heartbeat_gpa;\n\tpart->sn.uv.activate_gru_mq_desc_gpa =\n\t    remote_rp->sn.uv.activate_gru_mq_desc_gpa;\n\n\t \n\tif (part->sn.uv.remote_act_state == XPC_P_AS_INACTIVE) {\n\t\tmsg.rp_gpa = uv_gpa(xpc_rsvd_page);\n\t\tmsg.heartbeat_gpa = xpc_rsvd_page->sn.uv.heartbeat_gpa;\n\t\tmsg.activate_gru_mq_desc_gpa =\n\t\t    xpc_rsvd_page->sn.uv.activate_gru_mq_desc_gpa;\n\t\txpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\n\t\t\t\t\t   XPC_ACTIVATE_MQ_MSG_ACTIVATE_REQ_UV);\n\t}\n\n\tif (part->act_state == XPC_P_AS_INACTIVE)\n\t\txpc_send_local_activate_IRQ_uv(part, XPC_P_ASR_ACTIVATE_UV);\n}\n\nstatic void\nxpc_request_partition_reactivation_uv(struct xpc_partition *part)\n{\n\txpc_send_local_activate_IRQ_uv(part, XPC_P_ASR_ACTIVATE_UV);\n}\n\nstatic void\nxpc_request_partition_deactivation_uv(struct xpc_partition *part)\n{\n\tstruct xpc_activate_mq_msg_deactivate_req_uv msg;\n\n\t \n\tif (part->sn.uv.remote_act_state != XPC_P_AS_DEACTIVATING &&\n\t    part->sn.uv.remote_act_state != XPC_P_AS_INACTIVE) {\n\n\t\tmsg.reason = part->reason;\n\t\txpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\n\t\t\t\t\t XPC_ACTIVATE_MQ_MSG_DEACTIVATE_REQ_UV);\n\t}\n}\n\nstatic void\nxpc_cancel_partition_deactivation_request_uv(struct xpc_partition *part)\n{\n\t \n\treturn;\n}\n\nstatic void\nxpc_init_fifo_uv(struct xpc_fifo_head_uv *head)\n{\n\thead->first = NULL;\n\thead->last = NULL;\n\tspin_lock_init(&head->lock);\n\thead->n_entries = 0;\n}\n\nstatic void *\nxpc_get_fifo_entry_uv(struct xpc_fifo_head_uv *head)\n{\n\tunsigned long irq_flags;\n\tstruct xpc_fifo_entry_uv *first;\n\n\tspin_lock_irqsave(&head->lock, irq_flags);\n\tfirst = head->first;\n\tif (head->first != NULL) {\n\t\thead->first = first->next;\n\t\tif (head->first == NULL)\n\t\t\thead->last = NULL;\n\n\t\thead->n_entries--;\n\t\tBUG_ON(head->n_entries < 0);\n\n\t\tfirst->next = NULL;\n\t}\n\tspin_unlock_irqrestore(&head->lock, irq_flags);\n\treturn first;\n}\n\nstatic void\nxpc_put_fifo_entry_uv(struct xpc_fifo_head_uv *head,\n\t\t      struct xpc_fifo_entry_uv *last)\n{\n\tunsigned long irq_flags;\n\n\tlast->next = NULL;\n\tspin_lock_irqsave(&head->lock, irq_flags);\n\tif (head->last != NULL)\n\t\thead->last->next = last;\n\telse\n\t\thead->first = last;\n\thead->last = last;\n\thead->n_entries++;\n\tspin_unlock_irqrestore(&head->lock, irq_flags);\n}\n\nstatic int\nxpc_n_of_fifo_entries_uv(struct xpc_fifo_head_uv *head)\n{\n\treturn head->n_entries;\n}\n\n \nstatic enum xp_retval\nxpc_setup_ch_structures_uv(struct xpc_partition *part)\n{\n\tstruct xpc_channel_uv *ch_uv;\n\tint ch_number;\n\n\tfor (ch_number = 0; ch_number < part->nchannels; ch_number++) {\n\t\tch_uv = &part->channels[ch_number].sn.uv;\n\n\t\txpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\n\t\txpc_init_fifo_uv(&ch_uv->recv_msg_list);\n\t}\n\n\treturn xpSuccess;\n}\n\n \nstatic void\nxpc_teardown_ch_structures_uv(struct xpc_partition *part)\n{\n\t \n\treturn;\n}\n\nstatic enum xp_retval\nxpc_make_first_contact_uv(struct xpc_partition *part)\n{\n\tstruct xpc_activate_mq_msg_uv msg;\n\n\t \n\txpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\n\t\t\t\t      XPC_ACTIVATE_MQ_MSG_SYNC_ACT_STATE_UV);\n\n\twhile (!((part->sn.uv.remote_act_state == XPC_P_AS_ACTIVATING) ||\n\t\t (part->sn.uv.remote_act_state == XPC_P_AS_ACTIVE))) {\n\n\t\tdev_dbg(xpc_part, \"waiting to make first contact with \"\n\t\t\t\"partition %d\\n\", XPC_PARTID(part));\n\n\t\t \n\t\t(void)msleep_interruptible(250);\n\n\t\tif (part->act_state == XPC_P_AS_DEACTIVATING)\n\t\t\treturn part->reason;\n\t}\n\n\treturn xpSuccess;\n}\n\nstatic u64\nxpc_get_chctl_all_flags_uv(struct xpc_partition *part)\n{\n\tunsigned long irq_flags;\n\tunion xpc_channel_ctl_flags chctl;\n\n\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\tchctl = part->chctl;\n\tif (chctl.all_flags != 0)\n\t\tpart->chctl.all_flags = 0;\n\n\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\treturn chctl.all_flags;\n}\n\nstatic enum xp_retval\nxpc_allocate_send_msg_slot_uv(struct xpc_channel *ch)\n{\n\tstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\n\tstruct xpc_send_msg_slot_uv *msg_slot;\n\tunsigned long irq_flags;\n\tint nentries;\n\tint entry;\n\tsize_t nbytes;\n\n\tfor (nentries = ch->local_nentries; nentries > 0; nentries--) {\n\t\tnbytes = nentries * sizeof(struct xpc_send_msg_slot_uv);\n\t\tch_uv->send_msg_slots = kzalloc(nbytes, GFP_KERNEL);\n\t\tif (ch_uv->send_msg_slots == NULL)\n\t\t\tcontinue;\n\n\t\tfor (entry = 0; entry < nentries; entry++) {\n\t\t\tmsg_slot = &ch_uv->send_msg_slots[entry];\n\n\t\t\tmsg_slot->msg_slot_number = entry;\n\t\t\txpc_put_fifo_entry_uv(&ch_uv->msg_slot_free_list,\n\t\t\t\t\t      &msg_slot->next);\n\t\t}\n\n\t\tspin_lock_irqsave(&ch->lock, irq_flags);\n\t\tif (nentries < ch->local_nentries)\n\t\t\tch->local_nentries = nentries;\n\t\tspin_unlock_irqrestore(&ch->lock, irq_flags);\n\t\treturn xpSuccess;\n\t}\n\n\treturn xpNoMemory;\n}\n\nstatic enum xp_retval\nxpc_allocate_recv_msg_slot_uv(struct xpc_channel *ch)\n{\n\tstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\n\tstruct xpc_notify_mq_msg_uv *msg_slot;\n\tunsigned long irq_flags;\n\tint nentries;\n\tint entry;\n\tsize_t nbytes;\n\n\tfor (nentries = ch->remote_nentries; nentries > 0; nentries--) {\n\t\tnbytes = nentries * ch->entry_size;\n\t\tch_uv->recv_msg_slots = kzalloc(nbytes, GFP_KERNEL);\n\t\tif (ch_uv->recv_msg_slots == NULL)\n\t\t\tcontinue;\n\n\t\tfor (entry = 0; entry < nentries; entry++) {\n\t\t\tmsg_slot = ch_uv->recv_msg_slots +\n\t\t\t    entry * ch->entry_size;\n\n\t\t\tmsg_slot->hdr.msg_slot_number = entry;\n\t\t}\n\n\t\tspin_lock_irqsave(&ch->lock, irq_flags);\n\t\tif (nentries < ch->remote_nentries)\n\t\t\tch->remote_nentries = nentries;\n\t\tspin_unlock_irqrestore(&ch->lock, irq_flags);\n\t\treturn xpSuccess;\n\t}\n\n\treturn xpNoMemory;\n}\n\n \nstatic enum xp_retval\nxpc_setup_msg_structures_uv(struct xpc_channel *ch)\n{\n\tstatic enum xp_retval ret;\n\tstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\n\n\tDBUG_ON(ch->flags & XPC_C_SETUP);\n\n\tch_uv->cached_notify_gru_mq_desc = kmalloc(sizeof(struct\n\t\t\t\t\t\t   gru_message_queue_desc),\n\t\t\t\t\t\t   GFP_KERNEL);\n\tif (ch_uv->cached_notify_gru_mq_desc == NULL)\n\t\treturn xpNoMemory;\n\n\tret = xpc_allocate_send_msg_slot_uv(ch);\n\tif (ret == xpSuccess) {\n\n\t\tret = xpc_allocate_recv_msg_slot_uv(ch);\n\t\tif (ret != xpSuccess) {\n\t\t\tkfree(ch_uv->send_msg_slots);\n\t\t\txpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nstatic void\nxpc_teardown_msg_structures_uv(struct xpc_channel *ch)\n{\n\tstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\n\n\tlockdep_assert_held(&ch->lock);\n\n\tkfree(ch_uv->cached_notify_gru_mq_desc);\n\tch_uv->cached_notify_gru_mq_desc = NULL;\n\n\tif (ch->flags & XPC_C_SETUP) {\n\t\txpc_init_fifo_uv(&ch_uv->msg_slot_free_list);\n\t\tkfree(ch_uv->send_msg_slots);\n\t\txpc_init_fifo_uv(&ch_uv->recv_msg_list);\n\t\tkfree(ch_uv->recv_msg_slots);\n\t}\n}\n\nstatic void\nxpc_send_chctl_closerequest_uv(struct xpc_channel *ch, unsigned long *irq_flags)\n{\n\tstruct xpc_activate_mq_msg_chctl_closerequest_uv msg;\n\n\tmsg.ch_number = ch->number;\n\tmsg.reason = ch->reason;\n\txpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\n\t\t\t\t    XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREQUEST_UV);\n}\n\nstatic void\nxpc_send_chctl_closereply_uv(struct xpc_channel *ch, unsigned long *irq_flags)\n{\n\tstruct xpc_activate_mq_msg_chctl_closereply_uv msg;\n\n\tmsg.ch_number = ch->number;\n\txpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\n\t\t\t\t    XPC_ACTIVATE_MQ_MSG_CHCTL_CLOSEREPLY_UV);\n}\n\nstatic void\nxpc_send_chctl_openrequest_uv(struct xpc_channel *ch, unsigned long *irq_flags)\n{\n\tstruct xpc_activate_mq_msg_chctl_openrequest_uv msg;\n\n\tmsg.ch_number = ch->number;\n\tmsg.entry_size = ch->entry_size;\n\tmsg.local_nentries = ch->local_nentries;\n\txpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\n\t\t\t\t    XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREQUEST_UV);\n}\n\nstatic void\nxpc_send_chctl_openreply_uv(struct xpc_channel *ch, unsigned long *irq_flags)\n{\n\tstruct xpc_activate_mq_msg_chctl_openreply_uv msg;\n\n\tmsg.ch_number = ch->number;\n\tmsg.local_nentries = ch->local_nentries;\n\tmsg.remote_nentries = ch->remote_nentries;\n\tmsg.notify_gru_mq_desc_gpa = uv_gpa(xpc_notify_mq_uv->gru_mq_desc);\n\txpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\n\t\t\t\t    XPC_ACTIVATE_MQ_MSG_CHCTL_OPENREPLY_UV);\n}\n\nstatic void\nxpc_send_chctl_opencomplete_uv(struct xpc_channel *ch, unsigned long *irq_flags)\n{\n\tstruct xpc_activate_mq_msg_chctl_opencomplete_uv msg;\n\n\tmsg.ch_number = ch->number;\n\txpc_send_activate_IRQ_ch_uv(ch, irq_flags, &msg, sizeof(msg),\n\t\t\t\t    XPC_ACTIVATE_MQ_MSG_CHCTL_OPENCOMPLETE_UV);\n}\n\nstatic void\nxpc_send_chctl_local_msgrequest_uv(struct xpc_partition *part, int ch_number)\n{\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(&part->chctl_lock, irq_flags);\n\tpart->chctl.flags[ch_number] |= XPC_CHCTL_MSGREQUEST;\n\tspin_unlock_irqrestore(&part->chctl_lock, irq_flags);\n\n\txpc_wakeup_channel_mgr(part);\n}\n\nstatic enum xp_retval\nxpc_save_remote_msgqueue_pa_uv(struct xpc_channel *ch,\n\t\t\t       unsigned long gru_mq_desc_gpa)\n{\n\tstruct xpc_channel_uv *ch_uv = &ch->sn.uv;\n\n\tDBUG_ON(ch_uv->cached_notify_gru_mq_desc == NULL);\n\treturn xpc_cache_remote_gru_mq_desc_uv(ch_uv->cached_notify_gru_mq_desc,\n\t\t\t\t\t       gru_mq_desc_gpa);\n}\n\nstatic void\nxpc_indicate_partition_engaged_uv(struct xpc_partition *part)\n{\n\tstruct xpc_activate_mq_msg_uv msg;\n\n\txpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\n\t\t\t\t      XPC_ACTIVATE_MQ_MSG_MARK_ENGAGED_UV);\n}\n\nstatic void\nxpc_indicate_partition_disengaged_uv(struct xpc_partition *part)\n{\n\tstruct xpc_activate_mq_msg_uv msg;\n\n\txpc_send_activate_IRQ_part_uv(part, &msg, sizeof(msg),\n\t\t\t\t      XPC_ACTIVATE_MQ_MSG_MARK_DISENGAGED_UV);\n}\n\nstatic void\nxpc_assume_partition_disengaged_uv(short partid)\n{\n\tstruct xpc_partition_uv *part_uv = &xpc_partitions[partid].sn.uv;\n\tunsigned long irq_flags;\n\n\tspin_lock_irqsave(&part_uv->flags_lock, irq_flags);\n\tpart_uv->flags &= ~XPC_P_ENGAGED_UV;\n\tspin_unlock_irqrestore(&part_uv->flags_lock, irq_flags);\n}\n\nstatic int\nxpc_partition_engaged_uv(short partid)\n{\n\treturn (xpc_partitions[partid].sn.uv.flags & XPC_P_ENGAGED_UV) != 0;\n}\n\nstatic int\nxpc_any_partition_engaged_uv(void)\n{\n\tstruct xpc_partition_uv *part_uv;\n\tshort partid;\n\n\tfor (partid = 0; partid < XP_MAX_NPARTITIONS_UV; partid++) {\n\t\tpart_uv = &xpc_partitions[partid].sn.uv;\n\t\tif ((part_uv->flags & XPC_P_ENGAGED_UV) != 0)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic enum xp_retval\nxpc_allocate_msg_slot_uv(struct xpc_channel *ch, u32 flags,\n\t\t\t struct xpc_send_msg_slot_uv **address_of_msg_slot)\n{\n\tenum xp_retval ret;\n\tstruct xpc_send_msg_slot_uv *msg_slot;\n\tstruct xpc_fifo_entry_uv *entry;\n\n\twhile (1) {\n\t\tentry = xpc_get_fifo_entry_uv(&ch->sn.uv.msg_slot_free_list);\n\t\tif (entry != NULL)\n\t\t\tbreak;\n\n\t\tif (flags & XPC_NOWAIT)\n\t\t\treturn xpNoWait;\n\n\t\tret = xpc_allocate_msg_wait(ch);\n\t\tif (ret != xpInterrupted && ret != xpTimeout)\n\t\t\treturn ret;\n\t}\n\n\tmsg_slot = container_of(entry, struct xpc_send_msg_slot_uv, next);\n\t*address_of_msg_slot = msg_slot;\n\treturn xpSuccess;\n}\n\nstatic void\nxpc_free_msg_slot_uv(struct xpc_channel *ch,\n\t\t     struct xpc_send_msg_slot_uv *msg_slot)\n{\n\txpc_put_fifo_entry_uv(&ch->sn.uv.msg_slot_free_list, &msg_slot->next);\n\n\t \n\tif (atomic_read(&ch->n_on_msg_allocate_wq) > 0)\n\t\twake_up(&ch->msg_allocate_wq);\n}\n\nstatic void\nxpc_notify_sender_uv(struct xpc_channel *ch,\n\t\t     struct xpc_send_msg_slot_uv *msg_slot,\n\t\t     enum xp_retval reason)\n{\n\txpc_notify_func func = msg_slot->func;\n\n\tif (func != NULL && cmpxchg(&msg_slot->func, func, NULL) == func) {\n\n\t\tatomic_dec(&ch->n_to_notify);\n\n\t\tdev_dbg(xpc_chan, \"msg_slot->func() called, msg_slot=0x%p \"\n\t\t\t\"msg_slot_number=%d partid=%d channel=%d\\n\", msg_slot,\n\t\t\tmsg_slot->msg_slot_number, ch->partid, ch->number);\n\n\t\tfunc(reason, ch->partid, ch->number, msg_slot->key);\n\n\t\tdev_dbg(xpc_chan, \"msg_slot->func() returned, msg_slot=0x%p \"\n\t\t\t\"msg_slot_number=%d partid=%d channel=%d\\n\", msg_slot,\n\t\t\tmsg_slot->msg_slot_number, ch->partid, ch->number);\n\t}\n}\n\nstatic void\nxpc_handle_notify_mq_ack_uv(struct xpc_channel *ch,\n\t\t\t    struct xpc_notify_mq_msg_uv *msg)\n{\n\tstruct xpc_send_msg_slot_uv *msg_slot;\n\tint entry = msg->hdr.msg_slot_number % ch->local_nentries;\n\n\tmsg_slot = &ch->sn.uv.send_msg_slots[entry];\n\n\tBUG_ON(msg_slot->msg_slot_number != msg->hdr.msg_slot_number);\n\tmsg_slot->msg_slot_number += ch->local_nentries;\n\n\tif (msg_slot->func != NULL)\n\t\txpc_notify_sender_uv(ch, msg_slot, xpMsgDelivered);\n\n\txpc_free_msg_slot_uv(ch, msg_slot);\n}\n\nstatic void\nxpc_handle_notify_mq_msg_uv(struct xpc_partition *part,\n\t\t\t    struct xpc_notify_mq_msg_uv *msg)\n{\n\tstruct xpc_partition_uv *part_uv = &part->sn.uv;\n\tstruct xpc_channel *ch;\n\tstruct xpc_channel_uv *ch_uv;\n\tstruct xpc_notify_mq_msg_uv *msg_slot;\n\tunsigned long irq_flags;\n\tint ch_number = msg->hdr.ch_number;\n\n\tif (unlikely(ch_number >= part->nchannels)) {\n\t\tdev_err(xpc_part, \"xpc_handle_notify_IRQ_uv() received invalid \"\n\t\t\t\"channel number=0x%x in message from partid=%d\\n\",\n\t\t\tch_number, XPC_PARTID(part));\n\n\t\t \n\t\tspin_lock_irqsave(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\t\tif (part_uv->act_state_req == 0)\n\t\t\txpc_activate_IRQ_rcvd++;\n\t\tpart_uv->act_state_req = XPC_P_ASR_DEACTIVATE_UV;\n\t\tpart_uv->reason = xpBadChannelNumber;\n\t\tspin_unlock_irqrestore(&xpc_activate_IRQ_rcvd_lock, irq_flags);\n\n\t\twake_up_interruptible(&xpc_activate_IRQ_wq);\n\t\treturn;\n\t}\n\n\tch = &part->channels[ch_number];\n\txpc_msgqueue_ref(ch);\n\n\tif (!(ch->flags & XPC_C_CONNECTED)) {\n\t\txpc_msgqueue_deref(ch);\n\t\treturn;\n\t}\n\n\t \n\tif (msg->hdr.size == 0) {\n\t\txpc_handle_notify_mq_ack_uv(ch, msg);\n\t\txpc_msgqueue_deref(ch);\n\t\treturn;\n\t}\n\n\t \n\tch_uv = &ch->sn.uv;\n\n\tmsg_slot = ch_uv->recv_msg_slots +\n\t    (msg->hdr.msg_slot_number % ch->remote_nentries) * ch->entry_size;\n\n\tBUG_ON(msg_slot->hdr.size != 0);\n\n\tmemcpy(msg_slot, msg, msg->hdr.size);\n\n\txpc_put_fifo_entry_uv(&ch_uv->recv_msg_list, &msg_slot->hdr.u.next);\n\n\tif (ch->flags & XPC_C_CONNECTEDCALLOUT_MADE) {\n\t\t \n\t\tif (atomic_read(&ch->kthreads_idle) > 0)\n\t\t\twake_up_nr(&ch->idle_wq, 1);\n\t\telse\n\t\t\txpc_send_chctl_local_msgrequest_uv(part, ch->number);\n\t}\n\txpc_msgqueue_deref(ch);\n}\n\nstatic irqreturn_t\nxpc_handle_notify_IRQ_uv(int irq, void *dev_id)\n{\n\tstruct xpc_notify_mq_msg_uv *msg;\n\tshort partid;\n\tstruct xpc_partition *part;\n\n\twhile ((msg = gru_get_next_message(xpc_notify_mq_uv->gru_mq_desc)) !=\n\t       NULL) {\n\n\t\tpartid = msg->hdr.partid;\n\t\tif (partid < 0 || partid >= XP_MAX_NPARTITIONS_UV) {\n\t\t\tdev_err(xpc_part, \"xpc_handle_notify_IRQ_uv() received \"\n\t\t\t\t\"invalid partid=0x%x in message\\n\", partid);\n\t\t} else {\n\t\t\tpart = &xpc_partitions[partid];\n\n\t\t\tif (xpc_part_ref(part)) {\n\t\t\t\txpc_handle_notify_mq_msg_uv(part, msg);\n\t\t\t\txpc_part_deref(part);\n\t\t\t}\n\t\t}\n\n\t\tgru_free_message(xpc_notify_mq_uv->gru_mq_desc, msg);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int\nxpc_n_of_deliverable_payloads_uv(struct xpc_channel *ch)\n{\n\treturn xpc_n_of_fifo_entries_uv(&ch->sn.uv.recv_msg_list);\n}\n\nstatic void\nxpc_process_msg_chctl_flags_uv(struct xpc_partition *part, int ch_number)\n{\n\tstruct xpc_channel *ch = &part->channels[ch_number];\n\tint ndeliverable_payloads;\n\n\txpc_msgqueue_ref(ch);\n\n\tndeliverable_payloads = xpc_n_of_deliverable_payloads_uv(ch);\n\n\tif (ndeliverable_payloads > 0 &&\n\t    (ch->flags & XPC_C_CONNECTED) &&\n\t    (ch->flags & XPC_C_CONNECTEDCALLOUT_MADE)) {\n\n\t\txpc_activate_kthreads(ch, ndeliverable_payloads);\n\t}\n\n\txpc_msgqueue_deref(ch);\n}\n\nstatic enum xp_retval\nxpc_send_payload_uv(struct xpc_channel *ch, u32 flags, void *payload,\n\t\t    u16 payload_size, u8 notify_type, xpc_notify_func func,\n\t\t    void *key)\n{\n\tenum xp_retval ret = xpSuccess;\n\tstruct xpc_send_msg_slot_uv *msg_slot = NULL;\n\tstruct xpc_notify_mq_msg_uv *msg;\n\tu8 msg_buffer[XPC_NOTIFY_MSG_SIZE_UV];\n\tsize_t msg_size;\n\n\tDBUG_ON(notify_type != XPC_N_CALL);\n\n\tmsg_size = sizeof(struct xpc_notify_mq_msghdr_uv) + payload_size;\n\tif (msg_size > ch->entry_size)\n\t\treturn xpPayloadTooBig;\n\n\txpc_msgqueue_ref(ch);\n\n\tif (ch->flags & XPC_C_DISCONNECTING) {\n\t\tret = ch->reason;\n\t\tgoto out_1;\n\t}\n\tif (!(ch->flags & XPC_C_CONNECTED)) {\n\t\tret = xpNotConnected;\n\t\tgoto out_1;\n\t}\n\n\tret = xpc_allocate_msg_slot_uv(ch, flags, &msg_slot);\n\tif (ret != xpSuccess)\n\t\tgoto out_1;\n\n\tif (func != NULL) {\n\t\tatomic_inc(&ch->n_to_notify);\n\n\t\tmsg_slot->key = key;\n\t\tsmp_wmb();  \n\t\tmsg_slot->func = func;\n\n\t\tif (ch->flags & XPC_C_DISCONNECTING) {\n\t\t\tret = ch->reason;\n\t\t\tgoto out_2;\n\t\t}\n\t}\n\n\tmsg = (struct xpc_notify_mq_msg_uv *)&msg_buffer;\n\tmsg->hdr.partid = xp_partition_id;\n\tmsg->hdr.ch_number = ch->number;\n\tmsg->hdr.size = msg_size;\n\tmsg->hdr.msg_slot_number = msg_slot->msg_slot_number;\n\tmemcpy(&msg->payload, payload, payload_size);\n\n\tret = xpc_send_gru_msg(ch->sn.uv.cached_notify_gru_mq_desc, msg,\n\t\t\t       msg_size);\n\tif (ret == xpSuccess)\n\t\tgoto out_1;\n\n\tXPC_DEACTIVATE_PARTITION(&xpc_partitions[ch->partid], ret);\nout_2:\n\tif (func != NULL) {\n\t\t \n\t\tif (cmpxchg(&msg_slot->func, func, NULL) != func) {\n\t\t\tret = xpSuccess;\n\t\t\tgoto out_1;\n\t\t}\n\n\t\tmsg_slot->key = NULL;\n\t\tatomic_dec(&ch->n_to_notify);\n\t}\n\txpc_free_msg_slot_uv(ch, msg_slot);\nout_1:\n\txpc_msgqueue_deref(ch);\n\treturn ret;\n}\n\n \nstatic void\nxpc_notify_senders_of_disconnect_uv(struct xpc_channel *ch)\n{\n\tstruct xpc_send_msg_slot_uv *msg_slot;\n\tint entry;\n\n\tDBUG_ON(!(ch->flags & XPC_C_DISCONNECTING));\n\n\tfor (entry = 0; entry < ch->local_nentries; entry++) {\n\n\t\tif (atomic_read(&ch->n_to_notify) == 0)\n\t\t\tbreak;\n\n\t\tmsg_slot = &ch->sn.uv.send_msg_slots[entry];\n\t\tif (msg_slot->func != NULL)\n\t\t\txpc_notify_sender_uv(ch, msg_slot, ch->reason);\n\t}\n}\n\n \nstatic void *\nxpc_get_deliverable_payload_uv(struct xpc_channel *ch)\n{\n\tstruct xpc_fifo_entry_uv *entry;\n\tstruct xpc_notify_mq_msg_uv *msg;\n\tvoid *payload = NULL;\n\n\tif (!(ch->flags & XPC_C_DISCONNECTING)) {\n\t\tentry = xpc_get_fifo_entry_uv(&ch->sn.uv.recv_msg_list);\n\t\tif (entry != NULL) {\n\t\t\tmsg = container_of(entry, struct xpc_notify_mq_msg_uv,\n\t\t\t\t\t   hdr.u.next);\n\t\t\tpayload = &msg->payload;\n\t\t}\n\t}\n\treturn payload;\n}\n\nstatic void\nxpc_received_payload_uv(struct xpc_channel *ch, void *payload)\n{\n\tstruct xpc_notify_mq_msg_uv *msg;\n\tenum xp_retval ret;\n\n\tmsg = container_of(payload, struct xpc_notify_mq_msg_uv, payload);\n\n\t \n\n\tmsg->hdr.partid = xp_partition_id;\n\tmsg->hdr.size = 0;\t \n\n\tret = xpc_send_gru_msg(ch->sn.uv.cached_notify_gru_mq_desc, msg,\n\t\t\t       sizeof(struct xpc_notify_mq_msghdr_uv));\n\tif (ret != xpSuccess)\n\t\tXPC_DEACTIVATE_PARTITION(&xpc_partitions[ch->partid], ret);\n}\n\nstatic const struct xpc_arch_operations xpc_arch_ops_uv = {\n\t.setup_partitions = xpc_setup_partitions_uv,\n\t.teardown_partitions = xpc_teardown_partitions_uv,\n\t.process_activate_IRQ_rcvd = xpc_process_activate_IRQ_rcvd_uv,\n\t.get_partition_rsvd_page_pa = xpc_get_partition_rsvd_page_pa_uv,\n\t.setup_rsvd_page = xpc_setup_rsvd_page_uv,\n\n\t.allow_hb = xpc_allow_hb_uv,\n\t.disallow_hb = xpc_disallow_hb_uv,\n\t.disallow_all_hbs = xpc_disallow_all_hbs_uv,\n\t.increment_heartbeat = xpc_increment_heartbeat_uv,\n\t.offline_heartbeat = xpc_offline_heartbeat_uv,\n\t.online_heartbeat = xpc_online_heartbeat_uv,\n\t.heartbeat_init = xpc_heartbeat_init_uv,\n\t.heartbeat_exit = xpc_heartbeat_exit_uv,\n\t.get_remote_heartbeat = xpc_get_remote_heartbeat_uv,\n\n\t.request_partition_activation =\n\t\txpc_request_partition_activation_uv,\n\t.request_partition_reactivation =\n\t\txpc_request_partition_reactivation_uv,\n\t.request_partition_deactivation =\n\t\txpc_request_partition_deactivation_uv,\n\t.cancel_partition_deactivation_request =\n\t\txpc_cancel_partition_deactivation_request_uv,\n\n\t.setup_ch_structures = xpc_setup_ch_structures_uv,\n\t.teardown_ch_structures = xpc_teardown_ch_structures_uv,\n\n\t.make_first_contact = xpc_make_first_contact_uv,\n\n\t.get_chctl_all_flags = xpc_get_chctl_all_flags_uv,\n\t.send_chctl_closerequest = xpc_send_chctl_closerequest_uv,\n\t.send_chctl_closereply = xpc_send_chctl_closereply_uv,\n\t.send_chctl_openrequest = xpc_send_chctl_openrequest_uv,\n\t.send_chctl_openreply = xpc_send_chctl_openreply_uv,\n\t.send_chctl_opencomplete = xpc_send_chctl_opencomplete_uv,\n\t.process_msg_chctl_flags = xpc_process_msg_chctl_flags_uv,\n\n\t.save_remote_msgqueue_pa = xpc_save_remote_msgqueue_pa_uv,\n\n\t.setup_msg_structures = xpc_setup_msg_structures_uv,\n\t.teardown_msg_structures = xpc_teardown_msg_structures_uv,\n\n\t.indicate_partition_engaged = xpc_indicate_partition_engaged_uv,\n\t.indicate_partition_disengaged = xpc_indicate_partition_disengaged_uv,\n\t.assume_partition_disengaged = xpc_assume_partition_disengaged_uv,\n\t.partition_engaged = xpc_partition_engaged_uv,\n\t.any_partition_engaged = xpc_any_partition_engaged_uv,\n\n\t.n_of_deliverable_payloads = xpc_n_of_deliverable_payloads_uv,\n\t.send_payload = xpc_send_payload_uv,\n\t.get_deliverable_payload = xpc_get_deliverable_payload_uv,\n\t.received_payload = xpc_received_payload_uv,\n\t.notify_senders_of_disconnect = xpc_notify_senders_of_disconnect_uv,\n};\n\nstatic int\nxpc_init_mq_node(int nid)\n{\n\tint cpu;\n\n\tcpus_read_lock();\n\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\txpc_activate_mq_uv =\n\t\t\txpc_create_gru_mq_uv(XPC_ACTIVATE_MQ_SIZE_UV, nid,\n\t\t\t\t\t     XPC_ACTIVATE_IRQ_NAME,\n\t\t\t\t\t     xpc_handle_activate_IRQ_uv);\n\t\tif (!IS_ERR(xpc_activate_mq_uv))\n\t\t\tbreak;\n\t}\n\tif (IS_ERR(xpc_activate_mq_uv)) {\n\t\tcpus_read_unlock();\n\t\treturn PTR_ERR(xpc_activate_mq_uv);\n\t}\n\n\tfor_each_cpu(cpu, cpumask_of_node(nid)) {\n\t\txpc_notify_mq_uv =\n\t\t\txpc_create_gru_mq_uv(XPC_NOTIFY_MQ_SIZE_UV, nid,\n\t\t\t\t\t     XPC_NOTIFY_IRQ_NAME,\n\t\t\t\t\t     xpc_handle_notify_IRQ_uv);\n\t\tif (!IS_ERR(xpc_notify_mq_uv))\n\t\t\tbreak;\n\t}\n\tif (IS_ERR(xpc_notify_mq_uv)) {\n\t\txpc_destroy_gru_mq_uv(xpc_activate_mq_uv);\n\t\tcpus_read_unlock();\n\t\treturn PTR_ERR(xpc_notify_mq_uv);\n\t}\n\n\tcpus_read_unlock();\n\treturn 0;\n}\n\nint\nxpc_init_uv(void)\n{\n\tint nid;\n\tint ret = 0;\n\n\txpc_arch_ops = xpc_arch_ops_uv;\n\n\tif (sizeof(struct xpc_notify_mq_msghdr_uv) > XPC_MSG_HDR_MAX_SIZE) {\n\t\tdev_err(xpc_part, \"xpc_notify_mq_msghdr_uv is larger than %d\\n\",\n\t\t\tXPC_MSG_HDR_MAX_SIZE);\n\t\treturn -E2BIG;\n\t}\n\n\tif (xpc_mq_node < 0)\n\t\tfor_each_online_node(nid) {\n\t\t\tret = xpc_init_mq_node(nid);\n\n\t\t\tif (!ret)\n\t\t\t\tbreak;\n\t\t}\n\telse\n\t\tret = xpc_init_mq_node(xpc_mq_node);\n\n\tif (ret < 0)\n\t\tdev_err(xpc_part, \"xpc_init_mq_node() returned error=%d\\n\",\n\t\t\t-ret);\n\n\treturn ret;\n}\n\nvoid\nxpc_exit_uv(void)\n{\n\txpc_destroy_gru_mq_uv(xpc_notify_mq_uv);\n\txpc_destroy_gru_mq_uv(xpc_activate_mq_uv);\n}\n\nmodule_param(xpc_mq_node, int, 0);\nMODULE_PARM_DESC(xpc_mq_node, \"Node number on which to allocate message queues.\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}