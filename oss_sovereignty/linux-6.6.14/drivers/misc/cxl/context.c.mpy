{
  "module_name": "context.c",
  "hash_id": "bf7c0aeabd66db6751f0305393e5a6401a2cda5d127a46c7cde6cda1a0abc607",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/cxl/context.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/bitmap.h>\n#include <linux/sched.h>\n#include <linux/pid.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/debugfs.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/sched/mm.h>\n#include <linux/mmu_context.h>\n#include <asm/cputable.h>\n#include <asm/current.h>\n#include <asm/copro.h>\n\n#include \"cxl.h\"\n\n \nstruct cxl_context *cxl_context_alloc(void)\n{\n\treturn kzalloc(sizeof(struct cxl_context), GFP_KERNEL);\n}\n\n \nint cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)\n{\n\tint i;\n\n\tctx->afu = afu;\n\tctx->master = master;\n\tctx->pid = NULL;  \n\tmutex_init(&ctx->mapping_lock);\n\tctx->mapping = NULL;\n\tctx->tidr = 0;\n\tctx->assign_tidr = false;\n\n\tif (cxl_is_power8()) {\n\t\tspin_lock_init(&ctx->sste_lock);\n\n\t\t \n\t\ti = cxl_alloc_sst(ctx);\n\t\tif (i)\n\t\t\treturn i;\n\t}\n\n\tINIT_WORK(&ctx->fault_work, cxl_handle_fault);\n\n\tinit_waitqueue_head(&ctx->wq);\n\tspin_lock_init(&ctx->lock);\n\n\tctx->irq_bitmap = NULL;\n\tctx->pending_irq = false;\n\tctx->pending_fault = false;\n\tctx->pending_afu_err = false;\n\n\tINIT_LIST_HEAD(&ctx->irq_names);\n\n\t \n\tfor (i = 0; i < CXL_IRQ_RANGES; i++)\n\t\tctx->irqs.range[i] = 0;\n\n\tmutex_init(&ctx->status_mutex);\n\n\tctx->status = OPENED;\n\n\t \n\tmutex_lock(&afu->contexts_lock);\n\tidr_preload(GFP_KERNEL);\n\ti = idr_alloc(&ctx->afu->contexts_idr, ctx, 0,\n\t\t      ctx->afu->num_procs, GFP_NOWAIT);\n\tidr_preload_end();\n\tmutex_unlock(&afu->contexts_lock);\n\tif (i < 0)\n\t\treturn i;\n\n\tctx->pe = i;\n\tif (cpu_has_feature(CPU_FTR_HVMODE)) {\n\t\tctx->elem = &ctx->afu->native->spa[i];\n\t\tctx->external_pe = ctx->pe;\n\t} else {\n\t\tctx->external_pe = -1;  \n\t}\n\tctx->pe_inserted = false;\n\n\t \n\tcxl_afu_get(afu);\n\treturn 0;\n}\n\nvoid cxl_context_set_mapping(struct cxl_context *ctx,\n\t\t\tstruct address_space *mapping)\n{\n\tmutex_lock(&ctx->mapping_lock);\n\tctx->mapping = mapping;\n\tmutex_unlock(&ctx->mapping_lock);\n}\n\nstatic vm_fault_t cxl_mmap_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct cxl_context *ctx = vma->vm_file->private_data;\n\tu64 area, offset;\n\tvm_fault_t ret;\n\n\toffset = vmf->pgoff << PAGE_SHIFT;\n\n\tpr_devel(\"%s: pe: %i address: 0x%lx offset: 0x%llx\\n\",\n\t\t\t__func__, ctx->pe, vmf->address, offset);\n\n\tif (ctx->afu->current_mode == CXL_MODE_DEDICATED) {\n\t\tarea = ctx->afu->psn_phys;\n\t\tif (offset >= ctx->afu->adapter->ps_size)\n\t\t\treturn VM_FAULT_SIGBUS;\n\t} else {\n\t\tarea = ctx->psn_phys;\n\t\tif (offset >= ctx->psn_size)\n\t\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tmutex_lock(&ctx->status_mutex);\n\n\tif (ctx->status != STARTED) {\n\t\tmutex_unlock(&ctx->status_mutex);\n\t\tpr_devel(\"%s: Context not started, failing problem state access\\n\", __func__);\n\t\tif (ctx->mmio_err_ff) {\n\t\t\tif (!ctx->ff_page) {\n\t\t\t\tctx->ff_page = alloc_page(GFP_USER);\n\t\t\t\tif (!ctx->ff_page)\n\t\t\t\t\treturn VM_FAULT_OOM;\n\t\t\t\tmemset(page_address(ctx->ff_page), 0xff, PAGE_SIZE);\n\t\t\t}\n\t\t\tget_page(ctx->ff_page);\n\t\t\tvmf->page = ctx->ff_page;\n\t\t\tvma->vm_page_prot = pgprot_cached(vma->vm_page_prot);\n\t\t\treturn 0;\n\t\t}\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tret = vmf_insert_pfn(vma, vmf->address, (area + offset) >> PAGE_SHIFT);\n\n\tmutex_unlock(&ctx->status_mutex);\n\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct cxl_mmap_vmops = {\n\t.fault = cxl_mmap_fault,\n};\n\n \nint cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)\n{\n\tu64 start = vma->vm_pgoff << PAGE_SHIFT;\n\tu64 len = vma->vm_end - vma->vm_start;\n\n\tif (ctx->afu->current_mode == CXL_MODE_DEDICATED) {\n\t\tif (start + len > ctx->afu->adapter->ps_size)\n\t\t\treturn -EINVAL;\n\n\t\tif (cxl_is_power9()) {\n\t\t\t \n\t\t\tif (ctx->master && !ctx->afu->psa) {\n\t\t\t\tpr_devel(\"AFU doesn't support mmio space\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!ctx->afu->enabled)\n\t\t\t\treturn -EBUSY;\n\t\t}\n\t} else {\n\t\tif (start + len > ctx->psn_size)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif ((ctx->master && !ctx->afu->psa) || (!ctx->afu->pp_psa)) {\n\t\t\tpr_devel(\"AFU doesn't support mmio space\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (!ctx->afu->enabled)\n\t\t\treturn -EBUSY;\n\t}\n\n\tpr_devel(\"%s: mmio physical: %llx pe: %i master:%i\\n\", __func__,\n\t\t ctx->psn_phys, ctx->pe , ctx->master);\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP);\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tvma->vm_ops = &cxl_mmap_vmops;\n\treturn 0;\n}\n\n \nint __detach_context(struct cxl_context *ctx)\n{\n\tenum cxl_context_status status;\n\n\tmutex_lock(&ctx->status_mutex);\n\tstatus = ctx->status;\n\tctx->status = CLOSED;\n\tmutex_unlock(&ctx->status_mutex);\n\tif (status != STARTED)\n\t\treturn -EBUSY;\n\n\t \n\tWARN_ON(cxl_ops->detach_process(ctx) &&\n\t\tcxl_ops->link_ok(ctx->afu->adapter, ctx->afu));\n\tflush_work(&ctx->fault_work);  \n\n\t \n\tif (cxl_ops->irq_wait)\n\t\tcxl_ops->irq_wait(ctx);\n\n\t \n\tput_pid(ctx->pid);\n\n\tcxl_ctx_put();\n\n\t \n\tcxl_adapter_context_put(ctx->afu->adapter);\n\n\t \n\tcxl_context_mm_count_put(ctx);\n\tif (ctx->mm)\n\t\tmm_context_remove_copro(ctx->mm);\n\tctx->mm = NULL;\n\n\treturn 0;\n}\n\n \nvoid cxl_context_detach(struct cxl_context *ctx)\n{\n\tint rc;\n\n\trc = __detach_context(ctx);\n\tif (rc)\n\t\treturn;\n\n\tafu_release_irqs(ctx, ctx);\n\twake_up_all(&ctx->wq);\n}\n\n \nvoid cxl_context_detach_all(struct cxl_afu *afu)\n{\n\tstruct cxl_context *ctx;\n\tint tmp;\n\n\tmutex_lock(&afu->contexts_lock);\n\tidr_for_each_entry(&afu->contexts_idr, ctx, tmp) {\n\t\t \n\t\tcxl_context_detach(ctx);\n\n\t\t \n\t\tmutex_lock(&ctx->mapping_lock);\n\t\tif (ctx->mapping)\n\t\t\tunmap_mapping_range(ctx->mapping, 0, 0, 1);\n\t\tmutex_unlock(&ctx->mapping_lock);\n\t}\n\tmutex_unlock(&afu->contexts_lock);\n}\n\nstatic void reclaim_ctx(struct rcu_head *rcu)\n{\n\tstruct cxl_context *ctx = container_of(rcu, struct cxl_context, rcu);\n\n\tif (cxl_is_power8())\n\t\tfree_page((u64)ctx->sstp);\n\tif (ctx->ff_page)\n\t\t__free_page(ctx->ff_page);\n\tctx->sstp = NULL;\n\n\tbitmap_free(ctx->irq_bitmap);\n\n\t \n\tcxl_afu_put(ctx->afu);\n\n\tkfree(ctx);\n}\n\nvoid cxl_context_free(struct cxl_context *ctx)\n{\n\tif (ctx->kernelapi && ctx->mapping)\n\t\tcxl_release_mapping(ctx);\n\tmutex_lock(&ctx->afu->contexts_lock);\n\tidr_remove(&ctx->afu->contexts_idr, ctx->pe);\n\tmutex_unlock(&ctx->afu->contexts_lock);\n\tcall_rcu(&ctx->rcu, reclaim_ctx);\n}\n\nvoid cxl_context_mm_count_get(struct cxl_context *ctx)\n{\n\tif (ctx->mm)\n\t\tmmgrab(ctx->mm);\n}\n\nvoid cxl_context_mm_count_put(struct cxl_context *ctx)\n{\n\tif (ctx->mm)\n\t\tmmdrop(ctx->mm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}