{
  "module_name": "guest.c",
  "hash_id": "43c7bb43a2b3c4a32789cc810dee0dbe93c741fa734d996c01c501bc0f7f91d2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/cxl/guest.c",
  "human_readable_source": "\n \n\n#include <linux/spinlock.h>\n#include <linux/uaccess.h>\n#include <linux/delay.h>\n#include <linux/irqdomain.h>\n#include <linux/platform_device.h>\n\n#include \"cxl.h\"\n#include \"hcalls.h\"\n#include \"trace.h\"\n\n#define CXL_ERROR_DETECTED_EVENT\t1\n#define CXL_SLOT_RESET_EVENT\t\t2\n#define CXL_RESUME_EVENT\t\t3\n\nstatic void pci_error_handlers(struct cxl_afu *afu,\n\t\t\t\tint bus_error_event,\n\t\t\t\tpci_channel_state_t state)\n{\n\tstruct pci_dev *afu_dev;\n\tstruct pci_driver *afu_drv;\n\tconst struct pci_error_handlers *err_handler;\n\n\tif (afu->phb == NULL)\n\t\treturn;\n\n\tlist_for_each_entry(afu_dev, &afu->phb->bus->devices, bus_list) {\n\t\tafu_drv = to_pci_driver(afu_dev->dev.driver);\n\t\tif (!afu_drv)\n\t\t\tcontinue;\n\n\t\terr_handler = afu_drv->err_handler;\n\t\tswitch (bus_error_event) {\n\t\tcase CXL_ERROR_DETECTED_EVENT:\n\t\t\tafu_dev->error_state = state;\n\n\t\t\tif (err_handler &&\n\t\t\t    err_handler->error_detected)\n\t\t\t\terr_handler->error_detected(afu_dev, state);\n\t\t\tbreak;\n\t\tcase CXL_SLOT_RESET_EVENT:\n\t\t\tafu_dev->error_state = state;\n\n\t\t\tif (err_handler &&\n\t\t\t    err_handler->slot_reset)\n\t\t\t\terr_handler->slot_reset(afu_dev);\n\t\t\tbreak;\n\t\tcase CXL_RESUME_EVENT:\n\t\t\tif (err_handler &&\n\t\t\t    err_handler->resume)\n\t\t\t\terr_handler->resume(afu_dev);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic irqreturn_t guest_handle_psl_slice_error(struct cxl_context *ctx, u64 dsisr,\n\t\t\t\t\tu64 errstat)\n{\n\tpr_devel(\"in %s\\n\", __func__);\n\tdev_crit(&ctx->afu->dev, \"PSL ERROR STATUS: 0x%.16llx\\n\", errstat);\n\n\treturn cxl_ops->ack_irq(ctx, 0, errstat);\n}\n\nstatic ssize_t guest_collect_vpd(struct cxl *adapter, struct cxl_afu *afu,\n\t\t\tvoid *buf, size_t len)\n{\n\tunsigned int entries, mod;\n\tunsigned long **vpd_buf = NULL;\n\tstruct sg_list *le;\n\tint rc = 0, i, tocopy;\n\tu64 out = 0;\n\n\tif (buf == NULL)\n\t\treturn -EINVAL;\n\n\t \n\tentries = len / SG_BUFFER_SIZE;\n\tmod = len % SG_BUFFER_SIZE;\n\tif (mod)\n\t\tentries++;\n\n\tif (entries > SG_MAX_ENTRIES) {\n\t\tentries = SG_MAX_ENTRIES;\n\t\tlen = SG_MAX_ENTRIES * SG_BUFFER_SIZE;\n\t\tmod = 0;\n\t}\n\n\tvpd_buf = kcalloc(entries, sizeof(unsigned long *), GFP_KERNEL);\n\tif (!vpd_buf)\n\t\treturn -ENOMEM;\n\n\tle = (struct sg_list *)get_zeroed_page(GFP_KERNEL);\n\tif (!le) {\n\t\trc = -ENOMEM;\n\t\tgoto err1;\n\t}\n\n\tfor (i = 0; i < entries; i++) {\n\t\tvpd_buf[i] = (unsigned long *)get_zeroed_page(GFP_KERNEL);\n\t\tif (!vpd_buf[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err2;\n\t\t}\n\t\tle[i].phys_addr = cpu_to_be64(virt_to_phys(vpd_buf[i]));\n\t\tle[i].len = cpu_to_be64(SG_BUFFER_SIZE);\n\t\tif ((i == (entries - 1)) && mod)\n\t\t\tle[i].len = cpu_to_be64(mod);\n\t}\n\n\tif (adapter)\n\t\trc = cxl_h_collect_vpd_adapter(adapter->guest->handle,\n\t\t\t\t\tvirt_to_phys(le), entries, &out);\n\telse\n\t\trc = cxl_h_collect_vpd(afu->guest->handle, 0,\n\t\t\t\tvirt_to_phys(le), entries, &out);\n\tpr_devel(\"length of available (entries: %i), vpd: %#llx\\n\",\n\t\tentries, out);\n\n\tif (!rc) {\n\t\t \n\t\tif (out < len)\n\t\t\tlen = out;\n\t\trc = len;\n\t\tif (out) {\n\t\t\tfor (i = 0; i < entries; i++) {\n\t\t\t\tif (len < SG_BUFFER_SIZE)\n\t\t\t\t\ttocopy = len;\n\t\t\t\telse\n\t\t\t\t\ttocopy = SG_BUFFER_SIZE;\n\t\t\t\tmemcpy(buf, vpd_buf[i], tocopy);\n\t\t\t\tbuf += tocopy;\n\t\t\t\tlen -= tocopy;\n\t\t\t}\n\t\t}\n\t}\nerr2:\n\tfor (i = 0; i < entries; i++) {\n\t\tif (vpd_buf[i])\n\t\t\tfree_page((unsigned long) vpd_buf[i]);\n\t}\n\tfree_page((unsigned long) le);\nerr1:\n\tkfree(vpd_buf);\n\treturn rc;\n}\n\nstatic int guest_get_irq_info(struct cxl_context *ctx, struct cxl_irq_info *info)\n{\n\treturn cxl_h_collect_int_info(ctx->afu->guest->handle, ctx->process_token, info);\n}\n\nstatic irqreturn_t guest_psl_irq(int irq, void *data)\n{\n\tstruct cxl_context *ctx = data;\n\tstruct cxl_irq_info irq_info;\n\tint rc;\n\n\tpr_devel(\"%d: received PSL interrupt %i\\n\", ctx->pe, irq);\n\trc = guest_get_irq_info(ctx, &irq_info);\n\tif (rc) {\n\t\tWARN(1, \"Unable to get IRQ info: %i\\n\", rc);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\trc = cxl_irq_psl8(irq, ctx, &irq_info);\n\treturn rc;\n}\n\nstatic int afu_read_error_state(struct cxl_afu *afu, int *state_out)\n{\n\tu64 state;\n\tint rc = 0;\n\n\tif (!afu)\n\t\treturn -EIO;\n\n\trc = cxl_h_read_error_state(afu->guest->handle, &state);\n\tif (!rc) {\n\t\tWARN_ON(state != H_STATE_NORMAL &&\n\t\t\tstate != H_STATE_DISABLE &&\n\t\t\tstate != H_STATE_TEMP_UNAVAILABLE &&\n\t\t\tstate != H_STATE_PERM_UNAVAILABLE);\n\t\t*state_out = state & 0xffffffff;\n\t}\n\treturn rc;\n}\n\nstatic irqreturn_t guest_slice_irq_err(int irq, void *data)\n{\n\tstruct cxl_afu *afu = data;\n\tint rc;\n\tu64 serr, afu_error, dsisr;\n\n\trc = cxl_h_get_fn_error_interrupt(afu->guest->handle, &serr);\n\tif (rc) {\n\t\tdev_crit(&afu->dev, \"Couldn't read PSL_SERR_An: %d\\n\", rc);\n\t\treturn IRQ_HANDLED;\n\t}\n\tafu_error = cxl_p2n_read(afu, CXL_AFU_ERR_An);\n\tdsisr = cxl_p2n_read(afu, CXL_PSL_DSISR_An);\n\tcxl_afu_decode_psl_serr(afu, serr);\n\tdev_crit(&afu->dev, \"AFU_ERR_An: 0x%.16llx\\n\", afu_error);\n\tdev_crit(&afu->dev, \"PSL_DSISR_An: 0x%.16llx\\n\", dsisr);\n\n\trc = cxl_h_ack_fn_error_interrupt(afu->guest->handle, serr);\n\tif (rc)\n\t\tdev_crit(&afu->dev, \"Couldn't ack slice error interrupt: %d\\n\",\n\t\t\trc);\n\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int irq_alloc_range(struct cxl *adapter, int len, int *irq)\n{\n\tint i, n;\n\tstruct irq_avail *cur;\n\n\tfor (i = 0; i < adapter->guest->irq_nranges; i++) {\n\t\tcur = &adapter->guest->irq_avail[i];\n\t\tn = bitmap_find_next_zero_area(cur->bitmap, cur->range,\n\t\t\t\t\t0, len, 0);\n\t\tif (n < cur->range) {\n\t\t\tbitmap_set(cur->bitmap, n, len);\n\t\t\t*irq = cur->offset + n;\n\t\t\tpr_devel(\"guest: allocate IRQs %#x->%#x\\n\",\n\t\t\t\t*irq, *irq + len - 1);\n\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOSPC;\n}\n\nstatic int irq_free_range(struct cxl *adapter, int irq, int len)\n{\n\tint i, n;\n\tstruct irq_avail *cur;\n\n\tif (len == 0)\n\t\treturn -ENOENT;\n\n\tfor (i = 0; i < adapter->guest->irq_nranges; i++) {\n\t\tcur = &adapter->guest->irq_avail[i];\n\t\tif (irq >= cur->offset &&\n\t\t\t(irq + len) <= (cur->offset + cur->range)) {\n\t\t\tn = irq - cur->offset;\n\t\t\tbitmap_clear(cur->bitmap, n, len);\n\t\t\tpr_devel(\"guest: release IRQs %#x->%#x\\n\",\n\t\t\t\tirq, irq + len - 1);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -ENOENT;\n}\n\nstatic int guest_reset(struct cxl *adapter)\n{\n\tstruct cxl_afu *afu = NULL;\n\tint i, rc;\n\n\tpr_devel(\"Adapter reset request\\n\");\n\tspin_lock(&adapter->afu_list_lock);\n\tfor (i = 0; i < adapter->slices; i++) {\n\t\tif ((afu = adapter->afu[i])) {\n\t\t\tpci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\n\t\t\t\t\tpci_channel_io_frozen);\n\t\t\tcxl_context_detach_all(afu);\n\t\t}\n\t}\n\n\trc = cxl_h_reset_adapter(adapter->guest->handle);\n\tfor (i = 0; i < adapter->slices; i++) {\n\t\tif (!rc && (afu = adapter->afu[i])) {\n\t\t\tpci_error_handlers(afu, CXL_SLOT_RESET_EVENT,\n\t\t\t\t\tpci_channel_io_normal);\n\t\t\tpci_error_handlers(afu, CXL_RESUME_EVENT, 0);\n\t\t}\n\t}\n\tspin_unlock(&adapter->afu_list_lock);\n\treturn rc;\n}\n\nstatic int guest_alloc_one_irq(struct cxl *adapter)\n{\n\tint irq;\n\n\tspin_lock(&adapter->guest->irq_alloc_lock);\n\tif (irq_alloc_range(adapter, 1, &irq))\n\t\tirq = -ENOSPC;\n\tspin_unlock(&adapter->guest->irq_alloc_lock);\n\treturn irq;\n}\n\nstatic void guest_release_one_irq(struct cxl *adapter, int irq)\n{\n\tspin_lock(&adapter->guest->irq_alloc_lock);\n\tirq_free_range(adapter, irq, 1);\n\tspin_unlock(&adapter->guest->irq_alloc_lock);\n}\n\nstatic int guest_alloc_irq_ranges(struct cxl_irq_ranges *irqs,\n\t\t\t\tstruct cxl *adapter, unsigned int num)\n{\n\tint i, try, irq;\n\n\tmemset(irqs, 0, sizeof(struct cxl_irq_ranges));\n\n\tspin_lock(&adapter->guest->irq_alloc_lock);\n\tfor (i = 0; i < CXL_IRQ_RANGES && num; i++) {\n\t\ttry = num;\n\t\twhile (try) {\n\t\t\tif (irq_alloc_range(adapter, try, &irq) == 0)\n\t\t\t\tbreak;\n\t\t\ttry /= 2;\n\t\t}\n\t\tif (!try)\n\t\t\tgoto error;\n\t\tirqs->offset[i] = irq;\n\t\tirqs->range[i] = try;\n\t\tnum -= try;\n\t}\n\tif (num)\n\t\tgoto error;\n\tspin_unlock(&adapter->guest->irq_alloc_lock);\n\treturn 0;\n\nerror:\n\tfor (i = 0; i < CXL_IRQ_RANGES; i++)\n\t\tirq_free_range(adapter, irqs->offset[i], irqs->range[i]);\n\tspin_unlock(&adapter->guest->irq_alloc_lock);\n\treturn -ENOSPC;\n}\n\nstatic void guest_release_irq_ranges(struct cxl_irq_ranges *irqs,\n\t\t\t\tstruct cxl *adapter)\n{\n\tint i;\n\n\tspin_lock(&adapter->guest->irq_alloc_lock);\n\tfor (i = 0; i < CXL_IRQ_RANGES; i++)\n\t\tirq_free_range(adapter, irqs->offset[i], irqs->range[i]);\n\tspin_unlock(&adapter->guest->irq_alloc_lock);\n}\n\nstatic int guest_register_serr_irq(struct cxl_afu *afu)\n{\n\tafu->err_irq_name = kasprintf(GFP_KERNEL, \"cxl-%s-err\",\n\t\t\t\t      dev_name(&afu->dev));\n\tif (!afu->err_irq_name)\n\t\treturn -ENOMEM;\n\n\tif (!(afu->serr_virq = cxl_map_irq(afu->adapter, afu->serr_hwirq,\n\t\t\t\t guest_slice_irq_err, afu, afu->err_irq_name))) {\n\t\tkfree(afu->err_irq_name);\n\t\tafu->err_irq_name = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void guest_release_serr_irq(struct cxl_afu *afu)\n{\n\tcxl_unmap_irq(afu->serr_virq, afu);\n\tcxl_ops->release_one_irq(afu->adapter, afu->serr_hwirq);\n\tkfree(afu->err_irq_name);\n}\n\nstatic int guest_ack_irq(struct cxl_context *ctx, u64 tfc, u64 psl_reset_mask)\n{\n\treturn cxl_h_control_faults(ctx->afu->guest->handle, ctx->process_token,\n\t\t\t\ttfc >> 32, (psl_reset_mask != 0));\n}\n\nstatic void disable_afu_irqs(struct cxl_context *ctx)\n{\n\tirq_hw_number_t hwirq;\n\tunsigned int virq;\n\tint r, i;\n\n\tpr_devel(\"Disabling AFU(%d) interrupts\\n\", ctx->afu->slice);\n\tfor (r = 0; r < CXL_IRQ_RANGES; r++) {\n\t\thwirq = ctx->irqs.offset[r];\n\t\tfor (i = 0; i < ctx->irqs.range[r]; hwirq++, i++) {\n\t\t\tvirq = irq_find_mapping(NULL, hwirq);\n\t\t\tdisable_irq(virq);\n\t\t}\n\t}\n}\n\nstatic void enable_afu_irqs(struct cxl_context *ctx)\n{\n\tirq_hw_number_t hwirq;\n\tunsigned int virq;\n\tint r, i;\n\n\tpr_devel(\"Enabling AFU(%d) interrupts\\n\", ctx->afu->slice);\n\tfor (r = 0; r < CXL_IRQ_RANGES; r++) {\n\t\thwirq = ctx->irqs.offset[r];\n\t\tfor (i = 0; i < ctx->irqs.range[r]; hwirq++, i++) {\n\t\t\tvirq = irq_find_mapping(NULL, hwirq);\n\t\t\tenable_irq(virq);\n\t\t}\n\t}\n}\n\nstatic int _guest_afu_cr_readXX(int sz, struct cxl_afu *afu, int cr_idx,\n\t\t\tu64 offset, u64 *val)\n{\n\tunsigned long cr;\n\tchar c;\n\tint rc = 0;\n\n\tif (afu->crs_len < sz)\n\t\treturn -ENOENT;\n\n\tif (unlikely(offset >= afu->crs_len))\n\t\treturn -ERANGE;\n\n\tcr = get_zeroed_page(GFP_KERNEL);\n\tif (!cr)\n\t\treturn -ENOMEM;\n\n\trc = cxl_h_get_config(afu->guest->handle, cr_idx, offset,\n\t\t\tvirt_to_phys((void *)cr), sz);\n\tif (rc)\n\t\tgoto err;\n\n\tswitch (sz) {\n\tcase 1:\n\t\tc = *((char *) cr);\n\t\t*val = c;\n\t\tbreak;\n\tcase 2:\n\t\t*val = in_le16((u16 *)cr);\n\t\tbreak;\n\tcase 4:\n\t\t*val = in_le32((unsigned *)cr);\n\t\tbreak;\n\tcase 8:\n\t\t*val = in_le64((u64 *)cr);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t}\nerr:\n\tfree_page(cr);\n\treturn rc;\n}\n\nstatic int guest_afu_cr_read32(struct cxl_afu *afu, int cr_idx, u64 offset,\n\t\t\tu32 *out)\n{\n\tint rc;\n\tu64 val;\n\n\trc = _guest_afu_cr_readXX(4, afu, cr_idx, offset, &val);\n\tif (!rc)\n\t\t*out = (u32) val;\n\treturn rc;\n}\n\nstatic int guest_afu_cr_read16(struct cxl_afu *afu, int cr_idx, u64 offset,\n\t\t\tu16 *out)\n{\n\tint rc;\n\tu64 val;\n\n\trc = _guest_afu_cr_readXX(2, afu, cr_idx, offset, &val);\n\tif (!rc)\n\t\t*out = (u16) val;\n\treturn rc;\n}\n\nstatic int guest_afu_cr_read8(struct cxl_afu *afu, int cr_idx, u64 offset,\n\t\t\tu8 *out)\n{\n\tint rc;\n\tu64 val;\n\n\trc = _guest_afu_cr_readXX(1, afu, cr_idx, offset, &val);\n\tif (!rc)\n\t\t*out = (u8) val;\n\treturn rc;\n}\n\nstatic int guest_afu_cr_read64(struct cxl_afu *afu, int cr_idx, u64 offset,\n\t\t\tu64 *out)\n{\n\treturn _guest_afu_cr_readXX(8, afu, cr_idx, offset, out);\n}\n\nstatic int guest_afu_cr_write32(struct cxl_afu *afu, int cr, u64 off, u32 in)\n{\n\t \n\treturn -EPERM;\n}\n\nstatic int guest_afu_cr_write16(struct cxl_afu *afu, int cr, u64 off, u16 in)\n{\n\t \n\treturn -EPERM;\n}\n\nstatic int guest_afu_cr_write8(struct cxl_afu *afu, int cr, u64 off, u8 in)\n{\n\t \n\treturn -EPERM;\n}\n\nstatic int attach_afu_directed(struct cxl_context *ctx, u64 wed, u64 amr)\n{\n\tstruct cxl_process_element_hcall *elem;\n\tstruct cxl *adapter = ctx->afu->adapter;\n\tconst struct cred *cred;\n\tu32 pid, idx;\n\tint rc, r, i;\n\tu64 mmio_addr, mmio_size;\n\t__be64 flags = 0;\n\n\t \n\tif (!(elem = (struct cxl_process_element_hcall *)\n\t\t\tget_zeroed_page(GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\telem->version = cpu_to_be64(CXL_PROCESS_ELEMENT_VERSION);\n\tif (ctx->kernel) {\n\t\tpid = 0;\n\t\tflags |= CXL_PE_TRANSLATION_ENABLED;\n\t\tflags |= CXL_PE_PRIVILEGED_PROCESS;\n\t\tif (mfmsr() & MSR_SF)\n\t\t\tflags |= CXL_PE_64_BIT;\n\t} else {\n\t\tpid = current->pid;\n\t\tflags |= CXL_PE_PROBLEM_STATE;\n\t\tflags |= CXL_PE_TRANSLATION_ENABLED;\n\t\tif (!test_tsk_thread_flag(current, TIF_32BIT))\n\t\t\tflags |= CXL_PE_64_BIT;\n\t\tcred = get_current_cred();\n\t\tif (uid_eq(cred->euid, GLOBAL_ROOT_UID))\n\t\t\tflags |= CXL_PE_PRIVILEGED_PROCESS;\n\t\tput_cred(cred);\n\t}\n\telem->flags         = cpu_to_be64(flags);\n\telem->common.tid    = cpu_to_be32(0);  \n\telem->common.pid    = cpu_to_be32(pid);\n\telem->common.csrp   = cpu_to_be64(0);  \n\telem->common.u.psl8.aurp0  = cpu_to_be64(0);  \n\telem->common.u.psl8.aurp1  = cpu_to_be64(0);  \n\n\tcxl_prefault(ctx, wed);\n\n\telem->common.u.psl8.sstp0  = cpu_to_be64(ctx->sstp0);\n\telem->common.u.psl8.sstp1  = cpu_to_be64(ctx->sstp1);\n\n\t \n\tif (ctx->irqs.range[0] == 0) {\n\t\trc = afu_register_irqs(ctx, 0);\n\t\tif (rc)\n\t\t\tgoto out_free;\n\t}\n\n\tfor (r = 0; r < CXL_IRQ_RANGES; r++) {\n\t\tfor (i = 0; i < ctx->irqs.range[r]; i++) {\n\t\t\tif (r == 0 && i == 0) {\n\t\t\t\telem->pslVirtualIsn = cpu_to_be32(ctx->irqs.offset[0]);\n\t\t\t} else {\n\t\t\t\tidx = ctx->irqs.offset[r] + i - adapter->guest->irq_base_offset;\n\t\t\t\telem->applicationVirtualIsnBitmap[idx / 8] |= 0x80 >> (idx % 8);\n\t\t\t}\n\t\t}\n\t}\n\telem->common.amr = cpu_to_be64(amr);\n\telem->common.wed = cpu_to_be64(wed);\n\n\tdisable_afu_irqs(ctx);\n\n\trc = cxl_h_attach_process(ctx->afu->guest->handle, elem,\n\t\t\t\t&ctx->process_token, &mmio_addr, &mmio_size);\n\tif (rc == H_SUCCESS) {\n\t\tif (ctx->master || !ctx->afu->pp_psa) {\n\t\t\tctx->psn_phys = ctx->afu->psn_phys;\n\t\t\tctx->psn_size = ctx->afu->adapter->ps_size;\n\t\t} else {\n\t\t\tctx->psn_phys = mmio_addr;\n\t\t\tctx->psn_size = mmio_size;\n\t\t}\n\t\tif (ctx->afu->pp_psa && mmio_size &&\n\t\t\tctx->afu->pp_size == 0) {\n\t\t\t \n\t\t\tctx->afu->pp_size = mmio_size;\n\t\t}\n\t\t \n\t\tctx->external_pe = ctx->process_token & 0xFFFFFFFF;\n\t\tpr_devel(\"CXL pe=%i is known as %i for pHyp, mmio_size=%#llx\",\n\t\t\tctx->pe, ctx->external_pe, ctx->psn_size);\n\t\tctx->pe_inserted = true;\n\t\tenable_afu_irqs(ctx);\n\t}\n\nout_free:\n\tfree_page((u64)elem);\n\treturn rc;\n}\n\nstatic int guest_attach_process(struct cxl_context *ctx, bool kernel, u64 wed, u64 amr)\n{\n\tpr_devel(\"in %s\\n\", __func__);\n\n\tctx->kernel = kernel;\n\tif (ctx->afu->current_mode == CXL_MODE_DIRECTED)\n\t\treturn attach_afu_directed(ctx, wed, amr);\n\n\t \n\n\treturn -EINVAL;\n}\n\nstatic int detach_afu_directed(struct cxl_context *ctx)\n{\n\tif (!ctx->pe_inserted)\n\t\treturn 0;\n\tif (cxl_h_detach_process(ctx->afu->guest->handle, ctx->process_token))\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic int guest_detach_process(struct cxl_context *ctx)\n{\n\tpr_devel(\"in %s\\n\", __func__);\n\ttrace_cxl_detach(ctx);\n\n\tif (!cxl_ops->link_ok(ctx->afu->adapter, ctx->afu))\n\t\treturn -EIO;\n\n\tif (ctx->afu->current_mode == CXL_MODE_DIRECTED)\n\t\treturn detach_afu_directed(ctx);\n\n\treturn -EINVAL;\n}\n\nstatic void guest_release_afu(struct device *dev)\n{\n\tstruct cxl_afu *afu = to_cxl_afu(dev);\n\n\tpr_devel(\"%s\\n\", __func__);\n\n\tidr_destroy(&afu->contexts_idr);\n\n\tkfree(afu->guest);\n\tkfree(afu);\n}\n\nssize_t cxl_guest_read_afu_vpd(struct cxl_afu *afu, void *buf, size_t len)\n{\n\treturn guest_collect_vpd(NULL, afu, buf, len);\n}\n\n#define ERR_BUFF_MAX_COPY_SIZE PAGE_SIZE\nstatic ssize_t guest_afu_read_err_buffer(struct cxl_afu *afu, char *buf,\n\t\t\t\t\tloff_t off, size_t count)\n{\n\tvoid *tbuf = NULL;\n\tint rc = 0;\n\n\ttbuf = (void *) get_zeroed_page(GFP_KERNEL);\n\tif (!tbuf)\n\t\treturn -ENOMEM;\n\n\trc = cxl_h_get_afu_err(afu->guest->handle,\n\t\t\t       off & 0x7,\n\t\t\t       virt_to_phys(tbuf),\n\t\t\t       count);\n\tif (rc)\n\t\tgoto err;\n\n\tif (count > ERR_BUFF_MAX_COPY_SIZE)\n\t\tcount = ERR_BUFF_MAX_COPY_SIZE - (off & 0x7);\n\tmemcpy(buf, tbuf, count);\nerr:\n\tfree_page((u64)tbuf);\n\n\treturn rc;\n}\n\nstatic int guest_afu_check_and_enable(struct cxl_afu *afu)\n{\n\treturn 0;\n}\n\nstatic bool guest_support_attributes(const char *attr_name,\n\t\t\t\t     enum cxl_attrs type)\n{\n\tswitch (type) {\n\tcase CXL_ADAPTER_ATTRS:\n\t\tif ((strcmp(attr_name, \"base_image\") == 0) ||\n\t\t\t(strcmp(attr_name, \"load_image_on_perst\") == 0) ||\n\t\t\t(strcmp(attr_name, \"perst_reloads_same_image\") == 0) ||\n\t\t\t(strcmp(attr_name, \"image_loaded\") == 0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase CXL_AFU_MASTER_ATTRS:\n\t\tif ((strcmp(attr_name, \"pp_mmio_off\") == 0))\n\t\t\treturn false;\n\t\tbreak;\n\tcase CXL_AFU_ATTRS:\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn true;\n}\n\nstatic int activate_afu_directed(struct cxl_afu *afu)\n{\n\tint rc;\n\n\tdev_info(&afu->dev, \"Activating AFU(%d) directed mode\\n\", afu->slice);\n\n\tafu->current_mode = CXL_MODE_DIRECTED;\n\n\tafu->num_procs = afu->max_procs_virtualised;\n\n\tif ((rc = cxl_chardev_m_afu_add(afu)))\n\t\treturn rc;\n\n\tif ((rc = cxl_sysfs_afu_m_add(afu)))\n\t\tgoto err;\n\n\tif ((rc = cxl_chardev_s_afu_add(afu)))\n\t\tgoto err1;\n\n\treturn 0;\nerr1:\n\tcxl_sysfs_afu_m_remove(afu);\nerr:\n\tcxl_chardev_afu_remove(afu);\n\treturn rc;\n}\n\nstatic int guest_afu_activate_mode(struct cxl_afu *afu, int mode)\n{\n\tif (!mode)\n\t\treturn 0;\n\tif (!(mode & afu->modes_supported))\n\t\treturn -EINVAL;\n\n\tif (mode == CXL_MODE_DIRECTED)\n\t\treturn activate_afu_directed(afu);\n\n\tif (mode == CXL_MODE_DEDICATED)\n\t\tdev_err(&afu->dev, \"Dedicated mode not supported\\n\");\n\n\treturn -EINVAL;\n}\n\nstatic int deactivate_afu_directed(struct cxl_afu *afu)\n{\n\tdev_info(&afu->dev, \"Deactivating AFU(%d) directed mode\\n\", afu->slice);\n\n\tafu->current_mode = 0;\n\tafu->num_procs = 0;\n\n\tcxl_sysfs_afu_m_remove(afu);\n\tcxl_chardev_afu_remove(afu);\n\n\tcxl_ops->afu_reset(afu);\n\n\treturn 0;\n}\n\nstatic int guest_afu_deactivate_mode(struct cxl_afu *afu, int mode)\n{\n\tif (!mode)\n\t\treturn 0;\n\tif (!(mode & afu->modes_supported))\n\t\treturn -EINVAL;\n\n\tif (mode == CXL_MODE_DIRECTED)\n\t\treturn deactivate_afu_directed(afu);\n\treturn 0;\n}\n\nstatic int guest_afu_reset(struct cxl_afu *afu)\n{\n\tpr_devel(\"AFU(%d) reset request\\n\", afu->slice);\n\treturn cxl_h_reset_afu(afu->guest->handle);\n}\n\nstatic int guest_map_slice_regs(struct cxl_afu *afu)\n{\n\tif (!(afu->p2n_mmio = ioremap(afu->guest->p2n_phys, afu->guest->p2n_size))) {\n\t\tdev_err(&afu->dev, \"Error mapping AFU(%d) MMIO regions\\n\",\n\t\t\tafu->slice);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void guest_unmap_slice_regs(struct cxl_afu *afu)\n{\n\tif (afu->p2n_mmio)\n\t\tiounmap(afu->p2n_mmio);\n}\n\nstatic int afu_update_state(struct cxl_afu *afu)\n{\n\tint rc, cur_state;\n\n\trc = afu_read_error_state(afu, &cur_state);\n\tif (rc)\n\t\treturn rc;\n\n\tif (afu->guest->previous_state == cur_state)\n\t\treturn 0;\n\n\tpr_devel(\"AFU(%d) update state to %#x\\n\", afu->slice, cur_state);\n\n\tswitch (cur_state) {\n\tcase H_STATE_NORMAL:\n\t\tafu->guest->previous_state = cur_state;\n\t\tbreak;\n\n\tcase H_STATE_DISABLE:\n\t\tpci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\n\t\t\t\tpci_channel_io_frozen);\n\n\t\tcxl_context_detach_all(afu);\n\t\tif ((rc = cxl_ops->afu_reset(afu)))\n\t\t\tpr_devel(\"reset hcall failed %d\\n\", rc);\n\n\t\trc = afu_read_error_state(afu, &cur_state);\n\t\tif (!rc && cur_state == H_STATE_NORMAL) {\n\t\t\tpci_error_handlers(afu, CXL_SLOT_RESET_EVENT,\n\t\t\t\t\tpci_channel_io_normal);\n\t\t\tpci_error_handlers(afu, CXL_RESUME_EVENT, 0);\n\t\t}\n\t\tafu->guest->previous_state = 0;\n\t\tbreak;\n\n\tcase H_STATE_TEMP_UNAVAILABLE:\n\t\tafu->guest->previous_state = cur_state;\n\t\tbreak;\n\n\tcase H_STATE_PERM_UNAVAILABLE:\n\t\tdev_err(&afu->dev, \"AFU is in permanent error state\\n\");\n\t\tpci_error_handlers(afu, CXL_ERROR_DETECTED_EVENT,\n\t\t\t\tpci_channel_io_perm_failure);\n\t\tafu->guest->previous_state = cur_state;\n\t\tbreak;\n\n\tdefault:\n\t\tpr_err(\"Unexpected AFU(%d) error state: %#x\\n\",\n\t\t       afu->slice, cur_state);\n\t\treturn -EINVAL;\n\t}\n\n\treturn rc;\n}\n\nstatic void afu_handle_errstate(struct work_struct *work)\n{\n\tstruct cxl_afu_guest *afu_guest =\n\t\tcontainer_of(to_delayed_work(work), struct cxl_afu_guest, work_err);\n\n\tif (!afu_update_state(afu_guest->parent) &&\n\t    afu_guest->previous_state == H_STATE_PERM_UNAVAILABLE)\n\t\treturn;\n\n\tif (afu_guest->handle_err)\n\t\tschedule_delayed_work(&afu_guest->work_err,\n\t\t\t\t      msecs_to_jiffies(3000));\n}\n\nstatic bool guest_link_ok(struct cxl *cxl, struct cxl_afu *afu)\n{\n\tint state;\n\n\tif (afu && (!afu_read_error_state(afu, &state))) {\n\t\tif (state == H_STATE_NORMAL)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int afu_properties_look_ok(struct cxl_afu *afu)\n{\n\tif (afu->pp_irqs < 0) {\n\t\tdev_err(&afu->dev, \"Unexpected per-process minimum interrupt value\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (afu->max_procs_virtualised < 1) {\n\t\tdev_err(&afu->dev, \"Unexpected max number of processes virtualised value\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint cxl_guest_init_afu(struct cxl *adapter, int slice, struct device_node *afu_np)\n{\n\tstruct cxl_afu *afu;\n\tbool free = true;\n\tint rc;\n\n\tpr_devel(\"in %s - AFU(%d)\\n\", __func__, slice);\n\tif (!(afu = cxl_alloc_afu(adapter, slice)))\n\t\treturn -ENOMEM;\n\n\tif (!(afu->guest = kzalloc(sizeof(struct cxl_afu_guest), GFP_KERNEL))) {\n\t\tkfree(afu);\n\t\treturn -ENOMEM;\n\t}\n\n\tif ((rc = dev_set_name(&afu->dev, \"afu%i.%i\",\n\t\t\t\t\t  adapter->adapter_num,\n\t\t\t\t\t  slice)))\n\t\tgoto err1;\n\n\tadapter->slices++;\n\n\tif ((rc = cxl_of_read_afu_handle(afu, afu_np)))\n\t\tgoto err1;\n\n\tif ((rc = cxl_ops->afu_reset(afu)))\n\t\tgoto err1;\n\n\tif ((rc = cxl_of_read_afu_properties(afu, afu_np)))\n\t\tgoto err1;\n\n\tif ((rc = afu_properties_look_ok(afu)))\n\t\tgoto err1;\n\n\tif ((rc = guest_map_slice_regs(afu)))\n\t\tgoto err1;\n\n\tif ((rc = guest_register_serr_irq(afu)))\n\t\tgoto err2;\n\n\t \n\tif ((rc = cxl_register_afu(afu)))\n\t\tgoto err_put_dev;\n\n\tif ((rc = cxl_sysfs_afu_add(afu)))\n\t\tgoto err_del_dev;\n\n\t \n\tif (afu->max_procs_virtualised == 1)\n\t\tafu->modes_supported = CXL_MODE_DEDICATED;\n\telse\n\t\tafu->modes_supported = CXL_MODE_DIRECTED;\n\n\tif ((rc = cxl_afu_select_best_mode(afu)))\n\t\tgoto err_remove_sysfs;\n\n\tadapter->afu[afu->slice] = afu;\n\n\tafu->enabled = true;\n\n\t \n\tafu->guest->parent = afu;\n\tafu->guest->handle_err = true;\n\tINIT_DELAYED_WORK(&afu->guest->work_err, afu_handle_errstate);\n\tschedule_delayed_work(&afu->guest->work_err, msecs_to_jiffies(1000));\n\n\tif ((rc = cxl_pci_vphb_add(afu)))\n\t\tdev_info(&afu->dev, \"Can't register vPHB\\n\");\n\n\treturn 0;\n\nerr_remove_sysfs:\n\tcxl_sysfs_afu_remove(afu);\nerr_del_dev:\n\tdevice_del(&afu->dev);\nerr_put_dev:\n\tput_device(&afu->dev);\n\tfree = false;\n\tguest_release_serr_irq(afu);\nerr2:\n\tguest_unmap_slice_regs(afu);\nerr1:\n\tif (free) {\n\t\tkfree(afu->guest);\n\t\tkfree(afu);\n\t}\n\treturn rc;\n}\n\nvoid cxl_guest_remove_afu(struct cxl_afu *afu)\n{\n\tif (!afu)\n\t\treturn;\n\n\t \n\tafu->guest->handle_err = false;\n\tflush_delayed_work(&afu->guest->work_err);\n\n\tcxl_pci_vphb_remove(afu);\n\tcxl_sysfs_afu_remove(afu);\n\n\tspin_lock(&afu->adapter->afu_list_lock);\n\tafu->adapter->afu[afu->slice] = NULL;\n\tspin_unlock(&afu->adapter->afu_list_lock);\n\n\tcxl_context_detach_all(afu);\n\tcxl_ops->afu_deactivate_mode(afu, afu->current_mode);\n\tguest_release_serr_irq(afu);\n\tguest_unmap_slice_regs(afu);\n\n\tdevice_unregister(&afu->dev);\n}\n\nstatic void free_adapter(struct cxl *adapter)\n{\n\tstruct irq_avail *cur;\n\tint i;\n\n\tif (adapter->guest) {\n\t\tif (adapter->guest->irq_avail) {\n\t\t\tfor (i = 0; i < adapter->guest->irq_nranges; i++) {\n\t\t\t\tcur = &adapter->guest->irq_avail[i];\n\t\t\t\tbitmap_free(cur->bitmap);\n\t\t\t}\n\t\t\tkfree(adapter->guest->irq_avail);\n\t\t}\n\t\tkfree(adapter->guest->status);\n\t\tkfree(adapter->guest);\n\t}\n\tcxl_remove_adapter_nr(adapter);\n\tkfree(adapter);\n}\n\nstatic int properties_look_ok(struct cxl *adapter)\n{\n\t \n\tif (strlen(adapter->guest->status) &&\n\t    strcmp(adapter->guest->status, \"okay\")) {\n\t\tpr_err(\"ABORTING:Bad operational status of the device\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nssize_t cxl_guest_read_adapter_vpd(struct cxl *adapter, void *buf, size_t len)\n{\n\treturn guest_collect_vpd(adapter, NULL, buf, len);\n}\n\nvoid cxl_guest_remove_adapter(struct cxl *adapter)\n{\n\tpr_devel(\"in %s\\n\", __func__);\n\n\tcxl_sysfs_adapter_remove(adapter);\n\n\tcxl_guest_remove_chardev(adapter);\n\tdevice_unregister(&adapter->dev);\n}\n\nstatic void release_adapter(struct device *dev)\n{\n\tfree_adapter(to_cxl_adapter(dev));\n}\n\nstruct cxl *cxl_guest_init_adapter(struct device_node *np, struct platform_device *pdev)\n{\n\tstruct cxl *adapter;\n\tbool free = true;\n\tint rc;\n\n\tif (!(adapter = cxl_alloc_adapter()))\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (!(adapter->guest = kzalloc(sizeof(struct cxl_guest), GFP_KERNEL))) {\n\t\tfree_adapter(adapter);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tadapter->slices = 0;\n\tadapter->guest->pdev = pdev;\n\tadapter->dev.parent = &pdev->dev;\n\tadapter->dev.release = release_adapter;\n\tdev_set_drvdata(&pdev->dev, adapter);\n\n\t \n\tadapter->psl_timebase_synced = true;\n\n\tif ((rc = cxl_of_read_adapter_handle(adapter, np)))\n\t\tgoto err1;\n\n\tif ((rc = cxl_of_read_adapter_properties(adapter, np)))\n\t\tgoto err1;\n\n\tif ((rc = properties_look_ok(adapter)))\n\t\tgoto err1;\n\n\tif ((rc = cxl_guest_add_chardev(adapter)))\n\t\tgoto err1;\n\n\t \n\tif ((rc = cxl_register_adapter(adapter)))\n\t\tgoto err_put_dev;\n\n\tif ((rc = cxl_sysfs_adapter_add(adapter)))\n\t\tgoto err_del_dev;\n\n\t \n\tcxl_adapter_context_unlock(adapter);\n\n\treturn adapter;\n\nerr_del_dev:\n\tdevice_del(&adapter->dev);\nerr_put_dev:\n\tput_device(&adapter->dev);\n\tfree = false;\n\tcxl_guest_remove_chardev(adapter);\nerr1:\n\tif (free)\n\t\tfree_adapter(adapter);\n\treturn ERR_PTR(rc);\n}\n\nvoid cxl_guest_reload_module(struct cxl *adapter)\n{\n\tstruct platform_device *pdev;\n\n\tpdev = adapter->guest->pdev;\n\tcxl_guest_remove_adapter(adapter);\n\n\tcxl_of_probe(pdev);\n}\n\nconst struct cxl_backend_ops cxl_guest_ops = {\n\t.module = THIS_MODULE,\n\t.adapter_reset = guest_reset,\n\t.alloc_one_irq = guest_alloc_one_irq,\n\t.release_one_irq = guest_release_one_irq,\n\t.alloc_irq_ranges = guest_alloc_irq_ranges,\n\t.release_irq_ranges = guest_release_irq_ranges,\n\t.setup_irq = NULL,\n\t.handle_psl_slice_error = guest_handle_psl_slice_error,\n\t.psl_interrupt = guest_psl_irq,\n\t.ack_irq = guest_ack_irq,\n\t.attach_process = guest_attach_process,\n\t.detach_process = guest_detach_process,\n\t.update_ivtes = NULL,\n\t.support_attributes = guest_support_attributes,\n\t.link_ok = guest_link_ok,\n\t.release_afu = guest_release_afu,\n\t.afu_read_err_buffer = guest_afu_read_err_buffer,\n\t.afu_check_and_enable = guest_afu_check_and_enable,\n\t.afu_activate_mode = guest_afu_activate_mode,\n\t.afu_deactivate_mode = guest_afu_deactivate_mode,\n\t.afu_reset = guest_afu_reset,\n\t.afu_cr_read8 = guest_afu_cr_read8,\n\t.afu_cr_read16 = guest_afu_cr_read16,\n\t.afu_cr_read32 = guest_afu_cr_read32,\n\t.afu_cr_read64 = guest_afu_cr_read64,\n\t.afu_cr_write8 = guest_afu_cr_write8,\n\t.afu_cr_write16 = guest_afu_cr_write16,\n\t.afu_cr_write32 = guest_afu_cr_write32,\n\t.read_adapter_vpd = cxl_guest_read_adapter_vpd,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}