{
  "module_name": "fault.c",
  "hash_id": "4d7099e39f19a6de433276c7d1817cb0f17c38f43809b032bafed5b4b7d82d9d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/cxl/fault.c",
  "human_readable_source": "\n \n\n#include <linux/workqueue.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/pid.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n\n#undef MODULE_PARAM_PREFIX\n#define MODULE_PARAM_PREFIX \"cxl\" \".\"\n#include <asm/current.h>\n#include <asm/copro.h>\n#include <asm/mmu.h>\n\n#include \"cxl.h\"\n#include \"trace.h\"\n\nstatic bool sste_matches(struct cxl_sste *sste, struct copro_slb *slb)\n{\n\treturn ((sste->vsid_data == cpu_to_be64(slb->vsid)) &&\n\t\t(sste->esid_data == cpu_to_be64(slb->esid)));\n}\n\n \nstatic struct cxl_sste *find_free_sste(struct cxl_context *ctx,\n\t\t\t\t       struct copro_slb *slb)\n{\n\tstruct cxl_sste *primary, *sste, *ret = NULL;\n\tunsigned int mask = (ctx->sst_size >> 7) - 1;  \n\tunsigned int entry;\n\tunsigned int hash;\n\n\tif (slb->vsid & SLB_VSID_B_1T)\n\t\thash = (slb->esid >> SID_SHIFT_1T) & mask;\n\telse  \n\t\thash = (slb->esid >> SID_SHIFT) & mask;\n\n\tprimary = ctx->sstp + (hash << 3);\n\n\tfor (entry = 0, sste = primary; entry < 8; entry++, sste++) {\n\t\tif (!ret && !(be64_to_cpu(sste->esid_data) & SLB_ESID_V))\n\t\t\tret = sste;\n\t\tif (sste_matches(sste, slb))\n\t\t\treturn NULL;\n\t}\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = primary + ctx->sst_lru;\n\tctx->sst_lru = (ctx->sst_lru + 1) & 0x7;\n\n\treturn ret;\n}\n\nstatic void cxl_load_segment(struct cxl_context *ctx, struct copro_slb *slb)\n{\n\t \n\tstruct cxl_sste *sste;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->sste_lock, flags);\n\tsste = find_free_sste(ctx, slb);\n\tif (!sste)\n\t\tgoto out_unlock;\n\n\tpr_devel(\"CXL Populating SST[%li]: %#llx %#llx\\n\",\n\t\t\tsste - ctx->sstp, slb->vsid, slb->esid);\n\ttrace_cxl_ste_write(ctx, sste - ctx->sstp, slb->esid, slb->vsid);\n\n\tsste->vsid_data = cpu_to_be64(slb->vsid);\n\tsste->esid_data = cpu_to_be64(slb->esid);\nout_unlock:\n\tspin_unlock_irqrestore(&ctx->sste_lock, flags);\n}\n\nstatic int cxl_fault_segment(struct cxl_context *ctx, struct mm_struct *mm,\n\t\t\t     u64 ea)\n{\n\tstruct copro_slb slb = {0,0};\n\tint rc;\n\n\tif (!(rc = copro_calculate_slb(mm, ea, &slb))) {\n\t\tcxl_load_segment(ctx, &slb);\n\t}\n\n\treturn rc;\n}\n\nstatic void cxl_ack_ae(struct cxl_context *ctx)\n{\n\tunsigned long flags;\n\n\tcxl_ops->ack_irq(ctx, CXL_PSL_TFC_An_AE, 0);\n\n\tspin_lock_irqsave(&ctx->lock, flags);\n\tctx->pending_fault = true;\n\tctx->fault_addr = ctx->dar;\n\tctx->fault_dsisr = ctx->dsisr;\n\tspin_unlock_irqrestore(&ctx->lock, flags);\n\n\twake_up_all(&ctx->wq);\n}\n\nstatic int cxl_handle_segment_miss(struct cxl_context *ctx,\n\t\t\t\t   struct mm_struct *mm, u64 ea)\n{\n\tint rc;\n\n\tpr_devel(\"CXL interrupt: Segment fault pe: %i ea: %#llx\\n\", ctx->pe, ea);\n\ttrace_cxl_ste_miss(ctx, ea);\n\n\tif ((rc = cxl_fault_segment(ctx, mm, ea)))\n\t\tcxl_ack_ae(ctx);\n\telse {\n\n\t\tmb();  \n\t\tcxl_ops->ack_irq(ctx, CXL_PSL_TFC_An_R, 0);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nint cxl_handle_mm_fault(struct mm_struct *mm, u64 dsisr, u64 dar)\n{\n\tvm_fault_t flt = 0;\n\tint result;\n\tunsigned long access, flags, inv_flags = 0;\n\n\t \n\tif (mm && !cpumask_test_cpu(smp_processor_id(), mm_cpumask(mm))) {\n\t\tcpumask_set_cpu(smp_processor_id(), mm_cpumask(mm));\n\t\t \n\t\tsmp_mb();\n\t}\n\tif ((result = copro_handle_mm_fault(mm, dar, dsisr, &flt))) {\n\t\tpr_devel(\"copro_handle_mm_fault failed: %#x\\n\", result);\n\t\treturn result;\n\t}\n\n\tif (!radix_enabled()) {\n\t\t \n\t\taccess = _PAGE_PRESENT | _PAGE_READ;\n\t\tif (dsisr & CXL_PSL_DSISR_An_S)\n\t\t\taccess |= _PAGE_WRITE;\n\n\t\tif (!mm && (get_region_id(dar) != USER_REGION_ID))\n\t\t\taccess |= _PAGE_PRIVILEGED;\n\n\t\tif (dsisr & DSISR_NOHPTE)\n\t\t\tinv_flags |= HPTE_NOHPTE_UPDATE;\n\n\t\tlocal_irq_save(flags);\n\t\thash_page_mm(mm, dar, access, 0x300, inv_flags);\n\t\tlocal_irq_restore(flags);\n\t}\n\treturn 0;\n}\n\nstatic void cxl_handle_page_fault(struct cxl_context *ctx,\n\t\t\t\t  struct mm_struct *mm,\n\t\t\t\t  u64 dsisr, u64 dar)\n{\n\ttrace_cxl_pte_miss(ctx, dsisr, dar);\n\n\tif (cxl_handle_mm_fault(mm, dsisr, dar)) {\n\t\tcxl_ack_ae(ctx);\n\t} else {\n\t\tpr_devel(\"Page fault successfully handled for pe: %i!\\n\", ctx->pe);\n\t\tcxl_ops->ack_irq(ctx, CXL_PSL_TFC_An_R, 0);\n\t}\n}\n\n \nstatic struct mm_struct *get_mem_context(struct cxl_context *ctx)\n{\n\tif (ctx->mm == NULL)\n\t\treturn NULL;\n\n\tif (!mmget_not_zero(ctx->mm))\n\t\treturn NULL;\n\n\treturn ctx->mm;\n}\n\nstatic bool cxl_is_segment_miss(struct cxl_context *ctx, u64 dsisr)\n{\n\tif ((cxl_is_power8() && (dsisr & CXL_PSL_DSISR_An_DS)))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool cxl_is_page_fault(struct cxl_context *ctx, u64 dsisr)\n{\n\tif ((cxl_is_power8()) && (dsisr & CXL_PSL_DSISR_An_DM))\n\t\treturn true;\n\n\tif (cxl_is_power9())\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid cxl_handle_fault(struct work_struct *fault_work)\n{\n\tstruct cxl_context *ctx =\n\t\tcontainer_of(fault_work, struct cxl_context, fault_work);\n\tu64 dsisr = ctx->dsisr;\n\tu64 dar = ctx->dar;\n\tstruct mm_struct *mm = NULL;\n\n\tif (cpu_has_feature(CPU_FTR_HVMODE)) {\n\t\tif (cxl_p2n_read(ctx->afu, CXL_PSL_DSISR_An) != dsisr ||\n\t\t    cxl_p2n_read(ctx->afu, CXL_PSL_DAR_An) != dar ||\n\t\t    cxl_p2n_read(ctx->afu, CXL_PSL_PEHandle_An) != ctx->pe) {\n\t\t\t \n\t\t\tdev_notice(&ctx->afu->dev, \"cxl_handle_fault: Translation fault regs changed\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tif (ctx->status == CLOSED) {\n\t\tcxl_ack_ae(ctx);\n\t\treturn;\n\t}\n\n\tpr_devel(\"CXL BOTTOM HALF handling fault for afu pe: %i. \"\n\t\t\"DSISR: %#llx DAR: %#llx\\n\", ctx->pe, dsisr, dar);\n\n\tif (!ctx->kernel) {\n\n\t\tmm = get_mem_context(ctx);\n\t\tif (mm == NULL) {\n\t\t\tpr_devel(\"%s: unable to get mm for pe=%d pid=%i\\n\",\n\t\t\t\t __func__, ctx->pe, pid_nr(ctx->pid));\n\t\t\tcxl_ack_ae(ctx);\n\t\t\treturn;\n\t\t} else {\n\t\t\tpr_devel(\"Handling page fault for pe=%d pid=%i\\n\",\n\t\t\t\t ctx->pe, pid_nr(ctx->pid));\n\t\t}\n\t}\n\n\tif (cxl_is_segment_miss(ctx, dsisr))\n\t\tcxl_handle_segment_miss(ctx, mm, dar);\n\telse if (cxl_is_page_fault(ctx, dsisr))\n\t\tcxl_handle_page_fault(ctx, mm, dsisr, dar);\n\telse\n\t\tWARN(1, \"cxl_handle_fault has nothing to handle\\n\");\n\n\tif (mm)\n\t\tmmput(mm);\n}\n\nstatic u64 next_segment(u64 ea, u64 vsid)\n{\n\tif (vsid & SLB_VSID_B_1T)\n\t\tea |= (1ULL << 40) - 1;\n\telse\n\t\tea |= (1ULL << 28) - 1;\n\n\treturn ea + 1;\n}\n\nstatic void cxl_prefault_vma(struct cxl_context *ctx, struct mm_struct *mm)\n{\n\tu64 ea, last_esid = 0;\n\tstruct copro_slb slb;\n\tVMA_ITERATOR(vmi, mm, 0);\n\tstruct vm_area_struct *vma;\n\tint rc;\n\n\tmmap_read_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tfor (ea = vma->vm_start; ea < vma->vm_end;\n\t\t\t\tea = next_segment(ea, slb.vsid)) {\n\t\t\trc = copro_calculate_slb(mm, ea, &slb);\n\t\t\tif (rc)\n\t\t\t\tcontinue;\n\n\t\t\tif (last_esid == slb.esid)\n\t\t\t\tcontinue;\n\n\t\t\tcxl_load_segment(ctx, &slb);\n\t\t\tlast_esid = slb.esid;\n\t\t}\n\t}\n\tmmap_read_unlock(mm);\n}\n\nvoid cxl_prefault(struct cxl_context *ctx, u64 wed)\n{\n\tstruct mm_struct *mm = get_mem_context(ctx);\n\n\tif (mm == NULL) {\n\t\tpr_devel(\"cxl_prefault unable to get mm %i\\n\",\n\t\t\t pid_nr(ctx->pid));\n\t\treturn;\n\t}\n\n\tswitch (ctx->afu->prefault_mode) {\n\tcase CXL_PREFAULT_WED:\n\t\tcxl_fault_segment(ctx, mm, wed);\n\t\tbreak;\n\tcase CXL_PREFAULT_ALL:\n\t\tcxl_prefault_vma(ctx, mm);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmmput(mm);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}