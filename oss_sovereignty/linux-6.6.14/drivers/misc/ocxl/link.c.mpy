{
  "module_name": "link.c",
  "hash_id": "a4a5ff01624dd0df051a81d349b81be06b243fc8baef05f7ed52e84e1c417710",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/ocxl/link.c",
  "human_readable_source": "\n\n#include <linux/sched/mm.h>\n#include <linux/mutex.h>\n#include <linux/mm.h>\n#include <linux/mm_types.h>\n#include <linux/mmu_context.h>\n#include <linux/mmu_notifier.h>\n#include <linux/irqdomain.h>\n#include <asm/copro.h>\n#include <asm/pnv-ocxl.h>\n#include <asm/xive.h>\n#include <misc/ocxl.h>\n#include \"ocxl_internal.h\"\n#include \"trace.h\"\n\n\n#define SPA_PASID_BITS\t\t15\n#define SPA_PASID_MAX\t\t((1 << SPA_PASID_BITS) - 1)\n#define SPA_PE_MASK\t\tSPA_PASID_MAX\n#define SPA_SPA_SIZE_LOG\t22  \n\n#define SPA_CFG_SF\t\t(1ull << (63-0))\n#define SPA_CFG_TA\t\t(1ull << (63-1))\n#define SPA_CFG_HV\t\t(1ull << (63-3))\n#define SPA_CFG_UV\t\t(1ull << (63-4))\n#define SPA_CFG_XLAT_hpt\t(0ull << (63-6))  \n#define SPA_CFG_XLAT_roh\t(2ull << (63-6))  \n#define SPA_CFG_XLAT_ror\t(3ull << (63-6))  \n#define SPA_CFG_PR\t\t(1ull << (63-49))\n#define SPA_CFG_TC\t\t(1ull << (63-54))\n#define SPA_CFG_DR\t\t(1ull << (63-59))\n\n#define SPA_XSL_TF\t\t(1ull << (63-3))   \n#define SPA_XSL_S\t\t(1ull << (63-38))  \n\n#define SPA_PE_VALID\t\t0x80000000\n\nstruct ocxl_link;\n\nstruct pe_data {\n\tstruct mm_struct *mm;\n\t \n\tvoid (*xsl_err_cb)(void *data, u64 addr, u64 dsisr);\n\t \n\tvoid *xsl_err_data;\n\tstruct rcu_head rcu;\n\tstruct ocxl_link *link;\n\tstruct mmu_notifier mmu_notifier;\n};\n\nstruct spa {\n\tstruct ocxl_process_element *spa_mem;\n\tint spa_order;\n\tstruct mutex spa_lock;\n\tstruct radix_tree_root pe_tree;  \n\tchar *irq_name;\n\tint virq;\n\tvoid __iomem *reg_dsisr;\n\tvoid __iomem *reg_dar;\n\tvoid __iomem *reg_tfc;\n\tvoid __iomem *reg_pe_handle;\n\t \n\tstruct xsl_fault {\n\t\tstruct work_struct fault_work;\n\t\tu64 pe;\n\t\tu64 dsisr;\n\t\tu64 dar;\n\t\tstruct pe_data pe_data;\n\t} xsl_fault;\n};\n\n \nstruct ocxl_link {\n\tstruct list_head list;\n\tstruct kref ref;\n\tint domain;\n\tint bus;\n\tint dev;\n\tvoid __iomem *arva;      \n\tspinlock_t atsd_lock;    \n\tatomic_t irq_available;\n\tstruct spa *spa;\n\tvoid *platform_data;\n};\nstatic LIST_HEAD(links_list);\nstatic DEFINE_MUTEX(links_list_lock);\n\nenum xsl_response {\n\tCONTINUE,\n\tADDRESS_ERROR,\n\tRESTART,\n};\n\n\nstatic void read_irq(struct spa *spa, u64 *dsisr, u64 *dar, u64 *pe)\n{\n\tu64 reg;\n\n\t*dsisr = in_be64(spa->reg_dsisr);\n\t*dar = in_be64(spa->reg_dar);\n\treg = in_be64(spa->reg_pe_handle);\n\t*pe = reg & SPA_PE_MASK;\n}\n\nstatic void ack_irq(struct spa *spa, enum xsl_response r)\n{\n\tu64 reg = 0;\n\n\t \n\tif (r == RESTART)\n\t\treg = PPC_BIT(31);\n\telse if (r == ADDRESS_ERROR)\n\t\treg = PPC_BIT(30);\n\telse\n\t\tWARN(1, \"Invalid irq response %d\\n\", r);\n\n\tif (reg) {\n\t\ttrace_ocxl_fault_ack(spa->spa_mem, spa->xsl_fault.pe,\n\t\t\t\tspa->xsl_fault.dsisr, spa->xsl_fault.dar, reg);\n\t\tout_be64(spa->reg_tfc, reg);\n\t}\n}\n\nstatic void xsl_fault_handler_bh(struct work_struct *fault_work)\n{\n\tvm_fault_t flt = 0;\n\tunsigned long access, flags, inv_flags = 0;\n\tenum xsl_response r;\n\tstruct xsl_fault *fault = container_of(fault_work, struct xsl_fault,\n\t\t\t\t\tfault_work);\n\tstruct spa *spa = container_of(fault, struct spa, xsl_fault);\n\n\tint rc;\n\n\t \n\trc = copro_handle_mm_fault(fault->pe_data.mm, fault->dar, fault->dsisr,\n\t\t\t\t&flt);\n\tif (rc) {\n\t\tpr_debug(\"copro_handle_mm_fault failed: %d\\n\", rc);\n\t\tif (fault->pe_data.xsl_err_cb) {\n\t\t\tfault->pe_data.xsl_err_cb(\n\t\t\t\tfault->pe_data.xsl_err_data,\n\t\t\t\tfault->dar, fault->dsisr);\n\t\t}\n\t\tr = ADDRESS_ERROR;\n\t\tgoto ack;\n\t}\n\n\tif (!radix_enabled()) {\n\t\t \n\t\taccess = _PAGE_PRESENT | _PAGE_READ;\n\t\tif (fault->dsisr & SPA_XSL_S)\n\t\t\taccess |= _PAGE_WRITE;\n\n\t\tif (get_region_id(fault->dar) != USER_REGION_ID)\n\t\t\taccess |= _PAGE_PRIVILEGED;\n\n\t\tlocal_irq_save(flags);\n\t\thash_page_mm(fault->pe_data.mm, fault->dar, access, 0x300,\n\t\t\tinv_flags);\n\t\tlocal_irq_restore(flags);\n\t}\n\tr = RESTART;\nack:\n\tmmput(fault->pe_data.mm);\n\tack_irq(spa, r);\n}\n\nstatic irqreturn_t xsl_fault_handler(int irq, void *data)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) data;\n\tstruct spa *spa = link->spa;\n\tu64 dsisr, dar, pe_handle;\n\tstruct pe_data *pe_data;\n\tstruct ocxl_process_element *pe;\n\tint pid;\n\tbool schedule = false;\n\n\tread_irq(spa, &dsisr, &dar, &pe_handle);\n\ttrace_ocxl_fault(spa->spa_mem, pe_handle, dsisr, dar, -1);\n\n\tWARN_ON(pe_handle > SPA_PE_MASK);\n\tpe = spa->spa_mem + pe_handle;\n\tpid = be32_to_cpu(pe->pid);\n\t \n\tif (!(dsisr & SPA_XSL_TF)) {\n\t\tWARN(1, \"Invalid xsl interrupt fault register %#llx\\n\", dsisr);\n\t\tack_irq(spa, ADDRESS_ERROR);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\trcu_read_lock();\n\tpe_data = radix_tree_lookup(&spa->pe_tree, pe_handle);\n\tif (!pe_data) {\n\t\t \n\t\trcu_read_unlock();\n\t\tpr_debug(\"Unknown mm context for xsl interrupt\\n\");\n\t\tack_irq(spa, ADDRESS_ERROR);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tif (!pe_data->mm) {\n\t\t \n\t\trcu_read_unlock();\n\t\tpr_warn(\"Unresolved OpenCAPI xsl fault in kernel context\\n\");\n\t\tack_irq(spa, ADDRESS_ERROR);\n\t\treturn IRQ_HANDLED;\n\t}\n\tWARN_ON(pe_data->mm->context.id != pid);\n\n\tif (mmget_not_zero(pe_data->mm)) {\n\t\t\tspa->xsl_fault.pe = pe_handle;\n\t\t\tspa->xsl_fault.dar = dar;\n\t\t\tspa->xsl_fault.dsisr = dsisr;\n\t\t\tspa->xsl_fault.pe_data = *pe_data;\n\t\t\tschedule = true;\n\t\t\t \n\t}\n\trcu_read_unlock();\n\tif (schedule)\n\t\tschedule_work(&spa->xsl_fault.fault_work);\n\telse\n\t\tack_irq(spa, ADDRESS_ERROR);\n\treturn IRQ_HANDLED;\n}\n\nstatic void unmap_irq_registers(struct spa *spa)\n{\n\tpnv_ocxl_unmap_xsl_regs(spa->reg_dsisr, spa->reg_dar, spa->reg_tfc,\n\t\t\t\tspa->reg_pe_handle);\n}\n\nstatic int map_irq_registers(struct pci_dev *dev, struct spa *spa)\n{\n\treturn pnv_ocxl_map_xsl_regs(dev, &spa->reg_dsisr, &spa->reg_dar,\n\t\t\t\t&spa->reg_tfc, &spa->reg_pe_handle);\n}\n\nstatic int setup_xsl_irq(struct pci_dev *dev, struct ocxl_link *link)\n{\n\tstruct spa *spa = link->spa;\n\tint rc;\n\tint hwirq;\n\n\trc = pnv_ocxl_get_xsl_irq(dev, &hwirq);\n\tif (rc)\n\t\treturn rc;\n\n\trc = map_irq_registers(dev, spa);\n\tif (rc)\n\t\treturn rc;\n\n\tspa->irq_name = kasprintf(GFP_KERNEL, \"ocxl-xsl-%x-%x-%x\",\n\t\t\t\tlink->domain, link->bus, link->dev);\n\tif (!spa->irq_name) {\n\t\tdev_err(&dev->dev, \"Can't allocate name for xsl interrupt\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err_xsl;\n\t}\n\t \n\tspa->virq = irq_create_mapping(NULL, hwirq);\n\tif (!spa->virq) {\n\t\tdev_err(&dev->dev,\n\t\t\t\"irq_create_mapping failed for translation interrupt\\n\");\n\t\trc = -EINVAL;\n\t\tgoto err_name;\n\t}\n\n\tdev_dbg(&dev->dev, \"hwirq %d mapped to virq %d\\n\", hwirq, spa->virq);\n\n\trc = request_irq(spa->virq, xsl_fault_handler, 0, spa->irq_name,\n\t\t\tlink);\n\tif (rc) {\n\t\tdev_err(&dev->dev,\n\t\t\t\"request_irq failed for translation interrupt: %d\\n\",\n\t\t\trc);\n\t\trc = -EINVAL;\n\t\tgoto err_mapping;\n\t}\n\treturn 0;\n\nerr_mapping:\n\tirq_dispose_mapping(spa->virq);\nerr_name:\n\tkfree(spa->irq_name);\nerr_xsl:\n\tunmap_irq_registers(spa);\n\treturn rc;\n}\n\nstatic void release_xsl_irq(struct ocxl_link *link)\n{\n\tstruct spa *spa = link->spa;\n\n\tif (spa->virq) {\n\t\tfree_irq(spa->virq, link);\n\t\tirq_dispose_mapping(spa->virq);\n\t}\n\tkfree(spa->irq_name);\n\tunmap_irq_registers(spa);\n}\n\nstatic int alloc_spa(struct pci_dev *dev, struct ocxl_link *link)\n{\n\tstruct spa *spa;\n\n\tspa = kzalloc(sizeof(struct spa), GFP_KERNEL);\n\tif (!spa)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&spa->spa_lock);\n\tINIT_RADIX_TREE(&spa->pe_tree, GFP_KERNEL);\n\tINIT_WORK(&spa->xsl_fault.fault_work, xsl_fault_handler_bh);\n\n\tspa->spa_order = SPA_SPA_SIZE_LOG - PAGE_SHIFT;\n\tspa->spa_mem = (struct ocxl_process_element *)\n\t\t__get_free_pages(GFP_KERNEL | __GFP_ZERO, spa->spa_order);\n\tif (!spa->spa_mem) {\n\t\tdev_err(&dev->dev, \"Can't allocate Shared Process Area\\n\");\n\t\tkfree(spa);\n\t\treturn -ENOMEM;\n\t}\n\tpr_debug(\"Allocated SPA for %x:%x:%x at %p\\n\", link->domain, link->bus,\n\t\tlink->dev, spa->spa_mem);\n\n\tlink->spa = spa;\n\treturn 0;\n}\n\nstatic void free_spa(struct ocxl_link *link)\n{\n\tstruct spa *spa = link->spa;\n\n\tpr_debug(\"Freeing SPA for %x:%x:%x\\n\", link->domain, link->bus,\n\t\tlink->dev);\n\n\tif (spa && spa->spa_mem) {\n\t\tfree_pages((unsigned long) spa->spa_mem, spa->spa_order);\n\t\tkfree(spa);\n\t\tlink->spa = NULL;\n\t}\n}\n\nstatic int alloc_link(struct pci_dev *dev, int PE_mask, struct ocxl_link **out_link)\n{\n\tstruct ocxl_link *link;\n\tint rc;\n\n\tlink = kzalloc(sizeof(struct ocxl_link), GFP_KERNEL);\n\tif (!link)\n\t\treturn -ENOMEM;\n\n\tkref_init(&link->ref);\n\tlink->domain = pci_domain_nr(dev->bus);\n\tlink->bus = dev->bus->number;\n\tlink->dev = PCI_SLOT(dev->devfn);\n\tatomic_set(&link->irq_available, MAX_IRQ_PER_LINK);\n\tspin_lock_init(&link->atsd_lock);\n\n\trc = alloc_spa(dev, link);\n\tif (rc)\n\t\tgoto err_free;\n\n\trc = setup_xsl_irq(dev, link);\n\tif (rc)\n\t\tgoto err_spa;\n\n\t \n\trc = pnv_ocxl_spa_setup(dev, link->spa->spa_mem, PE_mask,\n\t\t\t\t&link->platform_data);\n\tif (rc)\n\t\tgoto err_xsl_irq;\n\n\t \n\tpnv_ocxl_map_lpar(dev, mfspr(SPRN_LPID), 0, &link->arva);\n\n\t*out_link = link;\n\treturn 0;\n\nerr_xsl_irq:\n\trelease_xsl_irq(link);\nerr_spa:\n\tfree_spa(link);\nerr_free:\n\tkfree(link);\n\treturn rc;\n}\n\nstatic void free_link(struct ocxl_link *link)\n{\n\trelease_xsl_irq(link);\n\tfree_spa(link);\n\tkfree(link);\n}\n\nint ocxl_link_setup(struct pci_dev *dev, int PE_mask, void **link_handle)\n{\n\tint rc = 0;\n\tstruct ocxl_link *link;\n\n\tmutex_lock(&links_list_lock);\n\tlist_for_each_entry(link, &links_list, list) {\n\t\t \n\t\tif (link->domain == pci_domain_nr(dev->bus) &&\n\t\t\tlink->bus == dev->bus->number &&\n\t\t\tlink->dev == PCI_SLOT(dev->devfn)) {\n\t\t\tkref_get(&link->ref);\n\t\t\t*link_handle = link;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\trc = alloc_link(dev, PE_mask, &link);\n\tif (rc)\n\t\tgoto unlock;\n\n\tlist_add(&link->list, &links_list);\n\t*link_handle = link;\nunlock:\n\tmutex_unlock(&links_list_lock);\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ocxl_link_setup);\n\nstatic void release_xsl(struct kref *ref)\n{\n\tstruct ocxl_link *link = container_of(ref, struct ocxl_link, ref);\n\n\tif (link->arva) {\n\t\tpnv_ocxl_unmap_lpar(link->arva);\n\t\tlink->arva = NULL;\n\t}\n\n\tlist_del(&link->list);\n\t \n\tpnv_ocxl_spa_release(link->platform_data);\n\tfree_link(link);\n}\n\nvoid ocxl_link_release(struct pci_dev *dev, void *link_handle)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\n\tmutex_lock(&links_list_lock);\n\tkref_put(&link->ref, release_xsl);\n\tmutex_unlock(&links_list_lock);\n}\nEXPORT_SYMBOL_GPL(ocxl_link_release);\n\nstatic void arch_invalidate_secondary_tlbs(struct mmu_notifier *mn,\n\t\t\t\t\tstruct mm_struct *mm,\n\t\t\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct pe_data *pe_data = container_of(mn, struct pe_data, mmu_notifier);\n\tstruct ocxl_link *link = pe_data->link;\n\tunsigned long addr, pid, page_size = PAGE_SIZE;\n\n\tpid = mm->context.id;\n\ttrace_ocxl_mmu_notifier_range(start, end, pid);\n\n\tspin_lock(&link->atsd_lock);\n\tfor (addr = start; addr < end; addr += page_size)\n\t\tpnv_ocxl_tlb_invalidate(link->arva, pid, addr, page_size);\n\tspin_unlock(&link->atsd_lock);\n}\n\nstatic const struct mmu_notifier_ops ocxl_mmu_notifier_ops = {\n\t.arch_invalidate_secondary_tlbs = arch_invalidate_secondary_tlbs,\n};\n\nstatic u64 calculate_cfg_state(bool kernel)\n{\n\tu64 state;\n\n\tstate = SPA_CFG_DR;\n\tif (mfspr(SPRN_LPCR) & LPCR_TC)\n\t\tstate |= SPA_CFG_TC;\n\tif (radix_enabled())\n\t\tstate |= SPA_CFG_XLAT_ror;\n\telse\n\t\tstate |= SPA_CFG_XLAT_hpt;\n\tstate |= SPA_CFG_HV;\n\tif (kernel) {\n\t\tif (mfmsr() & MSR_SF)\n\t\t\tstate |= SPA_CFG_SF;\n\t} else {\n\t\tstate |= SPA_CFG_PR;\n\t\tif (!test_tsk_thread_flag(current, TIF_32BIT))\n\t\t\tstate |= SPA_CFG_SF;\n\t}\n\treturn state;\n}\n\nint ocxl_link_add_pe(void *link_handle, int pasid, u32 pidr, u32 tidr,\n\t\tu64 amr, u16 bdf, struct mm_struct *mm,\n\t\tvoid (*xsl_err_cb)(void *data, u64 addr, u64 dsisr),\n\t\tvoid *xsl_err_data)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\tstruct spa *spa = link->spa;\n\tstruct ocxl_process_element *pe;\n\tint pe_handle, rc = 0;\n\tstruct pe_data *pe_data;\n\n\tBUILD_BUG_ON(sizeof(struct ocxl_process_element) != 128);\n\tif (pasid > SPA_PASID_MAX)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&spa->spa_lock);\n\tpe_handle = pasid & SPA_PE_MASK;\n\tpe = spa->spa_mem + pe_handle;\n\n\tif (pe->software_state) {\n\t\trc = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\tpe_data = kmalloc(sizeof(*pe_data), GFP_KERNEL);\n\tif (!pe_data) {\n\t\trc = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tpe_data->mm = mm;\n\tpe_data->xsl_err_cb = xsl_err_cb;\n\tpe_data->xsl_err_data = xsl_err_data;\n\tpe_data->link = link;\n\tpe_data->mmu_notifier.ops = &ocxl_mmu_notifier_ops;\n\n\tmemset(pe, 0, sizeof(struct ocxl_process_element));\n\tpe->config_state = cpu_to_be64(calculate_cfg_state(pidr == 0));\n\tpe->pasid = cpu_to_be32(pasid << (31 - 19));\n\tpe->bdf = cpu_to_be16(bdf);\n\tpe->lpid = cpu_to_be32(mfspr(SPRN_LPID));\n\tpe->pid = cpu_to_be32(pidr);\n\tpe->tid = cpu_to_be32(tidr);\n\tpe->amr = cpu_to_be64(amr);\n\tpe->software_state = cpu_to_be32(SPA_PE_VALID);\n\n\t \n\tif (mm) {\n\t\tmm_context_add_copro(mm);\n\t\tif (link->arva) {\n\t\t\t \n\t\t\ttrace_ocxl_init_mmu_notifier(pasid, mm->context.id);\n\t\t\tmmu_notifier_register(&pe_data->mmu_notifier, mm);\n\t\t}\n\t}\n\n\t \n\tmb();\n\tradix_tree_insert(&spa->pe_tree, pe_handle, pe_data);\n\n\t \n\tif (mm)\n\t\tmmgrab(mm);\n\ttrace_ocxl_context_add(current->pid, spa->spa_mem, pasid, pidr, tidr);\nunlock:\n\tmutex_unlock(&spa->spa_lock);\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ocxl_link_add_pe);\n\nint ocxl_link_update_pe(void *link_handle, int pasid, __u16 tid)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\tstruct spa *spa = link->spa;\n\tstruct ocxl_process_element *pe;\n\tint pe_handle, rc;\n\n\tif (pasid > SPA_PASID_MAX)\n\t\treturn -EINVAL;\n\n\tpe_handle = pasid & SPA_PE_MASK;\n\tpe = spa->spa_mem + pe_handle;\n\n\tmutex_lock(&spa->spa_lock);\n\n\tpe->tid = cpu_to_be32(tid);\n\n\t \n\tmb();\n\n\t \n\trc = pnv_ocxl_spa_remove_pe_from_cache(link->platform_data, pe_handle);\n\tWARN_ON(rc);\n\n\tmutex_unlock(&spa->spa_lock);\n\treturn rc;\n}\n\nint ocxl_link_remove_pe(void *link_handle, int pasid)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\tstruct spa *spa = link->spa;\n\tstruct ocxl_process_element *pe;\n\tstruct pe_data *pe_data;\n\tint pe_handle, rc;\n\n\tif (pasid > SPA_PASID_MAX)\n\t\treturn -EINVAL;\n\n\t \n\tpe_handle = pasid & SPA_PE_MASK;\n\tpe = spa->spa_mem + pe_handle;\n\n\tmutex_lock(&spa->spa_lock);\n\n\tif (!(be32_to_cpu(pe->software_state) & SPA_PE_VALID)) {\n\t\trc = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\ttrace_ocxl_context_remove(current->pid, spa->spa_mem, pasid,\n\t\t\t\tbe32_to_cpu(pe->pid), be32_to_cpu(pe->tid));\n\n\tmemset(pe, 0, sizeof(struct ocxl_process_element));\n\t \n\tmb();\n\n\t \n\trc = pnv_ocxl_spa_remove_pe_from_cache(link->platform_data, pe_handle);\n\tWARN_ON(rc);\n\n\tpe_data = radix_tree_delete(&spa->pe_tree, pe_handle);\n\tif (!pe_data) {\n\t\tWARN(1, \"Couldn't find pe data when removing PE\\n\");\n\t} else {\n\t\tif (pe_data->mm) {\n\t\t\tif (link->arva) {\n\t\t\t\ttrace_ocxl_release_mmu_notifier(pasid,\n\t\t\t\t\t\t\t\tpe_data->mm->context.id);\n\t\t\t\tmmu_notifier_unregister(&pe_data->mmu_notifier,\n\t\t\t\t\t\t\tpe_data->mm);\n\t\t\t\tspin_lock(&link->atsd_lock);\n\t\t\t\tpnv_ocxl_tlb_invalidate(link->arva,\n\t\t\t\t\t\t\tpe_data->mm->context.id,\n\t\t\t\t\t\t\t0ull,\n\t\t\t\t\t\t\tPAGE_SIZE);\n\t\t\t\tspin_unlock(&link->atsd_lock);\n\t\t\t}\n\t\t\tmm_context_remove_copro(pe_data->mm);\n\t\t\tmmdrop(pe_data->mm);\n\t\t}\n\t\tkfree_rcu(pe_data, rcu);\n\t}\nunlock:\n\tmutex_unlock(&spa->spa_lock);\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ocxl_link_remove_pe);\n\nint ocxl_link_irq_alloc(void *link_handle, int *hw_irq)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\tint irq;\n\n\tif (atomic_dec_if_positive(&link->irq_available) < 0)\n\t\treturn -ENOSPC;\n\n\tirq = xive_native_alloc_irq();\n\tif (!irq) {\n\t\tatomic_inc(&link->irq_available);\n\t\treturn -ENXIO;\n\t}\n\n\t*hw_irq = irq;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ocxl_link_irq_alloc);\n\nvoid ocxl_link_free_irq(void *link_handle, int hw_irq)\n{\n\tstruct ocxl_link *link = (struct ocxl_link *) link_handle;\n\n\txive_native_free_irq(hw_irq);\n\tatomic_inc(&link->irq_available);\n}\nEXPORT_SYMBOL_GPL(ocxl_link_free_irq);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}