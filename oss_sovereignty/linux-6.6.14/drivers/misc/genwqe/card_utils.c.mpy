{
  "module_name": "card_utils.c",
  "hash_id": "5c9d6d681ab36fcb610228650c9ca2bb13241aedb908fa96827fe8f8b5dd16e4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/genwqe/card_utils.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/vmalloc.h>\n#include <linux/page-flags.h>\n#include <linux/scatterlist.h>\n#include <linux/hugetlb.h>\n#include <linux/iommu.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/ctype.h>\n#include <linux/module.h>\n#include <linux/platform_device.h>\n#include <linux/delay.h>\n#include <linux/pgtable.h>\n\n#include \"genwqe_driver.h\"\n#include \"card_base.h\"\n#include \"card_ddcb.h\"\n\n \nint __genwqe_writeq(struct genwqe_dev *cd, u64 byte_offs, u64 val)\n{\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\n\t\treturn -EIO;\n\n\tif (cd->mmio == NULL)\n\t\treturn -EIO;\n\n\tif (pci_channel_offline(pci_dev))\n\t\treturn -EIO;\n\n\t__raw_writeq((__force u64)cpu_to_be64(val), cd->mmio + byte_offs);\n\treturn 0;\n}\n\n \nu64 __genwqe_readq(struct genwqe_dev *cd, u64 byte_offs)\n{\n\tif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\n\t\treturn 0xffffffffffffffffull;\n\n\tif ((cd->err_inject & GENWQE_INJECT_GFIR_FATAL) &&\n\t    (byte_offs == IO_SLC_CFGREG_GFIR))\n\t\treturn 0x000000000000ffffull;\n\n\tif ((cd->err_inject & GENWQE_INJECT_GFIR_INFO) &&\n\t    (byte_offs == IO_SLC_CFGREG_GFIR))\n\t\treturn 0x00000000ffff0000ull;\n\n\tif (cd->mmio == NULL)\n\t\treturn 0xffffffffffffffffull;\n\n\treturn be64_to_cpu((__force __be64)__raw_readq(cd->mmio + byte_offs));\n}\n\n \nint __genwqe_writel(struct genwqe_dev *cd, u64 byte_offs, u32 val)\n{\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\n\t\treturn -EIO;\n\n\tif (cd->mmio == NULL)\n\t\treturn -EIO;\n\n\tif (pci_channel_offline(pci_dev))\n\t\treturn -EIO;\n\n\t__raw_writel((__force u32)cpu_to_be32(val), cd->mmio + byte_offs);\n\treturn 0;\n}\n\n \nu32 __genwqe_readl(struct genwqe_dev *cd, u64 byte_offs)\n{\n\tif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\n\t\treturn 0xffffffff;\n\n\tif (cd->mmio == NULL)\n\t\treturn 0xffffffff;\n\n\treturn be32_to_cpu((__force __be32)__raw_readl(cd->mmio + byte_offs));\n}\n\n \nint genwqe_read_app_id(struct genwqe_dev *cd, char *app_name, int len)\n{\n\tint i, j;\n\tu32 app_id = (u32)cd->app_unitcfg;\n\n\tmemset(app_name, 0, len);\n\tfor (i = 0, j = 0; j < min(len, 4); j++) {\n\t\tchar ch = (char)((app_id >> (24 - j*8)) & 0xff);\n\n\t\tif (ch == ' ')\n\t\t\tcontinue;\n\t\tapp_name[i++] = isprint(ch) ? ch : 'X';\n\t}\n\treturn i;\n}\n\n#define CRC32_POLYNOMIAL\t0x20044009\nstatic u32 crc32_tab[256];\t \n\n \nvoid genwqe_init_crc32(void)\n{\n\tint i, j;\n\tu32 crc;\n\n\tfor (i = 0;  i < 256;  i++) {\n\t\tcrc = i << 24;\n\t\tfor (j = 0;  j < 8;  j++) {\n\t\t\tif (crc & 0x80000000)\n\t\t\t\tcrc = (crc << 1) ^ CRC32_POLYNOMIAL;\n\t\t\telse\n\t\t\t\tcrc = (crc << 1);\n\t\t}\n\t\tcrc32_tab[i] = crc;\n\t}\n}\n\n \nu32 genwqe_crc32(u8 *buff, size_t len, u32 init)\n{\n\tint i;\n\tu32 crc;\n\n\tcrc = init;\n\twhile (len--) {\n\t\ti = ((crc >> 24) ^ *buff++) & 0xFF;\n\t\tcrc = (crc << 8) ^ crc32_tab[i];\n\t}\n\treturn crc;\n}\n\nvoid *__genwqe_alloc_consistent(struct genwqe_dev *cd, size_t size,\n\t\t\t       dma_addr_t *dma_handle)\n{\n\tif (get_order(size) > MAX_ORDER)\n\t\treturn NULL;\n\n\treturn dma_alloc_coherent(&cd->pci_dev->dev, size, dma_handle,\n\t\t\t\t  GFP_KERNEL);\n}\n\nvoid __genwqe_free_consistent(struct genwqe_dev *cd, size_t size,\n\t\t\t     void *vaddr, dma_addr_t dma_handle)\n{\n\tif (vaddr == NULL)\n\t\treturn;\n\n\tdma_free_coherent(&cd->pci_dev->dev, size, vaddr, dma_handle);\n}\n\nstatic void genwqe_unmap_pages(struct genwqe_dev *cd, dma_addr_t *dma_list,\n\t\t\t      int num_pages)\n{\n\tint i;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tfor (i = 0; (i < num_pages) && (dma_list[i] != 0x0); i++) {\n\t\tdma_unmap_page(&pci_dev->dev, dma_list[i], PAGE_SIZE,\n\t\t\t       DMA_BIDIRECTIONAL);\n\t\tdma_list[i] = 0x0;\n\t}\n}\n\nstatic int genwqe_map_pages(struct genwqe_dev *cd,\n\t\t\t   struct page **page_list, int num_pages,\n\t\t\t   dma_addr_t *dma_list)\n{\n\tint i;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\t \n\tfor (i = 0; i < num_pages; i++) {\n\t\tdma_addr_t daddr;\n\n\t\tdma_list[i] = 0x0;\n\t\tdaddr = dma_map_page(&pci_dev->dev, page_list[i],\n\t\t\t\t     0,\t  \n\t\t\t\t     PAGE_SIZE,\n\t\t\t\t     DMA_BIDIRECTIONAL);   \n\n\t\tif (dma_mapping_error(&pci_dev->dev, daddr)) {\n\t\t\tdev_err(&pci_dev->dev,\n\t\t\t\t\"[%s] err: no dma addr daddr=%016llx!\\n\",\n\t\t\t\t__func__, (long long)daddr);\n\t\t\tgoto err;\n\t\t}\n\n\t\tdma_list[i] = daddr;\n\t}\n\treturn 0;\n\n err:\n\tgenwqe_unmap_pages(cd, dma_list, num_pages);\n\treturn -EIO;\n}\n\nstatic int genwqe_sgl_size(int num_pages)\n{\n\tint len, num_tlb = num_pages / 7;\n\n\tlen = sizeof(struct sg_entry) * (num_pages+num_tlb + 1);\n\treturn roundup(len, PAGE_SIZE);\n}\n\n \nint genwqe_alloc_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\n\t\t\t  void __user *user_addr, size_t user_size, int write)\n{\n\tint ret = -ENOMEM;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tsgl->fpage_offs = offset_in_page((unsigned long)user_addr);\n\tsgl->fpage_size = min_t(size_t, PAGE_SIZE-sgl->fpage_offs, user_size);\n\tsgl->nr_pages = DIV_ROUND_UP(sgl->fpage_offs + user_size, PAGE_SIZE);\n\tsgl->lpage_size = (user_size - sgl->fpage_size) % PAGE_SIZE;\n\n\tdev_dbg(&pci_dev->dev, \"[%s] uaddr=%p usize=%8ld nr_pages=%ld fpage_offs=%lx fpage_size=%ld lpage_size=%ld\\n\",\n\t\t__func__, user_addr, user_size, sgl->nr_pages,\n\t\tsgl->fpage_offs, sgl->fpage_size, sgl->lpage_size);\n\n\tsgl->user_addr = user_addr;\n\tsgl->user_size = user_size;\n\tsgl->write = write;\n\tsgl->sgl_size = genwqe_sgl_size(sgl->nr_pages);\n\n\tif (get_order(sgl->sgl_size) > MAX_ORDER) {\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"[%s] err: too much memory requested!\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\tsgl->sgl = __genwqe_alloc_consistent(cd, sgl->sgl_size,\n\t\t\t\t\t     &sgl->sgl_dma_addr);\n\tif (sgl->sgl == NULL) {\n\t\tdev_err(&pci_dev->dev,\n\t\t\t\"[%s] err: no memory available!\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\t \n\tif ((sgl->fpage_size != 0) && (sgl->fpage_size != PAGE_SIZE)) {\n\t\tsgl->fpage = __genwqe_alloc_consistent(cd, PAGE_SIZE,\n\t\t\t\t\t\t       &sgl->fpage_dma_addr);\n\t\tif (sgl->fpage == NULL)\n\t\t\tgoto err_out;\n\n\t\t \n\t\tif (copy_from_user(sgl->fpage + sgl->fpage_offs,\n\t\t\t\t   user_addr, sgl->fpage_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tif (sgl->lpage_size != 0) {\n\t\tsgl->lpage = __genwqe_alloc_consistent(cd, PAGE_SIZE,\n\t\t\t\t\t\t       &sgl->lpage_dma_addr);\n\t\tif (sgl->lpage == NULL)\n\t\t\tgoto err_out1;\n\n\t\t \n\t\tif (copy_from_user(sgl->lpage, user_addr + user_size -\n\t\t\t\t   sgl->lpage_size, sgl->lpage_size)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_out2;\n\t\t}\n\t}\n\treturn 0;\n\n err_out2:\n\t__genwqe_free_consistent(cd, PAGE_SIZE, sgl->lpage,\n\t\t\t\t sgl->lpage_dma_addr);\n\tsgl->lpage = NULL;\n\tsgl->lpage_dma_addr = 0;\n err_out1:\n\t__genwqe_free_consistent(cd, PAGE_SIZE, sgl->fpage,\n\t\t\t\t sgl->fpage_dma_addr);\n\tsgl->fpage = NULL;\n\tsgl->fpage_dma_addr = 0;\n err_out:\n\t__genwqe_free_consistent(cd, sgl->sgl_size, sgl->sgl,\n\t\t\t\t sgl->sgl_dma_addr);\n\tsgl->sgl = NULL;\n\tsgl->sgl_dma_addr = 0;\n\tsgl->sgl_size = 0;\n\n\treturn ret;\n}\n\nint genwqe_setup_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\n\t\t     dma_addr_t *dma_list)\n{\n\tint i = 0, j = 0, p;\n\tunsigned long dma_offs, map_offs;\n\tdma_addr_t prev_daddr = 0;\n\tstruct sg_entry *s, *last_s = NULL;\n\tsize_t size = sgl->user_size;\n\n\tdma_offs = 128;\t\t \n\tmap_offs = sgl->fpage_offs;  \n\n\ts = &sgl->sgl[0];\t \n\tp = 0;\t\t\t \n\twhile (p < sgl->nr_pages) {\n\t\tdma_addr_t daddr;\n\t\tunsigned int size_to_map;\n\n\t\t \n\t\tj = 0;\n\t\ts[j].target_addr = cpu_to_be64(sgl->sgl_dma_addr + dma_offs);\n\t\ts[j].len\t = cpu_to_be32(128);\n\t\ts[j].flags\t = cpu_to_be32(SG_CHAINED);\n\t\tj++;\n\n\t\twhile (j < 8) {\n\t\t\t \n\t\t\tsize_to_map = min(size, PAGE_SIZE - map_offs);\n\n\t\t\tif ((p == 0) && (sgl->fpage != NULL)) {\n\t\t\t\tdaddr = sgl->fpage_dma_addr + map_offs;\n\n\t\t\t} else if ((p == sgl->nr_pages - 1) &&\n\t\t\t\t   (sgl->lpage != NULL)) {\n\t\t\t\tdaddr = sgl->lpage_dma_addr;\n\t\t\t} else {\n\t\t\t\tdaddr = dma_list[p] + map_offs;\n\t\t\t}\n\n\t\t\tsize -= size_to_map;\n\t\t\tmap_offs = 0;\n\n\t\t\tif (prev_daddr == daddr) {\n\t\t\t\tu32 prev_len = be32_to_cpu(last_s->len);\n\n\t\t\t\t \n\n\t\t\t\tlast_s->len = cpu_to_be32(prev_len +\n\t\t\t\t\t\t\t  size_to_map);\n\n\t\t\t\tp++;  \n\t\t\t\tif (p == sgl->nr_pages)\n\t\t\t\t\tgoto fixup;   \n\n\t\t\t\tprev_daddr = daddr + size_to_map;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\ts[j].target_addr = cpu_to_be64(daddr);\n\t\t\ts[j].len\t = cpu_to_be32(size_to_map);\n\t\t\ts[j].flags\t = cpu_to_be32(SG_DATA);\n\t\t\tprev_daddr = daddr + size_to_map;\n\t\t\tlast_s = &s[j];\n\t\t\tj++;\n\n\t\t\tp++;\t \n\t\t\tif (p == sgl->nr_pages)\n\t\t\t\tgoto fixup;   \n\t\t}\n\t\tdma_offs += 128;\n\t\ts += 8;\t\t \n\t}\n fixup:\n\tif (j == 1) {\t\t \n\t\ts -= 8;\t\t \n\t\tj =  7;\t\t \n\t}\n\n\tfor (i = 0; i < j; i++)\t \n\t\ts[i] = s[i + 1];\n\n\ts[i].target_addr = cpu_to_be64(0);\n\ts[i].len\t = cpu_to_be32(0);\n\ts[i].flags\t = cpu_to_be32(SG_END_LIST);\n\treturn 0;\n}\n\n \nint genwqe_free_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl)\n{\n\tint rc = 0;\n\tsize_t offset;\n\tunsigned long res;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif (sgl->fpage) {\n\t\tif (sgl->write) {\n\t\t\tres = copy_to_user(sgl->user_addr,\n\t\t\t\tsgl->fpage + sgl->fpage_offs, sgl->fpage_size);\n\t\t\tif (res) {\n\t\t\t\tdev_err(&pci_dev->dev,\n\t\t\t\t\t\"[%s] err: copying fpage! (res=%lu)\\n\",\n\t\t\t\t\t__func__, res);\n\t\t\t\trc = -EFAULT;\n\t\t\t}\n\t\t}\n\t\t__genwqe_free_consistent(cd, PAGE_SIZE, sgl->fpage,\n\t\t\t\t\t sgl->fpage_dma_addr);\n\t\tsgl->fpage = NULL;\n\t\tsgl->fpage_dma_addr = 0;\n\t}\n\tif (sgl->lpage) {\n\t\tif (sgl->write) {\n\t\t\toffset = sgl->user_size - sgl->lpage_size;\n\t\t\tres = copy_to_user(sgl->user_addr + offset, sgl->lpage,\n\t\t\t\t\t   sgl->lpage_size);\n\t\t\tif (res) {\n\t\t\t\tdev_err(&pci_dev->dev,\n\t\t\t\t\t\"[%s] err: copying lpage! (res=%lu)\\n\",\n\t\t\t\t\t__func__, res);\n\t\t\t\trc = -EFAULT;\n\t\t\t}\n\t\t}\n\t\t__genwqe_free_consistent(cd, PAGE_SIZE, sgl->lpage,\n\t\t\t\t\t sgl->lpage_dma_addr);\n\t\tsgl->lpage = NULL;\n\t\tsgl->lpage_dma_addr = 0;\n\t}\n\t__genwqe_free_consistent(cd, sgl->sgl_size, sgl->sgl,\n\t\t\t\t sgl->sgl_dma_addr);\n\n\tsgl->sgl = NULL;\n\tsgl->sgl_dma_addr = 0x0;\n\tsgl->sgl_size = 0;\n\treturn rc;\n}\n\n \nint genwqe_user_vmap(struct genwqe_dev *cd, struct dma_mapping *m, void *uaddr,\n\t\t     unsigned long size)\n{\n\tint rc = -EINVAL;\n\tunsigned long data, offs;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif ((uaddr == NULL) || (size == 0)) {\n\t\tm->size = 0;\t \n\t\treturn -EINVAL;\n\t}\n\tm->u_vaddr = uaddr;\n\tm->size    = size;\n\n\t \n\tdata = (unsigned long)uaddr;\n\toffs = offset_in_page(data);\n\tif (size > ULONG_MAX - PAGE_SIZE - offs) {\n\t\tm->size = 0;\t \n\t\treturn -EINVAL;\n\t}\n\tm->nr_pages = DIV_ROUND_UP(offs + size, PAGE_SIZE);\n\n\tm->page_list = kcalloc(m->nr_pages,\n\t\t\t       sizeof(struct page *) + sizeof(dma_addr_t),\n\t\t\t       GFP_KERNEL);\n\tif (!m->page_list) {\n\t\tdev_err(&pci_dev->dev, \"err: alloc page_list failed\\n\");\n\t\tm->nr_pages = 0;\n\t\tm->u_vaddr = NULL;\n\t\tm->size = 0;\t \n\t\treturn -ENOMEM;\n\t}\n\tm->dma_list = (dma_addr_t *)(m->page_list + m->nr_pages);\n\n\t \n\trc = pin_user_pages_fast(data & PAGE_MASK,  \n\t\t\t\t m->nr_pages,\n\t\t\t\t m->write ? FOLL_WRITE : 0,\t \n\t\t\t\t m->page_list);\t \n\tif (rc < 0)\n\t\tgoto fail_pin_user_pages;\n\n\t \n\tif (rc < m->nr_pages) {\n\t\tunpin_user_pages_dirty_lock(m->page_list, rc, m->write);\n\t\trc = -EFAULT;\n\t\tgoto fail_pin_user_pages;\n\t}\n\n\trc = genwqe_map_pages(cd, m->page_list, m->nr_pages, m->dma_list);\n\tif (rc != 0)\n\t\tgoto fail_free_user_pages;\n\n\treturn 0;\n\n fail_free_user_pages:\n\tunpin_user_pages_dirty_lock(m->page_list, m->nr_pages, m->write);\n\n fail_pin_user_pages:\n\tkfree(m->page_list);\n\tm->page_list = NULL;\n\tm->dma_list = NULL;\n\tm->nr_pages = 0;\n\tm->u_vaddr = NULL;\n\tm->size = 0;\t\t \n\treturn rc;\n}\n\n \nint genwqe_user_vunmap(struct genwqe_dev *cd, struct dma_mapping *m)\n{\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif (!dma_mapping_used(m)) {\n\t\tdev_err(&pci_dev->dev, \"[%s] err: mapping %p not used!\\n\",\n\t\t\t__func__, m);\n\t\treturn -EINVAL;\n\t}\n\n\tif (m->dma_list)\n\t\tgenwqe_unmap_pages(cd, m->dma_list, m->nr_pages);\n\n\tif (m->page_list) {\n\t\tunpin_user_pages_dirty_lock(m->page_list, m->nr_pages,\n\t\t\t\t\t    m->write);\n\t\tkfree(m->page_list);\n\t\tm->page_list = NULL;\n\t\tm->dma_list = NULL;\n\t\tm->nr_pages = 0;\n\t}\n\n\tm->u_vaddr = NULL;\n\tm->size = 0;\t\t \n\treturn 0;\n}\n\n \nu8 genwqe_card_type(struct genwqe_dev *cd)\n{\n\tu64 card_type = cd->slu_unitcfg;\n\n\treturn (u8)((card_type & IO_SLU_UNITCFG_TYPE_MASK) >> 20);\n}\n\n \nint genwqe_card_reset(struct genwqe_dev *cd)\n{\n\tu64 softrst;\n\tstruct pci_dev *pci_dev = cd->pci_dev;\n\n\tif (!genwqe_is_privileged(cd))\n\t\treturn -ENODEV;\n\n\t \n\t__genwqe_writeq(cd, IO_SLC_CFGREG_SOFTRESET, 0x1ull);\n\tmsleep(1000);\n\t__genwqe_readq(cd, IO_HSU_FIR_CLR);\n\t__genwqe_readq(cd, IO_APP_FIR_CLR);\n\t__genwqe_readq(cd, IO_SLU_FIR_CLR);\n\n\t \n\tsoftrst = __genwqe_readq(cd, IO_SLC_CFGREG_SOFTRESET) & 0x3cull;\n\t__genwqe_writeq(cd, IO_SLC_CFGREG_SOFTRESET, softrst | 0x2ull);\n\n\t \n\tmsleep(50);\n\n\tif (genwqe_need_err_masking(cd)) {\n\t\tdev_info(&pci_dev->dev,\n\t\t\t \"[%s] masking errors for old bitstreams\\n\", __func__);\n\t\t__genwqe_writeq(cd, IO_SLC_MISC_DEBUG, 0x0aull);\n\t}\n\treturn 0;\n}\n\nint genwqe_read_softreset(struct genwqe_dev *cd)\n{\n\tu64 bitstream;\n\n\tif (!genwqe_is_privileged(cd))\n\t\treturn -ENODEV;\n\n\tbitstream = __genwqe_readq(cd, IO_SLU_BITSTREAM) & 0x1;\n\tcd->softreset = (bitstream == 0) ? 0x8ull : 0xcull;\n\treturn 0;\n}\n\n \nint genwqe_set_interrupt_capability(struct genwqe_dev *cd, int count)\n{\n\tint rc;\n\n\trc = pci_alloc_irq_vectors(cd->pci_dev, 1, count, PCI_IRQ_MSI);\n\tif (rc < 0)\n\t\treturn rc;\n\treturn 0;\n}\n\n \nvoid genwqe_reset_interrupt_capability(struct genwqe_dev *cd)\n{\n\tpci_free_irq_vectors(cd->pci_dev);\n}\n\n \nstatic int set_reg_idx(struct genwqe_dev *cd, struct genwqe_reg *r,\n\t\t       unsigned int *i, unsigned int m, u32 addr, u32 idx,\n\t\t       u64 val)\n{\n\tif (WARN_ON_ONCE(*i >= m))\n\t\treturn -EFAULT;\n\n\tr[*i].addr = addr;\n\tr[*i].idx = idx;\n\tr[*i].val = val;\n\t++*i;\n\treturn 0;\n}\n\nstatic int set_reg(struct genwqe_dev *cd, struct genwqe_reg *r,\n\t\t   unsigned int *i, unsigned int m, u32 addr, u64 val)\n{\n\treturn set_reg_idx(cd, r, i, m, addr, 0, val);\n}\n\nint genwqe_read_ffdc_regs(struct genwqe_dev *cd, struct genwqe_reg *regs,\n\t\t\t unsigned int max_regs, int all)\n{\n\tunsigned int i, j, idx = 0;\n\tu32 ufir_addr, ufec_addr, sfir_addr, sfec_addr;\n\tu64 gfir, sluid, appid, ufir, ufec, sfir, sfec;\n\n\t \n\tgfir = __genwqe_readq(cd, IO_SLC_CFGREG_GFIR);\n\tset_reg(cd, regs, &idx, max_regs, IO_SLC_CFGREG_GFIR, gfir);\n\n\t \n\tsluid = __genwqe_readq(cd, IO_SLU_UNITCFG);  \n\tset_reg(cd, regs, &idx, max_regs, IO_SLU_UNITCFG, sluid);\n\n\t \n\tappid = __genwqe_readq(cd, IO_APP_UNITCFG);  \n\tset_reg(cd, regs, &idx, max_regs, IO_APP_UNITCFG, appid);\n\n\t \n\tfor (i = 0; i < GENWQE_MAX_UNITS; i++) {\n\n\t\t \n\t\tufir_addr = (i << 24) | 0x008;\n\t\tufir = __genwqe_readq(cd, ufir_addr);\n\t\tset_reg(cd, regs, &idx, max_regs, ufir_addr, ufir);\n\n\t\t \n\t\tufec_addr = (i << 24) | 0x018;\n\t\tufec = __genwqe_readq(cd, ufec_addr);\n\t\tset_reg(cd, regs, &idx, max_regs, ufec_addr, ufec);\n\n\t\tfor (j = 0; j < 64; j++) {\n\t\t\t \n\t\t\tif (!all && (!(ufir & (1ull << j))))\n\t\t\t\tcontinue;\n\n\t\t\tsfir_addr = (i << 24) | (0x100 + 8 * j);\n\t\t\tsfir = __genwqe_readq(cd, sfir_addr);\n\t\t\tset_reg(cd, regs, &idx, max_regs, sfir_addr, sfir);\n\n\t\t\tsfec_addr = (i << 24) | (0x300 + 8 * j);\n\t\t\tsfec = __genwqe_readq(cd, sfec_addr);\n\t\t\tset_reg(cd, regs, &idx, max_regs, sfec_addr, sfec);\n\t\t}\n\t}\n\n\t \n\tfor (i = idx; i < max_regs; i++) {\n\t\tregs[i].addr = 0xffffffff;\n\t\tregs[i].val = 0xffffffffffffffffull;\n\t}\n\treturn idx;\n}\n\n \nint genwqe_ffdc_buff_size(struct genwqe_dev *cd, int uid)\n{\n\tint entries = 0, ring, traps, traces, trace_entries;\n\tu32 eevptr_addr, l_addr, d_len, d_type;\n\tu64 eevptr, val, addr;\n\n\teevptr_addr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_ERROR_POINTER;\n\teevptr = __genwqe_readq(cd, eevptr_addr);\n\n\tif ((eevptr != 0x0) && (eevptr != -1ull)) {\n\t\tl_addr = GENWQE_UID_OFFS(uid) | eevptr;\n\n\t\twhile (1) {\n\t\t\tval = __genwqe_readq(cd, l_addr);\n\n\t\t\tif ((val == 0x0) || (val == -1ull))\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\td_len  = (val & 0x0000007fff000000ull) >> 24;\n\n\t\t\t \n\t\t\td_type = (val & 0x0000008000000000ull) >> 36;\n\n\t\t\tif (d_type) {\t \n\t\t\t\tentries += d_len;\n\t\t\t} else {\t \n\t\t\t\tentries += d_len >> 3;\n\t\t\t}\n\n\t\t\tl_addr += 8;\n\t\t}\n\t}\n\n\tfor (ring = 0; ring < 8; ring++) {\n\t\taddr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_DIAG_MAP(ring);\n\t\tval = __genwqe_readq(cd, addr);\n\n\t\tif ((val == 0x0ull) || (val == -1ull))\n\t\t\tcontinue;\n\n\t\ttraps = (val >> 24) & 0xff;\n\t\ttraces = (val >> 16) & 0xff;\n\t\ttrace_entries = val & 0xffff;\n\n\t\tentries += traps + (traces * trace_entries);\n\t}\n\treturn entries;\n}\n\n \nint genwqe_ffdc_buff_read(struct genwqe_dev *cd, int uid,\n\t\t\t  struct genwqe_reg *regs, unsigned int max_regs)\n{\n\tint i, traps, traces, trace, trace_entries, trace_entry, ring;\n\tunsigned int idx = 0;\n\tu32 eevptr_addr, l_addr, d_addr, d_len, d_type;\n\tu64 eevptr, e, val, addr;\n\n\teevptr_addr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_ERROR_POINTER;\n\teevptr = __genwqe_readq(cd, eevptr_addr);\n\n\tif ((eevptr != 0x0) && (eevptr != 0xffffffffffffffffull)) {\n\t\tl_addr = GENWQE_UID_OFFS(uid) | eevptr;\n\t\twhile (1) {\n\t\t\te = __genwqe_readq(cd, l_addr);\n\t\t\tif ((e == 0x0) || (e == 0xffffffffffffffffull))\n\t\t\t\tbreak;\n\n\t\t\td_addr = (e & 0x0000000000ffffffull);\t     \n\t\t\td_len  = (e & 0x0000007fff000000ull) >> 24;  \n\t\t\td_type = (e & 0x0000008000000000ull) >> 36;  \n\t\t\td_addr |= GENWQE_UID_OFFS(uid);\n\n\t\t\tif (d_type) {\n\t\t\t\tfor (i = 0; i < (int)d_len; i++) {\n\t\t\t\t\tval = __genwqe_readq(cd, d_addr);\n\t\t\t\t\tset_reg_idx(cd, regs, &idx, max_regs,\n\t\t\t\t\t\t    d_addr, i, val);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\td_len >>= 3;  \n\t\t\t\tfor (i = 0; i < (int)d_len; i++, d_addr += 8) {\n\t\t\t\t\tval = __genwqe_readq(cd, d_addr);\n\t\t\t\t\tset_reg_idx(cd, regs, &idx, max_regs,\n\t\t\t\t\t\t    d_addr, 0, val);\n\t\t\t\t}\n\t\t\t}\n\t\t\tl_addr += 8;\n\t\t}\n\t}\n\n\t \n\tfor (ring = 0; ring < 8; ring++) {  \n\t\taddr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_DIAG_MAP(ring);\n\t\tval = __genwqe_readq(cd, addr);\n\n\t\tif ((val == 0x0ull) || (val == -1ull))\n\t\t\tcontinue;\n\n\t\ttraps = (val >> 24) & 0xff;\t \n\t\ttraces = (val >> 16) & 0xff;\t \n\t\ttrace_entries = val & 0xffff;\t \n\n\t\t \n\t\t \n\t\t \n\t\tfor (trace = 0; trace <= traces; trace++) {\n\t\t\tu32 diag_sel =\n\t\t\t\tGENWQE_EXTENDED_DIAG_SELECTOR(ring, trace);\n\n\t\t\taddr = (GENWQE_UID_OFFS(uid) |\n\t\t\t\tIO_EXTENDED_DIAG_SELECTOR);\n\t\t\t__genwqe_writeq(cd, addr, diag_sel);\n\n\t\t\tfor (trace_entry = 0;\n\t\t\t     trace_entry < (trace ? trace_entries : traps);\n\t\t\t     trace_entry++) {\n\t\t\t\taddr = (GENWQE_UID_OFFS(uid) |\n\t\t\t\t\tIO_EXTENDED_DIAG_READ_MBX);\n\t\t\t\tval = __genwqe_readq(cd, addr);\n\t\t\t\tset_reg_idx(cd, regs, &idx, max_regs, addr,\n\t\t\t\t\t    (diag_sel<<16) | trace_entry, val);\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nint genwqe_write_vreg(struct genwqe_dev *cd, u32 reg, u64 val, int func)\n{\n\t__genwqe_writeq(cd, IO_PF_SLC_VIRTUAL_WINDOW, func & 0xf);\n\t__genwqe_writeq(cd, reg, val);\n\treturn 0;\n}\n\n \nu64 genwqe_read_vreg(struct genwqe_dev *cd, u32 reg, int func)\n{\n\t__genwqe_writeq(cd, IO_PF_SLC_VIRTUAL_WINDOW, func & 0xf);\n\treturn __genwqe_readq(cd, reg);\n}\n\n \nint genwqe_base_clock_frequency(struct genwqe_dev *cd)\n{\n\tu16 speed;\t\t \n\tstatic const int speed_grade[] = { 250, 200, 166, 175 };\n\n\tspeed = (u16)((cd->slu_unitcfg >> 28) & 0x0full);\n\tif (speed >= ARRAY_SIZE(speed_grade))\n\t\treturn 0;\t \n\n\treturn speed_grade[speed];\n}\n\n \nvoid genwqe_stop_traps(struct genwqe_dev *cd)\n{\n\t__genwqe_writeq(cd, IO_SLC_MISC_DEBUG_SET, 0xcull);\n}\n\n \nvoid genwqe_start_traps(struct genwqe_dev *cd)\n{\n\t__genwqe_writeq(cd, IO_SLC_MISC_DEBUG_CLR, 0xcull);\n\n\tif (genwqe_need_err_masking(cd))\n\t\t__genwqe_writeq(cd, IO_SLC_MISC_DEBUG, 0x0aull);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}