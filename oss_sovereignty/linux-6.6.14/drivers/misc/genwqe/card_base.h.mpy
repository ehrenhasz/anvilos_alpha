{
  "module_name": "card_base.h",
  "hash_id": "0dbbb73b4a042aac6d3c662c5302093a91122233baf03bc75075dcfbfd5d1755",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/genwqe/card_base.h",
  "human_readable_source": " \n#ifndef __CARD_BASE_H__\n#define __CARD_BASE_H__\n\n \n\n \n\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/cdev.h>\n#include <linux/stringify.h>\n#include <linux/pci.h>\n#include <linux/semaphore.h>\n#include <linux/uaccess.h>\n#include <linux/io.h>\n#include <linux/debugfs.h>\n#include <linux/slab.h>\n\n#include <linux/genwqe/genwqe_card.h>\n#include \"genwqe_driver.h\"\n\n#define GENWQE_MSI_IRQS\t\t\t4   \n\n#define GENWQE_MAX_VFS\t\t\t15  \n#define GENWQE_MAX_FUNCS\t\t16  \n#define GENWQE_CARD_NO_MAX\t\t(16 * GENWQE_MAX_FUNCS)\n\n \n#define GENWQE_DDCB_MAX\t\t\t32  \n#define GENWQE_POLLING_ENABLED\t\t0   \n#define GENWQE_DDCB_SOFTWARE_TIMEOUT\t10  \n#define GENWQE_KILL_TIMEOUT\t\t8   \n#define GENWQE_VF_JOBTIMEOUT_MSEC\t250   \n#define GENWQE_PF_JOBTIMEOUT_MSEC\t8000  \n#define GENWQE_HEALTH_CHECK_INTERVAL\t4  \n\n \nextern const struct attribute_group *genwqe_attribute_groups[];\n\n \n#define PCI_DEVICE_GENWQE\t\t0x044b  \n\n#define PCI_SUBSYSTEM_ID_GENWQE5\t0x035f  \n#define PCI_SUBSYSTEM_ID_GENWQE5_NEW\t0x044b  \n#define PCI_CLASSCODE_GENWQE5\t\t0x1200  \n\n#define PCI_SUBVENDOR_ID_IBM_SRIOV\t0x0000\n#define PCI_SUBSYSTEM_ID_GENWQE5_SRIOV\t0x0000  \n#define PCI_CLASSCODE_GENWQE5_SRIOV\t0x1200  \n\n#define\tGENWQE_SLU_ARCH_REQ\t\t2  \n\n \nstruct genwqe_reg {\n\tu32 addr;\n\tu32 idx;\n\tu64 val;\n};\n\n \nenum genwqe_dbg_type {\n\tGENWQE_DBG_UNIT0 = 0,   \n\tGENWQE_DBG_UNIT1 = 1,\n\tGENWQE_DBG_UNIT2 = 2,\n\tGENWQE_DBG_UNIT3 = 3,\n\tGENWQE_DBG_UNIT4 = 4,\n\tGENWQE_DBG_UNIT5 = 5,\n\tGENWQE_DBG_UNIT6 = 6,\n\tGENWQE_DBG_UNIT7 = 7,\n\tGENWQE_DBG_REGS  = 8,\n\tGENWQE_DBG_DMA   = 9,\n\tGENWQE_DBG_UNITS = 10,  \n};\n\n \n#define GENWQE_INJECT_HARDWARE_FAILURE\t0x00000001  \n#define GENWQE_INJECT_BUS_RESET_FAILURE 0x00000002  \n#define GENWQE_INJECT_GFIR_FATAL\t0x00000004  \n#define GENWQE_INJECT_GFIR_INFO\t\t0x00000008  \n\n \n\n\n \nenum dma_mapping_type {\n\tGENWQE_MAPPING_RAW = 0,\t\t \n\tGENWQE_MAPPING_SGL_TEMP,\t \n\tGENWQE_MAPPING_SGL_PINNED,\t \n};\n\n \nstruct dma_mapping {\n\tenum dma_mapping_type type;\n\n\tvoid *u_vaddr;\t\t\t \n\tvoid *k_vaddr;\t\t\t \n\tdma_addr_t dma_addr;\t\t \n\n\tstruct page **page_list;\t \n\tdma_addr_t *dma_list;\t\t \n\tunsigned int nr_pages;\t\t \n\tunsigned int size;\t\t \n\n\tstruct list_head card_list;\t \n\tstruct list_head pin_list;\t \n\tint write;\t\t\t \n};\n\nstatic inline void genwqe_mapping_init(struct dma_mapping *m,\n\t\t\t\t       enum dma_mapping_type type)\n{\n\tmemset(m, 0, sizeof(*m));\n\tm->type = type;\n\tm->write = 1;  \n}\n\n \n\nstruct ddcb_queue {\n\tint ddcb_max;\t\t\t \n\tint ddcb_next;\t\t\t \n\tint ddcb_act;\t\t\t \n\tu16 ddcb_seq;\t\t\t \n\tunsigned int ddcbs_in_flight;\t \n\tunsigned int ddcbs_completed;\n\tunsigned int ddcbs_max_in_flight;\n\tunsigned int return_on_busy;     \n\tunsigned int wait_on_busy;\n\n\tdma_addr_t ddcb_daddr;\t\t \n\tstruct ddcb *ddcb_vaddr;\t \n\tstruct ddcb_requ **ddcb_req;\t \n\twait_queue_head_t *ddcb_waitqs;  \n\n\tspinlock_t ddcb_lock;\t\t \n\twait_queue_head_t busy_waitq;    \n\n\t \n\tu32 IO_QUEUE_CONFIG;\n\tu32 IO_QUEUE_STATUS;\n\tu32 IO_QUEUE_SEGMENT;\n\tu32 IO_QUEUE_INITSQN;\n\tu32 IO_QUEUE_WRAP;\n\tu32 IO_QUEUE_OFFSET;\n\tu32 IO_QUEUE_WTIME;\n\tu32 IO_QUEUE_ERRCNTS;\n\tu32 IO_QUEUE_LRW;\n};\n\n \n#define GENWQE_FFDC_REGS\t(3 + (8 * (2 + 2 * 64)))\n\nstruct genwqe_ffdc {\n\tunsigned int entries;\n\tstruct genwqe_reg *regs;\n};\n\n \nstruct genwqe_dev {\n\tenum genwqe_card_state card_state;\n\tspinlock_t print_lock;\n\n\tint card_idx;\t\t\t \n\tu64 flags;\t\t\t \n\n\t \n\tstruct genwqe_ffdc ffdc[GENWQE_DBG_UNITS];\n\n\t \n\tstruct task_struct *card_thread;\n\twait_queue_head_t queue_waitq;\n\tstruct ddcb_queue queue;\t \n\tunsigned int irqs_processed;\n\n\t \n\tstruct task_struct *health_thread;\n\twait_queue_head_t health_waitq;\n\n\tint use_platform_recovery;\t \n\n\t \n\tdev_t  devnum_genwqe;\t\t \n\tconst struct class *class_genwqe;\t \n\tstruct device *dev;\t\t \n\tstruct cdev cdev_genwqe;\t \n\n\tstruct dentry *debugfs_root;\t \n\tstruct dentry *debugfs_genwqe;\t \n\n\t \n\tstruct pci_dev *pci_dev;\t \n\tvoid __iomem *mmio;\t\t \n\tunsigned long mmio_len;\n\tint num_vfs;\n\tu32 vf_jobtimeout_msec[GENWQE_MAX_VFS];\n\tint is_privileged;\t\t \n\n\t \n\tu64 slu_unitcfg;\n\tu64 app_unitcfg;\n\tu64 softreset;\n\tu64 err_inject;\n\tu64 last_gfir;\n\tchar app_name[5];\n\n\tspinlock_t file_lock;\t\t \n\tstruct list_head file_list;\t \n\n\t \n\tint ddcb_software_timeout;\t \n\tint skip_recovery;\t\t \n\tint kill_timeout;\t\t \n};\n\n \nenum genwqe_requ_state {\n\tGENWQE_REQU_NEW      = 0,\n\tGENWQE_REQU_ENQUEUED = 1,\n\tGENWQE_REQU_TAPPED   = 2,\n\tGENWQE_REQU_FINISHED = 3,\n\tGENWQE_REQU_STATE_MAX,\n};\n\n \nstruct genwqe_sgl {\n\tdma_addr_t sgl_dma_addr;\n\tstruct sg_entry *sgl;\n\tsize_t sgl_size;\t \n\n\tvoid __user *user_addr;  \n\tsize_t user_size;        \n\n\tint write;\n\n\tunsigned long nr_pages;\n\tunsigned long fpage_offs;\n\tsize_t fpage_size;\n\tsize_t lpage_size;\n\n\tvoid *fpage;\n\tdma_addr_t fpage_dma_addr;\n\n\tvoid *lpage;\n\tdma_addr_t lpage_dma_addr;\n};\n\nint genwqe_alloc_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\n\t\t\t  void __user *user_addr, size_t user_size, int write);\n\nint genwqe_setup_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\n\t\t     dma_addr_t *dma_list);\n\nint genwqe_free_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl);\n\n \nstruct ddcb_requ {\n\t \n\tenum genwqe_requ_state req_state;  \n\tint num;\t\t\t   \n\tstruct ddcb_queue *queue;\t   \n\n\tstruct dma_mapping  dma_mappings[DDCB_FIXUPS];\n\tstruct genwqe_sgl sgls[DDCB_FIXUPS];\n\n\t \n\tstruct genwqe_ddcb_cmd cmd;\t \n\tstruct genwqe_debug_data debug_data;\n};\n\n \nstruct genwqe_file {\n\tstruct genwqe_dev *cd;\n\tstruct genwqe_driver *client;\n\tstruct file *filp;\n\n\tstruct fasync_struct *async_queue;\n\tstruct pid *opener;\n\tstruct list_head list;\t\t \n\n\tspinlock_t map_lock;\t\t \n\tstruct list_head map_list;\t \n\n\tspinlock_t pin_lock;\t\t \n\tstruct list_head pin_list;\t \n};\n\nint  genwqe_setup_service_layer(struct genwqe_dev *cd);  \nint  genwqe_finish_queue(struct genwqe_dev *cd);\nint  genwqe_release_service_layer(struct genwqe_dev *cd);\n\n \nstatic inline int genwqe_get_slu_id(struct genwqe_dev *cd)\n{\n\treturn (int)((cd->slu_unitcfg >> 32) & 0xff);\n}\n\nint  genwqe_ddcbs_in_flight(struct genwqe_dev *cd);\n\nu8   genwqe_card_type(struct genwqe_dev *cd);\nint  genwqe_card_reset(struct genwqe_dev *cd);\nint  genwqe_set_interrupt_capability(struct genwqe_dev *cd, int count);\nvoid genwqe_reset_interrupt_capability(struct genwqe_dev *cd);\n\nint  genwqe_device_create(struct genwqe_dev *cd);\nint  genwqe_device_remove(struct genwqe_dev *cd);\n\n \nvoid genwqe_init_debugfs(struct genwqe_dev *cd);\nvoid genqwe_exit_debugfs(struct genwqe_dev *cd);\n\nint  genwqe_read_softreset(struct genwqe_dev *cd);\n\n \nint  genwqe_recovery_on_fatal_gfir_required(struct genwqe_dev *cd);\nint  genwqe_flash_readback_fails(struct genwqe_dev *cd);\n\n \nint genwqe_write_vreg(struct genwqe_dev *cd, u32 reg, u64 val, int func);\n\n \nu64 genwqe_read_vreg(struct genwqe_dev *cd, u32 reg, int func);\n\n \nint  genwqe_ffdc_buff_size(struct genwqe_dev *cd, int unit_id);\nint  genwqe_ffdc_buff_read(struct genwqe_dev *cd, int unit_id,\n\t\t\t   struct genwqe_reg *regs, unsigned int max_regs);\nint  genwqe_read_ffdc_regs(struct genwqe_dev *cd, struct genwqe_reg *regs,\n\t\t\t   unsigned int max_regs, int all);\nint  genwqe_ffdc_dump_dma(struct genwqe_dev *cd,\n\t\t\t  struct genwqe_reg *regs, unsigned int max_regs);\n\nint  genwqe_init_debug_data(struct genwqe_dev *cd,\n\t\t\t    struct genwqe_debug_data *d);\n\nvoid genwqe_init_crc32(void);\nint  genwqe_read_app_id(struct genwqe_dev *cd, char *app_name, int len);\n\n \nint  genwqe_user_vmap(struct genwqe_dev *cd, struct dma_mapping *m,\n\t\t      void *uaddr, unsigned long size);\n\nint  genwqe_user_vunmap(struct genwqe_dev *cd, struct dma_mapping *m);\n\nstatic inline bool dma_mapping_used(struct dma_mapping *m)\n{\n\tif (!m)\n\t\treturn false;\n\treturn m->size != 0;\n}\n\n \nint  __genwqe_execute_ddcb(struct genwqe_dev *cd,\n\t\t\t   struct genwqe_ddcb_cmd *cmd, unsigned int f_flags);\n\n \nint  __genwqe_execute_raw_ddcb(struct genwqe_dev *cd,\n\t\t\t       struct genwqe_ddcb_cmd *cmd,\n\t\t\t       unsigned int f_flags);\nint  __genwqe_enqueue_ddcb(struct genwqe_dev *cd,\n\t\t\t   struct ddcb_requ *req,\n\t\t\t   unsigned int f_flags);\n\nint  __genwqe_wait_ddcb(struct genwqe_dev *cd, struct ddcb_requ *req);\nint  __genwqe_purge_ddcb(struct genwqe_dev *cd, struct ddcb_requ *req);\n\n \nint __genwqe_writeq(struct genwqe_dev *cd, u64 byte_offs, u64 val);\nu64 __genwqe_readq(struct genwqe_dev *cd, u64 byte_offs);\nint __genwqe_writel(struct genwqe_dev *cd, u64 byte_offs, u32 val);\nu32 __genwqe_readl(struct genwqe_dev *cd, u64 byte_offs);\n\nvoid *__genwqe_alloc_consistent(struct genwqe_dev *cd, size_t size,\n\t\t\t\t dma_addr_t *dma_handle);\nvoid __genwqe_free_consistent(struct genwqe_dev *cd, size_t size,\n\t\t\t      void *vaddr, dma_addr_t dma_handle);\n\n \nint  genwqe_base_clock_frequency(struct genwqe_dev *cd);\n\n \nvoid genwqe_stop_traps(struct genwqe_dev *cd);\nvoid genwqe_start_traps(struct genwqe_dev *cd);\n\n \nbool genwqe_need_err_masking(struct genwqe_dev *cd);\n\n \nstatic inline int genwqe_is_privileged(struct genwqe_dev *cd)\n{\n\treturn cd->is_privileged;\n}\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}