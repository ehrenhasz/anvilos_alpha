{
  "module_name": "grukservices.c",
  "hash_id": "44c9f53711ee9fc1094cc396325d09e833dcbb297a87d88ff90ff2a68d8fb430",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-gru/grukservices.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/device.h>\n#include <linux/miscdevice.h>\n#include <linux/proc_fs.h>\n#include <linux/interrupt.h>\n#include <linux/sync_core.h>\n#include <linux/uaccess.h>\n#include <linux/delay.h>\n#include <linux/export.h>\n#include <asm/io_apic.h>\n#include \"gru.h\"\n#include \"grulib.h\"\n#include \"grutables.h\"\n#include \"grukservices.h\"\n#include \"gru_instructions.h\"\n#include <asm/uv/uv_hub.h>\n\n \n\n\n#define ASYNC_HAN_TO_BID(h)\t((h) - 1)\n#define ASYNC_BID_TO_HAN(b)\t((b) + 1)\n#define ASYNC_HAN_TO_BS(h)\tgru_base[ASYNC_HAN_TO_BID(h)]\n\n#define GRU_NUM_KERNEL_CBR\t1\n#define GRU_NUM_KERNEL_DSR_BYTES 256\n#define GRU_NUM_KERNEL_DSR_CL\t(GRU_NUM_KERNEL_DSR_BYTES /\t\t\\\n\t\t\t\t\tGRU_CACHE_LINE_BYTES)\n\n \n#define IMA\t\t\tIMA_CB_DELAY\n\n \n#define __gru_cacheline_aligned__                               \\\n\t__attribute__((__aligned__(GRU_CACHE_LINE_BYTES)))\n\n#define MAGIC\t0x1234567887654321UL\n\n \n#define EXCEPTION_RETRY_LIMIT\t3\n\n \n#define MQS_EMPTY\t\t0\n#define MQS_FULL\t\t1\n#define MQS_NOOP\t\t2\n\n \n \nstruct message_queue {\n\tunion gru_mesqhead\thead __gru_cacheline_aligned__;\t \n\tint\t\t\tqlines;\t\t\t\t \n\tlong \t\t\thstatus[2];\n\tvoid \t\t\t*next __gru_cacheline_aligned__; \n\tvoid \t\t\t*limit;\n\tvoid \t\t\t*start;\n\tvoid \t\t\t*start2;\n\tchar\t\t\tdata ____cacheline_aligned;\t \n};\n\n \nstruct message_header {\n\tchar\tpresent;\n\tchar\tpresent2;\n\tchar \tlines;\n\tchar\tfill;\n};\n\n#define HSTATUS(mq, h)\t((mq) + offsetof(struct message_queue, hstatus[h]))\n\n \nstatic void gru_load_kernel_context(struct gru_blade_state *bs, int blade_id)\n{\n\tstruct gru_state *gru;\n\tstruct gru_thread_state *kgts;\n\tvoid *vaddr;\n\tint ctxnum, ncpus;\n\n\tup_read(&bs->bs_kgts_sema);\n\tdown_write(&bs->bs_kgts_sema);\n\n\tif (!bs->bs_kgts) {\n\t\tdo {\n\t\t\tbs->bs_kgts = gru_alloc_gts(NULL, 0, 0, 0, 0, 0);\n\t\t\tif (!IS_ERR(bs->bs_kgts))\n\t\t\t\tbreak;\n\t\t\tmsleep(1);\n\t\t} while (true);\n\t\tbs->bs_kgts->ts_user_blade_id = blade_id;\n\t}\n\tkgts = bs->bs_kgts;\n\n\tif (!kgts->ts_gru) {\n\t\tSTAT(load_kernel_context);\n\t\tncpus = uv_blade_nr_possible_cpus(blade_id);\n\t\tkgts->ts_cbr_au_count = GRU_CB_COUNT_TO_AU(\n\t\t\tGRU_NUM_KERNEL_CBR * ncpus + bs->bs_async_cbrs);\n\t\tkgts->ts_dsr_au_count = GRU_DS_BYTES_TO_AU(\n\t\t\tGRU_NUM_KERNEL_DSR_BYTES * ncpus +\n\t\t\t\tbs->bs_async_dsr_bytes);\n\t\twhile (!gru_assign_gru_context(kgts)) {\n\t\t\tmsleep(1);\n\t\t\tgru_steal_context(kgts);\n\t\t}\n\t\tgru_load_context(kgts);\n\t\tgru = bs->bs_kgts->ts_gru;\n\t\tvaddr = gru->gs_gru_base_vaddr;\n\t\tctxnum = kgts->ts_ctxnum;\n\t\tbs->kernel_cb = get_gseg_base_address_cb(vaddr, ctxnum, 0);\n\t\tbs->kernel_dsr = get_gseg_base_address_ds(vaddr, ctxnum, 0);\n\t}\n\tdowngrade_write(&bs->bs_kgts_sema);\n}\n\n \nstatic int gru_free_kernel_contexts(void)\n{\n\tstruct gru_blade_state *bs;\n\tstruct gru_thread_state *kgts;\n\tint bid, ret = 0;\n\n\tfor (bid = 0; bid < GRU_MAX_BLADES; bid++) {\n\t\tbs = gru_base[bid];\n\t\tif (!bs)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (down_write_trylock(&bs->bs_kgts_sema)) {\n\t\t\tkgts = bs->bs_kgts;\n\t\t\tif (kgts && kgts->ts_gru)\n\t\t\t\tgru_unload_context(kgts, 0);\n\t\t\tbs->bs_kgts = NULL;\n\t\t\tup_write(&bs->bs_kgts_sema);\n\t\t\tkfree(kgts);\n\t\t} else {\n\t\t\tret++;\n\t\t}\n\t}\n\treturn ret;\n}\n\n \nstatic struct gru_blade_state *gru_lock_kernel_context(int blade_id)\n{\n\tstruct gru_blade_state *bs;\n\tint bid;\n\n\tSTAT(lock_kernel_context);\nagain:\n\tbid = blade_id < 0 ? uv_numa_blade_id() : blade_id;\n\tbs = gru_base[bid];\n\n\t \n\tdown_read(&bs->bs_kgts_sema);\n\tif (blade_id < 0 && bid != uv_numa_blade_id()) {\n\t\tup_read(&bs->bs_kgts_sema);\n\t\tgoto again;\n\t}\n\tif (!bs->bs_kgts || !bs->bs_kgts->ts_gru)\n\t\tgru_load_kernel_context(bs, bid);\n\treturn bs;\n\n}\n\n \nstatic void gru_unlock_kernel_context(int blade_id)\n{\n\tstruct gru_blade_state *bs;\n\n\tbs = gru_base[blade_id];\n\tup_read(&bs->bs_kgts_sema);\n\tSTAT(unlock_kernel_context);\n}\n\n \nstatic int gru_get_cpu_resources(int dsr_bytes, void **cb, void **dsr)\n{\n\tstruct gru_blade_state *bs;\n\tint lcpu;\n\n\tBUG_ON(dsr_bytes > GRU_NUM_KERNEL_DSR_BYTES);\n\tpreempt_disable();\n\tbs = gru_lock_kernel_context(-1);\n\tlcpu = uv_blade_processor_id();\n\t*cb = bs->kernel_cb + lcpu * GRU_HANDLE_STRIDE;\n\t*dsr = bs->kernel_dsr + lcpu * GRU_NUM_KERNEL_DSR_BYTES;\n\treturn 0;\n}\n\n \nstatic void gru_free_cpu_resources(void *cb, void *dsr)\n{\n\tgru_unlock_kernel_context(uv_numa_blade_id());\n\tpreempt_enable();\n}\n\n \nunsigned long gru_reserve_async_resources(int blade_id, int cbrs, int dsr_bytes,\n\t\t\tstruct completion *cmp)\n{\n\tstruct gru_blade_state *bs;\n\tstruct gru_thread_state *kgts;\n\tint ret = 0;\n\n\tbs = gru_base[blade_id];\n\n\tdown_write(&bs->bs_kgts_sema);\n\n\t \n\tif (bs->bs_async_dsr_bytes + bs->bs_async_cbrs)\n\t\tgoto done;\n\tbs->bs_async_dsr_bytes = dsr_bytes;\n\tbs->bs_async_cbrs = cbrs;\n\tbs->bs_async_wq = cmp;\n\tkgts = bs->bs_kgts;\n\n\t \n\tif (kgts && kgts->ts_gru)\n\t\tgru_unload_context(kgts, 0);\n\tret = ASYNC_BID_TO_HAN(blade_id);\n\ndone:\n\tup_write(&bs->bs_kgts_sema);\n\treturn ret;\n}\n\n \nvoid gru_release_async_resources(unsigned long han)\n{\n\tstruct gru_blade_state *bs = ASYNC_HAN_TO_BS(han);\n\n\tdown_write(&bs->bs_kgts_sema);\n\tbs->bs_async_dsr_bytes = 0;\n\tbs->bs_async_cbrs = 0;\n\tbs->bs_async_wq = NULL;\n\tup_write(&bs->bs_kgts_sema);\n}\n\n \nvoid gru_wait_async_cbr(unsigned long han)\n{\n\tstruct gru_blade_state *bs = ASYNC_HAN_TO_BS(han);\n\n\twait_for_completion(bs->bs_async_wq);\n\tmb();\n}\n\n \nvoid gru_lock_async_resource(unsigned long han,  void **cb, void **dsr)\n{\n\tstruct gru_blade_state *bs = ASYNC_HAN_TO_BS(han);\n\tint blade_id = ASYNC_HAN_TO_BID(han);\n\tint ncpus;\n\n\tgru_lock_kernel_context(blade_id);\n\tncpus = uv_blade_nr_possible_cpus(blade_id);\n\tif (cb)\n\t\t*cb = bs->kernel_cb + ncpus * GRU_HANDLE_STRIDE;\n\tif (dsr)\n\t\t*dsr = bs->kernel_dsr + ncpus * GRU_NUM_KERNEL_DSR_BYTES;\n}\n\n \nvoid gru_unlock_async_resource(unsigned long han)\n{\n\tint blade_id = ASYNC_HAN_TO_BID(han);\n\n\tgru_unlock_kernel_context(blade_id);\n}\n\n \nint gru_get_cb_exception_detail(void *cb,\n\t\tstruct control_block_extended_exc_detail *excdet)\n{\n\tstruct gru_control_block_extended *cbe;\n\tstruct gru_thread_state *kgts = NULL;\n\tunsigned long off;\n\tint cbrnum, bid;\n\n\t \n\tfor_each_possible_blade(bid) {\n\t\tif (!gru_base[bid])\n\t\t\tbreak;\n\t\tkgts = gru_base[bid]->bs_kgts;\n\t\tif (!kgts || !kgts->ts_gru)\n\t\t\tcontinue;\n\t\toff = cb - kgts->ts_gru->gs_gru_base_vaddr;\n\t\tif (off < GRU_SIZE)\n\t\t\tbreak;\n\t\tkgts = NULL;\n\t}\n\tBUG_ON(!kgts);\n\tcbrnum = thread_cbr_number(kgts, get_cb_number(cb));\n\tcbe = get_cbe(GRUBASE(cb), cbrnum);\n\tgru_flush_cache(cbe);\t \n\tsync_core();\n\texcdet->opc = cbe->opccpy;\n\texcdet->exopc = cbe->exopccpy;\n\texcdet->ecause = cbe->ecause;\n\texcdet->exceptdet0 = cbe->idef1upd;\n\texcdet->exceptdet1 = cbe->idef3upd;\n\tgru_flush_cache(cbe);\n\treturn 0;\n}\n\nstatic char *gru_get_cb_exception_detail_str(int ret, void *cb,\n\t\t\t\t\t     char *buf, int size)\n{\n\tstruct gru_control_block_status *gen = cb;\n\tstruct control_block_extended_exc_detail excdet;\n\n\tif (ret > 0 && gen->istatus == CBS_EXCEPTION) {\n\t\tgru_get_cb_exception_detail(cb, &excdet);\n\t\tsnprintf(buf, size,\n\t\t\t\"GRU:%d exception: cb %p, opc %d, exopc %d, ecause 0x%x,\"\n\t\t\t\"excdet0 0x%lx, excdet1 0x%x\", smp_processor_id(),\n\t\t\tgen, excdet.opc, excdet.exopc, excdet.ecause,\n\t\t\texcdet.exceptdet0, excdet.exceptdet1);\n\t} else {\n\t\tsnprintf(buf, size, \"No exception\");\n\t}\n\treturn buf;\n}\n\nstatic int gru_wait_idle_or_exception(struct gru_control_block_status *gen)\n{\n\twhile (gen->istatus >= CBS_ACTIVE) {\n\t\tcpu_relax();\n\t\tbarrier();\n\t}\n\treturn gen->istatus;\n}\n\nstatic int gru_retry_exception(void *cb)\n{\n\tstruct gru_control_block_status *gen = cb;\n\tstruct control_block_extended_exc_detail excdet;\n\tint retry = EXCEPTION_RETRY_LIMIT;\n\n\twhile (1)  {\n\t\tif (gru_wait_idle_or_exception(gen) == CBS_IDLE)\n\t\t\treturn CBS_IDLE;\n\t\tif (gru_get_cb_message_queue_substatus(cb))\n\t\t\treturn CBS_EXCEPTION;\n\t\tgru_get_cb_exception_detail(cb, &excdet);\n\t\tif ((excdet.ecause & ~EXCEPTION_RETRY_BITS) ||\n\t\t\t\t(excdet.cbrexecstatus & CBR_EXS_ABORT_OCC))\n\t\t\tbreak;\n\t\tif (retry-- == 0)\n\t\t\tbreak;\n\t\tgen->icmd = 1;\n\t\tgru_flush_cache(gen);\n\t}\n\treturn CBS_EXCEPTION;\n}\n\nint gru_check_status_proc(void *cb)\n{\n\tstruct gru_control_block_status *gen = cb;\n\tint ret;\n\n\tret = gen->istatus;\n\tif (ret == CBS_EXCEPTION)\n\t\tret = gru_retry_exception(cb);\n\trmb();\n\treturn ret;\n\n}\n\nint gru_wait_proc(void *cb)\n{\n\tstruct gru_control_block_status *gen = cb;\n\tint ret;\n\n\tret = gru_wait_idle_or_exception(gen);\n\tif (ret == CBS_EXCEPTION)\n\t\tret = gru_retry_exception(cb);\n\trmb();\n\treturn ret;\n}\n\nstatic void gru_abort(int ret, void *cb, char *str)\n{\n\tchar buf[GRU_EXC_STR_SIZE];\n\n\tpanic(\"GRU FATAL ERROR: %s - %s\\n\", str,\n\t      gru_get_cb_exception_detail_str(ret, cb, buf, sizeof(buf)));\n}\n\nvoid gru_wait_abort_proc(void *cb)\n{\n\tint ret;\n\n\tret = gru_wait_proc(cb);\n\tif (ret)\n\t\tgru_abort(ret, cb, \"gru_wait_abort\");\n}\n\n\n \n\n \n#define MQIE_AGAIN\t\t-1\t \n\n\n \nstatic inline int get_present2(void *p)\n{\n\tstruct message_header *mhdr = p + GRU_CACHE_LINE_BYTES;\n\treturn mhdr->present;\n}\n\nstatic inline void restore_present2(void *p, int val)\n{\n\tstruct message_header *mhdr = p + GRU_CACHE_LINE_BYTES;\n\tmhdr->present = val;\n}\n\n \nint gru_create_message_queue(struct gru_message_queue_desc *mqd,\n\t\tvoid *p, unsigned int bytes, int nasid, int vector, int apicid)\n{\n\tstruct message_queue *mq = p;\n\tunsigned int qlines;\n\n\tqlines = bytes / GRU_CACHE_LINE_BYTES - 2;\n\tmemset(mq, 0, bytes);\n\tmq->start = &mq->data;\n\tmq->start2 = &mq->data + (qlines / 2 - 1) * GRU_CACHE_LINE_BYTES;\n\tmq->next = &mq->data;\n\tmq->limit = &mq->data + (qlines - 2) * GRU_CACHE_LINE_BYTES;\n\tmq->qlines = qlines;\n\tmq->hstatus[0] = 0;\n\tmq->hstatus[1] = 1;\n\tmq->head = gru_mesq_head(2, qlines / 2 + 1);\n\tmqd->mq = mq;\n\tmqd->mq_gpa = uv_gpa(mq);\n\tmqd->qlines = qlines;\n\tmqd->interrupt_pnode = nasid >> 1;\n\tmqd->interrupt_vector = vector;\n\tmqd->interrupt_apicid = apicid;\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(gru_create_message_queue);\n\n \nstatic int send_noop_message(void *cb, struct gru_message_queue_desc *mqd,\n\t\t\t\tvoid *mesg)\n{\n\tconst struct message_header noop_header = {\n\t\t\t\t\t.present = MQS_NOOP, .lines = 1};\n\tunsigned long m;\n\tint substatus, ret;\n\tstruct message_header save_mhdr, *mhdr = mesg;\n\n\tSTAT(mesq_noop);\n\tsave_mhdr = *mhdr;\n\t*mhdr = noop_header;\n\tgru_mesq(cb, mqd->mq_gpa, gru_get_tri(mhdr), 1, IMA);\n\tret = gru_wait(cb);\n\n\tif (ret) {\n\t\tsubstatus = gru_get_cb_message_queue_substatus(cb);\n\t\tswitch (substatus) {\n\t\tcase CBSS_NO_ERROR:\n\t\t\tSTAT(mesq_noop_unexpected_error);\n\t\t\tret = MQE_UNEXPECTED_CB_ERR;\n\t\t\tbreak;\n\t\tcase CBSS_LB_OVERFLOWED:\n\t\t\tSTAT(mesq_noop_lb_overflow);\n\t\t\tret = MQE_CONGESTION;\n\t\t\tbreak;\n\t\tcase CBSS_QLIMIT_REACHED:\n\t\t\tSTAT(mesq_noop_qlimit_reached);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\tcase CBSS_AMO_NACKED:\n\t\t\tSTAT(mesq_noop_amo_nacked);\n\t\t\tret = MQE_CONGESTION;\n\t\t\tbreak;\n\t\tcase CBSS_PUT_NACKED:\n\t\t\tSTAT(mesq_noop_put_nacked);\n\t\t\tm = mqd->mq_gpa + (gru_get_amo_value_head(cb) << 6);\n\t\t\tgru_vstore(cb, m, gru_get_tri(mesg), XTYPE_CL, 1, 1,\n\t\t\t\t\t\tIMA);\n\t\t\tif (gru_wait(cb) == CBS_IDLE)\n\t\t\t\tret = MQIE_AGAIN;\n\t\t\telse\n\t\t\t\tret = MQE_UNEXPECTED_CB_ERR;\n\t\t\tbreak;\n\t\tcase CBSS_PAGE_OVERFLOW:\n\t\t\tSTAT(mesq_noop_page_overflow);\n\t\t\tfallthrough;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t}\n\t*mhdr = save_mhdr;\n\treturn ret;\n}\n\n \nstatic int send_message_queue_full(void *cb, struct gru_message_queue_desc *mqd,\n\t\t\t\tvoid *mesg, int lines)\n{\n\tunion gru_mesqhead mqh;\n\tunsigned int limit, head;\n\tunsigned long avalue;\n\tint half, qlines;\n\n\t \n\tavalue = gru_get_amo_value(cb);\n\thead = gru_get_amo_value_head(cb);\n\tlimit = gru_get_amo_value_limit(cb);\n\n\tqlines = mqd->qlines;\n\thalf = (limit != qlines);\n\n\tif (half)\n\t\tmqh = gru_mesq_head(qlines / 2 + 1, qlines);\n\telse\n\t\tmqh = gru_mesq_head(2, qlines / 2 + 1);\n\n\t \n\tgru_gamir(cb, EOP_IR_CLR, HSTATUS(mqd->mq_gpa, half), XTYPE_DW, IMA);\n\tif (gru_wait(cb) != CBS_IDLE)\n\t\tgoto cberr;\n\tif (!gru_get_amo_value(cb)) {\n\t\tSTAT(mesq_qf_locked);\n\t\treturn MQE_QUEUE_FULL;\n\t}\n\n\t \n\tif (head != limit) {\n\t\tif (send_noop_message(cb, mqd, mesg)) {\n\t\t\tgru_gamir(cb, EOP_IR_INC, HSTATUS(mqd->mq_gpa, half),\n\t\t\t\t\tXTYPE_DW, IMA);\n\t\t\tif (gru_wait(cb) != CBS_IDLE)\n\t\t\t\tgoto cberr;\n\t\t\tSTAT(mesq_qf_noop_not_full);\n\t\t\treturn MQIE_AGAIN;\n\t\t}\n\t\tavalue++;\n\t}\n\n\t \n\tgru_gamer(cb, EOP_ERR_CSWAP, mqd->mq_gpa, XTYPE_DW, mqh.val, avalue,\n\t\t\t\t\t\t\tIMA);\n\tif (gru_wait(cb) != CBS_IDLE)\n\t\tgoto cberr;\n\n\t \n\tif (gru_get_amo_value(cb) != avalue) {\n\t\tSTAT(mesq_qf_switch_head_failed);\n\t\tgru_gamir(cb, EOP_IR_INC, HSTATUS(mqd->mq_gpa, half), XTYPE_DW,\n\t\t\t\t\t\t\tIMA);\n\t\tif (gru_wait(cb) != CBS_IDLE)\n\t\t\tgoto cberr;\n\t}\n\treturn MQIE_AGAIN;\ncberr:\n\tSTAT(mesq_qf_unexpected_error);\n\treturn MQE_UNEXPECTED_CB_ERR;\n}\n\n \nstatic int send_message_put_nacked(void *cb, struct gru_message_queue_desc *mqd,\n\t\t\tvoid *mesg, int lines)\n{\n\tunsigned long m;\n\tint ret, loops = 200;\t \n\n\tm = mqd->mq_gpa + (gru_get_amo_value_head(cb) << 6);\n\tif (lines == 2) {\n\t\tgru_vset(cb, m, 0, XTYPE_CL, lines, 1, IMA);\n\t\tif (gru_wait(cb) != CBS_IDLE)\n\t\t\treturn MQE_UNEXPECTED_CB_ERR;\n\t}\n\tgru_vstore(cb, m, gru_get_tri(mesg), XTYPE_CL, lines, 1, IMA);\n\tif (gru_wait(cb) != CBS_IDLE)\n\t\treturn MQE_UNEXPECTED_CB_ERR;\n\n\tif (!mqd->interrupt_vector)\n\t\treturn MQE_OK;\n\n\t \n\tdo {\n\t\tret = send_noop_message(cb, mqd, mesg);\n\t} while ((ret == MQIE_AGAIN || ret == MQE_CONGESTION) && (loops-- > 0));\n\n\tif (ret == MQIE_AGAIN || ret == MQE_CONGESTION) {\n\t\t \n\t\tret = MQE_OK;\n\t}\n\treturn ret;\n}\n\n \nstatic int send_message_failure(void *cb, struct gru_message_queue_desc *mqd,\n\t\t\t\tvoid *mesg, int lines)\n{\n\tint substatus, ret = 0;\n\n\tsubstatus = gru_get_cb_message_queue_substatus(cb);\n\tswitch (substatus) {\n\tcase CBSS_NO_ERROR:\n\t\tSTAT(mesq_send_unexpected_error);\n\t\tret = MQE_UNEXPECTED_CB_ERR;\n\t\tbreak;\n\tcase CBSS_LB_OVERFLOWED:\n\t\tSTAT(mesq_send_lb_overflow);\n\t\tret = MQE_CONGESTION;\n\t\tbreak;\n\tcase CBSS_QLIMIT_REACHED:\n\t\tSTAT(mesq_send_qlimit_reached);\n\t\tret = send_message_queue_full(cb, mqd, mesg, lines);\n\t\tbreak;\n\tcase CBSS_AMO_NACKED:\n\t\tSTAT(mesq_send_amo_nacked);\n\t\tret = MQE_CONGESTION;\n\t\tbreak;\n\tcase CBSS_PUT_NACKED:\n\t\tSTAT(mesq_send_put_nacked);\n\t\tret = send_message_put_nacked(cb, mqd, mesg, lines);\n\t\tbreak;\n\tcase CBSS_PAGE_OVERFLOW:\n\t\tSTAT(mesq_page_overflow);\n\t\tfallthrough;\n\tdefault:\n\t\tBUG();\n\t}\n\treturn ret;\n}\n\n \nint gru_send_message_gpa(struct gru_message_queue_desc *mqd, void *mesg,\n\t\t\t\tunsigned int bytes)\n{\n\tstruct message_header *mhdr;\n\tvoid *cb;\n\tvoid *dsr;\n\tint istatus, clines, ret;\n\n\tSTAT(mesq_send);\n\tBUG_ON(bytes < sizeof(int) || bytes > 2 * GRU_CACHE_LINE_BYTES);\n\n\tclines = DIV_ROUND_UP(bytes, GRU_CACHE_LINE_BYTES);\n\tif (gru_get_cpu_resources(bytes, &cb, &dsr))\n\t\treturn MQE_BUG_NO_RESOURCES;\n\tmemcpy(dsr, mesg, bytes);\n\tmhdr = dsr;\n\tmhdr->present = MQS_FULL;\n\tmhdr->lines = clines;\n\tif (clines == 2) {\n\t\tmhdr->present2 = get_present2(mhdr);\n\t\trestore_present2(mhdr, MQS_FULL);\n\t}\n\n\tdo {\n\t\tret = MQE_OK;\n\t\tgru_mesq(cb, mqd->mq_gpa, gru_get_tri(mhdr), clines, IMA);\n\t\tistatus = gru_wait(cb);\n\t\tif (istatus != CBS_IDLE)\n\t\t\tret = send_message_failure(cb, mqd, dsr, clines);\n\t} while (ret == MQIE_AGAIN);\n\tgru_free_cpu_resources(cb, dsr);\n\n\tif (ret)\n\t\tSTAT(mesq_send_failed);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gru_send_message_gpa);\n\n \nvoid gru_free_message(struct gru_message_queue_desc *mqd, void *mesg)\n{\n\tstruct message_queue *mq = mqd->mq;\n\tstruct message_header *mhdr = mq->next;\n\tvoid *next, *pnext;\n\tint half = -1;\n\tint lines = mhdr->lines;\n\n\tif (lines == 2)\n\t\trestore_present2(mhdr, MQS_EMPTY);\n\tmhdr->present = MQS_EMPTY;\n\n\tpnext = mq->next;\n\tnext = pnext + GRU_CACHE_LINE_BYTES * lines;\n\tif (next == mq->limit) {\n\t\tnext = mq->start;\n\t\thalf = 1;\n\t} else if (pnext < mq->start2 && next >= mq->start2) {\n\t\thalf = 0;\n\t}\n\n\tif (half >= 0)\n\t\tmq->hstatus[half] = 1;\n\tmq->next = next;\n}\nEXPORT_SYMBOL_GPL(gru_free_message);\n\n \nvoid *gru_get_next_message(struct gru_message_queue_desc *mqd)\n{\n\tstruct message_queue *mq = mqd->mq;\n\tstruct message_header *mhdr = mq->next;\n\tint present = mhdr->present;\n\n\t \n\twhile (present == MQS_NOOP) {\n\t\tgru_free_message(mqd, mhdr);\n\t\tmhdr = mq->next;\n\t\tpresent = mhdr->present;\n\t}\n\n\t \n\tif (present == MQS_FULL && mhdr->lines == 2 &&\n\t\t\t\tget_present2(mhdr) == MQS_EMPTY)\n\t\tpresent = MQS_EMPTY;\n\n\tif (!present) {\n\t\tSTAT(mesq_receive_none);\n\t\treturn NULL;\n\t}\n\n\tif (mhdr->lines == 2)\n\t\trestore_present2(mhdr, mhdr->present2);\n\n\tSTAT(mesq_receive);\n\treturn mhdr;\n}\nEXPORT_SYMBOL_GPL(gru_get_next_message);\n\n \n\n \nint gru_read_gpa(unsigned long *value, unsigned long gpa)\n{\n\tvoid *cb;\n\tvoid *dsr;\n\tint ret, iaa;\n\n\tSTAT(read_gpa);\n\tif (gru_get_cpu_resources(GRU_NUM_KERNEL_DSR_BYTES, &cb, &dsr))\n\t\treturn MQE_BUG_NO_RESOURCES;\n\tiaa = gpa >> 62;\n\tgru_vload_phys(cb, gpa, gru_get_tri(dsr), iaa, IMA);\n\tret = gru_wait(cb);\n\tif (ret == CBS_IDLE)\n\t\t*value = *(unsigned long *)dsr;\n\tgru_free_cpu_resources(cb, dsr);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gru_read_gpa);\n\n\n \nint gru_copy_gpa(unsigned long dest_gpa, unsigned long src_gpa,\n\t\t\t\tunsigned int bytes)\n{\n\tvoid *cb;\n\tvoid *dsr;\n\tint ret;\n\n\tSTAT(copy_gpa);\n\tif (gru_get_cpu_resources(GRU_NUM_KERNEL_DSR_BYTES, &cb, &dsr))\n\t\treturn MQE_BUG_NO_RESOURCES;\n\tgru_bcopy(cb, src_gpa, dest_gpa, gru_get_tri(dsr),\n\t\t  XTYPE_B, bytes, GRU_NUM_KERNEL_DSR_CL, IMA);\n\tret = gru_wait(cb);\n\tgru_free_cpu_resources(cb, dsr);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(gru_copy_gpa);\n\n \n \n\nstatic int quicktest0(unsigned long arg)\n{\n\tunsigned long word0;\n\tunsigned long word1;\n\tvoid *cb;\n\tvoid *dsr;\n\tunsigned long *p;\n\tint ret = -EIO;\n\n\tif (gru_get_cpu_resources(GRU_CACHE_LINE_BYTES, &cb, &dsr))\n\t\treturn MQE_BUG_NO_RESOURCES;\n\tp = dsr;\n\tword0 = MAGIC;\n\tword1 = 0;\n\n\tgru_vload(cb, uv_gpa(&word0), gru_get_tri(dsr), XTYPE_DW, 1, 1, IMA);\n\tif (gru_wait(cb) != CBS_IDLE) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest0: CBR failure 1\\n\", smp_processor_id());\n\t\tgoto done;\n\t}\n\n\tif (*p != MAGIC) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest0 bad magic 0x%lx\\n\", smp_processor_id(), *p);\n\t\tgoto done;\n\t}\n\tgru_vstore(cb, uv_gpa(&word1), gru_get_tri(dsr), XTYPE_DW, 1, 1, IMA);\n\tif (gru_wait(cb) != CBS_IDLE) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest0: CBR failure 2\\n\", smp_processor_id());\n\t\tgoto done;\n\t}\n\n\tif (word0 != word1 || word1 != MAGIC) {\n\t\tprintk(KERN_DEBUG\n\t\t       \"GRU:%d quicktest0 err: found 0x%lx, expected 0x%lx\\n\",\n\t\t     smp_processor_id(), word1, MAGIC);\n\t\tgoto done;\n\t}\n\tret = 0;\n\ndone:\n\tgru_free_cpu_resources(cb, dsr);\n\treturn ret;\n}\n\n#define ALIGNUP(p, q)\t((void *)(((unsigned long)(p) + (q) - 1) & ~(q - 1)))\n\nstatic int quicktest1(unsigned long arg)\n{\n\tstruct gru_message_queue_desc mqd;\n\tvoid *p, *mq;\n\tint i, ret = -EIO;\n\tchar mes[GRU_CACHE_LINE_BYTES], *m;\n\n\t \n\tp = kmalloc(4096, 0);\n\tif (p == NULL)\n\t\treturn -ENOMEM;\n\tmq = ALIGNUP(p, 1024);\n\tmemset(mes, 0xee, sizeof(mes));\n\n\tgru_create_message_queue(&mqd, mq, 8 * GRU_CACHE_LINE_BYTES, 0, 0, 0);\n\tfor (i = 0; i < 6; i++) {\n\t\tmes[8] = i;\n\t\tdo {\n\t\t\tret = gru_send_message_gpa(&mqd, mes, sizeof(mes));\n\t\t} while (ret == MQE_CONGESTION);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (ret != MQE_QUEUE_FULL || i != 4) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest1: unexpected status %d, i %d\\n\",\n\t\t       smp_processor_id(), ret, i);\n\t\tgoto done;\n\t}\n\n\tfor (i = 0; i < 6; i++) {\n\t\tm = gru_get_next_message(&mqd);\n\t\tif (!m || m[8] != i)\n\t\t\tbreak;\n\t\tgru_free_message(&mqd, m);\n\t}\n\tif (i != 4) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest2: bad message, i %d, m %p, m8 %d\\n\",\n\t\t\tsmp_processor_id(), i, m, m ? m[8] : -1);\n\t\tgoto done;\n\t}\n\tret = 0;\n\ndone:\n\tkfree(p);\n\treturn ret;\n}\n\nstatic int quicktest2(unsigned long arg)\n{\n\tstatic DECLARE_COMPLETION(cmp);\n\tunsigned long han;\n\tint blade_id = 0;\n\tint numcb = 4;\n\tint ret = 0;\n\tunsigned long *buf;\n\tvoid *cb0, *cb;\n\tstruct gru_control_block_status *gen;\n\tint i, k, istatus, bytes;\n\n\tbytes = numcb * 4 * 8;\n\tbuf = kmalloc(bytes, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tret = -EBUSY;\n\than = gru_reserve_async_resources(blade_id, numcb, 0, &cmp);\n\tif (!han)\n\t\tgoto done;\n\n\tgru_lock_async_resource(han, &cb0, NULL);\n\tmemset(buf, 0xee, bytes);\n\tfor (i = 0; i < numcb; i++)\n\t\tgru_vset(cb0 + i * GRU_HANDLE_STRIDE, uv_gpa(&buf[i * 4]), 0,\n\t\t\t\tXTYPE_DW, 4, 1, IMA_INTERRUPT);\n\n\tret = 0;\n\tk = numcb;\n\tdo {\n\t\tgru_wait_async_cbr(han);\n\t\tfor (i = 0; i < numcb; i++) {\n\t\t\tcb = cb0 + i * GRU_HANDLE_STRIDE;\n\t\t\tistatus = gru_check_status(cb);\n\t\t\tif (istatus != CBS_ACTIVE && istatus != CBS_CALL_OS)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (i == numcb)\n\t\t\tcontinue;\n\t\tif (istatus != CBS_IDLE) {\n\t\t\tprintk(KERN_DEBUG \"GRU:%d quicktest2: cb %d, exception\\n\", smp_processor_id(), i);\n\t\t\tret = -EFAULT;\n\t\t} else if (buf[4 * i] || buf[4 * i + 1] || buf[4 * i + 2] ||\n\t\t\t\tbuf[4 * i + 3]) {\n\t\t\tprintk(KERN_DEBUG \"GRU:%d quicktest2:cb %d,  buf 0x%lx, 0x%lx, 0x%lx, 0x%lx\\n\",\n\t\t\t       smp_processor_id(), i, buf[4 * i], buf[4 * i + 1], buf[4 * i + 2], buf[4 * i + 3]);\n\t\t\tret = -EIO;\n\t\t}\n\t\tk--;\n\t\tgen = cb;\n\t\tgen->istatus = CBS_CALL_OS;  \n\t} while (k);\n\tBUG_ON(cmp.done);\n\n\tgru_unlock_async_resource(han);\n\tgru_release_async_resources(han);\ndone:\n\tkfree(buf);\n\treturn ret;\n}\n\n#define BUFSIZE 200\nstatic int quicktest3(unsigned long arg)\n{\n\tchar buf1[BUFSIZE], buf2[BUFSIZE];\n\tint ret = 0;\n\n\tmemset(buf2, 0, sizeof(buf2));\n\tmemset(buf1, get_cycles() & 255, sizeof(buf1));\n\tgru_copy_gpa(uv_gpa(buf2), uv_gpa(buf1), BUFSIZE);\n\tif (memcmp(buf1, buf2, BUFSIZE)) {\n\t\tprintk(KERN_DEBUG \"GRU:%d quicktest3 error\\n\", smp_processor_id());\n\t\tret = -EIO;\n\t}\n\treturn ret;\n}\n\n \nint gru_ktest(unsigned long arg)\n{\n\tint ret = -EINVAL;\n\n\tswitch (arg & 0xff) {\n\tcase 0:\n\t\tret = quicktest0(arg);\n\t\tbreak;\n\tcase 1:\n\t\tret = quicktest1(arg);\n\t\tbreak;\n\tcase 2:\n\t\tret = quicktest2(arg);\n\t\tbreak;\n\tcase 3:\n\t\tret = quicktest3(arg);\n\t\tbreak;\n\tcase 99:\n\t\tret = gru_free_kernel_contexts();\n\t\tbreak;\n\t}\n\treturn ret;\n\n}\n\nint gru_kservices_init(void)\n{\n\treturn 0;\n}\n\nvoid gru_kservices_exit(void)\n{\n\tif (gru_free_kernel_contexts())\n\t\tBUG();\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}