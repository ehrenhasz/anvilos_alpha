{
  "module_name": "grutables.h",
  "hash_id": "f77fd555532aab941deab6a4b8e3a922d8a09ab88a8ca09808288042c58a1074",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-gru/grutables.h",
  "human_readable_source": " \n \n\n#ifndef __GRUTABLES_H__\n#define __GRUTABLES_H__\n\n \n\n#include <linux/refcount.h>\n#include <linux/rmap.h>\n#include <linux/interrupt.h>\n#include <linux/mutex.h>\n#include <linux/wait.h>\n#include <linux/mmu_notifier.h>\n#include <linux/mm_types.h>\n#include \"gru.h\"\n#include \"grulib.h\"\n#include \"gruhandles.h\"\n\nextern struct gru_stats_s gru_stats;\nextern struct gru_blade_state *gru_base[];\nextern unsigned long gru_start_paddr, gru_end_paddr;\nextern void *gru_start_vaddr;\nextern unsigned int gru_max_gids;\n\n#define GRU_MAX_BLADES\t\tMAX_NUMNODES\n#define GRU_MAX_GRUS\t\t(GRU_MAX_BLADES * GRU_CHIPLETS_PER_BLADE)\n\n#define GRU_DRIVER_ID_STR\t\"SGI GRU Device Driver\"\n#define GRU_DRIVER_VERSION_STR\t\"0.85\"\n\n \nstruct gru_stats_s {\n\tatomic_long_t vdata_alloc;\n\tatomic_long_t vdata_free;\n\tatomic_long_t gts_alloc;\n\tatomic_long_t gts_free;\n\tatomic_long_t gms_alloc;\n\tatomic_long_t gms_free;\n\tatomic_long_t gts_double_allocate;\n\tatomic_long_t assign_context;\n\tatomic_long_t assign_context_failed;\n\tatomic_long_t free_context;\n\tatomic_long_t load_user_context;\n\tatomic_long_t load_kernel_context;\n\tatomic_long_t lock_kernel_context;\n\tatomic_long_t unlock_kernel_context;\n\tatomic_long_t steal_user_context;\n\tatomic_long_t steal_kernel_context;\n\tatomic_long_t steal_context_failed;\n\tatomic_long_t nopfn;\n\tatomic_long_t asid_new;\n\tatomic_long_t asid_next;\n\tatomic_long_t asid_wrap;\n\tatomic_long_t asid_reuse;\n\tatomic_long_t intr;\n\tatomic_long_t intr_cbr;\n\tatomic_long_t intr_tfh;\n\tatomic_long_t intr_spurious;\n\tatomic_long_t intr_mm_lock_failed;\n\tatomic_long_t call_os;\n\tatomic_long_t call_os_wait_queue;\n\tatomic_long_t user_flush_tlb;\n\tatomic_long_t user_unload_context;\n\tatomic_long_t user_exception;\n\tatomic_long_t set_context_option;\n\tatomic_long_t check_context_retarget_intr;\n\tatomic_long_t check_context_unload;\n\tatomic_long_t tlb_dropin;\n\tatomic_long_t tlb_preload_page;\n\tatomic_long_t tlb_dropin_fail_no_asid;\n\tatomic_long_t tlb_dropin_fail_upm;\n\tatomic_long_t tlb_dropin_fail_invalid;\n\tatomic_long_t tlb_dropin_fail_range_active;\n\tatomic_long_t tlb_dropin_fail_idle;\n\tatomic_long_t tlb_dropin_fail_fmm;\n\tatomic_long_t tlb_dropin_fail_no_exception;\n\tatomic_long_t tfh_stale_on_fault;\n\tatomic_long_t mmu_invalidate_range;\n\tatomic_long_t mmu_invalidate_page;\n\tatomic_long_t flush_tlb;\n\tatomic_long_t flush_tlb_gru;\n\tatomic_long_t flush_tlb_gru_tgh;\n\tatomic_long_t flush_tlb_gru_zero_asid;\n\n\tatomic_long_t copy_gpa;\n\tatomic_long_t read_gpa;\n\n\tatomic_long_t mesq_receive;\n\tatomic_long_t mesq_receive_none;\n\tatomic_long_t mesq_send;\n\tatomic_long_t mesq_send_failed;\n\tatomic_long_t mesq_noop;\n\tatomic_long_t mesq_send_unexpected_error;\n\tatomic_long_t mesq_send_lb_overflow;\n\tatomic_long_t mesq_send_qlimit_reached;\n\tatomic_long_t mesq_send_amo_nacked;\n\tatomic_long_t mesq_send_put_nacked;\n\tatomic_long_t mesq_page_overflow;\n\tatomic_long_t mesq_qf_locked;\n\tatomic_long_t mesq_qf_noop_not_full;\n\tatomic_long_t mesq_qf_switch_head_failed;\n\tatomic_long_t mesq_qf_unexpected_error;\n\tatomic_long_t mesq_noop_unexpected_error;\n\tatomic_long_t mesq_noop_lb_overflow;\n\tatomic_long_t mesq_noop_qlimit_reached;\n\tatomic_long_t mesq_noop_amo_nacked;\n\tatomic_long_t mesq_noop_put_nacked;\n\tatomic_long_t mesq_noop_page_overflow;\n\n};\n\nenum mcs_op {cchop_allocate, cchop_start, cchop_interrupt, cchop_interrupt_sync,\n\tcchop_deallocate, tfhop_write_only, tfhop_write_restart,\n\ttghop_invalidate, mcsop_last};\n\nstruct mcs_op_statistic {\n\tatomic_long_t\tcount;\n\tatomic_long_t\ttotal;\n\tunsigned long\tmax;\n};\n\nextern struct mcs_op_statistic mcs_op_statistics[mcsop_last];\n\n#define OPT_DPRINT\t\t1\n#define OPT_STATS\t\t2\n\n\n#define IRQ_GRU\t\t\t110\t \n\n \n#define GRU_ASSIGN_DELAY\t((HZ * 20) / 1000)\n\n \n#define GRU_STEAL_DELAY\t\t((HZ * 200) / 1000)\n\n#define STAT(id)\tdo {\t\t\t\t\t\t\\\n\t\t\t\tif (gru_options & OPT_STATS)\t\t\\\n\t\t\t\t\tatomic_long_inc(&gru_stats.id);\t\\\n\t\t\t} while (0)\n\n#ifdef CONFIG_SGI_GRU_DEBUG\n#define gru_dbg(dev, fmt, x...)\t\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (gru_options & OPT_DPRINT)\t\t\t\t\\\n\t\t\tprintk(KERN_DEBUG \"GRU:%d %s: \" fmt, smp_processor_id(), __func__, x);\\\n\t} while (0)\n#else\n#define gru_dbg(x...)\n#endif\n\n \n#define MAX_ASID\t0xfffff0\n#define MIN_ASID\t8\n#define ASID_INC\t8\t \n\n \n#define VADDR_HI_BIT\t\t64\n#define GRUREGION(addr)\t\t((addr) >> (VADDR_HI_BIT - 3) & 3)\n#define GRUASID(asid, addr)\t((asid) + GRUREGION(addr))\n\n \n\nstruct gru_state;\n\n \nstruct gru_mm_tracker {\t\t\t\t \n\tunsigned int\t\tmt_asid_gen:24;\t \n\tunsigned int\t\tmt_asid:24;\t \n\tunsigned short\t\tmt_ctxbitmap:16; \n} __attribute__ ((packed));\n\nstruct gru_mm_struct {\n\tstruct mmu_notifier\tms_notifier;\n\tspinlock_t\t\tms_asid_lock;\t \n\tatomic_t\t\tms_range_active; \n\twait_queue_head_t\tms_wait_queue;\n\tDECLARE_BITMAP(ms_asidmap, GRU_MAX_GRUS);\n\tstruct gru_mm_tracker\tms_asids[GRU_MAX_GRUS];\n};\n\n \nstruct gru_vma_data {\n\tspinlock_t\t\tvd_lock;\t \n\tstruct list_head\tvd_head;\t \n\tlong\t\t\tvd_user_options; \n\tint\t\t\tvd_cbr_au_count;\n\tint\t\t\tvd_dsr_au_count;\n\tunsigned char\t\tvd_tlb_preload_count;\n};\n\n \nstruct gru_thread_state {\n\tstruct list_head\tts_next;\t \n\tstruct mutex\t\tts_ctxlock;\t \n\tstruct mm_struct\t*ts_mm;\t\t \n\tstruct vm_area_struct\t*ts_vma;\t \n\tstruct gru_state\t*ts_gru;\t \n\tstruct gru_mm_struct\t*ts_gms;\t \n\tunsigned char\t\tts_tlb_preload_count;  \n\tunsigned long\t\tts_cbr_map;\t \n\tunsigned long\t\tts_dsr_map;\t \n\tunsigned long\t\tts_steal_jiffies; \n\tlong\t\t\tts_user_options; \n\tpid_t\t\t\tts_tgid_owner;\t \n\tshort\t\t\tts_user_blade_id; \n\tsigned char\t\tts_user_chiplet_id; \n\tunsigned short\t\tts_sizeavail;\t \n\tint\t\t\tts_tsid;\t \n\tint\t\t\tts_tlb_int_select; \n\tint\t\t\tts_ctxnum;\t \n\trefcount_t\t\tts_refcnt;\t \n\tunsigned char\t\tts_dsr_au_count; \n\tunsigned char\t\tts_cbr_au_count; \n\tsigned char\t\tts_cch_req_slice; \n\tsigned char\t\tts_blade;\t \n\tsigned char\t\tts_force_cch_reload;\n\tsigned char\t\tts_cbr_idx[GRU_CBR_AU]; \n\tint\t\t\tts_data_valid;\t \n\tstruct gru_gseg_statistics ustats;\t \n\tunsigned long\t\tts_gdata[];\t \n};\n\n \n#define TSID(a, v)\t\t(((a) - (v)->vm_start) / GRU_GSEG_PAGESIZE)\n#define UGRUADDR(gts)\t\t((gts)->ts_vma->vm_start +\t\t\\\n\t\t\t\t\t(gts)->ts_tsid * GRU_GSEG_PAGESIZE)\n\n#define NULLCTX\t\t\t(-1)\t \n\n \n\n \nstruct gru_state {\n\tstruct gru_blade_state\t*gs_blade;\t\t \n\tunsigned long\t\tgs_gru_base_paddr;\t \n\tvoid\t\t\t*gs_gru_base_vaddr;\t \n\tunsigned short\t\tgs_gid;\t\t\t \n\tunsigned short\t\tgs_blade_id;\t\t \n\tunsigned char\t\tgs_chiplet_id;\t\t \n\tunsigned char\t\tgs_tgh_local_shift;\t \n\tunsigned char\t\tgs_tgh_first_remote;\t \n\tspinlock_t\t\tgs_asid_lock;\t\t \n\tspinlock_t\t\tgs_lock;\t\t \n\n\t \n\tunsigned int\t\tgs_asid;\t\t \n\tunsigned int\t\tgs_asid_limit;\t\t \n\tunsigned int\t\tgs_asid_gen;\t\t \n\n\t \n\tunsigned long\t\tgs_context_map;\t\t \n\tunsigned long\t\tgs_cbr_map;\t\t \n\tunsigned long\t\tgs_dsr_map;\t\t \n\tunsigned int\t\tgs_reserved_cbrs;\t \n\tunsigned int\t\tgs_reserved_dsr_bytes;\t \n\tunsigned short\t\tgs_active_contexts;\t \n\tstruct gru_thread_state\t*gs_gts[GRU_NUM_CCH];\t \n\tint\t\t\tgs_irq[GRU_NUM_TFM];\t \n};\n\n \nstruct gru_blade_state {\n\tvoid\t\t\t*kernel_cb;\t\t \n\tvoid\t\t\t*kernel_dsr;\t\t \n\tstruct rw_semaphore\tbs_kgts_sema;\t\t \n\tstruct gru_thread_state *bs_kgts;\t\t \n\n\t \n\tint\t\t\tbs_async_dsr_bytes;\t \n\tint\t\t\tbs_async_cbrs;\t\t \n\tstruct completion\t*bs_async_wq;\n\n\t \n\tspinlock_t\t\tbs_lock;\t\t \n\tint\t\t\tbs_lru_ctxnum;\t\t \n\tstruct gru_state\t*bs_lru_gru;\t\t \n\n\tstruct gru_state\tbs_grus[GRU_CHIPLETS_PER_BLADE];\n};\n\n \n#define get_tfm_for_cpu(g, c)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_map *)get_tfm((g)->gs_gru_base_vaddr, (c)))\n#define get_tfh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_fault_handle *)get_tfh((g)->gs_gru_base_vaddr, (i)))\n#define get_tgh_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_tlb_global_handle *)get_tgh((g)->gs_gru_base_vaddr, (i)))\n#define get_cbe_by_index(g, i)\t\t\t\t\t\t\\\n\t((struct gru_control_block_extended *)get_cbe((g)->gs_gru_base_vaddr,\\\n\t\t\t(i)))\n\n \n\n \n#define get_gru(b, c)\t\t(&gru_base[b]->bs_grus[c])\n\n \n#define DSR_BYTES(dsr)\t\t((dsr) * GRU_DSR_AU_BYTES)\n#define CBR_BYTES(cbr)\t\t((cbr) * GRU_HANDLE_BYTES * GRU_CBR_AU_SIZE * 2)\n\n \n#define thread_cbr_number(gts, n) ((gts)->ts_cbr_idx[(n) / GRU_CBR_AU_SIZE] \\\n\t\t\t\t  * GRU_CBR_AU_SIZE + (n) % GRU_CBR_AU_SIZE)\n\n \n#define GID_TO_GRU(gid)\t\t\t\t\t\t\t\\\n\t(gru_base[(gid) / GRU_CHIPLETS_PER_BLADE] ?\t\t\t\\\n\t\t(&gru_base[(gid) / GRU_CHIPLETS_PER_BLADE]->\t\t\\\n\t\t\tbs_grus[(gid) % GRU_CHIPLETS_PER_BLADE]) :\t\\\n\t NULL)\n\n \n#define for_each_gru_in_bitmap(gid, map)\t\t\t\t\\\n\tfor_each_set_bit((gid), (map), GRU_MAX_GRUS)\n\n \n#define for_each_gru_on_blade(gru, nid, i)\t\t\t\t\\\n\tfor ((gru) = gru_base[nid]->bs_grus, (i) = 0;\t\t\t\\\n\t\t\t(i) < GRU_CHIPLETS_PER_BLADE;\t\t\t\\\n\t\t\t(i)++, (gru)++)\n\n \n#define foreach_gid(gid)\t\t\t\t\t\t\\\n\tfor ((gid) = 0; (gid) < gru_max_gids; (gid)++)\n\n \n#define for_each_gts_on_gru(gts, gru, ctxnum)\t\t\t\t\\\n\tfor ((ctxnum) = 0; (ctxnum) < GRU_NUM_CCH; (ctxnum)++)\t\t\\\n\t\tif (((gts) = (gru)->gs_gts[ctxnum]))\n\n \n#define for_each_cbr_in_tfm(i, map)\t\t\t\t\t\\\n\tfor_each_set_bit((i), (map), GRU_NUM_CBE)\n\n \n#define for_each_cbr_in_allocation_map(i, map, k)\t\t\t\\\n\tfor_each_set_bit((k), (map), GRU_CBR_AU)\t\t\t\\\n\t\tfor ((i) = (k)*GRU_CBR_AU_SIZE;\t\t\t\t\\\n\t\t\t\t(i) < ((k) + 1) * GRU_CBR_AU_SIZE; (i)++)\n\n#define gseg_physical_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_paddr + ctxnum * GRU_GSEG_STRIDE)\n#define gseg_virtual_address(gru, ctxnum)\t\t\t\t\\\n\t\t((gru)->gs_gru_base_vaddr + ctxnum * GRU_GSEG_STRIDE)\n\n \n\n \n\n \nstatic inline int __trylock_handle(void *h)\n{\n\treturn !test_and_set_bit(1, h);\n}\n\nstatic inline void __lock_handle(void *h)\n{\n\twhile (test_and_set_bit(1, h))\n\t\tcpu_relax();\n}\n\nstatic inline void __unlock_handle(void *h)\n{\n\tclear_bit(1, h);\n}\n\nstatic inline int trylock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\treturn __trylock_handle(cch);\n}\n\nstatic inline void lock_cch_handle(struct gru_context_configuration_handle *cch)\n{\n\t__lock_handle(cch);\n}\n\nstatic inline void unlock_cch_handle(struct gru_context_configuration_handle\n\t\t\t\t     *cch)\n{\n\t__unlock_handle(cch);\n}\n\nstatic inline void lock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__lock_handle(tgh);\n}\n\nstatic inline void unlock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\t__unlock_handle(tgh);\n}\n\nstatic inline int is_kernel_context(struct gru_thread_state *gts)\n{\n\treturn !gts->ts_mm;\n}\n\n \n#define UV_MAX_INT_CORES\t\t8\n#define uv_cpu_socket_number(p)\t\t((cpu_physical_id(p) >> 5) & 1)\n#define uv_cpu_ht_number(p)\t\t(cpu_physical_id(p) & 1)\n#define uv_cpu_core_number(p)\t\t(((cpu_physical_id(p) >> 2) & 4) |\t\\\n\t\t\t\t\t((cpu_physical_id(p) >> 1) & 3))\n \nstruct gru_unload_context_req;\n\nextern const struct vm_operations_struct gru_vm_ops;\nextern struct device *grudev;\n\nextern struct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma,\n\t\t\t\tint tsid);\nextern struct gru_thread_state *gru_find_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct\n\t\t\t\t*vma, int tsid);\nextern struct gru_state *gru_assign_gru_context(struct gru_thread_state *gts);\nextern void gru_load_context(struct gru_thread_state *gts);\nextern void gru_steal_context(struct gru_thread_state *gts);\nextern void gru_unload_context(struct gru_thread_state *gts, int savestate);\nextern int gru_update_cch(struct gru_thread_state *gts);\nextern void gts_drop(struct gru_thread_state *gts);\nextern void gru_tgh_flush_init(struct gru_state *gru);\nextern int gru_kservices_init(void);\nextern void gru_kservices_exit(void);\nextern irqreturn_t gru0_intr(int irq, void *dev_id);\nextern irqreturn_t gru1_intr(int irq, void *dev_id);\nextern irqreturn_t gru_intr_mblade(int irq, void *dev_id);\nextern int gru_dump_chiplet_request(unsigned long arg);\nextern long gru_get_gseg_statistics(unsigned long arg);\nextern int gru_handle_user_call_os(unsigned long address);\nextern int gru_user_flush_tlb(unsigned long arg);\nextern int gru_user_unload_context(unsigned long arg);\nextern int gru_get_exception_detail(unsigned long arg);\nextern int gru_set_context_option(unsigned long address);\nextern int gru_check_context_placement(struct gru_thread_state *gts);\nextern int gru_cpu_fault_map_id(void);\nextern struct vm_area_struct *gru_find_vma(unsigned long vaddr);\nextern void gru_flush_all_tlb(struct gru_state *gru);\nextern int gru_proc_init(void);\nextern void gru_proc_exit(void);\n\nextern struct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid);\nextern unsigned long gru_reserve_cb_resources(struct gru_state *gru,\n\t\tint cbr_au_count, signed char *cbmap);\nextern unsigned long gru_reserve_ds_resources(struct gru_state *gru,\n\t\tint dsr_au_count, signed char *dsmap);\nextern vm_fault_t gru_fault(struct vm_fault *vmf);\nextern struct gru_mm_struct *gru_register_mmu_notifier(void);\nextern void gru_drop_mmu_notifier(struct gru_mm_struct *gms);\n\nextern int gru_ktest(unsigned long arg);\nextern void gru_flush_tlb_range(struct gru_mm_struct *gms, unsigned long start,\n\t\t\t\t\tunsigned long len);\n\nextern unsigned long gru_options;\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}