{
  "module_name": "grufault.c",
  "hash_id": "00892bf27810796f100d43b2167b8988bc34df792a3a0ae99ead6f9ddc9e6eb3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-gru/grufault.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/hugetlb.h>\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/uaccess.h>\n#include <linux/security.h>\n#include <linux/sync_core.h>\n#include <linux/prefetch.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"grulib.h\"\n#include \"gru_instructions.h\"\n#include <asm/uv/uv_hub.h>\n\n \n#define VTOP_SUCCESS               0\n#define VTOP_INVALID               -1\n#define VTOP_RETRY                 -2\n\n\n \nstatic inline int is_gru_paddr(unsigned long paddr)\n{\n\treturn paddr >= gru_start_paddr && paddr < gru_end_paddr;\n}\n\n \nstruct vm_area_struct *gru_find_vma(unsigned long vaddr)\n{\n\tstruct vm_area_struct *vma;\n\n\tvma = vma_lookup(current->mm, vaddr);\n\tif (vma && vma->vm_ops == &gru_vm_ops)\n\t\treturn vma;\n\treturn NULL;\n}\n\n \n\nstatic struct gru_thread_state *gru_find_lock_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = NULL;\n\n\tmmap_read_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (vma)\n\t\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (gts)\n\t\tmutex_lock(&gts->ts_ctxlock);\n\telse\n\t\tmmap_read_unlock(mm);\n\treturn gts;\n}\n\nstatic struct gru_thread_state *gru_alloc_locked_gts(unsigned long vaddr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tstruct gru_thread_state *gts = ERR_PTR(-EINVAL);\n\n\tmmap_write_lock(mm);\n\tvma = gru_find_vma(vaddr);\n\tif (!vma)\n\t\tgoto err;\n\n\tgts = gru_alloc_thread_state(vma, TSID(vaddr, vma));\n\tif (IS_ERR(gts))\n\t\tgoto err;\n\tmutex_lock(&gts->ts_ctxlock);\n\tmmap_write_downgrade(mm);\n\treturn gts;\n\nerr:\n\tmmap_write_unlock(mm);\n\treturn gts;\n}\n\n \nstatic void gru_unlock_gts(struct gru_thread_state *gts)\n{\n\tmutex_unlock(&gts->ts_ctxlock);\n\tmmap_read_unlock(current->mm);\n}\n\n \nstatic void gru_cb_set_istatus_active(struct gru_instruction_bits *cbk)\n{\n\tif (cbk) {\n\t\tcbk->istatus = CBS_ACTIVE;\n\t}\n}\n\n \nstatic void get_clear_fault_map(struct gru_state *gru,\n\t\t\t\tstruct gru_tlb_fault_map *imap,\n\t\t\t\tstruct gru_tlb_fault_map *dmap)\n{\n\tunsigned long i, k;\n\tstruct gru_tlb_fault_map *tfm;\n\n\ttfm = get_tfm_for_cpu(gru, gru_cpu_fault_map_id());\n\tprefetchw(tfm);\t\t \n\tfor (i = 0; i < BITS_TO_LONGS(GRU_NUM_CBE); i++) {\n\t\tk = tfm->fault_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->fault_bits[i], 0UL);\n\t\timap->fault_bits[i] = k;\n\t\tk = tfm->done_bits[i];\n\t\tif (k)\n\t\t\tk = xchg(&tfm->done_bits[i], 0UL);\n\t\tdmap->fault_bits[i] = k;\n\t}\n\n\t \n\tgru_flush_cache(tfm);\n}\n\n \nstatic int non_atomic_pte_lookup(struct vm_area_struct *vma,\n\t\t\t\t unsigned long vaddr, int write,\n\t\t\t\t unsigned long *paddr, int *pageshift)\n{\n\tstruct page *page;\n\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\tif (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &page) <= 0)\n\t\treturn -EFAULT;\n\t*paddr = page_to_phys(page);\n\tput_page(page);\n\treturn 0;\n}\n\n \nstatic int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,\n\tint write, unsigned long *paddr, int *pageshift)\n{\n\tpgd_t *pgdp;\n\tp4d_t *p4dp;\n\tpud_t *pudp;\n\tpmd_t *pmdp;\n\tpte_t pte;\n\n\tpgdp = pgd_offset(vma->vm_mm, vaddr);\n\tif (unlikely(pgd_none(*pgdp)))\n\t\tgoto err;\n\n\tp4dp = p4d_offset(pgdp, vaddr);\n\tif (unlikely(p4d_none(*p4dp)))\n\t\tgoto err;\n\n\tpudp = pud_offset(p4dp, vaddr);\n\tif (unlikely(pud_none(*pudp)))\n\t\tgoto err;\n\n\tpmdp = pmd_offset(pudp, vaddr);\n\tif (unlikely(pmd_none(*pmdp)))\n\t\tgoto err;\n#ifdef CONFIG_X86_64\n\tif (unlikely(pmd_large(*pmdp)))\n\t\tpte = ptep_get((pte_t *)pmdp);\n\telse\n#endif\n\t\tpte = *pte_offset_kernel(pmdp, vaddr);\n\n\tif (unlikely(!pte_present(pte) ||\n\t\t     (write && (!pte_write(pte) || !pte_dirty(pte)))))\n\t\treturn 1;\n\n\t*paddr = pte_pfn(pte) << PAGE_SHIFT;\n#ifdef CONFIG_HUGETLB_PAGE\n\t*pageshift = is_vm_hugetlb_page(vma) ? HPAGE_SHIFT : PAGE_SHIFT;\n#else\n\t*pageshift = PAGE_SHIFT;\n#endif\n\treturn 0;\n\nerr:\n\treturn 1;\n}\n\nstatic int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,\n\t\t    int write, int atomic, unsigned long *gpa, int *pageshift)\n{\n\tstruct mm_struct *mm = gts->ts_mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long paddr;\n\tint ret, ps;\n\n\tvma = find_vma(mm, vaddr);\n\tif (!vma)\n\t\tgoto inval;\n\n\t \n\trmb();\t \n\tret = atomic_pte_lookup(vma, vaddr, write, &paddr, &ps);\n\tif (ret) {\n\t\tif (atomic)\n\t\t\tgoto upm;\n\t\tif (non_atomic_pte_lookup(vma, vaddr, write, &paddr, &ps))\n\t\t\tgoto inval;\n\t}\n\tif (is_gru_paddr(paddr))\n\t\tgoto inval;\n\tpaddr = paddr & ~((1UL << ps) - 1);\n\t*gpa = uv_soc_phys_ram_to_gpa(paddr);\n\t*pageshift = ps;\n\treturn VTOP_SUCCESS;\n\ninval:\n\treturn VTOP_INVALID;\nupm:\n\treturn VTOP_RETRY;\n}\n\n\n \nstatic void gru_flush_cache_cbe(struct gru_control_block_extended *cbe)\n{\n\tif (unlikely(cbe)) {\n\t\tcbe->cbrexecstatus = 0;          \n\t\tgru_flush_cache(cbe);\n\t}\n}\n\n \nstatic void gru_preload_tlb(struct gru_state *gru,\n\t\t\tstruct gru_thread_state *gts, int atomic,\n\t\t\tunsigned long fault_vaddr, int asid, int write,\n\t\t\tunsigned char tlb_preload_count,\n\t\t\tstruct gru_tlb_fault_handle *tfh,\n\t\t\tstruct gru_control_block_extended *cbe)\n{\n\tunsigned long vaddr = 0, gpa;\n\tint ret, pageshift;\n\n\tif (cbe->opccpy != OP_BCOPY)\n\t\treturn;\n\n\tif (fault_vaddr == cbe->cbe_baddr0)\n\t\tvaddr = fault_vaddr + GRU_CACHE_LINE_BYTES * cbe->cbe_src_cl - 1;\n\telse if (fault_vaddr == cbe->cbe_baddr1)\n\t\tvaddr = fault_vaddr + (1 << cbe->xtypecpy) * cbe->cbe_nelemcur - 1;\n\n\tfault_vaddr &= PAGE_MASK;\n\tvaddr &= PAGE_MASK;\n\tvaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);\n\n\twhile (vaddr > fault_vaddr) {\n\t\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\t\tif (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t\t\t  GRU_PAGESIZE(pageshift)))\n\t\t\treturn;\n\t\tgru_dbg(grudev,\n\t\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, rw %d, ps %d, gpa 0x%lx\\n\",\n\t\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh,\n\t\t\tvaddr, asid, write, pageshift, gpa);\n\t\tvaddr -= PAGE_SIZE;\n\t\tSTAT(tlb_preload_page);\n\t}\n}\n\n \nstatic int gru_try_dropin(struct gru_state *gru,\n\t\t\t  struct gru_thread_state *gts,\n\t\t\t  struct gru_tlb_fault_handle *tfh,\n\t\t\t  struct gru_instruction_bits *cbk)\n{\n\tstruct gru_control_block_extended *cbe = NULL;\n\tunsigned char tlb_preload_count = gts->ts_tlb_preload_count;\n\tint pageshift = 0, asid, write, ret, atomic = !cbk, indexway;\n\tunsigned long gpa = 0, vaddr = 0;\n\n\t \n\n\t \n\tif (unlikely(tlb_preload_count)) {\n\t\tcbe = gru_tfh_to_cbe(tfh);\n\t\tprefetchw(cbe);\n\t}\n\n\t \n\tif (tfh->status != TFHSTATUS_EXCEPTION) {\n\t\tgru_flush_cache(tfh);\n\t\tsync_core();\n\t\tif (tfh->status != TFHSTATUS_EXCEPTION)\n\t\t\tgoto failnoexception;\n\t\tSTAT(tfh_stale_on_fault);\n\t}\n\tif (tfh->state == TFHSTATE_IDLE)\n\t\tgoto failidle;\n\tif (tfh->state == TFHSTATE_MISS_FMM && cbk)\n\t\tgoto failfmm;\n\n\twrite = (tfh->cause & TFHCAUSE_TLB_MOD) != 0;\n\tvaddr = tfh->missvaddr;\n\tasid = tfh->missasid;\n\tindexway = tfh->indexway;\n\tif (asid == 0)\n\t\tgoto failnoasid;\n\n\trmb();\t \n\n\t \n\tif (atomic_read(&gts->ts_gms->ms_range_active))\n\t\tgoto failactive;\n\n\tret = gru_vtop(gts, vaddr, write, atomic, &gpa, &pageshift);\n\tif (ret == VTOP_INVALID)\n\t\tgoto failinval;\n\tif (ret == VTOP_RETRY)\n\t\tgoto failupm;\n\n\tif (!(gts->ts_sizeavail & GRU_SIZEAVAIL(pageshift))) {\n\t\tgts->ts_sizeavail |= GRU_SIZEAVAIL(pageshift);\n\t\tif (atomic || !gru_update_cch(gts)) {\n\t\t\tgts->ts_force_cch_reload = 1;\n\t\t\tgoto failupm;\n\t\t}\n\t}\n\n\tif (unlikely(cbe) && pageshift == PAGE_SHIFT) {\n\t\tgru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);\n\t\tgru_flush_cache_cbe(cbe);\n\t}\n\n\tgru_cb_set_istatus_active(cbk);\n\tgts->ustats.tlbdropin++;\n\ttfh_write_restart(tfh, gpa, GAA_RAM, vaddr, asid, write,\n\t\t\t  GRU_PAGESIZE(pageshift));\n\tgru_dbg(grudev,\n\t\t\"%s: gid %d, gts 0x%p, tfh 0x%p, vaddr 0x%lx, asid 0x%x, indexway 0x%x,\"\n\t\t\" rw %d, ps %d, gpa 0x%lx\\n\",\n\t\tatomic ? \"atomic\" : \"non-atomic\", gru->gs_gid, gts, tfh, vaddr, asid,\n\t\tindexway, write, pageshift, gpa);\n\tSTAT(tlb_dropin);\n\treturn 0;\n\nfailnoasid:\n\t \n\tSTAT(tlb_dropin_fail_no_asid);\n\tgru_dbg(grudev, \"FAILED no_asid tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\treturn -EAGAIN;\n\nfailupm:\n\t \n\ttfh_user_polling_mode(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_upm);\n\tgru_dbg(grudev, \"FAILED upm tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn 1;\n\nfailfmm:\n\t \n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_fmm);\n\tgru_dbg(grudev, \"FAILED fmm tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailnoexception:\n\t \n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_no_exception);\n\tgru_dbg(grudev, \"FAILED non-exception tfh: 0x%p, status %d, state %d\\n\",\n\t\ttfh, tfh->status, tfh->state);\n\treturn 0;\n\nfailidle:\n\t \n\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tif (cbk)\n\t\tgru_flush_cache(cbk);\n\tSTAT(tlb_dropin_fail_idle);\n\tgru_dbg(grudev, \"FAILED idle tfh: 0x%p, state %d\\n\", tfh, tfh->state);\n\treturn 0;\n\nfailinval:\n\t \n\ttfh_exception(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_invalid);\n\tgru_dbg(grudev, \"FAILED inval tfh: 0x%p, vaddr 0x%lx\\n\", tfh, vaddr);\n\treturn -EFAULT;\n\nfailactive:\n\t \n\tif (!cbk)\n\t\ttfh_user_polling_mode(tfh);\n\telse\n\t\tgru_flush_cache(tfh);\n\tgru_flush_cache_cbe(cbe);\n\tSTAT(tlb_dropin_fail_range_active);\n\tgru_dbg(grudev, \"FAILED range active: tfh 0x%p, vaddr 0x%lx\\n\",\n\t\ttfh, vaddr);\n\treturn 1;\n}\n\n \nstatic irqreturn_t gru_intr(int chiplet, int blade)\n{\n\tstruct gru_state *gru;\n\tstruct gru_tlb_fault_map imap, dmap;\n\tstruct gru_thread_state *gts;\n\tstruct gru_tlb_fault_handle *tfh = NULL;\n\tstruct completion *cmp;\n\tint cbrnum, ctxnum;\n\n\tSTAT(intr);\n\n\tgru = &gru_base[blade]->bs_grus[chiplet];\n\tif (!gru) {\n\t\tdev_err(grudev, \"GRU: invalid interrupt: cpu %d, chiplet %d\\n\",\n\t\t\traw_smp_processor_id(), chiplet);\n\t\treturn IRQ_NONE;\n\t}\n\tget_clear_fault_map(gru, &imap, &dmap);\n\tgru_dbg(grudev,\n\t\t\"cpu %d, chiplet %d, gid %d, imap %016lx %016lx, dmap %016lx %016lx\\n\",\n\t\tsmp_processor_id(), chiplet, gru->gs_gid,\n\t\timap.fault_bits[0], imap.fault_bits[1],\n\t\tdmap.fault_bits[0], dmap.fault_bits[1]);\n\n\tfor_each_cbr_in_tfm(cbrnum, dmap.fault_bits) {\n\t\tSTAT(intr_cbr);\n\t\tcmp = gru->gs_blade->bs_async_wq;\n\t\tif (cmp)\n\t\t\tcomplete(cmp);\n\t\tgru_dbg(grudev, \"gid %d, cbr_done %d, done %d\\n\",\n\t\t\tgru->gs_gid, cbrnum, cmp ? cmp->done : -1);\n\t}\n\n\tfor_each_cbr_in_tfm(cbrnum, imap.fault_bits) {\n\t\tSTAT(intr_tfh);\n\t\ttfh = get_tfh_by_index(gru, cbrnum);\n\t\tprefetchw(tfh);\t \n\n\t\t \n\t\tctxnum = tfh->ctxnum;\n\t\tgts = gru->gs_gts[ctxnum];\n\n\t\t \n\t\tif (!gts) {\n\t\t\tSTAT(intr_spurious);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tgts->ustats.fmm_tlbmiss++;\n\t\tif (!gts->ts_force_cch_reload &&\n\t\t\t\t\tmmap_read_trylock(gts->ts_mm)) {\n\t\t\tgru_try_dropin(gru, gts, tfh, NULL);\n\t\t\tmmap_read_unlock(gts->ts_mm);\n\t\t} else {\n\t\t\ttfh_user_polling_mode(tfh);\n\t\t\tSTAT(intr_mm_lock_failed);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t gru0_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(0, uv_numa_blade_id());\n}\n\nirqreturn_t gru1_intr(int irq, void *dev_id)\n{\n\treturn gru_intr(1, uv_numa_blade_id());\n}\n\nirqreturn_t gru_intr_mblade(int irq, void *dev_id)\n{\n\tint blade;\n\n\tfor_each_possible_blade(blade) {\n\t\tif (uv_blade_nr_possible_cpus(blade))\n\t\t\tcontinue;\n\t\tgru_intr(0, blade);\n\t\tgru_intr(1, blade);\n\t}\n\treturn IRQ_HANDLED;\n}\n\n\nstatic int gru_user_dropin(struct gru_thread_state *gts,\n\t\t\t   struct gru_tlb_fault_handle *tfh,\n\t\t\t   void *cb)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tint ret;\n\n\tgts->ustats.upm_tlbmiss++;\n\twhile (1) {\n\t\twait_event(gms->ms_wait_queue,\n\t\t\t   atomic_read(&gms->ms_range_active) == 0);\n\t\tprefetchw(tfh);\t \n\t\tret = gru_try_dropin(gts->ts_gru, gts, tfh, cb);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\t\tSTAT(call_os_wait_queue);\n\t}\n}\n\n \nint gru_handle_user_call_os(unsigned long cb)\n{\n\tstruct gru_tlb_fault_handle *tfh;\n\tstruct gru_thread_state *gts;\n\tvoid *cbk;\n\tint ucbnum, cbrnum, ret = -EINVAL;\n\n\tSTAT(call_os);\n\n\t \n\tucbnum = get_cb_number((void *)cb);\n\tif ((cb & (GRU_HANDLE_STRIDE - 1)) || ucbnum >= GRU_NUM_CB)\n\t\treturn -EINVAL;\n\nagain:\n\tgts = gru_find_lock_gts(cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE)\n\t\tgoto exit;\n\n\tif (gru_check_context_placement(gts)) {\n\t\tgru_unlock_gts(gts);\n\t\tgru_unload_context(gts, 1);\n\t\tgoto again;\n\t}\n\n\t \n\tif (gts->ts_gru && gts->ts_force_cch_reload) {\n\t\tgts->ts_force_cch_reload = 0;\n\t\tgru_update_cch(gts);\n\t}\n\n\tret = -EAGAIN;\n\tcbrnum = thread_cbr_number(gts, ucbnum);\n\tif (gts->ts_gru) {\n\t\ttfh = get_tfh_by_index(gts->ts_gru, cbrnum);\n\t\tcbk = get_gseg_base_address_cb(gts->ts_gru->gs_gru_base_vaddr,\n\t\t\t\tgts->ts_ctxnum, ucbnum);\n\t\tret = gru_user_dropin(gts, tfh, cbk);\n\t}\nexit:\n\tgru_unlock_gts(gts);\n\treturn ret;\n}\n\n \nint gru_get_exception_detail(unsigned long arg)\n{\n\tstruct control_block_extended_exc_detail excdet;\n\tstruct gru_control_block_extended *cbe;\n\tstruct gru_thread_state *gts;\n\tint ucbnum, cbrnum, ret;\n\n\tSTAT(user_exception);\n\tif (copy_from_user(&excdet, (void __user *)arg, sizeof(excdet)))\n\t\treturn -EFAULT;\n\n\tgts = gru_find_lock_gts(excdet.cb);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgru_dbg(grudev, \"address 0x%lx, gid %d, gts 0x%p\\n\", excdet.cb, gts->ts_gru ? gts->ts_gru->gs_gid : -1, gts);\n\tucbnum = get_cb_number((void *)excdet.cb);\n\tif (ucbnum >= gts->ts_cbr_au_count * GRU_CBR_AU_SIZE) {\n\t\tret = -EINVAL;\n\t} else if (gts->ts_gru) {\n\t\tcbrnum = thread_cbr_number(gts, ucbnum);\n\t\tcbe = get_cbe_by_index(gts->ts_gru, cbrnum);\n\t\tgru_flush_cache(cbe);\t \n\t\tsync_core();\t\t \n\t\texcdet.opc = cbe->opccpy;\n\t\texcdet.exopc = cbe->exopccpy;\n\t\texcdet.ecause = cbe->ecause;\n\t\texcdet.exceptdet0 = cbe->idef1upd;\n\t\texcdet.exceptdet1 = cbe->idef3upd;\n\t\texcdet.cbrstate = cbe->cbrstate;\n\t\texcdet.cbrexecstatus = cbe->cbrexecstatus;\n\t\tgru_flush_cache_cbe(cbe);\n\t\tret = 0;\n\t} else {\n\t\tret = -EAGAIN;\n\t}\n\tgru_unlock_gts(gts);\n\n\tgru_dbg(grudev,\n\t\t\"cb 0x%lx, op %d, exopc %d, cbrstate %d, cbrexecstatus 0x%x, ecause 0x%x, \"\n\t\t\"exdet0 0x%lx, exdet1 0x%x\\n\",\n\t\texcdet.cb, excdet.opc, excdet.exopc, excdet.cbrstate, excdet.cbrexecstatus,\n\t\texcdet.ecause, excdet.exceptdet0, excdet.exceptdet1);\n\tif (!ret && copy_to_user((void __user *)arg, &excdet, sizeof(excdet)))\n\t\tret = -EFAULT;\n\treturn ret;\n}\n\n \nstatic int gru_unload_all_contexts(void)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_state *gru;\n\tint gid, ctxnum;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\tforeach_gid(gid) {\n\t\tgru = GID_TO_GRU(gid);\n\t\tspin_lock(&gru->gs_lock);\n\t\tfor (ctxnum = 0; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\tgts = gru->gs_gts[ctxnum];\n\t\t\tif (gts && mutex_trylock(&gts->ts_ctxlock)) {\n\t\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\t\tspin_lock(&gru->gs_lock);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&gru->gs_lock);\n\t}\n\treturn 0;\n}\n\nint gru_user_unload_context(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_unload_context_req req;\n\n\tSTAT(user_unload_context);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx\\n\", req.gseg);\n\n\tif (!req.gseg)\n\t\treturn gru_unload_all_contexts();\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tif (gts->ts_gru)\n\t\tgru_unload_context(gts, 1);\n\tgru_unlock_gts(gts);\n\n\treturn 0;\n}\n\n \nint gru_user_flush_tlb(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_flush_tlb_req req;\n\tstruct gru_mm_struct *gms;\n\n\tSTAT(user_flush_tlb);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\tgru_dbg(grudev, \"gseg 0x%lx, vaddr 0x%lx, len 0x%lx\\n\", req.gseg,\n\t\treq.vaddr, req.len);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts)\n\t\treturn -EINVAL;\n\n\tgms = gts->ts_gms;\n\tgru_unlock_gts(gts);\n\tgru_flush_tlb_range(gms, req.vaddr, req.len);\n\n\treturn 0;\n}\n\n \nlong gru_get_gseg_statistics(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_get_gseg_statistics_req req;\n\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\n\t \n\tgts = gru_find_lock_gts(req.gseg);\n\tif (gts) {\n\t\tmemcpy(&req.stats, &gts->ustats, sizeof(gts->ustats));\n\t\tgru_unlock_gts(gts);\n\t} else {\n\t\tmemset(&req.stats, 0, sizeof(gts->ustats));\n\t}\n\n\tif (copy_to_user((void __user *)arg, &req, sizeof(req)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n \nint gru_set_context_option(unsigned long arg)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_set_context_option_req req;\n\tint ret = 0;\n\n\tSTAT(set_context_option);\n\tif (copy_from_user(&req, (void __user *)arg, sizeof(req)))\n\t\treturn -EFAULT;\n\tgru_dbg(grudev, \"op %d, gseg 0x%lx, value1 0x%lx\\n\", req.op, req.gseg, req.val1);\n\n\tgts = gru_find_lock_gts(req.gseg);\n\tif (!gts) {\n\t\tgts = gru_alloc_locked_gts(req.gseg);\n\t\tif (IS_ERR(gts))\n\t\t\treturn PTR_ERR(gts);\n\t}\n\n\tswitch (req.op) {\n\tcase sco_blade_chiplet:\n\t\t \n\t\tif (req.val0 < -1 || req.val0 >= GRU_CHIPLETS_PER_HUB ||\n\t\t    req.val1 < -1 || req.val1 >= GRU_MAX_BLADES ||\n\t\t    (req.val1 >= 0 && !gru_base[req.val1])) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tgts->ts_user_blade_id = req.val1;\n\t\t\tgts->ts_user_chiplet_id = req.val0;\n\t\t\tif (gru_check_context_placement(gts)) {\n\t\t\t\tgru_unlock_gts(gts);\n\t\t\t\tgru_unload_context(gts, 1);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase sco_gseg_owner:\n \t\t \n\t\tgts->ts_tgid_owner = current->tgid;\n\t\tbreak;\n\tcase sco_cch_req_slice:\n \t\t \n\t\tgts->ts_cch_req_slice = req.val1 & 3;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\tgru_unlock_gts(gts);\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}