{
  "module_name": "grumain.c",
  "hash_id": "bc32947287e6750753802a4b13375a963d036582141e489c1ba20aa2badea128",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-gru/grumain.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/sched.h>\n#include <linux/device.h>\n#include <linux/list.h>\n#include <linux/err.h>\n#include <linux/prefetch.h>\n#include <asm/uv/uv_hub.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include \"gruhandles.h\"\n\nunsigned long gru_options __read_mostly;\n\nstatic struct device_driver gru_driver = {\n\t.name = \"gru\"\n};\n\nstatic struct device gru_device = {\n\t.init_name = \"\",\n\t.driver = &gru_driver,\n};\n\nstruct device *grudev = &gru_device;\n\n \nint gru_cpu_fault_map_id(void)\n{\n#ifdef CONFIG_IA64\n\treturn uv_blade_processor_id() % GRU_NUM_TFM;\n#else\n\tint cpu = smp_processor_id();\n\tint id, core;\n\n\tcore = uv_cpu_core_number(cpu);\n\tid = core + UV_MAX_INT_CORES * uv_cpu_socket_number(cpu);\n\treturn id;\n#endif\n}\n\n \n\n \nstatic int gru_wrap_asid(struct gru_state *gru)\n{\n\tgru_dbg(grudev, \"gid %d\\n\", gru->gs_gid);\n\tSTAT(asid_wrap);\n\tgru->gs_asid_gen++;\n\treturn MIN_ASID;\n}\n\n \nstatic int gru_reset_asid_limit(struct gru_state *gru, int asid)\n{\n\tint i, gid, inuse_asid, limit;\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\tSTAT(asid_next);\n\tlimit = MAX_ASID;\n\tif (asid >= limit)\n\t\tasid = gru_wrap_asid(gru);\n\tgru_flush_all_tlb(gru);\n\tgid = gru->gs_gid;\nagain:\n\tfor (i = 0; i < GRU_NUM_CCH; i++) {\n\t\tif (!gru->gs_gts[i] || is_kernel_context(gru->gs_gts[i]))\n\t\t\tcontinue;\n\t\tinuse_asid = gru->gs_gts[i]->ts_gms->ms_asids[gid].mt_asid;\n\t\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, inuse 0x%x, cxt %d\\n\",\n\t\t\tgru->gs_gid, gru->gs_gts[i], gru->gs_gts[i]->ts_gms,\n\t\t\tinuse_asid, i);\n\t\tif (inuse_asid == asid) {\n\t\t\tasid += ASID_INC;\n\t\t\tif (asid >= limit) {\n\t\t\t\t \n\t\t\t\tlimit = MAX_ASID;\n\t\t\t\tif (asid >= MAX_ASID)\n\t\t\t\t\tasid = gru_wrap_asid(gru);\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\n\t\tif ((inuse_asid > asid) && (inuse_asid < limit))\n\t\t\tlimit = inuse_asid;\n\t}\n\tgru->gs_asid_limit = limit;\n\tgru->gs_asid = asid;\n\tgru_dbg(grudev, \"gid %d, new asid 0x%x, new_limit 0x%x\\n\", gru->gs_gid,\n\t\t\t\t\tasid, limit);\n\treturn asid;\n}\n\n \nstatic int gru_assign_asid(struct gru_state *gru)\n{\n\tint asid;\n\n\tgru->gs_asid += ASID_INC;\n\tasid = gru->gs_asid;\n\tif (asid >= gru->gs_asid_limit)\n\t\tasid = gru_reset_asid_limit(gru, asid);\n\n\tgru_dbg(grudev, \"gid %d, asid 0x%x\\n\", gru->gs_gid, asid);\n\treturn asid;\n}\n\n \nstatic unsigned long reserve_resources(unsigned long *p, int n, int mmax,\n\t\t\t\t       signed char *idx)\n{\n\tunsigned long bits = 0;\n\tint i;\n\n\twhile (n--) {\n\t\ti = find_first_bit(p, mmax);\n\t\tif (i == mmax)\n\t\t\tBUG();\n\t\t__clear_bit(i, p);\n\t\t__set_bit(i, &bits);\n\t\tif (idx)\n\t\t\t*idx++ = i;\n\t}\n\treturn bits;\n}\n\nunsigned long gru_reserve_cb_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t\t       signed char *cbmap)\n{\n\treturn reserve_resources(&gru->gs_cbr_map, cbr_au_count, GRU_CBR_AU,\n\t\t\t\t cbmap);\n}\n\nunsigned long gru_reserve_ds_resources(struct gru_state *gru, int dsr_au_count,\n\t\t\t\t       signed char *dsmap)\n{\n\treturn reserve_resources(&gru->gs_dsr_map, dsr_au_count, GRU_DSR_AU,\n\t\t\t\t dsmap);\n}\n\nstatic void reserve_gru_resources(struct gru_state *gru,\n\t\t\t\t  struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts++;\n\tgts->ts_cbr_map =\n\t    gru_reserve_cb_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t     gts->ts_cbr_idx);\n\tgts->ts_dsr_map =\n\t    gru_reserve_ds_resources(gru, gts->ts_dsr_au_count, NULL);\n}\n\nstatic void free_gru_resources(struct gru_state *gru,\n\t\t\t       struct gru_thread_state *gts)\n{\n\tgru->gs_active_contexts--;\n\tgru->gs_cbr_map |= gts->ts_cbr_map;\n\tgru->gs_dsr_map |= gts->ts_dsr_map;\n}\n\n \nstatic int check_gru_resources(struct gru_state *gru, int cbr_au_count,\n\t\t\t       int dsr_au_count, int max_active_contexts)\n{\n\treturn hweight64(gru->gs_cbr_map) >= cbr_au_count\n\t\t&& hweight64(gru->gs_dsr_map) >= dsr_au_count\n\t\t&& gru->gs_active_contexts < max_active_contexts;\n}\n\n \nstatic int gru_load_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids = &gms->ms_asids[gru->gs_gid];\n\tunsigned short ctxbitmap = (1 << gts->ts_ctxnum);\n\tint asid;\n\n\tspin_lock(&gms->ms_asid_lock);\n\tasid = asids->mt_asid;\n\n\tspin_lock(&gru->gs_asid_lock);\n\tif (asid == 0 || (asids->mt_ctxbitmap == 0 && asids->mt_asid_gen !=\n\t\t\t  gru->gs_asid_gen)) {\n\t\tasid = gru_assign_asid(gru);\n\t\tasids->mt_asid = asid;\n\t\tasids->mt_asid_gen = gru->gs_asid_gen;\n\t\tSTAT(asid_new);\n\t} else {\n\t\tSTAT(asid_reuse);\n\t}\n\tspin_unlock(&gru->gs_asid_lock);\n\n\tBUG_ON(asids->mt_ctxbitmap & ctxbitmap);\n\tasids->mt_ctxbitmap |= ctxbitmap;\n\tif (!test_bit(gru->gs_gid, gms->ms_asidmap))\n\t\t__set_bit(gru->gs_gid, gms->ms_asidmap);\n\tspin_unlock(&gms->ms_asid_lock);\n\n\tgru_dbg(grudev,\n\t\t\"gid %d, gts %p, gms %p, ctxnum %d, asid 0x%x, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, asid,\n\t\tgms->ms_asidmap[0]);\n\treturn asid;\n}\n\nstatic void gru_unload_mm_tracker(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tstruct gru_mm_struct *gms = gts->ts_gms;\n\tstruct gru_mm_tracker *asids;\n\tunsigned short ctxbitmap;\n\n\tasids = &gms->ms_asids[gru->gs_gid];\n\tctxbitmap = (1 << gts->ts_ctxnum);\n\tspin_lock(&gms->ms_asid_lock);\n\tspin_lock(&gru->gs_asid_lock);\n\tBUG_ON((asids->mt_ctxbitmap & ctxbitmap) != ctxbitmap);\n\tasids->mt_ctxbitmap ^= ctxbitmap;\n\tgru_dbg(grudev, \"gid %d, gts %p, gms %p, ctxnum %d, asidmap 0x%lx\\n\",\n\t\tgru->gs_gid, gts, gms, gts->ts_ctxnum, gms->ms_asidmap[0]);\n\tspin_unlock(&gru->gs_asid_lock);\n\tspin_unlock(&gms->ms_asid_lock);\n}\n\n \nvoid gts_drop(struct gru_thread_state *gts)\n{\n\tif (gts && refcount_dec_and_test(&gts->ts_refcnt)) {\n\t\tif (gts->ts_gms)\n\t\t\tgru_drop_mmu_notifier(gts->ts_gms);\n\t\tkfree(gts);\n\t\tSTAT(gts_free);\n\t}\n}\n\n \nstatic struct gru_thread_state *gru_find_current_gts_nolock(struct gru_vma_data\n\t\t\t    *vdata, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\n\tlist_for_each_entry(gts, &vdata->vd_head, ts_next)\n\t    if (gts->ts_tsid == tsid)\n\t\treturn gts;\n\treturn NULL;\n}\n\n \nstruct gru_thread_state *gru_alloc_gts(struct vm_area_struct *vma,\n\t\tint cbr_au_count, int dsr_au_count,\n\t\tunsigned char tlb_preload_count, int options, int tsid)\n{\n\tstruct gru_thread_state *gts;\n\tstruct gru_mm_struct *gms;\n\tint bytes;\n\n\tbytes = DSR_BYTES(dsr_au_count) + CBR_BYTES(cbr_au_count);\n\tbytes += sizeof(struct gru_thread_state);\n\tgts = kmalloc(bytes, GFP_KERNEL);\n\tif (!gts)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tSTAT(gts_alloc);\n\tmemset(gts, 0, sizeof(struct gru_thread_state));  \n\trefcount_set(&gts->ts_refcnt, 1);\n\tmutex_init(&gts->ts_ctxlock);\n\tgts->ts_cbr_au_count = cbr_au_count;\n\tgts->ts_dsr_au_count = dsr_au_count;\n\tgts->ts_tlb_preload_count = tlb_preload_count;\n\tgts->ts_user_options = options;\n\tgts->ts_user_blade_id = -1;\n\tgts->ts_user_chiplet_id = -1;\n\tgts->ts_tsid = tsid;\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_tlb_int_select = -1;\n\tgts->ts_cch_req_slice = -1;\n\tgts->ts_sizeavail = GRU_SIZEAVAIL(PAGE_SHIFT);\n\tif (vma) {\n\t\tgts->ts_mm = current->mm;\n\t\tgts->ts_vma = vma;\n\t\tgms = gru_register_mmu_notifier();\n\t\tif (IS_ERR(gms))\n\t\t\tgoto err;\n\t\tgts->ts_gms = gms;\n\t}\n\n\tgru_dbg(grudev, \"alloc gts %p\\n\", gts);\n\treturn gts;\n\nerr:\n\tgts_drop(gts);\n\treturn ERR_CAST(gms);\n}\n\n \nstruct gru_vma_data *gru_alloc_vma_data(struct vm_area_struct *vma, int tsid)\n{\n\tstruct gru_vma_data *vdata = NULL;\n\n\tvdata = kmalloc(sizeof(*vdata), GFP_KERNEL);\n\tif (!vdata)\n\t\treturn NULL;\n\n\tSTAT(vdata_alloc);\n\tINIT_LIST_HEAD(&vdata->vd_head);\n\tspin_lock_init(&vdata->vd_lock);\n\tgru_dbg(grudev, \"alloc vdata %p\\n\", vdata);\n\treturn vdata;\n}\n\n \nstruct gru_thread_state *gru_find_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tgts = gru_find_current_gts_nolock(vdata, tsid);\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n \nstruct gru_thread_state *gru_alloc_thread_state(struct vm_area_struct *vma,\n\t\t\t\t\tint tsid)\n{\n\tstruct gru_vma_data *vdata = vma->vm_private_data;\n\tstruct gru_thread_state *gts, *ngts;\n\n\tgts = gru_alloc_gts(vma, vdata->vd_cbr_au_count,\n\t\t\t    vdata->vd_dsr_au_count,\n\t\t\t    vdata->vd_tlb_preload_count,\n\t\t\t    vdata->vd_user_options, tsid);\n\tif (IS_ERR(gts))\n\t\treturn gts;\n\n\tspin_lock(&vdata->vd_lock);\n\tngts = gru_find_current_gts_nolock(vdata, tsid);\n\tif (ngts) {\n\t\tgts_drop(gts);\n\t\tgts = ngts;\n\t\tSTAT(gts_double_allocate);\n\t} else {\n\t\tlist_add(&gts->ts_next, &vdata->vd_head);\n\t}\n\tspin_unlock(&vdata->vd_lock);\n\tgru_dbg(grudev, \"vma %p, gts %p\\n\", vma, gts);\n\treturn gts;\n}\n\n \nstatic void gru_free_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\n\tgru = gts->ts_gru;\n\tgru_dbg(grudev, \"gts %p, gid %d\\n\", gts, gru->gs_gid);\n\n\tspin_lock(&gru->gs_lock);\n\tgru->gs_gts[gts->ts_ctxnum] = NULL;\n\tfree_gru_resources(gru, gts);\n\tBUG_ON(test_bit(gts->ts_ctxnum, &gru->gs_context_map) == 0);\n\t__clear_bit(gts->ts_ctxnum, &gru->gs_context_map);\n\tgts->ts_ctxnum = NULLCTX;\n\tgts->ts_gru = NULL;\n\tgts->ts_blade = -1;\n\tspin_unlock(&gru->gs_lock);\n\n\tgts_drop(gts);\n\tSTAT(free_context);\n}\n\n \nstatic void prefetch_data(void *p, int num, int stride)\n{\n\twhile (num-- > 0) {\n\t\tprefetchw(p);\n\t\tp += stride;\n\t}\n}\n\nstatic inline long gru_copy_handle(void *d, void *s)\n{\n\tmemcpy(d, s, GRU_HANDLE_BYTES);\n\treturn GRU_HANDLE_BYTES;\n}\n\nstatic void gru_prefetch_context(void *gseg, void *cb, void *cbe,\n\t\t\t\tunsigned long cbrmap, unsigned long length)\n{\n\tint i, scr;\n\n\tprefetch_data(gseg + GRU_DS_BASE, length / GRU_CACHE_LINE_BYTES,\n\t\t      GRU_CACHE_LINE_BYTES);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tprefetch_data(cb, 1, GRU_CACHE_LINE_BYTES);\n\t\tprefetch_data(cbe + i * GRU_HANDLE_STRIDE, 1,\n\t\t\t      GRU_CACHE_LINE_BYTES);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n}\n\nstatic void gru_load_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t  unsigned long cbrmap, unsigned long dsrmap,\n\t\t\t\t  int data_valid)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tif (data_valid) {\n\t\t\tsave += gru_copy_handle(cb, save);\n\t\t\tsave += gru_copy_handle(cbe + i * GRU_HANDLE_STRIDE,\n\t\t\t\t\t\tsave);\n\t\t} else {\n\t\t\tmemset(cb, 0, GRU_CACHE_LINE_BYTES);\n\t\t\tmemset(cbe + i * GRU_HANDLE_STRIDE, 0,\n\t\t\t\t\t\tGRU_CACHE_LINE_BYTES);\n\t\t}\n\t\t \n\t\tmb();\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\n\tif (data_valid)\n\t\tmemcpy(gseg + GRU_DS_BASE, save, length);\n\telse\n\t\tmemset(gseg + GRU_DS_BASE, 0, length);\n}\n\nstatic void gru_unload_context_data(void *save, void *grubase, int ctxnum,\n\t\t\t\t    unsigned long cbrmap, unsigned long dsrmap)\n{\n\tvoid *gseg, *cb, *cbe;\n\tunsigned long length;\n\tint i, scr;\n\n\tgseg = grubase + ctxnum * GRU_GSEG_STRIDE;\n\tcb = gseg + GRU_CB_BASE;\n\tcbe = grubase + GRU_CBE_BASE;\n\tlength = hweight64(dsrmap) * GRU_DSR_AU_BYTES;\n\n\t \n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr)\n\t\tgru_flush_cache(cbe + i * GRU_HANDLE_STRIDE);\n\tmb();\t\t \n\n\tgru_prefetch_context(gseg, cb, cbe, cbrmap, length);\n\n\tfor_each_cbr_in_allocation_map(i, &cbrmap, scr) {\n\t\tsave += gru_copy_handle(save, cb);\n\t\tsave += gru_copy_handle(save, cbe + i * GRU_HANDLE_STRIDE);\n\t\tcb += GRU_HANDLE_STRIDE;\n\t}\n\tmemcpy(save, gseg + GRU_DS_BASE, length);\n}\n\nvoid gru_unload_context(struct gru_thread_state *gts, int savestate)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint ctxnum = gts->ts_ctxnum;\n\n\tif (!is_kernel_context(gts))\n\t\tzap_vma_ptes(gts->ts_vma, UGRUADDR(gts), GRU_GSEG_PAGESIZE);\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tgru_dbg(grudev, \"gts %p, cbrmap 0x%lx, dsrmap 0x%lx\\n\",\n\t\tgts, gts->ts_cbr_map, gts->ts_dsr_map);\n\tlock_cch_handle(cch);\n\tif (cch_interrupt_sync(cch))\n\t\tBUG();\n\n\tif (!is_kernel_context(gts))\n\t\tgru_unload_mm_tracker(gru, gts);\n\tif (savestate) {\n\t\tgru_unload_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr,\n\t\t\t\t\tctxnum, gts->ts_cbr_map,\n\t\t\t\t\tgts->ts_dsr_map);\n\t\tgts->ts_data_valid = 1;\n\t}\n\n\tif (cch_deallocate(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_free_gru_context(gts);\n}\n\n \nvoid gru_load_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru = gts->ts_gru;\n\tstruct gru_context_configuration_handle *cch;\n\tint i, err, asid, ctxnum = gts->ts_ctxnum;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\tlock_cch_handle(cch);\n\tcch->tfm_fault_bit_enable =\n\t    (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t     || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tcch->tlb_int_enable = (gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\tif (cch->tlb_int_enable) {\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gts->ts_tlb_int_select;\n\t}\n\tif (gts->ts_cch_req_slice >= 0) {\n\t\tcch->req_slice_set_enable = 1;\n\t\tcch->req_slice = gts->ts_cch_req_slice;\n\t} else {\n\t\tcch->req_slice_set_enable =0;\n\t}\n\tcch->tfm_done_bit_enable = 0;\n\tcch->dsr_allocation_map = gts->ts_dsr_map;\n\tcch->cbr_allocation_map = gts->ts_cbr_map;\n\n\tif (is_kernel_context(gts)) {\n\t\tcch->unmap_enable = 1;\n\t\tcch->tfm_done_bit_enable = 1;\n\t\tcch->cb_int_enable = 1;\n\t\tcch->tlb_int_select = 0;\t \n\t} else {\n\t\tcch->unmap_enable = 0;\n\t\tcch->tfm_done_bit_enable = 0;\n\t\tcch->cb_int_enable = 0;\n\t\tasid = gru_load_mm_tracker(gru, gts);\n\t\tfor (i = 0; i < 8; i++) {\n\t\t\tcch->asid[i] = asid + i;\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\t}\n\t}\n\n\terr = cch_allocate(cch);\n\tif (err) {\n\t\tgru_dbg(grudev,\n\t\t\t\"err %d: cch %p, gts %p, cbr 0x%lx, dsr 0x%lx\\n\",\n\t\t\terr, cch, gts, gts->ts_cbr_map, gts->ts_dsr_map);\n\t\tBUG();\n\t}\n\n\tgru_load_context_data(gts->ts_gdata, gru->gs_gru_base_vaddr, ctxnum,\n\t\t\tgts->ts_cbr_map, gts->ts_dsr_map, gts->ts_data_valid);\n\n\tif (cch_start(cch))\n\t\tBUG();\n\tunlock_cch_handle(cch);\n\n\tgru_dbg(grudev, \"gid %d, gts %p, cbrmap 0x%lx, dsrmap 0x%lx, tie %d, tis %d\\n\",\n\t\tgts->ts_gru->gs_gid, gts, gts->ts_cbr_map, gts->ts_dsr_map,\n\t\t(gts->ts_user_options == GRU_OPT_MISS_FMM_INTR), gts->ts_tlb_int_select);\n}\n\n \nint gru_update_cch(struct gru_thread_state *gts)\n{\n\tstruct gru_context_configuration_handle *cch;\n\tstruct gru_state *gru = gts->ts_gru;\n\tint i, ctxnum = gts->ts_ctxnum, ret = 0;\n\n\tcch = get_cch(gru->gs_gru_base_vaddr, ctxnum);\n\n\tlock_cch_handle(cch);\n\tif (cch->state == CCHSTATE_ACTIVE) {\n\t\tif (gru->gs_gts[gts->ts_ctxnum] != gts)\n\t\t\tgoto exit;\n\t\tif (cch_interrupt(cch))\n\t\t\tBUG();\n\t\tfor (i = 0; i < 8; i++)\n\t\t\tcch->sizeavail[i] = gts->ts_sizeavail;\n\t\tgts->ts_tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tlb_int_select = gru_cpu_fault_map_id();\n\t\tcch->tfm_fault_bit_enable =\n\t\t  (gts->ts_user_options == GRU_OPT_MISS_FMM_POLL\n\t\t    || gts->ts_user_options == GRU_OPT_MISS_FMM_INTR);\n\t\tif (cch_start(cch))\n\t\t\tBUG();\n\t\tret = 1;\n\t}\nexit:\n\tunlock_cch_handle(cch);\n\treturn ret;\n}\n\n \nstatic int gru_retarget_intr(struct gru_thread_state *gts)\n{\n\tif (gts->ts_tlb_int_select < 0\n\t    || gts->ts_tlb_int_select == gru_cpu_fault_map_id())\n\t\treturn 0;\n\n\tgru_dbg(grudev, \"retarget from %d to %d\\n\", gts->ts_tlb_int_select,\n\t\tgru_cpu_fault_map_id());\n\treturn gru_update_cch(gts);\n}\n\n \nstatic int gru_check_chiplet_assignment(struct gru_state *gru,\n\t\t\t\t\tstruct gru_thread_state *gts)\n{\n\tint blade_id;\n\tint chiplet_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\n\tchiplet_id = gts->ts_user_chiplet_id;\n\treturn gru->gs_blade_id == blade_id &&\n\t\t(chiplet_id < 0 || chiplet_id == gru->gs_chiplet_id);\n}\n\n \nint gru_check_context_placement(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru;\n\tint ret = 0;\n\n\t \n\tgru = gts->ts_gru;\n\t \n\tif (!gru || gts->ts_tgid_owner != current->tgid)\n\t\treturn ret;\n\n\tif (!gru_check_chiplet_assignment(gru, gts)) {\n\t\tSTAT(check_context_unload);\n\t\tret = -EINVAL;\n\t} else if (gru_retarget_intr(gts)) {\n\t\tSTAT(check_context_retarget_intr);\n\t}\n\n\treturn ret;\n}\n\n\n \n#define next_ctxnum(n)\t((n) <  GRU_NUM_CCH - 2 ? (n) + 1 : 0)\n#define next_gru(b, g)\t(((g) < &(b)->bs_grus[GRU_CHIPLETS_PER_BLADE - 1]) ?  \\\n\t\t\t\t ((g)+1) : &(b)->bs_grus[0])\n\nstatic int is_gts_stealable(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts))\n\t\treturn down_write_trylock(&bs->bs_kgts_sema);\n\telse\n\t\treturn mutex_trylock(&gts->ts_ctxlock);\n}\n\nstatic void gts_stolen(struct gru_thread_state *gts,\n\t\tstruct gru_blade_state *bs)\n{\n\tif (is_kernel_context(gts)) {\n\t\tup_write(&bs->bs_kgts_sema);\n\t\tSTAT(steal_kernel_context);\n\t} else {\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tSTAT(steal_user_context);\n\t}\n}\n\nvoid gru_steal_context(struct gru_thread_state *gts)\n{\n\tstruct gru_blade_state *blade;\n\tstruct gru_state *gru, *gru0;\n\tstruct gru_thread_state *ngts = NULL;\n\tint ctxnum, ctxnum0, flag = 0, cbr, dsr;\n\tint blade_id;\n\n\tblade_id = gts->ts_user_blade_id;\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\n\tcbr = gts->ts_cbr_au_count;\n\tdsr = gts->ts_dsr_au_count;\n\n\tblade = gru_base[blade_id];\n\tspin_lock(&blade->bs_lock);\n\n\tctxnum = next_ctxnum(blade->bs_lru_ctxnum);\n\tgru = blade->bs_lru_gru;\n\tif (ctxnum == 0)\n\t\tgru = next_gru(blade, gru);\n\tblade->bs_lru_gru = gru;\n\tblade->bs_lru_ctxnum = ctxnum;\n\tctxnum0 = ctxnum;\n\tgru0 = gru;\n\twhile (1) {\n\t\tif (gru_check_chiplet_assignment(gru, gts)) {\n\t\t\tif (check_gru_resources(gru, cbr, dsr, GRU_NUM_CCH))\n\t\t\t\tbreak;\n\t\t\tspin_lock(&gru->gs_lock);\n\t\t\tfor (; ctxnum < GRU_NUM_CCH; ctxnum++) {\n\t\t\t\tif (flag && gru == gru0 && ctxnum == ctxnum0)\n\t\t\t\t\tbreak;\n\t\t\t\tngts = gru->gs_gts[ctxnum];\n\t\t\t\t \n\t\t\t\tif (ngts && is_gts_stealable(ngts, blade))\n\t\t\t\t\tbreak;\n\t\t\t\tngts = NULL;\n\t\t\t}\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tif (ngts || (flag && gru == gru0 && ctxnum == ctxnum0))\n\t\t\t\tbreak;\n\t\t}\n\t\tif (flag && gru == gru0)\n\t\t\tbreak;\n\t\tflag = 1;\n\t\tctxnum = 0;\n\t\tgru = next_gru(blade, gru);\n\t}\n\tspin_unlock(&blade->bs_lock);\n\n\tif (ngts) {\n\t\tgts->ustats.context_stolen++;\n\t\tngts->ts_steal_jiffies = jiffies;\n\t\tgru_unload_context(ngts, is_kernel_context(ngts) ? 0 : 1);\n\t\tgts_stolen(ngts, blade);\n\t} else {\n\t\tSTAT(steal_context_failed);\n\t}\n\tgru_dbg(grudev,\n\t\t\"stole gid %d, ctxnum %d from gts %p. Need cb %d, ds %d;\"\n\t\t\" avail cb %ld, ds %ld\\n\",\n\t\tgru->gs_gid, ctxnum, ngts, cbr, dsr, hweight64(gru->gs_cbr_map),\n\t\thweight64(gru->gs_dsr_map));\n}\n\n \nstatic int gru_assign_context_number(struct gru_state *gru)\n{\n\tint ctxnum;\n\n\tctxnum = find_first_zero_bit(&gru->gs_context_map, GRU_NUM_CCH);\n\t__set_bit(ctxnum, &gru->gs_context_map);\n\treturn ctxnum;\n}\n\n \nstruct gru_state *gru_assign_gru_context(struct gru_thread_state *gts)\n{\n\tstruct gru_state *gru, *grux;\n\tint i, max_active_contexts;\n\tint blade_id = gts->ts_user_blade_id;\n\n\tif (blade_id < 0)\n\t\tblade_id = uv_numa_blade_id();\nagain:\n\tgru = NULL;\n\tmax_active_contexts = GRU_NUM_CCH;\n\tfor_each_gru_on_blade(grux, blade_id, i) {\n\t\tif (!gru_check_chiplet_assignment(grux, gts))\n\t\t\tcontinue;\n\t\tif (check_gru_resources(grux, gts->ts_cbr_au_count,\n\t\t\t\t\tgts->ts_dsr_au_count,\n\t\t\t\t\tmax_active_contexts)) {\n\t\t\tgru = grux;\n\t\t\tmax_active_contexts = grux->gs_active_contexts;\n\t\t\tif (max_active_contexts == 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (gru) {\n\t\tspin_lock(&gru->gs_lock);\n\t\tif (!check_gru_resources(gru, gts->ts_cbr_au_count,\n\t\t\t\t\t gts->ts_dsr_au_count, GRU_NUM_CCH)) {\n\t\t\tspin_unlock(&gru->gs_lock);\n\t\t\tgoto again;\n\t\t}\n\t\treserve_gru_resources(gru, gts);\n\t\tgts->ts_gru = gru;\n\t\tgts->ts_blade = gru->gs_blade_id;\n\t\tgts->ts_ctxnum = gru_assign_context_number(gru);\n\t\trefcount_inc(&gts->ts_refcnt);\n\t\tgru->gs_gts[gts->ts_ctxnum] = gts;\n\t\tspin_unlock(&gru->gs_lock);\n\n\t\tSTAT(assign_context);\n\t\tgru_dbg(grudev,\n\t\t\t\"gseg %p, gts %p, gid %d, ctx %d, cbr %d, dsr %d\\n\",\n\t\t\tgseg_virtual_address(gts->ts_gru, gts->ts_ctxnum), gts,\n\t\t\tgts->ts_gru->gs_gid, gts->ts_ctxnum,\n\t\t\tgts->ts_cbr_au_count, gts->ts_dsr_au_count);\n\t} else {\n\t\tgru_dbg(grudev, \"failed to allocate a GTS %s\\n\", \"\");\n\t\tSTAT(assign_context_failed);\n\t}\n\n\treturn gru;\n}\n\n \nvm_fault_t gru_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct gru_thread_state *gts;\n\tunsigned long paddr, vaddr;\n\tunsigned long expires;\n\n\tvaddr = vmf->address;\n\tgru_dbg(grudev, \"vma %p, vaddr 0x%lx (0x%lx)\\n\",\n\t\tvma, vaddr, GSEG_BASE(vaddr));\n\tSTAT(nopfn);\n\n\t \n\tgts = gru_find_thread_state(vma, TSID(vaddr, vma));\n\tif (!gts)\n\t\treturn VM_FAULT_SIGBUS;\n\nagain:\n\tmutex_lock(&gts->ts_ctxlock);\n\tpreempt_disable();\n\n\tif (gru_check_context_placement(gts)) {\n\t\tpreempt_enable();\n\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\tgru_unload_context(gts, 1);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\n\tif (!gts->ts_gru) {\n\t\tSTAT(load_user_context);\n\t\tif (!gru_assign_gru_context(gts)) {\n\t\t\tpreempt_enable();\n\t\t\tmutex_unlock(&gts->ts_ctxlock);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tschedule_timeout(GRU_ASSIGN_DELAY);   \n\t\t\texpires = gts->ts_steal_jiffies + GRU_STEAL_DELAY;\n\t\t\tif (time_before(expires, jiffies))\n\t\t\t\tgru_steal_context(gts);\n\t\t\tgoto again;\n\t\t}\n\t\tgru_load_context(gts);\n\t\tpaddr = gseg_physical_address(gts->ts_gru, gts->ts_ctxnum);\n\t\tremap_pfn_range(vma, vaddr & ~(GRU_GSEG_PAGESIZE - 1),\n\t\t\t\tpaddr >> PAGE_SHIFT, GRU_GSEG_PAGESIZE,\n\t\t\t\tvma->vm_page_prot);\n\t}\n\n\tpreempt_enable();\n\tmutex_unlock(&gts->ts_ctxlock);\n\n\treturn VM_FAULT_NOPAGE;\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}