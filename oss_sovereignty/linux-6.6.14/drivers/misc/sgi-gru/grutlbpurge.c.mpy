{
  "module_name": "grutlbpurge.c",
  "hash_id": "de56163a61144d2dd7d82c1348608ce5c27d261d3c41e0f1dcda36126482dc17",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/sgi-gru/grutlbpurge.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/device.h>\n#include <linux/hugetlb.h>\n#include <linux/delay.h>\n#include <linux/timex.h>\n#include <linux/srcu.h>\n#include <asm/processor.h>\n#include \"gru.h\"\n#include \"grutables.h\"\n#include <asm/uv/uv_hub.h>\n\n#define gru_random()\tget_cycles()\n\n \nstatic inline int get_off_blade_tgh(struct gru_state *gru)\n{\n\tint n;\n\n\tn = GRU_NUM_TGH - gru->gs_tgh_first_remote;\n\tn = gru_random() % n;\n\tn += gru->gs_tgh_first_remote;\n\treturn n;\n}\n\nstatic inline int get_on_blade_tgh(struct gru_state *gru)\n{\n\treturn uv_blade_processor_id() >> gru->gs_tgh_local_shift;\n}\n\nstatic struct gru_tlb_global_handle *get_lock_tgh_handle(struct gru_state\n\t\t\t\t\t\t\t *gru)\n{\n\tstruct gru_tlb_global_handle *tgh;\n\tint n;\n\n\tpreempt_disable();\n\tif (uv_numa_blade_id() == gru->gs_blade_id)\n\t\tn = get_on_blade_tgh(gru);\n\telse\n\t\tn = get_off_blade_tgh(gru);\n\ttgh = get_tgh_by_index(gru, n);\n\tlock_tgh_handle(tgh);\n\n\treturn tgh;\n}\n\nstatic void get_unlock_tgh_handle(struct gru_tlb_global_handle *tgh)\n{\n\tunlock_tgh_handle(tgh);\n\tpreempt_enable();\n}\n\n \n\nvoid gru_flush_tlb_range(struct gru_mm_struct *gms, unsigned long start,\n\t\t\t unsigned long len)\n{\n\tstruct gru_state *gru;\n\tstruct gru_mm_tracker *asids;\n\tstruct gru_tlb_global_handle *tgh;\n\tunsigned long num;\n\tint grupagesize, pagesize, pageshift, gid, asid;\n\n\t \n\tpageshift = PAGE_SHIFT;\n\tpagesize = (1UL << pageshift);\n\tgrupagesize = GRU_PAGESIZE(pageshift);\n\tnum = min(((len + pagesize - 1) >> pageshift), GRUMAXINVAL);\n\n\tSTAT(flush_tlb);\n\tgru_dbg(grudev, \"gms %p, start 0x%lx, len 0x%lx, asidmap 0x%lx\\n\", gms,\n\t\tstart, len, gms->ms_asidmap[0]);\n\n\tspin_lock(&gms->ms_asid_lock);\n\tfor_each_gru_in_bitmap(gid, gms->ms_asidmap) {\n\t\tSTAT(flush_tlb_gru);\n\t\tgru = GID_TO_GRU(gid);\n\t\tasids = gms->ms_asids + gid;\n\t\tasid = asids->mt_asid;\n\t\tif (asids->mt_ctxbitmap && asid) {\n\t\t\tSTAT(flush_tlb_gru_tgh);\n\t\t\tasid = GRUASID(asid, start);\n\t\t\tgru_dbg(grudev,\n\t\"  FLUSH gruid %d, asid 0x%x, vaddr 0x%lx, vamask 0x%x, num %ld, cbmap 0x%x\\n\",\n\t\t\t      gid, asid, start, grupagesize, num, asids->mt_ctxbitmap);\n\t\t\ttgh = get_lock_tgh_handle(gru);\n\t\t\ttgh_invalidate(tgh, start, ~0, asid, grupagesize, 0,\n\t\t\t\t       num - 1, asids->mt_ctxbitmap);\n\t\t\tget_unlock_tgh_handle(tgh);\n\t\t} else {\n\t\t\tSTAT(flush_tlb_gru_zero_asid);\n\t\t\tasids->mt_asid = 0;\n\t\t\t__clear_bit(gru->gs_gid, gms->ms_asidmap);\n\t\t\tgru_dbg(grudev,\n\t\"  CLEARASID gruid %d, asid 0x%x, cbtmap 0x%x, asidmap 0x%lx\\n\",\n\t\t\t\tgid, asid, asids->mt_ctxbitmap,\n\t\t\t\tgms->ms_asidmap[0]);\n\t\t}\n\t}\n\tspin_unlock(&gms->ms_asid_lock);\n}\n\n \nvoid gru_flush_all_tlb(struct gru_state *gru)\n{\n\tstruct gru_tlb_global_handle *tgh;\n\n\tgru_dbg(grudev, \"gid %d\\n\", gru->gs_gid);\n\ttgh = get_lock_tgh_handle(gru);\n\ttgh_invalidate(tgh, 0, ~0, 0, 1, 1, GRUMAXINVAL - 1, 0xffff);\n\tget_unlock_tgh_handle(tgh);\n}\n\n \nstatic int gru_invalidate_range_start(struct mmu_notifier *mn,\n\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\n\t\t\t\t\t\t ms_notifier);\n\n\tSTAT(mmu_invalidate_range);\n\tatomic_inc(&gms->ms_range_active);\n\tgru_dbg(grudev, \"gms %p, start 0x%lx, end 0x%lx, act %d\\n\", gms,\n\t\trange->start, range->end, atomic_read(&gms->ms_range_active));\n\tgru_flush_tlb_range(gms, range->start, range->end - range->start);\n\n\treturn 0;\n}\n\nstatic void gru_invalidate_range_end(struct mmu_notifier *mn,\n\t\t\tconst struct mmu_notifier_range *range)\n{\n\tstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\n\t\t\t\t\t\t ms_notifier);\n\n\t \n\t(void)atomic_dec_and_test(&gms->ms_range_active);\n\n\twake_up_all(&gms->ms_wait_queue);\n\tgru_dbg(grudev, \"gms %p, start 0x%lx, end 0x%lx\\n\",\n\t\tgms, range->start, range->end);\n}\n\nstatic struct mmu_notifier *gru_alloc_notifier(struct mm_struct *mm)\n{\n\tstruct gru_mm_struct *gms;\n\n\tgms = kzalloc(sizeof(*gms), GFP_KERNEL);\n\tif (!gms)\n\t\treturn ERR_PTR(-ENOMEM);\n\tSTAT(gms_alloc);\n\tspin_lock_init(&gms->ms_asid_lock);\n\tinit_waitqueue_head(&gms->ms_wait_queue);\n\n\treturn &gms->ms_notifier;\n}\n\nstatic void gru_free_notifier(struct mmu_notifier *mn)\n{\n\tkfree(container_of(mn, struct gru_mm_struct, ms_notifier));\n\tSTAT(gms_free);\n}\n\nstatic const struct mmu_notifier_ops gru_mmuops = {\n\t.invalidate_range_start\t= gru_invalidate_range_start,\n\t.invalidate_range_end\t= gru_invalidate_range_end,\n\t.alloc_notifier\t\t= gru_alloc_notifier,\n\t.free_notifier\t\t= gru_free_notifier,\n};\n\nstruct gru_mm_struct *gru_register_mmu_notifier(void)\n{\n\tstruct mmu_notifier *mn;\n\n\tmn = mmu_notifier_get_locked(&gru_mmuops, current->mm);\n\tif (IS_ERR(mn))\n\t\treturn ERR_CAST(mn);\n\n\treturn container_of(mn, struct gru_mm_struct, ms_notifier);\n}\n\nvoid gru_drop_mmu_notifier(struct gru_mm_struct *gms)\n{\n\tmmu_notifier_put(&gms->ms_notifier);\n}\n\n \n#define MAX_LOCAL_TGH\t16\n\nvoid gru_tgh_flush_init(struct gru_state *gru)\n{\n\tint cpus, shift = 0, n;\n\n\tcpus = uv_blade_nr_possible_cpus(gru->gs_blade_id);\n\n\t \n\tif (cpus) {\n\t\tn = 1 << fls(cpus - 1);\n\n\t\t \n\t\tshift = max(0, fls(n - 1) - fls(MAX_LOCAL_TGH - 1));\n\t}\n\tgru->gs_tgh_local_shift = shift;\n\n\t \n\tgru->gs_tgh_first_remote = (cpus + (1 << shift) - 1) >> shift;\n\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}