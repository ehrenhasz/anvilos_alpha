{
  "module_name": "vmci_queue_pair.c",
  "hash_id": "d729c70f2fd2d3a8b8c689bf8a2ebf6d439eb61b2468f78b7b9b8cbefe35e995",
  "original_prompt": "Ingested from linux-6.6.14/drivers/misc/vmw_vmci/vmci_queue_pair.c",
  "human_readable_source": "\n \n\n#include <linux/vmw_vmci_defs.h>\n#include <linux/vmw_vmci_api.h>\n#include <linux/highmem.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/pagemap.h>\n#include <linux/pci.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/uio.h>\n#include <linux/wait.h>\n#include <linux/vmalloc.h>\n#include <linux/skbuff.h>\n\n#include \"vmci_handle_array.h\"\n#include \"vmci_queue_pair.h\"\n#include \"vmci_datagram.h\"\n#include \"vmci_resource.h\"\n#include \"vmci_context.h\"\n#include \"vmci_driver.h\"\n#include \"vmci_event.h\"\n#include \"vmci_route.h\"\n\n \n\n \nstruct vmci_queue_kern_if {\n\tstruct mutex __mutex;\t \n\tstruct mutex *mutex;\t \n\tsize_t num_pages;\t \n\tbool host;\t\t \n\tunion {\n\t\tstruct {\n\t\t\tdma_addr_t *pas;\n\t\t\tvoid **vas;\n\t\t} g;\t\t \n\t\tstruct {\n\t\t\tstruct page **page;\n\t\t\tstruct page **header_page;\n\t\t} h;\t\t \n\t} u;\n};\n\n \nstruct vmci_qp {\n\tstruct vmci_handle handle;\n\tstruct vmci_queue *produce_q;\n\tstruct vmci_queue *consume_q;\n\tu64 produce_q_size;\n\tu64 consume_q_size;\n\tu32 peer;\n\tu32 flags;\n\tu32 priv_flags;\n\tbool guest_endpoint;\n\tunsigned int blocked;\n\tunsigned int generation;\n\twait_queue_head_t event;\n};\n\nenum qp_broker_state {\n\tVMCIQPB_NEW,\n\tVMCIQPB_CREATED_NO_MEM,\n\tVMCIQPB_CREATED_MEM,\n\tVMCIQPB_ATTACHED_NO_MEM,\n\tVMCIQPB_ATTACHED_MEM,\n\tVMCIQPB_SHUTDOWN_NO_MEM,\n\tVMCIQPB_SHUTDOWN_MEM,\n\tVMCIQPB_GONE\n};\n\n#define QPBROKERSTATE_HAS_MEM(_qpb) (_qpb->state == VMCIQPB_CREATED_MEM || \\\n\t\t\t\t     _qpb->state == VMCIQPB_ATTACHED_MEM || \\\n\t\t\t\t     _qpb->state == VMCIQPB_SHUTDOWN_MEM)\n\n \nstruct qp_entry {\n\tstruct list_head list_item;\n\tstruct vmci_handle handle;\n\tu32 peer;\n\tu32 flags;\n\tu64 produce_size;\n\tu64 consume_size;\n\tu32 ref_count;\n};\n\nstruct qp_broker_entry {\n\tstruct vmci_resource resource;\n\tstruct qp_entry qp;\n\tu32 create_id;\n\tu32 attach_id;\n\tenum qp_broker_state state;\n\tbool require_trusted_attach;\n\tbool created_by_trusted;\n\tbool vmci_page_files;\t \n\tstruct vmci_queue *produce_q;\n\tstruct vmci_queue *consume_q;\n\tstruct vmci_queue_header saved_produce_q;\n\tstruct vmci_queue_header saved_consume_q;\n\tvmci_event_release_cb wakeup_cb;\n\tvoid *client_data;\n\tvoid *local_mem;\t \n};\n\nstruct qp_guest_endpoint {\n\tstruct vmci_resource resource;\n\tstruct qp_entry qp;\n\tu64 num_ppns;\n\tvoid *produce_q;\n\tvoid *consume_q;\n\tstruct ppn_set ppn_set;\n};\n\nstruct qp_list {\n\tstruct list_head head;\n\tstruct mutex mutex;\t \n};\n\nstatic struct qp_list qp_broker_list = {\n\t.head = LIST_HEAD_INIT(qp_broker_list.head),\n\t.mutex = __MUTEX_INITIALIZER(qp_broker_list.mutex),\n};\n\nstatic struct qp_list qp_guest_endpoints = {\n\t.head = LIST_HEAD_INIT(qp_guest_endpoints.head),\n\t.mutex = __MUTEX_INITIALIZER(qp_guest_endpoints.mutex),\n};\n\n#define INVALID_VMCI_GUEST_MEM_ID  0\n#define QPE_NUM_PAGES(_QPE) ((u32) \\\n\t\t\t     (DIV_ROUND_UP(_QPE.produce_size, PAGE_SIZE) + \\\n\t\t\t      DIV_ROUND_UP(_QPE.consume_size, PAGE_SIZE) + 2))\n#define QP_SIZES_ARE_VALID(_prod_qsize, _cons_qsize) \\\n\t((_prod_qsize) + (_cons_qsize) >= max(_prod_qsize, _cons_qsize) && \\\n\t (_prod_qsize) + (_cons_qsize) <= VMCI_MAX_GUEST_QP_MEMORY)\n\n \nstatic void qp_free_queue(void *q, u64 size)\n{\n\tstruct vmci_queue *queue = q;\n\n\tif (queue) {\n\t\tu64 i;\n\n\t\t \n\t\tfor (i = 0; i < DIV_ROUND_UP(size, PAGE_SIZE) + 1; i++) {\n\t\t\tdma_free_coherent(&vmci_pdev->dev, PAGE_SIZE,\n\t\t\t\t\t  queue->kernel_if->u.g.vas[i],\n\t\t\t\t\t  queue->kernel_if->u.g.pas[i]);\n\t\t}\n\n\t\tvfree(queue);\n\t}\n}\n\n \nstatic void *qp_alloc_queue(u64 size, u32 flags)\n{\n\tu64 i;\n\tstruct vmci_queue *queue;\n\tsize_t pas_size;\n\tsize_t vas_size;\n\tsize_t queue_size = sizeof(*queue) + sizeof(*queue->kernel_if);\n\tu64 num_pages;\n\n\tif (size > SIZE_MAX - PAGE_SIZE)\n\t\treturn NULL;\n\tnum_pages = DIV_ROUND_UP(size, PAGE_SIZE) + 1;\n\tif (num_pages >\n\t\t (SIZE_MAX - queue_size) /\n\t\t (sizeof(*queue->kernel_if->u.g.pas) +\n\t\t  sizeof(*queue->kernel_if->u.g.vas)))\n\t\treturn NULL;\n\n\tpas_size = num_pages * sizeof(*queue->kernel_if->u.g.pas);\n\tvas_size = num_pages * sizeof(*queue->kernel_if->u.g.vas);\n\tqueue_size += pas_size + vas_size;\n\n\tqueue = vmalloc(queue_size);\n\tif (!queue)\n\t\treturn NULL;\n\n\tqueue->q_header = NULL;\n\tqueue->saved_header = NULL;\n\tqueue->kernel_if = (struct vmci_queue_kern_if *)(queue + 1);\n\tqueue->kernel_if->mutex = NULL;\n\tqueue->kernel_if->num_pages = num_pages;\n\tqueue->kernel_if->u.g.pas = (dma_addr_t *)(queue->kernel_if + 1);\n\tqueue->kernel_if->u.g.vas =\n\t\t(void **)((u8 *)queue->kernel_if->u.g.pas + pas_size);\n\tqueue->kernel_if->host = false;\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tqueue->kernel_if->u.g.vas[i] =\n\t\t\tdma_alloc_coherent(&vmci_pdev->dev, PAGE_SIZE,\n\t\t\t\t\t   &queue->kernel_if->u.g.pas[i],\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!queue->kernel_if->u.g.vas[i]) {\n\t\t\t \n\t\t\tqp_free_queue(queue, i * PAGE_SIZE);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\t \n\tqueue->q_header = queue->kernel_if->u.g.vas[0];\n\n\treturn queue;\n}\n\n \nstatic int qp_memcpy_to_queue_iter(struct vmci_queue *queue,\n\t\t\t\t  u64 queue_offset,\n\t\t\t\t  struct iov_iter *from,\n\t\t\t\t  size_t size)\n{\n\tstruct vmci_queue_kern_if *kernel_if = queue->kernel_if;\n\tsize_t bytes_copied = 0;\n\n\twhile (bytes_copied < size) {\n\t\tconst u64 page_index =\n\t\t\t(queue_offset + bytes_copied) / PAGE_SIZE;\n\t\tconst size_t page_offset =\n\t\t    (queue_offset + bytes_copied) & (PAGE_SIZE - 1);\n\t\tvoid *va;\n\t\tsize_t to_copy;\n\n\t\tif (kernel_if->host)\n\t\t\tva = kmap_local_page(kernel_if->u.h.page[page_index]);\n\t\telse\n\t\t\tva = kernel_if->u.g.vas[page_index + 1];\n\t\t\t \n\n\t\tif (size - bytes_copied > PAGE_SIZE - page_offset)\n\t\t\t \n\t\t\tto_copy = PAGE_SIZE - page_offset;\n\t\telse\n\t\t\tto_copy = size - bytes_copied;\n\n\t\tif (!copy_from_iter_full((u8 *)va + page_offset, to_copy,\n\t\t\t\t\t from)) {\n\t\t\tif (kernel_if->host)\n\t\t\t\tkunmap_local(va);\n\t\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t\t}\n\t\tbytes_copied += to_copy;\n\t\tif (kernel_if->host)\n\t\t\tkunmap_local(va);\n\t}\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic int qp_memcpy_from_queue_iter(struct iov_iter *to,\n\t\t\t\t    const struct vmci_queue *queue,\n\t\t\t\t    u64 queue_offset, size_t size)\n{\n\tstruct vmci_queue_kern_if *kernel_if = queue->kernel_if;\n\tsize_t bytes_copied = 0;\n\n\twhile (bytes_copied < size) {\n\t\tconst u64 page_index =\n\t\t\t(queue_offset + bytes_copied) / PAGE_SIZE;\n\t\tconst size_t page_offset =\n\t\t    (queue_offset + bytes_copied) & (PAGE_SIZE - 1);\n\t\tvoid *va;\n\t\tsize_t to_copy;\n\t\tint err;\n\n\t\tif (kernel_if->host)\n\t\t\tva = kmap_local_page(kernel_if->u.h.page[page_index]);\n\t\telse\n\t\t\tva = kernel_if->u.g.vas[page_index + 1];\n\t\t\t \n\n\t\tif (size - bytes_copied > PAGE_SIZE - page_offset)\n\t\t\t \n\t\t\tto_copy = PAGE_SIZE - page_offset;\n\t\telse\n\t\t\tto_copy = size - bytes_copied;\n\n\t\terr = copy_to_iter((u8 *)va + page_offset, to_copy, to);\n\t\tif (err != to_copy) {\n\t\t\tif (kernel_if->host)\n\t\t\t\tkunmap_local(va);\n\t\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t\t}\n\t\tbytes_copied += to_copy;\n\t\tif (kernel_if->host)\n\t\t\tkunmap_local(va);\n\t}\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic int qp_alloc_ppn_set(void *prod_q,\n\t\t\t    u64 num_produce_pages,\n\t\t\t    void *cons_q,\n\t\t\t    u64 num_consume_pages, struct ppn_set *ppn_set)\n{\n\tu64 *produce_ppns;\n\tu64 *consume_ppns;\n\tstruct vmci_queue *produce_q = prod_q;\n\tstruct vmci_queue *consume_q = cons_q;\n\tu64 i;\n\n\tif (!produce_q || !num_produce_pages || !consume_q ||\n\t    !num_consume_pages || !ppn_set)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tif (ppn_set->initialized)\n\t\treturn VMCI_ERROR_ALREADY_EXISTS;\n\n\tproduce_ppns =\n\t    kmalloc_array(num_produce_pages, sizeof(*produce_ppns),\n\t\t\t  GFP_KERNEL);\n\tif (!produce_ppns)\n\t\treturn VMCI_ERROR_NO_MEM;\n\n\tconsume_ppns =\n\t    kmalloc_array(num_consume_pages, sizeof(*consume_ppns),\n\t\t\t  GFP_KERNEL);\n\tif (!consume_ppns) {\n\t\tkfree(produce_ppns);\n\t\treturn VMCI_ERROR_NO_MEM;\n\t}\n\n\tfor (i = 0; i < num_produce_pages; i++)\n\t\tproduce_ppns[i] =\n\t\t\tproduce_q->kernel_if->u.g.pas[i] >> PAGE_SHIFT;\n\n\tfor (i = 0; i < num_consume_pages; i++)\n\t\tconsume_ppns[i] =\n\t\t\tconsume_q->kernel_if->u.g.pas[i] >> PAGE_SHIFT;\n\n\tppn_set->num_produce_pages = num_produce_pages;\n\tppn_set->num_consume_pages = num_consume_pages;\n\tppn_set->produce_ppns = produce_ppns;\n\tppn_set->consume_ppns = consume_ppns;\n\tppn_set->initialized = true;\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic void qp_free_ppn_set(struct ppn_set *ppn_set)\n{\n\tif (ppn_set->initialized) {\n\t\t \n\t\tkfree(ppn_set->produce_ppns);\n\t\tkfree(ppn_set->consume_ppns);\n\t}\n\tmemset(ppn_set, 0, sizeof(*ppn_set));\n}\n\n \nstatic int qp_populate_ppn_set(u8 *call_buf, const struct ppn_set *ppn_set)\n{\n\tif (vmci_use_ppn64()) {\n\t\tmemcpy(call_buf, ppn_set->produce_ppns,\n\t\t       ppn_set->num_produce_pages *\n\t\t       sizeof(*ppn_set->produce_ppns));\n\t\tmemcpy(call_buf +\n\t\t       ppn_set->num_produce_pages *\n\t\t       sizeof(*ppn_set->produce_ppns),\n\t\t       ppn_set->consume_ppns,\n\t\t       ppn_set->num_consume_pages *\n\t\t       sizeof(*ppn_set->consume_ppns));\n\t} else {\n\t\tint i;\n\t\tu32 *ppns = (u32 *) call_buf;\n\n\t\tfor (i = 0; i < ppn_set->num_produce_pages; i++)\n\t\t\tppns[i] = (u32) ppn_set->produce_ppns[i];\n\n\t\tppns = &ppns[ppn_set->num_produce_pages];\n\n\t\tfor (i = 0; i < ppn_set->num_consume_pages; i++)\n\t\t\tppns[i] = (u32) ppn_set->consume_ppns[i];\n\t}\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic struct vmci_queue *qp_host_alloc_queue(u64 size)\n{\n\tstruct vmci_queue *queue;\n\tsize_t queue_page_size;\n\tu64 num_pages;\n\tconst size_t queue_size = sizeof(*queue) + sizeof(*(queue->kernel_if));\n\n\tif (size > min_t(size_t, VMCI_MAX_GUEST_QP_MEMORY, SIZE_MAX - PAGE_SIZE))\n\t\treturn NULL;\n\tnum_pages = DIV_ROUND_UP(size, PAGE_SIZE) + 1;\n\tif (num_pages > (SIZE_MAX - queue_size) /\n\t\t sizeof(*queue->kernel_if->u.h.page))\n\t\treturn NULL;\n\n\tqueue_page_size = num_pages * sizeof(*queue->kernel_if->u.h.page);\n\n\tif (queue_size + queue_page_size > KMALLOC_MAX_SIZE)\n\t\treturn NULL;\n\n\tqueue = kzalloc(queue_size + queue_page_size, GFP_KERNEL);\n\tif (queue) {\n\t\tqueue->q_header = NULL;\n\t\tqueue->saved_header = NULL;\n\t\tqueue->kernel_if = (struct vmci_queue_kern_if *)(queue + 1);\n\t\tqueue->kernel_if->host = true;\n\t\tqueue->kernel_if->mutex = NULL;\n\t\tqueue->kernel_if->num_pages = num_pages;\n\t\tqueue->kernel_if->u.h.header_page =\n\t\t    (struct page **)((u8 *)queue + queue_size);\n\t\tqueue->kernel_if->u.h.page =\n\t\t\t&queue->kernel_if->u.h.header_page[1];\n\t}\n\n\treturn queue;\n}\n\n \nstatic void qp_host_free_queue(struct vmci_queue *queue, u64 queue_size)\n{\n\tkfree(queue);\n}\n\n \nstatic void qp_init_queue_mutex(struct vmci_queue *produce_q,\n\t\t\t\tstruct vmci_queue *consume_q)\n{\n\t \n\n\tif (produce_q->kernel_if->host) {\n\t\tproduce_q->kernel_if->mutex = &produce_q->kernel_if->__mutex;\n\t\tconsume_q->kernel_if->mutex = &produce_q->kernel_if->__mutex;\n\t\tmutex_init(produce_q->kernel_if->mutex);\n\t}\n}\n\n \nstatic void qp_cleanup_queue_mutex(struct vmci_queue *produce_q,\n\t\t\t\t   struct vmci_queue *consume_q)\n{\n\tif (produce_q->kernel_if->host) {\n\t\tproduce_q->kernel_if->mutex = NULL;\n\t\tconsume_q->kernel_if->mutex = NULL;\n\t}\n}\n\n \nstatic void qp_acquire_queue_mutex(struct vmci_queue *queue)\n{\n\tif (queue->kernel_if->host)\n\t\tmutex_lock(queue->kernel_if->mutex);\n}\n\n \nstatic void qp_release_queue_mutex(struct vmci_queue *queue)\n{\n\tif (queue->kernel_if->host)\n\t\tmutex_unlock(queue->kernel_if->mutex);\n}\n\n \nstatic void qp_release_pages(struct page **pages,\n\t\t\t     u64 num_pages, bool dirty)\n{\n\tint i;\n\n\tfor (i = 0; i < num_pages; i++) {\n\t\tif (dirty)\n\t\t\tset_page_dirty_lock(pages[i]);\n\n\t\tput_page(pages[i]);\n\t\tpages[i] = NULL;\n\t}\n}\n\n \nstatic int qp_host_get_user_memory(u64 produce_uva,\n\t\t\t\t   u64 consume_uva,\n\t\t\t\t   struct vmci_queue *produce_q,\n\t\t\t\t   struct vmci_queue *consume_q)\n{\n\tint retval;\n\tint err = VMCI_SUCCESS;\n\n\tretval = get_user_pages_fast((uintptr_t) produce_uva,\n\t\t\t\t     produce_q->kernel_if->num_pages,\n\t\t\t\t     FOLL_WRITE,\n\t\t\t\t     produce_q->kernel_if->u.h.header_page);\n\tif (retval < (int)produce_q->kernel_if->num_pages) {\n\t\tpr_debug(\"get_user_pages_fast(produce) failed (retval=%d)\",\n\t\t\tretval);\n\t\tif (retval > 0)\n\t\t\tqp_release_pages(produce_q->kernel_if->u.h.header_page,\n\t\t\t\t\tretval, false);\n\t\terr = VMCI_ERROR_NO_MEM;\n\t\tgoto out;\n\t}\n\n\tretval = get_user_pages_fast((uintptr_t) consume_uva,\n\t\t\t\t     consume_q->kernel_if->num_pages,\n\t\t\t\t     FOLL_WRITE,\n\t\t\t\t     consume_q->kernel_if->u.h.header_page);\n\tif (retval < (int)consume_q->kernel_if->num_pages) {\n\t\tpr_debug(\"get_user_pages_fast(consume) failed (retval=%d)\",\n\t\t\tretval);\n\t\tif (retval > 0)\n\t\t\tqp_release_pages(consume_q->kernel_if->u.h.header_page,\n\t\t\t\t\tretval, false);\n\t\tqp_release_pages(produce_q->kernel_if->u.h.header_page,\n\t\t\t\t produce_q->kernel_if->num_pages, false);\n\t\terr = VMCI_ERROR_NO_MEM;\n\t}\n\n out:\n\treturn err;\n}\n\n \nstatic int qp_host_register_user_memory(struct vmci_qp_page_store *page_store,\n\t\t\t\t\tstruct vmci_queue *produce_q,\n\t\t\t\t\tstruct vmci_queue *consume_q)\n{\n\tu64 produce_uva;\n\tu64 consume_uva;\n\n\t \n\tproduce_uva = page_store->pages;\n\tconsume_uva = page_store->pages +\n\t    produce_q->kernel_if->num_pages * PAGE_SIZE;\n\treturn qp_host_get_user_memory(produce_uva, consume_uva, produce_q,\n\t\t\t\t       consume_q);\n}\n\n \nstatic void qp_host_unregister_user_memory(struct vmci_queue *produce_q,\n\t\t\t\t\t   struct vmci_queue *consume_q)\n{\n\tqp_release_pages(produce_q->kernel_if->u.h.header_page,\n\t\t\t produce_q->kernel_if->num_pages, true);\n\tmemset(produce_q->kernel_if->u.h.header_page, 0,\n\t       sizeof(*produce_q->kernel_if->u.h.header_page) *\n\t       produce_q->kernel_if->num_pages);\n\tqp_release_pages(consume_q->kernel_if->u.h.header_page,\n\t\t\t consume_q->kernel_if->num_pages, true);\n\tmemset(consume_q->kernel_if->u.h.header_page, 0,\n\t       sizeof(*consume_q->kernel_if->u.h.header_page) *\n\t       consume_q->kernel_if->num_pages);\n}\n\n \nstatic int qp_host_map_queues(struct vmci_queue *produce_q,\n\t\t\t      struct vmci_queue *consume_q)\n{\n\tint result;\n\n\tif (!produce_q->q_header || !consume_q->q_header) {\n\t\tstruct page *headers[2];\n\n\t\tif (produce_q->q_header != consume_q->q_header)\n\t\t\treturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\n\n\t\tif (produce_q->kernel_if->u.h.header_page == NULL ||\n\t\t    *produce_q->kernel_if->u.h.header_page == NULL)\n\t\t\treturn VMCI_ERROR_UNAVAILABLE;\n\n\t\theaders[0] = *produce_q->kernel_if->u.h.header_page;\n\t\theaders[1] = *consume_q->kernel_if->u.h.header_page;\n\n\t\tproduce_q->q_header = vmap(headers, 2, VM_MAP, PAGE_KERNEL);\n\t\tif (produce_q->q_header != NULL) {\n\t\t\tconsume_q->q_header =\n\t\t\t    (struct vmci_queue_header *)((u8 *)\n\t\t\t\t\t\t\t produce_q->q_header +\n\t\t\t\t\t\t\t PAGE_SIZE);\n\t\t\tresult = VMCI_SUCCESS;\n\t\t} else {\n\t\t\tpr_warn(\"vmap failed\\n\");\n\t\t\tresult = VMCI_ERROR_NO_MEM;\n\t\t}\n\t} else {\n\t\tresult = VMCI_SUCCESS;\n\t}\n\n\treturn result;\n}\n\n \nstatic int qp_host_unmap_queues(u32 gid,\n\t\t\t\tstruct vmci_queue *produce_q,\n\t\t\t\tstruct vmci_queue *consume_q)\n{\n\tif (produce_q->q_header) {\n\t\tif (produce_q->q_header < consume_q->q_header)\n\t\t\tvunmap(produce_q->q_header);\n\t\telse\n\t\t\tvunmap(consume_q->q_header);\n\n\t\tproduce_q->q_header = NULL;\n\t\tconsume_q->q_header = NULL;\n\t}\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic struct qp_entry *qp_list_find(struct qp_list *qp_list,\n\t\t\t\t     struct vmci_handle handle)\n{\n\tstruct qp_entry *entry;\n\n\tif (vmci_handle_is_invalid(handle))\n\t\treturn NULL;\n\n\tlist_for_each_entry(entry, &qp_list->head, list_item) {\n\t\tif (vmci_handle_is_equal(entry->handle, handle))\n\t\t\treturn entry;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct qp_guest_endpoint *\nqp_guest_handle_to_entry(struct vmci_handle handle)\n{\n\tstruct qp_guest_endpoint *entry;\n\tstruct qp_entry *qp = qp_list_find(&qp_guest_endpoints, handle);\n\n\tentry = qp ? container_of(\n\t\tqp, struct qp_guest_endpoint, qp) : NULL;\n\treturn entry;\n}\n\n \nstatic struct qp_broker_entry *\nqp_broker_handle_to_entry(struct vmci_handle handle)\n{\n\tstruct qp_broker_entry *entry;\n\tstruct qp_entry *qp = qp_list_find(&qp_broker_list, handle);\n\n\tentry = qp ? container_of(\n\t\tqp, struct qp_broker_entry, qp) : NULL;\n\treturn entry;\n}\n\n \nstatic int qp_notify_peer_local(bool attach, struct vmci_handle handle)\n{\n\tu32 context_id = vmci_get_context_id();\n\tstruct vmci_event_qp ev;\n\n\tmemset(&ev, 0, sizeof(ev));\n\tev.msg.hdr.dst = vmci_make_handle(context_id, VMCI_EVENT_HANDLER);\n\tev.msg.hdr.src = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\n\t\t\t\t\t  VMCI_CONTEXT_RESOURCE_ID);\n\tev.msg.hdr.payload_size = sizeof(ev) - sizeof(ev.msg.hdr);\n\tev.msg.event_data.event =\n\t    attach ? VMCI_EVENT_QP_PEER_ATTACH : VMCI_EVENT_QP_PEER_DETACH;\n\tev.payload.peer_id = context_id;\n\tev.payload.handle = handle;\n\n\treturn vmci_event_dispatch(&ev.msg.hdr);\n}\n\n \nstatic struct qp_guest_endpoint *\nqp_guest_endpoint_create(struct vmci_handle handle,\n\t\t\t u32 peer,\n\t\t\t u32 flags,\n\t\t\t u64 produce_size,\n\t\t\t u64 consume_size,\n\t\t\t void *produce_q,\n\t\t\t void *consume_q)\n{\n\tint result;\n\tstruct qp_guest_endpoint *entry;\n\t \n\tconst u64 num_ppns = DIV_ROUND_UP(produce_size, PAGE_SIZE) +\n\t    DIV_ROUND_UP(consume_size, PAGE_SIZE) + 2;\n\n\tif (vmci_handle_is_invalid(handle)) {\n\t\tu32 context_id = vmci_get_context_id();\n\n\t\thandle = vmci_make_handle(context_id, VMCI_INVALID_ID);\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (entry) {\n\t\tentry->qp.peer = peer;\n\t\tentry->qp.flags = flags;\n\t\tentry->qp.produce_size = produce_size;\n\t\tentry->qp.consume_size = consume_size;\n\t\tentry->qp.ref_count = 0;\n\t\tentry->num_ppns = num_ppns;\n\t\tentry->produce_q = produce_q;\n\t\tentry->consume_q = consume_q;\n\t\tINIT_LIST_HEAD(&entry->qp.list_item);\n\n\t\t \n\t\tresult = vmci_resource_add(&entry->resource,\n\t\t\t\t\t   VMCI_RESOURCE_TYPE_QPAIR_GUEST,\n\t\t\t\t\t   handle);\n\t\tentry->qp.handle = vmci_resource_handle(&entry->resource);\n\t\tif ((result != VMCI_SUCCESS) ||\n\t\t    qp_list_find(&qp_guest_endpoints, entry->qp.handle)) {\n\t\t\tpr_warn(\"Failed to add new resource (handle=0x%x:0x%x), error: %d\",\n\t\t\t\thandle.context, handle.resource, result);\n\t\t\tkfree(entry);\n\t\t\tentry = NULL;\n\t\t}\n\t}\n\treturn entry;\n}\n\n \nstatic void qp_guest_endpoint_destroy(struct qp_guest_endpoint *entry)\n{\n\tqp_free_ppn_set(&entry->ppn_set);\n\tqp_cleanup_queue_mutex(entry->produce_q, entry->consume_q);\n\tqp_free_queue(entry->produce_q, entry->qp.produce_size);\n\tqp_free_queue(entry->consume_q, entry->qp.consume_size);\n\t \n\tvmci_resource_remove(&entry->resource);\n\n\tkfree(entry);\n}\n\n \nstatic int qp_alloc_hypercall(const struct qp_guest_endpoint *entry)\n{\n\tstruct vmci_qp_alloc_msg *alloc_msg;\n\tsize_t msg_size;\n\tsize_t ppn_size;\n\tint result;\n\n\tif (!entry || entry->num_ppns <= 2)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tppn_size = vmci_use_ppn64() ? sizeof(u64) : sizeof(u32);\n\tmsg_size = sizeof(*alloc_msg) +\n\t    (size_t) entry->num_ppns * ppn_size;\n\talloc_msg = kmalloc(msg_size, GFP_KERNEL);\n\tif (!alloc_msg)\n\t\treturn VMCI_ERROR_NO_MEM;\n\n\talloc_msg->hdr.dst = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\n\t\t\t\t\t      VMCI_QUEUEPAIR_ALLOC);\n\talloc_msg->hdr.src = VMCI_ANON_SRC_HANDLE;\n\talloc_msg->hdr.payload_size = msg_size - VMCI_DG_HEADERSIZE;\n\talloc_msg->handle = entry->qp.handle;\n\talloc_msg->peer = entry->qp.peer;\n\talloc_msg->flags = entry->qp.flags;\n\talloc_msg->produce_size = entry->qp.produce_size;\n\talloc_msg->consume_size = entry->qp.consume_size;\n\talloc_msg->num_ppns = entry->num_ppns;\n\n\tresult = qp_populate_ppn_set((u8 *)alloc_msg + sizeof(*alloc_msg),\n\t\t\t\t     &entry->ppn_set);\n\tif (result == VMCI_SUCCESS)\n\t\tresult = vmci_send_datagram(&alloc_msg->hdr);\n\n\tkfree(alloc_msg);\n\n\treturn result;\n}\n\n \nstatic int qp_detatch_hypercall(struct vmci_handle handle)\n{\n\tstruct vmci_qp_detach_msg detach_msg;\n\n\tdetach_msg.hdr.dst = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\n\t\t\t\t\t      VMCI_QUEUEPAIR_DETACH);\n\tdetach_msg.hdr.src = VMCI_ANON_SRC_HANDLE;\n\tdetach_msg.hdr.payload_size = sizeof(handle);\n\tdetach_msg.handle = handle;\n\n\treturn vmci_send_datagram(&detach_msg.hdr);\n}\n\n \nstatic void qp_list_add_entry(struct qp_list *qp_list, struct qp_entry *entry)\n{\n\tif (entry)\n\t\tlist_add(&entry->list_item, &qp_list->head);\n}\n\n \nstatic void qp_list_remove_entry(struct qp_list *qp_list,\n\t\t\t\t struct qp_entry *entry)\n{\n\tif (entry)\n\t\tlist_del(&entry->list_item);\n}\n\n \nstatic int qp_detatch_guest_work(struct vmci_handle handle)\n{\n\tint result;\n\tstruct qp_guest_endpoint *entry;\n\tu32 ref_count = ~0;\t \n\n\tmutex_lock(&qp_guest_endpoints.mutex);\n\n\tentry = qp_guest_handle_to_entry(handle);\n\tif (!entry) {\n\t\tmutex_unlock(&qp_guest_endpoints.mutex);\n\t\treturn VMCI_ERROR_NOT_FOUND;\n\t}\n\n\tif (entry->qp.flags & VMCI_QPFLAG_LOCAL) {\n\t\tresult = VMCI_SUCCESS;\n\n\t\tif (entry->qp.ref_count > 1) {\n\t\t\tresult = qp_notify_peer_local(false, handle);\n\t\t\t \n\t\t}\n\t} else {\n\t\tresult = qp_detatch_hypercall(handle);\n\t\tif (result < VMCI_SUCCESS) {\n\t\t\t \n\n\t\t\tmutex_unlock(&qp_guest_endpoints.mutex);\n\t\t\treturn result;\n\t\t}\n\t}\n\n\t \n\n\tentry->qp.ref_count--;\n\tif (entry->qp.ref_count == 0)\n\t\tqp_list_remove_entry(&qp_guest_endpoints, &entry->qp);\n\n\t \n\tif (entry)\n\t\tref_count = entry->qp.ref_count;\n\n\tmutex_unlock(&qp_guest_endpoints.mutex);\n\n\tif (ref_count == 0)\n\t\tqp_guest_endpoint_destroy(entry);\n\n\treturn result;\n}\n\n \nstatic int qp_alloc_guest_work(struct vmci_handle *handle,\n\t\t\t       struct vmci_queue **produce_q,\n\t\t\t       u64 produce_size,\n\t\t\t       struct vmci_queue **consume_q,\n\t\t\t       u64 consume_size,\n\t\t\t       u32 peer,\n\t\t\t       u32 flags,\n\t\t\t       u32 priv_flags)\n{\n\tconst u64 num_produce_pages =\n\t    DIV_ROUND_UP(produce_size, PAGE_SIZE) + 1;\n\tconst u64 num_consume_pages =\n\t    DIV_ROUND_UP(consume_size, PAGE_SIZE) + 1;\n\tvoid *my_produce_q = NULL;\n\tvoid *my_consume_q = NULL;\n\tint result;\n\tstruct qp_guest_endpoint *queue_pair_entry = NULL;\n\n\tif (priv_flags != VMCI_NO_PRIVILEGE_FLAGS)\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\tmutex_lock(&qp_guest_endpoints.mutex);\n\n\tqueue_pair_entry = qp_guest_handle_to_entry(*handle);\n\tif (queue_pair_entry) {\n\t\tif (queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) {\n\t\t\t \n\t\t\tif (queue_pair_entry->qp.ref_count > 1) {\n\t\t\t\tpr_devel(\"Error attempting to attach more than once\\n\");\n\t\t\t\tresult = VMCI_ERROR_UNAVAILABLE;\n\t\t\t\tgoto error_keep_entry;\n\t\t\t}\n\n\t\t\tif (queue_pair_entry->qp.produce_size != consume_size ||\n\t\t\t    queue_pair_entry->qp.consume_size !=\n\t\t\t    produce_size ||\n\t\t\t    queue_pair_entry->qp.flags !=\n\t\t\t    (flags & ~VMCI_QPFLAG_ATTACH_ONLY)) {\n\t\t\t\tpr_devel(\"Error mismatched queue pair in local attach\\n\");\n\t\t\t\tresult = VMCI_ERROR_QUEUEPAIR_MISMATCH;\n\t\t\t\tgoto error_keep_entry;\n\t\t\t}\n\n\t\t\t \n\t\t\tresult = qp_notify_peer_local(true, *handle);\n\t\t\tif (result < VMCI_SUCCESS)\n\t\t\t\tgoto error_keep_entry;\n\n\t\t\tmy_produce_q = queue_pair_entry->consume_q;\n\t\t\tmy_consume_q = queue_pair_entry->produce_q;\n\t\t\tgoto out;\n\t\t}\n\n\t\tresult = VMCI_ERROR_ALREADY_EXISTS;\n\t\tgoto error_keep_entry;\n\t}\n\n\tmy_produce_q = qp_alloc_queue(produce_size, flags);\n\tif (!my_produce_q) {\n\t\tpr_warn(\"Error allocating pages for produce queue\\n\");\n\t\tresult = VMCI_ERROR_NO_MEM;\n\t\tgoto error;\n\t}\n\n\tmy_consume_q = qp_alloc_queue(consume_size, flags);\n\tif (!my_consume_q) {\n\t\tpr_warn(\"Error allocating pages for consume queue\\n\");\n\t\tresult = VMCI_ERROR_NO_MEM;\n\t\tgoto error;\n\t}\n\n\tqueue_pair_entry = qp_guest_endpoint_create(*handle, peer, flags,\n\t\t\t\t\t\t    produce_size, consume_size,\n\t\t\t\t\t\t    my_produce_q, my_consume_q);\n\tif (!queue_pair_entry) {\n\t\tpr_warn(\"Error allocating memory in %s\\n\", __func__);\n\t\tresult = VMCI_ERROR_NO_MEM;\n\t\tgoto error;\n\t}\n\n\tresult = qp_alloc_ppn_set(my_produce_q, num_produce_pages, my_consume_q,\n\t\t\t\t  num_consume_pages,\n\t\t\t\t  &queue_pair_entry->ppn_set);\n\tif (result < VMCI_SUCCESS) {\n\t\tpr_warn(\"qp_alloc_ppn_set failed\\n\");\n\t\tgoto error;\n\t}\n\n\t \n\tif (queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) {\n\t\t \n\t\tu32 context_id = vmci_get_context_id();\n\n\t\t \n\t\tif (queue_pair_entry->qp.handle.context != context_id ||\n\t\t    (queue_pair_entry->qp.peer != VMCI_INVALID_ID &&\n\t\t     queue_pair_entry->qp.peer != context_id)) {\n\t\t\tresult = VMCI_ERROR_NO_ACCESS;\n\t\t\tgoto error;\n\t\t}\n\n\t\tif (queue_pair_entry->qp.flags & VMCI_QPFLAG_ATTACH_ONLY) {\n\t\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tresult = qp_alloc_hypercall(queue_pair_entry);\n\t\tif (result < VMCI_SUCCESS) {\n\t\t\tpr_devel(\"qp_alloc_hypercall result = %d\\n\", result);\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tqp_init_queue_mutex((struct vmci_queue *)my_produce_q,\n\t\t\t    (struct vmci_queue *)my_consume_q);\n\n\tqp_list_add_entry(&qp_guest_endpoints, &queue_pair_entry->qp);\n\n out:\n\tqueue_pair_entry->qp.ref_count++;\n\t*handle = queue_pair_entry->qp.handle;\n\t*produce_q = (struct vmci_queue *)my_produce_q;\n\t*consume_q = (struct vmci_queue *)my_consume_q;\n\n\t \n\tif ((queue_pair_entry->qp.flags & VMCI_QPFLAG_LOCAL) &&\n\t    queue_pair_entry->qp.ref_count == 1) {\n\t\tvmci_q_header_init((*produce_q)->q_header, *handle);\n\t\tvmci_q_header_init((*consume_q)->q_header, *handle);\n\t}\n\n\tmutex_unlock(&qp_guest_endpoints.mutex);\n\n\treturn VMCI_SUCCESS;\n\n error:\n\tmutex_unlock(&qp_guest_endpoints.mutex);\n\tif (queue_pair_entry) {\n\t\t \n\t\tqp_guest_endpoint_destroy(queue_pair_entry);\n\t} else {\n\t\tqp_free_queue(my_produce_q, produce_size);\n\t\tqp_free_queue(my_consume_q, consume_size);\n\t}\n\treturn result;\n\n error_keep_entry:\n\t \n\tmutex_unlock(&qp_guest_endpoints.mutex);\n\treturn result;\n}\n\n \nstatic int qp_broker_create(struct vmci_handle handle,\n\t\t\t    u32 peer,\n\t\t\t    u32 flags,\n\t\t\t    u32 priv_flags,\n\t\t\t    u64 produce_size,\n\t\t\t    u64 consume_size,\n\t\t\t    struct vmci_qp_page_store *page_store,\n\t\t\t    struct vmci_ctx *context,\n\t\t\t    vmci_event_release_cb wakeup_cb,\n\t\t\t    void *client_data, struct qp_broker_entry **ent)\n{\n\tstruct qp_broker_entry *entry = NULL;\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tbool is_local = flags & VMCI_QPFLAG_LOCAL;\n\tint result;\n\tu64 guest_produce_size;\n\tu64 guest_consume_size;\n\n\t \n\tif (flags & VMCI_QPFLAG_ATTACH_ONLY)\n\t\treturn VMCI_ERROR_NOT_FOUND;\n\n\t \n\tif (handle.context != context_id && handle.context != peer)\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\tif (VMCI_CONTEXT_IS_VM(context_id) && VMCI_CONTEXT_IS_VM(peer))\n\t\treturn VMCI_ERROR_DST_UNREACHABLE;\n\n\t \n\tif (is_local && peer != VMCI_INVALID_ID && context_id != peer)\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\tentry = kzalloc(sizeof(*entry), GFP_ATOMIC);\n\tif (!entry)\n\t\treturn VMCI_ERROR_NO_MEM;\n\n\tif (vmci_ctx_get_id(context) == VMCI_HOST_CONTEXT_ID && !is_local) {\n\t\t \n\n\t\tguest_produce_size = consume_size;\n\t\tguest_consume_size = produce_size;\n\t} else {\n\t\tguest_produce_size = produce_size;\n\t\tguest_consume_size = consume_size;\n\t}\n\n\tentry->qp.handle = handle;\n\tentry->qp.peer = peer;\n\tentry->qp.flags = flags;\n\tentry->qp.produce_size = guest_produce_size;\n\tentry->qp.consume_size = guest_consume_size;\n\tentry->qp.ref_count = 1;\n\tentry->create_id = context_id;\n\tentry->attach_id = VMCI_INVALID_ID;\n\tentry->state = VMCIQPB_NEW;\n\tentry->require_trusted_attach =\n\t    !!(context->priv_flags & VMCI_PRIVILEGE_FLAG_RESTRICTED);\n\tentry->created_by_trusted =\n\t    !!(priv_flags & VMCI_PRIVILEGE_FLAG_TRUSTED);\n\tentry->vmci_page_files = false;\n\tentry->wakeup_cb = wakeup_cb;\n\tentry->client_data = client_data;\n\tentry->produce_q = qp_host_alloc_queue(guest_produce_size);\n\tif (entry->produce_q == NULL) {\n\t\tresult = VMCI_ERROR_NO_MEM;\n\t\tgoto error;\n\t}\n\tentry->consume_q = qp_host_alloc_queue(guest_consume_size);\n\tif (entry->consume_q == NULL) {\n\t\tresult = VMCI_ERROR_NO_MEM;\n\t\tgoto error;\n\t}\n\n\tqp_init_queue_mutex(entry->produce_q, entry->consume_q);\n\n\tINIT_LIST_HEAD(&entry->qp.list_item);\n\n\tif (is_local) {\n\t\tu8 *tmp;\n\n\t\tentry->local_mem = kcalloc(QPE_NUM_PAGES(entry->qp),\n\t\t\t\t\t   PAGE_SIZE, GFP_KERNEL);\n\t\tif (entry->local_mem == NULL) {\n\t\t\tresult = VMCI_ERROR_NO_MEM;\n\t\t\tgoto error;\n\t\t}\n\t\tentry->state = VMCIQPB_CREATED_MEM;\n\t\tentry->produce_q->q_header = entry->local_mem;\n\t\ttmp = (u8 *)entry->local_mem + PAGE_SIZE *\n\t\t    (DIV_ROUND_UP(entry->qp.produce_size, PAGE_SIZE) + 1);\n\t\tentry->consume_q->q_header = (struct vmci_queue_header *)tmp;\n\t} else if (page_store) {\n\t\t \n\t\tresult = qp_host_register_user_memory(page_store,\n\t\t\t\t\t\t      entry->produce_q,\n\t\t\t\t\t\t      entry->consume_q);\n\t\tif (result < VMCI_SUCCESS)\n\t\t\tgoto error;\n\n\t\tentry->state = VMCIQPB_CREATED_MEM;\n\t} else {\n\t\t \n\t\tentry->state = VMCIQPB_CREATED_NO_MEM;\n\t}\n\n\tqp_list_add_entry(&qp_broker_list, &entry->qp);\n\tif (ent != NULL)\n\t\t*ent = entry;\n\n\t \n\tresult = vmci_resource_add(&entry->resource,\n\t\t\t\t   VMCI_RESOURCE_TYPE_QPAIR_HOST,\n\t\t\t\t   handle);\n\tif (result != VMCI_SUCCESS) {\n\t\tpr_warn(\"Failed to add new resource (handle=0x%x:0x%x), error: %d\",\n\t\t\thandle.context, handle.resource, result);\n\t\tgoto error;\n\t}\n\n\tentry->qp.handle = vmci_resource_handle(&entry->resource);\n\tif (is_local) {\n\t\tvmci_q_header_init(entry->produce_q->q_header,\n\t\t\t\t   entry->qp.handle);\n\t\tvmci_q_header_init(entry->consume_q->q_header,\n\t\t\t\t   entry->qp.handle);\n\t}\n\n\tvmci_ctx_qp_create(context, entry->qp.handle);\n\n\treturn VMCI_SUCCESS;\n\n error:\n\tif (entry != NULL) {\n\t\tqp_host_free_queue(entry->produce_q, guest_produce_size);\n\t\tqp_host_free_queue(entry->consume_q, guest_consume_size);\n\t\tkfree(entry);\n\t}\n\n\treturn result;\n}\n\n \nstatic int qp_notify_peer(bool attach,\n\t\t\t  struct vmci_handle handle,\n\t\t\t  u32 my_id,\n\t\t\t  u32 peer_id)\n{\n\tint rv;\n\tstruct vmci_event_qp ev;\n\n\tif (vmci_handle_is_invalid(handle) || my_id == VMCI_INVALID_ID ||\n\t    peer_id == VMCI_INVALID_ID)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\t \n\n\tmemset(&ev, 0, sizeof(ev));\n\tev.msg.hdr.dst = vmci_make_handle(peer_id, VMCI_EVENT_HANDLER);\n\tev.msg.hdr.src = vmci_make_handle(VMCI_HYPERVISOR_CONTEXT_ID,\n\t\t\t\t\t  VMCI_CONTEXT_RESOURCE_ID);\n\tev.msg.hdr.payload_size = sizeof(ev) - sizeof(ev.msg.hdr);\n\tev.msg.event_data.event = attach ?\n\t    VMCI_EVENT_QP_PEER_ATTACH : VMCI_EVENT_QP_PEER_DETACH;\n\tev.payload.handle = handle;\n\tev.payload.peer_id = my_id;\n\n\trv = vmci_datagram_dispatch(VMCI_HYPERVISOR_CONTEXT_ID,\n\t\t\t\t    &ev.msg.hdr, false);\n\tif (rv < VMCI_SUCCESS)\n\t\tpr_warn(\"Failed to enqueue queue_pair %s event datagram for context (ID=0x%x)\\n\",\n\t\t\tattach ? \"ATTACH\" : \"DETACH\", peer_id);\n\n\treturn rv;\n}\n\n \nstatic int qp_broker_attach(struct qp_broker_entry *entry,\n\t\t\t    u32 peer,\n\t\t\t    u32 flags,\n\t\t\t    u32 priv_flags,\n\t\t\t    u64 produce_size,\n\t\t\t    u64 consume_size,\n\t\t\t    struct vmci_qp_page_store *page_store,\n\t\t\t    struct vmci_ctx *context,\n\t\t\t    vmci_event_release_cb wakeup_cb,\n\t\t\t    void *client_data,\n\t\t\t    struct qp_broker_entry **ent)\n{\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tbool is_local = flags & VMCI_QPFLAG_LOCAL;\n\tint result;\n\n\tif (entry->state != VMCIQPB_CREATED_NO_MEM &&\n\t    entry->state != VMCIQPB_CREATED_MEM)\n\t\treturn VMCI_ERROR_UNAVAILABLE;\n\n\tif (is_local) {\n\t\tif (!(entry->qp.flags & VMCI_QPFLAG_LOCAL) ||\n\t\t    context_id != entry->create_id) {\n\t\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t\t}\n\t} else if (context_id == entry->create_id ||\n\t\t   context_id == entry->attach_id) {\n\t\treturn VMCI_ERROR_ALREADY_EXISTS;\n\t}\n\n\tif (VMCI_CONTEXT_IS_VM(context_id) &&\n\t    VMCI_CONTEXT_IS_VM(entry->create_id))\n\t\treturn VMCI_ERROR_DST_UNREACHABLE;\n\n\t \n\tif ((context->priv_flags & VMCI_PRIVILEGE_FLAG_RESTRICTED) &&\n\t    !entry->created_by_trusted)\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\t \n\tif (entry->require_trusted_attach &&\n\t    (!(priv_flags & VMCI_PRIVILEGE_FLAG_TRUSTED)))\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\t \n\tif (entry->qp.peer != VMCI_INVALID_ID && entry->qp.peer != context_id)\n\t\treturn VMCI_ERROR_NO_ACCESS;\n\n\tif (entry->create_id == VMCI_HOST_CONTEXT_ID) {\n\t\t \n\n\t\tif (!vmci_ctx_supports_host_qp(context))\n\t\t\treturn VMCI_ERROR_INVALID_RESOURCE;\n\n\t} else if (context_id == VMCI_HOST_CONTEXT_ID) {\n\t\tstruct vmci_ctx *create_context;\n\t\tbool supports_host_qp;\n\n\t\t \n\n\t\tcreate_context = vmci_ctx_get(entry->create_id);\n\t\tsupports_host_qp = vmci_ctx_supports_host_qp(create_context);\n\t\tvmci_ctx_put(create_context);\n\n\t\tif (!supports_host_qp)\n\t\t\treturn VMCI_ERROR_INVALID_RESOURCE;\n\t}\n\n\tif ((entry->qp.flags & ~VMCI_QP_ASYMM) != (flags & ~VMCI_QP_ASYMM_PEER))\n\t\treturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\n\n\tif (context_id != VMCI_HOST_CONTEXT_ID) {\n\t\t \n\n\t\tif (entry->qp.produce_size != produce_size ||\n\t\t    entry->qp.consume_size != consume_size) {\n\t\t\treturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\n\t\t}\n\t} else if (entry->qp.produce_size != consume_size ||\n\t\t   entry->qp.consume_size != produce_size) {\n\t\treturn VMCI_ERROR_QUEUEPAIR_MISMATCH;\n\t}\n\n\tif (context_id != VMCI_HOST_CONTEXT_ID) {\n\t\t \n\n\t\tif (entry->state != VMCIQPB_CREATED_NO_MEM)\n\t\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\t\tif (page_store != NULL) {\n\t\t\t \n\n\t\t\tresult = qp_host_register_user_memory(page_store,\n\t\t\t\t\t\t\t      entry->produce_q,\n\t\t\t\t\t\t\t      entry->consume_q);\n\t\t\tif (result < VMCI_SUCCESS)\n\t\t\t\treturn result;\n\n\t\t\tentry->state = VMCIQPB_ATTACHED_MEM;\n\t\t} else {\n\t\t\tentry->state = VMCIQPB_ATTACHED_NO_MEM;\n\t\t}\n\t} else if (entry->state == VMCIQPB_CREATED_NO_MEM) {\n\t\t \n\n\t\treturn VMCI_ERROR_UNAVAILABLE;\n\t} else {\n\t\t \n\t\tentry->state = VMCIQPB_ATTACHED_MEM;\n\t}\n\n\tif (entry->state == VMCIQPB_ATTACHED_MEM) {\n\t\tresult =\n\t\t    qp_notify_peer(true, entry->qp.handle, context_id,\n\t\t\t\t   entry->create_id);\n\t\tif (result < VMCI_SUCCESS)\n\t\t\tpr_warn(\"Failed to notify peer (ID=0x%x) of attach to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t\tentry->create_id, entry->qp.handle.context,\n\t\t\t\tentry->qp.handle.resource);\n\t}\n\n\tentry->attach_id = context_id;\n\tentry->qp.ref_count++;\n\tif (wakeup_cb) {\n\t\tentry->wakeup_cb = wakeup_cb;\n\t\tentry->client_data = client_data;\n\t}\n\n\t \n\tif (!is_local)\n\t\tvmci_ctx_qp_create(context, entry->qp.handle);\n\n\tif (ent != NULL)\n\t\t*ent = entry;\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic int qp_broker_alloc(struct vmci_handle handle,\n\t\t\t   u32 peer,\n\t\t\t   u32 flags,\n\t\t\t   u32 priv_flags,\n\t\t\t   u64 produce_size,\n\t\t\t   u64 consume_size,\n\t\t\t   struct vmci_qp_page_store *page_store,\n\t\t\t   struct vmci_ctx *context,\n\t\t\t   vmci_event_release_cb wakeup_cb,\n\t\t\t   void *client_data,\n\t\t\t   struct qp_broker_entry **ent,\n\t\t\t   bool *swap)\n{\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tbool create;\n\tstruct qp_broker_entry *entry = NULL;\n\tbool is_local = flags & VMCI_QPFLAG_LOCAL;\n\tint result;\n\n\tif (vmci_handle_is_invalid(handle) ||\n\t    (flags & ~VMCI_QP_ALL_FLAGS) || is_local ||\n\t    !(produce_size || consume_size) ||\n\t    !context || context_id == VMCI_INVALID_ID ||\n\t    handle.context == VMCI_INVALID_ID) {\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t}\n\n\tif (page_store && !VMCI_QP_PAGESTORE_IS_WELLFORMED(page_store))\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\t \n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\tif (!is_local && vmci_ctx_qp_exists(context, handle)) {\n\t\tpr_devel(\"Context (ID=0x%x) already attached to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tmutex_unlock(&qp_broker_list.mutex);\n\t\treturn VMCI_ERROR_ALREADY_EXISTS;\n\t}\n\n\tif (handle.resource != VMCI_INVALID_ID)\n\t\tentry = qp_broker_handle_to_entry(handle);\n\n\tif (!entry) {\n\t\tcreate = true;\n\t\tresult =\n\t\t    qp_broker_create(handle, peer, flags, priv_flags,\n\t\t\t\t     produce_size, consume_size, page_store,\n\t\t\t\t     context, wakeup_cb, client_data, ent);\n\t} else {\n\t\tcreate = false;\n\t\tresult =\n\t\t    qp_broker_attach(entry, peer, flags, priv_flags,\n\t\t\t\t     produce_size, consume_size, page_store,\n\t\t\t\t     context, wakeup_cb, client_data, ent);\n\t}\n\n\tmutex_unlock(&qp_broker_list.mutex);\n\n\tif (swap)\n\t\t*swap = (context_id == VMCI_HOST_CONTEXT_ID) &&\n\t\t    !(create && is_local);\n\n\treturn result;\n}\n\n \nstatic int qp_alloc_host_work(struct vmci_handle *handle,\n\t\t\t      struct vmci_queue **produce_q,\n\t\t\t      u64 produce_size,\n\t\t\t      struct vmci_queue **consume_q,\n\t\t\t      u64 consume_size,\n\t\t\t      u32 peer,\n\t\t\t      u32 flags,\n\t\t\t      u32 priv_flags,\n\t\t\t      vmci_event_release_cb wakeup_cb,\n\t\t\t      void *client_data)\n{\n\tstruct vmci_handle new_handle;\n\tstruct vmci_ctx *context;\n\tstruct qp_broker_entry *entry;\n\tint result;\n\tbool swap;\n\n\tif (vmci_handle_is_invalid(*handle)) {\n\t\tnew_handle = vmci_make_handle(\n\t\t\tVMCI_HOST_CONTEXT_ID, VMCI_INVALID_ID);\n\t} else\n\t\tnew_handle = *handle;\n\n\tcontext = vmci_ctx_get(VMCI_HOST_CONTEXT_ID);\n\tentry = NULL;\n\tresult =\n\t    qp_broker_alloc(new_handle, peer, flags, priv_flags,\n\t\t\t    produce_size, consume_size, NULL, context,\n\t\t\t    wakeup_cb, client_data, &entry, &swap);\n\tif (result == VMCI_SUCCESS) {\n\t\tif (swap) {\n\t\t\t \n\n\t\t\t*produce_q = entry->consume_q;\n\t\t\t*consume_q = entry->produce_q;\n\t\t} else {\n\t\t\t*produce_q = entry->produce_q;\n\t\t\t*consume_q = entry->consume_q;\n\t\t}\n\n\t\t*handle = vmci_resource_handle(&entry->resource);\n\t} else {\n\t\t*handle = VMCI_INVALID_HANDLE;\n\t\tpr_devel(\"queue pair broker failed to alloc (result=%d)\\n\",\n\t\t\t result);\n\t}\n\tvmci_ctx_put(context);\n\treturn result;\n}\n\n \nint vmci_qp_alloc(struct vmci_handle *handle,\n\t\t  struct vmci_queue **produce_q,\n\t\t  u64 produce_size,\n\t\t  struct vmci_queue **consume_q,\n\t\t  u64 consume_size,\n\t\t  u32 peer,\n\t\t  u32 flags,\n\t\t  u32 priv_flags,\n\t\t  bool guest_endpoint,\n\t\t  vmci_event_release_cb wakeup_cb,\n\t\t  void *client_data)\n{\n\tif (!handle || !produce_q || !consume_q ||\n\t    (!produce_size && !consume_size) || (flags & ~VMCI_QP_ALL_FLAGS))\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tif (guest_endpoint) {\n\t\treturn qp_alloc_guest_work(handle, produce_q,\n\t\t\t\t\t   produce_size, consume_q,\n\t\t\t\t\t   consume_size, peer,\n\t\t\t\t\t   flags, priv_flags);\n\t} else {\n\t\treturn qp_alloc_host_work(handle, produce_q,\n\t\t\t\t\t  produce_size, consume_q,\n\t\t\t\t\t  consume_size, peer, flags,\n\t\t\t\t\t  priv_flags, wakeup_cb, client_data);\n\t}\n}\n\n \nstatic int qp_detatch_host_work(struct vmci_handle handle)\n{\n\tint result;\n\tstruct vmci_ctx *context;\n\n\tcontext = vmci_ctx_get(VMCI_HOST_CONTEXT_ID);\n\n\tresult = vmci_qp_broker_detach(handle, context);\n\n\tvmci_ctx_put(context);\n\treturn result;\n}\n\n \nstatic int qp_detatch(struct vmci_handle handle, bool guest_endpoint)\n{\n\tif (vmci_handle_is_invalid(handle))\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tif (guest_endpoint)\n\t\treturn qp_detatch_guest_work(handle);\n\telse\n\t\treturn qp_detatch_host_work(handle);\n}\n\n \nstatic struct qp_entry *qp_list_get_head(struct qp_list *qp_list)\n{\n\tif (!list_empty(&qp_list->head)) {\n\t\tstruct qp_entry *entry =\n\t\t    list_first_entry(&qp_list->head, struct qp_entry,\n\t\t\t\t     list_item);\n\t\treturn entry;\n\t}\n\n\treturn NULL;\n}\n\nvoid vmci_qp_broker_exit(void)\n{\n\tstruct qp_entry *entry;\n\tstruct qp_broker_entry *be;\n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\twhile ((entry = qp_list_get_head(&qp_broker_list))) {\n\t\tbe = (struct qp_broker_entry *)entry;\n\n\t\tqp_list_remove_entry(&qp_broker_list, entry);\n\t\tkfree(be);\n\t}\n\n\tmutex_unlock(&qp_broker_list.mutex);\n}\n\n \nint vmci_qp_broker_alloc(struct vmci_handle handle,\n\t\t\t u32 peer,\n\t\t\t u32 flags,\n\t\t\t u32 priv_flags,\n\t\t\t u64 produce_size,\n\t\t\t u64 consume_size,\n\t\t\t struct vmci_qp_page_store *page_store,\n\t\t\t struct vmci_ctx *context)\n{\n\tif (!QP_SIZES_ARE_VALID(produce_size, consume_size))\n\t\treturn VMCI_ERROR_NO_RESOURCES;\n\n\treturn qp_broker_alloc(handle, peer, flags, priv_flags,\n\t\t\t       produce_size, consume_size,\n\t\t\t       page_store, context, NULL, NULL, NULL, NULL);\n}\n\n \nint vmci_qp_broker_set_page_store(struct vmci_handle handle,\n\t\t\t\t  u64 produce_uva,\n\t\t\t\t  u64 consume_uva,\n\t\t\t\t  struct vmci_ctx *context)\n{\n\tstruct qp_broker_entry *entry;\n\tint result;\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\n\tif (vmci_handle_is_invalid(handle) || !context ||\n\t    context_id == VMCI_INVALID_ID)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\t \n\n\tif (produce_uva == 0 || consume_uva == 0)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\tif (!vmci_ctx_qp_exists(context, handle)) {\n\t\tpr_warn(\"Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\tcontext_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tentry = qp_broker_handle_to_entry(handle);\n\tif (!entry) {\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\t \n\tif (entry->create_id != context_id &&\n\t    (entry->create_id != VMCI_HOST_CONTEXT_ID ||\n\t     entry->attach_id != context_id)) {\n\t\tresult = VMCI_ERROR_QUEUEPAIR_NOTOWNER;\n\t\tgoto out;\n\t}\n\n\tif (entry->state != VMCIQPB_CREATED_NO_MEM &&\n\t    entry->state != VMCIQPB_ATTACHED_NO_MEM) {\n\t\tresult = VMCI_ERROR_UNAVAILABLE;\n\t\tgoto out;\n\t}\n\n\tresult = qp_host_get_user_memory(produce_uva, consume_uva,\n\t\t\t\t\t entry->produce_q, entry->consume_q);\n\tif (result < VMCI_SUCCESS)\n\t\tgoto out;\n\n\tresult = qp_host_map_queues(entry->produce_q, entry->consume_q);\n\tif (result < VMCI_SUCCESS) {\n\t\tqp_host_unregister_user_memory(entry->produce_q,\n\t\t\t\t\t       entry->consume_q);\n\t\tgoto out;\n\t}\n\n\tif (entry->state == VMCIQPB_CREATED_NO_MEM)\n\t\tentry->state = VMCIQPB_CREATED_MEM;\n\telse\n\t\tentry->state = VMCIQPB_ATTACHED_MEM;\n\n\tentry->vmci_page_files = true;\n\n\tif (entry->state == VMCIQPB_ATTACHED_MEM) {\n\t\tresult =\n\t\t    qp_notify_peer(true, handle, context_id, entry->create_id);\n\t\tif (result < VMCI_SUCCESS) {\n\t\t\tpr_warn(\"Failed to notify peer (ID=0x%x) of attach to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t\tentry->create_id, entry->qp.handle.context,\n\t\t\t\tentry->qp.handle.resource);\n\t\t}\n\t}\n\n\tresult = VMCI_SUCCESS;\n out:\n\tmutex_unlock(&qp_broker_list.mutex);\n\treturn result;\n}\n\n \nstatic void qp_reset_saved_headers(struct qp_broker_entry *entry)\n{\n\tentry->produce_q->saved_header = NULL;\n\tentry->consume_q->saved_header = NULL;\n}\n\n \nint vmci_qp_broker_detach(struct vmci_handle handle, struct vmci_ctx *context)\n{\n\tstruct qp_broker_entry *entry;\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tu32 peer_id;\n\tbool is_local = false;\n\tint result;\n\n\tif (vmci_handle_is_invalid(handle) || !context ||\n\t    context_id == VMCI_INVALID_ID) {\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t}\n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\tif (!vmci_ctx_qp_exists(context, handle)) {\n\t\tpr_devel(\"Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tentry = qp_broker_handle_to_entry(handle);\n\tif (!entry) {\n\t\tpr_devel(\"Context (ID=0x%x) reports being attached to queue pair(handle=0x%x:0x%x) that isn't present in broker\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tif (context_id != entry->create_id && context_id != entry->attach_id) {\n\t\tresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\n\t\tgoto out;\n\t}\n\n\tif (context_id == entry->create_id) {\n\t\tpeer_id = entry->attach_id;\n\t\tentry->create_id = VMCI_INVALID_ID;\n\t} else {\n\t\tpeer_id = entry->create_id;\n\t\tentry->attach_id = VMCI_INVALID_ID;\n\t}\n\tentry->qp.ref_count--;\n\n\tis_local = entry->qp.flags & VMCI_QPFLAG_LOCAL;\n\n\tif (context_id != VMCI_HOST_CONTEXT_ID) {\n\t\tbool headers_mapped;\n\n\t\t \n\n\t\tqp_acquire_queue_mutex(entry->produce_q);\n\t\theaders_mapped = entry->produce_q->q_header ||\n\t\t    entry->consume_q->q_header;\n\t\tif (QPBROKERSTATE_HAS_MEM(entry)) {\n\t\t\tresult =\n\t\t\t    qp_host_unmap_queues(INVALID_VMCI_GUEST_MEM_ID,\n\t\t\t\t\t\t entry->produce_q,\n\t\t\t\t\t\t entry->consume_q);\n\t\t\tif (result < VMCI_SUCCESS)\n\t\t\t\tpr_warn(\"Failed to unmap queue headers for queue pair (handle=0x%x:0x%x,result=%d)\\n\",\n\t\t\t\t\thandle.context, handle.resource,\n\t\t\t\t\tresult);\n\n\t\t\tqp_host_unregister_user_memory(entry->produce_q,\n\t\t\t\t\t\t       entry->consume_q);\n\n\t\t}\n\n\t\tif (!headers_mapped)\n\t\t\tqp_reset_saved_headers(entry);\n\n\t\tqp_release_queue_mutex(entry->produce_q);\n\n\t\tif (!headers_mapped && entry->wakeup_cb)\n\t\t\tentry->wakeup_cb(entry->client_data);\n\n\t} else {\n\t\tif (entry->wakeup_cb) {\n\t\t\tentry->wakeup_cb = NULL;\n\t\t\tentry->client_data = NULL;\n\t\t}\n\t}\n\n\tif (entry->qp.ref_count == 0) {\n\t\tqp_list_remove_entry(&qp_broker_list, &entry->qp);\n\n\t\tif (is_local)\n\t\t\tkfree(entry->local_mem);\n\n\t\tqp_cleanup_queue_mutex(entry->produce_q, entry->consume_q);\n\t\tqp_host_free_queue(entry->produce_q, entry->qp.produce_size);\n\t\tqp_host_free_queue(entry->consume_q, entry->qp.consume_size);\n\t\t \n\t\tvmci_resource_remove(&entry->resource);\n\n\t\tkfree(entry);\n\n\t\tvmci_ctx_qp_destroy(context, handle);\n\t} else {\n\t\tqp_notify_peer(false, handle, context_id, peer_id);\n\t\tif (context_id == VMCI_HOST_CONTEXT_ID &&\n\t\t    QPBROKERSTATE_HAS_MEM(entry)) {\n\t\t\tentry->state = VMCIQPB_SHUTDOWN_MEM;\n\t\t} else {\n\t\t\tentry->state = VMCIQPB_SHUTDOWN_NO_MEM;\n\t\t}\n\n\t\tif (!is_local)\n\t\t\tvmci_ctx_qp_destroy(context, handle);\n\n\t}\n\tresult = VMCI_SUCCESS;\n out:\n\tmutex_unlock(&qp_broker_list.mutex);\n\treturn result;\n}\n\n \nint vmci_qp_broker_map(struct vmci_handle handle,\n\t\t       struct vmci_ctx *context,\n\t\t       u64 guest_mem)\n{\n\tstruct qp_broker_entry *entry;\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tint result;\n\n\tif (vmci_handle_is_invalid(handle) || !context ||\n\t    context_id == VMCI_INVALID_ID)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\tif (!vmci_ctx_qp_exists(context, handle)) {\n\t\tpr_devel(\"Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tentry = qp_broker_handle_to_entry(handle);\n\tif (!entry) {\n\t\tpr_devel(\"Context (ID=0x%x) reports being attached to queue pair (handle=0x%x:0x%x) that isn't present in broker\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tif (context_id != entry->create_id && context_id != entry->attach_id) {\n\t\tresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\n\t\tgoto out;\n\t}\n\n\tresult = VMCI_SUCCESS;\n\n\tif (context_id != VMCI_HOST_CONTEXT_ID &&\n\t    !QPBROKERSTATE_HAS_MEM(entry)) {\n\t\tstruct vmci_qp_page_store page_store;\n\n\t\tpage_store.pages = guest_mem;\n\t\tpage_store.len = QPE_NUM_PAGES(entry->qp);\n\n\t\tqp_acquire_queue_mutex(entry->produce_q);\n\t\tqp_reset_saved_headers(entry);\n\t\tresult =\n\t\t    qp_host_register_user_memory(&page_store,\n\t\t\t\t\t\t entry->produce_q,\n\t\t\t\t\t\t entry->consume_q);\n\t\tqp_release_queue_mutex(entry->produce_q);\n\t\tif (result == VMCI_SUCCESS) {\n\t\t\t \n\n\t\t\tentry->state++;\n\n\t\t\tif (entry->wakeup_cb)\n\t\t\t\tentry->wakeup_cb(entry->client_data);\n\t\t}\n\t}\n\n out:\n\tmutex_unlock(&qp_broker_list.mutex);\n\treturn result;\n}\n\n \nstatic int qp_save_headers(struct qp_broker_entry *entry)\n{\n\tint result;\n\n\tif (entry->produce_q->saved_header != NULL &&\n\t    entry->consume_q->saved_header != NULL) {\n\t\t \n\n\t\treturn VMCI_SUCCESS;\n\t}\n\n\tif (NULL == entry->produce_q->q_header ||\n\t    NULL == entry->consume_q->q_header) {\n\t\tresult = qp_host_map_queues(entry->produce_q, entry->consume_q);\n\t\tif (result < VMCI_SUCCESS)\n\t\t\treturn result;\n\t}\n\n\tmemcpy(&entry->saved_produce_q, entry->produce_q->q_header,\n\t       sizeof(entry->saved_produce_q));\n\tentry->produce_q->saved_header = &entry->saved_produce_q;\n\tmemcpy(&entry->saved_consume_q, entry->consume_q->q_header,\n\t       sizeof(entry->saved_consume_q));\n\tentry->consume_q->saved_header = &entry->saved_consume_q;\n\n\treturn VMCI_SUCCESS;\n}\n\n \nint vmci_qp_broker_unmap(struct vmci_handle handle,\n\t\t\t struct vmci_ctx *context,\n\t\t\t u32 gid)\n{\n\tstruct qp_broker_entry *entry;\n\tconst u32 context_id = vmci_ctx_get_id(context);\n\tint result;\n\n\tif (vmci_handle_is_invalid(handle) || !context ||\n\t    context_id == VMCI_INVALID_ID)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tmutex_lock(&qp_broker_list.mutex);\n\n\tif (!vmci_ctx_qp_exists(context, handle)) {\n\t\tpr_devel(\"Context (ID=0x%x) not attached to queue pair (handle=0x%x:0x%x)\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tentry = qp_broker_handle_to_entry(handle);\n\tif (!entry) {\n\t\tpr_devel(\"Context (ID=0x%x) reports being attached to queue pair (handle=0x%x:0x%x) that isn't present in broker\\n\",\n\t\t\t context_id, handle.context, handle.resource);\n\t\tresult = VMCI_ERROR_NOT_FOUND;\n\t\tgoto out;\n\t}\n\n\tif (context_id != entry->create_id && context_id != entry->attach_id) {\n\t\tresult = VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\n\t\tgoto out;\n\t}\n\n\tif (context_id != VMCI_HOST_CONTEXT_ID &&\n\t    QPBROKERSTATE_HAS_MEM(entry)) {\n\t\tqp_acquire_queue_mutex(entry->produce_q);\n\t\tresult = qp_save_headers(entry);\n\t\tif (result < VMCI_SUCCESS)\n\t\t\tpr_warn(\"Failed to save queue headers for queue pair (handle=0x%x:0x%x,result=%d)\\n\",\n\t\t\t\thandle.context, handle.resource, result);\n\n\t\tqp_host_unmap_queues(gid, entry->produce_q, entry->consume_q);\n\n\t\t \n\t\tqp_host_unregister_user_memory(entry->produce_q,\n\t\t\t\t\t       entry->consume_q);\n\n\t\t \n\t\tentry->state--;\n\n\t\tqp_release_queue_mutex(entry->produce_q);\n\t}\n\n\tresult = VMCI_SUCCESS;\n\n out:\n\tmutex_unlock(&qp_broker_list.mutex);\n\treturn result;\n}\n\n \nvoid vmci_qp_guest_endpoints_exit(void)\n{\n\tstruct qp_entry *entry;\n\tstruct qp_guest_endpoint *ep;\n\n\tmutex_lock(&qp_guest_endpoints.mutex);\n\n\twhile ((entry = qp_list_get_head(&qp_guest_endpoints))) {\n\t\tep = (struct qp_guest_endpoint *)entry;\n\n\t\t \n\t\tif (!(entry->flags & VMCI_QPFLAG_LOCAL))\n\t\t\tqp_detatch_hypercall(entry->handle);\n\n\t\t \n\t\tentry->ref_count = 0;\n\t\tqp_list_remove_entry(&qp_guest_endpoints, entry);\n\n\t\tqp_guest_endpoint_destroy(ep);\n\t}\n\n\tmutex_unlock(&qp_guest_endpoints.mutex);\n}\n\n \nstatic void qp_lock(const struct vmci_qp *qpair)\n{\n\tqp_acquire_queue_mutex(qpair->produce_q);\n}\n\n \nstatic void qp_unlock(const struct vmci_qp *qpair)\n{\n\tqp_release_queue_mutex(qpair->produce_q);\n}\n\n \nstatic int qp_map_queue_headers(struct vmci_queue *produce_q,\n\t\t\t\tstruct vmci_queue *consume_q)\n{\n\tint result;\n\n\tif (NULL == produce_q->q_header || NULL == consume_q->q_header) {\n\t\tresult = qp_host_map_queues(produce_q, consume_q);\n\t\tif (result < VMCI_SUCCESS)\n\t\t\treturn (produce_q->saved_header &&\n\t\t\t\tconsume_q->saved_header) ?\n\t\t\t    VMCI_ERROR_QUEUEPAIR_NOT_READY :\n\t\t\t    VMCI_ERROR_QUEUEPAIR_NOTATTACHED;\n\t}\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic int qp_get_queue_headers(const struct vmci_qp *qpair,\n\t\t\t\tstruct vmci_queue_header **produce_q_header,\n\t\t\t\tstruct vmci_queue_header **consume_q_header)\n{\n\tint result;\n\n\tresult = qp_map_queue_headers(qpair->produce_q, qpair->consume_q);\n\tif (result == VMCI_SUCCESS) {\n\t\t*produce_q_header = qpair->produce_q->q_header;\n\t\t*consume_q_header = qpair->consume_q->q_header;\n\t} else if (qpair->produce_q->saved_header &&\n\t\t   qpair->consume_q->saved_header) {\n\t\t*produce_q_header = qpair->produce_q->saved_header;\n\t\t*consume_q_header = qpair->consume_q->saved_header;\n\t\tresult = VMCI_SUCCESS;\n\t}\n\n\treturn result;\n}\n\n \nstatic int qp_wakeup_cb(void *client_data)\n{\n\tstruct vmci_qp *qpair = (struct vmci_qp *)client_data;\n\n\tqp_lock(qpair);\n\twhile (qpair->blocked > 0) {\n\t\tqpair->blocked--;\n\t\tqpair->generation++;\n\t\twake_up(&qpair->event);\n\t}\n\tqp_unlock(qpair);\n\n\treturn VMCI_SUCCESS;\n}\n\n \nstatic bool qp_wait_for_ready_queue(struct vmci_qp *qpair)\n{\n\tunsigned int generation;\n\n\tqpair->blocked++;\n\tgeneration = qpair->generation;\n\tqp_unlock(qpair);\n\twait_event(qpair->event, generation != qpair->generation);\n\tqp_lock(qpair);\n\n\treturn true;\n}\n\n \nstatic ssize_t qp_enqueue_locked(struct vmci_queue *produce_q,\n\t\t\t\t struct vmci_queue *consume_q,\n\t\t\t\t const u64 produce_q_size,\n\t\t\t\t struct iov_iter *from)\n{\n\ts64 free_space;\n\tu64 tail;\n\tsize_t buf_size = iov_iter_count(from);\n\tsize_t written;\n\tssize_t result;\n\n\tresult = qp_map_queue_headers(produce_q, consume_q);\n\tif (unlikely(result != VMCI_SUCCESS))\n\t\treturn result;\n\n\tfree_space = vmci_q_header_free_space(produce_q->q_header,\n\t\t\t\t\t      consume_q->q_header,\n\t\t\t\t\t      produce_q_size);\n\tif (free_space == 0)\n\t\treturn VMCI_ERROR_QUEUEPAIR_NOSPACE;\n\n\tif (free_space < VMCI_SUCCESS)\n\t\treturn (ssize_t) free_space;\n\n\twritten = (size_t) (free_space > buf_size ? buf_size : free_space);\n\ttail = vmci_q_header_producer_tail(produce_q->q_header);\n\tif (likely(tail + written < produce_q_size)) {\n\t\tresult = qp_memcpy_to_queue_iter(produce_q, tail, from, written);\n\t} else {\n\t\t \n\n\t\tconst size_t tmp = (size_t) (produce_q_size - tail);\n\n\t\tresult = qp_memcpy_to_queue_iter(produce_q, tail, from, tmp);\n\t\tif (result >= VMCI_SUCCESS)\n\t\t\tresult = qp_memcpy_to_queue_iter(produce_q, 0, from,\n\t\t\t\t\t\t written - tmp);\n\t}\n\n\tif (result < VMCI_SUCCESS)\n\t\treturn result;\n\n\t \n\tvirt_wmb();\n\n\tvmci_q_header_add_producer_tail(produce_q->q_header, written,\n\t\t\t\t\tproduce_q_size);\n\treturn written;\n}\n\n \nstatic ssize_t qp_dequeue_locked(struct vmci_queue *produce_q,\n\t\t\t\t struct vmci_queue *consume_q,\n\t\t\t\t const u64 consume_q_size,\n\t\t\t\t struct iov_iter *to,\n\t\t\t\t bool update_consumer)\n{\n\tsize_t buf_size = iov_iter_count(to);\n\ts64 buf_ready;\n\tu64 head;\n\tsize_t read;\n\tssize_t result;\n\n\tresult = qp_map_queue_headers(produce_q, consume_q);\n\tif (unlikely(result != VMCI_SUCCESS))\n\t\treturn result;\n\n\tbuf_ready = vmci_q_header_buf_ready(consume_q->q_header,\n\t\t\t\t\t    produce_q->q_header,\n\t\t\t\t\t    consume_q_size);\n\tif (buf_ready == 0)\n\t\treturn VMCI_ERROR_QUEUEPAIR_NODATA;\n\n\tif (buf_ready < VMCI_SUCCESS)\n\t\treturn (ssize_t) buf_ready;\n\n\t \n\tvirt_rmb();\n\n\tread = (size_t) (buf_ready > buf_size ? buf_size : buf_ready);\n\thead = vmci_q_header_consumer_head(produce_q->q_header);\n\tif (likely(head + read < consume_q_size)) {\n\t\tresult = qp_memcpy_from_queue_iter(to, consume_q, head, read);\n\t} else {\n\t\t \n\n\t\tconst size_t tmp = (size_t) (consume_q_size - head);\n\n\t\tresult = qp_memcpy_from_queue_iter(to, consume_q, head, tmp);\n\t\tif (result >= VMCI_SUCCESS)\n\t\t\tresult = qp_memcpy_from_queue_iter(to, consume_q, 0,\n\t\t\t\t\t\t   read - tmp);\n\n\t}\n\n\tif (result < VMCI_SUCCESS)\n\t\treturn result;\n\n\tif (update_consumer)\n\t\tvmci_q_header_add_consumer_head(produce_q->q_header,\n\t\t\t\t\t\tread, consume_q_size);\n\n\treturn read;\n}\n\n \nint vmci_qpair_alloc(struct vmci_qp **qpair,\n\t\t     struct vmci_handle *handle,\n\t\t     u64 produce_qsize,\n\t\t     u64 consume_qsize,\n\t\t     u32 peer,\n\t\t     u32 flags,\n\t\t     u32 priv_flags)\n{\n\tstruct vmci_qp *my_qpair;\n\tint retval;\n\tstruct vmci_handle src = VMCI_INVALID_HANDLE;\n\tstruct vmci_handle dst = vmci_make_handle(peer, VMCI_INVALID_ID);\n\tenum vmci_route route;\n\tvmci_event_release_cb wakeup_cb;\n\tvoid *client_data;\n\n\t \n\n\tif (!QP_SIZES_ARE_VALID(produce_qsize, consume_qsize))\n\t\treturn VMCI_ERROR_NO_RESOURCES;\n\n\tretval = vmci_route(&src, &dst, false, &route);\n\tif (retval < VMCI_SUCCESS)\n\t\troute = vmci_guest_code_active() ?\n\t\t    VMCI_ROUTE_AS_GUEST : VMCI_ROUTE_AS_HOST;\n\n\tif (flags & (VMCI_QPFLAG_NONBLOCK | VMCI_QPFLAG_PINNED)) {\n\t\tpr_devel(\"NONBLOCK OR PINNED set\");\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\t}\n\n\tmy_qpair = kzalloc(sizeof(*my_qpair), GFP_KERNEL);\n\tif (!my_qpair)\n\t\treturn VMCI_ERROR_NO_MEM;\n\n\tmy_qpair->produce_q_size = produce_qsize;\n\tmy_qpair->consume_q_size = consume_qsize;\n\tmy_qpair->peer = peer;\n\tmy_qpair->flags = flags;\n\tmy_qpair->priv_flags = priv_flags;\n\n\twakeup_cb = NULL;\n\tclient_data = NULL;\n\n\tif (VMCI_ROUTE_AS_HOST == route) {\n\t\tmy_qpair->guest_endpoint = false;\n\t\tif (!(flags & VMCI_QPFLAG_LOCAL)) {\n\t\t\tmy_qpair->blocked = 0;\n\t\t\tmy_qpair->generation = 0;\n\t\t\tinit_waitqueue_head(&my_qpair->event);\n\t\t\twakeup_cb = qp_wakeup_cb;\n\t\t\tclient_data = (void *)my_qpair;\n\t\t}\n\t} else {\n\t\tmy_qpair->guest_endpoint = true;\n\t}\n\n\tretval = vmci_qp_alloc(handle,\n\t\t\t       &my_qpair->produce_q,\n\t\t\t       my_qpair->produce_q_size,\n\t\t\t       &my_qpair->consume_q,\n\t\t\t       my_qpair->consume_q_size,\n\t\t\t       my_qpair->peer,\n\t\t\t       my_qpair->flags,\n\t\t\t       my_qpair->priv_flags,\n\t\t\t       my_qpair->guest_endpoint,\n\t\t\t       wakeup_cb, client_data);\n\n\tif (retval < VMCI_SUCCESS) {\n\t\tkfree(my_qpair);\n\t\treturn retval;\n\t}\n\n\t*qpair = my_qpair;\n\tmy_qpair->handle = *handle;\n\n\treturn retval;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_alloc);\n\n \nint vmci_qpair_detach(struct vmci_qp **qpair)\n{\n\tint result;\n\tstruct vmci_qp *old_qpair;\n\n\tif (!qpair || !(*qpair))\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\told_qpair = *qpair;\n\tresult = qp_detatch(old_qpair->handle, old_qpair->guest_endpoint);\n\n\t \n\n\tmemset(old_qpair, 0, sizeof(*old_qpair));\n\told_qpair->handle = VMCI_INVALID_HANDLE;\n\told_qpair->peer = VMCI_INVALID_ID;\n\tkfree(old_qpair);\n\t*qpair = NULL;\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_detach);\n\n \nint vmci_qpair_get_produce_indexes(const struct vmci_qp *qpair,\n\t\t\t\t   u64 *producer_tail,\n\t\t\t\t   u64 *consumer_head)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\tint result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tvmci_q_header_get_pointers(produce_q_header, consume_q_header,\n\t\t\t\t\t   producer_tail, consumer_head);\n\tqp_unlock(qpair);\n\n\tif (result == VMCI_SUCCESS &&\n\t    ((producer_tail && *producer_tail >= qpair->produce_q_size) ||\n\t     (consumer_head && *consumer_head >= qpair->produce_q_size)))\n\t\treturn VMCI_ERROR_INVALID_SIZE;\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_get_produce_indexes);\n\n \nint vmci_qpair_get_consume_indexes(const struct vmci_qp *qpair,\n\t\t\t\t   u64 *consumer_tail,\n\t\t\t\t   u64 *producer_head)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\tint result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tvmci_q_header_get_pointers(consume_q_header, produce_q_header,\n\t\t\t\t\t   consumer_tail, producer_head);\n\tqp_unlock(qpair);\n\n\tif (result == VMCI_SUCCESS &&\n\t    ((consumer_tail && *consumer_tail >= qpair->consume_q_size) ||\n\t     (producer_head && *producer_head >= qpair->consume_q_size)))\n\t\treturn VMCI_ERROR_INVALID_SIZE;\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_get_consume_indexes);\n\n \ns64 vmci_qpair_produce_free_space(const struct vmci_qp *qpair)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\ts64 result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tresult = vmci_q_header_free_space(produce_q_header,\n\t\t\t\t\t\t  consume_q_header,\n\t\t\t\t\t\t  qpair->produce_q_size);\n\telse\n\t\tresult = 0;\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_produce_free_space);\n\n \ns64 vmci_qpair_consume_free_space(const struct vmci_qp *qpair)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\ts64 result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tresult = vmci_q_header_free_space(consume_q_header,\n\t\t\t\t\t\t  produce_q_header,\n\t\t\t\t\t\t  qpair->consume_q_size);\n\telse\n\t\tresult = 0;\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_consume_free_space);\n\n \ns64 vmci_qpair_produce_buf_ready(const struct vmci_qp *qpair)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\ts64 result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tresult = vmci_q_header_buf_ready(produce_q_header,\n\t\t\t\t\t\t consume_q_header,\n\t\t\t\t\t\t qpair->produce_q_size);\n\telse\n\t\tresult = 0;\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_produce_buf_ready);\n\n \ns64 vmci_qpair_consume_buf_ready(const struct vmci_qp *qpair)\n{\n\tstruct vmci_queue_header *produce_q_header;\n\tstruct vmci_queue_header *consume_q_header;\n\ts64 result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\tresult =\n\t    qp_get_queue_headers(qpair, &produce_q_header, &consume_q_header);\n\tif (result == VMCI_SUCCESS)\n\t\tresult = vmci_q_header_buf_ready(consume_q_header,\n\t\t\t\t\t\t produce_q_header,\n\t\t\t\t\t\t qpair->consume_q_size);\n\telse\n\t\tresult = 0;\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_consume_buf_ready);\n\n \nssize_t vmci_qpair_enqueue(struct vmci_qp *qpair,\n\t\t\t   const void *buf,\n\t\t\t   size_t buf_size,\n\t\t\t   int buf_type)\n{\n\tssize_t result;\n\tstruct iov_iter from;\n\tstruct kvec v = {.iov_base = (void *)buf, .iov_len = buf_size};\n\n\tif (!qpair || !buf)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tiov_iter_kvec(&from, ITER_SOURCE, &v, 1, buf_size);\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_enqueue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->produce_q_size,\n\t\t\t\t\t   &from);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_enqueue);\n\n \nssize_t vmci_qpair_dequeue(struct vmci_qp *qpair,\n\t\t\t   void *buf,\n\t\t\t   size_t buf_size,\n\t\t\t   int buf_type)\n{\n\tssize_t result;\n\tstruct iov_iter to;\n\tstruct kvec v = {.iov_base = buf, .iov_len = buf_size};\n\n\tif (!qpair || !buf)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tiov_iter_kvec(&to, ITER_DEST, &v, 1, buf_size);\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_dequeue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->consume_q_size,\n\t\t\t\t\t   &to, true);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_dequeue);\n\n \nssize_t vmci_qpair_peek(struct vmci_qp *qpair,\n\t\t\tvoid *buf,\n\t\t\tsize_t buf_size,\n\t\t\tint buf_type)\n{\n\tstruct iov_iter to;\n\tstruct kvec v = {.iov_base = buf, .iov_len = buf_size};\n\tssize_t result;\n\n\tif (!qpair || !buf)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tiov_iter_kvec(&to, ITER_DEST, &v, 1, buf_size);\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_dequeue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->consume_q_size,\n\t\t\t\t\t   &to, false);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_peek);\n\n \nssize_t vmci_qpair_enquev(struct vmci_qp *qpair,\n\t\t\t  struct msghdr *msg,\n\t\t\t  size_t iov_size,\n\t\t\t  int buf_type)\n{\n\tssize_t result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_enqueue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->produce_q_size,\n\t\t\t\t\t   &msg->msg_iter);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_enquev);\n\n \nssize_t vmci_qpair_dequev(struct vmci_qp *qpair,\n\t\t\t  struct msghdr *msg,\n\t\t\t  size_t iov_size,\n\t\t\t  int buf_type)\n{\n\tssize_t result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_dequeue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->consume_q_size,\n\t\t\t\t\t   &msg->msg_iter, true);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_dequev);\n\n \nssize_t vmci_qpair_peekv(struct vmci_qp *qpair,\n\t\t\t struct msghdr *msg,\n\t\t\t size_t iov_size,\n\t\t\t int buf_type)\n{\n\tssize_t result;\n\n\tif (!qpair)\n\t\treturn VMCI_ERROR_INVALID_ARGS;\n\n\tqp_lock(qpair);\n\n\tdo {\n\t\tresult = qp_dequeue_locked(qpair->produce_q,\n\t\t\t\t\t   qpair->consume_q,\n\t\t\t\t\t   qpair->consume_q_size,\n\t\t\t\t\t   &msg->msg_iter, false);\n\n\t\tif (result == VMCI_ERROR_QUEUEPAIR_NOT_READY &&\n\t\t    !qp_wait_for_ready_queue(qpair))\n\t\t\tresult = VMCI_ERROR_WOULD_BLOCK;\n\n\t} while (result == VMCI_ERROR_QUEUEPAIR_NOT_READY);\n\n\tqp_unlock(qpair);\n\treturn result;\n}\nEXPORT_SYMBOL_GPL(vmci_qpair_peekv);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}