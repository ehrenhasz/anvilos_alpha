{
  "module_name": "vfio_iommu_type1.c",
  "hash_id": "61c8fc470a535f5ac5579d6943900afcea59b4658a65e6c261fdb751c1ba283e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/vfio_iommu_type1.c",
  "human_readable_source": "\n \n\n#include <linux/compat.h>\n#include <linux/device.h>\n#include <linux/fs.h>\n#include <linux/highmem.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/kthread.h>\n#include <linux/rbtree.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/vfio.h>\n#include <linux/workqueue.h>\n#include <linux/notifier.h>\n#include \"vfio.h\"\n\n#define DRIVER_VERSION  \"0.2\"\n#define DRIVER_AUTHOR   \"Alex Williamson <alex.williamson@redhat.com>\"\n#define DRIVER_DESC     \"Type1 IOMMU driver for VFIO\"\n\nstatic bool allow_unsafe_interrupts;\nmodule_param_named(allow_unsafe_interrupts,\n\t\t   allow_unsafe_interrupts, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(allow_unsafe_interrupts,\n\t\t \"Enable VFIO IOMMU support for on platforms without interrupt remapping support.\");\n\nstatic bool disable_hugepages;\nmodule_param_named(disable_hugepages,\n\t\t   disable_hugepages, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(disable_hugepages,\n\t\t \"Disable VFIO IOMMU support for IOMMU hugepages.\");\n\nstatic unsigned int dma_entry_limit __read_mostly = U16_MAX;\nmodule_param_named(dma_entry_limit, dma_entry_limit, uint, 0644);\nMODULE_PARM_DESC(dma_entry_limit,\n\t\t \"Maximum number of user DMA mappings per container (65535).\");\n\nstruct vfio_iommu {\n\tstruct list_head\tdomain_list;\n\tstruct list_head\tiova_list;\n\tstruct mutex\t\tlock;\n\tstruct rb_root\t\tdma_list;\n\tstruct list_head\tdevice_list;\n\tstruct mutex\t\tdevice_list_lock;\n\tunsigned int\t\tdma_avail;\n\tunsigned int\t\tvaddr_invalid_count;\n\tuint64_t\t\tpgsize_bitmap;\n\tuint64_t\t\tnum_non_pinned_groups;\n\tbool\t\t\tv2;\n\tbool\t\t\tnesting;\n\tbool\t\t\tdirty_page_tracking;\n\tstruct list_head\temulated_iommu_groups;\n};\n\nstruct vfio_domain {\n\tstruct iommu_domain\t*domain;\n\tstruct list_head\tnext;\n\tstruct list_head\tgroup_list;\n\tbool\t\t\tfgsp : 1;\t \n\tbool\t\t\tenforce_cache_coherency : 1;\n};\n\nstruct vfio_dma {\n\tstruct rb_node\t\tnode;\n\tdma_addr_t\t\tiova;\t\t \n\tunsigned long\t\tvaddr;\t\t \n\tsize_t\t\t\tsize;\t\t \n\tint\t\t\tprot;\t\t \n\tbool\t\t\tiommu_mapped;\n\tbool\t\t\tlock_cap;\t \n\tbool\t\t\tvaddr_invalid;\n\tstruct task_struct\t*task;\n\tstruct rb_root\t\tpfn_list;\t \n\tunsigned long\t\t*bitmap;\n\tstruct mm_struct\t*mm;\n\tsize_t\t\t\tlocked_vm;\n};\n\nstruct vfio_batch {\n\tstruct page\t\t**pages;\t \n\tstruct page\t\t*fallback_page;  \n\tint\t\t\tcapacity;\t \n\tint\t\t\tsize;\t\t \n\tint\t\t\toffset;\t\t \n};\n\nstruct vfio_iommu_group {\n\tstruct iommu_group\t*iommu_group;\n\tstruct list_head\tnext;\n\tbool\t\t\tpinned_page_dirty_scope;\n};\n\nstruct vfio_iova {\n\tstruct list_head\tlist;\n\tdma_addr_t\t\tstart;\n\tdma_addr_t\t\tend;\n};\n\n \nstruct vfio_pfn {\n\tstruct rb_node\t\tnode;\n\tdma_addr_t\t\tiova;\t\t \n\tunsigned long\t\tpfn;\t\t \n\tunsigned int\t\tref_count;\n};\n\nstruct vfio_regions {\n\tstruct list_head list;\n\tdma_addr_t iova;\n\tphys_addr_t phys;\n\tsize_t len;\n};\n\n#define DIRTY_BITMAP_BYTES(n)\t(ALIGN(n, BITS_PER_TYPE(u64)) / BITS_PER_BYTE)\n\n \n#define DIRTY_BITMAP_PAGES_MAX\t ((u64)INT_MAX)\n#define DIRTY_BITMAP_SIZE_MAX\t DIRTY_BITMAP_BYTES(DIRTY_BITMAP_PAGES_MAX)\n\nstatic int put_pfn(unsigned long pfn, int prot);\n\nstatic struct vfio_iommu_group*\nvfio_iommu_find_iommu_group(struct vfio_iommu *iommu,\n\t\t\t    struct iommu_group *iommu_group);\n\n \n\nstatic struct vfio_dma *vfio_find_dma(struct vfio_iommu *iommu,\n\t\t\t\t      dma_addr_t start, size_t size)\n{\n\tstruct rb_node *node = iommu->dma_list.rb_node;\n\n\twhile (node) {\n\t\tstruct vfio_dma *dma = rb_entry(node, struct vfio_dma, node);\n\n\t\tif (start + size <= dma->iova)\n\t\t\tnode = node->rb_left;\n\t\telse if (start >= dma->iova + dma->size)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn dma;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct rb_node *vfio_find_dma_first_node(struct vfio_iommu *iommu,\n\t\t\t\t\t\tdma_addr_t start, u64 size)\n{\n\tstruct rb_node *res = NULL;\n\tstruct rb_node *node = iommu->dma_list.rb_node;\n\tstruct vfio_dma *dma_res = NULL;\n\n\twhile (node) {\n\t\tstruct vfio_dma *dma = rb_entry(node, struct vfio_dma, node);\n\n\t\tif (start < dma->iova + dma->size) {\n\t\t\tres = node;\n\t\t\tdma_res = dma;\n\t\t\tif (start >= dma->iova)\n\t\t\t\tbreak;\n\t\t\tnode = node->rb_left;\n\t\t} else {\n\t\t\tnode = node->rb_right;\n\t\t}\n\t}\n\tif (res && size && dma_res->iova >= start + size)\n\t\tres = NULL;\n\treturn res;\n}\n\nstatic void vfio_link_dma(struct vfio_iommu *iommu, struct vfio_dma *new)\n{\n\tstruct rb_node **link = &iommu->dma_list.rb_node, *parent = NULL;\n\tstruct vfio_dma *dma;\n\n\twhile (*link) {\n\t\tparent = *link;\n\t\tdma = rb_entry(parent, struct vfio_dma, node);\n\n\t\tif (new->iova + new->size <= dma->iova)\n\t\t\tlink = &(*link)->rb_left;\n\t\telse\n\t\t\tlink = &(*link)->rb_right;\n\t}\n\n\trb_link_node(&new->node, parent, link);\n\trb_insert_color(&new->node, &iommu->dma_list);\n}\n\nstatic void vfio_unlink_dma(struct vfio_iommu *iommu, struct vfio_dma *old)\n{\n\trb_erase(&old->node, &iommu->dma_list);\n}\n\n\nstatic int vfio_dma_bitmap_alloc(struct vfio_dma *dma, size_t pgsize)\n{\n\tuint64_t npages = dma->size / pgsize;\n\n\tif (npages > DIRTY_BITMAP_PAGES_MAX)\n\t\treturn -EINVAL;\n\n\t \n\tdma->bitmap = kvzalloc(DIRTY_BITMAP_BYTES(npages) + sizeof(u64),\n\t\t\t       GFP_KERNEL);\n\tif (!dma->bitmap)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void vfio_dma_bitmap_free(struct vfio_dma *dma)\n{\n\tkvfree(dma->bitmap);\n\tdma->bitmap = NULL;\n}\n\nstatic void vfio_dma_populate_bitmap(struct vfio_dma *dma, size_t pgsize)\n{\n\tstruct rb_node *p;\n\tunsigned long pgshift = __ffs(pgsize);\n\n\tfor (p = rb_first(&dma->pfn_list); p; p = rb_next(p)) {\n\t\tstruct vfio_pfn *vpfn = rb_entry(p, struct vfio_pfn, node);\n\n\t\tbitmap_set(dma->bitmap, (vpfn->iova - dma->iova) >> pgshift, 1);\n\t}\n}\n\nstatic void vfio_iommu_populate_bitmap_full(struct vfio_iommu *iommu)\n{\n\tstruct rb_node *n;\n\tunsigned long pgshift = __ffs(iommu->pgsize_bitmap);\n\n\tfor (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\n\t\tbitmap_set(dma->bitmap, 0, dma->size >> pgshift);\n\t}\n}\n\nstatic int vfio_dma_bitmap_alloc_all(struct vfio_iommu *iommu, size_t pgsize)\n{\n\tstruct rb_node *n;\n\n\tfor (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\t\tint ret;\n\n\t\tret = vfio_dma_bitmap_alloc(dma, pgsize);\n\t\tif (ret) {\n\t\t\tstruct rb_node *p;\n\n\t\t\tfor (p = rb_prev(n); p; p = rb_prev(p)) {\n\t\t\t\tstruct vfio_dma *dma = rb_entry(n,\n\t\t\t\t\t\t\tstruct vfio_dma, node);\n\n\t\t\t\tvfio_dma_bitmap_free(dma);\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\t\tvfio_dma_populate_bitmap(dma, pgsize);\n\t}\n\treturn 0;\n}\n\nstatic void vfio_dma_bitmap_free_all(struct vfio_iommu *iommu)\n{\n\tstruct rb_node *n;\n\n\tfor (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\n\t\tvfio_dma_bitmap_free(dma);\n\t}\n}\n\n \nstatic struct vfio_pfn *vfio_find_vpfn(struct vfio_dma *dma, dma_addr_t iova)\n{\n\tstruct vfio_pfn *vpfn;\n\tstruct rb_node *node = dma->pfn_list.rb_node;\n\n\twhile (node) {\n\t\tvpfn = rb_entry(node, struct vfio_pfn, node);\n\n\t\tif (iova < vpfn->iova)\n\t\t\tnode = node->rb_left;\n\t\telse if (iova > vpfn->iova)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn vpfn;\n\t}\n\treturn NULL;\n}\n\nstatic void vfio_link_pfn(struct vfio_dma *dma,\n\t\t\t  struct vfio_pfn *new)\n{\n\tstruct rb_node **link, *parent = NULL;\n\tstruct vfio_pfn *vpfn;\n\n\tlink = &dma->pfn_list.rb_node;\n\twhile (*link) {\n\t\tparent = *link;\n\t\tvpfn = rb_entry(parent, struct vfio_pfn, node);\n\n\t\tif (new->iova < vpfn->iova)\n\t\t\tlink = &(*link)->rb_left;\n\t\telse\n\t\t\tlink = &(*link)->rb_right;\n\t}\n\n\trb_link_node(&new->node, parent, link);\n\trb_insert_color(&new->node, &dma->pfn_list);\n}\n\nstatic void vfio_unlink_pfn(struct vfio_dma *dma, struct vfio_pfn *old)\n{\n\trb_erase(&old->node, &dma->pfn_list);\n}\n\nstatic int vfio_add_to_pfn_list(struct vfio_dma *dma, dma_addr_t iova,\n\t\t\t\tunsigned long pfn)\n{\n\tstruct vfio_pfn *vpfn;\n\n\tvpfn = kzalloc(sizeof(*vpfn), GFP_KERNEL);\n\tif (!vpfn)\n\t\treturn -ENOMEM;\n\n\tvpfn->iova = iova;\n\tvpfn->pfn = pfn;\n\tvpfn->ref_count = 1;\n\tvfio_link_pfn(dma, vpfn);\n\treturn 0;\n}\n\nstatic void vfio_remove_from_pfn_list(struct vfio_dma *dma,\n\t\t\t\t      struct vfio_pfn *vpfn)\n{\n\tvfio_unlink_pfn(dma, vpfn);\n\tkfree(vpfn);\n}\n\nstatic struct vfio_pfn *vfio_iova_get_vfio_pfn(struct vfio_dma *dma,\n\t\t\t\t\t       unsigned long iova)\n{\n\tstruct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);\n\n\tif (vpfn)\n\t\tvpfn->ref_count++;\n\treturn vpfn;\n}\n\nstatic int vfio_iova_put_vfio_pfn(struct vfio_dma *dma, struct vfio_pfn *vpfn)\n{\n\tint ret = 0;\n\n\tvpfn->ref_count--;\n\tif (!vpfn->ref_count) {\n\t\tret = put_pfn(vpfn->pfn, dma->prot);\n\t\tvfio_remove_from_pfn_list(dma, vpfn);\n\t}\n\treturn ret;\n}\n\nstatic int mm_lock_acct(struct task_struct *task, struct mm_struct *mm,\n\t\t\tbool lock_cap, long npage)\n{\n\tint ret = mmap_write_lock_killable(mm);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = __account_locked_vm(mm, abs(npage), npage > 0, task, lock_cap);\n\tmmap_write_unlock(mm);\n\treturn ret;\n}\n\nstatic int vfio_lock_acct(struct vfio_dma *dma, long npage, bool async)\n{\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tif (!npage)\n\t\treturn 0;\n\n\tmm = dma->mm;\n\tif (async && !mmget_not_zero(mm))\n\t\treturn -ESRCH;  \n\n\tret = mm_lock_acct(dma->task, mm, dma->lock_cap, npage);\n\tif (!ret)\n\t\tdma->locked_vm += npage;\n\n\tif (async)\n\t\tmmput(mm);\n\n\treturn ret;\n}\n\n \nstatic bool is_invalid_reserved_pfn(unsigned long pfn)\n{\n\tif (pfn_valid(pfn))\n\t\treturn PageReserved(pfn_to_page(pfn));\n\n\treturn true;\n}\n\nstatic int put_pfn(unsigned long pfn, int prot)\n{\n\tif (!is_invalid_reserved_pfn(pfn)) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tunpin_user_pages_dirty_lock(&page, 1, prot & IOMMU_WRITE);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n#define VFIO_BATCH_MAX_CAPACITY (PAGE_SIZE / sizeof(struct page *))\n\nstatic void vfio_batch_init(struct vfio_batch *batch)\n{\n\tbatch->size = 0;\n\tbatch->offset = 0;\n\n\tif (unlikely(disable_hugepages))\n\t\tgoto fallback;\n\n\tbatch->pages = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!batch->pages)\n\t\tgoto fallback;\n\n\tbatch->capacity = VFIO_BATCH_MAX_CAPACITY;\n\treturn;\n\nfallback:\n\tbatch->pages = &batch->fallback_page;\n\tbatch->capacity = 1;\n}\n\nstatic void vfio_batch_unpin(struct vfio_batch *batch, struct vfio_dma *dma)\n{\n\twhile (batch->size) {\n\t\tunsigned long pfn = page_to_pfn(batch->pages[batch->offset]);\n\n\t\tput_pfn(pfn, dma->prot);\n\t\tbatch->offset++;\n\t\tbatch->size--;\n\t}\n}\n\nstatic void vfio_batch_fini(struct vfio_batch *batch)\n{\n\tif (batch->capacity == VFIO_BATCH_MAX_CAPACITY)\n\t\tfree_page((unsigned long)batch->pages);\n}\n\nstatic int follow_fault_pfn(struct vm_area_struct *vma, struct mm_struct *mm,\n\t\t\t    unsigned long vaddr, unsigned long *pfn,\n\t\t\t    bool write_fault)\n{\n\tpte_t *ptep;\n\tpte_t pte;\n\tspinlock_t *ptl;\n\tint ret;\n\n\tret = follow_pte(vma->vm_mm, vaddr, &ptep, &ptl);\n\tif (ret) {\n\t\tbool unlocked = false;\n\n\t\tret = fixup_user_fault(mm, vaddr,\n\t\t\t\t       FAULT_FLAG_REMOTE |\n\t\t\t\t       (write_fault ?  FAULT_FLAG_WRITE : 0),\n\t\t\t\t       &unlocked);\n\t\tif (unlocked)\n\t\t\treturn -EAGAIN;\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = follow_pte(vma->vm_mm, vaddr, &ptep, &ptl);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tpte = ptep_get(ptep);\n\n\tif (write_fault && !pte_write(pte))\n\t\tret = -EFAULT;\n\telse\n\t\t*pfn = pte_pfn(pte);\n\n\tpte_unmap_unlock(ptep, ptl);\n\treturn ret;\n}\n\n \nstatic int vaddr_get_pfns(struct mm_struct *mm, unsigned long vaddr,\n\t\t\t  long npages, int prot, unsigned long *pfn,\n\t\t\t  struct page **pages)\n{\n\tstruct vm_area_struct *vma;\n\tunsigned int flags = 0;\n\tint ret;\n\n\tif (prot & IOMMU_WRITE)\n\t\tflags |= FOLL_WRITE;\n\n\tmmap_read_lock(mm);\n\tret = pin_user_pages_remote(mm, vaddr, npages, flags | FOLL_LONGTERM,\n\t\t\t\t    pages, NULL);\n\tif (ret > 0) {\n\t\tint i;\n\n\t\t \n\t\tfor (i = 0 ; i < ret; i++) {\n\t\t\tif (unlikely(is_zero_pfn(page_to_pfn(pages[i]))))\n\t\t\t\tunpin_user_page(pages[i]);\n\t\t}\n\n\t\t*pfn = page_to_pfn(pages[0]);\n\t\tgoto done;\n\t}\n\n\tvaddr = untagged_addr_remote(mm, vaddr);\n\nretry:\n\tvma = vma_lookup(mm, vaddr);\n\n\tif (vma && vma->vm_flags & VM_PFNMAP) {\n\t\tret = follow_fault_pfn(vma, mm, vaddr, pfn, prot & IOMMU_WRITE);\n\t\tif (ret == -EAGAIN)\n\t\t\tgoto retry;\n\n\t\tif (!ret) {\n\t\t\tif (is_invalid_reserved_pfn(*pfn))\n\t\t\t\tret = 1;\n\t\t\telse\n\t\t\t\tret = -EFAULT;\n\t\t}\n\t}\ndone:\n\tmmap_read_unlock(mm);\n\treturn ret;\n}\n\n \nstatic long vfio_pin_pages_remote(struct vfio_dma *dma, unsigned long vaddr,\n\t\t\t\t  long npage, unsigned long *pfn_base,\n\t\t\t\t  unsigned long limit, struct vfio_batch *batch)\n{\n\tunsigned long pfn;\n\tstruct mm_struct *mm = current->mm;\n\tlong ret, pinned = 0, lock_acct = 0;\n\tbool rsvd;\n\tdma_addr_t iova = vaddr - dma->vaddr + dma->iova;\n\n\t \n\tif (!mm)\n\t\treturn -ENODEV;\n\n\tif (batch->size) {\n\t\t \n\t\t*pfn_base = page_to_pfn(batch->pages[batch->offset]);\n\t\tpfn = *pfn_base;\n\t\trsvd = is_invalid_reserved_pfn(*pfn_base);\n\t} else {\n\t\t*pfn_base = 0;\n\t}\n\n\twhile (npage) {\n\t\tif (!batch->size) {\n\t\t\t \n\t\t\tlong req_pages = min_t(long, npage, batch->capacity);\n\n\t\t\tret = vaddr_get_pfns(mm, vaddr, req_pages, dma->prot,\n\t\t\t\t\t     &pfn, batch->pages);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto unpin_out;\n\n\t\t\tbatch->size = ret;\n\t\t\tbatch->offset = 0;\n\n\t\t\tif (!*pfn_base) {\n\t\t\t\t*pfn_base = pfn;\n\t\t\t\trsvd = is_invalid_reserved_pfn(*pfn_base);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\twhile (true) {\n\t\t\tif (pfn != *pfn_base + pinned ||\n\t\t\t    rsvd != is_invalid_reserved_pfn(pfn))\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tif (!rsvd && !vfio_find_vpfn(dma, iova)) {\n\t\t\t\tif (!dma->lock_cap &&\n\t\t\t\t    mm->locked_vm + lock_acct + 1 > limit) {\n\t\t\t\t\tpr_warn(\"%s: RLIMIT_MEMLOCK (%ld) exceeded\\n\",\n\t\t\t\t\t\t__func__, limit << PAGE_SHIFT);\n\t\t\t\t\tret = -ENOMEM;\n\t\t\t\t\tgoto unpin_out;\n\t\t\t\t}\n\t\t\t\tlock_acct++;\n\t\t\t}\n\n\t\t\tpinned++;\n\t\t\tnpage--;\n\t\t\tvaddr += PAGE_SIZE;\n\t\t\tiova += PAGE_SIZE;\n\t\t\tbatch->offset++;\n\t\t\tbatch->size--;\n\n\t\t\tif (!batch->size)\n\t\t\t\tbreak;\n\n\t\t\tpfn = page_to_pfn(batch->pages[batch->offset]);\n\t\t}\n\n\t\tif (unlikely(disable_hugepages))\n\t\t\tbreak;\n\t}\n\nout:\n\tret = vfio_lock_acct(dma, lock_acct, false);\n\nunpin_out:\n\tif (batch->size == 1 && !batch->offset) {\n\t\t \n\t\tput_pfn(pfn, dma->prot);\n\t\tbatch->size = 0;\n\t}\n\n\tif (ret < 0) {\n\t\tif (pinned && !rsvd) {\n\t\t\tfor (pfn = *pfn_base ; pinned ; pfn++, pinned--)\n\t\t\t\tput_pfn(pfn, dma->prot);\n\t\t}\n\t\tvfio_batch_unpin(batch, dma);\n\n\t\treturn ret;\n\t}\n\n\treturn pinned;\n}\n\nstatic long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,\n\t\t\t\t    unsigned long pfn, long npage,\n\t\t\t\t    bool do_accounting)\n{\n\tlong unlocked = 0, locked = 0;\n\tlong i;\n\n\tfor (i = 0; i < npage; i++, iova += PAGE_SIZE) {\n\t\tif (put_pfn(pfn++, dma->prot)) {\n\t\t\tunlocked++;\n\t\t\tif (vfio_find_vpfn(dma, iova))\n\t\t\t\tlocked++;\n\t\t}\n\t}\n\n\tif (do_accounting)\n\t\tvfio_lock_acct(dma, locked - unlocked, true);\n\n\treturn unlocked;\n}\n\nstatic int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,\n\t\t\t\t  unsigned long *pfn_base, bool do_accounting)\n{\n\tstruct page *pages[1];\n\tstruct mm_struct *mm;\n\tint ret;\n\n\tmm = dma->mm;\n\tif (!mmget_not_zero(mm))\n\t\treturn -ENODEV;\n\n\tret = vaddr_get_pfns(mm, vaddr, 1, dma->prot, pfn_base, pages);\n\tif (ret != 1)\n\t\tgoto out;\n\n\tret = 0;\n\n\tif (do_accounting && !is_invalid_reserved_pfn(*pfn_base)) {\n\t\tret = vfio_lock_acct(dma, 1, false);\n\t\tif (ret) {\n\t\t\tput_pfn(*pfn_base, dma->prot);\n\t\t\tif (ret == -ENOMEM)\n\t\t\t\tpr_warn(\"%s: Task %s (%d) RLIMIT_MEMLOCK \"\n\t\t\t\t\t\"(%ld) exceeded\\n\", __func__,\n\t\t\t\t\tdma->task->comm, task_pid_nr(dma->task),\n\t\t\t\t\ttask_rlimit(dma->task, RLIMIT_MEMLOCK));\n\t\t}\n\t}\n\nout:\n\tmmput(mm);\n\treturn ret;\n}\n\nstatic int vfio_unpin_page_external(struct vfio_dma *dma, dma_addr_t iova,\n\t\t\t\t    bool do_accounting)\n{\n\tint unlocked;\n\tstruct vfio_pfn *vpfn = vfio_find_vpfn(dma, iova);\n\n\tif (!vpfn)\n\t\treturn 0;\n\n\tunlocked = vfio_iova_put_vfio_pfn(dma, vpfn);\n\n\tif (do_accounting)\n\t\tvfio_lock_acct(dma, -unlocked, true);\n\n\treturn unlocked;\n}\n\nstatic int vfio_iommu_type1_pin_pages(void *iommu_data,\n\t\t\t\t      struct iommu_group *iommu_group,\n\t\t\t\t      dma_addr_t user_iova,\n\t\t\t\t      int npage, int prot,\n\t\t\t\t      struct page **pages)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tstruct vfio_iommu_group *group;\n\tint i, j, ret;\n\tunsigned long remote_vaddr;\n\tstruct vfio_dma *dma;\n\tbool do_accounting;\n\n\tif (!iommu || !pages)\n\t\treturn -EINVAL;\n\n\t \n\tif (!iommu->v2)\n\t\treturn -EACCES;\n\n\tmutex_lock(&iommu->lock);\n\n\tif (WARN_ONCE(iommu->vaddr_invalid_count,\n\t\t      \"vfio_pin_pages not allowed with VFIO_UPDATE_VADDR\\n\")) {\n\t\tret = -EBUSY;\n\t\tgoto pin_done;\n\t}\n\n\t \n\tif (list_empty(&iommu->device_list)) {\n\t\tret = -EINVAL;\n\t\tgoto pin_done;\n\t}\n\n\t \n\tdo_accounting = list_empty(&iommu->domain_list);\n\n\tfor (i = 0; i < npage; i++) {\n\t\tunsigned long phys_pfn;\n\t\tdma_addr_t iova;\n\t\tstruct vfio_pfn *vpfn;\n\n\t\tiova = user_iova + PAGE_SIZE * i;\n\t\tdma = vfio_find_dma(iommu, iova, PAGE_SIZE);\n\t\tif (!dma) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto pin_unwind;\n\t\t}\n\n\t\tif ((dma->prot & prot) != prot) {\n\t\t\tret = -EPERM;\n\t\t\tgoto pin_unwind;\n\t\t}\n\n\t\tvpfn = vfio_iova_get_vfio_pfn(dma, iova);\n\t\tif (vpfn) {\n\t\t\tpages[i] = pfn_to_page(vpfn->pfn);\n\t\t\tcontinue;\n\t\t}\n\n\t\tremote_vaddr = dma->vaddr + (iova - dma->iova);\n\t\tret = vfio_pin_page_external(dma, remote_vaddr, &phys_pfn,\n\t\t\t\t\t     do_accounting);\n\t\tif (ret)\n\t\t\tgoto pin_unwind;\n\n\t\tif (!pfn_valid(phys_pfn)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto pin_unwind;\n\t\t}\n\n\t\tret = vfio_add_to_pfn_list(dma, iova, phys_pfn);\n\t\tif (ret) {\n\t\t\tif (put_pfn(phys_pfn, dma->prot) && do_accounting)\n\t\t\t\tvfio_lock_acct(dma, -1, true);\n\t\t\tgoto pin_unwind;\n\t\t}\n\n\t\tpages[i] = pfn_to_page(phys_pfn);\n\n\t\tif (iommu->dirty_page_tracking) {\n\t\t\tunsigned long pgshift = __ffs(iommu->pgsize_bitmap);\n\n\t\t\t \n\t\t\tbitmap_set(dma->bitmap,\n\t\t\t\t   (iova - dma->iova) >> pgshift, 1);\n\t\t}\n\t}\n\tret = i;\n\n\tgroup = vfio_iommu_find_iommu_group(iommu, iommu_group);\n\tif (!group->pinned_page_dirty_scope) {\n\t\tgroup->pinned_page_dirty_scope = true;\n\t\tiommu->num_non_pinned_groups--;\n\t}\n\n\tgoto pin_done;\n\npin_unwind:\n\tpages[i] = NULL;\n\tfor (j = 0; j < i; j++) {\n\t\tdma_addr_t iova;\n\n\t\tiova = user_iova + PAGE_SIZE * j;\n\t\tdma = vfio_find_dma(iommu, iova, PAGE_SIZE);\n\t\tvfio_unpin_page_external(dma, iova, do_accounting);\n\t\tpages[j] = NULL;\n\t}\npin_done:\n\tmutex_unlock(&iommu->lock);\n\treturn ret;\n}\n\nstatic void vfio_iommu_type1_unpin_pages(void *iommu_data,\n\t\t\t\t\t dma_addr_t user_iova, int npage)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tbool do_accounting;\n\tint i;\n\n\t \n\tif (WARN_ON(!iommu->v2))\n\t\treturn;\n\n\tmutex_lock(&iommu->lock);\n\n\tdo_accounting = list_empty(&iommu->domain_list);\n\tfor (i = 0; i < npage; i++) {\n\t\tdma_addr_t iova = user_iova + PAGE_SIZE * i;\n\t\tstruct vfio_dma *dma;\n\n\t\tdma = vfio_find_dma(iommu, iova, PAGE_SIZE);\n\t\tif (!dma)\n\t\t\tbreak;\n\n\t\tvfio_unpin_page_external(dma, iova, do_accounting);\n\t}\n\n\tmutex_unlock(&iommu->lock);\n\n\tWARN_ON(i != npage);\n}\n\nstatic long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,\n\t\t\t    struct list_head *regions,\n\t\t\t    struct iommu_iotlb_gather *iotlb_gather)\n{\n\tlong unlocked = 0;\n\tstruct vfio_regions *entry, *next;\n\n\tiommu_iotlb_sync(domain->domain, iotlb_gather);\n\n\tlist_for_each_entry_safe(entry, next, regions, list) {\n\t\tunlocked += vfio_unpin_pages_remote(dma,\n\t\t\t\t\t\t    entry->iova,\n\t\t\t\t\t\t    entry->phys >> PAGE_SHIFT,\n\t\t\t\t\t\t    entry->len >> PAGE_SHIFT,\n\t\t\t\t\t\t    false);\n\t\tlist_del(&entry->list);\n\t\tkfree(entry);\n\t}\n\n\tcond_resched();\n\n\treturn unlocked;\n}\n\n \n#define VFIO_IOMMU_TLB_SYNC_MAX\t\t512\n\nstatic size_t unmap_unpin_fast(struct vfio_domain *domain,\n\t\t\t       struct vfio_dma *dma, dma_addr_t *iova,\n\t\t\t       size_t len, phys_addr_t phys, long *unlocked,\n\t\t\t       struct list_head *unmapped_list,\n\t\t\t       int *unmapped_cnt,\n\t\t\t       struct iommu_iotlb_gather *iotlb_gather)\n{\n\tsize_t unmapped = 0;\n\tstruct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\n\tif (entry) {\n\t\tunmapped = iommu_unmap_fast(domain->domain, *iova, len,\n\t\t\t\t\t    iotlb_gather);\n\n\t\tif (!unmapped) {\n\t\t\tkfree(entry);\n\t\t} else {\n\t\t\tentry->iova = *iova;\n\t\t\tentry->phys = phys;\n\t\t\tentry->len  = unmapped;\n\t\t\tlist_add_tail(&entry->list, unmapped_list);\n\n\t\t\t*iova += unmapped;\n\t\t\t(*unmapped_cnt)++;\n\t\t}\n\t}\n\n\t \n\tif (*unmapped_cnt >= VFIO_IOMMU_TLB_SYNC_MAX || !unmapped) {\n\t\t*unlocked += vfio_sync_unpin(dma, domain, unmapped_list,\n\t\t\t\t\t     iotlb_gather);\n\t\t*unmapped_cnt = 0;\n\t}\n\n\treturn unmapped;\n}\n\nstatic size_t unmap_unpin_slow(struct vfio_domain *domain,\n\t\t\t       struct vfio_dma *dma, dma_addr_t *iova,\n\t\t\t       size_t len, phys_addr_t phys,\n\t\t\t       long *unlocked)\n{\n\tsize_t unmapped = iommu_unmap(domain->domain, *iova, len);\n\n\tif (unmapped) {\n\t\t*unlocked += vfio_unpin_pages_remote(dma, *iova,\n\t\t\t\t\t\t     phys >> PAGE_SHIFT,\n\t\t\t\t\t\t     unmapped >> PAGE_SHIFT,\n\t\t\t\t\t\t     false);\n\t\t*iova += unmapped;\n\t\tcond_resched();\n\t}\n\treturn unmapped;\n}\n\nstatic long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,\n\t\t\t     bool do_accounting)\n{\n\tdma_addr_t iova = dma->iova, end = dma->iova + dma->size;\n\tstruct vfio_domain *domain, *d;\n\tLIST_HEAD(unmapped_region_list);\n\tstruct iommu_iotlb_gather iotlb_gather;\n\tint unmapped_region_cnt = 0;\n\tlong unlocked = 0;\n\n\tif (!dma->size)\n\t\treturn 0;\n\n\tif (list_empty(&iommu->domain_list))\n\t\treturn 0;\n\n\t \n\tdomain = d = list_first_entry(&iommu->domain_list,\n\t\t\t\t      struct vfio_domain, next);\n\n\tlist_for_each_entry_continue(d, &iommu->domain_list, next) {\n\t\tiommu_unmap(d->domain, dma->iova, dma->size);\n\t\tcond_resched();\n\t}\n\n\tiommu_iotlb_gather_init(&iotlb_gather);\n\twhile (iova < end) {\n\t\tsize_t unmapped, len;\n\t\tphys_addr_t phys, next;\n\n\t\tphys = iommu_iova_to_phys(domain->domain, iova);\n\t\tif (WARN_ON(!phys)) {\n\t\t\tiova += PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tfor (len = PAGE_SIZE;\n\t\t     !domain->fgsp && iova + len < end; len += PAGE_SIZE) {\n\t\t\tnext = iommu_iova_to_phys(domain->domain, iova + len);\n\t\t\tif (next != phys + len)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tunmapped = unmap_unpin_fast(domain, dma, &iova, len, phys,\n\t\t\t\t\t    &unlocked, &unmapped_region_list,\n\t\t\t\t\t    &unmapped_region_cnt,\n\t\t\t\t\t    &iotlb_gather);\n\t\tif (!unmapped) {\n\t\t\tunmapped = unmap_unpin_slow(domain, dma, &iova, len,\n\t\t\t\t\t\t    phys, &unlocked);\n\t\t\tif (WARN_ON(!unmapped))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tdma->iommu_mapped = false;\n\n\tif (unmapped_region_cnt) {\n\t\tunlocked += vfio_sync_unpin(dma, domain, &unmapped_region_list,\n\t\t\t\t\t    &iotlb_gather);\n\t}\n\n\tif (do_accounting) {\n\t\tvfio_lock_acct(dma, -unlocked, true);\n\t\treturn 0;\n\t}\n\treturn unlocked;\n}\n\nstatic void vfio_remove_dma(struct vfio_iommu *iommu, struct vfio_dma *dma)\n{\n\tWARN_ON(!RB_EMPTY_ROOT(&dma->pfn_list));\n\tvfio_unmap_unpin(iommu, dma, true);\n\tvfio_unlink_dma(iommu, dma);\n\tput_task_struct(dma->task);\n\tmmdrop(dma->mm);\n\tvfio_dma_bitmap_free(dma);\n\tif (dma->vaddr_invalid)\n\t\tiommu->vaddr_invalid_count--;\n\tkfree(dma);\n\tiommu->dma_avail++;\n}\n\nstatic void vfio_update_pgsize_bitmap(struct vfio_iommu *iommu)\n{\n\tstruct vfio_domain *domain;\n\n\tiommu->pgsize_bitmap = ULONG_MAX;\n\n\tlist_for_each_entry(domain, &iommu->domain_list, next)\n\t\tiommu->pgsize_bitmap &= domain->domain->pgsize_bitmap;\n\n\t \n\tif (iommu->pgsize_bitmap & ~PAGE_MASK) {\n\t\tiommu->pgsize_bitmap &= PAGE_MASK;\n\t\tiommu->pgsize_bitmap |= PAGE_SIZE;\n\t}\n}\n\nstatic int update_user_bitmap(u64 __user *bitmap, struct vfio_iommu *iommu,\n\t\t\t      struct vfio_dma *dma, dma_addr_t base_iova,\n\t\t\t      size_t pgsize)\n{\n\tunsigned long pgshift = __ffs(pgsize);\n\tunsigned long nbits = dma->size >> pgshift;\n\tunsigned long bit_offset = (dma->iova - base_iova) >> pgshift;\n\tunsigned long copy_offset = bit_offset / BITS_PER_LONG;\n\tunsigned long shift = bit_offset % BITS_PER_LONG;\n\tunsigned long leftover;\n\n\t \n\tif (iommu->num_non_pinned_groups && dma->iommu_mapped)\n\t\tbitmap_set(dma->bitmap, 0, nbits);\n\n\tif (shift) {\n\t\tbitmap_shift_left(dma->bitmap, dma->bitmap, shift,\n\t\t\t\t  nbits + shift);\n\n\t\tif (copy_from_user(&leftover,\n\t\t\t\t   (void __user *)(bitmap + copy_offset),\n\t\t\t\t   sizeof(leftover)))\n\t\t\treturn -EFAULT;\n\n\t\tbitmap_or(dma->bitmap, dma->bitmap, &leftover, shift);\n\t}\n\n\tif (copy_to_user((void __user *)(bitmap + copy_offset), dma->bitmap,\n\t\t\t DIRTY_BITMAP_BYTES(nbits + shift)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int vfio_iova_dirty_bitmap(u64 __user *bitmap, struct vfio_iommu *iommu,\n\t\t\t\t  dma_addr_t iova, size_t size, size_t pgsize)\n{\n\tstruct vfio_dma *dma;\n\tstruct rb_node *n;\n\tunsigned long pgshift = __ffs(pgsize);\n\tint ret;\n\n\t \n\tdma = vfio_find_dma(iommu, iova, 1);\n\tif (dma && dma->iova != iova)\n\t\treturn -EINVAL;\n\n\tdma = vfio_find_dma(iommu, iova + size - 1, 0);\n\tif (dma && dma->iova + dma->size != iova + size)\n\t\treturn -EINVAL;\n\n\tfor (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\n\t\tif (dma->iova < iova)\n\t\t\tcontinue;\n\n\t\tif (dma->iova > iova + size - 1)\n\t\t\tbreak;\n\n\t\tret = update_user_bitmap(bitmap, iommu, dma, iova, pgsize);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tbitmap_clear(dma->bitmap, 0, dma->size >> pgshift);\n\t\tvfio_dma_populate_bitmap(dma, pgsize);\n\t}\n\treturn 0;\n}\n\nstatic int verify_bitmap_size(uint64_t npages, uint64_t bitmap_size)\n{\n\tif (!npages || !bitmap_size || (bitmap_size > DIRTY_BITMAP_SIZE_MAX) ||\n\t    (bitmap_size < DIRTY_BITMAP_BYTES(npages)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic void vfio_notify_dma_unmap(struct vfio_iommu *iommu,\n\t\t\t\t  struct vfio_dma *dma)\n{\n\tstruct vfio_device *device;\n\n\tif (list_empty(&iommu->device_list))\n\t\treturn;\n\n\t \n\tmutex_lock(&iommu->device_list_lock);\n\tmutex_unlock(&iommu->lock);\n\n\tlist_for_each_entry(device, &iommu->device_list, iommu_entry)\n\t\tdevice->ops->dma_unmap(device, dma->iova, dma->size);\n\n\tmutex_unlock(&iommu->device_list_lock);\n\tmutex_lock(&iommu->lock);\n}\n\nstatic int vfio_dma_do_unmap(struct vfio_iommu *iommu,\n\t\t\t     struct vfio_iommu_type1_dma_unmap *unmap,\n\t\t\t     struct vfio_bitmap *bitmap)\n{\n\tstruct vfio_dma *dma, *dma_last = NULL;\n\tsize_t unmapped = 0, pgsize;\n\tint ret = -EINVAL, retries = 0;\n\tunsigned long pgshift;\n\tdma_addr_t iova = unmap->iova;\n\tu64 size = unmap->size;\n\tbool unmap_all = unmap->flags & VFIO_DMA_UNMAP_FLAG_ALL;\n\tbool invalidate_vaddr = unmap->flags & VFIO_DMA_UNMAP_FLAG_VADDR;\n\tstruct rb_node *n, *first_n;\n\n\tmutex_lock(&iommu->lock);\n\n\t \n\tif (invalidate_vaddr && !list_empty(&iommu->emulated_iommu_groups)) {\n\t\tret = -EBUSY;\n\t\tgoto unlock;\n\t}\n\n\tpgshift = __ffs(iommu->pgsize_bitmap);\n\tpgsize = (size_t)1 << pgshift;\n\n\tif (iova & (pgsize - 1))\n\t\tgoto unlock;\n\n\tif (unmap_all) {\n\t\tif (iova || size)\n\t\t\tgoto unlock;\n\t\tsize = U64_MAX;\n\t} else if (!size || size & (pgsize - 1) ||\n\t\t   iova + size - 1 < iova || size > SIZE_MAX) {\n\t\tgoto unlock;\n\t}\n\n\t \n\tif ((unmap->flags & VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP) &&\n\t    (!iommu->dirty_page_tracking || (bitmap->pgsize != pgsize))) {\n\t\tgoto unlock;\n\t}\n\n\tWARN_ON((pgsize - 1) & PAGE_MASK);\nagain:\n\t \n\tif (iommu->v2 && !unmap_all) {\n\t\tdma = vfio_find_dma(iommu, iova, 1);\n\t\tif (dma && dma->iova != iova)\n\t\t\tgoto unlock;\n\n\t\tdma = vfio_find_dma(iommu, iova + size - 1, 0);\n\t\tif (dma && dma->iova + dma->size != iova + size)\n\t\t\tgoto unlock;\n\t}\n\n\tret = 0;\n\tn = first_n = vfio_find_dma_first_node(iommu, iova, size);\n\n\twhile (n) {\n\t\tdma = rb_entry(n, struct vfio_dma, node);\n\t\tif (dma->iova >= iova + size)\n\t\t\tbreak;\n\n\t\tif (!iommu->v2 && iova > dma->iova)\n\t\t\tbreak;\n\n\t\tif (invalidate_vaddr) {\n\t\t\tif (dma->vaddr_invalid) {\n\t\t\t\tstruct rb_node *last_n = n;\n\n\t\t\t\tfor (n = first_n; n != last_n; n = rb_next(n)) {\n\t\t\t\t\tdma = rb_entry(n,\n\t\t\t\t\t\t       struct vfio_dma, node);\n\t\t\t\t\tdma->vaddr_invalid = false;\n\t\t\t\t\tiommu->vaddr_invalid_count--;\n\t\t\t\t}\n\t\t\t\tret = -EINVAL;\n\t\t\t\tunmapped = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tdma->vaddr_invalid = true;\n\t\t\tiommu->vaddr_invalid_count++;\n\t\t\tunmapped += dma->size;\n\t\t\tn = rb_next(n);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!RB_EMPTY_ROOT(&dma->pfn_list)) {\n\t\t\tif (dma_last == dma) {\n\t\t\t\tBUG_ON(++retries > 10);\n\t\t\t} else {\n\t\t\t\tdma_last = dma;\n\t\t\t\tretries = 0;\n\t\t\t}\n\n\t\t\tvfio_notify_dma_unmap(iommu, dma);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (unmap->flags & VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP) {\n\t\t\tret = update_user_bitmap(bitmap->data, iommu, dma,\n\t\t\t\t\t\t iova, pgsize);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tunmapped += dma->size;\n\t\tn = rb_next(n);\n\t\tvfio_remove_dma(iommu, dma);\n\t}\n\nunlock:\n\tmutex_unlock(&iommu->lock);\n\n\t \n\tunmap->size = unmapped;\n\n\treturn ret;\n}\n\nstatic int vfio_iommu_map(struct vfio_iommu *iommu, dma_addr_t iova,\n\t\t\t  unsigned long pfn, long npage, int prot)\n{\n\tstruct vfio_domain *d;\n\tint ret;\n\n\tlist_for_each_entry(d, &iommu->domain_list, next) {\n\t\tret = iommu_map(d->domain, iova, (phys_addr_t)pfn << PAGE_SHIFT,\n\t\t\t\tnpage << PAGE_SHIFT, prot | IOMMU_CACHE,\n\t\t\t\tGFP_KERNEL);\n\t\tif (ret)\n\t\t\tgoto unwind;\n\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n\nunwind:\n\tlist_for_each_entry_continue_reverse(d, &iommu->domain_list, next) {\n\t\tiommu_unmap(d->domain, iova, npage << PAGE_SHIFT);\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n}\n\nstatic int vfio_pin_map_dma(struct vfio_iommu *iommu, struct vfio_dma *dma,\n\t\t\t    size_t map_size)\n{\n\tdma_addr_t iova = dma->iova;\n\tunsigned long vaddr = dma->vaddr;\n\tstruct vfio_batch batch;\n\tsize_t size = map_size;\n\tlong npage;\n\tunsigned long pfn, limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\tint ret = 0;\n\n\tvfio_batch_init(&batch);\n\n\twhile (size) {\n\t\t \n\t\tnpage = vfio_pin_pages_remote(dma, vaddr + dma->size,\n\t\t\t\t\t      size >> PAGE_SHIFT, &pfn, limit,\n\t\t\t\t\t      &batch);\n\t\tif (npage <= 0) {\n\t\t\tWARN_ON(!npage);\n\t\t\tret = (int)npage;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tret = vfio_iommu_map(iommu, iova + dma->size, pfn, npage,\n\t\t\t\t     dma->prot);\n\t\tif (ret) {\n\t\t\tvfio_unpin_pages_remote(dma, iova + dma->size, pfn,\n\t\t\t\t\t\tnpage, true);\n\t\t\tvfio_batch_unpin(&batch, dma);\n\t\t\tbreak;\n\t\t}\n\n\t\tsize -= npage << PAGE_SHIFT;\n\t\tdma->size += npage << PAGE_SHIFT;\n\t}\n\n\tvfio_batch_fini(&batch);\n\tdma->iommu_mapped = true;\n\n\tif (ret)\n\t\tvfio_remove_dma(iommu, dma);\n\n\treturn ret;\n}\n\n \nstatic bool vfio_iommu_iova_dma_valid(struct vfio_iommu *iommu,\n\t\t\t\t      dma_addr_t start, dma_addr_t end)\n{\n\tstruct list_head *iova = &iommu->iova_list;\n\tstruct vfio_iova *node;\n\n\tlist_for_each_entry(node, iova, list) {\n\t\tif (start >= node->start && end <= node->end)\n\t\t\treturn true;\n\t}\n\n\t \n\treturn list_empty(iova);\n}\n\nstatic int vfio_change_dma_owner(struct vfio_dma *dma)\n{\n\tstruct task_struct *task = current->group_leader;\n\tstruct mm_struct *mm = current->mm;\n\tlong npage = dma->locked_vm;\n\tbool lock_cap;\n\tint ret;\n\n\tif (mm == dma->mm)\n\t\treturn 0;\n\n\tlock_cap = capable(CAP_IPC_LOCK);\n\tret = mm_lock_acct(task, mm, lock_cap, npage);\n\tif (ret)\n\t\treturn ret;\n\n\tif (mmget_not_zero(dma->mm)) {\n\t\tmm_lock_acct(dma->task, dma->mm, dma->lock_cap, -npage);\n\t\tmmput(dma->mm);\n\t}\n\n\tif (dma->task != task) {\n\t\tput_task_struct(dma->task);\n\t\tdma->task = get_task_struct(task);\n\t}\n\tmmdrop(dma->mm);\n\tdma->mm = mm;\n\tmmgrab(dma->mm);\n\tdma->lock_cap = lock_cap;\n\treturn 0;\n}\n\nstatic int vfio_dma_do_map(struct vfio_iommu *iommu,\n\t\t\t   struct vfio_iommu_type1_dma_map *map)\n{\n\tbool set_vaddr = map->flags & VFIO_DMA_MAP_FLAG_VADDR;\n\tdma_addr_t iova = map->iova;\n\tunsigned long vaddr = map->vaddr;\n\tsize_t size = map->size;\n\tint ret = 0, prot = 0;\n\tsize_t pgsize;\n\tstruct vfio_dma *dma;\n\n\t \n\tif (map->size != size || map->vaddr != vaddr || map->iova != iova)\n\t\treturn -EINVAL;\n\n\t \n\tif (map->flags & VFIO_DMA_MAP_FLAG_WRITE)\n\t\tprot |= IOMMU_WRITE;\n\tif (map->flags & VFIO_DMA_MAP_FLAG_READ)\n\t\tprot |= IOMMU_READ;\n\n\tif ((prot && set_vaddr) || (!prot && !set_vaddr))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&iommu->lock);\n\n\tpgsize = (size_t)1 << __ffs(iommu->pgsize_bitmap);\n\n\tWARN_ON((pgsize - 1) & PAGE_MASK);\n\n\tif (!size || (size | iova | vaddr) & (pgsize - 1)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (iova + size - 1 < iova || vaddr + size - 1 < vaddr) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tdma = vfio_find_dma(iommu, iova, size);\n\tif (set_vaddr) {\n\t\tif (!dma) {\n\t\t\tret = -ENOENT;\n\t\t} else if (!dma->vaddr_invalid || dma->iova != iova ||\n\t\t\t   dma->size != size) {\n\t\t\tret = -EINVAL;\n\t\t} else {\n\t\t\tret = vfio_change_dma_owner(dma);\n\t\t\tif (ret)\n\t\t\t\tgoto out_unlock;\n\t\t\tdma->vaddr = vaddr;\n\t\t\tdma->vaddr_invalid = false;\n\t\t\tiommu->vaddr_invalid_count--;\n\t\t}\n\t\tgoto out_unlock;\n\t} else if (dma) {\n\t\tret = -EEXIST;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!iommu->dma_avail) {\n\t\tret = -ENOSPC;\n\t\tgoto out_unlock;\n\t}\n\n\tif (!vfio_iommu_iova_dma_valid(iommu, iova, iova + size - 1)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tdma = kzalloc(sizeof(*dma), GFP_KERNEL);\n\tif (!dma) {\n\t\tret = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tiommu->dma_avail--;\n\tdma->iova = iova;\n\tdma->vaddr = vaddr;\n\tdma->prot = prot;\n\n\t \n\tget_task_struct(current->group_leader);\n\tdma->task = current->group_leader;\n\tdma->lock_cap = capable(CAP_IPC_LOCK);\n\tdma->mm = current->mm;\n\tmmgrab(dma->mm);\n\n\tdma->pfn_list = RB_ROOT;\n\n\t \n\tvfio_link_dma(iommu, dma);\n\n\t \n\tif (list_empty(&iommu->domain_list))\n\t\tdma->size = size;\n\telse\n\t\tret = vfio_pin_map_dma(iommu, dma, size);\n\n\tif (!ret && iommu->dirty_page_tracking) {\n\t\tret = vfio_dma_bitmap_alloc(dma, pgsize);\n\t\tif (ret)\n\t\t\tvfio_remove_dma(iommu, dma);\n\t}\n\nout_unlock:\n\tmutex_unlock(&iommu->lock);\n\treturn ret;\n}\n\nstatic int vfio_iommu_replay(struct vfio_iommu *iommu,\n\t\t\t     struct vfio_domain *domain)\n{\n\tstruct vfio_batch batch;\n\tstruct vfio_domain *d = NULL;\n\tstruct rb_node *n;\n\tunsigned long limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\tint ret;\n\n\t \n\tif (!list_empty(&iommu->domain_list))\n\t\td = list_first_entry(&iommu->domain_list,\n\t\t\t\t     struct vfio_domain, next);\n\n\tvfio_batch_init(&batch);\n\n\tn = rb_first(&iommu->dma_list);\n\n\tfor (; n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma;\n\t\tdma_addr_t iova;\n\n\t\tdma = rb_entry(n, struct vfio_dma, node);\n\t\tiova = dma->iova;\n\n\t\twhile (iova < dma->iova + dma->size) {\n\t\t\tphys_addr_t phys;\n\t\t\tsize_t size;\n\n\t\t\tif (dma->iommu_mapped) {\n\t\t\t\tphys_addr_t p;\n\t\t\t\tdma_addr_t i;\n\n\t\t\t\tif (WARN_ON(!d)) {  \n\t\t\t\t\tret = -EINVAL;\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\n\t\t\t\tphys = iommu_iova_to_phys(d->domain, iova);\n\n\t\t\t\tif (WARN_ON(!phys)) {\n\t\t\t\t\tiova += PAGE_SIZE;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\tsize = PAGE_SIZE;\n\t\t\t\tp = phys + size;\n\t\t\t\ti = iova + size;\n\t\t\t\twhile (i < dma->iova + dma->size &&\n\t\t\t\t       p == iommu_iova_to_phys(d->domain, i)) {\n\t\t\t\t\tsize += PAGE_SIZE;\n\t\t\t\t\tp += PAGE_SIZE;\n\t\t\t\t\ti += PAGE_SIZE;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tunsigned long pfn;\n\t\t\t\tunsigned long vaddr = dma->vaddr +\n\t\t\t\t\t\t     (iova - dma->iova);\n\t\t\t\tsize_t n = dma->iova + dma->size - iova;\n\t\t\t\tlong npage;\n\n\t\t\t\tnpage = vfio_pin_pages_remote(dma, vaddr,\n\t\t\t\t\t\t\t      n >> PAGE_SHIFT,\n\t\t\t\t\t\t\t      &pfn, limit,\n\t\t\t\t\t\t\t      &batch);\n\t\t\t\tif (npage <= 0) {\n\t\t\t\t\tWARN_ON(!npage);\n\t\t\t\t\tret = (int)npage;\n\t\t\t\t\tgoto unwind;\n\t\t\t\t}\n\n\t\t\t\tphys = pfn << PAGE_SHIFT;\n\t\t\t\tsize = npage << PAGE_SHIFT;\n\t\t\t}\n\n\t\t\tret = iommu_map(domain->domain, iova, phys, size,\n\t\t\t\t\tdma->prot | IOMMU_CACHE, GFP_KERNEL);\n\t\t\tif (ret) {\n\t\t\t\tif (!dma->iommu_mapped) {\n\t\t\t\t\tvfio_unpin_pages_remote(dma, iova,\n\t\t\t\t\t\t\tphys >> PAGE_SHIFT,\n\t\t\t\t\t\t\tsize >> PAGE_SHIFT,\n\t\t\t\t\t\t\ttrue);\n\t\t\t\t\tvfio_batch_unpin(&batch, dma);\n\t\t\t\t}\n\t\t\t\tgoto unwind;\n\t\t\t}\n\n\t\t\tiova += size;\n\t\t}\n\t}\n\n\t \n\tfor (n = rb_first(&iommu->dma_list); n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\n\t\tdma->iommu_mapped = true;\n\t}\n\n\tvfio_batch_fini(&batch);\n\treturn 0;\n\nunwind:\n\tfor (; n; n = rb_prev(n)) {\n\t\tstruct vfio_dma *dma = rb_entry(n, struct vfio_dma, node);\n\t\tdma_addr_t iova;\n\n\t\tif (dma->iommu_mapped) {\n\t\t\tiommu_unmap(domain->domain, dma->iova, dma->size);\n\t\t\tcontinue;\n\t\t}\n\n\t\tiova = dma->iova;\n\t\twhile (iova < dma->iova + dma->size) {\n\t\t\tphys_addr_t phys, p;\n\t\t\tsize_t size;\n\t\t\tdma_addr_t i;\n\n\t\t\tphys = iommu_iova_to_phys(domain->domain, iova);\n\t\t\tif (!phys) {\n\t\t\t\tiova += PAGE_SIZE;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tsize = PAGE_SIZE;\n\t\t\tp = phys + size;\n\t\t\ti = iova + size;\n\t\t\twhile (i < dma->iova + dma->size &&\n\t\t\t       p == iommu_iova_to_phys(domain->domain, i)) {\n\t\t\t\tsize += PAGE_SIZE;\n\t\t\t\tp += PAGE_SIZE;\n\t\t\t\ti += PAGE_SIZE;\n\t\t\t}\n\n\t\t\tiommu_unmap(domain->domain, iova, size);\n\t\t\tvfio_unpin_pages_remote(dma, iova, phys >> PAGE_SHIFT,\n\t\t\t\t\t\tsize >> PAGE_SHIFT, true);\n\t\t}\n\t}\n\n\tvfio_batch_fini(&batch);\n\treturn ret;\n}\n\n \nstatic void vfio_test_domain_fgsp(struct vfio_domain *domain, struct list_head *regions)\n{\n\tint ret, order = get_order(PAGE_SIZE * 2);\n\tstruct vfio_iova *region;\n\tstruct page *pages;\n\tdma_addr_t start;\n\n\tpages = alloc_pages(GFP_KERNEL | __GFP_ZERO, order);\n\tif (!pages)\n\t\treturn;\n\n\tlist_for_each_entry(region, regions, list) {\n\t\tstart = ALIGN(region->start, PAGE_SIZE * 2);\n\t\tif (start >= region->end || (region->end - start < PAGE_SIZE * 2))\n\t\t\tcontinue;\n\n\t\tret = iommu_map(domain->domain, start, page_to_phys(pages), PAGE_SIZE * 2,\n\t\t\t\tIOMMU_READ | IOMMU_WRITE | IOMMU_CACHE, GFP_KERNEL);\n\t\tif (!ret) {\n\t\t\tsize_t unmapped = iommu_unmap(domain->domain, start, PAGE_SIZE);\n\n\t\t\tif (unmapped == PAGE_SIZE)\n\t\t\t\tiommu_unmap(domain->domain, start + PAGE_SIZE, PAGE_SIZE);\n\t\t\telse\n\t\t\t\tdomain->fgsp = true;\n\t\t}\n\t\tbreak;\n\t}\n\n\t__free_pages(pages, order);\n}\n\nstatic struct vfio_iommu_group *find_iommu_group(struct vfio_domain *domain,\n\t\t\t\t\t\t struct iommu_group *iommu_group)\n{\n\tstruct vfio_iommu_group *g;\n\n\tlist_for_each_entry(g, &domain->group_list, next) {\n\t\tif (g->iommu_group == iommu_group)\n\t\t\treturn g;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct vfio_iommu_group*\nvfio_iommu_find_iommu_group(struct vfio_iommu *iommu,\n\t\t\t    struct iommu_group *iommu_group)\n{\n\tstruct vfio_iommu_group *group;\n\tstruct vfio_domain *domain;\n\n\tlist_for_each_entry(domain, &iommu->domain_list, next) {\n\t\tgroup = find_iommu_group(domain, iommu_group);\n\t\tif (group)\n\t\t\treturn group;\n\t}\n\n\tlist_for_each_entry(group, &iommu->emulated_iommu_groups, next)\n\t\tif (group->iommu_group == iommu_group)\n\t\t\treturn group;\n\treturn NULL;\n}\n\nstatic bool vfio_iommu_has_sw_msi(struct list_head *group_resv_regions,\n\t\t\t\t  phys_addr_t *base)\n{\n\tstruct iommu_resv_region *region;\n\tbool ret = false;\n\n\tlist_for_each_entry(region, group_resv_regions, list) {\n\t\t \n\t\tif (region->type == IOMMU_RESV_MSI) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (region->type == IOMMU_RESV_SW_MSI) {\n\t\t\t*base = region->start;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int vfio_iommu_iova_insert(struct list_head *head,\n\t\t\t\t  dma_addr_t start, dma_addr_t end)\n{\n\tstruct vfio_iova *region;\n\n\tregion = kmalloc(sizeof(*region), GFP_KERNEL);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&region->list);\n\tregion->start = start;\n\tregion->end = end;\n\n\tlist_add_tail(&region->list, head);\n\treturn 0;\n}\n\n \nstatic bool vfio_iommu_aper_conflict(struct vfio_iommu *iommu,\n\t\t\t\t     dma_addr_t start, dma_addr_t end)\n{\n\tstruct vfio_iova *first, *last;\n\tstruct list_head *iova = &iommu->iova_list;\n\n\tif (list_empty(iova))\n\t\treturn false;\n\n\t \n\tfirst = list_first_entry(iova, struct vfio_iova, list);\n\tlast = list_last_entry(iova, struct vfio_iova, list);\n\tif (start > last->end || end < first->start)\n\t\treturn true;\n\n\t \n\tif (start > first->start) {\n\t\tif (vfio_find_dma(iommu, first->start, start - first->start))\n\t\t\treturn true;\n\t}\n\n\t \n\tif (end < last->end) {\n\t\tif (vfio_find_dma(iommu, end + 1, last->end - end))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int vfio_iommu_aper_resize(struct list_head *iova,\n\t\t\t\t  dma_addr_t start, dma_addr_t end)\n{\n\tstruct vfio_iova *node, *next;\n\n\tif (list_empty(iova))\n\t\treturn vfio_iommu_iova_insert(iova, start, end);\n\n\t \n\tlist_for_each_entry_safe(node, next, iova, list) {\n\t\tif (start < node->start)\n\t\t\tbreak;\n\t\tif (start >= node->start && start < node->end) {\n\t\t\tnode->start = start;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tlist_del(&node->list);\n\t\tkfree(node);\n\t}\n\n\t \n\tlist_for_each_entry_safe(node, next, iova, list) {\n\t\tif (end > node->end)\n\t\t\tcontinue;\n\t\tif (end > node->start && end <= node->end) {\n\t\t\tnode->end = end;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tlist_del(&node->list);\n\t\tkfree(node);\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool vfio_iommu_resv_conflict(struct vfio_iommu *iommu,\n\t\t\t\t     struct list_head *resv_regions)\n{\n\tstruct iommu_resv_region *region;\n\n\t \n\tlist_for_each_entry(region, resv_regions, list) {\n\t\tif (region->type == IOMMU_RESV_DIRECT_RELAXABLE)\n\t\t\tcontinue;\n\n\t\tif (vfio_find_dma(iommu, region->start, region->length))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int vfio_iommu_resv_exclude(struct list_head *iova,\n\t\t\t\t   struct list_head *resv_regions)\n{\n\tstruct iommu_resv_region *resv;\n\tstruct vfio_iova *n, *next;\n\n\tlist_for_each_entry(resv, resv_regions, list) {\n\t\tphys_addr_t start, end;\n\n\t\tif (resv->type == IOMMU_RESV_DIRECT_RELAXABLE)\n\t\t\tcontinue;\n\n\t\tstart = resv->start;\n\t\tend = resv->start + resv->length - 1;\n\n\t\tlist_for_each_entry_safe(n, next, iova, list) {\n\t\t\tint ret = 0;\n\n\t\t\t \n\t\t\tif (start > n->end || end < n->start)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (start > n->start)\n\t\t\t\tret = vfio_iommu_iova_insert(&n->list, n->start,\n\t\t\t\t\t\t\t     start - 1);\n\t\t\tif (!ret && end < n->end)\n\t\t\t\tret = vfio_iommu_iova_insert(&n->list, end + 1,\n\t\t\t\t\t\t\t     n->end);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\n\t\t\tlist_del(&n->list);\n\t\t\tkfree(n);\n\t\t}\n\t}\n\n\tif (list_empty(iova))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void vfio_iommu_resv_free(struct list_head *resv_regions)\n{\n\tstruct iommu_resv_region *n, *next;\n\n\tlist_for_each_entry_safe(n, next, resv_regions, list) {\n\t\tlist_del(&n->list);\n\t\tkfree(n);\n\t}\n}\n\nstatic void vfio_iommu_iova_free(struct list_head *iova)\n{\n\tstruct vfio_iova *n, *next;\n\n\tlist_for_each_entry_safe(n, next, iova, list) {\n\t\tlist_del(&n->list);\n\t\tkfree(n);\n\t}\n}\n\nstatic int vfio_iommu_iova_get_copy(struct vfio_iommu *iommu,\n\t\t\t\t    struct list_head *iova_copy)\n{\n\tstruct list_head *iova = &iommu->iova_list;\n\tstruct vfio_iova *n;\n\tint ret;\n\n\tlist_for_each_entry(n, iova, list) {\n\t\tret = vfio_iommu_iova_insert(iova_copy, n->start, n->end);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\n\treturn 0;\n\nout_free:\n\tvfio_iommu_iova_free(iova_copy);\n\treturn ret;\n}\n\nstatic void vfio_iommu_iova_insert_copy(struct vfio_iommu *iommu,\n\t\t\t\t\tstruct list_head *iova_copy)\n{\n\tstruct list_head *iova = &iommu->iova_list;\n\n\tvfio_iommu_iova_free(iova);\n\n\tlist_splice_tail(iova_copy, iova);\n}\n\nstatic int vfio_iommu_domain_alloc(struct device *dev, void *data)\n{\n\tstruct iommu_domain **domain = data;\n\n\t*domain = iommu_domain_alloc(dev->bus);\n\treturn 1;  \n}\n\nstatic int vfio_iommu_type1_attach_group(void *iommu_data,\n\t\tstruct iommu_group *iommu_group, enum vfio_group_type type)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tstruct vfio_iommu_group *group;\n\tstruct vfio_domain *domain, *d;\n\tbool resv_msi;\n\tphys_addr_t resv_msi_base = 0;\n\tstruct iommu_domain_geometry *geo;\n\tLIST_HEAD(iova_copy);\n\tLIST_HEAD(group_resv_regions);\n\tint ret = -EBUSY;\n\n\tmutex_lock(&iommu->lock);\n\n\t \n\tif (iommu->vaddr_invalid_count)\n\t\tgoto out_unlock;\n\n\t \n\tret = -EINVAL;\n\tif (vfio_iommu_find_iommu_group(iommu, iommu_group))\n\t\tgoto out_unlock;\n\n\tret = -ENOMEM;\n\tgroup = kzalloc(sizeof(*group), GFP_KERNEL);\n\tif (!group)\n\t\tgoto out_unlock;\n\tgroup->iommu_group = iommu_group;\n\n\tif (type == VFIO_EMULATED_IOMMU) {\n\t\tlist_add(&group->next, &iommu->emulated_iommu_groups);\n\t\t \n\t\tgroup->pinned_page_dirty_scope = true;\n\t\tret = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tret = -ENOMEM;\n\tdomain = kzalloc(sizeof(*domain), GFP_KERNEL);\n\tif (!domain)\n\t\tgoto out_free_group;\n\n\t \n\tret = -EIO;\n\tiommu_group_for_each_dev(iommu_group, &domain->domain,\n\t\t\t\t vfio_iommu_domain_alloc);\n\tif (!domain->domain)\n\t\tgoto out_free_domain;\n\n\tif (iommu->nesting) {\n\t\tret = iommu_enable_nesting(domain->domain);\n\t\tif (ret)\n\t\t\tgoto out_domain;\n\t}\n\n\tret = iommu_attach_group(domain->domain, group->iommu_group);\n\tif (ret)\n\t\tgoto out_domain;\n\n\t \n\tgeo = &domain->domain->geometry;\n\tif (vfio_iommu_aper_conflict(iommu, geo->aperture_start,\n\t\t\t\t     geo->aperture_end)) {\n\t\tret = -EINVAL;\n\t\tgoto out_detach;\n\t}\n\n\tret = iommu_get_group_resv_regions(iommu_group, &group_resv_regions);\n\tif (ret)\n\t\tgoto out_detach;\n\n\tif (vfio_iommu_resv_conflict(iommu, &group_resv_regions)) {\n\t\tret = -EINVAL;\n\t\tgoto out_detach;\n\t}\n\n\t \n\tret = vfio_iommu_iova_get_copy(iommu, &iova_copy);\n\tif (ret)\n\t\tgoto out_detach;\n\n\tret = vfio_iommu_aper_resize(&iova_copy, geo->aperture_start,\n\t\t\t\t     geo->aperture_end);\n\tif (ret)\n\t\tgoto out_detach;\n\n\tret = vfio_iommu_resv_exclude(&iova_copy, &group_resv_regions);\n\tif (ret)\n\t\tgoto out_detach;\n\n\tresv_msi = vfio_iommu_has_sw_msi(&group_resv_regions, &resv_msi_base);\n\n\tINIT_LIST_HEAD(&domain->group_list);\n\tlist_add(&group->next, &domain->group_list);\n\n\tif (!allow_unsafe_interrupts &&\n\t    !iommu_group_has_isolated_msi(iommu_group)) {\n\t\tpr_warn(\"%s: No interrupt remapping support.  Use the module param \\\"allow_unsafe_interrupts\\\" to enable VFIO IOMMU support on this platform\\n\",\n\t\t       __func__);\n\t\tret = -EPERM;\n\t\tgoto out_detach;\n\t}\n\n\t \n\tif (domain->domain->ops->enforce_cache_coherency)\n\t\tdomain->enforce_cache_coherency =\n\t\t\tdomain->domain->ops->enforce_cache_coherency(\n\t\t\t\tdomain->domain);\n\n\t \n\tlist_for_each_entry(d, &iommu->domain_list, next) {\n\t\tif (d->domain->ops == domain->domain->ops &&\n\t\t    d->enforce_cache_coherency ==\n\t\t\t    domain->enforce_cache_coherency) {\n\t\t\tiommu_detach_group(domain->domain, group->iommu_group);\n\t\t\tif (!iommu_attach_group(d->domain,\n\t\t\t\t\t\tgroup->iommu_group)) {\n\t\t\t\tlist_add(&group->next, &d->group_list);\n\t\t\t\tiommu_domain_free(domain->domain);\n\t\t\t\tkfree(domain);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tret = iommu_attach_group(domain->domain,\n\t\t\t\t\t\t group->iommu_group);\n\t\t\tif (ret)\n\t\t\t\tgoto out_domain;\n\t\t}\n\t}\n\n\tvfio_test_domain_fgsp(domain, &iova_copy);\n\n\t \n\tret = vfio_iommu_replay(iommu, domain);\n\tif (ret)\n\t\tgoto out_detach;\n\n\tif (resv_msi) {\n\t\tret = iommu_get_msi_cookie(domain->domain, resv_msi_base);\n\t\tif (ret && ret != -ENODEV)\n\t\t\tgoto out_detach;\n\t}\n\n\tlist_add(&domain->next, &iommu->domain_list);\n\tvfio_update_pgsize_bitmap(iommu);\ndone:\n\t \n\tvfio_iommu_iova_insert_copy(iommu, &iova_copy);\n\n\t \n\tiommu->num_non_pinned_groups++;\n\tmutex_unlock(&iommu->lock);\n\tvfio_iommu_resv_free(&group_resv_regions);\n\n\treturn 0;\n\nout_detach:\n\tiommu_detach_group(domain->domain, group->iommu_group);\nout_domain:\n\tiommu_domain_free(domain->domain);\n\tvfio_iommu_iova_free(&iova_copy);\n\tvfio_iommu_resv_free(&group_resv_regions);\nout_free_domain:\n\tkfree(domain);\nout_free_group:\n\tkfree(group);\nout_unlock:\n\tmutex_unlock(&iommu->lock);\n\treturn ret;\n}\n\nstatic void vfio_iommu_unmap_unpin_all(struct vfio_iommu *iommu)\n{\n\tstruct rb_node *node;\n\n\twhile ((node = rb_first(&iommu->dma_list)))\n\t\tvfio_remove_dma(iommu, rb_entry(node, struct vfio_dma, node));\n}\n\nstatic void vfio_iommu_unmap_unpin_reaccount(struct vfio_iommu *iommu)\n{\n\tstruct rb_node *n, *p;\n\n\tn = rb_first(&iommu->dma_list);\n\tfor (; n; n = rb_next(n)) {\n\t\tstruct vfio_dma *dma;\n\t\tlong locked = 0, unlocked = 0;\n\n\t\tdma = rb_entry(n, struct vfio_dma, node);\n\t\tunlocked += vfio_unmap_unpin(iommu, dma, false);\n\t\tp = rb_first(&dma->pfn_list);\n\t\tfor (; p; p = rb_next(p)) {\n\t\t\tstruct vfio_pfn *vpfn = rb_entry(p, struct vfio_pfn,\n\t\t\t\t\t\t\t node);\n\n\t\t\tif (!is_invalid_reserved_pfn(vpfn->pfn))\n\t\t\t\tlocked++;\n\t\t}\n\t\tvfio_lock_acct(dma, locked - unlocked, true);\n\t}\n}\n\n \nstatic void vfio_iommu_aper_expand(struct vfio_iommu *iommu,\n\t\t\t\t   struct list_head *iova_copy)\n{\n\tstruct vfio_domain *domain;\n\tstruct vfio_iova *node;\n\tdma_addr_t start = 0;\n\tdma_addr_t end = (dma_addr_t)~0;\n\n\tif (list_empty(iova_copy))\n\t\treturn;\n\n\tlist_for_each_entry(domain, &iommu->domain_list, next) {\n\t\tstruct iommu_domain_geometry *geo = &domain->domain->geometry;\n\n\t\tif (geo->aperture_start > start)\n\t\t\tstart = geo->aperture_start;\n\t\tif (geo->aperture_end < end)\n\t\t\tend = geo->aperture_end;\n\t}\n\n\t \n\tnode = list_first_entry(iova_copy, struct vfio_iova, list);\n\tnode->start = start;\n\tnode = list_last_entry(iova_copy, struct vfio_iova, list);\n\tnode->end = end;\n}\n\n \nstatic int vfio_iommu_resv_refresh(struct vfio_iommu *iommu,\n\t\t\t\t   struct list_head *iova_copy)\n{\n\tstruct vfio_domain *d;\n\tstruct vfio_iommu_group *g;\n\tstruct vfio_iova *node;\n\tdma_addr_t start, end;\n\tLIST_HEAD(resv_regions);\n\tint ret;\n\n\tif (list_empty(iova_copy))\n\t\treturn -EINVAL;\n\n\tlist_for_each_entry(d, &iommu->domain_list, next) {\n\t\tlist_for_each_entry(g, &d->group_list, next) {\n\t\t\tret = iommu_get_group_resv_regions(g->iommu_group,\n\t\t\t\t\t\t\t   &resv_regions);\n\t\t\tif (ret)\n\t\t\t\tgoto done;\n\t\t}\n\t}\n\n\tnode = list_first_entry(iova_copy, struct vfio_iova, list);\n\tstart = node->start;\n\tnode = list_last_entry(iova_copy, struct vfio_iova, list);\n\tend = node->end;\n\n\t \n\tvfio_iommu_iova_free(iova_copy);\n\n\tret = vfio_iommu_aper_resize(iova_copy, start, end);\n\tif (ret)\n\t\tgoto done;\n\n\t \n\tret = vfio_iommu_resv_exclude(iova_copy, &resv_regions);\ndone:\n\tvfio_iommu_resv_free(&resv_regions);\n\treturn ret;\n}\n\nstatic void vfio_iommu_type1_detach_group(void *iommu_data,\n\t\t\t\t\t  struct iommu_group *iommu_group)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tstruct vfio_domain *domain;\n\tstruct vfio_iommu_group *group;\n\tbool update_dirty_scope = false;\n\tLIST_HEAD(iova_copy);\n\n\tmutex_lock(&iommu->lock);\n\tlist_for_each_entry(group, &iommu->emulated_iommu_groups, next) {\n\t\tif (group->iommu_group != iommu_group)\n\t\t\tcontinue;\n\t\tupdate_dirty_scope = !group->pinned_page_dirty_scope;\n\t\tlist_del(&group->next);\n\t\tkfree(group);\n\n\t\tif (list_empty(&iommu->emulated_iommu_groups) &&\n\t\t    list_empty(&iommu->domain_list)) {\n\t\t\tWARN_ON(!list_empty(&iommu->device_list));\n\t\t\tvfio_iommu_unmap_unpin_all(iommu);\n\t\t}\n\t\tgoto detach_group_done;\n\t}\n\n\t \n\tvfio_iommu_iova_get_copy(iommu, &iova_copy);\n\n\tlist_for_each_entry(domain, &iommu->domain_list, next) {\n\t\tgroup = find_iommu_group(domain, iommu_group);\n\t\tif (!group)\n\t\t\tcontinue;\n\n\t\tiommu_detach_group(domain->domain, group->iommu_group);\n\t\tupdate_dirty_scope = !group->pinned_page_dirty_scope;\n\t\tlist_del(&group->next);\n\t\tkfree(group);\n\t\t \n\t\tif (list_empty(&domain->group_list)) {\n\t\t\tif (list_is_singular(&iommu->domain_list)) {\n\t\t\t\tif (list_empty(&iommu->emulated_iommu_groups)) {\n\t\t\t\t\tWARN_ON(!list_empty(\n\t\t\t\t\t\t&iommu->device_list));\n\t\t\t\t\tvfio_iommu_unmap_unpin_all(iommu);\n\t\t\t\t} else {\n\t\t\t\t\tvfio_iommu_unmap_unpin_reaccount(iommu);\n\t\t\t\t}\n\t\t\t}\n\t\t\tiommu_domain_free(domain->domain);\n\t\t\tlist_del(&domain->next);\n\t\t\tkfree(domain);\n\t\t\tvfio_iommu_aper_expand(iommu, &iova_copy);\n\t\t\tvfio_update_pgsize_bitmap(iommu);\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (!vfio_iommu_resv_refresh(iommu, &iova_copy))\n\t\tvfio_iommu_iova_insert_copy(iommu, &iova_copy);\n\telse\n\t\tvfio_iommu_iova_free(&iova_copy);\n\ndetach_group_done:\n\t \n\tif (update_dirty_scope) {\n\t\tiommu->num_non_pinned_groups--;\n\t\tif (iommu->dirty_page_tracking)\n\t\t\tvfio_iommu_populate_bitmap_full(iommu);\n\t}\n\tmutex_unlock(&iommu->lock);\n}\n\nstatic void *vfio_iommu_type1_open(unsigned long arg)\n{\n\tstruct vfio_iommu *iommu;\n\n\tiommu = kzalloc(sizeof(*iommu), GFP_KERNEL);\n\tif (!iommu)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tswitch (arg) {\n\tcase VFIO_TYPE1_IOMMU:\n\t\tbreak;\n\tcase VFIO_TYPE1_NESTING_IOMMU:\n\t\tiommu->nesting = true;\n\t\tfallthrough;\n\tcase VFIO_TYPE1v2_IOMMU:\n\t\tiommu->v2 = true;\n\t\tbreak;\n\tdefault:\n\t\tkfree(iommu);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tINIT_LIST_HEAD(&iommu->domain_list);\n\tINIT_LIST_HEAD(&iommu->iova_list);\n\tiommu->dma_list = RB_ROOT;\n\tiommu->dma_avail = dma_entry_limit;\n\tmutex_init(&iommu->lock);\n\tmutex_init(&iommu->device_list_lock);\n\tINIT_LIST_HEAD(&iommu->device_list);\n\tiommu->pgsize_bitmap = PAGE_MASK;\n\tINIT_LIST_HEAD(&iommu->emulated_iommu_groups);\n\n\treturn iommu;\n}\n\nstatic void vfio_release_domain(struct vfio_domain *domain)\n{\n\tstruct vfio_iommu_group *group, *group_tmp;\n\n\tlist_for_each_entry_safe(group, group_tmp,\n\t\t\t\t &domain->group_list, next) {\n\t\tiommu_detach_group(domain->domain, group->iommu_group);\n\t\tlist_del(&group->next);\n\t\tkfree(group);\n\t}\n\n\tiommu_domain_free(domain->domain);\n}\n\nstatic void vfio_iommu_type1_release(void *iommu_data)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tstruct vfio_domain *domain, *domain_tmp;\n\tstruct vfio_iommu_group *group, *next_group;\n\n\tlist_for_each_entry_safe(group, next_group,\n\t\t\t&iommu->emulated_iommu_groups, next) {\n\t\tlist_del(&group->next);\n\t\tkfree(group);\n\t}\n\n\tvfio_iommu_unmap_unpin_all(iommu);\n\n\tlist_for_each_entry_safe(domain, domain_tmp,\n\t\t\t\t &iommu->domain_list, next) {\n\t\tvfio_release_domain(domain);\n\t\tlist_del(&domain->next);\n\t\tkfree(domain);\n\t}\n\n\tvfio_iommu_iova_free(&iommu->iova_list);\n\n\tkfree(iommu);\n}\n\nstatic int vfio_domains_have_enforce_cache_coherency(struct vfio_iommu *iommu)\n{\n\tstruct vfio_domain *domain;\n\tint ret = 1;\n\n\tmutex_lock(&iommu->lock);\n\tlist_for_each_entry(domain, &iommu->domain_list, next) {\n\t\tif (!(domain->enforce_cache_coherency)) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&iommu->lock);\n\n\treturn ret;\n}\n\nstatic bool vfio_iommu_has_emulated(struct vfio_iommu *iommu)\n{\n\tbool ret;\n\n\tmutex_lock(&iommu->lock);\n\tret = !list_empty(&iommu->emulated_iommu_groups);\n\tmutex_unlock(&iommu->lock);\n\treturn ret;\n}\n\nstatic int vfio_iommu_type1_check_extension(struct vfio_iommu *iommu,\n\t\t\t\t\t    unsigned long arg)\n{\n\tswitch (arg) {\n\tcase VFIO_TYPE1_IOMMU:\n\tcase VFIO_TYPE1v2_IOMMU:\n\tcase VFIO_TYPE1_NESTING_IOMMU:\n\tcase VFIO_UNMAP_ALL:\n\t\treturn 1;\n\tcase VFIO_UPDATE_VADDR:\n\t\t \n\t\treturn iommu && !vfio_iommu_has_emulated(iommu);\n\tcase VFIO_DMA_CC_IOMMU:\n\t\tif (!iommu)\n\t\t\treturn 0;\n\t\treturn vfio_domains_have_enforce_cache_coherency(iommu);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic int vfio_iommu_iova_add_cap(struct vfio_info_cap *caps,\n\t\t struct vfio_iommu_type1_info_cap_iova_range *cap_iovas,\n\t\t size_t size)\n{\n\tstruct vfio_info_cap_header *header;\n\tstruct vfio_iommu_type1_info_cap_iova_range *iova_cap;\n\n\theader = vfio_info_cap_add(caps, size,\n\t\t\t\t   VFIO_IOMMU_TYPE1_INFO_CAP_IOVA_RANGE, 1);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tiova_cap = container_of(header,\n\t\t\t\tstruct vfio_iommu_type1_info_cap_iova_range,\n\t\t\t\theader);\n\tiova_cap->nr_iovas = cap_iovas->nr_iovas;\n\tmemcpy(iova_cap->iova_ranges, cap_iovas->iova_ranges,\n\t       cap_iovas->nr_iovas * sizeof(*cap_iovas->iova_ranges));\n\treturn 0;\n}\n\nstatic int vfio_iommu_iova_build_caps(struct vfio_iommu *iommu,\n\t\t\t\t      struct vfio_info_cap *caps)\n{\n\tstruct vfio_iommu_type1_info_cap_iova_range *cap_iovas;\n\tstruct vfio_iova *iova;\n\tsize_t size;\n\tint iovas = 0, i = 0, ret;\n\n\tlist_for_each_entry(iova, &iommu->iova_list, list)\n\t\tiovas++;\n\n\tif (!iovas) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tsize = struct_size(cap_iovas, iova_ranges, iovas);\n\n\tcap_iovas = kzalloc(size, GFP_KERNEL);\n\tif (!cap_iovas)\n\t\treturn -ENOMEM;\n\n\tcap_iovas->nr_iovas = iovas;\n\n\tlist_for_each_entry(iova, &iommu->iova_list, list) {\n\t\tcap_iovas->iova_ranges[i].start = iova->start;\n\t\tcap_iovas->iova_ranges[i].end = iova->end;\n\t\ti++;\n\t}\n\n\tret = vfio_iommu_iova_add_cap(caps, cap_iovas, size);\n\n\tkfree(cap_iovas);\n\treturn ret;\n}\n\nstatic int vfio_iommu_migration_build_caps(struct vfio_iommu *iommu,\n\t\t\t\t\t   struct vfio_info_cap *caps)\n{\n\tstruct vfio_iommu_type1_info_cap_migration cap_mig = {};\n\n\tcap_mig.header.id = VFIO_IOMMU_TYPE1_INFO_CAP_MIGRATION;\n\tcap_mig.header.version = 1;\n\n\tcap_mig.flags = 0;\n\t \n\tcap_mig.pgsize_bitmap = (size_t)1 << __ffs(iommu->pgsize_bitmap);\n\tcap_mig.max_dirty_bitmap_size = DIRTY_BITMAP_SIZE_MAX;\n\n\treturn vfio_info_add_capability(caps, &cap_mig.header, sizeof(cap_mig));\n}\n\nstatic int vfio_iommu_dma_avail_build_caps(struct vfio_iommu *iommu,\n\t\t\t\t\t   struct vfio_info_cap *caps)\n{\n\tstruct vfio_iommu_type1_info_dma_avail cap_dma_avail;\n\n\tcap_dma_avail.header.id = VFIO_IOMMU_TYPE1_INFO_DMA_AVAIL;\n\tcap_dma_avail.header.version = 1;\n\n\tcap_dma_avail.avail = iommu->dma_avail;\n\n\treturn vfio_info_add_capability(caps, &cap_dma_avail.header,\n\t\t\t\t\tsizeof(cap_dma_avail));\n}\n\nstatic int vfio_iommu_type1_get_info(struct vfio_iommu *iommu,\n\t\t\t\t     unsigned long arg)\n{\n\tstruct vfio_iommu_type1_info info = {};\n\tunsigned long minsz;\n\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\tint ret;\n\n\tminsz = offsetofend(struct vfio_iommu_type1_info, iova_pgsizes);\n\n\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tminsz = min_t(size_t, info.argsz, sizeof(info));\n\n\tmutex_lock(&iommu->lock);\n\tinfo.flags = VFIO_IOMMU_INFO_PGSIZES;\n\n\tinfo.iova_pgsizes = iommu->pgsize_bitmap;\n\n\tret = vfio_iommu_migration_build_caps(iommu, &caps);\n\n\tif (!ret)\n\t\tret = vfio_iommu_dma_avail_build_caps(iommu, &caps);\n\n\tif (!ret)\n\t\tret = vfio_iommu_iova_build_caps(iommu, &caps);\n\n\tmutex_unlock(&iommu->lock);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (caps.size) {\n\t\tinfo.flags |= VFIO_IOMMU_INFO_CAPS;\n\n\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t} else {\n\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\tif (copy_to_user((void __user *)arg +\n\t\t\t\t\tsizeof(info), caps.buf,\n\t\t\t\t\tcaps.size)) {\n\t\t\t\tkfree(caps.buf);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tinfo.cap_offset = sizeof(info);\n\t\t}\n\n\t\tkfree(caps.buf);\n\t}\n\n\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t-EFAULT : 0;\n}\n\nstatic int vfio_iommu_type1_map_dma(struct vfio_iommu *iommu,\n\t\t\t\t    unsigned long arg)\n{\n\tstruct vfio_iommu_type1_dma_map map;\n\tunsigned long minsz;\n\tuint32_t mask = VFIO_DMA_MAP_FLAG_READ | VFIO_DMA_MAP_FLAG_WRITE |\n\t\t\tVFIO_DMA_MAP_FLAG_VADDR;\n\n\tminsz = offsetofend(struct vfio_iommu_type1_dma_map, size);\n\n\tif (copy_from_user(&map, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (map.argsz < minsz || map.flags & ~mask)\n\t\treturn -EINVAL;\n\n\treturn vfio_dma_do_map(iommu, &map);\n}\n\nstatic int vfio_iommu_type1_unmap_dma(struct vfio_iommu *iommu,\n\t\t\t\t      unsigned long arg)\n{\n\tstruct vfio_iommu_type1_dma_unmap unmap;\n\tstruct vfio_bitmap bitmap = { 0 };\n\tuint32_t mask = VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP |\n\t\t\tVFIO_DMA_UNMAP_FLAG_VADDR |\n\t\t\tVFIO_DMA_UNMAP_FLAG_ALL;\n\tunsigned long minsz;\n\tint ret;\n\n\tminsz = offsetofend(struct vfio_iommu_type1_dma_unmap, size);\n\n\tif (copy_from_user(&unmap, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (unmap.argsz < minsz || unmap.flags & ~mask)\n\t\treturn -EINVAL;\n\n\tif ((unmap.flags & VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP) &&\n\t    (unmap.flags & (VFIO_DMA_UNMAP_FLAG_ALL |\n\t\t\t    VFIO_DMA_UNMAP_FLAG_VADDR)))\n\t\treturn -EINVAL;\n\n\tif (unmap.flags & VFIO_DMA_UNMAP_FLAG_GET_DIRTY_BITMAP) {\n\t\tunsigned long pgshift;\n\n\t\tif (unmap.argsz < (minsz + sizeof(bitmap)))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&bitmap,\n\t\t\t\t   (void __user *)(arg + minsz),\n\t\t\t\t   sizeof(bitmap)))\n\t\t\treturn -EFAULT;\n\n\t\tif (!access_ok((void __user *)bitmap.data, bitmap.size))\n\t\t\treturn -EINVAL;\n\n\t\tpgshift = __ffs(bitmap.pgsize);\n\t\tret = verify_bitmap_size(unmap.size >> pgshift,\n\t\t\t\t\t bitmap.size);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = vfio_dma_do_unmap(iommu, &unmap, &bitmap);\n\tif (ret)\n\t\treturn ret;\n\n\treturn copy_to_user((void __user *)arg, &unmap, minsz) ?\n\t\t\t-EFAULT : 0;\n}\n\nstatic int vfio_iommu_type1_dirty_pages(struct vfio_iommu *iommu,\n\t\t\t\t\tunsigned long arg)\n{\n\tstruct vfio_iommu_type1_dirty_bitmap dirty;\n\tuint32_t mask = VFIO_IOMMU_DIRTY_PAGES_FLAG_START |\n\t\t\tVFIO_IOMMU_DIRTY_PAGES_FLAG_STOP |\n\t\t\tVFIO_IOMMU_DIRTY_PAGES_FLAG_GET_BITMAP;\n\tunsigned long minsz;\n\tint ret = 0;\n\n\tif (!iommu->v2)\n\t\treturn -EACCES;\n\n\tminsz = offsetofend(struct vfio_iommu_type1_dirty_bitmap, flags);\n\n\tif (copy_from_user(&dirty, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (dirty.argsz < minsz || dirty.flags & ~mask)\n\t\treturn -EINVAL;\n\n\t \n\tif (__ffs(dirty.flags) != __fls(dirty.flags))\n\t\treturn -EINVAL;\n\n\tif (dirty.flags & VFIO_IOMMU_DIRTY_PAGES_FLAG_START) {\n\t\tsize_t pgsize;\n\n\t\tmutex_lock(&iommu->lock);\n\t\tpgsize = 1 << __ffs(iommu->pgsize_bitmap);\n\t\tif (!iommu->dirty_page_tracking) {\n\t\t\tret = vfio_dma_bitmap_alloc_all(iommu, pgsize);\n\t\t\tif (!ret)\n\t\t\t\tiommu->dirty_page_tracking = true;\n\t\t}\n\t\tmutex_unlock(&iommu->lock);\n\t\treturn ret;\n\t} else if (dirty.flags & VFIO_IOMMU_DIRTY_PAGES_FLAG_STOP) {\n\t\tmutex_lock(&iommu->lock);\n\t\tif (iommu->dirty_page_tracking) {\n\t\t\tiommu->dirty_page_tracking = false;\n\t\t\tvfio_dma_bitmap_free_all(iommu);\n\t\t}\n\t\tmutex_unlock(&iommu->lock);\n\t\treturn 0;\n\t} else if (dirty.flags & VFIO_IOMMU_DIRTY_PAGES_FLAG_GET_BITMAP) {\n\t\tstruct vfio_iommu_type1_dirty_bitmap_get range;\n\t\tunsigned long pgshift;\n\t\tsize_t data_size = dirty.argsz - minsz;\n\t\tsize_t iommu_pgsize;\n\n\t\tif (!data_size || data_size < sizeof(range))\n\t\t\treturn -EINVAL;\n\n\t\tif (copy_from_user(&range, (void __user *)(arg + minsz),\n\t\t\t\t   sizeof(range)))\n\t\t\treturn -EFAULT;\n\n\t\tif (range.iova + range.size < range.iova)\n\t\t\treturn -EINVAL;\n\t\tif (!access_ok((void __user *)range.bitmap.data,\n\t\t\t       range.bitmap.size))\n\t\t\treturn -EINVAL;\n\n\t\tpgshift = __ffs(range.bitmap.pgsize);\n\t\tret = verify_bitmap_size(range.size >> pgshift,\n\t\t\t\t\t range.bitmap.size);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tmutex_lock(&iommu->lock);\n\n\t\tiommu_pgsize = (size_t)1 << __ffs(iommu->pgsize_bitmap);\n\n\t\t \n\t\tif (range.bitmap.pgsize != iommu_pgsize) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (range.iova & (iommu_pgsize - 1)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (!range.size || range.size & (iommu_pgsize - 1)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (iommu->dirty_page_tracking)\n\t\t\tret = vfio_iova_dirty_bitmap(range.bitmap.data,\n\t\t\t\t\t\t     iommu, range.iova,\n\t\t\t\t\t\t     range.size,\n\t\t\t\t\t\t     range.bitmap.pgsize);\n\t\telse\n\t\t\tret = -EINVAL;\nout_unlock:\n\t\tmutex_unlock(&iommu->lock);\n\n\t\treturn ret;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic long vfio_iommu_type1_ioctl(void *iommu_data,\n\t\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\n\tswitch (cmd) {\n\tcase VFIO_CHECK_EXTENSION:\n\t\treturn vfio_iommu_type1_check_extension(iommu, arg);\n\tcase VFIO_IOMMU_GET_INFO:\n\t\treturn vfio_iommu_type1_get_info(iommu, arg);\n\tcase VFIO_IOMMU_MAP_DMA:\n\t\treturn vfio_iommu_type1_map_dma(iommu, arg);\n\tcase VFIO_IOMMU_UNMAP_DMA:\n\t\treturn vfio_iommu_type1_unmap_dma(iommu, arg);\n\tcase VFIO_IOMMU_DIRTY_PAGES:\n\t\treturn vfio_iommu_type1_dirty_pages(iommu, arg);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}\n\nstatic void vfio_iommu_type1_register_device(void *iommu_data,\n\t\t\t\t\t     struct vfio_device *vdev)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\n\tif (!vdev->ops->dma_unmap)\n\t\treturn;\n\n\t \n\tmutex_lock(&iommu->lock);\n\tmutex_lock(&iommu->device_list_lock);\n\tlist_add(&vdev->iommu_entry, &iommu->device_list);\n\tmutex_unlock(&iommu->device_list_lock);\n\tmutex_unlock(&iommu->lock);\n}\n\nstatic void vfio_iommu_type1_unregister_device(void *iommu_data,\n\t\t\t\t\t       struct vfio_device *vdev)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\n\tif (!vdev->ops->dma_unmap)\n\t\treturn;\n\n\tmutex_lock(&iommu->lock);\n\tmutex_lock(&iommu->device_list_lock);\n\tlist_del(&vdev->iommu_entry);\n\tmutex_unlock(&iommu->device_list_lock);\n\tmutex_unlock(&iommu->lock);\n}\n\nstatic int vfio_iommu_type1_dma_rw_chunk(struct vfio_iommu *iommu,\n\t\t\t\t\t dma_addr_t user_iova, void *data,\n\t\t\t\t\t size_t count, bool write,\n\t\t\t\t\t size_t *copied)\n{\n\tstruct mm_struct *mm;\n\tunsigned long vaddr;\n\tstruct vfio_dma *dma;\n\tbool kthread = current->mm == NULL;\n\tsize_t offset;\n\n\t*copied = 0;\n\n\tdma = vfio_find_dma(iommu, user_iova, 1);\n\tif (!dma)\n\t\treturn -EINVAL;\n\n\tif ((write && !(dma->prot & IOMMU_WRITE)) ||\n\t\t\t!(dma->prot & IOMMU_READ))\n\t\treturn -EPERM;\n\n\tmm = dma->mm;\n\tif (!mmget_not_zero(mm))\n\t\treturn -EPERM;\n\n\tif (kthread)\n\t\tkthread_use_mm(mm);\n\telse if (current->mm != mm)\n\t\tgoto out;\n\n\toffset = user_iova - dma->iova;\n\n\tif (count > dma->size - offset)\n\t\tcount = dma->size - offset;\n\n\tvaddr = dma->vaddr + offset;\n\n\tif (write) {\n\t\t*copied = copy_to_user((void __user *)vaddr, data,\n\t\t\t\t\t count) ? 0 : count;\n\t\tif (*copied && iommu->dirty_page_tracking) {\n\t\t\tunsigned long pgshift = __ffs(iommu->pgsize_bitmap);\n\t\t\t \n\t\t\tbitmap_set(dma->bitmap, offset >> pgshift,\n\t\t\t\t   ((offset + *copied - 1) >> pgshift) -\n\t\t\t\t   (offset >> pgshift) + 1);\n\t\t}\n\t} else\n\t\t*copied = copy_from_user(data, (void __user *)vaddr,\n\t\t\t\t\t   count) ? 0 : count;\n\tif (kthread)\n\t\tkthread_unuse_mm(mm);\nout:\n\tmmput(mm);\n\treturn *copied ? 0 : -EFAULT;\n}\n\nstatic int vfio_iommu_type1_dma_rw(void *iommu_data, dma_addr_t user_iova,\n\t\t\t\t   void *data, size_t count, bool write)\n{\n\tstruct vfio_iommu *iommu = iommu_data;\n\tint ret = 0;\n\tsize_t done;\n\n\tmutex_lock(&iommu->lock);\n\n\tif (WARN_ONCE(iommu->vaddr_invalid_count,\n\t\t      \"vfio_dma_rw not allowed with VFIO_UPDATE_VADDR\\n\")) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\twhile (count > 0) {\n\t\tret = vfio_iommu_type1_dma_rw_chunk(iommu, user_iova, data,\n\t\t\t\t\t\t    count, write, &done);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tcount -= done;\n\t\tdata += done;\n\t\tuser_iova += done;\n\t}\n\nout:\n\tmutex_unlock(&iommu->lock);\n\treturn ret;\n}\n\nstatic struct iommu_domain *\nvfio_iommu_type1_group_iommu_domain(void *iommu_data,\n\t\t\t\t    struct iommu_group *iommu_group)\n{\n\tstruct iommu_domain *domain = ERR_PTR(-ENODEV);\n\tstruct vfio_iommu *iommu = iommu_data;\n\tstruct vfio_domain *d;\n\n\tif (!iommu || !iommu_group)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tmutex_lock(&iommu->lock);\n\tlist_for_each_entry(d, &iommu->domain_list, next) {\n\t\tif (find_iommu_group(d, iommu_group)) {\n\t\t\tdomain = d->domain;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&iommu->lock);\n\n\treturn domain;\n}\n\nstatic const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {\n\t.name\t\t\t= \"vfio-iommu-type1\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.open\t\t\t= vfio_iommu_type1_open,\n\t.release\t\t= vfio_iommu_type1_release,\n\t.ioctl\t\t\t= vfio_iommu_type1_ioctl,\n\t.attach_group\t\t= vfio_iommu_type1_attach_group,\n\t.detach_group\t\t= vfio_iommu_type1_detach_group,\n\t.pin_pages\t\t= vfio_iommu_type1_pin_pages,\n\t.unpin_pages\t\t= vfio_iommu_type1_unpin_pages,\n\t.register_device\t= vfio_iommu_type1_register_device,\n\t.unregister_device\t= vfio_iommu_type1_unregister_device,\n\t.dma_rw\t\t\t= vfio_iommu_type1_dma_rw,\n\t.group_iommu_domain\t= vfio_iommu_type1_group_iommu_domain,\n};\n\nstatic int __init vfio_iommu_type1_init(void)\n{\n\treturn vfio_register_iommu_driver(&vfio_iommu_driver_ops_type1);\n}\n\nstatic void __exit vfio_iommu_type1_cleanup(void)\n{\n\tvfio_unregister_iommu_driver(&vfio_iommu_driver_ops_type1);\n}\n\nmodule_init(vfio_iommu_type1_init);\nmodule_exit(vfio_iommu_type1_cleanup);\n\nMODULE_VERSION(DRIVER_VERSION);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(DRIVER_AUTHOR);\nMODULE_DESCRIPTION(DRIVER_DESC);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}