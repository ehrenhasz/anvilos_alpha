{
  "module_name": "vfio_pci_core.c",
  "hash_id": "a7805c8bd978274cdaedbb36da6c683e5a9241856fa3c8fc76eb2e6513b111f0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/pci/vfio_pci_core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/aperture.h>\n#include <linux/device.h>\n#include <linux/eventfd.h>\n#include <linux/file.h>\n#include <linux/interrupt.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/notifier.h>\n#include <linux/pci.h>\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/vgaarb.h>\n#include <linux/nospec.h>\n#include <linux/sched/mm.h>\n#include <linux/iommufd.h>\n#if IS_ENABLED(CONFIG_EEH)\n#include <asm/eeh.h>\n#endif\n\n#include \"vfio_pci_priv.h\"\n\n#define DRIVER_AUTHOR   \"Alex Williamson <alex.williamson@redhat.com>\"\n#define DRIVER_DESC \"core driver for VFIO based PCI devices\"\n\nstatic bool nointxmask;\nstatic bool disable_vga;\nstatic bool disable_idle_d3;\n\n \nstatic DEFINE_MUTEX(vfio_pci_sriov_pfs_mutex);\nstatic LIST_HEAD(vfio_pci_sriov_pfs);\n\nstruct vfio_pci_dummy_resource {\n\tstruct resource\t\tresource;\n\tint\t\t\tindex;\n\tstruct list_head\tres_next;\n};\n\nstruct vfio_pci_vf_token {\n\tstruct mutex\t\tlock;\n\tuuid_t\t\t\tuuid;\n\tint\t\t\tusers;\n};\n\nstruct vfio_pci_mmap_vma {\n\tstruct vm_area_struct\t*vma;\n\tstruct list_head\tvma_next;\n};\n\nstatic inline bool vfio_vga_disabled(void)\n{\n#ifdef CONFIG_VFIO_PCI_VGA\n\treturn disable_vga;\n#else\n\treturn true;\n#endif\n}\n\n \nstatic unsigned int vfio_pci_set_decode(struct pci_dev *pdev, bool single_vga)\n{\n\tstruct pci_dev *tmp = NULL;\n\tunsigned char max_busnr;\n\tunsigned int decodes;\n\n\tif (single_vga || !vfio_vga_disabled() || pci_is_root_bus(pdev->bus))\n\t\treturn VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t       VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\n\tmax_busnr = pci_bus_max_busnr(pdev->bus);\n\tdecodes = VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM;\n\n\twhile ((tmp = pci_get_class(PCI_CLASS_DISPLAY_VGA << 8, tmp)) != NULL) {\n\t\tif (tmp == pdev ||\n\t\t    pci_domain_nr(tmp->bus) != pci_domain_nr(pdev->bus) ||\n\t\t    pci_is_root_bus(tmp->bus))\n\t\t\tcontinue;\n\n\t\tif (tmp->bus->number >= pdev->bus->number &&\n\t\t    tmp->bus->number <= max_busnr) {\n\t\t\tpci_dev_put(tmp);\n\t\t\tdecodes |= VGA_RSRC_LEGACY_IO | VGA_RSRC_LEGACY_MEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn decodes;\n}\n\nstatic void vfio_pci_probe_mmaps(struct vfio_pci_core_device *vdev)\n{\n\tstruct resource *res;\n\tint i;\n\tstruct vfio_pci_dummy_resource *dummy_res;\n\n\tfor (i = 0; i < PCI_STD_NUM_BARS; i++) {\n\t\tint bar = i + PCI_STD_RESOURCES;\n\n\t\tres = &vdev->pdev->resource[bar];\n\n\t\tif (!IS_ENABLED(CONFIG_VFIO_PCI_MMAP))\n\t\t\tgoto no_mmap;\n\n\t\tif (!(res->flags & IORESOURCE_MEM))\n\t\t\tgoto no_mmap;\n\n\t\t \n\t\tif (!resource_size(res))\n\t\t\tgoto no_mmap;\n\n\t\tif (resource_size(res) >= PAGE_SIZE) {\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!(res->start & ~PAGE_MASK)) {\n\t\t\t \n\t\t\tdummy_res =\n\t\t\t\tkzalloc(sizeof(*dummy_res), GFP_KERNEL_ACCOUNT);\n\t\t\tif (dummy_res == NULL)\n\t\t\t\tgoto no_mmap;\n\n\t\t\tdummy_res->resource.name = \"vfio sub-page reserved\";\n\t\t\tdummy_res->resource.start = res->end + 1;\n\t\t\tdummy_res->resource.end = res->start + PAGE_SIZE - 1;\n\t\t\tdummy_res->resource.flags = res->flags;\n\t\t\tif (request_resource(res->parent,\n\t\t\t\t\t\t&dummy_res->resource)) {\n\t\t\t\tkfree(dummy_res);\n\t\t\t\tgoto no_mmap;\n\t\t\t}\n\t\t\tdummy_res->index = bar;\n\t\t\tlist_add(&dummy_res->res_next,\n\t\t\t\t\t&vdev->dummy_resources_list);\n\t\t\tvdev->bar_mmap_supported[bar] = true;\n\t\t\tcontinue;\n\t\t}\n\t\t \nno_mmap:\n\t\tvdev->bar_mmap_supported[bar] = false;\n\t}\n}\n\nstruct vfio_pci_group_info;\nstatic void vfio_pci_dev_set_try_reset(struct vfio_device_set *dev_set);\nstatic int vfio_pci_dev_set_hot_reset(struct vfio_device_set *dev_set,\n\t\t\t\t      struct vfio_pci_group_info *groups,\n\t\t\t\t      struct iommufd_ctx *iommufd_ctx);\n\n \nstatic bool vfio_pci_nointx(struct pci_dev *pdev)\n{\n\tswitch (pdev->vendor) {\n\tcase PCI_VENDOR_ID_INTEL:\n\t\tswitch (pdev->device) {\n\t\t \n\t\tcase 0x1572:\n\t\tcase 0x1574:\n\t\tcase 0x1580 ... 0x1581:\n\t\tcase 0x1583 ... 0x158b:\n\t\tcase 0x37d0 ... 0x37d2:\n\t\t \n\t\tcase 0x1563:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic void vfio_pci_probe_power_state(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tu16 pmcsr;\n\n\tif (!pdev->pm_cap)\n\t\treturn;\n\n\tpci_read_config_word(pdev, pdev->pm_cap + PCI_PM_CTRL, &pmcsr);\n\n\tvdev->needs_pm_restore = !(pmcsr & PCI_PM_CTRL_NO_SOFT_RESET);\n}\n\n \nint vfio_pci_set_power_state(struct vfio_pci_core_device *vdev, pci_power_t state)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tbool needs_restore = false, needs_save = false;\n\tint ret;\n\n\t \n\tif (pci_num_vf(pdev) && state > PCI_D0)\n\t\treturn -EBUSY;\n\n\tif (vdev->needs_pm_restore) {\n\t\tif (pdev->current_state < PCI_D3hot && state >= PCI_D3hot) {\n\t\t\tpci_save_state(pdev);\n\t\t\tneeds_save = true;\n\t\t}\n\n\t\tif (pdev->current_state >= PCI_D3hot && state <= PCI_D0)\n\t\t\tneeds_restore = true;\n\t}\n\n\tret = pci_set_power_state(pdev, state);\n\n\tif (!ret) {\n\t\t \n\t\tif (needs_save && pdev->current_state >= PCI_D3hot) {\n\t\t\t \n\t\t\tkfree(vdev->pm_save);\n\t\t\tvdev->pm_save = pci_store_saved_state(pdev);\n\t\t} else if (needs_restore) {\n\t\t\tpci_load_and_free_saved_state(pdev, &vdev->pm_save);\n\t\t\tpci_restore_state(pdev);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int vfio_pci_runtime_pm_entry(struct vfio_pci_core_device *vdev,\n\t\t\t\t     struct eventfd_ctx *efdctx)\n{\n\t \n\tvfio_pci_zap_and_down_write_memory_lock(vdev);\n\tif (vdev->pm_runtime_engaged) {\n\t\tup_write(&vdev->memory_lock);\n\t\treturn -EINVAL;\n\t}\n\n\tvdev->pm_runtime_engaged = true;\n\tvdev->pm_wake_eventfd_ctx = efdctx;\n\tpm_runtime_put_noidle(&vdev->pdev->dev);\n\tup_write(&vdev->memory_lock);\n\n\treturn 0;\n}\n\nstatic int vfio_pci_core_pm_entry(struct vfio_device *device, u32 flags,\n\t\t\t\t  void __user *arg, size_t argsz)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(device, struct vfio_pci_core_device, vdev);\n\tint ret;\n\n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_SET, 0);\n\tif (ret != 1)\n\t\treturn ret;\n\n\t \n\treturn vfio_pci_runtime_pm_entry(vdev, NULL);\n}\n\nstatic int vfio_pci_core_pm_entry_with_wakeup(\n\tstruct vfio_device *device, u32 flags,\n\tstruct vfio_device_low_power_entry_with_wakeup __user *arg,\n\tsize_t argsz)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(device, struct vfio_pci_core_device, vdev);\n\tstruct vfio_device_low_power_entry_with_wakeup entry;\n\tstruct eventfd_ctx *efdctx;\n\tint ret;\n\n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_SET,\n\t\t\t\t sizeof(entry));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tif (copy_from_user(&entry, arg, sizeof(entry)))\n\t\treturn -EFAULT;\n\n\tif (entry.wakeup_eventfd < 0)\n\t\treturn -EINVAL;\n\n\tefdctx = eventfd_ctx_fdget(entry.wakeup_eventfd);\n\tif (IS_ERR(efdctx))\n\t\treturn PTR_ERR(efdctx);\n\n\tret = vfio_pci_runtime_pm_entry(vdev, efdctx);\n\tif (ret)\n\t\teventfd_ctx_put(efdctx);\n\n\treturn ret;\n}\n\nstatic void __vfio_pci_runtime_pm_exit(struct vfio_pci_core_device *vdev)\n{\n\tif (vdev->pm_runtime_engaged) {\n\t\tvdev->pm_runtime_engaged = false;\n\t\tpm_runtime_get_noresume(&vdev->pdev->dev);\n\n\t\tif (vdev->pm_wake_eventfd_ctx) {\n\t\t\teventfd_ctx_put(vdev->pm_wake_eventfd_ctx);\n\t\t\tvdev->pm_wake_eventfd_ctx = NULL;\n\t\t}\n\t}\n}\n\nstatic void vfio_pci_runtime_pm_exit(struct vfio_pci_core_device *vdev)\n{\n\t \n\tdown_write(&vdev->memory_lock);\n\t__vfio_pci_runtime_pm_exit(vdev);\n\tup_write(&vdev->memory_lock);\n}\n\nstatic int vfio_pci_core_pm_exit(struct vfio_device *device, u32 flags,\n\t\t\t\t void __user *arg, size_t argsz)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(device, struct vfio_pci_core_device, vdev);\n\tint ret;\n\n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_SET, 0);\n\tif (ret != 1)\n\t\treturn ret;\n\n\t \n\tvfio_pci_runtime_pm_exit(vdev);\n\treturn 0;\n}\n\n#ifdef CONFIG_PM\nstatic int vfio_pci_core_runtime_suspend(struct device *dev)\n{\n\tstruct vfio_pci_core_device *vdev = dev_get_drvdata(dev);\n\n\tdown_write(&vdev->memory_lock);\n\t \n\tvfio_pci_set_power_state(vdev, PCI_D0);\n\tup_write(&vdev->memory_lock);\n\n\t \n\tvdev->pm_intx_masked = ((vdev->irq_type == VFIO_PCI_INTX_IRQ_INDEX) &&\n\t\t\t\tvfio_pci_intx_mask(vdev));\n\n\treturn 0;\n}\n\nstatic int vfio_pci_core_runtime_resume(struct device *dev)\n{\n\tstruct vfio_pci_core_device *vdev = dev_get_drvdata(dev);\n\n\t \n\tdown_write(&vdev->memory_lock);\n\tif (vdev->pm_wake_eventfd_ctx) {\n\t\teventfd_signal(vdev->pm_wake_eventfd_ctx, 1);\n\t\t__vfio_pci_runtime_pm_exit(vdev);\n\t}\n\tup_write(&vdev->memory_lock);\n\n\tif (vdev->pm_intx_masked)\n\t\tvfio_pci_intx_unmask(vdev);\n\n\treturn 0;\n}\n#endif  \n\n \nstatic const struct dev_pm_ops vfio_pci_core_pm_ops = {\n\tSET_RUNTIME_PM_OPS(vfio_pci_core_runtime_suspend,\n\t\t\t   vfio_pci_core_runtime_resume,\n\t\t\t   NULL)\n};\n\nint vfio_pci_core_enable(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint ret;\n\tu16 cmd;\n\tu8 msix_pos;\n\n\tif (!disable_idle_d3) {\n\t\tret = pm_runtime_resume_and_get(&pdev->dev);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\t \n\tpci_clear_master(pdev);\n\n\tret = pci_enable_device(pdev);\n\tif (ret)\n\t\tgoto out_power;\n\n\t \n\tret = pci_try_reset_function(pdev);\n\tif (ret == -EAGAIN)\n\t\tgoto out_disable_device;\n\n\tvdev->reset_works = !ret;\n\tpci_save_state(pdev);\n\tvdev->pci_saved_state = pci_store_saved_state(pdev);\n\tif (!vdev->pci_saved_state)\n\t\tpci_dbg(pdev, \"%s: Couldn't store saved state\\n\", __func__);\n\n\tif (likely(!nointxmask)) {\n\t\tif (vfio_pci_nointx(pdev)) {\n\t\t\tpci_info(pdev, \"Masking broken INTx support\\n\");\n\t\t\tvdev->nointx = true;\n\t\t\tpci_intx(pdev, 0);\n\t\t} else\n\t\t\tvdev->pci_2_3 = pci_intx_mask_supported(pdev);\n\t}\n\n\tpci_read_config_word(pdev, PCI_COMMAND, &cmd);\n\tif (vdev->pci_2_3 && (cmd & PCI_COMMAND_INTX_DISABLE)) {\n\t\tcmd &= ~PCI_COMMAND_INTX_DISABLE;\n\t\tpci_write_config_word(pdev, PCI_COMMAND, cmd);\n\t}\n\n\tret = vfio_pci_zdev_open_device(vdev);\n\tif (ret)\n\t\tgoto out_free_state;\n\n\tret = vfio_config_init(vdev);\n\tif (ret)\n\t\tgoto out_free_zdev;\n\n\tmsix_pos = pdev->msix_cap;\n\tif (msix_pos) {\n\t\tu16 flags;\n\t\tu32 table;\n\n\t\tpci_read_config_word(pdev, msix_pos + PCI_MSIX_FLAGS, &flags);\n\t\tpci_read_config_dword(pdev, msix_pos + PCI_MSIX_TABLE, &table);\n\n\t\tvdev->msix_bar = table & PCI_MSIX_TABLE_BIR;\n\t\tvdev->msix_offset = table & PCI_MSIX_TABLE_OFFSET;\n\t\tvdev->msix_size = ((flags & PCI_MSIX_FLAGS_QSIZE) + 1) * 16;\n\t\tvdev->has_dyn_msix = pci_msix_can_alloc_dyn(pdev);\n\t} else {\n\t\tvdev->msix_bar = 0xFF;\n\t\tvdev->has_dyn_msix = false;\n\t}\n\n\tif (!vfio_vga_disabled() && vfio_pci_is_vga(pdev))\n\t\tvdev->has_vga = true;\n\n\n\treturn 0;\n\nout_free_zdev:\n\tvfio_pci_zdev_close_device(vdev);\nout_free_state:\n\tkfree(vdev->pci_saved_state);\n\tvdev->pci_saved_state = NULL;\nout_disable_device:\n\tpci_disable_device(pdev);\nout_power:\n\tif (!disable_idle_d3)\n\t\tpm_runtime_put(&pdev->dev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_enable);\n\nvoid vfio_pci_core_disable(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct vfio_pci_dummy_resource *dummy_res, *tmp;\n\tstruct vfio_pci_ioeventfd *ioeventfd, *ioeventfd_tmp;\n\tint i, bar;\n\n\t \n\tlockdep_assert_held(&vdev->vdev.dev_set->lock);\n\n\t \n\tvfio_pci_runtime_pm_exit(vdev);\n\tpm_runtime_resume(&pdev->dev);\n\n\t \n\tvfio_pci_set_power_state(vdev, PCI_D0);\n\n\t \n\tpci_clear_master(pdev);\n\n\tvfio_pci_set_irqs_ioctl(vdev, VFIO_IRQ_SET_DATA_NONE |\n\t\t\t\tVFIO_IRQ_SET_ACTION_TRIGGER,\n\t\t\t\tvdev->irq_type, 0, 0, NULL);\n\n\t \n\tlist_for_each_entry_safe(ioeventfd, ioeventfd_tmp,\n\t\t\t\t &vdev->ioeventfds_list, next) {\n\t\tvfio_virqfd_disable(&ioeventfd->virqfd);\n\t\tlist_del(&ioeventfd->next);\n\t\tkfree(ioeventfd);\n\t}\n\tvdev->ioeventfds_nr = 0;\n\n\tvdev->virq_disabled = false;\n\n\tfor (i = 0; i < vdev->num_regions; i++)\n\t\tvdev->region[i].ops->release(vdev, &vdev->region[i]);\n\n\tvdev->num_regions = 0;\n\tkfree(vdev->region);\n\tvdev->region = NULL;  \n\n\tvfio_config_free(vdev);\n\n\tfor (i = 0; i < PCI_STD_NUM_BARS; i++) {\n\t\tbar = i + PCI_STD_RESOURCES;\n\t\tif (!vdev->barmap[bar])\n\t\t\tcontinue;\n\t\tpci_iounmap(pdev, vdev->barmap[bar]);\n\t\tpci_release_selected_regions(pdev, 1 << bar);\n\t\tvdev->barmap[bar] = NULL;\n\t}\n\n\tlist_for_each_entry_safe(dummy_res, tmp,\n\t\t\t\t &vdev->dummy_resources_list, res_next) {\n\t\tlist_del(&dummy_res->res_next);\n\t\trelease_resource(&dummy_res->resource);\n\t\tkfree(dummy_res);\n\t}\n\n\tvdev->needs_reset = true;\n\n\tvfio_pci_zdev_close_device(vdev);\n\n\t \n\tif (pci_load_and_free_saved_state(pdev, &vdev->pci_saved_state)) {\n\t\tpci_info(pdev, \"%s: Couldn't reload saved state\\n\", __func__);\n\n\t\tif (!vdev->reset_works)\n\t\t\tgoto out;\n\n\t\tpci_save_state(pdev);\n\t}\n\n\t \n\tpci_write_config_word(pdev, PCI_COMMAND, PCI_COMMAND_INTX_DISABLE);\n\n\t \n\tif (vdev->reset_works && pci_dev_trylock(pdev)) {\n\t\tif (!__pci_reset_function_locked(pdev))\n\t\t\tvdev->needs_reset = false;\n\t\tpci_dev_unlock(pdev);\n\t}\n\n\tpci_restore_state(pdev);\nout:\n\tpci_disable_device(pdev);\n\n\tvfio_pci_dev_set_try_reset(vdev->vdev.dev_set);\n\n\t \n\tif (!disable_idle_d3)\n\t\tpm_runtime_put(&pdev->dev);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_disable);\n\nvoid vfio_pci_core_close_device(struct vfio_device *core_vdev)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tif (vdev->sriov_pf_core_dev) {\n\t\tmutex_lock(&vdev->sriov_pf_core_dev->vf_token->lock);\n\t\tWARN_ON(!vdev->sriov_pf_core_dev->vf_token->users);\n\t\tvdev->sriov_pf_core_dev->vf_token->users--;\n\t\tmutex_unlock(&vdev->sriov_pf_core_dev->vf_token->lock);\n\t}\n#if IS_ENABLED(CONFIG_EEH)\n\teeh_dev_release(vdev->pdev);\n#endif\n\tvfio_pci_core_disable(vdev);\n\n\tmutex_lock(&vdev->igate);\n\tif (vdev->err_trigger) {\n\t\teventfd_ctx_put(vdev->err_trigger);\n\t\tvdev->err_trigger = NULL;\n\t}\n\tif (vdev->req_trigger) {\n\t\teventfd_ctx_put(vdev->req_trigger);\n\t\tvdev->req_trigger = NULL;\n\t}\n\tmutex_unlock(&vdev->igate);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_close_device);\n\nvoid vfio_pci_core_finish_enable(struct vfio_pci_core_device *vdev)\n{\n\tvfio_pci_probe_mmaps(vdev);\n#if IS_ENABLED(CONFIG_EEH)\n\teeh_dev_open(vdev->pdev);\n#endif\n\n\tif (vdev->sriov_pf_core_dev) {\n\t\tmutex_lock(&vdev->sriov_pf_core_dev->vf_token->lock);\n\t\tvdev->sriov_pf_core_dev->vf_token->users++;\n\t\tmutex_unlock(&vdev->sriov_pf_core_dev->vf_token->lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_finish_enable);\n\nstatic int vfio_pci_get_irq_count(struct vfio_pci_core_device *vdev, int irq_type)\n{\n\tif (irq_type == VFIO_PCI_INTX_IRQ_INDEX) {\n\t\tu8 pin;\n\n\t\tif (!IS_ENABLED(CONFIG_VFIO_PCI_INTX) ||\n\t\t    vdev->nointx || vdev->pdev->is_virtfn)\n\t\t\treturn 0;\n\n\t\tpci_read_config_byte(vdev->pdev, PCI_INTERRUPT_PIN, &pin);\n\n\t\treturn pin ? 1 : 0;\n\t} else if (irq_type == VFIO_PCI_MSI_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msi_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSI_FLAGS, &flags);\n\t\t\treturn 1 << ((flags & PCI_MSI_FLAGS_QMASK) >> 1);\n\t\t}\n\t} else if (irq_type == VFIO_PCI_MSIX_IRQ_INDEX) {\n\t\tu8 pos;\n\t\tu16 flags;\n\n\t\tpos = vdev->pdev->msix_cap;\n\t\tif (pos) {\n\t\t\tpci_read_config_word(vdev->pdev,\n\t\t\t\t\t     pos + PCI_MSIX_FLAGS, &flags);\n\n\t\t\treturn (flags & PCI_MSIX_FLAGS_QSIZE) + 1;\n\t\t}\n\t} else if (irq_type == VFIO_PCI_ERR_IRQ_INDEX) {\n\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\treturn 1;\n\t} else if (irq_type == VFIO_PCI_REQ_IRQ_INDEX) {\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_count_devs(struct pci_dev *pdev, void *data)\n{\n\t(*(int *)data)++;\n\treturn 0;\n}\n\nstruct vfio_pci_fill_info {\n\tstruct vfio_pci_dependent_device __user *devices;\n\tstruct vfio_pci_dependent_device __user *devices_end;\n\tstruct vfio_device *vdev;\n\tu32 count;\n\tu32 flags;\n};\n\nstatic int vfio_pci_fill_devs(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_dependent_device info = {\n\t\t.segment = pci_domain_nr(pdev->bus),\n\t\t.bus = pdev->bus->number,\n\t\t.devfn = pdev->devfn,\n\t};\n\tstruct vfio_pci_fill_info *fill = data;\n\n\tfill->count++;\n\tif (fill->devices >= fill->devices_end)\n\t\treturn 0;\n\n\tif (fill->flags & VFIO_PCI_HOT_RESET_FLAG_DEV_ID) {\n\t\tstruct iommufd_ctx *iommufd = vfio_iommufd_device_ictx(fill->vdev);\n\t\tstruct vfio_device_set *dev_set = fill->vdev->dev_set;\n\t\tstruct vfio_device *vdev;\n\n\t\t \n\t\tvdev = vfio_find_device_in_devset(dev_set, &pdev->dev);\n\t\tif (!vdev) {\n\t\t\tinfo.devid = VFIO_PCI_DEVID_NOT_OWNED;\n\t\t} else {\n\t\t\tint id = vfio_iommufd_get_dev_id(vdev, iommufd);\n\n\t\t\tif (id > 0)\n\t\t\t\tinfo.devid = id;\n\t\t\telse if (id == -ENOENT)\n\t\t\t\tinfo.devid = VFIO_PCI_DEVID_OWNED;\n\t\t\telse\n\t\t\t\tinfo.devid = VFIO_PCI_DEVID_NOT_OWNED;\n\t\t}\n\t\t \n\t\tif (info.devid == VFIO_PCI_DEVID_NOT_OWNED)\n\t\t\tfill->flags &= ~VFIO_PCI_HOT_RESET_FLAG_DEV_ID_OWNED;\n\t} else {\n\t\tstruct iommu_group *iommu_group;\n\n\t\tiommu_group = iommu_group_get(&pdev->dev);\n\t\tif (!iommu_group)\n\t\t\treturn -EPERM;  \n\n\t\tinfo.group_id = iommu_group_id(iommu_group);\n\t\tiommu_group_put(iommu_group);\n\t}\n\n\tif (copy_to_user(fill->devices, &info, sizeof(info)))\n\t\treturn -EFAULT;\n\tfill->devices++;\n\treturn 0;\n}\n\nstruct vfio_pci_group_info {\n\tint count;\n\tstruct file **files;\n};\n\nstatic bool vfio_pci_dev_below_slot(struct pci_dev *pdev, struct pci_slot *slot)\n{\n\tfor (; pdev; pdev = pdev->bus->self)\n\t\tif (pdev->bus == slot->bus)\n\t\t\treturn (pdev->slot == slot);\n\treturn false;\n}\n\nstruct vfio_pci_walk_info {\n\tint (*fn)(struct pci_dev *pdev, void *data);\n\tvoid *data;\n\tstruct pci_dev *pdev;\n\tbool slot;\n\tint ret;\n};\n\nstatic int vfio_pci_walk_wrapper(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_pci_walk_info *walk = data;\n\n\tif (!walk->slot || vfio_pci_dev_below_slot(pdev, walk->pdev->slot))\n\t\twalk->ret = walk->fn(pdev, walk->data);\n\n\treturn walk->ret;\n}\n\nstatic int vfio_pci_for_each_slot_or_bus(struct pci_dev *pdev,\n\t\t\t\t\t int (*fn)(struct pci_dev *,\n\t\t\t\t\t\t   void *data), void *data,\n\t\t\t\t\t bool slot)\n{\n\tstruct vfio_pci_walk_info walk = {\n\t\t.fn = fn, .data = data, .pdev = pdev, .slot = slot, .ret = 0,\n\t};\n\n\tpci_walk_bus(pdev->bus, vfio_pci_walk_wrapper, &walk);\n\n\treturn walk.ret;\n}\n\nstatic int msix_mmappable_cap(struct vfio_pci_core_device *vdev,\n\t\t\t      struct vfio_info_cap *caps)\n{\n\tstruct vfio_info_cap_header header = {\n\t\t.id = VFIO_REGION_INFO_CAP_MSIX_MAPPABLE,\n\t\t.version = 1\n\t};\n\n\treturn vfio_info_add_capability(caps, &header, sizeof(header));\n}\n\nint vfio_pci_core_register_dev_region(struct vfio_pci_core_device *vdev,\n\t\t\t\t      unsigned int type, unsigned int subtype,\n\t\t\t\t      const struct vfio_pci_regops *ops,\n\t\t\t\t      size_t size, u32 flags, void *data)\n{\n\tstruct vfio_pci_region *region;\n\n\tregion = krealloc(vdev->region,\n\t\t\t  (vdev->num_regions + 1) * sizeof(*region),\n\t\t\t  GFP_KERNEL_ACCOUNT);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tvdev->region = region;\n\tvdev->region[vdev->num_regions].type = type;\n\tvdev->region[vdev->num_regions].subtype = subtype;\n\tvdev->region[vdev->num_regions].ops = ops;\n\tvdev->region[vdev->num_regions].size = size;\n\tvdev->region[vdev->num_regions].flags = flags;\n\tvdev->region[vdev->num_regions].data = data;\n\n\tvdev->num_regions++;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_register_dev_region);\n\nstatic int vfio_pci_info_atomic_cap(struct vfio_pci_core_device *vdev,\n\t\t\t\t    struct vfio_info_cap *caps)\n{\n\tstruct vfio_device_info_cap_pci_atomic_comp cap = {\n\t\t.header.id = VFIO_DEVICE_INFO_CAP_PCI_ATOMIC_COMP,\n\t\t.header.version = 1\n\t};\n\tstruct pci_dev *pdev = pci_physfn(vdev->pdev);\n\tu32 devcap2;\n\n\tpcie_capability_read_dword(pdev, PCI_EXP_DEVCAP2, &devcap2);\n\n\tif ((devcap2 & PCI_EXP_DEVCAP2_ATOMIC_COMP32) &&\n\t    !pci_enable_atomic_ops_to_root(pdev, PCI_EXP_DEVCAP2_ATOMIC_COMP32))\n\t\tcap.flags |= VFIO_PCI_ATOMIC_COMP32;\n\n\tif ((devcap2 & PCI_EXP_DEVCAP2_ATOMIC_COMP64) &&\n\t    !pci_enable_atomic_ops_to_root(pdev, PCI_EXP_DEVCAP2_ATOMIC_COMP64))\n\t\tcap.flags |= VFIO_PCI_ATOMIC_COMP64;\n\n\tif ((devcap2 & PCI_EXP_DEVCAP2_ATOMIC_COMP128) &&\n\t    !pci_enable_atomic_ops_to_root(pdev,\n\t\t\t\t\t   PCI_EXP_DEVCAP2_ATOMIC_COMP128))\n\t\tcap.flags |= VFIO_PCI_ATOMIC_COMP128;\n\n\tif (!cap.flags)\n\t\treturn -ENODEV;\n\n\treturn vfio_info_add_capability(caps, &cap.header, sizeof(cap));\n}\n\nstatic int vfio_pci_ioctl_get_info(struct vfio_pci_core_device *vdev,\n\t\t\t\t   struct vfio_device_info __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_device_info, num_irqs);\n\tstruct vfio_device_info info = {};\n\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\tint ret;\n\n\tif (copy_from_user(&info, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tminsz = min_t(size_t, info.argsz, sizeof(info));\n\n\tinfo.flags = VFIO_DEVICE_FLAGS_PCI;\n\n\tif (vdev->reset_works)\n\t\tinfo.flags |= VFIO_DEVICE_FLAGS_RESET;\n\n\tinfo.num_regions = VFIO_PCI_NUM_REGIONS + vdev->num_regions;\n\tinfo.num_irqs = VFIO_PCI_NUM_IRQS;\n\n\tret = vfio_pci_info_zdev_add_caps(vdev, &caps);\n\tif (ret && ret != -ENODEV) {\n\t\tpci_warn(vdev->pdev,\n\t\t\t \"Failed to setup zPCI info capabilities\\n\");\n\t\treturn ret;\n\t}\n\n\tret = vfio_pci_info_atomic_cap(vdev, &caps);\n\tif (ret && ret != -ENODEV) {\n\t\tpci_warn(vdev->pdev,\n\t\t\t \"Failed to setup AtomicOps info capability\\n\");\n\t\treturn ret;\n\t}\n\n\tif (caps.size) {\n\t\tinfo.flags |= VFIO_DEVICE_FLAGS_CAPS;\n\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t} else {\n\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\tif (copy_to_user(arg + 1, caps.buf, caps.size)) {\n\t\t\t\tkfree(caps.buf);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tinfo.cap_offset = sizeof(*arg);\n\t\t}\n\n\t\tkfree(caps.buf);\n\t}\n\n\treturn copy_to_user(arg, &info, minsz) ? -EFAULT : 0;\n}\n\nstatic int vfio_pci_ioctl_get_region_info(struct vfio_pci_core_device *vdev,\n\t\t\t\t\t  struct vfio_region_info __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_region_info, offset);\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct vfio_region_info info;\n\tstruct vfio_info_cap caps = { .buf = NULL, .size = 0 };\n\tint i, ret;\n\n\tif (copy_from_user(&info, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tswitch (info.index) {\n\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\tinfo.size = pdev->cfg_size;\n\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\tbreak;\n\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\tif (!info.size) {\n\t\t\tinfo.flags = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\t\tif (vdev->bar_mmap_supported[info.index]) {\n\t\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_MMAP;\n\t\t\tif (info.index == vdev->msix_bar) {\n\t\t\t\tret = msix_mmappable_cap(vdev, &caps);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tbreak;\n\tcase VFIO_PCI_ROM_REGION_INDEX: {\n\t\tvoid __iomem *io;\n\t\tsize_t size;\n\t\tu16 cmd;\n\n\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\tinfo.flags = 0;\n\n\t\t \n\t\tinfo.size = pci_resource_len(pdev, info.index);\n\t\tif (!info.size) {\n\t\t\t \n\t\t\tif (pdev->resource[PCI_ROM_RESOURCE].flags &\n\t\t\t    IORESOURCE_ROM_SHADOW)\n\t\t\t\tinfo.size = 0x20000;\n\t\t\telse\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tcmd = vfio_pci_memory_lock_and_enable(vdev);\n\t\tio = pci_map_rom(pdev, &size);\n\t\tif (io) {\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ;\n\t\t\tpci_unmap_rom(pdev, io);\n\t\t} else {\n\t\t\tinfo.size = 0;\n\t\t}\n\t\tvfio_pci_memory_unlock_and_restore(vdev, cmd);\n\n\t\tbreak;\n\t}\n\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\tif (!vdev->has_vga)\n\t\t\treturn -EINVAL;\n\n\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\tinfo.size = 0xc0000;\n\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t     VFIO_REGION_INFO_FLAG_WRITE;\n\n\t\tbreak;\n\tdefault: {\n\t\tstruct vfio_region_info_cap_type cap_type = {\n\t\t\t.header.id = VFIO_REGION_INFO_CAP_TYPE,\n\t\t\t.header.version = 1\n\t\t};\n\n\t\tif (info.index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\t\treturn -EINVAL;\n\t\tinfo.index = array_index_nospec(\n\t\t\tinfo.index, VFIO_PCI_NUM_REGIONS + vdev->num_regions);\n\n\t\ti = info.index - VFIO_PCI_NUM_REGIONS;\n\n\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\t\tinfo.size = vdev->region[i].size;\n\t\tinfo.flags = vdev->region[i].flags;\n\n\t\tcap_type.type = vdev->region[i].type;\n\t\tcap_type.subtype = vdev->region[i].subtype;\n\n\t\tret = vfio_info_add_capability(&caps, &cap_type.header,\n\t\t\t\t\t       sizeof(cap_type));\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (vdev->region[i].ops->add_capability) {\n\t\t\tret = vdev->region[i].ops->add_capability(\n\t\t\t\tvdev, &vdev->region[i], &caps);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\t}\n\n\tif (caps.size) {\n\t\tinfo.flags |= VFIO_REGION_INFO_FLAG_CAPS;\n\t\tif (info.argsz < sizeof(info) + caps.size) {\n\t\t\tinfo.argsz = sizeof(info) + caps.size;\n\t\t\tinfo.cap_offset = 0;\n\t\t} else {\n\t\t\tvfio_info_cap_shift(&caps, sizeof(info));\n\t\t\tif (copy_to_user(arg + 1, caps.buf, caps.size)) {\n\t\t\t\tkfree(caps.buf);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tinfo.cap_offset = sizeof(*arg);\n\t\t}\n\n\t\tkfree(caps.buf);\n\t}\n\n\treturn copy_to_user(arg, &info, minsz) ? -EFAULT : 0;\n}\n\nstatic int vfio_pci_ioctl_get_irq_info(struct vfio_pci_core_device *vdev,\n\t\t\t\t       struct vfio_irq_info __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_irq_info, count);\n\tstruct vfio_irq_info info;\n\n\tif (copy_from_user(&info, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (info.argsz < minsz || info.index >= VFIO_PCI_NUM_IRQS)\n\t\treturn -EINVAL;\n\n\tswitch (info.index) {\n\tcase VFIO_PCI_INTX_IRQ_INDEX ... VFIO_PCI_MSIX_IRQ_INDEX:\n\tcase VFIO_PCI_REQ_IRQ_INDEX:\n\t\tbreak;\n\tcase VFIO_PCI_ERR_IRQ_INDEX:\n\t\tif (pci_is_pcie(vdev->pdev))\n\t\t\tbreak;\n\t\tfallthrough;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tinfo.flags = VFIO_IRQ_INFO_EVENTFD;\n\n\tinfo.count = vfio_pci_get_irq_count(vdev, info.index);\n\n\tif (info.index == VFIO_PCI_INTX_IRQ_INDEX)\n\t\tinfo.flags |=\n\t\t\t(VFIO_IRQ_INFO_MASKABLE | VFIO_IRQ_INFO_AUTOMASKED);\n\telse if (info.index != VFIO_PCI_MSIX_IRQ_INDEX || !vdev->has_dyn_msix)\n\t\tinfo.flags |= VFIO_IRQ_INFO_NORESIZE;\n\n\treturn copy_to_user(arg, &info, minsz) ? -EFAULT : 0;\n}\n\nstatic int vfio_pci_ioctl_set_irqs(struct vfio_pci_core_device *vdev,\n\t\t\t\t   struct vfio_irq_set __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_irq_set, count);\n\tstruct vfio_irq_set hdr;\n\tu8 *data = NULL;\n\tint max, ret = 0;\n\tsize_t data_size = 0;\n\n\tif (copy_from_user(&hdr, arg, minsz))\n\t\treturn -EFAULT;\n\n\tmax = vfio_pci_get_irq_count(vdev, hdr.index);\n\n\tret = vfio_set_irqs_validate_and_prepare(&hdr, max, VFIO_PCI_NUM_IRQS,\n\t\t\t\t\t\t &data_size);\n\tif (ret)\n\t\treturn ret;\n\n\tif (data_size) {\n\t\tdata = memdup_user(&arg->data, data_size);\n\t\tif (IS_ERR(data))\n\t\t\treturn PTR_ERR(data);\n\t}\n\n\tmutex_lock(&vdev->igate);\n\n\tret = vfio_pci_set_irqs_ioctl(vdev, hdr.flags, hdr.index, hdr.start,\n\t\t\t\t      hdr.count, data);\n\n\tmutex_unlock(&vdev->igate);\n\tkfree(data);\n\n\treturn ret;\n}\n\nstatic int vfio_pci_ioctl_reset(struct vfio_pci_core_device *vdev,\n\t\t\t\tvoid __user *arg)\n{\n\tint ret;\n\n\tif (!vdev->reset_works)\n\t\treturn -EINVAL;\n\n\tvfio_pci_zap_and_down_write_memory_lock(vdev);\n\n\t \n\tvfio_pci_set_power_state(vdev, PCI_D0);\n\n\tret = pci_try_reset_function(vdev->pdev);\n\tup_write(&vdev->memory_lock);\n\n\treturn ret;\n}\n\nstatic int vfio_pci_ioctl_get_pci_hot_reset_info(\n\tstruct vfio_pci_core_device *vdev,\n\tstruct vfio_pci_hot_reset_info __user *arg)\n{\n\tunsigned long minsz =\n\t\toffsetofend(struct vfio_pci_hot_reset_info, count);\n\tstruct vfio_pci_hot_reset_info hdr;\n\tstruct vfio_pci_fill_info fill = {};\n\tbool slot = false;\n\tint ret = 0;\n\n\tif (copy_from_user(&hdr, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (hdr.argsz < minsz)\n\t\treturn -EINVAL;\n\n\thdr.flags = 0;\n\n\t \n\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\tslot = true;\n\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\treturn -ENODEV;\n\n\tfill.devices = arg->devices;\n\tfill.devices_end = arg->devices +\n\t\t\t   (hdr.argsz - sizeof(hdr)) / sizeof(arg->devices[0]);\n\tfill.vdev = &vdev->vdev;\n\n\tif (vfio_device_cdev_opened(&vdev->vdev))\n\t\tfill.flags |= VFIO_PCI_HOT_RESET_FLAG_DEV_ID |\n\t\t\t     VFIO_PCI_HOT_RESET_FLAG_DEV_ID_OWNED;\n\n\tmutex_lock(&vdev->vdev.dev_set->lock);\n\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_fill_devs,\n\t\t\t\t\t    &fill, slot);\n\tmutex_unlock(&vdev->vdev.dev_set->lock);\n\tif (ret)\n\t\treturn ret;\n\n\thdr.count = fill.count;\n\thdr.flags = fill.flags;\n\tif (copy_to_user(arg, &hdr, minsz))\n\t\treturn -EFAULT;\n\n\tif (fill.count > fill.devices - arg->devices)\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\nstatic int\nvfio_pci_ioctl_pci_hot_reset_groups(struct vfio_pci_core_device *vdev,\n\t\t\t\t    int array_count, bool slot,\n\t\t\t\t    struct vfio_pci_hot_reset __user *arg)\n{\n\tint32_t *group_fds;\n\tstruct file **files;\n\tstruct vfio_pci_group_info info;\n\tint file_idx, count = 0, ret = 0;\n\n\t \n\tret = vfio_pci_for_each_slot_or_bus(vdev->pdev, vfio_pci_count_devs,\n\t\t\t\t\t    &count, slot);\n\tif (ret)\n\t\treturn ret;\n\n\tif (array_count > count)\n\t\treturn -EINVAL;\n\n\tgroup_fds = kcalloc(array_count, sizeof(*group_fds), GFP_KERNEL);\n\tfiles = kcalloc(array_count, sizeof(*files), GFP_KERNEL);\n\tif (!group_fds || !files) {\n\t\tkfree(group_fds);\n\t\tkfree(files);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (copy_from_user(group_fds, arg->group_fds,\n\t\t\t   array_count * sizeof(*group_fds))) {\n\t\tkfree(group_fds);\n\t\tkfree(files);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tfor (file_idx = 0; file_idx < array_count; file_idx++) {\n\t\tstruct file *file = fget(group_fds[file_idx]);\n\n\t\tif (!file) {\n\t\t\tret = -EBADF;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!vfio_file_is_group(file)) {\n\t\t\tfput(file);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tfiles[file_idx] = file;\n\t}\n\n\tkfree(group_fds);\n\n\t \n\tif (ret)\n\t\tgoto hot_reset_release;\n\n\tinfo.count = array_count;\n\tinfo.files = files;\n\n\tret = vfio_pci_dev_set_hot_reset(vdev->vdev.dev_set, &info, NULL);\n\nhot_reset_release:\n\tfor (file_idx--; file_idx >= 0; file_idx--)\n\t\tfput(files[file_idx]);\n\n\tkfree(files);\n\treturn ret;\n}\n\nstatic int vfio_pci_ioctl_pci_hot_reset(struct vfio_pci_core_device *vdev,\n\t\t\t\t\tstruct vfio_pci_hot_reset __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_pci_hot_reset, count);\n\tstruct vfio_pci_hot_reset hdr;\n\tbool slot = false;\n\n\tif (copy_from_user(&hdr, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (hdr.argsz < minsz || hdr.flags)\n\t\treturn -EINVAL;\n\n\t \n\tif (!!hdr.count == vfio_device_cdev_opened(&vdev->vdev))\n\t\treturn -EINVAL;\n\n\t \n\tif (!pci_probe_reset_slot(vdev->pdev->slot))\n\t\tslot = true;\n\telse if (pci_probe_reset_bus(vdev->pdev->bus))\n\t\treturn -ENODEV;\n\n\tif (hdr.count)\n\t\treturn vfio_pci_ioctl_pci_hot_reset_groups(vdev, hdr.count, slot, arg);\n\n\treturn vfio_pci_dev_set_hot_reset(vdev->vdev.dev_set, NULL,\n\t\t\t\t\t  vfio_iommufd_device_ictx(&vdev->vdev));\n}\n\nstatic int vfio_pci_ioctl_ioeventfd(struct vfio_pci_core_device *vdev,\n\t\t\t\t    struct vfio_device_ioeventfd __user *arg)\n{\n\tunsigned long minsz = offsetofend(struct vfio_device_ioeventfd, fd);\n\tstruct vfio_device_ioeventfd ioeventfd;\n\tint count;\n\n\tif (copy_from_user(&ioeventfd, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (ioeventfd.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tif (ioeventfd.flags & ~VFIO_DEVICE_IOEVENTFD_SIZE_MASK)\n\t\treturn -EINVAL;\n\n\tcount = ioeventfd.flags & VFIO_DEVICE_IOEVENTFD_SIZE_MASK;\n\n\tif (hweight8(count) != 1 || ioeventfd.fd < -1)\n\t\treturn -EINVAL;\n\n\treturn vfio_pci_ioeventfd(vdev, ioeventfd.offset, ioeventfd.data, count,\n\t\t\t\t  ioeventfd.fd);\n}\n\nlong vfio_pci_core_ioctl(struct vfio_device *core_vdev, unsigned int cmd,\n\t\t\t unsigned long arg)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\tvoid __user *uarg = (void __user *)arg;\n\n\tswitch (cmd) {\n\tcase VFIO_DEVICE_GET_INFO:\n\t\treturn vfio_pci_ioctl_get_info(vdev, uarg);\n\tcase VFIO_DEVICE_GET_IRQ_INFO:\n\t\treturn vfio_pci_ioctl_get_irq_info(vdev, uarg);\n\tcase VFIO_DEVICE_GET_PCI_HOT_RESET_INFO:\n\t\treturn vfio_pci_ioctl_get_pci_hot_reset_info(vdev, uarg);\n\tcase VFIO_DEVICE_GET_REGION_INFO:\n\t\treturn vfio_pci_ioctl_get_region_info(vdev, uarg);\n\tcase VFIO_DEVICE_IOEVENTFD:\n\t\treturn vfio_pci_ioctl_ioeventfd(vdev, uarg);\n\tcase VFIO_DEVICE_PCI_HOT_RESET:\n\t\treturn vfio_pci_ioctl_pci_hot_reset(vdev, uarg);\n\tcase VFIO_DEVICE_RESET:\n\t\treturn vfio_pci_ioctl_reset(vdev, uarg);\n\tcase VFIO_DEVICE_SET_IRQS:\n\t\treturn vfio_pci_ioctl_set_irqs(vdev, uarg);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_ioctl);\n\nstatic int vfio_pci_core_feature_token(struct vfio_device *device, u32 flags,\n\t\t\t\t       uuid_t __user *arg, size_t argsz)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(device, struct vfio_pci_core_device, vdev);\n\tuuid_t uuid;\n\tint ret;\n\n\tif (!vdev->vf_token)\n\t\treturn -ENOTTY;\n\t \n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_SET,\n\t\t\t\t sizeof(uuid));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tif (copy_from_user(&uuid, arg, sizeof(uuid)))\n\t\treturn -EFAULT;\n\n\tmutex_lock(&vdev->vf_token->lock);\n\tuuid_copy(&vdev->vf_token->uuid, &uuid);\n\tmutex_unlock(&vdev->vf_token->lock);\n\treturn 0;\n}\n\nint vfio_pci_core_ioctl_feature(struct vfio_device *device, u32 flags,\n\t\t\t\tvoid __user *arg, size_t argsz)\n{\n\tswitch (flags & VFIO_DEVICE_FEATURE_MASK) {\n\tcase VFIO_DEVICE_FEATURE_LOW_POWER_ENTRY:\n\t\treturn vfio_pci_core_pm_entry(device, flags, arg, argsz);\n\tcase VFIO_DEVICE_FEATURE_LOW_POWER_ENTRY_WITH_WAKEUP:\n\t\treturn vfio_pci_core_pm_entry_with_wakeup(device, flags,\n\t\t\t\t\t\t\t  arg, argsz);\n\tcase VFIO_DEVICE_FEATURE_LOW_POWER_EXIT:\n\t\treturn vfio_pci_core_pm_exit(device, flags, arg, argsz);\n\tcase VFIO_DEVICE_FEATURE_PCI_VF_TOKEN:\n\t\treturn vfio_pci_core_feature_token(device, flags, arg, argsz);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_ioctl_feature);\n\nstatic ssize_t vfio_pci_rw(struct vfio_pci_core_device *vdev, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos, bool iswrite)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tint ret;\n\n\tif (index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\treturn -EINVAL;\n\n\tret = pm_runtime_resume_and_get(&vdev->pdev->dev);\n\tif (ret) {\n\t\tpci_info_ratelimited(vdev->pdev, \"runtime resume failed %d\\n\",\n\t\t\t\t     ret);\n\t\treturn -EIO;\n\t}\n\n\tswitch (index) {\n\tcase VFIO_PCI_CONFIG_REGION_INDEX:\n\t\tret = vfio_pci_config_rw(vdev, buf, count, ppos, iswrite);\n\t\tbreak;\n\n\tcase VFIO_PCI_ROM_REGION_INDEX:\n\t\tif (iswrite)\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tret = vfio_pci_bar_rw(vdev, buf, count, ppos, false);\n\t\tbreak;\n\n\tcase VFIO_PCI_BAR0_REGION_INDEX ... VFIO_PCI_BAR5_REGION_INDEX:\n\t\tret = vfio_pci_bar_rw(vdev, buf, count, ppos, iswrite);\n\t\tbreak;\n\n\tcase VFIO_PCI_VGA_REGION_INDEX:\n\t\tret = vfio_pci_vga_rw(vdev, buf, count, ppos, iswrite);\n\t\tbreak;\n\n\tdefault:\n\t\tindex -= VFIO_PCI_NUM_REGIONS;\n\t\tret = vdev->region[index].ops->rw(vdev, buf,\n\t\t\t\t\t\t   count, ppos, iswrite);\n\t\tbreak;\n\t}\n\n\tpm_runtime_put(&vdev->pdev->dev);\n\treturn ret;\n}\n\nssize_t vfio_pci_core_read(struct vfio_device *core_vdev, char __user *buf,\n\t\tsize_t count, loff_t *ppos)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(vdev, buf, count, ppos, false);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_read);\n\nssize_t vfio_pci_core_write(struct vfio_device *core_vdev, const char __user *buf,\n\t\tsize_t count, loff_t *ppos)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tif (!count)\n\t\treturn 0;\n\n\treturn vfio_pci_rw(vdev, (char __user *)buf, count, ppos, true);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_write);\n\n \nstatic int vfio_pci_zap_and_vma_lock(struct vfio_pci_core_device *vdev, bool try)\n{\n\tstruct vfio_pci_mmap_vma *mmap_vma, *tmp;\n\n\t \n\twhile (1) {\n\t\tstruct mm_struct *mm = NULL;\n\n\t\tif (try) {\n\t\t\tif (!mutex_trylock(&vdev->vma_lock))\n\t\t\t\treturn 0;\n\t\t} else {\n\t\t\tmutex_lock(&vdev->vma_lock);\n\t\t}\n\t\twhile (!list_empty(&vdev->vma_list)) {\n\t\t\tmmap_vma = list_first_entry(&vdev->vma_list,\n\t\t\t\t\t\t    struct vfio_pci_mmap_vma,\n\t\t\t\t\t\t    vma_next);\n\t\t\tmm = mmap_vma->vma->vm_mm;\n\t\t\tif (mmget_not_zero(mm))\n\t\t\t\tbreak;\n\n\t\t\tlist_del(&mmap_vma->vma_next);\n\t\t\tkfree(mmap_vma);\n\t\t\tmm = NULL;\n\t\t}\n\t\tif (!mm)\n\t\t\treturn 1;\n\t\tmutex_unlock(&vdev->vma_lock);\n\n\t\tif (try) {\n\t\t\tif (!mmap_read_trylock(mm)) {\n\t\t\t\tmmput(mm);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {\n\t\t\tmmap_read_lock(mm);\n\t\t}\n\t\tif (try) {\n\t\t\tif (!mutex_trylock(&vdev->vma_lock)) {\n\t\t\t\tmmap_read_unlock(mm);\n\t\t\t\tmmput(mm);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t} else {\n\t\t\tmutex_lock(&vdev->vma_lock);\n\t\t}\n\t\tlist_for_each_entry_safe(mmap_vma, tmp,\n\t\t\t\t\t &vdev->vma_list, vma_next) {\n\t\t\tstruct vm_area_struct *vma = mmap_vma->vma;\n\n\t\t\tif (vma->vm_mm != mm)\n\t\t\t\tcontinue;\n\n\t\t\tlist_del(&mmap_vma->vma_next);\n\t\t\tkfree(mmap_vma);\n\n\t\t\tzap_vma_ptes(vma, vma->vm_start,\n\t\t\t\t     vma->vm_end - vma->vm_start);\n\t\t}\n\t\tmutex_unlock(&vdev->vma_lock);\n\t\tmmap_read_unlock(mm);\n\t\tmmput(mm);\n\t}\n}\n\nvoid vfio_pci_zap_and_down_write_memory_lock(struct vfio_pci_core_device *vdev)\n{\n\tvfio_pci_zap_and_vma_lock(vdev, false);\n\tdown_write(&vdev->memory_lock);\n\tmutex_unlock(&vdev->vma_lock);\n}\n\nu16 vfio_pci_memory_lock_and_enable(struct vfio_pci_core_device *vdev)\n{\n\tu16 cmd;\n\n\tdown_write(&vdev->memory_lock);\n\tpci_read_config_word(vdev->pdev, PCI_COMMAND, &cmd);\n\tif (!(cmd & PCI_COMMAND_MEMORY))\n\t\tpci_write_config_word(vdev->pdev, PCI_COMMAND,\n\t\t\t\t      cmd | PCI_COMMAND_MEMORY);\n\n\treturn cmd;\n}\n\nvoid vfio_pci_memory_unlock_and_restore(struct vfio_pci_core_device *vdev, u16 cmd)\n{\n\tpci_write_config_word(vdev->pdev, PCI_COMMAND, cmd);\n\tup_write(&vdev->memory_lock);\n}\n\n \nstatic int __vfio_pci_add_vma(struct vfio_pci_core_device *vdev,\n\t\t\t      struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_mmap_vma *mmap_vma;\n\n\tmmap_vma = kmalloc(sizeof(*mmap_vma), GFP_KERNEL_ACCOUNT);\n\tif (!mmap_vma)\n\t\treturn -ENOMEM;\n\n\tmmap_vma->vma = vma;\n\tlist_add(&mmap_vma->vma_next, &vdev->vma_list);\n\n\treturn 0;\n}\n\n \nstatic void vfio_pci_mmap_open(struct vm_area_struct *vma)\n{\n\tzap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);\n}\n\nstatic void vfio_pci_mmap_close(struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_core_device *vdev = vma->vm_private_data;\n\tstruct vfio_pci_mmap_vma *mmap_vma;\n\n\tmutex_lock(&vdev->vma_lock);\n\tlist_for_each_entry(mmap_vma, &vdev->vma_list, vma_next) {\n\t\tif (mmap_vma->vma == vma) {\n\t\t\tlist_del(&mmap_vma->vma_next);\n\t\t\tkfree(mmap_vma);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&vdev->vma_lock);\n}\n\nstatic vm_fault_t vfio_pci_mmap_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct vfio_pci_core_device *vdev = vma->vm_private_data;\n\tstruct vfio_pci_mmap_vma *mmap_vma;\n\tvm_fault_t ret = VM_FAULT_NOPAGE;\n\n\tmutex_lock(&vdev->vma_lock);\n\tdown_read(&vdev->memory_lock);\n\n\t \n\tif (vdev->pm_runtime_engaged || !__vfio_pci_memory_enabled(vdev)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto up_out;\n\t}\n\n\t \n\tlist_for_each_entry(mmap_vma, &vdev->vma_list, vma_next) {\n\t\tif (mmap_vma->vma == vma)\n\t\t\tgoto up_out;\n\t}\n\n\tif (io_remap_pfn_range(vma, vma->vm_start, vma->vm_pgoff,\n\t\t\t       vma->vm_end - vma->vm_start,\n\t\t\t       vma->vm_page_prot)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tzap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);\n\t\tgoto up_out;\n\t}\n\n\tif (__vfio_pci_add_vma(vdev, vma)) {\n\t\tret = VM_FAULT_OOM;\n\t\tzap_vma_ptes(vma, vma->vm_start, vma->vm_end - vma->vm_start);\n\t}\n\nup_out:\n\tup_read(&vdev->memory_lock);\n\tmutex_unlock(&vdev->vma_lock);\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct vfio_pci_mmap_ops = {\n\t.open = vfio_pci_mmap_open,\n\t.close = vfio_pci_mmap_close,\n\t.fault = vfio_pci_mmap_fault,\n};\n\nint vfio_pci_core_mmap(struct vfio_device *core_vdev, struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\tstruct pci_dev *pdev = vdev->pdev;\n\tunsigned int index;\n\tu64 phys_len, req_len, pgoff, req_start;\n\tint ret;\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\n\tif (index >= VFIO_PCI_NUM_REGIONS + vdev->num_regions)\n\t\treturn -EINVAL;\n\tif (vma->vm_end < vma->vm_start)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (index >= VFIO_PCI_NUM_REGIONS) {\n\t\tint regnum = index - VFIO_PCI_NUM_REGIONS;\n\t\tstruct vfio_pci_region *region = vdev->region + regnum;\n\n\t\tif (region->ops && region->ops->mmap &&\n\t\t    (region->flags & VFIO_REGION_INFO_FLAG_MMAP))\n\t\t\treturn region->ops->mmap(vdev, region, vma);\n\t\treturn -EINVAL;\n\t}\n\tif (index >= VFIO_PCI_ROM_REGION_INDEX)\n\t\treturn -EINVAL;\n\tif (!vdev->bar_mmap_supported[index])\n\t\treturn -EINVAL;\n\n\tphys_len = PAGE_ALIGN(pci_resource_len(pdev, index));\n\treq_len = vma->vm_end - vma->vm_start;\n\tpgoff = vma->vm_pgoff &\n\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\treq_start = pgoff << PAGE_SHIFT;\n\n\tif (req_start + req_len > phys_len)\n\t\treturn -EINVAL;\n\n\t \n\tif (!vdev->barmap[index]) {\n\t\tret = pci_request_selected_regions(pdev,\n\t\t\t\t\t\t   1 << index, \"vfio-pci\");\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvdev->barmap[index] = pci_iomap(pdev, index, 0);\n\t\tif (!vdev->barmap[index]) {\n\t\t\tpci_release_selected_regions(pdev, 1 << index);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tvma->vm_private_data = vdev;\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tvma->vm_pgoff = (pci_resource_start(pdev, index) >> PAGE_SHIFT) + pgoff;\n\n\t \n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_ops = &vfio_pci_mmap_ops;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_mmap);\n\nvoid vfio_pci_core_request(struct vfio_device *core_vdev, unsigned int count)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\tstruct pci_dev *pdev = vdev->pdev;\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->req_trigger) {\n\t\tif (!(count % 10))\n\t\t\tpci_notice_ratelimited(pdev,\n\t\t\t\t\"Relaying device request to user (#%u)\\n\",\n\t\t\t\tcount);\n\t\teventfd_signal(vdev->req_trigger, 1);\n\t} else if (count == 0) {\n\t\tpci_warn(pdev,\n\t\t\t\"No device request channel registered, blocked until released by user\\n\");\n\t}\n\n\tmutex_unlock(&vdev->igate);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_request);\n\nstatic int vfio_pci_validate_vf_token(struct vfio_pci_core_device *vdev,\n\t\t\t\t      bool vf_token, uuid_t *uuid)\n{\n\t \n\tif (vdev->pdev->is_virtfn) {\n\t\tstruct vfio_pci_core_device *pf_vdev = vdev->sriov_pf_core_dev;\n\t\tbool match;\n\n\t\tif (!pf_vdev) {\n\t\t\tif (!vf_token)\n\t\t\t\treturn 0;  \n\n\t\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\t\"VF token incorrectly provided, PF not bound to vfio-pci\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!vf_token) {\n\t\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\t\"VF token required to access device\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\n\t\tmutex_lock(&pf_vdev->vf_token->lock);\n\t\tmatch = uuid_equal(uuid, &pf_vdev->vf_token->uuid);\n\t\tmutex_unlock(&pf_vdev->vf_token->lock);\n\n\t\tif (!match) {\n\t\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\t\"Incorrect VF token provided for device\\n\");\n\t\t\treturn -EACCES;\n\t\t}\n\t} else if (vdev->vf_token) {\n\t\tmutex_lock(&vdev->vf_token->lock);\n\t\tif (vdev->vf_token->users) {\n\t\t\tif (!vf_token) {\n\t\t\t\tmutex_unlock(&vdev->vf_token->lock);\n\t\t\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\t\t\"VF token required to access device\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\n\t\t\tif (!uuid_equal(uuid, &vdev->vf_token->uuid)) {\n\t\t\t\tmutex_unlock(&vdev->vf_token->lock);\n\t\t\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\t\t\"Incorrect VF token provided for device\\n\");\n\t\t\t\treturn -EACCES;\n\t\t\t}\n\t\t} else if (vf_token) {\n\t\t\tuuid_copy(&vdev->vf_token->uuid, uuid);\n\t\t}\n\n\t\tmutex_unlock(&vdev->vf_token->lock);\n\t} else if (vf_token) {\n\t\tpci_info_ratelimited(vdev->pdev,\n\t\t\t\"VF token incorrectly provided, not a PF or VF\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n#define VF_TOKEN_ARG \"vf_token=\"\n\nint vfio_pci_core_match(struct vfio_device *core_vdev, char *buf)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\tbool vf_token = false;\n\tuuid_t uuid;\n\tint ret;\n\n\tif (strncmp(pci_name(vdev->pdev), buf, strlen(pci_name(vdev->pdev))))\n\t\treturn 0;  \n\n\tif (strlen(buf) > strlen(pci_name(vdev->pdev))) {\n\t\tbuf += strlen(pci_name(vdev->pdev));\n\n\t\tif (*buf != ' ')\n\t\t\treturn 0;  \n\n\t\twhile (*buf) {\n\t\t\tif (*buf == ' ') {\n\t\t\t\tbuf++;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!vf_token && !strncmp(buf, VF_TOKEN_ARG,\n\t\t\t\t\t\t  strlen(VF_TOKEN_ARG))) {\n\t\t\t\tbuf += strlen(VF_TOKEN_ARG);\n\n\t\t\t\tif (strlen(buf) < UUID_STRING_LEN)\n\t\t\t\t\treturn -EINVAL;\n\n\t\t\t\tret = uuid_parse(buf, &uuid);\n\t\t\t\tif (ret)\n\t\t\t\t\treturn ret;\n\n\t\t\t\tvf_token = true;\n\t\t\t\tbuf += UUID_STRING_LEN;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = vfio_pci_validate_vf_token(vdev, vf_token, &uuid);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 1;  \n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_match);\n\nstatic int vfio_pci_bus_notifier(struct notifier_block *nb,\n\t\t\t\t unsigned long action, void *data)\n{\n\tstruct vfio_pci_core_device *vdev = container_of(nb,\n\t\t\t\t\t\t    struct vfio_pci_core_device, nb);\n\tstruct device *dev = data;\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct pci_dev *physfn = pci_physfn(pdev);\n\n\tif (action == BUS_NOTIFY_ADD_DEVICE &&\n\t    pdev->is_virtfn && physfn == vdev->pdev) {\n\t\tpci_info(vdev->pdev, \"Captured SR-IOV VF %s driver_override\\n\",\n\t\t\t pci_name(pdev));\n\t\tpdev->driver_override = kasprintf(GFP_KERNEL, \"%s\",\n\t\t\t\t\t\t  vdev->vdev.ops->name);\n\t} else if (action == BUS_NOTIFY_BOUND_DRIVER &&\n\t\t   pdev->is_virtfn && physfn == vdev->pdev) {\n\t\tstruct pci_driver *drv = pci_dev_driver(pdev);\n\n\t\tif (drv && drv != pci_dev_driver(vdev->pdev))\n\t\t\tpci_warn(vdev->pdev,\n\t\t\t\t \"VF %s bound to driver %s while PF bound to driver %s\\n\",\n\t\t\t\t pci_name(pdev), drv->name,\n\t\t\t\t pci_dev_driver(vdev->pdev)->name);\n\t}\n\n\treturn 0;\n}\n\nstatic int vfio_pci_vf_init(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct vfio_pci_core_device *cur;\n\tstruct pci_dev *physfn;\n\tint ret;\n\n\tif (pdev->is_virtfn) {\n\t\t \n\t\tphysfn = pci_physfn(vdev->pdev);\n\t\tmutex_lock(&vfio_pci_sriov_pfs_mutex);\n\t\tlist_for_each_entry(cur, &vfio_pci_sriov_pfs, sriov_pfs_item) {\n\t\t\tif (cur->pdev == physfn) {\n\t\t\t\tvdev->sriov_pf_core_dev = cur;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&vfio_pci_sriov_pfs_mutex);\n\t\treturn 0;\n\t}\n\n\t \n\tif (!pdev->is_physfn)\n\t\treturn 0;\n\n\tvdev->vf_token = kzalloc(sizeof(*vdev->vf_token), GFP_KERNEL);\n\tif (!vdev->vf_token)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&vdev->vf_token->lock);\n\tuuid_gen(&vdev->vf_token->uuid);\n\n\tvdev->nb.notifier_call = vfio_pci_bus_notifier;\n\tret = bus_register_notifier(&pci_bus_type, &vdev->nb);\n\tif (ret) {\n\t\tkfree(vdev->vf_token);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic void vfio_pci_vf_uninit(struct vfio_pci_core_device *vdev)\n{\n\tif (!vdev->vf_token)\n\t\treturn;\n\n\tbus_unregister_notifier(&pci_bus_type, &vdev->nb);\n\tWARN_ON(vdev->vf_token->users);\n\tmutex_destroy(&vdev->vf_token->lock);\n\tkfree(vdev->vf_token);\n}\n\nstatic int vfio_pci_vga_init(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint ret;\n\n\tif (!vfio_pci_is_vga(pdev))\n\t\treturn 0;\n\n\tret = aperture_remove_conflicting_pci_devices(pdev, vdev->vdev.ops->name);\n\tif (ret)\n\t\treturn ret;\n\n\tret = vga_client_register(pdev, vfio_pci_set_decode);\n\tif (ret)\n\t\treturn ret;\n\tvga_set_legacy_decoding(pdev, vfio_pci_set_decode(pdev, false));\n\treturn 0;\n}\n\nstatic void vfio_pci_vga_uninit(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\n\tif (!vfio_pci_is_vga(pdev))\n\t\treturn;\n\tvga_client_unregister(pdev);\n\tvga_set_legacy_decoding(pdev, VGA_RSRC_NORMAL_IO | VGA_RSRC_NORMAL_MEM |\n\t\t\t\t\t      VGA_RSRC_LEGACY_IO |\n\t\t\t\t\t      VGA_RSRC_LEGACY_MEM);\n}\n\nint vfio_pci_core_init_dev(struct vfio_device *core_vdev)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tvdev->pdev = to_pci_dev(core_vdev->dev);\n\tvdev->irq_type = VFIO_PCI_NUM_IRQS;\n\tmutex_init(&vdev->igate);\n\tspin_lock_init(&vdev->irqlock);\n\tmutex_init(&vdev->ioeventfds_lock);\n\tINIT_LIST_HEAD(&vdev->dummy_resources_list);\n\tINIT_LIST_HEAD(&vdev->ioeventfds_list);\n\tmutex_init(&vdev->vma_lock);\n\tINIT_LIST_HEAD(&vdev->vma_list);\n\tINIT_LIST_HEAD(&vdev->sriov_pfs_item);\n\tinit_rwsem(&vdev->memory_lock);\n\txa_init(&vdev->ctx);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_init_dev);\n\nvoid vfio_pci_core_release_dev(struct vfio_device *core_vdev)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tmutex_destroy(&vdev->igate);\n\tmutex_destroy(&vdev->ioeventfds_lock);\n\tmutex_destroy(&vdev->vma_lock);\n\tkfree(vdev->region);\n\tkfree(vdev->pm_save);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_release_dev);\n\nint vfio_pci_core_register_device(struct vfio_pci_core_device *vdev)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tstruct device *dev = &pdev->dev;\n\tint ret;\n\n\t \n\tif (WARN_ON(vdev != dev_get_drvdata(dev)))\n\t\treturn -EINVAL;\n\n\tif (pdev->hdr_type != PCI_HEADER_TYPE_NORMAL)\n\t\treturn -EINVAL;\n\n\tif (vdev->vdev.mig_ops) {\n\t\tif (!(vdev->vdev.mig_ops->migration_get_state &&\n\t\t      vdev->vdev.mig_ops->migration_set_state &&\n\t\t      vdev->vdev.mig_ops->migration_get_data_size) ||\n\t\t    !(vdev->vdev.migration_flags & VFIO_MIGRATION_STOP_COPY))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (vdev->vdev.log_ops && !(vdev->vdev.log_ops->log_start &&\n\t    vdev->vdev.log_ops->log_stop &&\n\t    vdev->vdev.log_ops->log_read_and_clear))\n\t\treturn -EINVAL;\n\n\t \n\tif (pci_num_vf(pdev)) {\n\t\tpci_warn(pdev, \"Cannot bind to PF with SR-IOV enabled\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (pci_is_root_bus(pdev->bus)) {\n\t\tret = vfio_assign_device_set(&vdev->vdev, vdev);\n\t} else if (!pci_probe_reset_slot(pdev->slot)) {\n\t\tret = vfio_assign_device_set(&vdev->vdev, pdev->slot);\n\t} else {\n\t\t \n\t\tret = vfio_assign_device_set(&vdev->vdev, pdev->bus);\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\tret = vfio_pci_vf_init(vdev);\n\tif (ret)\n\t\treturn ret;\n\tret = vfio_pci_vga_init(vdev);\n\tif (ret)\n\t\tgoto out_vf;\n\n\tvfio_pci_probe_power_state(vdev);\n\n\t \n\tvfio_pci_set_power_state(vdev, PCI_D0);\n\n\tdev->driver->pm = &vfio_pci_core_pm_ops;\n\tpm_runtime_allow(dev);\n\tif (!disable_idle_d3)\n\t\tpm_runtime_put(dev);\n\n\tret = vfio_register_group_dev(&vdev->vdev);\n\tif (ret)\n\t\tgoto out_power;\n\treturn 0;\n\nout_power:\n\tif (!disable_idle_d3)\n\t\tpm_runtime_get_noresume(dev);\n\n\tpm_runtime_forbid(dev);\nout_vf:\n\tvfio_pci_vf_uninit(vdev);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_register_device);\n\nvoid vfio_pci_core_unregister_device(struct vfio_pci_core_device *vdev)\n{\n\tvfio_pci_core_sriov_configure(vdev, 0);\n\n\tvfio_unregister_group_dev(&vdev->vdev);\n\n\tvfio_pci_vf_uninit(vdev);\n\tvfio_pci_vga_uninit(vdev);\n\n\tif (!disable_idle_d3)\n\t\tpm_runtime_get_noresume(&vdev->pdev->dev);\n\n\tpm_runtime_forbid(&vdev->pdev->dev);\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_unregister_device);\n\npci_ers_result_t vfio_pci_core_aer_err_detected(struct pci_dev *pdev,\n\t\t\t\t\t\tpci_channel_state_t state)\n{\n\tstruct vfio_pci_core_device *vdev = dev_get_drvdata(&pdev->dev);\n\n\tmutex_lock(&vdev->igate);\n\n\tif (vdev->err_trigger)\n\t\teventfd_signal(vdev->err_trigger, 1);\n\n\tmutex_unlock(&vdev->igate);\n\n\treturn PCI_ERS_RESULT_CAN_RECOVER;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_aer_err_detected);\n\nint vfio_pci_core_sriov_configure(struct vfio_pci_core_device *vdev,\n\t\t\t\t  int nr_virtfn)\n{\n\tstruct pci_dev *pdev = vdev->pdev;\n\tint ret = 0;\n\n\tdevice_lock_assert(&pdev->dev);\n\n\tif (nr_virtfn) {\n\t\tmutex_lock(&vfio_pci_sriov_pfs_mutex);\n\t\t \n\t\tif (!list_empty(&vdev->sriov_pfs_item)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tlist_add_tail(&vdev->sriov_pfs_item, &vfio_pci_sriov_pfs);\n\t\tmutex_unlock(&vfio_pci_sriov_pfs_mutex);\n\n\t\t \n\t\tret = pm_runtime_resume_and_get(&pdev->dev);\n\t\tif (ret)\n\t\t\tgoto out_del;\n\n\t\tdown_write(&vdev->memory_lock);\n\t\tvfio_pci_set_power_state(vdev, PCI_D0);\n\t\tret = pci_enable_sriov(pdev, nr_virtfn);\n\t\tup_write(&vdev->memory_lock);\n\t\tif (ret) {\n\t\t\tpm_runtime_put(&pdev->dev);\n\t\t\tgoto out_del;\n\t\t}\n\t\treturn nr_virtfn;\n\t}\n\n\tif (pci_num_vf(pdev)) {\n\t\tpci_disable_sriov(pdev);\n\t\tpm_runtime_put(&pdev->dev);\n\t}\n\nout_del:\n\tmutex_lock(&vfio_pci_sriov_pfs_mutex);\n\tlist_del_init(&vdev->sriov_pfs_item);\nout_unlock:\n\tmutex_unlock(&vfio_pci_sriov_pfs_mutex);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_sriov_configure);\n\nconst struct pci_error_handlers vfio_pci_core_err_handlers = {\n\t.error_detected = vfio_pci_core_aer_err_detected,\n};\nEXPORT_SYMBOL_GPL(vfio_pci_core_err_handlers);\n\nstatic bool vfio_dev_in_groups(struct vfio_device *vdev,\n\t\t\t       struct vfio_pci_group_info *groups)\n{\n\tunsigned int i;\n\n\tif (!groups)\n\t\treturn false;\n\n\tfor (i = 0; i < groups->count; i++)\n\t\tif (vfio_file_has_dev(groups->files[i], vdev))\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic int vfio_pci_is_device_in_set(struct pci_dev *pdev, void *data)\n{\n\tstruct vfio_device_set *dev_set = data;\n\n\treturn vfio_find_device_in_devset(dev_set, &pdev->dev) ? 0 : -ENODEV;\n}\n\n \nstatic struct pci_dev *\nvfio_pci_dev_set_resettable(struct vfio_device_set *dev_set)\n{\n\tstruct pci_dev *pdev;\n\n\tlockdep_assert_held(&dev_set->lock);\n\n\t \n\tpdev = list_first_entry(&dev_set->device_list,\n\t\t\t\tstruct vfio_pci_core_device,\n\t\t\t\tvdev.dev_set_list)->pdev;\n\n\t \n\tif (pci_probe_reset_slot(pdev->slot) && pci_probe_reset_bus(pdev->bus))\n\t\treturn NULL;\n\n\tif (vfio_pci_for_each_slot_or_bus(pdev, vfio_pci_is_device_in_set,\n\t\t\t\t\t  dev_set,\n\t\t\t\t\t  !pci_probe_reset_slot(pdev->slot)))\n\t\treturn NULL;\n\treturn pdev;\n}\n\nstatic int vfio_pci_dev_set_pm_runtime_get(struct vfio_device_set *dev_set)\n{\n\tstruct vfio_pci_core_device *cur;\n\tint ret;\n\n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list) {\n\t\tret = pm_runtime_resume_and_get(&cur->pdev->dev);\n\t\tif (ret)\n\t\t\tgoto unwind;\n\t}\n\n\treturn 0;\n\nunwind:\n\tlist_for_each_entry_continue_reverse(cur, &dev_set->device_list,\n\t\t\t\t\t     vdev.dev_set_list)\n\t\tpm_runtime_put(&cur->pdev->dev);\n\n\treturn ret;\n}\n\n \nstatic int vfio_pci_dev_set_hot_reset(struct vfio_device_set *dev_set,\n\t\t\t\t      struct vfio_pci_group_info *groups,\n\t\t\t\t      struct iommufd_ctx *iommufd_ctx)\n{\n\tstruct vfio_pci_core_device *cur_mem;\n\tstruct vfio_pci_core_device *cur_vma;\n\tstruct vfio_pci_core_device *cur;\n\tstruct pci_dev *pdev;\n\tbool is_mem = true;\n\tint ret;\n\n\tmutex_lock(&dev_set->lock);\n\tcur_mem = list_first_entry(&dev_set->device_list,\n\t\t\t\t   struct vfio_pci_core_device,\n\t\t\t\t   vdev.dev_set_list);\n\n\tpdev = vfio_pci_dev_set_resettable(dev_set);\n\tif (!pdev) {\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\t \n\tret = vfio_pci_dev_set_pm_runtime_get(dev_set);\n\tif (ret)\n\t\tgoto err_unlock;\n\n\tlist_for_each_entry(cur_vma, &dev_set->device_list, vdev.dev_set_list) {\n\t\tbool owned;\n\n\t\t \n\t\tif (iommufd_ctx) {\n\t\t\tint devid = vfio_iommufd_get_dev_id(&cur_vma->vdev,\n\t\t\t\t\t\t\t    iommufd_ctx);\n\n\t\t\towned = (devid > 0 || devid == -ENOENT);\n\t\t} else {\n\t\t\towned = vfio_dev_in_groups(&cur_vma->vdev, groups);\n\t\t}\n\n\t\tif (!owned) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_undo;\n\t\t}\n\n\t\t \n\t\tif (!vfio_pci_zap_and_vma_lock(cur_vma, true)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto err_undo;\n\t\t}\n\t}\n\tcur_vma = NULL;\n\n\tlist_for_each_entry(cur_mem, &dev_set->device_list, vdev.dev_set_list) {\n\t\tif (!down_write_trylock(&cur_mem->memory_lock)) {\n\t\t\tret = -EBUSY;\n\t\t\tgoto err_undo;\n\t\t}\n\t\tmutex_unlock(&cur_mem->vma_lock);\n\t}\n\tcur_mem = NULL;\n\n\t \n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list)\n\t\tvfio_pci_set_power_state(cur, PCI_D0);\n\n\tret = pci_reset_bus(pdev);\n\nerr_undo:\n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list) {\n\t\tif (cur == cur_mem)\n\t\t\tis_mem = false;\n\t\tif (cur == cur_vma)\n\t\t\tbreak;\n\t\tif (is_mem)\n\t\t\tup_write(&cur->memory_lock);\n\t\telse\n\t\t\tmutex_unlock(&cur->vma_lock);\n\t}\n\n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list)\n\t\tpm_runtime_put(&cur->pdev->dev);\nerr_unlock:\n\tmutex_unlock(&dev_set->lock);\n\treturn ret;\n}\n\nstatic bool vfio_pci_dev_set_needs_reset(struct vfio_device_set *dev_set)\n{\n\tstruct vfio_pci_core_device *cur;\n\tbool needs_reset = false;\n\n\t \n\tif (vfio_device_set_open_count(dev_set) > 1)\n\t\treturn false;\n\n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list)\n\t\tneeds_reset |= cur->needs_reset;\n\treturn needs_reset;\n}\n\n \nstatic void vfio_pci_dev_set_try_reset(struct vfio_device_set *dev_set)\n{\n\tstruct vfio_pci_core_device *cur;\n\tstruct pci_dev *pdev;\n\tbool reset_done = false;\n\n\tif (!vfio_pci_dev_set_needs_reset(dev_set))\n\t\treturn;\n\n\tpdev = vfio_pci_dev_set_resettable(dev_set);\n\tif (!pdev)\n\t\treturn;\n\n\t \n\tif (!disable_idle_d3 && vfio_pci_dev_set_pm_runtime_get(dev_set))\n\t\treturn;\n\n\tif (!pci_reset_bus(pdev))\n\t\treset_done = true;\n\n\tlist_for_each_entry(cur, &dev_set->device_list, vdev.dev_set_list) {\n\t\tif (reset_done)\n\t\t\tcur->needs_reset = false;\n\n\t\tif (!disable_idle_d3)\n\t\t\tpm_runtime_put(&cur->pdev->dev);\n\t}\n}\n\nvoid vfio_pci_core_set_params(bool is_nointxmask, bool is_disable_vga,\n\t\t\t      bool is_disable_idle_d3)\n{\n\tnointxmask = is_nointxmask;\n\tdisable_vga = is_disable_vga;\n\tdisable_idle_d3 = is_disable_idle_d3;\n}\nEXPORT_SYMBOL_GPL(vfio_pci_core_set_params);\n\nstatic void vfio_pci_core_cleanup(void)\n{\n\tvfio_pci_uninit_perm_bits();\n}\n\nstatic int __init vfio_pci_core_init(void)\n{\n\t \n\treturn vfio_pci_init_perm_bits();\n}\n\nmodule_init(vfio_pci_core_init);\nmodule_exit(vfio_pci_core_cleanup);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(DRIVER_AUTHOR);\nMODULE_DESCRIPTION(DRIVER_DESC);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}