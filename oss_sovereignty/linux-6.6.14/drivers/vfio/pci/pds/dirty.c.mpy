{
  "module_name": "dirty.c",
  "hash_id": "aae11363cb0d30cac32707d711401aaf4806fac223f666e3befc267ed376e221",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/pci/pds/dirty.c",
  "human_readable_source": "\n \n\n#include <linux/interval_tree.h>\n#include <linux/vfio.h>\n\n#include <linux/pds/pds_common.h>\n#include <linux/pds/pds_core_if.h>\n#include <linux/pds/pds_adminq.h>\n\n#include \"vfio_dev.h\"\n#include \"cmds.h\"\n#include \"dirty.h\"\n\n#define READ_SEQ true\n#define WRITE_ACK false\n\nbool pds_vfio_dirty_is_enabled(struct pds_vfio_pci_device *pds_vfio)\n{\n\treturn pds_vfio->dirty.is_enabled;\n}\n\nvoid pds_vfio_dirty_set_enabled(struct pds_vfio_pci_device *pds_vfio)\n{\n\tpds_vfio->dirty.is_enabled = true;\n}\n\nvoid pds_vfio_dirty_set_disabled(struct pds_vfio_pci_device *pds_vfio)\n{\n\tpds_vfio->dirty.is_enabled = false;\n}\n\nstatic void\npds_vfio_print_guest_region_info(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t u8 max_regions)\n{\n\tint len = max_regions * sizeof(struct pds_lm_dirty_region_info);\n\tstruct pci_dev *pdev = pds_vfio->vfio_coredev.pdev;\n\tstruct device *pdsc_dev = &pci_physfn(pdev)->dev;\n\tstruct pds_lm_dirty_region_info *region_info;\n\tdma_addr_t regions_dma;\n\tu8 num_regions;\n\tint err;\n\n\tregion_info = kcalloc(max_regions,\n\t\t\t      sizeof(struct pds_lm_dirty_region_info),\n\t\t\t      GFP_KERNEL);\n\tif (!region_info)\n\t\treturn;\n\n\tregions_dma =\n\t\tdma_map_single(pdsc_dev, region_info, len, DMA_FROM_DEVICE);\n\tif (dma_mapping_error(pdsc_dev, regions_dma))\n\t\tgoto out_free_region_info;\n\n\terr = pds_vfio_dirty_status_cmd(pds_vfio, regions_dma, &max_regions,\n\t\t\t\t\t&num_regions);\n\tdma_unmap_single(pdsc_dev, regions_dma, len, DMA_FROM_DEVICE);\n\tif (err)\n\t\tgoto out_free_region_info;\n\n\tfor (unsigned int i = 0; i < num_regions; i++)\n\t\tdev_dbg(&pdev->dev,\n\t\t\t\"region_info[%d]: dma_base 0x%llx page_count %u page_size_log2 %u\\n\",\n\t\t\ti, le64_to_cpu(region_info[i].dma_base),\n\t\t\tle32_to_cpu(region_info[i].page_count),\n\t\t\tregion_info[i].page_size_log2);\n\nout_free_region_info:\n\tkfree(region_info);\n}\n\nstatic int pds_vfio_dirty_alloc_bitmaps(struct pds_vfio_dirty *dirty,\n\t\t\t\t\tunsigned long bytes)\n{\n\tunsigned long *host_seq_bmp, *host_ack_bmp;\n\n\thost_seq_bmp = vzalloc(bytes);\n\tif (!host_seq_bmp)\n\t\treturn -ENOMEM;\n\n\thost_ack_bmp = vzalloc(bytes);\n\tif (!host_ack_bmp) {\n\t\tbitmap_free(host_seq_bmp);\n\t\treturn -ENOMEM;\n\t}\n\n\tdirty->host_seq.bmp = host_seq_bmp;\n\tdirty->host_ack.bmp = host_ack_bmp;\n\n\treturn 0;\n}\n\nstatic void pds_vfio_dirty_free_bitmaps(struct pds_vfio_dirty *dirty)\n{\n\tvfree(dirty->host_seq.bmp);\n\tvfree(dirty->host_ack.bmp);\n\tdirty->host_seq.bmp = NULL;\n\tdirty->host_ack.bmp = NULL;\n}\n\nstatic void __pds_vfio_dirty_free_sgl(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t      struct pds_vfio_bmp_info *bmp_info)\n{\n\tstruct pci_dev *pdev = pds_vfio->vfio_coredev.pdev;\n\tstruct device *pdsc_dev = &pci_physfn(pdev)->dev;\n\n\tdma_unmap_single(pdsc_dev, bmp_info->sgl_addr,\n\t\t\t bmp_info->num_sge * sizeof(struct pds_lm_sg_elem),\n\t\t\t DMA_BIDIRECTIONAL);\n\tkfree(bmp_info->sgl);\n\n\tbmp_info->num_sge = 0;\n\tbmp_info->sgl = NULL;\n\tbmp_info->sgl_addr = 0;\n}\n\nstatic void pds_vfio_dirty_free_sgl(struct pds_vfio_pci_device *pds_vfio)\n{\n\tif (pds_vfio->dirty.host_seq.sgl)\n\t\t__pds_vfio_dirty_free_sgl(pds_vfio, &pds_vfio->dirty.host_seq);\n\tif (pds_vfio->dirty.host_ack.sgl)\n\t\t__pds_vfio_dirty_free_sgl(pds_vfio, &pds_vfio->dirty.host_ack);\n}\n\nstatic int __pds_vfio_dirty_alloc_sgl(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t      struct pds_vfio_bmp_info *bmp_info,\n\t\t\t\t      u32 page_count)\n{\n\tstruct pci_dev *pdev = pds_vfio->vfio_coredev.pdev;\n\tstruct device *pdsc_dev = &pci_physfn(pdev)->dev;\n\tstruct pds_lm_sg_elem *sgl;\n\tdma_addr_t sgl_addr;\n\tsize_t sgl_size;\n\tu32 max_sge;\n\n\tmax_sge = DIV_ROUND_UP(page_count, PAGE_SIZE * 8);\n\tsgl_size = max_sge * sizeof(struct pds_lm_sg_elem);\n\n\tsgl = kzalloc(sgl_size, GFP_KERNEL);\n\tif (!sgl)\n\t\treturn -ENOMEM;\n\n\tsgl_addr = dma_map_single(pdsc_dev, sgl, sgl_size, DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(pdsc_dev, sgl_addr)) {\n\t\tkfree(sgl);\n\t\treturn -EIO;\n\t}\n\n\tbmp_info->sgl = sgl;\n\tbmp_info->num_sge = max_sge;\n\tbmp_info->sgl_addr = sgl_addr;\n\n\treturn 0;\n}\n\nstatic int pds_vfio_dirty_alloc_sgl(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t    u32 page_count)\n{\n\tstruct pds_vfio_dirty *dirty = &pds_vfio->dirty;\n\tint err;\n\n\terr = __pds_vfio_dirty_alloc_sgl(pds_vfio, &dirty->host_seq,\n\t\t\t\t\t page_count);\n\tif (err)\n\t\treturn err;\n\n\terr = __pds_vfio_dirty_alloc_sgl(pds_vfio, &dirty->host_ack,\n\t\t\t\t\t page_count);\n\tif (err) {\n\t\t__pds_vfio_dirty_free_sgl(pds_vfio, &dirty->host_seq);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int pds_vfio_dirty_enable(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t struct rb_root_cached *ranges, u32 nnodes,\n\t\t\t\t u64 *page_size)\n{\n\tstruct pci_dev *pdev = pds_vfio->vfio_coredev.pdev;\n\tstruct device *pdsc_dev = &pci_physfn(pdev)->dev;\n\tstruct pds_vfio_dirty *dirty = &pds_vfio->dirty;\n\tu64 region_start, region_size, region_page_size;\n\tstruct pds_lm_dirty_region_info *region_info;\n\tstruct interval_tree_node *node = NULL;\n\tu8 max_regions = 0, num_regions;\n\tdma_addr_t regions_dma = 0;\n\tu32 num_ranges = nnodes;\n\tu32 page_count;\n\tu16 len;\n\tint err;\n\n\tdev_dbg(&pdev->dev, \"vf%u: Start dirty page tracking\\n\",\n\t\tpds_vfio->vf_id);\n\n\tif (pds_vfio_dirty_is_enabled(pds_vfio))\n\t\treturn -EINVAL;\n\n\t \n\terr = pds_vfio_dirty_status_cmd(pds_vfio, 0, &max_regions,\n\t\t\t\t\t&num_regions);\n\tif (err < 0) {\n\t\tdev_err(&pdev->dev, \"Failed to get dirty status, err %pe\\n\",\n\t\t\tERR_PTR(err));\n\t\treturn err;\n\t} else if (num_regions) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Dirty tracking already enabled for %d regions\\n\",\n\t\t\tnum_regions);\n\t\treturn -EEXIST;\n\t} else if (!max_regions) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Device doesn't support dirty tracking, max_regions %d\\n\",\n\t\t\tmax_regions);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tmax_regions = 1;\n\tif (num_ranges > max_regions) {\n\t\tvfio_combine_iova_ranges(ranges, nnodes, max_regions);\n\t\tnum_ranges = max_regions;\n\t}\n\n\tnode = interval_tree_iter_first(ranges, 0, ULONG_MAX);\n\tif (!node)\n\t\treturn -EINVAL;\n\n\tregion_size = node->last - node->start + 1;\n\tregion_start = node->start;\n\tregion_page_size = *page_size;\n\n\tlen = sizeof(*region_info);\n\tregion_info = kzalloc(len, GFP_KERNEL);\n\tif (!region_info)\n\t\treturn -ENOMEM;\n\n\tpage_count = DIV_ROUND_UP(region_size, region_page_size);\n\n\tregion_info->dma_base = cpu_to_le64(region_start);\n\tregion_info->page_count = cpu_to_le32(page_count);\n\tregion_info->page_size_log2 = ilog2(region_page_size);\n\n\tregions_dma = dma_map_single(pdsc_dev, (void *)region_info, len,\n\t\t\t\t     DMA_BIDIRECTIONAL);\n\tif (dma_mapping_error(pdsc_dev, regions_dma)) {\n\t\terr = -ENOMEM;\n\t\tgoto out_free_region_info;\n\t}\n\n\terr = pds_vfio_dirty_enable_cmd(pds_vfio, regions_dma, max_regions);\n\tdma_unmap_single(pdsc_dev, regions_dma, len, DMA_BIDIRECTIONAL);\n\tif (err)\n\t\tgoto out_free_region_info;\n\n\t \n\tpage_count = le32_to_cpu(region_info->page_count);\n\n\tdev_dbg(&pdev->dev,\n\t\t\"region_info: regions_dma 0x%llx dma_base 0x%llx page_count %u page_size_log2 %u\\n\",\n\t\tregions_dma, region_start, page_count,\n\t\t(u8)ilog2(region_page_size));\n\n\terr = pds_vfio_dirty_alloc_bitmaps(dirty, page_count / BITS_PER_BYTE);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to alloc dirty bitmaps: %pe\\n\",\n\t\t\tERR_PTR(err));\n\t\tgoto out_free_region_info;\n\t}\n\n\terr = pds_vfio_dirty_alloc_sgl(pds_vfio, page_count);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Failed to alloc dirty sg lists: %pe\\n\",\n\t\t\tERR_PTR(err));\n\t\tgoto out_free_bitmaps;\n\t}\n\n\tdirty->region_start = region_start;\n\tdirty->region_size = region_size;\n\tdirty->region_page_size = region_page_size;\n\tpds_vfio_dirty_set_enabled(pds_vfio);\n\n\tpds_vfio_print_guest_region_info(pds_vfio, max_regions);\n\n\tkfree(region_info);\n\n\treturn 0;\n\nout_free_bitmaps:\n\tpds_vfio_dirty_free_bitmaps(dirty);\nout_free_region_info:\n\tkfree(region_info);\n\treturn err;\n}\n\nvoid pds_vfio_dirty_disable(struct pds_vfio_pci_device *pds_vfio, bool send_cmd)\n{\n\tif (pds_vfio_dirty_is_enabled(pds_vfio)) {\n\t\tpds_vfio_dirty_set_disabled(pds_vfio);\n\t\tif (send_cmd)\n\t\t\tpds_vfio_dirty_disable_cmd(pds_vfio);\n\t\tpds_vfio_dirty_free_sgl(pds_vfio);\n\t\tpds_vfio_dirty_free_bitmaps(&pds_vfio->dirty);\n\t}\n\n\tif (send_cmd)\n\t\tpds_vfio_send_host_vf_lm_status_cmd(pds_vfio, PDS_LM_STA_NONE);\n}\n\nstatic int pds_vfio_dirty_seq_ack(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t  struct pds_vfio_bmp_info *bmp_info,\n\t\t\t\t  u32 offset, u32 bmp_bytes, bool read_seq)\n{\n\tconst char *bmp_type_str = read_seq ? \"read_seq\" : \"write_ack\";\n\tu8 dma_dir = read_seq ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\n\tstruct pci_dev *pdev = pds_vfio->vfio_coredev.pdev;\n\tstruct device *pdsc_dev = &pci_physfn(pdev)->dev;\n\tunsigned long long npages;\n\tstruct sg_table sg_table;\n\tstruct scatterlist *sg;\n\tstruct page **pages;\n\tu32 page_offset;\n\tconst void *bmp;\n\tsize_t size;\n\tu16 num_sge;\n\tint err;\n\tint i;\n\n\tbmp = (void *)((u64)bmp_info->bmp + offset);\n\tpage_offset = offset_in_page(bmp);\n\tbmp -= page_offset;\n\n\t \n\tnpages = DIV_ROUND_UP_ULL(bmp_bytes + page_offset, PAGE_SIZE);\n\tpages = kmalloc_array(npages, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tfor (unsigned long long i = 0; i < npages; i++) {\n\t\tstruct page *page = vmalloc_to_page(bmp);\n\n\t\tif (!page) {\n\t\t\terr = -EFAULT;\n\t\t\tgoto out_free_pages;\n\t\t}\n\n\t\tpages[i] = page;\n\t\tbmp += PAGE_SIZE;\n\t}\n\n\terr = sg_alloc_table_from_pages(&sg_table, pages, npages, page_offset,\n\t\t\t\t\tbmp_bytes, GFP_KERNEL);\n\tif (err)\n\t\tgoto out_free_pages;\n\n\terr = dma_map_sgtable(pdsc_dev, &sg_table, dma_dir, 0);\n\tif (err)\n\t\tgoto out_free_sg_table;\n\n\tfor_each_sgtable_dma_sg(&sg_table, sg, i) {\n\t\tstruct pds_lm_sg_elem *sg_elem = &bmp_info->sgl[i];\n\n\t\tsg_elem->addr = cpu_to_le64(sg_dma_address(sg));\n\t\tsg_elem->len = cpu_to_le32(sg_dma_len(sg));\n\t}\n\n\tnum_sge = sg_table.nents;\n\tsize = num_sge * sizeof(struct pds_lm_sg_elem);\n\tdma_sync_single_for_device(pdsc_dev, bmp_info->sgl_addr, size, dma_dir);\n\terr = pds_vfio_dirty_seq_ack_cmd(pds_vfio, bmp_info->sgl_addr, num_sge,\n\t\t\t\t\t offset, bmp_bytes, read_seq);\n\tif (err)\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Dirty bitmap %s failed offset %u bmp_bytes %u num_sge %u DMA 0x%llx: %pe\\n\",\n\t\t\tbmp_type_str, offset, bmp_bytes,\n\t\t\tnum_sge, bmp_info->sgl_addr, ERR_PTR(err));\n\tdma_sync_single_for_cpu(pdsc_dev, bmp_info->sgl_addr, size, dma_dir);\n\n\tdma_unmap_sgtable(pdsc_dev, &sg_table, dma_dir, 0);\nout_free_sg_table:\n\tsg_free_table(&sg_table);\nout_free_pages:\n\tkfree(pages);\n\n\treturn err;\n}\n\nstatic int pds_vfio_dirty_write_ack(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t    u32 offset, u32 len)\n{\n\treturn pds_vfio_dirty_seq_ack(pds_vfio, &pds_vfio->dirty.host_ack,\n\t\t\t\t      offset, len, WRITE_ACK);\n}\n\nstatic int pds_vfio_dirty_read_seq(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t   u32 offset, u32 len)\n{\n\treturn pds_vfio_dirty_seq_ack(pds_vfio, &pds_vfio->dirty.host_seq,\n\t\t\t\t      offset, len, READ_SEQ);\n}\n\nstatic int pds_vfio_dirty_process_bitmaps(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t\t\t  struct iova_bitmap *dirty_bitmap,\n\t\t\t\t\t  u32 bmp_offset, u32 len_bytes)\n{\n\tu64 page_size = pds_vfio->dirty.region_page_size;\n\tu64 region_start = pds_vfio->dirty.region_start;\n\tu32 bmp_offset_bit;\n\t__le64 *seq, *ack;\n\tint dword_count;\n\n\tdword_count = len_bytes / sizeof(u64);\n\tseq = (__le64 *)((u64)pds_vfio->dirty.host_seq.bmp + bmp_offset);\n\tack = (__le64 *)((u64)pds_vfio->dirty.host_ack.bmp + bmp_offset);\n\tbmp_offset_bit = bmp_offset * 8;\n\n\tfor (int i = 0; i < dword_count; i++) {\n\t\tu64 xor = le64_to_cpu(seq[i]) ^ le64_to_cpu(ack[i]);\n\n\t\t \n\t\tack[i] = seq[i];\n\n\t\tfor (u8 bit_i = 0; bit_i < BITS_PER_TYPE(u64); ++bit_i) {\n\t\t\tif (xor & BIT(bit_i)) {\n\t\t\t\tu64 abs_bit_i = bmp_offset_bit +\n\t\t\t\t\t\ti * BITS_PER_TYPE(u64) + bit_i;\n\t\t\t\tu64 addr = abs_bit_i * page_size + region_start;\n\n\t\t\t\tiova_bitmap_set(dirty_bitmap, addr, page_size);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int pds_vfio_dirty_sync(struct pds_vfio_pci_device *pds_vfio,\n\t\t\t       struct iova_bitmap *dirty_bitmap,\n\t\t\t       unsigned long iova, unsigned long length)\n{\n\tstruct device *dev = &pds_vfio->vfio_coredev.pdev->dev;\n\tstruct pds_vfio_dirty *dirty = &pds_vfio->dirty;\n\tu64 bmp_offset, bmp_bytes;\n\tu64 bitmap_size, pages;\n\tint err;\n\n\tdev_dbg(dev, \"vf%u: Get dirty page bitmap\\n\", pds_vfio->vf_id);\n\n\tif (!pds_vfio_dirty_is_enabled(pds_vfio)) {\n\t\tdev_err(dev, \"vf%u: Sync failed, dirty tracking is disabled\\n\",\n\t\t\tpds_vfio->vf_id);\n\t\treturn -EINVAL;\n\t}\n\n\tpages = DIV_ROUND_UP(length, pds_vfio->dirty.region_page_size);\n\tbitmap_size =\n\t\tround_up(pages, sizeof(u64) * BITS_PER_BYTE) / BITS_PER_BYTE;\n\n\tdev_dbg(dev,\n\t\t\"vf%u: iova 0x%lx length %lu page_size %llu pages %llu bitmap_size %llu\\n\",\n\t\tpds_vfio->vf_id, iova, length, pds_vfio->dirty.region_page_size,\n\t\tpages, bitmap_size);\n\n\tif (!length || ((iova - dirty->region_start + length) > dirty->region_size)) {\n\t\tdev_err(dev, \"Invalid iova 0x%lx and/or length 0x%lx to sync\\n\",\n\t\t\tiova, length);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tbmp_bytes = ALIGN(DIV_ROUND_UP(length / dirty->region_page_size,\n\t\t\t\t       sizeof(u64)),\n\t\t\t  sizeof(u64));\n\tif (bmp_bytes != bitmap_size) {\n\t\tdev_err(dev,\n\t\t\t\"Calculated bitmap bytes %llu not equal to bitmap size %llu\\n\",\n\t\t\tbmp_bytes, bitmap_size);\n\t\treturn -EINVAL;\n\t}\n\n\tbmp_offset = DIV_ROUND_UP((iova - dirty->region_start) /\n\t\t\t\t  dirty->region_page_size, sizeof(u64));\n\n\tdev_dbg(dev,\n\t\t\"Syncing dirty bitmap, iova 0x%lx length 0x%lx, bmp_offset %llu bmp_bytes %llu\\n\",\n\t\tiova, length, bmp_offset, bmp_bytes);\n\n\terr = pds_vfio_dirty_read_seq(pds_vfio, bmp_offset, bmp_bytes);\n\tif (err)\n\t\treturn err;\n\n\terr = pds_vfio_dirty_process_bitmaps(pds_vfio, dirty_bitmap, bmp_offset,\n\t\t\t\t\t     bmp_bytes);\n\tif (err)\n\t\treturn err;\n\n\terr = pds_vfio_dirty_write_ack(pds_vfio, bmp_offset, bmp_bytes);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nint pds_vfio_dma_logging_report(struct vfio_device *vdev, unsigned long iova,\n\t\t\t\tunsigned long length, struct iova_bitmap *dirty)\n{\n\tstruct pds_vfio_pci_device *pds_vfio =\n\t\tcontainer_of(vdev, struct pds_vfio_pci_device,\n\t\t\t     vfio_coredev.vdev);\n\tint err;\n\n\tmutex_lock(&pds_vfio->state_mutex);\n\terr = pds_vfio_dirty_sync(pds_vfio, dirty, iova, length);\n\tpds_vfio_state_mutex_unlock(pds_vfio);\n\n\treturn err;\n}\n\nint pds_vfio_dma_logging_start(struct vfio_device *vdev,\n\t\t\t       struct rb_root_cached *ranges, u32 nnodes,\n\t\t\t       u64 *page_size)\n{\n\tstruct pds_vfio_pci_device *pds_vfio =\n\t\tcontainer_of(vdev, struct pds_vfio_pci_device,\n\t\t\t     vfio_coredev.vdev);\n\tint err;\n\n\tmutex_lock(&pds_vfio->state_mutex);\n\tpds_vfio_send_host_vf_lm_status_cmd(pds_vfio, PDS_LM_STA_IN_PROGRESS);\n\terr = pds_vfio_dirty_enable(pds_vfio, ranges, nnodes, page_size);\n\tpds_vfio_state_mutex_unlock(pds_vfio);\n\n\treturn err;\n}\n\nint pds_vfio_dma_logging_stop(struct vfio_device *vdev)\n{\n\tstruct pds_vfio_pci_device *pds_vfio =\n\t\tcontainer_of(vdev, struct pds_vfio_pci_device,\n\t\t\t     vfio_coredev.vdev);\n\n\tmutex_lock(&pds_vfio->state_mutex);\n\tpds_vfio_dirty_disable(pds_vfio, true);\n\tpds_vfio_state_mutex_unlock(pds_vfio);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}