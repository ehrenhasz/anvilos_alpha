{
  "module_name": "hisi_acc_vfio_pci.c",
  "hash_id": "7a6789ae5ebd679064a6534bfff3b8716acc05e50bf5ef8cda54ec1302a618be",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/pci/hisilicon/hisi_acc_vfio_pci.c",
  "human_readable_source": "\n \n\n#include <linux/device.h>\n#include <linux/eventfd.h>\n#include <linux/file.h>\n#include <linux/hisi_acc_qm.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/vfio.h>\n#include <linux/vfio_pci_core.h>\n#include <linux/anon_inodes.h>\n\n#include \"hisi_acc_vfio_pci.h\"\n\n \nstatic int qm_wait_dev_not_ready(struct hisi_qm *qm)\n{\n\tu32 val;\n\n\treturn readl_relaxed_poll_timeout(qm->io_base + QM_VF_STATE,\n\t\t\t\tval, !(val & 0x1), MB_POLL_PERIOD_US,\n\t\t\t\tMB_POLL_TIMEOUT_US);\n}\n\n \nstatic u32 qm_check_reg_state(struct hisi_qm *qm, u32 regs)\n{\n\tint check_times = 0;\n\tu32 state;\n\n\tstate = readl(qm->io_base + regs);\n\twhile (state && check_times < ERROR_CHECK_TIMEOUT) {\n\t\tudelay(CHECK_DELAY_TIME);\n\t\tstate = readl(qm->io_base + regs);\n\t\tcheck_times++;\n\t}\n\n\treturn state;\n}\n\nstatic int qm_read_regs(struct hisi_qm *qm, u32 reg_addr,\n\t\t\tu32 *data, u8 nums)\n{\n\tint i;\n\n\tif (nums < 1 || nums > QM_REGS_MAX_LEN)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < nums; i++) {\n\t\tdata[i] = readl(qm->io_base + reg_addr);\n\t\treg_addr += QM_REG_ADDR_OFFSET;\n\t}\n\n\treturn 0;\n}\n\nstatic int qm_write_regs(struct hisi_qm *qm, u32 reg,\n\t\t\t u32 *data, u8 nums)\n{\n\tint i;\n\n\tif (nums < 1 || nums > QM_REGS_MAX_LEN)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < nums; i++)\n\t\twritel(data[i], qm->io_base + reg + i * QM_REG_ADDR_OFFSET);\n\n\treturn 0;\n}\n\nstatic int qm_get_vft(struct hisi_qm *qm, u32 *base)\n{\n\tu64 sqc_vft;\n\tu32 qp_num;\n\tint ret;\n\n\tret = hisi_qm_mb(qm, QM_MB_CMD_SQC_VFT_V2, 0, 0, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tsqc_vft = readl(qm->io_base + QM_MB_CMD_DATA_ADDR_L) |\n\t\t  ((u64)readl(qm->io_base + QM_MB_CMD_DATA_ADDR_H) <<\n\t\t  QM_XQC_ADDR_OFFSET);\n\t*base = QM_SQC_VFT_BASE_MASK_V2 & (sqc_vft >> QM_SQC_VFT_BASE_SHIFT_V2);\n\tqp_num = (QM_SQC_VFT_NUM_MASK_V2 &\n\t\t  (sqc_vft >> QM_SQC_VFT_NUM_SHIFT_V2)) + 1;\n\n\treturn qp_num;\n}\n\nstatic int qm_get_sqc(struct hisi_qm *qm, u64 *addr)\n{\n\tint ret;\n\n\tret = hisi_qm_mb(qm, QM_MB_CMD_SQC_BT, 0, 0, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t*addr = readl(qm->io_base + QM_MB_CMD_DATA_ADDR_L) |\n\t\t  ((u64)readl(qm->io_base + QM_MB_CMD_DATA_ADDR_H) <<\n\t\t  QM_XQC_ADDR_OFFSET);\n\n\treturn 0;\n}\n\nstatic int qm_get_cqc(struct hisi_qm *qm, u64 *addr)\n{\n\tint ret;\n\n\tret = hisi_qm_mb(qm, QM_MB_CMD_CQC_BT, 0, 0, 1);\n\tif (ret)\n\t\treturn ret;\n\n\t*addr = readl(qm->io_base + QM_MB_CMD_DATA_ADDR_L) |\n\t\t  ((u64)readl(qm->io_base + QM_MB_CMD_DATA_ADDR_H) <<\n\t\t  QM_XQC_ADDR_OFFSET);\n\n\treturn 0;\n}\n\nstatic int qm_get_regs(struct hisi_qm *qm, struct acc_vf_data *vf_data)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tint ret;\n\n\tret = qm_read_regs(qm, QM_VF_AEQ_INT_MASK, &vf_data->aeq_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_VF_AEQ_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_read_regs(qm, QM_VF_EQ_INT_MASK, &vf_data->eq_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_VF_EQ_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_read_regs(qm, QM_IFC_INT_SOURCE_V,\n\t\t\t   &vf_data->ifc_int_source, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_IFC_INT_SOURCE_V\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_read_regs(qm, QM_IFC_INT_MASK, &vf_data->ifc_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_IFC_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_read_regs(qm, QM_IFC_INT_SET_V, &vf_data->ifc_int_set, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_IFC_INT_SET_V\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_read_regs(qm, QM_PAGE_SIZE, &vf_data->page_size, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_PAGE_SIZE\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = qm_read_regs(qm, QM_EQC_DW0, vf_data->qm_eqc_dw, 7);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_EQC_DW\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = qm_read_regs(qm, QM_AEQC_DW0, vf_data->qm_aeqc_dw, 7);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_AEQC_DW\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int qm_set_regs(struct hisi_qm *qm, struct acc_vf_data *vf_data)\n{\n\tstruct device *dev = &qm->pdev->dev;\n\tint ret;\n\n\t \n\tif (unlikely(hisi_qm_wait_mb_ready(qm))) {\n\t\tdev_err(&qm->pdev->dev, \"QM device is not ready to write\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tret = qm_write_regs(qm, QM_VF_AEQ_INT_MASK, &vf_data->aeq_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_VF_AEQ_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_VF_EQ_INT_MASK, &vf_data->eq_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_VF_EQ_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_IFC_INT_SOURCE_V,\n\t\t\t    &vf_data->ifc_int_source, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_IFC_INT_SOURCE_V\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_IFC_INT_MASK, &vf_data->ifc_int_mask, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_IFC_INT_MASK\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_IFC_INT_SET_V, &vf_data->ifc_int_set, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_IFC_INT_SET_V\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_QUE_ISO_CFG_V, &vf_data->que_iso_cfg, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_QUE_ISO_CFG_V\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_write_regs(qm, QM_PAGE_SIZE, &vf_data->page_size, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_PAGE_SIZE\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = qm_write_regs(qm, QM_EQC_DW0, vf_data->qm_eqc_dw, 7);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_EQC_DW\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = qm_write_regs(qm, QM_AEQC_DW0, vf_data->qm_aeqc_dw, 7);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_AEQC_DW\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void qm_db(struct hisi_qm *qm, u16 qn, u8 cmd,\n\t\t  u16 index, u8 priority)\n{\n\tu64 doorbell;\n\tu64 dbase;\n\tu16 randata = 0;\n\n\tif (cmd == QM_DOORBELL_CMD_SQ || cmd == QM_DOORBELL_CMD_CQ)\n\t\tdbase = QM_DOORBELL_SQ_CQ_BASE_V2;\n\telse\n\t\tdbase = QM_DOORBELL_EQ_AEQ_BASE_V2;\n\n\tdoorbell = qn | ((u64)cmd << QM_DB_CMD_SHIFT_V2) |\n\t\t   ((u64)randata << QM_DB_RAND_SHIFT_V2) |\n\t\t   ((u64)index << QM_DB_INDEX_SHIFT_V2)\t |\n\t\t   ((u64)priority << QM_DB_PRIORITY_SHIFT_V2);\n\n\twriteq(doorbell, qm->io_base + dbase);\n}\n\nstatic int pf_qm_get_qp_num(struct hisi_qm *qm, int vf_id, u32 *rbase)\n{\n\tunsigned int val;\n\tu64 sqc_vft;\n\tu32 qp_num;\n\tint ret;\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + QM_VFT_CFG_RDY, val,\n\t\t\t\t\t val & BIT(0), MB_POLL_PERIOD_US,\n\t\t\t\t\t MB_POLL_TIMEOUT_US);\n\tif (ret)\n\t\treturn ret;\n\n\twritel(0x1, qm->io_base + QM_VFT_CFG_OP_WR);\n\t \n\twritel(0x0, qm->io_base + QM_VFT_CFG_TYPE);\n\twritel(vf_id, qm->io_base + QM_VFT_CFG);\n\n\twritel(0x0, qm->io_base + QM_VFT_CFG_RDY);\n\twritel(0x1, qm->io_base + QM_VFT_CFG_OP_ENABLE);\n\n\tret = readl_relaxed_poll_timeout(qm->io_base + QM_VFT_CFG_RDY, val,\n\t\t\t\t\t val & BIT(0), MB_POLL_PERIOD_US,\n\t\t\t\t\t MB_POLL_TIMEOUT_US);\n\tif (ret)\n\t\treturn ret;\n\n\tsqc_vft = readl(qm->io_base + QM_VFT_CFG_DATA_L) |\n\t\t  ((u64)readl(qm->io_base + QM_VFT_CFG_DATA_H) <<\n\t\t  QM_XQC_ADDR_OFFSET);\n\t*rbase = QM_SQC_VFT_BASE_MASK_V2 &\n\t\t  (sqc_vft >> QM_SQC_VFT_BASE_SHIFT_V2);\n\tqp_num = (QM_SQC_VFT_NUM_MASK_V2 &\n\t\t  (sqc_vft >> QM_SQC_VFT_NUM_SHIFT_V2)) + 1;\n\n\treturn qp_num;\n}\n\nstatic void qm_dev_cmd_init(struct hisi_qm *qm)\n{\n\t \n\twritel(0x1, qm->io_base + QM_IFC_INT_SOURCE_V);\n\n\t \n\twritel(0x0, qm->io_base + QM_IFC_INT_MASK);\n}\n\nstatic int vf_qm_cache_wb(struct hisi_qm *qm)\n{\n\tunsigned int val;\n\n\twritel(0x1, qm->io_base + QM_CACHE_WB_START);\n\tif (readl_relaxed_poll_timeout(qm->io_base + QM_CACHE_WB_DONE,\n\t\t\t\t       val, val & BIT(0), MB_POLL_PERIOD_US,\n\t\t\t\t       MB_POLL_TIMEOUT_US)) {\n\t\tdev_err(&qm->pdev->dev, \"vf QM writeback sqc cache fail\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void vf_qm_fun_reset(struct hisi_qm *qm)\n{\n\tint i;\n\n\tfor (i = 0; i < qm->qp_num; i++)\n\t\tqm_db(qm, i, QM_DOORBELL_CMD_SQ, 0, 1);\n}\n\nstatic int vf_qm_func_stop(struct hisi_qm *qm)\n{\n\treturn hisi_qm_mb(qm, QM_MB_CMD_PAUSE_QM, 0, 0, 0);\n}\n\nstatic int vf_qm_check_match(struct hisi_acc_vf_core_device *hisi_acc_vdev,\n\t\t\t     struct hisi_acc_vf_migration_file *migf)\n{\n\tstruct acc_vf_data *vf_data = &migf->vf_data;\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\tstruct hisi_qm *pf_qm = hisi_acc_vdev->pf_qm;\n\tstruct device *dev = &vf_qm->pdev->dev;\n\tu32 que_iso_state;\n\tint ret;\n\n\tif (migf->total_length < QM_MATCH_SIZE || hisi_acc_vdev->match_done)\n\t\treturn 0;\n\n\tif (vf_data->acc_magic != ACC_DEV_MAGIC) {\n\t\tdev_err(dev, \"failed to match ACC_DEV_MAGIC\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (vf_data->dev_id != hisi_acc_vdev->vf_dev->device) {\n\t\tdev_err(dev, \"failed to match VF devices\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = qm_get_vft(vf_qm, &vf_qm->qp_base);\n\tif (ret <= 0) {\n\t\tdev_err(dev, \"failed to get vft qp nums\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (ret != vf_data->qp_num) {\n\t\tdev_err(dev, \"failed to match VF qp num\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tvf_qm->qp_num = ret;\n\n\t \n\tret = qm_read_regs(pf_qm, QM_QUE_ISO_CFG_V, &que_iso_state, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_QUE_ISO_CFG_V\\n\");\n\t\treturn ret;\n\t}\n\n\tif (vf_data->que_iso_cfg != que_iso_state) {\n\t\tdev_err(dev, \"failed to match isolation state\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = qm_write_regs(vf_qm, QM_VF_STATE, &vf_data->vf_qm_state, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to write QM_VF_STATE\\n\");\n\t\treturn ret;\n\t}\n\n\thisi_acc_vdev->vf_qm_state = vf_data->vf_qm_state;\n\thisi_acc_vdev->match_done = true;\n\treturn 0;\n}\n\nstatic int vf_qm_get_match_data(struct hisi_acc_vf_core_device *hisi_acc_vdev,\n\t\t\t\tstruct acc_vf_data *vf_data)\n{\n\tstruct hisi_qm *pf_qm = hisi_acc_vdev->pf_qm;\n\tstruct device *dev = &pf_qm->pdev->dev;\n\tint vf_id = hisi_acc_vdev->vf_id;\n\tint ret;\n\n\tvf_data->acc_magic = ACC_DEV_MAGIC;\n\t \n\tvf_data->dev_id = hisi_acc_vdev->vf_dev->device;\n\n\t \n\tret = pf_qm_get_qp_num(pf_qm, vf_id, &vf_data->qp_base);\n\tif (ret <= 0) {\n\t\tdev_err(dev, \"failed to get vft qp nums!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tvf_data->qp_num = ret;\n\n\t \n\tret = qm_read_regs(pf_qm, QM_QUE_ISO_CFG_V, &vf_data->que_iso_cfg, 1);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read QM_QUE_ISO_CFG_V!\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int vf_qm_load_data(struct hisi_acc_vf_core_device *hisi_acc_vdev,\n\t\t\t   struct hisi_acc_vf_migration_file *migf)\n{\n\tstruct hisi_qm *qm = &hisi_acc_vdev->vf_qm;\n\tstruct device *dev = &qm->pdev->dev;\n\tstruct acc_vf_data *vf_data = &migf->vf_data;\n\tint ret;\n\n\t \n\tif (migf->total_length == QM_MATCH_SIZE)\n\t\treturn 0;\n\n\tif (migf->total_length < sizeof(struct acc_vf_data))\n\t\treturn -EINVAL;\n\n\tqm->eqe_dma = vf_data->eqe_dma;\n\tqm->aeqe_dma = vf_data->aeqe_dma;\n\tqm->sqc_dma = vf_data->sqc_dma;\n\tqm->cqc_dma = vf_data->cqc_dma;\n\n\tqm->qp_base = vf_data->qp_base;\n\tqm->qp_num = vf_data->qp_num;\n\n\tret = qm_set_regs(qm, vf_data);\n\tif (ret) {\n\t\tdev_err(dev, \"set VF regs failed\\n\");\n\t\treturn ret;\n\t}\n\n\tret = hisi_qm_mb(qm, QM_MB_CMD_SQC_BT, qm->sqc_dma, 0, 0);\n\tif (ret) {\n\t\tdev_err(dev, \"set sqc failed\\n\");\n\t\treturn ret;\n\t}\n\n\tret = hisi_qm_mb(qm, QM_MB_CMD_CQC_BT, qm->cqc_dma, 0, 0);\n\tif (ret) {\n\t\tdev_err(dev, \"set cqc failed\\n\");\n\t\treturn ret;\n\t}\n\n\tqm_dev_cmd_init(qm);\n\treturn 0;\n}\n\nstatic int vf_qm_state_save(struct hisi_acc_vf_core_device *hisi_acc_vdev,\n\t\t\t    struct hisi_acc_vf_migration_file *migf)\n{\n\tstruct acc_vf_data *vf_data = &migf->vf_data;\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\tstruct device *dev = &vf_qm->pdev->dev;\n\tint ret;\n\n\tif (unlikely(qm_wait_dev_not_ready(vf_qm))) {\n\t\t \n\t\tvf_data->vf_qm_state = QM_NOT_READY;\n\t\thisi_acc_vdev->vf_qm_state = vf_data->vf_qm_state;\n\t\tmigf->total_length = QM_MATCH_SIZE;\n\t\treturn 0;\n\t}\n\n\tvf_data->vf_qm_state = QM_READY;\n\thisi_acc_vdev->vf_qm_state = vf_data->vf_qm_state;\n\n\tret = vf_qm_cache_wb(vf_qm);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to writeback QM Cache!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qm_get_regs(vf_qm, vf_data);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\t \n\tvf_data->eqe_dma = vf_data->qm_eqc_dw[1];\n\tvf_data->eqe_dma <<= QM_XQC_ADDR_OFFSET;\n\tvf_data->eqe_dma |= vf_data->qm_eqc_dw[0];\n\tvf_data->aeqe_dma = vf_data->qm_aeqc_dw[1];\n\tvf_data->aeqe_dma <<= QM_XQC_ADDR_OFFSET;\n\tvf_data->aeqe_dma |= vf_data->qm_aeqc_dw[0];\n\n\t \n\tret = qm_get_sqc(vf_qm, &vf_data->sqc_dma);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read SQC addr!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = qm_get_cqc(vf_qm, &vf_data->cqc_dma);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to read CQC addr!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmigf->total_length = sizeof(struct acc_vf_data);\n\treturn 0;\n}\n\nstatic struct hisi_acc_vf_core_device *hisi_acc_drvdata(struct pci_dev *pdev)\n{\n\tstruct vfio_pci_core_device *core_device = dev_get_drvdata(&pdev->dev);\n\n\treturn container_of(core_device, struct hisi_acc_vf_core_device,\n\t\t\t    core_device);\n}\n\n \nstatic int\nhisi_acc_check_int_state(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct hisi_qm *vfqm = &hisi_acc_vdev->vf_qm;\n\tstruct hisi_qm *qm = hisi_acc_vdev->pf_qm;\n\tstruct pci_dev *vf_pdev = hisi_acc_vdev->vf_dev;\n\tstruct device *dev = &qm->pdev->dev;\n\tu32 state;\n\n\t \n\tstate = qm_check_reg_state(qm, QM_ABNORMAL_INT_STATUS);\n\tif (state) {\n\t\tdev_err(dev, \"failed to check QM RAS state!\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tstate = qm_check_reg_state(vfqm, QM_IFC_INT_STATUS);\n\tif (state) {\n\t\tdev_err(dev, \"failed to check QM IFC INT state!\\n\");\n\t\treturn -EBUSY;\n\t}\n\tstate = qm_check_reg_state(vfqm, QM_IFC_INT_SET_V);\n\tif (state) {\n\t\tdev_err(dev, \"failed to check QM IFC INT SET state!\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tswitch (vf_pdev->device) {\n\tcase PCI_DEVICE_ID_HUAWEI_SEC_VF:\n\t\tstate = qm_check_reg_state(qm, SEC_CORE_INT_STATUS);\n\t\tif (state) {\n\t\t\tdev_err(dev, \"failed to check QM SEC Core INT state!\\n\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\treturn 0;\n\tcase PCI_DEVICE_ID_HUAWEI_HPRE_VF:\n\t\tstate = qm_check_reg_state(qm, HPRE_HAC_INT_STATUS);\n\t\tif (state) {\n\t\t\tdev_err(dev, \"failed to check QM HPRE HAC INT state!\\n\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\treturn 0;\n\tcase PCI_DEVICE_ID_HUAWEI_ZIP_VF:\n\t\tstate = qm_check_reg_state(qm, HZIP_CORE_INT_STATUS);\n\t\tif (state) {\n\t\t\tdev_err(dev, \"failed to check QM ZIP Core INT state!\\n\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\tdev_err(dev, \"failed to detect acc module type!\\n\");\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void hisi_acc_vf_disable_fd(struct hisi_acc_vf_migration_file *migf)\n{\n\tmutex_lock(&migf->lock);\n\tmigf->disabled = true;\n\tmigf->total_length = 0;\n\tmigf->filp->f_pos = 0;\n\tmutex_unlock(&migf->lock);\n}\n\nstatic void hisi_acc_vf_disable_fds(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tif (hisi_acc_vdev->resuming_migf) {\n\t\thisi_acc_vf_disable_fd(hisi_acc_vdev->resuming_migf);\n\t\tfput(hisi_acc_vdev->resuming_migf->filp);\n\t\thisi_acc_vdev->resuming_migf = NULL;\n\t}\n\n\tif (hisi_acc_vdev->saving_migf) {\n\t\thisi_acc_vf_disable_fd(hisi_acc_vdev->saving_migf);\n\t\tfput(hisi_acc_vdev->saving_migf->filp);\n\t\thisi_acc_vdev->saving_migf = NULL;\n\t}\n}\n\n \nstatic void\nhisi_acc_vf_state_mutex_unlock(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\nagain:\n\tspin_lock(&hisi_acc_vdev->reset_lock);\n\tif (hisi_acc_vdev->deferred_reset) {\n\t\thisi_acc_vdev->deferred_reset = false;\n\t\tspin_unlock(&hisi_acc_vdev->reset_lock);\n\t\thisi_acc_vdev->vf_qm_state = QM_NOT_READY;\n\t\thisi_acc_vdev->mig_state = VFIO_DEVICE_STATE_RUNNING;\n\t\thisi_acc_vf_disable_fds(hisi_acc_vdev);\n\t\tgoto again;\n\t}\n\tmutex_unlock(&hisi_acc_vdev->state_mutex);\n\tspin_unlock(&hisi_acc_vdev->reset_lock);\n}\n\nstatic void hisi_acc_vf_start_device(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\n\tif (hisi_acc_vdev->vf_qm_state != QM_READY)\n\t\treturn;\n\n\t \n\tqm_dev_cmd_init(vf_qm);\n\n\tvf_qm_fun_reset(vf_qm);\n}\n\nstatic int hisi_acc_vf_load_state(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct device *dev = &hisi_acc_vdev->vf_dev->dev;\n\tstruct hisi_acc_vf_migration_file *migf = hisi_acc_vdev->resuming_migf;\n\tint ret;\n\n\t \n\tret = vf_qm_load_data(hisi_acc_vdev, migf);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to recover the VF!\\n\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int hisi_acc_vf_release_file(struct inode *inode, struct file *filp)\n{\n\tstruct hisi_acc_vf_migration_file *migf = filp->private_data;\n\n\thisi_acc_vf_disable_fd(migf);\n\tmutex_destroy(&migf->lock);\n\tkfree(migf);\n\treturn 0;\n}\n\nstatic ssize_t hisi_acc_vf_resume_write(struct file *filp, const char __user *buf,\n\t\t\t\t\tsize_t len, loff_t *pos)\n{\n\tstruct hisi_acc_vf_migration_file *migf = filp->private_data;\n\tu8 *vf_data = (u8 *)&migf->vf_data;\n\tloff_t requested_length;\n\tssize_t done = 0;\n\tint ret;\n\n\tif (pos)\n\t\treturn -ESPIPE;\n\tpos = &filp->f_pos;\n\n\tif (*pos < 0 ||\n\t    check_add_overflow((loff_t)len, *pos, &requested_length))\n\t\treturn -EINVAL;\n\n\tif (requested_length > sizeof(struct acc_vf_data))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&migf->lock);\n\tif (migf->disabled) {\n\t\tdone = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tret = copy_from_user(vf_data + *pos, buf, len);\n\tif (ret) {\n\t\tdone = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\t*pos += len;\n\tdone = len;\n\tmigf->total_length += len;\n\n\tret = vf_qm_check_match(migf->hisi_acc_vdev, migf);\n\tif (ret)\n\t\tdone = -EFAULT;\nout_unlock:\n\tmutex_unlock(&migf->lock);\n\treturn done;\n}\n\nstatic const struct file_operations hisi_acc_vf_resume_fops = {\n\t.owner = THIS_MODULE,\n\t.write = hisi_acc_vf_resume_write,\n\t.release = hisi_acc_vf_release_file,\n\t.llseek = no_llseek,\n};\n\nstatic struct hisi_acc_vf_migration_file *\nhisi_acc_vf_pci_resume(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct hisi_acc_vf_migration_file *migf;\n\n\tmigf = kzalloc(sizeof(*migf), GFP_KERNEL_ACCOUNT);\n\tif (!migf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmigf->filp = anon_inode_getfile(\"hisi_acc_vf_mig\", &hisi_acc_vf_resume_fops, migf,\n\t\t\t\t\tO_WRONLY);\n\tif (IS_ERR(migf->filp)) {\n\t\tint err = PTR_ERR(migf->filp);\n\n\t\tkfree(migf);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tstream_open(migf->filp->f_inode, migf->filp);\n\tmutex_init(&migf->lock);\n\tmigf->hisi_acc_vdev = hisi_acc_vdev;\n\treturn migf;\n}\n\nstatic long hisi_acc_vf_precopy_ioctl(struct file *filp,\n\t\t\t\t      unsigned int cmd, unsigned long arg)\n{\n\tstruct hisi_acc_vf_migration_file *migf = filp->private_data;\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = migf->hisi_acc_vdev;\n\tloff_t *pos = &filp->f_pos;\n\tstruct vfio_precopy_info info;\n\tunsigned long minsz;\n\tint ret;\n\n\tif (cmd != VFIO_MIG_GET_PRECOPY_INFO)\n\t\treturn -ENOTTY;\n\n\tminsz = offsetofend(struct vfio_precopy_info, dirty_bytes);\n\n\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\treturn -EFAULT;\n\tif (info.argsz < minsz)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&hisi_acc_vdev->state_mutex);\n\tif (hisi_acc_vdev->mig_state != VFIO_DEVICE_STATE_PRE_COPY) {\n\t\tmutex_unlock(&hisi_acc_vdev->state_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tmutex_lock(&migf->lock);\n\n\tif (migf->disabled) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tif (*pos > migf->total_length) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tinfo.dirty_bytes = 0;\n\tinfo.initial_bytes = migf->total_length - *pos;\n\n\tret = copy_to_user((void __user *)arg, &info, minsz) ? -EFAULT : 0;\nout:\n\tmutex_unlock(&migf->lock);\n\tmutex_unlock(&hisi_acc_vdev->state_mutex);\n\treturn ret;\n}\n\nstatic ssize_t hisi_acc_vf_save_read(struct file *filp, char __user *buf, size_t len,\n\t\t\t\t     loff_t *pos)\n{\n\tstruct hisi_acc_vf_migration_file *migf = filp->private_data;\n\tssize_t done = 0;\n\tint ret;\n\n\tif (pos)\n\t\treturn -ESPIPE;\n\tpos = &filp->f_pos;\n\n\tmutex_lock(&migf->lock);\n\tif (*pos > migf->total_length) {\n\t\tdone = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (migf->disabled) {\n\t\tdone = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tlen = min_t(size_t, migf->total_length - *pos, len);\n\tif (len) {\n\t\tu8 *vf_data = (u8 *)&migf->vf_data;\n\n\t\tret = copy_to_user(buf, vf_data + *pos, len);\n\t\tif (ret) {\n\t\t\tdone = -EFAULT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\t*pos += len;\n\t\tdone = len;\n\t}\nout_unlock:\n\tmutex_unlock(&migf->lock);\n\treturn done;\n}\n\nstatic const struct file_operations hisi_acc_vf_save_fops = {\n\t.owner = THIS_MODULE,\n\t.read = hisi_acc_vf_save_read,\n\t.unlocked_ioctl = hisi_acc_vf_precopy_ioctl,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.release = hisi_acc_vf_release_file,\n\t.llseek = no_llseek,\n};\n\nstatic struct hisi_acc_vf_migration_file *\nhisi_acc_open_saving_migf(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct hisi_acc_vf_migration_file *migf;\n\tint ret;\n\n\tmigf = kzalloc(sizeof(*migf), GFP_KERNEL_ACCOUNT);\n\tif (!migf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmigf->filp = anon_inode_getfile(\"hisi_acc_vf_mig\", &hisi_acc_vf_save_fops, migf,\n\t\t\t\t\tO_RDONLY);\n\tif (IS_ERR(migf->filp)) {\n\t\tint err = PTR_ERR(migf->filp);\n\n\t\tkfree(migf);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tstream_open(migf->filp->f_inode, migf->filp);\n\tmutex_init(&migf->lock);\n\tmigf->hisi_acc_vdev = hisi_acc_vdev;\n\n\tret = vf_qm_get_match_data(hisi_acc_vdev, &migf->vf_data);\n\tif (ret) {\n\t\tfput(migf->filp);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn migf;\n}\n\nstatic struct hisi_acc_vf_migration_file *\nhisi_acc_vf_pre_copy(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct hisi_acc_vf_migration_file *migf;\n\n\tmigf = hisi_acc_open_saving_migf(hisi_acc_vdev);\n\tif (IS_ERR(migf))\n\t\treturn migf;\n\n\tmigf->total_length = QM_MATCH_SIZE;\n\treturn migf;\n}\n\nstatic struct hisi_acc_vf_migration_file *\nhisi_acc_vf_stop_copy(struct hisi_acc_vf_core_device *hisi_acc_vdev, bool open)\n{\n\tint ret;\n\tstruct hisi_acc_vf_migration_file *migf = NULL;\n\n\tif (open) {\n\t\t \n\t\tmigf = hisi_acc_open_saving_migf(hisi_acc_vdev);\n\t\tif (IS_ERR(migf))\n\t\t\treturn migf;\n\t} else {\n\t\tmigf = hisi_acc_vdev->saving_migf;\n\t}\n\n\tret = vf_qm_state_save(hisi_acc_vdev, migf);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn open ? migf : NULL;\n}\n\nstatic int hisi_acc_vf_stop_device(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct device *dev = &hisi_acc_vdev->vf_dev->dev;\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\tint ret;\n\n\tret = vf_qm_func_stop(vf_qm);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to stop QM VF function!\\n\");\n\t\treturn ret;\n\t}\n\n\tret = hisi_acc_check_int_state(hisi_acc_vdev);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to check QM INT state!\\n\");\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic struct file *\nhisi_acc_vf_set_device_state(struct hisi_acc_vf_core_device *hisi_acc_vdev,\n\t\t\t     u32 new)\n{\n\tu32 cur = hisi_acc_vdev->mig_state;\n\tint ret;\n\n\tif (cur == VFIO_DEVICE_STATE_RUNNING && new == VFIO_DEVICE_STATE_PRE_COPY) {\n\t\tstruct hisi_acc_vf_migration_file *migf;\n\n\t\tmigf = hisi_acc_vf_pre_copy(hisi_acc_vdev);\n\t\tif (IS_ERR(migf))\n\t\t\treturn ERR_CAST(migf);\n\t\tget_file(migf->filp);\n\t\thisi_acc_vdev->saving_migf = migf;\n\t\treturn migf->filp;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_PRE_COPY && new == VFIO_DEVICE_STATE_STOP_COPY) {\n\t\tstruct hisi_acc_vf_migration_file *migf;\n\n\t\tret = hisi_acc_vf_stop_device(hisi_acc_vdev);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\n\t\tmigf = hisi_acc_vf_stop_copy(hisi_acc_vdev, false);\n\t\tif (IS_ERR(migf))\n\t\t\treturn ERR_CAST(migf);\n\n\t\treturn NULL;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_RUNNING && new == VFIO_DEVICE_STATE_STOP) {\n\t\tret = hisi_acc_vf_stop_device(hisi_acc_vdev);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\treturn NULL;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_STOP_COPY) {\n\t\tstruct hisi_acc_vf_migration_file *migf;\n\n\t\tmigf = hisi_acc_vf_stop_copy(hisi_acc_vdev, true);\n\t\tif (IS_ERR(migf))\n\t\t\treturn ERR_CAST(migf);\n\t\tget_file(migf->filp);\n\t\thisi_acc_vdev->saving_migf = migf;\n\t\treturn migf->filp;\n\t}\n\n\tif ((cur == VFIO_DEVICE_STATE_STOP_COPY && new == VFIO_DEVICE_STATE_STOP)) {\n\t\thisi_acc_vf_disable_fds(hisi_acc_vdev);\n\t\treturn NULL;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_RESUMING) {\n\t\tstruct hisi_acc_vf_migration_file *migf;\n\n\t\tmigf = hisi_acc_vf_pci_resume(hisi_acc_vdev);\n\t\tif (IS_ERR(migf))\n\t\t\treturn ERR_CAST(migf);\n\t\tget_file(migf->filp);\n\t\thisi_acc_vdev->resuming_migf = migf;\n\t\treturn migf->filp;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_RESUMING && new == VFIO_DEVICE_STATE_STOP) {\n\t\tret = hisi_acc_vf_load_state(hisi_acc_vdev);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t\thisi_acc_vf_disable_fds(hisi_acc_vdev);\n\t\treturn NULL;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_PRE_COPY && new == VFIO_DEVICE_STATE_RUNNING) {\n\t\thisi_acc_vf_disable_fds(hisi_acc_vdev);\n\t\treturn NULL;\n\t}\n\n\tif (cur == VFIO_DEVICE_STATE_STOP && new == VFIO_DEVICE_STATE_RUNNING) {\n\t\thisi_acc_vf_start_device(hisi_acc_vdev);\n\t\treturn NULL;\n\t}\n\n\t \n\tWARN_ON(true);\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct file *\nhisi_acc_vfio_pci_set_device_state(struct vfio_device *vdev,\n\t\t\t\t   enum vfio_device_mig_state new_state)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = container_of(vdev,\n\t\t\tstruct hisi_acc_vf_core_device, core_device.vdev);\n\tenum vfio_device_mig_state next_state;\n\tstruct file *res = NULL;\n\tint ret;\n\n\tmutex_lock(&hisi_acc_vdev->state_mutex);\n\twhile (new_state != hisi_acc_vdev->mig_state) {\n\t\tret = vfio_mig_get_next_state(vdev,\n\t\t\t\t\t      hisi_acc_vdev->mig_state,\n\t\t\t\t\t      new_state, &next_state);\n\t\tif (ret) {\n\t\t\tres = ERR_PTR(-EINVAL);\n\t\t\tbreak;\n\t\t}\n\n\t\tres = hisi_acc_vf_set_device_state(hisi_acc_vdev, next_state);\n\t\tif (IS_ERR(res))\n\t\t\tbreak;\n\t\thisi_acc_vdev->mig_state = next_state;\n\t\tif (WARN_ON(res && new_state != hisi_acc_vdev->mig_state)) {\n\t\t\tfput(res);\n\t\t\tres = ERR_PTR(-EINVAL);\n\t\t\tbreak;\n\t\t}\n\t}\n\thisi_acc_vf_state_mutex_unlock(hisi_acc_vdev);\n\treturn res;\n}\n\nstatic int\nhisi_acc_vfio_pci_get_data_size(struct vfio_device *vdev,\n\t\t\t\tunsigned long *stop_copy_length)\n{\n\t*stop_copy_length = sizeof(struct acc_vf_data);\n\treturn 0;\n}\n\nstatic int\nhisi_acc_vfio_pci_get_device_state(struct vfio_device *vdev,\n\t\t\t\t   enum vfio_device_mig_state *curr_state)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = container_of(vdev,\n\t\t\tstruct hisi_acc_vf_core_device, core_device.vdev);\n\n\tmutex_lock(&hisi_acc_vdev->state_mutex);\n\t*curr_state = hisi_acc_vdev->mig_state;\n\thisi_acc_vf_state_mutex_unlock(hisi_acc_vdev);\n\treturn 0;\n}\n\nstatic void hisi_acc_vf_pci_aer_reset_done(struct pci_dev *pdev)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = hisi_acc_drvdata(pdev);\n\n\tif (hisi_acc_vdev->core_device.vdev.migration_flags !=\n\t\t\t\tVFIO_MIGRATION_STOP_COPY)\n\t\treturn;\n\n\t \n\tspin_lock(&hisi_acc_vdev->reset_lock);\n\thisi_acc_vdev->deferred_reset = true;\n\tif (!mutex_trylock(&hisi_acc_vdev->state_mutex)) {\n\t\tspin_unlock(&hisi_acc_vdev->reset_lock);\n\t\treturn;\n\t}\n\tspin_unlock(&hisi_acc_vdev->reset_lock);\n\thisi_acc_vf_state_mutex_unlock(hisi_acc_vdev);\n}\n\nstatic int hisi_acc_vf_qm_init(struct hisi_acc_vf_core_device *hisi_acc_vdev)\n{\n\tstruct vfio_pci_core_device *vdev = &hisi_acc_vdev->core_device;\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\tstruct pci_dev *vf_dev = vdev->pdev;\n\n\t \n\n\tvf_qm->io_base =\n\t\tioremap(pci_resource_start(vf_dev, VFIO_PCI_BAR2_REGION_INDEX),\n\t\t\tpci_resource_len(vf_dev, VFIO_PCI_BAR2_REGION_INDEX));\n\tif (!vf_qm->io_base)\n\t\treturn -EIO;\n\n\tvf_qm->fun_type = QM_HW_VF;\n\tvf_qm->pdev = vf_dev;\n\tmutex_init(&vf_qm->mailbox_lock);\n\n\treturn 0;\n}\n\nstatic struct hisi_qm *hisi_acc_get_pf_qm(struct pci_dev *pdev)\n{\n\tstruct hisi_qm\t*pf_qm;\n\tstruct pci_driver *pf_driver;\n\n\tif (!pdev->is_virtfn)\n\t\treturn NULL;\n\n\tswitch (pdev->device) {\n\tcase PCI_DEVICE_ID_HUAWEI_SEC_VF:\n\t\tpf_driver = hisi_sec_get_pf_driver();\n\t\tbreak;\n\tcase PCI_DEVICE_ID_HUAWEI_HPRE_VF:\n\t\tpf_driver = hisi_hpre_get_pf_driver();\n\t\tbreak;\n\tcase PCI_DEVICE_ID_HUAWEI_ZIP_VF:\n\t\tpf_driver = hisi_zip_get_pf_driver();\n\t\tbreak;\n\tdefault:\n\t\treturn NULL;\n\t}\n\n\tif (!pf_driver)\n\t\treturn NULL;\n\n\tpf_qm = pci_iov_get_pf_drvdata(pdev, pf_driver);\n\n\treturn !IS_ERR(pf_qm) ? pf_qm : NULL;\n}\n\nstatic int hisi_acc_pci_rw_access_check(struct vfio_device *core_vdev,\n\t\t\t\t\tsize_t count, loff_t *ppos,\n\t\t\t\t\tsize_t *new_count)\n{\n\tunsigned int index = VFIO_PCI_OFFSET_TO_INDEX(*ppos);\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\n\tif (index == VFIO_PCI_BAR2_REGION_INDEX) {\n\t\tloff_t pos = *ppos & VFIO_PCI_OFFSET_MASK;\n\t\tresource_size_t end = pci_resource_len(vdev->pdev, index) / 2;\n\n\t\t \n\t\tif (pos >= end)\n\t\t\treturn -EINVAL;\n\n\t\t*new_count = min(count, (size_t)(end - pos));\n\t}\n\n\treturn 0;\n}\n\nstatic int hisi_acc_vfio_pci_mmap(struct vfio_device *core_vdev,\n\t\t\t\t  struct vm_area_struct *vma)\n{\n\tstruct vfio_pci_core_device *vdev =\n\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\tunsigned int index;\n\n\tindex = vma->vm_pgoff >> (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT);\n\tif (index == VFIO_PCI_BAR2_REGION_INDEX) {\n\t\tu64 req_len, pgoff, req_start;\n\t\tresource_size_t end = pci_resource_len(vdev->pdev, index) / 2;\n\n\t\treq_len = vma->vm_end - vma->vm_start;\n\t\tpgoff = vma->vm_pgoff &\n\t\t\t((1U << (VFIO_PCI_OFFSET_SHIFT - PAGE_SHIFT)) - 1);\n\t\treq_start = pgoff << PAGE_SHIFT;\n\n\t\tif (req_start + req_len > end)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn vfio_pci_core_mmap(core_vdev, vma);\n}\n\nstatic ssize_t hisi_acc_vfio_pci_write(struct vfio_device *core_vdev,\n\t\t\t\t       const char __user *buf, size_t count,\n\t\t\t\t       loff_t *ppos)\n{\n\tsize_t new_count = count;\n\tint ret;\n\n\tret = hisi_acc_pci_rw_access_check(core_vdev, count, ppos, &new_count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn vfio_pci_core_write(core_vdev, buf, new_count, ppos);\n}\n\nstatic ssize_t hisi_acc_vfio_pci_read(struct vfio_device *core_vdev,\n\t\t\t\t      char __user *buf, size_t count,\n\t\t\t\t      loff_t *ppos)\n{\n\tsize_t new_count = count;\n\tint ret;\n\n\tret = hisi_acc_pci_rw_access_check(core_vdev, count, ppos, &new_count);\n\tif (ret)\n\t\treturn ret;\n\n\treturn vfio_pci_core_read(core_vdev, buf, new_count, ppos);\n}\n\nstatic long hisi_acc_vfio_pci_ioctl(struct vfio_device *core_vdev, unsigned int cmd,\n\t\t\t\t    unsigned long arg)\n{\n\tif (cmd == VFIO_DEVICE_GET_REGION_INFO) {\n\t\tstruct vfio_pci_core_device *vdev =\n\t\t\tcontainer_of(core_vdev, struct vfio_pci_core_device, vdev);\n\t\tstruct pci_dev *pdev = vdev->pdev;\n\t\tstruct vfio_region_info info;\n\t\tunsigned long minsz;\n\n\t\tminsz = offsetofend(struct vfio_region_info, offset);\n\n\t\tif (copy_from_user(&info, (void __user *)arg, minsz))\n\t\t\treturn -EFAULT;\n\n\t\tif (info.argsz < minsz)\n\t\t\treturn -EINVAL;\n\n\t\tif (info.index == VFIO_PCI_BAR2_REGION_INDEX) {\n\t\t\tinfo.offset = VFIO_PCI_INDEX_TO_OFFSET(info.index);\n\n\t\t\t \n\t\t\tinfo.size = pci_resource_len(pdev, info.index) / 2;\n\n\t\t\tinfo.flags = VFIO_REGION_INFO_FLAG_READ |\n\t\t\t\t\tVFIO_REGION_INFO_FLAG_WRITE |\n\t\t\t\t\tVFIO_REGION_INFO_FLAG_MMAP;\n\n\t\t\treturn copy_to_user((void __user *)arg, &info, minsz) ?\n\t\t\t\t\t    -EFAULT : 0;\n\t\t}\n\t}\n\treturn vfio_pci_core_ioctl(core_vdev, cmd, arg);\n}\n\nstatic int hisi_acc_vfio_pci_open_device(struct vfio_device *core_vdev)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = container_of(core_vdev,\n\t\t\tstruct hisi_acc_vf_core_device, core_device.vdev);\n\tstruct vfio_pci_core_device *vdev = &hisi_acc_vdev->core_device;\n\tint ret;\n\n\tret = vfio_pci_core_enable(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tif (core_vdev->mig_ops) {\n\t\tret = hisi_acc_vf_qm_init(hisi_acc_vdev);\n\t\tif (ret) {\n\t\t\tvfio_pci_core_disable(vdev);\n\t\t\treturn ret;\n\t\t}\n\t\thisi_acc_vdev->mig_state = VFIO_DEVICE_STATE_RUNNING;\n\t}\n\n\tvfio_pci_core_finish_enable(vdev);\n\treturn 0;\n}\n\nstatic void hisi_acc_vfio_pci_close_device(struct vfio_device *core_vdev)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = container_of(core_vdev,\n\t\t\tstruct hisi_acc_vf_core_device, core_device.vdev);\n\tstruct hisi_qm *vf_qm = &hisi_acc_vdev->vf_qm;\n\n\tiounmap(vf_qm->io_base);\n\tvfio_pci_core_close_device(core_vdev);\n}\n\nstatic const struct vfio_migration_ops hisi_acc_vfio_pci_migrn_state_ops = {\n\t.migration_set_state = hisi_acc_vfio_pci_set_device_state,\n\t.migration_get_state = hisi_acc_vfio_pci_get_device_state,\n\t.migration_get_data_size = hisi_acc_vfio_pci_get_data_size,\n};\n\nstatic int hisi_acc_vfio_pci_migrn_init_dev(struct vfio_device *core_vdev)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = container_of(core_vdev,\n\t\t\tstruct hisi_acc_vf_core_device, core_device.vdev);\n\tstruct pci_dev *pdev = to_pci_dev(core_vdev->dev);\n\tstruct hisi_qm *pf_qm = hisi_acc_get_pf_qm(pdev);\n\n\thisi_acc_vdev->vf_id = pci_iov_vf_id(pdev) + 1;\n\thisi_acc_vdev->pf_qm = pf_qm;\n\thisi_acc_vdev->vf_dev = pdev;\n\tmutex_init(&hisi_acc_vdev->state_mutex);\n\n\tcore_vdev->migration_flags = VFIO_MIGRATION_STOP_COPY | VFIO_MIGRATION_PRE_COPY;\n\tcore_vdev->mig_ops = &hisi_acc_vfio_pci_migrn_state_ops;\n\n\treturn vfio_pci_core_init_dev(core_vdev);\n}\n\nstatic const struct vfio_device_ops hisi_acc_vfio_pci_migrn_ops = {\n\t.name = \"hisi-acc-vfio-pci-migration\",\n\t.init = hisi_acc_vfio_pci_migrn_init_dev,\n\t.release = vfio_pci_core_release_dev,\n\t.open_device = hisi_acc_vfio_pci_open_device,\n\t.close_device = hisi_acc_vfio_pci_close_device,\n\t.ioctl = hisi_acc_vfio_pci_ioctl,\n\t.device_feature = vfio_pci_core_ioctl_feature,\n\t.read = hisi_acc_vfio_pci_read,\n\t.write = hisi_acc_vfio_pci_write,\n\t.mmap = hisi_acc_vfio_pci_mmap,\n\t.request = vfio_pci_core_request,\n\t.match = vfio_pci_core_match,\n\t.bind_iommufd = vfio_iommufd_physical_bind,\n\t.unbind_iommufd = vfio_iommufd_physical_unbind,\n\t.attach_ioas = vfio_iommufd_physical_attach_ioas,\n\t.detach_ioas = vfio_iommufd_physical_detach_ioas,\n};\n\nstatic const struct vfio_device_ops hisi_acc_vfio_pci_ops = {\n\t.name = \"hisi-acc-vfio-pci\",\n\t.init = vfio_pci_core_init_dev,\n\t.release = vfio_pci_core_release_dev,\n\t.open_device = hisi_acc_vfio_pci_open_device,\n\t.close_device = vfio_pci_core_close_device,\n\t.ioctl = vfio_pci_core_ioctl,\n\t.device_feature = vfio_pci_core_ioctl_feature,\n\t.read = vfio_pci_core_read,\n\t.write = vfio_pci_core_write,\n\t.mmap = vfio_pci_core_mmap,\n\t.request = vfio_pci_core_request,\n\t.match = vfio_pci_core_match,\n\t.bind_iommufd = vfio_iommufd_physical_bind,\n\t.unbind_iommufd = vfio_iommufd_physical_unbind,\n\t.attach_ioas = vfio_iommufd_physical_attach_ioas,\n\t.detach_ioas = vfio_iommufd_physical_detach_ioas,\n};\n\nstatic int hisi_acc_vfio_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev;\n\tconst struct vfio_device_ops *ops = &hisi_acc_vfio_pci_ops;\n\tstruct hisi_qm *pf_qm;\n\tint vf_id;\n\tint ret;\n\n\tpf_qm = hisi_acc_get_pf_qm(pdev);\n\tif (pf_qm && pf_qm->ver >= QM_HW_V3) {\n\t\tvf_id = pci_iov_vf_id(pdev);\n\t\tif (vf_id >= 0)\n\t\t\tops = &hisi_acc_vfio_pci_migrn_ops;\n\t\telse\n\t\t\tpci_warn(pdev, \"migration support failed, continue with generic interface\\n\");\n\t}\n\n\thisi_acc_vdev = vfio_alloc_device(hisi_acc_vf_core_device,\n\t\t\t\t\t  core_device.vdev, &pdev->dev, ops);\n\tif (IS_ERR(hisi_acc_vdev))\n\t\treturn PTR_ERR(hisi_acc_vdev);\n\n\tdev_set_drvdata(&pdev->dev, &hisi_acc_vdev->core_device);\n\tret = vfio_pci_core_register_device(&hisi_acc_vdev->core_device);\n\tif (ret)\n\t\tgoto out_put_vdev;\n\treturn 0;\n\nout_put_vdev:\n\tvfio_put_device(&hisi_acc_vdev->core_device.vdev);\n\treturn ret;\n}\n\nstatic void hisi_acc_vfio_pci_remove(struct pci_dev *pdev)\n{\n\tstruct hisi_acc_vf_core_device *hisi_acc_vdev = hisi_acc_drvdata(pdev);\n\n\tvfio_pci_core_unregister_device(&hisi_acc_vdev->core_device);\n\tvfio_put_device(&hisi_acc_vdev->core_device.vdev);\n}\n\nstatic const struct pci_device_id hisi_acc_vfio_pci_table[] = {\n\t{ PCI_DRIVER_OVERRIDE_DEVICE_VFIO(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_SEC_VF) },\n\t{ PCI_DRIVER_OVERRIDE_DEVICE_VFIO(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_HPRE_VF) },\n\t{ PCI_DRIVER_OVERRIDE_DEVICE_VFIO(PCI_VENDOR_ID_HUAWEI, PCI_DEVICE_ID_HUAWEI_ZIP_VF) },\n\t{ }\n};\n\nMODULE_DEVICE_TABLE(pci, hisi_acc_vfio_pci_table);\n\nstatic const struct pci_error_handlers hisi_acc_vf_err_handlers = {\n\t.reset_done = hisi_acc_vf_pci_aer_reset_done,\n\t.error_detected = vfio_pci_core_aer_err_detected,\n};\n\nstatic struct pci_driver hisi_acc_vfio_pci_driver = {\n\t.name = KBUILD_MODNAME,\n\t.id_table = hisi_acc_vfio_pci_table,\n\t.probe = hisi_acc_vfio_pci_probe,\n\t.remove = hisi_acc_vfio_pci_remove,\n\t.err_handler = &hisi_acc_vf_err_handlers,\n\t.driver_managed_dma = true,\n};\n\nmodule_pci_driver(hisi_acc_vfio_pci_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Liu Longfang <liulongfang@huawei.com>\");\nMODULE_AUTHOR(\"Shameer Kolothum <shameerali.kolothum.thodi@huawei.com>\");\nMODULE_DESCRIPTION(\"HiSilicon VFIO PCI - VFIO PCI driver with live migration support for HiSilicon ACC device family\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}