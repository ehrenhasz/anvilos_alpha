{
  "module_name": "cmd.c",
  "hash_id": "07f3b311e3aef1680c01c76b2bdf98e77c786041cb3b8000e84c47c13145033d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/pci/mlx5/cmd.c",
  "human_readable_source": "\n \n\n#include \"cmd.h\"\n\nenum { CQ_OK = 0, CQ_EMPTY = -1, CQ_POLL_ERR = -2 };\n\nstatic int mlx5vf_is_migratable(struct mlx5_core_dev *mdev, u16 func_id)\n{\n\tint query_sz = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tvoid *query_cap = NULL, *cap;\n\tint ret;\n\n\tquery_cap = kzalloc(query_sz, GFP_KERNEL);\n\tif (!query_cap)\n\t\treturn -ENOMEM;\n\n\tret = mlx5_vport_get_other_func_cap(mdev, func_id, query_cap,\n\t\t\t\t\t    MLX5_CAP_GENERAL_2);\n\tif (ret)\n\t\tgoto out;\n\n\tcap = MLX5_ADDR_OF(query_hca_cap_out, query_cap, capability);\n\tif (!MLX5_GET(cmd_hca_cap_2, cap, migratable))\n\t\tret = -EOPNOTSUPP;\nout:\n\tkfree(query_cap);\n\treturn ret;\n}\n\nstatic int mlx5vf_cmd_get_vhca_id(struct mlx5_core_dev *mdev, u16 function_id,\n\t\t\t\t  u16 *vhca_id);\nstatic void\n_mlx5vf_free_page_tracker_resources(struct mlx5vf_pci_core_device *mvdev);\n\nint mlx5vf_cmd_suspend_vhca(struct mlx5vf_pci_core_device *mvdev, u16 op_mod)\n{\n\tstruct mlx5_vf_migration_file *migf = mvdev->saving_migf;\n\tu32 out[MLX5_ST_SZ_DW(suspend_vhca_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(suspend_vhca_in)] = {};\n\tint err;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\t \n\tif (migf) {\n\t\terr = wait_for_completion_interruptible(&migf->save_comp);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tMLX5_SET(suspend_vhca_in, in, opcode, MLX5_CMD_OP_SUSPEND_VHCA);\n\tMLX5_SET(suspend_vhca_in, in, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(suspend_vhca_in, in, op_mod, op_mod);\n\n\terr = mlx5_cmd_exec_inout(mvdev->mdev, suspend_vhca, in, out);\n\tif (migf)\n\t\tcomplete(&migf->save_comp);\n\n\treturn err;\n}\n\nint mlx5vf_cmd_resume_vhca(struct mlx5vf_pci_core_device *mvdev, u16 op_mod)\n{\n\tu32 out[MLX5_ST_SZ_DW(resume_vhca_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(resume_vhca_in)] = {};\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\tMLX5_SET(resume_vhca_in, in, opcode, MLX5_CMD_OP_RESUME_VHCA);\n\tMLX5_SET(resume_vhca_in, in, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(resume_vhca_in, in, op_mod, op_mod);\n\n\treturn mlx5_cmd_exec_inout(mvdev->mdev, resume_vhca, in, out);\n}\n\nint mlx5vf_cmd_query_vhca_migration_state(struct mlx5vf_pci_core_device *mvdev,\n\t\t\t\t\t  size_t *state_size, u8 query_flags)\n{\n\tu32 out[MLX5_ST_SZ_DW(query_vhca_migration_state_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(query_vhca_migration_state_in)] = {};\n\tbool inc = query_flags & MLX5VF_QUERY_INC;\n\tint ret;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\t \n\tif (inc) {\n\t\tret = wait_for_completion_interruptible(&mvdev->saving_migf->save_comp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tif (mvdev->saving_migf->state ==\n\t\t    MLX5_MIGF_STATE_PRE_COPY_ERROR) {\n\t\t\t \n\t\t\tif (!(query_flags & MLX5VF_QUERY_FINAL)) {\n\t\t\t\t*state_size = 0;\n\t\t\t\tcomplete(&mvdev->saving_migf->save_comp);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\tquery_flags &= ~MLX5VF_QUERY_INC;\n\t\t}\n\t}\n\n\tMLX5_SET(query_vhca_migration_state_in, in, opcode,\n\t\t MLX5_CMD_OP_QUERY_VHCA_MIGRATION_STATE);\n\tMLX5_SET(query_vhca_migration_state_in, in, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(query_vhca_migration_state_in, in, op_mod, 0);\n\tMLX5_SET(query_vhca_migration_state_in, in, incremental,\n\t\t query_flags & MLX5VF_QUERY_INC);\n\n\tret = mlx5_cmd_exec_inout(mvdev->mdev, query_vhca_migration_state, in,\n\t\t\t\t  out);\n\tif (inc)\n\t\tcomplete(&mvdev->saving_migf->save_comp);\n\n\tif (ret)\n\t\treturn ret;\n\n\t*state_size = MLX5_GET(query_vhca_migration_state_out, out,\n\t\t\t       required_umem_size);\n\treturn 0;\n}\n\nstatic void set_tracker_error(struct mlx5vf_pci_core_device *mvdev)\n{\n\t \n\tmvdev->tracker.is_err = true;\n\tcomplete(&mvdev->tracker_comp);\n}\n\nstatic int mlx5fv_vf_event(struct notifier_block *nb,\n\t\t\t   unsigned long event, void *data)\n{\n\tstruct mlx5vf_pci_core_device *mvdev =\n\t\tcontainer_of(nb, struct mlx5vf_pci_core_device, nb);\n\n\tswitch (event) {\n\tcase MLX5_PF_NOTIFY_ENABLE_VF:\n\t\tmutex_lock(&mvdev->state_mutex);\n\t\tmvdev->mdev_detach = false;\n\t\tmlx5vf_state_mutex_unlock(mvdev);\n\t\tbreak;\n\tcase MLX5_PF_NOTIFY_DISABLE_VF:\n\t\tmlx5vf_cmd_close_migratable(mvdev);\n\t\tmutex_lock(&mvdev->state_mutex);\n\t\tmvdev->mdev_detach = true;\n\t\tmlx5vf_state_mutex_unlock(mvdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nvoid mlx5vf_cmd_close_migratable(struct mlx5vf_pci_core_device *mvdev)\n{\n\tif (!mvdev->migrate_cap)\n\t\treturn;\n\n\t \n\tset_tracker_error(mvdev);\n\tmutex_lock(&mvdev->state_mutex);\n\tmlx5vf_disable_fds(mvdev);\n\t_mlx5vf_free_page_tracker_resources(mvdev);\n\tmlx5vf_state_mutex_unlock(mvdev);\n}\n\nvoid mlx5vf_cmd_remove_migratable(struct mlx5vf_pci_core_device *mvdev)\n{\n\tif (!mvdev->migrate_cap)\n\t\treturn;\n\n\tmlx5_sriov_blocking_notifier_unregister(mvdev->mdev, mvdev->vf_id,\n\t\t\t\t\t\t&mvdev->nb);\n\tdestroy_workqueue(mvdev->cb_wq);\n}\n\nvoid mlx5vf_cmd_set_migratable(struct mlx5vf_pci_core_device *mvdev,\n\t\t\t       const struct vfio_migration_ops *mig_ops,\n\t\t\t       const struct vfio_log_ops *log_ops)\n{\n\tstruct pci_dev *pdev = mvdev->core_device.pdev;\n\tint ret;\n\n\tif (!pdev->is_virtfn)\n\t\treturn;\n\n\tmvdev->mdev = mlx5_vf_get_core_dev(pdev);\n\tif (!mvdev->mdev)\n\t\treturn;\n\n\tif (!MLX5_CAP_GEN(mvdev->mdev, migration))\n\t\tgoto end;\n\n\tmvdev->vf_id = pci_iov_vf_id(pdev);\n\tif (mvdev->vf_id < 0)\n\t\tgoto end;\n\n\tret = mlx5vf_is_migratable(mvdev->mdev, mvdev->vf_id + 1);\n\tif (ret)\n\t\tgoto end;\n\n\tif (mlx5vf_cmd_get_vhca_id(mvdev->mdev, mvdev->vf_id + 1,\n\t\t\t\t   &mvdev->vhca_id))\n\t\tgoto end;\n\n\tmvdev->cb_wq = alloc_ordered_workqueue(\"mlx5vf_wq\", 0);\n\tif (!mvdev->cb_wq)\n\t\tgoto end;\n\n\tmutex_init(&mvdev->state_mutex);\n\tspin_lock_init(&mvdev->reset_lock);\n\tmvdev->nb.notifier_call = mlx5fv_vf_event;\n\tret = mlx5_sriov_blocking_notifier_register(mvdev->mdev, mvdev->vf_id,\n\t\t\t\t\t\t    &mvdev->nb);\n\tif (ret) {\n\t\tdestroy_workqueue(mvdev->cb_wq);\n\t\tgoto end;\n\t}\n\n\tmvdev->migrate_cap = 1;\n\tmvdev->core_device.vdev.migration_flags =\n\t\tVFIO_MIGRATION_STOP_COPY |\n\t\tVFIO_MIGRATION_P2P;\n\tmvdev->core_device.vdev.mig_ops = mig_ops;\n\tinit_completion(&mvdev->tracker_comp);\n\tif (MLX5_CAP_GEN(mvdev->mdev, adv_virtualization))\n\t\tmvdev->core_device.vdev.log_ops = log_ops;\n\n\tif (MLX5_CAP_GEN_2(mvdev->mdev, migration_multi_load) &&\n\t    MLX5_CAP_GEN_2(mvdev->mdev, migration_tracking_state))\n\t\tmvdev->core_device.vdev.migration_flags |=\n\t\t\tVFIO_MIGRATION_PRE_COPY;\n\nend:\n\tmlx5_vf_put_core_dev(mvdev->mdev);\n}\n\nstatic int mlx5vf_cmd_get_vhca_id(struct mlx5_core_dev *mdev, u16 function_id,\n\t\t\t\t  u16 *vhca_id)\n{\n\tu32 in[MLX5_ST_SZ_DW(query_hca_cap_in)] = {};\n\tint out_size;\n\tvoid *out;\n\tint ret;\n\n\tout_size = MLX5_ST_SZ_BYTES(query_hca_cap_out);\n\tout = kzalloc(out_size, GFP_KERNEL);\n\tif (!out)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(query_hca_cap_in, in, opcode, MLX5_CMD_OP_QUERY_HCA_CAP);\n\tMLX5_SET(query_hca_cap_in, in, other_function, 1);\n\tMLX5_SET(query_hca_cap_in, in, function_id, function_id);\n\tMLX5_SET(query_hca_cap_in, in, op_mod,\n\t\t MLX5_SET_HCA_CAP_OP_MOD_GENERAL_DEVICE << 1 |\n\t\t HCA_CAP_OPMOD_GET_CUR);\n\n\tret = mlx5_cmd_exec_inout(mdev, query_hca_cap, in, out);\n\tif (ret)\n\t\tgoto err_exec;\n\n\t*vhca_id = MLX5_GET(query_hca_cap_out, out,\n\t\t\t    capability.cmd_hca_cap.vhca_id);\n\nerr_exec:\n\tkfree(out);\n\treturn ret;\n}\n\nstatic int _create_mkey(struct mlx5_core_dev *mdev, u32 pdn,\n\t\t\tstruct mlx5_vhca_data_buffer *buf,\n\t\t\tstruct mlx5_vhca_recv_buf *recv_buf,\n\t\t\tu32 *mkey)\n{\n\tsize_t npages = buf ? DIV_ROUND_UP(buf->allocated_length, PAGE_SIZE) :\n\t\t\t\trecv_buf->npages;\n\tint err = 0, inlen;\n\t__be64 *mtt;\n\tvoid *mkc;\n\tu32 *in;\n\n\tinlen = MLX5_ST_SZ_BYTES(create_mkey_in) +\n\t\tsizeof(*mtt) * round_up(npages, 2);\n\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tMLX5_SET(create_mkey_in, in, translations_octword_actual_size,\n\t\t DIV_ROUND_UP(npages, 2));\n\tmtt = (__be64 *)MLX5_ADDR_OF(create_mkey_in, in, klm_pas_mtt);\n\n\tif (buf) {\n\t\tstruct sg_dma_page_iter dma_iter;\n\n\t\tfor_each_sgtable_dma_page(&buf->table.sgt, &dma_iter, 0)\n\t\t\t*mtt++ = cpu_to_be64(sg_page_iter_dma_address(&dma_iter));\n\t} else {\n\t\tint i;\n\n\t\tfor (i = 0; i < npages; i++)\n\t\t\t*mtt++ = cpu_to_be64(recv_buf->dma_addrs[i]);\n\t}\n\n\tmkc = MLX5_ADDR_OF(create_mkey_in, in, memory_key_mkey_entry);\n\tMLX5_SET(mkc, mkc, access_mode_1_0, MLX5_MKC_ACCESS_MODE_MTT);\n\tMLX5_SET(mkc, mkc, lr, 1);\n\tMLX5_SET(mkc, mkc, lw, 1);\n\tMLX5_SET(mkc, mkc, rr, 1);\n\tMLX5_SET(mkc, mkc, rw, 1);\n\tMLX5_SET(mkc, mkc, pd, pdn);\n\tMLX5_SET(mkc, mkc, bsf_octword_size, 0);\n\tMLX5_SET(mkc, mkc, qpn, 0xffffff);\n\tMLX5_SET(mkc, mkc, log_page_size, PAGE_SHIFT);\n\tMLX5_SET(mkc, mkc, translations_octword_size, DIV_ROUND_UP(npages, 2));\n\tMLX5_SET64(mkc, mkc, len, npages * PAGE_SIZE);\n\terr = mlx5_core_create_mkey(mdev, mkey, in, inlen);\n\tkvfree(in);\n\treturn err;\n}\n\nstatic int mlx5vf_dma_data_buffer(struct mlx5_vhca_data_buffer *buf)\n{\n\tstruct mlx5vf_pci_core_device *mvdev = buf->migf->mvdev;\n\tstruct mlx5_core_dev *mdev = mvdev->mdev;\n\tint ret;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\tif (buf->dmaed || !buf->allocated_length)\n\t\treturn -EINVAL;\n\n\tret = dma_map_sgtable(mdev->device, &buf->table.sgt, buf->dma_dir, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tret = _create_mkey(mdev, buf->migf->pdn, buf, NULL, &buf->mkey);\n\tif (ret)\n\t\tgoto err;\n\n\tbuf->dmaed = true;\n\n\treturn 0;\nerr:\n\tdma_unmap_sgtable(mdev->device, &buf->table.sgt, buf->dma_dir, 0);\n\treturn ret;\n}\n\nvoid mlx5vf_free_data_buffer(struct mlx5_vhca_data_buffer *buf)\n{\n\tstruct mlx5_vf_migration_file *migf = buf->migf;\n\tstruct sg_page_iter sg_iter;\n\n\tlockdep_assert_held(&migf->mvdev->state_mutex);\n\tWARN_ON(migf->mvdev->mdev_detach);\n\n\tif (buf->dmaed) {\n\t\tmlx5_core_destroy_mkey(migf->mvdev->mdev, buf->mkey);\n\t\tdma_unmap_sgtable(migf->mvdev->mdev->device, &buf->table.sgt,\n\t\t\t\t  buf->dma_dir, 0);\n\t}\n\n\t \n\tfor_each_sgtable_page(&buf->table.sgt, &sg_iter, 0)\n\t\t__free_page(sg_page_iter_page(&sg_iter));\n\tsg_free_append_table(&buf->table);\n\tkfree(buf);\n}\n\nstruct mlx5_vhca_data_buffer *\nmlx5vf_alloc_data_buffer(struct mlx5_vf_migration_file *migf,\n\t\t\t size_t length,\n\t\t\t enum dma_data_direction dma_dir)\n{\n\tstruct mlx5_vhca_data_buffer *buf;\n\tint ret;\n\n\tbuf = kzalloc(sizeof(*buf), GFP_KERNEL_ACCOUNT);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf->dma_dir = dma_dir;\n\tbuf->migf = migf;\n\tif (length) {\n\t\tret = mlx5vf_add_migration_pages(buf,\n\t\t\t\tDIV_ROUND_UP_ULL(length, PAGE_SIZE));\n\t\tif (ret)\n\t\t\tgoto end;\n\n\t\tif (dma_dir != DMA_NONE) {\n\t\t\tret = mlx5vf_dma_data_buffer(buf);\n\t\t\tif (ret)\n\t\t\t\tgoto end;\n\t\t}\n\t}\n\n\treturn buf;\nend:\n\tmlx5vf_free_data_buffer(buf);\n\treturn ERR_PTR(ret);\n}\n\nvoid mlx5vf_put_data_buffer(struct mlx5_vhca_data_buffer *buf)\n{\n\tspin_lock_irq(&buf->migf->list_lock);\n\tlist_add_tail(&buf->buf_elm, &buf->migf->avail_list);\n\tspin_unlock_irq(&buf->migf->list_lock);\n}\n\nstruct mlx5_vhca_data_buffer *\nmlx5vf_get_data_buffer(struct mlx5_vf_migration_file *migf,\n\t\t       size_t length, enum dma_data_direction dma_dir)\n{\n\tstruct mlx5_vhca_data_buffer *buf, *temp_buf;\n\tstruct list_head free_list;\n\n\tlockdep_assert_held(&migf->mvdev->state_mutex);\n\tif (migf->mvdev->mdev_detach)\n\t\treturn ERR_PTR(-ENOTCONN);\n\n\tINIT_LIST_HEAD(&free_list);\n\n\tspin_lock_irq(&migf->list_lock);\n\tlist_for_each_entry_safe(buf, temp_buf, &migf->avail_list, buf_elm) {\n\t\tif (buf->dma_dir == dma_dir) {\n\t\t\tlist_del_init(&buf->buf_elm);\n\t\t\tif (buf->allocated_length >= length) {\n\t\t\t\tspin_unlock_irq(&migf->list_lock);\n\t\t\t\tgoto found;\n\t\t\t}\n\t\t\t \n\t\t\tlist_add(&buf->buf_elm, &free_list);\n\t\t}\n\t}\n\tspin_unlock_irq(&migf->list_lock);\n\tbuf = mlx5vf_alloc_data_buffer(migf, length, dma_dir);\n\nfound:\n\twhile ((temp_buf = list_first_entry_or_null(&free_list,\n\t\t\t\tstruct mlx5_vhca_data_buffer, buf_elm))) {\n\t\tlist_del(&temp_buf->buf_elm);\n\t\tmlx5vf_free_data_buffer(temp_buf);\n\t}\n\n\treturn buf;\n}\n\nvoid mlx5vf_mig_file_cleanup_cb(struct work_struct *_work)\n{\n\tstruct mlx5vf_async_data *async_data = container_of(_work,\n\t\tstruct mlx5vf_async_data, work);\n\tstruct mlx5_vf_migration_file *migf = container_of(async_data,\n\t\tstruct mlx5_vf_migration_file, async_data);\n\n\tmutex_lock(&migf->lock);\n\tif (async_data->status) {\n\t\tmlx5vf_put_data_buffer(async_data->buf);\n\t\tif (async_data->header_buf)\n\t\t\tmlx5vf_put_data_buffer(async_data->header_buf);\n\t\tif (async_data->status == MLX5_CMD_STAT_BAD_RES_STATE_ERR)\n\t\t\tmigf->state = MLX5_MIGF_STATE_PRE_COPY_ERROR;\n\t\telse\n\t\t\tmigf->state = MLX5_MIGF_STATE_ERROR;\n\t\twake_up_interruptible(&migf->poll_wait);\n\t}\n\tmutex_unlock(&migf->lock);\n\tkvfree(async_data->out);\n\tcomplete(&migf->save_comp);\n\tfput(migf->filp);\n}\n\nstatic int add_buf_header(struct mlx5_vhca_data_buffer *header_buf,\n\t\t\t  size_t image_size, bool initial_pre_copy)\n{\n\tstruct mlx5_vf_migration_file *migf = header_buf->migf;\n\tstruct mlx5_vf_migration_header header = {};\n\tunsigned long flags;\n\tstruct page *page;\n\tu8 *to_buff;\n\n\theader.record_size = cpu_to_le64(image_size);\n\theader.flags = cpu_to_le32(MLX5_MIGF_HEADER_FLAGS_TAG_MANDATORY);\n\theader.tag = cpu_to_le32(MLX5_MIGF_HEADER_TAG_FW_DATA);\n\tpage = mlx5vf_get_migration_page(header_buf, 0);\n\tif (!page)\n\t\treturn -EINVAL;\n\tto_buff = kmap_local_page(page);\n\tmemcpy(to_buff, &header, sizeof(header));\n\tkunmap_local(to_buff);\n\theader_buf->length = sizeof(header);\n\theader_buf->start_pos = header_buf->migf->max_pos;\n\tmigf->max_pos += header_buf->length;\n\tspin_lock_irqsave(&migf->list_lock, flags);\n\tlist_add_tail(&header_buf->buf_elm, &migf->buf_list);\n\tspin_unlock_irqrestore(&migf->list_lock, flags);\n\tif (initial_pre_copy)\n\t\tmigf->pre_copy_initial_bytes += sizeof(header);\n\treturn 0;\n}\n\nstatic void mlx5vf_save_callback(int status, struct mlx5_async_work *context)\n{\n\tstruct mlx5vf_async_data *async_data = container_of(context,\n\t\t\tstruct mlx5vf_async_data, cb_work);\n\tstruct mlx5_vf_migration_file *migf = container_of(async_data,\n\t\t\tstruct mlx5_vf_migration_file, async_data);\n\n\tif (!status) {\n\t\tsize_t image_size;\n\t\tunsigned long flags;\n\t\tbool initial_pre_copy = migf->state != MLX5_MIGF_STATE_PRE_COPY &&\n\t\t\t\t!async_data->last_chunk;\n\n\t\timage_size = MLX5_GET(save_vhca_state_out, async_data->out,\n\t\t\t\t      actual_image_size);\n\t\tif (async_data->header_buf) {\n\t\t\tstatus = add_buf_header(async_data->header_buf, image_size,\n\t\t\t\t\t\tinitial_pre_copy);\n\t\t\tif (status)\n\t\t\t\tgoto err;\n\t\t}\n\t\tasync_data->buf->length = image_size;\n\t\tasync_data->buf->start_pos = migf->max_pos;\n\t\tmigf->max_pos += async_data->buf->length;\n\t\tspin_lock_irqsave(&migf->list_lock, flags);\n\t\tlist_add_tail(&async_data->buf->buf_elm, &migf->buf_list);\n\t\tspin_unlock_irqrestore(&migf->list_lock, flags);\n\t\tif (initial_pre_copy)\n\t\t\tmigf->pre_copy_initial_bytes += image_size;\n\t\tmigf->state = async_data->last_chunk ?\n\t\t\tMLX5_MIGF_STATE_COMPLETE : MLX5_MIGF_STATE_PRE_COPY;\n\t\twake_up_interruptible(&migf->poll_wait);\n\t}\n\nerr:\n\t \n\tif (status == -EREMOTEIO)\n\t\tstatus = MLX5_GET(save_vhca_state_out, async_data->out, status);\n\tasync_data->status = status;\n\tqueue_work(migf->mvdev->cb_wq, &async_data->work);\n}\n\nint mlx5vf_cmd_save_vhca_state(struct mlx5vf_pci_core_device *mvdev,\n\t\t\t       struct mlx5_vf_migration_file *migf,\n\t\t\t       struct mlx5_vhca_data_buffer *buf, bool inc,\n\t\t\t       bool track)\n{\n\tu32 out_size = MLX5_ST_SZ_BYTES(save_vhca_state_out);\n\tu32 in[MLX5_ST_SZ_DW(save_vhca_state_in)] = {};\n\tstruct mlx5_vhca_data_buffer *header_buf = NULL;\n\tstruct mlx5vf_async_data *async_data;\n\tint err;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\terr = wait_for_completion_interruptible(&migf->save_comp);\n\tif (err)\n\t\treturn err;\n\n\tif (migf->state == MLX5_MIGF_STATE_PRE_COPY_ERROR)\n\t\t \n\t\tinc = false;\n\n\tMLX5_SET(save_vhca_state_in, in, opcode,\n\t\t MLX5_CMD_OP_SAVE_VHCA_STATE);\n\tMLX5_SET(save_vhca_state_in, in, op_mod, 0);\n\tMLX5_SET(save_vhca_state_in, in, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(save_vhca_state_in, in, mkey, buf->mkey);\n\tMLX5_SET(save_vhca_state_in, in, size, buf->allocated_length);\n\tMLX5_SET(save_vhca_state_in, in, incremental, inc);\n\tMLX5_SET(save_vhca_state_in, in, set_track, track);\n\n\tasync_data = &migf->async_data;\n\tasync_data->buf = buf;\n\tasync_data->last_chunk = !track;\n\tasync_data->out = kvzalloc(out_size, GFP_KERNEL);\n\tif (!async_data->out) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\tif (MLX5VF_PRE_COPY_SUPP(mvdev)) {\n\t\tif (async_data->last_chunk && migf->buf_header) {\n\t\t\theader_buf = migf->buf_header;\n\t\t\tmigf->buf_header = NULL;\n\t\t} else {\n\t\t\theader_buf = mlx5vf_get_data_buffer(migf,\n\t\t\t\tsizeof(struct mlx5_vf_migration_header), DMA_NONE);\n\t\t\tif (IS_ERR(header_buf)) {\n\t\t\t\terr = PTR_ERR(header_buf);\n\t\t\t\tgoto err_free;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (async_data->last_chunk)\n\t\tmigf->state = MLX5_MIGF_STATE_SAVE_LAST;\n\n\tasync_data->header_buf = header_buf;\n\tget_file(migf->filp);\n\terr = mlx5_cmd_exec_cb(&migf->async_ctx, in, sizeof(in),\n\t\t\t       async_data->out,\n\t\t\t       out_size, mlx5vf_save_callback,\n\t\t\t       &async_data->cb_work);\n\tif (err)\n\t\tgoto err_exec;\n\n\treturn 0;\n\nerr_exec:\n\tif (header_buf)\n\t\tmlx5vf_put_data_buffer(header_buf);\n\tfput(migf->filp);\nerr_free:\n\tkvfree(async_data->out);\nerr_out:\n\tcomplete(&migf->save_comp);\n\treturn err;\n}\n\nint mlx5vf_cmd_load_vhca_state(struct mlx5vf_pci_core_device *mvdev,\n\t\t\t       struct mlx5_vf_migration_file *migf,\n\t\t\t       struct mlx5_vhca_data_buffer *buf)\n{\n\tu32 out[MLX5_ST_SZ_DW(load_vhca_state_out)] = {};\n\tu32 in[MLX5_ST_SZ_DW(load_vhca_state_in)] = {};\n\tint err;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\tif (!buf->dmaed) {\n\t\terr = mlx5vf_dma_data_buffer(buf);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tMLX5_SET(load_vhca_state_in, in, opcode,\n\t\t MLX5_CMD_OP_LOAD_VHCA_STATE);\n\tMLX5_SET(load_vhca_state_in, in, op_mod, 0);\n\tMLX5_SET(load_vhca_state_in, in, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(load_vhca_state_in, in, mkey, buf->mkey);\n\tMLX5_SET(load_vhca_state_in, in, size, buf->length);\n\treturn mlx5_cmd_exec_inout(mvdev->mdev, load_vhca_state, in, out);\n}\n\nint mlx5vf_cmd_alloc_pd(struct mlx5_vf_migration_file *migf)\n{\n\tint err;\n\n\tlockdep_assert_held(&migf->mvdev->state_mutex);\n\tif (migf->mvdev->mdev_detach)\n\t\treturn -ENOTCONN;\n\n\terr = mlx5_core_alloc_pd(migf->mvdev->mdev, &migf->pdn);\n\treturn err;\n}\n\nvoid mlx5vf_cmd_dealloc_pd(struct mlx5_vf_migration_file *migf)\n{\n\tlockdep_assert_held(&migf->mvdev->state_mutex);\n\tif (migf->mvdev->mdev_detach)\n\t\treturn;\n\n\tmlx5_core_dealloc_pd(migf->mvdev->mdev, migf->pdn);\n}\n\nvoid mlx5fv_cmd_clean_migf_resources(struct mlx5_vf_migration_file *migf)\n{\n\tstruct mlx5_vhca_data_buffer *entry;\n\n\tlockdep_assert_held(&migf->mvdev->state_mutex);\n\tWARN_ON(migf->mvdev->mdev_detach);\n\n\tif (migf->buf) {\n\t\tmlx5vf_free_data_buffer(migf->buf);\n\t\tmigf->buf = NULL;\n\t}\n\n\tif (migf->buf_header) {\n\t\tmlx5vf_free_data_buffer(migf->buf_header);\n\t\tmigf->buf_header = NULL;\n\t}\n\n\tlist_splice(&migf->avail_list, &migf->buf_list);\n\n\twhile ((entry = list_first_entry_or_null(&migf->buf_list,\n\t\t\t\tstruct mlx5_vhca_data_buffer, buf_elm))) {\n\t\tlist_del(&entry->buf_elm);\n\t\tmlx5vf_free_data_buffer(entry);\n\t}\n\n\tmlx5vf_cmd_dealloc_pd(migf);\n}\n\nstatic int mlx5vf_create_tracker(struct mlx5_core_dev *mdev,\n\t\t\t\t struct mlx5vf_pci_core_device *mvdev,\n\t\t\t\t struct rb_root_cached *ranges, u32 nnodes)\n{\n\tint max_num_range =\n\t\tMLX5_CAP_ADV_VIRTUALIZATION(mdev, pg_track_max_num_range);\n\tstruct mlx5_vhca_page_tracker *tracker = &mvdev->tracker;\n\tint record_size = MLX5_ST_SZ_BYTES(page_track_range);\n\tu32 out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)] = {};\n\tstruct interval_tree_node *node = NULL;\n\tu64 total_ranges_len = 0;\n\tu32 num_ranges = nnodes;\n\tu8 log_addr_space_size;\n\tvoid *range_list_ptr;\n\tvoid *obj_context;\n\tvoid *cmd_hdr;\n\tint inlen;\n\tvoid *in;\n\tint err;\n\tint i;\n\n\tif (num_ranges > max_num_range) {\n\t\tvfio_combine_iova_ranges(ranges, nnodes, max_num_range);\n\t\tnum_ranges = max_num_range;\n\t}\n\n\tinlen = MLX5_ST_SZ_BYTES(create_page_track_obj_in) +\n\t\t\t\t record_size * num_ranges;\n\tin = kzalloc(inlen, GFP_KERNEL);\n\tif (!in)\n\t\treturn -ENOMEM;\n\n\tcmd_hdr = MLX5_ADDR_OF(create_page_track_obj_in, in,\n\t\t\t       general_obj_in_cmd_hdr);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode,\n\t\t MLX5_CMD_OP_CREATE_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type,\n\t\t MLX5_OBJ_TYPE_PAGE_TRACK);\n\tobj_context = MLX5_ADDR_OF(create_page_track_obj_in, in, obj_context);\n\tMLX5_SET(page_track, obj_context, vhca_id, mvdev->vhca_id);\n\tMLX5_SET(page_track, obj_context, track_type, 1);\n\tMLX5_SET(page_track, obj_context, log_page_size,\n\t\t ilog2(tracker->host_qp->tracked_page_size));\n\tMLX5_SET(page_track, obj_context, log_msg_size,\n\t\t ilog2(tracker->host_qp->max_msg_size));\n\tMLX5_SET(page_track, obj_context, reporting_qpn, tracker->fw_qp->qpn);\n\tMLX5_SET(page_track, obj_context, num_ranges, num_ranges);\n\n\trange_list_ptr = MLX5_ADDR_OF(page_track, obj_context, track_range);\n\tnode = interval_tree_iter_first(ranges, 0, ULONG_MAX);\n\tfor (i = 0; i < num_ranges; i++) {\n\t\tvoid *addr_range_i_base = range_list_ptr + record_size * i;\n\t\tunsigned long length = node->last - node->start + 1;\n\n\t\tMLX5_SET64(page_track_range, addr_range_i_base, start_address,\n\t\t\t   node->start);\n\t\tMLX5_SET64(page_track_range, addr_range_i_base, length, length);\n\t\ttotal_ranges_len += length;\n\t\tnode = interval_tree_iter_next(node, 0, ULONG_MAX);\n\t}\n\n\tWARN_ON(node);\n\tlog_addr_space_size = ilog2(roundup_pow_of_two(total_ranges_len));\n\tif (log_addr_space_size <\n\t    (MLX5_CAP_ADV_VIRTUALIZATION(mdev, pg_track_log_min_addr_space)) ||\n\t    log_addr_space_size >\n\t    (MLX5_CAP_ADV_VIRTUALIZATION(mdev, pg_track_log_max_addr_space))) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out;\n\t}\n\n\tMLX5_SET(page_track, obj_context, log_addr_space_size,\n\t\t log_addr_space_size);\n\terr = mlx5_cmd_exec(mdev, in, inlen, out, sizeof(out));\n\tif (err)\n\t\tgoto out;\n\n\ttracker->id = MLX5_GET(general_obj_out_cmd_hdr, out, obj_id);\nout:\n\tkfree(in);\n\treturn err;\n}\n\nstatic int mlx5vf_cmd_destroy_tracker(struct mlx5_core_dev *mdev,\n\t\t\t\t      u32 tracker_id)\n{\n\tu32 in[MLX5_ST_SZ_DW(general_obj_in_cmd_hdr)] = {};\n\tu32 out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)] = {};\n\n\tMLX5_SET(general_obj_in_cmd_hdr, in, opcode, MLX5_CMD_OP_DESTROY_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, in, obj_type, MLX5_OBJ_TYPE_PAGE_TRACK);\n\tMLX5_SET(general_obj_in_cmd_hdr, in, obj_id, tracker_id);\n\n\treturn mlx5_cmd_exec(mdev, in, sizeof(in), out, sizeof(out));\n}\n\nstatic int mlx5vf_cmd_modify_tracker(struct mlx5_core_dev *mdev,\n\t\t\t\t     u32 tracker_id, unsigned long iova,\n\t\t\t\t     unsigned long length, u32 tracker_state)\n{\n\tu32 in[MLX5_ST_SZ_DW(modify_page_track_obj_in)] = {};\n\tu32 out[MLX5_ST_SZ_DW(general_obj_out_cmd_hdr)] = {};\n\tvoid *obj_context;\n\tvoid *cmd_hdr;\n\n\tcmd_hdr = MLX5_ADDR_OF(modify_page_track_obj_in, in, general_obj_in_cmd_hdr);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, opcode, MLX5_CMD_OP_MODIFY_GENERAL_OBJECT);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_type, MLX5_OBJ_TYPE_PAGE_TRACK);\n\tMLX5_SET(general_obj_in_cmd_hdr, cmd_hdr, obj_id, tracker_id);\n\n\tobj_context = MLX5_ADDR_OF(modify_page_track_obj_in, in, obj_context);\n\tMLX5_SET64(page_track, obj_context, modify_field_select, 0x3);\n\tMLX5_SET64(page_track, obj_context, range_start_address, iova);\n\tMLX5_SET64(page_track, obj_context, length, length);\n\tMLX5_SET(page_track, obj_context, state, tracker_state);\n\n\treturn mlx5_cmd_exec(mdev, in, sizeof(in), out, sizeof(out));\n}\n\nstatic int alloc_cq_frag_buf(struct mlx5_core_dev *mdev,\n\t\t\t     struct mlx5_vhca_cq_buf *buf, int nent,\n\t\t\t     int cqe_size)\n{\n\tstruct mlx5_frag_buf *frag_buf = &buf->frag_buf;\n\tu8 log_wq_stride = 6 + (cqe_size == 128 ? 1 : 0);\n\tu8 log_wq_sz = ilog2(cqe_size);\n\tint err;\n\n\terr = mlx5_frag_buf_alloc_node(mdev, nent * cqe_size, frag_buf,\n\t\t\t\t       mdev->priv.numa_node);\n\tif (err)\n\t\treturn err;\n\n\tmlx5_init_fbc(frag_buf->frags, log_wq_stride, log_wq_sz, &buf->fbc);\n\tbuf->cqe_size = cqe_size;\n\tbuf->nent = nent;\n\treturn 0;\n}\n\nstatic void init_cq_frag_buf(struct mlx5_vhca_cq_buf *buf)\n{\n\tstruct mlx5_cqe64 *cqe64;\n\tvoid *cqe;\n\tint i;\n\n\tfor (i = 0; i < buf->nent; i++) {\n\t\tcqe = mlx5_frag_buf_get_wqe(&buf->fbc, i);\n\t\tcqe64 = buf->cqe_size == 64 ? cqe : cqe + 64;\n\t\tcqe64->op_own = MLX5_CQE_INVALID << 4;\n\t}\n}\n\nstatic void mlx5vf_destroy_cq(struct mlx5_core_dev *mdev,\n\t\t\t      struct mlx5_vhca_cq *cq)\n{\n\tmlx5_core_destroy_cq(mdev, &cq->mcq);\n\tmlx5_frag_buf_free(mdev, &cq->buf.frag_buf);\n\tmlx5_db_free(mdev, &cq->db);\n}\n\nstatic void mlx5vf_cq_event(struct mlx5_core_cq *mcq, enum mlx5_event type)\n{\n\tif (type != MLX5_EVENT_TYPE_CQ_ERROR)\n\t\treturn;\n\n\tset_tracker_error(container_of(mcq, struct mlx5vf_pci_core_device,\n\t\t\t\t       tracker.cq.mcq));\n}\n\nstatic int mlx5vf_event_notifier(struct notifier_block *nb, unsigned long type,\n\t\t\t\t void *data)\n{\n\tstruct mlx5_vhca_page_tracker *tracker =\n\t\tmlx5_nb_cof(nb, struct mlx5_vhca_page_tracker, nb);\n\tstruct mlx5vf_pci_core_device *mvdev = container_of(\n\t\ttracker, struct mlx5vf_pci_core_device, tracker);\n\tstruct mlx5_eqe *eqe = data;\n\tu8 event_type = (u8)type;\n\tu8 queue_type;\n\tint qp_num;\n\n\tswitch (event_type) {\n\tcase MLX5_EVENT_TYPE_WQ_CATAS_ERROR:\n\tcase MLX5_EVENT_TYPE_WQ_ACCESS_ERROR:\n\tcase MLX5_EVENT_TYPE_WQ_INVAL_REQ_ERROR:\n\t\tqueue_type = eqe->data.qp_srq.type;\n\t\tif (queue_type != MLX5_EVENT_QUEUE_TYPE_QP)\n\t\t\tbreak;\n\t\tqp_num = be32_to_cpu(eqe->data.qp_srq.qp_srq_n) & 0xffffff;\n\t\tif (qp_num != tracker->host_qp->qpn &&\n\t\t    qp_num != tracker->fw_qp->qpn)\n\t\t\tbreak;\n\t\tset_tracker_error(mvdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic void mlx5vf_cq_complete(struct mlx5_core_cq *mcq,\n\t\t\t       struct mlx5_eqe *eqe)\n{\n\tstruct mlx5vf_pci_core_device *mvdev =\n\t\tcontainer_of(mcq, struct mlx5vf_pci_core_device,\n\t\t\t     tracker.cq.mcq);\n\n\tcomplete(&mvdev->tracker_comp);\n}\n\nstatic int mlx5vf_create_cq(struct mlx5_core_dev *mdev,\n\t\t\t    struct mlx5_vhca_page_tracker *tracker,\n\t\t\t    size_t ncqe)\n{\n\tint cqe_size = cache_line_size() == 128 ? 128 : 64;\n\tu32 out[MLX5_ST_SZ_DW(create_cq_out)];\n\tstruct mlx5_vhca_cq *cq;\n\tint inlen, err, eqn;\n\tvoid *cqc, *in;\n\t__be64 *pas;\n\tint vector;\n\n\tcq = &tracker->cq;\n\tncqe = roundup_pow_of_two(ncqe);\n\terr = mlx5_db_alloc_node(mdev, &cq->db, mdev->priv.numa_node);\n\tif (err)\n\t\treturn err;\n\n\tcq->ncqe = ncqe;\n\tcq->mcq.set_ci_db = cq->db.db;\n\tcq->mcq.arm_db = cq->db.db + 1;\n\tcq->mcq.cqe_sz = cqe_size;\n\terr = alloc_cq_frag_buf(mdev, &cq->buf, ncqe, cqe_size);\n\tif (err)\n\t\tgoto err_db_free;\n\n\tinit_cq_frag_buf(&cq->buf);\n\tinlen = MLX5_ST_SZ_BYTES(create_cq_in) +\n\t\tMLX5_FLD_SZ_BYTES(create_cq_in, pas[0]) *\n\t\tcq->buf.frag_buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_buff;\n\t}\n\n\tvector = raw_smp_processor_id() % mlx5_comp_vectors_max(mdev);\n\terr = mlx5_comp_eqn_get(mdev, vector, &eqn);\n\tif (err)\n\t\tgoto err_vec;\n\n\tcqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);\n\tMLX5_SET(cqc, cqc, log_cq_size, ilog2(ncqe));\n\tMLX5_SET(cqc, cqc, c_eqn_or_apu_element, eqn);\n\tMLX5_SET(cqc, cqc, uar_page, tracker->uar->index);\n\tMLX5_SET(cqc, cqc, log_page_size, cq->buf.frag_buf.page_shift -\n\t\t MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET64(cqc, cqc, dbr_addr, cq->db.dma);\n\tpas = (__be64 *)MLX5_ADDR_OF(create_cq_in, in, pas);\n\tmlx5_fill_page_frag_array(&cq->buf.frag_buf, pas);\n\tcq->mcq.comp = mlx5vf_cq_complete;\n\tcq->mcq.event = mlx5vf_cq_event;\n\terr = mlx5_core_create_cq(mdev, &cq->mcq, in, inlen, out, sizeof(out));\n\tif (err)\n\t\tgoto err_vec;\n\n\tmlx5_cq_arm(&cq->mcq, MLX5_CQ_DB_REQ_NOT, tracker->uar->map,\n\t\t    cq->mcq.cons_index);\n\tkvfree(in);\n\treturn 0;\n\nerr_vec:\n\tkvfree(in);\nerr_buff:\n\tmlx5_frag_buf_free(mdev, &cq->buf.frag_buf);\nerr_db_free:\n\tmlx5_db_free(mdev, &cq->db);\n\treturn err;\n}\n\nstatic struct mlx5_vhca_qp *\nmlx5vf_create_rc_qp(struct mlx5_core_dev *mdev,\n\t\t    struct mlx5_vhca_page_tracker *tracker, u32 max_recv_wr)\n{\n\tu32 out[MLX5_ST_SZ_DW(create_qp_out)] = {};\n\tstruct mlx5_vhca_qp *qp;\n\tu8 log_rq_stride;\n\tu8 log_rq_sz;\n\tvoid *qpc;\n\tint inlen;\n\tvoid *in;\n\tint err;\n\n\tqp = kzalloc(sizeof(*qp), GFP_KERNEL_ACCOUNT);\n\tif (!qp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = mlx5_db_alloc_node(mdev, &qp->db, mdev->priv.numa_node);\n\tif (err)\n\t\tgoto err_free;\n\n\tif (max_recv_wr) {\n\t\tqp->rq.wqe_cnt = roundup_pow_of_two(max_recv_wr);\n\t\tlog_rq_stride = ilog2(MLX5_SEND_WQE_DS);\n\t\tlog_rq_sz = ilog2(qp->rq.wqe_cnt);\n\t\terr = mlx5_frag_buf_alloc_node(mdev,\n\t\t\twq_get_byte_sz(log_rq_sz, log_rq_stride),\n\t\t\t&qp->buf, mdev->priv.numa_node);\n\t\tif (err)\n\t\t\tgoto err_db_free;\n\t\tmlx5_init_fbc(qp->buf.frags, log_rq_stride, log_rq_sz, &qp->rq.fbc);\n\t}\n\n\tqp->rq.db = &qp->db.db[MLX5_RCV_DBR];\n\tinlen = MLX5_ST_SZ_BYTES(create_qp_in) +\n\t\tMLX5_FLD_SZ_BYTES(create_qp_in, pas[0]) *\n\t\tqp->buf.npages;\n\tin = kvzalloc(inlen, GFP_KERNEL);\n\tif (!in) {\n\t\terr = -ENOMEM;\n\t\tgoto err_in;\n\t}\n\n\tqpc = MLX5_ADDR_OF(create_qp_in, in, qpc);\n\tMLX5_SET(qpc, qpc, st, MLX5_QP_ST_RC);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QP_PM_MIGRATED);\n\tMLX5_SET(qpc, qpc, pd, tracker->pdn);\n\tMLX5_SET(qpc, qpc, uar_page, tracker->uar->index);\n\tMLX5_SET(qpc, qpc, log_page_size,\n\t\t qp->buf.page_shift - MLX5_ADAPTER_PAGE_SHIFT);\n\tMLX5_SET(qpc, qpc, ts_format, mlx5_get_qp_default_ts(mdev));\n\tif (MLX5_CAP_GEN(mdev, cqe_version) == 1)\n\t\tMLX5_SET(qpc, qpc, user_index, 0xFFFFFF);\n\tMLX5_SET(qpc, qpc, no_sq, 1);\n\tif (max_recv_wr) {\n\t\tMLX5_SET(qpc, qpc, cqn_rcv, tracker->cq.mcq.cqn);\n\t\tMLX5_SET(qpc, qpc, log_rq_stride, log_rq_stride - 4);\n\t\tMLX5_SET(qpc, qpc, log_rq_size, log_rq_sz);\n\t\tMLX5_SET(qpc, qpc, rq_type, MLX5_NON_ZERO_RQ);\n\t\tMLX5_SET64(qpc, qpc, dbr_addr, qp->db.dma);\n\t\tmlx5_fill_page_frag_array(&qp->buf,\n\t\t\t\t\t  (__be64 *)MLX5_ADDR_OF(create_qp_in,\n\t\t\t\t\t\t\t\t in, pas));\n\t} else {\n\t\tMLX5_SET(qpc, qpc, rq_type, MLX5_ZERO_LEN_RQ);\n\t}\n\n\tMLX5_SET(create_qp_in, in, opcode, MLX5_CMD_OP_CREATE_QP);\n\terr = mlx5_cmd_exec(mdev, in, inlen, out, sizeof(out));\n\tkvfree(in);\n\tif (err)\n\t\tgoto err_in;\n\n\tqp->qpn = MLX5_GET(create_qp_out, out, qpn);\n\treturn qp;\n\nerr_in:\n\tif (max_recv_wr)\n\t\tmlx5_frag_buf_free(mdev, &qp->buf);\nerr_db_free:\n\tmlx5_db_free(mdev, &qp->db);\nerr_free:\n\tkfree(qp);\n\treturn ERR_PTR(err);\n}\n\nstatic void mlx5vf_post_recv(struct mlx5_vhca_qp *qp)\n{\n\tstruct mlx5_wqe_data_seg *data;\n\tunsigned int ix;\n\n\tWARN_ON(qp->rq.pc - qp->rq.cc >= qp->rq.wqe_cnt);\n\tix = qp->rq.pc & (qp->rq.wqe_cnt - 1);\n\tdata = mlx5_frag_buf_get_wqe(&qp->rq.fbc, ix);\n\tdata->byte_count = cpu_to_be32(qp->max_msg_size);\n\tdata->lkey = cpu_to_be32(qp->recv_buf.mkey);\n\tdata->addr = cpu_to_be64(qp->recv_buf.next_rq_offset);\n\tqp->rq.pc++;\n\t \n\tdma_wmb();\n\t*qp->rq.db = cpu_to_be32(qp->rq.pc & 0xffff);\n}\n\nstatic int mlx5vf_activate_qp(struct mlx5_core_dev *mdev,\n\t\t\t      struct mlx5_vhca_qp *qp, u32 remote_qpn,\n\t\t\t      bool host_qp)\n{\n\tu32 init_in[MLX5_ST_SZ_DW(rst2init_qp_in)] = {};\n\tu32 rtr_in[MLX5_ST_SZ_DW(init2rtr_qp_in)] = {};\n\tu32 rts_in[MLX5_ST_SZ_DW(rtr2rts_qp_in)] = {};\n\tvoid *qpc;\n\tint ret;\n\n\t \n\tqpc = MLX5_ADDR_OF(rst2init_qp_in, init_in, qpc);\n\tMLX5_SET(qpc, qpc, primary_address_path.vhca_port_num, 1);\n\tMLX5_SET(qpc, qpc, pm_state, MLX5_QPC_PM_STATE_MIGRATED);\n\tMLX5_SET(qpc, qpc, rre, 1);\n\tMLX5_SET(qpc, qpc, rwe, 1);\n\tMLX5_SET(rst2init_qp_in, init_in, opcode, MLX5_CMD_OP_RST2INIT_QP);\n\tMLX5_SET(rst2init_qp_in, init_in, qpn, qp->qpn);\n\tret = mlx5_cmd_exec_in(mdev, rst2init_qp, init_in);\n\tif (ret)\n\t\treturn ret;\n\n\tif (host_qp) {\n\t\tstruct mlx5_vhca_recv_buf *recv_buf = &qp->recv_buf;\n\t\tint i;\n\n\t\tfor (i = 0; i < qp->rq.wqe_cnt; i++) {\n\t\t\tmlx5vf_post_recv(qp);\n\t\t\trecv_buf->next_rq_offset += qp->max_msg_size;\n\t\t}\n\t}\n\n\t \n\tqpc = MLX5_ADDR_OF(init2rtr_qp_in, rtr_in, qpc);\n\tMLX5_SET(init2rtr_qp_in, rtr_in, qpn, qp->qpn);\n\tMLX5_SET(qpc, qpc, mtu, IB_MTU_4096);\n\tMLX5_SET(qpc, qpc, log_msg_max, MLX5_CAP_GEN(mdev, log_max_msg));\n\tMLX5_SET(qpc, qpc, remote_qpn, remote_qpn);\n\tMLX5_SET(qpc, qpc, primary_address_path.vhca_port_num, 1);\n\tMLX5_SET(qpc, qpc, primary_address_path.fl, 1);\n\tMLX5_SET(qpc, qpc, min_rnr_nak, 1);\n\tMLX5_SET(init2rtr_qp_in, rtr_in, opcode, MLX5_CMD_OP_INIT2RTR_QP);\n\tMLX5_SET(init2rtr_qp_in, rtr_in, qpn, qp->qpn);\n\tret = mlx5_cmd_exec_in(mdev, init2rtr_qp, rtr_in);\n\tif (ret || host_qp)\n\t\treturn ret;\n\n\t \n\tqpc = MLX5_ADDR_OF(rtr2rts_qp_in, rts_in, qpc);\n\tMLX5_SET(rtr2rts_qp_in, rts_in, qpn, qp->qpn);\n\tMLX5_SET(qpc, qpc, retry_count, 7);\n\tMLX5_SET(qpc, qpc, rnr_retry, 7);  \n\tMLX5_SET(qpc, qpc, primary_address_path.ack_timeout, 0x8);  \n\tMLX5_SET(rtr2rts_qp_in, rts_in, opcode, MLX5_CMD_OP_RTR2RTS_QP);\n\tMLX5_SET(rtr2rts_qp_in, rts_in, qpn, qp->qpn);\n\n\treturn mlx5_cmd_exec_in(mdev, rtr2rts_qp, rts_in);\n}\n\nstatic void mlx5vf_destroy_qp(struct mlx5_core_dev *mdev,\n\t\t\t      struct mlx5_vhca_qp *qp)\n{\n\tu32 in[MLX5_ST_SZ_DW(destroy_qp_in)] = {};\n\n\tMLX5_SET(destroy_qp_in, in, opcode, MLX5_CMD_OP_DESTROY_QP);\n\tMLX5_SET(destroy_qp_in, in, qpn, qp->qpn);\n\tmlx5_cmd_exec_in(mdev, destroy_qp, in);\n\n\tmlx5_frag_buf_free(mdev, &qp->buf);\n\tmlx5_db_free(mdev, &qp->db);\n\tkfree(qp);\n}\n\nstatic void free_recv_pages(struct mlx5_vhca_recv_buf *recv_buf)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < recv_buf->npages; i++)\n\t\t__free_page(recv_buf->page_list[i]);\n\n\tkvfree(recv_buf->page_list);\n}\n\nstatic int alloc_recv_pages(struct mlx5_vhca_recv_buf *recv_buf,\n\t\t\t    unsigned int npages)\n{\n\tunsigned int filled = 0, done = 0;\n\tint i;\n\n\trecv_buf->page_list = kvcalloc(npages, sizeof(*recv_buf->page_list),\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!recv_buf->page_list)\n\t\treturn -ENOMEM;\n\n\tfor (;;) {\n\t\tfilled = alloc_pages_bulk_array(GFP_KERNEL_ACCOUNT,\n\t\t\t\t\t\tnpages - done,\n\t\t\t\t\t\trecv_buf->page_list + done);\n\t\tif (!filled)\n\t\t\tgoto err;\n\n\t\tdone += filled;\n\t\tif (done == npages)\n\t\t\tbreak;\n\t}\n\n\trecv_buf->npages = npages;\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < npages; i++) {\n\t\tif (recv_buf->page_list[i])\n\t\t\t__free_page(recv_buf->page_list[i]);\n\t}\n\n\tkvfree(recv_buf->page_list);\n\treturn -ENOMEM;\n}\n\nstatic int register_dma_recv_pages(struct mlx5_core_dev *mdev,\n\t\t\t\t   struct mlx5_vhca_recv_buf *recv_buf)\n{\n\tint i, j;\n\n\trecv_buf->dma_addrs = kvcalloc(recv_buf->npages,\n\t\t\t\t       sizeof(*recv_buf->dma_addrs),\n\t\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!recv_buf->dma_addrs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < recv_buf->npages; i++) {\n\t\trecv_buf->dma_addrs[i] = dma_map_page(mdev->device,\n\t\t\t\t\t\t      recv_buf->page_list[i],\n\t\t\t\t\t\t      0, PAGE_SIZE,\n\t\t\t\t\t\t      DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(mdev->device, recv_buf->dma_addrs[i]))\n\t\t\tgoto error;\n\t}\n\treturn 0;\n\nerror:\n\tfor (j = 0; j < i; j++)\n\t\tdma_unmap_single(mdev->device, recv_buf->dma_addrs[j],\n\t\t\t\t PAGE_SIZE, DMA_FROM_DEVICE);\n\n\tkvfree(recv_buf->dma_addrs);\n\treturn -ENOMEM;\n}\n\nstatic void unregister_dma_recv_pages(struct mlx5_core_dev *mdev,\n\t\t\t\t      struct mlx5_vhca_recv_buf *recv_buf)\n{\n\tint i;\n\n\tfor (i = 0; i < recv_buf->npages; i++)\n\t\tdma_unmap_single(mdev->device, recv_buf->dma_addrs[i],\n\t\t\t\t PAGE_SIZE, DMA_FROM_DEVICE);\n\n\tkvfree(recv_buf->dma_addrs);\n}\n\nstatic void mlx5vf_free_qp_recv_resources(struct mlx5_core_dev *mdev,\n\t\t\t\t\t  struct mlx5_vhca_qp *qp)\n{\n\tstruct mlx5_vhca_recv_buf *recv_buf = &qp->recv_buf;\n\n\tmlx5_core_destroy_mkey(mdev, recv_buf->mkey);\n\tunregister_dma_recv_pages(mdev, recv_buf);\n\tfree_recv_pages(&qp->recv_buf);\n}\n\nstatic int mlx5vf_alloc_qp_recv_resources(struct mlx5_core_dev *mdev,\n\t\t\t\t\t  struct mlx5_vhca_qp *qp, u32 pdn,\n\t\t\t\t\t  u64 rq_size)\n{\n\tunsigned int npages = DIV_ROUND_UP_ULL(rq_size, PAGE_SIZE);\n\tstruct mlx5_vhca_recv_buf *recv_buf = &qp->recv_buf;\n\tint err;\n\n\terr = alloc_recv_pages(recv_buf, npages);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = register_dma_recv_pages(mdev, recv_buf);\n\tif (err)\n\t\tgoto end;\n\n\terr = _create_mkey(mdev, pdn, NULL, recv_buf, &recv_buf->mkey);\n\tif (err)\n\t\tgoto err_create_mkey;\n\n\treturn 0;\n\nerr_create_mkey:\n\tunregister_dma_recv_pages(mdev, recv_buf);\nend:\n\tfree_recv_pages(recv_buf);\n\treturn err;\n}\n\nstatic void\n_mlx5vf_free_page_tracker_resources(struct mlx5vf_pci_core_device *mvdev)\n{\n\tstruct mlx5_vhca_page_tracker *tracker = &mvdev->tracker;\n\tstruct mlx5_core_dev *mdev = mvdev->mdev;\n\n\tlockdep_assert_held(&mvdev->state_mutex);\n\n\tif (!mvdev->log_active)\n\t\treturn;\n\n\tWARN_ON(mvdev->mdev_detach);\n\n\tmlx5_eq_notifier_unregister(mdev, &tracker->nb);\n\tmlx5vf_cmd_destroy_tracker(mdev, tracker->id);\n\tmlx5vf_destroy_qp(mdev, tracker->fw_qp);\n\tmlx5vf_free_qp_recv_resources(mdev, tracker->host_qp);\n\tmlx5vf_destroy_qp(mdev, tracker->host_qp);\n\tmlx5vf_destroy_cq(mdev, &tracker->cq);\n\tmlx5_core_dealloc_pd(mdev, tracker->pdn);\n\tmlx5_put_uars_page(mdev, tracker->uar);\n\tmvdev->log_active = false;\n}\n\nint mlx5vf_stop_page_tracker(struct vfio_device *vdev)\n{\n\tstruct mlx5vf_pci_core_device *mvdev = container_of(\n\t\tvdev, struct mlx5vf_pci_core_device, core_device.vdev);\n\n\tmutex_lock(&mvdev->state_mutex);\n\tif (!mvdev->log_active)\n\t\tgoto end;\n\n\t_mlx5vf_free_page_tracker_resources(mvdev);\n\tmvdev->log_active = false;\nend:\n\tmlx5vf_state_mutex_unlock(mvdev);\n\treturn 0;\n}\n\nint mlx5vf_start_page_tracker(struct vfio_device *vdev,\n\t\t\t      struct rb_root_cached *ranges, u32 nnodes,\n\t\t\t      u64 *page_size)\n{\n\tstruct mlx5vf_pci_core_device *mvdev = container_of(\n\t\tvdev, struct mlx5vf_pci_core_device, core_device.vdev);\n\tstruct mlx5_vhca_page_tracker *tracker = &mvdev->tracker;\n\tu8 log_tracked_page = ilog2(*page_size);\n\tstruct mlx5_vhca_qp *host_qp;\n\tstruct mlx5_vhca_qp *fw_qp;\n\tstruct mlx5_core_dev *mdev;\n\tu32 max_msg_size = PAGE_SIZE;\n\tu64 rq_size = SZ_2M;\n\tu32 max_recv_wr;\n\tint err;\n\n\tmutex_lock(&mvdev->state_mutex);\n\tif (mvdev->mdev_detach) {\n\t\terr = -ENOTCONN;\n\t\tgoto end;\n\t}\n\n\tif (mvdev->log_active) {\n\t\terr = -EINVAL;\n\t\tgoto end;\n\t}\n\n\tmdev = mvdev->mdev;\n\tmemset(tracker, 0, sizeof(*tracker));\n\ttracker->uar = mlx5_get_uars_page(mdev);\n\tif (IS_ERR(tracker->uar)) {\n\t\terr = PTR_ERR(tracker->uar);\n\t\tgoto end;\n\t}\n\n\terr = mlx5_core_alloc_pd(mdev, &tracker->pdn);\n\tif (err)\n\t\tgoto err_uar;\n\n\tmax_recv_wr = DIV_ROUND_UP_ULL(rq_size, max_msg_size);\n\terr = mlx5vf_create_cq(mdev, tracker, max_recv_wr);\n\tif (err)\n\t\tgoto err_dealloc_pd;\n\n\thost_qp = mlx5vf_create_rc_qp(mdev, tracker, max_recv_wr);\n\tif (IS_ERR(host_qp)) {\n\t\terr = PTR_ERR(host_qp);\n\t\tgoto err_cq;\n\t}\n\n\thost_qp->max_msg_size = max_msg_size;\n\tif (log_tracked_page < MLX5_CAP_ADV_VIRTUALIZATION(mdev,\n\t\t\t\tpg_track_log_min_page_size)) {\n\t\tlog_tracked_page = MLX5_CAP_ADV_VIRTUALIZATION(mdev,\n\t\t\t\tpg_track_log_min_page_size);\n\t} else if (log_tracked_page > MLX5_CAP_ADV_VIRTUALIZATION(mdev,\n\t\t\t\tpg_track_log_max_page_size)) {\n\t\tlog_tracked_page = MLX5_CAP_ADV_VIRTUALIZATION(mdev,\n\t\t\t\tpg_track_log_max_page_size);\n\t}\n\n\thost_qp->tracked_page_size = (1ULL << log_tracked_page);\n\terr = mlx5vf_alloc_qp_recv_resources(mdev, host_qp, tracker->pdn,\n\t\t\t\t\t     rq_size);\n\tif (err)\n\t\tgoto err_host_qp;\n\n\tfw_qp = mlx5vf_create_rc_qp(mdev, tracker, 0);\n\tif (IS_ERR(fw_qp)) {\n\t\terr = PTR_ERR(fw_qp);\n\t\tgoto err_recv_resources;\n\t}\n\n\terr = mlx5vf_activate_qp(mdev, host_qp, fw_qp->qpn, true);\n\tif (err)\n\t\tgoto err_activate;\n\n\terr = mlx5vf_activate_qp(mdev, fw_qp, host_qp->qpn, false);\n\tif (err)\n\t\tgoto err_activate;\n\n\ttracker->host_qp = host_qp;\n\ttracker->fw_qp = fw_qp;\n\terr = mlx5vf_create_tracker(mdev, mvdev, ranges, nnodes);\n\tif (err)\n\t\tgoto err_activate;\n\n\tMLX5_NB_INIT(&tracker->nb, mlx5vf_event_notifier, NOTIFY_ANY);\n\tmlx5_eq_notifier_register(mdev, &tracker->nb);\n\t*page_size = host_qp->tracked_page_size;\n\tmvdev->log_active = true;\n\tmlx5vf_state_mutex_unlock(mvdev);\n\treturn 0;\n\nerr_activate:\n\tmlx5vf_destroy_qp(mdev, fw_qp);\nerr_recv_resources:\n\tmlx5vf_free_qp_recv_resources(mdev, host_qp);\nerr_host_qp:\n\tmlx5vf_destroy_qp(mdev, host_qp);\nerr_cq:\n\tmlx5vf_destroy_cq(mdev, &tracker->cq);\nerr_dealloc_pd:\n\tmlx5_core_dealloc_pd(mdev, tracker->pdn);\nerr_uar:\n\tmlx5_put_uars_page(mdev, tracker->uar);\nend:\n\tmlx5vf_state_mutex_unlock(mvdev);\n\treturn err;\n}\n\nstatic void\nset_report_output(u32 size, int index, struct mlx5_vhca_qp *qp,\n\t\t  struct iova_bitmap *dirty)\n{\n\tu32 entry_size = MLX5_ST_SZ_BYTES(page_track_report_entry);\n\tu32 nent = size / entry_size;\n\tstruct page *page;\n\tu64 addr;\n\tu64 *buf;\n\tint i;\n\n\tif (WARN_ON(index >= qp->recv_buf.npages ||\n\t\t    (nent > qp->max_msg_size / entry_size)))\n\t\treturn;\n\n\tpage = qp->recv_buf.page_list[index];\n\tbuf = kmap_local_page(page);\n\tfor (i = 0; i < nent; i++) {\n\t\taddr = MLX5_GET(page_track_report_entry, buf + i,\n\t\t\t\tdirty_address_low);\n\t\taddr |= (u64)MLX5_GET(page_track_report_entry, buf + i,\n\t\t\t\t      dirty_address_high) << 32;\n\t\tiova_bitmap_set(dirty, addr, qp->tracked_page_size);\n\t}\n\tkunmap_local(buf);\n}\n\nstatic void\nmlx5vf_rq_cqe(struct mlx5_vhca_qp *qp, struct mlx5_cqe64 *cqe,\n\t      struct iova_bitmap *dirty, int *tracker_status)\n{\n\tu32 size;\n\tint ix;\n\n\tqp->rq.cc++;\n\t*tracker_status = be32_to_cpu(cqe->immediate) >> 28;\n\tsize = be32_to_cpu(cqe->byte_cnt);\n\tix = be16_to_cpu(cqe->wqe_counter) & (qp->rq.wqe_cnt - 1);\n\n\t \n\tWARN_ON(!size && *tracker_status == MLX5_PAGE_TRACK_STATE_REPORTING);\n\tif (size)\n\t\tset_report_output(size, ix, qp, dirty);\n\n\tqp->recv_buf.next_rq_offset = ix * qp->max_msg_size;\n\tmlx5vf_post_recv(qp);\n}\n\nstatic void *get_cqe(struct mlx5_vhca_cq *cq, int n)\n{\n\treturn mlx5_frag_buf_get_wqe(&cq->buf.fbc, n);\n}\n\nstatic struct mlx5_cqe64 *get_sw_cqe(struct mlx5_vhca_cq *cq, int n)\n{\n\tvoid *cqe = get_cqe(cq, n & (cq->ncqe - 1));\n\tstruct mlx5_cqe64 *cqe64;\n\n\tcqe64 = (cq->mcq.cqe_sz == 64) ? cqe : cqe + 64;\n\n\tif (likely(get_cqe_opcode(cqe64) != MLX5_CQE_INVALID) &&\n\t    !((cqe64->op_own & MLX5_CQE_OWNER_MASK) ^ !!(n & (cq->ncqe)))) {\n\t\treturn cqe64;\n\t} else {\n\t\treturn NULL;\n\t}\n}\n\nstatic int\nmlx5vf_cq_poll_one(struct mlx5_vhca_cq *cq, struct mlx5_vhca_qp *qp,\n\t\t   struct iova_bitmap *dirty, int *tracker_status)\n{\n\tstruct mlx5_cqe64 *cqe;\n\tu8 opcode;\n\n\tcqe = get_sw_cqe(cq, cq->mcq.cons_index);\n\tif (!cqe)\n\t\treturn CQ_EMPTY;\n\n\t++cq->mcq.cons_index;\n\t \n\trmb();\n\topcode = get_cqe_opcode(cqe);\n\tswitch (opcode) {\n\tcase MLX5_CQE_RESP_SEND_IMM:\n\t\tmlx5vf_rq_cqe(qp, cqe, dirty, tracker_status);\n\t\treturn CQ_OK;\n\tdefault:\n\t\treturn CQ_POLL_ERR;\n\t}\n}\n\nint mlx5vf_tracker_read_and_clear(struct vfio_device *vdev, unsigned long iova,\n\t\t\t\t  unsigned long length,\n\t\t\t\t  struct iova_bitmap *dirty)\n{\n\tstruct mlx5vf_pci_core_device *mvdev = container_of(\n\t\tvdev, struct mlx5vf_pci_core_device, core_device.vdev);\n\tstruct mlx5_vhca_page_tracker *tracker = &mvdev->tracker;\n\tstruct mlx5_vhca_cq *cq = &tracker->cq;\n\tstruct mlx5_core_dev *mdev;\n\tint poll_err, err;\n\n\tmutex_lock(&mvdev->state_mutex);\n\tif (!mvdev->log_active) {\n\t\terr = -EINVAL;\n\t\tgoto end;\n\t}\n\n\tif (mvdev->mdev_detach) {\n\t\terr = -ENOTCONN;\n\t\tgoto end;\n\t}\n\n\tmdev = mvdev->mdev;\n\terr = mlx5vf_cmd_modify_tracker(mdev, tracker->id, iova, length,\n\t\t\t\t\tMLX5_PAGE_TRACK_STATE_REPORTING);\n\tif (err)\n\t\tgoto end;\n\n\ttracker->status = MLX5_PAGE_TRACK_STATE_REPORTING;\n\twhile (tracker->status == MLX5_PAGE_TRACK_STATE_REPORTING &&\n\t       !tracker->is_err) {\n\t\tpoll_err = mlx5vf_cq_poll_one(cq, tracker->host_qp, dirty,\n\t\t\t\t\t      &tracker->status);\n\t\tif (poll_err == CQ_EMPTY) {\n\t\t\tmlx5_cq_arm(&cq->mcq, MLX5_CQ_DB_REQ_NOT, tracker->uar->map,\n\t\t\t\t    cq->mcq.cons_index);\n\t\t\tpoll_err = mlx5vf_cq_poll_one(cq, tracker->host_qp,\n\t\t\t\t\t\t      dirty, &tracker->status);\n\t\t\tif (poll_err == CQ_EMPTY) {\n\t\t\t\twait_for_completion(&mvdev->tracker_comp);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (poll_err == CQ_POLL_ERR) {\n\t\t\terr = -EIO;\n\t\t\tgoto end;\n\t\t}\n\t\tmlx5_cq_set_ci(&cq->mcq);\n\t}\n\n\tif (tracker->status == MLX5_PAGE_TRACK_STATE_ERROR)\n\t\ttracker->is_err = true;\n\n\tif (tracker->is_err)\n\t\terr = -EIO;\nend:\n\tmlx5vf_state_mutex_unlock(mvdev);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}