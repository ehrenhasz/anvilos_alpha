{
  "module_name": "vfio_main.c",
  "hash_id": "9b77d08a5c0e229d9b70f68c111f8dc282be660b5630f8cf53128045e351494c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vfio/vfio_main.c",
  "human_readable_source": "\n \n\n#include <linux/cdev.h>\n#include <linux/compat.h>\n#include <linux/device.h>\n#include <linux/fs.h>\n#include <linux/idr.h>\n#include <linux/iommu.h>\n#ifdef CONFIG_HAVE_KVM\n#include <linux/kvm_host.h>\n#endif\n#include <linux/list.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/pci.h>\n#include <linux/rwsem.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/stat.h>\n#include <linux/string.h>\n#include <linux/uaccess.h>\n#include <linux/vfio.h>\n#include <linux/wait.h>\n#include <linux/sched/signal.h>\n#include <linux/pm_runtime.h>\n#include <linux/interval_tree.h>\n#include <linux/iova_bitmap.h>\n#include <linux/iommufd.h>\n#include \"vfio.h\"\n\n#define DRIVER_VERSION\t\"0.3\"\n#define DRIVER_AUTHOR\t\"Alex Williamson <alex.williamson@redhat.com>\"\n#define DRIVER_DESC\t\"VFIO - User Level meta-driver\"\n\nstatic struct vfio {\n\tstruct class\t\t\t*device_class;\n\tstruct ida\t\t\tdevice_ida;\n} vfio;\n\n#ifdef CONFIG_VFIO_NOIOMMU\nbool vfio_noiommu __read_mostly;\nmodule_param_named(enable_unsafe_noiommu_mode,\n\t\t   vfio_noiommu, bool, S_IRUGO | S_IWUSR);\nMODULE_PARM_DESC(enable_unsafe_noiommu_mode, \"Enable UNSAFE, no-IOMMU mode.  This mode provides no device isolation, no DMA translation, no host kernel protection, cannot be used for device assignment to virtual machines, requires RAWIO permissions, and will taint the kernel.  If you do not know what this is for, step away. (default: false)\");\n#endif\n\nstatic DEFINE_XARRAY(vfio_device_set_xa);\n\nint vfio_assign_device_set(struct vfio_device *device, void *set_id)\n{\n\tunsigned long idx = (unsigned long)set_id;\n\tstruct vfio_device_set *new_dev_set;\n\tstruct vfio_device_set *dev_set;\n\n\tif (WARN_ON(!set_id))\n\t\treturn -EINVAL;\n\n\t \n\txa_lock(&vfio_device_set_xa);\n\tdev_set = xa_load(&vfio_device_set_xa, idx);\n\tif (dev_set)\n\t\tgoto found_get_ref;\n\txa_unlock(&vfio_device_set_xa);\n\n\tnew_dev_set = kzalloc(sizeof(*new_dev_set), GFP_KERNEL);\n\tif (!new_dev_set)\n\t\treturn -ENOMEM;\n\tmutex_init(&new_dev_set->lock);\n\tINIT_LIST_HEAD(&new_dev_set->device_list);\n\tnew_dev_set->set_id = set_id;\n\n\txa_lock(&vfio_device_set_xa);\n\tdev_set = __xa_cmpxchg(&vfio_device_set_xa, idx, NULL, new_dev_set,\n\t\t\t       GFP_KERNEL);\n\tif (!dev_set) {\n\t\tdev_set = new_dev_set;\n\t\tgoto found_get_ref;\n\t}\n\n\tkfree(new_dev_set);\n\tif (xa_is_err(dev_set)) {\n\t\txa_unlock(&vfio_device_set_xa);\n\t\treturn xa_err(dev_set);\n\t}\n\nfound_get_ref:\n\tdev_set->device_count++;\n\txa_unlock(&vfio_device_set_xa);\n\tmutex_lock(&dev_set->lock);\n\tdevice->dev_set = dev_set;\n\tlist_add_tail(&device->dev_set_list, &dev_set->device_list);\n\tmutex_unlock(&dev_set->lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vfio_assign_device_set);\n\nstatic void vfio_release_device_set(struct vfio_device *device)\n{\n\tstruct vfio_device_set *dev_set = device->dev_set;\n\n\tif (!dev_set)\n\t\treturn;\n\n\tmutex_lock(&dev_set->lock);\n\tlist_del(&device->dev_set_list);\n\tmutex_unlock(&dev_set->lock);\n\n\txa_lock(&vfio_device_set_xa);\n\tif (!--dev_set->device_count) {\n\t\t__xa_erase(&vfio_device_set_xa,\n\t\t\t   (unsigned long)dev_set->set_id);\n\t\tmutex_destroy(&dev_set->lock);\n\t\tkfree(dev_set);\n\t}\n\txa_unlock(&vfio_device_set_xa);\n}\n\nunsigned int vfio_device_set_open_count(struct vfio_device_set *dev_set)\n{\n\tstruct vfio_device *cur;\n\tunsigned int open_count = 0;\n\n\tlockdep_assert_held(&dev_set->lock);\n\n\tlist_for_each_entry(cur, &dev_set->device_list, dev_set_list)\n\t\topen_count += cur->open_count;\n\treturn open_count;\n}\nEXPORT_SYMBOL_GPL(vfio_device_set_open_count);\n\nstruct vfio_device *\nvfio_find_device_in_devset(struct vfio_device_set *dev_set,\n\t\t\t   struct device *dev)\n{\n\tstruct vfio_device *cur;\n\n\tlockdep_assert_held(&dev_set->lock);\n\n\tlist_for_each_entry(cur, &dev_set->device_list, dev_set_list)\n\t\tif (cur->dev == dev)\n\t\t\treturn cur;\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(vfio_find_device_in_devset);\n\n \n \nvoid vfio_device_put_registration(struct vfio_device *device)\n{\n\tif (refcount_dec_and_test(&device->refcount))\n\t\tcomplete(&device->comp);\n}\n\nbool vfio_device_try_get_registration(struct vfio_device *device)\n{\n\treturn refcount_inc_not_zero(&device->refcount);\n}\n\n \n \nstatic void vfio_device_release(struct device *dev)\n{\n\tstruct vfio_device *device =\n\t\t\tcontainer_of(dev, struct vfio_device, device);\n\n\tvfio_release_device_set(device);\n\tida_free(&vfio.device_ida, device->index);\n\n\tif (device->ops->release)\n\t\tdevice->ops->release(device);\n\n\tkvfree(device);\n}\n\nstatic int vfio_init_device(struct vfio_device *device, struct device *dev,\n\t\t\t    const struct vfio_device_ops *ops);\n\n \nstruct vfio_device *_vfio_alloc_device(size_t size, struct device *dev,\n\t\t\t\t       const struct vfio_device_ops *ops)\n{\n\tstruct vfio_device *device;\n\tint ret;\n\n\tif (WARN_ON(size < sizeof(struct vfio_device)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdevice = kvzalloc(size, GFP_KERNEL);\n\tif (!device)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = vfio_init_device(device, dev, ops);\n\tif (ret)\n\t\tgoto out_free;\n\treturn device;\n\nout_free:\n\tkvfree(device);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL_GPL(_vfio_alloc_device);\n\n \nstatic int vfio_init_device(struct vfio_device *device, struct device *dev,\n\t\t\t    const struct vfio_device_ops *ops)\n{\n\tint ret;\n\n\tret = ida_alloc_max(&vfio.device_ida, MINORMASK, GFP_KERNEL);\n\tif (ret < 0) {\n\t\tdev_dbg(dev, \"Error to alloc index\\n\");\n\t\treturn ret;\n\t}\n\n\tdevice->index = ret;\n\tinit_completion(&device->comp);\n\tdevice->dev = dev;\n\tdevice->ops = ops;\n\n\tif (ops->init) {\n\t\tret = ops->init(device);\n\t\tif (ret)\n\t\t\tgoto out_uninit;\n\t}\n\n\tdevice_initialize(&device->device);\n\tdevice->device.release = vfio_device_release;\n\tdevice->device.class = vfio.device_class;\n\tdevice->device.parent = device->dev;\n\treturn 0;\n\nout_uninit:\n\tvfio_release_device_set(device);\n\tida_free(&vfio.device_ida, device->index);\n\treturn ret;\n}\n\nstatic int __vfio_register_dev(struct vfio_device *device,\n\t\t\t       enum vfio_group_type type)\n{\n\tint ret;\n\n\tif (WARN_ON(IS_ENABLED(CONFIG_IOMMUFD) &&\n\t\t    (!device->ops->bind_iommufd ||\n\t\t     !device->ops->unbind_iommufd ||\n\t\t     !device->ops->attach_ioas ||\n\t\t     !device->ops->detach_ioas)))\n\t\treturn -EINVAL;\n\n\t \n\tif (!device->dev_set)\n\t\tvfio_assign_device_set(device, device);\n\n\tret = dev_set_name(&device->device, \"vfio%d\", device->index);\n\tif (ret)\n\t\treturn ret;\n\n\tret = vfio_device_set_group(device, type);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (type == VFIO_IOMMU && !vfio_device_is_noiommu(device) &&\n\t    !device_iommu_capable(device->dev, IOMMU_CAP_CACHE_COHERENCY)) {\n\t\tret = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tret = vfio_device_add(device);\n\tif (ret)\n\t\tgoto err_out;\n\n\t \n\trefcount_set(&device->refcount, 1);\n\n\tvfio_device_group_register(device);\n\n\treturn 0;\nerr_out:\n\tvfio_device_remove_group(device);\n\treturn ret;\n}\n\nint vfio_register_group_dev(struct vfio_device *device)\n{\n\treturn __vfio_register_dev(device, VFIO_IOMMU);\n}\nEXPORT_SYMBOL_GPL(vfio_register_group_dev);\n\n \nint vfio_register_emulated_iommu_dev(struct vfio_device *device)\n{\n\treturn __vfio_register_dev(device, VFIO_EMULATED_IOMMU);\n}\nEXPORT_SYMBOL_GPL(vfio_register_emulated_iommu_dev);\n\n \nvoid vfio_unregister_group_dev(struct vfio_device *device)\n{\n\tunsigned int i = 0;\n\tbool interrupted = false;\n\tlong rc;\n\n\t \n\tvfio_device_group_unregister(device);\n\n\t \n\tvfio_device_del(device);\n\n\tvfio_device_put_registration(device);\n\trc = try_wait_for_completion(&device->comp);\n\twhile (rc <= 0) {\n\t\tif (device->ops->request)\n\t\t\tdevice->ops->request(device, i++);\n\n\t\tif (interrupted) {\n\t\t\trc = wait_for_completion_timeout(&device->comp,\n\t\t\t\t\t\t\t HZ * 10);\n\t\t} else {\n\t\t\trc = wait_for_completion_interruptible_timeout(\n\t\t\t\t&device->comp, HZ * 10);\n\t\t\tif (rc < 0) {\n\t\t\t\tinterrupted = true;\n\t\t\t\tdev_warn(device->dev,\n\t\t\t\t\t \"Device is currently in use, task\"\n\t\t\t\t\t \" \\\"%s\\\" (%d) \"\n\t\t\t\t\t \"blocked until device is released\",\n\t\t\t\t\t current->comm, task_pid_nr(current));\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tvfio_device_remove_group(device);\n}\nEXPORT_SYMBOL_GPL(vfio_unregister_group_dev);\n\n#ifdef CONFIG_HAVE_KVM\nvoid vfio_device_get_kvm_safe(struct vfio_device *device, struct kvm *kvm)\n{\n\tvoid (*pfn)(struct kvm *kvm);\n\tbool (*fn)(struct kvm *kvm);\n\tbool ret;\n\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\tif (!kvm)\n\t\treturn;\n\n\tpfn = symbol_get(kvm_put_kvm);\n\tif (WARN_ON(!pfn))\n\t\treturn;\n\n\tfn = symbol_get(kvm_get_kvm_safe);\n\tif (WARN_ON(!fn)) {\n\t\tsymbol_put(kvm_put_kvm);\n\t\treturn;\n\t}\n\n\tret = fn(kvm);\n\tsymbol_put(kvm_get_kvm_safe);\n\tif (!ret) {\n\t\tsymbol_put(kvm_put_kvm);\n\t\treturn;\n\t}\n\n\tdevice->put_kvm = pfn;\n\tdevice->kvm = kvm;\n}\n\nvoid vfio_device_put_kvm(struct vfio_device *device)\n{\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\tif (!device->kvm)\n\t\treturn;\n\n\tif (WARN_ON(!device->put_kvm))\n\t\tgoto clear;\n\n\tdevice->put_kvm(device->kvm);\n\tdevice->put_kvm = NULL;\n\tsymbol_put(kvm_put_kvm);\n\nclear:\n\tdevice->kvm = NULL;\n}\n#endif\n\n \nstatic bool vfio_assert_device_open(struct vfio_device *device)\n{\n\treturn !WARN_ON_ONCE(!READ_ONCE(device->open_count));\n}\n\nstruct vfio_device_file *\nvfio_allocate_device_file(struct vfio_device *device)\n{\n\tstruct vfio_device_file *df;\n\n\tdf = kzalloc(sizeof(*df), GFP_KERNEL_ACCOUNT);\n\tif (!df)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdf->device = device;\n\tspin_lock_init(&df->kvm_ref_lock);\n\n\treturn df;\n}\n\nstatic int vfio_df_device_first_open(struct vfio_device_file *df)\n{\n\tstruct vfio_device *device = df->device;\n\tstruct iommufd_ctx *iommufd = df->iommufd;\n\tint ret;\n\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\tif (!try_module_get(device->dev->driver->owner))\n\t\treturn -ENODEV;\n\n\tif (iommufd)\n\t\tret = vfio_df_iommufd_bind(df);\n\telse\n\t\tret = vfio_device_group_use_iommu(device);\n\tif (ret)\n\t\tgoto err_module_put;\n\n\tif (device->ops->open_device) {\n\t\tret = device->ops->open_device(device);\n\t\tif (ret)\n\t\t\tgoto err_unuse_iommu;\n\t}\n\treturn 0;\n\nerr_unuse_iommu:\n\tif (iommufd)\n\t\tvfio_df_iommufd_unbind(df);\n\telse\n\t\tvfio_device_group_unuse_iommu(device);\nerr_module_put:\n\tmodule_put(device->dev->driver->owner);\n\treturn ret;\n}\n\nstatic void vfio_df_device_last_close(struct vfio_device_file *df)\n{\n\tstruct vfio_device *device = df->device;\n\tstruct iommufd_ctx *iommufd = df->iommufd;\n\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\tif (device->ops->close_device)\n\t\tdevice->ops->close_device(device);\n\tif (iommufd)\n\t\tvfio_df_iommufd_unbind(df);\n\telse\n\t\tvfio_device_group_unuse_iommu(device);\n\tmodule_put(device->dev->driver->owner);\n}\n\nint vfio_df_open(struct vfio_device_file *df)\n{\n\tstruct vfio_device *device = df->device;\n\tint ret = 0;\n\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\t \n\tif (device->open_count != 0 && !df->group)\n\t\treturn -EINVAL;\n\n\tdevice->open_count++;\n\tif (device->open_count == 1) {\n\t\tret = vfio_df_device_first_open(df);\n\t\tif (ret)\n\t\t\tdevice->open_count--;\n\t}\n\n\treturn ret;\n}\n\nvoid vfio_df_close(struct vfio_device_file *df)\n{\n\tstruct vfio_device *device = df->device;\n\n\tlockdep_assert_held(&device->dev_set->lock);\n\n\tvfio_assert_device_open(device);\n\tif (device->open_count == 1)\n\t\tvfio_df_device_last_close(df);\n\tdevice->open_count--;\n}\n\n \nstatic inline int vfio_device_pm_runtime_get(struct vfio_device *device)\n{\n\tstruct device *dev = device->dev;\n\n\tif (dev->driver && dev->driver->pm) {\n\t\tint ret;\n\n\t\tret = pm_runtime_resume_and_get(dev);\n\t\tif (ret) {\n\t\t\tdev_info_ratelimited(dev,\n\t\t\t\t\"vfio: runtime resume failed %d\\n\", ret);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline void vfio_device_pm_runtime_put(struct vfio_device *device)\n{\n\tstruct device *dev = device->dev;\n\n\tif (dev->driver && dev->driver->pm)\n\t\tpm_runtime_put(dev);\n}\n\n \nstatic int vfio_device_fops_release(struct inode *inode, struct file *filep)\n{\n\tstruct vfio_device_file *df = filep->private_data;\n\tstruct vfio_device *device = df->device;\n\n\tif (df->group)\n\t\tvfio_df_group_close(df);\n\telse\n\t\tvfio_df_unbind_iommufd(df);\n\n\tvfio_device_put_registration(device);\n\n\tkfree(df);\n\n\treturn 0;\n}\n\n \nint vfio_mig_get_next_state(struct vfio_device *device,\n\t\t\t    enum vfio_device_mig_state cur_fsm,\n\t\t\t    enum vfio_device_mig_state new_fsm,\n\t\t\t    enum vfio_device_mig_state *next_fsm)\n{\n\tenum { VFIO_DEVICE_NUM_STATES = VFIO_DEVICE_STATE_PRE_COPY_P2P + 1 };\n\t \n\tstatic const u8 vfio_from_fsm_table[VFIO_DEVICE_NUM_STATES][VFIO_DEVICE_NUM_STATES] = {\n\t\t[VFIO_DEVICE_STATE_STOP] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_STOP_COPY,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_RESUMING,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_RUNNING] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_PRE_COPY,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_PRE_COPY] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_PRE_COPY,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_PRE_COPY_P2P,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_PRE_COPY_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_PRE_COPY,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_PRE_COPY_P2P,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_STOP_COPY,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_STOP_COPY] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_STOP_COPY,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_RESUMING] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_RESUMING,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_RUNNING,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_PRE_COPY_P2P,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_STOP,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_RUNNING_P2P,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t\t[VFIO_DEVICE_STATE_ERROR] = {\n\t\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] = VFIO_DEVICE_STATE_ERROR,\n\t\t\t[VFIO_DEVICE_STATE_ERROR] = VFIO_DEVICE_STATE_ERROR,\n\t\t},\n\t};\n\n\tstatic const unsigned int state_flags_table[VFIO_DEVICE_NUM_STATES] = {\n\t\t[VFIO_DEVICE_STATE_STOP] = VFIO_MIGRATION_STOP_COPY,\n\t\t[VFIO_DEVICE_STATE_RUNNING] = VFIO_MIGRATION_STOP_COPY,\n\t\t[VFIO_DEVICE_STATE_PRE_COPY] =\n\t\t\tVFIO_MIGRATION_STOP_COPY | VFIO_MIGRATION_PRE_COPY,\n\t\t[VFIO_DEVICE_STATE_PRE_COPY_P2P] = VFIO_MIGRATION_STOP_COPY |\n\t\t\t\t\t\t   VFIO_MIGRATION_P2P |\n\t\t\t\t\t\t   VFIO_MIGRATION_PRE_COPY,\n\t\t[VFIO_DEVICE_STATE_STOP_COPY] = VFIO_MIGRATION_STOP_COPY,\n\t\t[VFIO_DEVICE_STATE_RESUMING] = VFIO_MIGRATION_STOP_COPY,\n\t\t[VFIO_DEVICE_STATE_RUNNING_P2P] =\n\t\t\tVFIO_MIGRATION_STOP_COPY | VFIO_MIGRATION_P2P,\n\t\t[VFIO_DEVICE_STATE_ERROR] = ~0U,\n\t};\n\n\tif (WARN_ON(cur_fsm >= ARRAY_SIZE(vfio_from_fsm_table) ||\n\t\t    (state_flags_table[cur_fsm] & device->migration_flags) !=\n\t\t\tstate_flags_table[cur_fsm]))\n\t\treturn -EINVAL;\n\n\tif (new_fsm >= ARRAY_SIZE(vfio_from_fsm_table) ||\n\t   (state_flags_table[new_fsm] & device->migration_flags) !=\n\t\t\tstate_flags_table[new_fsm])\n\t\treturn -EINVAL;\n\n\t \n\t*next_fsm = vfio_from_fsm_table[cur_fsm][new_fsm];\n\twhile ((state_flags_table[*next_fsm] & device->migration_flags) !=\n\t\t\tstate_flags_table[*next_fsm])\n\t\t*next_fsm = vfio_from_fsm_table[*next_fsm][new_fsm];\n\n\treturn (*next_fsm != VFIO_DEVICE_STATE_ERROR) ? 0 : -EINVAL;\n}\nEXPORT_SYMBOL_GPL(vfio_mig_get_next_state);\n\n \nstatic int vfio_ioct_mig_return_fd(struct file *filp, void __user *arg,\n\t\t\t\t   struct vfio_device_feature_mig_state *mig)\n{\n\tint ret;\n\tint fd;\n\n\tfd = get_unused_fd_flags(O_CLOEXEC);\n\tif (fd < 0) {\n\t\tret = fd;\n\t\tgoto out_fput;\n\t}\n\n\tmig->data_fd = fd;\n\tif (copy_to_user(arg, mig, sizeof(*mig))) {\n\t\tret = -EFAULT;\n\t\tgoto out_put_unused;\n\t}\n\tfd_install(fd, filp);\n\treturn 0;\n\nout_put_unused:\n\tput_unused_fd(fd);\nout_fput:\n\tfput(filp);\n\treturn ret;\n}\n\nstatic int\nvfio_ioctl_device_feature_mig_device_state(struct vfio_device *device,\n\t\t\t\t\t   u32 flags, void __user *arg,\n\t\t\t\t\t   size_t argsz)\n{\n\tsize_t minsz =\n\t\toffsetofend(struct vfio_device_feature_mig_state, data_fd);\n\tstruct vfio_device_feature_mig_state mig;\n\tstruct file *filp = NULL;\n\tint ret;\n\n\tif (!device->mig_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz,\n\t\t\t\t VFIO_DEVICE_FEATURE_SET |\n\t\t\t\t VFIO_DEVICE_FEATURE_GET,\n\t\t\t\t sizeof(mig));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tif (copy_from_user(&mig, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (flags & VFIO_DEVICE_FEATURE_GET) {\n\t\tenum vfio_device_mig_state curr_state;\n\n\t\tret = device->mig_ops->migration_get_state(device,\n\t\t\t\t\t\t\t   &curr_state);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tmig.device_state = curr_state;\n\t\tgoto out_copy;\n\t}\n\n\t \n\tfilp = device->mig_ops->migration_set_state(device, mig.device_state);\n\tif (IS_ERR(filp) || !filp)\n\t\tgoto out_copy;\n\n\treturn vfio_ioct_mig_return_fd(filp, arg, &mig);\nout_copy:\n\tmig.data_fd = -1;\n\tif (copy_to_user(arg, &mig, sizeof(mig)))\n\t\treturn -EFAULT;\n\tif (IS_ERR(filp))\n\t\treturn PTR_ERR(filp);\n\treturn 0;\n}\n\nstatic int\nvfio_ioctl_device_feature_migration_data_size(struct vfio_device *device,\n\t\t\t\t\t      u32 flags, void __user *arg,\n\t\t\t\t\t      size_t argsz)\n{\n\tstruct vfio_device_feature_mig_data_size data_size = {};\n\tunsigned long stop_copy_length;\n\tint ret;\n\n\tif (!device->mig_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_GET,\n\t\t\t\t sizeof(data_size));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tret = device->mig_ops->migration_get_data_size(device, &stop_copy_length);\n\tif (ret)\n\t\treturn ret;\n\n\tdata_size.stop_copy_length = stop_copy_length;\n\tif (copy_to_user(arg, &data_size, sizeof(data_size)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int vfio_ioctl_device_feature_migration(struct vfio_device *device,\n\t\t\t\t\t       u32 flags, void __user *arg,\n\t\t\t\t\t       size_t argsz)\n{\n\tstruct vfio_device_feature_migration mig = {\n\t\t.flags = device->migration_flags,\n\t};\n\tint ret;\n\n\tif (!device->mig_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz, VFIO_DEVICE_FEATURE_GET,\n\t\t\t\t sizeof(mig));\n\tif (ret != 1)\n\t\treturn ret;\n\tif (copy_to_user(arg, &mig, sizeof(mig)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nvoid vfio_combine_iova_ranges(struct rb_root_cached *root, u32 cur_nodes,\n\t\t\t      u32 req_nodes)\n{\n\tstruct interval_tree_node *prev, *curr, *comb_start, *comb_end;\n\tunsigned long min_gap, curr_gap;\n\n\t \n\tif (req_nodes == 1) {\n\t\tunsigned long last;\n\n\t\tcomb_start = interval_tree_iter_first(root, 0, ULONG_MAX);\n\t\tcurr = comb_start;\n\t\twhile (curr) {\n\t\t\tlast = curr->last;\n\t\t\tprev = curr;\n\t\t\tcurr = interval_tree_iter_next(curr, 0, ULONG_MAX);\n\t\t\tif (prev != comb_start)\n\t\t\t\tinterval_tree_remove(prev, root);\n\t\t}\n\t\tcomb_start->last = last;\n\t\treturn;\n\t}\n\n\t \n\twhile (cur_nodes > req_nodes) {\n\t\tprev = NULL;\n\t\tmin_gap = ULONG_MAX;\n\t\tcurr = interval_tree_iter_first(root, 0, ULONG_MAX);\n\t\twhile (curr) {\n\t\t\tif (prev) {\n\t\t\t\tcurr_gap = curr->start - prev->last;\n\t\t\t\tif (curr_gap < min_gap) {\n\t\t\t\t\tmin_gap = curr_gap;\n\t\t\t\t\tcomb_start = prev;\n\t\t\t\t\tcomb_end = curr;\n\t\t\t\t}\n\t\t\t}\n\t\t\tprev = curr;\n\t\t\tcurr = interval_tree_iter_next(curr, 0, ULONG_MAX);\n\t\t}\n\t\tcomb_start->last = comb_end->last;\n\t\tinterval_tree_remove(comb_end, root);\n\t\tcur_nodes--;\n\t}\n}\nEXPORT_SYMBOL_GPL(vfio_combine_iova_ranges);\n\n \n#define LOG_MAX_RANGES \\\n\t(PAGE_SIZE / sizeof(struct vfio_device_feature_dma_logging_range))\n\nstatic int\nvfio_ioctl_device_feature_logging_start(struct vfio_device *device,\n\t\t\t\t\tu32 flags, void __user *arg,\n\t\t\t\t\tsize_t argsz)\n{\n\tsize_t minsz =\n\t\toffsetofend(struct vfio_device_feature_dma_logging_control,\n\t\t\t    ranges);\n\tstruct vfio_device_feature_dma_logging_range __user *ranges;\n\tstruct vfio_device_feature_dma_logging_control control;\n\tstruct vfio_device_feature_dma_logging_range range;\n\tstruct rb_root_cached root = RB_ROOT_CACHED;\n\tstruct interval_tree_node *nodes;\n\tu64 iova_end;\n\tu32 nnodes;\n\tint i, ret;\n\n\tif (!device->log_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz,\n\t\t\t\t VFIO_DEVICE_FEATURE_SET,\n\t\t\t\t sizeof(control));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tif (copy_from_user(&control, arg, minsz))\n\t\treturn -EFAULT;\n\n\tnnodes = control.num_ranges;\n\tif (!nnodes)\n\t\treturn -EINVAL;\n\n\tif (nnodes > LOG_MAX_RANGES)\n\t\treturn -E2BIG;\n\n\tranges = u64_to_user_ptr(control.ranges);\n\tnodes = kmalloc_array(nnodes, sizeof(struct interval_tree_node),\n\t\t\t      GFP_KERNEL);\n\tif (!nodes)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nnodes; i++) {\n\t\tif (copy_from_user(&range, &ranges[i], sizeof(range))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto end;\n\t\t}\n\t\tif (!IS_ALIGNED(range.iova, control.page_size) ||\n\t\t    !IS_ALIGNED(range.length, control.page_size)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\n\t\tif (check_add_overflow(range.iova, range.length, &iova_end) ||\n\t\t    iova_end > ULONG_MAX) {\n\t\t\tret = -EOVERFLOW;\n\t\t\tgoto end;\n\t\t}\n\n\t\tnodes[i].start = range.iova;\n\t\tnodes[i].last = range.iova + range.length - 1;\n\t\tif (interval_tree_iter_first(&root, nodes[i].start,\n\t\t\t\t\t     nodes[i].last)) {\n\t\t\t \n\t\t\tret = -EINVAL;\n\t\t\tgoto end;\n\t\t}\n\t\tinterval_tree_insert(nodes + i, &root);\n\t}\n\n\tret = device->log_ops->log_start(device, &root, nnodes,\n\t\t\t\t\t &control.page_size);\n\tif (ret)\n\t\tgoto end;\n\n\tif (copy_to_user(arg, &control, sizeof(control))) {\n\t\tret = -EFAULT;\n\t\tdevice->log_ops->log_stop(device);\n\t}\n\nend:\n\tkfree(nodes);\n\treturn ret;\n}\n\nstatic int\nvfio_ioctl_device_feature_logging_stop(struct vfio_device *device,\n\t\t\t\t       u32 flags, void __user *arg,\n\t\t\t\t       size_t argsz)\n{\n\tint ret;\n\n\tif (!device->log_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz,\n\t\t\t\t VFIO_DEVICE_FEATURE_SET, 0);\n\tif (ret != 1)\n\t\treturn ret;\n\n\treturn device->log_ops->log_stop(device);\n}\n\nstatic int vfio_device_log_read_and_clear(struct iova_bitmap *iter,\n\t\t\t\t\t  unsigned long iova, size_t length,\n\t\t\t\t\t  void *opaque)\n{\n\tstruct vfio_device *device = opaque;\n\n\treturn device->log_ops->log_read_and_clear(device, iova, length, iter);\n}\n\nstatic int\nvfio_ioctl_device_feature_logging_report(struct vfio_device *device,\n\t\t\t\t\t u32 flags, void __user *arg,\n\t\t\t\t\t size_t argsz)\n{\n\tsize_t minsz =\n\t\toffsetofend(struct vfio_device_feature_dma_logging_report,\n\t\t\t    bitmap);\n\tstruct vfio_device_feature_dma_logging_report report;\n\tstruct iova_bitmap *iter;\n\tu64 iova_end;\n\tint ret;\n\n\tif (!device->log_ops)\n\t\treturn -ENOTTY;\n\n\tret = vfio_check_feature(flags, argsz,\n\t\t\t\t VFIO_DEVICE_FEATURE_GET,\n\t\t\t\t sizeof(report));\n\tif (ret != 1)\n\t\treturn ret;\n\n\tif (copy_from_user(&report, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (report.page_size < SZ_4K || !is_power_of_2(report.page_size))\n\t\treturn -EINVAL;\n\n\tif (check_add_overflow(report.iova, report.length, &iova_end) ||\n\t    iova_end > ULONG_MAX)\n\t\treturn -EOVERFLOW;\n\n\titer = iova_bitmap_alloc(report.iova, report.length,\n\t\t\t\t report.page_size,\n\t\t\t\t u64_to_user_ptr(report.bitmap));\n\tif (IS_ERR(iter))\n\t\treturn PTR_ERR(iter);\n\n\tret = iova_bitmap_for_each(iter, device,\n\t\t\t\t   vfio_device_log_read_and_clear);\n\n\tiova_bitmap_free(iter);\n\treturn ret;\n}\n\nstatic int vfio_ioctl_device_feature(struct vfio_device *device,\n\t\t\t\t     struct vfio_device_feature __user *arg)\n{\n\tsize_t minsz = offsetofend(struct vfio_device_feature, flags);\n\tstruct vfio_device_feature feature;\n\n\tif (copy_from_user(&feature, arg, minsz))\n\t\treturn -EFAULT;\n\n\tif (feature.argsz < minsz)\n\t\treturn -EINVAL;\n\n\t \n\tif (feature.flags &\n\t    ~(VFIO_DEVICE_FEATURE_MASK | VFIO_DEVICE_FEATURE_SET |\n\t      VFIO_DEVICE_FEATURE_GET | VFIO_DEVICE_FEATURE_PROBE))\n\t\treturn -EINVAL;\n\n\t \n\tif (!(feature.flags & VFIO_DEVICE_FEATURE_PROBE) &&\n\t    (feature.flags & VFIO_DEVICE_FEATURE_SET) &&\n\t    (feature.flags & VFIO_DEVICE_FEATURE_GET))\n\t\treturn -EINVAL;\n\n\tswitch (feature.flags & VFIO_DEVICE_FEATURE_MASK) {\n\tcase VFIO_DEVICE_FEATURE_MIGRATION:\n\t\treturn vfio_ioctl_device_feature_migration(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tcase VFIO_DEVICE_FEATURE_MIG_DEVICE_STATE:\n\t\treturn vfio_ioctl_device_feature_mig_device_state(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tcase VFIO_DEVICE_FEATURE_DMA_LOGGING_START:\n\t\treturn vfio_ioctl_device_feature_logging_start(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tcase VFIO_DEVICE_FEATURE_DMA_LOGGING_STOP:\n\t\treturn vfio_ioctl_device_feature_logging_stop(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tcase VFIO_DEVICE_FEATURE_DMA_LOGGING_REPORT:\n\t\treturn vfio_ioctl_device_feature_logging_report(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tcase VFIO_DEVICE_FEATURE_MIG_DATA_SIZE:\n\t\treturn vfio_ioctl_device_feature_migration_data_size(\n\t\t\tdevice, feature.flags, arg->data,\n\t\t\tfeature.argsz - minsz);\n\tdefault:\n\t\tif (unlikely(!device->ops->device_feature))\n\t\t\treturn -EINVAL;\n\t\treturn device->ops->device_feature(device, feature.flags,\n\t\t\t\t\t\t   arg->data,\n\t\t\t\t\t\t   feature.argsz - minsz);\n\t}\n}\n\nstatic long vfio_device_fops_unl_ioctl(struct file *filep,\n\t\t\t\t       unsigned int cmd, unsigned long arg)\n{\n\tstruct vfio_device_file *df = filep->private_data;\n\tstruct vfio_device *device = df->device;\n\tvoid __user *uptr = (void __user *)arg;\n\tint ret;\n\n\tif (cmd == VFIO_DEVICE_BIND_IOMMUFD)\n\t\treturn vfio_df_ioctl_bind_iommufd(df, uptr);\n\n\t \n\tif (!smp_load_acquire(&df->access_granted))\n\t\treturn -EINVAL;\n\n\tret = vfio_device_pm_runtime_get(device);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (IS_ENABLED(CONFIG_VFIO_DEVICE_CDEV) && !df->group) {\n\t\tswitch (cmd) {\n\t\tcase VFIO_DEVICE_ATTACH_IOMMUFD_PT:\n\t\t\tret = vfio_df_ioctl_attach_pt(df, uptr);\n\t\t\tgoto out;\n\n\t\tcase VFIO_DEVICE_DETACH_IOMMUFD_PT:\n\t\t\tret = vfio_df_ioctl_detach_pt(df, uptr);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tswitch (cmd) {\n\tcase VFIO_DEVICE_FEATURE:\n\t\tret = vfio_ioctl_device_feature(device, uptr);\n\t\tbreak;\n\n\tdefault:\n\t\tif (unlikely(!device->ops->ioctl))\n\t\t\tret = -EINVAL;\n\t\telse\n\t\t\tret = device->ops->ioctl(device, cmd, arg);\n\t\tbreak;\n\t}\nout:\n\tvfio_device_pm_runtime_put(device);\n\treturn ret;\n}\n\nstatic ssize_t vfio_device_fops_read(struct file *filep, char __user *buf,\n\t\t\t\t     size_t count, loff_t *ppos)\n{\n\tstruct vfio_device_file *df = filep->private_data;\n\tstruct vfio_device *device = df->device;\n\n\t \n\tif (!smp_load_acquire(&df->access_granted))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!device->ops->read))\n\t\treturn -EINVAL;\n\n\treturn device->ops->read(device, buf, count, ppos);\n}\n\nstatic ssize_t vfio_device_fops_write(struct file *filep,\n\t\t\t\t      const char __user *buf,\n\t\t\t\t      size_t count, loff_t *ppos)\n{\n\tstruct vfio_device_file *df = filep->private_data;\n\tstruct vfio_device *device = df->device;\n\n\t \n\tif (!smp_load_acquire(&df->access_granted))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!device->ops->write))\n\t\treturn -EINVAL;\n\n\treturn device->ops->write(device, buf, count, ppos);\n}\n\nstatic int vfio_device_fops_mmap(struct file *filep, struct vm_area_struct *vma)\n{\n\tstruct vfio_device_file *df = filep->private_data;\n\tstruct vfio_device *device = df->device;\n\n\t \n\tif (!smp_load_acquire(&df->access_granted))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!device->ops->mmap))\n\t\treturn -EINVAL;\n\n\treturn device->ops->mmap(device, vma);\n}\n\nconst struct file_operations vfio_device_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= vfio_device_fops_cdev_open,\n\t.release\t= vfio_device_fops_release,\n\t.read\t\t= vfio_device_fops_read,\n\t.write\t\t= vfio_device_fops_write,\n\t.unlocked_ioctl\t= vfio_device_fops_unl_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.mmap\t\t= vfio_device_fops_mmap,\n};\n\nstatic struct vfio_device *vfio_device_from_file(struct file *file)\n{\n\tstruct vfio_device_file *df = file->private_data;\n\n\tif (file->f_op != &vfio_device_fops)\n\t\treturn NULL;\n\treturn df->device;\n}\n\n \nbool vfio_file_is_valid(struct file *file)\n{\n\treturn vfio_group_from_file(file) ||\n\t       vfio_device_from_file(file);\n}\nEXPORT_SYMBOL_GPL(vfio_file_is_valid);\n\n \nbool vfio_file_enforced_coherent(struct file *file)\n{\n\tstruct vfio_device *device;\n\tstruct vfio_group *group;\n\n\tgroup = vfio_group_from_file(file);\n\tif (group)\n\t\treturn vfio_group_enforced_coherent(group);\n\n\tdevice = vfio_device_from_file(file);\n\tif (device)\n\t\treturn device_iommu_capable(device->dev,\n\t\t\t\t\t    IOMMU_CAP_ENFORCE_CACHE_COHERENCY);\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(vfio_file_enforced_coherent);\n\nstatic void vfio_device_file_set_kvm(struct file *file, struct kvm *kvm)\n{\n\tstruct vfio_device_file *df = file->private_data;\n\n\t \n\tspin_lock(&df->kvm_ref_lock);\n\tdf->kvm = kvm;\n\tspin_unlock(&df->kvm_ref_lock);\n}\n\n \nvoid vfio_file_set_kvm(struct file *file, struct kvm *kvm)\n{\n\tstruct vfio_group *group;\n\n\tgroup = vfio_group_from_file(file);\n\tif (group)\n\t\tvfio_group_set_kvm(group, kvm);\n\n\tif (vfio_device_from_file(file))\n\t\tvfio_device_file_set_kvm(file, kvm);\n}\nEXPORT_SYMBOL_GPL(vfio_file_set_kvm);\n\n \n \nstruct vfio_info_cap_header *vfio_info_cap_add(struct vfio_info_cap *caps,\n\t\t\t\t\t       size_t size, u16 id, u16 version)\n{\n\tvoid *buf;\n\tstruct vfio_info_cap_header *header, *tmp;\n\n\t \n\tsize = ALIGN(size, sizeof(u64));\n\n\tbuf = krealloc(caps->buf, caps->size + size, GFP_KERNEL);\n\tif (!buf) {\n\t\tkfree(caps->buf);\n\t\tcaps->buf = NULL;\n\t\tcaps->size = 0;\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tcaps->buf = buf;\n\theader = buf + caps->size;\n\n\t \n\tmemset(header, 0, size);\n\n\theader->id = id;\n\theader->version = version;\n\n\t \n\tfor (tmp = buf; tmp->next; tmp = buf + tmp->next)\n\t\t;  \n\n\ttmp->next = caps->size;\n\tcaps->size += size;\n\n\treturn header;\n}\nEXPORT_SYMBOL_GPL(vfio_info_cap_add);\n\nvoid vfio_info_cap_shift(struct vfio_info_cap *caps, size_t offset)\n{\n\tstruct vfio_info_cap_header *tmp;\n\tvoid *buf = (void *)caps->buf;\n\n\t \n\tWARN_ON(!IS_ALIGNED(offset, sizeof(u64)));\n\n\tfor (tmp = buf; tmp->next; tmp = buf + tmp->next - offset)\n\t\ttmp->next += offset;\n}\nEXPORT_SYMBOL(vfio_info_cap_shift);\n\nint vfio_info_add_capability(struct vfio_info_cap *caps,\n\t\t\t     struct vfio_info_cap_header *cap, size_t size)\n{\n\tstruct vfio_info_cap_header *header;\n\n\theader = vfio_info_cap_add(caps, size, cap->id, cap->version);\n\tif (IS_ERR(header))\n\t\treturn PTR_ERR(header);\n\n\tmemcpy(header + 1, cap + 1, size - sizeof(*header));\n\n\treturn 0;\n}\nEXPORT_SYMBOL(vfio_info_add_capability);\n\nint vfio_set_irqs_validate_and_prepare(struct vfio_irq_set *hdr, int num_irqs,\n\t\t\t\t       int max_irq_type, size_t *data_size)\n{\n\tunsigned long minsz;\n\tsize_t size;\n\n\tminsz = offsetofend(struct vfio_irq_set, count);\n\n\tif ((hdr->argsz < minsz) || (hdr->index >= max_irq_type) ||\n\t    (hdr->count >= (U32_MAX - hdr->start)) ||\n\t    (hdr->flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |\n\t\t\t\tVFIO_IRQ_SET_ACTION_TYPE_MASK)))\n\t\treturn -EINVAL;\n\n\tif (data_size)\n\t\t*data_size = 0;\n\n\tif (hdr->start >= num_irqs || hdr->start + hdr->count > num_irqs)\n\t\treturn -EINVAL;\n\n\tswitch (hdr->flags & VFIO_IRQ_SET_DATA_TYPE_MASK) {\n\tcase VFIO_IRQ_SET_DATA_NONE:\n\t\tsize = 0;\n\t\tbreak;\n\tcase VFIO_IRQ_SET_DATA_BOOL:\n\t\tsize = sizeof(uint8_t);\n\t\tbreak;\n\tcase VFIO_IRQ_SET_DATA_EVENTFD:\n\t\tsize = sizeof(int32_t);\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tif (size) {\n\t\tif (hdr->argsz - minsz < hdr->count * size)\n\t\t\treturn -EINVAL;\n\n\t\tif (!data_size)\n\t\t\treturn -EINVAL;\n\n\t\t*data_size = hdr->count * size;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(vfio_set_irqs_validate_and_prepare);\n\n \nint vfio_pin_pages(struct vfio_device *device, dma_addr_t iova,\n\t\t   int npage, int prot, struct page **pages)\n{\n\t \n\tif (!pages || !npage || WARN_ON(!vfio_assert_device_open(device)))\n\t\treturn -EINVAL;\n\tif (!device->ops->dma_unmap)\n\t\treturn -EINVAL;\n\tif (vfio_device_has_container(device))\n\t\treturn vfio_device_container_pin_pages(device, iova,\n\t\t\t\t\t\t       npage, prot, pages);\n\tif (device->iommufd_access) {\n\t\tint ret;\n\n\t\tif (iova > ULONG_MAX)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tret = iommufd_access_pin_pages(\n\t\t\tdevice->iommufd_access, ALIGN_DOWN(iova, PAGE_SIZE),\n\t\t\tnpage * PAGE_SIZE, pages,\n\t\t\t(prot & IOMMU_WRITE) ? IOMMUFD_ACCESS_RW_WRITE : 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\treturn npage;\n\t}\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(vfio_pin_pages);\n\n \nvoid vfio_unpin_pages(struct vfio_device *device, dma_addr_t iova, int npage)\n{\n\tif (WARN_ON(!vfio_assert_device_open(device)))\n\t\treturn;\n\tif (WARN_ON(!device->ops->dma_unmap))\n\t\treturn;\n\n\tif (vfio_device_has_container(device)) {\n\t\tvfio_device_container_unpin_pages(device, iova, npage);\n\t\treturn;\n\t}\n\tif (device->iommufd_access) {\n\t\tif (WARN_ON(iova > ULONG_MAX))\n\t\t\treturn;\n\t\tiommufd_access_unpin_pages(device->iommufd_access,\n\t\t\t\t\t   ALIGN_DOWN(iova, PAGE_SIZE),\n\t\t\t\t\t   npage * PAGE_SIZE);\n\t\treturn;\n\t}\n}\nEXPORT_SYMBOL(vfio_unpin_pages);\n\n \nint vfio_dma_rw(struct vfio_device *device, dma_addr_t iova, void *data,\n\t\tsize_t len, bool write)\n{\n\tif (!data || len <= 0 || !vfio_assert_device_open(device))\n\t\treturn -EINVAL;\n\n\tif (vfio_device_has_container(device))\n\t\treturn vfio_device_container_dma_rw(device, iova,\n\t\t\t\t\t\t    data, len, write);\n\n\tif (device->iommufd_access) {\n\t\tunsigned int flags = 0;\n\n\t\tif (iova > ULONG_MAX)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!current->mm)\n\t\t\tflags |= IOMMUFD_ACCESS_RW_KTHREAD;\n\t\tif (write)\n\t\t\tflags |= IOMMUFD_ACCESS_RW_WRITE;\n\t\treturn iommufd_access_rw(device->iommufd_access, iova, data,\n\t\t\t\t\t len, flags);\n\t}\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL(vfio_dma_rw);\n\n \nstatic int __init vfio_init(void)\n{\n\tint ret;\n\n\tida_init(&vfio.device_ida);\n\n\tret = vfio_group_init();\n\tif (ret)\n\t\treturn ret;\n\n\tret = vfio_virqfd_init();\n\tif (ret)\n\t\tgoto err_virqfd;\n\n\t \n\tvfio.device_class = class_create(\"vfio-dev\");\n\tif (IS_ERR(vfio.device_class)) {\n\t\tret = PTR_ERR(vfio.device_class);\n\t\tgoto err_dev_class;\n\t}\n\n\tret = vfio_cdev_init(vfio.device_class);\n\tif (ret)\n\t\tgoto err_alloc_dev_chrdev;\n\n\tpr_info(DRIVER_DESC \" version: \" DRIVER_VERSION \"\\n\");\n\treturn 0;\n\nerr_alloc_dev_chrdev:\n\tclass_destroy(vfio.device_class);\n\tvfio.device_class = NULL;\nerr_dev_class:\n\tvfio_virqfd_exit();\nerr_virqfd:\n\tvfio_group_cleanup();\n\treturn ret;\n}\n\nstatic void __exit vfio_cleanup(void)\n{\n\tida_destroy(&vfio.device_ida);\n\tvfio_cdev_cleanup();\n\tclass_destroy(vfio.device_class);\n\tvfio.device_class = NULL;\n\tvfio_virqfd_exit();\n\tvfio_group_cleanup();\n\txa_destroy(&vfio_device_set_xa);\n}\n\nmodule_init(vfio_init);\nmodule_exit(vfio_cleanup);\n\nMODULE_VERSION(DRIVER_VERSION);\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(DRIVER_AUTHOR);\nMODULE_DESCRIPTION(DRIVER_DESC);\nMODULE_SOFTDEP(\"post: vfio_iommu_type1 vfio_iommu_spapr_tce\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}