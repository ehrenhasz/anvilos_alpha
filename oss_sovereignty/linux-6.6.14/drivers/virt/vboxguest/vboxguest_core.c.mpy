{
  "module_name": "vboxguest_core.c",
  "hash_id": "d9a3a8928b80ba59093a906759bb571d291c062a1cf3693fc2eea320844f602b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/virt/vboxguest/vboxguest_core.c",
  "human_readable_source": " \n \n\n#include <linux/device.h>\n#include <linux/io.h>\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n#include <linux/vbox_err.h>\n#include <linux/vbox_utils.h>\n#include <linux/vmalloc.h>\n#include \"vboxguest_core.h\"\n#include \"vboxguest_version.h\"\n\n \n#define VBG_IOCTL_HGCM_CALL_PARMS(a) \\\n\t((struct vmmdev_hgcm_function_parameter *)( \\\n\t\t(u8 *)(a) + sizeof(struct vbg_ioctl_hgcm_call)))\n \n#define VBG_IOCTL_HGCM_CALL_PARMS32(a) \\\n\t((struct vmmdev_hgcm_function_parameter32 *)( \\\n\t\t(u8 *)(a) + sizeof(struct vbg_ioctl_hgcm_call)))\n\n#define GUEST_MAPPINGS_TRIES\t5\n\n#define VBG_KERNEL_REQUEST \\\n\t(VMMDEV_REQUESTOR_KERNEL | VMMDEV_REQUESTOR_USR_DRV | \\\n\t VMMDEV_REQUESTOR_CON_DONT_KNOW | VMMDEV_REQUESTOR_TRUST_NOT_GIVEN)\n\n \nstatic void vbg_guest_mappings_init(struct vbg_dev *gdev)\n{\n\tstruct vmmdev_hypervisorinfo *req;\n\tvoid *guest_mappings[GUEST_MAPPINGS_TRIES];\n\tstruct page **pages = NULL;\n\tu32 size, hypervisor_size;\n\tint i, rc;\n\n\t \n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_GET_HYPERVISOR_INFO,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn;\n\n\treq->hypervisor_start = 0;\n\treq->hypervisor_size = 0;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0)\n\t\tgoto out;\n\n\t \n\tif (req->hypervisor_size == 0)\n\t\tgoto out;\n\n\thypervisor_size = req->hypervisor_size;\n\t \n\tsize = PAGE_ALIGN(req->hypervisor_size) + SZ_4M;\n\n\tpages = kmalloc_array(size >> PAGE_SHIFT, sizeof(*pages), GFP_KERNEL);\n\tif (!pages)\n\t\tgoto out;\n\n\tgdev->guest_mappings_dummy_page = alloc_page(GFP_HIGHUSER);\n\tif (!gdev->guest_mappings_dummy_page)\n\t\tgoto out;\n\n\tfor (i = 0; i < (size >> PAGE_SHIFT); i++)\n\t\tpages[i] = gdev->guest_mappings_dummy_page;\n\n\t \n\tfor (i = 0; i < GUEST_MAPPINGS_TRIES; i++) {\n\t\tguest_mappings[i] = vmap(pages, (size >> PAGE_SHIFT),\n\t\t\t\t\t VM_MAP, PAGE_KERNEL_RO);\n\t\tif (!guest_mappings[i])\n\t\t\tbreak;\n\n\t\treq->header.request_type = VMMDEVREQ_SET_HYPERVISOR_INFO;\n\t\treq->header.rc = VERR_INTERNAL_ERROR;\n\t\treq->hypervisor_size = hypervisor_size;\n\t\treq->hypervisor_start =\n\t\t\t(unsigned long)PTR_ALIGN(guest_mappings[i], SZ_4M);\n\n\t\trc = vbg_req_perform(gdev, req);\n\t\tif (rc >= 0) {\n\t\t\tgdev->guest_mappings = guest_mappings[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\twhile (--i >= 0)\n\t\tvunmap(guest_mappings[i]);\n\n\t \n\tif (!gdev->guest_mappings) {\n\t\t__free_page(gdev->guest_mappings_dummy_page);\n\t\tgdev->guest_mappings_dummy_page = NULL;\n\t}\n\nout:\n\tvbg_req_free(req, sizeof(*req));\n\tkfree(pages);\n}\n\n \nstatic void vbg_guest_mappings_exit(struct vbg_dev *gdev)\n{\n\tstruct vmmdev_hypervisorinfo *req;\n\tint rc;\n\n\tif (!gdev->guest_mappings)\n\t\treturn;\n\n\t \n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_SET_HYPERVISOR_INFO,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn;\n\n\treq->hypervisor_start = 0;\n\treq->hypervisor_size = 0;\n\n\trc = vbg_req_perform(gdev, req);\n\n\tvbg_req_free(req, sizeof(*req));\n\n\tif (rc < 0) {\n\t\tvbg_err(\"%s error: %d\\n\", __func__, rc);\n\t\treturn;\n\t}\n\n\tvunmap(gdev->guest_mappings);\n\tgdev->guest_mappings = NULL;\n\n\t__free_page(gdev->guest_mappings_dummy_page);\n\tgdev->guest_mappings_dummy_page = NULL;\n}\n\n \nstatic int vbg_report_guest_info(struct vbg_dev *gdev)\n{\n\t \n\tstruct vmmdev_guest_info *req1 = NULL;\n\tstruct vmmdev_guest_info2 *req2 = NULL;\n\tint rc, ret = -ENOMEM;\n\n\treq1 = vbg_req_alloc(sizeof(*req1), VMMDEVREQ_REPORT_GUEST_INFO,\n\t\t\t     VBG_KERNEL_REQUEST);\n\treq2 = vbg_req_alloc(sizeof(*req2), VMMDEVREQ_REPORT_GUEST_INFO2,\n\t\t\t     VBG_KERNEL_REQUEST);\n\tif (!req1 || !req2)\n\t\tgoto out_free;\n\n\treq1->interface_version = VMMDEV_VERSION;\n\treq1->os_type = VMMDEV_OSTYPE_LINUX26;\n#if __BITS_PER_LONG == 64\n\treq1->os_type |= VMMDEV_OSTYPE_X64;\n#endif\n\n\treq2->additions_major = VBG_VERSION_MAJOR;\n\treq2->additions_minor = VBG_VERSION_MINOR;\n\treq2->additions_build = VBG_VERSION_BUILD;\n\treq2->additions_revision = VBG_SVN_REV;\n\treq2->additions_features =\n\t\tVMMDEV_GUEST_INFO2_ADDITIONS_FEATURES_REQUESTOR_INFO;\n\tstrscpy(req2->name, VBG_VERSION_STRING,\n\t\tsizeof(req2->name));\n\n\t \n\trc = vbg_req_perform(gdev, req2);\n\tif (rc >= 0) {\n\t\trc = vbg_req_perform(gdev, req1);\n\t} else if (rc == VERR_NOT_SUPPORTED || rc == VERR_NOT_IMPLEMENTED) {\n\t\trc = vbg_req_perform(gdev, req1);\n\t\tif (rc >= 0) {\n\t\t\trc = vbg_req_perform(gdev, req2);\n\t\t\tif (rc == VERR_NOT_IMPLEMENTED)\n\t\t\t\trc = VINF_SUCCESS;\n\t\t}\n\t}\n\tret = vbg_status_code_to_errno(rc);\n\nout_free:\n\tvbg_req_free(req2, sizeof(*req2));\n\tvbg_req_free(req1, sizeof(*req1));\n\treturn ret;\n}\n\n \nstatic int vbg_report_driver_status(struct vbg_dev *gdev, bool active)\n{\n\tstruct vmmdev_guest_status *req;\n\tint rc;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_REPORT_GUEST_STATUS,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->facility = VBOXGUEST_FACILITY_TYPE_VBOXGUEST_DRIVER;\n\tif (active)\n\t\treq->status = VBOXGUEST_FACILITY_STATUS_ACTIVE;\n\telse\n\t\treq->status = VBOXGUEST_FACILITY_STATUS_INACTIVE;\n\treq->flags = 0;\n\n\trc = vbg_req_perform(gdev, req);\n\tif (rc == VERR_NOT_IMPLEMENTED)\t \n\t\trc = VINF_SUCCESS;\n\n\tvbg_req_free(req, sizeof(*req));\n\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nstatic int vbg_balloon_inflate(struct vbg_dev *gdev, u32 chunk_idx)\n{\n\tstruct vmmdev_memballoon_change *req = gdev->mem_balloon.change_req;\n\tstruct page **pages;\n\tint i, rc, ret;\n\n\tpages = kmalloc_array(VMMDEV_MEMORY_BALLOON_CHUNK_PAGES,\n\t\t\t      sizeof(*pages),\n\t\t\t      GFP_KERNEL | __GFP_NOWARN);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\treq->header.size = sizeof(*req);\n\treq->inflate = true;\n\treq->pages = VMMDEV_MEMORY_BALLOON_CHUNK_PAGES;\n\n\tfor (i = 0; i < VMMDEV_MEMORY_BALLOON_CHUNK_PAGES; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!pages[i]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_error;\n\t\t}\n\n\t\treq->phys_page[i] = page_to_phys(pages[i]);\n\t}\n\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0) {\n\t\tvbg_err(\"%s error, rc: %d\\n\", __func__, rc);\n\t\tret = vbg_status_code_to_errno(rc);\n\t\tgoto out_error;\n\t}\n\n\tgdev->mem_balloon.pages[chunk_idx] = pages;\n\n\treturn 0;\n\nout_error:\n\twhile (--i >= 0)\n\t\t__free_page(pages[i]);\n\tkfree(pages);\n\n\treturn ret;\n}\n\n \nstatic int vbg_balloon_deflate(struct vbg_dev *gdev, u32 chunk_idx)\n{\n\tstruct vmmdev_memballoon_change *req = gdev->mem_balloon.change_req;\n\tstruct page **pages = gdev->mem_balloon.pages[chunk_idx];\n\tint i, rc;\n\n\treq->header.size = sizeof(*req);\n\treq->inflate = false;\n\treq->pages = VMMDEV_MEMORY_BALLOON_CHUNK_PAGES;\n\n\tfor (i = 0; i < VMMDEV_MEMORY_BALLOON_CHUNK_PAGES; i++)\n\t\treq->phys_page[i] = page_to_phys(pages[i]);\n\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0) {\n\t\tvbg_err(\"%s error, rc: %d\\n\", __func__, rc);\n\t\treturn vbg_status_code_to_errno(rc);\n\t}\n\n\tfor (i = 0; i < VMMDEV_MEMORY_BALLOON_CHUNK_PAGES; i++)\n\t\t__free_page(pages[i]);\n\tkfree(pages);\n\tgdev->mem_balloon.pages[chunk_idx] = NULL;\n\n\treturn 0;\n}\n\n \nstatic void vbg_balloon_work(struct work_struct *work)\n{\n\tstruct vbg_dev *gdev =\n\t\tcontainer_of(work, struct vbg_dev, mem_balloon.work);\n\tstruct vmmdev_memballoon_info *req = gdev->mem_balloon.get_req;\n\tu32 i, chunks;\n\tint rc, ret;\n\n\t \n\treq->event_ack = VMMDEV_EVENT_BALLOON_CHANGE_REQUEST;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0) {\n\t\tvbg_err(\"%s error, rc: %d)\\n\", __func__, rc);\n\t\treturn;\n\t}\n\n\t \n\tif (!gdev->mem_balloon.max_chunks) {\n\t\tgdev->mem_balloon.pages =\n\t\t\tdevm_kcalloc(gdev->dev, req->phys_mem_chunks,\n\t\t\t\t     sizeof(struct page **), GFP_KERNEL);\n\t\tif (!gdev->mem_balloon.pages)\n\t\t\treturn;\n\n\t\tgdev->mem_balloon.max_chunks = req->phys_mem_chunks;\n\t}\n\n\tchunks = req->balloon_chunks;\n\tif (chunks > gdev->mem_balloon.max_chunks) {\n\t\tvbg_err(\"%s: illegal balloon size %u (max=%u)\\n\",\n\t\t\t__func__, chunks, gdev->mem_balloon.max_chunks);\n\t\treturn;\n\t}\n\n\tif (chunks > gdev->mem_balloon.chunks) {\n\t\t \n\t\tfor (i = gdev->mem_balloon.chunks; i < chunks; i++) {\n\t\t\tret = vbg_balloon_inflate(gdev, i);\n\t\t\tif (ret < 0)\n\t\t\t\treturn;\n\n\t\t\tgdev->mem_balloon.chunks++;\n\t\t}\n\t} else {\n\t\t \n\t\tfor (i = gdev->mem_balloon.chunks; i-- > chunks;) {\n\t\t\tret = vbg_balloon_deflate(gdev, i);\n\t\t\tif (ret < 0)\n\t\t\t\treturn;\n\n\t\t\tgdev->mem_balloon.chunks--;\n\t\t}\n\t}\n}\n\n \nstatic void vbg_heartbeat_timer(struct timer_list *t)\n{\n\tstruct vbg_dev *gdev = from_timer(gdev, t, heartbeat_timer);\n\n\tvbg_req_perform(gdev, gdev->guest_heartbeat_req);\n\tmod_timer(&gdev->heartbeat_timer,\n\t\t  msecs_to_jiffies(gdev->heartbeat_interval_ms));\n}\n\n \nstatic int vbg_heartbeat_host_config(struct vbg_dev *gdev, bool enabled)\n{\n\tstruct vmmdev_heartbeat *req;\n\tint rc;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_HEARTBEAT_CONFIGURE,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->enabled = enabled;\n\treq->interval_ns = 0;\n\trc = vbg_req_perform(gdev, req);\n\tdo_div(req->interval_ns, 1000000);  \n\tgdev->heartbeat_interval_ms = req->interval_ns;\n\tvbg_req_free(req, sizeof(*req));\n\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nstatic int vbg_heartbeat_init(struct vbg_dev *gdev)\n{\n\tint ret;\n\n\t \n\tret = vbg_heartbeat_host_config(gdev, false);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = vbg_heartbeat_host_config(gdev, true);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tgdev->guest_heartbeat_req = vbg_req_alloc(\n\t\t\t\t\tsizeof(*gdev->guest_heartbeat_req),\n\t\t\t\t\tVMMDEVREQ_GUEST_HEARTBEAT,\n\t\t\t\t\tVBG_KERNEL_REQUEST);\n\tif (!gdev->guest_heartbeat_req)\n\t\treturn -ENOMEM;\n\n\tvbg_info(\"%s: Setting up heartbeat to trigger every %d milliseconds\\n\",\n\t\t __func__, gdev->heartbeat_interval_ms);\n\tmod_timer(&gdev->heartbeat_timer, 0);\n\n\treturn 0;\n}\n\n \nstatic void vbg_heartbeat_exit(struct vbg_dev *gdev)\n{\n\tdel_timer_sync(&gdev->heartbeat_timer);\n\tvbg_heartbeat_host_config(gdev, false);\n\tvbg_req_free(gdev->guest_heartbeat_req,\n\t\t     sizeof(*gdev->guest_heartbeat_req));\n}\n\n \nstatic bool vbg_track_bit_usage(struct vbg_bit_usage_tracker *tracker,\n\t\t\t\tu32 changed, u32 previous)\n{\n\tbool global_change = false;\n\n\twhile (changed) {\n\t\tu32 bit = ffs(changed) - 1;\n\t\tu32 bitmask = BIT(bit);\n\n\t\tif (bitmask & previous) {\n\t\t\ttracker->per_bit_usage[bit] -= 1;\n\t\t\tif (tracker->per_bit_usage[bit] == 0) {\n\t\t\t\tglobal_change = true;\n\t\t\t\ttracker->mask &= ~bitmask;\n\t\t\t}\n\t\t} else {\n\t\t\ttracker->per_bit_usage[bit] += 1;\n\t\t\tif (tracker->per_bit_usage[bit] == 1) {\n\t\t\t\tglobal_change = true;\n\t\t\t\ttracker->mask |= bitmask;\n\t\t\t}\n\t\t}\n\n\t\tchanged &= ~bitmask;\n\t}\n\n\treturn global_change;\n}\n\n \nstatic int vbg_reset_host_event_filter(struct vbg_dev *gdev,\n\t\t\t\t       u32 fixed_events)\n{\n\tstruct vmmdev_mask *req;\n\tint rc;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_CTL_GUEST_FILTER_MASK,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->not_mask = U32_MAX & ~fixed_events;\n\treq->or_mask = fixed_events;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0)\n\t\tvbg_err(\"%s error, rc: %d\\n\", __func__, rc);\n\n\tvbg_req_free(req, sizeof(*req));\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nstatic int vbg_set_session_event_filter(struct vbg_dev *gdev,\n\t\t\t\t\tstruct vbg_session *session,\n\t\t\t\t\tu32 or_mask, u32 not_mask,\n\t\t\t\t\tbool session_termination)\n{\n\tstruct vmmdev_mask *req;\n\tu32 changed, previous;\n\tint rc, ret = 0;\n\n\t \n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_CTL_GUEST_FILTER_MASK,\n\t\t\t    session_termination ? VBG_KERNEL_REQUEST :\n\t\t\t\t\t\t  session->requestor);\n\tif (!req) {\n\t\tif (!session_termination)\n\t\t\treturn -ENOMEM;\n\t\t \n\t}\n\n\tmutex_lock(&gdev->session_mutex);\n\n\t \n\tprevious = session->event_filter;\n\tsession->event_filter |= or_mask;\n\tsession->event_filter &= ~not_mask;\n\n\t \n\tchanged = previous ^ session->event_filter;\n\tif (!changed)\n\t\tgoto out;\n\n\tvbg_track_bit_usage(&gdev->event_filter_tracker, changed, previous);\n\tor_mask = gdev->fixed_events | gdev->event_filter_tracker.mask;\n\n\tif (gdev->event_filter_host == or_mask || !req)\n\t\tgoto out;\n\n\tgdev->event_filter_host = or_mask;\n\treq->or_mask = or_mask;\n\treq->not_mask = ~or_mask;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0) {\n\t\tret = vbg_status_code_to_errno(rc);\n\n\t\t \n\t\tgdev->event_filter_host = U32_MAX;\n\t\tif (session_termination)\n\t\t\tgoto out;\n\n\t\tvbg_track_bit_usage(&gdev->event_filter_tracker, changed,\n\t\t\t\t    session->event_filter);\n\t\tsession->event_filter = previous;\n\t}\n\nout:\n\tmutex_unlock(&gdev->session_mutex);\n\tvbg_req_free(req, sizeof(*req));\n\n\treturn ret;\n}\n\n \nstatic int vbg_reset_host_capabilities(struct vbg_dev *gdev)\n{\n\tstruct vmmdev_mask *req;\n\tint rc;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_SET_GUEST_CAPABILITIES,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->not_mask = U32_MAX;\n\treq->or_mask = 0;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0)\n\t\tvbg_err(\"%s error, rc: %d\\n\", __func__, rc);\n\n\tvbg_req_free(req, sizeof(*req));\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nstatic int vbg_set_host_capabilities(struct vbg_dev *gdev,\n\t\t\t\t     struct vbg_session *session,\n\t\t\t\t     bool session_termination)\n{\n\tstruct vmmdev_mask *req;\n\tu32 caps;\n\tint rc;\n\n\tWARN_ON(!mutex_is_locked(&gdev->session_mutex));\n\n\tcaps = gdev->acquired_guest_caps | gdev->set_guest_caps_tracker.mask;\n\n\tif (gdev->guest_caps_host == caps)\n\t\treturn 0;\n\n\t \n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_SET_GUEST_CAPABILITIES,\n\t\t\t    session_termination ? VBG_KERNEL_REQUEST :\n\t\t\t\t\t\t  session->requestor);\n\tif (!req) {\n\t\tgdev->guest_caps_host = U32_MAX;\n\t\treturn -ENOMEM;\n\t}\n\n\treq->or_mask = caps;\n\treq->not_mask = ~caps;\n\trc = vbg_req_perform(gdev, req);\n\tvbg_req_free(req, sizeof(*req));\n\n\tgdev->guest_caps_host = (rc >= 0) ? caps : U32_MAX;\n\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nstatic int vbg_acquire_session_capabilities(struct vbg_dev *gdev,\n\t\t\t\t\t    struct vbg_session *session,\n\t\t\t\t\t    u32 or_mask, u32 not_mask,\n\t\t\t\t\t    u32 flags, bool session_termination)\n{\n\tunsigned long irqflags;\n\tbool wakeup = false;\n\tint ret = 0;\n\n\tmutex_lock(&gdev->session_mutex);\n\n\tif (gdev->set_guest_caps_tracker.mask & or_mask) {\n\t\tvbg_err(\"%s error: cannot acquire caps which are currently set\\n\",\n\t\t\t__func__);\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tspin_lock_irqsave(&gdev->event_spinlock, irqflags);\n\tgdev->acquire_mode_guest_caps |= or_mask;\n\tspin_unlock_irqrestore(&gdev->event_spinlock, irqflags);\n\n\t \n\tif (flags & VBGL_IOC_AGC_FLAGS_CONFIG_ACQUIRE_MODE)\n\t\tgoto out;\n\n\tnot_mask &= ~or_mask;  \n\tnot_mask &= session->acquired_guest_caps;\n\tor_mask &= ~session->acquired_guest_caps;\n\n\tif (or_mask == 0 && not_mask == 0)\n\t\tgoto out;\n\n\tif (gdev->acquired_guest_caps & or_mask) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tgdev->acquired_guest_caps |= or_mask;\n\tgdev->acquired_guest_caps &= ~not_mask;\n\t \n\tspin_lock_irqsave(&gdev->event_spinlock, irqflags);\n\tsession->acquired_guest_caps |= or_mask;\n\tsession->acquired_guest_caps &= ~not_mask;\n\tspin_unlock_irqrestore(&gdev->event_spinlock, irqflags);\n\n\tret = vbg_set_host_capabilities(gdev, session, session_termination);\n\t \n\tif (ret < 0 && !session_termination) {\n\t\tgdev->acquired_guest_caps &= ~or_mask;\n\t\tgdev->acquired_guest_caps |= not_mask;\n\t\tspin_lock_irqsave(&gdev->event_spinlock, irqflags);\n\t\tsession->acquired_guest_caps &= ~or_mask;\n\t\tsession->acquired_guest_caps |= not_mask;\n\t\tspin_unlock_irqrestore(&gdev->event_spinlock, irqflags);\n\t}\n\n\t \n\tif (ret == 0 && or_mask != 0) {\n\t\tspin_lock_irqsave(&gdev->event_spinlock, irqflags);\n\n\t\tif (or_mask & VMMDEV_GUEST_SUPPORTS_SEAMLESS)\n\t\t\tgdev->pending_events |=\n\t\t\t\tVMMDEV_EVENT_SEAMLESS_MODE_CHANGE_REQUEST;\n\n\t\tif (gdev->pending_events)\n\t\t\twakeup = true;\n\n\t\tspin_unlock_irqrestore(&gdev->event_spinlock, irqflags);\n\n\t\tif (wakeup)\n\t\t\twake_up(&gdev->event_wq);\n\t}\n\nout:\n\tmutex_unlock(&gdev->session_mutex);\n\n\treturn ret;\n}\n\n \nstatic int vbg_set_session_capabilities(struct vbg_dev *gdev,\n\t\t\t\t\tstruct vbg_session *session,\n\t\t\t\t\tu32 or_mask, u32 not_mask,\n\t\t\t\t\tbool session_termination)\n{\n\tu32 changed, previous;\n\tint ret = 0;\n\n\tmutex_lock(&gdev->session_mutex);\n\n\tif (gdev->acquire_mode_guest_caps & or_mask) {\n\t\tvbg_err(\"%s error: cannot set caps which are in acquire_mode\\n\",\n\t\t\t__func__);\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\t \n\tprevious = session->set_guest_caps;\n\tsession->set_guest_caps |= or_mask;\n\tsession->set_guest_caps &= ~not_mask;\n\n\t \n\tchanged = previous ^ session->set_guest_caps;\n\tif (!changed)\n\t\tgoto out;\n\n\tvbg_track_bit_usage(&gdev->set_guest_caps_tracker, changed, previous);\n\n\tret = vbg_set_host_capabilities(gdev, session, session_termination);\n\t \n\tif (ret < 0 && !session_termination) {\n\t\tvbg_track_bit_usage(&gdev->set_guest_caps_tracker, changed,\n\t\t\t\t    session->set_guest_caps);\n\t\tsession->set_guest_caps = previous;\n\t}\n\nout:\n\tmutex_unlock(&gdev->session_mutex);\n\n\treturn ret;\n}\n\n \nstatic int vbg_query_host_version(struct vbg_dev *gdev)\n{\n\tstruct vmmdev_host_version *req;\n\tint rc, ret;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_GET_HOST_VERSION,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\trc = vbg_req_perform(gdev, req);\n\tret = vbg_status_code_to_errno(rc);\n\tif (ret) {\n\t\tvbg_err(\"%s error: %d\\n\", __func__, rc);\n\t\tgoto out;\n\t}\n\n\tsnprintf(gdev->host_version, sizeof(gdev->host_version), \"%u.%u.%ur%u\",\n\t\t req->major, req->minor, req->build, req->revision);\n\tgdev->host_features = req->features;\n\n\tvbg_info(\"vboxguest: host-version: %s %#x\\n\", gdev->host_version,\n\t\t gdev->host_features);\n\n\tif (!(req->features & VMMDEV_HVF_HGCM_PHYS_PAGE_LIST)) {\n\t\tvbg_err(\"vboxguest: Error host too old (does not support page-lists)\\n\");\n\t\tret = -ENODEV;\n\t}\n\nout:\n\tvbg_req_free(req, sizeof(*req));\n\treturn ret;\n}\n\n \nint vbg_core_init(struct vbg_dev *gdev, u32 fixed_events)\n{\n\tint ret = -ENOMEM;\n\n\tgdev->fixed_events = fixed_events | VMMDEV_EVENT_HGCM;\n\tgdev->event_filter_host = U32_MAX;\t \n\tgdev->guest_caps_host = U32_MAX;\t \n\n\tinit_waitqueue_head(&gdev->event_wq);\n\tinit_waitqueue_head(&gdev->hgcm_wq);\n\tspin_lock_init(&gdev->event_spinlock);\n\tmutex_init(&gdev->session_mutex);\n\tmutex_init(&gdev->cancel_req_mutex);\n\ttimer_setup(&gdev->heartbeat_timer, vbg_heartbeat_timer, 0);\n\tINIT_WORK(&gdev->mem_balloon.work, vbg_balloon_work);\n\n\tgdev->mem_balloon.get_req =\n\t\tvbg_req_alloc(sizeof(*gdev->mem_balloon.get_req),\n\t\t\t      VMMDEVREQ_GET_MEMBALLOON_CHANGE_REQ,\n\t\t\t      VBG_KERNEL_REQUEST);\n\tgdev->mem_balloon.change_req =\n\t\tvbg_req_alloc(sizeof(*gdev->mem_balloon.change_req),\n\t\t\t      VMMDEVREQ_CHANGE_MEMBALLOON,\n\t\t\t      VBG_KERNEL_REQUEST);\n\tgdev->cancel_req =\n\t\tvbg_req_alloc(sizeof(*(gdev->cancel_req)),\n\t\t\t      VMMDEVREQ_HGCM_CANCEL2,\n\t\t\t      VBG_KERNEL_REQUEST);\n\tgdev->ack_events_req =\n\t\tvbg_req_alloc(sizeof(*gdev->ack_events_req),\n\t\t\t      VMMDEVREQ_ACKNOWLEDGE_EVENTS,\n\t\t\t      VBG_KERNEL_REQUEST);\n\tgdev->mouse_status_req =\n\t\tvbg_req_alloc(sizeof(*gdev->mouse_status_req),\n\t\t\t      VMMDEVREQ_GET_MOUSE_STATUS,\n\t\t\t      VBG_KERNEL_REQUEST);\n\n\tif (!gdev->mem_balloon.get_req || !gdev->mem_balloon.change_req ||\n\t    !gdev->cancel_req || !gdev->ack_events_req ||\n\t    !gdev->mouse_status_req)\n\t\tgoto err_free_reqs;\n\n\tret = vbg_query_host_version(gdev);\n\tif (ret)\n\t\tgoto err_free_reqs;\n\n\tret = vbg_report_guest_info(gdev);\n\tif (ret) {\n\t\tvbg_err(\"vboxguest: vbg_report_guest_info error: %d\\n\", ret);\n\t\tgoto err_free_reqs;\n\t}\n\n\tret = vbg_reset_host_event_filter(gdev, gdev->fixed_events);\n\tif (ret) {\n\t\tvbg_err(\"vboxguest: Error setting fixed event filter: %d\\n\",\n\t\t\tret);\n\t\tgoto err_free_reqs;\n\t}\n\n\tret = vbg_reset_host_capabilities(gdev);\n\tif (ret) {\n\t\tvbg_err(\"vboxguest: Error clearing guest capabilities: %d\\n\",\n\t\t\tret);\n\t\tgoto err_free_reqs;\n\t}\n\n\tret = vbg_core_set_mouse_status(gdev, 0);\n\tif (ret) {\n\t\tvbg_err(\"vboxguest: Error clearing mouse status: %d\\n\", ret);\n\t\tgoto err_free_reqs;\n\t}\n\n\t \n\tvbg_guest_mappings_init(gdev);\n\tvbg_heartbeat_init(gdev);\n\n\t \n\tret = vbg_report_driver_status(gdev, true);\n\tif (ret < 0)\n\t\tvbg_err(\"vboxguest: Error reporting driver status: %d\\n\", ret);\n\n\treturn 0;\n\nerr_free_reqs:\n\tvbg_req_free(gdev->mouse_status_req,\n\t\t     sizeof(*gdev->mouse_status_req));\n\tvbg_req_free(gdev->ack_events_req,\n\t\t     sizeof(*gdev->ack_events_req));\n\tvbg_req_free(gdev->cancel_req,\n\t\t     sizeof(*gdev->cancel_req));\n\tvbg_req_free(gdev->mem_balloon.change_req,\n\t\t     sizeof(*gdev->mem_balloon.change_req));\n\tvbg_req_free(gdev->mem_balloon.get_req,\n\t\t     sizeof(*gdev->mem_balloon.get_req));\n\treturn ret;\n}\n\n \nvoid vbg_core_exit(struct vbg_dev *gdev)\n{\n\tvbg_heartbeat_exit(gdev);\n\tvbg_guest_mappings_exit(gdev);\n\n\t \n\tvbg_reset_host_event_filter(gdev, 0);\n\tvbg_reset_host_capabilities(gdev);\n\tvbg_core_set_mouse_status(gdev, 0);\n\n\tvbg_req_free(gdev->mouse_status_req,\n\t\t     sizeof(*gdev->mouse_status_req));\n\tvbg_req_free(gdev->ack_events_req,\n\t\t     sizeof(*gdev->ack_events_req));\n\tvbg_req_free(gdev->cancel_req,\n\t\t     sizeof(*gdev->cancel_req));\n\tvbg_req_free(gdev->mem_balloon.change_req,\n\t\t     sizeof(*gdev->mem_balloon.change_req));\n\tvbg_req_free(gdev->mem_balloon.get_req,\n\t\t     sizeof(*gdev->mem_balloon.get_req));\n}\n\n \nstruct vbg_session *vbg_core_open_session(struct vbg_dev *gdev, u32 requestor)\n{\n\tstruct vbg_session *session;\n\n\tsession = kzalloc(sizeof(*session), GFP_KERNEL);\n\tif (!session)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tsession->gdev = gdev;\n\tsession->requestor = requestor;\n\n\treturn session;\n}\n\n \nvoid vbg_core_close_session(struct vbg_session *session)\n{\n\tstruct vbg_dev *gdev = session->gdev;\n\tint i, rc;\n\n\tvbg_acquire_session_capabilities(gdev, session, 0, U32_MAX, 0, true);\n\tvbg_set_session_capabilities(gdev, session, 0, U32_MAX, true);\n\tvbg_set_session_event_filter(gdev, session, 0, U32_MAX, true);\n\n\tfor (i = 0; i < ARRAY_SIZE(session->hgcm_client_ids); i++) {\n\t\tif (!session->hgcm_client_ids[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tvbg_hgcm_disconnect(gdev, VBG_KERNEL_REQUEST,\n\t\t\t\t    session->hgcm_client_ids[i], &rc);\n\t}\n\n\tkfree(session);\n}\n\nstatic int vbg_ioctl_chk(struct vbg_ioctl_hdr *hdr, size_t in_size,\n\t\t\t size_t out_size)\n{\n\tif (hdr->size_in  != (sizeof(*hdr) + in_size) ||\n\t    hdr->size_out != (sizeof(*hdr) + out_size))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_driver_version_info(\n\tstruct vbg_ioctl_driver_version_info *info)\n{\n\tconst u16 vbg_maj_version = VBG_IOC_VERSION >> 16;\n\tu16 min_maj_version, req_maj_version;\n\n\tif (vbg_ioctl_chk(&info->hdr, sizeof(info->u.in), sizeof(info->u.out)))\n\t\treturn -EINVAL;\n\n\treq_maj_version = info->u.in.req_version >> 16;\n\tmin_maj_version = info->u.in.min_version >> 16;\n\n\tif (info->u.in.min_version > info->u.in.req_version ||\n\t    min_maj_version != req_maj_version)\n\t\treturn -EINVAL;\n\n\tif (info->u.in.min_version <= VBG_IOC_VERSION &&\n\t    min_maj_version == vbg_maj_version) {\n\t\tinfo->u.out.session_version = VBG_IOC_VERSION;\n\t} else {\n\t\tinfo->u.out.session_version = U32_MAX;\n\t\tinfo->hdr.rc = VERR_VERSION_MISMATCH;\n\t}\n\n\tinfo->u.out.driver_version  = VBG_IOC_VERSION;\n\tinfo->u.out.driver_revision = 0;\n\tinfo->u.out.reserved1      = 0;\n\tinfo->u.out.reserved2      = 0;\n\n\treturn 0;\n}\n\n \nstatic u32 vbg_get_allowed_event_mask_for_session(struct vbg_dev *gdev,\n\t\t\t\t\t\t  struct vbg_session *session)\n{\n\tu32 acquire_mode_caps = gdev->acquire_mode_guest_caps;\n\tu32 session_acquired_caps = session->acquired_guest_caps;\n\tu32 allowed_events = VMMDEV_EVENT_VALID_EVENT_MASK;\n\n\tif ((acquire_mode_caps & VMMDEV_GUEST_SUPPORTS_GRAPHICS) &&\n\t    !(session_acquired_caps & VMMDEV_GUEST_SUPPORTS_GRAPHICS))\n\t\tallowed_events &= ~VMMDEV_EVENT_DISPLAY_CHANGE_REQUEST;\n\n\tif ((acquire_mode_caps & VMMDEV_GUEST_SUPPORTS_SEAMLESS) &&\n\t    !(session_acquired_caps & VMMDEV_GUEST_SUPPORTS_SEAMLESS))\n\t\tallowed_events &= ~VMMDEV_EVENT_SEAMLESS_MODE_CHANGE_REQUEST;\n\n\treturn allowed_events;\n}\n\nstatic bool vbg_wait_event_cond(struct vbg_dev *gdev,\n\t\t\t\tstruct vbg_session *session,\n\t\t\t\tu32 event_mask)\n{\n\tunsigned long flags;\n\tbool wakeup;\n\tu32 events;\n\n\tspin_lock_irqsave(&gdev->event_spinlock, flags);\n\n\tevents = gdev->pending_events & event_mask;\n\tevents &= vbg_get_allowed_event_mask_for_session(gdev, session);\n\twakeup = events || session->cancel_waiters;\n\n\tspin_unlock_irqrestore(&gdev->event_spinlock, flags);\n\n\treturn wakeup;\n}\n\n \nstatic u32 vbg_consume_events_locked(struct vbg_dev *gdev,\n\t\t\t\t     struct vbg_session *session,\n\t\t\t\t     u32 event_mask)\n{\n\tu32 events = gdev->pending_events & event_mask;\n\n\tevents &= vbg_get_allowed_event_mask_for_session(gdev, session);\n\tgdev->pending_events &= ~events;\n\treturn events;\n}\n\nstatic int vbg_ioctl_wait_for_events(struct vbg_dev *gdev,\n\t\t\t\t     struct vbg_session *session,\n\t\t\t\t     struct vbg_ioctl_wait_for_events *wait)\n{\n\tu32 timeout_ms = wait->u.in.timeout_ms;\n\tu32 event_mask = wait->u.in.events;\n\tunsigned long flags;\n\tlong timeout;\n\tint ret = 0;\n\n\tif (vbg_ioctl_chk(&wait->hdr, sizeof(wait->u.in), sizeof(wait->u.out)))\n\t\treturn -EINVAL;\n\n\tif (timeout_ms == U32_MAX)\n\t\ttimeout = MAX_SCHEDULE_TIMEOUT;\n\telse\n\t\ttimeout = msecs_to_jiffies(timeout_ms);\n\n\twait->u.out.events = 0;\n\tdo {\n\t\ttimeout = wait_event_interruptible_timeout(\n\t\t\t\tgdev->event_wq,\n\t\t\t\tvbg_wait_event_cond(gdev, session, event_mask),\n\t\t\t\ttimeout);\n\n\t\tspin_lock_irqsave(&gdev->event_spinlock, flags);\n\n\t\tif (timeout < 0 || session->cancel_waiters) {\n\t\t\tret = -EINTR;\n\t\t} else if (timeout == 0) {\n\t\t\tret = -ETIMEDOUT;\n\t\t} else {\n\t\t\twait->u.out.events =\n\t\t\t   vbg_consume_events_locked(gdev, session, event_mask);\n\t\t}\n\n\t\tspin_unlock_irqrestore(&gdev->event_spinlock, flags);\n\n\t\t \n\t} while (ret == 0 && wait->u.out.events == 0);\n\n\treturn ret;\n}\n\nstatic int vbg_ioctl_interrupt_all_wait_events(struct vbg_dev *gdev,\n\t\t\t\t\t       struct vbg_session *session,\n\t\t\t\t\t       struct vbg_ioctl_hdr *hdr)\n{\n\tunsigned long flags;\n\n\tif (hdr->size_in != sizeof(*hdr) || hdr->size_out != sizeof(*hdr))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&gdev->event_spinlock, flags);\n\tsession->cancel_waiters = true;\n\tspin_unlock_irqrestore(&gdev->event_spinlock, flags);\n\n\twake_up(&gdev->event_wq);\n\n\treturn 0;\n}\n\n \nstatic int vbg_req_allowed(struct vbg_dev *gdev, struct vbg_session *session,\n\t\t\t   const struct vmmdev_request_header *req)\n{\n\tconst struct vmmdev_guest_status *guest_status;\n\tbool trusted_apps_only;\n\n\tswitch (req->request_type) {\n\t \n\tcase VMMDEVREQ_QUERY_CREDENTIALS:\n\tcase VMMDEVREQ_REPORT_CREDENTIALS_JUDGEMENT:\n\tcase VMMDEVREQ_REGISTER_SHARED_MODULE:\n\tcase VMMDEVREQ_UNREGISTER_SHARED_MODULE:\n\tcase VMMDEVREQ_WRITE_COREDUMP:\n\tcase VMMDEVREQ_GET_CPU_HOTPLUG_REQ:\n\tcase VMMDEVREQ_SET_CPU_HOTPLUG_STATUS:\n\tcase VMMDEVREQ_CHECK_SHARED_MODULES:\n\tcase VMMDEVREQ_GET_PAGE_SHARING_STATUS:\n\tcase VMMDEVREQ_DEBUG_IS_PAGE_SHARED:\n\tcase VMMDEVREQ_REPORT_GUEST_STATS:\n\tcase VMMDEVREQ_REPORT_GUEST_USER_STATE:\n\tcase VMMDEVREQ_GET_STATISTICS_CHANGE_REQ:\n\t\ttrusted_apps_only = true;\n\t\tbreak;\n\n\t \n\tcase VMMDEVREQ_GET_MOUSE_STATUS:\n\tcase VMMDEVREQ_SET_MOUSE_STATUS:\n\tcase VMMDEVREQ_SET_POINTER_SHAPE:\n\tcase VMMDEVREQ_GET_HOST_VERSION:\n\tcase VMMDEVREQ_IDLE:\n\tcase VMMDEVREQ_GET_HOST_TIME:\n\tcase VMMDEVREQ_SET_POWER_STATUS:\n\tcase VMMDEVREQ_ACKNOWLEDGE_EVENTS:\n\tcase VMMDEVREQ_CTL_GUEST_FILTER_MASK:\n\tcase VMMDEVREQ_REPORT_GUEST_STATUS:\n\tcase VMMDEVREQ_GET_DISPLAY_CHANGE_REQ:\n\tcase VMMDEVREQ_VIDEMODE_SUPPORTED:\n\tcase VMMDEVREQ_GET_HEIGHT_REDUCTION:\n\tcase VMMDEVREQ_GET_DISPLAY_CHANGE_REQ2:\n\tcase VMMDEVREQ_VIDEMODE_SUPPORTED2:\n\tcase VMMDEVREQ_VIDEO_ACCEL_ENABLE:\n\tcase VMMDEVREQ_VIDEO_ACCEL_FLUSH:\n\tcase VMMDEVREQ_VIDEO_SET_VISIBLE_REGION:\n\tcase VMMDEVREQ_VIDEO_UPDATE_MONITOR_POSITIONS:\n\tcase VMMDEVREQ_GET_DISPLAY_CHANGE_REQEX:\n\tcase VMMDEVREQ_GET_DISPLAY_CHANGE_REQ_MULTI:\n\tcase VMMDEVREQ_GET_SEAMLESS_CHANGE_REQ:\n\tcase VMMDEVREQ_GET_VRDPCHANGE_REQ:\n\tcase VMMDEVREQ_LOG_STRING:\n\tcase VMMDEVREQ_GET_SESSION_ID:\n\t\ttrusted_apps_only = false;\n\t\tbreak;\n\n\t \n\tcase VMMDEVREQ_REPORT_GUEST_CAPABILITIES:\n\t\tguest_status = (const struct vmmdev_guest_status *)req;\n\t\tswitch (guest_status->facility) {\n\t\tcase VBOXGUEST_FACILITY_TYPE_ALL:\n\t\tcase VBOXGUEST_FACILITY_TYPE_VBOXGUEST_DRIVER:\n\t\t\tvbg_err(\"Denying userspace vmm report guest cap. call facility %#08x\\n\",\n\t\t\t\tguest_status->facility);\n\t\t\treturn -EPERM;\n\t\tcase VBOXGUEST_FACILITY_TYPE_VBOX_SERVICE:\n\t\t\ttrusted_apps_only = true;\n\t\t\tbreak;\n\t\tcase VBOXGUEST_FACILITY_TYPE_VBOX_TRAY_CLIENT:\n\t\tcase VBOXGUEST_FACILITY_TYPE_SEAMLESS:\n\t\tcase VBOXGUEST_FACILITY_TYPE_GRAPHICS:\n\t\tdefault:\n\t\t\ttrusted_apps_only = false;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t \n\tdefault:\n\t\tvbg_err(\"Denying userspace vmm call type %#08x\\n\",\n\t\t\treq->request_type);\n\t\treturn -EPERM;\n\t}\n\n\tif (trusted_apps_only &&\n\t    (session->requestor & VMMDEV_REQUESTOR_USER_DEVICE)) {\n\t\tvbg_err(\"Denying userspace vmm call type %#08x through vboxuser device node\\n\",\n\t\t\treq->request_type);\n\t\treturn -EPERM;\n\t}\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_vmmrequest(struct vbg_dev *gdev,\n\t\t\t\tstruct vbg_session *session, void *data)\n{\n\tstruct vbg_ioctl_hdr *hdr = data;\n\tint ret;\n\n\tif (hdr->size_in != hdr->size_out)\n\t\treturn -EINVAL;\n\n\tif (hdr->size_in > VMMDEV_MAX_VMMDEVREQ_SIZE)\n\t\treturn -E2BIG;\n\n\tif (hdr->type == VBG_IOCTL_HDR_TYPE_DEFAULT)\n\t\treturn -EINVAL;\n\n\tret = vbg_req_allowed(gdev, session, data);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tvbg_req_perform(gdev, data);\n\tWARN_ON(hdr->rc == VINF_HGCM_ASYNC_EXECUTE);\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_hgcm_connect(struct vbg_dev *gdev,\n\t\t\t\t  struct vbg_session *session,\n\t\t\t\t  struct vbg_ioctl_hgcm_connect *conn)\n{\n\tu32 client_id;\n\tint i, ret;\n\n\tif (vbg_ioctl_chk(&conn->hdr, sizeof(conn->u.in), sizeof(conn->u.out)))\n\t\treturn -EINVAL;\n\n\t \n\tmutex_lock(&gdev->session_mutex);\n\tfor (i = 0; i < ARRAY_SIZE(session->hgcm_client_ids); i++) {\n\t\tif (!session->hgcm_client_ids[i]) {\n\t\t\tsession->hgcm_client_ids[i] = U32_MAX;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&gdev->session_mutex);\n\n\tif (i >= ARRAY_SIZE(session->hgcm_client_ids))\n\t\treturn -EMFILE;\n\n\tret = vbg_hgcm_connect(gdev, session->requestor, &conn->u.in.loc,\n\t\t\t       &client_id, &conn->hdr.rc);\n\n\tmutex_lock(&gdev->session_mutex);\n\tif (ret == 0 && conn->hdr.rc >= 0) {\n\t\tconn->u.out.client_id = client_id;\n\t\tsession->hgcm_client_ids[i] = client_id;\n\t} else {\n\t\tconn->u.out.client_id = 0;\n\t\tsession->hgcm_client_ids[i] = 0;\n\t}\n\tmutex_unlock(&gdev->session_mutex);\n\n\treturn ret;\n}\n\nstatic int vbg_ioctl_hgcm_disconnect(struct vbg_dev *gdev,\n\t\t\t\t     struct vbg_session *session,\n\t\t\t\t     struct vbg_ioctl_hgcm_disconnect *disconn)\n{\n\tu32 client_id;\n\tint i, ret;\n\n\tif (vbg_ioctl_chk(&disconn->hdr, sizeof(disconn->u.in), 0))\n\t\treturn -EINVAL;\n\n\tclient_id = disconn->u.in.client_id;\n\tif (client_id == 0 || client_id == U32_MAX)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&gdev->session_mutex);\n\tfor (i = 0; i < ARRAY_SIZE(session->hgcm_client_ids); i++) {\n\t\tif (session->hgcm_client_ids[i] == client_id) {\n\t\t\tsession->hgcm_client_ids[i] = U32_MAX;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&gdev->session_mutex);\n\n\tif (i >= ARRAY_SIZE(session->hgcm_client_ids))\n\t\treturn -EINVAL;\n\n\tret = vbg_hgcm_disconnect(gdev, session->requestor, client_id,\n\t\t\t\t  &disconn->hdr.rc);\n\n\tmutex_lock(&gdev->session_mutex);\n\tif (ret == 0 && disconn->hdr.rc >= 0)\n\t\tsession->hgcm_client_ids[i] = 0;\n\telse\n\t\tsession->hgcm_client_ids[i] = client_id;\n\tmutex_unlock(&gdev->session_mutex);\n\n\treturn ret;\n}\n\nstatic bool vbg_param_valid(enum vmmdev_hgcm_function_parameter_type type)\n{\n\tswitch (type) {\n\tcase VMMDEV_HGCM_PARM_TYPE_32BIT:\n\tcase VMMDEV_HGCM_PARM_TYPE_64BIT:\n\tcase VMMDEV_HGCM_PARM_TYPE_LINADDR:\n\tcase VMMDEV_HGCM_PARM_TYPE_LINADDR_IN:\n\tcase VMMDEV_HGCM_PARM_TYPE_LINADDR_OUT:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int vbg_ioctl_hgcm_call(struct vbg_dev *gdev,\n\t\t\t       struct vbg_session *session, bool f32bit,\n\t\t\t       struct vbg_ioctl_hgcm_call *call)\n{\n\tsize_t actual_size;\n\tu32 client_id;\n\tint i, ret;\n\n\tif (call->hdr.size_in < sizeof(*call))\n\t\treturn -EINVAL;\n\n\tif (call->hdr.size_in != call->hdr.size_out)\n\t\treturn -EINVAL;\n\n\tif (call->parm_count > VMMDEV_HGCM_MAX_PARMS)\n\t\treturn -E2BIG;\n\n\tclient_id = call->client_id;\n\tif (client_id == 0 || client_id == U32_MAX)\n\t\treturn -EINVAL;\n\n\tactual_size = sizeof(*call);\n\tif (f32bit)\n\t\tactual_size += call->parm_count *\n\t\t\t       sizeof(struct vmmdev_hgcm_function_parameter32);\n\telse\n\t\tactual_size += call->parm_count *\n\t\t\t       sizeof(struct vmmdev_hgcm_function_parameter);\n\tif (call->hdr.size_in < actual_size) {\n\t\tvbg_debug(\"VBG_IOCTL_HGCM_CALL: hdr.size_in %d required size is %zd\\n\",\n\t\t\t  call->hdr.size_in, actual_size);\n\t\treturn -EINVAL;\n\t}\n\tcall->hdr.size_out = actual_size;\n\n\t \n\tif (f32bit) {\n\t\tstruct vmmdev_hgcm_function_parameter32 *parm =\n\t\t\tVBG_IOCTL_HGCM_CALL_PARMS32(call);\n\n\t\tfor (i = 0; i < call->parm_count; i++)\n\t\t\tif (!vbg_param_valid(parm[i].type))\n\t\t\t\treturn -EINVAL;\n\t} else {\n\t\tstruct vmmdev_hgcm_function_parameter *parm =\n\t\t\tVBG_IOCTL_HGCM_CALL_PARMS(call);\n\n\t\tfor (i = 0; i < call->parm_count; i++)\n\t\t\tif (!vbg_param_valid(parm[i].type))\n\t\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tmutex_lock(&gdev->session_mutex);\n\tfor (i = 0; i < ARRAY_SIZE(session->hgcm_client_ids); i++)\n\t\tif (session->hgcm_client_ids[i] == client_id)\n\t\t\tbreak;\n\tmutex_unlock(&gdev->session_mutex);\n\tif (i >= ARRAY_SIZE(session->hgcm_client_ids)) {\n\t\tvbg_debug(\"VBG_IOCTL_HGCM_CALL: INVALID handle. u32Client=%#08x\\n\",\n\t\t\t  client_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (IS_ENABLED(CONFIG_COMPAT) && f32bit)\n\t\tret = vbg_hgcm_call32(gdev, session->requestor, client_id,\n\t\t\t\t      call->function, call->timeout_ms,\n\t\t\t\t      VBG_IOCTL_HGCM_CALL_PARMS32(call),\n\t\t\t\t      call->parm_count, &call->hdr.rc);\n\telse\n\t\tret = vbg_hgcm_call(gdev, session->requestor, client_id,\n\t\t\t\t    call->function, call->timeout_ms,\n\t\t\t\t    VBG_IOCTL_HGCM_CALL_PARMS(call),\n\t\t\t\t    call->parm_count, &call->hdr.rc);\n\n\tif (ret == -E2BIG) {\n\t\t \n\t\tcall->hdr.rc = VERR_OUT_OF_RANGE;\n\t\tret = 0;\n\t}\n\n\tif (ret && ret != -EINTR && ret != -ETIMEDOUT)\n\t\tvbg_err(\"VBG_IOCTL_HGCM_CALL error: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int vbg_ioctl_log(struct vbg_ioctl_log *log)\n{\n\tif (log->hdr.size_out != sizeof(log->hdr))\n\t\treturn -EINVAL;\n\n\tvbg_info(\"%.*s\", (int)(log->hdr.size_in - sizeof(log->hdr)),\n\t\t log->u.in.msg);\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_change_filter_mask(struct vbg_dev *gdev,\n\t\t\t\t\tstruct vbg_session *session,\n\t\t\t\t\tstruct vbg_ioctl_change_filter *filter)\n{\n\tu32 or_mask, not_mask;\n\n\tif (vbg_ioctl_chk(&filter->hdr, sizeof(filter->u.in), 0))\n\t\treturn -EINVAL;\n\n\tor_mask = filter->u.in.or_mask;\n\tnot_mask = filter->u.in.not_mask;\n\n\tif ((or_mask | not_mask) & ~VMMDEV_EVENT_VALID_EVENT_MASK)\n\t\treturn -EINVAL;\n\n\treturn vbg_set_session_event_filter(gdev, session, or_mask, not_mask,\n\t\t\t\t\t    false);\n}\n\nstatic int vbg_ioctl_acquire_guest_capabilities(struct vbg_dev *gdev,\n\t     struct vbg_session *session,\n\t     struct vbg_ioctl_acquire_guest_caps *caps)\n{\n\tu32 flags, or_mask, not_mask;\n\n\tif (vbg_ioctl_chk(&caps->hdr, sizeof(caps->u.in), 0))\n\t\treturn -EINVAL;\n\n\tflags = caps->u.in.flags;\n\tor_mask = caps->u.in.or_mask;\n\tnot_mask = caps->u.in.not_mask;\n\n\tif (flags & ~VBGL_IOC_AGC_FLAGS_VALID_MASK)\n\t\treturn -EINVAL;\n\n\tif ((or_mask | not_mask) & ~VMMDEV_GUEST_CAPABILITIES_MASK)\n\t\treturn -EINVAL;\n\n\treturn vbg_acquire_session_capabilities(gdev, session, or_mask,\n\t\t\t\t\t\tnot_mask, flags, false);\n}\n\nstatic int vbg_ioctl_change_guest_capabilities(struct vbg_dev *gdev,\n\t     struct vbg_session *session, struct vbg_ioctl_set_guest_caps *caps)\n{\n\tu32 or_mask, not_mask;\n\tint ret;\n\n\tif (vbg_ioctl_chk(&caps->hdr, sizeof(caps->u.in), sizeof(caps->u.out)))\n\t\treturn -EINVAL;\n\n\tor_mask = caps->u.in.or_mask;\n\tnot_mask = caps->u.in.not_mask;\n\n\tif ((or_mask | not_mask) & ~VMMDEV_GUEST_CAPABILITIES_MASK)\n\t\treturn -EINVAL;\n\n\tret = vbg_set_session_capabilities(gdev, session, or_mask, not_mask,\n\t\t\t\t\t   false);\n\tif (ret)\n\t\treturn ret;\n\n\tcaps->u.out.session_caps = session->set_guest_caps;\n\tcaps->u.out.global_caps = gdev->guest_caps_host;\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_check_balloon(struct vbg_dev *gdev,\n\t\t\t\t   struct vbg_ioctl_check_balloon *balloon_info)\n{\n\tif (vbg_ioctl_chk(&balloon_info->hdr, 0, sizeof(balloon_info->u.out)))\n\t\treturn -EINVAL;\n\n\tballoon_info->u.out.balloon_chunks = gdev->mem_balloon.chunks;\n\t \n\tballoon_info->u.out.handle_in_r3 = false;\n\n\treturn 0;\n}\n\nstatic int vbg_ioctl_write_core_dump(struct vbg_dev *gdev,\n\t\t\t\t     struct vbg_session *session,\n\t\t\t\t     struct vbg_ioctl_write_coredump *dump)\n{\n\tstruct vmmdev_write_core_dump *req;\n\n\tif (vbg_ioctl_chk(&dump->hdr, sizeof(dump->u.in), 0))\n\t\treturn -EINVAL;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_WRITE_COREDUMP,\n\t\t\t    session->requestor);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->flags = dump->u.in.flags;\n\tdump->hdr.rc = vbg_req_perform(gdev, req);\n\n\tvbg_req_free(req, sizeof(*req));\n\treturn 0;\n}\n\n \nint vbg_core_ioctl(struct vbg_session *session, unsigned int req, void *data)\n{\n\tunsigned int req_no_size = req & ~IOCSIZE_MASK;\n\tstruct vbg_dev *gdev = session->gdev;\n\tstruct vbg_ioctl_hdr *hdr = data;\n\tbool f32bit = false;\n\n\thdr->rc = VINF_SUCCESS;\n\tif (!hdr->size_out)\n\t\thdr->size_out = hdr->size_in;\n\n\t \n\n\t \n\tif (req_no_size == VBG_IOCTL_VMMDEV_REQUEST(0) ||\n\t    req == VBG_IOCTL_VMMDEV_REQUEST_BIG ||\n\t    req == VBG_IOCTL_VMMDEV_REQUEST_BIG_ALT)\n\t\treturn vbg_ioctl_vmmrequest(gdev, session, data);\n\n\tif (hdr->type != VBG_IOCTL_HDR_TYPE_DEFAULT)\n\t\treturn -EINVAL;\n\n\t \n\tswitch (req) {\n\tcase VBG_IOCTL_DRIVER_VERSION_INFO:\n\t\treturn vbg_ioctl_driver_version_info(data);\n\tcase VBG_IOCTL_HGCM_CONNECT:\n\t\treturn vbg_ioctl_hgcm_connect(gdev, session, data);\n\tcase VBG_IOCTL_HGCM_DISCONNECT:\n\t\treturn vbg_ioctl_hgcm_disconnect(gdev, session, data);\n\tcase VBG_IOCTL_WAIT_FOR_EVENTS:\n\t\treturn vbg_ioctl_wait_for_events(gdev, session, data);\n\tcase VBG_IOCTL_INTERRUPT_ALL_WAIT_FOR_EVENTS:\n\t\treturn vbg_ioctl_interrupt_all_wait_events(gdev, session, data);\n\tcase VBG_IOCTL_CHANGE_FILTER_MASK:\n\t\treturn vbg_ioctl_change_filter_mask(gdev, session, data);\n\tcase VBG_IOCTL_ACQUIRE_GUEST_CAPABILITIES:\n\t\treturn vbg_ioctl_acquire_guest_capabilities(gdev, session, data);\n\tcase VBG_IOCTL_CHANGE_GUEST_CAPABILITIES:\n\t\treturn vbg_ioctl_change_guest_capabilities(gdev, session, data);\n\tcase VBG_IOCTL_CHECK_BALLOON:\n\t\treturn vbg_ioctl_check_balloon(gdev, data);\n\tcase VBG_IOCTL_WRITE_CORE_DUMP:\n\t\treturn vbg_ioctl_write_core_dump(gdev, session, data);\n\t}\n\n\t \n\tswitch (req_no_size) {\n#ifdef CONFIG_COMPAT\n\tcase VBG_IOCTL_HGCM_CALL_32(0):\n\t\tf32bit = true;\n\t\tfallthrough;\n#endif\n\tcase VBG_IOCTL_HGCM_CALL(0):\n\t\treturn vbg_ioctl_hgcm_call(gdev, session, f32bit, data);\n\tcase VBG_IOCTL_LOG(0):\n\tcase VBG_IOCTL_LOG_ALT(0):\n\t\treturn vbg_ioctl_log(data);\n\t}\n\n\tvbg_err_ratelimited(\"Userspace made an unknown ioctl req %#08x\\n\", req);\n\treturn -ENOTTY;\n}\n\n \nint vbg_core_set_mouse_status(struct vbg_dev *gdev, u32 features)\n{\n\tstruct vmmdev_mouse_status *req;\n\tint rc;\n\n\treq = vbg_req_alloc(sizeof(*req), VMMDEVREQ_SET_MOUSE_STATUS,\n\t\t\t    VBG_KERNEL_REQUEST);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\treq->mouse_features = features;\n\treq->pointer_pos_x = 0;\n\treq->pointer_pos_y = 0;\n\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0)\n\t\tvbg_err(\"%s error, rc: %d\\n\", __func__, rc);\n\n\tvbg_req_free(req, sizeof(*req));\n\treturn vbg_status_code_to_errno(rc);\n}\n\n \nirqreturn_t vbg_core_isr(int irq, void *dev_id)\n{\n\tstruct vbg_dev *gdev = dev_id;\n\tstruct vmmdev_events *req = gdev->ack_events_req;\n\tbool mouse_position_changed = false;\n\tunsigned long flags;\n\tu32 events = 0;\n\tint rc;\n\n\tif (!gdev->mmio->V.V1_04.have_events)\n\t\treturn IRQ_NONE;\n\n\t \n\treq->header.rc = VERR_INTERNAL_ERROR;\n\treq->events = 0;\n\trc = vbg_req_perform(gdev, req);\n\tif (rc < 0) {\n\t\tvbg_err(\"Error performing events req, rc: %d\\n\", rc);\n\t\treturn IRQ_NONE;\n\t}\n\n\tevents = req->events;\n\n\tif (events & VMMDEV_EVENT_MOUSE_POSITION_CHANGED) {\n\t\tmouse_position_changed = true;\n\t\tevents &= ~VMMDEV_EVENT_MOUSE_POSITION_CHANGED;\n\t}\n\n\tif (events & VMMDEV_EVENT_HGCM) {\n\t\twake_up(&gdev->hgcm_wq);\n\t\tevents &= ~VMMDEV_EVENT_HGCM;\n\t}\n\n\tif (events & VMMDEV_EVENT_BALLOON_CHANGE_REQUEST) {\n\t\tschedule_work(&gdev->mem_balloon.work);\n\t\tevents &= ~VMMDEV_EVENT_BALLOON_CHANGE_REQUEST;\n\t}\n\n\tif (events) {\n\t\tspin_lock_irqsave(&gdev->event_spinlock, flags);\n\t\tgdev->pending_events |= events;\n\t\tspin_unlock_irqrestore(&gdev->event_spinlock, flags);\n\n\t\twake_up(&gdev->event_wq);\n\t}\n\n\tif (mouse_position_changed)\n\t\tvbg_linux_mouse_event(gdev);\n\n\treturn IRQ_HANDLED;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}