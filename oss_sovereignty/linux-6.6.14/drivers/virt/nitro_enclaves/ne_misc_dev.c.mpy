{
  "module_name": "ne_misc_dev.c",
  "hash_id": "4e25d5e4846703ccdb7de32f4c6a523908dd464301d09ba1f95c9da770b937d4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/virt/nitro_enclaves/ne_misc_dev.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/anon_inodes.h>\n#include <linux/capability.h>\n#include <linux/cpu.h>\n#include <linux/device.h>\n#include <linux/file.h>\n#include <linux/hugetlb.h>\n#include <linux/limits.h>\n#include <linux/list.h>\n#include <linux/miscdevice.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/nitro_enclaves.h>\n#include <linux/pci.h>\n#include <linux/poll.h>\n#include <linux/range.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <uapi/linux/vm_sockets.h>\n\n#include \"ne_misc_dev.h\"\n#include \"ne_pci_dev.h\"\n\n \n#define NE_CPUS_SIZE\t\t(512)\n\n \n#define NE_EIF_LOAD_OFFSET\t(8 * 1024UL * 1024UL)\n\n \n#define NE_MIN_ENCLAVE_MEM_SIZE\t(64 * 1024UL * 1024UL)\n\n \n#define NE_MIN_MEM_REGION_SIZE\t(2 * 1024UL * 1024UL)\n\n \n#define NE_PARENT_VM_CID\t(3)\n\nstatic long ne_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\n\nstatic const struct file_operations ne_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= noop_llseek,\n\t.unlocked_ioctl\t= ne_ioctl,\n};\n\nstatic struct miscdevice ne_misc_dev = {\n\t.minor\t= MISC_DYNAMIC_MINOR,\n\t.name\t= \"nitro_enclaves\",\n\t.fops\t= &ne_fops,\n\t.mode\t= 0660,\n};\n\nstruct ne_devs ne_devs = {\n\t.ne_misc_dev\t= &ne_misc_dev,\n};\n\n \nstatic int ne_set_kernel_param(const char *val, const struct kernel_param *kp);\n\nstatic const struct kernel_param_ops ne_cpu_pool_ops = {\n\t.get\t= param_get_string,\n\t.set\t= ne_set_kernel_param,\n};\n\nstatic char ne_cpus[NE_CPUS_SIZE];\nstatic struct kparam_string ne_cpus_arg = {\n\t.maxlen\t= sizeof(ne_cpus),\n\t.string\t= ne_cpus,\n};\n\nmodule_param_cb(ne_cpus, &ne_cpu_pool_ops, &ne_cpus_arg, 0644);\n \nstruct ne_cpu_pool {\n\tcpumask_var_t\t*avail_threads_per_core;\n\tstruct mutex\tmutex;\n\tunsigned int\tnr_parent_vm_cores;\n\tunsigned int\tnr_threads_per_core;\n\tint\t\tnuma_node;\n};\n\nstatic struct ne_cpu_pool ne_cpu_pool;\n\n \nstruct ne_phys_contig_mem_regions {\n\tunsigned long num;\n\tstruct range  *regions;\n};\n\n \nstatic bool ne_check_enclaves_created(void)\n{\n\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n\tbool ret = false;\n\n\tif (!ne_pci_dev)\n\t\treturn ret;\n\n\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n\n\tif (!list_empty(&ne_pci_dev->enclaves_list))\n\t\tret = true;\n\n\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\treturn ret;\n}\n\n \nstatic int ne_setup_cpu_pool(const char *ne_cpu_list)\n{\n\tint core_id = -1;\n\tunsigned int cpu = 0;\n\tcpumask_var_t cpu_pool;\n\tunsigned int cpu_sibling = 0;\n\tunsigned int i = 0;\n\tint numa_node = -1;\n\tint rc = -EINVAL;\n\n\tif (!zalloc_cpumask_var(&cpu_pool, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\trc = cpulist_parse(ne_cpu_list, cpu_pool);\n\tif (rc < 0) {\n\t\tpr_err(\"%s: Error in cpulist parse [rc=%d]\\n\", ne_misc_dev.name, rc);\n\n\t\tgoto free_pool_cpumask;\n\t}\n\n\tcpu = cpumask_any(cpu_pool);\n\tif (cpu >= nr_cpu_ids) {\n\t\tpr_err(\"%s: No CPUs available in CPU pool\\n\", ne_misc_dev.name);\n\n\t\trc = -EINVAL;\n\n\t\tgoto free_pool_cpumask;\n\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_pool)\n\t\tif (cpu_is_offline(cpu)) {\n\t\t\tpr_err(\"%s: CPU %d is offline, has to be online to get its metadata\\n\",\n\t\t\t       ne_misc_dev.name, cpu);\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto free_pool_cpumask;\n\t\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_pool)\n\t\tif (numa_node < 0) {\n\t\t\tnuma_node = cpu_to_node(cpu);\n\t\t\tif (numa_node < 0) {\n\t\t\t\tpr_err(\"%s: Invalid NUMA node %d\\n\",\n\t\t\t\t       ne_misc_dev.name, numa_node);\n\n\t\t\t\trc = -EINVAL;\n\n\t\t\t\tgoto free_pool_cpumask;\n\t\t\t}\n\t\t} else {\n\t\t\tif (numa_node != cpu_to_node(cpu)) {\n\t\t\t\tpr_err(\"%s: CPUs with different NUMA nodes\\n\",\n\t\t\t\t       ne_misc_dev.name);\n\n\t\t\t\trc = -EINVAL;\n\n\t\t\t\tgoto free_pool_cpumask;\n\t\t\t}\n\t\t}\n\n\t \n\tif (cpumask_test_cpu(0, cpu_pool)) {\n\t\tpr_err(\"%s: CPU 0 has to remain available\\n\", ne_misc_dev.name);\n\n\t\trc = -EINVAL;\n\n\t\tgoto free_pool_cpumask;\n\t}\n\n\tfor_each_cpu(cpu_sibling, topology_sibling_cpumask(0)) {\n\t\tif (cpumask_test_cpu(cpu_sibling, cpu_pool)) {\n\t\t\tpr_err(\"%s: CPU sibling %d for CPU 0 is in CPU pool\\n\",\n\t\t\t       ne_misc_dev.name, cpu_sibling);\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto free_pool_cpumask;\n\t\t}\n\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_pool) {\n\t\tfor_each_cpu(cpu_sibling, topology_sibling_cpumask(cpu)) {\n\t\t\tif (!cpumask_test_cpu(cpu_sibling, cpu_pool)) {\n\t\t\t\tpr_err(\"%s: CPU %d is not in CPU pool\\n\",\n\t\t\t\t       ne_misc_dev.name, cpu_sibling);\n\n\t\t\t\trc = -EINVAL;\n\n\t\t\t\tgoto free_pool_cpumask;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tcpu = cpumask_any(cpu_pool);\n\tfor_each_cpu(cpu_sibling, topology_sibling_cpumask(cpu))\n\t\tne_cpu_pool.nr_threads_per_core++;\n\n\tne_cpu_pool.nr_parent_vm_cores = nr_cpu_ids / ne_cpu_pool.nr_threads_per_core;\n\n\tne_cpu_pool.avail_threads_per_core = kcalloc(ne_cpu_pool.nr_parent_vm_cores,\n\t\t\t\t\t\t     sizeof(*ne_cpu_pool.avail_threads_per_core),\n\t\t\t\t\t\t     GFP_KERNEL);\n\tif (!ne_cpu_pool.avail_threads_per_core) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_pool_cpumask;\n\t}\n\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tif (!zalloc_cpumask_var(&ne_cpu_pool.avail_threads_per_core[i], GFP_KERNEL)) {\n\t\t\trc = -ENOMEM;\n\n\t\t\tgoto free_cores_cpumask;\n\t\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_pool) {\n\t\tcore_id = topology_core_id(cpu);\n\t\tif (core_id < 0 || core_id >= ne_cpu_pool.nr_parent_vm_cores) {\n\t\t\tpr_err(\"%s: Invalid core id  %d for CPU %d\\n\",\n\t\t\t       ne_misc_dev.name, core_id, cpu);\n\n\t\t\trc = -EINVAL;\n\n\t\t\tgoto clear_cpumask;\n\t\t}\n\n\t\tcpumask_set_cpu(cpu, ne_cpu_pool.avail_threads_per_core[core_id]);\n\t}\n\n\t \n\tfor_each_cpu(cpu, cpu_pool) {\n\t\trc = remove_cpu(cpu);\n\t\tif (rc != 0) {\n\t\t\tpr_err(\"%s: CPU %d is not offlined [rc=%d]\\n\",\n\t\t\t       ne_misc_dev.name, cpu, rc);\n\n\t\t\tgoto online_cpus;\n\t\t}\n\t}\n\n\tfree_cpumask_var(cpu_pool);\n\n\tne_cpu_pool.numa_node = numa_node;\n\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\treturn 0;\n\nonline_cpus:\n\tfor_each_cpu(cpu, cpu_pool)\n\t\tadd_cpu(cpu);\nclear_cpumask:\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tcpumask_clear(ne_cpu_pool.avail_threads_per_core[i]);\nfree_cores_cpumask:\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tfree_cpumask_var(ne_cpu_pool.avail_threads_per_core[i]);\n\tkfree(ne_cpu_pool.avail_threads_per_core);\nfree_pool_cpumask:\n\tfree_cpumask_var(cpu_pool);\n\tne_cpu_pool.nr_parent_vm_cores = 0;\n\tne_cpu_pool.nr_threads_per_core = 0;\n\tne_cpu_pool.numa_node = -1;\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\treturn rc;\n}\n\n \nstatic void ne_teardown_cpu_pool(void)\n{\n\tunsigned int cpu = 0;\n\tunsigned int i = 0;\n\tint rc = -EINVAL;\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\tif (!ne_cpu_pool.nr_parent_vm_cores) {\n\t\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++) {\n\t\tfor_each_cpu(cpu, ne_cpu_pool.avail_threads_per_core[i]) {\n\t\t\trc = add_cpu(cpu);\n\t\t\tif (rc != 0)\n\t\t\t\tpr_err(\"%s: CPU %d is not onlined [rc=%d]\\n\",\n\t\t\t\t       ne_misc_dev.name, cpu, rc);\n\t\t}\n\n\t\tcpumask_clear(ne_cpu_pool.avail_threads_per_core[i]);\n\n\t\tfree_cpumask_var(ne_cpu_pool.avail_threads_per_core[i]);\n\t}\n\n\tkfree(ne_cpu_pool.avail_threads_per_core);\n\tne_cpu_pool.nr_parent_vm_cores = 0;\n\tne_cpu_pool.nr_threads_per_core = 0;\n\tne_cpu_pool.numa_node = -1;\n\n\tmutex_unlock(&ne_cpu_pool.mutex);\n}\n\n \nstatic int ne_set_kernel_param(const char *val, const struct kernel_param *kp)\n{\n\tchar error_val[] = \"\";\n\tint rc = -EINVAL;\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EPERM;\n\n\tif (ne_check_enclaves_created()) {\n\t\tpr_err(\"%s: The CPU pool is used by enclave(s)\\n\", ne_misc_dev.name);\n\n\t\treturn -EPERM;\n\t}\n\n\tne_teardown_cpu_pool();\n\n\trc = ne_setup_cpu_pool(val);\n\tif (rc < 0) {\n\t\tpr_err(\"%s: Error in setup CPU pool [rc=%d]\\n\", ne_misc_dev.name, rc);\n\n\t\tparam_set_copystring(error_val, kp);\n\n\t\treturn rc;\n\t}\n\n\trc = param_set_copystring(val, kp);\n\tif (rc < 0) {\n\t\tpr_err(\"%s: Error in param set copystring [rc=%d]\\n\", ne_misc_dev.name, rc);\n\n\t\tne_teardown_cpu_pool();\n\n\t\tparam_set_copystring(error_val, kp);\n\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool ne_donated_cpu(struct ne_enclave *ne_enclave, unsigned int cpu)\n{\n\tif (cpumask_test_cpu(cpu, ne_enclave->vcpu_ids))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic int ne_get_unused_core_from_cpu_pool(void)\n{\n\tint core_id = -1;\n\tunsigned int i = 0;\n\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tif (!cpumask_empty(ne_cpu_pool.avail_threads_per_core[i])) {\n\t\t\tcore_id = i;\n\n\t\t\tbreak;\n\t\t}\n\n\treturn core_id;\n}\n\n \nstatic int ne_set_enclave_threads_per_core(struct ne_enclave *ne_enclave,\n\t\t\t\t\t   int core_id, u32 vcpu_id)\n{\n\tunsigned int cpu = 0;\n\n\tif (core_id < 0 && vcpu_id == 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"No CPUs available in NE CPU pool\\n\");\n\n\t\treturn -NE_ERR_NO_CPUS_AVAIL_IN_POOL;\n\t}\n\n\tif (core_id < 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"CPU %d is not in NE CPU pool\\n\", vcpu_id);\n\n\t\treturn -NE_ERR_VCPU_NOT_IN_CPU_POOL;\n\t}\n\n\tif (core_id >= ne_enclave->nr_parent_vm_cores) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Invalid core id %d - ne_enclave\\n\", core_id);\n\n\t\treturn -NE_ERR_VCPU_INVALID_CPU_CORE;\n\t}\n\n\tfor_each_cpu(cpu, ne_cpu_pool.avail_threads_per_core[core_id])\n\t\tcpumask_set_cpu(cpu, ne_enclave->threads_per_core[core_id]);\n\n\tcpumask_clear(ne_cpu_pool.avail_threads_per_core[core_id]);\n\n\treturn 0;\n}\n\n \nstatic int ne_get_cpu_from_cpu_pool(struct ne_enclave *ne_enclave, u32 *vcpu_id)\n{\n\tint core_id = -1;\n\tunsigned int cpu = 0;\n\tunsigned int i = 0;\n\tint rc = -EINVAL;\n\n\t \n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++)\n\t\tfor_each_cpu(cpu, ne_enclave->threads_per_core[i])\n\t\t\tif (!ne_donated_cpu(ne_enclave, cpu)) {\n\t\t\t\t*vcpu_id = cpu;\n\n\t\t\t\treturn 0;\n\t\t\t}\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\t \n\tcore_id = ne_get_unused_core_from_cpu_pool();\n\n\trc = ne_set_enclave_threads_per_core(ne_enclave, core_id, *vcpu_id);\n\tif (rc < 0)\n\t\tgoto unlock_mutex;\n\n\t*vcpu_id = cpumask_any(ne_enclave->threads_per_core[core_id]);\n\n\trc = 0;\n\nunlock_mutex:\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\treturn rc;\n}\n\n \nstatic int ne_get_vcpu_core_from_cpu_pool(u32 vcpu_id)\n{\n\tint core_id = -1;\n\tunsigned int i = 0;\n\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tif (cpumask_test_cpu(vcpu_id, ne_cpu_pool.avail_threads_per_core[i])) {\n\t\t\tcore_id = i;\n\n\t\t\tbreak;\n\t}\n\n\treturn core_id;\n}\n\n \nstatic int ne_check_cpu_in_cpu_pool(struct ne_enclave *ne_enclave, u32 vcpu_id)\n{\n\tint core_id = -1;\n\tunsigned int i = 0;\n\tint rc = -EINVAL;\n\n\tif (ne_donated_cpu(ne_enclave, vcpu_id)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"CPU %d already used\\n\", vcpu_id);\n\n\t\treturn -NE_ERR_VCPU_ALREADY_USED;\n\t}\n\n\t \n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++)\n\t\tif (cpumask_test_cpu(vcpu_id, ne_enclave->threads_per_core[i]))\n\t\t\treturn 0;\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\t \n\tcore_id = ne_get_vcpu_core_from_cpu_pool(vcpu_id);\n\n\trc = ne_set_enclave_threads_per_core(ne_enclave, core_id, vcpu_id);\n\tif (rc < 0)\n\t\tgoto unlock_mutex;\n\n\trc = 0;\n\nunlock_mutex:\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\treturn rc;\n}\n\n \nstatic int ne_add_vcpu_ioctl(struct ne_enclave *ne_enclave, u32 vcpu_id)\n{\n\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\tstruct pci_dev *pdev = ne_devs.ne_pci_dev->pdev;\n\tint rc = -EINVAL;\n\tstruct slot_add_vcpu_req slot_add_vcpu_req = {};\n\n\tif (ne_enclave->mm != current->mm)\n\t\treturn -EIO;\n\n\tslot_add_vcpu_req.slot_uid = ne_enclave->slot_uid;\n\tslot_add_vcpu_req.vcpu_id = vcpu_id;\n\n\trc = ne_do_request(pdev, SLOT_ADD_VCPU,\n\t\t\t   &slot_add_vcpu_req, sizeof(slot_add_vcpu_req),\n\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\tif (rc < 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in slot add vCPU [rc=%d]\\n\", rc);\n\n\t\treturn rc;\n\t}\n\n\tcpumask_set_cpu(vcpu_id, ne_enclave->vcpu_ids);\n\n\tne_enclave->nr_vcpus++;\n\n\treturn 0;\n}\n\n \nstatic int ne_sanity_check_user_mem_region(struct ne_enclave *ne_enclave,\n\t\t\t\t\t   struct ne_user_memory_region mem_region)\n{\n\tstruct ne_mem_region *ne_mem_region = NULL;\n\n\tif (ne_enclave->mm != current->mm)\n\t\treturn -EIO;\n\n\tif (mem_region.memory_size & (NE_MIN_MEM_REGION_SIZE - 1)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"User space memory size is not multiple of 2 MiB\\n\");\n\n\t\treturn -NE_ERR_INVALID_MEM_REGION_SIZE;\n\t}\n\n\tif (!IS_ALIGNED(mem_region.userspace_addr, NE_MIN_MEM_REGION_SIZE)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"User space address is not 2 MiB aligned\\n\");\n\n\t\treturn -NE_ERR_UNALIGNED_MEM_REGION_ADDR;\n\t}\n\n\tif ((mem_region.userspace_addr & (NE_MIN_MEM_REGION_SIZE - 1)) ||\n\t    !access_ok((void __user *)(unsigned long)mem_region.userspace_addr,\n\t\t       mem_region.memory_size)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Invalid user space address range\\n\");\n\n\t\treturn -NE_ERR_INVALID_MEM_REGION_ADDR;\n\t}\n\n\tlist_for_each_entry(ne_mem_region, &ne_enclave->mem_regions_list,\n\t\t\t    mem_region_list_entry) {\n\t\tu64 memory_size = ne_mem_region->memory_size;\n\t\tu64 userspace_addr = ne_mem_region->userspace_addr;\n\n\t\tif ((userspace_addr <= mem_region.userspace_addr &&\n\t\t     mem_region.userspace_addr < (userspace_addr + memory_size)) ||\n\t\t    (mem_region.userspace_addr <= userspace_addr &&\n\t\t    (mem_region.userspace_addr + mem_region.memory_size) > userspace_addr)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"User space memory region already used\\n\");\n\n\t\t\treturn -NE_ERR_MEM_REGION_ALREADY_USED;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ne_sanity_check_user_mem_region_page(struct ne_enclave *ne_enclave,\n\t\t\t\t\t\tstruct page *mem_region_page)\n{\n\tif (!PageHuge(mem_region_page)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Not a hugetlbfs page\\n\");\n\n\t\treturn -NE_ERR_MEM_NOT_HUGE_PAGE;\n\t}\n\n\tif (page_size(mem_region_page) & (NE_MIN_MEM_REGION_SIZE - 1)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Page size not multiple of 2 MiB\\n\");\n\n\t\treturn -NE_ERR_INVALID_PAGE_SIZE;\n\t}\n\n\tif (ne_enclave->numa_node != page_to_nid(mem_region_page)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Page is not from NUMA node %d\\n\",\n\t\t\t\t    ne_enclave->numa_node);\n\n\t\treturn -NE_ERR_MEM_DIFFERENT_NUMA_NODE;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ne_sanity_check_phys_mem_region(u64 phys_mem_region_paddr,\n\t\t\t\t\t   u64 phys_mem_region_size)\n{\n\tif (phys_mem_region_size & (NE_MIN_MEM_REGION_SIZE - 1)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Physical mem region size is not multiple of 2 MiB\\n\");\n\n\t\treturn -EINVAL;\n\t}\n\n\tif (!IS_ALIGNED(phys_mem_region_paddr, NE_MIN_MEM_REGION_SIZE)) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Physical mem region address is not 2 MiB aligned\\n\");\n\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\nne_merge_phys_contig_memory_regions(struct ne_phys_contig_mem_regions *phys_contig_regions,\n\t\t\t\t    u64 page_paddr, u64 page_size)\n{\n\tunsigned long num = phys_contig_regions->num;\n\tint rc = 0;\n\n\trc = ne_sanity_check_phys_mem_region(page_paddr, page_size);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tif (num && (phys_contig_regions->regions[num - 1].end + 1) == page_paddr) {\n\t\tphys_contig_regions->regions[num - 1].end += page_size;\n\t} else {\n\t\tphys_contig_regions->regions[num].start = page_paddr;\n\t\tphys_contig_regions->regions[num].end = page_paddr + page_size - 1;\n\t\tphys_contig_regions->num++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int ne_set_user_memory_region_ioctl(struct ne_enclave *ne_enclave,\n\t\t\t\t\t   struct ne_user_memory_region mem_region)\n{\n\tlong gup_rc = 0;\n\tunsigned long i = 0;\n\tunsigned long max_nr_pages = 0;\n\tunsigned long memory_size = 0;\n\tstruct ne_mem_region *ne_mem_region = NULL;\n\tstruct pci_dev *pdev = ne_devs.ne_pci_dev->pdev;\n\tstruct ne_phys_contig_mem_regions phys_contig_mem_regions = {};\n\tint rc = -EINVAL;\n\n\trc = ne_sanity_check_user_mem_region(ne_enclave, mem_region);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tne_mem_region = kzalloc(sizeof(*ne_mem_region), GFP_KERNEL);\n\tif (!ne_mem_region)\n\t\treturn -ENOMEM;\n\n\tmax_nr_pages = mem_region.memory_size / NE_MIN_MEM_REGION_SIZE;\n\n\tne_mem_region->pages = kcalloc(max_nr_pages, sizeof(*ne_mem_region->pages),\n\t\t\t\t       GFP_KERNEL);\n\tif (!ne_mem_region->pages) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tphys_contig_mem_regions.regions = kcalloc(max_nr_pages,\n\t\t\t\t\t\t  sizeof(*phys_contig_mem_regions.regions),\n\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!phys_contig_mem_regions.regions) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_mem_region;\n\t}\n\n\tdo {\n\t\ti = ne_mem_region->nr_pages;\n\n\t\tif (i == max_nr_pages) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Reached max nr of pages in the pages data struct\\n\");\n\n\t\t\trc = -ENOMEM;\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\tgup_rc = get_user_pages_unlocked(mem_region.userspace_addr + memory_size, 1,\n\t\t\t\t\t\t ne_mem_region->pages + i, FOLL_GET);\n\n\t\tif (gup_rc < 0) {\n\t\t\trc = gup_rc;\n\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in get user pages [rc=%d]\\n\", rc);\n\n\t\t\tgoto put_pages;\n\t\t}\n\n\t\trc = ne_sanity_check_user_mem_region_page(ne_enclave, ne_mem_region->pages[i]);\n\t\tif (rc < 0)\n\t\t\tgoto put_pages;\n\n\t\trc = ne_merge_phys_contig_memory_regions(&phys_contig_mem_regions,\n\t\t\t\t\t\t\t page_to_phys(ne_mem_region->pages[i]),\n\t\t\t\t\t\t\t page_size(ne_mem_region->pages[i]));\n\t\tif (rc < 0)\n\t\t\tgoto put_pages;\n\n\t\tmemory_size += page_size(ne_mem_region->pages[i]);\n\n\t\tne_mem_region->nr_pages++;\n\t} while (memory_size < mem_region.memory_size);\n\n\tif ((ne_enclave->nr_mem_regions + phys_contig_mem_regions.num) >\n\t    ne_enclave->max_mem_regions) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Reached max memory regions %lld\\n\",\n\t\t\t\t    ne_enclave->max_mem_regions);\n\n\t\trc = -NE_ERR_MEM_MAX_REGIONS;\n\n\t\tgoto put_pages;\n\t}\n\n\tfor (i = 0; i < phys_contig_mem_regions.num; i++) {\n\t\tu64 phys_region_addr = phys_contig_mem_regions.regions[i].start;\n\t\tu64 phys_region_size = range_len(&phys_contig_mem_regions.regions[i]);\n\n\t\trc = ne_sanity_check_phys_mem_region(phys_region_addr, phys_region_size);\n\t\tif (rc < 0)\n\t\t\tgoto put_pages;\n\t}\n\n\tne_mem_region->memory_size = mem_region.memory_size;\n\tne_mem_region->userspace_addr = mem_region.userspace_addr;\n\n\tlist_add(&ne_mem_region->mem_region_list_entry, &ne_enclave->mem_regions_list);\n\n\tfor (i = 0; i < phys_contig_mem_regions.num; i++) {\n\t\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\t\tstruct slot_add_mem_req slot_add_mem_req = {};\n\n\t\tslot_add_mem_req.slot_uid = ne_enclave->slot_uid;\n\t\tslot_add_mem_req.paddr = phys_contig_mem_regions.regions[i].start;\n\t\tslot_add_mem_req.size = range_len(&phys_contig_mem_regions.regions[i]);\n\n\t\trc = ne_do_request(pdev, SLOT_ADD_MEM,\n\t\t\t\t   &slot_add_mem_req, sizeof(slot_add_mem_req),\n\t\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\t\tif (rc < 0) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in slot add mem [rc=%d]\\n\", rc);\n\n\t\t\tkfree(phys_contig_mem_regions.regions);\n\n\t\t\t \n\t\t\treturn rc;\n\t\t}\n\n\t\tne_enclave->mem_size += slot_add_mem_req.size;\n\t\tne_enclave->nr_mem_regions++;\n\t}\n\n\tkfree(phys_contig_mem_regions.regions);\n\n\treturn 0;\n\nput_pages:\n\tfor (i = 0; i < ne_mem_region->nr_pages; i++)\n\t\tput_page(ne_mem_region->pages[i]);\nfree_mem_region:\n\tkfree(phys_contig_mem_regions.regions);\n\tkfree(ne_mem_region->pages);\n\tkfree(ne_mem_region);\n\n\treturn rc;\n}\n\n \nstatic int ne_start_enclave_ioctl(struct ne_enclave *ne_enclave,\n\t\t\t\t  struct ne_enclave_start_info *enclave_start_info)\n{\n\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\tunsigned int cpu = 0;\n\tstruct enclave_start_req enclave_start_req = {};\n\tunsigned int i = 0;\n\tstruct pci_dev *pdev = ne_devs.ne_pci_dev->pdev;\n\tint rc = -EINVAL;\n\n\tif (!ne_enclave->nr_mem_regions) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Enclave has no mem regions\\n\");\n\n\t\treturn -NE_ERR_NO_MEM_REGIONS_ADDED;\n\t}\n\n\tif (ne_enclave->mem_size < NE_MIN_ENCLAVE_MEM_SIZE) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Enclave memory is less than %ld\\n\",\n\t\t\t\t    NE_MIN_ENCLAVE_MEM_SIZE);\n\n\t\treturn -NE_ERR_ENCLAVE_MEM_MIN_SIZE;\n\t}\n\n\tif (!ne_enclave->nr_vcpus) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Enclave has no vCPUs\\n\");\n\n\t\treturn -NE_ERR_NO_VCPUS_ADDED;\n\t}\n\n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++)\n\t\tfor_each_cpu(cpu, ne_enclave->threads_per_core[i])\n\t\t\tif (!cpumask_test_cpu(cpu, ne_enclave->vcpu_ids)) {\n\t\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t\t    \"Full CPU cores not used\\n\");\n\n\t\t\t\treturn -NE_ERR_FULL_CORES_NOT_USED;\n\t\t\t}\n\n\tenclave_start_req.enclave_cid = enclave_start_info->enclave_cid;\n\tenclave_start_req.flags = enclave_start_info->flags;\n\tenclave_start_req.slot_uid = ne_enclave->slot_uid;\n\n\trc = ne_do_request(pdev, ENCLAVE_START,\n\t\t\t   &enclave_start_req, sizeof(enclave_start_req),\n\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\tif (rc < 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in enclave start [rc=%d]\\n\", rc);\n\n\t\treturn rc;\n\t}\n\n\tne_enclave->state = NE_STATE_RUNNING;\n\n\tenclave_start_info->enclave_cid = cmd_reply.enclave_cid;\n\n\treturn 0;\n}\n\n \nstatic long ne_enclave_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct ne_enclave *ne_enclave = file->private_data;\n\n\tswitch (cmd) {\n\tcase NE_ADD_VCPU: {\n\t\tint rc = -EINVAL;\n\t\tu32 vcpu_id = 0;\n\n\t\tif (copy_from_user(&vcpu_id, (void __user *)arg, sizeof(vcpu_id)))\n\t\t\treturn -EFAULT;\n\n\t\tmutex_lock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (ne_enclave->state != NE_STATE_INIT) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Enclave is not in init state\\n\");\n\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn -NE_ERR_NOT_IN_INIT_STATE;\n\t\t}\n\n\t\tif (vcpu_id >= (ne_enclave->nr_parent_vm_cores *\n\t\t    ne_enclave->nr_threads_per_core)) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"vCPU id higher than max CPU id\\n\");\n\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn -NE_ERR_INVALID_VCPU;\n\t\t}\n\n\t\tif (!vcpu_id) {\n\t\t\t \n\t\t\trc = ne_get_cpu_from_cpu_pool(ne_enclave, &vcpu_id);\n\t\t\tif (rc < 0) {\n\t\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t\t    \"Error in get CPU from pool [rc=%d]\\n\",\n\t\t\t\t\t\t    rc);\n\n\t\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\trc = ne_check_cpu_in_cpu_pool(ne_enclave, vcpu_id);\n\t\t\tif (rc < 0) {\n\t\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t\t    \"Error in check CPU %d in pool [rc=%d]\\n\",\n\t\t\t\t\t\t    vcpu_id, rc);\n\n\t\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t}\n\n\t\trc = ne_add_vcpu_ioctl(ne_enclave, vcpu_id);\n\t\tif (rc < 0) {\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn rc;\n\t\t}\n\n\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (copy_to_user((void __user *)arg, &vcpu_id, sizeof(vcpu_id)))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase NE_GET_IMAGE_LOAD_INFO: {\n\t\tstruct ne_image_load_info image_load_info = {};\n\n\t\tif (copy_from_user(&image_load_info, (void __user *)arg, sizeof(image_load_info)))\n\t\t\treturn -EFAULT;\n\n\t\tmutex_lock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (ne_enclave->state != NE_STATE_INIT) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Enclave is not in init state\\n\");\n\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn -NE_ERR_NOT_IN_INIT_STATE;\n\t\t}\n\n\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (!image_load_info.flags ||\n\t\t    image_load_info.flags >= NE_IMAGE_LOAD_MAX_FLAG_VAL) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Incorrect flag in enclave image load info\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_FLAG_VALUE;\n\t\t}\n\n\t\tif (image_load_info.flags == NE_EIF_IMAGE)\n\t\t\timage_load_info.memory_offset = NE_EIF_LOAD_OFFSET;\n\n\t\tif (copy_to_user((void __user *)arg, &image_load_info, sizeof(image_load_info)))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tcase NE_SET_USER_MEMORY_REGION: {\n\t\tstruct ne_user_memory_region mem_region = {};\n\t\tint rc = -EINVAL;\n\n\t\tif (copy_from_user(&mem_region, (void __user *)arg, sizeof(mem_region)))\n\t\t\treturn -EFAULT;\n\n\t\tif (mem_region.flags >= NE_MEMORY_REGION_MAX_FLAG_VAL) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Incorrect flag for user memory region\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_FLAG_VALUE;\n\t\t}\n\n\t\tmutex_lock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (ne_enclave->state != NE_STATE_INIT) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Enclave is not in init state\\n\");\n\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn -NE_ERR_NOT_IN_INIT_STATE;\n\t\t}\n\n\t\trc = ne_set_user_memory_region_ioctl(ne_enclave, mem_region);\n\t\tif (rc < 0) {\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn rc;\n\t\t}\n\n\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\treturn 0;\n\t}\n\n\tcase NE_START_ENCLAVE: {\n\t\tstruct ne_enclave_start_info enclave_start_info = {};\n\t\tint rc = -EINVAL;\n\n\t\tif (copy_from_user(&enclave_start_info, (void __user *)arg,\n\t\t\t\t   sizeof(enclave_start_info)))\n\t\t\treturn -EFAULT;\n\n\t\tif (enclave_start_info.flags >= NE_ENCLAVE_START_MAX_FLAG_VAL) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Incorrect flag in enclave start info\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_FLAG_VALUE;\n\t\t}\n\n\t\t \n\t\tif (enclave_start_info.enclave_cid > 0 &&\n\t\t    enclave_start_info.enclave_cid <= VMADDR_CID_HOST) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Well-known CID value, not to be used for enclaves\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_ENCLAVE_CID;\n\t\t}\n\n\t\tif (enclave_start_info.enclave_cid == U32_MAX) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Well-known CID value, not to be used for enclaves\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_ENCLAVE_CID;\n\t\t}\n\n\t\t \n\t\tif (enclave_start_info.enclave_cid == NE_PARENT_VM_CID) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"CID of the parent VM, not to be used for enclaves\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_ENCLAVE_CID;\n\t\t}\n\n\t\t \n\t\tif (enclave_start_info.enclave_cid > U32_MAX) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"64-bit CIDs not yet supported for the vsock device\\n\");\n\n\t\t\treturn -NE_ERR_INVALID_ENCLAVE_CID;\n\t\t}\n\n\t\tmutex_lock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (ne_enclave->state != NE_STATE_INIT) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Enclave is not in init state\\n\");\n\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn -NE_ERR_NOT_IN_INIT_STATE;\n\t\t}\n\n\t\trc = ne_start_enclave_ioctl(ne_enclave, &enclave_start_info);\n\t\tif (rc < 0) {\n\t\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\t\treturn rc;\n\t\t}\n\n\t\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\n\t\tif (copy_to_user((void __user *)arg, &enclave_start_info,\n\t\t\t\t sizeof(enclave_start_info)))\n\t\t\treturn -EFAULT;\n\n\t\treturn 0;\n\t}\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void ne_enclave_remove_all_mem_region_entries(struct ne_enclave *ne_enclave)\n{\n\tunsigned long i = 0;\n\tstruct ne_mem_region *ne_mem_region = NULL;\n\tstruct ne_mem_region *ne_mem_region_tmp = NULL;\n\n\tlist_for_each_entry_safe(ne_mem_region, ne_mem_region_tmp,\n\t\t\t\t &ne_enclave->mem_regions_list,\n\t\t\t\t mem_region_list_entry) {\n\t\tlist_del(&ne_mem_region->mem_region_list_entry);\n\n\t\tfor (i = 0; i < ne_mem_region->nr_pages; i++)\n\t\t\tput_page(ne_mem_region->pages[i]);\n\n\t\tkfree(ne_mem_region->pages);\n\n\t\tkfree(ne_mem_region);\n\t}\n}\n\n \nstatic void ne_enclave_remove_all_vcpu_id_entries(struct ne_enclave *ne_enclave)\n{\n\tunsigned int cpu = 0;\n\tunsigned int i = 0;\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++) {\n\t\tfor_each_cpu(cpu, ne_enclave->threads_per_core[i])\n\t\t\t \n\t\t\tcpumask_set_cpu(cpu, ne_cpu_pool.avail_threads_per_core[i]);\n\n\t\tfree_cpumask_var(ne_enclave->threads_per_core[i]);\n\t}\n\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\tkfree(ne_enclave->threads_per_core);\n\n\tfree_cpumask_var(ne_enclave->vcpu_ids);\n}\n\n \nstatic void ne_pci_dev_remove_enclave_entry(struct ne_enclave *ne_enclave,\n\t\t\t\t\t    struct ne_pci_dev *ne_pci_dev)\n{\n\tstruct ne_enclave *ne_enclave_entry = NULL;\n\tstruct ne_enclave *ne_enclave_entry_tmp = NULL;\n\n\tlist_for_each_entry_safe(ne_enclave_entry, ne_enclave_entry_tmp,\n\t\t\t\t &ne_pci_dev->enclaves_list, enclave_list_entry) {\n\t\tif (ne_enclave_entry->slot_uid == ne_enclave->slot_uid) {\n\t\t\tlist_del(&ne_enclave_entry->enclave_list_entry);\n\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic int ne_enclave_release(struct inode *inode, struct file *file)\n{\n\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\tstruct enclave_stop_req enclave_stop_request = {};\n\tstruct ne_enclave *ne_enclave = file->private_data;\n\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n\tstruct pci_dev *pdev = ne_pci_dev->pdev;\n\tint rc = -EINVAL;\n\tstruct slot_free_req slot_free_req = {};\n\n\tif (!ne_enclave)\n\t\treturn 0;\n\n\t \n\tif (!ne_enclave->slot_uid)\n\t\treturn 0;\n\n\t \n\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n\tmutex_lock(&ne_enclave->enclave_info_mutex);\n\n\tif (ne_enclave->state != NE_STATE_INIT && ne_enclave->state != NE_STATE_STOPPED) {\n\t\tenclave_stop_request.slot_uid = ne_enclave->slot_uid;\n\n\t\trc = ne_do_request(pdev, ENCLAVE_STOP,\n\t\t\t\t   &enclave_stop_request, sizeof(enclave_stop_request),\n\t\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\t\tif (rc < 0) {\n\t\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t\t    \"Error in enclave stop [rc=%d]\\n\", rc);\n\n\t\t\tgoto unlock_mutex;\n\t\t}\n\n\t\tmemset(&cmd_reply, 0, sizeof(cmd_reply));\n\t}\n\n\tslot_free_req.slot_uid = ne_enclave->slot_uid;\n\n\trc = ne_do_request(pdev, SLOT_FREE,\n\t\t\t   &slot_free_req, sizeof(slot_free_req),\n\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\tif (rc < 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in slot free [rc=%d]\\n\", rc);\n\n\t\tgoto unlock_mutex;\n\t}\n\n\tne_pci_dev_remove_enclave_entry(ne_enclave, ne_pci_dev);\n\tne_enclave_remove_all_mem_region_entries(ne_enclave);\n\tne_enclave_remove_all_vcpu_id_entries(ne_enclave);\n\n\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\tkfree(ne_enclave);\n\n\treturn 0;\n\nunlock_mutex:\n\tmutex_unlock(&ne_enclave->enclave_info_mutex);\n\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\treturn rc;\n}\n\n \nstatic __poll_t ne_enclave_poll(struct file *file, poll_table *wait)\n{\n\t__poll_t mask = 0;\n\tstruct ne_enclave *ne_enclave = file->private_data;\n\n\tpoll_wait(file, &ne_enclave->eventq, wait);\n\n\tif (ne_enclave->has_event)\n\t\tmask |= EPOLLHUP;\n\n\treturn mask;\n}\n\nstatic const struct file_operations ne_enclave_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.llseek\t\t= noop_llseek,\n\t.poll\t\t= ne_enclave_poll,\n\t.unlocked_ioctl\t= ne_enclave_ioctl,\n\t.release\t= ne_enclave_release,\n};\n\n \nstatic int ne_create_vm_ioctl(struct ne_pci_dev *ne_pci_dev, u64 __user *slot_uid)\n{\n\tstruct ne_pci_dev_cmd_reply cmd_reply = {};\n\tint enclave_fd = -1;\n\tstruct file *enclave_file = NULL;\n\tunsigned int i = 0;\n\tstruct ne_enclave *ne_enclave = NULL;\n\tstruct pci_dev *pdev = ne_pci_dev->pdev;\n\tint rc = -EINVAL;\n\tstruct slot_alloc_req slot_alloc_req = {};\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\tfor (i = 0; i < ne_cpu_pool.nr_parent_vm_cores; i++)\n\t\tif (!cpumask_empty(ne_cpu_pool.avail_threads_per_core[i]))\n\t\t\tbreak;\n\n\tif (i == ne_cpu_pool.nr_parent_vm_cores) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"No CPUs available in CPU pool\\n\");\n\n\t\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\t\treturn -NE_ERR_NO_CPUS_AVAIL_IN_POOL;\n\t}\n\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\tne_enclave = kzalloc(sizeof(*ne_enclave), GFP_KERNEL);\n\tif (!ne_enclave)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&ne_cpu_pool.mutex);\n\n\tne_enclave->nr_parent_vm_cores = ne_cpu_pool.nr_parent_vm_cores;\n\tne_enclave->nr_threads_per_core = ne_cpu_pool.nr_threads_per_core;\n\tne_enclave->numa_node = ne_cpu_pool.numa_node;\n\n\tmutex_unlock(&ne_cpu_pool.mutex);\n\n\tne_enclave->threads_per_core = kcalloc(ne_enclave->nr_parent_vm_cores,\n\t\t\t\t\t       sizeof(*ne_enclave->threads_per_core),\n\t\t\t\t\t       GFP_KERNEL);\n\tif (!ne_enclave->threads_per_core) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_ne_enclave;\n\t}\n\n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++)\n\t\tif (!zalloc_cpumask_var(&ne_enclave->threads_per_core[i], GFP_KERNEL)) {\n\t\t\trc = -ENOMEM;\n\n\t\t\tgoto free_cpumask;\n\t\t}\n\n\tif (!zalloc_cpumask_var(&ne_enclave->vcpu_ids, GFP_KERNEL)) {\n\t\trc = -ENOMEM;\n\n\t\tgoto free_cpumask;\n\t}\n\n\tenclave_fd = get_unused_fd_flags(O_CLOEXEC);\n\tif (enclave_fd < 0) {\n\t\trc = enclave_fd;\n\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in getting unused fd [rc=%d]\\n\", rc);\n\n\t\tgoto free_cpumask;\n\t}\n\n\tenclave_file = anon_inode_getfile(\"ne-vm\", &ne_enclave_fops, ne_enclave, O_RDWR);\n\tif (IS_ERR(enclave_file)) {\n\t\trc = PTR_ERR(enclave_file);\n\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in anon inode get file [rc=%d]\\n\", rc);\n\n\t\tgoto put_fd;\n\t}\n\n\trc = ne_do_request(pdev, SLOT_ALLOC,\n\t\t\t   &slot_alloc_req, sizeof(slot_alloc_req),\n\t\t\t   &cmd_reply, sizeof(cmd_reply));\n\tif (rc < 0) {\n\t\tdev_err_ratelimited(ne_misc_dev.this_device,\n\t\t\t\t    \"Error in slot alloc [rc=%d]\\n\", rc);\n\n\t\tgoto put_file;\n\t}\n\n\tinit_waitqueue_head(&ne_enclave->eventq);\n\tne_enclave->has_event = false;\n\tmutex_init(&ne_enclave->enclave_info_mutex);\n\tne_enclave->max_mem_regions = cmd_reply.mem_regions;\n\tINIT_LIST_HEAD(&ne_enclave->mem_regions_list);\n\tne_enclave->mm = current->mm;\n\tne_enclave->slot_uid = cmd_reply.slot_uid;\n\tne_enclave->state = NE_STATE_INIT;\n\n\tlist_add(&ne_enclave->enclave_list_entry, &ne_pci_dev->enclaves_list);\n\n\tif (copy_to_user(slot_uid, &ne_enclave->slot_uid, sizeof(ne_enclave->slot_uid))) {\n\t\t \n\t\tfput(enclave_file);\n\t\tput_unused_fd(enclave_fd);\n\n\t\treturn -EFAULT;\n\t}\n\n\tfd_install(enclave_fd, enclave_file);\n\n\treturn enclave_fd;\n\nput_file:\n\tfput(enclave_file);\nput_fd:\n\tput_unused_fd(enclave_fd);\nfree_cpumask:\n\tfree_cpumask_var(ne_enclave->vcpu_ids);\n\tfor (i = 0; i < ne_enclave->nr_parent_vm_cores; i++)\n\t\tfree_cpumask_var(ne_enclave->threads_per_core[i]);\n\tkfree(ne_enclave->threads_per_core);\nfree_ne_enclave:\n\tkfree(ne_enclave);\n\n\treturn rc;\n}\n\n \nstatic long ne_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase NE_CREATE_VM: {\n\t\tint enclave_fd = -1;\n\t\tstruct ne_pci_dev *ne_pci_dev = ne_devs.ne_pci_dev;\n\t\tu64 __user *slot_uid = (void __user *)arg;\n\n\t\tmutex_lock(&ne_pci_dev->enclaves_list_mutex);\n\t\tenclave_fd = ne_create_vm_ioctl(ne_pci_dev, slot_uid);\n\t\tmutex_unlock(&ne_pci_dev->enclaves_list_mutex);\n\n\t\treturn enclave_fd;\n\t}\n\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n\n\treturn 0;\n}\n\n#if defined(CONFIG_NITRO_ENCLAVES_MISC_DEV_TEST)\n#include \"ne_misc_dev_test.c\"\n#endif\n\nstatic int __init ne_init(void)\n{\n\tmutex_init(&ne_cpu_pool.mutex);\n\n\treturn pci_register_driver(&ne_pci_driver);\n}\n\nstatic void __exit ne_exit(void)\n{\n\tpci_unregister_driver(&ne_pci_driver);\n\n\tne_teardown_cpu_pool();\n}\n\nmodule_init(ne_init);\nmodule_exit(ne_exit);\n\nMODULE_AUTHOR(\"Amazon.com, Inc. or its affiliates\");\nMODULE_DESCRIPTION(\"Nitro Enclaves Driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}