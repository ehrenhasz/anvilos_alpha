{
  "module_name": "ivpu_mmu.c",
  "hash_id": "344d239083a96eb20ddfe57780f2c055510834d14fdb683ca4471985acbcc858",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/ivpu/ivpu_mmu.c",
  "human_readable_source": "\n \n\n#include <linux/circ_buf.h>\n#include <linux/highmem.h>\n\n#include \"ivpu_drv.h\"\n#include \"ivpu_hw_37xx_reg.h\"\n#include \"ivpu_hw_reg_io.h\"\n#include \"ivpu_mmu.h\"\n#include \"ivpu_mmu_context.h\"\n#include \"ivpu_pm.h\"\n\n#define IVPU_MMU_IDR0_REF\t\t0x080f3e0f\n#define IVPU_MMU_IDR0_REF_SIMICS\t0x080f3e1f\n#define IVPU_MMU_IDR1_REF\t\t0x0e739d18\n#define IVPU_MMU_IDR3_REF\t\t0x0000003c\n#define IVPU_MMU_IDR5_REF\t\t0x00040070\n#define IVPU_MMU_IDR5_REF_SIMICS\t0x00000075\n#define IVPU_MMU_IDR5_REF_FPGA\t\t0x00800075\n\n#define IVPU_MMU_CDTAB_ENT_SIZE\t\t64\n#define IVPU_MMU_CDTAB_ENT_COUNT_LOG2\t8  \n#define IVPU_MMU_CDTAB_ENT_COUNT\t((u32)1 << IVPU_MMU_CDTAB_ENT_COUNT_LOG2)\n\n#define IVPU_MMU_STREAM_ID0\t\t0\n#define IVPU_MMU_STREAM_ID3\t\t3\n\n#define IVPU_MMU_STRTAB_ENT_SIZE\t64\n#define IVPU_MMU_STRTAB_ENT_COUNT\t4\n#define IVPU_MMU_STRTAB_CFG_LOG2SIZE\t2\n#define IVPU_MMU_STRTAB_CFG\t\tIVPU_MMU_STRTAB_CFG_LOG2SIZE\n\n#define IVPU_MMU_Q_COUNT_LOG2\t\t4  \n#define IVPU_MMU_Q_COUNT\t\t((u32)1 << IVPU_MMU_Q_COUNT_LOG2)\n#define IVPU_MMU_Q_WRAP_BIT\t\t(IVPU_MMU_Q_COUNT << 1)\n#define IVPU_MMU_Q_WRAP_MASK\t\t(IVPU_MMU_Q_WRAP_BIT - 1)\n#define IVPU_MMU_Q_IDX_MASK\t\t(IVPU_MMU_Q_COUNT - 1)\n#define IVPU_MMU_Q_IDX(val)\t\t((val) & IVPU_MMU_Q_IDX_MASK)\n\n#define IVPU_MMU_CMDQ_CMD_SIZE\t\t16\n#define IVPU_MMU_CMDQ_SIZE\t\t(IVPU_MMU_Q_COUNT * IVPU_MMU_CMDQ_CMD_SIZE)\n\n#define IVPU_MMU_EVTQ_CMD_SIZE\t\t32\n#define IVPU_MMU_EVTQ_SIZE\t\t(IVPU_MMU_Q_COUNT * IVPU_MMU_EVTQ_CMD_SIZE)\n\n#define IVPU_MMU_CMD_OPCODE\t\tGENMASK(7, 0)\n\n#define IVPU_MMU_CMD_SYNC_0_CS\t\tGENMASK(13, 12)\n#define IVPU_MMU_CMD_SYNC_0_MSH\t\tGENMASK(23, 22)\n#define IVPU_MMU_CMD_SYNC_0_MSI_ATTR\tGENMASK(27, 24)\n#define IVPU_MMU_CMD_SYNC_0_MSI_ATTR\tGENMASK(27, 24)\n#define IVPU_MMU_CMD_SYNC_0_MSI_DATA\tGENMASK(63, 32)\n\n#define IVPU_MMU_CMD_CFGI_0_SSEC\tBIT(10)\n#define IVPU_MMU_CMD_CFGI_0_SSV\t\tBIT(11)\n#define IVPU_MMU_CMD_CFGI_0_SSID\tGENMASK(31, 12)\n#define IVPU_MMU_CMD_CFGI_0_SID\t\tGENMASK(63, 32)\n#define IVPU_MMU_CMD_CFGI_1_RANGE\tGENMASK(4, 0)\n\n#define IVPU_MMU_CMD_TLBI_0_ASID\tGENMASK(63, 48)\n#define IVPU_MMU_CMD_TLBI_0_VMID\tGENMASK(47, 32)\n\n#define CMD_PREFETCH_CFG\t\t0x1\n#define CMD_CFGI_STE\t\t\t0x3\n#define CMD_CFGI_ALL\t\t\t0x4\n#define CMD_CFGI_CD\t\t\t0x5\n#define CMD_CFGI_CD_ALL\t\t\t0x6\n#define CMD_TLBI_NH_ASID\t\t0x11\n#define CMD_TLBI_EL2_ALL\t\t0x20\n#define CMD_TLBI_NSNH_ALL\t\t0x30\n#define CMD_SYNC\t\t\t0x46\n\n#define IVPU_MMU_EVT_F_UUT\t\t0x01\n#define IVPU_MMU_EVT_C_BAD_STREAMID\t0x02\n#define IVPU_MMU_EVT_F_STE_FETCH\t0x03\n#define IVPU_MMU_EVT_C_BAD_STE\t\t0x04\n#define IVPU_MMU_EVT_F_BAD_ATS_TREQ\t0x05\n#define IVPU_MMU_EVT_F_STREAM_DISABLED\t0x06\n#define IVPU_MMU_EVT_F_TRANSL_FORBIDDEN\t0x07\n#define IVPU_MMU_EVT_C_BAD_SUBSTREAMID\t0x08\n#define IVPU_MMU_EVT_F_CD_FETCH\t\t0x09\n#define IVPU_MMU_EVT_C_BAD_CD\t\t0x0a\n#define IVPU_MMU_EVT_F_WALK_EABT\t0x0b\n#define IVPU_MMU_EVT_F_TRANSLATION\t0x10\n#define IVPU_MMU_EVT_F_ADDR_SIZE\t0x11\n#define IVPU_MMU_EVT_F_ACCESS\t\t0x12\n#define IVPU_MMU_EVT_F_PERMISSION\t0x13\n#define IVPU_MMU_EVT_F_TLB_CONFLICT\t0x20\n#define IVPU_MMU_EVT_F_CFG_CONFLICT\t0x21\n#define IVPU_MMU_EVT_E_PAGE_REQUEST\t0x24\n#define IVPU_MMU_EVT_F_VMS_FETCH\t0x25\n\n#define IVPU_MMU_EVT_OP_MASK\t\tGENMASK_ULL(7, 0)\n#define IVPU_MMU_EVT_SSID_MASK\t\tGENMASK_ULL(31, 12)\n\n#define IVPU_MMU_Q_BASE_RWA\t\tBIT(62)\n#define IVPU_MMU_Q_BASE_ADDR_MASK\tGENMASK_ULL(51, 5)\n#define IVPU_MMU_STRTAB_BASE_RA\t\tBIT(62)\n#define IVPU_MMU_STRTAB_BASE_ADDR_MASK\tGENMASK_ULL(51, 6)\n\n#define IVPU_MMU_IRQ_EVTQ_EN\t\tBIT(2)\n#define IVPU_MMU_IRQ_GERROR_EN\t\tBIT(0)\n\n#define IVPU_MMU_CR0_ATSCHK\t\tBIT(4)\n#define IVPU_MMU_CR0_CMDQEN\t\tBIT(3)\n#define IVPU_MMU_CR0_EVTQEN\t\tBIT(2)\n#define IVPU_MMU_CR0_PRIQEN\t\tBIT(1)\n#define IVPU_MMU_CR0_SMMUEN\t\tBIT(0)\n\n#define IVPU_MMU_CR1_TABLE_SH\t\tGENMASK(11, 10)\n#define IVPU_MMU_CR1_TABLE_OC\t\tGENMASK(9, 8)\n#define IVPU_MMU_CR1_TABLE_IC\t\tGENMASK(7, 6)\n#define IVPU_MMU_CR1_QUEUE_SH\t\tGENMASK(5, 4)\n#define IVPU_MMU_CR1_QUEUE_OC\t\tGENMASK(3, 2)\n#define IVPU_MMU_CR1_QUEUE_IC\t\tGENMASK(1, 0)\n#define IVPU_MMU_CACHE_NC\t\t0\n#define IVPU_MMU_CACHE_WB\t\t1\n#define IVPU_MMU_CACHE_WT\t\t2\n#define IVPU_MMU_SH_NSH\t\t\t0\n#define IVPU_MMU_SH_OSH\t\t\t2\n#define IVPU_MMU_SH_ISH\t\t\t3\n\n#define IVPU_MMU_CMDQ_OP\t\tGENMASK_ULL(7, 0)\n\n#define IVPU_MMU_CD_0_TCR_T0SZ\t\tGENMASK_ULL(5, 0)\n#define IVPU_MMU_CD_0_TCR_TG0\t\tGENMASK_ULL(7, 6)\n#define IVPU_MMU_CD_0_TCR_IRGN0\t\tGENMASK_ULL(9, 8)\n#define IVPU_MMU_CD_0_TCR_ORGN0\t\tGENMASK_ULL(11, 10)\n#define IVPU_MMU_CD_0_TCR_SH0\t\tGENMASK_ULL(13, 12)\n#define IVPU_MMU_CD_0_TCR_EPD0\t\tBIT_ULL(14)\n#define IVPU_MMU_CD_0_TCR_EPD1\t\tBIT_ULL(30)\n#define IVPU_MMU_CD_0_ENDI\t\tBIT(15)\n#define IVPU_MMU_CD_0_V\t\t\tBIT(31)\n#define IVPU_MMU_CD_0_TCR_IPS\t\tGENMASK_ULL(34, 32)\n#define IVPU_MMU_CD_0_TCR_TBI0\t\tBIT_ULL(38)\n#define IVPU_MMU_CD_0_AA64\t\tBIT(41)\n#define IVPU_MMU_CD_0_S\t\t\tBIT(44)\n#define IVPU_MMU_CD_0_R\t\t\tBIT(45)\n#define IVPU_MMU_CD_0_A\t\t\tBIT(46)\n#define IVPU_MMU_CD_0_ASET\t\tBIT(47)\n#define IVPU_MMU_CD_0_ASID\t\tGENMASK_ULL(63, 48)\n\n#define IVPU_MMU_T0SZ_48BIT             16\n#define IVPU_MMU_T0SZ_38BIT             26\n\n#define IVPU_MMU_IPS_48BIT\t\t5\n#define IVPU_MMU_IPS_44BIT\t\t4\n#define IVPU_MMU_IPS_42BIT\t\t3\n#define IVPU_MMU_IPS_40BIT\t\t2\n#define IVPU_MMU_IPS_36BIT\t\t1\n#define IVPU_MMU_IPS_32BIT\t\t0\n\n#define IVPU_MMU_CD_1_TTB0_MASK\t\tGENMASK_ULL(51, 4)\n\n#define IVPU_MMU_STE_0_S1CDMAX\t\tGENMASK_ULL(63, 59)\n#define IVPU_MMU_STE_0_S1FMT\t\tGENMASK_ULL(5, 4)\n#define IVPU_MMU_STE_0_S1FMT_LINEAR\t0\n#define IVPU_MMU_STE_DWORDS\t\t8\n#define IVPU_MMU_STE_0_CFG_S1_TRANS\t5\n#define IVPU_MMU_STE_0_CFG\t\tGENMASK_ULL(3, 1)\n#define IVPU_MMU_STE_0_S1CTXPTR_MASK\tGENMASK_ULL(51, 6)\n#define IVPU_MMU_STE_0_V\t\t\tBIT(0)\n\n#define IVPU_MMU_STE_1_STRW_NSEL1\t0ul\n#define IVPU_MMU_STE_1_CONT\t\tGENMASK_ULL(16, 13)\n#define IVPU_MMU_STE_1_STRW\t\tGENMASK_ULL(31, 30)\n#define IVPU_MMU_STE_1_PRIVCFG\t\tGENMASK_ULL(49, 48)\n#define IVPU_MMU_STE_1_PRIVCFG_UNPRIV\t2ul\n#define IVPU_MMU_STE_1_INSTCFG\t\tGENMASK_ULL(51, 50)\n#define IVPU_MMU_STE_1_INSTCFG_DATA\t2ul\n#define IVPU_MMU_STE_1_MEV\t\tBIT(19)\n#define IVPU_MMU_STE_1_S1STALLD\t\tBIT(27)\n#define IVPU_MMU_STE_1_S1C_CACHE_NC\t0ul\n#define IVPU_MMU_STE_1_S1C_CACHE_WBRA\t1ul\n#define IVPU_MMU_STE_1_S1C_CACHE_WT\t2ul\n#define IVPU_MMU_STE_1_S1C_CACHE_WB\t3ul\n#define IVPU_MMU_STE_1_S1CIR\t\tGENMASK_ULL(3, 2)\n#define IVPU_MMU_STE_1_S1COR\t\tGENMASK_ULL(5, 4)\n#define IVPU_MMU_STE_1_S1CSH\t\tGENMASK_ULL(7, 6)\n#define IVPU_MMU_STE_1_S1DSS\t\tGENMASK_ULL(1, 0)\n#define IVPU_MMU_STE_1_S1DSS_TERMINATE\t0x0\n\n#define IVPU_MMU_REG_TIMEOUT_US\t\t(10 * USEC_PER_MSEC)\n#define IVPU_MMU_QUEUE_TIMEOUT_US\t(100 * USEC_PER_MSEC)\n\n#define IVPU_MMU_GERROR_ERR_MASK ((REG_FLD(VPU_37XX_HOST_MMU_GERROR, CMDQ)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, EVTQ_ABT)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, PRIQ_ABT)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_CMDQ_ABT)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_EVTQ_ABT)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_PRIQ_ABT)) | \\\n\t\t\t\t  (REG_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_ABT)))\n\nstatic char *ivpu_mmu_event_to_str(u32 cmd)\n{\n\tswitch (cmd) {\n\tcase IVPU_MMU_EVT_F_UUT:\n\t\treturn \"Unsupported Upstream Transaction\";\n\tcase IVPU_MMU_EVT_C_BAD_STREAMID:\n\t\treturn \"Transaction StreamID out of range\";\n\tcase IVPU_MMU_EVT_F_STE_FETCH:\n\t\treturn \"Fetch of STE caused external abort\";\n\tcase IVPU_MMU_EVT_C_BAD_STE:\n\t\treturn \"Used STE invalid\";\n\tcase IVPU_MMU_EVT_F_BAD_ATS_TREQ:\n\t\treturn \"Address Request disallowed for a StreamID\";\n\tcase IVPU_MMU_EVT_F_STREAM_DISABLED:\n\t\treturn \"Transaction marks non-substream disabled\";\n\tcase IVPU_MMU_EVT_F_TRANSL_FORBIDDEN:\n\t\treturn \"MMU bypass is disallowed for this StreamID\";\n\tcase IVPU_MMU_EVT_C_BAD_SUBSTREAMID:\n\t\treturn \"Invalid StreamID\";\n\tcase IVPU_MMU_EVT_F_CD_FETCH:\n\t\treturn \"Fetch of CD caused external abort\";\n\tcase IVPU_MMU_EVT_C_BAD_CD:\n\t\treturn \"Fetched CD invalid\";\n\tcase IVPU_MMU_EVT_F_WALK_EABT:\n\t\treturn \" An external abort occurred fetching a TLB\";\n\tcase IVPU_MMU_EVT_F_TRANSLATION:\n\t\treturn \"Translation fault\";\n\tcase IVPU_MMU_EVT_F_ADDR_SIZE:\n\t\treturn \" Output address caused address size fault\";\n\tcase IVPU_MMU_EVT_F_ACCESS:\n\t\treturn \"Access flag fault\";\n\tcase IVPU_MMU_EVT_F_PERMISSION:\n\t\treturn \"Permission fault occurred on page access\";\n\tcase IVPU_MMU_EVT_F_TLB_CONFLICT:\n\t\treturn \"A TLB conflict\";\n\tcase IVPU_MMU_EVT_F_CFG_CONFLICT:\n\t\treturn \"A configuration cache conflict\";\n\tcase IVPU_MMU_EVT_E_PAGE_REQUEST:\n\t\treturn \"Page request hint from a client device\";\n\tcase IVPU_MMU_EVT_F_VMS_FETCH:\n\t\treturn \"Fetch of VMS caused external abort\";\n\tdefault:\n\t\treturn \"Unknown CMDQ command\";\n\t}\n}\n\nstatic void ivpu_mmu_config_check(struct ivpu_device *vdev)\n{\n\tu32 val_ref;\n\tu32 val;\n\n\tif (ivpu_is_simics(vdev))\n\t\tval_ref = IVPU_MMU_IDR0_REF_SIMICS;\n\telse\n\t\tval_ref = IVPU_MMU_IDR0_REF;\n\n\tval = REGV_RD32(VPU_37XX_HOST_MMU_IDR0);\n\tif (val != val_ref)\n\t\tivpu_dbg(vdev, MMU, \"IDR0 0x%x != IDR0_REF 0x%x\\n\", val, val_ref);\n\n\tval = REGV_RD32(VPU_37XX_HOST_MMU_IDR1);\n\tif (val != IVPU_MMU_IDR1_REF)\n\t\tivpu_dbg(vdev, MMU, \"IDR1 0x%x != IDR1_REF 0x%x\\n\", val, IVPU_MMU_IDR1_REF);\n\n\tval = REGV_RD32(VPU_37XX_HOST_MMU_IDR3);\n\tif (val != IVPU_MMU_IDR3_REF)\n\t\tivpu_dbg(vdev, MMU, \"IDR3 0x%x != IDR3_REF 0x%x\\n\", val, IVPU_MMU_IDR3_REF);\n\n\tif (ivpu_is_simics(vdev))\n\t\tval_ref = IVPU_MMU_IDR5_REF_SIMICS;\n\telse if (ivpu_is_fpga(vdev))\n\t\tval_ref = IVPU_MMU_IDR5_REF_FPGA;\n\telse\n\t\tval_ref = IVPU_MMU_IDR5_REF;\n\n\tval = REGV_RD32(VPU_37XX_HOST_MMU_IDR5);\n\tif (val != val_ref)\n\t\tivpu_dbg(vdev, MMU, \"IDR5 0x%x != IDR5_REF 0x%x\\n\", val, val_ref);\n}\n\nstatic int ivpu_mmu_cdtab_alloc(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_cdtab *cdtab = &mmu->cdtab;\n\tsize_t size = IVPU_MMU_CDTAB_ENT_COUNT * IVPU_MMU_CDTAB_ENT_SIZE;\n\n\tcdtab->base = dmam_alloc_coherent(vdev->drm.dev, size, &cdtab->dma, GFP_KERNEL);\n\tif (!cdtab->base)\n\t\treturn -ENOMEM;\n\n\tivpu_dbg(vdev, MMU, \"CDTAB alloc: dma=%pad size=%zu\\n\", &cdtab->dma, size);\n\n\treturn 0;\n}\n\nstatic int ivpu_mmu_strtab_alloc(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_strtab *strtab = &mmu->strtab;\n\tsize_t size = IVPU_MMU_STRTAB_ENT_COUNT * IVPU_MMU_STRTAB_ENT_SIZE;\n\n\tstrtab->base = dmam_alloc_coherent(vdev->drm.dev, size, &strtab->dma, GFP_KERNEL);\n\tif (!strtab->base)\n\t\treturn -ENOMEM;\n\n\tstrtab->base_cfg = IVPU_MMU_STRTAB_CFG;\n\tstrtab->dma_q = IVPU_MMU_STRTAB_BASE_RA;\n\tstrtab->dma_q |= strtab->dma & IVPU_MMU_STRTAB_BASE_ADDR_MASK;\n\n\tivpu_dbg(vdev, MMU, \"STRTAB alloc: dma=%pad dma_q=%pad size=%zu\\n\",\n\t\t &strtab->dma, &strtab->dma_q, size);\n\n\treturn 0;\n}\n\nstatic int ivpu_mmu_cmdq_alloc(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_queue *q = &mmu->cmdq;\n\n\tq->base = dmam_alloc_coherent(vdev->drm.dev, IVPU_MMU_CMDQ_SIZE, &q->dma, GFP_KERNEL);\n\tif (!q->base)\n\t\treturn -ENOMEM;\n\n\tq->dma_q = IVPU_MMU_Q_BASE_RWA;\n\tq->dma_q |= q->dma & IVPU_MMU_Q_BASE_ADDR_MASK;\n\tq->dma_q |= IVPU_MMU_Q_COUNT_LOG2;\n\n\tivpu_dbg(vdev, MMU, \"CMDQ alloc: dma=%pad dma_q=%pad size=%u\\n\",\n\t\t &q->dma, &q->dma_q, IVPU_MMU_CMDQ_SIZE);\n\n\treturn 0;\n}\n\nstatic int ivpu_mmu_evtq_alloc(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_queue *q = &mmu->evtq;\n\n\tq->base = dmam_alloc_coherent(vdev->drm.dev, IVPU_MMU_EVTQ_SIZE, &q->dma, GFP_KERNEL);\n\tif (!q->base)\n\t\treturn -ENOMEM;\n\n\tq->dma_q = IVPU_MMU_Q_BASE_RWA;\n\tq->dma_q |= q->dma & IVPU_MMU_Q_BASE_ADDR_MASK;\n\tq->dma_q |= IVPU_MMU_Q_COUNT_LOG2;\n\n\tivpu_dbg(vdev, MMU, \"EVTQ alloc: dma=%pad dma_q=%pad size=%u\\n\",\n\t\t &q->dma, &q->dma_q, IVPU_MMU_EVTQ_SIZE);\n\n\treturn 0;\n}\n\nstatic int ivpu_mmu_structs_alloc(struct ivpu_device *vdev)\n{\n\tint ret;\n\n\tret = ivpu_mmu_cdtab_alloc(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to allocate cdtab: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_strtab_alloc(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to allocate strtab: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_cmdq_alloc(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to allocate cmdq: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_evtq_alloc(vdev);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to allocate evtq: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int ivpu_mmu_reg_write(struct ivpu_device *vdev, u32 reg, u32 val)\n{\n\tu32 reg_ack = reg + 4;  \n\tu32 val_ack;\n\tint ret;\n\n\tREGV_WR32(reg, val);\n\n\tret = REGV_POLL(reg_ack, val_ack, (val == val_ack), IVPU_MMU_REG_TIMEOUT_US);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to write register 0x%x\\n\", reg);\n\n\treturn ret;\n}\n\nstatic int ivpu_mmu_irqs_setup(struct ivpu_device *vdev)\n{\n\tu32 irq_ctrl = IVPU_MMU_IRQ_EVTQ_EN | IVPU_MMU_IRQ_GERROR_EN;\n\tint ret;\n\n\tret = ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_IRQ_CTRL, 0);\n\tif (ret)\n\t\treturn ret;\n\n\treturn ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_IRQ_CTRL, irq_ctrl);\n}\n\nstatic int ivpu_mmu_cmdq_wait_for_cons(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_queue *cmdq = &vdev->mmu->cmdq;\n\n\treturn REGV_POLL(VPU_37XX_HOST_MMU_CMDQ_CONS, cmdq->cons, (cmdq->prod == cmdq->cons),\n\t\t\t IVPU_MMU_QUEUE_TIMEOUT_US);\n}\n\nstatic int ivpu_mmu_cmdq_cmd_write(struct ivpu_device *vdev, const char *name, u64 data0, u64 data1)\n{\n\tstruct ivpu_mmu_queue *q = &vdev->mmu->cmdq;\n\tu64 *queue_buffer = q->base;\n\tint idx = IVPU_MMU_Q_IDX(q->prod) * (IVPU_MMU_CMDQ_CMD_SIZE / sizeof(*queue_buffer));\n\n\tif (!CIRC_SPACE(IVPU_MMU_Q_IDX(q->prod), IVPU_MMU_Q_IDX(q->cons), IVPU_MMU_Q_COUNT)) {\n\t\tivpu_err(vdev, \"Failed to write MMU CMD %s\\n\", name);\n\t\treturn -EBUSY;\n\t}\n\n\tqueue_buffer[idx] = data0;\n\tqueue_buffer[idx + 1] = data1;\n\tq->prod = (q->prod + 1) & IVPU_MMU_Q_WRAP_MASK;\n\n\tivpu_dbg(vdev, MMU, \"CMD write: %s data: 0x%llx 0x%llx\\n\", name, data0, data1);\n\n\treturn 0;\n}\n\nstatic int ivpu_mmu_cmdq_sync(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_queue *q = &vdev->mmu->cmdq;\n\tu64 val;\n\tint ret;\n\n\tval = FIELD_PREP(IVPU_MMU_CMD_OPCODE, CMD_SYNC) |\n\t      FIELD_PREP(IVPU_MMU_CMD_SYNC_0_CS, 0x2) |\n\t      FIELD_PREP(IVPU_MMU_CMD_SYNC_0_MSH, 0x3) |\n\t      FIELD_PREP(IVPU_MMU_CMD_SYNC_0_MSI_ATTR, 0xf);\n\n\tret = ivpu_mmu_cmdq_cmd_write(vdev, \"SYNC\", val, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tclflush_cache_range(q->base, IVPU_MMU_CMDQ_SIZE);\n\tREGV_WR32(VPU_37XX_HOST_MMU_CMDQ_PROD, q->prod);\n\n\tret = ivpu_mmu_cmdq_wait_for_cons(vdev);\n\tif (ret)\n\t\tivpu_err(vdev, \"Timed out waiting for consumer: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int ivpu_mmu_cmdq_write_cfgi_all(struct ivpu_device *vdev)\n{\n\tu64 data0 = FIELD_PREP(IVPU_MMU_CMD_OPCODE, CMD_CFGI_ALL);\n\tu64 data1 = FIELD_PREP(IVPU_MMU_CMD_CFGI_1_RANGE, 0x1f);\n\n\treturn ivpu_mmu_cmdq_cmd_write(vdev, \"CFGI_ALL\", data0, data1);\n}\n\nstatic int ivpu_mmu_cmdq_write_tlbi_nh_asid(struct ivpu_device *vdev, u16 ssid)\n{\n\tu64 val = FIELD_PREP(IVPU_MMU_CMD_OPCODE, CMD_TLBI_NH_ASID) |\n\t\t  FIELD_PREP(IVPU_MMU_CMD_TLBI_0_ASID, ssid);\n\n\treturn ivpu_mmu_cmdq_cmd_write(vdev, \"TLBI_NH_ASID\", val, 0);\n}\n\nstatic int ivpu_mmu_cmdq_write_tlbi_nsnh_all(struct ivpu_device *vdev)\n{\n\tu64 val = FIELD_PREP(IVPU_MMU_CMD_OPCODE, CMD_TLBI_NSNH_ALL);\n\n\treturn ivpu_mmu_cmdq_cmd_write(vdev, \"TLBI_NSNH_ALL\", val, 0);\n}\n\nstatic int ivpu_mmu_reset(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tu32 val;\n\tint ret;\n\n\tmemset(mmu->cmdq.base, 0, IVPU_MMU_CMDQ_SIZE);\n\tclflush_cache_range(mmu->cmdq.base, IVPU_MMU_CMDQ_SIZE);\n\tmmu->cmdq.prod = 0;\n\tmmu->cmdq.cons = 0;\n\n\tmemset(mmu->evtq.base, 0, IVPU_MMU_EVTQ_SIZE);\n\tclflush_cache_range(mmu->evtq.base, IVPU_MMU_EVTQ_SIZE);\n\tmmu->evtq.prod = 0;\n\tmmu->evtq.cons = 0;\n\n\tret = ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_CR0, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tval = FIELD_PREP(IVPU_MMU_CR1_TABLE_SH, IVPU_MMU_SH_ISH) |\n\t      FIELD_PREP(IVPU_MMU_CR1_TABLE_OC, IVPU_MMU_CACHE_WB) |\n\t      FIELD_PREP(IVPU_MMU_CR1_TABLE_IC, IVPU_MMU_CACHE_WB) |\n\t      FIELD_PREP(IVPU_MMU_CR1_QUEUE_SH, IVPU_MMU_SH_ISH) |\n\t      FIELD_PREP(IVPU_MMU_CR1_QUEUE_OC, IVPU_MMU_CACHE_WB) |\n\t      FIELD_PREP(IVPU_MMU_CR1_QUEUE_IC, IVPU_MMU_CACHE_WB);\n\tREGV_WR32(VPU_37XX_HOST_MMU_CR1, val);\n\n\tREGV_WR64(VPU_37XX_HOST_MMU_STRTAB_BASE, mmu->strtab.dma_q);\n\tREGV_WR32(VPU_37XX_HOST_MMU_STRTAB_BASE_CFG, mmu->strtab.base_cfg);\n\n\tREGV_WR64(VPU_37XX_HOST_MMU_CMDQ_BASE, mmu->cmdq.dma_q);\n\tREGV_WR32(VPU_37XX_HOST_MMU_CMDQ_PROD, 0);\n\tREGV_WR32(VPU_37XX_HOST_MMU_CMDQ_CONS, 0);\n\n\tval = IVPU_MMU_CR0_CMDQEN;\n\tret = ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_CR0, val);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ivpu_mmu_cmdq_write_cfgi_all(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ivpu_mmu_cmdq_write_tlbi_nsnh_all(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ivpu_mmu_cmdq_sync(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tREGV_WR64(VPU_37XX_HOST_MMU_EVTQ_BASE, mmu->evtq.dma_q);\n\tREGV_WR32(VPU_37XX_HOST_MMU_EVTQ_PROD_SEC, 0);\n\tREGV_WR32(VPU_37XX_HOST_MMU_EVTQ_CONS_SEC, 0);\n\n\tval |= IVPU_MMU_CR0_EVTQEN;\n\tret = ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_CR0, val);\n\tif (ret)\n\t\treturn ret;\n\n\tval |= IVPU_MMU_CR0_ATSCHK;\n\tret = ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_CR0, val);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ivpu_mmu_irqs_setup(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tval |= IVPU_MMU_CR0_SMMUEN;\n\treturn ivpu_mmu_reg_write(vdev, VPU_37XX_HOST_MMU_CR0, val);\n}\n\nstatic void ivpu_mmu_strtab_link_cd(struct ivpu_device *vdev, u32 sid)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_strtab *strtab = &mmu->strtab;\n\tstruct ivpu_mmu_cdtab *cdtab = &mmu->cdtab;\n\tu64 *entry = strtab->base + (sid * IVPU_MMU_STRTAB_ENT_SIZE);\n\tu64 str[2];\n\n\tstr[0] = FIELD_PREP(IVPU_MMU_STE_0_CFG, IVPU_MMU_STE_0_CFG_S1_TRANS) |\n\t\t FIELD_PREP(IVPU_MMU_STE_0_S1CDMAX, IVPU_MMU_CDTAB_ENT_COUNT_LOG2) |\n\t\t FIELD_PREP(IVPU_MMU_STE_0_S1FMT, IVPU_MMU_STE_0_S1FMT_LINEAR) |\n\t\t IVPU_MMU_STE_0_V |\n\t\t (cdtab->dma & IVPU_MMU_STE_0_S1CTXPTR_MASK);\n\n\tstr[1] = FIELD_PREP(IVPU_MMU_STE_1_S1DSS, IVPU_MMU_STE_1_S1DSS_TERMINATE) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_S1CIR, IVPU_MMU_STE_1_S1C_CACHE_NC) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_S1COR, IVPU_MMU_STE_1_S1C_CACHE_NC) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_S1CSH, IVPU_MMU_SH_NSH) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_PRIVCFG, IVPU_MMU_STE_1_PRIVCFG_UNPRIV) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_INSTCFG, IVPU_MMU_STE_1_INSTCFG_DATA) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_STRW, IVPU_MMU_STE_1_STRW_NSEL1) |\n\t\t FIELD_PREP(IVPU_MMU_STE_1_CONT, IVPU_MMU_STRTAB_CFG_LOG2SIZE) |\n\t\t IVPU_MMU_STE_1_MEV |\n\t\t IVPU_MMU_STE_1_S1STALLD;\n\n\tWRITE_ONCE(entry[1], str[1]);\n\tWRITE_ONCE(entry[0], str[0]);\n\n\tclflush_cache_range(entry, IVPU_MMU_STRTAB_ENT_SIZE);\n\n\tivpu_dbg(vdev, MMU, \"STRTAB write entry (SSID=%u): 0x%llx, 0x%llx\\n\", sid, str[0], str[1]);\n}\n\nstatic int ivpu_mmu_strtab_init(struct ivpu_device *vdev)\n{\n\tivpu_mmu_strtab_link_cd(vdev, IVPU_MMU_STREAM_ID0);\n\tivpu_mmu_strtab_link_cd(vdev, IVPU_MMU_STREAM_ID3);\n\n\treturn 0;\n}\n\nint ivpu_mmu_invalidate_tlb(struct ivpu_device *vdev, u16 ssid)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tint ret = 0;\n\n\tmutex_lock(&mmu->lock);\n\tif (!mmu->on)\n\t\tgoto unlock;\n\n\tret = ivpu_mmu_cmdq_write_tlbi_nh_asid(vdev, ssid);\n\tif (ret)\n\t\tgoto unlock;\n\n\tret = ivpu_mmu_cmdq_sync(vdev);\nunlock:\n\tmutex_unlock(&mmu->lock);\n\treturn ret;\n}\n\nstatic int ivpu_mmu_cd_add(struct ivpu_device *vdev, u32 ssid, u64 cd_dma)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tstruct ivpu_mmu_cdtab *cdtab = &mmu->cdtab;\n\tu64 *entry;\n\tu64 cd[4];\n\tint ret = 0;\n\n\tif (ssid > IVPU_MMU_CDTAB_ENT_COUNT)\n\t\treturn -EINVAL;\n\n\tentry = cdtab->base + (ssid * IVPU_MMU_CDTAB_ENT_SIZE);\n\n\tif (cd_dma != 0) {\n\t\tcd[0] = FIELD_PREP(IVPU_MMU_CD_0_TCR_T0SZ, IVPU_MMU_T0SZ_48BIT) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_TCR_TG0, 0) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_TCR_IRGN0, 0) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_TCR_ORGN0, 0) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_TCR_SH0, 0) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_TCR_IPS, IVPU_MMU_IPS_48BIT) |\n\t\t\tFIELD_PREP(IVPU_MMU_CD_0_ASID, ssid) |\n\t\t\tIVPU_MMU_CD_0_TCR_EPD1 |\n\t\t\tIVPU_MMU_CD_0_AA64 |\n\t\t\tIVPU_MMU_CD_0_R |\n\t\t\tIVPU_MMU_CD_0_ASET |\n\t\t\tIVPU_MMU_CD_0_V;\n\t\tcd[1] = cd_dma & IVPU_MMU_CD_1_TTB0_MASK;\n\t\tcd[2] = 0;\n\t\tcd[3] = 0x0000000000007444;\n\n\t\t \n\t\tif (ssid == IVPU_GLOBAL_CONTEXT_MMU_SSID)\n\t\t\tcd[0] |= IVPU_MMU_CD_0_A;\n\t} else {\n\t\tmemset(cd, 0, sizeof(cd));\n\t}\n\n\tWRITE_ONCE(entry[1], cd[1]);\n\tWRITE_ONCE(entry[2], cd[2]);\n\tWRITE_ONCE(entry[3], cd[3]);\n\tWRITE_ONCE(entry[0], cd[0]);\n\n\tclflush_cache_range(entry, IVPU_MMU_CDTAB_ENT_SIZE);\n\n\tivpu_dbg(vdev, MMU, \"CDTAB %s entry (SSID=%u, dma=%pad): 0x%llx, 0x%llx, 0x%llx, 0x%llx\\n\",\n\t\t cd_dma ? \"write\" : \"clear\", ssid, &cd_dma, cd[0], cd[1], cd[2], cd[3]);\n\n\tmutex_lock(&mmu->lock);\n\tif (!mmu->on)\n\t\tgoto unlock;\n\n\tret = ivpu_mmu_cmdq_write_cfgi_all(vdev);\n\tif (ret)\n\t\tgoto unlock;\n\n\tret = ivpu_mmu_cmdq_sync(vdev);\nunlock:\n\tmutex_unlock(&mmu->lock);\n\treturn ret;\n}\n\nstatic int ivpu_mmu_cd_add_gbl(struct ivpu_device *vdev)\n{\n\tint ret;\n\n\tret = ivpu_mmu_cd_add(vdev, 0, vdev->gctx.pgtable.pgd_dma);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to add global CD entry: %d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int ivpu_mmu_cd_add_user(struct ivpu_device *vdev, u32 ssid, dma_addr_t cd_dma)\n{\n\tint ret;\n\n\tif (ssid == 0) {\n\t\tivpu_err(vdev, \"Invalid SSID: %u\\n\", ssid);\n\t\treturn -EINVAL;\n\t}\n\n\tret = ivpu_mmu_cd_add(vdev, ssid, cd_dma);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to add CD entry SSID=%u: %d\\n\", ssid, ret);\n\n\treturn ret;\n}\n\nint ivpu_mmu_init(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tint ret;\n\n\tivpu_dbg(vdev, MMU, \"Init..\\n\");\n\n\tdrmm_mutex_init(&vdev->drm, &mmu->lock);\n\tivpu_mmu_config_check(vdev);\n\n\tret = ivpu_mmu_structs_alloc(vdev);\n\tif (ret)\n\t\treturn ret;\n\n\tret = ivpu_mmu_strtab_init(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to initialize strtab: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_cd_add_gbl(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to initialize strtab: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_enable(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to resume MMU: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tivpu_dbg(vdev, MMU, \"Init done\\n\");\n\n\treturn 0;\n}\n\nint ivpu_mmu_enable(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\tint ret;\n\n\tmutex_lock(&mmu->lock);\n\n\tmmu->on = true;\n\n\tret = ivpu_mmu_reset(vdev);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to reset MMU: %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tret = ivpu_mmu_cmdq_write_cfgi_all(vdev);\n\tif (ret)\n\t\tgoto err;\n\n\tret = ivpu_mmu_cmdq_write_tlbi_nsnh_all(vdev);\n\tif (ret)\n\t\tgoto err;\n\n\tret = ivpu_mmu_cmdq_sync(vdev);\n\tif (ret)\n\t\tgoto err;\n\n\tmutex_unlock(&mmu->lock);\n\n\treturn 0;\nerr:\n\tmmu->on = false;\n\tmutex_unlock(&mmu->lock);\n\treturn ret;\n}\n\nvoid ivpu_mmu_disable(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_info *mmu = vdev->mmu;\n\n\tmutex_lock(&mmu->lock);\n\tmmu->on = false;\n\tmutex_unlock(&mmu->lock);\n}\n\nstatic void ivpu_mmu_dump_event(struct ivpu_device *vdev, u32 *event)\n{\n\tu32 ssid = FIELD_GET(IVPU_MMU_EVT_SSID_MASK, event[0]);\n\tu32 op = FIELD_GET(IVPU_MMU_EVT_OP_MASK, event[0]);\n\tu64 fetch_addr = ((u64)event[7]) << 32 | event[6];\n\tu64 in_addr = ((u64)event[5]) << 32 | event[4];\n\tu32 sid = event[1];\n\n\tivpu_err(vdev, \"MMU EVTQ: 0x%x (%s) SSID: %d SID: %d, e[2] %08x, e[3] %08x, in addr: 0x%llx, fetch addr: 0x%llx\\n\",\n\t\t op, ivpu_mmu_event_to_str(op), ssid, sid, event[2], event[3], in_addr, fetch_addr);\n}\n\nstatic u32 *ivpu_mmu_get_event(struct ivpu_device *vdev)\n{\n\tstruct ivpu_mmu_queue *evtq = &vdev->mmu->evtq;\n\tu32 idx = IVPU_MMU_Q_IDX(evtq->cons);\n\tu32 *evt = evtq->base + (idx * IVPU_MMU_EVTQ_CMD_SIZE);\n\n\tevtq->prod = REGV_RD32(VPU_37XX_HOST_MMU_EVTQ_PROD_SEC);\n\tif (!CIRC_CNT(IVPU_MMU_Q_IDX(evtq->prod), IVPU_MMU_Q_IDX(evtq->cons), IVPU_MMU_Q_COUNT))\n\t\treturn NULL;\n\n\tclflush_cache_range(evt, IVPU_MMU_EVTQ_CMD_SIZE);\n\n\tevtq->cons = (evtq->cons + 1) & IVPU_MMU_Q_WRAP_MASK;\n\tREGV_WR32(VPU_37XX_HOST_MMU_EVTQ_CONS_SEC, evtq->cons);\n\n\treturn evt;\n}\n\nvoid ivpu_mmu_irq_evtq_handler(struct ivpu_device *vdev)\n{\n\tbool schedule_recovery = false;\n\tu32 *event;\n\tu32 ssid;\n\n\tivpu_dbg(vdev, IRQ, \"MMU event queue\\n\");\n\n\twhile ((event = ivpu_mmu_get_event(vdev)) != NULL) {\n\t\tivpu_mmu_dump_event(vdev, event);\n\n\t\tssid = FIELD_GET(IVPU_MMU_EVT_SSID_MASK, event[0]);\n\t\tif (ssid == IVPU_GLOBAL_CONTEXT_MMU_SSID)\n\t\t\tschedule_recovery = true;\n\t\telse\n\t\t\tivpu_mmu_user_context_mark_invalid(vdev, ssid);\n\t}\n\n\tif (schedule_recovery)\n\t\tivpu_pm_schedule_recovery(vdev);\n}\n\nvoid ivpu_mmu_irq_gerr_handler(struct ivpu_device *vdev)\n{\n\tu32 gerror_val, gerrorn_val, active;\n\n\tivpu_dbg(vdev, IRQ, \"MMU error\\n\");\n\n\tgerror_val = REGV_RD32(VPU_37XX_HOST_MMU_GERROR);\n\tgerrorn_val = REGV_RD32(VPU_37XX_HOST_MMU_GERRORN);\n\n\tactive = gerror_val ^ gerrorn_val;\n\tif (!(active & IVPU_MMU_GERROR_ERR_MASK))\n\t\treturn;\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_ABT, active))\n\t\tivpu_warn_ratelimited(vdev, \"MMU MSI ABT write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_PRIQ_ABT, active))\n\t\tivpu_warn_ratelimited(vdev, \"MMU PRIQ MSI ABT write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_EVTQ_ABT, active))\n\t\tivpu_warn_ratelimited(vdev, \"MMU EVTQ MSI ABT write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, MSI_CMDQ_ABT, active))\n\t\tivpu_warn_ratelimited(vdev, \"MMU CMDQ MSI ABT write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, PRIQ_ABT, active))\n\t\tivpu_err_ratelimited(vdev, \"MMU PRIQ write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, EVTQ_ABT, active))\n\t\tivpu_err_ratelimited(vdev, \"MMU EVTQ write aborted\\n\");\n\n\tif (REG_TEST_FLD(VPU_37XX_HOST_MMU_GERROR, CMDQ, active))\n\t\tivpu_err_ratelimited(vdev, \"MMU CMDQ write aborted\\n\");\n\n\tREGV_WR32(VPU_37XX_HOST_MMU_GERRORN, gerror_val);\n}\n\nint ivpu_mmu_set_pgtable(struct ivpu_device *vdev, int ssid, struct ivpu_mmu_pgtable *pgtable)\n{\n\treturn ivpu_mmu_cd_add_user(vdev, ssid, pgtable->pgd_dma);\n}\n\nvoid ivpu_mmu_clear_pgtable(struct ivpu_device *vdev, int ssid)\n{\n\tivpu_mmu_cd_add_user(vdev, ssid, 0);  \n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}