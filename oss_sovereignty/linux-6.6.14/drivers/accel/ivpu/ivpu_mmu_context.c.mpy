{
  "module_name": "ivpu_mmu_context.c",
  "hash_id": "6de8d76ca576caba4af1f59402d05040bcd578f533e36f89db9235344127624b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/ivpu/ivpu_mmu_context.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/highmem.h>\n\n#include \"ivpu_drv.h\"\n#include \"ivpu_hw.h\"\n#include \"ivpu_mmu.h\"\n#include \"ivpu_mmu_context.h\"\n\n#define IVPU_MMU_VPU_ADDRESS_MASK        GENMASK(47, 12)\n#define IVPU_MMU_PGD_INDEX_MASK          GENMASK(47, 39)\n#define IVPU_MMU_PUD_INDEX_MASK          GENMASK(38, 30)\n#define IVPU_MMU_PMD_INDEX_MASK          GENMASK(29, 21)\n#define IVPU_MMU_PTE_INDEX_MASK          GENMASK(20, 12)\n#define IVPU_MMU_ENTRY_FLAGS_MASK        (BIT(52) | GENMASK(11, 0))\n#define IVPU_MMU_ENTRY_FLAG_CONT         BIT(52)\n#define IVPU_MMU_ENTRY_FLAG_NG           BIT(11)\n#define IVPU_MMU_ENTRY_FLAG_AF           BIT(10)\n#define IVPU_MMU_ENTRY_FLAG_USER         BIT(6)\n#define IVPU_MMU_ENTRY_FLAG_LLC_COHERENT BIT(2)\n#define IVPU_MMU_ENTRY_FLAG_TYPE_PAGE    BIT(1)\n#define IVPU_MMU_ENTRY_FLAG_VALID        BIT(0)\n\n#define IVPU_MMU_PAGE_SIZE       SZ_4K\n#define IVPU_MMU_CONT_PAGES_SIZE (IVPU_MMU_PAGE_SIZE * 16)\n#define IVPU_MMU_PTE_MAP_SIZE    (IVPU_MMU_PGTABLE_ENTRIES * IVPU_MMU_PAGE_SIZE)\n#define IVPU_MMU_PMD_MAP_SIZE    (IVPU_MMU_PGTABLE_ENTRIES * IVPU_MMU_PTE_MAP_SIZE)\n#define IVPU_MMU_PUD_MAP_SIZE    (IVPU_MMU_PGTABLE_ENTRIES * IVPU_MMU_PMD_MAP_SIZE)\n#define IVPU_MMU_PGD_MAP_SIZE    (IVPU_MMU_PGTABLE_ENTRIES * IVPU_MMU_PUD_MAP_SIZE)\n#define IVPU_MMU_PGTABLE_SIZE    (IVPU_MMU_PGTABLE_ENTRIES * sizeof(u64))\n\n#define IVPU_MMU_DUMMY_ADDRESS 0xdeadb000\n#define IVPU_MMU_ENTRY_VALID   (IVPU_MMU_ENTRY_FLAG_TYPE_PAGE | IVPU_MMU_ENTRY_FLAG_VALID)\n#define IVPU_MMU_ENTRY_INVALID (IVPU_MMU_DUMMY_ADDRESS & ~IVPU_MMU_ENTRY_FLAGS_MASK)\n#define IVPU_MMU_ENTRY_MAPPED  (IVPU_MMU_ENTRY_FLAG_AF | IVPU_MMU_ENTRY_FLAG_USER | \\\n\t\t\t\tIVPU_MMU_ENTRY_FLAG_NG | IVPU_MMU_ENTRY_VALID)\n\nstatic int ivpu_mmu_pgtable_init(struct ivpu_device *vdev, struct ivpu_mmu_pgtable *pgtable)\n{\n\tdma_addr_t pgd_dma;\n\n\tpgtable->pgd_dma_ptr = dma_alloc_coherent(vdev->drm.dev, IVPU_MMU_PGTABLE_SIZE, &pgd_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\tif (!pgtable->pgd_dma_ptr)\n\t\treturn -ENOMEM;\n\n\tpgtable->pgd_dma = pgd_dma;\n\n\treturn 0;\n}\n\nstatic void ivpu_mmu_pgtable_free(struct ivpu_device *vdev, u64 *cpu_addr, dma_addr_t dma_addr)\n{\n\tif (cpu_addr)\n\t\tdma_free_coherent(vdev->drm.dev, IVPU_MMU_PGTABLE_SIZE, cpu_addr,\n\t\t\t\t  dma_addr & ~IVPU_MMU_ENTRY_FLAGS_MASK);\n}\n\nstatic void ivpu_mmu_pgtables_free(struct ivpu_device *vdev, struct ivpu_mmu_pgtable *pgtable)\n{\n\tint pgd_idx, pud_idx, pmd_idx;\n\tdma_addr_t pud_dma, pmd_dma, pte_dma;\n\tu64 *pud_dma_ptr, *pmd_dma_ptr, *pte_dma_ptr;\n\n\tfor (pgd_idx = 0; pgd_idx < IVPU_MMU_PGTABLE_ENTRIES; ++pgd_idx) {\n\t\tpud_dma_ptr = pgtable->pud_ptrs[pgd_idx];\n\t\tpud_dma = pgtable->pgd_dma_ptr[pgd_idx];\n\n\t\tif (!pud_dma_ptr)\n\t\t\tcontinue;\n\n\t\tfor (pud_idx = 0; pud_idx < IVPU_MMU_PGTABLE_ENTRIES; ++pud_idx) {\n\t\t\tpmd_dma_ptr = pgtable->pmd_ptrs[pgd_idx][pud_idx];\n\t\t\tpmd_dma = pgtable->pud_ptrs[pgd_idx][pud_idx];\n\n\t\t\tif (!pmd_dma_ptr)\n\t\t\t\tcontinue;\n\n\t\t\tfor (pmd_idx = 0; pmd_idx < IVPU_MMU_PGTABLE_ENTRIES; ++pmd_idx) {\n\t\t\t\tpte_dma_ptr = pgtable->pte_ptrs[pgd_idx][pud_idx][pmd_idx];\n\t\t\t\tpte_dma = pgtable->pmd_ptrs[pgd_idx][pud_idx][pmd_idx];\n\n\t\t\t\tivpu_mmu_pgtable_free(vdev, pte_dma_ptr, pte_dma);\n\t\t\t}\n\n\t\t\tkfree(pgtable->pte_ptrs[pgd_idx][pud_idx]);\n\t\t\tivpu_mmu_pgtable_free(vdev, pmd_dma_ptr, pmd_dma);\n\t\t}\n\n\t\tkfree(pgtable->pmd_ptrs[pgd_idx]);\n\t\tkfree(pgtable->pte_ptrs[pgd_idx]);\n\t\tivpu_mmu_pgtable_free(vdev, pud_dma_ptr, pud_dma);\n\t}\n\n\tivpu_mmu_pgtable_free(vdev, pgtable->pgd_dma_ptr, pgtable->pgd_dma);\n}\n\nstatic u64*\nivpu_mmu_ensure_pud(struct ivpu_device *vdev, struct ivpu_mmu_pgtable *pgtable, int pgd_idx)\n{\n\tu64 *pud_dma_ptr = pgtable->pud_ptrs[pgd_idx];\n\tdma_addr_t pud_dma;\n\n\tif (pud_dma_ptr)\n\t\treturn pud_dma_ptr;\n\n\tpud_dma_ptr = dma_alloc_wc(vdev->drm.dev, IVPU_MMU_PGTABLE_SIZE, &pud_dma, GFP_KERNEL);\n\tif (!pud_dma_ptr)\n\t\treturn NULL;\n\n\tdrm_WARN_ON(&vdev->drm, pgtable->pmd_ptrs[pgd_idx]);\n\tpgtable->pmd_ptrs[pgd_idx] = kzalloc(IVPU_MMU_PGTABLE_SIZE, GFP_KERNEL);\n\tif (!pgtable->pmd_ptrs[pgd_idx])\n\t\tgoto err_free_pud_dma_ptr;\n\n\tdrm_WARN_ON(&vdev->drm, pgtable->pte_ptrs[pgd_idx]);\n\tpgtable->pte_ptrs[pgd_idx] = kzalloc(IVPU_MMU_PGTABLE_SIZE, GFP_KERNEL);\n\tif (!pgtable->pte_ptrs[pgd_idx])\n\t\tgoto err_free_pmd_ptrs;\n\n\tpgtable->pud_ptrs[pgd_idx] = pud_dma_ptr;\n\tpgtable->pgd_dma_ptr[pgd_idx] = pud_dma | IVPU_MMU_ENTRY_VALID;\n\n\treturn pud_dma_ptr;\n\nerr_free_pmd_ptrs:\n\tkfree(pgtable->pmd_ptrs[pgd_idx]);\n\nerr_free_pud_dma_ptr:\n\tivpu_mmu_pgtable_free(vdev, pud_dma_ptr, pud_dma);\n\treturn NULL;\n}\n\nstatic u64*\nivpu_mmu_ensure_pmd(struct ivpu_device *vdev, struct ivpu_mmu_pgtable *pgtable, int pgd_idx,\n\t\t    int pud_idx)\n{\n\tu64 *pmd_dma_ptr = pgtable->pmd_ptrs[pgd_idx][pud_idx];\n\tdma_addr_t pmd_dma;\n\n\tif (pmd_dma_ptr)\n\t\treturn pmd_dma_ptr;\n\n\tpmd_dma_ptr = dma_alloc_wc(vdev->drm.dev, IVPU_MMU_PGTABLE_SIZE, &pmd_dma, GFP_KERNEL);\n\tif (!pmd_dma_ptr)\n\t\treturn NULL;\n\n\tdrm_WARN_ON(&vdev->drm, pgtable->pte_ptrs[pgd_idx][pud_idx]);\n\tpgtable->pte_ptrs[pgd_idx][pud_idx] = kzalloc(IVPU_MMU_PGTABLE_SIZE, GFP_KERNEL);\n\tif (!pgtable->pte_ptrs[pgd_idx][pud_idx])\n\t\tgoto err_free_pmd_dma_ptr;\n\n\tpgtable->pmd_ptrs[pgd_idx][pud_idx] = pmd_dma_ptr;\n\tpgtable->pud_ptrs[pgd_idx][pud_idx] = pmd_dma | IVPU_MMU_ENTRY_VALID;\n\n\treturn pmd_dma_ptr;\n\nerr_free_pmd_dma_ptr:\n\tivpu_mmu_pgtable_free(vdev, pmd_dma_ptr, pmd_dma);\n\treturn NULL;\n}\n\nstatic u64*\nivpu_mmu_ensure_pte(struct ivpu_device *vdev, struct ivpu_mmu_pgtable *pgtable,\n\t\t    int pgd_idx, int pud_idx, int pmd_idx)\n{\n\tu64 *pte_dma_ptr = pgtable->pte_ptrs[pgd_idx][pud_idx][pmd_idx];\n\tdma_addr_t pte_dma;\n\n\tif (pte_dma_ptr)\n\t\treturn pte_dma_ptr;\n\n\tpte_dma_ptr = dma_alloc_wc(vdev->drm.dev, IVPU_MMU_PGTABLE_SIZE, &pte_dma, GFP_KERNEL);\n\tif (!pte_dma_ptr)\n\t\treturn NULL;\n\n\tpgtable->pte_ptrs[pgd_idx][pud_idx][pmd_idx] = pte_dma_ptr;\n\tpgtable->pmd_ptrs[pgd_idx][pud_idx][pmd_idx] = pte_dma | IVPU_MMU_ENTRY_VALID;\n\n\treturn pte_dma_ptr;\n}\n\nstatic int\nivpu_mmu_context_map_page(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx,\n\t\t\t  u64 vpu_addr, dma_addr_t dma_addr, u64 prot)\n{\n\tu64 *pte;\n\tint pgd_idx = FIELD_GET(IVPU_MMU_PGD_INDEX_MASK, vpu_addr);\n\tint pud_idx = FIELD_GET(IVPU_MMU_PUD_INDEX_MASK, vpu_addr);\n\tint pmd_idx = FIELD_GET(IVPU_MMU_PMD_INDEX_MASK, vpu_addr);\n\tint pte_idx = FIELD_GET(IVPU_MMU_PTE_INDEX_MASK, vpu_addr);\n\n\t \n\tif (!ivpu_mmu_ensure_pud(vdev, &ctx->pgtable, pgd_idx))\n\t\treturn -ENOMEM;\n\n\t \n\tif (!ivpu_mmu_ensure_pmd(vdev, &ctx->pgtable, pgd_idx, pud_idx))\n\t\treturn -ENOMEM;\n\n\t \n\tpte = ivpu_mmu_ensure_pte(vdev, &ctx->pgtable, pgd_idx, pud_idx, pmd_idx);\n\tif (!pte)\n\t\treturn -ENOMEM;\n\n\t \n\tpte[pte_idx] = dma_addr | prot;\n\n\treturn 0;\n}\n\nstatic int\nivpu_mmu_context_map_cont_64k(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx, u64 vpu_addr,\n\t\t\t      dma_addr_t dma_addr, u64 prot)\n{\n\tsize_t size = IVPU_MMU_CONT_PAGES_SIZE;\n\n\tdrm_WARN_ON(&vdev->drm, !IS_ALIGNED(vpu_addr, size));\n\tdrm_WARN_ON(&vdev->drm, !IS_ALIGNED(dma_addr, size));\n\n\tprot |= IVPU_MMU_ENTRY_FLAG_CONT;\n\n\twhile (size) {\n\t\tint ret = ivpu_mmu_context_map_page(vdev, ctx, vpu_addr, dma_addr, prot);\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tsize -= IVPU_MMU_PAGE_SIZE;\n\t\tvpu_addr += IVPU_MMU_PAGE_SIZE;\n\t\tdma_addr += IVPU_MMU_PAGE_SIZE;\n\t}\n\n\treturn 0;\n}\n\nstatic void ivpu_mmu_context_unmap_page(struct ivpu_mmu_context *ctx, u64 vpu_addr)\n{\n\tint pgd_idx = FIELD_GET(IVPU_MMU_PGD_INDEX_MASK, vpu_addr);\n\tint pud_idx = FIELD_GET(IVPU_MMU_PUD_INDEX_MASK, vpu_addr);\n\tint pmd_idx = FIELD_GET(IVPU_MMU_PMD_INDEX_MASK, vpu_addr);\n\tint pte_idx = FIELD_GET(IVPU_MMU_PTE_INDEX_MASK, vpu_addr);\n\n\t \n\tctx->pgtable.pte_ptrs[pgd_idx][pud_idx][pmd_idx][pte_idx] = IVPU_MMU_ENTRY_INVALID;\n}\n\nstatic void\nivpu_mmu_context_flush_page_tables(struct ivpu_mmu_context *ctx, u64 vpu_addr, size_t size)\n{\n\tstruct ivpu_mmu_pgtable *pgtable = &ctx->pgtable;\n\tu64 end_addr = vpu_addr + size;\n\n\t \n\tvpu_addr &= ~(IVPU_MMU_PTE_MAP_SIZE - 1);\n\n\twhile (vpu_addr < end_addr) {\n\t\tint pgd_idx = FIELD_GET(IVPU_MMU_PGD_INDEX_MASK, vpu_addr);\n\t\tu64 pud_end = (pgd_idx + 1) * (u64)IVPU_MMU_PUD_MAP_SIZE;\n\n\t\twhile (vpu_addr < end_addr && vpu_addr < pud_end) {\n\t\t\tint pud_idx = FIELD_GET(IVPU_MMU_PUD_INDEX_MASK, vpu_addr);\n\t\t\tu64 pmd_end = (pud_idx + 1) * (u64)IVPU_MMU_PMD_MAP_SIZE;\n\n\t\t\twhile (vpu_addr < end_addr && vpu_addr < pmd_end) {\n\t\t\t\tint pmd_idx = FIELD_GET(IVPU_MMU_PMD_INDEX_MASK, vpu_addr);\n\n\t\t\t\tclflush_cache_range(pgtable->pte_ptrs[pgd_idx][pud_idx][pmd_idx],\n\t\t\t\t\t\t    IVPU_MMU_PGTABLE_SIZE);\n\t\t\t\tvpu_addr += IVPU_MMU_PTE_MAP_SIZE;\n\t\t\t}\n\t\t\tclflush_cache_range(pgtable->pmd_ptrs[pgd_idx][pud_idx],\n\t\t\t\t\t    IVPU_MMU_PGTABLE_SIZE);\n\t\t}\n\t\tclflush_cache_range(pgtable->pud_ptrs[pgd_idx], IVPU_MMU_PGTABLE_SIZE);\n\t}\n\tclflush_cache_range(pgtable->pgd_dma_ptr, IVPU_MMU_PGTABLE_SIZE);\n}\n\nstatic int\nivpu_mmu_context_map_pages(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx,\n\t\t\t   u64 vpu_addr, dma_addr_t dma_addr, size_t size, u64 prot)\n{\n\tint map_size;\n\tint ret;\n\n\twhile (size) {\n\t\tif (!ivpu_disable_mmu_cont_pages && size >= IVPU_MMU_CONT_PAGES_SIZE &&\n\t\t    IS_ALIGNED(vpu_addr | dma_addr, IVPU_MMU_CONT_PAGES_SIZE)) {\n\t\t\tret = ivpu_mmu_context_map_cont_64k(vdev, ctx, vpu_addr, dma_addr, prot);\n\t\t\tmap_size = IVPU_MMU_CONT_PAGES_SIZE;\n\t\t} else {\n\t\t\tret = ivpu_mmu_context_map_page(vdev, ctx, vpu_addr, dma_addr, prot);\n\t\t\tmap_size = IVPU_MMU_PAGE_SIZE;\n\t\t}\n\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvpu_addr += map_size;\n\t\tdma_addr += map_size;\n\t\tsize -= map_size;\n\t}\n\n\treturn 0;\n}\n\nstatic void ivpu_mmu_context_unmap_pages(struct ivpu_mmu_context *ctx, u64 vpu_addr, size_t size)\n{\n\twhile (size) {\n\t\tivpu_mmu_context_unmap_page(ctx, vpu_addr);\n\t\tvpu_addr += IVPU_MMU_PAGE_SIZE;\n\t\tsize -= IVPU_MMU_PAGE_SIZE;\n\t}\n}\n\nint\nivpu_mmu_context_map_sgt(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx,\n\t\t\t u64 vpu_addr, struct sg_table *sgt,  bool llc_coherent)\n{\n\tstruct scatterlist *sg;\n\tint ret;\n\tu64 prot;\n\tu64 i;\n\n\tif (!IS_ALIGNED(vpu_addr, IVPU_MMU_PAGE_SIZE))\n\t\treturn -EINVAL;\n\n\tif (vpu_addr & ~IVPU_MMU_VPU_ADDRESS_MASK)\n\t\treturn -EINVAL;\n\n\tprot = IVPU_MMU_ENTRY_MAPPED;\n\tif (llc_coherent)\n\t\tprot |= IVPU_MMU_ENTRY_FLAG_LLC_COHERENT;\n\n\tmutex_lock(&ctx->lock);\n\n\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\tdma_addr_t dma_addr = sg_dma_address(sg) - sg->offset;\n\t\tsize_t size = sg_dma_len(sg) + sg->offset;\n\n\t\tret = ivpu_mmu_context_map_pages(vdev, ctx, vpu_addr, dma_addr, size, prot);\n\t\tif (ret) {\n\t\t\tivpu_err(vdev, \"Failed to map context pages\\n\");\n\t\t\tmutex_unlock(&ctx->lock);\n\t\t\treturn ret;\n\t\t}\n\t\tivpu_mmu_context_flush_page_tables(ctx, vpu_addr, size);\n\t\tvpu_addr += size;\n\t}\n\n\tmutex_unlock(&ctx->lock);\n\n\tret = ivpu_mmu_invalidate_tlb(vdev, ctx->id);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to invalidate TLB for ctx %u: %d\\n\", ctx->id, ret);\n\treturn ret;\n}\n\nvoid\nivpu_mmu_context_unmap_sgt(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx,\n\t\t\t   u64 vpu_addr, struct sg_table *sgt)\n{\n\tstruct scatterlist *sg;\n\tint ret;\n\tu64 i;\n\n\tif (!IS_ALIGNED(vpu_addr, IVPU_MMU_PAGE_SIZE))\n\t\tivpu_warn(vdev, \"Unaligned vpu_addr: 0x%llx\\n\", vpu_addr);\n\n\tmutex_lock(&ctx->lock);\n\n\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\tsize_t size = sg_dma_len(sg) + sg->offset;\n\n\t\tivpu_mmu_context_unmap_pages(ctx, vpu_addr, size);\n\t\tivpu_mmu_context_flush_page_tables(ctx, vpu_addr, size);\n\t\tvpu_addr += size;\n\t}\n\n\tmutex_unlock(&ctx->lock);\n\n\tret = ivpu_mmu_invalidate_tlb(vdev, ctx->id);\n\tif (ret)\n\t\tivpu_warn(vdev, \"Failed to invalidate TLB for ctx %u: %d\\n\", ctx->id, ret);\n}\n\nint\nivpu_mmu_context_insert_node_locked(struct ivpu_mmu_context *ctx,\n\t\t\t\t    const struct ivpu_addr_range *range,\n\t\t\t\t    u64 size, struct drm_mm_node *node)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tif (!ivpu_disable_mmu_cont_pages && size >= IVPU_MMU_CONT_PAGES_SIZE) {\n\t\tif (!drm_mm_insert_node_in_range(&ctx->mm, node, size, IVPU_MMU_CONT_PAGES_SIZE, 0,\n\t\t\t\t\t\t range->start, range->end, DRM_MM_INSERT_BEST))\n\t\t\treturn 0;\n\t}\n\n\treturn drm_mm_insert_node_in_range(&ctx->mm, node, size, IVPU_MMU_PAGE_SIZE, 0,\n\t\t\t\t\t   range->start, range->end, DRM_MM_INSERT_BEST);\n}\n\nvoid\nivpu_mmu_context_remove_node_locked(struct ivpu_mmu_context *ctx, struct drm_mm_node *node)\n{\n\tlockdep_assert_held(&ctx->lock);\n\n\tdrm_mm_remove_node(node);\n}\n\nstatic int\nivpu_mmu_context_init(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx, u32 context_id)\n{\n\tu64 start, end;\n\tint ret;\n\n\tmutex_init(&ctx->lock);\n\tINIT_LIST_HEAD(&ctx->bo_list);\n\n\tret = ivpu_mmu_pgtable_init(vdev, &ctx->pgtable);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!context_id) {\n\t\tstart = vdev->hw->ranges.global.start;\n\t\tend = vdev->hw->ranges.shave.end;\n\t} else {\n\t\tstart = vdev->hw->ranges.user.start;\n\t\tend = vdev->hw->ranges.dma.end;\n\t}\n\n\tdrm_mm_init(&ctx->mm, start, end - start);\n\tctx->id = context_id;\n\n\treturn 0;\n}\n\nstatic void ivpu_mmu_context_fini(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx)\n{\n\tif (drm_WARN_ON(&vdev->drm, !ctx->pgtable.pgd_dma_ptr))\n\t\treturn;\n\n\tmutex_destroy(&ctx->lock);\n\tivpu_mmu_pgtables_free(vdev, &ctx->pgtable);\n\tdrm_mm_takedown(&ctx->mm);\n\n\tctx->pgtable.pgd_dma_ptr = NULL;\n\tctx->pgtable.pgd_dma = 0;\n}\n\nint ivpu_mmu_global_context_init(struct ivpu_device *vdev)\n{\n\treturn ivpu_mmu_context_init(vdev, &vdev->gctx, IVPU_GLOBAL_CONTEXT_MMU_SSID);\n}\n\nvoid ivpu_mmu_global_context_fini(struct ivpu_device *vdev)\n{\n\treturn ivpu_mmu_context_fini(vdev, &vdev->gctx);\n}\n\nvoid ivpu_mmu_user_context_mark_invalid(struct ivpu_device *vdev, u32 ssid)\n{\n\tstruct ivpu_file_priv *file_priv;\n\n\txa_lock(&vdev->context_xa);\n\n\tfile_priv = xa_load(&vdev->context_xa, ssid);\n\tif (file_priv)\n\t\tfile_priv->has_mmu_faults = true;\n\n\txa_unlock(&vdev->context_xa);\n}\n\nint ivpu_mmu_user_context_init(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx, u32 ctx_id)\n{\n\tint ret;\n\n\tdrm_WARN_ON(&vdev->drm, !ctx_id);\n\n\tret = ivpu_mmu_context_init(vdev, ctx, ctx_id);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to initialize context: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = ivpu_mmu_set_pgtable(vdev, ctx_id, &ctx->pgtable);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to set page table: %d\\n\", ret);\n\t\tgoto err_context_fini;\n\t}\n\n\treturn 0;\n\nerr_context_fini:\n\tivpu_mmu_context_fini(vdev, ctx);\n\treturn ret;\n}\n\nvoid ivpu_mmu_user_context_fini(struct ivpu_device *vdev, struct ivpu_mmu_context *ctx)\n{\n\tdrm_WARN_ON(&vdev->drm, !ctx->id);\n\n\tivpu_mmu_clear_pgtable(vdev, ctx->id);\n\tivpu_mmu_context_fini(vdev, ctx);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}