{
  "module_name": "ivpu_gem.c",
  "hash_id": "bc334fc2d24a1508aa5d5a3c256048623ebd518ca767f9932d10e25f301f6859",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/ivpu/ivpu_gem.c",
  "human_readable_source": "\n \n\n#include <linux/dma-buf.h>\n#include <linux/highmem.h>\n#include <linux/module.h>\n#include <linux/set_memory.h>\n#include <linux/xarray.h>\n\n#include <drm/drm_cache.h>\n#include <drm/drm_debugfs.h>\n#include <drm/drm_file.h>\n#include <drm/drm_utils.h>\n\n#include \"ivpu_drv.h\"\n#include \"ivpu_gem.h\"\n#include \"ivpu_hw.h\"\n#include \"ivpu_mmu.h\"\n#include \"ivpu_mmu_context.h\"\n\nMODULE_IMPORT_NS(DMA_BUF);\n\nstatic const struct drm_gem_object_funcs ivpu_gem_funcs;\n\nstatic struct lock_class_key prime_bo_lock_class_key;\n\nstatic int __must_check prime_alloc_pages_locked(struct ivpu_bo *bo)\n{\n\t \n\treturn 0;\n}\n\nstatic void prime_free_pages_locked(struct ivpu_bo *bo)\n{\n\t \n}\n\nstatic int prime_map_pages_locked(struct ivpu_bo *bo)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tstruct sg_table *sgt;\n\n\tsgt = dma_buf_map_attachment_unlocked(bo->base.import_attach, DMA_BIDIRECTIONAL);\n\tif (IS_ERR(sgt)) {\n\t\tivpu_err(vdev, \"Failed to map attachment: %ld\\n\", PTR_ERR(sgt));\n\t\treturn PTR_ERR(sgt);\n\t}\n\n\tbo->sgt = sgt;\n\treturn 0;\n}\n\nstatic void prime_unmap_pages_locked(struct ivpu_bo *bo)\n{\n\tdma_buf_unmap_attachment_unlocked(bo->base.import_attach, bo->sgt, DMA_BIDIRECTIONAL);\n\tbo->sgt = NULL;\n}\n\nstatic const struct ivpu_bo_ops prime_ops = {\n\t.type = IVPU_BO_TYPE_PRIME,\n\t.name = \"prime\",\n\t.alloc_pages = prime_alloc_pages_locked,\n\t.free_pages = prime_free_pages_locked,\n\t.map_pages = prime_map_pages_locked,\n\t.unmap_pages = prime_unmap_pages_locked,\n};\n\nstatic int __must_check shmem_alloc_pages_locked(struct ivpu_bo *bo)\n{\n\tint npages = bo->base.size >> PAGE_SHIFT;\n\tstruct page **pages;\n\n\tpages = drm_gem_get_pages(&bo->base);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\tif (bo->flags & DRM_IVPU_BO_WC)\n\t\tset_pages_array_wc(pages, npages);\n\telse if (bo->flags & DRM_IVPU_BO_UNCACHED)\n\t\tset_pages_array_uc(pages, npages);\n\n\tbo->pages = pages;\n\treturn 0;\n}\n\nstatic void shmem_free_pages_locked(struct ivpu_bo *bo)\n{\n\tif (ivpu_bo_cache_mode(bo) != DRM_IVPU_BO_CACHED)\n\t\tset_pages_array_wb(bo->pages, bo->base.size >> PAGE_SHIFT);\n\n\tdrm_gem_put_pages(&bo->base, bo->pages, true, false);\n\tbo->pages = NULL;\n}\n\nstatic int ivpu_bo_map_pages_locked(struct ivpu_bo *bo)\n{\n\tint npages = bo->base.size >> PAGE_SHIFT;\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tstruct sg_table *sgt;\n\tint ret;\n\n\tsgt = drm_prime_pages_to_sg(&vdev->drm, bo->pages, npages);\n\tif (IS_ERR(sgt)) {\n\t\tivpu_err(vdev, \"Failed to allocate sgtable\\n\");\n\t\treturn PTR_ERR(sgt);\n\t}\n\n\tret = dma_map_sgtable(vdev->drm.dev, sgt, DMA_BIDIRECTIONAL, 0);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to map BO in IOMMU: %d\\n\", ret);\n\t\tgoto err_free_sgt;\n\t}\n\n\tbo->sgt = sgt;\n\treturn 0;\n\nerr_free_sgt:\n\tkfree(sgt);\n\treturn ret;\n}\n\nstatic void ivpu_bo_unmap_pages_locked(struct ivpu_bo *bo)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\n\tdma_unmap_sgtable(vdev->drm.dev, bo->sgt, DMA_BIDIRECTIONAL, 0);\n\tsg_free_table(bo->sgt);\n\tkfree(bo->sgt);\n\tbo->sgt = NULL;\n}\n\nstatic const struct ivpu_bo_ops shmem_ops = {\n\t.type = IVPU_BO_TYPE_SHMEM,\n\t.name = \"shmem\",\n\t.alloc_pages = shmem_alloc_pages_locked,\n\t.free_pages = shmem_free_pages_locked,\n\t.map_pages = ivpu_bo_map_pages_locked,\n\t.unmap_pages = ivpu_bo_unmap_pages_locked,\n};\n\nstatic int __must_check internal_alloc_pages_locked(struct ivpu_bo *bo)\n{\n\tunsigned int i, npages = bo->base.size >> PAGE_SHIFT;\n\tstruct page **pages;\n\tint ret;\n\n\tpages = kvmalloc_array(npages, sizeof(*bo->pages), GFP_KERNEL);\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpages[i] = alloc_page(GFP_KERNEL | __GFP_HIGHMEM | __GFP_ZERO);\n\t\tif (!pages[i]) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_free_pages;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\tbo->pages = pages;\n\treturn 0;\n\nerr_free_pages:\n\twhile (i--)\n\t\tput_page(pages[i]);\n\tkvfree(pages);\n\treturn ret;\n}\n\nstatic void internal_free_pages_locked(struct ivpu_bo *bo)\n{\n\tunsigned int i, npages = bo->base.size >> PAGE_SHIFT;\n\n\tif (ivpu_bo_cache_mode(bo) != DRM_IVPU_BO_CACHED)\n\t\tset_pages_array_wb(bo->pages, bo->base.size >> PAGE_SHIFT);\n\n\tfor (i = 0; i < npages; i++)\n\t\tput_page(bo->pages[i]);\n\n\tkvfree(bo->pages);\n\tbo->pages = NULL;\n}\n\nstatic const struct ivpu_bo_ops internal_ops = {\n\t.type = IVPU_BO_TYPE_INTERNAL,\n\t.name = \"internal\",\n\t.alloc_pages = internal_alloc_pages_locked,\n\t.free_pages = internal_free_pages_locked,\n\t.map_pages = ivpu_bo_map_pages_locked,\n\t.unmap_pages = ivpu_bo_unmap_pages_locked,\n};\n\nstatic int __must_check ivpu_bo_alloc_and_map_pages_locked(struct ivpu_bo *bo)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tint ret;\n\n\tlockdep_assert_held(&bo->lock);\n\tdrm_WARN_ON(&vdev->drm, bo->sgt);\n\n\tret = bo->ops->alloc_pages(bo);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to allocate pages for BO: %d\", ret);\n\t\treturn ret;\n\t}\n\n\tret = bo->ops->map_pages(bo);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to map pages for BO: %d\", ret);\n\t\tgoto err_free_pages;\n\t}\n\treturn ret;\n\nerr_free_pages:\n\tbo->ops->free_pages(bo);\n\treturn ret;\n}\n\nstatic void ivpu_bo_unmap_and_free_pages(struct ivpu_bo *bo)\n{\n\tmutex_lock(&bo->lock);\n\n\tWARN_ON(!bo->sgt);\n\tbo->ops->unmap_pages(bo);\n\tWARN_ON(bo->sgt);\n\tbo->ops->free_pages(bo);\n\tWARN_ON(bo->pages);\n\n\tmutex_unlock(&bo->lock);\n}\n\n \nint __must_check ivpu_bo_pin(struct ivpu_bo *bo)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tint ret = 0;\n\n\tmutex_lock(&bo->lock);\n\n\tif (!bo->vpu_addr) {\n\t\tivpu_err(vdev, \"vpu_addr not set for BO ctx_id: %d handle: %d\\n\",\n\t\t\t bo->ctx->id, bo->handle);\n\t\tret = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tif (!bo->sgt) {\n\t\tret = ivpu_bo_alloc_and_map_pages_locked(bo);\n\t\tif (ret)\n\t\t\tgoto unlock;\n\t}\n\n\tif (!bo->mmu_mapped) {\n\t\tret = ivpu_mmu_context_map_sgt(vdev, bo->ctx, bo->vpu_addr, bo->sgt,\n\t\t\t\t\t       ivpu_bo_is_snooped(bo));\n\t\tif (ret) {\n\t\t\tivpu_err(vdev, \"Failed to map BO in MMU: %d\\n\", ret);\n\t\t\tgoto unlock;\n\t\t}\n\t\tbo->mmu_mapped = true;\n\t}\n\nunlock:\n\tmutex_unlock(&bo->lock);\n\n\treturn ret;\n}\n\nstatic int\nivpu_bo_alloc_vpu_addr(struct ivpu_bo *bo, struct ivpu_mmu_context *ctx,\n\t\t       const struct ivpu_addr_range *range)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tint ret;\n\n\tif (!range) {\n\t\tif (bo->flags & DRM_IVPU_BO_SHAVE_MEM)\n\t\t\trange = &vdev->hw->ranges.shave;\n\t\telse if (bo->flags & DRM_IVPU_BO_DMA_MEM)\n\t\t\trange = &vdev->hw->ranges.dma;\n\t\telse\n\t\t\trange = &vdev->hw->ranges.user;\n\t}\n\n\tmutex_lock(&ctx->lock);\n\tret = ivpu_mmu_context_insert_node_locked(ctx, range, bo->base.size, &bo->mm_node);\n\tif (!ret) {\n\t\tbo->ctx = ctx;\n\t\tbo->vpu_addr = bo->mm_node.start;\n\t\tlist_add_tail(&bo->ctx_node, &ctx->bo_list);\n\t}\n\tmutex_unlock(&ctx->lock);\n\n\treturn ret;\n}\n\nstatic void ivpu_bo_free_vpu_addr(struct ivpu_bo *bo)\n{\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\tstruct ivpu_mmu_context *ctx = bo->ctx;\n\n\tivpu_dbg(vdev, BO, \"remove from ctx: ctx %d vpu_addr 0x%llx allocated %d mmu_mapped %d\\n\",\n\t\t ctx->id, bo->vpu_addr, (bool)bo->sgt, bo->mmu_mapped);\n\n\tmutex_lock(&bo->lock);\n\n\tif (bo->mmu_mapped) {\n\t\tdrm_WARN_ON(&vdev->drm, !bo->sgt);\n\t\tivpu_mmu_context_unmap_sgt(vdev, ctx, bo->vpu_addr, bo->sgt);\n\t\tbo->mmu_mapped = false;\n\t}\n\n\tmutex_lock(&ctx->lock);\n\tlist_del(&bo->ctx_node);\n\tbo->vpu_addr = 0;\n\tbo->ctx = NULL;\n\tivpu_mmu_context_remove_node_locked(ctx, &bo->mm_node);\n\tmutex_unlock(&ctx->lock);\n\n\tmutex_unlock(&bo->lock);\n}\n\nvoid ivpu_bo_remove_all_bos_from_context(struct ivpu_mmu_context *ctx)\n{\n\tstruct ivpu_bo *bo, *tmp;\n\n\tlist_for_each_entry_safe(bo, tmp, &ctx->bo_list, ctx_node)\n\t\tivpu_bo_free_vpu_addr(bo);\n}\n\nstatic struct ivpu_bo *\nivpu_bo_alloc(struct ivpu_device *vdev, struct ivpu_mmu_context *mmu_context,\n\t      u64 size, u32 flags, const struct ivpu_bo_ops *ops,\n\t      const struct ivpu_addr_range *range, u64 user_ptr)\n{\n\tstruct ivpu_bo *bo;\n\tint ret = 0;\n\n\tif (drm_WARN_ON(&vdev->drm, size == 0 || !PAGE_ALIGNED(size)))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tswitch (flags & DRM_IVPU_BO_CACHE_MASK) {\n\tcase DRM_IVPU_BO_CACHED:\n\tcase DRM_IVPU_BO_UNCACHED:\n\tcase DRM_IVPU_BO_WC:\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tbo = kzalloc(sizeof(*bo), GFP_KERNEL);\n\tif (!bo)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&bo->lock);\n\tbo->base.funcs = &ivpu_gem_funcs;\n\tbo->flags = flags;\n\tbo->ops = ops;\n\tbo->user_ptr = user_ptr;\n\n\tif (ops->type == IVPU_BO_TYPE_SHMEM)\n\t\tret = drm_gem_object_init(&vdev->drm, &bo->base, size);\n\telse\n\t\tdrm_gem_private_object_init(&vdev->drm, &bo->base, size);\n\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to initialize drm object\\n\");\n\t\tgoto err_free;\n\t}\n\n\tif (flags & DRM_IVPU_BO_MAPPABLE) {\n\t\tret = drm_gem_create_mmap_offset(&bo->base);\n\t\tif (ret) {\n\t\t\tivpu_err(vdev, \"Failed to allocate mmap offset\\n\");\n\t\t\tgoto err_release;\n\t\t}\n\t}\n\n\tif (mmu_context) {\n\t\tret = ivpu_bo_alloc_vpu_addr(bo, mmu_context, range);\n\t\tif (ret) {\n\t\t\tivpu_err(vdev, \"Failed to add BO to context: %d\\n\", ret);\n\t\t\tgoto err_release;\n\t\t}\n\t}\n\n\treturn bo;\n\nerr_release:\n\tdrm_gem_object_release(&bo->base);\nerr_free:\n\tkfree(bo);\n\treturn ERR_PTR(ret);\n}\n\nstatic void ivpu_bo_free(struct drm_gem_object *obj)\n{\n\tstruct ivpu_bo *bo = to_ivpu_bo(obj);\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\n\tif (bo->ctx)\n\t\tivpu_dbg(vdev, BO, \"free: ctx %d vpu_addr 0x%llx allocated %d mmu_mapped %d\\n\",\n\t\t\t bo->ctx->id, bo->vpu_addr, (bool)bo->sgt, bo->mmu_mapped);\n\telse\n\t\tivpu_dbg(vdev, BO, \"free: ctx (released) allocated %d mmu_mapped %d\\n\",\n\t\t\t (bool)bo->sgt, bo->mmu_mapped);\n\n\tdrm_WARN_ON(&vdev->drm, !dma_resv_test_signaled(obj->resv, DMA_RESV_USAGE_READ));\n\n\tvunmap(bo->kvaddr);\n\n\tif (bo->ctx)\n\t\tivpu_bo_free_vpu_addr(bo);\n\n\tif (bo->sgt)\n\t\tivpu_bo_unmap_and_free_pages(bo);\n\n\tif (bo->base.import_attach)\n\t\tdrm_prime_gem_destroy(&bo->base, bo->sgt);\n\n\tdrm_gem_object_release(&bo->base);\n\n\tmutex_destroy(&bo->lock);\n\tkfree(bo);\n}\n\nstatic int ivpu_bo_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct ivpu_bo *bo = to_ivpu_bo(obj);\n\tstruct ivpu_device *vdev = ivpu_bo_to_vdev(bo);\n\n\tivpu_dbg(vdev, BO, \"mmap: ctx %u handle %u vpu_addr 0x%llx size %zu type %s\",\n\t\t bo->ctx->id, bo->handle, bo->vpu_addr, bo->base.size, bo->ops->name);\n\n\tif (obj->import_attach) {\n\t\t \n\t\tdrm_gem_object_put(obj);\n\t\tvma->vm_private_data = NULL;\n\t\treturn dma_buf_mmap(obj->dma_buf, vma, 0);\n\t}\n\n\tvm_flags_set(vma, VM_PFNMAP | VM_DONTEXPAND);\n\tvma->vm_page_prot = ivpu_bo_pgprot(bo, vm_get_page_prot(vma->vm_flags));\n\n\treturn 0;\n}\n\nstatic struct sg_table *ivpu_bo_get_sg_table(struct drm_gem_object *obj)\n{\n\tstruct ivpu_bo *bo = to_ivpu_bo(obj);\n\tloff_t npages = obj->size >> PAGE_SHIFT;\n\tint ret = 0;\n\n\tmutex_lock(&bo->lock);\n\n\tif (!bo->sgt)\n\t\tret = ivpu_bo_alloc_and_map_pages_locked(bo);\n\n\tmutex_unlock(&bo->lock);\n\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn drm_prime_pages_to_sg(obj->dev, bo->pages, npages);\n}\n\nstatic vm_fault_t ivpu_vm_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct drm_gem_object *obj = vma->vm_private_data;\n\tstruct ivpu_bo *bo = to_ivpu_bo(obj);\n\tloff_t npages = obj->size >> PAGE_SHIFT;\n\tpgoff_t page_offset;\n\tstruct page *page;\n\tvm_fault_t ret;\n\tint err;\n\n\tmutex_lock(&bo->lock);\n\n\tif (!bo->sgt) {\n\t\terr = ivpu_bo_alloc_and_map_pages_locked(bo);\n\t\tif (err) {\n\t\t\tret = vmf_error(err);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\t \n\tpage_offset = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\n\tif (page_offset >= npages) {\n\t\tret = VM_FAULT_SIGBUS;\n\t} else {\n\t\tpage = bo->pages[page_offset];\n\t\tret = vmf_insert_pfn(vma, vmf->address, page_to_pfn(page));\n\t}\n\nunlock:\n\tmutex_unlock(&bo->lock);\n\n\treturn ret;\n}\n\nstatic const struct vm_operations_struct ivpu_vm_ops = {\n\t.fault = ivpu_vm_fault,\n\t.open = drm_gem_vm_open,\n\t.close = drm_gem_vm_close,\n};\n\nstatic const struct drm_gem_object_funcs ivpu_gem_funcs = {\n\t.free = ivpu_bo_free,\n\t.mmap = ivpu_bo_mmap,\n\t.vm_ops = &ivpu_vm_ops,\n\t.get_sg_table = ivpu_bo_get_sg_table,\n};\n\nint\nivpu_bo_create_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct ivpu_file_priv *file_priv = file->driver_priv;\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct drm_ivpu_bo_create *args = data;\n\tu64 size = PAGE_ALIGN(args->size);\n\tstruct ivpu_bo *bo;\n\tint ret;\n\n\tif (args->flags & ~DRM_IVPU_BO_FLAGS)\n\t\treturn -EINVAL;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tbo = ivpu_bo_alloc(vdev, &file_priv->ctx, size, args->flags, &shmem_ops, NULL, 0);\n\tif (IS_ERR(bo)) {\n\t\tivpu_err(vdev, \"Failed to create BO: %pe (ctx %u size %llu flags 0x%x)\",\n\t\t\t bo, file_priv->ctx.id, args->size, args->flags);\n\t\treturn PTR_ERR(bo);\n\t}\n\n\tret = drm_gem_handle_create(file, &bo->base, &bo->handle);\n\tif (!ret) {\n\t\targs->vpu_addr = bo->vpu_addr;\n\t\targs->handle = bo->handle;\n\t}\n\n\tdrm_gem_object_put(&bo->base);\n\n\tivpu_dbg(vdev, BO, \"alloc shmem: ctx %u vpu_addr 0x%llx size %zu flags 0x%x\\n\",\n\t\t file_priv->ctx.id, bo->vpu_addr, bo->base.size, bo->flags);\n\n\treturn ret;\n}\n\nstruct ivpu_bo *\nivpu_bo_alloc_internal(struct ivpu_device *vdev, u64 vpu_addr, u64 size, u32 flags)\n{\n\tconst struct ivpu_addr_range *range;\n\tstruct ivpu_addr_range fixed_range;\n\tstruct ivpu_bo *bo;\n\tpgprot_t prot;\n\tint ret;\n\n\tdrm_WARN_ON(&vdev->drm, !PAGE_ALIGNED(vpu_addr));\n\tdrm_WARN_ON(&vdev->drm, !PAGE_ALIGNED(size));\n\n\tif (vpu_addr) {\n\t\tfixed_range.start = vpu_addr;\n\t\tfixed_range.end = vpu_addr + size;\n\t\trange = &fixed_range;\n\t} else {\n\t\trange = &vdev->hw->ranges.global;\n\t}\n\n\tbo = ivpu_bo_alloc(vdev, &vdev->gctx, size, flags, &internal_ops, range, 0);\n\tif (IS_ERR(bo)) {\n\t\tivpu_err(vdev, \"Failed to create BO: %pe (vpu_addr 0x%llx size %llu flags 0x%x)\",\n\t\t\t bo, vpu_addr, size, flags);\n\t\treturn NULL;\n\t}\n\n\tret = ivpu_bo_pin(bo);\n\tif (ret)\n\t\tgoto err_put;\n\n\tif (ivpu_bo_cache_mode(bo) != DRM_IVPU_BO_CACHED)\n\t\tdrm_clflush_pages(bo->pages, bo->base.size >> PAGE_SHIFT);\n\n\tif (bo->flags & DRM_IVPU_BO_WC)\n\t\tset_pages_array_wc(bo->pages, bo->base.size >> PAGE_SHIFT);\n\telse if (bo->flags & DRM_IVPU_BO_UNCACHED)\n\t\tset_pages_array_uc(bo->pages, bo->base.size >> PAGE_SHIFT);\n\n\tprot = ivpu_bo_pgprot(bo, PAGE_KERNEL);\n\tbo->kvaddr = vmap(bo->pages, bo->base.size >> PAGE_SHIFT, VM_MAP, prot);\n\tif (!bo->kvaddr) {\n\t\tivpu_err(vdev, \"Failed to map BO into kernel virtual memory\\n\");\n\t\tgoto err_put;\n\t}\n\n\tivpu_dbg(vdev, BO, \"alloc internal: ctx 0 vpu_addr 0x%llx size %zu flags 0x%x\\n\",\n\t\t bo->vpu_addr, bo->base.size, flags);\n\n\treturn bo;\n\nerr_put:\n\tdrm_gem_object_put(&bo->base);\n\treturn NULL;\n}\n\nvoid ivpu_bo_free_internal(struct ivpu_bo *bo)\n{\n\tdrm_gem_object_put(&bo->base);\n}\n\nstruct drm_gem_object *ivpu_gem_prime_import(struct drm_device *dev, struct dma_buf *buf)\n{\n\tstruct ivpu_device *vdev = to_ivpu_device(dev);\n\tstruct dma_buf_attachment *attach;\n\tstruct ivpu_bo *bo;\n\n\tattach = dma_buf_attach(buf, dev->dev);\n\tif (IS_ERR(attach))\n\t\treturn ERR_CAST(attach);\n\n\tget_dma_buf(buf);\n\n\tbo = ivpu_bo_alloc(vdev, NULL, buf->size, DRM_IVPU_BO_MAPPABLE, &prime_ops, NULL, 0);\n\tif (IS_ERR(bo)) {\n\t\tivpu_err(vdev, \"Failed to import BO: %pe (size %lu)\", bo, buf->size);\n\t\tgoto err_detach;\n\t}\n\n\tlockdep_set_class(&bo->lock, &prime_bo_lock_class_key);\n\n\tbo->base.import_attach = attach;\n\n\treturn &bo->base;\n\nerr_detach:\n\tdma_buf_detach(buf, attach);\n\tdma_buf_put(buf);\n\treturn ERR_CAST(bo);\n}\n\nint ivpu_bo_info_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct ivpu_file_priv *file_priv = file->driver_priv;\n\tstruct ivpu_device *vdev = to_ivpu_device(dev);\n\tstruct drm_ivpu_bo_info *args = data;\n\tstruct drm_gem_object *obj;\n\tstruct ivpu_bo *bo;\n\tint ret = 0;\n\n\tobj = drm_gem_object_lookup(file, args->handle);\n\tif (!obj)\n\t\treturn -ENOENT;\n\n\tbo = to_ivpu_bo(obj);\n\n\tmutex_lock(&bo->lock);\n\n\tif (!bo->ctx) {\n\t\tret = ivpu_bo_alloc_vpu_addr(bo, &file_priv->ctx, NULL);\n\t\tif (ret) {\n\t\t\tivpu_err(vdev, \"Failed to allocate vpu_addr: %d\\n\", ret);\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\targs->flags = bo->flags;\n\targs->mmap_offset = drm_vma_node_offset_addr(&obj->vma_node);\n\targs->vpu_addr = bo->vpu_addr;\n\targs->size = obj->size;\nunlock:\n\tmutex_unlock(&bo->lock);\n\tdrm_gem_object_put(obj);\n\treturn ret;\n}\n\nint ivpu_bo_wait_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct drm_ivpu_bo_wait *args = data;\n\tstruct drm_gem_object *obj;\n\tunsigned long timeout;\n\tlong ret;\n\n\ttimeout = drm_timeout_abs_to_jiffies(args->timeout_ns);\n\n\tobj = drm_gem_object_lookup(file, args->handle);\n\tif (!obj)\n\t\treturn -EINVAL;\n\n\tret = dma_resv_wait_timeout(obj->resv, DMA_RESV_USAGE_READ, true, timeout);\n\tif (ret == 0) {\n\t\tret = -ETIMEDOUT;\n\t} else if (ret > 0) {\n\t\tret = 0;\n\t\targs->job_status = to_ivpu_bo(obj)->job_status;\n\t}\n\n\tdrm_gem_object_put(obj);\n\n\treturn ret;\n}\n\nstatic void ivpu_bo_print_info(struct ivpu_bo *bo, struct drm_printer *p)\n{\n\tunsigned long dma_refcount = 0;\n\n\tif (bo->base.dma_buf && bo->base.dma_buf->file)\n\t\tdma_refcount = atomic_long_read(&bo->base.dma_buf->file->f_count);\n\n\tdrm_printf(p, \"%5u %6d %16llx %10lu %10u %12lu %14s\\n\",\n\t\t   bo->ctx->id, bo->handle, bo->vpu_addr, bo->base.size,\n\t\t   kref_read(&bo->base.refcount), dma_refcount, bo->ops->name);\n}\n\nvoid ivpu_bo_list(struct drm_device *dev, struct drm_printer *p)\n{\n\tstruct ivpu_device *vdev = to_ivpu_device(dev);\n\tstruct ivpu_file_priv *file_priv;\n\tunsigned long ctx_id;\n\tstruct ivpu_bo *bo;\n\n\tdrm_printf(p, \"%5s %6s %16s %10s %10s %12s %14s\\n\",\n\t\t   \"ctx\", \"handle\", \"vpu_addr\", \"size\", \"refcount\", \"dma_refcount\", \"type\");\n\n\tmutex_lock(&vdev->gctx.lock);\n\tlist_for_each_entry(bo, &vdev->gctx.bo_list, ctx_node)\n\t\tivpu_bo_print_info(bo, p);\n\tmutex_unlock(&vdev->gctx.lock);\n\n\txa_for_each(&vdev->context_xa, ctx_id, file_priv) {\n\t\tfile_priv = ivpu_file_priv_get_by_ctx_id(vdev, ctx_id);\n\t\tif (!file_priv)\n\t\t\tcontinue;\n\n\t\tmutex_lock(&file_priv->ctx.lock);\n\t\tlist_for_each_entry(bo, &file_priv->ctx.bo_list, ctx_node)\n\t\t\tivpu_bo_print_info(bo, p);\n\t\tmutex_unlock(&file_priv->ctx.lock);\n\n\t\tivpu_file_priv_put(&file_priv);\n\t}\n}\n\nvoid ivpu_bo_list_print(struct drm_device *dev)\n{\n\tstruct drm_printer p = drm_info_printer(dev->dev);\n\n\tivpu_bo_list(dev, &p);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}