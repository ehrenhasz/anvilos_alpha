{
  "module_name": "ivpu_job.c",
  "hash_id": "f1c6d35aa9868b263aa778a5563d21c5294dbacab008b23a6783e7d79d38ef2f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/ivpu/ivpu_job.c",
  "human_readable_source": "\n \n\n#include <drm/drm_file.h>\n\n#include <linux/bitfield.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/pci.h>\n#include <linux/module.h>\n#include <uapi/drm/ivpu_accel.h>\n\n#include \"ivpu_drv.h\"\n#include \"ivpu_hw.h\"\n#include \"ivpu_ipc.h\"\n#include \"ivpu_job.h\"\n#include \"ivpu_jsm_msg.h\"\n#include \"ivpu_pm.h\"\n\n#define CMD_BUF_IDX\t     0\n#define JOB_ID_JOB_MASK\t     GENMASK(7, 0)\n#define JOB_ID_CONTEXT_MASK  GENMASK(31, 8)\n#define JOB_MAX_BUFFER_COUNT 65535\n\nstatic unsigned int ivpu_tdr_timeout_ms;\nmodule_param_named(tdr_timeout_ms, ivpu_tdr_timeout_ms, uint, 0644);\nMODULE_PARM_DESC(tdr_timeout_ms, \"Timeout for device hang detection, in milliseconds, 0 - default\");\n\nstatic void ivpu_cmdq_ring_db(struct ivpu_device *vdev, struct ivpu_cmdq *cmdq)\n{\n\tivpu_hw_reg_db_set(vdev, cmdq->db_id);\n}\n\nstatic struct ivpu_cmdq *ivpu_cmdq_alloc(struct ivpu_file_priv *file_priv, u16 engine)\n{\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct vpu_job_queue_header *jobq_header;\n\tstruct ivpu_cmdq *cmdq;\n\n\tcmdq = kzalloc(sizeof(*cmdq), GFP_KERNEL);\n\tif (!cmdq)\n\t\treturn NULL;\n\n\tcmdq->mem = ivpu_bo_alloc_internal(vdev, 0, SZ_4K, DRM_IVPU_BO_WC);\n\tif (!cmdq->mem)\n\t\tgoto cmdq_free;\n\n\tcmdq->db_id = file_priv->ctx.id + engine * ivpu_get_context_count(vdev);\n\tcmdq->entry_count = (u32)((cmdq->mem->base.size - sizeof(struct vpu_job_queue_header)) /\n\t\t\t\t  sizeof(struct vpu_job_queue_entry));\n\n\tcmdq->jobq = (struct vpu_job_queue *)cmdq->mem->kvaddr;\n\tjobq_header = &cmdq->jobq->header;\n\tjobq_header->engine_idx = engine;\n\tjobq_header->head = 0;\n\tjobq_header->tail = 0;\n\twmb();  \n\n\treturn cmdq;\n\ncmdq_free:\n\tkfree(cmdq);\n\treturn NULL;\n}\n\nstatic void ivpu_cmdq_free(struct ivpu_file_priv *file_priv, struct ivpu_cmdq *cmdq)\n{\n\tif (!cmdq)\n\t\treturn;\n\n\tivpu_bo_free_internal(cmdq->mem);\n\tkfree(cmdq);\n}\n\nstatic struct ivpu_cmdq *ivpu_cmdq_acquire(struct ivpu_file_priv *file_priv, u16 engine)\n{\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct ivpu_cmdq *cmdq = file_priv->cmdq[engine];\n\tint ret;\n\n\tlockdep_assert_held(&file_priv->lock);\n\n\tif (!cmdq) {\n\t\tcmdq = ivpu_cmdq_alloc(file_priv, engine);\n\t\tif (!cmdq)\n\t\t\treturn NULL;\n\t\tfile_priv->cmdq[engine] = cmdq;\n\t}\n\n\tif (cmdq->db_registered)\n\t\treturn cmdq;\n\n\tret = ivpu_jsm_register_db(vdev, file_priv->ctx.id, cmdq->db_id,\n\t\t\t\t   cmdq->mem->vpu_addr, cmdq->mem->base.size);\n\tif (ret)\n\t\treturn NULL;\n\n\tcmdq->db_registered = true;\n\n\treturn cmdq;\n}\n\nstatic void ivpu_cmdq_release_locked(struct ivpu_file_priv *file_priv, u16 engine)\n{\n\tstruct ivpu_cmdq *cmdq = file_priv->cmdq[engine];\n\n\tlockdep_assert_held(&file_priv->lock);\n\n\tif (cmdq) {\n\t\tfile_priv->cmdq[engine] = NULL;\n\t\tif (cmdq->db_registered)\n\t\t\tivpu_jsm_unregister_db(file_priv->vdev, cmdq->db_id);\n\n\t\tivpu_cmdq_free(file_priv, cmdq);\n\t}\n}\n\nvoid ivpu_cmdq_release_all(struct ivpu_file_priv *file_priv)\n{\n\tint i;\n\n\tmutex_lock(&file_priv->lock);\n\n\tfor (i = 0; i < IVPU_NUM_ENGINES; i++)\n\t\tivpu_cmdq_release_locked(file_priv, i);\n\n\tmutex_unlock(&file_priv->lock);\n}\n\n \nstatic void ivpu_cmdq_reset_locked(struct ivpu_file_priv *file_priv, u16 engine)\n{\n\tstruct ivpu_cmdq *cmdq = file_priv->cmdq[engine];\n\n\tlockdep_assert_held(&file_priv->lock);\n\n\tif (cmdq) {\n\t\tcmdq->db_registered = false;\n\t\tcmdq->jobq->header.head = 0;\n\t\tcmdq->jobq->header.tail = 0;\n\t\twmb();  \n\t}\n}\n\nstatic void ivpu_cmdq_reset_all(struct ivpu_file_priv *file_priv)\n{\n\tint i;\n\n\tmutex_lock(&file_priv->lock);\n\n\tfor (i = 0; i < IVPU_NUM_ENGINES; i++)\n\t\tivpu_cmdq_reset_locked(file_priv, i);\n\n\tmutex_unlock(&file_priv->lock);\n}\n\nvoid ivpu_cmdq_reset_all_contexts(struct ivpu_device *vdev)\n{\n\tstruct ivpu_file_priv *file_priv;\n\tunsigned long ctx_id;\n\n\txa_for_each(&vdev->context_xa, ctx_id, file_priv) {\n\t\tfile_priv = ivpu_file_priv_get_by_ctx_id(vdev, ctx_id);\n\t\tif (!file_priv)\n\t\t\tcontinue;\n\n\t\tivpu_cmdq_reset_all(file_priv);\n\n\t\tivpu_file_priv_put(&file_priv);\n\t}\n}\n\nstatic int ivpu_cmdq_push_job(struct ivpu_cmdq *cmdq, struct ivpu_job *job)\n{\n\tstruct ivpu_device *vdev = job->vdev;\n\tstruct vpu_job_queue_header *header = &cmdq->jobq->header;\n\tstruct vpu_job_queue_entry *entry;\n\tu32 tail = READ_ONCE(header->tail);\n\tu32 next_entry = (tail + 1) % cmdq->entry_count;\n\n\t \n\tif (next_entry == header->head) {\n\t\tivpu_dbg(vdev, JOB, \"Job queue full: ctx %d engine %d db %d head %d tail %d\\n\",\n\t\t\t job->file_priv->ctx.id, job->engine_idx, cmdq->db_id, header->head, tail);\n\t\treturn -EBUSY;\n\t}\n\n\tentry = &cmdq->jobq->job[tail];\n\tentry->batch_buf_addr = job->cmd_buf_vpu_addr;\n\tentry->job_id = job->job_id;\n\tentry->flags = 0;\n\twmb();  \n\theader->tail = next_entry;\n\twmb();  \n\n\treturn 0;\n}\n\nstruct ivpu_fence {\n\tstruct dma_fence base;\n\tspinlock_t lock;  \n\tstruct ivpu_device *vdev;\n};\n\nstatic inline struct ivpu_fence *to_vpu_fence(struct dma_fence *fence)\n{\n\treturn container_of(fence, struct ivpu_fence, base);\n}\n\nstatic const char *ivpu_fence_get_driver_name(struct dma_fence *fence)\n{\n\treturn DRIVER_NAME;\n}\n\nstatic const char *ivpu_fence_get_timeline_name(struct dma_fence *fence)\n{\n\tstruct ivpu_fence *ivpu_fence = to_vpu_fence(fence);\n\n\treturn dev_name(ivpu_fence->vdev->drm.dev);\n}\n\nstatic const struct dma_fence_ops ivpu_fence_ops = {\n\t.get_driver_name = ivpu_fence_get_driver_name,\n\t.get_timeline_name = ivpu_fence_get_timeline_name,\n};\n\nstatic struct dma_fence *ivpu_fence_create(struct ivpu_device *vdev)\n{\n\tstruct ivpu_fence *fence;\n\n\tfence = kzalloc(sizeof(*fence), GFP_KERNEL);\n\tif (!fence)\n\t\treturn NULL;\n\n\tfence->vdev = vdev;\n\tspin_lock_init(&fence->lock);\n\tdma_fence_init(&fence->base, &ivpu_fence_ops, &fence->lock, dma_fence_context_alloc(1), 1);\n\n\treturn &fence->base;\n}\n\nstatic void job_get(struct ivpu_job *job, struct ivpu_job **link)\n{\n\tstruct ivpu_device *vdev = job->vdev;\n\n\tkref_get(&job->ref);\n\t*link = job;\n\n\tivpu_dbg(vdev, KREF, \"Job get: id %u refcount %u\\n\", job->job_id, kref_read(&job->ref));\n}\n\nstatic void job_release(struct kref *ref)\n{\n\tstruct ivpu_job *job = container_of(ref, struct ivpu_job, ref);\n\tstruct ivpu_device *vdev = job->vdev;\n\tu32 i;\n\n\tfor (i = 0; i < job->bo_count; i++)\n\t\tif (job->bos[i])\n\t\t\tdrm_gem_object_put(&job->bos[i]->base);\n\n\tdma_fence_put(job->done_fence);\n\tivpu_file_priv_put(&job->file_priv);\n\n\tivpu_dbg(vdev, KREF, \"Job released: id %u\\n\", job->job_id);\n\tkfree(job);\n\n\t \n\tivpu_rpm_put(vdev);\n}\n\nstatic void job_put(struct ivpu_job *job)\n{\n\tstruct ivpu_device *vdev = job->vdev;\n\n\tivpu_dbg(vdev, KREF, \"Job put: id %u refcount %u\\n\", job->job_id, kref_read(&job->ref));\n\tkref_put(&job->ref, job_release);\n}\n\nstatic struct ivpu_job *\nivpu_create_job(struct ivpu_file_priv *file_priv, u32 engine_idx, u32 bo_count)\n{\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct ivpu_job *job;\n\tint ret;\n\n\tret = ivpu_rpm_get(vdev);\n\tif (ret < 0)\n\t\treturn NULL;\n\n\tjob = kzalloc(struct_size(job, bos, bo_count), GFP_KERNEL);\n\tif (!job)\n\t\tgoto err_rpm_put;\n\n\tkref_init(&job->ref);\n\n\tjob->vdev = vdev;\n\tjob->engine_idx = engine_idx;\n\tjob->bo_count = bo_count;\n\tjob->done_fence = ivpu_fence_create(vdev);\n\tif (!job->done_fence) {\n\t\tivpu_warn_ratelimited(vdev, \"Failed to create a fence\\n\");\n\t\tgoto err_free_job;\n\t}\n\n\tjob->file_priv = ivpu_file_priv_get(file_priv);\n\n\tivpu_dbg(vdev, JOB, \"Job created: ctx %2d engine %d\", file_priv->ctx.id, job->engine_idx);\n\n\treturn job;\n\nerr_free_job:\n\tkfree(job);\nerr_rpm_put:\n\tivpu_rpm_put(vdev);\n\treturn NULL;\n}\n\nstatic int ivpu_job_done(struct ivpu_device *vdev, u32 job_id, u32 job_status)\n{\n\tstruct ivpu_job *job;\n\n\tjob = xa_erase(&vdev->submitted_jobs_xa, job_id);\n\tif (!job)\n\t\treturn -ENOENT;\n\n\tif (job->file_priv->has_mmu_faults)\n\t\tjob_status = VPU_JSM_STATUS_ABORTED;\n\n\tjob->bos[CMD_BUF_IDX]->job_status = job_status;\n\tdma_fence_signal(job->done_fence);\n\n\tivpu_dbg(vdev, JOB, \"Job complete:  id %3u ctx %2d engine %d status 0x%x\\n\",\n\t\t job->job_id, job->file_priv->ctx.id, job->engine_idx, job_status);\n\n\tjob_put(job);\n\treturn 0;\n}\n\nstatic void ivpu_job_done_message(struct ivpu_device *vdev, void *msg)\n{\n\tstruct vpu_ipc_msg_payload_job_done *payload;\n\tstruct vpu_jsm_msg *job_ret_msg = msg;\n\tint ret;\n\n\tpayload = (struct vpu_ipc_msg_payload_job_done *)&job_ret_msg->payload;\n\n\tret = ivpu_job_done(vdev, payload->job_id, payload->job_status);\n\tif (ret)\n\t\tivpu_err(vdev, \"Failed to finish job %d: %d\\n\", payload->job_id, ret);\n}\n\nvoid ivpu_jobs_abort_all(struct ivpu_device *vdev)\n{\n\tstruct ivpu_job *job;\n\tunsigned long id;\n\n\txa_for_each(&vdev->submitted_jobs_xa, id, job)\n\t\tivpu_job_done(vdev, id, VPU_JSM_STATUS_ABORTED);\n}\n\nstatic int ivpu_direct_job_submission(struct ivpu_job *job)\n{\n\tstruct ivpu_file_priv *file_priv = job->file_priv;\n\tstruct ivpu_device *vdev = job->vdev;\n\tstruct xa_limit job_id_range;\n\tstruct ivpu_cmdq *cmdq;\n\tint ret;\n\n\tmutex_lock(&file_priv->lock);\n\n\tcmdq = ivpu_cmdq_acquire(job->file_priv, job->engine_idx);\n\tif (!cmdq) {\n\t\tivpu_warn(vdev, \"Failed get job queue, ctx %d engine %d\\n\",\n\t\t\t  file_priv->ctx.id, job->engine_idx);\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\n\tjob_id_range.min = FIELD_PREP(JOB_ID_CONTEXT_MASK, (file_priv->ctx.id - 1));\n\tjob_id_range.max = job_id_range.min | JOB_ID_JOB_MASK;\n\n\tjob_get(job, &job);\n\tret = xa_alloc(&vdev->submitted_jobs_xa, &job->job_id, job, job_id_range, GFP_KERNEL);\n\tif (ret) {\n\t\tivpu_warn_ratelimited(vdev, \"Failed to allocate job id: %d\\n\", ret);\n\t\tgoto err_job_put;\n\t}\n\n\tret = ivpu_cmdq_push_job(cmdq, job);\n\tif (ret)\n\t\tgoto err_xa_erase;\n\n\tivpu_dbg(vdev, JOB, \"Job submitted: id %3u addr 0x%llx ctx %2d engine %d next %d\\n\",\n\t\t job->job_id, job->cmd_buf_vpu_addr, file_priv->ctx.id,\n\t\t job->engine_idx, cmdq->jobq->header.tail);\n\n\tif (ivpu_test_mode == IVPU_TEST_MODE_NULL_HW) {\n\t\tivpu_job_done(vdev, job->job_id, VPU_JSM_STATUS_SUCCESS);\n\t\tcmdq->jobq->header.head = cmdq->jobq->header.tail;\n\t\twmb();  \n\t} else {\n\t\tivpu_cmdq_ring_db(vdev, cmdq);\n\t}\n\n\tmutex_unlock(&file_priv->lock);\n\treturn 0;\n\nerr_xa_erase:\n\txa_erase(&vdev->submitted_jobs_xa, job->job_id);\nerr_job_put:\n\tjob_put(job);\nerr_unlock:\n\tmutex_unlock(&file_priv->lock);\n\treturn ret;\n}\n\nstatic int\nivpu_job_prepare_bos_for_submit(struct drm_file *file, struct ivpu_job *job, u32 *buf_handles,\n\t\t\t\tu32 buf_count, u32 commands_offset)\n{\n\tstruct ivpu_file_priv *file_priv = file->driver_priv;\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct ww_acquire_ctx acquire_ctx;\n\tenum dma_resv_usage usage;\n\tstruct ivpu_bo *bo;\n\tint ret;\n\tu32 i;\n\n\tfor (i = 0; i < buf_count; i++) {\n\t\tstruct drm_gem_object *obj = drm_gem_object_lookup(file, buf_handles[i]);\n\n\t\tif (!obj)\n\t\t\treturn -ENOENT;\n\n\t\tjob->bos[i] = to_ivpu_bo(obj);\n\n\t\tret = ivpu_bo_pin(job->bos[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbo = job->bos[CMD_BUF_IDX];\n\tif (!dma_resv_test_signaled(bo->base.resv, DMA_RESV_USAGE_READ)) {\n\t\tivpu_warn(vdev, \"Buffer is already in use\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (commands_offset >= bo->base.size) {\n\t\tivpu_warn(vdev, \"Invalid command buffer offset %u\\n\", commands_offset);\n\t\treturn -EINVAL;\n\t}\n\n\tjob->cmd_buf_vpu_addr = bo->vpu_addr + commands_offset;\n\n\tret = drm_gem_lock_reservations((struct drm_gem_object **)job->bos, buf_count,\n\t\t\t\t\t&acquire_ctx);\n\tif (ret) {\n\t\tivpu_warn(vdev, \"Failed to lock reservations: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < buf_count; i++) {\n\t\tret = dma_resv_reserve_fences(job->bos[i]->base.resv, 1);\n\t\tif (ret) {\n\t\t\tivpu_warn(vdev, \"Failed to reserve fences: %d\\n\", ret);\n\t\t\tgoto unlock_reservations;\n\t\t}\n\t}\n\n\tfor (i = 0; i < buf_count; i++) {\n\t\tusage = (i == CMD_BUF_IDX) ? DMA_RESV_USAGE_WRITE : DMA_RESV_USAGE_BOOKKEEP;\n\t\tdma_resv_add_fence(job->bos[i]->base.resv, job->done_fence, usage);\n\t}\n\nunlock_reservations:\n\tdrm_gem_unlock_reservations((struct drm_gem_object **)job->bos, buf_count, &acquire_ctx);\n\n\twmb();  \n\n\treturn ret;\n}\n\nint ivpu_submit_ioctl(struct drm_device *dev, void *data, struct drm_file *file)\n{\n\tstruct ivpu_file_priv *file_priv = file->driver_priv;\n\tstruct ivpu_device *vdev = file_priv->vdev;\n\tstruct drm_ivpu_submit *params = data;\n\tstruct ivpu_job *job;\n\tu32 *buf_handles;\n\tint idx, ret;\n\n\tif (params->engine > DRM_IVPU_ENGINE_COPY)\n\t\treturn -EINVAL;\n\n\tif (params->buffer_count == 0 || params->buffer_count > JOB_MAX_BUFFER_COUNT)\n\t\treturn -EINVAL;\n\n\tif (!IS_ALIGNED(params->commands_offset, 8))\n\t\treturn -EINVAL;\n\n\tif (!file_priv->ctx.id)\n\t\treturn -EINVAL;\n\n\tif (file_priv->has_mmu_faults)\n\t\treturn -EBADFD;\n\n\tbuf_handles = kcalloc(params->buffer_count, sizeof(u32), GFP_KERNEL);\n\tif (!buf_handles)\n\t\treturn -ENOMEM;\n\n\tret = copy_from_user(buf_handles,\n\t\t\t     (void __user *)params->buffers_ptr,\n\t\t\t     params->buffer_count * sizeof(u32));\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto free_handles;\n\t}\n\n\tif (!drm_dev_enter(&vdev->drm, &idx)) {\n\t\tret = -ENODEV;\n\t\tgoto free_handles;\n\t}\n\n\tivpu_dbg(vdev, JOB, \"Submit ioctl: ctx %u buf_count %u\\n\",\n\t\t file_priv->ctx.id, params->buffer_count);\n\n\tjob = ivpu_create_job(file_priv, params->engine, params->buffer_count);\n\tif (!job) {\n\t\tivpu_err(vdev, \"Failed to create job\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto dev_exit;\n\t}\n\n\tret = ivpu_job_prepare_bos_for_submit(file, job, buf_handles, params->buffer_count,\n\t\t\t\t\t      params->commands_offset);\n\tif (ret) {\n\t\tivpu_err(vdev, \"Failed to prepare job, ret %d\\n\", ret);\n\t\tgoto job_put;\n\t}\n\n\tret = ivpu_direct_job_submission(job);\n\tif (ret) {\n\t\tdma_fence_signal(job->done_fence);\n\t\tivpu_err(vdev, \"Failed to submit job to the HW, ret %d\\n\", ret);\n\t}\n\njob_put:\n\tjob_put(job);\ndev_exit:\n\tdrm_dev_exit(idx);\nfree_handles:\n\tkfree(buf_handles);\n\n\treturn ret;\n}\n\nstatic int ivpu_job_done_thread(void *arg)\n{\n\tstruct ivpu_device *vdev = (struct ivpu_device *)arg;\n\tstruct ivpu_ipc_consumer cons;\n\tstruct vpu_jsm_msg jsm_msg;\n\tbool jobs_submitted;\n\tunsigned int timeout;\n\tint ret;\n\n\tivpu_dbg(vdev, JOB, \"Started %s\\n\", __func__);\n\n\tivpu_ipc_consumer_add(vdev, &cons, VPU_IPC_CHAN_JOB_RET);\n\n\twhile (!kthread_should_stop()) {\n\t\ttimeout = ivpu_tdr_timeout_ms ? ivpu_tdr_timeout_ms : vdev->timeout.tdr;\n\t\tjobs_submitted = !xa_empty(&vdev->submitted_jobs_xa);\n\t\tret = ivpu_ipc_receive(vdev, &cons, NULL, &jsm_msg, timeout);\n\t\tif (!ret) {\n\t\t\tivpu_job_done_message(vdev, &jsm_msg);\n\t\t} else if (ret == -ETIMEDOUT) {\n\t\t\tif (jobs_submitted && !xa_empty(&vdev->submitted_jobs_xa)) {\n\t\t\t\tivpu_err(vdev, \"TDR detected, timeout %d ms\", timeout);\n\t\t\t\tivpu_hw_diagnose_failure(vdev);\n\t\t\t\tivpu_pm_schedule_recovery(vdev);\n\t\t\t}\n\t\t}\n\t}\n\n\tivpu_ipc_consumer_del(vdev, &cons);\n\n\tivpu_jobs_abort_all(vdev);\n\n\tivpu_dbg(vdev, JOB, \"Stopped %s\\n\", __func__);\n\treturn 0;\n}\n\nint ivpu_job_done_thread_init(struct ivpu_device *vdev)\n{\n\tstruct task_struct *thread;\n\n\tthread = kthread_run(&ivpu_job_done_thread, (void *)vdev, \"ivpu_job_done_thread\");\n\tif (IS_ERR(thread)) {\n\t\tivpu_err(vdev, \"Failed to start job completion thread\\n\");\n\t\treturn -EIO;\n\t}\n\n\tget_task_struct(thread);\n\twake_up_process(thread);\n\n\tvdev->job_done_thread = thread;\n\n\treturn 0;\n}\n\nvoid ivpu_job_done_thread_fini(struct ivpu_device *vdev)\n{\n\tkthread_stop(vdev->job_done_thread);\n\tput_task_struct(vdev->job_done_thread);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}