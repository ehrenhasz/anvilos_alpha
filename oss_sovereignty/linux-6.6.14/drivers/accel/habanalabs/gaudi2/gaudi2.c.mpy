{
  "module_name": "gaudi2.c",
  "hash_id": "841d797508cb9b1b1b2e8b7b485af35e3c5dab81e99283293240d84ee4b83bec",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/gaudi2/gaudi2.c",
  "human_readable_source": "\n\n \n\n#include \"gaudi2P.h\"\n#include \"gaudi2_masks.h\"\n#include \"../include/gaudi2/gaudi2_special_blocks.h\"\n#include \"../include/hw_ip/mmu/mmu_general.h\"\n#include \"../include/hw_ip/mmu/mmu_v2_0.h\"\n#include \"../include/gaudi2/gaudi2_packets.h\"\n#include \"../include/gaudi2/gaudi2_reg_map.h\"\n#include \"../include/gaudi2/gaudi2_async_ids_map_extended.h\"\n#include \"../include/gaudi2/arc/gaudi2_arc_common_packets.h\"\n\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/hwmon.h>\n#include <linux/iommu.h>\n\n#define GAUDI2_DMA_POOL_BLK_SIZE\t\tSZ_256\t\t \n\n#define GAUDI2_RESET_TIMEOUT_MSEC\t\t2000\t\t \n\n#define GAUDI2_RESET_POLL_TIMEOUT_USEC\t\t500000\t\t \n#define GAUDI2_PLDM_HRESET_TIMEOUT_MSEC\t\t25000\t\t \n#define GAUDI2_PLDM_SRESET_TIMEOUT_MSEC\t\t25000\t\t \n#define GAUDI2_PLDM_RESET_POLL_TIMEOUT_USEC\t3000000\t\t \n#define GAUDI2_RESET_POLL_CNT\t\t\t3\n#define GAUDI2_RESET_WAIT_MSEC\t\t\t1\t\t \n#define GAUDI2_CPU_RESET_WAIT_MSEC\t\t100\t\t \n#define GAUDI2_PLDM_RESET_WAIT_MSEC\t\t1000\t\t \n#define GAUDI2_CB_POOL_CB_CNT\t\t\t512\n#define GAUDI2_CB_POOL_CB_SIZE\t\t\tSZ_128K\t\t \n#define GAUDI2_MSG_TO_CPU_TIMEOUT_USEC\t\t4000000\t\t \n#define GAUDI2_WAIT_FOR_BL_TIMEOUT_USEC\t\t25000000\t \n#define GAUDI2_TEST_QUEUE_WAIT_USEC\t\t100000\t\t \n#define GAUDI2_PLDM_TEST_QUEUE_WAIT_USEC\t1000000\t\t \n\n#define GAUDI2_ALLOC_CPU_MEM_RETRY_CNT\t\t3\n\n \n#define MAX_CLUSTER_BINNING_FAULTY_TPCS\t\t1\n#define MAX_FAULTY_XBARS\t\t\t1\n#define MAX_FAULTY_EDMAS\t\t\t1\n#define MAX_FAULTY_DECODERS\t\t\t1\n\n#define GAUDI2_TPC_FULL_MASK\t\t\t0x1FFFFFF\n#define GAUDI2_HIF_HMMU_FULL_MASK\t\t0xFFFF\n#define GAUDI2_DECODER_FULL_MASK\t\t0x3FF\n\n#define GAUDI2_NA_EVENT_CAUSE\t\t\t0xFF\n#define GAUDI2_NUM_OF_QM_ERR_CAUSE\t\t18\n#define GAUDI2_NUM_OF_LOWER_QM_ERR_CAUSE\t25\n#define GAUDI2_NUM_OF_QM_ARB_ERR_CAUSE\t\t3\n#define GAUDI2_NUM_OF_ARC_SEI_ERR_CAUSE\t\t14\n#define GAUDI2_NUM_OF_CPU_SEI_ERR_CAUSE\t\t3\n#define GAUDI2_NUM_OF_QM_SEI_ERR_CAUSE\t\t2\n#define GAUDI2_NUM_OF_ROT_ERR_CAUSE\t\t22\n#define GAUDI2_NUM_OF_TPC_INTR_CAUSE\t\t31\n#define GAUDI2_NUM_OF_DEC_ERR_CAUSE\t\t25\n#define GAUDI2_NUM_OF_MME_ERR_CAUSE\t\t16\n#define GAUDI2_NUM_OF_MME_SBTE_ERR_CAUSE\t5\n#define GAUDI2_NUM_OF_MME_WAP_ERR_CAUSE\t\t7\n#define GAUDI2_NUM_OF_DMA_CORE_INTR_CAUSE\t8\n#define GAUDI2_NUM_OF_MMU_SPI_SEI_CAUSE\t\t19\n#define GAUDI2_NUM_OF_HBM_SEI_CAUSE\t\t9\n#define GAUDI2_NUM_OF_SM_SEI_ERR_CAUSE\t\t3\n#define GAUDI2_NUM_OF_PCIE_ADDR_DEC_ERR_CAUSE\t3\n#define GAUDI2_NUM_OF_PMMU_FATAL_ERR_CAUSE\t2\n#define GAUDI2_NUM_OF_HIF_FATAL_ERR_CAUSE\t2\n#define GAUDI2_NUM_OF_AXI_DRAIN_ERR_CAUSE\t2\n#define GAUDI2_NUM_OF_HBM_MC_SPI_CAUSE\t\t5\n\n#define GAUDI2_MMU_CACHE_INV_TIMEOUT_USEC\t(MMU_CONFIG_TIMEOUT_USEC * 10)\n#define GAUDI2_PLDM_MMU_TIMEOUT_USEC\t\t(MMU_CONFIG_TIMEOUT_USEC * 200)\n#define GAUDI2_ARB_WDT_TIMEOUT\t\t\t(0x1000000)\n\n#define GAUDI2_VDEC_TIMEOUT_USEC\t\t10000\t\t \n#define GAUDI2_PLDM_VDEC_TIMEOUT_USEC\t\t(GAUDI2_VDEC_TIMEOUT_USEC * 100)\n\n#define KDMA_TIMEOUT_USEC\t\t\tUSEC_PER_SEC\n\n#define IS_DMA_IDLE(dma_core_sts0)\t\\\n\t(!((dma_core_sts0) & (DCORE0_EDMA0_CORE_STS0_BUSY_MASK)))\n\n#define IS_DMA_HALTED(dma_core_sts1)\t\\\n\t((dma_core_sts1) & (DCORE0_EDMA0_CORE_STS1_IS_HALT_MASK))\n\n#define IS_MME_IDLE(mme_arch_sts) (((mme_arch_sts) & MME_ARCH_IDLE_MASK) == MME_ARCH_IDLE_MASK)\n\n#define IS_TPC_IDLE(tpc_cfg_sts) (((tpc_cfg_sts) & (TPC_IDLE_MASK)) == (TPC_IDLE_MASK))\n\n#define IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts) \\\n\t((((qm_glbl_sts0) & (QM_IDLE_MASK)) == (QM_IDLE_MASK)) && \\\n\t(((qm_glbl_sts1) & (QM_ARC_IDLE_MASK)) == (QM_ARC_IDLE_MASK)) && \\\n\t(((qm_cgm_sts) & (CGM_IDLE_MASK)) == (CGM_IDLE_MASK)))\n\n#define PCIE_DEC_EN_MASK\t\t\t0x300\n#define DEC_WORK_STATE_IDLE\t\t\t0\n#define DEC_WORK_STATE_PEND\t\t\t3\n#define IS_DEC_IDLE(dec_swreg15) \\\n\t(((dec_swreg15) & DCORE0_DEC0_CMD_SWREG15_SW_WORK_STATE_MASK) == DEC_WORK_STATE_IDLE || \\\n\t((dec_swreg15) & DCORE0_DEC0_CMD_SWREG15_SW_WORK_STATE_MASK) ==  DEC_WORK_STATE_PEND)\n\n \n#define GAUDI2_HBM_MMU_SCRM_MEM_SIZE\t\tSZ_8M\n#define GAUDI2_HBM_MMU_SCRM_DIV_SHIFT\t\t26\n#define GAUDI2_HBM_MMU_SCRM_MOD_SHIFT\t\t0\n#define GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK\tDRAM_VA_HINT_MASK\n#define GAUDI2_COMPENSATE_TLB_PAGE_SIZE_FACTOR\t16\n#define MMU_RANGE_INV_VA_LSB_SHIFT\t\t12\n#define MMU_RANGE_INV_VA_MSB_SHIFT\t\t44\n#define MMU_RANGE_INV_EN_SHIFT\t\t\t0\n#define MMU_RANGE_INV_ASID_EN_SHIFT\t\t1\n#define MMU_RANGE_INV_ASID_SHIFT\t\t2\n\n \n#define GAUDI2_PMMU_SPI_SEI_ENABLE_MASK\t\tGENMASK(GAUDI2_NUM_OF_MMU_SPI_SEI_CAUSE - 2, 0)\n#define GAUDI2_HMMU_SPI_SEI_ENABLE_MASK\t\tGENMASK(GAUDI2_NUM_OF_MMU_SPI_SEI_CAUSE - 1, 0)\n\n#define GAUDI2_MAX_STRING_LEN\t\t\t64\n\n#define GAUDI2_VDEC_MSIX_ENTRIES\t\t(GAUDI2_IRQ_NUM_SHARED_DEC1_ABNRM - \\\n\t\t\t\t\t\t\tGAUDI2_IRQ_NUM_DCORE0_DEC0_NRM + 1)\n\n#define ENGINE_ID_DCORE_OFFSET (GAUDI2_DCORE1_ENGINE_ID_EDMA_0 - GAUDI2_DCORE0_ENGINE_ID_EDMA_0)\n\n \n#define RAZWI_GET_AXUSER_XY(x) \\\n\t((x & 0xF8001FF0) >> 4)\n\n#define RAZWI_GET_AXUSER_LOW_XY(x) \\\n\t((x & 0x00001FF0) >> 4)\n\n#define RAZWI_INITIATOR_AXUER_L_X_SHIFT\t\t0\n#define RAZWI_INITIATOR_AXUER_L_X_MASK\t\t0x1F\n#define RAZWI_INITIATOR_AXUER_L_Y_SHIFT\t\t5\n#define RAZWI_INITIATOR_AXUER_L_Y_MASK\t\t0xF\n\n#define RAZWI_INITIATOR_AXUER_H_X_SHIFT\t\t23\n#define RAZWI_INITIATOR_AXUER_H_X_MASK\t\t0x1F\n\n#define RAZWI_INITIATOR_ID_X_Y_LOW(x, y) \\\n\t((((y) & RAZWI_INITIATOR_AXUER_L_Y_MASK) << RAZWI_INITIATOR_AXUER_L_Y_SHIFT) | \\\n\t\t(((x) & RAZWI_INITIATOR_AXUER_L_X_MASK) << RAZWI_INITIATOR_AXUER_L_X_SHIFT))\n\n#define RAZWI_INITIATOR_ID_X_HIGH(x) \\\n\t\t(((x) & RAZWI_INITIATOR_AXUER_H_X_MASK) << RAZWI_INITIATOR_AXUER_H_X_SHIFT)\n\n#define RAZWI_INITIATOR_ID_X_Y(xl, yl, xh) \\\n\t(RAZWI_INITIATOR_ID_X_Y_LOW(xl, yl) | RAZWI_INITIATOR_ID_X_HIGH(xh))\n\n#define PSOC_RAZWI_ENG_STR_SIZE 128\n#define PSOC_RAZWI_MAX_ENG_PER_RTR 5\n\n \n#define HW_UNSCRAMBLED_BITS_MASK GENMASK_ULL(63, 26)\n\nstruct gaudi2_razwi_info {\n\tu32 axuser_xy;\n\tu32 rtr_ctrl;\n\tu16 eng_id;\n\tchar *eng_name;\n};\n\nstatic struct gaudi2_razwi_info common_razwi_info[] = {\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 0), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_DEC_0, \"DEC0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 4), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_DEC_1, \"DEC1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 18), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_DEC_0, \"DEC2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 14), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_DEC_1, \"DEC3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 0), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_DEC_0, \"DEC4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 4), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_DEC_1, \"DEC5\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 18), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_DEC_0, \"DEC6\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 14), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_DEC_1, \"DEC7\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 6), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_PCIE_ENGINE_ID_DEC_0, \"DEC8\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 7), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_PCIE_ENGINE_ID_DEC_0, \"DEC9\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(3, 4, 2), mmDCORE0_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_0, \"TPC0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(3, 4, 4), mmDCORE0_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_1, \"TPC1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(4, 4, 2), mmDCORE0_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_2, \"TPC2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(4, 4, 4), mmDCORE0_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_3, \"TPC3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(5, 4, 2), mmDCORE0_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_4, \"TPC4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(5, 4, 4), mmDCORE0_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_TPC_5, \"TPC5\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(16, 4, 14), mmDCORE1_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_0, \"TPC6\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(16, 4, 16), mmDCORE1_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_1, \"TPC7\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(15, 4, 14), mmDCORE1_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_2, \"TPC8\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(15, 4, 16), mmDCORE1_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_3, \"TPC9\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(14, 4, 14), mmDCORE1_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_4, \"TPC10\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(14, 4, 16), mmDCORE1_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_TPC_5, \"TPC11\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(5, 11, 2), mmDCORE2_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_0, \"TPC12\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(5, 11, 4), mmDCORE2_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_1, \"TPC13\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(4, 11, 2), mmDCORE2_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_2, \"TPC14\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(4, 11, 4), mmDCORE2_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_3, \"TPC15\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(3, 11, 2), mmDCORE2_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_4, \"TPC16\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(3, 11, 4), mmDCORE2_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_TPC_5, \"TPC17\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(14, 11, 14), mmDCORE3_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_0, \"TPC18\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(14, 11, 16), mmDCORE3_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_1, \"TPC19\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(15, 11, 14), mmDCORE3_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_2, \"TPC20\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(15, 11, 16), mmDCORE3_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_3, \"TPC21\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(16, 11, 14), mmDCORE3_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_4, \"TPC22\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(16, 11, 16), mmDCORE3_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_5, \"TPC23\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 2), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_TPC_5, \"TPC24\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 8), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC0_0, \"NIC0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 10), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC0_1, \"NIC1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 12), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC1_0, \"NIC2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 14), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC1_1, \"NIC3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 15), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC2_0, \"NIC4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 2), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC2_1, \"NIC5\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 4), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC3_0, \"NIC6\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 6), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC3_1, \"NIC7\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 8), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC4_0, \"NIC8\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 12), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC4_1, \"NIC9\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 14), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC5_0, \"NIC10\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 16), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_NIC5_1, \"NIC11\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 2), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_PDMA_0, \"PDMA0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 3), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_PDMA_1, \"PDMA1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 4), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"PMMU\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 4, 5), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"PCIE\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 16), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_ARC_FARM, \"ARC_FARM\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 4, 17), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_KDMA, \"KDMA\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 1), mmSFT0_HBW_RTR_IF1_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_EDMA_0, \"EDMA0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 1), mmSFT0_HBW_RTR_IF0_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_EDMA_1, \"EDMA1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 18), mmSFT1_HBW_RTR_IF1_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_EDMA_0, \"EDMA2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 18), mmSFT1_HBW_RTR_IF0_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_EDMA_1, \"EDMA3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 1), mmSFT2_HBW_RTR_IF0_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_EDMA_0, \"EDMA4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 1), mmSFT2_HBW_RTR_IF1_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_EDMA_1, \"EDMA5\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 18), mmSFT2_HBW_RTR_IF0_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_EDMA_0, \"EDMA6\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 18), mmSFT2_HBW_RTR_IF1_RTR_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_EDMA_1, \"EDMA7\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 0), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 19), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 0), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 19), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 0), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 19), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU5\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 5, 0), mmDCORE0_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU6\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 5, 19), mmDCORE1_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU7\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 0), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU8\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 19), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU9\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 0), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU10\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 19), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU11\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 0), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU12\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 19), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU13\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(1, 10, 0), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU14\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(18, 10, 19), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_SIZE, \"HMMU15\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 2), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_ROT_0, \"ROT0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 16), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_ROT_1, \"ROT1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(2, 11, 2), mmDCORE2_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_PSOC, \"CPU\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y(17, 11, 11), mmDCORE3_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_ENGINE_ID_PSOC, \"PSOC\"}\n};\n\nstatic struct gaudi2_razwi_info mme_razwi_info[] = {\n\t\t \n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(7, 4), mmDCORE0_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_WAP0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 4), mmDCORE0_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_WAP1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(8, 4), mmDCORE0_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_CTRL_WR\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 4), mmDCORE0_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_CTRL_RD\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(6, 4), mmDCORE0_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_SBTE0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(6, 4), mmDCORE0_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_SBTE1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(7, 4), mmDCORE0_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_SBTE2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(8, 4), mmDCORE0_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_SBTE3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 4), mmDCORE0_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE0_ENGINE_ID_MME, \"MME0_SBTE4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(12, 4), mmDCORE1_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_WAP0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 4), mmDCORE1_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_WAP1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(11, 4), mmDCORE1_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_CTRL_WR\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 4), mmDCORE1_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_CTRL_RD\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(13, 4), mmDCORE1_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_SBTE0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(13, 4), mmDCORE1_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_SBTE1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(12, 4), mmDCORE1_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_SBTE2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(11, 4), mmDCORE1_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_SBTE3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 4), mmDCORE1_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE1_ENGINE_ID_MME, \"MME1_SBTE4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(7, 11), mmDCORE2_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_WAP0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 11), mmDCORE2_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_WAP1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(8, 11), mmDCORE2_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_CTRL_WR\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 11), mmDCORE2_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_CTRL_RD\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(6, 11), mmDCORE2_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_SBTE0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(6, 11), mmDCORE2_RTR4_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_SBTE1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(7, 11), mmDCORE2_RTR5_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_SBTE2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(8, 11), mmDCORE2_RTR6_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_SBTE3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(9, 11), mmDCORE2_RTR7_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE2_ENGINE_ID_MME, \"MME2_SBTE4\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(12, 11), mmDCORE3_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_WAP0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 11), mmDCORE3_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_WAP1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(11, 11), mmDCORE3_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_CTRL_WR\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 11), mmDCORE3_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_CTRL_RD\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(13, 11), mmDCORE3_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_SBTE0\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(13, 11), mmDCORE3_RTR3_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_SBTE1\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(12, 11), mmDCORE3_RTR2_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_SBTE2\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(11, 11), mmDCORE3_RTR1_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_SBTE3\"},\n\t\t{RAZWI_INITIATOR_ID_X_Y_LOW(10, 11), mmDCORE3_RTR0_CTRL_BASE,\n\t\t\t\tGAUDI2_DCORE3_ENGINE_ID_MME, \"MME3_SBTE4\"}\n};\n\nenum hl_pmmu_fatal_cause {\n\tLATENCY_RD_OUT_FIFO_OVERRUN,\n\tLATENCY_WR_OUT_FIFO_OVERRUN,\n};\n\nenum hl_pcie_drain_ind_cause {\n\tLBW_AXI_DRAIN_IND,\n\tHBW_AXI_DRAIN_IND\n};\n\nstatic const u32 cluster_hmmu_hif_enabled_mask[GAUDI2_HBM_NUM] = {\n\t[HBM_ID0] = 0xFFFC,\n\t[HBM_ID1] = 0xFFCF,\n\t[HBM_ID2] = 0xF7F7,\n\t[HBM_ID3] = 0x7F7F,\n\t[HBM_ID4] = 0xFCFF,\n\t[HBM_ID5] = 0xCFFF,\n};\n\nstatic const u8 xbar_edge_to_hbm_cluster[EDMA_ID_SIZE] = {\n\t[0] = HBM_ID0,\n\t[1] = HBM_ID1,\n\t[2] = HBM_ID4,\n\t[3] = HBM_ID5,\n};\n\nstatic const u8 edma_to_hbm_cluster[EDMA_ID_SIZE] = {\n\t[EDMA_ID_DCORE0_INSTANCE0] = HBM_ID0,\n\t[EDMA_ID_DCORE0_INSTANCE1] = HBM_ID2,\n\t[EDMA_ID_DCORE1_INSTANCE0] = HBM_ID1,\n\t[EDMA_ID_DCORE1_INSTANCE1] = HBM_ID3,\n\t[EDMA_ID_DCORE2_INSTANCE0] = HBM_ID2,\n\t[EDMA_ID_DCORE2_INSTANCE1] = HBM_ID4,\n\t[EDMA_ID_DCORE3_INSTANCE0] = HBM_ID3,\n\t[EDMA_ID_DCORE3_INSTANCE1] = HBM_ID5,\n};\n\nstatic const int gaudi2_qman_async_event_id[] = {\n\t[GAUDI2_QUEUE_ID_PDMA_0_0] = GAUDI2_EVENT_PDMA0_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_0_1] = GAUDI2_EVENT_PDMA0_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_0_2] = GAUDI2_EVENT_PDMA0_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_0_3] = GAUDI2_EVENT_PDMA0_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_1_0] = GAUDI2_EVENT_PDMA1_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_1_1] = GAUDI2_EVENT_PDMA1_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_1_2] = GAUDI2_EVENT_PDMA1_QM,\n\t[GAUDI2_QUEUE_ID_PDMA_1_3] = GAUDI2_EVENT_PDMA1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0] = GAUDI2_EVENT_HDMA0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_1] = GAUDI2_EVENT_HDMA0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_2] = GAUDI2_EVENT_HDMA0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_3] = GAUDI2_EVENT_HDMA0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_0] = GAUDI2_EVENT_HDMA1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_1] = GAUDI2_EVENT_HDMA1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_2] = GAUDI2_EVENT_HDMA1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_3] = GAUDI2_EVENT_HDMA1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_0] = GAUDI2_EVENT_MME0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_1] = GAUDI2_EVENT_MME0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_2] = GAUDI2_EVENT_MME0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_3] = GAUDI2_EVENT_MME0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_0] = GAUDI2_EVENT_TPC0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_1] = GAUDI2_EVENT_TPC0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_2] = GAUDI2_EVENT_TPC0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_3] = GAUDI2_EVENT_TPC0_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_0] = GAUDI2_EVENT_TPC1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_1] = GAUDI2_EVENT_TPC1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_2] = GAUDI2_EVENT_TPC1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_3] = GAUDI2_EVENT_TPC1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_0] = GAUDI2_EVENT_TPC2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_1] = GAUDI2_EVENT_TPC2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_2] = GAUDI2_EVENT_TPC2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_3] = GAUDI2_EVENT_TPC2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_0] = GAUDI2_EVENT_TPC3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_1] = GAUDI2_EVENT_TPC3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_2] = GAUDI2_EVENT_TPC3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_3] = GAUDI2_EVENT_TPC3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_0] = GAUDI2_EVENT_TPC4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_1] = GAUDI2_EVENT_TPC4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_2] = GAUDI2_EVENT_TPC4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_3] = GAUDI2_EVENT_TPC4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_0] = GAUDI2_EVENT_TPC5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_1] = GAUDI2_EVENT_TPC5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_2] = GAUDI2_EVENT_TPC5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_3] = GAUDI2_EVENT_TPC5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_0] = GAUDI2_EVENT_TPC24_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_1] = GAUDI2_EVENT_TPC24_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_2] = GAUDI2_EVENT_TPC24_QM,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_3] = GAUDI2_EVENT_TPC24_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0] = GAUDI2_EVENT_HDMA2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_1] = GAUDI2_EVENT_HDMA2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_2] = GAUDI2_EVENT_HDMA2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_3] = GAUDI2_EVENT_HDMA2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_0] = GAUDI2_EVENT_HDMA3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_1] = GAUDI2_EVENT_HDMA3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_2] = GAUDI2_EVENT_HDMA3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_3] = GAUDI2_EVENT_HDMA3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_0] = GAUDI2_EVENT_MME1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_1] = GAUDI2_EVENT_MME1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_2] = GAUDI2_EVENT_MME1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_3] = GAUDI2_EVENT_MME1_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_0] = GAUDI2_EVENT_TPC6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_1] = GAUDI2_EVENT_TPC6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_2] = GAUDI2_EVENT_TPC6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_3] = GAUDI2_EVENT_TPC6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_0] = GAUDI2_EVENT_TPC7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_1] = GAUDI2_EVENT_TPC7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_2] = GAUDI2_EVENT_TPC7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_3] = GAUDI2_EVENT_TPC7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_0] = GAUDI2_EVENT_TPC8_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_1] = GAUDI2_EVENT_TPC8_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_2] = GAUDI2_EVENT_TPC8_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_3] = GAUDI2_EVENT_TPC8_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_0] = GAUDI2_EVENT_TPC9_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_1] = GAUDI2_EVENT_TPC9_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_2] = GAUDI2_EVENT_TPC9_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_3] = GAUDI2_EVENT_TPC9_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_0] = GAUDI2_EVENT_TPC10_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_1] = GAUDI2_EVENT_TPC10_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_2] = GAUDI2_EVENT_TPC10_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_3] = GAUDI2_EVENT_TPC10_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_0] = GAUDI2_EVENT_TPC11_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_1] = GAUDI2_EVENT_TPC11_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_2] = GAUDI2_EVENT_TPC11_QM,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_3] = GAUDI2_EVENT_TPC11_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0] = GAUDI2_EVENT_HDMA4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_1] = GAUDI2_EVENT_HDMA4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_2] = GAUDI2_EVENT_HDMA4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_3] = GAUDI2_EVENT_HDMA4_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_0] = GAUDI2_EVENT_HDMA5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_1] = GAUDI2_EVENT_HDMA5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_2] = GAUDI2_EVENT_HDMA5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_3] = GAUDI2_EVENT_HDMA5_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_0] = GAUDI2_EVENT_MME2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_1] = GAUDI2_EVENT_MME2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_2] = GAUDI2_EVENT_MME2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_3] = GAUDI2_EVENT_MME2_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_0] = GAUDI2_EVENT_TPC12_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_1] = GAUDI2_EVENT_TPC12_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_2] = GAUDI2_EVENT_TPC12_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_3] = GAUDI2_EVENT_TPC12_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_0] = GAUDI2_EVENT_TPC13_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_1] = GAUDI2_EVENT_TPC13_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_2] = GAUDI2_EVENT_TPC13_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_3] = GAUDI2_EVENT_TPC13_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_0] = GAUDI2_EVENT_TPC14_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_1] = GAUDI2_EVENT_TPC14_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_2] = GAUDI2_EVENT_TPC14_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_3] = GAUDI2_EVENT_TPC14_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_0] = GAUDI2_EVENT_TPC15_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_1] = GAUDI2_EVENT_TPC15_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_2] = GAUDI2_EVENT_TPC15_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_3] = GAUDI2_EVENT_TPC15_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_0] = GAUDI2_EVENT_TPC16_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_1] = GAUDI2_EVENT_TPC16_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_2] = GAUDI2_EVENT_TPC16_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_3] = GAUDI2_EVENT_TPC16_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_0] = GAUDI2_EVENT_TPC17_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_1] = GAUDI2_EVENT_TPC17_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_2] = GAUDI2_EVENT_TPC17_QM,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_3] = GAUDI2_EVENT_TPC17_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0] = GAUDI2_EVENT_HDMA6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_1] = GAUDI2_EVENT_HDMA6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_2] = GAUDI2_EVENT_HDMA6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_3] = GAUDI2_EVENT_HDMA6_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_0] = GAUDI2_EVENT_HDMA7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_1] = GAUDI2_EVENT_HDMA7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_2] = GAUDI2_EVENT_HDMA7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3] = GAUDI2_EVENT_HDMA7_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_0] = GAUDI2_EVENT_MME3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_1] = GAUDI2_EVENT_MME3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_2] = GAUDI2_EVENT_MME3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_3] = GAUDI2_EVENT_MME3_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_0] = GAUDI2_EVENT_TPC18_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_1] = GAUDI2_EVENT_TPC18_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_2] = GAUDI2_EVENT_TPC18_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_3] = GAUDI2_EVENT_TPC18_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_0] = GAUDI2_EVENT_TPC19_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_1] = GAUDI2_EVENT_TPC19_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_2] = GAUDI2_EVENT_TPC19_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_3] = GAUDI2_EVENT_TPC19_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_0] = GAUDI2_EVENT_TPC20_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_1] = GAUDI2_EVENT_TPC20_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_2] = GAUDI2_EVENT_TPC20_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_3] = GAUDI2_EVENT_TPC20_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_0] = GAUDI2_EVENT_TPC21_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_1] = GAUDI2_EVENT_TPC21_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_2] = GAUDI2_EVENT_TPC21_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_3] = GAUDI2_EVENT_TPC21_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_0] = GAUDI2_EVENT_TPC22_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_1] = GAUDI2_EVENT_TPC22_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_2] = GAUDI2_EVENT_TPC22_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_3] = GAUDI2_EVENT_TPC22_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_0] = GAUDI2_EVENT_TPC23_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_1] = GAUDI2_EVENT_TPC23_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_2] = GAUDI2_EVENT_TPC23_QM,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_3] = GAUDI2_EVENT_TPC23_QM,\n\t[GAUDI2_QUEUE_ID_NIC_0_0] = GAUDI2_EVENT_NIC0_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_0_1] = GAUDI2_EVENT_NIC0_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_0_2] = GAUDI2_EVENT_NIC0_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_0_3] = GAUDI2_EVENT_NIC0_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_1_0] = GAUDI2_EVENT_NIC0_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_1_1] = GAUDI2_EVENT_NIC0_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_1_2] = GAUDI2_EVENT_NIC0_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_1_3] = GAUDI2_EVENT_NIC0_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_2_0] = GAUDI2_EVENT_NIC1_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_2_1] = GAUDI2_EVENT_NIC1_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_2_2] = GAUDI2_EVENT_NIC1_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_2_3] = GAUDI2_EVENT_NIC1_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_3_0] = GAUDI2_EVENT_NIC1_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_3_1] = GAUDI2_EVENT_NIC1_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_3_2] = GAUDI2_EVENT_NIC1_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_3_3] = GAUDI2_EVENT_NIC1_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_4_0] = GAUDI2_EVENT_NIC2_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_4_1] = GAUDI2_EVENT_NIC2_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_4_2] = GAUDI2_EVENT_NIC2_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_4_3] = GAUDI2_EVENT_NIC2_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_5_0] = GAUDI2_EVENT_NIC2_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_5_1] = GAUDI2_EVENT_NIC2_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_5_2] = GAUDI2_EVENT_NIC2_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_5_3] = GAUDI2_EVENT_NIC2_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_6_0] = GAUDI2_EVENT_NIC3_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_6_1] = GAUDI2_EVENT_NIC3_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_6_2] = GAUDI2_EVENT_NIC3_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_6_3] = GAUDI2_EVENT_NIC3_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_7_0] = GAUDI2_EVENT_NIC3_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_7_1] = GAUDI2_EVENT_NIC3_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_7_2] = GAUDI2_EVENT_NIC3_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_7_3] = GAUDI2_EVENT_NIC3_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_8_0] = GAUDI2_EVENT_NIC4_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_8_1] = GAUDI2_EVENT_NIC4_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_8_2] = GAUDI2_EVENT_NIC4_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_8_3] = GAUDI2_EVENT_NIC4_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_9_0] = GAUDI2_EVENT_NIC4_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_9_1] = GAUDI2_EVENT_NIC4_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_9_2] = GAUDI2_EVENT_NIC4_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_9_3] = GAUDI2_EVENT_NIC4_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_10_0] = GAUDI2_EVENT_NIC5_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_10_1] = GAUDI2_EVENT_NIC5_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_10_2] = GAUDI2_EVENT_NIC5_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_10_3] = GAUDI2_EVENT_NIC5_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_11_0] = GAUDI2_EVENT_NIC5_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_11_1] = GAUDI2_EVENT_NIC5_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_11_2] = GAUDI2_EVENT_NIC5_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_11_3] = GAUDI2_EVENT_NIC5_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_12_0] = GAUDI2_EVENT_NIC6_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_12_1] = GAUDI2_EVENT_NIC6_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_12_2] = GAUDI2_EVENT_NIC6_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_12_3] = GAUDI2_EVENT_NIC6_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_13_0] = GAUDI2_EVENT_NIC6_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_13_1] = GAUDI2_EVENT_NIC6_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_13_2] = GAUDI2_EVENT_NIC6_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_13_3] = GAUDI2_EVENT_NIC6_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_14_0] = GAUDI2_EVENT_NIC7_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_14_1] = GAUDI2_EVENT_NIC7_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_14_2] = GAUDI2_EVENT_NIC7_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_14_3] = GAUDI2_EVENT_NIC7_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_15_0] = GAUDI2_EVENT_NIC7_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_15_1] = GAUDI2_EVENT_NIC7_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_15_2] = GAUDI2_EVENT_NIC7_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_15_3] = GAUDI2_EVENT_NIC7_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_16_0] = GAUDI2_EVENT_NIC8_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_16_1] = GAUDI2_EVENT_NIC8_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_16_2] = GAUDI2_EVENT_NIC8_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_16_3] = GAUDI2_EVENT_NIC8_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_17_0] = GAUDI2_EVENT_NIC8_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_17_1] = GAUDI2_EVENT_NIC8_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_17_2] = GAUDI2_EVENT_NIC8_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_17_3] = GAUDI2_EVENT_NIC8_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_18_0] = GAUDI2_EVENT_NIC9_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_18_1] = GAUDI2_EVENT_NIC9_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_18_2] = GAUDI2_EVENT_NIC9_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_18_3] = GAUDI2_EVENT_NIC9_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_19_0] = GAUDI2_EVENT_NIC9_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_19_1] = GAUDI2_EVENT_NIC9_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_19_2] = GAUDI2_EVENT_NIC9_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_19_3] = GAUDI2_EVENT_NIC9_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_20_0] = GAUDI2_EVENT_NIC10_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_20_1] = GAUDI2_EVENT_NIC10_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_20_2] = GAUDI2_EVENT_NIC10_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_20_3] = GAUDI2_EVENT_NIC10_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_21_0] = GAUDI2_EVENT_NIC10_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_21_1] = GAUDI2_EVENT_NIC10_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_21_2] = GAUDI2_EVENT_NIC10_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_21_3] = GAUDI2_EVENT_NIC10_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_22_0] = GAUDI2_EVENT_NIC11_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_22_1] = GAUDI2_EVENT_NIC11_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_22_2] = GAUDI2_EVENT_NIC11_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_22_3] = GAUDI2_EVENT_NIC11_QM0,\n\t[GAUDI2_QUEUE_ID_NIC_23_0] = GAUDI2_EVENT_NIC11_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_23_1] = GAUDI2_EVENT_NIC11_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_23_2] = GAUDI2_EVENT_NIC11_QM1,\n\t[GAUDI2_QUEUE_ID_NIC_23_3] = GAUDI2_EVENT_NIC11_QM1,\n\t[GAUDI2_QUEUE_ID_ROT_0_0] = GAUDI2_EVENT_ROTATOR0_ROT0_QM,\n\t[GAUDI2_QUEUE_ID_ROT_0_1] = GAUDI2_EVENT_ROTATOR0_ROT0_QM,\n\t[GAUDI2_QUEUE_ID_ROT_0_2] = GAUDI2_EVENT_ROTATOR0_ROT0_QM,\n\t[GAUDI2_QUEUE_ID_ROT_0_3] = GAUDI2_EVENT_ROTATOR0_ROT0_QM,\n\t[GAUDI2_QUEUE_ID_ROT_1_0] = GAUDI2_EVENT_ROTATOR1_ROT1_QM,\n\t[GAUDI2_QUEUE_ID_ROT_1_1] = GAUDI2_EVENT_ROTATOR1_ROT1_QM,\n\t[GAUDI2_QUEUE_ID_ROT_1_2] = GAUDI2_EVENT_ROTATOR1_ROT1_QM,\n\t[GAUDI2_QUEUE_ID_ROT_1_3] = GAUDI2_EVENT_ROTATOR1_ROT1_QM\n};\n\nstatic const int gaudi2_dma_core_async_event_id[] = {\n\t[DMA_CORE_ID_EDMA0] = GAUDI2_EVENT_HDMA0_CORE,\n\t[DMA_CORE_ID_EDMA1] = GAUDI2_EVENT_HDMA1_CORE,\n\t[DMA_CORE_ID_EDMA2] = GAUDI2_EVENT_HDMA2_CORE,\n\t[DMA_CORE_ID_EDMA3] = GAUDI2_EVENT_HDMA3_CORE,\n\t[DMA_CORE_ID_EDMA4] = GAUDI2_EVENT_HDMA4_CORE,\n\t[DMA_CORE_ID_EDMA5] = GAUDI2_EVENT_HDMA5_CORE,\n\t[DMA_CORE_ID_EDMA6] = GAUDI2_EVENT_HDMA6_CORE,\n\t[DMA_CORE_ID_EDMA7] = GAUDI2_EVENT_HDMA7_CORE,\n\t[DMA_CORE_ID_PDMA0] = GAUDI2_EVENT_PDMA0_CORE,\n\t[DMA_CORE_ID_PDMA1] = GAUDI2_EVENT_PDMA1_CORE,\n\t[DMA_CORE_ID_KDMA] = GAUDI2_EVENT_KDMA0_CORE,\n};\n\nstatic const char * const gaudi2_qm_sei_error_cause[GAUDI2_NUM_OF_QM_SEI_ERR_CAUSE] = {\n\t\"qman sei intr\",\n\t\"arc sei intr\"\n};\n\nstatic const char * const gaudi2_cpu_sei_error_cause[GAUDI2_NUM_OF_CPU_SEI_ERR_CAUSE] = {\n\t\"AXI_TERMINATOR WR\",\n\t\"AXI_TERMINATOR RD\",\n\t\"AXI SPLIT SEI Status\"\n};\n\nstatic const char * const gaudi2_arc_sei_error_cause[GAUDI2_NUM_OF_ARC_SEI_ERR_CAUSE] = {\n\t\"cbu_bresp_sei_intr_cause\",\n\t\"cbu_rresp_sei_intr_cause\",\n\t\"lbu_bresp_sei_intr_cause\",\n\t\"lbu_rresp_sei_intr_cause\",\n\t\"cbu_axi_split_intr_cause\",\n\t\"lbu_axi_split_intr_cause\",\n\t\"arc_ip_excptn_sei_intr_cause\",\n\t\"dmi_bresp_sei_intr_cause\",\n\t\"aux2apb_err_sei_intr_cause\",\n\t\"cfg_lbw_wr_terminated_intr_cause\",\n\t\"cfg_lbw_rd_terminated_intr_cause\",\n\t\"cfg_dccm_wr_terminated_intr_cause\",\n\t\"cfg_dccm_rd_terminated_intr_cause\",\n\t\"cfg_hbw_rd_terminated_intr_cause\"\n};\n\nstatic const char * const gaudi2_dec_error_cause[GAUDI2_NUM_OF_DEC_ERR_CAUSE] = {\n\t\"msix_vcd_hbw_sei\",\n\t\"msix_l2c_hbw_sei\",\n\t\"msix_nrm_hbw_sei\",\n\t\"msix_abnrm_hbw_sei\",\n\t\"msix_vcd_lbw_sei\",\n\t\"msix_l2c_lbw_sei\",\n\t\"msix_nrm_lbw_sei\",\n\t\"msix_abnrm_lbw_sei\",\n\t\"apb_vcd_lbw_sei\",\n\t\"apb_l2c_lbw_sei\",\n\t\"apb_nrm_lbw_sei\",\n\t\"apb_abnrm_lbw_sei\",\n\t\"dec_sei\",\n\t\"dec_apb_sei\",\n\t\"trc_apb_sei\",\n\t\"lbw_mstr_if_sei\",\n\t\"axi_split_bresp_err_sei\",\n\t\"hbw_axi_wr_viol_sei\",\n\t\"hbw_axi_rd_viol_sei\",\n\t\"lbw_axi_wr_viol_sei\",\n\t\"lbw_axi_rd_viol_sei\",\n\t\"vcd_spi\",\n\t\"l2c_spi\",\n\t\"nrm_spi\",\n\t\"abnrm_spi\",\n};\n\nstatic const char * const gaudi2_qman_error_cause[GAUDI2_NUM_OF_QM_ERR_CAUSE] = {\n\t\"PQ AXI HBW error\",\n\t\"CQ AXI HBW error\",\n\t\"CP AXI HBW error\",\n\t\"CP error due to undefined OPCODE\",\n\t\"CP encountered STOP OPCODE\",\n\t\"CP AXI LBW error\",\n\t\"CP WRREG32 or WRBULK returned error\",\n\t\"N/A\",\n\t\"FENCE 0 inc over max value and clipped\",\n\t\"FENCE 1 inc over max value and clipped\",\n\t\"FENCE 2 inc over max value and clipped\",\n\t\"FENCE 3 inc over max value and clipped\",\n\t\"FENCE 0 dec under min value and clipped\",\n\t\"FENCE 1 dec under min value and clipped\",\n\t\"FENCE 2 dec under min value and clipped\",\n\t\"FENCE 3 dec under min value and clipped\",\n\t\"CPDMA Up overflow\",\n\t\"PQC L2H error\"\n};\n\nstatic const char * const gaudi2_lower_qman_error_cause[GAUDI2_NUM_OF_LOWER_QM_ERR_CAUSE] = {\n\t\"RSVD0\",\n\t\"CQ AXI HBW error\",\n\t\"CP AXI HBW error\",\n\t\"CP error due to undefined OPCODE\",\n\t\"CP encountered STOP OPCODE\",\n\t\"CP AXI LBW error\",\n\t\"CP WRREG32 or WRBULK returned error\",\n\t\"N/A\",\n\t\"FENCE 0 inc over max value and clipped\",\n\t\"FENCE 1 inc over max value and clipped\",\n\t\"FENCE 2 inc over max value and clipped\",\n\t\"FENCE 3 inc over max value and clipped\",\n\t\"FENCE 0 dec under min value and clipped\",\n\t\"FENCE 1 dec under min value and clipped\",\n\t\"FENCE 2 dec under min value and clipped\",\n\t\"FENCE 3 dec under min value and clipped\",\n\t\"CPDMA Up overflow\",\n\t\"RSVD17\",\n\t\"CQ_WR_IFIFO_CI_ERR\",\n\t\"CQ_WR_CTL_CI_ERR\",\n\t\"ARC_CQF_RD_ERR\",\n\t\"ARC_CQ_WR_IFIFO_CI_ERR\",\n\t\"ARC_CQ_WR_CTL_CI_ERR\",\n\t\"ARC_AXI_ERR\",\n\t\"CP_SWITCH_WDT_ERR\"\n};\n\nstatic const char * const gaudi2_qman_arb_error_cause[GAUDI2_NUM_OF_QM_ARB_ERR_CAUSE] = {\n\t\"Choice push while full error\",\n\t\"Choice Q watchdog error\",\n\t\"MSG AXI LBW returned with error\"\n};\n\nstatic const char * const guadi2_rot_error_cause[GAUDI2_NUM_OF_ROT_ERR_CAUSE] = {\n\t\"qm_axi_err\",\n\t\"qm_trace_fence_events\",\n\t\"qm_sw_err\",\n\t\"qm_cp_sw_stop\",\n\t\"lbw_mstr_rresp_err\",\n\t\"lbw_mstr_bresp_err\",\n\t\"lbw_msg_slverr\",\n\t\"hbw_msg_slverr\",\n\t\"wbc_slverr\",\n\t\"hbw_mstr_rresp_err\",\n\t\"hbw_mstr_bresp_err\",\n\t\"sb_resp_intr\",\n\t\"mrsb_resp_intr\",\n\t\"core_dw_status_0\",\n\t\"core_dw_status_1\",\n\t\"core_dw_status_2\",\n\t\"core_dw_status_3\",\n\t\"core_dw_status_4\",\n\t\"core_dw_status_5\",\n\t\"core_dw_status_6\",\n\t\"core_dw_status_7\",\n\t\"async_arc2cpu_sei_intr\",\n};\n\nstatic const char * const gaudi2_tpc_interrupts_cause[GAUDI2_NUM_OF_TPC_INTR_CAUSE] = {\n\t\"tpc_address_exceed_slm\",\n\t\"tpc_div_by_0\",\n\t\"tpc_spu_mac_overflow\",\n\t\"tpc_spu_addsub_overflow\",\n\t\"tpc_spu_abs_overflow\",\n\t\"tpc_spu_fma_fp_dst_nan\",\n\t\"tpc_spu_fma_fp_dst_inf\",\n\t\"tpc_spu_convert_fp_dst_nan\",\n\t\"tpc_spu_convert_fp_dst_inf\",\n\t\"tpc_spu_fp_dst_denorm\",\n\t\"tpc_vpu_mac_overflow\",\n\t\"tpc_vpu_addsub_overflow\",\n\t\"tpc_vpu_abs_overflow\",\n\t\"tpc_vpu_convert_fp_dst_nan\",\n\t\"tpc_vpu_convert_fp_dst_inf\",\n\t\"tpc_vpu_fma_fp_dst_nan\",\n\t\"tpc_vpu_fma_fp_dst_inf\",\n\t\"tpc_vpu_fp_dst_denorm\",\n\t\"tpc_assertions\",\n\t\"tpc_illegal_instruction\",\n\t\"tpc_pc_wrap_around\",\n\t\"tpc_qm_sw_err\",\n\t\"tpc_hbw_rresp_err\",\n\t\"tpc_hbw_bresp_err\",\n\t\"tpc_lbw_rresp_err\",\n\t\"tpc_lbw_bresp_err\",\n\t\"st_unlock_already_locked\",\n\t\"invalid_lock_access\",\n\t\"LD_L protection violation\",\n\t\"ST_L protection violation\",\n\t\"D$ L0CS mismatch\",\n};\n\nstatic const char * const guadi2_mme_error_cause[GAUDI2_NUM_OF_MME_ERR_CAUSE] = {\n\t\"agu_resp_intr\",\n\t\"qman_axi_err\",\n\t\"wap sei (wbc axi err)\",\n\t\"arc sei\",\n\t\"cfg access error\",\n\t\"qm_sw_err\",\n\t\"sbte_dbg_intr_0\",\n\t\"sbte_dbg_intr_1\",\n\t\"sbte_dbg_intr_2\",\n\t\"sbte_dbg_intr_3\",\n\t\"sbte_dbg_intr_4\",\n\t\"sbte_prtn_intr_0\",\n\t\"sbte_prtn_intr_1\",\n\t\"sbte_prtn_intr_2\",\n\t\"sbte_prtn_intr_3\",\n\t\"sbte_prtn_intr_4\",\n};\n\nstatic const char * const guadi2_mme_sbte_error_cause[GAUDI2_NUM_OF_MME_SBTE_ERR_CAUSE] = {\n\t\"i0\",\n\t\"i1\",\n\t\"i2\",\n\t\"i3\",\n\t\"i4\",\n};\n\nstatic const char * const guadi2_mme_wap_error_cause[GAUDI2_NUM_OF_MME_WAP_ERR_CAUSE] = {\n\t\"WBC ERR RESP_0\",\n\t\"WBC ERR RESP_1\",\n\t\"AP SOURCE POS INF\",\n\t\"AP SOURCE NEG INF\",\n\t\"AP SOURCE NAN\",\n\t\"AP RESULT POS INF\",\n\t\"AP RESULT NEG INF\",\n};\n\nstatic const char * const gaudi2_dma_core_interrupts_cause[GAUDI2_NUM_OF_DMA_CORE_INTR_CAUSE] = {\n\t\"HBW Read returned with error RRESP\",\n\t\"HBW write returned with error BRESP\",\n\t\"LBW write returned with error BRESP\",\n\t\"descriptor_fifo_overflow\",\n\t\"KDMA SB LBW Read returned with error\",\n\t\"KDMA WBC LBW Write returned with error\",\n\t\"TRANSPOSE ENGINE DESC FIFO OVERFLOW\",\n\t\"WRONG CFG FOR COMMIT IN LIN DMA\"\n};\n\nstatic const char * const gaudi2_kdma_core_interrupts_cause[GAUDI2_NUM_OF_DMA_CORE_INTR_CAUSE] = {\n\t\"HBW/LBW Read returned with error RRESP\",\n\t\"HBW/LBW write returned with error BRESP\",\n\t\"LBW write returned with error BRESP\",\n\t\"descriptor_fifo_overflow\",\n\t\"KDMA SB LBW Read returned with error\",\n\t\"KDMA WBC LBW Write returned with error\",\n\t\"TRANSPOSE ENGINE DESC FIFO OVERFLOW\",\n\t\"WRONG CFG FOR COMMIT IN LIN DMA\"\n};\n\nstruct gaudi2_sm_sei_cause_data {\n\tconst char *cause_name;\n\tconst char *log_name;\n};\n\nstatic const struct gaudi2_sm_sei_cause_data\ngaudi2_sm_sei_cause[GAUDI2_NUM_OF_SM_SEI_ERR_CAUSE] = {\n\t{\"calculated SO value overflow/underflow\", \"SOB ID\"},\n\t{\"payload address of monitor is not aligned to 4B\", \"monitor addr\"},\n\t{\"armed monitor write got BRESP (SLVERR or DECERR)\", \"AXI id\"},\n};\n\nstatic const char * const\ngaudi2_pmmu_fatal_interrupts_cause[GAUDI2_NUM_OF_PMMU_FATAL_ERR_CAUSE] = {\n\t\"LATENCY_RD_OUT_FIFO_OVERRUN\",\n\t\"LATENCY_WR_OUT_FIFO_OVERRUN\",\n};\n\nstatic const char * const\ngaudi2_hif_fatal_interrupts_cause[GAUDI2_NUM_OF_HIF_FATAL_ERR_CAUSE] = {\n\t\"LATENCY_RD_OUT_FIFO_OVERRUN\",\n\t\"LATENCY_WR_OUT_FIFO_OVERRUN\",\n};\n\nstatic const char * const\ngaudi2_psoc_axi_drain_interrupts_cause[GAUDI2_NUM_OF_AXI_DRAIN_ERR_CAUSE] = {\n\t\"AXI drain HBW\",\n\t\"AXI drain LBW\",\n};\n\nstatic const char * const\ngaudi2_pcie_addr_dec_error_cause[GAUDI2_NUM_OF_PCIE_ADDR_DEC_ERR_CAUSE] = {\n\t\"HBW error response\",\n\t\"LBW error response\",\n\t\"TLP is blocked by RR\"\n};\n\nconst u32 gaudi2_qm_blocks_bases[GAUDI2_QUEUE_ID_SIZE] = {\n\t[GAUDI2_QUEUE_ID_PDMA_0_0] = mmPDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_0_1] = mmPDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_0_2] = mmPDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_0_3] = mmPDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_1_0] = mmPDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_1_1] = mmPDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_1_2] = mmPDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_PDMA_1_3] = mmPDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0] = mmDCORE0_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_1] = mmDCORE0_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_2] = mmDCORE0_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_3] = mmDCORE0_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_0] = mmDCORE0_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_1] = mmDCORE0_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_2] = mmDCORE0_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_3] = mmDCORE0_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_0] = mmDCORE0_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_1] = mmDCORE0_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_2] = mmDCORE0_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_3] = mmDCORE0_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_0] = mmDCORE0_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_1] = mmDCORE0_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_2] = mmDCORE0_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_3] = mmDCORE0_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_0] = mmDCORE0_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_1] = mmDCORE0_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_2] = mmDCORE0_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_3] = mmDCORE0_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_0] = mmDCORE0_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_1] = mmDCORE0_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_2] = mmDCORE0_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_3] = mmDCORE0_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_0] = mmDCORE0_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_1] = mmDCORE0_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_2] = mmDCORE0_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_3] = mmDCORE0_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_0] = mmDCORE0_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_1] = mmDCORE0_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_2] = mmDCORE0_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_3] = mmDCORE0_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_0] = mmDCORE0_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_1] = mmDCORE0_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_2] = mmDCORE0_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_3] = mmDCORE0_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_0] = mmDCORE0_TPC6_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_1] = mmDCORE0_TPC6_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_2] = mmDCORE0_TPC6_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_3] = mmDCORE0_TPC6_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0] = mmDCORE1_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_1] = mmDCORE1_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_2] = mmDCORE1_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_3] = mmDCORE1_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_0] = mmDCORE1_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_1] = mmDCORE1_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_2] = mmDCORE1_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_3] = mmDCORE1_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_0] = mmDCORE1_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_1] = mmDCORE1_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_2] = mmDCORE1_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_3] = mmDCORE1_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_0] = mmDCORE1_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_1] = mmDCORE1_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_2] = mmDCORE1_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_3] = mmDCORE1_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_0] = mmDCORE1_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_1] = mmDCORE1_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_2] = mmDCORE1_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_3] = mmDCORE1_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_0] = mmDCORE1_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_1] = mmDCORE1_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_2] = mmDCORE1_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_3] = mmDCORE1_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_0] = mmDCORE1_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_1] = mmDCORE1_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_2] = mmDCORE1_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_3] = mmDCORE1_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_0] = mmDCORE1_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_1] = mmDCORE1_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_2] = mmDCORE1_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_3] = mmDCORE1_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_0] = mmDCORE1_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_1] = mmDCORE1_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_2] = mmDCORE1_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_3] = mmDCORE1_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0] = mmDCORE2_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_1] = mmDCORE2_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_2] = mmDCORE2_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_3] = mmDCORE2_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_0] = mmDCORE2_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_1] = mmDCORE2_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_2] = mmDCORE2_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_3] = mmDCORE2_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_0] = mmDCORE2_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_1] = mmDCORE2_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_2] = mmDCORE2_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_3] = mmDCORE2_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_0] = mmDCORE2_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_1] = mmDCORE2_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_2] = mmDCORE2_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_3] = mmDCORE2_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_0] = mmDCORE2_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_1] = mmDCORE2_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_2] = mmDCORE2_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_3] = mmDCORE2_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_0] = mmDCORE2_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_1] = mmDCORE2_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_2] = mmDCORE2_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_3] = mmDCORE2_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_0] = mmDCORE2_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_1] = mmDCORE2_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_2] = mmDCORE2_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_3] = mmDCORE2_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_0] = mmDCORE2_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_1] = mmDCORE2_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_2] = mmDCORE2_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_3] = mmDCORE2_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_0] = mmDCORE2_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_1] = mmDCORE2_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_2] = mmDCORE2_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_3] = mmDCORE2_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0] = mmDCORE3_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_1] = mmDCORE3_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_2] = mmDCORE3_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_3] = mmDCORE3_EDMA0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_0] = mmDCORE3_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_1] = mmDCORE3_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_2] = mmDCORE3_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3] = mmDCORE3_EDMA1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_0] = mmDCORE3_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_1] = mmDCORE3_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_2] = mmDCORE3_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_3] = mmDCORE3_MME_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_0] = mmDCORE3_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_1] = mmDCORE3_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_2] = mmDCORE3_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_3] = mmDCORE3_TPC0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_0] = mmDCORE3_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_1] = mmDCORE3_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_2] = mmDCORE3_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_3] = mmDCORE3_TPC1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_0] = mmDCORE3_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_1] = mmDCORE3_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_2] = mmDCORE3_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_3] = mmDCORE3_TPC2_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_0] = mmDCORE3_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_1] = mmDCORE3_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_2] = mmDCORE3_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_3] = mmDCORE3_TPC3_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_0] = mmDCORE3_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_1] = mmDCORE3_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_2] = mmDCORE3_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_3] = mmDCORE3_TPC4_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_0] = mmDCORE3_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_1] = mmDCORE3_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_2] = mmDCORE3_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_3] = mmDCORE3_TPC5_QM_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_0_0] = mmNIC0_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_0_1] = mmNIC0_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_0_2] = mmNIC0_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_0_3] = mmNIC0_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_1_0] = mmNIC0_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_1_1] = mmNIC0_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_1_2] = mmNIC0_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_1_3] = mmNIC0_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_2_0] = mmNIC1_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_2_1] = mmNIC1_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_2_2] = mmNIC1_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_2_3] = mmNIC1_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_3_0] = mmNIC1_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_3_1] = mmNIC1_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_3_2] = mmNIC1_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_3_3] = mmNIC1_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_4_0] = mmNIC2_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_4_1] = mmNIC2_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_4_2] = mmNIC2_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_4_3] = mmNIC2_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_5_0] = mmNIC2_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_5_1] = mmNIC2_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_5_2] = mmNIC2_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_5_3] = mmNIC2_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_6_0] = mmNIC3_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_6_1] = mmNIC3_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_6_2] = mmNIC3_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_6_3] = mmNIC3_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_7_0] = mmNIC3_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_7_1] = mmNIC3_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_7_2] = mmNIC3_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_7_3] = mmNIC3_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_8_0] = mmNIC4_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_8_1] = mmNIC4_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_8_2] = mmNIC4_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_8_3] = mmNIC4_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_9_0] = mmNIC4_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_9_1] = mmNIC4_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_9_2] = mmNIC4_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_9_3] = mmNIC4_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_10_0] = mmNIC5_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_10_1] = mmNIC5_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_10_2] = mmNIC5_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_10_3] = mmNIC5_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_11_0] = mmNIC5_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_11_1] = mmNIC5_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_11_2] = mmNIC5_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_11_3] = mmNIC5_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_12_0] = mmNIC6_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_12_1] = mmNIC6_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_12_2] = mmNIC6_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_12_3] = mmNIC6_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_13_0] = mmNIC6_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_13_1] = mmNIC6_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_13_2] = mmNIC6_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_13_3] = mmNIC6_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_14_0] = mmNIC7_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_14_1] = mmNIC7_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_14_2] = mmNIC7_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_14_3] = mmNIC7_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_15_0] = mmNIC7_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_15_1] = mmNIC7_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_15_2] = mmNIC7_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_15_3] = mmNIC7_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_16_0] = mmNIC8_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_16_1] = mmNIC8_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_16_2] = mmNIC8_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_16_3] = mmNIC8_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_17_0] = mmNIC8_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_17_1] = mmNIC8_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_17_2] = mmNIC8_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_17_3] = mmNIC8_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_18_0] = mmNIC9_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_18_1] = mmNIC9_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_18_2] = mmNIC9_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_18_3] = mmNIC9_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_19_0] = mmNIC9_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_19_1] = mmNIC9_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_19_2] = mmNIC9_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_19_3] = mmNIC9_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_20_0] = mmNIC10_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_20_1] = mmNIC10_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_20_2] = mmNIC10_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_20_3] = mmNIC10_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_21_0] = mmNIC10_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_21_1] = mmNIC10_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_21_2] = mmNIC10_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_21_3] = mmNIC10_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_22_0] = mmNIC11_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_22_1] = mmNIC11_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_22_2] = mmNIC11_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_22_3] = mmNIC11_QM0_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_23_0] = mmNIC11_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_23_1] = mmNIC11_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_23_2] = mmNIC11_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_NIC_23_3] = mmNIC11_QM1_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_0_0] = mmROT0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_0_1] = mmROT0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_0_2] = mmROT0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_0_3] = mmROT0_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_1_0] = mmROT1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_1_1] = mmROT1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_1_2] = mmROT1_QM_BASE,\n\t[GAUDI2_QUEUE_ID_ROT_1_3] = mmROT1_QM_BASE\n};\n\nstatic const u32 gaudi2_arc_blocks_bases[NUM_ARC_CPUS] = {\n\t[CPU_ID_SCHED_ARC0] = mmARC_FARM_ARC0_AUX_BASE,\n\t[CPU_ID_SCHED_ARC1] = mmARC_FARM_ARC1_AUX_BASE,\n\t[CPU_ID_SCHED_ARC2] = mmARC_FARM_ARC2_AUX_BASE,\n\t[CPU_ID_SCHED_ARC3] = mmARC_FARM_ARC3_AUX_BASE,\n\t[CPU_ID_SCHED_ARC4] = mmDCORE1_MME_QM_ARC_AUX_BASE,\n\t[CPU_ID_SCHED_ARC5] = mmDCORE3_MME_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC0] = mmDCORE0_TPC0_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC1] = mmDCORE0_TPC1_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC2] = mmDCORE0_TPC2_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC3] = mmDCORE0_TPC3_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC4] = mmDCORE0_TPC4_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC5] = mmDCORE0_TPC5_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC6] = mmDCORE1_TPC0_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC7] = mmDCORE1_TPC1_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC8] = mmDCORE1_TPC2_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC9] = mmDCORE1_TPC3_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC10] = mmDCORE1_TPC4_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC11] = mmDCORE1_TPC5_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC12] = mmDCORE2_TPC0_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC13] = mmDCORE2_TPC1_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC14] = mmDCORE2_TPC2_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC15] = mmDCORE2_TPC3_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC16] = mmDCORE2_TPC4_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC17] = mmDCORE2_TPC5_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC18] = mmDCORE3_TPC0_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC19] = mmDCORE3_TPC1_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC20] = mmDCORE3_TPC2_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC21] = mmDCORE3_TPC3_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC22] = mmDCORE3_TPC4_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC23] = mmDCORE3_TPC5_QM_ARC_AUX_BASE,\n\t[CPU_ID_TPC_QMAN_ARC24] = mmDCORE0_TPC6_QM_ARC_AUX_BASE,\n\t[CPU_ID_MME_QMAN_ARC0] = mmDCORE0_MME_QM_ARC_AUX_BASE,\n\t[CPU_ID_MME_QMAN_ARC1] = mmDCORE2_MME_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC0] = mmDCORE0_EDMA0_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC1] = mmDCORE0_EDMA1_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC2] = mmDCORE1_EDMA0_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC3] = mmDCORE1_EDMA1_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC4] = mmDCORE2_EDMA0_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC5] = mmDCORE2_EDMA1_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC6] = mmDCORE3_EDMA0_QM_ARC_AUX_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC7] = mmDCORE3_EDMA1_QM_ARC_AUX_BASE,\n\t[CPU_ID_PDMA_QMAN_ARC0] = mmPDMA0_QM_ARC_AUX_BASE,\n\t[CPU_ID_PDMA_QMAN_ARC1] = mmPDMA1_QM_ARC_AUX_BASE,\n\t[CPU_ID_ROT_QMAN_ARC0] = mmROT0_QM_ARC_AUX_BASE,\n\t[CPU_ID_ROT_QMAN_ARC1] = mmROT1_QM_ARC_AUX_BASE,\n\t[CPU_ID_NIC_QMAN_ARC0] = mmNIC0_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC1] = mmNIC0_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC2] = mmNIC1_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC3] = mmNIC1_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC4] = mmNIC2_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC5] = mmNIC2_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC6] = mmNIC3_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC7] = mmNIC3_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC8] = mmNIC4_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC9] = mmNIC4_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC10] = mmNIC5_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC11] = mmNIC5_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC12] = mmNIC6_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC13] = mmNIC6_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC14] = mmNIC7_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC15] = mmNIC7_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC16] = mmNIC8_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC17] = mmNIC8_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC18] = mmNIC9_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC19] = mmNIC9_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC20] = mmNIC10_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC21] = mmNIC10_QM_ARC_AUX1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC22] = mmNIC11_QM_ARC_AUX0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC23] = mmNIC11_QM_ARC_AUX1_BASE,\n};\n\nstatic const u32 gaudi2_arc_dccm_bases[NUM_ARC_CPUS] = {\n\t[CPU_ID_SCHED_ARC0] = mmARC_FARM_ARC0_DCCM0_BASE,\n\t[CPU_ID_SCHED_ARC1] = mmARC_FARM_ARC1_DCCM0_BASE,\n\t[CPU_ID_SCHED_ARC2] = mmARC_FARM_ARC2_DCCM0_BASE,\n\t[CPU_ID_SCHED_ARC3] = mmARC_FARM_ARC3_DCCM0_BASE,\n\t[CPU_ID_SCHED_ARC4] = mmDCORE1_MME_QM_ARC_DCCM_BASE,\n\t[CPU_ID_SCHED_ARC5] = mmDCORE3_MME_QM_ARC_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC0] = mmDCORE0_TPC0_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC1] = mmDCORE0_TPC1_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC2] = mmDCORE0_TPC2_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC3] = mmDCORE0_TPC3_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC4] = mmDCORE0_TPC4_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC5] = mmDCORE0_TPC5_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC6] = mmDCORE1_TPC0_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC7] = mmDCORE1_TPC1_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC8] = mmDCORE1_TPC2_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC9] = mmDCORE1_TPC3_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC10] = mmDCORE1_TPC4_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC11] = mmDCORE1_TPC5_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC12] = mmDCORE2_TPC0_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC13] = mmDCORE2_TPC1_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC14] = mmDCORE2_TPC2_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC15] = mmDCORE2_TPC3_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC16] = mmDCORE2_TPC4_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC17] = mmDCORE2_TPC5_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC18] = mmDCORE3_TPC0_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC19] = mmDCORE3_TPC1_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC20] = mmDCORE3_TPC2_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC21] = mmDCORE3_TPC3_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC22] = mmDCORE3_TPC4_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC23] = mmDCORE3_TPC5_QM_DCCM_BASE,\n\t[CPU_ID_TPC_QMAN_ARC24] = mmDCORE0_TPC6_QM_DCCM_BASE,\n\t[CPU_ID_MME_QMAN_ARC0] = mmDCORE0_MME_QM_ARC_DCCM_BASE,\n\t[CPU_ID_MME_QMAN_ARC1] = mmDCORE2_MME_QM_ARC_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC0] = mmDCORE0_EDMA0_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC1] = mmDCORE0_EDMA1_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC2] = mmDCORE1_EDMA0_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC3] = mmDCORE1_EDMA1_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC4] = mmDCORE2_EDMA0_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC5] = mmDCORE2_EDMA1_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC6] = mmDCORE3_EDMA0_QM_DCCM_BASE,\n\t[CPU_ID_EDMA_QMAN_ARC7] = mmDCORE3_EDMA1_QM_DCCM_BASE,\n\t[CPU_ID_PDMA_QMAN_ARC0] = mmPDMA0_QM_ARC_DCCM_BASE,\n\t[CPU_ID_PDMA_QMAN_ARC1] = mmPDMA1_QM_ARC_DCCM_BASE,\n\t[CPU_ID_ROT_QMAN_ARC0] = mmROT0_QM_ARC_DCCM_BASE,\n\t[CPU_ID_ROT_QMAN_ARC1] = mmROT1_QM_ARC_DCCM_BASE,\n\t[CPU_ID_NIC_QMAN_ARC0] = mmNIC0_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC1] = mmNIC0_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC2] = mmNIC1_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC3] = mmNIC1_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC4] = mmNIC2_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC5] = mmNIC2_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC6] = mmNIC3_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC7] = mmNIC3_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC8] = mmNIC4_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC9] = mmNIC4_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC10] = mmNIC5_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC11] = mmNIC5_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC12] = mmNIC6_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC13] = mmNIC6_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC14] = mmNIC7_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC15] = mmNIC7_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC16] = mmNIC8_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC17] = mmNIC8_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC18] = mmNIC9_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC19] = mmNIC9_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC20] = mmNIC10_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC21] = mmNIC10_QM_DCCM1_BASE,\n\t[CPU_ID_NIC_QMAN_ARC22] = mmNIC11_QM_DCCM0_BASE,\n\t[CPU_ID_NIC_QMAN_ARC23] = mmNIC11_QM_DCCM1_BASE,\n};\n\nconst u32 gaudi2_mme_ctrl_lo_blocks_bases[MME_ID_SIZE] = {\n\t[MME_ID_DCORE0] = mmDCORE0_MME_CTRL_LO_BASE,\n\t[MME_ID_DCORE1] = mmDCORE1_MME_CTRL_LO_BASE,\n\t[MME_ID_DCORE2] = mmDCORE2_MME_CTRL_LO_BASE,\n\t[MME_ID_DCORE3] = mmDCORE3_MME_CTRL_LO_BASE,\n};\n\nstatic const u32 gaudi2_queue_id_to_arc_id[GAUDI2_QUEUE_ID_SIZE] = {\n\t[GAUDI2_QUEUE_ID_PDMA_0_0] = CPU_ID_PDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_PDMA_0_1] = CPU_ID_PDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_PDMA_0_2] = CPU_ID_PDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_PDMA_0_3] = CPU_ID_PDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_PDMA_1_0] = CPU_ID_PDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_PDMA_1_1] = CPU_ID_PDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_PDMA_1_2] = CPU_ID_PDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_PDMA_1_3] = CPU_ID_PDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0] = CPU_ID_EDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_1] = CPU_ID_EDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_2] = CPU_ID_EDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_0_3] = CPU_ID_EDMA_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_0] = CPU_ID_EDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_1] = CPU_ID_EDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_2] = CPU_ID_EDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_EDMA_1_3] = CPU_ID_EDMA_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_0] = CPU_ID_MME_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_1] = CPU_ID_MME_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_2] = CPU_ID_MME_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_MME_0_3] = CPU_ID_MME_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_0] = CPU_ID_TPC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_1] = CPU_ID_TPC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_2] = CPU_ID_TPC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_0_3] = CPU_ID_TPC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_0] = CPU_ID_TPC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_1] = CPU_ID_TPC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_2] = CPU_ID_TPC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_1_3] = CPU_ID_TPC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_0] = CPU_ID_TPC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_1] = CPU_ID_TPC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_2] = CPU_ID_TPC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_2_3] = CPU_ID_TPC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_0] = CPU_ID_TPC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_1] = CPU_ID_TPC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_2] = CPU_ID_TPC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_3_3] = CPU_ID_TPC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_0] = CPU_ID_TPC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_1] = CPU_ID_TPC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_2] = CPU_ID_TPC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_4_3] = CPU_ID_TPC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_0] = CPU_ID_TPC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_1] = CPU_ID_TPC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_2] = CPU_ID_TPC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_5_3] = CPU_ID_TPC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_0] = CPU_ID_TPC_QMAN_ARC24,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_1] = CPU_ID_TPC_QMAN_ARC24,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_2] = CPU_ID_TPC_QMAN_ARC24,\n\t[GAUDI2_QUEUE_ID_DCORE0_TPC_6_3] = CPU_ID_TPC_QMAN_ARC24,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0] = CPU_ID_EDMA_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_1] = CPU_ID_EDMA_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_2] = CPU_ID_EDMA_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_0_3] = CPU_ID_EDMA_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_0] = CPU_ID_EDMA_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_1] = CPU_ID_EDMA_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_2] = CPU_ID_EDMA_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE1_EDMA_1_3] = CPU_ID_EDMA_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_0] = CPU_ID_SCHED_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_1] = CPU_ID_SCHED_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_2] = CPU_ID_SCHED_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE1_MME_0_3] = CPU_ID_SCHED_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_0] = CPU_ID_TPC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_1] = CPU_ID_TPC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_2] = CPU_ID_TPC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_0_3] = CPU_ID_TPC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_0] = CPU_ID_TPC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_1] = CPU_ID_TPC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_2] = CPU_ID_TPC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_1_3] = CPU_ID_TPC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_0] = CPU_ID_TPC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_1] = CPU_ID_TPC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_2] = CPU_ID_TPC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_2_3] = CPU_ID_TPC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_0] = CPU_ID_TPC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_1] = CPU_ID_TPC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_2] = CPU_ID_TPC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_3_3] = CPU_ID_TPC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_0] = CPU_ID_TPC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_1] = CPU_ID_TPC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_2] = CPU_ID_TPC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_4_3] = CPU_ID_TPC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_0] = CPU_ID_TPC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_1] = CPU_ID_TPC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_2] = CPU_ID_TPC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_DCORE1_TPC_5_3] = CPU_ID_TPC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0] = CPU_ID_EDMA_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_1] = CPU_ID_EDMA_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_2] = CPU_ID_EDMA_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_0_3] = CPU_ID_EDMA_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_0] = CPU_ID_EDMA_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_1] = CPU_ID_EDMA_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_2] = CPU_ID_EDMA_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE2_EDMA_1_3] = CPU_ID_EDMA_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_0] = CPU_ID_MME_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_1] = CPU_ID_MME_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_2] = CPU_ID_MME_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE2_MME_0_3] = CPU_ID_MME_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_0] = CPU_ID_TPC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_1] = CPU_ID_TPC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_2] = CPU_ID_TPC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_0_3] = CPU_ID_TPC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_0] = CPU_ID_TPC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_1] = CPU_ID_TPC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_2] = CPU_ID_TPC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_1_3] = CPU_ID_TPC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_0] = CPU_ID_TPC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_1] = CPU_ID_TPC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_2] = CPU_ID_TPC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_2_3] = CPU_ID_TPC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_0] = CPU_ID_TPC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_1] = CPU_ID_TPC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_2] = CPU_ID_TPC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_3_3] = CPU_ID_TPC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_0] = CPU_ID_TPC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_1] = CPU_ID_TPC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_2] = CPU_ID_TPC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_4_3] = CPU_ID_TPC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_0] = CPU_ID_TPC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_1] = CPU_ID_TPC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_2] = CPU_ID_TPC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_DCORE2_TPC_5_3] = CPU_ID_TPC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0] = CPU_ID_EDMA_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_1] = CPU_ID_EDMA_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_2] = CPU_ID_EDMA_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_0_3] = CPU_ID_EDMA_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_0] = CPU_ID_EDMA_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_1] = CPU_ID_EDMA_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_2] = CPU_ID_EDMA_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3] = CPU_ID_EDMA_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_0] = CPU_ID_SCHED_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_1] = CPU_ID_SCHED_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_2] = CPU_ID_SCHED_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE3_MME_0_3] = CPU_ID_SCHED_ARC5,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_0] = CPU_ID_TPC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_1] = CPU_ID_TPC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_2] = CPU_ID_TPC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_0_3] = CPU_ID_TPC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_0] = CPU_ID_TPC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_1] = CPU_ID_TPC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_2] = CPU_ID_TPC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_1_3] = CPU_ID_TPC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_0] = CPU_ID_TPC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_1] = CPU_ID_TPC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_2] = CPU_ID_TPC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_2_3] = CPU_ID_TPC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_0] = CPU_ID_TPC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_1] = CPU_ID_TPC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_2] = CPU_ID_TPC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_3_3] = CPU_ID_TPC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_0] = CPU_ID_TPC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_1] = CPU_ID_TPC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_2] = CPU_ID_TPC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_4_3] = CPU_ID_TPC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_0] = CPU_ID_TPC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_1] = CPU_ID_TPC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_2] = CPU_ID_TPC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_DCORE3_TPC_5_3] = CPU_ID_TPC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_NIC_0_0] = CPU_ID_NIC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_NIC_0_1] = CPU_ID_NIC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_NIC_0_2] = CPU_ID_NIC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_NIC_0_3] = CPU_ID_NIC_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_NIC_1_0] = CPU_ID_NIC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_NIC_1_1] = CPU_ID_NIC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_NIC_1_2] = CPU_ID_NIC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_NIC_1_3] = CPU_ID_NIC_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_NIC_2_0] = CPU_ID_NIC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_NIC_2_1] = CPU_ID_NIC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_NIC_2_2] = CPU_ID_NIC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_NIC_2_3] = CPU_ID_NIC_QMAN_ARC2,\n\t[GAUDI2_QUEUE_ID_NIC_3_0] = CPU_ID_NIC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_NIC_3_1] = CPU_ID_NIC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_NIC_3_2] = CPU_ID_NIC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_NIC_3_3] = CPU_ID_NIC_QMAN_ARC3,\n\t[GAUDI2_QUEUE_ID_NIC_4_0] = CPU_ID_NIC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_NIC_4_1] = CPU_ID_NIC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_NIC_4_2] = CPU_ID_NIC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_NIC_4_3] = CPU_ID_NIC_QMAN_ARC4,\n\t[GAUDI2_QUEUE_ID_NIC_5_0] = CPU_ID_NIC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_NIC_5_1] = CPU_ID_NIC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_NIC_5_2] = CPU_ID_NIC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_NIC_5_3] = CPU_ID_NIC_QMAN_ARC5,\n\t[GAUDI2_QUEUE_ID_NIC_6_0] = CPU_ID_NIC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_NIC_6_1] = CPU_ID_NIC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_NIC_6_2] = CPU_ID_NIC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_NIC_6_3] = CPU_ID_NIC_QMAN_ARC6,\n\t[GAUDI2_QUEUE_ID_NIC_7_0] = CPU_ID_NIC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_NIC_7_1] = CPU_ID_NIC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_NIC_7_2] = CPU_ID_NIC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_NIC_7_3] = CPU_ID_NIC_QMAN_ARC7,\n\t[GAUDI2_QUEUE_ID_NIC_8_0] = CPU_ID_NIC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_NIC_8_1] = CPU_ID_NIC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_NIC_8_2] = CPU_ID_NIC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_NIC_8_3] = CPU_ID_NIC_QMAN_ARC8,\n\t[GAUDI2_QUEUE_ID_NIC_9_0] = CPU_ID_NIC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_NIC_9_1] = CPU_ID_NIC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_NIC_9_2] = CPU_ID_NIC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_NIC_9_3] = CPU_ID_NIC_QMAN_ARC9,\n\t[GAUDI2_QUEUE_ID_NIC_10_0] = CPU_ID_NIC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_NIC_10_1] = CPU_ID_NIC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_NIC_10_2] = CPU_ID_NIC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_NIC_10_3] = CPU_ID_NIC_QMAN_ARC10,\n\t[GAUDI2_QUEUE_ID_NIC_11_0] = CPU_ID_NIC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_NIC_11_1] = CPU_ID_NIC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_NIC_11_2] = CPU_ID_NIC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_NIC_11_3] = CPU_ID_NIC_QMAN_ARC11,\n\t[GAUDI2_QUEUE_ID_NIC_12_0] = CPU_ID_NIC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_NIC_12_1] = CPU_ID_NIC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_NIC_12_2] = CPU_ID_NIC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_NIC_12_3] = CPU_ID_NIC_QMAN_ARC12,\n\t[GAUDI2_QUEUE_ID_NIC_13_0] = CPU_ID_NIC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_NIC_13_1] = CPU_ID_NIC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_NIC_13_2] = CPU_ID_NIC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_NIC_13_3] = CPU_ID_NIC_QMAN_ARC13,\n\t[GAUDI2_QUEUE_ID_NIC_14_0] = CPU_ID_NIC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_NIC_14_1] = CPU_ID_NIC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_NIC_14_2] = CPU_ID_NIC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_NIC_14_3] = CPU_ID_NIC_QMAN_ARC14,\n\t[GAUDI2_QUEUE_ID_NIC_15_0] = CPU_ID_NIC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_NIC_15_1] = CPU_ID_NIC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_NIC_15_2] = CPU_ID_NIC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_NIC_15_3] = CPU_ID_NIC_QMAN_ARC15,\n\t[GAUDI2_QUEUE_ID_NIC_16_0] = CPU_ID_NIC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_NIC_16_1] = CPU_ID_NIC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_NIC_16_2] = CPU_ID_NIC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_NIC_16_3] = CPU_ID_NIC_QMAN_ARC16,\n\t[GAUDI2_QUEUE_ID_NIC_17_0] = CPU_ID_NIC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_NIC_17_1] = CPU_ID_NIC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_NIC_17_2] = CPU_ID_NIC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_NIC_17_3] = CPU_ID_NIC_QMAN_ARC17,\n\t[GAUDI2_QUEUE_ID_NIC_18_0] = CPU_ID_NIC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_NIC_18_1] = CPU_ID_NIC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_NIC_18_2] = CPU_ID_NIC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_NIC_18_3] = CPU_ID_NIC_QMAN_ARC18,\n\t[GAUDI2_QUEUE_ID_NIC_19_0] = CPU_ID_NIC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_NIC_19_1] = CPU_ID_NIC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_NIC_19_2] = CPU_ID_NIC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_NIC_19_3] = CPU_ID_NIC_QMAN_ARC19,\n\t[GAUDI2_QUEUE_ID_NIC_20_0] = CPU_ID_NIC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_NIC_20_1] = CPU_ID_NIC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_NIC_20_2] = CPU_ID_NIC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_NIC_20_3] = CPU_ID_NIC_QMAN_ARC20,\n\t[GAUDI2_QUEUE_ID_NIC_21_0] = CPU_ID_NIC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_NIC_21_1] = CPU_ID_NIC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_NIC_21_2] = CPU_ID_NIC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_NIC_21_3] = CPU_ID_NIC_QMAN_ARC21,\n\t[GAUDI2_QUEUE_ID_NIC_22_0] = CPU_ID_NIC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_NIC_22_1] = CPU_ID_NIC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_NIC_22_2] = CPU_ID_NIC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_NIC_22_3] = CPU_ID_NIC_QMAN_ARC22,\n\t[GAUDI2_QUEUE_ID_NIC_23_0] = CPU_ID_NIC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_NIC_23_1] = CPU_ID_NIC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_NIC_23_2] = CPU_ID_NIC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_NIC_23_3] = CPU_ID_NIC_QMAN_ARC23,\n\t[GAUDI2_QUEUE_ID_ROT_0_0] = CPU_ID_ROT_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_ROT_0_1] = CPU_ID_ROT_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_ROT_0_2] = CPU_ID_ROT_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_ROT_0_3] = CPU_ID_ROT_QMAN_ARC0,\n\t[GAUDI2_QUEUE_ID_ROT_1_0] = CPU_ID_ROT_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_ROT_1_1] = CPU_ID_ROT_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_ROT_1_2] = CPU_ID_ROT_QMAN_ARC1,\n\t[GAUDI2_QUEUE_ID_ROT_1_3] = CPU_ID_ROT_QMAN_ARC1\n};\n\nconst u32 gaudi2_dma_core_blocks_bases[DMA_CORE_ID_SIZE] = {\n\t[DMA_CORE_ID_PDMA0] = mmPDMA0_CORE_BASE,\n\t[DMA_CORE_ID_PDMA1] = mmPDMA1_CORE_BASE,\n\t[DMA_CORE_ID_EDMA0] = mmDCORE0_EDMA0_CORE_BASE,\n\t[DMA_CORE_ID_EDMA1] = mmDCORE0_EDMA1_CORE_BASE,\n\t[DMA_CORE_ID_EDMA2] = mmDCORE1_EDMA0_CORE_BASE,\n\t[DMA_CORE_ID_EDMA3] = mmDCORE1_EDMA1_CORE_BASE,\n\t[DMA_CORE_ID_EDMA4] = mmDCORE2_EDMA0_CORE_BASE,\n\t[DMA_CORE_ID_EDMA5] = mmDCORE2_EDMA1_CORE_BASE,\n\t[DMA_CORE_ID_EDMA6] = mmDCORE3_EDMA0_CORE_BASE,\n\t[DMA_CORE_ID_EDMA7] = mmDCORE3_EDMA1_CORE_BASE,\n\t[DMA_CORE_ID_KDMA] = mmARC_FARM_KDMA_BASE\n};\n\nconst u32 gaudi2_mme_acc_blocks_bases[MME_ID_SIZE] = {\n\t[MME_ID_DCORE0] = mmDCORE0_MME_ACC_BASE,\n\t[MME_ID_DCORE1] = mmDCORE1_MME_ACC_BASE,\n\t[MME_ID_DCORE2] = mmDCORE2_MME_ACC_BASE,\n\t[MME_ID_DCORE3] = mmDCORE3_MME_ACC_BASE\n};\n\nstatic const u32 gaudi2_tpc_cfg_blocks_bases[TPC_ID_SIZE] = {\n\t[TPC_ID_DCORE0_TPC0] = mmDCORE0_TPC0_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC1] = mmDCORE0_TPC1_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC2] = mmDCORE0_TPC2_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC3] = mmDCORE0_TPC3_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC4] = mmDCORE0_TPC4_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC5] = mmDCORE0_TPC5_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC0] = mmDCORE1_TPC0_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC1] = mmDCORE1_TPC1_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC2] = mmDCORE1_TPC2_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC3] = mmDCORE1_TPC3_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC4] = mmDCORE1_TPC4_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC5] = mmDCORE1_TPC5_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC0] = mmDCORE2_TPC0_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC1] = mmDCORE2_TPC1_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC2] = mmDCORE2_TPC2_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC3] = mmDCORE2_TPC3_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC4] = mmDCORE2_TPC4_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC5] = mmDCORE2_TPC5_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC0] = mmDCORE3_TPC0_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC1] = mmDCORE3_TPC1_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC2] = mmDCORE3_TPC2_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC3] = mmDCORE3_TPC3_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC4] = mmDCORE3_TPC4_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC5] = mmDCORE3_TPC5_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC6] = mmDCORE0_TPC6_CFG_BASE,\n};\n\nstatic const u32 gaudi2_tpc_eml_cfg_blocks_bases[TPC_ID_SIZE] = {\n\t[TPC_ID_DCORE0_TPC0] = mmDCORE0_TPC0_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC1] = mmDCORE0_TPC1_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC2] = mmDCORE0_TPC2_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC3] = mmDCORE0_TPC3_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC4] = mmDCORE0_TPC4_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC5] = mmDCORE0_TPC5_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC0] = mmDCORE1_TPC0_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC1] = mmDCORE1_TPC1_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC2] = mmDCORE1_TPC2_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC3] = mmDCORE1_TPC3_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC4] = mmDCORE1_TPC4_EML_CFG_BASE,\n\t[TPC_ID_DCORE1_TPC5] = mmDCORE1_TPC5_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC0] = mmDCORE2_TPC0_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC1] = mmDCORE2_TPC1_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC2] = mmDCORE2_TPC2_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC3] = mmDCORE2_TPC3_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC4] = mmDCORE2_TPC4_EML_CFG_BASE,\n\t[TPC_ID_DCORE2_TPC5] = mmDCORE2_TPC5_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC0] = mmDCORE3_TPC0_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC1] = mmDCORE3_TPC1_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC2] = mmDCORE3_TPC2_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC3] = mmDCORE3_TPC3_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC4] = mmDCORE3_TPC4_EML_CFG_BASE,\n\t[TPC_ID_DCORE3_TPC5] = mmDCORE3_TPC5_EML_CFG_BASE,\n\t[TPC_ID_DCORE0_TPC6] = mmDCORE0_TPC6_EML_CFG_BASE,\n};\n\nconst u32 gaudi2_rot_blocks_bases[ROTATOR_ID_SIZE] = {\n\t[ROTATOR_ID_0] = mmROT0_BASE,\n\t[ROTATOR_ID_1] = mmROT1_BASE\n};\n\nstatic const u32 gaudi2_tpc_id_to_queue_id[TPC_ID_SIZE] = {\n\t[TPC_ID_DCORE0_TPC0] = GAUDI2_QUEUE_ID_DCORE0_TPC_0_0,\n\t[TPC_ID_DCORE0_TPC1] = GAUDI2_QUEUE_ID_DCORE0_TPC_1_0,\n\t[TPC_ID_DCORE0_TPC2] = GAUDI2_QUEUE_ID_DCORE0_TPC_2_0,\n\t[TPC_ID_DCORE0_TPC3] = GAUDI2_QUEUE_ID_DCORE0_TPC_3_0,\n\t[TPC_ID_DCORE0_TPC4] = GAUDI2_QUEUE_ID_DCORE0_TPC_4_0,\n\t[TPC_ID_DCORE0_TPC5] = GAUDI2_QUEUE_ID_DCORE0_TPC_5_0,\n\t[TPC_ID_DCORE1_TPC0] = GAUDI2_QUEUE_ID_DCORE1_TPC_0_0,\n\t[TPC_ID_DCORE1_TPC1] = GAUDI2_QUEUE_ID_DCORE1_TPC_1_0,\n\t[TPC_ID_DCORE1_TPC2] = GAUDI2_QUEUE_ID_DCORE1_TPC_2_0,\n\t[TPC_ID_DCORE1_TPC3] = GAUDI2_QUEUE_ID_DCORE1_TPC_3_0,\n\t[TPC_ID_DCORE1_TPC4] = GAUDI2_QUEUE_ID_DCORE1_TPC_4_0,\n\t[TPC_ID_DCORE1_TPC5] = GAUDI2_QUEUE_ID_DCORE1_TPC_5_0,\n\t[TPC_ID_DCORE2_TPC0] = GAUDI2_QUEUE_ID_DCORE2_TPC_0_0,\n\t[TPC_ID_DCORE2_TPC1] = GAUDI2_QUEUE_ID_DCORE2_TPC_1_0,\n\t[TPC_ID_DCORE2_TPC2] = GAUDI2_QUEUE_ID_DCORE2_TPC_2_0,\n\t[TPC_ID_DCORE2_TPC3] = GAUDI2_QUEUE_ID_DCORE2_TPC_3_0,\n\t[TPC_ID_DCORE2_TPC4] = GAUDI2_QUEUE_ID_DCORE2_TPC_4_0,\n\t[TPC_ID_DCORE2_TPC5] = GAUDI2_QUEUE_ID_DCORE2_TPC_5_0,\n\t[TPC_ID_DCORE3_TPC0] = GAUDI2_QUEUE_ID_DCORE3_TPC_0_0,\n\t[TPC_ID_DCORE3_TPC1] = GAUDI2_QUEUE_ID_DCORE3_TPC_1_0,\n\t[TPC_ID_DCORE3_TPC2] = GAUDI2_QUEUE_ID_DCORE3_TPC_2_0,\n\t[TPC_ID_DCORE3_TPC3] = GAUDI2_QUEUE_ID_DCORE3_TPC_3_0,\n\t[TPC_ID_DCORE3_TPC4] = GAUDI2_QUEUE_ID_DCORE3_TPC_4_0,\n\t[TPC_ID_DCORE3_TPC5] = GAUDI2_QUEUE_ID_DCORE3_TPC_5_0,\n\t[TPC_ID_DCORE0_TPC6] = GAUDI2_QUEUE_ID_DCORE0_TPC_6_0,\n};\n\nstatic const u32 gaudi2_rot_id_to_queue_id[ROTATOR_ID_SIZE] = {\n\t[ROTATOR_ID_0] = GAUDI2_QUEUE_ID_ROT_0_0,\n\t[ROTATOR_ID_1] = GAUDI2_QUEUE_ID_ROT_1_0,\n};\n\nstatic const u32 gaudi2_tpc_engine_id_to_tpc_id[] = {\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_0] = TPC_ID_DCORE0_TPC0,\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_1] = TPC_ID_DCORE0_TPC1,\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_2] = TPC_ID_DCORE0_TPC2,\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_3] = TPC_ID_DCORE0_TPC3,\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_4] = TPC_ID_DCORE0_TPC4,\n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_5] = TPC_ID_DCORE0_TPC5,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_0] = TPC_ID_DCORE1_TPC0,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_1] = TPC_ID_DCORE1_TPC1,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_2] = TPC_ID_DCORE1_TPC2,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_3] = TPC_ID_DCORE1_TPC3,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_4] = TPC_ID_DCORE1_TPC4,\n\t[GAUDI2_DCORE1_ENGINE_ID_TPC_5] = TPC_ID_DCORE1_TPC5,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_0] = TPC_ID_DCORE2_TPC0,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_1] = TPC_ID_DCORE2_TPC1,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_2] = TPC_ID_DCORE2_TPC2,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_3] = TPC_ID_DCORE2_TPC3,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_4] = TPC_ID_DCORE2_TPC4,\n\t[GAUDI2_DCORE2_ENGINE_ID_TPC_5] = TPC_ID_DCORE2_TPC5,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_0] = TPC_ID_DCORE3_TPC0,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_1] = TPC_ID_DCORE3_TPC1,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_2] = TPC_ID_DCORE3_TPC2,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_3] = TPC_ID_DCORE3_TPC3,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_4] = TPC_ID_DCORE3_TPC4,\n\t[GAUDI2_DCORE3_ENGINE_ID_TPC_5] = TPC_ID_DCORE3_TPC5,\n\t \n\t[GAUDI2_DCORE0_ENGINE_ID_TPC_6] = TPC_ID_DCORE0_TPC6,\n};\n\nstatic const u32 gaudi2_mme_engine_id_to_mme_id[] = {\n\t[GAUDI2_DCORE0_ENGINE_ID_MME] = MME_ID_DCORE0,\n\t[GAUDI2_DCORE1_ENGINE_ID_MME] = MME_ID_DCORE1,\n\t[GAUDI2_DCORE2_ENGINE_ID_MME] = MME_ID_DCORE2,\n\t[GAUDI2_DCORE3_ENGINE_ID_MME] = MME_ID_DCORE3,\n};\n\nstatic const u32 gaudi2_edma_engine_id_to_edma_id[] = {\n\t[GAUDI2_ENGINE_ID_PDMA_0] = DMA_CORE_ID_PDMA0,\n\t[GAUDI2_ENGINE_ID_PDMA_1] = DMA_CORE_ID_PDMA1,\n\t[GAUDI2_DCORE0_ENGINE_ID_EDMA_0] = DMA_CORE_ID_EDMA0,\n\t[GAUDI2_DCORE0_ENGINE_ID_EDMA_1] = DMA_CORE_ID_EDMA1,\n\t[GAUDI2_DCORE1_ENGINE_ID_EDMA_0] = DMA_CORE_ID_EDMA2,\n\t[GAUDI2_DCORE1_ENGINE_ID_EDMA_1] = DMA_CORE_ID_EDMA3,\n\t[GAUDI2_DCORE2_ENGINE_ID_EDMA_0] = DMA_CORE_ID_EDMA4,\n\t[GAUDI2_DCORE2_ENGINE_ID_EDMA_1] = DMA_CORE_ID_EDMA5,\n\t[GAUDI2_DCORE3_ENGINE_ID_EDMA_0] = DMA_CORE_ID_EDMA6,\n\t[GAUDI2_DCORE3_ENGINE_ID_EDMA_1] = DMA_CORE_ID_EDMA7,\n\t[GAUDI2_ENGINE_ID_KDMA] = DMA_CORE_ID_KDMA,\n};\n\nconst u32 edma_stream_base[NUM_OF_EDMA_PER_DCORE * NUM_OF_DCORES] = {\n\tGAUDI2_QUEUE_ID_DCORE0_EDMA_0_0,\n\tGAUDI2_QUEUE_ID_DCORE0_EDMA_1_0,\n\tGAUDI2_QUEUE_ID_DCORE1_EDMA_0_0,\n\tGAUDI2_QUEUE_ID_DCORE1_EDMA_1_0,\n\tGAUDI2_QUEUE_ID_DCORE2_EDMA_0_0,\n\tGAUDI2_QUEUE_ID_DCORE2_EDMA_1_0,\n\tGAUDI2_QUEUE_ID_DCORE3_EDMA_0_0,\n\tGAUDI2_QUEUE_ID_DCORE3_EDMA_1_0,\n};\n\nstatic const char gaudi2_vdec_irq_name[GAUDI2_VDEC_MSIX_ENTRIES][GAUDI2_MAX_STRING_LEN] = {\n\t\"gaudi2 vdec 0_0\", \"gaudi2 vdec 0_0 abnormal\",\n\t\"gaudi2 vdec 0_1\", \"gaudi2 vdec 0_1 abnormal\",\n\t\"gaudi2 vdec 1_0\", \"gaudi2 vdec 1_0 abnormal\",\n\t\"gaudi2 vdec 1_1\", \"gaudi2 vdec 1_1 abnormal\",\n\t\"gaudi2 vdec 2_0\", \"gaudi2 vdec 2_0 abnormal\",\n\t\"gaudi2 vdec 2_1\", \"gaudi2 vdec 2_1 abnormal\",\n\t\"gaudi2 vdec 3_0\", \"gaudi2 vdec 3_0 abnormal\",\n\t\"gaudi2 vdec 3_1\", \"gaudi2 vdec 3_1 abnormal\",\n\t\"gaudi2 vdec s_0\", \"gaudi2 vdec s_0 abnormal\",\n\t\"gaudi2 vdec s_1\", \"gaudi2 vdec s_1 abnormal\"\n};\n\nenum rtr_id {\n\tDCORE0_RTR0,\n\tDCORE0_RTR1,\n\tDCORE0_RTR2,\n\tDCORE0_RTR3,\n\tDCORE0_RTR4,\n\tDCORE0_RTR5,\n\tDCORE0_RTR6,\n\tDCORE0_RTR7,\n\tDCORE1_RTR0,\n\tDCORE1_RTR1,\n\tDCORE1_RTR2,\n\tDCORE1_RTR3,\n\tDCORE1_RTR4,\n\tDCORE1_RTR5,\n\tDCORE1_RTR6,\n\tDCORE1_RTR7,\n\tDCORE2_RTR0,\n\tDCORE2_RTR1,\n\tDCORE2_RTR2,\n\tDCORE2_RTR3,\n\tDCORE2_RTR4,\n\tDCORE2_RTR5,\n\tDCORE2_RTR6,\n\tDCORE2_RTR7,\n\tDCORE3_RTR0,\n\tDCORE3_RTR1,\n\tDCORE3_RTR2,\n\tDCORE3_RTR3,\n\tDCORE3_RTR4,\n\tDCORE3_RTR5,\n\tDCORE3_RTR6,\n\tDCORE3_RTR7,\n};\n\nstatic const u32 gaudi2_tpc_initiator_hbw_rtr_id[NUM_OF_TPC_PER_DCORE * NUM_OF_DCORES + 1] = {\n\tDCORE0_RTR1, DCORE0_RTR1, DCORE0_RTR2, DCORE0_RTR2, DCORE0_RTR3, DCORE0_RTR3,\n\tDCORE1_RTR6, DCORE1_RTR6, DCORE1_RTR5, DCORE1_RTR5, DCORE1_RTR4, DCORE1_RTR4,\n\tDCORE2_RTR3, DCORE2_RTR3, DCORE2_RTR2, DCORE2_RTR2, DCORE2_RTR1, DCORE2_RTR1,\n\tDCORE3_RTR4, DCORE3_RTR4, DCORE3_RTR5, DCORE3_RTR5, DCORE3_RTR6, DCORE3_RTR6,\n\tDCORE0_RTR0\n};\n\nstatic const u32 gaudi2_tpc_initiator_lbw_rtr_id[NUM_OF_TPC_PER_DCORE * NUM_OF_DCORES + 1] = {\n\tDCORE0_RTR1, DCORE0_RTR1, DCORE0_RTR1, DCORE0_RTR1, DCORE0_RTR2, DCORE0_RTR2,\n\tDCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR6, DCORE1_RTR6, DCORE1_RTR5, DCORE1_RTR5,\n\tDCORE2_RTR2, DCORE2_RTR2, DCORE2_RTR1, DCORE2_RTR1, DCORE2_RTR0, DCORE2_RTR0,\n\tDCORE3_RTR5, DCORE3_RTR5, DCORE3_RTR6, DCORE3_RTR6, DCORE3_RTR7, DCORE3_RTR7,\n\tDCORE0_RTR0\n};\n\nstatic const u32 gaudi2_dec_initiator_hbw_rtr_id[NUMBER_OF_DEC] = {\n\tDCORE0_RTR0, DCORE0_RTR0, DCORE1_RTR7, DCORE1_RTR7, DCORE2_RTR0, DCORE2_RTR0,\n\tDCORE3_RTR7, DCORE3_RTR7, DCORE0_RTR0, DCORE0_RTR0\n};\n\nstatic const u32 gaudi2_dec_initiator_lbw_rtr_id[NUMBER_OF_DEC] = {\n\tDCORE0_RTR1, DCORE0_RTR1, DCORE1_RTR6, DCORE1_RTR6, DCORE2_RTR1, DCORE2_RTR1,\n\tDCORE3_RTR6, DCORE3_RTR6, DCORE0_RTR0, DCORE0_RTR0\n};\n\nstatic const u32 gaudi2_nic_initiator_hbw_rtr_id[NIC_NUMBER_OF_MACROS] = {\n\tDCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE2_RTR0,\n\tDCORE2_RTR0, DCORE2_RTR0, DCORE2_RTR0, DCORE3_RTR7, DCORE3_RTR7, DCORE3_RTR7\n};\n\nstatic const u32 gaudi2_nic_initiator_lbw_rtr_id[NIC_NUMBER_OF_MACROS] = {\n\tDCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE1_RTR7, DCORE2_RTR0,\n\tDCORE2_RTR0, DCORE2_RTR0, DCORE2_RTR0, DCORE3_RTR7, DCORE3_RTR7, DCORE3_RTR7\n};\n\nstatic const u32 gaudi2_edma_initiator_hbw_sft[NUM_OF_EDMA_PER_DCORE * NUM_OF_DCORES] = {\n\tmmSFT0_HBW_RTR_IF1_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT0_HBW_RTR_IF0_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT1_HBW_RTR_IF1_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT1_HBW_RTR_IF0_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT2_HBW_RTR_IF0_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT2_HBW_RTR_IF1_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT3_HBW_RTR_IF0_MSTR_IF_RR_SHRD_HBW_BASE,\n\tmmSFT3_HBW_RTR_IF1_MSTR_IF_RR_SHRD_HBW_BASE\n};\n\nstatic const u32 gaudi2_pdma_initiator_hbw_rtr_id[NUM_OF_PDMA] = {\n\tDCORE0_RTR0, DCORE0_RTR0\n};\n\nstatic const u32 gaudi2_pdma_initiator_lbw_rtr_id[NUM_OF_PDMA] = {\n\tDCORE0_RTR2, DCORE0_RTR2\n};\n\nstatic const u32 gaudi2_rot_initiator_hbw_rtr_id[NUM_OF_ROT] = {\n\tDCORE2_RTR0, DCORE3_RTR7\n};\n\nstatic const u32 gaudi2_rot_initiator_lbw_rtr_id[NUM_OF_ROT] = {\n\tDCORE2_RTR2, DCORE3_RTR5\n};\n\nstruct mme_initiators_rtr_id {\n\tu32 wap0;\n\tu32 wap1;\n\tu32 write;\n\tu32 read;\n\tu32 sbte0;\n\tu32 sbte1;\n\tu32 sbte2;\n\tu32 sbte3;\n\tu32 sbte4;\n};\n\nenum mme_initiators {\n\tMME_WAP0 = 0,\n\tMME_WAP1,\n\tMME_WRITE,\n\tMME_READ,\n\tMME_SBTE0,\n\tMME_SBTE1,\n\tMME_SBTE2,\n\tMME_SBTE3,\n\tMME_SBTE4,\n\tMME_INITIATORS_MAX\n};\n\nstatic const struct mme_initiators_rtr_id\ngaudi2_mme_initiator_rtr_id[NUM_OF_MME_PER_DCORE * NUM_OF_DCORES] = {\n\t{ .wap0 = 5, .wap1 = 7, .write = 6, .read = 7,\n\t.sbte0 = 7, .sbte1 = 4, .sbte2 = 4, .sbte3 = 5, .sbte4 = 6},\n\t{ .wap0 = 10, .wap1 = 8, .write = 9, .read = 8,\n\t.sbte0 = 11, .sbte1 = 11, .sbte2 = 10, .sbte3 = 9, .sbte4 = 8},\n\t{ .wap0 = 21, .wap1 = 23, .write = 22, .read = 23,\n\t.sbte0 = 20, .sbte1 = 20, .sbte2 = 21, .sbte3 = 22, .sbte4 = 23},\n\t{ .wap0 = 30, .wap1 = 28, .write = 29, .read = 30,\n\t.sbte0 = 31, .sbte1 = 31, .sbte2 = 30, .sbte3 = 29, .sbte4 = 28},\n};\n\nenum razwi_event_sources {\n\tRAZWI_TPC,\n\tRAZWI_MME,\n\tRAZWI_EDMA,\n\tRAZWI_PDMA,\n\tRAZWI_NIC,\n\tRAZWI_DEC,\n\tRAZWI_ROT\n};\n\nstruct hbm_mc_error_causes {\n\tu32 mask;\n\tchar cause[50];\n};\n\nstatic struct hl_special_block_info gaudi2_special_blocks[] = GAUDI2_SPECIAL_BLOCKS;\n\n \nstatic int gaudi2_iterator_skip_block_types[] = {\n\t\tGAUDI2_BLOCK_TYPE_PLL,\n\t\tGAUDI2_BLOCK_TYPE_EU_BIST,\n\t\tGAUDI2_BLOCK_TYPE_HBM,\n\t\tGAUDI2_BLOCK_TYPE_XFT\n};\n\nstatic struct range gaudi2_iterator_skip_block_ranges[] = {\n\t\t \n\t\t{mmPSOC_I2C_M0_BASE, mmPSOC_EFUSE_BASE},\n\t\t{mmPSOC_BTL_BASE, mmPSOC_MSTR_IF_RR_SHRD_HBW_BASE},\n\t\t \n\t\t{mmCPU_CA53_CFG_BASE, mmCPU_CA53_CFG_BASE},\n\t\t{mmCPU_TIMESTAMP_BASE, mmCPU_MSTR_IF_RR_SHRD_HBW_BASE}\n};\n\nstatic struct hbm_mc_error_causes hbm_mc_spi[GAUDI2_NUM_OF_HBM_MC_SPI_CAUSE] = {\n\t{HBM_MC_SPI_TEMP_PIN_CHG_MASK, \"temperature pins changed\"},\n\t{HBM_MC_SPI_THR_ENG_MASK, \"temperature-based throttling engaged\"},\n\t{HBM_MC_SPI_THR_DIS_ENG_MASK, \"temperature-based throttling disengaged\"},\n\t{HBM_MC_SPI_IEEE1500_COMP_MASK, \"IEEE1500 op comp\"},\n\t{HBM_MC_SPI_IEEE1500_PAUSED_MASK, \"IEEE1500 op paused\"},\n};\n\nstatic const char * const hbm_mc_sei_cause[GAUDI2_NUM_OF_HBM_SEI_CAUSE] = {\n\t[HBM_SEI_CMD_PARITY_EVEN] = \"SEI C/A parity even\",\n\t[HBM_SEI_CMD_PARITY_ODD] = \"SEI C/A parity odd\",\n\t[HBM_SEI_READ_ERR] = \"SEI read data error\",\n\t[HBM_SEI_WRITE_DATA_PARITY_ERR] = \"SEI write data parity error\",\n\t[HBM_SEI_CATTRIP] = \"SEI CATTRIP asserted\",\n\t[HBM_SEI_MEM_BIST_FAIL] = \"SEI memory BIST fail\",\n\t[HBM_SEI_DFI] = \"SEI DFI error\",\n\t[HBM_SEI_INV_TEMP_READ_OUT] = \"SEI invalid temp read\",\n\t[HBM_SEI_BIST_FAIL] = \"SEI BIST fail\"\n};\n\nstruct mmu_spi_sei_cause {\n\tchar cause[50];\n\tint clear_bit;\n};\n\nstatic const struct mmu_spi_sei_cause gaudi2_mmu_spi_sei[GAUDI2_NUM_OF_MMU_SPI_SEI_CAUSE] = {\n\t{\"page fault\", 1},\t\t \n\t{\"page access\", 1},\t\t \n\t{\"bypass ddr\", 2},\t\t \n\t{\"multi hit\", 2},\t\t \n\t{\"mmu rei0\", -1},\t\t \n\t{\"mmu rei1\", -1},\t\t \n\t{\"stlb rei0\", -1},\t\t \n\t{\"stlb rei1\", -1},\t\t \n\t{\"rr privileged write hit\", 2},\t \n\t{\"rr privileged read hit\", 2},\t \n\t{\"rr secure write hit\", 2},\t \n\t{\"rr secure read hit\", 2},\t \n\t{\"bist_fail no use\", 2},\t \n\t{\"bist_fail no use\", 2},\t \n\t{\"bist_fail no use\", 2},\t \n\t{\"bist_fail no use\", 2},\t \n\t{\"slave error\", 16},\t\t \n\t{\"dec error\", 17},\t\t \n\t{\"burst fifo full\", 2}\t\t \n};\n\nstruct gaudi2_cache_invld_params {\n\tu64 start_va;\n\tu64 end_va;\n\tu32 inv_start_val;\n\tu32 flags;\n\tbool range_invalidation;\n};\n\nstruct gaudi2_tpc_idle_data {\n\tstruct engines_data *e;\n\tunsigned long *mask;\n\tbool *is_idle;\n\tconst char *tpc_fmt;\n};\n\nstruct gaudi2_tpc_mmu_data {\n\tu32 rw_asid;\n};\n\nstatic s64 gaudi2_state_dump_specs_props[SP_MAX] = {0};\n\nstatic int gaudi2_memset_device_memory(struct hl_device *hdev, u64 addr, u64 size, u64 val);\nstatic bool gaudi2_is_queue_enabled(struct hl_device *hdev, u32 hw_queue_id);\nstatic bool gaudi2_is_arc_enabled(struct hl_device *hdev, u64 arc_id);\nstatic void gaudi2_clr_arc_id_cap(struct hl_device *hdev, u64 arc_id);\nstatic void gaudi2_set_arc_id_cap(struct hl_device *hdev, u64 arc_id);\nstatic void gaudi2_memset_device_lbw(struct hl_device *hdev, u32 addr, u32 size, u32 val);\nstatic int gaudi2_send_job_to_kdma(struct hl_device *hdev, u64 src_addr, u64 dst_addr, u32 size,\n\t\t\t\t\t\t\t\t\t\tbool is_memset);\nstatic bool gaudi2_get_tpc_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e);\nstatic bool gaudi2_get_mme_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e);\nstatic bool gaudi2_get_edma_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e);\nstatic u64 gaudi2_mmu_scramble_addr(struct hl_device *hdev, u64 raw_addr);\nstatic u64 gaudi2_mmu_descramble_addr(struct hl_device *hdev, u64 scrambled_addr);\n\nstatic void gaudi2_init_scrambler_hbm(struct hl_device *hdev)\n{\n\n}\n\nstatic u32 gaudi2_get_signal_cb_size(struct hl_device *hdev)\n{\n\treturn sizeof(struct packet_msg_short);\n}\n\nstatic u32 gaudi2_get_wait_cb_size(struct hl_device *hdev)\n{\n\treturn sizeof(struct packet_msg_short) * 4 + sizeof(struct packet_fence);\n}\n\nvoid gaudi2_iterate_tpcs(struct hl_device *hdev, struct iterate_module_ctx *ctx)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint dcore, inst, tpc_seq;\n\tu32 offset;\n\n\t \n\tctx->rc = 0;\n\n\tfor (dcore = 0; dcore < NUM_OF_DCORES; dcore++) {\n\t\tfor (inst = 0; inst < NUM_OF_TPC_PER_DCORE; inst++) {\n\t\t\ttpc_seq = dcore * NUM_OF_TPC_PER_DCORE + inst;\n\n\t\t\tif (!(prop->tpc_enabled_mask & BIT(tpc_seq)))\n\t\t\t\tcontinue;\n\n\t\t\toffset = (DCORE_OFFSET * dcore) + (DCORE_TPC_OFFSET * inst);\n\n\t\t\tctx->fn(hdev, dcore, inst, offset, ctx);\n\t\t\tif (ctx->rc) {\n\t\t\t\tdev_err(hdev->dev, \"TPC iterator failed for DCORE%d TPC%d\\n\",\n\t\t\t\t\t\t\tdcore, inst);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!(prop->tpc_enabled_mask & BIT(TPC_ID_DCORE0_TPC6)))\n\t\treturn;\n\n\t \n\toffset = DCORE_TPC_OFFSET * (NUM_DCORE0_TPC - 1);\n\tctx->fn(hdev, 0, NUM_DCORE0_TPC - 1, offset, ctx);\n\tif (ctx->rc)\n\t\tdev_err(hdev->dev, \"TPC iterator failed for DCORE0 TPC6\\n\");\n}\n\nstatic bool gaudi2_host_phys_addr_valid(u64 addr)\n{\n\tif ((addr < HOST_PHYS_BASE_0 + HOST_PHYS_SIZE_0) || (addr >= HOST_PHYS_BASE_1))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int set_number_of_functional_hbms(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu8 faulty_hbms = hweight64(hdev->dram_binning);\n\n\t \n\tif (!faulty_hbms) {\n\t\tdev_dbg(hdev->dev, \"All HBM are in use (no binning)\\n\");\n\t\tprop->num_functional_hbms = GAUDI2_HBM_NUM;\n\t\treturn 0;\n\t}\n\n\t \n\tif (faulty_hbms > MAX_FAULTY_HBMS) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"HBM binning supports max of %d faulty HBMs, supplied mask 0x%llx.\\n\",\n\t\t\tMAX_FAULTY_HBMS, hdev->dram_binning);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tprop->num_functional_hbms = GAUDI2_HBM_NUM - faulty_hbms;\n\treturn 0;\n}\n\nstatic int gaudi2_set_dram_properties(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 basic_hbm_page_size;\n\tint rc;\n\n\trc = set_number_of_functional_hbms(hdev);\n\tif (rc)\n\t\treturn -EINVAL;\n\n\t \n\tbasic_hbm_page_size = prop->num_functional_hbms * SZ_8M;\n\tprop->dram_page_size = GAUDI2_COMPENSATE_TLB_PAGE_SIZE_FACTOR * basic_hbm_page_size;\n\tprop->device_mem_alloc_default_page_size = prop->dram_page_size;\n\tprop->dram_size = prop->num_functional_hbms * SZ_16G;\n\tprop->dram_base_address = DRAM_PHYS_BASE;\n\tprop->dram_end_address = prop->dram_base_address + prop->dram_size;\n\tprop->dram_supports_virtual_memory = true;\n\n\tprop->dram_user_base_address = DRAM_PHYS_BASE + prop->dram_page_size;\n\tprop->dram_hints_align_mask = ~GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK;\n\tprop->hints_dram_reserved_va_range.start_addr = RESERVED_VA_RANGE_FOR_ARC_ON_HBM_START;\n\tprop->hints_dram_reserved_va_range.end_addr = RESERVED_VA_RANGE_FOR_ARC_ON_HBM_END;\n\n\t \n\tprop->dmmu.start_addr = prop->dram_base_address +\n\t\t\t(prop->dram_page_size *\n\t\t\t\tDIV_ROUND_UP_SECTOR_T(prop->dram_size, prop->dram_page_size));\n\n\tprop->dmmu.end_addr = prop->dmmu.start_addr + prop->dram_page_size *\n\t\t\tdiv_u64((VA_HBM_SPACE_END - prop->dmmu.start_addr), prop->dmmu.page_size);\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_fixed_properties(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hw_queue_properties *q_props;\n\tu32 num_sync_stream_queues = 0;\n\tint i;\n\n\tprop->max_queues = GAUDI2_QUEUE_ID_SIZE;\n\tprop->hw_queues_props = kcalloc(prop->max_queues, sizeof(struct hw_queue_properties),\n\t\t\t\t\tGFP_KERNEL);\n\n\tif (!prop->hw_queues_props)\n\t\treturn -ENOMEM;\n\n\tq_props = prop->hw_queues_props;\n\n\tfor (i = 0 ; i < GAUDI2_QUEUE_ID_CPU_PQ ; i++) {\n\t\tq_props[i].type = QUEUE_TYPE_HW;\n\t\tq_props[i].driver_only = 0;\n\n\t\tif (i >= GAUDI2_QUEUE_ID_NIC_0_0 && i <= GAUDI2_QUEUE_ID_NIC_23_3) {\n\t\t\tq_props[i].supports_sync_stream = 0;\n\t\t} else {\n\t\t\tq_props[i].supports_sync_stream = 1;\n\t\t\tnum_sync_stream_queues++;\n\t\t}\n\n\t\tq_props[i].cb_alloc_flags = CB_ALLOC_USER;\n\t}\n\n\tq_props[GAUDI2_QUEUE_ID_CPU_PQ].type = QUEUE_TYPE_CPU;\n\tq_props[GAUDI2_QUEUE_ID_CPU_PQ].driver_only = 1;\n\tq_props[GAUDI2_QUEUE_ID_CPU_PQ].cb_alloc_flags = CB_ALLOC_KERNEL;\n\n\tprop->cache_line_size = DEVICE_CACHE_LINE_SIZE;\n\tprop->cfg_base_address = CFG_BASE;\n\tprop->device_dma_offset_for_host_access = HOST_PHYS_BASE_0;\n\tprop->host_base_address = HOST_PHYS_BASE_0;\n\tprop->host_end_address = prop->host_base_address + HOST_PHYS_SIZE_0;\n\tprop->max_pending_cs = GAUDI2_MAX_PENDING_CS;\n\tprop->completion_queues_count = GAUDI2_RESERVED_CQ_NUMBER;\n\tprop->user_dec_intr_count = NUMBER_OF_DEC;\n\tprop->user_interrupt_count = GAUDI2_IRQ_NUM_USER_LAST - GAUDI2_IRQ_NUM_USER_FIRST + 1;\n\tprop->completion_mode = HL_COMPLETION_MODE_CS;\n\tprop->sync_stream_first_sob = GAUDI2_RESERVED_SOB_NUMBER;\n\tprop->sync_stream_first_mon = GAUDI2_RESERVED_MON_NUMBER;\n\n\tprop->sram_base_address = SRAM_BASE_ADDR;\n\tprop->sram_size = SRAM_SIZE;\n\tprop->sram_end_address = prop->sram_base_address + prop->sram_size;\n\tprop->sram_user_base_address = prop->sram_base_address + SRAM_USER_BASE_OFFSET;\n\n\tprop->hints_range_reservation = true;\n\n\tprop->rotator_enabled_mask = BIT(NUM_OF_ROT) - 1;\n\n\tif (hdev->pldm)\n\t\tprop->mmu_pgt_size = 0x800000;  \n\telse\n\t\tprop->mmu_pgt_size = MMU_PAGE_TABLES_INITIAL_SIZE;\n\n\tprop->mmu_pte_size = HL_PTE_SIZE;\n\tprop->mmu_hop_table_size = HOP_TABLE_SIZE_512_PTE;\n\tprop->mmu_hop0_tables_total_size = HOP0_512_PTE_TABLES_TOTAL_SIZE;\n\n\tprop->dmmu.hop_shifts[MMU_HOP0] = DHOP0_SHIFT;\n\tprop->dmmu.hop_shifts[MMU_HOP1] = DHOP1_SHIFT;\n\tprop->dmmu.hop_shifts[MMU_HOP2] = DHOP2_SHIFT;\n\tprop->dmmu.hop_shifts[MMU_HOP3] = DHOP3_SHIFT;\n\tprop->dmmu.hop_shifts[MMU_HOP4] = DHOP4_SHIFT;\n\tprop->dmmu.hop_masks[MMU_HOP0] = DHOP0_MASK;\n\tprop->dmmu.hop_masks[MMU_HOP1] = DHOP1_MASK;\n\tprop->dmmu.hop_masks[MMU_HOP2] = DHOP2_MASK;\n\tprop->dmmu.hop_masks[MMU_HOP3] = DHOP3_MASK;\n\tprop->dmmu.hop_masks[MMU_HOP4] = DHOP4_MASK;\n\tprop->dmmu.page_size = PAGE_SIZE_1GB;\n\tprop->dmmu.num_hops = MMU_ARCH_6_HOPS;\n\tprop->dmmu.last_mask = LAST_MASK;\n\tprop->dmmu.host_resident = 1;\n\tprop->dmmu.hop_table_size = prop->mmu_hop_table_size;\n\tprop->dmmu.hop0_tables_total_size = prop->mmu_hop0_tables_total_size;\n\n\t \n\tprop->dram_size = (GAUDI2_HBM_NUM - 1) * SZ_16G;\n\n\thdev->pmmu_huge_range = true;\n\tprop->pmmu.host_resident = 1;\n\tprop->pmmu.num_hops = MMU_ARCH_6_HOPS;\n\tprop->pmmu.last_mask = LAST_MASK;\n\tprop->pmmu.hop_table_size = prop->mmu_hop_table_size;\n\tprop->pmmu.hop0_tables_total_size = prop->mmu_hop0_tables_total_size;\n\n\tprop->hints_host_reserved_va_range.start_addr = RESERVED_VA_FOR_VIRTUAL_MSIX_DOORBELL_START;\n\tprop->hints_host_reserved_va_range.end_addr = RESERVED_VA_RANGE_FOR_ARC_ON_HOST_END;\n\tprop->hints_host_hpage_reserved_va_range.start_addr =\n\t\t\tRESERVED_VA_RANGE_FOR_ARC_ON_HOST_HPAGE_START;\n\tprop->hints_host_hpage_reserved_va_range.end_addr =\n\t\t\tRESERVED_VA_RANGE_FOR_ARC_ON_HOST_HPAGE_END;\n\n\tif (PAGE_SIZE == SZ_64K) {\n\t\tprop->pmmu.hop_shifts[MMU_HOP0] = HOP0_SHIFT_64K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP1] = HOP1_SHIFT_64K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP2] = HOP2_SHIFT_64K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP3] = HOP3_SHIFT_64K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP4] = HOP4_SHIFT_64K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP5] = HOP5_SHIFT_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP0] = HOP0_MASK_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP1] = HOP1_MASK_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP2] = HOP2_MASK_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP3] = HOP3_MASK_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP4] = HOP4_MASK_64K;\n\t\tprop->pmmu.hop_masks[MMU_HOP5] = HOP5_MASK_64K;\n\t\tprop->pmmu.start_addr = VA_HOST_SPACE_PAGE_START;\n\t\tprop->pmmu.end_addr = VA_HOST_SPACE_PAGE_END;\n\t\tprop->pmmu.page_size = PAGE_SIZE_64KB;\n\n\t\t \n\t\tmemcpy(&prop->pmmu_huge, &prop->pmmu, sizeof(prop->pmmu));\n\t\tprop->pmmu_huge.page_size = PAGE_SIZE_16MB;\n\t\tprop->pmmu_huge.start_addr = VA_HOST_SPACE_HPAGE_START;\n\t\tprop->pmmu_huge.end_addr = VA_HOST_SPACE_HPAGE_END;\n\t} else {\n\t\tprop->pmmu.hop_shifts[MMU_HOP0] = HOP0_SHIFT_4K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP1] = HOP1_SHIFT_4K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP2] = HOP2_SHIFT_4K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP3] = HOP3_SHIFT_4K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP4] = HOP4_SHIFT_4K;\n\t\tprop->pmmu.hop_shifts[MMU_HOP5] = HOP5_SHIFT_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP0] = HOP0_MASK_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP1] = HOP1_MASK_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP2] = HOP2_MASK_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP3] = HOP3_MASK_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP4] = HOP4_MASK_4K;\n\t\tprop->pmmu.hop_masks[MMU_HOP5] = HOP5_MASK_4K;\n\t\tprop->pmmu.start_addr = VA_HOST_SPACE_PAGE_START;\n\t\tprop->pmmu.end_addr = VA_HOST_SPACE_PAGE_END;\n\t\tprop->pmmu.page_size = PAGE_SIZE_4KB;\n\n\t\t \n\t\tmemcpy(&prop->pmmu_huge, &prop->pmmu, sizeof(prop->pmmu));\n\t\tprop->pmmu_huge.page_size = PAGE_SIZE_2MB;\n\t\tprop->pmmu_huge.start_addr = VA_HOST_SPACE_HPAGE_START;\n\t\tprop->pmmu_huge.end_addr = VA_HOST_SPACE_HPAGE_END;\n\t}\n\n\tprop->max_num_of_engines = GAUDI2_ENGINE_ID_SIZE;\n\tprop->num_engine_cores = CPU_ID_MAX;\n\tprop->cfg_size = CFG_SIZE;\n\tprop->max_asid = MAX_ASID;\n\tprop->num_of_events = GAUDI2_EVENT_SIZE;\n\n\tprop->supports_engine_modes = true;\n\n\tprop->dc_power_default = DC_POWER_DEFAULT;\n\n\tprop->cb_pool_cb_cnt = GAUDI2_CB_POOL_CB_CNT;\n\tprop->cb_pool_cb_size = GAUDI2_CB_POOL_CB_SIZE;\n\tprop->pcie_dbi_base_address = CFG_BASE + mmPCIE_DBI_BASE;\n\tprop->pcie_aux_dbi_reg_addr = CFG_BASE + mmPCIE_AUX_DBI;\n\n\tstrncpy(prop->cpucp_info.card_name, GAUDI2_DEFAULT_CARD_NAME, CARD_NAME_MAX_LEN);\n\n\tprop->mme_master_slave_mode = 1;\n\n\tprop->first_available_user_sob[0] = GAUDI2_RESERVED_SOB_NUMBER +\n\t\t\t\t\t(num_sync_stream_queues * HL_RSVD_SOBS);\n\n\tprop->first_available_user_mon[0] = GAUDI2_RESERVED_MON_NUMBER +\n\t\t\t\t\t(num_sync_stream_queues * HL_RSVD_MONS);\n\n\tprop->first_available_user_interrupt = GAUDI2_IRQ_NUM_USER_FIRST;\n\tprop->tpc_interrupt_id = GAUDI2_IRQ_NUM_TPC_ASSERT;\n\tprop->eq_interrupt_id = GAUDI2_IRQ_NUM_EVENT_QUEUE;\n\n\tprop->first_available_cq[0] = GAUDI2_RESERVED_CQ_NUMBER;\n\n\tprop->fw_cpu_boot_dev_sts0_valid = false;\n\tprop->fw_cpu_boot_dev_sts1_valid = false;\n\tprop->hard_reset_done_by_fw = false;\n\tprop->gic_interrupts_enable = true;\n\n\tprop->server_type = HL_SERVER_TYPE_UNKNOWN;\n\n\tprop->max_dec = NUMBER_OF_DEC;\n\n\tprop->clk_pll_index = HL_GAUDI2_MME_PLL;\n\n\tprop->dma_mask = 64;\n\n\tprop->hbw_flush_reg = mmPCIE_WRAP_SPECIAL_GLBL_SPARE_0;\n\n\treturn 0;\n}\n\nstatic int gaudi2_pci_bars_map(struct hl_device *hdev)\n{\n\tstatic const char * const name[] = {\"CFG_SRAM\", \"MSIX\", \"DRAM\"};\n\tbool is_wc[3] = {false, false, true};\n\tint rc;\n\n\trc = hl_pci_bars_map(hdev, name, is_wc);\n\tif (rc)\n\t\treturn rc;\n\n\thdev->rmmio = hdev->pcie_bar[SRAM_CFG_BAR_ID] + (CFG_BASE - STM_FLASH_BASE_ADDR);\n\n\treturn 0;\n}\n\nstatic u64 gaudi2_set_hbm_bar_base(struct hl_device *hdev, u64 addr)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct hl_inbound_pci_region pci_region;\n\tu64 old_addr = addr;\n\tint rc;\n\n\tif ((gaudi2) && (gaudi2->dram_bar_cur_addr == addr))\n\t\treturn old_addr;\n\n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\treturn U64_MAX;\n\n\t \n\tpci_region.mode = PCI_BAR_MATCH_MODE;\n\tpci_region.bar = DRAM_BAR_ID;\n\tpci_region.addr = addr;\n\trc = hl_pci_set_inbound_region(hdev, 2, &pci_region);\n\tif (rc)\n\t\treturn U64_MAX;\n\n\tif (gaudi2) {\n\t\told_addr = gaudi2->dram_bar_cur_addr;\n\t\tgaudi2->dram_bar_cur_addr = addr;\n\t}\n\n\treturn old_addr;\n}\n\nstatic int gaudi2_init_iatu(struct hl_device *hdev)\n{\n\tstruct hl_inbound_pci_region inbound_region;\n\tstruct hl_outbound_pci_region outbound_region;\n\tu32 bar_addr_low, bar_addr_high;\n\tint rc;\n\n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\treturn 0;\n\n\t \n\tinbound_region.mode = PCI_BAR_MATCH_MODE;\n\tinbound_region.bar = SRAM_CFG_BAR_ID;\n\t \n\tinbound_region.addr = STM_FLASH_BASE_ADDR - STM_FLASH_ALIGNED_OFF;\n\trc = hl_pci_set_inbound_region(hdev, 0, &inbound_region);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tbar_addr_high = RREG32(mmPCIE_DBI_BAR1_REG + STM_FLASH_ALIGNED_OFF);\n\tbar_addr_low = RREG32(mmPCIE_DBI_BAR0_REG + STM_FLASH_ALIGNED_OFF) & ~0xF;\n\n\thdev->pcie_bar_phys[SRAM_CFG_BAR_ID] = (u64)bar_addr_high << 32 | bar_addr_low;\n\n\t \n\tinbound_region.mode = PCI_ADDRESS_MATCH_MODE;\n\tinbound_region.bar = SRAM_CFG_BAR_ID;\n\tinbound_region.offset_in_bar = 0;\n\tinbound_region.addr = STM_FLASH_BASE_ADDR;\n\tinbound_region.size = CFG_REGION_SIZE;\n\trc = hl_pci_set_inbound_region(hdev, 0, &inbound_region);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tinbound_region.mode = PCI_ADDRESS_MATCH_MODE;\n\tinbound_region.bar = SRAM_CFG_BAR_ID;\n\tinbound_region.offset_in_bar = CFG_REGION_SIZE;\n\tinbound_region.addr = BAR0_RSRVD_BASE_ADDR;\n\tinbound_region.size = BAR0_RSRVD_SIZE + SRAM_SIZE;\n\trc = hl_pci_set_inbound_region(hdev, 1, &inbound_region);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tinbound_region.mode = PCI_BAR_MATCH_MODE;\n\tinbound_region.bar = DRAM_BAR_ID;\n\tinbound_region.addr = DRAM_PHYS_BASE;\n\trc = hl_pci_set_inbound_region(hdev, 2, &inbound_region);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\toutbound_region.addr = HOST_PHYS_BASE_0;\n\toutbound_region.size = HOST_PHYS_SIZE_0;\n\trc = hl_pci_set_outbound_region(hdev, &outbound_region);\n\n\treturn rc;\n}\n\nstatic enum hl_device_hw_state gaudi2_get_hw_state(struct hl_device *hdev)\n{\n\treturn RREG32(mmHW_STATE);\n}\n\nstatic int gaudi2_tpc_binning_init_prop(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\t \n\tif (hweight64(hdev->tpc_binning) > MAX_CLUSTER_BINNING_FAULTY_TPCS) {\n\t\tdev_err(hdev->dev, \"TPC binning is supported for max of %d faulty TPCs, provided mask 0x%llx\\n\",\n\t\t\t\t\tMAX_CLUSTER_BINNING_FAULTY_TPCS,\n\t\t\t\t\thdev->tpc_binning);\n\t\treturn -EINVAL;\n\t}\n\n\tprop->tpc_binning_mask = hdev->tpc_binning;\n\tprop->tpc_enabled_mask = GAUDI2_TPC_FULL_MASK;\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_tpc_binning_masks(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hw_queue_properties *q_props = prop->hw_queues_props;\n\tu64 tpc_binning_mask;\n\tu8 subst_idx = 0;\n\tint i, rc;\n\n\trc = gaudi2_tpc_binning_init_prop(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\ttpc_binning_mask = prop->tpc_binning_mask;\n\n\tfor (i = 0 ; i < MAX_FAULTY_TPCS ; i++) {\n\t\tu8 subst_seq, binned, qid_base;\n\n\t\tif (tpc_binning_mask == 0)\n\t\t\tbreak;\n\n\t\tif (subst_idx == 0) {\n\t\t\tsubst_seq = TPC_ID_DCORE0_TPC6;\n\t\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_TPC_6_0;\n\t\t} else {\n\t\t\tsubst_seq = TPC_ID_DCORE3_TPC5;\n\t\t\tqid_base = GAUDI2_QUEUE_ID_DCORE3_TPC_5_0;\n\t\t}\n\n\n\t\t \n\t\tbinned = __ffs(tpc_binning_mask);\n\t\t \n\t\tif (binned >= TPC_ID_SIZE) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Invalid binned TPC (binning mask: %llx)\\n\",\n\t\t\t\ttpc_binning_mask);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tclear_bit(binned, (unsigned long *)&tpc_binning_mask);\n\n\t\t \n\t\tclear_bit(subst_seq, (unsigned long *)&prop->tpc_enabled_mask);\n\n\t\t \n\t\tq_props[qid_base].binned = 1;\n\t\tq_props[qid_base + 1].binned = 1;\n\t\tq_props[qid_base + 2].binned = 1;\n\t\tq_props[qid_base + 3].binned = 1;\n\n\t\tsubst_idx++;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_dec_binning_masks(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu8 num_faulty;\n\n\tnum_faulty = hweight32(hdev->decoder_binning);\n\n\t \n\tif (num_faulty > MAX_FAULTY_DECODERS) {\n\t\tdev_err(hdev->dev, \"decoder binning is supported for max of single faulty decoder, provided mask 0x%x\\n\",\n\t\t\t\t\t\thdev->decoder_binning);\n\t\treturn -EINVAL;\n\t}\n\n\tprop->decoder_binning_mask = (hdev->decoder_binning & GAUDI2_DECODER_FULL_MASK);\n\n\tif (prop->decoder_binning_mask)\n\t\tprop->decoder_enabled_mask = (GAUDI2_DECODER_FULL_MASK & ~BIT(DEC_ID_PCIE_VDEC1));\n\telse\n\t\tprop->decoder_enabled_mask = GAUDI2_DECODER_FULL_MASK;\n\n\treturn 0;\n}\n\nstatic void gaudi2_set_dram_binning_masks(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\t \n\tif (!hdev->dram_binning) {\n\t\tprop->dram_binning_mask = 0;\n\t\tprop->dram_enabled_mask = GAUDI2_DRAM_FULL_MASK;\n\t\treturn;\n\t}\n\n\t \n\tprop->faulty_dram_cluster_map |= hdev->dram_binning;\n\tprop->dram_binning_mask = hdev->dram_binning;\n\tprop->dram_enabled_mask = GAUDI2_DRAM_FULL_MASK & ~BIT(HBM_ID5);\n}\n\nstatic int gaudi2_set_edma_binning_masks(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hw_queue_properties *q_props;\n\tu8 seq, num_faulty;\n\n\tnum_faulty = hweight32(hdev->edma_binning);\n\n\t \n\tif (num_faulty > MAX_FAULTY_EDMAS) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"EDMA binning is supported for max of single faulty EDMA, provided mask 0x%x\\n\",\n\t\t\thdev->edma_binning);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!hdev->edma_binning) {\n\t\tprop->edma_binning_mask = 0;\n\t\tprop->edma_enabled_mask = GAUDI2_EDMA_FULL_MASK;\n\t\treturn 0;\n\t}\n\n\tseq = __ffs((unsigned long)hdev->edma_binning);\n\n\t \n\tprop->faulty_dram_cluster_map |= BIT(edma_to_hbm_cluster[seq]);\n\tprop->edma_binning_mask = hdev->edma_binning;\n\tprop->edma_enabled_mask = GAUDI2_EDMA_FULL_MASK & ~BIT(EDMA_ID_DCORE3_INSTANCE1);\n\n\t \n\tq_props = prop->hw_queues_props;\n\tq_props[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_0].binned = 1;\n\tq_props[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_1].binned = 1;\n\tq_props[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_2].binned = 1;\n\tq_props[GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3].binned = 1;\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_xbar_edge_enable_mask(struct hl_device *hdev, u32 xbar_edge_iso_mask)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu8 num_faulty, seq;\n\n\t \n\tif (!xbar_edge_iso_mask) {\n\t\tprop->xbar_edge_enabled_mask = GAUDI2_XBAR_EDGE_FULL_MASK;\n\t\treturn 0;\n\t}\n\n\t \n\tnum_faulty = hweight32(xbar_edge_iso_mask);\n\n\t \n\tif (num_faulty > MAX_FAULTY_XBARS) {\n\t\tdev_err(hdev->dev, \"we cannot have more than %d faulty XBAR EDGE\\n\",\n\t\t\t\t\t\t\t\t\tMAX_FAULTY_XBARS);\n\t\treturn -EINVAL;\n\t}\n\n\tseq = __ffs((unsigned long)xbar_edge_iso_mask);\n\n\t \n\tprop->faulty_dram_cluster_map |= BIT(xbar_edge_to_hbm_cluster[seq]);\n\tprop->xbar_edge_enabled_mask = (~xbar_edge_iso_mask) & GAUDI2_XBAR_EDGE_FULL_MASK;\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_cluster_binning_masks_common(struct hl_device *hdev, u8 xbar_edge_iso_mask)\n{\n\tint rc;\n\n\t \n\thdev->asic_prop.faulty_dram_cluster_map = 0;\n\n\tgaudi2_set_dram_binning_masks(hdev);\n\n\trc = gaudi2_set_edma_binning_masks(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi2_set_xbar_edge_enable_mask(hdev, xbar_edge_iso_mask);\n\tif (rc)\n\t\treturn rc;\n\n\n\t \n\thdev->asic_prop.hmmu_hif_enabled_mask = GAUDI2_HIF_HMMU_FULL_MASK;\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_cluster_binning_masks(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint rc;\n\n\trc = gaudi2_set_cluster_binning_masks_common(hdev, prop->cpucp_info.xbar_binning_mask);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (prop->faulty_dram_cluster_map) {\n\t\tu8 cluster_seq = __ffs((unsigned long)prop->faulty_dram_cluster_map);\n\n\t\tprop->hmmu_hif_enabled_mask = cluster_hmmu_hif_enabled_mask[cluster_seq];\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_binning_masks(struct hl_device *hdev)\n{\n\tint rc;\n\n\trc = gaudi2_set_cluster_binning_masks(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi2_set_tpc_binning_masks(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi2_set_dec_binning_masks(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic int gaudi2_cpucp_info_get(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tlong max_power;\n\tu64 dram_size;\n\tint rc;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\t \n\tif (hdev->reset_info.in_compute_reset)\n\t\treturn 0;\n\n\trc = hl_fw_cpucp_handshake(hdev, mmCPU_BOOT_DEV_STS0, mmCPU_BOOT_DEV_STS1, mmCPU_BOOT_ERR0,\n\t\t\t\t\t\t\t\t\t\tmmCPU_BOOT_ERR1);\n\tif (rc)\n\t\treturn rc;\n\n\tdram_size = le64_to_cpu(prop->cpucp_info.dram_size);\n\tif (dram_size) {\n\t\t \n\n\t\tif ((dram_size != ((GAUDI2_HBM_NUM - 1) * SZ_16G)) &&\n\t\t\t\t\t(dram_size != (GAUDI2_HBM_NUM * SZ_16G))) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"F/W reported invalid DRAM size %llu. Trying to use default size %llu\\n\",\n\t\t\t\tdram_size, prop->dram_size);\n\t\t\tdram_size = prop->dram_size;\n\t\t}\n\n\t\tprop->dram_size = dram_size;\n\t\tprop->dram_end_address = prop->dram_base_address + dram_size;\n\t}\n\n\tif (!strlen(prop->cpucp_info.card_name))\n\t\tstrncpy(prop->cpucp_info.card_name, GAUDI2_DEFAULT_CARD_NAME, CARD_NAME_MAX_LEN);\n\n\t \n\thdev->dram_binning = prop->cpucp_info.dram_binning_mask;\n\thdev->edma_binning = prop->cpucp_info.edma_binning_mask;\n\thdev->tpc_binning = le64_to_cpu(prop->cpucp_info.tpc_binning_mask);\n\thdev->decoder_binning = lower_32_bits(le64_to_cpu(prop->cpucp_info.decoder_binning_mask));\n\n\tdev_dbg(hdev->dev, \"Read binning masks: tpc: 0x%llx, dram: 0x%llx, edma: 0x%x, dec: 0x%x\\n\",\n\t\t\thdev->tpc_binning, hdev->dram_binning, hdev->edma_binning,\n\t\t\thdev->decoder_binning);\n\n\t \n\trc = hdev->asic_funcs->set_dram_properties(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = hdev->asic_funcs->set_binning_masks(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\tmax_power = hl_fw_get_max_power(hdev);\n\tif (max_power < 0)\n\t\treturn max_power;\n\n\tprop->max_power_default = (u64) max_power;\n\n\treturn 0;\n}\n\nstatic int gaudi2_fetch_psoc_frequency(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu16 pll_freq_arr[HL_PLL_NUM_OUTPUTS];\n\tint rc;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\trc = hl_fw_cpucp_pll_info_get(hdev, HL_GAUDI2_CPU_PLL, pll_freq_arr);\n\tif (rc)\n\t\treturn rc;\n\n\thdev->asic_prop.psoc_timestamp_frequency = pll_freq_arr[3];\n\n\treturn 0;\n}\n\nstatic int gaudi2_early_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct pci_dev *pdev = hdev->pdev;\n\tresource_size_t pci_bar_size;\n\tint rc;\n\n\trc = gaudi2_set_fixed_properties(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tpci_bar_size = pci_resource_len(pdev, SRAM_CFG_BAR_ID);\n\n\tif (pci_bar_size != CFG_BAR_SIZE) {\n\t\tdev_err(hdev->dev, \"Not \" HL_NAME \"? BAR %d size %pa, expecting %llu\\n\",\n\t\t\tSRAM_CFG_BAR_ID, &pci_bar_size, CFG_BAR_SIZE);\n\t\trc = -ENODEV;\n\t\tgoto free_queue_props;\n\t}\n\n\tpci_bar_size = pci_resource_len(pdev, MSIX_BAR_ID);\n\tif (pci_bar_size != MSIX_BAR_SIZE) {\n\t\tdev_err(hdev->dev, \"Not \" HL_NAME \"? BAR %d size %pa, expecting %llu\\n\",\n\t\t\tMSIX_BAR_ID, &pci_bar_size, MSIX_BAR_SIZE);\n\t\trc = -ENODEV;\n\t\tgoto free_queue_props;\n\t}\n\n\tprop->dram_pci_bar_size = pci_resource_len(pdev, DRAM_BAR_ID);\n\thdev->dram_pci_bar_start = pci_resource_start(pdev, DRAM_BAR_ID);\n\n\t \n\tif (hdev->pldm)\n\t\thdev->asic_prop.iatu_done_by_fw = false;\n\telse\n\t\thdev->asic_prop.iatu_done_by_fw = true;\n\n\trc = hl_pci_init(hdev);\n\tif (rc)\n\t\tgoto free_queue_props;\n\n\t \n\trc = hl_fw_read_preboot_status(hdev);\n\tif (rc) {\n\t\tif (hdev->reset_on_preboot_fail)\n\t\t\t \n\t\t\thdev->asic_funcs->hw_fini(hdev, true, false);\n\t\tgoto pci_fini;\n\t}\n\n\tif (gaudi2_get_hw_state(hdev) == HL_DEVICE_HW_STATE_DIRTY) {\n\t\tdev_dbg(hdev->dev, \"H/W state is dirty, must reset before initializing\\n\");\n\t\trc = hdev->asic_funcs->hw_fini(hdev, true, false);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"failed to reset HW in dirty state (%d)\\n\", rc);\n\t\t\tgoto pci_fini;\n\t\t}\n\t}\n\n\treturn 0;\n\npci_fini:\n\thl_pci_fini(hdev);\nfree_queue_props:\n\tkfree(hdev->asic_prop.hw_queues_props);\n\treturn rc;\n}\n\nstatic int gaudi2_early_fini(struct hl_device *hdev)\n{\n\tkfree(hdev->asic_prop.hw_queues_props);\n\thl_pci_fini(hdev);\n\n\treturn 0;\n}\n\nstatic bool gaudi2_is_arc_nic_owned(u64 arc_id)\n{\n\tswitch (arc_id) {\n\tcase CPU_ID_NIC_QMAN_ARC0...CPU_ID_NIC_QMAN_ARC23:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool gaudi2_is_arc_tpc_owned(u64 arc_id)\n{\n\tswitch (arc_id) {\n\tcase CPU_ID_TPC_QMAN_ARC0...CPU_ID_TPC_QMAN_ARC24:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void gaudi2_init_arcs(struct hl_device *hdev)\n{\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 arc_id;\n\tu32 i;\n\n\tfor (i = CPU_ID_SCHED_ARC0 ; i <= CPU_ID_SCHED_ARC3 ; i++) {\n\t\tif (gaudi2_is_arc_enabled(hdev, i))\n\t\t\tcontinue;\n\n\t\tgaudi2_set_arc_id_cap(hdev, i);\n\t}\n\n\tfor (i = GAUDI2_QUEUE_ID_PDMA_0_0 ; i < GAUDI2_QUEUE_ID_CPU_PQ ; i += 4) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, i))\n\t\t\tcontinue;\n\n\t\tarc_id = gaudi2_queue_id_to_arc_id[i];\n\t\tif (gaudi2_is_arc_enabled(hdev, arc_id))\n\t\t\tcontinue;\n\n\t\tif (gaudi2_is_arc_nic_owned(arc_id) &&\n\t\t\t\t!(hdev->nic_ports_mask & BIT_ULL(arc_id - CPU_ID_NIC_QMAN_ARC0)))\n\t\t\tcontinue;\n\n\t\tif (gaudi2_is_arc_tpc_owned(arc_id) && !(gaudi2->tpc_hw_cap_initialized &\n\t\t\t\t\t\t\tBIT_ULL(arc_id - CPU_ID_TPC_QMAN_ARC0)))\n\t\t\tcontinue;\n\n\t\tgaudi2_set_arc_id_cap(hdev, arc_id);\n\t}\n\n\t \n\thdev->asic_prop.engine_core_interrupt_reg_addr =\n\t\tCFG_BASE + le32_to_cpu(dyn_regs->eng_arc_irq_ctrl);\n}\n\nstatic int gaudi2_scrub_arc_dccm(struct hl_device *hdev, u32 cpu_id)\n{\n\tu32 reg_base, reg_val;\n\tint rc;\n\n\tswitch (cpu_id) {\n\tcase CPU_ID_SCHED_ARC0 ... CPU_ID_SCHED_ARC3:\n\t\t \n\t\trc = gaudi2_send_job_to_kdma(hdev, 0, CFG_BASE + gaudi2_arc_dccm_bases[cpu_id],\n\t\t\t\t\t\tARC_DCCM_BLOCK_SIZE * 2, true);\n\t\tif (rc)\n\t\t\treturn rc;\n\t\tbreak;\n\tcase CPU_ID_SCHED_ARC4:\n\tcase CPU_ID_SCHED_ARC5:\n\tcase CPU_ID_MME_QMAN_ARC0:\n\tcase CPU_ID_MME_QMAN_ARC1:\n\t\treg_base = gaudi2_arc_blocks_bases[cpu_id];\n\n\t\t \n\t\trc = gaudi2_send_job_to_kdma(hdev, 0, CFG_BASE + gaudi2_arc_dccm_bases[cpu_id],\n\t\t\t\t\t\tARC_DCCM_BLOCK_SIZE, true);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\treg_val = FIELD_PREP(ARC_FARM_ARC0_AUX_MME_ARC_UPPER_DCCM_EN_VAL_MASK, 1);\n\t\tWREG32(reg_base + ARC_DCCM_UPPER_EN_OFFSET, reg_val);\n\n\t\t \n\t\trc = gaudi2_send_job_to_kdma(hdev, 0, CFG_BASE + gaudi2_arc_dccm_bases[cpu_id],\n\t\t\t\t\t\tARC_DCCM_BLOCK_SIZE, true);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\treg_val = FIELD_PREP(ARC_FARM_ARC0_AUX_MME_ARC_UPPER_DCCM_EN_VAL_MASK, 0);\n\t\tWREG32(reg_base + ARC_DCCM_UPPER_EN_OFFSET, reg_val);\n\t\tbreak;\n\tdefault:\n\t\trc = gaudi2_send_job_to_kdma(hdev, 0, CFG_BASE + gaudi2_arc_dccm_bases[cpu_id],\n\t\t\t\t\t\tARC_DCCM_BLOCK_SIZE, true);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_scrub_arcs_dccm(struct hl_device *hdev)\n{\n\tu16 arc_id;\n\tint rc;\n\n\tfor (arc_id = CPU_ID_SCHED_ARC0 ; arc_id < CPU_ID_MAX ; arc_id++) {\n\t\tif (!gaudi2_is_arc_enabled(hdev, arc_id))\n\t\t\tcontinue;\n\n\t\trc = gaudi2_scrub_arc_dccm(hdev, arc_id);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_late_init(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint rc;\n\n\thdev->asic_prop.supports_advanced_cpucp_rc = true;\n\n\trc = hl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_ENABLE_PCI_ACCESS,\n\t\t\t\t\tgaudi2->virt_msix_db_dma_addr);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to enable PCI access from CPU\\n\");\n\t\treturn rc;\n\t}\n\n\trc = gaudi2_fetch_psoc_frequency(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to fetch psoc frequency\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\tgaudi2_init_arcs(hdev);\n\n\trc = gaudi2_scrub_arcs_dccm(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to scrub arcs DCCM\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\tgaudi2_init_security(hdev);\n\n\treturn 0;\n\ndisable_pci_access:\n\thl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_DISABLE_PCI_ACCESS, 0x0);\n\n\treturn rc;\n}\n\nstatic void gaudi2_late_fini(struct hl_device *hdev)\n{\n\thl_hwmon_release_resources(hdev);\n}\n\nstatic void gaudi2_user_mapped_dec_init(struct gaudi2_device *gaudi2, u32 start_idx)\n{\n\tstruct user_mapped_block *blocks = gaudi2->mapped_blocks;\n\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE0_DEC0_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE0_DEC1_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE1_DEC0_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE1_DEC1_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE2_DEC0_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE2_DEC1_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE3_DEC0_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmDCORE3_DEC1_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx++], mmPCIE_DEC0_CMD_BASE, HL_BLOCK_SIZE);\n\tHL_USR_MAPPED_BLK_INIT(&blocks[start_idx], mmPCIE_DEC1_CMD_BASE, HL_BLOCK_SIZE);\n}\n\nstatic void gaudi2_user_mapped_blocks_init(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct user_mapped_block *blocks = gaudi2->mapped_blocks;\n\tu32 block_size, umr_start_idx, num_umr_blocks;\n\tint i;\n\n\tfor (i = 0 ; i < NUM_ARC_CPUS ; i++) {\n\t\tif (i >= CPU_ID_SCHED_ARC0 && i <= CPU_ID_SCHED_ARC3)\n\t\t\tblock_size = ARC_DCCM_BLOCK_SIZE * 2;\n\t\telse\n\t\t\tblock_size = ARC_DCCM_BLOCK_SIZE;\n\n\t\tblocks[i].address = gaudi2_arc_dccm_bases[i];\n\t\tblocks[i].size = block_size;\n\t}\n\n\tblocks[NUM_ARC_CPUS].address = mmARC_FARM_ARC0_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 1].address = mmARC_FARM_ARC1_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 1].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 2].address = mmARC_FARM_ARC2_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 2].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 3].address = mmARC_FARM_ARC3_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 3].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 4].address = mmDCORE0_MME_QM_ARC_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 4].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 5].address = mmDCORE1_MME_QM_ARC_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 5].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 6].address = mmDCORE2_MME_QM_ARC_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 6].size = HL_BLOCK_SIZE;\n\n\tblocks[NUM_ARC_CPUS + 7].address = mmDCORE3_MME_QM_ARC_ACP_ENG_BASE;\n\tblocks[NUM_ARC_CPUS + 7].size = HL_BLOCK_SIZE;\n\n\tumr_start_idx = NUM_ARC_CPUS + NUM_OF_USER_ACP_BLOCKS;\n\tnum_umr_blocks = NIC_NUMBER_OF_ENGINES * NUM_OF_USER_NIC_UMR_BLOCKS;\n\tfor (i = 0 ; i < num_umr_blocks ; i++) {\n\t\tu8 nic_id, umr_block_id;\n\n\t\tnic_id = i / NUM_OF_USER_NIC_UMR_BLOCKS;\n\t\tumr_block_id = i % NUM_OF_USER_NIC_UMR_BLOCKS;\n\n\t\tblocks[umr_start_idx + i].address =\n\t\t\tmmNIC0_UMR0_0_UNSECURE_DOORBELL0_BASE +\n\t\t\t(nic_id / NIC_NUMBER_OF_QM_PER_MACRO) * NIC_OFFSET +\n\t\t\t(nic_id % NIC_NUMBER_OF_QM_PER_MACRO) * NIC_QM_OFFSET +\n\t\t\tumr_block_id * NIC_UMR_OFFSET;\n\t\tblocks[umr_start_idx + i].size = HL_BLOCK_SIZE;\n\t}\n\n\t \n\tgaudi2_user_mapped_dec_init(gaudi2, USR_MAPPED_BLK_DEC_START_IDX);\n\n\tfor (i = 1; i < NUM_OF_DCORES; ++i) {\n\t\tblocks[USR_MAPPED_BLK_SM_START_IDX + 2 * (i - 1)].size = SM_OBJS_BLOCK_SIZE;\n\t\tblocks[USR_MAPPED_BLK_SM_START_IDX + 2 * (i - 1) + 1].size = HL_BLOCK_SIZE;\n\n\t\tblocks[USR_MAPPED_BLK_SM_START_IDX + 2 * (i - 1)].address =\n\t\t\t\t\t\tmmDCORE0_SYNC_MNGR_OBJS_BASE + i * DCORE_OFFSET;\n\n\t\tblocks[USR_MAPPED_BLK_SM_START_IDX + 2 * (i - 1) + 1].address =\n\t\t\t\t\t\tmmDCORE0_SYNC_MNGR_GLBL_BASE + i * DCORE_OFFSET;\n\t}\n}\n\nstatic int gaudi2_alloc_cpu_accessible_dma_mem(struct hl_device *hdev)\n{\n\tdma_addr_t dma_addr_arr[GAUDI2_ALLOC_CPU_MEM_RETRY_CNT] = {}, end_addr;\n\tvoid *virt_addr_arr[GAUDI2_ALLOC_CPU_MEM_RETRY_CNT] = {};\n\tint i, j, rc = 0;\n\n\t \n\n\tfor (i = 0 ; i < GAUDI2_ALLOC_CPU_MEM_RETRY_CNT ; i++) {\n\t\tvirt_addr_arr[i] = hl_asic_dma_alloc_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE,\n\t\t\t\t\t\t\t&dma_addr_arr[i], GFP_KERNEL | __GFP_ZERO);\n\t\tif (!virt_addr_arr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_dma_mem_arr;\n\t\t}\n\n\t\tend_addr = dma_addr_arr[i] + HL_CPU_ACCESSIBLE_MEM_SIZE - 1;\n\t\tif (GAUDI2_ARC_PCI_MSB_ADDR(dma_addr_arr[i]) == GAUDI2_ARC_PCI_MSB_ADDR(end_addr))\n\t\t\tbreak;\n\t}\n\n\tif (i == GAUDI2_ALLOC_CPU_MEM_RETRY_CNT) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"MSB of ARC accessible DMA memory are not identical in all range\\n\");\n\t\trc = -EFAULT;\n\t\tgoto free_dma_mem_arr;\n\t}\n\n\thdev->cpu_accessible_dma_mem = virt_addr_arr[i];\n\thdev->cpu_accessible_dma_address = dma_addr_arr[i];\n\nfree_dma_mem_arr:\n\tfor (j = 0 ; j < i ; j++)\n\t\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, virt_addr_arr[j],\n\t\t\t\t\t\tdma_addr_arr[j]);\n\n\treturn rc;\n}\n\nstatic void gaudi2_set_pci_memory_regions(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct pci_mem_region *region;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_CFG];\n\tregion->region_base = CFG_BASE;\n\tregion->region_size = CFG_SIZE;\n\tregion->offset_in_bar = CFG_BASE - STM_FLASH_BASE_ADDR;\n\tregion->bar_size = CFG_BAR_SIZE;\n\tregion->bar_id = SRAM_CFG_BAR_ID;\n\tregion->used = 1;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_SRAM];\n\tregion->region_base = SRAM_BASE_ADDR;\n\tregion->region_size = SRAM_SIZE;\n\tregion->offset_in_bar = CFG_REGION_SIZE + BAR0_RSRVD_SIZE;\n\tregion->bar_size = CFG_BAR_SIZE;\n\tregion->bar_id = SRAM_CFG_BAR_ID;\n\tregion->used = 1;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_DRAM];\n\tregion->region_base = DRAM_PHYS_BASE;\n\tregion->region_size = hdev->asic_prop.dram_size;\n\tregion->offset_in_bar = 0;\n\tregion->bar_size = prop->dram_pci_bar_size;\n\tregion->bar_id = DRAM_BAR_ID;\n\tregion->used = 1;\n}\n\nstatic void gaudi2_user_interrupt_setup(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint i, j, k;\n\n\t \n\tHL_USR_INTR_STRUCT_INIT(hdev->tpc_interrupt, hdev, 0, HL_USR_INTERRUPT_TPC);\n\n\t \n\tHL_USR_INTR_STRUCT_INIT(hdev->unexpected_error_interrupt, hdev, 0,\n\t\t\t\t\t\tHL_USR_INTERRUPT_UNEXPECTED);\n\n\t \n\tHL_USR_INTR_STRUCT_INIT(hdev->common_user_cq_interrupt, hdev,\n\t\t\t\tHL_COMMON_USER_CQ_INTERRUPT_ID, HL_USR_INTERRUPT_CQ);\n\n\t \n\tHL_USR_INTR_STRUCT_INIT(hdev->common_decoder_interrupt, hdev,\n\t\t\t\tHL_COMMON_DEC_INTERRUPT_ID, HL_USR_INTERRUPT_DECODER);\n\n\t \n\n\t \n\tfor (i = GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM, j = 0 ; i <= GAUDI2_IRQ_NUM_SHARED_DEC1_NRM;\n\t\t\t\t\t\t\t\t\t\ti += 2, j++)\n\t\tHL_USR_INTR_STRUCT_INIT(hdev->user_interrupt[j], hdev, i,\n\t\t\t\t\t\tHL_USR_INTERRUPT_DECODER);\n\n\tfor (i = GAUDI2_IRQ_NUM_USER_FIRST, k = 0 ; k < prop->user_interrupt_count; i++, j++, k++)\n\t\tHL_USR_INTR_STRUCT_INIT(hdev->user_interrupt[j], hdev, i, HL_USR_INTERRUPT_CQ);\n}\n\nstatic inline int gaudi2_get_non_zero_random_int(void)\n{\n\tint rand = get_random_u32();\n\n\treturn rand ? rand : 1;\n}\n\nstatic void gaudi2_special_blocks_free(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_skip_blocks_cfg *skip_special_blocks_cfg =\n\t\t\t&prop->skip_special_blocks_cfg;\n\n\tkfree(prop->special_blocks);\n\tkfree(skip_special_blocks_cfg->block_types);\n\tkfree(skip_special_blocks_cfg->block_ranges);\n}\n\nstatic void gaudi2_special_blocks_iterator_free(struct hl_device *hdev)\n{\n\tgaudi2_special_blocks_free(hdev);\n}\n\nstatic bool gaudi2_special_block_skip(struct hl_device *hdev,\n\t\tstruct hl_special_blocks_cfg *special_blocks_cfg,\n\t\tu32 blk_idx, u32 major, u32 minor, u32 sub_minor)\n{\n\treturn false;\n}\n\nstatic int gaudi2_special_blocks_config(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint i, rc;\n\n\t \n\tprop->glbl_err_cause_num = GAUDI2_NUM_OF_GLBL_ERR_CAUSE;\n\tprop->num_of_special_blocks = ARRAY_SIZE(gaudi2_special_blocks);\n\tprop->special_blocks = kmalloc_array(prop->num_of_special_blocks,\n\t\t\tsizeof(*prop->special_blocks), GFP_KERNEL);\n\tif (!prop->special_blocks)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0 ; i < prop->num_of_special_blocks ; i++)\n\t\tmemcpy(&prop->special_blocks[i], &gaudi2_special_blocks[i],\n\t\t\t\tsizeof(*prop->special_blocks));\n\n\t \n\tmemset(&prop->skip_special_blocks_cfg, 0, sizeof(prop->skip_special_blocks_cfg));\n\tprop->skip_special_blocks_cfg.skip_block_hook = gaudi2_special_block_skip;\n\n\tif (ARRAY_SIZE(gaudi2_iterator_skip_block_types)) {\n\t\tprop->skip_special_blocks_cfg.block_types =\n\t\t\t\tkmalloc_array(ARRAY_SIZE(gaudi2_iterator_skip_block_types),\n\t\t\t\t\tsizeof(gaudi2_iterator_skip_block_types[0]), GFP_KERNEL);\n\t\tif (!prop->skip_special_blocks_cfg.block_types) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_special_blocks;\n\t\t}\n\n\t\tmemcpy(prop->skip_special_blocks_cfg.block_types, gaudi2_iterator_skip_block_types,\n\t\t\t\tsizeof(gaudi2_iterator_skip_block_types));\n\n\t\tprop->skip_special_blocks_cfg.block_types_len =\n\t\t\t\t\tARRAY_SIZE(gaudi2_iterator_skip_block_types);\n\t}\n\n\tif (ARRAY_SIZE(gaudi2_iterator_skip_block_ranges)) {\n\t\tprop->skip_special_blocks_cfg.block_ranges =\n\t\t\t\tkmalloc_array(ARRAY_SIZE(gaudi2_iterator_skip_block_ranges),\n\t\t\t\t\tsizeof(gaudi2_iterator_skip_block_ranges[0]), GFP_KERNEL);\n\t\tif (!prop->skip_special_blocks_cfg.block_ranges) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_skip_special_blocks_types;\n\t\t}\n\n\t\tfor (i = 0 ; i < ARRAY_SIZE(gaudi2_iterator_skip_block_ranges) ; i++)\n\t\t\tmemcpy(&prop->skip_special_blocks_cfg.block_ranges[i],\n\t\t\t\t\t&gaudi2_iterator_skip_block_ranges[i],\n\t\t\t\t\tsizeof(struct range));\n\n\t\tprop->skip_special_blocks_cfg.block_ranges_len =\n\t\t\t\t\tARRAY_SIZE(gaudi2_iterator_skip_block_ranges);\n\t}\n\n\treturn 0;\n\nfree_skip_special_blocks_types:\n\tkfree(prop->skip_special_blocks_cfg.block_types);\nfree_special_blocks:\n\tkfree(prop->special_blocks);\n\n\treturn rc;\n}\n\nstatic int gaudi2_special_blocks_iterator_config(struct hl_device *hdev)\n{\n\treturn gaudi2_special_blocks_config(hdev);\n}\n\nstatic void gaudi2_test_queues_msgs_free(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_queues_test_info *msg_info = gaudi2->queues_test_info;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_TESTED_QS ; i++) {\n\t\t \n\t\tif (!msg_info[i].kern_addr)\n\t\t\tbreak;\n\n\t\thl_asic_dma_pool_free(hdev, msg_info[i].kern_addr, msg_info[i].dma_addr);\n\t\tmsg_info[i].kern_addr = NULL;\n\t}\n}\n\nstatic int gaudi2_test_queues_msgs_alloc(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_queues_test_info *msg_info = gaudi2->queues_test_info;\n\tint i, rc;\n\n\t \n\tfor (i = 0 ; i < GAUDI2_NUM_TESTED_QS ; i++) {\n\t\tmsg_info[i].kern_addr =\n\t\t\t(void *)hl_asic_dma_pool_zalloc(hdev, sizeof(struct packet_msg_short),\n\t\t\t\t\t\t\tGFP_KERNEL, &msg_info[i].dma_addr);\n\t\tif (!msg_info[i].kern_addr) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to allocate dma memory for H/W queue %d testing\\n\", i);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_exit;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_exit:\n\tgaudi2_test_queues_msgs_free(hdev);\n\treturn rc;\n}\n\nstatic int gaudi2_sw_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2;\n\tint i, rc;\n\n\t \n\tgaudi2 = kzalloc(sizeof(*gaudi2), GFP_KERNEL);\n\tif (!gaudi2)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0 ; i < ARRAY_SIZE(gaudi2_irq_map_table) ; i++) {\n\t\tif (gaudi2_irq_map_table[i].msg || !gaudi2_irq_map_table[i].valid)\n\t\t\tcontinue;\n\n\t\tif (gaudi2->num_of_valid_hw_events == GAUDI2_EVENT_SIZE) {\n\t\t\tdev_err(hdev->dev, \"H/W events array exceeds the limit of %u events\\n\",\n\t\t\t\tGAUDI2_EVENT_SIZE);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_gaudi2_device;\n\t\t}\n\n\t\tgaudi2->hw_events[gaudi2->num_of_valid_hw_events++] = gaudi2_irq_map_table[i].fc_id;\n\t}\n\n\tfor (i = 0 ; i < MME_NUM_OF_LFSR_SEEDS ; i++)\n\t\tgaudi2->lfsr_rand_seeds[i] = gaudi2_get_non_zero_random_int();\n\n\tgaudi2->cpucp_info_get = gaudi2_cpucp_info_get;\n\n\thdev->asic_specific = gaudi2;\n\n\t \n\thdev->dma_pool = dma_pool_create(dev_name(hdev->dev), &hdev->pdev->dev,\n\t\t\t\t\tGAUDI2_DMA_POOL_BLK_SIZE, DEVICE_CACHE_LINE_SIZE, 0);\n\tif (!hdev->dma_pool) {\n\t\tdev_err(hdev->dev, \"failed to create DMA pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_gaudi2_device;\n\t}\n\n\trc = gaudi2_alloc_cpu_accessible_dma_mem(hdev);\n\tif (rc)\n\t\tgoto free_dma_pool;\n\n\thdev->cpu_accessible_dma_pool = gen_pool_create(ilog2(32), -1);\n\tif (!hdev->cpu_accessible_dma_pool) {\n\t\tdev_err(hdev->dev, \"Failed to create CPU accessible DMA pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_cpu_dma_mem;\n\t}\n\n\trc = gen_pool_add(hdev->cpu_accessible_dma_pool, (uintptr_t) hdev->cpu_accessible_dma_mem,\n\t\t\t\tHL_CPU_ACCESSIBLE_MEM_SIZE, -1);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to add memory to CPU accessible DMA pool\\n\");\n\t\trc = -EFAULT;\n\t\tgoto free_cpu_accessible_dma_pool;\n\t}\n\n\tgaudi2->virt_msix_db_cpu_addr = hl_cpu_accessible_dma_pool_alloc(hdev, prop->pmmu.page_size,\n\t\t\t\t\t\t\t\t&gaudi2->virt_msix_db_dma_addr);\n\tif (!gaudi2->virt_msix_db_cpu_addr) {\n\t\tdev_err(hdev->dev, \"Failed to allocate DMA memory for virtual MSI-X doorbell\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_cpu_accessible_dma_pool;\n\t}\n\n\tspin_lock_init(&gaudi2->hw_queues_lock);\n\n\tgaudi2->scratchpad_kernel_address = hl_asic_dma_alloc_coherent(hdev, PAGE_SIZE,\n\t\t\t\t\t\t\t&gaudi2->scratchpad_bus_address,\n\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!gaudi2->scratchpad_kernel_address) {\n\t\trc = -ENOMEM;\n\t\tgoto free_virt_msix_db_mem;\n\t}\n\n\tgaudi2_user_mapped_blocks_init(hdev);\n\n\t \n\tgaudi2_user_interrupt_setup(hdev);\n\n\thdev->supports_coresight = true;\n\thdev->supports_sync_stream = true;\n\thdev->supports_cb_mapping = true;\n\thdev->supports_wait_for_multi_cs = false;\n\n\tprop->supports_compute_reset = true;\n\n\t \n\tif (hl_is_fw_sw_ver_below(hdev, 1, 11))\n\t\thdev->event_queue.check_eqe_index = false;\n\telse\n\t\thdev->event_queue.check_eqe_index = true;\n\n\thdev->asic_funcs->set_pci_memory_regions(hdev);\n\n\trc = gaudi2_special_blocks_iterator_config(hdev);\n\tif (rc)\n\t\tgoto free_scratchpad_mem;\n\n\trc = gaudi2_test_queues_msgs_alloc(hdev);\n\tif (rc)\n\t\tgoto special_blocks_free;\n\n\treturn 0;\n\nspecial_blocks_free:\n\tgaudi2_special_blocks_iterator_free(hdev);\nfree_scratchpad_mem:\n\thl_asic_dma_free_coherent(hdev, PAGE_SIZE, gaudi2->scratchpad_kernel_address,\n\t\t\t\t  gaudi2->scratchpad_bus_address);\nfree_virt_msix_db_mem:\n\thl_cpu_accessible_dma_pool_free(hdev, prop->pmmu.page_size, gaudi2->virt_msix_db_cpu_addr);\nfree_cpu_accessible_dma_pool:\n\tgen_pool_destroy(hdev->cpu_accessible_dma_pool);\nfree_cpu_dma_mem:\n\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, hdev->cpu_accessible_dma_mem,\n\t\t\t\t\thdev->cpu_accessible_dma_address);\nfree_dma_pool:\n\tdma_pool_destroy(hdev->dma_pool);\nfree_gaudi2_device:\n\tkfree(gaudi2);\n\treturn rc;\n}\n\nstatic int gaudi2_sw_fini(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tgaudi2_test_queues_msgs_free(hdev);\n\n\tgaudi2_special_blocks_iterator_free(hdev);\n\n\thl_cpu_accessible_dma_pool_free(hdev, prop->pmmu.page_size, gaudi2->virt_msix_db_cpu_addr);\n\n\tgen_pool_destroy(hdev->cpu_accessible_dma_pool);\n\n\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, hdev->cpu_accessible_dma_mem,\n\t\t\t\t\t\thdev->cpu_accessible_dma_address);\n\n\thl_asic_dma_free_coherent(hdev, PAGE_SIZE, gaudi2->scratchpad_kernel_address,\n\t\t\t\t\tgaudi2->scratchpad_bus_address);\n\n\tdma_pool_destroy(hdev->dma_pool);\n\n\tkfree(gaudi2);\n\n\treturn 0;\n}\n\nstatic void gaudi2_stop_qman_common(struct hl_device *hdev, u32 reg_base)\n{\n\tWREG32(reg_base + QM_GLBL_CFG1_OFFSET, QM_GLBL_CFG1_PQF_STOP |\n\t\t\t\t\t\tQM_GLBL_CFG1_CQF_STOP |\n\t\t\t\t\t\tQM_GLBL_CFG1_CP_STOP);\n\n\t \n\tWREG32(reg_base + QM_GLBL_CFG2_OFFSET, QM_GLBL_CFG2_ARC_CQF_STOP);\n}\n\nstatic void gaudi2_flush_qman_common(struct hl_device *hdev, u32 reg_base)\n{\n\tWREG32(reg_base + QM_GLBL_CFG1_OFFSET, QM_GLBL_CFG1_PQF_FLUSH |\n\t\t\t\t\t\tQM_GLBL_CFG1_CQF_FLUSH |\n\t\t\t\t\t\tQM_GLBL_CFG1_CP_FLUSH);\n}\n\nstatic void gaudi2_flush_qman_arc_common(struct hl_device *hdev, u32 reg_base)\n{\n\tWREG32(reg_base + QM_GLBL_CFG2_OFFSET, QM_GLBL_CFG2_ARC_CQF_FLUSH);\n}\n\n \nstatic void gaudi2_clear_qm_fence_counters_common(struct hl_device *hdev, u32 queue_id,\n\t\t\t\t\t\tbool skip_fence)\n{\n\tu32 size, reg_base;\n\tu32 addr, val;\n\n\treg_base = gaudi2_qm_blocks_bases[queue_id];\n\n\taddr = reg_base + QM_CP_FENCE0_CNT_0_OFFSET;\n\tsize = mmPDMA0_QM_CP_BARRIER_CFG - mmPDMA0_QM_CP_FENCE0_CNT_0;\n\n\t \n\tval = skip_fence ? U32_MAX : 0;\n\tgaudi2_memset_device_lbw(hdev, addr, size, val);\n}\n\nstatic void gaudi2_qman_manual_flush_common(struct hl_device *hdev, u32 queue_id)\n{\n\tu32 reg_base = gaudi2_qm_blocks_bases[queue_id];\n\n\tgaudi2_clear_qm_fence_counters_common(hdev, queue_id, true);\n\tgaudi2_flush_qman_common(hdev, reg_base);\n\tgaudi2_flush_qman_arc_common(hdev, reg_base);\n}\n\nstatic void gaudi2_stop_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint dcore, inst;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PDMA_MASK))\n\t\tgoto stop_edma_qmans;\n\n\t \n\tgaudi2_stop_qman_common(hdev, mmPDMA0_QM_BASE);\n\tgaudi2_stop_qman_common(hdev, mmPDMA1_QM_BASE);\n\nstop_edma_qmans:\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_EDMA_MASK))\n\t\treturn;\n\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (inst = 0 ; inst < NUM_OF_EDMA_PER_DCORE ; inst++) {\n\t\t\tu8 seq = dcore * NUM_OF_EDMA_PER_DCORE + inst;\n\t\t\tu32 qm_base;\n\n\t\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_EDMA_SHIFT + seq)))\n\t\t\t\tcontinue;\n\n\t\t\tqm_base = mmDCORE0_EDMA0_QM_BASE + dcore * DCORE_OFFSET +\n\t\t\t\t\tinst * DCORE_EDMA_OFFSET;\n\n\t\t\t \n\t\t\tgaudi2_stop_qman_common(hdev, qm_base);\n\t\t}\n\t}\n}\n\nstatic void gaudi2_stop_mme_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 offset, i;\n\n\toffset = mmDCORE1_MME_QM_BASE - mmDCORE0_MME_QM_BASE;\n\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++) {\n\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_MME_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\tgaudi2_stop_qman_common(hdev, mmDCORE0_MME_QM_BASE + (i * offset));\n\t}\n}\n\nstatic void gaudi2_stop_tpc_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\tint i;\n\n\tif (!(gaudi2->tpc_hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tfor (i = 0 ; i < TPC_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(HW_CAP_TPC_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[gaudi2_tpc_id_to_queue_id[i]];\n\t\tgaudi2_stop_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_stop_rot_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\tint i;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_ROT_MASK))\n\t\treturn;\n\n\tfor (i = 0 ; i < ROTATOR_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_ROT_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[gaudi2_rot_id_to_queue_id[i]];\n\t\tgaudi2_stop_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_stop_nic_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base, queue_id;\n\tint i;\n\n\tif (!(gaudi2->nic_hw_cap_initialized & HW_CAP_NIC_MASK))\n\t\treturn;\n\n\tqueue_id = GAUDI2_QUEUE_ID_NIC_0_0;\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++, queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\tif (!(hdev->nic_ports_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[queue_id];\n\t\tgaudi2_stop_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_stall_dma_common(struct hl_device *hdev, u32 reg_base)\n{\n\tu32 reg_val;\n\n\treg_val = FIELD_PREP(PDMA0_CORE_CFG_1_HALT_MASK, 0x1);\n\tWREG32(reg_base + DMA_CORE_CFG_1_OFFSET, reg_val);\n}\n\nstatic void gaudi2_dma_stall(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint dcore, inst;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PDMA_MASK))\n\t\tgoto stall_edma;\n\n\tgaudi2_stall_dma_common(hdev, mmPDMA0_CORE_BASE);\n\tgaudi2_stall_dma_common(hdev, mmPDMA1_CORE_BASE);\n\nstall_edma:\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_EDMA_MASK))\n\t\treturn;\n\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (inst = 0 ; inst < NUM_OF_EDMA_PER_DCORE ; inst++) {\n\t\t\tu8 seq = dcore * NUM_OF_EDMA_PER_DCORE + inst;\n\t\t\tu32 core_base;\n\n\t\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_EDMA_SHIFT + seq)))\n\t\t\t\tcontinue;\n\n\t\t\tcore_base = mmDCORE0_EDMA0_CORE_BASE + dcore * DCORE_OFFSET +\n\t\t\t\t\tinst * DCORE_EDMA_OFFSET;\n\n\t\t\t \n\t\t\tgaudi2_stall_dma_common(hdev, core_base);\n\t\t}\n\t}\n}\n\nstatic void gaudi2_mme_stall(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 offset, i;\n\n\toffset = mmDCORE1_MME_CTRL_LO_QM_STALL - mmDCORE0_MME_CTRL_LO_QM_STALL;\n\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++)\n\t\tif (gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_MME_SHIFT + i))\n\t\t\tWREG32(mmDCORE0_MME_CTRL_LO_QM_STALL + (i * offset), 1);\n}\n\nstatic void gaudi2_tpc_stall(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\tint i;\n\n\tif (!(gaudi2->tpc_hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tfor (i = 0 ; i < TPC_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(HW_CAP_TPC_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_tpc_cfg_blocks_bases[i];\n\t\tWREG32(reg_base + TPC_CFG_STALL_OFFSET, 1);\n\t}\n}\n\nstatic void gaudi2_rotator_stall(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_val;\n\tint i;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_ROT_MASK))\n\t\treturn;\n\n\treg_val = FIELD_PREP(ROT_MSS_HALT_WBC_MASK, 0x1) |\n\t\t\tFIELD_PREP(ROT_MSS_HALT_RSB_MASK, 0x1) |\n\t\t\tFIELD_PREP(ROT_MSS_HALT_MRSB_MASK, 0x1);\n\n\tfor (i = 0 ; i < ROTATOR_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_ROT_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\tWREG32(mmROT0_MSS_HALT + i * ROT_OFFSET, reg_val);\n\t}\n}\n\nstatic void gaudi2_disable_qman_common(struct hl_device *hdev, u32 reg_base)\n{\n\tWREG32(reg_base + QM_GLBL_CFG0_OFFSET, 0);\n}\n\nstatic void gaudi2_disable_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint dcore, inst;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PDMA_MASK))\n\t\tgoto stop_edma_qmans;\n\n\tgaudi2_disable_qman_common(hdev, mmPDMA0_QM_BASE);\n\tgaudi2_disable_qman_common(hdev, mmPDMA1_QM_BASE);\n\nstop_edma_qmans:\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_EDMA_MASK))\n\t\treturn;\n\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (inst = 0 ; inst < NUM_OF_EDMA_PER_DCORE ; inst++) {\n\t\t\tu8 seq = dcore * NUM_OF_EDMA_PER_DCORE + inst;\n\t\t\tu32 qm_base;\n\n\t\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_EDMA_SHIFT + seq)))\n\t\t\t\tcontinue;\n\n\t\t\tqm_base = mmDCORE0_EDMA0_QM_BASE + dcore * DCORE_OFFSET +\n\t\t\t\t\tinst * DCORE_EDMA_OFFSET;\n\n\t\t\t \n\t\t\tgaudi2_disable_qman_common(hdev, qm_base);\n\t\t}\n\t}\n}\n\nstatic void gaudi2_disable_mme_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 offset, i;\n\n\toffset = mmDCORE1_MME_QM_BASE - mmDCORE0_MME_QM_BASE;\n\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++)\n\t\tif (gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_MME_SHIFT + i))\n\t\t\tgaudi2_disable_qman_common(hdev, mmDCORE0_MME_QM_BASE + (i * offset));\n}\n\nstatic void gaudi2_disable_tpc_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\tint i;\n\n\tif (!(gaudi2->tpc_hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tfor (i = 0 ; i < TPC_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(HW_CAP_TPC_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[gaudi2_tpc_id_to_queue_id[i]];\n\t\tgaudi2_disable_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_disable_rot_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\tint i;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_ROT_MASK))\n\t\treturn;\n\n\tfor (i = 0 ; i < ROTATOR_ID_SIZE ; i++) {\n\t\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_ROT_SHIFT + i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[gaudi2_rot_id_to_queue_id[i]];\n\t\tgaudi2_disable_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_disable_nic_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base, queue_id;\n\tint i;\n\n\tif (!(gaudi2->nic_hw_cap_initialized & HW_CAP_NIC_MASK))\n\t\treturn;\n\n\tqueue_id = GAUDI2_QUEUE_ID_NIC_0_0;\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++, queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\tif (!(hdev->nic_ports_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\treg_base = gaudi2_qm_blocks_bases[queue_id];\n\t\tgaudi2_disable_qman_common(hdev, reg_base);\n\t}\n}\n\nstatic void gaudi2_enable_timestamp(struct hl_device *hdev)\n{\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE, 0);\n\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE + 0xC, 0);\n\tWREG32(mmPSOC_TIMESTAMP_BASE + 0x8, 0);\n\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE, 1);\n}\n\nstatic void gaudi2_disable_timestamp(struct hl_device *hdev)\n{\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE, 0);\n}\n\nstatic const char *gaudi2_irq_name(u16 irq_number)\n{\n\tswitch (irq_number) {\n\tcase GAUDI2_IRQ_NUM_EVENT_QUEUE:\n\t\treturn \"gaudi2 cpu eq\";\n\tcase GAUDI2_IRQ_NUM_COMPLETION:\n\t\treturn \"gaudi2 completion\";\n\tcase GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM ... GAUDI2_IRQ_NUM_SHARED_DEC1_ABNRM:\n\t\treturn gaudi2_vdec_irq_name[irq_number - GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM];\n\tcase GAUDI2_IRQ_NUM_TPC_ASSERT:\n\t\treturn \"gaudi2 tpc assert\";\n\tcase GAUDI2_IRQ_NUM_UNEXPECTED_ERROR:\n\t\treturn \"gaudi2 unexpected error\";\n\tcase GAUDI2_IRQ_NUM_USER_FIRST ... GAUDI2_IRQ_NUM_USER_LAST:\n\t\treturn \"gaudi2 user completion\";\n\tdefault:\n\t\treturn \"invalid\";\n\t}\n}\n\nstatic void gaudi2_dec_disable_msix(struct hl_device *hdev, u32 max_irq_num)\n{\n\tint i, irq, relative_idx;\n\tstruct hl_dec *dec;\n\n\tfor (i = GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM ; i < max_irq_num ; i++) {\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\trelative_idx = i - GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM;\n\n\t\tdec = hdev->dec + relative_idx / 2;\n\n\t\t \n\t\tfree_irq(irq, ((relative_idx % 2) ?\n\t\t\t\t(void *) dec :\n\t\t\t\t(void *) &hdev->user_interrupt[dec->core_id]));\n\t}\n}\n\nstatic int gaudi2_dec_enable_msix(struct hl_device *hdev)\n{\n\tint rc, i, irq_init_cnt, irq, relative_idx;\n\tstruct hl_dec *dec;\n\n\tfor (i = GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM, irq_init_cnt = 0;\n\t\t\ti <= GAUDI2_IRQ_NUM_SHARED_DEC1_ABNRM;\n\t\t\ti++, irq_init_cnt++) {\n\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\trelative_idx = i - GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM;\n\n\t\t \n\n\t\tdec = hdev->dec + relative_idx / 2;\n\t\tif (relative_idx % 2) {\n\t\t\trc = request_irq(irq, hl_irq_handler_dec_abnrm, 0,\n\t\t\t\t\t\tgaudi2_irq_name(i), (void *) dec);\n\t\t} else {\n\t\t\trc = request_threaded_irq(irq, hl_irq_handler_user_interrupt,\n\t\t\t\t\thl_irq_user_interrupt_thread_handler, IRQF_ONESHOT,\n\t\t\t\t\tgaudi2_irq_name(i),\n\t\t\t\t\t(void *) &hdev->user_interrupt[dec->core_id]);\n\t\t}\n\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\t\tgoto free_dec_irqs;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_dec_irqs:\n\tgaudi2_dec_disable_msix(hdev, (GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM + irq_init_cnt));\n\treturn rc;\n}\n\nstatic int gaudi2_enable_msix(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint rc, irq, i, j, user_irq_init_cnt;\n\tstruct hl_cq *cq;\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_MSIX)\n\t\treturn 0;\n\n\trc = pci_alloc_irq_vectors(hdev->pdev, GAUDI2_MSIX_ENTRIES, GAUDI2_MSIX_ENTRIES,\n\t\t\t\t\tPCI_IRQ_MSIX);\n\tif (rc < 0) {\n\t\tdev_err(hdev->dev, \"MSI-X: Failed to enable support -- %d/%d\\n\",\n\t\t\tGAUDI2_MSIX_ENTRIES, rc);\n\t\treturn rc;\n\t}\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_COMPLETION);\n\tcq = &hdev->completion_queue[GAUDI2_RESERVED_CQ_CS_COMPLETION];\n\trc = request_irq(irq, hl_irq_handler_cq, 0, gaudi2_irq_name(GAUDI2_IRQ_NUM_COMPLETION), cq);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\tgoto free_irq_vectors;\n\t}\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_EVENT_QUEUE);\n\trc = request_irq(irq, hl_irq_handler_eq, 0, gaudi2_irq_name(GAUDI2_IRQ_NUM_EVENT_QUEUE),\n\t\t\t&hdev->event_queue);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\tgoto free_completion_irq;\n\t}\n\n\trc = gaudi2_dec_enable_msix(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to enable decoder IRQ\");\n\t\tgoto free_event_irq;\n\t}\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_TPC_ASSERT);\n\trc = request_threaded_irq(irq, hl_irq_handler_user_interrupt,\n\t\t\thl_irq_user_interrupt_thread_handler, IRQF_ONESHOT,\n\t\t\tgaudi2_irq_name(GAUDI2_IRQ_NUM_TPC_ASSERT), &hdev->tpc_interrupt);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\tgoto free_dec_irq;\n\t}\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_UNEXPECTED_ERROR);\n\trc = request_irq(irq, hl_irq_handler_user_interrupt, 0,\n\t\t\tgaudi2_irq_name(GAUDI2_IRQ_NUM_UNEXPECTED_ERROR),\n\t\t\t\t\t&hdev->unexpected_error_interrupt);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\tgoto free_tpc_irq;\n\t}\n\n\tfor (i = GAUDI2_IRQ_NUM_USER_FIRST, j = prop->user_dec_intr_count, user_irq_init_cnt = 0;\n\t\t\tuser_irq_init_cnt < prop->user_interrupt_count;\n\t\t\ti++, j++, user_irq_init_cnt++) {\n\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\trc = request_threaded_irq(irq, hl_irq_handler_user_interrupt,\n\t\t\t\t\t\thl_irq_user_interrupt_thread_handler, IRQF_ONESHOT,\n\t\t\t\t\t\tgaudi2_irq_name(i), &hdev->user_interrupt[j]);\n\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"Failed to request IRQ %d\", irq);\n\t\t\tgoto free_user_irq;\n\t\t}\n\t}\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_MSIX;\n\n\treturn 0;\n\nfree_user_irq:\n\tfor (i = GAUDI2_IRQ_NUM_USER_FIRST, j = prop->user_dec_intr_count;\n\t\t\ti < GAUDI2_IRQ_NUM_USER_FIRST + user_irq_init_cnt ; i++, j++) {\n\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\tfree_irq(irq, &hdev->user_interrupt[j]);\n\t}\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_UNEXPECTED_ERROR);\n\tfree_irq(irq, &hdev->unexpected_error_interrupt);\nfree_tpc_irq:\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_TPC_ASSERT);\n\tfree_irq(irq, &hdev->tpc_interrupt);\nfree_dec_irq:\n\tgaudi2_dec_disable_msix(hdev, GAUDI2_IRQ_NUM_DEC_LAST + 1);\nfree_event_irq:\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_EVENT_QUEUE);\n\tfree_irq(irq, cq);\n\nfree_completion_irq:\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_COMPLETION);\n\tfree_irq(irq, cq);\n\nfree_irq_vectors:\n\tpci_free_irq_vectors(hdev->pdev);\n\n\treturn rc;\n}\n\nstatic void gaudi2_sync_irqs(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint i, j;\n\tint irq;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_MSIX))\n\t\treturn;\n\n\t \n\tsynchronize_irq(pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_COMPLETION));\n\n\tfor (i = GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM ; i <= GAUDI2_IRQ_NUM_SHARED_DEC1_ABNRM ; i++) {\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\tsynchronize_irq(irq);\n\t}\n\n\tsynchronize_irq(pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_TPC_ASSERT));\n\tsynchronize_irq(pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_UNEXPECTED_ERROR));\n\n\tfor (i = GAUDI2_IRQ_NUM_USER_FIRST, j = 0 ; j < hdev->asic_prop.user_interrupt_count;\n\t\t\t\t\t\t\t\t\t\ti++, j++) {\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\tsynchronize_irq(irq);\n\t}\n\n\tsynchronize_irq(pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_EVENT_QUEUE));\n}\n\nstatic void gaudi2_disable_msix(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct hl_cq *cq;\n\tint irq, i, j, k;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_MSIX))\n\t\treturn;\n\n\tgaudi2_sync_irqs(hdev);\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_EVENT_QUEUE);\n\tfree_irq(irq, &hdev->event_queue);\n\n\tgaudi2_dec_disable_msix(hdev, GAUDI2_IRQ_NUM_SHARED_DEC1_ABNRM + 1);\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_TPC_ASSERT);\n\tfree_irq(irq, &hdev->tpc_interrupt);\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_UNEXPECTED_ERROR);\n\tfree_irq(irq, &hdev->unexpected_error_interrupt);\n\n\tfor (i = GAUDI2_IRQ_NUM_USER_FIRST, j = prop->user_dec_intr_count, k = 0;\n\t\t\tk < hdev->asic_prop.user_interrupt_count ; i++, j++, k++) {\n\n\t\tirq = pci_irq_vector(hdev->pdev, i);\n\t\tfree_irq(irq, &hdev->user_interrupt[j]);\n\t}\n\n\tirq = pci_irq_vector(hdev->pdev, GAUDI2_IRQ_NUM_COMPLETION);\n\tcq = &hdev->completion_queue[GAUDI2_RESERVED_CQ_CS_COMPLETION];\n\tfree_irq(irq, cq);\n\n\tpci_free_irq_vectors(hdev->pdev);\n\n\tgaudi2->hw_cap_initialized &= ~HW_CAP_MSIX;\n}\n\nstatic void gaudi2_stop_dcore_dec(struct hl_device *hdev, int dcore_id)\n{\n\tu32 reg_val = FIELD_PREP(DCORE0_VDEC0_BRDG_CTRL_GRACEFUL_STOP_MASK, 0x1);\n\tu32 graceful_pend_mask = DCORE0_VDEC0_BRDG_CTRL_GRACEFUL_PEND_MASK;\n\tu32 timeout_usec, dec_id, dec_bit, offset, graceful;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI2_PLDM_VDEC_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = GAUDI2_VDEC_TIMEOUT_USEC;\n\n\tfor (dec_id = 0 ; dec_id < NUM_OF_DEC_PER_DCORE ; dec_id++) {\n\t\tdec_bit = dcore_id * NUM_OF_DEC_PER_DCORE + dec_id;\n\t\tif (!(hdev->asic_prop.decoder_enabled_mask & BIT(dec_bit)))\n\t\t\tcontinue;\n\n\t\toffset = dcore_id * DCORE_OFFSET + dec_id * DCORE_VDEC_OFFSET;\n\n\t\tWREG32(mmDCORE0_DEC0_CMD_SWREG16 + offset, 0);\n\n\t\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_GRACEFUL + offset, reg_val);\n\n\t\t \n\t\trc = hl_poll_timeout(\n\t\t\t\thdev,\n\t\t\t\tmmDCORE0_VDEC0_BRDG_CTRL_GRACEFUL + offset,\n\t\t\t\tgraceful,\n\t\t\t\t(graceful & graceful_pend_mask),\n\t\t\t\t100,\n\t\t\t\ttimeout_usec);\n\t\tif (rc)\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to stop traffic from DCORE%d Decoder %d\\n\",\n\t\t\t\tdcore_id, dec_id);\n\t}\n}\n\nstatic void gaudi2_stop_pcie_dec(struct hl_device *hdev)\n{\n\tu32 reg_val = FIELD_PREP(DCORE0_VDEC0_BRDG_CTRL_GRACEFUL_STOP_MASK, 0x1);\n\tu32 graceful_pend_mask = PCIE_VDEC0_BRDG_CTRL_GRACEFUL_PEND_MASK;\n\tu32 timeout_usec, dec_id, dec_bit, offset, graceful;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI2_PLDM_VDEC_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = GAUDI2_VDEC_TIMEOUT_USEC;\n\n\tfor (dec_id = 0 ; dec_id < NUM_OF_DEC_PER_DCORE ; dec_id++) {\n\t\tdec_bit = PCIE_DEC_SHIFT + dec_id;\n\t\tif (!(hdev->asic_prop.decoder_enabled_mask & BIT(dec_bit)))\n\t\t\tcontinue;\n\n\t\toffset = dec_id * PCIE_VDEC_OFFSET;\n\n\t\tWREG32(mmPCIE_DEC0_CMD_SWREG16 + offset, 0);\n\n\t\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_GRACEFUL + offset, reg_val);\n\n\t\t \n\t\trc = hl_poll_timeout(\n\t\t\t\thdev,\n\t\t\t\tmmPCIE_VDEC0_BRDG_CTRL_GRACEFUL + offset,\n\t\t\t\tgraceful,\n\t\t\t\t(graceful & graceful_pend_mask),\n\t\t\t\t100,\n\t\t\t\ttimeout_usec);\n\t\tif (rc)\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to stop traffic from PCIe Decoder %d\\n\",\n\t\t\t\tdec_id);\n\t}\n}\n\nstatic void gaudi2_stop_dec(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint dcore_id;\n\n\tif ((gaudi2->dec_hw_cap_initialized & HW_CAP_DEC_MASK) == 0)\n\t\treturn;\n\n\tfor (dcore_id = 0 ; dcore_id < NUM_OF_DCORES ; dcore_id++)\n\t\tgaudi2_stop_dcore_dec(hdev, dcore_id);\n\n\tgaudi2_stop_pcie_dec(hdev);\n}\n\nstatic void gaudi2_set_arc_running_mode(struct hl_device *hdev, u32 cpu_id, u32 run_mode)\n{\n\tu32 reg_base, reg_val;\n\n\treg_base = gaudi2_arc_blocks_bases[cpu_id];\n\tif (run_mode == HL_ENGINE_CORE_RUN)\n\t\treg_val = FIELD_PREP(ARC_FARM_ARC0_AUX_RUN_HALT_REQ_RUN_REQ_MASK, 1);\n\telse\n\t\treg_val = FIELD_PREP(ARC_FARM_ARC0_AUX_RUN_HALT_REQ_HALT_REQ_MASK, 1);\n\n\tWREG32(reg_base + ARC_HALT_REQ_OFFSET, reg_val);\n}\n\nstatic void gaudi2_halt_arcs(struct hl_device *hdev)\n{\n\tu16 arc_id;\n\n\tfor (arc_id = CPU_ID_SCHED_ARC0; arc_id < CPU_ID_MAX; arc_id++) {\n\t\tif (gaudi2_is_arc_enabled(hdev, arc_id))\n\t\t\tgaudi2_set_arc_running_mode(hdev, arc_id, HL_ENGINE_CORE_HALT);\n\t}\n}\n\nstatic int gaudi2_verify_arc_running_mode(struct hl_device *hdev, u32 cpu_id, u32 run_mode)\n{\n\tint rc;\n\tu32 reg_base, val, ack_mask, timeout_usec = 100000;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec *= 100;\n\n\treg_base = gaudi2_arc_blocks_bases[cpu_id];\n\tif (run_mode == HL_ENGINE_CORE_RUN)\n\t\tack_mask = ARC_FARM_ARC0_AUX_RUN_HALT_ACK_RUN_ACK_MASK;\n\telse\n\t\tack_mask = ARC_FARM_ARC0_AUX_RUN_HALT_ACK_HALT_ACK_MASK;\n\n\trc = hl_poll_timeout(hdev, reg_base + ARC_HALT_ACK_OFFSET,\n\t\t\t\tval, ((val & ack_mask) == ack_mask),\n\t\t\t\t1000, timeout_usec);\n\n\tif (!rc) {\n\t\t \n\t\tval = FIELD_PREP(ARC_FARM_ARC0_AUX_RUN_HALT_REQ_RUN_REQ_MASK, 0);\n\t\tWREG32(reg_base + ARC_HALT_REQ_OFFSET, val);\n\t}\n\n\treturn rc;\n}\n\nstatic void gaudi2_reset_arcs(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu16 arc_id;\n\n\tif (!gaudi2)\n\t\treturn;\n\n\tfor (arc_id = CPU_ID_SCHED_ARC0; arc_id < CPU_ID_MAX; arc_id++)\n\t\tif (gaudi2_is_arc_enabled(hdev, arc_id))\n\t\t\tgaudi2_clr_arc_id_cap(hdev, arc_id);\n}\n\nstatic void gaudi2_nic_qmans_manual_flush(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 queue_id;\n\tint i;\n\n\tif (!(gaudi2->nic_hw_cap_initialized & HW_CAP_NIC_MASK))\n\t\treturn;\n\n\tqueue_id = GAUDI2_QUEUE_ID_NIC_0_0;\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++, queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\tif (!(hdev->nic_ports_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\tgaudi2_qman_manual_flush_common(hdev, queue_id);\n\t}\n}\n\nstatic int gaudi2_set_engine_cores(struct hl_device *hdev, u32 *core_ids,\n\t\t\t\t\tu32 num_cores, u32 core_command)\n{\n\tint i, rc;\n\n\tfor (i = 0 ; i < num_cores ; i++) {\n\t\tif (gaudi2_is_arc_enabled(hdev, core_ids[i]))\n\t\t\tgaudi2_set_arc_running_mode(hdev, core_ids[i], core_command);\n\t}\n\n\tfor (i = 0 ; i < num_cores ; i++) {\n\t\tif (gaudi2_is_arc_enabled(hdev, core_ids[i])) {\n\t\t\trc = gaudi2_verify_arc_running_mode(hdev, core_ids[i], core_command);\n\n\t\t\tif (rc) {\n\t\t\t\tdev_err(hdev->dev, \"failed to %s arc: %d\\n\",\n\t\t\t\t\t(core_command == HL_ENGINE_CORE_HALT) ?\n\t\t\t\t\t\"HALT\" : \"RUN\", core_ids[i]);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_tpc_engine_mode(struct hl_device *hdev, u32 engine_id, u32 engine_command)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base, reg_addr, reg_val, tpc_id;\n\n\tif (!(gaudi2->tpc_hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn 0;\n\n\ttpc_id = gaudi2_tpc_engine_id_to_tpc_id[engine_id];\n\tif (!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(HW_CAP_TPC_SHIFT + tpc_id)))\n\t\treturn 0;\n\n\treg_base = gaudi2_tpc_cfg_blocks_bases[tpc_id];\n\treg_addr = reg_base + TPC_CFG_STALL_OFFSET;\n\treg_val = FIELD_PREP(DCORE0_TPC0_CFG_TPC_STALL_V_MASK,\n\t\t\t(engine_command == HL_ENGINE_STALL) ? 1 : 0);\n\tWREG32(reg_addr, reg_val);\n\n\tif (engine_command == HL_ENGINE_RESUME) {\n\t\treg_base = gaudi2_tpc_eml_cfg_blocks_bases[tpc_id];\n\t\treg_addr = reg_base + TPC_EML_CFG_DBG_CNT_OFFSET;\n\t\tRMWREG32(reg_addr, 0x1, DCORE0_TPC0_EML_CFG_DBG_CNT_DBG_EXIT_MASK);\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_mme_engine_mode(struct hl_device *hdev, u32 engine_id, u32 engine_command)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base, reg_addr, reg_val, mme_id;\n\n\tmme_id = gaudi2_mme_engine_id_to_mme_id[engine_id];\n\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_MME_SHIFT + mme_id)))\n\t\treturn 0;\n\n\treg_base = gaudi2_mme_ctrl_lo_blocks_bases[mme_id];\n\treg_addr = reg_base + MME_CTRL_LO_QM_STALL_OFFSET;\n\treg_val = FIELD_PREP(DCORE0_MME_CTRL_LO_QM_STALL_V_MASK,\n\t\t\t(engine_command == HL_ENGINE_STALL) ? 1 : 0);\n\tWREG32(reg_addr, reg_val);\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_edma_engine_mode(struct hl_device *hdev, u32 engine_id, u32 engine_command)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base, reg_addr, reg_val, edma_id;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_EDMA_MASK))\n\t\treturn 0;\n\n\tedma_id = gaudi2_edma_engine_id_to_edma_id[engine_id];\n\tif (!(gaudi2->hw_cap_initialized & BIT_ULL(HW_CAP_EDMA_SHIFT + edma_id)))\n\t\treturn 0;\n\n\treg_base = gaudi2_dma_core_blocks_bases[edma_id];\n\treg_addr = reg_base + EDMA_CORE_CFG_STALL_OFFSET;\n\treg_val = FIELD_PREP(DCORE0_EDMA0_CORE_CFG_1_HALT_MASK,\n\t\t\t(engine_command == HL_ENGINE_STALL) ? 1 : 0);\n\tWREG32(reg_addr, reg_val);\n\n\tif (engine_command == HL_ENGINE_STALL) {\n\t\treg_val = FIELD_PREP(DCORE0_EDMA0_CORE_CFG_1_HALT_MASK, 0x1) |\n\t\t\t\tFIELD_PREP(DCORE0_EDMA0_CORE_CFG_1_FLUSH_MASK, 0x1);\n\t\tWREG32(reg_addr, reg_val);\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_engine_modes(struct hl_device *hdev,\n\t\tu32 *engine_ids, u32 num_engines, u32 engine_command)\n{\n\tint i, rc;\n\n\tfor (i = 0 ; i < num_engines ; ++i) {\n\t\tswitch (engine_ids[i]) {\n\t\tcase GAUDI2_DCORE0_ENGINE_ID_TPC_0 ... GAUDI2_DCORE0_ENGINE_ID_TPC_5:\n\t\tcase GAUDI2_DCORE1_ENGINE_ID_TPC_0 ... GAUDI2_DCORE1_ENGINE_ID_TPC_5:\n\t\tcase GAUDI2_DCORE2_ENGINE_ID_TPC_0 ... GAUDI2_DCORE2_ENGINE_ID_TPC_5:\n\t\tcase GAUDI2_DCORE3_ENGINE_ID_TPC_0 ... GAUDI2_DCORE3_ENGINE_ID_TPC_5:\n\t\t\trc = gaudi2_set_tpc_engine_mode(hdev, engine_ids[i], engine_command);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\n\t\t\tbreak;\n\t\tcase GAUDI2_DCORE0_ENGINE_ID_MME:\n\t\tcase GAUDI2_DCORE1_ENGINE_ID_MME:\n\t\tcase GAUDI2_DCORE2_ENGINE_ID_MME:\n\t\tcase GAUDI2_DCORE3_ENGINE_ID_MME:\n\t\t\trc = gaudi2_set_mme_engine_mode(hdev, engine_ids[i], engine_command);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\n\t\t\tbreak;\n\t\tcase GAUDI2_DCORE0_ENGINE_ID_EDMA_0 ... GAUDI2_DCORE0_ENGINE_ID_EDMA_1:\n\t\tcase GAUDI2_DCORE1_ENGINE_ID_EDMA_0 ... GAUDI2_DCORE1_ENGINE_ID_EDMA_1:\n\t\tcase GAUDI2_DCORE2_ENGINE_ID_EDMA_0 ... GAUDI2_DCORE2_ENGINE_ID_EDMA_1:\n\t\tcase GAUDI2_DCORE3_ENGINE_ID_EDMA_0 ... GAUDI2_DCORE3_ENGINE_ID_EDMA_1:\n\t\t\trc = gaudi2_set_edma_engine_mode(hdev, engine_ids[i], engine_command);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hdev->dev, \"Invalid engine ID %u\\n\", engine_ids[i]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_set_engines(struct hl_device *hdev, u32 *engine_ids,\n\t\t\t\t\tu32 num_engines, u32 engine_command)\n{\n\tswitch (engine_command) {\n\tcase HL_ENGINE_CORE_HALT:\n\tcase HL_ENGINE_CORE_RUN:\n\t\treturn gaudi2_set_engine_cores(hdev, engine_ids, num_engines, engine_command);\n\n\tcase HL_ENGINE_STALL:\n\tcase HL_ENGINE_RESUME:\n\t\treturn gaudi2_set_engine_modes(hdev, engine_ids, num_engines, engine_command);\n\n\tdefault:\n\t\tdev_err(hdev->dev, \"failed to execute command id %u\\n\", engine_command);\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void gaudi2_halt_engines(struct hl_device *hdev, bool hard_reset, bool fw_reset)\n{\n\tu32 wait_timeout_ms;\n\n\tif (hdev->pldm)\n\t\twait_timeout_ms = GAUDI2_PLDM_RESET_WAIT_MSEC;\n\telse\n\t\twait_timeout_ms = GAUDI2_RESET_WAIT_MSEC;\n\n\tif (fw_reset)\n\t\tgoto skip_engines;\n\n\tgaudi2_stop_dma_qmans(hdev);\n\tgaudi2_stop_mme_qmans(hdev);\n\tgaudi2_stop_tpc_qmans(hdev);\n\tgaudi2_stop_rot_qmans(hdev);\n\tgaudi2_stop_nic_qmans(hdev);\n\tmsleep(wait_timeout_ms);\n\n\tgaudi2_halt_arcs(hdev);\n\tgaudi2_dma_stall(hdev);\n\tgaudi2_mme_stall(hdev);\n\tgaudi2_tpc_stall(hdev);\n\tgaudi2_rotator_stall(hdev);\n\n\tmsleep(wait_timeout_ms);\n\n\tgaudi2_stop_dec(hdev);\n\n\t \n\tif (!hard_reset)\n\t\tgaudi2_nic_qmans_manual_flush(hdev);\n\n\tgaudi2_disable_dma_qmans(hdev);\n\tgaudi2_disable_mme_qmans(hdev);\n\tgaudi2_disable_tpc_qmans(hdev);\n\tgaudi2_disable_rot_qmans(hdev);\n\tgaudi2_disable_nic_qmans(hdev);\n\tgaudi2_disable_timestamp(hdev);\n\nskip_engines:\n\tif (hard_reset) {\n\t\tgaudi2_disable_msix(hdev);\n\t\treturn;\n\t}\n\n\tgaudi2_sync_irqs(hdev);\n}\n\nstatic void gaudi2_init_firmware_preload_params(struct hl_device *hdev)\n{\n\tstruct pre_fw_load_props *pre_fw_load = &hdev->fw_loader.pre_fw_load;\n\n\tpre_fw_load->cpu_boot_status_reg = mmPSOC_GLOBAL_CONF_CPU_BOOT_STATUS;\n\tpre_fw_load->sts_boot_dev_sts0_reg = mmCPU_BOOT_DEV_STS0;\n\tpre_fw_load->sts_boot_dev_sts1_reg = mmCPU_BOOT_DEV_STS1;\n\tpre_fw_load->boot_err0_reg = mmCPU_BOOT_ERR0;\n\tpre_fw_load->boot_err1_reg = mmCPU_BOOT_ERR1;\n\tpre_fw_load->wait_for_preboot_timeout = GAUDI2_PREBOOT_REQ_TIMEOUT_USEC;\n}\n\nstatic void gaudi2_init_firmware_loader(struct hl_device *hdev)\n{\n\tstruct fw_load_mgr *fw_loader = &hdev->fw_loader;\n\tstruct dynamic_fw_load_mgr *dynamic_loader;\n\tstruct cpu_dyn_regs *dyn_regs;\n\n\t \n\tfw_loader->fw_comp_loaded = FW_TYPE_NONE;\n\tfw_loader->boot_fit_img.image_name = GAUDI2_BOOT_FIT_FILE;\n\tfw_loader->linux_img.image_name = GAUDI2_LINUX_FW_FILE;\n\tfw_loader->boot_fit_timeout = GAUDI2_BOOT_FIT_REQ_TIMEOUT_USEC;\n\tfw_loader->skip_bmc = false;\n\tfw_loader->sram_bar_id = SRAM_CFG_BAR_ID;\n\tfw_loader->dram_bar_id = DRAM_BAR_ID;\n\tfw_loader->cpu_timeout = GAUDI2_CPU_TIMEOUT_USEC;\n\n\t \n\tdynamic_loader = &hdev->fw_loader.dynamic_loader;\n\tdyn_regs = &dynamic_loader->comm_desc.cpu_dyn_regs;\n\tdyn_regs->kmd_msg_to_cpu = cpu_to_le32(mmPSOC_GLOBAL_CONF_KMD_MSG_TO_CPU);\n\tdyn_regs->cpu_cmd_status_to_host = cpu_to_le32(mmCPU_CMD_STATUS_TO_HOST);\n\tdynamic_loader->wait_for_bl_timeout = GAUDI2_WAIT_FOR_BL_TIMEOUT_USEC;\n}\n\nstatic int gaudi2_init_cpu(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint rc;\n\n\tif (!(hdev->fw_components & FW_TYPE_PREBOOT_CPU))\n\t\treturn 0;\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_CPU)\n\t\treturn 0;\n\n\trc = hl_fw_init_cpu(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_CPU;\n\n\treturn 0;\n}\n\nstatic int gaudi2_init_cpu_queues(struct hl_device *hdev, u32 cpu_timeout)\n{\n\tstruct hl_hw_queue *cpu_pq = &hdev->kernel_queues[GAUDI2_QUEUE_ID_CPU_PQ];\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct cpu_dyn_regs *dyn_regs;\n\tstruct hl_eq *eq;\n\tu32 status;\n\tint err;\n\n\tif (!hdev->cpu_queues_enable)\n\t\treturn 0;\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_CPU_Q)\n\t\treturn 0;\n\n\teq = &hdev->event_queue;\n\n\tdyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\n\tWREG32(mmCPU_IF_PQ_BASE_ADDR_LOW, lower_32_bits(cpu_pq->bus_address));\n\tWREG32(mmCPU_IF_PQ_BASE_ADDR_HIGH, upper_32_bits(cpu_pq->bus_address));\n\n\tWREG32(mmCPU_IF_EQ_BASE_ADDR_LOW, lower_32_bits(eq->bus_address));\n\tWREG32(mmCPU_IF_EQ_BASE_ADDR_HIGH, upper_32_bits(eq->bus_address));\n\n\tWREG32(mmCPU_IF_CQ_BASE_ADDR_LOW, lower_32_bits(hdev->cpu_accessible_dma_address));\n\tWREG32(mmCPU_IF_CQ_BASE_ADDR_HIGH, upper_32_bits(hdev->cpu_accessible_dma_address));\n\n\tWREG32(mmCPU_IF_PQ_LENGTH, HL_QUEUE_SIZE_IN_BYTES);\n\tWREG32(mmCPU_IF_EQ_LENGTH, HL_EQ_SIZE_IN_BYTES);\n\tWREG32(mmCPU_IF_CQ_LENGTH, HL_CPU_ACCESSIBLE_MEM_SIZE);\n\n\t \n\tWREG32(mmCPU_IF_EQ_RD_OFFS, 0);\n\n\tWREG32(mmCPU_IF_PF_PQ_PI, 0);\n\n\tWREG32(mmCPU_IF_QUEUE_INIT, PQ_INIT_STATUS_READY_FOR_CP);\n\n\t \n\n\tWREG32(le32_to_cpu(dyn_regs->gic_host_pi_upd_irq),\n\t\tgaudi2_irq_map_table[GAUDI2_EVENT_CPU_PI_UPDATE].cpu_id);\n\n\terr = hl_poll_timeout(\n\t\thdev,\n\t\tmmCPU_IF_QUEUE_INIT,\n\t\tstatus,\n\t\t(status == PQ_INIT_STATUS_READY_FOR_HOST),\n\t\t1000,\n\t\tcpu_timeout);\n\n\tif (err) {\n\t\tdev_err(hdev->dev, \"Failed to communicate with device CPU (timeout)\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\tif (prop->fw_cpu_boot_dev_sts0_valid)\n\t\tprop->fw_app_cpu_boot_dev_sts0 = RREG32(mmCPU_BOOT_DEV_STS0);\n\n\tif (prop->fw_cpu_boot_dev_sts1_valid)\n\t\tprop->fw_app_cpu_boot_dev_sts1 = RREG32(mmCPU_BOOT_DEV_STS1);\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_CPU_Q;\n\treturn 0;\n}\n\nstatic void gaudi2_init_qman_pq(struct hl_device *hdev, u32 reg_base,\n\t\t\t\tu32 queue_id_base)\n{\n\tstruct hl_hw_queue *q;\n\tu32 pq_id, pq_offset;\n\n\tfor (pq_id = 0 ; pq_id < NUM_OF_PQ_PER_QMAN ; pq_id++) {\n\t\tq = &hdev->kernel_queues[queue_id_base + pq_id];\n\t\tpq_offset = pq_id * 4;\n\n\t\tWREG32(reg_base + QM_PQ_BASE_LO_0_OFFSET + pq_offset,\n\t\t\t\tlower_32_bits(q->bus_address));\n\t\tWREG32(reg_base + QM_PQ_BASE_HI_0_OFFSET + pq_offset,\n\t\t\t\tupper_32_bits(q->bus_address));\n\t\tWREG32(reg_base + QM_PQ_SIZE_0_OFFSET + pq_offset, ilog2(HL_QUEUE_LENGTH));\n\t\tWREG32(reg_base + QM_PQ_PI_0_OFFSET + pq_offset, 0);\n\t\tWREG32(reg_base + QM_PQ_CI_0_OFFSET + pq_offset, 0);\n\t}\n}\n\nstatic void gaudi2_init_qman_cp(struct hl_device *hdev, u32 reg_base)\n{\n\tu32 cp_id, cp_offset, mtr_base_lo, mtr_base_hi, so_base_lo, so_base_hi;\n\n\tmtr_base_lo = lower_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_hi = upper_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_lo = lower_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_hi = upper_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tfor (cp_id = 0 ; cp_id < NUM_OF_CP_PER_QMAN; cp_id++) {\n\t\tcp_offset = cp_id * 4;\n\n\t\tWREG32(reg_base + QM_CP_MSG_BASE0_ADDR_LO_0_OFFSET + cp_offset, mtr_base_lo);\n\t\tWREG32(reg_base + QM_CP_MSG_BASE0_ADDR_HI_0_OFFSET + cp_offset,\tmtr_base_hi);\n\t\tWREG32(reg_base + QM_CP_MSG_BASE1_ADDR_LO_0_OFFSET + cp_offset,\tso_base_lo);\n\t\tWREG32(reg_base + QM_CP_MSG_BASE1_ADDR_HI_0_OFFSET + cp_offset,\tso_base_hi);\n\t}\n\n\t \n\tWREG32(reg_base + QM_CP_CFG_OFFSET, FIELD_PREP(PDMA0_QM_CP_CFG_SWITCH_EN_MASK, 0x1));\n}\n\nstatic void gaudi2_init_qman_pqc(struct hl_device *hdev, u32 reg_base,\n\t\t\t\tu32 queue_id_base)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 pq_id, pq_offset, so_base_lo, so_base_hi;\n\n\tso_base_lo = lower_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_hi = upper_32_bits(CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tfor (pq_id = 0 ; pq_id < NUM_OF_PQ_PER_QMAN ; pq_id++) {\n\t\tpq_offset = pq_id * 4;\n\n\t\t \n\t\tWREG32(reg_base + QM_PQC_HBW_BASE_LO_0_OFFSET + pq_offset,\n\t\t\t\tlower_32_bits(gaudi2->scratchpad_bus_address));\n\t\tWREG32(reg_base + QM_PQC_HBW_BASE_HI_0_OFFSET + pq_offset,\n\t\t\t\tupper_32_bits(gaudi2->scratchpad_bus_address));\n\t\tWREG32(reg_base + QM_PQC_SIZE_0_OFFSET + pq_offset,\n\t\t\t\tilog2(PAGE_SIZE / sizeof(struct hl_cq_entry)));\n\n\t\tWREG32(reg_base + QM_PQC_PI_0_OFFSET + pq_offset, 0);\n\t\tWREG32(reg_base + QM_PQC_LBW_WDATA_0_OFFSET + pq_offset, QM_PQC_LBW_WDATA);\n\t\tWREG32(reg_base + QM_PQC_LBW_BASE_LO_0_OFFSET + pq_offset, so_base_lo);\n\t\tWREG32(reg_base + QM_PQC_LBW_BASE_HI_0_OFFSET + pq_offset, so_base_hi);\n\t}\n\n\t \n\tWREG32(reg_base + QM_PQC_CFG_OFFSET, 1 << PDMA0_QM_PQC_CFG_EN_SHIFT);\n}\n\nstatic u32 gaudi2_get_dyn_sp_reg(struct hl_device *hdev, u32 queue_id_base)\n{\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 sp_reg_addr;\n\n\tswitch (queue_id_base) {\n\tcase GAUDI2_QUEUE_ID_PDMA_0_0...GAUDI2_QUEUE_ID_PDMA_1_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE0_EDMA_1_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE1_EDMA_1_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE2_EDMA_1_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3:\n\t\tsp_reg_addr = le32_to_cpu(dyn_regs->gic_dma_qm_irq_ctrl);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE0_MME_0_0...GAUDI2_QUEUE_ID_DCORE0_MME_0_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE1_MME_0_0...GAUDI2_QUEUE_ID_DCORE1_MME_0_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE2_MME_0_0...GAUDI2_QUEUE_ID_DCORE2_MME_0_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE3_MME_0_0...GAUDI2_QUEUE_ID_DCORE3_MME_0_3:\n\t\tsp_reg_addr = le32_to_cpu(dyn_regs->gic_mme_qm_irq_ctrl);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE0_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE0_TPC_6_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE1_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE1_TPC_5_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE2_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE2_TPC_5_3:\n\t\tfallthrough;\n\tcase GAUDI2_QUEUE_ID_DCORE3_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE3_TPC_5_3:\n\t\tsp_reg_addr = le32_to_cpu(dyn_regs->gic_tpc_qm_irq_ctrl);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_ROT_0_0...GAUDI2_QUEUE_ID_ROT_1_3:\n\t\tsp_reg_addr = le32_to_cpu(dyn_regs->gic_rot_qm_irq_ctrl);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_NIC_0_0...GAUDI2_QUEUE_ID_NIC_23_3:\n\t\tsp_reg_addr = le32_to_cpu(dyn_regs->gic_nic_qm_irq_ctrl);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hdev->dev, \"Unexpected h/w queue %d\\n\", queue_id_base);\n\t\treturn 0;\n\t}\n\n\treturn sp_reg_addr;\n}\n\nstatic void gaudi2_init_qman_common(struct hl_device *hdev, u32 reg_base,\n\t\t\t\t\tu32 queue_id_base)\n{\n\tu32 glbl_prot = QMAN_MAKE_TRUSTED, irq_handler_offset;\n\tint map_table_entry;\n\n\tWREG32(reg_base + QM_GLBL_PROT_OFFSET, glbl_prot);\n\n\tirq_handler_offset = gaudi2_get_dyn_sp_reg(hdev, queue_id_base);\n\tWREG32(reg_base + QM_GLBL_ERR_ADDR_LO_OFFSET, lower_32_bits(CFG_BASE + irq_handler_offset));\n\tWREG32(reg_base + QM_GLBL_ERR_ADDR_HI_OFFSET, upper_32_bits(CFG_BASE + irq_handler_offset));\n\n\tmap_table_entry = gaudi2_qman_async_event_id[queue_id_base];\n\tWREG32(reg_base + QM_GLBL_ERR_WDATA_OFFSET,\n\t\tgaudi2_irq_map_table[map_table_entry].cpu_id);\n\n\tWREG32(reg_base + QM_ARB_ERR_MSG_EN_OFFSET, QM_ARB_ERR_MSG_EN_MASK);\n\n\tWREG32(reg_base + QM_ARB_SLV_CHOISE_WDT_OFFSET, GAUDI2_ARB_WDT_TIMEOUT);\n\tWREG32(reg_base + QM_GLBL_CFG1_OFFSET, 0);\n\tWREG32(reg_base + QM_GLBL_CFG2_OFFSET, 0);\n\n\t \n\tif (reg_base == gaudi2_qm_blocks_bases[GAUDI2_QUEUE_ID_PDMA_1_0])\n\t\tWREG32(reg_base + QM_GLBL_CFG0_OFFSET, PDMA1_QMAN_ENABLE);\n\telse if (reg_base == gaudi2_qm_blocks_bases[GAUDI2_QUEUE_ID_PDMA_0_0])\n\t\tWREG32(reg_base + QM_GLBL_CFG0_OFFSET, PDMA0_QMAN_ENABLE);\n\telse\n\t\tWREG32(reg_base + QM_GLBL_CFG0_OFFSET, QMAN_ENABLE);\n}\n\nstatic void gaudi2_init_qman(struct hl_device *hdev, u32 reg_base,\n\t\tu32 queue_id_base)\n{\n\tu32 pq_id;\n\n\tfor (pq_id = 0 ; pq_id < NUM_OF_PQ_PER_QMAN ; pq_id++)\n\t\thdev->kernel_queues[queue_id_base + pq_id].cq_id = GAUDI2_RESERVED_CQ_CS_COMPLETION;\n\n\tgaudi2_init_qman_pq(hdev, reg_base, queue_id_base);\n\tgaudi2_init_qman_cp(hdev, reg_base);\n\tgaudi2_init_qman_pqc(hdev, reg_base, queue_id_base);\n\tgaudi2_init_qman_common(hdev, reg_base, queue_id_base);\n}\n\nstatic void gaudi2_init_dma_core(struct hl_device *hdev, u32 reg_base,\n\t\t\t\tu32 dma_core_id, bool is_secure)\n{\n\tu32 prot, irq_handler_offset;\n\tstruct cpu_dyn_regs *dyn_regs;\n\tint map_table_entry;\n\n\tprot = 1 << ARC_FARM_KDMA_PROT_ERR_VAL_SHIFT;\n\tif (is_secure)\n\t\tprot |= 1 << ARC_FARM_KDMA_PROT_VAL_SHIFT;\n\n\tWREG32(reg_base + DMA_CORE_PROT_OFFSET, prot);\n\n\tdyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tirq_handler_offset = le32_to_cpu(dyn_regs->gic_dma_core_irq_ctrl);\n\n\tWREG32(reg_base + DMA_CORE_ERRMSG_ADDR_LO_OFFSET,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\n\tWREG32(reg_base + DMA_CORE_ERRMSG_ADDR_HI_OFFSET,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\tmap_table_entry = gaudi2_dma_core_async_event_id[dma_core_id];\n\tWREG32(reg_base + DMA_CORE_ERRMSG_WDATA_OFFSET,\n\t\tgaudi2_irq_map_table[map_table_entry].cpu_id);\n\n\t \n\tWREG32(reg_base + DMA_CORE_CFG_0_OFFSET, 1 << ARC_FARM_KDMA_CFG_0_EN_SHIFT);\n}\n\nstatic void gaudi2_init_kdma(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\n\tif ((gaudi2->hw_cap_initialized & HW_CAP_KDMA) == HW_CAP_KDMA)\n\t\treturn;\n\n\treg_base = gaudi2_dma_core_blocks_bases[DMA_CORE_ID_KDMA];\n\n\tgaudi2_init_dma_core(hdev, reg_base, DMA_CORE_ID_KDMA, true);\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_KDMA;\n}\n\nstatic void gaudi2_init_pdma(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_base;\n\n\tif ((gaudi2->hw_cap_initialized & HW_CAP_PDMA_MASK) == HW_CAP_PDMA_MASK)\n\t\treturn;\n\n\treg_base = gaudi2_dma_core_blocks_bases[DMA_CORE_ID_PDMA0];\n\tgaudi2_init_dma_core(hdev, reg_base, DMA_CORE_ID_PDMA0, false);\n\n\treg_base = gaudi2_qm_blocks_bases[GAUDI2_QUEUE_ID_PDMA_0_0];\n\tgaudi2_init_qman(hdev, reg_base, GAUDI2_QUEUE_ID_PDMA_0_0);\n\n\treg_base = gaudi2_dma_core_blocks_bases[DMA_CORE_ID_PDMA1];\n\tgaudi2_init_dma_core(hdev, reg_base, DMA_CORE_ID_PDMA1, false);\n\n\treg_base = gaudi2_qm_blocks_bases[GAUDI2_QUEUE_ID_PDMA_1_0];\n\tgaudi2_init_qman(hdev, reg_base, GAUDI2_QUEUE_ID_PDMA_1_0);\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_PDMA_MASK;\n}\n\nstatic void gaudi2_init_edma_instance(struct hl_device *hdev, u8 seq)\n{\n\tu32 reg_base, base_edma_core_id, base_edma_qman_id;\n\n\tbase_edma_core_id = DMA_CORE_ID_EDMA0 + seq;\n\tbase_edma_qman_id = edma_stream_base[seq];\n\n\treg_base = gaudi2_dma_core_blocks_bases[base_edma_core_id];\n\tgaudi2_init_dma_core(hdev, reg_base, base_edma_core_id, false);\n\n\treg_base = gaudi2_qm_blocks_bases[base_edma_qman_id];\n\tgaudi2_init_qman(hdev, reg_base, base_edma_qman_id);\n}\n\nstatic void gaudi2_init_edma(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint dcore, inst;\n\n\tif ((gaudi2->hw_cap_initialized & HW_CAP_EDMA_MASK) == HW_CAP_EDMA_MASK)\n\t\treturn;\n\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (inst = 0 ; inst < NUM_OF_EDMA_PER_DCORE ; inst++) {\n\t\t\tu8 seq = dcore * NUM_OF_EDMA_PER_DCORE + inst;\n\n\t\t\tif (!(prop->edma_enabled_mask & BIT(seq)))\n\t\t\t\tcontinue;\n\n\t\t\tgaudi2_init_edma_instance(hdev, seq);\n\n\t\t\tgaudi2->hw_cap_initialized |= BIT_ULL(HW_CAP_EDMA_SHIFT + seq);\n\t\t}\n\t}\n}\n\n \nstatic void gaudi2_arm_monitors_for_virt_msix_db(struct hl_device *hdev, u32 sob_id,\n\t\t\t\t\t\t\tu32 first_mon_id, u32 interrupt_id)\n{\n\tu32 sob_offset, first_mon_offset, mon_offset, payload, sob_group, mode, arm, config;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 addr;\n\tu8 mask;\n\n\t \n\tsob_offset = sob_id * sizeof(u32);\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset, 0);\n\n\t \n\n\tfirst_mon_offset = first_mon_id * sizeof(u32);\n\n\t \n\tmon_offset = first_mon_offset + sizeof(u32);\n\n\taddr = CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset;\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + mon_offset, lower_32_bits(addr));\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0 + mon_offset, upper_32_bits(addr));\n\n\tpayload = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_VAL_MASK, 0x7FFF) |  \n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_SIGN_MASK, 1) |\n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_INC_MASK, 1);\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + mon_offset, payload);\n\n\t \n\tmon_offset = first_mon_offset + 2 * sizeof(u32);\n\n\taddr = CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_MON_ARM_0 + first_mon_offset;\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + mon_offset, lower_32_bits(addr));\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0 + mon_offset, upper_32_bits(addr));\n\n\tsob_group = sob_id / 8;\n\tmask = ~BIT(sob_id & 0x7);\n\tmode = 0;  \n\tarm = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SID_MASK, sob_group) |\n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_MASK_MASK, mask) |\n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SOP_MASK, mode) |\n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SOD_MASK, 1);\n\n\tpayload = arm;\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + mon_offset, payload);\n\n\t \n\tmon_offset = first_mon_offset;\n\n\tconfig = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_CONFIG_WR_NUM_MASK, 2);  \n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_CONFIG_0 + mon_offset, config);\n\n\taddr = gaudi2->virt_msix_db_dma_addr;\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + mon_offset, lower_32_bits(addr));\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0 + mon_offset, upper_32_bits(addr));\n\n\tpayload = interrupt_id;\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + mon_offset, payload);\n\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_ARM_0 + mon_offset, arm);\n}\n\nstatic void gaudi2_prepare_sm_for_virt_msix_db(struct hl_device *hdev)\n{\n\tu32 decoder_id, sob_id, first_mon_id, interrupt_id;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\t \n\tfor (decoder_id = 0 ; decoder_id < NUMBER_OF_DEC ; ++decoder_id) {\n\t\tif (!(prop->decoder_enabled_mask & BIT(decoder_id)))\n\t\t\tcontinue;\n\n\t\tsob_id = GAUDI2_RESERVED_SOB_DEC_NRM_FIRST + decoder_id;\n\t\tfirst_mon_id = GAUDI2_RESERVED_MON_DEC_NRM_FIRST + 3 * decoder_id;\n\t\tinterrupt_id = GAUDI2_IRQ_NUM_DCORE0_DEC0_NRM + 2 * decoder_id;\n\t\tgaudi2_arm_monitors_for_virt_msix_db(hdev, sob_id, first_mon_id, interrupt_id);\n\n\t\tsob_id = GAUDI2_RESERVED_SOB_DEC_ABNRM_FIRST + decoder_id;\n\t\tfirst_mon_id = GAUDI2_RESERVED_MON_DEC_ABNRM_FIRST + 3 * decoder_id;\n\t\tinterrupt_id += 1;\n\t\tgaudi2_arm_monitors_for_virt_msix_db(hdev, sob_id, first_mon_id, interrupt_id);\n\t}\n}\n\nstatic void gaudi2_init_sm(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 cq_address;\n\tu32 reg_val;\n\tint i;\n\n\t \n\treg_val = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_CONFIG_CQ_EN_MASK, 1);\n\treg_val |= FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_CONFIG_LBW_EN_MASK, 1);\n\n\tfor (i = 0 ; i < GAUDI2_MAX_PENDING_CS ; i++)\n\t\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_CONFIG_0 + (4 * i), reg_val);\n\n\t \n\treg_val = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_CONFIG_CQ_EN_MASK, 1);\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_CONFIG_0 + (4 * i), reg_val);\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_L_0, lower_32_bits(gaudi2->virt_msix_db_dma_addr));\n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_H_0, upper_32_bits(gaudi2->virt_msix_db_dma_addr));\n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_LBW_DATA_0, GAUDI2_IRQ_NUM_COMPLETION);\n\n\tfor (i = 0 ; i < GAUDI2_RESERVED_CQ_NUMBER ; i++) {\n\t\tcq_address =\n\t\t\thdev->completion_queue[i].bus_address;\n\n\t\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_L_0 + (4 * i),\n\t\t\t\t\t\t\tlower_32_bits(cq_address));\n\t\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_H_0 + (4 * i),\n\t\t\t\t\t\t\tupper_32_bits(cq_address));\n\t\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_CQ_SIZE_LOG2_0 + (4 * i),\n\t\t\t\t\t\t\tilog2(HL_CQ_SIZE_IN_BYTES));\n\t}\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_ASID_SEC, 0x10000);\n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_ASID_NONE_SEC_PRIV, 0);\n\n\t \n\tgaudi2_prepare_sm_for_virt_msix_db(hdev);\n}\n\nstatic void gaudi2_init_mme_acc(struct hl_device *hdev, u32 reg_base)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 reg_val;\n\tint i;\n\n\treg_val = FIELD_PREP(MME_ACC_INTR_MASK_WBC_ERR_RESP_MASK, 0);\n\treg_val |= FIELD_PREP(MME_ACC_INTR_MASK_AP_SRC_POS_INF_MASK, 1);\n\treg_val |= FIELD_PREP(MME_ACC_INTR_MASK_AP_SRC_NEG_INF_MASK, 1);\n\treg_val |= FIELD_PREP(MME_ACC_INTR_MASK_AP_SRC_NAN_MASK, 1);\n\treg_val |= FIELD_PREP(MME_ACC_INTR_MASK_AP_RESULT_POS_INF_MASK, 1);\n\treg_val |= FIELD_PREP(MME_ACC_INTR_MASK_AP_RESULT_NEG_INF_MASK, 1);\n\n\tWREG32(reg_base + MME_ACC_INTR_MASK_OFFSET, reg_val);\n\tWREG32(reg_base + MME_ACC_AP_LFSR_POLY_OFFSET, 0x80DEADAF);\n\n\tfor (i = 0 ; i < MME_NUM_OF_LFSR_SEEDS ; i++) {\n\t\tWREG32(reg_base + MME_ACC_AP_LFSR_SEED_SEL_OFFSET, i);\n\t\tWREG32(reg_base + MME_ACC_AP_LFSR_SEED_WDATA_OFFSET, gaudi2->lfsr_rand_seeds[i]);\n\t}\n}\n\nstatic void gaudi2_init_dcore_mme(struct hl_device *hdev, int dcore_id,\n\t\t\t\t\t\t\tbool config_qman_only)\n{\n\tu32 queue_id_base, reg_base;\n\n\tswitch (dcore_id) {\n\tcase 0:\n\t\tqueue_id_base = GAUDI2_QUEUE_ID_DCORE0_MME_0_0;\n\t\tbreak;\n\tcase 1:\n\t\tqueue_id_base = GAUDI2_QUEUE_ID_DCORE1_MME_0_0;\n\t\tbreak;\n\tcase 2:\n\t\tqueue_id_base = GAUDI2_QUEUE_ID_DCORE2_MME_0_0;\n\t\tbreak;\n\tcase 3:\n\t\tqueue_id_base = GAUDI2_QUEUE_ID_DCORE3_MME_0_0;\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hdev->dev, \"Invalid dcore id %u\\n\", dcore_id);\n\t\treturn;\n\t}\n\n\tif (!config_qman_only) {\n\t\treg_base = gaudi2_mme_acc_blocks_bases[dcore_id];\n\t\tgaudi2_init_mme_acc(hdev, reg_base);\n\t}\n\n\treg_base = gaudi2_qm_blocks_bases[queue_id_base];\n\tgaudi2_init_qman(hdev, reg_base, queue_id_base);\n}\n\nstatic void gaudi2_init_mme(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint i;\n\n\tif ((gaudi2->hw_cap_initialized & HW_CAP_MME_MASK) == HW_CAP_MME_MASK)\n\t\treturn;\n\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++) {\n\t\tgaudi2_init_dcore_mme(hdev, i, false);\n\n\t\tgaudi2->hw_cap_initialized |= BIT_ULL(HW_CAP_MME_SHIFT + i);\n\t}\n}\n\nstatic void gaudi2_init_tpc_cfg(struct hl_device *hdev, u32 reg_base)\n{\n\t \n\tWREG32(reg_base + TPC_CFG_TPC_INTR_MASK_OFFSET, 0x23FFFE);\n\n\t \n\tWREG32(reg_base + TPC_CFG_MSS_CONFIG_OFFSET,\n\t\t\t2 << DCORE0_TPC0_CFG_MSS_CONFIG_ICACHE_FETCH_LINE_NUM_SHIFT);\n}\n\nstruct gaudi2_tpc_init_cfg_data {\n\tenum gaudi2_queue_id dcore_tpc_qid_base[NUM_OF_DCORES];\n};\n\nstatic void gaudi2_init_tpc_config(struct hl_device *hdev, int dcore, int inst,\n\t\t\t\t\tu32 offset, struct iterate_module_ctx *ctx)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_tpc_init_cfg_data *cfg_data = ctx->data;\n\tu32 queue_id_base;\n\tu8 seq;\n\n\tqueue_id_base = cfg_data->dcore_tpc_qid_base[dcore] + (inst * NUM_OF_PQ_PER_QMAN);\n\n\tif (dcore == 0 && inst == (NUM_DCORE0_TPC - 1))\n\t\t \n\t\tseq = NUM_OF_DCORES * NUM_OF_TPC_PER_DCORE;\n\telse\n\t\tseq = dcore * NUM_OF_TPC_PER_DCORE + inst;\n\n\tgaudi2_init_tpc_cfg(hdev, mmDCORE0_TPC0_CFG_BASE + offset);\n\tgaudi2_init_qman(hdev, mmDCORE0_TPC0_QM_BASE + offset, queue_id_base);\n\n\tgaudi2->tpc_hw_cap_initialized |= BIT_ULL(HW_CAP_TPC_SHIFT + seq);\n}\n\nstatic void gaudi2_init_tpc(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_tpc_init_cfg_data init_cfg_data;\n\tstruct iterate_module_ctx tpc_iter;\n\n\tif (!hdev->asic_prop.tpc_enabled_mask)\n\t\treturn;\n\n\tif ((gaudi2->tpc_hw_cap_initialized & HW_CAP_TPC_MASK) == HW_CAP_TPC_MASK)\n\t\treturn;\n\n\tinit_cfg_data.dcore_tpc_qid_base[0] = GAUDI2_QUEUE_ID_DCORE0_TPC_0_0;\n\tinit_cfg_data.dcore_tpc_qid_base[1] = GAUDI2_QUEUE_ID_DCORE1_TPC_0_0;\n\tinit_cfg_data.dcore_tpc_qid_base[2] = GAUDI2_QUEUE_ID_DCORE2_TPC_0_0;\n\tinit_cfg_data.dcore_tpc_qid_base[3] = GAUDI2_QUEUE_ID_DCORE3_TPC_0_0;\n\ttpc_iter.fn = &gaudi2_init_tpc_config;\n\ttpc_iter.data = &init_cfg_data;\n\tgaudi2_iterate_tpcs(hdev, &tpc_iter);\n}\n\nstatic void gaudi2_init_rotator(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 i, reg_base, queue_id;\n\n\tqueue_id = GAUDI2_QUEUE_ID_ROT_0_0;\n\n\tfor (i = 0 ; i < NUM_OF_ROT ; i++, queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\treg_base = gaudi2_qm_blocks_bases[queue_id];\n\t\tgaudi2_init_qman(hdev, reg_base, queue_id);\n\n\t\tgaudi2->hw_cap_initialized |= BIT_ULL(HW_CAP_ROT_SHIFT + i);\n\t}\n}\n\nstatic void gaudi2_init_vdec_brdg_ctrl(struct hl_device *hdev, u64 base_addr, u32 decoder_id)\n{\n\tu32 sob_id;\n\n\t \n\tsob_id = GAUDI2_RESERVED_SOB_DEC_NRM_FIRST + decoder_id;\n\tWREG32(base_addr + BRDG_CTRL_NRM_MSIX_LBW_AWADDR,\n\t\t\tmmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_id * sizeof(u32));\n\tWREG32(base_addr + BRDG_CTRL_NRM_MSIX_LBW_WDATA, GAUDI2_SOB_INCREMENT_BY_ONE);\n\n\t \n\tsob_id = GAUDI2_RESERVED_SOB_DEC_ABNRM_FIRST + decoder_id;\n\tWREG32(base_addr + BRDG_CTRL_ABNRM_MSIX_LBW_AWADDR,\n\t\t\tmmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_id * sizeof(u32));\n\tWREG32(base_addr + BRDG_CTRL_ABNRM_MSIX_LBW_WDATA, GAUDI2_SOB_INCREMENT_BY_ONE);\n}\n\nstatic void gaudi2_init_dec(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 dcore_id, dec_id, dec_bit;\n\tu64 base_addr;\n\n\tif (!hdev->asic_prop.decoder_enabled_mask)\n\t\treturn;\n\n\tif ((gaudi2->dec_hw_cap_initialized & HW_CAP_DEC_MASK) == HW_CAP_DEC_MASK)\n\t\treturn;\n\n\tfor (dcore_id = 0 ; dcore_id < NUM_OF_DCORES ; dcore_id++)\n\t\tfor (dec_id = 0 ; dec_id < NUM_OF_DEC_PER_DCORE ; dec_id++) {\n\t\t\tdec_bit = dcore_id * NUM_OF_DEC_PER_DCORE + dec_id;\n\n\t\t\tif (!(hdev->asic_prop.decoder_enabled_mask & BIT(dec_bit)))\n\t\t\t\tcontinue;\n\n\t\t\tbase_addr =  mmDCORE0_DEC0_CMD_BASE +\n\t\t\t\t\tBRDG_CTRL_BLOCK_OFFSET +\n\t\t\t\t\tdcore_id * DCORE_OFFSET +\n\t\t\t\t\tdec_id * DCORE_VDEC_OFFSET;\n\n\t\t\tgaudi2_init_vdec_brdg_ctrl(hdev, base_addr, dec_bit);\n\n\t\t\tgaudi2->dec_hw_cap_initialized |= BIT_ULL(HW_CAP_DEC_SHIFT + dec_bit);\n\t\t}\n\n\tfor (dec_id = 0 ; dec_id < NUM_OF_PCIE_VDEC ; dec_id++) {\n\t\tdec_bit = PCIE_DEC_SHIFT + dec_id;\n\t\tif (!(hdev->asic_prop.decoder_enabled_mask & BIT(dec_bit)))\n\t\t\tcontinue;\n\n\t\tbase_addr = mmPCIE_DEC0_CMD_BASE + BRDG_CTRL_BLOCK_OFFSET +\n\t\t\t\tdec_id * DCORE_VDEC_OFFSET;\n\n\t\tgaudi2_init_vdec_brdg_ctrl(hdev, base_addr, dec_bit);\n\n\t\tgaudi2->dec_hw_cap_initialized |= BIT_ULL(HW_CAP_DEC_SHIFT + dec_bit);\n\t}\n}\n\nstatic int gaudi2_mmu_update_asid_hop0_addr(struct hl_device *hdev,\n\t\t\t\t\tu32 stlb_base, u32 asid, u64 phys_addr)\n{\n\tu32 status, timeout_usec;\n\tint rc;\n\n\tif (hdev->pldm || !hdev->pdev)\n\t\ttimeout_usec = GAUDI2_PLDM_MMU_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = MMU_CONFIG_TIMEOUT_USEC;\n\n\tWREG32(stlb_base + STLB_ASID_OFFSET, asid);\n\tWREG32(stlb_base + STLB_HOP0_PA43_12_OFFSET, phys_addr >> MMU_HOP0_PA43_12_SHIFT);\n\tWREG32(stlb_base + STLB_HOP0_PA63_44_OFFSET, phys_addr >> MMU_HOP0_PA63_44_SHIFT);\n\tWREG32(stlb_base + STLB_BUSY_OFFSET, 0x80000000);\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tstlb_base + STLB_BUSY_OFFSET,\n\t\tstatus,\n\t\t!(status & 0x80000000),\n\t\t1000,\n\t\ttimeout_usec);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Timeout during MMU hop0 config of asid %d\\n\", asid);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void gaudi2_mmu_send_invalidate_cache_cmd(struct hl_device *hdev, u32 stlb_base,\n\t\t\t\t\tu32 start_offset, u32 inv_start_val,\n\t\t\t\t\tu32 flags)\n{\n\t \n\tif (flags & MMU_OP_CLEAR_MEMCACHE)\n\t\tWREG32(mmPMMU_HBW_STLB_MEM_CACHE_INVALIDATION, 0x1);\n\n\tif (flags & MMU_OP_SKIP_LOW_CACHE_INV)\n\t\treturn;\n\n\tWREG32(stlb_base + start_offset, inv_start_val);\n}\n\nstatic int gaudi2_mmu_invalidate_cache_status_poll(struct hl_device *hdev, u32 stlb_base,\n\t\t\t\t\t\tstruct gaudi2_cache_invld_params *inv_params)\n{\n\tu32 status, timeout_usec, start_offset;\n\tint rc;\n\n\ttimeout_usec = (hdev->pldm) ? GAUDI2_PLDM_MMU_TIMEOUT_USEC :\n\t\t\t\t\tGAUDI2_MMU_CACHE_INV_TIMEOUT_USEC;\n\n\t \n\tif (inv_params->flags & MMU_OP_CLEAR_MEMCACHE) {\n\t\trc = hl_poll_timeout(\n\t\t\thdev,\n\t\t\tmmPMMU_HBW_STLB_MEM_CACHE_INV_STATUS,\n\t\t\tstatus,\n\t\t\tstatus & 0x1,\n\t\t\t1000,\n\t\t\ttimeout_usec);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\t \n\t\tWREG32(mmPMMU_HBW_STLB_MEM_CACHE_INV_STATUS, 0x0);\n\t}\n\n\t \n\tif (inv_params->flags & MMU_OP_SKIP_LOW_CACHE_INV)\n\t\treturn 0;\n\n\tstart_offset = inv_params->range_invalidation ?\n\t\t\tSTLB_RANGE_CACHE_INVALIDATION_OFFSET : STLB_INV_ALL_START_OFFSET;\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tstlb_base + start_offset,\n\t\tstatus,\n\t\t!(status & 0x1),\n\t\t1000,\n\t\ttimeout_usec);\n\n\treturn rc;\n}\n\nbool gaudi2_is_hmmu_enabled(struct hl_device *hdev, int dcore_id, int hmmu_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 hw_cap;\n\n\thw_cap = HW_CAP_DCORE0_DMMU0 << (NUM_OF_HMMU_PER_DCORE * dcore_id + hmmu_id);\n\n\tif (gaudi2->hw_cap_initialized & hw_cap)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic inline u32 get_hmmu_stlb_base(int dcore_id, int hmmu_id)\n{\n\tu32 offset;\n\n\toffset =  (u32) (dcore_id * DCORE_OFFSET + hmmu_id * DCORE_HMMU_OFFSET);\n\treturn (u32)(mmDCORE0_HMMU0_STLB_BASE + offset);\n}\n\nstatic void gaudi2_mmu_invalidate_cache_trigger(struct hl_device *hdev, u32 stlb_base,\n\t\t\t\t\t\tstruct gaudi2_cache_invld_params *inv_params)\n{\n\tu32 start_offset;\n\n\tif (inv_params->range_invalidation) {\n\t\t \n\t\tu64 start = inv_params->start_va - 1;\n\n\t\tstart_offset = STLB_RANGE_CACHE_INVALIDATION_OFFSET;\n\n\t\tWREG32(stlb_base + STLB_RANGE_INV_START_LSB_OFFSET,\n\t\t\t\tstart >> MMU_RANGE_INV_VA_LSB_SHIFT);\n\n\t\tWREG32(stlb_base + STLB_RANGE_INV_START_MSB_OFFSET,\n\t\t\t\tstart >> MMU_RANGE_INV_VA_MSB_SHIFT);\n\n\t\tWREG32(stlb_base + STLB_RANGE_INV_END_LSB_OFFSET,\n\t\t\t\tinv_params->end_va >> MMU_RANGE_INV_VA_LSB_SHIFT);\n\n\t\tWREG32(stlb_base + STLB_RANGE_INV_END_MSB_OFFSET,\n\t\t\t\tinv_params->end_va >> MMU_RANGE_INV_VA_MSB_SHIFT);\n\t} else {\n\t\tstart_offset = STLB_INV_ALL_START_OFFSET;\n\t}\n\n\tgaudi2_mmu_send_invalidate_cache_cmd(hdev, stlb_base, start_offset,\n\t\t\t\t\t\tinv_params->inv_start_val, inv_params->flags);\n}\n\nstatic inline void gaudi2_hmmu_invalidate_cache_trigger(struct hl_device *hdev,\n\t\t\t\t\t\tint dcore_id, int hmmu_id,\n\t\t\t\t\t\tstruct gaudi2_cache_invld_params *inv_params)\n{\n\tu32 stlb_base = get_hmmu_stlb_base(dcore_id, hmmu_id);\n\n\tgaudi2_mmu_invalidate_cache_trigger(hdev, stlb_base, inv_params);\n}\n\nstatic inline int gaudi2_hmmu_invalidate_cache_status_poll(struct hl_device *hdev,\n\t\t\t\t\t\tint dcore_id, int hmmu_id,\n\t\t\t\t\t\tstruct gaudi2_cache_invld_params *inv_params)\n{\n\tu32 stlb_base = get_hmmu_stlb_base(dcore_id, hmmu_id);\n\n\treturn gaudi2_mmu_invalidate_cache_status_poll(hdev, stlb_base, inv_params);\n}\n\nstatic int gaudi2_hmmus_invalidate_cache(struct hl_device *hdev,\n\t\t\t\t\t\tstruct gaudi2_cache_invld_params *inv_params)\n{\n\tint dcore_id, hmmu_id;\n\n\t \n\tfor (dcore_id = 0 ; dcore_id < NUM_OF_DCORES ; dcore_id++) {\n\t\tfor (hmmu_id = 0 ; hmmu_id < NUM_OF_HMMU_PER_DCORE ; hmmu_id++) {\n\t\t\tif (!gaudi2_is_hmmu_enabled(hdev, dcore_id, hmmu_id))\n\t\t\t\tcontinue;\n\n\t\t\tgaudi2_hmmu_invalidate_cache_trigger(hdev, dcore_id, hmmu_id, inv_params);\n\t\t}\n\t}\n\n\t \n\tfor (dcore_id = 0 ; dcore_id < NUM_OF_DCORES ; dcore_id++) {\n\t\tfor (hmmu_id = 0 ; hmmu_id < NUM_OF_HMMU_PER_DCORE ; hmmu_id++) {\n\t\t\tint rc;\n\n\t\t\tif (!gaudi2_is_hmmu_enabled(hdev, dcore_id, hmmu_id))\n\t\t\t\tcontinue;\n\n\t\t\trc = gaudi2_hmmu_invalidate_cache_status_poll(hdev, dcore_id, hmmu_id,\n\t\t\t\t\t\t\t\t\t\tinv_params);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_mmu_invalidate_cache(struct hl_device *hdev, bool is_hard, u32 flags)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_cache_invld_params invld_params;\n\tint rc = 0;\n\n\tif (hdev->reset_info.hard_reset_pending)\n\t\treturn rc;\n\n\tinvld_params.range_invalidation = false;\n\tinvld_params.inv_start_val = 1;\n\n\tif ((flags & MMU_OP_USERPTR) && (gaudi2->hw_cap_initialized & HW_CAP_PMMU)) {\n\t\tinvld_params.flags = flags;\n\t\tgaudi2_mmu_invalidate_cache_trigger(hdev, mmPMMU_HBW_STLB_BASE, &invld_params);\n\t\trc = gaudi2_mmu_invalidate_cache_status_poll(hdev, mmPMMU_HBW_STLB_BASE,\n\t\t\t\t\t\t\t\t\t\t&invld_params);\n\t} else if (flags & MMU_OP_PHYS_PACK) {\n\t\tinvld_params.flags = 0;\n\t\trc = gaudi2_hmmus_invalidate_cache(hdev, &invld_params);\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi2_mmu_invalidate_cache_range(struct hl_device *hdev, bool is_hard,\n\t\t\t\tu32 flags, u32 asid, u64 va, u64 size)\n{\n\tstruct gaudi2_cache_invld_params invld_params = {0};\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 start_va, end_va;\n\tu32 inv_start_val;\n\tint rc = 0;\n\n\tif (hdev->reset_info.hard_reset_pending)\n\t\treturn 0;\n\n\tinv_start_val = (1 << MMU_RANGE_INV_EN_SHIFT |\n\t\t\t1 << MMU_RANGE_INV_ASID_EN_SHIFT |\n\t\t\tasid << MMU_RANGE_INV_ASID_SHIFT);\n\tstart_va = va;\n\tend_va = start_va + size;\n\n\tif ((flags & MMU_OP_USERPTR) && (gaudi2->hw_cap_initialized & HW_CAP_PMMU)) {\n\t\t \n\t\tif (start_va) {\n\t\t\tinvld_params.range_invalidation = true;\n\t\t\tinvld_params.start_va = start_va;\n\t\t\tinvld_params.end_va = end_va;\n\t\t\tinvld_params.inv_start_val = inv_start_val;\n\t\t\tinvld_params.flags = flags | MMU_OP_CLEAR_MEMCACHE;\n\t\t} else {\n\t\t\tinvld_params.range_invalidation = false;\n\t\t\tinvld_params.inv_start_val = 1;\n\t\t\tinvld_params.flags = flags;\n\t\t}\n\n\n\t\tgaudi2_mmu_invalidate_cache_trigger(hdev, mmPMMU_HBW_STLB_BASE, &invld_params);\n\t\trc = gaudi2_mmu_invalidate_cache_status_poll(hdev, mmPMMU_HBW_STLB_BASE,\n\t\t\t\t\t\t\t\t\t\t&invld_params);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t} else if (flags & MMU_OP_PHYS_PACK) {\n\t\tinvld_params.start_va = gaudi2_mmu_scramble_addr(hdev, start_va);\n\t\tinvld_params.end_va = gaudi2_mmu_scramble_addr(hdev, end_va);\n\t\tinvld_params.inv_start_val = inv_start_val;\n\t\tinvld_params.flags = flags;\n\t\trc = gaudi2_hmmus_invalidate_cache(hdev, &invld_params);\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi2_mmu_update_hop0_addr(struct hl_device *hdev, u32 stlb_base)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 hop0_addr;\n\tu32 asid, max_asid = prop->max_asid;\n\tint rc;\n\n\t \n\tif (hdev->pldm)\n\t\tmax_asid = min((u32) 8, max_asid);\n\n\tfor (asid = 0 ; asid < max_asid ; asid++) {\n\t\thop0_addr = hdev->mmu_priv.hr.mmu_asid_hop0[asid].phys_addr;\n\t\trc = gaudi2_mmu_update_asid_hop0_addr(hdev, stlb_base, asid, hop0_addr);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"failed to set hop0 addr for asid %d\\n\", asid);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_mmu_init_common(struct hl_device *hdev, u32 mmu_base, u32 stlb_base)\n{\n\tu32 status, timeout_usec;\n\tint rc;\n\n\tif (hdev->pldm || !hdev->pdev)\n\t\ttimeout_usec = GAUDI2_PLDM_MMU_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = GAUDI2_MMU_CACHE_INV_TIMEOUT_USEC;\n\n\tWREG32(stlb_base + STLB_INV_ALL_START_OFFSET, 1);\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tstlb_base + STLB_SRAM_INIT_OFFSET,\n\t\tstatus,\n\t\t!status,\n\t\t1000,\n\t\ttimeout_usec);\n\n\tif (rc)\n\t\tdev_notice_ratelimited(hdev->dev, \"Timeout when waiting for MMU SRAM init\\n\");\n\n\trc = gaudi2_mmu_update_hop0_addr(hdev, stlb_base);\n\tif (rc)\n\t\treturn rc;\n\n\tWREG32(mmu_base + MMU_BYPASS_OFFSET, 0);\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tstlb_base + STLB_INV_ALL_START_OFFSET,\n\t\tstatus,\n\t\t!status,\n\t\t1000,\n\t\ttimeout_usec);\n\n\tif (rc)\n\t\tdev_notice_ratelimited(hdev->dev, \"Timeout when waiting for MMU invalidate all\\n\");\n\n\tWREG32(mmu_base + MMU_ENABLE_OFFSET, 1);\n\n\treturn rc;\n}\n\nstatic int gaudi2_pci_mmu_init(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 mmu_base, stlb_base;\n\tint rc;\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_PMMU)\n\t\treturn 0;\n\n\tmmu_base = mmPMMU_HBW_MMU_BASE;\n\tstlb_base = mmPMMU_HBW_STLB_BASE;\n\n\tRMWREG32_SHIFTED(stlb_base + STLB_HOP_CONFIGURATION_OFFSET,\n\t\t(0 << PMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_HOP_SHIFT) |\n\t\t(5 << PMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_SMALL_P_SHIFT) |\n\t\t(4 << PMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_LARGE_P_SHIFT) |\n\t\t(5 << PMMU_HBW_STLB_HOP_CONFIGURATION_LAST_HOP_SHIFT) |\n\t\t(5 << PMMU_HBW_STLB_HOP_CONFIGURATION_FOLLOWER_HOP_SHIFT),\n\t\tPMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_HOP_MASK |\n\t\tPMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_SMALL_P_MASK |\n\t\tPMMU_HBW_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_LARGE_P_MASK |\n\t\tPMMU_HBW_STLB_HOP_CONFIGURATION_LAST_HOP_MASK |\n\t\tPMMU_HBW_STLB_HOP_CONFIGURATION_FOLLOWER_HOP_MASK);\n\n\tWREG32(stlb_base + STLB_LL_LOOKUP_MASK_63_32_OFFSET, 0);\n\n\tif (PAGE_SIZE == SZ_64K) {\n\t\t \n\t\tRMWREG32_SHIFTED(mmu_base + MMU_STATIC_MULTI_PAGE_SIZE_OFFSET,\n\t\t\tFIELD_PREP(DCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_HOP5_PAGE_SIZE_MASK, 4) |\n\t\t\tFIELD_PREP(DCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_HOP4_PAGE_SIZE_MASK, 3) |\n\t\t\tFIELD_PREP(\n\t\t\t\tDCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_CFG_8_BITS_HOP_MODE_EN_MASK,\n\t\t\t\t1),\n\t\t\tDCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_HOP5_PAGE_SIZE_MASK |\n\t\t\tDCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_HOP4_PAGE_SIZE_MASK |\n\t\t\tDCORE0_HMMU0_MMU_STATIC_MULTI_PAGE_SIZE_CFG_8_BITS_HOP_MODE_EN_MASK);\n\t}\n\n\tWREG32(mmu_base + MMU_SPI_SEI_MASK_OFFSET, GAUDI2_PMMU_SPI_SEI_ENABLE_MASK);\n\n\trc = gaudi2_mmu_init_common(hdev, mmu_base, stlb_base);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi2->hw_cap_initialized |= HW_CAP_PMMU;\n\n\treturn 0;\n}\n\nstatic int gaudi2_dcore_hmmu_init(struct hl_device *hdev, int dcore_id,\n\t\t\t\tint hmmu_id)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 offset, mmu_base, stlb_base, hw_cap;\n\tu8 dmmu_seq;\n\tint rc;\n\n\tdmmu_seq = NUM_OF_HMMU_PER_DCORE * dcore_id + hmmu_id;\n\thw_cap = HW_CAP_DCORE0_DMMU0 << dmmu_seq;\n\n\t \n\tif ((gaudi2->hw_cap_initialized & hw_cap) || !(prop->hmmu_hif_enabled_mask & BIT(dmmu_seq)))\n\t\treturn 0;\n\n\toffset = (u32) (dcore_id * DCORE_OFFSET + hmmu_id * DCORE_HMMU_OFFSET);\n\tmmu_base = mmDCORE0_HMMU0_MMU_BASE + offset;\n\tstlb_base = mmDCORE0_HMMU0_STLB_BASE + offset;\n\n\tRMWREG32(mmu_base + MMU_STATIC_MULTI_PAGE_SIZE_OFFSET, 5  ,\n\t\t\tMMU_STATIC_MULTI_PAGE_SIZE_HOP4_PAGE_SIZE_MASK);\n\n\tRMWREG32_SHIFTED(stlb_base + STLB_HOP_CONFIGURATION_OFFSET,\n\t\tFIELD_PREP(DCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_HOP_MASK, 0) |\n\t\tFIELD_PREP(DCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_SMALL_P_MASK, 3) |\n\t\tFIELD_PREP(DCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_LARGE_P_MASK, 3) |\n\t\tFIELD_PREP(DCORE0_HMMU0_STLB_HOP_CONFIGURATION_LAST_HOP_MASK, 3) |\n\t\tFIELD_PREP(DCORE0_HMMU0_STLB_HOP_CONFIGURATION_FOLLOWER_HOP_MASK, 3),\n\t\t\tDCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_HOP_MASK |\n\t\t\tDCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_SMALL_P_MASK |\n\t\t\tDCORE0_HMMU0_STLB_HOP_CONFIGURATION_FIRST_LOOKUP_HOP_LARGE_P_MASK |\n\t\t\tDCORE0_HMMU0_STLB_HOP_CONFIGURATION_LAST_HOP_MASK |\n\t\t\tDCORE0_HMMU0_STLB_HOP_CONFIGURATION_FOLLOWER_HOP_MASK);\n\n\tRMWREG32(stlb_base + STLB_HOP_CONFIGURATION_OFFSET, 1,\n\t\t\tSTLB_HOP_CONFIGURATION_ONLY_LARGE_PAGE_MASK);\n\n\tWREG32(mmu_base + MMU_SPI_SEI_MASK_OFFSET, GAUDI2_HMMU_SPI_SEI_ENABLE_MASK);\n\n\trc = gaudi2_mmu_init_common(hdev, mmu_base, stlb_base);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi2->hw_cap_initialized |= hw_cap;\n\n\treturn 0;\n}\n\nstatic int gaudi2_hbm_mmu_init(struct hl_device *hdev)\n{\n\tint rc, dcore_id, hmmu_id;\n\n\tfor (dcore_id = 0 ; dcore_id < NUM_OF_DCORES ; dcore_id++)\n\t\tfor (hmmu_id = 0 ; hmmu_id < NUM_OF_HMMU_PER_DCORE; hmmu_id++) {\n\t\t\trc = gaudi2_dcore_hmmu_init(hdev, dcore_id, hmmu_id);\n\t\t\tif (rc)\n\t\t\t\treturn rc;\n\t\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_mmu_init(struct hl_device *hdev)\n{\n\tint rc;\n\n\trc = gaudi2_pci_mmu_init(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi2_hbm_mmu_init(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic int gaudi2_hw_init(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint rc;\n\n\t \n\tWREG32(mmHW_STATE, HL_DEVICE_HW_STATE_DIRTY);\n\n\t \n\tRREG32(mmHW_STATE);\n\n\t \n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\tgaudi2->dram_bar_cur_addr = DRAM_PHYS_BASE;\n\n\t \n\tif (gaudi2_set_hbm_bar_base(hdev, DRAM_PHYS_BASE) == U64_MAX) {\n\t\tdev_err(hdev->dev, \"failed to map HBM bar to DRAM base address\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = gaudi2_init_cpu(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to initialize CPU\\n\");\n\t\treturn rc;\n\t}\n\n\tgaudi2_init_scrambler_hbm(hdev);\n\tgaudi2_init_kdma(hdev);\n\n\trc = gaudi2_init_cpu_queues(hdev, GAUDI2_CPU_TIMEOUT_USEC);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to initialize CPU H/W queues %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\trc = gaudi2->cpucp_info_get(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to get cpucp info\\n\");\n\t\treturn rc;\n\t}\n\n\trc = gaudi2_mmu_init(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi2_init_pdma(hdev);\n\tgaudi2_init_edma(hdev);\n\tgaudi2_init_sm(hdev);\n\tgaudi2_init_tpc(hdev);\n\tgaudi2_init_mme(hdev);\n\tgaudi2_init_rotator(hdev);\n\tgaudi2_init_dec(hdev);\n\tgaudi2_enable_timestamp(hdev);\n\n\trc = gaudi2_coresight_init(hdev);\n\tif (rc)\n\t\tgoto disable_queues;\n\n\trc = gaudi2_enable_msix(hdev);\n\tif (rc)\n\t\tgoto disable_queues;\n\n\t \n\tRREG32(mmHW_STATE);\n\n\treturn 0;\n\ndisable_queues:\n\tgaudi2_disable_dma_qmans(hdev);\n\tgaudi2_disable_mme_qmans(hdev);\n\tgaudi2_disable_tpc_qmans(hdev);\n\tgaudi2_disable_rot_qmans(hdev);\n\tgaudi2_disable_nic_qmans(hdev);\n\n\tgaudi2_disable_timestamp(hdev);\n\n\treturn rc;\n}\n\n \nstatic void gaudi2_send_hard_reset_cmd(struct hl_device *hdev)\n{\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tbool heartbeat_reset, preboot_only, cpu_initialized = false;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 cpu_boot_status;\n\n\tpreboot_only = (hdev->fw_loader.fw_comp_loaded == FW_TYPE_PREBOOT_CPU);\n\theartbeat_reset = (hdev->reset_info.curr_reset_cause == HL_RESET_CAUSE_HEARTBEAT);\n\n\t \n\tcpu_boot_status = RREG32(mmPSOC_GLOBAL_CONF_CPU_BOOT_STATUS);\n\n\tif (gaudi2 && (gaudi2->hw_cap_initialized & HW_CAP_CPU) &&\n\t\t\t(cpu_boot_status == CPU_BOOT_STATUS_SRAM_AVAIL))\n\t\tcpu_initialized = true;\n\n\t \n\tif (!preboot_only && cpu_initialized) {\n\t\tWREG32(le32_to_cpu(dyn_regs->gic_host_halt_irq),\n\t\t\tgaudi2_irq_map_table[GAUDI2_EVENT_CPU_HALT_MACHINE].cpu_id);\n\n\t\tmsleep(GAUDI2_CPU_RESET_WAIT_MSEC);\n\t}\n\n\t \n\n\tif (heartbeat_reset || preboot_only || !cpu_initialized) {\n\t\tif (hdev->asic_prop.hard_reset_done_by_fw)\n\t\t\thl_fw_ask_hard_reset_without_linux(hdev);\n\t\telse\n\t\t\thl_fw_ask_halt_machine_without_linux(hdev);\n\t}\n}\n\n \nstatic void gaudi2_execute_hard_reset(struct hl_device *hdev)\n{\n\tif (hdev->asic_prop.hard_reset_done_by_fw) {\n\t\tgaudi2_send_hard_reset_cmd(hdev);\n\t\treturn;\n\t}\n\n\t \n\tWREG32(mmPCIE_AUX_FLR_CTRL,\n\t\t\t(PCIE_AUX_FLR_CTRL_HW_CTRL_MASK | PCIE_AUX_FLR_CTRL_INT_MASK_MASK));\n\n\tgaudi2_send_hard_reset_cmd(hdev);\n\n\tWREG32(mmPSOC_RESET_CONF_SW_ALL_RST, 1);\n}\n\nstatic int gaudi2_get_soft_rst_done_indication(struct hl_device *hdev, u32 poll_timeout_us)\n{\n\tint i, rc = 0;\n\tu32 reg_val;\n\n\tfor (i = 0 ; i < GAUDI2_RESET_POLL_CNT ; i++)\n\t\trc = hl_poll_timeout(\n\t\t\thdev,\n\t\t\tmmCPU_RST_STATUS_TO_HOST,\n\t\t\treg_val,\n\t\t\treg_val == CPU_RST_STATUS_SOFT_RST_DONE,\n\t\t\t1000,\n\t\t\tpoll_timeout_us);\n\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Timeout while waiting for FW to complete soft reset (0x%x)\\n\",\n\t\t\t\treg_val);\n\treturn rc;\n}\n\n \nstatic int gaudi2_execute_soft_reset(struct hl_device *hdev, bool driver_performs_reset,\n\t\t\t\t\t\tu32 poll_timeout_us)\n{\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tint rc = 0;\n\n\tif (!driver_performs_reset) {\n\t\tif (hl_is_fw_sw_ver_below(hdev, 1, 10)) {\n\t\t\t \n\t\t\tif (dyn_regs->cpu_rst_status)\n\t\t\t\tWREG32(le32_to_cpu(dyn_regs->cpu_rst_status), CPU_RST_STATUS_NA);\n\t\t\telse\n\t\t\t\tWREG32(mmCPU_RST_STATUS_TO_HOST, CPU_RST_STATUS_NA);\n\t\t\tWREG32(le32_to_cpu(dyn_regs->gic_host_soft_rst_irq),\n\t\t\t\tgaudi2_irq_map_table[GAUDI2_EVENT_CPU_SOFT_RESET].cpu_id);\n\n\t\t\t \n\t\t\trc = gaudi2_get_soft_rst_done_indication(hdev, poll_timeout_us);\n\t\t} else {\n\t\t\trc = hl_fw_send_soft_reset(hdev);\n\t\t}\n\t\treturn rc;\n\t}\n\n\t \n\tgaudi2_write_rr_to_all_lbw_rtrs(hdev, RR_TYPE_LONG, NUM_LONG_LBW_RR - 1,\n\t\t\t\t\tmmDCORE0_TPC0_QM_DCCM_BASE, mmPCIE_MSIX_BASE);\n\n\tgaudi2_write_rr_to_all_lbw_rtrs(hdev, RR_TYPE_LONG, NUM_LONG_LBW_RR - 2,\n\t\t\t\tmmPCIE_MSIX_BASE + HL_BLOCK_SIZE,\n\t\t\t\tmmPCIE_VDEC1_MSTR_IF_RR_SHRD_HBW_BASE + HL_BLOCK_SIZE);\n\n\tWREG32(mmPSOC_RESET_CONF_SOFT_RST, 1);\n\treturn 0;\n}\n\nstatic void gaudi2_poll_btm_indication(struct hl_device *hdev, u32 poll_timeout_us)\n{\n\tint i, rc = 0;\n\tu32 reg_val;\n\n\t \n\tfor (i = 0 ; i < GAUDI2_RESET_POLL_CNT ; i++)\n\t\trc = hl_poll_timeout(\n\t\t\thdev,\n\t\t\tmmPSOC_GLOBAL_CONF_BTM_FSM,\n\t\t\treg_val,\n\t\t\treg_val == 0,\n\t\t\t1000,\n\t\t\tpoll_timeout_us);\n\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Timeout while waiting for device to reset 0x%x\\n\", reg_val);\n}\n\nstatic int gaudi2_hw_fini(struct hl_device *hdev, bool hard_reset, bool fw_reset)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 poll_timeout_us, reset_sleep_ms;\n\tbool driver_performs_reset = false;\n\tint rc;\n\n\tif (hdev->pldm) {\n\t\treset_sleep_ms = hard_reset ? GAUDI2_PLDM_HRESET_TIMEOUT_MSEC :\n\t\t\t\t\t\tGAUDI2_PLDM_SRESET_TIMEOUT_MSEC;\n\t\tpoll_timeout_us = GAUDI2_PLDM_RESET_POLL_TIMEOUT_USEC;\n\t} else {\n\t\treset_sleep_ms = GAUDI2_RESET_TIMEOUT_MSEC;\n\t\tpoll_timeout_us = GAUDI2_RESET_POLL_TIMEOUT_USEC;\n\t}\n\n\tif (fw_reset)\n\t\tgoto skip_reset;\n\n\tgaudi2_reset_arcs(hdev);\n\n\tif (hard_reset) {\n\t\tdriver_performs_reset = !hdev->asic_prop.hard_reset_done_by_fw;\n\t\tgaudi2_execute_hard_reset(hdev);\n\t} else {\n\t\t \n\t\tdriver_performs_reset = (hdev->fw_components == FW_TYPE_PREBOOT_CPU &&\n\t\t\t\t\t\t\t!hdev->asic_prop.fw_security_enabled);\n\t\trc = gaudi2_execute_soft_reset(hdev, driver_performs_reset, poll_timeout_us);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\nskip_reset:\n\tif (driver_performs_reset || hard_reset) {\n\t\t \n\n\t\t \n\t\tmsleep(reset_sleep_ms);\n\n\t\tif (hdev->fw_components & FW_TYPE_PREBOOT_CPU)\n\t\t\thl_fw_wait_preboot_ready(hdev);\n\t\telse\n\t\t\tgaudi2_poll_btm_indication(hdev, poll_timeout_us);\n\t}\n\n\tif (!gaudi2)\n\t\treturn 0;\n\n\tgaudi2->dec_hw_cap_initialized &= ~(HW_CAP_DEC_MASK);\n\tgaudi2->tpc_hw_cap_initialized &= ~(HW_CAP_TPC_MASK);\n\n\t \n\tgaudi2->nic_hw_cap_initialized &= ~(HW_CAP_NIC_MASK);\n\n\tif (hard_reset) {\n\t\tgaudi2->hw_cap_initialized &=\n\t\t\t~(HW_CAP_DRAM | HW_CAP_CLK_GATE | HW_CAP_HBM_SCRAMBLER_MASK |\n\t\t\tHW_CAP_PMMU | HW_CAP_CPU | HW_CAP_CPU_Q |\n\t\t\tHW_CAP_SRAM_SCRAMBLER | HW_CAP_DMMU_MASK |\n\t\t\tHW_CAP_PDMA_MASK | HW_CAP_EDMA_MASK | HW_CAP_KDMA |\n\t\t\tHW_CAP_MME_MASK | HW_CAP_ROT_MASK);\n\n\t\tmemset(gaudi2->events_stat, 0, sizeof(gaudi2->events_stat));\n\t} else {\n\t\tgaudi2->hw_cap_initialized &=\n\t\t\t~(HW_CAP_CLK_GATE | HW_CAP_HBM_SCRAMBLER_SW_RESET |\n\t\t\tHW_CAP_PDMA_MASK | HW_CAP_EDMA_MASK | HW_CAP_MME_MASK |\n\t\t\tHW_CAP_ROT_MASK);\n\t}\n\treturn 0;\n}\n\nstatic int gaudi2_suspend(struct hl_device *hdev)\n{\n\tint rc;\n\n\trc = hl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_DISABLE_PCI_ACCESS, 0x0);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to disable PCI access from CPU\\n\");\n\n\treturn rc;\n}\n\nstatic int gaudi2_resume(struct hl_device *hdev)\n{\n\treturn gaudi2_init_iatu(hdev);\n}\n\nstatic int gaudi2_mmap(struct hl_device *hdev, struct vm_area_struct *vma,\n\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size)\n{\n\tint rc;\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP |\n\t\t\tVM_DONTCOPY | VM_NORESERVE);\n\n#ifdef _HAS_DMA_MMAP_COHERENT\n\n\trc = dma_mmap_coherent(hdev->dev, vma, cpu_addr, dma_addr, size);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"dma_mmap_coherent error %d\", rc);\n\n#else\n\n\trc = remap_pfn_range(vma, vma->vm_start,\n\t\t\t\tvirt_to_phys(cpu_addr) >> PAGE_SHIFT,\n\t\t\t\tsize, vma->vm_page_prot);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"remap_pfn_range error %d\", rc);\n\n#endif\n\n\treturn rc;\n}\n\nstatic bool gaudi2_is_queue_enabled(struct hl_device *hdev, u32 hw_queue_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 hw_cap_mask = 0;\n\tu64 hw_tpc_cap_bit = 0;\n\tu64 hw_nic_cap_bit = 0;\n\tu64 hw_test_cap_bit = 0;\n\n\tswitch (hw_queue_id) {\n\tcase GAUDI2_QUEUE_ID_PDMA_0_0:\n\tcase GAUDI2_QUEUE_ID_PDMA_0_1:\n\tcase GAUDI2_QUEUE_ID_PDMA_1_0:\n\t\thw_cap_mask = HW_CAP_PDMA_MASK;\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE0_EDMA_1_3:\n\t\thw_test_cap_bit = HW_CAP_EDMA_SHIFT +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0) >> 2);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE1_EDMA_1_3:\n\t\thw_test_cap_bit = HW_CAP_EDMA_SHIFT + NUM_OF_EDMA_PER_DCORE +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0) >> 2);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE2_EDMA_1_3:\n\t\thw_test_cap_bit = HW_CAP_EDMA_SHIFT + 2 * NUM_OF_EDMA_PER_DCORE +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0) >> 2);\n\t\tbreak;\n\tcase GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0...GAUDI2_QUEUE_ID_DCORE3_EDMA_1_3:\n\t\thw_test_cap_bit = HW_CAP_EDMA_SHIFT + 3 * NUM_OF_EDMA_PER_DCORE +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0) >> 2);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE0_MME_0_0 ... GAUDI2_QUEUE_ID_DCORE0_MME_0_3:\n\t\thw_test_cap_bit = HW_CAP_MME_SHIFT;\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE1_MME_0_0 ... GAUDI2_QUEUE_ID_DCORE1_MME_0_3:\n\t\thw_test_cap_bit = HW_CAP_MME_SHIFT + 1;\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE2_MME_0_0 ... GAUDI2_QUEUE_ID_DCORE2_MME_0_3:\n\t\thw_test_cap_bit = HW_CAP_MME_SHIFT + 2;\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE3_MME_0_0 ... GAUDI2_QUEUE_ID_DCORE3_MME_0_3:\n\t\thw_test_cap_bit = HW_CAP_MME_SHIFT + 3;\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE0_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE0_TPC_5_3:\n\t\thw_tpc_cap_bit = HW_CAP_TPC_SHIFT +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE0_TPC_0_0) >> 2);\n\n\t\t \n\t\tif (!hw_tpc_cap_bit)\n\t\t\treturn !!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(0));\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE1_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE1_TPC_5_3:\n\t\thw_tpc_cap_bit = HW_CAP_TPC_SHIFT + NUM_OF_TPC_PER_DCORE +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE1_TPC_0_0) >> 2);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE2_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE2_TPC_5_3:\n\t\thw_tpc_cap_bit = HW_CAP_TPC_SHIFT + (2 * NUM_OF_TPC_PER_DCORE) +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE2_TPC_0_0) >> 2);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE3_TPC_0_0 ... GAUDI2_QUEUE_ID_DCORE3_TPC_5_3:\n\t\thw_tpc_cap_bit = HW_CAP_TPC_SHIFT + (3 * NUM_OF_TPC_PER_DCORE) +\n\t\t\t((hw_queue_id - GAUDI2_QUEUE_ID_DCORE3_TPC_0_0) >> 2);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_DCORE0_TPC_6_0 ... GAUDI2_QUEUE_ID_DCORE0_TPC_6_3:\n\t\thw_tpc_cap_bit = HW_CAP_TPC_SHIFT + (4 * NUM_OF_TPC_PER_DCORE);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_ROT_0_0 ... GAUDI2_QUEUE_ID_ROT_1_3:\n\t\thw_test_cap_bit = HW_CAP_ROT_SHIFT + ((hw_queue_id - GAUDI2_QUEUE_ID_ROT_0_0) >> 2);\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_NIC_0_0 ... GAUDI2_QUEUE_ID_NIC_23_3:\n\t\thw_nic_cap_bit = HW_CAP_NIC_SHIFT + ((hw_queue_id - GAUDI2_QUEUE_ID_NIC_0_0) >> 2);\n\n\t\t \n\t\tif (!hw_nic_cap_bit)\n\t\t\treturn !!(gaudi2->nic_hw_cap_initialized & BIT_ULL(0));\n\t\tbreak;\n\n\tcase GAUDI2_QUEUE_ID_CPU_PQ:\n\t\treturn !!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q);\n\n\tdefault:\n\t\treturn false;\n\t}\n\n\tif (hw_tpc_cap_bit)\n\t\treturn  !!(gaudi2->tpc_hw_cap_initialized & BIT_ULL(hw_tpc_cap_bit));\n\n\tif (hw_nic_cap_bit)\n\t\treturn  !!(gaudi2->nic_hw_cap_initialized & BIT_ULL(hw_nic_cap_bit));\n\n\tif (hw_test_cap_bit)\n\t\thw_cap_mask = BIT_ULL(hw_test_cap_bit);\n\n\treturn !!(gaudi2->hw_cap_initialized & hw_cap_mask);\n}\n\nstatic bool gaudi2_is_arc_enabled(struct hl_device *hdev, u64 arc_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tswitch (arc_id) {\n\tcase CPU_ID_SCHED_ARC0 ... CPU_ID_SCHED_ARC5:\n\tcase CPU_ID_MME_QMAN_ARC0...CPU_ID_ROT_QMAN_ARC1:\n\t\treturn !!(gaudi2->active_hw_arc & BIT_ULL(arc_id));\n\n\tcase CPU_ID_TPC_QMAN_ARC0...CPU_ID_TPC_QMAN_ARC24:\n\t\treturn !!(gaudi2->active_tpc_arc & BIT_ULL(arc_id - CPU_ID_TPC_QMAN_ARC0));\n\n\tcase CPU_ID_NIC_QMAN_ARC0...CPU_ID_NIC_QMAN_ARC23:\n\t\treturn !!(gaudi2->active_nic_arc & BIT_ULL(arc_id - CPU_ID_NIC_QMAN_ARC0));\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void gaudi2_clr_arc_id_cap(struct hl_device *hdev, u64 arc_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tswitch (arc_id) {\n\tcase CPU_ID_SCHED_ARC0 ... CPU_ID_SCHED_ARC5:\n\tcase CPU_ID_MME_QMAN_ARC0...CPU_ID_ROT_QMAN_ARC1:\n\t\tgaudi2->active_hw_arc &= ~(BIT_ULL(arc_id));\n\t\tbreak;\n\n\tcase CPU_ID_TPC_QMAN_ARC0...CPU_ID_TPC_QMAN_ARC24:\n\t\tgaudi2->active_tpc_arc &= ~(BIT_ULL(arc_id - CPU_ID_TPC_QMAN_ARC0));\n\t\tbreak;\n\n\tcase CPU_ID_NIC_QMAN_ARC0...CPU_ID_NIC_QMAN_ARC23:\n\t\tgaudi2->active_nic_arc &= ~(BIT_ULL(arc_id - CPU_ID_NIC_QMAN_ARC0));\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n}\n\nstatic void gaudi2_set_arc_id_cap(struct hl_device *hdev, u64 arc_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tswitch (arc_id) {\n\tcase CPU_ID_SCHED_ARC0 ... CPU_ID_SCHED_ARC5:\n\tcase CPU_ID_MME_QMAN_ARC0...CPU_ID_ROT_QMAN_ARC1:\n\t\tgaudi2->active_hw_arc |= BIT_ULL(arc_id);\n\t\tbreak;\n\n\tcase CPU_ID_TPC_QMAN_ARC0...CPU_ID_TPC_QMAN_ARC24:\n\t\tgaudi2->active_tpc_arc |= BIT_ULL(arc_id - CPU_ID_TPC_QMAN_ARC0);\n\t\tbreak;\n\n\tcase CPU_ID_NIC_QMAN_ARC0...CPU_ID_NIC_QMAN_ARC23:\n\t\tgaudi2->active_nic_arc |= BIT_ULL(arc_id - CPU_ID_NIC_QMAN_ARC0);\n\t\tbreak;\n\n\tdefault:\n\t\treturn;\n\t}\n}\n\nstatic void gaudi2_ring_doorbell(struct hl_device *hdev, u32 hw_queue_id, u32 pi)\n{\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 pq_offset, reg_base, db_reg_offset, db_value;\n\n\tif (hw_queue_id != GAUDI2_QUEUE_ID_CPU_PQ) {\n\t\t \n\t\tpq_offset = (hw_queue_id & 0x3) * 4;\n\t\treg_base = gaudi2_qm_blocks_bases[hw_queue_id];\n\t\tdb_reg_offset = reg_base + QM_PQ_PI_0_OFFSET + pq_offset;\n\t} else {\n\t\tdb_reg_offset = mmCPU_IF_PF_PQ_PI;\n\t}\n\n\tdb_value = pi;\n\n\t \n\tWREG32(db_reg_offset, db_value);\n\n\tif (hw_queue_id == GAUDI2_QUEUE_ID_CPU_PQ) {\n\t\t \n\t\tmb();\n\t\tWREG32(le32_to_cpu(dyn_regs->gic_host_pi_upd_irq),\n\t\t\tgaudi2_irq_map_table[GAUDI2_EVENT_CPU_PI_UPDATE].cpu_id);\n\t}\n}\n\nstatic void gaudi2_pqe_write(struct hl_device *hdev, __le64 *pqe, struct hl_bd *bd)\n{\n\t__le64 *pbd = (__le64 *) bd;\n\n\t \n\tpqe[0] = pbd[0];\n\tpqe[1] = pbd[1];\n}\n\nstatic void *gaudi2_dma_alloc_coherent(struct hl_device *hdev, size_t size,\n\t\t\t\tdma_addr_t *dma_handle, gfp_t flags)\n{\n\treturn dma_alloc_coherent(&hdev->pdev->dev, size, dma_handle, flags);\n}\n\nstatic void gaudi2_dma_free_coherent(struct hl_device *hdev, size_t size,\n\t\t\t\tvoid *cpu_addr, dma_addr_t dma_handle)\n{\n\tdma_free_coherent(&hdev->pdev->dev, size, cpu_addr, dma_handle);\n}\n\nstatic int gaudi2_send_cpu_message(struct hl_device *hdev, u32 *msg, u16 len,\n\t\t\t\tu32 timeout, u64 *result)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q)) {\n\t\tif (result)\n\t\t\t*result = 0;\n\t\treturn 0;\n\t}\n\n\tif (!timeout)\n\t\ttimeout = GAUDI2_MSG_TO_CPU_TIMEOUT_USEC;\n\n\treturn hl_fw_send_cpu_message(hdev, GAUDI2_QUEUE_ID_CPU_PQ, msg, len, timeout, result);\n}\n\nstatic void *gaudi2_dma_pool_zalloc(struct hl_device *hdev, size_t size,\n\t\t\t\tgfp_t mem_flags, dma_addr_t *dma_handle)\n{\n\tif (size > GAUDI2_DMA_POOL_BLK_SIZE)\n\t\treturn NULL;\n\n\treturn dma_pool_zalloc(hdev->dma_pool, mem_flags, dma_handle);\n}\n\nstatic void gaudi2_dma_pool_free(struct hl_device *hdev, void *vaddr, dma_addr_t dma_addr)\n{\n\tdma_pool_free(hdev->dma_pool, vaddr, dma_addr);\n}\n\nstatic void *gaudi2_cpu_accessible_dma_pool_alloc(struct hl_device *hdev, size_t size,\n\t\t\t\t\t\tdma_addr_t *dma_handle)\n{\n\treturn hl_fw_cpu_accessible_dma_pool_alloc(hdev, size, dma_handle);\n}\n\nstatic void gaudi2_cpu_accessible_dma_pool_free(struct hl_device *hdev, size_t size, void *vaddr)\n{\n\thl_fw_cpu_accessible_dma_pool_free(hdev, size, vaddr);\n}\n\nstatic dma_addr_t gaudi2_dma_map_single(struct hl_device *hdev, void *addr, int len,\n\t\t\t\t\tenum dma_data_direction dir)\n{\n\tdma_addr_t dma_addr;\n\n\tdma_addr = dma_map_single(&hdev->pdev->dev, addr, len, dir);\n\tif (unlikely(dma_mapping_error(&hdev->pdev->dev, dma_addr)))\n\t\treturn 0;\n\n\treturn dma_addr;\n}\n\nstatic void gaudi2_dma_unmap_single(struct hl_device *hdev, dma_addr_t addr, int len,\n\t\t\t\t\tenum dma_data_direction dir)\n{\n\tdma_unmap_single(&hdev->pdev->dev, addr, len, dir);\n}\n\nstatic int gaudi2_validate_cb_address(struct hl_device *hdev, struct hl_cs_parser *parser)\n{\n\tstruct asic_fixed_properties *asic_prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!gaudi2_is_queue_enabled(hdev, parser->hw_queue_id)) {\n\t\tdev_err(hdev->dev, \"h/w queue %d is disabled\\n\", parser->hw_queue_id);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\n\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->sram_user_base_address,\n\t\t\t\t\tasic_prop->sram_end_address))\n\t\treturn 0;\n\n\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->dram_user_base_address,\n\t\t\t\t\tasic_prop->dram_end_address))\n\t\treturn 0;\n\n\tif ((gaudi2->hw_cap_initialized & HW_CAP_DMMU_MASK) &&\n\t\thl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\t\tasic_prop->dmmu.start_addr,\n\t\t\t\t\t\tasic_prop->dmmu.end_addr))\n\t\treturn 0;\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_PMMU) {\n\t\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->pmmu.start_addr,\n\t\t\t\t\tasic_prop->pmmu.end_addr) ||\n\t\t\thl_mem_area_inside_range(\n\t\t\t\t\t(u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->pmmu_huge.start_addr,\n\t\t\t\t\tasic_prop->pmmu_huge.end_addr))\n\t\t\treturn 0;\n\n\t} else if (gaudi2_host_phys_addr_valid((u64) (uintptr_t) parser->user_cb)) {\n\t\tif (!hdev->pdev)\n\t\t\treturn 0;\n\n\t\tif (!device_iommu_mapped(&hdev->pdev->dev))\n\t\t\treturn 0;\n\t}\n\n\tdev_err(hdev->dev, \"CB address %p + 0x%x for internal QMAN is not valid\\n\",\n\t\tparser->user_cb, parser->user_cb_size);\n\n\treturn -EFAULT;\n}\n\nstatic int gaudi2_cs_parser(struct hl_device *hdev, struct hl_cs_parser *parser)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!parser->is_kernel_allocated_cb)\n\t\treturn gaudi2_validate_cb_address(hdev, parser);\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PMMU)) {\n\t\tdev_err(hdev->dev, \"PMMU not initialized - Unsupported mode in Gaudi2\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_send_heartbeat(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_send_heartbeat(hdev);\n}\n\n \nstatic void gaudi2_kdma_set_mmbp_asid(struct hl_device *hdev,\n\t\t\t\t\t   bool mmu_bypass, u32 asid)\n{\n\tu32 rw_asid, rw_mmu_bp;\n\n\trw_asid = (asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_RD_SHIFT) |\n\t\t      (asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_WR_SHIFT);\n\n\trw_mmu_bp = (!!mmu_bypass << ARC_FARM_KDMA_CTX_AXUSER_HB_MMU_BP_RD_SHIFT) |\n\t\t\t(!!mmu_bypass << ARC_FARM_KDMA_CTX_AXUSER_HB_MMU_BP_WR_SHIFT);\n\n\tWREG32(mmARC_FARM_KDMA_CTX_AXUSER_HB_ASID, rw_asid);\n\tWREG32(mmARC_FARM_KDMA_CTX_AXUSER_HB_MMU_BP, rw_mmu_bp);\n}\n\nstatic void gaudi2_arm_cq_monitor(struct hl_device *hdev, u32 sob_id, u32 mon_id, u32 cq_id,\n\t\t\t\t\t\tu32 mon_payload, u32 sync_value)\n{\n\tu32 sob_offset, mon_offset, sync_group_id, mode, mon_arm;\n\tu8 mask;\n\n\tsob_offset = sob_id * 4;\n\tmon_offset = mon_id * 4;\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset, 0);\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + mon_offset, cq_id);\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + mon_offset, mon_payload);\n\n\tsync_group_id = sob_id / 8;\n\tmask = ~(1 << (sob_id & 0x7));\n\tmode = 1;  \n\n\tmon_arm = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SOD_MASK, sync_value);\n\tmon_arm |= FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SOP_MASK, mode);\n\tmon_arm |= FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_MASK_MASK, mask);\n\tmon_arm |= FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_MON_ARM_SID_MASK, sync_group_id);\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_MON_ARM_0 + mon_offset, mon_arm);\n}\n\n \nstatic int gaudi2_send_job_to_kdma(struct hl_device *hdev,\n\t\t\t\t\tu64 src_addr, u64 dst_addr,\n\t\t\t\t\tu32 size, bool is_memset)\n{\n\tu32 comp_val, commit_mask, *polling_addr, timeout, status = 0;\n\tstruct hl_cq_entry *cq_base;\n\tstruct hl_cq *cq;\n\tu64 comp_addr;\n\tint rc;\n\n\tgaudi2_arm_cq_monitor(hdev, GAUDI2_RESERVED_SOB_KDMA_COMPLETION,\n\t\t\t\tGAUDI2_RESERVED_MON_KDMA_COMPLETION,\n\t\t\t\tGAUDI2_RESERVED_CQ_KDMA_COMPLETION, 1, 1);\n\n\tcomp_addr = CFG_BASE + mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 +\n\t\t\t(GAUDI2_RESERVED_SOB_KDMA_COMPLETION * sizeof(u32));\n\n\tcomp_val = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_INC_MASK, 1) |\n\t\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_VAL_MASK, 1);\n\n\tWREG32(mmARC_FARM_KDMA_CTX_SRC_BASE_LO, lower_32_bits(src_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_SRC_BASE_HI, upper_32_bits(src_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_DST_BASE_LO, lower_32_bits(dst_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_DST_BASE_HI, upper_32_bits(dst_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_WR_COMP_ADDR_LO, lower_32_bits(comp_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_WR_COMP_ADDR_HI, upper_32_bits(comp_addr));\n\tWREG32(mmARC_FARM_KDMA_CTX_WR_COMP_WDATA, comp_val);\n\tWREG32(mmARC_FARM_KDMA_CTX_DST_TSIZE_0, size);\n\n\tcommit_mask = FIELD_PREP(ARC_FARM_KDMA_CTX_COMMIT_LIN_MASK, 1) |\n\t\t\t\tFIELD_PREP(ARC_FARM_KDMA_CTX_COMMIT_WR_COMP_EN_MASK, 1);\n\n\tif (is_memset)\n\t\tcommit_mask |= FIELD_PREP(ARC_FARM_KDMA_CTX_COMMIT_MEM_SET_MASK, 1);\n\n\tWREG32(mmARC_FARM_KDMA_CTX_COMMIT, commit_mask);\n\n\t \n\tcq = &hdev->completion_queue[GAUDI2_RESERVED_CQ_KDMA_COMPLETION];\n\tcq_base = cq->kernel_address;\n\tpolling_addr = (u32 *)&cq_base[cq->ci];\n\n\tif (hdev->pldm)\n\t\t \n\t\ttimeout = ((size / SZ_1M) + 1) * USEC_PER_SEC * 20;\n\telse\n\t\ttimeout = KDMA_TIMEOUT_USEC;\n\n\t \n\trc = hl_poll_timeout_memory(\n\t\t\thdev,\n\t\t\tpolling_addr,\n\t\t\tstatus,\n\t\t\t(status == 1),\n\t\t\t1000,\n\t\t\ttimeout,\n\t\t\ttrue);\n\n\t*polling_addr = 0;\n\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Timeout while waiting for KDMA to be idle\\n\");\n\t\tWREG32(mmARC_FARM_KDMA_CFG_1, 1 << ARC_FARM_KDMA_CFG_1_HALT_SHIFT);\n\t\treturn rc;\n\t}\n\n\tcq->ci = hl_cq_inc_ptr(cq->ci);\n\n\treturn 0;\n}\n\nstatic void gaudi2_memset_device_lbw(struct hl_device *hdev, u32 addr, u32 size, u32 val)\n{\n\tu32 i;\n\n\tfor (i = 0 ; i < size ; i += sizeof(u32))\n\t\tWREG32(addr + i, val);\n}\n\nstatic void gaudi2_qman_set_test_mode(struct hl_device *hdev, u32 hw_queue_id, bool enable)\n{\n\tu32 reg_base = gaudi2_qm_blocks_bases[hw_queue_id];\n\n\tif (enable) {\n\t\tWREG32(reg_base + QM_GLBL_PROT_OFFSET, QMAN_MAKE_TRUSTED_TEST_MODE);\n\t\tWREG32(reg_base + QM_PQC_CFG_OFFSET, 0);\n\t} else {\n\t\tWREG32(reg_base + QM_GLBL_PROT_OFFSET, QMAN_MAKE_TRUSTED);\n\t\tWREG32(reg_base + QM_PQC_CFG_OFFSET, 1 << PDMA0_QM_PQC_CFG_EN_SHIFT);\n\t}\n}\n\nstatic inline u32 gaudi2_test_queue_hw_queue_id_to_sob_id(struct hl_device *hdev, u32 hw_queue_id)\n{\n\treturn hdev->asic_prop.first_available_user_sob[0] +\n\t\t\t\thw_queue_id - GAUDI2_QUEUE_ID_PDMA_0_0;\n}\n\nstatic void gaudi2_test_queue_clear(struct hl_device *hdev, u32 hw_queue_id)\n{\n\tu32 sob_offset = gaudi2_test_queue_hw_queue_id_to_sob_id(hdev, hw_queue_id) * 4;\n\tu32 sob_addr = mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset;\n\n\t \n\tWREG32(sob_addr, 0);\n}\n\nstatic int gaudi2_test_queue_send_msg_short(struct hl_device *hdev, u32 hw_queue_id, u32 sob_val,\n\t\t\t\t\t    struct gaudi2_queues_test_info *msg_info)\n{\n\tu32 sob_offset =  gaudi2_test_queue_hw_queue_id_to_sob_id(hdev, hw_queue_id) * 4;\n\tu32 tmp, sob_base = 1;\n\tstruct packet_msg_short *msg_short_pkt = msg_info->kern_addr;\n\tsize_t pkt_size = sizeof(struct packet_msg_short);\n\tint rc;\n\n\ttmp = (PACKET_MSG_SHORT << GAUDI2_PKT_CTL_OPCODE_SHIFT) |\n\t\t(1 << GAUDI2_PKT_CTL_EB_SHIFT) |\n\t\t(1 << GAUDI2_PKT_CTL_MB_SHIFT) |\n\t\t(sob_base << GAUDI2_PKT_SHORT_CTL_BASE_SHIFT) |\n\t\t(sob_offset << GAUDI2_PKT_SHORT_CTL_ADDR_SHIFT);\n\n\tmsg_short_pkt->value = cpu_to_le32(sob_val);\n\tmsg_short_pkt->ctl = cpu_to_le32(tmp);\n\n\trc = hl_hw_queue_send_cb_no_cmpl(hdev, hw_queue_id, pkt_size, msg_info->dma_addr);\n\tif (rc)\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to send msg_short packet to H/W queue %d\\n\", hw_queue_id);\n\n\treturn rc;\n}\n\nstatic int gaudi2_test_queue_wait_completion(struct hl_device *hdev, u32 hw_queue_id, u32 sob_val)\n{\n\tu32 sob_offset = gaudi2_test_queue_hw_queue_id_to_sob_id(hdev, hw_queue_id) * 4;\n\tu32 sob_addr = mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset;\n\tu32 timeout_usec, tmp;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI2_PLDM_TEST_QUEUE_WAIT_USEC;\n\telse\n\t\ttimeout_usec = GAUDI2_TEST_QUEUE_WAIT_USEC;\n\n\trc = hl_poll_timeout(\n\t\t\thdev,\n\t\t\tsob_addr,\n\t\t\ttmp,\n\t\t\t(tmp == sob_val),\n\t\t\t1000,\n\t\t\ttimeout_usec);\n\n\tif (rc == -ETIMEDOUT) {\n\t\tdev_err(hdev->dev, \"H/W queue %d test failed (SOB_OBJ_0 == 0x%x)\\n\",\n\t\t\thw_queue_id, tmp);\n\t\trc = -EIO;\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi2_test_cpu_queue(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\t \n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_test_cpu_queue(hdev);\n}\n\nstatic int gaudi2_test_queues(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_queues_test_info *msg_info;\n\tu32 sob_val = 0x5a5a;\n\tint i, rc;\n\n\t \n\tfor (i = GAUDI2_QUEUE_ID_PDMA_0_0 ; i < GAUDI2_QUEUE_ID_CPU_PQ; i++) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, i))\n\t\t\tcontinue;\n\n\t\tmsg_info = &gaudi2->queues_test_info[i - GAUDI2_QUEUE_ID_PDMA_0_0];\n\t\tgaudi2_qman_set_test_mode(hdev, i, true);\n\t\tgaudi2_test_queue_clear(hdev, i);\n\t\trc = gaudi2_test_queue_send_msg_short(hdev, i, sob_val, msg_info);\n\t\tif (rc)\n\t\t\tgoto done;\n\t}\n\n\trc = gaudi2_test_cpu_queue(hdev);\n\tif (rc)\n\t\tgoto done;\n\n\t \n\tfor (i = GAUDI2_QUEUE_ID_PDMA_0_0 ; i < GAUDI2_QUEUE_ID_CPU_PQ; i++) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, i))\n\t\t\tcontinue;\n\n\t\trc = gaudi2_test_queue_wait_completion(hdev, i, sob_val);\n\t\tif (rc)\n\t\t\t \n\t\t\tgoto done;\n\n\t\tgaudi2_test_queue_clear(hdev, i);\n\t\tgaudi2_qman_set_test_mode(hdev, i, false);\n\t}\n\ndone:\n\treturn rc;\n}\n\nstatic int gaudi2_compute_reset_late_init(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tsize_t irq_arr_size;\n\tint rc;\n\n\tgaudi2_init_arcs(hdev);\n\n\trc = gaudi2_scrub_arcs_dccm(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to scrub arcs DCCM\\n\");\n\t\treturn rc;\n\t}\n\n\tgaudi2_init_security(hdev);\n\n\t \n\tirq_arr_size = gaudi2->num_of_valid_hw_events * sizeof(gaudi2->hw_events[0]);\n\treturn hl_fw_unmask_irq_arr(hdev, gaudi2->hw_events, irq_arr_size);\n}\n\nstatic bool gaudi2_get_edma_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tu32 qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts, dma_core_sts0, dma_core_sts1;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tconst char *edma_fmt = \"%-6d%-6d%-9s%#-14x%#-15x%#x\\n\";\n\tbool is_idle = true, is_eng_idle;\n\tint engine_idx, i, j;\n\tu64 offset;\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nCORE  EDMA  is_idle  QM_GLBL_STS0  DMA_CORE_STS0  DMA_CORE_STS1\\n\"\n\t\t\t\"----  ----  -------  ------------  -------------  -------------\\n\");\n\n\tfor (i = 0; i < NUM_OF_DCORES; i++) {\n\t\tfor (j = 0 ; j < NUM_OF_EDMA_PER_DCORE ; j++) {\n\t\t\tint seq = i * NUM_OF_EDMA_PER_DCORE + j;\n\n\t\t\tif (!(prop->edma_enabled_mask & BIT(seq)))\n\t\t\t\tcontinue;\n\n\t\t\tengine_idx = GAUDI2_DCORE0_ENGINE_ID_EDMA_0 +\n\t\t\t\t\ti * GAUDI2_ENGINE_ID_DCORE_OFFSET + j;\n\t\t\toffset = i * DCORE_OFFSET + j * DCORE_EDMA_OFFSET;\n\n\t\t\tdma_core_sts0 = RREG32(mmDCORE0_EDMA0_CORE_STS0 + offset);\n\t\t\tdma_core_sts1 = RREG32(mmDCORE0_EDMA0_CORE_STS1 + offset);\n\n\t\t\tqm_glbl_sts0 = RREG32(mmDCORE0_EDMA0_QM_GLBL_STS0 + offset);\n\t\t\tqm_glbl_sts1 = RREG32(mmDCORE0_EDMA0_QM_GLBL_STS1 + offset);\n\t\t\tqm_cgm_sts = RREG32(mmDCORE0_EDMA0_QM_CGM_STS + offset);\n\n\t\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts) &&\n\t\t\t\t\tIS_DMA_IDLE(dma_core_sts0) && !IS_DMA_HALTED(dma_core_sts1);\n\t\t\tis_idle &= is_eng_idle;\n\n\t\t\tif (mask && !is_eng_idle)\n\t\t\t\tset_bit(engine_idx, mask);\n\n\t\t\tif (e)\n\t\t\t\thl_engine_data_sprintf(e, edma_fmt, i, j, is_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\t\tqm_glbl_sts0, dma_core_sts0, dma_core_sts1);\n\t\t}\n\t}\n\n\treturn is_idle;\n}\n\nstatic bool gaudi2_get_pdma_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tu32 qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts, dma_core_sts0, dma_core_sts1;\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tconst char *pdma_fmt = \"%-6d%-9s%#-14x%#-15x%#x\\n\";\n\tbool is_idle = true, is_eng_idle;\n\tint engine_idx, i;\n\tu64 offset;\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\t\t\"\\nPDMA  is_idle  QM_GLBL_STS0  DMA_CORE_STS0  DMA_CORE_STS1\\n\"\n\t\t\t\t\t\"----  -------  ------------  -------------  -------------\\n\");\n\n\tfor (i = 0 ; i < NUM_OF_PDMA ; i++) {\n\t\tengine_idx = GAUDI2_ENGINE_ID_PDMA_0 + i;\n\t\toffset = i * PDMA_OFFSET;\n\t\tdma_core_sts0 = RREG32(mmPDMA0_CORE_STS0 + offset);\n\t\tdma_core_sts1 = RREG32(mmPDMA0_CORE_STS1 + offset);\n\n\t\tqm_glbl_sts0 = RREG32(mmPDMA0_QM_GLBL_STS0 + offset);\n\t\tqm_glbl_sts1 = RREG32(mmPDMA0_QM_GLBL_STS1 + offset);\n\t\tqm_cgm_sts = RREG32(mmPDMA0_QM_CGM_STS + offset);\n\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts) &&\n\t\t\t\tIS_DMA_IDLE(dma_core_sts0) && !IS_DMA_HALTED(dma_core_sts1);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(engine_idx, mask);\n\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, pdma_fmt, i, is_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\tqm_glbl_sts0, dma_core_sts0, dma_core_sts1);\n\t}\n\n\treturn is_idle;\n}\n\nstatic bool gaudi2_get_nic_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tconst char *nic_fmt = \"%-5d%-9s%#-14x%#-12x\\n\";\n\tu32 qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts;\n\tbool is_idle = true, is_eng_idle;\n\tint engine_idx, i;\n\tu64 offset = 0;\n\n\t \n\tif (e && hdev->nic_ports_mask)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\t\t\"\\nNIC  is_idle  QM_GLBL_STS0  QM_CGM_STS\\n\"\n\t\t\t\t\t\"---  -------  ------------  ----------\\n\");\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++) {\n\t\tif (!(i & 1))\n\t\t\toffset = i / 2 * NIC_OFFSET;\n\t\telse\n\t\t\toffset += NIC_QM_OFFSET;\n\n\t\tif (!(hdev->nic_ports_mask & BIT(i)))\n\t\t\tcontinue;\n\n\t\tengine_idx = GAUDI2_ENGINE_ID_NIC0_0 + i;\n\n\n\t\tqm_glbl_sts0 = RREG32(mmNIC0_QM0_GLBL_STS0 + offset);\n\t\tqm_glbl_sts1 = RREG32(mmNIC0_QM0_GLBL_STS1 + offset);\n\t\tqm_cgm_sts = RREG32(mmNIC0_QM0_CGM_STS + offset);\n\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(engine_idx, mask);\n\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, nic_fmt, i, is_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\tqm_glbl_sts0, qm_cgm_sts);\n\t}\n\n\treturn is_idle;\n}\n\nstatic bool gaudi2_get_mme_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tu32 qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts, mme_arch_sts;\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tconst char *mme_fmt = \"%-5d%-6s%-9s%#-14x%#x\\n\";\n\tbool is_idle = true, is_eng_idle;\n\tint engine_idx, i;\n\tu64 offset;\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\t\t\"\\nMME  Stub  is_idle  QM_GLBL_STS0  MME_ARCH_STATUS\\n\"\n\t\t\t\t\t\"---  ----  -------  ------------  ---------------\\n\");\n\t \n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++) {\n\t\tengine_idx = GAUDI2_DCORE0_ENGINE_ID_MME + i * GAUDI2_ENGINE_ID_DCORE_OFFSET;\n\t\toffset = i * DCORE_OFFSET;\n\n\t\tqm_glbl_sts0 = RREG32(mmDCORE0_MME_QM_GLBL_STS0 + offset);\n\t\tqm_glbl_sts1 = RREG32(mmDCORE0_MME_QM_GLBL_STS1 + offset);\n\t\tqm_cgm_sts = RREG32(mmDCORE0_MME_QM_CGM_STS + offset);\n\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts);\n\t\tis_idle &= is_eng_idle;\n\n\t\tmme_arch_sts = RREG32(mmDCORE0_MME_CTRL_LO_ARCH_STATUS + offset);\n\t\tis_eng_idle &= IS_MME_IDLE(mme_arch_sts);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, mme_fmt, i, \"N\",\n\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\tqm_glbl_sts0,\n\t\t\t\tmme_arch_sts);\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(engine_idx, mask);\n\t}\n\n\treturn is_idle;\n}\n\nstatic void gaudi2_is_tpc_engine_idle(struct hl_device *hdev, int dcore, int inst, u32 offset,\n\t\t\t\t\tstruct iterate_module_ctx *ctx)\n{\n\tstruct gaudi2_tpc_idle_data *idle_data = ctx->data;\n\tu32 tpc_cfg_sts, qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts;\n\tbool is_eng_idle;\n\tint engine_idx;\n\n\tif ((dcore == 0) && (inst == (NUM_DCORE0_TPC - 1)))\n\t\tengine_idx = GAUDI2_DCORE0_ENGINE_ID_TPC_6;\n\telse\n\t\tengine_idx = GAUDI2_DCORE0_ENGINE_ID_TPC_0 +\n\t\t\t\tdcore * GAUDI2_ENGINE_ID_DCORE_OFFSET + inst;\n\n\ttpc_cfg_sts = RREG32(mmDCORE0_TPC0_CFG_STATUS + offset);\n\tqm_glbl_sts0 = RREG32(mmDCORE0_TPC0_QM_GLBL_STS0 + offset);\n\tqm_glbl_sts1 = RREG32(mmDCORE0_TPC0_QM_GLBL_STS1 + offset);\n\tqm_cgm_sts = RREG32(mmDCORE0_TPC0_QM_CGM_STS + offset);\n\n\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts) &&\n\t\t\t\t\t\tIS_TPC_IDLE(tpc_cfg_sts);\n\t*(idle_data->is_idle) &= is_eng_idle;\n\n\tif (idle_data->mask && !is_eng_idle)\n\t\tset_bit(engine_idx, idle_data->mask);\n\n\tif (idle_data->e)\n\t\thl_engine_data_sprintf(idle_data->e,\n\t\t\t\t\tidle_data->tpc_fmt, dcore, inst,\n\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\tqm_glbl_sts0, qm_cgm_sts, tpc_cfg_sts);\n}\n\nstatic bool gaudi2_get_tpc_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tbool is_idle = true;\n\n\tstruct gaudi2_tpc_idle_data tpc_idle_data = {\n\t\t.tpc_fmt = \"%-6d%-5d%-9s%#-14x%#-12x%#x\\n\",\n\t\t.e = e,\n\t\t.mask = mask,\n\t\t.is_idle = &is_idle,\n\t};\n\tstruct iterate_module_ctx tpc_iter = {\n\t\t.fn = &gaudi2_is_tpc_engine_idle,\n\t\t.data = &tpc_idle_data,\n\t};\n\n\tif (e && prop->tpc_enabled_mask)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nCORE  TPC  is_idle  QM_GLBL_STS0  QM_CGM_STS  STATUS\\n\"\n\t\t\t\"----  ---  -------  ------------  ----------  ------\\n\");\n\n\tgaudi2_iterate_tpcs(hdev, &tpc_iter);\n\n\treturn *tpc_idle_data.is_idle;\n}\n\nstatic bool gaudi2_get_decoder_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tconst char *pcie_dec_fmt = \"%-10d%-9s%#x\\n\";\n\tconst char *dec_fmt = \"%-6d%-5d%-9s%#x\\n\";\n\tbool is_idle = true, is_eng_idle;\n\tu32 dec_swreg15, dec_enabled_bit;\n\tint engine_idx, i, j;\n\tu64 offset;\n\n\t \n\tif (e && (prop->decoder_enabled_mask & (~PCIE_DEC_EN_MASK)))\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nCORE  DEC  is_idle  VSI_CMD_SWREG15\\n\"\n\t\t\t\"----  ---  -------  ---------------\\n\");\n\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++) {\n\t\tfor (j = 0 ; j < NUM_OF_DEC_PER_DCORE ; j++) {\n\t\t\tdec_enabled_bit = 1 << (i * NUM_OF_DEC_PER_DCORE + j);\n\t\t\tif (!(prop->decoder_enabled_mask & dec_enabled_bit))\n\t\t\t\tcontinue;\n\n\t\t\tengine_idx = GAUDI2_DCORE0_ENGINE_ID_DEC_0 +\n\t\t\t\t\ti * GAUDI2_ENGINE_ID_DCORE_OFFSET + j;\n\t\t\toffset = i * DCORE_OFFSET + j * DCORE_DEC_OFFSET;\n\n\t\t\tdec_swreg15 = RREG32(mmDCORE0_DEC0_CMD_SWREG15 + offset);\n\t\t\tis_eng_idle = IS_DEC_IDLE(dec_swreg15);\n\t\t\tis_idle &= is_eng_idle;\n\n\t\t\tif (mask && !is_eng_idle)\n\t\t\t\tset_bit(engine_idx, mask);\n\n\t\t\tif (e)\n\t\t\t\thl_engine_data_sprintf(e, dec_fmt, i, j,\n\t\t\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\", dec_swreg15);\n\t\t}\n\t}\n\n\tif (e && (prop->decoder_enabled_mask & PCIE_DEC_EN_MASK))\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nPCIe DEC  is_idle  VSI_CMD_SWREG15\\n\"\n\t\t\t\"--------  -------  ---------------\\n\");\n\n\t \n\tfor (i = 0 ; i < NUM_OF_DEC_PER_DCORE ; i++) {\n\t\tdec_enabled_bit = PCIE_DEC_SHIFT + i;\n\t\tif (!(prop->decoder_enabled_mask & BIT(dec_enabled_bit)))\n\t\t\tcontinue;\n\n\t\tengine_idx = GAUDI2_PCIE_ENGINE_ID_DEC_0 + i;\n\t\toffset = i * DCORE_DEC_OFFSET;\n\t\tdec_swreg15 = RREG32(mmPCIE_DEC0_CMD_SWREG15 + offset);\n\t\tis_eng_idle = IS_DEC_IDLE(dec_swreg15);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(engine_idx, mask);\n\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, pcie_dec_fmt, i,\n\t\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\", dec_swreg15);\n\t}\n\n\treturn is_idle;\n}\n\nstatic bool gaudi2_get_rotator_idle_status(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tconst char *rot_fmt = \"%-6d%-5d%-9s%#-14x%#-14x%#x\\n\";\n\tunsigned long *mask = (unsigned long *) mask_arr;\n\tu32 qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts;\n\tbool is_idle = true, is_eng_idle;\n\tint engine_idx, i;\n\tu64 offset;\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nCORE  ROT  is_idle  QM_GLBL_STS0  QM_GLBL_STS1  QM_CGM_STS\\n\"\n\t\t\t\"----  ---  -------  ------------  ------------  ----------\\n\");\n\n\tfor (i = 0 ; i < NUM_OF_ROT ; i++) {\n\t\tengine_idx = GAUDI2_ENGINE_ID_ROT_0 + i;\n\n\t\toffset = i * ROT_OFFSET;\n\n\t\tqm_glbl_sts0 = RREG32(mmROT0_QM_GLBL_STS0 + offset);\n\t\tqm_glbl_sts1 = RREG32(mmROT0_QM_GLBL_STS1 + offset);\n\t\tqm_cgm_sts = RREG32(mmROT0_QM_CGM_STS + offset);\n\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(engine_idx, mask);\n\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, rot_fmt, i, 0, is_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\tqm_glbl_sts0, qm_glbl_sts1, qm_cgm_sts);\n\t}\n\n\treturn is_idle;\n}\n\nstatic bool gaudi2_is_device_idle(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\t\t\t\tstruct engines_data *e)\n{\n\tbool is_idle = true;\n\n\tis_idle &= gaudi2_get_edma_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_pdma_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_nic_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_mme_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_tpc_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_decoder_idle_status(hdev, mask_arr, mask_len, e);\n\tis_idle &= gaudi2_get_rotator_idle_status(hdev, mask_arr, mask_len, e);\n\n\treturn is_idle;\n}\n\nstatic void gaudi2_hw_queues_lock(struct hl_device *hdev)\n\t__acquires(&gaudi2->hw_queues_lock)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tspin_lock(&gaudi2->hw_queues_lock);\n}\n\nstatic void gaudi2_hw_queues_unlock(struct hl_device *hdev)\n\t__releases(&gaudi2->hw_queues_lock)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tspin_unlock(&gaudi2->hw_queues_lock);\n}\n\nstatic u32 gaudi2_get_pci_id(struct hl_device *hdev)\n{\n\treturn hdev->pdev->device;\n}\n\nstatic int gaudi2_get_eeprom_data(struct hl_device *hdev, void *data, size_t max_size)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_get_eeprom_data(hdev, data, max_size);\n}\n\nstatic void gaudi2_update_eq_ci(struct hl_device *hdev, u32 val)\n{\n\tWREG32(mmCPU_IF_EQ_RD_OFFS, val);\n}\n\nstatic void *gaudi2_get_events_stat(struct hl_device *hdev, bool aggregate, u32 *size)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (aggregate) {\n\t\t*size = (u32) sizeof(gaudi2->events_stat_aggregate);\n\t\treturn gaudi2->events_stat_aggregate;\n\t}\n\n\t*size = (u32) sizeof(gaudi2->events_stat);\n\treturn gaudi2->events_stat;\n}\n\nstatic void gaudi2_mmu_vdec_dcore_prepare(struct hl_device *hdev, int dcore_id,\n\t\t\t\tint dcore_vdec_id, u32 rw_asid, u32 rw_mmu_bp)\n{\n\tu32 offset = (mmDCORE0_VDEC1_BRDG_CTRL_BASE - mmDCORE0_VDEC0_BRDG_CTRL_BASE) *\n\t\t\tdcore_vdec_id + DCORE_OFFSET * dcore_id;\n\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_DEC_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_DEC_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_ABNRM_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_ABNRM_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_L2C_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_L2C_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_NRM_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_NRM_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_VCD_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmDCORE0_VDEC0_BRDG_CTRL_AXUSER_MSIX_VCD_HB_ASID + offset, rw_asid);\n}\n\nstatic void gaudi2_mmu_dcore_prepare(struct hl_device *hdev, int dcore_id, u32 asid)\n{\n\tu32 rw_asid = (asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_RD_SHIFT) |\n\t\t\t(asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_WR_SHIFT);\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 dcore_offset = dcore_id * DCORE_OFFSET;\n\tu32 vdec_id, i, ports_offset, reg_val;\n\tu8 edma_seq_base;\n\n\t \n\tedma_seq_base = dcore_id * NUM_OF_EDMA_PER_DCORE;\n\tif (prop->edma_enabled_mask & BIT(edma_seq_base)) {\n\t\tWREG32(mmDCORE0_EDMA0_QM_AXUSER_NONSECURED_HB_MMU_BP + dcore_offset, 0);\n\t\tWREG32(mmDCORE0_EDMA0_QM_AXUSER_NONSECURED_HB_ASID + dcore_offset, rw_asid);\n\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_AXUSER_HB_MMU_BP + dcore_offset, 0);\n\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_AXUSER_HB_ASID + dcore_offset, rw_asid);\n\t}\n\n\tif (prop->edma_enabled_mask & BIT(edma_seq_base + 1)) {\n\t\tWREG32(mmDCORE0_EDMA1_QM_AXUSER_NONSECURED_HB_MMU_BP + dcore_offset, 0);\n\t\tWREG32(mmDCORE0_EDMA1_QM_AXUSER_NONSECURED_HB_ASID + dcore_offset, rw_asid);\n\t\tWREG32(mmDCORE0_EDMA1_CORE_CTX_AXUSER_HB_ASID + dcore_offset, rw_asid);\n\t\tWREG32(mmDCORE0_EDMA1_CORE_CTX_AXUSER_HB_MMU_BP + dcore_offset, 0);\n\t}\n\n\t \n\tWREG32(mmDCORE0_SYNC_MNGR_GLBL_ASID_NONE_SEC_PRIV + dcore_offset, asid);\n\t \n\tif (dcore_id > 0) {\n\t\treg_val = (asid << DCORE0_SYNC_MNGR_MSTR_IF_AXUSER_HB_ASID_RD_SHIFT) |\n\t\t\t  (asid << DCORE0_SYNC_MNGR_MSTR_IF_AXUSER_HB_ASID_WR_SHIFT);\n\t\tWREG32(mmDCORE0_SYNC_MNGR_MSTR_IF_AXUSER_HB_ASID + dcore_offset, reg_val);\n\t\tWREG32(mmDCORE0_SYNC_MNGR_MSTR_IF_AXUSER_HB_MMU_BP + dcore_offset, 0);\n\t}\n\n\tWREG32(mmDCORE0_MME_CTRL_LO_MME_AXUSER_HB_MMU_BP + dcore_offset, 0);\n\tWREG32(mmDCORE0_MME_CTRL_LO_MME_AXUSER_HB_ASID + dcore_offset, rw_asid);\n\n\tfor (i = 0 ; i < NUM_OF_MME_SBTE_PORTS ; i++) {\n\t\tports_offset = i * DCORE_MME_SBTE_OFFSET;\n\t\tWREG32(mmDCORE0_MME_SBTE0_MSTR_IF_AXUSER_HB_MMU_BP +\n\t\t\t\tdcore_offset + ports_offset, 0);\n\t\tWREG32(mmDCORE0_MME_SBTE0_MSTR_IF_AXUSER_HB_ASID +\n\t\t\t\tdcore_offset + ports_offset, rw_asid);\n\t}\n\n\tfor (i = 0 ; i < NUM_OF_MME_WB_PORTS ; i++) {\n\t\tports_offset = i * DCORE_MME_WB_OFFSET;\n\t\tWREG32(mmDCORE0_MME_WB0_MSTR_IF_AXUSER_HB_MMU_BP +\n\t\t\t\tdcore_offset + ports_offset, 0);\n\t\tWREG32(mmDCORE0_MME_WB0_MSTR_IF_AXUSER_HB_ASID +\n\t\t\t\tdcore_offset + ports_offset, rw_asid);\n\t}\n\n\tWREG32(mmDCORE0_MME_QM_AXUSER_NONSECURED_HB_MMU_BP + dcore_offset, 0);\n\tWREG32(mmDCORE0_MME_QM_AXUSER_NONSECURED_HB_ASID + dcore_offset, rw_asid);\n\n\t \n\tfor (vdec_id = 0 ; vdec_id < NUM_OF_DEC_PER_DCORE ; vdec_id++) {\n\t\tif (prop->decoder_enabled_mask & BIT(dcore_id * NUM_OF_DEC_PER_DCORE + vdec_id))\n\t\t\tgaudi2_mmu_vdec_dcore_prepare(hdev, dcore_id, vdec_id, rw_asid, 0);\n\t}\n}\n\nstatic void gudi2_mmu_vdec_shared_prepare(struct hl_device *hdev,\n\t\t\t\tint shared_vdec_id, u32 rw_asid, u32 rw_mmu_bp)\n{\n\tu32 offset = (mmPCIE_VDEC1_BRDG_CTRL_BASE - mmPCIE_VDEC0_BRDG_CTRL_BASE) * shared_vdec_id;\n\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_DEC_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_DEC_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_ABNRM_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_ABNRM_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_L2C_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_L2C_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_NRM_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_NRM_HB_ASID + offset, rw_asid);\n\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_VCD_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmPCIE_VDEC0_BRDG_CTRL_AXUSER_MSIX_VCD_HB_ASID + offset, rw_asid);\n}\n\nstatic void gudi2_mmu_arc_farm_arc_dup_eng_prepare(struct hl_device *hdev, int arc_farm_id,\n\t\t\t\t\t\t\tu32 rw_asid, u32 rw_mmu_bp)\n{\n\tu32 offset = (mmARC_FARM_ARC1_DUP_ENG_BASE - mmARC_FARM_ARC0_DUP_ENG_BASE) * arc_farm_id;\n\n\tWREG32(mmARC_FARM_ARC0_DUP_ENG_AXUSER_HB_MMU_BP + offset, rw_mmu_bp);\n\tWREG32(mmARC_FARM_ARC0_DUP_ENG_AXUSER_HB_ASID + offset, rw_asid);\n}\n\nstatic void gaudi2_arc_mmu_prepare(struct hl_device *hdev, u32 cpu_id, u32 asid)\n{\n\tu32 reg_base, reg_offset, reg_val = 0;\n\n\treg_base = gaudi2_arc_blocks_bases[cpu_id];\n\n\t \n\treg_val = FIELD_PREP(ARC_FARM_ARC0_AUX_ARC_REGION_CFG_MMU_BP_MASK, 0);\n\treg_val |= FIELD_PREP(ARC_FARM_ARC0_AUX_ARC_REGION_CFG_0_ASID_MASK, asid);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION3_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION4_HBM0_FW);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION5_HBM1_GC_DATA);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION6_HBM2_GC_DATA);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION7_HBM3_GC_DATA);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION9_PCIE);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION10_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION11_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION12_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION13_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n\n\treg_offset = ARC_REGION_CFG_OFFSET(ARC_REGION14_GENERAL);\n\tWREG32(reg_base + reg_offset, reg_val);\n}\n\nstatic int gaudi2_arc_mmu_prepare_all(struct hl_device *hdev, u32 asid)\n{\n\tint i;\n\n\tif (hdev->fw_components & FW_TYPE_BOOT_CPU)\n\t\treturn hl_fw_cpucp_engine_core_asid_set(hdev, asid);\n\n\tfor (i = CPU_ID_SCHED_ARC0 ; i < NUM_OF_ARC_FARMS_ARC ; i++)\n\t\tgaudi2_arc_mmu_prepare(hdev, i, asid);\n\n\tfor (i = GAUDI2_QUEUE_ID_PDMA_0_0 ; i < GAUDI2_QUEUE_ID_CPU_PQ ; i += 4) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, i))\n\t\t\tcontinue;\n\n\t\tgaudi2_arc_mmu_prepare(hdev, gaudi2_queue_id_to_arc_id[i], asid);\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_mmu_shared_prepare(struct hl_device *hdev, u32 asid)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 rw_asid, offset;\n\tint rc, i;\n\n\trw_asid = FIELD_PREP(ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_RD_MASK, asid) |\n\t\t\tFIELD_PREP(ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_WR_MASK, asid);\n\n\tWREG32(mmPDMA0_QM_AXUSER_NONSECURED_HB_ASID, rw_asid);\n\tWREG32(mmPDMA0_QM_AXUSER_NONSECURED_HB_MMU_BP, 0);\n\tWREG32(mmPDMA0_CORE_CTX_AXUSER_HB_ASID, rw_asid);\n\tWREG32(mmPDMA0_CORE_CTX_AXUSER_HB_MMU_BP, 0);\n\n\tWREG32(mmPDMA1_QM_AXUSER_NONSECURED_HB_ASID, rw_asid);\n\tWREG32(mmPDMA1_QM_AXUSER_NONSECURED_HB_MMU_BP, 0);\n\tWREG32(mmPDMA1_CORE_CTX_AXUSER_HB_ASID, rw_asid);\n\tWREG32(mmPDMA1_CORE_CTX_AXUSER_HB_MMU_BP, 0);\n\n\t \n\tfor (i = 0 ; i < NUM_OF_ROT ; i++) {\n\t\toffset = i * ROT_OFFSET;\n\t\tWREG32(mmROT0_QM_AXUSER_NONSECURED_HB_ASID + offset, rw_asid);\n\t\tWREG32(mmROT0_QM_AXUSER_NONSECURED_HB_MMU_BP + offset, 0);\n\t\tRMWREG32(mmROT0_CPL_QUEUE_AWUSER + offset, asid, MMUBP_ASID_MASK);\n\t\tRMWREG32(mmROT0_DESC_HBW_ARUSER_LO + offset, asid, MMUBP_ASID_MASK);\n\t\tRMWREG32(mmROT0_DESC_HBW_AWUSER_LO + offset, asid, MMUBP_ASID_MASK);\n\t}\n\n\t \n\tif (prop->decoder_enabled_mask & BIT(NUM_OF_DCORES * NUM_OF_DEC_PER_DCORE + 0))\n\t\tgudi2_mmu_vdec_shared_prepare(hdev, 0, rw_asid, 0);\n\n\tif (prop->decoder_enabled_mask & BIT(NUM_OF_DCORES * NUM_OF_DEC_PER_DCORE + 1))\n\t\tgudi2_mmu_vdec_shared_prepare(hdev, 1, rw_asid, 0);\n\n\t \n\tfor (i = 0 ; i < NUM_OF_ARC_FARMS_ARC ; i++)\n\t\tgudi2_mmu_arc_farm_arc_dup_eng_prepare(hdev, i, rw_asid, 0);\n\n\trc = gaudi2_arc_mmu_prepare_all(hdev, asid);\n\tif (rc)\n\t\treturn rc;\n\n\treturn 0;\n}\n\nstatic void gaudi2_tpc_mmu_prepare(struct hl_device *hdev, int dcore, int inst,\tu32 offset,\n\t\t\t\t\tstruct iterate_module_ctx *ctx)\n{\n\tstruct gaudi2_tpc_mmu_data *mmu_data = ctx->data;\n\n\tWREG32(mmDCORE0_TPC0_CFG_AXUSER_HB_MMU_BP + offset, 0);\n\tWREG32(mmDCORE0_TPC0_CFG_AXUSER_HB_ASID + offset, mmu_data->rw_asid);\n\tWREG32(mmDCORE0_TPC0_QM_AXUSER_NONSECURED_HB_MMU_BP + offset, 0);\n\tWREG32(mmDCORE0_TPC0_QM_AXUSER_NONSECURED_HB_ASID + offset, mmu_data->rw_asid);\n}\n\n \nstatic int gaudi2_mmu_prepare(struct hl_device *hdev, u32 asid)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tstruct gaudi2_tpc_mmu_data tpc_mmu_data;\n\tstruct iterate_module_ctx tpc_iter = {\n\t\t.fn = &gaudi2_tpc_mmu_prepare,\n\t\t.data = &tpc_mmu_data,\n\t};\n\tint rc, i;\n\n\tif (asid & ~DCORE0_HMMU0_STLB_ASID_ASID_MASK) {\n\t\tdev_crit(hdev->dev, \"asid %u is too big\\n\", asid);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_MMU_MASK))\n\t\treturn 0;\n\n\trc = gaudi2_mmu_shared_prepare(hdev, asid);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\ttpc_mmu_data.rw_asid = (asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_RD_SHIFT) |\n\t\t\t\t(asid << ARC_FARM_KDMA_CTX_AXUSER_HB_ASID_WR_SHIFT);\n\tgaudi2_iterate_tpcs(hdev, &tpc_iter);\n\tfor (i = 0 ; i < NUM_OF_DCORES ; i++)\n\t\tgaudi2_mmu_dcore_prepare(hdev, i, asid);\n\n\treturn 0;\n}\n\nstatic inline bool is_info_event(u32 event)\n{\n\tswitch (event) {\n\tcase GAUDI2_EVENT_CPU_CPLD_SHUTDOWN_CAUSE:\n\tcase GAUDI2_EVENT_CPU_FIX_POWER_ENV_S ... GAUDI2_EVENT_CPU_FIX_THERMAL_ENV_E:\n\n\t \n\tcase GAUDI2_EVENT_CPU0_STATUS_NIC0_ENG0 ... GAUDI2_EVENT_CPU11_STATUS_NIC11_ENG1:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void gaudi2_print_event(struct hl_device *hdev, u16 event_type,\n\t\t\tbool ratelimited, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\n\tva_start(args, fmt);\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\n\tif (ratelimited)\n\t\tdev_err_ratelimited(hdev->dev, \"%s: %pV\\n\",\n\t\t\tgaudi2_irq_map_table[event_type].valid ?\n\t\t\tgaudi2_irq_map_table[event_type].name : \"N/A Event\", &vaf);\n\telse\n\t\tdev_err(hdev->dev, \"%s: %pV\\n\",\n\t\t\tgaudi2_irq_map_table[event_type].valid ?\n\t\t\tgaudi2_irq_map_table[event_type].name : \"N/A Event\", &vaf);\n\n\tva_end(args);\n}\n\nstatic bool gaudi2_handle_ecc_event(struct hl_device *hdev, u16 event_type,\n\t\tstruct hl_eq_ecc_data *ecc_data)\n{\n\tu64 ecc_address = 0, ecc_syndrom = 0;\n\tu8 memory_wrapper_idx = 0;\n\n\tecc_address = le64_to_cpu(ecc_data->ecc_address);\n\tecc_syndrom = le64_to_cpu(ecc_data->ecc_syndrom);\n\tmemory_wrapper_idx = ecc_data->memory_wrapper_idx;\n\n\tgaudi2_print_event(hdev, event_type, !ecc_data->is_critical,\n\t\t\"ECC error detected. address: %#llx. Syndrom: %#llx. block id %u. critical %u.\",\n\t\tecc_address, ecc_syndrom, memory_wrapper_idx, ecc_data->is_critical);\n\n\treturn !!ecc_data->is_critical;\n}\n\nstatic void print_lower_qman_data_on_err(struct hl_device *hdev, u64 qman_base)\n{\n\tu32 lo, hi, cq_ptr_size, arc_cq_ptr_size;\n\tu64 cq_ptr, arc_cq_ptr, cp_current_inst;\n\n\tlo = RREG32(qman_base + QM_CQ_PTR_LO_4_OFFSET);\n\thi = RREG32(qman_base + QM_CQ_PTR_HI_4_OFFSET);\n\tcq_ptr = ((u64) hi) << 32 | lo;\n\tcq_ptr_size = RREG32(qman_base + QM_CQ_TSIZE_4_OFFSET);\n\n\tlo = RREG32(qman_base + QM_ARC_CQ_PTR_LO_OFFSET);\n\thi = RREG32(qman_base + QM_ARC_CQ_PTR_HI_OFFSET);\n\tarc_cq_ptr = ((u64) hi) << 32 | lo;\n\tarc_cq_ptr_size = RREG32(qman_base + QM_ARC_CQ_TSIZE_OFFSET);\n\n\tlo = RREG32(qman_base + QM_CP_CURRENT_INST_LO_4_OFFSET);\n\thi = RREG32(qman_base + QM_CP_CURRENT_INST_HI_4_OFFSET);\n\tcp_current_inst = ((u64) hi) << 32 | lo;\n\n\tdev_info(hdev->dev,\n\t\t\"LowerQM. CQ: {ptr %#llx, size %u}, ARC_CQ: {ptr %#llx, size %u}, CP: {instruction %#llx}\\n\",\n\t\tcq_ptr, cq_ptr_size, arc_cq_ptr, arc_cq_ptr_size, cp_current_inst);\n}\n\nstatic int gaudi2_handle_qman_err_generic(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\t\t\tu64 qman_base, u32 qid_base)\n{\n\tu32 i, j, glbl_sts_val, arb_err_val, num_error_causes, error_count = 0;\n\tu64 glbl_sts_addr, arb_err_addr;\n\tchar reg_desc[32];\n\n\tglbl_sts_addr = qman_base + (mmDCORE0_TPC0_QM_GLBL_ERR_STS_0 - mmDCORE0_TPC0_QM_BASE);\n\tarb_err_addr = qman_base + (mmDCORE0_TPC0_QM_ARB_ERR_CAUSE - mmDCORE0_TPC0_QM_BASE);\n\n\t \n\tfor (i = 0 ; i < QMAN_STREAMS + 1 ; i++) {\n\t\tglbl_sts_val = RREG32(glbl_sts_addr + 4 * i);\n\n\t\tif (!glbl_sts_val)\n\t\t\tcontinue;\n\n\t\tif (i == QMAN_STREAMS) {\n\t\t\tsnprintf(reg_desc, ARRAY_SIZE(reg_desc), \"LowerQM\");\n\t\t\tnum_error_causes = GAUDI2_NUM_OF_LOWER_QM_ERR_CAUSE;\n\t\t} else {\n\t\t\tsnprintf(reg_desc, ARRAY_SIZE(reg_desc), \"stream%u\", i);\n\t\t\tnum_error_causes = GAUDI2_NUM_OF_QM_ERR_CAUSE;\n\t\t}\n\n\t\tfor (j = 0 ; j < num_error_causes ; j++)\n\t\t\tif (glbl_sts_val & BIT(j)) {\n\t\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\t\"%s. err cause: %s\", reg_desc,\n\t\t\t\t\ti == QMAN_STREAMS ?\n\t\t\t\t\tgaudi2_lower_qman_error_cause[j] :\n\t\t\t\t\tgaudi2_qman_error_cause[j]);\n\t\t\t\terror_count++;\n\t\t\t}\n\n\t\tif (i == QMAN_STREAMS)\n\t\t\tprint_lower_qman_data_on_err(hdev, qman_base);\n\t}\n\n\tarb_err_val = RREG32(arb_err_addr);\n\n\tif (!arb_err_val)\n\t\tgoto out;\n\n\tfor (j = 0 ; j < GAUDI2_NUM_OF_QM_ARB_ERR_CAUSE ; j++) {\n\t\tif (arb_err_val & BIT(j)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"ARB_ERR. err cause: %s\",\n\t\t\t\tgaudi2_qman_arb_error_cause[j]);\n\t\t\terror_count++;\n\t\t}\n\t}\n\nout:\n\treturn error_count;\n}\n\nstatic void gaudi2_razwi_rr_hbw_shared_printf_info(struct hl_device *hdev,\n\t\t\tu64 rtr_mstr_if_base_addr, bool is_write, char *name,\n\t\t\tenum gaudi2_engine_id id, u64 *event_mask)\n{\n\tu32 razwi_hi, razwi_lo, razwi_xy;\n\tu16 eng_id = id;\n\tu8 rd_wr_flag;\n\n\tif (is_write) {\n\t\trazwi_hi = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_HI);\n\t\trazwi_lo = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_LO);\n\t\trazwi_xy = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_XY);\n\t\trd_wr_flag = HL_RAZWI_WRITE;\n\t} else {\n\t\trazwi_hi = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_HI);\n\t\trazwi_lo = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_LO);\n\t\trazwi_xy = RREG32(rtr_mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_XY);\n\t\trd_wr_flag = HL_RAZWI_READ;\n\t}\n\n\thl_handle_razwi(hdev, (u64)razwi_hi << 32 | razwi_lo, &eng_id, 1,\n\t\t\t\trd_wr_flag | HL_RAZWI_HBW, event_mask);\n\n\tdev_err_ratelimited(hdev->dev,\n\t\t\"%s-RAZWI SHARED RR HBW %s error, address %#llx, Initiator coordinates 0x%x\\n\",\n\t\tname, is_write ? \"WR\" : \"RD\", (u64)razwi_hi << 32 | razwi_lo, razwi_xy);\n}\n\nstatic void gaudi2_razwi_rr_lbw_shared_printf_info(struct hl_device *hdev,\n\t\t\tu64 rtr_mstr_if_base_addr, bool is_write, char *name,\n\t\t\tenum gaudi2_engine_id id, u64 *event_mask)\n{\n\tu64 razwi_addr = CFG_BASE;\n\tu32 razwi_xy;\n\tu16 eng_id = id;\n\tu8 rd_wr_flag;\n\n\tif (is_write) {\n\t\trazwi_addr += RREG32(rtr_mstr_if_base_addr + RR_SHRD_LBW_AW_RAZWI);\n\t\trazwi_xy = RREG32(rtr_mstr_if_base_addr + RR_SHRD_LBW_AW_RAZWI_XY);\n\t\trd_wr_flag = HL_RAZWI_WRITE;\n\t} else {\n\t\trazwi_addr += RREG32(rtr_mstr_if_base_addr + RR_SHRD_LBW_AR_RAZWI);\n\t\trazwi_xy = RREG32(rtr_mstr_if_base_addr + RR_SHRD_LBW_AR_RAZWI_XY);\n\t\trd_wr_flag = HL_RAZWI_READ;\n\t}\n\n\thl_handle_razwi(hdev, razwi_addr, &eng_id, 1, rd_wr_flag | HL_RAZWI_LBW, event_mask);\n\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"%s-RAZWI SHARED RR LBW %s error, mstr_if 0x%llx, captured address 0x%llX Initiator coordinates 0x%x\\n\",\n\t\t\t\tname, is_write ? \"WR\" : \"RD\", rtr_mstr_if_base_addr, razwi_addr,\n\t\t\t\t\t\trazwi_xy);\n}\n\nstatic enum gaudi2_engine_id gaudi2_razwi_calc_engine_id(struct hl_device *hdev,\n\t\t\t\t\t\tenum razwi_event_sources module, u8 module_idx)\n{\n\tswitch (module) {\n\tcase RAZWI_TPC:\n\t\tif (module_idx == (NUM_OF_TPC_PER_DCORE * NUM_OF_DCORES))\n\t\t\treturn GAUDI2_DCORE0_ENGINE_ID_TPC_6;\n\t\treturn (((module_idx / NUM_OF_TPC_PER_DCORE) * ENGINE_ID_DCORE_OFFSET) +\n\t\t\t\t(module_idx % NUM_OF_TPC_PER_DCORE) +\n\t\t\t\t(GAUDI2_DCORE0_ENGINE_ID_TPC_0 - GAUDI2_DCORE0_ENGINE_ID_EDMA_0));\n\n\tcase RAZWI_MME:\n\t\treturn ((GAUDI2_DCORE0_ENGINE_ID_MME - GAUDI2_DCORE0_ENGINE_ID_EDMA_0) +\n\t\t\t(module_idx * ENGINE_ID_DCORE_OFFSET));\n\n\tcase RAZWI_EDMA:\n\t\treturn (((module_idx / NUM_OF_EDMA_PER_DCORE) * ENGINE_ID_DCORE_OFFSET) +\n\t\t\t(module_idx % NUM_OF_EDMA_PER_DCORE));\n\n\tcase RAZWI_PDMA:\n\t\treturn (GAUDI2_ENGINE_ID_PDMA_0 + module_idx);\n\n\tcase RAZWI_NIC:\n\t\treturn (GAUDI2_ENGINE_ID_NIC0_0 + (NIC_NUMBER_OF_QM_PER_MACRO * module_idx));\n\n\tcase RAZWI_DEC:\n\t\tif (module_idx == 8)\n\t\t\treturn GAUDI2_PCIE_ENGINE_ID_DEC_0;\n\n\t\tif (module_idx == 9)\n\t\t\treturn GAUDI2_PCIE_ENGINE_ID_DEC_1;\n\t\t\t\t\t;\n\t\treturn (((module_idx / NUM_OF_DEC_PER_DCORE) * ENGINE_ID_DCORE_OFFSET) +\n\t\t\t\t(module_idx % NUM_OF_DEC_PER_DCORE) +\n\t\t\t\t(GAUDI2_DCORE0_ENGINE_ID_DEC_0 - GAUDI2_DCORE0_ENGINE_ID_EDMA_0));\n\n\tcase RAZWI_ROT:\n\t\treturn GAUDI2_ENGINE_ID_ROT_0 + module_idx;\n\n\tdefault:\n\t\treturn GAUDI2_ENGINE_ID_SIZE;\n\t}\n}\n\n \nstatic void gaudi2_ack_module_razwi_event_handler(struct hl_device *hdev,\n\t\t\t\tenum razwi_event_sources module, u8 module_idx,\n\t\t\t\tu8 module_sub_idx, u64 *event_mask)\n{\n\tbool via_sft = false;\n\tu32 hbw_rtr_id, lbw_rtr_id, dcore_id, dcore_rtr_id, eng_id, binned_idx;\n\tu64 hbw_rtr_mstr_if_base_addr, lbw_rtr_mstr_if_base_addr;\n\tu32 hbw_shrd_aw = 0, hbw_shrd_ar = 0;\n\tu32 lbw_shrd_aw = 0, lbw_shrd_ar = 0;\n\tchar initiator_name[64];\n\n\tswitch (module) {\n\tcase RAZWI_TPC:\n\t\tsprintf(initiator_name, \"TPC_%u\", module_idx);\n\t\tif (hdev->tpc_binning) {\n\t\t\tbinned_idx = __ffs(hdev->tpc_binning);\n\t\t\tif (binned_idx == module_idx)\n\t\t\t\tmodule_idx = TPC_ID_DCORE0_TPC6;\n\t\t}\n\n\t\thbw_rtr_id = gaudi2_tpc_initiator_hbw_rtr_id[module_idx];\n\n\t\tif (hl_is_fw_sw_ver_below(hdev, 1, 9) &&\n\t\t\t\t!hdev->asic_prop.fw_security_enabled &&\n\t\t\t\t((module_idx == 0) || (module_idx == 1)))\n\t\t\tlbw_rtr_id = DCORE0_RTR0;\n\t\telse\n\t\t\tlbw_rtr_id = gaudi2_tpc_initiator_lbw_rtr_id[module_idx];\n\t\tbreak;\n\tcase RAZWI_MME:\n\t\tsprintf(initiator_name, \"MME_%u\", module_idx);\n\t\tswitch (module_sub_idx) {\n\t\tcase MME_WAP0:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].wap0;\n\t\t\tbreak;\n\t\tcase MME_WAP1:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].wap1;\n\t\t\tbreak;\n\t\tcase MME_WRITE:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].write;\n\t\t\tbreak;\n\t\tcase MME_READ:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].read;\n\t\t\tbreak;\n\t\tcase MME_SBTE0:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].sbte0;\n\t\t\tbreak;\n\t\tcase MME_SBTE1:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].sbte1;\n\t\t\tbreak;\n\t\tcase MME_SBTE2:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].sbte2;\n\t\t\tbreak;\n\t\tcase MME_SBTE3:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].sbte3;\n\t\t\tbreak;\n\t\tcase MME_SBTE4:\n\t\t\thbw_rtr_id = gaudi2_mme_initiator_rtr_id[module_idx].sbte4;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn;\n\t\t}\n\t\tlbw_rtr_id = hbw_rtr_id;\n\t\tbreak;\n\tcase RAZWI_EDMA:\n\t\thbw_rtr_mstr_if_base_addr = gaudi2_edma_initiator_hbw_sft[module_idx];\n\t\tdcore_id = module_idx / NUM_OF_EDMA_PER_DCORE;\n\t\t \n\t\tlbw_rtr_mstr_if_base_addr = mmSFT0_LBW_RTR_IF_MSTR_IF_RR_SHRD_HBW_BASE +\n\t\t\t\t\t\t\t\tdcore_id * SFT_DCORE_OFFSET;\n\t\tvia_sft = true;\n\t\tsprintf(initiator_name, \"EDMA_%u\", module_idx);\n\t\tbreak;\n\tcase RAZWI_PDMA:\n\t\thbw_rtr_id = gaudi2_pdma_initiator_hbw_rtr_id[module_idx];\n\t\tlbw_rtr_id = gaudi2_pdma_initiator_lbw_rtr_id[module_idx];\n\t\tsprintf(initiator_name, \"PDMA_%u\", module_idx);\n\t\tbreak;\n\tcase RAZWI_NIC:\n\t\thbw_rtr_id = gaudi2_nic_initiator_hbw_rtr_id[module_idx];\n\t\tlbw_rtr_id = gaudi2_nic_initiator_lbw_rtr_id[module_idx];\n\t\tsprintf(initiator_name, \"NIC_%u\", module_idx);\n\t\tbreak;\n\tcase RAZWI_DEC:\n\t\tsprintf(initiator_name, \"DEC_%u\", module_idx);\n\t\tif (hdev->decoder_binning) {\n\t\t\tbinned_idx = __ffs(hdev->decoder_binning);\n\t\t\tif (binned_idx == module_idx)\n\t\t\t\tmodule_idx = DEC_ID_PCIE_VDEC1;\n\t\t}\n\t\thbw_rtr_id = gaudi2_dec_initiator_hbw_rtr_id[module_idx];\n\t\tlbw_rtr_id = gaudi2_dec_initiator_lbw_rtr_id[module_idx];\n\t\tbreak;\n\tcase RAZWI_ROT:\n\t\thbw_rtr_id = gaudi2_rot_initiator_hbw_rtr_id[module_idx];\n\t\tlbw_rtr_id = gaudi2_rot_initiator_lbw_rtr_id[module_idx];\n\t\tsprintf(initiator_name, \"ROT_%u\", module_idx);\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\t \n\tif (!via_sft) {\n\t\tdcore_id = hbw_rtr_id / NUM_OF_RTR_PER_DCORE;\n\t\tdcore_rtr_id = hbw_rtr_id % NUM_OF_RTR_PER_DCORE;\n\t\thbw_rtr_mstr_if_base_addr = mmDCORE0_RTR0_CTRL_BASE +\n\t\t\t\tdcore_id * DCORE_OFFSET +\n\t\t\t\tdcore_rtr_id * DCORE_RTR_OFFSET +\n\t\t\t\tRTR_MSTR_IF_OFFSET;\n\t\tlbw_rtr_mstr_if_base_addr = hbw_rtr_mstr_if_base_addr +\n\t\t\t\t(((s32)lbw_rtr_id - hbw_rtr_id) * DCORE_RTR_OFFSET);\n\t}\n\n\t \n\thbw_shrd_aw = RREG32(hbw_rtr_mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_HAPPENED);\n\thbw_shrd_ar = RREG32(hbw_rtr_mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_HAPPENED);\n\tlbw_shrd_aw = RREG32(lbw_rtr_mstr_if_base_addr + RR_SHRD_LBW_AW_RAZWI_HAPPENED);\n\tlbw_shrd_ar = RREG32(lbw_rtr_mstr_if_base_addr + RR_SHRD_LBW_AR_RAZWI_HAPPENED);\n\n\teng_id = gaudi2_razwi_calc_engine_id(hdev, module, module_idx);\n\tif (hbw_shrd_aw) {\n\t\tgaudi2_razwi_rr_hbw_shared_printf_info(hdev, hbw_rtr_mstr_if_base_addr, true,\n\t\t\t\t\t\tinitiator_name, eng_id, event_mask);\n\n\t\t \n\t\tWREG32(hbw_rtr_mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_HAPPENED, hbw_shrd_aw);\n\t}\n\n\tif (hbw_shrd_ar) {\n\t\tgaudi2_razwi_rr_hbw_shared_printf_info(hdev, hbw_rtr_mstr_if_base_addr, false,\n\t\t\t\t\t\tinitiator_name, eng_id, event_mask);\n\n\t\t \n\t\tWREG32(hbw_rtr_mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_HAPPENED, hbw_shrd_ar);\n\t}\n\n\tif (lbw_shrd_aw) {\n\t\tgaudi2_razwi_rr_lbw_shared_printf_info(hdev, lbw_rtr_mstr_if_base_addr, true,\n\t\t\t\t\t\tinitiator_name, eng_id, event_mask);\n\n\t\t \n\t\tWREG32(lbw_rtr_mstr_if_base_addr + RR_SHRD_LBW_AW_RAZWI_HAPPENED, lbw_shrd_aw);\n\t}\n\n\tif (lbw_shrd_ar) {\n\t\tgaudi2_razwi_rr_lbw_shared_printf_info(hdev, lbw_rtr_mstr_if_base_addr, false,\n\t\t\t\t\t\tinitiator_name, eng_id, event_mask);\n\n\t\t \n\t\tWREG32(lbw_rtr_mstr_if_base_addr + RR_SHRD_LBW_AR_RAZWI_HAPPENED, lbw_shrd_ar);\n\t}\n}\n\nstatic void gaudi2_check_if_razwi_happened(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu8 mod_idx, sub_mod;\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < (NUM_OF_TPC_PER_DCORE * NUM_OF_DCORES + 1) ; mod_idx++) {\n\t\tif (prop->tpc_enabled_mask & BIT(mod_idx))\n\t\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_TPC, mod_idx, 0, NULL);\n\t}\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < (NUM_OF_MME_PER_DCORE * NUM_OF_DCORES) ; mod_idx++)\n\t\tfor (sub_mod = MME_WAP0 ; sub_mod < MME_INITIATORS_MAX ; sub_mod++)\n\t\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_MME, mod_idx,\n\t\t\t\t\t\t\t\t\tsub_mod, NULL);\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < (NUM_OF_EDMA_PER_DCORE * NUM_OF_DCORES) ; mod_idx++)\n\t\tif (prop->edma_enabled_mask & BIT(mod_idx))\n\t\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_EDMA, mod_idx, 0, NULL);\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < NUM_OF_PDMA ; mod_idx++)\n\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_PDMA, mod_idx, 0, NULL);\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < NIC_NUMBER_OF_PORTS ; mod_idx++)\n\t\tif (hdev->nic_ports_mask & BIT(mod_idx))\n\t\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_NIC, mod_idx >> 1, 0,\n\t\t\t\t\t\t\t\tNULL);\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < NUMBER_OF_DEC ; mod_idx++)\n\t\tif (prop->decoder_enabled_mask & BIT(mod_idx))\n\t\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_DEC, mod_idx, 0, NULL);\n\n\t \n\tfor (mod_idx = 0 ; mod_idx < NUM_OF_ROT ; mod_idx++)\n\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_ROT, mod_idx, 0, NULL);\n}\n\nstatic int gaudi2_psoc_razwi_get_engines(struct gaudi2_razwi_info *razwi_info, u32 array_size,\n\t\t\t\t\t\tu32 axuser_xy, u32 *base, u16 *eng_id,\n\t\t\t\t\t\tchar *eng_name)\n{\n\n\tint i, num_of_eng = 0;\n\tu16 str_size = 0;\n\n\tfor (i = 0 ; i < array_size ; i++) {\n\t\tif (axuser_xy != razwi_info[i].axuser_xy)\n\t\t\tcontinue;\n\n\t\teng_id[num_of_eng] = razwi_info[i].eng_id;\n\t\tbase[num_of_eng] = razwi_info[i].rtr_ctrl;\n\t\tif (!num_of_eng)\n\t\t\tstr_size += scnprintf(eng_name + str_size,\n\t\t\t\t\t\tPSOC_RAZWI_ENG_STR_SIZE - str_size, \"%s\",\n\t\t\t\t\t\trazwi_info[i].eng_name);\n\t\telse\n\t\t\tstr_size += scnprintf(eng_name + str_size,\n\t\t\t\t\t\tPSOC_RAZWI_ENG_STR_SIZE - str_size, \" or %s\",\n\t\t\t\t\t\trazwi_info[i].eng_name);\n\t\tnum_of_eng++;\n\t}\n\n\treturn num_of_eng;\n}\n\nstatic bool gaudi2_handle_psoc_razwi_happened(struct hl_device *hdev, u32 razwi_reg,\n\t\t\t\t\t\tu64 *event_mask)\n{\n\tu32 axuser_xy = RAZWI_GET_AXUSER_XY(razwi_reg), addr_hi = 0, addr_lo = 0;\n\tu32 base[PSOC_RAZWI_MAX_ENG_PER_RTR];\n\tu16 num_of_eng, eng_id[PSOC_RAZWI_MAX_ENG_PER_RTR];\n\tchar eng_name_str[PSOC_RAZWI_ENG_STR_SIZE];\n\tbool razwi_happened = false;\n\tu64 addr;\n\tint i;\n\n\tnum_of_eng = gaudi2_psoc_razwi_get_engines(common_razwi_info, ARRAY_SIZE(common_razwi_info),\n\t\t\t\t\t\t\taxuser_xy, base, eng_id, eng_name_str);\n\n\t \n\tif (!num_of_eng) {\n\t\taxuser_xy = RAZWI_GET_AXUSER_LOW_XY(razwi_reg);\n\t\tnum_of_eng = gaudi2_psoc_razwi_get_engines(mme_razwi_info,\n\t\t\t\t\t\t\t\tARRAY_SIZE(mme_razwi_info),\n\t\t\t\t\t\t\t\taxuser_xy, base, eng_id,\n\t\t\t\t\t\t\t\teng_name_str);\n\t}\n\n\tfor  (i = 0 ; i < num_of_eng ; i++) {\n\t\tif (RREG32(base[i] + DEC_RAZWI_HBW_AW_SET)) {\n\t\t\taddr_hi = RREG32(base[i] + DEC_RAZWI_HBW_AW_ADDR_HI);\n\t\t\taddr_lo = RREG32(base[i] + DEC_RAZWI_HBW_AW_ADDR_LO);\n\t\t\taddr = ((u64)addr_hi << 32) + addr_lo;\n\t\t\tif (addr) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"PSOC HBW AW RAZWI: %s, address (aligned to 128 byte): 0x%llX\\n\",\n\t\t\t\t\teng_name_str, addr);\n\t\t\t\thl_handle_razwi(hdev, addr, &eng_id[0],\n\t\t\t\t\tnum_of_eng, HL_RAZWI_HBW | HL_RAZWI_WRITE, event_mask);\n\t\t\t\trazwi_happened = true;\n\t\t\t}\n\t\t}\n\n\t\tif (RREG32(base[i] + DEC_RAZWI_HBW_AR_SET)) {\n\t\t\taddr_hi = RREG32(base[i] + DEC_RAZWI_HBW_AR_ADDR_HI);\n\t\t\taddr_lo = RREG32(base[i] + DEC_RAZWI_HBW_AR_ADDR_LO);\n\t\t\taddr = ((u64)addr_hi << 32) + addr_lo;\n\t\t\tif (addr) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"PSOC HBW AR RAZWI: %s, address (aligned to 128 byte): 0x%llX\\n\",\n\t\t\t\t\teng_name_str, addr);\n\t\t\t\thl_handle_razwi(hdev, addr, &eng_id[0],\n\t\t\t\t\tnum_of_eng, HL_RAZWI_HBW | HL_RAZWI_READ, event_mask);\n\t\t\t\trazwi_happened = true;\n\t\t\t}\n\t\t}\n\n\t\tif (RREG32(base[i] + DEC_RAZWI_LBW_AW_SET)) {\n\t\t\taddr_lo = RREG32(base[i] + DEC_RAZWI_LBW_AW_ADDR);\n\t\t\tif (addr_lo) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"PSOC LBW AW RAZWI: %s, address (aligned to 128 byte): 0x%X\\n\",\n\t\t\t\t\teng_name_str, addr_lo);\n\t\t\t\thl_handle_razwi(hdev, addr_lo, &eng_id[0],\n\t\t\t\t\tnum_of_eng, HL_RAZWI_LBW | HL_RAZWI_WRITE, event_mask);\n\t\t\t\trazwi_happened = true;\n\t\t\t}\n\t\t}\n\n\t\tif (RREG32(base[i] + DEC_RAZWI_LBW_AR_SET)) {\n\t\t\taddr_lo = RREG32(base[i] + DEC_RAZWI_LBW_AR_ADDR);\n\t\t\tif (addr_lo) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\t\"PSOC LBW AR RAZWI: %s, address (aligned to 128 byte): 0x%X\\n\",\n\t\t\t\t\t\teng_name_str, addr_lo);\n\t\t\t\thl_handle_razwi(hdev, addr_lo, &eng_id[0],\n\t\t\t\t\tnum_of_eng, HL_RAZWI_LBW | HL_RAZWI_READ, event_mask);\n\t\t\t\trazwi_happened = true;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (razwi_happened)\n\t\t\tbreak;\n\t}\n\n\treturn razwi_happened;\n}\n\n \nstatic int gaudi2_ack_psoc_razwi_event_handler(struct hl_device *hdev, u64 *event_mask)\n{\n\tu32 razwi_mask_info, razwi_intr = 0, error_count = 0;\n\n\tif (hdev->pldm || !(hdev->fw_components & FW_TYPE_LINUX)) {\n\t\trazwi_intr = RREG32(mmPSOC_GLOBAL_CONF_RAZWI_INTERRUPT);\n\t\tif (!razwi_intr)\n\t\t\treturn 0;\n\t}\n\n\trazwi_mask_info = RREG32(mmPSOC_GLOBAL_CONF_RAZWI_MASK_INFO);\n\n\tdev_err_ratelimited(hdev->dev,\n\t\t\"PSOC RAZWI interrupt: Mask %d, AR %d, AW %d, AXUSER_L 0x%x AXUSER_H 0x%x\\n\",\n\t\tFIELD_GET(PSOC_GLOBAL_CONF_RAZWI_MASK_INFO_MASK_MASK, razwi_mask_info),\n\t\tFIELD_GET(PSOC_GLOBAL_CONF_RAZWI_MASK_INFO_WAS_AR_MASK, razwi_mask_info),\n\t\tFIELD_GET(PSOC_GLOBAL_CONF_RAZWI_MASK_INFO_WAS_AW_MASK, razwi_mask_info),\n\t\tFIELD_GET(PSOC_GLOBAL_CONF_RAZWI_MASK_INFO_AXUSER_L_MASK, razwi_mask_info),\n\t\tFIELD_GET(PSOC_GLOBAL_CONF_RAZWI_MASK_INFO_AXUSER_H_MASK, razwi_mask_info));\n\n\tif (gaudi2_handle_psoc_razwi_happened(hdev, razwi_mask_info, event_mask))\n\t\terror_count++;\n\telse\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"PSOC RAZWI interrupt: invalid razwi info (0x%x)\\n\",\n\t\t\t\trazwi_mask_info);\n\n\t \n\tif (hdev->pldm || !(hdev->fw_components & FW_TYPE_LINUX))\n\t\tWREG32(mmPSOC_GLOBAL_CONF_RAZWI_INTERRUPT, razwi_intr);\n\n\treturn error_count;\n}\n\nstatic int _gaudi2_handle_qm_sei_err(struct hl_device *hdev, u64 qman_base, u16 event_type)\n{\n\tu32 i, sts_val, sts_clr_val = 0, error_count = 0;\n\n\tsts_val = RREG32(qman_base + QM_SEI_STATUS_OFFSET);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_QM_SEI_ERR_CAUSE ; i++) {\n\t\tif (sts_val & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_qm_sei_error_cause[i]);\n\t\t\tsts_clr_val |= BIT(i);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\tWREG32(qman_base + QM_SEI_STATUS_OFFSET, sts_clr_val);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_qm_sei_err(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tbool extended_err_check, u64 *event_mask)\n{\n\tenum razwi_event_sources module;\n\tu32 error_count = 0;\n\tu64 qman_base;\n\tu8 index;\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_TPC0_AXI_ERR_RSP ... GAUDI2_EVENT_TPC23_AXI_ERR_RSP:\n\t\tindex = event_type - GAUDI2_EVENT_TPC0_AXI_ERR_RSP;\n\t\tqman_base = mmDCORE0_TPC0_QM_BASE +\n\t\t\t\t(index / NUM_OF_TPC_PER_DCORE) * DCORE_OFFSET +\n\t\t\t\t(index % NUM_OF_TPC_PER_DCORE) * DCORE_TPC_OFFSET;\n\t\tmodule = RAZWI_TPC;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC24_AXI_ERR_RSP:\n\t\tqman_base = mmDCORE0_TPC6_QM_BASE;\n\t\tmodule = RAZWI_TPC;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME1_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME2_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME3_CTRL_AXI_ERROR_RESPONSE:\n\t\tindex = (event_type - GAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE) /\n\t\t\t\t(GAUDI2_EVENT_MME1_CTRL_AXI_ERROR_RESPONSE -\n\t\t\t\t\t\tGAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE);\n\t\tqman_base = mmDCORE0_MME_QM_BASE + index * DCORE_OFFSET;\n\t\tmodule = RAZWI_MME;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PDMA_CH0_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_PDMA_CH1_AXI_ERR_RSP:\n\t\tindex = event_type - GAUDI2_EVENT_PDMA_CH0_AXI_ERR_RSP;\n\t\tqman_base = mmPDMA0_QM_BASE + index * PDMA_OFFSET;\n\t\tmodule = RAZWI_PDMA;\n\t\tbreak;\n\tcase GAUDI2_EVENT_ROTATOR0_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_ROTATOR1_AXI_ERROR_RESPONSE:\n\t\tindex = event_type - GAUDI2_EVENT_ROTATOR0_AXI_ERROR_RESPONSE;\n\t\tqman_base = mmROT0_QM_BASE + index * ROT_OFFSET;\n\t\tmodule = RAZWI_ROT;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\terror_count = _gaudi2_handle_qm_sei_err(hdev, qman_base, event_type);\n\n\t \n\tif (event_type >= GAUDI2_EVENT_NIC0_AXI_ERROR_RESPONSE &&\n\t\t\tevent_type <= GAUDI2_EVENT_NIC11_AXI_ERROR_RESPONSE)\n\t\terror_count += _gaudi2_handle_qm_sei_err(hdev,\n\t\t\t\t\tqman_base + NIC_QM_OFFSET, event_type);\n\n\tif (extended_err_check) {\n\t\t \n\t\tgaudi2_ack_module_razwi_event_handler(hdev, module, 0, 0, event_mask);\n\t\thl_check_for_glbl_errors(hdev);\n\t}\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_qman_err(struct hl_device *hdev, u16 event_type, u64 *event_mask)\n{\n\tu32 qid_base, error_count = 0;\n\tu64 qman_base;\n\tu8 index = 0;\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_TPC0_QM ... GAUDI2_EVENT_TPC5_QM:\n\t\tindex = event_type - GAUDI2_EVENT_TPC0_QM;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_TPC_0_0 + index * QMAN_STREAMS;\n\t\tqman_base = mmDCORE0_TPC0_QM_BASE + index * DCORE_TPC_OFFSET;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC6_QM ... GAUDI2_EVENT_TPC11_QM:\n\t\tindex = event_type - GAUDI2_EVENT_TPC6_QM;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE1_TPC_0_0 + index * QMAN_STREAMS;\n\t\tqman_base = mmDCORE1_TPC0_QM_BASE + index * DCORE_TPC_OFFSET;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC12_QM ... GAUDI2_EVENT_TPC17_QM:\n\t\tindex = event_type - GAUDI2_EVENT_TPC12_QM;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE2_TPC_0_0 + index * QMAN_STREAMS;\n\t\tqman_base = mmDCORE2_TPC0_QM_BASE + index * DCORE_TPC_OFFSET;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC18_QM ... GAUDI2_EVENT_TPC23_QM:\n\t\tindex = event_type - GAUDI2_EVENT_TPC18_QM;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE3_TPC_0_0 + index * QMAN_STREAMS;\n\t\tqman_base = mmDCORE3_TPC0_QM_BASE + index * DCORE_TPC_OFFSET;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC24_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_TPC_6_0;\n\t\tqman_base = mmDCORE0_TPC6_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME0_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_MME_0_0;\n\t\tqman_base = mmDCORE0_MME_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME1_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE1_MME_0_0;\n\t\tqman_base = mmDCORE1_MME_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME2_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE2_MME_0_0;\n\t\tqman_base = mmDCORE2_MME_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME3_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE3_MME_0_0;\n\t\tqman_base = mmDCORE3_MME_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA0_QM:\n\t\tindex = 0;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0;\n\t\tqman_base = mmDCORE0_EDMA0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA1_QM:\n\t\tindex = 1;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE0_EDMA_1_0;\n\t\tqman_base = mmDCORE0_EDMA1_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA2_QM:\n\t\tindex = 2;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE1_EDMA_0_0;\n\t\tqman_base = mmDCORE1_EDMA0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA3_QM:\n\t\tindex = 3;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE1_EDMA_1_0;\n\t\tqman_base = mmDCORE1_EDMA1_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA4_QM:\n\t\tindex = 4;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE2_EDMA_0_0;\n\t\tqman_base = mmDCORE2_EDMA0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA5_QM:\n\t\tindex = 5;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE2_EDMA_1_0;\n\t\tqman_base = mmDCORE2_EDMA1_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA6_QM:\n\t\tindex = 6;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE3_EDMA_0_0;\n\t\tqman_base = mmDCORE3_EDMA0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_HDMA7_QM:\n\t\tindex = 7;\n\t\tqid_base = GAUDI2_QUEUE_ID_DCORE3_EDMA_1_0;\n\t\tqman_base = mmDCORE3_EDMA1_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PDMA0_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_PDMA_0_0;\n\t\tqman_base = mmPDMA0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PDMA1_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_PDMA_1_0;\n\t\tqman_base = mmPDMA1_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_ROTATOR0_ROT0_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_ROT_0_0;\n\t\tqman_base = mmROT0_QM_BASE;\n\t\tbreak;\n\tcase GAUDI2_EVENT_ROTATOR1_ROT1_QM:\n\t\tqid_base = GAUDI2_QUEUE_ID_ROT_1_0;\n\t\tqman_base = mmROT1_QM_BASE;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\terror_count = gaudi2_handle_qman_err_generic(hdev, event_type, qman_base, qid_base);\n\n\t \n\tif (event_type >= GAUDI2_EVENT_HDMA2_QM && event_type <= GAUDI2_EVENT_HDMA5_QM) {\n\t\terror_count += _gaudi2_handle_qm_sei_err(hdev, qman_base, event_type);\n\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_EDMA, index, 0, event_mask);\n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_arc_farm_sei_err(struct hl_device *hdev, u16 event_type)\n{\n\tu32 i, sts_val, sts_clr_val, error_count = 0, arc_farm;\n\n\tfor (arc_farm = 0 ; arc_farm < NUM_OF_ARC_FARMS_ARC ; arc_farm++) {\n\t\tsts_clr_val = 0;\n\t\tsts_val = RREG32(mmARC_FARM_ARC0_AUX_ARC_SEI_INTR_STS +\n\t\t\t\t(arc_farm * ARC_FARM_OFFSET));\n\n\t\tfor (i = 0 ; i < GAUDI2_NUM_OF_ARC_SEI_ERR_CAUSE ; i++) {\n\t\t\tif (sts_val & BIT(i)) {\n\t\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\t\t\"ARC FARM ARC %u err cause: %s\",\n\t\t\t\t\t\tarc_farm, gaudi2_arc_sei_error_cause[i]);\n\t\t\t\tsts_clr_val |= BIT(i);\n\t\t\t\terror_count++;\n\t\t\t}\n\t\t}\n\t\tWREG32(mmARC_FARM_ARC0_AUX_ARC_SEI_INTR_CLR + (arc_farm * ARC_FARM_OFFSET),\n\t\t\t\tsts_clr_val);\n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_cpu_sei_err(struct hl_device *hdev, u16 event_type)\n{\n\tu32 i, sts_val, sts_clr_val = 0, error_count = 0;\n\n\tsts_val = RREG32(mmCPU_IF_CPU_SEI_INTR_STS);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_CPU_SEI_ERR_CAUSE ; i++) {\n\t\tif (sts_val & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_cpu_sei_error_cause[i]);\n\t\t\tsts_clr_val |= BIT(i);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\tWREG32(mmCPU_IF_CPU_SEI_INTR_CLR, sts_clr_val);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_rot_err(struct hl_device *hdev, u8 rot_index, u16 event_type,\n\t\t\t\t\tstruct hl_eq_razwi_with_intr_cause *razwi_with_intr_cause,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu64 intr_cause_data = le64_to_cpu(razwi_with_intr_cause->intr_cause.intr_cause_data);\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_ROT_ERR_CAUSE ; i++)\n\t\tif (intr_cause_data & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", guadi2_rot_error_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\n\t \n\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_ROT, rot_index, 0, event_mask);\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_tpc_ack_interrupts(struct hl_device *hdev,  u8 tpc_index, u16 event_type,\n\t\t\t\t\tstruct hl_eq_razwi_with_intr_cause *razwi_with_intr_cause,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu64 intr_cause_data = le64_to_cpu(razwi_with_intr_cause->intr_cause.intr_cause_data);\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_TPC_INTR_CAUSE ; i++)\n\t\tif (intr_cause_data & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"interrupt cause: %s\",  gaudi2_tpc_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\n\t \n\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_TPC, tpc_index, 0, event_mask);\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_dec_err(struct hl_device *hdev, u8 dec_index, u16 event_type,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu32 sts_addr, sts_val, sts_clr_val = 0, error_count = 0;\n\tint i;\n\n\tif (dec_index < NUM_OF_VDEC_PER_DCORE * NUM_OF_DCORES)\n\t\t \n\t\tsts_addr = mmDCORE0_VDEC0_BRDG_CTRL_CAUSE_INTR +\n\t\t\t\tDCORE_OFFSET * (dec_index / NUM_OF_DEC_PER_DCORE) +\n\t\t\t\tDCORE_VDEC_OFFSET * (dec_index % NUM_OF_DEC_PER_DCORE);\n\telse\n\t\t \n\t\tsts_addr = mmPCIE_VDEC0_BRDG_CTRL_CAUSE_INTR + PCIE_VDEC_OFFSET *\n\t\t\t\t(dec_index - NUM_OF_VDEC_PER_DCORE * NUM_OF_DCORES);\n\n\tsts_val = RREG32(sts_addr);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_DEC_ERR_CAUSE ; i++) {\n\t\tif (sts_val & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_dec_error_cause[i]);\n\t\t\tsts_clr_val |= BIT(i);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\t \n\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_DEC, dec_index, 0, event_mask);\n\thl_check_for_glbl_errors(hdev);\n\n\t \n\tWREG32(sts_addr, sts_clr_val);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_mme_err(struct hl_device *hdev, u8 mme_index, u16 event_type,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu32 sts_addr, sts_val, sts_clr_addr, sts_clr_val = 0, error_count = 0;\n\tint i;\n\n\tsts_addr = mmDCORE0_MME_CTRL_LO_INTR_CAUSE + DCORE_OFFSET * mme_index;\n\tsts_clr_addr = mmDCORE0_MME_CTRL_LO_INTR_CLEAR + DCORE_OFFSET * mme_index;\n\n\tsts_val = RREG32(sts_addr);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_MME_ERR_CAUSE ; i++) {\n\t\tif (sts_val & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", guadi2_mme_error_cause[i]);\n\t\t\tsts_clr_val |= BIT(i);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\t \n\tfor (i = MME_WRITE ; i < MME_INITIATORS_MAX ; i++)\n\t\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_MME, mme_index, i, event_mask);\n\n\thl_check_for_glbl_errors(hdev);\n\n\tWREG32(sts_clr_addr, sts_clr_val);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_mme_sbte_err(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tu64 intr_cause_data)\n{\n\tint i, error_count = 0;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_MME_SBTE_ERR_CAUSE ; i++)\n\t\tif (intr_cause_data & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", guadi2_mme_sbte_error_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_mme_wap_err(struct hl_device *hdev, u8 mme_index, u16 event_type,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu32 sts_addr, sts_val, sts_clr_addr, sts_clr_val = 0, error_count = 0;\n\tint i;\n\n\tsts_addr = mmDCORE0_MME_ACC_INTR_CAUSE + DCORE_OFFSET * mme_index;\n\tsts_clr_addr = mmDCORE0_MME_ACC_INTR_CLEAR + DCORE_OFFSET * mme_index;\n\n\tsts_val = RREG32(sts_addr);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_MME_WAP_ERR_CAUSE ; i++) {\n\t\tif (sts_val & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", guadi2_mme_wap_error_cause[i]);\n\t\t\tsts_clr_val |= BIT(i);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\t \n\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_MME, mme_index, MME_WAP0, event_mask);\n\tgaudi2_ack_module_razwi_event_handler(hdev, RAZWI_MME, mme_index, MME_WAP1, event_mask);\n\thl_check_for_glbl_errors(hdev);\n\n\tWREG32(sts_clr_addr, sts_clr_val);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_kdma_core_event(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tu64 intr_cause_data)\n{\n\tu32 error_count = 0;\n\tint i;\n\n\t \n\tfor (i = 0 ; i < GAUDI2_NUM_OF_DMA_CORE_INTR_CAUSE ; i++)\n\t\tif (intr_cause_data & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_kdma_core_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_dma_core_event(struct hl_device *hdev, u16 event_type, u64 intr_cause)\n{\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_DMA_CORE_INTR_CAUSE ; i++)\n\t\tif (intr_cause & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_dma_core_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic void gaudi2_print_pcie_mstr_rr_mstr_if_razwi_info(struct hl_device *hdev, u64 *event_mask)\n{\n\tu32 mstr_if_base_addr = mmPCIE_MSTR_RR_MSTR_IF_RR_SHRD_HBW_BASE, razwi_happened_addr;\n\n\trazwi_happened_addr = mstr_if_base_addr + RR_SHRD_HBW_AW_RAZWI_HAPPENED;\n\tif (RREG32(razwi_happened_addr)) {\n\t\tgaudi2_razwi_rr_hbw_shared_printf_info(hdev, mstr_if_base_addr, true, \"PCIE\",\n\t\t\t\t\t\t\tGAUDI2_ENGINE_ID_PCIE, event_mask);\n\t\tWREG32(razwi_happened_addr, 0x1);\n\t}\n\n\trazwi_happened_addr = mstr_if_base_addr + RR_SHRD_HBW_AR_RAZWI_HAPPENED;\n\tif (RREG32(razwi_happened_addr)) {\n\t\tgaudi2_razwi_rr_hbw_shared_printf_info(hdev, mstr_if_base_addr, false, \"PCIE\",\n\t\t\t\t\t\t\tGAUDI2_ENGINE_ID_PCIE, event_mask);\n\t\tWREG32(razwi_happened_addr, 0x1);\n\t}\n\n\trazwi_happened_addr = mstr_if_base_addr + RR_SHRD_LBW_AW_RAZWI_HAPPENED;\n\tif (RREG32(razwi_happened_addr)) {\n\t\tgaudi2_razwi_rr_lbw_shared_printf_info(hdev, mstr_if_base_addr, true, \"PCIE\",\n\t\t\t\t\t\t\tGAUDI2_ENGINE_ID_PCIE, event_mask);\n\t\tWREG32(razwi_happened_addr, 0x1);\n\t}\n\n\trazwi_happened_addr = mstr_if_base_addr + RR_SHRD_LBW_AR_RAZWI_HAPPENED;\n\tif (RREG32(razwi_happened_addr)) {\n\t\tgaudi2_razwi_rr_lbw_shared_printf_info(hdev, mstr_if_base_addr, false, \"PCIE\",\n\t\t\t\t\t\t\tGAUDI2_ENGINE_ID_PCIE, event_mask);\n\t\tWREG32(razwi_happened_addr, 0x1);\n\t}\n}\n\nstatic int gaudi2_print_pcie_addr_dec_info(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tu64 intr_cause_data, u64 *event_mask)\n{\n\tu32 error_count = 0;\n\tint i;\n\n\tgaudi2_print_event(hdev, event_type, true,\n\t\t\"intr_cause_data: %#llx\", intr_cause_data);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_PCIE_ADDR_DEC_ERR_CAUSE ; i++) {\n\t\tif (!(intr_cause_data & BIT_ULL(i)))\n\t\t\tcontinue;\n\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\"err cause: %s\", gaudi2_pcie_addr_dec_error_cause[i]);\n\t\terror_count++;\n\n\t\t \n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\tgaudi2_print_pcie_mstr_rr_mstr_if_razwi_info(hdev, event_mask);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_pif_fatal(struct hl_device *hdev, u16 event_type,\n\t\t\t\tu64 intr_cause_data)\n\n{\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_PMMU_FATAL_ERR_CAUSE ; i++) {\n\t\tif (intr_cause_data & BIT_ULL(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_pmmu_fatal_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_hif_fatal(struct hl_device *hdev, u16 event_type, u64 intr_cause_data)\n{\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_HIF_FATAL_ERR_CAUSE ; i++) {\n\t\tif (intr_cause_data & BIT_ULL(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_hif_fatal_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\treturn error_count;\n}\n\nstatic void gaudi2_handle_page_error(struct hl_device *hdev, u64 mmu_base, bool is_pmmu,\n\t\t\t\t\tu64 *event_mask)\n{\n\tu32 valid, val;\n\tu64 addr;\n\n\tvalid = RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID));\n\n\tif (!(valid & DCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID_PAGE_ERR_VALID_ENTRY_MASK))\n\t\treturn;\n\n\tval = RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_PAGE_ERROR_CAPTURE));\n\taddr = val & DCORE0_HMMU0_MMU_PAGE_ERROR_CAPTURE_VA_63_32_MASK;\n\taddr <<= 32;\n\taddr |= RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_PAGE_ERROR_CAPTURE_VA));\n\n\tif (is_pmmu) {\n\t\tdev_err_ratelimited(hdev->dev, \"PMMU page fault on va 0x%llx\\n\", addr);\n\t} else {\n\n\t\taddr = gaudi2_mmu_descramble_addr(hdev, addr);\n\t\taddr &= HW_UNSCRAMBLED_BITS_MASK;\n\t\tdev_err_ratelimited(hdev->dev, \"HMMU page fault on va range 0x%llx - 0x%llx\\n\",\n\t\t\t\taddr, addr + ~HW_UNSCRAMBLED_BITS_MASK);\n\t}\n\n\thl_handle_page_fault(hdev, addr, 0, is_pmmu, event_mask);\n\n\tWREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID), 0);\n}\n\nstatic void gaudi2_handle_access_error(struct hl_device *hdev, u64 mmu_base, bool is_pmmu)\n{\n\tu32 valid, val;\n\tu64 addr;\n\n\tvalid = RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID));\n\n\tif (!(valid & DCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID_ACCESS_ERR_VALID_ENTRY_MASK))\n\t\treturn;\n\n\tval = RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_ERROR_CAPTURE));\n\taddr = val & DCORE0_HMMU0_MMU_ACCESS_ERROR_CAPTURE_VA_63_32_MASK;\n\taddr <<= 32;\n\taddr |= RREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_ERROR_CAPTURE_VA));\n\n\tif (!is_pmmu)\n\t\taddr = gaudi2_mmu_descramble_addr(hdev, addr);\n\n\tdev_err_ratelimited(hdev->dev, \"%s access error on va 0x%llx\\n\",\n\t\t\t\tis_pmmu ? \"PMMU\" : \"HMMU\", addr);\n\tWREG32(mmu_base + MMU_OFFSET(mmDCORE0_HMMU0_MMU_ACCESS_PAGE_ERROR_VALID), 0);\n}\n\nstatic int gaudi2_handle_mmu_spi_sei_generic(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\t\tu64 mmu_base, bool is_pmmu, u64 *event_mask)\n{\n\tu32 spi_sei_cause, interrupt_clr = 0x0, error_count = 0;\n\tint i;\n\n\tspi_sei_cause = RREG32(mmu_base + MMU_SPI_SEI_CAUSE_OFFSET);\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_MMU_SPI_SEI_CAUSE ; i++) {\n\t\tif (spi_sei_cause & BIT(i)) {\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s\", gaudi2_mmu_spi_sei[i].cause);\n\n\t\t\tif (i == 0)\n\t\t\t\tgaudi2_handle_page_error(hdev, mmu_base, is_pmmu, event_mask);\n\t\t\telse if (i == 1)\n\t\t\t\tgaudi2_handle_access_error(hdev, mmu_base, is_pmmu);\n\n\t\t\tif (gaudi2_mmu_spi_sei[i].clear_bit >= 0)\n\t\t\t\tinterrupt_clr |= BIT(gaudi2_mmu_spi_sei[i].clear_bit);\n\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\t \n\tWREG32_AND(mmu_base + MMU_SPI_SEI_CAUSE_OFFSET, ~spi_sei_cause);\n\n\t \n\tWREG32(mmu_base + MMU_INTERRUPT_CLR_OFFSET, interrupt_clr);\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_sm_err(struct hl_device *hdev, u16 event_type, u8 sm_index)\n{\n\tu32 sei_cause_addr, sei_cause_val, sei_cause_cause, sei_cause_log,\n\t\tcq_intr_addr, cq_intr_val, cq_intr_queue_index, error_count = 0;\n\tint i;\n\n\tsei_cause_addr = mmDCORE0_SYNC_MNGR_GLBL_SM_SEI_CAUSE + DCORE_OFFSET * sm_index;\n\tcq_intr_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_INTR + DCORE_OFFSET * sm_index;\n\n\tsei_cause_val = RREG32(sei_cause_addr);\n\tsei_cause_cause = FIELD_GET(DCORE0_SYNC_MNGR_GLBL_SM_SEI_CAUSE_CAUSE_MASK, sei_cause_val);\n\tcq_intr_val = RREG32(cq_intr_addr);\n\n\t \n\tif (sei_cause_cause) {\n\t\t \n\t\tsei_cause_log = FIELD_GET(DCORE0_SYNC_MNGR_GLBL_SM_SEI_CAUSE_LOG_MASK,\n\t\t\t\t\tsei_cause_val);\n\n\t\tfor (i = 0 ; i < GAUDI2_NUM_OF_SM_SEI_ERR_CAUSE ; i++) {\n\t\t\tif (!(sei_cause_cause & BIT(i)))\n\t\t\t\tcontinue;\n\n\t\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"err cause: %s. %s: 0x%X\",\n\t\t\t\tgaudi2_sm_sei_cause[i].cause_name,\n\t\t\t\tgaudi2_sm_sei_cause[i].log_name,\n\t\t\t\tsei_cause_log);\n\t\t\terror_count++;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tWREG32(sei_cause_addr, 0);\n\t}\n\n\t \n\tif (cq_intr_val & DCORE0_SYNC_MNGR_GLBL_CQ_INTR_CQ_SEC_INTR_MASK) {\n\t\tcq_intr_queue_index =\n\t\t\t\tFIELD_GET(DCORE0_SYNC_MNGR_GLBL_CQ_INTR_CQ_INTR_QUEUE_INDEX_MASK,\n\t\t\t\t\tcq_intr_val);\n\n\t\tdev_err_ratelimited(hdev->dev, \"SM%u err. err cause: CQ_INTR. queue index: %u\\n\",\n\t\t\t\tsm_index, cq_intr_queue_index);\n\t\terror_count++;\n\n\t\t \n\t\tWREG32(cq_intr_addr, 0);\n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic u64 get_hmmu_base(u16 event_type)\n{\n\tu8 dcore, index_in_dcore;\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_HMMU_0_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU0_SPI_BASE ... GAUDI2_EVENT_HMMU0_SECURITY_ERROR:\n\t\tdcore = 0;\n\t\tindex_in_dcore = 0;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_1_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU1_SPI_BASE ... GAUDI2_EVENT_HMMU1_SECURITY_ERROR:\n\t\tdcore = 1;\n\t\tindex_in_dcore = 0;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_2_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU2_SPI_BASE ... GAUDI2_EVENT_HMMU2_SECURITY_ERROR:\n\t\tdcore = 0;\n\t\tindex_in_dcore = 1;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_3_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU3_SPI_BASE ... GAUDI2_EVENT_HMMU3_SECURITY_ERROR:\n\t\tdcore = 1;\n\t\tindex_in_dcore = 1;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_4_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU4_SPI_BASE ... GAUDI2_EVENT_HMMU4_SECURITY_ERROR:\n\t\tdcore = 3;\n\t\tindex_in_dcore = 2;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_5_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU5_SPI_BASE ... GAUDI2_EVENT_HMMU5_SECURITY_ERROR:\n\t\tdcore = 2;\n\t\tindex_in_dcore = 2;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_6_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU6_SPI_BASE ... GAUDI2_EVENT_HMMU6_SECURITY_ERROR:\n\t\tdcore = 3;\n\t\tindex_in_dcore = 3;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_7_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU7_SPI_BASE ... GAUDI2_EVENT_HMMU7_SECURITY_ERROR:\n\t\tdcore = 2;\n\t\tindex_in_dcore = 3;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_8_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU8_SPI_BASE ... GAUDI2_EVENT_HMMU8_SECURITY_ERROR:\n\t\tdcore = 0;\n\t\tindex_in_dcore = 2;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_9_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU9_SPI_BASE ... GAUDI2_EVENT_HMMU9_SECURITY_ERROR:\n\t\tdcore = 1;\n\t\tindex_in_dcore = 2;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_10_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU10_SPI_BASE ... GAUDI2_EVENT_HMMU10_SECURITY_ERROR:\n\t\tdcore = 0;\n\t\tindex_in_dcore = 3;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_11_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU11_SPI_BASE ... GAUDI2_EVENT_HMMU11_SECURITY_ERROR:\n\t\tdcore = 1;\n\t\tindex_in_dcore = 3;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_12_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU12_SPI_BASE ... GAUDI2_EVENT_HMMU12_SECURITY_ERROR:\n\t\tdcore = 3;\n\t\tindex_in_dcore = 0;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_13_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU13_SPI_BASE ... GAUDI2_EVENT_HMMU13_SECURITY_ERROR:\n\t\tdcore = 2;\n\t\tindex_in_dcore = 0;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_14_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU14_SPI_BASE ... GAUDI2_EVENT_HMMU14_SECURITY_ERROR:\n\t\tdcore = 3;\n\t\tindex_in_dcore = 1;\n\tbreak;\n\tcase GAUDI2_EVENT_HMMU_15_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU15_SPI_BASE ... GAUDI2_EVENT_HMMU15_SECURITY_ERROR:\n\t\tdcore = 2;\n\t\tindex_in_dcore = 1;\n\tbreak;\n\tdefault:\n\t\treturn ULONG_MAX;\n\t}\n\n\treturn mmDCORE0_HMMU0_MMU_BASE + dcore * DCORE_OFFSET + index_in_dcore * DCORE_HMMU_OFFSET;\n}\n\nstatic int gaudi2_handle_mmu_spi_sei_err(struct hl_device *hdev, u16 event_type, u64 *event_mask)\n{\n\tbool is_pmmu = false;\n\tu32 error_count = 0;\n\tu64 mmu_base;\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_HMMU_0_AXI_ERR_RSP ... GAUDI2_EVENT_HMMU_12_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_HMMU0_SPI_BASE ... GAUDI2_EVENT_HMMU12_SECURITY_ERROR:\n\t\tmmu_base = get_hmmu_base(event_type);\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PMMU0_PAGE_FAULT_WR_PERM ... GAUDI2_EVENT_PMMU0_SECURITY_ERROR:\n\tcase GAUDI2_EVENT_PMMU_AXI_ERR_RSP_0:\n\t\tis_pmmu = true;\n\t\tmmu_base = mmPMMU_HBW_MMU_BASE;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\tif (mmu_base == ULONG_MAX)\n\t\treturn 0;\n\n\terror_count = gaudi2_handle_mmu_spi_sei_generic(hdev, event_type, mmu_base,\n\t\t\t\t\t\t\tis_pmmu, event_mask);\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\n\n \nstatic bool gaudi2_hbm_sei_handle_read_err(struct hl_device *hdev,\n\t\t\tstruct hl_eq_hbm_sei_read_err_intr_info *rd_err_data, u32 err_cnt)\n{\n\tu32 addr, beat, beat_shift;\n\tbool rc = false;\n\n\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"READ ERROR count: ECC SERR: %d, ECC DERR: %d, RD_PARITY: %d\\n\",\n\t\t\tFIELD_GET(HBM_ECC_SERR_CNTR_MASK, err_cnt),\n\t\t\tFIELD_GET(HBM_ECC_DERR_CNTR_MASK, err_cnt),\n\t\t\tFIELD_GET(HBM_RD_PARITY_CNTR_MASK, err_cnt));\n\n\taddr = le32_to_cpu(rd_err_data->dbg_rd_err_addr.rd_addr_val);\n\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"READ ERROR address: sid(%u), bg(%u), ba(%u), col(%u), row(%u)\\n\",\n\t\t\tFIELD_GET(HBM_RD_ADDR_SID_MASK, addr),\n\t\t\tFIELD_GET(HBM_RD_ADDR_BG_MASK, addr),\n\t\t\tFIELD_GET(HBM_RD_ADDR_BA_MASK, addr),\n\t\t\tFIELD_GET(HBM_RD_ADDR_COL_MASK, addr),\n\t\t\tFIELD_GET(HBM_RD_ADDR_ROW_MASK, addr));\n\n\t \n\tfor (beat = 0 ; beat < 4 ; beat++) {\n\t\tif (le32_to_cpu(rd_err_data->dbg_rd_err_misc) &\n\t\t\t(HBM_RD_ERR_SERR_BEAT0_MASK << beat))\n\t\t\tdev_err_ratelimited(hdev->dev, \"Beat%d ECC SERR: DM: %#x, Syndrome: %#x\\n\",\n\t\t\t\t\t\tbeat,\n\t\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_dm),\n\t\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_syndrome));\n\n\t\tif (le32_to_cpu(rd_err_data->dbg_rd_err_misc) &\n\t\t\t(HBM_RD_ERR_DERR_BEAT0_MASK << beat)) {\n\t\t\tdev_err_ratelimited(hdev->dev, \"Beat%d ECC DERR: DM: %#x, Syndrome: %#x\\n\",\n\t\t\t\t\t\tbeat,\n\t\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_dm),\n\t\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_syndrome));\n\t\t\trc |= true;\n\t\t}\n\n\t\tbeat_shift = beat * HBM_RD_ERR_BEAT_SHIFT;\n\t\tif (le32_to_cpu(rd_err_data->dbg_rd_err_misc) &\n\t\t\t(HBM_RD_ERR_PAR_ERR_BEAT0_MASK << beat_shift)) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\"Beat%d read PARITY: DM: %#x, PAR data: %#x\\n\",\n\t\t\t\t\tbeat,\n\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_dm),\n\t\t\t\t\t(le32_to_cpu(rd_err_data->dbg_rd_err_misc) &\n\t\t\t\t\t\t(HBM_RD_ERR_PAR_DATA_BEAT0_MASK << beat_shift)) >>\n\t\t\t\t\t\t(HBM_RD_ERR_PAR_DATA_BEAT0_SHIFT + beat_shift));\n\t\t\trc |= true;\n\t\t}\n\n\t\tdev_err_ratelimited(hdev->dev, \"Beat%d DQ data:\\n\", beat);\n\t\tdev_err_ratelimited(hdev->dev, \"\\t0x%08x\\n\",\n\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_data[beat * 2]));\n\t\tdev_err_ratelimited(hdev->dev, \"\\t0x%08x\\n\",\n\t\t\t\t\tle32_to_cpu(rd_err_data->dbg_rd_err_data[beat * 2 + 1]));\n\t}\n\n\treturn rc;\n}\n\nstatic void gaudi2_hbm_sei_print_wr_par_info(struct hl_device *hdev,\n\t\t\tstruct hl_eq_hbm_sei_wr_par_intr_info *wr_par_err_data, u32 err_cnt)\n{\n\tstruct hbm_sei_wr_cmd_address *wr_cmd_addr = wr_par_err_data->dbg_last_wr_cmds;\n\tu32 i, curr_addr, derr = wr_par_err_data->dbg_derr;\n\n\tdev_err_ratelimited(hdev->dev, \"WRITE PARITY ERROR count: %d\\n\", err_cnt);\n\n\tdev_err_ratelimited(hdev->dev, \"CK-0 DERR: 0x%02x, CK-1 DERR: 0x%02x\\n\",\n\t\t\t\tderr & 0x3, derr & 0xc);\n\n\t \n\tdev_err_ratelimited(hdev->dev, \"Last latched write commands addresses:\\n\");\n\tfor (i = 0 ; i < HBM_WR_PAR_CMD_LIFO_LEN ; i++) {\n\t\tcurr_addr = le32_to_cpu(wr_cmd_addr[i].dbg_wr_cmd_addr);\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"\\twrite cmd[%u]: Address: SID(%u) BG(%u) BA(%u) COL(%u).\\n\",\n\t\t\t\ti,\n\t\t\t\tFIELD_GET(WR_PAR_LAST_CMD_SID_MASK, curr_addr),\n\t\t\t\tFIELD_GET(WR_PAR_LAST_CMD_BG_MASK, curr_addr),\n\t\t\t\tFIELD_GET(WR_PAR_LAST_CMD_BA_MASK, curr_addr),\n\t\t\t\tFIELD_GET(WR_PAR_LAST_CMD_COL_MASK, curr_addr));\n\t}\n}\n\nstatic void gaudi2_hbm_sei_print_ca_par_info(struct hl_device *hdev,\n\t\tstruct hl_eq_hbm_sei_ca_par_intr_info *ca_par_err_data, u32 err_cnt)\n{\n\t__le32 *col_cmd = ca_par_err_data->dbg_col;\n\t__le16 *row_cmd = ca_par_err_data->dbg_row;\n\tu32 i;\n\n\tdev_err_ratelimited(hdev->dev, \"CA ERROR count: %d\\n\", err_cnt);\n\n\tdev_err_ratelimited(hdev->dev, \"Last latched C&R bus commands:\\n\");\n\tfor (i = 0 ; i < HBM_CA_ERR_CMD_LIFO_LEN ; i++)\n\t\tdev_err_ratelimited(hdev->dev, \"cmd%u: ROW(0x%04x) COL(0x%05x)\\n\", i,\n\t\t\tle16_to_cpu(row_cmd[i]) & (u16)GENMASK(13, 0),\n\t\t\tle32_to_cpu(col_cmd[i]) & (u32)GENMASK(17, 0));\n}\n\n \nstatic bool gaudi2_handle_hbm_mc_sei_err(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tstruct hl_eq_hbm_sei_data *sei_data)\n{\n\tbool require_hard_reset = false;\n\tu32 hbm_id, mc_id, cause_idx;\n\n\thbm_id = (event_type - GAUDI2_EVENT_HBM0_MC0_SEI_SEVERE) / 4;\n\tmc_id = ((event_type - GAUDI2_EVENT_HBM0_MC0_SEI_SEVERE) / 2) % 2;\n\n\tcause_idx = sei_data->hdr.sei_cause;\n\tif (cause_idx > GAUDI2_NUM_OF_HBM_SEI_CAUSE - 1) {\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\"err cause: %s\",\n\t\t\t\"Invalid HBM SEI event cause (%d) provided by FW\", cause_idx);\n\t\treturn true;\n\t}\n\n\tgaudi2_print_event(hdev, event_type, !sei_data->hdr.is_critical,\n\t\t\"System %s Error Interrupt - HBM(%u) MC(%u) MC_CH(%u) MC_PC(%u). Error cause: %s\",\n\t\tsei_data->hdr.is_critical ? \"Critical\" : \"Non-critical\",\n\t\thbm_id, mc_id, sei_data->hdr.mc_channel, sei_data->hdr.mc_pseudo_channel,\n\t\thbm_mc_sei_cause[cause_idx]);\n\n\t \n\tswitch (cause_idx) {\n\tcase HBM_SEI_CATTRIP:\n\t\trequire_hard_reset = true;\n\t\tbreak;\n\n\tcase  HBM_SEI_CMD_PARITY_EVEN:\n\t\tgaudi2_hbm_sei_print_ca_par_info(hdev, &sei_data->ca_parity_even_info,\n\t\t\t\t\t\tle32_to_cpu(sei_data->hdr.cnt));\n\t\trequire_hard_reset = true;\n\t\tbreak;\n\n\tcase  HBM_SEI_CMD_PARITY_ODD:\n\t\tgaudi2_hbm_sei_print_ca_par_info(hdev, &sei_data->ca_parity_odd_info,\n\t\t\t\t\t\tle32_to_cpu(sei_data->hdr.cnt));\n\t\trequire_hard_reset = true;\n\t\tbreak;\n\n\tcase HBM_SEI_WRITE_DATA_PARITY_ERR:\n\t\tgaudi2_hbm_sei_print_wr_par_info(hdev, &sei_data->wr_parity_info,\n\t\t\t\t\t\tle32_to_cpu(sei_data->hdr.cnt));\n\t\trequire_hard_reset = true;\n\t\tbreak;\n\n\tcase HBM_SEI_READ_ERR:\n\t\t \n\t\trequire_hard_reset = gaudi2_hbm_sei_handle_read_err(hdev,\n\t\t\t\t\t\t\t\t&sei_data->read_err_info,\n\t\t\t\t\t\t\t\tle32_to_cpu(sei_data->hdr.cnt));\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\trequire_hard_reset |= !!sei_data->hdr.is_critical;\n\n\treturn require_hard_reset;\n}\n\nstatic int gaudi2_handle_hbm_cattrip(struct hl_device *hdev, u16 event_type,\n\t\t\t\tu64 intr_cause_data)\n{\n\tif (intr_cause_data) {\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\"temperature error cause: %#llx\", intr_cause_data);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi2_handle_hbm_mc_spi(struct hl_device *hdev, u64 intr_cause_data)\n{\n\tu32 i, error_count = 0;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_HBM_MC_SPI_CAUSE ; i++)\n\t\tif (intr_cause_data & hbm_mc_spi[i].mask) {\n\t\t\tdev_dbg(hdev->dev, \"HBM spi event: notification cause(%s)\\n\",\n\t\t\t\thbm_mc_spi[i].cause);\n\t\t\terror_count++;\n\t\t}\n\n\treturn error_count;\n}\n\nstatic void gaudi2_print_clk_change_info(struct hl_device *hdev, u16 event_type, u64 *event_mask)\n{\n\tktime_t zero_time = ktime_set(0, 0);\n\n\tmutex_lock(&hdev->clk_throttling.lock);\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_CPU_FIX_POWER_ENV_S:\n\t\thdev->clk_throttling.current_reason |= HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.aggregated_reason |= HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].start = ktime_get();\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].end = zero_time;\n\t\tdev_dbg_ratelimited(hdev->dev, \"Clock throttling due to power consumption\\n\");\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_FIX_POWER_ENV_E:\n\t\thdev->clk_throttling.current_reason &= ~HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].end = ktime_get();\n\t\tdev_dbg_ratelimited(hdev->dev, \"Power envelop is safe, back to optimal clock\\n\");\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_FIX_THERMAL_ENV_S:\n\t\thdev->clk_throttling.current_reason |= HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.aggregated_reason |= HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].start = ktime_get();\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].end = zero_time;\n\t\t*event_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tdev_info_ratelimited(hdev->dev, \"Clock throttling due to overheating\\n\");\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_FIX_THERMAL_ENV_E:\n\t\thdev->clk_throttling.current_reason &= ~HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].end = ktime_get();\n\t\t*event_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tdev_info_ratelimited(hdev->dev, \"Thermal envelop is safe, back to optimal clock\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(hdev->dev, \"Received invalid clock change event %d\\n\", event_type);\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&hdev->clk_throttling.lock);\n}\n\nstatic void gaudi2_print_out_of_sync_info(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tstruct cpucp_pkt_sync_err *sync_err)\n{\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[GAUDI2_QUEUE_ID_CPU_PQ];\n\n\tgaudi2_print_event(hdev, event_type, false,\n\t\t\"FW: pi=%u, ci=%u, LKD: pi=%u, ci=%d\",\n\t\tle32_to_cpu(sync_err->pi), le32_to_cpu(sync_err->ci),\n\t\tq->pi, atomic_read(&q->ci));\n}\n\nstatic int gaudi2_handle_pcie_p2p_msix(struct hl_device *hdev, u16 event_type)\n{\n\tu32 p2p_intr, msix_gw_intr, error_count = 0;\n\n\tp2p_intr = RREG32(mmPCIE_WRAP_P2P_INTR);\n\tmsix_gw_intr = RREG32(mmPCIE_WRAP_MSIX_GW_INTR);\n\n\tif (p2p_intr) {\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\"pcie p2p transaction terminated due to security, req_id(0x%x)\",\n\t\t\tRREG32(mmPCIE_WRAP_P2P_REQ_ID));\n\n\t\tWREG32(mmPCIE_WRAP_P2P_INTR, 0x1);\n\t\terror_count++;\n\t}\n\n\tif (msix_gw_intr) {\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\"pcie msi-x gen denied due to vector num check failure, vec(0x%X)\",\n\t\t\tRREG32(mmPCIE_WRAP_MSIX_GW_VEC));\n\n\t\tWREG32(mmPCIE_WRAP_MSIX_GW_INTR, 0x1);\n\t\terror_count++;\n\t}\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_pcie_drain(struct hl_device *hdev,\n\t\t\tstruct hl_eq_pcie_drain_ind_data *drain_data)\n{\n\tu64 lbw_rd, lbw_wr, hbw_rd, hbw_wr, cause, error_count = 0;\n\n\tcause = le64_to_cpu(drain_data->intr_cause.intr_cause_data);\n\tlbw_rd = le64_to_cpu(drain_data->drain_rd_addr_lbw);\n\tlbw_wr = le64_to_cpu(drain_data->drain_wr_addr_lbw);\n\thbw_rd = le64_to_cpu(drain_data->drain_rd_addr_hbw);\n\thbw_wr = le64_to_cpu(drain_data->drain_wr_addr_hbw);\n\n\tif (cause & BIT_ULL(0)) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"PCIE AXI drain LBW completed, read_err %u, write_err %u\\n\",\n\t\t\t!!lbw_rd, !!lbw_wr);\n\t\terror_count++;\n\t}\n\n\tif (cause & BIT_ULL(1)) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"PCIE AXI drain HBW completed, raddr %#llx, waddr %#llx\\n\",\n\t\t\thbw_rd, hbw_wr);\n\t\terror_count++;\n\t}\n\n\treturn error_count;\n}\n\nstatic int gaudi2_handle_psoc_drain(struct hl_device *hdev, u64 intr_cause_data)\n{\n\tu32 error_count = 0;\n\tint i;\n\n\tfor (i = 0 ; i < GAUDI2_NUM_OF_AXI_DRAIN_ERR_CAUSE ; i++) {\n\t\tif (intr_cause_data & BIT_ULL(i)) {\n\t\t\tdev_err_ratelimited(hdev->dev, \"PSOC %s completed\\n\",\n\t\t\t\tgaudi2_psoc_axi_drain_interrupts_cause[i]);\n\t\t\terror_count++;\n\t\t}\n\t}\n\n\thl_check_for_glbl_errors(hdev);\n\n\treturn error_count;\n}\n\nstatic void gaudi2_print_cpu_pkt_failure_info(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tstruct cpucp_pkt_sync_err *sync_err)\n{\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[GAUDI2_QUEUE_ID_CPU_PQ];\n\n\tgaudi2_print_event(hdev, event_type, false,\n\t\t\"FW reported sanity check failure, FW: pi=%u, ci=%u, LKD: pi=%u, ci=%d\",\n\t\tle32_to_cpu(sync_err->pi), le32_to_cpu(sync_err->ci), q->pi, atomic_read(&q->ci));\n}\n\nstatic int hl_arc_event_handle(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tstruct hl_eq_engine_arc_intr_data *data)\n{\n\tstruct hl_engine_arc_dccm_queue_full_irq *q;\n\tu32 intr_type, engine_id;\n\tu64 payload;\n\n\tintr_type = le32_to_cpu(data->intr_type);\n\tengine_id = le32_to_cpu(data->engine_id);\n\tpayload = le64_to_cpu(data->payload);\n\n\tswitch (intr_type) {\n\tcase ENGINE_ARC_DCCM_QUEUE_FULL_IRQ:\n\t\tq = (struct hl_engine_arc_dccm_queue_full_irq *) &payload;\n\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"ARC DCCM Full event: EngId: %u, Intr_type: %u, Qidx: %u\",\n\t\t\t\tengine_id, intr_type, q->queue_index);\n\t\treturn 1;\n\tdefault:\n\t\tgaudi2_print_event(hdev, event_type, true, \"Unknown ARC event type\");\n\t\treturn 0;\n\t}\n}\n\nstatic void gaudi2_handle_eqe(struct hl_device *hdev, struct hl_eq_entry *eq_entry)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tbool reset_required = false, is_critical = false;\n\tu32 index, ctl, reset_flags = 0, error_count = 0;\n\tu64 event_mask = 0;\n\tu16 event_type;\n\n\tctl = le32_to_cpu(eq_entry->hdr.ctl);\n\tevent_type = ((ctl & EQ_CTL_EVENT_TYPE_MASK) >> EQ_CTL_EVENT_TYPE_SHIFT);\n\n\tif (event_type >= GAUDI2_EVENT_SIZE) {\n\t\tdev_err(hdev->dev, \"Event type %u exceeds maximum of %u\",\n\t\t\t\tevent_type, GAUDI2_EVENT_SIZE - 1);\n\t\treturn;\n\t}\n\n\tgaudi2->events_stat[event_type]++;\n\tgaudi2->events_stat_aggregate[event_type]++;\n\n\tswitch (event_type) {\n\tcase GAUDI2_EVENT_PCIE_CORE_SERR ... GAUDI2_EVENT_ARC0_ECC_DERR:\n\t\tfallthrough;\n\tcase GAUDI2_EVENT_ROTATOR0_SERR ... GAUDI2_EVENT_ROTATOR1_DERR:\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\treset_required = gaudi2_handle_ecc_event(hdev, event_type, &eq_entry->ecc_data);\n\t\tis_critical = eq_entry->ecc_data.is_critical;\n\t\terror_count++;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_TPC0_QM ... GAUDI2_EVENT_PDMA1_QM:\n\t\tfallthrough;\n\tcase GAUDI2_EVENT_ROTATOR0_ROT0_QM ... GAUDI2_EVENT_ROTATOR1_ROT1_QM:\n\t\tfallthrough;\n\tcase GAUDI2_EVENT_NIC0_QM0 ... GAUDI2_EVENT_NIC11_QM1:\n\t\terror_count = gaudi2_handle_qman_err(hdev, event_type, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_ARC_AXI_ERROR_RESPONSE_0:\n\t\terror_count = gaudi2_handle_arc_farm_sei_err(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_AXI_ERR_RSP:\n\t\terror_count = gaudi2_handle_cpu_sei_err(hdev, event_type);\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_CRITICL_FW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PDMA_CH0_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_PDMA_CH1_AXI_ERR_RSP:\n\t\terror_count = gaudi2_handle_qm_sei_err(hdev, event_type, true, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_ROTATOR0_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_ROTATOR1_AXI_ERROR_RESPONSE:\n\t\tindex = event_type - GAUDI2_EVENT_ROTATOR0_AXI_ERROR_RESPONSE;\n\t\terror_count = gaudi2_handle_rot_err(hdev, index, event_type,\n\t\t\t\t\t&eq_entry->razwi_with_intr_cause, &event_mask);\n\t\terror_count += gaudi2_handle_qm_sei_err(hdev, event_type, false, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_TPC0_AXI_ERR_RSP ... GAUDI2_EVENT_TPC24_AXI_ERR_RSP:\n\t\tindex = event_type - GAUDI2_EVENT_TPC0_AXI_ERR_RSP;\n\t\terror_count = gaudi2_tpc_ack_interrupts(hdev, index, event_type,\n\t\t\t\t\t\t&eq_entry->razwi_with_intr_cause, &event_mask);\n\t\terror_count += gaudi2_handle_qm_sei_err(hdev, event_type, false, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_DEC0_AXI_ERR_RSPONSE ... GAUDI2_EVENT_DEC9_AXI_ERR_RSPONSE:\n\t\tindex = event_type - GAUDI2_EVENT_DEC0_AXI_ERR_RSPONSE;\n\t\terror_count = gaudi2_handle_dec_err(hdev, index, event_type, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_TPC0_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC1_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC2_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC3_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC4_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC5_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC6_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC7_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC8_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC9_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC10_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC11_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC12_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC13_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC14_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC15_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC16_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC17_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC18_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC19_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC20_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC21_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC22_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC23_KERNEL_ERR:\n\tcase GAUDI2_EVENT_TPC24_KERNEL_ERR:\n\t\tindex = (event_type - GAUDI2_EVENT_TPC0_KERNEL_ERR) /\n\t\t\t(GAUDI2_EVENT_TPC1_KERNEL_ERR - GAUDI2_EVENT_TPC0_KERNEL_ERR);\n\t\terror_count = gaudi2_tpc_ack_interrupts(hdev, index, event_type,\n\t\t\t\t\t&eq_entry->razwi_with_intr_cause, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_DEC0_SPI:\n\tcase GAUDI2_EVENT_DEC1_SPI:\n\tcase GAUDI2_EVENT_DEC2_SPI:\n\tcase GAUDI2_EVENT_DEC3_SPI:\n\tcase GAUDI2_EVENT_DEC4_SPI:\n\tcase GAUDI2_EVENT_DEC5_SPI:\n\tcase GAUDI2_EVENT_DEC6_SPI:\n\tcase GAUDI2_EVENT_DEC7_SPI:\n\tcase GAUDI2_EVENT_DEC8_SPI:\n\tcase GAUDI2_EVENT_DEC9_SPI:\n\t\tindex = (event_type - GAUDI2_EVENT_DEC0_SPI) /\n\t\t\t\t(GAUDI2_EVENT_DEC1_SPI - GAUDI2_EVENT_DEC0_SPI);\n\t\terror_count = gaudi2_handle_dec_err(hdev, index, event_type, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME1_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME2_CTRL_AXI_ERROR_RESPONSE:\n\tcase GAUDI2_EVENT_MME3_CTRL_AXI_ERROR_RESPONSE:\n\t\tindex = (event_type - GAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE) /\n\t\t\t\t(GAUDI2_EVENT_MME1_CTRL_AXI_ERROR_RESPONSE -\n\t\t\t\t\t\tGAUDI2_EVENT_MME0_CTRL_AXI_ERROR_RESPONSE);\n\t\terror_count = gaudi2_handle_mme_err(hdev, index, event_type, &event_mask);\n\t\terror_count += gaudi2_handle_qm_sei_err(hdev, event_type, false, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_MME0_QMAN_SW_ERROR:\n\tcase GAUDI2_EVENT_MME1_QMAN_SW_ERROR:\n\tcase GAUDI2_EVENT_MME2_QMAN_SW_ERROR:\n\tcase GAUDI2_EVENT_MME3_QMAN_SW_ERROR:\n\t\tindex = (event_type - GAUDI2_EVENT_MME0_QMAN_SW_ERROR) /\n\t\t\t\t(GAUDI2_EVENT_MME1_QMAN_SW_ERROR -\n\t\t\t\t\tGAUDI2_EVENT_MME0_QMAN_SW_ERROR);\n\t\terror_count = gaudi2_handle_mme_err(hdev, index, event_type, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_MME0_WAP_SOURCE_RESULT_INVALID:\n\tcase GAUDI2_EVENT_MME1_WAP_SOURCE_RESULT_INVALID:\n\tcase GAUDI2_EVENT_MME2_WAP_SOURCE_RESULT_INVALID:\n\tcase GAUDI2_EVENT_MME3_WAP_SOURCE_RESULT_INVALID:\n\t\tindex = (event_type - GAUDI2_EVENT_MME0_WAP_SOURCE_RESULT_INVALID) /\n\t\t\t\t(GAUDI2_EVENT_MME1_WAP_SOURCE_RESULT_INVALID -\n\t\t\t\t\tGAUDI2_EVENT_MME0_WAP_SOURCE_RESULT_INVALID);\n\t\terror_count = gaudi2_handle_mme_wap_err(hdev, index, event_type, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_KDMA_CH0_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_KDMA0_CORE:\n\t\terror_count = gaudi2_handle_kdma_core_event(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HDMA2_CORE ... GAUDI2_EVENT_HDMA5_CORE:\n\t\terror_count = gaudi2_handle_dma_core_event(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PDMA0_CORE ... GAUDI2_EVENT_PDMA1_CORE:\n\t\terror_count = gaudi2_handle_dma_core_event(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PCIE_ADDR_DEC_ERR:\n\t\terror_count = gaudi2_print_pcie_addr_dec_info(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data), &event_mask);\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HMMU0_PAGE_FAULT_OR_WR_PERM ... GAUDI2_EVENT_HMMU12_SECURITY_ERROR:\n\tcase GAUDI2_EVENT_HMMU_0_AXI_ERR_RSP ... GAUDI2_EVENT_HMMU_12_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_PMMU0_PAGE_FAULT_WR_PERM ... GAUDI2_EVENT_PMMU0_SECURITY_ERROR:\n\tcase GAUDI2_EVENT_PMMU_AXI_ERR_RSP_0:\n\t\terror_count = gaudi2_handle_mmu_spi_sei_err(hdev, event_type, &event_mask);\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HIF0_FATAL ... GAUDI2_EVENT_HIF12_FATAL:\n\t\terror_count = gaudi2_handle_hif_fatal(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PMMU_FATAL_0:\n\t\terror_count = gaudi2_handle_pif_fatal(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PSOC63_RAZWI_OR_PID_MIN_MAX_INTERRUPT:\n\t\terror_count = gaudi2_ack_psoc_razwi_event_handler(hdev, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HBM0_MC0_SEI_SEVERE ... GAUDI2_EVENT_HBM5_MC1_SEI_NON_SEVERE:\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tif (gaudi2_handle_hbm_mc_sei_err(hdev, event_type, &eq_entry->sei_data)) {\n\t\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\t\treset_required = true;\n\t\t}\n\t\terror_count++;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HBM_CATTRIP_0 ... GAUDI2_EVENT_HBM_CATTRIP_5:\n\t\terror_count = gaudi2_handle_hbm_cattrip(hdev, event_type,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_HBM0_MC0_SPI ... GAUDI2_EVENT_HBM5_MC1_SPI:\n\t\terror_count = gaudi2_handle_hbm_mc_spi(hdev,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PCIE_DRAIN_COMPLETE:\n\t\terror_count = gaudi2_handle_pcie_drain(hdev, &eq_entry->pcie_drain_ind_data);\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PSOC59_RPM_ERROR_OR_DRAIN:\n\t\terror_count = gaudi2_handle_psoc_drain(hdev,\n\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_AXI_ECC:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_CPU_L2_RAM_ECC:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_MME0_SBTE0_AXI_ERR_RSP ... GAUDI2_EVENT_MME0_SBTE4_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_MME1_SBTE0_AXI_ERR_RSP ... GAUDI2_EVENT_MME1_SBTE4_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_MME2_SBTE0_AXI_ERR_RSP ... GAUDI2_EVENT_MME2_SBTE4_AXI_ERR_RSP:\n\tcase GAUDI2_EVENT_MME3_SBTE0_AXI_ERR_RSP ... GAUDI2_EVENT_MME3_SBTE4_AXI_ERR_RSP:\n\t\terror_count = gaudi2_handle_mme_sbte_err(hdev, event_type,\n\t\t\t\t\t\tle64_to_cpu(eq_entry->intr_cause.intr_cause_data));\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_VM0_ALARM_A ... GAUDI2_EVENT_VM3_ALARM_B:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PSOC_AXI_ERR_RSP:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PSOC_PRSTN_FALL:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PCIE_APB_TIMEOUT:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_PCIE_FATAL_ERR:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_TPC0_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC1_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC2_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC3_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC4_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC5_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC6_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC7_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC8_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC9_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC10_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC11_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC12_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC13_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC14_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC15_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC16_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC17_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC18_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC19_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC20_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC21_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC22_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC23_BMON_SPMU:\n\tcase GAUDI2_EVENT_TPC24_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME0_CTRL_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME0_SBTE_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME0_WAP_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME1_CTRL_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME1_SBTE_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME1_WAP_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME2_CTRL_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME2_SBTE_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME2_WAP_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME3_CTRL_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME3_SBTE_BMON_SPMU:\n\tcase GAUDI2_EVENT_MME3_WAP_BMON_SPMU:\n\tcase GAUDI2_EVENT_HDMA2_BM_SPMU ... GAUDI2_EVENT_PDMA1_BM_SPMU:\n\t\tfallthrough;\n\tcase GAUDI2_EVENT_DEC0_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC1_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC2_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC3_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC4_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC5_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC6_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC7_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC8_BMON_SPMU:\n\tcase GAUDI2_EVENT_DEC9_BMON_SPMU:\n\tcase GAUDI2_EVENT_ROTATOR0_BMON_SPMU ... GAUDI2_EVENT_SM3_BMON_SPMU:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_FIX_POWER_ENV_S:\n\tcase GAUDI2_EVENT_CPU_FIX_POWER_ENV_E:\n\tcase GAUDI2_EVENT_CPU_FIX_THERMAL_ENV_S:\n\tcase GAUDI2_EVENT_CPU_FIX_THERMAL_ENV_E:\n\t\tgaudi2_print_clk_change_info(hdev, event_type, &event_mask);\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_PKT_QUEUE_OUT_SYNC:\n\t\tgaudi2_print_out_of_sync_info(hdev, event_type, &eq_entry->pkt_sync_err);\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PCIE_FLR_REQUESTED:\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\t \n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PCIE_P2P_MSIX:\n\t\terror_count = gaudi2_handle_pcie_p2p_msix(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_SM0_AXI_ERROR_RESPONSE ... GAUDI2_EVENT_SM3_AXI_ERROR_RESPONSE:\n\t\tindex = event_type - GAUDI2_EVENT_SM0_AXI_ERROR_RESPONSE;\n\t\terror_count = gaudi2_handle_sm_err(hdev, event_type, index);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_PSOC_MME_PLL_LOCK_ERR ... GAUDI2_EVENT_DCORE2_HBM_PLL_LOCK_ERR:\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_CPLD_SHUTDOWN_CAUSE:\n\t\tdev_info(hdev->dev, \"CPLD shutdown cause, reset reason: 0x%llx\\n\",\n\t\t\t\t\t\tle64_to_cpu(eq_entry->data[0]));\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\tcase GAUDI2_EVENT_CPU_CPLD_SHUTDOWN_EVENT:\n\t\tdev_err(hdev->dev, \"CPLD shutdown event, reset reason: 0x%llx\\n\",\n\t\t\t\t\t\tle64_to_cpu(eq_entry->data[0]));\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_PKT_SANITY_FAILED:\n\t\tgaudi2_print_cpu_pkt_failure_info(hdev, event_type, &eq_entry->pkt_sync_err);\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\treset_flags |= HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_ARC_DCCM_FULL:\n\t\terror_count = hl_arc_event_handle(hdev, event_type, &eq_entry->arc_data);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI2_EVENT_CPU_FP32_NOT_SUPPORTED:\n\tcase GAUDI2_EVENT_CPU_DEV_RESET_REQ:\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\tis_critical = true;\n\t\tbreak;\n\n\tdefault:\n\t\tif (gaudi2_irq_map_table[event_type].valid) {\n\t\t\tdev_err_ratelimited(hdev->dev, \"Cannot find handler for event %d\\n\",\n\t\t\t\t\t\tevent_type);\n\t\t\terror_count = GAUDI2_NA_EVENT_CAUSE;\n\t\t}\n\t}\n\n\t \n\tif (error_count == GAUDI2_NA_EVENT_CAUSE && !is_info_event(event_type))\n\t\tgaudi2_print_event(hdev, event_type, true, \"%d\", event_type);\n\telse if (error_count == 0)\n\t\tgaudi2_print_event(hdev, event_type, true,\n\t\t\t\t\"No error cause for H/W event %u\", event_type);\n\n\tif ((gaudi2_irq_map_table[event_type].reset != EVENT_RESET_TYPE_NONE) ||\n\t\t\t\treset_required) {\n\t\tif (reset_required ||\n\t\t\t\t(gaudi2_irq_map_table[event_type].reset == EVENT_RESET_TYPE_HARD))\n\t\t\treset_flags |= HL_DRV_RESET_HARD;\n\n\t\tif (hdev->hard_reset_on_fw_events ||\n\t\t\t\t(hdev->asic_prop.fw_security_enabled && is_critical))\n\t\t\tgoto reset_device;\n\t}\n\n\t \n\tif (!gaudi2_irq_map_table[event_type].msg)\n\t\thl_fw_unmask_irq(hdev, event_type);\n\n\tif (event_mask)\n\t\thl_notifier_event_send_all(hdev, event_mask);\n\n\treturn;\n\nreset_device:\n\tif (hdev->asic_prop.fw_security_enabled && is_critical) {\n\t\treset_flags |= HL_DRV_RESET_BYPASS_REQ_TO_FW;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_UNAVAILABLE;\n\t} else {\n\t\treset_flags |= HL_DRV_RESET_DELAY;\n\t}\n\t \n\tif (event_mask & HL_NOTIFIER_EVENT_GENERAL_HW_ERR)\n\t\thl_handle_critical_hw_err(hdev, event_type, &event_mask);\n\n\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_RESET;\n\thl_device_cond_reset(hdev, reset_flags, event_mask);\n}\n\nstatic int gaudi2_memset_memory_chunk_using_edma_qm(struct hl_device *hdev,\n\t\t\tstruct packet_lin_dma *lin_dma_pkt, dma_addr_t pkt_dma_addr,\n\t\t\tu32 hw_queue_id, u32 size, u64 addr, u32 val)\n{\n\tu32 ctl, pkt_size;\n\tint rc = 0;\n\n\tctl = FIELD_PREP(GAUDI2_PKT_CTL_OPCODE_MASK, PACKET_LIN_DMA);\n\tctl |= FIELD_PREP(GAUDI2_PKT_LIN_DMA_CTL_MEMSET_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI2_PKT_LIN_DMA_CTL_WRCOMP_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_EB_MASK, 1);\n\n\tlin_dma_pkt->ctl = cpu_to_le32(ctl);\n\tlin_dma_pkt->src_addr = cpu_to_le64(val);\n\tlin_dma_pkt->dst_addr = cpu_to_le64(addr);\n\tlin_dma_pkt->tsize = cpu_to_le32(size);\n\n\tpkt_size = sizeof(struct packet_lin_dma);\n\n\trc = hl_hw_queue_send_cb_no_cmpl(hdev, hw_queue_id, pkt_size, pkt_dma_addr);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to send lin dma packet to H/W queue %d\\n\",\n\t\t\t\thw_queue_id);\n\n\treturn rc;\n}\n\nstatic int gaudi2_memset_device_memory(struct hl_device *hdev, u64 addr, u64 size, u64 val)\n{\n\tu32 edma_queues_id[] = {GAUDI2_QUEUE_ID_DCORE0_EDMA_0_0,\n\t\t\t\t\tGAUDI2_QUEUE_ID_DCORE1_EDMA_0_0,\n\t\t\t\t\tGAUDI2_QUEUE_ID_DCORE2_EDMA_0_0,\n\t\t\t\t\tGAUDI2_QUEUE_ID_DCORE3_EDMA_0_0};\n\tu32 chunk_size, dcore, edma_idx, sob_offset, sob_addr, comp_val,\n\t\told_mmubp, mmubp, num_of_pkts, busy, pkt_size;\n\tu64 comp_addr, cur_addr = addr, end_addr = addr + size;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tvoid *lin_dma_pkts_arr;\n\tdma_addr_t pkt_dma_addr;\n\tint rc = 0, dma_num = 0;\n\n\tif (prop->edma_enabled_mask == 0) {\n\t\tdev_info(hdev->dev, \"non of the EDMA engines is enabled - skip dram scrubbing\\n\");\n\t\treturn -EIO;\n\t}\n\n\tsob_offset = hdev->asic_prop.first_available_user_sob[0] * 4;\n\tsob_addr = mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + sob_offset;\n\tcomp_addr = CFG_BASE + sob_addr;\n\tcomp_val = FIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_INC_MASK, 1) |\n\t\tFIELD_PREP(DCORE0_SYNC_MNGR_OBJS_SOB_OBJ_VAL_MASK, 1);\n\tmmubp = FIELD_PREP(ARC_FARM_KDMA_CTX_AXUSER_HB_MMU_BP_WR_MASK, 1) |\n\t\tFIELD_PREP(ARC_FARM_KDMA_CTX_AXUSER_HB_MMU_BP_RD_MASK, 1);\n\n\t \n\tnum_of_pkts = div64_u64(round_up(size, SZ_2G), SZ_2G);\n\tpkt_size = sizeof(struct packet_lin_dma);\n\n\tlin_dma_pkts_arr = hl_asic_dma_alloc_coherent(hdev, pkt_size * num_of_pkts,\n\t\t\t\t\t&pkt_dma_addr, GFP_KERNEL);\n\tif (!lin_dma_pkts_arr)\n\t\treturn -ENOMEM;\n\n\t \n\told_mmubp = RREG32(mmDCORE0_EDMA0_CORE_CTX_AXUSER_HB_MMU_BP);\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (edma_idx = 0 ; edma_idx < NUM_OF_EDMA_PER_DCORE ; edma_idx++) {\n\t\t\tu32 edma_offset = dcore * DCORE_OFFSET + edma_idx * DCORE_EDMA_OFFSET;\n\t\t\tu32 edma_bit = dcore * NUM_OF_EDMA_PER_DCORE + edma_idx;\n\n\t\t\tif (!(prop->edma_enabled_mask & BIT(edma_bit)))\n\t\t\t\tcontinue;\n\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_AXUSER_HB_MMU_BP +\n\t\t\t\t\tedma_offset, mmubp);\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_ADDR_LO + edma_offset,\n\t\t\t\t\tlower_32_bits(comp_addr));\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_ADDR_HI + edma_offset,\n\t\t\t\t\tupper_32_bits(comp_addr));\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_WDATA + edma_offset,\n\t\t\t\t\tcomp_val);\n\t\t\tgaudi2_qman_set_test_mode(hdev,\n\t\t\t\t\tedma_queues_id[dcore] + 4 * edma_idx, true);\n\t\t}\n\t}\n\n\tWREG32(sob_addr, 0);\n\n\twhile (cur_addr < end_addr) {\n\t\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\t\tfor (edma_idx = 0 ; edma_idx < NUM_OF_EDMA_PER_DCORE ; edma_idx++) {\n\t\t\t\tu32 edma_bit = dcore * NUM_OF_EDMA_PER_DCORE + edma_idx;\n\n\t\t\t\tif (!(prop->edma_enabled_mask & BIT(edma_bit)))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tchunk_size = min_t(u64, SZ_2G, end_addr - cur_addr);\n\n\t\t\t\trc = gaudi2_memset_memory_chunk_using_edma_qm(hdev,\n\t\t\t\t\t(struct packet_lin_dma *)lin_dma_pkts_arr + dma_num,\n\t\t\t\t\tpkt_dma_addr + dma_num * pkt_size,\n\t\t\t\t\tedma_queues_id[dcore] + edma_idx * 4,\n\t\t\t\t\tchunk_size, cur_addr, val);\n\t\t\t\tif (rc)\n\t\t\t\t\tgoto end;\n\n\t\t\t\tdma_num++;\n\t\t\t\tcur_addr += chunk_size;\n\t\t\t\tif (cur_addr == end_addr)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\trc = hl_poll_timeout(hdev, sob_addr, busy, (busy == dma_num), 1000, 1000000);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"DMA Timeout during HBM scrubbing\\n\");\n\t\tgoto end;\n\t}\nend:\n\tfor (dcore = 0 ; dcore < NUM_OF_DCORES ; dcore++) {\n\t\tfor (edma_idx = 0 ; edma_idx < NUM_OF_EDMA_PER_DCORE ; edma_idx++) {\n\t\t\tu32 edma_offset = dcore * DCORE_OFFSET + edma_idx * DCORE_EDMA_OFFSET;\n\t\t\tu32 edma_bit = dcore * NUM_OF_EDMA_PER_DCORE + edma_idx;\n\n\t\t\tif (!(prop->edma_enabled_mask & BIT(edma_bit)))\n\t\t\t\tcontinue;\n\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_AXUSER_HB_MMU_BP + edma_offset, old_mmubp);\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_ADDR_LO + edma_offset, 0);\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_ADDR_HI + edma_offset, 0);\n\t\t\tWREG32(mmDCORE0_EDMA0_CORE_CTX_WR_COMP_WDATA + edma_offset, 0);\n\t\t\tgaudi2_qman_set_test_mode(hdev,\n\t\t\t\t\tedma_queues_id[dcore] + 4 * edma_idx, false);\n\t\t}\n\t}\n\n\tWREG32(sob_addr, 0);\n\thl_asic_dma_free_coherent(hdev, pkt_size * num_of_pkts, lin_dma_pkts_arr, pkt_dma_addr);\n\n\treturn rc;\n}\n\nstatic int gaudi2_scrub_device_dram(struct hl_device *hdev, u64 val)\n{\n\tint rc;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 size = prop->dram_end_address - prop->dram_user_base_address;\n\n\trc = gaudi2_memset_device_memory(hdev, prop->dram_user_base_address, size, val);\n\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to scrub dram, address: 0x%llx size: %llu\\n\",\n\t\t\t\tprop->dram_user_base_address, size);\n\treturn rc;\n}\n\nstatic int gaudi2_scrub_device_mem(struct hl_device *hdev)\n{\n\tint rc;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 val = hdev->memory_scrub_val;\n\tu64 addr, size;\n\n\tif (!hdev->memory_scrub)\n\t\treturn 0;\n\n\t \n\taddr = prop->sram_user_base_address;\n\tsize = hdev->pldm ? 0x10000 : (prop->sram_size - SRAM_USER_BASE_OFFSET);\n\tdev_dbg(hdev->dev, \"Scrubbing SRAM: 0x%09llx - 0x%09llx, val: 0x%llx\\n\",\n\t\t\taddr, addr + size, val);\n\trc = gaudi2_memset_device_memory(hdev, addr, size, val);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"scrubbing SRAM failed (%d)\\n\", rc);\n\t\treturn rc;\n\t}\n\n\t \n\trc = gaudi2_scrub_device_dram(hdev, val);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"scrubbing DRAM failed (%d)\\n\", rc);\n\t\treturn rc;\n\t}\n\treturn 0;\n}\n\nstatic void gaudi2_restore_user_sm_registers(struct hl_device *hdev)\n{\n\tu64 addr, mon_sts_addr, mon_cfg_addr, cq_lbw_l_addr, cq_lbw_h_addr,\n\t\tcq_lbw_data_addr, cq_base_l_addr, cq_base_h_addr, cq_size_addr;\n\tu32 val, size, offset;\n\tint dcore_id;\n\n\toffset = hdev->asic_prop.first_available_cq[0] * 4;\n\tcq_lbw_l_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_L_0 + offset;\n\tcq_lbw_h_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_H_0 + offset;\n\tcq_lbw_data_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_DATA_0 + offset;\n\tcq_base_l_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_L_0 + offset;\n\tcq_base_h_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_H_0 + offset;\n\tcq_size_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_SIZE_LOG2_0 + offset;\n\tsize = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_H_0 -\n\t\t\t(mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_L_0 + offset);\n\n\t \n\tgaudi2_memset_device_lbw(hdev, cq_lbw_l_addr, size, 0);\n\tgaudi2_memset_device_lbw(hdev, cq_lbw_h_addr, size, 0);\n\tgaudi2_memset_device_lbw(hdev, cq_lbw_data_addr, size, 0);\n\tgaudi2_memset_device_lbw(hdev, cq_base_l_addr, size, 0);\n\tgaudi2_memset_device_lbw(hdev, cq_base_h_addr, size, 0);\n\tgaudi2_memset_device_lbw(hdev, cq_size_addr, size, 0);\n\n\tcq_lbw_l_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_L_0 + DCORE_OFFSET;\n\tcq_lbw_h_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_H_0 + DCORE_OFFSET;\n\tcq_lbw_data_addr = mmDCORE0_SYNC_MNGR_GLBL_LBW_DATA_0 + DCORE_OFFSET;\n\tcq_base_l_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_L_0 + DCORE_OFFSET;\n\tcq_base_h_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_BASE_ADDR_H_0 + DCORE_OFFSET;\n\tcq_size_addr = mmDCORE0_SYNC_MNGR_GLBL_CQ_SIZE_LOG2_0 + DCORE_OFFSET;\n\tsize = mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_H_0 - mmDCORE0_SYNC_MNGR_GLBL_LBW_ADDR_L_0;\n\n\tfor (dcore_id = 1 ; dcore_id < NUM_OF_DCORES ; dcore_id++) {\n\t\tgaudi2_memset_device_lbw(hdev, cq_lbw_l_addr, size, 0);\n\t\tgaudi2_memset_device_lbw(hdev, cq_lbw_h_addr, size, 0);\n\t\tgaudi2_memset_device_lbw(hdev, cq_lbw_data_addr, size, 0);\n\t\tgaudi2_memset_device_lbw(hdev, cq_base_l_addr, size, 0);\n\t\tgaudi2_memset_device_lbw(hdev, cq_base_h_addr, size, 0);\n\t\tgaudi2_memset_device_lbw(hdev, cq_size_addr, size, 0);\n\n\t\tcq_lbw_l_addr += DCORE_OFFSET;\n\t\tcq_lbw_h_addr += DCORE_OFFSET;\n\t\tcq_lbw_data_addr += DCORE_OFFSET;\n\t\tcq_base_l_addr += DCORE_OFFSET;\n\t\tcq_base_h_addr += DCORE_OFFSET;\n\t\tcq_size_addr += DCORE_OFFSET;\n\t}\n\n\toffset = hdev->asic_prop.first_available_user_mon[0] * 4;\n\taddr = mmDCORE0_SYNC_MNGR_OBJS_MON_STATUS_0 + offset;\n\tval = 1 << DCORE0_SYNC_MNGR_OBJS_MON_STATUS_PROT_SHIFT;\n\tsize = mmDCORE0_SYNC_MNGR_OBJS_SM_SEC_0 - (mmDCORE0_SYNC_MNGR_OBJS_MON_STATUS_0 + offset);\n\n\t \n\tgaudi2_memset_device_lbw(hdev, addr, size, val);\n\n\taddr = mmDCORE0_SYNC_MNGR_OBJS_MON_CONFIG_0 + offset;\n\tgaudi2_memset_device_lbw(hdev, addr, size, 0);\n\n\tmon_sts_addr = mmDCORE0_SYNC_MNGR_OBJS_MON_STATUS_0 + DCORE_OFFSET;\n\tmon_cfg_addr = mmDCORE0_SYNC_MNGR_OBJS_MON_CONFIG_0 + DCORE_OFFSET;\n\tsize = mmDCORE0_SYNC_MNGR_OBJS_SM_SEC_0 - mmDCORE0_SYNC_MNGR_OBJS_MON_STATUS_0;\n\n\tfor (dcore_id = 1 ; dcore_id < NUM_OF_DCORES ; dcore_id++) {\n\t\tgaudi2_memset_device_lbw(hdev, mon_sts_addr, size, val);\n\t\tgaudi2_memset_device_lbw(hdev, mon_cfg_addr, size, 0);\n\t\tmon_sts_addr += DCORE_OFFSET;\n\t\tmon_cfg_addr += DCORE_OFFSET;\n\t}\n\n\toffset = hdev->asic_prop.first_available_user_sob[0] * 4;\n\taddr = mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + offset;\n\tval = 0;\n\tsize = mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 -\n\t\t\t(mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + offset);\n\n\t \n\tgaudi2_memset_device_lbw(hdev, addr, size, val);\n\n\taddr = mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + DCORE_OFFSET;\n\tsize = mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 - mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0;\n\n\tfor (dcore_id = 1 ; dcore_id < NUM_OF_DCORES ; dcore_id++) {\n\t\tgaudi2_memset_device_lbw(hdev, addr, size, val);\n\t\taddr += DCORE_OFFSET;\n\t}\n\n\t \n\tval = RREG32(mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + offset);\n}\n\nstatic void gaudi2_restore_user_qm_registers(struct hl_device *hdev)\n{\n\tu32 reg_base, hw_queue_id;\n\n\tfor (hw_queue_id = GAUDI2_QUEUE_ID_PDMA_0_0 ; hw_queue_id <= GAUDI2_QUEUE_ID_ROT_1_0;\n\t\t\t\t\t\t\thw_queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, hw_queue_id))\n\t\t\tcontinue;\n\n\t\tgaudi2_clear_qm_fence_counters_common(hdev, hw_queue_id, false);\n\n\t\treg_base = gaudi2_qm_blocks_bases[hw_queue_id];\n\t\tWREG32(reg_base + QM_ARB_CFG_0_OFFSET, 0);\n\t}\n\n\t \n\tRREG32(mmPDMA0_QM_ARB_CFG_0);\n}\n\nstatic void gaudi2_restore_nic_qm_registers(struct hl_device *hdev)\n{\n\tu32 reg_base, hw_queue_id;\n\n\tfor (hw_queue_id = GAUDI2_QUEUE_ID_NIC_0_0 ; hw_queue_id <= GAUDI2_QUEUE_ID_NIC_23_3;\n\t\t\t\t\t\t\thw_queue_id += NUM_OF_PQ_PER_QMAN) {\n\t\tif (!gaudi2_is_queue_enabled(hdev, hw_queue_id))\n\t\t\tcontinue;\n\n\t\tgaudi2_clear_qm_fence_counters_common(hdev, hw_queue_id, false);\n\n\t\treg_base = gaudi2_qm_blocks_bases[hw_queue_id];\n\t\tWREG32(reg_base + QM_ARB_CFG_0_OFFSET, 0);\n\t}\n\n\t \n\tRREG32(mmPDMA0_QM_ARB_CFG_0);\n}\n\nstatic int gaudi2_context_switch(struct hl_device *hdev, u32 asid)\n{\n\treturn 0;\n}\n\nstatic void gaudi2_restore_phase_topology(struct hl_device *hdev)\n{\n}\n\nstatic void gaudi2_init_block_instances(struct hl_device *hdev, u32 block_idx,\n\t\t\t\t\t\tstruct dup_block_ctx *cfg_ctx)\n{\n\tu64 block_base = cfg_ctx->base + block_idx * cfg_ctx->block_off;\n\tu8 seq;\n\tint i;\n\n\tfor (i = 0 ; i < cfg_ctx->instances ; i++) {\n\t\tseq = block_idx * cfg_ctx->instances + i;\n\n\t\t \n\t\tif (!(cfg_ctx->enabled_mask & BIT_ULL(seq)))\n\t\t\tcontinue;\n\n\t\tcfg_ctx->instance_cfg_fn(hdev, block_base + i * cfg_ctx->instance_off,\n\t\t\t\t\tcfg_ctx->data);\n\t}\n}\n\nstatic void gaudi2_init_blocks_with_mask(struct hl_device *hdev, struct dup_block_ctx *cfg_ctx,\n\t\t\t\t\t\tu64 mask)\n{\n\tint i;\n\n\tcfg_ctx->enabled_mask = mask;\n\n\tfor (i = 0 ; i < cfg_ctx->blocks ; i++)\n\t\tgaudi2_init_block_instances(hdev, i, cfg_ctx);\n}\n\nvoid gaudi2_init_blocks(struct hl_device *hdev, struct dup_block_ctx *cfg_ctx)\n{\n\tgaudi2_init_blocks_with_mask(hdev, cfg_ctx, U64_MAX);\n}\n\nstatic int gaudi2_debugfs_read_dma(struct hl_device *hdev, u64 addr, u32 size, void *blob_addr)\n{\n\tvoid *host_mem_virtual_addr;\n\tdma_addr_t host_mem_dma_addr;\n\tu64 reserved_va_base;\n\tu32 pos, size_left, size_to_dma;\n\tstruct hl_ctx *ctx;\n\tint rc = 0;\n\n\t \n\tctx = hl_get_compute_ctx(hdev);\n\tif (!ctx) {\n\t\tdev_err(hdev->dev, \"No ctx available\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\thost_mem_virtual_addr = hl_asic_dma_alloc_coherent(hdev, SZ_2M, &host_mem_dma_addr,\n\t\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (host_mem_virtual_addr == NULL) {\n\t\tdev_err(hdev->dev, \"Failed to allocate memory for KDMA read\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto put_ctx;\n\t}\n\n\t \n\treserved_va_base = hl_reserve_va_block(hdev, ctx, HL_VA_RANGE_TYPE_HOST, SZ_2M,\n\t\t\t\t\t\tHL_MMU_VA_ALIGNMENT_NOT_NEEDED);\n\tif (!reserved_va_base) {\n\t\tdev_err(hdev->dev, \"Failed to reserve vmem on asic\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_data_buffer;\n\t}\n\n\t \n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = hl_mmu_map_contiguous(ctx, reserved_va_base, host_mem_dma_addr, SZ_2M);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to create mapping on asic mmu\\n\");\n\t\tgoto unreserve_va;\n\t}\n\n\trc = hl_mmu_invalidate_cache_range(hdev, false,\n\t\t\t\t      MMU_OP_USERPTR | MMU_OP_SKIP_LOW_CACHE_INV,\n\t\t\t\t      ctx->asid, reserved_va_base, SZ_2M);\n\tif (rc) {\n\t\thl_mmu_unmap_contiguous(ctx, reserved_va_base, SZ_2M);\n\t\tgoto unreserve_va;\n\t}\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\t \n\tgaudi2_kdma_set_mmbp_asid(hdev, false, ctx->asid);\n\n\tpos = 0;\n\tsize_left = size;\n\tsize_to_dma = SZ_2M;\n\n\twhile (size_left > 0) {\n\t\tif (size_left < SZ_2M)\n\t\t\tsize_to_dma = size_left;\n\n\t\trc = gaudi2_send_job_to_kdma(hdev, addr, reserved_va_base, size_to_dma, false);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tmemcpy(blob_addr + pos, host_mem_virtual_addr, size_to_dma);\n\n\t\tif (size_left <= SZ_2M)\n\t\t\tbreak;\n\n\t\tpos += SZ_2M;\n\t\taddr += SZ_2M;\n\t\tsize_left -= SZ_2M;\n\t}\n\n\tgaudi2_kdma_set_mmbp_asid(hdev, true, HL_KERNEL_ASID_ID);\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = hl_mmu_unmap_contiguous(ctx, reserved_va_base, SZ_2M);\n\tif (rc)\n\t\tgoto unreserve_va;\n\n\trc = hl_mmu_invalidate_cache_range(hdev, false, MMU_OP_USERPTR,\n\t\t\t\t      ctx->asid, reserved_va_base, SZ_2M);\n\nunreserve_va:\n\tmutex_unlock(&hdev->mmu_lock);\n\thl_unreserve_va_block(hdev, ctx, reserved_va_base, SZ_2M);\nfree_data_buffer:\n\thl_asic_dma_free_coherent(hdev, SZ_2M, host_mem_virtual_addr, host_mem_dma_addr);\nput_ctx:\n\thl_ctx_put(ctx);\n\n\treturn rc;\n}\n\nstatic int gaudi2_internal_cb_pool_init(struct hl_device *hdev, struct hl_ctx *ctx)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint min_alloc_order, rc;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PMMU))\n\t\treturn 0;\n\n\thdev->internal_cb_pool_virt_addr = hl_asic_dma_alloc_coherent(hdev,\n\t\t\t\t\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ,\n\t\t\t\t\t\t\t\t&hdev->internal_cb_pool_dma_addr,\n\t\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\n\tif (!hdev->internal_cb_pool_virt_addr)\n\t\treturn -ENOMEM;\n\n\tmin_alloc_order = ilog2(min(gaudi2_get_signal_cb_size(hdev),\n\t\t\t\t\tgaudi2_get_wait_cb_size(hdev)));\n\n\thdev->internal_cb_pool = gen_pool_create(min_alloc_order, -1);\n\tif (!hdev->internal_cb_pool) {\n\t\tdev_err(hdev->dev, \"Failed to create internal CB pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_internal_cb_pool;\n\t}\n\n\trc = gen_pool_add(hdev->internal_cb_pool, (uintptr_t) hdev->internal_cb_pool_virt_addr,\n\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ, -1);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to add memory to internal CB pool\\n\");\n\t\trc = -EFAULT;\n\t\tgoto destroy_internal_cb_pool;\n\t}\n\n\thdev->internal_cb_va_base = hl_reserve_va_block(hdev, ctx, HL_VA_RANGE_TYPE_HOST,\n\t\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ, HL_MMU_VA_ALIGNMENT_NOT_NEEDED);\n\n\tif (!hdev->internal_cb_va_base) {\n\t\trc = -ENOMEM;\n\t\tgoto destroy_internal_cb_pool;\n\t}\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = hl_mmu_map_contiguous(ctx, hdev->internal_cb_va_base, hdev->internal_cb_pool_dma_addr,\n\t\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\n\tif (rc)\n\t\tgoto unreserve_internal_cb_pool;\n\n\trc = hl_mmu_invalidate_cache(hdev, false, MMU_OP_USERPTR);\n\tif (rc)\n\t\tgoto unmap_internal_cb_pool;\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\treturn 0;\n\nunmap_internal_cb_pool:\n\thl_mmu_unmap_contiguous(ctx, hdev->internal_cb_va_base, HOST_SPACE_INTERNAL_CB_SZ);\nunreserve_internal_cb_pool:\n\tmutex_unlock(&hdev->mmu_lock);\n\thl_unreserve_va_block(hdev, ctx, hdev->internal_cb_va_base, HOST_SPACE_INTERNAL_CB_SZ);\ndestroy_internal_cb_pool:\n\tgen_pool_destroy(hdev->internal_cb_pool);\nfree_internal_cb_pool:\n\thl_asic_dma_free_coherent(hdev, HOST_SPACE_INTERNAL_CB_SZ, hdev->internal_cb_pool_virt_addr,\n\t\t\t\t\thdev->internal_cb_pool_dma_addr);\n\n\treturn rc;\n}\n\nstatic void gaudi2_internal_cb_pool_fini(struct hl_device *hdev, struct hl_ctx *ctx)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_PMMU))\n\t\treturn;\n\n\tmutex_lock(&hdev->mmu_lock);\n\thl_mmu_unmap_contiguous(ctx, hdev->internal_cb_va_base, HOST_SPACE_INTERNAL_CB_SZ);\n\thl_unreserve_va_block(hdev, ctx, hdev->internal_cb_va_base, HOST_SPACE_INTERNAL_CB_SZ);\n\thl_mmu_invalidate_cache(hdev, true, MMU_OP_USERPTR);\n\tmutex_unlock(&hdev->mmu_lock);\n\n\tgen_pool_destroy(hdev->internal_cb_pool);\n\n\thl_asic_dma_free_coherent(hdev, HOST_SPACE_INTERNAL_CB_SZ, hdev->internal_cb_pool_virt_addr,\n\t\t\t\t\thdev->internal_cb_pool_dma_addr);\n}\n\nstatic void gaudi2_restore_user_registers(struct hl_device *hdev)\n{\n\tgaudi2_restore_user_sm_registers(hdev);\n\tgaudi2_restore_user_qm_registers(hdev);\n}\n\nstatic int gaudi2_map_virtual_msix_doorbell_memory(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint rc;\n\n\trc = hl_mmu_map_page(ctx, RESERVED_VA_FOR_VIRTUAL_MSIX_DOORBELL_START,\n\t\t\t\tgaudi2->virt_msix_db_dma_addr, prop->pmmu.page_size, true);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to map VA %#llx for virtual MSI-X doorbell memory\\n\",\n\t\t\tRESERVED_VA_FOR_VIRTUAL_MSIX_DOORBELL_START);\n\n\treturn rc;\n}\n\nstatic void gaudi2_unmap_virtual_msix_doorbell_memory(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint rc;\n\n\trc = hl_mmu_unmap_page(ctx, RESERVED_VA_FOR_VIRTUAL_MSIX_DOORBELL_START,\n\t\t\t\tprop->pmmu.page_size, true);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to unmap VA %#llx of virtual MSI-X doorbell memory\\n\",\n\t\t\tRESERVED_VA_FOR_VIRTUAL_MSIX_DOORBELL_START);\n}\n\nstatic int gaudi2_ctx_init(struct hl_ctx *ctx)\n{\n\tint rc;\n\n\trc = gaudi2_mmu_prepare(ctx->hdev, ctx->asid);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tif (ctx->hdev->reset_upon_device_release)\n\t\tgaudi2_restore_nic_qm_registers(ctx->hdev);\n\telse\n\t\tgaudi2_restore_user_registers(ctx->hdev);\n\n\trc = gaudi2_internal_cb_pool_init(ctx->hdev, ctx);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi2_map_virtual_msix_doorbell_memory(ctx);\n\tif (rc)\n\t\tgaudi2_internal_cb_pool_fini(ctx->hdev, ctx);\n\n\treturn rc;\n}\n\nstatic void gaudi2_ctx_fini(struct hl_ctx *ctx)\n{\n\tif (ctx->asid == HL_KERNEL_ASID_ID)\n\t\treturn;\n\n\tgaudi2_internal_cb_pool_fini(ctx->hdev, ctx);\n\n\tgaudi2_unmap_virtual_msix_doorbell_memory(ctx);\n}\n\nstatic int gaudi2_pre_schedule_cs(struct hl_cs *cs)\n{\n\tstruct hl_device *hdev = cs->ctx->hdev;\n\tint index = cs->sequence & (hdev->asic_prop.max_pending_cs - 1);\n\tu32 mon_payload, sob_id, mon_id;\n\n\tif (!cs_needs_completion(cs))\n\t\treturn 0;\n\n\t \n\n\tsob_id = mon_id = index;\n\tmon_payload = (1 << CQ_ENTRY_SHADOW_INDEX_VALID_SHIFT) |\n\t\t\t\t(1 << CQ_ENTRY_READY_SHIFT) | index;\n\n\tgaudi2_arm_cq_monitor(hdev, sob_id, mon_id, GAUDI2_RESERVED_CQ_CS_COMPLETION, mon_payload,\n\t\t\t\tcs->jobs_cnt);\n\n\treturn 0;\n}\n\nstatic u32 gaudi2_get_queue_id_for_cq(struct hl_device *hdev, u32 cq_idx)\n{\n\treturn HL_INVALID_QUEUE;\n}\n\nstatic u32 gaudi2_gen_signal_cb(struct hl_device *hdev, void *data, u16 sob_id, u32 size, bool eb)\n{\n\tstruct hl_cb *cb = data;\n\tstruct packet_msg_short *pkt;\n\tu32 value, ctl, pkt_size = sizeof(*pkt);\n\n\tpkt = (struct packet_msg_short *) (uintptr_t) (cb->kernel_address + size);\n\tmemset(pkt, 0, pkt_size);\n\n\t \n\tvalue = FIELD_PREP(GAUDI2_PKT_SHORT_VAL_SOB_SYNC_VAL_MASK, 1);\n\tvalue |= FIELD_PREP(GAUDI2_PKT_SHORT_VAL_SOB_MOD_MASK, 1);\n\n\tctl = FIELD_PREP(GAUDI2_PKT_SHORT_CTL_ADDR_MASK, sob_id * 4);\n\tctl |= FIELD_PREP(GAUDI2_PKT_SHORT_CTL_BASE_MASK, 1);  \n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_EB_MASK, eb);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_MB_MASK, 1);\n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn size + pkt_size;\n}\n\nstatic u32 gaudi2_add_mon_msg_short(struct packet_msg_short *pkt, u32 value, u16 addr)\n{\n\tu32 ctl, pkt_size = sizeof(*pkt);\n\n\tmemset(pkt, 0, pkt_size);\n\n\tctl = FIELD_PREP(GAUDI2_PKT_SHORT_CTL_ADDR_MASK, addr);\n\tctl |= FIELD_PREP(GAUDI2_PKT_SHORT_CTL_BASE_MASK, 0);   \n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_MB_MASK, 0);\n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic u32 gaudi2_add_arm_monitor_pkt(struct hl_device *hdev, struct packet_msg_short *pkt,\n\t\t\t\t\tu16 sob_base, u8 sob_mask, u16 sob_val, u16 addr)\n{\n\tu32 ctl, value, pkt_size = sizeof(*pkt);\n\tu8 mask;\n\n\tif (hl_gen_sob_mask(sob_base, sob_mask, &mask)) {\n\t\tdev_err(hdev->dev, \"sob_base %u (mask %#x) is not valid\\n\", sob_base, sob_mask);\n\t\treturn 0;\n\t}\n\n\tmemset(pkt, 0, pkt_size);\n\n\tvalue = FIELD_PREP(GAUDI2_PKT_SHORT_VAL_MON_SYNC_GID_MASK, sob_base / 8);\n\tvalue |= FIELD_PREP(GAUDI2_PKT_SHORT_VAL_MON_SYNC_VAL_MASK, sob_val);\n\tvalue |= FIELD_PREP(GAUDI2_PKT_SHORT_VAL_MON_MODE_MASK, 0);  \n\tvalue |= FIELD_PREP(GAUDI2_PKT_SHORT_VAL_MON_MASK_MASK, mask);\n\n\tctl = FIELD_PREP(GAUDI2_PKT_SHORT_CTL_ADDR_MASK, addr);\n\tctl |= FIELD_PREP(GAUDI2_PKT_SHORT_CTL_BASE_MASK, 0);  \n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_MB_MASK, 1);\n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic u32 gaudi2_add_fence_pkt(struct packet_fence *pkt)\n{\n\tu32 ctl, cfg, pkt_size = sizeof(*pkt);\n\n\tmemset(pkt, 0, pkt_size);\n\n\tcfg = FIELD_PREP(GAUDI2_PKT_FENCE_CFG_DEC_VAL_MASK, 1);\n\tcfg |= FIELD_PREP(GAUDI2_PKT_FENCE_CFG_TARGET_VAL_MASK, 1);\n\tcfg |= FIELD_PREP(GAUDI2_PKT_FENCE_CFG_ID_MASK, 2);\n\n\tctl = FIELD_PREP(GAUDI2_PKT_CTL_OPCODE_MASK, PACKET_FENCE);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI2_PKT_CTL_MB_MASK, 1);\n\n\tpkt->cfg = cpu_to_le32(cfg);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic u32 gaudi2_gen_wait_cb(struct hl_device *hdev, struct hl_gen_wait_properties *prop)\n{\n\tstruct hl_cb *cb = prop->data;\n\tvoid *buf = (void *) (uintptr_t) (cb->kernel_address);\n\n\tu64 monitor_base, fence_addr = 0;\n\tu32 stream_index, size = prop->size;\n\tu16 msg_addr_offset;\n\n\tstream_index = prop->q_idx % 4;\n\tfence_addr = CFG_BASE + gaudi2_qm_blocks_bases[prop->q_idx] +\n\t\t\tQM_FENCE2_OFFSET + stream_index * 4;\n\n\t \n\tmonitor_base = mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0;\n\n\t \n\tmsg_addr_offset = (mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + prop->mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi2_add_mon_msg_short(buf + size, (u32) fence_addr, msg_addr_offset);\n\n\t \n\tmsg_addr_offset = (mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0 + prop->mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi2_add_mon_msg_short(buf + size, (u32) (fence_addr >> 32), msg_addr_offset);\n\n\t \n\tmsg_addr_offset = (mmDCORE0_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + prop->mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi2_add_mon_msg_short(buf + size, 1, msg_addr_offset);\n\n\t \n\tmsg_addr_offset = (mmDCORE0_SYNC_MNGR_OBJS_MON_ARM_0 + prop->mon_id * 4) - monitor_base;\n\n\tsize += gaudi2_add_arm_monitor_pkt(hdev, buf + size, prop->sob_base, prop->sob_mask,\n\t\t\t\t\t\tprop->sob_val, msg_addr_offset);\n\n\t \n\tsize += gaudi2_add_fence_pkt(buf + size);\n\n\treturn size;\n}\n\nstatic void gaudi2_reset_sob(struct hl_device *hdev, void *data)\n{\n\tstruct hl_hw_sob *hw_sob = data;\n\n\tdev_dbg(hdev->dev, \"reset SOB, q_idx: %d, sob_id: %d\\n\", hw_sob->q_idx, hw_sob->sob_id);\n\n\tWREG32(mmDCORE0_SYNC_MNGR_OBJS_SOB_OBJ_0 + hw_sob->sob_id * 4, 0);\n\n\tkref_init(&hw_sob->kref);\n}\n\nstatic void gaudi2_reset_sob_group(struct hl_device *hdev, u16 sob_group)\n{\n}\n\nstatic u64 gaudi2_get_device_time(struct hl_device *hdev)\n{\n\tu64 device_time = ((u64) RREG32(mmPSOC_TIMESTAMP_CNTCVU)) << 32;\n\n\treturn device_time | RREG32(mmPSOC_TIMESTAMP_CNTCVL);\n}\n\nstatic int gaudi2_collective_wait_init_cs(struct hl_cs *cs)\n{\n\treturn 0;\n}\n\nstatic int gaudi2_collective_wait_create_jobs(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\t\t\t\tstruct hl_cs *cs, u32 wait_queue_id,\n\t\t\t\t\tu32 collective_engine_id, u32 encaps_signal_offset)\n{\n\treturn -EINVAL;\n}\n\n \nstatic u64 gaudi2_mmu_scramble_addr(struct hl_device *hdev, u64 raw_addr)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 divisor, mod_va;\n\tu64 div_va;\n\n\t \n\tif (hl_mem_area_inside_range(raw_addr, sizeof(raw_addr), DRAM_PHYS_BASE,\n\t\t\t\t\t\t\t\t\tVA_HBM_SPACE_END)) {\n\n\t\tdivisor = prop->num_functional_hbms * GAUDI2_HBM_MMU_SCRM_MEM_SIZE;\n\t\tdiv_va = div_u64_rem(raw_addr & GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK, divisor, &mod_va);\n\t\treturn (raw_addr & ~GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK) |\n\t\t\t(div_va << GAUDI2_HBM_MMU_SCRM_DIV_SHIFT) |\n\t\t\t(mod_va << GAUDI2_HBM_MMU_SCRM_MOD_SHIFT);\n\t}\n\n\treturn raw_addr;\n}\n\nstatic u64 gaudi2_mmu_descramble_addr(struct hl_device *hdev, u64 scrambled_addr)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 divisor, mod_va;\n\tu64 div_va;\n\n\t \n\tif (hl_mem_area_inside_range(scrambled_addr, sizeof(scrambled_addr), DRAM_PHYS_BASE,\n\t\t\t\t\t\t\t\t\tVA_HBM_SPACE_END)) {\n\n\t\tdivisor = prop->num_functional_hbms * GAUDI2_HBM_MMU_SCRM_MEM_SIZE;\n\t\tdiv_va = div_u64_rem(scrambled_addr & GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK,\n\t\t\t\t\tPAGE_SIZE_64MB, &mod_va);\n\n\t\treturn ((scrambled_addr & ~GAUDI2_HBM_MMU_SCRM_ADDRESS_MASK) +\n\t\t\t\t\t(div_va * divisor + mod_va));\n\t}\n\n\treturn scrambled_addr;\n}\n\nstatic u32 gaudi2_get_dec_base_addr(struct hl_device *hdev, u32 core_id)\n{\n\tu32 base = 0, dcore_id, dec_id;\n\n\tif (core_id >= NUMBER_OF_DEC) {\n\t\tdev_err(hdev->dev, \"Unexpected core number %d for DEC\\n\", core_id);\n\t\tgoto out;\n\t}\n\n\tif (core_id < 8) {\n\t\tdcore_id = core_id / NUM_OF_DEC_PER_DCORE;\n\t\tdec_id = core_id % NUM_OF_DEC_PER_DCORE;\n\n\t\tbase = mmDCORE0_DEC0_CMD_BASE + dcore_id * DCORE_OFFSET +\n\t\t\t\tdec_id * DCORE_VDEC_OFFSET;\n\t} else {\n\t\t \n\t\tbase = mmPCIE_DEC0_CMD_BASE + ((core_id % 8) * PCIE_VDEC_OFFSET);\n\t}\nout:\n\treturn base;\n}\n\nstatic int gaudi2_get_hw_block_id(struct hl_device *hdev, u64 block_addr,\n\t\t\t\tu32 *block_size, u32 *block_id)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tint i;\n\n\tfor (i = 0 ; i < NUM_USER_MAPPED_BLOCKS ; i++) {\n\t\tif (block_addr == CFG_BASE + gaudi2->mapped_blocks[i].address) {\n\t\t\t*block_id = i;\n\t\t\tif (block_size)\n\t\t\t\t*block_size = gaudi2->mapped_blocks[i].size;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tdev_err(hdev->dev, \"Invalid block address %#llx\", block_addr);\n\n\treturn -EINVAL;\n}\n\nstatic int gaudi2_block_mmap(struct hl_device *hdev, struct vm_area_struct *vma,\n\t\t\tu32 block_id, u32 block_size)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu64 offset_in_bar;\n\tu64 address;\n\tint rc;\n\n\tif (block_id >= NUM_USER_MAPPED_BLOCKS) {\n\t\tdev_err(hdev->dev, \"Invalid block id %u\", block_id);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (block_size != gaudi2->mapped_blocks[block_id].size) {\n\t\tdev_err(hdev->dev, \"Invalid block size %u\", block_size);\n\t\treturn -EINVAL;\n\t}\n\n\toffset_in_bar = CFG_BASE + gaudi2->mapped_blocks[block_id].address - STM_FLASH_BASE_ADDR;\n\n\taddress = pci_resource_start(hdev->pdev, SRAM_CFG_BAR_ID) + offset_in_bar;\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP |\n\t\t\tVM_DONTCOPY | VM_NORESERVE);\n\n\trc = remap_pfn_range(vma, vma->vm_start, address >> PAGE_SHIFT,\n\t\t\tblock_size, vma->vm_page_prot);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"remap_pfn_range error %d\", rc);\n\n\treturn rc;\n}\n\nstatic void gaudi2_enable_events_from_fw(struct hl_device *hdev)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tstruct cpu_dyn_regs *dyn_regs = &hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 irq_handler_offset = le32_to_cpu(dyn_regs->gic_host_ints_irq);\n\n\tif (gaudi2->hw_cap_initialized & HW_CAP_CPU_Q)\n\t\tWREG32(irq_handler_offset,\n\t\t\tgaudi2_irq_map_table[GAUDI2_EVENT_CPU_INTS_REGISTER].cpu_id);\n}\n\nstatic int gaudi2_get_mmu_base(struct hl_device *hdev, u64 mmu_id, u32 *mmu_base)\n{\n\tswitch (mmu_id) {\n\tcase HW_CAP_DCORE0_DMMU0:\n\t\t*mmu_base = mmDCORE0_HMMU0_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE0_DMMU1:\n\t\t*mmu_base = mmDCORE0_HMMU1_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE0_DMMU2:\n\t\t*mmu_base = mmDCORE0_HMMU2_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE0_DMMU3:\n\t\t*mmu_base = mmDCORE0_HMMU3_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE1_DMMU0:\n\t\t*mmu_base = mmDCORE1_HMMU0_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE1_DMMU1:\n\t\t*mmu_base = mmDCORE1_HMMU1_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE1_DMMU2:\n\t\t*mmu_base = mmDCORE1_HMMU2_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE1_DMMU3:\n\t\t*mmu_base = mmDCORE1_HMMU3_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE2_DMMU0:\n\t\t*mmu_base = mmDCORE2_HMMU0_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE2_DMMU1:\n\t\t*mmu_base = mmDCORE2_HMMU1_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE2_DMMU2:\n\t\t*mmu_base = mmDCORE2_HMMU2_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE2_DMMU3:\n\t\t*mmu_base = mmDCORE2_HMMU3_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE3_DMMU0:\n\t\t*mmu_base = mmDCORE3_HMMU0_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE3_DMMU1:\n\t\t*mmu_base = mmDCORE3_HMMU1_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE3_DMMU2:\n\t\t*mmu_base = mmDCORE3_HMMU2_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_DCORE3_DMMU3:\n\t\t*mmu_base = mmDCORE3_HMMU3_MMU_BASE;\n\t\tbreak;\n\tcase HW_CAP_PMMU:\n\t\t*mmu_base = mmPMMU_HBW_MMU_BASE;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void gaudi2_ack_mmu_error(struct hl_device *hdev, u64 mmu_id)\n{\n\tbool is_pmmu = (mmu_id == HW_CAP_PMMU);\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\tu32 mmu_base;\n\n\tif (!(gaudi2->hw_cap_initialized & mmu_id))\n\t\treturn;\n\n\tif (gaudi2_get_mmu_base(hdev, mmu_id, &mmu_base))\n\t\treturn;\n\n\tgaudi2_handle_page_error(hdev, mmu_base, is_pmmu, NULL);\n\tgaudi2_handle_access_error(hdev, mmu_base, is_pmmu);\n}\n\nstatic int gaudi2_ack_mmu_page_fault_or_access_error(struct hl_device *hdev, u64 mmu_cap_mask)\n{\n\tu32 i, mmu_id, num_of_hmmus = NUM_OF_HMMU_PER_DCORE * NUM_OF_DCORES;\n\n\t \n\tfor (i = 0 ; i < num_of_hmmus ; i++) {\n\t\tmmu_id = HW_CAP_DCORE0_DMMU0 << i;\n\n\t\tif (mmu_cap_mask & mmu_id)\n\t\t\tgaudi2_ack_mmu_error(hdev, mmu_id);\n\t}\n\n\t \n\tif (mmu_cap_mask & HW_CAP_PMMU)\n\t\tgaudi2_ack_mmu_error(hdev, HW_CAP_PMMU);\n\n\treturn 0;\n}\n\nstatic void gaudi2_get_msi_info(__le32 *table)\n{\n\ttable[CPUCP_EVENT_QUEUE_MSI_TYPE] = cpu_to_le32(GAUDI2_EVENT_QUEUE_MSIX_IDX);\n}\n\nstatic int gaudi2_map_pll_idx_to_fw_idx(u32 pll_idx)\n{\n\tswitch (pll_idx) {\n\tcase HL_GAUDI2_CPU_PLL: return CPU_PLL;\n\tcase HL_GAUDI2_PCI_PLL: return PCI_PLL;\n\tcase HL_GAUDI2_NIC_PLL: return NIC_PLL;\n\tcase HL_GAUDI2_DMA_PLL: return DMA_PLL;\n\tcase HL_GAUDI2_MESH_PLL: return MESH_PLL;\n\tcase HL_GAUDI2_MME_PLL: return MME_PLL;\n\tcase HL_GAUDI2_TPC_PLL: return TPC_PLL;\n\tcase HL_GAUDI2_IF_PLL: return IF_PLL;\n\tcase HL_GAUDI2_SRAM_PLL: return SRAM_PLL;\n\tcase HL_GAUDI2_HBM_PLL: return HBM_PLL;\n\tcase HL_GAUDI2_VID_PLL: return VID_PLL;\n\tcase HL_GAUDI2_MSS_PLL: return MSS_PLL;\n\tdefault: return -EINVAL;\n\t}\n}\n\nstatic int gaudi2_gen_sync_to_engine_map(struct hl_device *hdev, struct hl_sync_to_engine_map *map)\n{\n\t \n\treturn 0;\n}\n\nstatic int gaudi2_monitor_valid(struct hl_mon_state_dump *mon)\n{\n\t \n\treturn 0;\n}\n\nstatic int gaudi2_print_single_monitor(char **buf, size_t *size, size_t *offset,\n\t\t\t\tstruct hl_device *hdev, struct hl_mon_state_dump *mon)\n{\n\t \n\treturn 0;\n}\n\n\nstatic int gaudi2_print_fences_single_engine(struct hl_device *hdev, u64 base_offset,\n\t\t\t\tu64 status_base_offset, enum hl_sync_engine_type engine_type,\n\t\t\t\tu32 engine_id, char **buf, size_t *size, size_t *offset)\n{\n\t \n\treturn 0;\n}\n\n\nstatic struct hl_state_dump_specs_funcs gaudi2_state_dump_funcs = {\n\t.monitor_valid = gaudi2_monitor_valid,\n\t.print_single_monitor = gaudi2_print_single_monitor,\n\t.gen_sync_to_engine_map = gaudi2_gen_sync_to_engine_map,\n\t.print_fences_single_engine = gaudi2_print_fences_single_engine,\n};\n\nstatic void gaudi2_state_dump_init(struct hl_device *hdev)\n{\n\t \n\thdev->state_dump_specs.props = gaudi2_state_dump_specs_props;\n\thdev->state_dump_specs.funcs = gaudi2_state_dump_funcs;\n}\n\nstatic u32 gaudi2_get_sob_addr(struct hl_device *hdev, u32 sob_id)\n{\n\treturn 0;\n}\n\nstatic u32 *gaudi2_get_stream_master_qid_arr(void)\n{\n\treturn NULL;\n}\n\nstatic void gaudi2_add_device_attr(struct hl_device *hdev, struct attribute_group *dev_clk_attr_grp,\n\t\t\t\tstruct attribute_group *dev_vrm_attr_grp)\n{\n\thl_sysfs_add_dev_clk_attr(hdev, dev_clk_attr_grp);\n\thl_sysfs_add_dev_vrm_attr(hdev, dev_vrm_attr_grp);\n}\n\nstatic int gaudi2_mmu_get_real_page_size(struct hl_device *hdev, struct hl_mmu_properties *mmu_prop,\n\t\t\t\t\tu32 page_size, u32 *real_page_size, bool is_dram_addr)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\t \n\tif (!is_dram_addr) {\n\t\tif (page_size % mmu_prop->page_size)\n\t\t\tgoto page_size_err;\n\n\t\t*real_page_size = mmu_prop->page_size;\n\t\treturn 0;\n\t}\n\n\tif ((page_size % prop->dram_page_size) || (prop->dram_page_size > mmu_prop->page_size))\n\t\tgoto page_size_err;\n\n\t \n\t*real_page_size = prop->dram_page_size;\n\n\treturn 0;\n\npage_size_err:\n\tdev_err(hdev->dev, \"page size of %u is not %uKB aligned, can't map\\n\",\n\t\t\t\t\t\t\tpage_size, mmu_prop->page_size >> 10);\n\treturn -EFAULT;\n}\n\nstatic int gaudi2_get_monitor_dump(struct hl_device *hdev, void *data)\n{\n\treturn -EOPNOTSUPP;\n}\n\nint gaudi2_send_device_activity(struct hl_device *hdev, bool open)\n{\n\tstruct gaudi2_device *gaudi2 = hdev->asic_specific;\n\n\tif (!(gaudi2->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_send_device_activity(hdev, open);\n}\n\nstatic const struct hl_asic_funcs gaudi2_funcs = {\n\t.early_init = gaudi2_early_init,\n\t.early_fini = gaudi2_early_fini,\n\t.late_init = gaudi2_late_init,\n\t.late_fini = gaudi2_late_fini,\n\t.sw_init = gaudi2_sw_init,\n\t.sw_fini = gaudi2_sw_fini,\n\t.hw_init = gaudi2_hw_init,\n\t.hw_fini = gaudi2_hw_fini,\n\t.halt_engines = gaudi2_halt_engines,\n\t.suspend = gaudi2_suspend,\n\t.resume = gaudi2_resume,\n\t.mmap = gaudi2_mmap,\n\t.ring_doorbell = gaudi2_ring_doorbell,\n\t.pqe_write = gaudi2_pqe_write,\n\t.asic_dma_alloc_coherent = gaudi2_dma_alloc_coherent,\n\t.asic_dma_free_coherent = gaudi2_dma_free_coherent,\n\t.scrub_device_mem = gaudi2_scrub_device_mem,\n\t.scrub_device_dram = gaudi2_scrub_device_dram,\n\t.get_int_queue_base = NULL,\n\t.test_queues = gaudi2_test_queues,\n\t.asic_dma_pool_zalloc = gaudi2_dma_pool_zalloc,\n\t.asic_dma_pool_free = gaudi2_dma_pool_free,\n\t.cpu_accessible_dma_pool_alloc = gaudi2_cpu_accessible_dma_pool_alloc,\n\t.cpu_accessible_dma_pool_free = gaudi2_cpu_accessible_dma_pool_free,\n\t.asic_dma_unmap_single = gaudi2_dma_unmap_single,\n\t.asic_dma_map_single = gaudi2_dma_map_single,\n\t.hl_dma_unmap_sgtable = hl_dma_unmap_sgtable,\n\t.cs_parser = gaudi2_cs_parser,\n\t.asic_dma_map_sgtable = hl_dma_map_sgtable,\n\t.add_end_of_cb_packets = NULL,\n\t.update_eq_ci = gaudi2_update_eq_ci,\n\t.context_switch = gaudi2_context_switch,\n\t.restore_phase_topology = gaudi2_restore_phase_topology,\n\t.debugfs_read_dma = gaudi2_debugfs_read_dma,\n\t.add_device_attr = gaudi2_add_device_attr,\n\t.handle_eqe = gaudi2_handle_eqe,\n\t.get_events_stat = gaudi2_get_events_stat,\n\t.read_pte = NULL,\n\t.write_pte = NULL,\n\t.mmu_invalidate_cache = gaudi2_mmu_invalidate_cache,\n\t.mmu_invalidate_cache_range = gaudi2_mmu_invalidate_cache_range,\n\t.mmu_prefetch_cache_range = NULL,\n\t.send_heartbeat = gaudi2_send_heartbeat,\n\t.debug_coresight = gaudi2_debug_coresight,\n\t.is_device_idle = gaudi2_is_device_idle,\n\t.compute_reset_late_init = gaudi2_compute_reset_late_init,\n\t.hw_queues_lock = gaudi2_hw_queues_lock,\n\t.hw_queues_unlock = gaudi2_hw_queues_unlock,\n\t.get_pci_id = gaudi2_get_pci_id,\n\t.get_eeprom_data = gaudi2_get_eeprom_data,\n\t.get_monitor_dump = gaudi2_get_monitor_dump,\n\t.send_cpu_message = gaudi2_send_cpu_message,\n\t.pci_bars_map = gaudi2_pci_bars_map,\n\t.init_iatu = gaudi2_init_iatu,\n\t.rreg = hl_rreg,\n\t.wreg = hl_wreg,\n\t.halt_coresight = gaudi2_halt_coresight,\n\t.ctx_init = gaudi2_ctx_init,\n\t.ctx_fini = gaudi2_ctx_fini,\n\t.pre_schedule_cs = gaudi2_pre_schedule_cs,\n\t.get_queue_id_for_cq = gaudi2_get_queue_id_for_cq,\n\t.load_firmware_to_device = NULL,\n\t.load_boot_fit_to_device = NULL,\n\t.get_signal_cb_size = gaudi2_get_signal_cb_size,\n\t.get_wait_cb_size = gaudi2_get_wait_cb_size,\n\t.gen_signal_cb = gaudi2_gen_signal_cb,\n\t.gen_wait_cb = gaudi2_gen_wait_cb,\n\t.reset_sob = gaudi2_reset_sob,\n\t.reset_sob_group = gaudi2_reset_sob_group,\n\t.get_device_time = gaudi2_get_device_time,\n\t.pb_print_security_errors = gaudi2_pb_print_security_errors,\n\t.collective_wait_init_cs = gaudi2_collective_wait_init_cs,\n\t.collective_wait_create_jobs = gaudi2_collective_wait_create_jobs,\n\t.get_dec_base_addr = gaudi2_get_dec_base_addr,\n\t.scramble_addr = gaudi2_mmu_scramble_addr,\n\t.descramble_addr = gaudi2_mmu_descramble_addr,\n\t.ack_protection_bits_errors = gaudi2_ack_protection_bits_errors,\n\t.get_hw_block_id = gaudi2_get_hw_block_id,\n\t.hw_block_mmap = gaudi2_block_mmap,\n\t.enable_events_from_fw = gaudi2_enable_events_from_fw,\n\t.ack_mmu_errors = gaudi2_ack_mmu_page_fault_or_access_error,\n\t.get_msi_info = gaudi2_get_msi_info,\n\t.map_pll_idx_to_fw_idx = gaudi2_map_pll_idx_to_fw_idx,\n\t.init_firmware_preload_params = gaudi2_init_firmware_preload_params,\n\t.init_firmware_loader = gaudi2_init_firmware_loader,\n\t.init_cpu_scrambler_dram = gaudi2_init_scrambler_hbm,\n\t.state_dump_init = gaudi2_state_dump_init,\n\t.get_sob_addr = &gaudi2_get_sob_addr,\n\t.set_pci_memory_regions = gaudi2_set_pci_memory_regions,\n\t.get_stream_master_qid_arr = gaudi2_get_stream_master_qid_arr,\n\t.check_if_razwi_happened = gaudi2_check_if_razwi_happened,\n\t.mmu_get_real_page_size = gaudi2_mmu_get_real_page_size,\n\t.access_dev_mem = hl_access_dev_mem,\n\t.set_dram_bar_base = gaudi2_set_hbm_bar_base,\n\t.set_engine_cores = gaudi2_set_engine_cores,\n\t.set_engines = gaudi2_set_engines,\n\t.send_device_activity = gaudi2_send_device_activity,\n\t.set_dram_properties = gaudi2_set_dram_properties,\n\t.set_binning_masks = gaudi2_set_binning_masks,\n};\n\nvoid gaudi2_set_asic_funcs(struct hl_device *hdev)\n{\n\thdev->asic_funcs = &gaudi2_funcs;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}