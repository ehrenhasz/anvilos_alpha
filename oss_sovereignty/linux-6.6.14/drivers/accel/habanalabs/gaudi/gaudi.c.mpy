{
  "module_name": "gaudi.c",
  "hash_id": "ba53daf5e1751ae80ead9acbb588003fa012ae15c7a2166502be71481eefd632",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/gaudi/gaudi.c",
  "human_readable_source": "\n\n \n\n#include \"gaudiP.h\"\n#include \"../include/hw_ip/mmu/mmu_general.h\"\n#include \"../include/hw_ip/mmu/mmu_v1_1.h\"\n#include \"../include/gaudi/gaudi_masks.h\"\n#include \"../include/gaudi/gaudi_fw_if.h\"\n#include \"../include/gaudi/gaudi_reg_map.h\"\n#include \"../include/gaudi/gaudi_async_ids_map_extended.h\"\n\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/firmware.h>\n#include <linux/hwmon.h>\n#include <linux/iommu.h>\n#include <linux/seq_file.h>\n\n \n\n#define GAUDI_BOOT_FIT_FILE\t\"habanalabs/gaudi/gaudi-boot-fit.itb\"\n#define GAUDI_LINUX_FW_FILE\t\"habanalabs/gaudi/gaudi-fit.itb\"\n#define GAUDI_TPC_FW_FILE\t\"habanalabs/gaudi/gaudi_tpc.bin\"\n\n#define GAUDI_DMA_POOL_BLK_SIZE\t\t0x100  \n\n#define GAUDI_RESET_TIMEOUT_MSEC\t2000\t\t \n#define GAUDI_RESET_WAIT_MSEC\t\t1\t\t \n#define GAUDI_CPU_RESET_WAIT_MSEC\t200\t\t \n#define GAUDI_TEST_QUEUE_WAIT_USEC\t100000\t\t \n\n#define GAUDI_PLDM_RESET_WAIT_MSEC\t1000\t\t \n#define GAUDI_PLDM_HRESET_TIMEOUT_MSEC\t20000\t\t \n#define GAUDI_PLDM_TEST_QUEUE_WAIT_USEC\t1000000\t\t \n#define GAUDI_PLDM_MMU_TIMEOUT_USEC\t(MMU_CONFIG_TIMEOUT_USEC * 100)\n#define GAUDI_PLDM_QMAN0_TIMEOUT_USEC\t(HL_DEVICE_TIMEOUT_USEC * 30)\n#define GAUDI_PLDM_TPC_KERNEL_WAIT_USEC\t(HL_DEVICE_TIMEOUT_USEC * 30)\n#define GAUDI_BOOT_FIT_REQ_TIMEOUT_USEC\t4000000\t\t \n#define GAUDI_MSG_TO_CPU_TIMEOUT_USEC\t4000000\t\t \n#define GAUDI_WAIT_FOR_BL_TIMEOUT_USEC\t15000000\t \n\n#define GAUDI_QMAN0_FENCE_VAL\t\t0x72E91AB9\n\n#define GAUDI_MAX_STRING_LEN\t\t20\n\n#define GAUDI_CB_POOL_CB_CNT\t\t512\n#define GAUDI_CB_POOL_CB_SIZE\t\t0x20000  \n\n#define GAUDI_ALLOC_CPU_MEM_RETRY_CNT\t3\n\n#define GAUDI_NUM_OF_TPC_INTR_CAUSE\t20\n\n#define GAUDI_NUM_OF_QM_ERR_CAUSE\t16\n\n#define GAUDI_NUM_OF_QM_ARB_ERR_CAUSE\t3\n\n#define GAUDI_ARB_WDT_TIMEOUT\t\t0xEE6b27FF  \n\n#define HBM_SCRUBBING_TIMEOUT_US\t1000000  \n\n#define BIN_REG_STRING_SIZE\tsizeof(\"0b10101010101010101010101010101010\")\n\n#define MONITOR_SOB_STRING_SIZE\t\t256\n\nstatic u32 gaudi_stream_master[GAUDI_STREAM_MASTER_ARR_SIZE] = {\n\tGAUDI_QUEUE_ID_DMA_0_0,\n\tGAUDI_QUEUE_ID_DMA_0_1,\n\tGAUDI_QUEUE_ID_DMA_0_2,\n\tGAUDI_QUEUE_ID_DMA_0_3,\n\tGAUDI_QUEUE_ID_DMA_1_0,\n\tGAUDI_QUEUE_ID_DMA_1_1,\n\tGAUDI_QUEUE_ID_DMA_1_2,\n\tGAUDI_QUEUE_ID_DMA_1_3\n};\n\nstatic const u8 gaudi_dma_assignment[GAUDI_DMA_MAX] = {\n\t[GAUDI_PCI_DMA_1] = GAUDI_ENGINE_ID_DMA_0,\n\t[GAUDI_PCI_DMA_2] = GAUDI_ENGINE_ID_DMA_1,\n\t[GAUDI_HBM_DMA_1] = GAUDI_ENGINE_ID_DMA_2,\n\t[GAUDI_HBM_DMA_2] = GAUDI_ENGINE_ID_DMA_3,\n\t[GAUDI_HBM_DMA_3] = GAUDI_ENGINE_ID_DMA_4,\n\t[GAUDI_HBM_DMA_4] = GAUDI_ENGINE_ID_DMA_5,\n\t[GAUDI_HBM_DMA_5] = GAUDI_ENGINE_ID_DMA_6,\n\t[GAUDI_HBM_DMA_6] = GAUDI_ENGINE_ID_DMA_7\n};\n\nstatic const u8 gaudi_cq_assignment[NUMBER_OF_CMPLT_QUEUES] = {\n\t[0] = GAUDI_QUEUE_ID_DMA_0_0,\n\t[1] = GAUDI_QUEUE_ID_DMA_0_1,\n\t[2] = GAUDI_QUEUE_ID_DMA_0_2,\n\t[3] = GAUDI_QUEUE_ID_DMA_0_3,\n\t[4] = GAUDI_QUEUE_ID_DMA_1_0,\n\t[5] = GAUDI_QUEUE_ID_DMA_1_1,\n\t[6] = GAUDI_QUEUE_ID_DMA_1_2,\n\t[7] = GAUDI_QUEUE_ID_DMA_1_3,\n};\n\nstatic const u16 gaudi_packet_sizes[MAX_PACKET_ID] = {\n\t[PACKET_WREG_32]\t= sizeof(struct packet_wreg32),\n\t[PACKET_WREG_BULK]\t= sizeof(struct packet_wreg_bulk),\n\t[PACKET_MSG_LONG]\t= sizeof(struct packet_msg_long),\n\t[PACKET_MSG_SHORT]\t= sizeof(struct packet_msg_short),\n\t[PACKET_CP_DMA]\t\t= sizeof(struct packet_cp_dma),\n\t[PACKET_REPEAT]\t\t= sizeof(struct packet_repeat),\n\t[PACKET_MSG_PROT]\t= sizeof(struct packet_msg_prot),\n\t[PACKET_FENCE]\t\t= sizeof(struct packet_fence),\n\t[PACKET_LIN_DMA]\t= sizeof(struct packet_lin_dma),\n\t[PACKET_NOP]\t\t= sizeof(struct packet_nop),\n\t[PACKET_STOP]\t\t= sizeof(struct packet_stop),\n\t[PACKET_ARB_POINT]\t= sizeof(struct packet_arb_point),\n\t[PACKET_WAIT]\t\t= sizeof(struct packet_wait),\n\t[PACKET_LOAD_AND_EXE]\t= sizeof(struct packet_load_and_exe)\n};\n\nstatic inline bool validate_packet_id(enum packet_id id)\n{\n\tswitch (id) {\n\tcase PACKET_WREG_32:\n\tcase PACKET_WREG_BULK:\n\tcase PACKET_MSG_LONG:\n\tcase PACKET_MSG_SHORT:\n\tcase PACKET_CP_DMA:\n\tcase PACKET_REPEAT:\n\tcase PACKET_MSG_PROT:\n\tcase PACKET_FENCE:\n\tcase PACKET_LIN_DMA:\n\tcase PACKET_NOP:\n\tcase PACKET_STOP:\n\tcase PACKET_ARB_POINT:\n\tcase PACKET_WAIT:\n\tcase PACKET_LOAD_AND_EXE:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic const char * const\ngaudi_tpc_interrupts_cause[GAUDI_NUM_OF_TPC_INTR_CAUSE] = {\n\t\"tpc_address_exceed_slm\",\n\t\"tpc_div_by_0\",\n\t\"tpc_spu_mac_overflow\",\n\t\"tpc_spu_addsub_overflow\",\n\t\"tpc_spu_abs_overflow\",\n\t\"tpc_spu_fp_dst_nan_inf\",\n\t\"tpc_spu_fp_dst_denorm\",\n\t\"tpc_vpu_mac_overflow\",\n\t\"tpc_vpu_addsub_overflow\",\n\t\"tpc_vpu_abs_overflow\",\n\t\"tpc_vpu_fp_dst_nan_inf\",\n\t\"tpc_vpu_fp_dst_denorm\",\n\t\"tpc_assertions\",\n\t\"tpc_illegal_instruction\",\n\t\"tpc_pc_wrap_around\",\n\t\"tpc_qm_sw_err\",\n\t\"tpc_hbw_rresp_err\",\n\t\"tpc_hbw_bresp_err\",\n\t\"tpc_lbw_rresp_err\",\n\t\"tpc_lbw_bresp_err\"\n};\n\nstatic const char * const\ngaudi_qman_error_cause[GAUDI_NUM_OF_QM_ERR_CAUSE] = {\n\t\"PQ AXI HBW error\",\n\t\"CQ AXI HBW error\",\n\t\"CP AXI HBW error\",\n\t\"CP error due to undefined OPCODE\",\n\t\"CP encountered STOP OPCODE\",\n\t\"CP AXI LBW error\",\n\t\"CP WRREG32 or WRBULK returned error\",\n\t\"N/A\",\n\t\"FENCE 0 inc over max value and clipped\",\n\t\"FENCE 1 inc over max value and clipped\",\n\t\"FENCE 2 inc over max value and clipped\",\n\t\"FENCE 3 inc over max value and clipped\",\n\t\"FENCE 0 dec under min value and clipped\",\n\t\"FENCE 1 dec under min value and clipped\",\n\t\"FENCE 2 dec under min value and clipped\",\n\t\"FENCE 3 dec under min value and clipped\"\n};\n\nstatic const char * const\ngaudi_qman_arb_error_cause[GAUDI_NUM_OF_QM_ARB_ERR_CAUSE] = {\n\t\"Choice push while full error\",\n\t\"Choice Q watchdog error\",\n\t\"MSG AXI LBW returned with error\"\n};\n\nstatic enum hl_queue_type gaudi_queue_type[GAUDI_QUEUE_ID_SIZE] = {\n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_EXT,  \n\tQUEUE_TYPE_CPU,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n\tQUEUE_TYPE_INT,  \n};\n\nstatic struct hl_hw_obj_name_entry gaudi_so_id_to_str[] = {\n\t{ .id = 0,  .name = \"SYNC_OBJ_DMA_DOWN_FEEDBACK\" },\n\t{ .id = 1,  .name = \"SYNC_OBJ_DMA_UP_FEEDBACK\" },\n\t{ .id = 2,  .name = \"SYNC_OBJ_DMA_STATIC_DRAM_SRAM_FEEDBACK\" },\n\t{ .id = 3,  .name = \"SYNC_OBJ_DMA_SRAM_DRAM_FEEDBACK\" },\n\t{ .id = 4,  .name = \"SYNC_OBJ_FIRST_COMPUTE_FINISH\" },\n\t{ .id = 5,  .name = \"SYNC_OBJ_HOST_DRAM_DONE\" },\n\t{ .id = 6,  .name = \"SYNC_OBJ_DBG_CTR_DEPRECATED\" },\n\t{ .id = 7,  .name = \"SYNC_OBJ_DMA_ACTIVATIONS_DRAM_SRAM_FEEDBACK\" },\n\t{ .id = 8,  .name = \"SYNC_OBJ_ENGINE_SEM_MME_0\" },\n\t{ .id = 9,  .name = \"SYNC_OBJ_ENGINE_SEM_MME_1\" },\n\t{ .id = 10, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_0\" },\n\t{ .id = 11, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_1\" },\n\t{ .id = 12, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_2\" },\n\t{ .id = 13, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_3\" },\n\t{ .id = 14, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_4\" },\n\t{ .id = 15, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_5\" },\n\t{ .id = 16, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_6\" },\n\t{ .id = 17, .name = \"SYNC_OBJ_ENGINE_SEM_TPC_7\" },\n\t{ .id = 18, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_1\" },\n\t{ .id = 19, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_2\" },\n\t{ .id = 20, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_3\" },\n\t{ .id = 21, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_4\" },\n\t{ .id = 22, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_5\" },\n\t{ .id = 23, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_6\" },\n\t{ .id = 24, .name = \"SYNC_OBJ_ENGINE_SEM_DMA_7\" },\n\t{ .id = 25, .name = \"SYNC_OBJ_DBG_CTR_0\" },\n\t{ .id = 26, .name = \"SYNC_OBJ_DBG_CTR_1\" },\n};\n\nstatic struct hl_hw_obj_name_entry gaudi_monitor_id_to_str[] = {\n\t{ .id = 200, .name = \"MON_OBJ_DMA_DOWN_FEEDBACK_RESET\" },\n\t{ .id = 201, .name = \"MON_OBJ_DMA_UP_FEEDBACK_RESET\" },\n\t{ .id = 203, .name = \"MON_OBJ_DRAM_TO_SRAM_QUEUE_FENCE\" },\n\t{ .id = 204, .name = \"MON_OBJ_TPC_0_CLK_GATE\" },\n\t{ .id = 205, .name = \"MON_OBJ_TPC_1_CLK_GATE\" },\n\t{ .id = 206, .name = \"MON_OBJ_TPC_2_CLK_GATE\" },\n\t{ .id = 207, .name = \"MON_OBJ_TPC_3_CLK_GATE\" },\n\t{ .id = 208, .name = \"MON_OBJ_TPC_4_CLK_GATE\" },\n\t{ .id = 209, .name = \"MON_OBJ_TPC_5_CLK_GATE\" },\n\t{ .id = 210, .name = \"MON_OBJ_TPC_6_CLK_GATE\" },\n\t{ .id = 211, .name = \"MON_OBJ_TPC_7_CLK_GATE\" },\n};\n\nstatic s64 gaudi_state_dump_specs_props[] = {\n\t[SP_SYNC_OBJ_BASE_ADDR] = mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0,\n\t[SP_NEXT_SYNC_OBJ_ADDR] = NEXT_SYNC_OBJ_ADDR_INTERVAL,\n\t[SP_SYNC_OBJ_AMOUNT] = NUM_OF_SOB_IN_BLOCK,\n\t[SP_MON_OBJ_WR_ADDR_LOW] =\n\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0,\n\t[SP_MON_OBJ_WR_ADDR_HIGH] =\n\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0,\n\t[SP_MON_OBJ_WR_DATA] = mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_DATA_0,\n\t[SP_MON_OBJ_ARM_DATA] = mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_ARM_0,\n\t[SP_MON_OBJ_STATUS] = mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_STATUS_0,\n\t[SP_MONITORS_AMOUNT] = NUM_OF_MONITORS_IN_BLOCK,\n\t[SP_TPC0_CMDQ] = mmTPC0_QM_GLBL_CFG0,\n\t[SP_TPC0_CFG_SO] = mmTPC0_CFG_QM_SYNC_OBJECT_ADDR,\n\t[SP_NEXT_TPC] = mmTPC1_QM_GLBL_CFG0 - mmTPC0_QM_GLBL_CFG0,\n\t[SP_MME_CMDQ] = mmMME0_QM_GLBL_CFG0,\n\t[SP_MME_CFG_SO] = mmMME0_CTRL_ARCH_DESC_SYNC_OBJECT_ADDR_LOW_LOCAL,\n\t[SP_NEXT_MME] = mmMME2_QM_GLBL_CFG0 - mmMME0_QM_GLBL_CFG0,\n\t[SP_DMA_CMDQ] = mmDMA0_QM_GLBL_CFG0,\n\t[SP_DMA_CFG_SO] = mmDMA0_CORE_WR_COMP_ADDR_LO,\n\t[SP_DMA_QUEUES_OFFSET] = mmDMA1_QM_GLBL_CFG0 - mmDMA0_QM_GLBL_CFG0,\n\t[SP_NUM_OF_MME_ENGINES] = NUM_OF_MME_ENGINES,\n\t[SP_SUB_MME_ENG_NUM] = NUM_OF_MME_SUB_ENGINES,\n\t[SP_NUM_OF_DMA_ENGINES] = NUM_OF_DMA_ENGINES,\n\t[SP_NUM_OF_TPC_ENGINES] = NUM_OF_TPC_ENGINES,\n\t[SP_ENGINE_NUM_OF_QUEUES] = NUM_OF_QUEUES,\n\t[SP_ENGINE_NUM_OF_STREAMS] = NUM_OF_STREAMS,\n\t[SP_ENGINE_NUM_OF_FENCES] = NUM_OF_FENCES,\n\t[SP_FENCE0_CNT_OFFSET] =\n\t\tmmDMA0_QM_CP_FENCE0_CNT_0 - mmDMA0_QM_GLBL_CFG0,\n\t[SP_FENCE0_RDATA_OFFSET] =\n\t\tmmDMA0_QM_CP_FENCE0_RDATA_0 - mmDMA0_QM_GLBL_CFG0,\n\t[SP_CP_STS_OFFSET] = mmDMA0_QM_CP_STS_0 - mmDMA0_QM_GLBL_CFG0,\n\t[SP_NUM_CORES] = 1,\n};\n\nstatic const int gaudi_queue_id_to_engine_id[] = {\n\t[GAUDI_QUEUE_ID_DMA_0_0...GAUDI_QUEUE_ID_DMA_0_3] = GAUDI_ENGINE_ID_DMA_0,\n\t[GAUDI_QUEUE_ID_DMA_1_0...GAUDI_QUEUE_ID_DMA_1_3] = GAUDI_ENGINE_ID_DMA_1,\n\t[GAUDI_QUEUE_ID_CPU_PQ] = GAUDI_ENGINE_ID_SIZE,\n\t[GAUDI_QUEUE_ID_DMA_2_0...GAUDI_QUEUE_ID_DMA_2_3] = GAUDI_ENGINE_ID_DMA_2,\n\t[GAUDI_QUEUE_ID_DMA_3_0...GAUDI_QUEUE_ID_DMA_3_3] = GAUDI_ENGINE_ID_DMA_3,\n\t[GAUDI_QUEUE_ID_DMA_4_0...GAUDI_QUEUE_ID_DMA_4_3] = GAUDI_ENGINE_ID_DMA_4,\n\t[GAUDI_QUEUE_ID_DMA_5_0...GAUDI_QUEUE_ID_DMA_5_3] = GAUDI_ENGINE_ID_DMA_5,\n\t[GAUDI_QUEUE_ID_DMA_6_0...GAUDI_QUEUE_ID_DMA_6_3] = GAUDI_ENGINE_ID_DMA_6,\n\t[GAUDI_QUEUE_ID_DMA_7_0...GAUDI_QUEUE_ID_DMA_7_3] = GAUDI_ENGINE_ID_DMA_7,\n\t[GAUDI_QUEUE_ID_MME_0_0...GAUDI_QUEUE_ID_MME_0_3] = GAUDI_ENGINE_ID_MME_0,\n\t[GAUDI_QUEUE_ID_MME_1_0...GAUDI_QUEUE_ID_MME_1_3] = GAUDI_ENGINE_ID_MME_2,\n\t[GAUDI_QUEUE_ID_TPC_0_0...GAUDI_QUEUE_ID_TPC_0_3] = GAUDI_ENGINE_ID_TPC_0,\n\t[GAUDI_QUEUE_ID_TPC_1_0...GAUDI_QUEUE_ID_TPC_1_3] = GAUDI_ENGINE_ID_TPC_1,\n\t[GAUDI_QUEUE_ID_TPC_2_0...GAUDI_QUEUE_ID_TPC_2_3] = GAUDI_ENGINE_ID_TPC_2,\n\t[GAUDI_QUEUE_ID_TPC_3_0...GAUDI_QUEUE_ID_TPC_3_3] = GAUDI_ENGINE_ID_TPC_3,\n\t[GAUDI_QUEUE_ID_TPC_4_0...GAUDI_QUEUE_ID_TPC_4_3] = GAUDI_ENGINE_ID_TPC_4,\n\t[GAUDI_QUEUE_ID_TPC_5_0...GAUDI_QUEUE_ID_TPC_5_3] = GAUDI_ENGINE_ID_TPC_5,\n\t[GAUDI_QUEUE_ID_TPC_6_0...GAUDI_QUEUE_ID_TPC_6_3] = GAUDI_ENGINE_ID_TPC_6,\n\t[GAUDI_QUEUE_ID_TPC_7_0...GAUDI_QUEUE_ID_TPC_7_3] = GAUDI_ENGINE_ID_TPC_7,\n\t[GAUDI_QUEUE_ID_NIC_0_0...GAUDI_QUEUE_ID_NIC_0_3] = GAUDI_ENGINE_ID_NIC_0,\n\t[GAUDI_QUEUE_ID_NIC_1_0...GAUDI_QUEUE_ID_NIC_1_3] = GAUDI_ENGINE_ID_NIC_1,\n\t[GAUDI_QUEUE_ID_NIC_2_0...GAUDI_QUEUE_ID_NIC_2_3] = GAUDI_ENGINE_ID_NIC_2,\n\t[GAUDI_QUEUE_ID_NIC_3_0...GAUDI_QUEUE_ID_NIC_3_3] = GAUDI_ENGINE_ID_NIC_3,\n\t[GAUDI_QUEUE_ID_NIC_4_0...GAUDI_QUEUE_ID_NIC_4_3] = GAUDI_ENGINE_ID_NIC_4,\n\t[GAUDI_QUEUE_ID_NIC_5_0...GAUDI_QUEUE_ID_NIC_5_3] = GAUDI_ENGINE_ID_NIC_5,\n\t[GAUDI_QUEUE_ID_NIC_6_0...GAUDI_QUEUE_ID_NIC_6_3] = GAUDI_ENGINE_ID_NIC_6,\n\t[GAUDI_QUEUE_ID_NIC_7_0...GAUDI_QUEUE_ID_NIC_7_3] = GAUDI_ENGINE_ID_NIC_7,\n\t[GAUDI_QUEUE_ID_NIC_8_0...GAUDI_QUEUE_ID_NIC_8_3] = GAUDI_ENGINE_ID_NIC_8,\n\t[GAUDI_QUEUE_ID_NIC_9_0...GAUDI_QUEUE_ID_NIC_9_3] = GAUDI_ENGINE_ID_NIC_9,\n};\n\n \nstatic const char * const gaudi_sync_manager_names[] = {\n\t\"SYNC_MGR_E_N\",\n\t\"SYNC_MGR_W_N\",\n\t\"SYNC_MGR_E_S\",\n\t\"SYNC_MGR_W_S\",\n\tNULL\n};\n\nstruct ecc_info_extract_params {\n\tu64 block_address;\n\tu32 num_memories;\n\tbool derr;\n};\n\nstatic int gaudi_mmu_update_asid_hop0_addr(struct hl_device *hdev, u32 asid,\n\t\t\t\t\t\t\t\tu64 phys_addr);\nstatic int gaudi_send_job_on_qman0(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_job *job);\nstatic int gaudi_memset_device_memory(struct hl_device *hdev, u64 addr,\n\t\t\t\t\tu32 size, u64 val);\nstatic int gaudi_memset_registers(struct hl_device *hdev, u64 reg_base,\n\t\t\t\t\tu32 num_regs, u32 val);\nstatic int gaudi_run_tpc_kernel(struct hl_device *hdev, u64 tpc_kernel,\n\t\t\t\tu32 tpc_id);\nstatic int gaudi_mmu_clear_pgt_range(struct hl_device *hdev);\nstatic int gaudi_cpucp_info_get(struct hl_device *hdev);\nstatic void gaudi_disable_clock_gating(struct hl_device *hdev);\nstatic void gaudi_mmu_prepare(struct hl_device *hdev, u32 asid);\nstatic u32 gaudi_gen_signal_cb(struct hl_device *hdev, void *data, u16 sob_id,\n\t\t\t\tu32 size, bool eb);\nstatic u32 gaudi_gen_wait_cb(struct hl_device *hdev,\n\t\t\t\tstruct hl_gen_wait_properties *prop);\nstatic inline enum hl_collective_mode\nget_collective_mode(struct hl_device *hdev, u32 queue_id)\n{\n\tif (gaudi_queue_type[queue_id] == QUEUE_TYPE_EXT)\n\t\treturn HL_COLLECTIVE_MASTER;\n\n\tif (queue_id >= GAUDI_QUEUE_ID_DMA_5_0 &&\n\t\t\tqueue_id <= GAUDI_QUEUE_ID_DMA_5_3)\n\t\treturn HL_COLLECTIVE_SLAVE;\n\n\tif (queue_id >= GAUDI_QUEUE_ID_TPC_7_0 &&\n\t\t\tqueue_id <= GAUDI_QUEUE_ID_TPC_7_3)\n\t\treturn HL_COLLECTIVE_SLAVE;\n\n\tif (queue_id >= GAUDI_QUEUE_ID_NIC_0_0 &&\n\t\t\tqueue_id <= GAUDI_QUEUE_ID_NIC_9_3)\n\t\treturn HL_COLLECTIVE_SLAVE;\n\n\treturn HL_COLLECTIVE_NOT_SUPPORTED;\n}\n\nstatic inline void set_default_power_values(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\tif (hdev->card_type == cpucp_card_type_pmc) {\n\t\tprop->max_power_default = MAX_POWER_DEFAULT_PMC;\n\n\t\tif (prop->fw_security_enabled)\n\t\t\tprop->dc_power_default = DC_POWER_DEFAULT_PMC_SEC;\n\t\telse\n\t\t\tprop->dc_power_default = DC_POWER_DEFAULT_PMC;\n\t} else {\n\t\tprop->max_power_default = MAX_POWER_DEFAULT_PCI;\n\t\tprop->dc_power_default = DC_POWER_DEFAULT_PCI;\n\t}\n}\n\nstatic int gaudi_set_fixed_properties(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 num_sync_stream_queues = 0;\n\tint i;\n\n\tprop->max_queues = GAUDI_QUEUE_ID_SIZE;\n\tprop->hw_queues_props = kcalloc(prop->max_queues,\n\t\t\tsizeof(struct hw_queue_properties),\n\t\t\tGFP_KERNEL);\n\n\tif (!prop->hw_queues_props)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0 ; i < prop->max_queues ; i++) {\n\t\tif (gaudi_queue_type[i] == QUEUE_TYPE_EXT) {\n\t\t\tprop->hw_queues_props[i].type = QUEUE_TYPE_EXT;\n\t\t\tprop->hw_queues_props[i].driver_only = 0;\n\t\t\tprop->hw_queues_props[i].supports_sync_stream = 1;\n\t\t\tprop->hw_queues_props[i].cb_alloc_flags =\n\t\t\t\tCB_ALLOC_KERNEL;\n\t\t\tnum_sync_stream_queues++;\n\t\t} else if (gaudi_queue_type[i] == QUEUE_TYPE_CPU) {\n\t\t\tprop->hw_queues_props[i].type = QUEUE_TYPE_CPU;\n\t\t\tprop->hw_queues_props[i].driver_only = 1;\n\t\t\tprop->hw_queues_props[i].supports_sync_stream = 0;\n\t\t\tprop->hw_queues_props[i].cb_alloc_flags =\n\t\t\t\tCB_ALLOC_KERNEL;\n\t\t} else if (gaudi_queue_type[i] == QUEUE_TYPE_INT) {\n\t\t\tprop->hw_queues_props[i].type = QUEUE_TYPE_INT;\n\t\t\tprop->hw_queues_props[i].driver_only = 0;\n\t\t\tprop->hw_queues_props[i].supports_sync_stream = 0;\n\t\t\tprop->hw_queues_props[i].cb_alloc_flags =\n\t\t\t\tCB_ALLOC_USER;\n\n\t\t}\n\t\tprop->hw_queues_props[i].collective_mode =\n\t\t\t\t\t\tget_collective_mode(hdev, i);\n\t}\n\n\tprop->cache_line_size = DEVICE_CACHE_LINE_SIZE;\n\tprop->cfg_base_address = CFG_BASE;\n\tprop->device_dma_offset_for_host_access = HOST_PHYS_BASE;\n\tprop->host_base_address = HOST_PHYS_BASE;\n\tprop->host_end_address = prop->host_base_address + HOST_PHYS_SIZE;\n\tprop->completion_queues_count = NUMBER_OF_CMPLT_QUEUES;\n\tprop->completion_mode = HL_COMPLETION_MODE_JOB;\n\tprop->collective_first_sob = 0;\n\tprop->collective_first_mon = 0;\n\n\t \n\tprop->sync_stream_first_sob =\n\t\t\tALIGN(NUMBER_OF_SOBS_IN_GRP, HL_MAX_SOBS_PER_MONITOR)\n\t\t\t* QMAN_STREAMS * HL_RSVD_SOBS;\n\n\t \n\tprop->sync_stream_first_mon =\n\t\t\t(NUMBER_OF_COLLECTIVE_QUEUES * QMAN_STREAMS) +\n\t\t\t(NUMBER_OF_EXT_HW_QUEUES * 2);\n\n\tprop->dram_base_address = DRAM_PHYS_BASE;\n\tprop->dram_size = GAUDI_HBM_SIZE_32GB;\n\tprop->dram_end_address = prop->dram_base_address + prop->dram_size;\n\tprop->dram_user_base_address = DRAM_BASE_ADDR_USER;\n\n\tprop->sram_base_address = SRAM_BASE_ADDR;\n\tprop->sram_size = SRAM_SIZE;\n\tprop->sram_end_address = prop->sram_base_address + prop->sram_size;\n\tprop->sram_user_base_address =\n\t\t\tprop->sram_base_address + SRAM_USER_BASE_OFFSET;\n\n\tprop->mmu_cache_mng_addr = MMU_CACHE_MNG_ADDR;\n\tprop->mmu_cache_mng_size = MMU_CACHE_MNG_SIZE;\n\n\tprop->mmu_pgt_addr = MMU_PAGE_TABLES_ADDR;\n\tif (hdev->pldm)\n\t\tprop->mmu_pgt_size = 0x800000;  \n\telse\n\t\tprop->mmu_pgt_size = MMU_PAGE_TABLES_SIZE;\n\tprop->mmu_pte_size = HL_PTE_SIZE;\n\tprop->mmu_hop_table_size = HOP_TABLE_SIZE_512_PTE;\n\tprop->mmu_hop0_tables_total_size = HOP0_512_PTE_TABLES_TOTAL_SIZE;\n\tprop->dram_page_size = PAGE_SIZE_2MB;\n\tprop->device_mem_alloc_default_page_size = prop->dram_page_size;\n\tprop->dram_supports_virtual_memory = false;\n\n\tprop->pmmu.hop_shifts[MMU_HOP0] = MMU_V1_1_HOP0_SHIFT;\n\tprop->pmmu.hop_shifts[MMU_HOP1] = MMU_V1_1_HOP1_SHIFT;\n\tprop->pmmu.hop_shifts[MMU_HOP2] = MMU_V1_1_HOP2_SHIFT;\n\tprop->pmmu.hop_shifts[MMU_HOP3] = MMU_V1_1_HOP3_SHIFT;\n\tprop->pmmu.hop_shifts[MMU_HOP4] = MMU_V1_1_HOP4_SHIFT;\n\tprop->pmmu.hop_masks[MMU_HOP0] = MMU_V1_1_HOP0_MASK;\n\tprop->pmmu.hop_masks[MMU_HOP1] = MMU_V1_1_HOP1_MASK;\n\tprop->pmmu.hop_masks[MMU_HOP2] = MMU_V1_1_HOP2_MASK;\n\tprop->pmmu.hop_masks[MMU_HOP3] = MMU_V1_1_HOP3_MASK;\n\tprop->pmmu.hop_masks[MMU_HOP4] = MMU_V1_1_HOP4_MASK;\n\tprop->pmmu.start_addr = VA_HOST_SPACE_START;\n\tprop->pmmu.end_addr =\n\t\t\t(VA_HOST_SPACE_START + VA_HOST_SPACE_SIZE / 2) - 1;\n\tprop->pmmu.page_size = PAGE_SIZE_4KB;\n\tprop->pmmu.num_hops = MMU_ARCH_5_HOPS;\n\tprop->pmmu.last_mask = LAST_MASK;\n\t \n\tprop->pmmu.hop_table_size = prop->mmu_hop_table_size;\n\tprop->pmmu.hop0_tables_total_size = prop->mmu_hop0_tables_total_size;\n\n\t \n\tmemcpy(&prop->pmmu_huge, &prop->pmmu, sizeof(prop->pmmu));\n\tprop->pmmu_huge.page_size = PAGE_SIZE_2MB;\n\n\t \n\tmemcpy(&prop->dmmu, &prop->pmmu, sizeof(prop->pmmu));\n\tprop->dmmu.start_addr = (VA_HOST_SPACE_START + VA_HOST_SPACE_SIZE / 2);\n\tprop->dmmu.end_addr = VA_HOST_SPACE_END;\n\tprop->dmmu.page_size = PAGE_SIZE_2MB;\n\n\tprop->cfg_size = CFG_SIZE;\n\tprop->max_asid = MAX_ASID;\n\tprop->num_of_events = GAUDI_EVENT_SIZE;\n\tprop->max_num_of_engines = GAUDI_ENGINE_ID_SIZE;\n\tprop->tpc_enabled_mask = TPC_ENABLED_MASK;\n\n\tset_default_power_values(hdev);\n\n\tprop->cb_pool_cb_cnt = GAUDI_CB_POOL_CB_CNT;\n\tprop->cb_pool_cb_size = GAUDI_CB_POOL_CB_SIZE;\n\n\tprop->pcie_dbi_base_address = mmPCIE_DBI_BASE;\n\tprop->pcie_aux_dbi_reg_addr = CFG_BASE + mmPCIE_AUX_DBI;\n\n\tstrncpy(prop->cpucp_info.card_name, GAUDI_DEFAULT_CARD_NAME,\n\t\t\t\t\tCARD_NAME_MAX_LEN);\n\n\tprop->max_pending_cs = GAUDI_MAX_PENDING_CS;\n\n\tprop->first_available_user_sob[HL_GAUDI_WS_DCORE] =\n\t\t\tprop->sync_stream_first_sob +\n\t\t\t(num_sync_stream_queues * HL_RSVD_SOBS);\n\tprop->first_available_user_mon[HL_GAUDI_WS_DCORE] =\n\t\t\tprop->sync_stream_first_mon +\n\t\t\t(num_sync_stream_queues * HL_RSVD_MONS);\n\n\tprop->first_available_user_interrupt = USHRT_MAX;\n\tprop->tpc_interrupt_id = USHRT_MAX;\n\n\t \n\tprop->eq_interrupt_id = 0;\n\n\tfor (i = 0 ; i < HL_MAX_DCORES ; i++)\n\t\tprop->first_available_cq[i] = USHRT_MAX;\n\n\tprop->fw_cpu_boot_dev_sts0_valid = false;\n\tprop->fw_cpu_boot_dev_sts1_valid = false;\n\tprop->hard_reset_done_by_fw = false;\n\tprop->gic_interrupts_enable = true;\n\n\tprop->server_type = HL_SERVER_TYPE_UNKNOWN;\n\n\tprop->clk_pll_index = HL_GAUDI_MME_PLL;\n\tprop->max_freq_value = GAUDI_MAX_CLK_FREQ;\n\n\tprop->use_get_power_for_reset_history = true;\n\n\tprop->configurable_stop_on_err = true;\n\n\tprop->set_max_power_on_device_init = true;\n\n\tprop->dma_mask = 48;\n\n\tprop->hbw_flush_reg = mmPCIE_WRAP_RR_ELBI_RD_SEC_REG_CTRL;\n\n\treturn 0;\n}\n\nstatic int gaudi_pci_bars_map(struct hl_device *hdev)\n{\n\tstatic const char * const name[] = {\"SRAM\", \"CFG\", \"HBM\"};\n\tbool is_wc[3] = {false, false, true};\n\tint rc;\n\n\trc = hl_pci_bars_map(hdev, name, is_wc);\n\tif (rc)\n\t\treturn rc;\n\n\thdev->rmmio = hdev->pcie_bar[CFG_BAR_ID] +\n\t\t\t(CFG_BASE - SPI_FLASH_BASE_ADDR);\n\n\treturn 0;\n}\n\nstatic u64 gaudi_set_hbm_bar_base(struct hl_device *hdev, u64 addr)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct hl_inbound_pci_region pci_region;\n\tu64 old_addr = addr;\n\tint rc;\n\n\tif ((gaudi) && (gaudi->hbm_bar_cur_addr == addr))\n\t\treturn old_addr;\n\n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\treturn U64_MAX;\n\n\t \n\tpci_region.mode = PCI_BAR_MATCH_MODE;\n\tpci_region.bar = HBM_BAR_ID;\n\tpci_region.addr = addr;\n\trc = hl_pci_set_inbound_region(hdev, 2, &pci_region);\n\tif (rc)\n\t\treturn U64_MAX;\n\n\tif (gaudi) {\n\t\told_addr = gaudi->hbm_bar_cur_addr;\n\t\tgaudi->hbm_bar_cur_addr = addr;\n\t}\n\n\treturn old_addr;\n}\n\nstatic int gaudi_init_iatu(struct hl_device *hdev)\n{\n\tstruct hl_inbound_pci_region inbound_region;\n\tstruct hl_outbound_pci_region outbound_region;\n\tint rc;\n\n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\treturn 0;\n\n\t \n\tinbound_region.mode = PCI_BAR_MATCH_MODE;\n\tinbound_region.bar = SRAM_BAR_ID;\n\tinbound_region.addr = SRAM_BASE_ADDR;\n\trc = hl_pci_set_inbound_region(hdev, 0, &inbound_region);\n\tif (rc)\n\t\tgoto done;\n\n\t \n\tinbound_region.mode = PCI_BAR_MATCH_MODE;\n\tinbound_region.bar = CFG_BAR_ID;\n\tinbound_region.addr = SPI_FLASH_BASE_ADDR;\n\trc = hl_pci_set_inbound_region(hdev, 1, &inbound_region);\n\tif (rc)\n\t\tgoto done;\n\n\t \n\tinbound_region.mode = PCI_BAR_MATCH_MODE;\n\tinbound_region.bar = HBM_BAR_ID;\n\tinbound_region.addr = DRAM_PHYS_BASE;\n\trc = hl_pci_set_inbound_region(hdev, 2, &inbound_region);\n\tif (rc)\n\t\tgoto done;\n\n\t \n\toutbound_region.addr = HOST_PHYS_BASE;\n\toutbound_region.size = HOST_PHYS_SIZE;\n\trc = hl_pci_set_outbound_region(hdev, &outbound_region);\n\ndone:\n\treturn rc;\n}\n\nstatic enum hl_device_hw_state gaudi_get_hw_state(struct hl_device *hdev)\n{\n\treturn RREG32(mmHW_STATE);\n}\n\nstatic int gaudi_early_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct pci_dev *pdev = hdev->pdev;\n\tresource_size_t pci_bar_size;\n\tu32 fw_boot_status;\n\tint rc;\n\n\trc = gaudi_set_fixed_properties(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed setting fixed properties\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tpci_bar_size = pci_resource_len(pdev, SRAM_BAR_ID);\n\n\tif (pci_bar_size != SRAM_BAR_SIZE) {\n\t\tdev_err(hdev->dev, \"Not \" HL_NAME \"? BAR %d size %pa, expecting %llu\\n\",\n\t\t\tSRAM_BAR_ID, &pci_bar_size, SRAM_BAR_SIZE);\n\t\trc = -ENODEV;\n\t\tgoto free_queue_props;\n\t}\n\n\tpci_bar_size = pci_resource_len(pdev, CFG_BAR_ID);\n\n\tif (pci_bar_size != CFG_BAR_SIZE) {\n\t\tdev_err(hdev->dev, \"Not \" HL_NAME \"? BAR %d size %pa, expecting %llu\\n\",\n\t\t\tCFG_BAR_ID, &pci_bar_size, CFG_BAR_SIZE);\n\t\trc = -ENODEV;\n\t\tgoto free_queue_props;\n\t}\n\n\tprop->dram_pci_bar_size = pci_resource_len(pdev, HBM_BAR_ID);\n\thdev->dram_pci_bar_start = pci_resource_start(pdev, HBM_BAR_ID);\n\n\t \n\tif (hdev->asic_prop.fw_security_enabled) {\n\t\thdev->asic_prop.iatu_done_by_fw = true;\n\n\t\t \n\t\thdev->asic_prop.gic_interrupts_enable = false;\n\t\tgoto pci_init;\n\t}\n\n\trc = hl_pci_elbi_read(hdev, CFG_BASE + mmCPU_BOOT_DEV_STS0,\n\t\t\t\t&fw_boot_status);\n\tif (rc)\n\t\tgoto free_queue_props;\n\n\t \n\tif ((fw_boot_status & CPU_BOOT_DEV_STS0_ENABLED) &&\n\t\t\t(fw_boot_status & CPU_BOOT_DEV_STS0_FW_IATU_CONF_EN))\n\t\thdev->asic_prop.iatu_done_by_fw = true;\n\npci_init:\n\trc = hl_pci_init(hdev);\n\tif (rc)\n\t\tgoto free_queue_props;\n\n\t \n\trc = hl_fw_read_preboot_status(hdev);\n\tif (rc) {\n\t\tif (hdev->reset_on_preboot_fail)\n\t\t\t \n\t\t\thdev->asic_funcs->hw_fini(hdev, true, false);\n\t\tgoto pci_fini;\n\t}\n\n\tif (gaudi_get_hw_state(hdev) == HL_DEVICE_HW_STATE_DIRTY) {\n\t\tdev_dbg(hdev->dev, \"H/W state is dirty, must reset before initializing\\n\");\n\t\trc = hdev->asic_funcs->hw_fini(hdev, true, false);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"failed to reset HW in dirty state (%d)\\n\", rc);\n\t\t\tgoto pci_fini;\n\t\t}\n\t}\n\n\treturn 0;\n\npci_fini:\n\thl_pci_fini(hdev);\nfree_queue_props:\n\tkfree(hdev->asic_prop.hw_queues_props);\n\treturn rc;\n}\n\nstatic int gaudi_early_fini(struct hl_device *hdev)\n{\n\tkfree(hdev->asic_prop.hw_queues_props);\n\thl_pci_fini(hdev);\n\n\treturn 0;\n}\n\n \nstatic int gaudi_fetch_psoc_frequency(struct hl_device *hdev)\n{\n\tu32 nr = 0, nf = 0, od = 0, div_fctr = 0, pll_clk, div_sel;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu16 pll_freq_arr[HL_PLL_NUM_OUTPUTS], freq;\n\tint rc;\n\n\tif ((hdev->fw_components & FW_TYPE_LINUX) &&\n\t\t\t(prop->fw_app_cpu_boot_dev_sts0 & CPU_BOOT_DEV_STS0_PLL_INFO_EN)) {\n\t\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\t\treturn 0;\n\n\t\trc = hl_fw_cpucp_pll_info_get(hdev, HL_GAUDI_CPU_PLL, pll_freq_arr);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\tfreq = pll_freq_arr[2];\n\t} else {\n\t\t \n\t\tdiv_fctr = RREG32(mmPSOC_CPU_PLL_DIV_FACTOR_2);\n\t\tdiv_sel = RREG32(mmPSOC_CPU_PLL_DIV_SEL_2);\n\t\tnr = RREG32(mmPSOC_CPU_PLL_NR);\n\t\tnf = RREG32(mmPSOC_CPU_PLL_NF);\n\t\tod = RREG32(mmPSOC_CPU_PLL_OD);\n\n\t\tif (div_sel == DIV_SEL_REF_CLK ||\n\t\t\t\tdiv_sel == DIV_SEL_DIVIDED_REF) {\n\t\t\tif (div_sel == DIV_SEL_REF_CLK)\n\t\t\t\tfreq = PLL_REF_CLK;\n\t\t\telse\n\t\t\t\tfreq = PLL_REF_CLK / (div_fctr + 1);\n\t\t} else if (div_sel == DIV_SEL_PLL_CLK ||\n\t\t\tdiv_sel == DIV_SEL_DIVIDED_PLL) {\n\t\t\tpll_clk = PLL_REF_CLK * (nf + 1) /\n\t\t\t\t\t((nr + 1) * (od + 1));\n\t\t\tif (div_sel == DIV_SEL_PLL_CLK)\n\t\t\t\tfreq = pll_clk;\n\t\t\telse\n\t\t\t\tfreq = pll_clk / (div_fctr + 1);\n\t\t} else {\n\t\t\tdev_warn(hdev->dev, \"Received invalid div select value: %#x\", div_sel);\n\t\t\tfreq = 0;\n\t\t}\n\t}\n\n\tprop->psoc_timestamp_frequency = freq;\n\tprop->psoc_pci_pll_nr = nr;\n\tprop->psoc_pci_pll_nf = nf;\n\tprop->psoc_pci_pll_od = od;\n\tprop->psoc_pci_pll_div_factor = div_fctr;\n\n\treturn 0;\n}\n\nstatic int _gaudi_init_tpc_mem(struct hl_device *hdev,\n\t\tdma_addr_t tpc_kernel_src_addr, u32 tpc_kernel_size)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct packet_lin_dma *init_tpc_mem_pkt;\n\tstruct hl_cs_job *job;\n\tstruct hl_cb *cb;\n\tu64 dst_addr;\n\tu32 cb_size, ctl;\n\tu8 tpc_id;\n\tint rc;\n\n\tcb = hl_cb_kernel_create(hdev, PAGE_SIZE, false);\n\tif (!cb)\n\t\treturn -EFAULT;\n\n\tinit_tpc_mem_pkt = cb->kernel_address;\n\tcb_size = sizeof(*init_tpc_mem_pkt);\n\tmemset(init_tpc_mem_pkt, 0, cb_size);\n\n\tinit_tpc_mem_pkt->tsize = cpu_to_le32(tpc_kernel_size);\n\n\tctl = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_LIN_DMA);\n\tctl |= FIELD_PREP(GAUDI_PKT_LIN_DMA_CTL_LIN_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tinit_tpc_mem_pkt->ctl = cpu_to_le32(ctl);\n\n\tinit_tpc_mem_pkt->src_addr = cpu_to_le64(tpc_kernel_src_addr);\n\n\t \n\tdst_addr = FIELD_PREP(GAUDI_PKT_LIN_DMA_DST_ADDR_MASK,\n\t\t\t\tround_up(prop->sram_user_base_address, SZ_8K));\n\tinit_tpc_mem_pkt->dst_addr |= cpu_to_le64(dst_addr);\n\n\tjob = hl_cs_allocate_job(hdev, QUEUE_TYPE_EXT, true);\n\tif (!job) {\n\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto release_cb;\n\t}\n\n\tjob->id = 0;\n\tjob->user_cb = cb;\n\tatomic_inc(&job->user_cb->cs_cnt);\n\tjob->user_cb_size = cb_size;\n\tjob->hw_queue_id = GAUDI_QUEUE_ID_DMA_0_0;\n\tjob->patched_cb = job->user_cb;\n\tjob->job_cb_size = job->user_cb_size + sizeof(struct packet_msg_prot);\n\n\thl_debugfs_add_job(hdev, job);\n\n\trc = gaudi_send_job_on_qman0(hdev, job);\n\n\tif (rc)\n\t\tgoto free_job;\n\n\tfor (tpc_id = 0 ; tpc_id < TPC_NUMBER_OF_ENGINES ; tpc_id++) {\n\t\trc = gaudi_run_tpc_kernel(hdev, dst_addr, tpc_id);\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\nfree_job:\n\thl_userptr_delete_list(hdev, &job->userptr_list);\n\thl_debugfs_remove_job(hdev, job);\n\tkfree(job);\n\tatomic_dec(&cb->cs_cnt);\n\nrelease_cb:\n\thl_cb_put(cb);\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb->buf->handle);\n\n\treturn rc;\n}\n\n \nstatic int gaudi_init_tpc_mem(struct hl_device *hdev)\n{\n\tconst struct firmware *fw;\n\tsize_t fw_size;\n\tvoid *cpu_addr;\n\tdma_addr_t dma_handle;\n\tint rc, count = 5;\n\nagain:\n\trc = request_firmware(&fw, GAUDI_TPC_FW_FILE, hdev->dev);\n\tif (rc == -EINTR && count-- > 0) {\n\t\tmsleep(50);\n\t\tgoto again;\n\t}\n\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to load firmware file %s\\n\",\n\t\t\t\tGAUDI_TPC_FW_FILE);\n\t\tgoto out;\n\t}\n\n\tfw_size = fw->size;\n\tcpu_addr = hl_asic_dma_alloc_coherent(hdev, fw_size, &dma_handle, GFP_KERNEL | __GFP_ZERO);\n\tif (!cpu_addr) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate %zu of dma memory for TPC kernel\\n\",\n\t\t\tfw_size);\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmemcpy(cpu_addr, fw->data, fw_size);\n\n\trc = _gaudi_init_tpc_mem(hdev, dma_handle, fw_size);\n\n\thl_asic_dma_free_coherent(hdev, fw->size, cpu_addr, dma_handle);\n\nout:\n\trelease_firmware(fw);\n\treturn rc;\n}\n\nstatic void gaudi_collective_map_sobs(struct hl_device *hdev, u32 stream)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_collective_properties *prop = &gaudi->collective_props;\n\tstruct hl_hw_queue *q;\n\tu32 i, sob_id, sob_group_id, queue_id;\n\n\t \n\tsob_group_id =\n\t\tstream * HL_RSVD_SOBS + prop->curr_sob_group_idx[stream];\n\tsob_id = prop->hw_sob_group[sob_group_id].base_sob_id;\n\n\tqueue_id = GAUDI_QUEUE_ID_NIC_0_0 + stream;\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++) {\n\t\tq = &hdev->kernel_queues[queue_id + (4 * i)];\n\t\tq->sync_stream_prop.collective_sob_id = sob_id + i;\n\t}\n\n\t \n\tqueue_id = GAUDI_QUEUE_ID_DMA_5_0 + stream;\n\tq = &hdev->kernel_queues[queue_id];\n\tq->sync_stream_prop.collective_sob_id =\n\t\t\tsob_id + NIC_NUMBER_OF_ENGINES;\n\n\tqueue_id = GAUDI_QUEUE_ID_TPC_7_0 + stream;\n\tq = &hdev->kernel_queues[queue_id];\n\tq->sync_stream_prop.collective_sob_id =\n\t\t\tsob_id + NIC_NUMBER_OF_ENGINES;\n}\n\nstatic void gaudi_sob_group_hw_reset(struct kref *ref)\n{\n\tstruct gaudi_hw_sob_group *hw_sob_group =\n\t\tcontainer_of(ref, struct gaudi_hw_sob_group, kref);\n\tstruct hl_device *hdev = hw_sob_group->hdev;\n\tint i;\n\n\tfor (i = 0 ; i < NUMBER_OF_SOBS_IN_GRP ; i++)\n\t\tWREG32((mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0 +\n\t\t\t(hw_sob_group->base_sob_id * 4) + (i * 4)), 0);\n\n\tkref_init(&hw_sob_group->kref);\n}\n\nstatic void gaudi_sob_group_reset_error(struct kref *ref)\n{\n\tstruct gaudi_hw_sob_group *hw_sob_group =\n\t\tcontainer_of(ref, struct gaudi_hw_sob_group, kref);\n\tstruct hl_device *hdev = hw_sob_group->hdev;\n\n\tdev_crit(hdev->dev,\n\t\t\"SOB release shouldn't be called here, base_sob_id: %d\\n\",\n\t\thw_sob_group->base_sob_id);\n}\n\nstatic void gaudi_collective_mstr_sob_mask_set(struct gaudi_device *gaudi)\n{\n\tstruct gaudi_collective_properties *prop;\n\tint i;\n\n\tprop = &gaudi->collective_props;\n\n\tmemset(prop->mstr_sob_mask, 0, sizeof(prop->mstr_sob_mask));\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++)\n\t\tif (gaudi->hw_cap_initialized & BIT(HW_CAP_NIC_SHIFT + i))\n\t\t\tprop->mstr_sob_mask[i / HL_MAX_SOBS_PER_MONITOR] |=\n\t\t\t\t\tBIT(i % HL_MAX_SOBS_PER_MONITOR);\n\t \n\tprop->mstr_sob_mask[i / HL_MAX_SOBS_PER_MONITOR] |=\n\t\t\t\tBIT(i % HL_MAX_SOBS_PER_MONITOR);\n}\n\nstatic int gaudi_collective_init(struct hl_device *hdev)\n{\n\tu32 i, sob_id, reserved_sobs_per_group;\n\tstruct gaudi_collective_properties *prop;\n\tstruct gaudi_device *gaudi;\n\n\tgaudi = hdev->asic_specific;\n\tprop = &gaudi->collective_props;\n\tsob_id = hdev->asic_prop.collective_first_sob;\n\n\t \n\treserved_sobs_per_group =\n\t\tALIGN(NUMBER_OF_SOBS_IN_GRP, HL_MAX_SOBS_PER_MONITOR);\n\n\t \n\tfor (i = 0 ; i < NUM_SOB_GROUPS; i++) {\n\t\tprop->hw_sob_group[i].hdev = hdev;\n\t\tprop->hw_sob_group[i].base_sob_id = sob_id;\n\t\tsob_id += reserved_sobs_per_group;\n\t\tgaudi_sob_group_hw_reset(&prop->hw_sob_group[i].kref);\n\t}\n\n\tfor (i = 0 ; i < QMAN_STREAMS; i++) {\n\t\tprop->next_sob_group_val[i] = 1;\n\t\tprop->curr_sob_group_idx[i] = 0;\n\t\tgaudi_collective_map_sobs(hdev, i);\n\t}\n\n\tgaudi_collective_mstr_sob_mask_set(gaudi);\n\n\treturn 0;\n}\n\nstatic void gaudi_reset_sob_group(struct hl_device *hdev, u16 sob_group)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_collective_properties *cprop = &gaudi->collective_props;\n\n\tkref_put(&cprop->hw_sob_group[sob_group].kref,\n\t\t\t\t\tgaudi_sob_group_hw_reset);\n}\n\nstatic void gaudi_collective_master_init_job(struct hl_device *hdev,\n\t\tstruct hl_cs_job *job, u32 stream, u32 sob_group_offset)\n{\n\tu32 master_sob_base, master_monitor, queue_id, cb_size = 0;\n\tstruct gaudi_collective_properties *cprop;\n\tstruct hl_gen_wait_properties wait_prop;\n\tstruct hl_sync_stream_properties *prop;\n\tstruct gaudi_device *gaudi;\n\n\tgaudi = hdev->asic_specific;\n\tcprop = &gaudi->collective_props;\n\tqueue_id = job->hw_queue_id;\n\tprop = &hdev->kernel_queues[queue_id].sync_stream_prop;\n\n\tmaster_sob_base =\n\t\tcprop->hw_sob_group[sob_group_offset].base_sob_id;\n\tmaster_monitor = prop->collective_mstr_mon_id[0];\n\n\tcprop->hw_sob_group[sob_group_offset].queue_id = queue_id;\n\n\tdev_dbg(hdev->dev,\n\t\t\"Generate master wait CBs, sob %d (mask %#x), val:0x%x, mon %u, q %d\\n\",\n\t\tmaster_sob_base, cprop->mstr_sob_mask[0],\n\t\tcprop->next_sob_group_val[stream],\n\t\tmaster_monitor, queue_id);\n\n\twait_prop.data = (void *) job->patched_cb;\n\twait_prop.sob_base = master_sob_base;\n\twait_prop.sob_mask = cprop->mstr_sob_mask[0];\n\twait_prop.sob_val = cprop->next_sob_group_val[stream];\n\twait_prop.mon_id = master_monitor;\n\twait_prop.q_idx = queue_id;\n\twait_prop.size = cb_size;\n\tcb_size += gaudi_gen_wait_cb(hdev, &wait_prop);\n\n\tmaster_sob_base += HL_MAX_SOBS_PER_MONITOR;\n\tmaster_monitor = prop->collective_mstr_mon_id[1];\n\n\tdev_dbg(hdev->dev,\n\t\t\"Generate master wait CBs, sob %d (mask %#x), val:0x%x, mon %u, q %d\\n\",\n\t\tmaster_sob_base, cprop->mstr_sob_mask[1],\n\t\tcprop->next_sob_group_val[stream],\n\t\tmaster_monitor, queue_id);\n\n\twait_prop.sob_base = master_sob_base;\n\twait_prop.sob_mask = cprop->mstr_sob_mask[1];\n\twait_prop.mon_id = master_monitor;\n\twait_prop.size = cb_size;\n\tcb_size += gaudi_gen_wait_cb(hdev, &wait_prop);\n}\n\nstatic void gaudi_collective_slave_init_job(struct hl_device *hdev,\n\t\tstruct hl_cs_job *job, struct hl_cs_compl *cs_cmpl)\n{\n\tstruct hl_gen_wait_properties wait_prop;\n\tstruct hl_sync_stream_properties *prop;\n\tu32 queue_id, cb_size = 0;\n\n\tqueue_id = job->hw_queue_id;\n\tprop = &hdev->kernel_queues[queue_id].sync_stream_prop;\n\n\tif (job->cs->encaps_signals) {\n\t\t \n\t\thl_hw_queue_encaps_sig_set_sob_info(hdev, job->cs, job,\n\t\t\t\t\t\tcs_cmpl);\n\n\t\tdev_dbg(hdev->dev, \"collective wait: Sequence %llu found, sob_id: %u,  wait for sob_val: %u\\n\",\n\t\t\t\tjob->cs->sequence,\n\t\t\t\tcs_cmpl->hw_sob->sob_id,\n\t\t\t\tcs_cmpl->sob_val);\n\t}\n\n\t \n\twait_prop.data = (void *) job->user_cb;\n\twait_prop.sob_base = cs_cmpl->hw_sob->sob_id;\n\twait_prop.sob_mask = 0x1;\n\twait_prop.sob_val = cs_cmpl->sob_val;\n\twait_prop.mon_id = prop->collective_slave_mon_id;\n\twait_prop.q_idx = queue_id;\n\twait_prop.size = cb_size;\n\n\tdev_dbg(hdev->dev,\n\t\t\"Generate slave wait CB, sob %d, val:%x, mon %d, q %d\\n\",\n\t\tcs_cmpl->hw_sob->sob_id, cs_cmpl->sob_val,\n\t\tprop->collective_slave_mon_id, queue_id);\n\n\tcb_size += gaudi_gen_wait_cb(hdev, &wait_prop);\n\n\tdev_dbg(hdev->dev,\n\t\t\"generate signal CB, sob_id: %d, sob val: 1, q_idx: %d\\n\",\n\t\tprop->collective_sob_id, queue_id);\n\n\tcb_size += gaudi_gen_signal_cb(hdev, job->user_cb,\n\t\t\tprop->collective_sob_id, cb_size, false);\n}\n\nstatic int gaudi_collective_wait_init_cs(struct hl_cs *cs)\n{\n\tstruct hl_cs_compl *signal_cs_cmpl =\n\t\tcontainer_of(cs->signal_fence, struct hl_cs_compl, base_fence);\n\tstruct hl_cs_compl *cs_cmpl =\n\t\tcontainer_of(cs->fence, struct hl_cs_compl, base_fence);\n\tstruct hl_cs_encaps_sig_handle *handle = cs->encaps_sig_hdl;\n\tstruct gaudi_collective_properties *cprop;\n\tu32 stream, queue_id, sob_group_offset;\n\tstruct gaudi_device *gaudi;\n\tstruct hl_device *hdev;\n\tstruct hl_cs_job *job;\n\tstruct hl_ctx *ctx;\n\n\tctx = cs->ctx;\n\thdev = ctx->hdev;\n\tgaudi = hdev->asic_specific;\n\tcprop = &gaudi->collective_props;\n\n\tif (cs->encaps_signals) {\n\t\tcs_cmpl->hw_sob = handle->hw_sob;\n\t\t \n\t\tcs_cmpl->sob_val = 0;\n\t} else {\n\t\t \n\t\tcs_cmpl->hw_sob = signal_cs_cmpl->hw_sob;\n\t\tcs_cmpl->sob_val = signal_cs_cmpl->sob_val;\n\t}\n\n\t \n\tspin_lock(&signal_cs_cmpl->lock);\n\n\tif (completion_done(&cs->signal_fence->completion)) {\n\t\tspin_unlock(&signal_cs_cmpl->lock);\n\t\treturn -EINVAL;\n\t}\n\t \n\tkref_get(&cs_cmpl->hw_sob->kref);\n\n\tspin_unlock(&signal_cs_cmpl->lock);\n\n\t \n\tjob = list_first_entry(&cs->job_list, struct hl_cs_job, cs_node);\n\tstream = job->hw_queue_id % 4;\n\tsob_group_offset =\n\t\tstream * HL_RSVD_SOBS + cprop->curr_sob_group_idx[stream];\n\n\tlist_for_each_entry(job, &cs->job_list, cs_node) {\n\t\tqueue_id = job->hw_queue_id;\n\n\t\tif (hdev->kernel_queues[queue_id].collective_mode ==\n\t\t\t\tHL_COLLECTIVE_MASTER)\n\t\t\tgaudi_collective_master_init_job(hdev, job, stream,\n\t\t\t\t\t\tsob_group_offset);\n\t\telse\n\t\t\tgaudi_collective_slave_init_job(hdev, job, cs_cmpl);\n\t}\n\n\tcs_cmpl->sob_group = sob_group_offset;\n\n\t \n\tkref_get(&cprop->hw_sob_group[sob_group_offset].kref);\n\tcprop->next_sob_group_val[stream]++;\n\n\tif (cprop->next_sob_group_val[stream] == HL_MAX_SOB_VAL) {\n\t\t \n\t\tkref_put(&cprop->hw_sob_group[sob_group_offset].kref,\n\t\t\t\tgaudi_sob_group_reset_error);\n\t\tcprop->next_sob_group_val[stream] = 1;\n\t\t \n\t\tcprop->curr_sob_group_idx[stream] =\n\t\t\t(cprop->curr_sob_group_idx[stream] + 1) &\n\t\t\t\t\t\t\t(HL_RSVD_SOBS - 1);\n\n\t\tgaudi_collective_map_sobs(hdev, stream);\n\n\t\tdev_dbg(hdev->dev, \"switched to SOB group %d, stream: %d\\n\",\n\t\t\t\tcprop->curr_sob_group_idx[stream], stream);\n\t}\n\n\tmb();\n\thl_fence_put(cs->signal_fence);\n\tcs->signal_fence = NULL;\n\n\treturn 0;\n}\n\nstatic u32 gaudi_get_patched_cb_extra_size(u32 user_cb_size)\n{\n\tu32 cacheline_end, additional_commands;\n\n\tcacheline_end = round_up(user_cb_size, DEVICE_CACHE_LINE_SIZE);\n\tadditional_commands = sizeof(struct packet_msg_prot) * 2;\n\n\tif (user_cb_size + additional_commands > cacheline_end)\n\t\treturn cacheline_end - user_cb_size + additional_commands;\n\telse\n\t\treturn additional_commands;\n}\n\nstatic int gaudi_collective_wait_create_job(struct hl_device *hdev,\n\t\tstruct hl_ctx *ctx, struct hl_cs *cs,\n\t\tenum hl_collective_mode mode, u32 queue_id, u32 wait_queue_id,\n\t\tu32 encaps_signal_offset)\n{\n\tstruct hw_queue_properties *hw_queue_prop;\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_cs_job *job;\n\tstruct hl_cb *cb;\n\tu32 cb_size;\n\tbool patched_cb;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\n\tif (mode == HL_COLLECTIVE_MASTER) {\n\t\t \n\t\tcb_size = sizeof(struct packet_msg_short) * 8 +\n\t\t\t\tsizeof(struct packet_fence) * 2 +\n\t\t\t\tsizeof(struct packet_msg_prot) * 2;\n\t\tpatched_cb = true;\n\t} else {\n\t\t \n\t\tcb_size = sizeof(struct packet_msg_short) * 5 +\n\t\t\t\tsizeof(struct packet_fence);\n\t\tpatched_cb = false;\n\t}\n\n\thw_queue_prop = &hdev->asic_prop.hw_queues_props[queue_id];\n\tjob = hl_cs_allocate_job(hdev, hw_queue_prop->type, true);\n\tif (!job) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tcb = hl_cb_kernel_create(hdev, cb_size, !patched_cb);\n\tif (!cb) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\tkfree(job);\n\t\treturn -EFAULT;\n\t}\n\n\tjob->id = 0;\n\tjob->cs = cs;\n\tjob->user_cb = cb;\n\tatomic_inc(&job->user_cb->cs_cnt);\n\tjob->user_cb_size = cb_size;\n\tjob->hw_queue_id = queue_id;\n\n\t \n\tif (cs->encaps_signals)\n\t\tjob->encaps_sig_wait_offset = encaps_signal_offset;\n\n\t \n\tif (patched_cb)\n\t\tjob->patched_cb = job->user_cb;\n\telse\n\t\tjob->patched_cb = NULL;\n\n\tjob->job_cb_size = job->user_cb_size;\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb->buf->handle);\n\n\t \n\tif (hw_queue_prop->type == QUEUE_TYPE_EXT)\n\t\tcs_get(cs);\n\n\tcs->jobs_in_queue_cnt[job->hw_queue_id]++;\n\n\tlist_add_tail(&job->cs_node, &cs->job_list);\n\n\thl_debugfs_add_job(hdev, job);\n\n\treturn 0;\n}\n\nstatic int gaudi_collective_wait_create_jobs(struct hl_device *hdev,\n\t\tstruct hl_ctx *ctx, struct hl_cs *cs,\n\t\tu32 wait_queue_id, u32 collective_engine_id,\n\t\tu32 encaps_signal_offset)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct hw_queue_properties *hw_queue_prop;\n\tu32 queue_id, collective_queue, num_jobs;\n\tu32 stream, nic_queue, nic_idx = 0;\n\tbool skip;\n\tint i, rc = 0;\n\n\t \n\thw_queue_prop = &hdev->asic_prop.hw_queues_props[wait_queue_id];\n\tif (!(hw_queue_prop->collective_mode == HL_COLLECTIVE_MASTER)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Queue %d is not configured as collective master\\n\",\n\t\t\twait_queue_id);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (collective_engine_id != GAUDI_ENGINE_ID_DMA_5 &&\n\t\t\tcollective_engine_id != GAUDI_ENGINE_ID_TPC_7) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Collective wait does not support engine %u\\n\",\n\t\t\tcollective_engine_id);\n\t\treturn -EINVAL;\n\t}\n\n\tstream = wait_queue_id % 4;\n\n\tif (collective_engine_id == GAUDI_ENGINE_ID_DMA_5)\n\t\tcollective_queue = GAUDI_QUEUE_ID_DMA_5_0 + stream;\n\telse\n\t\tcollective_queue = GAUDI_QUEUE_ID_TPC_7_0 + stream;\n\n\tnum_jobs = NUMBER_OF_SOBS_IN_GRP + 1;\n\tnic_queue = GAUDI_QUEUE_ID_NIC_0_0 + stream;\n\n\t \n\tfor (i = 0 ; i < num_jobs ; i++) {\n\t\tif (i == 0) {\n\t\t\tqueue_id = wait_queue_id;\n\t\t\trc = gaudi_collective_wait_create_job(hdev, ctx, cs,\n\t\t\t\tHL_COLLECTIVE_MASTER, queue_id,\n\t\t\t\twait_queue_id, encaps_signal_offset);\n\t\t} else {\n\t\t\tif (nic_idx < NIC_NUMBER_OF_ENGINES) {\n\t\t\t\tif (gaudi->hw_cap_initialized &\n\t\t\t\t\tBIT(HW_CAP_NIC_SHIFT + nic_idx))\n\t\t\t\t\tskip = false;\n\t\t\t\telse\n\t\t\t\t\tskip = true;\n\n\t\t\t\tqueue_id = nic_queue;\n\t\t\t\tnic_queue += 4;\n\t\t\t\tnic_idx++;\n\n\t\t\t\tif (skip)\n\t\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tqueue_id = collective_queue;\n\t\t\t}\n\n\t\t\trc = gaudi_collective_wait_create_job(hdev, ctx, cs,\n\t\t\t\tHL_COLLECTIVE_SLAVE, queue_id,\n\t\t\t\twait_queue_id, encaps_signal_offset);\n\t\t}\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi_late_init(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tint rc;\n\n\trc = gaudi->cpucp_info_get(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to get cpucp info\\n\");\n\t\treturn rc;\n\t}\n\n\tif ((hdev->card_type == cpucp_card_type_pci) &&\n\t\t\t(hdev->nic_ports_mask & 0x3)) {\n\t\tdev_info(hdev->dev,\n\t\t\t\"PCI card detected, only 8 ports are enabled\\n\");\n\t\thdev->nic_ports_mask &= ~0x3;\n\n\t\t \n\t\tWREG32(mmNIC0_QM0_GLBL_CFG1, NIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\t\tWREG32(mmNIC0_QM1_GLBL_CFG1, NIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\t\tWREG32(mmNIC0_QM0_GLBL_CFG0, 0);\n\t\tWREG32(mmNIC0_QM1_GLBL_CFG0, 0);\n\n\t\tgaudi->hw_cap_initialized &= ~(HW_CAP_NIC0 | HW_CAP_NIC1);\n\t}\n\n\trc = hl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_ENABLE_PCI_ACCESS, 0x0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to enable PCI access from CPU\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\trc = hdev->asic_funcs->scrub_device_mem(hdev);\n\tif (rc)\n\t\tgoto disable_pci_access;\n\n\trc = gaudi_fetch_psoc_frequency(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to fetch psoc frequency\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\trc = gaudi_mmu_clear_pgt_range(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to clear MMU page tables range\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\trc = gaudi_init_tpc_mem(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to initialize TPC memories\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\trc = gaudi_collective_init(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to init collective\\n\");\n\t\tgoto disable_pci_access;\n\t}\n\n\t \n\tgaudi_mmu_prepare(hdev, 1);\n\n\thl_fw_set_pll_profile(hdev);\n\n\treturn 0;\n\ndisable_pci_access:\n\thl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_DISABLE_PCI_ACCESS, 0x0);\n\n\treturn rc;\n}\n\nstatic void gaudi_late_fini(struct hl_device *hdev)\n{\n\thl_hwmon_release_resources(hdev);\n}\n\nstatic int gaudi_alloc_cpu_accessible_dma_mem(struct hl_device *hdev)\n{\n\tdma_addr_t dma_addr_arr[GAUDI_ALLOC_CPU_MEM_RETRY_CNT] = {}, end_addr;\n\tvoid *virt_addr_arr[GAUDI_ALLOC_CPU_MEM_RETRY_CNT] = {};\n\tint i, j, rc = 0;\n\n\t \n\n\tfor (i = 0 ; i < GAUDI_ALLOC_CPU_MEM_RETRY_CNT ; i++) {\n\t\tvirt_addr_arr[i] = hl_asic_dma_alloc_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE,\n\t\t\t\t\t\t\t\t&dma_addr_arr[i],\n\t\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\t\tif (!virt_addr_arr[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_dma_mem_arr;\n\t\t}\n\n\t\tend_addr = dma_addr_arr[i] + HL_CPU_ACCESSIBLE_MEM_SIZE - 1;\n\t\tif (GAUDI_CPU_PCI_MSB_ADDR(dma_addr_arr[i]) ==\n\t\t\t\tGAUDI_CPU_PCI_MSB_ADDR(end_addr))\n\t\t\tbreak;\n\t}\n\n\tif (i == GAUDI_ALLOC_CPU_MEM_RETRY_CNT) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"MSB of CPU accessible DMA memory are not identical in all range\\n\");\n\t\trc = -EFAULT;\n\t\tgoto free_dma_mem_arr;\n\t}\n\n\thdev->cpu_accessible_dma_mem = virt_addr_arr[i];\n\thdev->cpu_accessible_dma_address = dma_addr_arr[i];\n\thdev->cpu_pci_msb_addr =\n\t\tGAUDI_CPU_PCI_MSB_ADDR(hdev->cpu_accessible_dma_address);\n\n\tif (!hdev->asic_prop.fw_security_enabled)\n\t\tGAUDI_PCI_TO_CPU_ADDR(hdev->cpu_accessible_dma_address);\n\nfree_dma_mem_arr:\n\tfor (j = 0 ; j < i ; j++)\n\t\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, virt_addr_arr[j],\n\t\t\t\t\t\tdma_addr_arr[j]);\n\n\treturn rc;\n}\n\nstatic void gaudi_free_internal_qmans_pq_mem(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tu32 i;\n\n\tfor (i = 0 ; i < GAUDI_QUEUE_ID_SIZE ; i++) {\n\t\tq = &gaudi->internal_qmans[i];\n\t\tif (!q->pq_kernel_addr)\n\t\t\tcontinue;\n\t\thl_asic_dma_free_coherent(hdev, q->pq_size, q->pq_kernel_addr, q->pq_dma_addr);\n\t}\n}\n\nstatic int gaudi_alloc_internal_qmans_pq_mem(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tint rc, i;\n\n\tfor (i = 0 ; i < GAUDI_QUEUE_ID_SIZE ; i++) {\n\t\tif (gaudi_queue_type[i] != QUEUE_TYPE_INT)\n\t\t\tcontinue;\n\n\t\tq = &gaudi->internal_qmans[i];\n\n\t\tswitch (i) {\n\t\tcase GAUDI_QUEUE_ID_DMA_2_0 ... GAUDI_QUEUE_ID_DMA_7_3:\n\t\t\tq->pq_size = HBM_DMA_QMAN_SIZE_IN_BYTES;\n\t\t\tbreak;\n\t\tcase GAUDI_QUEUE_ID_MME_0_0 ... GAUDI_QUEUE_ID_MME_1_3:\n\t\t\tq->pq_size = MME_QMAN_SIZE_IN_BYTES;\n\t\t\tbreak;\n\t\tcase GAUDI_QUEUE_ID_TPC_0_0 ... GAUDI_QUEUE_ID_TPC_7_3:\n\t\t\tq->pq_size = TPC_QMAN_SIZE_IN_BYTES;\n\t\t\tbreak;\n\t\tcase GAUDI_QUEUE_ID_NIC_0_0 ... GAUDI_QUEUE_ID_NIC_9_3:\n\t\t\tq->pq_size = NIC_QMAN_SIZE_IN_BYTES;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hdev->dev, \"Bad internal queue index %d\", i);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_internal_qmans_pq_mem;\n\t\t}\n\n\t\tq->pq_kernel_addr = hl_asic_dma_alloc_coherent(hdev, q->pq_size, &q->pq_dma_addr,\n\t\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\t\tif (!q->pq_kernel_addr) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_internal_qmans_pq_mem;\n\t\t}\n\t}\n\n\treturn 0;\n\nfree_internal_qmans_pq_mem:\n\tgaudi_free_internal_qmans_pq_mem(hdev);\n\treturn rc;\n}\n\nstatic void gaudi_set_pci_memory_regions(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct pci_mem_region *region;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_CFG];\n\tregion->region_base = CFG_BASE;\n\tregion->region_size = CFG_SIZE;\n\tregion->offset_in_bar = CFG_BASE - SPI_FLASH_BASE_ADDR;\n\tregion->bar_size = CFG_BAR_SIZE;\n\tregion->bar_id = CFG_BAR_ID;\n\tregion->used = 1;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_SRAM];\n\tregion->region_base = SRAM_BASE_ADDR;\n\tregion->region_size = SRAM_SIZE;\n\tregion->offset_in_bar = 0;\n\tregion->bar_size = SRAM_BAR_SIZE;\n\tregion->bar_id = SRAM_BAR_ID;\n\tregion->used = 1;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_DRAM];\n\tregion->region_base = DRAM_PHYS_BASE;\n\tregion->region_size = hdev->asic_prop.dram_size;\n\tregion->offset_in_bar = 0;\n\tregion->bar_size = prop->dram_pci_bar_size;\n\tregion->bar_id = HBM_BAR_ID;\n\tregion->used = 1;\n\n\t \n\tregion = &hdev->pci_mem_region[PCI_REGION_SP_SRAM];\n\tregion->region_base = PSOC_SCRATCHPAD_ADDR;\n\tregion->region_size = PSOC_SCRATCHPAD_SIZE;\n\tregion->offset_in_bar = PSOC_SCRATCHPAD_ADDR - SPI_FLASH_BASE_ADDR;\n\tregion->bar_size = CFG_BAR_SIZE;\n\tregion->bar_id = CFG_BAR_ID;\n\tregion->used = 1;\n}\n\nstatic int gaudi_sw_init(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi;\n\tu32 i, event_id = 0;\n\tint rc;\n\n\t \n\tgaudi = kzalloc(sizeof(*gaudi), GFP_KERNEL);\n\tif (!gaudi)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0 ; i < ARRAY_SIZE(gaudi_irq_map_table) ; i++) {\n\t\tif (gaudi_irq_map_table[i].valid) {\n\t\t\tif (event_id == GAUDI_EVENT_SIZE) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"Event array exceeds the limit of %u events\\n\",\n\t\t\t\t\tGAUDI_EVENT_SIZE);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto free_gaudi_device;\n\t\t\t}\n\n\t\t\tgaudi->events[event_id++] =\n\t\t\t\t\tgaudi_irq_map_table[i].fc_id;\n\t\t}\n\t}\n\n\tgaudi->cpucp_info_get = gaudi_cpucp_info_get;\n\n\thdev->asic_specific = gaudi;\n\n\t \n\thdev->dma_pool = dma_pool_create(dev_name(hdev->dev),\n\t\t\t&hdev->pdev->dev, GAUDI_DMA_POOL_BLK_SIZE, 8, 0);\n\tif (!hdev->dma_pool) {\n\t\tdev_err(hdev->dev, \"failed to create DMA pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_gaudi_device;\n\t}\n\n\trc = gaudi_alloc_cpu_accessible_dma_mem(hdev);\n\tif (rc)\n\t\tgoto free_dma_pool;\n\n\thdev->cpu_accessible_dma_pool = gen_pool_create(ilog2(32), -1);\n\tif (!hdev->cpu_accessible_dma_pool) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to create CPU accessible DMA pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_cpu_dma_mem;\n\t}\n\n\trc = gen_pool_add(hdev->cpu_accessible_dma_pool,\n\t\t\t\t(uintptr_t) hdev->cpu_accessible_dma_mem,\n\t\t\t\tHL_CPU_ACCESSIBLE_MEM_SIZE, -1);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to add memory to CPU accessible DMA pool\\n\");\n\t\trc = -EFAULT;\n\t\tgoto free_cpu_accessible_dma_pool;\n\t}\n\n\trc = gaudi_alloc_internal_qmans_pq_mem(hdev);\n\tif (rc)\n\t\tgoto free_cpu_accessible_dma_pool;\n\n\tspin_lock_init(&gaudi->hw_queues_lock);\n\n\thdev->supports_sync_stream = true;\n\thdev->supports_coresight = true;\n\thdev->supports_staged_submission = true;\n\thdev->supports_wait_for_multi_cs = true;\n\n\thdev->asic_funcs->set_pci_memory_regions(hdev);\n\thdev->stream_master_qid_arr =\n\t\t\t\thdev->asic_funcs->get_stream_master_qid_arr();\n\thdev->stream_master_qid_arr_size = GAUDI_STREAM_MASTER_ARR_SIZE;\n\n\treturn 0;\n\nfree_cpu_accessible_dma_pool:\n\tgen_pool_destroy(hdev->cpu_accessible_dma_pool);\nfree_cpu_dma_mem:\n\tif (!hdev->asic_prop.fw_security_enabled)\n\t\tGAUDI_CPU_TO_PCI_ADDR(hdev->cpu_accessible_dma_address,\n\t\t\t\t\thdev->cpu_pci_msb_addr);\n\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, hdev->cpu_accessible_dma_mem,\n\t\t\t\t\thdev->cpu_accessible_dma_address);\nfree_dma_pool:\n\tdma_pool_destroy(hdev->dma_pool);\nfree_gaudi_device:\n\tkfree(gaudi);\n\treturn rc;\n}\n\nstatic int gaudi_sw_fini(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tgaudi_free_internal_qmans_pq_mem(hdev);\n\n\tgen_pool_destroy(hdev->cpu_accessible_dma_pool);\n\n\tif (!hdev->asic_prop.fw_security_enabled)\n\t\tGAUDI_CPU_TO_PCI_ADDR(hdev->cpu_accessible_dma_address,\n\t\t\t\t\thdev->cpu_pci_msb_addr);\n\n\thl_asic_dma_free_coherent(hdev, HL_CPU_ACCESSIBLE_MEM_SIZE, hdev->cpu_accessible_dma_mem,\n\t\t\t\t\thdev->cpu_accessible_dma_address);\n\n\tdma_pool_destroy(hdev->dma_pool);\n\n\tkfree(gaudi);\n\n\treturn 0;\n}\n\nstatic irqreturn_t gaudi_irq_handler_single(int irq, void *arg)\n{\n\tstruct hl_device *hdev = arg;\n\tint i;\n\n\tif (hdev->disabled)\n\t\treturn IRQ_HANDLED;\n\n\tfor (i = 0 ; i < hdev->asic_prop.completion_queues_count ; i++)\n\t\thl_irq_handler_cq(irq, &hdev->completion_queue[i]);\n\n\thl_irq_handler_eq(irq, &hdev->event_queue);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic int gaudi_pci_irq_vector(struct hl_device *hdev, unsigned int nr,\n\t\t\t\tbool cpu_eq)\n{\n\tint msi_vec;\n\n\tif ((nr != GAUDI_EVENT_QUEUE_MSI_IDX) && (cpu_eq))\n\t\tdev_crit(hdev->dev, \"CPU EQ must use IRQ %d\\n\",\n\t\t\t\tGAUDI_EVENT_QUEUE_MSI_IDX);\n\n\tmsi_vec = ((nr < GAUDI_EVENT_QUEUE_MSI_IDX) || (cpu_eq)) ? nr :\n\t\t\t(nr + NIC_NUMBER_OF_ENGINES + 1);\n\n\treturn pci_irq_vector(hdev->pdev, msi_vec);\n}\n\nstatic int gaudi_enable_msi_single(struct hl_device *hdev)\n{\n\tint rc, irq;\n\n\tdev_dbg(hdev->dev, \"Working in single MSI IRQ mode\\n\");\n\n\tirq = gaudi_pci_irq_vector(hdev, 0, false);\n\trc = request_irq(irq, gaudi_irq_handler_single, 0,\n\t\t\t\"gaudi single msi\", hdev);\n\tif (rc)\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to request single MSI IRQ\\n\");\n\n\treturn rc;\n}\n\nstatic int gaudi_enable_msi(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tint rc;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_MSI)\n\t\treturn 0;\n\n\trc = pci_alloc_irq_vectors(hdev->pdev, 1, 1, PCI_IRQ_MSI);\n\tif (rc < 0) {\n\t\tdev_err(hdev->dev, \"MSI: Failed to enable support %d\\n\", rc);\n\t\treturn rc;\n\t}\n\n\trc = gaudi_enable_msi_single(hdev);\n\tif (rc)\n\t\tgoto free_pci_irq_vectors;\n\n\tgaudi->hw_cap_initialized |= HW_CAP_MSI;\n\n\treturn 0;\n\nfree_pci_irq_vectors:\n\tpci_free_irq_vectors(hdev->pdev);\n\treturn rc;\n}\n\nstatic void gaudi_sync_irqs(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MSI))\n\t\treturn;\n\n\t \n\tsynchronize_irq(gaudi_pci_irq_vector(hdev, 0, false));\n}\n\nstatic void gaudi_disable_msi(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MSI))\n\t\treturn;\n\n\tgaudi_sync_irqs(hdev);\n\tfree_irq(gaudi_pci_irq_vector(hdev, 0, false), hdev);\n\tpci_free_irq_vectors(hdev->pdev);\n\n\tgaudi->hw_cap_initialized &= ~HW_CAP_MSI;\n}\n\nstatic void gaudi_init_scrambler_sram(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (hdev->asic_prop.fw_security_enabled)\n\t\treturn;\n\n\tif (hdev->asic_prop.fw_app_cpu_boot_dev_sts0 &\n\t\t\t\t\t\tCPU_BOOT_DEV_STS0_SRAM_SCR_EN)\n\t\treturn;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_SRAM_SCRAMBLER)\n\t\treturn;\n\n\tWREG32(mmNIF_RTR_CTRL_0_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_1_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_2_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_3_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_4_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_5_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_6_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_7_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_0_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_1_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_2_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_3_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_4_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_5_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_6_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_7_SCRAM_SRAM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_SRAM_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_SCRAM_SRAM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_SRAM_EN_VAL_SHIFT);\n\n\tgaudi->hw_cap_initialized |= HW_CAP_SRAM_SCRAMBLER;\n}\n\nstatic void gaudi_init_scrambler_hbm(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (hdev->asic_prop.fw_security_enabled)\n\t\treturn;\n\n\tif (hdev->asic_prop.fw_bootfit_cpu_boot_dev_sts0 &\n\t\t\t\t\tCPU_BOOT_DEV_STS0_DRAM_SCR_EN)\n\t\treturn;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_HBM_SCRAMBLER)\n\t\treturn;\n\n\tWREG32(mmNIF_RTR_CTRL_0_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_1_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_2_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_3_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_4_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_5_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_6_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_7_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_0_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_1_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_2_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_3_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_4_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_5_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_6_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_7_SCRAM_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_SCRAM_HBM_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_SCRAM_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_SCRAM_HBM_EN_VAL_SHIFT);\n\n\tgaudi->hw_cap_initialized |= HW_CAP_HBM_SCRAMBLER;\n}\n\nstatic void gaudi_init_e2e(struct hl_device *hdev)\n{\n\tif (hdev->asic_prop.fw_security_enabled)\n\t\treturn;\n\n\tif (hdev->asic_prop.fw_bootfit_cpu_boot_dev_sts0 &\n\t\t\t\t\tCPU_BOOT_DEV_STS0_E2E_CRED_EN)\n\t\treturn;\n\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_HBM_WR_SIZE, 247 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_HBM_RD_SIZE, 785 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_PCI_WR_SIZE, 49);\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_PCI_RD_SIZE, 101);\n\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_HBM_WR_SIZE, 275 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_HBM_RD_SIZE, 614 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_PCI_RD_SIZE, 39);\n\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_HBM_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_HBM_RD_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_HBM_WR_SIZE, 176 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_HBM_RD_SIZE, 32 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_PCI_WR_SIZE, 19);\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_HBM_WR_SIZE, 176 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_HBM_RD_SIZE, 32 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_PCI_WR_SIZE, 19);\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_HBM_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_HBM_RD_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_HBM_WR_SIZE, 275 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_HBM_RD_SIZE, 614 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_PCI_RD_SIZE, 39);\n\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_HBM_WR_SIZE, 297 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_HBM_RD_SIZE, 908 >> 3);\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_PCI_WR_SIZE, 19);\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_PCI_RD_SIZE, 19);\n\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_HBM_WR_SIZE, 318 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_HBM_RD_SIZE, 956 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_PCI_WR_SIZE, 79);\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_PCI_RD_SIZE, 163);\n\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_HBM_WR_SIZE, 275 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_HBM_RD_SIZE, 614 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_PCI_RD_SIZE, 39);\n\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_HBM_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_HBM_RD_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_HBM_WR_SIZE, 176 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_HBM_RD_SIZE, 32 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_PCI_WR_SIZE, 19);\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_HBM_WR_SIZE, 176 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_HBM_RD_SIZE, 32 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_PCI_WR_SIZE, 19);\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_HBM_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_HBM_RD_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_PCI_RD_SIZE, 32);\n\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_HBM_WR_SIZE, 275 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_HBM_RD_SIZE, 614 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_PCI_WR_SIZE, 1);\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_PCI_RD_SIZE, 39);\n\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_HBM_WR_SIZE, 318 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_HBM_RD_SIZE, 956 >> 3);\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_PCI_WR_SIZE, 79);\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_PCI_RD_SIZE, 79);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_HBM_WR_SIZE, 344 >> 3);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_HBM_RD_SIZE, 1000 >> 3);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_PCI_WR_SIZE, 162);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_PCI_RD_SIZE, 338);\n\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_0_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_1_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_2_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_3_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_4_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_5_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_6_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmSIF_RTR_CTRL_7_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_0_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_1_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_2_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_3_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_4_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_5_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_6_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_HBM_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmNIF_RTR_CTRL_7_E2E_PCI_EN,\n\t\t\t1 << IF_RTR_CTRL_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH0_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_N_DOWN_CH1_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH0_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_E_S_DOWN_CH1_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH0_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_N_DOWN_CH1_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH0_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_HBM_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_HBM_EN_VAL_SHIFT);\n\tWREG32(mmDMA_IF_W_S_DOWN_CH1_E2E_PCI_EN,\n\t\t\t1 << DMA_IF_DOWN_CHX_E2E_PCI_EN_VAL_SHIFT);\n}\n\nstatic void gaudi_init_hbm_cred(struct hl_device *hdev)\n{\n\tu32 hbm0_wr, hbm1_wr, hbm0_rd, hbm1_rd;\n\n\tif (hdev->asic_prop.fw_security_enabled)\n\t\treturn;\n\n\tif (hdev->asic_prop.fw_bootfit_cpu_boot_dev_sts0 &\n\t\t\t\t\t\tCPU_BOOT_DEV_STS0_HBM_CRED_EN)\n\t\treturn;\n\n\thbm0_wr = 0x33333333;\n\thbm0_rd = 0x77777777;\n\thbm1_wr = 0x55555555;\n\thbm1_rd = 0xDDDDDDDD;\n\n\tWREG32(mmDMA_IF_E_N_HBM0_WR_CRED_CNT, hbm0_wr);\n\tWREG32(mmDMA_IF_E_N_HBM1_WR_CRED_CNT, hbm1_wr);\n\tWREG32(mmDMA_IF_E_N_HBM0_RD_CRED_CNT, hbm0_rd);\n\tWREG32(mmDMA_IF_E_N_HBM1_RD_CRED_CNT, hbm1_rd);\n\n\tWREG32(mmDMA_IF_E_S_HBM0_WR_CRED_CNT, hbm0_wr);\n\tWREG32(mmDMA_IF_E_S_HBM1_WR_CRED_CNT, hbm1_wr);\n\tWREG32(mmDMA_IF_E_S_HBM0_RD_CRED_CNT, hbm0_rd);\n\tWREG32(mmDMA_IF_E_S_HBM1_RD_CRED_CNT, hbm1_rd);\n\n\tWREG32(mmDMA_IF_W_N_HBM0_WR_CRED_CNT, hbm0_wr);\n\tWREG32(mmDMA_IF_W_N_HBM1_WR_CRED_CNT, hbm1_wr);\n\tWREG32(mmDMA_IF_W_N_HBM0_RD_CRED_CNT, hbm0_rd);\n\tWREG32(mmDMA_IF_W_N_HBM1_RD_CRED_CNT, hbm1_rd);\n\n\tWREG32(mmDMA_IF_W_S_HBM0_WR_CRED_CNT, hbm0_wr);\n\tWREG32(mmDMA_IF_W_S_HBM1_WR_CRED_CNT, hbm1_wr);\n\tWREG32(mmDMA_IF_W_S_HBM0_RD_CRED_CNT, hbm0_rd);\n\tWREG32(mmDMA_IF_W_S_HBM1_RD_CRED_CNT, hbm1_rd);\n\n\tWREG32(mmDMA_IF_E_N_HBM_CRED_EN_0,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_E_S_HBM_CRED_EN_0,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_W_N_HBM_CRED_EN_0,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_W_S_HBM_CRED_EN_0,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\n\tWREG32(mmDMA_IF_E_N_HBM_CRED_EN_1,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_E_S_HBM_CRED_EN_1,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_W_N_HBM_CRED_EN_1,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n\tWREG32(mmDMA_IF_W_S_HBM_CRED_EN_1,\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_READ_CREDIT_EN_SHIFT) |\n\t\t\t(1 << DMA_IF_HBM_CRED_EN_WRITE_CREDIT_EN_SHIFT));\n}\n\nstatic void gaudi_init_golden_registers(struct hl_device *hdev)\n{\n\tu32 tpc_offset;\n\tint tpc_id, i;\n\n\tgaudi_init_e2e(hdev);\n\tgaudi_init_hbm_cred(hdev);\n\n\tfor (tpc_id = 0, tpc_offset = 0;\n\t\t\t\ttpc_id < TPC_NUMBER_OF_ENGINES;\n\t\t\t\ttpc_id++, tpc_offset += TPC_CFG_OFFSET) {\n\t\t \n\t\tWREG32(mmTPC0_CFG_TPC_INTR_MASK + tpc_offset, 0x8FFE);\n\t\t \n\t\tWREG32_FIELD(TPC0_CFG_MSS_CONFIG, tpc_offset,\n\t\t\t\tICACHE_FETCH_LINE_NUM, 2);\n\t}\n\n\t \n\tfor (i = 0 ; i < 128 ; i += 8)\n\t\twriteq(0, hdev->pcie_bar[SRAM_BAR_ID] + i);\n\n\tWREG32(mmMME0_CTRL_EUS_ROLLUP_CNT_ADD, 3);\n\tWREG32(mmMME1_CTRL_EUS_ROLLUP_CNT_ADD, 3);\n\tWREG32(mmMME2_CTRL_EUS_ROLLUP_CNT_ADD, 3);\n\tWREG32(mmMME3_CTRL_EUS_ROLLUP_CNT_ADD, 3);\n}\n\nstatic void gaudi_init_pci_dma_qman(struct hl_device *hdev, int dma_id,\n\t\t\t\t\tint qman_id, dma_addr_t qman_pq_addr)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 mtr_base_en_lo, mtr_base_en_hi, mtr_base_ws_lo, mtr_base_ws_hi;\n\tu32 so_base_en_lo, so_base_en_hi, so_base_ws_lo, so_base_ws_hi;\n\tu32 q_off, dma_qm_offset;\n\tu32 dma_qm_err_cfg, irq_handler_offset;\n\n\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\n\tmtr_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tmtr_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tq_off = dma_qm_offset + qman_id * 4;\n\n\tWREG32(mmDMA0_QM_PQ_BASE_LO_0 + q_off, lower_32_bits(qman_pq_addr));\n\tWREG32(mmDMA0_QM_PQ_BASE_HI_0 + q_off, upper_32_bits(qman_pq_addr));\n\n\tWREG32(mmDMA0_QM_PQ_SIZE_0 + q_off, ilog2(HL_QUEUE_LENGTH));\n\tWREG32(mmDMA0_QM_PQ_PI_0 + q_off, 0);\n\tWREG32(mmDMA0_QM_PQ_CI_0 + q_off, 0);\n\n\tWREG32(mmDMA0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off, QMAN_LDMA_SIZE_OFFSET);\n\tWREG32(mmDMA0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SRC_OFFSET);\n\tWREG32(mmDMA0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_DST_OFFSET);\n\n\tWREG32(mmDMA0_QM_CP_MSG_BASE0_ADDR_LO_0 + q_off, mtr_base_en_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE0_ADDR_HI_0 + q_off, mtr_base_en_hi);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE1_ADDR_LO_0 + q_off, so_base_en_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE1_ADDR_HI_0 + q_off, so_base_en_hi);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE2_ADDR_LO_0 + q_off, mtr_base_ws_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE2_ADDR_HI_0 + q_off, mtr_base_ws_hi);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE3_ADDR_LO_0 + q_off, so_base_ws_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE3_ADDR_HI_0 + q_off, so_base_ws_hi);\n\n\tWREG32(mmDMA0_QM_CP_BARRIER_CFG_0 + q_off, 0x100);\n\n\t \n\tif (qman_id == 0) {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_dma_qm_irq_ctrl);\n\n\t\t \n\t\tdma_qm_err_cfg = PCI_DMA_QMAN_GLBL_ERR_CFG_MSG_EN_MASK;\n\t\tif (hdev->stop_on_err)\n\t\t\tdma_qm_err_cfg |=\n\t\t\t\tPCI_DMA_QMAN_GLBL_ERR_CFG_STOP_ON_ERR_EN_MASK;\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_CFG + dma_qm_offset, dma_qm_err_cfg);\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_ADDR_LO + dma_qm_offset,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_ADDR_HI + dma_qm_offset,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_WDATA + dma_qm_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_DMA0_QM].cpu_id +\n\t\t\t\t\t\t\t\t\tdma_id);\n\n\t\tWREG32(mmDMA0_QM_ARB_ERR_MSG_EN + dma_qm_offset,\n\t\t\t\tQM_ARB_ERR_MSG_EN_MASK);\n\n\t\t \n\t\tWREG32(mmDMA0_QM_ARB_SLV_CHOISE_WDT + dma_qm_offset, GAUDI_ARB_WDT_TIMEOUT);\n\n\t\tWREG32(mmDMA0_QM_GLBL_PROT + dma_qm_offset,\n\t\t\t\tQMAN_EXTERNAL_MAKE_TRUSTED);\n\n\t\tWREG32(mmDMA0_QM_GLBL_CFG1 + dma_qm_offset, 0);\n\t}\n}\n\nstatic void gaudi_init_dma_core(struct hl_device *hdev, int dma_id)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 dma_err_cfg = 1 << DMA0_CORE_ERR_CFG_ERR_MSG_EN_SHIFT;\n\tu32 dma_offset = dma_id * DMA_CORE_OFFSET;\n\tu32 irq_handler_offset;\n\n\t \n\tWREG32(mmDMA0_CORE_RD_MAX_OUTSTAND + dma_offset, 0);\n\tWREG32(mmDMA0_CORE_RD_MAX_SIZE + dma_offset, 0);\n\n\t \n\tWREG32(mmDMA0_CORE_LBW_MAX_OUTSTAND + dma_offset, 15);\n\n\t \n\tif (hdev->stop_on_err)\n\t\tdma_err_cfg |= 1 << DMA0_CORE_ERR_CFG_STOP_ON_ERR_SHIFT;\n\n\tWREG32(mmDMA0_CORE_ERR_CFG + dma_offset, dma_err_cfg);\n\n\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\tle32_to_cpu(dyn_regs->gic_dma_core_irq_ctrl);\n\n\tWREG32(mmDMA0_CORE_ERRMSG_ADDR_LO + dma_offset,\n\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\tWREG32(mmDMA0_CORE_ERRMSG_ADDR_HI + dma_offset,\n\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\tWREG32(mmDMA0_CORE_ERRMSG_WDATA + dma_offset,\n\t\tgaudi_irq_map_table[GAUDI_EVENT_DMA0_CORE].cpu_id + dma_id);\n\tWREG32(mmDMA0_CORE_PROT + dma_offset,\n\t\t\t1 << DMA0_CORE_PROT_ERR_VAL_SHIFT);\n\t \n\tWREG32(mmDMA0_CORE_SECURE_PROPS + dma_offset,\n\t\t\t1 << DMA0_CORE_SECURE_PROPS_MMBP_SHIFT);\n\tWREG32(mmDMA0_CORE_CFG_0 + dma_offset, 1 << DMA0_CORE_CFG_0_EN_SHIFT);\n}\n\nstatic void gaudi_enable_qman(struct hl_device *hdev, int dma_id,\n\t\t\t\tu32 enable_mask)\n{\n\tu32 dma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\n\tWREG32(mmDMA0_QM_GLBL_CFG0 + dma_qm_offset, enable_mask);\n}\n\nstatic void gaudi_init_pci_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct hl_hw_queue *q;\n\tint i, j, dma_id, cpu_skip, nic_skip, cq_id = 0, q_idx, msi_vec = 0;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_PCI_DMA)\n\t\treturn;\n\n\tfor (i = 0 ; i < PCI_DMA_NUMBER_OF_CHNLS ; i++) {\n\t\tdma_id = gaudi_dma_assignment[i];\n\t\t \n\t\tif (dma_id > 1) {\n\t\t\tcpu_skip = 1;\n\t\t\tnic_skip = NIC_NUMBER_OF_ENGINES;\n\t\t} else {\n\t\t\tcpu_skip = 0;\n\t\t\tnic_skip = 0;\n\t\t}\n\n\t\tfor (j = 0 ; j < QMAN_STREAMS ; j++) {\n\t\t\tq_idx = 4 * dma_id + j + cpu_skip;\n\t\t\tq = &hdev->kernel_queues[q_idx];\n\t\t\tq->cq_id = cq_id++;\n\t\t\tq->msi_vec = nic_skip + cpu_skip + msi_vec++;\n\t\t\tgaudi_init_pci_dma_qman(hdev, dma_id, j,\n\t\t\t\t\t\tq->bus_address);\n\t\t}\n\n\t\tgaudi_init_dma_core(hdev, dma_id);\n\n\t\tgaudi_enable_qman(hdev, dma_id, PCI_DMA_QMAN_ENABLE);\n\t}\n\n\tgaudi->hw_cap_initialized |= HW_CAP_PCI_DMA;\n}\n\nstatic void gaudi_init_hbm_dma_qman(struct hl_device *hdev, int dma_id,\n\t\t\t\t\tint qman_id, u64 qman_base_addr)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 mtr_base_en_lo, mtr_base_en_hi, mtr_base_ws_lo, mtr_base_ws_hi;\n\tu32 so_base_en_lo, so_base_en_hi, so_base_ws_lo, so_base_ws_hi;\n\tu32 dma_qm_err_cfg, irq_handler_offset;\n\tu32 q_off, dma_qm_offset;\n\n\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\n\tmtr_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tmtr_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tq_off = dma_qm_offset + qman_id * 4;\n\n\tif (qman_id < 4) {\n\t\tWREG32(mmDMA0_QM_PQ_BASE_LO_0 + q_off,\n\t\t\t\t\tlower_32_bits(qman_base_addr));\n\t\tWREG32(mmDMA0_QM_PQ_BASE_HI_0 + q_off,\n\t\t\t\t\tupper_32_bits(qman_base_addr));\n\n\t\tWREG32(mmDMA0_QM_PQ_SIZE_0 + q_off, ilog2(HBM_DMA_QMAN_LENGTH));\n\t\tWREG32(mmDMA0_QM_PQ_PI_0 + q_off, 0);\n\t\tWREG32(mmDMA0_QM_PQ_CI_0 + q_off, 0);\n\n\t\tWREG32(mmDMA0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SIZE_OFFSET);\n\t\tWREG32(mmDMA0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SRC_OFFSET);\n\t\tWREG32(mmDMA0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_DST_OFFSET);\n\t} else {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_dma_qm_irq_ctrl);\n\n\t\tWREG32(mmDMA0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SIZE_OFFSET);\n\t\tWREG32(mmDMA0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SRC_OFFSET);\n\t\tWREG32(mmDMA0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_DST_OFFSET);\n\n\t\t \n\t\tdma_qm_err_cfg = HBM_DMA_QMAN_GLBL_ERR_CFG_MSG_EN_MASK;\n\t\tif (hdev->stop_on_err)\n\t\t\tdma_qm_err_cfg |=\n\t\t\t\tHBM_DMA_QMAN_GLBL_ERR_CFG_STOP_ON_ERR_EN_MASK;\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_CFG + dma_qm_offset, dma_qm_err_cfg);\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_ADDR_LO + dma_qm_offset,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_ADDR_HI + dma_qm_offset,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\t\tWREG32(mmDMA0_QM_GLBL_ERR_WDATA + dma_qm_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_DMA0_QM].cpu_id +\n\t\t\t\t\t\t\t\t\tdma_id);\n\n\t\tWREG32(mmDMA0_QM_ARB_ERR_MSG_EN + dma_qm_offset,\n\t\t\t\tQM_ARB_ERR_MSG_EN_MASK);\n\n\t\t \n\t\tWREG32(mmDMA0_QM_ARB_SLV_CHOISE_WDT + dma_qm_offset, GAUDI_ARB_WDT_TIMEOUT);\n\n\t\tWREG32(mmDMA0_QM_GLBL_CFG1 + dma_qm_offset, 0);\n\t\tWREG32(mmDMA0_QM_GLBL_PROT + dma_qm_offset,\n\t\t\t\tQMAN_INTERNAL_MAKE_TRUSTED);\n\t}\n\n\tWREG32(mmDMA0_QM_CP_MSG_BASE0_ADDR_LO_0 + q_off, mtr_base_en_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE0_ADDR_HI_0 + q_off, mtr_base_en_hi);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE1_ADDR_LO_0 + q_off, so_base_en_lo);\n\tWREG32(mmDMA0_QM_CP_MSG_BASE1_ADDR_HI_0 + q_off, so_base_en_hi);\n\n\t \n\tif (gaudi_dma_assignment[dma_id] == GAUDI_ENGINE_ID_DMA_5) {\n\t\tWREG32(mmDMA0_QM_CP_MSG_BASE2_ADDR_LO_0 + q_off,\n\t\t\t\tmtr_base_ws_lo);\n\t\tWREG32(mmDMA0_QM_CP_MSG_BASE2_ADDR_HI_0 + q_off,\n\t\t\t\tmtr_base_ws_hi);\n\t\tWREG32(mmDMA0_QM_CP_MSG_BASE3_ADDR_LO_0 + q_off,\n\t\t\t\tso_base_ws_lo);\n\t\tWREG32(mmDMA0_QM_CP_MSG_BASE3_ADDR_HI_0 + q_off,\n\t\t\t\tso_base_ws_hi);\n\t}\n}\n\nstatic void gaudi_init_hbm_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tu64 qman_base_addr;\n\tint i, j, dma_id, internal_q_index;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_HBM_DMA)\n\t\treturn;\n\n\tfor (i = 0 ; i < HBM_DMA_NUMBER_OF_CHNLS ; i++) {\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_1 + i];\n\n\t\tfor (j = 0 ; j < QMAN_STREAMS ; j++) {\n\t\t\t  \n\t\t\tinternal_q_index = dma_id * QMAN_STREAMS + j + 1;\n\n\t\t\tq = &gaudi->internal_qmans[internal_q_index];\n\t\t\tqman_base_addr = (u64) q->pq_dma_addr;\n\t\t\tgaudi_init_hbm_dma_qman(hdev, dma_id, j,\n\t\t\t\t\t\tqman_base_addr);\n\t\t}\n\n\t\t \n\t\tgaudi_init_hbm_dma_qman(hdev, dma_id, 4, 0);\n\n\t\tgaudi_init_dma_core(hdev, dma_id);\n\n\t\tgaudi_enable_qman(hdev, dma_id, HBM_DMA_QMAN_ENABLE);\n\t}\n\n\tgaudi->hw_cap_initialized |= HW_CAP_HBM_DMA;\n}\n\nstatic void gaudi_init_mme_qman(struct hl_device *hdev, u32 mme_offset,\n\t\t\t\t\tint qman_id, u64 qman_base_addr)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 mtr_base_lo, mtr_base_hi;\n\tu32 so_base_lo, so_base_hi;\n\tu32 irq_handler_offset;\n\tu32 q_off, mme_id;\n\tu32 mme_qm_err_cfg;\n\n\tmtr_base_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tq_off = mme_offset + qman_id * 4;\n\n\tif (qman_id < 4) {\n\t\tWREG32(mmMME0_QM_PQ_BASE_LO_0 + q_off,\n\t\t\t\t\tlower_32_bits(qman_base_addr));\n\t\tWREG32(mmMME0_QM_PQ_BASE_HI_0 + q_off,\n\t\t\t\t\tupper_32_bits(qman_base_addr));\n\n\t\tWREG32(mmMME0_QM_PQ_SIZE_0 + q_off, ilog2(MME_QMAN_LENGTH));\n\t\tWREG32(mmMME0_QM_PQ_PI_0 + q_off, 0);\n\t\tWREG32(mmMME0_QM_PQ_CI_0 + q_off, 0);\n\n\t\tWREG32(mmMME0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SIZE_OFFSET);\n\t\tWREG32(mmMME0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SRC_OFFSET);\n\t\tWREG32(mmMME0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_DST_OFFSET);\n\t} else {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_mme_qm_irq_ctrl);\n\n\t\tWREG32(mmMME0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SIZE_OFFSET);\n\t\tWREG32(mmMME0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SRC_OFFSET);\n\t\tWREG32(mmMME0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_DST_OFFSET);\n\n\t\t \n\t\tmme_id = mme_offset /\n\t\t\t\t(mmMME1_QM_GLBL_CFG0 - mmMME0_QM_GLBL_CFG0) / 2;\n\n\t\tmme_qm_err_cfg = MME_QMAN_GLBL_ERR_CFG_MSG_EN_MASK;\n\t\tif (hdev->stop_on_err)\n\t\t\tmme_qm_err_cfg |=\n\t\t\t\tMME_QMAN_GLBL_ERR_CFG_STOP_ON_ERR_EN_MASK;\n\n\t\tWREG32(mmMME0_QM_GLBL_ERR_CFG + mme_offset, mme_qm_err_cfg);\n\n\t\tWREG32(mmMME0_QM_GLBL_ERR_ADDR_LO + mme_offset,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\t\tWREG32(mmMME0_QM_GLBL_ERR_ADDR_HI + mme_offset,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\t\tWREG32(mmMME0_QM_GLBL_ERR_WDATA + mme_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_MME0_QM].cpu_id +\n\t\t\t\t\t\t\t\t\tmme_id);\n\n\t\tWREG32(mmMME0_QM_ARB_ERR_MSG_EN + mme_offset,\n\t\t\t\tQM_ARB_ERR_MSG_EN_MASK);\n\n\t\t \n\t\tWREG32(mmMME0_QM_ARB_SLV_CHOISE_WDT + mme_offset, GAUDI_ARB_WDT_TIMEOUT);\n\n\t\tWREG32(mmMME0_QM_GLBL_CFG1 + mme_offset, 0);\n\t\tWREG32(mmMME0_QM_GLBL_PROT + mme_offset,\n\t\t\t\tQMAN_INTERNAL_MAKE_TRUSTED);\n\t}\n\n\tWREG32(mmMME0_QM_CP_MSG_BASE0_ADDR_LO_0 + q_off, mtr_base_lo);\n\tWREG32(mmMME0_QM_CP_MSG_BASE0_ADDR_HI_0 + q_off, mtr_base_hi);\n\tWREG32(mmMME0_QM_CP_MSG_BASE1_ADDR_LO_0 + q_off, so_base_lo);\n\tWREG32(mmMME0_QM_CP_MSG_BASE1_ADDR_HI_0 + q_off, so_base_hi);\n}\n\nstatic void gaudi_init_mme_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tu64 qman_base_addr;\n\tu32 mme_offset;\n\tint i, internal_q_index;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_MME)\n\t\treturn;\n\n\t \n\n\tmme_offset = mmMME2_QM_GLBL_CFG0 - mmMME0_QM_GLBL_CFG0;\n\n\tfor (i = 0 ; i < MME_NUMBER_OF_QMANS ; i++) {\n\t\tinternal_q_index = GAUDI_QUEUE_ID_MME_0_0 + i;\n\t\tq = &gaudi->internal_qmans[internal_q_index];\n\t\tqman_base_addr = (u64) q->pq_dma_addr;\n\t\tgaudi_init_mme_qman(hdev, mme_offset, (i & 0x3),\n\t\t\t\t\tqman_base_addr);\n\t\tif (i == 3)\n\t\t\tmme_offset = 0;\n\t}\n\n\t \n\tmme_offset = mmMME2_QM_GLBL_CFG0 - mmMME0_QM_GLBL_CFG0;\n\tgaudi_init_mme_qman(hdev, mme_offset, 4, 0);\n\tgaudi_init_mme_qman(hdev, 0, 4, 0);\n\n\tWREG32(mmMME2_QM_GLBL_CFG0, QMAN_MME_ENABLE);\n\tWREG32(mmMME0_QM_GLBL_CFG0, QMAN_MME_ENABLE);\n\n\tgaudi->hw_cap_initialized |= HW_CAP_MME;\n}\n\nstatic void gaudi_init_tpc_qman(struct hl_device *hdev, u32 tpc_offset,\n\t\t\t\tint qman_id, u64 qman_base_addr)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 mtr_base_en_lo, mtr_base_en_hi, mtr_base_ws_lo, mtr_base_ws_hi;\n\tu32 so_base_en_lo, so_base_en_hi, so_base_ws_lo, so_base_ws_hi;\n\tu32 tpc_qm_err_cfg, irq_handler_offset;\n\tu32 q_off, tpc_id;\n\n\tmtr_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_en_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tmtr_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_ws_lo = lower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tq_off = tpc_offset + qman_id * 4;\n\n\ttpc_id = tpc_offset /\n\t\t\t(mmTPC1_QM_GLBL_CFG0 - mmTPC0_QM_GLBL_CFG0);\n\n\tif (qman_id < 4) {\n\t\tWREG32(mmTPC0_QM_PQ_BASE_LO_0 + q_off,\n\t\t\t\t\tlower_32_bits(qman_base_addr));\n\t\tWREG32(mmTPC0_QM_PQ_BASE_HI_0 + q_off,\n\t\t\t\t\tupper_32_bits(qman_base_addr));\n\n\t\tWREG32(mmTPC0_QM_PQ_SIZE_0 + q_off, ilog2(TPC_QMAN_LENGTH));\n\t\tWREG32(mmTPC0_QM_PQ_PI_0 + q_off, 0);\n\t\tWREG32(mmTPC0_QM_PQ_CI_0 + q_off, 0);\n\n\t\tWREG32(mmTPC0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SIZE_OFFSET);\n\t\tWREG32(mmTPC0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_SRC_OFFSET);\n\t\tWREG32(mmTPC0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_CPDMA_DST_OFFSET);\n\t} else {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_tpc_qm_irq_ctrl);\n\n\t\tWREG32(mmTPC0_QM_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SIZE_OFFSET);\n\t\tWREG32(mmTPC0_QM_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SRC_OFFSET);\n\t\tWREG32(mmTPC0_QM_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_DST_OFFSET);\n\n\t\t \n\t\ttpc_qm_err_cfg = TPC_QMAN_GLBL_ERR_CFG_MSG_EN_MASK;\n\t\tif (hdev->stop_on_err)\n\t\t\ttpc_qm_err_cfg |=\n\t\t\t\tTPC_QMAN_GLBL_ERR_CFG_STOP_ON_ERR_EN_MASK;\n\n\t\tWREG32(mmTPC0_QM_GLBL_ERR_CFG + tpc_offset, tpc_qm_err_cfg);\n\n\t\tWREG32(mmTPC0_QM_GLBL_ERR_ADDR_LO + tpc_offset,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\t\tWREG32(mmTPC0_QM_GLBL_ERR_ADDR_HI + tpc_offset,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\t\tWREG32(mmTPC0_QM_GLBL_ERR_WDATA + tpc_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_TPC0_QM].cpu_id +\n\t\t\t\t\t\t\t\t\ttpc_id);\n\n\t\tWREG32(mmTPC0_QM_ARB_ERR_MSG_EN + tpc_offset,\n\t\t\t\tQM_ARB_ERR_MSG_EN_MASK);\n\n\t\t \n\t\tWREG32(mmTPC0_QM_ARB_SLV_CHOISE_WDT + tpc_offset, GAUDI_ARB_WDT_TIMEOUT);\n\n\t\tWREG32(mmTPC0_QM_GLBL_CFG1 + tpc_offset, 0);\n\t\tWREG32(mmTPC0_QM_GLBL_PROT + tpc_offset,\n\t\t\t\tQMAN_INTERNAL_MAKE_TRUSTED);\n\t}\n\n\tWREG32(mmTPC0_QM_CP_MSG_BASE0_ADDR_LO_0 + q_off, mtr_base_en_lo);\n\tWREG32(mmTPC0_QM_CP_MSG_BASE0_ADDR_HI_0 + q_off, mtr_base_en_hi);\n\tWREG32(mmTPC0_QM_CP_MSG_BASE1_ADDR_LO_0 + q_off, so_base_en_lo);\n\tWREG32(mmTPC0_QM_CP_MSG_BASE1_ADDR_HI_0 + q_off, so_base_en_hi);\n\n\t \n\tif (tpc_id == 6) {\n\t\tWREG32(mmTPC0_QM_CP_MSG_BASE2_ADDR_LO_0 + q_off,\n\t\t\t\tmtr_base_ws_lo);\n\t\tWREG32(mmTPC0_QM_CP_MSG_BASE2_ADDR_HI_0 + q_off,\n\t\t\t\tmtr_base_ws_hi);\n\t\tWREG32(mmTPC0_QM_CP_MSG_BASE3_ADDR_LO_0 + q_off,\n\t\t\t\tso_base_ws_lo);\n\t\tWREG32(mmTPC0_QM_CP_MSG_BASE3_ADDR_HI_0 + q_off,\n\t\t\t\tso_base_ws_hi);\n\t}\n}\n\nstatic void gaudi_init_tpc_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tu64 qman_base_addr;\n\tu32 so_base_hi, tpc_offset = 0;\n\tu32 tpc_delta = mmTPC1_CFG_SM_BASE_ADDRESS_HIGH -\n\t\t\tmmTPC0_CFG_SM_BASE_ADDRESS_HIGH;\n\tint i, tpc_id, internal_q_index;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_TPC_MASK)\n\t\treturn;\n\n\tso_base_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tfor (tpc_id = 0 ; tpc_id < TPC_NUMBER_OF_ENGINES ; tpc_id++) {\n\t\tfor (i = 0 ; i < QMAN_STREAMS ; i++) {\n\t\t\tinternal_q_index = GAUDI_QUEUE_ID_TPC_0_0 +\n\t\t\t\t\t\ttpc_id * QMAN_STREAMS + i;\n\t\t\tq = &gaudi->internal_qmans[internal_q_index];\n\t\t\tqman_base_addr = (u64) q->pq_dma_addr;\n\t\t\tgaudi_init_tpc_qman(hdev, tpc_offset, i,\n\t\t\t\t\t\tqman_base_addr);\n\n\t\t\tif (i == 3) {\n\t\t\t\t \n\t\t\t\tgaudi_init_tpc_qman(hdev, tpc_offset, 4, 0);\n\n\t\t\t\t \n\t\t\t\tWREG32(mmTPC0_QM_GLBL_CFG0 + tpc_offset,\n\t\t\t\t\t\tQMAN_TPC_ENABLE);\n\t\t\t}\n\t\t}\n\n\t\tWREG32(mmTPC0_CFG_SM_BASE_ADDRESS_HIGH + tpc_id * tpc_delta,\n\t\t\t\tso_base_hi);\n\n\t\ttpc_offset += mmTPC1_QM_GLBL_CFG0 - mmTPC0_QM_GLBL_CFG0;\n\n\t\tgaudi->hw_cap_initialized |=\n\t\t\t\tFIELD_PREP(HW_CAP_TPC_MASK, 1 << tpc_id);\n\t}\n}\n\nstatic void gaudi_init_nic_qman(struct hl_device *hdev, u32 nic_offset,\n\t\t\t\tint qman_id, u64 qman_base_addr, int nic_id)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 mtr_base_en_lo, mtr_base_en_hi, mtr_base_ws_lo, mtr_base_ws_hi;\n\tu32 so_base_en_lo, so_base_en_hi, so_base_ws_lo, so_base_ws_hi;\n\tu32 nic_qm_err_cfg, irq_handler_offset;\n\tu32 q_off;\n\n\tmtr_base_en_lo = lower_32_bits((CFG_BASE & U32_MAX) +\n\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_en_lo = lower_32_bits((CFG_BASE & U32_MAX) +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_en_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tmtr_base_ws_lo = lower_32_bits((CFG_BASE & U32_MAX) +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tmtr_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0);\n\tso_base_ws_lo = lower_32_bits((CFG_BASE & U32_MAX) +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\tso_base_ws_hi = upper_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0);\n\n\tq_off = nic_offset + qman_id * 4;\n\n\tWREG32(mmNIC0_QM0_PQ_BASE_LO_0 + q_off, lower_32_bits(qman_base_addr));\n\tWREG32(mmNIC0_QM0_PQ_BASE_HI_0 + q_off, upper_32_bits(qman_base_addr));\n\n\tWREG32(mmNIC0_QM0_PQ_SIZE_0 + q_off, ilog2(NIC_QMAN_LENGTH));\n\tWREG32(mmNIC0_QM0_PQ_PI_0 + q_off, 0);\n\tWREG32(mmNIC0_QM0_PQ_CI_0 + q_off, 0);\n\n\tWREG32(mmNIC0_QM0_CP_LDMA_TSIZE_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SIZE_OFFSET);\n\tWREG32(mmNIC0_QM0_CP_LDMA_SRC_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_SRC_OFFSET);\n\tWREG32(mmNIC0_QM0_CP_LDMA_DST_BASE_LO_OFFSET_0 + q_off,\n\t\t\t\t\t\t\tQMAN_LDMA_DST_OFFSET);\n\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE0_ADDR_LO_0 + q_off, mtr_base_en_lo);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE0_ADDR_HI_0 + q_off, mtr_base_en_hi);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE1_ADDR_LO_0 + q_off, so_base_en_lo);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE1_ADDR_HI_0 + q_off, so_base_en_hi);\n\n\t \n\tWREG32(mmNIC0_QM0_CP_MSG_BASE2_ADDR_LO_0 + q_off, mtr_base_ws_lo);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE2_ADDR_HI_0 + q_off, mtr_base_ws_hi);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE3_ADDR_LO_0 + q_off, so_base_ws_lo);\n\tWREG32(mmNIC0_QM0_CP_MSG_BASE3_ADDR_HI_0 + q_off, so_base_ws_hi);\n\n\tif (qman_id == 0) {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_nic_qm_irq_ctrl);\n\n\t\t \n\t\tnic_qm_err_cfg = NIC_QMAN_GLBL_ERR_CFG_MSG_EN_MASK;\n\t\tif (hdev->stop_on_err)\n\t\t\tnic_qm_err_cfg |=\n\t\t\t\tNIC_QMAN_GLBL_ERR_CFG_STOP_ON_ERR_EN_MASK;\n\n\t\tWREG32(mmNIC0_QM0_GLBL_ERR_CFG + nic_offset, nic_qm_err_cfg);\n\n\t\tWREG32(mmNIC0_QM0_GLBL_ERR_ADDR_LO + nic_offset,\n\t\t\tlower_32_bits(CFG_BASE + irq_handler_offset));\n\t\tWREG32(mmNIC0_QM0_GLBL_ERR_ADDR_HI + nic_offset,\n\t\t\tupper_32_bits(CFG_BASE + irq_handler_offset));\n\n\t\tWREG32(mmNIC0_QM0_GLBL_ERR_WDATA + nic_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_NIC0_QM0].cpu_id +\n\t\t\t\t\t\t\t\t\tnic_id);\n\n\t\tWREG32(mmNIC0_QM0_ARB_ERR_MSG_EN + nic_offset,\n\t\t\t\tQM_ARB_ERR_MSG_EN_MASK);\n\n\t\t \n\t\tWREG32(mmNIC0_QM0_ARB_SLV_CHOISE_WDT + nic_offset, GAUDI_ARB_WDT_TIMEOUT);\n\n\t\tWREG32(mmNIC0_QM0_GLBL_CFG1 + nic_offset, 0);\n\t\tWREG32(mmNIC0_QM0_GLBL_PROT + nic_offset,\n\t\t\t\tQMAN_INTERNAL_MAKE_TRUSTED);\n\t}\n}\n\nstatic void gaudi_init_nic_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\tu64 qman_base_addr;\n\tu32 nic_offset = 0;\n\tu32 nic_delta_between_qmans =\n\t\t\tmmNIC0_QM1_GLBL_CFG0 - mmNIC0_QM0_GLBL_CFG0;\n\tu32 nic_delta_between_nics =\n\t\t\tmmNIC1_QM0_GLBL_CFG0 - mmNIC0_QM0_GLBL_CFG0;\n\tint i, nic_id, internal_q_index;\n\n\tif (!hdev->nic_ports_mask)\n\t\treturn;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC_MASK)\n\t\treturn;\n\n\tdev_dbg(hdev->dev, \"Initializing NIC QMANs\\n\");\n\n\tfor (nic_id = 0 ; nic_id < NIC_NUMBER_OF_ENGINES ; nic_id++) {\n\t\tif (!(hdev->nic_ports_mask & (1 << nic_id))) {\n\t\t\tnic_offset += nic_delta_between_qmans;\n\t\t\tif (nic_id & 1) {\n\t\t\t\tnic_offset -= (nic_delta_between_qmans * 2);\n\t\t\t\tnic_offset += nic_delta_between_nics;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tfor (i = 0 ; i < QMAN_STREAMS ; i++) {\n\t\t\tinternal_q_index = GAUDI_QUEUE_ID_NIC_0_0 +\n\t\t\t\t\t\tnic_id * QMAN_STREAMS + i;\n\t\t\tq = &gaudi->internal_qmans[internal_q_index];\n\t\t\tqman_base_addr = (u64) q->pq_dma_addr;\n\t\t\tgaudi_init_nic_qman(hdev, nic_offset, (i & 0x3),\n\t\t\t\t\t\tqman_base_addr, nic_id);\n\t\t}\n\n\t\t \n\t\tWREG32(mmNIC0_QM0_GLBL_CFG0 + nic_offset, NIC_QMAN_ENABLE);\n\n\t\tnic_offset += nic_delta_between_qmans;\n\t\tif (nic_id & 1) {\n\t\t\tnic_offset -= (nic_delta_between_qmans * 2);\n\t\t\tnic_offset += nic_delta_between_nics;\n\t\t}\n\n\t\tgaudi->hw_cap_initialized |= 1 << (HW_CAP_NIC_SHIFT + nic_id);\n\t}\n}\n\nstatic void gaudi_disable_pci_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_PCI_DMA))\n\t\treturn;\n\n\tWREG32(mmDMA0_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA1_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA5_QM_GLBL_CFG0, 0);\n}\n\nstatic void gaudi_disable_hbm_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_HBM_DMA))\n\t\treturn;\n\n\tWREG32(mmDMA2_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA3_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA4_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA6_QM_GLBL_CFG0, 0);\n\tWREG32(mmDMA7_QM_GLBL_CFG0, 0);\n}\n\nstatic void gaudi_disable_mme_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MME))\n\t\treturn;\n\n\tWREG32(mmMME2_QM_GLBL_CFG0, 0);\n\tWREG32(mmMME0_QM_GLBL_CFG0, 0);\n}\n\nstatic void gaudi_disable_tpc_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 tpc_offset = 0;\n\tint tpc_id;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tfor (tpc_id = 0 ; tpc_id < TPC_NUMBER_OF_ENGINES ; tpc_id++) {\n\t\tWREG32(mmTPC0_QM_GLBL_CFG0 + tpc_offset, 0);\n\t\ttpc_offset += mmTPC1_QM_GLBL_CFG0 - mmTPC0_QM_GLBL_CFG0;\n\t}\n}\n\nstatic void gaudi_disable_nic_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 nic_mask, nic_offset = 0;\n\tu32 nic_delta_between_qmans =\n\t\t\tmmNIC0_QM1_GLBL_CFG0 - mmNIC0_QM0_GLBL_CFG0;\n\tu32 nic_delta_between_nics =\n\t\t\tmmNIC1_QM0_GLBL_CFG0 - mmNIC0_QM0_GLBL_CFG0;\n\tint nic_id;\n\n\tfor (nic_id = 0 ; nic_id < NIC_NUMBER_OF_ENGINES ; nic_id++) {\n\t\tnic_mask = 1 << (HW_CAP_NIC_SHIFT + nic_id);\n\n\t\tif (gaudi->hw_cap_initialized & nic_mask)\n\t\t\tWREG32(mmNIC0_QM0_GLBL_CFG0 + nic_offset, 0);\n\n\t\tnic_offset += nic_delta_between_qmans;\n\t\tif (nic_id & 1) {\n\t\t\tnic_offset -= (nic_delta_between_qmans * 2);\n\t\t\tnic_offset += nic_delta_between_nics;\n\t\t}\n\t}\n}\n\nstatic void gaudi_stop_pci_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_PCI_DMA))\n\t\treturn;\n\n\t \n\tWREG32(mmDMA0_QM_GLBL_CFG1, 0xF << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA1_QM_GLBL_CFG1, 0xF << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA5_QM_GLBL_CFG1, 0xF << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n}\n\nstatic void gaudi_stop_hbm_dma_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_HBM_DMA))\n\t\treturn;\n\n\t \n\n\tWREG32(mmDMA2_QM_GLBL_CFG1, 0x1F << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA3_QM_GLBL_CFG1, 0x1F << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA4_QM_GLBL_CFG1, 0x1F << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA6_QM_GLBL_CFG1, 0x1F << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmDMA7_QM_GLBL_CFG1, 0x1F << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n}\n\nstatic void gaudi_stop_mme_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MME))\n\t\treturn;\n\n\t \n\tWREG32(mmMME2_QM_GLBL_CFG1, 0x1F << MME0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmMME0_QM_GLBL_CFG1, 0x1F << MME0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n}\n\nstatic void gaudi_stop_tpc_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tWREG32(mmTPC0_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC1_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC2_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC3_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC4_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC5_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC6_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\tWREG32(mmTPC7_QM_GLBL_CFG1, 0x1F << TPC0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n}\n\nstatic void gaudi_stop_nic_qmans(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\t \n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC0)\n\t\tWREG32(mmNIC0_QM0_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC1)\n\t\tWREG32(mmNIC0_QM1_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC2)\n\t\tWREG32(mmNIC1_QM0_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC3)\n\t\tWREG32(mmNIC1_QM1_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC4)\n\t\tWREG32(mmNIC2_QM0_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC5)\n\t\tWREG32(mmNIC2_QM1_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC6)\n\t\tWREG32(mmNIC3_QM0_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC7)\n\t\tWREG32(mmNIC3_QM1_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC8)\n\t\tWREG32(mmNIC4_QM0_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC9)\n\t\tWREG32(mmNIC4_QM1_GLBL_CFG1,\n\t\t\t\tNIC0_QM0_GLBL_CFG1_PQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CQF_STOP_MASK |\n\t\t\t\tNIC0_QM0_GLBL_CFG1_CP_STOP_MASK);\n}\n\nstatic void gaudi_pci_dma_stall(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_PCI_DMA))\n\t\treturn;\n\n\tWREG32(mmDMA0_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA1_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA5_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n}\n\nstatic void gaudi_hbm_dma_stall(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_HBM_DMA))\n\t\treturn;\n\n\tWREG32(mmDMA2_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA3_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA4_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA6_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n\tWREG32(mmDMA7_CORE_CFG_1, 1 << DMA0_CORE_CFG_1_HALT_SHIFT);\n}\n\nstatic void gaudi_mme_stall(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MME))\n\t\treturn;\n\n\t \n\tWREG32(mmMME0_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME0_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME0_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME0_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME1_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME1_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME1_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME1_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME2_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME2_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME2_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME2_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME3_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME3_ACC_ACC_STALL, 1 << MME_ACC_ACC_STALL_R_SHIFT);\n\tWREG32(mmMME3_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n\tWREG32(mmMME3_SBAB_SB_STALL, 1 << MME_SBAB_SB_STALL_R_SHIFT);\n}\n\nstatic void gaudi_tpc_stall(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_TPC_MASK))\n\t\treturn;\n\n\tWREG32(mmTPC0_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC1_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC2_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC3_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC4_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC5_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC6_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n\tWREG32(mmTPC7_CFG_TPC_STALL, 1 << TPC0_CFG_TPC_STALL_V_SHIFT);\n}\n\nstatic void gaudi_disable_clock_gating(struct hl_device *hdev)\n{\n\tu32 qman_offset;\n\tint i;\n\n\tif (hdev->asic_prop.fw_security_enabled)\n\t\treturn;\n\n\tfor (i = 0, qman_offset = 0 ; i < DMA_NUMBER_OF_CHANNELS ; i++) {\n\t\tWREG32(mmDMA0_QM_CGM_CFG + qman_offset, 0);\n\t\tWREG32(mmDMA0_QM_CGM_CFG1 + qman_offset, 0);\n\n\t\tqman_offset += (mmDMA1_QM_CGM_CFG - mmDMA0_QM_CGM_CFG);\n\t}\n\n\tWREG32(mmMME0_QM_CGM_CFG, 0);\n\tWREG32(mmMME0_QM_CGM_CFG1, 0);\n\tWREG32(mmMME2_QM_CGM_CFG, 0);\n\tWREG32(mmMME2_QM_CGM_CFG1, 0);\n\n\tfor (i = 0, qman_offset = 0 ; i < TPC_NUMBER_OF_ENGINES ; i++) {\n\t\tWREG32(mmTPC0_QM_CGM_CFG + qman_offset, 0);\n\t\tWREG32(mmTPC0_QM_CGM_CFG1 + qman_offset, 0);\n\n\t\tqman_offset += (mmTPC1_QM_CGM_CFG - mmTPC0_QM_CGM_CFG);\n\t}\n}\n\nstatic void gaudi_enable_timestamp(struct hl_device *hdev)\n{\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE - CFG_BASE, 0);\n\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE - CFG_BASE + 0xC, 0);\n\tWREG32(mmPSOC_TIMESTAMP_BASE - CFG_BASE + 0x8, 0);\n\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE - CFG_BASE, 1);\n}\n\nstatic void gaudi_disable_timestamp(struct hl_device *hdev)\n{\n\t \n\tWREG32(mmPSOC_TIMESTAMP_BASE - CFG_BASE, 0);\n}\n\nstatic void gaudi_halt_engines(struct hl_device *hdev, bool hard_reset, bool fw_reset)\n{\n\tu32 wait_timeout_ms;\n\n\tif (hdev->pldm)\n\t\twait_timeout_ms = GAUDI_PLDM_RESET_WAIT_MSEC;\n\telse\n\t\twait_timeout_ms = GAUDI_RESET_WAIT_MSEC;\n\n\tif (fw_reset)\n\t\tgoto skip_engines;\n\n\tgaudi_stop_nic_qmans(hdev);\n\tgaudi_stop_mme_qmans(hdev);\n\tgaudi_stop_tpc_qmans(hdev);\n\tgaudi_stop_hbm_dma_qmans(hdev);\n\tgaudi_stop_pci_dma_qmans(hdev);\n\n\tmsleep(wait_timeout_ms);\n\n\tgaudi_pci_dma_stall(hdev);\n\tgaudi_hbm_dma_stall(hdev);\n\tgaudi_tpc_stall(hdev);\n\tgaudi_mme_stall(hdev);\n\n\tmsleep(wait_timeout_ms);\n\n\tgaudi_disable_nic_qmans(hdev);\n\tgaudi_disable_mme_qmans(hdev);\n\tgaudi_disable_tpc_qmans(hdev);\n\tgaudi_disable_hbm_dma_qmans(hdev);\n\tgaudi_disable_pci_dma_qmans(hdev);\n\n\tgaudi_disable_timestamp(hdev);\n\nskip_engines:\n\tgaudi_disable_msi(hdev);\n}\n\nstatic int gaudi_mmu_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu64 hop0_addr;\n\tint rc, i;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_MMU)\n\t\treturn 0;\n\n\tfor (i = 0 ; i < prop->max_asid ; i++) {\n\t\thop0_addr = prop->mmu_pgt_addr +\n\t\t\t\t(i * prop->mmu_hop_table_size);\n\n\t\trc = gaudi_mmu_update_asid_hop0_addr(hdev, i, hop0_addr);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"failed to set hop0 addr for asid %d\\n\", i);\n\t\t\treturn rc;\n\t\t}\n\t}\n\n\t \n\tWREG32(mmSTLB_CACHE_INV_BASE_39_8, prop->mmu_cache_mng_addr >> 8);\n\tWREG32(mmSTLB_CACHE_INV_BASE_49_40, prop->mmu_cache_mng_addr >> 40);\n\n\t \n\tWREG32(mmSTLB_MEM_CACHE_INVALIDATION, 1);\n\n\trc = hl_mmu_invalidate_cache(hdev, true, 0);\n\tif (rc)\n\t\treturn rc;\n\n\tWREG32(mmMMU_UP_MMU_ENABLE, 1);\n\tWREG32(mmMMU_UP_SPI_MASK, 0xF);\n\n\tWREG32(mmSTLB_HOP_CONFIGURATION, 0x30440);\n\n\t \n\tgaudi->mmu_cache_inv_pi = 1;\n\n\tgaudi->hw_cap_initialized |= HW_CAP_MMU;\n\n\treturn 0;\n}\n\nstatic int gaudi_load_firmware_to_device(struct hl_device *hdev)\n{\n\tvoid __iomem *dst;\n\n\tdst = hdev->pcie_bar[HBM_BAR_ID] + LINUX_FW_OFFSET;\n\n\treturn hl_fw_load_fw_to_device(hdev, GAUDI_LINUX_FW_FILE, dst, 0, 0);\n}\n\nstatic int gaudi_load_boot_fit_to_device(struct hl_device *hdev)\n{\n\tvoid __iomem *dst;\n\n\tdst = hdev->pcie_bar[SRAM_BAR_ID] + BOOT_FIT_SRAM_OFFSET;\n\n\treturn hl_fw_load_fw_to_device(hdev, GAUDI_BOOT_FIT_FILE, dst, 0, 0);\n}\n\nstatic void gaudi_init_dynamic_firmware_loader(struct hl_device *hdev)\n{\n\tstruct dynamic_fw_load_mgr *dynamic_loader;\n\tstruct cpu_dyn_regs *dyn_regs;\n\n\tdynamic_loader = &hdev->fw_loader.dynamic_loader;\n\n\t \n\tdyn_regs = &dynamic_loader->comm_desc.cpu_dyn_regs;\n\tdyn_regs->kmd_msg_to_cpu =\n\t\t\t\tcpu_to_le32(mmPSOC_GLOBAL_CONF_KMD_MSG_TO_CPU);\n\tdyn_regs->cpu_cmd_status_to_host =\n\t\t\t\tcpu_to_le32(mmCPU_CMD_STATUS_TO_HOST);\n\n\tdynamic_loader->wait_for_bl_timeout = GAUDI_WAIT_FOR_BL_TIMEOUT_USEC;\n}\n\nstatic void gaudi_init_static_firmware_loader(struct hl_device *hdev)\n{\n\tstruct static_fw_load_mgr *static_loader;\n\n\tstatic_loader = &hdev->fw_loader.static_loader;\n\n\tstatic_loader->preboot_version_max_off = SRAM_SIZE - VERSION_MAX_LEN;\n\tstatic_loader->boot_fit_version_max_off = SRAM_SIZE - VERSION_MAX_LEN;\n\tstatic_loader->kmd_msg_to_cpu_reg = mmPSOC_GLOBAL_CONF_KMD_MSG_TO_CPU;\n\tstatic_loader->cpu_cmd_status_to_host_reg = mmCPU_CMD_STATUS_TO_HOST;\n\tstatic_loader->cpu_boot_status_reg = mmPSOC_GLOBAL_CONF_CPU_BOOT_STATUS;\n\tstatic_loader->cpu_boot_dev_status0_reg = mmCPU_BOOT_DEV_STS0;\n\tstatic_loader->cpu_boot_dev_status1_reg = mmCPU_BOOT_DEV_STS1;\n\tstatic_loader->boot_err0_reg = mmCPU_BOOT_ERR0;\n\tstatic_loader->boot_err1_reg = mmCPU_BOOT_ERR1;\n\tstatic_loader->preboot_version_offset_reg = mmPREBOOT_VER_OFFSET;\n\tstatic_loader->boot_fit_version_offset_reg = mmUBOOT_VER_OFFSET;\n\tstatic_loader->sram_offset_mask = ~(lower_32_bits(SRAM_BASE_ADDR));\n\tstatic_loader->cpu_reset_wait_msec = hdev->pldm ?\n\t\t\tGAUDI_PLDM_RESET_WAIT_MSEC :\n\t\t\tGAUDI_CPU_RESET_WAIT_MSEC;\n}\n\nstatic void gaudi_init_firmware_preload_params(struct hl_device *hdev)\n{\n\tstruct pre_fw_load_props *pre_fw_load = &hdev->fw_loader.pre_fw_load;\n\n\tpre_fw_load->cpu_boot_status_reg = mmPSOC_GLOBAL_CONF_CPU_BOOT_STATUS;\n\tpre_fw_load->sts_boot_dev_sts0_reg = mmCPU_BOOT_DEV_STS0;\n\tpre_fw_load->sts_boot_dev_sts1_reg = mmCPU_BOOT_DEV_STS1;\n\tpre_fw_load->boot_err0_reg = mmCPU_BOOT_ERR0;\n\tpre_fw_load->boot_err1_reg = mmCPU_BOOT_ERR1;\n\tpre_fw_load->wait_for_preboot_timeout = GAUDI_BOOT_FIT_REQ_TIMEOUT_USEC;\n}\n\nstatic void gaudi_init_firmware_loader(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct fw_load_mgr *fw_loader = &hdev->fw_loader;\n\n\t \n\tfw_loader->fw_comp_loaded = FW_TYPE_NONE;\n\tfw_loader->boot_fit_img.image_name = GAUDI_BOOT_FIT_FILE;\n\tfw_loader->linux_img.image_name = GAUDI_LINUX_FW_FILE;\n\tfw_loader->cpu_timeout = GAUDI_CPU_TIMEOUT_USEC;\n\tfw_loader->boot_fit_timeout = GAUDI_BOOT_FIT_REQ_TIMEOUT_USEC;\n\tfw_loader->skip_bmc = !hdev->bmc_enable;\n\tfw_loader->sram_bar_id = SRAM_BAR_ID;\n\tfw_loader->dram_bar_id = HBM_BAR_ID;\n\n\tif (prop->dynamic_fw_load)\n\t\tgaudi_init_dynamic_firmware_loader(hdev);\n\telse\n\t\tgaudi_init_static_firmware_loader(hdev);\n}\n\nstatic int gaudi_init_cpu(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tint rc;\n\n\tif (!(hdev->fw_components & FW_TYPE_PREBOOT_CPU))\n\t\treturn 0;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_CPU)\n\t\treturn 0;\n\n\t \n\tif (!hdev->asic_prop.fw_security_enabled)\n\t\tWREG32(mmCPU_IF_CPU_MSB_ADDR, hdev->cpu_pci_msb_addr);\n\n\trc = hl_fw_init_cpu(hdev);\n\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi->hw_cap_initialized |= HW_CAP_CPU;\n\n\treturn 0;\n}\n\nstatic int gaudi_init_cpu_queues(struct hl_device *hdev, u32 cpu_timeout)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 status, irq_handler_offset;\n\tstruct hl_eq *eq;\n\tstruct hl_hw_queue *cpu_pq =\n\t\t\t&hdev->kernel_queues[GAUDI_QUEUE_ID_CPU_PQ];\n\tint err;\n\n\tif (!hdev->cpu_queues_enable)\n\t\treturn 0;\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_CPU_Q)\n\t\treturn 0;\n\n\teq = &hdev->event_queue;\n\n\tWREG32(mmCPU_IF_PQ_BASE_ADDR_LOW, lower_32_bits(cpu_pq->bus_address));\n\tWREG32(mmCPU_IF_PQ_BASE_ADDR_HIGH, upper_32_bits(cpu_pq->bus_address));\n\n\tWREG32(mmCPU_IF_EQ_BASE_ADDR_LOW, lower_32_bits(eq->bus_address));\n\tWREG32(mmCPU_IF_EQ_BASE_ADDR_HIGH, upper_32_bits(eq->bus_address));\n\n\tWREG32(mmCPU_IF_CQ_BASE_ADDR_LOW,\n\t\t\tlower_32_bits(hdev->cpu_accessible_dma_address));\n\tWREG32(mmCPU_IF_CQ_BASE_ADDR_HIGH,\n\t\t\tupper_32_bits(hdev->cpu_accessible_dma_address));\n\n\tWREG32(mmCPU_IF_PQ_LENGTH, HL_QUEUE_SIZE_IN_BYTES);\n\tWREG32(mmCPU_IF_EQ_LENGTH, HL_EQ_SIZE_IN_BYTES);\n\tWREG32(mmCPU_IF_CQ_LENGTH, HL_CPU_ACCESSIBLE_MEM_SIZE);\n\n\t \n\tWREG32(mmCPU_IF_EQ_RD_OFFS, 0);\n\n\tWREG32(mmCPU_IF_PF_PQ_PI, 0);\n\n\tWREG32(mmCPU_IF_QUEUE_INIT, PQ_INIT_STATUS_READY_FOR_CP_SINGLE_MSI);\n\n\tirq_handler_offset = prop->gic_interrupts_enable ?\n\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\tle32_to_cpu(dyn_regs->gic_host_pi_upd_irq);\n\n\tWREG32(irq_handler_offset,\n\t\tgaudi_irq_map_table[GAUDI_EVENT_PI_UPDATE].cpu_id);\n\n\terr = hl_poll_timeout(\n\t\thdev,\n\t\tmmCPU_IF_QUEUE_INIT,\n\t\tstatus,\n\t\t(status == PQ_INIT_STATUS_READY_FOR_HOST),\n\t\t1000,\n\t\tcpu_timeout);\n\n\tif (err) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to communicate with Device CPU (CPU-CP timeout)\\n\");\n\t\treturn -EIO;\n\t}\n\n\t \n\tif (prop->fw_cpu_boot_dev_sts0_valid)\n\t\tprop->fw_app_cpu_boot_dev_sts0 = RREG32(mmCPU_BOOT_DEV_STS0);\n\tif (prop->fw_cpu_boot_dev_sts1_valid)\n\t\tprop->fw_app_cpu_boot_dev_sts1 = RREG32(mmCPU_BOOT_DEV_STS1);\n\n\tgaudi->hw_cap_initialized |= HW_CAP_CPU_Q;\n\treturn 0;\n}\n\nstatic void gaudi_pre_hw_init(struct hl_device *hdev)\n{\n\t \n\tRREG32(mmHW_STATE);\n\n\tif (!hdev->asic_prop.fw_security_enabled) {\n\t\t \n\t\tWREG32(mmPCIE_WRAP_LBW_PROT_OVR,\n\t\t\t\t(PCIE_WRAP_LBW_PROT_OVR_RD_EN_MASK |\n\t\t\t\tPCIE_WRAP_LBW_PROT_OVR_WR_EN_MASK));\n\n\t\t \n\t\tRREG32(mmPCIE_WRAP_LBW_PROT_OVR);\n\t}\n\n\t \n\tWREG32(mmHW_STATE, HL_DEVICE_HW_STATE_DIRTY);\n}\n\nstatic int gaudi_hw_init(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tint rc;\n\n\tgaudi_pre_hw_init(hdev);\n\n\t \n\tif (hdev->asic_prop.iatu_done_by_fw)\n\t\tgaudi->hbm_bar_cur_addr = DRAM_PHYS_BASE;\n\n\t \n\tif (gaudi_set_hbm_bar_base(hdev, DRAM_PHYS_BASE) == U64_MAX) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"failed to map HBM bar to DRAM base address\\n\");\n\t\treturn -EIO;\n\t}\n\n\trc = gaudi_init_cpu(hdev);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to initialize CPU\\n\");\n\t\treturn rc;\n\t}\n\n\t \n\tgaudi_disable_clock_gating(hdev);\n\n\t \n\tgaudi_init_scrambler_sram(hdev);\n\n\t \n\tgaudi_init_scrambler_hbm(hdev);\n\n\tgaudi_init_golden_registers(hdev);\n\n\trc = gaudi_mmu_init(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi_init_security(hdev);\n\n\tgaudi_init_pci_dma_qmans(hdev);\n\n\tgaudi_init_hbm_dma_qmans(hdev);\n\n\tgaudi_init_mme_qmans(hdev);\n\n\tgaudi_init_tpc_qmans(hdev);\n\n\tgaudi_init_nic_qmans(hdev);\n\n\tgaudi_enable_timestamp(hdev);\n\n\t \n\trc = gaudi_enable_msi(hdev);\n\tif (rc)\n\t\tgoto disable_queues;\n\n\t \n\trc = gaudi_init_cpu_queues(hdev, GAUDI_CPU_TIMEOUT_USEC);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to initialize CPU H/W queues %d\\n\",\n\t\t\trc);\n\t\tgoto disable_msi;\n\t}\n\n\t \n\tRREG32(mmHW_STATE);\n\n\treturn 0;\n\ndisable_msi:\n\tgaudi_disable_msi(hdev);\ndisable_queues:\n\tgaudi_disable_mme_qmans(hdev);\n\tgaudi_disable_pci_dma_qmans(hdev);\n\n\treturn rc;\n}\n\nstatic int gaudi_hw_fini(struct hl_device *hdev, bool hard_reset, bool fw_reset)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 status, reset_timeout_ms, cpu_timeout_ms, irq_handler_offset;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tbool driver_performs_reset;\n\n\tif (!hard_reset) {\n\t\tdev_err(hdev->dev, \"GAUDI doesn't support soft-reset\\n\");\n\t\treturn 0;\n\t}\n\n\tif (hdev->pldm) {\n\t\treset_timeout_ms = GAUDI_PLDM_HRESET_TIMEOUT_MSEC;\n\t\tcpu_timeout_ms = GAUDI_PLDM_RESET_WAIT_MSEC;\n\t} else {\n\t\treset_timeout_ms = GAUDI_RESET_TIMEOUT_MSEC;\n\t\tcpu_timeout_ms = GAUDI_CPU_RESET_WAIT_MSEC;\n\t}\n\n\tif (fw_reset) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Firmware performs HARD reset, going to wait %dms\\n\",\n\t\t\treset_timeout_ms);\n\n\t\tgoto skip_reset;\n\t}\n\n\tdriver_performs_reset = !!(!hdev->asic_prop.fw_security_enabled &&\n\t\t\t\t\t!hdev->asic_prop.hard_reset_done_by_fw);\n\n\t \n\tif (driver_performs_reset)\n\t\tWREG32(mmPCIE_AUX_FLR_CTRL, (PCIE_AUX_FLR_CTRL_HW_CTRL_MASK |\n\t\t\t\t\tPCIE_AUX_FLR_CTRL_INT_MASK_MASK));\n\n\t \n\tif (hdev->fw_loader.fw_comp_loaded & FW_TYPE_LINUX) {\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_host_halt_irq);\n\n\t\tWREG32(irq_handler_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_HALT_MACHINE].cpu_id);\n\n\t\t \n\t\tif (hdev->reset_info.curr_reset_cause == HL_RESET_CAUSE_HEARTBEAT) {\n\t\t\tif (hdev->asic_prop.hard_reset_done_by_fw)\n\t\t\t\thl_fw_ask_hard_reset_without_linux(hdev);\n\t\t\telse\n\t\t\t\thl_fw_ask_halt_machine_without_linux(hdev);\n\t\t}\n\t} else {\n\t\tif (hdev->asic_prop.hard_reset_done_by_fw)\n\t\t\thl_fw_ask_hard_reset_without_linux(hdev);\n\t\telse\n\t\t\thl_fw_ask_halt_machine_without_linux(hdev);\n\t}\n\n\tif (driver_performs_reset) {\n\n\t\t \n\t\tWREG32(mmPSOC_GLOBAL_CONF_SOFT_RST_CFG_H,\n\t\t\t\t\t\t(CFG_RST_H_DMA_MASK |\n\t\t\t\t\t\tCFG_RST_H_MME_MASK |\n\t\t\t\t\t\tCFG_RST_H_SM_MASK |\n\t\t\t\t\t\tCFG_RST_H_TPC_7_MASK));\n\n\t\tWREG32(mmPSOC_GLOBAL_CONF_SOFT_RST_CFG_L, CFG_RST_L_TPC_MASK);\n\n\t\tWREG32(mmPSOC_GLOBAL_CONF_SW_ALL_RST_CFG_H,\n\t\t\t\t\t\t(CFG_RST_H_HBM_MASK |\n\t\t\t\t\t\tCFG_RST_H_TPC_7_MASK |\n\t\t\t\t\t\tCFG_RST_H_NIC_MASK |\n\t\t\t\t\t\tCFG_RST_H_SM_MASK |\n\t\t\t\t\t\tCFG_RST_H_DMA_MASK |\n\t\t\t\t\t\tCFG_RST_H_MME_MASK |\n\t\t\t\t\t\tCFG_RST_H_CPU_MASK |\n\t\t\t\t\t\tCFG_RST_H_MMU_MASK));\n\n\t\tWREG32(mmPSOC_GLOBAL_CONF_SW_ALL_RST_CFG_L,\n\t\t\t\t\t\t(CFG_RST_L_IF_MASK |\n\t\t\t\t\t\tCFG_RST_L_PSOC_MASK |\n\t\t\t\t\t\tCFG_RST_L_TPC_MASK));\n\n\t\tmsleep(cpu_timeout_ms);\n\n\t\t \n\t\tWREG32(mmPREBOOT_PCIE_EN, LKD_HARD_RESET_MAGIC);\n\n\t\t \n\t\tWREG32(mmPSOC_GLOBAL_CONF_BOOT_SEQ_RE_START, 1);\n\n\t\tWREG32(mmPSOC_GLOBAL_CONF_SW_ALL_RST,\n\t\t\t1 << PSOC_GLOBAL_CONF_SW_ALL_RST_IND_SHIFT);\n\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Issued HARD reset command, going to wait %dms\\n\",\n\t\t\treset_timeout_ms);\n\t} else {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Firmware performs HARD reset, going to wait %dms\\n\",\n\t\t\treset_timeout_ms);\n\t}\n\nskip_reset:\n\t \n\tmsleep(reset_timeout_ms);\n\n\tstatus = RREG32(mmPSOC_GLOBAL_CONF_BTM_FSM);\n\tif (status & PSOC_GLOBAL_CONF_BTM_FSM_STATE_MASK) {\n\t\tdev_err(hdev->dev, \"Timeout while waiting for device to reset 0x%x\\n\", status);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tif (gaudi) {\n\t\tgaudi->hw_cap_initialized &= ~(HW_CAP_CPU | HW_CAP_CPU_Q | HW_CAP_HBM |\n\t\t\t\t\t\tHW_CAP_PCI_DMA | HW_CAP_MME | HW_CAP_TPC_MASK |\n\t\t\t\t\t\tHW_CAP_HBM_DMA | HW_CAP_PLL | HW_CAP_NIC_MASK |\n\t\t\t\t\t\tHW_CAP_MMU | HW_CAP_SRAM_SCRAMBLER |\n\t\t\t\t\t\tHW_CAP_HBM_SCRAMBLER);\n\n\t\tmemset(gaudi->events_stat, 0, sizeof(gaudi->events_stat));\n\n\t\thdev->device_cpu_is_halted = false;\n\t}\n\treturn 0;\n}\n\nstatic int gaudi_suspend(struct hl_device *hdev)\n{\n\tint rc;\n\n\trc = hl_fw_send_pci_access_msg(hdev, CPUCP_PACKET_DISABLE_PCI_ACCESS, 0x0);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Failed to disable PCI access from CPU\\n\");\n\n\treturn rc;\n}\n\nstatic int gaudi_resume(struct hl_device *hdev)\n{\n\treturn gaudi_init_iatu(hdev);\n}\n\nstatic int gaudi_mmap(struct hl_device *hdev, struct vm_area_struct *vma,\n\t\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size)\n{\n\tint rc;\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP |\n\t\t\tVM_DONTCOPY | VM_NORESERVE);\n\n\trc = dma_mmap_coherent(hdev->dev, vma, cpu_addr,\n\t\t\t\t(dma_addr - HOST_PHYS_BASE), size);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"dma_mmap_coherent error %d\", rc);\n\n\treturn rc;\n}\n\nstatic void gaudi_ring_doorbell(struct hl_device *hdev, u32 hw_queue_id, u32 pi)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 db_reg_offset, db_value, dma_qm_offset, q_off, irq_handler_offset;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tbool invalid_queue = false;\n\tint dma_id;\n\n\tswitch (hw_queue_id) {\n\tcase GAUDI_QUEUE_ID_DMA_0_0...GAUDI_QUEUE_ID_DMA_0_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_PCI_DMA_1];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + (hw_queue_id & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_1_0...GAUDI_QUEUE_ID_DMA_1_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_PCI_DMA_2];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + (hw_queue_id & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_2_0...GAUDI_QUEUE_ID_DMA_2_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_1];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_3_0...GAUDI_QUEUE_ID_DMA_3_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_2];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_4_0...GAUDI_QUEUE_ID_DMA_4_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_3];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_5_0...GAUDI_QUEUE_ID_DMA_5_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_4];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_6_0...GAUDI_QUEUE_ID_DMA_6_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_5];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_DMA_7_0...GAUDI_QUEUE_ID_DMA_7_3:\n\t\tdma_id = gaudi_dma_assignment[GAUDI_HBM_DMA_6];\n\t\tdma_qm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tq_off = dma_qm_offset + ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmDMA0_QM_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_CPU_PQ:\n\t\tif (gaudi->hw_cap_initialized & HW_CAP_CPU_Q)\n\t\t\tdb_reg_offset = mmCPU_IF_PF_PQ_PI;\n\t\telse\n\t\t\tinvalid_queue = true;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_0_0:\n\t\tdb_reg_offset = mmMME2_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_0_1:\n\t\tdb_reg_offset = mmMME2_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_0_2:\n\t\tdb_reg_offset = mmMME2_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_0_3:\n\t\tdb_reg_offset = mmMME2_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_1_0:\n\t\tdb_reg_offset = mmMME0_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_1_1:\n\t\tdb_reg_offset = mmMME0_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_1_2:\n\t\tdb_reg_offset = mmMME0_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_MME_1_3:\n\t\tdb_reg_offset = mmMME0_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_0_0:\n\t\tdb_reg_offset = mmTPC0_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_0_1:\n\t\tdb_reg_offset = mmTPC0_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_0_2:\n\t\tdb_reg_offset = mmTPC0_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_0_3:\n\t\tdb_reg_offset = mmTPC0_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_1_0:\n\t\tdb_reg_offset = mmTPC1_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_1_1:\n\t\tdb_reg_offset = mmTPC1_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_1_2:\n\t\tdb_reg_offset = mmTPC1_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_1_3:\n\t\tdb_reg_offset = mmTPC1_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_2_0:\n\t\tdb_reg_offset = mmTPC2_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_2_1:\n\t\tdb_reg_offset = mmTPC2_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_2_2:\n\t\tdb_reg_offset = mmTPC2_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_2_3:\n\t\tdb_reg_offset = mmTPC2_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_3_0:\n\t\tdb_reg_offset = mmTPC3_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_3_1:\n\t\tdb_reg_offset = mmTPC3_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_3_2:\n\t\tdb_reg_offset = mmTPC3_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_3_3:\n\t\tdb_reg_offset = mmTPC3_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_4_0:\n\t\tdb_reg_offset = mmTPC4_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_4_1:\n\t\tdb_reg_offset = mmTPC4_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_4_2:\n\t\tdb_reg_offset = mmTPC4_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_4_3:\n\t\tdb_reg_offset = mmTPC4_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_5_0:\n\t\tdb_reg_offset = mmTPC5_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_5_1:\n\t\tdb_reg_offset = mmTPC5_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_5_2:\n\t\tdb_reg_offset = mmTPC5_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_5_3:\n\t\tdb_reg_offset = mmTPC5_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_6_0:\n\t\tdb_reg_offset = mmTPC6_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_6_1:\n\t\tdb_reg_offset = mmTPC6_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_6_2:\n\t\tdb_reg_offset = mmTPC6_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_6_3:\n\t\tdb_reg_offset = mmTPC6_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_7_0:\n\t\tdb_reg_offset = mmTPC7_QM_PQ_PI_0;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_7_1:\n\t\tdb_reg_offset = mmTPC7_QM_PQ_PI_1;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_7_2:\n\t\tdb_reg_offset = mmTPC7_QM_PQ_PI_2;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_TPC_7_3:\n\t\tdb_reg_offset = mmTPC7_QM_PQ_PI_3;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_0_0...GAUDI_QUEUE_ID_NIC_0_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC0))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC0_QM0_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_1_0...GAUDI_QUEUE_ID_NIC_1_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC1))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC0_QM1_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_2_0...GAUDI_QUEUE_ID_NIC_2_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC2))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC1_QM0_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_3_0...GAUDI_QUEUE_ID_NIC_3_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC3))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC1_QM1_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_4_0...GAUDI_QUEUE_ID_NIC_4_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC4))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC2_QM0_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_5_0...GAUDI_QUEUE_ID_NIC_5_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC5))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC2_QM1_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_6_0...GAUDI_QUEUE_ID_NIC_6_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC6))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC3_QM0_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_7_0...GAUDI_QUEUE_ID_NIC_7_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC7))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC3_QM1_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_8_0...GAUDI_QUEUE_ID_NIC_8_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC8))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC4_QM0_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tcase GAUDI_QUEUE_ID_NIC_9_0...GAUDI_QUEUE_ID_NIC_9_3:\n\t\tif (!(gaudi->hw_cap_initialized & HW_CAP_NIC9))\n\t\t\tinvalid_queue = true;\n\n\t\tq_off = ((hw_queue_id - 1) & 0x3) * 4;\n\t\tdb_reg_offset = mmNIC4_QM1_PQ_PI_0 + q_off;\n\t\tbreak;\n\n\tdefault:\n\t\tinvalid_queue = true;\n\t}\n\n\tif (invalid_queue) {\n\t\t \n\t\tdev_err(hdev->dev, \"h/w queue %d is invalid. Can't set pi\\n\",\n\t\t\thw_queue_id);\n\t\treturn;\n\t}\n\n\tdb_value = pi;\n\n\t \n\tWREG32(db_reg_offset, db_value);\n\n\tif (hw_queue_id == GAUDI_QUEUE_ID_CPU_PQ) {\n\t\t \n\t\tmb();\n\n\t\tirq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\t\tle32_to_cpu(dyn_regs->gic_host_pi_upd_irq);\n\n\t\tWREG32(irq_handler_offset,\n\t\t\tgaudi_irq_map_table[GAUDI_EVENT_PI_UPDATE].cpu_id);\n\t}\n}\n\nstatic void gaudi_pqe_write(struct hl_device *hdev, __le64 *pqe,\n\t\t\t\tstruct hl_bd *bd)\n{\n\t__le64 *pbd = (__le64 *) bd;\n\n\t \n\tpqe[0] = pbd[0];\n\tpqe[1] = pbd[1];\n}\n\nstatic void *gaudi_dma_alloc_coherent(struct hl_device *hdev, size_t size,\n\t\t\t\t\tdma_addr_t *dma_handle, gfp_t flags)\n{\n\tvoid *kernel_addr = dma_alloc_coherent(&hdev->pdev->dev, size,\n\t\t\t\t\t\tdma_handle, flags);\n\n\t \n\tif (kernel_addr)\n\t\t*dma_handle += HOST_PHYS_BASE;\n\n\treturn kernel_addr;\n}\n\nstatic void gaudi_dma_free_coherent(struct hl_device *hdev, size_t size,\n\t\tvoid *cpu_addr, dma_addr_t dma_handle)\n{\n\t \n\tdma_addr_t fixed_dma_handle = dma_handle - HOST_PHYS_BASE;\n\n\tdma_free_coherent(&hdev->pdev->dev, size, cpu_addr, fixed_dma_handle);\n}\n\nstatic int gaudi_scrub_device_dram(struct hl_device *hdev, u64 val)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 cur_addr = prop->dram_user_base_address;\n\tu32 chunk_size, busy;\n\tint rc, dma_id;\n\n\twhile (cur_addr < prop->dram_end_address) {\n\t\tfor (dma_id = 0 ; dma_id < DMA_NUMBER_OF_CHANNELS ; dma_id++) {\n\t\t\tu32 dma_offset = dma_id * DMA_CORE_OFFSET;\n\n\t\t\tchunk_size =\n\t\t\tmin((u64)SZ_2G, prop->dram_end_address - cur_addr);\n\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"Doing HBM scrubbing for 0x%09llx - 0x%09llx\\n\",\n\t\t\t\tcur_addr, cur_addr + chunk_size);\n\n\t\t\tWREG32(mmDMA0_CORE_SRC_BASE_LO + dma_offset,\n\t\t\t\t\tlower_32_bits(val));\n\t\t\tWREG32(mmDMA0_CORE_SRC_BASE_HI + dma_offset,\n\t\t\t\t\tupper_32_bits(val));\n\t\t\tWREG32(mmDMA0_CORE_DST_BASE_LO + dma_offset,\n\t\t\t\t\t\tlower_32_bits(cur_addr));\n\t\t\tWREG32(mmDMA0_CORE_DST_BASE_HI + dma_offset,\n\t\t\t\t\t\tupper_32_bits(cur_addr));\n\t\t\tWREG32(mmDMA0_CORE_DST_TSIZE_0 + dma_offset,\n\t\t\t\t\tchunk_size);\n\t\t\tWREG32(mmDMA0_CORE_COMMIT + dma_offset,\n\t\t\t\t\t((1 << DMA0_CORE_COMMIT_LIN_SHIFT) |\n\t\t\t\t\t(1 << DMA0_CORE_COMMIT_MEM_SET_SHIFT)));\n\n\t\t\tcur_addr += chunk_size;\n\n\t\t\tif (cur_addr == prop->dram_end_address)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tfor (dma_id = 0 ; dma_id < DMA_NUMBER_OF_CHANNELS ; dma_id++) {\n\t\t\tu32 dma_offset = dma_id * DMA_CORE_OFFSET;\n\n\t\t\trc = hl_poll_timeout(\n\t\t\t\thdev,\n\t\t\t\tmmDMA0_CORE_STS0 + dma_offset,\n\t\t\t\tbusy,\n\t\t\t\t((busy & DMA0_CORE_STS0_BUSY_MASK) == 0),\n\t\t\t\t1000,\n\t\t\t\tHBM_SCRUBBING_TIMEOUT_US);\n\n\t\t\tif (rc) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"DMA Timeout during HBM scrubbing of DMA #%d\\n\",\n\t\t\t\t\tdma_id);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi_scrub_device_mem(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 wait_to_idle_time = hdev->pdev ? HBM_SCRUBBING_TIMEOUT_US :\n\t\t\tmin_t(u64, HBM_SCRUBBING_TIMEOUT_US * 10, HL_SIM_MAX_TIMEOUT_US);\n\tu64 addr, size, val = hdev->memory_scrub_val;\n\tktime_t timeout;\n\tint rc = 0;\n\n\tif (!hdev->memory_scrub)\n\t\treturn 0;\n\n\ttimeout = ktime_add_us(ktime_get(), wait_to_idle_time);\n\twhile (!hdev->asic_funcs->is_device_idle(hdev, NULL, 0, NULL)) {\n\t\tif (ktime_compare(ktime_get(), timeout) > 0) {\n\t\t\tdev_err(hdev->dev, \"waiting for idle timeout\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tusleep_range((1000 >> 2) + 1, 1000);\n\t}\n\n\t \n\taddr = prop->sram_user_base_address;\n\tsize = hdev->pldm ? 0x10000 : prop->sram_size - SRAM_USER_BASE_OFFSET;\n\n\tdev_dbg(hdev->dev, \"Scrubbing SRAM: 0x%09llx - 0x%09llx val: 0x%llx\\n\",\n\t\t\taddr, addr + size, val);\n\trc = gaudi_memset_device_memory(hdev, addr, size, val);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to clear SRAM (%d)\\n\", rc);\n\t\treturn rc;\n\t}\n\n\t \n\trc = gaudi_scrub_device_dram(hdev, val);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to clear HBM (%d)\\n\", rc);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic void *gaudi_get_int_queue_base(struct hl_device *hdev,\n\t\t\t\tu32 queue_id, dma_addr_t *dma_handle,\n\t\t\t\tu16 *queue_len)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct gaudi_internal_qman_info *q;\n\n\tif (queue_id >= GAUDI_QUEUE_ID_SIZE ||\n\t\t\tgaudi_queue_type[queue_id] != QUEUE_TYPE_INT) {\n\t\tdev_err(hdev->dev, \"Got invalid queue id %d\\n\", queue_id);\n\t\treturn NULL;\n\t}\n\n\tq = &gaudi->internal_qmans[queue_id];\n\t*dma_handle = q->pq_dma_addr;\n\t*queue_len = q->pq_size / QMAN_PQ_ENTRY_SIZE;\n\n\treturn q->pq_kernel_addr;\n}\n\nstatic int gaudi_send_cpu_message(struct hl_device *hdev, u32 *msg,\n\t\t\t\tu16 len, u32 timeout, u64 *result)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q)) {\n\t\tif (result)\n\t\t\t*result = 0;\n\t\treturn 0;\n\t}\n\n\tif (!timeout)\n\t\ttimeout = GAUDI_MSG_TO_CPU_TIMEOUT_USEC;\n\n\treturn hl_fw_send_cpu_message(hdev, GAUDI_QUEUE_ID_CPU_PQ, msg, len,\n\t\t\t\t\t\ttimeout, result);\n}\n\nstatic int gaudi_test_queue(struct hl_device *hdev, u32 hw_queue_id)\n{\n\tstruct packet_msg_prot *fence_pkt;\n\tdma_addr_t pkt_dma_addr;\n\tu32 fence_val, tmp, timeout_usec;\n\tdma_addr_t fence_dma_addr;\n\tu32 *fence_ptr;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI_PLDM_TEST_QUEUE_WAIT_USEC;\n\telse\n\t\ttimeout_usec = GAUDI_TEST_QUEUE_WAIT_USEC;\n\n\tfence_val = GAUDI_QMAN0_FENCE_VAL;\n\n\tfence_ptr = hl_asic_dma_pool_zalloc(hdev, 4, GFP_KERNEL, &fence_dma_addr);\n\tif (!fence_ptr) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate memory for H/W queue %d testing\\n\",\n\t\t\thw_queue_id);\n\t\treturn -ENOMEM;\n\t}\n\n\t*fence_ptr = 0;\n\n\tfence_pkt = hl_asic_dma_pool_zalloc(hdev, sizeof(struct packet_msg_prot), GFP_KERNEL,\n\t\t\t\t\t\t&pkt_dma_addr);\n\tif (!fence_pkt) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate packet for H/W queue %d testing\\n\",\n\t\t\thw_queue_id);\n\t\trc = -ENOMEM;\n\t\tgoto free_fence_ptr;\n\t}\n\n\ttmp = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_PROT);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 1);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tfence_pkt->ctl = cpu_to_le32(tmp);\n\tfence_pkt->value = cpu_to_le32(fence_val);\n\tfence_pkt->addr = cpu_to_le64(fence_dma_addr);\n\n\trc = hl_hw_queue_send_cb_no_cmpl(hdev, hw_queue_id,\n\t\t\t\t\tsizeof(struct packet_msg_prot),\n\t\t\t\t\tpkt_dma_addr);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to send fence packet to H/W queue %d\\n\",\n\t\t\thw_queue_id);\n\t\tgoto free_pkt;\n\t}\n\n\trc = hl_poll_timeout_memory(hdev, fence_ptr, tmp, (tmp == fence_val),\n\t\t\t\t\t1000, timeout_usec, true);\n\n\thl_hw_queue_inc_ci_kernel(hdev, hw_queue_id);\n\n\tif (rc == -ETIMEDOUT) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"H/W queue %d test failed (scratch(0x%08llX) == 0x%08X)\\n\",\n\t\t\thw_queue_id, (unsigned long long) fence_dma_addr, tmp);\n\t\trc = -EIO;\n\t}\n\nfree_pkt:\n\thl_asic_dma_pool_free(hdev, (void *) fence_pkt, pkt_dma_addr);\nfree_fence_ptr:\n\thl_asic_dma_pool_free(hdev, (void *) fence_ptr, fence_dma_addr);\n\treturn rc;\n}\n\nstatic int gaudi_test_cpu_queue(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\t \n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_test_cpu_queue(hdev);\n}\n\nstatic int gaudi_test_queues(struct hl_device *hdev)\n{\n\tint i, rc, ret_val = 0;\n\n\tfor (i = 0 ; i < hdev->asic_prop.max_queues ; i++) {\n\t\tif (hdev->asic_prop.hw_queues_props[i].type == QUEUE_TYPE_EXT) {\n\t\t\trc = gaudi_test_queue(hdev, i);\n\t\t\tif (rc)\n\t\t\t\tret_val = -EINVAL;\n\t\t}\n\t}\n\n\trc = gaudi_test_cpu_queue(hdev);\n\tif (rc)\n\t\tret_val = -EINVAL;\n\n\treturn ret_val;\n}\n\nstatic void *gaudi_dma_pool_zalloc(struct hl_device *hdev, size_t size,\n\t\tgfp_t mem_flags, dma_addr_t *dma_handle)\n{\n\tvoid *kernel_addr;\n\n\tif (size > GAUDI_DMA_POOL_BLK_SIZE)\n\t\treturn NULL;\n\n\tkernel_addr = dma_pool_zalloc(hdev->dma_pool, mem_flags, dma_handle);\n\n\t \n\tif (kernel_addr)\n\t\t*dma_handle += HOST_PHYS_BASE;\n\n\treturn kernel_addr;\n}\n\nstatic void gaudi_dma_pool_free(struct hl_device *hdev, void *vaddr,\n\t\t\tdma_addr_t dma_addr)\n{\n\t \n\tdma_addr_t fixed_dma_addr = dma_addr - HOST_PHYS_BASE;\n\n\tdma_pool_free(hdev->dma_pool, vaddr, fixed_dma_addr);\n}\n\nstatic void *gaudi_cpu_accessible_dma_pool_alloc(struct hl_device *hdev,\n\t\t\t\t\tsize_t size, dma_addr_t *dma_handle)\n{\n\treturn hl_fw_cpu_accessible_dma_pool_alloc(hdev, size, dma_handle);\n}\n\nstatic void gaudi_cpu_accessible_dma_pool_free(struct hl_device *hdev,\n\t\t\t\t\t\tsize_t size, void *vaddr)\n{\n\thl_fw_cpu_accessible_dma_pool_free(hdev, size, vaddr);\n}\n\nstatic u32 gaudi_get_dma_desc_list_size(struct hl_device *hdev, struct sg_table *sgt)\n{\n\tstruct scatterlist *sg, *sg_next_iter;\n\tu32 count, dma_desc_cnt;\n\tu64 len, len_next;\n\tdma_addr_t addr, addr_next;\n\n\tdma_desc_cnt = 0;\n\n\tfor_each_sgtable_dma_sg(sgt, sg, count) {\n\t\tlen = sg_dma_len(sg);\n\t\taddr = sg_dma_address(sg);\n\n\t\tif (len == 0)\n\t\t\tbreak;\n\n\t\twhile ((count + 1) < sgt->nents) {\n\t\t\tsg_next_iter = sg_next(sg);\n\t\t\tlen_next = sg_dma_len(sg_next_iter);\n\t\t\taddr_next = sg_dma_address(sg_next_iter);\n\n\t\t\tif (len_next == 0)\n\t\t\t\tbreak;\n\n\t\t\tif ((addr + len == addr_next) &&\n\t\t\t\t(len + len_next <= DMA_MAX_TRANSFER_SIZE)) {\n\t\t\t\tlen += len_next;\n\t\t\t\tcount++;\n\t\t\t\tsg = sg_next_iter;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tdma_desc_cnt++;\n\t}\n\n\treturn dma_desc_cnt * sizeof(struct packet_lin_dma);\n}\n\nstatic int gaudi_pin_memory_before_cs(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_parser *parser,\n\t\t\t\tstruct packet_lin_dma *user_dma_pkt,\n\t\t\t\tu64 addr, enum dma_data_direction dir)\n{\n\tstruct hl_userptr *userptr;\n\tint rc;\n\n\tif (hl_userptr_is_pinned(hdev, addr, le32_to_cpu(user_dma_pkt->tsize),\n\t\t\tparser->job_userptr_list, &userptr))\n\t\tgoto already_pinned;\n\n\tuserptr = kzalloc(sizeof(*userptr), GFP_KERNEL);\n\tif (!userptr)\n\t\treturn -ENOMEM;\n\n\trc = hl_pin_host_memory(hdev, addr, le32_to_cpu(user_dma_pkt->tsize),\n\t\t\t\tuserptr);\n\tif (rc)\n\t\tgoto free_userptr;\n\n\tlist_add_tail(&userptr->job_node, parser->job_userptr_list);\n\n\trc = hdev->asic_funcs->asic_dma_map_sgtable(hdev, userptr->sgt, dir);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to map sgt with DMA region\\n\");\n\t\tgoto unpin_memory;\n\t}\n\n\tuserptr->dma_mapped = true;\n\tuserptr->dir = dir;\n\nalready_pinned:\n\tparser->patched_cb_size +=\n\t\t\tgaudi_get_dma_desc_list_size(hdev, userptr->sgt);\n\n\treturn 0;\n\nunpin_memory:\n\tlist_del(&userptr->job_node);\n\thl_unpin_host_memory(hdev, userptr);\nfree_userptr:\n\tkfree(userptr);\n\treturn rc;\n}\n\nstatic int gaudi_validate_dma_pkt_host(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_parser *parser,\n\t\t\t\tstruct packet_lin_dma *user_dma_pkt,\n\t\t\t\tbool src_in_host)\n{\n\tenum dma_data_direction dir;\n\tbool skip_host_mem_pin = false, user_memset;\n\tu64 addr;\n\tint rc = 0;\n\n\tuser_memset = (le32_to_cpu(user_dma_pkt->ctl) &\n\t\t\tGAUDI_PKT_LIN_DMA_CTL_MEMSET_MASK) >>\n\t\t\tGAUDI_PKT_LIN_DMA_CTL_MEMSET_SHIFT;\n\n\tif (src_in_host) {\n\t\tif (user_memset)\n\t\t\tskip_host_mem_pin = true;\n\n\t\tdev_dbg(hdev->dev, \"DMA direction is HOST --> DEVICE\\n\");\n\t\tdir = DMA_TO_DEVICE;\n\t\taddr = le64_to_cpu(user_dma_pkt->src_addr);\n\t} else {\n\t\tdev_dbg(hdev->dev, \"DMA direction is DEVICE --> HOST\\n\");\n\t\tdir = DMA_FROM_DEVICE;\n\t\taddr = (le64_to_cpu(user_dma_pkt->dst_addr) &\n\t\t\t\tGAUDI_PKT_LIN_DMA_DST_ADDR_MASK) >>\n\t\t\t\tGAUDI_PKT_LIN_DMA_DST_ADDR_SHIFT;\n\t}\n\n\tif (skip_host_mem_pin)\n\t\tparser->patched_cb_size += sizeof(*user_dma_pkt);\n\telse\n\t\trc = gaudi_pin_memory_before_cs(hdev, parser, user_dma_pkt,\n\t\t\t\t\t\taddr, dir);\n\n\treturn rc;\n}\n\nstatic int gaudi_validate_dma_pkt_no_mmu(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_parser *parser,\n\t\t\t\tstruct packet_lin_dma *user_dma_pkt)\n{\n\tbool src_in_host = false;\n\tu64 dst_addr = (le64_to_cpu(user_dma_pkt->dst_addr) &\n\t\t\tGAUDI_PKT_LIN_DMA_DST_ADDR_MASK) >>\n\t\t\tGAUDI_PKT_LIN_DMA_DST_ADDR_SHIFT;\n\n\tdev_dbg(hdev->dev, \"DMA packet details:\\n\");\n\tdev_dbg(hdev->dev, \"source == 0x%llx\\n\",\n\t\t\t\tle64_to_cpu(user_dma_pkt->src_addr));\n\tdev_dbg(hdev->dev, \"destination == 0x%llx\\n\", dst_addr);\n\tdev_dbg(hdev->dev, \"size == %u\\n\", le32_to_cpu(user_dma_pkt->tsize));\n\n\t \n\tif (!le32_to_cpu(user_dma_pkt->tsize)) {\n\t\tparser->patched_cb_size += sizeof(*user_dma_pkt);\n\t\treturn 0;\n\t}\n\n\tif (parser->hw_queue_id <= GAUDI_QUEUE_ID_DMA_0_3)\n\t\tsrc_in_host = true;\n\n\treturn gaudi_validate_dma_pkt_host(hdev, parser, user_dma_pkt,\n\t\t\t\t\t\tsrc_in_host);\n}\n\nstatic int gaudi_validate_load_and_exe_pkt(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_parser *parser,\n\t\t\t\t\tstruct packet_load_and_exe *user_pkt)\n{\n\tu32 cfg;\n\n\tcfg = le32_to_cpu(user_pkt->cfg);\n\n\tif (cfg & GAUDI_PKT_LOAD_AND_EXE_CFG_DST_MASK) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"User not allowed to use Load and Execute\\n\");\n\t\treturn -EPERM;\n\t}\n\n\tparser->patched_cb_size += sizeof(struct packet_load_and_exe);\n\n\treturn 0;\n}\n\nstatic int gaudi_validate_cb(struct hl_device *hdev,\n\t\t\tstruct hl_cs_parser *parser, bool is_mmu)\n{\n\tu32 cb_parsed_length = 0;\n\tint rc = 0;\n\n\tparser->patched_cb_size = 0;\n\n\t \n\twhile (cb_parsed_length < parser->user_cb_size) {\n\t\tenum packet_id pkt_id;\n\t\tu16 pkt_size;\n\t\tstruct gaudi_packet *user_pkt;\n\n\t\tuser_pkt = parser->user_cb->kernel_address + cb_parsed_length;\n\n\t\tpkt_id = (enum packet_id) (\n\t\t\t\t(le64_to_cpu(user_pkt->header) &\n\t\t\t\tPACKET_HEADER_PACKET_ID_MASK) >>\n\t\t\t\t\tPACKET_HEADER_PACKET_ID_SHIFT);\n\n\t\tif (!validate_packet_id(pkt_id)) {\n\t\t\tdev_err(hdev->dev, \"Invalid packet id %u\\n\", pkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tpkt_size = gaudi_packet_sizes[pkt_id];\n\t\tcb_parsed_length += pkt_size;\n\t\tif (cb_parsed_length > parser->user_cb_size) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"packet 0x%x is out of CB boundary\\n\", pkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (pkt_id) {\n\t\tcase PACKET_MSG_PROT:\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"User not allowed to use MSG_PROT\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_CP_DMA:\n\t\t\tdev_err(hdev->dev, \"User not allowed to use CP_DMA\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_STOP:\n\t\t\tdev_err(hdev->dev, \"User not allowed to use STOP\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_WREG_BULK:\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"User not allowed to use WREG_BULK\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_LOAD_AND_EXE:\n\t\t\trc = gaudi_validate_load_and_exe_pkt(hdev, parser,\n\t\t\t\t(struct packet_load_and_exe *) user_pkt);\n\t\t\tbreak;\n\n\t\tcase PACKET_LIN_DMA:\n\t\t\tparser->contains_dma_pkt = true;\n\t\t\tif (is_mmu)\n\t\t\t\tparser->patched_cb_size += pkt_size;\n\t\t\telse\n\t\t\t\trc = gaudi_validate_dma_pkt_no_mmu(hdev, parser,\n\t\t\t\t\t(struct packet_lin_dma *) user_pkt);\n\t\t\tbreak;\n\n\t\tcase PACKET_WREG_32:\n\t\tcase PACKET_MSG_LONG:\n\t\tcase PACKET_MSG_SHORT:\n\t\tcase PACKET_REPEAT:\n\t\tcase PACKET_FENCE:\n\t\tcase PACKET_NOP:\n\t\tcase PACKET_ARB_POINT:\n\t\t\tparser->patched_cb_size += pkt_size;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tdev_err(hdev->dev, \"Invalid packet header 0x%x\\n\",\n\t\t\t\tpkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (parser->completion)\n\t\tparser->patched_cb_size += gaudi_get_patched_cb_extra_size(\n\t\t\tparser->patched_cb_size);\n\n\treturn rc;\n}\n\nstatic int gaudi_patch_dma_packet(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_parser *parser,\n\t\t\t\tstruct packet_lin_dma *user_dma_pkt,\n\t\t\t\tstruct packet_lin_dma *new_dma_pkt,\n\t\t\t\tu32 *new_dma_pkt_size)\n{\n\tstruct hl_userptr *userptr;\n\tstruct scatterlist *sg, *sg_next_iter;\n\tu32 count, dma_desc_cnt, user_wrcomp_en_mask, ctl;\n\tu64 len, len_next;\n\tdma_addr_t dma_addr, dma_addr_next;\n\tu64 device_memory_addr, addr;\n\tenum dma_data_direction dir;\n\tstruct sg_table *sgt;\n\tbool src_in_host = false;\n\tbool skip_host_mem_pin = false;\n\tbool user_memset;\n\n\tctl = le32_to_cpu(user_dma_pkt->ctl);\n\n\tif (parser->hw_queue_id <= GAUDI_QUEUE_ID_DMA_0_3)\n\t\tsrc_in_host = true;\n\n\tuser_memset = (ctl & GAUDI_PKT_LIN_DMA_CTL_MEMSET_MASK) >>\n\t\t\tGAUDI_PKT_LIN_DMA_CTL_MEMSET_SHIFT;\n\n\tif (src_in_host) {\n\t\taddr = le64_to_cpu(user_dma_pkt->src_addr);\n\t\tdevice_memory_addr = le64_to_cpu(user_dma_pkt->dst_addr);\n\t\tdir = DMA_TO_DEVICE;\n\t\tif (user_memset)\n\t\t\tskip_host_mem_pin = true;\n\t} else {\n\t\taddr = le64_to_cpu(user_dma_pkt->dst_addr);\n\t\tdevice_memory_addr = le64_to_cpu(user_dma_pkt->src_addr);\n\t\tdir = DMA_FROM_DEVICE;\n\t}\n\n\tif ((!skip_host_mem_pin) &&\n\t\t(!hl_userptr_is_pinned(hdev, addr,\n\t\t\t\t\tle32_to_cpu(user_dma_pkt->tsize),\n\t\t\t\t\tparser->job_userptr_list, &userptr))) {\n\t\tdev_err(hdev->dev, \"Userptr 0x%llx + 0x%x NOT mapped\\n\",\n\t\t\t\taddr, user_dma_pkt->tsize);\n\t\treturn -EFAULT;\n\t}\n\n\tif ((user_memset) && (dir == DMA_TO_DEVICE)) {\n\t\tmemcpy(new_dma_pkt, user_dma_pkt, sizeof(*user_dma_pkt));\n\t\t*new_dma_pkt_size = sizeof(*user_dma_pkt);\n\t\treturn 0;\n\t}\n\n\tuser_wrcomp_en_mask = ctl & GAUDI_PKT_LIN_DMA_CTL_WRCOMP_EN_MASK;\n\n\tsgt = userptr->sgt;\n\tdma_desc_cnt = 0;\n\n\tfor_each_sgtable_dma_sg(sgt, sg, count) {\n\t\tlen = sg_dma_len(sg);\n\t\tdma_addr = sg_dma_address(sg);\n\n\t\tif (len == 0)\n\t\t\tbreak;\n\n\t\twhile ((count + 1) < sgt->nents) {\n\t\t\tsg_next_iter = sg_next(sg);\n\t\t\tlen_next = sg_dma_len(sg_next_iter);\n\t\t\tdma_addr_next = sg_dma_address(sg_next_iter);\n\n\t\t\tif (len_next == 0)\n\t\t\t\tbreak;\n\n\t\t\tif ((dma_addr + len == dma_addr_next) &&\n\t\t\t\t(len + len_next <= DMA_MAX_TRANSFER_SIZE)) {\n\t\t\t\tlen += len_next;\n\t\t\t\tcount++;\n\t\t\t\tsg = sg_next_iter;\n\t\t\t} else {\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tctl = le32_to_cpu(user_dma_pkt->ctl);\n\t\tif (likely(dma_desc_cnt))\n\t\t\tctl &= ~GAUDI_PKT_CTL_EB_MASK;\n\t\tctl &= ~GAUDI_PKT_LIN_DMA_CTL_WRCOMP_EN_MASK;\n\t\tnew_dma_pkt->ctl = cpu_to_le32(ctl);\n\t\tnew_dma_pkt->tsize = cpu_to_le32(len);\n\n\t\tif (dir == DMA_TO_DEVICE) {\n\t\t\tnew_dma_pkt->src_addr = cpu_to_le64(dma_addr);\n\t\t\tnew_dma_pkt->dst_addr = cpu_to_le64(device_memory_addr);\n\t\t} else {\n\t\t\tnew_dma_pkt->src_addr = cpu_to_le64(device_memory_addr);\n\t\t\tnew_dma_pkt->dst_addr = cpu_to_le64(dma_addr);\n\t\t}\n\n\t\tif (!user_memset)\n\t\t\tdevice_memory_addr += len;\n\t\tdma_desc_cnt++;\n\t\tnew_dma_pkt++;\n\t}\n\n\tif (!dma_desc_cnt) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Error of 0 SG entries when patching DMA packet\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\t \n\tnew_dma_pkt--;\n\tnew_dma_pkt->ctl |= cpu_to_le32(user_wrcomp_en_mask);\n\n\t*new_dma_pkt_size = dma_desc_cnt * sizeof(struct packet_lin_dma);\n\n\treturn 0;\n}\n\nstatic int gaudi_patch_cb(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_parser *parser)\n{\n\tu32 cb_parsed_length = 0;\n\tu32 cb_patched_cur_length = 0;\n\tint rc = 0;\n\n\t \n\twhile (cb_parsed_length < parser->user_cb_size) {\n\t\tenum packet_id pkt_id;\n\t\tu16 pkt_size;\n\t\tu32 new_pkt_size = 0;\n\t\tstruct gaudi_packet *user_pkt, *kernel_pkt;\n\n\t\tuser_pkt = parser->user_cb->kernel_address + cb_parsed_length;\n\t\tkernel_pkt = parser->patched_cb->kernel_address +\n\t\t\t\t\tcb_patched_cur_length;\n\n\t\tpkt_id = (enum packet_id) (\n\t\t\t\t(le64_to_cpu(user_pkt->header) &\n\t\t\t\tPACKET_HEADER_PACKET_ID_MASK) >>\n\t\t\t\t\tPACKET_HEADER_PACKET_ID_SHIFT);\n\n\t\tif (!validate_packet_id(pkt_id)) {\n\t\t\tdev_err(hdev->dev, \"Invalid packet id %u\\n\", pkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tpkt_size = gaudi_packet_sizes[pkt_id];\n\t\tcb_parsed_length += pkt_size;\n\t\tif (cb_parsed_length > parser->user_cb_size) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"packet 0x%x is out of CB boundary\\n\", pkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (pkt_id) {\n\t\tcase PACKET_LIN_DMA:\n\t\t\trc = gaudi_patch_dma_packet(hdev, parser,\n\t\t\t\t\t(struct packet_lin_dma *) user_pkt,\n\t\t\t\t\t(struct packet_lin_dma *) kernel_pkt,\n\t\t\t\t\t&new_pkt_size);\n\t\t\tcb_patched_cur_length += new_pkt_size;\n\t\t\tbreak;\n\n\t\tcase PACKET_MSG_PROT:\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"User not allowed to use MSG_PROT\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_CP_DMA:\n\t\t\tdev_err(hdev->dev, \"User not allowed to use CP_DMA\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_STOP:\n\t\t\tdev_err(hdev->dev, \"User not allowed to use STOP\\n\");\n\t\t\trc = -EPERM;\n\t\t\tbreak;\n\n\t\tcase PACKET_WREG_32:\n\t\tcase PACKET_WREG_BULK:\n\t\tcase PACKET_MSG_LONG:\n\t\tcase PACKET_MSG_SHORT:\n\t\tcase PACKET_REPEAT:\n\t\tcase PACKET_FENCE:\n\t\tcase PACKET_NOP:\n\t\tcase PACKET_ARB_POINT:\n\t\tcase PACKET_LOAD_AND_EXE:\n\t\t\tmemcpy(kernel_pkt, user_pkt, pkt_size);\n\t\t\tcb_patched_cur_length += pkt_size;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tdev_err(hdev->dev, \"Invalid packet header 0x%x\\n\",\n\t\t\t\tpkt_id);\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rc)\n\t\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi_parse_cb_mmu(struct hl_device *hdev,\n\t\tstruct hl_cs_parser *parser)\n{\n\tu64 handle;\n\tu32 patched_cb_size;\n\tstruct hl_cb *user_cb;\n\tint rc;\n\n\t \n\tif (parser->completion)\n\t\tparser->patched_cb_size = parser->user_cb_size +\n\t\t\t\tgaudi_get_patched_cb_extra_size(parser->user_cb_size);\n\telse\n\t\tparser->patched_cb_size = parser->user_cb_size;\n\n\trc = hl_cb_create(hdev, &hdev->kernel_mem_mgr, hdev->kernel_ctx,\n\t\t\t\tparser->patched_cb_size, false, false,\n\t\t\t\t&handle);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate patched CB for DMA CS %d\\n\",\n\t\t\trc);\n\t\treturn rc;\n\t}\n\n\tparser->patched_cb = hl_cb_get(&hdev->kernel_mem_mgr, handle);\n\t \n\tif (!parser->patched_cb) {\n\t\tdev_crit(hdev->dev, \"DMA CB handle invalid 0x%llx\\n\", handle);\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\tmemcpy(parser->patched_cb->kernel_address,\n\t\tparser->user_cb->kernel_address,\n\t\tparser->user_cb_size);\n\n\tpatched_cb_size = parser->patched_cb_size;\n\n\t \n\tuser_cb = parser->user_cb;\n\tparser->user_cb = parser->patched_cb;\n\trc = gaudi_validate_cb(hdev, parser, true);\n\tparser->user_cb = user_cb;\n\n\tif (rc) {\n\t\thl_cb_put(parser->patched_cb);\n\t\tgoto out;\n\t}\n\n\tif (patched_cb_size != parser->patched_cb_size) {\n\t\tdev_err(hdev->dev, \"user CB size mismatch\\n\");\n\t\thl_cb_put(parser->patched_cb);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\nout:\n\t \n\thl_cb_destroy(&hdev->kernel_mem_mgr, handle);\n\n\treturn rc;\n}\n\nstatic int gaudi_parse_cb_no_mmu(struct hl_device *hdev,\n\t\tstruct hl_cs_parser *parser)\n{\n\tu64 handle;\n\tint rc;\n\n\trc = gaudi_validate_cb(hdev, parser, false);\n\n\tif (rc)\n\t\tgoto free_userptr;\n\n\trc = hl_cb_create(hdev, &hdev->kernel_mem_mgr, hdev->kernel_ctx,\n\t\t\t\tparser->patched_cb_size, false, false,\n\t\t\t\t&handle);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate patched CB for DMA CS %d\\n\", rc);\n\t\tgoto free_userptr;\n\t}\n\n\tparser->patched_cb = hl_cb_get(&hdev->kernel_mem_mgr, handle);\n\t \n\tif (!parser->patched_cb) {\n\t\tdev_crit(hdev->dev, \"DMA CB handle invalid 0x%llx\\n\", handle);\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\trc = gaudi_patch_cb(hdev, parser);\n\n\tif (rc)\n\t\thl_cb_put(parser->patched_cb);\n\nout:\n\t \n\thl_cb_destroy(&hdev->kernel_mem_mgr, handle);\n\nfree_userptr:\n\tif (rc)\n\t\thl_userptr_delete_list(hdev, parser->job_userptr_list);\n\treturn rc;\n}\n\nstatic int gaudi_parse_cb_no_ext_queue(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_parser *parser)\n{\n\tstruct asic_fixed_properties *asic_prop = &hdev->asic_prop;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 nic_queue_offset, nic_mask_q_id;\n\n\tif ((parser->hw_queue_id >= GAUDI_QUEUE_ID_NIC_0_0) &&\n\t\t\t(parser->hw_queue_id <= GAUDI_QUEUE_ID_NIC_9_3)) {\n\t\tnic_queue_offset = parser->hw_queue_id - GAUDI_QUEUE_ID_NIC_0_0;\n\t\tnic_mask_q_id = 1 << (HW_CAP_NIC_SHIFT + (nic_queue_offset >> 2));\n\n\t\tif (!(gaudi->hw_cap_initialized & nic_mask_q_id)) {\n\t\t\tdev_err(hdev->dev, \"h/w queue %d is disabled\\n\", parser->hw_queue_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->sram_user_base_address,\n\t\t\t\t\tasic_prop->sram_end_address))\n\t\treturn 0;\n\n\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->dram_user_base_address,\n\t\t\t\t\tasic_prop->dram_end_address))\n\t\treturn 0;\n\n\t \n\tif (hl_mem_area_inside_range((u64) (uintptr_t) parser->user_cb,\n\t\t\t\t\tparser->user_cb_size,\n\t\t\t\t\tasic_prop->pmmu.start_addr,\n\t\t\t\t\tasic_prop->pmmu.end_addr))\n\t\treturn 0;\n\n\tdev_err(hdev->dev,\n\t\t\"CB address 0x%px + 0x%x for internal QMAN is not valid\\n\",\n\t\tparser->user_cb, parser->user_cb_size);\n\n\treturn -EFAULT;\n}\n\nstatic int gaudi_cs_parser(struct hl_device *hdev, struct hl_cs_parser *parser)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (parser->queue_type == QUEUE_TYPE_INT)\n\t\treturn gaudi_parse_cb_no_ext_queue(hdev, parser);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_MMU)\n\t\treturn gaudi_parse_cb_mmu(hdev, parser);\n\telse\n\t\treturn gaudi_parse_cb_no_mmu(hdev, parser);\n}\n\nstatic void gaudi_add_end_of_cb_packets(struct hl_device *hdev, void *kernel_address,\n\t\t\t\tu32 len, u32 original_len, u64 cq_addr, u32 cq_val,\n\t\t\t\tu32 msi_vec, bool eb)\n{\n\tstruct packet_msg_prot *cq_pkt;\n\tstruct packet_nop *cq_padding;\n\tu64 msi_addr;\n\tu32 tmp;\n\n\tcq_padding = kernel_address + original_len;\n\tcq_pkt = kernel_address + len - (sizeof(struct packet_msg_prot) * 2);\n\n\twhile ((void *)cq_padding < (void *)cq_pkt) {\n\t\tcq_padding->ctl = cpu_to_le32(FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_NOP));\n\t\tcq_padding++;\n\t}\n\n\ttmp = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_PROT);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tif (eb)\n\t\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 1);\n\n\tcq_pkt->ctl = cpu_to_le32(tmp);\n\tcq_pkt->value = cpu_to_le32(cq_val);\n\tcq_pkt->addr = cpu_to_le64(cq_addr);\n\n\tcq_pkt++;\n\n\ttmp = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_PROT);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\tcq_pkt->ctl = cpu_to_le32(tmp);\n\tcq_pkt->value = cpu_to_le32(1);\n\tmsi_addr = hdev->pdev ? mmPCIE_CORE_MSI_REQ : mmPCIE_MSI_INTR_0 + msi_vec * 4;\n\tcq_pkt->addr = cpu_to_le64(CFG_BASE + msi_addr);\n}\n\nstatic void gaudi_update_eq_ci(struct hl_device *hdev, u32 val)\n{\n\tWREG32(mmCPU_IF_EQ_RD_OFFS, val);\n}\n\nstatic int gaudi_memset_device_memory(struct hl_device *hdev, u64 addr,\n\t\t\t\t\tu32 size, u64 val)\n{\n\tstruct packet_lin_dma *lin_dma_pkt;\n\tstruct hl_cs_job *job;\n\tu32 cb_size, ctl, err_cause;\n\tstruct hl_cb *cb;\n\tint rc;\n\n\tcb = hl_cb_kernel_create(hdev, PAGE_SIZE, false);\n\tif (!cb)\n\t\treturn -EFAULT;\n\n\tlin_dma_pkt = cb->kernel_address;\n\tmemset(lin_dma_pkt, 0, sizeof(*lin_dma_pkt));\n\tcb_size = sizeof(*lin_dma_pkt);\n\n\tctl = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_LIN_DMA);\n\tctl |= FIELD_PREP(GAUDI_PKT_LIN_DMA_CTL_MEMSET_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_LIN_DMA_CTL_LIN_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\n\tlin_dma_pkt->ctl = cpu_to_le32(ctl);\n\tlin_dma_pkt->src_addr = cpu_to_le64(val);\n\tlin_dma_pkt->dst_addr |= cpu_to_le64(addr);\n\tlin_dma_pkt->tsize = cpu_to_le32(size);\n\n\tjob = hl_cs_allocate_job(hdev, QUEUE_TYPE_EXT, true);\n\tif (!job) {\n\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto release_cb;\n\t}\n\n\t \n\terr_cause = RREG32(mmDMA0_CORE_ERR_CAUSE);\n\tif (err_cause && !hdev->init_done) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Clearing DMA0 engine from errors (cause 0x%x)\\n\",\n\t\t\terr_cause);\n\t\tWREG32(mmDMA0_CORE_ERR_CAUSE, err_cause);\n\t}\n\n\tjob->id = 0;\n\tjob->user_cb = cb;\n\tatomic_inc(&job->user_cb->cs_cnt);\n\tjob->user_cb_size = cb_size;\n\tjob->hw_queue_id = GAUDI_QUEUE_ID_DMA_0_0;\n\tjob->patched_cb = job->user_cb;\n\tjob->job_cb_size = job->user_cb_size + sizeof(struct packet_msg_prot);\n\n\thl_debugfs_add_job(hdev, job);\n\n\trc = gaudi_send_job_on_qman0(hdev, job);\n\thl_debugfs_remove_job(hdev, job);\n\tkfree(job);\n\tatomic_dec(&cb->cs_cnt);\n\n\t \n\terr_cause = RREG32(mmDMA0_CORE_ERR_CAUSE);\n\tif (err_cause) {\n\t\tdev_err(hdev->dev, \"DMA Failed, cause 0x%x\\n\", err_cause);\n\t\trc = -EIO;\n\t\tif (!hdev->init_done) {\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"Clearing DMA0 engine from errors (cause 0x%x)\\n\",\n\t\t\t\terr_cause);\n\t\t\tWREG32(mmDMA0_CORE_ERR_CAUSE, err_cause);\n\t\t}\n\t}\n\nrelease_cb:\n\thl_cb_put(cb);\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb->buf->handle);\n\n\treturn rc;\n}\n\nstatic int gaudi_memset_registers(struct hl_device *hdev, u64 reg_base,\n\t\t\t\t\tu32 num_regs, u32 val)\n{\n\tstruct packet_msg_long *pkt;\n\tstruct hl_cs_job *job;\n\tu32 cb_size, ctl;\n\tstruct hl_cb *cb;\n\tint i, rc;\n\n\tcb_size = (sizeof(*pkt) * num_regs) + sizeof(struct packet_msg_prot);\n\n\tif (cb_size > SZ_2M) {\n\t\tdev_err(hdev->dev, \"CB size must be smaller than %uMB\", SZ_2M);\n\t\treturn -ENOMEM;\n\t}\n\n\tcb = hl_cb_kernel_create(hdev, cb_size, false);\n\tif (!cb)\n\t\treturn -EFAULT;\n\n\tpkt = cb->kernel_address;\n\n\tctl = FIELD_PREP(GAUDI_PKT_LONG_CTL_OP_MASK, 0);  \n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_LONG);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tfor (i = 0; i < num_regs ; i++, pkt++) {\n\t\tpkt->ctl = cpu_to_le32(ctl);\n\t\tpkt->value = cpu_to_le32(val);\n\t\tpkt->addr = cpu_to_le64(reg_base + (i * 4));\n\t}\n\n\tjob = hl_cs_allocate_job(hdev, QUEUE_TYPE_EXT, true);\n\tif (!job) {\n\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto release_cb;\n\t}\n\n\tjob->id = 0;\n\tjob->user_cb = cb;\n\tatomic_inc(&job->user_cb->cs_cnt);\n\tjob->user_cb_size = cb_size;\n\tjob->hw_queue_id = GAUDI_QUEUE_ID_DMA_0_0;\n\tjob->patched_cb = job->user_cb;\n\tjob->job_cb_size = cb_size;\n\n\thl_debugfs_add_job(hdev, job);\n\n\trc = gaudi_send_job_on_qman0(hdev, job);\n\thl_debugfs_remove_job(hdev, job);\n\tkfree(job);\n\tatomic_dec(&cb->cs_cnt);\n\nrelease_cb:\n\thl_cb_put(cb);\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb->buf->handle);\n\n\treturn rc;\n}\n\nstatic int gaudi_restore_sm_registers(struct hl_device *hdev)\n{\n\tu64 base_addr;\n\tu32 num_regs;\n\tint rc;\n\n\tbase_addr = CFG_BASE + mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0;\n\tnum_regs = NUM_OF_SOB_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_E_S_SYNC_MNGR_OBJS_SOB_OBJ_0;\n\tnum_regs = NUM_OF_SOB_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_W_N_SYNC_MNGR_OBJS_SOB_OBJ_0;\n\tnum_regs = NUM_OF_SOB_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_MON_STATUS_0;\n\tnum_regs = NUM_OF_MONITORS_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_E_S_SYNC_MNGR_OBJS_MON_STATUS_0;\n\tnum_regs = NUM_OF_MONITORS_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_W_N_SYNC_MNGR_OBJS_MON_STATUS_0;\n\tnum_regs = NUM_OF_MONITORS_IN_BLOCK;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0 +\n\t\t\t(GAUDI_FIRST_AVAILABLE_W_S_SYNC_OBJECT * 4);\n\tnum_regs = NUM_OF_SOB_IN_BLOCK - GAUDI_FIRST_AVAILABLE_W_S_SYNC_OBJECT;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\tbase_addr = CFG_BASE +  mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_STATUS_0 +\n\t\t\t(GAUDI_FIRST_AVAILABLE_W_S_MONITOR * 4);\n\tnum_regs = NUM_OF_MONITORS_IN_BLOCK - GAUDI_FIRST_AVAILABLE_W_S_MONITOR;\n\trc = gaudi_memset_registers(hdev, base_addr, num_regs, 0);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed resetting SM registers\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void gaudi_restore_dma_registers(struct hl_device *hdev)\n{\n\tu32 sob_delta = mmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_1 -\n\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0;\n\tint i;\n\n\tfor (i = 0 ; i < DMA_NUMBER_OF_CHANNELS ; i++) {\n\t\tu64 sob_addr = CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0 +\n\t\t\t\t(i * sob_delta);\n\t\tu32 dma_offset = i * DMA_CORE_OFFSET;\n\n\t\tWREG32(mmDMA0_CORE_WR_COMP_ADDR_LO + dma_offset,\n\t\t\t\tlower_32_bits(sob_addr));\n\t\tWREG32(mmDMA0_CORE_WR_COMP_ADDR_HI + dma_offset,\n\t\t\t\tupper_32_bits(sob_addr));\n\t\tWREG32(mmDMA0_CORE_WR_COMP_WDATA + dma_offset, 0x80000001);\n\n\t\t \n\t\tif (i > 1)\n\t\t\tWREG32(mmDMA0_CORE_WR_AWUSER_31_11 + dma_offset,\n\t\t\t\t\t\t\t\t0x00000001);\n\t}\n}\n\nstatic void gaudi_restore_qm_registers(struct hl_device *hdev)\n{\n\tu32 qman_offset;\n\tint i;\n\n\tfor (i = 0 ; i < DMA_NUMBER_OF_CHANNELS ; i++) {\n\t\tqman_offset = i * DMA_QMAN_OFFSET;\n\t\tWREG32(mmDMA0_QM_ARB_CFG_0 + qman_offset, 0);\n\t}\n\n\tfor (i = 0 ; i < MME_NUMBER_OF_MASTER_ENGINES ; i++) {\n\t\tqman_offset = i * (mmMME2_QM_BASE - mmMME0_QM_BASE);\n\t\tWREG32(mmMME0_QM_ARB_CFG_0 + qman_offset, 0);\n\t}\n\n\tfor (i = 0 ; i < TPC_NUMBER_OF_ENGINES ; i++) {\n\t\tqman_offset = i * TPC_QMAN_OFFSET;\n\t\tWREG32(mmTPC0_QM_ARB_CFG_0 + qman_offset, 0);\n\t}\n\n\tfor (i = 0 ; i < NIC_NUMBER_OF_ENGINES ; i++) {\n\t\tqman_offset = (i >> 1) * NIC_MACRO_QMAN_OFFSET +\n\t\t\t\t(i & 0x1) * NIC_ENGINE_QMAN_OFFSET;\n\t\tWREG32(mmNIC0_QM0_ARB_CFG_0 + qman_offset, 0);\n\t}\n}\n\nstatic int gaudi_restore_user_registers(struct hl_device *hdev)\n{\n\tint rc;\n\n\trc = gaudi_restore_sm_registers(hdev);\n\tif (rc)\n\t\treturn rc;\n\n\tgaudi_restore_dma_registers(hdev);\n\tgaudi_restore_qm_registers(hdev);\n\n\treturn 0;\n}\n\nstatic int gaudi_context_switch(struct hl_device *hdev, u32 asid)\n{\n\treturn 0;\n}\n\nstatic int gaudi_mmu_clear_pgt_range(struct hl_device *hdev)\n{\n\tu32 size = hdev->asic_prop.mmu_pgt_size +\n\t\t\thdev->asic_prop.mmu_cache_mng_size;\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu64 addr = hdev->asic_prop.mmu_pgt_addr;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU))\n\t\treturn 0;\n\n\treturn gaudi_memset_device_memory(hdev, addr, size, 0);\n}\n\nstatic void gaudi_restore_phase_topology(struct hl_device *hdev)\n{\n\n}\n\nstatic int gaudi_dma_core_transfer(struct hl_device *hdev, int dma_id, u64 addr,\n\t\t\t\t\tu32 size_to_dma, dma_addr_t dma_addr)\n{\n\tu32 err_cause, val;\n\tu64 dma_offset;\n\tint rc;\n\n\tdma_offset = dma_id * DMA_CORE_OFFSET;\n\n\tWREG32(mmDMA0_CORE_SRC_BASE_LO + dma_offset, lower_32_bits(addr));\n\tWREG32(mmDMA0_CORE_SRC_BASE_HI + dma_offset, upper_32_bits(addr));\n\tWREG32(mmDMA0_CORE_DST_BASE_LO + dma_offset, lower_32_bits(dma_addr));\n\tWREG32(mmDMA0_CORE_DST_BASE_HI + dma_offset, upper_32_bits(dma_addr));\n\tWREG32(mmDMA0_CORE_DST_TSIZE_0 + dma_offset, size_to_dma);\n\tWREG32(mmDMA0_CORE_COMMIT + dma_offset,\n\t\t\t(1 << DMA0_CORE_COMMIT_LIN_SHIFT));\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tmmDMA0_CORE_STS0 + dma_offset,\n\t\tval,\n\t\t((val & DMA0_CORE_STS0_BUSY_MASK) == 0),\n\t\t0,\n\t\t1000000);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"DMA %d timed-out during reading of 0x%llx\\n\",\n\t\t\tdma_id, addr);\n\t\treturn -EIO;\n\t}\n\n\t \n\terr_cause = RREG32(mmDMA0_CORE_ERR_CAUSE + dma_offset);\n\tif (err_cause) {\n\t\tdev_err(hdev->dev, \"DMA Failed, cause 0x%x\\n\", err_cause);\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Clearing DMA0 engine from errors (cause 0x%x)\\n\",\n\t\t\terr_cause);\n\t\tWREG32(mmDMA0_CORE_ERR_CAUSE + dma_offset, err_cause);\n\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi_debugfs_read_dma(struct hl_device *hdev, u64 addr, u32 size,\n\t\t\t\tvoid *blob_addr)\n{\n\tu32 dma_core_sts0, err_cause, cfg1, size_left, pos, size_to_dma;\n\tu32 qm_glbl_sts0, qm_cgm_sts;\n\tu64 dma_offset, qm_offset;\n\tdma_addr_t dma_addr;\n\tvoid *kernel_addr;\n\tbool is_eng_idle;\n\tint rc = 0, dma_id;\n\n\tkernel_addr = hl_asic_dma_alloc_coherent(hdev, SZ_2M, &dma_addr, GFP_KERNEL | __GFP_ZERO);\n\n\tif (!kernel_addr)\n\t\treturn -ENOMEM;\n\n\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\tdma_id = gaudi_dma_assignment[GAUDI_PCI_DMA_1];\n\tdma_offset = dma_id * DMA_CORE_OFFSET;\n\tqm_offset = dma_id * DMA_QMAN_OFFSET;\n\tdma_core_sts0 = RREG32(mmDMA0_CORE_STS0 + dma_offset);\n\tqm_glbl_sts0 = RREG32(mmDMA0_QM_GLBL_STS0 + qm_offset);\n\tqm_cgm_sts = RREG32(mmDMA0_QM_CGM_STS + qm_offset);\n\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts) &&\n\t\t      IS_DMA_IDLE(dma_core_sts0);\n\n\tif (!is_eng_idle) {\n\t\tdma_id = gaudi_dma_assignment[GAUDI_PCI_DMA_2];\n\t\tdma_offset = dma_id * DMA_CORE_OFFSET;\n\t\tqm_offset = dma_id * DMA_QMAN_OFFSET;\n\t\tdma_core_sts0 = RREG32(mmDMA0_CORE_STS0 + dma_offset);\n\t\tqm_glbl_sts0 = RREG32(mmDMA0_QM_GLBL_STS0 + qm_offset);\n\t\tqm_cgm_sts = RREG32(mmDMA0_QM_CGM_STS + qm_offset);\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts) &&\n\t\t\t      IS_DMA_IDLE(dma_core_sts0);\n\n\t\tif (!is_eng_idle) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"Can't read via DMA because it is BUSY\\n\");\n\t\t\trc = -EAGAIN;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tcfg1 = RREG32(mmDMA0_QM_GLBL_CFG1 + qm_offset);\n\tWREG32(mmDMA0_QM_GLBL_CFG1 + qm_offset,\n\t\t\t0xF << DMA0_QM_GLBL_CFG1_CP_STOP_SHIFT);\n\n\t \n\tWREG32_OR(mmDMA0_CORE_PROT + dma_offset, BIT(DMA0_CORE_PROT_VAL_SHIFT));\n\n\t \n\terr_cause = RREG32(mmDMA0_CORE_ERR_CAUSE + dma_offset);\n\tif (err_cause) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Clearing DMA0 engine from errors (cause 0x%x)\\n\",\n\t\t\terr_cause);\n\t\tWREG32(mmDMA0_CORE_ERR_CAUSE + dma_offset, err_cause);\n\t}\n\n\tpos = 0;\n\tsize_left = size;\n\tsize_to_dma = SZ_2M;\n\n\twhile (size_left > 0) {\n\n\t\tif (size_left < SZ_2M)\n\t\t\tsize_to_dma = size_left;\n\n\t\trc = gaudi_dma_core_transfer(hdev, dma_id, addr, size_to_dma,\n\t\t\t\t\t\tdma_addr);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tmemcpy(blob_addr + pos, kernel_addr, size_to_dma);\n\n\t\tif (size_left <= SZ_2M)\n\t\t\tbreak;\n\n\t\tpos += SZ_2M;\n\t\taddr += SZ_2M;\n\t\tsize_left -= SZ_2M;\n\t}\n\n\t \n\tWREG32_AND(mmDMA0_CORE_PROT + dma_offset,\n\t\t\t~BIT(DMA0_CORE_PROT_VAL_SHIFT));\n\n\tWREG32(mmDMA0_QM_GLBL_CFG1 + qm_offset, cfg1);\n\nout:\n\thdev->asic_funcs->hw_queues_unlock(hdev);\n\n\thl_asic_dma_free_coherent(hdev, SZ_2M, kernel_addr, dma_addr);\n\n\treturn rc;\n}\n\nstatic u64 gaudi_read_pte(struct hl_device *hdev, u64 addr)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (hdev->reset_info.hard_reset_pending)\n\t\treturn U64_MAX;\n\n\treturn readq(hdev->pcie_bar[HBM_BAR_ID] +\n\t\t\t(addr - gaudi->hbm_bar_cur_addr));\n}\n\nstatic void gaudi_write_pte(struct hl_device *hdev, u64 addr, u64 val)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (hdev->reset_info.hard_reset_pending)\n\t\treturn;\n\n\twriteq(val, hdev->pcie_bar[HBM_BAR_ID] +\n\t\t\t(addr - gaudi->hbm_bar_cur_addr));\n}\n\nvoid gaudi_mmu_prepare_reg(struct hl_device *hdev, u64 reg, u32 asid)\n{\n\t \n\tWREG32_AND(reg, ~0x7FF);\n\tWREG32_OR(reg, asid);\n}\n\nstatic void gaudi_mmu_prepare(struct hl_device *hdev, u32 asid)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU))\n\t\treturn;\n\n\tif (asid & ~DMA0_QM_GLBL_NON_SECURE_PROPS_0_ASID_MASK) {\n\t\tdev_crit(hdev->dev, \"asid %u is too big\\n\", asid);\n\t\treturn;\n\t}\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmDMA0_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA1_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA2_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA3_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA4_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA5_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA6_CORE_NON_SECURE_PROPS, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmDMA7_CORE_NON_SECURE_PROPS, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC0_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC1_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC2_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC3_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC4_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC5_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC6_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_CFG_ARUSER_LO, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmTPC7_CFG_AWUSER_LO, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_QM_GLBL_NON_SECURE_PROPS_0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_QM_GLBL_NON_SECURE_PROPS_1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_QM_GLBL_NON_SECURE_PROPS_2, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_QM_GLBL_NON_SECURE_PROPS_3, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_QM_GLBL_NON_SECURE_PROPS_4, asid);\n\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_SBAB_ARUSER0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_SBAB_ARUSER1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME1_SBAB_ARUSER0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME1_SBAB_ARUSER1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_SBAB_ARUSER0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_SBAB_ARUSER1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME3_SBAB_ARUSER0, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME3_SBAB_ARUSER1, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME0_ACC_WBC, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME1_ACC_WBC, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME2_ACC_WBC, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmMME3_ACC_WBC, asid);\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC0) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM0_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM0_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM0_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM0_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM0_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC1) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM1_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM1_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM1_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM1_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC0_QM1_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC2) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM0_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM0_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM0_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM0_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM0_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC3) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM1_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM1_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM1_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM1_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC1_QM1_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC4) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM0_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM0_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM0_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM0_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM0_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC5) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM1_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM1_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM1_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM1_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC2_QM1_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC6) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM0_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM0_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM0_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM0_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM0_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC7) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM1_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM1_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM1_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM1_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC3_QM1_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC8) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM0_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM0_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM0_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM0_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM0_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tif (gaudi->hw_cap_initialized & HW_CAP_NIC9) {\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM1_GLBL_NON_SECURE_PROPS_0,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM1_GLBL_NON_SECURE_PROPS_1,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM1_GLBL_NON_SECURE_PROPS_2,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM1_GLBL_NON_SECURE_PROPS_3,\n\t\t\t\tasid);\n\t\tgaudi_mmu_prepare_reg(hdev, mmNIC4_QM1_GLBL_NON_SECURE_PROPS_4,\n\t\t\t\tasid);\n\t}\n\n\tgaudi_mmu_prepare_reg(hdev, mmPSOC_GLOBAL_CONF_TRACE_ARUSER, asid);\n\tgaudi_mmu_prepare_reg(hdev, mmPSOC_GLOBAL_CONF_TRACE_AWUSER, asid);\n}\n\nstatic int gaudi_send_job_on_qman0(struct hl_device *hdev,\n\t\tstruct hl_cs_job *job)\n{\n\tstruct packet_msg_prot *fence_pkt;\n\tu32 *fence_ptr;\n\tdma_addr_t fence_dma_addr;\n\tstruct hl_cb *cb;\n\tu32 tmp, timeout, dma_offset;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout = GAUDI_PLDM_QMAN0_TIMEOUT_USEC;\n\telse\n\t\ttimeout = HL_DEVICE_TIMEOUT_USEC;\n\n\tfence_ptr = hl_asic_dma_pool_zalloc(hdev, 4, GFP_KERNEL, &fence_dma_addr);\n\tif (!fence_ptr) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate fence memory for QMAN0\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tcb = job->patched_cb;\n\n\tfence_pkt = cb->kernel_address +\n\t\t\tjob->job_cb_size - sizeof(struct packet_msg_prot);\n\n\ttmp = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_PROT);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 1);\n\ttmp |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tfence_pkt->ctl = cpu_to_le32(tmp);\n\tfence_pkt->value = cpu_to_le32(GAUDI_QMAN0_FENCE_VAL);\n\tfence_pkt->addr = cpu_to_le64(fence_dma_addr);\n\n\tdma_offset = gaudi_dma_assignment[GAUDI_PCI_DMA_1] * DMA_CORE_OFFSET;\n\n\tWREG32(mmDMA0_CORE_PROT + dma_offset,\n\t\t\tBIT(DMA0_CORE_PROT_ERR_VAL_SHIFT) | BIT(DMA0_CORE_PROT_VAL_SHIFT));\n\n\trc = hl_hw_queue_send_cb_no_cmpl(hdev, GAUDI_QUEUE_ID_DMA_0_0,\n\t\t\t\t\tjob->job_cb_size, cb->bus_address);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to send CB on QMAN0, %d\\n\", rc);\n\t\tgoto free_fence_ptr;\n\t}\n\n\trc = hl_poll_timeout_memory(hdev, fence_ptr, tmp,\n\t\t\t\t(tmp == GAUDI_QMAN0_FENCE_VAL), 1000,\n\t\t\t\ttimeout, true);\n\n\thl_hw_queue_inc_ci_kernel(hdev, GAUDI_QUEUE_ID_DMA_0_0);\n\n\tif (rc == -ETIMEDOUT) {\n\t\tdev_err(hdev->dev, \"QMAN0 Job timeout (0x%x)\\n\", tmp);\n\t\tgoto free_fence_ptr;\n\t}\n\nfree_fence_ptr:\n\tWREG32(mmDMA0_CORE_PROT + dma_offset, BIT(DMA0_CORE_PROT_ERR_VAL_SHIFT));\n\n\thl_asic_dma_pool_free(hdev, (void *) fence_ptr, fence_dma_addr);\n\treturn rc;\n}\n\nstatic void gaudi_get_event_desc(u16 event_type, char *desc, size_t size)\n{\n\tif (event_type >= GAUDI_EVENT_SIZE)\n\t\tgoto event_not_supported;\n\n\tif (!gaudi_irq_map_table[event_type].valid)\n\t\tgoto event_not_supported;\n\n\tsnprintf(desc, size, gaudi_irq_map_table[event_type].name);\n\n\treturn;\n\nevent_not_supported:\n\tsnprintf(desc, size, \"N/A\");\n}\n\nstatic const char *gaudi_get_razwi_initiator_dma_name(struct hl_device *hdev, u32 x_y,\n\t\t\t\t\t\t\tbool is_write, u16 *engine_id_1,\n\t\t\t\t\t\t\tu16 *engine_id_2)\n{\n\tu32 dma_id[2], dma_offset, err_cause[2], mask, i;\n\n\tmask = is_write ? DMA0_CORE_ERR_CAUSE_HBW_WR_ERR_MASK :\n\t\t\t\tDMA0_CORE_ERR_CAUSE_HBW_RD_ERR_MASK;\n\n\tswitch (x_y) {\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_1:\n\t\tdma_id[0] = 0;\n\t\tdma_id[1] = 2;\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_1:\n\t\tdma_id[0] = 1;\n\t\tdma_id[1] = 3;\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_1:\n\t\tdma_id[0] = 4;\n\t\tdma_id[1] = 6;\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_1:\n\t\tdma_id[0] = 5;\n\t\tdma_id[1] = 7;\n\t\tbreak;\n\tdefault:\n\t\tgoto unknown_initiator;\n\t}\n\n\tfor (i = 0 ; i < 2 ; i++) {\n\t\tdma_offset = dma_id[i] * DMA_CORE_OFFSET;\n\t\terr_cause[i] = RREG32(mmDMA0_CORE_ERR_CAUSE + dma_offset);\n\t}\n\n\tswitch (x_y) {\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_1:\n\t\tif ((err_cause[0] & mask) && !(err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_0;\n\t\t\treturn \"DMA0\";\n\t\t} else if (!(err_cause[0] & mask) && (err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_2;\n\t\t\treturn \"DMA2\";\n\t\t} else {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_0;\n\t\t\t*engine_id_2 = GAUDI_ENGINE_ID_DMA_2;\n\t\t\treturn \"DMA0 or DMA2\";\n\t\t}\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_1:\n\t\tif ((err_cause[0] & mask) && !(err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_1;\n\t\t\treturn \"DMA1\";\n\t\t} else if (!(err_cause[0] & mask) && (err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_3;\n\t\t\treturn \"DMA3\";\n\t\t} else {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_1;\n\t\t\t*engine_id_2 = GAUDI_ENGINE_ID_DMA_3;\n\t\t\treturn \"DMA1 or DMA3\";\n\t\t}\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_1:\n\t\tif ((err_cause[0] & mask) && !(err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_4;\n\t\t\treturn \"DMA4\";\n\t\t} else if (!(err_cause[0] & mask) && (err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_6;\n\t\t\treturn \"DMA6\";\n\t\t} else {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_4;\n\t\t\t*engine_id_2 = GAUDI_ENGINE_ID_DMA_6;\n\t\t\treturn \"DMA4 or DMA6\";\n\t\t}\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_1:\n\t\tif ((err_cause[0] & mask) && !(err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_5;\n\t\t\treturn \"DMA5\";\n\t\t} else if (!(err_cause[0] & mask) && (err_cause[1] & mask)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_7;\n\t\t\treturn \"DMA7\";\n\t\t} else {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_DMA_5;\n\t\t\t*engine_id_2 = GAUDI_ENGINE_ID_DMA_7;\n\t\t\treturn \"DMA5 or DMA7\";\n\t\t}\n\t}\n\nunknown_initiator:\n\treturn \"unknown initiator\";\n}\n\nstatic const char *gaudi_get_razwi_initiator_name(struct hl_device *hdev, bool is_write,\n\t\t\t\t\t\t\tu16 *engine_id_1, u16 *engine_id_2)\n{\n\tu32 val, x_y, axi_id;\n\n\tval = is_write ? RREG32(mmMMU_UP_RAZWI_WRITE_ID) :\n\t\t\t\tRREG32(mmMMU_UP_RAZWI_READ_ID);\n\tx_y = val & ((RAZWI_INITIATOR_Y_MASK << RAZWI_INITIATOR_Y_SHIFT) |\n\t\t\t(RAZWI_INITIATOR_X_MASK << RAZWI_INITIATOR_X_SHIFT));\n\taxi_id = val & (RAZWI_INITIATOR_AXI_ID_MASK <<\n\t\t\tRAZWI_INITIATOR_AXI_ID_SHIFT);\n\n\tswitch (x_y) {\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC0_NIC0:\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_TPC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_0;\n\t\t\treturn \"TPC0\";\n\t\t}\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_NIC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_NIC_0;\n\t\t\treturn \"NIC0\";\n\t\t}\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC1:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_1;\n\t\treturn \"TPC1\";\n\tcase RAZWI_INITIATOR_ID_X_Y_MME0_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_MME0_1:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_MME_0;\n\t\treturn \"MME0\";\n\tcase RAZWI_INITIATOR_ID_X_Y_MME1_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_MME1_1:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_MME_1;\n\t\treturn \"MME1\";\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC2:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_2;\n\t\treturn \"TPC2\";\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC3_PCI_CPU_PSOC:\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_TPC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_3;\n\t\t\treturn \"TPC3\";\n\t\t}\n\t\t \n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_PCI))\n\t\t\treturn \"PCI\";\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_CPU))\n\t\t\treturn \"CPU\";\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_PSOC))\n\t\t\treturn \"PSOC\";\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_S_1:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_S_1:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_W_N_1:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_DMA_IF_E_N_1:\n\t\treturn gaudi_get_razwi_initiator_dma_name(hdev, x_y, is_write,\n\t\t\t\tengine_id_1, engine_id_2);\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC4_NIC1_NIC2:\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_TPC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_4;\n\t\t\treturn \"TPC4\";\n\t\t}\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_NIC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_NIC_1;\n\t\t\treturn \"NIC1\";\n\t\t}\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_NIC_FT)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_NIC_2;\n\t\t\treturn \"NIC2\";\n\t\t}\n\t\tbreak;\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC5:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_5;\n\t\treturn \"TPC5\";\n\tcase RAZWI_INITIATOR_ID_X_Y_MME2_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_MME2_1:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_MME_2;\n\t\treturn \"MME2\";\n\tcase RAZWI_INITIATOR_ID_X_Y_MME3_0:\n\tcase RAZWI_INITIATOR_ID_X_Y_MME3_1:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_MME_3;\n\t\treturn \"MME3\";\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC6:\n\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_6;\n\t\treturn \"TPC6\";\n\tcase RAZWI_INITIATOR_ID_X_Y_TPC7_NIC4_NIC5:\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_TPC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_TPC_7;\n\t\t\treturn \"TPC7\";\n\t\t}\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_NIC)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_NIC_4;\n\t\t\treturn \"NIC4\";\n\t\t}\n\t\tif (axi_id == RAZWI_INITIATOR_ID_AXI_ID(AXI_ID_NIC_FT)) {\n\t\t\t*engine_id_1 = GAUDI_ENGINE_ID_NIC_5;\n\t\t\treturn \"NIC5\";\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tdev_err(hdev->dev,\n\t\t\"Unknown RAZWI initiator ID 0x%x [Y=%d, X=%d, AXI_ID=%d]\\n\",\n\t\tval,\n\t\t(val >> RAZWI_INITIATOR_Y_SHIFT) & RAZWI_INITIATOR_Y_MASK,\n\t\t(val >> RAZWI_INITIATOR_X_SHIFT) & RAZWI_INITIATOR_X_MASK,\n\t\t(val >> RAZWI_INITIATOR_AXI_ID_SHIFT) &\n\t\t\tRAZWI_INITIATOR_AXI_ID_MASK);\n\n\treturn \"unknown initiator\";\n}\n\nstatic void gaudi_print_and_get_razwi_info(struct hl_device *hdev, u16 *engine_id_1,\n\t\t\t\t\t\tu16 *engine_id_2, bool *is_read, bool *is_write)\n{\n\n\tif (RREG32(mmMMU_UP_RAZWI_WRITE_VLD)) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"RAZWI event caused by illegal write of %s\\n\",\n\t\t\tgaudi_get_razwi_initiator_name(hdev, true, engine_id_1, engine_id_2));\n\t\tWREG32(mmMMU_UP_RAZWI_WRITE_VLD, 0);\n\t\t*is_write = true;\n\t}\n\n\tif (RREG32(mmMMU_UP_RAZWI_READ_VLD)) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"RAZWI event caused by illegal read of %s\\n\",\n\t\t\tgaudi_get_razwi_initiator_name(hdev, false, engine_id_1, engine_id_2));\n\t\tWREG32(mmMMU_UP_RAZWI_READ_VLD, 0);\n\t\t*is_read = true;\n\t}\n}\n\nstatic void gaudi_print_and_get_mmu_error_info(struct hl_device *hdev, u64 *addr, u64 *event_mask)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 val;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU))\n\t\treturn;\n\n\tval = RREG32(mmMMU_UP_PAGE_ERROR_CAPTURE);\n\tif (val & MMU_UP_PAGE_ERROR_CAPTURE_ENTRY_VALID_MASK) {\n\t\t*addr = val & MMU_UP_PAGE_ERROR_CAPTURE_VA_49_32_MASK;\n\t\t*addr <<= 32;\n\t\t*addr |= RREG32(mmMMU_UP_PAGE_ERROR_CAPTURE_VA);\n\n\t\tdev_err_ratelimited(hdev->dev, \"MMU page fault on va 0x%llx\\n\", *addr);\n\t\thl_handle_page_fault(hdev, *addr, 0, true, event_mask);\n\n\t\tWREG32(mmMMU_UP_PAGE_ERROR_CAPTURE, 0);\n\t}\n\n\tval = RREG32(mmMMU_UP_ACCESS_ERROR_CAPTURE);\n\tif (val & MMU_UP_ACCESS_ERROR_CAPTURE_ENTRY_VALID_MASK) {\n\t\t*addr = val & MMU_UP_ACCESS_ERROR_CAPTURE_VA_49_32_MASK;\n\t\t*addr <<= 32;\n\t\t*addr |= RREG32(mmMMU_UP_ACCESS_ERROR_CAPTURE_VA);\n\n\t\tdev_err_ratelimited(hdev->dev, \"MMU access error on va 0x%llx\\n\", *addr);\n\n\t\tWREG32(mmMMU_UP_ACCESS_ERROR_CAPTURE, 0);\n\t}\n}\n\n \nstatic int gaudi_extract_ecc_info(struct hl_device *hdev,\n\t\tstruct ecc_info_extract_params *params, u64 *ecc_address,\n\t\tu64 *ecc_syndrom, u8 *memory_wrapper_idx)\n{\n\tu32 i, num_mem_regs, reg, err_bit;\n\tu64 err_addr, err_word = 0;\n\n\tnum_mem_regs = params->num_memories / 32 +\n\t\t\t((params->num_memories % 32) ? 1 : 0);\n\n\tif (params->block_address >= CFG_BASE)\n\t\tparams->block_address -= CFG_BASE;\n\n\tif (params->derr)\n\t\terr_addr = params->block_address + GAUDI_ECC_DERR0_OFFSET;\n\telse\n\t\terr_addr = params->block_address + GAUDI_ECC_SERR0_OFFSET;\n\n\t \n\t*memory_wrapper_idx = 0xFF;\n\n\t \n\tfor (i = 0 ; i < num_mem_regs ; i++) {\n\t\terr_addr += i * 4;\n\t\terr_word = RREG32(err_addr);\n\t\tif (err_word) {\n\t\t\terr_bit = __ffs(err_word);\n\t\t\t*memory_wrapper_idx = err_bit + (32 * i);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (*memory_wrapper_idx == 0xFF) {\n\t\tdev_err(hdev->dev, \"ECC error information cannot be found\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tWREG32(params->block_address + GAUDI_ECC_MEM_SEL_OFFSET,\n\t\t\t*memory_wrapper_idx);\n\n\t*ecc_address =\n\t\tRREG32(params->block_address + GAUDI_ECC_ADDRESS_OFFSET);\n\t*ecc_syndrom =\n\t\tRREG32(params->block_address + GAUDI_ECC_SYNDROME_OFFSET);\n\n\t \n\treg = RREG32(params->block_address + GAUDI_ECC_MEM_INFO_CLR_OFFSET);\n\tif (params->derr)\n\t\treg |= FIELD_PREP(GAUDI_ECC_MEM_INFO_CLR_DERR_MASK, 1);\n\telse\n\t\treg |= FIELD_PREP(GAUDI_ECC_MEM_INFO_CLR_SERR_MASK, 1);\n\n\tWREG32(params->block_address + GAUDI_ECC_MEM_INFO_CLR_OFFSET, reg);\n\n\treturn 0;\n}\n\n \nstatic inline u32 gaudi_queue_idx_dec(u32 idx, u32 q_len)\n{\n\tu32 mask = q_len - 1;\n\n\t \n\treturn (idx + q_len - 1) & mask;\n}\n\n \nstatic void gaudi_handle_sw_config_stream_data(struct hl_device *hdev, u32 stream,\n\t\t\t\t\t\tu64 qman_base, u64 event_mask)\n{\n\tu64 cq_ptr_lo, cq_ptr_hi, cq_tsize, cq_ptr;\n\tu32 cq_ptr_lo_off, size;\n\n\tcq_ptr_lo_off = mmTPC0_QM_CQ_PTR_LO_1 - mmTPC0_QM_CQ_PTR_LO_0;\n\n\tcq_ptr_lo = qman_base + (mmTPC0_QM_CQ_PTR_LO_0 - mmTPC0_QM_BASE) +\n\t\t\t\t\t\tstream * cq_ptr_lo_off;\n\tcq_ptr_hi = cq_ptr_lo +\n\t\t\t\t(mmTPC0_QM_CQ_PTR_HI_0 - mmTPC0_QM_CQ_PTR_LO_0);\n\tcq_tsize = cq_ptr_lo +\n\t\t\t\t(mmTPC0_QM_CQ_TSIZE_0 - mmTPC0_QM_CQ_PTR_LO_0);\n\n\tcq_ptr = (((u64) RREG32(cq_ptr_hi)) << 32) | RREG32(cq_ptr_lo);\n\tsize = RREG32(cq_tsize);\n\tdev_info(hdev->dev, \"stop on err: stream: %u, addr: %#llx, size: %u\\n\",\n\t\t\t\t\t\t\tstream, cq_ptr, size);\n\n\tif (event_mask & HL_NOTIFIER_EVENT_UNDEFINED_OPCODE) {\n\t\thdev->captured_err_info.undef_opcode.cq_addr = cq_ptr;\n\t\thdev->captured_err_info.undef_opcode.cq_size = size;\n\t\thdev->captured_err_info.undef_opcode.stream_id = stream;\n\t}\n}\n\n \nstatic void gaudi_handle_last_pqes_on_err(struct hl_device *hdev, u32 qid_base,\n\t\t\t\t\t\tu32 stream, u64 qman_base,\n\t\t\t\t\t\tu64 event_mask,\n\t\t\t\t\t\tbool pr_sw_conf)\n{\n\tu32 ci, qm_ci_stream_off, queue_len;\n\tstruct hl_hw_queue *q;\n\tu64 pq_ci, addr[PQ_FETCHER_CACHE_SIZE];\n\tint i;\n\n\tq = &hdev->kernel_queues[qid_base + stream];\n\n\tqm_ci_stream_off = mmTPC0_QM_PQ_CI_1 - mmTPC0_QM_PQ_CI_0;\n\tpq_ci = qman_base + (mmTPC0_QM_PQ_CI_0 - mmTPC0_QM_BASE) +\n\t\t\t\t\t\tstream * qm_ci_stream_off;\n\n\tqueue_len = (q->queue_type == QUEUE_TYPE_INT) ?\n\t\t\t\t\tq->int_queue_len : HL_QUEUE_LENGTH;\n\n\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\tif (pr_sw_conf)\n\t\tgaudi_handle_sw_config_stream_data(hdev, stream, qman_base, event_mask);\n\n\tci = RREG32(pq_ci);\n\n\t \n\tci = gaudi_queue_idx_dec(ci, queue_len);\n\tmemset(addr, 0, sizeof(addr));\n\n\tfor (i = 0; i < PQ_FETCHER_CACHE_SIZE; i++) {\n\t\tstruct hl_bd *bd;\n\t\tu32 len;\n\n\t\tbd = q->kernel_address;\n\t\tbd += ci;\n\n\t\tlen = le32_to_cpu(bd->len);\n\t\t \n\t\tif (!len)\n\t\t\tbreak;\n\n\t\taddr[i] = le64_to_cpu(bd->ptr);\n\n\t\tdev_info(hdev->dev, \"stop on err PQE(stream %u): ci: %u, addr: %#llx, size: %u\\n\",\n\t\t\t\t\t\t\tstream, ci, addr[i], len);\n\n\t\t \n\t\tci = gaudi_queue_idx_dec(ci, queue_len);\n\t}\n\n\tif (event_mask & HL_NOTIFIER_EVENT_UNDEFINED_OPCODE) {\n\t\tstruct undefined_opcode_info *undef_opcode = &hdev->captured_err_info.undef_opcode;\n\t\tu32 arr_idx = undef_opcode->cb_addr_streams_len;\n\n\t\tif (arr_idx == 0) {\n\t\t\tundef_opcode->timestamp = ktime_get();\n\t\t\tundef_opcode->engine_id = gaudi_queue_id_to_engine_id[qid_base];\n\t\t}\n\n\t\tmemcpy(undef_opcode->cb_addr_streams[arr_idx], addr, sizeof(addr));\n\t\tundef_opcode->cb_addr_streams_len++;\n\t}\n\n\thdev->asic_funcs->hw_queues_unlock(hdev);\n}\n\n \nstatic void handle_qman_data_on_err(struct hl_device *hdev, u32 qid_base,\n\t\t\t\t   u32 stream, u64 qman_base, u64 event_mask)\n{\n\tu32 i;\n\n\tif (stream != QMAN_STREAMS) {\n\t\tgaudi_handle_last_pqes_on_err(hdev, qid_base, stream,\n\t\t\tqman_base, event_mask, true);\n\t\treturn;\n\t}\n\n\t \n\tgaudi_handle_sw_config_stream_data(hdev, stream, qman_base, event_mask);\n\n\tfor (i = 0; i < QMAN_STREAMS; i++)\n\t\tgaudi_handle_last_pqes_on_err(hdev, qid_base, i,\n\t\t\tqman_base, event_mask, false);\n}\n\nstatic void gaudi_handle_qman_err_generic(struct hl_device *hdev,\n\t\t\t\t\t  const char *qm_name,\n\t\t\t\t\t  u64 qman_base,\n\t\t\t\t\t  u32 qid_base,\n\t\t\t\t\t  u64 *event_mask)\n{\n\tu32 i, j, glbl_sts_val, arb_err_val, glbl_sts_clr_val;\n\tu64 glbl_sts_addr, arb_err_addr;\n\tchar reg_desc[32];\n\n\tglbl_sts_addr = qman_base + (mmTPC0_QM_GLBL_STS1_0 - mmTPC0_QM_BASE);\n\tarb_err_addr = qman_base + (mmTPC0_QM_ARB_ERR_CAUSE - mmTPC0_QM_BASE);\n\n\t \n\tfor (i = 0 ; i < QMAN_STREAMS + 1 ; i++) {\n\t\tglbl_sts_clr_val = 0;\n\t\tglbl_sts_val = RREG32(glbl_sts_addr + 4 * i);\n\n\t\tif (!glbl_sts_val)\n\t\t\tcontinue;\n\n\t\tif (i == QMAN_STREAMS)\n\t\t\tsnprintf(reg_desc, ARRAY_SIZE(reg_desc), \"LowerCP\");\n\t\telse\n\t\t\tsnprintf(reg_desc, ARRAY_SIZE(reg_desc), \"stream%u\", i);\n\n\t\tfor (j = 0 ; j < GAUDI_NUM_OF_QM_ERR_CAUSE ; j++) {\n\t\t\tif (glbl_sts_val & BIT(j)) {\n\t\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\t\"%s %s. err cause: %s\\n\",\n\t\t\t\t\t\tqm_name, reg_desc,\n\t\t\t\t\t\tgaudi_qman_error_cause[j]);\n\t\t\t\tglbl_sts_clr_val |= BIT(j);\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (glbl_sts_val & TPC0_QM_GLBL_STS1_CP_UNDEF_CMD_ERR_MASK &&\n\t\t\t\thdev->captured_err_info.undef_opcode.write_enable) {\n\t\t\tmemset(&hdev->captured_err_info.undef_opcode, 0,\n\t\t\t\t\t\tsizeof(hdev->captured_err_info.undef_opcode));\n\n\t\t\thdev->captured_err_info.undef_opcode.write_enable = false;\n\t\t\t*event_mask |= HL_NOTIFIER_EVENT_UNDEFINED_OPCODE;\n\t\t}\n\n\t\t \n\t\tif (!hdev->stop_on_err)\n\t\t\tWREG32(glbl_sts_addr + 4 * i, glbl_sts_clr_val);\n\t\telse\n\t\t\thandle_qman_data_on_err(hdev, qid_base, i, qman_base, *event_mask);\n\t}\n\n\tarb_err_val = RREG32(arb_err_addr);\n\n\tif (!arb_err_val)\n\t\treturn;\n\n\tfor (j = 0 ; j < GAUDI_NUM_OF_QM_ARB_ERR_CAUSE ; j++) {\n\t\tif (arb_err_val & BIT(j)) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\"%s ARB_ERR. err cause: %s\\n\",\n\t\t\t\t\tqm_name,\n\t\t\t\t\tgaudi_qman_arb_error_cause[j]);\n\t\t}\n\t}\n}\n\nstatic void gaudi_print_sm_sei_info(struct hl_device *hdev, u16 event_type,\n\t\tstruct hl_eq_sm_sei_data *sei_data)\n{\n\tu32 index = event_type - GAUDI_EVENT_DMA_IF_SEI_0;\n\n\t \n\tindex = (index ^ 0x3) & 0x3;\n\n\tswitch (sei_data->sei_cause) {\n\tcase SM_SEI_SO_OVERFLOW:\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"%s SEI Error: SOB Group %u overflow/underflow\",\n\t\t\tgaudi_sync_manager_names[index],\n\t\t\tle32_to_cpu(sei_data->sei_log));\n\t\tbreak;\n\tcase SM_SEI_LBW_4B_UNALIGNED:\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"%s SEI Error: Unaligned 4B LBW access, monitor agent address low - %#x\",\n\t\t\tgaudi_sync_manager_names[index],\n\t\t\tle32_to_cpu(sei_data->sei_log));\n\t\tbreak;\n\tcase SM_SEI_AXI_RESPONSE_ERR:\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"%s SEI Error: AXI ID %u response error\",\n\t\t\tgaudi_sync_manager_names[index],\n\t\t\tle32_to_cpu(sei_data->sei_log));\n\t\tbreak;\n\tdefault:\n\t\tdev_err_ratelimited(hdev->dev, \"Unknown SM SEI cause %u\",\n\t\t\t\tle32_to_cpu(sei_data->sei_log));\n\t\tbreak;\n\t}\n}\n\nstatic void gaudi_handle_ecc_event(struct hl_device *hdev, u16 event_type,\n\t\tstruct hl_eq_ecc_data *ecc_data)\n{\n\tstruct ecc_info_extract_params params;\n\tu64 ecc_address = 0, ecc_syndrom = 0;\n\tu8 index, memory_wrapper_idx = 0;\n\tbool extract_info_from_fw;\n\tint rc;\n\n\tif (hdev->asic_prop.fw_security_enabled) {\n\t\textract_info_from_fw = true;\n\t\tgoto extract_ecc_info;\n\t}\n\n\tswitch (event_type) {\n\tcase GAUDI_EVENT_PCIE_CORE_SERR ... GAUDI_EVENT_PCIE_PHY_DERR:\n\tcase GAUDI_EVENT_DMA0_SERR_ECC ... GAUDI_EVENT_MMU_DERR:\n\t\textract_info_from_fw = true;\n\t\tbreak;\n\tcase GAUDI_EVENT_TPC0_SERR ... GAUDI_EVENT_TPC7_SERR:\n\t\tindex = event_type - GAUDI_EVENT_TPC0_SERR;\n\t\tparams.block_address = mmTPC0_CFG_BASE + index * TPC_CFG_OFFSET;\n\t\tparams.num_memories = 90;\n\t\tparams.derr = false;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tcase GAUDI_EVENT_TPC0_DERR ... GAUDI_EVENT_TPC7_DERR:\n\t\tindex = event_type - GAUDI_EVENT_TPC0_DERR;\n\t\tparams.block_address =\n\t\t\tmmTPC0_CFG_BASE + index * TPC_CFG_OFFSET;\n\t\tparams.num_memories = 90;\n\t\tparams.derr = true;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tcase GAUDI_EVENT_MME0_ACC_SERR:\n\tcase GAUDI_EVENT_MME1_ACC_SERR:\n\tcase GAUDI_EVENT_MME2_ACC_SERR:\n\tcase GAUDI_EVENT_MME3_ACC_SERR:\n\t\tindex = (event_type - GAUDI_EVENT_MME0_ACC_SERR) / 4;\n\t\tparams.block_address = mmMME0_ACC_BASE + index * MME_ACC_OFFSET;\n\t\tparams.num_memories = 128;\n\t\tparams.derr = false;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tcase GAUDI_EVENT_MME0_ACC_DERR:\n\tcase GAUDI_EVENT_MME1_ACC_DERR:\n\tcase GAUDI_EVENT_MME2_ACC_DERR:\n\tcase GAUDI_EVENT_MME3_ACC_DERR:\n\t\tindex = (event_type - GAUDI_EVENT_MME0_ACC_DERR) / 4;\n\t\tparams.block_address = mmMME0_ACC_BASE + index * MME_ACC_OFFSET;\n\t\tparams.num_memories = 128;\n\t\tparams.derr = true;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tcase GAUDI_EVENT_MME0_SBAB_SERR:\n\tcase GAUDI_EVENT_MME1_SBAB_SERR:\n\tcase GAUDI_EVENT_MME2_SBAB_SERR:\n\tcase GAUDI_EVENT_MME3_SBAB_SERR:\n\t\tindex = (event_type - GAUDI_EVENT_MME0_SBAB_SERR) / 4;\n\t\tparams.block_address =\n\t\t\tmmMME0_SBAB_BASE + index * MME_ACC_OFFSET;\n\t\tparams.num_memories = 33;\n\t\tparams.derr = false;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tcase GAUDI_EVENT_MME0_SBAB_DERR:\n\tcase GAUDI_EVENT_MME1_SBAB_DERR:\n\tcase GAUDI_EVENT_MME2_SBAB_DERR:\n\tcase GAUDI_EVENT_MME3_SBAB_DERR:\n\t\tindex = (event_type - GAUDI_EVENT_MME0_SBAB_DERR) / 4;\n\t\tparams.block_address =\n\t\t\tmmMME0_SBAB_BASE + index * MME_ACC_OFFSET;\n\t\tparams.num_memories = 33;\n\t\tparams.derr = true;\n\t\textract_info_from_fw = false;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\nextract_ecc_info:\n\tif (extract_info_from_fw) {\n\t\tecc_address = le64_to_cpu(ecc_data->ecc_address);\n\t\tecc_syndrom = le64_to_cpu(ecc_data->ecc_syndrom);\n\t\tmemory_wrapper_idx = ecc_data->memory_wrapper_idx;\n\t} else {\n\t\trc = gaudi_extract_ecc_info(hdev, &params, &ecc_address,\n\t\t\t\t&ecc_syndrom, &memory_wrapper_idx);\n\t\tif (rc)\n\t\t\treturn;\n\t}\n\n\tdev_err(hdev->dev,\n\t\t\"ECC error detected. address: %#llx. Syndrom: %#llx. block id %u\\n\",\n\t\tecc_address, ecc_syndrom, memory_wrapper_idx);\n}\n\nstatic void gaudi_handle_qman_err(struct hl_device *hdev, u16 event_type, u64 *event_mask)\n{\n\tu64 qman_base;\n\tchar desc[32];\n\tu32 qid_base;\n\tu8 index;\n\n\tswitch (event_type) {\n\tcase GAUDI_EVENT_TPC0_QM ... GAUDI_EVENT_TPC7_QM:\n\t\tindex = event_type - GAUDI_EVENT_TPC0_QM;\n\t\tqid_base = GAUDI_QUEUE_ID_TPC_0_0 + index * QMAN_STREAMS;\n\t\tqman_base = mmTPC0_QM_BASE + index * TPC_QMAN_OFFSET;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"%s%d\", \"TPC_QM\", index);\n\t\tbreak;\n\tcase GAUDI_EVENT_MME0_QM ... GAUDI_EVENT_MME2_QM:\n\t\tif (event_type == GAUDI_EVENT_MME0_QM) {\n\t\t\tindex = 0;\n\t\t\tqid_base = GAUDI_QUEUE_ID_MME_0_0;\n\t\t} else {  \n\t\t\tindex = 2;\n\t\t\tqid_base = GAUDI_QUEUE_ID_MME_1_0;\n\t\t}\n\t\tqman_base = mmMME0_QM_BASE + index * MME_QMAN_OFFSET;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"%s%d\", \"MME_QM\", index);\n\t\tbreak;\n\tcase GAUDI_EVENT_DMA0_QM ... GAUDI_EVENT_DMA7_QM:\n\t\tindex = event_type - GAUDI_EVENT_DMA0_QM;\n\t\tqid_base = GAUDI_QUEUE_ID_DMA_0_0 + index * QMAN_STREAMS;\n\t\t \n\t\tif (index > 1)\n\t\t\tqid_base++;\n\t\tqman_base = mmDMA0_QM_BASE + index * DMA_QMAN_OFFSET;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"%s%d\", \"DMA_QM\", index);\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC0_QM0:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_0_0;\n\t\tqman_base = mmNIC0_QM0_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC0_QM0\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC0_QM1:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_1_0;\n\t\tqman_base = mmNIC0_QM1_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC0_QM1\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC1_QM0:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_2_0;\n\t\tqman_base = mmNIC1_QM0_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC1_QM0\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC1_QM1:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_3_0;\n\t\tqman_base = mmNIC1_QM1_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC1_QM1\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC2_QM0:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_4_0;\n\t\tqman_base = mmNIC2_QM0_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC2_QM0\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC2_QM1:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_5_0;\n\t\tqman_base = mmNIC2_QM1_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC2_QM1\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC3_QM0:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_6_0;\n\t\tqman_base = mmNIC3_QM0_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC3_QM0\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC3_QM1:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_7_0;\n\t\tqman_base = mmNIC3_QM1_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC3_QM1\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC4_QM0:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_8_0;\n\t\tqman_base = mmNIC4_QM0_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC4_QM0\");\n\t\tbreak;\n\tcase GAUDI_EVENT_NIC4_QM1:\n\t\tqid_base = GAUDI_QUEUE_ID_NIC_9_0;\n\t\tqman_base = mmNIC4_QM1_BASE;\n\t\tsnprintf(desc, ARRAY_SIZE(desc), \"NIC4_QM1\");\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\tgaudi_handle_qman_err_generic(hdev, desc, qman_base, qid_base, event_mask);\n}\n\nstatic void gaudi_print_irq_info(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\tbool check_razwi, u64 *event_mask)\n{\n\tbool is_read = false, is_write = false;\n\tu16 engine_id[2], num_of_razwi_eng = 0;\n\tchar desc[64] = \"\";\n\tu64 razwi_addr = 0;\n\tu8 razwi_flags = 0;\n\n\t \n\tengine_id[0] = HL_RAZWI_NA_ENG_ID;\n\tengine_id[1] = HL_RAZWI_NA_ENG_ID;\n\n\tgaudi_get_event_desc(event_type, desc, sizeof(desc));\n\tdev_err_ratelimited(hdev->dev, \"Received H/W interrupt %d [\\\"%s\\\"]\\n\",\n\t\tevent_type, desc);\n\n\tif (check_razwi) {\n\t\tgaudi_print_and_get_razwi_info(hdev, &engine_id[0], &engine_id[1], &is_read,\n\t\t\t\t\t\t&is_write);\n\t\tgaudi_print_and_get_mmu_error_info(hdev, &razwi_addr, event_mask);\n\n\t\tif (is_read)\n\t\t\trazwi_flags |= HL_RAZWI_READ;\n\t\tif (is_write)\n\t\t\trazwi_flags |= HL_RAZWI_WRITE;\n\n\t\tif (engine_id[0] != HL_RAZWI_NA_ENG_ID) {\n\t\t\tif (engine_id[1] != HL_RAZWI_NA_ENG_ID)\n\t\t\t\tnum_of_razwi_eng = 2;\n\t\t\telse\n\t\t\t\tnum_of_razwi_eng = 1;\n\t\t}\n\n\t\tif (razwi_flags)\n\t\t\thl_handle_razwi(hdev, razwi_addr, engine_id, num_of_razwi_eng,\n\t\t\t\t\trazwi_flags, event_mask);\n\t}\n}\n\nstatic void gaudi_print_out_of_sync_info(struct hl_device *hdev,\n\t\t\t\t\tstruct cpucp_pkt_sync_err *sync_err)\n{\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[GAUDI_QUEUE_ID_CPU_PQ];\n\n\tdev_err(hdev->dev, \"Out of sync with FW, FW: pi=%u, ci=%u, LKD: pi=%u, ci=%d\\n\",\n\t\tle32_to_cpu(sync_err->pi), le32_to_cpu(sync_err->ci), q->pi, atomic_read(&q->ci));\n}\n\nstatic void gaudi_print_fw_alive_info(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_eq_fw_alive *fw_alive)\n{\n\tdev_err(hdev->dev,\n\t\t\"FW alive report: severity=%s, process_id=%u, thread_id=%u, uptime=%llu seconds\\n\",\n\t\t(fw_alive->severity == FW_ALIVE_SEVERITY_MINOR) ? \"Minor\" : \"Critical\",\n\t\tle32_to_cpu(fw_alive->process_id),\n\t\tle32_to_cpu(fw_alive->thread_id),\n\t\tle64_to_cpu(fw_alive->uptime_seconds));\n}\n\nstatic void gaudi_print_nic_axi_irq_info(struct hl_device *hdev, u16 event_type,\n\t\t\t\t\t\tvoid *data)\n{\n\tchar desc[64] = \"\", *type;\n\tstruct eq_nic_sei_event *eq_nic_sei = data;\n\tu16 nic_id = event_type - GAUDI_EVENT_NIC_SEI_0;\n\n\tswitch (eq_nic_sei->axi_error_cause) {\n\tcase RXB:\n\t\ttype = \"RXB\";\n\t\tbreak;\n\tcase RXE:\n\t\ttype = \"RXE\";\n\t\tbreak;\n\tcase TXS:\n\t\ttype = \"TXS\";\n\t\tbreak;\n\tcase TXE:\n\t\ttype = \"TXE\";\n\t\tbreak;\n\tcase QPC_RESP:\n\t\ttype = \"QPC_RESP\";\n\t\tbreak;\n\tcase NON_AXI_ERR:\n\t\ttype = \"NON_AXI_ERR\";\n\t\tbreak;\n\tcase TMR:\n\t\ttype = \"TMR\";\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hdev->dev, \"unknown NIC AXI cause %d\\n\",\n\t\t\teq_nic_sei->axi_error_cause);\n\t\ttype = \"N/A\";\n\t\tbreak;\n\t}\n\n\tsnprintf(desc, sizeof(desc), \"NIC%d_%s%d\", nic_id, type,\n\t\t\teq_nic_sei->id);\n\tdev_err_ratelimited(hdev->dev, \"Received H/W interrupt %d [\\\"%s\\\"]\\n\",\n\t\tevent_type, desc);\n}\n\nstatic int gaudi_compute_reset_late_init(struct hl_device *hdev)\n{\n\t \n\treturn -EPERM;\n}\n\nstatic int gaudi_hbm_read_interrupts(struct hl_device *hdev, int device,\n\t\t\tstruct hl_eq_hbm_ecc_data *hbm_ecc_data)\n{\n\tu32 base, val, val2, wr_par, rd_par, ca_par, derr, serr, type, ch;\n\tint rc = 0;\n\n\tif (hdev->asic_prop.fw_app_cpu_boot_dev_sts0 &\n\t\t\t\t\tCPU_BOOT_DEV_STS0_HBM_ECC_EN) {\n\t\tif (!hbm_ecc_data) {\n\t\t\tdev_err(hdev->dev, \"No FW ECC data\");\n\t\t\treturn 0;\n\t\t}\n\n\t\twr_par = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_WR_PAR_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\trd_par = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_RD_PAR_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\tca_par = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_CA_PAR_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\tderr = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_DERR_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\tserr = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_SERR_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\ttype = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_TYPE_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\t\tch = FIELD_GET(CPUCP_PKT_HBM_ECC_INFO_HBM_CH_MASK,\n\t\t\t\tle32_to_cpu(hbm_ecc_data->hbm_ecc_info));\n\n\t\tdev_err(hdev->dev,\n\t\t\t\"HBM%d pc%d interrupts info: WR_PAR=%d, RD_PAR=%d, CA_PAR=%d, SERR=%d, DERR=%d\\n\",\n\t\t\tdevice, ch, wr_par, rd_par, ca_par, serr, derr);\n\t\tdev_err(hdev->dev,\n\t\t\t\"HBM%d pc%d ECC info: 1ST_ERR_ADDR=0x%x, 1ST_ERR_TYPE=%d, SEC_CONT_CNT=%u, SEC_CNT=%d, DEC_CNT=%d\\n\",\n\t\t\tdevice, ch, hbm_ecc_data->first_addr, type,\n\t\t\thbm_ecc_data->sec_cont_cnt, hbm_ecc_data->sec_cnt,\n\t\t\thbm_ecc_data->dec_cnt);\n\t\treturn 0;\n\t}\n\n\tif (hdev->asic_prop.fw_security_enabled) {\n\t\tdev_info(hdev->dev, \"Cannot access MC regs for ECC data while security is enabled\\n\");\n\t\treturn 0;\n\t}\n\n\tbase = GAUDI_HBM_CFG_BASE + device * GAUDI_HBM_CFG_OFFSET;\n\tfor (ch = 0 ; ch < GAUDI_HBM_CHANNELS ; ch++) {\n\t\tval = RREG32_MASK(base + ch * 0x1000 + 0x06C, 0x0000FFFF);\n\t\tval = (val & 0xFF) | ((val >> 8) & 0xFF);\n\t\tif (val) {\n\t\t\trc = -EIO;\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"HBM%d pc%d interrupts info: WR_PAR=%d, RD_PAR=%d, CA_PAR=%d, SERR=%d, DERR=%d\\n\",\n\t\t\t\tdevice, ch * 2, val & 0x1, (val >> 1) & 0x1,\n\t\t\t\t(val >> 2) & 0x1, (val >> 3) & 0x1,\n\t\t\t\t(val >> 4) & 0x1);\n\n\t\t\tval2 = RREG32(base + ch * 0x1000 + 0x060);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"HBM%d pc%d ECC info: 1ST_ERR_ADDR=0x%x, 1ST_ERR_TYPE=%d, SEC_CONT_CNT=%d, SEC_CNT=%d, DEC_CNT=%d\\n\",\n\t\t\t\tdevice, ch * 2,\n\t\t\t\tRREG32(base + ch * 0x1000 + 0x064),\n\t\t\t\t(val2 & 0x200) >> 9, (val2 & 0xFC00) >> 10,\n\t\t\t\t(val2 & 0xFF0000) >> 16,\n\t\t\t\t(val2 & 0xFF000000) >> 24);\n\t\t}\n\n\t\tval = RREG32_MASK(base + ch * 0x1000 + 0x07C, 0x0000FFFF);\n\t\tval = (val & 0xFF) | ((val >> 8) & 0xFF);\n\t\tif (val) {\n\t\t\trc = -EIO;\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"HBM%d pc%d interrupts info: WR_PAR=%d, RD_PAR=%d, CA_PAR=%d, SERR=%d, DERR=%d\\n\",\n\t\t\t\tdevice, ch * 2 + 1, val & 0x1, (val >> 1) & 0x1,\n\t\t\t\t(val >> 2) & 0x1, (val >> 3) & 0x1,\n\t\t\t\t(val >> 4) & 0x1);\n\n\t\t\tval2 = RREG32(base + ch * 0x1000 + 0x070);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"HBM%d pc%d ECC info: 1ST_ERR_ADDR=0x%x, 1ST_ERR_TYPE=%d, SEC_CONT_CNT=%d, SEC_CNT=%d, DEC_CNT=%d\\n\",\n\t\t\t\tdevice, ch * 2 + 1,\n\t\t\t\tRREG32(base + ch * 0x1000 + 0x074),\n\t\t\t\t(val2 & 0x200) >> 9, (val2 & 0xFC00) >> 10,\n\t\t\t\t(val2 & 0xFF0000) >> 16,\n\t\t\t\t(val2 & 0xFF000000) >> 24);\n\t\t}\n\n\t\t \n\t\tRMWREG32(base + (ch * 0x1000) + 0x060, 0x1C8, 0x1FF);\n\t\tRMWREG32(base + (ch * 0x1000) + 0x070, 0x1C8, 0x1FF);\n\t\tWREG32(base + (ch * 0x1000) + 0x06C, 0x1F1F);\n\t\tWREG32(base + (ch * 0x1000) + 0x07C, 0x1F1F);\n\t\tRMWREG32(base + (ch * 0x1000) + 0x060, 0x0, 0xF);\n\t\tRMWREG32(base + (ch * 0x1000) + 0x070, 0x0, 0xF);\n\t}\n\n\tval  = RREG32(base + 0x8F30);\n\tval2 = RREG32(base + 0x8F34);\n\tif (val | val2) {\n\t\trc = -EIO;\n\t\tdev_err(hdev->dev,\n\t\t\t\"HBM %d MC SRAM SERR info: Reg 0x8F30=0x%x, Reg 0x8F34=0x%x\\n\",\n\t\t\tdevice, val, val2);\n\t}\n\tval  = RREG32(base + 0x8F40);\n\tval2 = RREG32(base + 0x8F44);\n\tif (val | val2) {\n\t\trc = -EIO;\n\t\tdev_err(hdev->dev,\n\t\t\t\"HBM %d MC SRAM DERR info: Reg 0x8F40=0x%x, Reg 0x8F44=0x%x\\n\",\n\t\t\tdevice, val, val2);\n\t}\n\n\treturn rc;\n}\n\nstatic int gaudi_hbm_event_to_dev(u16 hbm_event_type)\n{\n\tswitch (hbm_event_type) {\n\tcase GAUDI_EVENT_HBM0_SPI_0:\n\tcase GAUDI_EVENT_HBM0_SPI_1:\n\t\treturn 0;\n\tcase GAUDI_EVENT_HBM1_SPI_0:\n\tcase GAUDI_EVENT_HBM1_SPI_1:\n\t\treturn 1;\n\tcase GAUDI_EVENT_HBM2_SPI_0:\n\tcase GAUDI_EVENT_HBM2_SPI_1:\n\t\treturn 2;\n\tcase GAUDI_EVENT_HBM3_SPI_0:\n\tcase GAUDI_EVENT_HBM3_SPI_1:\n\t\treturn 3;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic bool gaudi_tpc_read_interrupts(struct hl_device *hdev, u8 tpc_id,\n\t\t\t\t\tchar *interrupt_name)\n{\n\tu32 tpc_offset = tpc_id * TPC_CFG_OFFSET, tpc_interrupts_cause, i;\n\tbool soft_reset_required = false;\n\n\ttpc_interrupts_cause = RREG32(mmTPC0_CFG_TPC_INTR_CAUSE + tpc_offset) &\n\t\t\t\tTPC0_CFG_TPC_INTR_CAUSE_CAUSE_MASK;\n\n\tfor (i = 0 ; i < GAUDI_NUM_OF_TPC_INTR_CAUSE ; i++)\n\t\tif (tpc_interrupts_cause & BIT(i)) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\"TPC%d_%s interrupt cause: %s\\n\",\n\t\t\t\t\ttpc_id, interrupt_name,\n\t\t\t\t\tgaudi_tpc_interrupts_cause[i]);\n\t\t\t \n\t\t\tif (i == 15)\n\t\t\t\tsoft_reset_required = true;\n\t\t}\n\n\t \n\tWREG32(mmTPC0_CFG_TPC_INTR_CAUSE + tpc_offset, 0);\n\n\treturn soft_reset_required;\n}\n\nstatic int tpc_dec_event_to_tpc_id(u16 tpc_dec_event_type)\n{\n\treturn (tpc_dec_event_type - GAUDI_EVENT_TPC0_DEC) >> 1;\n}\n\nstatic int tpc_krn_event_to_tpc_id(u16 tpc_dec_event_type)\n{\n\treturn (tpc_dec_event_type - GAUDI_EVENT_TPC0_KRN_ERR) / 6;\n}\n\nstatic void gaudi_print_clk_change_info(struct hl_device *hdev, u16 event_type, u64 *event_mask)\n{\n\tktime_t zero_time = ktime_set(0, 0);\n\n\tmutex_lock(&hdev->clk_throttling.lock);\n\n\tswitch (event_type) {\n\tcase GAUDI_EVENT_FIX_POWER_ENV_S:\n\t\thdev->clk_throttling.current_reason |= HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.aggregated_reason |= HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].start = ktime_get();\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].end = zero_time;\n\t\tdev_info_ratelimited(hdev->dev,\n\t\t\t\"Clock throttling due to power consumption\\n\");\n\t\tbreak;\n\n\tcase GAUDI_EVENT_FIX_POWER_ENV_E:\n\t\thdev->clk_throttling.current_reason &= ~HL_CLK_THROTTLE_POWER;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_POWER].end = ktime_get();\n\t\tdev_info_ratelimited(hdev->dev,\n\t\t\t\"Power envelop is safe, back to optimal clock\\n\");\n\t\tbreak;\n\n\tcase GAUDI_EVENT_FIX_THERMAL_ENV_S:\n\t\thdev->clk_throttling.current_reason |= HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.aggregated_reason |= HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].start = ktime_get();\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].end = zero_time;\n\t\t*event_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tdev_info_ratelimited(hdev->dev,\n\t\t\t\"Clock throttling due to overheating\\n\");\n\t\tbreak;\n\n\tcase GAUDI_EVENT_FIX_THERMAL_ENV_E:\n\t\thdev->clk_throttling.current_reason &= ~HL_CLK_THROTTLE_THERMAL;\n\t\thdev->clk_throttling.timestamp[HL_CLK_THROTTLE_TYPE_THERMAL].end = ktime_get();\n\t\t*event_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tdev_info_ratelimited(hdev->dev,\n\t\t\t\"Thermal envelop is safe, back to optimal clock\\n\");\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(hdev->dev, \"Received invalid clock change event %d\\n\",\n\t\t\tevent_type);\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&hdev->clk_throttling.lock);\n}\n\nstatic void gaudi_handle_eqe(struct hl_device *hdev, struct hl_eq_entry *eq_entry)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct hl_info_fw_err_info fw_err_info;\n\tu64 data = le64_to_cpu(eq_entry->data[0]), event_mask = 0;\n\tu32 ctl = le32_to_cpu(eq_entry->hdr.ctl);\n\tu32 fw_fatal_err_flag = 0, flags = 0;\n\tu16 event_type = ((ctl & EQ_CTL_EVENT_TYPE_MASK)\n\t\t\t>> EQ_CTL_EVENT_TYPE_SHIFT);\n\tbool reset_required, reset_direct = false;\n\tu8 cause;\n\tint rc;\n\n\tif (event_type >= GAUDI_EVENT_SIZE) {\n\t\tdev_err(hdev->dev, \"Event type %u exceeds maximum of %u\",\n\t\t\t\tevent_type, GAUDI_EVENT_SIZE - 1);\n\t\treturn;\n\t}\n\n\tgaudi->events_stat[event_type]++;\n\tgaudi->events_stat_aggregate[event_type]++;\n\n\tswitch (event_type) {\n\tcase GAUDI_EVENT_PCIE_CORE_DERR:\n\tcase GAUDI_EVENT_PCIE_IF_DERR:\n\tcase GAUDI_EVENT_PCIE_PHY_DERR:\n\tcase GAUDI_EVENT_TPC0_DERR ... GAUDI_EVENT_TPC7_DERR:\n\tcase GAUDI_EVENT_MME0_ACC_DERR:\n\tcase GAUDI_EVENT_MME0_SBAB_DERR:\n\tcase GAUDI_EVENT_MME1_ACC_DERR:\n\tcase GAUDI_EVENT_MME1_SBAB_DERR:\n\tcase GAUDI_EVENT_MME2_ACC_DERR:\n\tcase GAUDI_EVENT_MME2_SBAB_DERR:\n\tcase GAUDI_EVENT_MME3_ACC_DERR:\n\tcase GAUDI_EVENT_MME3_SBAB_DERR:\n\tcase GAUDI_EVENT_DMA0_DERR_ECC ... GAUDI_EVENT_DMA7_DERR_ECC:\n\t\tfallthrough;\n\tcase GAUDI_EVENT_CPU_IF_ECC_DERR:\n\tcase GAUDI_EVENT_PSOC_MEM_DERR:\n\tcase GAUDI_EVENT_PSOC_CORESIGHT_DERR:\n\tcase GAUDI_EVENT_SRAM0_DERR ... GAUDI_EVENT_SRAM28_DERR:\n\tcase GAUDI_EVENT_NIC0_DERR ... GAUDI_EVENT_NIC4_DERR:\n\tcase GAUDI_EVENT_DMA_IF0_DERR ... GAUDI_EVENT_DMA_IF3_DERR:\n\tcase GAUDI_EVENT_HBM_0_DERR ... GAUDI_EVENT_HBM_3_DERR:\n\tcase GAUDI_EVENT_MMU_DERR:\n\tcase GAUDI_EVENT_NIC0_CS_DBG_DERR ... GAUDI_EVENT_NIC4_CS_DBG_DERR:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\tgaudi_handle_ecc_event(hdev, event_type, &eq_entry->ecc_data);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tfw_fatal_err_flag = HL_DRV_RESET_FW_FATAL_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_GIC500:\n\tcase GAUDI_EVENT_AXI_ECC:\n\tcase GAUDI_EVENT_L2_RAM_ECC:\n\tcase GAUDI_EVENT_PLL0 ... GAUDI_EVENT_PLL17:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tfw_fatal_err_flag = HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_HBM0_SPI_0:\n\tcase GAUDI_EVENT_HBM1_SPI_0:\n\tcase GAUDI_EVENT_HBM2_SPI_0:\n\tcase GAUDI_EVENT_HBM3_SPI_0:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tgaudi_hbm_read_interrupts(hdev,\n\t\t\t\tgaudi_hbm_event_to_dev(event_type),\n\t\t\t\t&eq_entry->hbm_ecc_data);\n\t\tfw_fatal_err_flag = HL_DRV_RESET_FW_FATAL_ERR;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_HBM0_SPI_1:\n\tcase GAUDI_EVENT_HBM1_SPI_1:\n\tcase GAUDI_EVENT_HBM2_SPI_1:\n\tcase GAUDI_EVENT_HBM3_SPI_1:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tgaudi_hbm_read_interrupts(hdev,\n\t\t\t\tgaudi_hbm_event_to_dev(event_type),\n\t\t\t\t&eq_entry->hbm_ecc_data);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_TPC0_DEC:\n\tcase GAUDI_EVENT_TPC1_DEC:\n\tcase GAUDI_EVENT_TPC2_DEC:\n\tcase GAUDI_EVENT_TPC3_DEC:\n\tcase GAUDI_EVENT_TPC4_DEC:\n\tcase GAUDI_EVENT_TPC5_DEC:\n\tcase GAUDI_EVENT_TPC6_DEC:\n\tcase GAUDI_EVENT_TPC7_DEC:\n\t\t \n\t\tevent_mask |= HL_NOTIFIER_EVENT_TPC_ASSERT;\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\treset_required = gaudi_tpc_read_interrupts(hdev,\n\t\t\t\t\ttpc_dec_event_to_tpc_id(event_type),\n\t\t\t\t\t\"AXI_SLV_DEC_Error\");\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tif (reset_required) {\n\t\t\tdev_err(hdev->dev, \"reset required due to %s\\n\",\n\t\t\t\tgaudi_irq_map_table[event_type].name);\n\n\t\t\treset_direct = true;\n\t\t\tgoto reset_device;\n\t\t} else {\n\t\t\thl_fw_unmask_irq(hdev, event_type);\n\t\t\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_RESET;\n\t\t}\n\t\tbreak;\n\n\tcase GAUDI_EVENT_TPC0_KRN_ERR:\n\tcase GAUDI_EVENT_TPC1_KRN_ERR:\n\tcase GAUDI_EVENT_TPC2_KRN_ERR:\n\tcase GAUDI_EVENT_TPC3_KRN_ERR:\n\tcase GAUDI_EVENT_TPC4_KRN_ERR:\n\tcase GAUDI_EVENT_TPC5_KRN_ERR:\n\tcase GAUDI_EVENT_TPC6_KRN_ERR:\n\tcase GAUDI_EVENT_TPC7_KRN_ERR:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\treset_required = gaudi_tpc_read_interrupts(hdev,\n\t\t\t\t\ttpc_krn_event_to_tpc_id(event_type),\n\t\t\t\t\t\"KRN_ERR\");\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tif (reset_required) {\n\t\t\tdev_err(hdev->dev, \"reset required due to %s\\n\",\n\t\t\t\tgaudi_irq_map_table[event_type].name);\n\n\t\t\treset_direct = true;\n\t\t\tgoto reset_device;\n\t\t} else {\n\t\t\thl_fw_unmask_irq(hdev, event_type);\n\t\t\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_RESET;\n\t\t}\n\t\tbreak;\n\n\tcase GAUDI_EVENT_PCIE_CORE_SERR:\n\tcase GAUDI_EVENT_PCIE_IF_SERR:\n\tcase GAUDI_EVENT_PCIE_PHY_SERR:\n\tcase GAUDI_EVENT_TPC0_SERR ... GAUDI_EVENT_TPC7_SERR:\n\tcase GAUDI_EVENT_MME0_ACC_SERR:\n\tcase GAUDI_EVENT_MME0_SBAB_SERR:\n\tcase GAUDI_EVENT_MME1_ACC_SERR:\n\tcase GAUDI_EVENT_MME1_SBAB_SERR:\n\tcase GAUDI_EVENT_MME2_ACC_SERR:\n\tcase GAUDI_EVENT_MME2_SBAB_SERR:\n\tcase GAUDI_EVENT_MME3_ACC_SERR:\n\tcase GAUDI_EVENT_MME3_SBAB_SERR:\n\tcase GAUDI_EVENT_DMA0_SERR_ECC ... GAUDI_EVENT_DMA7_SERR_ECC:\n\tcase GAUDI_EVENT_CPU_IF_ECC_SERR:\n\tcase GAUDI_EVENT_PSOC_MEM_SERR:\n\tcase GAUDI_EVENT_PSOC_CORESIGHT_SERR:\n\tcase GAUDI_EVENT_SRAM0_SERR ... GAUDI_EVENT_SRAM28_SERR:\n\tcase GAUDI_EVENT_NIC0_SERR ... GAUDI_EVENT_NIC4_SERR:\n\tcase GAUDI_EVENT_DMA_IF0_SERR ... GAUDI_EVENT_DMA_IF3_SERR:\n\tcase GAUDI_EVENT_HBM_0_SERR ... GAUDI_EVENT_HBM_3_SERR:\n\t\tfallthrough;\n\tcase GAUDI_EVENT_MMU_SERR:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\tgaudi_handle_ecc_event(hdev, event_type, &eq_entry->ecc_data);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_PCIE_DEC:\n\tcase GAUDI_EVENT_CPU_AXI_SPLITTER:\n\tcase GAUDI_EVENT_PSOC_AXI_DEC:\n\tcase GAUDI_EVENT_PSOC_PRSTN_FALL:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_MMU_PAGE_FAULT:\n\tcase GAUDI_EVENT_MMU_WR_PERM:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_MME0_WBC_RSP:\n\tcase GAUDI_EVENT_MME0_SBAB0_RSP:\n\tcase GAUDI_EVENT_MME1_WBC_RSP:\n\tcase GAUDI_EVENT_MME1_SBAB0_RSP:\n\tcase GAUDI_EVENT_MME2_WBC_RSP:\n\tcase GAUDI_EVENT_MME2_SBAB0_RSP:\n\tcase GAUDI_EVENT_MME3_WBC_RSP:\n\tcase GAUDI_EVENT_MME3_SBAB0_RSP:\n\tcase GAUDI_EVENT_RAZWI_OR_ADC:\n\tcase GAUDI_EVENT_MME0_QM ... GAUDI_EVENT_MME2_QM:\n\tcase GAUDI_EVENT_DMA0_QM ... GAUDI_EVENT_DMA7_QM:\n\t\tfallthrough;\n\tcase GAUDI_EVENT_NIC0_QM0:\n\tcase GAUDI_EVENT_NIC0_QM1:\n\tcase GAUDI_EVENT_NIC1_QM0:\n\tcase GAUDI_EVENT_NIC1_QM1:\n\tcase GAUDI_EVENT_NIC2_QM0:\n\tcase GAUDI_EVENT_NIC2_QM1:\n\tcase GAUDI_EVENT_NIC3_QM0:\n\tcase GAUDI_EVENT_NIC3_QM1:\n\tcase GAUDI_EVENT_NIC4_QM0:\n\tcase GAUDI_EVENT_NIC4_QM1:\n\tcase GAUDI_EVENT_DMA0_CORE ... GAUDI_EVENT_DMA7_CORE:\n\tcase GAUDI_EVENT_TPC0_QM ... GAUDI_EVENT_TPC7_QM:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\tgaudi_handle_qman_err(hdev, event_type, &event_mask);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= (HL_NOTIFIER_EVENT_USER_ENGINE_ERR | HL_NOTIFIER_EVENT_DEVICE_RESET);\n\t\tbreak;\n\n\tcase GAUDI_EVENT_RAZWI_OR_ADC_SW:\n\t\tgaudi_print_irq_info(hdev, event_type, true, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_TPC0_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC1_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC2_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC3_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC4_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC5_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC6_BMON_SPMU:\n\tcase GAUDI_EVENT_TPC7_BMON_SPMU:\n\tcase GAUDI_EVENT_DMA_BM_CH0 ... GAUDI_EVENT_DMA_BM_CH7:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_NIC_SEI_0 ... GAUDI_EVENT_NIC_SEI_4:\n\t\tgaudi_print_nic_axi_irq_info(hdev, event_type, &data);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_DMA_IF_SEI_0 ... GAUDI_EVENT_DMA_IF_SEI_3:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tgaudi_print_sm_sei_info(hdev, event_type,\n\t\t\t\t\t&eq_entry->sm_sei_data);\n\t\trc = hl_state_dump(hdev);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tif (rc)\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Error during system state dump %d\\n\", rc);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tbreak;\n\n\tcase GAUDI_EVENT_STATUS_NIC0_ENG0 ... GAUDI_EVENT_STATUS_NIC4_ENG1:\n\t\tbreak;\n\n\tcase GAUDI_EVENT_FIX_POWER_ENV_S ... GAUDI_EVENT_FIX_THERMAL_ENV_E:\n\t\tgaudi_print_clk_change_info(hdev, event_type, &event_mask);\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\tbreak;\n\n\tcase GAUDI_EVENT_PSOC_GPIO_U16_0:\n\t\tcause = le64_to_cpu(eq_entry->data[0]) & 0xFF;\n\t\tdev_err(hdev->dev,\n\t\t\t\"Received high temp H/W interrupt %d (cause %d)\\n\",\n\t\t\tevent_type, cause);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_USER_ENGINE_ERR;\n\t\tbreak;\n\n\tcase GAUDI_EVENT_DEV_RESET_REQ:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_PKT_QUEUE_OUT_SYNC:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tgaudi_print_out_of_sync_info(hdev, &eq_entry->pkt_sync_err);\n\t\tevent_mask |= HL_NOTIFIER_EVENT_GENERAL_HW_ERR;\n\t\tgoto reset_device;\n\n\tcase GAUDI_EVENT_FW_ALIVE_S:\n\t\tgaudi_print_irq_info(hdev, event_type, false, &event_mask);\n\t\tgaudi_print_fw_alive_info(hdev, &eq_entry->fw_alive);\n\t\tfw_err_info.err_type = HL_INFO_FW_REPORTED_ERR;\n\t\tfw_err_info.event_id = event_type;\n\t\tfw_err_info.event_mask = &event_mask;\n\t\thl_handle_fw_err(hdev, &fw_err_info);\n\t\tgoto reset_device;\n\n\tdefault:\n\t\tdev_err(hdev->dev, \"Received invalid H/W interrupt %d\\n\",\n\t\t\t\tevent_type);\n\t\tbreak;\n\t}\n\n\tif (event_mask)\n\t\thl_notifier_event_send_all(hdev, event_mask);\n\n\treturn;\n\nreset_device:\n\treset_required = true;\n\n\tif (hdev->asic_prop.fw_security_enabled && !reset_direct) {\n\t\tflags = HL_DRV_RESET_HARD | HL_DRV_RESET_BYPASS_REQ_TO_FW | fw_fatal_err_flag;\n\n\t\t \n\t\tevent_mask |= (HL_NOTIFIER_EVENT_DEVICE_RESET |\n\t\t\t\t\tHL_NOTIFIER_EVENT_DEVICE_UNAVAILABLE);\n\t} else if (hdev->hard_reset_on_fw_events) {\n\t\tflags = HL_DRV_RESET_HARD | HL_DRV_RESET_DELAY | fw_fatal_err_flag;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_RESET;\n\t} else {\n\t\treset_required = false;\n\t}\n\n\tif (reset_required) {\n\t\t \n\t\tif (event_mask & HL_NOTIFIER_EVENT_GENERAL_HW_ERR)\n\t\t\thl_handle_critical_hw_err(hdev, event_type, &event_mask);\n\n\t\thl_device_cond_reset(hdev, flags, event_mask);\n\t} else {\n\t\thl_fw_unmask_irq(hdev, event_type);\n\t\t \n\t\tif (event_mask)\n\t\t\thl_notifier_event_send_all(hdev, event_mask);\n\t}\n}\n\nstatic void *gaudi_get_events_stat(struct hl_device *hdev, bool aggregate, u32 *size)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (aggregate) {\n\t\t*size = (u32) sizeof(gaudi->events_stat_aggregate);\n\t\treturn gaudi->events_stat_aggregate;\n\t}\n\n\t*size = (u32) sizeof(gaudi->events_stat);\n\treturn gaudi->events_stat;\n}\n\nstatic int gaudi_mmu_invalidate_cache(struct hl_device *hdev, bool is_hard, u32 flags)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tu32 status, timeout_usec;\n\tint rc;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU) ||\n\t\thdev->reset_info.hard_reset_pending)\n\t\treturn 0;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI_PLDM_MMU_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = MMU_CONFIG_TIMEOUT_USEC;\n\n\t \n\tWREG32(mmSTLB_INV_PS, 3);\n\tWREG32(mmSTLB_CACHE_INV, gaudi->mmu_cache_inv_pi++);\n\tWREG32(mmSTLB_INV_PS, 2);\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tmmSTLB_INV_PS,\n\t\tstatus,\n\t\t!status,\n\t\t1000,\n\t\ttimeout_usec);\n\n\tWREG32(mmSTLB_INV_SET, 0);\n\n\treturn rc;\n}\n\nstatic int gaudi_mmu_invalidate_cache_range(struct hl_device *hdev,\n\t\t\t\t\t\tbool is_hard, u32 flags,\n\t\t\t\t\t\tu32 asid, u64 va, u64 size)\n{\n\t \n\treturn hdev->asic_funcs->mmu_invalidate_cache(hdev, is_hard, flags);\n}\n\nstatic int gaudi_mmu_update_asid_hop0_addr(struct hl_device *hdev, u32 asid, u64 phys_addr)\n{\n\tu32 status, timeout_usec;\n\tint rc;\n\n\tif (hdev->pldm)\n\t\ttimeout_usec = GAUDI_PLDM_MMU_TIMEOUT_USEC;\n\telse\n\t\ttimeout_usec = MMU_CONFIG_TIMEOUT_USEC;\n\n\tWREG32(MMU_ASID, asid);\n\tWREG32(MMU_HOP0_PA43_12, phys_addr >> MMU_HOP0_PA43_12_SHIFT);\n\tWREG32(MMU_HOP0_PA49_44, phys_addr >> MMU_HOP0_PA49_44_SHIFT);\n\tWREG32(MMU_BUSY, 0x80000000);\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tMMU_BUSY,\n\t\tstatus,\n\t\t!(status & 0x80000000),\n\t\t1000,\n\t\ttimeout_usec);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Timeout during MMU hop0 config of asid %d\\n\", asid);\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi_send_heartbeat(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_send_heartbeat(hdev);\n}\n\nstatic int gaudi_cpucp_info_get(struct hl_device *hdev)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint rc;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\trc = hl_fw_cpucp_handshake(hdev, mmCPU_BOOT_DEV_STS0,\n\t\t\t\t\tmmCPU_BOOT_DEV_STS1, mmCPU_BOOT_ERR0,\n\t\t\t\t\tmmCPU_BOOT_ERR1);\n\tif (rc)\n\t\treturn rc;\n\n\tif (!strlen(prop->cpucp_info.card_name))\n\t\tstrncpy(prop->cpucp_info.card_name, GAUDI_DEFAULT_CARD_NAME,\n\t\t\t\tCARD_NAME_MAX_LEN);\n\n\thdev->card_type = le32_to_cpu(hdev->asic_prop.cpucp_info.card_type);\n\n\tset_default_power_values(hdev);\n\n\treturn 0;\n}\n\nstatic bool gaudi_is_device_idle(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\tstruct engines_data *e)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tconst char *fmt = \"%-5d%-9s%#-14x%#-12x%#x\\n\";\n\tconst char *mme_slave_fmt = \"%-5d%-9s%-14s%-12s%#x\\n\";\n\tconst char *nic_fmt = \"%-5d%-9s%#-14x%#x\\n\";\n\tunsigned long *mask = (unsigned long *)mask_arr;\n\tu32 qm_glbl_sts0, qm_cgm_sts, dma_core_sts0, tpc_cfg_sts, mme_arch_sts;\n\tbool is_idle = true, is_eng_idle, is_slave;\n\tu64 offset;\n\tint i, dma_id, port;\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nDMA  is_idle  QM_GLBL_STS0  QM_CGM_STS  DMA_CORE_STS0\\n\"\n\t\t\t\"---  -------  ------------  ----------  -------------\\n\");\n\n\tfor (i = 0 ; i < DMA_NUMBER_OF_CHNLS ; i++) {\n\t\tdma_id = gaudi_dma_assignment[i];\n\t\toffset = dma_id * DMA_QMAN_OFFSET;\n\n\t\tqm_glbl_sts0 = RREG32(mmDMA0_QM_GLBL_STS0 + offset);\n\t\tqm_cgm_sts = RREG32(mmDMA0_QM_CGM_STS + offset);\n\t\tdma_core_sts0 = RREG32(mmDMA0_CORE_STS0 + offset);\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts) &&\n\t\t\t\tIS_DMA_IDLE(dma_core_sts0);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(GAUDI_ENGINE_ID_DMA_0 + dma_id, mask);\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, fmt, dma_id,\n\t\t\t\tis_eng_idle ? \"Y\" : \"N\", qm_glbl_sts0,\n\t\t\t\tqm_cgm_sts, dma_core_sts0);\n\t}\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nTPC  is_idle  QM_GLBL_STS0  QM_CGM_STS  CFG_STATUS\\n\"\n\t\t\t\"---  -------  ------------  ----------  ----------\\n\");\n\n\tfor (i = 0 ; i < TPC_NUMBER_OF_ENGINES ; i++) {\n\t\toffset = i * TPC_QMAN_OFFSET;\n\t\tqm_glbl_sts0 = RREG32(mmTPC0_QM_GLBL_STS0 + offset);\n\t\tqm_cgm_sts = RREG32(mmTPC0_QM_CGM_STS + offset);\n\t\ttpc_cfg_sts = RREG32(mmTPC0_CFG_STATUS + offset);\n\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts) &&\n\t\t\t\tIS_TPC_IDLE(tpc_cfg_sts);\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(GAUDI_ENGINE_ID_TPC_0 + i, mask);\n\t\tif (e)\n\t\t\thl_engine_data_sprintf(e, fmt, i,\n\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\tqm_glbl_sts0, qm_cgm_sts, tpc_cfg_sts);\n\t}\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\"\\nMME  is_idle  QM_GLBL_STS0  QM_CGM_STS  ARCH_STATUS\\n\"\n\t\t\t\"---  -------  ------------  ----------  -----------\\n\");\n\n\tfor (i = 0 ; i < MME_NUMBER_OF_ENGINES ; i++) {\n\t\toffset = i * MME_QMAN_OFFSET;\n\t\tmme_arch_sts = RREG32(mmMME0_CTRL_ARCH_STATUS + offset);\n\t\tis_eng_idle = IS_MME_IDLE(mme_arch_sts);\n\n\t\t \n\t\tis_slave = i % 2;\n\t\tif (!is_slave) {\n\t\t\tqm_glbl_sts0 = RREG32(mmMME0_QM_GLBL_STS0 + offset);\n\t\t\tqm_cgm_sts = RREG32(mmMME0_QM_CGM_STS + offset);\n\t\t\tis_eng_idle &= IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts);\n\t\t}\n\n\t\tis_idle &= is_eng_idle;\n\n\t\tif (mask && !is_eng_idle)\n\t\t\tset_bit(GAUDI_ENGINE_ID_MME_0 + i, mask);\n\t\tif (e) {\n\t\t\tif (!is_slave)\n\t\t\t\thl_engine_data_sprintf(e, fmt, i,\n\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\tqm_glbl_sts0, qm_cgm_sts, mme_arch_sts);\n\t\t\telse\n\t\t\t\thl_engine_data_sprintf(e, mme_slave_fmt, i,\n\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\", \"-\",\n\t\t\t\t\t\"-\", mme_arch_sts);\n\t\t}\n\t}\n\n\tif (e)\n\t\thl_engine_data_sprintf(e,\n\t\t\t\t\"\\nNIC  is_idle  QM_GLBL_STS0  QM_CGM_STS\\n\"\n\t\t\t\t\"---  -------  ------------  ----------\\n\");\n\n\tfor (i = 0 ; i < (NIC_NUMBER_OF_ENGINES / 2) ; i++) {\n\t\toffset = i * NIC_MACRO_QMAN_OFFSET;\n\t\tport = 2 * i;\n\t\tif (gaudi->hw_cap_initialized & BIT(HW_CAP_NIC_SHIFT + port)) {\n\t\t\tqm_glbl_sts0 = RREG32(mmNIC0_QM0_GLBL_STS0 + offset);\n\t\t\tqm_cgm_sts = RREG32(mmNIC0_QM0_CGM_STS + offset);\n\t\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts);\n\t\t\tis_idle &= is_eng_idle;\n\n\t\t\tif (mask && !is_eng_idle)\n\t\t\t\tset_bit(GAUDI_ENGINE_ID_NIC_0 + port, mask);\n\t\t\tif (e)\n\t\t\t\thl_engine_data_sprintf(e, nic_fmt, port,\n\t\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\tqm_glbl_sts0, qm_cgm_sts);\n\t\t}\n\n\t\tport = 2 * i + 1;\n\t\tif (gaudi->hw_cap_initialized & BIT(HW_CAP_NIC_SHIFT + port)) {\n\t\t\tqm_glbl_sts0 = RREG32(mmNIC0_QM1_GLBL_STS0 + offset);\n\t\t\tqm_cgm_sts = RREG32(mmNIC0_QM1_CGM_STS + offset);\n\t\t\tis_eng_idle = IS_QM_IDLE(qm_glbl_sts0, qm_cgm_sts);\n\t\t\tis_idle &= is_eng_idle;\n\n\t\t\tif (mask && !is_eng_idle)\n\t\t\t\tset_bit(GAUDI_ENGINE_ID_NIC_0 + port, mask);\n\t\t\tif (e)\n\t\t\t\thl_engine_data_sprintf(e, nic_fmt, port,\n\t\t\t\t\t\tis_eng_idle ? \"Y\" : \"N\",\n\t\t\t\t\t\tqm_glbl_sts0, qm_cgm_sts);\n\t\t}\n\t}\n\n\tif (e)\n\t\thl_engine_data_sprintf(e, \"\\n\");\n\n\treturn is_idle;\n}\n\nstatic void gaudi_hw_queues_lock(struct hl_device *hdev)\n\t__acquires(&gaudi->hw_queues_lock)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tspin_lock(&gaudi->hw_queues_lock);\n}\n\nstatic void gaudi_hw_queues_unlock(struct hl_device *hdev)\n\t__releases(&gaudi->hw_queues_lock)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tspin_unlock(&gaudi->hw_queues_lock);\n}\n\nstatic u32 gaudi_get_pci_id(struct hl_device *hdev)\n{\n\treturn hdev->pdev->device;\n}\n\nstatic int gaudi_get_eeprom_data(struct hl_device *hdev, void *data,\n\t\t\t\tsize_t max_size)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_get_eeprom_data(hdev, data, max_size);\n}\n\nstatic int gaudi_get_monitor_dump(struct hl_device *hdev, void *data)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_CPU_Q))\n\t\treturn 0;\n\n\treturn hl_fw_get_monitor_dump(hdev, data);\n}\n\n \nstatic int gaudi_run_tpc_kernel(struct hl_device *hdev, u64 tpc_kernel,\tu32 tpc_id)\n{\n\tu64 kernel_timeout;\n\tu32 status, offset;\n\tint rc;\n\n\toffset = tpc_id * (mmTPC1_CFG_STATUS - mmTPC0_CFG_STATUS);\n\n\tif (hdev->pldm)\n\t\tkernel_timeout = GAUDI_PLDM_TPC_KERNEL_WAIT_USEC;\n\telse\n\t\tkernel_timeout = HL_DEVICE_TIMEOUT_USEC;\n\n\tWREG32(mmTPC0_CFG_QM_KERNEL_BASE_ADDRESS_LOW + offset,\n\t\t\tlower_32_bits(tpc_kernel));\n\tWREG32(mmTPC0_CFG_QM_KERNEL_BASE_ADDRESS_HIGH + offset,\n\t\t\tupper_32_bits(tpc_kernel));\n\n\tWREG32(mmTPC0_CFG_ICACHE_BASE_ADDERESS_LOW + offset,\n\t\t\tlower_32_bits(tpc_kernel));\n\tWREG32(mmTPC0_CFG_ICACHE_BASE_ADDERESS_HIGH + offset,\n\t\t\tupper_32_bits(tpc_kernel));\n\t \n\tWREG32(mmTPC0_CFG_LUT_FUNC256_BASE_ADDR_LO + offset,\n\t\t\tlower_32_bits(tpc_kernel));\n\tWREG32(mmTPC0_CFG_LUT_FUNC256_BASE_ADDR_HI + offset,\n\t\t\tupper_32_bits(tpc_kernel));\n\n\tWREG32(mmTPC0_CFG_QM_SYNC_OBJECT_ADDR + offset,\n\t\t\tlower_32_bits(CFG_BASE +\n\t\t\t\tmmSYNC_MNGR_E_N_SYNC_MNGR_OBJS_SOB_OBJ_0));\n\n\tWREG32(mmTPC0_CFG_TPC_CMD + offset,\n\t\t\t(1 << TPC0_CFG_TPC_CMD_ICACHE_INVALIDATE_SHIFT |\n\t\t\t1 << TPC0_CFG_TPC_CMD_ICACHE_PREFETCH_64KB_SHIFT));\n\t \n\tusleep_range(1000, 1500);\n\n\t \n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tmmTPC0_CFG_STATUS + offset,\n\t\tstatus,\n\t\t(status & TPC0_CFG_STATUS_VECTOR_PIPE_EMPTY_MASK) ==\n\t\t\t\tTPC0_CFG_STATUS_VECTOR_PIPE_EMPTY_MASK,\n\t\t1000,\n\t\tkernel_timeout);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Timeout while waiting for TPC%d icache prefetch\\n\",\n\t\t\ttpc_id);\n\t\treturn -EIO;\n\t}\n\n\tWREG32(mmTPC0_CFG_TPC_EXECUTE + offset,\n\t\t\t1 << TPC0_CFG_TPC_EXECUTE_V_SHIFT);\n\n\t \n\tusleep_range(1000, 1500);\n\n\t \n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tmmTPC0_CFG_STATUS + offset,\n\t\tstatus,\n\t\t(status & TPC0_CFG_STATUS_VECTOR_PIPE_EMPTY_MASK) ==\n\t\t\t\tTPC0_CFG_STATUS_VECTOR_PIPE_EMPTY_MASK,\n\t\t1000,\n\t\tkernel_timeout);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Timeout while waiting for TPC%d vector pipe\\n\",\n\t\t\ttpc_id);\n\t\treturn -EIO;\n\t}\n\n\trc = hl_poll_timeout(\n\t\thdev,\n\t\tmmTPC0_CFG_WQ_INFLIGHT_CNTR + offset,\n\t\tstatus,\n\t\t(status == 0),\n\t\t1000,\n\t\tkernel_timeout);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Timeout while waiting for TPC%d kernel to execute\\n\",\n\t\t\ttpc_id);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int gaudi_internal_cb_pool_init(struct hl_device *hdev,\n\t\tstruct hl_ctx *ctx)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\tint min_alloc_order, rc, collective_cb_size;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU))\n\t\treturn 0;\n\n\thdev->internal_cb_pool_virt_addr = hl_asic_dma_alloc_coherent(hdev,\n\t\t\t\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ,\n\t\t\t\t\t\t\t&hdev->internal_cb_pool_dma_addr,\n\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\n\tif (!hdev->internal_cb_pool_virt_addr)\n\t\treturn -ENOMEM;\n\n\tcollective_cb_size = sizeof(struct packet_msg_short) * 5 +\n\t\t\tsizeof(struct packet_fence);\n\tmin_alloc_order = ilog2(collective_cb_size);\n\n\thdev->internal_cb_pool = gen_pool_create(min_alloc_order, -1);\n\tif (!hdev->internal_cb_pool) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to create internal CB pool\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto free_internal_cb_pool;\n\t}\n\n\trc = gen_pool_add(hdev->internal_cb_pool,\n\t\t\t\t(uintptr_t) hdev->internal_cb_pool_virt_addr,\n\t\t\t\tHOST_SPACE_INTERNAL_CB_SZ, -1);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to add memory to internal CB pool\\n\");\n\t\trc = -EFAULT;\n\t\tgoto destroy_internal_cb_pool;\n\t}\n\n\thdev->internal_cb_va_base = hl_reserve_va_block(hdev, ctx,\n\t\t\tHL_VA_RANGE_TYPE_HOST, HOST_SPACE_INTERNAL_CB_SZ,\n\t\t\tHL_MMU_VA_ALIGNMENT_NOT_NEEDED);\n\n\tif (!hdev->internal_cb_va_base) {\n\t\trc = -ENOMEM;\n\t\tgoto destroy_internal_cb_pool;\n\t}\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = hl_mmu_map_contiguous(ctx, hdev->internal_cb_va_base,\n\t\t\thdev->internal_cb_pool_dma_addr,\n\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\n\tif (rc)\n\t\tgoto unreserve_internal_cb_pool;\n\n\trc = hl_mmu_invalidate_cache(hdev, false, MMU_OP_USERPTR);\n\tif (rc)\n\t\tgoto unmap_internal_cb_pool;\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\treturn 0;\n\nunmap_internal_cb_pool:\n\thl_mmu_unmap_contiguous(ctx, hdev->internal_cb_va_base,\n\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\nunreserve_internal_cb_pool:\n\tmutex_unlock(&hdev->mmu_lock);\n\thl_unreserve_va_block(hdev, ctx, hdev->internal_cb_va_base,\n\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\ndestroy_internal_cb_pool:\n\tgen_pool_destroy(hdev->internal_cb_pool);\nfree_internal_cb_pool:\n\thl_asic_dma_free_coherent(hdev, HOST_SPACE_INTERNAL_CB_SZ, hdev->internal_cb_pool_virt_addr,\n\t\t\t\t\thdev->internal_cb_pool_dma_addr);\n\n\treturn rc;\n}\n\nstatic void gaudi_internal_cb_pool_fini(struct hl_device *hdev,\n\t\tstruct hl_ctx *ctx)\n{\n\tstruct gaudi_device *gaudi = hdev->asic_specific;\n\n\tif (!(gaudi->hw_cap_initialized & HW_CAP_MMU))\n\t\treturn;\n\n\tmutex_lock(&hdev->mmu_lock);\n\thl_mmu_unmap_contiguous(ctx, hdev->internal_cb_va_base,\n\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\n\thl_unreserve_va_block(hdev, ctx, hdev->internal_cb_va_base,\n\t\t\tHOST_SPACE_INTERNAL_CB_SZ);\n\thl_mmu_invalidate_cache(hdev, true, MMU_OP_USERPTR);\n\tmutex_unlock(&hdev->mmu_lock);\n\n\tgen_pool_destroy(hdev->internal_cb_pool);\n\n\thl_asic_dma_free_coherent(hdev, HOST_SPACE_INTERNAL_CB_SZ, hdev->internal_cb_pool_virt_addr,\n\t\t\t\t\thdev->internal_cb_pool_dma_addr);\n}\n\nstatic int gaudi_ctx_init(struct hl_ctx *ctx)\n{\n\tint rc;\n\n\tif (ctx->asid == HL_KERNEL_ASID_ID)\n\t\treturn 0;\n\n\trc = gaudi_internal_cb_pool_init(ctx->hdev, ctx);\n\tif (rc)\n\t\treturn rc;\n\n\trc = gaudi_restore_user_registers(ctx->hdev);\n\tif (rc)\n\t\tgaudi_internal_cb_pool_fini(ctx->hdev, ctx);\n\n\treturn rc;\n}\n\nstatic void gaudi_ctx_fini(struct hl_ctx *ctx)\n{\n\tif (ctx->asid == HL_KERNEL_ASID_ID)\n\t\treturn;\n\n\tgaudi_internal_cb_pool_fini(ctx->hdev, ctx);\n}\n\nstatic int gaudi_pre_schedule_cs(struct hl_cs *cs)\n{\n\treturn 0;\n}\n\nstatic u32 gaudi_get_queue_id_for_cq(struct hl_device *hdev, u32 cq_idx)\n{\n\treturn gaudi_cq_assignment[cq_idx];\n}\n\nstatic u32 gaudi_get_signal_cb_size(struct hl_device *hdev)\n{\n\treturn sizeof(struct packet_msg_short) +\n\t\t\tsizeof(struct packet_msg_prot) * 2;\n}\n\nstatic u32 gaudi_get_wait_cb_size(struct hl_device *hdev)\n{\n\treturn sizeof(struct packet_msg_short) * 4 +\n\t\t\tsizeof(struct packet_fence) +\n\t\t\tsizeof(struct packet_msg_prot) * 2;\n}\n\nstatic u32 gaudi_get_sob_addr(struct hl_device *hdev, u32 sob_id)\n{\n\treturn mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0 + (sob_id * 4);\n}\n\nstatic u32 gaudi_gen_signal_cb(struct hl_device *hdev, void *data, u16 sob_id,\n\t\t\t\tu32 size, bool eb)\n{\n\tstruct hl_cb *cb = (struct hl_cb *) data;\n\tstruct packet_msg_short *pkt;\n\tu32 value, ctl, pkt_size = sizeof(*pkt);\n\n\tpkt = cb->kernel_address + size;\n\tmemset(pkt, 0, pkt_size);\n\n\t \n\tvalue = FIELD_PREP(GAUDI_PKT_SHORT_VAL_SOB_SYNC_VAL_MASK, 1);\n\tvalue |= FIELD_PREP(GAUDI_PKT_SHORT_VAL_SOB_MOD_MASK, 1);\n\n\tctl = FIELD_PREP(GAUDI_PKT_SHORT_CTL_ADDR_MASK, sob_id * 4);\n\tctl |= FIELD_PREP(GAUDI_PKT_SHORT_CTL_OP_MASK, 0);  \n\tctl |= FIELD_PREP(GAUDI_PKT_SHORT_CTL_BASE_MASK, 3);  \n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, eb);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn size + pkt_size;\n}\n\nstatic u32 gaudi_add_mon_msg_short(struct packet_msg_short *pkt, u32 value,\n\t\t\t\t\tu16 addr)\n{\n\tu32 ctl, pkt_size = sizeof(*pkt);\n\n\tmemset(pkt, 0, pkt_size);\n\n\tctl = FIELD_PREP(GAUDI_PKT_SHORT_CTL_ADDR_MASK, addr);\n\tctl |= FIELD_PREP(GAUDI_PKT_SHORT_CTL_BASE_MASK, 2);   \n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 0);  \n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic u32 gaudi_add_arm_monitor_pkt(struct hl_device *hdev,\n\t\tstruct packet_msg_short *pkt, u16 sob_base, u8 sob_mask,\n\t\tu16 sob_val, u16 mon_id)\n{\n\tu64 monitor_base;\n\tu32 ctl, value, pkt_size = sizeof(*pkt);\n\tu16 msg_addr_offset;\n\tu8 mask;\n\n\tif (hl_gen_sob_mask(sob_base, sob_mask, &mask)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"sob_base %u (mask %#x) is not valid\\n\",\n\t\t\tsob_base, sob_mask);\n\t\treturn 0;\n\t}\n\n\t \n\tmonitor_base = mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0;\n\n\tmsg_addr_offset =\n\t\t(mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0 + mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tmemset(pkt, 0, pkt_size);\n\n\t \n\tvalue = FIELD_PREP(GAUDI_PKT_SHORT_VAL_MON_SYNC_GID_MASK, sob_base / 8);\n\tvalue |= FIELD_PREP(GAUDI_PKT_SHORT_VAL_MON_SYNC_VAL_MASK, sob_val);\n\tvalue |= FIELD_PREP(GAUDI_PKT_SHORT_VAL_MON_MODE_MASK,\n\t\t\t0);  \n\tvalue |= FIELD_PREP(GAUDI_PKT_SHORT_VAL_MON_MASK_MASK, mask);\n\n\tctl = FIELD_PREP(GAUDI_PKT_SHORT_CTL_ADDR_MASK, msg_addr_offset);\n\tctl |= FIELD_PREP(GAUDI_PKT_SHORT_CTL_OP_MASK, 0);  \n\tctl |= FIELD_PREP(GAUDI_PKT_SHORT_CTL_BASE_MASK, 2);  \n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_MSG_SHORT);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tpkt->value = cpu_to_le32(value);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic u32 gaudi_add_fence_pkt(struct packet_fence *pkt)\n{\n\tu32 ctl, cfg, pkt_size = sizeof(*pkt);\n\n\tmemset(pkt, 0, pkt_size);\n\n\tcfg = FIELD_PREP(GAUDI_PKT_FENCE_CFG_DEC_VAL_MASK, 1);\n\tcfg |= FIELD_PREP(GAUDI_PKT_FENCE_CFG_TARGET_VAL_MASK, 1);\n\tcfg |= FIELD_PREP(GAUDI_PKT_FENCE_CFG_ID_MASK, 2);\n\n\tctl = FIELD_PREP(GAUDI_PKT_CTL_OPCODE_MASK, PACKET_FENCE);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_EB_MASK, 0);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_RB_MASK, 1);\n\tctl |= FIELD_PREP(GAUDI_PKT_CTL_MB_MASK, 1);\n\n\tpkt->cfg = cpu_to_le32(cfg);\n\tpkt->ctl = cpu_to_le32(ctl);\n\n\treturn pkt_size;\n}\n\nstatic int gaudi_get_fence_addr(struct hl_device *hdev, u32 queue_id, u64 *addr)\n{\n\tu32 offset, nic_index;\n\n\tswitch (queue_id) {\n\tcase GAUDI_QUEUE_ID_DMA_0_0:\n\t\toffset = mmDMA0_QM_CP_FENCE2_RDATA_0;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_0_1:\n\t\toffset = mmDMA0_QM_CP_FENCE2_RDATA_1;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_0_2:\n\t\toffset = mmDMA0_QM_CP_FENCE2_RDATA_2;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_0_3:\n\t\toffset = mmDMA0_QM_CP_FENCE2_RDATA_3;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_1_0:\n\t\toffset = mmDMA1_QM_CP_FENCE2_RDATA_0;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_1_1:\n\t\toffset = mmDMA1_QM_CP_FENCE2_RDATA_1;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_1_2:\n\t\toffset = mmDMA1_QM_CP_FENCE2_RDATA_2;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_1_3:\n\t\toffset = mmDMA1_QM_CP_FENCE2_RDATA_3;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_5_0:\n\t\toffset = mmDMA5_QM_CP_FENCE2_RDATA_0;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_5_1:\n\t\toffset = mmDMA5_QM_CP_FENCE2_RDATA_1;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_5_2:\n\t\toffset = mmDMA5_QM_CP_FENCE2_RDATA_2;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_DMA_5_3:\n\t\toffset = mmDMA5_QM_CP_FENCE2_RDATA_3;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_TPC_7_0:\n\t\toffset = mmTPC7_QM_CP_FENCE2_RDATA_0;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_TPC_7_1:\n\t\toffset = mmTPC7_QM_CP_FENCE2_RDATA_1;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_TPC_7_2:\n\t\toffset = mmTPC7_QM_CP_FENCE2_RDATA_2;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_TPC_7_3:\n\t\toffset = mmTPC7_QM_CP_FENCE2_RDATA_3;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_NIC_0_0:\n\tcase GAUDI_QUEUE_ID_NIC_1_0:\n\tcase GAUDI_QUEUE_ID_NIC_2_0:\n\tcase GAUDI_QUEUE_ID_NIC_3_0:\n\tcase GAUDI_QUEUE_ID_NIC_4_0:\n\tcase GAUDI_QUEUE_ID_NIC_5_0:\n\tcase GAUDI_QUEUE_ID_NIC_6_0:\n\tcase GAUDI_QUEUE_ID_NIC_7_0:\n\tcase GAUDI_QUEUE_ID_NIC_8_0:\n\tcase GAUDI_QUEUE_ID_NIC_9_0:\n\t\tnic_index = (queue_id - GAUDI_QUEUE_ID_NIC_0_0) >> 2;\n\t\toffset = mmNIC0_QM0_CP_FENCE2_RDATA_0 +\n\t\t\t\t(nic_index >> 1) * NIC_MACRO_QMAN_OFFSET +\n\t\t\t\t(nic_index & 0x1) * NIC_ENGINE_QMAN_OFFSET;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_NIC_0_1:\n\tcase GAUDI_QUEUE_ID_NIC_1_1:\n\tcase GAUDI_QUEUE_ID_NIC_2_1:\n\tcase GAUDI_QUEUE_ID_NIC_3_1:\n\tcase GAUDI_QUEUE_ID_NIC_4_1:\n\tcase GAUDI_QUEUE_ID_NIC_5_1:\n\tcase GAUDI_QUEUE_ID_NIC_6_1:\n\tcase GAUDI_QUEUE_ID_NIC_7_1:\n\tcase GAUDI_QUEUE_ID_NIC_8_1:\n\tcase GAUDI_QUEUE_ID_NIC_9_1:\n\t\tnic_index = (queue_id - GAUDI_QUEUE_ID_NIC_0_1) >> 2;\n\t\toffset = mmNIC0_QM0_CP_FENCE2_RDATA_1 +\n\t\t\t\t(nic_index >> 1) * NIC_MACRO_QMAN_OFFSET +\n\t\t\t\t(nic_index & 0x1) * NIC_ENGINE_QMAN_OFFSET;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_NIC_0_2:\n\tcase GAUDI_QUEUE_ID_NIC_1_2:\n\tcase GAUDI_QUEUE_ID_NIC_2_2:\n\tcase GAUDI_QUEUE_ID_NIC_3_2:\n\tcase GAUDI_QUEUE_ID_NIC_4_2:\n\tcase GAUDI_QUEUE_ID_NIC_5_2:\n\tcase GAUDI_QUEUE_ID_NIC_6_2:\n\tcase GAUDI_QUEUE_ID_NIC_7_2:\n\tcase GAUDI_QUEUE_ID_NIC_8_2:\n\tcase GAUDI_QUEUE_ID_NIC_9_2:\n\t\tnic_index = (queue_id - GAUDI_QUEUE_ID_NIC_0_2) >> 2;\n\t\toffset = mmNIC0_QM0_CP_FENCE2_RDATA_2 +\n\t\t\t\t(nic_index >> 1) * NIC_MACRO_QMAN_OFFSET +\n\t\t\t\t(nic_index & 0x1) * NIC_ENGINE_QMAN_OFFSET;\n\t\tbreak;\n\tcase GAUDI_QUEUE_ID_NIC_0_3:\n\tcase GAUDI_QUEUE_ID_NIC_1_3:\n\tcase GAUDI_QUEUE_ID_NIC_2_3:\n\tcase GAUDI_QUEUE_ID_NIC_3_3:\n\tcase GAUDI_QUEUE_ID_NIC_4_3:\n\tcase GAUDI_QUEUE_ID_NIC_5_3:\n\tcase GAUDI_QUEUE_ID_NIC_6_3:\n\tcase GAUDI_QUEUE_ID_NIC_7_3:\n\tcase GAUDI_QUEUE_ID_NIC_8_3:\n\tcase GAUDI_QUEUE_ID_NIC_9_3:\n\t\tnic_index = (queue_id - GAUDI_QUEUE_ID_NIC_0_3) >> 2;\n\t\toffset = mmNIC0_QM0_CP_FENCE2_RDATA_3 +\n\t\t\t\t(nic_index >> 1) * NIC_MACRO_QMAN_OFFSET +\n\t\t\t\t(nic_index & 0x1) * NIC_ENGINE_QMAN_OFFSET;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t*addr = CFG_BASE + offset;\n\n\treturn 0;\n}\n\nstatic u32 gaudi_add_mon_pkts(void *buf, u16 mon_id, u64 fence_addr)\n{\n\tu64 monitor_base;\n\tu32 size = 0;\n\tu16 msg_addr_offset;\n\n\t \n\tmonitor_base = mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0;\n\n\t \n\tmsg_addr_offset =\n\t\t(mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRL_0 + mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi_add_mon_msg_short(buf + size, (u32) fence_addr,\n\t\t\t\t\tmsg_addr_offset);\n\n\t \n\tmsg_addr_offset =\n\t\t(mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_ADDRH_0 + mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi_add_mon_msg_short(buf + size, (u32) (fence_addr >> 32),\n\t\t\t\t\tmsg_addr_offset);\n\n\t \n\tmsg_addr_offset =\n\t\t(mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_PAY_DATA_0 + mon_id * 4) -\n\t\t\t\tmonitor_base;\n\n\tsize += gaudi_add_mon_msg_short(buf + size, 1, msg_addr_offset);\n\n\treturn size;\n}\n\nstatic u32 gaudi_gen_wait_cb(struct hl_device *hdev,\n\t\t\t\tstruct hl_gen_wait_properties *prop)\n{\n\tstruct hl_cb *cb = (struct hl_cb *) prop->data;\n\tvoid *buf = cb->kernel_address;\n\tu64 fence_addr = 0;\n\tu32 size = prop->size;\n\n\tif (gaudi_get_fence_addr(hdev, prop->q_idx, &fence_addr)) {\n\t\tdev_crit(hdev->dev, \"wrong queue id %d for wait packet\\n\",\n\t\t\t\tprop->q_idx);\n\t\treturn 0;\n\t}\n\n\tsize += gaudi_add_mon_pkts(buf + size, prop->mon_id, fence_addr);\n\tsize += gaudi_add_arm_monitor_pkt(hdev, buf + size, prop->sob_base,\n\t\t\tprop->sob_mask, prop->sob_val, prop->mon_id);\n\tsize += gaudi_add_fence_pkt(buf + size);\n\n\treturn size;\n}\n\nstatic void gaudi_reset_sob(struct hl_device *hdev, void *data)\n{\n\tstruct hl_hw_sob *hw_sob = (struct hl_hw_sob *) data;\n\n\tdev_dbg(hdev->dev, \"reset SOB, q_idx: %d, sob_id: %d\\n\", hw_sob->q_idx,\n\t\thw_sob->sob_id);\n\n\tWREG32(mmSYNC_MNGR_W_S_SYNC_MNGR_OBJS_SOB_OBJ_0 +\n\t\t\thw_sob->sob_id * 4, 0);\n\n\tkref_init(&hw_sob->kref);\n}\n\nstatic u64 gaudi_get_device_time(struct hl_device *hdev)\n{\n\tu64 device_time = ((u64) RREG32(mmPSOC_TIMESTAMP_CNTCVU)) << 32;\n\n\treturn device_time | RREG32(mmPSOC_TIMESTAMP_CNTCVL);\n}\n\nstatic int gaudi_get_hw_block_id(struct hl_device *hdev, u64 block_addr,\n\t\t\t\tu32 *block_size, u32 *block_id)\n{\n\treturn -EPERM;\n}\n\nstatic int gaudi_block_mmap(struct hl_device *hdev,\n\t\t\t\tstruct vm_area_struct *vma,\n\t\t\t\tu32 block_id, u32 block_size)\n{\n\treturn -EPERM;\n}\n\nstatic void gaudi_enable_events_from_fw(struct hl_device *hdev)\n{\n\tstruct cpu_dyn_regs *dyn_regs =\n\t\t\t&hdev->fw_loader.dynamic_loader.comm_desc.cpu_dyn_regs;\n\tu32 irq_handler_offset = hdev->asic_prop.gic_interrupts_enable ?\n\t\t\tmmGIC_DISTRIBUTOR__5_GICD_SETSPI_NSR :\n\t\t\tle32_to_cpu(dyn_regs->gic_host_ints_irq);\n\n\tWREG32(irq_handler_offset,\n\t\tgaudi_irq_map_table[GAUDI_EVENT_INTS_REGISTER].cpu_id);\n}\n\nstatic int gaudi_ack_mmu_page_fault_or_access_error(struct hl_device *hdev, u64 mmu_cap_mask)\n{\n\treturn -EINVAL;\n}\n\nstatic int gaudi_map_pll_idx_to_fw_idx(u32 pll_idx)\n{\n\tswitch (pll_idx) {\n\tcase HL_GAUDI_CPU_PLL: return CPU_PLL;\n\tcase HL_GAUDI_PCI_PLL: return PCI_PLL;\n\tcase HL_GAUDI_NIC_PLL: return NIC_PLL;\n\tcase HL_GAUDI_DMA_PLL: return DMA_PLL;\n\tcase HL_GAUDI_MESH_PLL: return MESH_PLL;\n\tcase HL_GAUDI_MME_PLL: return MME_PLL;\n\tcase HL_GAUDI_TPC_PLL: return TPC_PLL;\n\tcase HL_GAUDI_IF_PLL: return IF_PLL;\n\tcase HL_GAUDI_SRAM_PLL: return SRAM_PLL;\n\tcase HL_GAUDI_HBM_PLL: return HBM_PLL;\n\tdefault: return -EINVAL;\n\t}\n}\n\nstatic int gaudi_add_sync_to_engine_map_entry(\n\tstruct hl_sync_to_engine_map *map, u32 reg_value,\n\tenum hl_sync_engine_type engine_type, u32 engine_id)\n{\n\tstruct hl_sync_to_engine_map_entry *entry;\n\n\t \n\tif (reg_value == 0 || reg_value == 0xffffffff)\n\t\treturn 0;\n\treg_value -= lower_32_bits(CFG_BASE);\n\n\t \n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\tentry->engine_type = engine_type;\n\tentry->engine_id = engine_id;\n\tentry->sync_id = reg_value;\n\thash_add(map->tb, &entry->node, reg_value);\n\n\treturn 0;\n}\n\nstatic int gaudi_gen_sync_to_engine_map(struct hl_device *hdev,\n\t\t\t\tstruct hl_sync_to_engine_map *map)\n{\n\tstruct hl_state_dump_specs *sds = &hdev->state_dump_specs;\n\tint i, j, rc;\n\tu32 reg_value;\n\n\t \n\tfor (i = 0; i < sds->props[SP_NUM_OF_TPC_ENGINES]; ++i) {\n\n\t\treg_value = RREG32(sds->props[SP_TPC0_CFG_SO] +\n\t\t\t\t\tsds->props[SP_NEXT_TPC] * i);\n\n\t\trc = gaudi_add_sync_to_engine_map_entry(map, reg_value,\n\t\t\t\t\t\t\tENGINE_TPC, i);\n\t\tif (rc)\n\t\t\tgoto free_sync_to_engine_map;\n\t}\n\n\t \n\tfor (i = 0; i < sds->props[SP_NUM_OF_MME_ENGINES]; ++i) {\n\t\tfor (j = 0; j < sds->props[SP_SUB_MME_ENG_NUM]; ++j) {\n\n\t\t\treg_value = RREG32(sds->props[SP_MME_CFG_SO] +\n\t\t\t\t\t\tsds->props[SP_NEXT_MME] * i +\n\t\t\t\t\t\tj * sizeof(u32));\n\n\t\t\trc = gaudi_add_sync_to_engine_map_entry(\n\t\t\t\tmap, reg_value, ENGINE_MME,\n\t\t\t\ti * sds->props[SP_SUB_MME_ENG_NUM] + j);\n\t\t\tif (rc)\n\t\t\t\tgoto free_sync_to_engine_map;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < sds->props[SP_NUM_OF_DMA_ENGINES]; ++i) {\n\t\treg_value = RREG32(sds->props[SP_DMA_CFG_SO] +\n\t\t\t\t\tsds->props[SP_DMA_QUEUES_OFFSET] * i);\n\t\trc = gaudi_add_sync_to_engine_map_entry(map, reg_value,\n\t\t\t\t\t\t\tENGINE_DMA, i);\n\t\tif (rc)\n\t\t\tgoto free_sync_to_engine_map;\n\t}\n\n\treturn 0;\n\nfree_sync_to_engine_map:\n\thl_state_dump_free_sync_to_engine_map(map);\n\n\treturn rc;\n}\n\nstatic int gaudi_monitor_valid(struct hl_mon_state_dump *mon)\n{\n\treturn FIELD_GET(\n\t\tSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_STATUS_0_VALID_MASK,\n\t\tmon->status);\n}\n\nstatic void gaudi_fill_sobs_from_mon(char *sobs, struct hl_mon_state_dump *mon)\n{\n\tconst size_t max_write = 10;\n\tu32 gid, mask, sob;\n\tint i, offset;\n\n\t \n\tgid = FIELD_GET(SYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0_SID_MASK,\n\t\t\tmon->arm_data);\n\tmask = FIELD_GET(SYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0_MASK_MASK,\n\t\t\tmon->arm_data);\n\n\tfor (i = 0, offset = 0; mask && offset < MONITOR_SOB_STRING_SIZE -\n\t\tmax_write; mask >>= 1, i++) {\n\t\tif (!(mask & 1)) {\n\t\t\tsob = gid * MONITOR_MAX_SOBS + i;\n\n\t\t\tif (offset > 0)\n\t\t\t\toffset += snprintf(sobs + offset, max_write,\n\t\t\t\t\t\t\t\", \");\n\n\t\t\toffset += snprintf(sobs + offset, max_write, \"%u\", sob);\n\t\t}\n\t}\n}\n\nstatic int gaudi_print_single_monitor(char **buf, size_t *size, size_t *offset,\n\t\t\t\tstruct hl_device *hdev,\n\t\t\t\tstruct hl_mon_state_dump *mon)\n{\n\tconst char *name;\n\tchar scratch_buf1[BIN_REG_STRING_SIZE],\n\t\tscratch_buf2[BIN_REG_STRING_SIZE];\n\tchar monitored_sobs[MONITOR_SOB_STRING_SIZE] = {0};\n\n\tname = hl_state_dump_get_monitor_name(hdev, mon);\n\tif (!name)\n\t\tname = \"\";\n\n\tgaudi_fill_sobs_from_mon(monitored_sobs, mon);\n\n\treturn hl_snprintf_resize(\n\t\tbuf, size, offset,\n\t\t\"Mon id: %u%s, wait for group id: %u mask %s to reach val: %u and write %u to address 0x%llx. Pending: %s. Means sync objects [%s] are being monitored.\",\n\t\tmon->id, name,\n\t\tFIELD_GET(SYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0_SID_MASK,\n\t\t\t\tmon->arm_data),\n\t\thl_format_as_binary(\n\t\t\tscratch_buf1, sizeof(scratch_buf1),\n\t\t\tFIELD_GET(\n\t\t\t\tSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0_MASK_MASK,\n\t\t\t\tmon->arm_data)),\n\t\tFIELD_GET(SYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_ARM_0_SOD_MASK,\n\t\t\t\tmon->arm_data),\n\t\tmon->wr_data,\n\t\t(((u64)mon->wr_addr_high) << 32) | mon->wr_addr_low,\n\t\thl_format_as_binary(\n\t\t\tscratch_buf2, sizeof(scratch_buf2),\n\t\t\tFIELD_GET(\n\t\t\t\tSYNC_MNGR_W_S_SYNC_MNGR_OBJS_MON_STATUS_0_PENDING_MASK,\n\t\t\t\tmon->status)),\n\t\tmonitored_sobs);\n}\n\n\nstatic int gaudi_print_fences_single_engine(\n\tstruct hl_device *hdev, u64 base_offset, u64 status_base_offset,\n\tenum hl_sync_engine_type engine_type, u32 engine_id, char **buf,\n\tsize_t *size, size_t *offset)\n{\n\tstruct hl_state_dump_specs *sds = &hdev->state_dump_specs;\n\tint rc = -ENOMEM, i;\n\tu32 *statuses, *fences;\n\n\tstatuses = kcalloc(sds->props[SP_ENGINE_NUM_OF_QUEUES],\n\t\t\tsizeof(*statuses), GFP_KERNEL);\n\tif (!statuses)\n\t\tgoto out;\n\n\tfences = kcalloc(sds->props[SP_ENGINE_NUM_OF_FENCES] *\n\t\t\t\tsds->props[SP_ENGINE_NUM_OF_QUEUES],\n\t\t\t sizeof(*fences), GFP_KERNEL);\n\tif (!fences)\n\t\tgoto free_status;\n\n\tfor (i = 0; i < sds->props[SP_ENGINE_NUM_OF_FENCES]; ++i)\n\t\tstatuses[i] = RREG32(status_base_offset + i * sizeof(u32));\n\n\tfor (i = 0; i < sds->props[SP_ENGINE_NUM_OF_FENCES] *\n\t\t\t\tsds->props[SP_ENGINE_NUM_OF_QUEUES]; ++i)\n\t\tfences[i] = RREG32(base_offset + i * sizeof(u32));\n\n\t \n\tfor (i = 0; i < sds->props[SP_ENGINE_NUM_OF_QUEUES]; ++i) {\n\t\tu32 fence_id;\n\t\tu64 fence_cnt, fence_rdata;\n\t\tconst char *engine_name;\n\n\t\tif (!FIELD_GET(TPC0_QM_CP_STS_0_FENCE_IN_PROGRESS_MASK,\n\t\t\tstatuses[i]))\n\t\t\tcontinue;\n\n\t\tfence_id =\n\t\t\tFIELD_GET(TPC0_QM_CP_STS_0_FENCE_ID_MASK, statuses[i]);\n\t\tfence_cnt = base_offset + CFG_BASE +\n\t\t\tsizeof(u32) *\n\t\t\t(i + fence_id * sds->props[SP_ENGINE_NUM_OF_QUEUES]);\n\t\tfence_rdata = fence_cnt - sds->props[SP_FENCE0_CNT_OFFSET] +\n\t\t\t\tsds->props[SP_FENCE0_RDATA_OFFSET];\n\t\tengine_name = hl_sync_engine_to_string(engine_type);\n\n\t\trc = hl_snprintf_resize(\n\t\t\tbuf, size, offset,\n\t\t\t\"%s%u, stream %u: fence id %u cnt = 0x%llx (%s%u_QM.CP_FENCE%u_CNT_%u) rdata = 0x%llx (%s%u_QM.CP_FENCE%u_RDATA_%u) value = %u, cp_status = %u\\n\",\n\t\t\tengine_name, engine_id,\n\t\t\ti, fence_id,\n\t\t\tfence_cnt, engine_name, engine_id, fence_id, i,\n\t\t\tfence_rdata, engine_name, engine_id, fence_id, i,\n\t\t\tfences[fence_id],\n\t\t\tstatuses[i]);\n\t\tif (rc)\n\t\t\tgoto free_fences;\n\t}\n\n\trc = 0;\n\nfree_fences:\n\tkfree(fences);\nfree_status:\n\tkfree(statuses);\nout:\n\treturn rc;\n}\n\n\nstatic struct hl_state_dump_specs_funcs gaudi_state_dump_funcs = {\n\t.monitor_valid = gaudi_monitor_valid,\n\t.print_single_monitor = gaudi_print_single_monitor,\n\t.gen_sync_to_engine_map = gaudi_gen_sync_to_engine_map,\n\t.print_fences_single_engine = gaudi_print_fences_single_engine,\n};\n\nstatic void gaudi_state_dump_init(struct hl_device *hdev)\n{\n\tstruct hl_state_dump_specs *sds = &hdev->state_dump_specs;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(gaudi_so_id_to_str); ++i)\n\t\thash_add(sds->so_id_to_str_tb,\n\t\t\t&gaudi_so_id_to_str[i].node,\n\t\t\tgaudi_so_id_to_str[i].id);\n\n\tfor (i = 0; i < ARRAY_SIZE(gaudi_monitor_id_to_str); ++i)\n\t\thash_add(sds->monitor_id_to_str_tb,\n\t\t\t&gaudi_monitor_id_to_str[i].node,\n\t\t\tgaudi_monitor_id_to_str[i].id);\n\n\tsds->props = gaudi_state_dump_specs_props;\n\n\tsds->sync_namager_names = gaudi_sync_manager_names;\n\n\tsds->funcs = gaudi_state_dump_funcs;\n}\n\nstatic u32 *gaudi_get_stream_master_qid_arr(void)\n{\n\treturn gaudi_stream_master;\n}\n\nstatic int gaudi_set_dram_properties(struct hl_device *hdev)\n{\n\treturn 0;\n}\n\nstatic int gaudi_set_binning_masks(struct hl_device *hdev)\n{\n\treturn 0;\n}\n\nstatic void gaudi_check_if_razwi_happened(struct hl_device *hdev)\n{\n}\n\nstatic ssize_t infineon_ver_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct hl_device *hdev = dev_get_drvdata(dev);\n\tstruct cpucp_info *cpucp_info;\n\n\tcpucp_info = &hdev->asic_prop.cpucp_info;\n\n\treturn sprintf(buf, \"%#04x\\n\", le32_to_cpu(cpucp_info->infineon_version));\n}\n\nstatic DEVICE_ATTR_RO(infineon_ver);\n\nstatic struct attribute *gaudi_vrm_dev_attrs[] = {\n\t&dev_attr_infineon_ver.attr,\n\tNULL,\n};\n\nstatic void gaudi_add_device_attr(struct hl_device *hdev, struct attribute_group *dev_clk_attr_grp,\n\t\t\t\t\tstruct attribute_group *dev_vrm_attr_grp)\n{\n\thl_sysfs_add_dev_clk_attr(hdev, dev_clk_attr_grp);\n\tdev_vrm_attr_grp->attrs = gaudi_vrm_dev_attrs;\n}\n\nstatic int gaudi_send_device_activity(struct hl_device *hdev, bool open)\n{\n\treturn 0;\n}\n\nstatic const struct hl_asic_funcs gaudi_funcs = {\n\t.early_init = gaudi_early_init,\n\t.early_fini = gaudi_early_fini,\n\t.late_init = gaudi_late_init,\n\t.late_fini = gaudi_late_fini,\n\t.sw_init = gaudi_sw_init,\n\t.sw_fini = gaudi_sw_fini,\n\t.hw_init = gaudi_hw_init,\n\t.hw_fini = gaudi_hw_fini,\n\t.halt_engines = gaudi_halt_engines,\n\t.suspend = gaudi_suspend,\n\t.resume = gaudi_resume,\n\t.mmap = gaudi_mmap,\n\t.ring_doorbell = gaudi_ring_doorbell,\n\t.pqe_write = gaudi_pqe_write,\n\t.asic_dma_alloc_coherent = gaudi_dma_alloc_coherent,\n\t.asic_dma_free_coherent = gaudi_dma_free_coherent,\n\t.scrub_device_mem = gaudi_scrub_device_mem,\n\t.scrub_device_dram = gaudi_scrub_device_dram,\n\t.get_int_queue_base = gaudi_get_int_queue_base,\n\t.test_queues = gaudi_test_queues,\n\t.asic_dma_pool_zalloc = gaudi_dma_pool_zalloc,\n\t.asic_dma_pool_free = gaudi_dma_pool_free,\n\t.cpu_accessible_dma_pool_alloc = gaudi_cpu_accessible_dma_pool_alloc,\n\t.cpu_accessible_dma_pool_free = gaudi_cpu_accessible_dma_pool_free,\n\t.hl_dma_unmap_sgtable = hl_dma_unmap_sgtable,\n\t.cs_parser = gaudi_cs_parser,\n\t.asic_dma_map_sgtable = hl_dma_map_sgtable,\n\t.add_end_of_cb_packets = gaudi_add_end_of_cb_packets,\n\t.update_eq_ci = gaudi_update_eq_ci,\n\t.context_switch = gaudi_context_switch,\n\t.restore_phase_topology = gaudi_restore_phase_topology,\n\t.debugfs_read_dma = gaudi_debugfs_read_dma,\n\t.add_device_attr = gaudi_add_device_attr,\n\t.handle_eqe = gaudi_handle_eqe,\n\t.get_events_stat = gaudi_get_events_stat,\n\t.read_pte = gaudi_read_pte,\n\t.write_pte = gaudi_write_pte,\n\t.mmu_invalidate_cache = gaudi_mmu_invalidate_cache,\n\t.mmu_invalidate_cache_range = gaudi_mmu_invalidate_cache_range,\n\t.mmu_prefetch_cache_range = NULL,\n\t.send_heartbeat = gaudi_send_heartbeat,\n\t.debug_coresight = gaudi_debug_coresight,\n\t.is_device_idle = gaudi_is_device_idle,\n\t.compute_reset_late_init = gaudi_compute_reset_late_init,\n\t.hw_queues_lock = gaudi_hw_queues_lock,\n\t.hw_queues_unlock = gaudi_hw_queues_unlock,\n\t.get_pci_id = gaudi_get_pci_id,\n\t.get_eeprom_data = gaudi_get_eeprom_data,\n\t.get_monitor_dump = gaudi_get_monitor_dump,\n\t.send_cpu_message = gaudi_send_cpu_message,\n\t.pci_bars_map = gaudi_pci_bars_map,\n\t.init_iatu = gaudi_init_iatu,\n\t.rreg = hl_rreg,\n\t.wreg = hl_wreg,\n\t.halt_coresight = gaudi_halt_coresight,\n\t.ctx_init = gaudi_ctx_init,\n\t.ctx_fini = gaudi_ctx_fini,\n\t.pre_schedule_cs = gaudi_pre_schedule_cs,\n\t.get_queue_id_for_cq = gaudi_get_queue_id_for_cq,\n\t.load_firmware_to_device = gaudi_load_firmware_to_device,\n\t.load_boot_fit_to_device = gaudi_load_boot_fit_to_device,\n\t.get_signal_cb_size = gaudi_get_signal_cb_size,\n\t.get_wait_cb_size = gaudi_get_wait_cb_size,\n\t.gen_signal_cb = gaudi_gen_signal_cb,\n\t.gen_wait_cb = gaudi_gen_wait_cb,\n\t.reset_sob = gaudi_reset_sob,\n\t.reset_sob_group = gaudi_reset_sob_group,\n\t.get_device_time = gaudi_get_device_time,\n\t.pb_print_security_errors = NULL,\n\t.collective_wait_init_cs = gaudi_collective_wait_init_cs,\n\t.collective_wait_create_jobs = gaudi_collective_wait_create_jobs,\n\t.get_dec_base_addr = NULL,\n\t.scramble_addr = hl_mmu_scramble_addr,\n\t.descramble_addr = hl_mmu_descramble_addr,\n\t.ack_protection_bits_errors = gaudi_ack_protection_bits_errors,\n\t.get_hw_block_id = gaudi_get_hw_block_id,\n\t.hw_block_mmap = gaudi_block_mmap,\n\t.enable_events_from_fw = gaudi_enable_events_from_fw,\n\t.ack_mmu_errors = gaudi_ack_mmu_page_fault_or_access_error,\n\t.map_pll_idx_to_fw_idx = gaudi_map_pll_idx_to_fw_idx,\n\t.init_firmware_preload_params = gaudi_init_firmware_preload_params,\n\t.init_firmware_loader = gaudi_init_firmware_loader,\n\t.init_cpu_scrambler_dram = gaudi_init_scrambler_hbm,\n\t.state_dump_init = gaudi_state_dump_init,\n\t.get_sob_addr = gaudi_get_sob_addr,\n\t.set_pci_memory_regions = gaudi_set_pci_memory_regions,\n\t.get_stream_master_qid_arr = gaudi_get_stream_master_qid_arr,\n\t.check_if_razwi_happened = gaudi_check_if_razwi_happened,\n\t.mmu_get_real_page_size = hl_mmu_get_real_page_size,\n\t.access_dev_mem = hl_access_dev_mem,\n\t.set_dram_bar_base = gaudi_set_hbm_bar_base,\n\t.send_device_activity = gaudi_send_device_activity,\n\t.set_dram_properties = gaudi_set_dram_properties,\n\t.set_binning_masks = gaudi_set_binning_masks,\n};\n\n \nvoid gaudi_set_asic_funcs(struct hl_device *hdev)\n{\n\thdev->asic_funcs = &gaudi_funcs;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}