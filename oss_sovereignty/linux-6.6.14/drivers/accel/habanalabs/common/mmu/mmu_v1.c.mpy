{
  "module_name": "mmu_v1.c",
  "hash_id": "94dd8e9c39223951068e1ce2ef27f911f84ff0b17e4855e0ac960687cc636fbb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/mmu/mmu_v1.c",
  "human_readable_source": "\n\n \n\n#include \"../habanalabs.h\"\n#include \"../../include/hw_ip/mmu/mmu_general.h\"\n\n#include <linux/slab.h>\n\n#define MMU_V1_MAX_HOPS\t(MMU_HOP4 + 1)\n\nstatic inline u64 get_phys_addr(struct hl_ctx *ctx, u64 shadow_addr);\n\nstatic struct pgt_info *get_pgt_info(struct hl_ctx *ctx, u64 hop_addr)\n{\n\tstruct pgt_info *pgt_info = NULL;\n\n\thash_for_each_possible(ctx->mmu_shadow_hash, pgt_info, node,\n\t\t\t\t(unsigned long) hop_addr)\n\t\tif (hop_addr == pgt_info->shadow_addr)\n\t\t\tbreak;\n\n\treturn pgt_info;\n}\n\nstatic void _free_hop(struct hl_ctx *ctx, struct pgt_info *pgt_info)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\n\tgen_pool_free(hdev->mmu_priv.dr.mmu_pgt_pool, pgt_info->phys_addr,\n\t\t\thdev->asic_prop.mmu_hop_table_size);\n\thash_del(&pgt_info->node);\n\tkfree((u64 *) (uintptr_t) pgt_info->shadow_addr);\n\tkfree(pgt_info);\n}\n\nstatic void free_hop(struct hl_ctx *ctx, u64 hop_addr)\n{\n\tstruct pgt_info *pgt_info = get_pgt_info(ctx, hop_addr);\n\n\t_free_hop(ctx, pgt_info);\n}\n\nstatic u64 alloc_hop(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct pgt_info *pgt_info;\n\tu64 phys_addr, shadow_addr;\n\n\tpgt_info = kmalloc(sizeof(*pgt_info), GFP_KERNEL);\n\tif (!pgt_info)\n\t\treturn ULLONG_MAX;\n\n\tphys_addr = (u64) gen_pool_alloc(hdev->mmu_priv.dr.mmu_pgt_pool,\n\t\t\t\t\tprop->mmu_hop_table_size);\n\tif (!phys_addr) {\n\t\tdev_err(hdev->dev, \"failed to allocate page\\n\");\n\t\tgoto pool_add_err;\n\t}\n\n\tshadow_addr = (u64) (uintptr_t) kzalloc(prop->mmu_hop_table_size,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!shadow_addr)\n\t\tgoto shadow_err;\n\n\tpgt_info->phys_addr = phys_addr;\n\tpgt_info->shadow_addr = shadow_addr;\n\tpgt_info->ctx = ctx;\n\tpgt_info->num_of_ptes = 0;\n\thash_add(ctx->mmu_shadow_hash, &pgt_info->node, shadow_addr);\n\n\treturn shadow_addr;\n\nshadow_err:\n\tgen_pool_free(hdev->mmu_priv.dr.mmu_pgt_pool, phys_addr,\n\t\t\tprop->mmu_hop_table_size);\npool_add_err:\n\tkfree(pgt_info);\n\n\treturn ULLONG_MAX;\n}\n\nstatic inline u64 get_phys_hop0_addr(struct hl_ctx *ctx)\n{\n\treturn ctx->hdev->asic_prop.mmu_pgt_addr +\n\t\t\t(ctx->asid * ctx->hdev->asic_prop.mmu_hop_table_size);\n}\n\nstatic inline u64 get_hop0_addr(struct hl_ctx *ctx)\n{\n\treturn (u64) (uintptr_t) ctx->hdev->mmu_priv.dr.mmu_shadow_hop0 +\n\t\t\t(ctx->asid * ctx->hdev->asic_prop.mmu_hop_table_size);\n}\n\nstatic void flush(struct hl_ctx *ctx)\n{\n\t \n\tmb();\n\tctx->hdev->asic_funcs->read_pte(ctx->hdev, get_phys_hop0_addr(ctx));\n}\n\n \nstatic inline void write_pte(struct hl_ctx *ctx, u64 shadow_pte_addr, u64 val)\n{\n\t \n\tu64 phys_val = get_phys_addr(ctx, val & HOP_PHYS_ADDR_MASK) |\n\t\t\t\t(val & FLAGS_MASK);\n\n\tctx->hdev->asic_funcs->write_pte(ctx->hdev,\n\t\t\t\t\tget_phys_addr(ctx, shadow_pte_addr),\n\t\t\t\t\tphys_val);\n\n\t*(u64 *) (uintptr_t) shadow_pte_addr = val;\n}\n\n \nstatic inline void write_final_pte(struct hl_ctx *ctx, u64 shadow_pte_addr,\n\t\t\t\t\tu64 val)\n{\n\tctx->hdev->asic_funcs->write_pte(ctx->hdev,\n\t\t\t\t\tget_phys_addr(ctx, shadow_pte_addr),\n\t\t\t\t\tval);\n\t*(u64 *) (uintptr_t) shadow_pte_addr = val;\n}\n\n \nstatic inline void clear_pte(struct hl_ctx *ctx, u64 pte_addr)\n{\n\t \n\twrite_final_pte(ctx, pte_addr, 0);\n}\n\nstatic inline void get_pte(struct hl_ctx *ctx, u64 hop_addr)\n{\n\tget_pgt_info(ctx, hop_addr)->num_of_ptes++;\n}\n\n \nstatic inline int put_pte(struct hl_ctx *ctx, u64 hop_addr)\n{\n\tstruct pgt_info *pgt_info = get_pgt_info(ctx, hop_addr);\n\tint num_of_ptes_left;\n\n\tpgt_info->num_of_ptes--;\n\n\t \n\tnum_of_ptes_left = pgt_info->num_of_ptes;\n\tif (!num_of_ptes_left)\n\t\t_free_hop(ctx, pgt_info);\n\n\treturn num_of_ptes_left;\n}\n\nstatic inline u64 get_hop_pte_addr(struct hl_ctx *ctx, struct hl_mmu_properties *mmu_prop,\n\t\t\t\t\tu64 *hop_addr_arr, u64 virt_addr, enum mmu_hop_num hop_idx)\n{\n\tu64 mask, shift;\n\n\tmask = mmu_prop->hop_masks[hop_idx];\n\tshift = mmu_prop->hop_shifts[hop_idx];\n\treturn hop_addr_arr[hop_idx] +\n\t\t\tctx->hdev->asic_prop.mmu_pte_size * ((virt_addr & mask) >> shift);\n}\n\nstatic inline u64 get_alloc_next_hop_addr(struct hl_ctx *ctx, u64 curr_pte,\n\t\t\t\t\t\tbool *is_new_hop)\n{\n\tu64 hop_addr = hl_mmu_get_next_hop_addr(ctx, curr_pte);\n\n\tif (hop_addr == ULLONG_MAX) {\n\t\thop_addr = alloc_hop(ctx);\n\t\t*is_new_hop = (hop_addr != ULLONG_MAX);\n\t}\n\n\treturn hop_addr;\n}\n\n \nstatic inline u64 get_phys_addr(struct hl_ctx *ctx, u64 shadow_addr)\n{\n\tu64 page_mask = (ctx->hdev->asic_prop.mmu_hop_table_size - 1);\n\tu64 shadow_hop_addr = shadow_addr & ~page_mask;\n\tu64 pte_offset = shadow_addr & page_mask;\n\tu64 phys_hop_addr;\n\n\tif (shadow_hop_addr != get_hop0_addr(ctx))\n\t\tphys_hop_addr = get_pgt_info(ctx, shadow_hop_addr)->phys_addr;\n\telse\n\t\tphys_hop_addr = get_phys_hop0_addr(ctx);\n\n\treturn phys_hop_addr + pte_offset;\n}\n\nstatic int dram_default_mapping_init(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 num_of_hop3, total_hops, hop0_addr, hop1_addr, hop2_addr,\n\t\thop2_pte_addr, hop3_pte_addr, pte_val;\n\tint rc, i, j, hop3_allocated = 0;\n\n\tif ((!prop->dram_supports_virtual_memory) ||\n\t\t\t(!hdev->dram_default_page_mapping) ||\n\t\t\t(ctx->asid == HL_KERNEL_ASID_ID))\n\t\treturn 0;\n\n\tnum_of_hop3 = prop->dram_size_for_default_page_mapping;\n\tdo_div(num_of_hop3, prop->dram_page_size);\n\tdo_div(num_of_hop3, HOP_PTE_ENTRIES_512);\n\n\t \n\ttotal_hops = num_of_hop3 + 2;\n\n\tctx->dram_default_hops = kzalloc(HL_PTE_SIZE * total_hops,  GFP_KERNEL);\n\tif (!ctx->dram_default_hops)\n\t\treturn -ENOMEM;\n\n\thop0_addr = get_hop0_addr(ctx);\n\n\thop1_addr = alloc_hop(ctx);\n\tif (hop1_addr == ULLONG_MAX) {\n\t\tdev_err(hdev->dev, \"failed to alloc hop 1\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto hop1_err;\n\t}\n\n\tctx->dram_default_hops[total_hops - 1] = hop1_addr;\n\n\thop2_addr = alloc_hop(ctx);\n\tif (hop2_addr == ULLONG_MAX) {\n\t\tdev_err(hdev->dev, \"failed to alloc hop 2\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto hop2_err;\n\t}\n\n\tctx->dram_default_hops[total_hops - 2] = hop2_addr;\n\n\tfor (i = 0 ; i < num_of_hop3 ; i++) {\n\t\tctx->dram_default_hops[i] = alloc_hop(ctx);\n\t\tif (ctx->dram_default_hops[i] == ULLONG_MAX) {\n\t\t\tdev_err(hdev->dev, \"failed to alloc hop 3, i: %d\\n\", i);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto hop3_err;\n\t\t}\n\t\thop3_allocated++;\n\t}\n\n\t \n\tpte_val = (hop1_addr & HOP_PHYS_ADDR_MASK) | PAGE_PRESENT_MASK;\n\twrite_pte(ctx, hop0_addr, pte_val);\n\n\tpte_val = (hop2_addr & HOP_PHYS_ADDR_MASK) | PAGE_PRESENT_MASK;\n\twrite_pte(ctx, hop1_addr, pte_val);\n\tget_pte(ctx, hop1_addr);\n\n\thop2_pte_addr = hop2_addr;\n\tfor (i = 0 ; i < num_of_hop3 ; i++) {\n\t\tpte_val = (ctx->dram_default_hops[i] & HOP_PHYS_ADDR_MASK) |\n\t\t\t\tPAGE_PRESENT_MASK;\n\t\twrite_pte(ctx, hop2_pte_addr, pte_val);\n\t\tget_pte(ctx, hop2_addr);\n\t\thop2_pte_addr += HL_PTE_SIZE;\n\t}\n\n\tpte_val = (prop->mmu_dram_default_page_addr & HOP_PHYS_ADDR_MASK) |\n\t\t\tLAST_MASK | PAGE_PRESENT_MASK;\n\n\tfor (i = 0 ; i < num_of_hop3 ; i++) {\n\t\thop3_pte_addr = ctx->dram_default_hops[i];\n\t\tfor (j = 0 ; j < HOP_PTE_ENTRIES_512 ; j++) {\n\t\t\twrite_final_pte(ctx, hop3_pte_addr, pte_val);\n\t\t\tget_pte(ctx, ctx->dram_default_hops[i]);\n\t\t\thop3_pte_addr += HL_PTE_SIZE;\n\t\t}\n\t}\n\n\tflush(ctx);\n\n\treturn 0;\n\nhop3_err:\n\tfor (i = 0 ; i < hop3_allocated ; i++)\n\t\tfree_hop(ctx, ctx->dram_default_hops[i]);\n\n\tfree_hop(ctx, hop2_addr);\nhop2_err:\n\tfree_hop(ctx, hop1_addr);\nhop1_err:\n\tkfree(ctx->dram_default_hops);\n\n\treturn rc;\n}\n\nstatic void dram_default_mapping_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 num_of_hop3, total_hops, hop0_addr, hop1_addr, hop2_addr,\n\t\thop2_pte_addr, hop3_pte_addr;\n\tint i, j;\n\n\tif ((!prop->dram_supports_virtual_memory) ||\n\t\t\t(!hdev->dram_default_page_mapping) ||\n\t\t\t(ctx->asid == HL_KERNEL_ASID_ID))\n\t\treturn;\n\n\tnum_of_hop3 = prop->dram_size_for_default_page_mapping;\n\tdo_div(num_of_hop3, prop->dram_page_size);\n\tdo_div(num_of_hop3, HOP_PTE_ENTRIES_512);\n\n\thop0_addr = get_hop0_addr(ctx);\n\t \n\ttotal_hops = num_of_hop3 + 2;\n\thop1_addr = ctx->dram_default_hops[total_hops - 1];\n\thop2_addr = ctx->dram_default_hops[total_hops - 2];\n\n\tfor (i = 0 ; i < num_of_hop3 ; i++) {\n\t\thop3_pte_addr = ctx->dram_default_hops[i];\n\t\tfor (j = 0 ; j < HOP_PTE_ENTRIES_512 ; j++) {\n\t\t\tclear_pte(ctx, hop3_pte_addr);\n\t\t\tput_pte(ctx, ctx->dram_default_hops[i]);\n\t\t\thop3_pte_addr += HL_PTE_SIZE;\n\t\t}\n\t}\n\n\thop2_pte_addr = hop2_addr;\n\tfor (i = 0 ; i < num_of_hop3 ; i++) {\n\t\tclear_pte(ctx, hop2_pte_addr);\n\t\tput_pte(ctx, hop2_addr);\n\t\thop2_pte_addr += HL_PTE_SIZE;\n\t}\n\n\tclear_pte(ctx, hop1_addr);\n\tput_pte(ctx, hop1_addr);\n\tclear_pte(ctx, hop0_addr);\n\n\tkfree(ctx->dram_default_hops);\n\n\tflush(ctx);\n}\n\n \nstatic int hl_mmu_v1_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint rc;\n\n\thdev->mmu_priv.dr.mmu_pgt_pool =\n\t\t\tgen_pool_create(__ffs(prop->mmu_hop_table_size), -1);\n\n\tif (!hdev->mmu_priv.dr.mmu_pgt_pool) {\n\t\tdev_err(hdev->dev, \"Failed to create page gen pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\trc = gen_pool_add(hdev->mmu_priv.dr.mmu_pgt_pool, prop->mmu_pgt_addr +\n\t\t\tprop->mmu_hop0_tables_total_size,\n\t\t\tprop->mmu_pgt_size - prop->mmu_hop0_tables_total_size,\n\t\t\t-1);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to add memory to page gen pool\\n\");\n\t\tgoto err_pool_add;\n\t}\n\n\thdev->mmu_priv.dr.mmu_shadow_hop0 = kvcalloc(prop->max_asid, prop->mmu_hop_table_size,\n\t\t\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(hdev->mmu_priv.dr.mmu_shadow_hop0)) {\n\t\trc = -ENOMEM;\n\t\tgoto err_pool_add;\n\t}\n\n\t \n\n\treturn 0;\n\nerr_pool_add:\n\tgen_pool_destroy(hdev->mmu_priv.dr.mmu_pgt_pool);\n\n\treturn rc;\n}\n\n \nstatic void hl_mmu_v1_fini(struct hl_device *hdev)\n{\n\t \n\n\tif (!ZERO_OR_NULL_PTR(hdev->mmu_priv.dr.mmu_shadow_hop0)) {\n\t\tkvfree(hdev->mmu_priv.dr.mmu_shadow_hop0);\n\t\tgen_pool_destroy(hdev->mmu_priv.dr.mmu_pgt_pool);\n\n\t\t \n\t\thdev->mmu_priv.dr.mmu_shadow_hop0 = NULL;\n\t}\n}\n\n \nstatic int hl_mmu_v1_ctx_init(struct hl_ctx *ctx)\n{\n\thash_init(ctx->mmu_shadow_hash);\n\treturn dram_default_mapping_init(ctx);\n}\n\n \nstatic void hl_mmu_v1_ctx_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct pgt_info *pgt_info;\n\tstruct hlist_node *tmp;\n\tint i;\n\n\tdram_default_mapping_fini(ctx);\n\n\tif (!hash_empty(ctx->mmu_shadow_hash))\n\t\tdev_err(hdev->dev, \"ctx %d is freed while it has pgts in use\\n\",\n\t\t\tctx->asid);\n\n\thash_for_each_safe(ctx->mmu_shadow_hash, i, tmp, pgt_info, node) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"pgt_info of addr 0x%llx of asid %d was not destroyed, num_ptes: %d\\n\",\n\t\t\tpgt_info->phys_addr, ctx->asid, pgt_info->num_of_ptes);\n\t\t_free_hop(ctx, pgt_info);\n\t}\n}\n\nstatic int hl_mmu_v1_unmap(struct hl_ctx *ctx,\n\t\t\t\tu64 virt_addr, bool is_dram_addr)\n{\n\tu64 hop_addr[MMU_V1_MAX_HOPS] = {0}, hop_pte_addr[MMU_V1_MAX_HOPS] = {0}, curr_pte = 0;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_mmu_properties *mmu_prop;\n\tbool is_huge, clear_hop3 = true;\n\tint hop_idx;\n\n\t \n\tmmu_prop = is_dram_addr ? &prop->dmmu : &prop->pmmu;\n\n\tfor (hop_idx = MMU_HOP0; hop_idx < MMU_HOP4; hop_idx++) {\n\t\tif (hop_idx == MMU_HOP0) {\n\t\t\thop_addr[hop_idx] = get_hop0_addr(ctx);\n\t\t} else {\n\t\t\thop_addr[hop_idx] = hl_mmu_get_next_hop_addr(ctx, curr_pte);\n\t\t\tif (hop_addr[hop_idx] == ULLONG_MAX)\n\t\t\t\tgoto not_mapped;\n\t\t}\n\n\t\thop_pte_addr[hop_idx] =\n\t\t\t\tget_hop_pte_addr(ctx, mmu_prop, hop_addr, virt_addr, hop_idx);\n\n\t\tcurr_pte = *(u64 *) (uintptr_t) hop_pte_addr[hop_idx];\n\t}\n\n\tis_huge = curr_pte & mmu_prop->last_mask;\n\n\tif (is_dram_addr && !is_huge) {\n\t\tdev_err(hdev->dev, \"DRAM unmapping should use huge pages only\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\tif (!is_huge) {\n\t\thop_idx = MMU_HOP4;\n\t\thop_addr[hop_idx] = hl_mmu_get_next_hop_addr(ctx, curr_pte);\n\t\tif (hop_addr[hop_idx] == ULLONG_MAX)\n\t\t\tgoto not_mapped;\n\n\t\thop_pte_addr[hop_idx] =\n\t\t\t\tget_hop_pte_addr(ctx, mmu_prop, hop_addr, virt_addr, hop_idx);\n\t\tcurr_pte = *(u64 *) (uintptr_t) hop_pte_addr[hop_idx];\n\t\tclear_hop3 = false;\n\t}\n\n\tif (hdev->dram_default_page_mapping && is_dram_addr) {\n\t\tu64 default_pte = (prop->mmu_dram_default_page_addr &\n\t\t\t\tHOP_PHYS_ADDR_MASK) | mmu_prop->last_mask |\n\t\t\t\t\tPAGE_PRESENT_MASK;\n\t\tif (curr_pte == default_pte) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"DRAM: hop3 PTE points to zero page, can't unmap, va: 0x%llx\\n\",\n\t\t\t\t\tvirt_addr);\n\t\t\tgoto not_mapped;\n\t\t}\n\n\t\tif (!(curr_pte & PAGE_PRESENT_MASK)) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"DRAM: hop3 PTE is cleared! can't unmap, va: 0x%llx\\n\",\n\t\t\t\t\tvirt_addr);\n\t\t\tgoto not_mapped;\n\t\t}\n\n\t\thop_idx = MMU_HOP3;\n\t\twrite_final_pte(ctx, hop_pte_addr[hop_idx], default_pte);\n\t\tput_pte(ctx, hop_addr[hop_idx]);\n\t} else {\n\t\tif (!(curr_pte & PAGE_PRESENT_MASK))\n\t\t\tgoto not_mapped;\n\n\t\tif (hop_addr[MMU_HOP4])\n\t\t\tclear_pte(ctx, hop_pte_addr[MMU_HOP4]);\n\t\telse\n\t\t\tclear_pte(ctx, hop_pte_addr[MMU_HOP3]);\n\n\t\tif (hop_addr[MMU_HOP4] && !put_pte(ctx, hop_addr[MMU_HOP4]))\n\t\t\tclear_hop3 = true;\n\n\t\tif (!clear_hop3)\n\t\t\tgoto mapped;\n\n\t\tfor (hop_idx = MMU_HOP3; hop_idx >= 0; hop_idx--) {\n\t\t\tclear_pte(ctx, hop_pte_addr[hop_idx]);\n\n\t\t\tif (hop_idx == MMU_HOP0)\n\t\t\t\tbreak;\n\n\t\t\tif (put_pte(ctx, hop_addr[hop_idx]))\n\t\t\t\tgoto mapped;\n\t\t}\n\t}\n\nmapped:\n\treturn 0;\n\nnot_mapped:\n\tdev_err(hdev->dev, \"virt addr 0x%llx is not mapped to phys addr\\n\",\n\t\tvirt_addr);\n\n\treturn -EINVAL;\n}\n\nstatic int hl_mmu_v1_map(struct hl_ctx *ctx, u64 virt_addr, u64 phys_addr,\n\t\t\tu32 page_size, bool is_dram_addr)\n{\n\tu64 hop_addr[MMU_V1_MAX_HOPS] = {0}, hop_pte_addr[MMU_V1_MAX_HOPS] = {0}, curr_pte = 0;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_mmu_properties *mmu_prop;\n\tbool is_huge, hop_new[MMU_V1_MAX_HOPS] = {false};\n\tint num_hops, hop_idx, prev_hop, rc = -ENOMEM;\n\n\t \n\tif (is_dram_addr) {\n\t\tmmu_prop = &prop->dmmu;\n\t\tis_huge = true;\n\t} else if (page_size == prop->pmmu_huge.page_size) {\n\t\tmmu_prop = &prop->pmmu_huge;\n\t\tis_huge = true;\n\t} else {\n\t\tmmu_prop = &prop->pmmu;\n\t\tis_huge = false;\n\t}\n\n\tnum_hops = is_huge ? (MMU_V1_MAX_HOPS - 1) : MMU_V1_MAX_HOPS;\n\n\tfor (hop_idx = MMU_HOP0; hop_idx < num_hops; hop_idx++) {\n\t\tif (hop_idx == MMU_HOP0) {\n\t\t\thop_addr[hop_idx] = get_hop0_addr(ctx);\n\t\t} else {\n\t\t\thop_addr[hop_idx] =\n\t\t\t\t\tget_alloc_next_hop_addr(ctx, curr_pte, &hop_new[hop_idx]);\n\t\t\tif (hop_addr[hop_idx] == ULLONG_MAX)\n\t\t\t\tgoto err;\n\t\t}\n\n\t\thop_pte_addr[hop_idx] =\n\t\t\t\tget_hop_pte_addr(ctx, mmu_prop, hop_addr, virt_addr, hop_idx);\n\t\tcurr_pte = *(u64 *) (uintptr_t) hop_pte_addr[hop_idx];\n\t}\n\n\tif (hdev->dram_default_page_mapping && is_dram_addr) {\n\t\tu64 default_pte = (prop->mmu_dram_default_page_addr &\n\t\t\t\t\tHOP_PHYS_ADDR_MASK) | mmu_prop->last_mask |\n\t\t\t\t\t\tPAGE_PRESENT_MASK;\n\n\t\tif (curr_pte != default_pte) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"DRAM: mapping already exists for virt_addr 0x%llx\\n\",\n\t\t\t\t\tvirt_addr);\n\t\t\trc = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (hop_idx = MMU_HOP1; hop_idx < num_hops; hop_idx++) {\n\t\t\tif (hop_new[hop_idx]) {\n\t\t\t\tdev_err(hdev->dev, \"DRAM mapping should not allocate more hops\\n\");\n\t\t\t\trc = -EFAULT;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\t} else if (curr_pte & PAGE_PRESENT_MASK) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"mapping already exists for virt_addr 0x%llx\\n\",\n\t\t\t\tvirt_addr);\n\n\t\tfor (hop_idx = MMU_HOP0; hop_idx < num_hops; hop_idx++)\n\t\t\tdev_dbg(hdev->dev, \"hop%d pte: 0x%llx (0x%llx)\\n\", hop_idx,\n\t\t\t\t\t*(u64 *) (uintptr_t) hop_pte_addr[hop_idx],\n\t\t\t\t\thop_pte_addr[hop_idx]);\n\n\t\trc = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tcurr_pte = (phys_addr & HOP_PHYS_ADDR_MASK) | mmu_prop->last_mask\n\t\t\t| PAGE_PRESENT_MASK;\n\n\twrite_final_pte(ctx, hop_pte_addr[num_hops - 1], curr_pte);\n\n\tfor (hop_idx = MMU_HOP1; hop_idx < num_hops; hop_idx++) {\n\t\tprev_hop = hop_idx - 1;\n\n\t\tif (hop_new[hop_idx]) {\n\t\t\tcurr_pte = (hop_addr[hop_idx] & HOP_PHYS_ADDR_MASK) | PAGE_PRESENT_MASK;\n\t\t\twrite_pte(ctx, hop_pte_addr[prev_hop], curr_pte);\n\t\t\tif (hop_idx != MMU_HOP1)\n\t\t\t\tget_pte(ctx, hop_addr[prev_hop]);\n\t\t}\n\t}\n\n\tget_pte(ctx, hop_addr[num_hops - 1]);\n\n\treturn 0;\n\nerr:\n\tfor (hop_idx = num_hops; hop_idx > MMU_HOP0; hop_idx--) {\n\t\tif (hop_new[hop_idx])\n\t\t\tfree_hop(ctx, hop_addr[hop_idx]);\n\t}\n\n\treturn rc;\n}\n\n \nstatic void hl_mmu_v1_swap_out(struct hl_ctx *ctx)\n{\n\n}\n\n \nstatic void hl_mmu_v1_swap_in(struct hl_ctx *ctx)\n{\n\n}\n\nstatic int hl_mmu_v1_get_tlb_info(struct hl_ctx *ctx, u64 virt_addr,\n\t\t\t\tstruct hl_mmu_hop_info *hops)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_mmu_properties *mmu_prop;\n\tbool is_dram_addr, is_pmmu_addr, is_pmmu_h_addr, is_huge;\n\tint i, used_hops;\n\n\tis_dram_addr = hl_mem_area_inside_range(virt_addr, prop->dmmu.page_size,\n\t\t\t\t\t\tprop->dmmu.start_addr,\n\t\t\t\t\t\tprop->dmmu.end_addr);\n\tis_pmmu_addr = hl_mem_area_inside_range(virt_addr, prop->pmmu.page_size,\n\t\t\t\t\t\tprop->pmmu.start_addr,\n\t\t\t\t\t\tprop->pmmu.end_addr);\n\tis_pmmu_h_addr = hl_mem_area_inside_range(virt_addr,\n\t\t\t\t\t\tprop->pmmu_huge.page_size,\n\t\t\t\t\t\tprop->pmmu_huge.start_addr,\n\t\t\t\t\t\tprop->pmmu_huge.end_addr);\n\tif (is_dram_addr) {\n\t\tmmu_prop = &prop->dmmu;\n\t\tis_huge = true;\n\t} else if (is_pmmu_addr) {\n\t\tmmu_prop = &prop->pmmu;\n\t\tis_huge = false;\n\t} else if (is_pmmu_h_addr) {\n\t\tmmu_prop = &prop->pmmu_huge;\n\t\tis_huge = true;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tused_hops = mmu_prop->num_hops;\n\n\t \n\tif (is_huge)\n\t\tused_hops--;\n\n\thops->hop_info[0].hop_addr = get_phys_hop0_addr(ctx);\n\thops->hop_info[0].hop_pte_addr =\n\t\t\thl_mmu_get_hop_pte_phys_addr(ctx, mmu_prop, 0,\n\t\t\t\t\thops->hop_info[0].hop_addr, virt_addr);\n\thops->hop_info[0].hop_pte_val =\n\t\t\thdev->asic_funcs->read_pte(hdev,\n\t\t\t\t\t\thops->hop_info[0].hop_pte_addr);\n\n\tfor (i = 1 ; i < used_hops ; i++) {\n\t\thops->hop_info[i].hop_addr =\n\t\t\thl_mmu_get_next_hop_addr(ctx,\n\t\t\t\t\thops->hop_info[i - 1].hop_pte_val);\n\t\tif (hops->hop_info[i].hop_addr == ULLONG_MAX)\n\t\t\treturn -EFAULT;\n\n\t\thops->hop_info[i].hop_pte_addr =\n\t\t\t\thl_mmu_get_hop_pte_phys_addr(ctx, mmu_prop, i,\n\t\t\t\t\t\thops->hop_info[i].hop_addr,\n\t\t\t\t\t\tvirt_addr);\n\t\thops->hop_info[i].hop_pte_val =\n\t\t\t\thdev->asic_funcs->read_pte(hdev,\n\t\t\t\t\t\thops->hop_info[i].hop_pte_addr);\n\n\t\tif (!(hops->hop_info[i].hop_pte_val & PAGE_PRESENT_MASK))\n\t\t\treturn -EFAULT;\n\n\t\tif (hops->hop_info[i].hop_pte_val & mmu_prop->last_mask)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (i == mmu_prop->num_hops)\n\t\treturn -EFAULT;\n\n\tif (!(hops->hop_info[i].hop_pte_val & PAGE_PRESENT_MASK))\n\t\treturn -EFAULT;\n\n\thops->used_hops = i + 1;\n\n\treturn 0;\n}\n\n \nvoid hl_mmu_v1_set_funcs(struct hl_device *hdev, struct hl_mmu_funcs *mmu)\n{\n\tmmu->init = hl_mmu_v1_init;\n\tmmu->fini = hl_mmu_v1_fini;\n\tmmu->ctx_init = hl_mmu_v1_ctx_init;\n\tmmu->ctx_fini = hl_mmu_v1_ctx_fini;\n\tmmu->map = hl_mmu_v1_map;\n\tmmu->unmap = hl_mmu_v1_unmap;\n\tmmu->flush = flush;\n\tmmu->swap_out = hl_mmu_v1_swap_out;\n\tmmu->swap_in = hl_mmu_v1_swap_in;\n\tmmu->get_tlb_info = hl_mmu_v1_get_tlb_info;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}