{
  "module_name": "irq.c",
  "hash_id": "57778c68ad35fae09206576846b76b937f70ef88bd0a5d2f5e46fd456d698f63",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/irq.c",
  "human_readable_source": "\n\n \n\n#include \"habanalabs.h\"\n\n#include <linux/slab.h>\n\n \nstruct hl_eqe_work {\n\tstruct work_struct\teq_work;\n\tstruct hl_device\t*hdev;\n\tstruct hl_eq_entry\teq_entry;\n};\n\n \ninline u32 hl_cq_inc_ptr(u32 ptr)\n{\n\tptr++;\n\tif (unlikely(ptr == HL_CQ_LENGTH))\n\t\tptr = 0;\n\treturn ptr;\n}\n\n \nstatic inline u32 hl_eq_inc_ptr(u32 ptr)\n{\n\tptr++;\n\tif (unlikely(ptr == HL_EQ_LENGTH))\n\t\tptr = 0;\n\treturn ptr;\n}\n\nstatic void irq_handle_eqe(struct work_struct *work)\n{\n\tstruct hl_eqe_work *eqe_work = container_of(work, struct hl_eqe_work,\n\t\t\t\t\t\t\teq_work);\n\tstruct hl_device *hdev = eqe_work->hdev;\n\n\thdev->asic_funcs->handle_eqe(hdev, &eqe_work->eq_entry);\n\n\tkfree(eqe_work);\n}\n\n \nstatic void job_finish(struct hl_device *hdev, u32 cs_seq, struct hl_cq *cq, ktime_t timestamp)\n{\n\tstruct hl_hw_queue *queue;\n\tstruct hl_cs_job *job;\n\n\tqueue = &hdev->kernel_queues[cq->hw_queue_id];\n\tjob = queue->shadow_queue[hl_pi_2_offset(cs_seq)];\n\tjob->timestamp = timestamp;\n\tqueue_work(hdev->cq_wq[cq->cq_idx], &job->finish_work);\n\n\tatomic_inc(&queue->ci);\n}\n\n \nstatic void cs_finish(struct hl_device *hdev, u16 cs_seq, ktime_t timestamp)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_hw_queue *queue;\n\tstruct hl_cs *cs;\n\tstruct hl_cs_job *job;\n\n\tcs = hdev->shadow_cs_queue[cs_seq & (prop->max_pending_cs - 1)];\n\tif (!cs) {\n\t\tdev_warn(hdev->dev,\n\t\t\t\"No pointer to CS in shadow array at index %d\\n\",\n\t\t\tcs_seq);\n\t\treturn;\n\t}\n\n\tlist_for_each_entry(job, &cs->job_list, cs_node) {\n\t\tqueue = &hdev->kernel_queues[job->hw_queue_id];\n\t\tatomic_inc(&queue->ci);\n\t}\n\n\tcs->completion_timestamp = timestamp;\n\tqueue_work(hdev->cs_cmplt_wq, &cs->finish_work);\n}\n\n \nirqreturn_t hl_irq_handler_cq(int irq, void *arg)\n{\n\tstruct hl_cq *cq = arg;\n\tstruct hl_device *hdev = cq->hdev;\n\tbool shadow_index_valid, entry_ready;\n\tu16 shadow_index;\n\tstruct hl_cq_entry *cq_entry, *cq_base;\n\tktime_t timestamp = ktime_get();\n\n\tif (hdev->disabled) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Device disabled but received IRQ %d for CQ %d\\n\",\n\t\t\tirq, cq->hw_queue_id);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tcq_base = cq->kernel_address;\n\n\twhile (1) {\n\t\tcq_entry = (struct hl_cq_entry *) &cq_base[cq->ci];\n\n\t\tentry_ready = !!FIELD_GET(CQ_ENTRY_READY_MASK,\n\t\t\t\tle32_to_cpu(cq_entry->data));\n\t\tif (!entry_ready)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tshadow_index_valid =\n\t\t\t!!FIELD_GET(CQ_ENTRY_SHADOW_INDEX_VALID_MASK,\n\t\t\t\t\tle32_to_cpu(cq_entry->data));\n\n\t\tshadow_index = FIELD_GET(CQ_ENTRY_SHADOW_INDEX_MASK,\n\t\t\t\tle32_to_cpu(cq_entry->data));\n\n\t\t \n\t\tif (shadow_index_valid && !hdev->disabled) {\n\t\t\tif (hdev->asic_prop.completion_mode ==\n\t\t\t\t\tHL_COMPLETION_MODE_CS)\n\t\t\t\tcs_finish(hdev, shadow_index, timestamp);\n\t\t\telse\n\t\t\t\tjob_finish(hdev, shadow_index, cq, timestamp);\n\t\t}\n\n\t\t \n\t\tcq_entry->data = cpu_to_le32(le32_to_cpu(cq_entry->data) &\n\t\t\t\t\t\t~CQ_ENTRY_READY_MASK);\n\n\t\tcq->ci = hl_cq_inc_ptr(cq->ci);\n\n\t\t \n\t\tatomic_inc(&cq->free_slots_cnt);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void hl_ts_free_objects(struct work_struct *work)\n{\n\tstruct timestamp_reg_work_obj *job =\n\t\t\tcontainer_of(work, struct timestamp_reg_work_obj, free_obj);\n\tstruct timestamp_reg_free_node *free_obj, *temp_free_obj;\n\tstruct list_head *free_list_head = job->free_obj_head;\n\tstruct hl_device *hdev = job->hdev;\n\n\tlist_for_each_entry_safe(free_obj, temp_free_obj, free_list_head, free_objects_node) {\n\t\tdev_dbg(hdev->dev, \"About to put refcount to buf (%p) cq_cb(%p)\\n\",\n\t\t\t\t\tfree_obj->buf,\n\t\t\t\t\tfree_obj->cq_cb);\n\n\t\thl_mmap_mem_buf_put(free_obj->buf);\n\t\thl_cb_put(free_obj->cq_cb);\n\t\tkfree(free_obj);\n\t}\n\n\tkfree(free_list_head);\n\tkfree(job);\n}\n\n \nstatic int handle_registration_node(struct hl_device *hdev, struct hl_user_pending_interrupt *pend,\n\t\t\t\t\t\tstruct list_head **free_list, ktime_t now)\n{\n\tstruct timestamp_reg_free_node *free_node;\n\tu64 timestamp;\n\n\tif (!(*free_list)) {\n\t\t \n\t\t*free_list = kmalloc(sizeof(struct list_head), GFP_ATOMIC);\n\t\tif (!(*free_list))\n\t\t\treturn -ENOMEM;\n\n\t\tINIT_LIST_HEAD(*free_list);\n\t}\n\n\tfree_node = kmalloc(sizeof(*free_node), GFP_ATOMIC);\n\tif (!free_node)\n\t\treturn -ENOMEM;\n\n\ttimestamp = ktime_to_ns(now);\n\n\t*pend->ts_reg_info.timestamp_kernel_addr = timestamp;\n\n\tdev_dbg(hdev->dev, \"Timestamp is set to ts cb address (%p), ts: 0x%llx\\n\",\n\t\t\tpend->ts_reg_info.timestamp_kernel_addr,\n\t\t\t*(u64 *)pend->ts_reg_info.timestamp_kernel_addr);\n\n\tlist_del(&pend->wait_list_node);\n\n\t \n\tpend->ts_reg_info.in_use = 0;\n\n\t \n\tfree_node->buf = pend->ts_reg_info.buf;\n\tfree_node->cq_cb = pend->ts_reg_info.cq_cb;\n\tlist_add(&free_node->free_objects_node, *free_list);\n\n\treturn 0;\n}\n\nstatic void handle_user_interrupt(struct hl_device *hdev, struct hl_user_interrupt *intr)\n{\n\tstruct hl_user_pending_interrupt *pend, *temp_pend;\n\tstruct list_head *ts_reg_free_list_head = NULL;\n\tstruct timestamp_reg_work_obj *job;\n\tbool reg_node_handle_fail = false;\n\tint rc;\n\n\t \n\tjob = kmalloc(sizeof(*job), GFP_ATOMIC);\n\tif (!job)\n\t\treturn;\n\n\tspin_lock(&intr->wait_list_lock);\n\tlist_for_each_entry_safe(pend, temp_pend, &intr->wait_list_head, wait_list_node) {\n\t\tif ((pend->cq_kernel_addr && *(pend->cq_kernel_addr) >= pend->cq_target_value) ||\n\t\t\t\t!pend->cq_kernel_addr) {\n\t\t\tif (pend->ts_reg_info.buf) {\n\t\t\t\tif (!reg_node_handle_fail) {\n\t\t\t\t\trc = handle_registration_node(hdev, pend,\n\t\t\t\t\t\t\t&ts_reg_free_list_head, intr->timestamp);\n\t\t\t\t\tif (rc)\n\t\t\t\t\t\treg_node_handle_fail = true;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tpend->fence.timestamp = intr->timestamp;\n\t\t\t\tcomplete_all(&pend->fence.completion);\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&intr->wait_list_lock);\n\n\tif (ts_reg_free_list_head) {\n\t\tINIT_WORK(&job->free_obj, hl_ts_free_objects);\n\t\tjob->free_obj_head = ts_reg_free_list_head;\n\t\tjob->hdev = hdev;\n\t\tqueue_work(hdev->ts_free_obj_wq, &job->free_obj);\n\t} else {\n\t\tkfree(job);\n\t}\n}\n\nstatic void handle_tpc_interrupt(struct hl_device *hdev)\n{\n\tu64 event_mask;\n\tu32 flags;\n\n\tevent_mask = HL_NOTIFIER_EVENT_TPC_ASSERT |\n\t\tHL_NOTIFIER_EVENT_USER_ENGINE_ERR |\n\t\tHL_NOTIFIER_EVENT_DEVICE_RESET;\n\n\tflags = HL_DRV_RESET_DELAY;\n\n\tdev_err_ratelimited(hdev->dev, \"Received TPC assert\\n\");\n\thl_device_cond_reset(hdev, flags, event_mask);\n}\n\nstatic void handle_unexpected_user_interrupt(struct hl_device *hdev)\n{\n\tdev_err_ratelimited(hdev->dev, \"Received unexpected user error interrupt\\n\");\n}\n\n \nirqreturn_t hl_irq_handler_user_interrupt(int irq, void *arg)\n{\n\tstruct hl_user_interrupt *user_int = arg;\n\n\tuser_int->timestamp = ktime_get();\n\n\treturn IRQ_WAKE_THREAD;\n}\n\n \nirqreturn_t hl_irq_user_interrupt_thread_handler(int irq, void *arg)\n{\n\tstruct hl_user_interrupt *user_int = arg;\n\tstruct hl_device *hdev = user_int->hdev;\n\n\tswitch (user_int->type) {\n\tcase HL_USR_INTERRUPT_CQ:\n\t\thandle_user_interrupt(hdev, &hdev->common_user_cq_interrupt);\n\n\t\t \n\t\thandle_user_interrupt(hdev, user_int);\n\t\tbreak;\n\tcase HL_USR_INTERRUPT_DECODER:\n\t\thandle_user_interrupt(hdev, &hdev->common_decoder_interrupt);\n\n\t\t \n\t\thandle_user_interrupt(hdev, user_int);\n\t\tbreak;\n\tcase HL_USR_INTERRUPT_TPC:\n\t\thandle_tpc_interrupt(hdev);\n\t\tbreak;\n\tcase HL_USR_INTERRUPT_UNEXPECTED:\n\t\thandle_unexpected_user_interrupt(hdev);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nirqreturn_t hl_irq_handler_eq(int irq, void *arg)\n{\n\tstruct hl_eq *eq = arg;\n\tstruct hl_device *hdev = eq->hdev;\n\tstruct hl_eq_entry *eq_entry;\n\tstruct hl_eq_entry *eq_base;\n\tstruct hl_eqe_work *handle_eqe_work;\n\tbool entry_ready;\n\tu32 cur_eqe, ctl;\n\tu16 cur_eqe_index, event_type;\n\n\teq_base = eq->kernel_address;\n\n\twhile (1) {\n\t\tcur_eqe = le32_to_cpu(eq_base[eq->ci].hdr.ctl);\n\t\tentry_ready = !!FIELD_GET(EQ_CTL_READY_MASK, cur_eqe);\n\n\t\tif (!entry_ready)\n\t\t\tbreak;\n\n\t\tcur_eqe_index = FIELD_GET(EQ_CTL_INDEX_MASK, cur_eqe);\n\t\tif ((hdev->event_queue.check_eqe_index) &&\n\t\t\t\t(((eq->prev_eqe_index + 1) & EQ_CTL_INDEX_MASK) != cur_eqe_index)) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"EQE %#x in queue is ready but index does not match %d!=%d\",\n\t\t\t\tcur_eqe,\n\t\t\t\t((eq->prev_eqe_index + 1) & EQ_CTL_INDEX_MASK),\n\t\t\t\tcur_eqe_index);\n\t\t\tbreak;\n\t\t}\n\n\t\teq->prev_eqe_index++;\n\n\t\teq_entry = &eq_base[eq->ci];\n\n\t\t \n\t\tdma_rmb();\n\n\t\tif (hdev->disabled && !hdev->reset_info.in_compute_reset) {\n\t\t\tctl = le32_to_cpu(eq_entry->hdr.ctl);\n\t\t\tevent_type = ((ctl & EQ_CTL_EVENT_TYPE_MASK) >> EQ_CTL_EVENT_TYPE_SHIFT);\n\t\t\tdev_warn(hdev->dev,\n\t\t\t\t\"Device disabled but received an EQ event (%u)\\n\", event_type);\n\t\t\tgoto skip_irq;\n\t\t}\n\n\t\thandle_eqe_work = kmalloc(sizeof(*handle_eqe_work), GFP_ATOMIC);\n\t\tif (handle_eqe_work) {\n\t\t\tINIT_WORK(&handle_eqe_work->eq_work, irq_handle_eqe);\n\t\t\thandle_eqe_work->hdev = hdev;\n\n\t\t\tmemcpy(&handle_eqe_work->eq_entry, eq_entry,\n\t\t\t\t\tsizeof(*eq_entry));\n\n\t\t\tqueue_work(hdev->eq_wq, &handle_eqe_work->eq_work);\n\t\t}\nskip_irq:\n\t\t \n\t\teq_entry->hdr.ctl =\n\t\t\tcpu_to_le32(le32_to_cpu(eq_entry->hdr.ctl) &\n\t\t\t\t\t\t\t~EQ_CTL_READY_MASK);\n\n\t\teq->ci = hl_eq_inc_ptr(eq->ci);\n\n\t\thdev->asic_funcs->update_eq_ci(hdev, eq->ci);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nirqreturn_t hl_irq_handler_dec_abnrm(int irq, void *arg)\n{\n\tstruct hl_dec *dec = arg;\n\n\tschedule_work(&dec->abnrm_intr_work);\n\n\treturn IRQ_HANDLED;\n}\n\n \nint hl_cq_init(struct hl_device *hdev, struct hl_cq *q, u32 hw_queue_id)\n{\n\tvoid *p;\n\n\tp = hl_asic_dma_alloc_coherent(hdev, HL_CQ_SIZE_IN_BYTES, &q->bus_address,\n\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tq->hdev = hdev;\n\tq->kernel_address = p;\n\tq->hw_queue_id = hw_queue_id;\n\tq->ci = 0;\n\tq->pi = 0;\n\n\tatomic_set(&q->free_slots_cnt, HL_CQ_LENGTH);\n\n\treturn 0;\n}\n\n \nvoid hl_cq_fini(struct hl_device *hdev, struct hl_cq *q)\n{\n\thl_asic_dma_free_coherent(hdev, HL_CQ_SIZE_IN_BYTES, q->kernel_address, q->bus_address);\n}\n\nvoid hl_cq_reset(struct hl_device *hdev, struct hl_cq *q)\n{\n\tq->ci = 0;\n\tq->pi = 0;\n\n\tatomic_set(&q->free_slots_cnt, HL_CQ_LENGTH);\n\n\t \n\n\tmemset(q->kernel_address, 0, HL_CQ_SIZE_IN_BYTES);\n}\n\n \nint hl_eq_init(struct hl_device *hdev, struct hl_eq *q)\n{\n\tvoid *p;\n\n\tp = hl_cpu_accessible_dma_pool_alloc(hdev, HL_EQ_SIZE_IN_BYTES, &q->bus_address);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tq->hdev = hdev;\n\tq->kernel_address = p;\n\tq->ci = 0;\n\tq->prev_eqe_index = 0;\n\n\treturn 0;\n}\n\n \nvoid hl_eq_fini(struct hl_device *hdev, struct hl_eq *q)\n{\n\tflush_workqueue(hdev->eq_wq);\n\n\thl_cpu_accessible_dma_pool_free(hdev, HL_EQ_SIZE_IN_BYTES, q->kernel_address);\n}\n\nvoid hl_eq_reset(struct hl_device *hdev, struct hl_eq *q)\n{\n\tq->ci = 0;\n\tq->prev_eqe_index = 0;\n\n\t \n\n\tmemset(q->kernel_address, 0, HL_EQ_SIZE_IN_BYTES);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}