{
  "module_name": "habanalabs.h",
  "hash_id": "182da3501d4ac3c1a355e42c985998f5d23fd712d94758a397dee1b60a7a340a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/habanalabs.h",
  "human_readable_source": " \n\n#ifndef HABANALABSP_H_\n#define HABANALABSP_H_\n\n#include \"../include/common/cpucp_if.h\"\n#include \"../include/common/qman_if.h\"\n#include \"../include/hw_ip/mmu/mmu_general.h\"\n#include <uapi/drm/habanalabs_accel.h>\n\n#include <linux/cdev.h>\n#include <linux/iopoll.h>\n#include <linux/irqreturn.h>\n#include <linux/dma-direction.h>\n#include <linux/scatterlist.h>\n#include <linux/hashtable.h>\n#include <linux/debugfs.h>\n#include <linux/rwsem.h>\n#include <linux/eventfd.h>\n#include <linux/bitfield.h>\n#include <linux/genalloc.h>\n#include <linux/sched/signal.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/coresight.h>\n#include <linux/dma-buf.h>\n\n#include \"security.h\"\n\n#define HL_NAME\t\t\t\t\"habanalabs\"\n\nstruct hl_device;\nstruct hl_fpriv;\n\n#define PCI_VENDOR_ID_HABANALABS\t0x1da3\n\n \n#define HL_MMAP_TYPE_SHIFT\t\t(59 - PAGE_SHIFT)\n#define HL_MMAP_TYPE_MASK\t\t(0x1full << HL_MMAP_TYPE_SHIFT)\n#define HL_MMAP_TYPE_TS_BUFF\t\t(0x10ull << HL_MMAP_TYPE_SHIFT)\n#define HL_MMAP_TYPE_BLOCK\t\t(0x4ull << HL_MMAP_TYPE_SHIFT)\n#define HL_MMAP_TYPE_CB\t\t\t(0x2ull << HL_MMAP_TYPE_SHIFT)\n\n#define HL_MMAP_OFFSET_VALUE_MASK\t(0x1FFFFFFFFFFFull >> PAGE_SHIFT)\n#define HL_MMAP_OFFSET_VALUE_GET(off)\t(off & HL_MMAP_OFFSET_VALUE_MASK)\n\n#define HL_PENDING_RESET_PER_SEC\t\t10\n#define HL_PENDING_RESET_MAX_TRIALS\t\t60  \n#define HL_PENDING_RESET_LONG_SEC\t\t60\n \n#define HL_WAIT_PROCESS_KILL_ON_DEVICE_FINI\t600\n\n#define HL_HARD_RESET_MAX_TIMEOUT\t120\n#define HL_PLDM_HARD_RESET_MAX_TIMEOUT\t(HL_HARD_RESET_MAX_TIMEOUT * 3)\n\n#define HL_DEVICE_TIMEOUT_USEC\t\t1000000  \n\n#define HL_HEARTBEAT_PER_USEC\t\t5000000  \n\n#define HL_PLL_LOW_JOB_FREQ_USEC\t5000000  \n\n#define HL_CPUCP_INFO_TIMEOUT_USEC\t10000000  \n#define HL_CPUCP_EEPROM_TIMEOUT_USEC\t10000000  \n#define HL_CPUCP_MON_DUMP_TIMEOUT_USEC\t10000000  \n#define HL_CPUCP_SEC_ATTEST_INFO_TINEOUT_USEC 10000000  \n\n#define HL_FW_STATUS_POLL_INTERVAL_USEC\t\t10000  \n#define HL_FW_COMMS_STATUS_PLDM_POLL_INTERVAL_USEC\t1000000  \n\n#define HL_PCI_ELBI_TIMEOUT_MSEC\t10  \n\n#define HL_SIM_MAX_TIMEOUT_US\t\t100000000  \n\n#define HL_INVALID_QUEUE\t\tUINT_MAX\n\n#define HL_COMMON_USER_CQ_INTERRUPT_ID\t0xFFF\n#define HL_COMMON_DEC_INTERRUPT_ID\t0xFFE\n\n#define HL_STATE_DUMP_HIST_LEN\t\t5\n\n \n#define HL_RESET_TRIGGER_DEFAULT\t0xFF\n\n#define OBJ_NAMES_HASH_TABLE_BITS\t7  \n#define SYNC_TO_ENGINE_HASH_TABLE_BITS\t7  \n\n \n#define MEM_HASH_TABLE_BITS\t\t7  \n\n \n#define MMU_HASH_TABLE_BITS\t\t7  \n\n \nenum hl_mmu_page_table_location {\n\tMMU_DR_PGT = 0,\t\t \n\tMMU_HR_PGT,\t\t \n\tMMU_NUM_PGT_LOCATIONS\t \n};\n\n \n#define HL_RSVD_SOBS\t\t\t2\n#define HL_RSVD_MONS\t\t\t1\n\n \n#define HL_COLLECTIVE_RSVD_MSTR_MONS\t2\n\n#define HL_MAX_SOB_VAL\t\t\t(1 << 15)\n\n#define IS_POWER_OF_2(n)\t\t(n != 0 && ((n & (n - 1)) == 0))\n#define IS_MAX_PENDING_CS_VALID(n)\t(IS_POWER_OF_2(n) && (n > 1))\n\n#define HL_PCI_NUM_BARS\t\t\t6\n\n \n#define HL_COMPLETION_MODE_JOB\t\t0\n \n#define HL_COMPLETION_MODE_CS\t\t1\n\n#define HL_MAX_DCORES\t\t\t8\n\n \n#define hl_asic_dma_alloc_coherent(hdev, size, dma_handle, flags) \\\n\thl_asic_dma_alloc_coherent_caller(hdev, size, dma_handle, flags, __func__)\n\n#define hl_asic_dma_pool_zalloc(hdev, size, mem_flags, dma_handle) \\\n\thl_asic_dma_pool_zalloc_caller(hdev, size, mem_flags, dma_handle, __func__)\n\n#define hl_asic_dma_free_coherent(hdev, size, cpu_addr, dma_handle) \\\n\thl_asic_dma_free_coherent_caller(hdev, size, cpu_addr, dma_handle, __func__)\n\n#define hl_asic_dma_pool_free(hdev, vaddr, dma_addr) \\\n\thl_asic_dma_pool_free_caller(hdev, vaddr, dma_addr, __func__)\n\n \n\n#define HL_DRV_RESET_HARD\t\t(1 << 0)\n#define HL_DRV_RESET_FROM_RESET_THR\t(1 << 1)\n#define HL_DRV_RESET_HEARTBEAT\t\t(1 << 2)\n#define HL_DRV_RESET_TDR\t\t(1 << 3)\n#define HL_DRV_RESET_DEV_RELEASE\t(1 << 4)\n#define HL_DRV_RESET_BYPASS_REQ_TO_FW\t(1 << 5)\n#define HL_DRV_RESET_FW_FATAL_ERR\t(1 << 6)\n#define HL_DRV_RESET_DELAY\t\t(1 << 7)\n#define HL_DRV_RESET_FROM_WD_THR\t(1 << 8)\n\n \n\n#define HL_PB_SHARED\t\t1\n#define HL_PB_NA\t\t0\n#define HL_PB_SINGLE_INSTANCE\t1\n#define HL_BLOCK_SIZE\t\t0x1000\n#define HL_BLOCK_GLBL_ERR_MASK\t0xF40\n#define HL_BLOCK_GLBL_ERR_ADDR\t0xF44\n#define HL_BLOCK_GLBL_ERR_CAUSE\t0xF48\n#define HL_BLOCK_GLBL_SEC_OFFS\t0xF80\n#define HL_BLOCK_GLBL_SEC_SIZE\t(HL_BLOCK_SIZE - HL_BLOCK_GLBL_SEC_OFFS)\n#define HL_BLOCK_GLBL_SEC_LEN\t(HL_BLOCK_GLBL_SEC_SIZE / sizeof(u32))\n#define UNSET_GLBL_SEC_BIT(array, b) ((array)[((b) / 32)] |= (1 << ((b) % 32)))\n\nenum hl_protection_levels {\n\tSECURED_LVL,\n\tPRIVILEGED_LVL,\n\tNON_SECURED_LVL\n};\n\n \nstruct iterate_module_ctx {\n\t \n\tvoid (*fn)(struct hl_device *hdev, int block, int inst, u32 offset,\n\t\t\tstruct iterate_module_ctx *ctx);\n\tvoid *data;\n\tint rc;\n};\n\nstruct hl_block_glbl_sec {\n\tu32 sec_array[HL_BLOCK_GLBL_SEC_LEN];\n};\n\n#define HL_MAX_SOBS_PER_MONITOR\t8\n\n \nstruct hl_gen_wait_properties {\n\tvoid\t*data;\n\tu32\tq_idx;\n\tu32\tsize;\n\tu16\tsob_base;\n\tu16\tsob_val;\n\tu16\tmon_id;\n\tu8\tsob_mask;\n};\n\n \nstruct pgt_info {\n\tstruct hlist_node\tnode;\n\tu64\t\t\tphys_addr;\n\tu64\t\t\tvirt_addr;\n\tu64\t\t\tshadow_addr;\n\tstruct hl_ctx\t\t*ctx;\n\tint\t\t\tnum_of_ptes;\n};\n\n \nenum hl_pci_match_mode {\n\tPCI_ADDRESS_MATCH_MODE,\n\tPCI_BAR_MATCH_MODE\n};\n\n \nenum hl_fw_component {\n\tFW_COMP_BOOT_FIT,\n\tFW_COMP_PREBOOT,\n\tFW_COMP_LINUX,\n};\n\n \nenum hl_fw_types {\n\tFW_TYPE_NONE = 0x0,\n\tFW_TYPE_LINUX = 0x1,\n\tFW_TYPE_BOOT_CPU = 0x2,\n\tFW_TYPE_PREBOOT_CPU = 0x4,\n\tFW_TYPE_ALL_TYPES =\n\t\t(FW_TYPE_LINUX | FW_TYPE_BOOT_CPU | FW_TYPE_PREBOOT_CPU)\n};\n\n \nenum hl_queue_type {\n\tQUEUE_TYPE_NA,\n\tQUEUE_TYPE_EXT,\n\tQUEUE_TYPE_INT,\n\tQUEUE_TYPE_CPU,\n\tQUEUE_TYPE_HW\n};\n\nenum hl_cs_type {\n\tCS_TYPE_DEFAULT,\n\tCS_TYPE_SIGNAL,\n\tCS_TYPE_WAIT,\n\tCS_TYPE_COLLECTIVE_WAIT,\n\tCS_RESERVE_SIGNALS,\n\tCS_UNRESERVE_SIGNALS,\n\tCS_TYPE_ENGINE_CORE,\n\tCS_TYPE_ENGINES,\n\tCS_TYPE_FLUSH_PCI_HBW_WRITES,\n};\n\n \nstruct hl_inbound_pci_region {\n\tenum hl_pci_match_mode\tmode;\n\tu64\t\t\taddr;\n\tu64\t\t\tsize;\n\tu64\t\t\toffset_in_bar;\n\tu8\t\t\tbar;\n};\n\n \nstruct hl_outbound_pci_region {\n\tu64\taddr;\n\tu64\tsize;\n};\n\n \nenum queue_cb_alloc_flags {\n\tCB_ALLOC_KERNEL = 0x1,\n\tCB_ALLOC_USER   = 0x2\n};\n\n \nstruct hl_hw_sob {\n\tstruct hl_device\t*hdev;\n\tstruct kref\t\tkref;\n\tu32\t\t\tsob_id;\n\tu32\t\t\tsob_addr;\n\tu32\t\t\tq_idx;\n\tbool\t\t\tneed_reset;\n};\n\nenum hl_collective_mode {\n\tHL_COLLECTIVE_NOT_SUPPORTED = 0x0,\n\tHL_COLLECTIVE_MASTER = 0x1,\n\tHL_COLLECTIVE_SLAVE = 0x2\n};\n\n \nstruct hw_queue_properties {\n\tenum hl_queue_type\t\ttype;\n\tenum queue_cb_alloc_flags\tcb_alloc_flags;\n\tenum hl_collective_mode\t\tcollective_mode;\n\tu8\t\t\t\tdriver_only;\n\tu8\t\t\t\tbinned;\n\tu8\t\t\t\tsupports_sync_stream;\n};\n\n \nenum vm_type {\n\tVM_TYPE_USERPTR = 0x1,\n\tVM_TYPE_PHYS_PACK = 0x2\n};\n\n \nenum mmu_op_flags {\n\tMMU_OP_USERPTR = 0x1,\n\tMMU_OP_PHYS_PACK = 0x2,\n\tMMU_OP_CLEAR_MEMCACHE = 0x4,\n\tMMU_OP_SKIP_LOW_CACHE_INV = 0x8,\n};\n\n\n \nenum hl_device_hw_state {\n\tHL_DEVICE_HW_STATE_CLEAN = 0,\n\tHL_DEVICE_HW_STATE_DIRTY\n};\n\n#define HL_MMU_VA_ALIGNMENT_NOT_NEEDED 0\n\n \nstruct hl_mmu_properties {\n\tu64\tstart_addr;\n\tu64\tend_addr;\n\tu64\thop_shifts[MMU_HOP_MAX];\n\tu64\thop_masks[MMU_HOP_MAX];\n\tu64\tlast_mask;\n\tu64\tpgt_size;\n\tu64\tsupported_pages_mask;\n\tu32\tpage_size;\n\tu32\tnum_hops;\n\tu32\thop_table_size;\n\tu32\thop0_tables_total_size;\n\tu8\thost_resident;\n};\n\n \nstruct hl_hints_range {\n\tu64 start_addr;\n\tu64 end_addr;\n};\n\n \nstruct asic_fixed_properties {\n\tstruct hw_queue_properties\t*hw_queues_props;\n\tstruct hl_special_block_info\t*special_blocks;\n\tstruct hl_skip_blocks_cfg\tskip_special_blocks_cfg;\n\tstruct cpucp_info\t\tcpucp_info;\n\tchar\t\t\t\tuboot_ver[VERSION_MAX_LEN];\n\tchar\t\t\t\tpreboot_ver[VERSION_MAX_LEN];\n\tstruct hl_mmu_properties\tdmmu;\n\tstruct hl_mmu_properties\tpmmu;\n\tstruct hl_mmu_properties\tpmmu_huge;\n\tstruct hl_hints_range\t\thints_dram_reserved_va_range;\n\tstruct hl_hints_range\t\thints_host_reserved_va_range;\n\tstruct hl_hints_range\t\thints_host_hpage_reserved_va_range;\n\tu64\t\t\t\tsram_base_address;\n\tu64\t\t\t\tsram_end_address;\n\tu64\t\t\t\tsram_user_base_address;\n\tu64\t\t\t\tdram_base_address;\n\tu64\t\t\t\tdram_end_address;\n\tu64\t\t\t\tdram_user_base_address;\n\tu64\t\t\t\tdram_size;\n\tu64\t\t\t\tdram_pci_bar_size;\n\tu64\t\t\t\tmax_power_default;\n\tu64\t\t\t\tdc_power_default;\n\tu64\t\t\t\tdram_size_for_default_page_mapping;\n\tu64\t\t\t\tpcie_dbi_base_address;\n\tu64\t\t\t\tpcie_aux_dbi_reg_addr;\n\tu64\t\t\t\tmmu_pgt_addr;\n\tu64\t\t\t\tmmu_dram_default_page_addr;\n\tu64\t\t\t\ttpc_enabled_mask;\n\tu64\t\t\t\ttpc_binning_mask;\n\tu64\t\t\t\tdram_enabled_mask;\n\tu64\t\t\t\tdram_binning_mask;\n\tu64\t\t\t\tdram_hints_align_mask;\n\tu64\t\t\t\tcfg_base_address;\n\tu64\t\t\t\tmmu_cache_mng_addr;\n\tu64\t\t\t\tmmu_cache_mng_size;\n\tu64\t\t\t\tdevice_dma_offset_for_host_access;\n\tu64\t\t\t\thost_base_address;\n\tu64\t\t\t\thost_end_address;\n\tu64\t\t\t\tmax_freq_value;\n\tu64\t\t\t\tengine_core_interrupt_reg_addr;\n\tu32\t\t\t\tclk_pll_index;\n\tu32\t\t\t\tmmu_pgt_size;\n\tu32\t\t\t\tmmu_pte_size;\n\tu32\t\t\t\tmmu_hop_table_size;\n\tu32\t\t\t\tmmu_hop0_tables_total_size;\n\tu32\t\t\t\tdram_page_size;\n\tu32\t\t\t\tcfg_size;\n\tu32\t\t\t\tsram_size;\n\tu32\t\t\t\tmax_asid;\n\tu32\t\t\t\tnum_of_events;\n\tu32\t\t\t\tpsoc_pci_pll_nr;\n\tu32\t\t\t\tpsoc_pci_pll_nf;\n\tu32\t\t\t\tpsoc_pci_pll_od;\n\tu32\t\t\t\tpsoc_pci_pll_div_factor;\n\tu32\t\t\t\tpsoc_timestamp_frequency;\n\tu32\t\t\t\thigh_pll;\n\tu32\t\t\t\tcb_pool_cb_cnt;\n\tu32\t\t\t\tcb_pool_cb_size;\n\tu32\t\t\t\tdecoder_enabled_mask;\n\tu32\t\t\t\tdecoder_binning_mask;\n\tu32\t\t\t\trotator_enabled_mask;\n\tu32\t\t\t\tedma_enabled_mask;\n\tu32\t\t\t\tedma_binning_mask;\n\tu32\t\t\t\tmax_pending_cs;\n\tu32\t\t\t\tmax_queues;\n\tu32\t\t\t\tfw_preboot_cpu_boot_dev_sts0;\n\tu32\t\t\t\tfw_preboot_cpu_boot_dev_sts1;\n\tu32\t\t\t\tfw_bootfit_cpu_boot_dev_sts0;\n\tu32\t\t\t\tfw_bootfit_cpu_boot_dev_sts1;\n\tu32\t\t\t\tfw_app_cpu_boot_dev_sts0;\n\tu32\t\t\t\tfw_app_cpu_boot_dev_sts1;\n\tu32\t\t\t\tmax_dec;\n\tu32\t\t\t\thmmu_hif_enabled_mask;\n\tu32\t\t\t\tfaulty_dram_cluster_map;\n\tu32\t\t\t\txbar_edge_enabled_mask;\n\tu32\t\t\t\tdevice_mem_alloc_default_page_size;\n\tu32\t\t\t\tnum_engine_cores;\n\tu32\t\t\t\tmax_num_of_engines;\n\tu32\t\t\t\tnum_of_special_blocks;\n\tu32\t\t\t\tglbl_err_cause_num;\n\tu32\t\t\t\thbw_flush_reg;\n\tu16\t\t\t\tcollective_first_sob;\n\tu16\t\t\t\tcollective_first_mon;\n\tu16\t\t\t\tsync_stream_first_sob;\n\tu16\t\t\t\tsync_stream_first_mon;\n\tu16\t\t\t\tfirst_available_user_sob[HL_MAX_DCORES];\n\tu16\t\t\t\tfirst_available_user_mon[HL_MAX_DCORES];\n\tu16\t\t\t\tfirst_available_user_interrupt;\n\tu16\t\t\t\tfirst_available_cq[HL_MAX_DCORES];\n\tu16\t\t\t\tuser_interrupt_count;\n\tu16\t\t\t\tuser_dec_intr_count;\n\tu16\t\t\t\ttpc_interrupt_id;\n\tu16\t\t\t\teq_interrupt_id;\n\tu16\t\t\t\tcache_line_size;\n\tu16\t\t\t\tserver_type;\n\tu8\t\t\t\tcompletion_queues_count;\n\tu8\t\t\t\tcompletion_mode;\n\tu8\t\t\t\tmme_master_slave_mode;\n\tu8\t\t\t\tfw_security_enabled;\n\tu8\t\t\t\tfw_cpu_boot_dev_sts0_valid;\n\tu8\t\t\t\tfw_cpu_boot_dev_sts1_valid;\n\tu8\t\t\t\tdram_supports_virtual_memory;\n\tu8\t\t\t\thard_reset_done_by_fw;\n\tu8\t\t\t\tnum_functional_hbms;\n\tu8\t\t\t\thints_range_reservation;\n\tu8\t\t\t\tiatu_done_by_fw;\n\tu8\t\t\t\tdynamic_fw_load;\n\tu8\t\t\t\tgic_interrupts_enable;\n\tu8\t\t\t\tuse_get_power_for_reset_history;\n\tu8\t\t\t\tsupports_compute_reset;\n\tu8\t\t\t\tallow_inference_soft_reset;\n\tu8\t\t\t\tconfigurable_stop_on_err;\n\tu8\t\t\t\tset_max_power_on_device_init;\n\tu8\t\t\t\tsupports_user_set_page_size;\n\tu8\t\t\t\tdma_mask;\n\tu8\t\t\t\tsupports_advanced_cpucp_rc;\n\tu8\t\t\t\tsupports_engine_modes;\n};\n\n \nstruct hl_fence {\n\tstruct completion\tcompletion;\n\tstruct kref\t\trefcount;\n\tu64\t\t\tcs_sequence;\n\tu32\t\t\tstream_master_qid_map;\n\tint\t\t\terror;\n\tktime_t\t\t\ttimestamp;\n\tu8\t\t\tmcs_handling_done;\n};\n\n \nstruct hl_cs_compl {\n\tstruct hl_fence\t\tbase_fence;\n\tspinlock_t\t\tlock;\n\tstruct hl_device\t*hdev;\n\tstruct hl_hw_sob\t*hw_sob;\n\tstruct hl_cs_encaps_sig_handle *encaps_sig_hdl;\n\tu64\t\t\tcs_seq;\n\tenum hl_cs_type\t\ttype;\n\tu16\t\t\tsob_val;\n\tu16\t\t\tsob_group;\n\tbool\t\t\tencaps_signals;\n};\n\n \n\n \nstruct hl_ts_buff {\n\tvoid\t\t\t*kernel_buff_address;\n\tvoid\t\t\t*user_buff_address;\n\tu32\t\t\tkernel_buff_size;\n};\n\nstruct hl_mmap_mem_buf;\n\n \nstruct hl_mem_mgr {\n\tstruct device *dev;\n\tspinlock_t lock;\n\tstruct idr handles;\n};\n\n \nstruct hl_mmap_mem_buf_behavior {\n\tconst char *topic;\n\tu64 mem_id;\n\n\tint (*alloc)(struct hl_mmap_mem_buf *buf, gfp_t gfp, void *args);\n\tint (*mmap)(struct hl_mmap_mem_buf *buf, struct vm_area_struct *vma, void *args);\n\tvoid (*release)(struct hl_mmap_mem_buf *buf);\n};\n\n \nstruct hl_mmap_mem_buf {\n\tstruct hl_mmap_mem_buf_behavior *behavior;\n\tstruct hl_mem_mgr *mmg;\n\tstruct kref refcount;\n\tvoid *private;\n\tatomic_t mmap;\n\tu64 real_mapped_size;\n\tu64 mappable_size;\n\tu64 handle;\n};\n\n \nstruct hl_cb {\n\tstruct hl_device\t*hdev;\n\tstruct hl_ctx\t\t*ctx;\n\tstruct hl_mmap_mem_buf\t*buf;\n\tstruct list_head\tdebugfs_list;\n\tstruct list_head\tpool_list;\n\tvoid\t\t\t*kernel_address;\n\tu64\t\t\tvirtual_addr;\n\tdma_addr_t\t\tbus_address;\n\tu32\t\t\tsize;\n\tu32\t\t\troundup_size;\n\tatomic_t\t\tcs_cnt;\n\tatomic_t\t\tis_handle_destroyed;\n\tu8\t\t\tis_pool;\n\tu8\t\t\tis_internal;\n\tu8\t\t\tis_mmu_mapped;\n};\n\n\n \n\nstruct hl_cs_job;\n\n \n#define HL_QUEUE_LENGTH\t\t\t4096\n#define HL_QUEUE_SIZE_IN_BYTES\t\t(HL_QUEUE_LENGTH * HL_BD_SIZE)\n\n#if (HL_MAX_JOBS_PER_CS > HL_QUEUE_LENGTH)\n#error \"HL_QUEUE_LENGTH must be greater than HL_MAX_JOBS_PER_CS\"\n#endif\n\n \n#define HL_CQ_LENGTH\t\t\tHL_QUEUE_LENGTH\n#define HL_CQ_SIZE_IN_BYTES\t\t(HL_CQ_LENGTH * HL_CQ_ENTRY_SIZE)\n\n \n#define HL_EQ_LENGTH\t\t\t64\n#define HL_EQ_SIZE_IN_BYTES\t\t(HL_EQ_LENGTH * HL_EQ_ENTRY_SIZE)\n\n \n#define HL_CPU_ACCESSIBLE_MEM_SIZE\tSZ_2M\n\n \nstruct hl_sync_stream_properties {\n\tstruct hl_hw_sob hw_sob[HL_RSVD_SOBS];\n\tu16\t\tnext_sob_val;\n\tu16\t\tbase_sob_id;\n\tu16\t\tbase_mon_id;\n\tu16\t\tcollective_mstr_mon_id[HL_COLLECTIVE_RSVD_MSTR_MONS];\n\tu16\t\tcollective_slave_mon_id;\n\tu16\t\tcollective_sob_id;\n\tu8\t\tcurr_sob_offset;\n};\n\n \nstruct hl_encaps_signals_mgr {\n\tspinlock_t\t\tlock;\n\tstruct idr\t\thandles;\n};\n\n \nstruct hl_hw_queue {\n\tstruct hl_cs_job\t\t\t**shadow_queue;\n\tstruct hl_sync_stream_properties\tsync_stream_prop;\n\tenum hl_queue_type\t\t\tqueue_type;\n\tenum hl_collective_mode\t\t\tcollective_mode;\n\tvoid\t\t\t\t\t*kernel_address;\n\tdma_addr_t\t\t\t\tbus_address;\n\tu32\t\t\t\t\tpi;\n\tatomic_t\t\t\t\tci;\n\tu32\t\t\t\t\thw_queue_id;\n\tu32\t\t\t\t\tcq_id;\n\tu32\t\t\t\t\tmsi_vec;\n\tu16\t\t\t\t\tint_queue_len;\n\tu8\t\t\t\t\tvalid;\n\tu8\t\t\t\t\tsupports_sync_stream;\n};\n\n \nstruct hl_cq {\n\tstruct hl_device\t*hdev;\n\tvoid\t\t\t*kernel_address;\n\tdma_addr_t\t\tbus_address;\n\tu32\t\t\tcq_idx;\n\tu32\t\t\thw_queue_id;\n\tu32\t\t\tci;\n\tu32\t\t\tpi;\n\tatomic_t\t\tfree_slots_cnt;\n};\n\nenum hl_user_interrupt_type {\n\tHL_USR_INTERRUPT_CQ = 0,\n\tHL_USR_INTERRUPT_DECODER,\n\tHL_USR_INTERRUPT_TPC,\n\tHL_USR_INTERRUPT_UNEXPECTED\n};\n\n \nstruct hl_user_interrupt {\n\tstruct hl_device\t\t*hdev;\n\tenum hl_user_interrupt_type\ttype;\n\tstruct list_head\t\twait_list_head;\n\tspinlock_t\t\t\twait_list_lock;\n\tktime_t\t\t\t\ttimestamp;\n\tu32\t\t\t\tinterrupt_id;\n};\n\n \nstruct timestamp_reg_free_node {\n\tstruct list_head\tfree_objects_node;\n\tstruct hl_cb\t\t*cq_cb;\n\tstruct hl_mmap_mem_buf\t*buf;\n};\n\n \nstruct timestamp_reg_work_obj {\n\tstruct work_struct\tfree_obj;\n\tstruct hl_device\t*hdev;\n\tstruct list_head\t*free_obj_head;\n};\n\n \nstruct timestamp_reg_info {\n\tstruct hl_mmap_mem_buf\t*buf;\n\tstruct hl_cb\t\t*cq_cb;\n\tu64\t\t\t*timestamp_kernel_addr;\n\tu8\t\t\tin_use;\n};\n\n \nstruct hl_user_pending_interrupt {\n\tstruct timestamp_reg_info\tts_reg_info;\n\tstruct list_head\t\twait_list_node;\n\tstruct hl_fence\t\t\tfence;\n\tu64\t\t\t\tcq_target_value;\n\tu64\t\t\t\t*cq_kernel_addr;\n};\n\n \nstruct hl_eq {\n\tstruct hl_device\t*hdev;\n\tvoid\t\t\t*kernel_address;\n\tdma_addr_t\t\tbus_address;\n\tu32\t\t\tci;\n\tu32\t\t\tprev_eqe_index;\n\tbool\t\t\tcheck_eqe_index;\n};\n\n \nstruct hl_dec {\n\tstruct hl_device\t*hdev;\n\tstruct work_struct\tabnrm_intr_work;\n\tu32\t\t\tcore_id;\n\tu32\t\t\tbase_addr;\n};\n\n \nenum hl_asic_type {\n\tASIC_INVALID,\n\tASIC_GOYA,\n\tASIC_GAUDI,\n\tASIC_GAUDI_SEC,\n\tASIC_GAUDI2,\n\tASIC_GAUDI2B,\n};\n\nstruct hl_cs_parser;\n\n \nenum hl_pm_mng_profile {\n\tPM_AUTO = 1,\n\tPM_MANUAL,\n\tPM_LAST\n};\n\n \nenum hl_pll_frequency {\n\tPLL_HIGH = 1,\n\tPLL_LOW,\n\tPLL_LAST\n};\n\n#define PLL_REF_CLK 50\n\nenum div_select_defs {\n\tDIV_SEL_REF_CLK = 0,\n\tDIV_SEL_PLL_CLK = 1,\n\tDIV_SEL_DIVIDED_REF = 2,\n\tDIV_SEL_DIVIDED_PLL = 3,\n};\n\nenum debugfs_access_type {\n\tDEBUGFS_READ8,\n\tDEBUGFS_WRITE8,\n\tDEBUGFS_READ32,\n\tDEBUGFS_WRITE32,\n\tDEBUGFS_READ64,\n\tDEBUGFS_WRITE64,\n};\n\nenum pci_region {\n\tPCI_REGION_CFG,\n\tPCI_REGION_SRAM,\n\tPCI_REGION_DRAM,\n\tPCI_REGION_SP_SRAM,\n\tPCI_REGION_NUMBER,\n};\n\n \nstruct pci_mem_region {\n\tu64 region_base;\n\tu64 region_size;\n\tu64 bar_size;\n\tu64 offset_in_bar;\n\tu8 bar_id;\n\tu8 used;\n};\n\n \nstruct static_fw_load_mgr {\n\tu64 preboot_version_max_off;\n\tu64 boot_fit_version_max_off;\n\tu32 kmd_msg_to_cpu_reg;\n\tu32 cpu_cmd_status_to_host_reg;\n\tu32 cpu_boot_status_reg;\n\tu32 cpu_boot_dev_status0_reg;\n\tu32 cpu_boot_dev_status1_reg;\n\tu32 boot_err0_reg;\n\tu32 boot_err1_reg;\n\tu32 preboot_version_offset_reg;\n\tu32 boot_fit_version_offset_reg;\n\tu32 sram_offset_mask;\n\tu32 cpu_reset_wait_msec;\n};\n\n \nstruct fw_response {\n\tu32 ram_offset;\n\tu8 ram_type;\n\tu8 status;\n};\n\n \nstruct dynamic_fw_load_mgr {\n\tstruct fw_response response;\n\tstruct lkd_fw_comms_desc comm_desc;\n\tstruct pci_mem_region *image_region;\n\tsize_t fw_image_size;\n\tu32 wait_for_bl_timeout;\n\tbool fw_desc_valid;\n};\n\n \nstruct pre_fw_load_props {\n\tu32 cpu_boot_status_reg;\n\tu32 sts_boot_dev_sts0_reg;\n\tu32 sts_boot_dev_sts1_reg;\n\tu32 boot_err0_reg;\n\tu32 boot_err1_reg;\n\tu32 wait_for_preboot_timeout;\n};\n\n \nstruct fw_image_props {\n\tchar *image_name;\n\tu32 src_off;\n\tu32 copy_size;\n};\n\n \nstruct fw_load_mgr {\n\tunion {\n\t\tstruct dynamic_fw_load_mgr dynamic_loader;\n\t\tstruct static_fw_load_mgr static_loader;\n\t};\n\tstruct pre_fw_load_props pre_fw_load;\n\tstruct fw_image_props boot_fit_img;\n\tstruct fw_image_props linux_img;\n\tu32 cpu_timeout;\n\tu32 boot_fit_timeout;\n\tu8 skip_bmc;\n\tu8 sram_bar_id;\n\tu8 dram_bar_id;\n\tu8 fw_comp_loaded;\n};\n\nstruct hl_cs;\n\n \nstruct engines_data {\n\tchar *buf;\n\tint actual_size;\n\tu32 allocated_buf_size;\n};\n\n \nstruct hl_asic_funcs {\n\tint (*early_init)(struct hl_device *hdev);\n\tint (*early_fini)(struct hl_device *hdev);\n\tint (*late_init)(struct hl_device *hdev);\n\tvoid (*late_fini)(struct hl_device *hdev);\n\tint (*sw_init)(struct hl_device *hdev);\n\tint (*sw_fini)(struct hl_device *hdev);\n\tint (*hw_init)(struct hl_device *hdev);\n\tint (*hw_fini)(struct hl_device *hdev, bool hard_reset, bool fw_reset);\n\tvoid (*halt_engines)(struct hl_device *hdev, bool hard_reset, bool fw_reset);\n\tint (*suspend)(struct hl_device *hdev);\n\tint (*resume)(struct hl_device *hdev);\n\tint (*mmap)(struct hl_device *hdev, struct vm_area_struct *vma,\n\t\t\tvoid *cpu_addr, dma_addr_t dma_addr, size_t size);\n\tvoid (*ring_doorbell)(struct hl_device *hdev, u32 hw_queue_id, u32 pi);\n\tvoid (*pqe_write)(struct hl_device *hdev, __le64 *pqe,\n\t\t\tstruct hl_bd *bd);\n\tvoid* (*asic_dma_alloc_coherent)(struct hl_device *hdev, size_t size,\n\t\t\t\t\tdma_addr_t *dma_handle, gfp_t flag);\n\tvoid (*asic_dma_free_coherent)(struct hl_device *hdev, size_t size,\n\t\t\t\t\tvoid *cpu_addr, dma_addr_t dma_handle);\n\tint (*scrub_device_mem)(struct hl_device *hdev);\n\tint (*scrub_device_dram)(struct hl_device *hdev, u64 val);\n\tvoid* (*get_int_queue_base)(struct hl_device *hdev, u32 queue_id,\n\t\t\t\tdma_addr_t *dma_handle, u16 *queue_len);\n\tint (*test_queues)(struct hl_device *hdev);\n\tvoid* (*asic_dma_pool_zalloc)(struct hl_device *hdev, size_t size,\n\t\t\t\tgfp_t mem_flags, dma_addr_t *dma_handle);\n\tvoid (*asic_dma_pool_free)(struct hl_device *hdev, void *vaddr,\n\t\t\t\tdma_addr_t dma_addr);\n\tvoid* (*cpu_accessible_dma_pool_alloc)(struct hl_device *hdev,\n\t\t\t\tsize_t size, dma_addr_t *dma_handle);\n\tvoid (*cpu_accessible_dma_pool_free)(struct hl_device *hdev,\n\t\t\t\tsize_t size, void *vaddr);\n\tvoid (*asic_dma_unmap_single)(struct hl_device *hdev,\n\t\t\t\tdma_addr_t dma_addr, int len,\n\t\t\t\tenum dma_data_direction dir);\n\tdma_addr_t (*asic_dma_map_single)(struct hl_device *hdev,\n\t\t\t\tvoid *addr, int len,\n\t\t\t\tenum dma_data_direction dir);\n\tvoid (*hl_dma_unmap_sgtable)(struct hl_device *hdev,\n\t\t\t\tstruct sg_table *sgt,\n\t\t\t\tenum dma_data_direction dir);\n\tint (*cs_parser)(struct hl_device *hdev, struct hl_cs_parser *parser);\n\tint (*asic_dma_map_sgtable)(struct hl_device *hdev, struct sg_table *sgt,\n\t\t\t\tenum dma_data_direction dir);\n\tvoid (*add_end_of_cb_packets)(struct hl_device *hdev,\n\t\t\t\t\tvoid *kernel_address, u32 len,\n\t\t\t\t\tu32 original_len,\n\t\t\t\t\tu64 cq_addr, u32 cq_val, u32 msix_num,\n\t\t\t\t\tbool eb);\n\tvoid (*update_eq_ci)(struct hl_device *hdev, u32 val);\n\tint (*context_switch)(struct hl_device *hdev, u32 asid);\n\tvoid (*restore_phase_topology)(struct hl_device *hdev);\n\tint (*debugfs_read_dma)(struct hl_device *hdev, u64 addr, u32 size,\n\t\t\t\tvoid *blob_addr);\n\tvoid (*add_device_attr)(struct hl_device *hdev, struct attribute_group *dev_clk_attr_grp,\n\t\t\t\tstruct attribute_group *dev_vrm_attr_grp);\n\tvoid (*handle_eqe)(struct hl_device *hdev,\n\t\t\t\tstruct hl_eq_entry *eq_entry);\n\tvoid* (*get_events_stat)(struct hl_device *hdev, bool aggregate,\n\t\t\t\tu32 *size);\n\tu64 (*read_pte)(struct hl_device *hdev, u64 addr);\n\tvoid (*write_pte)(struct hl_device *hdev, u64 addr, u64 val);\n\tint (*mmu_invalidate_cache)(struct hl_device *hdev, bool is_hard,\n\t\t\t\t\tu32 flags);\n\tint (*mmu_invalidate_cache_range)(struct hl_device *hdev, bool is_hard,\n\t\t\t\tu32 flags, u32 asid, u64 va, u64 size);\n\tint (*mmu_prefetch_cache_range)(struct hl_ctx *ctx, u32 flags, u32 asid, u64 va, u64 size);\n\tint (*send_heartbeat)(struct hl_device *hdev);\n\tint (*debug_coresight)(struct hl_device *hdev, struct hl_ctx *ctx, void *data);\n\tbool (*is_device_idle)(struct hl_device *hdev, u64 *mask_arr, u8 mask_len,\n\t\t\t\tstruct engines_data *e);\n\tint (*compute_reset_late_init)(struct hl_device *hdev);\n\tvoid (*hw_queues_lock)(struct hl_device *hdev);\n\tvoid (*hw_queues_unlock)(struct hl_device *hdev);\n\tu32 (*get_pci_id)(struct hl_device *hdev);\n\tint (*get_eeprom_data)(struct hl_device *hdev, void *data, size_t max_size);\n\tint (*get_monitor_dump)(struct hl_device *hdev, void *data);\n\tint (*send_cpu_message)(struct hl_device *hdev, u32 *msg,\n\t\t\t\tu16 len, u32 timeout, u64 *result);\n\tint (*pci_bars_map)(struct hl_device *hdev);\n\tint (*init_iatu)(struct hl_device *hdev);\n\tu32 (*rreg)(struct hl_device *hdev, u32 reg);\n\tvoid (*wreg)(struct hl_device *hdev, u32 reg, u32 val);\n\tvoid (*halt_coresight)(struct hl_device *hdev, struct hl_ctx *ctx);\n\tint (*ctx_init)(struct hl_ctx *ctx);\n\tvoid (*ctx_fini)(struct hl_ctx *ctx);\n\tint (*pre_schedule_cs)(struct hl_cs *cs);\n\tu32 (*get_queue_id_for_cq)(struct hl_device *hdev, u32 cq_idx);\n\tint (*load_firmware_to_device)(struct hl_device *hdev);\n\tint (*load_boot_fit_to_device)(struct hl_device *hdev);\n\tu32 (*get_signal_cb_size)(struct hl_device *hdev);\n\tu32 (*get_wait_cb_size)(struct hl_device *hdev);\n\tu32 (*gen_signal_cb)(struct hl_device *hdev, void *data, u16 sob_id,\n\t\t\tu32 size, bool eb);\n\tu32 (*gen_wait_cb)(struct hl_device *hdev,\n\t\t\tstruct hl_gen_wait_properties *prop);\n\tvoid (*reset_sob)(struct hl_device *hdev, void *data);\n\tvoid (*reset_sob_group)(struct hl_device *hdev, u16 sob_group);\n\tu64 (*get_device_time)(struct hl_device *hdev);\n\tvoid (*pb_print_security_errors)(struct hl_device *hdev,\n\t\t\tu32 block_addr, u32 cause, u32 offended_addr);\n\tint (*collective_wait_init_cs)(struct hl_cs *cs);\n\tint (*collective_wait_create_jobs)(struct hl_device *hdev,\n\t\t\tstruct hl_ctx *ctx, struct hl_cs *cs,\n\t\t\tu32 wait_queue_id, u32 collective_engine_id,\n\t\t\tu32 encaps_signal_offset);\n\tu32 (*get_dec_base_addr)(struct hl_device *hdev, u32 core_id);\n\tu64 (*scramble_addr)(struct hl_device *hdev, u64 addr);\n\tu64 (*descramble_addr)(struct hl_device *hdev, u64 addr);\n\tvoid (*ack_protection_bits_errors)(struct hl_device *hdev);\n\tint (*get_hw_block_id)(struct hl_device *hdev, u64 block_addr,\n\t\t\t\tu32 *block_size, u32 *block_id);\n\tint (*hw_block_mmap)(struct hl_device *hdev, struct vm_area_struct *vma,\n\t\t\tu32 block_id, u32 block_size);\n\tvoid (*enable_events_from_fw)(struct hl_device *hdev);\n\tint (*ack_mmu_errors)(struct hl_device *hdev, u64 mmu_cap_mask);\n\tvoid (*get_msi_info)(__le32 *table);\n\tint (*map_pll_idx_to_fw_idx)(u32 pll_idx);\n\tvoid (*init_firmware_preload_params)(struct hl_device *hdev);\n\tvoid (*init_firmware_loader)(struct hl_device *hdev);\n\tvoid (*init_cpu_scrambler_dram)(struct hl_device *hdev);\n\tvoid (*state_dump_init)(struct hl_device *hdev);\n\tu32 (*get_sob_addr)(struct hl_device *hdev, u32 sob_id);\n\tvoid (*set_pci_memory_regions)(struct hl_device *hdev);\n\tu32* (*get_stream_master_qid_arr)(void);\n\tvoid (*check_if_razwi_happened)(struct hl_device *hdev);\n\tint (*mmu_get_real_page_size)(struct hl_device *hdev, struct hl_mmu_properties *mmu_prop,\n\t\t\t\t\tu32 page_size, u32 *real_page_size, bool is_dram_addr);\n\tint (*access_dev_mem)(struct hl_device *hdev, enum pci_region region_type,\n\t\t\t\tu64 addr, u64 *val, enum debugfs_access_type acc_type);\n\tu64 (*set_dram_bar_base)(struct hl_device *hdev, u64 addr);\n\tint (*set_engine_cores)(struct hl_device *hdev, u32 *core_ids,\n\t\t\t\t\tu32 num_cores, u32 core_command);\n\tint (*set_engines)(struct hl_device *hdev, u32 *engine_ids,\n\t\t\t\t\tu32 num_engines, u32 engine_command);\n\tint (*send_device_activity)(struct hl_device *hdev, bool open);\n\tint (*set_dram_properties)(struct hl_device *hdev);\n\tint (*set_binning_masks)(struct hl_device *hdev);\n};\n\n\n \n\n#define HL_KERNEL_ASID_ID\t0\n\n \nenum hl_va_range_type {\n\tHL_VA_RANGE_TYPE_HOST,\n\tHL_VA_RANGE_TYPE_HOST_HUGE,\n\tHL_VA_RANGE_TYPE_DRAM,\n\tHL_VA_RANGE_TYPE_MAX\n};\n\n \nstruct hl_va_range {\n\tstruct mutex\t\tlock;\n\tstruct list_head\tlist;\n\tu64\t\t\tstart_addr;\n\tu64\t\t\tend_addr;\n\tu32\t\t\tpage_size;\n};\n\n \nstruct hl_cs_counters_atomic {\n\tatomic64_t out_of_mem_drop_cnt;\n\tatomic64_t parsing_drop_cnt;\n\tatomic64_t queue_full_drop_cnt;\n\tatomic64_t device_in_reset_drop_cnt;\n\tatomic64_t max_cs_in_flight_drop_cnt;\n\tatomic64_t validation_drop_cnt;\n};\n\n \nstruct hl_dmabuf_priv {\n\tstruct dma_buf\t\t\t*dmabuf;\n\tstruct hl_ctx\t\t\t*ctx;\n\tstruct hl_vm_phys_pg_pack\t*phys_pg_pack;\n\tstruct hl_vm_hash_node\t\t*memhash_hnode;\n\tuint64_t\t\t\tdevice_address;\n};\n\n#define HL_CS_OUTCOME_HISTORY_LEN 256\n\n \nstruct hl_cs_outcome {\n\tstruct list_head list_link;\n\tstruct hlist_node map_link;\n\tktime_t ts;\n\tu64 seq;\n\tint error;\n};\n\n \nstruct hl_cs_outcome_store {\n\tDECLARE_HASHTABLE(outcome_map, 8);\n\tstruct list_head used_list;\n\tstruct list_head free_list;\n\tstruct hl_cs_outcome nodes_pool[HL_CS_OUTCOME_HISTORY_LEN];\n\tspinlock_t db_lock;\n};\n\n \nstruct hl_ctx {\n\tDECLARE_HASHTABLE(mem_hash, MEM_HASH_TABLE_BITS);\n\tDECLARE_HASHTABLE(mmu_shadow_hash, MMU_HASH_TABLE_BITS);\n\tDECLARE_HASHTABLE(hr_mmu_phys_hash, MMU_HASH_TABLE_BITS);\n\tstruct hl_fpriv\t\t\t*hpriv;\n\tstruct hl_device\t\t*hdev;\n\tstruct kref\t\t\trefcount;\n\tstruct hl_fence\t\t\t**cs_pending;\n\tstruct hl_cs_outcome_store\toutcome_store;\n\tstruct hl_va_range\t\t*va_range[HL_VA_RANGE_TYPE_MAX];\n\tstruct mutex\t\t\tmem_hash_lock;\n\tstruct mutex\t\t\thw_block_list_lock;\n\tstruct list_head\t\tdebugfs_list;\n\tstruct list_head\t\thw_block_mem_list;\n\tstruct hl_cs_counters_atomic\tcs_counters;\n\tstruct gen_pool\t\t\t*cb_va_pool;\n\tstruct hl_encaps_signals_mgr\tsig_mgr;\n\tu64\t\t\t\tcb_va_pool_base;\n\tu64\t\t\t\tcs_sequence;\n\tu64\t\t\t\t*dram_default_hops;\n\tspinlock_t\t\t\tcs_lock;\n\tatomic64_t\t\t\tdram_phys_mem;\n\tatomic_t\t\t\tthread_ctx_switch_token;\n\tu32\t\t\t\tthread_ctx_switch_wait_token;\n\tu32\t\t\t\tasid;\n\tu32\t\t\t\thandle;\n};\n\n \nstruct hl_ctx_mgr {\n\tstruct mutex\tlock;\n\tstruct idr\thandles;\n};\n\n\n \n\n \nstruct hl_userptr {\n\tenum vm_type\t\tvm_type;  \n\tstruct list_head\tjob_node;\n\tstruct page\t\t**pages;\n\tunsigned int\t\tnpages;\n\tstruct sg_table\t\t*sgt;\n\tenum dma_data_direction dir;\n\tstruct list_head\tdebugfs_list;\n\tpid_t\t\t\tpid;\n\tu64\t\t\taddr;\n\tu64\t\t\tsize;\n\tu8\t\t\tdma_mapped;\n};\n\n \nstruct hl_cs {\n\tu16\t\t\t*jobs_in_queue_cnt;\n\tstruct hl_ctx\t\t*ctx;\n\tstruct list_head\tjob_list;\n\tspinlock_t\t\tjob_lock;\n\tstruct kref\t\trefcount;\n\tstruct hl_fence\t\t*fence;\n\tstruct hl_fence\t\t*signal_fence;\n\tstruct work_struct\tfinish_work;\n\tstruct delayed_work\twork_tdr;\n\tstruct list_head\tmirror_node;\n\tstruct list_head\tstaged_cs_node;\n\tstruct list_head\tdebugfs_list;\n\tstruct hl_cs_encaps_sig_handle *encaps_sig_hdl;\n\tktime_t\t\t\tcompletion_timestamp;\n\tu64\t\t\tsequence;\n\tu64\t\t\tstaged_sequence;\n\tu64\t\t\ttimeout_jiffies;\n\tu64\t\t\tsubmission_time_jiffies;\n\tenum hl_cs_type\t\ttype;\n\tu32\t\t\tjobs_cnt;\n\tu32\t\t\tencaps_sig_hdl_id;\n\tu32\t\t\tsob_addr_offset;\n\tu16\t\t\tinitial_sob_count;\n\tu8\t\t\tsubmitted;\n\tu8\t\t\tcompleted;\n\tu8\t\t\ttimedout;\n\tu8\t\t\ttdr_active;\n\tu8\t\t\taborted;\n\tu8\t\t\ttimestamp;\n\tu8\t\t\tstaged_last;\n\tu8\t\t\tstaged_first;\n\tu8\t\t\tstaged_cs;\n\tu8\t\t\tskip_reset_on_timeout;\n\tu8\t\t\tencaps_signals;\n};\n\n \nstruct hl_cs_job {\n\tstruct list_head\tcs_node;\n\tstruct hl_cs\t\t*cs;\n\tstruct hl_cb\t\t*user_cb;\n\tstruct hl_cb\t\t*patched_cb;\n\tstruct work_struct\tfinish_work;\n\tstruct list_head\tuserptr_list;\n\tstruct list_head\tdebugfs_list;\n\tstruct kref\t\trefcount;\n\tenum hl_queue_type\tqueue_type;\n\tktime_t\t\t\ttimestamp;\n\tu32\t\t\tid;\n\tu32\t\t\thw_queue_id;\n\tu32\t\t\tuser_cb_size;\n\tu32\t\t\tjob_cb_size;\n\tu32\t\t\tencaps_sig_wait_offset;\n\tu8\t\t\tis_kernel_allocated_cb;\n\tu8\t\t\tcontains_dma_pkt;\n};\n\n \nstruct hl_cs_parser {\n\tstruct hl_cb\t\t*user_cb;\n\tstruct hl_cb\t\t*patched_cb;\n\tstruct list_head\t*job_userptr_list;\n\tu64\t\t\tcs_sequence;\n\tenum hl_queue_type\tqueue_type;\n\tu32\t\t\tctx_id;\n\tu32\t\t\thw_queue_id;\n\tu32\t\t\tuser_cb_size;\n\tu32\t\t\tpatched_cb_size;\n\tu8\t\t\tjob_id;\n\tu8\t\t\tis_kernel_allocated_cb;\n\tu8\t\t\tcontains_dma_pkt;\n\tu8\t\t\tcompletion;\n};\n\n \n\n \nstruct hl_vm_hash_node {\n\tstruct hlist_node\tnode;\n\tu64\t\t\tvaddr;\n\tu64\t\t\thandle;\n\tvoid\t\t\t*ptr;\n\tint\t\t\texport_cnt;\n};\n\n \nstruct hl_vm_hw_block_list_node {\n\tstruct list_head\tnode;\n\tstruct hl_ctx\t\t*ctx;\n\tunsigned long\t\tvaddr;\n\tu32\t\t\tblock_size;\n\tu32\t\t\tmapped_size;\n\tu32\t\t\tid;\n};\n\n \nstruct hl_vm_phys_pg_pack {\n\tenum vm_type\t\tvm_type;  \n\tu64\t\t\t*pages;\n\tu64\t\t\tnpages;\n\tu64\t\t\ttotal_size;\n\tu64\t\t\texported_size;\n\tstruct list_head\tnode;\n\tatomic_t\t\tmapping_cnt;\n\tu32\t\t\tasid;\n\tu32\t\t\tpage_size;\n\tu32\t\t\tflags;\n\tu32\t\t\thandle;\n\tu32\t\t\toffset;\n\tu8\t\t\tcontiguous;\n\tu8\t\t\tcreated_from_userptr;\n};\n\n \nstruct hl_vm_va_block {\n\tstruct list_head\tnode;\n\tu64\t\t\tstart;\n\tu64\t\t\tend;\n\tu64\t\t\tsize;\n};\n\n \nstruct hl_vm {\n\tstruct gen_pool\t\t*dram_pg_pool;\n\tstruct kref\t\tdram_pg_pool_refcount;\n\tspinlock_t\t\tidr_lock;\n\tstruct idr\t\tphys_pg_pack_handles;\n\tu8\t\t\tinit_done;\n};\n\n\n \n\n \nstruct hl_debug_params {\n\tvoid *input;\n\tvoid *output;\n\tu32 output_size;\n\tu32 reg_idx;\n\tu32 op;\n\tbool enable;\n};\n\n \nstruct hl_notifier_event {\n\tstruct eventfd_ctx\t*eventfd;\n\tstruct mutex\t\tlock;\n\tu64\t\t\tevents_mask;\n};\n\n \n\n \nstruct hl_fpriv {\n\tstruct hl_device\t\t*hdev;\n\tstruct file\t\t\t*filp;\n\tstruct pid\t\t\t*taskpid;\n\tstruct hl_ctx\t\t\t*ctx;\n\tstruct hl_ctx_mgr\t\tctx_mgr;\n\tstruct hl_mem_mgr\t\tmem_mgr;\n\tstruct hl_notifier_event\tnotifier_event;\n\tstruct list_head\t\tdebugfs_list;\n\tstruct list_head\t\tdev_node;\n\tstruct kref\t\t\trefcount;\n\tstruct mutex\t\t\trestore_phase_mutex;\n\tstruct mutex\t\t\tctx_lock;\n};\n\n\n \n\n \nstruct hl_info_list {\n\tconst char\t*name;\n\tint\t\t(*show)(struct seq_file *s, void *data);\n\tssize_t\t\t(*write)(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *f_pos);\n};\n\n \nstruct hl_debugfs_entry {\n\tconst struct hl_info_list\t*info_ent;\n\tstruct hl_dbg_device_entry\t*dev_entry;\n};\n\n \nstruct hl_dbg_device_entry {\n\tstruct dentry\t\t\t*root;\n\tstruct hl_device\t\t*hdev;\n\tstruct hl_debugfs_entry\t\t*entry_arr;\n\tstruct list_head\t\tfile_list;\n\tstruct mutex\t\t\tfile_mutex;\n\tstruct list_head\t\tcb_list;\n\tspinlock_t\t\t\tcb_spinlock;\n\tstruct list_head\t\tcs_list;\n\tspinlock_t\t\t\tcs_spinlock;\n\tstruct list_head\t\tcs_job_list;\n\tspinlock_t\t\t\tcs_job_spinlock;\n\tstruct list_head\t\tuserptr_list;\n\tspinlock_t\t\t\tuserptr_spinlock;\n\tstruct list_head\t\tctx_mem_hash_list;\n\tstruct mutex\t\t\tctx_mem_hash_mutex;\n\tstruct debugfs_blob_wrapper\tdata_dma_blob_desc;\n\tstruct debugfs_blob_wrapper\tmon_dump_blob_desc;\n\tchar\t\t\t\t*state_dump[HL_STATE_DUMP_HIST_LEN];\n\tstruct rw_semaphore\t\tstate_dump_sem;\n\tu64\t\t\t\taddr;\n\tu64\t\t\t\tmmu_addr;\n\tu64\t\t\t\tmmu_cap_mask;\n\tu64\t\t\t\tuserptr_lookup;\n\tu32\t\t\t\tmmu_asid;\n\tu32\t\t\t\tstate_dump_head;\n\tu8\t\t\t\ti2c_bus;\n\tu8\t\t\t\ti2c_addr;\n\tu8\t\t\t\ti2c_reg;\n\tu8\t\t\t\ti2c_len;\n};\n\n \nstruct hl_hw_obj_name_entry {\n\tstruct hlist_node\tnode;\n\tconst char\t\t*name;\n\tu32\t\t\tid;\n};\n\nenum hl_state_dump_specs_props {\n\tSP_SYNC_OBJ_BASE_ADDR,\n\tSP_NEXT_SYNC_OBJ_ADDR,\n\tSP_SYNC_OBJ_AMOUNT,\n\tSP_MON_OBJ_WR_ADDR_LOW,\n\tSP_MON_OBJ_WR_ADDR_HIGH,\n\tSP_MON_OBJ_WR_DATA,\n\tSP_MON_OBJ_ARM_DATA,\n\tSP_MON_OBJ_STATUS,\n\tSP_MONITORS_AMOUNT,\n\tSP_TPC0_CMDQ,\n\tSP_TPC0_CFG_SO,\n\tSP_NEXT_TPC,\n\tSP_MME_CMDQ,\n\tSP_MME_CFG_SO,\n\tSP_NEXT_MME,\n\tSP_DMA_CMDQ,\n\tSP_DMA_CFG_SO,\n\tSP_DMA_QUEUES_OFFSET,\n\tSP_NUM_OF_MME_ENGINES,\n\tSP_SUB_MME_ENG_NUM,\n\tSP_NUM_OF_DMA_ENGINES,\n\tSP_NUM_OF_TPC_ENGINES,\n\tSP_ENGINE_NUM_OF_QUEUES,\n\tSP_ENGINE_NUM_OF_STREAMS,\n\tSP_ENGINE_NUM_OF_FENCES,\n\tSP_FENCE0_CNT_OFFSET,\n\tSP_FENCE0_RDATA_OFFSET,\n\tSP_CP_STS_OFFSET,\n\tSP_NUM_CORES,\n\n\tSP_MAX\n};\n\nenum hl_sync_engine_type {\n\tENGINE_TPC,\n\tENGINE_DMA,\n\tENGINE_MME,\n};\n\n \nstruct hl_mon_state_dump {\n\tu32\t\tid;\n\tu32\t\twr_addr_low;\n\tu32\t\twr_addr_high;\n\tu32\t\twr_data;\n\tu32\t\tarm_data;\n\tu32\t\tstatus;\n};\n\n \nstruct hl_sync_to_engine_map_entry {\n\tstruct hlist_node\t\tnode;\n\tenum hl_sync_engine_type\tengine_type;\n\tu32\t\t\t\tengine_id;\n\tu32\t\t\t\tsync_id;\n};\n\n \nstruct hl_sync_to_engine_map {\n\tDECLARE_HASHTABLE(tb, SYNC_TO_ENGINE_HASH_TABLE_BITS);\n};\n\n \nstruct hl_state_dump_specs_funcs {\n\tint (*gen_sync_to_engine_map)(struct hl_device *hdev,\n\t\t\t\tstruct hl_sync_to_engine_map *map);\n\tint (*print_single_monitor)(char **buf, size_t *size, size_t *offset,\n\t\t\t\t    struct hl_device *hdev,\n\t\t\t\t    struct hl_mon_state_dump *mon);\n\tint (*monitor_valid)(struct hl_mon_state_dump *mon);\n\tint (*print_fences_single_engine)(struct hl_device *hdev,\n\t\t\t\t\tu64 base_offset,\n\t\t\t\t\tu64 status_base_offset,\n\t\t\t\t\tenum hl_sync_engine_type engine_type,\n\t\t\t\t\tu32 engine_id, char **buf,\n\t\t\t\t\tsize_t *size, size_t *offset);\n};\n\n \nstruct hl_state_dump_specs {\n\tDECLARE_HASHTABLE(so_id_to_str_tb, OBJ_NAMES_HASH_TABLE_BITS);\n\tDECLARE_HASHTABLE(monitor_id_to_str_tb, OBJ_NAMES_HASH_TABLE_BITS);\n\tstruct hl_state_dump_specs_funcs\tfuncs;\n\tconst char * const\t\t\t*sync_namager_names;\n\ts64\t\t\t\t\t*props;\n};\n\n\n \n\n#define HL_STR_MAX\t32\n\n#define HL_DEV_STS_MAX (HL_DEVICE_STATUS_LAST + 1)\n\n \n#define HL_MAX_MINORS\t256\n\n \n\nu32 hl_rreg(struct hl_device *hdev, u32 reg);\nvoid hl_wreg(struct hl_device *hdev, u32 reg, u32 val);\n\n#define RREG32(reg) hdev->asic_funcs->rreg(hdev, (reg))\n#define WREG32(reg, v) hdev->asic_funcs->wreg(hdev, (reg), (v))\n#define DREG32(reg) pr_info(\"REGISTER: \" #reg \" : 0x%08X\\n\",\t\\\n\t\t\thdev->asic_funcs->rreg(hdev, (reg)))\n\n#define WREG32_P(reg, val, mask)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\\\n\t\tu32 tmp_ = RREG32(reg);\t\t\t\t\\\n\t\ttmp_ &= (mask);\t\t\t\t\t\\\n\t\ttmp_ |= ((val) & ~(mask));\t\t\t\\\n\t\tWREG32(reg, tmp_);\t\t\t\t\\\n\t} while (0)\n#define WREG32_AND(reg, and) WREG32_P(reg, 0, and)\n#define WREG32_OR(reg, or) WREG32_P(reg, or, ~(or))\n\n#define RMWREG32_SHIFTED(reg, val, mask) WREG32_P(reg, val, ~(mask))\n\n#define RMWREG32(reg, val, mask) RMWREG32_SHIFTED(reg, (val) << __ffs(mask), mask)\n\n#define RREG32_MASK(reg, mask) ((RREG32(reg) & mask) >> __ffs(mask))\n\n#define REG_FIELD_SHIFT(reg, field) reg##_##field##_SHIFT\n#define REG_FIELD_MASK(reg, field) reg##_##field##_MASK\n#define WREG32_FIELD(reg, offset, field, val)\t\\\n\tWREG32(mm##reg + offset, (RREG32(mm##reg + offset) & \\\n\t\t\t\t~REG_FIELD_MASK(reg, field)) | \\\n\t\t\t\t(val) << REG_FIELD_SHIFT(reg, field))\n\n \n#define hl_poll_timeout_common(hdev, addr, val, cond, sleep_us, timeout_us, elbi) \\\n({ \\\n\tktime_t __timeout; \\\n\tu32 __elbi_read; \\\n\tint __rc = 0; \\\n\t__timeout = ktime_add_us(ktime_get(), timeout_us); \\\n\tmight_sleep_if(sleep_us); \\\n\tfor (;;) { \\\n\t\tif (elbi) { \\\n\t\t\t__rc = hl_pci_elbi_read(hdev, addr, &__elbi_read); \\\n\t\t\tif (__rc) \\\n\t\t\t\tbreak; \\\n\t\t\t(val) = __elbi_read; \\\n\t\t} else {\\\n\t\t\t(val) = RREG32(lower_32_bits(addr)); \\\n\t\t} \\\n\t\tif (cond) \\\n\t\t\tbreak; \\\n\t\tif (timeout_us && ktime_compare(ktime_get(), __timeout) > 0) { \\\n\t\t\tif (elbi) { \\\n\t\t\t\t__rc = hl_pci_elbi_read(hdev, addr, &__elbi_read); \\\n\t\t\t\tif (__rc) \\\n\t\t\t\t\tbreak; \\\n\t\t\t\t(val) = __elbi_read; \\\n\t\t\t} else {\\\n\t\t\t\t(val) = RREG32(lower_32_bits(addr)); \\\n\t\t\t} \\\n\t\t\tbreak; \\\n\t\t} \\\n\t\tif (sleep_us) \\\n\t\t\tusleep_range((sleep_us >> 2) + 1, sleep_us); \\\n\t} \\\n\t__rc ? __rc : ((cond) ? 0 : -ETIMEDOUT); \\\n})\n\n#define hl_poll_timeout(hdev, addr, val, cond, sleep_us, timeout_us) \\\n\t\thl_poll_timeout_common(hdev, addr, val, cond, sleep_us, timeout_us, false)\n\n#define hl_poll_timeout_elbi(hdev, addr, val, cond, sleep_us, timeout_us) \\\n\t\thl_poll_timeout_common(hdev, addr, val, cond, sleep_us, timeout_us, true)\n\n \n#define hl_poll_reg_array_timeout_common(hdev, addr_arr, arr_size, expected_val, sleep_us, \\\n\t\t\t\t\t\ttimeout_us, elbi) \\\n({ \\\n\tktime_t __timeout; \\\n\tu64 __elem_bitmask; \\\n\tu32 __read_val;\t\\\n\tu8 __arr_idx;\t\\\n\tint __rc = 0; \\\n\t\\\n\t__timeout = ktime_add_us(ktime_get(), timeout_us); \\\n\tmight_sleep_if(sleep_us); \\\n\tif (arr_size >= 64) \\\n\t\t__rc = -EINVAL; \\\n\telse \\\n\t\t__elem_bitmask = BIT_ULL(arr_size) - 1; \\\n\tfor (;;) { \\\n\t\tif (__rc) \\\n\t\t\tbreak; \\\n\t\tfor (__arr_idx = 0; __arr_idx < (arr_size); __arr_idx++) {\t\\\n\t\t\tif (!(__elem_bitmask & BIT_ULL(__arr_idx)))\t\\\n\t\t\t\tcontinue;\t\\\n\t\t\tif (elbi) { \\\n\t\t\t\t__rc = hl_pci_elbi_read(hdev, (addr_arr)[__arr_idx], &__read_val); \\\n\t\t\t\tif (__rc) \\\n\t\t\t\t\tbreak; \\\n\t\t\t} else { \\\n\t\t\t\t__read_val = RREG32(lower_32_bits(addr_arr[__arr_idx])); \\\n\t\t\t} \\\n\t\t\tif (__read_val == (expected_val))\t\\\n\t\t\t\t__elem_bitmask &= ~BIT_ULL(__arr_idx);\t\\\n\t\t}\t\\\n\t\tif (__rc || (__elem_bitmask == 0)) \\\n\t\t\tbreak; \\\n\t\tif (timeout_us && ktime_compare(ktime_get(), __timeout) > 0) \\\n\t\t\tbreak; \\\n\t\tif (sleep_us) \\\n\t\t\tusleep_range((sleep_us >> 2) + 1, sleep_us); \\\n\t} \\\n\t__rc ? __rc : ((__elem_bitmask == 0) ? 0 : -ETIMEDOUT); \\\n})\n\n#define hl_poll_reg_array_timeout(hdev, addr_arr, arr_size, expected_val, sleep_us, \\\n\t\t\t\t\ttimeout_us) \\\n\thl_poll_reg_array_timeout_common(hdev, addr_arr, arr_size, expected_val, sleep_us, \\\n\t\t\t\t\t\ttimeout_us, false)\n\n#define hl_poll_reg_array_timeout_elbi(hdev, addr_arr, arr_size, expected_val, sleep_us, \\\n\t\t\t\t\ttimeout_us) \\\n\thl_poll_reg_array_timeout_common(hdev, addr_arr, arr_size, expected_val, sleep_us, \\\n\t\t\t\t\t\ttimeout_us, true)\n\n \n#define hl_poll_timeout_memory(hdev, addr, val, cond, sleep_us, timeout_us, \\\n\t\t\t\tmem_written_by_device) \\\n({ \\\n\tktime_t __timeout; \\\n\t\\\n\t__timeout = ktime_add_us(ktime_get(), timeout_us); \\\n\tmight_sleep_if(sleep_us); \\\n\tfor (;;) { \\\n\t\t  \\\n\t\tmb(); \\\n\t\t(val) = *((u32 *)(addr)); \\\n\t\tif (mem_written_by_device) \\\n\t\t\t(val) = le32_to_cpu(*(__le32 *) &(val)); \\\n\t\tif (cond) \\\n\t\t\tbreak; \\\n\t\tif (timeout_us && ktime_compare(ktime_get(), __timeout) > 0) { \\\n\t\t\t(val) = *((u32 *)(addr)); \\\n\t\t\tif (mem_written_by_device) \\\n\t\t\t\t(val) = le32_to_cpu(*(__le32 *) &(val)); \\\n\t\t\tbreak; \\\n\t\t} \\\n\t\tif (sleep_us) \\\n\t\t\tusleep_range((sleep_us >> 2) + 1, sleep_us); \\\n\t} \\\n\t(cond) ? 0 : -ETIMEDOUT; \\\n})\n\n#define HL_USR_MAPPED_BLK_INIT(blk, base, sz) \\\n({ \\\n\tstruct user_mapped_block *p = blk; \\\n\\\n\tp->address = base; \\\n\tp->size = sz; \\\n})\n\n#define HL_USR_INTR_STRUCT_INIT(usr_intr, hdev, intr_id, intr_type) \\\n({ \\\n\tusr_intr.hdev = hdev; \\\n\tusr_intr.interrupt_id = intr_id; \\\n\tusr_intr.type = intr_type; \\\n\tINIT_LIST_HEAD(&usr_intr.wait_list_head); \\\n\tspin_lock_init(&usr_intr.wait_list_lock); \\\n})\n\nstruct hwmon_chip_info;\n\n \nstruct hl_device_reset_work {\n\tstruct delayed_work\treset_work;\n\tstruct hl_device\t*hdev;\n\tu32\t\t\tflags;\n};\n\n \nstruct hl_mmu_hr_priv {\n\tstruct gen_pool\t*mmu_pgt_pool;\n\tstruct pgt_info\t*mmu_asid_hop0;\n};\n\n \nstruct hl_mmu_dr_priv {\n\tstruct gen_pool *mmu_pgt_pool;\n\tvoid *mmu_shadow_hop0;\n};\n\n \nstruct hl_mmu_priv {\n\tstruct hl_mmu_dr_priv dr;\n\tstruct hl_mmu_hr_priv hr;\n};\n\n \nstruct hl_mmu_per_hop_info {\n\tu64 hop_addr;\n\tu64 hop_pte_addr;\n\tu64 hop_pte_val;\n};\n\n \nstruct hl_mmu_hop_info {\n\tu64 scrambled_vaddr;\n\tu64 unscrambled_paddr;\n\tstruct hl_mmu_per_hop_info hop_info[MMU_ARCH_6_HOPS];\n\tu32 used_hops;\n\tenum hl_va_range_type range_type;\n};\n\n \nstruct hl_hr_mmu_funcs {\n\tstruct pgt_info *(*get_hop0_pgt_info)(struct hl_ctx *ctx);\n\tstruct pgt_info *(*get_pgt_info)(struct hl_ctx *ctx, u64 phys_hop_addr);\n\tvoid (*add_pgt_info)(struct hl_ctx *ctx, struct pgt_info *pgt_info, dma_addr_t phys_addr);\n\tint (*get_tlb_mapping_params)(struct hl_device *hdev, struct hl_mmu_properties **mmu_prop,\n\t\t\t\t\t\t\t\tstruct hl_mmu_hop_info *hops,\n\t\t\t\t\t\t\t\tu64 virt_addr, bool *is_huge);\n};\n\n \nstruct hl_mmu_funcs {\n\tint (*init)(struct hl_device *hdev);\n\tvoid (*fini)(struct hl_device *hdev);\n\tint (*ctx_init)(struct hl_ctx *ctx);\n\tvoid (*ctx_fini)(struct hl_ctx *ctx);\n\tint (*map)(struct hl_ctx *ctx, u64 virt_addr, u64 phys_addr, u32 page_size,\n\t\t\t\tbool is_dram_addr);\n\tint (*unmap)(struct hl_ctx *ctx, u64 virt_addr, bool is_dram_addr);\n\tvoid (*flush)(struct hl_ctx *ctx);\n\tvoid (*swap_out)(struct hl_ctx *ctx);\n\tvoid (*swap_in)(struct hl_ctx *ctx);\n\tint (*get_tlb_info)(struct hl_ctx *ctx, u64 virt_addr, struct hl_mmu_hop_info *hops);\n\tstruct hl_hr_mmu_funcs hr_funcs;\n};\n\n \nstruct hl_prefetch_work {\n\tstruct work_struct\tprefetch_work;\n\tstruct hl_ctx\t\t*ctx;\n\tu64\t\t\tva;\n\tu64\t\t\tsize;\n\tu32\t\t\tflags;\n\tu32\t\t\tasid;\n};\n\n \n#define MULTI_CS_MAX_USER_CTX\t2\n\n \nstruct multi_cs_completion {\n\tstruct completion\tcompletion;\n\tspinlock_t\t\tlock;\n\ts64\t\t\ttimestamp;\n\tu32\t\t\tstream_master_qid_map;\n\tu8\t\t\tused;\n};\n\n \nstruct multi_cs_data {\n\tstruct hl_ctx\t*ctx;\n\tstruct hl_fence\t**fence_arr;\n\tu64\t\t*seq_arr;\n\ts64\t\ttimeout_jiffies;\n\ts64\t\ttimestamp;\n\tlong\t\twait_status;\n\tu32\t\tcompletion_bitmap;\n\tu8\t\tarr_len;\n\tu8\t\tgone_cs;\n\tu8\t\tupdate_ts;\n};\n\n \nstruct hl_clk_throttle_timestamp {\n\tktime_t\t\tstart;\n\tktime_t\t\tend;\n};\n\n \nstruct hl_clk_throttle {\n\tstruct hl_clk_throttle_timestamp timestamp[HL_CLK_THROTTLE_TYPE_MAX];\n\tstruct mutex\tlock;\n\tu32\t\tcurrent_reason;\n\tu32\t\taggregated_reason;\n};\n\n \nstruct user_mapped_block {\n\tu32 address;\n\tu32 size;\n};\n\n \nstruct cs_timeout_info {\n\tktime_t\t\ttimestamp;\n\tatomic_t\twrite_enable;\n\tu64\t\tseq;\n};\n\n#define MAX_QMAN_STREAMS_INFO\t\t4\n#define OPCODE_INFO_MAX_ADDR_SIZE\t8\n \nstruct undefined_opcode_info {\n\tktime_t timestamp;\n\tu64 cb_addr_streams[MAX_QMAN_STREAMS_INFO][OPCODE_INFO_MAX_ADDR_SIZE];\n\tu64 cq_addr;\n\tu32 cq_size;\n\tu32 cb_addr_streams_len;\n\tu32 engine_id;\n\tu32 stream_id;\n\tbool write_enable;\n};\n\n \nstruct page_fault_info {\n\tstruct hl_page_fault_info\tpage_fault;\n\tstruct hl_user_mapping\t\t*user_mappings;\n\tu64\t\t\t\tnum_of_user_mappings;\n\tatomic_t\t\t\tpage_fault_detected;\n\tbool\t\t\t\tpage_fault_info_available;\n};\n\n \nstruct razwi_info {\n\tstruct hl_info_razwi_event\trazwi;\n\tatomic_t\t\t\trazwi_detected;\n\tbool\t\t\t\trazwi_info_available;\n};\n\n \nstruct hw_err_info {\n\tstruct hl_info_hw_err_event\tevent;\n\tatomic_t\t\t\tevent_detected;\n\tbool\t\t\t\tevent_info_available;\n};\n\n \nstruct fw_err_info {\n\tstruct hl_info_fw_err_event\tevent;\n\tatomic_t\t\t\tevent_detected;\n\tbool\t\t\t\tevent_info_available;\n};\n\n \nstruct hl_error_info {\n\tstruct cs_timeout_info\t\tcs_timeout;\n\tstruct razwi_info\t\trazwi_info;\n\tstruct undefined_opcode_info\tundef_opcode;\n\tstruct page_fault_info\t\tpage_fault_info;\n\tstruct hw_err_info\t\thw_err;\n\tstruct fw_err_info\t\tfw_err;\n};\n\n \nstruct hl_reset_info {\n\tspinlock_t\tlock;\n\tu32\t\tcompute_reset_cnt;\n\tu32\t\thard_reset_cnt;\n\tu32\t\thard_reset_schedule_flags;\n\tu8\t\tin_reset;\n\tu8\t\tin_compute_reset;\n\tu8\t\tneeds_reset;\n\tu8\t\thard_reset_pending;\n\tu8\t\tcurr_reset_cause;\n\tu8\t\tprev_reset_trigger;\n\tu8\t\treset_trigger_repeated;\n\tu8\t\tskip_reset_on_timeout;\n\tu8\t\twatchdog_active;\n};\n\n \nstruct hl_device {\n\tstruct pci_dev\t\t\t*pdev;\n\tu64\t\t\t\tpcie_bar_phys[HL_PCI_NUM_BARS];\n\tvoid __iomem\t\t\t*pcie_bar[HL_PCI_NUM_BARS];\n\tvoid __iomem\t\t\t*rmmio;\n\tstruct class\t\t\t*hclass;\n\tstruct cdev\t\t\tcdev;\n\tstruct cdev\t\t\tcdev_ctrl;\n\tstruct device\t\t\t*dev;\n\tstruct device\t\t\t*dev_ctrl;\n\tstruct delayed_work\t\twork_heartbeat;\n\tstruct hl_device_reset_work\tdevice_reset_work;\n\tstruct hl_device_reset_work\tdevice_release_watchdog_work;\n\tchar\t\t\t\tasic_name[HL_STR_MAX];\n\tchar\t\t\t\tstatus[HL_DEV_STS_MAX][HL_STR_MAX];\n\tenum hl_asic_type\t\tasic_type;\n\tstruct hl_cq\t\t\t*completion_queue;\n\tstruct hl_user_interrupt\t*user_interrupt;\n\tstruct hl_user_interrupt\ttpc_interrupt;\n\tstruct hl_user_interrupt\tunexpected_error_interrupt;\n\tstruct hl_user_interrupt\tcommon_user_cq_interrupt;\n\tstruct hl_user_interrupt\tcommon_decoder_interrupt;\n\tstruct hl_cs\t\t\t**shadow_cs_queue;\n\tstruct workqueue_struct\t\t**cq_wq;\n\tstruct workqueue_struct\t\t*eq_wq;\n\tstruct workqueue_struct\t\t*cs_cmplt_wq;\n\tstruct workqueue_struct\t\t*ts_free_obj_wq;\n\tstruct workqueue_struct\t\t*prefetch_wq;\n\tstruct workqueue_struct\t\t*reset_wq;\n\tstruct hl_ctx\t\t\t*kernel_ctx;\n\tstruct hl_hw_queue\t\t*kernel_queues;\n\tstruct list_head\t\tcs_mirror_list;\n\tspinlock_t\t\t\tcs_mirror_lock;\n\tstruct hl_mem_mgr\t\tkernel_mem_mgr;\n\tstruct hl_eq\t\t\tevent_queue;\n\tstruct dma_pool\t\t\t*dma_pool;\n\tvoid\t\t\t\t*cpu_accessible_dma_mem;\n\tdma_addr_t\t\t\tcpu_accessible_dma_address;\n\tstruct gen_pool\t\t\t*cpu_accessible_dma_pool;\n\tunsigned long\t\t\t*asid_bitmap;\n\tstruct mutex\t\t\tasid_mutex;\n\tstruct mutex\t\t\tsend_cpu_message_lock;\n\tstruct mutex\t\t\tdebug_lock;\n\tstruct mutex\t\t\tmmu_lock;\n\tstruct asic_fixed_properties\tasic_prop;\n\tconst struct hl_asic_funcs\t*asic_funcs;\n\tvoid\t\t\t\t*asic_specific;\n\tstruct hl_vm\t\t\tvm;\n\tstruct device\t\t\t*hwmon_dev;\n\tstruct hwmon_chip_info\t\t*hl_chip_info;\n\n\tstruct hl_dbg_device_entry\thl_debugfs;\n\n\tstruct list_head\t\tcb_pool;\n\tspinlock_t\t\t\tcb_pool_lock;\n\n\tvoid\t\t\t\t*internal_cb_pool_virt_addr;\n\tdma_addr_t\t\t\tinternal_cb_pool_dma_addr;\n\tstruct gen_pool\t\t\t*internal_cb_pool;\n\tu64\t\t\t\tinternal_cb_va_base;\n\n\tstruct list_head\t\tfpriv_list;\n\tstruct list_head\t\tfpriv_ctrl_list;\n\tstruct mutex\t\t\tfpriv_list_lock;\n\tstruct mutex\t\t\tfpriv_ctrl_list_lock;\n\n\tstruct hl_cs_counters_atomic\taggregated_cs_counters;\n\n\tstruct hl_mmu_priv\t\tmmu_priv;\n\tstruct hl_mmu_funcs\t\tmmu_func[MMU_NUM_PGT_LOCATIONS];\n\n\tstruct hl_dec\t\t\t*dec;\n\n\tstruct fw_load_mgr\t\tfw_loader;\n\n\tstruct pci_mem_region\t\tpci_mem_region[PCI_REGION_NUMBER];\n\n\tstruct hl_state_dump_specs\tstate_dump_specs;\n\n\tstruct multi_cs_completion\tmulti_cs_completion[\n\t\t\t\t\t\t\tMULTI_CS_MAX_USER_CTX];\n\tstruct hl_clk_throttle\t\tclk_throttling;\n\tstruct hl_error_info\t\tcaptured_err_info;\n\n\tstruct hl_reset_info\t\treset_info;\n\n\tu32\t\t\t\t*stream_master_qid_arr;\n\tu32\t\t\t\tfw_inner_major_ver;\n\tu32\t\t\t\tfw_inner_minor_ver;\n\tu32\t\t\t\tfw_sw_major_ver;\n\tu32\t\t\t\tfw_sw_minor_ver;\n\tu32\t\t\t\tfw_sw_sub_minor_ver;\n\tatomic64_t\t\t\tdram_used_mem;\n\tu64\t\t\t\tmemory_scrub_val;\n\tu64\t\t\t\ttimeout_jiffies;\n\tu64\t\t\t\tmax_power;\n\tu64\t\t\t\tboot_error_status_mask;\n\tu64\t\t\t\tdram_pci_bar_start;\n\tu64\t\t\t\tlast_successful_open_jif;\n\tu64\t\t\t\tlast_open_session_duration_jif;\n\tu64\t\t\t\topen_counter;\n\tu64\t\t\t\tfw_poll_interval_usec;\n\tktime_t\t\t\t\tlast_successful_open_ktime;\n\tu64\t\t\t\tfw_comms_poll_interval_usec;\n\tu64\t\t\t\tdram_binning;\n\tu64\t\t\t\ttpc_binning;\n\tatomic_t\t\t\tdmabuf_export_cnt;\n\tenum cpucp_card_types\t\tcard_type;\n\tu32\t\t\t\tmajor;\n\tu32\t\t\t\thigh_pll;\n\tu32\t\t\t\tdecoder_binning;\n\tu32\t\t\t\tedma_binning;\n\tu32\t\t\t\tdevice_release_watchdog_timeout_sec;\n\tu32\t\t\t\trotator_binning;\n\tu16\t\t\t\tid;\n\tu16\t\t\t\tid_control;\n\tu16\t\t\t\tcdev_idx;\n\tu16\t\t\t\tcpu_pci_msb_addr;\n\tu8\t\t\t\tis_in_dram_scrub;\n\tu8\t\t\t\tdisabled;\n\tu8\t\t\t\tlate_init_done;\n\tu8\t\t\t\thwmon_initialized;\n\tu8\t\t\t\treset_on_lockup;\n\tu8\t\t\t\tdram_default_page_mapping;\n\tu8\t\t\t\tmemory_scrub;\n\tu8\t\t\t\tpmmu_huge_range;\n\tu8\t\t\t\tinit_done;\n\tu8\t\t\t\tdevice_cpu_disabled;\n\tu8\t\t\t\tin_debug;\n\tu8\t\t\t\tcdev_sysfs_debugfs_created;\n\tu8\t\t\t\tstop_on_err;\n\tu8\t\t\t\tsupports_sync_stream;\n\tu8\t\t\t\tsync_stream_queue_idx;\n\tu8\t\t\t\tcollective_mon_idx;\n\tu8\t\t\t\tsupports_coresight;\n\tu8\t\t\t\tsupports_cb_mapping;\n\tu8\t\t\t\tprocess_kill_trial_cnt;\n\tu8\t\t\t\tdevice_fini_pending;\n\tu8\t\t\t\tsupports_staged_submission;\n\tu8\t\t\t\tdevice_cpu_is_halted;\n\tu8\t\t\t\tsupports_wait_for_multi_cs;\n\tu8\t\t\t\tstream_master_qid_arr_size;\n\tu8\t\t\t\tis_compute_ctx_active;\n\tu8\t\t\t\tcompute_ctx_in_release;\n\tu8\t\t\t\tsupports_mmu_prefetch;\n\tu8\t\t\t\treset_upon_device_release;\n\tu8\t\t\t\tsupports_ctx_switch;\n\tu8\t\t\t\tsupport_preboot_binning;\n\n\t \n\tu64\t\t\t\tnic_ports_mask;\n\tu64\t\t\t\tfw_components;\n\tu8\t\t\t\tmmu_disable;\n\tu8\t\t\t\tcpu_queues_enable;\n\tu8\t\t\t\tpldm;\n\tu8\t\t\t\thard_reset_on_fw_events;\n\tu8\t\t\t\tbmc_enable;\n\tu8\t\t\t\treset_on_preboot_fail;\n\tu8\t\t\t\theartbeat;\n};\n\n\n \nstruct hl_cs_encaps_sig_handle {\n\tstruct kref refcount;\n\tstruct hl_device *hdev;\n\tstruct hl_hw_sob *hw_sob;\n\tstruct hl_ctx *ctx;\n\tu64  cs_seq;\n\tu32  id;\n\tu32  q_idx;\n\tu32  pre_sob_val;\n\tu32  count;\n};\n\n \nstruct hl_info_fw_err_info {\n\tenum hl_info_fw_err_type err_type;\n\tu64 *event_mask;\n\tu16 event_id;\n};\n\n \n\n \ntypedef int hl_ioctl_t(struct hl_fpriv *hpriv, void *data);\n\n \nstruct hl_ioctl_desc {\n\tunsigned int cmd;\n\thl_ioctl_t *func;\n};\n\nstatic inline bool hl_is_fw_sw_ver_below(struct hl_device *hdev, u32 fw_sw_major, u32 fw_sw_minor)\n{\n\tif (hdev->fw_sw_major_ver < fw_sw_major)\n\t\treturn true;\n\tif (hdev->fw_sw_major_ver > fw_sw_major)\n\t\treturn false;\n\tif (hdev->fw_sw_minor_ver < fw_sw_minor)\n\t\treturn true;\n\treturn false;\n}\n\n \n\n \nstatic inline u32 hl_get_sg_info(struct scatterlist *sg, dma_addr_t *dma_addr)\n{\n\t*dma_addr = sg_dma_address(sg);\n\n\treturn ((((*dma_addr) & (PAGE_SIZE - 1)) + sg_dma_len(sg)) +\n\t\t\t(PAGE_SIZE - 1)) >> PAGE_SHIFT;\n}\n\n \nstatic inline bool hl_mem_area_inside_range(u64 address, u64 size,\n\t\t\t\tu64 range_start_address, u64 range_end_address)\n{\n\tu64 end_address = address + size;\n\n\tif ((address >= range_start_address) &&\n\t\t\t(end_address <= range_end_address) &&\n\t\t\t(end_address > address))\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic inline bool hl_mem_area_crosses_range(u64 address, u32 size,\n\t\t\t\tu64 range_start_address, u64 range_end_address)\n{\n\tu64 end_address = address + size - 1;\n\n\treturn ((address <= range_end_address) && (range_start_address <= end_address));\n}\n\nuint64_t hl_set_dram_bar_default(struct hl_device *hdev, u64 addr);\nvoid *hl_cpu_accessible_dma_pool_alloc(struct hl_device *hdev, size_t size, dma_addr_t *dma_handle);\nvoid hl_cpu_accessible_dma_pool_free(struct hl_device *hdev, size_t size, void *vaddr);\nvoid *hl_asic_dma_alloc_coherent_caller(struct hl_device *hdev, size_t size, dma_addr_t *dma_handle,\n\t\t\t\t\tgfp_t flag, const char *caller);\nvoid hl_asic_dma_free_coherent_caller(struct hl_device *hdev, size_t size, void *cpu_addr,\n\t\t\t\t\tdma_addr_t dma_handle, const char *caller);\nvoid *hl_asic_dma_pool_zalloc_caller(struct hl_device *hdev, size_t size, gfp_t mem_flags,\n\t\t\t\t\tdma_addr_t *dma_handle, const char *caller);\nvoid hl_asic_dma_pool_free_caller(struct hl_device *hdev, void *vaddr, dma_addr_t dma_addr,\n\t\t\t\t\tconst char *caller);\nint hl_dma_map_sgtable(struct hl_device *hdev, struct sg_table *sgt, enum dma_data_direction dir);\nvoid hl_dma_unmap_sgtable(struct hl_device *hdev, struct sg_table *sgt,\n\t\t\t\tenum dma_data_direction dir);\nint hl_access_sram_dram_region(struct hl_device *hdev, u64 addr, u64 *val,\n\tenum debugfs_access_type acc_type, enum pci_region region_type, bool set_dram_bar);\nint hl_access_cfg_region(struct hl_device *hdev, u64 addr, u64 *val,\n\tenum debugfs_access_type acc_type);\nint hl_access_dev_mem(struct hl_device *hdev, enum pci_region region_type,\n\t\t\tu64 addr, u64 *val, enum debugfs_access_type acc_type);\nint hl_device_open(struct inode *inode, struct file *filp);\nint hl_device_open_ctrl(struct inode *inode, struct file *filp);\nbool hl_device_operational(struct hl_device *hdev,\n\t\tenum hl_device_status *status);\nbool hl_ctrl_device_operational(struct hl_device *hdev,\n\t\tenum hl_device_status *status);\nenum hl_device_status hl_device_status(struct hl_device *hdev);\nint hl_device_set_debug_mode(struct hl_device *hdev, struct hl_ctx *ctx, bool enable);\nint hl_hw_queues_create(struct hl_device *hdev);\nvoid hl_hw_queues_destroy(struct hl_device *hdev);\nint hl_hw_queue_send_cb_no_cmpl(struct hl_device *hdev, u32 hw_queue_id,\n\t\tu32 cb_size, u64 cb_ptr);\nvoid hl_hw_queue_submit_bd(struct hl_device *hdev, struct hl_hw_queue *q,\n\t\tu32 ctl, u32 len, u64 ptr);\nint hl_hw_queue_schedule_cs(struct hl_cs *cs);\nu32 hl_hw_queue_add_ptr(u32 ptr, u16 val);\nvoid hl_hw_queue_inc_ci_kernel(struct hl_device *hdev, u32 hw_queue_id);\nvoid hl_hw_queue_update_ci(struct hl_cs *cs);\nvoid hl_hw_queue_reset(struct hl_device *hdev, bool hard_reset);\n\n#define hl_queue_inc_ptr(p)\t\thl_hw_queue_add_ptr(p, 1)\n#define hl_pi_2_offset(pi)\t\t((pi) & (HL_QUEUE_LENGTH - 1))\n\nint hl_cq_init(struct hl_device *hdev, struct hl_cq *q, u32 hw_queue_id);\nvoid hl_cq_fini(struct hl_device *hdev, struct hl_cq *q);\nint hl_eq_init(struct hl_device *hdev, struct hl_eq *q);\nvoid hl_eq_fini(struct hl_device *hdev, struct hl_eq *q);\nvoid hl_cq_reset(struct hl_device *hdev, struct hl_cq *q);\nvoid hl_eq_reset(struct hl_device *hdev, struct hl_eq *q);\nirqreturn_t hl_irq_handler_cq(int irq, void *arg);\nirqreturn_t hl_irq_handler_eq(int irq, void *arg);\nirqreturn_t hl_irq_handler_dec_abnrm(int irq, void *arg);\nirqreturn_t hl_irq_handler_user_interrupt(int irq, void *arg);\nirqreturn_t hl_irq_user_interrupt_thread_handler(int irq, void *arg);\nu32 hl_cq_inc_ptr(u32 ptr);\n\nint hl_asid_init(struct hl_device *hdev);\nvoid hl_asid_fini(struct hl_device *hdev);\nunsigned long hl_asid_alloc(struct hl_device *hdev);\nvoid hl_asid_free(struct hl_device *hdev, unsigned long asid);\n\nint hl_ctx_create(struct hl_device *hdev, struct hl_fpriv *hpriv);\nvoid hl_ctx_free(struct hl_device *hdev, struct hl_ctx *ctx);\nint hl_ctx_init(struct hl_device *hdev, struct hl_ctx *ctx, bool is_kernel_ctx);\nvoid hl_ctx_do_release(struct kref *ref);\nvoid hl_ctx_get(struct hl_ctx *ctx);\nint hl_ctx_put(struct hl_ctx *ctx);\nstruct hl_ctx *hl_get_compute_ctx(struct hl_device *hdev);\nstruct hl_fence *hl_ctx_get_fence(struct hl_ctx *ctx, u64 seq);\nint hl_ctx_get_fences(struct hl_ctx *ctx, u64 *seq_arr,\n\t\t\t\tstruct hl_fence **fence, u32 arr_len);\nvoid hl_ctx_mgr_init(struct hl_ctx_mgr *mgr);\nvoid hl_ctx_mgr_fini(struct hl_device *hdev, struct hl_ctx_mgr *mgr);\n\nint hl_device_init(struct hl_device *hdev);\nvoid hl_device_fini(struct hl_device *hdev);\nint hl_device_suspend(struct hl_device *hdev);\nint hl_device_resume(struct hl_device *hdev);\nint hl_device_reset(struct hl_device *hdev, u32 flags);\nint hl_device_cond_reset(struct hl_device *hdev, u32 flags, u64 event_mask);\nvoid hl_hpriv_get(struct hl_fpriv *hpriv);\nint hl_hpriv_put(struct hl_fpriv *hpriv);\nint hl_device_utilization(struct hl_device *hdev, u32 *utilization);\n\nint hl_build_hwmon_channel_info(struct hl_device *hdev,\n\t\tstruct cpucp_sensor *sensors_arr);\n\nvoid hl_notifier_event_send_all(struct hl_device *hdev, u64 event_mask);\n\nint hl_sysfs_init(struct hl_device *hdev);\nvoid hl_sysfs_fini(struct hl_device *hdev);\n\nint hl_hwmon_init(struct hl_device *hdev);\nvoid hl_hwmon_fini(struct hl_device *hdev);\nvoid hl_hwmon_release_resources(struct hl_device *hdev);\n\nint hl_cb_create(struct hl_device *hdev, struct hl_mem_mgr *mmg,\n\t\t\tstruct hl_ctx *ctx, u32 cb_size, bool internal_cb,\n\t\t\tbool map_cb, u64 *handle);\nint hl_cb_destroy(struct hl_mem_mgr *mmg, u64 cb_handle);\nint hl_hw_block_mmap(struct hl_fpriv *hpriv, struct vm_area_struct *vma);\nstruct hl_cb *hl_cb_get(struct hl_mem_mgr *mmg, u64 handle);\nvoid hl_cb_put(struct hl_cb *cb);\nstruct hl_cb *hl_cb_kernel_create(struct hl_device *hdev, u32 cb_size,\n\t\t\t\t\tbool internal_cb);\nint hl_cb_pool_init(struct hl_device *hdev);\nint hl_cb_pool_fini(struct hl_device *hdev);\nint hl_cb_va_pool_init(struct hl_ctx *ctx);\nvoid hl_cb_va_pool_fini(struct hl_ctx *ctx);\n\nvoid hl_cs_rollback_all(struct hl_device *hdev, bool skip_wq_flush);\nstruct hl_cs_job *hl_cs_allocate_job(struct hl_device *hdev,\n\t\tenum hl_queue_type queue_type, bool is_kernel_allocated_cb);\nvoid hl_sob_reset_error(struct kref *ref);\nint hl_gen_sob_mask(u16 sob_base, u8 sob_mask, u8 *mask);\nvoid hl_fence_put(struct hl_fence *fence);\nvoid hl_fences_put(struct hl_fence **fence, int len);\nvoid hl_fence_get(struct hl_fence *fence);\nvoid cs_get(struct hl_cs *cs);\nbool cs_needs_completion(struct hl_cs *cs);\nbool cs_needs_timeout(struct hl_cs *cs);\nbool is_staged_cs_last_exists(struct hl_device *hdev, struct hl_cs *cs);\nstruct hl_cs *hl_staged_cs_find_first(struct hl_device *hdev, u64 cs_seq);\nvoid hl_multi_cs_completion_init(struct hl_device *hdev);\nu32 hl_get_active_cs_num(struct hl_device *hdev);\n\nvoid goya_set_asic_funcs(struct hl_device *hdev);\nvoid gaudi_set_asic_funcs(struct hl_device *hdev);\nvoid gaudi2_set_asic_funcs(struct hl_device *hdev);\n\nint hl_vm_ctx_init(struct hl_ctx *ctx);\nvoid hl_vm_ctx_fini(struct hl_ctx *ctx);\n\nint hl_vm_init(struct hl_device *hdev);\nvoid hl_vm_fini(struct hl_device *hdev);\n\nvoid hl_hw_block_mem_init(struct hl_ctx *ctx);\nvoid hl_hw_block_mem_fini(struct hl_ctx *ctx);\n\nu64 hl_reserve_va_block(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\tenum hl_va_range_type type, u64 size, u32 alignment);\nint hl_unreserve_va_block(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\tu64 start_addr, u64 size);\nint hl_pin_host_memory(struct hl_device *hdev, u64 addr, u64 size,\n\t\t\tstruct hl_userptr *userptr);\nvoid hl_unpin_host_memory(struct hl_device *hdev, struct hl_userptr *userptr);\nvoid hl_userptr_delete_list(struct hl_device *hdev,\n\t\t\t\tstruct list_head *userptr_list);\nbool hl_userptr_is_pinned(struct hl_device *hdev, u64 addr, u32 size,\n\t\t\t\tstruct list_head *userptr_list,\n\t\t\t\tstruct hl_userptr **userptr);\n\nint hl_mmu_init(struct hl_device *hdev);\nvoid hl_mmu_fini(struct hl_device *hdev);\nint hl_mmu_ctx_init(struct hl_ctx *ctx);\nvoid hl_mmu_ctx_fini(struct hl_ctx *ctx);\nint hl_mmu_map_page(struct hl_ctx *ctx, u64 virt_addr, u64 phys_addr,\n\t\tu32 page_size, bool flush_pte);\nint hl_mmu_get_real_page_size(struct hl_device *hdev, struct hl_mmu_properties *mmu_prop,\n\t\t\t\tu32 page_size, u32 *real_page_size, bool is_dram_addr);\nint hl_mmu_unmap_page(struct hl_ctx *ctx, u64 virt_addr, u32 page_size,\n\t\tbool flush_pte);\nint hl_mmu_map_contiguous(struct hl_ctx *ctx, u64 virt_addr,\n\t\t\t\t\tu64 phys_addr, u32 size);\nint hl_mmu_unmap_contiguous(struct hl_ctx *ctx, u64 virt_addr, u32 size);\nint hl_mmu_invalidate_cache(struct hl_device *hdev, bool is_hard, u32 flags);\nint hl_mmu_invalidate_cache_range(struct hl_device *hdev, bool is_hard,\n\t\t\t\t\tu32 flags, u32 asid, u64 va, u64 size);\nint hl_mmu_prefetch_cache_range(struct hl_ctx *ctx, u32 flags, u32 asid, u64 va, u64 size);\nu64 hl_mmu_get_next_hop_addr(struct hl_ctx *ctx, u64 curr_pte);\nu64 hl_mmu_get_hop_pte_phys_addr(struct hl_ctx *ctx, struct hl_mmu_properties *mmu_prop,\n\t\t\t\t\tu8 hop_idx, u64 hop_addr, u64 virt_addr);\nvoid hl_mmu_hr_flush(struct hl_ctx *ctx);\nint hl_mmu_hr_init(struct hl_device *hdev, struct hl_mmu_hr_priv *hr_priv, u32 hop_table_size,\n\t\t\tu64 pgt_size);\nvoid hl_mmu_hr_fini(struct hl_device *hdev, struct hl_mmu_hr_priv *hr_priv, u32 hop_table_size);\nvoid hl_mmu_hr_free_hop_remove_pgt(struct pgt_info *pgt_info, struct hl_mmu_hr_priv *hr_priv,\n\t\t\t\tu32 hop_table_size);\nu64 hl_mmu_hr_pte_phys_to_virt(struct hl_ctx *ctx, struct pgt_info *pgt, u64 phys_pte_addr,\n\t\t\t\t\t\t\tu32 hop_table_size);\nvoid hl_mmu_hr_write_pte(struct hl_ctx *ctx, struct pgt_info *pgt_info, u64 phys_pte_addr,\n\t\t\t\t\t\t\tu64 val, u32 hop_table_size);\nvoid hl_mmu_hr_clear_pte(struct hl_ctx *ctx, struct pgt_info *pgt_info, u64 phys_pte_addr,\n\t\t\t\t\t\t\tu32 hop_table_size);\nint hl_mmu_hr_put_pte(struct hl_ctx *ctx, struct pgt_info *pgt_info, struct hl_mmu_hr_priv *hr_priv,\n\t\t\t\t\t\t\tu32 hop_table_size);\nvoid hl_mmu_hr_get_pte(struct hl_ctx *ctx, struct hl_hr_mmu_funcs *hr_func, u64 phys_hop_addr);\nstruct pgt_info *hl_mmu_hr_get_next_hop_pgt_info(struct hl_ctx *ctx,\n\t\t\t\t\t\t\tstruct hl_hr_mmu_funcs *hr_func,\n\t\t\t\t\t\t\tu64 curr_pte);\nstruct pgt_info *hl_mmu_hr_alloc_hop(struct hl_ctx *ctx, struct hl_mmu_hr_priv *hr_priv,\n\t\t\t\t\t\t\tstruct hl_hr_mmu_funcs *hr_func,\n\t\t\t\t\t\t\tstruct hl_mmu_properties *mmu_prop);\nstruct pgt_info *hl_mmu_hr_get_alloc_next_hop(struct hl_ctx *ctx,\n\t\t\t\t\t\t\tstruct hl_mmu_hr_priv *hr_priv,\n\t\t\t\t\t\t\tstruct hl_hr_mmu_funcs *hr_func,\n\t\t\t\t\t\t\tstruct hl_mmu_properties *mmu_prop,\n\t\t\t\t\t\t\tu64 curr_pte, bool *is_new_hop);\nint hl_mmu_hr_get_tlb_info(struct hl_ctx *ctx, u64 virt_addr, struct hl_mmu_hop_info *hops,\n\t\t\t\t\t\t\tstruct hl_hr_mmu_funcs *hr_func);\nint hl_mmu_if_set_funcs(struct hl_device *hdev);\nvoid hl_mmu_v1_set_funcs(struct hl_device *hdev, struct hl_mmu_funcs *mmu);\nvoid hl_mmu_v2_hr_set_funcs(struct hl_device *hdev, struct hl_mmu_funcs *mmu);\nint hl_mmu_va_to_pa(struct hl_ctx *ctx, u64 virt_addr, u64 *phys_addr);\nint hl_mmu_get_tlb_info(struct hl_ctx *ctx, u64 virt_addr,\n\t\t\tstruct hl_mmu_hop_info *hops);\nu64 hl_mmu_scramble_addr(struct hl_device *hdev, u64 addr);\nu64 hl_mmu_descramble_addr(struct hl_device *hdev, u64 addr);\nbool hl_is_dram_va(struct hl_device *hdev, u64 virt_addr);\n\nint hl_fw_load_fw_to_device(struct hl_device *hdev, const char *fw_name,\n\t\t\t\tvoid __iomem *dst, u32 src_offset, u32 size);\nint hl_fw_send_pci_access_msg(struct hl_device *hdev, u32 opcode, u64 value);\nint hl_fw_send_cpu_message(struct hl_device *hdev, u32 hw_queue_id, u32 *msg,\n\t\t\t\tu16 len, u32 timeout, u64 *result);\nint hl_fw_unmask_irq(struct hl_device *hdev, u16 event_type);\nint hl_fw_unmask_irq_arr(struct hl_device *hdev, const u32 *irq_arr,\n\t\tsize_t irq_arr_size);\nint hl_fw_test_cpu_queue(struct hl_device *hdev);\nvoid *hl_fw_cpu_accessible_dma_pool_alloc(struct hl_device *hdev, size_t size,\n\t\t\t\t\t\tdma_addr_t *dma_handle);\nvoid hl_fw_cpu_accessible_dma_pool_free(struct hl_device *hdev, size_t size,\n\t\t\t\t\tvoid *vaddr);\nint hl_fw_send_heartbeat(struct hl_device *hdev);\nint hl_fw_cpucp_info_get(struct hl_device *hdev,\n\t\t\t\tu32 sts_boot_dev_sts0_reg,\n\t\t\t\tu32 sts_boot_dev_sts1_reg, u32 boot_err0_reg,\n\t\t\t\tu32 boot_err1_reg);\nint hl_fw_cpucp_handshake(struct hl_device *hdev,\n\t\t\t\tu32 sts_boot_dev_sts0_reg,\n\t\t\t\tu32 sts_boot_dev_sts1_reg, u32 boot_err0_reg,\n\t\t\t\tu32 boot_err1_reg);\nint hl_fw_get_eeprom_data(struct hl_device *hdev, void *data, size_t max_size);\nint hl_fw_get_monitor_dump(struct hl_device *hdev, void *data);\nint hl_fw_cpucp_pci_counters_get(struct hl_device *hdev,\n\t\tstruct hl_info_pci_counters *counters);\nint hl_fw_cpucp_total_energy_get(struct hl_device *hdev,\n\t\t\tu64 *total_energy);\nint get_used_pll_index(struct hl_device *hdev, u32 input_pll_index,\n\t\t\t\t\t\tenum pll_index *pll_index);\nint hl_fw_cpucp_pll_info_get(struct hl_device *hdev, u32 pll_index,\n\t\tu16 *pll_freq_arr);\nint hl_fw_cpucp_power_get(struct hl_device *hdev, u64 *power);\nvoid hl_fw_ask_hard_reset_without_linux(struct hl_device *hdev);\nvoid hl_fw_ask_halt_machine_without_linux(struct hl_device *hdev);\nint hl_fw_init_cpu(struct hl_device *hdev);\nint hl_fw_wait_preboot_ready(struct hl_device *hdev);\nint hl_fw_read_preboot_status(struct hl_device *hdev);\nint hl_fw_dynamic_send_protocol_cmd(struct hl_device *hdev,\n\t\t\t\tstruct fw_load_mgr *fw_loader,\n\t\t\t\tenum comms_cmd cmd, unsigned int size,\n\t\t\t\tbool wait_ok, u32 timeout);\nint hl_fw_dram_replaced_row_get(struct hl_device *hdev,\n\t\t\t\tstruct cpucp_hbm_row_info *info);\nint hl_fw_dram_pending_row_get(struct hl_device *hdev, u32 *pend_rows_num);\nint hl_fw_cpucp_engine_core_asid_set(struct hl_device *hdev, u32 asid);\nint hl_fw_send_device_activity(struct hl_device *hdev, bool open);\nint hl_fw_send_soft_reset(struct hl_device *hdev);\nint hl_pci_bars_map(struct hl_device *hdev, const char * const name[3],\n\t\t\tbool is_wc[3]);\nint hl_pci_elbi_read(struct hl_device *hdev, u64 addr, u32 *data);\nint hl_pci_iatu_write(struct hl_device *hdev, u32 addr, u32 data);\nint hl_pci_set_inbound_region(struct hl_device *hdev, u8 region,\n\t\tstruct hl_inbound_pci_region *pci_region);\nint hl_pci_set_outbound_region(struct hl_device *hdev,\n\t\tstruct hl_outbound_pci_region *pci_region);\nenum pci_region hl_get_pci_memory_region(struct hl_device *hdev, u64 addr);\nint hl_pci_init(struct hl_device *hdev);\nvoid hl_pci_fini(struct hl_device *hdev);\n\nlong hl_fw_get_frequency(struct hl_device *hdev, u32 pll_index, bool curr);\nvoid hl_fw_set_frequency(struct hl_device *hdev, u32 pll_index, u64 freq);\nint hl_get_temperature(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nint hl_set_temperature(struct hl_device *hdev, int sensor_index, u32 attr, long value);\nint hl_get_voltage(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nint hl_get_current(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nint hl_get_fan_speed(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nint hl_get_pwm_info(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nvoid hl_set_pwm_info(struct hl_device *hdev, int sensor_index, u32 attr, long value);\nlong hl_fw_get_max_power(struct hl_device *hdev);\nvoid hl_fw_set_max_power(struct hl_device *hdev);\nint hl_fw_get_sec_attest_info(struct hl_device *hdev, struct cpucp_sec_attest_info *sec_attest_info,\n\t\t\t\tu32 nonce);\nint hl_set_voltage(struct hl_device *hdev, int sensor_index, u32 attr, long value);\nint hl_set_current(struct hl_device *hdev, int sensor_index, u32 attr, long value);\nint hl_set_power(struct hl_device *hdev, int sensor_index, u32 attr, long value);\nint hl_get_power(struct hl_device *hdev, int sensor_index, u32 attr, long *value);\nint hl_fw_get_clk_rate(struct hl_device *hdev, u32 *cur_clk, u32 *max_clk);\nvoid hl_fw_set_pll_profile(struct hl_device *hdev);\nvoid hl_sysfs_add_dev_clk_attr(struct hl_device *hdev, struct attribute_group *dev_clk_attr_grp);\nvoid hl_sysfs_add_dev_vrm_attr(struct hl_device *hdev, struct attribute_group *dev_vrm_attr_grp);\nint hl_fw_send_generic_request(struct hl_device *hdev, enum hl_passthrough_type sub_opcode,\n\t\t\t\t\t\tdma_addr_t buff, u32 *size);\n\nvoid hw_sob_get(struct hl_hw_sob *hw_sob);\nvoid hw_sob_put(struct hl_hw_sob *hw_sob);\nvoid hl_encaps_release_handle_and_put_ctx(struct kref *ref);\nvoid hl_encaps_release_handle_and_put_sob_ctx(struct kref *ref);\nvoid hl_hw_queue_encaps_sig_set_sob_info(struct hl_device *hdev,\n\t\t\tstruct hl_cs *cs, struct hl_cs_job *job,\n\t\t\tstruct hl_cs_compl *cs_cmpl);\n\nint hl_dec_init(struct hl_device *hdev);\nvoid hl_dec_fini(struct hl_device *hdev);\nvoid hl_dec_ctx_fini(struct hl_ctx *ctx);\n\nvoid hl_release_pending_user_interrupts(struct hl_device *hdev);\nvoid hl_abort_waiting_for_cs_completions(struct hl_device *hdev);\nint hl_cs_signal_sob_wraparound_handler(struct hl_device *hdev, u32 q_idx,\n\t\t\tstruct hl_hw_sob **hw_sob, u32 count, bool encaps_sig);\n\nint hl_state_dump(struct hl_device *hdev);\nconst char *hl_state_dump_get_sync_name(struct hl_device *hdev, u32 sync_id);\nconst char *hl_state_dump_get_monitor_name(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_mon_state_dump *mon);\nvoid hl_state_dump_free_sync_to_engine_map(struct hl_sync_to_engine_map *map);\n__printf(4, 5) int hl_snprintf_resize(char **buf, size_t *size, size_t *offset,\n\t\t\t\t\tconst char *format, ...);\nchar *hl_format_as_binary(char *buf, size_t buf_len, u32 n);\nconst char *hl_sync_engine_to_string(enum hl_sync_engine_type engine_type);\n\nvoid hl_mem_mgr_init(struct device *dev, struct hl_mem_mgr *mmg);\nvoid hl_mem_mgr_fini(struct hl_mem_mgr *mmg);\nvoid hl_mem_mgr_idr_destroy(struct hl_mem_mgr *mmg);\nint hl_mem_mgr_mmap(struct hl_mem_mgr *mmg, struct vm_area_struct *vma,\n\t\t    void *args);\nstruct hl_mmap_mem_buf *hl_mmap_mem_buf_get(struct hl_mem_mgr *mmg,\n\t\t\t\t\t\t   u64 handle);\nint hl_mmap_mem_buf_put_handle(struct hl_mem_mgr *mmg, u64 handle);\nint hl_mmap_mem_buf_put(struct hl_mmap_mem_buf *buf);\nstruct hl_mmap_mem_buf *\nhl_mmap_mem_buf_alloc(struct hl_mem_mgr *mmg,\n\t\t      struct hl_mmap_mem_buf_behavior *behavior, gfp_t gfp,\n\t\t      void *args);\n__printf(2, 3) void hl_engine_data_sprintf(struct engines_data *e, const char *fmt, ...);\nvoid hl_capture_razwi(struct hl_device *hdev, u64 addr, u16 *engine_id, u16 num_of_engines,\n\t\t\tu8 flags);\nvoid hl_handle_razwi(struct hl_device *hdev, u64 addr, u16 *engine_id, u16 num_of_engines,\n\t\t\tu8 flags, u64 *event_mask);\nvoid hl_capture_page_fault(struct hl_device *hdev, u64 addr, u16 eng_id, bool is_pmmu);\nvoid hl_handle_page_fault(struct hl_device *hdev, u64 addr, u16 eng_id, bool is_pmmu,\n\t\t\t\tu64 *event_mask);\nvoid hl_handle_critical_hw_err(struct hl_device *hdev, u16 event_id, u64 *event_mask);\nvoid hl_handle_fw_err(struct hl_device *hdev, struct hl_info_fw_err_info *info);\nvoid hl_enable_err_info_capture(struct hl_error_info *captured_err_info);\n\n#ifdef CONFIG_DEBUG_FS\n\nvoid hl_debugfs_init(void);\nvoid hl_debugfs_fini(void);\nint hl_debugfs_device_init(struct hl_device *hdev);\nvoid hl_debugfs_device_fini(struct hl_device *hdev);\nvoid hl_debugfs_add_device(struct hl_device *hdev);\nvoid hl_debugfs_remove_device(struct hl_device *hdev);\nvoid hl_debugfs_add_file(struct hl_fpriv *hpriv);\nvoid hl_debugfs_remove_file(struct hl_fpriv *hpriv);\nvoid hl_debugfs_add_cb(struct hl_cb *cb);\nvoid hl_debugfs_remove_cb(struct hl_cb *cb);\nvoid hl_debugfs_add_cs(struct hl_cs *cs);\nvoid hl_debugfs_remove_cs(struct hl_cs *cs);\nvoid hl_debugfs_add_job(struct hl_device *hdev, struct hl_cs_job *job);\nvoid hl_debugfs_remove_job(struct hl_device *hdev, struct hl_cs_job *job);\nvoid hl_debugfs_add_userptr(struct hl_device *hdev, struct hl_userptr *userptr);\nvoid hl_debugfs_remove_userptr(struct hl_device *hdev,\n\t\t\t\tstruct hl_userptr *userptr);\nvoid hl_debugfs_add_ctx_mem_hash(struct hl_device *hdev, struct hl_ctx *ctx);\nvoid hl_debugfs_remove_ctx_mem_hash(struct hl_device *hdev, struct hl_ctx *ctx);\nvoid hl_debugfs_set_state_dump(struct hl_device *hdev, char *data,\n\t\t\t\t\tunsigned long length);\n\n#else\n\nstatic inline void __init hl_debugfs_init(void)\n{\n}\n\nstatic inline void hl_debugfs_fini(void)\n{\n}\n\nstatic inline int hl_debugfs_device_init(struct hl_device *hdev)\n{\n\treturn 0;\n}\n\nstatic inline void hl_debugfs_device_fini(struct hl_device *hdev)\n{\n}\n\nstatic inline void hl_debugfs_add_device(struct hl_device *hdev)\n{\n}\n\nstatic inline void hl_debugfs_remove_device(struct hl_device *hdev)\n{\n}\n\nstatic inline void hl_debugfs_add_file(struct hl_fpriv *hpriv)\n{\n}\n\nstatic inline void hl_debugfs_remove_file(struct hl_fpriv *hpriv)\n{\n}\n\nstatic inline void hl_debugfs_add_cb(struct hl_cb *cb)\n{\n}\n\nstatic inline void hl_debugfs_remove_cb(struct hl_cb *cb)\n{\n}\n\nstatic inline void hl_debugfs_add_cs(struct hl_cs *cs)\n{\n}\n\nstatic inline void hl_debugfs_remove_cs(struct hl_cs *cs)\n{\n}\n\nstatic inline void hl_debugfs_add_job(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_job *job)\n{\n}\n\nstatic inline void hl_debugfs_remove_job(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_job *job)\n{\n}\n\nstatic inline void hl_debugfs_add_userptr(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_userptr *userptr)\n{\n}\n\nstatic inline void hl_debugfs_remove_userptr(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_userptr *userptr)\n{\n}\n\nstatic inline void hl_debugfs_add_ctx_mem_hash(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_ctx *ctx)\n{\n}\n\nstatic inline void hl_debugfs_remove_ctx_mem_hash(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_ctx *ctx)\n{\n}\n\nstatic inline void hl_debugfs_set_state_dump(struct hl_device *hdev,\n\t\t\t\t\tchar *data, unsigned long length)\n{\n}\n\n#endif\n\n \nint hl_unsecure_register(struct hl_device *hdev, u32 mm_reg_addr, int offset,\n\t\tconst u32 pb_blocks[], struct hl_block_glbl_sec sgs_array[],\n\t\tint array_size);\nint hl_unsecure_registers(struct hl_device *hdev, const u32 mm_reg_array[],\n\t\tint mm_array_size, int offset, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[], int blocks_array_size);\nvoid hl_config_glbl_sec(struct hl_device *hdev, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[], u32 block_offset,\n\t\tint array_size);\nvoid hl_secure_block(struct hl_device *hdev,\n\t\tstruct hl_block_glbl_sec sgs_array[], int array_size);\nint hl_init_pb_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *regs_array, u32 regs_array_size, u64 mask);\nint hl_init_pb(struct hl_device *hdev, u32 num_dcores, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *regs_array, u32 regs_array_size);\nint hl_init_pb_ranges_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *regs_range_array, u32 regs_range_array_size,\n\t\tu64 mask);\nint hl_init_pb_ranges(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *regs_range_array,\n\t\tu32 regs_range_array_size);\nint hl_init_pb_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *regs_array, u32 regs_array_size);\nint hl_init_pb_ranges_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *regs_range_array,\n\t\tu32 regs_range_array_size);\nvoid hl_ack_pb(struct hl_device *hdev, u32 num_dcores, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size);\nvoid hl_ack_pb_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size, u64 mask);\nvoid hl_ack_pb_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size);\n\n \nlong hl_ioctl(struct file *filep, unsigned int cmd, unsigned long arg);\nlong hl_ioctl_control(struct file *filep, unsigned int cmd, unsigned long arg);\nint hl_cb_ioctl(struct hl_fpriv *hpriv, void *data);\nint hl_cs_ioctl(struct hl_fpriv *hpriv, void *data);\nint hl_wait_ioctl(struct hl_fpriv *hpriv, void *data);\nint hl_mem_ioctl(struct hl_fpriv *hpriv, void *data);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}