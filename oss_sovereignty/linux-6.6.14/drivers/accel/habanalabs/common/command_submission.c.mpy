{
  "module_name": "command_submission.c",
  "hash_id": "5f4b881eb8b3904f403dad38915d324fbc104716bd9afc4b9e112f9ab74f0757",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/command_submission.c",
  "human_readable_source": "\n\n \n\n#include <uapi/drm/habanalabs_accel.h>\n#include \"habanalabs.h\"\n\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n\n#define HL_CS_FLAGS_TYPE_MASK\t(HL_CS_FLAGS_SIGNAL | HL_CS_FLAGS_WAIT | \\\n\t\t\tHL_CS_FLAGS_COLLECTIVE_WAIT | HL_CS_FLAGS_RESERVE_SIGNALS_ONLY | \\\n\t\t\tHL_CS_FLAGS_UNRESERVE_SIGNALS_ONLY | HL_CS_FLAGS_ENGINE_CORE_COMMAND | \\\n\t\t\tHL_CS_FLAGS_ENGINES_COMMAND | HL_CS_FLAGS_FLUSH_PCI_HBW_WRITES)\n\n\n#define MAX_TS_ITER_NUM 100\n\n \nenum hl_cs_wait_status {\n\tCS_WAIT_STATUS_BUSY,\n\tCS_WAIT_STATUS_COMPLETED,\n\tCS_WAIT_STATUS_GONE\n};\n\nstatic void job_wq_completion(struct work_struct *work);\nstatic int _hl_cs_wait_ioctl(struct hl_device *hdev, struct hl_ctx *ctx, u64 timeout_us, u64 seq,\n\t\t\t\tenum hl_cs_wait_status *status, s64 *timestamp);\nstatic void cs_do_release(struct kref *ref);\n\nstatic void hl_push_cs_outcome(struct hl_device *hdev,\n\t\t\t       struct hl_cs_outcome_store *outcome_store,\n\t\t\t       u64 seq, ktime_t ts, int error)\n{\n\tstruct hl_cs_outcome *node;\n\tunsigned long flags;\n\n\t \n\n\tspin_lock_irqsave(&outcome_store->db_lock, flags);\n\n\tif (list_empty(&outcome_store->free_list)) {\n\t\tnode = list_last_entry(&outcome_store->used_list,\n\t\t\t\t       struct hl_cs_outcome, list_link);\n\t\thash_del(&node->map_link);\n\t\tdev_dbg(hdev->dev, \"CS %llu outcome was lost\\n\", node->seq);\n\t} else {\n\t\tnode = list_last_entry(&outcome_store->free_list,\n\t\t\t\t       struct hl_cs_outcome, list_link);\n\t}\n\n\tlist_del_init(&node->list_link);\n\n\tnode->seq = seq;\n\tnode->ts = ts;\n\tnode->error = error;\n\n\tlist_add(&node->list_link, &outcome_store->used_list);\n\thash_add(outcome_store->outcome_map, &node->map_link, node->seq);\n\n\tspin_unlock_irqrestore(&outcome_store->db_lock, flags);\n}\n\nstatic bool hl_pop_cs_outcome(struct hl_cs_outcome_store *outcome_store,\n\t\t\t       u64 seq, ktime_t *ts, int *error)\n{\n\tstruct hl_cs_outcome *node;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&outcome_store->db_lock, flags);\n\n\thash_for_each_possible(outcome_store->outcome_map, node, map_link, seq)\n\t\tif (node->seq == seq) {\n\t\t\t*ts = node->ts;\n\t\t\t*error = node->error;\n\n\t\t\thash_del(&node->map_link);\n\t\t\tlist_del_init(&node->list_link);\n\t\t\tlist_add(&node->list_link, &outcome_store->free_list);\n\n\t\t\tspin_unlock_irqrestore(&outcome_store->db_lock, flags);\n\n\t\t\treturn true;\n\t\t}\n\n\tspin_unlock_irqrestore(&outcome_store->db_lock, flags);\n\n\treturn false;\n}\n\nstatic void hl_sob_reset(struct kref *ref)\n{\n\tstruct hl_hw_sob *hw_sob = container_of(ref, struct hl_hw_sob,\n\t\t\t\t\t\t\tkref);\n\tstruct hl_device *hdev = hw_sob->hdev;\n\n\tdev_dbg(hdev->dev, \"reset sob id %u\\n\", hw_sob->sob_id);\n\n\thdev->asic_funcs->reset_sob(hdev, hw_sob);\n\n\thw_sob->need_reset = false;\n}\n\nvoid hl_sob_reset_error(struct kref *ref)\n{\n\tstruct hl_hw_sob *hw_sob = container_of(ref, struct hl_hw_sob,\n\t\t\t\t\t\t\tkref);\n\tstruct hl_device *hdev = hw_sob->hdev;\n\n\tdev_crit(hdev->dev,\n\t\t\"SOB release shouldn't be called here, q_idx: %d, sob_id: %d\\n\",\n\t\thw_sob->q_idx, hw_sob->sob_id);\n}\n\nvoid hw_sob_put(struct hl_hw_sob *hw_sob)\n{\n\tif (hw_sob)\n\t\tkref_put(&hw_sob->kref, hl_sob_reset);\n}\n\nstatic void hw_sob_put_err(struct hl_hw_sob *hw_sob)\n{\n\tif (hw_sob)\n\t\tkref_put(&hw_sob->kref, hl_sob_reset_error);\n}\n\nvoid hw_sob_get(struct hl_hw_sob *hw_sob)\n{\n\tif (hw_sob)\n\t\tkref_get(&hw_sob->kref);\n}\n\n \nint hl_gen_sob_mask(u16 sob_base, u8 sob_mask, u8 *mask)\n{\n\tint i;\n\n\tif (sob_mask == 0)\n\t\treturn -EINVAL;\n\n\tif (sob_mask == 0x1) {\n\t\t*mask = ~(1 << (sob_base & 0x7));\n\t} else {\n\t\t \n\t\tfor (i = BITS_PER_BYTE - 1 ; i >= 0 ; i--)\n\t\t\tif (BIT(i) & sob_mask)\n\t\t\t\tbreak;\n\n\t\tif (i > (HL_MAX_SOBS_PER_MONITOR - (sob_base & 0x7) - 1))\n\t\t\treturn -EINVAL;\n\n\t\t*mask = ~sob_mask;\n\t}\n\n\treturn 0;\n}\n\nstatic void hl_fence_release(struct kref *kref)\n{\n\tstruct hl_fence *fence =\n\t\tcontainer_of(kref, struct hl_fence, refcount);\n\tstruct hl_cs_compl *hl_cs_cmpl =\n\t\tcontainer_of(fence, struct hl_cs_compl, base_fence);\n\n\tkfree(hl_cs_cmpl);\n}\n\nvoid hl_fence_put(struct hl_fence *fence)\n{\n\tif (IS_ERR_OR_NULL(fence))\n\t\treturn;\n\tkref_put(&fence->refcount, hl_fence_release);\n}\n\nvoid hl_fences_put(struct hl_fence **fence, int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++, fence++)\n\t\thl_fence_put(*fence);\n}\n\nvoid hl_fence_get(struct hl_fence *fence)\n{\n\tif (fence)\n\t\tkref_get(&fence->refcount);\n}\n\nstatic void hl_fence_init(struct hl_fence *fence, u64 sequence)\n{\n\tkref_init(&fence->refcount);\n\tfence->cs_sequence = sequence;\n\tfence->error = 0;\n\tfence->timestamp = ktime_set(0, 0);\n\tfence->mcs_handling_done = false;\n\tinit_completion(&fence->completion);\n}\n\nvoid cs_get(struct hl_cs *cs)\n{\n\tkref_get(&cs->refcount);\n}\n\nstatic int cs_get_unless_zero(struct hl_cs *cs)\n{\n\treturn kref_get_unless_zero(&cs->refcount);\n}\n\nstatic void cs_put(struct hl_cs *cs)\n{\n\tkref_put(&cs->refcount, cs_do_release);\n}\n\nstatic void cs_job_do_release(struct kref *ref)\n{\n\tstruct hl_cs_job *job = container_of(ref, struct hl_cs_job, refcount);\n\n\tkfree(job);\n}\n\nstatic void hl_cs_job_put(struct hl_cs_job *job)\n{\n\tkref_put(&job->refcount, cs_job_do_release);\n}\n\nbool cs_needs_completion(struct hl_cs *cs)\n{\n\t \n\tif (cs->staged_cs && !cs->staged_last)\n\t\treturn false;\n\n\treturn true;\n}\n\nbool cs_needs_timeout(struct hl_cs *cs)\n{\n\t \n\tif (cs->staged_cs && !cs->staged_first)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool is_cb_patched(struct hl_device *hdev, struct hl_cs_job *job)\n{\n\t \n\treturn (job->queue_type == QUEUE_TYPE_EXT);\n}\n\n \nstatic int cs_parser(struct hl_fpriv *hpriv, struct hl_cs_job *job)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_cs_parser parser;\n\tint rc;\n\n\tparser.ctx_id = job->cs->ctx->asid;\n\tparser.cs_sequence = job->cs->sequence;\n\tparser.job_id = job->id;\n\n\tparser.hw_queue_id = job->hw_queue_id;\n\tparser.job_userptr_list = &job->userptr_list;\n\tparser.patched_cb = NULL;\n\tparser.user_cb = job->user_cb;\n\tparser.user_cb_size = job->user_cb_size;\n\tparser.queue_type = job->queue_type;\n\tparser.is_kernel_allocated_cb = job->is_kernel_allocated_cb;\n\tjob->patched_cb = NULL;\n\tparser.completion = cs_needs_completion(job->cs);\n\n\trc = hdev->asic_funcs->cs_parser(hdev, &parser);\n\n\tif (is_cb_patched(hdev, job)) {\n\t\tif (!rc) {\n\t\t\tjob->patched_cb = parser.patched_cb;\n\t\t\tjob->job_cb_size = parser.patched_cb_size;\n\t\t\tjob->contains_dma_pkt = parser.contains_dma_pkt;\n\t\t\tatomic_inc(&job->patched_cb->cs_cnt);\n\t\t}\n\n\t\t \n\t\tatomic_dec(&job->user_cb->cs_cnt);\n\t\thl_cb_put(job->user_cb);\n\t\tjob->user_cb = NULL;\n\t} else if (!rc) {\n\t\tjob->job_cb_size = job->user_cb_size;\n\t}\n\n\treturn rc;\n}\n\nstatic void hl_complete_job(struct hl_device *hdev, struct hl_cs_job *job)\n{\n\tstruct hl_cs *cs = job->cs;\n\n\tif (is_cb_patched(hdev, job)) {\n\t\thl_userptr_delete_list(hdev, &job->userptr_list);\n\n\t\t \n\t\tif (job->patched_cb) {\n\t\t\tatomic_dec(&job->patched_cb->cs_cnt);\n\t\t\thl_cb_put(job->patched_cb);\n\t\t}\n\t}\n\n\t \n\tif (job->is_kernel_allocated_cb &&\n\t\t\t(job->queue_type == QUEUE_TYPE_HW || job->queue_type == QUEUE_TYPE_INT)) {\n\t\tatomic_dec(&job->user_cb->cs_cnt);\n\t\thl_cb_put(job->user_cb);\n\t}\n\n\t \n\tspin_lock(&cs->job_lock);\n\tlist_del(&job->cs_node);\n\tspin_unlock(&cs->job_lock);\n\n\thl_debugfs_remove_job(hdev, job);\n\n\t \n\tif (cs_needs_completion(cs) &&\n\t\t\t(job->queue_type == QUEUE_TYPE_EXT || job->queue_type == QUEUE_TYPE_HW)) {\n\n\t\t \n\t\tif (hdev->asic_prop.completion_mode == HL_COMPLETION_MODE_JOB)\n\t\t\tcs->completion_timestamp = job->timestamp;\n\n\t\tcs_put(cs);\n\t}\n\n\thl_cs_job_put(job);\n}\n\n \nstruct hl_cs *hl_staged_cs_find_first(struct hl_device *hdev, u64 cs_seq)\n{\n\tstruct hl_cs *cs;\n\n\tlist_for_each_entry_reverse(cs, &hdev->cs_mirror_list, mirror_node)\n\t\tif (cs->staged_cs && cs->staged_first &&\n\t\t\t\tcs->sequence == cs_seq)\n\t\t\treturn cs;\n\n\treturn NULL;\n}\n\n \nbool is_staged_cs_last_exists(struct hl_device *hdev, struct hl_cs *cs)\n{\n\tstruct hl_cs *last_entry;\n\n\tlast_entry = list_last_entry(&cs->staged_cs_node, struct hl_cs,\n\t\t\t\t\t\t\t\tstaged_cs_node);\n\n\tif (last_entry->staged_last)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void staged_cs_get(struct hl_device *hdev, struct hl_cs *cs)\n{\n\t \n\tif (!cs->staged_last)\n\t\tcs_get(cs);\n}\n\n \nstatic void staged_cs_put(struct hl_device *hdev, struct hl_cs *cs)\n{\n\t \n\tif (!cs_needs_completion(cs))\n\t\tcs_put(cs);\n}\n\nstatic void cs_handle_tdr(struct hl_device *hdev, struct hl_cs *cs)\n{\n\tstruct hl_cs *next = NULL, *iter, *first_cs;\n\n\tif (!cs_needs_timeout(cs))\n\t\treturn;\n\n\tspin_lock(&hdev->cs_mirror_lock);\n\n\t \n\tif (cs->staged_cs && cs->staged_last) {\n\t\tfirst_cs = hl_staged_cs_find_first(hdev, cs->staged_sequence);\n\t\tif (first_cs)\n\t\t\tcs = first_cs;\n\t}\n\n\tspin_unlock(&hdev->cs_mirror_lock);\n\n\t \n\tif (cs->timedout || hdev->timeout_jiffies == MAX_SCHEDULE_TIMEOUT)\n\t\treturn;\n\n\tif (cs->tdr_active)\n\t\tcancel_delayed_work_sync(&cs->work_tdr);\n\n\tspin_lock(&hdev->cs_mirror_lock);\n\n\t \n\tlist_for_each_entry(iter, &hdev->cs_mirror_list, mirror_node)\n\t\tif (cs_needs_timeout(iter)) {\n\t\t\tnext = iter;\n\t\t\tbreak;\n\t\t}\n\n\tif (next && !next->tdr_active) {\n\t\tnext->tdr_active = true;\n\t\tschedule_delayed_work(&next->work_tdr, next->timeout_jiffies);\n\t}\n\n\tspin_unlock(&hdev->cs_mirror_lock);\n}\n\n \nstatic void force_complete_multi_cs(struct hl_device *hdev)\n{\n\tint i;\n\n\tfor (i = 0; i < MULTI_CS_MAX_USER_CTX; i++) {\n\t\tstruct multi_cs_completion *mcs_compl;\n\n\t\tmcs_compl = &hdev->multi_cs_completion[i];\n\n\t\tspin_lock(&mcs_compl->lock);\n\n\t\tif (!mcs_compl->used) {\n\t\t\tspin_unlock(&mcs_compl->lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tdev_err(hdev->dev,\n\t\t\t\t\"multi-CS completion context %d still waiting when calling force completion\\n\",\n\t\t\t\ti);\n\t\tcomplete_all(&mcs_compl->completion);\n\t\tspin_unlock(&mcs_compl->lock);\n\t}\n}\n\n \nstatic void complete_multi_cs(struct hl_device *hdev, struct hl_cs *cs)\n{\n\tstruct hl_fence *fence = cs->fence;\n\tint i;\n\n\t \n\tif (cs->staged_cs && !cs->staged_first)\n\t\treturn;\n\n\tfor (i = 0; i < MULTI_CS_MAX_USER_CTX; i++) {\n\t\tstruct multi_cs_completion *mcs_compl;\n\n\t\tmcs_compl = &hdev->multi_cs_completion[i];\n\t\tif (!mcs_compl->used)\n\t\t\tcontinue;\n\n\t\tspin_lock(&mcs_compl->lock);\n\n\t\t \n\t\tif (mcs_compl->used &&\n\t\t\t\t(fence->stream_master_qid_map &\n\t\t\t\t\tmcs_compl->stream_master_qid_map)) {\n\t\t\t \n\t\t\tif (!mcs_compl->timestamp)\n\t\t\t\tmcs_compl->timestamp = ktime_to_ns(fence->timestamp);\n\n\t\t\tcomplete_all(&mcs_compl->completion);\n\n\t\t\t \n\t\t\tfence->mcs_handling_done = true;\n\t\t}\n\n\t\tspin_unlock(&mcs_compl->lock);\n\t}\n\t \n\tfence->mcs_handling_done = true;\n}\n\nstatic inline void cs_release_sob_reset_handler(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs *cs,\n\t\t\t\t\tstruct hl_cs_compl *hl_cs_cmpl)\n{\n\t \n\tif (!hl_cs_cmpl->hw_sob || !cs->submitted)\n\t\treturn;\n\n\tspin_lock(&hl_cs_cmpl->lock);\n\n\t \n\tif ((hl_cs_cmpl->type == CS_TYPE_SIGNAL) ||\n\t\t\t(hl_cs_cmpl->type == CS_TYPE_WAIT) ||\n\t\t\t(hl_cs_cmpl->type == CS_TYPE_COLLECTIVE_WAIT) ||\n\t\t\t(!!hl_cs_cmpl->encaps_signals)) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"CS 0x%llx type %d finished, sob_id: %d, sob_val: %u\\n\",\n\t\t\t\thl_cs_cmpl->cs_seq,\n\t\t\t\thl_cs_cmpl->type,\n\t\t\t\thl_cs_cmpl->hw_sob->sob_id,\n\t\t\t\thl_cs_cmpl->sob_val);\n\n\t\thw_sob_put(hl_cs_cmpl->hw_sob);\n\n\t\tif (hl_cs_cmpl->type == CS_TYPE_COLLECTIVE_WAIT)\n\t\t\thdev->asic_funcs->reset_sob_group(hdev,\n\t\t\t\t\thl_cs_cmpl->sob_group);\n\t}\n\n\tspin_unlock(&hl_cs_cmpl->lock);\n}\n\nstatic void cs_do_release(struct kref *ref)\n{\n\tstruct hl_cs *cs = container_of(ref, struct hl_cs, refcount);\n\tstruct hl_device *hdev = cs->ctx->hdev;\n\tstruct hl_cs_job *job, *tmp;\n\tstruct hl_cs_compl *hl_cs_cmpl =\n\t\t\tcontainer_of(cs->fence, struct hl_cs_compl, base_fence);\n\n\tcs->completed = true;\n\n\t \n\tlist_for_each_entry_safe(job, tmp, &cs->job_list, cs_node)\n\t\thl_complete_job(hdev, job);\n\n\tif (!cs->submitted) {\n\t\t \n\t\tif (cs->type == CS_TYPE_WAIT ||\n\t\t\t\tcs->type == CS_TYPE_COLLECTIVE_WAIT)\n\t\t\thl_fence_put(cs->signal_fence);\n\n\t\tgoto out;\n\t}\n\n\t \n\thl_hw_queue_update_ci(cs);\n\n\t \n\tspin_lock(&hdev->cs_mirror_lock);\n\tlist_del_init(&cs->mirror_node);\n\tspin_unlock(&hdev->cs_mirror_lock);\n\n\tcs_handle_tdr(hdev, cs);\n\n\tif (cs->staged_cs) {\n\t\t \n\t\tif (cs->staged_last) {\n\t\t\tstruct hl_cs *staged_cs, *tmp_cs;\n\n\t\t\tlist_for_each_entry_safe(staged_cs, tmp_cs,\n\t\t\t\t\t&cs->staged_cs_node, staged_cs_node)\n\t\t\t\tstaged_cs_put(hdev, staged_cs);\n\t\t}\n\n\t\t \n\t\tif (cs->submitted) {\n\t\t\tspin_lock(&hdev->cs_mirror_lock);\n\t\t\tlist_del(&cs->staged_cs_node);\n\t\t\tspin_unlock(&hdev->cs_mirror_lock);\n\t\t}\n\n\t\t \n\t\tif (hl_cs_cmpl->encaps_signals)\n\t\t\tkref_put(&hl_cs_cmpl->encaps_sig_hdl->refcount,\n\t\t\t\t\thl_encaps_release_handle_and_put_ctx);\n\t}\n\n\tif ((cs->type == CS_TYPE_WAIT || cs->type == CS_TYPE_COLLECTIVE_WAIT) && cs->encaps_signals)\n\t\tkref_put(&cs->encaps_sig_hdl->refcount, hl_encaps_release_handle_and_put_ctx);\n\nout:\n\t \n\thl_debugfs_remove_cs(cs);\n\n\thdev->shadow_cs_queue[cs->sequence & (hdev->asic_prop.max_pending_cs - 1)] = NULL;\n\n\t \n\tif (cs->timedout)\n\t\tcs->fence->error = -ETIMEDOUT;\n\telse if (cs->aborted)\n\t\tcs->fence->error = -EIO;\n\telse if (!cs->submitted)\n\t\tcs->fence->error = -EBUSY;\n\n\tif (unlikely(cs->skip_reset_on_timeout)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Command submission %llu completed after %llu (s)\\n\",\n\t\t\tcs->sequence,\n\t\t\tdiv_u64(jiffies - cs->submission_time_jiffies, HZ));\n\t}\n\n\tif (cs->timestamp) {\n\t\tcs->fence->timestamp = cs->completion_timestamp;\n\t\thl_push_cs_outcome(hdev, &cs->ctx->outcome_store, cs->sequence,\n\t\t\t\t   cs->fence->timestamp, cs->fence->error);\n\t}\n\n\thl_ctx_put(cs->ctx);\n\n\tcomplete_all(&cs->fence->completion);\n\tcomplete_multi_cs(hdev, cs);\n\n\tcs_release_sob_reset_handler(hdev, cs, hl_cs_cmpl);\n\n\thl_fence_put(cs->fence);\n\n\tkfree(cs->jobs_in_queue_cnt);\n\tkfree(cs);\n}\n\nstatic void cs_timedout(struct work_struct *work)\n{\n\tstruct hl_cs *cs = container_of(work, struct hl_cs, work_tdr.work);\n\tbool skip_reset_on_timeout, device_reset = false;\n\tstruct hl_device *hdev;\n\tu64 event_mask = 0x0;\n\tuint timeout_sec;\n\tint rc;\n\n\tskip_reset_on_timeout = cs->skip_reset_on_timeout;\n\n\trc = cs_get_unless_zero(cs);\n\tif (!rc)\n\t\treturn;\n\n\tif ((!cs->submitted) || (cs->completed)) {\n\t\tcs_put(cs);\n\t\treturn;\n\t}\n\n\thdev = cs->ctx->hdev;\n\n\tif (likely(!skip_reset_on_timeout)) {\n\t\tif (hdev->reset_on_lockup)\n\t\t\tdevice_reset = true;\n\t\telse\n\t\t\thdev->reset_info.needs_reset = true;\n\n\t\t \n\t\tcs->timedout = true;\n\t}\n\n\t \n\trc = atomic_cmpxchg(&hdev->captured_err_info.cs_timeout.write_enable, 1, 0);\n\tif (rc) {\n\t\thdev->captured_err_info.cs_timeout.timestamp = ktime_get();\n\t\thdev->captured_err_info.cs_timeout.seq = cs->sequence;\n\t\tevent_mask |= HL_NOTIFIER_EVENT_CS_TIMEOUT;\n\t}\n\n\ttimeout_sec = jiffies_to_msecs(hdev->timeout_jiffies) / 1000;\n\n\tswitch (cs->type) {\n\tcase CS_TYPE_SIGNAL:\n\t\tdev_err(hdev->dev,\n\t\t\t\"Signal command submission %llu has not finished in %u seconds!\\n\",\n\t\t\tcs->sequence, timeout_sec);\n\t\tbreak;\n\n\tcase CS_TYPE_WAIT:\n\t\tdev_err(hdev->dev,\n\t\t\t\"Wait command submission %llu has not finished in %u seconds!\\n\",\n\t\t\tcs->sequence, timeout_sec);\n\t\tbreak;\n\n\tcase CS_TYPE_COLLECTIVE_WAIT:\n\t\tdev_err(hdev->dev,\n\t\t\t\"Collective Wait command submission %llu has not finished in %u seconds!\\n\",\n\t\t\tcs->sequence, timeout_sec);\n\t\tbreak;\n\n\tdefault:\n\t\tdev_err(hdev->dev,\n\t\t\t\"Command submission %llu has not finished in %u seconds!\\n\",\n\t\t\tcs->sequence, timeout_sec);\n\t\tbreak;\n\t}\n\n\trc = hl_state_dump(hdev);\n\tif (rc)\n\t\tdev_err(hdev->dev, \"Error during system state dump %d\\n\", rc);\n\n\tcs_put(cs);\n\n\tif (device_reset) {\n\t\tevent_mask |= HL_NOTIFIER_EVENT_DEVICE_RESET;\n\t\thl_device_cond_reset(hdev, HL_DRV_RESET_TDR, event_mask);\n\t} else if (event_mask) {\n\t\thl_notifier_event_send_all(hdev, event_mask);\n\t}\n}\n\nstatic int allocate_cs(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\t\tenum hl_cs_type cs_type, u64 user_sequence,\n\t\t\tstruct hl_cs **cs_new, u32 flags, u32 timeout)\n{\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_fence *other = NULL;\n\tstruct hl_cs_compl *cs_cmpl;\n\tstruct hl_cs *cs;\n\tint rc;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\n\tcs = kzalloc(sizeof(*cs), GFP_ATOMIC);\n\tif (!cs)\n\t\tcs = kzalloc(sizeof(*cs), GFP_KERNEL);\n\n\tif (!cs) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\thl_ctx_get(ctx);\n\n\tcs->ctx = ctx;\n\tcs->submitted = false;\n\tcs->completed = false;\n\tcs->type = cs_type;\n\tcs->timestamp = !!(flags & HL_CS_FLAGS_TIMESTAMP);\n\tcs->encaps_signals = !!(flags & HL_CS_FLAGS_ENCAP_SIGNALS);\n\tcs->timeout_jiffies = timeout;\n\tcs->skip_reset_on_timeout =\n\t\thdev->reset_info.skip_reset_on_timeout ||\n\t\t!!(flags & HL_CS_FLAGS_SKIP_RESET_ON_TIMEOUT);\n\tcs->submission_time_jiffies = jiffies;\n\tINIT_LIST_HEAD(&cs->job_list);\n\tINIT_DELAYED_WORK(&cs->work_tdr, cs_timedout);\n\tkref_init(&cs->refcount);\n\tspin_lock_init(&cs->job_lock);\n\n\tcs_cmpl = kzalloc(sizeof(*cs_cmpl), GFP_ATOMIC);\n\tif (!cs_cmpl)\n\t\tcs_cmpl = kzalloc(sizeof(*cs_cmpl), GFP_KERNEL);\n\n\tif (!cs_cmpl) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\trc = -ENOMEM;\n\t\tgoto free_cs;\n\t}\n\n\tcs->jobs_in_queue_cnt = kcalloc(hdev->asic_prop.max_queues,\n\t\t\tsizeof(*cs->jobs_in_queue_cnt), GFP_ATOMIC);\n\tif (!cs->jobs_in_queue_cnt)\n\t\tcs->jobs_in_queue_cnt = kcalloc(hdev->asic_prop.max_queues,\n\t\t\t\tsizeof(*cs->jobs_in_queue_cnt), GFP_KERNEL);\n\n\tif (!cs->jobs_in_queue_cnt) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\trc = -ENOMEM;\n\t\tgoto free_cs_cmpl;\n\t}\n\n\tcs_cmpl->hdev = hdev;\n\tcs_cmpl->type = cs->type;\n\tspin_lock_init(&cs_cmpl->lock);\n\tcs->fence = &cs_cmpl->base_fence;\n\n\tspin_lock(&ctx->cs_lock);\n\n\tcs_cmpl->cs_seq = ctx->cs_sequence;\n\tother = ctx->cs_pending[cs_cmpl->cs_seq &\n\t\t\t\t(hdev->asic_prop.max_pending_cs - 1)];\n\n\tif (other && !completion_done(&other->completion)) {\n\t\t \n\t\tif (other->cs_sequence == user_sequence)\n\t\t\tdev_crit_ratelimited(hdev->dev,\n\t\t\t\t\"Staged CS %llu deadlock due to lack of resources\",\n\t\t\t\tuser_sequence);\n\n\t\tdev_dbg_ratelimited(hdev->dev,\n\t\t\t\"Rejecting CS because of too many in-flights CS\\n\");\n\t\tatomic64_inc(&ctx->cs_counters.max_cs_in_flight_drop_cnt);\n\t\tatomic64_inc(&cntr->max_cs_in_flight_drop_cnt);\n\t\trc = -EAGAIN;\n\t\tgoto free_fence;\n\t}\n\n\t \n\thl_fence_init(&cs_cmpl->base_fence, cs_cmpl->cs_seq);\n\n\tcs->sequence = cs_cmpl->cs_seq;\n\n\tctx->cs_pending[cs_cmpl->cs_seq &\n\t\t\t(hdev->asic_prop.max_pending_cs - 1)] =\n\t\t\t\t\t\t\t&cs_cmpl->base_fence;\n\tctx->cs_sequence++;\n\n\thl_fence_get(&cs_cmpl->base_fence);\n\n\thl_fence_put(other);\n\n\tspin_unlock(&ctx->cs_lock);\n\n\t*cs_new = cs;\n\n\treturn 0;\n\nfree_fence:\n\tspin_unlock(&ctx->cs_lock);\n\tkfree(cs->jobs_in_queue_cnt);\nfree_cs_cmpl:\n\tkfree(cs_cmpl);\nfree_cs:\n\tkfree(cs);\n\thl_ctx_put(ctx);\n\treturn rc;\n}\n\nstatic void cs_rollback(struct hl_device *hdev, struct hl_cs *cs)\n{\n\tstruct hl_cs_job *job, *tmp;\n\n\tstaged_cs_put(hdev, cs);\n\n\tlist_for_each_entry_safe(job, tmp, &cs->job_list, cs_node)\n\t\thl_complete_job(hdev, job);\n}\n\n \nstatic void release_reserved_encaps_signals(struct hl_device *hdev)\n{\n\tstruct hl_ctx *ctx = hl_get_compute_ctx(hdev);\n\tstruct hl_cs_encaps_sig_handle *handle;\n\tstruct hl_encaps_signals_mgr *mgr;\n\tu32 id;\n\n\tif (!ctx)\n\t\treturn;\n\n\tmgr = &ctx->sig_mgr;\n\n\tidr_for_each_entry(&mgr->handles, handle, id)\n\t\tif (handle->cs_seq == ULLONG_MAX)\n\t\t\tkref_put(&handle->refcount, hl_encaps_release_handle_and_put_sob_ctx);\n\n\thl_ctx_put(ctx);\n}\n\nvoid hl_cs_rollback_all(struct hl_device *hdev, bool skip_wq_flush)\n{\n\tint i;\n\tstruct hl_cs *cs, *tmp;\n\n\tif (!skip_wq_flush) {\n\t\tflush_workqueue(hdev->ts_free_obj_wq);\n\n\t\t \n\t\tfor (i = 0 ; i < hdev->asic_prop.completion_queues_count ; i++)\n\t\t\tflush_workqueue(hdev->cq_wq[i]);\n\n\t\tflush_workqueue(hdev->cs_cmplt_wq);\n\t}\n\n\t \n\tlist_for_each_entry_safe(cs, tmp, &hdev->cs_mirror_list, mirror_node) {\n\t\tcs_get(cs);\n\t\tcs->aborted = true;\n\t\tdev_warn_ratelimited(hdev->dev, \"Killing CS %d.%llu\\n\",\n\t\t\t\t\tcs->ctx->asid, cs->sequence);\n\t\tcs_rollback(hdev, cs);\n\t\tcs_put(cs);\n\t}\n\n\tforce_complete_multi_cs(hdev);\n\n\trelease_reserved_encaps_signals(hdev);\n}\n\nstatic void\nwake_pending_user_interrupt_threads(struct hl_user_interrupt *interrupt)\n{\n\tstruct hl_user_pending_interrupt *pend, *temp;\n\n\tspin_lock(&interrupt->wait_list_lock);\n\tlist_for_each_entry_safe(pend, temp, &interrupt->wait_list_head, wait_list_node) {\n\t\tif (pend->ts_reg_info.buf) {\n\t\t\tlist_del(&pend->wait_list_node);\n\t\t\thl_mmap_mem_buf_put(pend->ts_reg_info.buf);\n\t\t\thl_cb_put(pend->ts_reg_info.cq_cb);\n\t\t} else {\n\t\t\tpend->fence.error = -EIO;\n\t\t\tcomplete_all(&pend->fence.completion);\n\t\t}\n\t}\n\tspin_unlock(&interrupt->wait_list_lock);\n}\n\nvoid hl_release_pending_user_interrupts(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_user_interrupt *interrupt;\n\tint i;\n\n\tif (!prop->user_interrupt_count)\n\t\treturn;\n\n\t \n\n\tfor (i = 0 ; i < prop->user_interrupt_count ; i++) {\n\t\tinterrupt = &hdev->user_interrupt[i];\n\t\twake_pending_user_interrupt_threads(interrupt);\n\t}\n\n\tinterrupt = &hdev->common_user_cq_interrupt;\n\twake_pending_user_interrupt_threads(interrupt);\n\n\tinterrupt = &hdev->common_decoder_interrupt;\n\twake_pending_user_interrupt_threads(interrupt);\n}\n\nstatic void force_complete_cs(struct hl_device *hdev)\n{\n\tstruct hl_cs *cs;\n\n\tspin_lock(&hdev->cs_mirror_lock);\n\n\tlist_for_each_entry(cs, &hdev->cs_mirror_list, mirror_node) {\n\t\tcs->fence->error = -EIO;\n\t\tcomplete_all(&cs->fence->completion);\n\t}\n\n\tspin_unlock(&hdev->cs_mirror_lock);\n}\n\nvoid hl_abort_waiting_for_cs_completions(struct hl_device *hdev)\n{\n\tforce_complete_cs(hdev);\n\tforce_complete_multi_cs(hdev);\n}\n\nstatic void job_wq_completion(struct work_struct *work)\n{\n\tstruct hl_cs_job *job = container_of(work, struct hl_cs_job,\n\t\t\t\t\t\tfinish_work);\n\tstruct hl_cs *cs = job->cs;\n\tstruct hl_device *hdev = cs->ctx->hdev;\n\n\t \n\thl_complete_job(hdev, job);\n}\n\nstatic void cs_completion(struct work_struct *work)\n{\n\tstruct hl_cs *cs = container_of(work, struct hl_cs, finish_work);\n\tstruct hl_device *hdev = cs->ctx->hdev;\n\tstruct hl_cs_job *job, *tmp;\n\n\tlist_for_each_entry_safe(job, tmp, &cs->job_list, cs_node)\n\t\thl_complete_job(hdev, job);\n}\n\nu32 hl_get_active_cs_num(struct hl_device *hdev)\n{\n\tu32 active_cs_num = 0;\n\tstruct hl_cs *cs;\n\n\tspin_lock(&hdev->cs_mirror_lock);\n\n\tlist_for_each_entry(cs, &hdev->cs_mirror_list, mirror_node)\n\t\tif (!cs->completed)\n\t\t\tactive_cs_num++;\n\n\tspin_unlock(&hdev->cs_mirror_lock);\n\n\treturn active_cs_num;\n}\n\nstatic int validate_queue_index(struct hl_device *hdev,\n\t\t\t\tstruct hl_cs_chunk *chunk,\n\t\t\t\tenum hl_queue_type *queue_type,\n\t\t\t\tbool *is_kernel_allocated_cb)\n{\n\tstruct asic_fixed_properties *asic = &hdev->asic_prop;\n\tstruct hw_queue_properties *hw_queue_prop;\n\n\t \n\tif (chunk->queue_index >= asic->max_queues) {\n\t\tdev_err(hdev->dev, \"Queue index %d is invalid\\n\",\n\t\t\tchunk->queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\thw_queue_prop = &asic->hw_queues_props[chunk->queue_index];\n\n\tif (hw_queue_prop->type == QUEUE_TYPE_NA) {\n\t\tdev_err(hdev->dev, \"Queue index %d is not applicable\\n\",\n\t\t\tchunk->queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\tif (hw_queue_prop->binned) {\n\t\tdev_err(hdev->dev, \"Queue index %d is binned out\\n\",\n\t\t\tchunk->queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\tif (hw_queue_prop->driver_only) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Queue index %d is restricted for the kernel driver\\n\",\n\t\t\tchunk->queue_index);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (hw_queue_prop->type == QUEUE_TYPE_HW) {\n\t\tif (chunk->cs_chunk_flags & HL_CS_CHUNK_FLAGS_USER_ALLOC_CB) {\n\t\t\tif (!(hw_queue_prop->cb_alloc_flags & CB_ALLOC_USER)) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"Queue index %d doesn't support user CB\\n\",\n\t\t\t\t\tchunk->queue_index);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t*is_kernel_allocated_cb = false;\n\t\t} else {\n\t\t\tif (!(hw_queue_prop->cb_alloc_flags &\n\t\t\t\t\tCB_ALLOC_KERNEL)) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"Queue index %d doesn't support kernel CB\\n\",\n\t\t\t\t\tchunk->queue_index);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t*is_kernel_allocated_cb = true;\n\t\t}\n\t} else {\n\t\t*is_kernel_allocated_cb = !!(hw_queue_prop->cb_alloc_flags\n\t\t\t\t\t\t& CB_ALLOC_KERNEL);\n\t}\n\n\t*queue_type = hw_queue_prop->type;\n\treturn 0;\n}\n\nstatic struct hl_cb *get_cb_from_cs_chunk(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_mem_mgr *mmg,\n\t\t\t\t\tstruct hl_cs_chunk *chunk)\n{\n\tstruct hl_cb *cb;\n\n\tcb = hl_cb_get(mmg, chunk->cb_handle);\n\tif (!cb) {\n\t\tdev_err(hdev->dev, \"CB handle 0x%llx invalid\\n\", chunk->cb_handle);\n\t\treturn NULL;\n\t}\n\n\tif ((chunk->cb_size < 8) || (chunk->cb_size > cb->size)) {\n\t\tdev_err(hdev->dev, \"CB size %u invalid\\n\", chunk->cb_size);\n\t\tgoto release_cb;\n\t}\n\n\tatomic_inc(&cb->cs_cnt);\n\n\treturn cb;\n\nrelease_cb:\n\thl_cb_put(cb);\n\treturn NULL;\n}\n\nstruct hl_cs_job *hl_cs_allocate_job(struct hl_device *hdev,\n\t\tenum hl_queue_type queue_type, bool is_kernel_allocated_cb)\n{\n\tstruct hl_cs_job *job;\n\n\tjob = kzalloc(sizeof(*job), GFP_ATOMIC);\n\tif (!job)\n\t\tjob = kzalloc(sizeof(*job), GFP_KERNEL);\n\n\tif (!job)\n\t\treturn NULL;\n\n\tkref_init(&job->refcount);\n\tjob->queue_type = queue_type;\n\tjob->is_kernel_allocated_cb = is_kernel_allocated_cb;\n\n\tif (is_cb_patched(hdev, job))\n\t\tINIT_LIST_HEAD(&job->userptr_list);\n\n\tif (job->queue_type == QUEUE_TYPE_EXT)\n\t\tINIT_WORK(&job->finish_work, job_wq_completion);\n\n\treturn job;\n}\n\nstatic enum hl_cs_type hl_cs_get_cs_type(u32 cs_type_flags)\n{\n\tif (cs_type_flags & HL_CS_FLAGS_SIGNAL)\n\t\treturn CS_TYPE_SIGNAL;\n\telse if (cs_type_flags & HL_CS_FLAGS_WAIT)\n\t\treturn CS_TYPE_WAIT;\n\telse if (cs_type_flags & HL_CS_FLAGS_COLLECTIVE_WAIT)\n\t\treturn CS_TYPE_COLLECTIVE_WAIT;\n\telse if (cs_type_flags & HL_CS_FLAGS_RESERVE_SIGNALS_ONLY)\n\t\treturn CS_RESERVE_SIGNALS;\n\telse if (cs_type_flags & HL_CS_FLAGS_UNRESERVE_SIGNALS_ONLY)\n\t\treturn CS_UNRESERVE_SIGNALS;\n\telse if (cs_type_flags & HL_CS_FLAGS_ENGINE_CORE_COMMAND)\n\t\treturn CS_TYPE_ENGINE_CORE;\n\telse if (cs_type_flags & HL_CS_FLAGS_ENGINES_COMMAND)\n\t\treturn CS_TYPE_ENGINES;\n\telse if (cs_type_flags & HL_CS_FLAGS_FLUSH_PCI_HBW_WRITES)\n\t\treturn CS_TYPE_FLUSH_PCI_HBW_WRITES;\n\telse\n\t\treturn CS_TYPE_DEFAULT;\n}\n\nstatic int hl_cs_sanity_checks(struct hl_fpriv *hpriv, union hl_cs_args *args)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tu32 cs_type_flags, num_chunks;\n\tenum hl_device_status status;\n\tenum hl_cs_type cs_type;\n\tbool is_sync_stream;\n\tint i;\n\n\tfor (i = 0 ; i < sizeof(args->in.pad) ; i++)\n\t\tif (args->in.pad[i]) {\n\t\t\tdev_dbg(hdev->dev, \"Padding bytes must be 0\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\tif (!hl_device_operational(hdev, &status)) {\n\t\treturn -EBUSY;\n\t}\n\n\tif ((args->in.cs_flags & HL_CS_FLAGS_STAGED_SUBMISSION) &&\n\t\t\t!hdev->supports_staged_submission) {\n\t\tdev_err(hdev->dev, \"staged submission not supported\");\n\t\treturn -EPERM;\n\t}\n\n\tcs_type_flags = args->in.cs_flags & HL_CS_FLAGS_TYPE_MASK;\n\n\tif (unlikely(cs_type_flags && !is_power_of_2(cs_type_flags))) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"CS type flags are mutually exclusive, context %d\\n\",\n\t\t\tctx->asid);\n\t\treturn -EINVAL;\n\t}\n\n\tcs_type = hl_cs_get_cs_type(cs_type_flags);\n\tnum_chunks = args->in.num_chunks_execute;\n\n\tis_sync_stream = (cs_type == CS_TYPE_SIGNAL || cs_type == CS_TYPE_WAIT ||\n\t\t\tcs_type == CS_TYPE_COLLECTIVE_WAIT);\n\n\tif (unlikely(is_sync_stream && !hdev->supports_sync_stream)) {\n\t\tdev_err(hdev->dev, \"Sync stream CS is not supported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cs_type == CS_TYPE_DEFAULT) {\n\t\tif (!num_chunks) {\n\t\t\tdev_err(hdev->dev, \"Got execute CS with 0 chunks, context %d\\n\", ctx->asid);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (is_sync_stream && num_chunks != 1) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Sync stream CS mandates one chunk only, context %d\\n\",\n\t\t\tctx->asid);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int hl_cs_copy_chunk_array(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_cs_chunk **cs_chunk_array,\n\t\t\t\t\tvoid __user *chunks, u32 num_chunks,\n\t\t\t\t\tstruct hl_ctx *ctx)\n{\n\tu32 size_to_copy;\n\n\tif (num_chunks > HL_MAX_JOBS_PER_CS) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.validation_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"Number of chunks can NOT be larger than %d\\n\",\n\t\t\tHL_MAX_JOBS_PER_CS);\n\t\treturn -EINVAL;\n\t}\n\n\t*cs_chunk_array = kmalloc_array(num_chunks, sizeof(**cs_chunk_array),\n\t\t\t\t\tGFP_ATOMIC);\n\tif (!*cs_chunk_array)\n\t\t*cs_chunk_array = kmalloc_array(num_chunks,\n\t\t\t\t\tsizeof(**cs_chunk_array), GFP_KERNEL);\n\tif (!*cs_chunk_array) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.out_of_mem_drop_cnt);\n\t\treturn -ENOMEM;\n\t}\n\n\tsize_to_copy = num_chunks * sizeof(struct hl_cs_chunk);\n\tif (copy_from_user(*cs_chunk_array, chunks, size_to_copy)) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.validation_drop_cnt);\n\t\tdev_err(hdev->dev, \"Failed to copy cs chunk array from user\\n\");\n\t\tkfree(*cs_chunk_array);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int cs_staged_submission(struct hl_device *hdev, struct hl_cs *cs,\n\t\t\t\tu64 sequence, u32 flags,\n\t\t\t\tu32 encaps_signal_handle)\n{\n\tif (!(flags & HL_CS_FLAGS_STAGED_SUBMISSION))\n\t\treturn 0;\n\n\tcs->staged_last = !!(flags & HL_CS_FLAGS_STAGED_SUBMISSION_LAST);\n\tcs->staged_first = !!(flags & HL_CS_FLAGS_STAGED_SUBMISSION_FIRST);\n\n\tif (cs->staged_first) {\n\t\t \n\t\tINIT_LIST_HEAD(&cs->staged_cs_node);\n\t\tcs->staged_sequence = cs->sequence;\n\n\t\tif (cs->encaps_signals)\n\t\t\tcs->encaps_sig_hdl_id = encaps_signal_handle;\n\t} else {\n\t\t \n\t\tcs->staged_sequence = sequence;\n\t}\n\n\t \n\tstaged_cs_get(hdev, cs);\n\n\tcs->staged_cs = true;\n\n\treturn 0;\n}\n\nstatic u32 get_stream_master_qid_mask(struct hl_device *hdev, u32 qid)\n{\n\tint i;\n\n\tfor (i = 0; i < hdev->stream_master_qid_arr_size; i++)\n\t\tif (qid == hdev->stream_master_qid_arr[i])\n\t\t\treturn BIT(i);\n\n\treturn 0;\n}\n\nstatic int cs_ioctl_default(struct hl_fpriv *hpriv, void __user *chunks,\n\t\t\t\tu32 num_chunks, u64 *cs_seq, u32 flags,\n\t\t\t\tu32 encaps_signals_handle, u32 timeout,\n\t\t\t\tu16 *signal_initial_sob_count)\n{\n\tbool staged_mid, int_queues_only = true, using_hw_queues = false;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_cs_chunk *cs_chunk_array;\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tstruct hl_cs_job *job;\n\tstruct hl_cs *cs;\n\tstruct hl_cb *cb;\n\tu64 user_sequence;\n\tu8 stream_master_qid_map = 0;\n\tint rc, i;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\tuser_sequence = *cs_seq;\n\t*cs_seq = ULLONG_MAX;\n\n\trc = hl_cs_copy_chunk_array(hdev, &cs_chunk_array, chunks, num_chunks,\n\t\t\thpriv->ctx);\n\tif (rc)\n\t\tgoto out;\n\n\tif ((flags & HL_CS_FLAGS_STAGED_SUBMISSION) &&\n\t\t\t!(flags & HL_CS_FLAGS_STAGED_SUBMISSION_FIRST))\n\t\tstaged_mid = true;\n\telse\n\t\tstaged_mid = false;\n\n\trc = allocate_cs(hdev, hpriv->ctx, CS_TYPE_DEFAULT,\n\t\t\tstaged_mid ? user_sequence : ULLONG_MAX, &cs, flags,\n\t\t\ttimeout);\n\tif (rc)\n\t\tgoto free_cs_chunk_array;\n\n\t*cs_seq = cs->sequence;\n\n\thl_debugfs_add_cs(cs);\n\n\trc = cs_staged_submission(hdev, cs, user_sequence, flags,\n\t\t\t\t\t\tencaps_signals_handle);\n\tif (rc)\n\t\tgoto free_cs_object;\n\n\t \n\tif (cs->staged_cs)\n\t\t*cs_seq = cs->staged_sequence;\n\n\t \n\tfor (i = 0 ; i < num_chunks ; i++) {\n\t\tstruct hl_cs_chunk *chunk = &cs_chunk_array[i];\n\t\tenum hl_queue_type queue_type;\n\t\tbool is_kernel_allocated_cb;\n\n\t\trc = validate_queue_index(hdev, chunk, &queue_type,\n\t\t\t\t\t\t&is_kernel_allocated_cb);\n\t\tif (rc) {\n\t\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\tgoto free_cs_object;\n\t\t}\n\n\t\tif (is_kernel_allocated_cb) {\n\t\t\tcb = get_cb_from_cs_chunk(hdev, &hpriv->mem_mgr, chunk);\n\t\t\tif (!cb) {\n\t\t\t\tatomic64_inc(\n\t\t\t\t\t&ctx->cs_counters.validation_drop_cnt);\n\t\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto free_cs_object;\n\t\t\t}\n\t\t} else {\n\t\t\tcb = (struct hl_cb *) (uintptr_t) chunk->cb_handle;\n\t\t}\n\n\t\tif (queue_type == QUEUE_TYPE_EXT ||\n\t\t\t\t\t\tqueue_type == QUEUE_TYPE_HW) {\n\t\t\tint_queues_only = false;\n\n\t\t\t \n\t\t\tif (hdev->supports_wait_for_multi_cs)\n\t\t\t\tstream_master_qid_map |=\n\t\t\t\t\tget_stream_master_qid_mask(hdev,\n\t\t\t\t\t\t\tchunk->queue_index);\n\t\t}\n\n\t\tif (queue_type == QUEUE_TYPE_HW)\n\t\t\tusing_hw_queues = true;\n\n\t\tjob = hl_cs_allocate_job(hdev, queue_type,\n\t\t\t\t\t\tis_kernel_allocated_cb);\n\t\tif (!job) {\n\t\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\t\trc = -ENOMEM;\n\t\t\tif (is_kernel_allocated_cb)\n\t\t\t\tgoto release_cb;\n\n\t\t\tgoto free_cs_object;\n\t\t}\n\n\t\tjob->id = i + 1;\n\t\tjob->cs = cs;\n\t\tjob->user_cb = cb;\n\t\tjob->user_cb_size = chunk->cb_size;\n\t\tjob->hw_queue_id = chunk->queue_index;\n\n\t\tcs->jobs_in_queue_cnt[job->hw_queue_id]++;\n\t\tcs->jobs_cnt++;\n\n\t\tlist_add_tail(&job->cs_node, &cs->job_list);\n\n\t\t \n\t\tif (cs_needs_completion(cs) &&\n\t\t\t(job->queue_type == QUEUE_TYPE_EXT ||\n\t\t\t\tjob->queue_type == QUEUE_TYPE_HW))\n\t\t\tcs_get(cs);\n\n\t\thl_debugfs_add_job(hdev, job);\n\n\t\trc = cs_parser(hpriv, job);\n\t\tif (rc) {\n\t\t\tatomic64_inc(&ctx->cs_counters.parsing_drop_cnt);\n\t\t\tatomic64_inc(&cntr->parsing_drop_cnt);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to parse JOB %d.%llu.%d, err %d, rejecting the CS\\n\",\n\t\t\t\tcs->ctx->asid, cs->sequence, job->id, rc);\n\t\t\tgoto free_cs_object;\n\t\t}\n\t}\n\n\t \n\tif (int_queues_only && cs_needs_completion(cs)) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"Reject CS %d.%llu since it contains only internal queues jobs and needs completion\\n\",\n\t\t\tcs->ctx->asid, cs->sequence);\n\t\trc = -EINVAL;\n\t\tgoto free_cs_object;\n\t}\n\n\tif (using_hw_queues)\n\t\tINIT_WORK(&cs->finish_work, cs_completion);\n\n\t \n\tif (hdev->supports_wait_for_multi_cs)\n\t\tcs->fence->stream_master_qid_map = stream_master_qid_map;\n\n\trc = hl_hw_queue_schedule_cs(cs);\n\tif (rc) {\n\t\tif (rc != -EAGAIN)\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to submit CS %d.%llu to H/W queues, error %d\\n\",\n\t\t\t\tcs->ctx->asid, cs->sequence, rc);\n\t\tgoto free_cs_object;\n\t}\n\n\t*signal_initial_sob_count = cs->initial_sob_count;\n\n\trc = HL_CS_STATUS_SUCCESS;\n\tgoto put_cs;\n\nrelease_cb:\n\tatomic_dec(&cb->cs_cnt);\n\thl_cb_put(cb);\nfree_cs_object:\n\tcs_rollback(hdev, cs);\n\t*cs_seq = ULLONG_MAX;\n\t \nput_cs:\n\t \n\tcs_put(cs);\nfree_cs_chunk_array:\n\tkfree(cs_chunk_array);\nout:\n\treturn rc;\n}\n\nstatic int hl_cs_ctx_switch(struct hl_fpriv *hpriv, union hl_cs_args *args,\n\t\t\t\tu64 *cs_seq)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tbool need_soft_reset = false;\n\tint rc = 0, do_ctx_switch = 0;\n\tvoid __user *chunks;\n\tu32 num_chunks, tmp;\n\tu16 sob_count;\n\tint ret;\n\n\tif (hdev->supports_ctx_switch)\n\t\tdo_ctx_switch = atomic_cmpxchg(&ctx->thread_ctx_switch_token, 1, 0);\n\n\tif (do_ctx_switch || (args->in.cs_flags & HL_CS_FLAGS_FORCE_RESTORE)) {\n\t\tmutex_lock(&hpriv->restore_phase_mutex);\n\n\t\tif (do_ctx_switch) {\n\t\t\trc = hdev->asic_funcs->context_switch(hdev, ctx->asid);\n\t\t\tif (rc) {\n\t\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\"Failed to switch to context %d, rejecting CS! %d\\n\",\n\t\t\t\t\tctx->asid, rc);\n\t\t\t\t \n\t\t\t\tif ((rc == -ETIMEDOUT) || (rc == -EBUSY))\n\t\t\t\t\tneed_soft_reset = true;\n\t\t\t\tmutex_unlock(&hpriv->restore_phase_mutex);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\thdev->asic_funcs->restore_phase_topology(hdev);\n\n\t\tchunks = (void __user *) (uintptr_t) args->in.chunks_restore;\n\t\tnum_chunks = args->in.num_chunks_restore;\n\n\t\tif (!num_chunks) {\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"Need to run restore phase but restore CS is empty\\n\");\n\t\t\trc = 0;\n\t\t} else {\n\t\t\trc = cs_ioctl_default(hpriv, chunks, num_chunks,\n\t\t\t\t\tcs_seq, 0, 0, hdev->timeout_jiffies, &sob_count);\n\t\t}\n\n\t\tmutex_unlock(&hpriv->restore_phase_mutex);\n\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to submit restore CS for context %d (%d)\\n\",\n\t\t\t\tctx->asid, rc);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (num_chunks) {\n\t\t\tenum hl_cs_wait_status status;\nwait_again:\n\t\t\tret = _hl_cs_wait_ioctl(hdev, ctx,\n\t\t\t\t\tjiffies_to_usecs(hdev->timeout_jiffies),\n\t\t\t\t\t*cs_seq, &status, NULL);\n\t\t\tif (ret) {\n\t\t\t\tif (ret == -ERESTARTSYS) {\n\t\t\t\t\tusleep_range(100, 200);\n\t\t\t\t\tgoto wait_again;\n\t\t\t\t}\n\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"Restore CS for context %d failed to complete %d\\n\",\n\t\t\t\t\tctx->asid, ret);\n\t\t\t\trc = -ENOEXEC;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tif (hdev->supports_ctx_switch)\n\t\t\tctx->thread_ctx_switch_wait_token = 1;\n\n\t} else if (hdev->supports_ctx_switch && !ctx->thread_ctx_switch_wait_token) {\n\t\trc = hl_poll_timeout_memory(hdev,\n\t\t\t&ctx->thread_ctx_switch_wait_token, tmp, (tmp == 1),\n\t\t\t100, jiffies_to_usecs(hdev->timeout_jiffies), false);\n\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"context switch phase timeout (%d)\\n\", tmp);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\tif ((rc == -ETIMEDOUT || rc == -EBUSY) && (need_soft_reset))\n\t\thl_device_reset(hdev, 0);\n\n\treturn rc;\n}\n\n \nint hl_cs_signal_sob_wraparound_handler(struct hl_device *hdev, u32 q_idx,\n\t\t\tstruct hl_hw_sob **hw_sob, u32 count, bool encaps_sig)\n\n{\n\tstruct hl_sync_stream_properties *prop;\n\tstruct hl_hw_sob *sob = *hw_sob, *other_sob;\n\tu8 other_sob_offset;\n\n\tprop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\n\thw_sob_get(sob);\n\n\t \n\tif (prop->next_sob_val + count >= HL_MAX_SOB_VAL) {\n\t\t \n\t\thw_sob_put_err(sob);\n\n\t\t \n\t\tother_sob_offset = (prop->curr_sob_offset + 1) % HL_RSVD_SOBS;\n\t\tother_sob = &prop->hw_sob[other_sob_offset];\n\n\t\tif (kref_read(&other_sob->kref) != 1) {\n\t\t\tdev_err(hdev->dev, \"error: Cannot switch SOBs q_idx: %d\\n\",\n\t\t\t\t\t\t\t\tq_idx);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (encaps_sig)\n\t\t\tprop->next_sob_val = count + 1;\n\t\telse\n\t\t\tprop->next_sob_val = count;\n\n\t\t \n\t\tprop->curr_sob_offset = other_sob_offset;\n\t\t*hw_sob = other_sob;\n\n\t\t \n\t\tif (other_sob->need_reset)\n\t\t\thw_sob_put(other_sob);\n\n\t\tif (encaps_sig) {\n\t\t\t \n\t\t\tsob->need_reset = true;\n\t\t\thw_sob_get(other_sob);\n\t\t}\n\n\t\tdev_dbg(hdev->dev, \"switched to SOB %d, q_idx: %d\\n\",\n\t\t\t\tprop->curr_sob_offset, q_idx);\n\t} else {\n\t\tprop->next_sob_val += count;\n\t}\n\n\treturn 0;\n}\n\nstatic int cs_ioctl_extract_signal_seq(struct hl_device *hdev,\n\t\tstruct hl_cs_chunk *chunk, u64 *signal_seq, struct hl_ctx *ctx,\n\t\tbool encaps_signals)\n{\n\tu64 *signal_seq_arr = NULL;\n\tu32 size_to_copy, signal_seq_arr_len;\n\tint rc = 0;\n\n\tif (encaps_signals) {\n\t\t*signal_seq = chunk->encaps_signal_seq;\n\t\treturn 0;\n\t}\n\n\tsignal_seq_arr_len = chunk->num_signal_seq_arr;\n\n\t \n\tif (signal_seq_arr_len != 1) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.validation_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"Wait for signal CS supports only one signal CS seq\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsignal_seq_arr = kmalloc_array(signal_seq_arr_len,\n\t\t\t\t\tsizeof(*signal_seq_arr),\n\t\t\t\t\tGFP_ATOMIC);\n\tif (!signal_seq_arr)\n\t\tsignal_seq_arr = kmalloc_array(signal_seq_arr_len,\n\t\t\t\t\tsizeof(*signal_seq_arr),\n\t\t\t\t\tGFP_KERNEL);\n\tif (!signal_seq_arr) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.out_of_mem_drop_cnt);\n\t\treturn -ENOMEM;\n\t}\n\n\tsize_to_copy = signal_seq_arr_len * sizeof(*signal_seq_arr);\n\tif (copy_from_user(signal_seq_arr,\n\t\t\t\tu64_to_user_ptr(chunk->signal_seq_arr),\n\t\t\t\tsize_to_copy)) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&hdev->aggregated_cs_counters.validation_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to copy signal seq array from user\\n\");\n\t\trc = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\t*signal_seq = signal_seq_arr[0];\n\nout:\n\tkfree(signal_seq_arr);\n\n\treturn rc;\n}\n\nstatic int cs_ioctl_signal_wait_create_jobs(struct hl_device *hdev,\n\t\tstruct hl_ctx *ctx, struct hl_cs *cs,\n\t\tenum hl_queue_type q_type, u32 q_idx, u32 encaps_signal_offset)\n{\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_cs_job *job;\n\tstruct hl_cb *cb;\n\tu32 cb_size;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\n\tjob = hl_cs_allocate_job(hdev, q_type, true);\n\tif (!job) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\tdev_err(hdev->dev, \"Failed to allocate a new job\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (cs->type == CS_TYPE_WAIT)\n\t\tcb_size = hdev->asic_funcs->get_wait_cb_size(hdev);\n\telse\n\t\tcb_size = hdev->asic_funcs->get_signal_cb_size(hdev);\n\n\tcb = hl_cb_kernel_create(hdev, cb_size, q_type == QUEUE_TYPE_HW);\n\tif (!cb) {\n\t\tatomic64_inc(&ctx->cs_counters.out_of_mem_drop_cnt);\n\t\tatomic64_inc(&cntr->out_of_mem_drop_cnt);\n\t\tkfree(job);\n\t\treturn -EFAULT;\n\t}\n\n\tjob->id = 0;\n\tjob->cs = cs;\n\tjob->user_cb = cb;\n\tatomic_inc(&job->user_cb->cs_cnt);\n\tjob->user_cb_size = cb_size;\n\tjob->hw_queue_id = q_idx;\n\n\tif ((cs->type == CS_TYPE_WAIT || cs->type == CS_TYPE_COLLECTIVE_WAIT)\n\t\t\t&& cs->encaps_signals)\n\t\tjob->encaps_sig_wait_offset = encaps_signal_offset;\n\t \n\tjob->patched_cb = job->user_cb;\n\tjob->job_cb_size = job->user_cb_size;\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb->buf->handle);\n\n\t \n\tcs_get(cs);\n\n\tcs->jobs_in_queue_cnt[job->hw_queue_id]++;\n\tcs->jobs_cnt++;\n\n\tlist_add_tail(&job->cs_node, &cs->job_list);\n\n\thl_debugfs_add_job(hdev, job);\n\n\treturn 0;\n}\n\nstatic int cs_ioctl_reserve_signals(struct hl_fpriv *hpriv,\n\t\t\t\tu32 q_idx, u32 count,\n\t\t\t\tu32 *handle_id, u32 *sob_addr,\n\t\t\t\tu32 *signals_count)\n{\n\tstruct hw_queue_properties *hw_queue_prop;\n\tstruct hl_sync_stream_properties *prop;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_cs_encaps_sig_handle *handle;\n\tstruct hl_encaps_signals_mgr *mgr;\n\tstruct hl_hw_sob *hw_sob;\n\tint hdl_id;\n\tint rc = 0;\n\n\tif (count >= HL_MAX_SOB_VAL) {\n\t\tdev_err(hdev->dev, \"signals count(%u) exceeds the max SOB value\\n\",\n\t\t\t\t\t\tcount);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (q_idx >= hdev->asic_prop.max_queues) {\n\t\tdev_err(hdev->dev, \"Queue index %d is invalid\\n\",\n\t\t\tq_idx);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\thw_queue_prop = &hdev->asic_prop.hw_queues_props[q_idx];\n\n\tif (!hw_queue_prop->supports_sync_stream) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Queue index %d does not support sync stream operations\\n\",\n\t\t\t\t\t\t\t\t\tq_idx);\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tprop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\n\thandle = kzalloc(sizeof(*handle), GFP_KERNEL);\n\tif (!handle) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\thandle->count = count;\n\n\thl_ctx_get(hpriv->ctx);\n\thandle->ctx = hpriv->ctx;\n\tmgr = &hpriv->ctx->sig_mgr;\n\n\tspin_lock(&mgr->lock);\n\thdl_id = idr_alloc(&mgr->handles, handle, 1, 0, GFP_ATOMIC);\n\tspin_unlock(&mgr->lock);\n\n\tif (hdl_id < 0) {\n\t\tdev_err(hdev->dev, \"Failed to allocate IDR for a new signal reservation\\n\");\n\t\trc = -EINVAL;\n\t\tgoto put_ctx;\n\t}\n\n\thandle->id = hdl_id;\n\thandle->q_idx = q_idx;\n\thandle->hdev = hdev;\n\tkref_init(&handle->refcount);\n\n\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\thw_sob = &prop->hw_sob[prop->curr_sob_offset];\n\n\t \n\trc = hl_cs_signal_sob_wraparound_handler(hdev, q_idx, &hw_sob, count,\n\t\t\t\t\t\t\t\ttrue);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to switch SOB\\n\");\n\t\thdev->asic_funcs->hw_queues_unlock(hdev);\n\t\trc = -EINVAL;\n\t\tgoto remove_idr;\n\t}\n\t \n\thandle->hw_sob = hw_sob;\n\n\t \n\thandle->pre_sob_val = prop->next_sob_val - handle->count;\n\n\thandle->cs_seq = ULLONG_MAX;\n\n\t*signals_count = prop->next_sob_val;\n\thdev->asic_funcs->hw_queues_unlock(hdev);\n\n\t*sob_addr = handle->hw_sob->sob_addr;\n\t*handle_id = hdl_id;\n\n\tdev_dbg(hdev->dev,\n\t\t\"Signals reserved, sob_id: %d, sob addr: 0x%x, last sob_val: %u, q_idx: %d, hdl_id: %d\\n\",\n\t\t\thw_sob->sob_id, handle->hw_sob->sob_addr,\n\t\t\tprop->next_sob_val - 1, q_idx, hdl_id);\n\tgoto out;\n\nremove_idr:\n\tspin_lock(&mgr->lock);\n\tidr_remove(&mgr->handles, hdl_id);\n\tspin_unlock(&mgr->lock);\n\nput_ctx:\n\thl_ctx_put(handle->ctx);\n\tkfree(handle);\n\nout:\n\treturn rc;\n}\n\nstatic int cs_ioctl_unreserve_signals(struct hl_fpriv *hpriv, u32 handle_id)\n{\n\tstruct hl_cs_encaps_sig_handle *encaps_sig_hdl;\n\tstruct hl_sync_stream_properties *prop;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_encaps_signals_mgr *mgr;\n\tstruct hl_hw_sob *hw_sob;\n\tu32 q_idx, sob_addr;\n\tint rc = 0;\n\n\tmgr = &hpriv->ctx->sig_mgr;\n\n\tspin_lock(&mgr->lock);\n\tencaps_sig_hdl = idr_find(&mgr->handles, handle_id);\n\tif (encaps_sig_hdl) {\n\t\tdev_dbg(hdev->dev, \"unreserve signals, handle: %u, SOB:0x%x, count: %u\\n\",\n\t\t\t\thandle_id, encaps_sig_hdl->hw_sob->sob_addr,\n\t\t\t\t\tencaps_sig_hdl->count);\n\n\t\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\t\tq_idx = encaps_sig_hdl->q_idx;\n\t\tprop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\t\thw_sob = &prop->hw_sob[prop->curr_sob_offset];\n\t\tsob_addr = hdev->asic_funcs->get_sob_addr(hdev, hw_sob->sob_id);\n\n\t\t \n\t\tif (encaps_sig_hdl->pre_sob_val + encaps_sig_hdl->count\n\t\t\t\t!= prop->next_sob_val ||\n\t\t\t\tsob_addr != encaps_sig_hdl->hw_sob->sob_addr) {\n\t\t\tdev_err(hdev->dev, \"Cannot unreserve signals, SOB val ran out of sync, expected: %u, actual val: %u\\n\",\n\t\t\t\tencaps_sig_hdl->pre_sob_val,\n\t\t\t\t(prop->next_sob_val - encaps_sig_hdl->count));\n\n\t\t\thdev->asic_funcs->hw_queues_unlock(hdev);\n\t\t\trc = -EINVAL;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\t \n\t\tprop->next_sob_val -= encaps_sig_hdl->count;\n\n\t\thdev->asic_funcs->hw_queues_unlock(hdev);\n\n\t\thw_sob_put(hw_sob);\n\n\t\t \n\t\tidr_remove(&mgr->handles, handle_id);\n\n\t\t \n\t\tspin_unlock(&mgr->lock);\n\t\thl_ctx_put(encaps_sig_hdl->ctx);\n\t\tkfree(encaps_sig_hdl);\n\t\tgoto out;\n\t} else {\n\t\trc = -EINVAL;\n\t\tdev_err(hdev->dev, \"failed to unreserve signals, cannot find handler\\n\");\n\t}\n\nout_unlock:\n\tspin_unlock(&mgr->lock);\n\nout:\n\treturn rc;\n}\n\nstatic int cs_ioctl_signal_wait(struct hl_fpriv *hpriv, enum hl_cs_type cs_type,\n\t\t\t\tvoid __user *chunks, u32 num_chunks,\n\t\t\t\tu64 *cs_seq, u32 flags, u32 timeout,\n\t\t\t\tu32 *signal_sob_addr_offset, u16 *signal_initial_sob_count)\n{\n\tstruct hl_cs_encaps_sig_handle *encaps_sig_hdl = NULL;\n\tbool handle_found = false, is_wait_cs = false,\n\t\t\twait_cs_submitted = false,\n\t\t\tcs_encaps_signals = false;\n\tstruct hl_cs_chunk *cs_chunk_array, *chunk;\n\tbool staged_cs_with_encaps_signals = false;\n\tstruct hw_queue_properties *hw_queue_prop;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_cs_compl *sig_waitcs_cmpl;\n\tu32 q_idx, collective_engine_id = 0;\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_fence *sig_fence = NULL;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tenum hl_queue_type q_type;\n\tstruct hl_cs *cs;\n\tu64 signal_seq;\n\tint rc;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\t*cs_seq = ULLONG_MAX;\n\n\trc = hl_cs_copy_chunk_array(hdev, &cs_chunk_array, chunks, num_chunks,\n\t\t\tctx);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tchunk = &cs_chunk_array[0];\n\n\tif (chunk->queue_index >= hdev->asic_prop.max_queues) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\tdev_err(hdev->dev, \"Queue index %d is invalid\\n\",\n\t\t\tchunk->queue_index);\n\t\trc = -EINVAL;\n\t\tgoto free_cs_chunk_array;\n\t}\n\n\tq_idx = chunk->queue_index;\n\thw_queue_prop = &hdev->asic_prop.hw_queues_props[q_idx];\n\tq_type = hw_queue_prop->type;\n\n\tif (!hw_queue_prop->supports_sync_stream) {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"Queue index %d does not support sync stream operations\\n\",\n\t\t\tq_idx);\n\t\trc = -EINVAL;\n\t\tgoto free_cs_chunk_array;\n\t}\n\n\tif (cs_type == CS_TYPE_COLLECTIVE_WAIT) {\n\t\tif (!(hw_queue_prop->collective_mode == HL_COLLECTIVE_MASTER)) {\n\t\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Queue index %d is invalid\\n\", q_idx);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\n\t\tif (!hdev->nic_ports_mask) {\n\t\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Collective operations not supported when NIC ports are disabled\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\n\t\tcollective_engine_id = chunk->collective_engine_id;\n\t}\n\n\tis_wait_cs = !!(cs_type == CS_TYPE_WAIT ||\n\t\t\tcs_type == CS_TYPE_COLLECTIVE_WAIT);\n\n\tcs_encaps_signals = !!(flags & HL_CS_FLAGS_ENCAP_SIGNALS);\n\n\tif (is_wait_cs) {\n\t\trc = cs_ioctl_extract_signal_seq(hdev, chunk, &signal_seq,\n\t\t\t\tctx, cs_encaps_signals);\n\t\tif (rc)\n\t\t\tgoto free_cs_chunk_array;\n\n\t\tif (cs_encaps_signals) {\n\t\t\t \n\t\t\tstruct idr *idp;\n\t\t\tu32 id;\n\n\t\t\tspin_lock(&ctx->sig_mgr.lock);\n\t\t\tidp = &ctx->sig_mgr.handles;\n\t\t\tidr_for_each_entry(idp, encaps_sig_hdl, id) {\n\t\t\t\tif (encaps_sig_hdl->cs_seq == signal_seq) {\n\t\t\t\t\t \n\t\t\t\t\tif (kref_get_unless_zero(&encaps_sig_hdl->refcount))\n\t\t\t\t\t\thandle_found = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tspin_unlock(&ctx->sig_mgr.lock);\n\n\t\t\tif (!handle_found) {\n\t\t\t\t \n\t\t\t\tdev_dbg(hdev->dev, \"Cannot find encapsulated signals handle for seq 0x%llx\\n\",\n\t\t\t\t\t\tsignal_seq);\n\t\t\t\trc = 0;\n\t\t\t\tgoto free_cs_chunk_array;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (chunk->encaps_signal_offset >\n\t\t\t\t\tencaps_sig_hdl->count) {\n\t\t\t\tdev_err(hdev->dev, \"offset(%u) value exceed max reserved signals count(%u)!\\n\",\n\t\t\t\t\t\tchunk->encaps_signal_offset,\n\t\t\t\t\t\tencaps_sig_hdl->count);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tgoto free_cs_chunk_array;\n\t\t\t}\n\t\t}\n\n\t\tsig_fence = hl_ctx_get_fence(ctx, signal_seq);\n\t\tif (IS_ERR(sig_fence)) {\n\t\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to get signal CS with seq 0x%llx\\n\",\n\t\t\t\tsignal_seq);\n\t\t\trc = PTR_ERR(sig_fence);\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\n\t\tif (!sig_fence) {\n\t\t\t \n\t\t\trc = 0;\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\n\t\tsig_waitcs_cmpl =\n\t\t\tcontainer_of(sig_fence, struct hl_cs_compl, base_fence);\n\n\t\tstaged_cs_with_encaps_signals = !!\n\t\t\t\t(sig_waitcs_cmpl->type == CS_TYPE_DEFAULT &&\n\t\t\t\t(flags & HL_CS_FLAGS_ENCAP_SIGNALS));\n\n\t\tif (sig_waitcs_cmpl->type != CS_TYPE_SIGNAL &&\n\t\t\t\t!staged_cs_with_encaps_signals) {\n\t\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"CS seq 0x%llx is not of a signal/encaps-signal CS\\n\",\n\t\t\t\tsignal_seq);\n\t\t\thl_fence_put(sig_fence);\n\t\t\trc = -EINVAL;\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\n\t\tif (completion_done(&sig_fence->completion)) {\n\t\t\t \n\t\t\thl_fence_put(sig_fence);\n\t\t\trc = 0;\n\t\t\tgoto free_cs_chunk_array;\n\t\t}\n\t}\n\n\trc = allocate_cs(hdev, ctx, cs_type, ULLONG_MAX, &cs, flags, timeout);\n\tif (rc) {\n\t\tif (is_wait_cs)\n\t\t\thl_fence_put(sig_fence);\n\n\t\tgoto free_cs_chunk_array;\n\t}\n\n\t \n\tif (is_wait_cs) {\n\t\tcs->signal_fence = sig_fence;\n\t\t \n\t\tif (cs->encaps_signals)\n\t\t\tcs->encaps_sig_hdl = encaps_sig_hdl;\n\t}\n\n\thl_debugfs_add_cs(cs);\n\n\t*cs_seq = cs->sequence;\n\n\tif (cs_type == CS_TYPE_WAIT || cs_type == CS_TYPE_SIGNAL)\n\t\trc = cs_ioctl_signal_wait_create_jobs(hdev, ctx, cs, q_type,\n\t\t\t\tq_idx, chunk->encaps_signal_offset);\n\telse if (cs_type == CS_TYPE_COLLECTIVE_WAIT)\n\t\trc = hdev->asic_funcs->collective_wait_create_jobs(hdev, ctx,\n\t\t\t\tcs, q_idx, collective_engine_id,\n\t\t\t\tchunk->encaps_signal_offset);\n\telse {\n\t\tatomic64_inc(&ctx->cs_counters.validation_drop_cnt);\n\t\tatomic64_inc(&cntr->validation_drop_cnt);\n\t\trc = -EINVAL;\n\t}\n\n\tif (rc)\n\t\tgoto free_cs_object;\n\n\tif (q_type == QUEUE_TYPE_HW)\n\t\tINIT_WORK(&cs->finish_work, cs_completion);\n\n\trc = hl_hw_queue_schedule_cs(cs);\n\tif (rc) {\n\t\t \n\t\tif (is_wait_cs)\n\t\t\trc = 0;\n\t\telse if (rc != -EAGAIN)\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to submit CS %d.%llu to H/W queues, error %d\\n\",\n\t\t\t\tctx->asid, cs->sequence, rc);\n\t\tgoto free_cs_object;\n\t}\n\n\t*signal_sob_addr_offset = cs->sob_addr_offset;\n\t*signal_initial_sob_count = cs->initial_sob_count;\n\n\trc = HL_CS_STATUS_SUCCESS;\n\tif (is_wait_cs)\n\t\twait_cs_submitted = true;\n\tgoto put_cs;\n\nfree_cs_object:\n\tcs_rollback(hdev, cs);\n\t*cs_seq = ULLONG_MAX;\n\t \nput_cs:\n\t \n\tcs_put(cs);\nfree_cs_chunk_array:\n\tif (!wait_cs_submitted && cs_encaps_signals && handle_found && is_wait_cs)\n\t\tkref_put(&encaps_sig_hdl->refcount, hl_encaps_release_handle_and_put_ctx);\n\tkfree(cs_chunk_array);\nout:\n\treturn rc;\n}\n\nstatic int cs_ioctl_engine_cores(struct hl_fpriv *hpriv, u64 engine_cores,\n\t\t\t\t\t\tu32 num_engine_cores, u32 core_command)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tvoid __user *engine_cores_arr;\n\tu32 *cores;\n\tint rc;\n\n\tif (!hdev->asic_prop.supports_engine_modes)\n\t\treturn -EPERM;\n\n\tif (!num_engine_cores || num_engine_cores > hdev->asic_prop.num_engine_cores) {\n\t\tdev_err(hdev->dev, \"Number of engine cores %d is invalid\\n\", num_engine_cores);\n\t\treturn -EINVAL;\n\t}\n\n\tif (core_command != HL_ENGINE_CORE_RUN && core_command != HL_ENGINE_CORE_HALT) {\n\t\tdev_err(hdev->dev, \"Engine core command is invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tengine_cores_arr = (void __user *) (uintptr_t) engine_cores;\n\tcores = kmalloc_array(num_engine_cores, sizeof(u32), GFP_KERNEL);\n\tif (!cores)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(cores, engine_cores_arr, num_engine_cores * sizeof(u32))) {\n\t\tdev_err(hdev->dev, \"Failed to copy core-ids array from user\\n\");\n\t\tkfree(cores);\n\t\treturn -EFAULT;\n\t}\n\n\trc = hdev->asic_funcs->set_engine_cores(hdev, cores, num_engine_cores, core_command);\n\tkfree(cores);\n\n\treturn rc;\n}\n\nstatic int cs_ioctl_engines(struct hl_fpriv *hpriv, u64 engines_arr_user_addr,\n\t\t\t\t\t\tu32 num_engines, enum hl_engine_command command)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tu32 *engines, max_num_of_engines;\n\tvoid __user *engines_arr;\n\tint rc;\n\n\tif (!hdev->asic_prop.supports_engine_modes)\n\t\treturn -EPERM;\n\n\tif (command >= HL_ENGINE_COMMAND_MAX) {\n\t\tdev_err(hdev->dev, \"Engine command is invalid\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tmax_num_of_engines = hdev->asic_prop.max_num_of_engines;\n\tif (command == HL_ENGINE_CORE_RUN || command == HL_ENGINE_CORE_HALT)\n\t\tmax_num_of_engines = hdev->asic_prop.num_engine_cores;\n\n\tif (!num_engines || num_engines > max_num_of_engines) {\n\t\tdev_err(hdev->dev, \"Number of engines %d is invalid\\n\", num_engines);\n\t\treturn -EINVAL;\n\t}\n\n\tengines_arr = (void __user *) (uintptr_t) engines_arr_user_addr;\n\tengines = kmalloc_array(num_engines, sizeof(u32), GFP_KERNEL);\n\tif (!engines)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(engines, engines_arr, num_engines * sizeof(u32))) {\n\t\tdev_err(hdev->dev, \"Failed to copy engine-ids array from user\\n\");\n\t\tkfree(engines);\n\t\treturn -EFAULT;\n\t}\n\n\trc = hdev->asic_funcs->set_engines(hdev, engines, num_engines, command);\n\tkfree(engines);\n\n\treturn rc;\n}\n\nstatic int cs_ioctl_flush_pci_hbw_writes(struct hl_fpriv *hpriv)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\tif (!prop->hbw_flush_reg) {\n\t\tdev_dbg(hdev->dev, \"HBW flush is not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tRREG32(prop->hbw_flush_reg);\n\n\treturn 0;\n}\n\nint hl_cs_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tunion hl_cs_args *args = data;\n\tenum hl_cs_type cs_type = 0;\n\tu64 cs_seq = ULONG_MAX;\n\tvoid __user *chunks;\n\tu32 num_chunks, flags, timeout,\n\t\tsignals_count = 0, sob_addr = 0, handle_id = 0;\n\tu16 sob_initial_count = 0;\n\tint rc;\n\n\trc = hl_cs_sanity_checks(hpriv, args);\n\tif (rc)\n\t\tgoto out;\n\n\trc = hl_cs_ctx_switch(hpriv, args, &cs_seq);\n\tif (rc)\n\t\tgoto out;\n\n\tcs_type = hl_cs_get_cs_type(args->in.cs_flags &\n\t\t\t\t\t~HL_CS_FLAGS_FORCE_RESTORE);\n\tchunks = (void __user *) (uintptr_t) args->in.chunks_execute;\n\tnum_chunks = args->in.num_chunks_execute;\n\tflags = args->in.cs_flags;\n\n\t \n\tif ((flags & HL_CS_FLAGS_STAGED_SUBMISSION) &&\n\t\t\t!(flags & HL_CS_FLAGS_STAGED_SUBMISSION_FIRST))\n\t\tcs_seq = args->in.seq;\n\n\ttimeout = flags & HL_CS_FLAGS_CUSTOM_TIMEOUT\n\t\t\t? msecs_to_jiffies(args->in.timeout * 1000)\n\t\t\t: hpriv->hdev->timeout_jiffies;\n\n\tswitch (cs_type) {\n\tcase CS_TYPE_SIGNAL:\n\tcase CS_TYPE_WAIT:\n\tcase CS_TYPE_COLLECTIVE_WAIT:\n\t\trc = cs_ioctl_signal_wait(hpriv, cs_type, chunks, num_chunks,\n\t\t\t\t\t&cs_seq, args->in.cs_flags, timeout,\n\t\t\t\t\t&sob_addr, &sob_initial_count);\n\t\tbreak;\n\tcase CS_RESERVE_SIGNALS:\n\t\trc = cs_ioctl_reserve_signals(hpriv,\n\t\t\t\t\targs->in.encaps_signals_q_idx,\n\t\t\t\t\targs->in.encaps_signals_count,\n\t\t\t\t\t&handle_id, &sob_addr, &signals_count);\n\t\tbreak;\n\tcase CS_UNRESERVE_SIGNALS:\n\t\trc = cs_ioctl_unreserve_signals(hpriv,\n\t\t\t\t\targs->in.encaps_sig_handle_id);\n\t\tbreak;\n\tcase CS_TYPE_ENGINE_CORE:\n\t\trc = cs_ioctl_engine_cores(hpriv, args->in.engine_cores,\n\t\t\t\targs->in.num_engine_cores, args->in.core_command);\n\t\tbreak;\n\tcase CS_TYPE_ENGINES:\n\t\trc = cs_ioctl_engines(hpriv, args->in.engines,\n\t\t\t\targs->in.num_engines, args->in.engine_command);\n\t\tbreak;\n\tcase CS_TYPE_FLUSH_PCI_HBW_WRITES:\n\t\trc = cs_ioctl_flush_pci_hbw_writes(hpriv);\n\t\tbreak;\n\tdefault:\n\t\trc = cs_ioctl_default(hpriv, chunks, num_chunks, &cs_seq,\n\t\t\t\t\t\targs->in.cs_flags,\n\t\t\t\t\t\targs->in.encaps_sig_handle_id,\n\t\t\t\t\t\ttimeout, &sob_initial_count);\n\t\tbreak;\n\t}\nout:\n\tif (rc != -EAGAIN) {\n\t\tmemset(args, 0, sizeof(*args));\n\n\t\tswitch (cs_type) {\n\t\tcase CS_RESERVE_SIGNALS:\n\t\t\targs->out.handle_id = handle_id;\n\t\t\targs->out.sob_base_addr_offset = sob_addr;\n\t\t\targs->out.count = signals_count;\n\t\t\tbreak;\n\t\tcase CS_TYPE_SIGNAL:\n\t\t\targs->out.sob_base_addr_offset = sob_addr;\n\t\t\targs->out.sob_count_before_submission = sob_initial_count;\n\t\t\targs->out.seq = cs_seq;\n\t\t\tbreak;\n\t\tcase CS_TYPE_DEFAULT:\n\t\t\targs->out.sob_count_before_submission = sob_initial_count;\n\t\t\targs->out.seq = cs_seq;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\targs->out.seq = cs_seq;\n\t\t\tbreak;\n\t\t}\n\n\t\targs->out.status = rc;\n\t}\n\n\treturn rc;\n}\n\nstatic int hl_wait_for_fence(struct hl_ctx *ctx, u64 seq, struct hl_fence *fence,\n\t\t\t\tenum hl_cs_wait_status *status, u64 timeout_us, s64 *timestamp)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tktime_t timestamp_kt;\n\tlong completion_rc;\n\tint rc = 0, error;\n\n\tif (IS_ERR(fence)) {\n\t\trc = PTR_ERR(fence);\n\t\tif (rc == -EINVAL)\n\t\t\tdev_notice_ratelimited(hdev->dev,\n\t\t\t\t\"Can't wait on CS %llu because current CS is at seq %llu\\n\",\n\t\t\t\tseq, ctx->cs_sequence);\n\t\treturn rc;\n\t}\n\n\tif (!fence) {\n\t\tif (!hl_pop_cs_outcome(&ctx->outcome_store, seq, &timestamp_kt, &error)) {\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"Can't wait on seq %llu because current CS is at seq %llu (Fence is gone)\\n\",\n\t\t\t\tseq, ctx->cs_sequence);\n\t\t\t*status = CS_WAIT_STATUS_GONE;\n\t\t\treturn 0;\n\t\t}\n\n\t\tcompletion_rc = 1;\n\t\tgoto report_results;\n\t}\n\n\tif (!timeout_us) {\n\t\tcompletion_rc = completion_done(&fence->completion);\n\t} else {\n\t\tunsigned long timeout;\n\n\t\ttimeout = (timeout_us == MAX_SCHEDULE_TIMEOUT) ?\n\t\t\t\ttimeout_us : usecs_to_jiffies(timeout_us);\n\t\tcompletion_rc =\n\t\t\twait_for_completion_interruptible_timeout(\n\t\t\t\t&fence->completion, timeout);\n\t}\n\n\terror = fence->error;\n\ttimestamp_kt = fence->timestamp;\n\nreport_results:\n\tif (completion_rc > 0) {\n\t\t*status = CS_WAIT_STATUS_COMPLETED;\n\t\tif (timestamp)\n\t\t\t*timestamp = ktime_to_ns(timestamp_kt);\n\t} else {\n\t\t*status = CS_WAIT_STATUS_BUSY;\n\t}\n\n\tif (completion_rc == -ERESTARTSYS)\n\t\trc = completion_rc;\n\telse if (error == -ETIMEDOUT || error == -EIO)\n\t\trc = error;\n\n\treturn rc;\n}\n\n \nstatic int hl_cs_poll_fences(struct multi_cs_data *mcs_data, struct multi_cs_completion *mcs_compl)\n{\n\tstruct hl_fence **fence_ptr = mcs_data->fence_arr;\n\tstruct hl_device *hdev = mcs_data->ctx->hdev;\n\tint i, rc, arr_len = mcs_data->arr_len;\n\tu64 *seq_arr = mcs_data->seq_arr;\n\tktime_t max_ktime, first_cs_time;\n\tenum hl_cs_wait_status status;\n\n\tmemset(fence_ptr, 0, arr_len * sizeof(struct hl_fence *));\n\n\t \n\trc = hl_ctx_get_fences(mcs_data->ctx, seq_arr, fence_ptr, arr_len);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\treinit_completion(&mcs_compl->completion);\n\n\t \n\tmax_ktime = ktime_set(KTIME_SEC_MAX, 0);\n\tfirst_cs_time = max_ktime;\n\n\tfor (i = 0; i < arr_len; i++, fence_ptr++) {\n\t\tstruct hl_fence *fence = *fence_ptr;\n\n\t\t \n\t\tif (fence)\n\t\t\tmcs_compl->stream_master_qid_map |= fence->stream_master_qid_map;\n\n\t\t \n\t\trc = hl_wait_for_fence(mcs_data->ctx, seq_arr[i], fence, &status, 0, NULL);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"wait_for_fence error :%d for CS seq %llu\\n\",\n\t\t\t\t\t\t\t\trc, seq_arr[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (status) {\n\t\tcase CS_WAIT_STATUS_BUSY:\n\t\t\t \n\t\t\tbreak;\n\t\tcase CS_WAIT_STATUS_COMPLETED:\n\t\t\t \n\t\t\tif (fence && !fence->mcs_handling_done) {\n\t\t\t\t \n\t\t\t\tcomplete_all(&mcs_compl->completion);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmcs_data->completion_bitmap |= BIT(i);\n\t\t\t \n\t\t\tif (fence && mcs_data->update_ts &&\n\t\t\t\t\t(ktime_compare(fence->timestamp, first_cs_time) < 0))\n\t\t\t\tfirst_cs_time = fence->timestamp;\n\t\t\tbreak;\n\t\tcase CS_WAIT_STATUS_GONE:\n\t\t\tmcs_data->update_ts = false;\n\t\t\tmcs_data->gone_cs = true;\n\t\t\t \n\t\t\tmcs_data->completion_bitmap |= BIT(i);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdev_err(hdev->dev, \"Invalid fence status\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t}\n\n\thl_fences_put(mcs_data->fence_arr, arr_len);\n\n\tif (mcs_data->update_ts &&\n\t\t\t(ktime_compare(first_cs_time, max_ktime) != 0))\n\t\tmcs_data->timestamp = ktime_to_ns(first_cs_time);\n\n\treturn rc;\n}\n\nstatic int _hl_cs_wait_ioctl(struct hl_device *hdev, struct hl_ctx *ctx, u64 timeout_us, u64 seq,\n\t\t\t\tenum hl_cs_wait_status *status, s64 *timestamp)\n{\n\tstruct hl_fence *fence;\n\tint rc = 0;\n\n\tif (timestamp)\n\t\t*timestamp = 0;\n\n\thl_ctx_get(ctx);\n\n\tfence = hl_ctx_get_fence(ctx, seq);\n\n\trc = hl_wait_for_fence(ctx, seq, fence, status, timeout_us, timestamp);\n\thl_fence_put(fence);\n\thl_ctx_put(ctx);\n\n\treturn rc;\n}\n\nstatic inline unsigned long hl_usecs64_to_jiffies(const u64 usecs)\n{\n\tif (usecs <= U32_MAX)\n\t\treturn usecs_to_jiffies(usecs);\n\n\t \n\tif (usecs >= ((u64)(U64_MAX / NSEC_PER_USEC)))\n\t\treturn nsecs_to_jiffies(U64_MAX);\n\n\treturn nsecs_to_jiffies(usecs * NSEC_PER_USEC);\n}\n\n \nstatic struct multi_cs_completion *hl_wait_multi_cs_completion_init(struct hl_device *hdev)\n{\n\tstruct multi_cs_completion *mcs_compl;\n\tint i;\n\n\t \n\tfor (i = 0; i < MULTI_CS_MAX_USER_CTX; i++) {\n\t\tmcs_compl = &hdev->multi_cs_completion[i];\n\t\tspin_lock(&mcs_compl->lock);\n\t\tif (!mcs_compl->used) {\n\t\t\tmcs_compl->used = 1;\n\t\t\tmcs_compl->timestamp = 0;\n\t\t\t \n\t\t\tmcs_compl->stream_master_qid_map = 0;\n\t\t\tspin_unlock(&mcs_compl->lock);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&mcs_compl->lock);\n\t}\n\n\tif (i == MULTI_CS_MAX_USER_CTX) {\n\t\tdev_err(hdev->dev, \"no available multi-CS completion structure\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\treturn mcs_compl;\n}\n\n \nstatic void hl_wait_multi_cs_completion_fini(\n\t\t\t\t\tstruct multi_cs_completion *mcs_compl)\n{\n\t \n\tspin_lock(&mcs_compl->lock);\n\tmcs_compl->used = 0;\n\tspin_unlock(&mcs_compl->lock);\n}\n\n \nstatic int hl_wait_multi_cs_completion(struct multi_cs_data *mcs_data,\n\t\t\t\t\t\tstruct multi_cs_completion *mcs_compl)\n{\n\tlong completion_rc;\n\n\tcompletion_rc = wait_for_completion_interruptible_timeout(&mcs_compl->completion,\n\t\t\t\t\t\t\t\t\tmcs_data->timeout_jiffies);\n\n\t \n\tif (completion_rc > 0)\n\t\tmcs_data->timestamp = mcs_compl->timestamp;\n\n\tif (completion_rc == -ERESTARTSYS)\n\t\treturn completion_rc;\n\n\tmcs_data->wait_status = completion_rc;\n\n\treturn 0;\n}\n\n \nvoid hl_multi_cs_completion_init(struct hl_device *hdev)\n{\n\tstruct multi_cs_completion *mcs_cmpl;\n\tint i;\n\n\tfor (i = 0; i < MULTI_CS_MAX_USER_CTX; i++) {\n\t\tmcs_cmpl = &hdev->multi_cs_completion[i];\n\t\tmcs_cmpl->used = 0;\n\t\tspin_lock_init(&mcs_cmpl->lock);\n\t\tinit_completion(&mcs_cmpl->completion);\n\t}\n}\n\n \nstatic int hl_multi_cs_wait_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tstruct multi_cs_completion *mcs_compl;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct multi_cs_data mcs_data = {};\n\tunion hl_wait_cs_args *args = data;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tstruct hl_fence **fence_arr;\n\tvoid __user *seq_arr;\n\tu32 size_to_copy;\n\tu64 *cs_seq_arr;\n\tu8 seq_arr_len;\n\tint rc, i;\n\n\tfor (i = 0 ; i < sizeof(args->in.pad) ; i++)\n\t\tif (args->in.pad[i]) {\n\t\t\tdev_dbg(hdev->dev, \"Padding bytes must be 0\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\tif (!hdev->supports_wait_for_multi_cs) {\n\t\tdev_err(hdev->dev, \"Wait for multi CS is not supported\\n\");\n\t\treturn -EPERM;\n\t}\n\n\tseq_arr_len = args->in.seq_arr_len;\n\n\tif (seq_arr_len > HL_WAIT_MULTI_CS_LIST_MAX_LEN) {\n\t\tdev_err(hdev->dev, \"Can wait only up to %d CSs, input sequence is of length %u\\n\",\n\t\t\t\tHL_WAIT_MULTI_CS_LIST_MAX_LEN, seq_arr_len);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tcs_seq_arr =\n\t\tkmalloc_array(seq_arr_len, sizeof(*cs_seq_arr), GFP_KERNEL);\n\tif (!cs_seq_arr)\n\t\treturn -ENOMEM;\n\n\t \n\tseq_arr = (void __user *) (uintptr_t) args->in.seq;\n\tsize_to_copy = seq_arr_len * sizeof(*cs_seq_arr);\n\tif (copy_from_user(cs_seq_arr, seq_arr, size_to_copy)) {\n\t\tdev_err(hdev->dev, \"Failed to copy multi-cs sequence array from user\\n\");\n\t\trc = -EFAULT;\n\t\tgoto free_seq_arr;\n\t}\n\n\t \n\tfence_arr = kmalloc_array(seq_arr_len, sizeof(struct hl_fence *), GFP_KERNEL);\n\tif (!fence_arr) {\n\t\trc = -ENOMEM;\n\t\tgoto free_seq_arr;\n\t}\n\n\t \n\tmcs_data.ctx = ctx;\n\tmcs_data.seq_arr = cs_seq_arr;\n\tmcs_data.fence_arr = fence_arr;\n\tmcs_data.arr_len = seq_arr_len;\n\n\thl_ctx_get(ctx);\n\n\t \n\tmcs_data.timeout_jiffies = hl_usecs64_to_jiffies(args->in.timeout_us);\n\tmcs_compl = hl_wait_multi_cs_completion_init(hdev);\n\tif (IS_ERR(mcs_compl)) {\n\t\trc = PTR_ERR(mcs_compl);\n\t\tgoto put_ctx;\n\t}\n\n\t \n\tmcs_data.update_ts = true;\n\trc = hl_cs_poll_fences(&mcs_data, mcs_compl);\n\t \n\tif (rc || mcs_data.completion_bitmap || !args->in.timeout_us)\n\t\tgoto completion_fini;\n\n\twhile (true) {\n\t\trc = hl_wait_multi_cs_completion(&mcs_data, mcs_compl);\n\t\tif (rc || (mcs_data.wait_status == 0))\n\t\t\tbreak;\n\n\t\t \n\t\tmcs_data.update_ts = false;\n\t\trc = hl_cs_poll_fences(&mcs_data, mcs_compl);\n\n\t\tif (rc || mcs_data.completion_bitmap)\n\t\t\tbreak;\n\n\t\t \n\t\tmcs_data.timeout_jiffies = mcs_data.wait_status;\n\t\tmcs_compl->timestamp = 0;\n\t}\n\ncompletion_fini:\n\thl_wait_multi_cs_completion_fini(mcs_compl);\n\nput_ctx:\n\thl_ctx_put(ctx);\n\tkfree(fence_arr);\n\nfree_seq_arr:\n\tkfree(cs_seq_arr);\n\n\tif (rc == -ERESTARTSYS) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"user process got signal while waiting for Multi-CS\\n\");\n\t\trc = -EINTR;\n\t}\n\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tmemset(args, 0, sizeof(*args));\n\n\tif (mcs_data.completion_bitmap) {\n\t\targs->out.status = HL_WAIT_CS_STATUS_COMPLETED;\n\t\targs->out.cs_completion_map = mcs_data.completion_bitmap;\n\n\t\t \n\t\tif (mcs_data.timestamp) {\n\t\t\targs->out.timestamp_nsec = mcs_data.timestamp;\n\t\t\targs->out.flags |= HL_WAIT_CS_STATUS_FLAG_TIMESTAMP_VLD;\n\t\t}\n\n\t\t \n\t\tif (!mcs_data.timestamp)\n\t\t\targs->out.flags |= HL_WAIT_CS_STATUS_FLAG_GONE;\n\t} else {\n\t\targs->out.status = HL_WAIT_CS_STATUS_BUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic int hl_cs_wait_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tunion hl_wait_cs_args *args = data;\n\tenum hl_cs_wait_status status;\n\tu64 seq = args->in.seq;\n\ts64 timestamp;\n\tint rc;\n\n\trc = _hl_cs_wait_ioctl(hdev, hpriv->ctx, args->in.timeout_us, seq, &status, &timestamp);\n\n\tif (rc == -ERESTARTSYS) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"user process got signal while waiting for CS handle %llu\\n\",\n\t\t\tseq);\n\t\treturn -EINTR;\n\t}\n\n\tmemset(args, 0, sizeof(*args));\n\n\tif (rc) {\n\t\tif (rc == -ETIMEDOUT) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"CS %llu has timed-out while user process is waiting for it\\n\",\n\t\t\t\tseq);\n\t\t\targs->out.status = HL_WAIT_CS_STATUS_TIMEDOUT;\n\t\t} else if (rc == -EIO) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"CS %llu has been aborted while user process is waiting for it\\n\",\n\t\t\t\tseq);\n\t\t\targs->out.status = HL_WAIT_CS_STATUS_ABORTED;\n\t\t}\n\t\treturn rc;\n\t}\n\n\tif (timestamp) {\n\t\targs->out.flags |= HL_WAIT_CS_STATUS_FLAG_TIMESTAMP_VLD;\n\t\targs->out.timestamp_nsec = timestamp;\n\t}\n\n\tswitch (status) {\n\tcase CS_WAIT_STATUS_GONE:\n\t\targs->out.flags |= HL_WAIT_CS_STATUS_FLAG_GONE;\n\t\tfallthrough;\n\tcase CS_WAIT_STATUS_COMPLETED:\n\t\targs->out.status = HL_WAIT_CS_STATUS_COMPLETED;\n\t\tbreak;\n\tcase CS_WAIT_STATUS_BUSY:\n\tdefault:\n\t\targs->out.status = HL_WAIT_CS_STATUS_BUSY;\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic int ts_buff_get_kernel_ts_record(struct hl_mmap_mem_buf *buf,\n\t\t\t\t\tstruct hl_cb *cq_cb,\n\t\t\t\t\tu64 ts_offset, u64 cq_offset, u64 target_value,\n\t\t\t\t\tspinlock_t *wait_list_lock,\n\t\t\t\t\tstruct hl_user_pending_interrupt **pend)\n{\n\tstruct hl_ts_buff *ts_buff = buf->private;\n\tstruct hl_user_pending_interrupt *requested_offset_record =\n\t\t\t\t(struct hl_user_pending_interrupt *)ts_buff->kernel_buff_address +\n\t\t\t\tts_offset;\n\tstruct hl_user_pending_interrupt *cb_last =\n\t\t\t(struct hl_user_pending_interrupt *)ts_buff->kernel_buff_address +\n\t\t\t(ts_buff->kernel_buff_size / sizeof(struct hl_user_pending_interrupt));\n\tunsigned long iter_counter = 0;\n\tu64 current_cq_counter;\n\tktime_t timestamp;\n\n\t \n\tif (requested_offset_record >= cb_last) {\n\t\tdev_err(buf->mmg->dev, \"Ts offset exceeds max CB offset(0x%llx)\\n\",\n\t\t\t\t\t\t\t\t(u64)(uintptr_t)cb_last);\n\t\treturn -EINVAL;\n\t}\n\n\ttimestamp = ktime_get();\n\nstart_over:\n\tspin_lock(wait_list_lock);\n\n\t \n\tif (requested_offset_record->ts_reg_info.in_use) {\n\t\tcurrent_cq_counter = *requested_offset_record->cq_kernel_addr;\n\t\tif (current_cq_counter < requested_offset_record->cq_target_value) {\n\t\t\tlist_del(&requested_offset_record->wait_list_node);\n\t\t\tspin_unlock(wait_list_lock);\n\n\t\t\thl_mmap_mem_buf_put(requested_offset_record->ts_reg_info.buf);\n\t\t\thl_cb_put(requested_offset_record->ts_reg_info.cq_cb);\n\n\t\t\tdev_dbg(buf->mmg->dev,\n\t\t\t\t\"ts node removed from interrupt list now can re-use\\n\");\n\t\t} else {\n\t\t\tdev_dbg(buf->mmg->dev,\n\t\t\t\t\"ts node in middle of irq handling\\n\");\n\n\t\t\t \n\t\t\tspin_unlock(wait_list_lock);\n\t\t\tusleep_range(100, 1000);\n\t\t\tif (++iter_counter == MAX_TS_ITER_NUM) {\n\t\t\t\tdev_err(buf->mmg->dev,\n\t\t\t\t\t\"Timestamp offset processing reached timeout of %lld ms\\n\",\n\t\t\t\t\tktime_ms_delta(ktime_get(), timestamp));\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\n\t\t\tgoto start_over;\n\t\t}\n\t} else {\n\t\t \n\t\trequested_offset_record->ts_reg_info.buf = buf;\n\t\trequested_offset_record->ts_reg_info.cq_cb = cq_cb;\n\t\trequested_offset_record->ts_reg_info.timestamp_kernel_addr =\n\t\t\t\t(u64 *) ts_buff->user_buff_address + ts_offset;\n\t\trequested_offset_record->cq_kernel_addr =\n\t\t\t\t(u64 *) cq_cb->kernel_address + cq_offset;\n\t\trequested_offset_record->cq_target_value = target_value;\n\n\t\tspin_unlock(wait_list_lock);\n\t}\n\n\t*pend = requested_offset_record;\n\n\tdev_dbg(buf->mmg->dev, \"Found available node in TS kernel CB %p\\n\",\n\t\trequested_offset_record);\n\treturn 0;\n}\n\nstatic int _hl_interrupt_wait_ioctl(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\t\t\tstruct hl_mem_mgr *cb_mmg, struct hl_mem_mgr *mmg,\n\t\t\t\tu64 timeout_us, u64 cq_counters_handle,\tu64 cq_counters_offset,\n\t\t\t\tu64 target_value, struct hl_user_interrupt *interrupt,\n\t\t\t\tbool register_ts_record, u64 ts_handle, u64 ts_offset,\n\t\t\t\tu32 *status, u64 *timestamp)\n{\n\tstruct hl_user_pending_interrupt *pend;\n\tstruct hl_mmap_mem_buf *buf;\n\tstruct hl_cb *cq_cb;\n\tunsigned long timeout;\n\tlong completion_rc;\n\tint rc = 0;\n\n\ttimeout = hl_usecs64_to_jiffies(timeout_us);\n\n\thl_ctx_get(ctx);\n\n\tcq_cb = hl_cb_get(cb_mmg, cq_counters_handle);\n\tif (!cq_cb) {\n\t\trc = -EINVAL;\n\t\tgoto put_ctx;\n\t}\n\n\t \n\tif (((u64 *) cq_cb->kernel_address + cq_counters_offset) >=\n\t\t\t((u64 *) cq_cb->kernel_address + (cq_cb->size / sizeof(u64)))) {\n\t\trc = -EINVAL;\n\t\tgoto put_cq_cb;\n\t}\n\n\tif (register_ts_record) {\n\t\tdev_dbg(hdev->dev, \"Timestamp registration: interrupt id: %u, ts offset: %llu, cq_offset: %llu\\n\",\n\t\t\t\t\tinterrupt->interrupt_id, ts_offset, cq_counters_offset);\n\t\tbuf = hl_mmap_mem_buf_get(mmg, ts_handle);\n\t\tif (!buf) {\n\t\t\trc = -EINVAL;\n\t\t\tgoto put_cq_cb;\n\t\t}\n\n\t\t \n\t\trc = ts_buff_get_kernel_ts_record(buf, cq_cb, ts_offset,\n\t\t\t\t\t\tcq_counters_offset, target_value,\n\t\t\t\t\t\t&interrupt->wait_list_lock, &pend);\n\t\tif (rc)\n\t\t\tgoto put_ts_buff;\n\t} else {\n\t\tpend = kzalloc(sizeof(*pend), GFP_KERNEL);\n\t\tif (!pend) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto put_cq_cb;\n\t\t}\n\t\thl_fence_init(&pend->fence, ULONG_MAX);\n\t\tpend->cq_kernel_addr = (u64 *) cq_cb->kernel_address + cq_counters_offset;\n\t\tpend->cq_target_value = target_value;\n\t}\n\n\tspin_lock(&interrupt->wait_list_lock);\n\n\t \n\tif (*pend->cq_kernel_addr >= target_value) {\n\t\tif (register_ts_record)\n\t\t\tpend->ts_reg_info.in_use = 0;\n\t\tspin_unlock(&interrupt->wait_list_lock);\n\n\t\t*status = HL_WAIT_CS_STATUS_COMPLETED;\n\n\t\tif (register_ts_record) {\n\t\t\t*pend->ts_reg_info.timestamp_kernel_addr = ktime_get_ns();\n\t\t\tgoto put_ts_buff;\n\t\t} else {\n\t\t\tpend->fence.timestamp = ktime_get();\n\t\t\tgoto set_timestamp;\n\t\t}\n\t} else if (!timeout_us) {\n\t\tspin_unlock(&interrupt->wait_list_lock);\n\t\t*status = HL_WAIT_CS_STATUS_BUSY;\n\t\tpend->fence.timestamp = ktime_get();\n\t\tgoto set_timestamp;\n\t}\n\n\t \n\tif (register_ts_record)\n\t\tpend->ts_reg_info.in_use = 1;\n\n\tlist_add_tail(&pend->wait_list_node, &interrupt->wait_list_head);\n\tspin_unlock(&interrupt->wait_list_lock);\n\n\tif (register_ts_record) {\n\t\trc = *status = HL_WAIT_CS_STATUS_COMPLETED;\n\t\tgoto ts_registration_exit;\n\t}\n\n\t \n\tcompletion_rc = wait_for_completion_interruptible_timeout(&pend->fence.completion,\n\t\t\t\t\t\t\t\ttimeout);\n\tif (completion_rc > 0) {\n\t\t*status = HL_WAIT_CS_STATUS_COMPLETED;\n\t} else {\n\t\tif (completion_rc == -ERESTARTSYS) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\"user process got signal while waiting for interrupt ID %d\\n\",\n\t\t\t\t\tinterrupt->interrupt_id);\n\t\t\trc = -EINTR;\n\t\t\t*status = HL_WAIT_CS_STATUS_ABORTED;\n\t\t} else {\n\t\t\tif (pend->fence.error == -EIO) {\n\t\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\t\t\"interrupt based wait ioctl aborted(error:%d) due to a reset cycle initiated\\n\",\n\t\t\t\t\t\tpend->fence.error);\n\t\t\t\trc = -EIO;\n\t\t\t\t*status = HL_WAIT_CS_STATUS_ABORTED;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\trc = 0;\n\t\t\t\t*status = HL_WAIT_CS_STATUS_BUSY;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tspin_lock(&interrupt->wait_list_lock);\n\tlist_del(&pend->wait_list_node);\n\tspin_unlock(&interrupt->wait_list_lock);\n\nset_timestamp:\n\t*timestamp = ktime_to_ns(pend->fence.timestamp);\n\tkfree(pend);\n\thl_cb_put(cq_cb);\nts_registration_exit:\n\thl_ctx_put(ctx);\n\n\treturn rc;\n\nput_ts_buff:\n\thl_mmap_mem_buf_put(buf);\nput_cq_cb:\n\thl_cb_put(cq_cb);\nput_ctx:\n\thl_ctx_put(ctx);\n\n\treturn rc;\n}\n\nstatic int _hl_interrupt_wait_ioctl_user_addr(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\t\t\tu64 timeout_us, u64 user_address,\n\t\t\t\tu64 target_value, struct hl_user_interrupt *interrupt,\n\t\t\t\tu32 *status,\n\t\t\t\tu64 *timestamp)\n{\n\tstruct hl_user_pending_interrupt *pend;\n\tunsigned long timeout;\n\tu64 completion_value;\n\tlong completion_rc;\n\tint rc = 0;\n\n\ttimeout = hl_usecs64_to_jiffies(timeout_us);\n\n\thl_ctx_get(ctx);\n\n\tpend = kzalloc(sizeof(*pend), GFP_KERNEL);\n\tif (!pend) {\n\t\thl_ctx_put(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\thl_fence_init(&pend->fence, ULONG_MAX);\n\n\t \n\tspin_lock(&interrupt->wait_list_lock);\n\tlist_add_tail(&pend->wait_list_node, &interrupt->wait_list_head);\n\tspin_unlock(&interrupt->wait_list_lock);\n\n\t \n\tif (copy_from_user(&completion_value, u64_to_user_ptr(user_address), 8)) {\n\t\tdev_err(hdev->dev, \"Failed to copy completion value from user\\n\");\n\t\trc = -EFAULT;\n\t\tgoto remove_pending_user_interrupt;\n\t}\n\n\tif (completion_value >= target_value) {\n\t\t*status = HL_WAIT_CS_STATUS_COMPLETED;\n\t\t \n\t\tpend->fence.timestamp = ktime_get();\n\t} else {\n\t\t*status = HL_WAIT_CS_STATUS_BUSY;\n\t}\n\n\tif (!timeout_us || (*status == HL_WAIT_CS_STATUS_COMPLETED))\n\t\tgoto remove_pending_user_interrupt;\n\nwait_again:\n\t \n\tcompletion_rc = wait_for_completion_interruptible_timeout(&pend->fence.completion,\n\t\t\t\t\t\t\t\t\t\ttimeout);\n\n\t \n\tif (completion_rc > 0) {\n\t\tspin_lock(&interrupt->wait_list_lock);\n\t\t \n\t\treinit_completion(&pend->fence.completion);\n\t\tspin_unlock(&interrupt->wait_list_lock);\n\n\t\tif (copy_from_user(&completion_value, u64_to_user_ptr(user_address), 8)) {\n\t\t\tdev_err(hdev->dev, \"Failed to copy completion value from user\\n\");\n\t\t\trc = -EFAULT;\n\n\t\t\tgoto remove_pending_user_interrupt;\n\t\t}\n\n\t\tif (completion_value >= target_value) {\n\t\t\t*status = HL_WAIT_CS_STATUS_COMPLETED;\n\t\t} else if (pend->fence.error) {\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"interrupt based wait ioctl aborted(error:%d) due to a reset cycle initiated\\n\",\n\t\t\t\tpend->fence.error);\n\t\t\t \n\t\t\t*status = HL_WAIT_CS_STATUS_ABORTED;\n\t\t} else {\n\t\t\ttimeout = completion_rc;\n\t\t\tgoto wait_again;\n\t\t}\n\t} else if (completion_rc == -ERESTARTSYS) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"user process got signal while waiting for interrupt ID %d\\n\",\n\t\t\tinterrupt->interrupt_id);\n\t\trc = -EINTR;\n\t} else {\n\t\t \n\t\trc = 0;\n\t\t*status = HL_WAIT_CS_STATUS_BUSY;\n\t}\n\nremove_pending_user_interrupt:\n\tspin_lock(&interrupt->wait_list_lock);\n\tlist_del(&pend->wait_list_node);\n\tspin_unlock(&interrupt->wait_list_lock);\n\n\t*timestamp = ktime_to_ns(pend->fence.timestamp);\n\n\tkfree(pend);\n\thl_ctx_put(ctx);\n\n\treturn rc;\n}\n\nstatic int hl_interrupt_wait_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tu16 interrupt_id, first_interrupt, last_interrupt;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct asic_fixed_properties *prop;\n\tstruct hl_user_interrupt *interrupt;\n\tunion hl_wait_cs_args *args = data;\n\tu32 status = HL_WAIT_CS_STATUS_BUSY;\n\tu64 timestamp = 0;\n\tint rc, int_idx;\n\n\tprop = &hdev->asic_prop;\n\n\tif (!(prop->user_interrupt_count + prop->user_dec_intr_count)) {\n\t\tdev_err(hdev->dev, \"no user interrupts allowed\");\n\t\treturn -EPERM;\n\t}\n\n\tinterrupt_id = FIELD_GET(HL_WAIT_CS_FLAGS_INTERRUPT_MASK, args->in.flags);\n\n\tfirst_interrupt = prop->first_available_user_interrupt;\n\tlast_interrupt = prop->first_available_user_interrupt + prop->user_interrupt_count - 1;\n\n\tif (interrupt_id < prop->user_dec_intr_count) {\n\n\t\t \n\t\tif (!(prop->decoder_enabled_mask & BIT(interrupt_id))) {\n\t\t\tdev_err(hdev->dev, \"interrupt on a disabled core(%u) not allowed\",\n\t\t\t\tinterrupt_id);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tinterrupt = &hdev->user_interrupt[interrupt_id];\n\n\t} else if (interrupt_id >= first_interrupt && interrupt_id <= last_interrupt) {\n\n\t\tint_idx = interrupt_id - first_interrupt + prop->user_dec_intr_count;\n\t\tinterrupt = &hdev->user_interrupt[int_idx];\n\n\t} else if (interrupt_id == HL_COMMON_USER_CQ_INTERRUPT_ID) {\n\t\tinterrupt = &hdev->common_user_cq_interrupt;\n\t} else if (interrupt_id == HL_COMMON_DEC_INTERRUPT_ID) {\n\t\tinterrupt = &hdev->common_decoder_interrupt;\n\t} else {\n\t\tdev_err(hdev->dev, \"invalid user interrupt %u\", interrupt_id);\n\t\treturn -EINVAL;\n\t}\n\n\tif (args->in.flags & HL_WAIT_CS_FLAGS_INTERRUPT_KERNEL_CQ)\n\t\trc = _hl_interrupt_wait_ioctl(hdev, hpriv->ctx, &hpriv->mem_mgr, &hpriv->mem_mgr,\n\t\t\t\targs->in.interrupt_timeout_us, args->in.cq_counters_handle,\n\t\t\t\targs->in.cq_counters_offset,\n\t\t\t\targs->in.target, interrupt,\n\t\t\t\t!!(args->in.flags & HL_WAIT_CS_FLAGS_REGISTER_INTERRUPT),\n\t\t\t\targs->in.timestamp_handle, args->in.timestamp_offset,\n\t\t\t\t&status, &timestamp);\n\telse\n\t\trc = _hl_interrupt_wait_ioctl_user_addr(hdev, hpriv->ctx,\n\t\t\t\targs->in.interrupt_timeout_us, args->in.addr,\n\t\t\t\targs->in.target, interrupt, &status,\n\t\t\t\t&timestamp);\n\tif (rc)\n\t\treturn rc;\n\n\tmemset(args, 0, sizeof(*args));\n\targs->out.status = status;\n\n\tif (timestamp) {\n\t\targs->out.timestamp_nsec = timestamp;\n\t\targs->out.flags |= HL_WAIT_CS_STATUS_FLAG_TIMESTAMP_VLD;\n\t}\n\n\treturn 0;\n}\n\nint hl_wait_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tstruct hl_device *hdev = hpriv->hdev;\n\tunion hl_wait_cs_args *args = data;\n\tu32 flags = args->in.flags;\n\tint rc;\n\n\t \n\tif (!hl_device_operational(hpriv->hdev, NULL) || hdev->reset_info.watchdog_active)\n\t\treturn -EBUSY;\n\n\tif (flags & HL_WAIT_CS_FLAGS_INTERRUPT)\n\t\trc = hl_interrupt_wait_ioctl(hpriv, data);\n\telse if (flags & HL_WAIT_CS_FLAGS_MULTI_CS)\n\t\trc = hl_multi_cs_wait_ioctl(hpriv, data);\n\telse\n\t\trc = hl_cs_wait_ioctl(hpriv, data);\n\n\treturn rc;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}