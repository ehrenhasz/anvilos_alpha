{
  "module_name": "hw_queue.c",
  "hash_id": "b51f8fd0d05fbbf2f684f5ddf3b3dbeb9dd787810f2be45e50a120efb958d617",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/hw_queue.c",
  "human_readable_source": "\n\n \n\n#include \"habanalabs.h\"\n\n#include <linux/slab.h>\n\n \ninline u32 hl_hw_queue_add_ptr(u32 ptr, u16 val)\n{\n\tptr += val;\n\tptr &= ((HL_QUEUE_LENGTH << 1) - 1);\n\treturn ptr;\n}\nstatic inline int queue_ci_get(atomic_t *ci, u32 queue_len)\n{\n\treturn atomic_read(ci) & ((queue_len << 1) - 1);\n}\n\nstatic inline int queue_free_slots(struct hl_hw_queue *q, u32 queue_len)\n{\n\tint delta = (q->pi - queue_ci_get(&q->ci, queue_len));\n\n\tif (delta >= 0)\n\t\treturn (queue_len - delta);\n\telse\n\t\treturn (abs(delta) - queue_len);\n}\n\nvoid hl_hw_queue_update_ci(struct hl_cs *cs)\n{\n\tstruct hl_device *hdev = cs->ctx->hdev;\n\tstruct hl_hw_queue *q;\n\tint i;\n\n\tif (hdev->disabled)\n\t\treturn;\n\n\tq = &hdev->kernel_queues[0];\n\n\t \n\tif (!hdev->asic_prop.max_queues || q->queue_type == QUEUE_TYPE_HW)\n\t\treturn;\n\n\t \n\tfor (i = 0 ; i < hdev->asic_prop.max_queues ; i++, q++) {\n\t\tif (!cs_needs_completion(cs) || q->queue_type == QUEUE_TYPE_INT)\n\t\t\tatomic_add(cs->jobs_in_queue_cnt[i], &q->ci);\n\t}\n}\n\n \nvoid hl_hw_queue_submit_bd(struct hl_device *hdev, struct hl_hw_queue *q,\n\t\tu32 ctl, u32 len, u64 ptr)\n{\n\tstruct hl_bd *bd;\n\n\tbd = q->kernel_address;\n\tbd += hl_pi_2_offset(q->pi);\n\tbd->ctl = cpu_to_le32(ctl);\n\tbd->len = cpu_to_le32(len);\n\tbd->ptr = cpu_to_le64(ptr);\n\n\tq->pi = hl_queue_inc_ptr(q->pi);\n\thdev->asic_funcs->ring_doorbell(hdev, q->hw_queue_id, q->pi);\n}\n\n \nstatic int ext_queue_sanity_checks(struct hl_device *hdev,\n\t\t\t\tstruct hl_hw_queue *q, int num_of_entries,\n\t\t\t\tbool reserve_cq_entry)\n{\n\tatomic_t *free_slots =\n\t\t\t&hdev->completion_queue[q->cq_id].free_slots_cnt;\n\tint free_slots_cnt;\n\n\t \n\tfree_slots_cnt = queue_free_slots(q, HL_QUEUE_LENGTH);\n\n\tif (free_slots_cnt < num_of_entries) {\n\t\tdev_dbg(hdev->dev, \"Queue %d doesn't have room for %d CBs\\n\",\n\t\t\tq->hw_queue_id, num_of_entries);\n\t\treturn -EAGAIN;\n\t}\n\n\tif (reserve_cq_entry) {\n\t\t \n\t\tif (atomic_add_negative(num_of_entries * -1, free_slots)) {\n\t\t\tdev_dbg(hdev->dev, \"No space for %d on CQ %d\\n\",\n\t\t\t\tnum_of_entries, q->hw_queue_id);\n\t\t\tatomic_add(num_of_entries, free_slots);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int int_queue_sanity_checks(struct hl_device *hdev,\n\t\t\t\t\tstruct hl_hw_queue *q,\n\t\t\t\t\tint num_of_entries)\n{\n\tint free_slots_cnt;\n\n\tif (num_of_entries > q->int_queue_len) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Cannot populate queue %u with %u jobs\\n\",\n\t\t\tq->hw_queue_id, num_of_entries);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfree_slots_cnt = queue_free_slots(q, q->int_queue_len);\n\n\tif (free_slots_cnt < num_of_entries) {\n\t\tdev_dbg(hdev->dev, \"Queue %d doesn't have room for %d CBs\\n\",\n\t\t\tq->hw_queue_id, num_of_entries);\n\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int hw_queue_sanity_checks(struct hl_device *hdev, struct hl_hw_queue *q,\n\t\t\t\t\tint num_of_entries)\n{\n\tint free_slots_cnt;\n\n\t \n\tfree_slots_cnt = queue_free_slots(q, HL_QUEUE_LENGTH);\n\n\tif (free_slots_cnt < num_of_entries) {\n\t\tdev_dbg(hdev->dev, \"Queue %d doesn't have room for %d CBs\\n\",\n\t\t\tq->hw_queue_id, num_of_entries);\n\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\n \nint hl_hw_queue_send_cb_no_cmpl(struct hl_device *hdev, u32 hw_queue_id,\n\t\t\t\tu32 cb_size, u64 cb_ptr)\n{\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[hw_queue_id];\n\tint rc = 0;\n\n\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\tif (hdev->disabled) {\n\t\trc = -EPERM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (q->queue_type != QUEUE_TYPE_HW) {\n\t\trc = ext_queue_sanity_checks(hdev, q, 1, false);\n\t\tif (rc)\n\t\t\tgoto out;\n\t}\n\n\thl_hw_queue_submit_bd(hdev, q, 0, cb_size, cb_ptr);\n\nout:\n\thdev->asic_funcs->hw_queues_unlock(hdev);\n\n\treturn rc;\n}\n\n \nstatic void ext_queue_schedule_job(struct hl_cs_job *job)\n{\n\tstruct hl_device *hdev = job->cs->ctx->hdev;\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[job->hw_queue_id];\n\tstruct hl_cq_entry cq_pkt;\n\tstruct hl_cq *cq;\n\tu64 cq_addr;\n\tstruct hl_cb *cb;\n\tu32 ctl;\n\tu32 len;\n\tu64 ptr;\n\n\t \n\tctl = ((q->pi << BD_CTL_SHADOW_INDEX_SHIFT) & BD_CTL_SHADOW_INDEX_MASK);\n\n\tcb = job->patched_cb;\n\tlen = job->job_cb_size;\n\tptr = cb->bus_address;\n\n\t \n\tif (!cs_needs_completion(job->cs))\n\t\tgoto submit_bd;\n\n\tcq_pkt.data = cpu_to_le32(\n\t\t\t((q->pi << CQ_ENTRY_SHADOW_INDEX_SHIFT)\n\t\t\t\t& CQ_ENTRY_SHADOW_INDEX_MASK) |\n\t\t\tFIELD_PREP(CQ_ENTRY_SHADOW_INDEX_VALID_MASK, 1) |\n\t\t\tFIELD_PREP(CQ_ENTRY_READY_MASK, 1));\n\n\t \n\tcq = &hdev->completion_queue[q->cq_id];\n\tcq_addr = cq->bus_address + cq->pi * sizeof(struct hl_cq_entry);\n\n\thdev->asic_funcs->add_end_of_cb_packets(hdev, cb->kernel_address, len,\n\t\t\t\t\t\tjob->user_cb_size,\n\t\t\t\t\t\tcq_addr,\n\t\t\t\t\t\tle32_to_cpu(cq_pkt.data),\n\t\t\t\t\t\tq->msi_vec,\n\t\t\t\t\t\tjob->contains_dma_pkt);\n\n\tq->shadow_queue[hl_pi_2_offset(q->pi)] = job;\n\n\tcq->pi = hl_cq_inc_ptr(cq->pi);\n\nsubmit_bd:\n\thl_hw_queue_submit_bd(hdev, q, ctl, len, ptr);\n}\n\n \nstatic void int_queue_schedule_job(struct hl_cs_job *job)\n{\n\tstruct hl_device *hdev = job->cs->ctx->hdev;\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[job->hw_queue_id];\n\tstruct hl_bd bd;\n\t__le64 *pi;\n\n\tbd.ctl = 0;\n\tbd.len = cpu_to_le32(job->job_cb_size);\n\n\tif (job->is_kernel_allocated_cb)\n\t\t \n\t\tbd.ptr = cpu_to_le64(job->user_cb->bus_address);\n\telse\n\t\tbd.ptr = cpu_to_le64((u64) (uintptr_t) job->user_cb);\n\n\tpi = q->kernel_address + (q->pi & (q->int_queue_len - 1)) * sizeof(bd);\n\n\tq->pi++;\n\tq->pi &= ((q->int_queue_len << 1) - 1);\n\n\thdev->asic_funcs->pqe_write(hdev, pi, &bd);\n\n\thdev->asic_funcs->ring_doorbell(hdev, q->hw_queue_id, q->pi);\n}\n\n \nstatic void hw_queue_schedule_job(struct hl_cs_job *job)\n{\n\tstruct hl_device *hdev = job->cs->ctx->hdev;\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[job->hw_queue_id];\n\tu64 ptr;\n\tu32 offset, ctl, len;\n\n\t \n\toffset = job->cs->sequence & (hdev->asic_prop.max_pending_cs - 1);\n\tctl = ((offset << BD_CTL_COMP_OFFSET_SHIFT) & BD_CTL_COMP_OFFSET_MASK) |\n\t\t((q->pi << BD_CTL_COMP_DATA_SHIFT) & BD_CTL_COMP_DATA_MASK);\n\n\tlen = job->job_cb_size;\n\n\t \n\tif (job->patched_cb)\n\t\tptr = job->patched_cb->bus_address;\n\telse if (job->is_kernel_allocated_cb)\n\t\tptr = job->user_cb->bus_address;\n\telse\n\t\tptr = (u64) (uintptr_t) job->user_cb;\n\n\thl_hw_queue_submit_bd(hdev, q, ctl, len, ptr);\n}\n\nstatic int init_signal_cs(struct hl_device *hdev,\n\t\tstruct hl_cs_job *job, struct hl_cs_compl *cs_cmpl)\n{\n\tstruct hl_sync_stream_properties *prop;\n\tstruct hl_hw_sob *hw_sob;\n\tu32 q_idx;\n\tint rc = 0;\n\n\tq_idx = job->hw_queue_id;\n\tprop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\thw_sob = &prop->hw_sob[prop->curr_sob_offset];\n\n\tcs_cmpl->hw_sob = hw_sob;\n\tcs_cmpl->sob_val = prop->next_sob_val;\n\n\tdev_dbg(hdev->dev,\n\t\t\"generate signal CB, sob_id: %d, sob val: %u, q_idx: %d, seq: %llu\\n\",\n\t\tcs_cmpl->hw_sob->sob_id, cs_cmpl->sob_val, q_idx,\n\t\tcs_cmpl->cs_seq);\n\n\t \n\thdev->asic_funcs->gen_signal_cb(hdev, job->patched_cb,\n\t\t\t\tcs_cmpl->hw_sob->sob_id, 0, true);\n\n\trc = hl_cs_signal_sob_wraparound_handler(hdev, q_idx, &hw_sob, 1,\n\t\t\t\t\t\t\t\tfalse);\n\n\tjob->cs->sob_addr_offset = hw_sob->sob_addr;\n\tjob->cs->initial_sob_count = prop->next_sob_val - 1;\n\n\treturn rc;\n}\n\nvoid hl_hw_queue_encaps_sig_set_sob_info(struct hl_device *hdev,\n\t\t\tstruct hl_cs *cs, struct hl_cs_job *job,\n\t\t\tstruct hl_cs_compl *cs_cmpl)\n{\n\tstruct hl_cs_encaps_sig_handle *handle = cs->encaps_sig_hdl;\n\tu32 offset = 0;\n\n\tcs_cmpl->hw_sob = handle->hw_sob;\n\n\t \n\tif (job->encaps_sig_wait_offset)\n\t\toffset = job->encaps_sig_wait_offset - 1;\n\n\tcs_cmpl->sob_val = handle->pre_sob_val + offset;\n}\n\nstatic int init_wait_cs(struct hl_device *hdev, struct hl_cs *cs,\n\t\tstruct hl_cs_job *job, struct hl_cs_compl *cs_cmpl)\n{\n\tstruct hl_gen_wait_properties wait_prop;\n\tstruct hl_sync_stream_properties *prop;\n\tstruct hl_cs_compl *signal_cs_cmpl;\n\tu32 q_idx;\n\n\tq_idx = job->hw_queue_id;\n\tprop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\n\tsignal_cs_cmpl = container_of(cs->signal_fence,\n\t\t\t\t\tstruct hl_cs_compl,\n\t\t\t\t\tbase_fence);\n\n\tif (cs->encaps_signals) {\n\t\t \n\t\thl_hw_queue_encaps_sig_set_sob_info(hdev, cs, job, cs_cmpl);\n\n\t\tdev_dbg(hdev->dev, \"Wait for encaps signals handle, qidx(%u), CS sequence(%llu), sob val: 0x%x, offset: %u\\n\",\n\t\t\t\tcs->encaps_sig_hdl->q_idx,\n\t\t\t\tcs->encaps_sig_hdl->cs_seq,\n\t\t\t\tcs_cmpl->sob_val,\n\t\t\t\tjob->encaps_sig_wait_offset);\n\t} else {\n\t\t \n\t\tcs_cmpl->hw_sob = signal_cs_cmpl->hw_sob;\n\t\tcs_cmpl->sob_val = signal_cs_cmpl->sob_val;\n\t}\n\n\t \n\tspin_lock(&signal_cs_cmpl->lock);\n\n\tif (completion_done(&cs->signal_fence->completion)) {\n\t\tspin_unlock(&signal_cs_cmpl->lock);\n\t\treturn -EINVAL;\n\t}\n\n\tkref_get(&cs_cmpl->hw_sob->kref);\n\n\tspin_unlock(&signal_cs_cmpl->lock);\n\n\tdev_dbg(hdev->dev,\n\t\t\"generate wait CB, sob_id: %d, sob_val: 0x%x, mon_id: %d, q_idx: %d, seq: %llu\\n\",\n\t\tcs_cmpl->hw_sob->sob_id, cs_cmpl->sob_val,\n\t\tprop->base_mon_id, q_idx, cs->sequence);\n\n\twait_prop.data = (void *) job->patched_cb;\n\twait_prop.sob_base = cs_cmpl->hw_sob->sob_id;\n\twait_prop.sob_mask = 0x1;\n\twait_prop.sob_val = cs_cmpl->sob_val;\n\twait_prop.mon_id = prop->base_mon_id;\n\twait_prop.q_idx = q_idx;\n\twait_prop.size = 0;\n\n\thdev->asic_funcs->gen_wait_cb(hdev, &wait_prop);\n\n\tmb();\n\thl_fence_put(cs->signal_fence);\n\tcs->signal_fence = NULL;\n\n\treturn 0;\n}\n\n \nstatic int init_signal_wait_cs(struct hl_cs *cs)\n{\n\tstruct hl_ctx *ctx = cs->ctx;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_cs_job *job;\n\tstruct hl_cs_compl *cs_cmpl =\n\t\t\tcontainer_of(cs->fence, struct hl_cs_compl, base_fence);\n\tint rc = 0;\n\n\t \n\tjob = list_first_entry(&cs->job_list, struct hl_cs_job,\n\t\t\t\tcs_node);\n\n\tif (cs->type & CS_TYPE_SIGNAL)\n\t\trc = init_signal_cs(hdev, job, cs_cmpl);\n\telse if (cs->type & CS_TYPE_WAIT)\n\t\trc = init_wait_cs(hdev, cs, job, cs_cmpl);\n\n\treturn rc;\n}\n\nstatic int encaps_sig_first_staged_cs_handler\n\t\t\t(struct hl_device *hdev, struct hl_cs *cs)\n{\n\tstruct hl_cs_compl *cs_cmpl =\n\t\t\tcontainer_of(cs->fence,\n\t\t\t\t\tstruct hl_cs_compl, base_fence);\n\tstruct hl_cs_encaps_sig_handle *encaps_sig_hdl;\n\tstruct hl_encaps_signals_mgr *mgr;\n\tint rc = 0;\n\n\tmgr = &cs->ctx->sig_mgr;\n\n\tspin_lock(&mgr->lock);\n\tencaps_sig_hdl = idr_find(&mgr->handles, cs->encaps_sig_hdl_id);\n\tif (encaps_sig_hdl) {\n\t\t \n\t\tencaps_sig_hdl->cs_seq = cs->sequence;\n\t\t \n\t\tcs_cmpl->encaps_signals = true;\n\t\tcs_cmpl->encaps_sig_hdl = encaps_sig_hdl;\n\n\t\t \n\t\tcs_cmpl->hw_sob = encaps_sig_hdl->hw_sob;\n\t\tcs_cmpl->sob_val = encaps_sig_hdl->pre_sob_val +\n\t\t\t\t\t\tencaps_sig_hdl->count;\n\n\t\tdev_dbg(hdev->dev, \"CS seq (%llu) added to encaps signal handler id (%u), count(%u), qidx(%u), sob(%u), val(%u)\\n\",\n\t\t\t\tcs->sequence, encaps_sig_hdl->id,\n\t\t\t\tencaps_sig_hdl->count,\n\t\t\t\tencaps_sig_hdl->q_idx,\n\t\t\t\tcs_cmpl->hw_sob->sob_id,\n\t\t\t\tcs_cmpl->sob_val);\n\n\t} else {\n\t\tdev_err(hdev->dev, \"encaps handle id(%u) wasn't found!\\n\",\n\t\t\t\tcs->encaps_sig_hdl_id);\n\t\trc = -EINVAL;\n\t}\n\n\tspin_unlock(&mgr->lock);\n\n\treturn rc;\n}\n\n \nint hl_hw_queue_schedule_cs(struct hl_cs *cs)\n{\n\tenum hl_device_status status;\n\tstruct hl_cs_counters_atomic *cntr;\n\tstruct hl_ctx *ctx = cs->ctx;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_cs_job *job, *tmp;\n\tstruct hl_hw_queue *q;\n\tint rc = 0, i, cq_cnt;\n\tbool first_entry;\n\tu32 max_queues;\n\n\tcntr = &hdev->aggregated_cs_counters;\n\n\thdev->asic_funcs->hw_queues_lock(hdev);\n\n\tif (!hl_device_operational(hdev, &status)) {\n\t\tatomic64_inc(&cntr->device_in_reset_drop_cnt);\n\t\tatomic64_inc(&ctx->cs_counters.device_in_reset_drop_cnt);\n\t\tdev_err(hdev->dev,\n\t\t\t\"device is %s, CS rejected!\\n\", hdev->status[status]);\n\t\trc = -EPERM;\n\t\tgoto out;\n\t}\n\n\tmax_queues = hdev->asic_prop.max_queues;\n\n\tq = &hdev->kernel_queues[0];\n\tfor (i = 0, cq_cnt = 0 ; i < max_queues ; i++, q++) {\n\t\tif (cs->jobs_in_queue_cnt[i]) {\n\t\t\tswitch (q->queue_type) {\n\t\t\tcase QUEUE_TYPE_EXT:\n\t\t\t\trc = ext_queue_sanity_checks(hdev, q,\n\t\t\t\t\t\tcs->jobs_in_queue_cnt[i],\n\t\t\t\t\t\tcs_needs_completion(cs) ?\n\t\t\t\t\t\t\t\ttrue : false);\n\t\t\t\tbreak;\n\t\t\tcase QUEUE_TYPE_INT:\n\t\t\t\trc = int_queue_sanity_checks(hdev, q,\n\t\t\t\t\t\tcs->jobs_in_queue_cnt[i]);\n\t\t\t\tbreak;\n\t\t\tcase QUEUE_TYPE_HW:\n\t\t\t\trc = hw_queue_sanity_checks(hdev, q,\n\t\t\t\t\t\tcs->jobs_in_queue_cnt[i]);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tdev_err(hdev->dev, \"Queue type %d is invalid\\n\",\n\t\t\t\t\tq->queue_type);\n\t\t\t\trc = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (rc) {\n\t\t\t\tatomic64_inc(\n\t\t\t\t\t&ctx->cs_counters.queue_full_drop_cnt);\n\t\t\t\tatomic64_inc(&cntr->queue_full_drop_cnt);\n\t\t\t\tgoto unroll_cq_resv;\n\t\t\t}\n\n\t\t\tif (q->queue_type == QUEUE_TYPE_EXT)\n\t\t\t\tcq_cnt++;\n\t\t}\n\t}\n\n\tif ((cs->type == CS_TYPE_SIGNAL) || (cs->type == CS_TYPE_WAIT)) {\n\t\trc = init_signal_wait_cs(cs);\n\t\tif (rc)\n\t\t\tgoto unroll_cq_resv;\n\t} else if (cs->type == CS_TYPE_COLLECTIVE_WAIT) {\n\t\trc = hdev->asic_funcs->collective_wait_init_cs(cs);\n\t\tif (rc)\n\t\t\tgoto unroll_cq_resv;\n\t}\n\n\trc = hdev->asic_funcs->pre_schedule_cs(cs);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed in pre-submission operations of CS %d.%llu\\n\",\n\t\t\tctx->asid, cs->sequence);\n\t\tgoto unroll_cq_resv;\n\t}\n\n\thdev->shadow_cs_queue[cs->sequence &\n\t\t\t\t(hdev->asic_prop.max_pending_cs - 1)] = cs;\n\n\tif (cs->encaps_signals && cs->staged_first) {\n\t\trc = encaps_sig_first_staged_cs_handler(hdev, cs);\n\t\tif (rc)\n\t\t\tgoto unroll_cq_resv;\n\t}\n\n\tspin_lock(&hdev->cs_mirror_lock);\n\n\t \n\tif (cs->staged_cs && !cs->staged_first) {\n\t\tstruct hl_cs *staged_cs;\n\n\t\tstaged_cs = hl_staged_cs_find_first(hdev, cs->staged_sequence);\n\t\tif (!staged_cs) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Cannot find staged submission sequence %llu\",\n\t\t\t\tcs->staged_sequence);\n\t\t\trc = -EINVAL;\n\t\t\tgoto unlock_cs_mirror;\n\t\t}\n\n\t\tif (is_staged_cs_last_exists(hdev, staged_cs)) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Staged submission sequence %llu already submitted\",\n\t\t\t\tcs->staged_sequence);\n\t\t\trc = -EINVAL;\n\t\t\tgoto unlock_cs_mirror;\n\t\t}\n\n\t\tlist_add_tail(&cs->staged_cs_node, &staged_cs->staged_cs_node);\n\n\t\t \n\t\tif (hdev->supports_wait_for_multi_cs)\n\t\t\tstaged_cs->fence->stream_master_qid_map |=\n\t\t\t\t\tcs->fence->stream_master_qid_map;\n\t}\n\n\tlist_add_tail(&cs->mirror_node, &hdev->cs_mirror_list);\n\n\t \n\tfirst_entry = list_first_entry(&hdev->cs_mirror_list,\n\t\t\t\t\tstruct hl_cs, mirror_node) == cs;\n\tif ((hdev->timeout_jiffies != MAX_SCHEDULE_TIMEOUT) &&\n\t\t\t\tfirst_entry && cs_needs_timeout(cs)) {\n\t\tcs->tdr_active = true;\n\t\tschedule_delayed_work(&cs->work_tdr, cs->timeout_jiffies);\n\n\t}\n\n\tspin_unlock(&hdev->cs_mirror_lock);\n\n\tlist_for_each_entry_safe(job, tmp, &cs->job_list, cs_node)\n\t\tswitch (job->queue_type) {\n\t\tcase QUEUE_TYPE_EXT:\n\t\t\text_queue_schedule_job(job);\n\t\t\tbreak;\n\t\tcase QUEUE_TYPE_INT:\n\t\t\tint_queue_schedule_job(job);\n\t\t\tbreak;\n\t\tcase QUEUE_TYPE_HW:\n\t\t\thw_queue_schedule_job(job);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\tcs->submitted = true;\n\n\tgoto out;\n\nunlock_cs_mirror:\n\tspin_unlock(&hdev->cs_mirror_lock);\nunroll_cq_resv:\n\tq = &hdev->kernel_queues[0];\n\tfor (i = 0 ; (i < max_queues) && (cq_cnt > 0) ; i++, q++) {\n\t\tif ((q->queue_type == QUEUE_TYPE_EXT) &&\n\t\t\t\t\t\t(cs->jobs_in_queue_cnt[i])) {\n\t\t\tatomic_t *free_slots =\n\t\t\t\t&hdev->completion_queue[i].free_slots_cnt;\n\t\t\tatomic_add(cs->jobs_in_queue_cnt[i], free_slots);\n\t\t\tcq_cnt--;\n\t\t}\n\t}\n\nout:\n\thdev->asic_funcs->hw_queues_unlock(hdev);\n\n\treturn rc;\n}\n\n \nvoid hl_hw_queue_inc_ci_kernel(struct hl_device *hdev, u32 hw_queue_id)\n{\n\tstruct hl_hw_queue *q = &hdev->kernel_queues[hw_queue_id];\n\n\tatomic_inc(&q->ci);\n}\n\nstatic int ext_and_cpu_queue_init(struct hl_device *hdev, struct hl_hw_queue *q,\n\t\t\t\t\tbool is_cpu_queue)\n{\n\tvoid *p;\n\tint rc;\n\n\tif (is_cpu_queue)\n\t\tp = hl_cpu_accessible_dma_pool_alloc(hdev, HL_QUEUE_SIZE_IN_BYTES, &q->bus_address);\n\telse\n\t\tp = hl_asic_dma_alloc_coherent(hdev, HL_QUEUE_SIZE_IN_BYTES, &q->bus_address,\n\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tq->kernel_address = p;\n\n\tq->shadow_queue = kmalloc_array(HL_QUEUE_LENGTH, sizeof(struct hl_cs_job *), GFP_KERNEL);\n\tif (!q->shadow_queue) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate shadow queue for H/W queue %d\\n\",\n\t\t\tq->hw_queue_id);\n\t\trc = -ENOMEM;\n\t\tgoto free_queue;\n\t}\n\n\t \n\tatomic_set(&q->ci, 0);\n\tq->pi = 0;\n\n\treturn 0;\n\nfree_queue:\n\tif (is_cpu_queue)\n\t\thl_cpu_accessible_dma_pool_free(hdev, HL_QUEUE_SIZE_IN_BYTES, q->kernel_address);\n\telse\n\t\thl_asic_dma_free_coherent(hdev, HL_QUEUE_SIZE_IN_BYTES, q->kernel_address,\n\t\t\t\t\t\tq->bus_address);\n\n\treturn rc;\n}\n\nstatic int int_queue_init(struct hl_device *hdev, struct hl_hw_queue *q)\n{\n\tvoid *p;\n\n\tp = hdev->asic_funcs->get_int_queue_base(hdev, q->hw_queue_id,\n\t\t\t\t\t&q->bus_address, &q->int_queue_len);\n\tif (!p) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to get base address for internal queue %d\\n\",\n\t\t\tq->hw_queue_id);\n\t\treturn -EFAULT;\n\t}\n\n\tq->kernel_address = p;\n\tq->pi = 0;\n\tatomic_set(&q->ci, 0);\n\n\treturn 0;\n}\n\nstatic int cpu_queue_init(struct hl_device *hdev, struct hl_hw_queue *q)\n{\n\treturn ext_and_cpu_queue_init(hdev, q, true);\n}\n\nstatic int ext_queue_init(struct hl_device *hdev, struct hl_hw_queue *q)\n{\n\treturn ext_and_cpu_queue_init(hdev, q, false);\n}\n\nstatic int hw_queue_init(struct hl_device *hdev, struct hl_hw_queue *q)\n{\n\tvoid *p;\n\n\tp = hl_asic_dma_alloc_coherent(hdev, HL_QUEUE_SIZE_IN_BYTES, &q->bus_address,\n\t\t\t\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!p)\n\t\treturn -ENOMEM;\n\n\tq->kernel_address = p;\n\n\t \n\tatomic_set(&q->ci, 0);\n\tq->pi = 0;\n\n\treturn 0;\n}\n\nstatic void sync_stream_queue_init(struct hl_device *hdev, u32 q_idx)\n{\n\tstruct hl_sync_stream_properties *sync_stream_prop;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_hw_sob *hw_sob;\n\tint sob, reserved_mon_idx, queue_idx;\n\n\tsync_stream_prop = &hdev->kernel_queues[q_idx].sync_stream_prop;\n\n\t \n\tif (hdev->kernel_queues[q_idx].collective_mode ==\n\t\t\tHL_COLLECTIVE_MASTER) {\n\t\treserved_mon_idx = hdev->collective_mon_idx;\n\n\t\t \n\t\tsync_stream_prop->collective_mstr_mon_id[0] =\n\t\t\tprop->collective_first_mon + reserved_mon_idx;\n\n\t\t \n\t\tsync_stream_prop->collective_mstr_mon_id[1] =\n\t\t\tprop->collective_first_mon + reserved_mon_idx + 1;\n\n\t\thdev->collective_mon_idx += HL_COLLECTIVE_RSVD_MSTR_MONS;\n\t} else if (hdev->kernel_queues[q_idx].collective_mode ==\n\t\t\tHL_COLLECTIVE_SLAVE) {\n\t\treserved_mon_idx = hdev->collective_mon_idx++;\n\n\t\t \n\t\tsync_stream_prop->collective_slave_mon_id =\n\t\t\tprop->collective_first_mon + reserved_mon_idx;\n\t}\n\n\tif (!hdev->kernel_queues[q_idx].supports_sync_stream)\n\t\treturn;\n\n\tqueue_idx = hdev->sync_stream_queue_idx++;\n\n\tsync_stream_prop->base_sob_id = prop->sync_stream_first_sob +\n\t\t\t(queue_idx * HL_RSVD_SOBS);\n\tsync_stream_prop->base_mon_id = prop->sync_stream_first_mon +\n\t\t\t(queue_idx * HL_RSVD_MONS);\n\tsync_stream_prop->next_sob_val = 1;\n\tsync_stream_prop->curr_sob_offset = 0;\n\n\tfor (sob = 0 ; sob < HL_RSVD_SOBS ; sob++) {\n\t\thw_sob = &sync_stream_prop->hw_sob[sob];\n\t\thw_sob->hdev = hdev;\n\t\thw_sob->sob_id = sync_stream_prop->base_sob_id + sob;\n\t\thw_sob->sob_addr =\n\t\t\thdev->asic_funcs->get_sob_addr(hdev, hw_sob->sob_id);\n\t\thw_sob->q_idx = q_idx;\n\t\tkref_init(&hw_sob->kref);\n\t}\n}\n\nstatic void sync_stream_queue_reset(struct hl_device *hdev, u32 q_idx)\n{\n\tstruct hl_sync_stream_properties *prop =\n\t\t\t&hdev->kernel_queues[q_idx].sync_stream_prop;\n\n\t \n\tkref_init(&prop->hw_sob[prop->curr_sob_offset].kref);\n\tprop->curr_sob_offset = 0;\n\tprop->next_sob_val = 1;\n}\n\n \nstatic int queue_init(struct hl_device *hdev, struct hl_hw_queue *q,\n\t\t\tu32 hw_queue_id)\n{\n\tint rc;\n\n\tq->hw_queue_id = hw_queue_id;\n\n\tswitch (q->queue_type) {\n\tcase QUEUE_TYPE_EXT:\n\t\trc = ext_queue_init(hdev, q);\n\t\tbreak;\n\tcase QUEUE_TYPE_INT:\n\t\trc = int_queue_init(hdev, q);\n\t\tbreak;\n\tcase QUEUE_TYPE_CPU:\n\t\trc = cpu_queue_init(hdev, q);\n\t\tbreak;\n\tcase QUEUE_TYPE_HW:\n\t\trc = hw_queue_init(hdev, q);\n\t\tbreak;\n\tcase QUEUE_TYPE_NA:\n\t\tq->valid = 0;\n\t\treturn 0;\n\tdefault:\n\t\tdev_crit(hdev->dev, \"wrong queue type %d during init\\n\",\n\t\t\tq->queue_type);\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\n\tsync_stream_queue_init(hdev, q->hw_queue_id);\n\n\tif (rc)\n\t\treturn rc;\n\n\tq->valid = 1;\n\n\treturn 0;\n}\n\n \nstatic void queue_fini(struct hl_device *hdev, struct hl_hw_queue *q)\n{\n\tif (!q->valid)\n\t\treturn;\n\n\t \n\n\tif (q->queue_type == QUEUE_TYPE_INT)\n\t\treturn;\n\n\tkfree(q->shadow_queue);\n\n\tif (q->queue_type == QUEUE_TYPE_CPU)\n\t\thl_cpu_accessible_dma_pool_free(hdev, HL_QUEUE_SIZE_IN_BYTES, q->kernel_address);\n\telse\n\t\thl_asic_dma_free_coherent(hdev, HL_QUEUE_SIZE_IN_BYTES, q->kernel_address,\n\t\t\t\t\t\tq->bus_address);\n}\n\nint hl_hw_queues_create(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *asic = &hdev->asic_prop;\n\tstruct hl_hw_queue *q;\n\tint i, rc, q_ready_cnt;\n\n\thdev->kernel_queues = kcalloc(asic->max_queues,\n\t\t\t\tsizeof(*hdev->kernel_queues), GFP_KERNEL);\n\n\tif (!hdev->kernel_queues) {\n\t\tdev_err(hdev->dev, \"Not enough memory for H/W queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (i = 0, q_ready_cnt = 0, q = hdev->kernel_queues;\n\t\t\ti < asic->max_queues ; i++, q_ready_cnt++, q++) {\n\n\t\tq->queue_type = asic->hw_queues_props[i].type;\n\t\tq->supports_sync_stream =\n\t\t\t\tasic->hw_queues_props[i].supports_sync_stream;\n\t\tq->collective_mode = asic->hw_queues_props[i].collective_mode;\n\t\trc = queue_init(hdev, q, i);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"failed to initialize queue %d\\n\", i);\n\t\t\tgoto release_queues;\n\t\t}\n\t}\n\n\treturn 0;\n\nrelease_queues:\n\tfor (i = 0, q = hdev->kernel_queues ; i < q_ready_cnt ; i++, q++)\n\t\tqueue_fini(hdev, q);\n\n\tkfree(hdev->kernel_queues);\n\n\treturn rc;\n}\n\nvoid hl_hw_queues_destroy(struct hl_device *hdev)\n{\n\tstruct hl_hw_queue *q;\n\tu32 max_queues = hdev->asic_prop.max_queues;\n\tint i;\n\n\tfor (i = 0, q = hdev->kernel_queues ; i < max_queues ; i++, q++)\n\t\tqueue_fini(hdev, q);\n\n\tkfree(hdev->kernel_queues);\n}\n\nvoid hl_hw_queue_reset(struct hl_device *hdev, bool hard_reset)\n{\n\tstruct hl_hw_queue *q;\n\tu32 max_queues = hdev->asic_prop.max_queues;\n\tint i;\n\n\tfor (i = 0, q = hdev->kernel_queues ; i < max_queues ; i++, q++) {\n\t\tif ((!q->valid) ||\n\t\t\t((!hard_reset) && (q->queue_type == QUEUE_TYPE_CPU)))\n\t\t\tcontinue;\n\t\tq->pi = 0;\n\t\tatomic_set(&q->ci, 0);\n\n\t\tif (q->supports_sync_stream)\n\t\t\tsync_stream_queue_reset(hdev, q->hw_queue_id);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}