{
  "module_name": "context.c",
  "hash_id": "8e3338fa157be86b429bbe77ef01b85cc382874abb73195f90f102e37f0af82f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/context.c",
  "human_readable_source": "\n\n \n\n#include \"habanalabs.h\"\n\n#include <linux/slab.h>\n\nstatic void encaps_handle_do_release(struct hl_cs_encaps_sig_handle *handle, bool put_hw_sob,\n\t\t\t\t\tbool put_ctx)\n{\n\tstruct hl_encaps_signals_mgr *mgr = &handle->ctx->sig_mgr;\n\n\tif (put_hw_sob)\n\t\thw_sob_put(handle->hw_sob);\n\n\tspin_lock(&mgr->lock);\n\tidr_remove(&mgr->handles, handle->id);\n\tspin_unlock(&mgr->lock);\n\n\tif (put_ctx)\n\t\thl_ctx_put(handle->ctx);\n\n\tkfree(handle);\n}\n\nvoid hl_encaps_release_handle_and_put_ctx(struct kref *ref)\n{\n\tstruct hl_cs_encaps_sig_handle *handle =\n\t\t\tcontainer_of(ref, struct hl_cs_encaps_sig_handle, refcount);\n\n\tencaps_handle_do_release(handle, false, true);\n}\n\nstatic void hl_encaps_release_handle_and_put_sob(struct kref *ref)\n{\n\tstruct hl_cs_encaps_sig_handle *handle =\n\t\t\tcontainer_of(ref, struct hl_cs_encaps_sig_handle, refcount);\n\n\tencaps_handle_do_release(handle, true, false);\n}\n\nvoid hl_encaps_release_handle_and_put_sob_ctx(struct kref *ref)\n{\n\tstruct hl_cs_encaps_sig_handle *handle =\n\t\t\tcontainer_of(ref, struct hl_cs_encaps_sig_handle, refcount);\n\n\tencaps_handle_do_release(handle, true, true);\n}\n\nstatic void hl_encaps_sig_mgr_init(struct hl_encaps_signals_mgr *mgr)\n{\n\tspin_lock_init(&mgr->lock);\n\tidr_init(&mgr->handles);\n}\n\nstatic void hl_encaps_sig_mgr_fini(struct hl_device *hdev, struct hl_encaps_signals_mgr *mgr)\n{\n\tstruct hl_cs_encaps_sig_handle *handle;\n\tstruct idr *idp;\n\tu32 id;\n\n\tidp = &mgr->handles;\n\n\t \n\tif (!idr_is_empty(idp)) {\n\t\tdev_warn(hdev->dev,\n\t\t\t\"device released while some encaps signals handles are still allocated\\n\");\n\t\tidr_for_each_entry(idp, handle, id)\n\t\t\tkref_put(&handle->refcount, hl_encaps_release_handle_and_put_sob);\n\t}\n\n\tidr_destroy(&mgr->handles);\n}\n\nstatic void hl_ctx_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tint i;\n\n\t \n\thl_hw_block_mem_fini(ctx);\n\n\t \n\n\tfor (i = 0 ; i < hdev->asic_prop.max_pending_cs ; i++)\n\t\thl_fence_put(ctx->cs_pending[i]);\n\n\tkfree(ctx->cs_pending);\n\n\tif (ctx->asid != HL_KERNEL_ASID_ID) {\n\t\tdev_dbg(hdev->dev, \"closing user context %d\\n\", ctx->asid);\n\n\t\t \n\t\tif (hdev->in_debug)\n\t\t\thl_device_set_debug_mode(hdev, ctx, false);\n\n\t\thdev->asic_funcs->ctx_fini(ctx);\n\n\t\thl_dec_ctx_fini(ctx);\n\n\t\thl_cb_va_pool_fini(ctx);\n\t\thl_vm_ctx_fini(ctx);\n\t\thl_asid_free(hdev, ctx->asid);\n\t\thl_encaps_sig_mgr_fini(hdev, &ctx->sig_mgr);\n\t} else {\n\t\tdev_dbg(hdev->dev, \"closing kernel context\\n\");\n\t\thdev->asic_funcs->ctx_fini(ctx);\n\t\thl_vm_ctx_fini(ctx);\n\t\thl_mmu_ctx_fini(ctx);\n\t}\n}\n\nvoid hl_ctx_do_release(struct kref *ref)\n{\n\tstruct hl_ctx *ctx;\n\n\tctx = container_of(ref, struct hl_ctx, refcount);\n\n\thl_ctx_fini(ctx);\n\n\tif (ctx->hpriv) {\n\t\tstruct hl_fpriv *hpriv = ctx->hpriv;\n\n\t\tmutex_lock(&hpriv->ctx_lock);\n\t\thpriv->ctx = NULL;\n\t\tmutex_unlock(&hpriv->ctx_lock);\n\n\t\thl_hpriv_put(hpriv);\n\t}\n\n\tkfree(ctx);\n}\n\nint hl_ctx_create(struct hl_device *hdev, struct hl_fpriv *hpriv)\n{\n\tstruct hl_ctx_mgr *ctx_mgr = &hpriv->ctx_mgr;\n\tstruct hl_ctx *ctx;\n\tint rc;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx) {\n\t\trc = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\tmutex_lock(&ctx_mgr->lock);\n\trc = idr_alloc(&ctx_mgr->handles, ctx, 1, 0, GFP_KERNEL);\n\tmutex_unlock(&ctx_mgr->lock);\n\n\tif (rc < 0) {\n\t\tdev_err(hdev->dev, \"Failed to allocate IDR for a new CTX\\n\");\n\t\tgoto free_ctx;\n\t}\n\n\tctx->handle = rc;\n\n\trc = hl_ctx_init(hdev, ctx, false);\n\tif (rc)\n\t\tgoto remove_from_idr;\n\n\thl_hpriv_get(hpriv);\n\tctx->hpriv = hpriv;\n\n\t \n\thpriv->ctx = ctx;\n\n\t \n\thdev->is_compute_ctx_active = true;\n\n\treturn 0;\n\nremove_from_idr:\n\tmutex_lock(&ctx_mgr->lock);\n\tidr_remove(&ctx_mgr->handles, ctx->handle);\n\tmutex_unlock(&ctx_mgr->lock);\nfree_ctx:\n\tkfree(ctx);\nout_err:\n\treturn rc;\n}\n\nint hl_ctx_init(struct hl_device *hdev, struct hl_ctx *ctx, bool is_kernel_ctx)\n{\n\tint rc = 0, i;\n\n\tctx->hdev = hdev;\n\n\tkref_init(&ctx->refcount);\n\n\tctx->cs_sequence = 1;\n\tspin_lock_init(&ctx->cs_lock);\n\tatomic_set(&ctx->thread_ctx_switch_token, 1);\n\tctx->thread_ctx_switch_wait_token = 0;\n\tctx->cs_pending = kcalloc(hdev->asic_prop.max_pending_cs,\n\t\t\t\tsizeof(struct hl_fence *),\n\t\t\t\tGFP_KERNEL);\n\tif (!ctx->cs_pending)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&ctx->outcome_store.used_list);\n\tINIT_LIST_HEAD(&ctx->outcome_store.free_list);\n\thash_init(ctx->outcome_store.outcome_map);\n\tfor (i = 0; i < ARRAY_SIZE(ctx->outcome_store.nodes_pool); ++i)\n\t\tlist_add(&ctx->outcome_store.nodes_pool[i].list_link,\n\t\t\t &ctx->outcome_store.free_list);\n\n\thl_hw_block_mem_init(ctx);\n\n\tif (is_kernel_ctx) {\n\t\tctx->asid = HL_KERNEL_ASID_ID;  \n\t\trc = hl_vm_ctx_init(ctx);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"Failed to init mem ctx module\\n\");\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_hw_block_mem_fini;\n\t\t}\n\n\t\trc = hdev->asic_funcs->ctx_init(ctx);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"ctx_init failed\\n\");\n\t\t\tgoto err_vm_ctx_fini;\n\t\t}\n\t} else {\n\t\tctx->asid = hl_asid_alloc(hdev);\n\t\tif (!ctx->asid) {\n\t\t\tdev_err(hdev->dev, \"No free ASID, failed to create context\\n\");\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_hw_block_mem_fini;\n\t\t}\n\n\t\trc = hl_vm_ctx_init(ctx);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"Failed to init mem ctx module\\n\");\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_asid_free;\n\t\t}\n\n\t\trc = hl_cb_va_pool_init(ctx);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Failed to init VA pool for mapped CB\\n\");\n\t\t\tgoto err_vm_ctx_fini;\n\t\t}\n\n\t\trc = hdev->asic_funcs->ctx_init(ctx);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev, \"ctx_init failed\\n\");\n\t\t\tgoto err_cb_va_pool_fini;\n\t\t}\n\n\t\thl_encaps_sig_mgr_init(&ctx->sig_mgr);\n\n\t\tdev_dbg(hdev->dev, \"create user context %d\\n\", ctx->asid);\n\t}\n\n\treturn 0;\n\nerr_cb_va_pool_fini:\n\thl_cb_va_pool_fini(ctx);\nerr_vm_ctx_fini:\n\thl_vm_ctx_fini(ctx);\nerr_asid_free:\n\tif (ctx->asid != HL_KERNEL_ASID_ID)\n\t\thl_asid_free(hdev, ctx->asid);\nerr_hw_block_mem_fini:\n\thl_hw_block_mem_fini(ctx);\n\tkfree(ctx->cs_pending);\n\n\treturn rc;\n}\n\nstatic int hl_ctx_get_unless_zero(struct hl_ctx *ctx)\n{\n\treturn kref_get_unless_zero(&ctx->refcount);\n}\n\nvoid hl_ctx_get(struct hl_ctx *ctx)\n{\n\tkref_get(&ctx->refcount);\n}\n\nint hl_ctx_put(struct hl_ctx *ctx)\n{\n\treturn kref_put(&ctx->refcount, hl_ctx_do_release);\n}\n\nstruct hl_ctx *hl_get_compute_ctx(struct hl_device *hdev)\n{\n\tstruct hl_ctx *ctx = NULL;\n\tstruct hl_fpriv *hpriv;\n\n\tmutex_lock(&hdev->fpriv_list_lock);\n\n\tlist_for_each_entry(hpriv, &hdev->fpriv_list, dev_node) {\n\t\tmutex_lock(&hpriv->ctx_lock);\n\t\tctx = hpriv->ctx;\n\t\tif (ctx && !hl_ctx_get_unless_zero(ctx))\n\t\t\tctx = NULL;\n\t\tmutex_unlock(&hpriv->ctx_lock);\n\n\t\t \n\t\tbreak;\n\t}\n\n\tmutex_unlock(&hdev->fpriv_list_lock);\n\n\treturn ctx;\n}\n\n \nstatic struct hl_fence *hl_ctx_get_fence_locked(struct hl_ctx *ctx, u64 seq)\n{\n\tstruct asic_fixed_properties *asic_prop = &ctx->hdev->asic_prop;\n\tstruct hl_fence *fence;\n\n\tif (seq >= ctx->cs_sequence)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (seq + asic_prop->max_pending_cs < ctx->cs_sequence)\n\t\treturn NULL;\n\n\tfence = ctx->cs_pending[seq & (asic_prop->max_pending_cs - 1)];\n\thl_fence_get(fence);\n\treturn fence;\n}\n\nstruct hl_fence *hl_ctx_get_fence(struct hl_ctx *ctx, u64 seq)\n{\n\tstruct hl_fence *fence;\n\n\tspin_lock(&ctx->cs_lock);\n\n\tfence = hl_ctx_get_fence_locked(ctx, seq);\n\n\tspin_unlock(&ctx->cs_lock);\n\n\treturn fence;\n}\n\n \nint hl_ctx_get_fences(struct hl_ctx *ctx, u64 *seq_arr,\n\t\t\t\tstruct hl_fence **fence, u32 arr_len)\n{\n\tstruct hl_fence **fence_arr_base = fence;\n\tint i, rc = 0;\n\n\tspin_lock(&ctx->cs_lock);\n\n\tfor (i = 0; i < arr_len; i++, fence++) {\n\t\tu64 seq = seq_arr[i];\n\n\t\t*fence = hl_ctx_get_fence_locked(ctx, seq);\n\n\t\tif (IS_ERR(*fence)) {\n\t\t\tdev_err(ctx->hdev->dev,\n\t\t\t\t\"Failed to get fence for CS with seq 0x%llx\\n\",\n\t\t\t\t\tseq);\n\t\t\trc = PTR_ERR(*fence);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock(&ctx->cs_lock);\n\n\tif (rc)\n\t\thl_fences_put(fence_arr_base, i);\n\n\treturn rc;\n}\n\n \nvoid hl_ctx_mgr_init(struct hl_ctx_mgr *ctx_mgr)\n{\n\tmutex_init(&ctx_mgr->lock);\n\tidr_init(&ctx_mgr->handles);\n}\n\n \nvoid hl_ctx_mgr_fini(struct hl_device *hdev, struct hl_ctx_mgr *ctx_mgr)\n{\n\tstruct hl_ctx *ctx;\n\tstruct idr *idp;\n\tu32 id;\n\n\tidp = &ctx_mgr->handles;\n\n\tidr_for_each_entry(idp, ctx, id)\n\t\tkref_put(&ctx->refcount, hl_ctx_do_release);\n\n\tidr_destroy(&ctx_mgr->handles);\n\tmutex_destroy(&ctx_mgr->lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}