{
  "module_name": "command_buffer.c",
  "hash_id": "01db6653801037a97f91ec860a9550669158b6408cf38bd05d93af054a5ab945",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/command_buffer.c",
  "human_readable_source": "\n\n \n\n#include <uapi/drm/habanalabs_accel.h>\n#include \"habanalabs.h\"\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n\n#define CB_VA_POOL_SIZE\t\t(4UL * SZ_1G)\n\nstatic int cb_map_mem(struct hl_ctx *ctx, struct hl_cb *cb)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu32 page_size = prop->pmmu.page_size;\n\tint rc;\n\n\tif (!hdev->supports_cb_mapping) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"Mapping a CB to the device's MMU is not supported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (cb->is_mmu_mapped)\n\t\treturn 0;\n\n\tcb->roundup_size = roundup(cb->size, page_size);\n\n\tcb->virtual_addr = (u64) gen_pool_alloc(ctx->cb_va_pool, cb->roundup_size);\n\tif (!cb->virtual_addr) {\n\t\tdev_err(hdev->dev, \"Failed to allocate device virtual address for CB\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = hl_mmu_map_contiguous(ctx, cb->virtual_addr, cb->bus_address, cb->roundup_size);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to map VA %#llx to CB\\n\", cb->virtual_addr);\n\t\tgoto err_va_pool_free;\n\t}\n\n\trc = hl_mmu_invalidate_cache(hdev, false, MMU_OP_USERPTR | MMU_OP_SKIP_LOW_CACHE_INV);\n\tif (rc)\n\t\tgoto err_mmu_unmap;\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\tcb->is_mmu_mapped = true;\n\n\treturn 0;\n\nerr_mmu_unmap:\n\thl_mmu_unmap_contiguous(ctx, cb->virtual_addr, cb->roundup_size);\nerr_va_pool_free:\n\tmutex_unlock(&hdev->mmu_lock);\n\tgen_pool_free(ctx->cb_va_pool, cb->virtual_addr, cb->roundup_size);\n\n\treturn rc;\n}\n\nstatic void cb_unmap_mem(struct hl_ctx *ctx, struct hl_cb *cb)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\n\tmutex_lock(&hdev->mmu_lock);\n\thl_mmu_unmap_contiguous(ctx, cb->virtual_addr, cb->roundup_size);\n\thl_mmu_invalidate_cache(hdev, true, MMU_OP_USERPTR);\n\tmutex_unlock(&hdev->mmu_lock);\n\n\tgen_pool_free(ctx->cb_va_pool, cb->virtual_addr, cb->roundup_size);\n}\n\nstatic void cb_fini(struct hl_device *hdev, struct hl_cb *cb)\n{\n\tif (cb->is_internal)\n\t\tgen_pool_free(hdev->internal_cb_pool,\n\t\t\t\t(uintptr_t)cb->kernel_address, cb->size);\n\telse\n\t\thl_asic_dma_free_coherent(hdev, cb->size, cb->kernel_address, cb->bus_address);\n\n\tkfree(cb);\n}\n\nstatic void cb_do_release(struct hl_device *hdev, struct hl_cb *cb)\n{\n\tif (cb->is_pool) {\n\t\tatomic_set(&cb->is_handle_destroyed, 0);\n\t\tspin_lock(&hdev->cb_pool_lock);\n\t\tlist_add(&cb->pool_list, &hdev->cb_pool);\n\t\tspin_unlock(&hdev->cb_pool_lock);\n\t} else {\n\t\tcb_fini(hdev, cb);\n\t}\n}\n\nstatic struct hl_cb *hl_cb_alloc(struct hl_device *hdev, u32 cb_size,\n\t\t\t\t\tint ctx_id, bool internal_cb)\n{\n\tstruct hl_cb *cb = NULL;\n\tu32 cb_offset;\n\tvoid *p;\n\n\t \n\tif (ctx_id == HL_KERNEL_ASID_ID && !hdev->disabled)\n\t\tcb = kzalloc(sizeof(*cb), GFP_ATOMIC);\n\n\tif (!cb)\n\t\tcb = kzalloc(sizeof(*cb), GFP_KERNEL);\n\n\tif (!cb)\n\t\treturn NULL;\n\n\tif (internal_cb) {\n\t\tp = (void *) gen_pool_alloc(hdev->internal_cb_pool, cb_size);\n\t\tif (!p) {\n\t\t\tkfree(cb);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tcb_offset = p - hdev->internal_cb_pool_virt_addr;\n\t\tcb->is_internal = true;\n\t\tcb->bus_address =  hdev->internal_cb_va_base + cb_offset;\n\t} else if (ctx_id == HL_KERNEL_ASID_ID) {\n\t\tp = hl_asic_dma_alloc_coherent(hdev, cb_size, &cb->bus_address, GFP_ATOMIC);\n\t\tif (!p)\n\t\t\tp = hl_asic_dma_alloc_coherent(hdev, cb_size, &cb->bus_address, GFP_KERNEL);\n\t} else {\n\t\tp = hl_asic_dma_alloc_coherent(hdev, cb_size, &cb->bus_address,\n\t\t\t\t\t\tGFP_USER | __GFP_ZERO);\n\t}\n\n\tif (!p) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"failed to allocate %d of dma memory for CB\\n\",\n\t\t\tcb_size);\n\t\tkfree(cb);\n\t\treturn NULL;\n\t}\n\n\tcb->kernel_address = p;\n\tcb->size = cb_size;\n\n\treturn cb;\n}\n\nstruct hl_cb_mmap_mem_alloc_args {\n\tstruct hl_device *hdev;\n\tstruct hl_ctx *ctx;\n\tu32 cb_size;\n\tbool internal_cb;\n\tbool map_cb;\n};\n\nstatic void hl_cb_mmap_mem_release(struct hl_mmap_mem_buf *buf)\n{\n\tstruct hl_cb *cb = buf->private;\n\n\thl_debugfs_remove_cb(cb);\n\n\tif (cb->is_mmu_mapped)\n\t\tcb_unmap_mem(cb->ctx, cb);\n\n\thl_ctx_put(cb->ctx);\n\n\tcb_do_release(cb->hdev, cb);\n}\n\nstatic int hl_cb_mmap_mem_alloc(struct hl_mmap_mem_buf *buf, gfp_t gfp, void *args)\n{\n\tstruct hl_cb_mmap_mem_alloc_args *cb_args = args;\n\tstruct hl_cb *cb;\n\tint rc, ctx_id = cb_args->ctx->asid;\n\tbool alloc_new_cb = true;\n\n\tif (!cb_args->internal_cb) {\n\t\t \n\t\tif (cb_args->cb_size < PAGE_SIZE)\n\t\t\tcb_args->cb_size = PAGE_SIZE;\n\n\t\tif (ctx_id == HL_KERNEL_ASID_ID &&\n\t\t\t\tcb_args->cb_size <= cb_args->hdev->asic_prop.cb_pool_cb_size) {\n\n\t\t\tspin_lock(&cb_args->hdev->cb_pool_lock);\n\t\t\tif (!list_empty(&cb_args->hdev->cb_pool)) {\n\t\t\t\tcb = list_first_entry(&cb_args->hdev->cb_pool,\n\t\t\t\t\t\ttypeof(*cb), pool_list);\n\t\t\t\tlist_del(&cb->pool_list);\n\t\t\t\tspin_unlock(&cb_args->hdev->cb_pool_lock);\n\t\t\t\talloc_new_cb = false;\n\t\t\t} else {\n\t\t\t\tspin_unlock(&cb_args->hdev->cb_pool_lock);\n\t\t\t\tdev_dbg(cb_args->hdev->dev, \"CB pool is empty\\n\");\n\t\t\t}\n\t\t}\n\t}\n\n\tif (alloc_new_cb) {\n\t\tcb = hl_cb_alloc(cb_args->hdev, cb_args->cb_size, ctx_id, cb_args->internal_cb);\n\t\tif (!cb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tcb->hdev = cb_args->hdev;\n\tcb->ctx = cb_args->ctx;\n\tcb->buf = buf;\n\tcb->buf->mappable_size = cb->size;\n\tcb->buf->private = cb;\n\n\thl_ctx_get(cb->ctx);\n\n\tif (cb_args->map_cb) {\n\t\tif (ctx_id == HL_KERNEL_ASID_ID) {\n\t\t\tdev_err(cb_args->hdev->dev,\n\t\t\t\t\"CB mapping is not supported for kernel context\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto release_cb;\n\t\t}\n\n\t\trc = cb_map_mem(cb_args->ctx, cb);\n\t\tif (rc)\n\t\t\tgoto release_cb;\n\t}\n\n\thl_debugfs_add_cb(cb);\n\n\treturn 0;\n\nrelease_cb:\n\thl_ctx_put(cb->ctx);\n\tcb_do_release(cb_args->hdev, cb);\n\n\treturn rc;\n}\n\nstatic int hl_cb_mmap(struct hl_mmap_mem_buf *buf,\n\t\t\t\t      struct vm_area_struct *vma, void *args)\n{\n\tstruct hl_cb *cb = buf->private;\n\n\treturn cb->hdev->asic_funcs->mmap(cb->hdev, vma, cb->kernel_address,\n\t\t\t\t\tcb->bus_address, cb->size);\n}\n\nstatic struct hl_mmap_mem_buf_behavior cb_behavior = {\n\t.topic = \"CB\",\n\t.mem_id = HL_MMAP_TYPE_CB,\n\t.alloc = hl_cb_mmap_mem_alloc,\n\t.release = hl_cb_mmap_mem_release,\n\t.mmap = hl_cb_mmap,\n};\n\nint hl_cb_create(struct hl_device *hdev, struct hl_mem_mgr *mmg,\n\t\t\tstruct hl_ctx *ctx, u32 cb_size, bool internal_cb,\n\t\t\tbool map_cb, u64 *handle)\n{\n\tstruct hl_cb_mmap_mem_alloc_args args = {\n\t\t.hdev = hdev,\n\t\t.ctx = ctx,\n\t\t.cb_size = cb_size,\n\t\t.internal_cb = internal_cb,\n\t\t.map_cb = map_cb,\n\t};\n\tstruct hl_mmap_mem_buf *buf;\n\tint ctx_id = ctx->asid;\n\n\tif ((hdev->disabled) || (hdev->reset_info.in_reset && (ctx_id != HL_KERNEL_ASID_ID))) {\n\t\tdev_warn_ratelimited(hdev->dev,\n\t\t\t\"Device is disabled or in reset. Can't create new CBs\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tif (cb_size > SZ_2M) {\n\t\tdev_err(hdev->dev, \"CB size %d must be less than %d\\n\",\n\t\t\tcb_size, SZ_2M);\n\t\treturn -EINVAL;\n\t}\n\n\tbuf = hl_mmap_mem_buf_alloc(\n\t\tmmg, &cb_behavior,\n\t\tctx_id == HL_KERNEL_ASID_ID ? GFP_ATOMIC : GFP_KERNEL, &args);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t*handle = buf->handle;\n\n\treturn 0;\n}\n\nint hl_cb_destroy(struct hl_mem_mgr *mmg, u64 cb_handle)\n{\n\tstruct hl_cb *cb;\n\tint rc;\n\n\tcb = hl_cb_get(mmg, cb_handle);\n\tif (!cb) {\n\t\tdev_dbg(mmg->dev, \"CB destroy failed, no CB was found for handle %#llx\\n\",\n\t\t\tcb_handle);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\trc = atomic_cmpxchg(&cb->is_handle_destroyed, 0, 1);\n\thl_cb_put(cb);\n\tif (rc) {\n\t\tdev_dbg(mmg->dev, \"CB destroy failed, handle %#llx was already destroyed\\n\",\n\t\t\tcb_handle);\n\t\treturn -EINVAL;\n\t}\n\n\trc = hl_mmap_mem_buf_put_handle(mmg, cb_handle);\n\tif (rc < 0)\n\t\treturn rc;  \n\n\tif (rc == 0)\n\t\tdev_dbg(mmg->dev, \"CB 0x%llx is destroyed while still in use\\n\", cb_handle);\n\n\treturn 0;\n}\n\nstatic int hl_cb_info(struct hl_mem_mgr *mmg,\n\t\t\tu64 handle, u32 flags, u32 *usage_cnt, u64 *device_va)\n{\n\tstruct hl_cb *cb;\n\tint rc = 0;\n\n\tcb = hl_cb_get(mmg, handle);\n\tif (!cb) {\n\t\tdev_err(mmg->dev,\n\t\t\t\"CB info failed, no match to handle 0x%llx\\n\", handle);\n\t\treturn -EINVAL;\n\t}\n\n\tif (flags & HL_CB_FLAGS_GET_DEVICE_VA) {\n\t\tif (cb->is_mmu_mapped) {\n\t\t\t*device_va = cb->virtual_addr;\n\t\t} else {\n\t\t\tdev_err(mmg->dev, \"CB is not mapped to the device's MMU\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t*usage_cnt = atomic_read(&cb->cs_cnt);\n\t}\n\nout:\n\thl_cb_put(cb);\n\treturn rc;\n}\n\nint hl_cb_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tunion hl_cb_args *args = data;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tu64 handle = 0, device_va = 0;\n\tenum hl_device_status status;\n\tu32 usage_cnt = 0;\n\tint rc;\n\n\tif (!hl_device_operational(hdev, &status)) {\n\t\tdev_dbg_ratelimited(hdev->dev,\n\t\t\t\"Device is %s. Can't execute CB IOCTL\\n\",\n\t\t\thdev->status[status]);\n\t\treturn -EBUSY;\n\t}\n\n\tswitch (args->in.op) {\n\tcase HL_CB_OP_CREATE:\n\t\tif (args->in.cb_size > HL_MAX_CB_SIZE) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"User requested CB size %d must be less than %d\\n\",\n\t\t\t\targs->in.cb_size, HL_MAX_CB_SIZE);\n\t\t\trc = -EINVAL;\n\t\t} else {\n\t\t\trc = hl_cb_create(hdev, &hpriv->mem_mgr, hpriv->ctx,\n\t\t\t\t\targs->in.cb_size, false,\n\t\t\t\t\t!!(args->in.flags & HL_CB_FLAGS_MAP),\n\t\t\t\t\t&handle);\n\t\t}\n\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.cb_handle = handle;\n\t\tbreak;\n\n\tcase HL_CB_OP_DESTROY:\n\t\trc = hl_cb_destroy(&hpriv->mem_mgr,\n\t\t\t\t\targs->in.cb_handle);\n\t\tbreak;\n\n\tcase HL_CB_OP_INFO:\n\t\trc = hl_cb_info(&hpriv->mem_mgr, args->in.cb_handle,\n\t\t\t\targs->in.flags,\n\t\t\t\t&usage_cnt,\n\t\t\t\t&device_va);\n\t\tif (rc)\n\t\t\tbreak;\n\n\t\tmemset(&args->out, 0, sizeof(args->out));\n\n\t\tif (args->in.flags & HL_CB_FLAGS_GET_DEVICE_VA)\n\t\t\targs->out.device_va = device_va;\n\t\telse\n\t\t\targs->out.usage_cnt = usage_cnt;\n\t\tbreak;\n\n\tdefault:\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\n\treturn rc;\n}\n\nstruct hl_cb *hl_cb_get(struct hl_mem_mgr *mmg, u64 handle)\n{\n\tstruct hl_mmap_mem_buf *buf;\n\n\tbuf = hl_mmap_mem_buf_get(mmg, handle);\n\tif (!buf)\n\t\treturn NULL;\n\treturn buf->private;\n\n}\n\nvoid hl_cb_put(struct hl_cb *cb)\n{\n\thl_mmap_mem_buf_put(cb->buf);\n}\n\nstruct hl_cb *hl_cb_kernel_create(struct hl_device *hdev, u32 cb_size,\n\t\t\t\t\tbool internal_cb)\n{\n\tu64 cb_handle;\n\tstruct hl_cb *cb;\n\tint rc;\n\n\trc = hl_cb_create(hdev, &hdev->kernel_mem_mgr, hdev->kernel_ctx, cb_size,\n\t\t\t\tinternal_cb, false, &cb_handle);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to allocate CB for the kernel driver %d\\n\", rc);\n\t\treturn NULL;\n\t}\n\n\tcb = hl_cb_get(&hdev->kernel_mem_mgr, cb_handle);\n\t \n\tif (!cb) {\n\t\tdev_crit(hdev->dev, \"Kernel CB handle invalid 0x%x\\n\",\n\t\t\t\t(u32) cb_handle);\n\t\tgoto destroy_cb;\n\t}\n\n\treturn cb;\n\ndestroy_cb:\n\thl_cb_destroy(&hdev->kernel_mem_mgr, cb_handle);\n\n\treturn NULL;\n}\n\nint hl_cb_pool_init(struct hl_device *hdev)\n{\n\tstruct hl_cb *cb;\n\tint i;\n\n\tINIT_LIST_HEAD(&hdev->cb_pool);\n\tspin_lock_init(&hdev->cb_pool_lock);\n\n\tfor (i = 0 ; i < hdev->asic_prop.cb_pool_cb_cnt ; i++) {\n\t\tcb = hl_cb_alloc(hdev, hdev->asic_prop.cb_pool_cb_size,\n\t\t\t\tHL_KERNEL_ASID_ID, false);\n\t\tif (cb) {\n\t\t\tcb->is_pool = true;\n\t\t\tlist_add(&cb->pool_list, &hdev->cb_pool);\n\t\t} else {\n\t\t\thl_cb_pool_fini(hdev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint hl_cb_pool_fini(struct hl_device *hdev)\n{\n\tstruct hl_cb *cb, *tmp;\n\n\tlist_for_each_entry_safe(cb, tmp, &hdev->cb_pool, pool_list) {\n\t\tlist_del(&cb->pool_list);\n\t\tcb_fini(hdev, cb);\n\t}\n\n\treturn 0;\n}\n\nint hl_cb_va_pool_init(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tint rc;\n\n\tif (!hdev->supports_cb_mapping)\n\t\treturn 0;\n\n\tctx->cb_va_pool = gen_pool_create(__ffs(prop->pmmu.page_size), -1);\n\tif (!ctx->cb_va_pool) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to create VA gen pool for CB mapping\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->cb_va_pool_base = hl_reserve_va_block(hdev, ctx, HL_VA_RANGE_TYPE_HOST,\n\t\t\t\t\tCB_VA_POOL_SIZE, HL_MMU_VA_ALIGNMENT_NOT_NEEDED);\n\tif (!ctx->cb_va_pool_base) {\n\t\trc = -ENOMEM;\n\t\tgoto err_pool_destroy;\n\t}\n\trc = gen_pool_add(ctx->cb_va_pool, ctx->cb_va_pool_base, CB_VA_POOL_SIZE, -1);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to add memory to VA gen pool for CB mapping\\n\");\n\t\tgoto err_unreserve_va_block;\n\t}\n\n\treturn 0;\n\nerr_unreserve_va_block:\n\thl_unreserve_va_block(hdev, ctx, ctx->cb_va_pool_base, CB_VA_POOL_SIZE);\nerr_pool_destroy:\n\tgen_pool_destroy(ctx->cb_va_pool);\n\n\treturn rc;\n}\n\nvoid hl_cb_va_pool_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\n\tif (!hdev->supports_cb_mapping)\n\t\treturn;\n\n\tgen_pool_destroy(ctx->cb_va_pool);\n\thl_unreserve_va_block(hdev, ctx, ctx->cb_va_pool_base, CB_VA_POOL_SIZE);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}