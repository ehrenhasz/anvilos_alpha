{
  "module_name": "memory.c",
  "hash_id": "a4f8f7c821699d71750c0d03cd6319c44b0f06f8bf2a34b9a8746847e0a9e51c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/memory.c",
  "human_readable_source": "\n\n \n\n#include <uapi/drm/habanalabs_accel.h>\n#include \"habanalabs.h\"\n#include \"../include/hw_ip/mmu/mmu_general.h\"\n\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/pci-p2pdma.h>\n\nMODULE_IMPORT_NS(DMA_BUF);\n\n#define HL_MMU_DEBUG\t0\n\n \n#define DRAM_POOL_PAGE_SIZE\tSZ_8M\n\n#define MEM_HANDLE_INVALID\tULONG_MAX\n\nstatic int allocate_timestamps_buffers(struct hl_fpriv *hpriv,\n\t\t\tstruct hl_mem_in *args, u64 *handle);\n\nstatic int set_alloc_page_size(struct hl_device *hdev, struct hl_mem_in *args, u32 *page_size)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 psize;\n\n\t \n\tif (prop->supports_user_set_page_size && args->alloc.page_size) {\n\t\tpsize = args->alloc.page_size;\n\n\t\tif (!is_power_of_2(psize)) {\n\t\t\tdev_err(hdev->dev, \"user page size (%#llx) is not power of 2\\n\", psize);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else {\n\t\tpsize = prop->device_mem_alloc_default_page_size;\n\t}\n\n\t*page_size = psize;\n\n\treturn 0;\n}\n\n \n\n \nstatic int alloc_device_memory(struct hl_ctx *ctx, struct hl_mem_in *args,\n\t\t\t\tu32 *ret_handle)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_vm *vm = &hdev->vm;\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tu64 paddr = 0, total_size, num_pgs, i;\n\tu32 num_curr_pgs, page_size;\n\tbool contiguous;\n\tint handle, rc;\n\n\tnum_curr_pgs = 0;\n\n\trc = set_alloc_page_size(hdev, args, &page_size);\n\tif (rc)\n\t\treturn rc;\n\n\tnum_pgs = DIV_ROUND_UP_ULL(args->alloc.mem_size, page_size);\n\ttotal_size = num_pgs * page_size;\n\n\tif (!total_size) {\n\t\tdev_err(hdev->dev, \"Cannot allocate 0 bytes\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcontiguous = args->flags & HL_MEM_CONTIGUOUS;\n\n\tif (contiguous) {\n\t\tif (is_power_of_2(page_size))\n\t\t\tpaddr = (uintptr_t) gen_pool_dma_alloc_align(vm->dram_pg_pool,\n\t\t\t\t\t\t\t\t     total_size, NULL, page_size);\n\t\telse\n\t\t\tpaddr = gen_pool_alloc(vm->dram_pg_pool, total_size);\n\t\tif (!paddr) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Cannot allocate %llu contiguous pages with total size of %llu\\n\",\n\t\t\t\tnum_pgs, total_size);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tphys_pg_pack = kzalloc(sizeof(*phys_pg_pack), GFP_KERNEL);\n\tif (!phys_pg_pack) {\n\t\trc = -ENOMEM;\n\t\tgoto pages_pack_err;\n\t}\n\n\tphys_pg_pack->vm_type = VM_TYPE_PHYS_PACK;\n\tphys_pg_pack->asid = ctx->asid;\n\tphys_pg_pack->npages = num_pgs;\n\tphys_pg_pack->page_size = page_size;\n\tphys_pg_pack->total_size = total_size;\n\tphys_pg_pack->flags = args->flags;\n\tphys_pg_pack->contiguous = contiguous;\n\n\tphys_pg_pack->pages = kvmalloc_array(num_pgs, sizeof(u64), GFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(phys_pg_pack->pages)) {\n\t\trc = -ENOMEM;\n\t\tgoto pages_arr_err;\n\t}\n\n\tif (phys_pg_pack->contiguous) {\n\t\tfor (i = 0 ; i < num_pgs ; i++)\n\t\t\tphys_pg_pack->pages[i] = paddr + i * page_size;\n\t} else {\n\t\tfor (i = 0 ; i < num_pgs ; i++) {\n\t\t\tif (is_power_of_2(page_size))\n\t\t\t\tphys_pg_pack->pages[i] =\n\t\t\t\t\t(uintptr_t)gen_pool_dma_alloc_align(vm->dram_pg_pool,\n\t\t\t\t\t\t\t\t\t    page_size, NULL,\n\t\t\t\t\t\t\t\t\t    page_size);\n\t\t\telse\n\t\t\t\tphys_pg_pack->pages[i] = gen_pool_alloc(vm->dram_pg_pool,\n\t\t\t\t\t\t\t\t\tpage_size);\n\n\t\t\tif (!phys_pg_pack->pages[i]) {\n\t\t\t\tdev_err(hdev->dev,\n\t\t\t\t\t\"Cannot allocate device memory (out of memory)\\n\");\n\t\t\t\trc = -ENOMEM;\n\t\t\t\tgoto page_err;\n\t\t\t}\n\n\t\t\tnum_curr_pgs++;\n\t\t}\n\t}\n\n\tspin_lock(&vm->idr_lock);\n\thandle = idr_alloc(&vm->phys_pg_pack_handles, phys_pg_pack, 1, 0,\n\t\t\t\tGFP_ATOMIC);\n\tspin_unlock(&vm->idr_lock);\n\n\tif (handle < 0) {\n\t\tdev_err(hdev->dev, \"Failed to get handle for page\\n\");\n\t\trc = -EFAULT;\n\t\tgoto idr_err;\n\t}\n\n\tfor (i = 0 ; i < num_pgs ; i++)\n\t\tkref_get(&vm->dram_pg_pool_refcount);\n\n\tphys_pg_pack->handle = handle;\n\n\tatomic64_add(phys_pg_pack->total_size, &ctx->dram_phys_mem);\n\tatomic64_add(phys_pg_pack->total_size, &hdev->dram_used_mem);\n\n\t*ret_handle = handle;\n\n\treturn 0;\n\nidr_err:\npage_err:\n\tif (!phys_pg_pack->contiguous)\n\t\tfor (i = 0 ; i < num_curr_pgs ; i++)\n\t\t\tgen_pool_free(vm->dram_pg_pool, phys_pg_pack->pages[i],\n\t\t\t\t\tpage_size);\n\n\tkvfree(phys_pg_pack->pages);\npages_arr_err:\n\tkfree(phys_pg_pack);\npages_pack_err:\n\tif (contiguous)\n\t\tgen_pool_free(vm->dram_pg_pool, paddr, total_size);\n\n\treturn rc;\n}\n\n \nstatic int dma_map_host_va(struct hl_device *hdev, u64 addr, u64 size,\n\t\t\t\tstruct hl_userptr **p_userptr)\n{\n\tstruct hl_userptr *userptr;\n\tint rc;\n\n\tuserptr = kzalloc(sizeof(*userptr), GFP_KERNEL);\n\tif (!userptr) {\n\t\trc = -ENOMEM;\n\t\tgoto userptr_err;\n\t}\n\n\trc = hl_pin_host_memory(hdev, addr, size, userptr);\n\tif (rc)\n\t\tgoto pin_err;\n\n\tuserptr->dma_mapped = true;\n\tuserptr->dir = DMA_BIDIRECTIONAL;\n\tuserptr->vm_type = VM_TYPE_USERPTR;\n\n\t*p_userptr = userptr;\n\n\trc = hdev->asic_funcs->asic_dma_map_sgtable(hdev, userptr->sgt, DMA_BIDIRECTIONAL);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to map sgt with DMA region\\n\");\n\t\tgoto dma_map_err;\n\t}\n\n\treturn 0;\n\ndma_map_err:\n\thl_unpin_host_memory(hdev, userptr);\npin_err:\n\tkfree(userptr);\nuserptr_err:\n\n\treturn rc;\n}\n\n \nstatic void dma_unmap_host_va(struct hl_device *hdev,\n\t\t\t\tstruct hl_userptr *userptr)\n{\n\thl_unpin_host_memory(hdev, userptr);\n\tkfree(userptr);\n}\n\n \nstatic void dram_pg_pool_do_release(struct kref *ref)\n{\n\tstruct hl_vm *vm = container_of(ref, struct hl_vm,\n\t\t\tdram_pg_pool_refcount);\n\n\t \n\tidr_destroy(&vm->phys_pg_pack_handles);\n\tgen_pool_destroy(vm->dram_pg_pool);\n}\n\n \nstatic void free_phys_pg_pack(struct hl_device *hdev,\n\t\t\t\tstruct hl_vm_phys_pg_pack *phys_pg_pack)\n{\n\tstruct hl_vm *vm = &hdev->vm;\n\tu64 i;\n\n\tif (phys_pg_pack->created_from_userptr)\n\t\tgoto end;\n\n\tif (phys_pg_pack->contiguous) {\n\t\tgen_pool_free(vm->dram_pg_pool, phys_pg_pack->pages[0],\n\t\t\tphys_pg_pack->total_size);\n\n\t\tfor (i = 0; i < phys_pg_pack->npages ; i++)\n\t\t\tkref_put(&vm->dram_pg_pool_refcount,\n\t\t\t\tdram_pg_pool_do_release);\n\t} else {\n\t\tfor (i = 0 ; i < phys_pg_pack->npages ; i++) {\n\t\t\tgen_pool_free(vm->dram_pg_pool,\n\t\t\t\tphys_pg_pack->pages[i],\n\t\t\t\tphys_pg_pack->page_size);\n\t\t\tkref_put(&vm->dram_pg_pool_refcount,\n\t\t\t\tdram_pg_pool_do_release);\n\t\t}\n\t}\n\nend:\n\tkvfree(phys_pg_pack->pages);\n\tkfree(phys_pg_pack);\n\n\treturn;\n}\n\n \nstatic int free_device_memory(struct hl_ctx *ctx, struct hl_mem_in *args)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_vm *vm = &hdev->vm;\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tu32 handle = args->free.handle;\n\n\tspin_lock(&vm->idr_lock);\n\tphys_pg_pack = idr_find(&vm->phys_pg_pack_handles, handle);\n\tif (!phys_pg_pack) {\n\t\tspin_unlock(&vm->idr_lock);\n\t\tdev_err(hdev->dev, \"free device memory failed, no match for handle %u\\n\", handle);\n\t\treturn -EINVAL;\n\t}\n\n\tif (atomic_read(&phys_pg_pack->mapping_cnt) > 0) {\n\t\tspin_unlock(&vm->idr_lock);\n\t\tdev_err(hdev->dev, \"handle %u is mapped, cannot free\\n\", handle);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tidr_remove(&vm->phys_pg_pack_handles, handle);\n\tspin_unlock(&vm->idr_lock);\n\n\tatomic64_sub(phys_pg_pack->total_size, &ctx->dram_phys_mem);\n\tatomic64_sub(phys_pg_pack->total_size, &hdev->dram_used_mem);\n\n\tfree_phys_pg_pack(hdev, phys_pg_pack);\n\n\treturn 0;\n}\n\n \nstatic void clear_va_list_locked(struct hl_device *hdev,\n\t\tstruct list_head *va_list)\n{\n\tstruct hl_vm_va_block *va_block, *tmp;\n\n\tlist_for_each_entry_safe(va_block, tmp, va_list, node) {\n\t\tlist_del(&va_block->node);\n\t\tkfree(va_block);\n\t}\n}\n\n \nstatic void print_va_list_locked(struct hl_device *hdev,\n\t\tstruct list_head *va_list)\n{\n#if HL_MMU_DEBUG\n\tstruct hl_vm_va_block *va_block;\n\n\tdev_dbg(hdev->dev, \"print va list:\\n\");\n\n\tlist_for_each_entry(va_block, va_list, node)\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"va block, start: 0x%llx, end: 0x%llx, size: %llu\\n\",\n\t\t\tva_block->start, va_block->end, va_block->size);\n#endif\n}\n\n \nstatic void merge_va_blocks_locked(struct hl_device *hdev,\n\t\tstruct list_head *va_list, struct hl_vm_va_block *va_block)\n{\n\tstruct hl_vm_va_block *prev, *next;\n\n\tprev = list_prev_entry(va_block, node);\n\tif (&prev->node != va_list && prev->end + 1 == va_block->start) {\n\t\tprev->end = va_block->end;\n\t\tprev->size = prev->end - prev->start + 1;\n\t\tlist_del(&va_block->node);\n\t\tkfree(va_block);\n\t\tva_block = prev;\n\t}\n\n\tnext = list_next_entry(va_block, node);\n\tif (&next->node != va_list && va_block->end + 1 == next->start) {\n\t\tnext->start = va_block->start;\n\t\tnext->size = next->end - next->start + 1;\n\t\tlist_del(&va_block->node);\n\t\tkfree(va_block);\n\t}\n}\n\n \nstatic int add_va_block_locked(struct hl_device *hdev,\n\t\tstruct list_head *va_list, u64 start, u64 end)\n{\n\tstruct hl_vm_va_block *va_block, *res = NULL;\n\tu64 size = end - start + 1;\n\n\tprint_va_list_locked(hdev, va_list);\n\n\tlist_for_each_entry(va_block, va_list, node) {\n\t\t \n\t\tif (hl_mem_area_crosses_range(start, size, va_block->start,\n\t\t\t\tva_block->end)) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"block crossing ranges at start 0x%llx, end 0x%llx\\n\",\n\t\t\t\tva_block->start, va_block->end);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (va_block->end < start)\n\t\t\tres = va_block;\n\t}\n\n\tva_block = kmalloc(sizeof(*va_block), GFP_KERNEL);\n\tif (!va_block)\n\t\treturn -ENOMEM;\n\n\tva_block->start = start;\n\tva_block->end = end;\n\tva_block->size = size;\n\n\tif (!res)\n\t\tlist_add(&va_block->node, va_list);\n\telse\n\t\tlist_add(&va_block->node, &res->node);\n\n\tmerge_va_blocks_locked(hdev, va_list, va_block);\n\n\tprint_va_list_locked(hdev, va_list);\n\n\treturn 0;\n}\n\n \nstatic inline int add_va_block(struct hl_device *hdev,\n\t\tstruct hl_va_range *va_range, u64 start, u64 end)\n{\n\tint rc;\n\n\tmutex_lock(&va_range->lock);\n\trc = add_va_block_locked(hdev, &va_range->list, start, end);\n\tmutex_unlock(&va_range->lock);\n\n\treturn rc;\n}\n\n \nstatic inline bool is_hint_crossing_range(enum hl_va_range_type range_type,\n\t\tu64 start_addr, u32 size, struct asic_fixed_properties *prop) {\n\tbool range_cross;\n\n\tif (range_type == HL_VA_RANGE_TYPE_DRAM)\n\t\trange_cross =\n\t\t\thl_mem_area_crosses_range(start_addr, size,\n\t\t\tprop->hints_dram_reserved_va_range.start_addr,\n\t\t\tprop->hints_dram_reserved_va_range.end_addr);\n\telse if (range_type == HL_VA_RANGE_TYPE_HOST)\n\t\trange_cross =\n\t\t\thl_mem_area_crosses_range(start_addr,\tsize,\n\t\t\tprop->hints_host_reserved_va_range.start_addr,\n\t\t\tprop->hints_host_reserved_va_range.end_addr);\n\telse\n\t\trange_cross =\n\t\t\thl_mem_area_crosses_range(start_addr, size,\n\t\t\tprop->hints_host_hpage_reserved_va_range.start_addr,\n\t\t\tprop->hints_host_hpage_reserved_va_range.end_addr);\n\n\treturn range_cross;\n}\n\n \nstatic u64 get_va_block(struct hl_device *hdev,\n\t\t\t\tstruct hl_va_range *va_range,\n\t\t\t\tu64 size, u64 hint_addr, u32 va_block_align,\n\t\t\t\tenum hl_va_range_type range_type,\n\t\t\t\tu32 flags)\n{\n\tstruct hl_vm_va_block *va_block, *new_va_block = NULL;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 tmp_hint_addr, valid_start, valid_size, prev_start, prev_end,\n\t\talign_mask, reserved_valid_start = 0, reserved_valid_size = 0,\n\t\tdram_hint_mask = prop->dram_hints_align_mask;\n\tbool add_prev = false;\n\tbool is_align_pow_2  = is_power_of_2(va_range->page_size);\n\tbool is_hint_dram_addr = hl_is_dram_va(hdev, hint_addr);\n\tbool force_hint = flags & HL_MEM_FORCE_HINT;\n\tint rc;\n\n\tif (is_align_pow_2)\n\t\talign_mask = ~((u64)va_block_align - 1);\n\telse\n\t\t \n\t\tsize = DIV_ROUND_UP_ULL(size, va_range->page_size) *\n\t\t\t\t\t\t\tva_range->page_size;\n\n\ttmp_hint_addr = hint_addr & ~dram_hint_mask;\n\n\t \n\tif ((is_align_pow_2 && (hint_addr & (va_block_align - 1))) ||\n\t\t\t(!is_align_pow_2 && is_hint_dram_addr &&\n\t\t\tdo_div(tmp_hint_addr, va_range->page_size))) {\n\n\t\tif (force_hint) {\n\t\t\t \n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Hint address 0x%llx is not page aligned - cannot be respected\\n\",\n\t\t\t\thint_addr);\n\t\t\treturn 0;\n\t\t}\n\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Hint address 0x%llx will be ignored because it is not aligned\\n\",\n\t\t\thint_addr);\n\t\thint_addr = 0;\n\t}\n\n\tmutex_lock(&va_range->lock);\n\n\tprint_va_list_locked(hdev, &va_range->list);\n\n\tlist_for_each_entry(va_block, &va_range->list, node) {\n\t\t \n\t\tvalid_start = va_block->start;\n\n\t\tif (is_align_pow_2 && (valid_start & (va_block_align - 1))) {\n\t\t\tvalid_start &= align_mask;\n\t\t\tvalid_start += va_block_align;\n\t\t\tif (valid_start > va_block->end)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tvalid_size = va_block->end - valid_start + 1;\n\t\tif (valid_size < size)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (prop->hints_range_reservation && !hint_addr)\n\t\t\tif (is_hint_crossing_range(range_type, valid_start,\n\t\t\t\t\tsize, prop))\n\t\t\t\tcontinue;\n\n\t\t \n\t\tif (!new_va_block || (valid_size < reserved_valid_size)) {\n\t\t\tnew_va_block = va_block;\n\t\t\treserved_valid_start = valid_start;\n\t\t\treserved_valid_size = valid_size;\n\t\t}\n\n\t\tif (hint_addr && hint_addr >= valid_start &&\n\t\t\t\t\t(hint_addr + size) <= va_block->end) {\n\t\t\tnew_va_block = va_block;\n\t\t\treserved_valid_start = hint_addr;\n\t\t\treserved_valid_size = valid_size;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!new_va_block) {\n\t\tdev_err(hdev->dev, \"no available va block for size %llu\\n\",\n\t\t\t\t\t\t\t\tsize);\n\t\tgoto out;\n\t}\n\n\tif (force_hint && reserved_valid_start != hint_addr) {\n\t\t \n\t\tdev_err(hdev->dev,\n\t\t\t\"Hint address 0x%llx could not be respected\\n\",\n\t\t\thint_addr);\n\t\treserved_valid_start = 0;\n\t\tgoto out;\n\t}\n\n\t \n\tif (reserved_valid_start > new_va_block->start) {\n\t\tprev_start = new_va_block->start;\n\t\tprev_end = reserved_valid_start - 1;\n\n\t\tnew_va_block->start = reserved_valid_start;\n\t\tnew_va_block->size = reserved_valid_size;\n\n\t\tadd_prev = true;\n\t}\n\n\tif (new_va_block->size > size) {\n\t\tnew_va_block->start += size;\n\t\tnew_va_block->size = new_va_block->end - new_va_block->start + 1;\n\t} else {\n\t\tlist_del(&new_va_block->node);\n\t\tkfree(new_va_block);\n\t}\n\n\tif (add_prev) {\n\t\trc = add_va_block_locked(hdev, &va_range->list, prev_start, prev_end);\n\t\tif (rc) {\n\t\t\treserved_valid_start = 0;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tprint_va_list_locked(hdev, &va_range->list);\nout:\n\tmutex_unlock(&va_range->lock);\n\n\treturn reserved_valid_start;\n}\n\n \nu64 hl_reserve_va_block(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\tenum hl_va_range_type type, u64 size, u32 alignment)\n{\n\treturn get_va_block(hdev, ctx->va_range[type], size, 0,\n\t\t\tmax(alignment, ctx->va_range[type]->page_size),\n\t\t\ttype, 0);\n}\n\n \nstatic int hl_get_va_range_type(struct hl_ctx *ctx, u64 address, u64 size,\n\t\t\tenum hl_va_range_type *type)\n{\n\tint i;\n\n\tfor (i = 0 ; i < HL_VA_RANGE_TYPE_MAX; i++) {\n\t\tif (hl_mem_area_inside_range(address, size,\n\t\t\t\tctx->va_range[i]->start_addr,\n\t\t\t\tctx->va_range[i]->end_addr)) {\n\t\t\t*type = i;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\n \nint hl_unreserve_va_block(struct hl_device *hdev, struct hl_ctx *ctx,\n\t\tu64 start_addr, u64 size)\n{\n\tenum hl_va_range_type type;\n\tint rc;\n\n\trc = hl_get_va_range_type(ctx, start_addr, size, &type);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"cannot find va_range for va %#llx size %llu\",\n\t\t\tstart_addr, size);\n\t\treturn rc;\n\t}\n\n\trc = add_va_block(hdev, ctx->va_range[type], start_addr,\n\t\t\t\t\t\tstart_addr + size - 1);\n\tif (rc)\n\t\tdev_warn(hdev->dev,\n\t\t\t\"add va block failed for vaddr: 0x%llx\\n\", start_addr);\n\n\treturn rc;\n}\n\n \nstatic int init_phys_pg_pack_from_userptr(struct hl_ctx *ctx,\n\t\t\t\tstruct hl_userptr *userptr,\n\t\t\t\tstruct hl_vm_phys_pg_pack **pphys_pg_pack,\n\t\t\t\tbool force_regular_page)\n{\n\tu32 npages, page_size = PAGE_SIZE,\n\t\thuge_page_size = ctx->hdev->asic_prop.pmmu_huge.page_size;\n\tu32 pgs_in_huge_page = huge_page_size >> __ffs(page_size);\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tbool first = true, is_huge_page_opt;\n\tu64 page_mask, total_npages;\n\tstruct scatterlist *sg;\n\tdma_addr_t dma_addr;\n\tint rc, i, j;\n\n\tphys_pg_pack = kzalloc(sizeof(*phys_pg_pack), GFP_KERNEL);\n\tif (!phys_pg_pack)\n\t\treturn -ENOMEM;\n\n\tphys_pg_pack->vm_type = userptr->vm_type;\n\tphys_pg_pack->created_from_userptr = true;\n\tphys_pg_pack->asid = ctx->asid;\n\tatomic_set(&phys_pg_pack->mapping_cnt, 1);\n\n\tis_huge_page_opt = (force_regular_page ? false : true);\n\n\t \n\ttotal_npages = 0;\n\tfor_each_sgtable_dma_sg(userptr->sgt, sg, i) {\n\t\tnpages = hl_get_sg_info(sg, &dma_addr);\n\n\t\ttotal_npages += npages;\n\n\t\tif ((npages % pgs_in_huge_page) ||\n\t\t\t\t\t(dma_addr & (huge_page_size - 1)))\n\t\t\tis_huge_page_opt = false;\n\t}\n\n\tif (is_huge_page_opt) {\n\t\tpage_size = huge_page_size;\n\t\tdo_div(total_npages, pgs_in_huge_page);\n\t}\n\n\tpage_mask = ~(((u64) page_size) - 1);\n\n\tphys_pg_pack->pages = kvmalloc_array(total_npages, sizeof(u64),\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (ZERO_OR_NULL_PTR(phys_pg_pack->pages)) {\n\t\trc = -ENOMEM;\n\t\tgoto page_pack_arr_mem_err;\n\t}\n\n\tphys_pg_pack->npages = total_npages;\n\tphys_pg_pack->page_size = page_size;\n\tphys_pg_pack->total_size = total_npages * page_size;\n\n\tj = 0;\n\tfor_each_sgtable_dma_sg(userptr->sgt, sg, i) {\n\t\tnpages = hl_get_sg_info(sg, &dma_addr);\n\n\t\t \n\t\tif (first) {\n\t\t\tfirst = false;\n\t\t\tphys_pg_pack->offset = dma_addr & (page_size - 1);\n\t\t\tdma_addr &= page_mask;\n\t\t}\n\n\t\twhile (npages) {\n\t\t\tphys_pg_pack->pages[j++] = dma_addr;\n\t\t\tdma_addr += page_size;\n\n\t\t\tif (is_huge_page_opt)\n\t\t\t\tnpages -= pgs_in_huge_page;\n\t\t\telse\n\t\t\t\tnpages--;\n\t\t}\n\t}\n\n\t*pphys_pg_pack = phys_pg_pack;\n\n\treturn 0;\n\npage_pack_arr_mem_err:\n\tkfree(phys_pg_pack);\n\n\treturn rc;\n}\n\n \nstatic int map_phys_pg_pack(struct hl_ctx *ctx, u64 vaddr,\n\t\t\t\tstruct hl_vm_phys_pg_pack *phys_pg_pack)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tu64 next_vaddr = vaddr, paddr, mapped_pg_cnt = 0, i;\n\tu32 page_size = phys_pg_pack->page_size;\n\tint rc = 0;\n\tbool is_host_addr;\n\n\tfor (i = 0 ; i < phys_pg_pack->npages ; i++) {\n\t\tpaddr = phys_pg_pack->pages[i];\n\n\t\trc = hl_mmu_map_page(ctx, next_vaddr, paddr, page_size,\n\t\t\t\t(i + 1) == phys_pg_pack->npages);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"map failed for handle %u, npages: %llu, mapped: %llu\",\n\t\t\t\tphys_pg_pack->handle, phys_pg_pack->npages,\n\t\t\t\tmapped_pg_cnt);\n\t\t\tgoto err;\n\t\t}\n\n\t\tmapped_pg_cnt++;\n\t\tnext_vaddr += page_size;\n\t}\n\n\treturn 0;\n\nerr:\n\tis_host_addr = !hl_is_dram_va(hdev, vaddr);\n\n\tnext_vaddr = vaddr;\n\tfor (i = 0 ; i < mapped_pg_cnt ; i++) {\n\t\tif (hl_mmu_unmap_page(ctx, next_vaddr, page_size,\n\t\t\t\t\t(i + 1) == mapped_pg_cnt))\n\t\t\tdev_warn_ratelimited(hdev->dev,\n\t\t\t\t\"failed to unmap handle %u, va: 0x%llx, pa: 0x%llx, page size: %u\\n\",\n\t\t\t\t\tphys_pg_pack->handle, next_vaddr,\n\t\t\t\t\tphys_pg_pack->pages[i], page_size);\n\n\t\tnext_vaddr += page_size;\n\n\t\t \n\t\tif (hdev->pldm || (is_host_addr && (i & 0x7FFF) == 0))\n\t\t\tusleep_range(50, 200);\n\t}\n\n\treturn rc;\n}\n\n \nstatic void unmap_phys_pg_pack(struct hl_ctx *ctx, u64 vaddr,\n\t\t\t\tstruct hl_vm_phys_pg_pack *phys_pg_pack)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tu64 next_vaddr, i;\n\tbool is_host_addr;\n\tu32 page_size;\n\n\tis_host_addr = !hl_is_dram_va(hdev, vaddr);\n\tpage_size = phys_pg_pack->page_size;\n\tnext_vaddr = vaddr;\n\n\tfor (i = 0 ; i < phys_pg_pack->npages ; i++, next_vaddr += page_size) {\n\t\tif (hl_mmu_unmap_page(ctx, next_vaddr, page_size,\n\t\t\t\t       (i + 1) == phys_pg_pack->npages))\n\t\t\tdev_warn_ratelimited(hdev->dev,\n\t\t\t\"unmap failed for vaddr: 0x%llx\\n\", next_vaddr);\n\n\t\t \n\t\tif (hdev->pldm || (is_host_addr && (i & 0x7FFF) == 0))\n\t\t\tusleep_range(50, 200);\n\t}\n}\n\n \nstatic int map_device_va(struct hl_ctx *ctx, struct hl_mem_in *args, u64 *device_addr)\n{\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tenum hl_va_range_type va_range_type = 0;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_userptr *userptr = NULL;\n\tu32 handle = 0, va_block_align;\n\tstruct hl_vm_hash_node *hnode;\n\tstruct hl_vm *vm = &hdev->vm;\n\tstruct hl_va_range *va_range;\n\tbool is_userptr, do_prefetch;\n\tu64 ret_vaddr, hint_addr;\n\tenum vm_type *vm_type;\n\tint rc;\n\n\t \n\tis_userptr = args->flags & HL_MEM_USERPTR;\n\tdo_prefetch = hdev->supports_mmu_prefetch && (args->flags & HL_MEM_PREFETCH);\n\n\t \n\t*device_addr = 0;\n\n\tif (is_userptr) {\n\t\tu64 addr = args->map_host.host_virt_addr,\n\t\t\tsize = args->map_host.mem_size;\n\t\tu32 page_size = hdev->asic_prop.pmmu.page_size,\n\t\t\thuge_page_size = hdev->asic_prop.pmmu_huge.page_size;\n\n\t\trc = dma_map_host_va(hdev, addr, size, &userptr);\n\t\tif (rc)\n\t\t\treturn rc;\n\n\t\trc = init_phys_pg_pack_from_userptr(ctx, userptr,\n\t\t\t\t&phys_pg_pack, false);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"unable to init page pack for vaddr 0x%llx\\n\",\n\t\t\t\taddr);\n\t\t\tgoto init_page_pack_err;\n\t\t}\n\n\t\tvm_type = (enum vm_type *) userptr;\n\t\thint_addr = args->map_host.hint_addr;\n\t\thandle = phys_pg_pack->handle;\n\n\t\t \n\t\tif (phys_pg_pack->page_size == page_size) {\n\t\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_HOST];\n\t\t\tva_range_type = HL_VA_RANGE_TYPE_HOST;\n\t\t\t \n\t\t\tif (addr & (huge_page_size - 1))\n\t\t\t\tva_block_align = page_size;\n\t\t\telse\n\t\t\t\tva_block_align = huge_page_size;\n\t\t} else {\n\t\t\t \n\t\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE];\n\t\t\tva_range_type = HL_VA_RANGE_TYPE_HOST_HUGE;\n\t\t\tva_block_align = huge_page_size;\n\t\t}\n\t} else {\n\t\thandle = lower_32_bits(args->map_device.handle);\n\n\t\tspin_lock(&vm->idr_lock);\n\t\tphys_pg_pack = idr_find(&vm->phys_pg_pack_handles, handle);\n\t\tif (!phys_pg_pack) {\n\t\t\tspin_unlock(&vm->idr_lock);\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"no match for handle %u\\n\", handle);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tatomic_inc(&phys_pg_pack->mapping_cnt);\n\n\t\tspin_unlock(&vm->idr_lock);\n\n\t\tvm_type = (enum vm_type *) phys_pg_pack;\n\n\t\thint_addr = args->map_device.hint_addr;\n\n\t\t \n\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_DRAM];\n\t\tva_range_type = HL_VA_RANGE_TYPE_DRAM;\n\t\tva_block_align = hdev->asic_prop.dmmu.page_size;\n\t}\n\n\t \n\tif (!is_userptr && !(phys_pg_pack->flags & HL_MEM_SHARED) &&\n\t\t\tphys_pg_pack->asid != ctx->asid) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to map memory, handle %u is not shared\\n\",\n\t\t\thandle);\n\t\trc = -EPERM;\n\t\tgoto shared_err;\n\t}\n\n\thnode = kzalloc(sizeof(*hnode), GFP_KERNEL);\n\tif (!hnode) {\n\t\trc = -ENOMEM;\n\t\tgoto hnode_err;\n\t}\n\n\tif (hint_addr && phys_pg_pack->offset) {\n\t\tif (args->flags & HL_MEM_FORCE_HINT) {\n\t\t\t \n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"Hint address 0x%llx cannot be respected because source memory is not aligned 0x%x\\n\",\n\t\t\t\thint_addr, phys_pg_pack->offset);\n\t\t\trc = -EINVAL;\n\t\t\tgoto va_block_err;\n\t\t}\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"Hint address 0x%llx will be ignored because source memory is not aligned 0x%x\\n\",\n\t\t\thint_addr, phys_pg_pack->offset);\n\t}\n\n\tret_vaddr = get_va_block(hdev, va_range, phys_pg_pack->total_size,\n\t\t\t\t\thint_addr, va_block_align,\n\t\t\t\t\tva_range_type, args->flags);\n\tif (!ret_vaddr) {\n\t\tdev_err(hdev->dev, \"no available va block for handle %u\\n\",\n\t\t\t\thandle);\n\t\trc = -ENOMEM;\n\t\tgoto va_block_err;\n\t}\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\trc = map_phys_pg_pack(ctx, ret_vaddr, phys_pg_pack);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"mapping page pack failed for handle %u\\n\", handle);\n\t\tmutex_unlock(&hdev->mmu_lock);\n\t\tgoto map_err;\n\t}\n\n\trc = hl_mmu_invalidate_cache_range(hdev, false, *vm_type | MMU_OP_SKIP_LOW_CACHE_INV,\n\t\t\t\tctx->asid, ret_vaddr, phys_pg_pack->total_size);\n\tmutex_unlock(&hdev->mmu_lock);\n\tif (rc)\n\t\tgoto map_err;\n\n\t \n\tif (do_prefetch) {\n\t\trc = hl_mmu_prefetch_cache_range(ctx, *vm_type, ctx->asid, ret_vaddr,\n\t\t\t\t\t\t\tphys_pg_pack->total_size);\n\t\tif (rc)\n\t\t\tgoto map_err;\n\t}\n\n\tret_vaddr += phys_pg_pack->offset;\n\n\thnode->ptr = vm_type;\n\thnode->vaddr = ret_vaddr;\n\thnode->handle = is_userptr ? MEM_HANDLE_INVALID : handle;\n\n\tmutex_lock(&ctx->mem_hash_lock);\n\thash_add(ctx->mem_hash, &hnode->node, ret_vaddr);\n\tmutex_unlock(&ctx->mem_hash_lock);\n\n\t*device_addr = ret_vaddr;\n\n\tif (is_userptr)\n\t\tfree_phys_pg_pack(hdev, phys_pg_pack);\n\n\treturn rc;\n\nmap_err:\n\tif (add_va_block(hdev, va_range, ret_vaddr,\n\t\t\t\tret_vaddr + phys_pg_pack->total_size - 1))\n\t\tdev_warn(hdev->dev,\n\t\t\t\"release va block failed for handle 0x%x, vaddr: 0x%llx\\n\",\n\t\t\t\thandle, ret_vaddr);\n\nva_block_err:\n\tkfree(hnode);\nhnode_err:\nshared_err:\n\tatomic_dec(&phys_pg_pack->mapping_cnt);\n\tif (is_userptr)\n\t\tfree_phys_pg_pack(hdev, phys_pg_pack);\ninit_page_pack_err:\n\tif (is_userptr)\n\t\tdma_unmap_host_va(hdev, userptr);\n\n\treturn rc;\n}\n\n \nstatic struct hl_vm_hash_node *get_vm_hash_node_locked(struct hl_ctx *ctx, u64 vaddr)\n{\n\tstruct hl_vm_hash_node *hnode;\n\n\thash_for_each_possible(ctx->mem_hash, hnode, node, vaddr)\n\t\tif (vaddr == hnode->vaddr)\n\t\t\treturn hnode;\n\n\treturn NULL;\n}\n\n \nstatic int unmap_device_va(struct hl_ctx *ctx, struct hl_mem_in *args,\n\t\t\t\tbool ctx_free)\n{\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack = NULL;\n\tu64 vaddr = args->unmap.device_virt_addr;\n\tstruct asic_fixed_properties *prop;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_userptr *userptr = NULL;\n\tstruct hl_vm_hash_node *hnode;\n\tstruct hl_va_range *va_range;\n\tenum vm_type *vm_type;\n\tbool is_userptr;\n\tint rc = 0;\n\n\tprop = &hdev->asic_prop;\n\n\t \n\tmutex_lock(&ctx->mem_hash_lock);\n\thnode = get_vm_hash_node_locked(ctx, vaddr);\n\tif (!hnode) {\n\t\tmutex_unlock(&ctx->mem_hash_lock);\n\t\tdev_err(hdev->dev, \"unmap failed, no mem hnode for vaddr 0x%llx\\n\", vaddr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (hnode->export_cnt) {\n\t\tmutex_unlock(&ctx->mem_hash_lock);\n\t\tdev_err(hdev->dev, \"failed to unmap %#llx, memory is exported\\n\", vaddr);\n\t\treturn -EINVAL;\n\t}\n\n\thash_del(&hnode->node);\n\tmutex_unlock(&ctx->mem_hash_lock);\n\n\tvm_type = hnode->ptr;\n\n\tif (*vm_type == VM_TYPE_USERPTR) {\n\t\tis_userptr = true;\n\t\tuserptr = hnode->ptr;\n\n\t\trc = init_phys_pg_pack_from_userptr(ctx, userptr, &phys_pg_pack,\n\t\t\t\t\t\t\tfalse);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"unable to init page pack for vaddr 0x%llx\\n\",\n\t\t\t\tvaddr);\n\t\t\tgoto vm_type_err;\n\t\t}\n\n\t\tif (phys_pg_pack->page_size ==\n\t\t\t\t\thdev->asic_prop.pmmu.page_size)\n\t\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_HOST];\n\t\telse\n\t\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE];\n\t} else if (*vm_type == VM_TYPE_PHYS_PACK) {\n\t\tis_userptr = false;\n\t\tva_range = ctx->va_range[HL_VA_RANGE_TYPE_DRAM];\n\t\tphys_pg_pack = hnode->ptr;\n\t} else {\n\t\tdev_warn(hdev->dev,\n\t\t\t\"unmap failed, unknown vm desc for vaddr 0x%llx\\n\",\n\t\t\t\tvaddr);\n\t\trc = -EFAULT;\n\t\tgoto vm_type_err;\n\t}\n\n\tif (atomic_read(&phys_pg_pack->mapping_cnt) == 0) {\n\t\tdev_err(hdev->dev, \"vaddr 0x%llx is not mapped\\n\", vaddr);\n\t\trc = -EINVAL;\n\t\tgoto mapping_cnt_err;\n\t}\n\n\tif (!is_userptr && !is_power_of_2(phys_pg_pack->page_size))\n\t\tvaddr = prop->dram_base_address +\n\t\t\tDIV_ROUND_DOWN_ULL(vaddr - prop->dram_base_address,\n\t\t\t\t\t\tphys_pg_pack->page_size) *\n\t\t\t\t\t\t\tphys_pg_pack->page_size;\n\telse\n\t\tvaddr &= ~(((u64) phys_pg_pack->page_size) - 1);\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\tunmap_phys_pg_pack(ctx, vaddr, phys_pg_pack);\n\n\t \n\tif (!ctx_free)\n\t\trc = hl_mmu_invalidate_cache_range(hdev, true, *vm_type, ctx->asid, vaddr,\n\t\t\t\t\t\t\tphys_pg_pack->total_size);\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\t \n\tif (!ctx_free) {\n\t\tint tmp_rc;\n\n\t\ttmp_rc = add_va_block(hdev, va_range, vaddr,\n\t\t\t\t\tvaddr + phys_pg_pack->total_size - 1);\n\t\tif (tmp_rc) {\n\t\t\tdev_warn(hdev->dev,\n\t\t\t\t\t\"add va block failed for vaddr: 0x%llx\\n\",\n\t\t\t\t\tvaddr);\n\t\t\tif (!rc)\n\t\t\t\trc = tmp_rc;\n\t\t}\n\t}\n\n\tatomic_dec(&phys_pg_pack->mapping_cnt);\n\tkfree(hnode);\n\n\tif (is_userptr) {\n\t\tfree_phys_pg_pack(hdev, phys_pg_pack);\n\t\tdma_unmap_host_va(hdev, userptr);\n\t}\n\n\treturn rc;\n\nmapping_cnt_err:\n\tif (is_userptr)\n\t\tfree_phys_pg_pack(hdev, phys_pg_pack);\nvm_type_err:\n\tmutex_lock(&ctx->mem_hash_lock);\n\thash_add(ctx->mem_hash, &hnode->node, vaddr);\n\tmutex_unlock(&ctx->mem_hash_lock);\n\n\treturn rc;\n}\n\nstatic int map_block(struct hl_device *hdev, u64 address, u64 *handle, u32 *size)\n{\n\tu32 block_id;\n\tint rc;\n\n\t*handle = 0;\n\tif (size)\n\t\t*size = 0;\n\n\trc = hdev->asic_funcs->get_hw_block_id(hdev, address, size, &block_id);\n\tif (rc)\n\t\treturn rc;\n\n\t*handle = block_id | HL_MMAP_TYPE_BLOCK;\n\t*handle <<= PAGE_SHIFT;\n\n\treturn 0;\n}\n\nstatic void hw_block_vm_close(struct vm_area_struct *vma)\n{\n\tstruct hl_vm_hw_block_list_node *lnode =\n\t\t(struct hl_vm_hw_block_list_node *) vma->vm_private_data;\n\tstruct hl_ctx *ctx = lnode->ctx;\n\tlong new_mmap_size;\n\n\tnew_mmap_size = lnode->mapped_size - (vma->vm_end - vma->vm_start);\n\tif (new_mmap_size > 0) {\n\t\tlnode->mapped_size = new_mmap_size;\n\t\treturn;\n\t}\n\n\tmutex_lock(&ctx->hw_block_list_lock);\n\tlist_del(&lnode->node);\n\tmutex_unlock(&ctx->hw_block_list_lock);\n\thl_ctx_put(ctx);\n\tkfree(lnode);\n\tvma->vm_private_data = NULL;\n}\n\nstatic const struct vm_operations_struct hw_block_vm_ops = {\n\t.close = hw_block_vm_close\n};\n\n \nint hl_hw_block_mmap(struct hl_fpriv *hpriv, struct vm_area_struct *vma)\n{\n\tstruct hl_vm_hw_block_list_node *lnode;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tu32 block_id, block_size;\n\tint rc;\n\n\t \n\tblock_id = vma->vm_pgoff;\n\tvma->vm_pgoff = 0;\n\n\t \n\tblock_size = vma->vm_end - vma->vm_start;\n\n\tif (!access_ok((void __user *) (uintptr_t) vma->vm_start, block_size)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"user pointer is invalid - 0x%lx\\n\",\n\t\t\tvma->vm_start);\n\n\t\treturn -EINVAL;\n\t}\n\n\tlnode = kzalloc(sizeof(*lnode), GFP_KERNEL);\n\tif (!lnode)\n\t\treturn -ENOMEM;\n\n\trc = hdev->asic_funcs->hw_block_mmap(hdev, vma, block_id, block_size);\n\tif (rc) {\n\t\tkfree(lnode);\n\t\treturn rc;\n\t}\n\n\thl_ctx_get(ctx);\n\n\tlnode->ctx = ctx;\n\tlnode->vaddr = vma->vm_start;\n\tlnode->block_size = block_size;\n\tlnode->mapped_size = lnode->block_size;\n\tlnode->id = block_id;\n\n\tvma->vm_private_data = lnode;\n\tvma->vm_ops = &hw_block_vm_ops;\n\n\tmutex_lock(&ctx->hw_block_list_lock);\n\tlist_add_tail(&lnode->node, &ctx->hw_block_mem_list);\n\tmutex_unlock(&ctx->hw_block_list_lock);\n\n\tvma->vm_pgoff = block_id;\n\n\treturn 0;\n}\n\nstatic int set_dma_sg(struct scatterlist *sg, u64 bar_address, u64 chunk_size,\n\t\t\tstruct device *dev, enum dma_data_direction dir)\n{\n\tdma_addr_t addr;\n\tint rc;\n\n\taddr = dma_map_resource(dev, bar_address, chunk_size, dir,\n\t\t\t\tDMA_ATTR_SKIP_CPU_SYNC);\n\trc = dma_mapping_error(dev, addr);\n\tif (rc)\n\t\treturn rc;\n\n\tsg_set_page(sg, NULL, chunk_size, 0);\n\tsg_dma_address(sg) = addr;\n\tsg_dma_len(sg) = chunk_size;\n\n\treturn 0;\n}\n\nstatic struct sg_table *alloc_sgt_from_device_pages(struct hl_device *hdev, u64 *pages, u64 npages,\n\t\t\t\t\t\tu64 page_size, u64 exported_size,\n\t\t\t\t\t\tstruct device *dev, enum dma_data_direction dir)\n{\n\tu64 chunk_size, bar_address, dma_max_seg_size, cur_size_to_export, cur_npages;\n\tstruct asic_fixed_properties *prop;\n\tint rc, i, j, nents, cur_page;\n\tstruct scatterlist *sg;\n\tstruct sg_table *sgt;\n\n\tprop = &hdev->asic_prop;\n\n\tdma_max_seg_size = dma_get_max_seg_size(dev);\n\n\t \n\tdma_max_seg_size = ALIGN_DOWN(dma_max_seg_size, PAGE_SIZE);\n\tif (dma_max_seg_size < PAGE_SIZE) {\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"dma_max_seg_size %llu can't be smaller than PAGE_SIZE\\n\",\n\t\t\t\tdma_max_seg_size);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tsgt = kzalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tcur_size_to_export = exported_size ? exported_size : (npages * page_size);\n\n\t \n\tif (page_size > dma_max_seg_size) {\n\t\t \n\t\tcur_npages = DIV_ROUND_UP_SECTOR_T(cur_size_to_export, page_size);\n\t\tnents = cur_npages * DIV_ROUND_UP_SECTOR_T(page_size, dma_max_seg_size);\n\t} else {\n\t\tcur_npages = npages;\n\n\t\t \n\t\tfor (i = 1, nents = 1, chunk_size = page_size ; i < cur_npages ; i++) {\n\t\t\tif (pages[i - 1] + page_size != pages[i] ||\n\t\t\t\t\tchunk_size + page_size > dma_max_seg_size) {\n\t\t\t\tnents++;\n\t\t\t\tchunk_size = page_size;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tchunk_size += page_size;\n\t\t}\n\t}\n\n\trc = sg_alloc_table(sgt, nents, GFP_KERNEL | __GFP_ZERO);\n\tif (rc)\n\t\tgoto error_free;\n\n\tcur_page = 0;\n\n\tif (page_size > dma_max_seg_size) {\n\t\tu64 size_left, cur_device_address = 0;\n\n\t\tsize_left = page_size;\n\n\t\t \n\t\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\t\tif (size_left == page_size)\n\t\t\t\tcur_device_address =\n\t\t\t\t\tpages[cur_page] - prop->dram_base_address;\n\t\t\telse\n\t\t\t\tcur_device_address += dma_max_seg_size;\n\n\t\t\t \n\t\t\tchunk_size = min3(size_left, dma_max_seg_size, cur_size_to_export);\n\n\t\t\tbar_address = hdev->dram_pci_bar_start + cur_device_address;\n\n\t\t\trc = set_dma_sg(sg, bar_address, chunk_size, dev, dir);\n\t\t\tif (rc)\n\t\t\t\tgoto error_unmap;\n\n\t\t\tcur_size_to_export -= chunk_size;\n\n\t\t\tif (size_left > dma_max_seg_size) {\n\t\t\t\tsize_left -= dma_max_seg_size;\n\t\t\t} else {\n\t\t\t\tcur_page++;\n\t\t\t\tsize_left = page_size;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\t\tchunk_size = page_size;\n\t\t\tfor (j = cur_page + 1 ; j < cur_npages ; j++) {\n\t\t\t\tif (pages[j - 1] + page_size != pages[j] ||\n\t\t\t\t\t\tchunk_size + page_size > dma_max_seg_size)\n\t\t\t\t\tbreak;\n\n\t\t\t\tchunk_size += page_size;\n\t\t\t}\n\n\t\t\tbar_address = hdev->dram_pci_bar_start +\n\t\t\t\t\t(pages[cur_page] - prop->dram_base_address);\n\n\t\t\t \n\t\t\tchunk_size = min(chunk_size, cur_size_to_export);\n\t\t\trc = set_dma_sg(sg, bar_address, chunk_size, dev, dir);\n\t\t\tif (rc)\n\t\t\t\tgoto error_unmap;\n\n\t\t\tcur_size_to_export -= chunk_size;\n\t\t\tcur_page = j;\n\t\t}\n\t}\n\n\t \n\tsgt->orig_nents = 0;\n\n\treturn sgt;\n\nerror_unmap:\n\tfor_each_sgtable_dma_sg(sgt, sg, i) {\n\t\tif (!sg_dma_len(sg))\n\t\t\tcontinue;\n\n\t\tdma_unmap_resource(dev, sg_dma_address(sg),\n\t\t\t\t\tsg_dma_len(sg), dir,\n\t\t\t\t\tDMA_ATTR_SKIP_CPU_SYNC);\n\t}\n\n\tsg_free_table(sgt);\n\nerror_free:\n\tkfree(sgt);\n\treturn ERR_PTR(rc);\n}\n\nstatic int hl_dmabuf_attach(struct dma_buf *dmabuf,\n\t\t\t\tstruct dma_buf_attachment *attachment)\n{\n\tstruct hl_dmabuf_priv *hl_dmabuf;\n\tstruct hl_device *hdev;\n\tint rc;\n\n\thl_dmabuf = dmabuf->priv;\n\thdev = hl_dmabuf->ctx->hdev;\n\n\trc = pci_p2pdma_distance(hdev->pdev, attachment->dev, true);\n\n\tif (rc < 0)\n\t\tattachment->peer2peer = false;\n\treturn 0;\n}\n\nstatic struct sg_table *hl_map_dmabuf(struct dma_buf_attachment *attachment,\n\t\t\t\t\tenum dma_data_direction dir)\n{\n\tstruct dma_buf *dma_buf = attachment->dmabuf;\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tstruct hl_dmabuf_priv *hl_dmabuf;\n\tstruct hl_device *hdev;\n\tstruct sg_table *sgt;\n\n\thl_dmabuf = dma_buf->priv;\n\thdev = hl_dmabuf->ctx->hdev;\n\tphys_pg_pack = hl_dmabuf->phys_pg_pack;\n\n\tif (!attachment->peer2peer) {\n\t\tdev_dbg(hdev->dev, \"Failed to map dmabuf because p2p is disabled\\n\");\n\t\treturn ERR_PTR(-EPERM);\n\t}\n\n\tif (phys_pg_pack)\n\t\tsgt = alloc_sgt_from_device_pages(hdev,\n\t\t\t\t\t\tphys_pg_pack->pages,\n\t\t\t\t\t\tphys_pg_pack->npages,\n\t\t\t\t\t\tphys_pg_pack->page_size,\n\t\t\t\t\t\tphys_pg_pack->exported_size,\n\t\t\t\t\t\tattachment->dev,\n\t\t\t\t\t\tdir);\n\telse\n\t\tsgt = alloc_sgt_from_device_pages(hdev,\n\t\t\t\t\t\t&hl_dmabuf->device_address,\n\t\t\t\t\t\t1,\n\t\t\t\t\t\thl_dmabuf->dmabuf->size,\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tattachment->dev,\n\t\t\t\t\t\tdir);\n\n\tif (IS_ERR(sgt))\n\t\tdev_err(hdev->dev, \"failed (%ld) to initialize sgt for dmabuf\\n\", PTR_ERR(sgt));\n\n\treturn sgt;\n}\n\nstatic void hl_unmap_dmabuf(struct dma_buf_attachment *attachment,\n\t\t\t\t  struct sg_table *sgt,\n\t\t\t\t  enum dma_data_direction dir)\n{\n\tstruct scatterlist *sg;\n\tint i;\n\n\t \n\tfor_each_sgtable_dma_sg(sgt, sg, i)\n\t\tdma_unmap_resource(attachment->dev, sg_dma_address(sg),\n\t\t\t\t\tsg_dma_len(sg), dir,\n\t\t\t\t\tDMA_ATTR_SKIP_CPU_SYNC);\n\n\t \n\tsgt->orig_nents = sgt->nents;\n\tsg_free_table(sgt);\n\tkfree(sgt);\n}\n\nstatic struct hl_vm_hash_node *memhash_node_export_get(struct hl_ctx *ctx, u64 addr)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_vm_hash_node *hnode;\n\n\t \n\tmutex_lock(&ctx->mem_hash_lock);\n\thnode = get_vm_hash_node_locked(ctx, addr);\n\tif (!hnode) {\n\t\tmutex_unlock(&ctx->mem_hash_lock);\n\t\tdev_dbg(hdev->dev, \"map address %#llx not found\\n\", addr);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (upper_32_bits(hnode->handle)) {\n\t\tmutex_unlock(&ctx->mem_hash_lock);\n\t\tdev_dbg(hdev->dev, \"invalid handle %#llx for map address %#llx\\n\",\n\t\t\t\thnode->handle, addr);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t \n\thnode->export_cnt++;\n\tmutex_unlock(&ctx->mem_hash_lock);\n\n\treturn hnode;\n}\n\nstatic void memhash_node_export_put(struct hl_ctx *ctx, struct hl_vm_hash_node *hnode)\n{\n\tmutex_lock(&ctx->mem_hash_lock);\n\thnode->export_cnt--;\n\tmutex_unlock(&ctx->mem_hash_lock);\n}\n\nstatic void hl_release_dmabuf(struct dma_buf *dmabuf)\n{\n\tstruct hl_dmabuf_priv *hl_dmabuf = dmabuf->priv;\n\tstruct hl_ctx *ctx;\n\n\tif (!hl_dmabuf)\n\t\treturn;\n\n\tctx = hl_dmabuf->ctx;\n\n\tif (hl_dmabuf->memhash_hnode)\n\t\tmemhash_node_export_put(ctx, hl_dmabuf->memhash_hnode);\n\n\tatomic_dec(&ctx->hdev->dmabuf_export_cnt);\n\thl_ctx_put(ctx);\n\n\t \n\tfput(ctx->hpriv->filp);\n\n\tkfree(hl_dmabuf);\n}\n\nstatic const struct dma_buf_ops habanalabs_dmabuf_ops = {\n\t.attach = hl_dmabuf_attach,\n\t.map_dma_buf = hl_map_dmabuf,\n\t.unmap_dma_buf = hl_unmap_dmabuf,\n\t.release = hl_release_dmabuf,\n};\n\nstatic int export_dmabuf(struct hl_ctx *ctx,\n\t\t\t\tstruct hl_dmabuf_priv *hl_dmabuf,\n\t\t\t\tu64 total_size, int flags, int *dmabuf_fd)\n{\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\tstruct hl_device *hdev = ctx->hdev;\n\tint rc, fd;\n\n\texp_info.ops = &habanalabs_dmabuf_ops;\n\texp_info.size = total_size;\n\texp_info.flags = flags;\n\texp_info.priv = hl_dmabuf;\n\n\thl_dmabuf->dmabuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(hl_dmabuf->dmabuf)) {\n\t\tdev_err(hdev->dev, \"failed to export dma-buf\\n\");\n\t\treturn PTR_ERR(hl_dmabuf->dmabuf);\n\t}\n\n\tfd = dma_buf_fd(hl_dmabuf->dmabuf, flags);\n\tif (fd < 0) {\n\t\tdev_err(hdev->dev, \"failed to get a file descriptor for a dma-buf, %d\\n\", fd);\n\t\trc = fd;\n\t\tgoto err_dma_buf_put;\n\t}\n\n\thl_dmabuf->ctx = ctx;\n\thl_ctx_get(hl_dmabuf->ctx);\n\tatomic_inc(&ctx->hdev->dmabuf_export_cnt);\n\n\t \n\tget_file(ctx->hpriv->filp);\n\n\t*dmabuf_fd = fd;\n\n\treturn 0;\n\nerr_dma_buf_put:\n\thl_dmabuf->dmabuf->priv = NULL;\n\tdma_buf_put(hl_dmabuf->dmabuf);\n\treturn rc;\n}\n\nstatic int validate_export_params_common(struct hl_device *hdev, u64 device_addr, u64 size)\n{\n\tif (!IS_ALIGNED(device_addr, PAGE_SIZE)) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"exported device memory address 0x%llx should be aligned to 0x%lx\\n\",\n\t\t\tdevice_addr, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tif (size < PAGE_SIZE) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"exported device memory size %llu should be equal to or greater than %lu\\n\",\n\t\t\tsize, PAGE_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_export_params_no_mmu(struct hl_device *hdev, u64 device_addr, u64 size)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 bar_address;\n\tint rc;\n\n\trc = validate_export_params_common(hdev, device_addr, size);\n\tif (rc)\n\t\treturn rc;\n\n\tif (device_addr < prop->dram_user_base_address ||\n\t\t\t\t(device_addr + size) > prop->dram_end_address ||\n\t\t\t\t(device_addr + size) < device_addr) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"DRAM memory range 0x%llx (+0x%llx) is outside of DRAM boundaries\\n\",\n\t\t\tdevice_addr, size);\n\t\treturn -EINVAL;\n\t}\n\n\tbar_address = hdev->dram_pci_bar_start + (device_addr - prop->dram_base_address);\n\n\tif ((bar_address + size) > (hdev->dram_pci_bar_start + prop->dram_pci_bar_size) ||\n\t\t\t(bar_address + size) < bar_address) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"DRAM memory range 0x%llx (+0x%llx) is outside of PCI BAR boundaries\\n\",\n\t\t\tdevice_addr, size);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int validate_export_params(struct hl_device *hdev, u64 device_addr, u64 size, u64 offset,\n\t\t\t\t\tstruct hl_vm_phys_pg_pack *phys_pg_pack)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tu64 bar_address;\n\tint i, rc;\n\n\trc = validate_export_params_common(hdev, device_addr, size);\n\tif (rc)\n\t\treturn rc;\n\n\tif ((offset + size) > phys_pg_pack->total_size) {\n\t\tdev_dbg(hdev->dev, \"offset %#llx and size %#llx exceed total map size %#llx\\n\",\n\t\t\t\toffset, size, phys_pg_pack->total_size);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0 ; i < phys_pg_pack->npages ; i++) {\n\n\t\tbar_address = hdev->dram_pci_bar_start +\n\t\t\t\t\t(phys_pg_pack->pages[i] - prop->dram_base_address);\n\n\t\tif ((bar_address + phys_pg_pack->page_size) >\n\t\t\t\t(hdev->dram_pci_bar_start + prop->dram_pci_bar_size) ||\n\t\t\t\t(bar_address + phys_pg_pack->page_size) < bar_address) {\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"DRAM memory range 0x%llx (+0x%x) is outside of PCI BAR boundaries\\n\",\n\t\t\t\t\tphys_pg_pack->pages[i],\n\t\t\t\t\tphys_pg_pack->page_size);\n\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic struct hl_vm_phys_pg_pack *get_phys_pg_pack_from_hash_node(struct hl_device *hdev,\n\t\t\t\t\t\t\tstruct hl_vm_hash_node *hnode)\n{\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack;\n\tstruct hl_vm *vm = &hdev->vm;\n\n\tspin_lock(&vm->idr_lock);\n\tphys_pg_pack = idr_find(&vm->phys_pg_pack_handles, (u32) hnode->handle);\n\tif (!phys_pg_pack) {\n\t\tspin_unlock(&vm->idr_lock);\n\t\tdev_dbg(hdev->dev, \"no match for handle 0x%x\\n\", (u32) hnode->handle);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tspin_unlock(&vm->idr_lock);\n\n\tif (phys_pg_pack->vm_type != VM_TYPE_PHYS_PACK) {\n\t\tdev_dbg(hdev->dev, \"handle 0x%llx does not represent DRAM memory\\n\", hnode->handle);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\treturn phys_pg_pack;\n}\n\n \nstatic int export_dmabuf_from_addr(struct hl_ctx *ctx, u64 addr, u64 size, u64 offset,\n\t\t\t\t\tint flags, int *dmabuf_fd)\n{\n\tstruct hl_vm_phys_pg_pack *phys_pg_pack = NULL;\n\tstruct hl_vm_hash_node *hnode = NULL;\n\tstruct asic_fixed_properties *prop;\n\tstruct hl_dmabuf_priv *hl_dmabuf;\n\tstruct hl_device *hdev;\n\tu64 export_addr;\n\tint rc;\n\n\thdev = ctx->hdev;\n\tprop = &hdev->asic_prop;\n\n\t \n\tif (!prop->dram_supports_virtual_memory && offset) {\n\t\tdev_dbg(hdev->dev, \"offset is not allowed in device without virtual memory\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\texport_addr = addr + offset;\n\n\thl_dmabuf = kzalloc(sizeof(*hl_dmabuf), GFP_KERNEL);\n\tif (!hl_dmabuf)\n\t\treturn -ENOMEM;\n\n\tif (prop->dram_supports_virtual_memory) {\n\t\thnode = memhash_node_export_get(ctx, addr);\n\t\tif (IS_ERR(hnode)) {\n\t\t\trc = PTR_ERR(hnode);\n\t\t\tgoto err_free_dmabuf_wrapper;\n\t\t}\n\t\tphys_pg_pack = get_phys_pg_pack_from_hash_node(hdev, hnode);\n\t\tif (IS_ERR(phys_pg_pack)) {\n\t\t\trc = PTR_ERR(phys_pg_pack);\n\t\t\tgoto dec_memhash_export_cnt;\n\t\t}\n\t\trc = validate_export_params(hdev, export_addr, size, offset, phys_pg_pack);\n\t\tif (rc)\n\t\t\tgoto dec_memhash_export_cnt;\n\n\t\tphys_pg_pack->exported_size = size;\n\t\thl_dmabuf->phys_pg_pack = phys_pg_pack;\n\t\thl_dmabuf->memhash_hnode = hnode;\n\t} else {\n\t\trc = validate_export_params_no_mmu(hdev, export_addr, size);\n\t\tif (rc)\n\t\t\tgoto err_free_dmabuf_wrapper;\n\t}\n\n\thl_dmabuf->device_address = export_addr;\n\n\trc = export_dmabuf(ctx, hl_dmabuf, size, flags, dmabuf_fd);\n\tif (rc)\n\t\tgoto dec_memhash_export_cnt;\n\n\treturn 0;\n\ndec_memhash_export_cnt:\n\tif (prop->dram_supports_virtual_memory)\n\t\tmemhash_node_export_put(ctx, hnode);\nerr_free_dmabuf_wrapper:\n\tkfree(hl_dmabuf);\n\treturn rc;\n}\n\nstatic void ts_buff_release(struct hl_mmap_mem_buf *buf)\n{\n\tstruct hl_ts_buff *ts_buff = buf->private;\n\n\tvfree(ts_buff->kernel_buff_address);\n\tvfree(ts_buff->user_buff_address);\n\tkfree(ts_buff);\n}\n\nstatic int hl_ts_mmap(struct hl_mmap_mem_buf *buf, struct vm_area_struct *vma, void *args)\n{\n\tstruct hl_ts_buff *ts_buff = buf->private;\n\n\tvm_flags_set(vma, VM_DONTEXPAND | VM_DONTDUMP | VM_DONTCOPY | VM_NORESERVE);\n\treturn remap_vmalloc_range(vma, ts_buff->user_buff_address, 0);\n}\n\nstatic int hl_ts_alloc_buf(struct hl_mmap_mem_buf *buf, gfp_t gfp, void *args)\n{\n\tstruct hl_ts_buff *ts_buff = NULL;\n\tu32 num_elements;\n\tsize_t size;\n\tvoid *p;\n\n\tnum_elements = *(u32 *)args;\n\n\tts_buff = kzalloc(sizeof(*ts_buff), gfp);\n\tif (!ts_buff)\n\t\treturn -ENOMEM;\n\n\t \n\tsize = num_elements * sizeof(u64);\n\tp = vmalloc_user(size);\n\tif (!p)\n\t\tgoto free_mem;\n\n\tts_buff->user_buff_address = p;\n\tbuf->mappable_size = size;\n\n\t \n\tsize = num_elements * sizeof(struct hl_user_pending_interrupt);\n\tp = vzalloc(size);\n\tif (!p)\n\t\tgoto free_user_buff;\n\n\tts_buff->kernel_buff_address = p;\n\tts_buff->kernel_buff_size = size;\n\n\tbuf->private = ts_buff;\n\n\treturn 0;\n\nfree_user_buff:\n\tvfree(ts_buff->user_buff_address);\nfree_mem:\n\tkfree(ts_buff);\n\treturn -ENOMEM;\n}\n\nstatic struct hl_mmap_mem_buf_behavior hl_ts_behavior = {\n\t.topic = \"TS\",\n\t.mem_id = HL_MMAP_TYPE_TS_BUFF,\n\t.mmap = hl_ts_mmap,\n\t.alloc = hl_ts_alloc_buf,\n\t.release = ts_buff_release,\n};\n\n \nstatic int allocate_timestamps_buffers(struct hl_fpriv *hpriv, struct hl_mem_in *args, u64 *handle)\n{\n\tstruct hl_mem_mgr *mmg = &hpriv->mem_mgr;\n\tstruct hl_mmap_mem_buf *buf;\n\n\tif (args->num_of_elements > TS_MAX_ELEMENTS_NUM) {\n\t\tdev_err(mmg->dev, \"Num of elements exceeds Max allowed number (0x%x > 0x%x)\\n\",\n\t\t\t\targs->num_of_elements, TS_MAX_ELEMENTS_NUM);\n\t\treturn -EINVAL;\n\t}\n\n\tbuf = hl_mmap_mem_buf_alloc(mmg, &hl_ts_behavior, GFP_KERNEL, &args->num_of_elements);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\t*handle = buf->handle;\n\n\treturn 0;\n}\n\nint hl_mem_ioctl(struct hl_fpriv *hpriv, void *data)\n{\n\tenum hl_device_status status;\n\tunion hl_mem_args *args = data;\n\tstruct hl_device *hdev = hpriv->hdev;\n\tstruct hl_ctx *ctx = hpriv->ctx;\n\tu64 block_handle, device_addr = 0;\n\tu32 handle = 0, block_size;\n\tint rc, dmabuf_fd = -EBADF;\n\n\tif (!hl_device_operational(hdev, &status)) {\n\t\tdev_dbg_ratelimited(hdev->dev,\n\t\t\t\"Device is %s. Can't execute MEMORY IOCTL\\n\",\n\t\t\thdev->status[status]);\n\t\treturn -EBUSY;\n\t}\n\n\tswitch (args->in.op) {\n\tcase HL_MEM_OP_ALLOC:\n\t\tif (args->in.alloc.mem_size == 0) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"alloc size must be larger than 0\\n\");\n\t\t\trc = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (!hdev->asic_prop.dram_supports_virtual_memory) {\n\t\t\tatomic64_add(args->in.alloc.mem_size,\n\t\t\t\t\t&ctx->dram_phys_mem);\n\t\t\tatomic64_add(args->in.alloc.mem_size,\n\t\t\t\t\t&hdev->dram_used_mem);\n\n\t\t\tdev_dbg(hdev->dev, \"DRAM alloc is not supported\\n\");\n\t\t\trc = 0;\n\n\t\t\tmemset(args, 0, sizeof(*args));\n\t\t\targs->out.handle = 0;\n\t\t\tgoto out;\n\t\t}\n\n\t\trc = alloc_device_memory(ctx, &args->in, &handle);\n\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.handle = (__u64) handle;\n\t\tbreak;\n\n\tcase HL_MEM_OP_FREE:\n\t\t \n\t\tif (!hdev->asic_prop.dram_supports_virtual_memory) {\n\t\t\tatomic64_sub(args->in.alloc.mem_size,\n\t\t\t\t\t&ctx->dram_phys_mem);\n\t\t\tatomic64_sub(args->in.alloc.mem_size,\n\t\t\t\t\t&hdev->dram_used_mem);\n\n\t\t\tdev_dbg(hdev->dev, \"DRAM alloc is not supported\\n\");\n\t\t\trc = 0;\n\n\t\t\tgoto out;\n\t\t}\n\n\t\trc = free_device_memory(ctx, &args->in);\n\t\tbreak;\n\n\tcase HL_MEM_OP_MAP:\n\t\trc = map_device_va(ctx, &args->in, &device_addr);\n\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.device_virt_addr = device_addr;\n\t\tbreak;\n\n\tcase HL_MEM_OP_UNMAP:\n\t\trc = unmap_device_va(ctx, &args->in, false);\n\t\tbreak;\n\n\tcase HL_MEM_OP_MAP_BLOCK:\n\t\trc = map_block(hdev, args->in.map_block.block_addr,\n\t\t\t\t&block_handle, &block_size);\n\t\targs->out.block_handle = block_handle;\n\t\targs->out.block_size = block_size;\n\t\tbreak;\n\n\tcase HL_MEM_OP_EXPORT_DMABUF_FD:\n\t\trc = export_dmabuf_from_addr(ctx,\n\t\t\t\targs->in.export_dmabuf_fd.addr,\n\t\t\t\targs->in.export_dmabuf_fd.mem_size,\n\t\t\t\targs->in.export_dmabuf_fd.offset,\n\t\t\t\targs->in.flags,\n\t\t\t\t&dmabuf_fd);\n\t\tmemset(args, 0, sizeof(*args));\n\t\targs->out.fd = dmabuf_fd;\n\t\tbreak;\n\n\tcase HL_MEM_OP_TS_ALLOC:\n\t\trc = allocate_timestamps_buffers(hpriv, &args->in, &args->out.handle);\n\t\tbreak;\n\tdefault:\n\t\tdev_err(hdev->dev, \"Unknown opcode for memory IOCTL\\n\");\n\t\trc = -EINVAL;\n\t\tbreak;\n\t}\n\nout:\n\treturn rc;\n}\n\nstatic int get_user_memory(struct hl_device *hdev, u64 addr, u64 size,\n\t\t\t\tu32 npages, u64 start, u32 offset,\n\t\t\t\tstruct hl_userptr *userptr)\n{\n\tint rc;\n\n\tif (!access_ok((void __user *) (uintptr_t) addr, size)) {\n\t\tdev_err(hdev->dev, \"user pointer is invalid - 0x%llx\\n\", addr);\n\t\treturn -EFAULT;\n\t}\n\n\tuserptr->pages = kvmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);\n\tif (!userptr->pages)\n\t\treturn -ENOMEM;\n\n\trc = pin_user_pages_fast(start, npages, FOLL_WRITE | FOLL_LONGTERM,\n\t\t\t\t userptr->pages);\n\n\tif (rc != npages) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed (%d) to pin host memory with user ptr 0x%llx, size 0x%llx, npages %d\\n\",\n\t\t\trc, addr, size, npages);\n\t\tif (rc < 0)\n\t\t\tgoto destroy_pages;\n\t\tnpages = rc;\n\t\trc = -EFAULT;\n\t\tgoto put_pages;\n\t}\n\tuserptr->npages = npages;\n\n\trc = sg_alloc_table_from_pages(userptr->sgt,\n\t\t\t\t       userptr->pages,\n\t\t\t\t       npages, offset, size, GFP_KERNEL);\n\tif (rc < 0) {\n\t\tdev_err(hdev->dev, \"failed to create SG table from pages\\n\");\n\t\tgoto put_pages;\n\t}\n\n\treturn 0;\n\nput_pages:\n\tunpin_user_pages(userptr->pages, npages);\ndestroy_pages:\n\tkvfree(userptr->pages);\n\treturn rc;\n}\n\n \nint hl_pin_host_memory(struct hl_device *hdev, u64 addr, u64 size,\n\t\t\t\t\tstruct hl_userptr *userptr)\n{\n\tu64 start, end;\n\tu32 npages, offset;\n\tint rc;\n\n\tif (!size) {\n\t\tdev_err(hdev->dev, \"size to pin is invalid - %llu\\n\", size);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (((addr + size) < addr) ||\n\t\t\tPAGE_ALIGN(addr + size) < (addr + size)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"user pointer 0x%llx + %llu causes integer overflow\\n\",\n\t\t\taddr, size);\n\t\treturn -EINVAL;\n\t}\n\n\tuserptr->pid = current->pid;\n\tuserptr->sgt = kzalloc(sizeof(*userptr->sgt), GFP_KERNEL);\n\tif (!userptr->sgt)\n\t\treturn -ENOMEM;\n\n\tstart = addr & PAGE_MASK;\n\toffset = addr & ~PAGE_MASK;\n\tend = PAGE_ALIGN(addr + size);\n\tnpages = (end - start) >> PAGE_SHIFT;\n\n\tuserptr->size = size;\n\tuserptr->addr = addr;\n\tuserptr->dma_mapped = false;\n\tINIT_LIST_HEAD(&userptr->job_node);\n\n\trc = get_user_memory(hdev, addr, size, npages, start, offset,\n\t\t\t\tuserptr);\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"failed to get user memory for address 0x%llx\\n\",\n\t\t\taddr);\n\t\tgoto free_sgt;\n\t}\n\n\thl_debugfs_add_userptr(hdev, userptr);\n\n\treturn 0;\n\nfree_sgt:\n\tkfree(userptr->sgt);\n\treturn rc;\n}\n\n \nvoid hl_unpin_host_memory(struct hl_device *hdev, struct hl_userptr *userptr)\n{\n\thl_debugfs_remove_userptr(hdev, userptr);\n\n\tif (userptr->dma_mapped)\n\t\thdev->asic_funcs->hl_dma_unmap_sgtable(hdev, userptr->sgt, userptr->dir);\n\n\tunpin_user_pages_dirty_lock(userptr->pages, userptr->npages, true);\n\tkvfree(userptr->pages);\n\n\tlist_del(&userptr->job_node);\n\n\tsg_free_table(userptr->sgt);\n\tkfree(userptr->sgt);\n}\n\n \nvoid hl_userptr_delete_list(struct hl_device *hdev,\n\t\t\t\tstruct list_head *userptr_list)\n{\n\tstruct hl_userptr *userptr, *tmp;\n\n\tlist_for_each_entry_safe(userptr, tmp, userptr_list, job_node) {\n\t\thl_unpin_host_memory(hdev, userptr);\n\t\tkfree(userptr);\n\t}\n\n\tINIT_LIST_HEAD(userptr_list);\n}\n\n \nbool hl_userptr_is_pinned(struct hl_device *hdev, u64 addr,\n\t\t\t\tu32 size, struct list_head *userptr_list,\n\t\t\t\tstruct hl_userptr **userptr)\n{\n\tlist_for_each_entry((*userptr), userptr_list, job_node) {\n\t\tif ((addr == (*userptr)->addr) && (size == (*userptr)->size))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic int va_range_init(struct hl_device *hdev, struct hl_va_range **va_ranges,\n\t\t\t\tenum hl_va_range_type range_type, u64 start,\n\t\t\t\tu64 end, u32 page_size)\n{\n\tstruct hl_va_range *va_range = va_ranges[range_type];\n\tint rc;\n\n\tINIT_LIST_HEAD(&va_range->list);\n\n\t \n\n\tif (is_power_of_2(page_size)) {\n\t\tstart = round_up(start, page_size);\n\n\t\t \n\t\tend = round_down(end + 1, page_size) - 1;\n\t}\n\n\tif (start >= end) {\n\t\tdev_err(hdev->dev, \"too small vm range for va list\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\trc = add_va_block(hdev, va_range, start, end);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"Failed to init host va list\\n\");\n\t\treturn rc;\n\t}\n\n\tva_range->start_addr = start;\n\tva_range->end_addr = end;\n\tva_range->page_size = page_size;\n\n\treturn 0;\n}\n\n \nstatic void va_range_fini(struct hl_device *hdev, struct hl_va_range *va_range)\n{\n\tmutex_lock(&va_range->lock);\n\tclear_va_list_locked(hdev, &va_range->list);\n\tmutex_unlock(&va_range->lock);\n\n\tmutex_destroy(&va_range->lock);\n\tkfree(va_range);\n}\n\n \nstatic int vm_ctx_init_with_ranges(struct hl_ctx *ctx,\n\t\t\t\t\tu64 host_range_start,\n\t\t\t\t\tu64 host_range_end,\n\t\t\t\t\tu32 host_page_size,\n\t\t\t\t\tu64 host_huge_range_start,\n\t\t\t\t\tu64 host_huge_range_end,\n\t\t\t\t\tu32 host_huge_page_size,\n\t\t\t\t\tu64 dram_range_start,\n\t\t\t\t\tu64 dram_range_end,\n\t\t\t\t\tu32 dram_page_size)\n{\n\tstruct hl_device *hdev = ctx->hdev;\n\tint i, rc;\n\n\tfor (i = 0 ; i < HL_VA_RANGE_TYPE_MAX ; i++) {\n\t\tctx->va_range[i] =\n\t\t\tkzalloc(sizeof(struct hl_va_range), GFP_KERNEL);\n\t\tif (!ctx->va_range[i]) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_va_range;\n\t\t}\n\t}\n\n\trc = hl_mmu_ctx_init(ctx);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to init context %d\\n\", ctx->asid);\n\t\tgoto free_va_range;\n\t}\n\n\tmutex_init(&ctx->mem_hash_lock);\n\thash_init(ctx->mem_hash);\n\n\tmutex_init(&ctx->va_range[HL_VA_RANGE_TYPE_HOST]->lock);\n\n\trc = va_range_init(hdev, ctx->va_range, HL_VA_RANGE_TYPE_HOST,\n\t\t\thost_range_start, host_range_end, host_page_size);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to init host vm range\\n\");\n\t\tgoto mmu_ctx_fini;\n\t}\n\n\tif (hdev->pmmu_huge_range) {\n\t\tmutex_init(&ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]->lock);\n\n\t\trc = va_range_init(hdev,\n\t\t\tctx->va_range, HL_VA_RANGE_TYPE_HOST_HUGE,\n\t\t\thost_huge_range_start, host_huge_range_end,\n\t\t\thost_huge_page_size);\n\t\tif (rc) {\n\t\t\tdev_err(hdev->dev,\n\t\t\t\t\"failed to init host huge vm range\\n\");\n\t\t\tgoto clear_host_va_range;\n\t\t}\n\t} else {\n\t\tkfree(ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]);\n\t\tctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE] =\n\t\t\t\tctx->va_range[HL_VA_RANGE_TYPE_HOST];\n\t}\n\n\tmutex_init(&ctx->va_range[HL_VA_RANGE_TYPE_DRAM]->lock);\n\n\trc = va_range_init(hdev, ctx->va_range, HL_VA_RANGE_TYPE_DRAM,\n\t\t\tdram_range_start, dram_range_end, dram_page_size);\n\tif (rc) {\n\t\tdev_err(hdev->dev, \"failed to init dram vm range\\n\");\n\t\tgoto clear_host_huge_va_range;\n\t}\n\n\thl_debugfs_add_ctx_mem_hash(hdev, ctx);\n\n\treturn 0;\n\nclear_host_huge_va_range:\n\tmutex_destroy(&ctx->va_range[HL_VA_RANGE_TYPE_DRAM]->lock);\n\n\tif (hdev->pmmu_huge_range) {\n\t\tmutex_lock(&ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]->lock);\n\t\tclear_va_list_locked(hdev,\n\t\t\t&ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]->list);\n\t\tmutex_unlock(&ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]->lock);\n\t}\nclear_host_va_range:\n\tif (hdev->pmmu_huge_range)\n\t\tmutex_destroy(&ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]->lock);\n\tmutex_lock(&ctx->va_range[HL_VA_RANGE_TYPE_HOST]->lock);\n\tclear_va_list_locked(hdev, &ctx->va_range[HL_VA_RANGE_TYPE_HOST]->list);\n\tmutex_unlock(&ctx->va_range[HL_VA_RANGE_TYPE_HOST]->lock);\nmmu_ctx_fini:\n\tmutex_destroy(&ctx->va_range[HL_VA_RANGE_TYPE_HOST]->lock);\n\tmutex_destroy(&ctx->mem_hash_lock);\n\thl_mmu_ctx_fini(ctx);\nfree_va_range:\n\tfor (i = 0 ; i < HL_VA_RANGE_TYPE_MAX ; i++)\n\t\tkfree(ctx->va_range[i]);\n\n\treturn rc;\n}\n\nint hl_vm_ctx_init(struct hl_ctx *ctx)\n{\n\tstruct asic_fixed_properties *prop = &ctx->hdev->asic_prop;\n\tu64 host_range_start, host_range_end, host_huge_range_start,\n\t\thost_huge_range_end, dram_range_start, dram_range_end;\n\tu32 host_page_size, host_huge_page_size, dram_page_size;\n\n\tatomic64_set(&ctx->dram_phys_mem, 0);\n\n\t \n\tif (ctx->hdev->mmu_disable)\n\t\treturn 0;\n\n\tdram_range_start = prop->dmmu.start_addr;\n\tdram_range_end = prop->dmmu.end_addr - 1;\n\tdram_page_size = prop->dram_page_size ?\n\t\t\t\tprop->dram_page_size : prop->dmmu.page_size;\n\thost_range_start = prop->pmmu.start_addr;\n\thost_range_end = prop->pmmu.end_addr - 1;\n\thost_page_size = prop->pmmu.page_size;\n\thost_huge_range_start = prop->pmmu_huge.start_addr;\n\thost_huge_range_end = prop->pmmu_huge.end_addr - 1;\n\thost_huge_page_size = prop->pmmu_huge.page_size;\n\n\treturn vm_ctx_init_with_ranges(ctx, host_range_start, host_range_end,\n\t\t\thost_page_size, host_huge_range_start,\n\t\t\thost_huge_range_end, host_huge_page_size,\n\t\t\tdram_range_start, dram_range_end, dram_page_size);\n}\n\n \nvoid hl_vm_ctx_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_vm_phys_pg_pack *phys_pg_list, *tmp_phys_node;\n\tstruct hl_device *hdev = ctx->hdev;\n\tstruct hl_vm_hash_node *hnode;\n\tstruct hl_vm *vm = &hdev->vm;\n\tstruct hlist_node *tmp_node;\n\tstruct list_head free_list;\n\tstruct hl_mem_in args;\n\tint i;\n\n\tif (hdev->mmu_disable)\n\t\treturn;\n\n\thl_debugfs_remove_ctx_mem_hash(hdev, ctx);\n\n\t \n\tif (!hdev->reset_info.hard_reset_pending && !hash_empty(ctx->mem_hash))\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"user released device without removing its memory mappings\\n\");\n\n\thash_for_each_safe(ctx->mem_hash, i, tmp_node, hnode, node) {\n\t\tdev_dbg(hdev->dev,\n\t\t\t\"hl_mem_hash_node of vaddr 0x%llx of asid %d is still alive\\n\",\n\t\t\thnode->vaddr, ctx->asid);\n\t\targs.unmap.device_virt_addr = hnode->vaddr;\n\t\tunmap_device_va(ctx, &args, true);\n\t}\n\n\tmutex_lock(&hdev->mmu_lock);\n\n\t \n\thl_mmu_invalidate_cache(hdev, true, MMU_OP_USERPTR);\n\thl_mmu_invalidate_cache(hdev, true, MMU_OP_PHYS_PACK);\n\n\tmutex_unlock(&hdev->mmu_lock);\n\n\tINIT_LIST_HEAD(&free_list);\n\n\tspin_lock(&vm->idr_lock);\n\tidr_for_each_entry(&vm->phys_pg_pack_handles, phys_pg_list, i)\n\t\tif (phys_pg_list->asid == ctx->asid) {\n\t\t\tdev_dbg(hdev->dev,\n\t\t\t\t\"page list 0x%px of asid %d is still alive\\n\",\n\t\t\t\tphys_pg_list, ctx->asid);\n\n\t\t\tatomic64_sub(phys_pg_list->total_size, &hdev->dram_used_mem);\n\t\t\tidr_remove(&vm->phys_pg_pack_handles, i);\n\t\t\tlist_add(&phys_pg_list->node, &free_list);\n\t\t}\n\tspin_unlock(&vm->idr_lock);\n\n\tlist_for_each_entry_safe(phys_pg_list, tmp_phys_node, &free_list, node)\n\t\tfree_phys_pg_pack(hdev, phys_pg_list);\n\n\tva_range_fini(hdev, ctx->va_range[HL_VA_RANGE_TYPE_DRAM]);\n\tva_range_fini(hdev, ctx->va_range[HL_VA_RANGE_TYPE_HOST]);\n\n\tif (hdev->pmmu_huge_range)\n\t\tva_range_fini(hdev, ctx->va_range[HL_VA_RANGE_TYPE_HOST_HUGE]);\n\n\tmutex_destroy(&ctx->mem_hash_lock);\n\thl_mmu_ctx_fini(ctx);\n\n\t \n\tif (ctx->asid != HL_KERNEL_ASID_ID &&\n\t\t\t!hdev->asic_prop.dram_supports_virtual_memory)\n\t\tatomic64_set(&hdev->dram_used_mem, 0);\n}\n\n \nint hl_vm_init(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_vm *vm = &hdev->vm;\n\tint rc;\n\n\tif (is_power_of_2(prop->dram_page_size))\n\t\tvm->dram_pg_pool =\n\t\t\tgen_pool_create(__ffs(prop->dram_page_size), -1);\n\telse\n\t\tvm->dram_pg_pool =\n\t\t\tgen_pool_create(__ffs(DRAM_POOL_PAGE_SIZE), -1);\n\n\tif (!vm->dram_pg_pool) {\n\t\tdev_err(hdev->dev, \"Failed to create dram page pool\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tkref_init(&vm->dram_pg_pool_refcount);\n\n\trc = gen_pool_add(vm->dram_pg_pool, prop->dram_user_base_address,\n\t\t\tprop->dram_end_address - prop->dram_user_base_address,\n\t\t\t-1);\n\n\tif (rc) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Failed to add memory to dram page pool %d\\n\", rc);\n\t\tgoto pool_add_err;\n\t}\n\n\tspin_lock_init(&vm->idr_lock);\n\tidr_init(&vm->phys_pg_pack_handles);\n\n\tatomic64_set(&hdev->dram_used_mem, 0);\n\n\tvm->init_done = true;\n\n\treturn 0;\n\npool_add_err:\n\tgen_pool_destroy(vm->dram_pg_pool);\n\n\treturn rc;\n}\n\n \nvoid hl_vm_fini(struct hl_device *hdev)\n{\n\tstruct hl_vm *vm = &hdev->vm;\n\n\tif (!vm->init_done)\n\t\treturn;\n\n\t \n\tif (kref_put(&vm->dram_pg_pool_refcount, dram_pg_pool_do_release) != 1)\n\t\tdev_warn(hdev->dev, \"dram_pg_pool was not destroyed on %s\\n\",\n\t\t\t\t__func__);\n\n\tvm->init_done = false;\n}\n\n \nvoid hl_hw_block_mem_init(struct hl_ctx *ctx)\n{\n\tmutex_init(&ctx->hw_block_list_lock);\n\tINIT_LIST_HEAD(&ctx->hw_block_mem_list);\n}\n\n \nvoid hl_hw_block_mem_fini(struct hl_ctx *ctx)\n{\n\tstruct hl_vm_hw_block_list_node *lnode, *tmp;\n\n\tif (!list_empty(&ctx->hw_block_mem_list))\n\t\tdev_crit(ctx->hdev->dev, \"HW block mem list isn't empty\\n\");\n\n\tlist_for_each_entry_safe(lnode, tmp, &ctx->hw_block_mem_list, node) {\n\t\tlist_del(&lnode->node);\n\t\tkfree(lnode);\n\t}\n\n\tmutex_destroy(&ctx->hw_block_list_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}