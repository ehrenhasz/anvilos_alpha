{
  "module_name": "security.c",
  "hash_id": "114f7d3afc9c340b3075341eb84cbdf5c6b369d472ce8936ba1e724acb1dafe9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/habanalabs/common/security.c",
  "human_readable_source": "\n\n \n\n#include \"habanalabs.h\"\n\nstatic const char * const hl_glbl_error_cause[HL_MAX_NUM_OF_GLBL_ERR_CAUSE] = {\n\t\"Error due to un-priv read\",\n\t\"Error due to un-secure read\",\n\t\"Error due to read from unmapped reg\",\n\t\"Error due to un-priv write\",\n\t\"Error due to un-secure write\",\n\t\"Error due to write to unmapped reg\",\n\t\"External I/F write sec violation\",\n\t\"External I/F write to un-mapped reg\",\n\t\"Read to write only\",\n\t\"Write to read only\"\n};\n\n \nstatic int hl_get_pb_block(struct hl_device *hdev, u32 mm_reg_addr,\n\t\tconst u32 pb_blocks[], int array_size)\n{\n\tint i;\n\tu32 start_addr, end_addr;\n\n\tfor (i = 0 ; i < array_size ; i++) {\n\t\tstart_addr = pb_blocks[i];\n\t\tend_addr = start_addr + HL_BLOCK_SIZE;\n\n\t\tif ((mm_reg_addr >= start_addr) && (mm_reg_addr < end_addr))\n\t\t\treturn i;\n\t}\n\n\tdev_err(hdev->dev, \"No protection domain was found for 0x%x\\n\",\n\t\t\tmm_reg_addr);\n\treturn -EDOM;\n}\n\n \nstatic int hl_unset_pb_in_block(struct hl_device *hdev, u32 reg_offset,\n\t\t\t\tstruct hl_block_glbl_sec *sgs_entry)\n{\n\tif ((reg_offset >= HL_BLOCK_SIZE) || (reg_offset & 0x3)) {\n\t\tdev_err(hdev->dev,\n\t\t\t\"Register offset(%d) is out of range(%d) or invalid\\n\",\n\t\t\treg_offset, HL_BLOCK_SIZE);\n\t\treturn -EINVAL;\n\t}\n\n\tUNSET_GLBL_SEC_BIT(sgs_entry->sec_array,\n\t\t\t (reg_offset & (HL_BLOCK_SIZE - 1)) >> 2);\n\n\treturn 0;\n}\n\n \nint hl_unsecure_register(struct hl_device *hdev, u32 mm_reg_addr, int offset,\n\t\tconst u32 pb_blocks[], struct hl_block_glbl_sec sgs_array[],\n\t\tint array_size)\n{\n\tu32 reg_offset;\n\tint block_num;\n\n\tblock_num = hl_get_pb_block(hdev, mm_reg_addr + offset, pb_blocks,\n\t\t\tarray_size);\n\tif (block_num < 0)\n\t\treturn block_num;\n\n\treg_offset = (mm_reg_addr + offset) - pb_blocks[block_num];\n\n\treturn hl_unset_pb_in_block(hdev, reg_offset, &sgs_array[block_num]);\n}\n\n \nstatic int hl_unsecure_register_range(struct hl_device *hdev,\n\t\tstruct range mm_reg_range, int offset, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[],\n\t\tint array_size)\n{\n\tu32 reg_offset;\n\tint i, block_num, rc = 0;\n\n\tblock_num = hl_get_pb_block(hdev,\n\t\t\tmm_reg_range.start + offset, pb_blocks,\n\t\t\tarray_size);\n\tif (block_num < 0)\n\t\treturn block_num;\n\n\tfor (i = mm_reg_range.start ; i <= mm_reg_range.end ; i += 4) {\n\t\treg_offset = (i + offset) - pb_blocks[block_num];\n\t\trc |= hl_unset_pb_in_block(hdev, reg_offset,\n\t\t\t\t\t&sgs_array[block_num]);\n\t}\n\n\treturn rc;\n}\n\n \nint hl_unsecure_registers(struct hl_device *hdev, const u32 mm_reg_array[],\n\t\tint mm_array_size, int offset, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[], int blocks_array_size)\n{\n\tint i, rc = 0;\n\n\tfor (i = 0 ; i < mm_array_size ; i++) {\n\t\trc = hl_unsecure_register(hdev, mm_reg_array[i], offset,\n\t\t\t\tpb_blocks, sgs_array, blocks_array_size);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn rc;\n}\n\n \nstatic int hl_unsecure_registers_range(struct hl_device *hdev,\n\t\tconst struct range mm_reg_range_array[], int mm_array_size,\n\t\tint offset, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[], int blocks_array_size)\n{\n\tint i, rc = 0;\n\n\tfor (i = 0 ; i < mm_array_size ; i++) {\n\t\trc = hl_unsecure_register_range(hdev, mm_reg_range_array[i],\n\t\t\toffset, pb_blocks, sgs_array, blocks_array_size);\n\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\n\treturn rc;\n}\n\n \nstatic void hl_ack_pb_security_violations(struct hl_device *hdev,\n\t\tconst u32 pb_blocks[], u32 block_offset, int array_size)\n{\n\tint i;\n\tu32 cause, addr, block_base;\n\n\tfor (i = 0 ; i < array_size ; i++) {\n\t\tblock_base = pb_blocks[i] + block_offset;\n\t\tcause = RREG32(block_base + HL_BLOCK_GLBL_ERR_CAUSE);\n\t\tif (cause) {\n\t\t\taddr = RREG32(block_base + HL_BLOCK_GLBL_ERR_ADDR);\n\t\t\thdev->asic_funcs->pb_print_security_errors(hdev,\n\t\t\t\t\tblock_base, cause, addr);\n\t\t\tWREG32(block_base + HL_BLOCK_GLBL_ERR_CAUSE, cause);\n\t\t}\n\t}\n}\n\n \nvoid hl_config_glbl_sec(struct hl_device *hdev, const u32 pb_blocks[],\n\t\tstruct hl_block_glbl_sec sgs_array[], u32 block_offset,\n\t\tint array_size)\n{\n\tint i, j;\n\tu32 sgs_base;\n\n\tif (hdev->pldm)\n\t\tusleep_range(100, 1000);\n\n\tfor (i = 0 ; i < array_size ; i++) {\n\t\tsgs_base = block_offset + pb_blocks[i] +\n\t\t\t\tHL_BLOCK_GLBL_SEC_OFFS;\n\n\t\tfor (j = 0 ; j < HL_BLOCK_GLBL_SEC_LEN ; j++)\n\t\t\tWREG32(sgs_base + j * sizeof(u32),\n\t\t\t\tsgs_array[i].sec_array[j]);\n\t}\n}\n\n \nvoid hl_secure_block(struct hl_device *hdev,\n\t\tstruct hl_block_glbl_sec sgs_array[], int array_size)\n{\n\tint i;\n\n\tfor (i = 0 ; i < array_size ; i++)\n\t\tmemset((char *)(sgs_array[i].sec_array), 0,\n\t\t\tHL_BLOCK_GLBL_SEC_SIZE);\n}\n\n \nint hl_init_pb_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *user_regs_array, u32 user_regs_array_size, u64 mask)\n{\n\tint i, j;\n\tstruct hl_block_glbl_sec *glbl_sec;\n\n\tglbl_sec = kcalloc(blocks_array_size,\n\t\t\tsizeof(struct hl_block_glbl_sec),\n\t\t\tGFP_KERNEL);\n\tif (!glbl_sec)\n\t\treturn -ENOMEM;\n\n\thl_secure_block(hdev, glbl_sec, blocks_array_size);\n\thl_unsecure_registers(hdev, user_regs_array, user_regs_array_size, 0,\n\t\t\tpb_blocks, glbl_sec, blocks_array_size);\n\n\t \n\tfor (i = 0 ; i < num_dcores ; i++) {\n\t\tfor (j = 0 ; j < num_instances ; j++) {\n\t\t\tint seq = i * num_instances + j;\n\n\t\t\tif (!(mask & BIT_ULL(seq)))\n\t\t\t\tcontinue;\n\n\t\t\thl_config_glbl_sec(hdev, pb_blocks, glbl_sec,\n\t\t\t\t\ti * dcore_offset + j * instance_offset,\n\t\t\t\t\tblocks_array_size);\n\t\t}\n\t}\n\n\tkfree(glbl_sec);\n\n\treturn 0;\n}\n\n \nint hl_init_pb(struct hl_device *hdev, u32 num_dcores, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *user_regs_array, u32 user_regs_array_size)\n{\n\treturn hl_init_pb_with_mask(hdev, num_dcores, dcore_offset,\n\t\t\tnum_instances, instance_offset, pb_blocks,\n\t\t\tblocks_array_size, user_regs_array,\n\t\t\tuser_regs_array_size, ULLONG_MAX);\n}\n\n \nint hl_init_pb_ranges_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *user_regs_range_array,\n\t\tu32 user_regs_range_array_size, u64 mask)\n{\n\tint i, j, rc = 0;\n\tstruct hl_block_glbl_sec *glbl_sec;\n\n\tglbl_sec = kcalloc(blocks_array_size,\n\t\t\tsizeof(struct hl_block_glbl_sec),\n\t\t\tGFP_KERNEL);\n\tif (!glbl_sec)\n\t\treturn -ENOMEM;\n\n\thl_secure_block(hdev, glbl_sec, blocks_array_size);\n\trc = hl_unsecure_registers_range(hdev, user_regs_range_array,\n\t\t\tuser_regs_range_array_size, 0, pb_blocks, glbl_sec,\n\t\t\tblocks_array_size);\n\tif (rc)\n\t\tgoto free_glbl_sec;\n\n\t \n\tfor (i = 0 ; i < num_dcores ; i++) {\n\t\tfor (j = 0 ; j < num_instances ; j++) {\n\t\t\tint seq = i * num_instances + j;\n\n\t\t\tif (!(mask & BIT_ULL(seq)))\n\t\t\t\tcontinue;\n\n\t\t\thl_config_glbl_sec(hdev, pb_blocks, glbl_sec,\n\t\t\t\t\ti * dcore_offset + j * instance_offset,\n\t\t\t\t\tblocks_array_size);\n\t\t}\n\t}\n\nfree_glbl_sec:\n\tkfree(glbl_sec);\n\n\treturn rc;\n}\n\n \nint hl_init_pb_ranges(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *user_regs_range_array,\n\t\tu32 user_regs_range_array_size)\n{\n\treturn hl_init_pb_ranges_with_mask(hdev, num_dcores, dcore_offset,\n\t\t\tnum_instances, instance_offset, pb_blocks,\n\t\t\tblocks_array_size, user_regs_range_array,\n\t\t\tuser_regs_range_array_size, ULLONG_MAX);\n}\n\n \nint hl_init_pb_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst u32 *user_regs_array, u32 user_regs_array_size)\n{\n\tint i, rc = 0;\n\tstruct hl_block_glbl_sec *glbl_sec;\n\n\tglbl_sec = kcalloc(blocks_array_size,\n\t\t\tsizeof(struct hl_block_glbl_sec),\n\t\t\tGFP_KERNEL);\n\tif (!glbl_sec)\n\t\treturn -ENOMEM;\n\n\thl_secure_block(hdev, glbl_sec, blocks_array_size);\n\trc = hl_unsecure_registers(hdev, user_regs_array, user_regs_array_size,\n\t\t\t0, pb_blocks, glbl_sec, blocks_array_size);\n\tif (rc)\n\t\tgoto free_glbl_sec;\n\n\t \n\tfor (i = 0 ; i < num_instances ; i++)\n\t\thl_config_glbl_sec(hdev, pb_blocks, glbl_sec,\n\t\t\t\tdcore_offset + i * instance_offset,\n\t\t\t\tblocks_array_size);\n\nfree_glbl_sec:\n\tkfree(glbl_sec);\n\n\treturn rc;\n}\n\n \nint hl_init_pb_ranges_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size,\n\t\tconst struct range *user_regs_range_array, u32 user_regs_range_array_size)\n{\n\tint i;\n\tstruct hl_block_glbl_sec *glbl_sec;\n\n\tglbl_sec = kcalloc(blocks_array_size,\n\t\t\tsizeof(struct hl_block_glbl_sec),\n\t\t\tGFP_KERNEL);\n\tif (!glbl_sec)\n\t\treturn -ENOMEM;\n\n\thl_secure_block(hdev, glbl_sec, blocks_array_size);\n\thl_unsecure_registers_range(hdev, user_regs_range_array,\n\t\t\tuser_regs_range_array_size, 0, pb_blocks, glbl_sec,\n\t\t\tblocks_array_size);\n\n\t \n\tfor (i = 0 ; i < num_instances ; i++)\n\t\thl_config_glbl_sec(hdev, pb_blocks, glbl_sec,\n\t\t\t\tdcore_offset + i * instance_offset,\n\t\t\t\tblocks_array_size);\n\n\tkfree(glbl_sec);\n\n\treturn 0;\n}\n\n \nvoid hl_ack_pb_with_mask(struct hl_device *hdev, u32 num_dcores,\n\t\tu32 dcore_offset, u32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size, u64 mask)\n{\n\tint i, j;\n\n\t \n\tfor (i = 0 ; i < num_dcores ; i++) {\n\t\tfor (j = 0 ; j < num_instances ; j++) {\n\t\t\tint seq = i * num_instances + j;\n\n\t\t\tif (!(mask & BIT_ULL(seq)))\n\t\t\t\tcontinue;\n\n\t\t\thl_ack_pb_security_violations(hdev, pb_blocks,\n\t\t\t\t\ti * dcore_offset + j * instance_offset,\n\t\t\t\t\tblocks_array_size);\n\t\t}\n\t}\n}\n\n \nvoid hl_ack_pb(struct hl_device *hdev, u32 num_dcores, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size)\n{\n\thl_ack_pb_with_mask(hdev, num_dcores, dcore_offset, num_instances,\n\t\t\tinstance_offset, pb_blocks, blocks_array_size,\n\t\t\tULLONG_MAX);\n}\n\n \nvoid hl_ack_pb_single_dcore(struct hl_device *hdev, u32 dcore_offset,\n\t\tu32 num_instances, u32 instance_offset,\n\t\tconst u32 pb_blocks[], u32 blocks_array_size)\n{\n\tint i;\n\n\t \n\tfor (i = 0 ; i < num_instances ; i++)\n\t\thl_ack_pb_security_violations(hdev, pb_blocks,\n\t\t\t\tdcore_offset + i * instance_offset,\n\t\t\t\tblocks_array_size);\n\n}\n\nstatic u32 hl_automated_get_block_base_addr(struct hl_device *hdev,\n\t\tstruct hl_special_block_info *block_info,\n\t\tu32 major, u32 minor, u32 sub_minor)\n{\n\tu32 fw_block_base_address = block_info->base_addr +\n\t\t\tmajor * block_info->major_offset +\n\t\t\tminor * block_info->minor_offset +\n\t\t\tsub_minor * block_info->sub_minor_offset;\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\n\t \n\treturn (fw_block_base_address - lower_32_bits(prop->cfg_base_address));\n}\n\nstatic bool hl_check_block_type_exclusion(struct hl_skip_blocks_cfg *skip_blocks_cfg,\n\t\tint block_type)\n{\n\tint i;\n\n\t \n\tfor (i = 0 ; i < skip_blocks_cfg->block_types_len ; i++)\n\t\tif (block_type == skip_blocks_cfg->block_types[i])\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool hl_check_block_range_exclusion(struct hl_device *hdev,\n\t\tstruct hl_skip_blocks_cfg *skip_blocks_cfg,\n\t\tstruct hl_special_block_info *block_info,\n\t\tu32 major, u32 minor, u32 sub_minor)\n{\n\tu32 blocks_in_range, block_base_addr_in_range, block_base_addr;\n\tint i, j;\n\n\tblock_base_addr = hl_automated_get_block_base_addr(hdev, block_info,\n\t\t\tmajor, minor, sub_minor);\n\n\tfor (i = 0 ; i < skip_blocks_cfg->block_ranges_len ; i++) {\n\t\tblocks_in_range = (skip_blocks_cfg->block_ranges[i].end -\n\t\t\t\tskip_blocks_cfg->block_ranges[i].start) /\n\t\t\t\tHL_BLOCK_SIZE + 1;\n\t\tfor (j = 0 ; j < blocks_in_range ; j++) {\n\t\t\tblock_base_addr_in_range = skip_blocks_cfg->block_ranges[i].start +\n\t\t\t\t\tj * HL_BLOCK_SIZE;\n\t\t\tif (block_base_addr == block_base_addr_in_range)\n\t\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int hl_read_glbl_errors(struct hl_device *hdev,\n\t\tu32 blk_idx, u32 major, u32 minor, u32 sub_minor, void *data)\n{\n\tstruct hl_special_block_info *special_blocks = hdev->asic_prop.special_blocks;\n\tstruct hl_special_block_info *current_block = &special_blocks[blk_idx];\n\tu32 glbl_err_addr, glbl_err_cause, addr_val, cause_val, block_base,\n\t\tbase = current_block->base_addr - lower_32_bits(hdev->asic_prop.cfg_base_address);\n\tint i;\n\n\tblock_base = base + major * current_block->major_offset +\n\t\t\tminor * current_block->minor_offset +\n\t\t\tsub_minor * current_block->sub_minor_offset;\n\n\tglbl_err_cause = block_base + HL_GLBL_ERR_CAUSE_OFFSET;\n\tcause_val = RREG32(glbl_err_cause);\n\tif (!cause_val)\n\t\treturn 0;\n\n\tglbl_err_addr = block_base + HL_GLBL_ERR_ADDR_OFFSET;\n\taddr_val = RREG32(glbl_err_addr);\n\n\tfor (i = 0 ; i < hdev->asic_prop.glbl_err_cause_num ; i++) {\n\t\tif (cause_val & BIT(i))\n\t\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\t\"%s, addr %#llx\\n\",\n\t\t\t\thl_glbl_error_cause[i],\n\t\t\t\thdev->asic_prop.cfg_base_address + block_base +\n\t\t\t\tFIELD_GET(HL_GLBL_ERR_ADDRESS_MASK, addr_val));\n\t}\n\n\tWREG32(glbl_err_cause, cause_val);\n\n\treturn 0;\n}\n\nvoid hl_check_for_glbl_errors(struct hl_device *hdev)\n{\n\tstruct asic_fixed_properties *prop = &hdev->asic_prop;\n\tstruct hl_special_blocks_cfg special_blocks_cfg;\n\tstruct iterate_special_ctx glbl_err_iter;\n\tint rc;\n\n\tmemset(&special_blocks_cfg, 0, sizeof(special_blocks_cfg));\n\tspecial_blocks_cfg.skip_blocks_cfg = &prop->skip_special_blocks_cfg;\n\n\tglbl_err_iter.fn = &hl_read_glbl_errors;\n\tglbl_err_iter.data = &special_blocks_cfg;\n\n\trc = hl_iterate_special_blocks(hdev, &glbl_err_iter);\n\tif (rc)\n\t\tdev_err_ratelimited(hdev->dev,\n\t\t\t\"Could not iterate special blocks, glbl error check failed\\n\");\n}\n\nint hl_iterate_special_blocks(struct hl_device *hdev, struct iterate_special_ctx *ctx)\n{\n\tstruct hl_special_blocks_cfg *special_blocks_cfg =\n\t\t\t(struct hl_special_blocks_cfg *)ctx->data;\n\tstruct hl_skip_blocks_cfg *skip_blocks_cfg =\n\t\t\tspecial_blocks_cfg->skip_blocks_cfg;\n\tu32 major, minor, sub_minor, blk_idx, num_blocks;\n\tstruct hl_special_block_info *block_info_arr;\n\tint rc;\n\n\tblock_info_arr = hdev->asic_prop.special_blocks;\n\tif (!block_info_arr)\n\t\treturn -EINVAL;\n\n\tnum_blocks = hdev->asic_prop.num_of_special_blocks;\n\n\tfor (blk_idx = 0 ; blk_idx < num_blocks ; blk_idx++, block_info_arr++) {\n\t\tif (hl_check_block_type_exclusion(skip_blocks_cfg, block_info_arr->block_type))\n\t\t\tcontinue;\n\n\t\tfor (major = 0 ; major < block_info_arr->major ; major++) {\n\t\t\tminor = 0;\n\t\t\tdo {\n\t\t\t\tsub_minor = 0;\n\t\t\t\tdo {\n\t\t\t\t\tif ((hl_check_block_range_exclusion(hdev,\n\t\t\t\t\t\t\tskip_blocks_cfg, block_info_arr,\n\t\t\t\t\t\t\tmajor, minor, sub_minor)) ||\n\t\t\t\t\t\t(skip_blocks_cfg->skip_block_hook &&\n\t\t\t\t\t\tskip_blocks_cfg->skip_block_hook(hdev,\n\t\t\t\t\t\t\tspecial_blocks_cfg,\n\t\t\t\t\t\t\tblk_idx, major, minor, sub_minor))) {\n\t\t\t\t\t\tsub_minor++;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\n\t\t\t\t\trc = ctx->fn(hdev, blk_idx, major, minor,\n\t\t\t\t\t\t\t\tsub_minor, ctx->data);\n\t\t\t\t\tif (rc)\n\t\t\t\t\t\treturn rc;\n\n\t\t\t\t\tsub_minor++;\n\t\t\t\t} while (sub_minor < block_info_arr->sub_minor);\n\n\t\t\t\tminor++;\n\t\t\t} while (minor < block_info_arr->minor);\n\t\t}\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}