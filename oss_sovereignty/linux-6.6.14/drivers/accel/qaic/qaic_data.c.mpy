{
  "module_name": "qaic_data.c",
  "hash_id": "6bd3858e1a684aec3ba42e76e3beea495caeab84d16141b372b0461599ca94e1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/qaic/qaic_data.c",
  "human_readable_source": "\n\n \n \n\n#include <linux/bitfield.h>\n#include <linux/bits.h>\n#include <linux/completion.h>\n#include <linux/delay.h>\n#include <linux/dma-buf.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/math64.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/scatterlist.h>\n#include <linux/spinlock.h>\n#include <linux/srcu.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/wait.h>\n#include <drm/drm_file.h>\n#include <drm/drm_gem.h>\n#include <drm/drm_prime.h>\n#include <drm/drm_print.h>\n#include <uapi/drm/qaic_accel.h>\n\n#include \"qaic.h\"\n\n#define SEM_VAL_MASK\tGENMASK_ULL(11, 0)\n#define SEM_INDEX_MASK\tGENMASK_ULL(4, 0)\n#define BULK_XFER\tBIT(3)\n#define GEN_COMPLETION\tBIT(4)\n#define INBOUND_XFER\t1\n#define OUTBOUND_XFER\t2\n#define REQHP_OFF\t0x0  \n#define REQTP_OFF\t0x4  \n#define RSPHP_OFF\t0x8  \n#define RSPTP_OFF\t0xc  \n\n#define ENCODE_SEM(val, index, sync, cmd, flags)\t\t\t\\\n\t\t({\t\t\t\t\t\t\t\\\n\t\t\tFIELD_PREP(GENMASK(11, 0), (val)) |\t\t\\\n\t\t\tFIELD_PREP(GENMASK(20, 16), (index)) |\t\t\\\n\t\t\tFIELD_PREP(BIT(22), (sync)) |\t\t\t\\\n\t\t\tFIELD_PREP(GENMASK(26, 24), (cmd)) |\t\t\\\n\t\t\tFIELD_PREP(GENMASK(30, 29), (flags)) |\t\t\\\n\t\t\tFIELD_PREP(BIT(31), (cmd) ? 1 : 0);\t\t\\\n\t\t})\n#define NUM_EVENTS\t128\n#define NUM_DELAYS\t10\n\nstatic unsigned int wait_exec_default_timeout_ms = 5000;  \nmodule_param(wait_exec_default_timeout_ms, uint, 0600);\nMODULE_PARM_DESC(wait_exec_default_timeout_ms, \"Default timeout for DRM_IOCTL_QAIC_WAIT_BO\");\n\nstatic unsigned int datapath_poll_interval_us = 100;  \nmodule_param(datapath_poll_interval_us, uint, 0600);\nMODULE_PARM_DESC(datapath_poll_interval_us,\n\t\t \"Amount of time to sleep between activity when datapath polling is enabled\");\n\nstruct dbc_req {\n\t \n\t__le16\treq_id;\n\t \n\t__u8\tseq_id;\n\t \n\t__u8\tcmd;\n\t__le32\tresv;\n\t \n\t__le64\tsrc_addr;\n\t \n\t__le64\tdest_addr;\n\t \n\t__le32\tlen;\n\t__le32\tresv2;\n\t \n\t__le64\tdb_addr;\n\t \n\t__u8\tdb_len;\n\t__u8\tresv3;\n\t__le16\tresv4;\n\t \n\t__le32\tdb_data;\n\t \n\t__le32\tsem_cmd0;\n\t__le32\tsem_cmd1;\n\t__le32\tsem_cmd2;\n\t__le32\tsem_cmd3;\n} __packed;\n\nstruct dbc_rsp {\n\t \n\t__le16\treq_id;\n\t \n\t__le16\tstatus;\n} __packed;\n\ninline int get_dbc_req_elem_size(void)\n{\n\treturn sizeof(struct dbc_req);\n}\n\ninline int get_dbc_rsp_elem_size(void)\n{\n\treturn sizeof(struct dbc_rsp);\n}\n\nstatic void free_slice(struct kref *kref)\n{\n\tstruct bo_slice *slice = container_of(kref, struct bo_slice, ref_count);\n\n\tlist_del(&slice->slice);\n\tdrm_gem_object_put(&slice->bo->base);\n\tsg_free_table(slice->sgt);\n\tkfree(slice->sgt);\n\tkfree(slice->reqs);\n\tkfree(slice);\n}\n\nstatic int clone_range_of_sgt_for_slice(struct qaic_device *qdev, struct sg_table **sgt_out,\n\t\t\t\t\tstruct sg_table *sgt_in, u64 size, u64 offset)\n{\n\tint total_len, len, nents, offf = 0, offl = 0;\n\tstruct scatterlist *sg, *sgn, *sgf, *sgl;\n\tstruct sg_table *sgt;\n\tint ret, j;\n\n\t \n\ttotal_len = 0;\n\tsgf = NULL;\n\tsgl = NULL;\n\tnents = 0;\n\n\tsize = size ? size : PAGE_SIZE;\n\tfor (sg = sgt_in->sgl; sg; sg = sg_next(sg)) {\n\t\tlen = sg_dma_len(sg);\n\n\t\tif (!len)\n\t\t\tcontinue;\n\t\tif (offset >= total_len && offset < total_len + len) {\n\t\t\tsgf = sg;\n\t\t\tofff = offset - total_len;\n\t\t}\n\t\tif (sgf)\n\t\t\tnents++;\n\t\tif (offset + size >= total_len &&\n\t\t    offset + size <= total_len + len) {\n\t\t\tsgl = sg;\n\t\t\toffl = offset + size - total_len;\n\t\t\tbreak;\n\t\t}\n\t\ttotal_len += len;\n\t}\n\n\tif (!sgf || !sgl) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tsgt = kzalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tret = sg_alloc_table(sgt, nents, GFP_KERNEL);\n\tif (ret)\n\t\tgoto free_sgt;\n\n\t \n\tsgn = sgf;\n\tfor_each_sgtable_sg(sgt, sg, j) {\n\t\tmemcpy(sg, sgn, sizeof(*sg));\n\t\tif (sgn == sgf) {\n\t\t\tsg_dma_address(sg) += offf;\n\t\t\tsg_dma_len(sg) -= offf;\n\t\t\tsg_set_page(sg, sg_page(sgn), sg_dma_len(sg), offf);\n\t\t} else {\n\t\t\tofff = 0;\n\t\t}\n\t\tif (sgn == sgl) {\n\t\t\tsg_dma_len(sg) = offl - offf;\n\t\t\tsg_set_page(sg, sg_page(sgn), offl - offf, offf);\n\t\t\tsg_mark_end(sg);\n\t\t\tbreak;\n\t\t}\n\t\tsgn = sg_next(sgn);\n\t}\n\n\t*sgt_out = sgt;\n\treturn ret;\n\nfree_sgt:\n\tkfree(sgt);\nout:\n\t*sgt_out = NULL;\n\treturn ret;\n}\n\nstatic int encode_reqs(struct qaic_device *qdev, struct bo_slice *slice,\n\t\t       struct qaic_attach_slice_entry *req)\n{\n\t__le64 db_addr = cpu_to_le64(req->db_addr);\n\t__le32 db_data = cpu_to_le32(req->db_data);\n\tstruct scatterlist *sg;\n\t__u8 cmd = BULK_XFER;\n\tint presync_sem;\n\tu64 dev_addr;\n\t__u8 db_len;\n\tint i;\n\n\tif (!slice->no_xfer)\n\t\tcmd |= (slice->dir == DMA_TO_DEVICE ? INBOUND_XFER : OUTBOUND_XFER);\n\n\tif (req->db_len && !IS_ALIGNED(req->db_addr, req->db_len / 8))\n\t\treturn -EINVAL;\n\n\tpresync_sem = req->sem0.presync + req->sem1.presync + req->sem2.presync + req->sem3.presync;\n\tif (presync_sem > 1)\n\t\treturn -EINVAL;\n\n\tpresync_sem = req->sem0.presync << 0 | req->sem1.presync << 1 |\n\t\t      req->sem2.presync << 2 | req->sem3.presync << 3;\n\n\tswitch (req->db_len) {\n\tcase 32:\n\t\tdb_len = BIT(7);\n\t\tbreak;\n\tcase 16:\n\t\tdb_len = BIT(7) | 1;\n\t\tbreak;\n\tcase 8:\n\t\tdb_len = BIT(7) | 2;\n\t\tbreak;\n\tcase 0:\n\t\tdb_len = 0;  \n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;  \n\t}\n\n\t \n\tdev_addr = req->dev_addr;\n\tfor_each_sgtable_sg(slice->sgt, sg, i) {\n\t\tslice->reqs[i].cmd = cmd;\n\t\tslice->reqs[i].src_addr = cpu_to_le64(slice->dir == DMA_TO_DEVICE ?\n\t\t\t\t\t\t      sg_dma_address(sg) : dev_addr);\n\t\tslice->reqs[i].dest_addr = cpu_to_le64(slice->dir == DMA_TO_DEVICE ?\n\t\t\t\t\t\t       dev_addr : sg_dma_address(sg));\n\t\t \n\t\tslice->reqs[i].len = cpu_to_le32((u32)sg_dma_len(sg));\n\t\tswitch (presync_sem) {\n\t\tcase BIT(0):\n\t\t\tslice->reqs[i].sem_cmd0 = cpu_to_le32(ENCODE_SEM(req->sem0.val,\n\t\t\t\t\t\t\t\t\t req->sem0.index,\n\t\t\t\t\t\t\t\t\t req->sem0.presync,\n\t\t\t\t\t\t\t\t\t req->sem0.cmd,\n\t\t\t\t\t\t\t\t\t req->sem0.flags));\n\t\t\tbreak;\n\t\tcase BIT(1):\n\t\t\tslice->reqs[i].sem_cmd1 = cpu_to_le32(ENCODE_SEM(req->sem1.val,\n\t\t\t\t\t\t\t\t\t req->sem1.index,\n\t\t\t\t\t\t\t\t\t req->sem1.presync,\n\t\t\t\t\t\t\t\t\t req->sem1.cmd,\n\t\t\t\t\t\t\t\t\t req->sem1.flags));\n\t\t\tbreak;\n\t\tcase BIT(2):\n\t\t\tslice->reqs[i].sem_cmd2 = cpu_to_le32(ENCODE_SEM(req->sem2.val,\n\t\t\t\t\t\t\t\t\t req->sem2.index,\n\t\t\t\t\t\t\t\t\t req->sem2.presync,\n\t\t\t\t\t\t\t\t\t req->sem2.cmd,\n\t\t\t\t\t\t\t\t\t req->sem2.flags));\n\t\t\tbreak;\n\t\tcase BIT(3):\n\t\t\tslice->reqs[i].sem_cmd3 = cpu_to_le32(ENCODE_SEM(req->sem3.val,\n\t\t\t\t\t\t\t\t\t req->sem3.index,\n\t\t\t\t\t\t\t\t\t req->sem3.presync,\n\t\t\t\t\t\t\t\t\t req->sem3.cmd,\n\t\t\t\t\t\t\t\t\t req->sem3.flags));\n\t\t\tbreak;\n\t\t}\n\t\tdev_addr += sg_dma_len(sg);\n\t}\n\t \n\ti--;\n\tslice->reqs[i].cmd |= GEN_COMPLETION;\n\tslice->reqs[i].db_addr = db_addr;\n\tslice->reqs[i].db_len = db_len;\n\tslice->reqs[i].db_data = db_data;\n\t \n\tif (i && !presync_sem)\n\t\treq->sem0.flags |= (slice->dir == DMA_TO_DEVICE ?\n\t\t\t\t    QAIC_SEM_INSYNCFENCE : QAIC_SEM_OUTSYNCFENCE);\n\tslice->reqs[i].sem_cmd0 = cpu_to_le32(ENCODE_SEM(req->sem0.val, req->sem0.index,\n\t\t\t\t\t\t\t req->sem0.presync, req->sem0.cmd,\n\t\t\t\t\t\t\t req->sem0.flags));\n\tslice->reqs[i].sem_cmd1 = cpu_to_le32(ENCODE_SEM(req->sem1.val, req->sem1.index,\n\t\t\t\t\t\t\t req->sem1.presync, req->sem1.cmd,\n\t\t\t\t\t\t\t req->sem1.flags));\n\tslice->reqs[i].sem_cmd2 = cpu_to_le32(ENCODE_SEM(req->sem2.val, req->sem2.index,\n\t\t\t\t\t\t\t req->sem2.presync, req->sem2.cmd,\n\t\t\t\t\t\t\t req->sem2.flags));\n\tslice->reqs[i].sem_cmd3 = cpu_to_le32(ENCODE_SEM(req->sem3.val, req->sem3.index,\n\t\t\t\t\t\t\t req->sem3.presync, req->sem3.cmd,\n\t\t\t\t\t\t\t req->sem3.flags));\n\n\treturn 0;\n}\n\nstatic int qaic_map_one_slice(struct qaic_device *qdev, struct qaic_bo *bo,\n\t\t\t      struct qaic_attach_slice_entry *slice_ent)\n{\n\tstruct sg_table *sgt = NULL;\n\tstruct bo_slice *slice;\n\tint ret;\n\n\tret = clone_range_of_sgt_for_slice(qdev, &sgt, bo->sgt, slice_ent->size, slice_ent->offset);\n\tif (ret)\n\t\tgoto out;\n\n\tslice = kmalloc(sizeof(*slice), GFP_KERNEL);\n\tif (!slice) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sgt;\n\t}\n\n\tslice->reqs = kcalloc(sgt->nents, sizeof(*slice->reqs), GFP_KERNEL);\n\tif (!slice->reqs) {\n\t\tret = -ENOMEM;\n\t\tgoto free_slice;\n\t}\n\n\tslice->no_xfer = !slice_ent->size;\n\tslice->sgt = sgt;\n\tslice->nents = sgt->nents;\n\tslice->dir = bo->dir;\n\tslice->bo = bo;\n\tslice->size = slice_ent->size;\n\tslice->offset = slice_ent->offset;\n\n\tret = encode_reqs(qdev, slice, slice_ent);\n\tif (ret)\n\t\tgoto free_req;\n\n\tbo->total_slice_nents += sgt->nents;\n\tkref_init(&slice->ref_count);\n\tdrm_gem_object_get(&bo->base);\n\tlist_add_tail(&slice->slice, &bo->slices);\n\n\treturn 0;\n\nfree_req:\n\tkfree(slice->reqs);\nfree_slice:\n\tkfree(slice);\nfree_sgt:\n\tsg_free_table(sgt);\n\tkfree(sgt);\nout:\n\treturn ret;\n}\n\nstatic int create_sgt(struct qaic_device *qdev, struct sg_table **sgt_out, u64 size)\n{\n\tstruct scatterlist *sg;\n\tstruct sg_table *sgt;\n\tstruct page **pages;\n\tint *pages_order;\n\tint buf_extra;\n\tint max_order;\n\tint nr_pages;\n\tint ret = 0;\n\tint i, j, k;\n\tint order;\n\n\tif (size) {\n\t\tnr_pages = DIV_ROUND_UP(size, PAGE_SIZE);\n\t\t \n\t\tbuf_extra = (PAGE_SIZE - size % PAGE_SIZE) % PAGE_SIZE;\n\t\tmax_order = min(MAX_ORDER - 1, get_order(size));\n\t} else {\n\t\t \n\t\tnr_pages = 1;\n\t\tbuf_extra = 0;\n\t\tmax_order = 0;\n\t}\n\n\tpages = kvmalloc_array(nr_pages, sizeof(*pages) + sizeof(*pages_order), GFP_KERNEL);\n\tif (!pages) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tpages_order = (void *)pages + sizeof(*pages) * nr_pages;\n\n\t \n\ti = 0;\n\twhile (nr_pages > 0) {\n\t\torder = min(get_order(nr_pages * PAGE_SIZE), max_order);\n\t\twhile (1) {\n\t\t\tpages[i] = alloc_pages(GFP_KERNEL | GFP_HIGHUSER |\n\t\t\t\t\t       __GFP_NOWARN | __GFP_ZERO |\n\t\t\t\t\t       (order ? __GFP_NORETRY : __GFP_RETRY_MAYFAIL),\n\t\t\t\t\t       order);\n\t\t\tif (pages[i])\n\t\t\t\tbreak;\n\t\t\tif (!order--) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto free_partial_alloc;\n\t\t\t}\n\t\t}\n\n\t\tmax_order = order;\n\t\tpages_order[i] = order;\n\n\t\tnr_pages -= 1 << order;\n\t\tif (nr_pages <= 0)\n\t\t\t \n\t\t\tbuf_extra += abs(nr_pages) * PAGE_SIZE;\n\t\ti++;\n\t}\n\n\tsgt = kmalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt) {\n\t\tret = -ENOMEM;\n\t\tgoto free_partial_alloc;\n\t}\n\n\tif (sg_alloc_table(sgt, i, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sgt;\n\t}\n\n\t \n\tsg = sgt->sgl;\n\tfor (k = 0; k < i; k++, sg = sg_next(sg)) {\n\t\t \n\t\tif (k < i - 1) {\n\t\t\tsg_set_page(sg, pages[k], PAGE_SIZE << pages_order[k], 0);\n\t\t} else {\n\t\t\tsg_set_page(sg, pages[k], (PAGE_SIZE << pages_order[k]) - buf_extra, 0);\n\t\t\tsg_mark_end(sg);\n\t\t}\n\t}\n\n\tkvfree(pages);\n\t*sgt_out = sgt;\n\treturn ret;\n\nfree_sgt:\n\tkfree(sgt);\nfree_partial_alloc:\n\tfor (j = 0; j < i; j++)\n\t\t__free_pages(pages[j], pages_order[j]);\n\tkvfree(pages);\nout:\n\t*sgt_out = NULL;\n\treturn ret;\n}\n\nstatic bool invalid_sem(struct qaic_sem *sem)\n{\n\tif (sem->val & ~SEM_VAL_MASK || sem->index & ~SEM_INDEX_MASK ||\n\t    !(sem->presync == 0 || sem->presync == 1) || sem->pad ||\n\t    sem->flags & ~(QAIC_SEM_INSYNCFENCE | QAIC_SEM_OUTSYNCFENCE) ||\n\t    sem->cmd > QAIC_SEM_WAIT_GT_0)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int qaic_validate_req(struct qaic_device *qdev, struct qaic_attach_slice_entry *slice_ent,\n\t\t\t     u32 count, u64 total_size)\n{\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!(slice_ent[i].db_len == 32 || slice_ent[i].db_len == 16 ||\n\t\t      slice_ent[i].db_len == 8 || slice_ent[i].db_len == 0) ||\n\t\t      invalid_sem(&slice_ent[i].sem0) || invalid_sem(&slice_ent[i].sem1) ||\n\t\t      invalid_sem(&slice_ent[i].sem2) || invalid_sem(&slice_ent[i].sem3))\n\t\t\treturn -EINVAL;\n\n\t\tif (slice_ent[i].offset + slice_ent[i].size > total_size)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void qaic_free_sgt(struct sg_table *sgt)\n{\n\tstruct scatterlist *sg;\n\n\tfor (sg = sgt->sgl; sg; sg = sg_next(sg))\n\t\tif (sg_page(sg))\n\t\t\t__free_pages(sg_page(sg), get_order(sg->length));\n\tsg_free_table(sgt);\n\tkfree(sgt);\n}\n\nstatic void qaic_gem_print_info(struct drm_printer *p, unsigned int indent,\n\t\t\t\tconst struct drm_gem_object *obj)\n{\n\tstruct qaic_bo *bo = to_qaic_bo(obj);\n\n\tdrm_printf_indent(p, indent, \"user requested size=%llu\\n\", bo->size);\n}\n\nstatic const struct vm_operations_struct drm_vm_ops = {\n\t.open = drm_gem_vm_open,\n\t.close = drm_gem_vm_close,\n};\n\nstatic int qaic_gem_object_mmap(struct drm_gem_object *obj, struct vm_area_struct *vma)\n{\n\tstruct qaic_bo *bo = to_qaic_bo(obj);\n\tunsigned long offset = 0;\n\tstruct scatterlist *sg;\n\tint ret = 0;\n\n\tif (obj->import_attach)\n\t\treturn -EINVAL;\n\n\tfor (sg = bo->sgt->sgl; sg; sg = sg_next(sg)) {\n\t\tif (sg_page(sg)) {\n\t\t\tret = remap_pfn_range(vma, vma->vm_start + offset, page_to_pfn(sg_page(sg)),\n\t\t\t\t\t      sg->length, vma->vm_page_prot);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\toffset += sg->length;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void qaic_free_object(struct drm_gem_object *obj)\n{\n\tstruct qaic_bo *bo = to_qaic_bo(obj);\n\n\tif (obj->import_attach) {\n\t\t \n\t\tdrm_prime_gem_destroy(obj, NULL);\n\t} else {\n\t\t \n\t\tqaic_free_sgt(bo->sgt);\n\t}\n\n\tdrm_gem_object_release(obj);\n\tkfree(bo);\n}\n\nstatic const struct drm_gem_object_funcs qaic_gem_funcs = {\n\t.free = qaic_free_object,\n\t.print_info = qaic_gem_print_info,\n\t.mmap = qaic_gem_object_mmap,\n\t.vm_ops = &drm_vm_ops,\n};\n\nstatic struct qaic_bo *qaic_alloc_init_bo(void)\n{\n\tstruct qaic_bo *bo;\n\n\tbo = kzalloc(sizeof(*bo), GFP_KERNEL);\n\tif (!bo)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&bo->slices);\n\tinit_completion(&bo->xfer_done);\n\tcomplete_all(&bo->xfer_done);\n\n\treturn bo;\n}\n\nint qaic_create_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_create_bo *args = data;\n\tint usr_rcu_id, qdev_rcu_id;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_device *qdev;\n\tstruct qaic_user *usr;\n\tstruct qaic_bo *bo;\n\tsize_t size;\n\tint ret;\n\n\tif (args->pad)\n\t\treturn -EINVAL;\n\n\tsize = PAGE_ALIGN(args->size);\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tbo = qaic_alloc_init_bo();\n\tif (IS_ERR(bo)) {\n\t\tret = PTR_ERR(bo);\n\t\tgoto unlock_dev_srcu;\n\t}\n\tobj = &bo->base;\n\n\tdrm_gem_private_object_init(dev, obj, size);\n\n\tobj->funcs = &qaic_gem_funcs;\n\tret = create_sgt(qdev, &bo->sgt, size);\n\tif (ret)\n\t\tgoto free_bo;\n\n\tbo->size = args->size;\n\n\tret = drm_gem_handle_create(file_priv, obj, &args->handle);\n\tif (ret)\n\t\tgoto free_sgt;\n\n\tbo->handle = args->handle;\n\tdrm_gem_object_put(obj);\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\n\treturn 0;\n\nfree_sgt:\n\tqaic_free_sgt(bo->sgt);\nfree_bo:\n\tkfree(bo);\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nint qaic_mmap_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_mmap_bo *args = data;\n\tint usr_rcu_id, qdev_rcu_id;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_device *qdev;\n\tstruct qaic_user *usr;\n\tint ret;\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tobj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!obj) {\n\t\tret = -ENOENT;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tret = drm_gem_create_mmap_offset(obj);\n\tif (ret == 0)\n\t\targs->offset = drm_vma_node_offset_addr(&obj->vma_node);\n\n\tdrm_gem_object_put(obj);\n\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nstruct drm_gem_object *qaic_gem_prime_import(struct drm_device *dev, struct dma_buf *dma_buf)\n{\n\tstruct dma_buf_attachment *attach;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_bo *bo;\n\tint ret;\n\n\tbo = qaic_alloc_init_bo();\n\tif (IS_ERR(bo)) {\n\t\tret = PTR_ERR(bo);\n\t\tgoto out;\n\t}\n\n\tobj = &bo->base;\n\tget_dma_buf(dma_buf);\n\n\tattach = dma_buf_attach(dma_buf, dev->dev);\n\tif (IS_ERR(attach)) {\n\t\tret = PTR_ERR(attach);\n\t\tgoto attach_fail;\n\t}\n\n\tif (!attach->dmabuf->size) {\n\t\tret = -EINVAL;\n\t\tgoto size_align_fail;\n\t}\n\n\tdrm_gem_private_object_init(dev, obj, attach->dmabuf->size);\n\t \n\n\tobj->funcs = &qaic_gem_funcs;\n\tobj->import_attach = attach;\n\tobj->resv = dma_buf->resv;\n\n\treturn obj;\n\nsize_align_fail:\n\tdma_buf_detach(dma_buf, attach);\nattach_fail:\n\tdma_buf_put(dma_buf);\n\tkfree(bo);\nout:\n\treturn ERR_PTR(ret);\n}\n\nstatic int qaic_prepare_import_bo(struct qaic_bo *bo, struct qaic_attach_slice_hdr *hdr)\n{\n\tstruct drm_gem_object *obj = &bo->base;\n\tstruct sg_table *sgt;\n\tint ret;\n\n\tif (obj->import_attach->dmabuf->size < hdr->size)\n\t\treturn -EINVAL;\n\n\tsgt = dma_buf_map_attachment(obj->import_attach, hdr->dir);\n\tif (IS_ERR(sgt)) {\n\t\tret = PTR_ERR(sgt);\n\t\treturn ret;\n\t}\n\n\tbo->sgt = sgt;\n\tbo->size = hdr->size;\n\n\treturn 0;\n}\n\nstatic int qaic_prepare_export_bo(struct qaic_device *qdev, struct qaic_bo *bo,\n\t\t\t\t  struct qaic_attach_slice_hdr *hdr)\n{\n\tint ret;\n\n\tif (bo->size != hdr->size)\n\t\treturn -EINVAL;\n\n\tret = dma_map_sgtable(&qdev->pdev->dev, bo->sgt, hdr->dir, 0);\n\tif (ret)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int qaic_prepare_bo(struct qaic_device *qdev, struct qaic_bo *bo,\n\t\t\t   struct qaic_attach_slice_hdr *hdr)\n{\n\tint ret;\n\n\tif (bo->base.import_attach)\n\t\tret = qaic_prepare_import_bo(bo, hdr);\n\telse\n\t\tret = qaic_prepare_export_bo(qdev, bo, hdr);\n\n\tif (ret == 0)\n\t\tbo->dir = hdr->dir;\n\n\treturn ret;\n}\n\nstatic void qaic_unprepare_import_bo(struct qaic_bo *bo)\n{\n\tdma_buf_unmap_attachment(bo->base.import_attach, bo->sgt, bo->dir);\n\tbo->sgt = NULL;\n\tbo->size = 0;\n}\n\nstatic void qaic_unprepare_export_bo(struct qaic_device *qdev, struct qaic_bo *bo)\n{\n\tdma_unmap_sgtable(&qdev->pdev->dev, bo->sgt, bo->dir, 0);\n}\n\nstatic void qaic_unprepare_bo(struct qaic_device *qdev, struct qaic_bo *bo)\n{\n\tif (bo->base.import_attach)\n\t\tqaic_unprepare_import_bo(bo);\n\telse\n\t\tqaic_unprepare_export_bo(qdev, bo);\n\n\tbo->dir = 0;\n}\n\nstatic void qaic_free_slices_bo(struct qaic_bo *bo)\n{\n\tstruct bo_slice *slice, *temp;\n\n\tlist_for_each_entry_safe(slice, temp, &bo->slices, slice)\n\t\tkref_put(&slice->ref_count, free_slice);\n}\n\nstatic int qaic_attach_slicing_bo(struct qaic_device *qdev, struct qaic_bo *bo,\n\t\t\t\t  struct qaic_attach_slice_hdr *hdr,\n\t\t\t\t  struct qaic_attach_slice_entry *slice_ent)\n{\n\tint ret, i;\n\n\tfor (i = 0; i < hdr->count; i++) {\n\t\tret = qaic_map_one_slice(qdev, bo, &slice_ent[i]);\n\t\tif (ret) {\n\t\t\tqaic_free_slices_bo(bo);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (bo->total_slice_nents > qdev->dbc[hdr->dbc_id].nelem) {\n\t\tqaic_free_slices_bo(bo);\n\t\treturn -ENOSPC;\n\t}\n\n\tbo->sliced = true;\n\tbo->nr_slice = hdr->count;\n\tlist_add_tail(&bo->bo_list, &qdev->dbc[hdr->dbc_id].bo_lists);\n\n\treturn 0;\n}\n\nint qaic_attach_slice_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_attach_slice_entry *slice_ent;\n\tstruct qaic_attach_slice *args = data;\n\tint rcu_id, usr_rcu_id, qdev_rcu_id;\n\tstruct dma_bridge_chan\t*dbc;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_device *qdev;\n\tunsigned long arg_size;\n\tstruct qaic_user *usr;\n\tu8 __user *user_data;\n\tstruct qaic_bo *bo;\n\tint ret;\n\n\tif (args->hdr.count == 0)\n\t\treturn -EINVAL;\n\n\targ_size = args->hdr.count * sizeof(*slice_ent);\n\tif (arg_size / args->hdr.count != sizeof(*slice_ent))\n\t\treturn -EINVAL;\n\n\tif (args->hdr.size == 0)\n\t\treturn -EINVAL;\n\n\tif (!(args->hdr.dir == DMA_TO_DEVICE || args->hdr.dir == DMA_FROM_DEVICE))\n\t\treturn -EINVAL;\n\n\tif (args->data == 0)\n\t\treturn -EINVAL;\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tif (args->hdr.dbc_id >= qdev->num_dbc) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tuser_data = u64_to_user_ptr(args->data);\n\n\tslice_ent = kzalloc(arg_size, GFP_KERNEL);\n\tif (!slice_ent) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tret = copy_from_user(slice_ent, user_data, arg_size);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto free_slice_ent;\n\t}\n\n\tret = qaic_validate_req(qdev, slice_ent, args->hdr.count, args->hdr.size);\n\tif (ret)\n\t\tgoto free_slice_ent;\n\n\tobj = drm_gem_object_lookup(file_priv, args->hdr.handle);\n\tif (!obj) {\n\t\tret = -ENOENT;\n\t\tgoto free_slice_ent;\n\t}\n\n\tbo = to_qaic_bo(obj);\n\n\tif (bo->sliced) {\n\t\tret = -EINVAL;\n\t\tgoto put_bo;\n\t}\n\n\tdbc = &qdev->dbc[args->hdr.dbc_id];\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\tif (dbc->usr != usr) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_ch_srcu;\n\t}\n\n\tret = qaic_prepare_bo(qdev, bo, &args->hdr);\n\tif (ret)\n\t\tgoto unlock_ch_srcu;\n\n\tret = qaic_attach_slicing_bo(qdev, bo, &args->hdr, slice_ent);\n\tif (ret)\n\t\tgoto unprepare_bo;\n\n\tif (args->hdr.dir == DMA_TO_DEVICE)\n\t\tdma_sync_sgtable_for_cpu(&qdev->pdev->dev, bo->sgt, args->hdr.dir);\n\n\tbo->dbc = dbc;\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\tdrm_gem_object_put(obj);\n\tkfree(slice_ent);\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\n\treturn 0;\n\nunprepare_bo:\n\tqaic_unprepare_bo(qdev, bo);\nunlock_ch_srcu:\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\nput_bo:\n\tdrm_gem_object_put(obj);\nfree_slice_ent:\n\tkfree(slice_ent);\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nstatic inline int copy_exec_reqs(struct qaic_device *qdev, struct bo_slice *slice, u32 dbc_id,\n\t\t\t\t u32 head, u32 *ptail)\n{\n\tstruct dma_bridge_chan *dbc = &qdev->dbc[dbc_id];\n\tstruct dbc_req *reqs = slice->reqs;\n\tu32 tail = *ptail;\n\tu32 avail;\n\n\tavail = head - tail;\n\tif (head <= tail)\n\t\tavail += dbc->nelem;\n\n\t--avail;\n\n\tif (avail < slice->nents)\n\t\treturn -EAGAIN;\n\n\tif (tail + slice->nents > dbc->nelem) {\n\t\tavail = dbc->nelem - tail;\n\t\tavail = min_t(u32, avail, slice->nents);\n\t\tmemcpy(dbc->req_q_base + tail * get_dbc_req_elem_size(), reqs,\n\t\t       sizeof(*reqs) * avail);\n\t\treqs += avail;\n\t\tavail = slice->nents - avail;\n\t\tif (avail)\n\t\t\tmemcpy(dbc->req_q_base, reqs, sizeof(*reqs) * avail);\n\t} else {\n\t\tmemcpy(dbc->req_q_base + tail * get_dbc_req_elem_size(), reqs,\n\t\t       sizeof(*reqs) * slice->nents);\n\t}\n\n\t*ptail = (tail + slice->nents) % dbc->nelem;\n\n\treturn 0;\n}\n\n \nstatic inline int copy_partial_exec_reqs(struct qaic_device *qdev, struct bo_slice *slice,\n\t\t\t\t\t u64 resize, u32 dbc_id, u32 head, u32 *ptail)\n{\n\tstruct dma_bridge_chan *dbc = &qdev->dbc[dbc_id];\n\tstruct dbc_req *reqs = slice->reqs;\n\tstruct dbc_req *last_req;\n\tu32 tail = *ptail;\n\tu64 total_bytes;\n\tu64 last_bytes;\n\tu32 first_n;\n\tu32 avail;\n\tint ret;\n\tint i;\n\n\tavail = head - tail;\n\tif (head <= tail)\n\t\tavail += dbc->nelem;\n\n\t--avail;\n\n\ttotal_bytes = 0;\n\tfor (i = 0; i < slice->nents; i++) {\n\t\ttotal_bytes += le32_to_cpu(reqs[i].len);\n\t\tif (total_bytes >= resize)\n\t\t\tbreak;\n\t}\n\n\tif (total_bytes < resize) {\n\t\t \n\t\tret = -EINVAL;\n\t\treturn ret;\n\t}\n\n\tfirst_n = i;\n\tlast_bytes = i ? resize + le32_to_cpu(reqs[i].len) - total_bytes : resize;\n\n\tif (avail < (first_n + 1))\n\t\treturn -EAGAIN;\n\n\tif (first_n) {\n\t\tif (tail + first_n > dbc->nelem) {\n\t\t\tavail = dbc->nelem - tail;\n\t\t\tavail = min_t(u32, avail, first_n);\n\t\t\tmemcpy(dbc->req_q_base + tail * get_dbc_req_elem_size(), reqs,\n\t\t\t       sizeof(*reqs) * avail);\n\t\t\tlast_req = reqs + avail;\n\t\t\tavail = first_n - avail;\n\t\t\tif (avail)\n\t\t\t\tmemcpy(dbc->req_q_base, last_req, sizeof(*reqs) * avail);\n\t\t} else {\n\t\t\tmemcpy(dbc->req_q_base + tail * get_dbc_req_elem_size(), reqs,\n\t\t\t       sizeof(*reqs) * first_n);\n\t\t}\n\t}\n\n\t \n\tlast_req = dbc->req_q_base + (tail + first_n) % dbc->nelem * get_dbc_req_elem_size();\n\tmemcpy(last_req, reqs + slice->nents - 1, sizeof(*reqs));\n\n\t \n\tlast_req->len = cpu_to_le32((u32)last_bytes);\n\tlast_req->src_addr = reqs[first_n].src_addr;\n\tlast_req->dest_addr = reqs[first_n].dest_addr;\n\n\t*ptail = (tail + first_n + 1) % dbc->nelem;\n\n\treturn 0;\n}\n\nstatic int send_bo_list_to_device(struct qaic_device *qdev, struct drm_file *file_priv,\n\t\t\t\t  struct qaic_execute_entry *exec, unsigned int count,\n\t\t\t\t  bool is_partial, struct dma_bridge_chan *dbc, u32 head,\n\t\t\t\t  u32 *tail)\n{\n\tstruct qaic_partial_execute_entry *pexec = (struct qaic_partial_execute_entry *)exec;\n\tstruct drm_gem_object *obj;\n\tstruct bo_slice *slice;\n\tunsigned long flags;\n\tstruct qaic_bo *bo;\n\tbool queued;\n\tint i, j;\n\tint ret;\n\n\tfor (i = 0; i < count; i++) {\n\t\t \n\t\tobj = drm_gem_object_lookup(file_priv,\n\t\t\t\t\t    is_partial ? pexec[i].handle : exec[i].handle);\n\t\tif (!obj) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto failed_to_send_bo;\n\t\t}\n\n\t\tbo = to_qaic_bo(obj);\n\n\t\tif (!bo->sliced) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto failed_to_send_bo;\n\t\t}\n\n\t\tif (is_partial && pexec[i].resize > bo->size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto failed_to_send_bo;\n\t\t}\n\n\t\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\t\tqueued = bo->queued;\n\t\tbo->queued = true;\n\t\tif (queued) {\n\t\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\t\tret = -EINVAL;\n\t\t\tgoto failed_to_send_bo;\n\t\t}\n\n\t\tbo->req_id = dbc->next_req_id++;\n\n\t\tlist_for_each_entry(slice, &bo->slices, slice) {\n\t\t\t \n\t\t\tif (is_partial && pexec[i].resize && pexec[i].resize <= slice->offset)\n\t\t\t\tcontinue;\n\n\t\t\tfor (j = 0; j < slice->nents; j++)\n\t\t\t\tslice->reqs[j].req_id = cpu_to_le16(bo->req_id);\n\n\t\t\t \n\t\t\tif (is_partial && pexec[i].resize &&\n\t\t\t    pexec[i].resize < slice->offset + slice->size)\n\t\t\t\tret = copy_partial_exec_reqs(qdev, slice,\n\t\t\t\t\t\t\t     pexec[i].resize - slice->offset,\n\t\t\t\t\t\t\t     dbc->id, head, tail);\n\t\t\telse\n\t\t\t\tret = copy_exec_reqs(qdev, slice, dbc->id, head, tail);\n\t\t\tif (ret) {\n\t\t\t\tbo->queued = false;\n\t\t\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\t\t\tgoto failed_to_send_bo;\n\t\t\t}\n\t\t}\n\t\treinit_completion(&bo->xfer_done);\n\t\tlist_add_tail(&bo->xfer_list, &dbc->xfer_list);\n\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\tdma_sync_sgtable_for_device(&qdev->pdev->dev, bo->sgt, bo->dir);\n\t}\n\n\treturn 0;\n\nfailed_to_send_bo:\n\tif (likely(obj))\n\t\tdrm_gem_object_put(obj);\n\tfor (j = 0; j < i; j++) {\n\t\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\t\tbo = list_last_entry(&dbc->xfer_list, struct qaic_bo, xfer_list);\n\t\tobj = &bo->base;\n\t\tbo->queued = false;\n\t\tlist_del(&bo->xfer_list);\n\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\tdma_sync_sgtable_for_cpu(&qdev->pdev->dev, bo->sgt, bo->dir);\n\t\tdrm_gem_object_put(obj);\n\t}\n\treturn ret;\n}\n\nstatic void update_profiling_data(struct drm_file *file_priv,\n\t\t\t\t  struct qaic_execute_entry *exec, unsigned int count,\n\t\t\t\t  bool is_partial, u64 received_ts, u64 submit_ts, u32 queue_level)\n{\n\tstruct qaic_partial_execute_entry *pexec = (struct qaic_partial_execute_entry *)exec;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_bo *bo;\n\tint i;\n\n\tfor (i = 0; i < count; i++) {\n\t\t \n\t\tobj = drm_gem_object_lookup(file_priv,\n\t\t\t\t\t    is_partial ? pexec[i].handle : exec[i].handle);\n\t\tif (!obj)\n\t\t\tbreak;\n\t\tbo = to_qaic_bo(obj);\n\t\tbo->perf_stats.req_received_ts = received_ts;\n\t\tbo->perf_stats.req_submit_ts = submit_ts;\n\t\tbo->perf_stats.queue_level_before = queue_level;\n\t\tqueue_level += bo->total_slice_nents;\n\t\tdrm_gem_object_put(obj);\n\t}\n}\n\nstatic int __qaic_execute_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv,\n\t\t\t\t   bool is_partial)\n{\n\tstruct qaic_execute *args = data;\n\tstruct qaic_execute_entry *exec;\n\tstruct dma_bridge_chan *dbc;\n\tint usr_rcu_id, qdev_rcu_id;\n\tstruct qaic_device *qdev;\n\tstruct qaic_user *usr;\n\tu8 __user *user_data;\n\tunsigned long n;\n\tu64 received_ts;\n\tu32 queue_level;\n\tu64 submit_ts;\n\tint rcu_id;\n\tu32 head;\n\tu32 tail;\n\tu64 size;\n\tint ret;\n\n\treceived_ts = ktime_get_ns();\n\n\tsize = is_partial ? sizeof(struct qaic_partial_execute_entry) : sizeof(*exec);\n\tn = (unsigned long)size * args->hdr.count;\n\tif (args->hdr.count == 0 || n / args->hdr.count != size)\n\t\treturn -EINVAL;\n\n\tuser_data = u64_to_user_ptr(args->data);\n\n\texec = kcalloc(args->hdr.count, size, GFP_KERNEL);\n\tif (!exec)\n\t\treturn -ENOMEM;\n\n\tif (copy_from_user(exec, user_data, n)) {\n\t\tret = -EFAULT;\n\t\tgoto free_exec;\n\t}\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tif (args->hdr.dbc_id >= qdev->num_dbc) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tdbc = &qdev->dbc[args->hdr.dbc_id];\n\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\tif (!dbc->usr || dbc->usr->handle != usr->handle) {\n\t\tret = -EPERM;\n\t\tgoto release_ch_rcu;\n\t}\n\n\thead = readl(dbc->dbc_base + REQHP_OFF);\n\ttail = readl(dbc->dbc_base + REQTP_OFF);\n\n\tif (head == U32_MAX || tail == U32_MAX) {\n\t\t \n\t\tret = -ENODEV;\n\t\tgoto release_ch_rcu;\n\t}\n\n\tqueue_level = head <= tail ? tail - head : dbc->nelem - (head - tail);\n\n\tret = send_bo_list_to_device(qdev, file_priv, exec, args->hdr.count, is_partial, dbc,\n\t\t\t\t     head, &tail);\n\tif (ret)\n\t\tgoto release_ch_rcu;\n\n\t \n\tsubmit_ts = ktime_get_ns();\n\twritel(tail, dbc->dbc_base + REQTP_OFF);\n\n\tupdate_profiling_data(file_priv, exec, args->hdr.count, is_partial, received_ts,\n\t\t\t      submit_ts, queue_level);\n\n\tif (datapath_polling)\n\t\tschedule_work(&dbc->poll_work);\n\nrelease_ch_rcu:\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\nfree_exec:\n\tkfree(exec);\n\treturn ret;\n}\n\nint qaic_execute_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\treturn __qaic_execute_bo_ioctl(dev, data, file_priv, false);\n}\n\nint qaic_partial_execute_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\treturn __qaic_execute_bo_ioctl(dev, data, file_priv, true);\n}\n\n \nirqreturn_t dbc_irq_handler(int irq, void *data)\n{\n\tstruct dma_bridge_chan *dbc = data;\n\tint rcu_id;\n\tu32 head;\n\tu32 tail;\n\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\n\tif (!dbc->usr) {\n\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\thead = readl(dbc->dbc_base + RSPHP_OFF);\n\tif (head == U32_MAX) {  \n\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\treturn IRQ_NONE;\n\t}\n\n\ttail = readl(dbc->dbc_base + RSPTP_OFF);\n\tif (tail == U32_MAX) {  \n\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (head == tail) {  \n\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\treturn IRQ_NONE;\n\t}\n\n\tdisable_irq_nosync(irq);\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\treturn IRQ_WAKE_THREAD;\n}\n\nvoid irq_polling_work(struct work_struct *work)\n{\n\tstruct dma_bridge_chan *dbc = container_of(work, struct dma_bridge_chan,  poll_work);\n\tunsigned long flags;\n\tint rcu_id;\n\tu32 head;\n\tu32 tail;\n\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\n\twhile (1) {\n\t\tif (dbc->qdev->in_reset) {\n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\t\tif (!dbc->usr) {\n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\t\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\t\tif (list_empty(&dbc->xfer_list)) {\n\t\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\n\t\thead = readl(dbc->dbc_base + RSPHP_OFF);\n\t\tif (head == U32_MAX) {  \n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\n\t\ttail = readl(dbc->dbc_base + RSPTP_OFF);\n\t\tif (tail == U32_MAX) {  \n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\n\t\tif (head != tail) {\n\t\t\tirq_wake_thread(dbc->irq, dbc);\n\t\t\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\t\t\treturn;\n\t\t}\n\n\t\tcond_resched();\n\t\tusleep_range(datapath_poll_interval_us, 2 * datapath_poll_interval_us);\n\t}\n}\n\nirqreturn_t dbc_irq_threaded_fn(int irq, void *data)\n{\n\tstruct dma_bridge_chan *dbc = data;\n\tint event_count = NUM_EVENTS;\n\tint delay_count = NUM_DELAYS;\n\tstruct qaic_device *qdev;\n\tstruct qaic_bo *bo, *i;\n\tstruct dbc_rsp *rsp;\n\tunsigned long flags;\n\tint rcu_id;\n\tu16 status;\n\tu16 req_id;\n\tu32 head;\n\tu32 tail;\n\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\n\thead = readl(dbc->dbc_base + RSPHP_OFF);\n\tif (head == U32_MAX)  \n\t\tgoto error_out;\n\n\tqdev = dbc->qdev;\nread_fifo:\n\n\tif (!event_count) {\n\t\tevent_count = NUM_EVENTS;\n\t\tcond_resched();\n\t}\n\n\t \n\tif (!dbc->usr)\n\t\tgoto error_out;\n\n\ttail = readl(dbc->dbc_base + RSPTP_OFF);\n\tif (tail == U32_MAX)  \n\t\tgoto error_out;\n\n\tif (head == tail) {  \n\t\tif (delay_count) {\n\t\t\t--delay_count;\n\t\t\tusleep_range(100, 200);\n\t\t\tgoto read_fifo;  \n\t\t}\n\t\tgoto normal_out;\n\t}\n\n\tdelay_count = NUM_DELAYS;\n\twhile (head != tail) {\n\t\tif (!event_count)\n\t\t\tbreak;\n\t\t--event_count;\n\t\trsp = dbc->rsp_q_base + head * sizeof(*rsp);\n\t\treq_id = le16_to_cpu(rsp->req_id);\n\t\tstatus = le16_to_cpu(rsp->status);\n\t\tif (status)\n\t\t\tpci_dbg(qdev->pdev, \"req_id %d failed with status %d\\n\", req_id, status);\n\t\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\t\t \n\t\tlist_for_each_entry_safe(bo, i, &dbc->xfer_list, xfer_list) {\n\t\t\tif (bo->req_id == req_id)\n\t\t\t\tbo->nr_slice_xfer_done++;\n\t\t\telse\n\t\t\t\tcontinue;\n\n\t\t\tif (bo->nr_slice_xfer_done < bo->nr_slice)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tdma_sync_sgtable_for_cpu(&qdev->pdev->dev, bo->sgt, bo->dir);\n\t\t\tbo->nr_slice_xfer_done = 0;\n\t\t\tbo->queued = false;\n\t\t\tlist_del(&bo->xfer_list);\n\t\t\tbo->perf_stats.req_processed_ts = ktime_get_ns();\n\t\t\tcomplete_all(&bo->xfer_done);\n\t\t\tdrm_gem_object_put(&bo->base);\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\thead = (head + 1) % dbc->nelem;\n\t}\n\n\t \n\twritel(head, dbc->dbc_base + RSPHP_OFF);\n\n\t \n\tgoto read_fifo;\n\nnormal_out:\n\tif (likely(!datapath_polling))\n\t\tenable_irq(irq);\n\telse\n\t\tschedule_work(&dbc->poll_work);\n\t \n\ttail = readl(dbc->dbc_base + RSPTP_OFF);\n\tif (tail != U32_MAX && head != tail) {\n\t\tif (likely(!datapath_polling))\n\t\t\tdisable_irq_nosync(irq);\n\t\tgoto read_fifo;\n\t}\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\treturn IRQ_HANDLED;\n\nerror_out:\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\n\tif (likely(!datapath_polling))\n\t\tenable_irq(irq);\n\telse\n\t\tschedule_work(&dbc->poll_work);\n\n\treturn IRQ_HANDLED;\n}\n\nint qaic_wait_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_wait *args = data;\n\tint usr_rcu_id, qdev_rcu_id;\n\tstruct dma_bridge_chan *dbc;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_device *qdev;\n\tunsigned long timeout;\n\tstruct qaic_user *usr;\n\tstruct qaic_bo *bo;\n\tint rcu_id;\n\tint ret;\n\n\tif (args->pad != 0)\n\t\treturn -EINVAL;\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tif (args->dbc_id >= qdev->num_dbc) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tdbc = &qdev->dbc[args->dbc_id];\n\n\trcu_id = srcu_read_lock(&dbc->ch_lock);\n\tif (dbc->usr != usr) {\n\t\tret = -EPERM;\n\t\tgoto unlock_ch_srcu;\n\t}\n\n\tobj = drm_gem_object_lookup(file_priv, args->handle);\n\tif (!obj) {\n\t\tret = -ENOENT;\n\t\tgoto unlock_ch_srcu;\n\t}\n\n\tbo = to_qaic_bo(obj);\n\ttimeout = args->timeout ? args->timeout : wait_exec_default_timeout_ms;\n\ttimeout = msecs_to_jiffies(timeout);\n\tret = wait_for_completion_interruptible_timeout(&bo->xfer_done, timeout);\n\tif (!ret) {\n\t\tret = -ETIMEDOUT;\n\t\tgoto put_obj;\n\t}\n\tif (ret > 0)\n\t\tret = 0;\n\n\tif (!dbc->usr)\n\t\tret = -EPERM;\n\nput_obj:\n\tdrm_gem_object_put(obj);\nunlock_ch_srcu:\n\tsrcu_read_unlock(&dbc->ch_lock, rcu_id);\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nint qaic_perf_stats_bo_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_perf_stats_entry *ent = NULL;\n\tstruct qaic_perf_stats *args = data;\n\tint usr_rcu_id, qdev_rcu_id;\n\tstruct drm_gem_object *obj;\n\tstruct qaic_device *qdev;\n\tstruct qaic_user *usr;\n\tstruct qaic_bo *bo;\n\tint ret, i;\n\n\tusr = file_priv->driver_priv;\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_usr_srcu;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tret = -ENODEV;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tif (args->hdr.dbc_id >= qdev->num_dbc) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tent = kcalloc(args->hdr.count, sizeof(*ent), GFP_KERNEL);\n\tif (!ent) {\n\t\tret = -EINVAL;\n\t\tgoto unlock_dev_srcu;\n\t}\n\n\tret = copy_from_user(ent, u64_to_user_ptr(args->data), args->hdr.count * sizeof(*ent));\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto free_ent;\n\t}\n\n\tfor (i = 0; i < args->hdr.count; i++) {\n\t\tobj = drm_gem_object_lookup(file_priv, ent[i].handle);\n\t\tif (!obj) {\n\t\t\tret = -ENOENT;\n\t\t\tgoto free_ent;\n\t\t}\n\t\tbo = to_qaic_bo(obj);\n\t\t \n\t\tif (bo->perf_stats.req_processed_ts < bo->perf_stats.req_submit_ts) {\n\t\t\tent[i].device_latency_us = 0;\n\t\t} else {\n\t\t\tent[i].device_latency_us = div_u64((bo->perf_stats.req_processed_ts -\n\t\t\t\t\t\t\t    bo->perf_stats.req_submit_ts), 1000);\n\t\t}\n\t\tent[i].submit_latency_us = div_u64((bo->perf_stats.req_submit_ts -\n\t\t\t\t\t\t    bo->perf_stats.req_received_ts), 1000);\n\t\tent[i].queue_level_before = bo->perf_stats.queue_level_before;\n\t\tent[i].num_queue_element = bo->total_slice_nents;\n\t\tdrm_gem_object_put(obj);\n\t}\n\n\tif (copy_to_user(u64_to_user_ptr(args->data), ent, args->hdr.count * sizeof(*ent)))\n\t\tret = -EFAULT;\n\nfree_ent:\n\tkfree(ent);\nunlock_dev_srcu:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\nunlock_usr_srcu:\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nstatic void empty_xfer_list(struct qaic_device *qdev, struct dma_bridge_chan *dbc)\n{\n\tunsigned long flags;\n\tstruct qaic_bo *bo;\n\n\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\twhile (!list_empty(&dbc->xfer_list)) {\n\t\tbo = list_first_entry(&dbc->xfer_list, typeof(*bo), xfer_list);\n\t\tbo->queued = false;\n\t\tlist_del(&bo->xfer_list);\n\t\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n\t\tdma_sync_sgtable_for_cpu(&qdev->pdev->dev, bo->sgt, bo->dir);\n\t\tcomplete_all(&bo->xfer_done);\n\t\tdrm_gem_object_put(&bo->base);\n\t\tspin_lock_irqsave(&dbc->xfer_lock, flags);\n\t}\n\tspin_unlock_irqrestore(&dbc->xfer_lock, flags);\n}\n\nint disable_dbc(struct qaic_device *qdev, u32 dbc_id, struct qaic_user *usr)\n{\n\tif (!qdev->dbc[dbc_id].usr || qdev->dbc[dbc_id].usr->handle != usr->handle)\n\t\treturn -EPERM;\n\n\tqdev->dbc[dbc_id].usr = NULL;\n\tsynchronize_srcu(&qdev->dbc[dbc_id].ch_lock);\n\treturn 0;\n}\n\n \nvoid enable_dbc(struct qaic_device *qdev, u32 dbc_id, struct qaic_user *usr)\n{\n\tqdev->dbc[dbc_id].usr = usr;\n}\n\nvoid wakeup_dbc(struct qaic_device *qdev, u32 dbc_id)\n{\n\tstruct dma_bridge_chan *dbc = &qdev->dbc[dbc_id];\n\n\tdbc->usr = NULL;\n\tempty_xfer_list(qdev, dbc);\n\tsynchronize_srcu(&dbc->ch_lock);\n\t \n\tempty_xfer_list(qdev, dbc);\n}\n\nvoid release_dbc(struct qaic_device *qdev, u32 dbc_id)\n{\n\tstruct bo_slice *slice, *slice_temp;\n\tstruct qaic_bo *bo, *bo_temp;\n\tstruct dma_bridge_chan *dbc;\n\n\tdbc = &qdev->dbc[dbc_id];\n\tif (!dbc->in_use)\n\t\treturn;\n\n\twakeup_dbc(qdev, dbc_id);\n\n\tdma_free_coherent(&qdev->pdev->dev, dbc->total_size, dbc->req_q_base, dbc->dma_addr);\n\tdbc->total_size = 0;\n\tdbc->req_q_base = NULL;\n\tdbc->dma_addr = 0;\n\tdbc->nelem = 0;\n\tdbc->usr = NULL;\n\n\tlist_for_each_entry_safe(bo, bo_temp, &dbc->bo_lists, bo_list) {\n\t\tlist_for_each_entry_safe(slice, slice_temp, &bo->slices, slice)\n\t\t\tkref_put(&slice->ref_count, free_slice);\n\t\tbo->sliced = false;\n\t\tINIT_LIST_HEAD(&bo->slices);\n\t\tbo->total_slice_nents = 0;\n\t\tbo->dir = 0;\n\t\tbo->dbc = NULL;\n\t\tbo->nr_slice = 0;\n\t\tbo->nr_slice_xfer_done = 0;\n\t\tbo->queued = false;\n\t\tbo->req_id = 0;\n\t\tinit_completion(&bo->xfer_done);\n\t\tcomplete_all(&bo->xfer_done);\n\t\tlist_del(&bo->bo_list);\n\t\tbo->perf_stats.req_received_ts = 0;\n\t\tbo->perf_stats.req_submit_ts = 0;\n\t\tbo->perf_stats.req_processed_ts = 0;\n\t\tbo->perf_stats.queue_level_before = 0;\n\t}\n\n\tdbc->in_use = false;\n\twake_up(&dbc->dbc_release);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}