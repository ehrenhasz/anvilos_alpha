{
  "module_name": "qaic_control.c",
  "hash_id": "4bdbc8b70967e22946d9387fdeef08752336b05a490736149a6369a448e52b5c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/accel/qaic/qaic_control.c",
  "human_readable_source": "\n\n \n \n\n#include <asm/byteorder.h>\n#include <linux/completion.h>\n#include <linux/crc32.h>\n#include <linux/delay.h>\n#include <linux/dma-mapping.h>\n#include <linux/kref.h>\n#include <linux/list.h>\n#include <linux/mhi.h>\n#include <linux/mm.h>\n#include <linux/moduleparam.h>\n#include <linux/mutex.h>\n#include <linux/overflow.h>\n#include <linux/pci.h>\n#include <linux/scatterlist.h>\n#include <linux/types.h>\n#include <linux/uaccess.h>\n#include <linux/workqueue.h>\n#include <linux/wait.h>\n#include <drm/drm_device.h>\n#include <drm/drm_file.h>\n#include <uapi/drm/qaic_accel.h>\n\n#include \"qaic.h\"\n\n#define MANAGE_MAGIC_NUMBER\t\t((__force __le32)0x43494151)  \n#define QAIC_DBC_Q_GAP\t\t\tSZ_256\n#define QAIC_DBC_Q_BUF_ALIGN\t\tSZ_4K\n#define QAIC_MANAGE_EXT_MSG_LENGTH\tSZ_64K  \n#define QAIC_WRAPPER_MAX_SIZE\t\tSZ_4K\n#define QAIC_MHI_RETRY_WAIT_MS\t\t100\n#define QAIC_MHI_RETRY_MAX\t\t20\n\nstatic unsigned int control_resp_timeout_s = 60;  \nmodule_param(control_resp_timeout_s, uint, 0600);\nMODULE_PARM_DESC(control_resp_timeout_s, \"Timeout for NNC responses from QSM\");\n\nstruct manage_msg {\n\tu32 len;\n\tu32 count;\n\tu8 data[];\n};\n\n \nstruct wire_msg_hdr {\n\t__le32 crc32;  \n\t__le32 magic_number;\n\t__le32 sequence_number;\n\t__le32 len;  \n\t__le32 count;  \n\t__le32 handle;  \n\t__le32 partition_id;  \n\t__le32 padding;  \n} __packed;\n\nstruct wire_msg {\n\tstruct wire_msg_hdr hdr;\n\tu8 data[];\n} __packed;\n\nstruct wire_trans_hdr {\n\t__le32 type;\n\t__le32 len;\n} __packed;\n\n \nstruct wrapper_msg {\n\tstruct list_head list;\n\tstruct kref ref_count;\n\tu32 len;  \n\tstruct wrapper_list *head;\n\tunion {\n\t\tstruct wire_msg msg;\n\t\tstruct wire_trans_hdr trans;\n\t};\n};\n\nstruct wrapper_list {\n\tstruct list_head list;\n\tspinlock_t lock;  \n};\n\nstruct wire_trans_passthrough {\n\tstruct wire_trans_hdr hdr;\n\tu8 data[];\n} __packed;\n\nstruct wire_addr_size_pair {\n\t__le64 addr;\n\t__le64 size;\n} __packed;\n\nstruct wire_trans_dma_xfer {\n\tstruct wire_trans_hdr hdr;\n\t__le32 tag;\n\t__le32 count;\n\t__le32 dma_chunk_id;\n\t__le32 padding;\n\tstruct wire_addr_size_pair data[];\n} __packed;\n\n \nstruct wire_trans_dma_xfer_cont {\n\tstruct wire_trans_hdr hdr;\n\t__le32 dma_chunk_id;\n\t__le32 padding;\n\t__le64 xferred_size;\n} __packed;\n\nstruct wire_trans_activate_to_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le64 req_q_addr;\n\t__le64 rsp_q_addr;\n\t__le32 req_q_size;\n\t__le32 rsp_q_size;\n\t__le32 buf_len;\n\t__le32 options;  \n} __packed;\n\nstruct wire_trans_activate_from_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 status;\n\t__le32 dbc_id;\n\t__le64 options;  \n} __packed;\n\nstruct wire_trans_deactivate_from_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 status;\n\t__le32 dbc_id;\n} __packed;\n\nstruct wire_trans_terminate_to_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 handle;\n\t__le32 padding;\n} __packed;\n\nstruct wire_trans_terminate_from_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 status;\n\t__le32 padding;\n} __packed;\n\nstruct wire_trans_status_to_dev {\n\tstruct wire_trans_hdr hdr;\n} __packed;\n\nstruct wire_trans_status_from_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le16 major;\n\t__le16 minor;\n\t__le32 status;\n\t__le64 status_flags;\n} __packed;\n\nstruct wire_trans_validate_part_to_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 part_id;\n\t__le32 padding;\n} __packed;\n\nstruct wire_trans_validate_part_from_dev {\n\tstruct wire_trans_hdr hdr;\n\t__le32 status;\n\t__le32 padding;\n} __packed;\n\nstruct xfer_queue_elem {\n\t \n\tstruct list_head list;\n\t \n\tu32 seq_num;\n\t \n\tstruct completion xfer_done;\n\t \n\tvoid *buf;\n};\n\nstruct dma_xfer {\n\t \n\tstruct list_head list;\n\t \n\tstruct sg_table *sgt;\n\t \n\tstruct page **page_list;\n\t \n\tunsigned long nr_pages;\n};\n\nstruct ioctl_resources {\n\t \n\tstruct list_head dma_xfers;\n\t \n\tvoid *buf;\n\t \n\tdma_addr_t dma_addr;\n\t \n\tu32 total_size;\n\t \n\tu32 nelem;\n\t \n\tvoid *rsp_q_base;\n\t \n\tu32 status;\n\t \n\tu32 dbc_id;\n\t \n\tu32 dma_chunk_id;\n\t \n\tu64 xferred_dma_size;\n\t \n\tvoid *trans_hdr;\n};\n\nstruct resp_work {\n\tstruct work_struct work;\n\tstruct qaic_device *qdev;\n\tvoid *buf;\n};\n\n \nstatic __le32 incr_le32(__le32 val)\n{\n\treturn cpu_to_le32(le32_to_cpu(val) + 1);\n}\n\nstatic u32 gen_crc(void *msg)\n{\n\tstruct wrapper_list *wrappers = msg;\n\tstruct wrapper_msg *w;\n\tu32 crc = ~0;\n\n\tlist_for_each_entry(w, &wrappers->list, list)\n\t\tcrc = crc32(crc, &w->msg, w->len);\n\n\treturn crc ^ ~0;\n}\n\nstatic u32 gen_crc_stub(void *msg)\n{\n\treturn 0;\n}\n\nstatic bool valid_crc(void *msg)\n{\n\tstruct wire_msg_hdr *hdr = msg;\n\tbool ret;\n\tu32 crc;\n\n\t \n\tcrc = le32_to_cpu(hdr->crc32);\n\thdr->crc32 = 0;\n\tret = (crc32(~0, msg, le32_to_cpu(hdr->len)) ^ ~0) == crc;\n\thdr->crc32 = cpu_to_le32(crc);\n\treturn ret;\n}\n\nstatic bool valid_crc_stub(void *msg)\n{\n\treturn true;\n}\n\nstatic void free_wrapper(struct kref *ref)\n{\n\tstruct wrapper_msg *wrapper = container_of(ref, struct wrapper_msg, ref_count);\n\n\tlist_del(&wrapper->list);\n\tkfree(wrapper);\n}\n\nstatic void save_dbc_buf(struct qaic_device *qdev, struct ioctl_resources *resources,\n\t\t\t struct qaic_user *usr)\n{\n\tu32 dbc_id = resources->dbc_id;\n\n\tif (resources->buf) {\n\t\twait_event_interruptible(qdev->dbc[dbc_id].dbc_release, !qdev->dbc[dbc_id].in_use);\n\t\tqdev->dbc[dbc_id].req_q_base = resources->buf;\n\t\tqdev->dbc[dbc_id].rsp_q_base = resources->rsp_q_base;\n\t\tqdev->dbc[dbc_id].dma_addr = resources->dma_addr;\n\t\tqdev->dbc[dbc_id].total_size = resources->total_size;\n\t\tqdev->dbc[dbc_id].nelem = resources->nelem;\n\t\tenable_dbc(qdev, dbc_id, usr);\n\t\tqdev->dbc[dbc_id].in_use = true;\n\t\tresources->buf = NULL;\n\t}\n}\n\nstatic void free_dbc_buf(struct qaic_device *qdev, struct ioctl_resources *resources)\n{\n\tif (resources->buf)\n\t\tdma_free_coherent(&qdev->pdev->dev, resources->total_size, resources->buf,\n\t\t\t\t  resources->dma_addr);\n\tresources->buf = NULL;\n}\n\nstatic void free_dma_xfers(struct qaic_device *qdev, struct ioctl_resources *resources)\n{\n\tstruct dma_xfer *xfer;\n\tstruct dma_xfer *x;\n\tint i;\n\n\tlist_for_each_entry_safe(xfer, x, &resources->dma_xfers, list) {\n\t\tdma_unmap_sgtable(&qdev->pdev->dev, xfer->sgt, DMA_TO_DEVICE, 0);\n\t\tsg_free_table(xfer->sgt);\n\t\tkfree(xfer->sgt);\n\t\tfor (i = 0; i < xfer->nr_pages; ++i)\n\t\t\tput_page(xfer->page_list[i]);\n\t\tkfree(xfer->page_list);\n\t\tlist_del(&xfer->list);\n\t\tkfree(xfer);\n\t}\n}\n\nstatic struct wrapper_msg *add_wrapper(struct wrapper_list *wrappers, u32 size)\n{\n\tstruct wrapper_msg *w = kzalloc(size, GFP_KERNEL);\n\n\tif (!w)\n\t\treturn NULL;\n\tlist_add_tail(&w->list, &wrappers->list);\n\tkref_init(&w->ref_count);\n\tw->head = wrappers;\n\treturn w;\n}\n\nstatic int encode_passthrough(struct qaic_device *qdev, void *trans, struct wrapper_list *wrappers,\n\t\t\t      u32 *user_len)\n{\n\tstruct qaic_manage_trans_passthrough *in_trans = trans;\n\tstruct wire_trans_passthrough *out_trans;\n\tstruct wrapper_msg *trans_wrapper;\n\tstruct wrapper_msg *wrapper;\n\tstruct wire_msg *msg;\n\tu32 msg_hdr_len;\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\tmsg_hdr_len = le32_to_cpu(msg->hdr.len);\n\n\tif (in_trans->hdr.len % 8 != 0)\n\t\treturn -EINVAL;\n\n\tif (size_add(msg_hdr_len, in_trans->hdr.len) > QAIC_MANAGE_EXT_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\ttrans_wrapper = add_wrapper(wrappers,\n\t\t\t\t    offsetof(struct wrapper_msg, trans) + in_trans->hdr.len);\n\tif (!trans_wrapper)\n\t\treturn -ENOMEM;\n\ttrans_wrapper->len = in_trans->hdr.len;\n\tout_trans = (struct wire_trans_passthrough *)&trans_wrapper->trans;\n\n\tmemcpy(out_trans->data, in_trans->data, in_trans->hdr.len - sizeof(in_trans->hdr));\n\tmsg->hdr.len = cpu_to_le32(msg_hdr_len + in_trans->hdr.len);\n\tmsg->hdr.count = incr_le32(msg->hdr.count);\n\t*user_len += in_trans->hdr.len;\n\tout_trans->hdr.type = cpu_to_le32(QAIC_TRANS_PASSTHROUGH_TO_DEV);\n\tout_trans->hdr.len = cpu_to_le32(in_trans->hdr.len);\n\n\treturn 0;\n}\n\n \nstatic int find_and_map_user_pages(struct qaic_device *qdev,\n\t\t\t\t   struct qaic_manage_trans_dma_xfer *in_trans,\n\t\t\t\t   struct ioctl_resources *resources, struct dma_xfer *xfer)\n{\n\tu64 xfer_start_addr, remaining, end, total;\n\tunsigned long need_pages;\n\tstruct page **page_list;\n\tunsigned long nr_pages;\n\tstruct sg_table *sgt;\n\tint ret;\n\tint i;\n\n\tif (check_add_overflow(in_trans->addr, resources->xferred_dma_size, &xfer_start_addr))\n\t\treturn -EINVAL;\n\n\tif (in_trans->size < resources->xferred_dma_size)\n\t\treturn -EINVAL;\n\tremaining = in_trans->size - resources->xferred_dma_size;\n\tif (remaining == 0)\n\t\treturn 0;\n\n\tif (check_add_overflow(xfer_start_addr, remaining, &end))\n\t\treturn -EINVAL;\n\n\ttotal = remaining + offset_in_page(xfer_start_addr);\n\tif (total >= SIZE_MAX)\n\t\treturn -EINVAL;\n\n\tneed_pages = DIV_ROUND_UP(total, PAGE_SIZE);\n\n\tnr_pages = need_pages;\n\n\twhile (1) {\n\t\tpage_list = kmalloc_array(nr_pages, sizeof(*page_list), GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!page_list) {\n\t\t\tnr_pages = nr_pages / 2;\n\t\t\tif (!nr_pages)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tret = get_user_pages_fast(xfer_start_addr, nr_pages, 0, page_list);\n\tif (ret < 0)\n\t\tgoto free_page_list;\n\tif (ret != nr_pages) {\n\t\tnr_pages = ret;\n\t\tret = -EFAULT;\n\t\tgoto put_pages;\n\t}\n\n\tsgt = kmalloc(sizeof(*sgt), GFP_KERNEL);\n\tif (!sgt) {\n\t\tret = -ENOMEM;\n\t\tgoto put_pages;\n\t}\n\n\tret = sg_alloc_table_from_pages(sgt, page_list, nr_pages,\n\t\t\t\t\toffset_in_page(xfer_start_addr),\n\t\t\t\t\tremaining, GFP_KERNEL);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto free_sgt;\n\t}\n\n\tret = dma_map_sgtable(&qdev->pdev->dev, sgt, DMA_TO_DEVICE, 0);\n\tif (ret)\n\t\tgoto free_table;\n\n\txfer->sgt = sgt;\n\txfer->page_list = page_list;\n\txfer->nr_pages = nr_pages;\n\n\treturn need_pages > nr_pages ? 1 : 0;\n\nfree_table:\n\tsg_free_table(sgt);\nfree_sgt:\n\tkfree(sgt);\nput_pages:\n\tfor (i = 0; i < nr_pages; ++i)\n\t\tput_page(page_list[i]);\nfree_page_list:\n\tkfree(page_list);\n\treturn ret;\n}\n\n \nstatic int encode_addr_size_pairs(struct dma_xfer *xfer, struct wrapper_list *wrappers,\n\t\t\t\t  struct ioctl_resources *resources, u32 msg_hdr_len, u32 *size,\n\t\t\t\t  struct wire_trans_dma_xfer **out_trans)\n{\n\tstruct wrapper_msg *trans_wrapper;\n\tstruct sg_table *sgt = xfer->sgt;\n\tstruct wire_addr_size_pair *asp;\n\tstruct scatterlist *sg;\n\tstruct wrapper_msg *w;\n\tunsigned int dma_len;\n\tu64 dma_chunk_len;\n\tvoid *boundary;\n\tint nents_dma;\n\tint nents;\n\tint i;\n\n\tnents = sgt->nents;\n\tnents_dma = nents;\n\t*size = QAIC_MANAGE_EXT_MSG_LENGTH - msg_hdr_len - sizeof(**out_trans);\n\tfor_each_sgtable_sg(sgt, sg, i) {\n\t\t*size -= sizeof(*asp);\n\t\t \n\t\tif (*size < SZ_1K) {\n\t\t\tnents_dma = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttrans_wrapper = add_wrapper(wrappers, QAIC_WRAPPER_MAX_SIZE);\n\tif (!trans_wrapper)\n\t\treturn -ENOMEM;\n\t*out_trans = (struct wire_trans_dma_xfer *)&trans_wrapper->trans;\n\n\tasp = (*out_trans)->data;\n\tboundary = (void *)trans_wrapper + QAIC_WRAPPER_MAX_SIZE;\n\t*size = 0;\n\n\tdma_len = 0;\n\tw = trans_wrapper;\n\tdma_chunk_len = 0;\n\tfor_each_sg(sgt->sgl, sg, nents_dma, i) {\n\t\tasp->size = cpu_to_le64(dma_len);\n\t\tdma_chunk_len += dma_len;\n\t\tif (dma_len) {\n\t\t\tasp++;\n\t\t\tif ((void *)asp + sizeof(*asp) > boundary) {\n\t\t\t\tw->len = (void *)asp - (void *)&w->msg;\n\t\t\t\t*size += w->len;\n\t\t\t\tw = add_wrapper(wrappers, QAIC_WRAPPER_MAX_SIZE);\n\t\t\t\tif (!w)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tboundary = (void *)w + QAIC_WRAPPER_MAX_SIZE;\n\t\t\t\tasp = (struct wire_addr_size_pair *)&w->msg;\n\t\t\t}\n\t\t}\n\t\tasp->addr = cpu_to_le64(sg_dma_address(sg));\n\t\tdma_len = sg_dma_len(sg);\n\t}\n\t \n\tasp->size = cpu_to_le64(dma_len);\n\tw->len = (void *)asp + sizeof(*asp) - (void *)&w->msg;\n\t*size += w->len;\n\tdma_chunk_len += dma_len;\n\tresources->xferred_dma_size += dma_chunk_len;\n\n\treturn nents_dma < nents ? 1 : 0;\n}\n\nstatic void cleanup_xfer(struct qaic_device *qdev, struct dma_xfer *xfer)\n{\n\tint i;\n\n\tdma_unmap_sgtable(&qdev->pdev->dev, xfer->sgt, DMA_TO_DEVICE, 0);\n\tsg_free_table(xfer->sgt);\n\tkfree(xfer->sgt);\n\tfor (i = 0; i < xfer->nr_pages; ++i)\n\t\tput_page(xfer->page_list[i]);\n\tkfree(xfer->page_list);\n}\n\nstatic int encode_dma(struct qaic_device *qdev, void *trans, struct wrapper_list *wrappers,\n\t\t      u32 *user_len, struct ioctl_resources *resources, struct qaic_user *usr)\n{\n\tstruct qaic_manage_trans_dma_xfer *in_trans = trans;\n\tstruct wire_trans_dma_xfer *out_trans;\n\tstruct wrapper_msg *wrapper;\n\tstruct dma_xfer *xfer;\n\tstruct wire_msg *msg;\n\tbool need_cont_dma;\n\tu32 msg_hdr_len;\n\tu32 size;\n\tint ret;\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\tmsg_hdr_len = le32_to_cpu(msg->hdr.len);\n\n\t \n\tif (size_add(msg_hdr_len, sizeof(*out_trans) + sizeof(struct wire_addr_size_pair)) >\n\t    QAIC_MANAGE_EXT_MSG_LENGTH)\n\t\treturn -ENOMEM;\n\n\txfer = kmalloc(sizeof(*xfer), GFP_KERNEL);\n\tif (!xfer)\n\t\treturn -ENOMEM;\n\n\tret = find_and_map_user_pages(qdev, in_trans, resources, xfer);\n\tif (ret < 0)\n\t\tgoto free_xfer;\n\n\tneed_cont_dma = (bool)ret;\n\n\tret = encode_addr_size_pairs(xfer, wrappers, resources, msg_hdr_len, &size, &out_trans);\n\tif (ret < 0)\n\t\tgoto cleanup_xfer;\n\n\tneed_cont_dma = need_cont_dma || (bool)ret;\n\n\tmsg->hdr.len = cpu_to_le32(msg_hdr_len + size);\n\tmsg->hdr.count = incr_le32(msg->hdr.count);\n\n\tout_trans->hdr.type = cpu_to_le32(QAIC_TRANS_DMA_XFER_TO_DEV);\n\tout_trans->hdr.len = cpu_to_le32(size);\n\tout_trans->tag = cpu_to_le32(in_trans->tag);\n\tout_trans->count = cpu_to_le32((size - sizeof(*out_trans)) /\n\t\t\t\t\t\t\t\tsizeof(struct wire_addr_size_pair));\n\n\t*user_len += in_trans->hdr.len;\n\n\tif (resources->dma_chunk_id) {\n\t\tout_trans->dma_chunk_id = cpu_to_le32(resources->dma_chunk_id);\n\t} else if (need_cont_dma) {\n\t\twhile (resources->dma_chunk_id == 0)\n\t\t\tresources->dma_chunk_id = atomic_inc_return(&usr->chunk_id);\n\n\t\tout_trans->dma_chunk_id = cpu_to_le32(resources->dma_chunk_id);\n\t}\n\tresources->trans_hdr = trans;\n\n\tlist_add(&xfer->list, &resources->dma_xfers);\n\treturn 0;\n\ncleanup_xfer:\n\tcleanup_xfer(qdev, xfer);\nfree_xfer:\n\tkfree(xfer);\n\treturn ret;\n}\n\nstatic int encode_activate(struct qaic_device *qdev, void *trans, struct wrapper_list *wrappers,\n\t\t\t   u32 *user_len, struct ioctl_resources *resources)\n{\n\tstruct qaic_manage_trans_activate_to_dev *in_trans = trans;\n\tstruct wire_trans_activate_to_dev *out_trans;\n\tstruct wrapper_msg *trans_wrapper;\n\tstruct wrapper_msg *wrapper;\n\tstruct wire_msg *msg;\n\tdma_addr_t dma_addr;\n\tu32 msg_hdr_len;\n\tvoid *buf;\n\tu32 nelem;\n\tu32 size;\n\tint ret;\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\tmsg_hdr_len = le32_to_cpu(msg->hdr.len);\n\n\tif (size_add(msg_hdr_len, sizeof(*out_trans)) > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\tif (!in_trans->queue_size)\n\t\treturn -EINVAL;\n\n\tif (in_trans->pad)\n\t\treturn -EINVAL;\n\n\tnelem = in_trans->queue_size;\n\tsize = (get_dbc_req_elem_size() + get_dbc_rsp_elem_size()) * nelem;\n\tif (size / nelem != get_dbc_req_elem_size() + get_dbc_rsp_elem_size())\n\t\treturn -EINVAL;\n\n\tif (size + QAIC_DBC_Q_GAP + QAIC_DBC_Q_BUF_ALIGN < size)\n\t\treturn -EINVAL;\n\n\tsize = ALIGN((size + QAIC_DBC_Q_GAP), QAIC_DBC_Q_BUF_ALIGN);\n\n\tbuf = dma_alloc_coherent(&qdev->pdev->dev, size, &dma_addr, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\ttrans_wrapper = add_wrapper(wrappers,\n\t\t\t\t    offsetof(struct wrapper_msg, trans) + sizeof(*out_trans));\n\tif (!trans_wrapper) {\n\t\tret = -ENOMEM;\n\t\tgoto free_dma;\n\t}\n\ttrans_wrapper->len = sizeof(*out_trans);\n\tout_trans = (struct wire_trans_activate_to_dev *)&trans_wrapper->trans;\n\n\tout_trans->hdr.type = cpu_to_le32(QAIC_TRANS_ACTIVATE_TO_DEV);\n\tout_trans->hdr.len = cpu_to_le32(sizeof(*out_trans));\n\tout_trans->buf_len = cpu_to_le32(size);\n\tout_trans->req_q_addr = cpu_to_le64(dma_addr);\n\tout_trans->req_q_size = cpu_to_le32(nelem);\n\tout_trans->rsp_q_addr = cpu_to_le64(dma_addr + size - nelem * get_dbc_rsp_elem_size());\n\tout_trans->rsp_q_size = cpu_to_le32(nelem);\n\tout_trans->options = cpu_to_le32(in_trans->options);\n\n\t*user_len += in_trans->hdr.len;\n\tmsg->hdr.len = cpu_to_le32(msg_hdr_len + sizeof(*out_trans));\n\tmsg->hdr.count = incr_le32(msg->hdr.count);\n\n\tresources->buf = buf;\n\tresources->dma_addr = dma_addr;\n\tresources->total_size = size;\n\tresources->nelem = nelem;\n\tresources->rsp_q_base = buf + size - nelem * get_dbc_rsp_elem_size();\n\treturn 0;\n\nfree_dma:\n\tdma_free_coherent(&qdev->pdev->dev, size, buf, dma_addr);\n\treturn ret;\n}\n\nstatic int encode_deactivate(struct qaic_device *qdev, void *trans,\n\t\t\t     u32 *user_len, struct qaic_user *usr)\n{\n\tstruct qaic_manage_trans_deactivate *in_trans = trans;\n\n\tif (in_trans->dbc_id >= qdev->num_dbc || in_trans->pad)\n\t\treturn -EINVAL;\n\n\t*user_len += in_trans->hdr.len;\n\n\treturn disable_dbc(qdev, in_trans->dbc_id, usr);\n}\n\nstatic int encode_status(struct qaic_device *qdev, void *trans, struct wrapper_list *wrappers,\n\t\t\t u32 *user_len)\n{\n\tstruct qaic_manage_trans_status_to_dev *in_trans = trans;\n\tstruct wire_trans_status_to_dev *out_trans;\n\tstruct wrapper_msg *trans_wrapper;\n\tstruct wrapper_msg *wrapper;\n\tstruct wire_msg *msg;\n\tu32 msg_hdr_len;\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\tmsg_hdr_len = le32_to_cpu(msg->hdr.len);\n\n\tif (size_add(msg_hdr_len, in_trans->hdr.len) > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\ttrans_wrapper = add_wrapper(wrappers, sizeof(*trans_wrapper));\n\tif (!trans_wrapper)\n\t\treturn -ENOMEM;\n\n\ttrans_wrapper->len = sizeof(*out_trans);\n\tout_trans = (struct wire_trans_status_to_dev *)&trans_wrapper->trans;\n\n\tout_trans->hdr.type = cpu_to_le32(QAIC_TRANS_STATUS_TO_DEV);\n\tout_trans->hdr.len = cpu_to_le32(in_trans->hdr.len);\n\tmsg->hdr.len = cpu_to_le32(msg_hdr_len + in_trans->hdr.len);\n\tmsg->hdr.count = incr_le32(msg->hdr.count);\n\t*user_len += in_trans->hdr.len;\n\n\treturn 0;\n}\n\nstatic int encode_message(struct qaic_device *qdev, struct manage_msg *user_msg,\n\t\t\t  struct wrapper_list *wrappers, struct ioctl_resources *resources,\n\t\t\t  struct qaic_user *usr)\n{\n\tstruct qaic_manage_trans_hdr *trans_hdr;\n\tstruct wrapper_msg *wrapper;\n\tstruct wire_msg *msg;\n\tu32 user_len = 0;\n\tint ret;\n\tint i;\n\n\tif (!user_msg->count ||\n\t    user_msg->len < sizeof(*trans_hdr)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\n\tmsg->hdr.len = cpu_to_le32(sizeof(msg->hdr));\n\n\tif (resources->dma_chunk_id) {\n\t\tret = encode_dma(qdev, resources->trans_hdr, wrappers, &user_len, resources, usr);\n\t\tmsg->hdr.count = cpu_to_le32(1);\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < user_msg->count; ++i) {\n\t\tif (user_len > user_msg->len - sizeof(*trans_hdr)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\ttrans_hdr = (struct qaic_manage_trans_hdr *)(user_msg->data + user_len);\n\t\tif (trans_hdr->len < sizeof(trans_hdr) ||\n\t\t    size_add(user_len, trans_hdr->len) > user_msg->len) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (trans_hdr->type) {\n\t\tcase QAIC_TRANS_PASSTHROUGH_FROM_USR:\n\t\t\tret = encode_passthrough(qdev, trans_hdr, wrappers, &user_len);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_DMA_XFER_FROM_USR:\n\t\t\tret = encode_dma(qdev, trans_hdr, wrappers, &user_len, resources, usr);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_ACTIVATE_FROM_USR:\n\t\t\tret = encode_activate(qdev, trans_hdr, wrappers, &user_len, resources);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_DEACTIVATE_FROM_USR:\n\t\t\tret = encode_deactivate(qdev, trans_hdr, &user_len, usr);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_STATUS_FROM_USR:\n\t\t\tret = encode_status(qdev, trans_hdr, wrappers, &user_len);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (user_len != user_msg->len)\n\t\tret = -EINVAL;\nout:\n\tif (ret) {\n\t\tfree_dma_xfers(qdev, resources);\n\t\tfree_dbc_buf(qdev, resources);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int decode_passthrough(struct qaic_device *qdev, void *trans, struct manage_msg *user_msg,\n\t\t\t      u32 *msg_len)\n{\n\tstruct qaic_manage_trans_passthrough *out_trans;\n\tstruct wire_trans_passthrough *in_trans = trans;\n\tu32 len;\n\n\tout_trans = (void *)user_msg->data + user_msg->len;\n\n\tlen = le32_to_cpu(in_trans->hdr.len);\n\tif (len % 8 != 0)\n\t\treturn -EINVAL;\n\n\tif (user_msg->len + len > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\tmemcpy(out_trans->data, in_trans->data, len - sizeof(in_trans->hdr));\n\tuser_msg->len += len;\n\t*msg_len += len;\n\tout_trans->hdr.type = le32_to_cpu(in_trans->hdr.type);\n\tout_trans->hdr.len = len;\n\n\treturn 0;\n}\n\nstatic int decode_activate(struct qaic_device *qdev, void *trans, struct manage_msg *user_msg,\n\t\t\t   u32 *msg_len, struct ioctl_resources *resources, struct qaic_user *usr)\n{\n\tstruct qaic_manage_trans_activate_from_dev *out_trans;\n\tstruct wire_trans_activate_from_dev *in_trans = trans;\n\tu32 len;\n\n\tout_trans = (void *)user_msg->data + user_msg->len;\n\n\tlen = le32_to_cpu(in_trans->hdr.len);\n\tif (user_msg->len + len > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\tuser_msg->len += len;\n\t*msg_len += len;\n\tout_trans->hdr.type = le32_to_cpu(in_trans->hdr.type);\n\tout_trans->hdr.len = len;\n\tout_trans->status = le32_to_cpu(in_trans->status);\n\tout_trans->dbc_id = le32_to_cpu(in_trans->dbc_id);\n\tout_trans->options = le64_to_cpu(in_trans->options);\n\n\tif (!resources->buf)\n\t\t \n\t\treturn -EINVAL;\n\n\tif (out_trans->dbc_id >= qdev->num_dbc)\n\t\t \n\t\treturn -ENODEV;\n\n\tif (out_trans->status)\n\t\t \n\t\treturn -ECANCELED;\n\n\tresources->status = out_trans->status;\n\tresources->dbc_id = out_trans->dbc_id;\n\tsave_dbc_buf(qdev, resources, usr);\n\n\treturn 0;\n}\n\nstatic int decode_deactivate(struct qaic_device *qdev, void *trans, u32 *msg_len,\n\t\t\t     struct qaic_user *usr)\n{\n\tstruct wire_trans_deactivate_from_dev *in_trans = trans;\n\tu32 dbc_id = le32_to_cpu(in_trans->dbc_id);\n\tu32 status = le32_to_cpu(in_trans->status);\n\n\tif (dbc_id >= qdev->num_dbc)\n\t\t \n\t\treturn -ENODEV;\n\n\tif (status) {\n\t\t \n\t\tenable_dbc(qdev, dbc_id, usr);\n\t\treturn -ECANCELED;\n\t}\n\n\trelease_dbc(qdev, dbc_id);\n\t*msg_len += sizeof(*in_trans);\n\n\treturn 0;\n}\n\nstatic int decode_status(struct qaic_device *qdev, void *trans, struct manage_msg *user_msg,\n\t\t\t u32 *user_len, struct wire_msg *msg)\n{\n\tstruct qaic_manage_trans_status_from_dev *out_trans;\n\tstruct wire_trans_status_from_dev *in_trans = trans;\n\tu32 len;\n\n\tout_trans = (void *)user_msg->data + user_msg->len;\n\n\tlen = le32_to_cpu(in_trans->hdr.len);\n\tif (user_msg->len + len > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -ENOSPC;\n\n\tout_trans->hdr.type = QAIC_TRANS_STATUS_FROM_DEV;\n\tout_trans->hdr.len = len;\n\tout_trans->major = le16_to_cpu(in_trans->major);\n\tout_trans->minor = le16_to_cpu(in_trans->minor);\n\tout_trans->status_flags = le64_to_cpu(in_trans->status_flags);\n\tout_trans->status = le32_to_cpu(in_trans->status);\n\t*user_len += le32_to_cpu(in_trans->hdr.len);\n\tuser_msg->len += len;\n\n\tif (out_trans->status)\n\t\treturn -ECANCELED;\n\tif (out_trans->status_flags & BIT(0) && !valid_crc(msg))\n\t\treturn -EPIPE;\n\n\treturn 0;\n}\n\nstatic int decode_message(struct qaic_device *qdev, struct manage_msg *user_msg,\n\t\t\t  struct wire_msg *msg, struct ioctl_resources *resources,\n\t\t\t  struct qaic_user *usr)\n{\n\tu32 msg_hdr_len = le32_to_cpu(msg->hdr.len);\n\tstruct wire_trans_hdr *trans_hdr;\n\tu32 msg_len = 0;\n\tint ret;\n\tint i;\n\n\tif (msg_hdr_len < sizeof(*trans_hdr) ||\n\t    msg_hdr_len > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -EINVAL;\n\n\tuser_msg->len = 0;\n\tuser_msg->count = le32_to_cpu(msg->hdr.count);\n\n\tfor (i = 0; i < user_msg->count; ++i) {\n\t\tu32 hdr_len;\n\n\t\tif (msg_len > msg_hdr_len - sizeof(*trans_hdr))\n\t\t\treturn -EINVAL;\n\n\t\ttrans_hdr = (struct wire_trans_hdr *)(msg->data + msg_len);\n\t\thdr_len = le32_to_cpu(trans_hdr->len);\n\t\tif (hdr_len < sizeof(*trans_hdr) ||\n\t\t    size_add(msg_len, hdr_len) > msg_hdr_len)\n\t\t\treturn -EINVAL;\n\n\t\tswitch (le32_to_cpu(trans_hdr->type)) {\n\t\tcase QAIC_TRANS_PASSTHROUGH_FROM_DEV:\n\t\t\tret = decode_passthrough(qdev, trans_hdr, user_msg, &msg_len);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_ACTIVATE_FROM_DEV:\n\t\t\tret = decode_activate(qdev, trans_hdr, user_msg, &msg_len, resources, usr);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_DEACTIVATE_FROM_DEV:\n\t\t\tret = decode_deactivate(qdev, trans_hdr, &msg_len, usr);\n\t\t\tbreak;\n\t\tcase QAIC_TRANS_STATUS_FROM_DEV:\n\t\t\tret = decode_status(qdev, trans_hdr, user_msg, &msg_len, msg);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (msg_len != (msg_hdr_len - sizeof(msg->hdr)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void *msg_xfer(struct qaic_device *qdev, struct wrapper_list *wrappers, u32 seq_num,\n\t\t      bool ignore_signal)\n{\n\tstruct xfer_queue_elem elem;\n\tstruct wire_msg *out_buf;\n\tstruct wrapper_msg *w;\n\tlong ret = -EAGAIN;\n\tint xfer_count = 0;\n\tint retry_count;\n\n\tif (qdev->in_reset) {\n\t\tmutex_unlock(&qdev->cntl_mutex);\n\t\treturn ERR_PTR(-ENODEV);\n\t}\n\n\t \n\tlist_for_each_entry(w, &wrappers->list, list)\n\t\txfer_count++;\n\n\tfor (retry_count = 0; retry_count < QAIC_MHI_RETRY_MAX; retry_count++) {\n\t\tif (xfer_count <= mhi_get_free_desc_count(qdev->cntl_ch, DMA_TO_DEVICE)) {\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tmsleep_interruptible(QAIC_MHI_RETRY_WAIT_MS);\n\t\tif (signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\tif (ret) {\n\t\tmutex_unlock(&qdev->cntl_mutex);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\telem.seq_num = seq_num;\n\telem.buf = NULL;\n\tinit_completion(&elem.xfer_done);\n\tif (likely(!qdev->cntl_lost_buf)) {\n\t\t \n\t\tout_buf = kmalloc(QAIC_MANAGE_MAX_MSG_LENGTH, GFP_KERNEL);\n\t\tif (!out_buf) {\n\t\t\tmutex_unlock(&qdev->cntl_mutex);\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\t}\n\n\t\tret = mhi_queue_buf(qdev->cntl_ch, DMA_FROM_DEVICE, out_buf,\n\t\t\t\t    QAIC_MANAGE_MAX_MSG_LENGTH, MHI_EOT);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&qdev->cntl_mutex);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t} else {\n\t\t \n\t\tqdev->cntl_lost_buf = false;\n\t}\n\n\tlist_for_each_entry(w, &wrappers->list, list) {\n\t\tkref_get(&w->ref_count);\n\t\tretry_count = 0;\n\t\tret = mhi_queue_buf(qdev->cntl_ch, DMA_TO_DEVICE, &w->msg, w->len,\n\t\t\t\t    list_is_last(&w->list, &wrappers->list) ? MHI_EOT : MHI_CHAIN);\n\t\tif (ret) {\n\t\t\tqdev->cntl_lost_buf = true;\n\t\t\tkref_put(&w->ref_count, free_wrapper);\n\t\t\tmutex_unlock(&qdev->cntl_mutex);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t}\n\n\tlist_add_tail(&elem.list, &qdev->cntl_xfer_list);\n\tmutex_unlock(&qdev->cntl_mutex);\n\n\tif (ignore_signal)\n\t\tret = wait_for_completion_timeout(&elem.xfer_done, control_resp_timeout_s * HZ);\n\telse\n\t\tret = wait_for_completion_interruptible_timeout(&elem.xfer_done,\n\t\t\t\t\t\t\t\tcontrol_resp_timeout_s * HZ);\n\t \n\tmutex_lock(&qdev->cntl_mutex);\n\tif (!list_empty(&elem.list))\n\t\tlist_del(&elem.list);\n\tif (!ret && !elem.buf)\n\t\tret = -ETIMEDOUT;\n\telse if (ret > 0 && !elem.buf)\n\t\tret = -EIO;\n\tmutex_unlock(&qdev->cntl_mutex);\n\n\tif (ret < 0) {\n\t\tkfree(elem.buf);\n\t\treturn ERR_PTR(ret);\n\t} else if (!qdev->valid_crc(elem.buf)) {\n\t\tkfree(elem.buf);\n\t\treturn ERR_PTR(-EPIPE);\n\t}\n\n\treturn elem.buf;\n}\n\n \nstatic int abort_dma_cont(struct qaic_device *qdev, struct wrapper_list *wrappers, u32 dma_chunk_id)\n{\n\tstruct wire_trans_dma_xfer *out_trans;\n\tu32 size = sizeof(*out_trans);\n\tstruct wrapper_msg *wrapper;\n\tstruct wrapper_msg *w;\n\tstruct wire_msg *msg;\n\n\twrapper = list_first_entry(&wrappers->list, struct wrapper_msg, list);\n\tmsg = &wrapper->msg;\n\n\t \n\tlist_for_each_entry_safe(wrapper, w, &wrappers->list, list)\n\t\tif (!list_is_first(&wrapper->list, &wrappers->list))\n\t\t\tkref_put(&wrapper->ref_count, free_wrapper);\n\n\twrapper = add_wrapper(wrappers, offsetof(struct wrapper_msg, trans) + sizeof(*out_trans));\n\n\tif (!wrapper)\n\t\treturn -ENOMEM;\n\n\tout_trans = (struct wire_trans_dma_xfer *)&wrapper->trans;\n\tout_trans->hdr.type = cpu_to_le32(QAIC_TRANS_DMA_XFER_TO_DEV);\n\tout_trans->hdr.len = cpu_to_le32(size);\n\tout_trans->tag = cpu_to_le32(0);\n\tout_trans->count = cpu_to_le32(0);\n\tout_trans->dma_chunk_id = cpu_to_le32(dma_chunk_id);\n\n\tmsg->hdr.len = cpu_to_le32(size + sizeof(*msg));\n\tmsg->hdr.count = cpu_to_le32(1);\n\twrapper->len = size;\n\n\treturn 0;\n}\n\nstatic struct wrapper_list *alloc_wrapper_list(void)\n{\n\tstruct wrapper_list *wrappers;\n\n\twrappers = kmalloc(sizeof(*wrappers), GFP_KERNEL);\n\tif (!wrappers)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&wrappers->list);\n\tspin_lock_init(&wrappers->lock);\n\n\treturn wrappers;\n}\n\nstatic int qaic_manage_msg_xfer(struct qaic_device *qdev, struct qaic_user *usr,\n\t\t\t\tstruct manage_msg *user_msg, struct ioctl_resources *resources,\n\t\t\t\tstruct wire_msg **rsp)\n{\n\tstruct wrapper_list *wrappers;\n\tstruct wrapper_msg *wrapper;\n\tstruct wrapper_msg *w;\n\tbool all_done = false;\n\tstruct wire_msg *msg;\n\tint ret;\n\n\twrappers = alloc_wrapper_list();\n\tif (!wrappers)\n\t\treturn -ENOMEM;\n\n\twrapper = add_wrapper(wrappers, sizeof(*wrapper));\n\tif (!wrapper) {\n\t\tkfree(wrappers);\n\t\treturn -ENOMEM;\n\t}\n\n\tmsg = &wrapper->msg;\n\twrapper->len = sizeof(*msg);\n\n\tret = encode_message(qdev, user_msg, wrappers, resources, usr);\n\tif (ret && resources->dma_chunk_id)\n\t\tret = abort_dma_cont(qdev, wrappers, resources->dma_chunk_id);\n\tif (ret)\n\t\tgoto encode_failed;\n\n\tret = mutex_lock_interruptible(&qdev->cntl_mutex);\n\tif (ret)\n\t\tgoto lock_failed;\n\n\tmsg->hdr.magic_number = MANAGE_MAGIC_NUMBER;\n\tmsg->hdr.sequence_number = cpu_to_le32(qdev->next_seq_num++);\n\n\tif (usr) {\n\t\tmsg->hdr.handle = cpu_to_le32(usr->handle);\n\t\tmsg->hdr.partition_id = cpu_to_le32(usr->qddev->partition_id);\n\t} else {\n\t\tmsg->hdr.handle = 0;\n\t\tmsg->hdr.partition_id = cpu_to_le32(QAIC_NO_PARTITION);\n\t}\n\n\tmsg->hdr.padding = cpu_to_le32(0);\n\tmsg->hdr.crc32 = cpu_to_le32(qdev->gen_crc(wrappers));\n\n\t \n\t*rsp = msg_xfer(qdev, wrappers, qdev->next_seq_num - 1, false);\n\tif (IS_ERR(*rsp))\n\t\tret = PTR_ERR(*rsp);\n\nlock_failed:\n\tfree_dma_xfers(qdev, resources);\nencode_failed:\n\tspin_lock(&wrappers->lock);\n\tlist_for_each_entry_safe(wrapper, w, &wrappers->list, list)\n\t\tkref_put(&wrapper->ref_count, free_wrapper);\n\tall_done = list_empty(&wrappers->list);\n\tspin_unlock(&wrappers->lock);\n\tif (all_done)\n\t\tkfree(wrappers);\n\n\treturn ret;\n}\n\nstatic int qaic_manage(struct qaic_device *qdev, struct qaic_user *usr, struct manage_msg *user_msg)\n{\n\tstruct wire_trans_dma_xfer_cont *dma_cont = NULL;\n\tstruct ioctl_resources resources;\n\tstruct wire_msg *rsp = NULL;\n\tint ret;\n\n\tmemset(&resources, 0, sizeof(struct ioctl_resources));\n\n\tINIT_LIST_HEAD(&resources.dma_xfers);\n\n\tif (user_msg->len > QAIC_MANAGE_MAX_MSG_LENGTH ||\n\t    user_msg->count > QAIC_MANAGE_MAX_MSG_LENGTH / sizeof(struct qaic_manage_trans_hdr))\n\t\treturn -EINVAL;\n\ndma_xfer_continue:\n\tret = qaic_manage_msg_xfer(qdev, usr, user_msg, &resources, &rsp);\n\tif (ret)\n\t\treturn ret;\n\t \n\tif (le32_to_cpu(rsp->hdr.count) == 1) {\n\t\tdma_cont = (struct wire_trans_dma_xfer_cont *)rsp->data;\n\t\tif (le32_to_cpu(dma_cont->hdr.type) != QAIC_TRANS_DMA_XFER_CONT)\n\t\t\tdma_cont = NULL;\n\t}\n\tif (dma_cont) {\n\t\tif (le32_to_cpu(dma_cont->dma_chunk_id) == resources.dma_chunk_id &&\n\t\t    le64_to_cpu(dma_cont->xferred_size) == resources.xferred_dma_size) {\n\t\t\tkfree(rsp);\n\t\t\tgoto dma_xfer_continue;\n\t\t}\n\n\t\tret = -EINVAL;\n\t\tgoto dma_cont_failed;\n\t}\n\n\tret = decode_message(qdev, user_msg, rsp, &resources, usr);\n\ndma_cont_failed:\n\tfree_dbc_buf(qdev, &resources);\n\tkfree(rsp);\n\treturn ret;\n}\n\nint qaic_manage_ioctl(struct drm_device *dev, void *data, struct drm_file *file_priv)\n{\n\tstruct qaic_manage_msg *user_msg = data;\n\tstruct qaic_device *qdev;\n\tstruct manage_msg *msg;\n\tstruct qaic_user *usr;\n\tu8 __user *user_data;\n\tint qdev_rcu_id;\n\tint usr_rcu_id;\n\tint ret;\n\n\tif (user_msg->len > QAIC_MANAGE_MAX_MSG_LENGTH)\n\t\treturn -EINVAL;\n\n\tusr = file_priv->driver_priv;\n\n\tusr_rcu_id = srcu_read_lock(&usr->qddev_lock);\n\tif (!usr->qddev) {\n\t\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\t\treturn -ENODEV;\n\t}\n\n\tqdev = usr->qddev->qdev;\n\n\tqdev_rcu_id = srcu_read_lock(&qdev->dev_lock);\n\tif (qdev->in_reset) {\n\t\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\n\t\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\t\treturn -ENODEV;\n\t}\n\n\tmsg = kzalloc(QAIC_MANAGE_MAX_MSG_LENGTH + sizeof(*msg), GFP_KERNEL);\n\tif (!msg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tmsg->len = user_msg->len;\n\tmsg->count = user_msg->count;\n\n\tuser_data = u64_to_user_ptr(user_msg->data);\n\n\tif (copy_from_user(msg->data, user_data, user_msg->len)) {\n\t\tret = -EFAULT;\n\t\tgoto free_msg;\n\t}\n\n\tret = qaic_manage(qdev, usr, msg);\n\n\t \n\tif (ret == -ECANCELED || !ret) {\n\t\tif (copy_to_user(user_data, msg->data, msg->len)) {\n\t\t\tret = -EFAULT;\n\t\t} else {\n\t\t\tuser_msg->len = msg->len;\n\t\t\tuser_msg->count = msg->count;\n\t\t}\n\t}\n\nfree_msg:\n\tkfree(msg);\nout:\n\tsrcu_read_unlock(&qdev->dev_lock, qdev_rcu_id);\n\tsrcu_read_unlock(&usr->qddev_lock, usr_rcu_id);\n\treturn ret;\n}\n\nint get_cntl_version(struct qaic_device *qdev, struct qaic_user *usr, u16 *major, u16 *minor)\n{\n\tstruct qaic_manage_trans_status_from_dev *status_result;\n\tstruct qaic_manage_trans_status_to_dev *status_query;\n\tstruct manage_msg *user_msg;\n\tint ret;\n\n\tuser_msg = kmalloc(sizeof(*user_msg) + sizeof(*status_result), GFP_KERNEL);\n\tif (!user_msg) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tuser_msg->len = sizeof(*status_query);\n\tuser_msg->count = 1;\n\n\tstatus_query = (struct qaic_manage_trans_status_to_dev *)user_msg->data;\n\tstatus_query->hdr.type = QAIC_TRANS_STATUS_FROM_USR;\n\tstatus_query->hdr.len = sizeof(status_query->hdr);\n\n\tret = qaic_manage(qdev, usr, user_msg);\n\tif (ret)\n\t\tgoto kfree_user_msg;\n\tstatus_result = (struct qaic_manage_trans_status_from_dev *)user_msg->data;\n\t*major = status_result->major;\n\t*minor = status_result->minor;\n\n\tif (status_result->status_flags & BIT(0)) {  \n\t\t \n\t\tqdev->valid_crc = valid_crc;\n\t} else {\n\t\t \n\t\tqdev->gen_crc = gen_crc_stub;\n\t}\n\nkfree_user_msg:\n\tkfree(user_msg);\nout:\n\treturn ret;\n}\n\nstatic void resp_worker(struct work_struct *work)\n{\n\tstruct resp_work *resp = container_of(work, struct resp_work, work);\n\tstruct qaic_device *qdev = resp->qdev;\n\tstruct wire_msg *msg = resp->buf;\n\tstruct xfer_queue_elem *elem;\n\tstruct xfer_queue_elem *i;\n\tbool found = false;\n\n\tmutex_lock(&qdev->cntl_mutex);\n\tlist_for_each_entry_safe(elem, i, &qdev->cntl_xfer_list, list) {\n\t\tif (elem->seq_num == le32_to_cpu(msg->hdr.sequence_number)) {\n\t\t\tfound = true;\n\t\t\tlist_del_init(&elem->list);\n\t\t\telem->buf = msg;\n\t\t\tcomplete_all(&elem->xfer_done);\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&qdev->cntl_mutex);\n\n\tif (!found)\n\t\t \n\t\tkfree(msg);\n\n\tkfree(resp);\n}\n\nstatic void free_wrapper_from_list(struct wrapper_list *wrappers, struct wrapper_msg *wrapper)\n{\n\tbool all_done = false;\n\n\tspin_lock(&wrappers->lock);\n\tkref_put(&wrapper->ref_count, free_wrapper);\n\tall_done = list_empty(&wrappers->list);\n\tspin_unlock(&wrappers->lock);\n\n\tif (all_done)\n\t\tkfree(wrappers);\n}\n\nvoid qaic_mhi_ul_xfer_cb(struct mhi_device *mhi_dev, struct mhi_result *mhi_result)\n{\n\tstruct wire_msg *msg = mhi_result->buf_addr;\n\tstruct wrapper_msg *wrapper = container_of(msg, struct wrapper_msg, msg);\n\n\tfree_wrapper_from_list(wrapper->head, wrapper);\n}\n\nvoid qaic_mhi_dl_xfer_cb(struct mhi_device *mhi_dev, struct mhi_result *mhi_result)\n{\n\tstruct qaic_device *qdev = dev_get_drvdata(&mhi_dev->dev);\n\tstruct wire_msg *msg = mhi_result->buf_addr;\n\tstruct resp_work *resp;\n\n\tif (mhi_result->transaction_status || msg->hdr.magic_number != MANAGE_MAGIC_NUMBER) {\n\t\tkfree(msg);\n\t\treturn;\n\t}\n\n\tresp = kmalloc(sizeof(*resp), GFP_ATOMIC);\n\tif (!resp) {\n\t\tkfree(msg);\n\t\treturn;\n\t}\n\n\tINIT_WORK(&resp->work, resp_worker);\n\tresp->qdev = qdev;\n\tresp->buf = msg;\n\tqueue_work(qdev->cntl_wq, &resp->work);\n}\n\nint qaic_control_open(struct qaic_device *qdev)\n{\n\tif (!qdev->cntl_ch)\n\t\treturn -ENODEV;\n\n\tqdev->cntl_lost_buf = false;\n\t \n\tqdev->gen_crc = gen_crc;\n\tqdev->valid_crc = valid_crc_stub;\n\n\treturn mhi_prepare_for_transfer(qdev->cntl_ch);\n}\n\nvoid qaic_control_close(struct qaic_device *qdev)\n{\n\tmhi_unprepare_from_transfer(qdev->cntl_ch);\n}\n\nvoid qaic_release_usr(struct qaic_device *qdev, struct qaic_user *usr)\n{\n\tstruct wire_trans_terminate_to_dev *trans;\n\tstruct wrapper_list *wrappers;\n\tstruct wrapper_msg *wrapper;\n\tstruct wire_msg *msg;\n\tstruct wire_msg *rsp;\n\n\twrappers = alloc_wrapper_list();\n\tif (!wrappers)\n\t\treturn;\n\n\twrapper = add_wrapper(wrappers, sizeof(*wrapper) + sizeof(*msg) + sizeof(*trans));\n\tif (!wrapper)\n\t\treturn;\n\n\tmsg = &wrapper->msg;\n\n\ttrans = (struct wire_trans_terminate_to_dev *)msg->data;\n\n\ttrans->hdr.type = cpu_to_le32(QAIC_TRANS_TERMINATE_TO_DEV);\n\ttrans->hdr.len = cpu_to_le32(sizeof(*trans));\n\ttrans->handle = cpu_to_le32(usr->handle);\n\n\tmutex_lock(&qdev->cntl_mutex);\n\twrapper->len = sizeof(msg->hdr) + sizeof(*trans);\n\tmsg->hdr.magic_number = MANAGE_MAGIC_NUMBER;\n\tmsg->hdr.sequence_number = cpu_to_le32(qdev->next_seq_num++);\n\tmsg->hdr.len = cpu_to_le32(wrapper->len);\n\tmsg->hdr.count = cpu_to_le32(1);\n\tmsg->hdr.handle = cpu_to_le32(usr->handle);\n\tmsg->hdr.padding = cpu_to_le32(0);\n\tmsg->hdr.crc32 = cpu_to_le32(qdev->gen_crc(wrappers));\n\n\t \n\trsp = msg_xfer(qdev, wrappers, qdev->next_seq_num - 1, true);\n\tif (!IS_ERR(rsp))\n\t\tkfree(rsp);\n\tfree_wrapper_from_list(wrappers, wrapper);\n}\n\nvoid wake_all_cntl(struct qaic_device *qdev)\n{\n\tstruct xfer_queue_elem *elem;\n\tstruct xfer_queue_elem *i;\n\n\tmutex_lock(&qdev->cntl_mutex);\n\tlist_for_each_entry_safe(elem, i, &qdev->cntl_xfer_list, list) {\n\t\tlist_del_init(&elem->list);\n\t\tcomplete_all(&elem->xfer_done);\n\t}\n\tmutex_unlock(&qdev->cntl_mutex);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}