{
  "module_name": "bcm-pdc-mailbox.c",
  "hash_id": "6b8bf8ed3c7656c552fc5532974b252c5da36c57a7c3ae4501676eed4cd878da",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mailbox/bcm-pdc-mailbox.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/errno.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/debugfs.h>\n#include <linux/interrupt.h>\n#include <linux/wait.h>\n#include <linux/platform_device.h>\n#include <linux/io.h>\n#include <linux/of.h>\n#include <linux/of_device.h>\n#include <linux/of_address.h>\n#include <linux/of_irq.h>\n#include <linux/mailbox_controller.h>\n#include <linux/mailbox/brcm-message.h>\n#include <linux/scatterlist.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n\n#define PDC_SUCCESS  0\n\n#define RING_ENTRY_SIZE   sizeof(struct dma64dd)\n\n \n#define PDC_RING_ENTRIES  512\n \n#define PDC_RING_SPACE_MIN  15\n\n#define PDC_RING_SIZE    (PDC_RING_ENTRIES * RING_ENTRY_SIZE)\n \n#define RING_ALIGN_ORDER  13\n#define RING_ALIGN        BIT(RING_ALIGN_ORDER)\n\n#define RX_BUF_ALIGN_ORDER  5\n#define RX_BUF_ALIGN\t    BIT(RX_BUF_ALIGN_ORDER)\n\n \n#define XXD(x, max_mask)              ((x) & (max_mask))\n#define TXD(x, max_mask)              XXD((x), (max_mask))\n#define RXD(x, max_mask)              XXD((x), (max_mask))\n#define NEXTTXD(i, max_mask)          TXD((i) + 1, (max_mask))\n#define PREVTXD(i, max_mask)          TXD((i) - 1, (max_mask))\n#define NEXTRXD(i, max_mask)          RXD((i) + 1, (max_mask))\n#define PREVRXD(i, max_mask)          RXD((i) - 1, (max_mask))\n#define NTXDACTIVE(h, t, max_mask)    TXD((t) - (h), (max_mask))\n#define NRXDACTIVE(h, t, max_mask)    RXD((t) - (h), (max_mask))\n\n \n#define BCM_HDR_LEN  8\n\n \n#define PDC_RINGSET  0\n\n \n#define PDC_RCVINT_0         (16 + PDC_RINGSET)\n#define PDC_RCVINTEN_0       BIT(PDC_RCVINT_0)\n#define PDC_INTMASK\t     (PDC_RCVINTEN_0)\n#define PDC_LAZY_FRAMECOUNT  1\n#define PDC_LAZY_TIMEOUT     10000\n#define PDC_LAZY_INT  (PDC_LAZY_TIMEOUT | (PDC_LAZY_FRAMECOUNT << 24))\n#define PDC_INTMASK_OFFSET   0x24\n#define PDC_INTSTATUS_OFFSET 0x20\n#define PDC_RCVLAZY0_OFFSET  (0x30 + 4 * PDC_RINGSET)\n#define FA_RCVLAZY0_OFFSET   0x100\n\n \n#define PDC_SPU2_RESP_HDR_LEN  17\n#define PDC_CKSUM_CTRL         BIT(27)\n#define PDC_CKSUM_CTRL_OFFSET  0x400\n\n#define PDC_SPUM_RESP_HDR_LEN  32\n\n \n#define PDC_TX_CTL\t\t0x000C0800\n\n \n#define PDC_TX_ENABLE\t\t0x1\n\n \n#define PDC_RX_CTL\t\t0x000C0E00\n\n \n#define PDC_RX_ENABLE\t\t0x1\n\n#define CRYPTO_D64_RS0_CD_MASK   ((PDC_RING_ENTRIES * RING_ENTRY_SIZE) - 1)\n\n \n#define D64_CTRL1_EOT   BIT(28)\t \n#define D64_CTRL1_IOC   BIT(29)\t \n#define D64_CTRL1_EOF   BIT(30)\t \n#define D64_CTRL1_SOF   BIT(31)\t \n\n#define RX_STATUS_OVERFLOW       0x00800000\n#define RX_STATUS_LEN            0x0000FFFF\n\n#define PDC_TXREGS_OFFSET  0x200\n#define PDC_RXREGS_OFFSET  0x220\n\n \n#define PDC_DMA_BUF_MAX 16384\n\nenum pdc_hw {\n\tFA_HW,\t\t \n\tPDC_HW\t\t \n};\n\nstruct pdc_dma_map {\n\tvoid *ctx;           \n};\n\n \nstruct dma64dd {\n\tu32 ctrl1;       \n\tu32 ctrl2;       \n\tu32 addrlow;     \n\tu32 addrhigh;    \n};\n\n \nstruct dma64_regs {\n\tu32  control;    \n\tu32  ptr;        \n\tu32  addrlow;    \n\tu32  addrhigh;   \n\tu32  status0;    \n\tu32  status1;    \n};\n\n \n#ifndef PAD\n#define _PADLINE(line)  pad ## line\n#define _XSTR(line)     _PADLINE(line)\n#define PAD             _XSTR(__LINE__)\n#endif   \n\n \nstruct dma64 {\n\tstruct dma64_regs dmaxmt;   \n\tu32          PAD[2];\n\tstruct dma64_regs dmarcv;   \n\tu32          PAD[2];\n};\n\n \nstruct pdc_regs {\n\tu32  devcontrol;              \n\tu32  devstatus;               \n\tu32  PAD;\n\tu32  biststatus;              \n\tu32  PAD[4];\n\tu32  intstatus;               \n\tu32  intmask;                 \n\tu32  gptimer;                 \n\n\tu32  PAD;\n\tu32  intrcvlazy_0;            \n\tu32  intrcvlazy_1;            \n\tu32  intrcvlazy_2;            \n\tu32  intrcvlazy_3;            \n\n\tu32  PAD[48];\n\tu32  fa_intrecvlazy;          \n\tu32  flowctlthresh;           \n\tu32  wrrthresh;               \n\tu32  gmac_idle_cnt_thresh;    \n\n\tu32  PAD[4];\n\tu32  ifioaccessaddr;          \n\tu32  ifioaccessbyte;          \n\tu32  ifioaccessdata;          \n\n\tu32  PAD[21];\n\tu32  phyaccess;               \n\tu32  PAD;\n\tu32  phycontrol;              \n\tu32  txqctl;                  \n\tu32  rxqctl;                  \n\tu32  gpioselect;              \n\tu32  gpio_output_en;          \n\tu32  PAD;                     \n\tu32  txq_rxq_mem_ctl;         \n\tu32  memory_ecc_status;       \n\tu32  serdes_ctl;              \n\tu32  serdes_status0;          \n\tu32  serdes_status1;          \n\tu32  PAD[11];                 \n\tu32  clk_ctl_st;              \n\tu32  hw_war;                  \n\tu32  pwrctl;                  \n\tu32  PAD[5];\n\n#define PDC_NUM_DMA_RINGS   4\n\tstruct dma64 dmaregs[PDC_NUM_DMA_RINGS];   \n\n\t \n};\n\n \nstruct pdc_ring_alloc {\n\tdma_addr_t  dmabase;  \n\tvoid\t   *vbase;    \n\tu32\t    size;     \n};\n\n \nstruct pdc_rx_ctx {\n\tvoid *rxp_ctx;\n\tstruct scatterlist *dst_sg;\n\tu32  rxin_numd;\n\tvoid *resp_hdr;\n\tdma_addr_t resp_hdr_daddr;\n};\n\n \nstruct pdc_state {\n\t \n\tu8 pdc_idx;\n\n\t \n\tstruct platform_device *pdev;\n\n\t \n\tstruct mbox_controller mbc;\n\n\tunsigned int pdc_irq;\n\n\t \n\tstruct tasklet_struct rx_tasklet;\n\n\t \n\tu32 rx_status_len;\n\t \n\tbool use_bcm_hdr;\n\t \n\tu32 pdc_resp_hdr_len;\n\n\t \n\tvoid __iomem *pdc_reg_vbase;\n\n\t \n\tstruct dma_pool *ring_pool;\n\n\t \n\tstruct dma_pool *rx_buf_pool;\n\n\t \n\tstruct pdc_ring_alloc tx_ring_alloc;\n\tstruct pdc_ring_alloc rx_ring_alloc;\n\n\tstruct pdc_regs *regs;     \n\n\tstruct dma64_regs *txregs_64;  \n\tstruct dma64_regs *rxregs_64;  \n\n\t \n\tstruct dma64dd   *txd_64;   \n\tstruct dma64dd   *rxd_64;   \n\n\t \n\tu32      ntxd;        \n\tu32      nrxd;        \n\tu32      nrxpost;     \n\tu32      ntxpost;     \n\n\t \n\tu32  txin;\n\n\t \n\tu32  tx_msg_start;\n\n\t \n\tu32  txout;\n\n\t \n\tu32      txin_numd[PDC_RING_ENTRIES];\n\n\t \n\tu32  rxin;\n\n\t \n\tu32  rx_msg_start;\n\n\t \n\tu32  last_rx_curr;\n\n\t \n\tu32  rxout;\n\n\tstruct pdc_rx_ctx rx_ctx[PDC_RING_ENTRIES];\n\n\t \n\tstruct scatterlist *src_sg[PDC_RING_ENTRIES];\n\n\t \n\tu32  pdc_requests;      \n\tu32  pdc_replies;       \n\tu32  last_tx_not_done;  \n\tu32  tx_ring_full;      \n\tu32  rx_ring_full;      \n\tu32  txnobuf;           \n\tu32  rxnobuf;           \n\tu32  rx_oflow;          \n\n\t \n\tenum pdc_hw hw_type;\n};\n\n \n\nstruct pdc_globals {\n\t \n\tu32 num_spu;\n};\n\nstatic struct pdc_globals pdcg;\n\n \nstatic struct dentry *debugfs_dir;\n\nstatic ssize_t pdc_debugfs_read(struct file *filp, char __user *ubuf,\n\t\t\t\tsize_t count, loff_t *offp)\n{\n\tstruct pdc_state *pdcs;\n\tchar *buf;\n\tssize_t ret, out_offset, out_count;\n\n\tout_count = 512;\n\n\tbuf = kmalloc(out_count, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tpdcs = filp->private_data;\n\tout_offset = 0;\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"SPU %u stats:\\n\", pdcs->pdc_idx);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"PDC requests....................%u\\n\",\n\t\t\t       pdcs->pdc_requests);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"PDC responses...................%u\\n\",\n\t\t\t       pdcs->pdc_replies);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Tx not done.....................%u\\n\",\n\t\t\t       pdcs->last_tx_not_done);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Tx ring full....................%u\\n\",\n\t\t\t       pdcs->tx_ring_full);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Rx ring full....................%u\\n\",\n\t\t\t       pdcs->rx_ring_full);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Tx desc write fail. Ring full...%u\\n\",\n\t\t\t       pdcs->txnobuf);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Rx desc write fail. Ring full...%u\\n\",\n\t\t\t       pdcs->rxnobuf);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Receive overflow................%u\\n\",\n\t\t\t       pdcs->rx_oflow);\n\tout_offset += scnprintf(buf + out_offset, out_count - out_offset,\n\t\t\t       \"Num frags in rx ring............%u\\n\",\n\t\t\t       NRXDACTIVE(pdcs->rxin, pdcs->last_rx_curr,\n\t\t\t\t\t  pdcs->nrxpost));\n\n\tif (out_offset > out_count)\n\t\tout_offset = out_count;\n\n\tret = simple_read_from_buffer(ubuf, count, offp, buf, out_offset);\n\tkfree(buf);\n\treturn ret;\n}\n\nstatic const struct file_operations pdc_debugfs_stats = {\n\t.owner = THIS_MODULE,\n\t.open = simple_open,\n\t.read = pdc_debugfs_read,\n};\n\n \nstatic void pdc_setup_debugfs(struct pdc_state *pdcs)\n{\n\tchar spu_stats_name[16];\n\n\tif (!debugfs_initialized())\n\t\treturn;\n\n\tsnprintf(spu_stats_name, 16, \"pdc%d_stats\", pdcs->pdc_idx);\n\tif (!debugfs_dir)\n\t\tdebugfs_dir = debugfs_create_dir(KBUILD_MODNAME, NULL);\n\n\t \n\tdebugfs_create_file(spu_stats_name, 0400, debugfs_dir, pdcs,\n\t\t\t    &pdc_debugfs_stats);\n}\n\nstatic void pdc_free_debugfs(void)\n{\n\tdebugfs_remove_recursive(debugfs_dir);\n\tdebugfs_dir = NULL;\n}\n\n \nstatic inline void\npdc_build_rxd(struct pdc_state *pdcs, dma_addr_t dma_addr,\n\t      u32 buf_len, u32 flags)\n{\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct dma64dd *rxd = &pdcs->rxd_64[pdcs->rxout];\n\n\tdev_dbg(dev,\n\t\t\"Writing rx descriptor for PDC %u at index %u with length %u. flags %#x\\n\",\n\t\tpdcs->pdc_idx, pdcs->rxout, buf_len, flags);\n\n\trxd->addrlow = cpu_to_le32(lower_32_bits(dma_addr));\n\trxd->addrhigh = cpu_to_le32(upper_32_bits(dma_addr));\n\trxd->ctrl1 = cpu_to_le32(flags);\n\trxd->ctrl2 = cpu_to_le32(buf_len);\n\n\t \n\tpdcs->rxout = NEXTRXD(pdcs->rxout, pdcs->nrxpost);\n}\n\n \nstatic inline void\npdc_build_txd(struct pdc_state *pdcs, dma_addr_t dma_addr, u32 buf_len,\n\t      u32 flags)\n{\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct dma64dd *txd = &pdcs->txd_64[pdcs->txout];\n\n\tdev_dbg(dev,\n\t\t\"Writing tx descriptor for PDC %u at index %u with length %u, flags %#x\\n\",\n\t\tpdcs->pdc_idx, pdcs->txout, buf_len, flags);\n\n\ttxd->addrlow = cpu_to_le32(lower_32_bits(dma_addr));\n\ttxd->addrhigh = cpu_to_le32(upper_32_bits(dma_addr));\n\ttxd->ctrl1 = cpu_to_le32(flags);\n\ttxd->ctrl2 = cpu_to_le32(buf_len);\n\n\t \n\tpdcs->txout = NEXTTXD(pdcs->txout, pdcs->ntxpost);\n}\n\n \nstatic int\npdc_receive_one(struct pdc_state *pdcs)\n{\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct mbox_controller *mbc;\n\tstruct mbox_chan *chan;\n\tstruct brcm_message mssg;\n\tu32 len, rx_status;\n\tu32 num_frags;\n\tu8 *resp_hdr;     \n\tu32 frags_rdy;    \n\tu32 rx_idx;       \n\tdma_addr_t resp_hdr_daddr;\n\tstruct pdc_rx_ctx *rx_ctx;\n\n\tmbc = &pdcs->mbc;\n\tchan = &mbc->chans[0];\n\tmssg.type = BRCM_MESSAGE_SPU;\n\n\t \n\tfrags_rdy = NRXDACTIVE(pdcs->rxin, pdcs->last_rx_curr, pdcs->nrxpost);\n\tif ((frags_rdy == 0) ||\n\t    (frags_rdy < pdcs->rx_ctx[pdcs->rxin].rxin_numd))\n\t\t \n\t\treturn -EAGAIN;\n\n\tnum_frags = pdcs->txin_numd[pdcs->txin];\n\tWARN_ON(num_frags == 0);\n\n\tdma_unmap_sg(dev, pdcs->src_sg[pdcs->txin],\n\t\t     sg_nents(pdcs->src_sg[pdcs->txin]), DMA_TO_DEVICE);\n\n\tpdcs->txin = (pdcs->txin + num_frags) & pdcs->ntxpost;\n\n\tdev_dbg(dev, \"PDC %u reclaimed %d tx descriptors\",\n\t\tpdcs->pdc_idx, num_frags);\n\n\trx_idx = pdcs->rxin;\n\trx_ctx = &pdcs->rx_ctx[rx_idx];\n\tnum_frags = rx_ctx->rxin_numd;\n\t \n\tmssg.ctx = rx_ctx->rxp_ctx;\n\trx_ctx->rxp_ctx = NULL;\n\tresp_hdr = rx_ctx->resp_hdr;\n\tresp_hdr_daddr = rx_ctx->resp_hdr_daddr;\n\tdma_unmap_sg(dev, rx_ctx->dst_sg, sg_nents(rx_ctx->dst_sg),\n\t\t     DMA_FROM_DEVICE);\n\n\tpdcs->rxin = (pdcs->rxin + num_frags) & pdcs->nrxpost;\n\n\tdev_dbg(dev, \"PDC %u reclaimed %d rx descriptors\",\n\t\tpdcs->pdc_idx, num_frags);\n\n\tdev_dbg(dev,\n\t\t\"PDC %u txin %u, txout %u, rxin %u, rxout %u, last_rx_curr %u\\n\",\n\t\tpdcs->pdc_idx, pdcs->txin, pdcs->txout, pdcs->rxin,\n\t\tpdcs->rxout, pdcs->last_rx_curr);\n\n\tif (pdcs->pdc_resp_hdr_len == PDC_SPUM_RESP_HDR_LEN) {\n\t\t \n\t\trx_status = *((u32 *)resp_hdr);\n\t\tlen = rx_status & RX_STATUS_LEN;\n\t\tdev_dbg(dev,\n\t\t\t\"SPU response length %u bytes\", len);\n\t\tif (unlikely(((rx_status & RX_STATUS_OVERFLOW) || (!len)))) {\n\t\t\tif (rx_status & RX_STATUS_OVERFLOW) {\n\t\t\t\tdev_err_ratelimited(dev,\n\t\t\t\t\t\t    \"crypto receive overflow\");\n\t\t\t\tpdcs->rx_oflow++;\n\t\t\t} else {\n\t\t\t\tdev_info_ratelimited(dev, \"crypto rx len = 0\");\n\t\t\t}\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tdma_pool_free(pdcs->rx_buf_pool, resp_hdr, resp_hdr_daddr);\n\n\tmbox_chan_received_data(chan, &mssg);\n\n\tpdcs->pdc_replies++;\n\treturn PDC_SUCCESS;\n}\n\n \nstatic int\npdc_receive(struct pdc_state *pdcs)\n{\n\tint rx_status;\n\n\t \n\tpdcs->last_rx_curr =\n\t    (ioread32((const void __iomem *)&pdcs->rxregs_64->status0) &\n\t     CRYPTO_D64_RS0_CD_MASK) / RING_ENTRY_SIZE;\n\n\tdo {\n\t\t \n\t\trx_status = pdc_receive_one(pdcs);\n\t} while (rx_status == PDC_SUCCESS);\n\n\treturn 0;\n}\n\n \nstatic int pdc_tx_list_sg_add(struct pdc_state *pdcs, struct scatterlist *sg)\n{\n\tu32 flags = 0;\n\tu32 eot;\n\tu32 tx_avail;\n\n\t \n\tu32 num_desc;\n\tu32 desc_w = 0;\t \n\tu32 bufcnt;\t \n\tdma_addr_t databufptr;\t \n\n\tnum_desc = (u32)sg_nents(sg);\n\n\t \n\ttx_avail = pdcs->ntxpost - NTXDACTIVE(pdcs->txin, pdcs->txout,\n\t\t\t\t\t      pdcs->ntxpost);\n\tif (unlikely(num_desc > tx_avail)) {\n\t\tpdcs->txnobuf++;\n\t\treturn -ENOSPC;\n\t}\n\n\t \n\tif (pdcs->tx_msg_start == pdcs->txout) {\n\t\t \n\t\tpdcs->txin_numd[pdcs->tx_msg_start] = 0;\n\t\tpdcs->src_sg[pdcs->txout] = sg;\n\t\tflags = D64_CTRL1_SOF;\n\t}\n\n\twhile (sg) {\n\t\tif (unlikely(pdcs->txout == (pdcs->ntxd - 1)))\n\t\t\teot = D64_CTRL1_EOT;\n\t\telse\n\t\t\teot = 0;\n\n\t\t \n\t\tbufcnt = sg_dma_len(sg);\n\t\tdatabufptr = sg_dma_address(sg);\n\t\twhile (bufcnt > PDC_DMA_BUF_MAX) {\n\t\t\tpdc_build_txd(pdcs, databufptr, PDC_DMA_BUF_MAX,\n\t\t\t\t      flags | eot);\n\t\t\tdesc_w++;\n\t\t\tbufcnt -= PDC_DMA_BUF_MAX;\n\t\t\tdatabufptr += PDC_DMA_BUF_MAX;\n\t\t\tif (unlikely(pdcs->txout == (pdcs->ntxd - 1)))\n\t\t\t\teot = D64_CTRL1_EOT;\n\t\t\telse\n\t\t\t\teot = 0;\n\t\t}\n\t\tsg = sg_next(sg);\n\t\tif (!sg)\n\t\t\t \n\t\t\tflags |= (D64_CTRL1_EOF | D64_CTRL1_IOC);\n\t\tpdc_build_txd(pdcs, databufptr, bufcnt, flags | eot);\n\t\tdesc_w++;\n\t\t \n\t\tflags &= ~D64_CTRL1_SOF;\n\t}\n\tpdcs->txin_numd[pdcs->tx_msg_start] += desc_w;\n\n\treturn PDC_SUCCESS;\n}\n\n \nstatic int pdc_tx_list_final(struct pdc_state *pdcs)\n{\n\t \n\twmb();\n\tiowrite32(pdcs->rxout << 4, &pdcs->rxregs_64->ptr);\n\tiowrite32(pdcs->txout << 4, &pdcs->txregs_64->ptr);\n\tpdcs->pdc_requests++;\n\n\treturn PDC_SUCCESS;\n}\n\n \nstatic int pdc_rx_list_init(struct pdc_state *pdcs, struct scatterlist *dst_sg,\n\t\t\t    void *ctx)\n{\n\tu32 flags = 0;\n\tu32 rx_avail;\n\tu32 rx_pkt_cnt = 1;\t \n\tdma_addr_t daddr;\n\tvoid *vaddr;\n\tstruct pdc_rx_ctx *rx_ctx;\n\n\trx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\n\t\t\t\t\t      pdcs->nrxpost);\n\tif (unlikely(rx_pkt_cnt > rx_avail)) {\n\t\tpdcs->rxnobuf++;\n\t\treturn -ENOSPC;\n\t}\n\n\t \n\tvaddr = dma_pool_zalloc(pdcs->rx_buf_pool, GFP_ATOMIC, &daddr);\n\tif (unlikely(!vaddr))\n\t\treturn -ENOMEM;\n\n\t \n\tpdcs->rx_msg_start = pdcs->rxout;\n\tpdcs->tx_msg_start = pdcs->txout;\n\n\t \n\tflags = D64_CTRL1_SOF;\n\tpdcs->rx_ctx[pdcs->rx_msg_start].rxin_numd = 1;\n\n\tif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\n\t\tflags |= D64_CTRL1_EOT;\n\n\trx_ctx = &pdcs->rx_ctx[pdcs->rxout];\n\trx_ctx->rxp_ctx = ctx;\n\trx_ctx->dst_sg = dst_sg;\n\trx_ctx->resp_hdr = vaddr;\n\trx_ctx->resp_hdr_daddr = daddr;\n\tpdc_build_rxd(pdcs, daddr, pdcs->pdc_resp_hdr_len, flags);\n\treturn PDC_SUCCESS;\n}\n\n \nstatic int pdc_rx_list_sg_add(struct pdc_state *pdcs, struct scatterlist *sg)\n{\n\tu32 flags = 0;\n\tu32 rx_avail;\n\n\t \n\tu32 num_desc;\n\tu32 desc_w = 0;\t \n\tu32 bufcnt;\t \n\tdma_addr_t databufptr;\t \n\n\tnum_desc = (u32)sg_nents(sg);\n\n\trx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\n\t\t\t\t\t      pdcs->nrxpost);\n\tif (unlikely(num_desc > rx_avail)) {\n\t\tpdcs->rxnobuf++;\n\t\treturn -ENOSPC;\n\t}\n\n\twhile (sg) {\n\t\tif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\n\t\t\tflags = D64_CTRL1_EOT;\n\t\telse\n\t\t\tflags = 0;\n\n\t\t \n\t\tbufcnt = sg_dma_len(sg);\n\t\tdatabufptr = sg_dma_address(sg);\n\t\twhile (bufcnt > PDC_DMA_BUF_MAX) {\n\t\t\tpdc_build_rxd(pdcs, databufptr, PDC_DMA_BUF_MAX, flags);\n\t\t\tdesc_w++;\n\t\t\tbufcnt -= PDC_DMA_BUF_MAX;\n\t\t\tdatabufptr += PDC_DMA_BUF_MAX;\n\t\t\tif (unlikely(pdcs->rxout == (pdcs->nrxd - 1)))\n\t\t\t\tflags = D64_CTRL1_EOT;\n\t\t\telse\n\t\t\t\tflags = 0;\n\t\t}\n\t\tpdc_build_rxd(pdcs, databufptr, bufcnt, flags);\n\t\tdesc_w++;\n\t\tsg = sg_next(sg);\n\t}\n\tpdcs->rx_ctx[pdcs->rx_msg_start].rxin_numd += desc_w;\n\n\treturn PDC_SUCCESS;\n}\n\n \nstatic irqreturn_t pdc_irq_handler(int irq, void *data)\n{\n\tstruct device *dev = (struct device *)data;\n\tstruct pdc_state *pdcs = dev_get_drvdata(dev);\n\tu32 intstatus = ioread32(pdcs->pdc_reg_vbase + PDC_INTSTATUS_OFFSET);\n\n\tif (unlikely(intstatus == 0))\n\t\treturn IRQ_NONE;\n\n\t \n\tiowrite32(0, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\n\n\t \n\tiowrite32(intstatus, pdcs->pdc_reg_vbase + PDC_INTSTATUS_OFFSET);\n\n\t \n\ttasklet_schedule(&pdcs->rx_tasklet);\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void pdc_tasklet_cb(struct tasklet_struct *t)\n{\n\tstruct pdc_state *pdcs = from_tasklet(pdcs, t, rx_tasklet);\n\n\tpdc_receive(pdcs);\n\n\t \n\tiowrite32(PDC_INTMASK, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\n}\n\n \nstatic int pdc_ring_init(struct pdc_state *pdcs, int ringset)\n{\n\tint i;\n\tint err = PDC_SUCCESS;\n\tstruct dma64 *dma_reg;\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct pdc_ring_alloc tx;\n\tstruct pdc_ring_alloc rx;\n\n\t \n\ttx.vbase = dma_pool_zalloc(pdcs->ring_pool, GFP_KERNEL, &tx.dmabase);\n\tif (unlikely(!tx.vbase)) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\t \n\trx.vbase = dma_pool_zalloc(pdcs->ring_pool, GFP_KERNEL, &rx.dmabase);\n\tif (unlikely(!rx.vbase)) {\n\t\terr = -ENOMEM;\n\t\tgoto fail_dealloc;\n\t}\n\n\tdev_dbg(dev, \" - base DMA addr of tx ring      %pad\", &tx.dmabase);\n\tdev_dbg(dev, \" - base virtual addr of tx ring  %p\", tx.vbase);\n\tdev_dbg(dev, \" - base DMA addr of rx ring      %pad\", &rx.dmabase);\n\tdev_dbg(dev, \" - base virtual addr of rx ring  %p\", rx.vbase);\n\n\tmemcpy(&pdcs->tx_ring_alloc, &tx, sizeof(tx));\n\tmemcpy(&pdcs->rx_ring_alloc, &rx, sizeof(rx));\n\n\tpdcs->rxin = 0;\n\tpdcs->rx_msg_start = 0;\n\tpdcs->last_rx_curr = 0;\n\tpdcs->rxout = 0;\n\tpdcs->txin = 0;\n\tpdcs->tx_msg_start = 0;\n\tpdcs->txout = 0;\n\n\t \n\tpdcs->txd_64 = (struct dma64dd *)pdcs->tx_ring_alloc.vbase;\n\tpdcs->rxd_64 = (struct dma64dd *)pdcs->rx_ring_alloc.vbase;\n\n\t \n\tdma_reg = &pdcs->regs->dmaregs[ringset];\n\n\t \n\tiowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\n\tiowrite32((PDC_RX_CTL + (pdcs->rx_status_len << 1)),\n\t\t  &dma_reg->dmarcv.control);\n\tiowrite32(0, &dma_reg->dmaxmt.ptr);\n\tiowrite32(0, &dma_reg->dmarcv.ptr);\n\n\t \n\tiowrite32(lower_32_bits(pdcs->tx_ring_alloc.dmabase),\n\t\t  &dma_reg->dmaxmt.addrlow);\n\tiowrite32(upper_32_bits(pdcs->tx_ring_alloc.dmabase),\n\t\t  &dma_reg->dmaxmt.addrhigh);\n\n\tiowrite32(lower_32_bits(pdcs->rx_ring_alloc.dmabase),\n\t\t  &dma_reg->dmarcv.addrlow);\n\tiowrite32(upper_32_bits(pdcs->rx_ring_alloc.dmabase),\n\t\t  &dma_reg->dmarcv.addrhigh);\n\n\t \n\tiowrite32(PDC_TX_CTL | PDC_TX_ENABLE, &dma_reg->dmaxmt.control);\n\tiowrite32((PDC_RX_CTL | PDC_RX_ENABLE | (pdcs->rx_status_len << 1)),\n\t\t  &dma_reg->dmarcv.control);\n\n\t \n\tfor (i = 0; i < PDC_RING_ENTRIES; i++) {\n\t\t \n\t\tif (i != pdcs->ntxpost) {\n\t\t\tiowrite32(D64_CTRL1_SOF | D64_CTRL1_EOF,\n\t\t\t\t  &pdcs->txd_64[i].ctrl1);\n\t\t} else {\n\t\t\t \n\t\t\tiowrite32(D64_CTRL1_SOF | D64_CTRL1_EOF |\n\t\t\t\t  D64_CTRL1_EOT, &pdcs->txd_64[i].ctrl1);\n\t\t}\n\n\t\t \n\t\tif (i != pdcs->nrxpost) {\n\t\t\tiowrite32(D64_CTRL1_SOF,\n\t\t\t\t  &pdcs->rxd_64[i].ctrl1);\n\t\t} else {\n\t\t\t \n\t\t\tiowrite32(D64_CTRL1_SOF | D64_CTRL1_EOT,\n\t\t\t\t  &pdcs->rxd_64[i].ctrl1);\n\t\t}\n\t}\n\treturn PDC_SUCCESS;\n\nfail_dealloc:\n\tdma_pool_free(pdcs->ring_pool, tx.vbase, tx.dmabase);\ndone:\n\treturn err;\n}\n\nstatic void pdc_ring_free(struct pdc_state *pdcs)\n{\n\tif (pdcs->tx_ring_alloc.vbase) {\n\t\tdma_pool_free(pdcs->ring_pool, pdcs->tx_ring_alloc.vbase,\n\t\t\t      pdcs->tx_ring_alloc.dmabase);\n\t\tpdcs->tx_ring_alloc.vbase = NULL;\n\t}\n\n\tif (pdcs->rx_ring_alloc.vbase) {\n\t\tdma_pool_free(pdcs->ring_pool, pdcs->rx_ring_alloc.vbase,\n\t\t\t      pdcs->rx_ring_alloc.dmabase);\n\t\tpdcs->rx_ring_alloc.vbase = NULL;\n\t}\n}\n\n \nstatic u32 pdc_desc_count(struct scatterlist *sg)\n{\n\tu32 cnt = 0;\n\n\twhile (sg) {\n\t\tcnt += ((sg->length / PDC_DMA_BUF_MAX) + 1);\n\t\tsg = sg_next(sg);\n\t}\n\treturn cnt;\n}\n\n \nstatic bool pdc_rings_full(struct pdc_state *pdcs, int tx_cnt, int rx_cnt)\n{\n\tu32 rx_avail;\n\tu32 tx_avail;\n\tbool full = false;\n\n\t \n\trx_avail = pdcs->nrxpost - NRXDACTIVE(pdcs->rxin, pdcs->rxout,\n\t\t\t\t\t      pdcs->nrxpost);\n\tif (unlikely(rx_cnt > rx_avail)) {\n\t\tpdcs->rx_ring_full++;\n\t\tfull = true;\n\t}\n\n\tif (likely(!full)) {\n\t\ttx_avail = pdcs->ntxpost - NTXDACTIVE(pdcs->txin, pdcs->txout,\n\t\t\t\t\t\t      pdcs->ntxpost);\n\t\tif (unlikely(tx_cnt > tx_avail)) {\n\t\t\tpdcs->tx_ring_full++;\n\t\t\tfull = true;\n\t\t}\n\t}\n\treturn full;\n}\n\n \nstatic bool pdc_last_tx_done(struct mbox_chan *chan)\n{\n\tstruct pdc_state *pdcs = chan->con_priv;\n\tbool ret;\n\n\tif (unlikely(pdc_rings_full(pdcs, PDC_RING_SPACE_MIN,\n\t\t\t\t    PDC_RING_SPACE_MIN))) {\n\t\tpdcs->last_tx_not_done++;\n\t\tret = false;\n\t} else {\n\t\tret = true;\n\t}\n\treturn ret;\n}\n\n \nstatic int pdc_send_data(struct mbox_chan *chan, void *data)\n{\n\tstruct pdc_state *pdcs = chan->con_priv;\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct brcm_message *mssg = data;\n\tint err = PDC_SUCCESS;\n\tint src_nent;\n\tint dst_nent;\n\tint nent;\n\tu32 tx_desc_req;\n\tu32 rx_desc_req;\n\n\tif (unlikely(mssg->type != BRCM_MESSAGE_SPU))\n\t\treturn -ENOTSUPP;\n\n\tsrc_nent = sg_nents(mssg->spu.src);\n\tif (likely(src_nent)) {\n\t\tnent = dma_map_sg(dev, mssg->spu.src, src_nent, DMA_TO_DEVICE);\n\t\tif (unlikely(nent == 0))\n\t\t\treturn -EIO;\n\t}\n\n\tdst_nent = sg_nents(mssg->spu.dst);\n\tif (likely(dst_nent)) {\n\t\tnent = dma_map_sg(dev, mssg->spu.dst, dst_nent,\n\t\t\t\t  DMA_FROM_DEVICE);\n\t\tif (unlikely(nent == 0)) {\n\t\t\tdma_unmap_sg(dev, mssg->spu.src, src_nent,\n\t\t\t\t     DMA_TO_DEVICE);\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t \n\ttx_desc_req = pdc_desc_count(mssg->spu.src);\n\trx_desc_req = pdc_desc_count(mssg->spu.dst);\n\tif (unlikely(pdc_rings_full(pdcs, tx_desc_req, rx_desc_req + 1)))\n\t\treturn -ENOSPC;\n\n\t \n\terr = pdc_rx_list_init(pdcs, mssg->spu.dst, mssg->ctx);\n\terr |= pdc_rx_list_sg_add(pdcs, mssg->spu.dst);\n\n\t \n\terr |= pdc_tx_list_sg_add(pdcs, mssg->spu.src);\n\terr |= pdc_tx_list_final(pdcs);\t \n\n\tif (unlikely(err))\n\t\tdev_err(&pdcs->pdev->dev,\n\t\t\t\"%s failed with error %d\", __func__, err);\n\n\treturn err;\n}\n\nstatic int pdc_startup(struct mbox_chan *chan)\n{\n\treturn pdc_ring_init(chan->con_priv, PDC_RINGSET);\n}\n\nstatic void pdc_shutdown(struct mbox_chan *chan)\n{\n\tstruct pdc_state *pdcs = chan->con_priv;\n\n\tif (!pdcs)\n\t\treturn;\n\n\tdev_dbg(&pdcs->pdev->dev,\n\t\t\"Shutdown mailbox channel for PDC %u\", pdcs->pdc_idx);\n\tpdc_ring_free(pdcs);\n}\n\n \nstatic\nvoid pdc_hw_init(struct pdc_state *pdcs)\n{\n\tstruct platform_device *pdev;\n\tstruct device *dev;\n\tstruct dma64 *dma_reg;\n\tint ringset = PDC_RINGSET;\n\n\tpdev = pdcs->pdev;\n\tdev = &pdev->dev;\n\n\tdev_dbg(dev, \"PDC %u initial values:\", pdcs->pdc_idx);\n\tdev_dbg(dev, \"state structure:                   %p\",\n\t\tpdcs);\n\tdev_dbg(dev, \" - base virtual addr of hw regs    %p\",\n\t\tpdcs->pdc_reg_vbase);\n\n\t \n\tpdcs->regs = (struct pdc_regs *)pdcs->pdc_reg_vbase;\n\tpdcs->txregs_64 = (struct dma64_regs *)\n\t    (((u8 *)pdcs->pdc_reg_vbase) +\n\t\t     PDC_TXREGS_OFFSET + (sizeof(struct dma64) * ringset));\n\tpdcs->rxregs_64 = (struct dma64_regs *)\n\t    (((u8 *)pdcs->pdc_reg_vbase) +\n\t\t     PDC_RXREGS_OFFSET + (sizeof(struct dma64) * ringset));\n\n\tpdcs->ntxd = PDC_RING_ENTRIES;\n\tpdcs->nrxd = PDC_RING_ENTRIES;\n\tpdcs->ntxpost = PDC_RING_ENTRIES - 1;\n\tpdcs->nrxpost = PDC_RING_ENTRIES - 1;\n\tiowrite32(0, &pdcs->regs->intmask);\n\n\tdma_reg = &pdcs->regs->dmaregs[ringset];\n\n\t \n\tiowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\n\n\tiowrite32(PDC_RX_CTL + (pdcs->rx_status_len << 1),\n\t\t  &dma_reg->dmarcv.control);\n\n\t \n\tiowrite32(0, &dma_reg->dmaxmt.ptr);\n\tiowrite32(0, &dma_reg->dmarcv.ptr);\n\n\tif (pdcs->pdc_resp_hdr_len == PDC_SPU2_RESP_HDR_LEN)\n\t\tiowrite32(PDC_CKSUM_CTRL,\n\t\t\t  pdcs->pdc_reg_vbase + PDC_CKSUM_CTRL_OFFSET);\n}\n\n \nstatic void pdc_hw_disable(struct pdc_state *pdcs)\n{\n\tstruct dma64 *dma_reg;\n\n\tdma_reg = &pdcs->regs->dmaregs[PDC_RINGSET];\n\tiowrite32(PDC_TX_CTL, &dma_reg->dmaxmt.control);\n\tiowrite32(PDC_RX_CTL + (pdcs->rx_status_len << 1),\n\t\t  &dma_reg->dmarcv.control);\n}\n\n \nstatic int pdc_rx_buf_pool_create(struct pdc_state *pdcs)\n{\n\tstruct platform_device *pdev;\n\tstruct device *dev;\n\n\tpdev = pdcs->pdev;\n\tdev = &pdev->dev;\n\n\tpdcs->pdc_resp_hdr_len = pdcs->rx_status_len;\n\tif (pdcs->use_bcm_hdr)\n\t\tpdcs->pdc_resp_hdr_len += BCM_HDR_LEN;\n\n\tpdcs->rx_buf_pool = dma_pool_create(\"pdc rx bufs\", dev,\n\t\t\t\t\t    pdcs->pdc_resp_hdr_len,\n\t\t\t\t\t    RX_BUF_ALIGN, 0);\n\tif (!pdcs->rx_buf_pool)\n\t\treturn -ENOMEM;\n\n\treturn PDC_SUCCESS;\n}\n\n \nstatic int pdc_interrupts_init(struct pdc_state *pdcs)\n{\n\tstruct platform_device *pdev = pdcs->pdev;\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *dn = pdev->dev.of_node;\n\tint err;\n\n\t \n\tiowrite32(PDC_INTMASK, pdcs->pdc_reg_vbase + PDC_INTMASK_OFFSET);\n\n\tif (pdcs->hw_type == FA_HW)\n\t\tiowrite32(PDC_LAZY_INT, pdcs->pdc_reg_vbase +\n\t\t\t  FA_RCVLAZY0_OFFSET);\n\telse\n\t\tiowrite32(PDC_LAZY_INT, pdcs->pdc_reg_vbase +\n\t\t\t  PDC_RCVLAZY0_OFFSET);\n\n\t \n\tpdcs->pdc_irq = irq_of_parse_and_map(dn, 0);\n\tdev_dbg(dev, \"pdc device %s irq %u for pdcs %p\",\n\t\tdev_name(dev), pdcs->pdc_irq, pdcs);\n\n\terr = devm_request_irq(dev, pdcs->pdc_irq, pdc_irq_handler, 0,\n\t\t\t       dev_name(dev), dev);\n\tif (err) {\n\t\tdev_err(dev, \"IRQ %u request failed with err %d\\n\",\n\t\t\tpdcs->pdc_irq, err);\n\t\treturn err;\n\t}\n\treturn PDC_SUCCESS;\n}\n\nstatic const struct mbox_chan_ops pdc_mbox_chan_ops = {\n\t.send_data = pdc_send_data,\n\t.last_tx_done = pdc_last_tx_done,\n\t.startup = pdc_startup,\n\t.shutdown = pdc_shutdown\n};\n\n \nstatic int pdc_mb_init(struct pdc_state *pdcs)\n{\n\tstruct device *dev = &pdcs->pdev->dev;\n\tstruct mbox_controller *mbc;\n\tint chan_index;\n\tint err;\n\n\tmbc = &pdcs->mbc;\n\tmbc->dev = dev;\n\tmbc->ops = &pdc_mbox_chan_ops;\n\tmbc->num_chans = 1;\n\tmbc->chans = devm_kcalloc(dev, mbc->num_chans, sizeof(*mbc->chans),\n\t\t\t\t  GFP_KERNEL);\n\tif (!mbc->chans)\n\t\treturn -ENOMEM;\n\n\tmbc->txdone_irq = false;\n\tmbc->txdone_poll = true;\n\tmbc->txpoll_period = 1;\n\tfor (chan_index = 0; chan_index < mbc->num_chans; chan_index++)\n\t\tmbc->chans[chan_index].con_priv = pdcs;\n\n\t \n\terr = devm_mbox_controller_register(dev, mbc);\n\tif (err) {\n\t\tdev_crit(dev,\n\t\t\t \"Failed to register PDC mailbox controller. Error %d.\",\n\t\t\t err);\n\t\treturn err;\n\t}\n\treturn 0;\n}\n\n \nstatic const int pdc_hw = PDC_HW;\nstatic const int fa_hw = FA_HW;\n\nstatic const struct of_device_id pdc_mbox_of_match[] = {\n\t{.compatible = \"brcm,iproc-pdc-mbox\", .data = &pdc_hw},\n\t{.compatible = \"brcm,iproc-fa2-mbox\", .data = &fa_hw},\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, pdc_mbox_of_match);\n\n \nstatic int pdc_dt_read(struct platform_device *pdev, struct pdc_state *pdcs)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *dn = pdev->dev.of_node;\n\tconst struct of_device_id *match;\n\tconst int *hw_type;\n\tint err;\n\n\terr = of_property_read_u32(dn, \"brcm,rx-status-len\",\n\t\t\t\t   &pdcs->rx_status_len);\n\tif (err < 0)\n\t\tdev_err(dev,\n\t\t\t\"%s failed to get DMA receive status length from device tree\",\n\t\t\t__func__);\n\n\tpdcs->use_bcm_hdr = of_property_read_bool(dn, \"brcm,use-bcm-hdr\");\n\n\tpdcs->hw_type = PDC_HW;\n\n\tmatch = of_match_device(of_match_ptr(pdc_mbox_of_match), dev);\n\tif (match != NULL) {\n\t\thw_type = match->data;\n\t\tpdcs->hw_type = *hw_type;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int pdc_probe(struct platform_device *pdev)\n{\n\tint err = 0;\n\tstruct device *dev = &pdev->dev;\n\tstruct resource *pdc_regs;\n\tstruct pdc_state *pdcs;\n\n\t \n\tpdcs = devm_kzalloc(dev, sizeof(*pdcs), GFP_KERNEL);\n\tif (!pdcs) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\n\tpdcs->pdev = pdev;\n\tplatform_set_drvdata(pdev, pdcs);\n\tpdcs->pdc_idx = pdcg.num_spu;\n\tpdcg.num_spu++;\n\n\terr = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(39));\n\tif (err) {\n\t\tdev_warn(dev, \"PDC device cannot perform DMA. Error %d.\", err);\n\t\tgoto cleanup;\n\t}\n\n\t \n\tpdcs->ring_pool = dma_pool_create(\"pdc rings\", dev, PDC_RING_SIZE,\n\t\t\t\t\t  RING_ALIGN, 0);\n\tif (!pdcs->ring_pool) {\n\t\terr = -ENOMEM;\n\t\tgoto cleanup;\n\t}\n\n\terr = pdc_dt_read(pdev, pdcs);\n\tif (err)\n\t\tgoto cleanup_ring_pool;\n\n\tpdcs->pdc_reg_vbase = devm_platform_get_and_ioremap_resource(pdev, 0, &pdc_regs);\n\tif (IS_ERR(pdcs->pdc_reg_vbase)) {\n\t\terr = PTR_ERR(pdcs->pdc_reg_vbase);\n\t\tgoto cleanup_ring_pool;\n\t}\n\tdev_dbg(dev, \"PDC register region res.start = %pa, res.end = %pa\",\n\t\t&pdc_regs->start, &pdc_regs->end);\n\n\t \n\terr = pdc_rx_buf_pool_create(pdcs);\n\tif (err)\n\t\tgoto cleanup_ring_pool;\n\n\tpdc_hw_init(pdcs);\n\n\t \n\ttasklet_setup(&pdcs->rx_tasklet, pdc_tasklet_cb);\n\n\terr = pdc_interrupts_init(pdcs);\n\tif (err)\n\t\tgoto cleanup_buf_pool;\n\n\t \n\terr = pdc_mb_init(pdcs);\n\tif (err)\n\t\tgoto cleanup_buf_pool;\n\n\tpdc_setup_debugfs(pdcs);\n\n\tdev_dbg(dev, \"pdc_probe() successful\");\n\treturn PDC_SUCCESS;\n\ncleanup_buf_pool:\n\ttasklet_kill(&pdcs->rx_tasklet);\n\tdma_pool_destroy(pdcs->rx_buf_pool);\n\ncleanup_ring_pool:\n\tdma_pool_destroy(pdcs->ring_pool);\n\ncleanup:\n\treturn err;\n}\n\nstatic int pdc_remove(struct platform_device *pdev)\n{\n\tstruct pdc_state *pdcs = platform_get_drvdata(pdev);\n\n\tpdc_free_debugfs();\n\n\ttasklet_kill(&pdcs->rx_tasklet);\n\n\tpdc_hw_disable(pdcs);\n\n\tdma_pool_destroy(pdcs->rx_buf_pool);\n\tdma_pool_destroy(pdcs->ring_pool);\n\treturn 0;\n}\n\nstatic struct platform_driver pdc_mbox_driver = {\n\t.probe = pdc_probe,\n\t.remove = pdc_remove,\n\t.driver = {\n\t\t   .name = \"brcm-iproc-pdc-mbox\",\n\t\t   .of_match_table = pdc_mbox_of_match,\n\t\t   },\n};\nmodule_platform_driver(pdc_mbox_driver);\n\nMODULE_AUTHOR(\"Rob Rice <rob.rice@broadcom.com>\");\nMODULE_DESCRIPTION(\"Broadcom PDC mailbox driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}