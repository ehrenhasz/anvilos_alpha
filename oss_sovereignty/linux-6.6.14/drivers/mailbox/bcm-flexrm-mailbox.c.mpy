{
  "module_name": "bcm-flexrm-mailbox.c",
  "hash_id": "9141e11edaf809d9ebc2a8bee9f6fff1fb85f8807e302093f29279384fa0330f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mailbox/bcm-flexrm-mailbox.c",
  "human_readable_source": "\n\n\n \n\n#include <asm/barrier.h>\n#include <asm/byteorder.h>\n#include <linux/atomic.h>\n#include <linux/bitmap.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/dmapool.h>\n#include <linux/err.h>\n#include <linux/interrupt.h>\n#include <linux/kernel.h>\n#include <linux/mailbox_controller.h>\n#include <linux/mailbox_client.h>\n#include <linux/mailbox/brcm-message.h>\n#include <linux/module.h>\n#include <linux/msi.h>\n#include <linux/of_address.h>\n#include <linux/of_irq.h>\n#include <linux/platform_device.h>\n#include <linux/spinlock.h>\n\n \n\n \n#define RING_REGS_SIZE\t\t\t\t\t0x10000\n#define RING_DESC_SIZE\t\t\t\t\t8\n#define RING_DESC_INDEX(offset)\t\t\t\t\\\n\t\t\t((offset) / RING_DESC_SIZE)\n#define RING_DESC_OFFSET(index)\t\t\t\t\\\n\t\t\t((index) * RING_DESC_SIZE)\n#define RING_MAX_REQ_COUNT\t\t\t\t1024\n#define RING_BD_ALIGN_ORDER\t\t\t\t12\n#define RING_BD_ALIGN_CHECK(addr)\t\t\t\\\n\t\t\t(!((addr) & ((0x1 << RING_BD_ALIGN_ORDER) - 1)))\n#define RING_BD_TOGGLE_INVALID(offset)\t\t\t\\\n\t\t\t(((offset) >> RING_BD_ALIGN_ORDER) & 0x1)\n#define RING_BD_TOGGLE_VALID(offset)\t\t\t\\\n\t\t\t(!RING_BD_TOGGLE_INVALID(offset))\n#define RING_BD_DESC_PER_REQ\t\t\t\t32\n#define RING_BD_DESC_COUNT\t\t\t\t\\\n\t\t\t(RING_MAX_REQ_COUNT * RING_BD_DESC_PER_REQ)\n#define RING_BD_SIZE\t\t\t\t\t\\\n\t\t\t(RING_BD_DESC_COUNT * RING_DESC_SIZE)\n#define RING_CMPL_ALIGN_ORDER\t\t\t\t13\n#define RING_CMPL_DESC_COUNT\t\t\t\tRING_MAX_REQ_COUNT\n#define RING_CMPL_SIZE\t\t\t\t\t\\\n\t\t\t(RING_CMPL_DESC_COUNT * RING_DESC_SIZE)\n#define RING_VER_MAGIC\t\t\t\t\t0x76303031\n\n \n#define RING_VER\t\t\t\t\t0x000\n#define RING_BD_START_ADDR\t\t\t\t0x004\n#define RING_BD_READ_PTR\t\t\t\t0x008\n#define RING_BD_WRITE_PTR\t\t\t\t0x00c\n#define RING_BD_READ_PTR_DDR_LS\t\t\t\t0x010\n#define RING_BD_READ_PTR_DDR_MS\t\t\t\t0x014\n#define RING_CMPL_START_ADDR\t\t\t\t0x018\n#define RING_CMPL_WRITE_PTR\t\t\t\t0x01c\n#define RING_NUM_REQ_RECV_LS\t\t\t\t0x020\n#define RING_NUM_REQ_RECV_MS\t\t\t\t0x024\n#define RING_NUM_REQ_TRANS_LS\t\t\t\t0x028\n#define RING_NUM_REQ_TRANS_MS\t\t\t\t0x02c\n#define RING_NUM_REQ_OUTSTAND\t\t\t\t0x030\n#define RING_CONTROL\t\t\t\t\t0x034\n#define RING_FLUSH_DONE\t\t\t\t\t0x038\n#define RING_MSI_ADDR_LS\t\t\t\t0x03c\n#define RING_MSI_ADDR_MS\t\t\t\t0x040\n#define RING_MSI_CONTROL\t\t\t\t0x048\n#define RING_BD_READ_PTR_DDR_CONTROL\t\t\t0x04c\n#define RING_MSI_DATA_VALUE\t\t\t\t0x064\n\n \n#define BD_LAST_UPDATE_HW_SHIFT\t\t\t\t28\n#define BD_LAST_UPDATE_HW_MASK\t\t\t\t0x1\n#define BD_START_ADDR_VALUE(pa)\t\t\t\t\\\n\t((u32)((((dma_addr_t)(pa)) >> RING_BD_ALIGN_ORDER) & 0x0fffffff))\n#define BD_START_ADDR_DECODE(val)\t\t\t\\\n\t((dma_addr_t)((val) & 0x0fffffff) << RING_BD_ALIGN_ORDER)\n\n \n#define CMPL_START_ADDR_VALUE(pa)\t\t\t\\\n\t((u32)((((u64)(pa)) >> RING_CMPL_ALIGN_ORDER) & 0x07ffffff))\n\n \n#define CONTROL_MASK_DISABLE_CONTROL\t\t\t12\n#define CONTROL_FLUSH_SHIFT\t\t\t\t5\n#define CONTROL_ACTIVE_SHIFT\t\t\t\t4\n#define CONTROL_RATE_ADAPT_MASK\t\t\t\t0xf\n#define CONTROL_RATE_DYNAMIC\t\t\t\t0x0\n#define CONTROL_RATE_FAST\t\t\t\t0x8\n#define CONTROL_RATE_MEDIUM\t\t\t\t0x9\n#define CONTROL_RATE_SLOW\t\t\t\t0xa\n#define CONTROL_RATE_IDLE\t\t\t\t0xb\n\n \n#define FLUSH_DONE_MASK\t\t\t\t\t0x1\n\n \n#define MSI_TIMER_VAL_SHIFT\t\t\t\t16\n#define MSI_TIMER_VAL_MASK\t\t\t\t0xffff\n#define MSI_ENABLE_SHIFT\t\t\t\t15\n#define MSI_ENABLE_MASK\t\t\t\t\t0x1\n#define MSI_COUNT_SHIFT\t\t\t\t\t0\n#define MSI_COUNT_MASK\t\t\t\t\t0x3ff\n\n \n#define BD_READ_PTR_DDR_TIMER_VAL_SHIFT\t\t\t16\n#define BD_READ_PTR_DDR_TIMER_VAL_MASK\t\t\t0xffff\n#define BD_READ_PTR_DDR_ENABLE_SHIFT\t\t\t15\n#define BD_READ_PTR_DDR_ENABLE_MASK\t\t\t0x1\n\n \n\n \n#define CMPL_OPAQUE_SHIFT\t\t\t0\n#define CMPL_OPAQUE_MASK\t\t\t0xffff\n#define CMPL_ENGINE_STATUS_SHIFT\t\t16\n#define CMPL_ENGINE_STATUS_MASK\t\t\t0xffff\n#define CMPL_DME_STATUS_SHIFT\t\t\t32\n#define CMPL_DME_STATUS_MASK\t\t\t0xffff\n#define CMPL_RM_STATUS_SHIFT\t\t\t48\n#define CMPL_RM_STATUS_MASK\t\t\t0xffff\n\n \n#define DME_STATUS_MEM_COR_ERR\t\t\tBIT(0)\n#define DME_STATUS_MEM_UCOR_ERR\t\t\tBIT(1)\n#define DME_STATUS_FIFO_UNDERFLOW\t\tBIT(2)\n#define DME_STATUS_FIFO_OVERFLOW\t\tBIT(3)\n#define DME_STATUS_RRESP_ERR\t\t\tBIT(4)\n#define DME_STATUS_BRESP_ERR\t\t\tBIT(5)\n#define DME_STATUS_ERROR_MASK\t\t\t(DME_STATUS_MEM_COR_ERR | \\\n\t\t\t\t\t\t DME_STATUS_MEM_UCOR_ERR | \\\n\t\t\t\t\t\t DME_STATUS_FIFO_UNDERFLOW | \\\n\t\t\t\t\t\t DME_STATUS_FIFO_OVERFLOW | \\\n\t\t\t\t\t\t DME_STATUS_RRESP_ERR | \\\n\t\t\t\t\t\t DME_STATUS_BRESP_ERR)\n\n \n#define RM_STATUS_CODE_SHIFT\t\t\t0\n#define RM_STATUS_CODE_MASK\t\t\t0x3ff\n#define RM_STATUS_CODE_GOOD\t\t\t0x0\n#define RM_STATUS_CODE_AE_TIMEOUT\t\t0x3ff\n\n \n#define DESC_TYPE_SHIFT\t\t\t\t60\n#define DESC_TYPE_MASK\t\t\t\t0xf\n#define DESC_PAYLOAD_SHIFT\t\t\t0\n#define DESC_PAYLOAD_MASK\t\t\t0x0fffffffffffffff\n\n \n#define NULL_TYPE\t\t\t\t0\n#define NULL_TOGGLE_SHIFT\t\t\t58\n#define NULL_TOGGLE_MASK\t\t\t0x1\n\n \n#define HEADER_TYPE\t\t\t\t1\n#define HEADER_TOGGLE_SHIFT\t\t\t58\n#define HEADER_TOGGLE_MASK\t\t\t0x1\n#define HEADER_ENDPKT_SHIFT\t\t\t57\n#define HEADER_ENDPKT_MASK\t\t\t0x1\n#define HEADER_STARTPKT_SHIFT\t\t\t56\n#define HEADER_STARTPKT_MASK\t\t\t0x1\n#define HEADER_BDCOUNT_SHIFT\t\t\t36\n#define HEADER_BDCOUNT_MASK\t\t\t0x1f\n#define HEADER_BDCOUNT_MAX\t\t\tHEADER_BDCOUNT_MASK\n#define HEADER_FLAGS_SHIFT\t\t\t16\n#define HEADER_FLAGS_MASK\t\t\t0xffff\n#define HEADER_OPAQUE_SHIFT\t\t\t0\n#define HEADER_OPAQUE_MASK\t\t\t0xffff\n\n \n#define SRC_TYPE\t\t\t\t2\n#define SRC_LENGTH_SHIFT\t\t\t44\n#define SRC_LENGTH_MASK\t\t\t\t0xffff\n#define SRC_ADDR_SHIFT\t\t\t\t0\n#define SRC_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define DST_TYPE\t\t\t\t3\n#define DST_LENGTH_SHIFT\t\t\t44\n#define DST_LENGTH_MASK\t\t\t\t0xffff\n#define DST_ADDR_SHIFT\t\t\t\t0\n#define DST_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define IMM_TYPE\t\t\t\t4\n#define IMM_DATA_SHIFT\t\t\t\t0\n#define IMM_DATA_MASK\t\t\t\t0x0fffffffffffffff\n\n \n#define NPTR_TYPE\t\t\t\t5\n#define NPTR_TOGGLE_SHIFT\t\t\t58\n#define NPTR_TOGGLE_MASK\t\t\t0x1\n#define NPTR_ADDR_SHIFT\t\t\t\t0\n#define NPTR_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define MSRC_TYPE\t\t\t\t6\n#define MSRC_LENGTH_SHIFT\t\t\t44\n#define MSRC_LENGTH_MASK\t\t\t0xffff\n#define MSRC_ADDR_SHIFT\t\t\t\t0\n#define MSRC_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define MDST_TYPE\t\t\t\t7\n#define MDST_LENGTH_SHIFT\t\t\t44\n#define MDST_LENGTH_MASK\t\t\t0xffff\n#define MDST_ADDR_SHIFT\t\t\t\t0\n#define MDST_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define SRCT_TYPE\t\t\t\t8\n#define SRCT_LENGTH_SHIFT\t\t\t44\n#define SRCT_LENGTH_MASK\t\t\t0xffff\n#define SRCT_ADDR_SHIFT\t\t\t\t0\n#define SRCT_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define DSTT_TYPE\t\t\t\t9\n#define DSTT_LENGTH_SHIFT\t\t\t44\n#define DSTT_LENGTH_MASK\t\t\t0xffff\n#define DSTT_ADDR_SHIFT\t\t\t\t0\n#define DSTT_ADDR_MASK\t\t\t\t0x00000fffffffffff\n\n \n#define IMMT_TYPE\t\t\t\t10\n#define IMMT_DATA_SHIFT\t\t\t\t0\n#define IMMT_DATA_MASK\t\t\t\t0x0fffffffffffffff\n\n \n#define DESC_DEC(_d, _s, _m)\t\t\t(((_d) >> (_s)) & (_m))\n#define DESC_ENC(_d, _v, _s, _m)\t\t\\\n\t\t\tdo { \\\n\t\t\t\t(_d) &= ~((u64)(_m) << (_s)); \\\n\t\t\t\t(_d) |= (((u64)(_v) & (_m)) << (_s)); \\\n\t\t\t} while (0)\n\n \n\nstruct flexrm_ring {\n\t \n\tint num;\n\tstruct flexrm_mbox *mbox;\n\tvoid __iomem *regs;\n\tbool irq_requested;\n\tunsigned int irq;\n\tcpumask_t irq_aff_hint;\n\tunsigned int msi_timer_val;\n\tunsigned int msi_count_threshold;\n\tstruct brcm_message *requests[RING_MAX_REQ_COUNT];\n\tvoid *bd_base;\n\tdma_addr_t bd_dma_base;\n\tu32 bd_write_offset;\n\tvoid *cmpl_base;\n\tdma_addr_t cmpl_dma_base;\n\t \n\tatomic_t msg_send_count;\n\tatomic_t msg_cmpl_count;\n\t \n\tspinlock_t lock;\n\tDECLARE_BITMAP(requests_bmap, RING_MAX_REQ_COUNT);\n\tu32 cmpl_read_offset;\n};\n\nstruct flexrm_mbox {\n\tstruct device *dev;\n\tvoid __iomem *regs;\n\tu32 num_rings;\n\tstruct flexrm_ring *rings;\n\tstruct dma_pool *bd_pool;\n\tstruct dma_pool *cmpl_pool;\n\tstruct dentry *root;\n\tstruct mbox_controller controller;\n};\n\n \n\nstatic u64 flexrm_read_desc(void *desc_ptr)\n{\n\treturn le64_to_cpu(*((u64 *)desc_ptr));\n}\n\nstatic void flexrm_write_desc(void *desc_ptr, u64 desc)\n{\n\t*((u64 *)desc_ptr) = cpu_to_le64(desc);\n}\n\nstatic u32 flexrm_cmpl_desc_to_reqid(u64 cmpl_desc)\n{\n\treturn (u32)(cmpl_desc & CMPL_OPAQUE_MASK);\n}\n\nstatic int flexrm_cmpl_desc_to_error(u64 cmpl_desc)\n{\n\tu32 status;\n\n\tstatus = DESC_DEC(cmpl_desc, CMPL_DME_STATUS_SHIFT,\n\t\t\t  CMPL_DME_STATUS_MASK);\n\tif (status & DME_STATUS_ERROR_MASK)\n\t\treturn -EIO;\n\n\tstatus = DESC_DEC(cmpl_desc, CMPL_RM_STATUS_SHIFT,\n\t\t\t  CMPL_RM_STATUS_MASK);\n\tstatus &= RM_STATUS_CODE_MASK;\n\tif (status == RM_STATUS_CODE_AE_TIMEOUT)\n\t\treturn -ETIMEDOUT;\n\n\treturn 0;\n}\n\nstatic bool flexrm_is_next_table_desc(void *desc_ptr)\n{\n\tu64 desc = flexrm_read_desc(desc_ptr);\n\tu32 type = DESC_DEC(desc, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\n\treturn (type == NPTR_TYPE) ? true : false;\n}\n\nstatic u64 flexrm_next_table_desc(u32 toggle, dma_addr_t next_addr)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, NPTR_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, toggle, NPTR_TOGGLE_SHIFT, NPTR_TOGGLE_MASK);\n\tDESC_ENC(desc, next_addr, NPTR_ADDR_SHIFT, NPTR_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_null_desc(u32 toggle)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, NULL_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, toggle, NULL_TOGGLE_SHIFT, NULL_TOGGLE_MASK);\n\n\treturn desc;\n}\n\nstatic u32 flexrm_estimate_header_desc_count(u32 nhcnt)\n{\n\tu32 hcnt = nhcnt / HEADER_BDCOUNT_MAX;\n\n\tif (!(nhcnt % HEADER_BDCOUNT_MAX))\n\t\thcnt += 1;\n\n\treturn hcnt;\n}\n\nstatic void flexrm_flip_header_toggle(void *desc_ptr)\n{\n\tu64 desc = flexrm_read_desc(desc_ptr);\n\n\tif (desc & ((u64)0x1 << HEADER_TOGGLE_SHIFT))\n\t\tdesc &= ~((u64)0x1 << HEADER_TOGGLE_SHIFT);\n\telse\n\t\tdesc |= ((u64)0x1 << HEADER_TOGGLE_SHIFT);\n\n\tflexrm_write_desc(desc_ptr, desc);\n}\n\nstatic u64 flexrm_header_desc(u32 toggle, u32 startpkt, u32 endpkt,\n\t\t\t       u32 bdcount, u32 flags, u32 opaque)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, HEADER_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, toggle, HEADER_TOGGLE_SHIFT, HEADER_TOGGLE_MASK);\n\tDESC_ENC(desc, startpkt, HEADER_STARTPKT_SHIFT, HEADER_STARTPKT_MASK);\n\tDESC_ENC(desc, endpkt, HEADER_ENDPKT_SHIFT, HEADER_ENDPKT_MASK);\n\tDESC_ENC(desc, bdcount, HEADER_BDCOUNT_SHIFT, HEADER_BDCOUNT_MASK);\n\tDESC_ENC(desc, flags, HEADER_FLAGS_SHIFT, HEADER_FLAGS_MASK);\n\tDESC_ENC(desc, opaque, HEADER_OPAQUE_SHIFT, HEADER_OPAQUE_MASK);\n\n\treturn desc;\n}\n\nstatic void flexrm_enqueue_desc(u32 nhpos, u32 nhcnt, u32 reqid,\n\t\t\t\t u64 desc, void **desc_ptr, u32 *toggle,\n\t\t\t\t void *start_desc, void *end_desc)\n{\n\tu64 d;\n\tu32 nhavail, _toggle, _startpkt, _endpkt, _bdcount;\n\n\t \n\tif (nhcnt <= nhpos)\n\t\treturn;\n\n\t \n\n\tif ((nhpos % HEADER_BDCOUNT_MAX == 0) && (nhcnt - nhpos)) {\n\t\t \n\t\tnhavail = (nhcnt - nhpos);\n\t\t_toggle = (nhpos == 0) ? !(*toggle) : (*toggle);\n\t\t_startpkt = (nhpos == 0) ? 0x1 : 0x0;\n\t\t_endpkt = (nhavail <= HEADER_BDCOUNT_MAX) ? 0x1 : 0x0;\n\t\t_bdcount = (nhavail <= HEADER_BDCOUNT_MAX) ?\n\t\t\t\tnhavail : HEADER_BDCOUNT_MAX;\n\t\tif (nhavail <= HEADER_BDCOUNT_MAX)\n\t\t\t_bdcount = nhavail;\n\t\telse\n\t\t\t_bdcount = HEADER_BDCOUNT_MAX;\n\t\td = flexrm_header_desc(_toggle, _startpkt, _endpkt,\n\t\t\t\t\t_bdcount, 0x0, reqid);\n\n\t\t \n\t\tflexrm_write_desc(*desc_ptr, d);\n\n\t\t \n\t\t*desc_ptr += sizeof(desc);\n\t\tif (*desc_ptr == end_desc)\n\t\t\t*desc_ptr = start_desc;\n\n\t\t \n\t\twhile (flexrm_is_next_table_desc(*desc_ptr)) {\n\t\t\t*toggle = (*toggle) ? 0 : 1;\n\t\t\t*desc_ptr += sizeof(desc);\n\t\t\tif (*desc_ptr == end_desc)\n\t\t\t\t*desc_ptr = start_desc;\n\t\t}\n\t}\n\n\t \n\tflexrm_write_desc(*desc_ptr, desc);\n\n\t \n\t*desc_ptr += sizeof(desc);\n\tif (*desc_ptr == end_desc)\n\t\t*desc_ptr = start_desc;\n\n\t \n\twhile (flexrm_is_next_table_desc(*desc_ptr)) {\n\t\t*toggle = (*toggle) ? 0 : 1;\n\t\t*desc_ptr += sizeof(desc);\n\t\tif (*desc_ptr == end_desc)\n\t\t\t*desc_ptr = start_desc;\n\t}\n}\n\nstatic u64 flexrm_src_desc(dma_addr_t addr, unsigned int length)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, SRC_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length, SRC_LENGTH_SHIFT, SRC_LENGTH_MASK);\n\tDESC_ENC(desc, addr, SRC_ADDR_SHIFT, SRC_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_msrc_desc(dma_addr_t addr, unsigned int length_div_16)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, MSRC_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length_div_16, MSRC_LENGTH_SHIFT, MSRC_LENGTH_MASK);\n\tDESC_ENC(desc, addr, MSRC_ADDR_SHIFT, MSRC_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_dst_desc(dma_addr_t addr, unsigned int length)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, DST_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length, DST_LENGTH_SHIFT, DST_LENGTH_MASK);\n\tDESC_ENC(desc, addr, DST_ADDR_SHIFT, DST_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_mdst_desc(dma_addr_t addr, unsigned int length_div_16)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, MDST_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length_div_16, MDST_LENGTH_SHIFT, MDST_LENGTH_MASK);\n\tDESC_ENC(desc, addr, MDST_ADDR_SHIFT, MDST_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_imm_desc(u64 data)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, IMM_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, data, IMM_DATA_SHIFT, IMM_DATA_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_srct_desc(dma_addr_t addr, unsigned int length)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, SRCT_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length, SRCT_LENGTH_SHIFT, SRCT_LENGTH_MASK);\n\tDESC_ENC(desc, addr, SRCT_ADDR_SHIFT, SRCT_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_dstt_desc(dma_addr_t addr, unsigned int length)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, DSTT_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, length, DSTT_LENGTH_SHIFT, DSTT_LENGTH_MASK);\n\tDESC_ENC(desc, addr, DSTT_ADDR_SHIFT, DSTT_ADDR_MASK);\n\n\treturn desc;\n}\n\nstatic u64 flexrm_immt_desc(u64 data)\n{\n\tu64 desc = 0;\n\n\tDESC_ENC(desc, IMMT_TYPE, DESC_TYPE_SHIFT, DESC_TYPE_MASK);\n\tDESC_ENC(desc, data, IMMT_DATA_SHIFT, IMMT_DATA_MASK);\n\n\treturn desc;\n}\n\nstatic bool flexrm_spu_sanity_check(struct brcm_message *msg)\n{\n\tstruct scatterlist *sg;\n\n\tif (!msg->spu.src || !msg->spu.dst)\n\t\treturn false;\n\tfor (sg = msg->spu.src; sg; sg = sg_next(sg)) {\n\t\tif (sg->length & 0xf) {\n\t\t\tif (sg->length > SRC_LENGTH_MASK)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (sg->length > (MSRC_LENGTH_MASK * 16))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\tfor (sg = msg->spu.dst; sg; sg = sg_next(sg)) {\n\t\tif (sg->length & 0xf) {\n\t\t\tif (sg->length > DST_LENGTH_MASK)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\tif (sg->length > (MDST_LENGTH_MASK * 16))\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic u32 flexrm_spu_estimate_nonheader_desc_count(struct brcm_message *msg)\n{\n\tu32 cnt = 0;\n\tunsigned int dst_target = 0;\n\tstruct scatterlist *src_sg = msg->spu.src, *dst_sg = msg->spu.dst;\n\n\twhile (src_sg || dst_sg) {\n\t\tif (src_sg) {\n\t\t\tcnt++;\n\t\t\tdst_target = src_sg->length;\n\t\t\tsrc_sg = sg_next(src_sg);\n\t\t} else\n\t\t\tdst_target = UINT_MAX;\n\n\t\twhile (dst_target && dst_sg) {\n\t\t\tcnt++;\n\t\t\tif (dst_sg->length < dst_target)\n\t\t\t\tdst_target -= dst_sg->length;\n\t\t\telse\n\t\t\t\tdst_target = 0;\n\t\t\tdst_sg = sg_next(dst_sg);\n\t\t}\n\t}\n\n\treturn cnt;\n}\n\nstatic int flexrm_spu_dma_map(struct device *dev, struct brcm_message *msg)\n{\n\tint rc;\n\n\trc = dma_map_sg(dev, msg->spu.src, sg_nents(msg->spu.src),\n\t\t\tDMA_TO_DEVICE);\n\tif (!rc)\n\t\treturn -EIO;\n\n\trc = dma_map_sg(dev, msg->spu.dst, sg_nents(msg->spu.dst),\n\t\t\tDMA_FROM_DEVICE);\n\tif (!rc) {\n\t\tdma_unmap_sg(dev, msg->spu.src, sg_nents(msg->spu.src),\n\t\t\t     DMA_TO_DEVICE);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic void flexrm_spu_dma_unmap(struct device *dev, struct brcm_message *msg)\n{\n\tdma_unmap_sg(dev, msg->spu.dst, sg_nents(msg->spu.dst),\n\t\t     DMA_FROM_DEVICE);\n\tdma_unmap_sg(dev, msg->spu.src, sg_nents(msg->spu.src),\n\t\t     DMA_TO_DEVICE);\n}\n\nstatic void *flexrm_spu_write_descs(struct brcm_message *msg, u32 nhcnt,\n\t\t\t\t     u32 reqid, void *desc_ptr, u32 toggle,\n\t\t\t\t     void *start_desc, void *end_desc)\n{\n\tu64 d;\n\tu32 nhpos = 0;\n\tvoid *orig_desc_ptr = desc_ptr;\n\tunsigned int dst_target = 0;\n\tstruct scatterlist *src_sg = msg->spu.src, *dst_sg = msg->spu.dst;\n\n\twhile (src_sg || dst_sg) {\n\t\tif (src_sg) {\n\t\t\tif (sg_dma_len(src_sg) & 0xf)\n\t\t\t\td = flexrm_src_desc(sg_dma_address(src_sg),\n\t\t\t\t\t\t     sg_dma_len(src_sg));\n\t\t\telse\n\t\t\t\td = flexrm_msrc_desc(sg_dma_address(src_sg),\n\t\t\t\t\t\t      sg_dma_len(src_sg)/16);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t\tdst_target = sg_dma_len(src_sg);\n\t\t\tsrc_sg = sg_next(src_sg);\n\t\t} else\n\t\t\tdst_target = UINT_MAX;\n\n\t\twhile (dst_target && dst_sg) {\n\t\t\tif (sg_dma_len(dst_sg) & 0xf)\n\t\t\t\td = flexrm_dst_desc(sg_dma_address(dst_sg),\n\t\t\t\t\t\t     sg_dma_len(dst_sg));\n\t\t\telse\n\t\t\t\td = flexrm_mdst_desc(sg_dma_address(dst_sg),\n\t\t\t\t\t\t      sg_dma_len(dst_sg)/16);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t\tif (sg_dma_len(dst_sg) < dst_target)\n\t\t\t\tdst_target -= sg_dma_len(dst_sg);\n\t\t\telse\n\t\t\t\tdst_target = 0;\n\t\t\tdst_sg = sg_next(dst_sg);\n\t\t}\n\t}\n\n\t \n\tflexrm_write_desc(desc_ptr, flexrm_null_desc(!toggle));\n\n\t \n\twmb();\n\n\t \n\tflexrm_flip_header_toggle(orig_desc_ptr);\n\n\treturn desc_ptr;\n}\n\nstatic bool flexrm_sba_sanity_check(struct brcm_message *msg)\n{\n\tu32 i;\n\n\tif (!msg->sba.cmds || !msg->sba.cmds_count)\n\t\treturn false;\n\n\tfor (i = 0; i < msg->sba.cmds_count; i++) {\n\t\tif (((msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_B) ||\n\t\t     (msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_C)) &&\n\t\t    (msg->sba.cmds[i].flags & BRCM_SBA_CMD_HAS_OUTPUT))\n\t\t\treturn false;\n\t\tif ((msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_B) &&\n\t\t    (msg->sba.cmds[i].data_len > SRCT_LENGTH_MASK))\n\t\t\treturn false;\n\t\tif ((msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_C) &&\n\t\t    (msg->sba.cmds[i].data_len > SRCT_LENGTH_MASK))\n\t\t\treturn false;\n\t\tif ((msg->sba.cmds[i].flags & BRCM_SBA_CMD_HAS_RESP) &&\n\t\t    (msg->sba.cmds[i].resp_len > DSTT_LENGTH_MASK))\n\t\t\treturn false;\n\t\tif ((msg->sba.cmds[i].flags & BRCM_SBA_CMD_HAS_OUTPUT) &&\n\t\t    (msg->sba.cmds[i].data_len > DSTT_LENGTH_MASK))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic u32 flexrm_sba_estimate_nonheader_desc_count(struct brcm_message *msg)\n{\n\tu32 i, cnt;\n\n\tcnt = 0;\n\tfor (i = 0; i < msg->sba.cmds_count; i++) {\n\t\tcnt++;\n\n\t\tif ((msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_B) ||\n\t\t    (msg->sba.cmds[i].flags & BRCM_SBA_CMD_TYPE_C))\n\t\t\tcnt++;\n\n\t\tif (msg->sba.cmds[i].flags & BRCM_SBA_CMD_HAS_RESP)\n\t\t\tcnt++;\n\n\t\tif (msg->sba.cmds[i].flags & BRCM_SBA_CMD_HAS_OUTPUT)\n\t\t\tcnt++;\n\t}\n\n\treturn cnt;\n}\n\nstatic void *flexrm_sba_write_descs(struct brcm_message *msg, u32 nhcnt,\n\t\t\t\t     u32 reqid, void *desc_ptr, u32 toggle,\n\t\t\t\t     void *start_desc, void *end_desc)\n{\n\tu64 d;\n\tu32 i, nhpos = 0;\n\tstruct brcm_sba_command *c;\n\tvoid *orig_desc_ptr = desc_ptr;\n\n\t \n\tfor (i = 0; i < msg->sba.cmds_count; i++) {\n\t\tc = &msg->sba.cmds[i];\n\n\t\tif ((c->flags & BRCM_SBA_CMD_HAS_RESP) &&\n\t\t    (c->flags & BRCM_SBA_CMD_HAS_OUTPUT)) {\n\t\t\t \n\t\t\td = flexrm_dst_desc(c->resp, c->resp_len);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t} else if (c->flags & BRCM_SBA_CMD_HAS_RESP) {\n\t\t\t \n\t\t\td = flexrm_dstt_desc(c->resp, c->resp_len);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t}\n\n\t\tif (c->flags & BRCM_SBA_CMD_HAS_OUTPUT) {\n\t\t\t \n\t\t\td = flexrm_dstt_desc(c->data, c->data_len);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t}\n\n\t\tif (c->flags & BRCM_SBA_CMD_TYPE_B) {\n\t\t\t \n\t\t\td = flexrm_imm_desc(c->cmd);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t} else {\n\t\t\t \n\t\t\td = flexrm_immt_desc(c->cmd);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t}\n\n\t\tif ((c->flags & BRCM_SBA_CMD_TYPE_B) ||\n\t\t    (c->flags & BRCM_SBA_CMD_TYPE_C)) {\n\t\t\t \n\t\t\td = flexrm_srct_desc(c->data, c->data_len);\n\t\t\tflexrm_enqueue_desc(nhpos, nhcnt, reqid,\n\t\t\t\t\t     d, &desc_ptr, &toggle,\n\t\t\t\t\t     start_desc, end_desc);\n\t\t\tnhpos++;\n\t\t}\n\t}\n\n\t \n\tflexrm_write_desc(desc_ptr, flexrm_null_desc(!toggle));\n\n\t \n\twmb();\n\n\t \n\tflexrm_flip_header_toggle(orig_desc_ptr);\n\n\treturn desc_ptr;\n}\n\nstatic bool flexrm_sanity_check(struct brcm_message *msg)\n{\n\tif (!msg)\n\t\treturn false;\n\n\tswitch (msg->type) {\n\tcase BRCM_MESSAGE_SPU:\n\t\treturn flexrm_spu_sanity_check(msg);\n\tcase BRCM_MESSAGE_SBA:\n\t\treturn flexrm_sba_sanity_check(msg);\n\tdefault:\n\t\treturn false;\n\t};\n}\n\nstatic u32 flexrm_estimate_nonheader_desc_count(struct brcm_message *msg)\n{\n\tif (!msg)\n\t\treturn 0;\n\n\tswitch (msg->type) {\n\tcase BRCM_MESSAGE_SPU:\n\t\treturn flexrm_spu_estimate_nonheader_desc_count(msg);\n\tcase BRCM_MESSAGE_SBA:\n\t\treturn flexrm_sba_estimate_nonheader_desc_count(msg);\n\tdefault:\n\t\treturn 0;\n\t};\n}\n\nstatic int flexrm_dma_map(struct device *dev, struct brcm_message *msg)\n{\n\tif (!dev || !msg)\n\t\treturn -EINVAL;\n\n\tswitch (msg->type) {\n\tcase BRCM_MESSAGE_SPU:\n\t\treturn flexrm_spu_dma_map(dev, msg);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic void flexrm_dma_unmap(struct device *dev, struct brcm_message *msg)\n{\n\tif (!dev || !msg)\n\t\treturn;\n\n\tswitch (msg->type) {\n\tcase BRCM_MESSAGE_SPU:\n\t\tflexrm_spu_dma_unmap(dev, msg);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void *flexrm_write_descs(struct brcm_message *msg, u32 nhcnt,\n\t\t\t\tu32 reqid, void *desc_ptr, u32 toggle,\n\t\t\t\tvoid *start_desc, void *end_desc)\n{\n\tif (!msg || !desc_ptr || !start_desc || !end_desc)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\tif ((desc_ptr < start_desc) || (end_desc <= desc_ptr))\n\t\treturn ERR_PTR(-ERANGE);\n\n\tswitch (msg->type) {\n\tcase BRCM_MESSAGE_SPU:\n\t\treturn flexrm_spu_write_descs(msg, nhcnt, reqid,\n\t\t\t\t\t       desc_ptr, toggle,\n\t\t\t\t\t       start_desc, end_desc);\n\tcase BRCM_MESSAGE_SBA:\n\t\treturn flexrm_sba_write_descs(msg, nhcnt, reqid,\n\t\t\t\t\t       desc_ptr, toggle,\n\t\t\t\t\t       start_desc, end_desc);\n\tdefault:\n\t\treturn ERR_PTR(-ENOTSUPP);\n\t};\n}\n\n \n\nstatic void flexrm_write_config_in_seqfile(struct flexrm_mbox *mbox,\n\t\t\t\t\t   struct seq_file *file)\n{\n\tint i;\n\tconst char *state;\n\tstruct flexrm_ring *ring;\n\n\tseq_printf(file, \"%-5s %-9s %-18s %-10s %-18s %-10s\\n\",\n\t\t   \"Ring#\", \"State\", \"BD_Addr\", \"BD_Size\",\n\t\t   \"Cmpl_Addr\", \"Cmpl_Size\");\n\n\tfor (i = 0; i < mbox->num_rings; i++) {\n\t\tring = &mbox->rings[i];\n\t\tif (readl(ring->regs + RING_CONTROL) &\n\t\t    BIT(CONTROL_ACTIVE_SHIFT))\n\t\t\tstate = \"active\";\n\t\telse\n\t\t\tstate = \"inactive\";\n\t\tseq_printf(file,\n\t\t\t   \"%-5d %-9s 0x%016llx 0x%08x 0x%016llx 0x%08x\\n\",\n\t\t\t   ring->num, state,\n\t\t\t   (unsigned long long)ring->bd_dma_base,\n\t\t\t   (u32)RING_BD_SIZE,\n\t\t\t   (unsigned long long)ring->cmpl_dma_base,\n\t\t\t   (u32)RING_CMPL_SIZE);\n\t}\n}\n\nstatic void flexrm_write_stats_in_seqfile(struct flexrm_mbox *mbox,\n\t\t\t\t\t  struct seq_file *file)\n{\n\tint i;\n\tu32 val, bd_read_offset;\n\tstruct flexrm_ring *ring;\n\n\tseq_printf(file, \"%-5s %-10s %-10s %-10s %-11s %-11s\\n\",\n\t\t   \"Ring#\", \"BD_Read\", \"BD_Write\",\n\t\t   \"Cmpl_Read\", \"Submitted\", \"Completed\");\n\n\tfor (i = 0; i < mbox->num_rings; i++) {\n\t\tring = &mbox->rings[i];\n\t\tbd_read_offset = readl_relaxed(ring->regs + RING_BD_READ_PTR);\n\t\tval = readl_relaxed(ring->regs + RING_BD_START_ADDR);\n\t\tbd_read_offset *= RING_DESC_SIZE;\n\t\tbd_read_offset += (u32)(BD_START_ADDR_DECODE(val) -\n\t\t\t\t\tring->bd_dma_base);\n\t\tseq_printf(file, \"%-5d 0x%08x 0x%08x 0x%08x %-11d %-11d\\n\",\n\t\t\t   ring->num,\n\t\t\t   (u32)bd_read_offset,\n\t\t\t   (u32)ring->bd_write_offset,\n\t\t\t   (u32)ring->cmpl_read_offset,\n\t\t\t   (u32)atomic_read(&ring->msg_send_count),\n\t\t\t   (u32)atomic_read(&ring->msg_cmpl_count));\n\t}\n}\n\nstatic int flexrm_new_request(struct flexrm_ring *ring,\n\t\t\t\tstruct brcm_message *batch_msg,\n\t\t\t\tstruct brcm_message *msg)\n{\n\tvoid *next;\n\tunsigned long flags;\n\tu32 val, count, nhcnt;\n\tu32 read_offset, write_offset;\n\tbool exit_cleanup = false;\n\tint ret = 0, reqid;\n\n\t \n\tif (!flexrm_sanity_check(msg))\n\t\treturn -EIO;\n\tmsg->error = 0;\n\n\t \n\tspin_lock_irqsave(&ring->lock, flags);\n\treqid = bitmap_find_free_region(ring->requests_bmap,\n\t\t\t\t\tRING_MAX_REQ_COUNT, 0);\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\tif (reqid < 0)\n\t\treturn -ENOSPC;\n\tring->requests[reqid] = msg;\n\n\t \n\tret = flexrm_dma_map(ring->mbox->dev, msg);\n\tif (ret < 0) {\n\t\tring->requests[reqid] = NULL;\n\t\tspin_lock_irqsave(&ring->lock, flags);\n\t\tbitmap_release_region(ring->requests_bmap, reqid, 0);\n\t\tspin_unlock_irqrestore(&ring->lock, flags);\n\t\treturn ret;\n\t}\n\n\t \n\tread_offset = readl_relaxed(ring->regs + RING_BD_READ_PTR);\n\tval = readl_relaxed(ring->regs + RING_BD_START_ADDR);\n\tread_offset *= RING_DESC_SIZE;\n\tread_offset += (u32)(BD_START_ADDR_DECODE(val) - ring->bd_dma_base);\n\n\t \n\tnhcnt = flexrm_estimate_nonheader_desc_count(msg);\n\tcount = flexrm_estimate_header_desc_count(nhcnt) + nhcnt + 1;\n\n\t \n\twrite_offset = ring->bd_write_offset;\n\twhile (count) {\n\t\tif (!flexrm_is_next_table_desc(ring->bd_base + write_offset))\n\t\t\tcount--;\n\t\twrite_offset += RING_DESC_SIZE;\n\t\tif (write_offset == RING_BD_SIZE)\n\t\t\twrite_offset = 0x0;\n\t\tif (write_offset == read_offset)\n\t\t\tbreak;\n\t}\n\tif (count) {\n\t\tret = -ENOSPC;\n\t\texit_cleanup = true;\n\t\tgoto exit;\n\t}\n\n\t \n\tnext = flexrm_write_descs(msg, nhcnt, reqid,\n\t\t\tring->bd_base + ring->bd_write_offset,\n\t\t\tRING_BD_TOGGLE_VALID(ring->bd_write_offset),\n\t\t\tring->bd_base, ring->bd_base + RING_BD_SIZE);\n\tif (IS_ERR(next)) {\n\t\tret = PTR_ERR(next);\n\t\texit_cleanup = true;\n\t\tgoto exit;\n\t}\n\n\t \n\tring->bd_write_offset = (unsigned long)(next - ring->bd_base);\n\n\t \n\tatomic_inc_return(&ring->msg_send_count);\n\nexit:\n\t \n\tmsg->error = ret;\n\n\t \n\tif (exit_cleanup) {\n\t\tflexrm_dma_unmap(ring->mbox->dev, msg);\n\t\tring->requests[reqid] = NULL;\n\t\tspin_lock_irqsave(&ring->lock, flags);\n\t\tbitmap_release_region(ring->requests_bmap, reqid, 0);\n\t\tspin_unlock_irqrestore(&ring->lock, flags);\n\t}\n\n\treturn ret;\n}\n\nstatic int flexrm_process_completions(struct flexrm_ring *ring)\n{\n\tu64 desc;\n\tint err, count = 0;\n\tunsigned long flags;\n\tstruct brcm_message *msg = NULL;\n\tu32 reqid, cmpl_read_offset, cmpl_write_offset;\n\tstruct mbox_chan *chan = &ring->mbox->controller.chans[ring->num];\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\n\t \n\tcmpl_write_offset = readl_relaxed(ring->regs + RING_CMPL_WRITE_PTR);\n\tcmpl_write_offset *= RING_DESC_SIZE;\n\tcmpl_read_offset = ring->cmpl_read_offset;\n\tring->cmpl_read_offset = cmpl_write_offset;\n\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\n\t \n\treqid = 0;\n\twhile (cmpl_read_offset != cmpl_write_offset) {\n\t\t \n\t\tdesc = *((u64 *)(ring->cmpl_base + cmpl_read_offset));\n\n\t\t \n\t\tcmpl_read_offset += RING_DESC_SIZE;\n\t\tif (cmpl_read_offset == RING_CMPL_SIZE)\n\t\t\tcmpl_read_offset = 0;\n\n\t\t \n\t\terr = flexrm_cmpl_desc_to_error(desc);\n\t\tif (err < 0) {\n\t\t\tdev_warn(ring->mbox->dev,\n\t\t\t\"ring%d got completion desc=0x%lx with error %d\\n\",\n\t\t\tring->num, (unsigned long)desc, err);\n\t\t}\n\n\t\t \n\t\treqid = flexrm_cmpl_desc_to_reqid(desc);\n\n\t\t \n\t\tmsg = ring->requests[reqid];\n\t\tif (!msg) {\n\t\t\tdev_warn(ring->mbox->dev,\n\t\t\t\"ring%d null msg pointer for completion desc=0x%lx\\n\",\n\t\t\tring->num, (unsigned long)desc);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tring->requests[reqid] = NULL;\n\t\tspin_lock_irqsave(&ring->lock, flags);\n\t\tbitmap_release_region(ring->requests_bmap, reqid, 0);\n\t\tspin_unlock_irqrestore(&ring->lock, flags);\n\n\t\t \n\t\tflexrm_dma_unmap(ring->mbox->dev, msg);\n\n\t\t \n\t\tmsg->error = err;\n\t\tmbox_chan_received_data(chan, msg);\n\n\t\t \n\t\tatomic_inc_return(&ring->msg_cmpl_count);\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\n \n\nstatic int flexrm_debugfs_conf_show(struct seq_file *file, void *offset)\n{\n\tstruct flexrm_mbox *mbox = dev_get_drvdata(file->private);\n\n\t \n\tflexrm_write_config_in_seqfile(mbox, file);\n\n\treturn 0;\n}\n\nstatic int flexrm_debugfs_stats_show(struct seq_file *file, void *offset)\n{\n\tstruct flexrm_mbox *mbox = dev_get_drvdata(file->private);\n\n\t \n\tflexrm_write_stats_in_seqfile(mbox, file);\n\n\treturn 0;\n}\n\n \n\nstatic irqreturn_t flexrm_irq_event(int irq, void *dev_id)\n{\n\t \n\t \n\n\treturn IRQ_WAKE_THREAD;\n}\n\nstatic irqreturn_t flexrm_irq_thread(int irq, void *dev_id)\n{\n\tflexrm_process_completions(dev_id);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic int flexrm_send_data(struct mbox_chan *chan, void *data)\n{\n\tint i, rc;\n\tstruct flexrm_ring *ring = chan->con_priv;\n\tstruct brcm_message *msg = data;\n\n\tif (msg->type == BRCM_MESSAGE_BATCH) {\n\t\tfor (i = msg->batch.msgs_queued;\n\t\t     i < msg->batch.msgs_count; i++) {\n\t\t\trc = flexrm_new_request(ring, msg,\n\t\t\t\t\t\t &msg->batch.msgs[i]);\n\t\t\tif (rc) {\n\t\t\t\tmsg->error = rc;\n\t\t\t\treturn rc;\n\t\t\t}\n\t\t\tmsg->batch.msgs_queued++;\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn flexrm_new_request(ring, NULL, data);\n}\n\nstatic bool flexrm_peek_data(struct mbox_chan *chan)\n{\n\tint cnt = flexrm_process_completions(chan->con_priv);\n\n\treturn (cnt > 0) ? true : false;\n}\n\nstatic int flexrm_startup(struct mbox_chan *chan)\n{\n\tu64 d;\n\tu32 val, off;\n\tint ret = 0;\n\tdma_addr_t next_addr;\n\tstruct flexrm_ring *ring = chan->con_priv;\n\n\t \n\tring->bd_base = dma_pool_alloc(ring->mbox->bd_pool,\n\t\t\t\t       GFP_KERNEL, &ring->bd_dma_base);\n\tif (!ring->bd_base) {\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"can't allocate BD memory for ring%d\\n\",\n\t\t\tring->num);\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tfor (off = 0; off < RING_BD_SIZE; off += RING_DESC_SIZE) {\n\t\tnext_addr = off + RING_DESC_SIZE;\n\t\tif (next_addr == RING_BD_SIZE)\n\t\t\tnext_addr = 0;\n\t\tnext_addr += ring->bd_dma_base;\n\t\tif (RING_BD_ALIGN_CHECK(next_addr))\n\t\t\td = flexrm_next_table_desc(RING_BD_TOGGLE_VALID(off),\n\t\t\t\t\t\t    next_addr);\n\t\telse\n\t\t\td = flexrm_null_desc(RING_BD_TOGGLE_INVALID(off));\n\t\tflexrm_write_desc(ring->bd_base + off, d);\n\t}\n\n\t \n\tring->cmpl_base = dma_pool_zalloc(ring->mbox->cmpl_pool,\n\t\t\t\t\t GFP_KERNEL, &ring->cmpl_dma_base);\n\tif (!ring->cmpl_base) {\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"can't allocate completion memory for ring%d\\n\",\n\t\t\tring->num);\n\t\tret = -ENOMEM;\n\t\tgoto fail_free_bd_memory;\n\t}\n\n\t \n\tif (ring->irq == UINT_MAX) {\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"ring%d IRQ not available\\n\", ring->num);\n\t\tret = -ENODEV;\n\t\tgoto fail_free_cmpl_memory;\n\t}\n\tret = request_threaded_irq(ring->irq,\n\t\t\t\t   flexrm_irq_event,\n\t\t\t\t   flexrm_irq_thread,\n\t\t\t\t   0, dev_name(ring->mbox->dev), ring);\n\tif (ret) {\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"failed to request ring%d IRQ\\n\", ring->num);\n\t\tgoto fail_free_cmpl_memory;\n\t}\n\tring->irq_requested = true;\n\n\t \n\tring->irq_aff_hint = CPU_MASK_NONE;\n\tval = ring->mbox->num_rings;\n\tval = (num_online_cpus() < val) ? val / num_online_cpus() : 1;\n\tcpumask_set_cpu((ring->num / val) % num_online_cpus(),\n\t\t\t&ring->irq_aff_hint);\n\tret = irq_update_affinity_hint(ring->irq, &ring->irq_aff_hint);\n\tif (ret) {\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"failed to set IRQ affinity hint for ring%d\\n\",\n\t\t\tring->num);\n\t\tgoto fail_free_irq;\n\t}\n\n\t \n\twritel_relaxed(0x0, ring->regs + RING_CONTROL);\n\n\t \n\tval = BD_START_ADDR_VALUE(ring->bd_dma_base);\n\twritel_relaxed(val, ring->regs + RING_BD_START_ADDR);\n\n\t \n\tring->bd_write_offset =\n\t\t\treadl_relaxed(ring->regs + RING_BD_WRITE_PTR);\n\tring->bd_write_offset *= RING_DESC_SIZE;\n\n\t \n\tval = CMPL_START_ADDR_VALUE(ring->cmpl_dma_base);\n\twritel_relaxed(val, ring->regs + RING_CMPL_START_ADDR);\n\n\t \n\tring->cmpl_read_offset =\n\t\t\treadl_relaxed(ring->regs + RING_CMPL_WRITE_PTR);\n\tring->cmpl_read_offset *= RING_DESC_SIZE;\n\n\t \n\treadl_relaxed(ring->regs + RING_NUM_REQ_RECV_LS);\n\treadl_relaxed(ring->regs + RING_NUM_REQ_RECV_MS);\n\treadl_relaxed(ring->regs + RING_NUM_REQ_TRANS_LS);\n\treadl_relaxed(ring->regs + RING_NUM_REQ_TRANS_MS);\n\treadl_relaxed(ring->regs + RING_NUM_REQ_OUTSTAND);\n\n\t \n\tval = 0;\n\tval |= (ring->msi_timer_val << MSI_TIMER_VAL_SHIFT);\n\tval |= BIT(MSI_ENABLE_SHIFT);\n\tval |= (ring->msi_count_threshold & MSI_COUNT_MASK) << MSI_COUNT_SHIFT;\n\twritel_relaxed(val, ring->regs + RING_MSI_CONTROL);\n\n\t \n\tval = BIT(CONTROL_ACTIVE_SHIFT);\n\twritel_relaxed(val, ring->regs + RING_CONTROL);\n\n\t \n\tatomic_set(&ring->msg_send_count, 0);\n\tatomic_set(&ring->msg_cmpl_count, 0);\n\n\treturn 0;\n\nfail_free_irq:\n\tfree_irq(ring->irq, ring);\n\tring->irq_requested = false;\nfail_free_cmpl_memory:\n\tdma_pool_free(ring->mbox->cmpl_pool,\n\t\t      ring->cmpl_base, ring->cmpl_dma_base);\n\tring->cmpl_base = NULL;\nfail_free_bd_memory:\n\tdma_pool_free(ring->mbox->bd_pool,\n\t\t      ring->bd_base, ring->bd_dma_base);\n\tring->bd_base = NULL;\nfail:\n\treturn ret;\n}\n\nstatic void flexrm_shutdown(struct mbox_chan *chan)\n{\n\tu32 reqid;\n\tunsigned int timeout;\n\tstruct brcm_message *msg;\n\tstruct flexrm_ring *ring = chan->con_priv;\n\n\t \n\twritel_relaxed(0x0, ring->regs + RING_CONTROL);\n\n\t \n\ttimeout = 1000;  \n\twritel_relaxed(BIT(CONTROL_FLUSH_SHIFT),\n\t\t\tring->regs + RING_CONTROL);\n\tdo {\n\t\tif (readl_relaxed(ring->regs + RING_FLUSH_DONE) &\n\t\t    FLUSH_DONE_MASK)\n\t\t\tbreak;\n\t\tmdelay(1);\n\t} while (--timeout);\n\tif (!timeout)\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"setting ring%d flush state timedout\\n\", ring->num);\n\n\t \n\ttimeout = 1000;  \n\twritel_relaxed(0x0, ring->regs + RING_CONTROL);\n\tdo {\n\t\tif (!(readl_relaxed(ring->regs + RING_FLUSH_DONE) &\n\t\t      FLUSH_DONE_MASK))\n\t\t\tbreak;\n\t\tmdelay(1);\n\t} while (--timeout);\n\tif (!timeout)\n\t\tdev_err(ring->mbox->dev,\n\t\t\t\"clearing ring%d flush state timedout\\n\", ring->num);\n\n\t \n\tfor (reqid = 0; reqid < RING_MAX_REQ_COUNT; reqid++) {\n\t\tmsg = ring->requests[reqid];\n\t\tif (!msg)\n\t\t\tcontinue;\n\n\t\t \n\t\tring->requests[reqid] = NULL;\n\n\t\t \n\t\tflexrm_dma_unmap(ring->mbox->dev, msg);\n\n\t\t \n\t\tmsg->error = -EIO;\n\t\tmbox_chan_received_data(chan, msg);\n\t}\n\n\t \n\tbitmap_zero(ring->requests_bmap, RING_MAX_REQ_COUNT);\n\n\t \n\tif (ring->irq_requested) {\n\t\tirq_update_affinity_hint(ring->irq, NULL);\n\t\tfree_irq(ring->irq, ring);\n\t\tring->irq_requested = false;\n\t}\n\n\t \n\tif (ring->cmpl_base) {\n\t\tdma_pool_free(ring->mbox->cmpl_pool,\n\t\t\t      ring->cmpl_base, ring->cmpl_dma_base);\n\t\tring->cmpl_base = NULL;\n\t}\n\n\t \n\tif (ring->bd_base) {\n\t\tdma_pool_free(ring->mbox->bd_pool,\n\t\t\t      ring->bd_base, ring->bd_dma_base);\n\t\tring->bd_base = NULL;\n\t}\n}\n\nstatic const struct mbox_chan_ops flexrm_mbox_chan_ops = {\n\t.send_data\t= flexrm_send_data,\n\t.startup\t= flexrm_startup,\n\t.shutdown\t= flexrm_shutdown,\n\t.peek_data\t= flexrm_peek_data,\n};\n\nstatic struct mbox_chan *flexrm_mbox_of_xlate(struct mbox_controller *cntlr,\n\t\t\t\t\tconst struct of_phandle_args *pa)\n{\n\tstruct mbox_chan *chan;\n\tstruct flexrm_ring *ring;\n\n\tif (pa->args_count < 3)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (pa->args[0] >= cntlr->num_chans)\n\t\treturn ERR_PTR(-ENOENT);\n\n\tif (pa->args[1] > MSI_COUNT_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (pa->args[2] > MSI_TIMER_VAL_MASK)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tchan = &cntlr->chans[pa->args[0]];\n\tring = chan->con_priv;\n\tring->msi_count_threshold = pa->args[1];\n\tring->msi_timer_val = pa->args[2];\n\n\treturn chan;\n}\n\n \n\nstatic void flexrm_mbox_msi_write(struct msi_desc *desc, struct msi_msg *msg)\n{\n\tstruct device *dev = msi_desc_to_dev(desc);\n\tstruct flexrm_mbox *mbox = dev_get_drvdata(dev);\n\tstruct flexrm_ring *ring = &mbox->rings[desc->msi_index];\n\n\t \n\twritel_relaxed(msg->address_lo, ring->regs + RING_MSI_ADDR_LS);\n\twritel_relaxed(msg->address_hi, ring->regs + RING_MSI_ADDR_MS);\n\twritel_relaxed(msg->data, ring->regs + RING_MSI_DATA_VALUE);\n}\n\nstatic int flexrm_mbox_probe(struct platform_device *pdev)\n{\n\tint index, ret = 0;\n\tvoid __iomem *regs;\n\tvoid __iomem *regs_end;\n\tstruct resource *iomem;\n\tstruct flexrm_ring *ring;\n\tstruct flexrm_mbox *mbox;\n\tstruct device *dev = &pdev->dev;\n\n\t \n\tmbox = devm_kzalloc(dev, sizeof(*mbox), GFP_KERNEL);\n\tif (!mbox) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tmbox->dev = dev;\n\tplatform_set_drvdata(pdev, mbox);\n\n\t \n\tmbox->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &iomem);\n\tif (!iomem || (resource_size(iomem) < RING_REGS_SIZE)) {\n\t\tret = -ENODEV;\n\t\tgoto fail;\n\t} else if (IS_ERR(mbox->regs)) {\n\t\tret = PTR_ERR(mbox->regs);\n\t\tgoto fail;\n\t}\n\tregs_end = mbox->regs + resource_size(iomem);\n\n\t \n\tmbox->num_rings = 0;\n\tfor (regs = mbox->regs; regs < regs_end; regs += RING_REGS_SIZE) {\n\t\tif (readl_relaxed(regs + RING_VER) == RING_VER_MAGIC)\n\t\t\tmbox->num_rings++;\n\t}\n\tif (!mbox->num_rings) {\n\t\tret = -ENODEV;\n\t\tgoto fail;\n\t}\n\n\t \n\tring = devm_kcalloc(dev, mbox->num_rings, sizeof(*ring), GFP_KERNEL);\n\tif (!ring) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\tmbox->rings = ring;\n\n\t \n\tregs = mbox->regs;\n\tfor (index = 0; index < mbox->num_rings; index++) {\n\t\tring = &mbox->rings[index];\n\t\tring->num = index;\n\t\tring->mbox = mbox;\n\t\twhile ((regs < regs_end) &&\n\t\t       (readl_relaxed(regs + RING_VER) != RING_VER_MAGIC))\n\t\t\tregs += RING_REGS_SIZE;\n\t\tif (regs_end <= regs) {\n\t\t\tret = -ENODEV;\n\t\t\tgoto fail;\n\t\t}\n\t\tring->regs = regs;\n\t\tregs += RING_REGS_SIZE;\n\t\tring->irq = UINT_MAX;\n\t\tring->irq_requested = false;\n\t\tring->msi_timer_val = MSI_TIMER_VAL_MASK;\n\t\tring->msi_count_threshold = 0x1;\n\t\tmemset(ring->requests, 0, sizeof(ring->requests));\n\t\tring->bd_base = NULL;\n\t\tring->bd_dma_base = 0;\n\t\tring->cmpl_base = NULL;\n\t\tring->cmpl_dma_base = 0;\n\t\tatomic_set(&ring->msg_send_count, 0);\n\t\tatomic_set(&ring->msg_cmpl_count, 0);\n\t\tspin_lock_init(&ring->lock);\n\t\tbitmap_zero(ring->requests_bmap, RING_MAX_REQ_COUNT);\n\t\tring->cmpl_read_offset = 0;\n\t}\n\n\t \n\tret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(40));\n\tif (ret) {\n\t\tret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));\n\t\tif (ret)\n\t\t\tgoto fail;\n\t}\n\n\t \n\tmbox->bd_pool = dma_pool_create(\"bd\", dev, RING_BD_SIZE,\n\t\t\t\t\t1 << RING_BD_ALIGN_ORDER, 0);\n\tif (!mbox->bd_pool) {\n\t\tret = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\tmbox->cmpl_pool = dma_pool_create(\"cmpl\", dev, RING_CMPL_SIZE,\n\t\t\t\t\t  1 << RING_CMPL_ALIGN_ORDER, 0);\n\tif (!mbox->cmpl_pool) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_destroy_bd_pool;\n\t}\n\n\t \n\tret = platform_msi_domain_alloc_irqs(dev, mbox->num_rings,\n\t\t\t\t\t\tflexrm_mbox_msi_write);\n\tif (ret)\n\t\tgoto fail_destroy_cmpl_pool;\n\n\t \n\tfor (index = 0; index < mbox->num_rings; index++)\n\t\tmbox->rings[index].irq = msi_get_virq(dev, index);\n\n\t \n\tif (!debugfs_initialized())\n\t\tgoto skip_debugfs;\n\n\t \n\tmbox->root = debugfs_create_dir(dev_name(mbox->dev), NULL);\n\n\t \n\tdebugfs_create_devm_seqfile(mbox->dev, \"config\", mbox->root,\n\t\t\t\t    flexrm_debugfs_conf_show);\n\n\t \n\tdebugfs_create_devm_seqfile(mbox->dev, \"stats\", mbox->root,\n\t\t\t\t    flexrm_debugfs_stats_show);\n\nskip_debugfs:\n\n\t \n\tmbox->controller.txdone_irq = false;\n\tmbox->controller.txdone_poll = false;\n\tmbox->controller.ops = &flexrm_mbox_chan_ops;\n\tmbox->controller.dev = dev;\n\tmbox->controller.num_chans = mbox->num_rings;\n\tmbox->controller.of_xlate = flexrm_mbox_of_xlate;\n\tmbox->controller.chans = devm_kcalloc(dev, mbox->num_rings,\n\t\t\t\tsizeof(*mbox->controller.chans), GFP_KERNEL);\n\tif (!mbox->controller.chans) {\n\t\tret = -ENOMEM;\n\t\tgoto fail_free_debugfs_root;\n\t}\n\tfor (index = 0; index < mbox->num_rings; index++)\n\t\tmbox->controller.chans[index].con_priv = &mbox->rings[index];\n\n\t \n\tret = devm_mbox_controller_register(dev, &mbox->controller);\n\tif (ret)\n\t\tgoto fail_free_debugfs_root;\n\n\tdev_info(dev, \"registered flexrm mailbox with %d channels\\n\",\n\t\t\tmbox->controller.num_chans);\n\n\treturn 0;\n\nfail_free_debugfs_root:\n\tdebugfs_remove_recursive(mbox->root);\n\tplatform_msi_domain_free_irqs(dev);\nfail_destroy_cmpl_pool:\n\tdma_pool_destroy(mbox->cmpl_pool);\nfail_destroy_bd_pool:\n\tdma_pool_destroy(mbox->bd_pool);\nfail:\n\treturn ret;\n}\n\nstatic int flexrm_mbox_remove(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct flexrm_mbox *mbox = platform_get_drvdata(pdev);\n\n\tdebugfs_remove_recursive(mbox->root);\n\n\tplatform_msi_domain_free_irqs(dev);\n\n\tdma_pool_destroy(mbox->cmpl_pool);\n\tdma_pool_destroy(mbox->bd_pool);\n\n\treturn 0;\n}\n\nstatic const struct of_device_id flexrm_mbox_of_match[] = {\n\t{ .compatible = \"brcm,iproc-flexrm-mbox\", },\n\t{},\n};\nMODULE_DEVICE_TABLE(of, flexrm_mbox_of_match);\n\nstatic struct platform_driver flexrm_mbox_driver = {\n\t.driver = {\n\t\t.name = \"brcm-flexrm-mbox\",\n\t\t.of_match_table = flexrm_mbox_of_match,\n\t},\n\t.probe\t\t= flexrm_mbox_probe,\n\t.remove\t\t= flexrm_mbox_remove,\n};\nmodule_platform_driver(flexrm_mbox_driver);\n\nMODULE_AUTHOR(\"Anup Patel <anup.patel@broadcom.com>\");\nMODULE_DESCRIPTION(\"Broadcom FlexRM mailbox driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}